question_id,title,body,tags
2697275,Is the occurence of two perfect numbers a coincidence?,"The expression $$n^n+n!+1$$ is prime for the following non-negative integers $n\le 7\ 600$ : $$[0, 1, 2, 4, 28, 496]$$ if we assume $0^0=1$ The numbers $28$ and $496$ are perfect numbers. Is this just a coincidence or is there an explanation ? Do further primes of the given form exist ? $1$ is the only odd possible $n$ Proof : Suppose $n\ge 3$ is odd and $p$ is a prime factor of $n+1$. Then clearly $p\le n$ (because $n+1\ge 4$ is even, hence no prime) , hence $p|n!$ and we have $n^n\equiv (-1)^n\equiv -1\mod p$ , ruling out odd numbers $n\ge 3$ What about the even numbers $n$ ? Are there additional conditions explanating the perfect numbers and accelerating the search for further primes ?","['perfect-numbers', 'number-theory', 'factorial', 'prime-numbers', 'elementary-number-theory']"
2697288,Radial derivative of the characteristic function of the unit disc in $\mathbb R^2$.,"I need help to answer this question from Hörmander,
Let $u$ be the characteristic function of the unit disc in $\mathbb  R^2$. 
Calculate $x \frac{\partial u}{\partial x} + y \frac{\partial u}{\partial y}$. The answer in the book is:
minus the arc length measure on the unit circle. I am not sure if my calculations are right. Let $\phi \in D(\mathbb R^2)$, $$\left<x \frac{\partial u}{\partial x} + y\frac{\partial u}{\partial y}, \phi\right>= - \left<u,\frac{\partial {(x \phi(x,y))}}{\partial x} + \frac{\partial (y \phi(x,y))}{\partial y} \right>= - \left<u, \operatorname{div} \begin{bmatrix}x \phi(x,y)\\y \phi(x,y)\end{bmatrix}\right>. $$ Now I think I have to use the Gauss-Green formula to get the length measure, but I don't know how to finish my answer if the above calculations are right. Any help please would be perfect.","['fourier-analysis', 'distribution-theory', 'partial-derivative', 'functional-analysis', 'ordinary-differential-equations']"
2697303,Can the theory of determinants be derived using the definition by row operations?,"This year I'm teaching an elementary course on linear algebra for physics students. Because of that I have been researching the different ways of presenting the definition of the determinant (one of the hardest topics for such a course, in my opinion). In the lecture notes by Terry Loring a definition using elementary row operations is given http://www.math.unm.edu/~loring/links/linear_s06/det_def.pdf I like this definition since seems to me very algorithmic, and uses
an idea which is familiar to the students. Compare this with the other
more popular definitions which are The explicit formula using the sign of permutations The recursive formula using the Laplace development The axiomatic one as the unique multilinear alternate form by rows
(or by columns) that takes the value 1 at the identity matrix. Of course, this definition by row elimination seems very close to this last axiomatic definition. However my question is, do you know if there is some way of deriving the
whole theory of determinants from the definition based on row elimination?
Indeed, it even seems hard to prove directly that the definition is correct (non ambiguous). Do you know some book using this approach?","['education', 'book-recommendation', 'linear-algebra']"
2697407,Does $ \sin^2 x + \cos^2 x = 1 $ still apply on a general manifold?,"So the proof of $ \sin^2 x + \cos^2 x = 1 $ rests upon Pythagoras' theorem and the definitions $ \sin x := O/H $ and $ \cos x := A/H $, where $O$,$H$ and $A$ are the opposite, hypotenuse and adjacent side of a right angled triangle respectively. However if I was on a general manifold with a metric tensor $g$, then Pythagoras' theorem would read $$ g_{ij}x^i x^j = c^2 $$ which would not give me the desired equation above. So my question: Does $ \sin^2 x + \cos^2 x = 1 $ only hold in Euclidean space?","['manifolds', 'trigonometry']"
2697440,Question with the proof of irrationality of $\sqrt{2}$,"I've seen a famous ""one-line"" proof of irrationality of the square root of two that I don't understand at all. It says that if $\sqrt{2}=\frac{m}n$ in lowest terms, $\sqrt{2}=\frac{2n-m}{m-n}$ in lower terms. I have a few questions about this - most fundamentally, how does one get from that first expression to the other? It doesn't seem to be a trivial algebraic manipulation, at least not that I see. Secondly, how do we know that $m-n$ is smaller than $n$?","['algebra-precalculus', 'irrational-numbers', 'proof-explanation']"
2697448,Different ideal and Kähler differentials,"Let $A$ be a Dedekind domain with fraction field $K$, $L/K$ be a finite separable extension and $B$ be the integral closure of $A$ in $L$, which is also a Dedekind domain. Assume all residue extensions are separable, e.g. if $A=\mathbb{Z}$. Let $\delta_{L/K}$ be the different ideal, which is the inverse fractional ideal of the trace-dual of $B$. We can compute the different as the annihilator of the module of Kähler differentials of $B$ over $A$. So the zero locus of the different ideal is the set of points on which the Kähler differentials vanish. But we also know that those primes are the primes that are ramified over some prime in $A$. And if we draw $\operatorname{Spec}(A)$ as a horizontal line (call it $x$-axis) and project down the regular curve $\operatorname{Spec}(B)$, those primes are the points with vertical on which the curve has vertical slope. ""The $x$-coordinate of the tangent vector vanishes"", and Dedekin's central theorem regarding different and ramification tells us moreover that it vanishes with the right order. Is everything I have said so far correct? If so, is there any relation between the Kähler differentials and the $x$-coordinate of the ""tangent vector"" to the curve $B$? And if this is the case, how does one connect the definition of the different with the trace-dual and the inverse fractional ideal to this geometric interpretation? Points where I am stuck: I don't know how to interpret the trace pairing in a geometric way, so I don't know how to interpret trace-dual ideals in a geometric way. Extra (secondary) question: $B$ may not be generated by a single element over $A$, but $\Omega_{B/A}$ can always be generated by a single element. What is the intuition behind this? I get that if $B=A[\alpha]$ then $\Omega_{B/A}$ is generated by $d\alpha$, but if this is not the case, who is the generator and why can we expect to have a single generator?","['number-theory', 'arithmetic-geometry', 'algebraic-number-theory', 'algebraic-geometry']"
2697460,Example where Ermakoff's convergence test is inconclusive,"Ermakoff's test states that for a nonincreasing function $f(x)$, $\sum f(n)$ converges if $$\limsup_{x \to \infty} \frac{e^x f(e^x)}{f(x)} < 1$$ and diverges if $$\liminf_{x \to \infty} \frac{e^x f(e^x)}{f(x)} > 1.$$ This test seems to be very powerful in that any example I try gives a limit of $0$ or $\infty$. Question: Is there an example where Ermakoff's test is inconclusive, i.e. is there an $f(x)$ that gives a limit of 1? More generally, is there an $f(x)$ that gives a finite positive limit? My approach was to toe the line between slowly diverging summands like $\frac{1}{n}, \frac{1}{n \log n}, \frac{1}{n \log n \log\log n}, \ldots$ I tried throwing in special functions that grow differently than a power of a nested logarithm, such as $li(x)$ and $W(x)$ . This didn't get me very far as I ended up wandering aimlessly through examples. By taking the logarithm of the limit and performing substitutions, it's equivalent to look for a nondecreasing function $g(x)$ where $\lim_{x \to \infty} (x - g(x) + g(\log x)) = 0$. This form makes working with series at $x = \infty$ a bit easier. The test can be generalized by looking at $\phi'(x) f(\phi(x))/f(x)$ for any increasing $\phi(x) > x$. Letting $\phi(x) = e^x$ gives Ermakoff's test and letting $\phi(x) = x + 1$ gives d'Alembert's ratio test. Bonus question: Is there a way to find an inconclusive example for any $\phi(x)$? I imagine the faster $\phi(x)$ grows, the harder it becomes.","['sequences-and-series', 'convergence-divergence']"
2697494,Is $\sqrt[4]{-1}=\sqrt(i)$?,I get $\sqrt[4]{-1}=((-1)^{\frac{1}{2}})^{\frac{1}{2}}=\sqrt{i}$. But on the one hand $\sqrt[4]{-1}$ has $4$ roots and on the other $\sqrt{i}$ has just $2$.,['complex-analysis']
2697502,Can permutations be solved by cases? [duplicate],"This question already has an answer here : In how manys can a programming team and a hacking team be selected if Jack does not serve on both teams? (1 answer) Closed 6 years ago . ""A Math Club has 12 members. They have to choose euclid contest team
  of 3 members and a National Math Competition team with 4 members. Students can be on both teams, but James agrees to
  only be on at most one team. In how many ways can both teams be chosen?"" Here's what I thought should be the answer but doesn't make much sense: Case 1: James is in Euclid team: $\therefore $  $11\choose2$ for Euclid Team This would be because James is in team Euclid assumed so we only have to get 2 other players and we have 11 students left. Also : $11\choose4$ for Math team so total ways would be  $11\choose2$ . $11\choose4$ (Product Rule) Case 2: James is in Math team: $\therefore $  $11\choose3$ for Math team This would be because james is in team Math assumed so we only have to get 3 other players and we have 11 students left. Also : $11\choose3$ for Euclid team , as the 11 students can form a team of 3 for Euclid competition here. so total ways would be  $11\choose3$ . $11\choose3$ (Product Rule) Case 3: James in no team $\therefore $  $11\choose3$ . $11\choose4$ (Product Rule) as James is in no team so we are left with 11 people for both teams Now the question is did I do it correctly? Because I have never seen cases being used in a permutation questions? Therfore, The total number of ways would be to add all the cases meaning Case 1 + Case 2 + Case 3 : ($11\choose2$ . $11\choose4$) + ($11\choose3$ . $11\choose3$) +($11\choose3$ . $11\choose4$) Is that correct?","['permutations', 'combinatorics', 'combinations', 'discrete-mathematics']"
2697558,Understanding Generalised Quadrangles,"I have a project to do on Generalised Quadrangles, specifically GQ(2,2). The project needs to have information about the construction of GQ(2,2), to prove this construction meets the conditions of a generalised quadrangle and showing that $S_{6}$ acts ""flag transitively"" on this object. I'm aware now of the definition of $GQ(2,2)$, and I have some idea on how to construct it. Essentially, it seems you take $6$ points, and then take all possible triplets of pairs that can be formed from this (and there are $\binom{6}{2} = 15$ of these unordered pairs) and then you just put lines between triplets who share points. However, I'm struggling to prove that this matches the definition of a generalized quadrangle. Furthermore, I don't really understand what flag transitivity means, or how to show that $S_{6}$ acts in this way.","['group-actions', 'finite-geometry', 'algebraic-graph-theory', 'group-theory']"
2697651,Polynomials- Prove that function can be only polynomial,"Suppose there exist a function $f : \mathbb R \to \mathbb R$ such that $f(x)\cdot f'(x)$ is polynomial It. is trivial that if $f(x)$ is polynomial, then $f(x)\cdot f'(x)$ is polynomial. My question is: how would one prove that $f(x)$ can be only polynomial?","['real-analysis', 'polynomials']"
2697664,Jensen inequality on square root,"For a concave function $f$, Jensen inequality states that $$
f(ax+by) \geq af(x) + bf(y)
$$ Now, $\sqrt{x}$ is a concave function, hence by Jensen inequality $$
\sqrt{x+y} \geq \sqrt{x} + \sqrt{y}
$$ But by elementary algebra, we know $$
(\sqrt{x} + \sqrt{y})^2 = x + y + 2\sqrt{xy}
$$
and hence
$$
\sqrt{x} + \sqrt{y} \geq \sqrt{x + y}
$$ Why I get contradictory results ?","['algebra-precalculus', 'jensen-inequality']"
2697686,"Limit of two-variable function: ${\lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|}}$","I am stuck with this limit, which according to Wolfram Alpha does not exist. $$
\begin{align}
\lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|} = \lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\sqrt{x^2+y^2}} = \lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r}
\end{align}
$$ From there, I thought about using the first-degree Taylor expansion in order to get rid of the exponential (which I'm not even sure I can do). $$
\begin{split}
\lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r} &= \lim_{r \to 0\; \forall\theta} \frac{1+r\cos{\theta}(r\sin{\theta}+1) -r\cos{\theta} -1}{r} \\
&=\lim_{r \to 0\; \forall\theta} \frac{1+ r^2\cos{\theta}\sin{\theta}+r\cos{\theta} -r\cos{\theta} -1}{r}\\
& = \lim_{r \to 0\; \forall\theta} \frac{r^2\cos{\theta}\sin{\theta}}{r} \\
&= \lim_{r \to 0\; \forall\theta} {r\cos{\theta}\sin{\theta}}  = 0
\end{split}
$$ This is apparently wrong, but I cannot think of any other way to solve this problem, and through this method the result is clearly 0. Since I am pretty sure using Taylor's theorem here is not allowed, how else could you go about solving the limit?","['multivariable-calculus', 'limits-without-lhopital', 'limits']"
2697687,How to understand multivalued functions in terms of Riemann surfaces,"This note says in the last sentence, ""The discontinuities of multivalued functions in the complex plane are commonly handled through the adoption of branch cuts, but use of Riemann surfaces is another possibility."" I have never heard of this before.  Can someone explain what Riemann surfaces have to do with multivalued functions?  For example, can one explain the meaning and holomorphicity of a function like $f(z) = \sqrt[m]{z}$ in terms of Riemann surfaces instead of branch cuts?","['riemann-surfaces', 'complex-analysis']"
2697726,Parametrizing $(2x+y)^2(x+y)=x$,I need to find $x=x(t)$ and $y=y(t)$ so that the implicitly defined curve on $\mathbb R^2$ $$(2x+y)^2(x+y)=x$$ is converted into an explicit function of the parameter $t$ that can be analysed using single variable calculus. The usual parametrization $x(t)=t$ and $y(t)=xt$ yielded a very complicated form that was hard to work with. Could you help me with finding a better parametrization? Polar coordinates also didn't help much.,"['plane-curves', 'curves', 'algebra-precalculus', 'implicit-function', 'parametrization']"
2697735,Definition of topology induced by seminorms,"Let $V$ be a normed vector space and let $(p_{\alpha})_{\alpha \in A}$ be a set of seminorms on $V$ such that for all $x \in V$ such that $p_{\alpha}(x)=0$ for all $\alpha$ we have $x = 0$. I know two different definitions of the topology induced by these seminorms: Definition 1: The topology is the initial topology w.r.t. the maps $f_{(\alpha,v)}:V \to \mathbb{R}: x \mapsto p_{\alpha}(v-x)$ for $(\alpha,v) \in A \times V$. Definition 2: The subbase of the topology consists of $\{x \in V\;|\;p_{\alpha}(v-x) <\epsilon\}$ for all $\alpha \in A$ and $v \in V$ and $\epsilon > 0$. It is clear that the topology in definition 1 is finer than the one given in definition 2. Is it true that these topologies are equivalent? Is every set $f_{(\alpha,v)}^{-1}(U)$ (where $U \subseteq \mathbb{R}$ is open) a union of open sets of topology in the second definition?","['general-topology', 'topological-vector-spaces']"
2697737,Derivative of a limit,"Suppose that we are given a sequence of continuous functions $f_n$ which are non-negative, integrate to $1$ and supported on intervals $[-x_n,x_n]$ with $\sum_{n \geq 1} x_n$ converging where $x_n \geq 0$ for every $n$. If we set $F_n;=f_1 *\cdots *f_n$, where $*$ denotes convolution, is it true that $\lim_{n \to \infty} F_n$ is differentiable?","['functional-analysis', 'real-analysis', 'analysis']"
2697778,Reference for group cohomology,"I would like to know more about group cohomology. I know that there are chapters about group cohomology in some group theory textbooks, for example in Rotman's. However my PhD asvisor told me that he did not really like the way in which it was presented, but could not give my any other reference. I was also wondering if there is some text completely devoted to an introduction to group cohomology...","['homology-cohomology', 'reference-request', 'abstract-algebra', 'group-cohomology', 'group-theory']"
2697831,If $u_n = (1+\frac{1}{n^2})\ldots(1+\frac{n}{n^2})$ then $\lim u_n=\sqrt{e}$? [duplicate],"This question already has answers here : How to calculate $\lim_{n\to\infty}(1+1/n^2)(1+2/n^2)\cdots(1+n/n^2)$? (4 answers) Closed 4 years ago . I'm solving an exercise and I'm asked to prove that if $$u_n = \left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\left(1+\frac{3}{n^2}\right)\ldots\left(1+\frac{n}{n^2}\right)$$ then $\lim u_n = \sqrt{e}$. This is after it asks me to show that $\forall x>0,$ $$x-\frac{1}{2}x^2<\ln(1+x)<x$$ which I did, but even with this result I'm not being able to even get how to use it to show $\lim u_n = \sqrt{e}$.",['limits']
2697929,Proof that $\lim_{x\to \infty} (1+(x/n))^n = e^x$ Using Monotone Convergence Theorem,"Use monotone convergence theorem to prove that $e(x):=\lim_{n\to\infty}(1+x/n)^n$ exists for all real $x$.  Then show that $e(-x):=\lim_{n\to\infty}(1-x/n)^n=1/e(x)$. I'm struggling to use monotone convergence theorem with this; obviously $e(x)$ is monotone when $x>0$, but it's not when $-n<x<0$.  So now what? Further, what makes the second part so difficult is that we can't use any derivatives, logarithms, prior knowledge of $e^x$, etc.  We have to do this using just the Bernoulli inequality and finite geometric series, or the regular definition of convergence.  I'm just not making any progress and I don't understand how I'm supposed to do this. How can you show both of these things?","['real-analysis', 'limits', 'functions', 'calculus', 'limits-without-lhopital']"
2697936,"Does there exist a set containing infinite elements, whose elements themselves are sets containing infinite elements?","Does there exist a set containing infinite elements, whose elements themselves are sets containing infinite elements? I think the answer is no, there is a famous paradox for it but I'm forgetting.",['elementary-set-theory']
2697962,Adjoint of the attached space in operator algebras,"J. Conway considers in his book ""A course in operator theory"" a linear subspace $\mathcal S \subset B(\mathcal H)$ of bounded operators acting on a Hilbert space. He defines the attached space as
$$ \mathrm{Ref} \ \mathcal S = \{ T \in B(\mathcal H) \ | \ \forall h \in \mathcal H \ : \ Th \in \mathrm{cl} (\mathcal S h)  \}, $$
where $\mathrm{cl}$ denotes closure of a set and $\mathcal S h$ is the set of all vectors of the form $Ah$ with $A \in \mathcal S$. He leaves as a simple exercise to prove that $\mathrm{Ref} \ (\mathcal S^*)=(\mathrm{Ref} \ \mathcal S)^*$ - that is space attached to the set of adjoint operators consists precisely of adjoints of elements in the attached space. I have no idea how to proceed here. It seems that it is necessary to somehow relate ranges of an operator and its adjoint, but such relation certainly doesn't exist - instead range is connected with kernel. Explanation of this equality or hints with be appreciated","['functional-analysis', 'operator-algebras', 'operator-theory', 'hilbert-spaces']"
2697974,Total number of roots of a polynomial of degree $n$ over a ring $R$,"Let $R$ be a commutative ring with unity and $f$ be a polynomial of degree $n$ over $R$. Under what conditions on $R$, does $f$ has at most $n$ roots ? I am asking because in $\mathbb{Z}/12\mathbb{Z}$, the polynomial $x^2-4$ has 4 roots $2,4,8,10$ since $X^2-4=(x-8)(x-4)=(x-10)(x-2)$. The above example shows unique factorization is necessary. So I believe, $R$ should be a UFD as $R[x]$ will also be a UFD then.","['abstract-algebra', 'roots', 'polynomials']"
2697987,Showing that g is integrable and $\int^b_a{f}$ = $\int^b_a{g}$,"Let $f$ be integrable on $[a,b]$, and suppose g is a function on $[a,b]$ such that $g$($x$) = $f$($x$) except for finitely many $x$ in $[a,b]$. Show $g$ is
integrable and $\int^b_a{f}$ = $\int^b_a{g}$.
I started out solving this problem by letting a function $h = f - g$ such that $h(x) = 0$ expect at one point in [a,b]. Since $f$ is integrable, then there exists a partition P such that $U(f,P)-L(f,P)$ < $\epsilon$. I'm not really sure how to proceed from here. Any hints and help is much appreciated.","['real-analysis', 'partitions-for-integration']"
2697997,Limit of $S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} $ [duplicate],"This question already has answers here : Evaluate $\lim\limits_{n\to\infty}\frac{\sum_{k=1}^n k^m}{n^{m+1}}$ (5 answers) Closed 5 years ago . Let $S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} $ . For what values of $k$ the series $S_n$ is convergent and what is the value of convergence ? I'm really unable to understand $S_n$ because I haven't seen any series which is similar to that , so applying tests isn't possible for me . Also I've searched over the internet but didn't find any useful result .","['sequences-and-series', 'limits']"
2698108,"Suppose $I\oplus K$ is a free module, then ""$KI\subseteq K\cap I$""","I have been staring at this proof in the Proceedings of the AMS, and I don't follow the author's logic. Here's the setup: $I$ is an ideal in a ring $R$, and it is projective as a right $R$ module. Therefore it is a summand in a free right $R$ module, $F=I\oplus K$. Now, the line of reasoning continues (verbatim except for symbol changes): Suppose $K\neq 0$. Since $F$ is isomorphic to a direct sum of copies of $R$, it has canonical multiplication. Let $\operatorname{Ann}_F(I)$ be the annihilator of $I$ in $F$. Then $KI\subseteq K\cap I=0$, so $\operatorname{Ann}_F(I)\neq 0$. Now if $K$ was a right ideal of $R$, then $KI\subseteq K\cap I$ would be easy to understand. The only set $K$ and $I$ are comparable in $F$, so $(I\oplus 0)\cap( 0\oplus K)=0\oplus 0$ in the direct sum.  But the left side is apparently multiplying $K$ through the $F$ module action, so that we're actually talking about $(0\oplus K)I$. Why would one say that's a subset of $I\oplus 0$? Sure, every element of $(0\oplus K)I$, when expressed as a tuple in $F$ has entries in $I$, but as far as I can see, that doesn't mean anything about its membership in $I\oplus 0$. I should also say that the ring $R$ is right self-injective and the ideal $I$ has zero left annihilator in $R$, but I'm not sure that it makes a difference. The author appeals to neither property in the line of thought above. In fact, that $I$ has zero left annihilator immediately allows you to say that $Ann_F(I)=0$, but since the whole point of this is to derive a contradiction, I need to see if there's any merit in what the author has claimed. I haven't managed to cook up a counterexample yet, mostly because I have a hard time realizing projective ideals as summands in free modules. Am I missing some observation or is my doubt justified? I have a sneaking suspicion that a cognitive error occurred on the author's part.","['abstract-algebra', 'ring-theory', 'modules']"
2698133,Given $RHR^{-1}=D$ where $H$ is Hermitian and $D$ is Diagonal. Show that $R$ is Unitary.,"Given $RHR^{-1}=D$ where $H$ is Hermitian and $D$ is Diagonal. Show that $R$ is Unitary. where Unitary Matrix: $U^\dagger U=UU^\dagger =I$ and Hermitian Matrix: $H=H^\dagger$ My try to this question $RHR^{-1}=D \iff RH=DR$ and then applying dagger on both sides I get $HR^\dagger=R^\dagger D^\dagger$ I know $D$ consists of eigenvalues of $H$ and since $H$ is Hermitian implies eigen values are real. Thus, I get $HR^\dagger=R^\dagger D$. And the above equation implies $R^\dagger R H=H R^\dagger R $","['matrices', 'diagonalization', 'eigenvalues-eigenvectors']"
2698134,Idempotence of iterating a function,"In the context of fixed points and fixed point combinators (for lambda calculus) I have repeatedly encountered the handwavingly motivated claim that applying a function to an infinite iteration of itself will not make a difference to the result of the iteration. Or formally: $$\lim_{n \to \infty} f^n(x) = f(\lim_{n \to \infty} f^n(x))$$ for some $x$ in the domain of $f$. I mean, I don't think $\forall n \in \mathbb{N}. n + \infty = \infty$ is a convincing argument nor a valid looking proof here. The context in which this theorem appeared was that if $\textit{iterate}(f, x)$ is a function that iterates $f$ on $x$ then $\textit{iterate}(f,x)$ is a fixed point of $~f$, as $$\text{iterate}(f,x) = \lim_{n \to \infty} f^n(x) \overset{\text{by above theorem}}{=} f(\lim_{n \to \infty} 
 f^n(x)) = f(\text{iterate}(f,x))$$ $$\iff f(\text{iterate}(f,x)) = \text{iterate}(f,x)$$ But what is a valid proof for said theorem in the first place and does it have a name?","['fixed-points', 'fixed-point-theorems', 'lambda-calculus', 'functions']"
2698210,Intuition on variance of linear combination of variables,"I have the formulas for the variance of linear combintion of variables given as $$
\text{Var}\Bigl(\,\sum_{i=1}^n X_i\,\Bigr)=  \sum_{i=1}^n\text{Var}( X_i)+
 2\sum_{i< j} \text{Cov}(X_i,X_j).
$$
I know this is derived from variance of 2 variables and extrapolation. However, what is the intuition behind this formula? Can someone explain further on how I could  use this formula to understand covariance better?",['statistics']
2698217,Asymptotic distribution of non-linear least squares,"Assume an i.i.d sample of a scalar dependent variable $y_i$ and a $k$-dimensional regressor $x_i$. Assume that $\mathbb{E}[y_i \mid x_i] = \exp\left(x_i'\beta_0\right)$ and $Var(y_i \mid x_i) = \exp\left(x_i'\gamma_0\right)$. Also assume that the non-linear least squares estimator $\widehat{\beta}$ of $\beta_0$ is $\sqrt{N}$-consistent and asymptotically normal, i.e., $$\sqrt{N}\left(\widehat{\beta} - \beta_0\right) \overset{d}{\rightarrow} N(0, H_0^{-1}V_0 H_0^{-1}) $$
Find an expression for $H_0$ and $V_0$. I know the general formulas for $H_0$ and $V_0$ for non-linear least squares, that is, for the non-linear least squares estimator $$\widehat{\theta}_N = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^{N} (y_i - g(x_i, \theta))^2 $$ where $g(x_i, \theta) = \mathbb{E}\left[y_i \mid x_i = x \right]$, we have:
$$V_0 = 4\mathbb{E}\left[(y_i - g(x_i, \theta_0))^2 \frac{\partial g(x_i, \theta_0)}{\partial \theta} \frac{\partial g(x_i, \theta_0)}{\partial \theta'} \right] $$ and $$H_0 = 2\mathbb{E}\left[ \frac{\partial g(x_i, \theta_0)}{\partial \theta} \frac{\partial g(x_i, \theta_0)}{\partial \theta'}\right] $$ where $\theta_0$ denotes the true parameter. I'm unsure of how to compute the expressions using these results.","['regression', 'normal-distribution', 'asymptotics', 'probability-distributions', 'statistics']"
2698240,Curves and Surfaces,"Suppose $$S=\{(x,y,z) \in \mathbb{R}^3 : x^2+y^2+(z-1)^2=1\}$$ and                $$T=\left\{(x,y,z) \in \mathbb{R}^3 : \frac{(z+1)^2}{4}=x^2+y^2, z \geq-1 \right\}.$$ (1) Find the $z$-coordinates of the points of intersection of $S$ and $T$ and sketch the projection into the $xy$-plane of the curves of intersection. I tried to solve simultaneously and found that $z=1/5, 1$. Not sure if this is correct. Also am unsure about the sketch. Thanks!","['multivariable-calculus', 'calculus']"
2698260,Adeles under base change,"Let $ K \subset L $ be a  finite  separable extension of  global fields with set of places $ M_{K} $, $ M_{L} $ and adele rings $ \mathbb{ A}_{K}$, $ \mathbb{A}_{L} $ respectively. I have trouble understanding the proof of Proposition 22.10 in these notes . The first line says that $$ \mathbb{A}_{K} \otimes L    $$ is equal to the restricted direct product of $ K_{v} \otimes_{K} L  $ with respect to $ \mathcal{O}_{v} \otimes _ { \mathcal{O}_{K}}   \mathcal{O } _ { L }  $. First, why does $ a_{v} \otimes x $ lies in $  \mathcal{O}_{v} \otimes_{\mathcal{O}_{K}}  \mathcal{O}_{L} $ for all but finitely many places of $ v \in M_{K} $? Second, how can I rigorously convince myself this is a ring isomorphism (kernel is zero) and a homeomorphism of topological spaces?","['number-theory', 'class-field-theory', 'local-field']"
2698305,"Does there exist some probability space $(\Omega,\mathcal F,\mathbb P)$ that admits random variables with all possible laws on $\mathbb R^n$?","I have a question about a statement which intuitively seems like it should be a canonical fact, but which I cannot find in any common textbook on probability. Namely, does there exist a probability space $(\Omega,\mathcal F,\mathbb P)$ such that for any probability measure $\mu$ on $\mathbb R^d$, there exists a random variable $X:\Omega\to\mathbb R^d$ such that $X$ has law $\mu$, i.e. $X_\#\mathbb P=\mu$? Can we do this on a single probability space as $d$ ranges over the natural numbers? I suspect that the space $$([0,1]^\omega,\mathcal B_{[0,1]}^{\otimes\omega},\mathcal L^\omega)$$ should do the trick, where $\mathcal L$ is the Lebesgue measure on $[0,1]$, which I see can simply reduce to proving that $$([0,1]^d,\mathcal B_{[0,1]}^{\otimes d},\mathcal L^d)$$ works for $\mathbb R^d$, but the details of this last part are a bit beyond me.","['probability-theory', 'probability']"
2698335,Proof for norm of sum of vectors is less/equal to sum of norm of vectors,"Prove $||v+w|| \le ||v|| + ||w||$ My proof so far is:
$$ ||v+w|| = \sqrt{\langle v+w,v+w\rangle}$$
$$= \sqrt{\langle v,v\rangle +2\langle v,w\rangle +\langle w,w\rangle }$$
So, $||v+w||^2$
$$=||v||^2 +2\langle v,w\rangle + ||w||^2$$ When I squared and expanded the right side, I got
$$(||v||+||w||)^2=||v||^2 +2||v||\ ||w||+||w||^2$$
Which means I need to prove that $\langle v,w \rangle \le ||v||\ ||w||$. I'm not sure how to prove this. Is there a different answer to the question that sidesteps this? How can I go about proving that statement?","['linear-algebra', 'proof-verification']"
2698344,If the analytic function $f(z)$ has a zero of order $N$ at $z_0$ then $f(z)=g(z)^N$,show that if the analytic function $f(z)$ has  a zero of order $N$ at $z_0$ then $f(z)=g(z)^N$ for some function $g(z)$ analytic near $z_0$ and satisfies $g'(z_0) \neq 0$ what i tried here $f(z)$ has  a zero of order $N$ at $z_0$ so we can write $f(z)=(z-z_0)^Nh(z)$ where $h(z_0)\neq 0$ but how to prove our conclusion,['complex-analysis']
2698349,My Proof: Every convergent sequence is a Cauchy sequence.,"Let $(x_n)_{n\in\Bbb N}$ be a real sequence. $\textbf{Definition 1.}$ $(x_n)$ is $\textit{convergent}$ iff
there is an $x\in\Bbb R$ such that,
for every $\varepsilon\in\Bbb R$ with $\varepsilon>0$ ,
there is an $N\in\Bbb N$ such that,
for every $n\in\Bbb N$ with $n>N$ ,
we have $|x_n-x|<\varepsilon$ . $\textbf{Definition 2.}$ $(x_n)$ is a $\textit{Cauchy sequence}$ iff,
for every $\varepsilon \in\Bbb R$ with $\varepsilon > 0$ ,
there is an $N\in\Bbb N$ such that,
for every $m,n\in\Bbb N$ with $m,n > N$ ,
we have $|x_m - x_n| < \varepsilon$ . $\textbf{Theorem.}$ If $(x_n)$ is convergent,
then it is a Cauchy sequence. Proof : Since $(x_n)\to x$ we have the following for for some $\varepsilon_1, \varepsilon_2 > 0$ there exists $N_1, N_2 \in \Bbb N$ such for all $n_1>N_1$ and $n_2>N_2$ following holds $$|x_{n_1}-x|<\varepsilon_1\\ |x_{n_2}-x|<\varepsilon_2$$ So both will hold for all $n_1, n_2 >\max(N_1, N_2)=N$ , say $\varepsilon = \max(\varepsilon_1, \varepsilon_2)$ then $$|x_{n_1}-x-(x_{n_2}-x)|<\varepsilon\\\implies |x_{n_1}-x_{n_2}|<\varepsilon$$ Hence all convergent sequences are Cauchy. Is this proof correct? I also saw this question and copied some of the content(definition and theorem) from there.https://math.stackexchange.com/q/1105255","['real-analysis', 'cauchy-sequences', 'solution-verification']"
2698379,How to prove this specific function has limit either $\infty$ or 0?,"If $f:\mathbb R^+ \to \mathbb R^+$, and for all $y>0$, the following equation holds (i.e. it is slowly varying fuction.)
$$\lim_{x\to +\infty} \frac{f(xy)}{f(x)}=1$$ How to prove the following?
$$\lim_{x\to +\infty} x^\rho f(x) = \begin{cases}0 & \rho<0\\\infty & \rho>0\end{cases}$$ Notice that $f$ is not necessarily continuous. This problem seems simple but I can't solve it.","['real-analysis', 'calculus', 'limits']"
2698470,Solve $f(n)=\begin{cases} n-3 \ \ \ \ \ \ \ \qquad{\text{if $n\ge1000$}}\\ f\big(f(n+5)\big)\quad{\text{if $n<1000$}} \end{cases}$,"Suppose I have a function $f(n)$ below, find $f(84)$ :
$$f(n)=\begin{cases}
n-3 \ \ \ \ \ \ \ \qquad{\text{if $n\ge1000$}}\\
f\big(f(n+5)\big)\quad{\text{if $n<1000$}}
\end{cases}$$
My attempt:
$$\begin{align}
f(84)=&f\big(f(89)\big)\\
f(89)=&f\big(f(94)\big)\\
f(94)=&f\big(f(99)\big)\\
&\vdots\\
f(999)=&f\big(f(1004)\big)\\
f(1004)=&1001
\end{align}
$$
Now:
$$\begin{align}
f(999)&=f(1001)=998\\
f(994)&=f\big(f(999)\big)=f(998)\Rightarrow
f(998)=f\big(f(1003)\big)=f(1000)=997\\
&f(994)=f(998)=997\\
f(989)&=f\big(f(994)\big)=f(997)\Rightarrow f(997)=f\big(f(1002)\big)=f(999)=998\\
&f(989)=f(997)=998\\
f(984)&=f\big(f(989)\big)=f(998)=997\\
f(979)&=f\big(f(984)\big)=f(997)=998\\
\vdots
&&
\end{align}$$
Obviously since it the value of $f(n)$ alternates between $997$ & $998$, depending on whether $n$ is odd or even, thus:
$$\vdots\\
f(89)=998\\
\therefore\bbox[10px, border:1px solid black]{f(84)=997}\\
$$ My question: is there a more efficient way to solve this?","['algebra-precalculus', 'functions']"
2698475,Probability that the sum of 'n' positive numbers less than 2 is less than 2,"I'm a high school student,
And I stumbled across this problem
Q) if you arbitrarily choose 3 real positive numbers less than or equal to 2
What is the probability that their sum is less than or equal to 2? In my course I have learnt to solve 2 number based probability problems by using the ratio of the areas plotted by their graphs.
This was an application question that required me to plot it in 3 dimensions and find the ratio of their volumes to get the required probability, and so I managed to pull it off and got the required answer. My question then arose... How do I deal with N dimensions? As I only have the knowledge to visualize 3 dimensions at my level of Math,
Can anyone help me out? I am really curious to see
1) the actual function and how it's growth is
2) the way you solve such kind of problems! (some kind of multivariable calculus??) Thanks!",['probability']
2698481,About spaces $\Bbb R^n/\!\sim$ in which certain coordinate permutations do not matter.,"Let $\sim$ be an equivalence relation on the permutations of $(1,...,n)$. I thought about the topological spaces $\Bbb R^n/\!\sim$, i.e. $\Bbb R^n$ with standard topology after associating certain permutations $\sigma$ of the indices. More precisely: $$(x_1,...,x_n)\sim_{\Bbb R^n}(x_{\sigma(1)},...,x_{\sigma(n)})\quad\Longleftrightarrow\quad (1,...,n)\sim(\sigma(1),...,\sigma(n)).$$ Example : $\Bbb R^n/S_n:=\Bbb R^n/\!\sim$ with $\sim$ associating all permutations, is the space in which the order of the coordinates does not matter. I made the following observations: $\Bbb R^n/S_n\cong \Bbb R^n_+=\Bbb R^{n-1}\times\{x\in\Bbb R\mid x\ge 0\}$, the $n$-dimensional half-space. This can be seen by associating $\Bbb R^n/S_n$ with the subset $\{(x_1,...,x_n)\mid x_1\le\cdots\le x_n\}\subset\Bbb R^n$ with increasing coordinates. $\Bbb C^n/S_n\cong \Bbb C^n$ (a homeomorphism is given by Vièta's map which associates unordered roots with ordered coefficients of polynomials). This can be expressed as $\Bbb R^{2n}/\!\sim\;\cong \Bbb R^{2n}$ where $\sim$ only associates permutations in which certain consecutive pairs of indices permute in lockstep, e.g. $$(\color{red}{1,2},\color{blue}{3,4})\sim(\color{blue}{3,4},\color{red}{1,2})\qquad\text{but not}\qquad (1,2,\color{red}3,\color{blue}4)\sim(1,2,\color{blue}3,\color{red}4).$$ Question : I was wondering what different spaces can emerge from $\Bbb R^n/\!\sim$. I would conjecture that it only can be $\Bbb R^n$ and $\Bbb R^n_+$. In this case I was wondering whether there is a simple characterization of the relations $\sim$ which generate either one.","['quotient-spaces', 'equivalence-relations', 'permutations', 'general-topology', 'symmetric-groups']"
2698503,Action of universal covering deck transformations group,"Let $X$ be a topological space """"good enough"""" , let $\tilde{X}$ be its universal covering (suppose that $X$ has one), then for every connected covering $P$ on $X$ with fiber $F$, it is true that the group of deck transformation $Aut(\tilde{X})$ acts on $F$? I want to solve this problem using the properties of $\tilde{X}$ to be connected and a normal covering and the fact that it covers all the connected coverings on $X$. So I don't want to mention explicitly the monodromy action of the fundamental group. How can I do that?","['algebraic-topology', 'general-topology']"
2698528,Infinity norm quotient,"Let $V = \mathbb{R}^n$ the canonical $n$ dimensional real vector space.
We endow $V$ with the infinity norm defined as $\|x\|_\infty = \max_i |x_i|$ for any vector $x = (x_1, \dots, x_n) \in V$. Let then $V'$ be a strict subspace of $V$. One can consider the quotient 
$$V/V'$$ which is naturally normed by the quotient norm:
$$ \| x + V' \|_\infty = \inf_{v\in V'} \|x-v\|_\infty.$$ Is there a convenient way to compute generically this norm or at least get (non-trivial) bounds on it ? In the $\ell_2$ norm case, this reduces easily to the computation of the
orthogonal projection of $x$ on the space $V'$, which can be performed by
Gram-Schmidt orthogonalisation.","['normed-spaces', 'banach-spaces', 'linear-algebra', 'quotient-spaces']"
2698554,What is the relationship between blue area and red area?,"This is a grade school problem! Consider the following figure: It is very easy to show that the red area and the blue area equal. I can demonstrate this based on my knowledge related to the computation of the surface areas of circular sectors and triangles. Both areas equal $2\left(\frac{r^2\pi}4-\frac{r^2}2\right)$ where $r$ is the radius of the smaller circles. But, how am I going to show the same if I forget, for good, the formula providing the area of a triangle? I am not able to get rid of my thought process using triangles.","['recreational-mathematics', 'euclidean-geometry', 'area', 'geometry']"
2698555,Find all the rational values of $x$ at which $y=\sqrt{x^2+x+3}$ is a rational number,"Question Find all the rational values of $x$ at which $y=\sqrt{x^2+x+3}$ My attempt Since we only have to find the rational values of $x$ and $y$, we can assume that 
$$ x \in Q$$
$$ y \in Q$$
$$ y-x \in Q $$
Let$$ d = y-x$$
$$d=\sqrt{x^2+x+3}-x$$
$$d+x=\sqrt{x^2+x+3}$$
$$(d+x)^2=(\sqrt{x^2+x+3})^2$$
$$d^2 + x^2 + 2dx =x^2+x+3$$
$$d^2 +2dx = x +3$$
$$x = \frac{3-d^2}{2d-1}$$ $$d \neq \frac{1}{2}$$ So $x$ will be rational as long as $d \neq \frac{1}{2}$. Now
$$ y = \sqrt{x^2+x+3}$$
$$ y = \sqrt{(\frac{3-d^2}{2d-1})^2 + \frac{3-d^2}{2d-1} + 3}$$
$$ y = \sqrt{\frac{(3-d^2)^2}{(2d-1)^2} + \frac{(3-d^2)(2d-1)}{(2d-1)^2} + 3\frac{(2d-1)^2}{(2d-1)^2}}$$
$$ y = \sqrt{\frac{(3-d^2)^2 + (3-d^2)(2d-1) + 3(2d-1)^2}{(2d-1)^2}} $$
$$ y = \frac{\sqrt{(3-d^2)^2 + (3-d^2)(2d-1) + 3(2d-1)^2}}{(2d-1)}$$
$$ y = \frac{\sqrt{d^4-2d^3+7d^2-6d+9}}{(2d-1)}$$ I know that again $d \neq \frac{1}{2}$ but I don't know what to do with the numerator. Help",['algebra-precalculus']
2698594,Primitive cohomology (problems with intuition),"I am using primitivie cohomology more or less as a black box right now. The very little intuition I have comes from this answer on MathOverflow and appendix C in 3264 and all that , by Eisenbud and Harris. If $X$ is an $n$-dimensional (complex dimension) smooth projective variety and $h\in H^{2}(X)$ is the [complex de Rahm] cohomology [= singular cohomology tensor $\mathbb{C}$, I hope] class corresponding to a hyperplane section, then the cup product induces isomorphisms
$$ (-) \cup h^{k} \colon  H^{n-k}(X) \to H^{n+k}(X) $$ We can take Poincaré duals and visualize this as intersecting with an hyperplane. It is (slightly) intuitive to have an isomorphism because the dimensions are complementary and we are in projective space. For $k\leqslant n$ we define the $n-k$ primitive cohomology group $H^{n-k}_{pr}(X)\subseteq H^{n-k}(X)$ to be the kernel of the map induced by cupping one more time with $h$:
$$ (-) \cup h^{k+1} \colon  H^{n-k}(X) \to H^{n+k+2}(X) $$ Again, for dimensional reasons, it is intuitive to expect the possiblity of a non-trivial kernel now. My question : it is often used in the notes I am following that the middle primitive Betti and Hodge numbers are the usual Betti and Hodge numbers minus 1. I think this should also be slightly intuitive by dimensional arguments in projective space as above, but I couldn't make this precise and I think part of the problem is that complex and real dimensions are getting mixed. Can anyone make these arguments more precise and explain intuitively why we expect the middle primitive Betti numbers to be one less that the usual middle Betti numbers? I suspect it is also related to the Lefschetz decomposition, but I also don't have an intuitive understanding of this fact. Any hints are appreciated, thanks.","['homology-cohomology', 'projective-space', 'projective-geometry', 'algebraic-geometry']"
2698605,Does the existence of a merely finitely additive probability on a Boolean algebra require axioms beyond ZF?,"As happens every so often, I find myself in a set-theoretical morass. Please help me out of it. Let $\Omega$ be a set and $\mathcal{B}$ a Boolean algebra of subsets of $\Omega$. The question, broadly, is how to reconcile the following three observations. (1) From this post , it seems that the existence of a merely finitely additive probability (i.e., a probability that is finitely but not countably additive) on $(\Omega, \mathcal{B})$ cannot be shown in ZF. The proof of this claim proceeds as follows. It is known that the existence of a merely finitely additive probability $\mu$ on $(\mathbb{N}, 2^\mathbb{N})$, such that $\mu(n)=0$ for all $n$, cannot be shown in ZF. If there exists a merely finitely additive probability $P$ on $(\Omega, \mathcal{B})$, then  a simple construction shows that $P$ can be used to define a merely finitely additive probability $\mu$ on $(\mathbb{N}, 2^\mathbb{N})$ such that $\mu(n)=0$ (see the linked post for details). Since the existence of $\mu$ cannot be shown in ZF, neither can the existence of $P$. (2) Now, a friend of mine denies this conclusion. He thinks that merely finitely additive probabilities on $(\Omega, \mathcal{B})$ can be shown to exist using only the ZF axioms. His argument is essentially the same as the one that appears in this post . He assumes that $\mathcal{B}$ is well-ordered and extends the filter of cofinite subsets of $\Omega$ to a non-principal ultrafilter using transfinite induction on the well-order of $\mathcal{B}$. This yields a non-principal ultrafilter which, restricted to $\mathcal{B}$, defines a merely finitely additive probability (details can be provided if need be). So long as defining the well order on $\mathcal{B}$ doesn't require any choice (for example, let $\mathcal{B}$ be countable), the argument does seem to demonstrate that choice is not needed to show the existence of a merely finitely additive $P$. (3) But now, to complicate things further, there is Theorem 2 of this paper by Pincus and Solovay, which states: It is consistent with ZF, DC, and the Hahn-Banach theorem that every ultrafilter on any set is principal. This seems to contradict my friend's claim in (2), adding further evidence to the claim made in (1), that the existence of a merely finitely additive $P$ on $(\Omega, \mathcal{B})$ cannot be shown in ZF. To sum up, I am left wondering Does there exist a set $\Omega$ and Boolean algebra $\mathcal{B}$ of subsets of $\Omega$ such that the existence of a merely finitely additive probability $P$ on $(\Omega, \mathcal{B})$ can be shown using only the ZF axioms? and Is there a set $\Omega$ such that the existence of a non-principal ultrafilter of subsets of $\Omega$ can be shown using only the ZF axioms? Added. Here is the proof of (2), which seems to contradict the result cited in (3). Perhaps someone can indicate where it goes wrong. We assume that $\mathcal{B}$ can be well-ordered without using the axiom of choice. For example, assume that $\mathcal{B}$ is countable. Let $\mathcal{F}_0$ be the filter of cofinite subsets of $\Omega$. We extend $F_0$ to a non-principal, proper ultrafilter of subsets of $\Omega$ by transfinite induction on the well order of $\mathcal{B}$. Any proper filter $\mathcal{F}$ that extends $\mathcal{F}_0$ is clearly non-principal. Let $B \in \mathcal{B}$. The members of the filter generated by $\mathcal{F}$ and $B$ are supersets of sets that are intersections of members of $\mathcal{F}$ and $B$. If $B \cap F_1 = \emptyset$ where $F_1 \in \mathcal{F}$ and $(\Omega - B) \cap F_2 = \emptyset$ where $F_2 \in \mathcal{F}$, then $F_1 \cap F_2 = \emptyset$, contradicting the assumption that $\mathcal{F}$ is a proper filter. This shows that at each stage in the induction, we can add a member $B \in \mathcal{B}$ or its complement. At limit ordinals, we consider the filter that is the union of the chain of filters and continue until we end up with a non-principal, proper ultrafilter.","['probability-theory', 'set-theory', 'measure-theory', 'axiom-of-choice']"
2698609,Calculate $f(x)$ at a specific point,"Question:
Calculate $f(x) = \frac{49}{x^2} + x^2$ at points for which $\frac{7}{x}+x =3$ My attempt:-
I tried to find the value of $x$ and insert in $f(x)$
$$\frac{7}{x}+x =3$$
$$7+x^2 =3x$$
$$x^2 -3x + 7=0$$ $$x = \frac{3+\sqrt{9-7*4}}{2}$$ Now $x$ is coming out to be irrational and things get a lot more difficult from there. What should I do?",['algebra-precalculus']
2698653,On the Lambert series for the Möbius function,"The Lambert series for the Möbius function is given by :
$$\sum_{n=1}^{\infty}\mu(n)\frac{q^{n}}{1-q^{n}}=\sum_{n=1}^{\infty}\frac{\mu(n)}{q^{-n}-1}=q \;\;\;\;\;\; |q|<1$$
This follows from Möbius inversion of the geometric series :
$$\frac{q}{1-q}=\sum_{n=1}^{\infty}q^{n}\;\;\;\;\;\;|q|<1$$
Now, consider the slightly different problem : 
$$\sum_{n=1}^{\infty}\mu(n)\frac{1}{q^{-n}-p^{-n}}=\sum_{n=1}^{\infty}\mu(n)\frac{q^{n}}{1-\left(\frac{q}{p}\right)^{n}}\;\;\;\;\;\;|q|<|p|$$
We have : 
$$\frac{q}{1-\left(\frac{q}{p}\right)}=p\sum_{n=1}^{\infty}\left(\frac{q}{p}\right)^{n}$$
Which is not an explicit function in $n$ -Please notice the $p$ factor- Thus, Möbius inversion doesn't work. My question : is there a way to get a closed form for : 
$$\sum_{n=1}^{\infty}\frac{\mu(n)}{q^{-n}-p^{-n}}$$ EDIT : By simple manipulation, We have :
$$\sum_{n=1}^{\infty}\mu(n)\frac{1}{q^{-n}-p^{-n}}=\sum_{n=1}^{\infty}\mu(n)\frac{p^{n}\left(\frac{q}{p} \right )^{n}}{1-\left(\frac{q}{p} \right )^{n}}=\sum_{m=1}^{\infty}f_{m}(p)\left(\frac{q}{p} \right )^{m}$$
Where the polynomials $f_{m}(p)$ are given by :
$$f_{m}(p)=\sum_{n|m}\mu(n)p^{n}$$
And they satisfy :
$$\sum_{n|m}f_{m}(p^{\frac{m}{n}})=p$$
And the question becomes, what are these polynomials, and what is their algebraic meaning ?","['number-theory', 'mobius-function', 'sequences-and-series', 'mobius-inversion']"
2698757,Showing that a linear map does not achieve its norm,"If we take the Banach spaces $(X, \|\cdot \|_X )$ and $(Y, \|\cdot \|_Y )$ and the bounded linear map $F: X \rightarrow Y$, then the norm of $F$ is $$\|F\| = \underset{\|x\|_X = 1}{\textrm{sup}} \|F(x)\|_Y.$$ My question is how we can determine, generally, whether or not the map $F$ actually achieves that supremum. This is the example I have in mind: $(X, \|\cdot \|_X ) = (C([0, 1], \mathbb{R}), \|\cdot \|_{\infty})$ and $(Y, \|\cdot \|_Y)= (\mathbb{R}, |\cdot |)$, and the map $F: X \rightarrow Y$ is given by $F(f) = \int_{0}^{1} xf(x)dx$. It can be shown in a standard way that $\|F\| = \frac{1}{\sqrt3}$ and that $F$ actually achieves this norm when $f(x) = x$. But now consider the situation where instead of $X$ being all continuous functions from $[0,1]$ to $\mathbb{R}$, it's only those functions where $f(1)=0$. I believe it can be shown that $F$ doesn't achieve its norm in this case, although we can show that the norm of $F$ remains $\frac{1}{\sqrt3}$ by taking the limit of $|F(f_n )|$ where $$f_n (x) =  \begin{cases} 
      x & 0\leq x\leq 1-\frac{1}{n} \\
      (1-n)x + (n-1)& 1-\frac{1}{n}< x\leq 1
   \end{cases}
$$ and this limit will be $\frac{1}{\sqrt3}$ because $f_n \rightarrow x$ on $[0,1]$. But since $f(x) = x$ isn't in our new space, we can't use it to prove that $F$ reaches its norm here. But we don't necessarily know that there's only one function that lets $F$ achieve its norm, do we? Maybe a theorem of Riesz can be used to show this? Otherwise, how can we show that $F$ won't achieve its norm on this new space? And how can we do that generally? $\bf{\textrm{EDIT}}$: I miscalculated the norm of $F$ -- it is actually $\frac{1}{2}$.","['functional-analysis', 'normed-spaces', 'supremum-and-infimum', 'linear-transformations']"
2698798,Finding the the derivative of $y=\sqrt{1-\sin x}; 0<x<\pi/2$.,"A question I'm attempting is: Find the derivative of $ y = \sqrt {1 - \sin x} ; 0 < x <\pi/2$. I did this:
$y = \sqrt {1 - \sin x} = \sqrt {\cos^2\frac{x}{2} + \sin^2\frac{x}{2} - 2\sin \frac{x}{2}\cos \frac{x}{2}} = \sqrt { (\sin \frac{x}{2}-\cos \frac{x}{2})^2} = \sin \frac{x}{2} - \cos \frac{x}{2}$ So,  $\frac{dy}{dx} = \frac{1}{2} \cdot (\cos\frac{x}{2} + \sin\frac{x}{2})$. But apparently this is wrong. The correct solution is:
$\frac{dy}{dx} = -\frac{1}{2}\cdot(\cos\frac{x}{2} + \sin\frac{x}{2})$. So I want to know what I have done wrongly here. Why is my answer not right?","['derivatives', 'trigonometry', 'calculus']"
2698849,Find all positives integers $n$ such that $n^3+1$ is a perfect square [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question A solution as follows: $n^3+1=x^2$ $n^3=x^2-1$ $n^3=(x-1)(x+1)$ $x-1=(x+1)^2~~or~~x+1=(x-1)^2$ $x^2+x+2=0~~or~~x^2-3x=0$ $x(x-3)=0$ $x=0~~or~~x=3~~\Longrightarrow~~n=2$ Does it cover all possible solutions? How to prove that 2 is the only which solves the problem.",['number-theory']
2698903,"Can/cannot one integrate over a measurable set $A \subset [-\pi, \pi]$?","So this has caused some confusion. In one exercise one was asked to prove that $$\lim_{k \rightarrow \infty} \int_A \cos(kt)dt=0$$ where $A \subset [-\pi, \pi]$ is a measurable set. My initial idea was to take any $a,b \in A$ and then show that: $$\lim_{k \rightarrow \infty} \int_a^b cos(kt)dt= \lim_{k \rightarrow \infty} \frac{\sin(bk)-\sin(ak)}{k} =0$$ But my instructor said that this wouldn't work out because the lengths of the partitions on $A$ would make my computation impossible. I'm not sure about this.","['definite-integrals', 'measure-theory']"
2698943,Why the forgetful functor from $\mathbf{Ab}$ to $\mathbf{Grp}$ does not admit a right adjoint?,"Recently I was having an algebra lecture that introduces category theory and encountered a problem about the nonexistense of the right adjoint functor of the forgetful functor $U\colon\mathbf{Ab}\to\mathbf{Grp}$. I know it admits a left adjoint, which is defined by the abelianization functor $\mathbf{Grp}\to\mathbf{Ab}$. However it is not so obvious to me that $U$ does not have a right adjoint. What I think may be useful is a proposition that  $F\colon\mathcal C\to\mathcal D$ has a right adjoint if and only if the functor $\mathrm{Hom}(F-,Y)$ is representable for each $Y\in\mathrm{Ob}(\mathcal D)$ but it seems a bit difficult to show a functor not representable, nor nonexistence of natural transformations between two functors. So I would like to ask for some explanation of the nonexistence of right adjoint of such a forgetful functor, and thanks in advance...","['category-theory', 'abstract-algebra', 'adjoint-functors']"
2698961,composition of rational maps between two curves in Silverman,"I am reading Silverman's book the arithmetic of elliptic curves. On page 20, the author says: Let $C_{1}/K$ and $C_{2}/K$ be curves and let $\phi:C_{1}\to C_{2}$ be a nonconstant rational map defined over $K$. Then composition with $\phi$ induces an injection of function fields fixing $K$,
  $$
\phi^{*}:K(C_{2})\to K(C_{1}), \quad \phi^{*}f=f\circ \phi.
$$ My question: why do we have $f\circ \phi\in K(C_{1})$? Some background: (1). I think here the curves should be smooth, because then we can apply proposition 2.1 to get $\phi$ is a morphism, and then by theorem 2.3, we get $\phi$ is surjective. But I'm not sure if we have that. At the beginning of the chapter, the author only says ""We generally deal with curves that are smooth."" (2). From example 2.2, we have that for $C/K$ a smooth curve, there is a bijection of these two sets:
$$
K(C)\cup \{\infty\}\longleftrightarrow \{\textrm{rational maps } C\to \mathbb{P}^{1} \textrm{ defined over } K\},
$$
where $\infty$ corresponds to $[1,0]$. The author says ""we will often implicitly identify these two sets"". Based on the above information, I tried the following: If I assume they are smooth, then $\phi:C_{1}\to C_{2}$ is a map and $f:C_{2}\to \mathbb{P}^{1}$ is a map. Then $f\circ \phi:C_{1}\to \mathbb{P}^{1}$ is a map. I want to show it is a rational map. Let $C_{1}\subset \mathbb{P}^{m}$ and $C_{2}\subset \mathbb{P}^{n}$. Let $\phi=[\phi_{0},\cdots,\phi_{n}]$ and $f=[f_{0},f_{1}]$. Then some $\phi_{i}\notin I(C_{1})$ and some $f_{j}\notin I(C_{2})$. I'm trying to prove $[f_{0}(\phi_{0},\cdots,\phi_{n}),f_{1}(\phi_{0},\cdots,\phi_{n})]$ is a rational map and it equals $f\circ \phi$. I'm checking the conditions for it to be a rational map. But I cannot verify that $f_{0}(\phi_{0},\cdots,\phi_{n})$ or $f_{1}(\phi_{0},\cdots,\phi_{n})$ is not in $I(C_{1})$. Thank you for your help!","['algebraic-curves', 'abstract-algebra', 'elliptic-curves', 'algebraic-geometry']"
2698998,Notation for function spaces,"I'd like to have suggestions (maybe existing well-established practice?) for rigorous notation of certain operations on functions and function spaces (with the category of sets in mind, so no additional implied structure). Notes: the requested notation is intended to be used in / processed by a computer system (as opposed to human reading only); therefore, it needs to be 100% accurate and must be fully explicit and unambiguous (cannot use implied contextual information, for example) the term ""function"" is used in the category theory sense, i.e. each function's definition includes the domain, the codomain and the graph of the function. Two functions are considered identical iff all three of the above are identical. (In fact, defining the graph establishes the domain as well, but it is customary to have it separately anyway. However, for a given graph, all sorts of different codomains may be chosen, each yielding a different function in the intended sense.) I know that some of the guys here have an urge to respond to such questions with ""use whatever you like but define your notation"". I am well aware of that, but the reason for asking a community is to find notation which is intuitive and not arbitrary, so I would appreciate actual suggestions, if possible. Spaces These are the spaces I need notation for: (1) Set of functions with given domain $X$ and codomain $Y$ . This is the most basic one. Note that only functions with domain / codomain matching exactly should be included. Existing notation that may be considered: $X \rightarrow Y$ , $X \longrightarrow Y$ , $Y^X$ . The shorter arrow is used as a logical operator for implication, so the longer arrow is preferred. (2) Set of functions with given domain $X$ , and codomain $Y ^\prime \subset Y$ (for given $Y$ ) . Very similar in concept to (1). In practice, it is customary not to make a difference at all between (1) and (2). However, because of the nature of the use case, a different notation is required, as the sets defined by (1) and (2) are different sets. (3) Same as (2) but limited to functions that are surjective to their own codomain. This is an important practical concept: the set constructed this way essentially has a ""canonical"" representant member for all graphs mapping $X$ to $Y$ (namely, the function whose codomain is equal to its image). (2) does not have this property: any given graph (unless surjective to the whole $Y$ ) will have different representants (by choosing $Y^\prime$ to be a set containing the image, each such $Y^\prime$ will yield a different function by definition) Union of functions (4) Given two functions, $f : X \longrightarrow Y$ and $g : Z \longrightarrow W$ , one can take the pairwise union of the domains, the codomains and the graphs. However, the ""object"" constructed this way is not necessarily a function. Therefore, the notation $f \cup g$ is not expressive enough in all scenarios. There are three possible scenarios and maybe required notations: (4a) ""disjoint function union"" : operator which is only valid if the domains of $f$ and $g$ are disjoint. Maybe something like $f \sqcup g$ ? (4b) ""matching function union"" : this operator would yield a valid value if $f$ and $g$ are identical on the intersection $X \cap Z$ . Maybe something like $f ⊍ g$ ? (4c) ""overriding function union"" : this operator would always yield a function. On the intersection of the domains, $g$ would ""override"" $f$ (I know wikipedia suggests the circled plus operator for this, but, frankly, I have never met anyone using that operator for this purpose) Union Space (5) In practical scenarios it would be nice to construct the following space as well: given an arbitrary $X = \lbrace x_1, x_2, \ldots, x_n \rbrace$ , and arbitrary sets $Y_1, Y_2,\ldots, Y_n$ , the set of functions that contains all $f$ such that the domain of $f$ is $X$ , the graph satisfies $\forall i f(x_i) \in Y_i$ , and the codomain of f is equal to the range of $f$ . Note that $X$ is finite in practice but $Y_i$ can be any set. Of course, this can be naturally extended to infinite $X$ as well. Extension of the codomain of a function We all know the notation for restriction : for the function $f:D \longrightarrow C$ and any $D^\prime \subset D$ , the notation $f|_{D^\prime}$ defines a new function $f|_{D^\prime}:D^\prime \longrightarrow C$ , with the graph of this new function being equal to $f \cap (D^\prime \times C)$ . (6) A very similar concept would be to extend the codomain, i.e. for a function $f:D \longrightarrow C$ and any $C^\prime \supset C$ , we can construct the new function, which has the same graph as $f$ , but the codomain is extended to be $C^\prime$ . I have never seen notation for that, though.","['elementary-set-theory', 'notation', 'functions']"
2699083,$\mathbb Z$ is integrally closed in $\mathbb Q$?,"I understand that an integral domain $D$ is said to be integrally closed in $S$ if whenever an element $s \in S$ can be viewed as a root of a polynomial with coefficients in $D$, it must be in $D$. However, apparently $\mathbb Z$ is integrally closed in $\mathbb Q$, which I don't get, since surely given any $\frac{a}{b} (a, b \in \mathbb Z$), it is the root of the polynomial $bx - a$, and yet $\frac{a}{b}$ may not be in $\mathbb Z$. Where have I gone wrong here?","['abstract-algebra', 'algebraic-number-theory', 'commutative-algebra']"
2699112,Why is this telescoping sum divergent?,"I'm trying to figure out whether $$\sum_{x = 1}^{\infty}\ln\bigg(\frac{x}{x+1}\bigg)$$ converges or diverges. Here is what I tried doing: \begin{align}\sum_{x = 1}^{\infty}\ln\bigg(\frac{x}{x+1}\bigg)
 &= \sum_{x = 1}^{\infty}\ln(x)-\ln(x+1) \\
 &= \sum_{x = 1}^{\infty}\ln(x) - \sum_{x = 1}^{\infty}\ln(x+1) \\
 &=(\ln(1) + \ln(2) + \cdots)-(\ln(2)+\ln(3) + \cdots) \\
 &=\ln(1)\end{align} So I though that the series would converge to $\ln(1) = 0$ since all of the other terms cancel out. I checked using WolframAlpha and the series actually diverges. I was wondering why is this the case and what is wrong with my work above?",['calculus']
2699140,Calculate $ \int_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}} dx $,"$ \int_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}} dx $ = -$ 
2.887270... $ The function has no antiderivate and there's no symmetry here to help, if you solve this I would be thankful if you would for a highschooler ^^","['integration', 'calculus']"
2699153,Disproving a misconception about Grandi's Sum,"I understand most people do not like questions about Grandi's Sum, but I was hoping to ask more about the reasoning flaw behind it and ask if my proof against it makes sense, as I'm taking real analysis this year and was wondering if I'm using the principles I'm learning in the correct fashion. Essentially, the misconception I'm talking about is the fact that Grandi's sum, which is $1 - 1 + 1 - 1 + 1 - 1 + ... + (-1)^n + ... $, is equivalent to $\frac{1}{2}$. My proof against this claim is as follows, and is actually a proof showing the sum does not equal ANY number at all... Take the sequence $s_n = \sum\limits_{k=0}^{n} (-1)^k$. That is, $s_0 = 1, s_1 = 1-1 = 0, ...$ and so on. We can construct two subsequences from $s_n$: one of the even-indexed terms of $s_n$, and another of the odd-indexed terms of $s_n$: $\{s_{2k}\} = 1,1,1 ...$ $\{s_{2k+1}\} = 0, 0, 0 ...$ As such, we have: \begin{align*}
\lim_{k \to \infty} s_{2k} &= 1 \\
\lim_{k \to \infty} s_{2k+1} &= 0 \\
\end{align*} (By the Subsequence Theorem) If the sequence $s_n$ is convergent, i.e. has a limit $L$, then all of its subsequences are convergent to $L$. However, we have two subsequences of $s_n$ converge to different limits, and so we conclude $s_n$ is not convergent, i.e. its limit does not exist. We note the following: $\lim_{n \to \infty} s_n = \sum\limits_{k=0}^{\infty} (-1)^k$ Since the limit of $s_n$ as $n \rightarrow \infty$ does not exist, the summation on the RHS does not converge to a value. As such, we cannot propose that $1 - 1 + 1 - 1 + ... + (-1)^n + ...$ is equal to a number $S$, as that implies the summation is convergent, which we have shown is untrue. Thus, the claim $1 - 1 + 1 - 1 + ... + (-1)^n + ... = \frac{1}{2}$ is invalid. I think one of the fatal flaws in the claim $1 - 1 + 1 - ... = \frac{1}{2}$ is assuming the convergence of the series in the first place, which I am trying to point out using some of the facts I am currently learning. Is this proof correct?","['real-analysis', 'sequences-and-series', 'proof-verification']"
2699175,Inequality $¥frac{a^3+b^3+c^3}{3}¥geq¥sqrt{¥frac{a^4+b^4+c^4}{3}}$,"Let $a$, $b$ and $c$ be non-negative numbers such that $(a+b)(a+c)(b+c)=8$. Prove that:
$$¥frac{a^3+b^3+c^3}{3}¥geq¥sqrt{¥frac{a^4+b^4+c^4}{3}}$$ Some attempts: From the condition follows $a^3+b^3+c^3 = (a+b+c)^3 -24$ It is known ( see here ) 
$$¥frac{a+b+c}{3}¥geq¥sqrt[27]{¥frac{a^3+b^3+c^3}{3}}$$ Setting $2x=a+b$, $2y = b+c$, $2z = a+c$, we can express $a =x+z-y$ etc.  The condition then becomes $xyz = 1$ which can be parametrized with free variables $0¥leq q ¥leq 2 ¥pi /3 $ and arbitrary $r$ by
$$
x =  ¥exp(r ¥cos q) ¥; ; ¥;  y =  ¥exp(r ¥cos (q + 2 ¥pi /3)) ¥; ; ¥;  z =  ¥exp(r ¥cos (q - 2 ¥pi /3)) 
$$
Using that, the condition can be removed and then calculus may be used. The question may be interpreted geometrically. Expressions such as $a^3+b^3+c^3 = $const. and $a^4+b^4+c^4 =$ const. can be  interpreted as hypersurfaces of what has been called  an N(3)-dimensional ball in p-norm, see here . A nice visualization is given in here . Then properties such as extrema, convexity etc. of these surfaces can be used. I couldn't put the pieces together.","['inequality', 'geometry']"
2699274,show $\int_0^{\infty} \frac{x\sin ax}{x^2+t^2}dx = \frac{\pi}{2}e^{-at}$,"I'm asked to show  $\int_0^{\infty} \frac{x\sin ax}{x^2+t^2}dx = \frac{\pi}{2}e^{-at}$ when $t,a > 0$.
I tried using integration by parts integrating $\frac{x}{x^2+t^2}$. But it seems that $\frac{x\sin ax}{x^2+t^2}$ has no antiderivative. What should I try?","['integration', 'definite-integrals', 'measure-theory']"
2699293,Random walk on integers keeping direction with prob $p$ and changing direction with prob $1-p$,"An insect moves on the integers in steps $\pm 1$ , in a way that it keeps its direction with prob $p$ and changes direction with prob $1-p$ . Its position at time $t=0$ is $0$ (coming from $-1$ ). (1)  Define a Markov chain that describes the insect's movement and find the transition probabilities. (2)  Find the probability the insect reaches state $N$ before it reaches $-N$ . Attempt. Let $X_n$ be the insect's position after $n$ steps. Then $X_n$ takes values on integers and if fact $X_{2n}$ takes values $-2n,\ldots,-2,0,2,\ldots,2n$ (for example $$P(X_2=+2)=p^2,~P(X_2=0)=(1-p)p+(1-p)^2,~P(X_2=-2)=(1-p)p)$$ and $X_{2n+1}$ takes values $-2n-1,\ldots,-1,1,\ldots,2n+1$ .
As for the transition probabilities, if $X_{2n}=2k$ then the probability $X_{2n+1}=2k+1$ equals: $$p\times \textrm{number of ways to reach position $2k$ from left in $2n$ steps}~+$$ $$(1-p)\times \textrm{number of ways to reach position $2k$ from right in $2n$ steps}~~~(\star).$$ Of course $$P(X_{2n+1}=2k-1\mid X_{2n}=2k)=1-P(X_{2n+1}=2k+1\mid X_{2n}=2k),$$ $$P(X_{2n}=2k\mid X_{2n-1}=2k-1)=\ldots~(\textrm{as above}),$$ $$P(X_{2n}=2k-2\mid X_{2n-1}=2k-1)=1-P(X_{2n}=2k\mid X_{2n-1}=2k-1).$$ I have stuck on evaluating the above number of ways in ( $\star$ ). Thanks in advance for the help.","['stochastic-processes', 'markov-chains', 'probability-theory']"
2699295,"Showing $P(S_m<m, \forall\ 1\leq m\leq n | S_n)=\max\{0, 1-S_n/n\}$","Let $X_1, X_2,\ldots$ be a sequence of iid random variables such that for each $i$, $X_i$ takes value as nonnegative integer and is in $L^1$. Let $ S_n = \sum_{i=1}^n X_i$. How to show that \begin{equation}
P(S_m<m, \forall\ 1\leq m\leq n | S_n)=\max\{0, 1-S_n/n\} ?
\end{equation} I think that there is something to do with martingales, but I am not really sure where to start.
Thanks!","['conditional-expectation', 'probability', 'martingales']"
2699358,How exactly to use Cayley's Hamilton's theorem to find $A^{50}$ in this case? (matrix recursion equation),"Say $$A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$$ I'm supposed to find $A^{50}$ using the Cayley Hamilton theorem. My Attempt: $|A-\lambda I| = 0 \implies \lambda^2-2\lambda+1=0$ So $A^2-2A+I = O \implies A^{50}=2A^{49} - A^{48}$ But using this method, I'll need to know $A^{49}$ and $A^{48}$ to find $A^{50}$. But that doesn't seem the right way. This seems like some sort of matrix recursion but I'm not sure how to proceed from here.","['matrices', 'linear-algebra']"
2699383,How does WolframAlpha get an exact answer here? ${}{}$,"I had a simple thing to compute with a calculator:
$$\sin\left(2\cos^{-1}\left(\frac{15}{17}\right)\right)$$
I got the decimal answer of about $0.83044983$, but when I typed it in WolframAlpha, it also gave an exact answer of $\frac{240}{289}$. How in the world would one get an exact answer here?","['wolfram-alpha', 'trigonometry']"
2699410,What kind of matrix is this and why does this happen?,"So I was studying Markov chains and I came across this matrix  \begin{align*}P=\left( \begin{array}{ccccc}
        0 & \frac{1}{4} & \frac{3}{4} & 0 & 0\\
        \frac{1}{4} & 0 & 0 & \frac{1}{4} & \frac{1}{2}\\
        \frac{1}{2} & 0 & 0 & \frac{1}{4}& \frac{1}{4}\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
    \end{array} \right).\end{align*} I noticed (by brute force) that \begin{align*}P^2=\left( \begin{array}{ccccc}
        \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
        \frac{3}{8} & 0 & 0 & \frac{1}{4}& \frac{1}{2}\\
        \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\
    \end{array} \right),\end{align*}
and \begin{align*}P^3=\left( \begin{array}{ccccc}
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
        \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\
        \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
        0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\
    \end{array} \right).\end{align*} In fact; using a computer I found that every even power takes the form of the $P^2$ matrix and every odd power takes the form of the $P^3$ matrix. I just wanted to know why that oscillation occurs? Is there a special name for the kind of matrix that $P$ is for it to exhibit that kind of behaviour?",['matrices']
2699429,"Why are $i$ and $-i$ ""more indistinguishable"" than $\sqrt{2}$ and $-\sqrt{2}$?","Today I learned that two roots of an irreducible polynomial are ""algebraically indistinguishable."" In $\mathbb{Q}(\sqrt{2})$, define the conjugate of $a+b\sqrt{2}$ as $\overline{a+b\sqrt{2}} = a - b\sqrt{2}$. My understanding/intuition on ""algebraically indistinguishable"" is that if $P$ and $Q$ are algebraic expressions (meaning using the ring operations only) in $\mathbb{Q}(\sqrt{2})$ with $P$ = $Q$, then then we also have $\overline{P} = \overline{Q}$ where $\overline{P}$ and $\overline{Q}$ are the algebraic expressions $P$ and $Q$ except every member of $\mathbb{Q}(\sqrt{2})$ in the expression is replaced with its conjugate. For example, $2(3+\sqrt{2})(4+\sqrt{2}) - (6-\sqrt{2}) = 22 + 15\sqrt{2}$ and indeed, $2(3-\sqrt{2})(4-\sqrt{2}) - (6+\sqrt{2}) = 22 - 15\sqrt{2}$. And in $\mathbb{R}(i) = \mathbb{C}$, we're guaranteed the same phenomenon: $2(3+i)(4+i) - (6-i) = 16 + 15i$ $2(3-i)(4-i) - (6+i) = 16 - 15i$ But what I find interesting is that once I involve familiar operations outside of multiplication and addition, this will no longer hold for $\mathbb{Q}(\sqrt{2})$, but it still holds for $\mathbb{R}(i) = \mathbb{C}$. For example,
$(-\sqrt{2})^{-\sqrt{2}} \neq \overline{\sqrt{2}^{\sqrt{2}}}$
while on the other hand,$(-i)^{-i} = \overline{i^{i}}$. My initial answer to this is that exponentiation viewed as a binary operation is not closed in $\mathbb{Q}(\sqrt{2})$, so I must view this as an operation on $\mathbb{R}(i) = \mathbb{C}$, but this is a larger field than extending $\mathbb{Q}$ to include the roots of $x^{2}-2$, so in this field with exponentiation, I can now distinguish $\sqrt{2}$ and $-\sqrt{2}$. On the other hand, exponentiation on $\mathbb{R}(i) = \mathbb{C}$ is closed, so even exponentiation does not allow us to algebraically distinguish between $a+bi$ and $a-bi$.","['irreducible-polynomials', 'complex-numbers', 'abstract-algebra', 'extension-field', 'field-theory']"
2699450,"If $P$ is a polynomial with $P(3)=10$ and $P(1)=1$, then why can't all the coefficients of $P$ be integers? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If $P$ is a polynomial with $P(3)=10$ and $P(1)=1$ , then why can't all the coefficients of $P$ be integers? This question was deleted for not enough details half a year ago, therefore I'm providing them. In this question specifically, I'm asking for your help to solve this problem in 8th grader way, because I'm sure that this question might help other students to understand polynomials much better, without any higher, university-level knowledge (some provided answers there are very elegant and understandable even for a 6th grader). To mention more, right now it would be quite hypocritical to say that 'I tried < insert any theorem > but  got stuck, hence I'm asking for your help'. So I'm not saying it now, instead of that just simply asking for you to undelete this question for the reasons I mentioned above. Thank you!","['algebra-precalculus', 'polynomials', 'elementary-number-theory']"
2699483,Probability of two heads given the probability of a head on a Saturday,"The probability that a fair-coin lands on either Heads or Tails on a certain day of the week is $1/14$. Example: (H, Monday), (H, Tuesday) $...$ (T, Monday), (T, Tuesday) $...$ Thus, $(1/2 \cdot 1/7) = 1/14$. There are $14$ such outcomes. In some arbitrary week, Tom flips two fair-coins. You don't know if they were flipped on the same day, or on different days. After this arbitrary week, Tom tells you that at least one of the flips was a Heads which he flipped on Saturday . Determine the probability that Tom flipped two heads in that week. I know that this is a conditional probability problem. The probability of getting two heads is $(1/2)^2 = 1/4$. Call this event $P$ . I am trying to figure out the probability of Tom flipping at least one head on a Saturday. To get this probability, I know that we must compute the probability of there being no (H, Saturday) which is $1 - 1/14 = 13/14$. But then to get this ""at least"", we need to do $1 - 13/14$ which gives us $1/14$ again. Call this event $Q$. So is the probability of event $Q = 1/14$? It doesn't sound right to me. Afterwards we  must do $Pr(P | Q) = \frac{P(P \cap Q)}{Pr(Q)}$. Now I'm not quite sure what $P \cap Q$ means in this context.","['combinatorics', 'probability']"
2699487,What groups have the property: All nontrivial isomorphic subgroups have a nontrivial intersection?,"This is a generalization of my previous question: What are all finite groups such that all isomorphic subgroups are identical? Specifically, what finite groups $G$ have the following property: For any two subgroups $1 < H, H' < G$ that are isomorphic, then $H \cap H'$ is non-trivial. Some quick results: This property is equivalent to all prime order cyclic subgroups being unique. Also, each of these subgroups are normal. If $G$ is an abelian group, it turns out $G$ is cyclic. (Proof: $G=G_1 \times \cdots \times G_n$, each cyclic, with $|G_i|$ dividing $|G_{i+1}|$. Two factors share a common prime $p$, hence two disjoint copies of $C_p$, so no two factors exist.) However, there are non-abelian groups that satisfies this condition (if I've thought through it right). Take the Quaternion group , which from the subgroup lattice is easily seen to have the desired property. Can the (non-abelian) groups with this property be easily characterized?","['finite-groups', 'abstract-algebra', 'group-theory']"
2699534,Would f ''(a)=0 and f ''(x) does not change sign at x=a result in an inflection point?,"My note says: "" If f ''(a)=0  and f ''(x) changes sign at x=a, there is a point of inflection at (a, f(a))."" I was wondering how the original graph would appear if f ''(a)=0  and f ''(x) does not change sign at x=a, would there still be a point of inflection?","['derivatives', 'calculus']"
2699582,Is there a bell-curve-like function where n and m control the curve shape between 0 and 1?,"I'm trying to come up with a function $f(x, n, m)$ which mainly focuses on the range $0 ≤ x ≤ 1$ and $m ≤ y ≤ 1$, where $m$ is the $y$-intercept at both $0$ and $1$, and $n$ is the sole maximum of $x$. It must always be the case that $f(n) = 1$, and $f(0) = f(1) = m$. When $x < 0$ and $x > 1$, the curve must approach, but never reach, $0$. I've sketched an example: I think this involves $ln$, but I can't quite grok it. Can anyone help me design such a function $f(x, n, m)$? I can imagine an approximation using Bézier curves , so I made that . I hope this helps.","['algebra-precalculus', 'trigonometry', 'functions', 'graphing-functions']"
2699596,Analytic function $f$ such that $(f(z))^n= f(z^n)$,"Let $f(z) = \sum_{k\geq 0}a_kz^k$ be an analytic function, where $a_k\in\mathbb{C}$ for $k\geq 0$. I am trying to get some conditions for $a_k$ that give us the general form of $f$ such that $(f(z))^n= f(z^n)$. What I tried is to use the Multinomial Theorem , but I get stuck while trying to compute the coefficients of $(f(z))^n$. I would like to obtain the general solution, but in fact I need only two cases: if either $a_0 = f(0) = 0$ or $a_1 = f'(0) = 0$. Thank you!","['complex-analysis', 'multinomial-coefficients', 'analytic-functions', 'functional-equations']"
2699614,Connecting points in a plane,"If I have an even number $n$ of points in a plane (no $3$ of them colinear), half of them red and half of them blue, is there always a way to draw $\frac{n}{2}$ lines to join each red with a blue and each blue with a red in such a way that none of the lines between them intersect? This was extra credit on an exam and I have no clue where to start. We've done exercises where we counted the number of ways to join red and blue points on a circle, but never something like this. How can I prove/disprove this claim?","['combinatorics', 'graph-theory', 'geometry']"
2699630,What is the analytical form of a recursive function,"This maybe a question answered in some textbooks, I had trouble figure out how to solve it. Here is a function defined recursively, $ F_1(x) = x $ $ F_2(x) = x^2 -2 $ $ F_n(x) = F_{n-1}^{2}(x) -2 $ Is there a way to express $F_n(x) $ in term of $x$ directly?","['sequence-of-function', 'functions']"
2699655,finding a particular solution of $\frac{dy}{dx}=\frac{3y}{x}$,"I need a help with the solving the differential equation explicitly. $\frac{dy}{dx}=\frac{3y}{x}$, $y(1)=1$ and here is what I have so far: $\frac{1}{y}dy=\frac{3}{x}dx$ (separate the variables) $\int\frac{1}{y}\,dy=\int\frac{3}{x}\,dx$ (integrate) $\ln\vert y\vert =3\ln\vert x\vert +C$ substitute the initial condition and get $C=0$ $\ln\vert y\vert =3\ln\vert x\vert$ $e^{\ln\vert y\vert}=e^{3\ln\vert x\vert}$ $\vert y\vert=\vert x\vert ^3$ $y=\pm\vert x\vert ^3$ $y=\vert x\vert ^3$ (choosing + judging by the initial condition) and this is where I'm stuck. The answer key says it's $y=x^3$. -EDIT- now the next part of the question is asking what the domain of the particular solution is. The answer says it's $(0,\,\infty)$. I understand the initial condition of $x=1$ is in there, but I don't understand why I need to choose it, ignoring $(-\infty,\,0)$. If the problem stated the particular solution has no removable discontinuity, then I agree with their answer. But for now, I keep getting the domain of $x\neq 0$. -EDIT2- After more thorough research about this, I found that a solution to a differentiable equation is supposed to be ""continuously differentiable function"". Thus, only choosing a continuous interval that contains the initial condition. This is well-discussed in DE textbooks, but rarely in Calculus textbooks (The Chair of AP Calculus Development Committee agrees on this). For example, Stewart's Calculus textbooks don't even mention about the domain of solutions (granted, those textbooks are not designed to prep for AP exams). But I just hope that this is discussed by most instructors. Source: http://ecademy.agnesscott.edu/~lriddle/apcalculus/apcentral-domain.pdf","['ordinary-differential-equations', 'calculus']"
2699663,"If $f(x)=\frac{x(x^2-1)}{(x^2+1)^2}$, find four distinct rational $x_i> 1$ such that $f(x_2)-f(x_0)=2f(x_2)-f(x_1)=f(x_3)$","I have been working on this problem for quite a while and frankly, I ran out of ideas quite a while ago and hence I decided to ask it here(my ideas are below). I will greatly appreciate any help. Given $(x_0,x_1,x_2,x_3)\in \mathbb{Q}$ where all $(x_0,x_1,x_2,x_3)$ are larger than one and they are all unique and given
$$f(x) = \dfrac{x(x^2 - 1)}{(x^2 + 1)^2}$$
I was trying to determine a valid $(x_0,x_1,x_2,x_3)$ such that
$$f(x_2) - f(x_0) - f(x_3) = 0$$
and
$$2f(x_2) - f(x_1) - f(x_3) = 0$$ My approaches Trigonometry As $\cos \theta$ and $\sin \theta$ can be parametarized by $\dfrac{2x}{x^2 + 1}$ and $\dfrac{x^2 - 1}{x^2 + 1}$, $f(x)$ can be thought to be $\dfrac{\sin 2\theta}{4}$. Hence, the first condition can be thought as 
$$\sin 2\theta _2 - sin 2\theta _0 - \sin 2\theta _3 = 0$$
Yet I failed to reach any ideas from here. Groebner basis I am quite new to this idea of Groebner basis yet using Sage, I found 9 Groebner bases of an ideal defined by the variance given by the two conditions above. Yet I found that two of those 9 were the two conditions above while for the others to be 0, $(x_0,x_1,x_2,x_3)$ cannot be all unique or larger than one. Yet as I said, I am quite new to this concept as I am quite new to algebraic geometry, I may be wrong. If this is the case I will greatly appreciate any corrections Brute Force I defined two terms $m_i$ and $n_i$ where they are both integers which gives
$$f(x_i) = \dfrac{n_im_i(n_i^2 - m_i^2)}{(n_i^2 + m_i^2)^2}$$ 
where $x_i = \dfrac{n_i}{m_i}$. Here, I ran a octa-loop(horrendous amounts of loops), making sure n_i and m_i are mutually prime, with hope that some $n_0, m_0$.....$n_3, m_3$ might be found. I was unsuccessful. I'm fairly confident that there is a better way to go about doing this and I am just unfamiliar with it. Any help is appreciated. Thank you in advance!","['number-theory', 'trigonometry', 'algebraic-geometry']"
2699678,How to prove combinatorical sum identity? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question $$\sum_{k=0}^n 2^{2n+1-2k}\binom{2n+1-k}{k}(-1)^k=2(n+1)$$ According to wolfram, this is true. How would one prove this either algebraically or combinatorically?","['combinatorics', 'sequences-and-series']"
2699708,"In general, what techniques can be used to show that 2 groups are not isomorphic?","Say I have 2 groups $G$ and $H$, what techniques can be used to show that they are not isomorphic? A simple one I can think of is proving that their order is different, thus showing there cannot be a bijection in between the 2. However I am interested in other approaches as well.","['abstract-algebra', 'group-theory', 'group-isomorphism']"
2699738,Find geometric interpretation of differential operators in vector fields.,"I'm trying to create some notes on multivariable calculus, but It has been hard to find the geometric point of view of this: Jacobian Matrix (Derivative of a VF) Laplacian Nabla operator Curl Divergence And most commonly some useful identities such as the curl of a gradient, divergence of a gradient. Many of the book that I have look up are theorical based. And yep, many videos on YouTube show kind of the overall the physical phenomena but they assume some background on physics. :( I'm interested in explaining this operators with the help of Mathematica or any kind of graphics to show how they work on the vector field. For example, I do know that the divergence is a measurement of how much flow enters on the neighborhood of P compared to how much it leaves. And the curl measures the rotation of the vector field in the neighborhood around P. 
But I can't think of a graph to explain this. Neither I have found such a explanation for the Jacobian matrix besides being the linear map which works as a the derivative nor the laplacian or nabla operators. I would love if you can share some of your favorite books or any idea, having in mind that my knowledge in physics is merely basic.","['multivariable-calculus', 'vector-fields', 'education', 'vector-analysis']"
2699772,DNA arrangement problem (ATGC),"Our genetic material, DNA, is formed from a $4$ letter alphabet"" of bases: A, T, G, C (adenine, thymine, guanine,
and cytosine). The order in which the letters are arranged is important, but because a molecule can move, there is
no difference between a sequence and the same sequence reversed. How many distinct DNA sequences of $5$ bases are
there? The only bit I've gotten so far is $4^5$ I have no idea how to start with the palindrome. Can anyone walk me through on the thought process please? Thank you",['combinatorics']
2699811,Computing the derivative of $Axx^TB^T$ with respect to $x$,"I want to compute the derivative of \begin{align}
f(x) = Axx^\top B^\top \label{eqn}
\end{align} with respect to $x$ where $A$ and $B$ are $n\times n$ matrices and $x$ is a (column) vector of size $n \times 1$. By this I mean the derivative of each component of $f(x)$ with respect to each component of $x$. I can prove that if 
$$
g(x) = xx^\top
$$ Then the derivative can be expressed as,
$$
\frac{\partial g}{\partial x} = x \otimes I_n + I_n \otimes x
$$
where $I_n$ is the $n\times n$ identity matrix. In here I am
vectorizing $xx^\top$  and then taking the derivative with respect
to each of the components of $x$. Question: Is there a way to extend this result to $f(x)$. My gut feeling is that this
should be possible. Any thoughts?. If that's not possible
how do I go about computing it? EDIT (After Rodrigo de Azevedo's comment): You are right. But I mean the derivative in the following flattened sense. I hope this makes it a bit clearer. Let us consider the $2 \times 2$ case. Then $ Y =f(x)$ is a $2 \times 2$
matrix. If I vectorize $f(x)$ then I can view $f$ as,
$$
f: \mathbb{R}^2 \to \mathbb{R}^4
$$
More precisely
\begin{align}
f:
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix} \to
  \begin{bmatrix}
    Y_{11} \\
    Y_{21} \\
    Y_{12} \\
    Y_{22}
  \end{bmatrix}
\end{align}
Then by the symbol $\frac{\partial{f(x)}}{\partial{x}}$ I mean the
following:
\begin{align}
  \frac{\partial{f(x)}}{\partial{x}}
  & =
    \begin{bmatrix}
      \frac{\partial{Y_{11}}}{\partial{x_1}} &
      \frac{\partial{Y_{11}}}{\partial{x_2}} \\
      \frac{\partial{Y_{21}}}{\partial{x_1}} &
      \frac{\partial{Y_{21}}}{\partial{x_2}} \\
      \frac{\partial{Y_{12}}}{\partial{x_1}} &
      \frac{\partial{Y_{12}}}{\partial{x_2}} \\
      \frac{\partial{Y_{22}}}{\partial{x_1}} &
      \frac{\partial{Y_{22}}}{\partial{x_2}}
    \end{bmatrix}
\end{align}","['matrices', 'kronecker-product', 'matrix-calculus', 'derivatives']"
2699834,Reduction to separable ODE,"The problem is find the particular solution of $\frac{dy}{dx}$=$\frac{y-x}{y+x}$ when f(7)=7 For $\frac{dy}{dx}$=$\frac{y-x}{y+x}$,
first i substitute y and $\frac{dy}{dx}$ to
$ux$ and $x\frac{du}{dx}+u$ so the equation change to $x\frac{du}{dx}+u$=$\frac{u-1}{u+1}$
now i think this equation is separable.
$(u+1)du$=$\frac{-2}{x}dx$
Then I integration the equation and solve the problem but I wrong
I think i solve the problem right way and I can't find my mistake what is the wrong part of my solution?",['ordinary-differential-equations']
2699919,Show the continuous image of a compact set is compact. [duplicate],"This question already has answers here : Continuous image of compact sets are compact (3 answers) Closed 6 years ago . Using the open-cover definition of compactness (if X is a topological space, a collection of sets {$U_{\alpha}|\alpha\in A$} with each being open in X, is said to be an open cover of X if $X=\bigcup_{\alpha\in
A}U_\alpha$. The space $X$ is compact if for every open cover of $X$, there is a finite subcollection of {$U_\alpha$} that is also an open cover of X.) prove that the continuous image of a compact set is compact. This implies compactness is a topological property? The exercise hints at starting with an arbitrary open cover of the image space. Any further hints would be appreciated. Thanks in advance! Edit: I believe the possible duplicate question concerns a less general case in $R^n$. Also, the answers do not address open covers which is the source of much of my confusion.","['general-topology', 'compactness']"
2699924,Linear algebra objective type question csir 2017.,For every $4\times 4$ real non singular symmetric matrix $A$ there exist a positive integer $p$ such that $pI+A$ is positive definite. $A^p$ is positive definite. $A^{-p}$ is positive definite. $ e^{pA}-I$ is positive definite. $ 1$st option is correct as we can choose $p$ which is greater than each eigen value of matrix $A$. Similarly option second and third is also correct as in these case choose  $p$ an even integer. But I don’t know how to deal with third option . Please help me in this case . Thanks .,['linear-algebra']
2699926,Proving Fibonacci Identity,"in this question I am asked to prove that if $A=\begin{bmatrix}1 & 1 \\ 1 & 0 \end{bmatrix}$ then $A^n= \begin{bmatrix} F_{n+1} & F_n \\ F_n & F_{n-1} \end{bmatrix}$ which I have successfully proved by induction on n. 
Then we are asked to prove that $F_{k+j} = F_kF_{j+1} + F_{k−1}F_j = F_{k+1}F_j + F_kF_{j−1}$ by comparing the identity $A^{k+j}= A^kA^j $
And then a similar question for $A^{k+j+l}= A^kA^jA^l $  , I have tried answering these questions by computing the matrix identities but it got too complicated and I got lost in the algebra, is there an easier way to prove this? Thank you!","['matrices', 'fibonacci-numbers']"
2699935,Puzzle on the proof of that test function space is not metrizable,"Rudin argues in ""functional analysis"", the test function space $D(\Omega)$ on nonempty open set $\Omega$ is not metrizable. Let $D_K$ be the subspace of $D(\Omega)$ consisting of functions with support contained in compact $K$. Rudin said that ""it is obvious that each $D_K$ has empty interior relative to $D(\Omega)$"". I don't understand after thinking for hours. Any help is appreciated.",['functional-analysis']
2699949,Prove $\cos(x)$ is continuous,"I'm stuck at a particular step and could use some help. By definition, a function is continuous at $x=a$ iff $\lim_{x \to a} f(x) = f(a)$. So I assume to prove $\cos(x)$ is continuous we must use the definition of a limit to show that: $$\lim_{x \to a} \cos(x) = \cos(a) \iff \forall \epsilon > 0, \exists \delta>0 : 0 < |x - a| < \delta \implies |\cos(x)-\cos(a)| < \epsilon$$ From some triangle proofs it can be shown that: $\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha) \sin(\beta)$ $\cos(\alpha - \beta) = \cos(\alpha)\cos(\beta) + \sin(\alpha) \sin(\beta)$ Subtract the two equations: $$\cos(\alpha + \beta) - \cos(\alpha - \beta) = -2\sin(\alpha) \sin(\beta)$$ Let $x = \alpha + \beta$ and $a = \alpha - \beta$. Then $\alpha = \frac{x+a}{2}$ and $\beta = \frac{x-a}{2}$, implying: $$\cos(x) - \cos(a) = -2\sin\left(\frac{x+a}{2}\right)\sin\left(\frac{x-a}{2}\right)$$ So $$\left|\cos(x)-\cos(a)\right| = 2\left|\sin\left(\frac{x+a}{2}\right)\right| \cdot \left|\sin\left(\frac{x-a}{2}\right)\right|$$ I feel like I am close because I was able to change it so at least we have an $x-a$ term but now I'm not sure where to go from here.","['trigonometry', 'calculus', 'proof-verification', 'continuity', 'proof-writing']"
2700013,Morphism from complete variety to an affine variety,"I am working on Mumfords Abelian Varieties and am not 100% familiar with varieties yet.
In some proof it is used that any morphism from a complete variety to an affine variety is constant. I could not find any references on that. Thanks a lot in advance.","['abelian-varieties', 'affine-varieties', 'algebraic-geometry']"
2700022,Prove that $\sin(1 + k^3) \not \to 0$,"How can we prove that for $k\in \mathbb{Z}\quad k\to \infty$ $$\sin(1 + k^3) \not \to 0$$ Lately I've encoutered it a couple of times in some OP posted here about series and by the discussion I had, also with expert users, it seems there is not a simple solution. One possible strategy I had in mind is to show that for $$\sin(1+k^3) \approx0$$ then $$\sin(1+(k+1)^3)=\sin(1+k^3+3k^2+3k+1)=\sin(1+k^3)\cos(3k^2+3k+1)+\sin(3k^2+3k+1)\cos(1+k^3)\approx \pm \sin(3k^2+3k+1)$$ and show that $\sin(3k^2+3k+1)$ is ""far"" from zero.","['sequences-and-series', 'limits']"
2700025,"7 boys and 4 girls, to form a committee of 6 when the committee needs exactly 2 girls. Find the number of ways.","This question is very easy indeed as we just take $${4 \choose 2} \times {7 \choose 4} = 210$$ However, let us assume for a moment that we are also interested in the possible arrangements of the members of the committee: That is we care about ordering. Now being someone who is super bad at combinatorics: My question is: Should we choose $${4 \choose 2} \times {7 \choose 4} \times 6!$$ or $${4 \choose 2}\times 2! \times {7 \choose 4} \times 4!$$ I think we should choose the former (first one). However i fail to understand why the second one doesn't work.",['combinatorics']
2700053,From conditional independence to independence,"Let $(\Omega, \mathcal{F}, {\bf P})$ be a probability space. Let $A, B \in \mathcal{F}$ be two events and $Z$ a random variable with values in a nice space $(S, \mathcal{S})$. Suppose that $A$ and $B$ are conditionally independent given $Z$ (i.e.,  independent of the $\sigma$-algebra generated by $Z$). It is known that $A$ and $B$ are not necessarily independent. However, it seems possible to show that if $A$ and $B$ are themself independent of $Z$, then $A$ and $B$ are also independent. Is the proof below correct ? If yes could you give some intuition of why ? Thanks a lot ! ${\bf Proof:}$ Denote by ${\bf P}( . | Z = z)$ the regular conditional probability of ${\bf P}$ given $Z$ and by ${\bf P}(Z \in dz)$ the law of $Z$. We have
$${\bf P}(A \cap B) = {\bf P}((A \cap B) \cap \{Z \in S\}) = \int_S{\bf P}(A \cap B | Z = z){\bf P}(Z \in dz)$$
$$= \int_S{\bf P}(A | Z = z){\bf P}(B | Z = z){\bf P}(Z \in dz)$$
$$= \int_S{\bf P}(A){\bf P}(B){\bf P}(Z \in dz) = {\bf P}(A){\bf P}(B).$$","['independence', 'probability']"
2700067,Free group generated by two generators is isomorphic to product of two infinite cyclic groups,Is the following statement true or not? The free group generated by two generators is isomorphic to the direct product of two infinite cyclic groups. I know that if the generated group is abelian then the statement is True but I don't know if it's not abelian. I think it's wrong but can't come up with a counter example. Thanks in advance.,"['abelian-groups', 'group-theory', 'linear-algebra', 'free-groups']"
2700141,Codimension-1 submanifold as a inverse image of regular value. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $M$ be a manifold and $N\subset M$ be a codimension-1 submanifold. Is it possible to find a function $H: M\rightarrow \mathbb{R}$ such that $N\subset H^{-1}(a)$ for some regular value a of $H$?. If it exists, what is necessary condition for $N=H^{-1}(a)?.$ I have no idea how to proceed it. Thanks, advance.","['smooth-manifolds', 'differential-geometry', 'differential-topology', 'morse-theory']"
2700152,"Fixed point of a monotone on [0,1].","prove or disprove: Let $f:[0,1]\rightarrow[0,1]$ be a monotone (need not be strict) function then  f has a fixed point. Can I have a hint?",['real-analysis']
2700156,How to show that the $\lim_{n \to \infty} x_n$ exists?,"Consider the sequence $x_{n+1}=\frac{x_n+3}{3x_n+1}$ where $n \geq 1$ with $0<x_1<1$. I tried it this way, splitting the terms gives $x_{n+1}-x_n=3(1-x_{n+1}x_n)$. But that doesn’t seems to be useful. Actually I am trying to find a recurrence relation between $x_{n+1}-x_n$ and $x_n-x_{n-1}$. But don’t know how?? Any other approach?? Thanks for any help!!","['real-analysis', 'calculus', 'limits']"
2700191,Connection between Kronecker pairing and duality of cohomology/homology,"In the Lecture Notes in Algebraic Topology by Davis & Kirk , we consider the Kronecker pairing which is defined by $$H^n(C;R) \times H_n(C;R) \to R \\ ([\varphi],[\alpha]) \mapsto \varphi(\alpha)$$ where $H^n(C;R)$ is the cohomology and $H_n(C;R)$ is the homology with coefficients over a ring $R$. We know this defines a map $$H^n(C;R) \to \operatorname{Hom}(H_n(C;R),R) \\ [\varphi] \mapsto ([\alpha] \mapsto \varphi(\alpha)). $$ Davis & Kirk want to prove that cohomology is not the dual of homology in general and mentioned that The map $H^n(C;R) \to \operatorname{Hom}(H_n(C;R),R)$ does not need to be injective nor surjective. My question : Why is it enough to consider this specific map? If cohomology is the dual of homology, then $H^n(C;R) \simeq \operatorname{Hom}(H_n(C;R),R)$. So why can't there be another bijective map $H^n(C;R) \to \operatorname{Hom}(H_n(C;R),R)$ then? Thank you in advance!","['algebraic-topology', 'abstract-algebra', 'homology-cohomology', 'homological-algebra']"
2700194,"If $f(f(x))+f(x)=x^4+3x^2+3$, prove that $f$ is even","Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous function such that for all $x$ we have $$f(f(x))+f(x)=x^4+3x^2+3,$$
  prove that for all $x \in \mathbb{R}$, $f(-x)=f(x)$. I noticed that $f$ cannot have any fixed points. If it had one, say $t$, we would have $$0=t^4+3t^2-2t+3=(t^2+1)^2+(t-1)^2+1$$ which is impossible. So, since $f$ is continuous, we either have $f(x)<x, \: \forall x \in \mathbb{R}$ or $f(x)>x, \: \forall x \in \mathbb{R}$. If the first were true, then we would get 
$$x^4+3x^2+3=f(f(x))+f(x)<f(x)+x<2x, \quad \forall x \in \mathbb{R}$$
which is absurd. So $f(x)>x, \: \forall x \in \mathbb{R}$. Using this and the fact that $x^4+3x^2+3$ is strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty,0]$, I managed to prove that $f$ is  strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty, 0]$. This is where I got stuck. Edit : I think I made some progess. Suppose that there is $x_0$ such that $f(x_0)<0$. Then we get $0>f(x_0)>x_0$ and since $f$ is strictly decreasing on $(-\infty,0]$ it means that $f(0)<f(x_0)<0$, which contradicts $f(0)>0$. So $f(x) \geq 0, \: \forall x \in \mathbb{R}$ Now, suppose $f(x)>f(-x)>0$, for $x \neq 0$. Then $f(f(x))>f(f(-x))$ and summing these yields a contradiction with the hypothesis. The same happens if we suppose $0<f(x)<f(-x)$. So $f(x)=f(-x), \: \forall x \in \mathbb{R}$.","['real-analysis', 'calculus']"
2700231,Glass marble on a plane,"There is a glass marble standing still on an absolutely smooth planar surface.
Assuming ideal conditions, the marble touches the surface on a single point O (O is a point on the plane).
We take away the marble and put it back so that it stands still on the same point O.
Is it possible to find an orientation, such that no single point on the marble surface is on the same 3d coordinates as before? My attempt:  If we rotate the marble by half a circle, on a great circle that passes through the contact point? And this is only by intuition.","['spheres', 'geometry']"
2700391,Evaluating the limit ${\lim_\limits{x\to0^+}\frac{e^x-\cos(\lambda \sqrt x)}{\sqrt {1+\sin(\lambda x)}-1}}$,"Evaluate $$\lim_\limits{x\to0^+}\frac{e^x-\cos(\lambda \sqrt x)}{\sqrt {1+\sin(\lambda x)}-1}=(*)$$ My attempt: I have used Taylor expansion of $e^x, \ \cos x, \ \sin x:$ $$(*)=\lim_\limits{x\to0^+}\frac{1+x-1+\frac{\lambda^2 x}2+ o(x)}{\sqrt {1+\lambda x+o(x)}-1}=\lim_\limits{x\to0^+}\frac{(x+\frac{\lambda^2 x}2)(\sqrt{1+\lambda x+o(x)}+1)}{\lambda x}=\\ =\left(\frac1{\lambda}+\frac{\lambda}2\right)\lim_\limits{x\to0^+}\frac{\sqrt{1+\lambda x}+1}{\lambda x}$$ $\left(\dfrac1{\lambda}+\dfrac{\lambda}2\right)$ is the result written on my textbook, but there seems to be a typo. Thanks in advance P.S. the exercise comes from Calculus Problems , $8.20$ page $144$ EDIT: Actually the last step should have been:
$$\left(\frac1{\lambda}+\frac{\lambda}2\right)\lim_\limits{x\to0^+}\sqrt{1+\lambda x}+1=\frac2{\lambda}+\lambda$$","['real-analysis', 'taylor-expansion', 'calculus', 'limits']"
