question_id,title,body,tags
2222495,Approaching to a number and limit,"Consider $f(x)$ function . We want to calculate $\lim_{x \to 3}f(x)$. So for left limit , we approach to $3$ and then compute $f(2.9) , f(2.99) , f(2.999)$ and so on . Now there is a weird thing . It is obvious that $2.9999.... = 3$ and also when we are talking about limit , point isn't important . In this case we don't take care about $f(3)$ but when we approach to $3$ infinitely , we get $3$ as $2.9999.... = 3$ ! . I'm very confused about these two concepts .",['limits']
2222513,Find the number of positive roots of equation $x^{x+1}=(x+1)^x$,Find the number of positive roots of equation $$x^{x+1}=(x+1)^x$$ My work so far: $$\ln x^{x+1}=\ln (x+1)^x$$ $$(x+1)\ln x=x\ln (x+1)$$ $$\frac{\ln x}{x}=\frac{\ln(x+1)}{x+1}$$ Let $f(t)=\frac{\ln t}{t}$ . $$f'(t)=\frac{1-\ln t}{t^2}$$ Answer: 1(one),"['logarithms', 'functions']"
2222541,Surface gradient and curvature,"I am trying to get familiar with the surface gradient operator, i.e. $\nabla_S = (I-\mathbf{n}\mathbf{n})\cdot \nabla$, where $\mathbf{n}$ is the unit normal to the surface and $I$ the identity tensor. I have seen in a paper that, for an axysimmetric surface, the mean curvature is $\kappa = \nabla \cdot\mathbf{n} = \nabla_S \cdot \mathbf{n}$, however I cannot get the latter. We can define the surface as $f = r - a(z)$, thus $\mathbf{n} = \nabla \, f / ||\nabla \,f|| = 1/\sqrt{1+(a')^2} \, \mathbf{e_r} -a'/\sqrt{1+(a')^2} \, \mathbf{e_z} = (1/\sqrt{1+(a')^2},0,-a'/\sqrt{1+(a')^2})$, and: $\kappa = \nabla \cdot \mathbf{n} = \dfrac{1}{a\sqrt{1+(a')^2}} - \dfrac{a''}{[1+(a')^2]^{3/2}}$ However I do not obtain the same result with the surface operator, $\nabla_S = (I-\mathbf{n}\mathbf{n})\cdot \nabla = \left(\frac{(a')^2}{1+(a')^2}\frac{\partial}{\partial r} + \frac{a'}{1+(a')^2}\frac{\partial }{\partial z},0,\frac{a'}{1+(a')^2} \frac{\partial}{\partial r} + \frac{1}{1+(a')^2} \frac{\partial}{\partial z}\right)$, $\nabla_S \cdot \mathbf{n} = - \dfrac{a''}{[1+(a')^2]^{3/2}}$, which is just the axial contirbution of the mean curvature. Is this correct and the paper is wrong? Or am I making a mistake in the procedure?","['curvature', 'differential-geometry', 'surfaces']"
2222582,"$f \in H(\mathbb C) $ s.t. restricted to any strip of finite width ( including straight lines ) , $f(z) \to 0$ as $ z \to \infty$ ; is $f$ constant?","Let $f:\mathbb C \to \mathbb C$ be an entire function . It is known that it is possible to have non-constant $f$ with the property that $f(z)\to 0$ as $z \to \infty$ restricted on any straight line . My question is : Does there exist a non-constant entire function $f:\mathbb C \to \mathbb C$ such that when restricted to any strip of finite width ( zero width hence just straight lines are allowed also ) , $f(z) \to 0$ as $ z \to \infty$ ?","['complex-analysis', 'holomorphic-functions', 'entire-functions']"
2222589,Is $\frac{dy}{dx} = \frac{d(y+c)}{dx}$?,"Is the following true? $$\frac{dy}{dx} = \frac{d(y+c)}{dx}$$ where $c$ is an arbitrary real constant. I believe it is true, and my reasoning goes like this: $dy$ is an infinitesimal, so the addition of another constant would still be an infinitesimal. I do not know if my reasoning is correct. Do note that I'm not familiar with epilson delta and university calculus. I would appreciate it if someone could explain the above simply. EDIT: I couldn't see why $d(y+c)= dy+dc$. What is $d$? Is it a number, or a function?","['derivatives', 'calculus']"
2222618,For which finite groups $G$ is $K = \{g^2 \mid g \in G\}$ a subgroup,"This question was inspired by this question . It is clear that all abelian groups satisfy this property (let us call it the K-property). Also if two groups $G$ and $H$ satisfy the K-property then so does $G \times H$. All dihedral groups also possess this property, as do the symmetric groups $S_3, S_4,$ and $S_5$. From $S_6$ on this is not the case anymore. An interesting note is that if $G$ has the $K$-property then $G/K$ is the direct product of copies of $C_2$. So candidates are to be sought among the extensions of such direct products but with what? Some simple groups work like $C_2 \ltimes PSL(3,2)$, produced with the automorphism of matrices $(M^T)^{-1}$. Is there a way to classify those groups? Edit Thanks to a comment from j.p. we can add the groups of odd order to the list. Moreover they possess the stronger property that $K = G$. Indeed, let $x \in G$ then $x$ has odd order $p$ so we have for $y = x^{\frac{p+1}{2}}$ that $y^2 = x$.","['finite-groups', 'group-theory']"
2222644,Distribution of primes of 4n+1 and 4n+3 forms,Can we give a $k$ such that the number of primes of the form $4n+1$ (upto m) will always exceed primes of the form $4n+3$ $\forall m>k$ or vice versa... $4n+3$ will always exceed primes of the form $4n+1$ . Basically what I want is a stage where number of primes of one form outnumbers number of primes of the other form and it never retreats from there?,"['number-theory', 'elementary-number-theory']"
2222685,Theorem 5.5 in Baby Rudin: Do we need the continuity of $f$ on the entire interval?,"Here is Theorem 5.5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is continuous on $[a, b]$, $f^\prime(x)$ exists at some point $x \in [a, b]$, $g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the point $f(x)$. If $$h(t) = g \left( f(t) \right) \ \ \ (a \leq t \leq b), $$ then $h$ is differentiable at $x$, and 
  $$\tag{3} h^\prime(x) = g^\prime \left( f(x) \right) f^\prime(x).$$ And, here is Rudin's proof. Let $y = f(x)$. By the definition of the derivative, we have 
  $$ \tag{4} f(t) - f(x) = (t-x) \left[ f^\prime(x) + u(t) \right], $$
  $$ \tag{5} g(s) - g(y) = (s-y) \left[ g^\prime(y) + v(s) \right], $$
  where $t \in [a, b]$, $s \in I$, and $u(t) \to 0$ as $t \to x$, $v(s) \to 0$ as $s \to y$. Let $s = f(t)$. Using first (5) and then (4), we obtain 
  $$ 
\begin{align}
h(t) - h(x) &= g\left( f(t) \right) - g\left( f(x) \right) \\ 
&= \left[ f(t) - f(x) \right] \cdot \left[ g^\prime(y) + v(s) \right] \\ 
&= (t-x) \cdot \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right],
\end{align}
$$
  or, if $t \neq x$, 
  $$\tag{6} \frac{h(t) - h(x) }{t-x} = \left[ f^\prime(x) + u(t) \right] \cdot \left[ g^\prime(y) + v(s) \right]. $$
  Letting $t \to x$, we see that $s \to y$, by the continuity of $f$, so that the right side of (6) tends to $g^\prime(y) f^\prime(x)$, which gives (3). Now my question is, is the continuity of $f$ on the entire interval $[a, b]$ essential in this theorem as Rudin has stated and proved it? Or, is it sufficient to just assume the continuity of $f$ at the point $x \in [a, b]$?","['derivatives', 'real-analysis', 'chain-rule', 'calculus', 'analysis']"
2222745,Counterexample to the chain-rule,"I made the following observation Let $f(t):=\left(\begin{matrix} 0 &e^{it} \\ e^{-it} & 0 \end{matrix}\right),$ then $f(t)^2= \operatorname{id}$.
Thus, we have $\frac{d}{dt}f(t)^2= \frac{d}{dt}\operatorname{id}=0.$
On the other hand yields the chain-rule
$$\frac{d}{dt}f(t)^2= 2 f(t)f'(t)=0.$$ However, $f(t)$ and $f'(t)$ are both matrices with full-rank. So something is very wrong here, no? Can anybody explain to me what just happened?","['real-analysis', 'matrices', 'matrix-calculus', 'calculus', 'multivariable-calculus']"
2222763,Theory question: How to use Mean Absolute Error properly in a log scaled linear regression,"First of all, I had a look here and in a couple of other questions: I couldn't find what I am looking for. So my question is purely theoretical (although I have an example by my hands). Suppose I have some data $(x_i,y_i)$ for $i=1,..,n$. 
Suppose I fit the following models with IID $\epsilon_i \sim N(0, \sigma^2)$ for $i=1,..,n$ $M_1: \log(y_i)= \beta_0+\beta_1x_i+\epsilon_i$ $M_2: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\epsilon_i$ $M_3: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i$ Now I want to see which of these models is better, so I use the following (maybe weird, but stay with me) method, to evaluate their ""predictive powers"": Use $(x_i, \log(y_i))$ for $i=1,..,\frac{n}{2}$, to fit $M_1, M_2, M_3$ respectively. Now use the fitted model (so $M_1, M_2,M_3$ respectively), to predict $y_i$'s using the $x_i$'s from the remaining $\frac{n}{2}$ data , so from $i = \frac{n}{2}+1, .., n$ (careful, predict $y_i$ not $\log(y_i)$) Use MAE or Mean Absolute Error ( here ) $MAE = \frac{1}{\frac{n}{2}}\sum_{i=\frac{n}{2}+1}^{n}|y_i-\hat{y}_i|$, being careful that $\hat{y}_i$ is in the original scale of values! So now my question: If I do point $1.$ and I fit the three models (hence obtaining estimates for the parameters, their standard errors etc..) and then use these parameters (respectively of course!) to predict the responses of the other $x_i$'s: Will I be predicting $\log(y_i)$'s right? And this is true...  Is it also true that in order to get $\hat{y}_i$'s , instead of $\widehat{\log{(y)}}_i$, I should just take the exponential of those terms? So in general, is it true $\hat{y}_i = e^{\widehat{\log{(y)}}_i}$? Once I find the three MAE's, how do I judge the models? Should I be looking for the one with smaller MAE? EDIT For example suppose I have $1000$ data points. I use the first $500$ to fit model $M_1$. Once I've fitted it, I can predict new values. Hence I predict the new responses of the other $500$ $x_i$'s left. of course, the prediction will be given in logarithmic scale. But I want to calculate MAE on the normal scale. This is the context of my question, of course I would do this procedure for all the three models and compare the MAEs.","['regression', 'mathematical-modeling', 'statistics', 'transformation']"
2222770,$4$-digit numbers whose digit sum equal $10$?,"How many $4$ -digit numbers are there whose digit sum equals $10$ ? For example, $2017$ is such a $4$ -digit number. I got this from a $10$ th grade math competition review. I asked my algebra- $2$ teacher and she doesn't know how to solve it.",['combinatorics']
2222773,Finding surface integral of a vector field over quarter of a cylinder,"Currently I am studying vector calculus at my university, and I came across a question that I was having problem in solving. The question is this Question Evaluate $$\iint\limits_S \vec{A} \cdot \hat{n}\,dS$$
over the entire surface $S$ of the region bounded by the cylinder $x^2 +z^2 =9$, 
$x=0$, $y=0$, $y=8$, $z=0$, if $\vec{A}= 6z\,\hat{i}+(2x+y)\,\hat{j}-x\,\hat{k} $ The given answer is 18$\pi$ .","['multivariable-calculus', 'surface-integrals', 'divergence-operator', 'vector-analysis']"
2222780,How do we prove in general that $\lim_{t \to x} {t^a} = x^a$,"How do we prove that for $a \in \mathbb{R}$,  $\lim_{t \to x} {t^a} = x^a$ in general using the epsilon/delta definition? My friend and I just spent an hour showing it for $t^{1/3}$, and the proof was very reliant on specific factoring and bounding a polynomial which I can see becoming highly nontrivial.","['real-analysis', 'limits']"
2222804,Union of fat Cantor sets?,"A question came up asking me to find two disjoint sets $A, B$ such that $[0, 1] = A \cup B$, $A$ is meager and $m(B) = 0$. My thought was the following: Let $\mathcal{C}_k$ denote the fat Cantor set obtained, starting with $[0, 1]$, by removing the middle open interval of length $(1/k)^n$ for each $n^{th}$ iteration, ad infinitum. Each fat Cantor set will be nowhere dense, and so the union $A = \bigcup_{k=4}^{\infty} \mathcal{C}_k$ is clearly meager, but does it have full measure? (in which case, $B = A^c$ would work)",['real-analysis']
2222838,Help with using a Cauchy criterion for Riemann Integrability to show that a continuous function is Riemann Integrable,"Let $f:[a,b]\rightarrow\mathbb{R}$.  A tagged partition, $\mathcal{P}$ of $[a,b]$ is a set of ordered pairs defined as $$\mathcal{P}:=\{([x_{k−1},x_k]),t_k)\}^n_{k=0},$$ where $a=x_0<...<x_n=b$ and the ""tags"" $t_k∈[x_{k−1},x_k]$, where $\mathbb{P}_{[a,b]}$ is the set of all tagged partitions over $[a,b]$. $$\|\mathcal{P}\|:=\sup\{x_k-x_{k-1}|1\leq k\leq n\}$$ is the mesh of the partition. The Riemann sum of $f$ over $[a,b]$ w.r.t $\mathcal{P}$ is defined as $$S(f,\mathcal{P}):=\sum\limits^n_{k=1}f(t_k)(x_k−x_{k−1})$$ and $f$ is said to be Riemann integrable with $\int_a^bf=L$ iff
$$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\Rightarrow |S(f,\mathcal{P})-L|<\epsilon\bigg)$$ A Cauchy integrability criterion which is a straightforward exercise says that $f$ Riemann integrable on $[a,b]$ iff
$$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})(\forall \mathcal{Q}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\wedge\|Q\|<\delta\Rightarrow |S(f,\mathcal{P})-S(f,\mathcal{Q})|<\epsilon\bigg).$$ My question is that how can we use the above definitions and Cauchy criterion to show that if $f$ is continuous on $[a,b]$ then $f$ is intgrable on $[a,b]$? If we use the Darboux definition for integrability which uses the upper and lower Riemann sums, then we have an analogous Cauachy integrabilityly condition with the difference of the upper and lower sums. With this and the fact that a continuous function is uniformly continuous on a compact set it is a very simple task to show that continuity is a sufficient condition for integrability.  However in this situation we do not have much flexibility as we need to take two different partitions so there is not much we can do with $|S(f,\mathcal{P})-S(f,\mathcal{Q})|$, and taking the common refinement partition of $\mathcal{P}\cup\mathcal{Q}$ leads nowhere.  So how do we in this setup up and not using upper and lower sums show that continuity implies integrability? So any input and assistance is greatly needed and appreciated.  Also if you have other proofs that don't use the Darboux definition of integrability it will be most welcomed. Thanks in advance","['continuity', 'real-analysis', 'riemann-integration', 'riemann-sum']"
2222840,Upper Bound on the Chromatic Number of a Graph with No Two Disjoint Odd Cycles,"Prove that if a graph does not have two disjoint odd cycles then χ(G) ≤ 5, where χ(G) denotes the minimum number of colors needed to color the vertices of G. χ(G) is the chromatic number of G. Intuitions: It is clear that any odd-cycle must have a chromatic number of 3. Each clique of the graph that has an odd-cycle must thus have a chromatic number of three, but I don't see how this helps the proof. On a related note: would it be easier to prove the contrapositive or use a proof by contradiction? For self-study.","['graph-theory', 'coloring']"
2222844,Inradius of regular simplex in $\mathbb{R}^n$,"Let $r_n$ denote the inradius of a regular $n$-simplex $\triangle^n$ in $\mathbb{R}^n$, and $a$ denote the uniform edge length. It is well-known that $r_1 = a \frac{1}{2} \\
 r_2 = a \frac{1}{6} \sqrt{3} \\
 r_3 = a \frac{1}{12} \sqrt{6}$ But how to generalize $r_n$ to arbitrary dimensions?","['simplex', 'polytopes', 'geometry']"
2222871,Differentiable function with conditions.,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ a differentiable function, with: $f(x+1)-f(x)=f'(x)$; exist $\lim_{x\rightarrow \infty }f'(x)=a\in \mathbb{R}$. Prove that $f(x)=ax+b$ I used Lagrange","['derivatives', 'real-analysis']"
2222896,Examples of Computations of the Divisor Class Group or Picard Group,"I'm looking for a handful of examples of varieties $X$ and their associated divisor class groups $\operatorname{Div} X$, or their Picard groups. I've got a bunch of relatively simple examples I've worked out myself. The problem is that because they are so simple, I feel like I do not have the tools to compute such groups in general, thus I'd like to see a small collection of examples that illustrate the general method of computing these groups. Here are the examples I've worked out. Example 1: Affine space. In this example, we know that every irreducible codimension $1$ subvariety $C$ of the affine space is defined by a single equation. So we can write $\mathfrak{U}_C = (F)$ for some polynomial $F \in k[x_1, ... , x_n]$. Thus $C = \operatorname{div} F$, and so every prime divisor is principle. But every divisor is a sum of prime divisors, and so every divisor $D = \sum C_i$ is the divisor $\operatorname{div}(\prod f_i)$. Thus every divisor is principle, and so the divisor class group is just the trivial group. Uninteresting. Example 2: Projective space. Here is is also true that every irreducible codimension $1$ subvariety $C$ is given by a single equation, now this equation is homogeneous. Let's again call it $F$.  Now if $F$ has degree $k$ then in affine chart $U_i$, then $\mathfrak{a}_C = (F/T_i^k)$ where the $T_i$ are the variables in projective space (this is the notation of Shafarevich, I'm not sure if it's standard). Now given an $f \in k(\mathbb{P}^n)$, we write $f = F/G$ for some forms $F$ and $G$ of equal degree. We can factor them into irreducibles, $F = \prod H_i^{k_i}$ and $G = \prod L_j^{m_j}$. Then for each $H_i$ or $L_j$, we can find an associated divisor $C_i$ and $D_j$ respectively, and then $\operatorname{div} f = \sum k_i C_i - \sum m_j D_j$. In this case, it is not hard to show that the principle divisors are exactly those for which the degree of the divisor is $0$. From this it will follow that the divisor class of group of projective space is $\mathbb{Z}$. This is at least a little more interesting. I also think I know how to treat products - at the very least I can do products of projective spaces - and I think a similar kind of reasoning will work in general. I guess the main problem I have when trying to compute the Picard group is that it seems really hard on more general varieties to figure out what the principle divisors are. Also, given only the equation(s) of the variety, it seems like the set of divisors itself is really huge, since I can probably drum up any number of codimension $1$ subvarieties just by taking intersections with different hypersurfaces, or even just hyperplanes. So what would be really great is if someone could give me some strategies for how to work this group out, and maybe a few worked examples of slightly less trivial cases. I imagine that like most things in algebraic geometry, things get hairy with things more complicated than curves or surfaces with degrees more than $2$ or $3$, but even those would suffice for me. If someone has got some notes online containing some examples, that would also be just fine. Throughout my examples, I looked at just the divisor class group because the varieties I was looking at are non-singular. Singular examples are also OK!","['reference-request', 'examples-counterexamples', 'algebraic-geometry']"
2222898,Given $f(n)$ how to find the maximum value of $f(n^5)$?,"A friend of mine recently asked me this question. For a positive integer $n$, a function $f(n)$ is defined as: $f(n)=$ sum of digits in $n$. given $f(n)=5$ find the maximum value of $f(n^5)$. I tried solving this problem by putting random values, but my friend gave me a hint that the answer is greater than 100. Now I am completely lost because I couldn't find a single $n$ for which the value of $f(n^5)$ comes out to be greater than 100. Is there a proper way to solve it?",['number-theory']
2222976,Ito integral representation of cosine of Brownian motion and expected value,"I'm working through an exercise related to SDEs and I'm getting some conflicting results. Hoping someone can set me in the right direction. The problem: Let $X_T = \cos(B_T)$ (where $B$ is Brownian motion or Wiener process). Find the process $\mu$ such that $X_T = \mathbb{E}[X_T] + \int_0^T \mu_s \, dB_s$ and calculate $\mathbb{E}[X_T]$ . So, we can use a change of variable and consider that $X_T = e^{\frac{1}{2}t} \cos B_t$ and apply Ito: $dX_t = -e^{\frac{1}{2}t}\sin B_t \, dB_t \iff d(e^{\frac{1}{2}t}\cos B_t) = -e^{\frac{1}{2}t} \sin B_t \, dB_t$ , or $e^{\frac{1}{2}T} \cos B_t = \int_0^T-e^{\frac{1}{2}t} \sin B_t \, dB_t \iff \cos(B_t) = \int_0^T-e^{\frac{1}{2}(t-T)} \sin B_t \, dB_t$ So, we have shown $\cos(B_t)$ is equivalent to just a stochastic integral, that is, it is a martingale with no drift and thus no expectation. But, I've been searching around and it seems that a direct expectation on $\cos(B_t)$ yields $e^{\frac{-t}{2}}$ ?","['brownian-motion', 'probability-theory', 'stochastic-calculus', 'stochastic-integrals']"
2223020,Bounded stopping times and martingales,"I am trying to show that if for every bounded stopping time $\tau$ it holds that $E(X_{\tau})=E(X_{0})$ then ${X_{n}}$ is a discrete time martingale. I have a few questions on my ideas on solving this. First I consider a set $A \in F_{n}.$  I define $\tau= n-1$ for $\omega \in A$ and $\tau= n$ for $\omega \in A^{c}.$  I want to show that this definition of $\tau$ is a stopping time; that is $\{ \tau \le t \}  \in F_{t}$ for all $t$.  I am not quite sure if I know how to show this correctly:  $\{ \tau \le t \}=(A\cap\{n-1 \le t\})\cup (A^c\cap\{n \le t\}) \in F_{t}.$  I am not sure if I formulated the algebra correctly here.  Any help would be appreciated. If I can show that $\tau$ is a stopping time then I know I can show $E(X_{n-1}1_{A})=E(X_{n}1_{A}),$ where $1_{A}$ is the indicator function for the set A.  Is this enough to conclude $X_{n}$ is a martingale.  Why?","['probability-theory', 'martingales', 'stopping-times']"
2223100,Which set is greater in cardinality?,"Which set is greater in cardinality, the set of all functions from $\mathbb R$(the real numbers) into $\mathbb N$(the natural numbers) or the set of all functions from $\mathbb N$ into $\mathbb R$? I have a feeling that the set of all functions from $\mathbb R$ into $\mathbb N$ is equinumerous to the power set of $\mathbb R$ and that the set of all functions from $\mathbb N$ into $\mathbb R$ is equinumerous to the set $\mathbb R$. Can someone please provide me an answer with proof?","['cardinals', 'elementary-set-theory']"
2223101,A norm making $M_n(A)$ a $C^*$-algebra when $A$ is a $C^*$-algebra,"Show that there is an unique norm on $M_n(A)$ making it a $C^*$-algebra, where $A$ is a $C^*$-algebra itself. It is enough to show this when $A$ is a concrete C^*-algebra. Suppose we are able to find a norm $\|.\|_1$ for such a case. Because for any $C^*$-algebra $B$,  there is an isometric isomorphism $\pi$ from $B$ to a concrete $C^*$-algebra. Then for $X \in M_n(B)$, we define $\|X\|=\|\pi(X)\|_1$, where
$$\pi(X)=\begin{bmatrix}                         \pi(b_{11})&\pi(b_{12})&\ldots&\pi(b_{1n})\\ \vdots&\vdots& \vdots &\vdots\\ \pi(b_{n1})&\pi(b_{n2})&\ldots&\pi(b_{nn})\\ \end{bmatrix}$$ when 
$$X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\                                                   \end{bmatrix}$$ Then $$\|X^*X\|=\|\pi(X^*X)\|_1=\|(\pi(X))^*\pi(X)\|_1=\|\pi(X)\|_1^2=\|X\|^2$$ So the problem now boils down to finding a norm on $M_n(A$) when $A$ is a concrete C^*-algebra.  Let $X \in M_n(A)$. Suppose that $X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\ \end{bmatrix}$ Then for $$a=(a_1,a_2,\ldots,a_n) \in H^n,
Xa=(\sum_{j=1}^n b_{1j}a_j,\ldots,\sum_{j=1}^nb_{nj}a_j)$$ Naturally I think
 $$\|Xa\|^2=\langle \sum_{j=1}^nb_{1j}a_j, \sum_{j=1}^nb_{1j}a_j \rangle + \ldots + \langle \sum_{j=1}^nb_{nj}a_j, \sum_{j=1}^nb_{nj}a_j \rangle=\sum_{i=1}^n \left\|\sum_{j=1}^nb_{ij}a_j\right\|^2$$
So I should define
$$\|X\|=\sup_{\|a\| \le 1}\|Xa\|$$ where
$$\|a\|=\left(\sum_{i=1}^n\|a_i\|^2\right)^{1/2}$$ I am not sure. Is there any other norm? 
Thanks for the help!!","['functional-analysis', 'c-star-algebras', 'operator-algebras', 'operator-theory']"
2223108,"$f:I\to \mathbb{R}$ is a differentiable path. If $a\in I$ is an accumulation point of $f^{-1}(v)$, then $f'(a) = 0$","I need to show that: Let $f:I\to \mathbb{R}^n$ be a differentiable path. If $a\in I$ is an
  accumulation point of the set $f^{-1}(v)$ for some $v\in
 \mathbb{R}^n$, then $f'(a) = 0$ *ps: for a differentiable path we mean a function $f:I\subset \mathbb{R}\to \mathbb{R}^n$ such that its cordinates are differentiable If for a given $v$, it's set $f^{-1}(v)$ has an accumulation point, it means that there is more than one point $a$ such that $f(a) = v$, right? Because for definition, an accumulation point $a$ is a point $a$ not necessarily in $I$ (but here it's in $I$) such that every ball centered in $a$ also touches another point of $I$. So I imagine that there is at least one more point different from $a$ such that $f(a) = v$, and this point, or points, are near $a$. I know that since they're infinitely close, the derivative at point $a$ would be $0$ because it's like the function were constant at that point, but I don't know how to write it. Could somebody help me?","['derivatives', 'real-analysis', 'calculus']"
2223113,Is the limit of a finite sequence simply its last term?,"I haven't dealt with convergence of finite sequences yet and my textbook doesn't say much about this. Using the definition of convergence, I was able to show that the limit of a finite sequence is its last term. I don't know if this is correct. Can you please let me know? Let $\{a_n \}_{n \in N}$ be a finite sequence where $N \subset \mathbb{N}$. Denote the number of elements in $N$ by $\bar{N}$. Now we know that for all $\epsilon > 0, \ \exists \ \bar{N} \in \mathbb{N}$ such that if $ n \geq \bar{N}, \ \mid a_\bar{N}-a_n \mid<\epsilon$. But this is the definition of a sequence that converges to $a_\bar{N}$. So does this mean that the limit of a finite sequence is simply the last element of the sequence?","['real-analysis', 'sequences-and-series', 'limits']"
2223118,How many sequences of $L$s and $R$s of length $n$ have no consecutive subsequences where $|\#L -\#R| > k$?,"This question popped up in discussion with a colleague, and I'm anxious to take a crack at it. Given some sequence of $L$s and $R$s, how many such sequences of length $n$ have no consecutive subsequences such that difference between the amount of $L$s and $R$s is more than a given integer $k$? A consecutive subsequence of some sequence $a_{n}$ is a subsequence $a_{n_{k}}$ such that $\forall k, n_{k+1}-n_{k} =1$. Example: $n = 10,k = 2$.
$RRLRRLLRLR$. This sequence does not satisfy the condition, because the consecutive subsequence $RRLRR$ has $4$ $R$s and $1$ $L$, resulting in a difference of $3$. Considering the sequence $RLRRLLRLR$, however, when $n = 9$ and $k=2$, one sees that the sequence satisfies the condition as no consecutive subsequence has a difference of more than $2$. My question is to solve this for general $n,k$. I've tried some work with recursive relations, but I keep running into roadblocks. I don't know of any other simpler methods to solve this, but that's why I'm asking here!","['combinatorics', 'sequences-and-series']"
2223146,Generalizations of the Robbins lemma and Gaussian integration by parts,"The Robbins lemma, named after Herbert Robbins, says that if $X\sim\operatorname{Poisson}(\lambda)$ and $g$ is a function for which $\operatorname{E}(|X g(X)|) < \infty,$ then $$\operatorname{E}(Xg(X)) = \lambda \operatorname{E}(g(X+1)).$$ ""Gaussian integration by parts"" is an identity that says that under suitable assumptions about the function $g$, if $X\sim N(0,\sigma^2),$ then $$ \operatorname{E}(Xg(X)) = \sigma^2\operatorname{E} (g\,'(X)). $$ Both of these propositions are used in empirical Bayes methods. Both of these are of the form $$ \operatorname{E}(Xg(X)) = \operatorname{var}(X) \cdot \operatorname{E} ((Tg)(X)) $$
where $T$ is a linear operator on functions $g$. QUESTION: Might there be, for each linear operator $T$, some probability distribution for which this holds? And might all of these be useful in empirical Bayes methods?","['functional-analysis', 'empirical-bayes', 'probability-distributions']"
2223148,"If there's a line $L\subset \mathbb{R}^n$ and a sequence $t_k\to a$ where $f(t_k)\in L$, then $L$ is the tangent line at $f(a)$","Let $f:I\to\mathbb{R}^n$ be a differentiable path, with $f'(a)\neq 0$
  for some $a\in I$. If there is a line $L\subset \mathbb{R}^n$ and a
  sequence of distinct numbers such that $t_k\to a$ such that $f(t_k)\in
 L$, then the line $L$ is the tangent line to $f$ at the point $f(a)$ First, the tangent line $L$ at the point $a$ is just the line that has 'slope' $f'(a)$ and passes at the point $f(a)$, right? So I need to prove that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$
? Intuitively, I see that $t_k\to a$ implying $f(t_k)\in L$ means that, no matter which side we approximate, if we're getting closer and closer to $a$, then $f(t_k)$ is in $L$. I guess the other definition of limit would help me better: $$L = \lim_{t\to a}\frac{f(t)-f(a)}{t-a}$$ But how to show that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$ implies in the limit definition above?","['multivariable-calculus', 'real-analysis', 'calculus', 'derivatives']"
2223191,"Is a Function From $X$ ""into""$Y$ Injective?","I am working my way through Naive Set Theory by Halmos, and I am trying to solve the exercise: Suppose that $f$ is a mapping from $X$ into $Y$ and $g$ is a mapping from $Y$ into $X$. Prove that there exist subsets $A$ and $B$ of $X$ and
  $Y$ respectively, such that $f(A) = B$ and $g(Y - B) = X-A$. This result can be used to give a proof of the Schroder-Bernstein theorem
  that looks quite different from the one above. Given the context, it seems that saying $f$ maps $X$ ""into"" $Y$ implies injectivity. However, I have read conflicting opinions on whether ""into"" implies injectivity or whether it just means that $Y$ is the co-domain of $f$. Was ""into"" common slang for an injection in older texts?","['elementary-set-theory', 'terminology', 'functions']"
2223193,Find continuous function $f: \mathbb R \to \mathbb R$ that is differentiable everywhere except $ \forall c \in \mathbb Z$,"Find continuous function $f: \mathbb R \to \mathbb R$ that is differentiable everywhere except $\forall c \in \mathbb Z$ Attempt: Let $$f(x) = \sqrt{1-\cos(2 \pi x)}$$
Then $f: \mathbb R \to \mathbb R$ and is continuous everywhere. $$f'(x) = \frac{\pi \sin(2 \pi x)}{\sqrt{1 - \cos(2 \pi x)}}$$ This is undefined $\forall c \in \mathbb Z$ However, since this is a case of $\frac00$, it seems necessary to me to observe the limiting behavior before I conclude it's undefined for all integers. A quick test on Mathematica showed that the left hand limit is $-\sqrt2 \pi$ and right hand limit is $\sqrt2 \pi$, so the limit does not exist. This should validate my function, however I'm still a little unsatisfied. First, is the function a valid candidate? second, how would I go about calculating that limit if so?","['derivatives', 'continuity', 'limits']"
2223213,How to apply the Empirical Rule to find the percentage?,"So I am working on this question but I don't know how to apply the Empirical Rule. Question: A Cornell University researcher measured the mouth volumes of a large number of men and women and reported that the distribution of the mouth volumes for men is approximately bell-shaped with a mean of 66 cc and a standard deviation of 17 cc. Moreover, the distribution of the mouth volumes for women is also approximately bell-shaped with a mean of 54 cc and a standard deviation of 14.5 cc. a) Applying the Empirical Rule, what percentage of man has mouth volume
over 100 cc or less than 49 cc? Now this is what I did. 100-66=34 which is 2(17). Thus, 100 is 2 standard deviation to the right of the mean. 66-49=17 which is 1(17). Thus, 17 is 1 standard deviation below the mean From here, I don't know which formula to apply to get the percentage that I did for this question. So can someone help me solve this. Thank you.","['means', 'statistics', 'standard-deviation', 'confidence-interval']"
2223231,Integrals of Infinitely Iterated Functions,"As a curiosity, I was looking at functions such as $y = x^{x^{x^{.^{.^{.}}}}}$ and finding their derivatives. I realize this is quite easy. For this problem, we can write $y = x^y$ and use implicit differentiation, and essentially the same concept can be used for any infinitely iterated function like this. I then tried to integrate one of these and am finding it much more difficult. Is anything known about $\int x^{x^{x^{.^{.^{.}}}}}dx$ or any other functions involving some sort of infinite iteration? (like $\sqrt{x + \sqrt{x + ...}}$)?","['integration', 'calculus']"
2223267,How does $e^{-j\pi n}$ become $(-1)^n$,"For $$e^{-j\pi n}$$ How does this become 
$$(-1)^n$$ or is it actually 
$$(-1)^{-n}$$
I have checked on calculator and values are all the same when the same n value is used","['exponential-function', 'trigonometry']"
2223288,"""Converse"" of optional stopping theorem","Assume we are considering the discrete case. If $\{X_n\}_{n\in\mathbb{N}}$ is a martingale adapted to $F_n$ and $X_n\in L^1$ , then for any bounded stopping time $\tau$ , according to the optional stopping theorem (or optional sampling theorem), we will have that $E(X_\tau)=E(X_0)$ . I've seen some discussions on the condition such as the boundedness of the stopping time. However, now I am curious about that if we don't have the assumption that $X_n$ is a martingale does $E(X_\tau)=E(X_0)$ still holds for any bounded stopping time? In another word, does there exist any integrable random process $X_n$ adapted to $F_n$ which is not a martingale such that for any bounded stopping time $\tau$ it holds $E(X_\tau)=E(X_0)$ ? Or we can rephrase this as ""converse"" of OST: if integrable random process $X_n$ adapted to $F_n$ and for any bounded stopping time $\tau$ it holds $E(X_\tau)=E(X_0)$ , then $X_n$ is a martingale. Is this converse true or not?","['stochastic-processes', 'probability-theory', 'stopping-times', 'probability', 'martingales']"
2223334,Why I can't understand the derivative (with example). Can someone help me understand?,"The derivative of a function is said to be $\lim_{\Delta{x}\to 0} \frac{f(\Delta{x}+x)-f(x)}{\Delta{x}}$. This I believe I understand. But, I always have trouble believing something that is so theoretical without walking through a concrete example. So, I took the derivative of the function $f(x)=x^2+2x+4$ and I, of course, got $f'(x)=2x+2$. So, this says that for a marginal change in $x$, there will be a change in $y$ of $2(1)+2=4$. I evaluate the original function at $x=1$ and I find that $y=7$. Then I evaluate the original function at $x=2$ and I find that $y=12$. In this case $\frac{\Delta{y}}{\Delta{x}}=5$, not the $4$ predicted by our derivative function. Where is the mistake in my thinking? Can someone provide me with a concrete example of the derivative that will show me that it does work?","['derivatives', 'calculus']"
2223357,Variant of Baby Rudin Ch 3 Ex 11,"Suppose that $a_n > 0$. Then a part of a problem in Rudin asks if $\sum_n a_n = \infty$ implies that $\sum_n \frac{a_n}{1+na_n} = \infty$. This question is nicely answered here: Convergence/divergence of $\sum\frac{a_n}{1+na_n}$ when $a_n\geq0$ and $\sum a_n$ diverges But this solution is non-monotonic. My question is if there exists a monotonic sequence that makes this converge. I was trying to bound this below by something, to no avail. I then tried to subtract the summand by some $\frac{c_n}{n}$, where $c_n$ is bounded, but I was not able to get a difference that converged to imply divergence since $\sum_n \frac{c_n}{n} = \infty$. (I was trying $0 < c_n < M$, but I suppose that I should have tried $c_n$ bounded below since I want divergence of our modified harmonic series.) Obviously, such a method is doomed to fail since I am trying to prove that this always converges when obvious tries like $a_n = \frac{1}{n}$ makes our series diverge, so I have to creatively apply some sort of condition to make it converge (like $na_n \to 0$, which also doesn't work because of logs), but that did not work. So, I was wondering if there was any ideas that anyone had. Thanks. :)","['real-analysis', 'sequences-and-series']"
2223358,Sum converging to $\zeta(\tfrac12) + 2$,"In answering this question , I obtained the infinite sum $$\sum _{k=1}^{\infty } \frac{\left(\sqrt{k+1}-\sqrt{k}\right)^2}{\sqrt{k}}.$$
To prove convergence, we don't actually have to figure out what this sum is ; just that it is bounded above by, for instance, $\sum_{k=1}^\infty \frac1{4k^{3/2}}$. However, Mathematica tells me that this sum does actually have a relatively simple closed form: $$\sum _{k=1}^{\infty } \frac{\left(\sqrt{k+1}-\sqrt{k}\right)^2}{\sqrt{k}} = 2 + \zeta(\tfrac12).$$ If this answer had $\zeta(s)$ for $s>1$ in it, I would expect it to be possible to fiddle with the sum until the summation for $\zeta(s)$ popped out, but as it is, I'm lost. How can we obtain this result? Also, does it generalize in some way, for instance to an expression with $\sqrt[3]{k}$ and $\zeta(\frac13)$?","['riemann-zeta', 'sequences-and-series']"
2223400,How to evaluate this limit??,Evaluate: $$\ f(x)= \lim_{n\rightarrow \infty}\left( \dfrac{n^n(x+n)\left( x+\dfrac{n}{2}\right)\left( x+\dfrac{n}{3}\right)... \left( x+\dfrac{n}{n}\right)}{n!(x^2+n^2)\left( x^2+\dfrac{n^2}{4}\right)\left( x^2+\dfrac{n^2}{9}\right)...\left( x^2+\dfrac{n^2}{n^2}\right)}\right)^{\dfrac{x}{n}}$$ $x\in R^+$ Find the coordinates of the maxima of $f(x)$ .,"['derivatives', 'limits', 'calculus', 'integration', 'riemann-sum']"
2223412,A combinatorial solution to the tool maker problem,"I'm trying to verify the claim presented in this question . Let's assume that we are interested in the following probability in both cases (i.e. sorted piles(SP) and messy heap(MH)): The probability of choosing $k$ stones from $N$ stones with which the tool maker can make $k$ different tools. For the (SP) case: The probability is actually one. Because we have distinct piles of each stone's type. So, one can pick one stone from each pile (Assume we have $k$ types of stone with the frequency of $n_k$ such that $\sum_{k} n_k = N$): $P_{SP}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = 1$ For the (MH) case: The selection space now is all of the stones, as there is no labelled type to any arbitrary stone. Hence: $P_{MH}(X) = \dfrac{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}}   {{N}\choose{k}}$ The claim is: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = k$ Now, one can use Stirling's Approximation leading to: $\lim_{N\to\infty} \dfrac{P_{SP}(X)}{P_{MH}(X)} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{{{n_1}\choose{1}}{{n_2}\choose{1}}...{{n_k}\choose{1}}} = \lim_{N\to\infty} \dfrac{\dfrac{N!}{k!(N-k)!}}{n_{1}*n_{2}*...*n_{k}}$ How should I proceed now?! Update: I myself solved the question: Let's assume that all frequencies are the same (for the sake of simplicity): $n_{1} = n_{2} = ... n_{k} = \psi$, therefore, $k\psi = N \Rightarrow \psi = \dfrac{N}{k}$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{n_{1}*n_{2}*...*n_{k}} = \lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}}$ Now by the following approximation, we have: ${{N}\choose{k}} \approx \dfrac{N^k}{k!}$ if $N\gg k$ $\lim_{N\to\infty} \dfrac{{N}\choose{k}}{(\dfrac{N}{k})^{k}} = \lim_{N\to\infty} \dfrac{\dfrac{N^k}{k!}}{\dfrac{N^k}{k^k}} = \dfrac{k^k}{k!}$ Let $\phi = \dfrac{k^k}{k!}$ We can use the following approximation here: $\log(k!) \approx k\log k - k$, So, $\log \phi = \log k^{k} - \log k! = k\log k - [k\log k -k] = k$ Finally, $\phi = exp(k)$ I should have reached to $k$, not $exp(k)$. What's wrong with my argument?!","['combinatorics', 'probability']"
2223429,"Evaluate the$\int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy$ and $\int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx$","Evaluate the following integrals:
  \begin{gather}
\int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy;\\
\int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx
\end{gather} I tried doing integration by parts, got nowhere as nothing seems to reduce. I tried converting $\cos(x)^2$ into $1-\sin(x)^2$ and u-sub, but ended up with $\sqrt{2-u^2}$. Again stuck. Know the hyperbolic equivalents $\sinh x = \frac{e^x-e^{-x}}{2}$, and evaluate the integral with $x=z^2$. Help with number 1? Also maybe links to how to deal with these types of integrals?","['multivariable-calculus', 'integration']"
2223463,Divisor Function Analytic Continuation,"I'm trying to define the divisor function $d(k)$ over a larger domain than the integers but the result I produce appears to converge nowhere, including the integer points i originally expected it to Here is my process: Consider: $$G(x) =  \sum_{k=1}^{\infty} \left[ \frac{1}{1-x^k} - 1 \right] = \sum_{i=1}^{\infty} d(i)x^i $$ Observe therefore that  $$ d(w) =\frac{1}{ \Gamma(w+1)}\frac{d^{(w)}G}{dx^{(w)}}[0]$$ Note: $$\frac{1}{1-x^k} = \frac{1}{k}\sum_{j=0}^{k-1} \frac{1}{1-e^{2i\pi(\frac{j}{k})}x}$$ $$ d(w) = \frac{1}{\Gamma(w+1)}\frac{d^{(w)}G}{dx^{(w)}} = \frac{1}{\Gamma(w+1)}\sum_{k=1}^{\infty} \left[ \frac{d^{(w)}}{dx^{(w)}} \left[ \frac{1}{k} \sum_{j=0}^{k-1} \frac{1}{1-e^{2i\pi(\frac{j}{k})}x} - 1  \right]\right][0]$$ If we let $w \in \mathbb{N}$ then this simplifies to $$ d(w) = \frac{1}{\Gamma(w+1)}\sum_{k=1}^{\infty} \left[ \frac{d^{(w)}}{dx^{(w)}} \left[ \frac{1}{k} \sum_{j=0}^{k-1} \frac{1}{1-e^{2i\pi(\frac{j}{k})}x}  \right]\right][0]= $$
$$\frac{1}{\Gamma(w+1)}\sum_{k=1}^{\infty} \left[  \frac{1}{k} \sum_{j=0}^{k-1} \frac{(-1)^w \Gamma(w+1)e^{2i\pi w(\frac{j}{k})}}{(1-e^{2i\pi(\frac{j}{k})}x)^w}  \right][0]$$ Cancelling out the gamma terms we have $$ d(w)= \sum_{k=1}^{\infty} \left[  \frac{1}{k} \sum_{j=0}^{k-1} \frac{(-1)^w e^{2i\pi w (\frac{j}{k})}}{(1-e^{2i\pi(\frac{j}{k})}x)^w}  \right][0]$$ Now setting $x=0$ yields $$ d(w) = \sum_{k=1}^{\infty} \left[  \frac{1}{k} \sum_{j=0}^{k-1} (-1)^w e^{2i\pi w (\frac{j}{k})} \right]$$ Factoring out common terms: $$ d(w) = \sum_{k=1}^{\infty} \left[ \frac{(-1)^w}{k} \sum_{j=0}^{k-1}   e^{2i\pi w (\frac{j}{k})} \right]$$ Applying Geometric Series formula: $$ d(w) = \sum_{k=1}^{\infty} \left[ \frac{(-1)^w}{k} \frac{1 - e^{2 i \pi w}}{1 - e^{\frac{2 wi\pi}{k}}} \right]$$ Which factors out nicely now as $$ d(w) = (-1)^w (1 - e^{2 wi \pi}) \sum_{k=1}^{\infty} \left[   \frac{1}{k- ke^{\frac{2 wi\pi}{k}}} \right]$$ But it's unclear that this is well defined as the denominator seems to tend to 0 as k tends to infinity qualitatively. Perhaps evaluating this as some type of limit is necessary","['number-theory', 'complex-analysis', 'divisor-counting-function']"
2223465,Values for $a$ and $b$ in $y=\cos(x)+a\cos(bx)$ such that every real value for $x$ has either a positive or $0$ value for $y$,If there is a function in the form $y=\cos(x)+a\cos(bx)$ do there exists real number values for $a$ and $b$ such that for every real number value for $x$ there is either a positive number value for $y$ or a $0$ value for $y$?,['trigonometry']
2223562,Definition of hyperfinite von Neumann algebras,"Let $M$ be a (not necessarily separable) von Neumann algebra. I am interested to understand the non separable case. In the book [1, page 49], the authors says that $M$ is hyperfinite if we can write $M=\overline{\cup_\alpha M_\alpha}^{w^*}$ where $(M_\alpha)$ is a net of finite dimensional subalgebras directed by inclusion. 1) Is it equivalent to the definition of [2, page 97] ? This definition says that $M$ is approximately finite dimensional if for any $x_1,x_2,\ldots,x_n \in M$ and any $\sigma$-strong* neighborhood $V$ of $0$ in $M$ there exists a finite dimensional $*$-subalgebra $N$ such that  $x_j \in N + V$ for any $j = 1,2,\ldots,n$. 2) In these definitions, is it to be assumed that $1_M \in M_\alpha$ and $1_M \in N$ (i.e. the subalgebras are unital subalgebras)? Is there any difference ? [1] Sinclair, Smith, Finite von Neumann algebras and MASAS [2] Takesaki, Theory of operator algebras 3","['functional-analysis', 'operator-algebras', 'von-neumann-algebras']"
2223665,"Does there exists a surjective group homomorphism from $(\mathbb{R},+)$ to $(\mathbb{Z},+)$?","Examine whether there exists any surjective group homomorphism (1) from $(\mathbb{Q}(\sqrt{2}),+)$ to $(\mathbb{Q},+)$ (2) from $(\mathbb{R},+)$ to $(\mathbb{Z},+)$ My try: (1) I think $\phi :\mathbb{Q}(\sqrt{2})\rightarrow \mathbb{Q}$ given by $\phi(a+b\sqrt 2)=a$ works well here. (2) I'm utterly clueless about this one. I tried many things always hitting a road block: I tried to figure of if $f:(\mathbb{R},+) \to (\mathbb{Z},+)$ such a homomorphism then how would a rational $\frac{a}{b} $ satisfying $f(\frac{a}{b}) =1$ look like. I eventually realized then it must be  $\frac{a}{b}=1$. Now that we have $f(1)=1$ if we can each $a\in(0,1)$ figure out $f(a)$ then as every $r\in\mathbb{R}$ can be written as $r=n+a$ where $n\in\mathbb{Z}$ and $a\in(0,1)$ and $f(n+a)=f(n)+f(a)=n+f(a)$ we will be done. I'm stuck here. Please help me out.","['alternative-proof', 'abstract-algebra', 'group-theory', 'proof-verification']"
2223702,"Let A and B are $2 \times 2$ matrices, where AB = BA, show that $\text{A B}^2 = \text{B}^2 \text{A}$","Let A and B are $2\times 2$ matrices, where $$\text{AB} = \text{BA}$$show that :
  $$\text{A B}^2 = \text{B}^2 \text{A}$$ Well, I assumed A and B are symmetric matrices, so $$ AB^2 = A^T B pow(2T) = (B^ 2A)^T = B^2T = A^T = B^2 A $$ I thought for long time but I didn't get the right solution, any expert can help please!","['matrices', 'transpose', 'matrix-equations', 'linear-algebra']"
2223725,Probability of Dice,"Q) If 20 fair dice are rolled, then the probability that the sum obtained is between 30 and 40 is 0.325. True or False? A) So far I have that $ \mu = \frac{7}{2}$ and $ \sigma^2 = \frac{35}{12} $. So for 20 rolls $ \mu = 20 \times \frac{7}{2} $ and $\sigma^2 = 20 \times \frac{35}{12}$. So using the continuity correction we have $ \frac{29.5-\mu}{\sigma}\leq z \leq \frac{40.5-\mu}{\sigma}$. However, this gives the probability equal to $0.0001$ using the normal distribution tables. I don't know if what I have done is right and the statement is false or if I have made a mistake somewhere. Thanks for the help!","['statistics', 'summation', 'probability', 'dice']"
2223773,How second-order differential equations do not violate causality? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I copy this question from physics stack exchang e The second order differential equations are time reversible. That means: they don't distinguish the time arrow direction. There is no reason for the time to flow forward. My professor told me that there are two solutions to such equations, one of which describes processes going forward and one backward in time. The ""backward"" solution violates causality, so we say that only the ""forward"" solution is physical and the other simply don't exist. Is this explanation correct? Has this anything to do with the fact we use imaginary numbers? And one more question: How is causality not violated?",['ordinary-differential-equations']
2223776,Staircase spectrum: is there a known solution for this problem?,"•   Let $A$ be a (real) dense (non-sparse) matrix with size $rp\times M$  ($M < rp$) with orthonormal columns (i.e. $A^TA = I_M$). •   Similarly, let $B$ be a dense matrix with size $rq\times N$  ($N < rq$) with orthonormal columns. •   Let $V=\bigoplus_{i=1}^qA.$ Equivalently, $V=I_q\bigotimes A,$ where $\bigoplus$ denotes the direct sum, and $\bigotimes$ denotes the Kronecker
product. •   Let $W=\bigoplus_{j=1}^pB.$ Equivalently, $W=I_p\bigotimes B.$ •   Let $R=V^TW,$ and therefore, 
$$
R=\big(\bigoplus_{i=1}^qA^T\big)\big(\bigoplus_{j=1}^pB\big)
$$
$$
=\big(I_q\bigotimes A^T\big)\big(I_p\bigotimes B\big) 
$$ •   Assume:  $rpq > qM + pN$ (so that the augmented matrix $\left[\begin{array}{c|c}V & W\end{array}\right]$ is a tall matrix). •   Note:  $p\neq q$, $p>1$ and $q>1$. Prove that the singular values of $R$ come in $n$-tuples, with
$$
n=\text{GCD}(p,q),
$$ where $\text{GCD}(p,q)$ is the greatest common divisor of $p$ and $q$. In other words, prove that a singular value of $R$ has
a multiplicity of $\text{GCD}(p,q)$. Note: I am ONLY interested in proving that the largest singular value of $R$ is distinct when $p$ and $q$ are relatively
prime ($\text{GCD}(p,q) = 1$). However, I think giving the larger picture of the behavior of the singular values of $R$ is far more interesting, and probably even helpful in proving the specific case I'm interested in. Please refer to the Matlab code below to verify that $n=\text{GCD}(p,q)$. Matlab code: clear all; close all;

r = 3; % integer >= 1

p = 12; q = 16; %17 % for the singular values of R to be distinct, p and q must be relatively prime integers and larger than 1 (i.e. GCD(p,q) = 1 and p > 1 and q > 1) 

M = 8; % # of columns of A
N = 14; % # of columns of B. M and N MUST satisfy 1 < M < rp, 1 < N < rq AND they must satisfy assumption that r*p*q/(q*M+p*N) > 1.

[A,~] = qr(rand(r*p,M),0); % size = rp X M (M < rp, A'A = I) % Note: you can orthonormalize (e.g. via Gram Schmidt or SVD) any arbitrary 
                       % (full rank) data matrix other than noise, I'm only using
                       % noise as a data matrix because it's conevniently easy to generate.
[B,~] = qr(rand(r*q,N),0); % size = rq X N (N < rq, B'B = I)

V = kron(eye(q),A); % size = q(rp) X qM
W = kron(eye(p),B); % size = p(rq) X pN

R = V'*W; % i.e. R = direct sum (over q) of (A') X direct sum (over p) of (B)

% assumption
r*p*q/(q*M+p*N) % check is greater than 1 % This assumption is required for the augmented matrix [V W] to be tall. 
gcd(p,q) % check for relative primeness of p and q. % This assumption is required for the singular values of R to be distinct.

figure;stem(svd(R));   
title 'note how the singular values of R come in GCD(p,q)-tuples'","['matrices', 'linear-algebra']"
2223794,Intuition behind the difference between $\frac{\partial f}{\partial t}$ and $\frac{df}{dt}$,"Let $f=f(x,y,t)$. Using the definition of the differential $df$, we can easily conclude that
$\frac{df}{dt} = \frac{\partial f}{\partial t} + (\vec{v} \cdot \vec{\nabla})f$
$\hspace{2mm} (1)$ So my question is: We refer to both $\frac{df}{dt}$ and
$\frac{\partial f}{\partial t}$ as a change of the quantity $f$ with respect to $t$. But what does each time derivative symbolize in eq$(1)$? How could I measure them? (I know the definition of partial derivative and the role of
$(\vec{v} \cdot \vec{\nabla})f$ in eq$(1)$. All I want is to understand the connection the $2$ derivatives in eq$(1)$.)","['derivatives', 'partial-derivative', 'soft-question', 'vector-analysis']"
2223798,A set of distinct numbers fulfiling a property,"If $x, y, z, u \in \Bbb{N}$ are distinct numbers, do any set of numbers exist to fulfil the following property
  $$x^y =y^z =z^u.$$ It is certainly true for only three numbers $x, y, z$. Is there any formula to find a set of these numbers?",['number-theory']
2223818,Computing a Galois Group by Reducing Mod P,"On page 274 of Lang's Algebra , he states the following theorem (paraphrasing): Let $f(x) \in \mathbb{Z}[x]$ be a monic polynomial. Let $p$ be a
  prime. Let $\bar{f} = f \bmod p$ be the polynomial obtained by
  reducing the coefficients mod $p$. Assume that $\bar f$ has no
  multiple roots in an algebraic closure of $\mathbb{F}_p$. Then there
  is an embedding of the Galois group of $\bar f$ into the Galois group
  of $f$. I'm confused about what this means; in particular, I don't know how exactly to interpret the phrase ""Galois group of $\bar f$."" Does it just mean the Galois group of $\bar f$ over $\mathbb{Q}$ where we forget that we ever reduced the coefficients mod $p$? Example: I think I can apply this theorem to compute the Galois group of $x^4 + 3x^2 - 3x - 2$ over $\mathbb{Q}$. Here's my argument. Please let me know if I'm applying the theorem correctly. Reducing $f$ mod 3 we get $x^4 - 2$, which has no multiple roots (since it is coprime with its derivative). The Galois group of $x^4 - 2$ over $\mathbb{Q}$ is the dihedral group with 8 elements. Reducing $f$ mod 2 we get $x^4 + x^3 - x = x(x^3 + x^2 - 1)$ which also has no multiple roots. The cubic has discriminant $-23$, which is not a square in $\mathbb{Q}$, so it has Galois group $S_3$. Let $G$ be the Galois group of $f$. We know $G$ embeds into $S_4$ since $f$ has degree 4. Now applying the theorem, both $S_3$ and $D_8$ embed into $G$, so $G$ has elements of order 3 and 4. They generate a subgroup of order at least 12, so $G$ must be $A_4$ or $S_4$. Since $D_8$ has order 8, it cannot embed into $A_4$, which has order 12, so $G = S_4$. Am I applying the theorem correctly?","['abstract-algebra', 'galois-theory', 'field-theory']"
2223884,Which trigonometric function to use?,Which function do I need to use to calculate the length of a side on an Isosceles triangle if I know $3$ angles and $1$ side? Much appreciated.,"['plane-geometry', 'trigonometry', 'triangles']"
2223887,Subspace of $\Bbb R^2$ with fundamental group $\Bbb Z \oplus \Bbb Z$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Does there exist a subspace of $\Bbb R^2$ with fundamental group $\Bbb Z \oplus \Bbb Z$?","['algebraic-topology', 'geometric-topology', 'general-topology']"
2223970,How to set up polar parametrization for the following double integral?,"If the mass per unit area of a surface is given by $\rho=xy $ find the mass
$\int\int_S xy\ dS$
if $S$ is the part of the cylinder $y^2+z^2=16$ which is in the first octant and contained within the cylinder $x^2+y^2=9$? First I set up parametrization as $x=x, y=4\cos\theta, z=4\sin\theta$. So $|r_x\times r_\theta|=\sqrt{(4\cos\theta)^2+(4\sin\theta)^2}=4$ Then $dS=4dxd\theta$. So $\int\int_S xy\ dS=\int^\frac{\pi}{2}_0\int^?_0 16x\cos\theta\ dxd\theta$. $($it's in first quadrant$)$ I am uncertain about the '?' part. I guessed $?=\sqrt{9-(4\cos\theta)^2}$ but it gives answer different from correct answer, which is $11.36$.","['multivariable-calculus', 'area', 'density-function']"
2223994,Corollary to Theorem 5.12 in Rudin's PMA: The derivative of a function differentiable on an interval cannot have any simple discontinuities [duplicate],"This question already has answers here : Discontinuities of the derivative of a differentiable function on closed interval (2 answers) Closed 1 year ago . Here is Theorem 5.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real differentiable function on $[a, b]$ and suppose $f^\prime(a) < \lambda < f^\prime(b)$. Then there is a point $x \in (a, b)$ such that $f^\prime(x) = \lambda$. A similar result holds of course if $f^\prime(a) > \lambda > f^\prime(b)$. And, here is the Corollary by Rudin to the above theorem. If $f$ is differentiable on $[a, b]$, then $f^\prime$ cannot have any simple discontinuities on $[a, b]$. Finally, here is Definition 4.26 in Baby Rudin, 3rd edition: Let $f$ be defined on $(a, b)$. If $f$ is discontinuous at a point $x$, and if $f(x+)$ and $f(x-)$ exist, then $f$ is said to have a discontinuity of the first kind , or a simple discontinuity , at $x$. Otherwise the discontinuity is said to be of the second kind . There are two ways in which a function can have a simple discontinuity: either $f(x+) \neq f(x-)$ [in which case the value $f(x)$ is immaterial], or $f(x+) = f(x-) \neq f(x)$. I've just found out that this very question has an answer at the following link. Discontinuities of the derivative of a differentiable function on closed interval However, I'm stuck on the following part of the answer. For the the second kind, say $f'(x-)<f'(x+)$ and pick $\lambda\in(f'(x-),f'(x+))$, and let $u<x<v$ with $u$, $v$ sufficiently close to $x$ so that $f'(z)<\lambda$ for $z\in[u,x)$, and $f'(z)>\lambda$ for $z\in(x,v]$. Why can we not have $f^\prime(x) = \lambda$ in this case (and thus fail to get our desired contradiction)? In order to demonstrate my understanding of the answer, I'll rephrase it, or rather expand upon it. Suppose $f$ is a real function which is differentiable on a closed interval $[a, b]$, and suppose $f^\prime$ has a simple discontinuity at a point $p \in [a, b]$. Then as both $f^\prime(p+)$ and $f^\prime(p-)$ exist, so we must have $p \in (a, b)$. Am I right? Now there are two possible cases, according to whether (i) $f^\prime(p-) = f^\prime(p+) \neq f^\prime(p)$ or (ii) $f^\prime(p-) \neq f^\prime(p+)$. Case (i): We can assume without loss of generality that $f^\prime(p-) = f^\prime(p+) < f^\prime(p)$. Let $\lambda$ be a real number such that 
  $$\lambda \in \left( \ \lim_{x \to p} f^\prime(x), \ f^\prime(p) \ \right);$$
  that is, 
  $$ \lim_{x \to p} f^\prime(x) < \lambda < f^\prime(p).$$
  Now let $\varepsilon$ be any real number such that 
  $$0 < \varepsilon <  \lambda - \lim_{x \to p} f^\prime(x).$$
  Then we can find a real number $\delta > 0$ such that 
  $$ (p-\delta, p+\delta) \subset (a, b), $$
  and 
  $$ \left\vert f^\prime(x) - \lim_{x \to p} f^\prime(x) \right\vert < \varepsilon$$
  for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$; 
  that is, 
  $$ \lim_{x \to p} f^\prime(x)  - \varepsilon < f^\prime(x) < \lim_{x \to p} f^\prime(x) + \varepsilon $$
  for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$. But 
  $$ \lim_{x \to p} f^\prime(x) + \varepsilon < \lambda.$$
  So we can conclude that 
  $$ f^\prime(x) < \lambda \ \mbox{ for all } \ x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}.$$
  Let us put 
  $$y \colon= p - \frac{\delta}{2}, \ \mbox{ and } \ z \colon= p + \frac{\delta}{2}.$$
  Then we can conclude that 
  $$ f^\prime(x) <  \lambda \ \mbox{ for all } \ x \in [y, p). \tag{1} $$
  Thus in particular, 
  $$f^\prime(y) < \lambda.$$ But we have assumed that 
  $$\lambda < f^\prime(p). $$ 
  So we must have a point $x \in (y, p)$ for which $f^\prime(x) = \lambda$, which contradicts (1) above. [We can of course take the argument forward with $z$ instead of $y$.] Am I right? Case (ii): We can assume without loss of generality that $f^\prime(p-) < f^\prime(p+)$. Let $\lambda$ be a real number such that 
  $$ f^\prime(p-) < \lambda < f^\prime(p+).$$
  Let's choose a real number $\varepsilon$ such that 
  $$0 < \varepsilon < \min \left\{ \ f^\prime(p+) - \lambda, \ \lambda - f^\prime(p-) \ \right\}. \tag{2} $$ 
  For this $\varepsilon$, we can find positive real numbers $\delta_1$ and $\delta_2$ such that 
  $$(p, p+\delta_1) \subset (a, b), \ \mbox{ and }  \ (p-\delta_2, p) \subset (a, b),$$
  and also 
  $$ \left\vert f^\prime(x) - f^\prime(p+) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1),$$
  and 
  $$ \left\vert f^\prime(x) - f^\prime(p-) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p).$$
  That is, 
  $$ f^\prime(p+) - \varepsilon < f^\prime(x) < f^\prime(p+) + \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1), \tag{3} $$
  and 
  $$ f^\prime(p-) - \varepsilon < f^\prime(x) < f^\prime(p-) + \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p). \tag{4}$$
  But from (2), we note that 
  $$f^\prime(p-) + \varepsilon < \lambda < f^\prime(p+) - \varepsilon. \tag{5} $$ 
  From (3) and (5) we conclude that 
  $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, p+\delta_1).$$
  And, from (4) and (5) we conclude that 
  $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in (p - \delta_2, p). $$ 
  Then if we put 
  $$y \colon= p-\frac{\delta_2}{2} \ \mbox{ and } \ z \colon= p + \frac{\delta_1}{2},$$
  then we conclude that 
  $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, z ], \tag{6} $$
  and 
  $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in [y, p). \tag{7}$$ 
  In particular, 
  $$f^\prime(y) < \lambda < f^\prime(z).$$
  So there is some point $x \in (y, z)$ such that $$f^\prime(x) = \lambda, $$ 
  which in the light of (6) and (7) implies that $$f^\prime(p) = \lambda.$$ Is my argument up to this point correct? If so, then where do we get our desired contradiction?","['continuity', 'real-analysis', 'calculus', 'analysis']"
2223998,An interesting pattern in the general solution of the $n$-th order ODE: $\frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0$.,"Context: My friend (The same one who gave me this ODE) originally challenged me to obtain the general solution to this ODE:
$$\frac{d^3 y}{dx^3}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{1}$$
Where $\alpha,\beta \in \mathbb{R}$. I could not figure out any substitution without the use of power series. I thus decided to play around with Wolfram|Alpha , and decided to generalize the order of the ODE, like this: $$\frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{2}$$ Upon increasing the value of $n$, I noticed an interesting pattern: When $n=3$, the solution is:
$$\small y(x)=\frac{\sqrt[3]{\alpha}\cdot c_2 x \cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{1}{3};\frac{2}{3},\frac{4}{3}; -\frac{x^3 \alpha}{9}\right)}{3^{2/3}}+c_1\cdot {_1F_2}\left(\frac{\beta}{3\alpha};\frac{1}{3};\frac{2}{3};-\frac{x^3\alpha}{9}\right)+\frac{\alpha^{2/3} c_3 x^2\cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{2}{3};\frac{4}{3},\frac{5}{3};-\frac{x^3 \alpha}{9}\right)}{\sqrt[3]{3}} \tag{3}$$
When $n=4$, the solution is:
$$\small y(x)=\frac{\sqrt[4]{\alpha}c_2 \cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{4};\frac{1}{2},\frac{3}{4},\frac{5}{4};-\frac{x^4\alpha}{64}\right)}{\sqrt{2}}+c_1\cdot {_1 F_3}\left(\frac{\beta}{4\alpha};\frac{1}{4},\frac{1}{2},\frac{3}{4};-\frac{x^4 \alpha}{64}\right)+\frac{\alpha^{3/4} c_4 x^3\cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{3}{4};\frac{5}{4},\frac{3}{2},\frac{7}{4};-\frac{x^4 \alpha}{64}\right)}{\sqrt{2}}+\sqrt{\alpha}c_3 x^2 {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{2};\frac{3}{4},\frac{5}{4},\frac{3}{2};-\frac{x^4 \alpha}{64}\right)$$
When $n=5$, the solution is:
$$\small y(x)=\frac{\sqrt[5]{\alpha} c_2 x \cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{1}{5};\frac{2}{5},\frac{3}{5},\frac{4}{5},\frac{6}{5};-\frac{x^5 \alpha}{625}\right)}{5^{4/5}}+c_1\cdot {_1F_4}\left(\frac{\beta}{5\alpha};\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};-\frac{x^5 \alpha}{625}\right)+\frac{\alpha^{4/5}c_5 x^4\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{4}{5};\frac{6}{5},\frac{7}{5},\frac{8}{5},\frac{9}{5};-\frac{x^5 \alpha}{625}\right)}{\sqrt[5]{5}}+\frac{\alpha^{3/5} c_4 x^3\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{3}{5};\frac{4}{5},\frac{6}{5},\frac{7}{5},\frac{8}{5};-\frac{x^5 \alpha}{625}\right)}{5^{2/5}}+\frac{\alpha^{2/5} c_3 x^2 {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{2}{5};\frac{3}{5},\frac{4}{5},\frac{6}{5},\frac{7}{5};-\frac{x^5 \alpha}{625}\right)}{5^{3/5}}$$
If anyone is interested, here are the solutions for $n=6$ , $n=7$ , $n=8$ and $n=9$ . I apologize if I mistyped one of the solutions. As we can see, there is a pattern. I would therefore like to find a reason for this pattern. Here, I attempt to explain it using a power series solution: We make the ansatz:
$$y=\sum_{k=0}^{\infty} A_k x^k$$
Thus, if we substitute this into our ODE on $(2)$, we obtain:
$$\sum_{k=n}^{\infty} k(k-1)(k-2)\cdots (k-n+1)A_{k}x^{k-n}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$
Shifting the first sum, we obtain:
$$\sum_{k=0}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$
Hence, we have:
$$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=1}^{\infty} A_k x^k=0$$
Thus, we obtain:
$$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} \left[(k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}+\alpha k A_k+\beta\cdot A_k\right]\cdot x^k=0$$
If the series is a solution, then all the coefficients must equal zero:
$$\begin{cases} A_n=-\frac{\beta}{n!}\cdot A_0 & k=0\\ A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k &  k>0, k\in \mathbb{N}\end{cases}$$
The system of equations is redundant, thus one may just solve the following recurrence relation:
$$\small A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k=-\frac{k!(\alpha k+\beta)}{(k+n)!}A_k=-\frac{\alpha k+\beta}{(k+1)_n} A_k \tag{4}$$
Where $(x)_n$ is the Pochhammer Symbol . The Hypergeometric function ${_1P_q}(a_1; b_1,\cdots,b_q;x)$ is defined as :
$${_1P_q}(a_1; b_1,\cdots,b_q;x)=\sum_{k=0}^{\infty} \frac{(a_1)_k}{(b_1)_k\cdots (b_q)_k}\cdot \frac{x^k}{k!}$$
Given the recurrence, this should follow the pattern for general values of $n\geq 3, n \in \mathbb{Z}^+$. The problem: However, I am not satisfied with this. I would prefer if one could use one (or several) substitution(s) which works for general values of $n$ and which does not involve a series solution. My idea was to reduce the differential equation on $(2)$ to Kummer's Equation to obtain these hypergeometric functions on their solutions:
$$z\frac{d^2 w}{dz^2}+(b-z)\frac{dw}{dz}-aw=0 \tag{5}$$
To do this, I've attempted to use the same substitution as done on my previous question :
$$\ln y=\ln{z}-\frac{(1+x)^2}{4}$$
However, the resulting ODE turns out to be even worse than it was previously (Even for $n=3$). The question: Is there any way we can make several substitutions such that we obtain the general solution for general values of $n$ where $n\geq 3, n \in \mathbb{Z^+}$? If this is too difficult or not possible, is there a way we can use several substitutions for the specific case $n=3$ to obtain the general solution? i.e. The same one, or in a similar form to the one on equation $(3)$. Thanks in advance.","['special-functions', 'hypergeometric-function', 'ordinary-differential-equations', 'calculus']"
2225002,"Banach limit, Hahn-Banach theorem","Let $l_{\mathbb{R}}^{\infty}$ be the linear space of all real-valued bounded sequences over $\mathbb{R}$ for $x=(x_n)_n$ define 
$$
p(x) = \lim_{n \to \infty} \sup \frac{x_1 + \dots x_n}{n} \\
W = \{x \in l_{\mathbb{R}}^{\infty} : \lim_{n\to \infty} \frac{x_1 + \dots + x_n}{n} \text{ exists } \}
$$ Prove that there is a linear functional LIM on $l_{\mathbb{R}}^{\infty}$ with the following properties a) $LIM(x_1,x_2,\dots) = LIM(x_2,x_3,\dots)$ b) $\lim \inf_{n \to \infty} x_n \leq LIM(x_1,x_2, \dots) \leq \lim \sup_{n \to \infty} x_n$ c) LIM  is continuous with $\|LIM\|=1$. Since $W$ is a linear subspace I defined a functional $\psi_0(x) = \lim_{n \to \infty} \frac{x_1 + \dots + x_n}{n}$ where $\psi_0(x) = p(x)$ on $W$. Then by Hahn-Banach there exits $\psi$ s.t $\psi(x) = \psi_0 (x)$ on $W$ and $\psi(x) \leq p(x)$ on $V$. But how can I continue?",['functional-analysis']
2225036,Solving an inverse problem with machine learning,"I am running up against a very tough inverse problem that I suspect might be solvable using machine learning. Here is the problem. Overview I am studying an object $X$ which, internally, is composed of two functions $f_1(x)$ and $f_2(x)$, $x \in [0,1]$. I want to know what $f_1$ and $f_2$ are, but I can't measure them directly. What I can measure is a list of numbers $\omega_1, \omega_2, \ldots, \omega_N$ which are determined by $f_1$ and $f_2$. Using these numbers $\vec \omega$, I want to infer $f_1$ and $f_2$ the best I can. The current approach When $N$ is very large and the measurement errors on each of the $\omega$'s is small, the following method works well. I have a model that I consider to be a good but not exact description of $X$. It differs in its internal $f_1$ and $f_2$'s, which causes differences in $\omega$. Additionally, there is an arbitrary polynomial difference in $\omega$ whose form is known but whose coefficients are unknown. Using the relative differences between the $\vec \omega$'s that come from the model and the $\vec \omega$'s that I observe in $X$, and also fitting for the polynomial difference in $\omega$, I can calculate the relative difference in $f_1$ and $f_2$ between the model and $X$ using the following system of equations: $$ \delta \omega_i = c_1(\delta\omega)^{-1} + c_2(\delta\omega)^3 + \int (K^{f_1}_i \cdot \delta f_1 + K^{f_2}_i \cdot \delta f_2) \; \text{d}x, \qquad i=1,\ldots,N$$
where $c_1$ and $c_2$ are constants that have to be determined (in the least squares sense) along with $\delta f_1$ and $\delta f_2$. The functions $K^{f_1}_i$ and $K^{f_2}_i$ are known functions called kernel functions that do not need to be estimated - they come for free from the model. Clearly this is an inverse problem because $\delta f_1$ and $\delta f_2$ are trapped in that integral. However, by representing $\delta f_1$ and $\delta f_2$ by basis functions, we can estimate the coefficients of the basis functions using normal techniques (e.g. regularized least squares) and then determine $f_1$ and $f_2$. A new approach? I am studying $X$'s for which $N$ is small and the measurement error on the $\omega$'s is large. The previous method fails to give reliable estimates, which I can test by using two models and pretending one of them is $X$. I am thinking that a machine learning approach might work here. In particular, I can calculate a large number of models and ask how their values of $\vec \omega$ correspond to $f_1$ and $f_2$'s. Then, I can put in the $\vec \omega$ that I observe and get out the real $f_1$ and $f_2$. That is, I will train a machine learning algorithm to predict $$f(\vec \omega) = [f_1(x_i), f_2(x_i)], \; x_i \in [0.01, 0.02, \ldots, 1.00]$$ based on a large number of simulations. The problem that I am coming up against is the arbitrary function of $\omega$ that I mentioned in the previous section. There is always a difference in practice between the real $\vec \omega$ and the model $\vec \omega$. We know the form of the difference is a polynomial with a cubic term and an inverse term, but we don't know the coefficients - they are determined simultaneously when determining the differences due to $f_1$ and $f_2$. Any advice?","['inverse-problems', 'machine-learning', 'statistics', 'estimation', 'inverse']"
2225127,Find stationary point of $y = \frac {e^{2x}} {4 + e^{3x}}$,"The curve with equation $y = \frac {e^{2x}} {4 + e^{3x}}$ has one stationary point. Find the exact values of the coordinates of this point. I got to the point where this is my $\frac {dy} {dx}$:$$\frac{ (4 + e^{3x}) (2e{^{2x}})-e^{2x}(3e^{3x})}{(4+e^{3x})^2} = 0$$ Is this correct to find the stationary points, if it is, how do I get $x $ from this equation?",['derivatives']
2225145,Convergence of Random Vector to Gaussian Process in Distribution,"I am trying to show the following: Let $\{N_{c}(t)\}$ be a poisson process with parameter $c \ge 0$.  Define the process $X_{c}(t)=N_{c}(t)-ct$ for $t \ge 0.$ Show for every $k \in \mathbb{N}$ and $(t_1,...,t_k) \in \mathbb{R}^+$ the random vector $\frac{1}{c^{1/2}}(X_{c}(t_1),...,X_{c}(t_k))$ converges in distribution to $(Y(t_1),...,Y(t_k))$ where $\{Y(t)\}$ is a Gaussian process. Describe this process. My thoughts are this can be shown via characteristic functions similarly to how the CLT is proven, that is $\phi_{\frac{1}{c^{1/2}}N_{c}-ct}(t)$ converges to a characteristic function of a multivariate normal for each $t$ and $k$.  This would be sufficient to show the resulting limit is a Gaussian process.  I am struggling to actually show this algebraically. As far as describing this limit, is it interpreted similarly to the CLT?  Since the poisson process is a counting process, does the limit Gaussian process imply that the arithmetic mean of poisson process after sufficiently many time steps will follow a multivariate normal distribution?","['stochastic-processes', 'poisson-process', 'probability-theory', 'weak-convergence']"
2225173,Why the product of two manifolds is paracompact?,"Some authors define a manifold as a paracompact Hausdorff space that is locally Euclidean.  Also it is said that a product of two manifolds is a manifold.
However, we know that product of a two paracompact spaces is not necessarily paracompact.
So how can we be sure that a product of two manifolds is also paracompact and thus is also a manifold? Is it somehow related to the second-countability property that is usually defined along paracompactness?","['manifolds', 'paracompactness', 'general-topology', 'differential-geometry']"
2225182,Find all $3 \times 3$ magic square matrices $M$ such that $M^2$ is also magic,"A magic square matrix $M$ is a square matrix with real entries such that the sum of the entries in each column, each row, and each main diagonal is the same. The problem is to characterize all $3 \times 3$ magic square matrices $M$ such that $M^2$ is also a magic square matrix. Supposedly, there is an elegant way to do this besides writing out variables and bashing out the multiplication, but I haven't yet found such a solution.","['matrices', 'magic-square', 'linear-algebra']"
2225186,approximation for value of $2^x$ without using calculator,"How to find an approximation for value of $2^x$ without using calculator? For example, $2^{4.3}$.","['algebra-precalculus', 'number-theory', 'calculus']"
2225215,complete Riemannian manifolds of constant curvature $-1$,"I know that the hyperbolic plane $\mathbb{H}$ has constant curvature $-1$. I know that if a discrete group $G$ acts properly and freely on $\mathbb{H}$, then $\mathbb{H}/G$ becomes a Riemannian manifold of constant curvature $-1$, too. I am wondering if there exists a Riemannian manifold of constant curvature $-1$ which is not isometric to some quotient $\mathbb{H}/G$? Is this possible? EDIT: What happens if we have a complete Riemannian manifold $M$ of constant curvature $-1$? Best regards","['manifolds', 'riemannian-geometry', 'differential-geometry']"
2225224,The value of $\prod_2^\infty \left(1-2/(n(n+1))\right)$,"I'm trying to find the value of $\prod_2^\infty \left(1-2/(n(n+1))\right)$. So far I have the following.
\begin{align*}
\prod_2^\infty \left(1-2/(n(n+1))\right) &= (1-2/6)*(1-2/12)*(1-2/20)*(1-2/30)... \\&= (1-1/3)*(1-1/6)*(1-1/10)*(1-1/15)... \\&= (2/3)*(5/6)*(9/10)*(14/15)... \\&= (2/3) * ((1*5)/(2*3)) * ((3*3)/(5*2)) * ((2*7)/(3*5))...
\end{align*}
The partial product with just the first four terms equals $(1/3)*(7/5)$ because almost everything cancels out. I claim that the term which $(1/3)$ gets multiplied by goes to zero since it goes from $5/3$ to $3/2$ to $7/5$... (making the value $1/3$), but I don't know the explicit formula for each factor of each term so I can't prove that. Does anyone know the explicit formula for the factors?","['real-analysis', 'analysis']"
2225229,Vector magnitude subtraction,"Please help! This was in a textbook and I cannot seem to make sense of it. Would it not be 2? The two vectors a and b are perpendicular. If a has magnitude 8 and b has magnitude 3, what is  |a−2b|?? I",['multivariable-calculus']
2225244,"Detail of a proof ""Sobolev inequality $\Rightarrow$ Isoperimetric inequality"".","From: Sobolev inequality: For all $u\in C_c^{\infty}(\mathbb{R}^n)$ $$\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\leq C \|\nabla u\|_{L^1(\mathbb{R}^n)}.$$ I want to prove: Isoperimetric inequality: For every bounded open set $E\subseteq\mathbb{R}^n$ with boundary $\partial E$ of class $C^1$, $$|E|^{\frac{n-1}{n}}\leq C|\partial E|.$$ The idea is to put $u=\chi_E$ in the Sobolev inequality, because in such a case $\|u\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}$ is the left hand-side of the isoperimetric inequality. The problem is that $\chi_E$ is not in the Sobolev space $W^{1,1}(\mathbb{R}^n)$. Thus, we regularize via convolutions: define $u_\epsilon=\chi_E\ast \rho_\epsilon$, where $\rho_\epsilon(x)=(1/\epsilon^n)\rho(x/\epsilon)$, $\rho\in C_c^\infty(\mathbb{R}^n)$, $\text{support}(\rho)\subseteq B(0,1)$, $\rho\geq0$ and $\int \rho=1$. We have:
$$C\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\geq \|u_\epsilon\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}\stackrel{\epsilon\rightarrow0^+}{\longrightarrow} \|\chi_E\|_{L^{\frac{n}{n-1}}(\mathbb{R}^n)}=|E|^{\frac{n-1}{n}}.$$ We want to prove that $\|\nabla u_\epsilon\|_{L^1(\mathbb{R}^n)}\leq |\partial E|$ for all $\epsilon>0$. We use the following result: For all $v\in C_c^{\infty}(\mathbb{R}^n)$ $$\int_{\mathbb{R}^n}|\nabla v|\,dx=\sup\left\{-\int_{\mathbb{R}^n}\nabla v\cdot X\,dx:\,X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\,|X(x)|=\left(\sum_{i=1}^n (X^i(x))^2\right)^{\frac12}\leq 1\right\}.$$ It suffices then $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx\leq |\partial E|$$ for all $X\in C_c^1(\mathbb{R}^n,\mathbb{R}^n)$ and $|X|=(\sum_{i=1}^n (X^i)^2)^{\frac12}\leq 1$. We have, using integration by parts, $$-\int_{\mathbb{R}^n}\nabla u_\epsilon\cdot X\,dx=\int_{\mathbb{R}^n}u_\epsilon\,\text{Div}X\,dx=\int_E \text{Div}X_\epsilon\,dx=\int_{\partial E}X_\epsilon\cdot\nu\,d\sigma,$$ where $(X_\epsilon)^i=\rho_\epsilon\ast X^i$. It suffices to see that $|X_\epsilon(x)|\leq 1$ using $|X(x)|\leq 1$. I was not able to prove this. Any idea?","['surface-integrals', 'real-analysis', 'sobolev-spaces', 'vector-analysis']"
2225247,Theorem 5.13 in Baby Rudin: Is this statement of the L'Hospital's rule the most optimal one?,"Here is Theorem 5.13 (L'Hospital's Rule) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ and $g$ are real and differentiable in $(a, b)$, and $g^\prime(x) \neq 0$ for all $x \in (a, b)$, where $-\infty \leq a < b \leq +\infty$. Suppose 
  $$ \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13} $$
  If 
  $$ f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14} $$
  or if 
  $$ g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15} $$
  then 
  $$ \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16}$$
  The analogous statement is of course also true if $x \to b$, or if $g(x) \to -\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let $f$ be a real function defined on $E \subset \mathbb{R}$. We say that 
  $$ f(t) \to A \ \mbox{ as } \ t \to x, $$
  where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$, $t \neq x$. And, here is Rudin's proof: We first consider the case in which $-\infty \leq A < +\infty$. Choose a real number $q$ such that $A < q$, and then choose $r$ such that $A < r < q$. By (13) there is a point $c \in (a, b)$ such that $a < x < c$ implies 
  $$ \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17} $$
  If $a < x < y < c$, then Theorem 5.9 shows that there is a point $t \in (x, y)$ such that 
  $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$
  Suppose (14) holds. Letting $x \to a$ in (18), we see that 
  $$ \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19} $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choos a point $c_1 \in (a, y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$. Multiplying (18) by $\left[ g(x)- g(y) \right]/g(x)$, we obtain
  $$ \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20}$$
  If we let $x \to a$ in (20), (15) shows that there is a point $c_2 \in \left( a, c_1 \right)$ such that 
  $$ \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21} $$ Summing up, (19) and (21) show that for any $q$, subject only to the condition $A < q$, there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$. In the same manner, if $-\infty < A \leq +\infty$, and $p$ is chosen so that $p < A$, we can find a point $c_3$ such that 
  $$ p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22} $$ 
  and (16) follows from these two statements. Now I have the following queries: In (20), why has the term $f(x)/g(x)$ not been affected as we let $x \to a$, although two of the three terms on the right side have gone to $0$? Don't we need any assumption about $f$ in (15)?","['derivatives', 'real-analysis', 'calculus', 'analysis']"
2225278,Help with a problem related to Brownian motion,"Let $M(t)$ be the maximum of Brownian motion $B(t)$ up to time $t$, show that ${M(t)-B(t)}_{t\ge 0}$ and ${|B(t)|}_{t\ge 0}$ have the same distributions. I have been working on this for a long time and can't get anywhere. Any help would be appreciated. Thank you.","['stochastic-processes', 'probability-theory']"
2225280,"Find the maximum of a functional over all $f$ on $[0,1]$, where $0 \leq f(x) \leq x$ (difficult) [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $X$ be the space of all continuous functions $f : [0,1] \rightarrow \mathbb{R}$  with $0 \leq f(x) \leq x$ for every $x\in [0,1]$. Consider the functional 
$$F(f)=\int_{0}^1 f(x)^2 dx - \left( \int_0^1 f (x) dx \right)^2, \quad f\in X.$$
Does $F$ admit a maximum in $X$?","['real-analysis', 'definite-integrals', 'probability', 'calculus']"
2225315,Are the following sets countable or uncountable,"I have been trying to figure out this problem for a while now, Determine whether the following sets are countable or uncountable. Prove your answer. a)the set of real numbers with decimal representation consisting of all 2’s (2.22 and 22.222 . . . are such numbers). b)the set of real numbers with decimal representation consisting of 2’s and 5’s For a, i would say that it is countable due to that I can have a base of 2 to where I can count up for example 2.2, 2.22,... 222.22222 and so on. For b, I was confused on how to even start this one. I am just looking for a push in the right direction and if I am some what correct for the first one.","['cardinals', 'discrete-mathematics']"
2225331,Factorial system [duplicate],"This question already has an answer here : Calculating a Factorial Base Representation (1 answer) Closed 7 years ago . Factorial system, hi how can I convert decimal number to ""factorial system"" like this: $(100)_{10}$ $= 4⋅4! + 0⋅3! + 2⋅2! + 0⋅1! $ = $(4020)_{!}$ Multipliers of consecutive positions are defined by the force of consecutive positive natural numbers.",['number-theory']
2225403,Dimensionality/Combinatorial Argument for Parallelizability of 1-Manifolds,"It just occurred to me that all one-dimensional manifolds are parallelizable. We clearly have examples of many, many (many) manifolds that are not, but since any one-dimensional manifold can be written as a countable disjoint union of $\mathbb{S}^1$ or $\mathbb{R}$, and both of these spaces are parallelizable, then any one-dimensional manifold is as well. I was wondering if there's anything in the classical literature or modern research literature that gives a proof of this simply based on dimensionality.  Is there a particular reason 1-manifolds all have this property even though it doesn't extend to higher dimensions?  Is there a particular property of manifolds this is indicative of?","['manifolds', 'reference-request', 'differential-geometry']"
2225420,Normal distribution problem with 2 normal distributions?,"I have this problem: A professor of statistics noticed that the marks in his course are normally distributed. He also noticed that his morning classes average 70% with a standard deviation of 13% on their final exams. His afternoon classes average 79% with a standard deviation of 9%. What is the probability that a randomly selected student in the morning class has a higher final exam mark than a randomly selected student from an afternoon class? I've been trying to solve this problem myself for quite awhile and not really sure what is wrong with my approach. I let the morning class = X1 and the afternoon class = X2. So E(X1) = 70, σ(X1) = 13, E(X2) = 79, and σ(X2) = 9. Then I let the variable Y = X1-X2, and since X1 and X2 are both normally distributed, then Y will be as well. So I have the following for Y: $$E(Y) = E(X_1)-E(X_2) = 70-79 = -9$$
$$σ(Y) = \sqrt{σ(X_1)^2 + σ(X_2)^2} = \sqrt{13^2+9^2} = \sqrt{250}$$ In oder to find the probability that a randmly selected student in the morning class has a higher final exam mark than one from the afternoon class, we want P(X1 > X2) = P(X1 - X2 > 0) = P(Y > 0). So I converted this to z-score... $$Z = \frac{X-E(Y)}σ = \frac{0-(-9)}{\sqrt{250}} = \frac 9{\sqrt{250}} \approx 0.57$$ So that we are looking for P(Z > 0.57) or 1-P(Z$\leq$0.57). I looked at my z-score table and found my answer to be 0.2843 , which is marked as wrong. I've tried doing different things for rounding etc., and still can't seem to find the right answer. Any insight is greatly appreciated - maybe I made some dumb error that I can't seem to see. Thank you! :)","['statistics', 'normal-distribution']"
2225449,Image of the veronese map for the rational normal curve,"Given the veronese mapping for n=1
\begin{align*}
	v_d:\mathbb{P}^1(\mathbb{C})&\longrightarrow\mathbb{P}^d(\mathbb{C})\\
	\langle z_0,z_1\rangle&\longmapsto\langle z_0^d,z_0^{d-1}z_1,\cdots,z_1^d\rangle
	\end{align*}
my goal is to show that $$v_d(\mathbb{P}^1(\mathbb{C}))=Z(\langle
 x_ix_j-x_{j-1}x_{i+1}\rangle).$$ The inclusion $\subset$ is clear. The other inclusion however I need a little help. Attempt: we let $\langle x_0,\cdots ,x_d\rangle\in \mathbb{P}^d$, which satisfies $x_ix_j-x_{j-1}x_{i+1}=0$.
Then let $i$ such that we always assume $x_i$ is non zero. Since $x_i^2=x_{i-1}x_{i+1}$, we may assume by induction that either $i=0$ or $i=d$. For $i=0$: It follows that $x_j = x_{j-1}\frac{x_1}{x_0}$ , for $1\leq j \leq d$. So we let $z=\frac{x_1}{x_0}$ and it follows that
\begin{align*}
\langle x_0,\cdots, x_d\rangle &= \langle x_0,zx_0,z^2x_0,\cdots,z^dx_0\rangle\\
&=v_d(\langle 1,z\rangle)
\end{align*} Could anyone tell me how to then proceed with the case $i=d$? Thanks in advance!","['complex-analysis', 'complex-geometry', 'algebraic-geometry']"
2225455,RSA proof Wikipedia Clarification?,"I have a question involving how this line of the RSA proof from Wikipedia is simplified. Proof I know that $m^{p-1} \equiv 1(mod p)$
but how does $m^{p-1}$ simplify to 1 from the image?","['cryptography', 'discrete-mathematics']"
2225494,"A series that gives inconclusive results by the root & ratio tests, but converges STRONGLY.","I'm trying to come up with a positive sequence $\{a_n\}_1^\infty$ such that $\lim_{n\to\infty} \left(\sqrt[\leftroot{-2}\uproot{2}n]{a_n}\right) =
\lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = 1$, but $\forall \alpha > 0$ the series $\sum_1^\infty a_n n^\alpha$ converges. I know $1/n^n$ goes to zero faster than $n^\alpha$ goes to infinity, but it converges by the two tests. I've tried screwing around with $1/n^n$ but had no luck. Any thoughts?","['real-analysis', 'analysis']"
2225544,"How to solve $a_n = 2a_{n-1} + 1, a_0 = 0, a_1 = 1$?","$a_n = 2a_{n-1} + 1, a_0 = 0, a_1 = 1$ So to get the closed form of this recurrence relation, I would usually try to get it into the form of $a_n = ra_{n-1}$ and then $a_n = r^na_0$. But what am I supposed to do with the $1$? Thanks!","['recurrence-relations', 'discrete-mathematics']"
2225558,Is there ever a case where a two-dimensional limit exists but not the one dimensional limits?,"I'm just learning some proof-based multivariable calculus, and in Apostol Volume 2 chapter 8.5 exercise number 2 there is the following statement. If 
$$
\lim_{(x,y)\to (a,b)}f(x,y)=L
$$
and $\lim_{x\to a}f(x,y)$ and $\lim_{y\to b}f(x,y)$ both exist, prove that $$
\lim_{x \to a}[\lim_{y \to b}f(x,y)] = \lim_{y \to b}[\lim_{x \to a}f(x,y)]=L.
$$ This is easy enough to believe. My question is whether it is redundant to state that the single variable limits exist, given that the two variable limit exists. Is there ever a case where the two dimensional limit exists but the individual one dimensional limits don't? Thanks.","['multivariable-calculus', 'real-analysis', 'proof-writing']"
2225575,Why can't this matrix have a right inverse?,"Let $A$ be an $m \times n$ matrix with m > n. Why can't $A$ have a right inverse. We want $AB = I_m$, why is this impossible if $m > n$?","['matrices', 'linear-algebra', 'vector-spaces']"
2225597,"Determining the domain of the adjoint of $T = i\frac{d}{dx}$ on $C^1[0,1] \subseteq L^2[0,1]$","I have read this post Distinguishing between symmetric, Hermitian and self-adjoint operators But I have some specific questions relating to the operator $$T : \psi \longmapsto i \frac{d}{dx} \psi(x).$$ This operator is symmetric, meaning that $\langle \psi, T\varphi \rangle = \langle T\psi, \varphi \rangle$ for all $\varphi, \psi \in D(T)$. I'm aware that $T$ will be self-adjoint if $D(T^{\ast}) \subset D(T)$. My question is, since we know that $T$ is symmetric, does that immediately give us that the formula for $T^{\ast}$ will be $T$ itself? Moreover, if this is true, how do we determine the domain of $T^{\ast}$. For example, suppose the domain of $T$ is given to be $\psi \in C^1[0,1]$ with vanishing boundary conditions $\psi(0) = \psi(1) =0$. How do we determine the domain $D(T^{\ast}$)?","['operator-theory', 'adjoint-operators', 'functional-analysis', 'differential-operators', 'unbounded-operators']"
2225636,"Prove that there exists $x \in \mathbb{R}^n$ such that $(Ax, x) > 0$ and $(Bx, x) < 0$","Let $A$ and $B$ be two symmetric matrices from $\mathbb{R}^{n \times n}$ such that $\operatorname{tr}{A} > 0$ and $\operatorname{tr}B < 0$. How one can prove that there exists a vector $x \in \mathbb{R}^n$ such that $(Ax, x) > 0$ and $(Bx, x) < 0$? Since the matrices are real and symmetric we can select a basis in which $A$ is diagonal. In this case, its trace doesn't change. If $A = \operatorname{diag}(d_1, d_2, \dots, d_n)$, then we can note that 
$$(Ax, x) = \sum\limits_{i=1}^n d_ix_i^2$$
So, it is easy to make this one positive by chosing appropriate $x$, say, $x = (1, 1, \dots, 1)^T$. But we also need to ensure that $(Bx, x) < 0$. So the question now is how to choose $x$ so that both inequalities hold.","['expectation', 'matrices', 'diagonalization', 'trace', 'linear-algebra']"
2225637,why $\Bbb Z_2\times\Bbb Z_2$ and $\Bbb Z_4$ are not isomorphic?,"Example 8. Consider
  $$\Bbb Z_2\times\Bbb Z_2 = \{(0,0),(0,1),(1,0),(1,1)\}$$
  Although $\Bbb Z_2\times\Bbb Z_2$ and $\Bbb Z_4$ both contain four elements, it is easy to see that they are not isomorphic since for every element $(a,b)$ in $\Bbb Z_2\times\Bbb Z_2$, $$(a,b) + (a,b) = (0,0),$$ but
  $\Bbb Z_4$ is cyclic. I understand the composition $\Bbb Z_2\times\Bbb Z_2 = \{(0,0),(0,1),(1,0),(1,1)\}$ and that $\Bbb Z_4=\{0,1,2,3\}$. I understand also that ""for every element $(a,b)$ in $\Bbb Z_2\times\Bbb Z_2$, $(a,b) + (a,b) = (0,0)$"". But my questions remain: why does this show that they are not isomorphic?and how does the argument ""but $\Bbb Z_4$ is cyclic"" help answering whether or not they are isomorphic.?",['abstract-algebra']
2225638,derive a parametric equation of a sphere (on the first octant) with a dent in the middle,"I was trying to derive the surface equation of a sphere where there is a dent in the middle of the surface. I am drawing on the first octant. I want the dent to be formed by changing the radius. the equation of the radius: $$r(x,y) = \frac{1}{2}(\cos(2\pi K x) + \cos(2\pi K y))\quad K = 1 \quad (K = \text{number of dents})$$ and the parametric equation of the surface looks like this:
$$
\begin{eqnarray}
f_x(u,v) &=& r(u,v) \cos(u\frac{\pi}{2}) \cos(v\frac{\pi}{2})\\
f_y(u,v) &=& r(u,v) \cos(u\frac{\pi}{2}) \sin(v\frac{\pi}{2})\\
f_z(u,v) &=& r(u,v) \sin(u\frac{\pi}{2})\\
&& \text{where,}\quad 0 \leq u,v \leq 1
\end{eqnarray}
$$ If I plot $r(x,y)$ within $0 \leq x,y \leq 1$, I get this: Now if I draw the parametric equation, I get this: To make a proper adjustment, I change $r(x,y)$ to this:
$$r(x,y) = 5 + (\cos(2\pi K x) + \cos(2\pi K y))\quad K = 1$$ Now, if I draw it again, I get this: Still, if you look at the top of the z-axis, a fold appears which I do not want. Moreover, the scale of the sphere is changed, the radius has been changed to $[0.0, 7.0]$. But what I need is a unit sphere on the first octant without any fold on the top and the lowest point of the dent appears at $(0.5,0.5,0.5)$ coordinate (instead of $(3.5, 3.5, 2.xx)$-ish). How do I do it properly? Moreover, can I do it without $r()$ being a surface? Please help.","['parametric', 'spheres', 'geometry', 'surfaces', 'graphing-functions']"
2225694,"Prove the indicator function is not Riemann integrable on $[0,1]$.","Let $\{p_1, p_2, p_3,...\} = \mathbb{Q}\cap[0,1]$ and $S=\cup_{n=1}^\infty B(p_n, 10^{-n})\cap [0,1]$, where $B(p,r)$ is the ball around the point $p$ with radius $r$. Prove the indicator function, $1_S$, is not Riemann integrable. I think it would be possible to show the set of discontinuities of $1_S$ is of positive measure, which by Lebesgue's criterion would prove the fact. I'm not sure how to do this though. Thoughts?","['real-analysis', 'analysis']"
2225726,"Proof: if $a$ and $b$ are integers, then $a^2-4b-3\neq 0$.","I was wondering if someone could take the time to look over this proof and make sure it is correct. I greatly appreciate the help. Proposition: If $a$ and $b$ are integers, then $a^2-4b-3\neq 0$. Proof: Assume $a,b\in\mathbb{Z}$ and, for contradiction's sake, $a^2-4b-3=0$. Solving for $a^2$, we find $a^2=4b+3$. Clearly, $a^2 \equiv 3($mod $4)$. Now, we can factor 2 out of the left-hand side of $a^2=4b+3$ yielding $a^2=2(2b+1)+1$. Thus, by the definition of odd, $a^2$ is odd. Since $a^2$ is odd, $a$ must be odd. By the definition of odd, we can write $a=2c+1$ where $c\in\mathbb{Z}$. Now we can substitute for $a$ in $a^2$ to find $a^2=(2c+1)^2=4c^2+4c+1$. Factoring 4 out from the first two terms, we discover $a^2=4(c^2+c)+1$. Clearly, $a^2\equiv 1($mod $4)$. Earlier, however, we found that $a^2 \equiv 3($mod $4)$. Since $a$ can not be congruent to both 1 and 3 modulo 4, we have a contradiction. Therefore, if $a,b\in\mathbb{Z}$, then $a^2-4b-4\neq0$.","['modular-arithmetic', 'proof-verification', 'discrete-mathematics']"
2225740,"The group of $k$-automorphisms of $k[[x,y]]$, $k$ is a field","Let $k$ be a field. Is the group of $k$-automorphisms of $k[[x,y]]$ known?
  ($k[[x,y]]$ is the ring of formal power series in two variables,
  see Wikipedia .) A somewhat relevant question is this question , which deals with $k[[x]]$, with $k$ any commutative ring. Thanks for any hints and comments.","['abstract-algebra', 'formal-power-series', 'group-theory', 'commutative-algebra']"
2225743,Reduction formula for $\int \tan^n x dx$,"Reduction formula for  $$I_n = \int \tan^n x dx$$ Let $u(x) = \tan^{n-2} x \qquad \qquad v^{'}(x) = \tan^2 x$ Integrating by parts $$I_n = (\tan x - x)(\tan^{n-2} x) - \int (\tan(x) - x)(n-2)(\tan^{n - 3} x + \tan^{n- 1} x) dx \tag{0}$$ Let the second integral be $J$, After expanding and simplifying I get, $$J = (n-2)I_{n-2} + (n-2)I_n - (n-2)\int x\tan^{n-3} x + x\tan^{n-1} dx \tag{1}$$ Let $w(x) = \tan^n x \qquad \qquad z^{'}(x) = 1$ Integrating $I_n$ again by parts, $$I_n = x\tan^n x - n\int x\tan^{n+1} x  + x\tan^{n-1} x dx$$ For $I_{n-2}$, $$(n-2)\int x\tan^{n-3} x + x\tan^{n-1} dx = x\tan^{n-2} x - I_{n-2}$$ Substituting this in $(1)$ $$J = I_{n-2}(n-1) + I_n(n-2) - x\tan^{n-2} x $$ Substituting this in $(0)$, we get, $$I_{n} = {\tan^{n-1} x \over n-1} - I_{n-2}$$. Is this correct ? I know the final answer is correct but is the method correct ? looks a bit circular to me.","['integration', 'calculus']"
2225752,"""Extraneous solution"" solves original equation","For the given equation: $$x - 10 = \sqrt{9x}$$ when one simplifies, through the following steps: \begin{align*}
x^2 - 20x + 100 &= 9x\\
x^2 - 29x + 100 &= 0\\
x &= 25, 4
\end{align*} we check for extraneous solutions to make sure we have not altered from the set of solutions of the original equation while simplifying. Hence $25$:
\begin{align*}
25-10 &= \sqrt{3 \times 3 \times 5 \times 5}\\
15 &= 15
\end{align*}
Then $4$:
\begin{align*}
4-10 &= \sqrt{36}\\
-6 &= \sqrt{36}
\end{align*}
Since $-6$ is a square root of $36$, $-6 = -6$. Solutions: $25$, $4$. However, when I checked this through graphing, it appears that only $25$ is a solution and $4$ is considered extraneous . Why is this so? Square roots accounts for both a positive and a negative value and so the statement $-6 = \sqrt{36}$ should be true. Looking at the graph of the equation, you could take the positive or the negative of the square root, ending up with two different graphs. Why did we accept $25$ as a solution but reject $4$? Any help/explanation would be much appreciated.","['algebra-precalculus', 'radical-equations', 'proof-verification']"
2225758,Solving simple complex equations,"I'm starting to read a book on complex analysis, and I'm having some troubles envolving simple equations with complex numbers. How can I solve equations envolving these numbers- what methods and strategies do you recommend? When it envolves aspects like $\bar z$, $|z|$ and $Arg(z)$, what should I do? Please take this equation as an example:$$|z|-z=1+2i$$","['complex-analysis', 'complex-numbers']"
2225775,Matrix differentiation for the trace of matrix multiplication of Hadamard product,"I struggle with taking the derivative of the following equation: $\frac{∂}{∂B}Tr(A(B⊙C))$ where A,B,C are matrices, $Tr(.)$ is the trace of a matrix, and ⊙ is the Hadamard product. I appreciate any help.","['derivatives', 'hadamard-product', 'matrix-calculus', 'partial-derivative', 'trace']"
2225809,"Prove that $\frac{x}{x+1}<\ln(1+x)<x$ for $x>-1,x\neq 0$","I found this problem in the mean value theorem section in a real analysis book. I did not know, how to use the mean value theorem but I tried to find $f'$ 
and I found $$\frac{1}{(x+1)^{2}}<\frac{1}{1+x}<1.$$ Clearly, $\frac{1}{(x+1)^{2}}$decrease faster than$\frac{1}{1+x}$, when $x$ grow larger. What I will do next if I want to start from this, or is there another way to prove it?","['derivatives', 'real-analysis', 'inequality', 'logarithms', 'proof-writing']"
2225856,Prove that the largest number of $1$'s in the $n\times n$ invertible matrix $A$ with entries $0$ or $1$ is $n^2-n+1$,"Prove that the largest number of $1$'s in the $n\times n$ invertible matrix $A$ with entries $0$ or $1$ is $n^2-n+1$. My approach. If we denote $M=$number of $1$ in $A$ and $N=$number of $0$ . Then $M+N=n^2$. We are required to prove $M\leq n^2-n+1 $ alternatively $N\geq n-1$. Assume $n-1 >N$. Claim 1: There will be at least one column with all entries $1$. Proof: There are $n$ columns and if each were to contain at least one $0$ then number of $N\geq n$. Contrary to assumption that $n-1>N$. Proved. Place this column with all entries $1$ in the first column. This can always be done by column interchange without affecting the invertiblity of $A$. Claim 2: Now there will be a second column with at most one $0$. Proof: As we already selected the first column we are left with $n-1$ columns. If all of them contains more than one 0 then there will be more than $n-1$ zero contradicting the assumption again. Now place this column to second column. Now two things can happen to second column. As this column contains at  most one $0$ either (1) The second column contains all $1$ (2) The second column contains exactly one $0$ and $n-1$ numbers of $1$ . In (1) We have 1st and 2nd column equal . So their Determinant $0$. (2) $1$st column contains all $1$ and $2$nd column contains exactly one zero. Wlog assume this zero is in the first row. (This can be achieved by row interchange.)
Now the 1st columns contains all 1 and second column contains $0$ in the first component ad rest are all $1$. Now subtract $2$nd column from $1$st column in the 1st column. This will leave the $1$st column with $1$ in the $(1,1)$ position of the matrix and all entries $0$. Now expand by coffactors. We see that required deteminant is now equal to a determinant of $(n-1)\times(n-1)$ whose all entries in the $1$st column equal to $1$. Now we are back to again where we started. If one keeps doing this we eventually reach determinant $0$.
Hence proved. This how i proved it. I'm sure about the correctness of (1) but not so much about (2) . So i would like to hear your opinion about it. If you have any other proof please provide it. Thank you.","['matrices', 'linear-algebra', 'proof-verification', 'linear-transformations']"
2225857,Show $\Bbb Q[\sqrt[3]2]\cong \Bbb Q[x]/(x^3-2)$ via First Isomorphism Theorem,"I am asked to prove that $\mathbb{Q}[x]/(x^3-2)\cong R$ , where $R$ is the set of numbers $a+b\sqrt[3]2+c\sqrt[3]4$ with $a,b,c \in\mathbb{Q}$ . By, first isomorphism theorem of rings: I have defined my map as $\mathbb{Q}[x] \rightarrow R$ by $(f(x)) \mapsto f(\sqrt[3]2)$ . I am not sure how to show surjective or how to go about the kernel. Would proving containment be best for the kernel?","['abstract-algebra', 'ring-theory', 'ring-isomorphism', 'field-theory']"
2225877,Dimension of open subsets of $R^n$,Does an open subset of $R^n$ exist that has dimension less than $n$ in the standard topology? All the less than $n$ dimensional subsets I can think of are not open.,['general-topology']
2225881,Integration of differential forms - how to extend (and fix?) this intuition?,"For context, I'm a first-year undergrad in a linear algebra / multivariable calculus course. I've developed some intuition about $k$-vectors and $k$-forms, and I want to know: Are there any problems with this intuition (mistakes or areas in which it is counterproductive)? How can this be extended to things like closed/exact forms, the exterior derivative, and integration of differential forms?
$\newcommand{\reals}{\mathbb{R}}$ Oh, and I've heard that all of this can be extended to ""fields"" instead of just $\reals$, and the complex numbers are an example of such a field. I haven't worked with them, though, so I'm just going to use $\reals$ for now. Okay, so I'm familiar with vectors (both as elements of an abstract vector space and as elements of $\reals^n$ - in my class, we're mainly working with $\reals^n$, though). From what I understand, a bivector (in Euclidean space) can be thought of as an ""oriented parallelogram"" similarly to how vectors can be thought of as oriented line segments. Similarly, a trivector can be thought of as an oriented parallelepiped, and this is extended to higher dimensions that are harder to visualize. We can construct a $k$-vector from $k$ regular vectors by taking the wedge product of those vectors. The wedge product is multilinear and alternating. (This defines an ""orientation"" - the equivalent of forward/back or clockwise/counterclockwise - for each $k$-vector that switches if you switch two vectors in the wedge product.) Geometrically, if two oriented parallelepipeds have the same orientation, they ""span the same subspace of $\reals^n$"", and one's ""component vectors"" can be rotated and scaled to ""meet"" the other's without leaving that subspace or using reflections. (If they ""share the same subspace"" but cannot be rotated/scaled to ""meet"" each other, then one is a negative scalar multiple of the other.) (I'd never state it formally like this, but this is just about getting across my intuition, so hopefully it makes sense.) A covector is an element of the dual space of a vector space, $V^*$, and is a linear transformation from $V$ to $\reals$. $V^*$ is also a vector space. Covectors are also called $1$-forms. They can also be wedged together to create $k$-forms, which are alternating multilinear transformations from $V^n$ to $\reals$. (We write the set of all $k$-forms as $\Lambda^k(\reals^n)$.) (I'm vaguely familiar with this visualization of $k$-forms, but the exterior derivative part loses me.) A smooth vector field is a $\mathcal C^\infty$ function that maps every point in $\reals^n$ to a vector. (Though I've never heard the terms, I assume there are also ""bivector fields"", ""trivector fields"", and so on.) A differential $k$-form is a smooth $k$-covector field, which is the same as a smooth vector field but replacing ""vector"" with ""$k$-covector"" or ""$k$-form"". Is this intuition ""correct""? Can it be extended to describe closed/exact $k$-forms, the exterior derivative, and/or integration of differential forms?","['multivariable-calculus', 'intuition', 'differential-geometry', 'exterior-algebra']"
2225897,"What is the difference between ""Polynomial"" and ""Multinomial"" in two or more variables?","What is the difference between ""Polynomial"" and ""Multinomial"" in two or more variables? Since, by definition: Multinomial: An algebraic expression having two or more (unlike) terms is called a Multinomial. For example: $5x^2 - 2x$ is a multinomial having $2$ terms, $5x^3- 2xy + 7y^2$ is a multinomial having $3$ terms, $7xy - 9yz + 6zx - 7$ is a multinomial having $4$ terms. Polynomials in two or more variables: An algebraic expression in two or more variables is called a Polynomial if the Power of every variable in each term is a whole number. Some books say ""Multinomial"" is one of the types of ""Polynomial"", and the other discuss it in particular. Is the function $f$ cross ""Polynomial"" or ""Multinomial""? Why? $$f(x, y)=x y + y^2 + 2 x y^2 + y^3 - 3 x y^3 + x y^4,$$","['algebra-precalculus', 'polynomials', 'terminology']"
2225909,"Evaluating $\int_{0}^{\pi/4}\left[4\cos^2(x)-1\right]{\sqrt{\tan x}}\,\mathrm dx$ in a less tedious way","Consider this integral $$\int_{0}^{\pi/4}\left[4\cos^2(x)-1\right]\color{red}{\sqrt{\tan x}}\,\mathrm dx\tag1=1$$ An attempt: Rewrite $(1)$ as $$\int_{0}^{\pi/4}4\cos^2(x)\sqrt{\tan x}\,\mathrm dx-\int_{0}^{\pi/4}\sqrt{\tan x}\,\mathrm dx=I_1-I_2\tag2$$ For $I_2$ we choose $u=\sqrt{\tan x}\implies {2u\over 1+u^2}dx=dx$, then it becomes $$\int_{0}^{1}{2u^2\mathrm du\over 1+u^4}\tag3$$ Rearrange $(3)$ in the form of $$\int_{0}^{1}{1+{1\over u^2}\over u^2+{1\over u^2}}\mathrm du+\int_{0}^{1}{1-{1\over u^2}\over u^2+{1\over u^2}}\mathrm du\tag4$$ Apply completing the square to $(4)$ $$\int_{0}^{1}{1+{1\over u^2}\over \left(u-{1\over u}\right)^2+2}\mathrm du+\int_{0}^{1}{1-{1\over u^2}\over \left(u+{1\over u}\right)^2-2}\mathrm du=I_3+I_4\tag5$$ For $I_3$ setting $v_1=u-{1\over u}$ and for $I_4$ setting $v_2=u+{1\over u}$ then $$\left.{1\over \sqrt{2}}\tan^{-1}\left({v_1\over \sqrt{2}}\right)\right|_{0}^{\infty}-{1\over \sqrt{8}}
\left.\ln{\left({v_2+\sqrt{2}\over v_2-\sqrt{2}}\right)}\right|_{2}^{\infty}={\pi\over \sqrt{8}}+{1\over \sqrt{8}}\ln{(3+2\sqrt{2})}\tag6$$ We can rewrite $I_1$ as $$I_1=2\int_{0}^{\pi/4}\sqrt{\tan x}\,\mathrm dx+2\int_{0}^{\pi/4}\cos(2x)\sqrt{\tan x}\,\mathrm dx\tag7$$ $$I_1=2\cdot(6)+2\int_{0}^{\pi/4}\cos(2x)\sqrt{\tan x}\,\mathrm dx\tag8$$ $(8)$ we can apply integration by part, I am sure it would be lengthy. How else can we tackle $(1)$ in another less lengthy way?","['integration', 'definite-integrals', 'calculus']"
2225932,How to physically interpret conjugate functions?,"If given a convex function $f: \mathbb{R} \to \mathbb{R}$, then the conjugate function $f^*$ is defined as $$f^*(s) = \sup_{t \in \mathbb{R}} (st-f(t))$$ Now i want to understand what is the physical interpretation of this conjugate function? What is its exposition? Please help me.","['convex-optimization', 'convex-analysis', 'functions']"
