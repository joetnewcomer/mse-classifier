question_id,title,body,tags
238898,What kind of matrix $A$ satisfies $Ax\geq 0\Rightarrow x\geq 0$?,"$A\in \mathbb{R}^{n\times n}$ is an n-by-n matrix. $x=(x_1,x_2,\ldots ,x_n)\in \mathbb{R}^n$ is a vector. $x\geq 0$ means $x_i\geq 0,\forall i$. Q1: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,x\geq 0\Rightarrow Ax\geq 0$? Q2: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,Ax\geq 0\Rightarrow x\geq 0$? Q1 is solved by Martin Argerami(The matrix's entries are all positive). But what are the matrices in Q2? In Q2, the answer is not ""entries all positive"". The counterexample is $A=\begin{pmatrix} 1 & 1 \\ 1 & 1\end{pmatrix}$, $A\begin{pmatrix} -1 \\ 2 \end{pmatrix}\geq 0$. Let's see an example $A=\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}$.$A\begin{pmatrix} x \\ y \end{pmatrix} \geq 0\Rightarrow x\geq 0,y\geq 0$. So $A$ is in answer to Q2. What general properties of matrix $A$ would ensure Q2 be satisfied? Another obvious class of matrix in Q2 is diagnoal matrices with positive diagonal entries. Q2 is settled by Robert Israel($A^{-1}$ has entries all positive).",['matrices']
238906,"How many distinct n-letter ""words"" can be formed from a set of k letters where some of the letters are repeated?","How many distinct n-letter ""words"" can be formed from a set of k letters where some of the letters are repeated? Examples: __ How many 6-letter words can be formed from the letters: ABBCCC? This is elementary. There are 6! arrangements counting repeats. Then we just divide by 2!3! to account for the repeats caused by the 2!3! identical arrangements of the Bs and Cs How many 5-letter words can be formed from the letters: ABBCCC? Excluding A we have $\frac{5!}{2!3!}$ Excluding either B $\frac{5!}{3!}$ Excluding any of the Cs $\frac{5!}{2!2!}$ Then take the sum. How many 4-letter words can be formed from the letters: ABBCCC? At this point I find it difficult to procede without far too many cases. Is there a general approach?","['probability', 'combinatorics']"
238909,Complete first order theory with finite model is categorical,"I am trying to prove that if $T$ is a complete first order theory that has a finite model then it has exactly one model up to isomorphism. To this end, I assumed that $T$ is complete with a finite model $M_n$. Then I assumed that $M_m$ was another model of size $m \geq n$. We know that if a theory has two finite models with different cardinalities then the theory is incomplete hence $m = n$. Now two things remain to be shown: one is that any two models of finite size $n$ are isomorphic and the other is that every infinite model is also isomorphic to this finite model (is this even possible? but we clearly need to do something about the infinite case) Thanks for helping me finish this proof. For the record: this is an exercise in Just/Weese, page 84. Edit I'm looking for a sentence $\varphi$ such that if $M_n \models \varphi$ then $M_n \cong M_m$. There are no assumptions on the language. But I think it's not possible to have a finite model for an infinite language so the language must be finite. Edit 2 After some more thinking, if there is a finite model $M$ of size $n$, let $$ \varphi = \exists v_1, \dots , v_n ((v_1 \neq v_2) \land \dots \land (v_{n-1} \neq v_n)) \land \lnot \exists v_{n+1} ((v_{n+1} \neq v_1) \land \dots \land (v_{n+1} \neq v_n))$$
that is, $\varphi$ says that the model has exactly $n$ elements. Since $T$ is complete, either $\varphi$ or $\lnot \varphi$ is provable from $T$. Since we have a model in which $\varphi$ is true we therefore know that $T \vdash \varphi$. Hence any model of $T$ must have exactly $n$ elements. Now the question is, how do I show that any two $n$-element models of $T$ must be isomorphic?","['elementary-set-theory', 'model-theory']"
238920,Reflexive Transitive Closure,"The problem I am working on is, ""Show that a finite poset can be reconstructed from its covering relation. [Hint:Show that the poset is the reflexive transitive closure of its covering relation.]"" I have been searching through my textbook, and on the internet, for the definition of reflexive transitive closure, but I was not successful. Could someone please explain this concept to me?","['relations', 'discrete-mathematics']"
238934,proof: set countable iff there is a bijection,"In class we had the following definiton of a countable set: A set $M$ is countable if there is a bijection between $\mathbb N$ and $M$. In our exam today, we had the following thesis given:If $A$ is a countable set, then there is a bijection $\mathbb N\rightarrow A$. So I am really not sure if the thesis and therefore the equivalence in the definition is right. So is it correct? And how do you proove it? Thanks a lot!","['elementary-set-theory', 'real-analysis']"
238940,"Understanding equivalence class, equivalence relation, partition","I'm having difficulty grasping a couple of set theory concepts, specifically concepts dealing with relations. Here are the ones I'm having trouble with and their definitions. 1) The collection of equivalence classes w.r.t. $R$ Def : Let $R$ be an equivalence relation in a set $X$ . The collection of equivalence classes w.r.t. $R$ is the set: $$[X]/R =\{S|(\exists x)(x\in X\land S=[x]/R)\}=\{[x]/R|x\in X\}$$ 2) Partition Def : Let $X$ be a set. A collection of sets $C$ is a partition of $X$ if: (i) $$\bigcup_{S\in\ C} S=X.$$ (ii) $$\forall S \in C, S \neq \varnothing$$ (iii) $$\forall S, S' \in C, S' \neq S \Rightarrow S \cap S' = \varnothing$$ 3) Relation induced by Def : Let $C$ be a partition of $X$ . The relation induced by $C$ , denoted by $X/C$ , is a relation in $X$ such that $$X/C = \{(x,y) | (\exists S \in C)(\exists x \in S \land \exists y \in S)\}$$","['relations', 'equivalence-relations', 'elementary-set-theory', 'set-partition']"
238944,Stuck on existence proofs involving measurability and simple functions,"Some classmates and I have been working through a sequence of problems in Royden's real analysis text, which are in the chapter on Lebesgue measurable functions revolving around the Sequential Pointwise Limits and Simple Function approximations. We have some of them done, but are stuck on others. For each of the problems, we assume we have $I$ a closed/bounded interval. Let $E$ be a measurable subset of $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h=\chi_E$ on $F$ and $m(I\setminus F)<\epsilon$ Let $\psi$ be a simple function defined on $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h=\psi$ on $F$ and $m(I\setminus F)<\epsilon$ Let $f$ be a bounded measurable function defined on $I$ . Let $\epsilon > 0$ . Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $|h-f|<\epsilon$ on $F$ and $m(I\setminus F)<\epsilon$ . Clearly all these problems are very similar and build upon one another. You are then showing existence of a step function on $F \subset I$ where $m(I\setminus F)<\epsilon$ . In the first you show that $\chi_E$ exists. Then you show a simple function $\psi$ exists which we know is of the form $\psi=\sum_{k=1}^n a_k \chi_{E_k}$ . Then you do it for any bounded measurable function $f$ . Ideas for 1: There exists a finite open cover of $I$ ( $O=\bigcup_{k=1}^n I_k$ ), which should also cover $E \subset I \subset O$ with the property that $m(O\setminus I)<\epsilon$ . If we can set $F=(O\setminus E)^c$ we could try to show that $m(I\setminus F) = m(O\setminus E)<\epsilon$ . Also we notice that $\chi_E = \chi_O$ on $F$ . Though we haven't quite put it all together. Ideas for 2: Let $\psi=\sum_{k=1}^n a_k \chi_{I_k}$ where again $O=\bigcup_{k=1}^n I_k$ is an open cover of $I$ . We also know there is a closed $F_i \subset I_i$ where $m(I_i \setminus F_i)<\frac \epsilon n$ which could lead to $m(I \setminus F) \le m(\bigcup_{i=1}^n I_i \setminus \bigcup_{i=1}^n F_i) = \sum_{i=1}^n m(I_i \setminus F_i) < \sum_{i=1}^n \frac \epsilon n = \epsilon$ Ideas for 3: Use simple approximation theorem which says that $f$ is measurable on $E$ iff exists a sequence of simple functions which converge p.w. on $E$ to $f$ s.t. $|\psi_n|\le|f|$ on $E$ for all $n$ . Any suggestions towards putting the ideas together or simpler solutions would be greatly appreciated!","['measure-theory', 'real-analysis', 'analysis']"
238959,Solution Sets of Trigonometric Equations,"Introduction Hi there. In advance, I apologize if this question is too broad. Please do not downvote if that is the case, as this question is purely imaginative curiosity. I will close it should it turn out to be too broad. Motivation So, this might be the most ridiculous inspiration ever. However, I have been reading Trigonometry for Dummies by Mary Jane Sterling. On pages 250 and 251, Sterling uses graphing to solve the following equations:
$$
\begin{align}
\cos (2x)&=2\cos(x).\\
\cos^2(x)-0.4\sin(x)&=0.6.
\end{align}
$$
It was clear to me that these two equations are just quadratics in terms of $\cos(x)$ and $\sin(x)$ respectively. This inspired me to implore as to why the author was motivated to use graphing to solve these two equations. I cannot see what motivates that, but I may very well be missing something. What follows is the true question that arose. Question After considering the above, I asked myself: Are there messy trigonometric equations? By messy, I mean the sense that quintic equations are considered messy in general since they do not possess a general solution in terms of the basic arithmetical operations of addition, subtraction, extraction of roots, multiplication, and division. (Pardon me if I'm forgetting something here.) I searched a little bit and there seems to be no resources on this. Here is what I am curious about: Is there a subfield of study akin to Galois theory (or perhaps an application of Galois theory?) that studies the solvability of trigonometric equations and the nature of the solutions to trigonometric equations? It should be noted that what I mean by 'trignometric equations' is any equation which uses any of the three trigonometric functions (sine, cosine, and tangent), addition, multiplication, subtraction, division, and extraction of roots. P.S. As part of an answer to this question, I would like to see some pathological trigonometric equations if you are willing to provide them. Further Thoughts Looking at Wiki for the specifics, I see that . . . . . . whether a polynomial was solvable or not was equivalent to whether or not the permutation group of its roots – in modern terms, its Galois group – had a certain structure – in modern terms, whether or not it was a solvable group. In essence, if the Galois group of a polynomial is a solvable group, then the polynomial is solvable. Wiki goes on to explain . . . This group was always solvable for polynomials of degree four or less, but not always so for polynomials of degree five and greater, which explains why there is no general solution in higher degree. In this sense I am curious about the solutions of trigonometric equations. Is there any such concept similar to the Galois group that is applicable to the study of trigonometric equations as defined above? It should be noted I don't fully understand these concepts, and I may be completely off in my thinking. I apologize if that is the case.","['galois-theory', 'trigonometry']"
238965,"Group of sphere transformations, impressing friends","Ok, so here's the story: I am reading a book on algebra and, via some exercises, discovered that in any group $G$, the order of $x \cdot y$, written $o(x \cdot y)$, equals $o(y \cdot x)$. Now, this is trivial in an abelian group, but I was looking for examples of a non-abelian group (simply because the result was interesting) to see this happen. Of course, I knew $GL(2, \mathbb{R})$ and the permutation groups. However, literally by chance (I had a ball in my hand), I realized that $m(90)$ degree rotations of a sphere - $m \in \mathbb{N}$ - are also a non-abelian group. (That is, let $G$ be the set of transformations of some particular distinguished point on a sphere through right angles, like the transformation forward , or clockwise , or right . The group operation is composition, and the identity is the ""don't do anything"" transformation.) This discovery lends itself to a trick I find neat: take a sequence of operations, and find their order. (Like $o(\text{fwd cwise left left})$, which is $3$.) Then, take any cyclic permutation of that sequence, and you have a sequence of the same order. If you actually have a sphere on you (I took a ball and drew a little arrow on it) and you ask someone for an eight-term sequence, and then instantly give them back a (well-mixed, unrecognizable) sequence of the same order, and show them that you're right, on the spot -  this is impressive. Well, okay... actually, that's the thing. I find it impressive; my friends don't. This bums me out. So, my question: how can I amp this trick up? I thought about memorizing a ""basis"" for all sequences of a certain length, i.e., knowing enough sequences that any of them are commutation-equivalent with the ones I know; but, unfortunately, this is impractical. Does anybody know how to make this cooler?","['puzzle', 'recreational-mathematics', 'group-theory', 'abstract-algebra']"
238970,How to evaluate fractional tetrations?,"Recently I've come across 'tetration' in my studies of math, and I've become intrigued how they can be evaluated when the ""tetration number"" is not whole. For those who do not know, tetrations are the next in sequence of iteration functions. (The first three being addition, multiplication, and exponentiation, while the proceeding iteration function is pentation) As an example, 2 with a tetration number of 2 is equal to $$2^2$$ 3 with a tetration number of 3 is equal to $$3^{3^3}$$ and so forth. My question is simply, or maybe not so simply, what is the value of a number ""raised"" to a fractional tetration number. What would the value of 3 with a tetration number of 4/3 be? Thanks for anyone's insight","['exponentiation', 'tetration', 'functions']"
238972,Rao-Cramer lower bound for Rician distribution,"I derived ML estimator for Rician distributed data and I am trying to show Rao-Cramer lower bound of $\hat{A}$ estimator variance. $$f(x_k|A,\sigma) = \frac{x_k}{\sigma^2}\exp\left(-\frac{x^2_k+A^2}{2\sigma^2}\right)I_0\left(\frac{xA}{\sigma^2}\right) \tag{Rician distribution}$$ where $I_n(x) = \left( {1 \over 2}x \right )^{n} \sum_{k=0}^{\infty} \frac{\left( {1 \over 4}x^2 \right )^k}{k! (n+k)!}$ is modified Bessel function of the first kind of $n$-th order ($n \in N$) The ML estimator, that I derived: $$\hat{A} = \frac{1}{N} \sum_{k=1}^{N} x_k  \frac{  I_1 \left( \frac{x_k A}{\sigma^2} \right )}{   I_0 \left( \frac{x_k A}{\sigma^2} \right )}$$ Unfortunately, variance derivation looks like challenging problem. Anyone could suggest a trial? $$\operatorname{Var}_\hat{A} = \frac{1}{N^2}E_{\hat{A}} \left( \sum_{k,l=1}^N\left( x_k-E_{\hat{A}}\left(x_k\frac{I_1}{I_0}\right) \right) \left( x_l-E_{\hat{A}}\left(x_l\frac{I_1}{I_0}\right) \right) \right)$$ where $I_0, I_1$ is of course $I_0 \left( \frac{x_k A }{\sigma^2} \right )$, $I_1 \left( \frac{x_k A }{\sigma^2} \right )$, respectively.","['statistics', 'information-geometry']"
238980,Exponents in Odd and Even Functions,"I was hoping someone could show or explain why it is that a function of the form $f(x) = ax^d + bx^e + cx^g+\cdots $  going on for some arbitrary length will be an odd function assuming $d, e, g$ and so on are all odd numbers, and likewise why it will be even if $d, e, g$ and so on are all even numbers.  Furthermore, why is it if say, $d$ and $e$ are even but $g$ is odd that $f(x)$ will then become neither even nor odd? Thanks.","['symmetry', 'functions']"
238981,Matrix Equation with Quadratic form,"I am working in a problem that involves multivariate normal distributions and, at a given point, I need to solve the following matrix equation: $$x=\sqrt{x^{\prime}\Sigma^{-1}x} \cdot y$$ Where $x$ is a $N\times1$  vector, $\Sigma$ is a $N\times N$ variance-covariance matrix (therefore symmetric, positive definite and with each element being positive), and $y$ is a different $N\times1$ vector ($y$ can take positive and negative values). I am interested in a (hopefully explicit) solution for $x$ as a function of $\Sigma$ and $y$, which are known parameters. There is a trivial solution $x=0$, and this is the only solution when $N=1$. My question is: Are there any other solutions when $N>1$? How can they be characterized?","['matrices', 'quadratic-forms']"
238985,Sheaf of Relative Differentials for Curves (Hartshorne),"I am trying to understand the sheaf of relative differentials for the case of nonsingular curves. Let's use Hartshorne as a reference, thus a curve is an integral scheme of dimension 1, proper over $k$, all of whose local rings are regular. Based on the definition of the curve, and Theorem 8.15 at p. 177, i see that if $X$ is a curve, then $\Omega_{X/k}=\Omega_X$ is a locally free $O_{X}$-module of rank $1$. Now in page 300, it is mentioned that if $u$ is a local parameter at $P \in X$, then $du$ is a generator of the free $O_P$-module $\Omega_{X,P}$. Could somebody please explain: 1) How does it follow from the fact that $\Omega_{X}$ is a locally free $O_X$-module of rank $1$, that the stalk $\Omega_{X,P}$ is a free $O_{X,P}$ module of rank $1$?
2) I understand that $d$ is some universal derivation; is it the universal $O_{X,P}$ derivation corresponding to the $O_{X,P}$-module $\Omega_{X,P}$?
3) Why is $\Omega_{X,P}$ generated by $du$? As it might be obvious, i am completely missing the picture here. Thanks.",['algebraic-geometry']
238997,Prove the divergence of the sequence $\left\{ \sin(n) \right\}_{n=1}^{\infty}$.,"I am looking for nice ways of proving the divergence of the sequence $\left\{x_n\right\}_{n=1}^{\infty}$ defined by
$$x_n=\sin{(n)}.$$
One (not so nice) way is to construct two subsequences: one where the indexes are picked such that they lie in the intervals 
$$I_k=\left(\dfrac{\pi}{6}+2\pi(k-1),\dfrac{5\pi}{6}+2\pi(k-1)\right)$$
and one where they lie in
$$J_k=\left(\dfrac{7\pi}{6}+2\pi(k-1),\dfrac{11\pi}{6}+2\pi(k-1)\right).$$
If ${x_n}$ converges, then all its subsequences must converge to the same limit, but here the first subsequence has all its values in the interval $\left[ \frac{1}{2}, 1\right]$ while the second has its values in $\left[-1,-\frac{1}{2}\right]$. Contradiction. Filling the details of this proof is rather tedious and not very elegant. Anyone has a better idea?",['sequences-and-series']
239024,"Question from Folland, criteron for a function to belong to $L^p$","This question is from Folland 6.38,
Show that $f \in L^p $ iff $\sum_{k=-\infty}^ {\infty} 2^{pk} \mu \{{x: |f(x)|>2^{k}}\} \lt \infty$ If $f \in L^p $, I applied the Chebyshev's inequality But for the other direction, I don't know how to begin. Any advice would be appreciated.
Thanks","['measure-theory', 'lp-spaces', 'real-analysis']"
239027,How to show convolution of an $L^p$ function and a Schwartz function is a Schwartz function,"We have the Schwartz space $\mathcal{S}$ of $C^\infty(\mathbb{R^n})$ functions $h$ such that $(1+|x|^m)|\partial^\alpha h(x)|$ is bounded for all $m \in \mathbb{N_0}$ and all multi-indices $\alpha$.
We are given an $f \in L^p$  with $1\leqslant p < \infty$ and $g \in \mathcal{S}$.
We want to show that $f \star g \in \mathcal{S}$, where $\star$ denotes the convolution operator. I have already shown that $f \star g \in C^\infty$ by proving that $\partial^\alpha (f \star g) = f \star (\partial^\alpha g)$. Now I need to show that $(1+|x|^m)|\partial^\alpha(f \star g)(x) = (1+|x|^m)|f \star (\partial^\alpha g)(x)|$ is bounded. Since $\mathcal{S}$ is closed under differentiation, it suffices to consider $\alpha = 0$. I write
$$
\int_{\mathbb{R}^n}f(y)g(x-y)(1+|x|^m)dy
$$
and try to bound it but can't seem to make it work out. Could anyone help me proceed?","['convolution', 'measure-theory', 'harmonic-analysis', 'lp-spaces']"
239063,Intervals are connected and the only connected sets in $\mathbb{R}$,"As the topic, prove that Intervals are connected and only connected  in $\mathbb{R}$. I know what is the definition of connected set. But not sure how to prove that.","['general-topology', 'connectedness']"
239102,Is the inverse function continuous?,"Suppose $f:X\rightarrow Y$ is 1-1 and continuous. Is $f^{-1}:f(X)\rightarrow X$ continuous too? If not, can you explain it?","['calculus', 'functions', 'analysis']"
239105,Special integrals,"There are special integrals such as the logarithmic integral and exponential integrals. I want to know if there are primitives for such integrals. If not, why not?",['real-analysis']
239108,Classification of the compactifications of $\mathbb{R}$,"Do you know if there exists a classification of the compactifications of $\mathbb{R}$ ? From here , we can deduce that there exist only two compactifications with finite remainder: $[0,1]$ and $\mathbb{S}^1$ ; and from here , you can show that there doesn't exist a compactification with a countable remainder (but an example is given for a compactification with a remainder of cardinality $\mathfrak{c}$ ). On the other hand, the biggest compactification of $\mathbb{R}$ is $\beta \mathbb{R}$ with a remainder of cardinality $2^{\mathfrak{c}}$ . Can we deduce a complete classification of the compactifications of $\mathbb{R}$ ?","['general-topology', 'compactification', 'real-numbers', 'compactness']"
239118,what is total order - explanation please,"sorry for the dumbest question ever, but i want to understand total order in an intuitive way, 
this is the defition of total order: i) If $a ≤ b$ and $b ≤ a$ then $a = b$ (antisymmetry); ii) If $a ≤ b$ and $b ≤ c$ then $a ≤ c$ (transitivity); iii) $a ≤ b$ or $b ≤ a$ (totality). totality means that any pair of the total ordered pair is mutually comparable. i dont understand what they mean under comparable , what is the defition of comparability? i can also compare the elements of partial order, where is problem? why is partial order not not mutually comparable? can someone explain me please in simple words :(","['order-theory', 'elementary-set-theory', 'definition']"
239133,Algebraic classes in Hodge decomposition.,"Let $X$ be a Kähler manifold. The torsion free part of the singular cohomology $H^n(X,\mathbb{C})$ has a Hodge decomposition
$$
H^n(X,\mathbb{C})=\bigoplus_{p+q=n}H^{p,q}(X), 
$$
where $H^{p,q}(X)$ can either be viewed as the space of de Rham classes of bidegree $(p,q)$ or as the Dolbeault cohomology $H^{q}(X,\Omega^p)$. For any subvariety $Z\subset X$, the fundamental class $[Z] \in H^{2k}(X,\mathbb{C})$ for some $k$. Since $[Z] \in H^{2k}(X,\mathbb{Z})$, it defines a real class, but how can one prove that it lies in $H^{k,k}(X)$? Why not $[Z] \in H^{k-1,k+1}(X,\mathbb{Z})\oplus H^{k+1,k-1}(X,\mathbb{Z})$?","['algebraic-geometry', 'hodge-theory', 'complex-geometry', 'kahler-manifolds']"
239146,Consistency and asymptotically unbiasedness?,"Does consistency imply asymptotically unbiasedness? I know the statement doesn't work in the other direction. My guess is it does, although it obviously does not imply unbiasedness. I think it wouldn't be too hard if one digs into measure theory and makes use of convergence in measure. But I have a gut feeling that this could be proved with only elementary probability theory. But can anybody give me a proof? Thanks!",['probability']
239166,What is the intuition for using definiteness to compare matrices?,"If $a$ and $b$ are two numbers on the real line, we compare $a$ and $b$ by knowing which of them comes first as we move from $-\infty$ to $\infty$ on the real line. However when $A$ and $B$ are matrices, the comparison is through definiteness. We say $A \succ B$ iff $A-B$ is positive definite. Positive definiteness of $A$ means $x^TAx>0\ \forall x$; essentially the function $f(x)=x^TAx$ takes the form of a bowl with its base at origin. How does this ""bowl"" help in comparing two matrices? What is the intuition behind using definiteness in matrices for ordering?","['matrices', 'linear-algebra']"
239171,Questions on Bayesian analysis of an opinion poll (an example in a book),"I'm sorry in advance for rather long questions. This is an example in ""Bayesian logical data analysis for physical sciences"" by P. C. Gregory and I have some questions about the example. In a poll of 800 decided voters, 440 voters supported the political party A. Let's denote the poll result as $D$. The quantity of interest is the probability that the party A will achieve a majority of at least 51% in the upcoming election, assuming the poll will be representative of the population at the time of the election. The book regards the problem as a model selection problem. $M_1$ : The party A will achieve a majority with a parameter $H$ that has uniform prior in the range $0.51 \le H \le 1$. $M_2$ : The party A will not achieve a majority with a parameter $H$ that has uniform prior in the range $0 \le H < 0.51$. If we have no prior reason to prefer $M_1$ over $M_2$, we can write the odds ratio $$\begin{aligned}
O_{12}&=p(M_1|D,I)/p(M_2|D,I)\\
&=p(D|M_1,I)/p(D|M_2,I)\\
&=\frac{\int_{0.51}^1 p(H|M_1,I)p(D|H,M_1,I) dH }{\int_{0}^{0.51} p(H|M_2,I)p(D|H,M_2,I) dH}\\
&=\frac{\int_{0.51}^1 (1/0.49)p(D|H,M_1,I) dH }{\int_{0}^{0.51} (1/0.51)p(D|H,M_2,I) dH}\\
&=87.68
\end{aligned}$$ Here are my questions. The book don't give explicit expressions for $p(D|H,M_1,I)$ and $p(D|H,M_2,I)$. If I use binomial distribution $$p(D|H,M_1,I)=p(D|H,M_2,I)=\frac{800! H^{440}(1-H)^{800-440}}{440!(800-440)!}$$
I get $87.03$ as a result. It is not same to the value $87.68$ of the Book. What probability distribution should I use for the likelihoods? I have another question. Why do I have to introduce the models $M_1$ and $M_2$? Is $$
O_{12}=\frac{\int_{0.51}^1 p(H|D,I) dH}{\int_{0}^{0.51} p(H|D,I) dH}
$$ not an appropriate aproach for the problem? It does not have the factor $(1/0.49)/(1/0.51)$ introduced with the models $M_1$ and $M_2$.","['statistics', 'bayesian', 'probability']"
239202,How to perform a fair coin toss experiment over phone?,"I was recently asked this question by my friend. Suppose the two individuals participating in a toss are not near each other, but could communicate over a telephone. How does one construct a fair coin toss experiment that is mutually agreeable to both of them? They can't agree on a function of quantities like the time or the telephone number, as these decide the winner a priori (before the experiment is conducted). I suggested they disconnect the call and try again; whoever manages to reach the other first is the winner. But the state machine involved here is a bit complicated to get the simple (0.5,0.5) probabilities. PS: They do not trust each other, so one of them can't toss a fair coin and convey its outcome to the other. Both of them throwing simultaneously also doesn't work, as the second person has the incentive to lie when they are communicating the results to each other.","['puzzle', 'probability']"
239206,Proving that $\frac{n+1}{2n+3}$ and $\frac{3n-5}{4n-7}$ are irreducible for all $n$,"I am trying to solve the following problem: Prove that the following fractions are irreducible for any n (n is a natural number and it cannot be null). $\frac{n}{n+1}$ $\frac{n+1}{2n+3}$ $\frac{3n-5}{4n-7}$ I don't know if my logic is right! Now for the first one, i used the property which states that $n$ and $n+1$ 
are always coprime. For the second one i relied on the following property, if $n|a$ and $n|b$ then $n|(a*k - b*p)$. So lets suppose $\frac{n+1}{2n+3}$ is reducible, then there is a whole number $d$ which divides both the numerator and denominator. By property 1, $d|[2n+3 - 2(n+1)]$ this means $d|1$ so $d=1\ldots$. So the fraction is not reducible since every number is divisible by 1. Is my logic right? how would someone go about solving this problem?","['fractions', 'algebra-precalculus', 'divisibility']"
239207,Hessian matrix of a quadratic form,Prove that the Hessian matrix of a quadratic form $f(x)=x^TAx$ is $f^{\prime\prime}(x) = A + A^T$ . I am not even sure what the Jacobian looks like (I never did one for $x \in \Bbb R^n$ ). Please help.,"['multivariable-calculus', 'hessian-matrix', 'quadratic-forms', 'matrices', 'derivatives']"
239235,Characterisation of Cantor-connectedness,"For Cantor-connectedness I use the following definition: A $p$-metric space $(X,d)$ is Cantor-connected if for any $\epsilon > 0$, any two points $x, y \in X$ can be connected by an $\epsilon$-chain, i.e., there exist points $x = x_{0}, x_{1}, \cdots, x_{n} = y$ in $X$ such that $d(x_{i-1},x_{i}) \leq \epsilon$ for all $i \leq n$. I would like to prove the following characterisation of Cantor-connectedness: A space $(X,d)$ is Cantor-connected if and only if it cannot be partitioned into two sets $A$ and $B$ such that $d(A,B) > 0.$ I already have the following: We first prove that if $(X,d)$ is Cantor-connected, $X$ cannot be partitioned into two sets $A$ and $B$ such that $d(A,B) > 0.$ Suppose that there exists a partition of $X$ into sets $A$ and $B$ with $d(A,B) > 0.$ Take $a \in A$ and $b \in B$, take $\epsilon > 0$. By the fact that $X$ is Cantor-connected, there exist $a = x_{0}, x_{1}, \cdots, x_{n} = b$ in $X$ such that $d(x_{i-1},x_{i}) \leq \epsilon$ for all $i \in \{1, \cdots,n\}$. But then we get $$d(a,b) \leq \sum_{i=1}^{n}d(x_{i-1},x_{i}) \leq n \epsilon.$$ By arbitrariness of $\epsilon$ we get that $d(a,b) = 0$, hence $d(A,B)=0$. This is a contradiction. Now we show that if $X$ cannot be partitioned into two sets $A$ and $B$ with $d(A,B) > 0$, then $X$ is Cantor-connected. Suppose that $x, y \in X$ and $\epsilon > 0$ arbitrary. Set $x_{0} = x$. Then there exists $x_{1} \in X$ such that $d(x_{0},x_{1}) \leq \epsilon$. If not, $\{x\}$ and $X \setminus \{x\}$ would form a partition with $d(\{x\}, X \setminus \{x\}) > 0$. Analogously we find $x_{2} \in X$ such that $d(x_{1},x_{2}) \leq \epsilon.$ This will give us $x_{0}, x_{1}, x_{2}, \cdots \in X$ with $d(x_{i-1},x_{i}) \leq \epsilon.$ Now we have to prove that after a finite number of steps, we get $x_{n}= y$ and $d(x_{n-1}, x_{n}) \leq \epsilon.$ Can anyone explain to me why the process in the last step stops after a finite number of steps?","['general-topology', 'connectedness', 'metric-spaces']"
239240,Differentiate then solve or vice versa. Why is there a difference?,"The other day I stumbled upon the following problem. Start with a rectangular piece of card an integer number wide, by an
  integer number long, with one of those values being prime.  Then cut
  four identical squares from each corner of the card. Fold up the four
  ""flaps"" to create an open topped box, where the corners have been cut
  in such a way to allow the maximum volume possible for the box. Once
  the box is made, it turns out that the base length is 4 times larger
  than the base width. What are the dimensions of the original card? I did solve the problem. But I still have a question. Let $L$ and $W$ be the length and width of the original paper and $x$ the length of the cut-off square. The volume of the box is a function $V(L, W, x)$. $$V(L,W,x) = x(L-2x)(W-2x)\tag{1}$$ We also have that $$L-2x = 4(W-2x)\tag{2}$$ I solved the problem by first finding the derivative of $V(L,W,x)$ with respect to $x$ and then making use of the second formula, finding a relationship between $W$ and $L$, and considering they're both whole and one of them is prime, I got the correct answers for $W$ and $L$ ($W = 3$, $L = 8$). However, if I first substituted the second formula into $V(L,W,x)$ and then differentiate, I arrived at wrong results. I mean, if I first rewrite $V$ as $$V(W,x) = 4x(W-2x)^2$$ Differentiate, find $x$ expressed with $W$, and substitute in $(2)$, I get wrong results. Why?",['derivatives']
239254,Prove this inequality from functional analysis,"I want to prove this equality used in out lecture notes: Let $D=(0,r)^2 \subset \mathbb{R}^2, r\geqslant 0$. Then, for any $u \in  H^1(D)$, there holds $$\lVert u\rVert  \leqslant \frac 1 r \left|\int_D u(x)dx\right| + \sqrt{2}r \lVert\nabla u\rVert$$ where $\lVert\cdot\rVert$ is the $L^2$-norm on $D$. I have no clue how to prove this estimate on $D$.  Can  somebody  help me?","['sobolev-spaces', 'inequality', 'functional-analysis']"
239278,Puzzle on the triangle.,"In triangle top four figures that have to be repositioned to form the ""triangle"" without a unit square. How to explain this? Thank's.","['geometry', 'puzzle', 'recreational-mathematics']"
239281,"How to prove that, for $U$ unitary, $|\det U| = 1$ but $\det U\neq \det U^H$?","Prove that unitary matrix $U$ satisfies $|\det U| = 1$, but $\det U$ is different from $\det U^{H}$. How can I prove these two statements? I guess I should use the fact that every column of unitary matirx is orthonormal, but I'm not sure where to put that...","['matrices', 'unitary-matrices', 'linear-algebra', 'determinant']"
239284,Can a differential equation have non unique solutions?,There are theorems of existence and uniqueness of differential equations. I was wondering if it is possible that a differential equations has a solution but it is not unique.,['ordinary-differential-equations']
239288,Infinite expected value of a random variable,"How can a positive random variable $X$ which never takes on the value $+\infty$, have expected value $\mathbb{E}[X] = +\infty$?","['infinity', 'probability', 'random-variables']"
239307,What good are free groups?,"In Algebra: Chapter 0 , one learns two definitions of free groups associating with sets. Let $A$ be a set, the free group of $A$, $F(A)$ is the initial object in the category $\mathcal{C}$, where \begin{equation}
\operatorname{Obj}(\mathcal{C})=\{A\xrightarrow{g}G\},
\end{equation}where the codomain $G$ are groups, and \begin{equation}
\operatorname{Hom}(\mathcal{C})=\{\text{Commutative Diagrams } A\xrightarrow{g_1}G_1\xrightarrow{\phi}G_2\xleftarrow{g_2}A\} 
\end{equation} where $\phi$ are group homomorphisms. Also $F(A)$ has the concrete construction with elements being non-redundant words with alphabet $A$, and the group multiplication being juxtaposition and reduction. But what good are free groups? Why are they useful? Wikipedia says they are useful in topology, but does not explain why explicity. Can someone give some examples that an undergraduate can understand? I am asking about examples that can show the usefulness of this abstract construction. So I guess the identification of $\mathbb{Z}=F({a})$ does not really count.","['category-theory', 'free-groups', 'group-theory', 'abstract-algebra']"
239320,How to prove that such a function is linear?,"Let $f:(a.b)\rightarrow \mathbb R$ be a continuous function. How to prove that if 
for $\varepsilon >0$ there is a $\delta >0$ such that for $x\in (a,b)$, $|h|< \delta$ such that $x+2h \in (a,b)$ $$\left|\frac{f(x+2h)-2f(x+h)+f(x)}{h^2}\right|<\epsilon, $$ then $f$ is of the form $f(x)=\alpha x+\beta$, where $\alpha,\beta$ are constants ?","['real-analysis', 'analysis']"
239324,"Is ""integrability"" equivalent to ""having antiderivative""?","I am wondering if ""a function $f(x)$ is integrable on a domain $D$"" this proposition is equivalent to ""$f(x)$ has antiderivative on domain $D$"". If it is not the case, give me a counter example. Thank you.","['integration', 'real-analysis']"
239331,elementary prove thru induction - dumb stumbling,i am trying to prove this statement for all $n \in \mathbb{N}$ with the help of induction: $4 \sum_{k=1}^{n} (-1)^kk=(-1)^n(2n+1)-1$ base case: n=1 $4 \sum_{k=1}^{1} (-1)^11=-4=(-1)^1(2*1+1)-1$ .. OK induction hypothesis: for all $n \in \mathbb{N}$ let be $4 \sum_{k=1}^{n} (-1)^kk=(-1)^n(2n+1)-1$ inductive step: $n \rightarrow n+1$ $4 \sum_{k=1}^{n+1} (-1)^kk=4 \sum_{k=1}^{n} (-1)^kk+(-1)^{n+1}(n+1)=(-1)^n(2n+1)-1+(-1)^{n+1}(n+1)=...help...=(-1)^{n+1}(2(n+1)+1)-1$ i need help for $..help..$ thanks a lot,"['induction', 'summation', 'discrete-mathematics', 'problem-solving']"
239352,Trace minimization with constraints,"For positive semi-definite matrices $A,B$, how can I find an $X$ that minimizes $\text{Trace}(AX^TBX$) under 'either' one of these constraints: a) Sum of squares of Euclidean-distances between pairs of rows in $X$ is a constant $\nu$. or b) $X$ is orthogonal. Not a hw problem- though the style of writing might suggest so. 
All the matrices have real entries and $A,B$ are square while $X$ is rectangular. Thanks. This is what I have: Define $B=F^{T}F$. Define $Y=FX$. You get the above problem as 
\begin{align}
\min_{Y}~ \text{trace}(AY^{T}Y)
\end{align} But now I want $X^*$ that minimizes the original problem. This is what is confusing me!","['optimization', 'matrices', 'qcqp', 'linear-algebra']"
239364,Surjective endomorphisms of finitely generated modules are isomorphisms,"My Problem: Let $M$ be a finitely generated $A$-module and $T$ an endomorphism. I want to show that if $T$ is surjective then it is invertible. My attempt: Let $m_1,...,m_n$ be the generators of $M$ over $A$. For every $b = b_1 m_1 + ... + b_n m_n$ with $b_i \in A$ there is $a = a_1 m_1 + ... + a_n m_n$ with $a_i \in A$ such that 
$$ T(a)=b $$
or in matrix-vector notation
$$ T \vec{a} = \vec{b} $$
where $\vec{x}$ is the column vector of $x_1,...,x_n$ where $x = x_1 m_1 + ... + x_n m_n$. I multiply by the adjugate matrix to get
$$ \mathrm{adj}(T) \vec{b} = \mathrm{adj}(T) T \vec{a} = \det(T) I_n \vec{a} = \det(T) \vec{a} \ .
$$
Now take $\vec{b}=0$. Then $\vec{0} = \det(T) \vec{a}$ and thus $T$ is injective if and only if
$\det(T)$ is not a zero divisor. If I prove that $T$ is injective, then I'll get it is invertible. For that, I think the way is to prove that $\det(T)$ is not a zero divisor. The importance of finitely generated condition: Let $M = A^{\aleph_0} =\{ ( a_1 , a_2 , ... ) \mid a_i \in A \}$ be a not finitely generated $A$-module. Let $T : M \to M$ defined by
$$ T(a_1, a_2, a_3, ... ) = (a_2, a_3, ... ) \ . $$
Then clearly $T$ is surjective but not injective ($\ker T = \{ ( a , 0 , 0 , ... ) \mid a \in A \}$),
and thus not invertible. The importance of surjective and not injective condition: Need to find a counter-example.","['modules', 'commutative-algebra', 'abstract-algebra']"
239380,exercise of derivates and divisibility,How to prove that if $p$ is a prime number for all $i\geq p$ and $k\geq 0$ the coefficient of $$\frac{d^i}{dx^i}\left(\frac{x^{p+k}}{(p-1)!}\right)$$ is a integer number multiple of $p$,['derivatives']
239420,Calculus 3 Explained,"I am trying to learn some calculus 3 and I understand HOW to do the problems but I just don't understand WHY I'm doing what I'm doing. So does anyone have any good recommendations on books that are really down to earth, and explain the concepts in terms that humans can understand. Here are the topics that I want to understand: A. Calculation of Geometric Quantities
   1) Surface Area
   2) Arc Length
   3) Curvature of Paths
      a) velocity
      b) speed
      c) acceleration
         i. tangential component
         ii. normal component
B. Line integrals of vector fields
   1) fundamental theorem of line integrals
   2) green's theorem
   3) finding the underlying scalar field for a conservative vector field
   4) direct calculation of a line integral","['multivariable-calculus', 'calculus', 'reference-request', 'integration']"
239456,Why do number rings have no endomorphisms,"This question is about the analogy between number fields and function fields. It's a soft question and the title  misrepresents the question. Consider the projective line over a field. This has many endomorphisms of degree $>1$, e.g., $x\mapsto x^n$. Now, a well-known analogy states that $\mathbf{P}^1_{\mathbf{F}_p}$ is ""similar"" to Spec $\mathbf{Z}$. But $\mathbf{Z}$ has no endomorphisms. I don't know really how to phrase this question, but here the analogy seems to break down and I was just wondering why. If I'm not mistaken, for any number ring $O_K$, every endomorphism is an automorphism.","['algebraic-geometry', 'algebraic-curves', 'arithmetic-geometry', 'soft-question', 'function-fields']"
239481,Measure Convergence Version of Lebesgue Dominated Convergence Theorem,"I want to prove that LDCT(Lebesgue Dominated Convergence Theorem) continues to hold if I replace the hypothesis $f_n \to f$ (convergence pointwise) with $f_n\to f$ (convergence in measure):
    $$\int fd\lambda=\lim_{n\to\infty}\int f_nd\lambda.$$","['lebesgue-integral', 'measure-theory', 'convergence-divergence', 'real-analysis']"
239491,Time to find first copy of a duplicate,"Consider a random process which samples uniformly with replacement from [n]. The expected time to find a duplicate is a constant factor times $\sqrt{n}$. This is a version of the famous Birthday Problem. How can one find the expected time to find the first copy of the first duplicate? This will obviously occur some time before. Also, what is the distribution of this time? If you fix the position of the later copy of the duplicate then the earlier copy seems to occur uniformly before it but I am not sure how helpful that is to answer the questions. By time I simply mean that each sample takes one unit of time so the time is just the number of samples at that point. This is not a question about algorithms or computation.",['probability']
239493,Cauchy product and the exponential function,"Simplify the following series using the Cauchy product $$\sum\limits_{k=1}^\infty\frac{1}{k!}\cdot\sum\limits_{j=1}^\infty\frac{1}{j!}$$ $$\sum\limits_{k=0}^\infty\frac{(-1)^kx^{2k}}{(2k)!}\cdot\sum\limits_{j=0}^\infty\frac{(-1)^jx^{2j+1}}{(2j+1)!}$$ Which relations did you compute with these two series? Problem 1 First of all we define
$$a_k=\frac{1}{k!}\quad\text{and}\quad b_j=\frac{1}{j!}$$
and now we have the series
$$\left(\sum\limits_{n=0}^\infty c_{n}\right)=\left(\sum\limits_{k=0}^\infty a_{k+1}\right)\left(\sum\limits_{j=0}^\infty b_{j+1}\right)$$
where I think that
$$c_n=\sum\limits_{m=0}^na_{n-m+1}b_{m+1}=\sum\limits_{m=0}^n\frac{1}{(n-m+1)!}\cdot\frac{1}{(m+1)!}=\sum\limits_{m=0}^n\frac{\binom{n}{m+1}}{n!}$$ however I thought that it will be something like $\exp(1)\cdot\exp(1)=\exp(2)$ but the indexes $k$ and $j$ start at $1$ and not $0$ so I am really confused how to simplify my solution (is it even correct?) and what the result will be. I would like to hear some hints what to do here. Problem 2 Knowing from Wolframalpha that the series equals to $\sin(x)\cos(x)=1/2\sin(2x)$ I computed this: $$\begin{align}
\sum\limits_{k=0}^\infty\frac{(-1)^kx^{2k}}{(2k)!}\cdot\sum\limits_{j=0}^\infty\frac{(-1)^jx^{2j+1}}{(2j+1)!}
&= \sum\limits_{n=0}^\infty\sum\limits_{k=0}^n(-1)^k\frac{x^{2k}}{(2k)!}(-1)^{n-k}\frac{x^{2(n-k)+1}}{(2(n-k)+1)!} \\
&= \sum\limits_{n=0}^\infty(-1)^n\frac{1}{(2n+1)!}\sum\limits_{k=0}^n\binom{2n+1}{2k}x^{2k}x^{2(n-k)+1} \\
&= \sum\limits_{n=0}^\infty(-1)^n\frac{4^nx^{2n+1}}{(2n+1)!}
\end{align}$$ How should I do the last steps?","['trigonometry', 'power-series', 'sequences-and-series', 'real-analysis']"
239521,Why no trace operator in $L^2(\Omega)$?,"We have trace operator which allows us to define boundary values of an $H^1$ function. This is because of the fact that $C^\infty$ is dense in $H^1$ under the $H^1$ norm, I believe. I'm sure either $C^0$ or $C^\infty$ is also dense in $L^2$ in the $L^2$ norm, so why no trace operator in this case? Or am I wrong?","['operator-theory', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
239523,How to show how primorials grow asymptotically?,"The primorial $p_n\# $ is defined as the product of the first $n$ primes:
$$p_n\# = \prod_{k = 1}^n p_k.$$
Asymptotically, primorials grow like
$$p_n\# = e^{(1 + o(1))n\ln n)}.$$
How does one derive this asymptotic formula?","['asymptotics', 'number-theory']"
239562,Writing functions without 'x' - using only composition and partial-application of other functions?,"For example, could the polynomial: $f(x) = x^2 + 3x$ Be written without $x$, using only the exponential, addition and multiplication functions, and high-order functions such as composition and partial application?","['notation', 'functions', 'function-and-relation-composition']"
239566,Subset of a finite set is finite,"We define $A$ to be a finite set if there is a bijection between $A$ and a set of the form $\{0,\ldots,n-1\}$ for some $n\in\mathbb N$. How can we prove that a subset of a finite set is finite? It is of course sufficient to show that for a subset of $\{0,\ldots,n-1\}$. But how do I do that?",['elementary-set-theory']
239575,$\sum_{n=1}^{\infty} E|X_n - X| < \infty$ imples $X_n$ converges to $X$ almost surely,"$\sum_{n=1}^{\infty} E|X_n - X| < \infty$ imples $X_n$ converges to $X$ almost surely I'm sure this is an obvious one line proof and I'm being stupid here, but I cannot see how to show the above. I can see that $\sum_{n=1}^{\infty} P(|X_n - X|>\epsilon) < \infty$ would give the result via the Borel Cantelli lemma, am not sure how I can use that fact. Could I maybe approximate $|X_n - X|$ by simple functions? Then turn the expected value operator into a probability.","['probability-theory', 'probability', 'real-analysis']"
239596,Algebraic definition of blow-ups,"Let $X$ be a scheme. Choose $C\subset X$ be a subscheme of $X$ and let $\mathcal{I}\subset \mathcal{O}$ be the corresponding ideal sheaf. Then $\mathcal{B}=\oplus_{d\ge0}\mathcal{I}^d$ is a sheaf of $\mathcal{O}$-algebra The blow-up of $X$ along $C$ is defined as 
$$
Y=Proj \mathcal{B} \rightarrow X. 
$$
My question is, how can one understand $Proj \mathcal{B}$ to $see$ geometric description of blow-up? More precisely, when both $X$ and $C$ are smooth complex variety, $Y$ is obtained by replacing $C$ by $\mathbb{P}(N_{C/X})$, but I cann't really see this description from $Proj \mathcal{B}$. THank you for your help.","['blowup', 'algebraic-geometry']"
239600,Is every connected metric space with at least two points uncountable? [duplicate],"This question already has answers here : Connected metric spaces with at least 2 points are uncountable. (8 answers) Closed 10 years ago . As the topic, how to prove that every connected metric space with at least two points uncountable? Of course i know the definition that a countable set mean there is a bijection between the set and the positive integer. Connected is opposite of disconnected where the set can partition into two disjoint open sets.","['general-topology', 'analysis']"
239603,What are the dimensions of the singular vectors matrices in the singular value decomposition?,"Consider a $p\times q$ matrix $Y$ with rank $r$ . I read in a paper that the SVD can be written as $$Y=U_0 \Sigma_0 V^*_0 + U_1 \Sigma_1 V^*_1,$$ where $U_0$ and $V_0$ are the singular vectors associated with the singular values greater than $\tau$ , and $U_1$ and $V_1$ are the singular vectors associated with singular values less than $\tau$ . Also, $*$ denotes the transpose of a matrix. Could someone tell me what the dimensions of the various submatrices are so that the decomposition is true? You can assume whatever you want about the number of eigenvalues greater than $\tau$ and the number of eigenvalues less than $\tau$ . I'm trying to understand how the original $Y$ can be written that way. I understand that it can be written
as $Y = U \Sigma V^*$ which is the regular SVD but I don't understand how this regular SVD can be broken down into the form at the top with the sub-matrices. Thanks. Also, if there's a paper or book that shows it, that's fine also.","['matrix-decomposition', 'svd', 'matrices', 'linear-algebra', 'singular-values']"
239624,"Why is $\operatorname{Var}(X+Y) = \operatorname{Cov}(X,X) + \operatorname{Cov}(X,Y) + \operatorname{Cov}(Y,X) + \operatorname{Cov}(Y,Y)$","I know $\operatorname{Cov}(X,Y) = E[(X-u_x)(Y-u_y)]$ and $$
\operatorname{Cov}(X+Y, Z+W) = \operatorname{Cov}(X,Z) + \operatorname{Cov}(X,W) + \operatorname{Cov}(Y,Z) + \operatorname{Cov}(Y,W),
$$ but how does one get $$
\operatorname{Var}(X+Y) = \operatorname{Cov}(X,X) + \operatorname{Cov}(X,Y) + \operatorname{Cov}(Y,X) + \operatorname{Cov}(Y,Y)?
$$","['statistics', 'probability']"
239633,"In terms of complexity, is there a quicker way of checking if a matrix is nonsingular than computing the determinant?","To repeat the question, let $A$ be a square matrix. We wish to determine if $A$ is nonsingular, that is, invertible. One way is compute its determinant and check if it is nonzero. However, if $A$ is invertible, calculating its determinant gives us strictly more information that knowing that it is nonzero. Although the naive complexity for calculating the determinant is $O(n!)$, faster $O(n^3)$ algorithms exist. It does not seem unreasonable to suppose that their might be an algorithm that checks for nonsingularity that has strictly lower complexity than the fastest algorithms known for computing the determinant. Is such an algorithm known to exist? Can such an algorithm exist? (I am quite ignorant in these areas of computational complexity) I am primarily interested in the case where $A$ is a real or complex matrix, but the case of rational matrices, or matrices over finite fields are also of interest. Feel free to discuss matrices over an arbitrary ring if you think it can clarify this discussion. Thanks in advance for any insight you can spare!","['numerical-linear-algebra', 'linear-algebra', 'computational-complexity']"
239667,Surface of a 2-sphere expressed as union of two closed disks,"I'm reading a First Course in Differential Geometry by Chuan-Chih Hsiung and on page 8 he says ""A closed disk that is homeomorphic to $I^2$ [i.e. $I\times I$, where $I = [a, b]$] is connected. The surface $S^2$ of a 2-sphere can be expressed as the union of two closed disks with nonempty intersection."" I'm not sure what he means by the second sentence. Am I supposed to imagine two disks being deformed into the two halves of the sphere (so the disks touch each other at their circumferences)? I don't understand what it means to express the spherical surface as a union of two disks.","['general-topology', 'differential-geometry']"
239677,"If $Q$ is an orthogonal, then is $Q+\frac{1}{2}I$ invertible?","If $Q$ is an orthogonal matrix, then the matrix has orthonormal columns. I asked this question to my friend and he says: Let $Q= -\frac{1}{2}I$, then it is orthogonal, and $Q+\frac{1}{2}I$ is zero, so not invertible. But I think if we set $Q=-\frac{1}{2}I$, then it is not an orthogonal matrix since it doesn't have unit length, right? Does anyone know whether it is true or false? Let me know the reason or counter example.",['linear-algebra']
239684,Linear combinations of sequences of uniformly integrable functions,"Let $\{f_n\}$ and $\{g_n\}$ be sequences of uniformly integrable functions on $E$.  Show that for $\alpha$, $\beta$, the sequences $\alpha f_n + \beta g_n$ are also uniformly integrable. Attempt at a proof: Since both sequences are uniformly integrable I can find a $\delta=\min\{\delta_{g_n}, \delta_{f_n}\}$ such that $|\int_{A_{f_n}\cup A_{g_n}} f_n +g_n|< \epsilon$. I would like to know how to show this if I knew that $f_n$ would converge pointwise to $f$.  I'm not given this detail however. Edit: A family $\mathscr{F}$ of measurable functions on $E$ is said to be uniformly integrable over $E$ provided for each $\epsilon >0$, there is a $\delta >0$ such that for each $f \in \mathscr{F}$, if $A \subseteq E$ is measurable and $m(A)<\delta$, the $\int_A |f|< \epsilon$.",['real-analysis']
239688,Very basic question about the definition of continuous of a functions,"Suppose say $f:\{0,1\}\to \{1,2\}$ is $f$ continuous? Say $f(0)=2,f(1)=1$ I know the definition of continuous function. In my point of view, i think it is continuous as we can simply take $\epsilon=\delta$, so for any $x\in \{0,1\},|x-1|<\delta \implies |f(x)-f(1)|<\epsilon$. Is it correct?","['functions', 'continuity', 'real-analysis', 'analysis']"
239703,"If $A$ is singular, is $A^3+A^2+A$ singular?","Suppose that $A$ is singular, is $A^3 + A^2 + A$ singular as well?","['matrices', 'linear-algebra']"
239724,Tangent space of a point of an algebraic variety,"Let $V$ be a non-singular affine variety in $\mathbb{C}^n$.
$V$ can be regarded as a complex manifold.
Let $p = (a_1,\dots,a_n) $ be a point of $V$.
Let $\mathcal{O}_p$ be the local ring of $V$ at $p$.
A tangent vector $v$ at $p$ is a derivation $\mathcal{O}_p \rightarrow \mathbb{C}$, i.e.
a $\mathbb{C}$-linear map $v$ such that $v(fg) = v(f)g(p) + f(p)v(g)$ for $f, g \in \mathcal{O}_p$.
Let $T_p$ be the set of tangent vectors at $p$.
We regard $T_p$ as a vector space over $\mathbb{C}$ in the obvious way. On the other hand, we can define a tangent space at $p$ as follows.
Let $f_1,\dots f_r$ be defining polynomials for $V$.
Let $L_i$ be the hyperplane defined by $\sum_k \frac{\partial f_i}{\partial x_k}(p)(x_k - a_k) = 0$.
Let $S_p = \bigcap_i L_i$. Are $T_p$ and $S_p$(or rather the vector space attached to it) canonically isomorphic?",['algebraic-geometry']
239726,Atiyah-Macdonald Chapter 3 Qn 19 viii,"I am having a problem with a question in Atiyah-Macdonald Chapter 3 qn 19(viii). The question is as follows: If $f:A\rightarrow B$ is a ring homomorphism, and $M$ is finitely generated $A$-module, then $\mbox{Supp}(B\otimes_{A}M)=f^{*-1}(\mbox{Supp}(M))$ I tried to show $\mbox{Supp}(B\otimes_{A}M)\subseteq f^{*-1}(\mbox{Supp}(M))$. Suppose ${\frak{p}}\notin f^{*-1}(\mbox{Supp}(M))$, then $f^{-1}(\mathfrak{p})\notin \mbox{Supp}(M)$. Thus for each generator $m_{i}$ of $M$, there exists $s_{i}\in A-f^{-1}(\mathfrak{p})$ such that $s_{i}m_{i}=0$. If $f(s_{i})=0$ then since $f(s_{i})\in B-\mathfrak{p}$, thus $0\in B-\mathfrak{p}$ and we are done. Otherwise for all $f(s_{i})$ if they are not zero, we have $f(s_{1})f(s_{2})\ldots f(s_{k})(b\otimes m)=b\otimes (s_{1}s_{2}\ldots s_{k})m=0$ But I am stuck with $f^{*-1}(\mbox{Supp}(M))\subseteq \mbox{Supp}(B\otimes_{A}M)$. The problem is if I start letting $\mathfrak{p}\in LHS$, so each of $m_{i}$ is not zero, but I can't show that $1\otimes m_{i}$ is not zero. Or if I start from $\mathfrak{p}\notin RHS$, I can't show that $(A-f^{-1}\mathfrak{p})M=0$. How should I approach? Thanks!","['commutative-algebra', 'abstract-algebra']"
239734,A Book for abstract Algebra,"I am self learning abstract algebra. I am using the book Algebra by Serge Lang. The book has different definitions for some algebraic structures. (For example, according to that book rings are defined to have multiplicative identities. Also modules are defined slightly differently....etc) Given that I like the book, is it OK to keep reading this book or should I get another one? Thank you","['self-learning', 'book-recommendation', 'reference-request', 'abstract-algebra']"
239750,Prove that R is a ring under 'special' definitions of multiplication and addition,"Question: Let R be a ring with a 1. Define $\bar R$ to have the same elements of R with addition 
$$\oplus: a \oplus b = a +b +1$$ andmultiplication $$\otimes: a \otimes b = ab + a +b$$ Prove that $\bar R$ is a ring under $\oplus$ and $\otimes$ and has a 1. My Attempt: My understanding of a ring is it should be a non-empty set together with operations addition and multiplication satisfying the following conditions: R is an Abelian group under multiplication R must be closed and associative under multiplication Multiplication must be distributive over addition i.e. $a(b+c) = ab + ac$ and $(a+b)c= ac +bc$ So I try to show that all those conditions are satisfied: $$1$$ $$ a \otimes b = ab + a + b$$
$$ b \otimes a = ba + b +a $$
In order for $a \otimes b$ to be equal to $b \otimes a$ it must be that $ba = ab$ (how do I prove this? - do I even need to prove this?) $$2$$ Closed: $$ a \otimes b = ab + a + b$$ if $a, b \in \bar R$ then $ab, \in \bar R$, $(a + b) \in \bar R$ and therefore $ab + a + b$ must be in $ \bar R$. (Do I need to prove this? Or is that pattern of thought good enough?) Associative: let's introduce an element $c \in \bar R$ and examine the associativity property. $$ \begin{align} 
(a \otimes b) \otimes c & = \ (ab + a + b) \otimes c\\
& = \ abc + ac + bc + ab + a + b + c \\
& = \ a \otimes (bc + b + c) \\
& = \ a \otimes (b \otimes c) \\
& = \ a \otimes b \otimes c \\
\end{align} $$ So it is indeed associative under multiplication. $$3$$ $$ \begin{align}
a \otimes (b \oplus c) & = \ a \otimes (b + c + 1) \\
& = \ ab + ac + a + a + b + c + 1 \\
& = \ ab + b + a + ac + a + c + 1 \\
& = \ a\otimes b \oplus \ a \otimes c \\
\end{align} $$ And I'm sure if I tinker around with the $(a + b)c = ac + bc$ equation I will find that it is also true so it does satisfy the distributive law. The second part of the question demands that I show that $ \bar R$ has a 1. I understand a 1 to have the property: $a\times1= a$. So considering it in this ring we're looking for a $x$ that does this: $$a \otimes x = a$$ 
$$ a \otimes x = ax + x + a = a$$
Just from looking at the equation and using my intuition one can see that when $x=0$ the equation is true. So I would say that x=0 is the 1 of this ring. 0 is the multiplicative identity of $\bar R$ My Concerns: Can I conclude after having done all this that $\bar R$ is in fact a Ring?
Have I answered the question sufficiently enough?","['ring-theory', 'abstract-algebra', 'abelian-groups']"
239751,"What is an example of a situation where AB is not subgroup of G, when A, B are subgroups of G?","What is an example of a situation where AB is not subgroup of G, when A, B are subgroups of G? My first instinct is always to go for some dihedral group or other...But I could not find an example of a case when the above is true. Maybe I am doing it wrong...","['examples-counterexamples', 'finite-groups', 'group-theory', 'abstract-algebra']"
239760,Given the Hasse diagram tell if the structure is a lattice,"Let's consider the following Hasse diagram: I need to tell whether this is a lattice. By lattice definition I can prove the above shown structure $M_5$ to be a lattice if and only if $\forall x,y \in M_5$, $\{x,y\}$ has supremum and infimum in $M_5$. Putting all such subsets in a table, not mentioning those subset where $x=y$: $$\begin{array}{|c || c | c|}
\hline
Subset & x \wedge y & x \vee y \\
\hline
\{a,b\} & b & a \\
\{a,c\} & d & e \\
\{a,d\} & d & a \\
\{a,e\} & a & e \\
\{b,c\} & d & e \\
\{b,d\} & d & b \\
\{b,e\} & b & e \\
\{c,d\} & d & c \\
\{c,e\} & c & e \\
\{d,e\} & d & e \\
\hline
\end{array}$$ So the $M_5$ is a lattice. Is my reasoning in detecting supremum and infimum for each given subset correct? Have I come up with the right conclusion?","['lattice-orders', 'elementary-set-theory', 'abstract-algebra', 'order-theory']"
239765,Prove that the unit open ball in $\mathbb{R}^2$ cannot be expressed as a countable disjoint union of open rectangles.,"Prove that the unit open ball in $\mathbb{R}^2$ cannot be expressed as a countable disjoint union of open rectangles.
Open rectangles in $\mathbb{R}^2$ are subsets of the form $(a,b)\times(c,d)$. Thanks a lot!","['general-topology', 'real-analysis']"
239780,Function whose inverse is also its derivative?,"What are some good examples of a function  $f : \mathbb{R} \to \mathbb{R}$ where its derivative is equal to its inverse? I attempted to find a monomial that satisfied it by starting with $f(x) = ax^b$ and showing that $f^{-1}(x) = f'(x) \implies b-1=\frac{1}{b} \implies b=\phi$ and got
$$f(x) = \frac{x^\phi}{\sqrt[\phi]{\phi}}$$
Which seems to work according to WolframAlpha, but I'm having trouble double-checking it. Any other ideas?","['ordinary-differential-equations', 'functional-equations']"
239783,Maps - question about $f(A \cup B)=f(A) \cup f(B)$ and $ f(A \cap B)=f(A) \cap f(B)$,"I am struggling to prove this map statement on sets. The statement is: Let $f:X \rightarrow Y$ be a map. i) $\forall_{A,B \subset X}: f(A \cup B)=f(A) \cup f(B)$ ii) $\forall_{A,B \subset X}: f(A \cap B) \subset f(A) \cap f(B)$ iii) $f$ is injective $ \Longleftrightarrow$ $\forall_{A,B \subset X}: f(A \cap B)=f(A) \cap f(B)$ My problem is: I know how to operate on sets, I know how to operate on sets, but I don't know how and where to start the proof,  the biggest problem in mathematics, I think. Thanks for help!","['proof-writing', 'elementary-set-theory', 'functions', 'problem-solving']"
239792,Double Expectation of a Conditional Expectation,"If there are two sub-sigma algebras $\mathcal{G}$ and $\mathcal{H}$ of $\mathcal{F}$, neither a subset of the other from a probability space $(Y,\mathcal{F},P)$ and a random variable $X$ which is not measurable with respect to either $\mathcal{G}$ or $\mathcal{H}$, can I apply double expectation on the conditional expectation of $X|\mathcal{G}$ like this: $$
E[E[X|\mathcal{G}]|\mathcal{H}] = E[X|\mathcal{G}]
$$ Thanks.","['probability-theory', 'measure-theory', 'probability']"
239794,"Axiomatically define a function that can solve otherwise impossible differential equations, like $i$ solves otherwise impossible polynomial equations","We all know that roots of polinomials are not always real numbers or when we take the square root of a negative number, we need to immmediately define an imaginary number called $i$ or $j$ for futher calculations. There is also an analogy between a polynomial and ordinary differential equations, one of which has a solutions with a number and the other with a function . According to 1. and 2. it is a strightforward question to ask if there is a definition of a function called imaginary function for futher possible computations? Or does it make sense? EDIT : As it might be confusing with a complex function $f(a+bj)$; I want a basic function like the imaginary number $i$ as a solution to unsolvable equations and via use of this function, I want to have some solutions to other unsolvable differential equations. Like for the number $i$, and a polynomial.","['elementary-number-theory', 'functions']"
239808,"Integral of cosec squared ($\operatorname{cosec}^2x$, $\csc^2x$)","According to my sheet of standard integrals, $\int \csc^2x \, dx = -\cot x + C$. I am interested in a proof for the integral of $\operatorname{cosec}^2x$ that does not require differentiating $\cot x$. (I already know how to prove it using differentiation, but am interested in how one would calculate it without differentiation.)","['trigonometry', 'integration']"
239839,"Any partition of $\{1,2,\ldots,9\}$ must contain a $3$-Term Arithmetic Progression","Prove that for any way of dividing the set $X=\{1,2,3,\dots,9\}$ into $2$ sets, there always exist at least one arithmetic progression of length $3$ in one of the two sets.","['ramsey-theory', 'combinatorics']"
239843,Wedge product with a non-degenerate form,"Let $\alpha$ be a non-degenerate form in $\Lambda^k(V)$ for some vector space $V$, $\dim V = n$. (Here non-degenerate means that if $x\in V$ is nonzero, then $(y_1 , ... , y_{k-1}) \mapsto \alpha(x , y_1 , ... , y_{k-1})$ is a nonzero form). Is it true that if $\beta$ is a nonzero $\ell$-form for $\ell  + k\leq n$, then $\alpha \wedge\beta \not=0$? Is this true under stricter conditions on $\beta$ (e.g. $\beta$ also non-degenerate)?","['exterior-algebra', 'differential-forms', 'differential-geometry']"
239849,Answering Questions For A Poset.,"The question I am looking at is, ""Answer these questions for the poset $(\{3,5,9,15,
24,45\},|)$."" a) Find the maximal elements. b)Find the minimal elements. c) Is there a greatest element? d) Is there a least element? e) Find all upper bounds of $\{3,5\}$. f) Find the least upper bound of$\{3,5\}$, if it exists. g) Find all lower bounds of $\{15,45\}$. h)Find the greatest lower bound of $\{15,45\}$, if it exists. I have answers, I just want to make certain that I answered them properly, used the proper reasoning to answer them, and used the terminology correctly. For a): For elements in the poset to be a maximal element, it has to be divisible by all the elements it is comparable to. The elements 24 and 45 are maximal, because they are divisible by every element they are comparable to. The reason why neither maximal element is comparable to the other is because $24|45$ and $45|24$ are both false statements. For b): For elements in the poset to be minimal, is for them to be able to divide all other elements, in the relation, they are comparable to. $3|5$ and $5|3$ are both false statements, meaning they are incomparable elements, and since they divide every element they are comparable, they are minimal elements of the poset. For c): No, because 24 and 45 are incomparable elements, meaning one doesn't precede the other. For d): No, because 3 and 5 are incomparable elements, meaning one doesn't precede the other. For e): To find the upper-bound of the set $\{3,5\}$, with respect to the poset, is to find all of the elements that both 3 and 5 divide. These elements are 15 and 35. For f): To find the least upper-bound of the set $\{3,5\}$, is to consider the upper-bounds, and find the one that divides the others. $15|45$, so 15 is the least upper-bound. For g): To find the lower bounds of the set $\{15,45\}$, with respect to the poset, is to find the eleents that divide into 15 and 45. These elements are 3,5, and 15. For h): To find the greater lower bound of the set $\{15,45\}$, is to consider the lower bounds, and see which one is divisible by them all. $3|15$ and $5|15$, therefore 15 is the greatest lower bound. I have a few other questions. If you have more than one extremal element, is it possible to have a greatest or least element? For parts a through d, I used this comparability argument; but now I don't feel so confident that it was the proper argument to use. If the comparability argument is correct, could someone explain to me why?--I answered this question yesterday, yesterday was so long ago.","['relations', 'discrete-mathematics', 'elementary-set-theory']"
239861,Limit problem for $L^p$ function,"I am having problems with proving the following: Let $f$ be a $L^p$ function on $[0,1]$ , $f:[0,1] \to \overline{\mathbb{R}}$ . Prove that $$\lim_{t \to \infty} t^p \mu(x: |f(x)| \geq t) = 0.$$ I know that the right-hand side value is always finite for each $t \in \mathbb{R}$ due to Chebyshev's inequality. I was also able to prove that $\lim_{t \to \infty} \mu(x: |f(x)| \geq t) = 0$ . But unfortunately can find anything about how fast this value goes to $0$ as $t \to \infty$ . Would be grateful, if you could give an idea and help with this.","['measure-theory', 'lp-spaces', 'real-analysis', 'limits']"
239862,Boundedness and pointwise convergence imply weak convergence in $\ell^p$,"Let $p\in(1,+\infty)$ and consider the space $\ell^p$ with its usual norm. The following are equivalent: (1) $x_n \rightharpoonup x$ (i.e. $x_n$ weakly converges to $x$); (2) $$\exists M>0: \quad \sup_{n} \Vert x_n \Vert_p \le M \quad \text{ and } \quad \forall k\in\mathbb N: \,\,\ x^{(k)}_n\to x^{(k)}$$ I think I've proved (1) $\Rightarrow$ (2) : indeed, every weak convergent sequence is bounded (hence we get $\sup_{n} \Vert x_n \Vert_p \le M$ for some positive $M$); on the other hand, we can simply observe that the ""projections"" $p_k \colon \ell^{p} \to \mathbb R$ definde by $x=(x_n)_{n=1}^{\infty} \mapsto x_k$ are linear and continuous. So, briefly, we have that weak convergence implies boundness and pointwise convergence. What about (2) $\Rightarrow$ (1) ? I'm really puzzled and I don't know how to begin.
Thanks in advance.","['normed-spaces', 'convergence-divergence', 'functional-analysis']"
239863,Study of a series of functions,"I've to study this series: $$\sum_{n=1}^\infty e^{\sqrt n\,x}$$ My teacher wrote that with the asymptotic comparison with this series: $$\sum_{n=1}^\infty\frac{1}{n^2}$$ My series converges for every $$x<0$$ I don't understand the motivation, hoping for someone to enlighten me! =) Thanks. Leonardo.","['convergence-divergence', 'sequences-and-series', 'functions', 'asymptotics', 'uniform-convergence']"
239875,Element chasing proof that $(A\setminus C)\setminus(B\setminus C) = (A\setminus B)\setminus C$,"I would like to construct a formal proof of the following: $$(A\setminus C)\setminus(B\setminus C) = (A\setminus B)\setminus C$$ Let $a∈A$ be an arbitrary element, we will show that $a\in A \cap \overline B \cap \overline C$. For LHS, since $a\in A$, we have that $a\in (A \cap  \overline C) \cap  (\overline B \cap \overline C)$. This is equivalent to $a\in (A \cap  \overline B) \cap \overline C$. For RHS, since $a\in A$, we have that $a\in (A \cap  \overline B) \cap \overline C$ $\therefore lhs \equiv rhs$ and this concludes the proof I would be grateful for any feed back on this element chasing proof. Is it flawed or where should improvements be made? Thanks",['elementary-set-theory']
239893,"Inverse Gaussian, Limiting Distributions","I'm trying to understand the nature of limiting distributions and distributions, specifically $1/Z_n \longrightarrow ~?$ where $Z_n\longrightarrow Z -Gaussian(0,1)$ I understand that the gamma distribution converges to the gaussian for a large enough $n$, so would it be inverted gamma until the $n$ is sufficiently large? But the wikipedia article for Inverted Gamma and Inverse Gaussian are completely different, even though they're both written as $IG( ~, ~)$ But I also have the same distribution as Inverted Gamma with $\alpha,\beta$ as parameters Attempts: 
Let $Y=1/N$ where $N=Gaussian(0,1)$ $F(Y)=P(Y<y)=P(1/N<y)=P(N>1/y)=1-F(1/y)$ But this gets me nothing that looks like what Wikipedia has http://en.wikipedia.org/wiki/Inverse_Gaussian_distribution",['statistics']
239894,Complex analysis integration with residues.,"I have to show that $$\int_{0}^{2\pi}\frac{d\theta}{(a^{2}\cos^{2}\theta+b^{2}\sin^{2}\theta)^{2}} =\frac{ \pi(a^{2}+b^{2})}{a^{3}b^{3}}$$ where $a,b>0$. I have tried using double angle formulas and Euler's trig identities to simplify this in order to use either residues or Cauchy's integral formula, but everything I have tried has made it messier and messier. Either I'm trying to simplify some things too early or something, but any help or tips would be much appreciated. Okay, I made the substitutions, and right now just focusing on the denominator, I got to $$(a^{2}(z+z^{-1})^2-b^{2}(z-z^{-1})^2)^2$$ (i took out the common factor of 4 that I got). Then here is where I'm stuck. It looks like I have a difference of 2 perfect squares, so I then wrote $$((a(z+z^{-1})-b(z+z^{-1}))(a(z+z^{-1})+b(z+z^{-1})))^2$$ which I then wrote as $$(a(z+z^{-1})-b(z+z^{-1}))^2(a(z+z^{-1})+b(z+z^{-1}))^2$$ If I did it right, which I hope I did, Now I'm not sure where to go from here into getting a rational function.","['residue-calculus', 'complex-integration', 'complex-analysis']"
239898,simple question involving trigonometry,"Can anybody explain what $$\tan(\sin^{-1}(\frac x y))$$ equals? I have to determine whether $$y'' \left(\tan \left(\sin^{-1}\left(\frac x y \right)\right) - \frac{x}{\sqrt{y^2-x^2}} \right)=0$$ is a linear differential equation. I thought that the left term might be equal to the right term, but I guess not.","['trigonometry', 'ordinary-differential-equations']"
239946,Finding the Fourier transform of $f(x) = \frac{a}{\pi} \frac{1}{a^2 + x^2}$ with the residue theorem,"I keep getting the wrong answer for this problem! Find the Fourier transform of $f(x) = \frac{a}{\pi} \frac{1}{a^2 + x^2}$ using the residue theorem. Well, by definition: $$\hat f(k) = \frac{1}{\sqrt{2\pi}} \frac{a}{\pi}\int_{-\infty}^{+\infty}\frac{e^{-ikx}}{a^2 + x^2}\mathrm{d}x$$ I define the complex function: $$g(z) \doteqdot \frac{e^{-ikz}}{a^2 + z^2} = \frac{e^{-ikz}}{(z-ai)(z+ai)}$$ Let's pick the simple pole at $z=ai$; the residue is: $$\text{Res}(g,ai) = \lim_{z \to ai}\frac{e^{-ikz}}{z+ai} = \frac{e^{ak}}{2ai}$$ Now for a contour, choose a line segment in the real axis from -R to +R and an arc of a circle of radius R centred at the origin connecting the two ends of the segment. This contour includes the pole ai. As R tends to infinity, the integral over the arc vanishes (Jordan's lemma) and the integral over the segment becomes an integral over the real line. So, by the residue theorem: $$\int_{-\infty}^{+\infty}\frac{e^{-ikx}}{a^2 + x^2}\mathrm{d}x = 2\pi i\frac{e^{ak}}{2ai} = \pi\frac{e^{ak}}{a}$$ The Fourier transform then is:
$$\hat f(k) = \frac{1}{\sqrt{2\pi}}\frac{a}{\pi} \pi\frac{e^{ak}}{a}= \frac{1}{\sqrt{2\pi}}e^{ak}$$ Which is wrong; the correct answer is $\frac{1}{\sqrt{2\pi}}e^{-a|k|}$ but I don't see how the absolute value can pop up.","['residue-calculus', 'fourier-analysis', 'complex-analysis']"
239949,Why do we require absolute convergence in the definition of expectation?,"I am studying some basic probability and the lecture have defined
the expectation of a $R.V$ ,$X$, in the cases that $X$ is discrete
or continues. In both definitions we first required some absolute convergence, for
example, in the continues case we required that $\int|x|f(x)dx<\infty$
and defined the expectation to be $\int xf(x)dx$. What is the reason for this requirement ? if $\int xf(x)dx$ then
it seems that it should be called the expectation regardless of if
$\int|x|f(x)dx<\infty$ is convergent.",['probability-theory']
239959,"Conormal Sheaf (Morphisms of Schemes, Stacks Project)","The first part of this question refers to Lemma 33.2 from the chapter ""Morphisms of Schemes"" of the Stacks Project. In particular, if $i: Z \rightarrow X$ is an immersion and $\mathcal{I}$ is the corresponding ideal sheaf, then the conormal sheaf is $C_{Z/X} = i^*(\mathcal{I})$. What i don't see is why $i^*(\mathcal{I}) = i^{-1}(\mathcal{I}/\mathcal{I}^2)$. My efforts: if i apply the definition of the pullback $i^*$ i get 
$i^*(\mathcal{I}) = i^{-1} \mathcal{I} \otimes_{i^{-1}O_X} O_Z$. Additionally, i also see that if $R$ is a ring and $I$ some ideal then $I/I^2 = I \times_R R/I$. But i am having trouble combining these two facts to obtain $i^{-1}(\mathcal{I}/\mathcal{I}^2)$.",['algebraic-geometry']
239965,Solving problem 3-29 in Spivak´s Calculus on Manifolds without using change of variables,"Problem 3-29 (p. 61) in the section treating Fubini´s theorem reads: Use Fubini´s theorem to derive an expression for the volume of a set of $\mathbb{R}^{3}$ obtained by revolving a Jordan-measurable set in the $yz$-plane about the $z$-axis . By making some simplying assumptions about the plane region and changing variables to cylindrical coordinates we can obtain an expression for the volume. However, the material on the change of variables is treated two sections later (p. 66). Thus, is there is a way to solve the problem without using change of variables? The definition of Jordan-measurable can be found on p. 56 (See also Theorem 3-9, p.55), but I sumarize it here: Spivak defines a bounded subset $C$ of $\mathbb{R}^{n}$ to be Jordan measurable if the topological boundary of $C$ has measure $0$, i.e. if for any $\varepsilon>0$ there is a cover $\{U_{i}\}$ of $\mathrm{Bd}(C)$ by closed $n$-cubes such that $\sum_{i}v(U_{i})<\varepsilon$ (where $v(U_{i})$ denotes the $n$-dimensional volume of the rectangle $U_{i}$). If $C$ is Jordan measurable it is contained inside some closed $n$-cube $A$ and the characteristic function $\chi_{C}$ is integrable on $A$. The integral $\int_{A}\chi_{C}$ is called the $n$-dimensional volume of $C$.","['differential-topology', 'multivariable-calculus', 'integration']"
239967,Group does not contain any elements of order $p^2$?,"I am trying to show that the group of $3 \times 3 $ upper triangular matrices over the field $ \mathbb{F}_p $ with diagonal entries 1 does not contain any elements of order $p^2$ when $ p \geq 3$. I've tried to argue by contradiction: suppose it had an element $g$ of order $p^2$, then the subgroup generated by $g$ has index $p$ so it is normal, and from there I would like to find an element $h$ of order $p$ whose cyclic subgroup has trivial intersection with $ \langle g \rangle$. Then $G$ is the semi-direct product of $\langle g \rangle$ and $ \langle h \rangle$, and I am hoping this will give me a contradiction by telling me that $G$ is abelian or something similar. Any help is appreciated!","['matrices', 'finite-fields', 'finite-groups', 'abstract-algebra']"
239998,Compact sets are closed?,"I feel really ignorant in asking this question but I am really just don't understand how a compact set can be considered closed. By definition of a compact set it means that given an open cover we can find a finite subcover the covers the topological space. I think the word ""open cover"" is bothering me because if it is an open cover doesn't that mean it consists of open sets in the topology? If that is the case how can we have a ""closed compact set""? I know a topology can be defined with the notion of closed sets rather than open sets but I guess I am just really confused by this terminology. Please any explanation would be helpful to help clear up this confusion. Thank you!","['general-topology', 'compactness']"
240001,Inequality in a Hilbert space,"This was a homework question a couple weeks ago that I couldn't solve.  I'd appreciate a solution. Let $\mathcal{H}$ be a Hilbert space and $\{\eta_n \}_{n \in \mathbb{Z}}$ a set of not necessarily orthogonal elements of $\mathcal{H}$.  Let $A$ be the matrix, defined on $\ell_2(\mathbb{Z})$, such that $a_{nm} = |(\eta_n,\eta_m)|$.  Show that for all $f \in \mathcal{H}$,
  $$\sum_{n \in \mathbb{Z}} |(f,\eta_n)|^2 \le ||A|| \, ||f||^2$$","['functional-analysis', 'analysis']"
240013,Showing that a certain function is a local diffeomorphism,"I have to show that $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2: (x,y) \mapsto (e^x(x \cos y - y \sin y),e^x(x \sin y + y \cos y)$ is a local diffeomorphism in ever point not $(-1,0)$. I have no idea how to invert $f$ on some restricted domain. My guess is that we have to restrict the domain, jut say cos and sin have inverses. However, I have no idea what I could pick as a viable inverse? Since taking logarithms, dividing the result and multiplying or squaring just makes the result more complicated. Could anyone give me a hint?",['analysis']
240026,All derivatives zero at a point $\implies$ constant function?,"Suppose $f: \mathbb{R} \rightarrow \mathbb{R} $ is a continuous function, and there exists some $a \in \mathbb{R}$ where all derivatives of $f$ exist and are identically $0$, i.e. $f'(a) = 0, f''(a) = 0, \ldots$ Must $f$ be a constant function? or if not, are there examples of non-constant $f$ that satisfy these properties? What if the hypothesis is changed so that the derivatives of $f$ are identically $0$ on an open interval, i.e. $f'(A) = 0, f''(A) = 0, \ldots$ for some open interval $A$. Are the answers still the same?",['calculus']
240054,Discrete Math Course [closed],"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 11 years ago . I am in currently in college, wanting to study applied mathematics and physics. How many discrete math courses am I required to take; is it necessary that I take any at all?",['discrete-mathematics']
240070,Variance of binomial distribution,"Why for $X\sim B(n,p)$ is $Var(X)=np(1-p)$? $Var(X)=\sum x_i^2 p_i -(\sum x_i p_i)^2=\sum_{r=0}^n r^2 \binom{n}{r}p^r(1-p)^{n-r}+( \sum_{r=0}^n r \binom{n}{r}p^r(1-p)^{n-r} )^2$ In my short-sightedness, I don't see any viable ways to derive the variance from this.","['intuition', 'probability', 'combinatorics']"
240072,Dual of $\ell_\infty(X)$,"Given a Banach space $X$. Consider the space $\ell_\infty(X)$ which is the $\ell_\infty$-sum of countably many copies of $X$. Is there any accessible respresentation of the dual space $\ell_\infty(X)^*$? In particular, is this dual space isomorphic to the space of finitely additive $X^*$-valued measures on the powerset of $\mathbb N$ equipped with the semivariation norm? Any references will be appreciated.","['measure-theory', 'functional-analysis', 'banach-spaces']"
240077,Solving differential equation invariant,"I have eqn: $\frac{dx}{dt} = -y(t)$ and $\frac{dy}{dt} = x(t)$ I know that $(x(0),y(0))= (1,0)$. I want to solve eqn and show that it admits an invariant $I = x(t)^2 + y(t)^2$. I know $x' = -y$, $y' = x$, $x^{\prime\prime} = -y' = -x$ I know general solution of $x"" = -x$ is
$x = a\sin x = b\cos x$. I know 
$x(0) = a\sin 0 + b\cos 0 = 1$
So $b = 1$ How can I show $a = 1$? (I think it should!) I tried $x' = a\cos x - b\sin x$ since $y = -x$ but it just gives $ a = 0$.",['ordinary-differential-equations']
240085,Proving $A$ is invertible if $A + A^2 = I$.,"I'm trying to prove $A$ is invertible by proving there is an $A'$, for $AA' = I$. So I got to this stage $A(I + A) = I$, now I determine that $A' = I + A$, 
and from that I get $AA' = I$. I wanted to know if this is valid,  casue it doesn't seem to make sense,and I can't find a 'real' solution for such equation. Note:  $A$ is $n \times n$ and  $I$ represents the identity matrix and is also of the same dimensions.","['matrices', 'linear-algebra']"
240108,Equivalent definition of lower semi-continuity,"I have some problems about solving an exercise: Prove that one function $f \colon [a,b] \to \mathbb{R}$ is lower semi-continuous if and only if, for all $x \in [a,b]$, we have $$f(x)=\sup \{g(x) \mid g \in C[a,b] \text{ and } g \le f  \text{ over } [a,b] \}\;.$$ Assuming true that formula, I had no problems showing that $f^{-1}((t,\infty \ ])$ is open, using the property of supremum and the continuity of $g$'s. I have difficulties proving the opposite implication. Beacuse $f \ge g$, we have that $f(x) \ge \sup{g(x)}$, but I am not able to show the other inequality. Thank you.","['real-analysis', 'analysis']"
