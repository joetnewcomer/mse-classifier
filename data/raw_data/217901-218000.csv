question_id,title,body,tags
4447149,"Analytic formula for $f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds$","For any $\alpha \in (0,1)$ , define $\displaystyle f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds$ . Question. Is there an analytic formula for $f(\alpha)$ , say, in terms of special functions ? Note. I'm also fine with good upper-bounds for $f(\alpha)$ in terms of simpler expressions in $\alpha$ .","['fourier-analysis', 'special-functions', 'analysis', 'real-analysis']"
4447161,Prove that if $\lim_{x\to a^+}f'(x)$ exists then $\lim_{x\to a^+}f(x)$ exists.,"Prove that if $\lim_{x\to a^+}f'(x)$ exists then $\lim_{x\to a^+}f(x)$ exists. I am a high school math teacher, not a professional mathematician. I just thought of this question, in the context of trying to show that if $f''(a)>0$ then the curve does not necessarily have a U-shape around $x=a$ , but that is not important here and I am not inviting discussion of that; I just mentioned that to add context. I have searched for this question, here at MSE and also approachzero, but haven't found anything helpful. The statement I'm trying to prove here seems intuitively obvious to me but difficult to prove. I have tried using the formal definition of limit, and differentiation from first principles, to no avail. (I just asked a related question which turned out to be a flawed question, because what I was asking to be proved, is not true. Responders there said that the case with one-sided limits is true, so assuming that is correct, I am asking about that here. In that related question, I showed my flawed attempt at the proof.) Feel free to let me know if there's anything I can do to improve this question.","['limits', 'calculus', 'derivatives', 'real-analysis']"
4447179,Covering the sphere with connected closed sets,"I am given the following question . Can $S^{2}$ be covered by finite number of connected closed sets $A_{1},...,A_{n}$ such that Each has diameter less than a given $\epsilon$ . $\cup A_{n}= S^{2}$ $\displaystyle A_{k}\cap\bigg(\bigcup_{i=1}^{k-1}A_{i}\bigg)$ is connected for all $k\leq n$ ? My attempt:- My first thought was to view it as a standard classic football and prove it for pentagons and hexagons. But then I realized that it is not very simple as ordering the sets such that property $3$ becomes a problem and moreover the pentagons and hexagons have a curvature in this case. So I thought of covering with small enough ""curved"" squares . But I am struggling to make the argument fully rigorous. I view the height of the sphere which is $1$ as the interval $[0,1]$ and divide into $\frac{1}{n^{2}}=\frac{1}{m}$ many may pieces where $\epsilon>\frac{1}{n}$ . Now the surface area of the sphere between $0\leq h\leq \frac{1}{m}$ is just $\frac{2\pi}{m}$ . And so I divide this surface area into $m$ many ""curved squares"" of area $\frac{2\pi}{m^{2}}$ and stack them so as the property $3$ holds. Now it is hard to find the exact diameter $=\sup\{d(x,y):\,x,y\in A_{i}\}$ . But as the area of the square is taken to be small enough , we can say that it is less than $\epsilon$ . If not then we can again scale it to make it so. Now we proceed inductively and cover the sphere up with such squares for each interval of heights $[\frac{k}{m},\frac{k+1}{m}]$ for $k=1,...,m-1$ . Now this seems intuitively okay but I am worried about the connectedness part as we move up a level in height and I can't seem to make it fully rigorous. Can someone  explain to me if I am going wrong somewhere or can suggest me a better way of solving this ? Any help is appreciated. PS. This was asked in a Algebraic Topology course as a similar argument was used to lift a path $\alpha :[0,1]\to X$ given a space $Y$ and a covering map $p$ .","['general-topology', 'geometry', 'algebraic-topology']"
4447185,$N$ lousy shooters in a gunfight,"$N$ players are in a gunfight. Starting from player 1, each player takes turns to act in the order of $1,2,...,N,1,2,...$ . In their turn, a player randomly chooses one of the other remaining players as the target, and fires one shot at them. If hit, the target is eliminated. The game continues until there's only one survivor. All shots hit targets with probability $p$ . Is it true that when $p$ is small enough, player 1 has the highest surviving probability for any $N$ ? ---------- Edit There seems to be some misunderstanding in the comments so let me clarify. I'm asking if there exists some $p^{*}\gt 0$ , such that if everybody has hit probability $p^{*}$ , player 1's position is the best position for any choice of $N$ . For example, if $p=1$ , then player 1's position is not the best for $N=3$ . If $p=0.5$ , then player 1's position is not the best for $N=5$ . But if $p=0.45$ , player 1's position is the best for $N\leq 10000$ . (I haven't checked for more because the algorithm is $\mathcal{O}(N^3)$ and $N\leq 10000$ took me more than 20 minutes)","['nonlinear-system', 'recreational-mathematics', 'probability', 'dynamical-systems']"
4447205,Book/Note recommendation for topology on spaces of matrices,"Recently I have faced two questions, which are solved in this website. One is to show that the space of orthogonal matrices is compact, another one is to show that the space of nilpotent matrices is connected. I want to study and solve problems like these on topological aspects on the set of matrices, but unable to find any book or notes specially on this topic. It will be beneficial if someone can suggest some materials. Thanks in advance!!","['book-recommendation', 'lecture-notes', 'matrices', 'reference-request', 'general-topology']"
4447217,Proving that $\sqrt{x-1}=3+\sqrt{x}$ has no real solutions,"Prove that there is no real number $x$ such that $\sqrt{x-11}=3+\sqrt{x}.$ The book suggested a hint ""Start Squaring by both sides"". Now I have tried this as. Squaring both sides we get $x-1=9+6\sqrt{x}+x.$ It follows that $0=10+6\sqrt{x}$ , so $\sqrt{x}=-10/6. \tag3$ Now I do not know what I do next.",['algebra-precalculus']
4447237,Is this Lie group isometric to the Euclidean plane?,"Consider the Lie group $$G := \left\{ \begin{pmatrix}
1 & 0 & 0 \\
x & 1 & 0 \\
y & x & 1
\end{pmatrix}: x, y \in \mathbb{R}\right\}$$ equipped with the left-invariant Riemannian metric $g$ given by the left translation in $G$ . Then, $g$ has expression $$ g = (1 + x^{2}) dx \otimes dx - x dx \otimes dy - x dy \otimes dx + dy \otimes dy.$$ Easily, we can compute the Christoffel symbols of the Levi-Civita connection. The only non-zero symbol is $\Gamma_{11}^{2} = -1$ . Then, $G$ has constant sectional curvature $K = 0$ , as the Euclidean plane $\mathbb{R}^{2}$ . Now, are $\mathbb{R}^{2}$ and $G$ isometric Riemannian manifolds? I know that if two Riemannian manifolds have  different sectional curvatures in corresponding points, they cannot be isometric but, this not the case. Should I try to find conditions for the existence of such isometry? Thanks in advance. SOLUTION: As @Didier pointed out, one way to prove that $\mathbb{R}^{2}$ and $G$ are isometric is to use that $G$ is a complete simply connected Riemannian manifold with constant curvature $K = 0$ , so by Theorem 4. 1 (page 163) of Do Carmo's book Riemannian Geometry, $\mathbb{R}^{2}$ and $G$ are isometric. Nevertheless, we can also find explicitly an isometry $\phi: \mathbb{R}^{2} \to G$ . Note that $$\phi(x,y) = \begin{pmatrix}
1 & 0 & 0 \\
\phi_{1}(x,y) & 1 & 0 \\
\phi_{2}(x,y) & \phi_{1}(x,y) & 1
\end{pmatrix},$$ where $\phi_{j}: \mathbb{R}^{2} \to \mathbb{R}$ is differentiable. Let's say that $A$ is the matrix of the metric $g$ of $G$ , so we want to get a matrix $B$ (the matrix of the differential of $\phi$ ), such that $B^{T} A B = I$ . Doing Gaussian operations in the matrix $A$ we find that $$B = \begin{pmatrix} 1 & 0 \\
x &1 \end{pmatrix},$$ so $\phi_{1}(x, y) = x$ and $\phi_{2}(x,y) = \frac{x^{2}}{2} + y$ .","['lie-groups', 'riemannian-geometry', 'differential-geometry']"
4447289,Does the limit of the cubic formula approach the quadratic one as the cubic coefficient goes to $0$?,"The formula for solving a cubic equation of the form $ax^3+bx^2+cx+d=0$ does not seem to yield the quadratic formula for the limit $\lim _{a \rightarrow 0} \text{(cubic formula)}$ . But, if one tries the same thing with the quadratic formula the limit exists for the right choice of the square root sign. My question is, is there a way to take the limit $\lim _{a \rightarrow 0} \text{(cubic formula)}$ and produce the quadratic formula? or does the limit simply not exist? Finally, if the limit does not exist, is there a technical reason for that? Any input is very much appreciated. Thanks Edit: I was trying to tackle the simple case of $b=0$ . Doing so, the cubic formula reduces to $$\left(-\frac{d}{2a} - \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}} + \left(-\frac{d}{2a} + \left(\frac{4c^3 + 27ad^2}{108a^3}\right)^{\frac{1}{2}}\right)^{\frac{1}{3}}$$ If one only considers the first term cubed, then it can be written as $$\frac{1}{a}\left(-\frac{d}{2} - \left(4c^3+27ad^2\right)^{\frac{1}{2}}\right)$$ and I don't see how is it possible to find a finite limit as $a\rightarrow 0$ . Am I missing something trivial?","['cubics', 'calculus', 'polynomials', 'limits', 'quadratics']"
4447293,"Solve $a_n=n(a_{n-1}+a_{n-2})$ where $a_0=1,a_1=2$ using generating functions","I am trying to solve a recurrence relation, $a_n=n(a_{n-1}+a_{n-2})$ where $a_0=1,a_1=2$ , using generating functions. So, I did: let $$A(x)=\sum_{n\geq 0}a_n\frac{x^{n}}{n!}$$ $$\sum_{n\geq 0}a_{n+2}\frac{x^{n+2}}{(n+2)!}=\sum_{n\geq 0}a_{n+1}\frac{x^{n+2}}{(n+1)!}+\sum_{n\geq 0}a_n\frac{x^{n+2}}{(n+1)!}$$ $$[A(x)-a_1x-a_0]=\frac{[A(x)-a_0]}{x}+\frac{A(x)x^2}{(n+1)}$$ However, $$\sum_{n\geq 0}a_n\frac{x^{n+2}}{(n+1)!}=\frac{A(x)x^2}{(n+1)}$$ does not allow me make process further, so I need help to find $A(x)$ . Note that the answer should be $a_n=(n+1)!$ . However, I want to arrive at it using only generating functions not differential equations or linear algebra techniques. Edit : I saw that I made a silly mistake in $$\sum_{n\geq 0}a_n\frac{x^{n+2}}{(n+1)!}=\frac{A(x)x^2}{(n+1)}$$ Do you have any suggestion to write $\sum_{n\geq 0}a_n\frac{x^{n+2}}{(n+1)!}$ in terms of $A(x)=\sum_{n\geq 0}a_n\frac{x^{n}}{n!}$ ? Original question: Formula for $a_n$ where $a_n$ = n*($a_{n-1}$+$a_{n-2}$)","['combinatorics', 'recurrence-relations', 'generating-functions']"
4447336,Solve $\frac{d^{2} y}{d x^{2}}+(\tan x-3 \cos x) \frac{d y}{d x}+2 y \cos ^{2} x=\cos ^{4} x$,"Solve the differential equation $$\frac{d^{2} y}{d x^{2}}+(\tan x-3 \cos x) \frac{d y}{d x}+2 y \cos ^{2} x=\cos ^{4} x$$ My try: I multiplied throughout with $\sec^2 x$ , getting $$\sec^{2} x \frac{d^{2} y}{d x}+\sec^{2} x \tan x \frac{d y}{d x}-3 \sec x \frac{d y}{d x}+2 y=\cos ^{2} x$$ Which can be written as: $$\frac{d}{d x}\left(\sec ^{2} x \frac{d y}{d x}\right)-\tan x\left(\sec ^{2} x \frac{d y}{d x}\right)-3 \sec x \frac{d y}{d x}+2 y=\cos ^{2} x$$ Then I assumed $z=\sec^2 x\frac{dy}{dx}$ , so we get $$\frac{d z}{d x}-z \tan x-3 \sec x \frac{d y}{d x}+2 y=\cos ^{2} x$$ I am stuck now.","['derivatives', 'ordinary-differential-equations']"
4447414,Show that $\sin47^\circ+\sin61^\circ-\sin11^\circ-\sin25^\circ=\cos7^\circ$ [duplicate],"This question already has answers here : What is the value of $\sin 47^{\circ}+\sin 61^{\circ}- \sin25^{\circ} -\sin11^{\circ}$? (3 answers) Closed 2 years ago . Show that $$\sin47^\circ+\sin61^\circ-\sin11^\circ-\sin25^\circ=\cos7^\circ$$ NOTE: I have seen the other questions and solutions for this problem. I have a particular question if my idea has a potential. I decided to rearrange the LHS (without a particular reason, it just felt right to me) as follows $$\begin{align}(&\sin47^\circ-\sin11^\circ)+(\sin61^\circ-\sin25^\circ)\\&=2\cos\frac{47^\circ+11^\circ}{2}\sin\frac{47^\circ-11^\circ}{2}+2\cos\frac{61^\circ+25^\circ}{2}\sin\frac{61^\circ-25^\circ}{2}\\&=2\cos29^\circ\sin18^\circ+2\cos43^\circ\sin18^\circ\\&=2\sin18^\circ(\cos29^\circ+\cos43^\circ)\\&=4\sin18^\circ\cos36^\circ\cos7^\circ\end{align}$$ If this won't work, what is the intuition that leads to the appropriate rearranging?",['trigonometry']
4447435,"Can Machine Learning models be considered as ""Approximate Dynamic Programming""?","In the context of certain statistical/machine learning models, such as models that are trying to estimate ""optimal policies"" (e.g. reinforcement learning) - can we consider these models as ""approximate dynamic programming""? My understanding is that the principle of ""Bellman Optimality"" states that in optimization problems in which ""optimal substructure"" is believed to exist* - Dynamic Programming in theory is able to provide a globally optimal solution by breaking the original problem into multiple smaller problems and solve the original problem by solving all the smaller problems. I am not sure if I understood this correctly - but Bellman Optimality also states that for problems with Optimal Substructure, Dynamic Programming can work recursively with ""limited lookahead"", implying that Optimal Solutions can be found without spending ""too much time"" moving backwards/forwards and reconsidering policies at previous steps. However, Dynamic Programming in the classical sense is unable to work effectively in larger problems - therefore, a ""Value Function"" is created to estimate the approximate value of different states, and then Dynamic Programming can be used to approximate a globally optimal solution. Dynamic Programming in this approximate form (i.e. Approximate Dynamic Programming) is implemented through a Statistical/Machine Learning model. Is my understanding of this correct - can certain Statistical/Machine Learning Models be considered as Approximate Dynamic Programming? Thanks! *Note: I am told that there is no real way to determine if an optimization problem indeed has ""optimal substructure"" - we often assume that they do and solve them under this assumption. This results in solutions in real world problems being locally optimal instead of globally optimal. Note : I think that the main difference between Greedy Search and Dynamic Programming is that Greedy Search returns an optimal solution only at each subproblem and reaches the final solution by joining all these optimal solution together - this only results in a globally optimal solution in some problems. Dynamic Programming tries to calculate optimal solutions at each time point by considering/projecting the effect of these solutions at future time points, and can reconsider previously chosen solutions. In short, Dynamic Programming returns optimal solutions on all problems that Greedy Search returns optimal solutions - but there are some problems in which Greedy Search will not return an Optimal Solution but Dynamic Programming will return an Optimal Solution.","['machine-learning', 'optimization', 'statistics', 'dynamic-programming']"
4447441,Random walk with positive drift,"I'm working on something where the following claim, if true, would be quite helpful: Let $S$ be a random variable distributed over $\{-1,0\}\cup\mathbb N$ with $\mathbb E[S]>0$ . Consider a one dimensional random walk with step size $S$ , starting at $1$ . Is it true that, with nonzero probability, the random walk never reaches 0? Intuitively this claim seems true (as the random walk has positive drift) but I'm not sure how to show it. If the claim does not hold in general, are there other conditions we can impose on the distribution of $S$ that would make the claim true?","['random-walk', 'probability']"
4447450,10 good and 3 bad batteries are mixed and then 5 are chosen. What is the probability of the fifth one being dead given that the first 4 aren't?,"I approached this as follows: $A$ : First four are good, $B$ : Fifth one is bad. Then $$|A|={10\choose4}$$ because this is the number of ways we can choose 4 good batteries from the available 10. Then $$P(A)=\frac{10\choose4}{13\choose4}$$ I want to find $P(B|A)=\frac{P(A\cap B)}{P(A)}$ . For this I need: $A\cap B$ : First four are good and fifth one is dead. Then $|A\cap B|={10\choose4}{3\choose1}$ because this is the number of ways we can choose 4 good batteries from the available 10 followed by choosing 1 of the available 3 dead ones. Then $$P(A\cap B)=\frac{{10\choose4}\cdot{3\choose1}}{13\choose5}$$ However, once a substitute these values into the conitional probability expression I get $\frac53$ .","['conditional-probability', 'combinatorics', 'probability']"
4447462,Find $\lim_{x \rightarrow \infty} \frac{\arctan (x^{3/2})}{\sqrt x}$.,"$$\lim_{x \rightarrow \infty} \frac{\arctan \left(x^\frac{3}{2}\right)}{\sqrt x}.$$ My method was that as the numerator can never exceed $\pi/2$ , some finite value/infinity tends to $0$ , but is there a proper way of doing it? As such it's not $\infty/\infty$ or $0/0$ form so l'hopital will not work, so does any other way exist?","['limits', 'calculus']"
4447477,Show existence of bounded linear functional,"To solve problem about a bounded linear functional, I am having a problem with the Hahn Banach Theorem. Problem is: For $n \in \mathbb{N}$ and $1 \leq p < \infty$ , let $X_n \subset L^p([0,1]$ be a collection of polynomials of degree $\leq n$ and let $X = \cup_{n=1}^\infty X_n$ . Is there a bounded linear functional $\Lambda$ on $L^p([0,1]$ such that $\Lambda(f) = f'(0)$ for all $f \in X$ ? First I show that for fixed $n \in \mathbb{N}$ , Consider $\Lambda : X_n \rightarrow \mathbb{R}$ which satisfies $\Lambda(f) = f'(0)$ . Since $f \in X_n$ , we can express $f$ as $f(x) = a_nx^n + \cdots + a_1x + a_0$ . $a_i$ are real numbers. By using fact that a derivative as a linear functional $(D)$ is linear and bounded, we know that $\Lambda$ is a bounded linear functional. Therefore we can use the Hahn Banach Theorem, that there exists an extension $\Lambda^* : L^p([0,1]) \rightarrow \mathbb{R}$ which satisfies $\Lambda^*(f) = f'(0)$ (Since $X_n$ is subspace in $L^p([0,1]))$ . But I am not sure how to extend this result to $X$ . Is it enough to show that $X$ is a subspace in $L^p([0,1])$ ?","['real-analysis', 'lp-spaces', 'functional-analysis', 'hahn-banach-theorem', 'derivatives']"
4447510,Proof: $\sin \mathrm{A}+\sin \mathrm{B}+\sin \mathrm{C} \leq \cos \frac{\mathrm{A}}{2}+\cos \frac{\mathrm{B}}{2}+\cos \frac{\mathrm{C}}{2}$,"Let $\mathrm{I} \subseteq \mathbf{R}$ be an interval and $\mathrm{f}: \mathrm{I} \rightarrow \mathbf{R}$ .
We have this inequality for any $x, y, z \in I$ , $$
f\left(\frac{x+y}{2}\right)+f\left(\frac{y+z}{2}\right)+f\left(\frac{z+x}{2}\right) \geq f(x)+f(y)+f(z) .
$$ If A, B, C are the measures of the angles of a triangle $ \mathrm {ABC} $ , expressed in radians, show that: $$\sin \mathrm{A}+\sin \mathrm{B}+\sin \mathrm{C} \leq \cos \frac{\mathrm{A}}{2}+\cos \frac{\mathrm{B}}{2}+\cos \frac{\mathrm{C}}{2}$$ I have to prove the inequality by using f. I don't know what ""function"" to take in order to prove inequality.","['trigonometry', 'triangles', 'inequality']"
4447519,Find the range of $\frac{\sqrt{(x-1)(x+3)}}{x+2}$,"Find the range of $\frac{\sqrt{(x-1)(x+3)}}{x+2}$ My Attempt: $$y^2=\frac{x^2+2x-3}{x^2+4x+4}=z\\\implies  x^2(z-1)+x(4z-2)+4z+3=0$$ Discriminant greater than equal to zero, so, $$(4z-2)^2-4(z-1)(4z+3)\ge0\\\implies z\le\frac43\\\implies -\frac2{\sqrt3}\le y\le\frac2{\sqrt3}$$ But the answer given is $[-\frac2{\sqrt3},1]$ What am I missing?","['calculus', 'functions']"
4447579,Convergence of non iid observations on the empirical distribution,"Let $f$ be a function on domain $X$ with binary output $f: X\to \{0,1\}$ . We define an arbiatry distribution $\mathcal{Q}$ over $X$ and the empircal distribution of $n$ samples from $\mathcal{Q}$ -- $\mathbf{Q}^n$ By the Glivenko–Cantelli theorem the expected average value of $f$ on the empircal distribution converges to the probability that $f(x)=1$ : \begin{equation}
\mathbb{E}_{\mathbf{Q}^n}\left[\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1]
\end{equation} Now to makes things slightly more complicated we consider $F$ to be some class of binary functions. We can consider a simple game where we draw $n$ samples from $\mathcal{Q}$ (i.e sample an empircal distribution $\mathbf{Q}^n$ ) then find the function $f\in F$ that minimizes the sum $\sum_{x\in \mathbf{Q}^n} f(x)$ . I would like to show that this the mean of $f$ on $\mathbf{Q}^n$ converges (at least weakly): \begin{equation}
\mathbb{E}_{\mathbf{Q}^n}\left[\min_{f\in F}\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \min_{f\in F} \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1]
\end{equation} This seems intuitively true to me, but the problem is that $f(x)$ is not iid for all $x\in \mathbf{Q}^n$ (a simple example is to let $F$ be the class of functions that output $1$ only in some fixed $L_p$ ball then if you observe $f(x_i)=1$ you know that $f(x_j)=0$ for any $x_j$ that cannot be contained in a ball with $x_i$ ). I'm a bit stuck on how to reason about this dependence structure, but I'm still convinced that Equation 2 should converge, at least for some minimal set of assumptions on the behaviour of $F$ . Thanks","['expected-value', 'weak-convergence', 'empirical-processes', 'probability']"
4447592,Understanding compatibility of PDE,"THEOREM $1$ : The equations $$f(x, y, z, p, q)=0\qquad (1)$$ and $$g(x, y, z, p, q)=0\qquad(2)$$ are compatible on a domain $D$ if
(i) $J=\frac{\partial(f, g)}{\partial(p, q)}=\left|\begin{array}{ll}f_{p} & f_{q} \\ g_{p} & g_{q}\end{array}\right| \neq 0$ on $D .$ (ii) $p$ and $q$ can be explicitly solved from $(1)$ and $(2)$ as $p=\phi(x, y, z)$ and $q=\psi(x, y, z)$ .
Further, the equation $$
d z=\phi(x, y, z) d x+\psi(x, y, z) d y
$$ is integrable. THEOREM $2$ : A necessary and sufficient condition for the integrability of the equation $d z=\phi(x, y, z) d x+\psi(x, y, z) d y$ is $$
[f, g] \equiv \frac{\partial(f, g)}{\partial(x, p)}+\frac{\partial(f, g)}{\partial(y, q)}+p \frac{\partial(f, g)}{\partial(z, p)}+q \frac{\partial(f, g)}{\partial(z, q)}=0\qquad(3)
$$ In other words, the equations (1) and (2) are compatible iff (3) holds. In my lecture note, these two theorems are mentioned without any proof. But I am wondering how the Jacobian tell me the system are compatible? And how the expression $[f, g]$ guarantee the integrability of the equation $d z=\phi(x, y, z) d x+\psi(x, y, z) d y$ ? The definition of compatibility they followed are, A system of two first-order PDEs are said to be compatible if they have a common solution I am more interested to get the answer about their intuition (or connection) rather a complete proof (which may be beyond my scope of the syllabus). Thanks in advance.","['intuition', 'soft-question', 'analysis', 'partial-differential-equations']"
4447607,"How to define a (linear, invertible) mapping from a square to a triangle","This problem has come up when analyzing one type of HSV color selector: Note: h,s,v are in 0...1 range. For the given hue (i.e. red) on the three vertices of the triangle we have: W: white, when s=0, v=1; R: red, when s=1, v=1; B: black otherwise. and we have three sides: going from one vertex to another we see a linear change of either s or v. We can observe a similar fact on the square SxV=[0,1]x[0,1] below. The B vertex is however ""special"" as it is the projection of the fourth side v=0 of this square. (S on the horizontal axis, V on the vertical axis upwards) it looks like the mapping (s,v) --> (x,y) on triangle, it has been defined as: (x, y, ...) = f(s, v) = (1 - v) * B + v * ((1 - s) * W + s * R) and again all the points (s, 0) go into the same vertex B. But B is the only non-invertible point. For any line (s, vk) where vk is a constant value in (0,1], there is a corresponding line in the triangle, parallel to the WR segment. How to invert this mapping, i.e. compute (s, v) given (x, y)?","['euclidean-geometry', 'hyperbolic-geometry', 'geometry']"
4447614,"Solution verification: Usage of L'Hôpital's rule, derivatives of trigonometric functions","I did the following problem: Find $\displaystyle\lim_{x\to 0} \frac{\cos^2x-1}{x^2}$ The following solution was given: $$\lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \sin x \cdot \cos x}{2x} = 1$$ My questions regarding this solution are these: In the denominator of the second limit in the solution above we have $2x$ and since we have $x$ aproaching $0$ this would mean division by $0$ which is not allowed. So, using L'Hôpital's rule, we should find the next higher derivative. And regarding the numerator of this second limit I think, it should be $2 \cos x \cdot (- \sin x)$ by usage of the chain rule. Thus I came the following solution: $$\lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \cos x \cdot (- \sin x)}{2x} = \lim_{x\to 0} \frac{2\sin^2 x - 2 \cos^2 x}{2} = \frac{-2}{2} = -1$$ So, since these are different solutions, which is correct and where did I make a mistake? I would be thankful for explanations.","['indeterminate-forms', 'limits', 'calculus', 'trigonometry']"
4447641,"Using the definition of a derivative, solve $\lim_{x \to 0} \frac{(2+h)^{3+h} - 8}{h}$","I can't figure out where to even start, I have looked up the answer on Desmos but it uses L'Hopitals rule which i haven't learned yet. $$\lim_{h \to 0} \frac{(2+h)^{3+h} - 8}{h}$$ I see that i can use the log rules to rewrite it as $$\lim_{h \to 0} \frac{e^{(3+h)\ln(2+h)} - 8}{h}$$ but that only confuses me more.","['limits-without-lhopital', 'derivatives', 'logarithms']"
4447646,Differences between groups and rings when understood as categories,"A group can be understood as a category with a single object $*$ whose hom-set $\text{hom}(*,*)$ forms a group with composition as the group multiplication. A ring can be understood as a category that is preadditive with a single object $*$ . Preadditive means that for a category $C$ , $\text{hom}_C(A,B)$ forms an abelian group that is distributive with respect to composition of arrows. The group structure appears in different ways in these two categories. It seems weird to me that the group multiplication is the composition of arrows for groups as categories but for rings it is some binary map of arrows in the same hom-set. Why is this the 'best' way to understand these structures as categories ? Is there some way to understand a ring as a category where composition of arrows forms a group (like the categorical sense of a group)?","['abstract-algebra', 'category-theory']"
4447665,Convergence proof in probability,"I would appreciate help with how to proceed to prove the following statement, I have some ideas on how to solve it but do not really get it all together and would appreciate any help/feedback that I can get! Let $X_1, X_2,...$ be random variables defined by the relations $$P(X_n=0)=1-\frac{1}{n}, \ \ P(X_n=1)=\frac{1}{2n}, \ \  P(X_n=-1)=\frac{1}{2n}, \ \ \ n \ge 1.$$ Show that $X_n \xrightarrow[]{p}0$ as $n \rightarrow \infty$ . This is my thoughts : First off, I'm sure we need to use the following definition: Def : $X_n$ converges in probability to the random variables X as $n \rightarrow \infty$ iff, for all $\epsilon >0$ : $P(|X_n-X|>\epsilon)\rightarrow 0 \text{ as } n\rightarrow \infty.$ Hence, we are goin to prove that $P(|X_n|>\epsilon)\rightarrow 0 $ as $n \rightarrow \infty$ . Then I noted that since we want the absolute value of $X_n$ and $P(X_n=1)=P(X_n=-1)$ we only have to apply the definition on $P(X_n=1)$ , is that right? If yes we get that: $$P(|X_n|>\epsilon)=\lim_{n\rightarrow \infty}\frac{1}{2n}=0, \ \ 0<\epsilon<1.$$ And then I noticed that the definition only applies for $\epsilon>0$ . And since our last relation, $P(X_n=0)$ have $X_n=0$ then $Xn>\epsilon$ cannot be achieved and we can exclude this relation. Here, however, I am very unsure if this is correct. This is as long as I have come so far. The problem is that even if the above is correct I still only have a proof for $0<\epsilon<1$ and not for all $\epsilon$ greater than zero as the definition says. How should i continue from here?","['convergence-divergence', 'probability']"
4447670,"Show that $C^\infty(0,T; L^2(\Omega(t)))$ is dense in $L^2(0,T; L^2(\Omega(t)))$.","Let $T > 0$ , $\Omega \subset \mathbb R^2$ and $f:[0,T] \to \mathbb R^2$ a continuous and bounded function. We define $$\Omega(t) = \Omega + f(t),$$ and $\widetilde \Omega \subset \mathbb R^2$ such that $\Omega(t)\subset \widetilde \Omega$ for all $t$ . Moreover, we set \begin{align}
L^2(0,T; L^2(\Omega(t))) &= \{v \in L^2(0,T; L^2(\widetilde \Omega ))~|~ v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0 ~\text{ a.e. in }[0,T] \},\\
C^\infty(0,T; L^2(\Omega(t))) &= \{v \in C^\infty(0,T; L^2(\widetilde \Omega ))~|~v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0~~\forall t \in [0,T] \}.
\end{align} It is not too hard to show that $ C^\infty(0,T; L^2(\widetilde \Omega ))$ is dense in $L^2(0,T; L^2(\widetilde \Omega ))$ . Now, I would like to know if $C^\infty(0,T; L^2(\Omega(t)))$ is dense in $L^2(0,T; L^2(\Omega(t)))$ . Here is my attempt: Let $v \in L^2(0,T; L^2(\Omega(t)))$ and $(v_n)_n \subset C^\infty(0,T; L^2(\widetilde \Omega ))$ such that $v_n \to v$ in $L^2(0,T; L^2(\widetilde \Omega ))$ . This means that $$\int_0^T\int_{\widetilde \Omega} |v - v_n|^2dy dt \to 0 \quad \text{for }n \to \infty.$$ This implies that $\|v(t, \cdot) - v_n(t, \cdot)\|_{L^2(\widetilde \Omega)} \to 0$ for every $t \in [0,T] \backslash E$ , where $E$ has a zero measure. Therefore, there exists $N(t) \ge 0$ such that $\forall n \ge N(t)$ , $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ . Now for $t \in E$ , we can find a sequence $t_k \subset [0,T] \backslash E$ such that $t_k \to t$ and by continuity of the function $f$ , for $k$ and $n$ sufficiently big, we have $\text{supp } v_n(t_k, \cdot) \subset \Omega(t) \cap \Omega(t_k)$ and as $t \mapsto v_n(t, \cdot)$ is continuous, $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ . Now I would like to discard a finite number of $v_n$ 's in my sequence so that $\text{supp } v(t, \cdot) \subset \Omega(t)$ for every $t \in [0,T]$ . My problem is that $N(t)$ depends on $t$ , such that for every $t$ I can discard a finite number for $n$ 's in my sequence but how can I find a $N$ that does not depend on $t$ ? These $N(t)$ 's could go to infinity for $t$ going to a certain value... Is it the right way to do that ? How can I conclude ? EDIT: Actually the above argument is wrong. Indeed, when I deduce that $\forall n \ge N(t)$ , $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ , it is as if I assumed that the support of $v(t, \cdot)$ was strictly include in $\Omega(t)$ while it is not true, we only have that the trace of $v$ is zero, but the support may not be strictly included. I should find a way to approach $v$ from inside $\Omega(t)$ . I really have no idea how to do that..","['lp-spaces', 'functional-analysis', 'analysis']"
4447692,Find the equation for the angle in which the particle abandons the hemicircle. No friction.,"I think I missed something in this mechanics problem. We're given a polish and homogeneous hemicircle which has mass $M$ and a particle of mass $m$ laying on the top of it. There is also no friction between the hemicircle and the ground. Find the equation for the angle $\theta$ in which the particle abandons the hemicircle surface. I can't find my mistake. As we have no external horizontal forces acting on the system particle + hemicircle we must have conservation of the horizontal position of the center of mass. I made this horrible drawing to try to understand the movement: we would have $x = \frac{mR \sin(\theta)}{M+m}$ , making $M = k \cdot m \iff k = \frac M m$ , we would have: $$x = \frac{R \sin(\theta)}{1+k} \implies \dot x = \frac{R \dot \theta \cos(\theta)}{1+k}$$ from this, I found that the velocity of the particle with respect to the ground was: $$R \dot \theta (\cos (\theta)(\frac k{k+1})u_x - \sin (\theta) u_y) = \vec v_P $$ And conservation of energy $$mgR = mgR\cos(\theta) + \frac{mv_P^2+ M \dot x^2}2$$ gave me: $$2\frac gR(1-\cos(\theta))(1+k) = \dot \theta^2 [\sin^2(\theta) +k]$$ Finally, I used the abandonment equation: $\frac gR \cos(\theta) = \dot \theta^2$ Which led me to: $$3 \cos(\theta) - \frac{\cos^3(\theta)}{1+k} = 2$$ but I feel something is wrong because $k=0$ should give me $\cos(\theta) = \frac23$ any insights on my mistakes? EDIT: I think on the line of the energy conservation the term $\sin^2(\theta)$ is the one causing trouble. I'm very confident about the expression for the velocity of the particle.","['physics', 'classical-mechanics', 'ordinary-differential-equations']"
4447721,Composition of functions is injective implies the functions are injective,"I have reason to believe the following proof is flawed, but I am not able to pin down the inconsistency. Let $f:A \to B$ and $g:B \to C$ and the composition $g \circ f : A \to C$ be injective. Let $a_1$ and $a_2$ be some elements in $A$ such that $a_1 \neq a_2$ . As the composition is injective, we know also that $g(f(a_1)) \neq g(f(a_2))$ . Assume for the sake of argument that $f(a_1)=f(a_2)$ . By definition, $g$ is a function and so the image of $f(a_1)$ and $f(a_2)$ under $g$ must be equal if $f(a_1)$ and $f(a_2)$ are equal. However, this would contradict our assumptions. Thus $f(a_1)$ and $f(a_2)$ must be distinct. This implies that $g$ is injective. Assume that $f$ is not an injection. In other words, there exists some $a_1,a_2 \in A$ such that $f(a_1) \neq f(a_2)$ and $a_1 = a_2$ . However, we know that $g$ is an injection, so the image of $f(a_1)$ and $f(a_2)$ under $g$ are distinct. But, since the composition is injective, we must also have $a_1 \neq a_2$ , which is a contradiction. Thus $f$ is an injection. My suspicion is: In attempting to show that $g$ is injective, I have only shown that for some elements of $B$ (the image of $f$ ) we have the needed condition, but I must in fact show that it holds for any arbitrary element of $B$ . Would the proof hold under the additional condition that $g$ is surjective?",['elementary-set-theory']
4447737,Derivative of vector-valued function's norm-squared,I'm trying to understand the partial chain rule and want to check my understanding. Let $$g(\mathbf{x})=\Vert{\mathbf{f}(\mathbf{x})}\Vert^2=\mathbf{f}(\mathbf{x})^T\mathbf{f}(\mathbf{x})$$ Then am I right in saying $$\frac{\text{d}g}{\text{d}\mathbf{x}}=2(\mathbf{J}(\mathbf{f})(\mathbf{x}))^T\mathbf{f}(\mathbf{x})$$,"['jacobian', 'multivariable-calculus', 'derivatives']"
4447739,Surprising appearance of triangular numbers during simple coin flip game,"I have the following problem: You have $n$ coins in a row in some beginning state of heads/tails.
Define a process as follows: If you have $k>0$ heads, flip over the coin
in the $k$ th position from the left. If you have $k=0$ heads, stop.
Otherwise repeat. For example, for THT, the process goes THT $\to$ HHT $\to$ HTT $\to$ TTT For fixed $n$ , calculate the average number of steps it takes to
terminate over all $2^n$ possible beginning states. I saw the correct answer is $$\frac{n^2+n}{4}$$ which is the $n_{th}$ triangular number divided by $2$ . I'm not sure why this is the case. I assume this involves a recursive computation. I can see that for $k$ heads and $n$ total coins, you have a $\frac{k}{n}$ probability of flipping a head to a tail, which leaves you with $k-1$ heads. You could then solve for every value of $k \leq n$ , and take the average. However this problem is deterministic and not probabilistic, so I don't think this approach is valid.","['statistics', 'expected-value', 'combinatorics', 'probability-theory', 'probability']"
4447779,Proving the function $f(x) = x^2 + ax + b$ is not injective. Does my proof make sense?,"The question I’ve been working on is:
Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be the function defined by $f(x)=x^2+ax+b$ , where $a,b\in\mathbb{R}$ . Prove that $f$ is not injective. Here's what I've come up with. I'm hoping someone can tell me if I messed up somewhere. (I'm not sure whether the last part is clear enough, or whether makes sense or not). Also this isn't for a homework assignment--this is just for me. $\text{Let } m,n\in\mathbb{R} \text{ such that } f(m) = f(n).$ We consider two cases: Case 1: $a=0.$ Then, substituting, we have $$ m^2 +b=n^2+b\\m^2=n^2\\\text{either }m=n \text{ or }m=-n$$ Therefore, we can have $f(n)=f(-n) \text{ but } n\neq -n$ . Therefore, $f$ is not injective. Case 2: $a\neq0.$ Then, we have $$m^2+am+b=n^2+an+b\\m^2+am=n^2+an\\
m^2-n^2=a(n-m)\\(m+n)(m-n)=a(n-m)\\(1/a)(m+n)(m-n)=(n-m)\\-(1/a)(m+n)(m-n)=(m-n)$$ This means that either $m=n$ , or, $-(1/a)(m+n)=1$ . We will consider the latter equation. Or, in other words, $a=-(m+n)$ . Consequently, $n=-m-a$ . Therefore it is possible for $m\neq n$ . Thus, $f$ is not injective.","['proof-writing', 'functions', 'solution-verification']"
4447783,Maclaurin series of $\frac{x^2}{1- x \cot x}$,"I wonder if there is an explicit formula for the Maclaurin expansion of $\frac{x^2}{1 - x \cot x}$ . We know an explicit formula for $1- x \cot x$ . Due to the continued fraction formula for $\tan x$ , we know that all of the coefficients after the first are negative. $\bf{Added:}$ I was lead to this question in trying to prove that in the Maclaurin expansion of $ \frac{x^2}{1 - x \cot x} + \frac{3}{5} ( 1 - x \cot x) - 2 $ all of the coefficients are positive. Since we have formulas for the expansion of the second term, we are interested in explicit formulas for the expansion of the first term. $\bf{Added:}$ We have the continued fraction $$\frac{x^2}{1 - x \cot x} = 3 - \frac{x^2}{ 5 - \frac{x^2}{ 7 - \cdots} } $$ following easily from the continued fraction for $\tan x$ .","['taylor-expansion', 'calculus', 'trigonometry']"
4447820,Smoothness of the function $\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2$,"Let $y\in \mathbb{R}^d$ and suppose that $\mathcal{C}\subset \mathbb{R}^d$ is a compact convex set. I want to know if the function $\psi\colon\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2$ is smooth/Lipschitz-continuous on the interval $[0,1]$ in the sense that $\psi$ is differentiable and there exists some $L>0$ (that may depend on $\mathcal{C}$ and $y$ ) such that \begin{align}
|\psi'(\lambda)- \psi'(\gamma)|\leq L |\lambda - \gamma|, \quad \text{for all } \lambda, \gamma \in [0,1].
\end{align} What I know: The function $\psi$ can be written as the (squared) perspective transform of the function $\varphi\colon y \mapsto \inf_{x\in \mathcal{C}}\|x - y\|$ . That is, $\psi(\lambda)= (\lambda\cdot  \varphi(y/\lambda))^2$ . This implies that I) $\psi$ is convex since the perspective transform of a convex function is convex and the composition of a positive convex function with $w \mapsto w^2$ is convex; and II) that $\psi$ is differentiable on $(0,1]$ , since $\varphi^2$ is differentiable. The latter follows by the fact that $\varphi^2$ is the Moreau-Yosida envelope of the indicator function of the set $\mathcal{C}$ , which Moreau showed is $C^1$ . Ultimate goal: My ultimate goal is to be able to numerically find the minimum of the function $\Theta\colon \lambda \mapsto\lambda^2 + \psi(\lambda)$ in the interval $[0,1]$ very efficiently (for different $y$ 's). The function $\Theta$ is 1-strongly convex (since $\psi$ is convex). But it seems that I am missing smoothness/Lipschitz-continuity to be able to show very fast (e.g. linear) convergence to the minimum using known optimization algorithms.","['smooth-functions', 'convex-analysis', 'functional-analysis', 'lipschitz-functions']"
4447841,Finding the expected stopping time of random walk,"been stuck on this optional stopping theorem quesiton for some time. I feel like I'm heading in the right direction but I am not sure if I am right. Problem Consider a random walk $\left(S_{n}\right)_{n}$ with i.i.d. increments $$
X_{i}= \begin{cases}-1 & 1 / 2 \\ 0 & 1 / 4 \\ 1 & 1 / 4\end{cases}
$$ Suppose $S_{0}=0$ . Let $T=\inf \left\{n: S_{n}\text{ is either }100\text{ or }-10\right\}$ . Find $E[T]$ . Find $\mathbb{P}\left(S_{T}=100\right)$ . My attempt $$E[x_i]=(-1) \cdot \frac{1}{2}+(0) \cdot \frac{1}{4}+(1) \cdot \frac{1}{4} = -\frac{1}{4}$$ We need a martingale to use optional stopping theorem. Hence, I used: $$M_{n}=\sum^{n} x_{i}+\frac{n}{4}$$ I showed this a martingale. However I am stuck on how to move forward from here: $$E\left[M_{T}\right]=E\left[\sum^{T} x_{i}+\frac{T}{4}\right]$$ $$E[T]=\frac{- E\left[S_{T}\right]}{4}$$ So would this mean $E[T] = -400$ or $40$ depending if $S_{n} = -100$ or $10$ .","['expected-value', 'stopping-times', 'random-walk', 'probability']"
4447849,"Does ""Entropy"" explain why the Normal Distribution is so ""Popular""?","Recently, I have learned about the Principle of Maximum Entropy with regards to Probability Distribution - in particular, when certain ""information"" (i.e. constraints) is available about some class of probability distribution function (e.g. domain over which the probability function is defined, expectation, etc.), we can use the principle of Maximum Entropy to determine the ""most informative"" probability distribution function from this class of probability distribution functions in this situation. Apparently, in many real world situations (e.g. when the data is continuous and can take any value between negative infinity and positive infinity) - the Normal Distribution ends up being the probability distribution function with the Maximum Entropy, thus often resulting in the ""most informative"" choice of probability distribution function when compared to any other candidate. My Question: Can this fact about the ""Maximum Entropy"" of the Normal Distribution corresponding to the ""most informative"" probability distribution function be used to explain its prevalence and popularity in statistics? Perhaps this ""most informativeness"" property of the normal distribution ""naturally"" resulted in ""more successful applications"" (e.g. real world statistical models with higher consistency, higher accuracy and lower variance) and in turn made it more ""popular""? Thanks!","['statistics', 'entropy', 'optimization', 'soft-question', 'probability']"
4447852,Finding optimal (=lowest cost) roads connecting all cities where cost of roads are iid exponentially distributed (minimum spanning tree),"I’m trying to solve/understand some exercises about problems involving exponential distributions. The professor in the following video ( https://www.youtube.com/watch?v=DfROPYAjbfM&list=PLV3oHJg9b1NRk4_LKUdqXPoN9jOWRypKI&index=48 ), presents the so-called “Minimum Spanning Tree” problem, which says: There are $n$ cities and roads must be constructed so that every pair
of cities is connected. Assume that all the $\binom{n}{2}$ roads
connecting any two cities have independent costs that are
exponentially distributed with parameter one.  Find the expected value
of the minimal cost $c_n$ to connect all cities, assuming that $n=3$ .
That is, find $E(c_3)$ . He argues as follows. Let $X_i = $ the cost of the $i$ -th cheapest road. Since we only have $3$ cities, the cheapest road connecting all three cities will of course simply be the two cheapest roads together. So, $$E(c_3) = E(X_1 + X_2) = E(X_1) + E(X_2).$$ Now, $X_1$ is the minimum of three independent exponential rv’s each of which has parameter equal to $1$ , so $X_1$ is (by the superposition principle) in distribution equal to $\exp(3)$ . This immediately implies that $E(X_1)=1/3$ .
So far, so good, I (think I) understand it still. But now we wish to calculate $E(X_2)$ . The precise argument that he gives, I don’t know (for I don’t understand it). He says, consider $X_2$ and split it up as $X_2 = X_1 + (X_2-X_1)$ . Then I think (?) he says that the distribution of the increment $X_2-X_1$ has distribution $\exp(2)$ . But this I find weird, since I’d argued as follows: $X_2$ is the minimum of two independent exponentially distributed rv’s, so $X_2$ is $\exp(2)$ . I do not understand it.
He finally concludes that $E(c_3) =  E(X_1) + E(X_2) = 1/3 + (1/3 + 1/2).$ So my problem is that I don't understand why $E(X_2) = 1/3 + 1/2.$ I’m generally confused by the memoryless principle (which, I believe, plays a role in this—although it’s not mentioned explicitly in any of the explanations in the video). Since I’ve gotten stuck, and extremely confused, and no further thinking by myself seems to help, it would be great if someone could shed some light on this. It would be great to have a (formal) explanation, a proof, of this; to have it clearly stated which principle is used in which step, and how the claimed result follows. Thanks in advance!","['graph-theory', 'exponential-distribution', 'probability-theory']"
4447858,"Let $x \in (0,1).$ How to show that $x \in \bigcup \left(0, \frac{n}{n+1}\right)$?","Let $x \in (0,1).$ How to show that $x \in \bigcup \left(0, \frac{n}{n+1}\right)$ ? My attempt: It seems to me that $(0,1) \subseteq \left(0, \frac{n}{n+1}\right)$ is a valid argument for some $n \in \mathbb{N}$ . And if that is the case, it will be easy to show that $x \in \left(0, \frac{n}{n+1}\right)$ . But is that really valid or is there a more convincing logic to use?","['elementary-set-theory', 'proof-writing', 'solution-verification', 'logic']"
4447874,Application of character theory to structure of groups,"One application of character theory in the investigation of structure of finite groups is for Burnside's theorem. Can one mention some other results in Group theory, whose proofs are elementary from character theory, but difficult without character theory? (There was a similar question on mathstack, but I did not find many answers in the direction above.)","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
4447954,Why $[(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ is Gaussian with i.i.d. Gaussian $\mathbf{A}$?,"1. Background: It is presented in the paper of approximate message passing (AMP) algorithm [ Paper Link ] that (the conclusion below is slightly modified without changing its original meaning): Given a fixed vector $\mathbf{x}\in\mathbb{R}^N$ , for a random
measurement matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ with $M\ll
N$ , in which the entries are i.i.d. and $\mathbf{A}_{ij}\sim\mathcal{N}(0,\frac{1}{M})$ , then $[(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ is also a
Gaussian vector whose entries have variance $\frac{\lVert
\mathbf{x}\rVert_2^2}{M}$ (the assumption of $\mathbf{A}$ is in Sec. I-A , and the above conclusion is in Sec. I-D , please see 5. Appendix for more details). 2. My Problems: I am confused by the statements in the paper about the above conclusion in [ 1. Background ], and have three subproblems posted here (the first two subproblems are coupled, and the second one is more general): $(2.1)$ How to prove the above conclusion in [ 1. Background ]? $(2.2)$ For a more general case: $\mathbf{A}_{ij}\sim\mathcal{N}(\mu,\sigma^2)$ with $\mu\in\mathbb{R}$ and $\sigma\in\mathbb{R}^+$ , given $\rho\in\mathbb{R}$ , will the vector $[(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ be still Gaussian? What are the distribution parameters ( e.g. means and variances)? $(2.3)$ When $\mathbf{A}$ is a random Bernoulli matrix, i.e. , the entries are i.i.d. and $P(\mathbf{A}_{ij}=a)=P(\mathbf{A}_{ij}=b)=\frac{1}{2}$ with $a<b$ , will the vector $[(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ obey a special distribution? What are the parameters ( e.g. means and variances)? 3. My Efforts: $(3.1)$ I have been convinced by the above conclusion in [ 1. Background ] by writing a program to randomly generate hundreds of vectors with various $M$ s and $N$ s. The histograms are approximately Gaussian with matched variances. $(3.2)$ I write out the expression of each element of the result vector, but it is too complicated for me. Especially, the elements of $\mathbf{A}^\top\mathbf{A}$ are difficult for me to analyse, since the multiplication of two Gaussian variables are not Gaussian [ Reference ]. $(3.3)$ After a long struggle, I still do not know if there is an efficient way to analyse the subproblems $(2.1)$ , $(2.2)$ and $(2.3)$ . 4. My Experiments: Here is my Python code for experiments: import numpy as np

for i in range(3):
    n = np.random.randint(2, 10000)
    m = np.random.randint(1, n)

    A = np.random.randn(m, n) * ((1 / m) ** 0.5)
    x = np.random.rand(n,)
    e = np.matmul(np.eye(n) - np.matmul(np.transpose(A, [1, 0]), A), x)  # (I-A'A)x

    print(np.linalg.norm(x) * ((1/m) ** 0.5))
    print(np.std(e))
    print('===')

    try:
        from matplotlib import pyplot as plt
        import seaborn
        plt.cla()
        seaborn.histplot(e, bins=300)
        plt.title('mean = %f, var = %f' % (float(e.mean()), float(e.std())))
        plt.savefig('%d.png' % i, dpi=300)
    except:
        pass The standard deviation and the predicted one are generally matched (I have run the code multiple times): 0.7619008975832263
0.7371794446157226
===
0.5792213637974852
0.5936062808535417
===
0.5991335956466841
0.6256026437096703
=== 5. Appendix Here are two fragments of the paper: (from Sec. I-A ) (from Sec. I-D ) Additionally, this paper [ Paper Link ] published on  IEEE Transactions on Image Processing 2021 refers this conclusion (around the Equation (6)):","['statistics', 'convex-optimization', 'normal-distribution', 'random-matrices', 'probability']"
4447970,Expressing $ \sum_{n=1}^{\infty}\frac{\mu(n)}{n^{2}} \left \lfloor x^{1/n}-1 \right \rfloor$ in terms of the nontrivial zeros of $\zeta(s)$,"Let $\left \lfloor \cdot \right \rfloor$ be the floor function. Is there a way to express the function $A(x)$ given by : $$A(x)=\sum_{n=1}^{\infty}\frac{\mu(n)}{n^{2}} \left \lfloor x^{1/n}-1 \right \rfloor\;\;\;\;\; (x \geqslant 2)$$ in terms of the nontrivial zeros of the Riemann zeta function? The motivation behind the question is that there is a somewhat similar situation with [the second Chebyshev function $\psi(x)$ ][1], where we have the formula: $$\log\left(\left \lfloor x \right \rfloor !\right )=\sum_{n=1}^{\infty}\psi\left(\frac{x}{n} \right )$$ and by Möbius inversion, we have : $$\psi(x)=\sum_{n=1}^{\infty}\mu(n)\log\left(\left \lfloor \frac{x}{n} \right \rfloor !\right )=x-\sum_{\rho}\frac{x^{\rho}}{\rho}-\log(2\pi)-\frac{1}{2}\log(1-x^{-2})$$ So, on the LHS we have a summation in terms of a trivial number-theoretic function - $\log\left \lfloor x \right \rfloor !$ - that terminates at a finite $n$ , and on the RHS we have a Fourier series in terms of the nontrivial zeros of zeta. My attempt: We won't deal with the smooth part of $A(x)$ , as it's quite easy to obtain. The oscillatory part is given by: $$f(x)=\sum_{n=1}^{\infty}\frac{\mu(n)}{n^{2}}\left \{ x^{1/n} \right \}$$ For convenience,  we replace f(x) by the function $g(x)$ : $$g(x)=f\left(e^{x}\right)-\frac{3}{\pi^{2}}=\sum_{n=1}^{\infty}\frac{\mu(n)}{n^{2}}p\left(e^{x/n}\right)$$ where $p(\cdot)$ is the sawtooth function, which has the Fourier expansion : $$p\left(e^{x/n}\right)=-\sum_{m=1}^{\infty}\frac{1}{2\pi i m}\left(\exp(2\pi i me^{x/n} )-\exp(-2\pi i me^{x/n} )\right)$$ $$=-\sum_{m=1}^{\infty}\frac{1}{2\pi i m}\left(\exp\left(2\pi i m\left(e^{x/n}-1\right)\right) -\exp\left(-2\pi i m\left(e^{x/n}-1\right)\right) \right)$$ Assuming we can reverse the order of the summation - i have no proof! - we have that : $$g(x)=\sum_{m=1}^{\infty}\frac{\phi(m,x)}{m}$$ where we define : $$\phi(m,x)=-\frac{1}{2\pi i }\sum_{n=1}^{\infty}\frac{\mu(n)}{n^{2}}\left(\exp\left(2\pi i m\left(e^{x/n}-1\right)\right) -\exp\left(-2\pi i m\left(e^{x/n}-1\right)\right) \right)$$ Using the generating function of the [Touchard polynomials][2] $T_{k}(\cdot)$ , we have : $$\frac{1}{2\pi i}\left(\exp\left(2\pi i m\left(e^{x/n}-1\right)\right) -\exp\left(-2\pi i m\left(e^{x/n}-1\right)\right) \right)=\frac{1}{\pi}\sum_{k=0}^{\infty}\frac{\tilde{T}_{k}(2\pi m)}{k!}\left(\frac{x}{n}\right)^{k}$$ Where we define : $$\tilde{T}_{k}(2\pi m)=\frac{1}{2i}\left(T_{k}(2\pi i m)-T_{k}(-2\pi i m)\right)=\sum_{l=0}^{\infty}\frac{(2\pi m)^{l}}{l!}l^{k}\sin\left(\frac{\pi l}{2}\right)$$ Thus : $$\phi(m,x)=-\frac{1}{\pi}\sum_{k=0}^{\infty}\frac{\tilde{T}_{k}(2\pi m)}{\zeta(k+2)k!}x^{k}=-\frac{1}{\pi}\sum_{k=0}^{\infty}\frac{(-1)^{k}\tilde{T}_{k}(2\pi m)}{\zeta(k+2)k!}(-x)^{k}$$ Making use of Ramanujan's master theorem, we have for some vertical strip in the complex $s$ plane : $$\int_{0}^{\infty}\phi(x,m)x^{s-1}dx=-\frac{1}{\pi}\frac{\Gamma(s)G_{m}(s)}{\zeta(2-s)}$$ where we define the Dirichlet-Taylor series : $$G_{m}(s)=(-1)^{-s}\sum_{l=1}^{\infty}\frac{(2\pi m)^{l}}{l^{s}l!}\sin\left(\frac{\pi l }{2}\right)$$ Now, by Millin inversion theorem, we have : $$\phi(m,x)=-\frac{1}{2\pi^{2} i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(s)G_{m}(s)}{\zeta(2-s)}x^{-s}ds$$ The function $G_{m}(s)$ is clearly holomorphic. and if one shifts the path of integration properly, the contributions to the resulting series come only from the nontrivial zeros of $\zeta(2-s)$ Is this line of thought legit?
[1]: https://mathworld.wolfram.com/MangoldtFunction.html [2]: https://en.wikipedia.org/wiki/Touchard_polynomials","['complex-analysis', 'fourier-series', 'analytic-number-theory']"
4447973,Consider vertex $v$ in graph $G$. Let $v$ have at least three adjacent vertices with degrees of two. Prove that $G$ is not a Hamiltonian graph.,"Consider vertex $v$ in graph $G$ . Let $v$ have at least three adjacent vertices with degrees of two. Prove that $G$ is not a Hamiltonian graph. Proof Suppose $G$ is Hamiltonian. Let $x$ , $y$ and $z$ be the vertex adjacent to $v$ . We know that $G$ contains at least 4 vertices: $v$ , $x$ , $y$ and $z$ . $G$ must contain at least one more vertex $w$ because otherwise the sum of all degrees ( $2+2+2+3=9$ ) would not equal twice the number of edges ( $2\cdot 4=8$ ). Now, there cannot be an edge between any of the edges $x$ , $y$ and $z$ , because that would create a cycle, e.g. $vxyv$ . If we tried to construct a Hamiltonian cycle, there would be no way to ""exit"" without passing through the same vertex twice and whatever we try to do (i.e add more vertices), this problem still occurs. Thus $G$ cannot be Hamiltonian. Another approach I tried Suppose $G$ is Hamiltonian with $n$ vertices. Let $x$ , $y$ and $z$ be the vertex adjacent to $v$ . From Dirac's theorem it follows that each vertex must have a degree of $\frac{n}{2}$ and therefore for all pairs of adjacent vertices the sum of their degrees must equal at least $n$ . $G$ must contain at least one more vertex $w$ because otherwise the sum of all degrees ( $2+2+2+3=9$ ) would not equal twice the number of edges ( $2\cdot 4=8$ ). So $n\geq5$ and therefore there can be no edges between any of the pairs $x$ , $y$ and $z$ , because $2+2<5$ . ... Now there should be a way to reach a contradiction here but I don't really know how. Any help or tips are much appreciated!","['graph-theory', 'hamiltonicity', 'discrete-mathematics']"
4447986,Lebesgue differentiation theorem and mollifiers,"Let $F(x,u)$ be Caratheodory function. i.e. $F(x,\cdot)$ is continous for a.e. with respect to $x\in \mathbb{R}$ and $F(\cdot,u)$ measurable for $u.$ In addition, assume that $F(x,\cdot)$ is Lipschitz continuous and $F(x,0)=0.$ Now, suppose $u \in L^1 \cap L^{\infty}(\mathbb{R})$ and $\phi \in C_c(\mathbb{R})$ : how to prove the following statement? \begin{eqnarray}
\lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}^2} F(x,u(y))\eta_{\epsilon}(x-y) \phi(x)dx dy= \int\limits_{\mathbb{R}}F(x,u(x))\phi(x)dx
\end{eqnarray} P.S.: I do understand that we have to apply Lebesgue differentiation theorem but I am looking for a proper justification.
For example what guarantees the measurability of the set $$
\left\{x\in\Bbb R: \lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}} F(x,u(y))\eta_{\epsilon}(x-y) dy=0\right\}\;?
$$","['measure-theory', 'analysis']"
4448007,Error in computing a limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I was given the following limit to compute, $$\lim_{k\to\infty}\left[k-\sqrt{k^{2}+1}\right]$$ My approach: $$= \lim_{k\to\infty}\left[k-\sqrt{k^{2}(1+k^{-2})}\right]$$ $$= \lim_{k\to\infty}[k-k]=0$$ So the following evaluates to $0$ . But my book gives that answer is $\frac{-1}{2}$ . Where did my process go wrong?","['limits', 'calculus']"
4448026,Are there an odd number of numbers in the unit interval?,"Or, four questions that I think are equivalent, but I don't know if will be thought to make sense: Are there an odd number of numbers in $[0, 1]$ ? Are there an even number of numbers in $[0, 1)$ ? Are there an even number of numbers in $(0, 1]$ ? Are there an odd number of numbers in $(0, 1)$ ? My basic reasoning is that there must be the same number of (real) numbers in $[0, 0.5)$ as in $(0.5, 1]$ , meaning that there are an even number of numbers in their union, and therefore additionally including $0.5$ there are an odd number of numbers in $[0, 1]$ . Seems straightforward, but uncountable sets aren't, and therefore I don't know if their parity is a sensible concept to talk about...","['elementary-set-theory', 'real-numbers', 'parity']"
4448078,Bound on expectation of minimum of two stopping times (proving integrability),"Let $(M_n)_{n\geq0}$ be a non-negative martingale with filtration $(\mathcal{F}_n)_{n\geq0}$ . Suppose $M_0=1$ and set $$T=\min\{n\geq0:M_n=0\}.$$ Also, for $R>0$ , consider the stopping time $$T_R=\min\{n\geq0:M_n\geq R\}.$$ In previous parts of the question we have shown that $M_n=0$ for all $n\geq T$ almost surely, and that $\mathbb{P}(T_R<\infty)\leq 1/R$ . Now suppose further that, for some constants $\sigma\in(0,\infty)$ and $\tau\in[1,\infty)$ we have $$\mathbb{E}((M_{n+1}-M_n)^2|\mathcal{F}_n)\geq\sigma^21_{\{T>n\}}\hspace{0.5cm}\text{and}\hspace{0.5cm}M_{T_R}\leq\tau R.$$ Set $Y_n=M_n^2-\sigma^2n$ and $X_n=Y_{n\wedge T\wedge T_R}$ . I have shown that $X_n$ is a submartingale, and am now required to show that $$\mathbb{E}(T\wedge T_R)\leq\frac{\tau^2R}{\sigma^2}.$$ My attempt so far: For any $n\geq0$ we have $$|X_n|=|Y_{n\wedge T\wedge T_R}|=|M_{n\wedge T\wedge T_r}^2-\sigma^2(n\wedge T\wedge T_R)|\leq\tau^2R^2+\sigma^2(T\wedge T_R).$$ Thus, if I can show that $(T\wedge T_R)$ is integrable, we can use the dominated convergence theorem on $X_n$ , which as $n\to\infty$ would converge almost surely to $X_{T\wedge T_R}$ (because integrability also implies that $T\wedge T_R$ is almost surely finite), and the result would follow by the fact that $\mathbb{E}(X_n)\geq\mathbb{E}(X_0)=1$ by the optional stopping theorem. However, I can't see how to prove integrability of $(T\wedge T_R)$ , so any advice and hints would be greatly appreciated!","['conditional-expectation', 'stochastic-processes', 'martingales', 'stopping-times', 'probability-theory']"
4448082,Prove intrinsic metric does not equal metric induced,"Let $U= (0,\infty) \times \mathbb{R}$ and consider the function $f:U \longrightarrow \mathbb{R}^{3}$ defined by: $f(u,v)=(\sinh u\cos v, \sinh u\sin v, \cosh u).$ (i) Let $\mathbb{H}^{2} = \{(x,y,z) \in \mathbb{R}^{3} \mid \, x^{2}+y^{2}-z^{2} = -1, \, \, z>0\}$ be the upper sheet of a hyperboloid in $\mathbb{R}^{3}$ and note the $S=\mathbb{H}^{2} \backslash \{(0,0,1)\}$ . Prove the intrinsic metric $d_{int}$ on $S\subseteq \mathbb{R}^{3}$ does not equal the metric induced by $d$ . So for this all I really know is that $\mathbb{H}^{2}$ with the metric $d(x,y)=\operatorname{arcosh}(-<x\mid y>)$ is a model for the hyperbolic plane, where $<\cdot \mid \cdot >$ is the Lorentz scalar product. I've been told to consider the points $A=(\sinh u,0,\cosh u)$ and $B=(-\sinh u,0,\cosh u)$ as a hint but I'm unsure how to. Any help would be appreciated! Edit: I'm not sure if what was in the previous parts will be relevant, but if so I do already know that $f(u,v)$ is a surface patch and I have the first and second fundamentals forms of $S=f(U)$ along with the Gauss Curvature of $S$","['geometry', 'differential-geometry']"
4448089,Is the following statement true about generating set of a group?,"Suppose $G$ be a group. Let $X$ be a subset of $G$ and $H$ be a subgroup of $G$ . Then is the following statement true?
Suppose that $$G=\langle X, H\rangle.$$ Can we write $$G=\langle X\rangle H?$$ where the above product is internal product of subgroups. My Attempt: if $H$ is trivial subgroup, then above statement is true. For non-trivial subgroup, it seems to me that it is correct. But unable to have a proof of the same. Please help.",['group-theory']
4448140,Exercise 3.13 Paolo Baldi stochastic calculus: bounds on Brownian motion,"I have been brushing up on stochastic analysis before the start of my PhD, and I encountered this exercise on the book ""Stochastic Calculus with applications"" by Paolo Baldi.
The text is as follows. Let $B=(B_t)_{t \in \mathbb{R}_{+}}$ be a Brownian Motion. Prove that for every $a>0,T>0$ , $$P(B_t \leq a \sqrt{t} \ \ \forall \ t \in [0,T])=0$$ Now, my first instinct was to simplify the problem to get some intuition in the matter, and thus I tried to find $$P(B_t < a  \ \ \forall \ t \in [0,T])$$ What I did was applying the reflection principle, that is, $$
\begin{align*}
P(B_t < a  \ \ \forall \ t \in [0,T])&=P(\max_{t \in[0,T]}B_t < a)=1-P(\max_{t \in[0,T]}B_t \geq a) \\
&= 1-2P(B_T\geq a)=P(B_T < a) - P(B_T \geq a)
\end{align*}$$ Now, since $a$ is strictly positive, $P(B_T < a) > \frac{1}{2}$ and $P(B_T \geq a)<\frac{1}{2}$ and thus we obtain $P(B_t < a  \ \ \forall \ t \in [0,T])>0$ . Now, while the derivation I have performed is not in line with what I was asked, I am a bit stuck. I do not understand how the result that was asked at the beginning can possibly hold, if my derivation is correct: intuitively, the $\sqrt{t}$ factor should at best strengthen my result. So I am starting to think that I simply made a mistake, but I cannot find it.","['stopping-times', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4448175,"Solution verification on Munkres Topology, Section 33, Exercise 4","Problem A set $A$ is "" $G_\delta$ set"" in $X$ if $A$ is the intersection of a countable collection of open sets of $X$ . Let $X$ be normal. Show that there exists a continuous function $f: X \longrightarrow [0, 1]$ such that $f(x) = 0$ for $x \in A$ , and $f(x) > 0$ for $x \notin A$ , if and only if $A$ is a closed $G_\delta$ set in $X$ . Note A function satisfying the requirements of this theorem is said to vanish precisely on $A$ . Solution ( $\implies$ ) The unit interval is Hausdorff, and $\{0\}$ is closed. It follows that $f^{-1}(\{0\})$ is closed. Also, $$
f^{-1}(\{0\}) = \bigcap^{\infty}_{n = 1} f^{-1} \left( \left[0, \frac{1}{n} \right) \right),
$$ which is a countable intersection of open sets. ( $\impliedby$ ) Let $A = \bigcap^{\infty}_{n = 1} U_n$ . Since $X$ is normal, we can construct a continuous function $f_n$ which satisfies $f_n(A) = \{0\}$ and $f_n({U_n}^c) = \left\{ \frac{1}{2^n} \right\}$ by applying Urysohn's lemma. It is trivial that $f = \sum^{\infty}_{n = 1} f_n$ converges pointwise(by monotone convergence theorem) and vanishes precisely on $A$ . For any $\epsilon > 0$ , there exists $N \in \mathbb{N}$ such that $\epsilon > \frac{1}{2^N}$ . Then $\left|f_m(x) - f_n(x)\right| < \epsilon$ for any $m, n > N + 1$ . It follows that $f_n$ converges uniformly to some function, namely $f$ , which is desired function. How does it look? Does it contain any critical errors?","['general-topology', 'solution-verification']"
4448213,"Why does the Cauchy PV of $\mathsf E(1/X)$, $X\sim\mathcal N(\mu,\sigma^2)$ accurately reflect the sample mean when $|\sigma/\mu|$ is small?","Suppose $X\sim\mathcal N(\mu,\sigma^2)$ . The first negative moment $\mathsf E(1/X)$ does not exist; however, we can define it in the sense of the Cauchy principal value: $$
\tag{1}
\mathsf E(1/X)\overset{PV}{=}\frac{\sqrt{2}}{\sigma}\,\mathcal{D}\left(\frac{\mu}{\sqrt{2}\sigma}\right),
$$ where $\mathcal{D}(z)=e^{-z^{2}}\int_{0}^{z}e^{t^{2}}\,\mathrm{d}t$ is the Dawson integral. The nonexitence of the $\mathsf E(1/X)$ manifests itself in the sample mean as $$
(\overline{1/X})_n=\frac{1}{n}\sum_{k=1}^n\frac{1}{X_k}
$$ never settles down to any particular value as $n$ increases ( $1/X$ is in the domain of attraction of the Cauchy law and therefore does not abide by the CLT). For example, consider the running mean generated from sampling $X\sim\mathcal N(1,1)$ : Because $|\sigma/\mu|$ is relatively large we regularly observe $X$ near zero and our sample mean never converges. However, in practice, this behavior is not always observed.  Consider the same experiment except with sampling taken from $X\sim\mathcal N(6,1)$ : We see the sample mean does settle down with increasing $n$ .  Moreover, the value the sample mean is approaching is the principal value moment $(1)$ . In theory, this behavior is just an artifact of finite sampling in that if we continue increasing $n$ we should eventually observe values of $X$ close to zero and our sample mean will be disrupted. However, no matter how large I make $n$ , I never actually observe these values in practice due to the relatively small value of $|\sigma/\mu|$ . Does this make $(1)$ a useful analytical expression for the expected value $\mathsf E(1/X)$ so long as $|\sigma/\mu|$ is small?  If so, what would be a coherent theoretical justification for such a statement? For example, if $|\sigma/\mu|$ is small, we may not observe the set $\{X|\epsilon>|X|\}$ in practice and since $\mathsf E(1/X|\epsilon\leq |X|)$ exists, our sample mean is well behaved.  But why should the sample mean converge to $(1)$ in this case? Edit: From my previous question we can further define the higher order moments $\mathsf E(1/X^m)$ for $m\in\Bbb N$ with the use of a generating function via: $$
\tag{2}
\mathsf E(1/X^m):=\frac{\sqrt 2}{\sigma (m-1)!}\partial_t^{m-1}\mathcal D\left(\frac{\mu-t}{\sqrt 2 \sigma}\right)\bigg|_{t=0}.
$$ As before, the sample moments $(\overline{1/X^m})_n=\frac{1}{n}\sum_{k=1}^nX_k^{-m}$ will agree with the ""regularized"" moments $(2)$ whenever $|\sigma/\mu|$ is small.  But why?","['cauchy-principal-value', 'probability-theory']"
4448264,Attempt to improve from $L^1$-boundedness to uniform integrability,"Let $(X_n)_{n\geq0}$ be a discrete-time martingale, and let $T$ be an almost surely finite stopping time such that $$\mathbb{E}(|X_T|)<\infty,\hspace{2cm}\lim_{n\to\infty}\mathbb{E}(|X_n|1_{\{T>n\}})=0.$$ Show that the stopped process $(X_{T\wedge n})_{n\geq0}$ is a uniformly integrable martingale. My attempt so far: For $n\geq0$ we have $$|X_{n\wedge T}|=|X_T|1_{\{T\leq n\}}+|X_n|1_{\{T>n\}}.$$ We have that $|X_T|1_{\{T\leq n\}}$ is increasing to $|X_T|$ , so by monotone convergence and the assumptions on the stopping time $T$ we have $$\lim_{n\to\infty}\mathbb{E}(|X_{n\wedge T}|)=\mathbb{E}(|X_T|)<\infty.$$ Additionally, as $X^T$ is a martingale we have that $|X^T|$ is a submartingale, and so $\sup_{n\geq0}\mathbb{E}(|X_{n\wedge T}|)=\lim_{n\to\infty}\mathbb{E}(|X_{n\wedge T}|)=\mathbb{E}(|X_T|)<\infty$ , (not sure how rigorous that supremum to limit argument is) and thus $X^T$ is $L^1$ -bounded. I'm not sure unfortunately how to show uniform integrability, and so any help / advice would be greatly appreciated!","['stochastic-processes', 'martingales', 'stopping-times', 'probability-theory', 'random-variables']"
4448302,Fubini's theorem and time integrals of stochastic processes,"Let $(X_t)$ be a real continuous stochastic process such that $E[ \int_0^1 (\frac{X_t}{1-t})^2 dt ] <\infty$ . Let $f_t$ be the probability density function (PDF) of $(X_t)$ . Since $Y_t= \frac{X_t}{1-t}$ is $L^2$ in time and space, and that the measure of $\Omega \times [0,1]$ is finite, $(Y_t)$ is also $L^1$ in time and space, and hence, $$I:=E\left[ \int_0^1 \frac{X_t}{1-t} dt \right] <\infty.$$ I would like to know if the following integral is equal to $I$ : $$\int_\mathbb R x \left(\int_0^1 \frac{f_t(x)}{1-t} dt\right) d x,$$ and if so, is $\int_0^1 \frac{f_t(x)}{1-t} dt$ the PDF of the random variable $\int_0^1 \frac{X_t}{1-t} dt$ ? Is the function $x \rightarrow x\int_0^1 \frac{f_t(x)}{1-t} dt$ guaranteed to be integrable in $x$ ? I obtained this integral by following these steps: $$I = \int_0^1 \frac{E[X_t]}{1-t} dt  = \int_0^1 \frac{ \int_\mathbb R x f_t(x) dx}{1-t} dt =\int_\mathbb R x \left(\int_0^1 \frac{f_t(x)}{1-t} dt\right) d x $$ I used Fubini two times, and I am not sure if knowing $I < \infty$ is enough to use Fubini in these ways.","['measure-theory', 'real-analysis', 'stochastic-processes', 'fubini-tonelli-theorems', 'probability-theory']"
4448321,The lyapunov function of gradient system,"Given a dynamical system $$\frac{dx}{dt}=-\nabla f(x)$$ which $x=0$ is the only equilibrium point, i.e. $-\nabla f(x)|_{x=0}=0$ . I am reading this tutorial , and it states: $f(x)$ is a lyapunov function such that $x=0$ is a locally asymptotic stable equilibrium point. I am confusing about this statement because from my understanding, to be a lyapunov function, the function should satisfy the following three conditions. However I can only see the condition (3) is satisfied and have no idea how condition (1) (2) are satisfied. Could someone help me to understand? Thanks a lot! (1) $f(x)|_{x=0}=0$ (2) $f(x)|_{x\neq 0}>0$ (3) $\frac{df(x)}{dt}|_{x\neq 0}<0$ Note: Condition (3) holds, since $\frac{df(x)}{dt}|_{x\neq 0}=\nabla f(x)\cdot (-\nabla f(x))|_{x\neq 0}=-\|\nabla f(x)\|^2_{x\neq 0}<0$ . But I have no idea about condition (1) and (2). I think maybe I have a misunderstanding of this point, maybe this would be correct: $f(x)$ is not neccessary to be a lyapunov function. If in addition, $f(x)$ meets the conditions (1) (2), then it is a lyapunov funciton. Could someone help me to clarify this point? Thanks a lot for any suggestion.","['gradient-flows', 'lyapunov-functions', 'ordinary-differential-equations', 'dynamical-systems']"
4448334,What's the problem with the evaluation map not being continuous?,"When introducing differentiable functions between locally convex spaces, many authors (e.g. Bastiani, Keller, Kriegl-Michor) notice that the evaluation map $$ E\times E^*\to\mathbb R,\qquad (x,L)\mapsto L(x)
$$ is not continuous with respect to any locally convex topology on $E^*$ , where $E$ is a non normable locally convex space and $E^*$ is the set of all continuous linear functionals $E\to\mathbb R$ . Then they argue that, for this reason, it is not good to define $f\colon E\to\mathbb R$ to be continuously differentiable by requiring $$ x\mapsto Df(x)\in E^*$$ to be continuous with respect to some locally convex topology on $E^*$ (say, for example, the finest locally convex topology, in order to make the strongest assumption). My question is: what's the problem with this definition? More precisely, what is an example of missing properties of $f\in C^1$ defined as above? I was thinking about continuity of $f$ being not implied by $f\in C^1$ , or maybe the failure of the chain rule, but I didn't find an explicit issue. For example, the definition is strong enough to have a mean value theorem $$ f(x+h)-f(x)=\int_0^1 Df(x+th)h\,dt$$ Moreover, which of these classical properties actually require the evaluation map to be continuous? I am particularly interested in the case of $E$ Fréchet space.","['locally-convex-spaces', 'derivatives', 'dual-spaces']"
4448356,Continuity of orthogonal projection in Hilbert space with respect to different inner products,"Suppose we are given a Hilbert space A, an infinite dimensional closed subspace B and an inner product $<\cdot,\cdot>$ . A sequence of inner products $<\cdot,\cdot>_i$ converge to $<\cdot,\cdot>$ in the sense that $C^{-1}||v||\le||v||_i\le C ||v||,\forall i,v$ and for any $v,w\in A$ , $<v,w>_i\rightarrow <v,w>$ as $i\rightarrow \infty$ . Can we prove that for any $v\in A$ , the orthogonal projection to B with respect to $<\cdot,\cdot>_i$ , $P_i(v)$ converges to $P(v)$ (defined similarly)? If not, what additional conditions are required?",['functional-analysis']
4448402,Can $\sum_{n=1}^{k} \frac1{\sqrt{n}}$ be an integer for $k>1$?,"In one of new YouTube videos, I've seen the problem: to show that $$16 < \sum_{n=1}^{80} \frac1{\sqrt{n}} < 17$$ I've solved the problem easily, using $$\int_2^{81}\frac1{\sqrt{x}}\, dx < \sum_{n=2}^{80} \frac1{\sqrt{n}} < \int_1^{81}\frac1{\sqrt{x}}\, dx$$ But this problem aimed me on another problem: Can the sum $\sum_{n=1}^{k}  \frac1{\sqrt{n}}$ be an integer number at some positive integer $k>1$ ? Is this problem well-known open or closed problem? Or maybe there is some approach allowing to solve this problem easily without using hard mathematical skills? I've seen the same problem without square roots and it was solved easily.",['number-theory']
4448425,Is every ring a homomorphic image of some abelian group's endomorphism ring?,"Is every ring a homomorphic image of some abelian group's endomorphism ring? I ask because I've never liked to identify rings as being subrings of endomorphism rings.  A subring is basically a ring within another ring, so if you answer ""what is natural about the ring axioms (and by extension, rings as a whole)"" with ""because they are (essentially) subrings of an endomorphism ring"", then to me it feels like saying ""rings are naturals because they are rings""... well yes, but why those axioms? I'm a beginner in ring theory. I don't know if this is true and google didn't really help me out here.","['ring-theory', 'abelian-groups', 'abstract-algebra']"
4448450,Asymptotics of $G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)).$,"Consider $ S^n,$ the space of Schur-convex, simply connected, closed topological $n-$ manifolds as subsets of the unit $(n+1)-$ cube, which include $p=(0,0,\cdot\cdot\cdot,0)$ and $q=(1,1,\cdot\cdot\cdot,1).$ Consider the set of all Schur-convexity preserving maps from $S^n$ to itself. Define a class of $S^n,$ denoted $\zeta(\mathbf X),$ in the following way: Let $ L^n_+$ be the set of all $n$ -dimensional nonnegative random vectors $\mathbf X = (X_1, X_2,\cdot\cdot\cdot,X_n)^⊤$ with finite and positive marginal expectations, and let $\mathbf Ψ^{(n)}$ be the class of all measurable functions from $\Bbb R^n_+$ to $[0, 1].$ Then $\zeta(\mathbf X)$ of the random vector $\mathbf X$ with joint CDF $F$ is: $$\zeta(\mathbf X)=\bigg\{\bigg(\int \psi(\mathbf x)dF(\mathbf x), \int \frac{x_1\psi(\mathbf x)}{E(X_1)}dF(\mathbf x),\cdot\cdot\cdot,\int \frac{x_n\psi(\mathbf x)}{E(X_n)}dF(\mathbf x):\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\}, $$ $$ =\bigg\{\bigg(E\psi(\mathbf X), \frac{E(X_1\psi(\mathbf X))}{E(X_1)},\cdot\cdot\cdot,\frac{E(X_n\psi(\mathbf X))}{E(X_n)}:\psi \in \mathbf Ψ^{(n)}\bigg)\bigg\}. $$ Let me provide a concrete example. I have a 3D graph of $\zeta$ for a bivariate II Pareto distribution with parameters $(\mu_1,\mu_2,\sigma_1,\sigma_2)=(0,0,1,1)$ and $\alpha=9$ so I know the qualitative nature of the shape of any given surface given by any distribution. Here is a picture to give some intuition about the definition: I was interested in obtaining the volume for $\zeta(\mathbf X).$ I calculated it to be: $$\mathrm{Vol}(\zeta(\mathbf X))= \frac{1}{(n+1)!}E\big(|\det(Q)|\big). $$ where $Q$ is a $(n+1)\times(n+1)$ matrix whose $i$ th row is $(1, \mathbf{ \bar X}^{(i)}),$ $i=1,2,\cdot\cdot\cdot , n+1.$ Here I'll use a normalized version of $\mathbf X$ denoted by $\mathbf{\bar X}$ such that $\bar X_i= X_i/E(X_i),$ $i=1,2,\cdot\cdot\cdot, n.$ Consider $n+1$ $\mathrm{iid}$ $n$ -dim. random vectors $\mathbf{ \bar X}^{(1)},\cdot\cdot\cdot, \mathbf{ \bar X}^{(n+1)} $ each with the same distribution as $\mathbf{\bar X}.$ I made the following observation: $$\lim_{n \to \infty} \frac{\mathrm{Vol}(\zeta(\mathbf X))}{\mathrm{Vol}(H^{n+1})}= \lim_{n \to \infty} \mathrm{Vol}(\zeta(\mathbf X))=0$$ where $H^{n+1}$ is the unit $(n+1)$ -cube. How do we get precise asymptotics for the following? $$G(x)=\sum_{n \in \Bbb N}^x \mathrm{Vol}(\zeta(\mathbf X)). $$ Let $\mathbf X$ be normally distributed.","['discrete-geometry', 'asymptotics', 'limits', 'random-matrices', 'random-variables']"
4448455,"Is the one-dimensional Hausdorff measure of the graph of a continuous function defined on $[0,1]$ at least 1?","Let $f:[0,1]\to\mathbb{R}$ be continuous. We define the graph of $f$ to be the set $$\text{graph}\,f=\{(x,f(x)):x\in [0,1]\}.$$ Is $\mathcal{H}^1(\text{graph}\,f)\ge 1$ , where $\mathcal{H}^1$ is the one-dimensional Hausdorff measure on $\mathbb{R}^2$ ? I know that if $f$ is Lipschitz continuous, then it is differentiable almost everywhere and $$\mathcal{H}^1(\text{graph}\,f)=\int_0^1\sqrt{1+(f'(x))^2}dx\ge 1.$$ I believe that the result still holds in the case where $f$ is just continuous but I don't know how to show this.","['measure-theory', 'hausdorff-measure']"
4448471,"Volume above a cone and within a sphere, using triple integrals and cylindrical polar coordinates","Consider the region above the cone $z = \sqrt{x^2+y^2}$ and inside the sphere $x^2+y^2+z^2 = 16$ . Use cylindrical polar coordinates to show that the volume of region $R$ is $\frac{64\pi}{3}(2-\sqrt{2})$ . To solve this, I took the limits of $\theta$ as $0$ to $2π$ . I then took the limits of $z$ by substituting $z^2 = x^2+y^2$ (cone) into the equation of the sphere, giving me $z = 2\sqrt{2}$ as my upper bound (and $0$ as the lower bound). I finally then took the limits of $r$ by changing the equation of the sphere into $r = \sqrt{16-z^2}$ (using $r^2 = x^2+y^2$ ), and $r = z$ (using $r^2 = x^2+y^2 = z^2$ , the equation of the cone). This gave me $$\int_{0}^{2\pi}\int_{0}^{2\sqrt{2}}\int_{z}^{\sqrt{16-z^{2}}}rdrdzd\theta$$ ""∫2π 0 ∫2√2 0 ∫(√16-Z^2) Z  rdrdZdθ"" However, the answer I got from this integral was $2π(16√2 - 16/3(√2))$ , which was incorrect.","['integration', 'volume', 'multivariable-calculus', 'calculus', 'polar-coordinates']"
4448485,Determine the geometric centre of a circle with a quarter missing,"The question I have a circle of radius $a$ , (where $a$ is a known positive constant), centred at Cartesian coordinates $(a,a)$ . The bottom left quarter of the circle is missing. Let the two-dimensional region $R$ be formed by this circle with a missing quarter. I'd like to compute the geometric centre of the region $R$ (or the centre of mass if we were to assume it has uniform density $\rho(x,y)=1)$ . My attempt I know that for uniform density, the geometric centre of a region $R$ , $(\bar x, \bar y)$ is $$\bar x={1\over|R|}\iint_Rx\,\mathrm dA$$ $$\bar y={1\over|R|}\iint_Ry\,\mathrm dA$$ where $|R|$ is the area of the region. I got the area of the region by $|R|=\frac{3\pi a^2}{4}$ . I recognise that these integrals are better suited to be calculated in polar coordinates so I parametrise as such: $$\begin{cases}x&=a\cos(\theta)+a\\y&=a\sin(\theta)+a\end{cases}$$ where $-\frac{\pi}{2}\leq\theta\leq\pi$ . I will then show a calculation for $\bar x$ , which is erroneous (the calculation for $\bar y$ is the same): $$\begin{align*}\bar x&={1\over |R|}\iint_R x\,\mathrm dA\\&={1\over |R|}\int_?^?\int_?^?(a\cos(\theta)+a)\,\mathrm da\,\mathrm d\theta\end{align*}$$ I think I may have made a mistake setting up the integral limits--this part confuses me: $$\begin{align*}&={1\over |R|}\int_{-\pi\over 2}^\pi\int_0^a(a\cos(\theta)+a)\,\mathrm da\,\mathrm d\theta\\&=\frac{4}{3\pi a^2}\times\frac{3\pi a^2+2a}{4}\\&=\frac{2+3\pi}{4}\end{align*}$$ which does not even depend on $a$ . I have clearly done something wrong.","['integration', 'calculus', 'geometry']"
4448531,find the probability both planes can park at a gate,"Two airplanes are supposed to park at the same gate of a concourse. The arrival times of the airplanes are independent and randomly distributed throughout the 24 hours of the day. What is the probability both can park at the gate, provided the first to arrive will stay for two hours, while the second can wait behind it for one hour? The question seems a little ambiguous to me, and I think an example would help. Suppose the first plane arrives at hour 0. Then it'll stay for 2 hours, so as long as the second arrives within that time, can't both airplanes park? What's the significance of saying ""The second can wait behind the first one for one hour""? I found the solution below, but I have some questions about it: Can someone elaborate on why the desired region is obtained by removing the points satisfying $x-1 \leq y \leq x+1$ and $x-2\leq y\leq x+2$ ?","['contest-math', 'geometry', 'probability']"
4448552,"Derivative at a boundary or limit point, Heine definition of limit","I have recently read this post on the differentiability of a function at a boundary point. The answer recieving the top vote created a new definition that can lead to the derivative at a limit point. Definition Let $S\subseteq \mathbb{R}$ , and $f$ a $\mathbb{R}$ -valued function defined over $S$ . Let $x\in S$ be a limit point of $S$ . Then we say that $f$ is differentiable at $x$ if there exists a linear function $L$ such that for every sequence of points $x_n\in S$ different from $x$ but converging to $x$ , we have that $$ \lim_{n\to\infty} \frac{f(x_n) - f(x) - L(x_n-x)}{|x_n - x|} = 0 $$ While I understand the basic idea of this definition, I am a little bit confused with the absolute value bar in the denominator and the linear function $L$ in the numerator. When I first saw this definition, it reminded me of Heine's definition of limit, so I proposed the definition down below. Definition Let $S\subseteq \mathbb{R}$ , and $f$ a $\mathbb{R}$ -valued function defined over $S$ . Let $x\in S$ be a limit point of $S$ . Then we say that $f$ is differentiable at $x$ , if there exists a value k such that for every sequence of points $x_{n} \in S$ different from x but converging to x, we have $$\lim_{n \to \infty} \frac{f(x_{n}) - f(x)}{x_{n} - x} = k$$ Since x is a limit point, I think the above definition is a direct result of Heine's definition of limit. So is this definition accurate? If so, why did the original answerer include an absolute value bar and adopted the linear function? Also, can we as well use Heine definition of limit to define continuity? Although this is not related to the question above (you don't have to answer), I wonder if we limit a function's domain to all rational numbers, theorems such as the intermediate value theorem will still hold.","['limits', 'calculus', 'derivatives', 'real-analysis']"
4448581,Must every group with a one-half morphism embed into a ring in which the morphism is division by 2?,"Let $\alpha$ be an endomorphism of a group $G$ . We say $\alpha$ is a one-half morphism if $\alpha(gg) = g$ for all $g \in G$ . The existence of such a morphism in fact implies that $G$ is abelian, hence the name ""one-half morphism"" rather than ""square-root morphism"". A few examples: Any cyclic group of odd order has a one-half morphism. More generally, if $G$ is finite, $G$ has a one-half morphism if and only if it is abelian and has no element of order 2, if and only if $G$ is a direct product of cyclic groups of odd order. If $G$ is the additive group of a ring in which 2 is invertible, then multiplication by $1/2$ in the ring gives a one-half morphism of $G$ . And certainly any direct sum/product of groups with one-half morphisms yields a group with a one-half morphism. There are groups $G$ with one-half morphisms which are not simply division by 2 in a ring structure on $G$ , e.g., the direct sum of all odd prime cyclic groups. But this embeds into the direct product, in which the one-half morphism is again just division by 2. My question is: is this always true? I.e, is every $G$ with a one-half morphism $\alpha$ simply a subgroup of the additive group of a ring in which $\alpha$ is division by 2?","['group-theory', 'abelian-groups']"
4448596,Find a rectangular equation from a parametric equation. Why is my approach wrong?,"Question I have to find the rectangular equation for $x = \dfrac{t+1}{t}$ and $y = \dfrac{t - 1}{t}$ . If I solve for $t$ in terms of $x$ , I get $t = \dfrac{1}{x - 1}$ , and I substitute this into $y$ , and get $y = - x + 2$ . However, the correct answer is $x^2 - y^2 = 4$ and it is solved a different way. Why is my method wrong? My Calculations $x=\frac{t+1}{t}$ . And so by rearrangement, $t=\frac{1}{x-1}$ And $y=\frac{t-1}t$ , can be simplified by substituting in for $x$ . This gives us $y=(1/(x-1)-1)$ , which simplifies to $y=(1/(x-1)-1)*((x-1)/1)$ After reducing this expression, I get $y=-x+2$ , which is clearly not equal to the correct answer listed above. I am interested in knowing what I have done in my working, and how I can get to the correct answer $x^2-y^2=4$ .","['calculus', 'solution-verification', 'parametric']"
4448619,Triple bar meaning in proof of the Principle of Superposition,"What does the triple bar mean in this context? ""Thus $y(t)≡0,...$ ""","['proof-explanation', 'ordinary-differential-equations']"
4448635,Expected length of sum of vectors,"Suppose we have $n$ arbitrary unit vectors $v_1, v_2, v_3, \dots, v_n$ . (Here, a ""random"" unit vector is defined as $\langle \cos(x), \sin(x) \rangle$ for a random $x$ such that $0 \leq x < 2 \pi$ ). Evaluate the expected value of $$\left|\sum_{i=1}^{n}v_i\right|$$ where $|v|$ denotes the magnitude of $v$ . For $1$ vector, the answer is trivially $1$ . For $2$ vectors, we could fix $v_1$ to the $x$ axis, and the answer would be $$\frac{1}{2\pi} \int_{0}^{2\pi}\left(\sqrt{\left(1+\cos\left(x\right)\right)^{2}+\sin\left(x\right)^{2}}\right)dx$$ First, let us evaluate the numerator. $$\begin{align}&\int_{0}^{2\pi}\left(\sqrt{\left(1+\cos\left(x\right)\right)^{2}+\sin\left(x\right)^{2}}\right)dx \\
&=\int_{0}^{2\pi}\left(\sqrt{2+2\cos\left(x\right)}\right)dx \\
&=2\cdot\int_{0}^{2\pi}\left(\sqrt{\frac{1+\cos\left(x\right)}{2}}\right)dx \\
&=2\cdot\int_{0}^{2\pi}\left(\left|\cos\left(\frac{x}{2}\right)\right|\right)dx \\
&=2\cdot\int_{-\pi}^{\pi}\left(\cos\left(\frac{x}{2}\right)\right)dx \\
&=4\sin\left(\frac{\pi}{2}\right)-4\sin\left(-\frac{\pi}{2}\right) \\
&=8 \end{align}$$ Then, our answer for $2$ vectors would be $\frac{8}{2\pi} = \frac{4}{\pi}$ . For $3$ vectors, we have $$\frac{1}{4\pi^{2}} \int_{0}^{2\pi}\int_{0}^{2\pi}\left(\sqrt{\left(1+\cos\left(x\right)+\cos\left(y\right)\right)^{2}+\left(\sin\left(x\right)+\sin\left(y\right)\right)^{2}}\right)dx\ dy$$ This seems hard to understand, does it have an elementary answer or a way of approaching?","['expected-value', 'multivariable-calculus', 'calculus', 'linear-algebra']"
4448712,"Finding the value of $\lim_{a\to \infty}\int_0^1 a^x x^a \,dx$","I'm trying to find the value of $$\lim_{a\to \infty}\int_0^1 a^x x^a \,dx$$ My attempt:
Let $\epsilon  >0$ be given. $ x\mapsto a^{x}$ is continuous at $ 1$ so there is a $d_a\in ( 0,1)$ such that $|a^{x} -a|< \epsilon $ for all $ x\in [d_a,1]$ . WLOG, let $d_a<1/2$ . $ |\int _{0}^{1} x^{a} a^{x} \ dx-\ \int _{0}^{1} \ ax^{a} |=|\int _{0}^{1}\left( a^{x} -a\right) x^{a} \ dx|\leq |\int _{0}^{d}\left( a^{x} -a\right) x^{a} \ dx|+|\int _{d}^{1}\left( a^{x} -a\right) x^{a} \ dx|$ \begin{align*}
\left|\int _{0}^{1} x^{a} a^{x} \ dx-\ \int _{0}^{1} \ ax^{a} \right| & \leq \left|\int _{0}^{d_a}\left( a^{x} -a\right) x^{a} \ dx\right|+\left|\int _{d_a}^{1}\left( a^{x} -a\right) x^{a} \ dx\right|\\
 & \leq \int _{0}^{d_a}\left( a -a^{x}\right) x^{a} \ dx+\epsilon \left|\int _{d_a}^{1} x^{a} \ dx\right|\\
 & \leq \int _{0}^{d_a}\left( a -a^{x}\right) x^{a} \ dx+\epsilon \\
 & \leq \int _{0}^{1/2} a(1/2)^{a} \ dx-a\int _{0}^{d_a} x^{a} dx+\epsilon \\
 & \leq a(1/2)^{a}  +\epsilon 
\end{align*} $0\leq \lim _{a\rightarrow \infty }\inf |\int _{0}^{1} x^{a} a^{x} \ dx-\ \int _{0}^{1} \ ax^{a} |\leq \lim _{a\rightarrow \infty }\sup |\int _{0}^{1} x^{a} a^{x} \ dx-\ \int _{0}^{1} \ ax^{a} |\leq \epsilon $ Since this is true for every $\epsilon  >0,$ it follows that $ \lim _{a\rightarrow \infty }\int _{0}^{1} x^{a} a^{x} \ dx-\ \int _{0}^{1} \ ax^{a} =0$ . Is my proof correct? Thanks.","['limits', 'solution-verification', 'real-analysis']"
4448744,Show that $f = \sum_{k=1}^{\infty} \frac{1}{k} \mathbb{1}_{A_k}$ is a representation for a measurable function,"I want to show that for a (Borel-)measurable Function $f \in \mathcal{M}^+(\Omega,\mathfrak{S})$ , $f: \Omega \rightarrow [0,\infty])$ exists a representation of the form $$f = \sum_{k=1}^{\infty} \frac{1}{k} \mathbb{1}_{F_k}$$ for $\mathfrak{S}$ measurable $F_k$ . I think I need to look at $F_1:=[f\geq 1]$ and for $k\geq2$ : $$F_k:=[f\geq \frac{1}{k} + \sum_{i=1}^{k-1} \frac{1}{i} \mathbb{1}_{F_i}]$$ but I have no idea if that representation even makes sense. Any help/solutions would be greatly appreciated.","['borel-sets', 'measure-theory']"
4448752,Composition of a relation with its inverse,"I'm self-learning my way through Set theory and came across this question. Now I have a few difficulties to gain an entry to this question. The textbook and the lecture videos which I'm using gives no explanation about composition of relations. It just touches the basics of composition of functions (which I was already familiar from Pre-Calculus). What does it mean to compose two relations, in particular, a relation with its inverse? What is $Id_B$ in simple terms of set relations? Googling led me to wikipedia entry of Identity function , but there I am thrown to entirely new definitions like monoids, morphism etc. which looks like an intellectual leap for me at this stage.","['elementary-set-theory', 'function-and-relation-composition']"
4448796,Can we reach every number of the form $8k+7$ with those $4$ functions?,"Suppose you start with number $1$ and at each step you can apply one of the functions $$\{2x+1, 3x, 3x+2, 3x+7\}$$ to it.
Can you reach at every number of $8k+7$ form? P.s What I already know : the same is not true for the $4k+3$ instead of $8k+7$ , there is a counter example for that ( some number bigger than a thousand which I don't remember exactly) I could prove that there is a number $l$ such that all $2^lk+2^l-1$ are reachable There is computer aided proof to show all numbers like $128k+127$ can be reached (from comment) I was playing with affine maps and their properties which I encountered this example and property which I verified by computer but can't prove it.","['number-theory', 'elementary-number-theory']"
4448826,Is this correct notation for the area under a curve using a limit?,"I've just had a resource tell me the following: The area under a curve from x=a to x=b is A= $lim_{\Delta x{\rightarrow}0}\sum_{k=a}^{b}f(x_{k})\Delta x$ I generally see $x_{k}=a+k\Delta x$ , but in that case I'm struggling to understand how this is correct if the sum limit is b, shouldn't it instead be the number of subintervals? ie $lim_{\Delta x{\rightarrow}0}\sum_{k=1}^{N}f(x_{k})\Delta x$ , where $N=\frac{b}{\Delta x}$ either this is wrong or i am misinterpreting what $x_{k}$ is. Thanks","['integration', 'limits']"
4448855,"Is there a very small gap or no gap in this proof? (""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler.)","I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. Let $V$ be a vector space. Let $V'$ be the dual space of $V$ . Let $W$ be a vector space. Let $W'$ be the dual space of $W$ . Definition: If $T\in\mathcal{L}(V,W)$ , then the dual map of $T$ is the linear map $T'\in\mathcal{L}(W',V')$ defined by $T'(\varphi)=\varphi\circ T$ for $\varphi\in W'$ . Definition: For $U\subset V$ . The annihilator of $U$ , denoted $U^0$ , is defined by $$U^0=\{\varphi\in V' : \varphi(u)=0\text{ for all }u\in U\}.$$ The next result is from ""Linear Algebra Done Right 3rd Edition"". (on p.107) 3.108 $T$ surjective is equivalent to $T'$ injective Suppose $V$ and $W$ are finite-dimensional and $T\in\mathcal{L}(V,W)$ . Then $T$ is surjective if and only if $T'$ is injective. Proof The map $T\in\mathcal{L}(V,W)$ is surjective if and only if $\operatorname{range} T=W$ , which happens if and only if $(\operatorname{range} T)^0=\{0\}$ , which happens if and only if $\operatorname{null} T'=\{0\}$ [by 3.107(a)], which happens if and only if $T'$ is injective. 3.107(a) is the following equation: If $T\in\mathcal{L}(V,W)$ , $$\operatorname{null} T'=(\operatorname{range} T)^0.$$ The author is very kind to the readers and, usually, there is no gaps in his proofs in this book. But I felt a small gap in the proof of 3.108 above. I felt the following fact is not so obvious. Fact 1: If $(\operatorname{range} T)^0=\{0\}$ , then $\operatorname{range} T=W$ . Proof: Assume that $(\operatorname{range} T)^0=\{0\}$ but $\operatorname{range} T\neq W$ . Then, $\dim \operatorname{range} T < \dim W$ . Let $v_1,\dots,v_k$ be a basis of $\operatorname{range} T$ . Let $v_1,\dots,v_k,\dots,v_l$ be a basis of $W$ ( $k<l$ ). Let $\varphi\in W'$ be a linear functional such that $\varphi(v_i)=0$ for all $i\in\{1,\dots,l-1\}$ and $\varphi(v_l)=1$ . Then, $\varphi\neq 0$ and $\varphi(v)=0$ for any $v\in\operatorname{range} T$ . So, $0\neq\varphi\in(\operatorname{range} T)^0$ . This is a contradiction. Does the above fact immediately follow from some famous result? By the way, the author commented about 3.107(a) as follows: The proof of part (a) of the result below does not use the hypothesis that $V$ and $W$ are finite-dimensional. And in 3.108, the author assumed that $V$ and $W$ are finite-dimensional. So, we need to use the assumption that $V$ and $W$ are finite-dimensional in the proof of 3.108. I guess we need to use the assumption that $W$ is finite-dimensional to prove Fact 1 above. I guess we don't need the assumption that $V$ is finite-dimensional to prove  3.108.","['solution-verification', 'linear-algebra', 'dual-spaces']"
4448856,Constructing the solution to a linear backward SDE,"For a linear backward stochastic differential equation (BSDE): for any given $\xi \in L^2(\mathscr{F}_T)$ , $$-dY_s = (a_s Y_s + b_s Z_s +c_s)ds-Z_sdB_s$$ Where $a_t,b_t,c_t \in L^2_\mathscr{F}(0,T;R)$ , $a_t, b_t$ are bounded, with the terminal condition $Y_T = \xi$ . It is easy to check this BSDE has a unique solution (by the theorem of existence and uniqueness of BSDE). One way to find the solution is to use SDE: $$dX_s = a_s X_s ds + b_s X_s dB_s$$ With $X_t = 1$ . We then calculate $d(X_sY_s)$ which we wish is a martingale, so we can use the conditional expectation to find out $X_sY_s$ . Here comes the problem. When I calculate the $d(X_sY_s)$ , the result shows it is not a martingale. The following is my step. $$
\begin{align}
d(X_sY_s) &= X_sdY_s+Y_sdX_s + dX_sdY_s
\\
&=X_s[-(a_sY_s+b_sZ_s+c_s)ds+Z_sdB_s]+Y_S(a_sX_sds+b_sX_sdB_s)+dX_sdY_s
\end{align}
$$ Since we know $dX_sdY_s$ is a quadratic variation, we have: $$d(X_sY_s) = X_sc_sds + X_sZ_sdB_s + b_sY_sX_sdB_s$$ I think there’s something wrong and I really have no idea about how to solve it.","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4448877,Is the space of $n\times n$ real symmetric matrices with strictly positive determinant connected within the vector space of $n\times n$ real matrices?,"I want to make clear that I am aware of the connectedness in the case of general real matrices. But here I ask about the subspace of symmetric ones. If it is not the case, which are the connected components of such topological space? If it is the case, what would be the path on such space connecting say a signature matrix $J$ with positive determinant with the identity $I$ ? That is, give a nontrivial path of symmetric matrices with positive determinant from some signature matrix $J\neq I$ with $\det(J)=1>0$ to the identity $I$ . Remember that a signature matrix is a diagonal matrix whose diagonal entries belong to $\{-1,1\}$ . Note also that, as this matrix in my question has to have positive determinant $-1$ appears an even number of times in such diagonal.","['connectedness', 'topological-vector-spaces', 'matrices', 'linear-algebra', 'symmetric-matrices']"
4448899,Steady state density of Langevin SDE,"Suppose we have the SDE $$dX_t = -U’(X_t) dt+\sqrt{2} D dW_t$$ where $D>0$ . This has Fokker-Planck equation $$\partial_t p =-\partial_x \left[-U’(x) p(t,x)\right]+\partial_{xx} \left[D^2 p(t,x)\right]$$ and to find the steady state density $$f(x)=\lim_{t\to \infty} p(t,x)$$ we take the limit as $t\to \infty$ and assume $\partial_t p(t,x)\to 0$ . This yields the ODE $$\frac{d}{dx} \left[ U’(x) f(x)\right] +D^2 f’’(x)=0.$$ My question How do we solve this ODE to show the steady state is given by $$f(x) \propto e^{-U(x)/D^2} ?$$ My attempt Note we can “factor” out a derivative to write $$\frac{d}{dx} \left[ U’(x)f(x)+D^2 f’(x) \right]=0$$ which implies the expression inside the parentheses must be constant, i.e. $$D^2 f’(x) +U’(x) f(x) =C$$ and we can solve this using the integrating factor method, so that $$f(x)= C_1 e^{-U(x)/D^2}+\frac{C}{D^2} e^{-U(x)/D^2} \int e^{-U(x)/D^2} dx$$ How do I conclude that $C=0$ from here? For we must have $\int_{\mathbb{R}} f(x) dx <\infty$ so we can normalize, a la Boltzmann statistics, but I can’t see the justification. Or perhaps I made a mistake. An alternative derivation is okay but I would prefer to see how this is corrected/finished. Thank you.","['ordinary-differential-equations', 'partial-differential-equations']"
4448902,maximal number of independent subsets (bitstrings with uniform distribution)?,"Let $B=\{0,1\}^n$ be the set of length $n$ bit-strings with the uniform distribution, and let $\mathcal{F}=\{f:B\to \{0,1\}\}$ be the set of all binary functions on $B$ . What is the maximal size of an independent collection of functions from $\mathcal{F}$ ?  I.e., what is the largest $N$ such that there exist $f_i\in\mathcal{F}$ with $$
\mathbb{P}(f_i=\epsilon_i, i\in I)=\prod_{i\in I}\mathbb{P}(f_i=\epsilon_i)
$$ for all choices $\epsilon_i\in\{0,1\}$ and subsets $I\subseteq\{1,\ldots, N\}$ ? I guess this is equivalent to asking for the maximal number of independent subsets of $B$ , i.e. what is the largest number $N$ such that there exist $A_i\subseteq B$ with $$
\mathbb{P}\left(\bigcap_{i\in I}A_i\right)=\prod_{i\in I}\mathbb{P}(A_i),
$$ for all $I\subseteq\{1,\ldots, N\}$ , where $\mathbb{P}(A)=|A|/2^n$ ? I don't see any way to count these off the top of my head, although there are maybe some obvious sets of independent functions (e.g. the coordinate functionals).  Perhaps this is the best one can do, the intuition being "" $n$ independent bits $\Leftrightarrow$ $n$ independent subsets""? (Disregarding the ""trivial"" events $\emptyset$ , $B$ , corresponding to constant functions.) The paper Independent Events in a Discrete Uniform Probability Space considers the problem more generally, with the uniform distribution on finite sets. Theorem 5 there backs up the ""intuitive"" answer $N=n$ to my question (ignoring constant functions/(co)null sets).","['combinatorial-designs', 'combinatorics', 'probability', 'reference-request']"
4448910,Globally asymptotic stable gradient system has unstable point,"Given a gradient system $$\frac{d\theta_1}{dt}=-\sin(\theta_1-\theta_2)$$ $$\frac{d\theta_2}{dt}=-\sin(\theta_2-\theta_1)$$ The system is a gradient system since $$\frac{d\vec \theta}{dt}=-\nabla V(\vec\theta) = -\nabla(1-\cos(\theta_1-\theta_2))$$ Since the system is invariant by replacing all $\theta_i$ to $\theta_i+\alpha$ , this will leads to a continuum of equilibria. To remove this freedom, we fix $\theta_1=0$ , then the system becomes $$\frac{d\theta_2}{dt}=-\sin(\theta_2)$$ It has two equilibrium points $\theta_2=0$ and $\theta_2=\pi$ . The energy function is $V(\theta_2)=-\cos(\theta_2)+1.$ The second derivative of $V(\theta_2)$ is $\cos\theta_2$ . Since the second derivative is negative on $\theta_2=\pi$ , it is an unstable equilibrium point. Since the second derivative is positive $\theta_2=0$ , it is a locally asymptotically stable (LAS) equilibrium point. Since this is a gradient system, only $\theta_2=0$ is locally asymptotically stable, then it is globally asymptotically stable (GAS). But it is a little counterintuitive in the sense of convexity of energy function: how comes that the energy function is concave on $\theta_2=\pi$ , and $\theta_2=0$ is globally asymptotically stable? For example, let say the figure of an energy function looks like the following and it is convex on the local minimum, and concave on the local maximum. How come all trajectories will converge to the local minimum? As illustrated in the figure, trajectories starting from the part behind the local maximum cannot converge to the local minimum.","['gradient-flows', 'ordinary-differential-equations', 'stability-in-odes', 'vector-analysis', 'dynamical-systems']"
4448948,Finding the number of roots of $f(z) = z^5 + z^3 + 3z + 1$ in the unit disk,Suppose we have $$f(z) = z^5 + z^3 + 3z + 1$$ Find how many roots this function has in the open unit disc $\{z : |z| < 1\}$ . Here's what I think about it: I tried to split $f$ into two functions $$g(z) = z^5 + z^3$$ $$h(z) = 3z + 1$$ and use Rouché's theorem (to prove that $f = g + h$ has only one root). But $|h(z)| > |g(z)|$ has some troubles on unit circle (at least at $-1$ point). Is there any way to use Rouché's theorem here? Maybe we can use it twice or  work around problems at this point? Have no idea how to fix it.,"['complex-analysis', 'roots', 'polynomials', 'quintics']"
4448997,Why is complex derivative direction independent,"Visual complex analysis by Tristan Needham provides a great intuition by explaining the beauty of complex analysis. I have a question regarding the complex derivative. The image above shows a function $w(z)$ from $\mathbb{C}$ to $\mathbb{C}$ . When the complex number $z$ , is moved to $z+dz_{1}$ or $z+dz_2$ the corresponding values are $w(z+dz_1)$ and $w(z+dz_2)$ . Let's denote $dw_1=w(z+dz_1)-w(z)$ and similar thing with $dw_2$ . In that book complex derivative is defined, as a complex number such that $$dw_1=w'(z)dz_1$$ $$dw_2=w'(z)dz_2$$ .... My question:- Now, I am unable to convince myself about why $dz_1$ and $dz_2$ are multiplied with the same complex number to get $dw_1$ and $dw_2$ respectively.","['complex-analysis', 'complex-numbers']"
4449004,APMO 2020 Geometry Problem | Proving lines to be concurrent,"PROBLEM Let $\Gamma$ be the circumcircle of $∆ABC$ . Let $D$ be a point on the side $BC$ . The tangent to $\Gamma$ at $A$ intersects the parallel line to $BA$ through $D$ at point $E$ . The segment $CE$ intersects $\Gamma$ again at $F$ . Suppose $B$ , $D$ , $F$ , $E$ are concyclic . Prove that $AC, BF, DE$ are concurrent. MY APPROACH Let $X$ be some point on tangent line then, $$\angle XAB=\angle ACB=\angle ACD \quad(1)$$ Since $AB ||DE$ , we cany say that $$\angle AED=\angle XAB \quad(2)$$ From $(1)$ and $(2)$ , we can say that $$\angle AED=\angle ACD$$ Therefore $ADCE$ is cyclic. Consider the Circles of $BDEF$ , Circles of $ADCE$ and Circumcircle of $∆ABC$ . We see that $AC,BF$ and $DE$ are the radical axes of these circles which means they are concurrent at the radical Centre. Hence, Proved! But APMO - $2020$ haven't included my solution in their Official Answers . Is something wrong with my Proof? DIAGRAM","['contest-math', 'angle', 'circles', 'geometry', 'triangles']"
4449019,Rademacher random variables limit,"Let $X_1, X_2,\dots$ be an i.i.d sequence of random variables on a probability space $(\Omega,$ F $, \mathbb{P})$ with $\mathbb{P}(X_1 = 1) = \mathbb{P}(X_1 = −1) = \frac12$ . Show that $\phi_{X_i} (t) = \cos(t)$ . ( $\phi$ denotes the characteristic function) Use 1 to prove that for every $t\in \mathbb{R}$ $\lim_{n\to\infty}\cos^n(\frac{t}n)=1$ . Use 2 to show that the weak law of large numbers holds for $X_i$ ’s, i.e., that $\frac{X_1+\dots+X_n}n\to0$ in prob. My attempt: We have $\phi_{X_i}(t)=\mathbb{E}[e^{iX_it}]=\frac{e^{it}+e^{-it}}2=\cos(t)$ Here I already get stuck: how should I use my prevoius result? Do I have to consider maybe $(\cos(\frac {t}n))^n=(\frac{e^{i\frac{t}n}+e^{-i\frac{t}n}}2)^n$ ?? From a Corollary I know that for $S:=X_1+\dots+X_n$ we have $\phi_S(t)=\phi_{X_1}(t)\cdots\phi_{X_n}(t)=\cos^n(t)\not=\cos^n(\frac{t}n)$ , so how do I employ part (2)?
Then maybe we should get that $\frac{X_1+\dots+X_n}n\to\frac1n\to0$ pointwise (so why ask for convergence in probability? Thanks for any advice","['characteristic-functions', 'rademacher-distribution', 'probability-theory', 'probability']"
4449038,Can the product of a $\sigma$-finite measure with a non-$\sigma$-finite one be a $\sigma$-finite measure?,"EDIT: To avoid the situation described in Mason's comment, all the measures considered below are assumed to be non-zero. Given two measurable spaces $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ , assume there is a $\sigma$ -finite measure $\pi\neq 0$ be a $\sigma$ -finite measure on $(X\times Y,\mathcal{A}\times\mathcal{B})$ such that $$
\pi(A\times B)=\mu(A)\nu(B)
$$ for all measurable sets $A\in\mathcal{A}$ and $B \in\mathcal{B}$ , where $\mu$ is a $\sigma$ -finite measure on $(X,\mathcal{A})$ and $\nu$ is a measure on $(Y,\mathcal{B})$ . Can we conclude that $\nu$ is $\sigma$ -finite too? Stated differently, let $(X,\mathcal{A},\mu)$ be a $\sigma$ -finite measure space (i.e., the measure $\mu\neq 0$ is $\sigma$ -finite) and $(Y,\mathcal{B},\nu)$ be a non- $\sigma$ -finite measure space (i.e., the measure $\nu\neq 0$ is not $\sigma$ -finite).
Is there a product measure $\pi$ on $(X\times Y,\mathcal{A}\times\mathcal{B}) $ which is $\sigma$ -finite?",['measure-theory']
4449065,Solve the integral $\int_{-\frac{\pi}2}^{\frac{\pi}2} \frac{\sin^3x}{\tan^3x+\cot^3x} dx$,"Question Solve the integral, $$\int_{-\frac{\pi}2}^{\frac{\pi}2} \frac{\sin^3x}{\tan^3x+\cot^3x} dx$$ Attempt I converted the equation in terms of $\sin(x)$ and $\cos(x)$ using the definition of $\tan(x)$ and $\cot(x)$ , and then applied the substitution $t=\sin(x)$ , however, this has proved itself to be quite a difficult integral to resolve in and of itself. I would be great full for any suggestions of a more compact method. Any hints would be also be greatly appreciated.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'trigonometry']"
4449076,"Does a covering by trivializations imply existence of a linearly compatible one, for a smooth $\mathbb{R}^k$ fiber bundle?","Let $\pi : E\rightarrow B$ be a surjective submersion between smooth submanifolds. Let $k=\mathrm{dim}\,\mathrm{ker}\,\pi_*$ . We define a local trivialization of $\pi$ with fiber $\mathbb{R}^k$ to be a pair $(U,\phi)$ where $U\subset B$ is open and $\phi : \pi^{-1}(U) \xrightarrow{\approx} U\times \mathbb{R}^k$ is a diffeomorphism such that $\phi(y) \in \{\pi(y)\} \times \mathbb{R}^k \;\forall y\in \pi^{-1}(U)$ . We define an atlas of trivializations to be a collection $\{(U_\alpha,\phi_\alpha)\}_\alpha$ where each $(U_\alpha,\phi_\alpha)$ is a local trivialization and the $U_\alpha$ 's cover $B$ . We say an atlas of trivializations is linearly compatible if for every indices $\alpha,\beta$ , for every $p \in U_\alpha \cap U_\beta$ ,
the map $\mathbb{R}^k \rightarrow \{p\} \times \mathbb{R}^k : v \mapsto \phi_\alpha(\phi_\beta^{-1}(p,v))$ followed by $\{p\} \times \mathbb{R}^k \rightarrow \mathbb{R}^k : (p,w) \mapsto w$ is $\mathbb{R}$ -linear. My question is, if $\pi$ admits an atlas of trivializations with fiber $\mathbb{R}^k$ , then does it also admit a linearly compatible atlas? Edit: as has been pointed out in the comments, this fails if we relax $E$ and $B$ to topological manifolds and ""smooth""/""diffeo"" to ""continuous""/""homeo"". I've now changed the question to focus on the smooth case.","['general-topology', 'vector-bundles', 'differential-geometry']"
4449078,Do Carmo Problem - Section 3.3 - 12,"The problem statement is as follows - Consider the parametrised surface $$ x(u,v) = \bigg(\sin u\cos v , \sin u\sin v , \cos u + \log(\tan\frac{u}{2}) 
+\phi(v)  \bigg) $$ where $\ \phi\ $ is a differentiable function. Prove that a. The curves $\ v \ $ = const. are contained in planes which pass through the z axis and intersect the surface under a constant angle $\ \theta\ $ given by $$ \cos \theta = \frac{\phi'}{\sqrt{1+(\phi')^2}}  $$ My attempt : For given parametrized surface $$ x(u,v) = \bigg(\sin u\cos v , \sin u\sin v , \cos u + \log(\tan\frac{u}{2}) 
+\phi(v)  \bigg) $$ with $ \ \phi \ $ - differentiable function we can write $ x_u = \bigg( \cos u\cos v \ ,\cos u \sin v \ , \cot u \cos u \bigg)$ $ x_v = \bigg( -\sin u\sin v \ ,\sin u \cos v \ , \phi' \bigg)$ $x_u\wedge x_v  = \begin{vmatrix}
i & j & k\\ 
\cos u \cos v & \cos u \sin v  & \cot u \cos u \\ 
-\sin u \sin v & \sin u \cos v & \phi' 
\end{vmatrix} \\ = i\bigg( \phi' \cos u \sin v - \cos ^2 u \cos v \bigg) - j\bigg( \phi' \cos u \cos v + \cos ^2 u \sin v \bigg) + k\bigg( \sin u \cos u  \bigg) $ The unit normal  N = $\frac{x_u\wedge x_v}{||x_u\wedge x_v||}$ . We first calculate $||x_u\wedge x_v||$ $$||x_u\wedge x_v|| \ = \ \sqrt{(\phi')^2\cos ^2u \sin ^2 v + \cos ^4u \cos ^2v - 2\phi'\cos ^3u \sin v \cos v + (\phi')^2\cos ^2u \cos ^2v \\ + \cos ^4u \sin ^2v  + 2\phi'\cos ^3u \cos v\sin v   + \sin ^2u \cos ^2u }$$ $||x_u\wedge x_v|| = \cos u \sqrt{1 + (\phi')^2}$ Hence, the unit normal to surface $ x(u,v) $ is given by $$N = \frac{1}{\sqrt{1 + (\phi')^2}} \bigg( \phi' \sin v - \cos u \cos v , \  - \phi' \cos v - \cos u \sin v , \  \sin u   \bigg)$$ Now, the unit normal vector to the plane passing through the curve $ v $ = const. and the z-axis , say V, will be given by $ V = (-\sin u, \cos u , 0 ) $ and then $ \theta $ will be $ \cos\theta = N\cdot V$ $$ \cos \theta  = \frac{\phi' (-\sin ^2 v ) + \cos u \cos v \sin v - \phi' \cos ^2 v - \cos u \sin v \cos v }{\sqrt{1+ (\phi')^2}}$$ $$ \cos \theta  = \frac{ - \phi' } {\sqrt{1+ (\phi')^2}}$$ Why am I getting the undesired -ve sign here? Is it due to the wrong direction of vector V or something else went wrong?",['differential-geometry']
4449079,Very fast but inaccurate estimations of multivariate Gaussian integral over a hypercube,"$\def\Z{\mathbb{Z}}\def\R{\mathbb{R}}\def\A{\mathcal{A}}\def\N{\mathcal{N}}$ I'm working on 4D positive real values, i.e. $\R^4_{\geq 0}$ , where it is gridized with hypercubes of side length $a > 0$ . So, an index vector $I = [i ~~ j ~~ k ~~ \ell]^T$ uniquely represents the region $$\A_I := \{ x \in \R^4_{\geq 0} ~~|~~ a I_n \leq x_n < a (I_n+1),~~ n = 1,2,3,4 \}$$ where $x_n$ and $I_n$ represents the $n$ th entity of $x$ and $I$ respectively. I have a multivariate Gaussian distribution in $\R^4$ , which is $N(m,P)$ , i.e. with mean $m$ and covariance matrix $P$ where $m \in \R^4$ and $P\in\R^{4\times 4}$ is symmetric. Let $m \in \A_I$ for some $I$ and $\N_I := \{J \in \Z^4 ~~|~~ I_n-1 \leq J_n \leq I_n+1\}$ be the index set that represents the immediate neighbours of $\A_I$ (including itself). For each $J \in \N_I$ I want to estimate the Gaussian integral over $\A_J$ , i.e. $$\alpha_J := |2 \pi P|^{-1/2} \int_{\A_J} \exp\left(-\frac{1}{2} (x-m)^T P^{-1} (x-m)\right) dx $$ where $|\cdot|$ represents the determinant. So, I need to estimate $3^4=81$ numbers very fast, e.g. under 1 milliseconds on a typical mid-range laptop if possible. The estimations do not need to be very accurate though, just rough values. One method that comes to mind is to calculate the integrand for the vertices and midpoint of regions, and use trapezoidal integration. This would require the calculation of the integrand $4^4+3^4=337$ times, which might be too much. Another method could be to calculate the integrand for only the midpoints and use them as estimations or maybe use some sort of interpolation and averaging centered around $m$ . Although both methods can run very fast, I'm not sure they would create relevant estimations especially when $m$ is close to some boundary between regions. Is there a name for these kind of problems? Is there very fast and robust algorithms (produces relevant results) suited to this problem? Note: The values of $m$ and $P$ are not known beforehand. So, a precomputation table cannot be used (I guess). But if the values in a table for a specific $P$ can be used by adjusting the values for the incoming $P$ , that would be ok.","['multivariable-calculus', 'gaussian-integral']"
4449108,Is there an easier way to derive this equation for this mechanics problem?,"We're given a semicircle whose total mass $M$ is evenly distributed along its contour and a little particle of mass $m$ is dropped from its top left corner as in the image below all over an horizontal plane. There is no friction between the semicircle and the particle nor between the semicircle and the ground (so no rolling without slipping!). $P_0$ represents the particle at the start of the movement and $P$ represents it on a random position after the release of it. The center of the semicircle is $O$ . The angles that I used to describe this crazy movement were $\theta = \angle P_0OA$ and $\alpha = AOP$ and, as I'm not that great with rigid body dynamics, I actually solved this problem using Lagrange's equations and I couldn't help but to notice that one of the equations that describe this movement is fairly simple: $$2\frac gR \cos (\alpha) = (\ddot \theta + \ddot \alpha) 2\cos (\theta) - \pi \ddot \theta \sin(\theta+\alpha)$$ it is pretty remarkable that this equation does not involve the masses $m$ nor $M$ and I've been wondering if there is an easy way to get to this equation by analysing the momentum of the semicircle. But I struggled a bit with the acceleration of the system in a non inertial frame and I wonder if someone could hint on how to tackle it without Lagrange's equations. This is the Lagrangian I got and I'm 100% sure that it is correct: $$ \mathcal L (\theta, \alpha)= -\frac{R^2}{2(M+m)}(\frac 2{\pi}M \dot \theta \cos(\theta) + m \sin(\theta+\alpha)(\dot \theta + \dot \alpha))^2 + \frac{MR^2 \dot \theta ^2}2 + \frac{mR^2(\dot \theta + \dot \alpha)^2}2 + mgR\sin(\theta +\alpha) + \frac 2{\pi} MR\cos(\theta)g$$ EDIT: @eyeballfrog managed to turn that equation in a pendulum like equation: $$\frac gR \sin\theta +(\pi/2)\ddot{\theta} - \cos\theta \ddot{x} = 0$$ where $x$ is the size of $OO_0$ (with signal), looks like a pendulum in a non inertial frame","['classical-mechanics', 'kinematics', 'rigid-transformation', 'ordinary-differential-equations']"
4449151,The derivative of a semialgebraic map is semialgebraic,"Coste's notes on semialgebraic geometry have the question: If $f:U \to \mathbb{R}$ is semialgebraic, with $U$ an open semialgebraic set, then each partial derivative $\dfrac{\partial f}{\partial x_{i}}$ is semialgebraic Once, a teacher told me how to answer this question. He said to write the graph of $\dfrac{\partial f}{\partial x_{i}}$ as $$\left\{(x,y) \in U \times \mathbb{R}: \forall \varepsilon > 0, \exists \delta > 0 \left(\|t\|^{2} \geq \delta \lor \left|\left|y - \dfrac{f(p+te_{i})-f(p)}{t}\right|\right|^{2} < \varepsilon \right) \right\}$$ and use Tarski-Seidenberg to finish the proof. However, upon reviewing the proof, it is wrong, since this set is not defined using the first-order language of ordered fields. $f$ is a semialgebraic function, not a polynomial. I don't know how to proceed here, and I appreciate any help.","['algebraic-geometry', 'derivatives', 'real-algebraic-geometry', 'semialgebraic-geometry']"
4449267,A version of Brower's fixed point theorem for contractible sets?,"Brouwer's fixed point theorem states that a continuous map $f:B^n\to B^n$ ( $B^n\subset\Bbb R^n$ being the $n$ -dimensional ball) has a fixed point. It is clear that we can replace $B^n$ with a space $X$ homeomorphic to the ball. Question: Is there a version of this fixed point theorem that only requires $X$ to be compact and contractible? Do we need more, e.g. locally contractible or something like this? I am happy to assume that $X\subset \Bbb R^n$ for some finite $n\ge 1$ . This question has a note about a contractible space for which the fixed point theorem does not hold, but it is unclear to me whether the counterexample stands if $X$ is required to be a subset of a finite-dimensional Euclidean space.","['fixed-point-theorems', 'geometric-topology', 'general-topology', 'algebraic-topology', 'compactness']"
4449280,"Spivak, Chapter 10, ""Differentiation"", problem 9: Rate of change of distance between moving particles. Is the solution manual solution incorrect?","Spivak, Chapter 10, ""Differentiation"", problem 9. Particle A moves along the positive horizontal axis, and particle B along the graph of $f(x)=-\sqrt{3}x, x \leq 0$ . At a certain time, A
is at the point (5,0) and moving with speed 3 units/sec; and B is at a
distance of 3 units from the origin and moving with speed 4 units/sec.
At what rate is the distance between A and B changing? The solution I came up with is similar to the solution manual solution, but it seems we disagree on certain signs which leads to different results. My result is that the distance is changing at a rate of $-\frac{5}{14}$ units/sec (ie it is decreasing), and the solution manual says the distance is increasing at $\frac{83}{14}$ units/sec. Given that the vertical distance is certainly decreasing, and that since horizontally both particles are moving to the right the relative speed must be smaller than 3 units/second, it seems to me that the result $\frac{83}{14} \approx 5.92$ units/sec is incorrect. Here is my solution Everything in this problem is analyzed at a single point in time. The variables below are all functions of time, though I will omit the time parameter. Initial Position of Particle B $$D_{BO}^2=x_B^2+y_B^2$$ $$y_B=-\sqrt{3}x_B$$ $$D_{BO}^2=x_B^2+3x_B^2=4x_B^2\tag{1}$$ $$9=4x_B^2$$ $$x_B=\frac{3}{2}$$ $$\implies y_B=-\frac{3\sqrt{3}}{2}$$ Rates of Change of B's Coordinates $$y_B(t)=-\sqrt{3}x_B(t)$$ $$y_B'=-\sqrt{3}x_B'$$ Here is a tricky part regarding signs. If we take the square root of $(1)$ then $$D_{BO}=2x_B\tag{2}$$ Note that $D_{BO}$ is a distance, so it must be positive. $x_B$ on the other hand is actually negative. But $(2)$ is actually incorrect, it should be $$D_{BO}=2|x_B|$$ $$=\begin{cases} -2x_B, x_B<0 \\ 2x_B, x_B \geq 0 \end{cases}$$ But in our single point in time we are in a situation where $x_B<0$ . In this scenario, we have $$D_{BO}'=-2x_B'$$ Note that $D_{BO}'=-4$ because particle B is moving towards the origin. $$-4=-2x_B'$$ $$x_B'=2$$ $$\implies y_B=-2\sqrt{3}$$ These derivatives make sense: $x_B$ is increasing from a negative value towards $0$ , and $y_B$ is decreasing from a positive value. Rate of Change of Distance Between the Two Particles $$D = \sqrt{y_B^2+(x_A-x_B)^2}$$ Note that since $x_B$ is negative, I am subtracting from $x_A$ to get difference in x-coordinate between the particles. $$D'=\frac{y_By_B'+(x_A-x_B)(x_A'-x_B')}{\sqrt{y_B^2+(x_A-x_B)^2}}$$ We have the values of all these variables at the time we are considering. If we plug them in we get $$D'=-\frac{5}{14}$$ Thus the distance between the particles is decreasing at a rate of $\frac{5}{14}$ units/sec. Is this correct (ie is the solution manual wrong?)","['calculus', 'solution-verification', 'derivatives']"
4449291,Good applied differential geometry books,"I'm searching a book which goes about how Differential Geometry can be applied to solve Real world problems. I tried William L Burke's book, but I found it to be all over the place. The information, at least in the first chapter, seemed to have no continuity. A book that I liked is Tristan Needham's Visual Differential Geometry, and while it only goes indirectly over the matter, I liked Penrose's Road to reality as well.","['reference-request', 'differential-geometry']"
4449305,Calculate the power series centered in $z_0=1$,"I did this, I don't know if is correct $$
\begin{gathered}
h(z)=\left(\frac{z}{z+1}\right)^{2} \\
u=z-1,z_0=1 \\
h(z)=\left(\frac{z}{z+1}\right)^{2}=z^{2} \frac{1}{(z+1)^{2}}=z^{2} \frac{1}{(1-(-z))^{2}} \rightarrow \text { geometric series } \\
\sum_{n=0}^{\infty} u^{n}=\frac{1}{1-u}, \quad|z|<1, \\
\\\frac{d}{d z} \frac{1}{1-u}=\frac{1}{(1-u)^{2}}
\\\left(\frac{z}{z+1}\right)^{2}=z^{2} \frac{d}{d u} \sum_{n=0}^{\infty} u^{n}=z^{2} \sum_{n=0}^{\infty} n u^{n-1}=z^{2} \sum_{n=0}^{\infty} n(z-1)^{n-1}=\sum_{n=0}^{\infty} n(z-1)^{n+1} \\
\left(\frac{z}{z+1}\right)^{2}=\sum_{n=0}^{\infty} n(z-1)^{n+1},|z-1|<1 \rightarrow|z|<2 \\
\\
\end{gathered}
$$ I need to use a geometric serie to Calculate the power series I have a problem, the problem is in $z_0=1$ I don't know if I use it right","['complex-analysis', 'power-series']"
4449306,"Prove that points $E, H,$ and $F$ are collinear","Let $\triangle ABC$ be a triangle. Let $M$ be the midpoint of side $[BC]$ . $H,$ and $I$ are respectively the orthocenter and incenter of $\triangle ABC$ . Let $D = (MH)\cap(AI)$ . $E$ and $F$ are the feet of perpendiculars from $D$ to $(AB)$ and $(AC)$ , respectively. Prove that $E, F$ and $H$ are collinear. Here is the source of the problem (in french) here I have solved it using barycentric coordinates. As a matter of fact, one can get that lines $(MH)$ and $(AI)$ have equations, respectively: $\left[\displaystyle \frac{c^2-b^2} {S_{BC}}:\frac1{S_A}:-\frac1{S_A}\right]$ and $\left[\displaystyle 0:-c:b\right]$ $($ Here, $S_A=\displaystyle \frac{b^2+c^2-a^2}2$ , define cyclically $S_B$ and $S_C$ , it's Conway's Notation) Intersecting these lines gives un-normalized: $D\left(\displaystyle\frac{S_{BC}}{S_A(b+c)}:b:c\right)$ , which in turn gives: $F\left(\displaystyle\frac{S_C}{b}+\frac{S_{BC}}{S_A(b+c)}:0:\frac{S_A}{b}+c\right)$ and: $E\left(\displaystyle\frac{S_B}{c}+\frac{S_{BC}}{S_A(b+c)}:\frac{S_A}{c}+b:0\right)$ . Now, clearly the deteminant formed by $E,F$ and $H$ is null. The conclusion follows. What I'm asking for is a synthetic solution to this problem. I have tried to come up with one, but couldn't. The main thing I noticed is the line connecting the two touch-points of the incircle with sides $(AC)$ and $(AB)$ is parallel to line $(EF)$ , so maybe what we're looking for is a convenient homethety.","['contest-math', 'geometry']"
4449399,Proof that $\inf(A)=-\sup(-A)$,"Prepping for a master's program in pure mathematics. I'm working on my problem-solving skills and was hoping someone would kindly verify my proof. Let $A$ be a nonempty set of real numbers which is bounded below. Let $-A = \{-x| x \in A\}$ . Prove  that $$
\inf A = -\sup (-A)
$$ My attempt: Lets begin with a lemma. Lemma: $x$ is a lower bound of $A$ if and only if $-x$ is an upper bound of $-A$ . proof of lemma: For each $a \in A$ , \begin{align*}
x \text{ is a lower bound of } A &\iff x \leq a \\
&\iff -x \geq -a \\
&\iff -x \text{ is an upper bound of } -A. 
\end{align*} Now, let $\inf A = \alpha$ . Then $- \alpha$ is an upper bound of $-A$ . Let $y \in \mathbb{R}$ and suppose $y < -\alpha$ . So, $-y > \alpha$ which implies $-y$ not a lower bound $A$ . Hence, $y$ is an upper bound of $A$ . Therefore, $-\alpha = \sup (-A)$ and $\inf (A) = - \sup (-A)$ .","['elementary-set-theory', 'solution-verification']"
4449416,What's the maximal number of chess pieces under this rule?,"Suppose we have a $n\times n$ chessboard, where $n\geq 3$ is a positive integer. We place chess pieces on the board such that any three of them are not standing next to each other and on the same line (row or column). For example, when $n=3$ ( $\times$ denotes a chess piece), $$
\begin{matrix}
\times &\times &\\
&\times &\times\\
& &\times
\end{matrix}
$$ is valid, but $$
\begin{matrix}
\times&\times&\\
&\times&\\
&\times&\times
\end{matrix}
$$ is invalid, because the second column does not meet the requirement. What's the maximal number of chess pieces you can put onto the chessboard? MY PROGRESS I have found that if you put chess pieces on all positions on the diagonal, $1^\text{st}$ superdiagonal, $2^\text{nd}$ subdiagonal, $3^\text{rd}$ superdiagonal, $4^\text{th}$ subdiagonal, ... , then any extra chess piece makes it invalid. However, I cannot prove that this strategy of positioning leads to the maximal number of chess pieces.","['number-theory', 'combinatorics', 'optimization', 'chessboard']"
4449524,Geometry of a diagonal in a flag,"Problem I came across the flag of Trinidad and Tobago and it got me thinking about the geometry of that diagonal. Picture below. If you look at the diagonal, you'll see it doesn't just go from one corner to the opposite. It's placed in such a way that the lower border of the stripe touches one corner, and the upper border touches the opposite corner. I found a few other flags with diagonals placed in that way, like the flags of Brunei , Republic of Congo , or the Democratic Republic of Congo . I was thinking, how would you draw that diagonal if you only knew the flag's width and height, and the stripe width. I haven't found an easy way to solve this, and I was wondering if someone has thought about this before, or wanted to give it a try. I've also read this post before posting this question, thanks to a recommendation from ""similar questions"". That one is actually the same setup, but the questions asked are different, so I'm still unsure how to solve this problem. What I've done so far I'll start with some definitions. The dimensions I've defined can be found on the figure below. The bottom-left corner detail with more dimensions below: Flag's width: $w$ . Flag's height: $h$ . Flag's diagonal: $d$ . Note this is also the stripe's diagonal. Stripe's width, or ""thickness"": $t$ . Stripe's length: $l$ . Note this is the distance $\overline{AD}$ , or $\overline{BC}$ . The angle between the stripe and the bottom border of the flag: $\alpha$ . Vertical distance $\overline{AB}$ : $y$ . Horizontal distance $\overline{AB}$ : $x$ . I ultimately want to know how to define $x$ and $y$ in terms of $w$ , $h$ , and $t$ . i.e. $x(w, h, t)$ and $y(w, h, t)$ . One thought that I have in mind, but haven't been able to write mathematically is the relation between the thickness and the angle. It should be clear that: $$
\begin{align*}
t &= 0 \to \alpha = \arctan(h / w), &B \equiv A\\
t &= w \to \alpha = \pi/2, &B \equiv G
\end{align*}
$$ The relations I've found: $$
\begin{align}
t^2 + l^2 = d^2 \tag{1} \\
w^2 + h^2 = d^2 \tag{2} \\
x^2 + y^2 = t^2 \tag{3} \\
l \cos{\alpha} = w - x \tag{4} \\
l \sin{\alpha} = h + y \tag{5}
\end{align}
$$ Equations $(1)$ , $(2)$ and $(3)$ come from applying the Pythagoras theorem to the triangles $\triangle{ABC}$ , $\triangle{AGC}$ , and $\triangle{AJB}$ , respectively. Equations $(4)$ and $(5)$ come from defining the flag's width and height using the stripe dimensions. With them, I've done the following steps: $(4)^2 + (5)^2$ , to remove the $\sin$ and $\cos$ , arriving to: $$
\begin{align}
x^2 - 2wx + y^2 + 2hy + w^2 + h^2 - l^2 = 0 \tag{6} \\
\end{align}
$$ Rewrite $(1)$ as $l^2 = d^2 - t^2$ , and substitute the value of $d$ from $(2)$ , and the value of $t$ from $(3)$ , arriving to: $$
\begin{align}
l^2 = w^2 + h^2 - x^2 - y^2 \tag{7} \\
\end{align}
$$ Substitute $(7)$ in $(6)$ , arriving to: $$
\begin{align}
x^2 - wx + y^2 + hy = 0 \tag{8} \\
\end{align}
$$ So far this looks good to me, but I'm not sure how to get to $x(w, h, t)$ and $y(w, h, t)$ . I can do $x(y)$ as: $$
\begin{align}
x = \frac{w\pm\sqrt{w^2-4(y^2 + hy)}}{2} \tag{9} \\
\end{align}
$$ And from here I could substitute that value of $x$ in $(3)$ , but I haven't found a way to isolate $y$ and reach $y(w, h, t)$ . Questions Am I heading in the right direction? Have I missed some relation that would make this simpler? Thanks!","['triangles', 'trigonometry', 'puzzle', 'geometry']"
4449557,Help Proving that Set Invariant for a Dynamical System.,"I'm practicing for an upcoming test, and this one has been giving me some problems. Suppose we have $$
\begin{cases}
\dot x = x^2 + 2y - 4 \\
\dot y = -2xy
\end{cases}.
$$ Let $R = \{(x,y) : |x| \leq 2,\, 0 \leq y \leq 4 - x^2\}$ . I'm to prove this is invariant, i.e., if $x_0 \in R$ , then $$
\Phi(t, x_0) \in R, \hspace{1em} \forall t\in \mathbb{R}.
$$ So far, I have $y = 0$ stays on $\{(x,0) : |x| \leq 2\}$ , so the bottom line boundary is invariant. But, I'm having trouble with the boundary of the parabola. If it's invariant, then obviously we are done; but it may just be a trapping region. Any hints on where to go would be most appreciated.","['ordinary-differential-equations', 'dynamical-systems']"
4449660,Graph $G$ has $134$ edges and $67$ vertices. All vertices of $G$ except $p$ and $q$ have a degree of $4$. Find all values for $\deg(p)$ and $\deg(q)$,"Problem: Graph $G$ has $134$ edges and $67$ vertices. All vertices of $G$ except $p$ and $q$ have a degree of $4$ . Find all possible values for $\deg(p)$ and $\deg(q)$ considering $\deg(p)<\deg(q)$ and edge $pq$ is a bridge. Attempt: $67-2=65$ vertices have a degree of $4$ . The sum of all degrees must equal twice the number of edges, thus $$65\cdot 4 +1 \cdot \deg(p)+1 \cdot \deg(q)=2\cdot 134$$ from which $$\deg(p)+\deg(q)=8$$ Considering $\deg(p)<\deg(q)$ , we get that $1$ . $\deg(p)=3$ and $\deg(q)=5$ $2$ . $\deg(p)=2$ and $\deg(q)=6$ $3$ . $\deg(p)=1$ and $\deg(q)=7$ Is this correct?","['graph-theory', 'solution-verification', 'discrete-mathematics']"
4449674,Set of densities of spectral measures,"Let $(X,\mathbb{A},E)$ be a spectral measure on Hilbert space $H$ and $f,g \in H$ . We can define scalar measures $\mu_{f,g}(\delta)=(E(\delta)f,g)$ and $\mu_{g}(\delta)=(E(\delta)g,g)$ . Now fix $g$ . It is clear that $\mu_{f,g}$ is absolutely continuous with respect to $\mu_{g}$ . What can we say about the set of densities $D=\displaystyle\bigg\{\frac{d\mu_{f,g}}{d\mu_{g}}\bigg\}_{f\in H}$ . My hypothesis is that $D=L^1(X,\mu_{g})$ . Inclusion $D\subset L^1(X,\mu_{g})$ is obvious. But if $\phi \in L^1(X,\mu_{g})$ is given,  how can we construct $f \in H$ s.t. $(E(\delta)f,g)=\displaystyle\int\limits_{\delta}\phi d\mu_{g}$ for every $\delta \in \mathbb{A}$ ? All my attempts failed. Any help is appreciated. Thanks in advance.","['measure-theory', 'operator-theory', 'functional-analysis']"
