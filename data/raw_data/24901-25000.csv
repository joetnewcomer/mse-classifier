question_id,title,body,tags
222381,Verification that $S^{n}$ is a differentiable manifold.,"Setting $S^{n} := \{x\in\mathbb{R}^{n+1}: \|x\| = 1\}$, and
labelling the north and south poles as
$N:= (0,\ldots,0,1)$, $S:=(0,\ldots,0,-1)$, I can set the coordinate charts up as follows: Let $U_N = S^n - N$ and $U_S = S^n - S$. Taking the usual stereographical projections $\phi_{N}:U_{N}\to\mathbb{R}^{n}$ and $\phi_S : U_S \to\mathbb{R}^n$, we turn $S^n$ into a topological manifold of dimension $n$.\
\
I'm having trouble verifying that this structure satisfies the definition of a differentiable manifold, as I do not know how to check that $f:=\phi_{N}\circ \phi_{S}^{-1}:\mathbb{R}^{n} - 0$ is $C^{\infty}$.  It is clear how to show (once I draw the picture) that $f$ is a bijection. Do I need to derive a formula for the $\phi_{N}$ and $\phi_{S}^{-1}$ map in order to show that it is $C^{\infty}$?  Any references I found this in usually display it as an example and mention casually that it is easy to show that $f$ is $C^\infty$, but I'm not even sure how to begin.",['differential-geometry']
222390,What do you call a function differentiated with respect to all of its arguments?,"Just a simple question. Let $f(x_1, x_2, \ldots, x_n)$ be a smooth function. Is there a particular name for the function $$\frac{\partial^n f}{\partial x_1 \, \partial x_2 \cdots \partial x_n}$$","['multivariable-calculus', 'calculus', 'terminology']"
222391,"Generalizing a surjection ${\bf GL}(2,\Bbb R)\cong (\bf M,\circ)$","Consider the set of all invertible $2\times 2$ matrices over $\mathbb R$ (I think we can do it over $\Bbb C$ but I didn't look at it) $${\bf GL}(2,\Bbb R)=\left\{\left(\begin{matrix}  a&b \\c &d\end{matrix}\right):a,b,c,d\in\Bbb R,ac-bd\neq 0 \right\}$$ Consider now the set $\bf M$ of all functions of the form $$f(x)=\frac{ax+b}{cx+d}$$ again with $a,b,c,d\in\Bbb R,ad-bc\neq 0 $. If we identify each of these with a matrix 
$$\left(\begin{matrix}  a&b \\c &d\end{matrix}\right)$$ then we can define an isomoprhism between $\rm GL$ and $(\bf M,\circ)$ as groups with operations of matrix multiplications and functional composition, respectively, and identities$$\left(\begin{matrix}  1&0 \\0 &1\end{matrix}\right)=e$$
$$x=\frac{1x+0}{0x+1}=e'$$ 
since if $$\eqalign{
  & f = \frac{{ax + b}}{{cx + d}}  \cr 
  & g = \frac{{ex + f}}{{gx + h}} \cr} $$ then $$f \circ g = \frac{{\left( {ae + bg} \right)x + af + bh}}{{\left( {ce + dg} \right)x + dh + cf}}$$
which corresponds to $$\left(\begin{matrix}  a&b \\b &c\end{matrix}\right)\left(\begin{matrix}  e&f \\g &h\end{matrix}\right)=\left(\begin{matrix}  ae + bg&af + bh \\ce + dg &dh + cf\end{matrix}\right)$$
and similarily for inversion, $${f^{ - 1}} = \frac{1}{{ac - bd}}\frac{{dx - b}}{{ - cx + a}}$$ and $$\left(\begin{matrix}  \frac  d\Delta& \frac {-b}\Delta \\\frac{-c}\Delta &\frac a\Delta \end{matrix}\right)=e$$ I include the determinant $\Delta=ac-bd$ inside the matrix to avoid any multiplication by scalars considerations. My question is : How can this be generalized to ${\bf GL}(n,K)$, and what is the theoretical relevance of this? I know Möbius Transformations are important in Complex Analysis, for instance, but I haven't seen any ""higher dimesional"" equivalent around. ADD As users noted, for every $a\in \Bbb R$,$$\left(\begin{matrix}a&0\\0&a\end{matrix}\right)\mapsto x$$ so the isomoprhism is actually obtained by quoting ${\bf GL}(2,\Bbb R)$ by $I=\{aI_2:a\in\Bbb R\}$","['linear-algebra', 'complex-analysis']"
222402,Is the product of two positive semidefinite matrices positive semidefinite?,"If $X$ and $W$ are real, square, symmetric, positive semidefinite matrices of the same dimension, does $XW + WX$ have to be positive semidefinite? This is not homework.","['matrices', 'linear-algebra']"
222403,Prove that $\bigcap\limits_{i = 1}^n {\left( {{A_i} - B} \right)} = \bigcap\limits_{i = 1}^n {{A_i}} - B$,"Prove that if $A_1, A_2, \ldots , A_n$ and $B$ are sets, then 
$$(A_1 − B) \cap (A_2 − B) \cap \cdots \cap (A_n − B) = (A_1 \cap A_2 \cap \cdots \cap A_n) − B.$$","['induction', 'discrete-mathematics', 'elementary-set-theory']"
222404,Why $\cos^2 (2x) = \frac{1}{2}(1+\cos (4x))$?,"Why: $$\cos ^2(2x) = \frac{1}{2}(1+\cos (4x))$$ I don't understand this, how I must to multiply two trigonometric functions? Thanks a lot.","['trigonometry', 'algebra-precalculus']"
222406,Relationship between Legendre polynomials and Legendre functions of the second kind,"I'm taking an ODE course at the moment, and my instructor gave us the following problem: Derive the following formula for Legendre functions $Q_n(x)$ of the second kind: $$Q_n(x) = P_n(x) \int \frac{1}{[P_n(x)]^2 (1-x^2)}dx$$ where $P_n(x)$ is the $n$ -th Legendre polynomial . He introduced Legendre functions in the context of second order ODEs,  but we haven't really used them for anything - moreover, this is the only problem we were assigned that has anything to do with them. As a result, I'm sort of at a loss of where to start. I've tried a couple of things (like using the actual Legendre ODE $$(1-x^2)y^{\prime \prime} - 2xy^{\prime} + n(n+1)y = 0$$ and plugging in the solution $y(x)=a_1P_n(x)+a_2Q_n(x)$ and proceeding from there) but so far, haven't been able to go anywhere. Any help (preferably as elementary as possible) would be much appreciated. Thanks!","['ordinary-differential-equations', 'special-functions']"
222408,Unclear step on proof of Laplace transform of a derivative,"Reading the article on the Laplace Transform in Wolfram MathWorld , I found the proof that $\mathcal{L}[f'(t)] = sF(s) - f(0)$. I understand the first and second steps, but I don't understand the third one. Why is it that $lim_{a \to \infty} [e^{-sa} f(a)] = 0$? $e^{-sa}$ does get closer to 0 when $-sa$ approaches to $\infty$, but why does $f(a)$ get closer to 0? As far as I know, $f(a)$ could be anything, so it could be possible that $lim_{x \to \infty} f(a)$ doesn't exist. What guarantees that the limit of $f(a)$ always exists? I hope I'm not missing some simple property of limits here.","['laplace-transform', 'proof-writing', 'limits']"
222431,"How to prove$|a-b|^p\leq \max(1,2^{p-1})(|a|^p+|b|^P)$?","I am stuck with this question:
How to prove$|a-b|^p\leq \max(1,2^{p-1})(|a|^p+|b|^p)$ I forgot to say a ,b are both complex number","['complex-numbers', 'complex-analysis']"
222458,Question about bounded function $f(x)= \frac{1-\cos x}{x^{2}}$,"I want to show that the function $\displaystyle f(x)= \frac{1-\cos x}{x^{2}}$ is bounded in $(-\infty,\infty)$.
I know that $\displaystyle h(x)=1-\cos x$ is bounded on $R$ but $\displaystyle g(x)=\frac{1}{x^{2}}$ is not bounded in nbhd. of $0$. So what about $h(x).g(x)$ on $R$? Is it bounded? Why?","['functions', 'real-analysis']"
222485,"Compute $\lim_{n\to\infty}\int_0^n \left(1+\frac{x}{2n}\right)^ne^{-x}\,dx$.","I'm trying to teach myself some analysis (I'm currently studying algebra), and I'm a bit stuck on this question. It's strange because of the $n$ appearing as a limit of integration; I want to apply something like LDCT (I guess), but it doesn't seem that can be done directly. I have noticed that the change of variables $u=1+\frac{x}{2n}$ helps. With this, the problem becomes
$$
\lim_{n\to\infty}\int_1^{3/2}2nu^ne^{-2n(u-1)}\,du.
$$ This at least solves the issue of the integration limits. Let's let $f_n(u):=2nu^ne^{-2n(u-1)}$ for brevity. I believe it can be shown that
$$
\lim_{n\to\infty}f_n(u)=\cases{\infty,\,u=1\\0,\,1<u\leq 3/2}
$$
using L'Hopital's rule and the fact that $u^n$ intersects $e^{2n(u-1)}$ where $u=1$, and so the exponential function is larger than $u^n$ for $n>1$. I think I was also able to show that $\{f_n\}$ is eventually decreasing on $(1,3/2]$, and so Dini's Theorem says that the sequence is uniformly convergent to $0$ on $[u_0,3/2]$ for any $u_0\in (1,3/2]$. Since each $f_n$ is continuous on the closed and bounded interval $[u_0,3/2]$, each is bounded; as the convergence is uniform, the sequence is uniformly bounded. Thus, the Lebesgue Dominated Convergence Theorem says
$$
\lim_{n\to\infty}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du=\int_{u_0}^{3/2}0\,du=0.
$$
So it looks like I'm almost there, I just need to extend the lower limit all the way to $1$. I think this amounts to asking whether we can switch the order of the limits in
$$\lim_{n\to\infty}\lim_{u_0\to 1^+}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du,
$$
and (finally!) this is where I'm stuck. I feel like this step should be easy, and it's quite possible I'm missing something obvious. That happens a lot when I try to do analysis because of my practically nonexistent background.","['measure-theory', 'integration', 'real-analysis', 'limits']"
222505,Is there an intuitive way to see this property of random walks?,"For an $n$-step symmetric simple random walk (start at origin 0 and each step 1 unit towards left or right with equal probability,) an interesting fact is that the probability that you stop exactly at $r$ is equal to the probability that in the whole walk you've never reached $r+1$ but you've been to $r$. Is there a intuitive way to see this? Here, $n$ and $r$ are positive even numbers.","['random-walk', 'probability']"
222509,Proving $|f(z)|$ is constant on the boundary of a domain implies $f$ is a constant function,"Let $D \subset \mathbb{C}$ be a bounded domain and $f$ a function holomorphic in $D$ and continuous in its closure. Suppose that $|f(z)|$ is constant on the boundary of $D$ and that $f$ does not have zeroes in $D$. Prove that $f$ is a constant function. I think that if I can prove that $f$ attains both its maximum and minimum values on the boundary, then the result follows from the maximum principle. But I've been unable to show this. Is this the right way to approach this problem? If so, how do I show this result? Thanks in advance!",['complex-analysis']
222511,Preimage of a singleton set.,"My question is aroused by this article; ""By definition of a function, the image of an element x of the domain is always a single element y of the codomain. Conversely, though, the preimage of a singleton set (a set with exactly one element) may in general contain any number of elements. For example, if f(x) = 7 (the constant function taking value 7), then the preimage of {5} is the empty set but the preimage of {7} is the entire domain."" Here,I can understand that the preimage of the singleton set is the entire domain.But if so,then how does the inverse of a singleton function be a ""function"" according to the function definition as there exists more than one element map singleton set to preimage.",['functions']
222532,Finding $a$ such that $x^a \cdot \sin(1/x)$ is uniformly continuous.,"Assuming that $\sin x$ is continuous on $\mathbb R$, find all real $\alpha$ such that $x^\alpha\sin (1/x)$ is uniformly continuous on the open interval (0,1). I'm guessing that I need to show that $x^\alpha\sin x$ is continuously extendable to [0,1]. Doing that for $x=1$ is pretty trivial, but I am having trouble doing that for $x=0$. I believe that the $\lim_{x\to 0}x^\alpha\sin (1/x)=0$, but how can I find what $f(0)$ equals? I would appreciate any guidance! Thanks for your help in advance.","['continuity', 'analysis']"
222560,How to show any convergent sequence is strongly discrete in Hausdorff space?,"Given a space $X$ and $C \subset X$, we say that $C$ is strongly discrete if there exists a disjoint family $\{U_x: x\in C\} $ of open sets in $X$ such that $x\in U_x$ for all $x\in C$. The question is this: How to show any convergent sequence is strongly discrete in Hausdorff space? Thanks ahead.","['general-topology', 'proof-writing']"
222583,Two matrices of complementary rank that sum to the identity have zero product.,"Suppose $A$, $B$ are real $n \times n$ matrices with $A + B = I$ and $\operatorname{rank} (A) + \operatorname{rank} (B) = n$. How can one show that $AB = BA = 0$?","['matrices', 'linear-algebra']"
222602,Is this constant of integration necessary at this step?,"I came across a differential equation: $$\frac{dy}{dx}=\frac{\sin(\log x)}{\log y}$$.
Here is what I tried to do: I transformed it into this form $$\log y dy=\sin(\log x)dx$$ i.e. $$\int \log y dy=\int \sin(\log x)dx\dots(2)$$ and after that I used integration by parts to finish off the problem. However,I was told by my teacher that it should instead be $$\int \log y dy=\int \sin(\log x)dx+C$$ where $C$ is a constant of a integration.I argued that the integration had not yet been carried out and so there was no need for the constant.(and I was told it $had$ to be there.) Can anyone please convince me why my teacher is right and I wrong? Thanks.","['ordinary-differential-equations', 'integration']"
222641,How to show that this function is bijective [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: Proving the Cantor Pairing Function Bijective Assume I define $$ f: \mathbb N \times \mathbb N \to \mathbb N, (a,b) \mapsto a + \frac{(a + b ) ( a + b + 1)}{2} $$ How to show that this function is bijective? For injectivity I tried to show that if $f(a,b) = f(n,m) $ then $(a,b) = (n,m)$ but I end up getting something like $3(n-a) + (n+m)^2 -(a+b)^2 + m - b = 0$ and don't see how to proceed from there. There has to be something cleverer than treating all possible cases of $a \leq n, b \leq m$ etc. For surjectivity I'm just stuck. If I do $f(0,n)$ and $f(n,0)$ it doesn't seem to lead anywhere. Thanks for your help.","['elementary-set-theory', 'functions']"
222642,Solving the differential equation $\frac{dy}{dx}=\frac{3x+4y+7}{x-2y-11}$,How do we solve the differential equation $$\frac{dy}{dx}=\frac{3x+4y+7}{x-2y-11}$$? I tried substituting $v=yx$ but I do not seem to be getting anywhere.Putting $u=x-2y$ yielded nothing better. Thanks!,['ordinary-differential-equations']
222667,Prove that sphere is the only surface which can be generated by rotation in more than one way,"In Hilbert's book Geometry and the imagination , he said that sphere is the only surface which can be generated by rotation in more
  than one way. It is quite intuitive, but I can't give a rigorous proof. How to prove it? PS: Here rotation means rotating a closed curve with respect to the axis of symmetry of it that is in the same plane.",['geometry']
222672,Line integral over ellipse in first quadrant,"Evaluate $ \int_{C} xy\,ds $ where C is the arc of the ellipse $ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 $ in the first quadrant. Let $x = a\cos t$ and $ y= b\sin t$ and use a substitution of $ u = a^2 \sin^2t + b^2\cos^2t $ to simply the expression under the sqrt root when dealing with $ ds$.
I eventually get to $$\frac{ab}{2(a^2-b^2)} [\frac{2}{3} \sqrt{(a^2\sin^2t + b^2 \cos^2t)^3}]$$evaluated between $\pi/2$ and $0$. Should I take $\sqrt{a^6} = a^3$ here? (my thoughts being that $ a>0 $  in first quadrant)",['multivariable-calculus']
222674,Average bus waiting time,"My friends and I were ""thinking"" yesterday in the pub about the following: if a person is standing on a bus stop that is served by a single bus which comes every p minutes, we would expect the average waiting time to be p/2 (which may or may not be correct). But we had no idea how to calculate the average waiting time if there is more than one bus. So let's assume there is n many buses serving the stop, and each comes once in m1, m2 ... mn minutes. How would we go about calculating the average time a person has to wait for a bus? What is the theory behind it? Thank you","['average', 'probability']"
222675,Riesz basis in Hilbert space,"We know that a collection of vectors $\{x_{k}\}$ in a Hilbert space called Riesz basis if it is an image of orthonormal for H under invertible linear transformation. How to prove that there is constants $A,B$ such that for all $x\in H$
$$
A||x||^2\leq\sum_{k}\langle x,x_k \rangle^2\leq B||x||^2?
$$","['hilbert-spaces', 'functional-analysis']"
222689,Eigenvalue decomposition of block covariance matrix for Canonical Correlation Analysis (CCA),"Edited: My question is related to a tutorial I was reading. The covariance matrix is a block matrix where $C_{xx}$ and $C_{yy}$ are within-set covariance matrices and $C_{xy} = C_{yx}^T$ are between-sets covariance matrices. $$
  \left[\begin{array}{r r}
    C_{xx} & C_{xy}\\
    C_{yx} & C_{yy}
  \end{array}\right]
$$ The tutorial says that the canonical correlations between $x$ and $y$ can be found by solving the eigenvalue equations $$
  C_{xx}^{-1}C_{xy}C_{yy}^{-1}C_{yx} \hat w_x = \rho^2 \hat w_x \\
  C_{yy}^{-1}C_{yx}C_{xx}^{-1}C_{xy} \hat w_y = \rho^2 \hat w_y 
$$ where the eigenvalues  are the squared canonical correlations and the eigenvectors  and  are the normalized canonical correlation basis vectors. What I do not understand is how the eigenvalue equations are found by using the covariance matrix? Can someone please explain how we get those sets of equations? Thanks.","['eigenvalues-eigenvectors', 'statistics', 'linear-algebra', 'block-matrices', 'correlation']"
222690,Number of integral solutions for $|x | + | y | + | z | = 10$,"How can I  find the number of integral solution to the
equation $|x | + | y | + | z | = 10.$ I am using the formula, Number of integral solutions for $|x| +|y| +|z| = p$ is $(4P^2) +2 $, So the answer is 402. But, I want to know, How we can find it without using formula. any suggestion !!! Please help","['diophantine-equations', 'number-theory', 'combinatorics']"
222702,Sufficient conditions on target space for the existence of regular conditional probability,"Suppose $(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space, $(S, \mathcal{B}(S))$ and $(T, \mathcal{B}(T))$ are topological spaces with their Borel $\sigma$-algebras, and $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables. I know there are conditions I can put on $(\Omega, \mathcal{F}, \mathbb{P})$ to guarantee I can find regular conditional probabilities for an arbitrary random variable and measurable map. I'm wondering whether there are topological conditions I can put on $S$ and $T$ which guarantee that there exists a regular conditional probability for $Y$ given $X$. By regular conditional probability, I mean a map $\nu: S\times \mathcal{B}(T) \to [0,1]$ such that: (1) For each $s \in S$, $\nu(s, \cdot)$ is a probability measure on $(T, \mathcal{B}(T))$, (2) For each $B\in \mathcal{B}(T)$, $\nu(\cdot, B)$ is measurable, (3) For each $A\in \mathcal{B}(S), B\in \mathcal{B}(T)$, $\mathbb{P}\{X\in A, Y\in B\} = \int_A \nu(\cdot,B)d\mathbb{P}_X$. Where $\mathbb{P}_X$ is the pushforward probability measure of $X$.","['probability-theory', 'measure-theory']"
222709,Inverting the Cantor pairing function,"Towards the end of this , how do I get $$ \frac{\sqrt{8z + 1} - 1}{2} < w + 1 $$ Here $w = x + y \geq 0, t = (w^2 + w)/2 , z = t + w, x,y \in \mathbb N_{\geq 0}$. Thanks.","['inequality', 'algebra-precalculus']"
222716,Multiplicative norm on $\mathbb{R}[X]$.,"How to prove that : there is no function $N\colon \mathbb{R}[X] \rightarrow \mathbb{R}$, such that : $N$ is a norm of $\mathbb{R}$-vector space and $N(PQ)=N(P)N(Q)$ for all $P,Q \in \mathbb{R}[X]$. Once, my teacher asked if there is a multipicative norm on $\mathbb{R}[X]$, and one of my classmate proved that there was none. But I can't remember the proof (all I remember is that he was using integration somewhere...).","['normed-spaces', 'linear-algebra', 'inner-products', 'polynomials']"
222720,Gradient of a Vector Valued function,"I read somewhere, gradient vector is defined only for scalar valued functions and not for vector valued functions. When gradient of a vector is definitely defined(correct, right?), why is gradient vector of a vector valued function not defined? 
Is my understanding incorrect? Is there not a contradiction? I would appreciate clear clarification. Thank You.","['multivariable-calculus', 'derivatives']"
222755,Is the injection $\ell^p \subset \ell^q$ continuous for $p<q$?,"It is easy to show that $\ell^p \subset \ell^q$ when $1 \leq p<q \leq + \infty$, but is the injection continuous? If so is $\ell^{\infty}$ the direct limit $\lim\limits_{\rightarrow} \ \ell^p$ as topological space? NB: If some result depends on the set we are working on, I am working on $\mathbb{R}$ or $\mathbb{C}$.","['general-topology', 'functional-analysis', 'lp-spaces']"
222767,Convergence of a sequence,"I have to prove the following statement: Let $(X_n)_{n\in\mathbb{N}}$ be a sequence of real numbers, such that $\exp(itX_n)$ converges for every $t\in\mathbb{R}$. Show that the sequence $(X_n)$ converges. The problem I see is, that the complex logarithm is not continous. Hence, I have to work around, but I dont know how. I hope someone can help.","['convergence-divergence', 'sequences-and-series']"
222768,Subfields of the field of complex numbers with finite index rather than the real number field [duplicate],"This question already has an answer here : Closed 11 years ago . Possible Duplicate: Finiteness of the Algebraic Closure For short, I wonder if there are other fields $F\subset \mathbb{C}$ rather than $\mathbb{R}$, with finite index $[\mathbb{C}:F]$. Since $\mathbb{C}=\mathbb{R}+\mathbb{R}\sqrt{-1}$, we naively hope that if we are given a $p$th primitive unit root then we ""should"" find a subfield $F\subset \mathbb{C}$, such that $F(\omega)=\mathbb{C}$. Applying Zorn's lemma, we do be able to find a maximal subfield $F$ with respect to $\omega\notin F$. And for any finite algebraic extension $E/F$ with $E\subset \mathbb{C}$, we can show that $E/F$ is Golois with cyclic Galois group. Because if $L/F$ is finite Galois then pick an $\sigma\notin Gal(L/F(\omega))$, we obtain $Gal(L/F)=(\sigma)$. However, I do not know how to go forward to get a finite index subfield yet. Just for curiousness. Thanks.","['galois-theory', 'abstract-algebra']"
222792,How do I find $\frac{\text{d}}{\text{d}z}\left(z\bar{z}\right)$?,"I am seeking $\frac{\text{d}}{\text{d}z}\left(z\bar{z}\right)$ where $f(z)=z\bar{z}.$ And I know that I need to use the following definition of the derivative:
$$f'(z)=\lim_{\Delta z\to 0}{\frac{f(z_0+\Delta z)-f(z_0)}{\Delta z}}.$$
However, I'm not sure if I'm using the definition correctly when I plug in $f(z)$:
\begin{align*}
f'(z)&=\lim_{\Delta z\to 0}{\frac{(z+\Delta z)(\overline{z+\Delta z})-z\bar{z}}{\Delta z}}\\&=\lim_{\Delta z\to 0}{\frac{\overline{\Delta z}(z+\Delta z)+\bar{z}\Delta z}{\Delta z}}\\&=\lim_{\Delta z\to 0}{\frac{\overline{\Delta z}(z+\Delta z)}{\Delta z}}+\lim_{\Delta z\to 0}{\frac{\bar{z}\Delta z}{\Delta z}}\\&=\lim_{\Delta z\to 0}{\frac{\overline{\Delta z}(z+\Delta z)}{\Delta z}}+\bar{z}
\end{align*}
Assuming that I've maneuvered the limit above properly, I'm not sure how to continue from the final line...","['derivatives', 'complex-analysis', 'limits']"
222810,Borel $\sigma$ algebra on a topological subspace. [duplicate],"This question already has answers here : Preimage of generated $\sigma$-algebra (3 answers) Question about Borel sets (3 answers) Closed 11 years ago . Let $T$ be a topological space, with Borel $\sigma$-algebra $B(T)$ (generated by the open sets of $T$). If $S\in B(T)$, then the set $C:=\{A\subset S:A\in B(T)\}$ is a $\sigma$-algebra of $S$. My question is, if I also generated the Borel $\sigma$-algebra $B(S)$ treating $S$ as a topological subspace, with the inherited topology from $T$, is it true that $B(S)=C$?",['measure-theory']
222813,existence of hyperbolic groups,"I perfectly understand that the Milnor-Schwarz lemma tells me that cocompact lattice in semisimple Lie groups of higher rank are not hyperbolic (in the sense of Gromov). But do there exist noncocompact lattices in higher rank semisimple Lie groups which are hyperbolic? (wikipedia is saying ""no"" without any reference, but I do not trust that article since it contains mistakes...)","['reference-request', 'group-theory']"
222817,O(n) as embedded submanifold,"I want to show that the set of orthogonal matrices, $O(n) = \{A \in M_{n \times n} | A^tA=Id\}$, is an embedded submanifold of the set of all $n \times n$ matrices $M_{n \times n}$. So far, I have used that this set can be described as $O(n) = f^{-1}(Id)$, where $f: M_{n \times n} \rightarrow Sym_n = \{A \in M_{n \times n} | A^t = A\}$ is given by $f(A) = AA^t$, and that the map $f$ is smooth. Hence I still need to show that $Id$ is a regular point of this map, i.e. that the differential map $f_*$ (or $df$ if you wish) has maximal rank in all points of $O(n)$. How do I find this map? I tried taking a path $\gamma = A + tX$ in $O(n)$ and finding the speed of $f \circ \gamma$ at $t=0$, which appears to be $XA^t + AX^t$, but don't see how to proceed. Another way I thought of was by expressing everything as vectors in $\mathbb{R}^{n^2}$ and $\mathbb{R}^{\frac{n(n+1)}{2}}$, but the expressions got too complicated and I lost track.","['manifolds', 'differential-geometry']"
222833,Smith normal form of a polynomial matrix,"I have the following matrix $$P(s) := \begin{bmatrix}
s^2 & s-1 \\
s   & s^2
\end{bmatrix}$$ How does one compute the Smith normal form of this matrix? I can't quite grasp the algorithm.","['control-theory', 'matrices', 'smith-normal-form', 'polynomials']"
222853,Group of order $pq$ is not simple,Is the following correct way of showing that there is no simple group of order $pq$ where $p$ and $q$ are distinct primes? If $|G|=n=pq$ then the only two Sylow subgroups are of order $p$ and $q$. From Sylow's third theorem we know that $n_p | q$ which means that $n_p=1$ or $n_p=q$. If $n_p=1$ then we are done (by a corollary of Sylow's theorem) If $n_p=q$ then we have accounted for $q(p-1)=pq-q$ elements of $G$ and so there is only one group of order $q$ and again we are done. Is that correct?,"['group-theory', 'abstract-algebra']"
222869,Tensor products commute with inductive limit,"How to prove, that tensor products commute with direct limits, if the main ring is not the same? For every $i$ we have modules $L_i$ and $M_i$ over a ring $A_i$, and for every $i \geq j$ homomorphisms $f^i_j: L_i \rightarrow L_j$, $g^i_j: M_i \rightarrow M_j$, $u^i_j: A_i \rightarrow A_j$, such that $f^i_j (al) = u^i_j(a)f^i_j(l)$, $g^i_j(am) = u^i_j (a) g^i_j (m)$ for every $a \in A_i,\,  l\in L_i, \, m \in M_i$. To prove, that $\varinjlim (L_i \otimes_{A_i} M_i) = (\varinjlim L_i)\otimes_{\varinjlim A_i} (\varinjlim M_i)$.","['category-theory', 'commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
222894,How to take the gradient of the quadratic form?,"It's stated that the gradient of: $$\frac{1}{2}x^TAx - b^Tx +c$$ is $$\frac{1}{2}A^Tx + \frac{1}{2}Ax - b$$ How do you grind out this equation? Or specifically, how do you get from $x^TAx$ to $A^Tx + Ax$?","['multivariable-calculus', 'quadratic-forms', 'matrix-calculus', 'derivatives', 'scalar-fields']"
222901,Where is $f(z)=\Re (z)$ differentiable?,"Let $f:\mathbb{C}\to\mathbb{C}$  be given by $z\mapsto\Re\left(z\right)$ . Where is $f$  differentiable? $f$  is definitely differentiable on $\mathbb{R}$  because on these values $f\left(z\right)=z$ . As for $\mathbb{C}\backslash\mathbb{R}$ , I am not sure. I am guessing it's not, but I can't prove it. The way I'm trying to show it is by constructing a sequence such that $z_{n}\to z$  but $\frac{\Re\left(z_{n}\right)-\Re\left(z\right)}{z_{n}-z}$  does not converge. Any help in showing this would be appreciated.",['complex-analysis']
222907,Primes in Gaussian Integers,"Let $p$ be a rational prime. It is is well known that if $p\equiv 3\;\;mod\;4$, then $p$ is inert in the ring of gaussian integers $G$, that is, $p$ is a gaussian prime. If $p\equiv 1\;mod\;4$ then $p$ is  decomposed  in $G$, that is, $p=\pi_1\pi_2$ where $\pi_1$ and $pi_2$ are gaussian primes not  associated. The rational prime $2$ ramifies in $G$, that is $2=u\pi^2$, where $u$ is a unit in $G$ and $\pi$ a prime in $G$. where can I find a proof of this fact?  I want a direct proof, not a proof for the quadratic integers and then deduce this as a particular case.",['number-theory']
222915,What taking gcd of two even numbers gives,What does taking the gcd of two even numbers $y$ and $z$ give? Does it give another indefinite even number $x$?,"['elementary-number-theory', 'discrete-mathematics']"
222924,Triangle equality implies vector dependence.,"I am trying to prove this statement: Show that if $x$ and $y$ are two vectors in an inner product space such that $||x+y||=||x||+||y||$, then $x$ and $y$ are linearly dependent. Squaring the equality I get $$\langle x+y,x+y\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$
then, using linearity of the inner product I get $$ \langle x,x\rangle +\langle y,y\rangle+\langle x,y\rangle+\langle y,x\rangle=\langle x,x\rangle +2||x||\cdot||y||+\langle y,y\rangle $$ After all the cancellation I finally arrive at $$ \mathrm{Re}\langle x,y\rangle=||x||\cdot||y|| $$ This looks like Cauchy-Schwarz inequality, so the only thing left to show is that $\mathrm{Re}\langle x,y\rangle=|\langle x,y\rangle|$, how can I do that?",['linear-algebra']
222927,Absolute value and limit reasoning.,"I am trying to develop my reasoning ability with absolute value. So, I wanted to know if the following reasoning is correct: Find $\lim_{x \to -6}\dfrac{2x+12}{|x+6|}$ By definition of absolute value we have $|x| = x$ when $x > 0$ and $|x| = -x$ when $x<0$ So for the above limit we can consider the limit from the left and the limit from the right: $(x+6)<0$ and $(x+6)>0$: Case $(x+6)<0$: $\dfrac{2x+12}{-(x+6)} = \dfrac{2(x+6)}{-(x+6)} = -2$ Case $(x+6) > 0$: $\dfrac{2x+12}{(x+6)} = \dfrac{2(x+6)}{(x+6)} = 2$ Hence, the limits from the left and right are not and equal and we conclude that the limit does not exist. If we were not considering the limit at $-6$ we could just evaluate the function at any point since the function is continuous everywhere else I graphed this function and I see that I have a vertical asymptote at $x=-6$. What do these left and right limits evaluating to $-2$ and $2$ mean then?","['calculus', 'real-analysis', 'limits']"
222954,Is a function that is one-to-one necessarily onto?,"I'd would like to know that, because I don't want to prove a function is onto if I don't have to. If the answer is no, is there any particular case where it is true?",['discrete-mathematics']
222964,Transition functions on a quotient manifold,"Here's an exercise given during a course in Differential Geometry that I'm taking. Let $M$ denote a smooth manifold and let $G$ be a finite group of diffeomorphisms acting on it without fixed points (that is, $g(p)=p$ for some $p\in M$ forces $g$ to be the identity). We then have on the quotient space $M/G$ a differentiable structure. An atlas for it is obtained via the following observation: if $[p]$ is a point in $M/G$ then there exist a representative $p\in M$ and a chart $(U, \phi)$ in $p$ such that the projection $\pi\colon M\to M/G$ is injective on $U$. It then makes sense to define a chart
$$(\overline{U}, \phi_{\overline{U}})=\left(\pi(U), \phi\circ\left(\pi|_{U}\right)^{-1}\right).$$
The family of all such charts is an atlas for $M/G$. Exercise Show that the transition functions $\phi_{\overline{V}}\circ\phi_{\overline{U}}^{-1}$ can be identified
  with elements of $G$. I find this question to be somewhat vague. Identified in which sense? The set of those transition functions might well be infinite, while $G$ is not. Also, I cannot see any relationship between the two. Can somebody provide me with some hint? Thank you.",['differential-geometry']
222970,How is Cantor's diagonal argument related to Russell's paradox in naive set theory?,"I was wondering whether anyone can shed proper light on this issue. I read both and it seems like they are somewhat similar, yet I can't quite see it.",['elementary-set-theory']
222975,A question on a linear system on a complex surface.,"For a complex surface $X$ with a line bundle $L$, the base locus Bs$|L|$ consists of $0$-dimensional and $1$-dimensional components. The fixed part of $|L|$ is the $1$-dimensional locus and of Bs$|L|$ and we denote it by $F$. Why is $h^0(X,O(F))=1$? How should I think of the map $L(-F)\hookrightarrow L$, which induces an isomorphism $H^0(X,L(-F))\cong H^0(X,L)$? As to the second question, I have trouble about how to understand the line bundle $L(-F)=L\otimes O(-F)$. If I understand global sections of $L(-F)$ are global sections of $L$ which are zeros on $F$, the last isomorphism $H^0(X,L(-F))\cong H^0(X,L)$ is reasonable; since any global section of $L$ has zeros on $F$, they are global section of $L(-F)$.","['algebraic-geometry', 'complex-geometry']"
222979,Divisor of a modular form,"I am trying to compute the divisor of $\Delta(z)/\Delta(pz)$ on the modular curve $X_0(p)$ where $p$ is a prime. I know that as a function on the full modular group, the $\Delta$ function has only a simple zero at infinity, but I can't get much further than this. I know that $X_0(p)$ has two cusps, $0$ and $\infty$, so I'm thinking I can compute this by considering the map from $X_0(p)$ to to the half plane modulo the full modular group, but I can't quite figure this out either. If I'm to believe Gross in his paper on Heegner points, I expect the answer to be
$$(p-1)\{(0)-(\infty)\}$$ Thanks for any insight.","['modular-forms', 'complex-analysis']"
222989,Limit of square root without L'Hopital's rule.,How might one go about taking the following limit without using L'Hopital's rule? I am stumped: $$\lim_{x \to \infty} \sqrt{x^2 + x} - x$$,"['limits', 'analysis']"
223011,What is the probability on rolling $2n$ dice that the sum of the first $n$ equals the sum of the last $n$?,"The Question What is the probability, rolling $n$ six-sided dice twice, that their sum each time totals to the same amount? For example, if $n = 4$, and we roll $1,3,4,6$ and $2,2,5,5$, adding them gives $$
1+3+4+6 = 14 = 2+2+5+5
$$ What is the probability this happens as a function of $n$? Early Investigation This problem is not too hard for $n = 1$ or $n = 2$ via brute force... For $n = 2$: Tie at a total of $2$:
$$
\frac{1}{36} * \frac{1}{36} = \frac{1}{1296}
$$ Tie at a total of $3$:
$$
\frac{2}{36} * \frac{2}{36} = \frac{4}{1296}
$$ etc. so the answer is 
$$
\frac{1^2 + 2^2 + 3^2 + ... + 6^6 + 5^2 + ... + 1^2}{1296} = \frac{\frac{(6)(7)(13)}{6} + \frac{(5)(6)(11)}{6}}{1296} 
= \frac{146}{1296}
$$ Note that I use the formula: $\sum_{k=1}^{n}k^2=\frac{(n)(n+1)(2n+1)}{6}$. Is there a way to do this in general for $n$ dice? Or at least a process
for coming up with a reasonably fast brute force formula? The Difficulty The problem arises that the sum of squares is not so simple when we
get to three dice. Using a spreadsheet, I figured out we need to sum these squares for 3
dice: $$
1, 3, 6, 10, 15, 21, 25, 27, 27, 25, 21, 15, 10, 6, 3, 1
$$ For a brute force answer of $\frac{4332}{46656}$. Note how we can no longer use
the sum of squares formula, as the squares we need to sum are no
longer linear. Some Thoughts I am no closer to figuring out an answer for $n$ dice, and obviously the
question becomes increasingly more difficult for more dice. One thing I noticed: I see a resemblance to Pascal's Triangle here,
except we start with the first row being six $1$, not one $1$. Se we have: 1 1 1 1 1 1
          1 2 3 4 5 6 5 4 3 2 1
 1 3 6 10 15 21 25 27 27 25 21 15 10 6 3 1
1 4 9 16 25 36 46 52 54 52 46 36 25 16 9 4 1
... but that's still a process, not a formula. And still not practical for
$n = 200$. I know how to prove the formula for any cell in Pascal's Triangle to
be $C(n,r) = \frac{n!}{r!(n-r)!}$... using induction; that doesn't really
give me any hints to deterministically figuring out a similar formula
for my modified triangle. Also there is no immediately obvious sum for
a row of this triangle like there is (powers of 2) in Pascal's Triangle. Any insight would be appreciated. Thanks in advance!","['dice', 'summation', 'probability']"
223026,Proving $(A \triangle B)\cup C = (A\cup C)\triangle (B\setminus C)$ using set algebra,"I tried to prove this equation $(A\bigtriangleup B)\cup C=(A\cup C)\bigtriangleup(B\setminus C)$ by elementhood and set algebra but with no result. I can see that equality stands in Venn's diagrams, and I also proved it with truth tables, but I would like to have solution with set algebra or elementhood. I would appreciate any pointers in solving this. This is from Velleman's How to Prove It, chapter 1 section 4 exercise 13. Solution with set algebra After some time pounding this exercise, I came up with following solution: $$(A\bigtriangleup B)\cup C=(A\cup C)\bigtriangleup(B\setminus C)\\
=((A\cup C)\cap (B\cap C^C)^C)\cup((B\cap C^C)\cap(A\cup C)^C)\\
=((A\cup C)\cap (B^C\cup C))\cup(B\cap C^C\cap A^C)\\
=(C\cup(A\cap B^C))\cup(B\cap C^C\cap A^C)\\
=C\cup(A\cap B^C)\cup(B\cap A^C)\\
=(A\bigtriangleup B)\cup C$$",['elementary-set-theory']
223032,Smooth Monotone $\mathbb{R}^3$ curve with constant (nontrivial) curvature,"So I was trying to construct a closed curve in $\mathbb{R}^3$ with constant positive curvature and non-trivial torsion.  To do this I tried to glue two helices together in a smooth way with a curve that is: 
Smooth, Monotone, and has the same curvature as a helix $(\cos(t),\sin(t),t)$. Anyway this type of curve should exist but I cannot construct it.. Alternativly, I was thinking could we reconstruct the curve from its torion and curvature functions; since they determine a unique curve (up to rigid motion) in Euclidean space. If so, the curve would have to satisfy $k(s)=1/\sqrt 2$ and $t(s)=1-2s$. Many thanks in advance! :)","['vector-analysis', 'differential-geometry']"
223058,Does $\lim_{n\rightarrow \infty} \int_X f_n - \int_X f\gt 0$ implies that convergence of $f_n$ to $f$ a.e. fails?,"I've come across this problem as a part of another proof that I'm writing and I want to know if this is a right conclusion: Let $X$ be a finite measure space and $\{f_n\}$ be a sequence of nonnegative integrable functions. If I know: $$\lim_{n\rightarrow \infty} \int_X f_n - \int_X f \geq \delta > 0,$$ can I conclude that $\mu\{x: f_n \nrightarrow f\} > 0$ or in other words $f_n$ doesn't convege to $f\ a.e.$?","['measure-theory', 'convergence-divergence', 'integration', 'real-analysis']"
223091,Non-trivial homomorphism between multiplicative group of rationals and integers,"Let $\mathbb{Q}^{\times}$ be the multiplicative group of non-zero rationals. Is there a non-trivial homomorphism $\mathbb{Q}^{\times} \to \mathbb{Z}$? In the same spirit, is there a homomorphism $\mathbb{Z} \to \mathbb{Q}^{\times}$?","['group-theory', 'abelian-groups']"
223096,How to prove positive definiteness?,"$B_{(n+1)(n+1)}$ = $       \begin{bmatrix}
        A & u \\
        u^T & 1 \\
        \end{bmatrix}$ is given, and $A$ is a positive definite matrix where its Cholesky factorization is given by $A=L*L^T$ formula. $A$ is $n\times n$ matrix and $u$ is a n_vector. Now assuming $\|L\|^{-1}\leq 1$, I need to show that $B$ is a positive definite for all $\|u\|<1$. Thanks","['matrices', 'normed-spaces']"
223122,Finding the image of a mapping over a region.,"I'm having a very hard time understanding the concept of images and mappings in the complex plane. Considering the map $w=e^{z}=e^{x}e^{iy}$, find the image of the region $\left\lbrace x+iy:x\geq 0, 0\leq y \leq\pi \right\rbrace$. Based on my current understanding, I have rewritten $w=e^z$ by breaking it apart with Euler's Formula:
$$w=e^{x}\left(\cos{y}+i\sin{y}\right)=e^{x}\cos{y}+ie^{x}\sin{y}.$$
From here, we know that $u(x,y)=e^{x}\cos{y}$ and $v(x,y)=e^x\sin{y}$. Could I then rewrite the mapping as $f(x,y)=\left(e^{x}\cos{y},e^x\sin{y}\right)$ in order to sketch the seperate $xy$ and $uv$ planes?","['complex-numbers', 'complex-analysis']"
223129,"Show there is no measure on $\mathbb{N}$ such that $\mu(\{0,k,2k,\ldots\})=\frac{1}{k}$ for all $k\ge 1$","For $k\ge1$, let $A_k=\{0,k,2k,\ldots\}.$ Show that there is no measure $\mu$ on $\mathbb{N}$ satisfying $\mu(A_k)=\frac{1}{k}$ for all $k\ge1$. What I have done so far: I am trying to apply Borel-Cantelli lemma ($\mu(\mathbb{N})=1)$. Let $(p_n)_{n\in \mathbb{N}}=(2,3,5,\ldots)$ be the increasing sequence of all prime numbers. It is the case that for all $k\in\mathbb{N}$ and $i_1\lt i_2 \lt \ldots \lt i_k$ we have $\mu\left(A_{p_{i_1}}\cap\ldots\cap A_{p_{i_k}}\right)=\mu\left(A_{p_{i_1}}\right)\ldots\mu\left(A_{p_{i_k}}\right)$
so all $A_n$ are independent. We know that the sum of the reciprocals of the primes $\sum\limits_{n=1}^\infty \frac{1}{p_n}$diverges and hence, by B-C second lemma, $$\mu\left(\bigcap\limits_{n=1}^\infty\bigcup\limits_{m=n}^\infty A_{p_m}\right)=1$$ holds. However, I cannot figure out how to conclude the reasoning. I would appreciate any help.",['measure-theory']
223142,What are some good intuitions for understanding Souslin's operation $\mathcal{A}$?,"What are some good intuitions for understanding Souslin's operation $\mathcal{A}$? Recall the definition: Let $S = \mathbb{N^{<N}} = \bigcup_{n = 1}^\infty \mathbb{N}^n$ be the set of non-empty finite sequences in $\mathbb{N}$ and let $\mathcal{E} \subseteq P(X)$ be a family of subsets of a given set $X$. A Souslin scheme is an assignment $E \colon S \to \mathcal{E}, s \mapsto E_s$ and one defines its kernel to be
$$
\mathcal{A}E = \mathcal{A}_s E_s = \bigcup_{\sigma \in \mathbb{N}^{\mathbb N}}\bigcap_{n=1}^\infty E_{\sigma{\upharpoonright}n} \subseteq X
$$
where $\sigma{\upharpoonright}n = \langle \sigma(0),\dots,\sigma(n-1)\rangle \in \mathbb{N}^n$. The collection of all subsets of $X$ obtained from $\mathcal{E}$ in this fashion is denoted by $\mathcal{A}(\mathcal E)$. I feel comfortable with the fundamental properties of the $\mathcal{A}$-operation (and their proofs). To list a few of the basic facts I think I understand: it subsumes countable unions and intersections; idempotence: if $\mathcal{E} \subseteq P(X)$ is any class of subsets then $\mathcal{A}(\mathcal{E}) = \mathcal{A}(\mathcal{A}(\mathcal{E}))$; if $\emptyset, X \in \mathcal{A}(\mathcal{E})$ and $X \setminus E \in \mathcal{A}(\mathcal{E})$ for all $E \in \mathcal{E}$ then $\sigma(\mathcal{E}) \subseteq \mathcal{A}(\mathcal{E})$. In particular if $\mathcal{E} \subset P(\mathbb{R})$ is the family of closed intervals with rational endpoints then $\mathcal{A}(\mathcal{E})$ contains the $\sigma$-algebra Borel sets (and in fact the containment is strict). if $(X,\Sigma,\mu)$ is a measure space obtained from Carathéodory's construction on some outer measure on $X$ then $\Sigma$ is closed under the $\mathcal{A}$-operation: $\mathcal{A}\Sigma = \Sigma$. the kernel of a Souslin scheme can be interpreted as the image $R[\mathbb{N^N}]$ of a relation $R \subseteq \mathbb{N}^\mathbb{N} \times X$, in particular if $X$ is Polish then the $\mathcal{A}$-operation on closed sets gives us the analytic sets. If $\langle E_s : s \in \mathbb{N}^{\lt \mathbb{N}}\rangle$ is a regular Souslin scheme of closed sets with vanishing diameter then its associated relation $R \subset \mathbb{N}^\mathbb{N} \times X$ is the graph of a continuous function $f\colon D \to  X$ defined on some closed subset $D$ of $\mathbb{N^N}$. etc. The point of this list is just to mention that I think that I've done my share of the manipulations with trees and $\mathbb{N}^\mathbb{N}$ that come along with $\mathcal{A}$, but I still have the feeling that something fundamental escapes me. After looking at the two 1917 Comptes Rendus papers Sur une définition des ensembles mesurables $B$ sans nombres transfinis by Souslin and Sur la classification de M. Baire by Lusin, I also think I understand that part of the inspiration was the continued fraction representation of real numbers. Given the importance of the $\mathcal{A}$-operation (entire books were written on its uses, e.g. C.A. Rogers et al., Analytic Sets where there is a wealth of applications) it would be nice to have some good intuitions that allow me to have a firmer grasp of what is going on. Somehow it seems that the $\mathcal{A}$-operation is mostly presented as a technical device having an enormous range of applications, but this doesn't seem to do justice to the concept.","['set-theory', 'descriptive-set-theory', 'general-topology', 'measure-theory', 'intuition']"
223146,Closed-form expression for sum of Vandermonde matrix elements,"Given the Vandermonde matrix: $$\begin{pmatrix}1^0 & 1^1 & 1^2 & ... & 1^n \\
2^0 & 2^1 & 2^2 & ... & 2^n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
N^0 & N^1 & N^2 & ... & N^n\end{pmatrix}$$ the row sums can be calculated using the closed-form expression for a geometric series (or the column sums calculated using the Bernoulli expression for n-th powers).  Is there a closed-form expression to simplify the sum of all elements within the Vandermonde matrix?  That is, is there a closed-form expression for: $$\begin{align*}
1+n+\sum_{r=2}^{N}\frac{(1-r^{n+1})}{(1-r)}
\end{align*}$$ Or alternatively, is there any way to expedite summing all elements within the Vandermonde matrix?  Thanks.","['sequences-and-series', 'discrete-mathematics', 'polynomials', 'matrices', 'combinatorics']"
223148,$L_p$ Spaces and limits of translated functions,"If $g\in L^p(\mathbb{R}^n)$  and $1\leq p<\infty$ then show $$\lim_{|t|\to \infty}\lVert g_{(t)}+g\rVert_p=2^{1/p}\lVert g\rVert_p,$$ where $g_{(t)}(x):=g(t+x)$. Any hints? Try to give me only hints/outlines not complete solutions Not sure where to go from there?","['functional-analysis', 'measure-theory', 'real-analysis', 'analysis', 'lp-spaces']"
223153,Functions satisfying Cauchy Riemann equations at a point,"Do there exist two functions $u,v$ defined on an open set containing $(0,0)$ with values in $\mathbb{R}$ such that (1) $u,v$ are differentiable at $(0,0)$; (2) $u_x=v_y, u_y=-v_x$ at $(0,0)$; (3) at least one of $u_x,u_y,v_x,v_y$ is not continuous at $(0,0)$ ?","['multivariable-calculus', 'complex-analysis']"
223176,What is the derivative of $f(x)=e^{f^{\prime \prime}}$,"I made this problem: $f(x)=e^{f^{\prime \prime}}$ I have just been taught the first derivative, and was thinking about what if the derivative depended upon it own derivative.  I understand that $e^x$ is its ""own"" derivative, but the problem I made I was thinking that the first derviative is not logical, because to know the first derivative you then must know the 2nd or 3rd derviative, it seems self-referenecing. Is the problem I made, a real problem or just some abstract idea?","['calculus', 'derivatives']"
223184,How can I find the limiting value of a time-dependent PDE?,"I've managed to reduce a question in probability to the following simple looking PDE:
$$
u_t = -t u_x + \frac{1}{2} u_{xx}, {\rm ~for~} x>0, \, t \in \mathbb{R} \;,
$$
with a limiting initial condition:
$$
u(x,t) \to 1, {\rm ~as~} t \to -\infty \;,
$$
a boundary condition at $x=0$:
$$
u_x(0,t) = \left\{ \begin{array}{lc}
-2 a t u(0,t) \;, & t \le 0 \\
0 \;, & t \ge 0
\end{array} \right. \;,
$$
where $a > 0$ is a constant,
and a suitable boundary condition as $x \to \infty$: either $u \to 1$ or $u$ is bounded. All I want is an exact expression for $p = \lim_{t \to \infty} u(0,t)$, which represents the particular probability that I am after, but I don't know how to do this or if it's possible. I'm hoping there's some sort of method out there that I don't know about to derive $p$ without having to solve the whole boundary value problem. Any ideas? A few comments: 1)
I've solved it numerically (with finite differences) and $u(x,t)$ looks nice and smooth and only takes values between 0 and 1. 2)
I don't expect the piecewise nature of the boundary condition at $x=0$ to be a major issue. Presumably we can solve up to $t=0$, pause, and then solve for $t>0$.
For $t \le 0$, the boundary condition is a time-dependent Robin boundary condition, which I've never seen dealt with before. 3)
To me the major problem seems to be the explicit time-dependency in the PDE and the boundary condition at $x=0$. I can get rid of the time-dependency in the PDE by defining
$$
v(x,t) = {\rm e}^{\frac{1}{6} t^3 - t x} u(x,t) \;,
$$
which gives
$$
v_t = -x v + \frac{1}{2} v_{xx} \;,
$$
$$
v_x(0,t) = \left\{ \begin{array}{lc}
-(1+2a)tv(0,t) \;, & t \le 0 \\
-tv(0,t) \;, & t \ge 0
\end{array} \right. \;.
$$ 4)
To the boundary value problem in $v$, I can't get separation of variables, or Laplace transforms in time, or some sort of half Fourier transform in space, to work. 5)
Incidentally, $\lim_{t \to \infty} u(x,t)$, should be independent of $x$, that is, pointwise, $u(x,t)$ approaches the constant value $p$.","['probability', 'partial-differential-equations', 'limits']"
223221,"If $X$ is infinite dimensional, all open sets in the $\sigma(X,X^{\ast})$ topology are unbounded.","As in the title, if $X$ is infinite dimensional, all open sets in the $\sigma(X,X^{\ast})$ topology are unbounded. The $\sigma(X,X^{\ast})$ topology is the weakest topology that makes linear functionals on $X^\ast$ continuous. How does one show this? How does having an infinite basis relate to open sets being unbounded? I can't see this, please help and thanks in advance!","['general-topology', 'vector-spaces', 'functional-analysis']"
223223,The number of possible factorizations of a positive integer.,"Given a positive integer $n>1$ with prime factorization $$n=\prod_{p_i \text{ prime}}p_i^{k_i}, \space i\ge1, \space k_i \in \mathbb N^*$$ how can I compute the number of factorizations of $n$, $\text F(n)$ (multiplications by $1$ are excluded) ? $5\times 24$ and $4\times 5\times 6$ are two different factorizations of $120$. The prime factorization of a number is of course one of its factorizations. $\text F(p) = 0$ for any prime number $p$. If there is a no formula, an algorithm will be appreciated.","['prime-numbers', 'elementary-number-theory', 'number-theory', 'combinatorics']"
223238,What is the average of rolling two dice and only taking the value of the higher dice roll?,"What is the average result of rolling two dice, and only taking the value of the higher dice roll? To make sure the situation I am asking about is clear, here is an example: 
I roll two dice and one comes up as a four and the other a six, the result would just be six. Would the average dice roll be the same or higher than just rolling one dice?","['average', 'dice', 'probability']"
223240,How many distinct functions can be defined from set A to B?,"In my discrete mathematics class our notes say that between set $A$ (having $6$ elements) and set $B$ (having $8$ elements), there are $8^6$ distinct functions that can be formed, in other words: $|B|^{|A|}$ distinct functions. But no explanation is offered and I can't seem to figure out why this is true. Can anyone elaborate?","['discrete-mathematics', 'combinatorics']"
223247,What is the intuition behind the Lie derivative of a vector field.,"We have the following two formula about the Lie derivative of a vector field: $$
\left.\frac{d}{dt}\right|_{t=0}T\varphi_{-t}\cdot Y_{\varphi_t(p)}=[X,Y]_p = (\mathcal{L}_XY)(p)
$$ where $\varphi=\varphi^X(t,p)$ is the flow along the vector field $X$, and equivalently, $$
\mathcal{L}_XY=\left.\frac{d}{dt}\right|_{t=0}(\varphi_t^{-X})^*Y
$$ where $(\varphi_t^{-X})^*$ is the pull-back of $\varphi_t^{-X}$. I have a rough idea about what this formula is saying: let $Y$ is a vector field defined along a integral curve of $X$, and we ""pull-back"" the vector of $Y$ at point $q=\varphi_t(p)$ to its original point $p$ and measure the change rate w.r.t $t$. But such explanation is quite forced and can not satisfy me. Can anyone provide a intuitive explanation of the Lie derivative?...","['intuition', 'differential-geometry']"
223252,Why is gradient the direction of steepest ascent?,"$$f(x_1,x_2,\dots, x_n):\mathbb{R}^n \to \mathbb{R}$$ The definition of the gradient is $$ \frac{\partial f}{\partial x_1}\hat{e}_1 +\ \cdots +\frac{\partial f}{\partial x_n}\hat{e}_n$$ which is a vector. Reading this definition makes me consider that each component of the gradient corresponds to the rate of change with respect to my objective function if I go along with the direction $\hat{e}_i$ . But I can't see why this vector ( defined by the definition of the gradient ) has anything to do with the steepest ascent. Why do I get maximal value again if I move along with the direction of gradient?","['multivariable-calculus', 'vector-analysis', 'scalar-fields']"
223258,Singularity at infinity of a function entire,"How to prove that every non-constant entire function $\,\,f:\mathbb{C}\rightarrow\mathbb{C}\,\,$ has a singularity at infinity? What type of singularity must this be?",['complex-analysis']
223280,Induced Exact Sequence of Dual Spaces,"So given a short exact sequence of vector spaces $$0\longrightarrow U\longrightarrow V \longrightarrow W\longrightarrow 0$$  With linear transformations $S$ and $T$ from left to right in the non-trivial places. I want to show that the corresponding sequence of duals is also exact, namely that $$0\longleftarrow U^*\longleftarrow V^* \longleftarrow W^*\longleftarrow 0$$ with functions $\circ S$ and $\circ T$ again from left to right in the non-trivial spots.  So I'm a bit lost here.  Namely, I'm not chasing with particular effectiveness.  Certainly this ""circle"" notation is pretty suggestive, and I suspect that this is a generalization of the ordinary transpose, but I'm not entirely sure there either. Any hints and tips are much appreciated.","['vector-spaces', 'linear-algebra', 'duality-theorems', 'abstract-algebra']"
223295,Linear Algebra and Trig Identity Proof,"I am working on the following question. It involves finding a proof for a trig identity using linear algebra. The proof is one involving $sin(\alpha +\theta)$ and $cos(\alpha +\theta)$, as you will see. I will go through where I am up to, progressing through each part of the question. Let $T_{\alpha}$ be the linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$
    which is the rotation counterclockwiseby $\alpha$, and $T_{\theta}$ the counterclockwise rotation by θ. (A) Write down the standard matrices for $T_{\alpha}$ and $T_{\theta}$, explain your reasoning. Let the standard matrix for $T_{\alpha}$ be $A$ and let the standard matrix for $T_{\theta}$ be $B$, then $$A=\begin{bmatrix}
       \cos (\alpha) & -\sin (\alpha) \\
       \sin (\alpha) & \cos (\alpha) \\
     \end{bmatrix}$$ and $$B=\begin{bmatrix}
       \cos (\theta) & -\sin (\theta) \\
       \sin (\theta) & \cos (\theta) \\
     \end{bmatrix}$$ On to the next part of the queston. (B) Explain what the linear transformation $T_{\alpha} \circ T_{\theta}$ does to $\mathbb{R}^2$. It first rotates a given point by $\alpha$ degrees and then rotates the given point by $\theta$ degrees. (C) Compute the matrix for $T_{\alpha} \circ T_{\theta}$ by multiplying the matrices for $T_{\alpha}$ and $T_{\theta}$ So, $$AB = \begin{bmatrix}
       \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\
       \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\
     \end{bmatrix}$$ (D) On the other hand, from the description in part (b), you can directly write down
    the matrix for $T_{\alpha} \circ T_{\theta}$. What is that matrix? If that matrix is $C$, then $$C=\begin{bmatrix}
       \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\
       \sin (\alpha + \theta) & \cos (\alpha + \theta) \\
     \end{bmatrix}$$ (E) Since the matrices from parts (c) and (d) are describe the same linear transformation, they must be equal. What identities among sin and cos must therefore be
    true? So I must set $AB=C$. Then $$\begin{bmatrix}
       \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\
       \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\
     \end{bmatrix}=\begin{bmatrix}
       \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\
       \sin (\alpha + \theta) & \cos (\alpha + \theta) \\
     \end{bmatrix}$$ These are the identities I was looking for! Now the next part has me worried. Using a similar idea, ﬁnd formulas for $\sin(3\theta)$ and $\cos(3\theta)$ in terms of $\sin(\theta)$ and $\cos(\theta)$. Now I am not completely hopeless - I was able to come up with this next bit. But I am not sure if I have done things correctly. I'll use the transformation $T_{\theta}$ from before. The standard matrix is $$A=\begin{bmatrix}
       \cos (\alpha) & -\sin (\alpha) \\
       \sin (\alpha) & \cos (\alpha) \\
     \end{bmatrix}$$ I thought that maybe if I transformed a point three times it would rotate it $3\theta$ degrees, i.e with a standard matrix $AAA$. The result was
$$AAA=B$$
$$=$$$$ \begin{bmatrix}
       \cos^{3}(\theta)-\cos(\theta)\sin^2(\theta)-2\cos(\theta)\sin^2(\theta) & -\sin(\theta)\cos^2(\theta)+\sin^3(\theta)-2\cos^2(\theta)\sin(\theta) \\ 
      2\sin(\theta)\cos^2(\theta)-\sin^3(\theta)+\cos^2(\theta)\sin(\theta)  & -2\sin^3(\theta)\cos(\theta)-\sin^2(\theta)cos(\theta)+\cos^3(\theta)\ \\
     \end{bmatrix}$$ As before I would say that $AAA$ is equal to a transformation matrix $$A'=\begin{bmatrix}
       \cos (3\alpha) & -\sin (3\alpha) \\
       \sin (3\alpha) & \cos (3\alpha) \\
     \end{bmatrix}$$ Then setting $A'=B$ would result in some identities. They just seem quite long and I am not sure if what I have done is correct or checks out. Any help would be appreciated.",['linear-algebra']
223306,"If $u = f(x,y)$, where $x=e^s \cos t$ and $y = e^s \sin t$, calculate $\frac{\partial^2u}{\partial x^2} + \frac{\partial^2u}{\partial y^2}$","If $ u = f(x,y)$, where $x=e^s \cos t$ and $y = e^s \sin t$, show that $$ \frac{\partial ^2u}{\partial x^2}  + \frac{\partial ^2u}{\partial y^2}= e^{-2s}\left[\frac{\partial ^2 u}{\partial s^2}+ \frac{\partial ^2 u}{\partial t^2}\right].$$ Currently, what I've done: $$\begin{align}
f_x &= (f_s, f_t),
\\f_s &= e^s \cos t, 
\\f_t &= -e^s \sin t,
\\(f_x)_x &= ((f_s)_s, (f_t)_t),
\\(f_s)_s &= e^s \cos t,
\\(f_s)_t &= -e^s \sin t
\\(f_t)_s &= -e^s \sin t
\\ (f_t)_t &= -e^s \cos t
\\ \ 
\\f_y &=(f_s, f_t), 
\\f_s &= e^s \sin t, 
\\f_t &= e^s \cos t,
\\(f_y)_y &= ((f_s)_s, (f_t)_t),
\\(f_s)_s &= e^s \sin t,
\\(f_s)_t &= e^s \cos t
\\(f_t)_s &= e^s \cos t
\\ (f_t)_t &= -e^s \sin t
\end{align}$$ Could tutors over here advise me whether I am on the right track, as I have no idea how to proceed from here. Thanks :)",['multivariable-calculus']
223308,integral of a measure zero set,"let $X$ be a finite measure space and $\{f_n\}$ be a sequence of nonnegative integrable functions, $f_n \rightarrow f\ a.e.$ on $ X$.
We know that 
$\lim_{n \rightarrow \infty}\int_X f_n d\mu=\int_X fd\mu$ and on any measurable $E_i \subset X$ I should apply Egoroff's theorem to conclude that $\lim_{n \rightarrow \infty}\int_X |f_n-f|d\mu=0$. My attempt: I broke the set $X$ to two sets: $F_\sigma$ on which $f_n \rightarrow f$ uniformly based on Egoroff's theorem and $X\backslash F_\sigma$ which is a very small set, i.e. $\mu\{X\backslash F_\sigma\}=\sigma$ and $f_n \nrightarrow f$ I want to show that on each of these sets, the integral is less than $\frac{\epsilon}{2}\ \forall \epsilon$ to finish. 
How can I show this for the set $X \backslash F_\sigma$?","['calculus', 'convergence-divergence', 'measure-theory', 'real-analysis', 'uniform-convergence']"
223352,Krull dimension and transcendence degree,"What is the simplest proof of the fact that an integral algebra $R$ over a field $k$
has the same Krull dimension as transcendence degree $\operatorname{trdeg}_k R$?
Is it possible to use only Noether normalization theorem?","['krull-dimension', 'ring-theory', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
223373,"iid variables, do they need to have the same mean and variance?","If two random variables $x$ and $y$ are identical and independently distributed, do they need to have the same mean and variance? Can there exist a case where they are iid and still have different parameters?","['probability-theory', 'examples-counterexamples']"
223405,Can elements in a set be duplicated?,"If $A = \{x \mid x \text{ is a letter of the word 'contrast'}\}$ Represent it in a Venn Diagram, and then find the $n(A)$. Do I need to write the letter 't' twice inside the venn diagram? What should be the answer for $n(A)$?",['elementary-set-theory']
223406,At most three different eigenvalues,"I have a problem with this first order DE: let $-\infty<a\leq x\leq b<+\infty$ and $$u'(x)+(\lambda+q(x))u(x)=0,\tag{1}$$ where $u$ is a continuous and real valued, while $\lambda$ is a parameter not depending on $x$. A strange non trivial boundary condition is given, namely $$\alpha u(a)+\alpha'u'(a)+\beta u(b)+\beta'u'(b)=0.$$ Then I have to show that this problem admits at most three eigenvalues. What I have tried: basically to convert this problem into a Sturm Liouville problem, however I couldn't conclude anything. Can anybody help me? How to go through this kind of problems? thanks in advance. -Guido-",['ordinary-differential-equations']
223421,Domain of convergence of power series,"For $|x| < 1$ we have the identity
$$ \frac{1}{1 + x^2} = 1 - x^2 + x^4 -x^6 \dots$$ I read that this holds because the left-hand side has singularities at $\pm i$ and by requiring $|x|<1$ we exclude them. But: if we think of $\mathbb C$ as $\mathbb R^2$ then there are more points than $|x|<1$ where the left-hand side is defined, for example $2+2i$. Does the identity above not hold for all $x \in \mathbb C \setminus \{i, -i\}$? And if yes, why do we require $|x|<1$ and in general, power series' domain of convergence to be disk shaped? (like $|z|<K$ instead of giving a precise domain like $\mathbb C \setminus \{ \text{ points where it goes wrong} \}$)",['complex-analysis']
223426,Naming quadrilaterals,"Is there a rule for naming quadrilaterals in English? What I am expected to know about are: square, rhombus, rectangle, parallelogram, trapezium, kite. But how do we name other quadrilaterals?","['geometry', 'terminology']"
223439,Showing $1/x^2$ is Lebesgue Integrable on $\mathbb{R}_{\ge 1}$,"I am trying to show that $\int_{\mathbb{R}_{\ge 1}} 1/x^2 < \infty$. (1) By definition, $\int_{\mathbb{R}_{\ge 1}} 1/x^2 = \underset{0 \le \phi \le 1/x^2}{sup} \int_{\mathbb{R}_{\ge 1}} \phi$ for $\phi$ a simple function. (2) If we let $\psi_n$ be a simple function s.t. $\psi_n = \Sigma_{k=2}^n 1/k^2 \chi_{[k,k-1]}$ then since the p-series $\Sigma_{n=2}^\infty 1/n^2 < \infty$ we have that $\forall n \in \mathbb{N}$, $
\int_{\mathbb{R}_{\ge 1}} \psi_n < \infty.$ From this it seems clear that the supremum of the $\int_{\mathbb{R}_{\ge 1}} \phi$ couldn't possibly be greater than the supremum of the $\int_{\mathbb{R}_{\ge 1}} \psi_n$ so that $\int_{\mathbb{R}_{\ge 1}} 1/x^2 < \infty$, yet how could I show this last step rigorously?",['measure-theory']
223448,Deformation to the normal cone,"Fulton in his book ""Intersection theory"" uses local description of this deformation that I can't understand. I quote paragraph from page 87 and insert my questions. Assume $Y=\operatorname{Spec}(A)$, and $X$ is defined by the ideal $I$ in $A$. To study blow-up of $Y \times \mathbb{P}^1$ near $\infty$ identify $\mathbb{P}^1-\{0\}$ with $\mathbb{A}^1=\operatorname{Spec} K[T]$, where $K$ is the ground field. The blow-up of $Y \times \mathbb{A}^1$ along $X \times \{0\}$ is $\operatorname{Proj}(S^{\bullet})$, with $$S^n=(I, T)^n=I^n+I^{n-1}T+\ldots+ AT^n+AT^{n+1}+\ldots.$$ First thing that I don't understand is why sum don't stop after $AT^n$, i.e. I'd suppose that $$S^n=(I, T)^n=I^n+I^{n-1}T+\ldots+ AT^n.$$ $\operatorname{Proj}(S^{\bullet})$ is covered by affine open sets $\operatorname{Spec}(S^{\bullet}_{(a)})$, where $S^{\bullet}_{(a)}$ is the ring of fractions $$S^{\bullet}_{(a)}=\{s/a^n | s \in S^n\},$$
and $a$ runs through a set of generators for the ideal $(I, T)$ in $A[T].$ Next claim that I don't understand is: for $a \in I,$ the exceptional divisor $P(C \oplus 1)$ is defined in $\operatorname{Spec}(S^{\bullet}_{(a)})$ by the equation $a/1$, $a \in S^0$. How is that possible first invert element $a$ and than consider equation $a=0$? Any help would be much appreciated. Update: graded algebra that correspond to the exceptional locus is $R=\sum_{n } (I, T)^n/(I,T)^{n+1}$, then we have short exact sequence $$0 \to (I, T)S^{\bullet} \to S^{\bullet} \to R^{\bullet} \to 0.$$ So $((I,T)S^{\bullet})_{(a)}=\{s/a^n | s \in S^{n+1}\}=\{a \frac{s}{a^{n+1}}| s \in S^{n+1}\}=aS^{\bullet}_{(a)}.$ After localization we get $$0 \to aS^{\bullet}_{(a)} \to (S^{\bullet})_{(a)} \to (R^{\bullet})_{(a)} \to 0.$$",['algebraic-geometry']
223454,Relation of mean of standard deviations and standard deviation,"Let $\{x_{i,j} : i=1..7,j=1,..n\}$ be a set of samples from $n$ weeks (where $i$ denotes the day of the week).
Is there any interesting information to be gleaned from the relationship (ratio, difference, etc.) between $ \mathbb{E}_{i} [std(x_i)]$ and $std(x)$?","['statistics', 'standard-deviation']"
223461,Completely regular-Topology,Prove that every metric space is a Tychonoff space. Can somebody please help me to show this space satisfies the completely regular axiom and the $T_1$ axiom.,['general-topology']
223462,Finding the equations of two tangents to a circle given the point of intersection.,"Find the equations of the two lines which pass through the point $(0,4)$ and form tangents to a circle of radius $2$, centered on the origin. Firstly, we have the equation of the circle $x^{2}+y^{2}=4$. Which we can rearrange to get $y$ in terms of $x$: $$y^{2}=4-x^{2}\implies y=\pm\sqrt{4-x^{2}}$$ However, we know that the tangents must touch points on the top half of the circle, therefore we can simply take the principle square root, $y=\sqrt{4-x^{2}}$. Moreover, since the lines originate from the same point, and are tangential to the same circle, the two tangential points are $(x,\sqrt{4-x^{2}})$ and $(-x,\sqrt{4-x^{2}})$. The gradients of the  tangents at these points can be found by implictly differentiating the original equation and obtaining: $$\frac{dy}{dx}=\frac{-x}{y}=\left\{\frac{-x}{\sqrt{4-x^{2}}},\frac{x}{\sqrt{4-x^{2}}}\right\}$$ However, I'm unsure how to go about completing this problem. I know it's a simple question, but I simply cannot see how to solve it. Thanks in advance!","['analytic-geometry', 'geometry', 'algebra-precalculus', 'circles']"
223469,"Is there a name for the set $\{T,F\}$?","Is there a name for the set containing the two Boolean values, i.e. $\{T,F\}$? I am also thinking if $B = \{T,F\}$, and $B^n = \underbrace{B \times B\times B ... \times B}_n$, then is there a proper name for $B^n$? I thought of something like ""Boolean n-space"", but Google shows me that's not how people refer to it. I really appreciate it it someone can point me to the relevant terms and concepts.","['terminology', 'boolean-algebra', 'elementary-set-theory']"
223478,topology - Quotient topology,Let $k:X \to Y$ be an onto map.How to prove that the quotient topology on $Y$ induced by $k$ is the largest topology relative to  which $k$ is continuous.,['general-topology']
223495,How to prove this function is coercive?,"$\def\R{\mathbb R}$$f\colon\R^n\to\R$ is a coercive function. Given that $A$ is a positive definite matrix in $\R^{n\times n}$ and $b$ is a vector in $\R^n$ ,prove the function $g\colon  \R^n\to\R$ defined by $g(x)=f(Ax+b)$ is coercive ( $x\in \R^n$ in this question). You may use the fact that $x^\top Ax≥λ_\min (A) x^\top x$ for any nonzero $x\in\R^n$ where $λ_\min (A)$ is the smallest eigenvalue of $A$ . ( $x^\top$ means transpose of $x$ ). Even though there is a hint, I still could not get it through.
I was thinking that if I can prove that $$\|Ax+b\|^2 > P(λ_\min (A)) \cdot \|x\|^2 = P(λ_\min (A)) \cdot x^\top x > P(λ_\min (A)) \cdot  r_1^2$$ where $P$ is a positive polynomial, $r_1$ is found by the fact that $f$ is coercive: for any $M>0$ , there exist $r_1>0$ such that for any $\|x\|\ge r_1$ , $f(x)>M$ .
If I can construct such $P$ , then let $P(λ_\min (A))r_1^2 = r^2$ , then for any $M>0$ , there exists $r>0$ , such that for any $\|x\|>r$ , $\|Ax+b\|>r_1$ , thus $g(x)=f(Ax+b)>M$ . Thus $g$ is coercive. But I just could not find a working $P$ .","['matrices', 'functions']"
223510,"Eigenvalues and determinant of conjugate, transpose and hermitian of a complex matrix.","For a strictly complex matrix $A$, 1) Can we comment on determinant of $A^{*}$ (conjugate of entries of $A$) , $A^{T}$ (transpose of A) and $A^{H}$ (hermitian of $A$). I know that for real matrices, $\det(A)=\det(A^{T})$. Does it carry over to complex matrices, i.e. does $\det(A)=\det(A^{T})$ in general? I understand $\det(A)=\det(A^{H})$ (from Schur triangularization). 2) The same question as first, now about eigenvalues of $A$. I would like to know about special cases, for instance what if $A$ is hermitian or positive definite and so on.","['matrices', 'linear-algebra']"
223516,Computing a finite binomial sum,"I want to compute $$S(n,m,a)=\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}\cdot a^k.$$
With $n,m\in\mathbb N$, $a\neq0$ and $S(n,0,a)=(a+1)^n$. What I have found already: I don't see any other options then integrating until we've got Newton's formula and then differentiating it as many times as we integrated. I have found some values:$$S(n,1,a)=na\cdot(a+1)^{n-1}$$ $$S(n,2,a)=an(an+1)(a+1)^{n-2}$$ $$S(n,3,a)=an(a^2n^2+3an-a+1)(a+1)^{n-3}.$$ And, since $\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^k=a\cdot\frac{\mathrm d}{\mathrm da}\int\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^{k-1}\mathrm da=a\cdot\frac{\mathrm d}{\mathrm da}\sum_{k=0}^{n}k^{m-1}\cdot\binom{n}{k}a^k$, I have the recursion $$S(n,m,a)=a\cdot\frac{\mathrm d}{\mathrm da}S(n,m-1,a).$$
But I can't find any general formula.","['binomial-coefficients', 'summation', 'integration', 'derivatives']"
223522,Quotient rule or algebra mistake,"I'm trying to derive this $$ f(x)=\frac{6}{1+2e^{-5x}}$$ and getting this $$ f'(x)=\frac{0(1+2e^{-5x})-6(0-10e^{-5x})}{(1+2e^{-5x})^2}$$ $$=\frac{60e^{-5x}}{(1+2e^{-5x})^2}$$ But the answer I get when checking on Wolfram Alpha is $$f'(x)=\frac{60e^{5x}}{(2e^{5x}+2)^2}$$ I don't understand how this works. How do the exponents for e become positive all of a sudden, and where does the +2 in the denominator come from?","['calculus', 'derivatives']"
223525,$f$ mapping open sets to open sets,I know that the definition of a continuous mapping between two topologies is defined as: For $\mathcal{X}$ and $\mathcal{Y}$ and $f$ such that $f:\mathcal{X}\rightarrow \mathcal{Y}$ if $f^{-1}$ maps open sets in $\mathcal{Y}$ to $\mathcal{X}$ then $f$ is continuous. However I was wondering if instead of $f^{-1}$ mapping open set to open sets what if $f$ maps open sets to open sets? I know that this does not define continuity but does it define anything that is in anyway useful? (It obviously defines something) I mean I can see that any homeomorphism satisfies this property so I'm thinking its maybe not that important? Thanks for any help,"['general-topology', 'analysis']"
223547,"Show that the function $g(x)=x^4+x^3+1$ is one-to-one on $[0,2]$","Show that the function $g(x)=x^4+x^3+1$ is one-to-one on $[0,2]$. My attempt To prove one-to-oneness, we shall use the definition, that is, if $f(x_1)=f(x_2)$ , then $x_1=x_2$ for all $x_1,x_2\in[0,2]$ Suppose $f(x_1)=f(x_2)$, then ${x_1}^4+{x_1}^3+1={x_2}^4+{x_2}^3+1$ Which wasn't much of an attempt, as I got stuck. Also I am rather new to proving conjections and surjections. Help! Thanks in advance!",['functions']
223552,Special Differential Equation (continued-2),May you help me out in solving inhomogeneous differential equation looking like[this is radial part of Schrodinger equation]: $$R^{\prime\prime}+\frac{1}{r}R^{\prime}-\frac{a^{2}}{r^{2}}R+b^{2}R-\frac{c^{2}}{r}\delta(r-\rho)R-d^{2}R=F\exp(ikr)$$,['ordinary-differential-equations']
223565,Continuous Mapping Theorem (CMT) for a sequence of random vectors,"I need help proving the Continuous Mapping Theorem (CMT) for random vectors. I'm currently reading Econometric Analysis for Cross Section and Panel Data by Jeffrey M. Wooldridge (Chapter 3, pp. 40 - 41, 2nd edition). Unfortunately, he leaves it to the reader to prove most asymptotic results. Additionally, almost every other econometrics textbook I read simply states the result. Definition 1: A sequence of random variables $x_n$ converges in distribution to a continuous random variable $x$ if and only if $\forall s \in \mathbb{R} \ \forall \epsilon >0   \ \exists N \ s.t. \  \forall n>N \; |Prob(x_n \leq s) - Prob(x \leq s)|<\epsilon$. We write $x_n \to^d x.$ [Note: A continuous random variable is one for which the cumulative distribution function is continuous.] Definition 2: A sequence of K $\times$ 1 random vectors $\mathbf{x}_n$ converges in distribution to the continuous random $K \times 1$ vector $\mathbf{x}$ if and only if $\forall \mathbf{c} \in \mathbb{R}^{K}$ such that $\mathbf{c}^T\mathbf{c} = 1$, $\mathbf{c}^T\mathbf{x}_n \to^d \mathbf{c}^T\mathbf{x}$, and we write $\mathbf{x}_n \to^d \mathbf{x}.$ Theorem 1: Let $\mathbf{x}_n$ be a sequence of $K \times 1$ random vectors such that $\mathbf{x}_n \to^d \mathbf{x}$. If $\mathbf{g}:\mathbb{R}^k\to\mathbb{R}^{\ell}$ is a continuous function, then $\mathbf{g}(\mathbf{x}_n)$ $\to^d$ $\mathbf{g}(\mathbf{x}).$ Definition 3: A sequence of random variables $x_n$ is bounded in probability if and only if $\forall \epsilon>0 \ \exists b_{\epsilon}>0 \ \exists N \ s.t. \forall n>N \ Prob(|x_n|>b_{\epsilon})$. A vector $\mathbf{x}_n$ is bounded in probability if and only if the random variables which constitute the vector of random variables are themselves bounded in probability. Theorem 2: If $\mathbf{x}_n \to^d \mathbf{x}$, where $\mathbf{x}$ is a $K \times 1$ vector, then $\mathbf{x}_n = O_p(1)$. I need rigorous proofs for Theorems 1 and 2. This problem has been frustrating me for a couple days now, so any help would go a long way. Thanks. CS","['statistics', 'convergence-divergence', 'probability', 'probability-theory']"
223566,Construct tangent to a circle,"Using a ruler and a compass how can construct a line through a point and tangent to a circle. What I don't want is to eyeball the line by trying to line-up the ruler over the circle. Best if I could construct the point of intersection first and then draw the line. PS. I know how to do it mathematically, I just don't know the steps for geometry, given A, C and the circle to find D. Update Based on answers here is the constructors. Thanks for the quick responses.","['geometry', 'geometric-construction', 'euclidean-geometry']"
223577,Eigenspace of the companion matrix of a monic polynomial,"How do I prove that the eigenspace of an $n\times n$ companion matrix 
$$
C_p=\begin{bmatrix}
0 & 1 & 0 &\cdots & 0\\
0 & 0 & 1 &\cdots & 0 \\
\vdots&\vdots &\vdots&\ddots&\vdots\\
0 & 0 & 0 &\cdots &1 \\
-\alpha_0 &-\alpha_1 &-\alpha_2 &\cdots&-\alpha_{n-1}
\end{bmatrix}
$$
equals $\operatorname{Span}\{v_{\lambda} \} $ where $v_{\lambda}$ is an eigenvector of the companion matrix w.r.t. the eigenvalue $\lambda$: $$ v_{\lambda} =  \begin{bmatrix} 1 \\ \lambda\\ \lambda^{2} \\ \vdots\\ \lambda^{n-1} \end{bmatrix}. $$","['matrices', 'linear-algebra', 'companion-matrices', 'eigenvalues-eigenvectors']"
