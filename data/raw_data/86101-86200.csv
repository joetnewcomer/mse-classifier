question_id,title,body,tags
1139459,"How many positive integers less than $2011$ cannot be expressed in the form $4a + 5b$, where $a$ and $b$ are positive integers?","How would I solve the following question: How many positive integers less than $2011$ cannot be expressed in the form $4a + 5b$, where $a$ and $b$ are positive integers? I was trying to apply the Chicken McNugget theorem to it, but to no avail. Is there a simpler way to solve the question? (The answer is $13$ btw)","['contest-math', 'number-theory']"
1139466,Expecatation and Variance question,A clown moves forward in jumps of 1 unit to the right or one unit to the left. He jumps left with P=p and right with $P=1-p$. With independence between jumps. Let $C$ be the position of the clown after n jumps. What is $E[C]$ and $V[C]$. I basically said the clown will be  np jumps to the left and  $n(1-p)$ to the right. So  $E[C]=np-n(1-p)=2np-n$. Where a positive expectation means the clown will be positioned to the left. Does this seem correct? And would variance be  $np(1-p)$?,"['statistics', 'probability', 'expectation']"
1139471,how to show existence of limit? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I'm working some problems, and the questions states; ""decide if a limit exists. If it exists, find it"". But how am I supposed to do this? The tools I have at my disposal are the limit definition and what the limit is when we combine functions with known limits. It's a function in two variables.",['limits']
1139479,Are there any examples of a random variable with infinite second moment and finite variance?,"The theorem in my notes says that: If $|X - a|^\alpha$ has expectation for any $a \in \mathbb{R}$, and any $\alpha > 0$, then $|X - b|^\beta$ has expectation for any $b \in \mathbb{R}$ and any $\beta$ such that $0 \leq \beta \leq \alpha$. So if we know that the second moment $E \, X^2$ is not finite while $\operatorname{Var} X$ is, are $E\,X$ and $E(X - E(X \mid Y))^2$ finite? Is there any example or counter-example for this case? Update: From the theorem, it seems that $|X−a|^\alpha$ can have expectation for only a set of $a$ (say $a\in A$) at a specific $\alpha$. So is it possible that $|X−E\,X|^2$ has expectation while $X^2$ doesn't?","['probability-theory', 'conditional-expectation']"
1139488,"Prove that if the equation $x^{2} \equiv a\pmod{pq}$ has any solutions, then it has four solutions.","Suppose $n = pq$ with $p$ and $q$ distinct odd primes.
  Suppose that $\gcd(a,pq)=1$. Prove that if the equation $x^{2} \equiv a\pmod n$ has any solutions, then it has four solutions. Proof: Suppose $n = pq$ with $p$ and $q$ distinct odd primes and that gcd($a,pq$) = 1. Let us have the equation $x^{2} \equiv a\pmod n$. Then, $x^{2} \equiv a$ (mod $pq$), $p \not = q$. By definition, we can separate the equation into two equations such that $y^{2} \equiv a \equiv b$ (mod $p$) and $z^{2} \equiv a \equiv c$ (mod $q$). Let $g_{p}$ be a primitive root modulo $p$ and $g_{q}$ be a primitive root modulo $q$. Then, $b$ is equal to some power of $g_{p}$ and $c$ is equal to some power of $g_{q}$. With the fact that $b$ has a square root modulo $p$ (i.e. $r^{2} \equiv b$(mod $\ p)$) and $c$ has a square root modulo $q$ (i.e. $t^{2} \equiv c$(mod $\ q)$), there is an even power of $g_{p}$ and of $g_{q}$ such that $b = g_{p}^{2k_{1}}$(mod $\ p)$ and $c = g_{q}^{2k_{2}}$(mod $\ q)$ for some $k_{1}, k_{2} \in Z$. By computing, we have the following: $r^{2} \equiv b$(mod $\ p)$
      $\equiv b^{(p + 1) / 2}($mod $\ p)$
      $\equiv (g_{p}^{2k_{1}})^{(p + 1) / 2}($mod $\ p)$
      $\equiv (g_{p}^{p + 1})^{k_{1}}($mod $\ p)$
      $\equiv g_{p}^{2k_{1} + (p - 1)k_{1}}($mod $\ p)$
      $\equiv b \cdot g_{p}^{(p - 1)k_{1}}($mod $\ p)$
      $\equiv b$
and
$t^{2} \equiv c$(mod $\ q)$
       $\equiv c^{(q + 1) / 2}($mod $\ q)$
       $\equiv (g_{q}^{2k_{2}})^{(q + 1) / 2}($mod $\ q)$
       $\equiv (g_{q}^{q + 1})^{k_{2}}($mod $\ q)$
       $\equiv g_{q}^{2k_{2} + (q - 1)k_{2}}($mod $\ q)$
       $\equiv c \cdot g_{q}^{(q - 1)k_{2}}($mod $\ q)$
       $\equiv c$ Hence, $r$ is a square root of a modulo $p$ and $t$ is a square root of a modulo $q$, which means there are two solutions for each $p$ and $q$. Since $p \not = q \in \mathbb{Z}_{n}$, we have isomorphism such that $\mathbb{Z}_{n} \simeq \mathbb{Z}_{p} \cdot \mathbb{Z}_{q}$. Therefore, if the equation $x^{2} \equiv a$ (mod $n$) has any solutions, it must have four solutions. $\blacklozenge$ What do you think about the proof I wrote?","['elementary-number-theory', 'proof-verification', 'abstract-algebra']"
1139490,"The union of two sets A, B is the set AUB. Prove that if A and B are nonempty bounded subsets of R, then AUB is bounded and supAUB = max{supA, supB}.","Proof: If A,B C R and are nonempty and bounded, ==> There exists a least upper bound M s.t: x <= M for all x in A or x in B, by the Completeness Axiom. If A and B are bounded ==> AUB is bounded. Since AUB is bounded, ==> There exists a least upper bound M s.t: x <= M for x in AUB, by the Completeness Axiom. So AUB has a supremum, namely the maximum of either set: supA or supB. ==> supAUB = max{supA, supB}. QED. Thoughts?","['proof-writing', 'elementary-set-theory', 'proof-verification']"
1139526,Limit of $\frac{n^4+4^n}{n+4^{n+1}}$,"Just when i  thought i finally got the hang of limits, i stumbled upon this: $$\frac{n^4+4^n}{n+4^{n+1}}$$ Now, this kinda makes sense in my head because $4^n$ grows  a lot faster than $n^4$, let alone $n$. Now my question is if this assumption is a valid tool for solving this limit i.e. if i divide both denominator and numerator with $4^n$, can that be user without further explanation or proof. This, of course, gives the result of $\frac{1}{4}$ which seems to be correct. EDIT: $n \in \mathbb{N}$",['limits']
1139528,"Books with huge collections of hard, and original, problems on High School & Olympiad-level l Algebra?","I'm searching for great and unknown books with huge collections of hard and original problems for my students on High School Algebra or Olympiad level Algebra,  related ( but not limited ) to factoring polynomials and identities. What I need is a huge collection of hard and unseen problems in high school algebra, because unfortunately most of the problems in these books below are well-known to my students and cannot feed their hunger . My students are about 13 and 14, are really talented. Problem Solving Strategies by Arthur Engel 101 Problems in Algebra by Titu Andreescu and Zuming Feng Introduction to Algebra by Richard Rusczyk","['book-recommendation', 'algebra-precalculus', 'reference-request']"
1139565,"How many maximal ideals of $\mathbb{C}[x,y]$ contain $(f(x,y))$?","Consider $\mathbb{C}[x,y]$, and let $f(x,y)$ be irreducible. My question is how many maximal ideals $(x-a,y-b)$ contain $(f(x,y))$? I am trying to visualize the curves in $\mathbb{A}_{\mathbb{C}}^2$ as (finite union of) the zariski closure of $(f(x,y))$. I want to see how many zero dimensional points lie on a one dimensional point.","['algebraic-geometry', 'abstract-algebra']"
1139569,Equivalence of two definitions of the derivative of a real function,"The derivative of $ x $ in an interval $ [a,b] $ on which a function $ f $ is defined is defined as.. $$f'(x)=\lim_{t \to x}\frac{f(t)-f(x)}{t-x}$$ Why is this equal to $$ f'(t)=\lim_{x \to t}\frac{f(x)-f(t)}{x-t}?$$","['definition', 'derivatives', 'real-analysis', 'limits']"
1139579,Why is mathematical induction a valid proof technique? [duplicate],"This question already has answers here : What makes induction a valid proof technique? (4 answers) We all use mathematical induction to prove results, but is there a proof of mathematical induction itself? (6 answers) Closed 8 years ago . Context: I'm studying for my discrete mathematics exam and I keep running into this question that I've failed to solve. The question is as follows. Problem: The main form for normal induction over natural numbers $n$ takes the following form: $P(1)$ is true, and for every $n, P(n-1)\to P(n).\qquad\qquad\qquad\text{[Sometimes written as $P(n)\to P(n+1)$]}$ If both 1 and 2 are true, then $P(n)$ is true for every $n$. The question is to prove the correctness of the form above. My work: My idea was to make a boolean statement and if it's a tautology in a true false table. That means the statement is always correct. I've tried many times but I've failed. Here is the original question in Hebrew: n טענה כלשהי לגבי מםפר טבעי  $P(n)$ תהי אם מתקימיים שני התנאים הבאים: 1- הטענה $P(0)$ נכונה 2- לכל n>0, $P(n)$ נכונות הטענה $P(n-1)$ גוררת את נכונות הטענה אז $P(n)$ נכונה לכל מספר טבעי n הוכיחו את נכונות משפט זה","['philosophy', 'logic', 'induction', 'discrete-mathematics']"
1139588,Good article or book to q-analogs?,"I want to learn more about the following Topics: $q$-analog calculus what can be done with the $q$-Pochhammer Symbol applications of $q$-analogs in combinatorics However, I have found very Little that answers These questions comprehensively. Are there some books or good articles where These Topics are explained? Every reply will be appreciated.","['calculus', 'reference-request', 'q-analogs', 'combinatorics']"
1139592,Show that $A[X]/(aX+b)$ is an integral domain,"Let $A$ be an integral domain, $a$ and $b \in A-\{0\}$, and let $B = A[X]/(aX+b)$. Show that, if $Aa \cap Ab=Aab$, then $B$ is an integral domain. My attempt at proof (following a hint). Denote by $K$ the field of fractions of $A$. Let $\phi: A[X] \to K$, with $\phi(X)=-b/a$ and $\phi(y)=y, y \in A$. Then $\phi(aX+b)=-b+b=0$. If $p(X) \in A[X]$ and $p(X) \not \in (aX+b)$, then $p(X)=q(X)(aX+b)+r$, $r \in A$. Therefore, $\phi(p(X))=\phi(r)=r$, so $p(X) \in \ker(\phi) \iff r=0 \iff p(X) \in (aX+b)$. Hence $\ker(\phi)=(aX+b)$; by the first isomorphism theorem, $A[X]/(aX+b)$ must be isomorphic to Im$(\phi)$. In conclusion $B$ is isomorphic to a subfield of $K$, so it's an integral domain. The problem is, I don't know where I used the condition $Aa \cap Ab=Aab$. Am I doing something wrong?","['divisibility', 'ring-theory', 'integral-domain', 'abstract-algebra', 'polynomials']"
1139603,Check my proof -- The completion of a $\sigma$-finite measure,"This is a homework problem and I need some guidance on a proof. Let $(X,\mathcal{M},\mu)$ be a measure space, $\mu^*$ the outer measure induced by $\mu$ according to (1.12), $\mathcal{M}^*$ the $\sigma$-algebra of $\mu^*$-measurable sets, and $\overline{\mu}=\mu^*\mid_{\mathcal{M}^*}$. (a) If $\mu$ is $\sigma$-finite, then $\overline{\mu}$ is the completion of $\mu$. ${}$ (1.12) $\mu^*(E) = \inf\left\{ \sum_{1}^\infty \mu(A_j)\ \mid\ A_j \in \mathcal{M},\ E \subset \bigcup_1^\infty A_j \right\}.$ Definition : $A \subset X$ is $\mu^*$-measurable for an outer measure $\mu^*$ if $$\mu^*(E) = \mu^*(E \cap B) + \mu^*(E \cap B^C)$$
  for all $E \subset X$. We have the following Lemma: For an measure $\mu^*$ induced by a premeasure, if $\mu^*(E) < \infty$, then $E$ is $\mu^*$-measurable iff there exists $B \in \mathcal{A}_{\sigma\delta}$ with $E \subset B$ and $\mu^*(B\backslash E) =0$, where $\mathcal{A}_\sigma$ is the collection of countable unions of sets in $\mathcal{M}$, and $A_{\sigma\delta}$ is the collection of countable intersections of sets in $\mathcal{A}_\sigma$. with the following Corollary: If $\mu$ is $\sigma$-finite, then the condition $\mu^*(E) < \infty$ is superfluous. What I started with: Suppose there is some $E\in \mathcal{M}$ such that $E$ is not $\mu^*$-measurable but $\mu(E) = 0$. Then, since $E \in \mathcal{M}$, we have
\begin{equation}
\mu^*(E) = \inf \left\{ \sum \mu(A_j)\ \mid\ A_j \in \mathcal{M},\ E \subset \bigcup A_j\right\} = \mu(E) = 0.
\end{equation}
Consider that $\mu$ is $\sigma$-finite. Then $X = \bigcup_{j=1}^\infty F_j$, so $X \in \mathcal{M}_\sigma$, the collection of countable unions of sets in $X$. Likewise, $X \in \mathcal{M}_{\sigma\delta}$, the collection of countable intersections of sets in $\mathcal{M}_\sigma$. Then, $\mu^*(X\backslash\ E^C) = 0$, so $E^C$ is $\mu^*$-measurable, and hence $E$ is $\mu^*$-measurable. Therefore, no such $E$ can exist, and every $\mu$-null set is a $\mu^*$-null set in $\mathcal{M}^*$. Therefore, $\overline{\mu}$ is the unique completion of $\mu$. Is this the right track? From Daniel Fischer's comments, which I do not understand: By the proposition, every $E \in \mathcal{M}$ is $\mu^*$ measurable, since $\mathcal{M} = \mathcal{M}_\sigma = \mathcal{M}_{\sigma\delta}$. Since every $\mu^*$-measurable set can be approximated from within, assume $\mu^*(E) = \mu(E) = 0$. Then, $A \subset E \subset B \in \mathcal{M}$ and $\mu(B\setminus A) = 0$, so again by the lemma, $A \in \mathcal{M}^*$. Thus every subset of a null set in $\mathcal{M}$ is in $\mathcal{M}^*$.",['measure-theory']
1139620,Pairwise and Mutually disjoint sets,What is the difference between Pairwise and Mutually disjoint sets? The context of this is measure theory.,"['measure-theory', 'elementary-set-theory', 'real-analysis']"
1139638,Find the general solution of the differential equation,"It has been a long time (years) since I have worked with differential equations and I just want to check to make sure that my method is ok and also I had a few questions. Find the general solution of the differential equation. $$\frac{dy}{dt}+ycost=0$$ $$a(t)=cost$$ $$y(t)=Ce^{-\int a(t) dt}$$
$$y(t)=Ce^{-\int cost dt}$$
$$y(t)=Ce^{-sint }$$
$$y(t)=\frac{C}{e^{sint}}$$ $$\frac{dy}{dt}+\frac{2t}{1+t^2}y=\frac{1}{1+t^2}$$ $$a(t)=\frac{2t}{1+t^2}$$ $$μ(t)=e^{\int a(t)dt}$$
$$μ(t)=e^{\int \frac{2t}{1+t^2}dt}$$ Then doing a u substitution with $u=1+t^2$ and $du=2tdt$
(Would $μ(t)$ become $μ(u)$ or just $μ$ when I am doing the u substitution? $$μ(t)=e^{\int \frac{1}{u}du}$$
$$μ(t)=e^{ln|u|}$$
$$μ(t)=|u|$$ (Do I need to keep the absolute value signs here?) Resubstitute $1+t^2$ in for u: $$μ(t)=|1+t^2|$$ Multiply both sides of the original equation by $μ(t)$:
$$μ(t)[\frac{dy}{dt}+\frac{2t}{1+t^2}y]=μ(t)[\frac{1}{1+t^2}]$$
$$|1+t^2|[\frac{dy}{dt}+\frac{2t}{1+t^2}y]=|1+t^2|[\frac{1}{1+t^2}]$$
$$\frac{d}{dt}(1+t^2)y=1$$
$$(1+t^2)y=\int dt$$
$$(1+t^2)y=t + C$$
$$y=\frac{t}{1+t^2} + C$$ So I just need to use the $μ(t)$ method when the original equation has a right hand side not equal to 0 (is a nonhomogeneous differential equation, right?)",['ordinary-differential-equations']
1139652,Is $\{\varnothing\}$ a subset of every set $A$?,"The following statement appears as a true or false question: $\{ \varnothing \} \subseteq A$ for all sets $A$. Because it is the set containing the null set, instead of just the null set itself, I'm hesitant to say whether this is true or false. Can anyone clarify for me?",['elementary-set-theory']
1139685,Cauchy-Riemann conditions satisfying the Laplacian,"How does one convert the Cauchy-Riemann conditions
 into the form: $$\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2u}{\partial y^2} = 0, \qquad \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2v}{\partial y^2} = 0$$ from $$\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \qquad \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}$$ to show that a differentiable complex function has real and imaginary parts that separately satisfy the Laplace equation?
The book that I'm working with says that by ""differentiating first with respect to x and then with respect to y, one easily obtains (equation 2)"", but my calc3 is weak and I fail to see it.","['multivariable-calculus', 'derivatives', 'complex-analysis']"
1139697,If $f:G \to H$ is a homomorphism with kernel $N$ and $K$ is a subgroup of $G$ then prove that $f^{-1}(f(K))=KN$,"If $f:G \to H$ is a homomorphism with kernel $N$ and $K$ is a subgroup of $G$ then prove that $f^{-1}(f(K))=KN$ Ok, so what I know from this: $G$ and $H$ are groups that must preserve operation. It is associative, has an identity and an inverse. If this is so, then I could possibly show that the identity of $G$ maps to the identity of $H$, therefore inverse if preserved? The concept of the kernel has always confounded me, not sure why. Any homomophism defines an equivalence relation. The kernel is that relation. If $K$ is a subgroup of $G$, then $K$ is nonempty and closed, has an identity and has an inverse. So, take $K < G$ Show $f^{-1}(f(K))=KN$ when ${kn/ K\in K, n \in N}$ and $ker f=N< G$ so we can say $f^{-1}(f(K)) \subseteq KN \subseteq f^{-1}(f(K))$ I know that there is a similar equation already on this site, but I still need help. Thanks","['group-theory', 'abstract-algebra']"
1139712,Problem from Biggs graph theory,"From Norman Biggs, Algebraic Graph Theory , 2nd edition 1993, p. 13, exercise 2i: 2i. An upper bound for the largest eigenvalue. Suppose that the eigenvalues of $\Gamma$ are $\lambda_0 \geq \lambda_1 \geq \ldots \geq \lambda_{n-1}$, where $\Gamma$ has $n$ vertices and $m$ edges. From 2h we obtain $\sum \lambda_i = 0$ and $\sum \lambda_i^2 = 2m$. It follows that $$\lambda_0 \leq \left(\dfrac{2m\left(n-1\right)}{n}\right)^{\frac{1}{2}}.$$ I have tried to solve this problem but I just can't.  The matrix of a graph on $n$ vertices is $n\times n$ with all entries $0$ or $1$, and diagonal $0$. My attempt: We know that for such a matrix $|\lambda_0|\leq n-1$. \begin{eqnarray*}
\lambda_{0}^{2} & = & \frac{\lambda_{0}^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\
 & = & \frac{\left|\lambda_{0}\right|^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\
 & \leq & \left|\sum_{1\leq i}\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\
 & \leq & \sum_{1\leq i}\left|\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\
 & \leq & \sum_{1\leq i}\lambda_{i}^{2}\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\
 & = & \sum\lambda_{i}^{2}\left(\frac{n-1}{n}\right)\\
 & = & \frac{2m(n-1)}{n}
\end{eqnarray*} I think everything is ok until the last $\leq$. This would be true if all the eigenvalues were integers, but as someone pointed out, the eigenvalues of these matrices don't have to be integers.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1139723,"Given the differential equation $\frac{dy}{dt}+a(t)y=f(t)$ , show that every solution tends to 0 as t approaches infinity.","Given the differential equation $\frac{dy}{dt}+a(t)y=f(t)$ with a(t) and f(t) continuous for: $-\infty<t<\infty$ $a(t) \ge c>0$ $lim_{t_->0}f(t)=0$ Show that every solution tends to 0 as t approaches infinity. $$\frac{dy}{dt}+a(t)y=f(t)$$ $$μ(t)=e^{\int a(t)dt}$$
$$μ(t)=e^{\frac{1}{2}a^2(t)}$$ Multiplying both sides of the equation by $μ(t)$: $$\frac{dy}{dt}+a(t)y=f(t)$$ 
$$μ(t)[\frac{dy}{dt}+a(t)y]=μ(t)[f(t)]$$ 
$$e^{\frac{1}{2}a^2(t)}[\frac{dy}{dt}+a(t)y]=e^{\frac{1}{2}a^2(t)}[f(t)]$$ 
$$\frac{d}{dt}e^{\frac{1}{2}a^2(t)}y=e^{\frac{1}{2}a^2(t)}[f(t)]$$ 
$$e^{\frac{1}{2}a^2(t)}y=\int e^{\frac{1}{2}a^2(t)}[f(t)]dt$$ Did I make a mistake in the integrating factor step? I am not sure how to proceed..",['ordinary-differential-equations']
1139739,Can one differentiate an infinite sum?,"Suppose we have $|x|<1$, then
$$1+x+x^2+...=(1-x)^{-1}$$
Differentiating both parts we get
$$1+2x+3x^2+...=(1-x)^{-2}$$ If yes, then how one can prove that?
What about integrating both parts?
What if sum diverges?","['sequences-and-series', 'calculus']"
1139740,Prove that $z_n \rightarrow z_0$ if and only if $\bar{z_n} \rightarrow \bar{z_0}$ as $n \rightarrow \infty$.,"In my Complex Analysis course, I'm supposed to prove that.  I'm not really too sure where to start, however.  Any pointers? Thanks. :-)",['complex-analysis']
1139743,Higly axiomatic geometry book recomendation,"Recently I have started dipping my toes in mathematical waters besides calculus,and with varying success I have started learning bit of something about ""everything"". But I have one issue,namely I can not find a satisfactory book regarding geometry. My problem with book choice is two-fold and it will be laid out in few points below. First issue is that I know almost no geometry. Besides calculating areas of basic shapes,and a few very very basic theorems about chords and circles,I am a blank slate. Second issue I have is that I can not handle non-axiomatic arguments,which are laid out in many good books.It is not that I find them bad or anything,I just like precision. So now I will list few points which a book should satisfy,and then I will list a few examples of books which do not satisfy,and why. Firstly book or book series should contain both plane a 3D geometry(or however it is called). Exercises should be abundant(not essential) The more theorems proved in the text,the better. It should start from scratch.Namely from basic axioms, be it Euclidean or Hilbert or any other axiomatization.Then it should proceed from these axioms,and using strictly them,prove theorems.For reference think of Enderton's Set Theory book,first it lays out the axioms and then proceeds to results, given book should proceed in similar manner. Any terms used used should be previously or timely defined It should start from very basic concepts,like lines,circles,angles and etc. and proceed up to higher concepts,whatever they may be. Now for books which do not satisfy and why. First example is Kiselev's Planimetry and Stereometry. These two books are great, but lack of rigor is very frustrating. Hartshorne's Geometry book is great according to reviews but my problem with it is that it assumes knowledge which I do not posses. Same goes for Coxeter. I thank you for your generous help,in advance.","['geometry', 'book-recommendation', 'big-list', 'reference-request', 'soft-question']"
1139789,Is $f(z)=z^n$ holomorphic?,"Is $f(z)=z^n$ holomorphic? I have tested a number of other functions using the Cauchy Riemann equations $u_x=v_y$, $v_x=-u_y$. However in the case of $f(z)=z^n$ I cannot think of a way to find the functions $u(x,y)$ and $v(x,y)$ without using a binomial expansion of $(x+iy)^n$. Any help or pointers is appreciated. edit - the problem requires the use of the Cauchy - Riemann equations and not the formal definition of complex differentiation.",['complex-analysis']
1139817,divisor of a section of the sheaf of logarithmic differentials,"Let $S=\{0, 1, \infty\} \subset \mathbb{P}^1$ and let $\Omega^1_{\mathbb{P}^1}(\log S)$ be the line bundle of logarithmic differentials along $S$. Consider the form
$$
\omega=\frac{dx}{x}+\frac{dx}{x-1} 
$$ which is an element in $H^0(\mathbb{P}^1, \Omega^1_{\mathbb{P}^1}(\log S))$ with residues $1$, $1$ and $-2$ at $0$, $1$ and $\infty$. What is the divisor of $\omega$ regarded as (rational) section of $\Omega^1_{\mathbb{P}^1}(\log S)$? Writing 
$$
\omega=\frac{2x-1}{x(x-1)}dx, 
$$ I would simply say that 
$$
\mathrm{div}(\omega)=[1/2],
$$ since $x=1/2$ is the point where the numerator has a simple zero. But I am a bit confused about the difference between considering $\omega$ as a rational section of $\Omega^1_{\mathbb{P}^1}$ or $\Omega^1_{\mathbb{P}^1}(\log S)$. Is it OK that $\mathrm{div}(\omega)$ has degree 1? Am I just computing the Chern class of $\Omega^1_{\mathbb{P}^1}(\log S)$? Can someone help?","['differential-forms', 'algebraic-geometry']"
1139885,"If a and b are in G and ab=ba, then we say a and b commute. Assuming a and b commute, prove:","If $a$ and $b$ are in a group $G$ and $ab=ba$, show that $xax^{-1}$ commutes with $xbx^{-1}$ for any $x \in G$. So I wrote: WWTS: $\bf{xax^{-1} \times xbx^{-1}=xbx^{-1}\times xax^{-1} }$ Now, the problem I have is I don't know where to start. Let's say if I start with what is given: ab=ba, then am I allow to multiply each side by x and x$^{-1}$ and use the associative law since this is a group. So for example: ab=ba $xabx^{-1}=xbax^{-1}$
and then by associative i can change: $xax^{-1} b=xbx^{-1} a$
and multiply by x and x^-1 on the right $xax^{-1} \times bxx^{-1}=xbx^{-1} \times axx^{-1}$ and use associative again $xax^{-1} \times xbx^{-1}=xbx^{-1}\times xax^{-1}$ Any ideas?","['group-theory', 'abstract-algebra']"
1139893,Can someone explain to expression $(n+1) + 2^k - 2^{k-1}$ was simplified to $(n+1) + 2^{k-1}$?,"Need help to understand how we simplified the following:
$$(n+1) + 2^k - 2^{k-1}$$ was simplified to $$(n+1) + 2^{k-1}$$
I know it is the basic algebra, but I am stuck. Thanks :(",['algebra-precalculus']
1139901,Motivation behind standard deviation?,"Let's take the numbers 0-10.  Their mean is 5, and the individual deviations from 5 are -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5 And so the average (magnitude of) deviation from the mean is $30/11 \approx 2.72$. However, this is not the standard deviation.  The standard deviation is $\sqrt{10} \approx 3.16$. The first mean-deviation is a simpler and by far more intuitive definition of the ""standard-deviation"" , so I'm sure it's the first definition statisticians worked with.  However, for some reason they decided to adopt the second definition instead.  What is the reasoning behind that decision?","['statistics', 'standard-deviation', 'intuition']"
1139971,Prove the inverse of this differentiable function is differentiable? [duplicate],"This question already has an answer here : Alternative proof for differentiability of inverse function? (1 answer) Closed 9 years ago . Suppose we have a differentiable function $ g $ that maps from a real interval $ I $ to the real numbers and suppose $ g'(r)>0$ for all $ r$ in $ I $. 
Then I want to show that $ g^{-1}$ is differentiable on $g(I). $ Intuitively this makes sense but I can't come up with a neat proof. I was thinking to use the mean value theorem but I'm not sure if that would get me anywhere.","['derivatives', 'functions', 'analysis']"
1139990,How is the entire $SO(2)$ group the standard rotation matrix?,"In a book I am using, the following is presented, $$\mathcal{R}(\phi) = \begin{pmatrix} \cos (\phi ) &\sin (\phi ) \\ -\sin (\phi ) &\cos (\phi )\end{pmatrix}$$ The group's name is $\textrm{SO}(2)$ , if the angle $\phi$ varies continuously from $0$ to $2\pi$ ; $\textrm{SO}(2)$ has infinitely many elements and is compact. From my understanding, $\textrm{SO}(2)$ is the group of orthogonal matrices each of whose determinants are $+1$ . I can definitely see how the rotation matrix forms a subgroup, but I don't see how equality (rotation matrix = $\textrm{SO}(2)$ ) can be shown (especially so easily as to sweep it under the rug so quickly in this book). Could you tell me how this can be shown?","['finite-groups', 'group-theory', 'abelian-groups']"
1140014,Simplification of a combination of 6 values of the gamma function,"I'm trying to simplify this combination of gamma functions:
$$\frac{\Gamma\left(\frac{2}{25}\right)\Gamma\left(\frac{7}{25}\right)\Gamma\left(\frac{12}{25}\right)}{\Gamma\left(\frac{2}{5}\right)\Gamma\left(\frac{3}{25}\right)\Gamma\left(\frac{8}{25}\right)}$$
I tried to apply Gauss's multiplication formula , as it was done in this answer , but without any success. It is possible to simplify this expression at all?","['gamma-function', 'special-functions', 'analysis']"
1140024,Rate of convergence of Bayesian posterior,"Suppose a data generating process (DGP) is parameterized by some unknown parameter $\theta_0$, say $P_{\theta_0}$, and we want to estimate the value of $\theta_0$ using Bayesian method. Let $\pi(\theta)$ be the prior over the possible values of $\theta$, $\Theta$. I understand that for almost all priors, if the data observed are iid, then the Bayesian posterior will eventually concentrate on $\theta_0$, as the number of observations ($n$) tends to infinity. (Correct me if I'm wrong.) My question: Are there any results that predict differential rates of convergence of the posterior distribution based on different priors ? For example, suppose $\Theta=\{\theta_0,\theta_1\}$ and the DGP is $P_{\theta_0}$. Consider two priors on $\Theta$: 
$$\pi_1(\theta_0)=0.2,\qquad \pi_1(\theta_1)=0.8$$ and $$\pi_2(\theta_0)=\pi_2(\theta_1)=0.5.$$ Are there any results that says the posterior based on $\pi_2$ converges faster than that based on $\pi_1$? (or the other way around?) Intuitively I would expect the posterior based on $\pi_2$ to have faster convergence, as it is ""closer"" to $\theta_0$. But in my experience, intuition is hardly reliable when it comes to probability theory.","['probability-theory', 'bayesian', 'convergence-divergence', 'reference-request']"
1140071,"If a map between separable Banach spaces has closed graph, does it have a point of continuity?","It is well known that the closed graph theorem does not directly extend to nonlinear maps: even for functions from $\mathbb{R}$ to $\mathbb{R}$ , having closed graph does not imply continuity. But let's consider the following reformulation of the closed graph theorem: If a linear map between Banach spaces has closed graph, it has a point of continuity. This does not essentially change the meaning, since a linear map is either everywhere continuous or everywhere discontinuous. But now there is a better chance of nonlinear generalization. Hence, my question : Suppose that a nonlinear map between separable Banach spaces has closed graph. Does it necessarily have a point of continuity? Nonseparable counterexample Since  the above seems too good to be true,   I tried to find a counterexample. So far, found it only in nonseparable setting. Let $X$ the space of all bounded functions $x:(0,1]\to\mathbb R$ with the supremum norm. Let $(q_n)_{n=1}^\infty$ be an enumeration of the rationals. Define the function $y=F(x)$ separately on each subinterval $(2^{-n},2^{1-n}]$, $n=1,2,\dots$ as $$
y(t) = \begin{cases} 1 \quad &\text{if   $f(2^nt-1)>q_n$} \\ 
0 \quad &\text{if  $f(2^n t-1)\le q_n$}
 \end{cases} 
$$
I claim that the map $F:X\to X$ satisfies $\|F(x_1)-F(x_2)\|=1$ whenever $x_1\ne x_2$.
Hence, it is nowhere continuous and its graph is a discrete (in particular closed) set. Indeed, since $x_1\ne x_2$, there is a point $s\in (0,1]$ and a number $n\in\mathbb N$  such that $q_n$ is strictly between $x_1(s)$ and $x_2(s)$; according to the definition of $F$ this implies that the functions $F(x_1)$ and  $F(x_2)$ take on different values at the point $t=2^{-n}(s+1)$. Finite-dimensional case In finite dimensions, a map with a closed graph is continuous outside of a closed set with empty interior. The proof is not very interesting, so I link to it instead of adding it to the post.","['continuity', 'functional-analysis', 'banach-spaces']"
1140073,How to prove that 2-norm of matrix A is <= infinite norm of matrix A,"Now a bit of a disclaimer, its been two years since I last took a math class, so I have little to no memory of how to construct or go about formulating proofs. My tutor unfortunately is very and excruciatingly slow at figuring them out and this one the tutor cannot seem to solve. What I was taught is to start with ""What do we want?"" and ""What do we know?"" What we want: $||A||_2 \leq \sqrt{n}||A||_\infty$ What we know: $||A||_2^2$ = $\lambda _\max(A^tA)$ $\sqrt{n}||A||_\infty = \sqrt{n} * max_i | \Sigma a_{ij} y_j |$ Where j= $1 \dots n$ . Otherwise the maximum absolute row sum. Thus: $\sqrt{n}^2||A||_\infty^2 = n * max_i | \Sigma a_{ij} y_j |^2$ Where j= $1 \dots n$ . Or max row sum squared times n. I feel like that the square of the max times n is likely going to be bigger than the largest positive eigenvalue of A, but I have no means of showing/knowing this and nothing in the notes indicates how to find the bounds other than maybe Banach-Lemma but I think that's only if A is invertible no? And I don't think that's given. Was a homework question, but I'm way past due anyways and just working on it to understand it/prepare for the first mid term.","['matrices', 'normed-spaces', 'numerical-methods']"
1140099,Group action with two orbits,"Suppose a group $G$ acts faithfully on a set of five elements, inducing two orbits of size $3$ and $2$ respectively.  What group may $G$ be? There is clearly a homomorphism $G \mapsto S_3$ and another $G \mapsto S_2$. $|G|$ cannot be greater than $|S_2 \times S_3| = 12$, the total number of simultaneous permutations, or its action would not be faithful. It seems to me that $D_3=S_3$, $C_6$, and $S_3 \times C_2$ are possibilities.  Is this list correct and exhaustive, and how do I prove it?","['group-actions', 'group-theory', 'proof-verification']"
1140102,"Proof: If $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $","This is a corollary following Rudin's theorem 5.12, which states: Suppose $ f $ is a real differentiable function on $ [a,b]$ and suppose that $f'(a)<\lambda<f'(b).$ Then there is a point $x \in (a,b) $ such that $ f'(x)=\lambda$. The corollary says if $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $. Intuitively this makes sense but I can't come up with a neat proof. I was thinking to do it by contradiction; to suppose $ f' $ has a simple discontinuity. This means the lefthand and righthand limits at $ x\in (a,b) $ exist but there are two cases;either $ f(x+)$ does not equal $ f(x)$ or $f(x-) $ does not equal $f(x) $.
Then I'm not sure where to go from here/how to apply the theorem above.","['continuity', 'derivatives', 'analysis']"
1140111,Probability no two pairs are grouped together twice in a row?,"There is a room with 48 people divided into 16 groups of 3 in round 1. In round 2, the group is again randomly divided into 16 groups. What is the probability that no two groupmates in round one are in the same group again in round two? Progress I thought at first that if you label two arbitrary people A and B then the probability that they will not be in the same group in round two if they were in round one is $1/21$ (from $3/63$ ). From there I thought maybe do $1 - (1/21)*$ the number of possible pairs, but that will yield a negative answer.","['discrete-mathematics', 'probability', 'combinatorics']"
1140134,"If we are given that a list of $n$ numbers has $11,660$ derangements, what is the value of $n$?","The Full Question For the positive integers $1,2,3,\dots n-1,n$, there are $11,660$ where $1,2,3,4,5$ appear in the first five positions. What is the value of $n$? My Work First I considered all derangements of the first $5$, this is just all the derangements of integers $1$ to $5$. Which is $44$. I'll say that $N = 44$ for the rest of the question. If we let $P$ represent all the derangments of the remaining $n-5$ numbers, then we clearly have $N\times P = 11,660 \iff 44P = 11,660 \implies P = \frac{11,660}{44} = 265$. My Problem It seems I have reduced it into a simple algebra exercise. However looking closely we have $P = 265 = (n-5)! - \binom{n-5}{1}[n-6]! + \binom{n-5}{2}[n-7]! - \binom{n-5}{3}[n-8]! + \cdots (-1)^{n-5}[0!]$ and we have to solve for $n$. I can't figure out a way how to solve for $n$ in this equation. Can anyone give me some help?","['permutations', 'inclusion-exclusion', 'combinatorics']"
1140137,"What vectors in the plane z=x are orthogonal to $(1,-1,0)$?","I believe all such vectors should be of the form $(a,b,a)$. Hence, 
$$ (a,b,a) \cdot (1,-1,0) = a - b + 0 = 0 \implies a=b$$
So all the vectors we seek are of the form
$$ (a,a,a)$$
where $a \in \mathbb{R}$. Is this the right approach?","['multivariable-calculus', 'proof-verification']"
1140177,non zero complex numbers are orthogonal iff dot product is zero,"Recall that the dot (scalar) product of two planar vectors $v_1 = (x_1, y_1)$ and $v_2 = (x_2 , y_2)$ is given by $v_1. v_2 = x_1x_2 + y_1y_2$. Exercise: Show that the vectors represented by the (nonzero) complex numbers $z_1$ and $z_2$ are orthogonal if and only if $z_1. z_2 = 0$ Note/Recall: that the dot product of the vectors represented by the complex numbers $z_1$ and $z_2$ is given by $z_1. z_2$ = Re$(\bar {z_1} z_2)$. And orthogonality holds precisely when $z_1 = icz_2$ for some real $c$. Attempt: Suppose that the vectors represented by the (nonzero) complex numbers $z_1$ and $z_2$ are orthogonal. Then $z_1 = icz_2$ for some real $c$. Then  $z_1. z_2 = icz_2(z_2) = ic|z_2|^2$ Converse: Suppose $z_1. z_2 = 0$, then $z_1 . z_2 = (x_1 + iy_1)(x_2 + iy_2) = x_1x_2 + ix_1y_2 + ix_2y_1 - y_1y_2 = x_1x_2 - y_1y_2 + i(x_1y_2 + x_2y_1)$ must be equal to zero? I don't know how to continue. Please can anyone please help me? Anything help/suggestion can help.
Thank you So","['vectors', 'complex-analysis']"
1140178,System of 4 tedious nonlinear equations: $ (a+k)(b+k)(c+k)(d+k) = $ constant for $1 \le k \le 4$,"It is given that
$$(a+1)(b+1)(c+1)(d+1)=15$$$$(a+2)(b+2)(c+2)(d+2)=45$$$$(a+3)(b+3)(c+3)(d+3)=133$$$$(a+4)(b+4)(c+4)(d+4)=339$$ How do I find the value of $(a+5)(b+5)(c+5)(d+5)$. I could think only of opening each expression and then manipulating, and I also thought of integer solutions(none exist). How do I solve it then?","['algebra-precalculus', 'systems-of-equations']"
1140193,Transformation of Extreme Value Distribution,"Let $X$ be a random variable following distribution function (i.e., generalized Pareto distribution) $$
F_{\gamma, \sigma}(x) = 1-\left( 1+\frac{\gamma x}{\sigma} \right)^{-\frac{1}{\gamma}},
$$ where $\sigma>0, \gamma\in\mathbb R$. In addition,$x\in[0, \infty)$ if $\gamma\geq 0$ and $x\in\left[0, -\frac{\sigma}{\gamma}\right]$ if $\gamma<0$. It is claimed that $U:=\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}$ is uniformly distributed on $(0,1)$. To check this claim it is sufficient to show that the distribution function for $U$ is $F_U(u)=u$. $$
F_U(u) = \mathbb P(U\leq u) = \mathbb P \left[\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}\leq u\right] = F_{\gamma, \sigma}\left[ \frac{\sigma}{\gamma}\left( u^{-\gamma}-1 \right) \right] = 1-u.
$$ From the above calculation, it seems that the claim does NOT check out. Is there any explanation for the claim, please? Thank you!","['statistics', 'self-learning', 'probability']"
1140196,Proof of whitney's embedding theorem?,"While learning about the rigorous definition of manifolds, my text mentions that any $n$-dimensional manifold can be embedded in $\Bbb{R}^{2n}$, which is called Whitney's embedding theorem. I have attempted to prove this theorem using the rigorous definition of a manifold, but I am stuck. I have only covered tensors and manifolds in my study of differential geometry, so do I have to know more mathematics to prove this theorem? If not, what is the rigorous proof of Whitney's embedding theorem?","['manifolds', 'differential-geometry']"
1140225,Sum of $\lfloor k^{1/3} \rfloor$,I am faced with the following sum: $$\sum_{k=0}^m \lfloor k^{1/3} \rfloor$$ Where $m$ is a positive integer. I have determined a formula for the last couple of terms such that $\lfloor n^{1/3} \rfloor^3 = \lfloor m^{1/3} \rfloor^3$. For example if the sum is from 0 to 11 I can find the sum of the terms 8 through 11. I am stuck on what formula can be applied to find the sum of the terms before the final stretch however.,"['radicals', 'summation', 'sequences-and-series', 'ceiling-and-floor-functions']"
1140259,Is $\mathbb{R}$ an algebraic extension of some proper subfield?,"Is there a (proper) subfield $K$ of $\mathbb{R}$ such that $\mathbb{R}$ is an algebraic extension of $K$? From this question, Is there a proper subfield $K\subset \mathbb R$ such that $[\mathbb R:K]$ is finite? , it is clear that for such an $K$, $[\mathbb{R}:K]=\infty$. This question seems to be non-trivial, and I suspect any existence result will be non-constructive. Any references will be appreciated.","['abstract-algebra', 'field-theory']"
1140266,1000 numbers on a blackboard,"The numbers $1, 2, …,1000$ are written on a blackboard, in some order. Between every pair of consecutive terms, the absolute difference of the two terms is written between them, and then all the original numbers are erased. (In other words, if the numbers are $a_1, a_2, …, a_n$, they are replaced by the numbers $|a_1−a_2|, |a_2−a_3|, …, |a_{(n−1)}−a_n|$.) This procedure is repeated $999$ times, when there is only one number left on the blackboard. What is the largest possible value of this last number?","['elementary-number-theory', 'algebra-precalculus', 'contest-math']"
1140304,Number of squarefree numbers with three distinct prime factors below $N$,"Is there any way to calculate the number of squarefree numbers with three distinct prime factors below given $N$? I.e. how many numbers below $N$ can be factored to the form
$$
prq
$$ 
where $p$,$r$ and $q$ are distinct prime numbers? Thanks!",['number-theory']
1140351,How to prove $f$ is Lipschitz continuous,"Let $U\subset \mathbb R^N$ be open and convex, and the function $f: U\to \mathbb R$ is differentiable in $U$ . I've got to show that: $f$ is Lipschitz continuous iff $\exists\: M>0$ such that $\|\nabla f(x)\|\leq M$ for all $x\in U$ . I understand that a function is Lipschitz continuous on a subset $E$ of $\mathbb R^N$ if for all $x, y\in E$ , $$|f(x)-f(y)|\le L|x-y|$$ for some $L>0$ . But how do I work with this definition to get to my result? I sense that I would have to use the fact the norm is a convex function and show that $$\|\nabla f(x)\| < \frac{|f(x)-f(y)|}{|x-y|}$$ but I'm not sure if this is the right approach.","['lipschitz-functions', 'derivatives', 'real-analysis']"
1140357,Derivative of an inverse,"Let $f(x)=2x^3+7x−1$, and let $g(x)$ be the inverse of $f(x)$. Then find $g′(191/4)$. I know only one way of doing this. Solving the cubic equation for x and then differentiating it. This is too too long. How to solve it more easily?","['calculus', 'derivatives']"
1140374,Finding the projection matrix of $\mathbb R^3$ onto the plane $x-y-z=0$,"Find the matrix of the projection of $\mathbb{R}^3$ onto the plane $x-y-z = 0.$ I can find a normal unit vector of the plane, which is $\vec{n}=(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}})^T$ And then the vectors $\vec{u}=(1,1,0)^T, \ \vec{v} = (1,0,1)^T$ form a basis of $\mathbb{R}^3$. but why would the solution be $$A = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}?$$","['projection-matrices', 'linear-algebra']"
1140383,Prove that a Lipschitz continuous function is differentiable at a point ${\bf x}_0$,"Consider $f: B({\bf x}_0,r)\to \Bbb R$, that apart from being Lipschitz continuous, has directional derivatives at the point $x_0$ and $\frac{\partial f}{\partial{\bf v}}({\bf x}_0)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}({\bf x}_0)v_i$ for every direction $\bf v$. 
How do I use these facts to show that it is differentiable at $x_0$?
I am pretty clueless about how to go about this problem; the hint given says we need to use contradiction by assuming it is not differentiable and use the Bolzano Weierstrass theorem.","['lipschitz-functions', 'convergence-divergence', 'partial-derivative', 'real-analysis', 'derivatives']"
1140386,Battleship game - logic for positioning ships,"I'm working on a project where I'm programming a battleship game using objected-oriented principles of programming. I got stuck at one problem that is purely mathematical and relates to the positioning of the battleships. I would like to ask you for an equation to help me out. Let get straight to the point: I have converted a A2, G8, I9 notation into a lineta system with ( x+1 + y*10 ) ( x and y starting from 0) so that each mast has its numerical position (from 1 to 100). Lets take the ship in the top-left corner as an example. It occupies positions 1 and 2 on the board. In this case squares that cannot accept any battleships are: 3, 11, 12 (ships cannot touch one another) Problem: What is the equation that would tell me what squares cannot accept any masts given the position of an already set mast? Rules: 1. Ships cannot touch one another,
2. Masts of of a battleship can only be positioned along one another' sides - not diagonally.",['discrete-mathematics']
1140410,"What is the ratio between $|\langle Av,v \rangle |$ and $\|v\|^2$ where $A$ is an $n\times n$ unitary matrix and $v\in \mathbb C^n$?","I'm trying to determine the ratio between $|\langle Av,v \rangle |$ and $\|v\|^2$ where $A$ is an $n\times n$ unitary matrix and $v\in \mathbb C^n$. In particular I'm trying to determine whether $|\langle Av,v \rangle | \le \|v\|^2$ or $|\langle Av,v \rangle | \gt \|v\|^2$. This is what I have so far: Let $w,u\in \mathbb C^n$ such that $w+u=v$ then: Computing $|\langle Av,v \rangle |$ :
$$|\langle Av,v \rangle | =|\langle A(w+u),w+u \rangle |=|\langle Aw+Au,w+u \rangle | = |\langle Aw,w \rangle + \langle Aw,u \rangle + \langle Au,w \rangle + \langle Au,u \rangle |$$ Computing $\|v\|^2$ : $$\|v\|^2 = \langle v,v \rangle = \langle w+u,w+u \rangle = \langle w,w \rangle + \langle w,u \rangle + \langle u,w \rangle + \langle u,u \rangle$$ I'm not quite sure how to continue. I can use the exchange lemma and the fact that $A^{-1} = A^*$ because $A$ is an unitary matrix but I can't see how it helps me to simply the first equation. Note : $\langle \cdot \rangle$ denotes the dot product.","['matrices', 'linear-algebra']"
1140412,Sum $\sum_{n=2}^{\infty} \frac{n^4+3n^2+10n+10}{2^n(n^4+4)}$,I want to evaluate the sum $$\large\sum_{n=2}^{\infty} \frac{n^4+3n^2+10n+10}{2^n(n^4+4)}.$$  I did partial fraction decomposition to get $$\frac{1}{2^n}\left(\frac{-1}{n^2+2n+2}+\frac{4}{n^2-2n+2}+1\right)$$ I am absolutely stuck after this.,['sequences-and-series']
1140420,$T$ is diagonalizable if $T^n$ is identity for some $n$,"Suppose $T$ is a linear operator on a $\mathbb{C}$-vector space $V$. Further, assume $T^n$ is the identity operator, for some $n$. Then, $T$ is diagonalizable. I think there is a proof using Jordan theory, but I wish to find one without using it.",['linear-algebra']
1140427,"Rank, nuclear and Frobenius norms of a matrix","The nuclear norm, denoted $\| \cdot \|_*$ is a good surrogate for the rank when minimizing problems like $$\label{pb1}\tag{1} \min_X \operatorname{rank} (X) : AX = B $$ Here, we're trying to find a low-rank matrix $X$ such that $AX=B$ .
If I recall correctly, when the singular values of $X$ are bounded above by $1$ , one can replace $\operatorname{rank}$ by $\| \cdot \|_*$ in the problem \eqref{pb1}. What about the Frobenius norm? Can the Frobenius norm be a good surrogate to the nuclear norm? Under which assumptions? Since we have $$\|X\|_* = \min_{X=UV^\top} \|U\|_F\|V\|_F$$ in particular we have $\|XX^\top\|_* = \|X\|_F^2$ . Then, because $\|X\|_1\le 1$ , we also have $\|XX^\top\|_1\le 1$ . So, this is pretty direct, no? Is the Frobenius norm also a good surrogate?","['optimization', 'matrix-rank', 'matrices', 'normed-spaces', 'nuclear-norm']"
1140438,If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$,"Prove that if $A$ is a square matrix and $\mathrm{Tr}(A)=0$ , then there exists an invertible matrix $R$ such that the matrix $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ . It seems like the formula $A=S\Lambda S^{-1}$ , but maybe it does not help. Thanks so much.","['trace', 'matrices', 'linear-algebra', 'matrix-decomposition']"
1140447,Jacobian Matrix of 6DOF Body (with IMU),"I am trying to derive the analytical Jacobian for a system that is essentially the equations of motion of a body (6 degrees of freedom) with gyro and accelerometer measurements.
This is part of an Extended Kalman Filter. The system state is given by:
$
\mathbf{x} = \left(
\begin{array}{c}
\mathbf{q}\\
\mathbf{b_\omega}\\
\mathbf{v}\\
\mathbf{b_a}\\
\mathbf{p}\\
\end{array}
\right)
$ where $q$ is the quaternion orientation of the body expressed in the global frame, $b_\omega$ and $b_a$ are the biases in the gyro and accelerometer respectively (expressed in the body frame) and $v$ and $p$ are the velocity and position of the body expressed in the global frame. All vectors are [3x1] except $q$ which is [4x1] in $[w,x,y,z]^\top$ format, and $R$ (below) which is [3x3]. The equations of motion $\frac{dx}{dt}=\dot{x}$ (t is time) are:
$$
\dot{\mathbf{q}} = \frac{1}{2}\mathbf{q} \otimes 
\left(
\begin{array}{c}
0\\
\hat{\omega}\\
\end{array}
\right) \\
\dot{\mathbf{b_\omega}} = 0 \\
\dot{\mathbf{v}} = R^\top (\hat{\mathbf{a}} + [\hat{\mathbf{\omega}}\times]R \mathbf{v})+ g \\
\dot{\mathbf{b_a}} = 0 \\
\dot{\mathbf{p}} = \mathbf{v}
$$
Second-order terms are ignored. $\hat{a} = a - b_a$ and $\hat{\omega} = \omega - b_\omega$ are the corrected accelerometer and gyro biases ($a$ and $\omega$ are known) and are expressed in the body frame. $R$ is the rotation matrix (DCM) formed from $q$ and $g$ is the gravity vector $[0,0,9.81]^\top$. These equations have been validated against an aerospace engineering software library. I need the jacobian $F = \frac{d\dot{x}}{dx}$ but I cannot find this jacobian in any texts (I do find the error-state jacobian eg this paper ).
I am struggling with doing this myself because I don't know how to handle the quaternion norm constraints. I also am concerned about the validity of a solution given through numerical differentiation. Any help or explanation would be greatly appreciated. This is going towards an open-source robot localisation project.","['nonlinear-system', 'control-theory', 'quaternions', 'partial-derivative', 'derivatives']"
1140462,Why is $\pi_r(L)$ a linear transformation into $\Lambda^r(V)$,"I'm reading Linear Algebra by Hoffman and Kunze, and there's a proof that just doesn't seem correct. He wants to prove that $\pi_r$ is a linear transformation from $M^r(V)$ into $\Lambda^r(V)$ . In the proof, he establishes that if $\tau$ is a permutation of $\{1,2,\ldots,r\}$ then $(\pi_rL)(\alpha_{\tau1},\ldots,\alpha_{\tau r})=(\text{sgn }\tau)(\pi_rL)(\alpha_1,\ldots,\alpha_r)$ . He then concludes that $\pi_rL$ is an alternating form. Now, if $L$ is an alternating form then $L_\tau=(\text{sgn }\tau)L$ . However, the converse isn't necessarily true, is it? Can someone please explain to me what I didn't understand? It's on page 170, if that would be of any help. Thanks in advance.","['linear-algebra', 'multilinear-algebra']"
1140503,Nested radicals of 1000 square roots,"Find integer solutions to the equation
$$\underbrace{\sqrt{x+\sqrt{x+\sqrt{x+\cdots+\sqrt{x+\sqrt{x}}}}}}_{1000}=y$$
The first thing that popped to my mind was the infinitely nested radical and the recurrence relation $$a_{n+1}=\sqrt{x+a_n}$$
That lead me nowhere so I tried to show there aren't any such $(x,y)$ except for the trivial $(0,0)$ I think I proved that $$\sqrt{a_0}\leq a_{n}\leq \sqrt{a_0}+1$$ but the proof itself was pretty vague and even if it is correct doesn't help me.I have no idea what else to do,any hints would be appreciated.","['nested-radicals', 'algebra-precalculus']"
1140507,Is there a geometric interpretation of the product integral?,"Riemann's ""way to the Integral"" is loosely speaking the limit of sums of this kind
\begin{equation}
\sum_if(x_i)\Delta x_i
\end{equation}
Now, if we replace the sum with a product and the multiplication by $\Delta x_i$ with exponentiation, we are led to the idea of a ""product"" integral:
\begin{equation}
\prod_if(x_i)^{\Delta x_i}
\end{equation}
The relation with the usual integral and the ""product"" integral is the following:
\begin{equation}
\prod f(x)^{dx}=\ e^{\int \ln f(x) dx}
\end{equation}
My question is: is there a geometrical (or measure-theoretical) interpretation of the product integral? As we all know the usual integral (in one variable) is the signed area under the graph of $f$.","['products', 'calculus', 'integration']"
1140536,Problem with differentiation under integral sign,"Original problem: I have a problem in which i need to evaluate the integral:
$$
\int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\,
$$
I have tried to evaluate it taking the $\alpha$ derivative, here i give you all the steps i have done:
$$
\int_1^\infty \dfrac{\sqrt{r^2-1}e^{-\alpha r}}{r} dr\,=-\int d\alpha \int_1^\infty\sqrt{r^2-1}e^{-\alpha r}=-\int d\alpha \dfrac{K_1(\alpha)}{\alpha}\\ =-\frac{1}{4} G^{2,1}_{0,1}\left(\frac{\alpha}{2},\frac{1}{2} \middle|
\begin{array}{c}
 1 \\
 -1/2,1/2,0 \\
\end{array}
\right)
$$
The two last steps in the last equation were obtained using Mathematica, and have been checked numerically, however i have calculated numerically the first integral for a few different values for $\alpha$ and it doesn't match the values the MeijerG function gives. So now i think i have two possibilities: -There is something wrong in the first step, the differentiation under integral sign doesn't work and i don't know why this could be the case. -It's a numerical problem. Mathematica could be giving me wrong values of the integral, but i think this can not be because i have been changing the precision and the method for the numerical integral, obtaining the same results. I want to know what is happening, i have tried similar examples with the same method and i have obtained the good results. Of course, if you have another way of evaluating this integral, don't hesitate, i would like to know it, but also i would like to know what is wrong here. EDIT: Comparison between numerical solution vs MeijerG solution (using differentiation under integral sign) as $\alpha$ functions: EDIT 2: I have found that the difference between the numeric integral and the analytic solution obtained using differentiation under integral sign is exactly $\pi/2$ for all $\alpha$. Actually I have found that for an integral of the form: $$
\int_z^\infty \dfrac{(r^2-z)^c e^{-\alpha r}}{r} dr\,
$$
Being z a positive number and $0<c<1$.
The differentiation under integral sign method gives the result up to a function f(z,c). For example: -For z=1 and c=1/2, this function is equal to $\pi/2$ -For z=1 and c=1/3 this function is equal to $\pi/\sqrt{3}$ -For c=1/2 this function is exactly $f(z,1/2)=-1+(1+\frac{\pi}{2})z$ With this i can continue with my calculations but,
does anyone know why does this happen? Do you know a way for obtaining this function f(z,c) analytically?","['calculus', 'integration', 'derivatives']"
1140541,Showing limit of improper integral using Riemann sum,"I can show that if $f$ is continuous for $x\ge0$ with $\lim\limits_{x \to \infty} f(x) = A$, then
  \begin{equation}
    \lim_{x \to \infty} \frac{1}{x} \int_0^x \! f(t) \, \mathrm{d}t = A
  \end{equation}
using the definition of limits, etc. (a homework question). However, just for my understanding I'm trying to wrap my head around showing the same thing using a Riemann sum. If I divide the interval of the integral in $n$ parts, each part will be $\frac{x}{n}$ wide. In each such subinterval, I pick the right endpoint: $c_i = \frac{x}{n} \cdot i$ for $i = 1,2,3,\dots,n$. \begin{align}
  \lim_{x \to \infty} \frac{1}{x} \int_0^x \! f(t) \, \mathrm{d}t & = A \\
  \lim_{x \to \infty} \frac{1}{x} \sum_{i=1}^n f(c_i) \frac{x}{n} &= A \\
  \lim_{x \to \infty} \frac{1}{x} \sum_{i=1}^n \frac{f(c_i)}{n} x &= A
\end{align} The $x$ does not depend on $i$ so I can move that out of the summation. \begin{align}
  \lim_{x \to \infty} \frac{1}{x} x \sum_{i=1}^n \frac{f(c_i)}{n} &= A \\
  \lim_{x \to \infty} 1 \sum_{i=1}^n \frac{f(c_i)}{n} &= A
\end{align} I can see that each $i$ provides an $n$:th contribution to the sum and there's $n$ parts altogether. I can make calculations and see that the sum does converge using various functions for $f$ but I can't quite see why the sum of $f(c_i) = f(\frac{x}{n} \cdot i)$ behaves the way it does when $x\to\infty$ and $n\to\infty$. I hope that was clear enough (first question here! :)","['riemann-sum', 'integration', 'limits']"
1140545,"Proving $\arctan x > \frac x{1+x^2}, \forall x >0$ with a helper function","Prove $\arctan x > \frac x{1+x^2}, \forall x >0$ There's the approach using Lagrange's, but is it also possible to define a function like so? $f(x)=\arctan x - \frac x{1+x^2}$, take the derivative: $f'(x)=\frac {2 x^2} {(1+x^2)^2}$, if $f'(x)=0$ then $x=0$. From the second derivative test we get that $f''(0)=0$ so $x=0$ isn't an extrema and from Rolle's we know that there's at most only one solution for $f(x)$, we know that the only solution is at $x=0$ so we can conclude that $f(x)>0, \forall x>0$. Is this a valid way?","['inequality', 'calculus', 'proof-verification', 'functions']"
1140556,Reconstructing Lie group globally from the exponential map,"This should be an elementary question in Lie group theory, although I'm pretty sure I'm mixing up concepts. Any help clarifying would be much appreciated. Set up Let $G$ be finite-dimensional real Lie group, and take it, for the sake of simplicity, to be connected . There is a three-fold correspondence between: tangent space at the identity element : $T_eG$, left-invariant vector fields : $\mathfrak{g}$, one-parameter subgroups of $G$ : $\text{Hom}_{\text{LieGr}}(\mathbb{R},G)$, which are interchangeably called the Lie algebra of $G$. The exponential map of $G$ is defined as $$
\exp_G:\mathfrak{g}\to G\,,\quad X\mapsto \exp_G(X):=\phi_X(1)\,,
$$ where $\phi_X$ is the one-parameter subgroup generated by the (left-)invariant vector field corresponding to the tangent vector $X\in T_eG$. In particular, we have for all $t\in\mathbb{R}$ that $\phi_X(t)=\exp_G(tX)$ and therefore the exponential map sends the one-dimensional vector subspace through $X$ to the whole one-parameter subgroup generated by $X$. (a) The map $\exp_G$ is just a local diffeomorphism. Nevertheless, its image $\exp_G(\mathfrak{g})$ is a (connected) neighbourhood of the identity. Hence, the subgroup $\langle \exp_G(\mathfrak{g})\rangle$ algebraically generated by this image is the whole Lie group $G$ back again (since $G$ is assumed connected ) Prop A4.25 . (b) On the other hand, $\exp_G$ is but a local diffeomorphism and therefore shouldn't be able to retrieve global information about $G$. That is, Lie groups with isomorphic Lie algebras are not necessarily isomorphic, and the issue here is the usual simply-connectedness . However, it seems that in (a) we are perfectly able to distinguish the different Lie groups with a given Lie algebra.
$$
$$ Question(s) Where is the topological information coming from in the generating process in (a)? Or, where is it lost in (b)? Or, if the questions don't make sense, where is the confusion? $$
$$ My thoughts is that in (a) we are really generating the Lie group $G$ by the image of one-parameter subgroups $\text{Hom}(\mathbb{R},G)$, which already contain enough topological information about $G$. In particular, by simple Yoneda if $\text{Hom}(\mathbb{R},G)\cong \text{Hom}(\mathbb{R},H)$ then $G\cong H$. [See comment.] However, the Hom is equally well isomorphic to the tangent space at the identity $T_eG$ and therefore it seems that we can always reproduce the whole group by this small bit of simply `vectorial' data. In (b) , when we worry about non-isomorphic Lie groups with isomorphic Lie algebras we are really completely forgetting about their underlying exponential maps, and just keeping the local diffeomorphism property. The Lie algebras are then purely algebraic objects with no a priori attached (global) Lie groups. The `corresponding' local Lie groups may be constructed via the BCH formula, which is defined entirely in terms of the Lie algebra structure. Therefore, in (a) saying we reconstruct the whole Lie group from the exponential map may be seen as a tautology because the exponential map itself is defined globally with respect to a given Lie group. And, although Lie algebras may be isomorphic they are not the same , and the exponential map distinguishes just this. $$
$$
Please, (re)tag more appropriately and/or edit the question if need be.","['lie-algebras', 'lie-groups', 'differential-geometry']"
1140581,How many non-increasing sequences are there over the natural numbers?,"How many non-increasing sequences are there over the natural numbers? By splitting it to categories, I sort of got it has to be $\aleph_0$. Nevertheless, I haven't seen such a question and therefore I don't know if I am even correct in my result. I would appreciate your help.","['elementary-set-theory', 'combinatorics']"
1140589,A question about the integral form of the Cauchy-Schwarz inequality.,"I'm confused about the following form of the Cauchy-Schwarz inequality: $$\int{f(x)g(x)} dx\leq \sqrt{\int{f(x)^2dx}}\sqrt{\int{g(x)^2dx}} \tag{A}$$ An analogous form for the inequality is $(ac+bd)\leq (a^2+b^2)(c^2+d^2)$. Assuming that the integral inequality above is the same in spirit as this, we should have something like $$\begin{align}f(x_1)g(x_1)&+f(x_2)g(x_2)+\dots f(x_n)g(x_n)\\&\leq \sqrt{f(x_1)^2+f(x_2)^2+\dots+f(x_n)^2}+\sqrt{g(x_1)^2+g(x_2)^2+\dots+g(x_n)^2}\end{align}$$ Extending the argument to an infinite number of $x_i$'s, we still do not get A. This is because it is not like if we had an infinite number of $x_i$'s on the interval $[a,b]$, we'd have $$f(x_1)+f(x_2)+\dots=\int_a^b{f(x)dx}$$ By my understanding, $\int{f(x)dx}$  is not the summation of $f(x_i)$. It is the summation of $\lim\limits_{\Delta x\to\infty}f(x_i)\Delta x$. Where am I going wrong?","['real-analysis', 'analysis']"
1140657,"Prove that there is no $(x,n,m)$ such that $f^n(x)=f^{n+km}(x)$ if $f$ is injective","Assume that $f : X\mapsto  X$ is injective. And, we write $f\circ f\circ\dots\circ f(x)=f^n(x)$ for $n$ times composition. Prove that there is no $x\in X$ and no $n,m\in \mathbb{N}$ such that the following equality holds: $f^n(x)=f^{n+m}(x)=f^{n+2m}(x)=\dots$ It is an impenetrable problem for me, even don't know how to start. Of cousre, it's easy to show that if $f : X\mapsto  X$ is injective, then $f^n : X\mapsto X$ is injective for each $n\in \mathbb{N}$ ; but does it helps at all? Thank you in advance. EDIT: The original question: Assume that $f : X \to X$ is injective. a. Prove that $f^n : X \to X$ is injective for each $n \in \mathbb{Z}_+$ . b. Prove that $f$ has no eventual periodic points. From the book: C. Adams, R. Franzosa, Introduction to Topology: Pure and Applied . Also from the book: We say that $x$ is an eventual periodic point if $x$ is not a periodic
point but $f^n(x)$ is a periodic point for some $n\in \mathbb{Z_+}$ .","['general-topology', 'functions', 'real-analysis']"
1140668,Infinity as an element,"Can infinity be an element of a set? For example: {$1,2, \infty,4$} I understand $\infty$ is not a real number",['elementary-set-theory']
1140683,Terence Tao Exercise 5.4.3: Integer part of $x$ proof.,"I am reading Terence Tao: Analysis 1. 
As you may be aware, certain objects are introduced bit by bit, so if i am not 'allowed' to use something yet, please understand. Show that for every real number $x$ there is exactly one integer $N$ such that $ N \leq x < N + 1$ If $x = 0$ then we can take $N = 0$. 
If $x > 0$ then $x$ is the formal limit of some Cauchy sequence $(a_n)$, of which is positively bounded away from zero. Since $(a_n)$ is Cauchy it is bounded by some rational $M$, this implies that $x \leq M$, if we take $M < N + 1$ for some integer $N$ then $x < N + 1$. I seem to think this part is fine, but thinking is dangerous. I now need to show that $ N \leq x $.
I know that the sequence $(a_n)$ is bounded away from zero, so every term of the sequence $a_n > c$ for some rational $c$ But i don't think this gets me any further. Surely i could use this as i know x is the formal limit of the sequence of rationals $(a_n)$ but i cant quite make the connection. EDIT: I know i have to prove that this is true when x < 0, but i have not completed from x > 0. I know i can use Cauchy sequences, division algorithm... its pretty hard for me to say what i can't use because I am an undergraduate. I'm sorry if that makes it impossible to answer for some, it's very awkward i must admit. What i can tell you is the next chapter is on the least upper bound property.",['analysis']
1140689,Proof for a differential equation,"The problem is as follows: Show that the general solution to the differential equation 
\begin{equation}
x \frac{dy}{dx} = y\ln(x) 
\end{equation}
is  $y = Cx^{\ln\sqrt{x}}$ I thought there was a typo, but it is not, since if we differentiate the second part we can get to the first one.","['ordinary-differential-equations', 'calculus', 'analysis']"
1140697,Determine whether $712! + 1$ is a prime number or not,"Let $n = 712! + 1$ If $n$ was a prime number then, by Wilson's theorem: $ (712!)! \equiv -1 \pmod{712}$ The double factorial makes it seriously more difficult... But We can require: $$712!! + 1 \equiv 0 \pmod{712}$$ $$712 = 2\cdot 356$$ Hence: $712!! = 2^{356} 356!$ Lets compute: $356! \pmod{712}$. Obviously: $$356! = (356 \cdot 2) \cdot (355!/2)  = 712 \cdot 355!/2$$ Then: $356! \pmod{712} \equiv 0$. Now, $2^{356} \pmod{712}$ $2^{356} = 2^{6 + 350}$ Nevermind, what should I do?","['elementary-number-theory', 'contest-math', 'number-theory']"
1140714,Question on tangent spaces,"In this question, if I also had that $f$ were a diffeomorphism and $f^k = I$ for some positive integer $k$ would it make a difference to the answer being in the negative? Here, by $f^k$ I mean $f$ composed with itself $k$ time. Thanks!","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
1140717,Ambiguous matrix representation of imaginary unit?,"This question is triggered by another one: What does it mean to represent a number in term of a 2x2 matrix? Suppose we seek a $2\times2$ matrix equivalent of
the imaginary unit, that is a matrix $\,i\,$ such that $\,i^2 = -1\,$ , or,
with $\;a,b,c,d \in \mathbb{R}$ :
$$
i^2 = \begin{bmatrix} a&b\\c&d \end{bmatrix} 
\begin{bmatrix} a&b\\c&d \end{bmatrix} =
\begin{bmatrix} a^2+bc&ab+bd\\ac+cd&bc+d^2 \end{bmatrix} =
- \begin{bmatrix} 1&0\\0&1 \end{bmatrix}
$$
Leading to the following equations:
$$
a^2+bc=-1 \quad ; \quad b(a+d)=0 \quad ; \quad c(a+d)=0 \quad ; \quad bc+d^2=-1
$$
Subtracting the first from the last one gives two possible solutions:
$$
a^2-d^2=0 \quad \Longrightarrow \quad a = \pm\, d
$$
The solution $\,a=d\ne0\,$ leads to $\,b=0\,$ and $\,c=0\,$ , giving $\,a^2 = -1$ ,
which is impossible in the reals. The other possibility is $\,d = -a$ :
$$
i = \begin{bmatrix} a&b\\c&-a \end{bmatrix}
\quad \mbox{with} \quad a^2+bc = -1 \quad \mbox{or} \quad
\begin{vmatrix} a&b\\c&-a \end{vmatrix} = 1
$$
But here I'm stuck. IMO it does not follow that $\,i\,$ is
a special case of the above matrix, namely the one with
$\,a=0\,$ and $\,b=-1$ , $c=1$ :
$$
i = \begin{bmatrix} 0&-1\\1&0 \end{bmatrix}
$$
Am I missing something obvious? Note. In a comment by Meelo it is remarked that <quote>One can notice that this representation is not unique, though one must always treat $1$ as the matrix
$$ \pmatrix{1&&0\\0&&1} $$ any matrix with characteristic polynomial $x^2+1$ suffices to represent $i$ . For instance, you could use
$$\pmatrix{1&&-2\\1&&-1}$$ for $i$.</quote> That's right, because
$$
\begin{vmatrix}a-\lambda&b\\c&-a-\lambda\end{vmatrix}=(a-\lambda)(-a-\lambda)-bc=\lambda^2+1
$$
I don't see, however, how this can be matched with my answer at the same place:
$$
e^{\begin{bmatrix} 1 & -2 \\ 1 & -1 \end{bmatrix}\theta} = \mbox{?} =
\begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix}
$$","['matrices', 'complex-numbers']"
1140731,Solve: $ab+bc+ca\mid (a+b+c)^2$,"I couldn't make any progress on this problem, can anyone help? I found it's the same as:
Find all integers $a,b,c$ such that $ab+bc+ca$ divides $a^2+b^2+c^2$. I found a solution $a=-b=1$, and $c$ any integer. Any more solutions?","['elementary-number-theory', 'divisibility', 'vieta-jumping', 'number-theory']"
1140765,If $\lambda^n $ is an eigenvalue of $A^n$ then $\lambda $ is an eigenvalue of $A$?,"I'm trying to figure out whether the following statement is correct: If $\lambda^n $ is an eigenvalue of $A^n$ associated to a certain
  eigenvector $v$ then $\lambda $ is an eigenvalue of $A$ associated to
  the same eigenvector $v$. I know that the opposite statement is correct, that is, if $\lambda $ is an eigenvalue of $A$ associated to a certain eigenvector $v$ then $\lambda^n $ is an eigenvalue of $A^n$ associated to the same eigenvector $v$. I tried to guess some matrices to find out a counter example but I couldn't find anything. It seems that if we take $A$ to be diagonal matrix then the statement is always correct. In particular if we take $A$ such that $A^n$=$\lambda^n I $ then it must be that $A=\lambda I$.","['matrices', 'linear-algebra']"
1140803,How do you interpret probabilities?,"I am a college student, and i have been doin probabilities since 4 years and I always had one question which puzzled me.... Its like this : If i were told to  find the probability of getting a head in a single toss of a coin ,the answer is pretty easy right? Its 1/2.
But i cant  interpret this answer logically. like does it mean  1 out of every 2 two tosses of the coin would be a head ( which is so obvoiusly false ) or does it mean that my chances of getting a heads is like 'half'. What im tryin to say is i really cant find any real life significance  of the concept of probability.  Can anyone help me out, please? P.S.: THNX PEOPLE, ithink i have a fairly good idea now, mainly from the frequentist interpretation",['probability']
1140805,Proof using Monotone Class Theorem,"As you know I have been grappling with this question since days ago, which I copy down here for convenience: Let $X$ be set of $\mathbb R$ , and let $\mathcal B$ be its Borel $\sigma$ -algebra, and finally let $\mu_1$ and $\mu_2$ be the two measures on $(X,\mathcal B)$ such that $\mu_1((a,b))= \mu_2((a,b)) < \infty$ whenever $−\infty < a < b < \infty$ . Show that $\mu_1(A) = \mu_2(A)$ whenever $A \in \mathcal B$ .​ As you know again that this question comes from early chapters of Richard F. Bass's introductory book , therefore any solution should involve no advanced theorems such as Dynkin's. The consensus I got so far is to use the Monotone Class Theorem: Suppose $\mathscr A_0$ is an algebra, $\mathscr A$ is the smallest $\sigma$ -algebra containing $\mathscr A_0$ , and $\mathscr M$ is the smallest monotone class containing $\mathscr A_0$ , then $\mathscr A = \mathscr M$ . Here are my three questions: (1) My proof idea (naive, perhaps) is to show that, $$\begin{align}
\text{if } A \in \mathcal B &\text{ then } A \in \mathscr A \\
\text{if } (a, b) \in \mathbb R^1 &\text{ then } (a, b) \in \mathscr M. \\
\end{align}$$ Since by the Monotone Theorem $\mathscr A = \mathscr M$ , therefore I can arrive at the result. Is this idea so flawed that it is dead on arrival? (Who knows, since breakthroughs sometimes happen when someone asks a really dumb question. :-) ) (2) If my idea is valid, how do you prove that if $A \in \mathcal B \rightarrow A \in \mathscr A$ ? I mean the mechanic of going from $A \in \mathcal B$ to $ A \in \mathscr A$ ? (3) And finally, what is the mechanics of proving that if $(a, b) \in \mathbb R^1 \rightarrow (a, b) \in \mathscr M$ ? Thank you for your time. POST SCRIPT! POST SCRIPT! : I finally came up with solution without any advanced theorems, even the Monotone Theorem, adapted from a solution by @JoshKeneda, who had used Dynkin's Theorem. My solution is posted here at the answer page down below. I have submitted this work to my professor, he ok'd it except for (5) because it is true only when the $A_i$ 's are pairwise disjoint. Feel free to drop me a message if you have ideas to improve (5). Thanks to all and especially to @JoshKeneda.","['monotone-class-theorem', 'measure-theory', 'real-analysis', 'analysis']"
1140829,How to solve $4x^3-3z^2=y^6$ in positive integers?,"Solve in positive integers $$4x^3-3z^2=y^6$$ We are given that $\gcd (x,y) = \gcd (y,z) = \gcd  (x,z) = \gcd (x,y,z) = 1$. I do not have the slightest idea how to even begin this question. Seeing the L.H.S. , all I can think is about the cosine triple angle formula but I don't think that'll help here ☺ Any help will be appreciated. Thanks.","['elementary-number-theory', 'algebra-precalculus', 'diophantine-equations', 'polynomials']"
1140831,"Hypermatrices, hyperdeterminants and Grassmannians.","Let $Gr(k,n)$ the Grassmannian manifold of the $k$-planes in $\mathbb{C}^n$ and consider the Plucker embedding $\pi: Gr(k,n) \to \mathbb{P}(\Lambda^k \mathbb{C}^n)$. Let $A$ be the set of $n \times n$ skew-symmetric matrices and fix a natural number $s=2u$. In this way we can define the determinantal variety $Y_s:=\{\phi \in A \,|\, rank(\phi) \le s\}$.  It is very easy to verify that there cone over $Gr(2,n)$, say $\Gamma(Gr(2,n))$ is isomorphic to the determinantal variety defined by the vanishing of $4 \times 4$ pfaffians. It is also well know that the ideal defining Grassmannian is given by quadratic relations: the Plucker relations, so I think that it is impossible to give an isomorphism between $\Gamma(Gr(k,n))$ and $Y_s$ is $k \ne2$. Now, my questions are: 
1) Is it possible to give a notion of skew-symmetric hypermatrix? 2) Is it possible to give a notion of (skew-symmetric) hyperdeterminantal variety? (In some sense...) 3) Is it possible to realize the cone over Grassmannian ($\Gamma(Gr(k,n))$) as a hyperdeterminantal variety? My question is motivated by the fact that, roughly speaking, I need more indexes to fit into a matrix the Plucker relations. I'm sorry if my question is not very precise, thank you for the help!","['pfaffian', 'algebraic-geometry', 'grassmannian', 'determinant']"
1140833,"If $A$ is positive definite, $B$ is symmetric and $BAB = B$, how to show that $A^{1/2}BA^{1/2}$ is a diagonal matrix?","If $A,B$ are symmetric $m$-by-$m$ matrices with $A$ positive definite, I want to show that $A^{1/2}BA^{1/2}$ is a diagonal matrix if it is given that $BAB=B$. Here $A^{1/2}$ is the positive definite square root of $A$. I have been stuck on this problem for a long time. I have tried the decomposition of $A,B$ into the products of orthogonal matrices and diagonal matrices, but maybe the correct path has not hit me. I have not yet arrived at any notable result in trying to prove this. All my attempts have failed. I would really appreciate some help.","['matrices', 'linear-algebra', 'orthogonality']"
1140865,Left orderable Group has infinite order,"An $order$ on a set $S$ is an (anti-symmetric) relation $<$ on $G$ so that for each $a,b\in G$ exactly one of the following is true: $a<b, b<a$ or $a=b$.
A group $G$ is called left orderable if there is an order $<$
on $G$ so that whenever $a,b,g\in G$ with $a<b$, $ga<gb$. Show that
every element of a left orderable group has infinite order. Solution Attempt 
I think that because whenever $a < b$ then for whatever $g$ I choose than $ga$ is never the identity $e$ so does this mean because it never is equal to $e$ that the order is infinite?","['ordered-groups', 'group-theory']"
1140866,Using Lindeberg’s Condition together with the Central Limit Theorem,"I have the following problem: Problem. Let $ (X_{n})_{n \in \mathbb{N}} $ be a sequence of independent random variables such that
  $$
  \mathbf{Pr} \! \left( X_{n} = \sqrt{n} + 1 \right)
= \mathbf{Pr} \! \left( X_{n} = - \sqrt{n} - 1 \right)
= \frac{1}{2 (\sqrt{n} + 1)^{2}}
$$
  and
  $$
\mathbf{Pr} \! \left( X_{n} = 0 \right) = 1 - \frac{1}{(\sqrt{n} + 1)^{2}}.
$$
  Then prove that the random variable $ \dfrac{\sum_{i = 1}^{n} X_{i}}{\sqrt{n}} $ converges in distribution to one with a standard normal distribution. I have to use Lindeberg’s Condition together with the Central Limit Theorem in order to solve this, but I’m stuck. Any assistance would be greatly appreciated. Thanks!","['statistics', 'probability-limit-theorems', 'statistical-inference', 'probability-theory', 'probability']"
1140878,Is this map uniformly continuous? continuous?,"Let $s$ denote the metric space of all sequences of complex numbers with the metric 
$$d(x,y) \colon= \sum_{j=1}^\infty \frac{1}{2^j} \frac{\vert \xi_j(x) - \xi_j(y)\vert}{1+\vert \xi_j(x) - \xi_j(y) \vert} $$ 
for all $x, y \in s$. Here $\xi_j(x)$ denotes the $j$-th term of $x$. Let $k$ be a fixed natural number. Then is the map $T_k \colon s \to \mathbb{C}$ defined by 
$$T_k x \colon= \xi_k(x) \qquad \mbox{ for all } x \in s$$ 
uniformly continuous? continuous?","['continuity', 'real-analysis', 'analysis', 'metric-spaces', 'functional-analysis']"
1140898,"How many numbers from $1$ to $2^n$ will have $``11""$ as substring in binary representation?","For example say, $n = 2$. So our set is $\{1, 2, 3, 4\}$ in base $10$ and  $\{1, 10, 11, 100\}$ in base $2$. So Output $1$, because only one number i.e. $3$ is there such that it has $``11""$ in it. For $n = 3$, we get $\{3, 6, 7\}$ as having $``11""$, so output $3$.","['binary', 'combinatorics']"
1140927,Is this graph and its spectrum understood?,"Consider the graph whose vertices are labelled by the binary representation of the integers from $0$ to $2^{d}-1$ for some $d \in \mathbb{N}$. So its a graph with $2^d$ vertices. An edge exists between two such vertices if the two binary strings differ in at most 2 bits. (for example for $d=2$ this is $K_4$. for $d=3$ this is the cube with all the face diagonals) Is this graph and its spectrum known? 
If something is known about this graph's spectrum then cans someone kindly give the references?","['graph-theory', 'spectral-theory', 'spectral-graph-theory', 'combinatorics', 'algebraic-graph-theory']"
1140948,Expected number of trials until first success,"I am trying to calculate the expected number of attempts to obtain a character in a game. The way the game works is there is a certain probability in order to capture the character. Given that you capture the character, there is now another probability that you will actually obtain (aka Recruit) the character. If the recruit fails, the probability to recruit increases by a certain amount. For example: There is a 25% chance to capture CharX. Given CharX is captured, there is now a 10% chance to recruit CharX. If not recruited, the chance to recruit on the next try jumps to 15% instead of 10%. I can calculate the probability of recruiting based on one trial, but am not able to calculate the number of overall attempts expected because of the increasing probability on each trial. Can someone please help? Thanks. EDIT: if not clear, on each trial you have to successfully capture AND recruit in order to obtain the character. code: from random import randint

repetitions = 10000
trials = 0
caps = 0
recs = 0

for i in range(0, repetitions):
    captureRate = 25
    recruitRate = 10
    failed = True
    while failed:
        trials += 1
        num = randint(1,100)
        if num <= captureRate:
            caps += 1
            num2 = randint(1,100)
            if num2 <= recruitRate:
                recs += 1
                failed = False
            else:
                recruitRate += 5
                if recruitRate > 100:
                    recruitRate = 100
recRate = (float(recs) / float(trials))
print 'Recruit Rate: %0.6f  ->  1 / %0.6f' % (recRate, 1.0 / recRate) Output of this being: Recruit Rate: 0.055254  ->  1 / 18.098260",['probability']
1140968,An estimator for the c.d.f $F$ at a point $x_0$?,"Problem: Let $X_1,X_2,\ldots,X_n$ be independent identically distributed random variables (i.i.d's) with common CDF $F$. Fix $x_0\in\mathbb{R}$ and find an unbiased estimator for $F(x_0)$. Show that your estimator is an UMVUE for $F(x_0)$ (i.e. a uniformly minimum variance unbiased estimator ). So the problem is to find an unbiased estimator $\hat{F}(x_0)$ for $F(x_0)$ such that if $T$ is any unbiased estimator for $F(x_0)$, then $\operatorname{Var}(T)\geq \operatorname{Var}(\hat{F}(x_0))$. Attempt: I think I found what should be our unbiased estimator for $F(x_0)$. But after many, many attempts, I have no idea on how to show that it is an UMVUE. My estimator is
$$\hat{F}(x_0)=\frac{1}{n}\sum_{i=1}^nI\{X_i\leq x_0\}$$
where $I\{X_i\leq x_0\}$ is the indicator function that equals $1$ when $X_i\leq x_0$ and $0$ otherwise. We have
$$E[I\{X_i\leq x_0\}]=\int_{-\infty}^{x_0}f(x)dx=F(x_0),$$
so it is clear that $\hat{F}(x_0)$ is unbiased. But to show that it is UMVUE, the only tool that I have is the Cramer-Rao Inequality :
$$\operatorname{Var}(T)\geq\frac{[g'(\theta)]^2}{n E[(\frac{\partial}{\partial \theta}\log f(X;\theta))^2]}\tag{1}$$
for any unbiased estimator $T$ of $g(\theta)$. But here if $\theta=F(x_0)$ how do we differentiate $f$ with respect to $\theta$? I got
$$\operatorname{Var}(\hat{F}(x_0))=\frac{F(x_0)(1-F(x_0))}{n},\tag{2}$$
so I am trying to show that the r.h.s of $(1)$ is $(2)$ if $\theta=F(x_0)$ and $g(\theta)=\theta$. Edit: I looked it up and this estimator for $F(x_0)$ appears to be known as the empirical distribution function . However, there is no clue that this is an UMVUE...",['statistics']
1140972,Probability of infinite intersection.,"I came to the following problem: 
Let $A_1, A_2, ...$ be events in a probability space $(\Omega, F, \mathbb{P})$ and $\mathbb{P}[A_j]=1$ for all $j>1$. I need to show that the probability of the intersection of all those events $A_j$, where j goes from 1 to infinity, is also $1$. From what I understand, the events we have are not dependent so we can use the formula for a joint probability, so it will be the product of the probabilities of the events. However, I am not sure whether that formula holds in the general case. Any suggestions?","['probability-theory', 'stochastic-processes', 'probability']"
1141045,Does $\sum_{n=1}^\infty |\sin n|^{cn}$ converge? What about the terms?,"Does $\sum_{n=1}^\infty |\sin n|^{cn}$ converge for some $c > 0$? If so, what $c$? And for the $c$ that give divergence, what about the terms of the series? Do they converge? I saw a question where it was $|\sin n|^n$ instead of $|\sin n|^{cn}$, and it was shown that the sequence diverges (hence the series diverges), hence the question. So let's assume $c > 1$.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
1141049,How can I compute $\sum\limits_{k = 1}^n \binom{n - 1}{k - 1}$?,"I know what $n \choose k$ equals, but I don't see how that would help me solve the sum of $n - 1 \choose k - 1$ from $k = 1$ to $n$.  Is there any special trick I should know?","['summation', 'discrete-mathematics', 'binomial-coefficients']"
1141102,How to find the degree of a field extension,"I don't quite understand how to find the degree of a field extension. First, what does the notation [R:K] mean exactly? If I had, for example, to find the degree of $\mathbb Q (\sqrt7)$ over $\mathbb Q$, how would I go about it? And how would it be different from, say, $\mathbb C (\sqrt7)$ over $\mathbb C$? Would it involve finding the minimal polynomial? In the case of $\mathbb Q (\sqrt7)$, I find the minimal polynomial to be $x^2-7$ which is of degree 2, so would this be the value of $[\mathbb Q (\sqrt7):\mathbb Q]$? In $\mathbb C $, this polynomial is reducible, so I assume somehow it would need to be reduced to find the minimal polynomial. (I am guessing since $x^2-7=(x^2+1)-8=i-8$ this is the polynomial in $\mathbb C$) Would the degree of this be the degree of$[\mathbb C (\sqrt7):\mathbb C]$?","['definition', 'extension-field', 'abstract-algebra', 'field-theory']"
1141112,Limit of $(\sum_{k=0}^{n}k^4)/n^5$,"So i was trying to find this limit: $$\lim_{n\to\infty}\frac{  \sum_{k=0}^{n}k^4}{n^5}$$ which at first made me think it's zero but soon i realized that it's probably not. I tried expanding that but there's  no $n^5$ in the explansion. Eventialy i tried something like $$ \sum_{k=0}^{n+1}k^4 - \sum_{k=0}^{n}k^4=(x+1)^4= An^4+Bn^3+Cn^2+Dn+E$$
But again this has no $n^5$ involved.. If someone could provide a hint..","['summation', 'limits']"
1141115,Ahlfors method for partial fraction decomposition,"On page 31 in Complex Analysis by Ahlfors, he discusses decomposing a rational function into 
$$
R(z) = G(z) + H(z)\tag{12}
$$ 
where $G(z)$ is a polynomial without a constant term and is the singular part. Then he goes on to say let $\beta_i$ be the distinct finite poles of $R(z)$. The function $R(\beta_i + 1/\zeta)$ is a rational function of $\zeta$ with a pole at $\infty$. By use of the decomposition of $(12)$, we can write
$$
R(\beta_i+1/\zeta) = G_i(\zeta) + H_i(\zeta)
$$ 
or with a change of variables
$$
R(z) = G_i(1/(z-\beta_i)) + H_i(1/(z-\beta_i))
$$ 
Using the method developed in the text, write 
$$
\frac{z^4}{z^3-1}
$$ 
in partial fractions. If I just do division, I get
$$
\frac{z^4}{z^3-1} = z + \frac{z}{z^3-1}
$$
and the solution is quick and efficient. If I try to use the method developed in the text, (1) I am not able to get the correct answer and (2) it seems to hold no advantage to polynomial division. The poles are $z=1, e^{2\pi i/3}, e^{4\pi i/3}$ so
$$
R(1+1/\zeta) = \frac{(1+1/\zeta)^4}{(1+1/\zeta)^3 - 1}
$$
Should I expand the polynomials or multiple through by $\zeta^4$ and then expand? Both methods for the first pole would take more time then just polynomial division so I am under the impression there has to be an option three or this method wouldn't be of benefit to learn.","['partial-fractions', 'complex-analysis', 'polynomials']"
1141128,Can an arbitrary probability space be simulated by coin tosses?,"Let $B=\left(\{\mathrm{heads},\mathrm{tails}\},\mathcal{P}\{\mathrm{heads},\mathrm{tails}\},\mu\right)$ be the probability space for a single fair coin toss. For any set $S$ let $B^S$ be the independent product probability space, representing an independent fair coin toss for each element of $S$ . For each probability space $A$ , is there some set $S$ such that there exists a measure-preserving measurable function $B^S\rightarrow A$ ?","['probability-theory', 'measure-theory']"
1141170,Determine a parameterization for the line which is tangent to the curve at t=2,"(1) A curve is given by the function $$r(t)=(t^3 -3t^2 +2t +4)i + (13-5t)j +(t^2 -t-3)k$$ Determine a parameterization for the line which is tangent to the curve at $t=2$ I started by solving for when $t=2$, and got the vector $4i + 3j - k$ I don't know what this vector means or if this was even the correct approach. Do I need to find the vector perpendicular to this, or what should I be doing instead?","['multivariable-calculus', 'parametric', 'calculus', 'functions']"
1141204,Does $\sum_n |\sin n|^{cn^2}$ converge?,"So I recently asked a question about convergence of $\sum_n |\sin n|^{cn}$ for arbitrary $c > 0$ and it turns out that the terms of the series don't even converge, for any $c > 0$, so the series is always divergent. But what about  $\sum_n |\sin n|^{cn^2}$ for $c > 0$? Are there $c$ so that the series converges, and if there are $c > 0$ such that the series diverges, do the terms of the series still converge? If that's too hard, what if we replace $n^2$ in the exponent by $n^\alpha$ for some different $\alpha > 1$? Which $\alpha$ do we know the answer for?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
1141219,Algebraic field extensions: Why $k(\alpha)=k[\alpha]$.,"If $K$ and $k$ are fields, $K\supset k$ is a field extension and $\alpha \in K$ is algebraic over $k$, then we denote by $k[\alpha]$ the set of elements of $K$ which can be obtained as polynomial expressions of $\alpha$. $$k[\alpha] = \left\{P(\alpha): \quad P\in k[X]  \right\} $$ Also, we denote by $k(\alpha)$ the smallest subfield of $K$ containing both $k$ and $\{\alpha\}$. This is easily seen to be equal to the set of fractions of polynomial expressions in $\alpha$ (just thinking about the way the ""smallest"" subfield must be generated): $$k(\alpha) = \left\{\dfrac{P(\alpha)}{Q(\alpha)}: \quad P,Q\in k[X] \text{  and } Q(\alpha)\neq 0 \right\} $$ If $M$ is the minimal polynomial of $\alpha$, it is easily shown that $k[\alpha]\simeq k[X]/\langle M\rangle$ is a field  and therefore a subfield of $k(\alpha)$ containing both $k$ and $\{\alpha\}$. Therefore $k(\alpha)=k[\alpha]$, because $k(\alpha)$ is the smallest. It follows that all quotients $\dfrac{P(\alpha)}{Q(\alpha)}$ are equal to $H(\alpha)$ for some polynomial $H\in k[X]$. What I want to see is a more direct proof of this fact. Given $P$ and $Q$, how do you produce this $H$?","['extension-field', 'abstract-algebra', 'field-theory']"
1141223,Occupying seats in a classroom,"Here's a nice probability puzzle I have thought about for a class I'm TAing, I'm curious to see different solutions :) It goes like this: We have a classroom with $n$ seats available and $m \leq n$ incoming students. Each student has an (ordered) list of $k \leq n$ preferences for the seat he is going to take, where $k$ is some fixed positive integer. If at the moment of his arrival, a person's $k$ favorite seats are already taken, then he randomly chooses a seat from the remaining $n-k$. What is the probability that everyone occupies one of his favorite $k$ seats?","['discrete-mathematics', 'probability']"
