question_id,title,body,tags
1092105,Definition of disconnected subsets in metric spaces and in more general settings,"I found the following paragraph in a Real Analysis book (namely, Carother's one). A subset $E$ of a metric space $M$ is disconnected in $E$ if there
  exist disjoint, nonempty, open (in $E$) sets $U$ and $V$ such that $E=
 U \cup V$. Now, it is immediate that this gives us a pair of open sets
  $A$ and $B$ in $M$ such that $U=A \cap E$ and $V = B \cap E$. And so
  ""unrelating"" the relative definition, by writing it in terms of $A$
  and $B$, yields: $A \cap E \neq \varnothing$, $B \cap E \neq
 \varnothing$, $(A \cap E) \cap (B \cap E) = \varnothing$, and $E = (A \cap E) \cup (B \cap E)$, or $E \subset A \cup B$.
  (Phew!) This mess would be greatly simplified if we could take $A$ and
  $B$ to be disjoint in $M$. While this need not hold true in more
  general settings , luck is with us in a metric space. And then Carothers proceeds by giving the following lemma: Let $E$ be a subset of a metric space $M$. If $U$ and $V$ are disjoint
  open sets in $E$, then there are disjoint open sets $A$ and $B$ in $M$
  such that $U = A \cap E$ and $V = B \cap E$. I am wondering, what are the more general settings where this lemma does not hold?","['general-topology', 'connectedness', 'real-analysis']"
1092119,Closed unit ball of $B(H)$ with wot topology is compact,"The following is a Theorem of Conway's operator theory: I can not understand how he proves it. I think $\phi(\text{ ball B(H)})$ is compact if $\phi(\text{ ball B(H)})$ is closed subset of compact set $X$, but why is $X$ compact?","['operator-theory', 'topological-vector-spaces', 'functional-analysis', 'c-star-algebras']"
1092136,$n$ solutions Sudoku,"Is there an algorithm which can calculate a $9 \times 9$ Sudoku with non-trivial $n$ possible solutions? So if you play it you can play it for example 4 times? So if you play it there's a choice in which way you solve the Sudoku. So, for example, we have a row in which it's possible to place two numbers in 2 ways for getting a valid result for the whole Sudoku that would mean you can play it 2 times. My question is if there's a suitable method for ""constructing"" this kind of Sudokus?","['algorithms', 'combinatorics']"
1092148,Proof that there is no continuous 1-1 map from the unit circle in $\Bbb R^2$ to $\Bbb R$.,"Let $S^1=\{(x,y) \in \Bbb R^2 : x^2+y^2=1\}$. I'm trying to prove that there is no 1-1 continuous mapping between $S^1$ and the real line. The map is not necessarily onto. Proof so far:
Suppose such a function $f$ existed. Since $S^1$ is compact and $f$ is continuous and 1-1, $f$ inverse exists and is also continuous. Hence the range of $f$ is closed. Not sure where to proceed next.","['general-topology', 'real-analysis']"
1092154,Why is the convergence absolute?,"There is one thing my book uses in a proof after Abels theorem which I do not understand: Lets say that $\sum_{n=0}^\infty a_n$ converges. For $0\le x<1$, we look at $\sum_{n=0}^\infty a_n x^n$. The book says that this series converges absolutely for all the values of $x$ we have defined it. But why? We started with a sequence that might not even converge absolutely. And how do we know that we even have convergence when we introduce the $x$ variable? It would have been easy to see convergence if the orginal series was absolutely convergent, not only convergent, but they only state that the original series is convergent. UPDATE: If you are interested I got the picture from the book. Theorem 8.2 is Abels theorem, Definition 3.48 is the Cauchy-product(but this is clear from the picture), and what Theorem 3.51 states is also clear from the picture:","['absolute-convergence', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
1092185,Is a normal matrix satisfying $A^TA=...$ circulant?,"Let $A=\{a_{ij}\}$ be a normal matrix such that $a_{ij}\geq 0$ with equality iff $i=j$. Suppose that
$$
A^TA=\begin{pmatrix}
a & b & \cdots & b\\
b & a & \ddots & \vdots\\
\vdots & \ddots & a & b\\
b & \cdots & b & a\\
\end{pmatrix},\ where\ b>0.
$$
Does it follow that $A$ is a circulant matrix? Note: There is a partial classification of non-negative normal matrices posted here , which seems like it can be used to attack this problem. There is a geometric interpretation as well: both the set of rows and the set of columns of $A$ form equidistant sets of vectors on a sphere, and basic geometry appears to severely restrict the possibilities.","['geometry', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'diagonalization']"
1092193,Divergence theorem in complex analysis,"I am revisiting my understanding of integration by parts in several complex variables, but I have run into an apparent contradiction.  This shows my understanding is flawed, which is somewhat embarrassing, but I guess I must fess up and humbly ask for clarification here.  I reproduce the problem specified to one complex variable to make it more generally accessible. The differential $1$-form $\mathrm{d}z = \mathrm{d}x+i\mathrm{d}y$ on $\mathbb{C}$ eats a complexified tangent vector $a\frac{\partial}{\partial x}+b\frac{\partial}{\partial y}$, where $a,b \in \mathbb{C}$, and returns the complex number $a+ib$. Let $r : \mathbb{C} \to \mathbb{R}$ be a continuously differentiable function.  Assume that $\Omega = \{z \in \mathbb{C} : r(z)<0\}$ is smoothly bounded, and that $\nabla r = \frac{\partial r}{\partial x} \frac{\partial }{\partial x} + \frac{\partial r}{\partial y} \frac{\partial }{\partial y}$ has length $1$ on the boundary of $\Omega$.  Note that here I am thinking of $\nabla r$ as a ""real tangent vector"", but these are canonically included in the complexified tangent bundle as well. Since $\nabla r$ is perpendicular to the boundary of $\Omega$, then by geometry $J(\nabla r) = \frac{\partial r}{\partial x} \frac{\partial }{\partial y} - \frac{\partial r}{\partial y} \frac{\partial }{\partial x}$ must be tangent to the boundary, where $J$ is the complex structure tensor.  This is also of length $1$, so any real tangent vector to the boundary can be written $v = \alpha J(\nabla r)$. Noting that $\mathrm{d}S(v) = \alpha$, it seems that we have point wise equalities $
\begin{align*}
\mathrm{d}z (v) &=  \mathrm{d}z (\alpha J(\nabla r)) \\
&=\alpha \mathrm{d}z \left(\frac{\partial r}{\partial x} \frac{\partial }{\partial y} - \frac{\partial r}{\partial y} \frac{\partial }{\partial x}\right)\\
&=\left( -\frac{\partial r}{\partial y}+i\frac{\partial r}{\partial x}\right) \mathrm{d}S(v)\\
&=2i\cdot \frac{1}{2} \left( \frac{\partial r}{\partial x}+i\frac{\partial r}{\partial y}\right)\mathrm{d}S(v)\\
&=2i\frac{\partial r}{\partial \overline{z}} \mathrm{d}S(v)
\end{align*}$ So we can apparently conclude that $\mathrm{d}z = 2i\frac{\partial r}{\partial \overline{z}} \mathrm{d}S$ as $1$-forms on the boundary of $\Omega$ (at least for real tangent vectors which is all we ever plug into either form when integrating something). Unfortunately, this does not pass the following sanity check.  Let $r(z) = |z|^2-1 = z\bar{z}-1$, so that $\Omega$ is the unit disk.  Then, attempting to use the above identity, we obtain $
\begin{align*}
\int_{b\Omega} \frac{1}{z} \mathrm{d}z &= \int_{b\Omega} 2i \frac{1}{z} \frac{\partial r}{\partial \overline{z}} \mathrm{d}S\\
&=2i \int_{b\Omega} \mathrm{d}S\\
&=4\pi i
\end{align*}
$ which is a factor of $2$ more than it should be. I have scoured the above calculations for the $2$ at fault, but have not pinpointed it.  It seems likely to stem from the $\frac{1}{2}$ in the definition of the Wirtinger derivative $\frac{\partial}{\partial \overline{z}}$, but that $\frac{1}{2}$ really does need to be there. Can anyone help me out?","['differential-geometry', 'differential-forms', 'several-complex-variables', 'complex-analysis']"
1092217,Solution of $ f \circ f=f'$,"Let $f:\mathbb R \to \mathbb R $ be a function such that $f \circ  f=f'$ and $f(0)=0$ , I proved that $f$ is the null function. Can we prove that the same result holds if we change $f \circ f=f'$ by $f \circ f \circ f \circ f=f'$? Thank you for your help.","['ordinary-differential-equations', 'functions']"
1092246,Domain of a Trigonometric Composite Function,"I am struggling with a question based on finding the domain of a composite function. I have managed to complete questions such as in the following document at the bottom of page three: http://www.sinclair.edu/centers/mathlab/pub/findyourcourse/worksheets/Algebra/CompositeFunctionsAndTheirDomains.pdf However, this question feels different and I am unsure of how the answer was gained: Let $f$ and $g$ be the functions: $$ f : [0, 2\pi] \to \mathbb R, \quad f(x) = \cos (x), $$ and $$g : [0, 20] \to\mathbb R, \quad g(x) = x^2$$ Determine the domain for each of the compositions
$f \circ g$ and $g \circ f$. "" The given answers are: $$\operatorname{Dom}(f \circ g) = [0,\sqrt{2\pi} ]$$ and $$\operatorname{Dom}(g \circ f) = \:\left[0,\frac{\pi }{2}\right]\:\bigcup \:\left[\frac{3\pi }{2},\:2\pi \right]$$ Could anybody help me in working out how to find these domains please? Thank you, Lewis","['trigonometry', 'functions']"
1092260,Function such that $f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!}$,"I was trying to solve another problem and come up with the problem if there is a function with closed form such that $$f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!};(n\ge1).$$
I tried to check the condition for compositions of some elementary functions but could not find such function. Any hints and suggestions would be appreciated. Thanks!","['power-series', 'derivatives']"
1092262,"Number of sequences of $0$s, $1$s, and $2$s with length $n$ such that there is a $0$ somewhere between every pair of $2$s","Let $a(n)$ be the number of sequences with length $n$ which consists the digits $0,1,2$ such that between every two occurrences of $2$ there is an occurrence of $0$ (not necessarily next to the $2$ 's). An example for good sequence is $0102102$ . An example for bad sequence is $01212$ . A. Find a recursion formula for $a(n)$ B. Find an explicit expression for $a(n)$ . My try: let $x(n)$ be the number of sequences with length $n$ that if we add $2$ at the end of sequence it is still a good sequence with length $n+1$ . Now, if the last digit in the sequence was $0$ , then we have to look on the number of good sequences with length $n-1$ , hence $\displaystyle a(n-1)$ . If the last digit was $1$ , then we now need to look on $x(n-1)$ and work with the same manipulation. This way we get the recursion $x(n)=a(n-1)+x(n-1)$ . By induction I can get a recursion formula for $x(n)$ , but I don't know how to find a formula for $a(n)$ or find an explicit expression for $a(n)$ . Any help will be appreciated, thank you!","['recursion', 'combinatorics']"
1092298,Is there a non-dense subspace of $\ell_1$ which is dense in every finite-dimensional subspace?,"We want to find a subspace $F \subseteq \ell_1(\mathbb{R})$ such that for any finite $n \in \mathbb{N}$ and tuple $x=(x_1,\dots,x_n) \in \mathbb{R}^n$ we can find a sequence
$(f^k)_k \subseteq F$ with $f^k|_n = (f^k_1,\dots,f^k_n) \to x$ for $k \to \infty$. Actually, I would expect that this does not exist, but I can't quite find a way to prove this. The usual approach for me would be to say that as $F$ is not dense, there is $h \in \ell^\infty$ orthogonal to $F$, but every initial segment can be approximated by a sequence in $F$ and then try to use a diagonal argument to obtain a contradiction, but I can't make that work. I hope somebody has some more intuition on that problem than I do.","['sequences-and-series', 'functional-analysis', 'banach-spaces']"
1092299,"For f continuous on $[0,1]$, show that there exist points $\alpha_k$ such that $\sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n $","Suppose that $f$ is continous on $[0,1]$ , differentiable on $(0,1)$ , and $f(0)=0$ and  $f(1)=1$.For every integer $n$ show  that there must exist $n$ distinct points $\alpha_1,\alpha_2\cdots,\alpha_n$ in that interval so that
   $\sum \limits_{k=1}^n \frac{1}{f'(\alpha_k)} = n $ Is it theorem or i can show it with basic differentiation knowledge .Please give me hint .","['continuity', 'derivatives', 'real-analysis', 'analysis']"
1092309,Uniqueness of solution to $u_{t} - \Delta u + |\nabla u|^{2} = 0$,"The problem I am working on is as follows: Let $\Omega$ be a connected bounded domain in $\mathbb{R}^{n}$ with smooth boundary and let $f, g: \mathbb{R}^{n} \rightarrow R$ be smooth. Show that there is at most one smooth solution $u(x, t)$ such that
\begin{align*}
\begin{cases}
u_{t} - \Delta u + |\nabla u|^{2} = 0 & \text{ in } \Omega \times (0, \infty)\\
u(x, t) = g(x) & \text{ on } \partial \Omega \times (0, \infty)\\
u(x, 0) = f(x) & \text{ in } \Omega.
\end{cases}
\end{align*} This seems like an energy method type problem. Suppose $u_{1}, u_{2}$ are 2 smooth solutions of the above equation. Let $w := u_{1} - u_{2}$. Then
\begin{align*}
\begin{cases}
w_{t} - \Delta w + \nabla w \cdot \nabla(u_{1} + u_{2})= 0 & \text{ in } \Omega \times (0, \infty)\\
w(x, t) = 0 & \text{ on } \partial \Omega \times (0, \infty)\\
w(x, 0) = 0 & \text{ in } \Omega.
\end{cases}
\end{align*} My question is: how can I define an $E(t)$ such that $E(t) \leq 0$ for all time $t \geq 0$? As a first attempt, I tried defining $E(t) := \frac{1}{2}\int_{\Omega} w^{2}\, dx$, but I get
$$E'(t) = \int_{\Omega}w(\Delta w - \nabla w \cdot \nabla (u_{1} + u_{2}))\, dx.$$ However, I am not sure how to work with the $\nabla (u_{1} + u_{2})$ term. Any hints on how to get around this?","['multivariable-calculus', 'partial-differential-equations']"
1092312,Basic computation for the degree of an isogeny,"I am trying to compute the degree of the isogeny $\phi:E_{1} \to E_{2}$
where $\phi(x,y)=(\frac{y^2}{x^2},\frac{y(b-x^{2})}{x^2})$ with $E_{1} : y^{2} = x^{3} + ax^{2} + bx$,
$E_{2} : Y^{2} = X^{3} - 2aX^{2} + rX$, $char(K) \neq 2$ and $a, b \in K$ verifying $b \neq 0$ and $r = a^{2} - 4b \neq 0$. This is an example mentioned in Silverman's Arithmetic of Elliptic Curves, page 70. The book provides the expression of the dual isogeny as well, so I can just compose them and get the $[2]$ isogeny. However, I am interested in a general way of doing things. My guess here is to either
compute the degree of the field extension $[\bar{K}(E1):\phi^{\star}\bar{K}(E2)]$ or to try to use the sum of the ramification indexes for all the preimages of a point. My algebra skills are letting me down though, since I am not even able to compute $\phi^{\star}$. Any help or suggestions are greatly appreciated.","['algebraic-geometry', 'elliptic-curves', 'algebraic-curves']"
1092322,Find $\sin x \cos x$ if $\sin x = 4 \cos x$,"If $\sin x = 4 \cos x$, find $(\sin x)( \cos x)$. I feel like this is a silly question, and I think I am overthinking this. Can someone give me a hint on how to start? Would finding x be effective or would it be too messy?","['geometry', 'trigonometry']"
1092323,"Why isn't $f(x) = x\cos\frac{\pi}{x}$ differentiable at $x=0$, and how do we foresee it?","Consider $$f(x)=\begin{cases}
x\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\ 
0 & \text{for} \ x=0.
\end{cases}
$$ Its difference quotient $\frac{\Delta\left(f(x)\right)}{\Delta(x)}$ approaches $\cos\frac{\pi}{h}$ as $x$ gets closer to $0$, and thus $f$ is not differentiable in the origin because $\lim\limits_{h\to0}\cos\frac{\pi}{h}$ does not exist. This is the plot of $y=x \cos \frac{\pi}{x}$: But here's how my book goes on: Examining the figure we can foresee that the tangent line in a generic point $P$ of the graph doesn't tend to any limiting position as $P$ tends to the origin along the curve itself. One may think this happens because the graph of the function completes infinitely many oscillations in any neighbourhood of the origin. In fact, no: indeed the function thus defined: $$g(x)=\begin{cases}
x^2\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\ 
0 & \text{for} \ x=0
\end{cases}
$$
  has a graph that completes infinitely many oscillations in any neighbourhood of the origin, but, as you can verify, it is differentiable at $x=0$ and we have $g'(0)=0$. This is the plot of $y=x^2 \cos \frac{\pi}{x}$: So, I have two questions related to what I quoted from the book: how do we foresee the non-differentiability of $f$, given that, correctly, the infinitude of the oscillations is not an argument for it? And then, why isn't $f$ differentiable, instead of $g$? I shall emphasise that I know that, simply, the limit as $h\to 0$ of the difference ratio of $f$ doesn't exist, while that of $g$ does, but I've been wondering about an other kind of reason after reading that excerpt. Or is my book wrong in mentioning other reasons?","['calculus', 'derivatives', 'real-analysis', 'analysis']"
1092346,How can I prove that $\int_{0}^{\infty }\frac{\log(1+x)}{x(1+x)}dx=\sum_{n=1}^{\infty }\frac{1}{n^2}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How can I prove that $$\int_{0}^{\infty }\frac{\log(1+x)}{x(1+x)}dx=\sum_{n=1}^{\infty }\frac{1}{n^2}$$","['sequences-and-series', 'integration']"
1092358,Proof of the Intermediate Value Theorem,"Theorem: Let $f$ be continuous on $[a,\,b]$ and assume $f(a)<f(b)$. Then for every $k$ such that $f(a)<k<f(b)$, there exists a $c\in[a,\,b]$ such that $f(c)=k$. proof: $f$ continuous at $a\implies$ for $\varepsilon=k-f(a)>0$, $\exists\delta>0$ s.t. $$|f(x)-f(a)|<\varepsilon=k-f(a)\quad\forall x\colon |x-a|<\delta.$$ Consider the set $H=\{x\in[a,\,b]\colon f(x)<k\}\not=\emptyset \implies c=\sup{(H)}$. Show $f(c)=k$, suppose $f(c)<k\iff k-f(c)>0$. We know $f$ is continuous at $c$ so $\forall\varepsilon>k-f(c)>0$ $\exists\delta>0:|f(x)-f(c)|<\varepsilon=k-f(c)$ when $|x-c|<\delta$. $\implies f(x)-f(c)<k-f(c)$. Say $x=c+\delta/2\implies f(x)<k\implies c+\delta/2\in H$ which contradicts the fact $c=\sup{(H)}$, since $\delta>0$. Same proof works if $f(c)>k$, thus $f(c)=k$. This is a proof for the intermediate value theorem given by my lecturer, I was wondering if someone could explain a few things: What is the set $H$, what does it define? Why does contradicting the fact that $c=\sup{(H)}$ prove that $f(c)\not<k$? How would I continue to actually finish this proof, ie. show $f(c)\not>k$?","['proof-explanation', 'real-analysis']"
1092364,Prove it is the incenter.,"Let $\triangle ABC$ be an acute-angled triangle. Let $H$  be the foot of the perpendicular from A to BC. Let $K$ be the foot of the  the perpendicular of $H$ to $AB$, let $L$ be the foot of the perpendicular from $H$ to $AC$. Let $AH$ intersect the circumcircle of $ \triangle ABC$ in $T$, and let the line through $K$ and $L$ intersect the circumcircle of $\triangle ABC$ in $P$ and $Q$. Prove that $H$ is the incenter of $\triangle PQT$.",['geometry']
1092370,Confusion about this set operations,"I was reading this excellent answer on this particular thread about ordered pair definition: How can an ordered pair be expressed as a set? At one point the accepted answer says: The problem with your proposal is that it does not have the defining property we want for ordered pairs: for example, $\emptyset\neq\{\emptyset\}$, so we want $(\emptyset,\{\emptyset\}) \neq (\{\emptyset\},\emptyset)$. But in your proposal, we have:
  $$\begin{align*}
(\emptyset,\{\emptyset\}) &= \Bigl\{ \{\emptyset\}, \bigl\{\{\emptyset\}\bigr\}, \bigl\{ \emptyset, \{\emptyset\}\bigr\}\Bigr\},\\
(\{\emptyset\},\emptyset) &= \Bigl\{ \bigl\{ \{\emptyset\}\bigr\}, \{\emptyset\}, \bigl\{ \{\emptyset\},\emptyset\bigr\}\Bigr\};
\end{align*}$$
  so that $(\emptyset,\{\emptyset\}) = (\{\emptyset\},\emptyset)$. I am not sure how the RHS of both the equations are equal here. What set operations are used to achieve this?",['elementary-set-theory']
1092396,"Find $\sin^3 a + \cos^3 a$, if $\sin a + \cos a$ is known","Given that $\sin \phi +\cos \phi =1.2$, find $\sin^3\phi + \cos^3\phi$. My work so far: (I am replacing $\phi$ with the variable a for this) $\sin^3 a + 3\sin^2 a *\cos a + 3\sin a *\cos^2 a + \cos^3 a = 1.728$. (This comes from cubing the already given statement with 1.2 in it.) $\sin^3 a + 3\sin a * \cos a (\sin a + \cos a) + \cos^3 a = 1.728$ $\sin^3 a + 3\sin a * \cos a (1.2) + \cos^3 a = 1.728$ $\sin^3 a + \cos^3 a = 1.728 - 3\sin a * \cos (a) *(1.2)$ Now I am stuck. What do I do next? Any hints?",['trigonometry']
1092404,Prove the empty set is a subset of every set. Does induction work?,"I've taken a look at the proofs by contrapositive and by vacuous truths (for the above title), but I was wondering whether or not the following proof by induction works. The following proof proceeds by induction on the number of elements in a given set. Let $\phi$ be some set with one element and $\phi\prime$ be some nonempty set that shares no common elements with $\phi$. Then by definition,
$$\phi \cap \phi\prime = \emptyset$$
Which implies that $\emptyset \subseteq\phi$, establishing the base case. Then for the inductive step, let $\psi$ be some set with $n+1$ elements and let $\psi\prime$ be some nonempty set that shares no common elements with $\psi$. Then,
$$\psi \cap \psi\prime = \emptyset$$
Meaning that $\emptyset \subseteq\psi$, which proves the inductive step, completing the proof by induction. Is this logic fine?","['induction', 'elementary-set-theory']"
1092405,"Does the phrase ""orthogonal"" mean the same thing when used in the terms ""orthogonal function"" and ""orthogonal vector""?","I was reading about Fourier series when I came across the term ""orthogonal"" in relation to functions. http://tutorial.math.lamar.edu/Classes/DE/PeriodicOrthogonal.aspx#BVPFourier_Orthog_Ex2 I've never heard of this. The idea that two vectors are orthogonal makes sense to me because I can imagine, for instance, $\vec{a}=(1,0)$ and $\vec{b}=(0,1)$, such that $\vec{a} \cdot \vec{b} = (1)(0) + (0)(1) = 0$. But no simple picture comes to mind for functions. Wikipedia wasn't very helpful for me. http://en.m.wikipedia.org/wiki/Orthogonal_functions Can someone explain what this concept is and give a simple example? Remark: My intuition says maybe intersecting lines would be an example of two orthogonal functions. $f(x) = x$ $g(x) = -x$ But that's just a shot in the dark and I don't think that makes sense because the integral is just $\int -x^2 = - \frac{x^{3}}{3} + C$, which isn't zero.","['fourier-series', 'orthogonality', 'functions']"
1092410,Solve logarithmic equation for $x$ to find the inverse of $f(x)= \ln(x+\sqrt{x^2+1})$,"Let $f(x)= \ln(x+\sqrt{x^2+1})$. Find $f^{-1}(x)$. Here is what I got so far: $y= \ln(x+\sqrt{x^2+1})$, rewrite as $x= \ln(y+\sqrt{y^2+1})$, 
then  $$e^x= y+\sqrt{y^2+1}$$ $$e^x-y= \sqrt{y^2+1}$$ $$  y^2+ e^{2x}-2(e^x)y= 1$$ 
So if   $e^x= a$, then $a^2-2ay-1= 0$","['logarithms', 'algebra-precalculus', 'inverse']"
1092417,Proving the 'letters' of a free group generate the group,A group $F$ is free over a set $X$ if there exists an injection $\sigma: X \to F$ such that for any function $\alpha: X \to G$ to any group $G$ there exists a unique homomorphism $\phi : F \to G$ such that $\phi \sigma = \alpha$. There is an exercise in 'A Course in the Theory of Groups' that asks us to prove that $ \langle Im \ \ \sigma \rangle = F$ using only the categorical definition. I am very frustrated because the problem seems very easy but I have still not found the solution. I imagine that if we let $\alpha = \sigma$ there should be some problem with the uniqueness of $\phi$. Also if $\langle Im \ \ \sigma \rangle$ is a proper normal subgroup of $F$ then letting $\alpha: F \to F \langle Im \ \ \sigma \rangle$ be the zero map will allow for both the zero homomorphism on F and the standard epimorphism to a quotient to make the diagram commute.,"['category-theory', 'free-groups', 'group-theory']"
1092418,"Intuitively, why should the coefficient of the derivative of $x^n$ be $n$?","I am able to differentiate $x^n$ with respect to $x$ from first principles using the definition of differentiation. Also it seems natural that the gradient of a finite polynomial will be one order lower. However the fact that the coefficient of the derivative of $x^5$ should be $5$, for example, seems less obvious. Is there a way of showing this result so that it is intuitive?","['soft-question', 'calculus', 'derivatives', 'polynomials']"
1092426,Probability of rolling a sum of N with up to infinite rolls of a die,"I'm trying to figure out if there is some (relatively simple) formula for calculating the probability of rolling a sum of N with as many rolls as needed with a single regular six-sided die.
For example: $N=1$ is $0.16666 = 1/6 = 1/6$ (1) $N=2$ is $0.19444 = 7/36 = 1/6 + 1/36$ (2 or 1,1) $N=3$ is $0.22685 = 49/216 = 1/6 + 2/36 + 1/216$ (3 or 1,2/2,1 or 1,1,1) $N=4$ is $0.26466 = 343/1296 = 1/6 + 3/36 + 3/216 + 1/1296$ (4 or 2,1/1,2/2,2 or 1,1,2/1,2,1/2,1,1 or 1,1,1,1) ... Does this series converge to essentially one and is there some good formula? It seems like pascal's triangle is involved somehow (at least for n=1-6, but I'm not sure how (if even possible) to convert it into a formula. Any help/advice is appreciated.","['dice', 'probability']"
1092471,Cambridge Mathematical Tripos Question - 1871,"This is a question from the Cambridge Mathematical Tripos in 1871, Scanned copy added at the end of the post. A ship $A$ sees another ship 
  $B$ whose course is not known. Given that they have the same speed, prove that chance of them coming within a distance $d$ of each other is always $\frac{2sin^{-1}(\frac{d}{a})}{\pi}$ no matter the course of $A$, provided that it's inclination to $AB$ is not greater than $cos^{-1}(\frac{d}{a})$where $AB=a$ This is a summary of my method so far: First I constructed a vector triangle $ABC$ where $C$ was the intersection of the courses of $A$ and $B$. In this triangle I denoted the angle between $A$ and $AB$ as $\theta$ and $B$ and $AB$ as $\phi$. I then used trigonometry to get an expression for $d$ which I subsequently differentiated with respect to time and set as a minimum so I could get $t=\frac{b+c}{2v}$ where $v$ is the velocity of the ships. Using this I got $d=acos(\frac{\theta + \phi}{2})$ which implies $\phi=2cos^{-1}(\frac{d}{a})-\theta$ Could someone help me proceed and  post a solution themselves if they have a better method? It would also be helpful if you could point out if this is possible to complete having only high school maths knowledge.","['classical-mechanics', 'probability']"
1092485,The n-th prime is less than $n^2$? [duplicate],"This question already has answers here : Is there a way to show that $\sqrt{p_{n}} < n$? (2 answers) Closed 7 years ago . Let $p_n$ be the n-th prime number, e.g. $p_1=2,p_2=3,p_3=5$. How do I show that for all $n>1$, $p_n<n^2$?","['prime-numbers', 'number-theory']"
1092503,How does polynomial long division work?,"I get normal long division but this doesn't make sense. How can doing it by only dividing by the leading term work? The problem is $$\dfrac{3x^3 - 2x^2 + 4x - 3 } {x^2 + 3x + 3},$$ not $$\dfrac{3x^3 - 2x^2 + 4x - 3} {x^2}.$$","['algebra-precalculus', 'polynomials']"
1092514,finding variables of one side of the equation given cos and tan,"These are actually two questions, but they seem to be similar to one another. 1) Find $d$, $e$, $f$, and $g$ such that  $4\cos(x)\cos(2x)\cos(4x) = \cos(dx) + \cos(ex) + \cos(fx) + \cos(gx)$. $d$, $e$, $f$, and $g$ are positive integers . I substituted some formulas for $\cos(2x)$ and $\cos(4x)$. After expanding, I got: $7\cos^4(x)-6\cos^2(x)+\sin^4(x)$. I'm not sure how to get rid of the $\sin^4(x)$ and, after doing that, find $d$, $e$, $f$, and $g$. Any hints? 2) Find $h$, $i$, $j$, and $k$ such that $ \tan 7.5^\circ =\sqrt{h}+\sqrt{i}-\sqrt{j}-\sqrt{k} $ All I have so far is that $\tan 7.5^\circ = 0.1317$ radians. Other than that, I am not sure where to go from there. Any hints on this one as well?","['geometry', 'trigonometry']"
1092515,"Why do ODEs that have solutions that have closed form solutions, *have* closed form solutions?","Why do certain classes of ODEs have closed form solutions? Is there something these classes have common apart from the fact that they have closed form solutions? When I say ""closed form"", I mean something vague, like there is no finite combination of usual operators (addition, multiplication, powers, trigonometric and logarithmic operators) and operands that can be used to write the solution.",['ordinary-differential-equations']
1092526,identity of $(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}]$,"I am trying to understand the identity 
$$(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}] \quad (*),$$
where $T \in \mathbb{C}^{n\times n},z\in \mathbb{C}$ and the spectral radius of $T$, $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ and $w$ is a primitive $n$th root of 1,i.e. $w =e^{i2\pi/n}$. I have tried using the identity   $$[I-A]^{-1} = I+A+A^2+A^3+... $$ which holds when $A^n$ becomes zero matrix. This  works as  $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ implies the convergence of $A^n$. So I expand both sides of $(*)$ and then compared the coefficient of the two power series. But this approach seems to be a bit tedious, I wonder whether there is a more clean and simpler proof.","['matrix-equations', 'matrices', 'complex-numbers', 'complex-analysis']"
1092528,The density of the square of the exponentially distributed random variable,"Let $\xi$ - a random variable with an exponential distribution $p_{\xi} (x) = \lambda e^{-\lambda x}$
$$
p_{\xi} (x) = 
\begin{cases} 
0, & x < 0, \\ \lambda e^{-\lambda x}, & x \ge 0.
\end{cases}
$$ I want to find the density of a square $\xi^2$. Can I say that $p_{\xi^2} (x) = p_{\xi}^2 (x)$
or should I look for density according to the definition $p_{\xi^2} (x)  = \big ( F_{\xi^2} (x) \big )' = (\mathbb{P} \big ( \xi^2 < x) \big )'$ ?","['probability-theory', 'probability']"
1092533,Probability of knocking off all of a dragon's heads,"You are fighting a dragon with three heads. Each time you swing at the dragon, you have a $20\%$ of hitting off two heads, a $60\%$ chance of hitting off one head and a $20\%$ of missing altogether. If you knock off one head, the head grows back immediately before the next iteration. If you miss, an additional head grows immediately before the next iteration. If you knock off two heads, the heads stay knocked off and you move to the next iteration. You win if you knock off all of the dragon's heads and the dragon wins if at any time it has five heads. What are the odds you win the game? Anyone have a definitive answer for this? Was able to get an answer finding the probabilities for each potential number of heads and the chances of winning/losing within $3$ iterations, but wanted to see if it is right. Then if you get back to $3$ heads, multiply by $p$ through recursion. Can you ignore the $60\%$ as nothing in the game changes if this occurs? I thought: $p$ = chance of two hits in a row + chance of two hits, one miss + probability of getting back to $3$ heads with two miss, $$\text{one hit}*p
= (1/5*1/5) + 2\left(1/5*1/5*1/5\right) + 2\left(1/5*1/5*1/5\right)*p$$ Seems a bit low, likely because I'm leaving out that $60\%$.. but not sure where to include it. Any help would be greatly appreciated.","['probability', 'recursion']"
1092565,For what values does $\sin(x+h) = \cos x$?,For what and how many values of h such that $0 \leq h \leq 100$ are the graphs $a(x) = \sin(x+h)$ and $b(x) = \cos x$  identical? I am not sure where to start. Hints only please.,"['geometry', 'trigonometry']"
1092577,Confusion concerning Cantor's theorem.,"I'm a little confused about Cantor's theorem stating that the cardinality of a set cannot be equal to the cardinality of its power set. Consider the power set of $\mathbb{N}$. Couldn't the power set of $\mathbb{N}$ be considered to be a subset of the union of the  sets $\mathbb{N^1}, \mathbb{N^2}, \mathbb{N^3}, \ldots$? All the sets are denumerable, and clearly the list of sets is denumerable, so shouldn't their union and thus an infinite subset of their union be denumerable, implying that the cardinality is the same as $\mathbb{N}$?",['elementary-set-theory']
1092597,Fano-ness of moduli space of stable vector bundles when determinant line bundle is *not* fixed...,"According to Drezet-Narasimhan, Invent. Math. 97 (1989), no. 1, 53--94, the moduli space $\mathbb M$ of slope-stable holomorphic vector bundles with fixed rank $r$ and fixed determinant line bundle $L$ (such that $\gcd(\deg L,r)=1$) on a smooth, compact, connected Riemann surface $X$ is Fano, which means that the anticanonical line bundle of $\mathbb M$ has positive degree.  By earlier works (I believe), $\mathbb M$ is smooth and compact itself. What can be said about the structure of the moduli space when the degree of $L$ is fixed but the isomorphism class of $L$ is allowed to vary? I suspect it is no longer Fano, for any $r$. Evidence: the $r=1$ version of $\mathbb M$ with $\deg L$ fixed, but with the isomorphism class of $L$ allowed to vary, is the Jacobian of $X$ itself. If the genus of $X$ is $g$, then $\mbox{Jac}(X)$ is a $g$-dimensional complex torus, and hence has trivial anticanonical (and canonical) bundle.  In particular, it is ""Calabi-Yau"" rather than Fano. If we take $\mathbb M$ to be the moduli space of slope-stable holomorphic vector bundles with fixed rank $r>1$ and fixed degree $d$ that are coprime (but not fixed determinant) on a smooth, compact, connected Riemann surface $X$, then is it: (a) Fano (suspect not), (b) Calabi-Yau, or (c) something else? A reference would be helpful, but an intuitive explanation of how going from fixed determinant to non-fixed determinant changes the overall geometry of the moduli space would be great.","['algebraic-geometry', 'holomorphic-bundles', 'complex-geometry']"
1092600,Definition of continuity in topological spaces does not seem quite right.,Here's how continuity is defined in most standard topology texts A function from $X$ to $Y$ is continuous iff the inverse image of each open set of $Y$ is open in $X$. This definition does not quite seem right to me because it assumes that each element of $Y$ has an inverse image in $X$ and the image is unique. Clearly the definition does not quite hold if the function is not bijective. So how does one define continuity for topological functions which are not bijective?,"['general-topology', 'continuity']"
1092603,What can be said about $P (A \setminus B) \setminus (P (A) \setminus P (B))$?,"This is one of the problem I have been solving in Velleman's How to prove book: Suppose A and B are sets. What can you prove about $P (A \setminus B) \setminus (P (A) \setminus P (B))$ ? Now, I started solving it like this assuming that $ x \in P (A \setminus B) \setminus (P (A) \setminus P (B))$: $ (x \in P(A \setminus B)) \land (x \notin (P(A) \setminus P(B))) $ $ (x \subseteq A \setminus B) \land (\neg (x \in P(A) \setminus P(B))) $ $ (x \subseteq A \setminus B) \land (\neg (x \in P(A) \land x \notin P(B))) $ $ (x \subseteq A \setminus B) \land ((x \notin P(A) \lor x \in P(B))) $ $ (x \subseteq A \setminus B) \land ((x \notin P(A) \lor x \subseteq B)) $ Now, I see that in the solution they have concluded that $x$ is $\emptyset$. But I cannot figure out how $\emptyset$ comes for $x$ ?","['logic', 'elementary-set-theory', 'proof-verification']"
1092627,Centroids and Harmonic Means,"A triangle $ABC$ with centroid $G$ is such that a line $l$ passing through $G$ intersects $AB$, $BC$, and $AC$ at $H, I, J$, respectively. Show that out of the 3 distances $d(G, I), d(G, H), d(G, J)$, one is the harmonic mean of the others.","['geometry', 'contest-math']"
1092637,A simple example of an incomplete probability space?,A probability space is complete if every subset of a set of measure zero is measurable. The probability space is incomplete if otherwise. But is there a simple example of an incomplete probability space?,"['measure-theory', 'probability']"
1092669,Uniform distribution on unit disk,"Let $(X, Y)$ be a random point chosen according to the uniform distribution in the disk of radius 1 centered at the origin. Compute the densities of $X$ and of $Y$. I know that the joint density of $X$ and $Y$ is $\frac{1}{\pi}$ since when we integrate $\frac{1}{\pi}$ over the unit circle, we get $1$. So if I wanted to find the density of $X$, I was thinking of finding the cumulative distribution of $X$ and the differentiate it to get its density. In order to get its cumulative distribution function, I was going to use the fact that $P(X<x)=P(X<x, -\infty < Y < \infty)$, but this integral doesn't seem nice to work with. Am I on the right track or is there a better way?","['probability-distributions', 'probability']"
1092678,How to solve sum of sines and cosines system of equations?,"I have a set of equations to solve which in the following form: $ \cos(t_1 + t_2 + t_3 + t_4) + \sin(t_1 + t_2 - t_3 + t_4) + \cos(t_1 - t_4 + t_3 - t_5) + \sin(t_1 - t_2 + t_3 - t_5) + \cos(t_1 + t_3 + t_4 - t_5) + \sin(t_2 + t_3 - t_4 + t_5) = a $ $ \cos(t_1 + t_2 - t_3 - t_4) + \cos(t_1 + t_2 - t_4 + t_5) + \sin(t_1 + t_2 + t_4 + t_3) + \sin(t_1 - t_2 + t_4 - t_5) + \cos(t_2 + t_3 + t_4 + t_5) + \sin(t_1 - t_3 + t_4 - t_5) = b $ For $t_1, t_2, t_3, t_4, t_5$ scalar real variables and $a, b$ a real numbers. This example above is a specific (smaller) instance of the equations I need to solve, where each equation has dozens of cos/sin terms with linear combinations of 5 scalar variables as the argument. I need to be able to find all the possible solutions (or none) for such a form of problem. Any recommendations on how I should approach such a problem? I am looking to use computational resources, and would be interested in a numerical/optimization approach of some type. Seeing that the problem is nonconvex, I'm at a bit of a loss.","['optimization', 'trigonometry', 'systems-of-equations']"
1092687,A question about two common definitions,"Two definitions make me puzzled ! 1. The definition of $\textbf{Functions Differentiable at a Point}$ : A function $f$ defined in  a neighborhood $(x_{0}-\delta,x_{0}+\delta)$of a point $x_{0}$, if  $$\lim_{x\rightarrow {x}_{0}}\frac{f(x)-f({x}_{0})}{x-{x}_{0}}=A\in\mathbb{R}$$ then 
   We called the  function $f$ is differentiable at the point $x_{0}$! 2. The defintion of  $\textbf{Functions Continuous at a Point}$: Let $X \subset \mathbb{R},f(x):X\rightarrow \mathbb{R},x_{0}\in X$, if $$\forall \epsilon >0, \exists\delta>0;s.t.\forall x\in X,|x-x_{0}|<\delta \Rightarrow |f(x)-f(x_{0})|<\epsilon .$$ 
then We called the  function $f$ is Continuous at the point $x_{0}$! My confusion: $\textbf{A isolated point} \quad\hat{x}\in X,$ according to The defintion of  $\textbf{Functions Continuous at a Point}$,  $f$ is Continuous at the point $\hat{x}$.But $f$ has no vaules in  $\hat{x}$ neighborhood $(\hat{x}-\delta_{\hat{x}},\hat{x}+\delta_{\hat{x}})$ of the point $\hat{x}$,so on the basis of The definition of  $\textbf{Functions Differentiable at a Point}$, $f$ is not differentiable at the point $\hat{x}$. But the conclusion drawn from the results described above is contradicted against $\textbf{f(x) differentiable at a point must be continuous at the point}.$ I need clartity to eliminate the confusion!Any of your help will be appreciated! I am sorry I made some obvious mistakes! There is no contradiction!","['definition', 'general-topology', 'continuity', 'real-analysis', 'derivatives']"
1092701,"Closed form for a formula with a summation over $i\binom{n-i}{k-1}$, and combinatorial proof?",I was trying to simply an expression in an exercise related to randomized algorithms. Here is the expression which I have obtained at the end. $$ \displaystyle\frac{\displaystyle\sum_{i=1}^{n+k-1} i \binom{n-i}{k-1}}{ \displaystyle{n \choose k}}$$ Is there any way to simplify the numerator so that the whole expression simplifies into a nice closed formula? A combinatorial approach would be greatly appreciated.,"['binomial-coefficients', 'combinatorics']"
1092702,"The meaning of notation like $f\colon \mathbb R^2 \to \mathbb R$, $x \in \mathbb R^n$, and $x \in \mathbb R$.","I am in second year university and am taking linear algebra this semester.  Never having been a strong maths student, I am certainly struggling with some basic concepts and especially notation. I have tried searching on the web but have had difficulty in finding something which properly explains the meaning of notation like $$ f: \Bbb{R^2} \to \Bbb{R}$$ or the difference between
$x\in \Bbb{R^n}$ and $x \in \Bbb{R}$ I can basically read these, and know the literal pronounciation of the symbols, but have no idea what they actually mean. The first one would be $f$ maps $\Bbb{R^2}$ to $\Bbb{R}$. What does this mean exactly? Is it saying that on an $(x,y)$ plane, the function $f$ returns a single number?
E.g $f(x) = 3x^2$
$f(1) = 3$? Is the second one saying that $x$ is an element of a vector space with $n$ elements $(ax_1, bx_2,....,a_nx_n)$, whereas the first one is saying that $x$ is just some real number? I would really appreciate if someone could help me with this, either explaining it or referring me to a nice book that is appropriate at a beginner level. 
Further more does this type of notation have any specific name?","['notation', 'linear-algebra', 'functions']"
1092715,Equivalent definitions of a surface,"do Carmo Differential Geometry of Curves and Surfaces defines a regular surface as per the below post. Lee Introduction to Smooth Manifolds defines an embedded or regular surface to be an embedded or regular submanifold of $\mathbb{R}^3$ of codimension 1, namely a subset $S\subset\mathbb{R}^3$ that is itself a smooth $2$-dimensional manifold whose topology is the subspace topology and whose smooth structure ensures the inclusion map $\iota:S\hookrightarrow\mathbb{R}^3$ is an embedding. Question : Are these definitions equivalent?  If so can someone present or point to in the literature a detailed proof .","['differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds', 'surfaces']"
1092717,Infinite series involving factorials of squares,Does $$\sum_{n=0}^\infty \frac{1}{(n^2)!}=2.04167\dots$$ possess a closed form?,"['closed-form', 'sequences-and-series']"
1092737,Whats the difference between modular forms of different levels?,"We have a natural surjective group homomorphism: $\phi : SL_2(\mathbb{Z}) \to SL_2(\mathbb{Z}/(n\mathbb{Z}))$ from which, given any subgroup $H<SL_2(\mathbb{Z}/(n\mathbb{Z}))$, we may take the pre-image under $\phi$. This gives us a ""congruence subgroup"". Most notably, we have $\Gamma(n) : = \left \{\left[ \begin{array}{cc}
a & b \\
c & d \end{array} \right]
\equiv \left[ \begin{array}{cc}
1 & 0 \\
0 & 1 \end{array} \right] \mod n \right \} = \operatorname{Ker}(\phi)$ $\Gamma_1(n) : = \left \{ \left[ \begin{array}{cc}
a& b \\
c & d \end{array} \right] 
\equiv
\left[ \begin{array}{cc}
1 & * \\
0 & 1 \end{array} \right] \mod n \right \}  = \phi^{-1} ( U)$, U-unipotent matrices in $SL_2(\mathbb{Z}/(n\mathbb{Z}))$ $\Gamma_0(n) : = \left \{\left[ \begin{array}{cc}
a & b \\
c & d \end{array} \right]
\equiv
\left[ \begin{array}{cc}
* & * \\
0 & * \end{array} \right]\mod n \right \} = \phi^{-1}(T), $ T-triangular matrices in $SL_2(\mathbb{Z}/(n\mathbb{Z}))$ So I see these congruence subgroups everywhere but I never have found an explanation for any qualitative differences between modular forms of these different levels (or any congruence subgroups). For instance, If $f \in S_k(\Gamma_0(n))$, versus $f \in S_k(\Gamma_1(n))$, does this say anything about it's corresponding invaraints? (L-function, automorphic representations, Galois representations, jacobians, etc). Is there any difference between the modular curves $X_{\Gamma}: = \mathbf{H} \cup P^1(\mathbb{Q})/\Gamma$, for these different levels? The modularity theorem, for instance, guarantees a modular form of level $\Gamma_0(n)$. I have to imagine that $\Gamma_0$ was an important part of this implying Fermat's Last Theorem, but I could be wrong. I'm interested in any sources available to help me understand any, if at all, differences between these subgroups and the rest of the story of modular forms.","['modular-forms', 'geometric-group-theory', 'group-theory']"
1092752,Why is the complement of an affine subset of projective space a hyperplane?,"Let $P$ be a projective space of dimension $n$ and $Q$ a linear subspace of it.
If the complement of $Q$ is affine, why must $Q$ be of dimension $n - 1$? The following is my thought: Take the homogeneous coordinate system $[T_0:\cdots:T_n]$.
(Suppose that $Q$ is not a hyperplane.)
Let $Q$ be of dimension $d(d < n - 1)$, satisfying $T_{i} = 0(i\in\{d+1,\cdots, n\})$.
Then what's left is to show that $\{[T_0:\cdots:T_n]:\exists i\in\{d+1,\cdots, n\}, s.t. T_i\neq 0\}$ is not affine. How to prove this assertion? For the other direction, I know it's obvious.
Many thanks!",['algebraic-geometry']
1092796,Regarding the derivative of the $j$-invariant,"Is anyone aware of a formula for the derivative of the $j$-invariant $j(\tau)$ with respect to $\tau$? Here, $\tau$ is in the upper half-plane. I would image there are probably quite a few formulae for $j'(\tau)$, but they are not well-known. I looked through a few books but I could not find a single formula. Any help would be greatly appreciated.","['modular-forms', 'derivatives']"
1092804,Abelianization of general linear group?,"I am asking purely out of interest: What the abelianization of general linear group $GL(n,\mathbb{R})$?","['group-theory', 'abstract-algebra', 'linear-groups']"
1092810,Why is it important to have the closed form of a generating function?,"I am having introductory lectures on combinatorial analysis, I've been presented to the concept of generating functions and it's applications to solving combinatorial problems. The generating function of the sequence $(1,1,1,1,1,1,\ldots)$ is: $$1+x+x^2+x^3+x^4+x^5+\ldots\tag{1}$$ Which can be reduce to a closed form expression: $$\displaystyle\frac{1}{1-x}\tag{2}$$ But until the present moment, the exercises I made in the chapters we're studying asked us to write $(1)$ as $(2)$, there wasn't anything about using the closed form to obtain further results. In the combinatorial exercises I made, I had only to evaluate the coefficient of a certain term in the generating function or the exponent of a certain coefficient in the formal series and at least in the book I'm using, there is still no use to the closed form of certain formal power series. So why is it important to have a closed form such as $(2)$? I still don't get why it is important/useful.","['generating-functions', 'combinatorics']"
1092840,Left invertible matrices over rings with some special property,Suppose $R$ is a ring in which every left invertible element is invertible. Does this condition imply that every left invertible matrix in $\mathrm{M}_{n\times n}(R)$ is necessarily invertible?,"['matrices', 'ring-theory', 'abstract-algebra']"
1092846,Prove that: $ \left( \sum\limits_{i\neq j}a_{i}b_{j} \right)^2 \geq \sum\limits_{i\neq j}a_{i}a_{j} \sum\limits_{i\neq j}b_{i}b_{j}$,"Let $a_{1}, \cdots, a_{n}, b_{1}, \cdots, b_{n}$ be positive real numbers. Prove that: $$
\left( \sum\limits_{i\neq j}a_{i}b_{j} \right)^2 \geq \left( \sum\limits_{i\neq j}a_{i}a_{j} \right) \left( \sum_{i\neq j}b_{i}b_{j}  \right)
$$ One solution is: Let us denote: $p = \sum_{i=1}^{n}a_{i}, q = \sum_{i=1}^{n}b_{i}, k = \sum_{i=1}^{n}a_{i}^2, l = \sum_{i=1}^{n}b_{i}^2, m = \sum_{i=1}^{n}a_{i}b_{i}$ Then: $\sum_{i \neq j}a_{i}b_{j} = pq - m, \sum_{i \neq j}a_{i}a_{j} = p^2 - k, \sum_{i \neq j}b_{i}b_{j} = q^2 - l$ So, the required inequality is equivalent to: $(pq-m)^2 \geq (p^2 - k)(q^2-l) \iff lp^2-2qm.p + m^2 + q^2k - kl \geq 0$ If we prove that its discriminant is less than or equal to 0, we are done. That condition can be
written as: $q^2m^2 -l(m^2+q^2k -kl) \leq 0 \iff (lk-m^2)(q^2-l) \geq 0$ The last inequality is true because $q^2-l = \sum_{i \neq j}b_{i}b_{j} \geq 0 (b_{i}$ are positive), and $lk-m^2 \geq 0 $ (Cauchy-Schwarz inequality) The equality holds if and only if $lk - m^2 = 0$ Now, I'm looking for other solutions to prove it, please comment on","['inequality', 'algebra-precalculus', 'contest-math', 'quadratics', 'proof-verification']"
1092851,Is there a closed-form of $ \sum_{n=0}^{\infty }\frac{(-1)^n}{(2n+1)(2n+2)(2n+3)(2n+4)}$,Is there a closed-form of $$\sum_{n=0}^{\infty }\frac{(-1)^n}{(2n+1)(2n+2)(2n+3)(2n+4)}$$ Thanks for any help,['sequences-and-series']
1092852,A necessary and sufficient condition for symmetry of a random variable,"Prove that if $X$ is an integrable random variable, it has a symmetric distribution if and only if: 
$$E(X|X^2)=0$$ 
Can anyone check my solution? Firstly $$EX=0$$
Then we have: $$E(E(X|X^2))=EX=0$$ 
Secondly: 
$$E(X|X^2)=0 $$ 
So: $$E(X)=E(E(X|X^2)=E(0)=0$$ I will be grateful for help!Thanks","['probability-theory', 'conditional-expectation', 'probability', 'random-variables']"
1092859,Find the number of natural numbers,"$N$ is a natural number greater than 1 and less than 100. $F(1), F(2), \dots, F(n)$ are the factors of $N$ in such a way that $1=F(1)< F(2)< F(3)< \dots < F(n)=N$. Further, $D= F(1)*F(2)+F(2)*F(3)+ \dots +F(n-1)*F(n)$. If $D$ is a factor of $N^2$, then how many values of $N$ will be there? I have tried using different angle. But couldn't find the correct approach. Please help.
The answer given is 25.","['discrete-mathematics', 'elementary-number-theory', 'combinatorics']"
1092868,"How prove that $\int_0^\pi {x\,f(\sin x)\,} dx = \frac{\pi }{2}\int_0^\pi {f(\sin x)} \,dx$ [duplicate]","This question already has answers here : Evaluate $\int_0^\pi xf(\sin x)dx$ (2 answers) Closed 9 years ago . To prove that $\int_0^\pi  {x\,f(\sin x)\,} dx = \frac{\pi }{2}\int_0^\pi  {f(\sin x)} \,dx$ is true, first I started calculating the integral of the left indefinitely
$$ \int {x\,f(\sin x)\,\,dx}  $$
using substitution:
$$ \sin x = t,\quad  x = \arcsin t, \quad {dx = \frac{{dt}}{{\sqrt {1 - {t^2}} }}}$$
is obtained:
$$ \int {x\,f(\sin x)\,\,dx} = \int {\arcsin t \cdot f(t) \cdot \frac{{dt}}{{\sqrt {1 - {t^2}} }}}$$
$$ \qquad\quad = \int {\frac{{\arcsin t\,dt}}{{\sqrt {1 - {t^2}} }} \cdot f(t)} $$
Then using integration by parts:
$$ \begin{array}{*{20}{c}}
  {u = f(t)},&{dv = \frac{{\arcsin t\,dt}}{{\sqrt {1 - {t^2}} }}} \\ 
  {du = f'(t)\,dt},&{v = \frac{{{{(\arcsin t)}^2}}}{2}} 
\end{array} $$
then:
\begin{align*}
  \int {x\,f(\sin x)\,\,dx}  &= f(t) \cdot \frac{{{{(\arcsin t)}^2}}}{2} - \int {\frac{{{{(\arcsin t)}^2}}}{2}}  \cdot f'(t)\,dt \\ 
   &= f(\sin x) \cdot \frac{{{x^2}}}{2} - \int {\frac{{{x^2}}}{2} \cdot f'(\sin x)\,\cos x\,dx}  \\ 
\end{align*}
Now, evaluating from 0 to $\pi$
\begin{align}
  \int_0^\pi  {x\,f(\sin x)} \,dx & = \left[ {f(t) \cdot \frac{{{x^2}}}{2}} \right]_0^\pi  - \int_0^\pi  {\frac{{{x^2}}}{2} \cdot f'(\sin x)\,\cos x\,dx} \\
  \int_0^\pi  {x\,f(\sin x)} \,dx & = f(0) \cdot \frac{{{\pi ^2}}}{2} - \int_0^\pi  {\frac{{{x^2}}}{2} \cdot f'(\sin x)\,\cos x\,dx} \qquad ..[1] \\ 
\end{align}
On the other hand, doing the same process with the integral on the right side I get:
\begin{equation}\int_0^\pi  {f(\sin x)} \,dx = f(0) \cdot \pi  - \int_0^\pi  {x \cdot f'(\sin x)\,\cos x\,dx}  \qquad ..[2] \end{equation}
And even here I do not have enough data to say that equality $\int_0^\pi  {x\,f(\sin x)\,} dx = \frac{\pi }{2}\int_0^\pi  {f(\sin x)} \,dx$ is true. Can anyone suggest me what to do with the equalities [1] and [2]? Thanks in advance.","['calculus', 'integration', 'real-analysis']"
1092871,Fixed point and fractional iteration: if $F(k)=k$ then $F^{1\over n}(k)$ is another fixed point of $F$,"My knowledge of the fixed points and iteration equals zero, same for the notation and terminology but I really need to know if this deduction has trivial errors or is really as nice as it seems. I would like to prove the following: Notation 1 - Given a function $f:A\rightarrow A$ define the set $Fix(f)\subseteq A$ as the set of $f$'s fixed points $Fix(f):=\{\phi:f(\phi)=\phi\}$ Notation 2 - Given a function $f:A\rightarrow A$ and the definition of function composition $\circ$ define the function $f^n$ by recursion $i)$ $f^0:=\operatorname{id}_A$ $i)$ $f^{n+1}:=f\circ f^n$ Definition 1 - Given a function $F:X\rightarrow X$, the "" $1\over n$-iterate"" of $F$ is a function $\Psi:X\rightarrow X$ with this property $\forall x(x\in X) (\Psi^n(x)=F(x))$ I guess that we can write $\Psi=F^{1\over n}$ To Prove - If $k\in X$ is a fixed point of $F$ and exists a fucntion $\Psi$ such that $\Psi^n=F$ then $F^{1\over n}(k)$ is a fixed point of $F$ $$k\in Fix(F)\implies \forall n(n\ge1)(F^{1\over n}(k)\in  Fix(F))$$ Proof 1 - For a fixed $n\gt 1$ define $\lambda:=\Psi(k)=F^{1\over n}(k)$ $\lambda:=\Psi (k)=\Psi(\Psi^n(k))$ because $k=\Psi^n(k)$ $\lambda=\Psi^n(\Psi(k))$ because iterates commute $\lambda=\Psi^n(\lambda)$ because $\Psi(k)=\lambda$ by definition Since $\Psi^n=F$ by definiton we conclude that $\lambda=F(\lambda)$ and thus $$\lambda\in Fix(F)$$ Anyways this proof seems weird to me... I feel like there is something missing: I want to prove that for every natural number (greater than zero), if $F^{1\over n}$ exists,  $F^{1\over n}(k)$ is a fixed point so maybe I need to use induction but I really don't know how I could do it Questions $1)$ - Is this proof correct? If yes and it is a known result, can you
   add some info about it? $2)$ - Is it possible to use induction for the proof? Or it is useless? $3)$ - If the proof is correct, is this a result that can be
   strengthened? In fact it seems to me that the real generalized result
   would be something like $$k\in Fix(F)\implies \forall q(q\in\Bbb
 Q\land 0\lt q \lt 1)(F^{q}(k)\in Fix(F))$$","['fixed-point-theorems', 'proof-verification', 'functions', 'analysis']"
1092874,When is $(x-1)(y-1)(z-1)$ a factor of $xyz-1$?,"Let $x$, $y$, $z$ be three natural numbers such that $1< x< y< z$. For how many sets of values of $(x,y,z)$, is $(x-1)(y-1)(z-1)$ a factor of $xyz-1$? I noticed that $(x-1)(y-1)(z-1)=(xyz-1)-z(x+y-1)-xy+x+y$. But i don't know how to proceed from here. Any clues?","['elementary-number-theory', 'discrete-mathematics']"
1092895,Joint measure from two Markov kernels,"Let $(X,\mathcal{X})$ and $(Y,\mathcal{Y})$ be two measurable spaces (Polish if you wish), let $K$ be a Markov kernel from $(X,\mathcal{X})$ to $(Y,\mathcal{Y})$ and let $K'$ be a Markov kernel from $(Y,\mathcal{Y})$ to $(X,\mathcal{X})$. The question is: under which condition the set function $\mu$ from $(X \times Y, \mathcal{X} \otimes \mathcal{Y})$ to $[0,1]$ such that
$$
\mu(A \times B) = \int_{A \times B} K(x,dy) K'(y,dx), \qquad \forall A \times B \in \mathcal{X} \otimes \mathcal{Y},
$$
is a measure? Clearly, if any of the two kernels is actually just a measure on the target space (independent of the point on the source space), then all this is well defined and $\mu$ is a measure. In the general case, I have a bad feeling about the integral itself but I cannot point out a specific reason. Any direction would be welcomed!","['probability-theory', 'measure-theory']"
1092909,Proving $f(C\cap D)=f(C)\cap f(D)$ and $f(C\cup D)=f(C)\cup f(D)$,"Let $A,B$ be sets, $f:A\to B$ a function. Prove/disprove: $\forall C,D\subseteq A$: $f(C\cup D)=f(C)\cup f(D)$ $f(C\cap D)=f(C)\cap f(D)$ I disproved both but I'm not sure it's right: Let $C=\{1\}, D=\{2,3\}, A=\{1,2,3\}$ and $f(X)=\begin{cases}1 &,|X|\le 1\\ 
                                                               2 &,|X|\ge2  \end{cases}$ So $C\cup D=\{1,2,3\} so $ $f(C\cup D)=2$ but $f(C)\cup f(D)=\{1,2\}$ Let $C=\emptyset, D=\{1\}, A=\{1,2,3\}$ and $f(X)=\begin{cases}1 &,|X|=0\\ 
                                                               2 &,|X|>0  \end{cases}$ So $C\cap D=\{1\}$ so $f(C\cap D)=1$ but $f(C)\cap f(D)=2$","['elementary-set-theory', 'functions']"
1092918,Proving measurability of a function only by checking generating sets,"Theorem:
Suppose that $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ are measurable spaces and $\mathcal{B}$ = $\sigma(\mathcal{G})$ is generated by a family $\mathcal{G}\subset\mathcal{P}(Y)$. Then $f : X \rightarrow Y$ is measurable if and only if $f^{-1}(G)\in \mathcal{A}$ for every $G \in \mathcal{G}$. The proof goes as follows: Set operations are natural under pull-backs, meaning that $f^{-1}(Y\setminus B) = X \setminus f^{-1}(B)$ and $\displaystyle f^{-1}\left(\bigcup_{i=1}^{\infty} B_i\right) = \bigcup_{i=1}^{\infty}f^{-1}(B_i)$, $\displaystyle f^{-1}\left(\bigcap_{i=1}^{\infty}B_{i}\right) = \bigcap_{i=1}^{\infty}f^{-1}(B_{i}).$ It follows that $\mathcal M = \{ B \subset Y : f^{-1}(B) \in \mathcal{A}\}$ is a $\sigma$-algebra on $Y$. By assumption, $\mathcal{M} \supset \mathcal{G}$ and therefore $\mathcal{M} \supset \sigma(\mathcal{G}) = \mathcal{B}$, which implies that $f$ is measurable. Now I got a few questions: 1) How do we know $\mathcal M$ is a $\sigma$-algebra on $Y$? 2) ""By assumption, $\mathcal{M} \supset \mathcal{G}$"" - why? $\mathcal M$ is just a sigma algebra on $Y$, one of many, it doesn't have to contain every subset of $Y$, so we don't know if it contains $\mathcal{G}$. 3) Why $\mathcal{M} \supset \sigma(\mathcal{G}) = \mathcal{B}$ implies that $f$ is measurable? Please explain as simple as possible.",['measure-theory']
1092938,Orthogonal decomposition in Hilbert spaces,"Corollary 2.76 (Orthogonal decomposition). Let H be a Hilbert space,
  and let $Y \subset H$ be a closed subspace. Then $Y^\bot $ is a closed subspace with
  $$H = Y \oplus Y^\bot$$,
  meaning that every element $h \in H$ can be written in the form
  $$h = y + z$$
  with $y \in Y$ and $z \in Y^\bot$ and $y$ and $z$ are unique with these properties The uniqueness part is easily shown. But for the existance part it's said that we want to use the unique approximation within a closed convex set (i.e $\forall w\in V \exists ! v_o\in K \text{ s.t. } \|w-v_0\|=\inf_{k\in K}\|k-v_0\|$). The argumentation goes as follows: Fix $h \in H$, and
  apply Theorem 2.73 with $K = Y$ to find a point $y \in Y$ that is closest to $h$.
  Let $z := h  y$, so that for any $v \in Y$ and any scalar $t$ we have (noting that the first inequality holds because $(tv+y)\in Y$ and is strict for all $t\ne0$ by the lemma and the second equality holds for the properties of the inner product) 
  $$\|z\|^2\le\|h-(tv+y)\|^2=\|z-tv\|=\|z\|^2-2\Re(t\langle v,z\rangle)+|t|^2\|v\|^2$$ (And now the problematic part) However , this shows that
  $$\Re(t\langle v,z\rangle)=0$$
  for all scalars $t$ and $v \in Y$ , and so
  $$\langle v,z\rangle=0$$ How should I interpret that ""However"" ? Where should I use the fact that this particular $y$ is the unique closest in $Y$? Many thanks in advance","['inequality', 'functional-analysis']"
1092945,"Asking for Various proofs of uncountability of $[0,1]$","How many different proofs are there of the uncountability of the set $[0,1]$ ? I know of the nested interval proof and the Baire Category theorem proof ; please suggest other proofs . Thanks in advance . $EDIT$ : Is there a proof of this uncountabilty using Shroder-Bernstein Theorem ?","['reference-request', 'elementary-set-theory', 'real-analysis']"
1092952,King and knight moving on an infinite chess board,"There are actually two separate problems: King problem: How many squares can a king moving on an infinite chess board reach in
  N moves? Knight problem: How many squares can a knight moving on an infinite chess board reach
  in N moves? I solved the first one, but the second one seems much more difficult.","['infinite-games', 'combinatorics']"
1092968,Random signs - a remark of david williams:probability with martingales,"This can be found in David Williams p.113-114. Suppose that $(a_n)$ is a sequence of real numbers and that $(\epsilon_n)$ is a sequence of IID random variables with $P(\epsilon_n=\pm1)=\frac{1}{2}$. The result of 12.2 shows that $(1) \sum \epsilon_na_n \text{ converges a.s. if and only if} \sum a_n^2<\infty $ and $(2)\sum\epsilon_na_n \text{ oscillates infinitely if and only if} \sum a_n^2=\infty $ It's very clear to me why the first equivalence is true. Just the usage of 12.2 in David Williams which states that: (12.2) $\sum X_k \text{ converges a.s. if and only if} \sum Var(X_k)<\infty$ for a sequence of independent zero mean random variables $(X_k)$. I want to understand now why the second equivalence is true. For $""\Rightarrow""$: If $\liminf_{n\rightarrow \infty}\sum \epsilon_na_n\neq \limsup_{n\rightarrow \infty}\sum \epsilon_na_n$ then $\sum \epsilon_na_n$ does not converge and theorem 12.2 yields that $\sum Var(\epsilon_na_n)=\sum a_n^2=\infty$ as desired. But now I don't how to show the other direction. Can someone help?","['probability-theory', 'almost-everywhere', 'probability', 'expectation']"
1092969,Changing the cartesian coordinate system,"The cartesian coordinate in 3D is given as: Are we allowed to make our own coordinate system (switching axes around). The question is can we change the axes around? ** Like: **LOOKING AT THE FIRST ONE: Change the $z$-axis to the $x$-axis? or change the $y$-axis to the $z$-axis?? Also in 2D for example: Can the $y$-axis be replaced with the $x$-axis? If so then suppose we must graph: $$y = x^2$$ With $y$-vertical it would be upface parabola, but with $x$-vertical it would be not be the same parabola would it? Then how do we do this? Question 2: Is by definition, the dependent variable vertical and the independent variable horizontal?? Thanks!",['algebra-precalculus']
1092995,Are eigen spaces orthogonal?,Let $A$ be a $N$ x $N$ matrix which has $k < N$ distinct eigenvalues. Are eigenspaces corresponding to different eigenvalues orthogonal in general ? I know it is true if $A$ is normal matrix. But can't prove in general.,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1093017,"If $u \in L^1(0,\infty)$, then $|u(x)| \to 0$ as $x \to \infty$?","Let $u \in L^1(0,\infty)$. Does this mean necessarily that $|u(x)| \to 0$ as $x \to \infty$? I think it has to decay otherwise the integral will be infinite. Can I get a hint on how to prove this? Thank you.","['functional-analysis', 'lp-spaces']"
1093047,Weak/strong law of large numbers for dependent variables with bounded covariance,"Let $(X_i)_{i\in\mathbb{N}}$ be a sequence of $L^2$ random variables with expected value $m$ for all $n$. Let $S_n=\sum_{i=1}^n X_i$ and $|\mathrm{Cov}(X_i,X_j)|\leq\epsilon_{|i-j|}$ for finite, non-negative constants $\epsilon_k$. Show that: (1) If $\lim_{n\to\infty} \epsilon_n=0$ then $S_n/n\to m$ in $L^2$ and probability (2) If $\sum_{k=1}^\infty \epsilon_k<\infty$, then $\mathrm{Var}(S_n/n)$ is of order $O(1/n)$ and $S_n/n\to m$ almost surely (1) First of, I found a similar looking question here , but we don't have, that the constants are bounded by $1$, so I don't know if the Chebyshev-inequality approach works here. (2) We have $\mathrm{Var}(S_n/n)=\dfrac{1}{n^2}Var(S_n)=\dfrac{1}{n^2}\sum_{i\ne j}\mathrm{Cov}(X_i,X_j)=\dfrac{1}{n^2}(\sum_{k=1}^n\mathrm{Cov}(X_i,X_i)+\sum_{i=1}^{n-1}\sum_{j=i}^n \mathrm{Cov}(X_i,X_j))\leq\dfrac{1}{n^2}(n\cdot\epsilon_0+\sum_{i=1}^{n-1}\sum_{j=i}^n \mathrm{Cov}(X_i,X_j))$. This is where I'm stuck.","['convergence-divergence', 'probability-limit-theorems', 'probability-theory', 'law-of-large-numbers', 'covariance']"
1093091,Substitution for definite integrals,"In my experience, Calculus II students dislike changing bounds in definite integrals involving substitution.  When facing an integral like
$$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx,$$
for example, most US Calc II students would introduce $u=x^2$ and compute
\begin{align}
  \int x \sin \left(x^2\right)dx &= \frac{1}{2}\int \sin(u) \, du \\
  &= -\frac{1}{2} \cos(u)+c = -\frac{1}{2} \cos(x^2)+c.
\end{align}
Afterward, they would conclude that
$$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right) \, dx = -\frac{1}{2} \cos(x^2) \big|_0^{\sqrt{\pi}} = 1.$$
I would generally encourage them to write
\begin{align}
  \int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx &= \frac{1}{2}\int_0^{\pi} \sin(u) \, du \\
  &= -\frac{1}{2} \cos(u) \big|_0^{\pi} = 1.
\end{align} This question expresses opinion of a typical such student and this answer correctly expresses the fact that the two step process favored my most calculus students is actually more work . I think there's more to it than this, though.  Specifically, the identity
$$\int_a^b f(g(x)) g'(x) \, dx = \int_{g(a)}^{g(b)} f(u) \, du$$
is a relationship between definite integrals which could have applications other than symbolic evaluation of the integral on the left.  In this case, the change of the bounds of integration is important in its own right.  Thus my question: What are some important applications of change of variables in definite integration, other than symbolic evaluation? I have at least one answer but would be happy to hear more, particularly those that are easily understandable by Calc II students, as I think it's an important pedagogical question.","['definite-integrals', 'education', 'calculus', 'integration']"
1093104,Isomorphism of semidirect products [D&F],"I want to solve the following problem from Dummit & Foote's Abstract Algebra text (p. 184 Exercise 6): Assume that $K$ is a cyclic group, $H$ is an arbitrary group and $\varphi_1$ and $\varphi_2$ are homomorphisms from $K$ into $\text{Aut}(H)$ such that $\varphi_1(K)$ and $\varphi_2(K)$ are subgroups of $\text{Aut}(H)$. If $K$ is infinite assume $\varphi_1$ and $\varphi_2$ are injective. Prove by constructing an explicit isomorphism that $H \rtimes_{\varphi_1} K \cong H \rtimes_{\varphi_2} K$ (in particular, if the subgroups $\varphi_1(K)$ and $\varphi_2(K)$ are equal in $\text{Aut}(H)$, then the resulting semidirect products are isomorphic). [Suppose $\sigma \varphi_1(K) \sigma^{-1}=\varphi_2(K)$ so that for some $a \in \mathbb{Z}$ we have $\sigma \varphi_1(k) \sigma^{-1}=\varphi_2(k)^a$ for all $k \in K$. Show that the map $\psi:H \rtimes_{\varphi_1} K \to H \rtimes_{\varphi_2} K$ defined by $\psi((h,k))=(\sigma(h),k^a)$ is a homomorphism. Show $\psi$ is bijective by constructing a 2-sided inverse.] My attempt: Let $K=\langle x \rangle$, and suppose $\sigma \in \text{Aut}(H)$ conjugates $\varphi_1(K)$ to $\varphi_2(K)$, that is $$\sigma \varphi_1(K) \sigma^{-1}=\varphi_2(K) \tag{1}.$$ From this we find
$$\sigma \varphi_1(x) \sigma^{-1}=\varphi_2(x^a) \tag{2} $$
for some $a \in \mathbb{Z}$. Since all elements of $K$ are powers $x^n$ of $x$, raising this equality to the $n$-th power gives
$$\forall k \in K:\sigma \varphi_1(k) \sigma^{-1}=\varphi_2(k^a)=\varphi_2(k)^a \tag{3}.$$ We now prove that the suggested $\psi$ is a group homomorphism: \begin{equation}
\begin{split}
\psi((h_1,k_1)(h_2,k_2))&=\psi(h_1 \varphi_1(k_1)(h_2),k_1k_2)=(\sigma(h_1 \varphi_1(k_1)(h_2)),(k_1k_2)^a)\\
&=(\sigma(h_1)(\sigma \varphi_1(k_1))(h_2),k_1^a k_2^a)=(\sigma(h_1)(\varphi_2(k_1^a) \sigma)(h_2),k_1^ak_2^a)\\
&=(\sigma(h_1),k_1^a)(\sigma(h_2),k_2^a)=\psi((h_1,k_1))\psi((h_2,k_2))
\end{split}
\end{equation} Where we have used the fact that $K$ is abelian, alongside with the homomorphism law for $\sigma$ and equation $(3)$. We're left with showing that $\psi$ has a 2-sided inverse, which will be done in two cases: Assume $K=\langle x \rangle$ is infinite cyclic. Property $(3)$ gives 
$$\varphi_2(K)=\varphi_2(K^a) \tag{4}$$
where $K^a$ is the image of $K$ under the $a$-th power homomorphism $K \to K:k \mapsto k^a$. Since $\varphi_2$ is injective, this is only possible if the $a$-th power homomorphism is surjective, which happens iff $a=\pm 1$. We can thus see that
$$\chi((h,k))=(\sigma^{-1}(h),k^a)$$
is a 2-sided inverse of $\psi$. Assume $K=\langle x \rangle \cong Z_n$ is finite cyclic of order $n$, and denote the orders of the cyclic groups $\varphi_1(K),\varphi_2(K)$ by $m$. I believe that $(a,n) = 1$ so that there is some integer $b$ such that $ab \equiv 1 \pmod{n}$. If we have this, we can see that
$$\chi((h,k))=(\sigma^{-1}(h),k^b) $$
is a 2-sided inverse of $\psi$. However, all I could show is the following. Obviously, $m|n$. Since $\varphi_1(K)=\langle \varphi_1(x) \rangle$ and $\varphi_2(K)=\langle \varphi_2(K) \rangle$ we have $|\varphi_1(x)|=|\varphi_2(x)|$. According to equation (3) $\varphi_2(K)$ is also generated by $\varphi_2(x^a)$, so that $|\varphi_2(x^a)|=|\varphi_2(x)|$ which gives $(a,m)=1$. Raising equation $(2)$ to the power of $|x^a|=\frac{n}{(a,n)}$ gives $1=\varphi_2(1)=\varphi_2((x^{a})^{\frac{n}{(a,n)}})=\varphi_2(x^a)^{\frac{n}{(a,n)}}$, so  that $m| \frac{n}{(a,n)}$. From the first occurrence of ""$a$"" in equation $(2)$, and the fact that $\varphi_2(x^a)=\varphi_2(x)^a$ we can see that $a$ may shifted by any multiple of $m$. My questions: Are there any flaws with my proof? It seems that in the infinite case, it is only necessary to assume one of the $\varphi_i$'s to be injective. Is this true? Is it possible to prove that ""$a$"" can be chosen coprime with $n$? Thank you!","['semidirect-product', 'group-theory', 'abstract-algebra']"
1093132,Easiest way to calculate $ \int_{0}^{1} \frac{ \log (1+x)}{x} dx$,"What is the easiest way to calculate $$ \int_{0}^{1} \frac{ \log (1+x)}{x}\, dx$$ ? Need a hint.","['definite-integrals', 'integration']"
1093151,Visualizing Mathematical Objects - Tips & Tricks,"It has been a while since I am kind of stuck with my skills concerning the visualization of mathematical objects. Here there is the problem. First of all, let me point out that I am completely self-taught. In other words, this actually means that this site is the only possibility I have to speak mathematics, which is a bit like trying to learn japanese by rarely pronouncing some utterance to a random native speaker hoping not to sound too idiotic. In second place, I am trying to move towards rather abstract things, and from time to time I do have the feeling I have a decent grasp of what is going on. However There are some objects that I simply dont know how to approach! I think the problem can be rephrased in terms of extensive vs. intensive definition of a given mathematical object. I always tend to look for an extensive definition, but when you start to deal with real analysis, topology or measure theory those kind of definitions, that quite literally show how an object looks like, simply start to rarify. Thus, when I find a definition of lets say $\ell^p$, or $\mathbb{N}^{\mathbb{N}}$, or Borel Sets, then I really fail to see what is going on. Maybe I can actually manipulate the symbols, and even get a proof or something that could almost look like a proof, but still, in most of the cases I have no idea what is going on. [Small aside: this is actually interesting. As long as definitions and objects are sort of trivial, there are ""pictures"" or advice to visualize objects, but when you get to the level of those objects, it is assumed that the reader/learner somewhere has got the skills to visualize those structures.] If you want an analogy, it is a bit like being in a completely dark room, with various objects and the explicit task to make them fit perfectly. I can do it (from time to time), but this does not imply that I see what the single objects where at the beginning, and how they look like now that they are assambled.
Said so, here it comes the questions. How do you actually perceive or visualize the mathematical objects? How did your teachers/professors/supervisors taught you to visualize them? How do you teach your students to visualize mathematical objects? What are the tricks? There is something else that should be added, and that I think is related to the fact that I am self-taught. A lot of books of advanced maths simply discard pictures, even if (I suppose) maybe the authors were implicitly referring to them when they started to learn the topics. But here there is the point of not -being self taught: there is somebody who gives you this tricks (if you dont like the word, we can use "" upaya "" instead) to build, and then erase every trace of the use of those hints. Thank you for your time and for any feedback that could come! PS: For those who are wondering, I added the real-analysis and measure-theory tags, because I would love to actually see the objects I was referring in the text, and to get tips and hints on how to do it. EDIT: After the nice feedback from user86418, I would like to point out something. The idea behind the question is not to head to the psychology of mathematics , or in other words how each user see things in her/his own mathematical world. Actually, the idea is to look for the general tips that are shown on the whiteboard (quite literally!) to visualize objects. An example could be (if I remember correctly history of math) the Argan plane: Gauss was the first to get the idea, but he erased any mention to it, and people were kind of stuck to visualize properly complex numbers for a while!","['soft-question', 'measure-theory', 'self-learning', 'real-analysis']"
1093155,Show that $d+1$-dimensional Lebesgue measure of set $G$ equals $0$,"Let $D \subset \mathbb{R}^d$ and let $f:D \rightarrow \mathbb{R} $ be measurable function.
Let $G=\{(x_1,x_2,\ldots,x_d,f(x_1,x_2,\ldots,x_d))\in \mathbb{R}^{d+1}:(x_1,x_2,\ldots,x_d)\in D \} $ be the graph of $f$. Show that $d+1$-dimensional Lebesgue measure of $G$ equals $0$. I honestly don't know how to bite that. I thought of using an integral and applying Fubini's theorem, but I don't even know how to start. I would appreciate any hints.","['lebesgue-integral', 'measure-theory', 'lebesgue-measure', 'analysis']"
1093167,Number of integral solutions to a polynomial,"Given a polynomial of $n$th order, represented by $$f(x)=a_{0}x^{n}+a_{1}x^{n-1}+a_{2}x^{n-2}+\cdots+a_{n-2}x^{2}+a_{n-1}x+a_{n}=0$$ Is it possible to find the number of integral solutions/roots to any general polynomial like this?","['roots', 'algebra-precalculus', 'diophantine-equations', 'polynomials']"
1093199,Is $\mathbb{P}^{1}$ a fine moduli scheme?,"I want to show that $\mathbb{P}^{1}_{\mathbb{C}}$ is a fine moduli scheme for the families of lines through the origin of the affine plane. I took a flat family $\mathcal{D}\rightarrow B$ and I tried to associate to it a morphism $B\rightarrow \mathbb{P}^{1}_{\mathbb{C}}$ in order to prove that the moduli functor is representable, but right now I'm a little bit stuck because of the big generality (almost for me) of the request. Can someone give me some help? Thank you in advance!","['algebraic-geometry', 'moduli-space']"
1093240,Where is the border between functional analysis and real analysis?,"I always thought that real analysis deals with analysis on the real line, eventually on the Euclidean space $\mathbb R^n$. But why does someone have to label a course as real analysis when it is obviously an generalization and extension of the above to functions on more abstract spaces? Isn't it functional analysis then? Take $L^p$ space, for example. How can it be in both real and functional analysis?","['terminology', 'functional-analysis', 'real-analysis']"
1093263,Please explain definition of determinant using permutations?,"Many people (in different texts) use the following famous definition of the determinant of a matrix $A$: \begin{align*} \det(A) = \sum_{\tau \in S_n}\operatorname{sgn}(\tau)\,a_{1,\tau(1)}a_{2,\tau(2)} \ldots a_{n,\tau(n)}, \end{align*} where the sum is over all permutations of $n$ elements over the symmetric group. None of them actually explains how one interprets this definition, so this makes me suspicious and think they don't know it either. This is what I understand so far: Definition: A permutation $\tau$ of $n$ elements is a bijective function having the set $ \left\{1, 2, ..., n\right\}$ both as its domain and codomain. The number of permutations of $n$ elements, and hence the cardinality of the set $S_n$ is $n!$ So for example, for every integer $i \in \left\{1, 2, ..., n\right\}$ there exists exactly one integer $j \in \left\{1, 2, ..., n\right\}$ for which $\tau(j) = i$. Permutations can also be represented in matrices, for example if $\tau(1) = 3, \tau(2) = 1, \tau(3) =4, \tau(4) =5, \tau(5) =2$, then \begin{align*} \tau = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & 1 & 4 & 5 & 2 \end{pmatrix}. \end{align*} Definition: Let $\tau \in S_n$ be a permutation. Then an inversion pair $(i,j)$ of $\tau$ is a pair of positive integers $i, j \in \left\{1, 2, ..., n\right\}$ for which $i < j$ but $\tau(i) > \tau(j)$. This determines how many elements are 'out of order'. For example if $\tau = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 3 & 2 \end{pmatrix}$, then $\tau$ has one single inversion pair $(2,3)$, since $\tau(2) = 3 > \tau(3) = 2$. Definition: A transposition , called $t_{ij}$, is the permutation that interchanges $i$ and $j$ while leaving all other integers fixed in place. The numbers of inversions in a transposition is always odd, because one can compute that the number of inversion pairs in $t_{ij}$ is exactly $2(j-1)-1$. Definition: Let $\tau \in S_n$ be a permutation. Then the sign of $\tau$, denoted by sign$(\tau)$ is defined by \begin{align*} sign(\tau) = (-1)^{\text{# of inversion pairs in}\ \tau} \end{align*} This is $+1$ if the number of inversions is even, and $-1$ if the number is odd. Every transposition is an odd permutation. This is all clear to me, but can someone explain to me, in an understandable fashion, how one interprets the definition of the determinant on the basis of all this information? That would be greatly appreciated (not only by me, but I think by many others aswell). For example: what do I make of the $a_{1,\tau(1)}$ etc. in the definition of the determinant, all the way up to $n$? What do they represent?","['permutations', 'linear-algebra', 'determinant']"
1093266,Numbers $n$ such that the digit sum of $n^2$ is a square,"Let $S(n)$ be the digit sum of $n\in\mathbb N$ in the decimal system. About a month ago, a friend of mine taught me the following: $$S\left(9\color{red}{^2}\right)=S(81)=8+1=3\color{red}{^2}$$ $$S\left(10\color{red}{^2}\right)=S(100)=1+0+0=1\color{red}{^2}$$ $$S\left(11\color{red}{^2}\right)=S(121)=1+2+1=2\color{red}{^2}$$ $$S\left(12\color{red}{^2}\right)=S(144)=1+4+4=3\color{red}{^2}$$ $$S\left(13\color{red}{^2}\right)=S(169)=1+6+9=4\color{red}{^2}$$ $$S\left(14\color{red}{^2}\right)=S(196)=1+9+6=4\color{red}{^2}$$ $$S\left(15\color{red}{^2}\right)=S(225)=2+2+5=3\color{red}{^2}$$ Then, I've got the following: For every $m\in\mathbb N$ , each of the following $7$ numbers is a square. $$S\left(\left(10^{(3m-2)^2}-1\right)^2\right),S\left(\left(10^{(3m-2)^2}\right)^2\right),\cdots,S\left(\left(10^{(3m-2)^2}+5\right)^2\right)$$ However, I'm facing difficulty in finding such $8$ consecutive numbers. So, here is my question: Question : What is the max of $k\in\mathbb N$ such that there exists at least one $n$ which satisfies the following condition? Condition : Each of the following $k$ numbers is a square. $$S\left((n+1)^2\right),S\left((n+2)^2\right),\cdots,S\left((n+k-1)^2\right),S\left((n+k)^2\right)$$ Note that we have $k\ge 7$ . Can anyone help? Added : A user Peter found the following example of $k=8$ : $$S\left(46045846^2\right)=8^2,S\left(46045847^2\right)=7^2,\cdots,S\left(46045852^2\right)=7^2,S\left(46045853^2\right)=8^2$$ Hence, we have $k\ge 8$ .","['square-numbers', 'decimal-expansion', 'number-theory']"
1093278,"Given positive numbers $a, b, c, x, y, z$, such that $a + x = b + y = c + z = S$, prove that $ay + bz +cx < S^2$","Given positive numbers $a, b, c, x, y, z$, such that $a + x = b + y = c + z = S$, prove that $ay + bz +cx < S^2$ One solution is: Denote $T = S/2$. One of the triples $(a, b, c)$ and $(x, y, z)$ has the property that at least two of its members are greater than or equal to $T$ . Assume that $(a, b, c)$ is the one, and choose $\alpha = a - T, \beta = b-T$ and $\gamma = c-T$. We then have $x = T-\alpha, y = T - \beta$ and $z= T - \gamma$. Now the required inequality is equivalent to $$
(T+\alpha)(T-\beta)+(T+\beta)(T-\gamma)+(T+\gamma)(T-\alpha) < 4T^2
$$ After simplifying we get that what we need to prove is $$
-(\alpha \beta + \beta \gamma + \gamma \alpha) <T^2  \ \ \ \ \ \ (1)
$$ We also know that at most one of the numbers $\alpha, \beta, \gamma$ is negative. If all are positive, there is nothing to prove. Assume that $\gamma < 0$. Now (1) can be rewritten as $-\alpha \beta -\gamma(\alpha+\beta) < T^2$. Since $-\gamma < T$ we have that $-\alpha\beta - \gamma(\alpha+\beta) < -\alpha\beta +T(\alpha+\beta)$ and the last term is less tham $T$ since $(T-\alpha)(T-\beta)>0$ Now, I'm looking for other solutions to prove it, please comment on","['inequality', 'algebra-precalculus', 'proof-verification', 'contest-math']"
1093297,Finding value of 1 variable in a 3-variable $2^{nd}$ degree equation,"The question is: If $a,b,\space (a^2+b^2)/(ab-1)=q$ are positive integers, then prove that $q=5$. Also prove that for $q=5$ there are infinitely many solutions in $\mathbf N$ for $a$ and $b$. I simplified the equation as follows:-$$\frac {a^2+b^2}{ab-1}=q\\\begin{align}\\&=>\frac {2a^2+2b^2}{ab-1}=2q\\&=>\frac{a^2+b^2+2ab+a^2+b^2-2ab}{ab-1}=2q\\&=>(a+b)^2+(a-b)^2=2q(ab-1)\\&=>2(a+b)^2+2(a-b)^2=q(4ab-4)\\&=>2(a+b)^2+2(a-b)^2=q((a+b)^2-(a-b)^2-4)\end{align}$$Substituting $a+b=X$ and $a-b=Y$, we get $$2X^2+2Y^2=q(X^2-Y^2-4)\\\begin{align}&=>(q-2)X^2=(q+2)Y^2+4q\end{align}$$Now using the quadratic residues modulo $5$, I know that $X^2,Y^2\equiv0, \pm1(mod\space 5)$. But using this directly doesn't give the answer. So what to do after this? An answer without the use of co-ordinate geometry would be greatly appreciated as it seems there is a very good resemblance of the equation to a pair of hyperbolas which are symmetric with respect to the line $y=x$ but I don't understand co-ordinate geometry very well.","['elementary-number-theory', 'algebra-precalculus']"
1093323,What is the difference between an undulation point and other critical values?,"A necessary but not sufficient condition for a point of inflection is that $$f''(x)=0$$ If the second derivative is 0 and the point is not a point of inflection, Wikipedia tells me that is called an undulation point , which apparently means a point on a curve where the curvature vanishes but does not change sign. An example given is $f(x)=x^4$ at $(0,0)$. What I do not understand is why $(0,0)$ is not classified as a local minimum? Surely for $f(x)=x^4$ the first derivative changes polarity either side of $(0,0)$. Is there a better example of an 'undulation point' for a smooth function? Do 'undulation points' exist for more than one variable or is the condition that all partial derivatives are zero a necessary and sufficient one?","['multivariable-calculus', 'calculus']"
1093373,Question about quotient of a compact Hausdorff space,"I am reading the book 'Algebraic Topology' by Tammo Tom Dieck. On page 12 in the proposition 1.4.4 he states that : Let $X$ be a compact Hausdorff space and $f : X \rightarrow Y$ be a quotient map. Then the following assertions are equivalent : (1) Y is a Hausdorff space, (2) $f$ is closed, (3) $R=\{ (x_1,x_2)|f(x_1)=f(x_2)\}$ is closed in $ X \times X$. I am able to prove that (1) implies (2) and that (1) implies (3) but not able to prove the other implications. I will appreciate any help. Thinking about this question, a related issue comes up. We all know that compact subsets of Hausdorff spaces are closed. Is it true that if all compact subspaces of a space are closed then the space is Hausdorff ?","['general-topology', 'quotient-spaces', 'separation-axioms', 'compactness']"
1093374,How does an Iterated Integral Work?,"I am simply confused because of planes now. Consider: $$J = \int_{R}\int_{R} e^{-(x^2 + y^2)} dxdy$$ What is the geometrical aspect of this integral? This represents the volume under $h(x,y) = e^{-(x^2 + y^2)}$ I believe. Then how is: $$I = \int_{R} e^{-x^2} dx = \int_{R} e^{-y^2} dy$$? How does that statement hold true? Which are independent/dependent variables? I am utterly confused.","['multivariable-calculus', 'calculus', 'integration', 'real-analysis', 'analysis']"
1093431,Prove: A group of order 315 with a normal 3-Sylow group is Abelian. [duplicate],"This question already has answers here : On Groups of Order 315 with a unique sylow 3-subgroup . (3 answers) Closed 9 years ago . Prove: A group of order 315 with a normal 3-Sylow group is Abelian.
I know that $315 = 3^2\times5\times7$ I've tried using Sylow's theorems on 5, 7 but haven't got anywhere meaningful. I would appreciate any help.","['sylow-theory', 'group-theory', 'abstract-algebra']"
1093450,Calculating the second cohomology group for trivial group action,"Let $G$ be a finite group acting trivially on $\mathbb{R}^*$. How can I compute $H^2(G,\mathbb{R}^*)$? It seems that direct calculations are somewhat hopeless, but the answer should be simple anyway.","['homology-cohomology', 'homological-algebra', 'group-cohomology', 'finite-groups', 'group-theory']"
1093458,"Dirichlet's integral $\int_{V}\ x^{p}\,y^{q}\,z^{r}\ \left(\, 1 - x - y - z\,\right)^{\,s}\,{\rm d}x\,{\rm d}y\,{\rm d}z$","I found such an exercise: Calculate the Dirichlet's integral: $$
\int_{V}\ x^{p}\,y^{q}\,z^{r}\
\left(\, 1 - x - y - z\,\right)^{\,s}\,{\rm d}x\,{\rm d}y\,{\rm d}z
\quad\mbox{where}\quad p, q, r, s >0
$$
and $V=\left\{\,\left(\, x,y,z\,\right) \in {\mathbb R}^{3}_{+}:
x + y + z\ \leq\ 1\right\}$ I thought that I could put $x + y + z = \alpha$. I got a clue, that it is a correct approach, but I should also put $y + z = \alpha\beta$ and $z=\alpha\beta\gamma$. So: $z=\alpha\beta\gamma\,,\quad y=\alpha\beta\left(\, 1 - \gamma\,\right)\,,\quad x=\alpha\left(\, 1 - \beta\,\right)$ Should I change $x,y,z$ under the integral sign to $\alpha,\beta,\gamma$ now ?.","['definite-integrals', 'multivariable-calculus', 'integration']"
1093470,"Using the definition of limit, prove $\frac{x+1}{x+2}\to 1$ as $x\to\infty$",How can I show a limit using the mathematical definition? I always computed limits but never learned how to prove it. $$\lim_{x\rightarrow+\infty}\frac{x+1}{x+2}=1$$,"['epsilon-delta', 'limits-without-lhopital', 'calculus', 'limits']"
1093475,"Deduce that if $G$ is a finite $p$-group, the number of subgroups of $G$ that are not normal is divisible by $p$","Given : Let $G$ be a group, and let $\mathcal{S}$ be the set of subgroups of $G$. For $g\in G$ and $H\in S$, let $g\cdot H=gHg^{-1}$ Question : Deduce that if $G$ is a finite $p$-group, for some prime $p$, the number of subgroups of $G$ that are not normal is divisible by $p$. Comments : The subgroups of $G$ that are normal have the property that $g\cdot H=gHg^{-1}=H$ The question is equivalent to showing $p$ divides $\left|G\right|-\left|\{H\in S\vert gHg^{-1}\neq H\}\right|$ $p$-group: $\forall g\in G,|g|=p^k$ for $k\in\mathbb{N}^+$ I don't know where to start with this problem, been thinking about it for a while to no avail, I feel that there are too many definitions for me to consider when finding a solution. My problems I have considered and not been able to answer are: what is the order of $|G|$? I think it must be of order that is divisible by $p$, hence the question becomes show $p$ divides the order of $\left|\{H\in S\vert gHg^{-1}\neq H\}\right|$. How do you compute the order of this set? It seems like quite a standard problem, so I would really appreciate it if I could be directed to more information about whatever problem it is.","['group-theory', 'p-groups', 'normal-subgroups']"
1093479,Computing a nasty integral (probably with computer algebra system),"I'm trying to do this integral, not sure if it is possible: $$
\int_{1}^{\infty}\int_{0}^{\infty}
\exp\left(\, -\,{x^{2} \over y^{2}}\,\right)
\exp\left(\,-\,{y^{2} \over z^{2}}\,\right)
\exp\left(\, -\left[\,z-1-\log\left(\, z\,\right)\,\right]\,\right)
\,{\rm d}y\,{\rm d}z
$$ I am happy with computer algebra solutions if they exist. So far I have had some issues with Maple, which just returns back the original integral.","['improper-integrals', 'closed-form', 'special-functions', 'integration', 'computer-algebra-systems']"
1093485,Limit of bounded harmonic functions is harmonic,"I am trying to solve this old qual problem: Suppose $\{u_n\}_{n=1}^\infty$ is a sequence of functions harmonic on an open set $U \subset \mathbb{C}$ and uniformly bounded by 1.  Suppose there is a function $u : U \to \mathbb{R}$ such that $u_n \to u$ pointwise.  Show $u$ is harmonic on $U$. I would like to solve this using only elementary complex analysis.  My idea is this: Pick a point $z \in U$ and a disk $D_r(z) \subset U$.  Since the disk is simply connected, each $u_n$ is the real part of some holomorphic $f_n$.  Now, we would like to show that $f_n$ converges uniformly on compact subsets... I haven't gotten any further than this. Will this approach work?  Other ideas which use complex analysis are appreciated.","['harmonic-functions', 'harmonic-analysis', 'complex-analysis']"
1093501,"Did Landau prove that there is a prime on $\bigl(x,\frac65x\bigr)$?","Was Landau the first to prove that there is a prime on $\bigl(x,\frac65x\bigr)$ ? In his Handbuch $\!^1$ discussing the limit $$\lim_{n\to\infty} \bigl(\pi\bigl((1+\epsilon)x\bigr)-\pi(x)\bigr)=\infty  $$ he seems to say that in the next chapter he will prove the relation for all $\epsilon > \frac{1}{5}.$ On the face of it this is the answer and Jitsuro Nagura's proof $^2$ for $\epsilon \geq 1/5$ is not only an improvement but uses methods that Landau felt were exhausted by his proof, which is what I take from
Landau's "" da man doch nicht auf diesem elementaren Wege das Ziel erreichen kann... "" In the following section (21) of the following chapter (50) he derives a constant from sums involving the $\psi$ function and in section 22 proves Bertrand's Postulate. In 23 entitled "" Weitere Verengerung der Schranken "" [further narrowing of bounds] he performs another series of manipulations of $\psi(x)$ and derives that $$(A)\hspace{7mm} \limsup_{x\to\infty}\frac{\psi(x)}{x}\leq\frac{171\cdot 6}{175\cdot 5}a \approx 1.08028 $$ in which $a\approx 0.92129\dots$ and on the same page finds $\psi(x)\geq ax+o(x) \approx 0.92129 x + o(x).$ So for comparison, Nagura obtains $0.916x-2.318 < \psi(x) < 1.086x$ and Landau obtains $0.92129x + o(x) < \psi(x) < 1.08028 x.$ Landau halts his proof after (A), concluding that (A) "" besser als $\frac{6a}{5}$ ist, "" leaving the reader I think a bit of work. I haven't done the calculations yet but if as a quick check we substitute $0.93 x\leq \psi(x)\leq 1.085x$ into Nagura's expressions $^3$ for the difference $\vartheta\bigl(\frac{n+1}{n}x\bigr)-\vartheta(x)$ we get positive values for $ n = 5$ and $x$ near $5000$ . So my question is whether Landau could have said $\epsilon\geq \frac{1}{5}$ using his own methods? Landau and Nagura worked in different circumstances so this is really just curiosity about the nature of Landau's result. $^1$ Handbuch (1909)p. 87. $^2$ Nagura, On the Interval Containing At Least One Prime Number (1952). $^3$ These values are a little worse than those Landau obtained assuming Landau's expression for the difference may involve some loss of numerical leverage over the expression in Nagura's paper.","['prime-gaps', 'math-history', 'number-theory']"
1093507,Solving $\tan x-\tan(2x)=2\sqrt{3}$,"$$\tan x-\tan(2x)=2\sqrt{3}$$ TRY #1 $$\begin{align*}
\tan x-\tan(2x)=2\sqrt{3}&\implies\tan x=2\sqrt{3}+\tan{2x}\\
&\implies \tan^2x=\tan^2(2 x)+4 \sqrt{3} \tan(2 x)+12\\
&\implies\tan^2x=(\frac{2\tan x}{1-\tan^2 x})^2+4\sqrt{3}\frac{2\tan x}{1-\tan^2x}+12
\end{align*}$$ but this will give me an equation with $\tan^4$ which needs quartic formula, too difficult!! TRY #2 $$\begin{align*}
\tan x-\tan(2x)=2\sqrt{3} &\implies \frac{\sin x}{\cos x}-\frac{\sin 2x}{\cos 2x}=2\sqrt3 \\
&\implies\frac{\sin x\cos 2x-\sin 2x\cos x}{\cos x\cos 2x}=2\sqrt{3}\\
&\implies\frac{-\sin x}{\cos x\cos 2x}=2\sqrt{3}\\
&\implies\frac{-\sin x-2\sqrt{3}\cos x\cos 2x}{1}=0
\end{align*}
$$ then i can't!! can anyone help me?",['trigonometry']
1093534,How do I find this limit: $\lim_{n\to\infty} \left[ n-{n\over e}\left(1+{1\over n}\right)^n \right]$?,"I'm working on a practice exam for my masters quals and I am having difficulties with the following limit. According to wolfram alpha, it's value is 1/2. Does anyone know how to find this limit? $$\lim_{n\to\infty} \left[ n-{n\over e}\left(1+{1\over n}\right)^n \right].$$","['exponential-function', 'calculus', 'limits']"
1093574,Differentation uder the integral sign,"Let $F(x)=\int_{\sin x}^{\cos x} e^{x\sqrt{1-y^2}} \, dy $. My task is to calculate $F'(x)$. My idea is to use http://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign and I get: $$F'(x)=\int_{\sin x}^{\cos x}\sqrt{1-y^2} e^{x\sqrt{1-y^2}}\,dy -\sin x \cdot e^{x\sqrt{1-\cos^2x}}-\cos x \cdot e^{x\sqrt{1-\sin^2x}},$$ but now it is not any easier. The integral is still not solved. Have you got any ideas?","['multivariable-calculus', 'calculus', 'integration', 'real-analysis', 'analysis']"
