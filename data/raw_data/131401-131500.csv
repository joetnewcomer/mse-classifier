question_id,title,body,tags
2053624,Does a one-to-one function need to have every element in its domain have a corresponding element in the range?,"The definition of a one-to-one function is such that no two elements in the domain are mapped into the same element in the range. Mathematically: $$
\forall a,b \in A, \;\; f(a)=f(b) \Rightarrow a=b
$$ However I am wondering if there can be elements $a,b \in A$ that are never sent out. In other words, they do not have a corresponding mapping in the range. Is this possible? thanks.","['real-analysis', 'functions']"
2053690,Why are all metrics are essentially equivalent on compact spaces?,"In his book Poincar√©'s Legacies , Terence Tao writes on p. 215: Since all metrics are essentially equivalent on compact spaces, we
  see that <...> What exactly does he mean by that? Could someone give a reference? I had problems finding one.","['big-picture', 'reference-request', 'normed-spaces', 'general-topology', 'metric-spaces']"
2053721,Show that $\sin \left( \frac{\pi}{12} \right) > \frac{1}{4}$,"Show that $\sin \left( \frac{\pi}{12} \right) > \frac{1}{4}$ I'm trying to show this, but I can't seem to get it right. Graphically this equality seems to be intuitive, but I can't see how to prove it. I tried setting $f(x)=\sin x$ and $g(x)=x$ and looking at the way that $f(x)$ decreases along $(0,\pi)$ by looking at its second derivative and comparing that to $g(x)$, but it didn't seem to work that way. Although I was close, it doesn't seem like to me that that method will work. Does anyone have a better way (preferably one that doesn't involve having to physically draw graphs?)","['inequality', 'trigonometry']"
2053733,ENS Paris preparation,"Good morning everyone, I'm a french student who wants to pass the contest of ENS Ulm I need specific problems and exercises, I hope someone would help me here... I know there are many french student in this forum. 
Let's start with this difficult oral which was given in ENS Paris: What is the dimension of vector space of symmetric polynomials with $k$ variables and degree at most $n$ ? I've seen this problem in this forum but no convincing answer were given. Thanks for your help, I'll be so much thankful.","['combinatorics', 'advice', 'linear-algebra']"
2053777,How should we understand change of the basis matrix?,"I'm rather new to linear algebra and my professor isn't very clear, so although this question may seem ""too easy"" for MSE it would be of great help to me! I'm trying to understand the concept behind a change of basis matrix. Let's say we have some $m \times n$ matrix $A$, formed by a basis $\alpha$. This means that all the columns of $A$ are a linear combination of elements of $\alpha$, right? And if we want a change of basis to $\beta$, we're essentially finding $[A]_\beta$, right? And, the thought behind this is ""reforming"" the columns of $A$ with respect to the basis $\beta$? And, to do this, we take the elements of $\beta$ as columns of a new matrix $C$, then multiply it by the coordinates of $A$ w.r.t. $\beta$? Am I on the right track to understanding this, or am I mistaken?","['matrices', 'change-of-basis', 'linear-algebra']"
2053802,"$\sinh(x) - \sin(x) \ge 0 $ for all $x \ge 0$, $x \in \mathbb{R} $","How can I show that $\sinh(x) - \sin(x) \ge 0 $ for all $x \ge 0, x \in \mathbb{R}? $ I couldn't find anything on search. I have tried following: Let $f(x) = \sinh(x) - \sin(x)$. Now we know that $\sinh(x) = (e^x - e^{-x})/2$ so $f(x) = (e^x - e^{-x})/2 - \sin(x) $. Differentiating that we get $f'(x) = (e^x + e^{-x})/2 - \cos(x).$ I'm having trouble showing that $ f'(x) \ge 0$ for all $x \ge 0.$ If we could do that we can argue that the derivative is positive so $f$ is increasing and $f(0) = 0$, therefore $f(x) \ge 0 $ for all $x \ge 0$. We haven't covered any series stuff yet.","['real-analysis', 'trigonometry']"
2053861,"Condition on a,b and c satisfying an equation(TIFR GS 2017)","Let $a,b,c$ be positive real numbers satisfying $$(1+a+b+c)\left(1+\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)=16,$$ then $a+b+c=3$. I thought about the application of the AM-GM-HM inequality, but in vain. I also thought about splitting 16 into factors and comparing but went nowhere. Any ideas. Thanks beforehand.","['algebra-precalculus', 'inequality']"
2053864,"Find all the possible values of $(a,b,c,d)$.","Find all quadruples of real numbers $(a,b,c,d)$ satisfying the system of equations $(b+c+d)^{2010}=3a$ $(a+c+d)^{2010}=3b$ $(a+b+d)^{2010}=3c$ $(a+b+c)^{2010}=3d$ I tried to find the solutions using hit and trial and by using some logic also I find two solutions which are $(0,0,0,0)$ and $(\frac{1}{3},\frac{1}{3},\frac{1}{3},\frac{1}{3})$ I think these are the only solutions that exist.","['number-theory', 'logarithms']"
2053906,How to prove this nice $10$th power identity for $x_1^6+x_2^6+x_3^6 =y_1^6+y_2^6+y_3^6$?,"Ramanujan's 6-10-8 Identity turns out to depend on a special case of,
$$u_1^k+u_2^k+u_3^k =v_1^k+v_2^k+v_3^k$$
simultaneously valid for $k=2,4$. I was investigating if the next system $k=2,6$,
$$x_1^2+x_2^2+x_3^2 =y_1^2+y_2^2+y_3^2\\x_1^6+x_2^6+x_3^6 =y_1^6+y_2^6+y_3^6\tag1$$
would have something similar. I observed empirically that,
$$\left(\sum_{i=1}^3\big(x_i^{10}-y_i^{10}\big)\right)\left(\sum_{i=1}^3\big(x_i^{4}-y_i^{4}\big)\right)^2=20\prod_{i=1}^3\prod_{j=1}^3\big(x_i^2-y_j^2\big)\tag2$$ Example: $$10^k+15^k+23^k = 3^k+19^k+22^k$$
yields,
$$\small \text{LHS}= \big(10^{10} + 15^{10} + 23^{10} - 3^{10} - 19^{10} - 22^{10}\big)\big(10^4 + 15^4 + 23^4 - 3^4 - 19^4 - 22^4\big)^2$$
$$\small \text{RHS}=20(10^2 - 3^2)(10^2 - 19^2)(10^2 - 22^2)(15^2 - 3^2)(15^2 - 19^2)(15^2 - 22^2)(23^2 - 3^2)(23^2 - 19^2)(23^2 - 22^2)$$
$$\small\text{LHS}=\text{RHS}=37739520^2\times3830610$$
I've also tested it with more general parametric solutions and it works just fine. Q: But how do we prove $(2)$ rigorously?","['number-theory', 'diophantine-equations']"
2053947,How to show that $f$ and $\mathcal{F}(f)$ can't both be compactly supported?,"Let $f : \mathbb{R}\to \mathbb{R}$ be continuous, I want to show that it can't happen that both $f$ and its Fourier transform $\mathcal{F}(f)$ are compactly supported unless $f = 0$. That means that I want to show that if both $f$ and $\mathcal{F}(f)$ are compactly supported, $f = 0$. I've seem some questions like this here, but I believe this is not duplicate. That because on those questions I've seem people using the Fourier series , while here I'm talking about the Fourier transform. In the Fourier series we require $f$ to be periodic. Here $f$ needs not to be periodic. In that case, I can't restrict $f$ to $[-\pi,\pi]$ and talk about its Fourier series, because it need not be the case that $f(-\pi)=f(\pi)$, since $f$ is arbitrary. Now, if $f$ is compactly supported, there's an interval $[a,b]$ such that $f(x) = 0$ for all $x\notin [a,b]$. If $\mathcal{F}(f)$ also is compactly supported there's $[c,d]$ such that $\mathcal{F}(f)(\xi)=0$ if $\xi\notin [c,d]$. I've then thought of two approaches: Try to use this together with the Fourier inversion formula, to show that $f(x) = 0$. Try to use this together with Plancherel's theorem to show that $\|f\|_2 =0$ and hence $f = 0$. Now, in the first case I get $$f(x)=\int_{c}^{d}\mathcal{F}(f)(\xi)e^{2\pi i x\xi}d\xi = \int_c^d \int_a^b f(y)e^{-2\pi i y\xi}e^{2\pi ix\xi} dyd\xi$$ Or yet $$f(x)=\int_a^b f(y)\int_c^d e^{2\pi i\xi (x-y)}d\xi dy,$$ but this doesn't seem to lead anywhere. Plancherel's theorem also doesn't seem of great aid here. We have $$\int_a^b |f(x)|^2 dx = \int_c^d |\mathcal{F}(f)(\xi)|^2 d\xi,$$ which doesn't help much indeed. How can I show that if both $f$ and its Fourier transform are compactly supporetd then $f = 0$ in the context of the Fourier transform and not Fourier series ?","['real-analysis', 'integration', 'fourier-analysis', 'analysis']"
2053959,The limit of a function with sum of two roots.,"I need to find the limit of the following function:
$$\lim_{x\to-\infty} \left(\sqrt{x^2+2x}+\sqrt[3]{x^3+x^2}\right).$$
I derived it to the form of:
$$\lim_{x\to-\infty}\frac{(x^2+2x)^3-(x^3+x^2)^2}{\left(\sqrt{x^2+2x}-\sqrt[3]{x^3+x^2}\right)\left((x^2+2x)^2+(x^2+2x)\sqrt[3]{(x^3+x^2)^2}+\sqrt[3]{(x^3+x^2)^4}\right)},$$
hoping that I'd be able to simplify something but still have $\frac{0}{0}$ and don't see how to do that.","['roots', 'limits-without-lhopital', 'functions', 'limits']"
2053980,$\sum_{n=1}^\infty\frac{z^n}{n}$ does not converge uniformly on $\mathbb{D}$.,"It is required to prove that the series $\sum_{n=1}^\infty\frac{z^n}{n}$ does not converge uniformly on the open unit disc centered at $0$, i.e. $\mathbb{D}$. Clearly by virtue of the ratio test the series converges pointwise on  $\mathbb{D}$. However I find it difficult to see why the convergence is not uniform. Could someone please give me a hint?","['complex-analysis', 'uniform-convergence', 'sequences-and-series', 'power-series']"
2054004,How to express $\lfloor(3+\sqrt{3})^n\rfloor$ with a formula?,"We suppose that $n\in\mathbb{N}^*$ Now I can express $\lfloor(2+\sqrt{3})^n\rfloor$ with a formula by this way: we know that:
$$
(2+\sqrt{3})^n+(2-\sqrt{3})^n \in \mathbb{Z}
$$
and also:
$$
0 < (2-\sqrt{3})^n < 1
$$
so we know that the fractional part of $(2+\sqrt{3})^n$ is:
$$
1-(2-\sqrt{3})^n
$$
so the integral part of $(2+\sqrt{3})^n$ is:
$$
(2+\sqrt{3})^n+(2-\sqrt{3})^n-1
$$ But there are some differences with $(3+\sqrt{3})^n$. My question is: How to express the integral part of $(3+\sqrt{3})^n$ with a formula just like before description?",['number-theory']
2054023,Does there exist a set of exactly five positive integers such that the sum of any three distinct elements is prime?,Does there exist $A \subseteq \mathbb N $ such that $|A|=5$ and sum of any $3$ distinct elements of $A$ is a prime number ?,"['number-theory', 'combinatorics', 'prime-numbers']"
2054026,"Limit of recursive sequence defined by $a_0=0$, $a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right)$","Given the following sequence: $a_0=0$, $$a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right),\ \forall n\ge 0.$$ 
Find $\lim\limits_{n\to\infty}a_n$.","['real-analysis', 'sequences-and-series', 'limits']"
2054084,Show that the collection of matrices which commute with every idempotent matrix are the scalar matrices,Show that the collection of $n\times n$ real matrices which commute with every idempotent matrix are the scalar matrices . Let $\mathcal P$ denote set of all idempotent matrices . Let $A=\{B:BP=PB\forall P\in \mathcal P\}$. So I need to show that $A=\{cI:c\in \Bbb R\}$. I am feeling totally confused on this. Will you kindly give some hints here?,"['matrices', 'linear-algebra']"
2054109,Inverse of $I+BA$ is $(I+BA)^{-1} = I-B(I+AB)^{-1}A$ [duplicate],"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . Assume $I+AB$ is invertible, prove then that $I+BA$ is invertible and $(I+BA)^{-1} = I-B(I+AB)^{-1}A$. My work: $(I+AB)(I+AB)^{-1} =  I$ $B(I+AB)(I+AB)^{-1} = B $ $(I+BA)B(I+AB)^{-1} = B$ $(I+BA)B(I+AB)^{-1}B^{-1} = I$ Thus $(I+BA)$ is invertible and $B(I+AB)^{-1}B^{-1}$ is its inverse. But I have no clue how to arrive to the given inverse formula. I feel like I'm missing something. Can anyone help?","['matrices', 'linear-algebra', 'inverse']"
2054117,Should a linear function always fix the origin? [duplicate],"This question already has answers here : What is the difference between linear and affine function? (5 answers) Closed 7 years ago . I became very confused about linear functions after reading this question What is the difference between linear and affine function? In the comments it says that $F(x)=2 \cdot x+4$ is not a linear function, (but an affine one). All my professors gave such examples when teaching linear functions. I am really confused now. Should a linear function always be of the form $f(x)=t \cdot x$ , where t is a constant? I think this could help me understand better linear transformations. I think one of the reasons I did not understand them is because I had a slightly wrong definition of linear functions. However, on Wikipedia, the definition of linear functions seems to accepts functions that also have a constant added or subtracted from the first (linear?) part. https://en.wikipedia.org/wiki/Linear_function https://upload.wikimedia.org/wikipedia/commons/0/0e/Linear_Function_Graph.svg So is Wikipedia wrong on this one?","['terminology', 'linear-algebra', 'functions', 'linear-transformations']"
2054128,Bayes rule with multiple tests,"Say you are given the following \begin{align*}
P(cancer) &= 0.008,            & P(\neg cancer) &= 0.992\\
P(\oplus|cancer) &= 0.98,      & P(\ominus|cancer) &= 0.02\\
P(\oplus|\neg cancer) &= 0.03, & P(\ominus|\neg cancer) &= 0.97
\end{align*} and you test a patient to see if they have cancer or not and it returns positive. If the patient wants to be tested another time, and it once again returns positive, what happens to the probability that the patient has cancer after testing a second time?","['bayes-theorem', 'probability']"
2054153,Expected distance of a dart to the center of board,I have a dart board of radius R. You hit a dart. What is the expected distance of your hit with respect to the center of the board? Attempt: Let $x$= distance away from the center. $E[X]=\int_{x=0}^{R}1-P(X\leq x) dx$ And $P(X\leq x)=x^2\pi/(\pi R^2)$ Do you guys think my approach is correct?,"['probability-theory', 'probability', 'random-variables', 'probability-distributions']"
2054204,Does there exist a continuous onto function from $S^2$ to $S^1$?,"Here $S^2$ is 2 dimensional unit sphere in $\mathbb{R^3}$ and $S^1$ is unit circle in $\mathbb{R^2}$. Does there exist a continuous onto function from $S^2$ to $S^1$? Till now, I am able to construct a continuous map from $S^2$ to closed unit disk in $\mathbb{R^2}$( from projection map from $\mathbb{R^3}$ to $\mathbb{R^2}$). Continuous map from closed unit disc to unit circle is defined in the following way: First we construct an onto map $f$ from closed unit disc to $\mathbb{R}\cup \{\infty\}$. $f(x,y)=\frac{1}{\sqrt{x^2+y^2}-\frac{1}{2}}$ if $\sqrt{x^2+y^2}\neq\frac{1}{2}$ and $f(x,y)=\infty$ if $\sqrt{x^2+y^2}=\frac{1}{2}$. As we know that $\mathbb{R}\cup \{\infty\}$ is homeomorphic to unit circle  we can construct an onto continuous function from closed unit disc to unit circle using $f$. The composition of function from $S^2$ onto unit disc, and function from unit disc to circle is continuous onto function. Is my reasoning correct? If not, can one construct an onto continuous function from $S^2$ to $S^1$?",['general-topology']
2054225,How can I evaluate $\lim\limits_{x\to0}\frac{\sin x}{\sqrt{x}}$?,"$$f(x)=\lim\limits_{x\to0}\frac{\sin x}{\sqrt{x}}$$ Well, in lot of places I saw that the limit of this function as $x$ tends to zero is zero, but I don't think so. What do you guys think? (I mean limit from both sides.) This will clear what i mean..
Limit from the right exist and is zero, but limit form the left does not exist,,
Hence the limit of the function as it tends to 0 does not exist?? or does it? i am just asking that......","['calculus', 'limits']"
2054265,Why do some integrals and derivatives have absolute values?,"I have noticed that some integrals and derivatives have absolute  value. For example: $$\int\frac 1 x \, dx=\ln|x|+c$$ for $x$ is not equal to zero.
So, the point is why is there the absolute value of $x$?","['integration', 'calculus']"
2054273,$\cos(\arcsin(x)) = \cdots $,"I've been asked to prove $$y=\frac{\sqrt{3}} 2 x+\frac 1 2 \sqrt{1-x^2}$$ given $x=\sin(t)$ & $y=\sin(t+\frac \pi 6)$ I did $t=\arcsin(x)$ and plugged that into the $y$ equation.
Used the $\sin(a+b)$ identity to get: $$y=x\cos\left(\frac \pi 6\right)+\frac{\cos(\arcsin(x))}2 = \frac{\sqrt3} 2 x+\frac{\cos(\arcsin(x))} 2$$ Now I'm sure there must be an identity for $cos(arcsin(x))$ however I'm unaware of it.
I'm also unaware of how to prove it. I did a quick google and I found this page which says: which is seemingly exactly what I need to complete the question; however, it wouldn't be proving that $y = \text{answer}$ if I didn't show how to get to this result. Is there a ""more correct"" way to complete this question without having to fiddle with this formula / arcsins etc.","['parametric', 'trigonometry', 'inverse-function']"
2054357,9 points in a unit square with pairwise distances greater than $\frac12$,"Consider a square with sides of length 1. Can we find 9 points in it such that the distance between every pair of points is strictly greater than $\frac12$? We allow the points to be on the boundary of the square. The question arose in a conversation, and we are pretty confident that there is no such arrangement, and moreover the only configuration with pairwise distances at least half is the one where we take the vertices, the midpoints and the center. Our thoughts However, proving this seems nontrivial. An equivalent formulation is to look at the $\frac14$ blowup of the square and ask whether we can fit 9 circles with radius $\frac14$ in it or not. Moreover, the pigeonhole principle seems intuitively to be too weak for proving this, because if the shapes will be disjoint then each will have area around $\frac18$ and for, say, squares diameter half enforces area exactly $\frac18$, but we can't fit 8 such squares without gaps.","['packing-problem', 'recreational-mathematics', 'euclidean-geometry', 'geometry']"
2054394,Integral of $\int{\tan^{5}(x)\sec^4(x)}dx$?,"Here's my attempt at the problem: $\int{\tan^{5}(x)\sec^4(x)}dx= 
\int{\frac{\sin^5(x)}{\cos^9(x)}}\,dx=
\int{\frac{\sin(x)(\sin^2(x))^2}{\cos^9(x)}}\,dx=
\int{\frac{\sin(x)(1-\cos^2(x))^2}{\cos^9(x)}}\,dx=
\int{\frac{(1-u^2)^2}{u^9}}\,du=
\int{\frac{u^4-2u^2+1}{u^9}}\,du=
\int{\Big(\frac{1}{u^5}-\frac{2}{u^7}+\frac{1}{u^9}}\Big)  \,du=
-\frac{1}{4u^4}+\frac{1}{3u^6}-\frac{1}{8u^8}+C=
-\frac{\sec^4(x)}{4}+\frac{\sec^6(x)}{3}-\frac{\sec^8(x)}{8}+C$ It seems, however, that the actual answer should be: $\frac{\sec^4(x)}{4}-\frac{\sec^6(x)}{3}+\frac{\sec^8(x)}{8}+C$ What am I doing wrong?","['integration', 'trigonometry', 'calculus']"
2054401,Why does $\sin{x} = \frac{1}{2}$ have two solutions but $\arcsin{\frac{1}{2}}$ has one solution?,Can anyone help me understand why $\sin{x} = \frac{1}{2}$ have two solutions but $\arcsin{\frac{1}{2}}$ has one solution? Aren't they equivalent?,"['algebra-precalculus', 'trigonometry']"
2054410,strategies to find explicit formulae for series,"I have been manipulating a certain series for several hours without finding any pattern. Hence I am wondering what some of the better strategies are to find patterns and thus an explicit formula for a series. Among the things I have tried so far are: looking for a common difference between terms looking for a common ratio between terms reversing the order of the terms and summing them up, to check whether the result will be the same for all terms bringing the terms to a common denominator and looking for a obvious pattern in the numerator I had no luck with any of these and others. The series btw. is $\sum_{k = 1}^n\frac{k - 1}{k(k + 1)(k + 2)}$ This is oen of the things I tried: $\begin{align*}
                    S_n & = \frac{0}{6} + \frac{1}{24} + \frac{2}{60} + \frac{3}{120} + \frac{4}{210} + \frac{6}{336}\\
                        & = \frac{0}{6} + \frac{1}{24} + \frac{1}{30} + \frac{1}{40} + \frac{1}{52,5} + \frac{1}{56}\\
                        & = \frac{0}{1680} + \frac{70}{1680} + \frac{56}{1680} + \frac{42}{1680} + \frac{32}{1680} + \frac{30}{1680}\\
        a_n - a_{n+1} : &  -\frac{70}{1680}; \frac{14}{1680}; \frac{14}{1680}; \frac{10}{1680}; \frac{8}{1680}; 
\end{align*}$ The differences between the terms get ever smaller and the sum approaches $.25$, but any internal pattern remains hidden after the things I tried. So, are there a number of useful methods to uncover patterns in series?","['sequences-and-series', 'closed-form', 'limits']"
2054419,Taking a limit of a sequence of functions,"I want to compute the following limit for the sequence of functions $f_n(x)=\frac{1+nx^2}{(1+x)^n}$, where $f_n:[0,1]\rightarrow \mathbb{R}$. $$\lim_{n\rightarrow \infty} \frac{1+nx^2}{(1+x)^n}=\lim_{n\rightarrow \infty} \frac{1+nx^2}{x^n+nx^{n-1}+\ldots +\binom{n}{2}x^2+nx+1},$$ where in the second part I used the binomial theorem in the denominator. I think the answer should be the zero function, but I'm having trouble showing that. Could someone guide me? Thank you.","['real-analysis', 'sequences-and-series', 'functions', 'limits']"
2054450,Finding Cramer Rao Lower bound for a bivariate parameter,"I am having a hard time figuring out the Cramer Rao lower bound for a random sample of size $n$ from a population with $\Gamma(p, \theta)$ with $p, \theta$ unknown. The problem doesn't say what formulation to use for the gamma function, but I have used $$
\frac{\theta^p}{\Gamma(p)} x^{p-1} e^{-\theta x} I_{[0,\infty)}(x).
$$ So I know that I have to find the Fisher Information matrix and compute the inverse, however the entries I get are not ""nice"". The derivatives of the logarithm of the pdf is (considering only $x_1$) $$
\frac{ \partial }{ \partial p} \ln f(x_1|p,\theta) = \ln \theta - \frac{\partial}{\partial p} (\ln \Gamma (p)) + \ln x_1, \\
\frac{\partial}{\partial \theta} \ln f(x_1|p,\theta) = \frac{p}{\theta}-x_1.
$$ Now, the entries in the Fisher Information matrix would be, with $\beta = [p, \theta]^\mathrm{T}$, $$
I_{ij} = n \mathrm{E} \left[ \left( \frac{ \partial}{\partial \beta_i} \ln f(X| p, \theta) \right) \left( \frac{ \partial}{\partial \beta_j} \ln f(X| p, \theta) \right)  \right].
$$ However, with the derivatives obtained above the entries become horrible... Apart from $I_{22} = np/\theta^2 $, that is. Am I on the right track here?","['fisher-information', 'statistics', 'statistical-inference']"
2054470,Rewriting a second-order nonlinear ode in first-order system - how?,"I wanna show that $y''+\alpha \sin(y)=f(x)$  with  $y(a)=y_0$ and $y'(a)=y_1$ has got an unique solution.
I wanted to rewrite this ODE in an ODE system of first order and than apply picard-lindel√∂f on each row. My problem is that I don't know how to rewrite this in an ODE of first order because of $\sin(y)$ and because it's non-homogeneous. I thought of substituting $z:=y'$ than we've got:
$z'+\alpha \sin(y) = f(x)$ and if we look at the homogeneous ODE $z'+\alpha \sin(y) = 0$ we get $z(y)= \alpha \cos(y)+c_0$ as a solution. Substituting back we get  $y'(x)= \alpha \cos(y(x))+c_0$ I've read that we can rewrite this equation in $Ax=b$ with $x=\pmatrix{x \\ \dot x}$ and $b=\pmatrix{0 \\f(t)}$ , but I'm not sure about the matrix I guess it's something like $A=\pmatrix{0 & 1 \\ -\alpha \sin & 0}$. Any hints or good books to recommend?",['ordinary-differential-equations']
2054472,Where is the mistake in this problem?,"Let $$S = \sum^{\infty}_{n=1} a(n)$$
be an infinite series such that the nth partial sum is given by:
$$S(n) = \frac{n - 1}{n + 1}$$ since $$
a(n)=S(n)-S(n-1)=\frac{2}{n(n+1)} 
$$ Now, $S(1)=a(1)\Leftrightarrow 0=1.$  Where is the mistake?","['fake-proofs', 'sequences-and-series']"
2054541,Using Cantor-Schroder-Bernstein Theorem to prove a statement (with my proposal),"My Question reads: Use the Cantor‚ÄìSchr√∂der‚ÄìBernstein Theorem to prove the following. The set of all integers whose digits are 6, 7, or 8 is denumerable Now, my understanding of the definition of CSB Theorem is if cardinality of A less than or equal to cardinality B and cardinality B less than or equal to cardinality A, then cardinality A equals cardinality B. I have started by setting S=the set of all integers whose digits are 6,7, or 8. From there I am not too sure what function to define. From this set S to what other set? And I know I would also have to show a function the other way around; in both cases one-to-one functions. I am more concerned as to how to go about setting this up. I am not too sure what is meant by integers whose digits are 6,7,or 8. Would this be a number like 67, or 86? Updated Answer I am working on defining f: Z->A by defining: f(x)={6x, if x>0; 7x, if x=0; 8x, if x>0} Can something like this be used to solve this direction?","['proof-verification', 'proof-writing', 'elementary-set-theory', 'cardinals', 'discrete-mathematics']"
2054545,"$Ax=b$ is solvable, then it has the same solutions of $A^TAx=A^Tb$","I have to prove that given a matrix $A \in \mathbb{R}_{m\times n}$ and $b \in \mathbb{R}^n$. Suppose that the system $Ax=b$ is solvable, x is solution of $Ax=b$ If and only if x is solution of $A^TAx=A^Tb$. It's easy to show that if x is solution of $Ax=b$ them it is also solution if $A^tAx=A^Tb$, but I can't figure out why I have the other direction even if $A$ doesn't have full rank (that part√≠cular case is easy to solve).","['least-squares', 'linear-algebra', 'systems-of-equations']"
2054552,The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal,"Anyone can help provide a proof for The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal. I found one related question Why is the largest element of symmetric, positive semidefinite matrix on the diagonal? whose answer proves a similar claim for symmetric positive definite matrix. However, I think the prove fails for a matrix with complex numbers, as the imaginary part also contributes to the magnitude.",['linear-algebra']
2054563,"Some confusion about what a function ""really is"".","Despite my username, my background is mostly in functional analysis where (at least to my understanding), a function $f$ is considered as a mathematical object in its own right distinctly different from the values it takes under point evaluation (i.e. $f(x)$). Another way of stating this is that the possible values of a function under evaluation are properties of the function, when considered as its own mathematical object. However, I am reading a book about the foundations of mathematics by Kunen and he refers to a function as being identified with its graph (i.e. a set of ordered pairs) in axiomatic set theory. I was under the impression that this definition of a function as a set of ordered pairs was an oversimplification that teachers used in high school that one grew out of past calculus. So anyways, what is the most fundamental definition of a function? Obviously we all (students of mathematics) know what a function is intuitively but formally, I have a hard time swallowing the idea that a function is the same thing as its graph. I realize that the whole point of axiomatic set theory is to make it possible to denominate every mathematical object in terms of sets but I find this definition to be particularly disappointing. I suspect that this is one of those things that just depends on what area one chooses to work in but I'd love to see what thoughts some of the more experienced mathematicians on here can offer.","['functions', 'foundations', 'elementary-set-theory', 'definition', 'philosophy']"
2054592,Do Bernoulli random variables always satisfy the Lyapunov condition?,"It seems to me that the Lyapunov CLT condition holds for any sequence of independent Bernoulli random variables $X_1,X_2,\dots,X_n$ no matter how they are distributed.
Restating the condition says that we can apply the CLT if there exists a $\delta>0$ such that
$$
\lim_{n\rightarrow\infty}\frac{1}{s^{2+\delta}} \sum_{i=1}^n E[ |X_i - E[X_i]|^{2+\delta}] = 0,
$$
where $s = \sqrt{\sum_{i=1}^n Var[X_i]}$. 
For every $i$ and probability $p_i = Pr[X_i=1]$, setting $\delta=1$ shows that 
$$
E[ |X_i - E[X_i]|^{3}] = p_i |1 - p_i|^{3} + (1 - p_i)|(-p_i)|^3 ‚â§ p_i(1 - p_i) = Var[X_i],   
$$
and hence the sum is upper bounded by the sum of the variances. 
Since in the denominator we have $s^{2+\delta} = (\sum_{i=1}^n Var[X_i])^{3/2}$, the limit goes to $0$. What am I missing?",['probability-theory']
2054600,"$AB ‚àí BA = A$, $A$ is a nonzero matrix then $B$ is not nilpotent.","Let $A,B$ $n\times n$ matrices such that $$AB-BA=A$$ and $A$ is a nonzero matrix. Prove that $B$ is not nilpotent. I know why $A$ is nilpotent, but how can I prove $B$ is not nilpotent?","['matrices', 'matrix-equations', 'linear-algebra']"
2054623,On the definition of trigonometric functions in the right triangle.,"The question is the following: what are the principal reasons to define the sine, cosine, etc., of an angle, in terms of the right triangle and not, for example, in terms of an obtuse triangle? 1) Is the pythagorean theorem a good reason for using a right triangle? Why? Also, is it true that the ratios of sides of a triangle are always functions of the angle, or this is only true in right-angled ones? 2) Also, I think that, if we define the trigonometric functions in terms of ratios of the sides, we need the right triangle, because it's the only triangle for which their sides can be localized. For example, if we have an obtuse or acute triangle, we cannot determine what is the adjacent side to the angle $A$, because there are two sides who satisfies that property, but in the right-angled triangle, the hypotenuse is always bigger and hence clearly distinguishable of the adjacent side of the angle $A$. Is this a good reason to prefer the right angles triangles to define the trigonometric ratios? Also, every triangle can be decomposed into two right angled triangles. Is this also a good reason to prefer right triangles, or also any triangle can be decomposed into an obtuse triangle?","['trigonometry', 'triangles', 'geometry']"
2054636,Almost sure and Probability Convergence of the Maximum of I.I.D. Random Variables,"I am going over the Borel-Cantelli Problems in Durrett, but I am really stuck on Exercise 2.3.15.. Suppose $Y_{1},Y_{2},\ldots$ are i.i.d.. I need to prove: 1) $\frac{\max _{1\leq m\leq n}Y_{m}}{n}\to 0$ in probability if and only if $nP(Y_{i}>n)\to 0$ 2) $\frac{\max _{1\leq m\leq n}Y_{m}}{n}\to 0$ almost surely if and only if $E\left(\max\{0,Y_{i}\}\right)<\infty$. There are two easier parts to the problem that I was able to solve. I proved that 
$$
\frac{Y_{n}}{n}\to 0\text{ in probability }\iff P(|Y_{i}|<\infty)=1
$$
and
$$
\frac{Y_{n}}{n}\to 0\text{ almost surely }\iff E|Y_{i}|<\infty.
$$
However, I am unsure if these facts can be used to solve the above two problems. I'd really appreciate some help with these problems. Thanks!","['almost-everywhere', 'independence', 'probability-theory', 'borel-cantelli-lemmas', 'convergence-divergence']"
2054654,Risk and Die-Rolling Probabilities,"If you aren't familiar with Risk, here is a short description of how the game works: In the game of Risk, players control countries by occupying them with a variable number of ‚Äúarmies.‚Äù  The object is to gain more territory by conducting battles between neighboring countries.  A battle consists of the armies of a single country going against an opponent‚Äôs armies occupying a neighboring country.  The battle progresses by each player rolling a prescribed number of dice, and applying rules to determine from the outcome how many armies are lost for each player.  The dice are rolled repeatedly until either the defender loses all his armies (in which case the attacker can occupy the disputed country), or until the attacker is reduced to a single army (in which case there is no army to spare for occupying the opponent‚Äôs country).  An attacker can never lose control of his own country.  The battle may also be stopped at any prior time at the attacker‚Äôs discretion. The rules for determining how many dice a player may shake are as follows: The attacker may shake one less die than the number of armies on his country, to a maximum of three. The defender may shake as many dice as the number of armies on his country, to a maximum of two. The rules for deciding the outcome of a particular throw of the dice are as follows: The highest attacker die is compared against the highest defender die.  Whoever has the lower number loses one army.  Ties go to the defender. The procedure is repeated for the second-highest dice. In cases where either the attacker or the defender only rolls a single die, a total of only one army will be lost; in all other cases a total of two armies will be lost. For the 1-1 die rolling situation, it is easy to determine that the probability of winning one battle is 15/16. How can I determine probabilities for the other scenarios? Do I have to brute force the calculation (which wouldn't be hard if I were to use python), or does there exist a mathematical way to determine the probabilities of the attacking side winning?",['probability']
2054708,Is there a name for $\frac{1}{\frac{1}{x} + \frac{1}{y}}$?,"The function $$\frac{1}{\;\;\dfrac{1}{x} + \dfrac1{y}\;\;}$$ shows up a lot, e.g., in parallel resistance or series conductance.  Does it have a name?  It is similar to harmonic mean with the difference that the numerator is one rather than the number of terms. Can it be called harmonic sum?  I think it stands to reason that since: you can replace $n$ series resistors with $n$ equal resistors having resistance equal to the mean of those resistors, you can replace $n$ parallel resistors with $n$ equal reistors having resistance equal to the harmonic mean of those resistance, and you can replace $n$ series resistor with 1 resistor having resistance equal to the sum of those resistors, then ‚Äî by analogy ‚Äî you should be able to replace $n$ parallel resistors with 1 resistor having resistance equal to the ‚Äúharmonic sum‚Äù of those resistors. That is, mean is to harmonic mean as sum is to ‚Äú harmonic sum ‚Äù.",['functions']
2054792,An explicit realization of the similarity of the transpose of a matrix in function field.,"Let $K=\mathbb{F}(a,b,c,d)$ be the field of rational functions in four variables over a field $\mathbb{F}$. The matrix $$ A=\left( {\begin{array}{cc}
   a & b \\       c & d \      \end{array} } \right)$$ over $K$ is conjugate to its transpose. Hence there exists an invertible matrix $P$ over $K$ such that $A^t=PAP^{-1}$. What is an explicit formula for $P$, in terms of $a,b,c,d$ ? Can we choose $P$ to have polynomial entries in $a,b,c,d$ ?","['matrices', 'abstract-algebra', 'linear-algebra']"
2054826,Proving a sequence converges $\epsilon-N$,"It seems my biggest nightmare has come to haunt me again! I first came across the formal proof of a limit $\epsilon-\delta$ in Calculus 1, I never truly mastered it since at the time it was just racking my brain. I have now begun self-teaching a course in sequences & series and it has already come up. It has also been emphasised to me that being able to understand and do this is fundamental for success in this course. I'm almost 100% certain that I'd have no trouble applying the methods, I've seen in tutorials to prove a sequence converges. However, I don't understand how the $\epsilon-N$ proof actually proves a sequence converges; if that makes much sense. Here are a few questions, I'd like to ask: What does this statement mean: $|a_n - L| < \epsilon $ L refers to the limit. Why does this proof actually prove a sequence converges? Why does $\epsilon>0$? Those are the only questions that currently come to mind, also if anyone can provide any additional advice on how I can wrap my head around these proofs please feel free to post!","['epsilon-delta', 'sequences-and-series', 'convergence-divergence', 'limits']"
2054859,How do they simplify these regression formulas,"My book writes $$\text{SSres}=\sum (y_j-\hat y_j)^2$$ $$=\sum y^{2}_{j}-n \bar y ^{2} - \beta_{1} 
Sxy$$ How is this the same formula?","['regression', 'statistics']"
2054881,Computing expectation of MLE?,"This is a part of lecture note. Reviewing it I am not clear with the process. 
How can I expand $\operatorname{E}\left[\dfrac 1 {\left(\sum X_i\right)}\right]$ to make it $\displaystyle\int {(\lambda^n)(x^{n-1}) \exp(-\lambda x)\,dx \over x(n+1)!}$?
It's usung MLE of Gamma distribution for $X_i$'s from exponential distribution.","['statistics', 'probability', 'expectation', 'calculus']"
2054883,Probability of selecting two numbers with a sum of squares divisible by 10,Two natural numbers $x$ and $y$ are chosen at random.  Find the probability that $x^2 + y^2$ is divisible by 10. I could not understand how select two numbers from any natural number (infinite).,['probability']
2054900,Simplifying $\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n)$,"I would like to show
$$\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n) = \dfrac{1}{n}\sum_{i=1}^{n}U_iV_i+\bar{U}_n\bar{V}_n$$
where $\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^{n}X_i$ and similarly for $\bar{Y}_n$, $\bar{U}_n$, and $\bar{V}_n$; $U_i = X_i - \mu_x$, $V_i = Y_i - \mu_y$, where $\mu_x$ and $\mu_y$ are constants. My attempt:
$$\begin{align}
\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X}_n)(Y_i - \bar{Y}_n) &= \dfrac{1}{n}\sum_{i=1}^{n}[(X_i-\mu_x)-(\bar{X}_n-\mu_x)][(Y_i-\mu_y)-(\bar{Y}_n-\mu_y)] \\
&= \dfrac{1}{n}\sum_{i=1}^{n}(X_i-\mu_x)(Y_i-\mu_y)-\dfrac{2}{n}(\bar{X}_n-\mu_x)(\bar{Y}_n-\mu_y)\\
&+(\bar{X}_n-\mu_x)(\bar{Y}_n-\mu_y)\text{.}
\end{align}$$
This doesn't quite match what I'm looking for. Is my desired equation wrong, or is my work wrong?","['statistics', 'summation']"
2054930,Binomial coefficients are odd if and only if $n = 2^k-1$,"Let $n$ be an integer, $n > 0$. Prove that all the coefficients of the expansion of the Newtonian binomial $(a+b)^n$ are odd if and only if $n$ is of the form $2^k-1$. Why in the solution below do they take $n > 8$? Also, I don't get the middle paragraph argument about $n_1$. What do those values represent using $n_1$? Solution: For $n \leq 8$ the theorem is immediately verifiable. It is therefore sufficient to assume that the theorem is true for the binomials $a+b,(a+b)^2,\ldots,(a+b)^{n-1}$ for $n > 8$, and prove that the theorem holds for $(a+b)^n$. The coefficients of the binomial expansion (except for the extreme ones both equal to $1$) are $$\dfrac{n}{1}, \dfrac{n(n-1)}{1 \cdot 2},\ldots,\dfrac{n(n-1) \cdots 1}{1 \cdot 2 \cdots (n-1)}.$$ A necessary and sufficient condition in order that all these numbers be odd is that $n$ is odd and the numbers obtained by deleting the odd factors from the numerators and denominators of the remaining numbers be odd. But setting $n = 2n_1+1$ these numbers can be represented by the terms of the sequence $$\dfrac{n_1}{1},\dfrac{n_1(n_1-1)}{1 \cdot 2}, \ldots, \dfrac{n_1(n_1-1) \cdots 1}{1 \cdot 2 \cdots (n_1-1)}.$$ Since $n_1 < n$, the latter are all odd if and only if $n_1$ is of the form $2^k-1$, i.e. if and only if $n$ is of the form $2(2^k-1)+1 = 2^{k+1}-1$.",['number-theory']
2054967,Compute the Fundamental Group of the Infinite Grid,"Consider the space $G\subset \mathbb{R}^2$ constructed by taking a horizontal line at every integer on the $y$-axis and a vertical line at every integer on the $x$-axis. I need to compute $\pi_1(G,b)$ where b is your favourite base point, but I haven't been able to make much progress. Help please! [Edit: The Count pointed out that I should probably be more specific with what subset I'm talking about! The simplest definition I can think of is $G=(\mathbb{Z}\times\mathbb{R})\cup(\mathbb{R}\times\mathbb{Z})$]","['algebraic-topology', 'general-topology', 'group-theory']"
2054968,When quotient ring is a field?,"Let $K = \mathbb{Z}/7\mathbb{Z}$. For what values of $a$ quotient ring $K[x]/\left<x^2 + a\right>$ is a field? I tried to use the bruteforce method here. Since we are quotienting by $\left<x^2 + a\right>$ all our elements can be represented as : $a_0 + a_1 \cdot x$. Initially I assumed $a_1 = 0$. So I'm considering the elements with non-zero $a_0$ like $1, 2, 3,4,5,6$. Let $a_0\cdot(p_0 + p_1\cdot x) = a_0 \cdot p_0 + a_0 \cdot p_1 \cdot x = 1$. I can take $p_1 = 0$, so it only lasts $a_0 \cdot p_0 = 1$ and I checked that $\forall a_0\ \exists p_0 \ | a_0 \cdot p_0 = 0$. So this case gives me no information about my $a$. Then I considered the case when $a_0 = 0$ and $a_1 \neq 0$. $1 = a_1x(p_0 + p_1x)  = a_1p_0x + a_1p_1x^2$. I know that $x^2 = -a \Rightarrow a_1p_0x + a_1p_1(-a)$. As before I can let $p_0 = 0$ and only deal with $a_1p_1(-a) = 1$. And here I got stuck, because I don't how to choose $a$.","['abstract-algebra', 'ring-theory']"
2054979,Is a p-Sylow subgroup of the underlying additive group of a finite commutative ring an ideal?,"Let $R$ be a finite commutative ring. Let $H$ be a $p$-Sylow subgroup of the additive group $(R,+)$. Is $H$ an ideal in $R$? For ex: 
R= Z/6Z is a finite commutative ring. 
H={0,3} is a 2-sylow subgroup which in itself is an ideal of the ring.
Is this true for any finite commutative ring R?","['abstract-algebra', 'ring-theory', 'finite-rings', 'ideals']"
2054992,Symmetric integral of an odd function,"I recently came across this problem in class: Determine whether $\int_{-\infty}^\infty\frac{x}{x^2+1}dx$ converges or
  diverges. My first thought was that since $\frac{x}{x^2+1}$ is an odd function, and the integral is symmetric about the y-axis, then it must be 0. However, the answer said that it's wrong, and that the integral diverges, since both $\int_{0}^\infty\frac{x}{x^2+1}dx$ and $\int_{-\infty}^0\frac{x}{x^2+1}dx$ diverge. It then said it's wrong to say that since $\int_{-R}^R\frac{x}{x^2+1}dx = 0$, then $\int_{-\infty}^\infty\frac{x}{x^2+1}dx$ must also be 0. Why is this ""wrong""?","['fake-proofs', 'improper-integrals', 'integration', 'calculus']"
2055027,How to transform affine dynamical system into linear system of difference equations,"I have a linear difference equation for a system with the form $c_j(t+1) = \frac{b}{n} + (1-b)\sum_{i=1}^{n}B_{ij}c_i(t)$
where B is a square matrix
I'm trying to get this into the form $c(t+1) = Ac(t)$ , where $A$ is a square matrix that governs the dynamics of the system. However, I'm having trouble because of the $\frac{b}{n}$ term. I have: $$\left( \begin{array}{}
c_1(t+1)\\
c_2(t+1) \\
 ...\\ 
c_n(t+1)\end{array} \right) =\left( \begin{array}{}
\frac{b}{n} + (1-b)(B_{11}c_1(t) + B_{21}c_2(t) + ... + B_{n1}c_n(t))\\
\frac{b}{n} + (1-b)(B_{12}c_1(t) + B_{22}c_2(t) + ... + B_{n2}c_n(t)) \\
 ...\\ 
 \frac{b}{n} + (1-b)(B_{1n}c_1(t) + B_{2n}c_2(t) + ... + B_{nn}c_n(t))\end{array} \right)$$ I want this in the form $c(t+1) = Ac(t)$
so that I can solve the system using $c(t) = A^tc(0) $, but I'm not seeing how to achieve this form. It may be that I'm approaching this entirely incorrectly.
 If you have thoughts on how to proceed, I would very much appreciate them. edit: I would like it specifically in the form stated above without any additions to the c vector as suggested in the answers below. If Is this even possible?","['matrix-equations', 'dynamical-systems', 'discrete-mathematics']"
2055050,What is this group called?,"Let $X$ denote a set. There's a corresponding group obtained by taking the group freely generated by $X^2$ and then quotienting out by the following families of relations: $(x,x) = 1$ $(x,y)(y,z) = (x,z)$ $(x,y)(y,x) = 1$ For each quadruple $(x,y,x',y')$ such that $\{x,y\} \cap \{x',y'\} = \emptyset$, we have:
$$(x,y)(x',y') = (x',y')(x,y)$$ Question. What is this group called? My motivation for considering this group is that it acts on the set $(\{0,1\}^\mathbb{N})^X,$ of $X$-many binary streams, by interpreting $(x,y)$ as the act of taking the first digit of stream $x$, removing it from $x$, and appending it to the beginning of $y$. Each of the four families above can be explained in these terms; for example $(x,x)=1$ is saying that if I take the first digit of stream $x$, and put it back on stream $x$, nothing changes.","['group-theory', 'computer-science']"
2055057,Showing a function is absolutely continuous and finding its derivative,"Let $A$ be a measurable subset of $[0,2]$ and define $f:\mathbb{R}\to\mathbb{R}$ by letting $f(x) = \mu((-\infty,x]\cap A)$ for every $x\in\mathbb{R}$. (a) Show that $f$ is absolutely continuous on $\mathbb{R}$, calculate $f'$ and $\int_0^3f'(x)dx$ (b) Show that for every $0<b<\mu(A)$ there exists $x_0\in\mathbb{R}$ such that $b=\mu((-\infty,x_0]\cap A)$ For (a), I was able to show that $f$ is absolutely continuous by applying the definition that for every $\varepsilon>0$ there is a $\delta>0$ such that for every finite disjoint collection $\{(a_k,b_k) \}_{k=1}^n$ of open intervals then $\sum b_k-a_k < \delta \Rightarrow \sum|f(b_k)-f(a_k)|<\varepsilon$. Taking $\delta=\varepsilon$, it's pretty easy to see that this definition is satisfied. I also know that since $f$ is absolutely continuous, then $\int_a^bf'(x)dx$=f(b)-f(a), so $\int_0^3f'(x)dx= f(3)-f(0) = \mu(A)$. I'm having trouble actually calculating what $f'$ is though. I tried using the difference quotient, and I simplify it down to $$f'=\lim_{h\to0}\frac{\mu((t,t+h]\cap A)}{h} $$
I'm not sure where to proceed from here though, if this is even the right way to go. Any help would be much appreciated. I think part (b) is simply the Intermediate Value Theorem, but if it's not I would appreciate a hint.","['derivatives', 'real-analysis', 'measure-theory']"
2055083,Numerical integration of $\int_0^{2\pi}\frac{dx}{3+\sin x}$,"We know that numerical integration using end points is first order, trapezoid and mid-piont rules second order, and Simpson's rule fourth order. However, with $\int_0^{2\pi}\frac{dx}{3+\sin x}$, we have the following errors: n   Left point              Right point             Trapezoid               Mid-point               Simpson
1   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     2.02200572599407E-2     2.02200572599407E-2
2   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     -2.14466094067260E-2    -7.55772051783715E-3
3   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579204725E-5
4   -6.13276073392621E-4    -6.13276073392621E-4    -6.13276073392621E-4    6.12214122685750E-4     2.03717390659608E-4
5   1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8
6   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     -1.80379781545281E-5    -6.01296612956492E-6
7   1.35448319227294E-11    1.35448319227294E-11    1.35448319227294E-11    1.35449429450318E-11    1.35449429450318E-11
8   -5.30975353407737E-7    -5.30975353463248E-7    -5.30975353407737E-7    5.30974556545161E-7     1.76991253264536E-7
9   1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14
10  1.56304431619958E-8     1.56304431619958E-8     1.56304431619958E-8     -1.56304434395516E-8    -5.21014786869500E-9 Simpson is surprised. Why are the end points over-performing? Another interesting fact is that the errors are not converging steadily but rather in an oscillating manner. Is there any explanation for this?","['numerical-methods', 'integration', 'calculus']"
2055095,How to solve laplace inverse using convolution,"The problem is find the laplace inverse of:
$Y(s) =\frac{18}{s(s-6)} + \frac{2}{s-6}$. Are you able to solve this problem using convolution, by breaking the terms into: $18 * \mathscr{L} (\frac{1}{s}) * \mathscr{L}(\frac{1}{s-6})$ I tried to solve it using this method, but did not get the same solution as solving through partial fractions.   Thanks in advance for the help! The answer I found through partial fractions is: $5e^{6t}-3$ My solution is shown in the image below!",['ordinary-differential-equations']
2055099,A finite group $G$ has two elements of the same order ; does there exists a group $H$ containing $G$ such that those elements are conjugate in $H$?,"Let $G$ be a finite group and $x,y \in G$ have the same order . Then is it true that there exists a group $H$ containing $G$ as a subgroup ( or an isomorphic copy) such that $x,y$ are conjugate in $H$ ? What I thought is to embed $G$ in $S_{|G|}$ , but the thing is can we show that the natural images of $x,y$ in $S_n$ ($n=|G|$ ) has same cycle structure ? If not , then is there any other way (is the claim true ?) ? Please help.","['finite-groups', 'group-homomorphism', 'group-theory']"
2055102,Step By Step how to graph trig from local maxima and minimum,"Question The Secant Graph has the local minimum point at $(-0.75,-1.5)$ and it has a local maximum at $(2.25,-2.5)$ Give the equation in the form: $y=A \sec(Bx+C)+D$ My thought process&steps until I got stuck First I find the Period I do this by finding the horizontal distance from the minimum and maximum and multiply that number by $2$ to get the distance aka the period. that means the distance from the minimum to the maximum is $3$ so the period is $6$. Then I find $B$ As anyone knows, the way you get the period is to do $\frac{2\pi}{B}$ that means $\frac{2\pi}{B}=6$. then we can deduce that $B$ is going to be $\frac{\pi}{3}$. We also recognize the fact that ($\frac{\pi}{3}(x)+c)=0$ sets up the beginning of the primary period. Then I find the amplitude This fairly easy, i just found it to be $0.5$ What I have so far we have the following right now: $\frac{1}{2}\sec(\frac{\pi}{3}x+C)-2$ Now my question is how to find what is $C$? After that i plug in -0.75 into the equation as follows: $\frac{1}{2}\sec(\frac{\pi}{3}(\frac{-3}{4})+C)-2=1.5$","['algebra-precalculus', 'trigonometry', 'graphing-functions']"
2055106,Four Equations in Four Unknowns,"Four Equations in Four Unknowns Completely solve the following equation! $$\begin{eqnarray}
x&+&y&+&z&+&w&=&10 \\
x^2&+&y^2&+&z^2&+&w^2&=&30 \\
x^3&+&y^3&+&z^3&+&w^3&=&100\\
&&&&&&xyzw&=&24
\end{eqnarray}$$ This is problem 3 on page 3  of Mathematical Quickies 270 Stimulating Problems with Solutions by Charles W. Trigg Dover Publications, Inc., New York ISBN 0-486-24949-2 Here is the solution from page 78 of the book: By inspection $(1,2,3,4)$ is a solution if the first and fourth equation and satisfies the second and third equations. Since the equations are symmetrical in $x$, $y$, $z$, $w$ the other $23$ permutations of $1,2,3,4$ are solutions also. But these are all the solutions, since the product of the degrees of the equations is 4! I assume that the exclamation mark at the end of the last sentence is a factorial symbol because the product of the degree of the equation is 24. 
This seems to be a property that is similar to the fact that a univariate polynomial of degree $n$ has at most $n$ zeroes. But I can't see how to generalize this to multivariate equations. Why does this system of equations where the product of the degree of the equations is 24 has at most 24 solutions?","['algebra-precalculus', 'recreational-mathematics']"
2055146,Lipschitz Implicit Function,"I am reading a paper entitles ""Sphere Tracing: a Robust Antialiased Rendering of Distance Based Implicit Surfaces"" , written by John C. Hart. Here are some definitions: Surfaces/shapes can be defined implicitly. An implicit function is a continuous mapping $f$ : $R^n \rightarrow R$ that describes the set $A \subset R^n$ as the locus of points: $A \equiv \{ x : f(x) \le 0\}.$ The boundary of the surface may be denoted $f^{-1}(0)$ . Distance to a set. The distance $d(x,A)$ from a point $x \in R^3$ to a closed set $A \subset R^3$ is given by $d(x,A) = \min_{y \in A}||x - y||.$ Then Hart writes that if you have $|f(x)| = |d(x, f^{-1}(0))|$ (eq. 1) then you have what he calls a distance implicit function or DIF. And if you have $|f(x)| \le |d(x, f^{-1}(0))|$ then you have distance underestimate implicit function or DUF. I don't want to get into the details of why he is interested in the DUF type of function but in short within the context of ray-tracing (in computer graphics) this helps the process of finding if a ray (defined by an origin and a direction) intersects surfaces (which can be defined implicitly in this case). The property of DUF functions is that they guarantee to return a distance to the surface from a given point $x$ within which the ray ""cannot interest the surface or can intersect the surface (where the intersection point is exactly on the surface)"" but in no circumstance can overshoot the surface. That's just to provide some context to my questions. Then he goes on defining Lipschitz implicit functions or LIF. It is an implicit function such that $|f(x) - f(y)| \le \lambda ||x-y||$ (eq. 2) for some positive constant $\lambda$ . This constant is denoted the Lipschitz constant and is written as $Lip \; f$ . Any LIF can be made into a DUF. Let $f$ be a LIF with Lipschitz constant $\lambda$ . Let $y \in f^{-1}(0)$ be one of the points such that: $||x -y|| = d(x, f^{-1}(0))$ (eq. 3). Then by eq. 2 $|f(x)| \le Lip \; f \; d(x, f^{-1}(0)).$ Hence, $f(x)/ Lip \; f$ is a DUF for any LIF $f$ . QUESTIONS: I believe that Hart's point is to show that if an implicit function is a Lipschitz function, then it has a DUF, and thus this DUF can be used later on to compute the ""underestimate"" distance of a ray's origin to a given surface. Okay I get that. The proof from his paper is something I hope I understand as well: $\lambda ||x-y|| \ge |f(x) - f(y)|\\$ then replace $||x-y||$ by $d(x,f^{-1}(0))$ (eq. 3) $\lambda \; d(x,f^{-1}(0)) \ge |f(x) - f(y)|\\$ $f(y)=0$ in this particular case since $y \in A$ thus we can re-write: $\lambda \; d(x,f^{-1}(0)) \ge |f(x)|\\$ and finally: $ \dfrac {f(x)}{\lambda} \le d(x,f^{-1}(0))$ If this correct? Finally, this is the part where I am the most interested in an explanation. Hart writes: The Lipschitz constant of a continuous function is its maximum slope. The maximum slope can be found by setting the function's second derivative equal to zero and solving for x. Since x is a vector, these derivatives can be the squared magnitude of the gradient. Could someone explain me this part with possibly an example? (using a real function) PS: as a helper later on in the paper Hart uses the function: $C(r) = 2 r^3/R^3 - 3r^2/R^2 + 1$ then he computes the first and second derivative: $C'(r) = 6r^2/R^3 - 6r/R^2$ and $C''(r) = 12r/R^3 - 6/R^2.$ Setting C''(r) = 0 yield the max slope, which occurs at the midpoint r=R/2, which is C'(R/2) = -3/2R = Lip C(r). I really don't understand this at all. Could someone please explain? EDIT I believe I understand most of it myself now. The paper says the Lipschitz constant is equal to the maxima of the function which you can find if you know the second derivative of that function and solve for f''(x) = 0. So it would be great to learn/know how I can explain this property. So I understand how Hart finds the value of $r=R/2$ for C''(r) but why do we re-inject this value in C'(r) ( $C'(R/2)$ ) in order to find about the value of Lip C(r)? Is that because: $\lambda = \dfrac{|f(x) - f(y)|}{||x - y||}.$ Can be assumed as the first order derivative of the function f(x)?","['derivatives', 'real-analysis', 'implicit-differentiation']"
2055183,Finding polynomial.,Let $f(x)$ be a fifth degree polynomial. Such that$(x-1)^3$ divides $(f(x)+2)$ &$(x+1)^3$ divides $(f(x)-2)$. Find $f(x)$. It was to be find without using calculus.but l don't know how to do this . I was surprised that how it can be done using calculus. Please tell me how to solve such questions I would be thankful.,"['algebra-precalculus', 'calculus']"
2055207,How to find $\int_{0}^{4} \sqrt{16-x^2} dx$?,"What is the easiest way of solving this integral: $$\int_{0}^{4} \sqrt{16-x^2} dx$$ My idea was to substitute $x$ with $4\sin u$ and to get under the square root $\cos^{2}{u}$ so i can get rid of it, but then i get again $\cos^{2}u$. I suppose that could be solved using formula $$\cos^{2}{\frac{u}{2}} = \frac{1+\cos u}{2}$$ But then I got troubles with getting back substitution. Am I making somewhere mistake and if not how should I precede, or is there some easier way of solving it?","['integration', 'definite-integrals']"
2055276,Change of variables in complex integration,"Assume $z=x+iy$ and $\bar{z}=x-iy$. Then one might want to change the integration variables (for some arbitrary integral) over $dz d\bar{z}$ to something proportional to $dxdy$. Then it seems that $$dz d\bar{z} = 2i dxdy$$ 
but I do not see how this is obtained. What I know is that
$dz = dx+idy$ and $d\bar{z}=dx-idy$ thus
$$
dz d\bar{z} =(dx+idy)(dx-idy)=dx^2+dy^2
$$
so, my question is, which of the two is the correct one? It seems to be like a very trivial question which I am unable to get the answer.","['complex-analysis', 'several-complex-variables']"
2055282,What is an algebraic curve?,"I know that this question is at least ""ridiculous"" if you have internet access, though maybe this is the problem as it seems (many times, at least in my head). I found many books (or notes), like Miranda's Riemann Surface and Algebraic Curves , Hartshorne's Algebraic Geometry , Fulton's Algebraic Curves and so on, each one with a different definition. Apparently, depends on the context that you're investigating them, however, because I'm not an expert and have no clue what's the right definition, or the most general one, can you give me please the most general one that comprises all the others (I'm saying that because the dimension plays important role here, for instance in dimension one over $\mathbb{C}$ we have another definition sometimes, as it admits a ""Riemann surface"" structure and a couple of notes define them alternatively as in this way).","['riemann-surfaces', 'algebraic-geometry', 'algebraic-number-theory', 'algebraic-curves', 'definition']"
2055314,"Given a continuous function $f:\Bbb Q \to\Bbb Q$ . Does there exist a continuous function $g:\Bbb R\to \Bbb R$, such that restriction of $g$ is $f$? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $f$ be a continuous function $f:\Bbb Q\to \Bbb Q$.
Does there exist a continuous function $g:\Bbb R\to \Bbb R$, such that restriction of $g$ to $\Bbb Q$ is $f$?",['functions']
2055355,Find $P(n+1)$ where $P(x)$ is a $n$ degree polynomial. [duplicate],"This question already has answers here : If for a polynomial $P(k) = 2^k$ for $k = 0, 1, . . . , n$, what is $P(n+1)$? (2 answers) Closed 7 years ago . A polynomial $P(x)$ of $n$ degree satisfies $P(k)=2^k$ for $k = 0,1,2,3......,n$.  Find the value of $P(n+1)$. How can I proceed in solving such problems.",['algebra-precalculus']
2055374,Transitivity of Algebraic Field Extensions,"Consider the fields $F, E,$ and $K$, where $F \subseteq E \subseteq K$. If $E$ is algebraic over $F$, and $K$ is algebraic over $E$, show that $K$ must be algebraic over $F$. I know this is a well-known, proved theorem, but I'm trying to understand it on my own. If $E$ is algebraic over $F$, then that means there is some $\alpha_1$ in $E$ s.t. $f(\alpha_1)$ = $0$ for some $f(x) \in F[x]$. Similarly for $K$ and $E$, there is some $\alpha_2$ in $K$ s.t. $e(\alpha_2$) = $0$ for some $e(x) \in E[x]$. What I'm stuck on is connecting $\alpha_1$ and $\alpha_2$ together. If $\alpha_2$ is algebraic over $E$, how does that translate to it also being algebraic over $F$? Thank you for your help.","['abstract-algebra', 'extension-field', 'field-theory']"
2055402,Why do we unavoidably (or not) use Riemann integral to define It√¥ integral?,"https://en.wikipedia.org/wiki/It√¥_calculus Define
$$\int_0^tH_tdB_t\equiv \lim_{n\rightarrow\infty}\sum_{i=1}^nH_{t_i}(B_{t_i}-B_{t_{i-1}})$$ But I'm wondering why not defining this using Lebesgue Integral ? It looks more consistent, meaning we can somehow 'obsolete' Riemann integral after knowing Lebesgue Integral. Moreover, we can integrate processes that are hard to integrate. Isn't this good? Quote from wikipedia: Suppose that $B$ is a Wiener process (Brownian motion) and that $H$ is a right-continuous (cadlag), adapted and locally bounded process. If 
$\{œÄ_n\}$ is a sequence of partitions of $[0, t]$ with mesh going to zero, then the It√¥ integral of H with respect to B up to time t is a random variable. It can be shown that this limit converges in probability... Or maybe because the above condition that the limit converge is strong enough in most situations?","['stochastic-analysis', 'probability-theory', 'brownian-motion']"
2055403,"The average determinant of all integer matrices with coefficients $0,1,2$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $S$ denote the set of $A \in M(n,\mathbb R)$ such that every entry of $A$ is either of $0$, $1$ or $2$, then is it true that $$\sum_{A \in S} \det A \ge 3^{n^2}\ ?$$","['matrices', 'determinant']"
2055436,Are closed subgroups of positive measure open?,Let $G$ be a locally compact Hausdorff group and $H \le G$ a closed subgroup with positive Haar measure. Is $H$ open then? For Lie groups this should be true (since positive measure gives full dimension) but I'm wondering if the result can be generalized.,"['abstract-algebra', 'topological-groups', 'measure-theory', 'lie-groups']"
2055461,Example of Holmested formula for real interpolation spaces [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Can anybody give me an example of Holmested formula for real interpolation spaces.","['functional-analysis', 'interpolation-theory']"
2055480,Conditional convergence of a power series on the $z \in \mathbb{C}$ for which $|z|=\rho$ [duplicate],"This question already has answers here : Behavior of $\sum_{n=1}^\infty n^{-1}z^n$ on the circle of convergence (2 answers) Closed 7 years ago . Consider the following complex power series 
$$\sum_{n \geq 1} \frac{z^n}{n} \,\,\,\,\,\,\, z \in \mathbb{C}$$ It surely converges conditionally for $z=-1$ (for alternating series test) and for $z=1$ it diverges (it is the harmonic series). My question is: how can one show that the power series converges conditionally for any $z \in \mathbb{C}$ such that $|z|=1$ (except for $z=1$)?","['sequences-and-series', 'calculus', 'complex-analysis', 'power-series', 'convergence-divergence']"
2055481,Why not learn the multi-variate chain rule in Calculus I?,"I am wondering why we don't learn the multi-variate chain rule in Calculus I?  I know the name implies it is more suitable for multi-variable Calculus, but after learning it, I've found it very useful.  Notably, one does not need to remember product rule or quotient rule or regular chain rule, and I don't think you would have to learn about logarithmic differentiation either. So with all these advantages, why don't we teach it?","['multivariable-calculus', 'education', 'calculus', 'derivatives']"
2055505,What is $3^{\sqrt2}$? Definition of irrational Powers,"What is $3^{\sqrt2}$ ? Clearly we can say that $a^m = a \times a \times ...\times a \times a$ (m times) That is  $$3^2=3 \times 3 $$ But how can we define $3^{\sqrt2}$ ? How to understand the definition of irrational Powers ?
I do not need the value. I need a definition to understand this.","['number-theory', 'exponentiation']"
2055507,Percentage of natural numbers that are perfect squares?,While reading about the prime number distribution I came across this fact that the percentage of natural numbers that are perfect square is zero. How do I prove this ?,"['number-theory', 'elementary-number-theory']"
2055508,A bag contains 9 balls 3 of which are blue,"A bag contains 9 balls 3 of which are blue. Suppose one draws one ball at a time, until the bag is empty. What is the probability of drawing three blue balls consecutively?",['probability']
2055535,Cat dead or alive?,"You put a cat into a toxic box, it might be dead or alive after an
  hour. Two witches $A$ and $B$ have the ability to predict the status of the cat
  with the accuracy of $p_1$ and $p_2$, respectively.Suppose their prediction are independent. Both of them claim
  that they ""see"" the cat is still alive in the future. Question: from
  their words, can you calculate the probability that the cat is still
  alive after an hour? I have two solutions, I am so confused about them, especially the second one, please have patience and feel my confusion. Solution 1 Two possibilities: $A$ right, $B$ right, so cat is alive $A$ wrong, $B$ wrong, so cat is dead One right and one wrong is impossible, therefore the probability that the cat is alive $p_1p_2/(p_1p_2+(1-p_1)(1-p_2))$ Solution 2 I will translate A has the accuracy of $p_1$ to
$$
p_1 = P(L)P(A_L|L)+P(D)P(A_D|D)
$$
above formula says if the cat is alive, $A$ predict it is alive; if the cat is dead, $A$ predict it is dead. Similarly:
$$
p_2 = P(L)P(B_L|L)+P(D)P(B_D|D)
$$
Because the cat is either alive or dead, so $P(L)+P(D)=1$ We want to calculate the probability that the cat is alive based on the words of $A$ and $B$: $$
P(L|A_L,B_L) = \frac{P(A_L,B_L|L)P(L)}{P(A_L,B_L)} = \frac{P(A_L,B_L|L)P(L)}{P(A_L,B_L|L)P(L)+P(A_L,B_L|D)P(D)}
$$ which seems not the same as solution 1 as far as I can tell. This solution makes me very uncomfortable about the meaning of $P(D)$ and $P(L)$ , are they the unknown probability of the cat is dead or alive if I have no witches words? Edit I found the solution 1 is the result of explaining the accuracy of prediction differently than solution 2. Explicitly, it explain $p_1=P(D|A_D)=P(L|A_L)$, which means, when $A$ says the cat is dead, the probability that the cat turn out to be dead is $p_1$; when $A$ says the cat is alive, the probability that the cat turn out to be alive is also $p_1$. Therefore, $$
P(L|A_L,B_L)=\frac{P(L|A_L,B_L)}{P(L|A_L,B_L)+ P(D|A_L,B_L)}= \frac{p_1p_2}{p_1p_2+(1-p_1)(1-p_2)}
$$ Edit2 As discussed, I think the most confusing part is how to properly define the accuracy of the witches, do you think there are ambiguities in the definition of this quantity? At least three candidates make some sense to me: $p_1=P(A_L|L)=P(A_D|D)$ This says: if the cat is alive, then the witch predict it as alive with probability $p_1$; if the cat is dead, then witch predict it as dead with probability $p_1$; $p_1 = P(L|A_L) = P(D|A_D)$ This says: if the witch says the cat is alive, then the probability that the cat turn out to be alive with probability $p_1$; same as when the witch says the cat is dead. $p_1 = P(A_L|L)P(L) + P(A_D|D)P(D)$ This says: the cat is either dead or alive, if it is alive, the witch predict it as alive; if it is dead, the witch predict it as dead; the sum of them should be the accuracy of the witch. As pointed out by one answer, this definition allows that $P(A_L|L)\neq P(A_D|D)$, but is this a big problem? Which of the above three make most sense to you, which is nonsense and which make some sense?","['bayesian', 'probability-theory', 'probability']"
2055536,Index of positivity of a symmetric form,"Old qual question: Let $V$ be a finite dimensional positive definite inner product space over $\mathbb{R}$ under $\langle,\rangle$ and let $A:V\to V$ be a linear map. Define $B=A^tA$. Now define a new bilinear form $\{,\}$ by
$$\{v,w\}=\langle Bv,w\rangle.$$
Assume that $\ker A=m$. Prove that $\{,\}$ is symmetric. Calculate the index of positivity, negativity, nullity of the bilinear form $\{,\}$. (Recall that the index of positivity (resp. negativity, nullity of $\{,\}$ means the number of the positive (resp. negative, zero) eigenvalues of the corresponding symmetric matrix.) OK so part 1 is easy, and I did this by putting $V$ in an ONB and then the adjoint of $A$ is just $A^t$. However, I'm not sure how to get going on part 2. My friend suggested showing that $\ker A=\ker A^tA$, and I can do that, but I'm still not sure where to go. Thank you.","['abstract-algebra', 'linear-algebra', 'inner-products']"
2055543,On the sum of all elements of inverted correlation matrix,"Assume I have a correlation matrix,$A$
 \begin{equation}
A_{i,j} =  
\begin{cases}
1,& \text{if}\ i=j\\
\rho_{i,j},& \text{otherwise}  \\
\end{cases}
\end{equation} 
Where $ 0\leq \rho_{i,j} \leq 1 $  and $A$ is positive definite. Now define a correlation matrix $\bar{A}$ as follows:
\begin{equation}
\overline{A} =  
\begin{cases}
1,& \text{if}\ i=j\\
\overline{\rho},& \text{otherwise} \\
\end{cases}
\end{equation}
Where $\overline{\rho}=\frac{\sum_{i \neq j} \rho_{i,j}}{n^{2}-n}$ I want to prove that the sum of elements in the inverse matrix $\overline{A}^{-1}$ is less than the the sum of elements in the inverse matrix ${A}^{-1}$. Any ideas? I wrote a program that makes random matrices of this sort and it was true for all matrices in many different dimensions so it can't be just by chance. I just don't know how to prove it. Thanks","['probability', 'linear-algebra']"
2055557,How to prove $1-\ln{m}<x_{1}x_{2}<\frac{\ln{m}}{m-1}$,"Let $x_{1},x_{2}$ be two roots of the equation 
  $$x-\ln{x}-m=0\; (m>1).$$ 
  Show that 
  $$1-\ln{m}<x_{1}x_{2}<\dfrac{\ln{m}}{m-1}.$$ So far, I only made
$$\begin{cases}
x_{1}-\ln{x_{1}}=m\\
x_{2}-\ln{x_{2}}=m
\end{cases}\Longrightarrow x_{1}-x_{2}=\ln{\dfrac{x_{1}}{x_{2}}}$$
and set $\dfrac{x_{1}}{x_{2}}=t>1$, then we have
$$x_{2}=\dfrac{\ln{t}}{t-1},x_{1}=\dfrac{t\ln{t}}{t-1}.$$
So it remains to prove $$1-\ln{m}<\dfrac{t\ln^2{t}}{(t-1)^2}<\dfrac{\ln{m}}{m-1}.$$","['logarithms', 'inequality', 'calculus']"
2055559,"Let $a,b,c$ be the length of sides of a triangle then prove that $a^2b(a-b)+b^2c(b-c)+c^2a(c-a)\ge0$ [duplicate]","This question already has answers here : For any triangle with sides $a$, $b$, $c$, show that $a^2b(a-b)+b^2c(b-c)+c^2a(c-a)\ge 0$ (6 answers) Closed 5 years ago . Let $a,b,c$ be the length of sides of a triangle then prove that: $a^2b(a-b)+b^2c(b-c)+c^2a(c-a)\ge0$ Please help me!!!","['polynomials', 'substitution', 'geometric-inequalities', 'geometry', 'buffalo-way']"
2055667,A reference/proof request for a result on real radical extensions,"I am looking for an English reference or a proof of the following result due to Holder: ""Let $ f(x) $ be an irreducible polynomial over a real field $ K $. If all the roots of $ f(x) $ are real and expressible by real radicals (in the sense that they lie in some real radical extension of $ K $), then the Galois group of $ f $ over $ K $ is a $ 2 $-group."" (quoted verbatim from this article .) In the linked article, there is a reference, but it is German. I have found a page on this result on planetmath, but their argument depends on this proof , where I do not see how $ L' = F'(\sqrt[n]{\beta}) $ implies that $ L = F(\sqrt[n]{\beta}) $, or even that $ \beta \in F $ in the first place. I would appreciate it if someone provided an English reference for this result, or even better, posted a proof sketch as an answer. Note: The planetmath references have made a previous appearance on the site in the comments section of this question , but Gerry Myerson's proof sketch is extremely lacking. (I do not see how to make his argument work, concretely.)","['abstract-algebra', 'galois-theory', 'field-theory']"
2055751,How to prove there are no solutions to the equation $a^2-4ab+b^2=0$ if $a$ and $b$ are real numbers and $b \neq 0$?,I am trying to prove $a^2-4ab+b^2=0$ has no solutions for all real numbers $a$ and $b$ and $b \neq 0$ My attempt: We know that $a^2 \geq 0$ and $b^2 > 0$ since $b \neq 0$. So then $a^2 + b^2 >0$. Now I'm stuck as I'm not sure how to show that $a^2 + b^2 = 4ab$ has no solutions given the above conditions. A little assistance would be greatly appreciated.,['algebra-precalculus']
2055816,"Mathematical Induction, Want to check I'm getting this right [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I just want to make sure i'm doing this right for other questions. i have a question: 
$$\sum\limits_{j=1}^{n}\frac{1}{j(j+1)} =  \frac{1}{n(n+1)}$$ and have to show that the function $$f(n) = \frac{n}{n+1}$$ applies for all n in N . so it's the same setup right? where i do the n =1. then assume that it's true for n and prove that its true for n + 1 by simply saying $$\frac{n+1}{n+2}$$ right?","['induction', 'proof-explanation', 'proof-verification', 'discrete-mathematics']"
2055833,How can I show that $4^{1536} - 9^{4824}$ can be divided by $35$ without remainder?,"How can I show that $4^{1536} - 9^{4824}$ can be divided by $35$ without remainder? I'm not even sure how to begin solving this, any hints are welcomed! $$(4^{1536} - 9^{4824}) \pmod{35} = 0$$",['discrete-mathematics']
2055910,Lipschitz continuous Gradient and bounded Hessian,"Is the following proposition true? Is there a simple proof? Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a twice continuously differentiable function. If there exists $M$ such that $||\nabla^2 f(x)||_2 \leq M$ for all $x$ then 
$$
\forall x, y \in \mathbb{R}^n: \quad ||\nabla f(x) - \nabla f(y)||_2 \leq M ||x - y||_2
$$ Update I believe I was able to prove it. Please tell me if I am wrong here, and where. First, we remember that any real symmetric matrix $H$ is diagonalizable $H = P^T \Lambda P$, and therefore:
$$
u^T H u = (P u)^T \Lambda (P u) \leq \lambda_H ||P u||^2 = \lambda_H ||u||^2
$$
where 
$$\lambda_H = \max_{i = 1, \dots, n} \{ |\lambda_1(H)|, \dots, |\lambda_n(H)| \}$$
Also, since $||H||_2^2 = \lambda_{max}(H^T H)$ and we know that $H^T H = P^T \Lambda^2 P$, then we also know that $||H||_2 = \lambda_H$ and therefore:
$$
u^T H u \leq ||H||_2 ||u||^2
$$ Now, using the multivariate first order Taylor approximation, we have:
$$
\begin{aligned}
f(x) - f(y) = \nabla f(y)^T (x - y) + 0.5 (y - z)^T \nabla^2 f(z) (y - z) \\
f(y) - f(x) = -\nabla f(x)^T (x - y) + 0.5 (x - w)^T \nabla^2 f(w) (x - w)
\end{aligned}
$$
where $z \in [x, y]$ and $w \in [x, y]$. Adding both equations, and re-arranging, we have:
$$
\begin{aligned}
(\nabla f(y) - \nabla f(x))^T(x - y) 
 &=  -0.5 (y - z)^T \nabla^2 f(z) (y - z) - 0.5 (x - w)^T \nabla^2 f(w) (x - w) \\
 &\geq -0.5 ||\nabla^2 f(z)||_2 ||y - z||^2 - 0.5 ||\nabla^2 f(w)||_2 ||x - w||^2 \\
 &\geq -0.5 M ||y - z||^2 - 0.5 M ||x - w||^2 \\
 &\geq -M ||x - y||^2
\end{aligned}
$$
Using Cauchy-Schwartz we also obtain
$$
- ||\nabla f(y) - \nabla f(x)|| \cdot ||y - x|| \geq (\nabla f(y) - \nabla f(x))^T(x - y) \geq -M ||x - y||^2
$$
Dividing both sides by $-||y - x||$ gives the desired result.","['lipschitz-functions', 'analysis']"
2055927,Is a map between Riemannian manifolds with bounded derivative Lipschitz?,"Let $X,Y$ be Riemannian manifolds. Let $f:X \to Y$ be an everywhere differentiable map, such that the operator norm $\|df_p\|_{op}$ is globally bounded (from above) by $L$. Is it true that $f$ is Lipchitz? (what about $L$-Lipschitzity?) I am interested in the general question in the case of low regularity , i.e I do not assume $f$ is $C^1$ (then it's quite easy ). In particular this means that for a smooth path $\alpha:I \to X$, we do not know a-priori that $f\circ \alpha$ is Lipschitz, so we cannot necessarily write its length as the integral of its speed. Note that in the case where $X,Y$ are Euclidean spaces, the answer is positive and is known as the ""mean value inequality"". However, the standard proof does not seem to pass over to the general case. Updated ""conjecture"": I guess $f$ does have to be Lipschitz, and even $L$-Lipschitz. First, since locally the Riemannian metric is ""close"" to Euclidean metric, I guess one could claim local-Lipschitzity certainly holds (admittedly, some details are probably required to justify this rigorously). Then, for any smooth path $\alpha$ in $X$, $f \circ \alpha$ will also be locally-Lipchitz and (compactness?) even Lipchitz. So we can express its length as the integral of its speed, so $f$ will be $L$-Lipschitz by the usual argument.","['derivatives', 'riemannian-geometry', 'lipschitz-functions', 'metric-spaces', 'differential-geometry']"
2055955,rank(AB) = rank(A) if B is invertible [duplicate],"This question already has an answer here : Prove $\operatorname{rank}(AB) = \operatorname{rank}(B)$ (1 answer) Closed 7 years ago . If $B$ is invertible, show that rank($AB$) = rank($A$). I've seen this question asked elsewhere but all had answers I didn't understand. I know how to solve the following problem If $A$ is invertible, then rank($AB$) = rank($B$) Because if $Bx=0$, then $ABx = A0 = 0$, and when $ABx=0$ then $Bx=0$ because $A$ is invertible, so null($AB$)=null($A$), and by the rank-nullity theorem, rank($A$) = rank($AB$). However when $B$ is invertible, as in the problem we have to tackle, I don't know how to use that fact. $ABx = 0$, but $B$ is in the middle so we can't simply get rid of it to get a meaningful expression. Does someone know how to tackle this?","['matrices', 'matrix-rank', 'linear-algebra']"
2055976,Bhattacharyya coefficient convergence,"Suppose we have a sequence of (continuous) random variables $(X_n)_{n\geq 1}$ with corresponding densities $p_n(x)$ and another sequence $(Y_n)_{n\geq 1}$ with corresponding densities $q_n(x)$ such that $X_n \Rightarrow X \sim \mathcal{N}(\mu_X,\Sigma_X)$ in distribution and $Y_n \Rightarrow Y \sim \mathcal{N}(\mu_Y,\Sigma_Y)$ in distribution. If we let $p(x)$ and $q(x)$ be the densities of $\mathcal{N}(\mu_X,\Sigma_X)$ and $\mathcal{N}(\mu_Y,\Sigma_Y)$ respectively, is it true that the Bhattacharyya coefficient between $p_n$ and $q_n$ given by $$BC(p_n,q_n) = \int \sqrt{p_n(x)q_n(x)}dx$$
converges to the BC of $p$ and $q$, $BC(p,q)=\int \sqrt{p(x)q(x)}dx$ ? I wonder if any conclusions can be made as convergence in distribution does not even necessarily imply convergence of the respective densities.","['density-function', 'normal-distribution', 'probability-theory', 'convergence-divergence', 'random-variables']"
2056020,Draw a rectangle bounded by a curve and two lines,"Consider $f(x) = x^2-6x+9$ , $y=x+2$ and $y=8-x$. If we want to draw a rectangle with two vertices on $f(x)$ and the other two vertices are on $y=x+2$, $y=8-x$. What the largest possible area of such a rectangle. If we have an edge parallel to the x-axis this question is easy but what about the other case? I think the other case is impossible, but I could not fund out why. My attempt Let $a(x_1,y_1) , b(x_2,y_2)$ the two vertices on $f(x)$ and $c(x_3,y_3), d(x_4,y_4)$ the other two on the lines $y=8-x, y= x+2$. I want to show that if $ab = cd$ and $ab // cd$ with $<abc = 90$ we should get the slope of $ab=0$. Any hints. 
Thanks in advance.","['maxima-minima', 'calculus', 'geometry']"
2056036,Question about an equivalent definition of a simple group,"A basic consequence of the first isomorphism theorem is that a finite group G is simple if and only if its only homomorphic images are G and the trivial group (up to isomorphism). However, I'm not sure whether or not this generalizes to infinite groups. If it doesn't, then that must mean there exists an infinite group G with at least one non-trivial proper normal subgroup, such that for each proper normal subgroup H of G, G is isomorphic to G/H. So does this equivalent definition generalize to infinite groups?","['infinite-groups', 'group-theory', 'simple-groups']"
2056041,"Can we find a function $f:\mathbb{R}\rightarrow \mathbb{R}$ that is open, closed, but not continuous?","The question is described in the title. To clarify, I'm looking for an example where the domain of such a function is $\mathbb{R}$ , so the example here will not work. The highest upvote answer here gives an example of discontinuous open map from $\mathbb{R}$ to $\mathbb{R}$ , but that map can hardly be closed (closed set does not need to contain any interval!). I have difficulty generalizing the construction and know very little about any sufficient condition that leads to a closed map without continuity (piecewise constant will work, but that kind of map cannot be open!). I'll appreciate any example, non-constructive prove/disprove, or any reference that covers this. *In case I need to clarify this, we use std. metric topology on $\mathbb{R}$ .","['reference-request', 'real-analysis', 'calculus']"
2056094,Standard error of exteremely biased coin,"OK, so I know that the typical standard error of a coin is estimated by $$\sigma_p=\sqrt{ \frac{p*(1-p)}n }$$
where $p$ is the estimated probability and and $n$ is the number of samples. This seems reasonable at high $n$ and $p \sim 0.5$; however, it seems unreasonable if I have $p = 1$ and $n = 20$, $\sigma_p = 0$. Is there a better formula for standard error when $ p \sim 0$ or $p \sim 1$ and $n$ is low? Note: this is a real-world problem and increasing $n$ is non-trivial. Thanks!","['statistics', 'probability']"
2056108,"$f:[-1,1] \to \mathbb{R}$, $f(0)=0$, $f'(0)\ne0$. Show that $\sum_{k=2}^\infty f \left( \frac{1}{n \ln n} \right)$ diverges.","$f:[-1,1] \to \mathbb{R}$,  $f(0)=0$, $f'(0)\ne 0$. Show that $\sum_{k=2}^\infty f \left( \frac{1}{n \ln n} \right)$ diverges. I can't figure out how the information about the derivative helps me. Hint, please?","['summation', 'sequences-and-series']"
2056120,"How do I find all prime solutions $p, q, r$ of the equation $\displaystyle p(p+1)+q(q+1) = r(r+1)$?","Find primes $p, q, r$ of the equation $$p(p+1)+q(q+1)
= r(r+1)$$ I know that it has only one solution namely $p = q = 2,r = 3$ . But i can't show that. Thank you for any help","['number-theory', 'divisibility', 'prime-numbers', 'elementary-number-theory']"
2056150,"$f: \mathbb{R} \rightarrow \mathbb{R}, f(0)=1, f(x+y) \leq f(x)f(y)$. Show that if $f$ continuous in $x=0$, $f$ is continuous in $\mathbb{R}$.","So far I have shown that the equality only holds when $x=0$ or $y=0$.
I also have found out that $f(nx) \leq f(x)^n$ for natural $n$ (otherwise the summation doesn't make sense). But I do not know how to deduce the continuity. 
My idea is to show it with the sequence criterion for continuity, but the only thing I've shown with it is that for $n \rightarrow 0$ the function $f(nx) \rightarrow f(0)=1$, which doesn't help a lot. I also could use that $f$ is continuous in $x=0$ to make any $f(x)$ a product of $f(1)$, but it only holds for natural numbers, thus not leading to a solution. I would appreciate hints, not solutions. Thank you for help.",['analysis']
2056151,Prove that there does not exist a largest natural number,I need a very simple proof for this. I could say let $x$ be the largest natural number. Then $x + 1 > x$. QED. But that doesn't really work does it? What's a very simple proof (no number theory or any sort like that)?,"['number-theory', 'discrete-mathematics']"
2056235,What is the geometric interpretation of the value of the secant and cosecant of an angle?,"I am confused about what is the geometric representation and interpretation of the secant and cosecant of an angle. I understand how to calculate them but I do not know what they mean, geometrically.","['circles', 'trigonometry', 'terminology', 'triangles', 'geometry']"
