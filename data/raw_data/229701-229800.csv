question_id,title,body,tags
4759476,"Is possibile to prove the equality $\left(⋃_{X∈\mathfrak X}X\right)×\left(⋃_{Y\in\mathfrak Y}Y\right)=⋃_{(X,Y)∈\mathfrak X×Y}(X×Y)$ using projections?","So I know that if $A$ is a subset of a set $X$ and if $X$ is a subset of a set $Y$ then the equalities $$
\begin{equation}
    \tag{1}\label{1}A\times B=\pi_X^{-1}[A]\cap\pi_Y^{-1}[B]
\end{equation}
$$ holds, right ?. Moreover, if $\mathfrak X$ and $\mathfrak Y$ are collections then we know that the euqlity $$
\begin{equation}
    \tag{2}\label{2}\left(\bigcap\mathfrak X\right)\cap\left(\bigcap\mathfrak Y\right)=\bigcap_{(X,Y)\in\mathfrak X\times\mathfrak Y}(X\cap Y)
\end{equation}
$$ holds. Finally if $f$ is a function from a set $H$ to a set $K$ we know that for any collection $\mathfrak L$ the equality $$
\begin{equation}
    \tag{3}\label{3}f^{-1}\left[\bigcap\mathfrak L\right]=\bigcap_{L\in\mathfrak L}f^{-1}[L]
\end{equation}
$$ holds. So I asked to me if using eq. \eqref{1}, eq. \eqref{2} and eq. \eqref{3} is possibile to prove the equality $$
\begin{equation}
    \tag{4}\label{4}\left(\bigcap_{U\in\mathfrak U}U\right)\times\left(\bigcap_{V\in\mathfrak V}V\right)=\bigcap_{(U,V)\in\mathfrak U\times\mathfrak V}(U\times V)
\end{equation}
$$ where $\mathfrak U$ and $\mathfrak V$ are arbitrary collections. So to do this I put $$
M:=\bigcup\mathfrak U\quad\text{and}\quad N:=\bigcup\mathfrak V
$$ and thus I observed the inclusion $$
\bigcap\mathfrak U\subseteq M
$$ and the inclusion $$
\bigcap\mathfrak V\subseteq N
$$ hold so that by \eqref{1}, \eqref{2} and \eqref{3} I argued the equality $$
\left(\bigcap_{U\in\mathfrak U}U\right)\times\left(\bigcap_{V\in\mathfrak V}V\right)=\pi_M^{-1}\left[\bigcap_{U\in\mathfrak U}U\right]\cap\pi_N^{-1}\left[\bigcap_{V\in\mathfrak V}V\right]=\left(\bigcap_{U\in\mathfrak U}\pi_M^{-1}[U]\right)\cap\left(\bigcap_{V\in\mathfrak V}\pi_N^{-1}[V]\right)=\bigcap_{(U,V)\in\mathfrak U\times\mathfrak V}\big(\pi_M^{-1}[U]\cap\pi_N^{-1}[V]\big)=\bigcap_{(U,V)\in\mathfrak U\times\mathfrak V}(U\times V)
$$ since any element of $\mathfrak U$ is a subset of $M$ and since any element of $\mathfrak V$ is a subset of $N$ . Now let be $A$ , $B$ , $C$ and $D$ sets so that let's we put $$
P:=A\cup B\quad\text{and}\quad Q:=C\cup D
$$ so that the inclusion $$
A,B\subseteq P
$$ and the inclusion $$
C,D\subseteq Q
$$ holds. So I observed that the equality holds $$
\begin{equation}
\tag{5}\label{5}
(A\cap B)\times(C\cap D)=\pi_P^{-1}[A\cap B]\cap\pi_Q^{-1}[C\cap D]=\big(\pi_P^{-1}[A]\cap\pi_P^{-1}[B]\big)\cap\big(\pi_Q^{-1}[C]\cap\pi_Q^{-1}[D]\big)=\big(\pi_P^{-1}[A]\cap\pi_Q^{-1}[C]\big)\cap(\pi_P^{-1}[B]\cap\pi_Q^{-1}[D]\big)=(A\times C)\cap(B\times D)
\end{equation}
$$ Now I am quite sure that \eqref{4} holds since it is differently proved in really many texts. However in a text ( Teoria de Conjuntos by Fernando Hernandez) it is written that \eqref{5} is true surely if $A$ and $B$ are subset of any set $X$ and if $C$ and $D$ are subset of any set $Y$ but into this wiki article it is said apparently that \eqref{5} generally holds: so if $A$ and $B$ are not contained into a set $X$ and if $C$ and $D$ are not contained into a set $Y$ then this is not evident by more ordinary arguments (e.g. if $x$ is in $(A\cap B)\times(C\cap D)$ then... ) to me so that I thought I take a mistake and thus I thought to put a question here where I ask if \eqref{5} generally holds and so if I well proved \eqref{4} and \eqref{5} since if \eqref{5} does not holds then it seems to me I do not well proved \eqref{4}. So could someone help me, please?","['elementary-set-theory', 'solution-verification', 'alternative-proof', 'examples-counterexamples']"
4759487,When is row equivalence equivalent to the equivalence of systems of equations?,"Theorem 3 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 3: If $A$ and $B$ are row-equivalent $m \times n$ matrices, the homogeneous systems of linear equations $AX = 0$ and $BX = 0$ have exactly the same solutions. The proof intimately uses that the systems in question are homogeneous, but I'm not sure I can quite see how. I am also led to wonder about how this homogeneous assumption can be relaxed (perhaps I am jumping the gun and HK will discuss this). I wonder about these two questions at any rate: Can someone explicate exactly how the homogeneous assumption is used? In the crucial step, Hoffman and Kunze write (after reducing the proof by induction to equivalence of systems differing by just one row operation) No matter which of the three types the row operation is, each equation in the system $A_{j+1}X$ = 0 will be a linear combination of the equations in the system $AX = 0$ . Now my confusion is that row operations only act on the matrix $A$ of coefficients, not the vector $Y$ of inhomogeneous scalars. Is the point that, in general, we would also have to do the row operations on $Y$ in order to obtain a system of equations which is a linear combination of the original equations, but that here since $Y = 0$ this action is trivial? Theorem 3 can perhaps be rewritten as ""if two systems of equations are homogeneous then row equivalence of their coefficient matrices implies that the systems are equivalent"" (here we recall that systems of equations being equivalent means that their constituent equations can be written as linear combinations of one another). My question here is (a) does the converse hold (""if two systems of equations are homogeneous then their equivalence implies row equivalence of their coefficient matrices"") and (b) if (a) is true, is there an if and only if statement to be similarly made if we relax homogeneity (presumably we'd need to add a hypothesis about the $Y$ vector)? If it's possible for any answer to be really clear about all the possible cases at play here I would greatly appreciate it, as I know there are some pathological cases involving inconsistent systems which sometimes confuse me.","['matrices', 'matrix-equations', 'systems-of-equations', 'linear-algebra']"
4759493,How deduce $\prod_{k=1}^{(n-1)/2} \sin^2 \frac{k\pi}{n} =\frac{n}{2^{n-1}}$ from $\prod_{k=1}^{n-1} \sin \frac{k\pi}{n} =\frac{n}{2^{n-1}}$?,"I know that $\displaystyle\prod_{k=1}^{n-1} \sin \frac{k\pi}{n} =\frac{n}{2^{n-1}}$ for any integer $n \geq 1$ is true. Now, suppose that $n$ is odd, how show $$
\prod_{k=1}^{(n-1)/2} \sin^2 \frac{k\pi}{n} =\frac{n}{2^{n-1}} ?
$$ I read in a book (Bourbaki) that the first equality implies the second.","['trigonometry', 'summation', 'real-analysis']"
4759501,Why do I mistakenly think there's an error in this proof by Hoffman and Kunze?,"Theorem 4 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 4: Every $m \times n$ matrix over the field $F$ is row-equivalent to a row-reduced matrix. Before supplying the proof, I'll note their definition of a row-reduced matrix: Definition. An $m \times n$ matrix $R$ is called row-reduced if:
(a) the first non-zero entry in each non-zero row of R is equal to 1;
(b) each column of $R$ which contains the leading non-zero entry of some row has all its other entries 0. Their proof of Theorem 4 is now given as below: Proof. Let A be an $m \times n$ matrix over $F$ . If every entry in the first row of $A$ is 0, then condition (a) is satisfied insofar as row 1 is concerned. If row 1 has a non-zero entry, let $k$ be the smallest positive integer $j$ for which $A_{1j} \neq 0$ . Multiply row 1 by $A_{1k}^{-1}$ , and then condition (a) is satisfied with regard to row 1. Now for each $i \geq 2$ , add ( $-A_{ik}$ ) times row 1 to row i. Now the leading non-zero entry of row 1 occurs in column $k$ , that entry is 1, and every other entry in column $k$ is 0.
Now consider the matrix which has resulted from above. If every entry in row 2 is 0, we do nothing to row 2. If some entry in row 2 is different from 0, we multiply row 2 by a scalar so that the leading non-zero entry is 1. In the event that row 1 had a leading non-zero entry in column $k$ , this leading non-zero entry of row 2 cannot occur in column $k$ ; say it occurs in column $k' \neq k$ . By adding suitable multiples of row 2 to the various rows, we can arrange that all entries in column $k'$ are 0, except the 1 in row 2. The important thing to notice is this: In carrying out these last operations, we will not change the entries of row 1 in columns $1, . . . , k$ ; nor will we change any entry of column $k$ . Of course, if row 1 was identically 0, the operations with row 2 will not affect row 1.
Working with one row at a time in the above manner, it is clear that in a finite number of steps we will arrive at a row-reduced matrix. My concern is with the bolded parts, and in particular the claim that ""these last operations, we will not change the entries of row 1 in columns $1, . . . , k$ "". Surely if $k>k'$ this need not be true in general, for then any entry $A_{1m}$ with $k' \leq m <k$ will be changed since it is permitted that $A_{2m} \neq 0$ for such $m$ ? Is it possible that HK make an error here and instead meant to say ""these last operations, we will not change the entries of row 1 in columns $1, . . . , k'$ ""? Or, perhaps, if we consult this proof from ProofWiki, we see that the first step is to consider the first non-zero column $j$ , and then take any row which contains a non-zero entry in said column. HK do no such thing, and just plug on ahead. If they did do this, I think their proof would be rescued since we would always have $k<k'$ , right? This did not appear as an erratum in this thread, so I am concerned that I am misunderstanding the proof.","['proof-explanation', 'linear-algebra']"
4759512,$\chi^2$ Distribution for Rectified Gaussians,"We know that a Chi-squared distribution is the distribution of the sum of squared independent Gaussian random variables $Z_i \sim N(0, 1)$ : $$ Y = \sum_{i=0}^k Z_i^2 $$ . If we have another random variable $Z^R_I$ which is a rectified Gaussian, $$ Z_i^R = max(0, Z_i) $$ , Would it be logical to say that the distribution of the sum of squared independent rectified Gaussian random variables $Z_i^R$ is equivalent to the sum of to half a chi-squared distribution? $$ Y^R = \sum_{i=0}^{k} (Z^R_{i})^2 = \sum_{i=0}^{k/2} Z_i^2 $$ $$ Y^R \sim \chi^2_{k/2} $$ Furthermore , if $Var[Y] = 2k$ , would $Var[Y^R] = k$ and $E[Y] = k$ , would $E[Y^R] = k/2$ ?","['chi-squared', 'statistics', 'probability-distributions', 'probability']"
4759514,"Estimating how many of the first $10,000$ Fibonacci numbers start with the digit $9$","Consider the problem of estimating how many of the first $10,000$ Fibonacci numbers begin with the digit $9$ . The only ideas I have so far: Obviously, if we assume that the every first digit is equally likely, the answer is around $1000$ (Note: Dietrich Burde points out that is wrong. $0$ can't be the first digit, so I should divide by $9$ , not $10$ ). Listing out the Fibonacci numbers: $1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, ...$ , we can see that their first digits, $1, 1, 2, 3, 5, 8, 1, 2, 3, 5, 8, 1, 2, 3, 6, 9, 1, ...$ seem to follow a pattern somewhat similar to $1, 2, 3, 5, 8$ , with $9$ 's introduced less often. So perhaps the answer is less than $1000$ . I'll put the answer here: 456 of the first $10,000$ Fibonacci numbers start with the digit $9$ . Any ideas/hints of how to estimate or compute this analytically?","['contest-math', 'fibonacci-numbers', 'estimation', 'decimal-expansion', 'probability']"
4759536,How do we prove that :$\tan^2(10)+\tan^2(50)+\tan^2(70) =9$,"Prove : $\tan^2(10) + \tan^2(50) 
+ \tan^2(70) =9$ my attempt Let $\text{t} :=\tan(10)$ $$\tan^2(10) + \tan^2(50) 
+ \tan^2(70) = \tan^2(10) + \tan^2(60-10) 
+ \tan^2(60+10)=t^2 + \left({\frac{\sqrt{3}-t}{1+\sqrt{3}t}}\right)^2+\left({\frac{\sqrt{3}+t}{1-\sqrt{3}t}}\right)^2=\frac{9t^6+45t^2+6}{(1-3t^2)^2}$$ and therfore : $$\tan^2(10) + \tan^2(50) 
+ \tan^2(70) =\frac{9\tan^6(10)+45\tan^2(10)+6}{(1-3\tan^2(10))^2}$$ I need help completing the proof or give another way and I appreciate everyone's interest","['trigonometry', 'summation']"
4759597,Classical Algebraic Geometry,"I've been wanting to start learning about some algebraic geometry and after talking to some grad students of my institute I've noticed that the overall recommendation is to start by learning some of the classical way to do it, meaning without the theory of schemes. The problem is 99% of the department learnt this way by the hands of a professor that is already retired and so I would like to know some recommendations of good texts about the subject.","['algebraic-geometry', 'book-recommendation', 'reference-request']"
4759602,"If $B$ is compact, then $\{x\}\times B$ is compact","In Spivak's Calculus on Manifolds , he writes on page 8: If $B\subset\mathbb R^m$ is compact, and $x\in\mathbb R^n$ , then it is easy to see that $\{x\}\times B\subset\mathbb R^{n+m}$ is compact. Here, $\{x\}\times B$ is defined as $\{(x^1,\dots,x^n,b^1,\dots,b^m):(b^1,\dots,b^m)\in B\}$ . Spivak uses the above fact to prove that the product of two compact sets is compact, and so we are not allowed use this more general statement to conclude the above. Despite Spivak's assertion that this is ""easy to see"", I don't think the proof is completely trivial (or at least it is not short). Is my attempt correct? The basic idea is that if $\{A_i\}_{i\in I}$ is an open cover of $\{x\}\times B$ , then for each collection $A_i$ of open sets of $\mathbb R^{n+m}$ , we can find a corresponding collection $\alpha_i$ of open sets of $\mathbb R^m$ ; from there, we can use the compactness of $B$ to find a finite subcover of $\{A_i\}_{i\in I}$ . In more detail, suppose $\{A_i\}_{i\in I}$ is an open cover of $\{x\}\times B$ . For each $A_i\subset \mathbb R^{n+m}$ , define $\alpha_i\subset\mathbb R^m$ so that $(y^1,\dots,y^m)\in\alpha_i$ if and only if $(x^1,\dots,x^n,y^1\dots,y^m)\in A_i$ . We claim that $\{\alpha_i\}_{i\in I}$ is an open cover of $B$ . Indeed, if $(b^1,\dots,b^m)\in B$ , then $(x^1,\dots,x^n,b^1,\dots,b^m)\in\{x\}\times B$ , and so there is an $i\in I$ such that $(x^1,\dots,x^n,b^1,\dots,b^m)\in A_i$ , hence $(b^1,\dots,b^m)\in\alpha_i$ ; this shows that $\{\alpha_i\}_{i\in I}$ is a cover of $B$ . To prove that it is an open cover, note that if $i\in I$ and $(y^1,\dots,y^m)\in \alpha_i$ , then $(x^1,\dots,x^n,y_1,\dots,y^m)\in A_i$ . Since $A_i$ is open, there is a $\delta>0$ such that $$
(x^1-\delta,x^1+\delta)\times\dots\times(x^n-\delta,x^n+\delta)\times(y^1-\delta,y^1+\delta)\times\dots\times(y^m-\delta,y^m+\delta)\subset A_i \, .
$$ Hence, $$
(y^1-\delta,y^1+\delta)\times\dots\times(y^m-\delta,y^m+\delta)\subset\alpha_i \, ,
$$ and $\alpha_i$ is open. Since $\{\alpha_i\}_{i\in I}$ is an open cover of $B$ , it has a finite subcover $\{\alpha_i\}_{i\in I'}$ . Then, $\{A_i\}_{i\in I'}$ is a finite subcover of $\{A_i\}_{i\in I}$ , for if $(x^1,\dots,x^n,b^1,\dots,b^m)\in\{x\}\times B$ , then $(b^1,\dots,b^m)\in B$ , so there is an $i\in I'$ such that $(b^1,\dots,b^m)\in\alpha_i$ , and consequently $(x^1,\dots,x^n,b^1,\dots,b^m)\in A_i$ . This completes the proof.","['real-analysis', 'multivariable-calculus', 'solution-verification', 'general-topology', 'compactness']"
4759630,How to find the formula of the union of two sets that are defined by linear functions?,"$$S = \{ax + b | x \in \Bbb N \}$$ $$T = \{gx + k | x \in \Bbb N \}$$ $$V = S \cup T$$ $$V_n = ? \quad\text{sorted in ascending order}$$ For example, $$S = \{3x + 2 | x \in \Bbb N \} = \{2, 5, 8, 11, 14, 17, 20, 23, 26, 29, ...\}$$ $$T = \{2x + 4 | x \in \Bbb N \} = \{4, 6, 8, 10, 12, 14, 16, 18, 20, 22, ...\}$$ $$V = S \cup T  = \{2, 4, 5, 6, 8, 10, 11, 12, 14, 16, 17, 18, 20, ...\}$$ How do I find the formula for $V_n$ given the formula for $S_n$ and $T_n$ ? Would it also be a linear function? Is there any kind of 'formula for the formula'?","['algebra-precalculus', 'functions']"
4759678,Are Bernoulli distributions log-concave?,"Question: I am aware that the common continuous distributions (like Gaussian, Uniform, Gamma) are log-concave. I am wondering if Bernoulli distributions (a discrete distribution) is log-concave? If so, then does this extend to the Categorical distribution ? My attempt: For a Bernoulli( $p$ ) distribution $$
f(k)=p^k(1-p)^{1-k} \text{ for } k\in\{0,1\},
$$ we need to show that for some $\theta\in(0,1)$ , we have $$
f\Big(\theta k_1+(1-\theta) k_2\Big)\geq f(k_1)^\theta f(k_2)^{1-\theta}.
$$ I can show this by considering two cases (i) $\theta k_1+(1-\theta) k_2=0$ and $\theta k_1+(1-\theta) k_2=1$ . Case (i) leads to $k_1,k_2=0$ which leads to equality in the log-concave inequality. Case (ii) leads to either $k_1,k_2=1$ which leads to equality in the log-concave inequality, or $k_1,k_2\notin\{0,1\}$ which leads to strict inequality in the log-concave inequality.","['statistics', 'probability-distributions', 'bernoulli-distribution', 'inequality', 'probability']"
4759687,Limit $\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right)$,"I am interested in finding this limit and the answer seems to be: $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right)=-\sin1$$ I kind of have a method for this but that is based on how I came across this series. I would love to see other solutions that are not based on how it was derived. EDIT: Also as a side note it seems that, $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{k+r}}{\left(2k-2r-1\right)!}\right)=-\sin1$$ The series without $\zeta$ equals the same. EDIT 2: I realized we could first simplify a bit by $r\to k-1+1-r=k-r$ $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2k-2r\right)\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1$$ Also notice that, $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1$$ So I thought lets try it on other series, Numerically, $$\lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{\zeta\left(2k-2r\right)}{r!}\right)=e$$ Also, notice that. $$\lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{1}{r!}\right)=e$$ This is kind of interesting, shouldn't the $\zeta$ term affect the summation?","['riemann-zeta', 'limits', 'calculus', 'summation']"
4759724,proof of almost everywhere differentiability of monotone functions,"I'm reading ""An introduction to measure theory"" from Terry Tao and I'm stuck understanding part of the proof of the following theorem:(whole argument can be found in his weblog Theorem 53.) (Monotone differentiation theorem) Any function ${F: {\bf R} \rightarrow {\bf R}}$ which is monotone (either monotone non-decreasing or monotone non-increasing) is differentiable almost everywhere. The idea to prove this theorem, is considering Dini Derivatives of a function $F$ .namely: $\bullet$ The upper right derivative $${\overline{D^+} F(x) := \limsup_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The lower right derivative $${\underline{D^+} F(x) := \liminf_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The upper left derivative $${\overline{D^-} F(x) := \limsup_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The lower right derivative $${\underline{D^-} F(x) := \liminf_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}}$$ now to show a.e differentiability we should show these 4 quantities agree a.e but it's easy to see that the only thing that is needed to be proved is ${\overline{D^+} F(x)} = {\underline{D^-} F(x)}$ a.e.
to prove this the idea is to show that $E_{r,R} := \{x \in R: {\overline{D^+} F(x)} > R > r > {\underline{D^-} F(x)}\}$ is a null set and then by letting $R>r$ both iterate over all rational numbers, conclusion follows. Tao proceeds with showing that $E_{r,R}$ has no Lebesgue point and it would give the desired conclusion. Hence the only thing one need to prove is following lemma: For any interval ${[a,b]}$ and any ${0 < r < R}$ , one has ${m( E_{r,R} \cap [a,b] ) \leq \frac{r}{R} |b-a|}$ . The proof is as follows: We begin by applying the rising sun lemma to the function ${G(x) := r x + F(-x)}$ on ${[-b,-a]}$ ; the large number of negative signs present here is needed in order to properly deal with the lower left Dini derivative ${\underline{D_-} F}$ . This gives an at most countable family of disjoint intervals ${-I_n = (-b_n,-a_n)}$ in ${(-b,-a)}$ , such that ${G(-a_n) \geq G(-b_n)}$ for all ${n}$ , and such that ${G(-x) \leq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ . Observe that if ${x \in (a,b)}$ , and ${G(-x) \leq G(-y)}$ for all ${-x \leq -y \leq -a}$ , then ${\underline{D_-} F(x) \geq r}$ . Thus we see that ${E_{r,R} \cap (a,b)}$ is contained inside the union of the intervals ${I_n = (a_n,b_n)}$ ... (full proof is not given here and can be found in the first link given above) I don't understand how rising sun lemma is applied to $G$ since I think applying rising sun lemma to $G$ gives an at most countable family of disjoint intervals ${-I_n = (-b_n,-a_n)}$ in ${(-b,-a)}$ , such that ${G(-a_n) \geq G(-b_n)}$ for all ${n}$ , and such that ${G(-x) \geq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ .I don' understand why in proof given by tao, it's said that ${G(-x) \leq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ .
Thanks.","['proof-explanation', 'measure-theory', 'real-analysis']"
4759760,Dividing a tempered distribution by a polynomial,"Let $p=p(x_1,...,x_N)$ be a non-zero polynomial in $N$ variables (real coefficients). Let $\mathscr{S}$ be the Schwartz space on $\mathbb{R}^N$ and let $\mathscr{S}'$ be its topological dual (i.e. the space of tempered distributions). I know that the map $\mathscr{S}'\to \mathscr{S}', \ \ \ T\mapsto pT$ (where $\langle pT,\psi \rangle:=\langle T, p\psi \rangle$ ) is linear and continous. My class notes affirm that this map has also a continous inverse given by $\mathscr{S}'\to \mathscr{S}', \ \ \ T\mapsto T/p$ and I don't understand this. This works if $p$ hasn't any real zeroes, but otherwise $T/p$ shouldn't be well-defined. I understand that the zeroes of $p$ form a null measure set so maybe there is some way of solving this issue. But I don't know how. Can you help me?","['schwartz-space', 'functional-analysis', 'distribution-theory']"
4759762,On the definition of locally integrable functions on an abstract measure space.,"Recently I've been able to find a very cheap copy of the nice monograph [1], where I can find (chapter 10, §1, p. 163) the following general definition of Locally integrable function (respect to a measure $\mu$ ) Let $\mathscr C$ be a clan of subsets of $T$ , $E$ a Banach space and $\mu$ a positive measure on $\mathscr C$ . Definition 1 . A function $\boldsymbol f$ defined on $T$ with values in $E$ or in $\overline{\Bbb R}$ is said to be locally integrable with respect to $\mu$ , or locally $\mu$ -integrable, if for every $A\in\mathscr{C}$ the set function ${\boldsymbol f}\varphi_A$ is $\mu$ -integrable. In the above definition A clan $\mathscr C$ ([1], chapter 1, §1, p.1, also called a ring of sets as in [2], chapter I, §4, p. 19) is a family of subsets of $T$ (i.e. $\mathscr C\subset\mathscr P(T)$ ) characterized by the following two properties $A\setminus B\in\mathscr{C}$ for all $A, B\in \mathscr{C}$ . $A\cup B\in\mathscr{C}$ for all $A, B\in \mathscr{C}$ . $\varphi_A$ is the characteristic function of $A\in \mathscr C$ , i.e. the function $$
\varphi_A (x)=
\begin{cases}
1 & x\in A\\
0 & x\notin A
\end{cases} \qquad A\in\mathscr C.
$$ Question . How can I show that, when $T$ is a finite dimensional vector space, or more generally a topological vector space, definition 1 above reduces to the standard one(s) as given for example in the Wikipedia entry "" Locally integrable function ""? What I've tried . Basing on the above definition of a clan/ring of sets, I tried to see if  the properties of these families of sets imply their compactness when defined on a topological space, but I failed. Precisely, I was not able to prove that for any given open covering $\mathscr{B}$ of the sets $A\in\mathscr{C}$ it is possible to select one $\mathscr{B}_f\subseteq\mathscr{B}$ containing finitely many members. Context . Years ago I contributed a lot to the said Wikipedia entry : jointly with other contributors we were able to describe quite accurately the case of locally integrable functions on a finite dimensional measurable space $T$ , i.e. $T=\Bbb R^n$ , $n\ge1$ . Moreover, as stated in the entry itself, we also stated that the extension of the concept when $T$ is a general topological space does not involve any true difficulty, since the meaning of compactness of a set $A\subset T$ is obvious as in the finite dimensional case. Nevertheless I know that the concept of a locally integrable function makes sense also when $T$ is simply an abstract set without any other assumptions: since a description of this extension is also asked by an anonymous commenter in the entry talk page ,  I aim to complete the entry in that sense. References [1] Nicolae Dinculeanu, Vector measures (English), Hochschulbücher für Mathematik. 64, Berlin: VEB Deutscher Verlag der Wissenschaften, pp. X+432 (1966), MR206189 , Zbl 0142.10502 . (Also published by Pergamon Press in 1967 as volume 95 of their ""International Series of Monographs in Pure and Applied Mathematics""). [2] Paul R. Halmos, Measure theory , 2nd printing, (English)
Graduate Texts in Mathematics, 18, New York-Heidelberg-Berlin: Springer-Verlag, pp. XI+304 (1974), MR0033869 , Zbl 0283.28001 .","['integration', 'measurable-sets', 'measure-theory', 'measurable-functions']"
4759817,Are vector fields topologically equivalent when their limitings sets coincide,"Let $M$ be a compact manifold and we are given two VFs $F,F' \in \mathcal{X}(M)$ . We define a DS by $$\dot{x}=F(x)$$ and equivalently for $F'$ . The $\omega$ -limiting set is given by $$\omega(f,x)=\bigcap_{s \in \mathbb{R}} \overline{\{\varphi(x, t): t>s\}}$$ The two vector fields are topologically equivalent , denoted by $F \simeq F' \, $ if there exists a homeomorphism $h: U \rightarrow U$ , mapping orbits of the first system onto
orbits of the second system, i.e. $$\forall t \in \mathbb{R}, \ \forall x \in U: \quad \phi^{F}_t(x)=h^{-1} \circ \phi^{F'}_{\tau} \circ h(x),$$ with $ \tau: U \times \mathbb{R} \rightarrow \mathbb{R}, \quad \frac{\partial \tau(x,t)}{\partial t} >0 \quad \forall x \in U $ . This means the time direction of the orbits is preserved. My question is the following: Given that alle limiting sets are equal, $\forall x \in U: \ \omega(F,x)=\omega(F',x)$ , can we conclude that $F$ and $F'$ are topologically equivalent?","['calculus', 'dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
4759818,Operatornorm of powers of Matricies with integer coefficient,"Let $A\in GL_n(\mathbb{Z})$ have infinite order, so $A^k\neq Id_n$ for all $k>0$ . The operator norm is defined by $\lVert A \rVert=\max\{\lVert Av\rVert \mid v\in\mathbb{R}^n: \lVert v\rVert=1\}$ . It´s easy to see, that $\lVert A\rVert > 1$ .
My question is: Is $\lVert A^k \rVert\geq k$ for all (big enough) $k$ ? If not, is there a constant $C>0$ with $\lVert A^k \rVert\geq Ck$ for all (big enough) $k$ ? It feels like there should be some easy solution, since there are only finitely many matrices with integer coefficients with operator norm at most $k$ (or $Ck$ ), and since $A$ has infinite order, I should eventually outgrow them, but I have no nice formal argument.","['matrices', 'general-linear-group', 'normed-spaces', 'linear-algebra']"
4759856,Why is integration of differential forms defined in this way?,"From what I can say, in most books about differential geometry and differential forms (see for example Flanders, Differential Forms with applications to the Physical Sciences ), the integral of a $p$ -differential form $\omega$ on a chain is defined as follows: one takes a simplex (or a cube) $\Delta$ in $\mathbb R^p$ and maps it into the manifold $M$ on which $\omega$ is defined, with a smooth mapping $\phi:\,\Delta\subseteq\mathbb R^p\to M$ . Then the pullback $\phi^*\omega$ is a $p$ -form on $\mathbb R^p$ which can be uniquely written as $$\phi^*\omega=A(x^1,\,x^2,\,\ldots,\,x^p)\,\mathrm dx^1\wedge\mathrm dx^2\wedge\ldots\wedge\mathrm dx^p. \label{1}\tag{1}$$ The integral is defined as $$
\int_\phi \omega=\int_\Delta A(x^1,\,x^2,\,\ldots,\,x^p)\,\mathrm dx^1\mathrm dx^2\ldots\mathrm dx^p. \label{2}\tag{2}
$$ The obvious “problem” with this definition is that, at least in principle, it is dependent on a choice of coordinates. However, thanks to the change of variable formula, the invariance is ensured. Despite the fact that this definition works fine, it seems to me that the change of variable formula should be a consequence of the wedge product rules. Instead, what happens in the definition above is that the wedge product rules just “match” with the change of variable formula, making the integral well defined. Question : why don't we define the integral in an invariant way, such that the change of variable formula follows from the wedge product rules? For example, one could make a construction similar to the one used for the Riemann integral, breaking $\Delta$ into small rectangles, each corresponding to a $p$ -vector, then letting the pullback $\phi^*\omega$ act on these $p$ -vectors, then summing and taking the limit for finer and finer partitions. I know this is exactly the idea behind integration of forms, but why isn't this idea made into an explicit construction, such that the integral is already invariant from the beginning? Is there a textbook that does so? Or am I missing a step?","['differential-forms', 'differential-geometry']"
4759887,"Find the number of non-empty subsets of $\{1,2,3,\dots,8\}$ which do not contain two consecutive numbers.","I obtained that for the set $A:=\{1,2\ldots,n\}$ , the number of subsets $S$ of size $k$ such that no two elements of $S$ are consecutive numbers is $\binom{n-k+1}{k}$ . Applying $$\sum_{k=1}^{4} \binom{9-k}{k}$$ gives me $54$ , however, the answer given is $34$ . Is there any mistake I have made? Derivation of $\binom{n-k+1}{k}$ : Define $X:=\{-1,0,1,\dots,n\}$ . Note that no. of $k+1$ length subsets $S$ of $X$ with no two consecutive integers such that $-1 \in S$ forms a bijection with the number of $k$ length subsets of $A$ with no 2 consecutive numbers. Write the elements of $X$ in a row and colour $-1$ . $$\color{red}{-1} \ \ 0 \ \ 1 \ \ \ldots\ \ n$$ We want to colour $k$ more numbers such that no two coloured numbers are consecutive. Define $g_i$ as the gap between the $i$ th coloured number and the $i+1$ th coloured number. For example, if we only colour $-1$ and $n$ , $g_1 = n$ . Thus, if we colour $k$ numbers other than $-1$ , we observe that $$\sum_{i=1}^k g_i \le n-k+1 \implies \left(\sum_{i=1}^k g_i\right) + t = n-k+1$$ where $t$ is a non-negative integer. We note that each $g_i$ is greater than 0 (since we cannot colour consecutive integers), so we define $a_i = g_i-1$ , so that $a_i$ are non-negative. $$\left(\sum_{i=1}^k a_i+1\right) + t = n-k+1 \implies \left(\sum_{i=1}^k a_i\right) + t = n-2k+1$$ By stars and bars, the no. of solutions for this is $$\binom{(n-2k+1)+(k+1)-1}{(k+1)-1} = \color{green}{\binom{n-k+1}{k}}$$","['combinations', 'combinatorics']"
4759889,A conjecture involving series with zeta function,"Recently, I tried to evaluate a limit proposed by MSE user Black Emperor. In the process of evaluating the limit, I have obtained the following equality. $$
\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{\left( -1 \right) ^n}{\left( 2n+1 \right) !}\zeta \left( N-n \right)}=\sin\left(1\right)
$$ But I believe this can applied to a broader case of limits. Namely, if we have a function $f$ that is holomorphic around zero with a infinite radius of convergence. The following should hold \begin{align*}
\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\zeta \left( N-n \right)}f^{\left( n \right)}\left( 0 \right) 
=&\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\sum_{k=1}^{\infty}{\frac{k^n}{k^N}}}f^{\left( n \right)}\left( 0 \right) 
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{N-2}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}}
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{\infty}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}}
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{f\left( k \right)}{k^N}}=f\left( 1 \right) 
\end{align*} I have verified this result numerically for a some functions, and it seems that this equality holds for functions like: $\mathbb{sin}\left(x\right)$ , $\mathbb{cos}\left(x\right)$ , $\mathbb{exp}\left(x\right)$ , $\mathbb{sinh}\left(x\right)$ , $\mathbb{cosh}\left(x\right)$ , $\sqrt{x+1}$ , $\mathbb{arctan}\left(x\right)$ , $\mathbb{ln}\left(x\right)$ , $\mathbb{erf}\left(x\right)$ . However, it did no longer work when I plugged $\mathrm{W}\left(x\right)$ in the Lambert W function. I have three question here. Is this correct? As you can see, much of the examples do not have a infinite radius of convergence, yet, the equality still holds. Therefore, What's the ture criteria of this equality? Can we generalize this equality to a bigger family of functions?","['summation', 'real-analysis', 'taylor-expansion', 'power-series', 'limits']"
4759943,Integration of exponential of a function of cosines [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . The community reviewed whether to reopen this question 10 months ago and left it closed: Original close reason(s) were not resolved Improve this question I am trying to solve an integration in the form $$\int_{0}^{2\pi} e^{a \cos{(\theta-b)} + c \cos{(2\theta)}} d\theta$$ where $a$ , $b$ , and $c$ are constants. I know that if $c=0$ , the integration reduces to $2\pi I_0(a)$ . However, I want to solve it for $c\ne 0$ . Any suggestions? even in the form of a series expansion? Thanks","['integration', 'calculus', 'exponential-function', 'special-functions']"
4759971,Does the Axiom of Choice imply the existence of all the choice functions of a set?,"We know that, given a set $X$ , there exists at least one choice function $f:X\rightarrow\cup X$ thanks to the Axiom of Choice (AC). Can we conclude that all choice functions for a generic set $X$ exist? Do we have any information regarding the cardinality of the set of choice functions?","['elementary-set-theory', 'axiom-of-choice', 'logic']"
4759995,Romanian Math Competition (2007): Group Cardinality Inequality $|G| \geq 1+\prod_{i=1}^n\left(p_i^2-1\right)$,"Problem $43$ on PDF-Page $20$ of: https://blngcc.files.wordpress.com/2008/11/rmc2007.pdf Let $G$ be a finite group and $p_1, p_2, \ldots, p_n$ be distinct prime divisors of $|G|$ , such that for each $p_i$ there exists $x_i, y_i$ in $G$ with $$\operatorname{ord}\left(x_i\right)=\operatorname{ord}\left(y_i\right)=p_i$$ and $y_i$ is not a power of $x_i$ .
Prove that $$
|G| \geq 1+\prod_{i=1}^n\left(p_i^2-1\right).
$$ Update: $S_4$ is a counterexample as $$
|S_4|=4!=24<25=1+(2^2-1)(3^2-1)
$$ and all the conditions for $S_4$ are fulfilled. I suggest a new version of the Problem: Let $G$ be a finite group and $p_1, p_2, \ldots, p_n$ be distinct prime divisors of $|G|$ , such that for each $p_i$ there exists $x_i, y_i$ in $G$ with $$\operatorname{ord}\left(x_i\right)=\operatorname{ord}\left(y_i\right)=p_i$$ and $y_i$ is not a power of $x_i$ .
Prove that $$
|G| \geq 1+\sum_{i=1}^n\left(p_i^2-1\right).
$$","['contest-math', 'group-theory', 'inequality']"
4760014,Factoring the $L$-function of the Eisenstein series of a non-primitive character.,"Let $\chi$ be an odd Dirichlet character modulo $N$ . Let $E_{1,\chi}\in M_1(N,\chi)$ be the (normalized) weight 1 Eisenstein series of $\chi$ . If $\chi$ is primitive, this series has $q$ -expansion \begin{equation}\tag{1}
E_{1,\chi}(z)=\frac{ L(\chi,0)}{2} + \sum_{n=1}^\infty \left(\sum_{d|n}\chi(d)\right)q^n,
\end{equation} where $q=e^{2\pi i z}$ . In this case, we can see explicitly that $L$ -function of $E_{1,\chi}$ factors into a product of classical $L$ -functions, namely \begin{align*}
L(E_{1,\chi},s) &= \sum_{n=1}^\infty \left(\sum_{d|n}\chi(d)\right) n^{-s}\\
&= \sum_{a=1}^\infty \sum_{b=1}^\infty \chi(a) (ab)^{-s}\\
&= \left(\sum_{a=1}^\infty \chi(a)a^{-s}\right) \left(\sum_{b=1}^\infty b^{-s}\right)\\
&= L(\chi,s)\zeta(s).
\end{align*} Now, when $\chi$ is not primitive, one can still consider the Eisenstein series $E_{1,\chi}$ , however, the $q$ -expansion is given by the more complicated formula \begin{equation}\tag{2}
E_{1,\chi}(z)=-\frac{NL(\bar{\chi},1)}{2\pi i\,\tau(\bar{\chi}')}+\sum_{n=1}^\infty \left(\sum_{d|n}\sum_{r|(d,h)}\mu\Big(\frac{h}{r}\Big)\bar{\chi}'\Big(\frac{h}{r}\Big)\chi'\Big(\frac{d}{r}\Big) r  \right)q^n,
\end{equation} where $\chi'$ is the primitive character of $\chi$ modulo its conductor $N'$ , and $h=N/N'$ (this formula is taken from Miyake's book titled "" Modular Forms "", Theorem 7.2.13). My question is: What happens to the $L$ -function of $E_{1,\chi}$ when $\chi$ is not primitive? Can we still factor $L(E_{1,\chi},s)$ in this case? In all the literature I have come across, factorizations such as this are handled only for primitive characters. Nonetheless, it seems (to me at least) that it should still be possible to factor for general $\chi$ . Perhaps we would miss a few Euler factors or introduce an explicit error, and that is ok. I have tried to manipulate the summations inside the coefficients in (2) to make it work, but I failed. The $a_n$ in (2) are way more complicated to rearrange than the ones in (1). I may be missing something. Also, if someone knows a reference for these kinds of factorizations when $\chi$ is not primitive, it would be deeply appreciated.","['number-theory', 'modular-forms', 'reference-request']"
4760081,Is the maximum likelihood estimator for the mean equal to the sample mean under all densities?,"Suppose, I have a sample from an unknown distribution. I want to prove/disprove (mathematically!) the following statement: The maximum likelihood (ML) estimator for the (unknown) population mean will always be equal to the sample mean irrespective of the likelihood function. We suppose that, the likelihood function is twice differentiable with respect to its parameters and the mean and variance is finite under the inverse likelihood function (pdf). I believe for symmetric distributions it is straightforward. To maximize the likelihood the center of the mass should be placed at the mean. So, sample mean should be the best estimator to maximize the likelihood of the observed data. But I can not relate to it mathematically.","['log-likelihood', 'probability-distributions', 'maximum-likelihood', 'probability-theory', 'probability']"
4760144,Generating a random prime,"How can I generate a random prime of the form $2^ab+1$ for small $b$ value without actually creating a list of such primes, and then choose from the list at random? For example : I can generate a random n bit prime by choosing a random n bit integer $\textbf{c}$ from $[2^n,2^{n+1}],$ and then pick the next prime (or the previous prime) after $\textbf{c}.$ Does there exist such strategy for primes of the form $2^ab+1$ ? Any idea or suggestion will be greatly appreciated","['prime-factorization', 'number-theory', 'elementary-number-theory', 'primality-test', 'prime-gaps']"
4760152,A question about the criterion of diagonalizable normal operator on a Hilbert space (perhaps non-separable).,"This is the excercise 2.11 in Chapter IX of A Course in Functional Analysis by John B. Conway: Suppose $H$ is a Hilbert space, $\,N$ is a bounded normal operator with associated spectral measure $E.$ Let $\sigma(N)$ be the spectrum of $N$ and $\sigma_p(N)$ be the set of all eigenvalues of $N.$ I need to prove that if $\sigma_p(N)$ is Borel measurable, then $N$ is diagonalizable iff $E(\sigma(N)\backslash\sigma_p(N))=0.$ This is easy when $\sigma_p(N)$ is at most countable since $$E(\sigma_p(N))=E(\cup_{\lambda\in\sigma_p(N)}\{\lambda\})=\Sigma_{\lambda\in\sigma_p(N)}E(\{\lambda\}).$$ But I have no idea to dealt with the case $\sigma_p(N)$ is not countable. Could someone help me?","['hilbert-spaces', 'diagonalization', 'functional-analysis', 'normal-operator']"
4760158,Prove $n \rightarrow 2^+$ as $a \rightarrow 0$,"While studying solids of revolution at college, I came across a problem related to physics that seems to have an answer difficult to prove mathematically, which I have not been able to obtain. Motivation Consider the surface $z=|x|^n+|y|^n$ for $0 \le z \le h$ and $n \ge 1$ . If this surface were a physical object standing in a table, it is very clear that for values of $n \approx 1$ it will be in an unstable equilibrium, while at great values, such as $n=10$ it will be at an stable equilibrium, that is, the object may or may not tumble/fall under any force $F>0$ depending on the value of $n$ . The question is then at which value of $n$ does it transition from unstable to stable? $n=1.5$ and $n=10$ Problem statement From the motivating problem I was able to obtain the equation below. Solving for $n$ as $a \rightarrow 0$ gives the answer, for any constant $c>0$ in the physical problem, $c$ is the z-coordinate of the center of mass
of the object and $a$ is associated with at which point it will
tumble. Taking $a \rightarrow 0$ provides a situation in which it may
fall if moved by any infinitesimal amount, which is an unstable
equilibrium $$\left (c-a^n \right )na^{n-2}=1$$ However, there seems to be no feasible way to isolate $n$ in order to apply the limit $a \rightarrow 0$ . Using Wolfram Alpha, it seems the value of $n$ approaches $2$ by using $a \approx 0$ , but I see no way to prove it.","['limits', 'physics']"
4760214,Spliting trapezoid to traingles with constant angle,"I have this right-angled trapezoid that I want to split in $n$ parts in such way that each diagonal will be at the same angle to the bottom line. I'm starting with: first and last height - $h_0$ , $h_n$ angle between top and bottom line - $\alpha$ length of bottom line - $X$ I think I'm able to calculate it for $n = 2$ just from ratio (below) but anything more is above me. $$\frac{h_0}{|P_0 P_1|} = \frac{h_2}{|P_1 P_2|}$$ Real world example is tapered truss and its webs. I really appreciate any help, even stating that it is impossible to calculate. I haven't thought about trigonometry that much in the last 10 years and I'm sick of it. EDIT #1: Thank you for answering! I should have guessed that my description will put you in solving this specific case when I'm searching for more broad solution. Nevertheless, your answers guided me on the right track, and I have one piece of the puzzle. Going with @CiaPan idea I found the ratio: $\left(\frac{h_n}{h_0}\right)^\frac{2}{n}$ that helped me in calculating distances between $P$ points. But expanding my question to this trapezoid as well to the cases with odd $n$ parts seems making this approach invalid as I don't know anything about similar triangles ( $P_0, y_1, P_2$ and $P_2, y_3, P_n$ ).
My only idea for this case is to find ratio ( $R$ ) between bases of each triangle: $$\frac{|P_0 P_2|}{|P_2 P_4|}=\frac{|P_2 P_4|}{|P_4 P_6|}=...=\frac{|P_{n-4} P_{n-2}|}{|P_{n-2} P_n|}$$ I'm almost certain that it won't help with odd $n$ cases but this would allow me to calculate everything the same way as in the first case. The only thing I know is that sum of this geometrical sequence would equal to the length of bottom line $X = |P_0 P_2| \cdot \frac{1-R^n}{1-R}$ . Thank you once again I look forward to solving this with you!","['trigonometry', 'geometry']"
4760225,Does the Diophantine equation $z_1^5 +z_2^5+z_3^5+z_4^5+z_5^5=\beta^5$ have a solution for every integer $\beta$?,"(Note: The exponent $k=3$ has been answered in the affirmative in this post .) I. Data For simplicity, assume all terms $\in \mathbb{Z},$ so we can transform the equation to the more symmetric, $$x_1^5+x_2^5+x_3^5 = y_1^5+y_2^5+y_3^5$$ where $(x_1, x_2, x_3) = (y_1, y_2, y_3)$ are considered trivial solutions. In 2009, by an exhaustive search, Duncan Moore found roughly 5400 primitive solutions within a search radius of about $17700.$ For example, \begin{align}
1^5 & + 89^5 + 118^5  = 123^5 + 47^5 + 38^5\\ 
2^5 & + 97^5 + 258^5   = 257^5 + 125^5 + 35^5\\ 
3^5 & + 54^5 + 62^5  = 67^5 + 28^5 + 24^5\\
4^5 & + 32^5 + 498^5   = 463^5 + 369^5 + 302^5\\ 
5^5 & + 145^5 + 224^5    = 214^5 + 157^5 + 153^5 \\
6^5 & + 265^5+ 614^5 = 543^5+ 527^5+ 235^5\\ 
7^5 & + 201^5+ 303^5  = 307^5+ 173^5+ 31^5\\ 
8^5 & + 62^5+ 68^5 \,=\, 74^5+ 43^5+ 21^5\\  
9^5 & + 206^5+ 430^5 = 418^5+ 297^5+ 20^5\\ 
10^5 & + 100^5+ 972^5  = 951^5+ 617^5+ 204^5\\
\vdots\\[4pt]
200^5 & +  334^5 + 676^5  = 679^5 +  256 ^5 +  185^5   
\end{align} up to $x_1 = 200$ which is quite a long stretch. But there were three missing: namely $x_1 = (22,\,88,\,176)$ ,  all of which are multiples of $11$ and a power of $2$ . Update : The list for $0\leq x_1\leq 1000$ is now complete , with the last two, namely ( $410, 840$ ), found by Oleg567. See his answer below. II. Updates As Adam Bailey pointed out, there doesn't seem to be an obvious congruence obstruction for $x_1 = 22$ and others. So it may be possible for all $x_1$ just like its cousin $z_1^3+z_2^3 = z_4^3+z_4^3$ (though by FLT, this has no $z_1 = 0$ ). There is $0^5  + 220^5 + 14132^5 = 14068^5 + 6237^5 + 5027^5$ , so $x_1=0$ is now possible for $5$ th powers. Oleg567 found $x_1=176$ valid for $k=1,5$ using a larger search radius, $$176^5+20117^5+22952^5=5781^5+12692^5+24772^5$$ $$176+20117+ 22952=5781+12692+24772$$ Moore found $x_1=22$ also valid for $k=1,5.$ (See his answer below.) Using the form ( $5,2,4$ ), wxffles found $x_1=88$ and $x_1=858$ valid only for $k=5.$ (See addendum to Moore's answer for $x_1<1000$ .) James Waldby's database for ( $5,1,5$ ) can be found here for other missing $x_1$ . III. Question Can we in fact find a primitive solution $\in \mathbb{Z}$ for any integer $x_1$ , $$x_1^5+x_2^5+x_3^5 = y_1^5+y_2^5+y_3^5$$ hence the absence of $x_1 = 22, 88, 176,$ etc. is simply an artifact of the search radius?","['number-theory', 'modular-arithmetic', 'diophantine-equations']"
4760231,Electric Field Due to uniformly charged infinitely long wire,"Calculate the field at a point P at distance r from infinitely long wire with charge density $\lambda$ Generally the electric field in this case at point P, distance r from the wire is derived using : $$ \vec E . 2 \pi r l = \frac{\lambda l}{\epsilon _○}$$ I tried to do the same but using Coulombs law As force between two point charges is given by:(field is force per unit charge) $$\vec F = \frac{1}{4 \pi \epsilon_○}×\frac{Q_1 Q_2}{r^2}(\hat r)= \frac{kQ_1Q_2}{r^2}(\hat r)$$ And Vertical components get cancelled out I defined the following integral: $$\int^\infty _{-\infty} \frac{k}{r^2+x^2}\cos \theta \ dQ$$ $$=\int^\infty _{-\infty} \frac{k}{r^2+x^2}×\frac{r}{\sqrt{r^2+x^2}} \ \lambda \ dx$$ It finally evaluates to $$\frac{k \lambda}{r} × \left[\frac {x}{\sqrt{x^2+r^2}}\right]^{\infty}_{-\infty}$$ Is my process correct? How do I further evaluate to obtain the expression by putting the limits? I proceeded without knowing what to do and got $$\frac{k\lambda}{r}×\left[ \frac{\infty}{\infty}- \frac{- \infty}{\infty} \right]$$ $$ = \frac{k \lambda}{r}× \frac{2×\infty}{\infty}=\frac{2k\lambda}{r}$$ The doubt is:  Since in this case the answer comes out correct. Does it it mean we can treat indeterminate forms by cancelling infinity with infinity as equal to 1/1 like in this case? I know it is wrong mathematically but does it generally give the correct answer treated this way? (in similar cases maybe)","['integration', 'limits', 'definite-integrals', 'indeterminate-forms']"
4760236,For which probability distributions does $\lim_{n\to\infty} \mathbb{E}\left[\left(\sum_1^n X_i\right)^2 / \left(\sum_1^n {X_i}^2\right)\right]$ exist?,"Say, we have $n$ positive i.i.d. random variables, $X_1, X_2, \dots, X_n$ which are distributed according to some probability distribution, $f$ . So, only distributions on either $[0, 1]$ or $[0, +\infty]$ . The expected value of the ratio between the square of the sum and the sum of squares is then a function of $n$ and the distribution $f$ , $$ G(n; f) = \mathbb{E}\left[\frac{\left(\sum_{i=1}^{n} X_i\right)^2}{\sum_{i=1}^{n} X_i^2 } \right] $$ In general, I am interested in the behavior of this function, especially in the large- $n$ limit, or as $n \to \infty$ . Specifically, for which distributions is $G(n; f)$ asymptotically constant? It's easy to see that $G(n)$ is invariant to scaling such as $X_i \to a X_i$ . So, there is no loss of generality when limiting ourselves to either distributions on $[0, 1]$ or $[0, +\infty]$ . However, I suspect that it is not possible to get constant $G$ at large- $n$ for distributions on bounded intervals, anyway. Here's how far I've gotten myself: Firstly, we know that $1 \le G(n) \le n$ by the Cauchy-Schwarz inequality. $G(n) = n$ is only true for the Dirac delta distribution ( $X_i = 1$ ). It seems for some common distributions I tried out numerically, $G(n; f) \propto n$ in the large- $n$ limit. For example, with the uniform distribution, as $n \to \infty$ $$ G(n; U(0, 1)) \sim \frac{3}{4} n$$ I used some very non-rigorous manipulation (""physicist math""), assuming that the expected value of the ratio is the ratio of expected values in the large- $n$ limit, to derive $$ G(n; f) \sim \frac{\mu^2}{\mu^2 + \sigma^2} n$$ where $f$ is a distribution with mean $\mu$ and variance $\sigma^2$ . Numerically, this seems to agree with the uniform and log-normal distributions. My guess is that it applies to any distribution with finite mean and variance. Numerically, I also found that for power-law distributions with heavy tails (such as Pareto distribution with $\alpha < 1$ ), $G(n)$ does appear to converge to a constant. But I can't seem to show this mathematically or find an expression for $G(n)$ in terms of $\alpha$ . Edit: An interesting, perhaps useful result from Albrecher & Teugels (2007) , is that if $f$ is a Pareto-type distribution with $0 < \alpha < 1$ , then $$\lim_{n\to\infty} \mathbb{E}\left[\frac{\sum_{i=1}^{n} X_i^2 }{\left(\sum_{i=1}^{n} X_i\right)^2}\right] = {1 - \alpha} $$","['statistics', 'probability-distributions', 'probability', 'random-variables']"
4760275,What happens to $y=\ln x$ when it is translated and then dilated?,"I am struggling to understand my teacher's answer for the following question. The curve $y=\ln x$ is translated to the left by $\pi$ units and then dilated horizontally by a scale factor of 3. What is the equation that describes the new curve? I thought it was $y=\ln(x/3 +\pi)$ since the translation is applied first, which isn't typical order. But my teacher has the answer as $y=\ln\frac{x+\pi}3$ , which would make sense to me if the order was dilation then translation, but I'm struggling to understand how that comes from translation then dilation. Is my teacher correct, and if so, can someone explain?",['functions']
4760279,"Are there ""differentiable manifolds"" that don't admit a $C^1$-structure","It is well known that every $C^1$ manifold admits a smooth manifold structure. What if we relax the definition of smooth manifold so the transition maps need only be differentiable? Does every such ""differentiable manifold"" admit a compatible $C^1$ atlas?","['manifolds', 'derivatives', 'smooth-manifolds']"
4760309,How are k-Hausdorff and weakly Hausdorff distinct?,"In this pull request to the pi-Base database , we encountered this situation. A space $X$ is said to be weakly Hausdorff provided for every compact Hausdorff space $K$ and every continuous $f:K\to X$ , $f[K]$ is closed in $X$ . A space is $X$ said to be k-Hausdorff provided its diagonal $\Delta=\{(x,x):x\in x\}\subseteq X\times X$ is ""k-closed"". In Quotients of k-semigroups a set is said to be k-closed if its complement is k-open, that is, its intersection with every compact set is open in the subspace. Call this $k_1$ -closed , and its corresponding version of k-Hausdorff $k_1H$ . There it is proven that every $k_1H$ space has the property that compact sets are closed, which in turn implies the space is weakly Hausdorff. On the other hand, in Compactly Generated Spaces , a set is said to be k-closed if given any continuous map of a compact Hausdorff $K$ into the space, the set's inverse image is closed in $K$ . Call this $k_2$ -closed , and its corresponding version of k-Hausdorff $k_2H$ . There it's shown that every weakly Hausdorff space is $k_2H$ . However, it seems weakly Hausdorff and k-Hausdorff aren't actually equivalent. So what gives? (Hints: of course we have two different definitions of k-closed here. Additionally, some care should be given to checking when ""compact"" means ""compact and Hausdorff"".)","['separation-axioms', 'general-topology', 'compactness']"
4760319,Verifying a combinatorics identity with rth derivative of this function,"Problem 14 of the chapter IV of the Feller's book ""An introduction to probability"" reads as follows: From the result of problem 12 conclude that $$\sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_r = 0\qquad\qquad(1)$$ if r<n and for r=n $$\sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_n ={s^n}{n!}. \qquad\qquad(2)$$ Verify this by evaluating the r th derivative, at x=0, of $$\frac{1}{{(1-x)}^{ns-r+1}}{[1-{(1-x)}^s]}^n\qquad\qquad(3)$$ The problem 12 that was mentioned in the problem description, is a particular version of coupon collector's problem: A pack of cards consisting of s identical series, each containing n cards numbered 1,2,...,n . A random sample of $r≥n$ cards is drawn from the pack without replacement. Calculate the probability $u_r$ that each number is represented in the sample. The answer to this latter problem is $$u_r=\sum_{k=0}^n (-1)^k{n \choose k}\frac{{(ns-ks)}_r}{(ns)_r}\qquad\qquad(4)  $$ which is not so hard to arrive at. Also with this answer we can show that (1) and (2) hold. But my problem is with the verification with the r th derivative of (3). With direct differentiation, I calculated the below as the r th derivative at x=0: $$\sum_{k=0}^r {r \choose k}{\frac{(ns-k)!}{(ns-r)!}}\sum_{\sum_{}{i_j}=k}{(-1)}^{k+n}{k \choose {}i_1,i_2,...,i_n} [\prod_{j=1}^n \frac{s!}{({s-{i_j}})!}]A(I) \qquad\qquad(5) $$ where A(I) is a function of the n-tuple $I=(i_1,i_2,....,i_n)$ and is equal to zero if at least one of the elements of I is zero, otherwise A(I) is one (I used multinomial theorem ) Now it is easy to verify that (5) become $s^n{n!}$ if r=n and zero for r<n , but still I can't see how the cumbersome summation (5) can be simplified to be something as straightforward as (1). I expected to be able to reproduce the summation (1) by differentiating (3); so how one can arrive at (1) by (5) or by differentiating (3)? Thank you all.","['derivatives', 'combinatorics']"
4760351,Does every sequence of group epimorphisms (between finitely generated groups) contain a stable subsequence?,"I have a question that is related to the topic of limit groups: Let $G$ and $H$ be finitely generated groups and let $(\varphi_n: G \to H)_{n \in \mathbb{N}}$ be a sequence of group epimorphisms. Does there exist a stable subsequence of $(\varphi_n)_{n \in \mathbb{N}}$ ? This is the definition of ""stable sequence"": Let G be a group and $(\varphi_n) _{n \in \mathbb{N}}⊂ Hom(G, H)$ . The sequence $(\varphi_n) _{n \in \mathbb{N}}$ is stable if
for any $g ∈ G$ either $\varphi_n(g) = 1$ for almost all $n$ or $\varphi_n(g)\neq 1$ for almost all $n$ . I conjecture that the answer to the question is positive, because the statement of the question is used on page 27 of the following article (in the article we have a sequence of epimorphisms $g_n: F_l \to L$ to which the statement of my question is applied to): https://arxiv.org/pdf/2002.10278.pdf Edit: In the above article we have the situation that G is a free group of rank at least two and H is a non-abelian limit group. So it would be sufficient if somebody can answer my question for this situation. Of course you can still say something to the general situation. Edit: ""Almost all"" means that the statement ist true for all natural numbers with possibly a finite number of exceptions.","['geometric-group-theory', 'group-theory', 'abstract-algebra', 'geometry']"
4760354,"Enumerating the ""discrete straight lines"" in an $n \times n$ grid.","This question is related to this other question . I would like to write an algorithm to enumerate all subsets of the $n^2$ squares of an $n \times n$ grid such that, for each subset, there exists a straight line crossing the inner region of all and only the squares of that subset. Inner region means that the line cannot cross just the border or the corner of the square. Here you can find an interactive implementation of these ""discrete straight lines"". The obvious algorithm that comes to mind is to take any two point on the outer border of the grid, draw a line passing through the two points and calculate all crossed squares. The problem is an actual algorithm can try only a finite number of couple of points. Therefore a side question could be how many points in the outer border must be considered, e.g. $2n$ for each side? And with what disposition? Any other idea?","['discrete-geometry', 'computational-geometry', 'geometry', 'combinatorics', 'discrete-mathematics']"
4760367,Proof that a first integral is not a constant function,"Let $M^n$ be a (compact) $n$ susbet of $\mathbb{R}^n$ . And we are given a set of $m$ basis functions $$B=\{\psi_i(x): M^n \rightarrow \mathbb{R}| i=1,...,m \}$$ such that all of them are differentiable and non-constant. We define a DS by $$\dot{x}(t)=F(x(t))$$ and set $\Phi: M \times \mathbb{R} \rightarrow M, \quad (x_0, t) \mapsto \Phi(x_0,t)$ as the flow of the DS. Now, we assume to be given  an open subset $U \subset M$ such that $$ \forall x_0 \in U \ \exists \theta(x_0) \in \mathbb{R}^m, \ \theta^{x_0}   \neq 0 : \quad \Lambda(x(t))=\sum_{i=1}^m \theta_i(x_0) \psi_i(\Phi(x_0,t))=-\theta_0(x_0) \quad \forall t
 $$ I want to show that $\Lambda$ is a (local) first integral. I know that the Lie derivative $L_F(\Lambda)=0$ is zero, which is the first thing I need to prove. But I am struggling to prove that the function is not locally constant. My question is the following: Can we prove that $\Lambda $ is not locally constant? More formally, sis it true that $$
\exists x_0^1,x_0^2 \in U: \quad \Lambda(x^1(t))=-\theta_0(x_0^1) \neq -\theta_0(x_0^2)=\Lambda(x^2(t))\;?
$$ I think this has to be true, since for two different inital conditions $\Phi(x_0^1,t) \neq \Phi(x_0^2,t) \ \forall t$ . And since all functions are non-constant the parameters can't be the same but I am lacking a formal proof. Especially how I can conclude from the fact that the parameters are not the same that the value of the function is not the same. (If it helps , the vector field $F$ itself is a linear combination of the basis functions $\psi_i$ in each dimension.)","['calculus', 'dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
4760383,Local basis criterion for topological group / vector space?,"Let $X$ be a topological vector space (TVS), and let $\mathcal B$ be nonempty family of subsets of $X$ that each contain $0$ . Are there simple conditions that guarantee $\mathcal B$ is a local basis for a topology that turns $X$ into a TVS? How does the answer change if we relax $X$ to being an abelian group or just a group? My first idea is as follows. If $\mathcal B$ is the local basis for a topology $\mathcal T$ that makes $X$ a TVS, then the set of all translates of elements in $\mathcal B$ must be an ordinary basis for $\mathcal T$ . These sets must satisfy the ""ordinary basis criterion,"" which here is equivalent to a local basis criterion: For any $V_1, V_2 \in \mathcal B$ and $x_1, x_2 \in X$ such that $0 \in (V_1 + x_1) \cap (V_2 + x_2),$ there is some $V \in \mathcal B$ such that $V \subseteq (V_1 + x_1) \cap (V_2 + x_2)$ . This guarantees that the translates of $\mathcal B$ are the basis for a topology on $X$ . However, this doesn't seem to be enough to show that $\mathcal T$ turns $X$ into a TVS. But it seems there are many natural candidate properties to assume for $\mathcal B$ that might help the situation, e.g., every $V \in \mathcal B$ is absorbing, star-shaped, balanced, etc.","['general-topology', 'topological-vector-spaces', 'topological-groups']"
4760392,Proving associativity of matrix multiplication,"I'm trying to prove that matrix multiplication is associative, but seem to be making mistakes in each of my past write-ups, so hopefully someone can check over my work. Theorem. Let $A$ be $\alpha \times \beta$, $B$ be $\beta \times \gamma$, and $C$ be $\gamma \times \delta$. Prove that $(AB)C = A(BC)$. Proof. Define general entries of the matrices $A$, $B$, and $C$ by $a_{g,h}$, $b_{i,j}$, and $c_{k,m}$, respectively. Then, for the LHS:
\begin{align*}
& (AB)_{\alpha, \gamma} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,\gamma} \\
& \left((AB)C\right)_{\alpha, \delta} = \sum\limits_{n=1}^{\gamma} \left(AB\right)_{\alpha, n} c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \left(\sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,n} \right) c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \sum\limits_{p=1}^{\beta} \left(a_{\alpha,p} b_{p,n}\right) c_{n, \delta}.
\end{align*}
For the RHS: 
\begin{align*}
& \left(BC\right)_{\beta, \delta} = \sum\limits_{n=1}^{\gamma} b_{\beta, n} c_{n, \delta} \\
& \left(A\left(BC\right)\right)_{\alpha,\delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} (BC)_{p, \delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} \left(\sum\limits_{n=1}^{\gamma} b_{p, n} c_{n, \delta} \right) = \sum\limits_{p=1}^{\beta} \sum\limits_{n=1}^{\gamma} a_{\alpha,p} \left(b_{p, n} c_{n, \delta} \right).
\end{align*}
Assuming I have written these correctly, we can make two observations: first, the summands are equivalent, as multiplication is associative. Second, the order of the summations doesn't matter when we're summing a finite number of entries. Thus, $(AB)C = A(BC)$. How does this look?","['proof-verification', 'linear-algebra']"
4760396,Proof for Inclusion-exclusion doesn't make sense,"Im currently trying to grasp my professors proof for Inclusion-exclusion, and there is one part of his proof that just doesn't make sense to me and I was hoping for some help here. Theorem: Let $A_1,A_2, ... , A_n$ be finite sets. Then $\displaystyle |\cup_{i=1}^n A_i| = \sum_{i=1}^{k} |A_i|- \sum_{i \neq j}|A_i \cap A_j|+ \sum_{i \neq j \neq k}|A_i \cap A_j \cap A_k|-...+(-1)^{n-1}|A_1 \cap ... \cap A_n|$ Proof: Lemma needed: Let $n \geq 1$ , then $\sum_{k=0}^n (-1)^k {n \choose k}= 0$ Proof Lemma: Apply binomial theorem: $(x+y)^n= \sum_{k=0}^n {n \choose k} x^k y^{n-k}$ with $x=-1, y=1$ Proof of the theorem: Let $x$ be any element of the union.
It suffices to show that $x$ is counted exactly once in the sum. Suppose $x$ belong to $m$ of the $n$ sets $A_i$ , for some $1 \leq m \leq n$ .
Due to symmetry of formula we can assumed WLOG that $x$ belong to $A_1,...,A_m$ . Part where I don't understand below Then $x$ is counted $(-1)^{k-1}$ times for each $k$ -element  subset of ${1,2,...,m}$ , $1 \leq k \leq m$ . Hence total number of times $x$ is counted is $$\sum_{k=1}^m (-1)^{k-1} {m \choose k} = - \Big( \sum_{k=0}^m (-1)^k {m \choose k} -(-1)^0 {m \choose k} = -(0-1)=1$$ What does he mean that $x$ is counted $(-1)^{k-1}$ times for each k-element subset? I really don't understand where he get that from.",['discrete-mathematics']
4760407,How to Rewrite an Infinite Sum for Easier Differentiability?,"I have come across the following simplification $$
g(x)=\sum_{k\in\mathbb{Z}} \frac{e^{-xk^2 }}{2k+1}=\sum_{k\in\mathbb{Z}}\frac{e^{-xk^2 }}{1-4k^2}
$$ To show this, we can compute its difference $$
\sum_{k\in\mathbb{Z}} \frac{e^{-xk^2 }}{2k+1}-\sum_{k\in\mathbb{Z}}\frac{e^{-xk^2 }}{1-4k^2}=\sum_{k\in\mathbb{Z}} \frac{2ke^{-xk^2}}{4k^2-1} =0
$$ since opposite signed terms cancel out. However, my question is, provided we do not know the right-hand side a priori , how would one go about rewriting the expression on the left to get the expression on the right? The particular motivation behind this rewriting is that, in that form, it's relatively easy to show that $g$ satisfies $$
g(x)+4g'(x)=\sum_{k\in\mathbb{Z}}e^{-xk^2 }.
$$ Any ideas?","['algebra-precalculus', 'exponential-function', 'sequences-and-series']"
4760419,"Find Var(Y) given Y is uniformly distributed in (X,1) and X is uniformly distributed in (0,1)","I approached this problem by using the formula Var(Y) = $E(Y^2)-E(Y)^{2}$ . E(Y) = $\frac{3}{4}$ However, my difficulty was finding $E(Y^2)$ . Can someone explain why this integral setup is wrong? $E(Y^2) =  \int_{0}^{1} \int_{x}^{1} y^2  dy dx $","['expected-value', 'statistics', 'variance', 'probability']"
4760431,Pointwise a.e. approximation by a sequence of smooth functions with supremum bound,"There are many well known results on approximations by regular functions. Here are some of them. If $f : \mathbb R \to \mathbb R$ is (Lebesgue) measurable, then there exists a sequence of continuous functions $f_n : \mathbb R \to \mathbb R$ such that $f_n \to f$ pointwise a.e. (see here for a proof). If $f \in C(\mathbb R)$ is a continuous function then there exists a sequence of functions $f_n \in C^{\infty}(\mathbb R)$ such that $f_n \to f$ uniformly on every compact subset of $\mathbb R$ . Furthermore, if $f \in C^K(\mathbb R)$ then the convergence can be ensured to uniformly hold on every compact set for all derivatives up to order $K$ . The set $C_c^{\infty}(\mathbb R^d)$ is dense in each of the spaces $L^p(\mathbb R^d), 1 \leq p < \infty$ in their norms. I am aware that pointwise a.e. convergence is a very weak form of convergence to ask for. A tool like Lusin's theorem with an approximation argument is enough to guarantee a convergence result. Keeping this in mind, I'm thinking that a slightly stronger version of this convergence, which I require, should also be true. The key difference with this problem is that I require a uniform bound on the approximating functions in question. Suppose that $g : \mathbb R \to \mathbb R$ is a Borel measurable function such that $|g|$ is essentially bounded above by $1$ . Then, does there exist a sequence of functions $\{f_n\}_{n \geq 1}$ , such that each $f_n$ is twice differentiable with bounded and continuous derivatives, $|f_n| \leq 1$ on $\mathbb R^d$ for all $n$ , and $f_n \to g$ pointwise a.e.? Now, without the condition $|f_n| \leq 1$ on $\mathbb R^d$ , there is no problem at all. We may consider a sequence of continuous functions $h_n$ which approach $g$ pointwise a.e., and for each $h_n$ we can consider a sequence of smooth functions $h_{ni} \to h_n$ pointwise a.e. We'll be done by a diagonal argument. However, it's not possible to place a restriction on $|h_{ni}|$ , it seems, in any particular way. That insight is what I'm looking for here.","['measure-theory', 'lebesgue-measure', 'smooth-functions', 'functional-analysis']"
4760459,Please suggest books (especially from Dover) that are good alternatives to Folland's and Royden's books for measure-theoretical real analysis.,"Can you refer me to books that are at the same advanced level of real analysis via measure theory as Folland's and Royden's books? I want to have a solid foundation and I've been referred to books like Cohn's and Axler's, but I don't have deep knowledge of real analysis (I'm still going to start studying introduction to analysis) and I don't know if these books combine measure theory and real analysis like the ones I mentioned in the title or whether they are purely measure theory. If possible it was a Dover book because they have affordable prices here in Brazil. Thanks to everyone who can help me.","['measure-theory', 'book-recommendation', 'real-analysis']"
4760461,Dimension of the manifold of symmetric rank $r$ $n\times n$ matrices,"I'm currently reading through the paper ""Low-rank matrix completion by Riemannian
optimization—extended version"" by Vandereycken, and in this paper the author states that the set $\mathcal{M}_k = \{X\in\mathbb{R}^{n\times n}\vert \text{rank}(X) = k\}$ is a smooth manifold of dimension $k(2n-k)$ . For the proof, the author cites Example 5.30 on page 117 of ""Introduction to Smooth Manifolds"" by Lee, which uses a fairly standard submersion proof to show that $\mathcal{M}_k$ is a submanifold with the dimension given above. My question is twofold: First, is the set $\mathcal{M}_k^{sym} = \{X\in\mathbb{R}^{n\times n}\vert X = X^T, \text{rank}(X) = k\}$ a manifold as well? If one assumes positive semi-definiteness, the answer to this post ( Is the set of symmetric positive semi-definite matrices a smooth manifold with boundary ) indicates that this is a manifold. I'm wondering if the relaxation away from positive semi-definite breaks this. Second, if this is in fact a manifold (which I think it is), what is the embedded dimension of said manifold? The proof in Lee shows that the codimension of $\mathcal{M}_k$ is $(n-k)^2$ , and I thought the proof should generalize to symmetric matrices, but the codimension would have to change as the dimension of symmetric $n\times n$ matrices is $\frac{n(n+1)}{2}$ , which is for small $k$ is smaller than $(n-k)^2$ . I'd appreciate any input on this question/pointers in the right direction.","['matrix-analysis', 'differential-topology', 'non-convex-optimization', 'differential-geometry']"
4760465,Integral $\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$,"For $a, b, c \in \text{N}$ : $$I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$$ Some Results using numerical Evaluation: $$I(4k-2,1,1)=(2k-1)\pi$$ $$I(a,2,1)=\left(\frac{3a-1}{2}\right)\pi$$ Keeping 2 variables constant and changing one, we can see the pattern for many cases. And confirming for many cases, it comes out to be a multiple of $\pi$ . One observation is that answer will be symmetric in terms of $a, b, c$ . Maybe there is an Anti-Derivative too, but I can't seem to find it. Some Examples: $$\int_{0}^{\pi}\cot\left(10t\right)\cot\left(11t\right)\cot\left(12t\right)\left(\sin\left(1320t\right)\right)^{3}dt=\frac{1979}{2}\pi$$ EDIT: Here is a table I made too, Keeping $c=1$ : $$
\begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}
 a/b                                       & 1  &2&3&4&5&6&7&8&9&10 \\\hline
 1                                       &0&1&0&2&0&3&0&4&0&5 \\\hline
 2                                       &1&2.5&4&5.5&7&8.5&10&11.5&13&14.5\\\hline
 3                                       &0&4&0&9&0&13&0&17&0&22\\\hline
 4 &2&5.5&9&11.5&14&17.5&21&23.5&26&29.5\\\hline
 5&0&7&0&14&0&23&0&30&0&37\\\hline
 6&3&8.5&13&17.5&23&26.5&30&35.5&40&44.5\\\hline
 7&0&10&0&21&0&30&0&43&0&52\\\hline
 8&4&11.5&17&23.5&30&35.5&43&47.5&52&59.5\\\hline
 9&0&13&0&26&0&40&0&52&0&69\\\hline
 10&5&14.5&22&29.5&37&44.5&52&59.5&69&74.5\\\hline
\end{array}
$$ Now obviously the table will be symmetric about the diagonal but I made it fully anyways. Maybe someone can conjecture a form using these. EDIT 2: Using more data, maybe: For $a, b, c$ : all odd Natural Numbers. $$\boxed{I(a,b,c)=0}$$ Proof: $$I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$$ $t\to\pi-t$ $$I(a,b,c)=\int_{0}^{\pi}\cot\left(a\left(\pi-t\right)\right)\cot\left(b\left(\pi-t\right)\right)\cot\left(c\left(\pi-t\right)\right)\left(\sin\left(abc\left(\pi-t\right)\right)\right)^{3}dt$$ For $a, b, c$ odd we have $abc$ odd too. Therefore, $$I(a, b, c)=-I(a, b, c)$$ $$I(a,b,c)=0$$","['integration', 'calculus', 'definite-integrals']"
4760472,Complex analysis exercise: prove that $f$ is constant given an inequality [duplicate],"This question already has an answer here : How can i prove that this function is bounded in order to apply Liouville's theorem? (1 answer) Closed 10 months ago . I am studying complex analysis and I came upon this exercise: If $f$ is a holomorphic function in $\mathbb{C}$ and for every $z\in\mathbb{C}$ we have $|f(z)|\leq\sqrt{|z|+2}$ then show that $f$ is a constant function. Any ideas for solving this problem? I tried with Liouville's theorem, but I think that it isn't the right way to solve this.","['complex-analysis', 'analysis']"
4760485,Toric vector bundle - Klyachko's classification,"I am trying to understand Klyachko's classification of toric vector bundles on a toric variety ( his article: Equivariant vector bundles on toric varieties and some problems of linear algebra). I am reading Sam Payne's version of it as presented in the preliminaries of his article Moduli of Toric Vector Bundles. I am having some troubles understanding two things. The setting: Let $N$ be a lattice, $M = \text{Hom}(N,\mathbb{Z})$ the dual lattice, $\Delta$ a fan in $N \otimes_\mathbb{Z}\mathbb{R}$ , $ X = X(\Delta)$ the corresponding toric variety and $\mathscr{E}$ a toric vector bundle on $X$ . An algebraic action of the torus $T$ on $\Gamma(X,\mathscr{E})$ , the vector space of global sections, is given by $$ (t\cdot s)(x) = t(s(t^{-1}x))$$ for $t\in T, s\in\Gamma(X,\mathscr{E})$ and $x\in X$ .
So far so good. This action induces the following decomposition into T-eigenspaces $$ \Gamma(X,\mathscr{E}) = \oplus_{u\in M}\Gamma(X,\mathscr{E})_u, $$ where $$ t\cdot s = \chi^{u}(t)s$$ for $t\in T$ and $s\in \Gamma(X,\mathscr{E})_u$ . Question 1:
Why does this decomposition exist? I have read (Cox, Toric Varieties Porp 1.1.2) that, for an algebraic action, such a decomposition exists if the action is linear and the vector space is of a finite dimension. I see that the above action is linear but cannot see why the vector space $\Gamma(X,\mathscr{E})$ is finite dimensional. Question 2: I know that for $u\in M$ the character $\chi^{u}$ is a regular function on $X$ . I don't understand why in the above setting, the eigenfuction $\chi^{u}$ is an element of $\Gamma(X, O_X)_{-u}$ Additional questions (after the above were answered thanks to sti9111)
Continuing my pursue of understanding Klyachko's theorem, I am trying to understand the following point mentioned both in Payne (p.7) and in Gonzalez (p. 7).
Continuing with Payne's notation let $E$ be the fiber at the identity element of the Torus, $x_0$ . Consider the map $\Gamma(X,\mathscr{E})_u\to E$ given by the evaluation at $x_0$ . For $\sigma\in\Delta$ , denote with $E^{\sigma}_{u}$ the image of $\Gamma(U_\sigma,\mathscr{E})_u$ under this evaluation map. For a ray $\rho\in\Delta$ , we denote with $E^{\rho}(i)=E^{\rho}_{u}$ for any $u$ such that $<u,v_\rho>=i$ . My question: both in Payne's proof of Klyachko's classification theorem (p.7) and in Gonzalez's article (p.7), the fllowing is mentioned $E^{\sigma}_{u}=\cap_{\substack{\rho\preceq\sigma}}E^{\rho}(<u,v_\rho>)$ and in Gonzalez, the image of $H^{0}(X,\mathscr{E})_u$ under the evaluation map is equal to $\mathscr{E}^{\rho1}(<u,v_1>)\cap\dots\cap\mathscr{E}^{\rho_{d}}(<u,v_d>)$ .
I cannot see why this holds.","['algebraic-vector-bundles', 'algebraic-groups', 'algebraic-geometry', 'toric-varieties', 'group-actions']"
4760492,Integer solutions of $2\cos\left(\frac{p\pi}n\right)+2\cos\left(\frac{q\pi}n\right)+4\cos\left(\frac{p\pi}n\right)\cos\left(\frac{q\pi}n\right)=1$,"I came across the following problem while trying to solve an eigenvalue problem. I want to know the integer solutions of the following equation $$ \cos\left(\frac{p\pi}n\right) + \cos\left(\frac{q\pi}n\right) + 2 \cos\left(\frac{p\pi}n\right) \cos\left(\frac{q\pi}n\right) = \frac{1}{2}$$ where $p, q \in \{1, \dots, n-1\}$ . I have already observed that if $n = 6k$ for some $k \in \mathbb{N}$ , then there is an immediate solution $p = 2k, q = 3k$ . From numerical considerations, it seems that there are no solutions when $6 \not\mid n$ . But, I do not have any explanation for it. Any help would be appreciated. I also posted this question on MathOverflow , and received an answer that uses some advanced methods. I am looking forward for another solution that uses more elementary methods.","['algebraic-number-theory', 'number-theory', 'diophantine-equations', 'functions', 'trigonometry']"
4760567,Compute a limit,"Fix $\delta >0$ . What is the value of the below limit? $$\lim_{s\downarrow 0}s\int_0^{\infty}e^{-st}\cdot\frac{e^{-\delta^2/(2t)}}{2\pi t}dt.$$ I believe the limit equals zero, but I do not see a simple way to evaluate the limit. It's important to note that dominated convergence does not apply because $t\mapsto \frac{e^{-\delta^2/(2t)}}{2\pi t}$ is not integrable, and monotone convergence gives $0\cdot \infty$ so we couldn't use it either. For those curious, this limit comes from Kai Lai Chung and Zhongxin Zhao's From Brownian Motion to Schrodinger Equation , page 40.","['limits', 'calculus']"
4760573,Why is this subset associated to a $2$-tensor open and dense?,"Let $S$ be a symmetric $(0, 2)$ tensor on a Riemannian manifold $M$ . Define $E_S : M \to \mathbb{Z}$ by $E_S(x) = \left(\text{the number of distinct eigenvalues of } S_x\right)$ . I've seen the following claims in several papers: $M_S \doteq \left\{x \in M \ \vert \ E_s \text{ is constant in a neighbourhood of } x \right\}$ is an open dense subset of $M$ The eigenvalues of $S$ are distinct and smooth in each connected component $U$ of $M_S$ . I'm a having a hard time proving these facts in a rigorous enough manner. It ""feels"" true since in some sense the eigenvalues should be smooth, but I can't see how to formalize this precisely (smooth from where to where? how to prove smoothness?). I'd appreciate any help.","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4760614,"If $\lim_{n\to \infty} x_n =a$ and $\lim_{n\to \infty} (x_n - y_n) =0$, then $\lim_{n\to \infty} y_n =a$?","I guess I get the gist of the idea but I don't know how to write it rigorously, I know that we have: For all $\epsilon > 0$ , there is $n_0 \in N$ such that if $n>n_0$ , then $|x_n - a| <\epsilon$ . For all $\epsilon > 0$ , there is $n_0 \in N$ such that if $n>n_0$ , then $|x_n - y_n - 0| <\epsilon$ . What do I do now? I tried to use the triangle inequality: $$|(x_n - a)| -|(y_n -a)| \leq|(x_n - a) -(y_n -a)|=|x_n - y_n - (a-a)| <\epsilon$$ And due to the first inequality ( $|x_n - a| <\epsilon$ ), we can ""squeeze it to 0"" but I guess that with this inequality, we can't guarantee that we can ""squeeze"" ( $|y_n - a| <\epsilon$ ) to 0.","['limits', 'real-analysis']"
4760626,Units and $ax^2L^2+bxL+c=0$ in the real world?,"It seems that most math equations that come from the real world usually come with dimensions, even though those dimensions are generally ignored. I'm speaking of general dimensions, which include not only time and space but also temperature, mass, volume, or anything else that can be measured. Given this, I've always wondered where polynomials come from, since they seem to violate dimensionality. For example if you are talking lengths you might have: $$ax^2L^2+bxL+c=0$$ where $L$ stands for a length dimension applied to the factor immediately proceeding. This equation implies adding an $L^2$ to an $L$ and that to a unitless quantity, which is invalid. Now you could obviously fix it by adding appropriate dimensions to the constants; for example: $$aL^{-1} x^2L^2+bxL+cL=0L$$ But I can't think of any real-world application that would produce this kind of equation. Of course geometry naturally gives rise to squares and cubes, but they are isolated squares or cubes; squares aren't added to linear values and cubes aren't added to squares. So my question is, where do polynomials come from? Are there features of the physical world that give rise to them?","['dimensional-analysis', 'unit-of-measure', 'polynomials', 'physics', 'algebra-precalculus']"
4760743,Prove (or disprove) that two pairs of matrices generating the same subgroup give rise to isomorphic groups,"Let $A,B\in \operatorname{GL}(k,\mathbb{Z})$ be matrices of finite order such that $AB=BA$ and $A\neq \operatorname{I}$ , $B\neq \operatorname{I}$ . Define the group $\Sigma_{A,B}:=\mathbb{Z^2}\ltimes_{A,B} \mathbb{Z}^k$ , that is, the multiplication is given by $$\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) \cdot \left(s_1,s_2, \left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right)\right)=\left(r_1+s_1,r_2+s_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)+A^{r_1} B^{r_2}\left(\begin{array}{c}{t'_1\\ \vdots \\ t'_k}\end{array}\right) \right).$$ Note that this multiplication is well defined since $AB=BA$ . Question: Assume that $C,D\in\operatorname{GL}(k,\mathbb{Z})$ are commuting matrices of finite order such that $\langle A,B\rangle=\langle C,D\rangle$ , i.e. the pairs of matrices $\{A,B\}$ and $\{C,D\}$ generate the same subgroup. Also assume that the group $\langle A,B\rangle$ is not cyclic. Are $\Sigma_{A,B}$ and $\Sigma_{C,D}$ isomorphic? We have the following lemma: Lemma: (1) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{B,A}$ (2) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{A^{-1},B}$ (3) $\Sigma_{A,B}$ is isomorphic to $\Sigma_{A,AB}$ Proof: The isomorphisms are: For (1), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_2,r_1, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right) $ For (2), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(-r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)$ For (3), $\varphi\left(r_1,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)=\left(r_1-r_2,r_2, \left(\begin{array}{c}{t_1\\ \vdots \\ t_k}\end{array}\right)\right)$ I was thinking that if we could go from one generator set $\{A,B\}$ to another one $\{A^p B^q, A^r B^s\}$ applying the 3 operations above then the groups would be isomorphic, but I don't know if this is true, and I also don't know which properties must satisfy $p,q,r,s$ in order for $\{A^p B^q, A^r B^s\}$ to generate the subgroup $\langle A,B\rangle$ . Help would be greatly appreciated. Thanks!","['matrices', 'group-theory', 'abstract-algebra', 'finite-groups']"
4760746,Is there a way to computationally verify that the sporadic groups are simple?,"I'm trying to understand the ""easy"" direction of the CFSG: namely, the proofs that the 18 infinite families and 26/27 sporadic groups are indeed simple. I'm working through Simple Groups of Lie Type by Carter, and The Finite Simple Groups by Wilson, which uses Iwasawa's Lemma to show the simplicity of the Mathieu groups, the Leech lattice groups, and $Fi_{22}$ . However, I'm having trouble finding anything beyond that. I'm happy to settle for definitions like ""Let $J_1$ be the subgroup of $GL(20, \mathbb{F}_2)$ generated by [the matrices at https://brauer.maths.qmul.ac.uk/Atlas/v3/matrep/J1G1-f2r20B0]"", if there's a way to use those representations to check the simplicity. I'm guessing there isn't really a way to do this for a general group other than applying the classification, but given that these groups are indeed simple, and lots is known about them, maybe there is some particular thing I could look at that would allow a computationally feasible verification of that fact. For example, to show one of the groups G is perfect, no matter its size all I'd need to do is use generators $a, b$ with $a^2 = b^3 = (ab)^p$ with $p >= 7$ (which exist for the ones I care about according to https://projecteuclid.org/journals/illinois-journal-of-mathematics/volume-33/issue-3/On-Hurwitz-generation-and-genus-actions-of-sporadic-groups/10.1215/ijm/1255988653.full , and a lot of them are simply the standard generators listed in the ATLAS). Then I would define $G$ as the group generated by $a$ and $b$ , provide explicit formulas* for $a$ and $b$ as products of commutators, and that would prove $[G, G] = G$ . Does anyone have any ideas for something like that that doesn't depend on the order of the group, but shows it's simple instead of perfect? * \begin{align*}
[b, a][a, bb]([a, b][bb, a][b, a][a, bb])^m &= babbaabbab(ababbbbabababbaabbab)^m \\
&= babab(abababababab)^m \\
&= b(ab)^{2+6m} = b(ab)^{pr} = b \text{ for some m and r}
\end{align*} \begin{align*}
[a, b][bb, a]([b, a][a, bb][a, b][bb, a])^n &= ababbbbaba(babbaabbabababbbbaba)^n \\
&= abababa(babababababa)^n \\
&= a(ba)^{3+6n} = a(ba)^{ps} = a \text{ for some n and s}
\end{align*}","['computational-algebra', 'group-theory', 'simple-groups', 'finite-groups']"
4760747,"Evaluating $\int_{0}^{1}\mathrm{d}x\,\frac{\operatorname{arsinh}{(ax)}\operatorname{arsinh}{(bx)}}{x}$ in terms of polylogarithms","Define the function $\mathcal{I}:\mathbb{R}^{2}\rightarrow\mathbb{R}$ by the definite integral $$\mathcal{I}{\left(a,b\right)}:=\int_{0}^{1}\mathrm{d}x\,\frac{\operatorname{arsinh}{\left(ax\right)}\operatorname{arsinh}{\left(bx\right)}}{x},$$ where the inverse hyperbolic sine is given by $$\operatorname{arsinh}{\left(z\right)}:=\int_{0}^{z}\mathrm{d}t\,\frac{1}{\sqrt{1+t^{2}}}=\ln{\left(z+\sqrt{1+z^{2}}\right)};~~~\small{z\in\mathbb{R}}.$$ Basic properties of $\mathcal{I}$ include: $$\mathcal{I}{\left(a,b\right)}=\mathcal{I}{\left(b,a\right)},$$ $$\mathcal{I}{\left(0,b\right)}=\mathcal{I}{\left(a,0\right)}=0,$$ $$\mathcal{I}{\left(-a,b\right)}=\mathcal{I}{\left(a,-b\right)}=-\mathcal{I}{\left(a,b\right)}.$$ It suffices then to consider the $0<a\le b$ case to complete a general evaluation of $\mathcal{I}{\left(a,b\right)}$ . The case of equal parameters can be evaluated in terms of polylogarithms after making the appropriate Euler substitution, as we'll show below. Unfortunately, that method doesn't seem to extend to the case of unequal parameters. Question: Given $0<a<b$ , can we find a closed form expression for $\mathcal{I}{\left(a,b\right)}$ in terms of polylogarithms? Equal Parameter Case: Suppose $a\in\mathbb{R}\land a>0$ , and set $\alpha:=\operatorname{arsinh}{\left(a\right)}>0$ . We find $$\begin{align}
\mathcal{I}{\left(a,a\right)}
&=\int_{0}^{1}\mathrm{d}x\,\frac{\left[\operatorname{arsinh}{\left(ax\right)}\right]^{2}}{x}\\
&=\int_{0}^{a}\mathrm{d}y\,\frac{\left[\operatorname{arsinh}{\left(y\right)}\right]^{2}}{y};~~~\small{\left[x=a^{-1}y\right]}\\
&=\int_{0}^{\operatorname{arsinh}{\left(a\right)}}\mathrm{d}\tau\,\frac{\tau^{2}\cosh{\left(\tau\right)}}{\sinh{\left(\tau\right)}};~~~\small{\left[y=\sinh{\left(\tau\right)}\right]}\\
&=\int_{0}^{\alpha}\mathrm{d}\tau\,\tau^{2}\coth{\left(\tau\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}-\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\ln{\left(\sinh{\left(\tau\right)}\right)};~~~\small{I.B.P.}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}-\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\ln{\left(\frac{e^{\tau}-e^{-\tau}}{2}\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\ln{\left(\frac{2e^{-\tau}}{1-e^{-2\tau}}\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\left[\ln{\left(2\right)}-\tau-\ln{\left(1-e^{-2\tau}\right)}\right]\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\ln{\left(2\right)}-\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau^{2}-\int_{0}^{\alpha}\mathrm{d}\tau\,2\tau\ln{\left(1-e^{-2\tau}\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\alpha^{2}\ln{\left(2\right)}-\frac23\alpha^{3}-\int_{0}^{\alpha}\mathrm{d}\tau\,\tau\frac{d}{d\tau}\operatorname{Li}_{2}{\left(e^{-2\tau}\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\alpha^{2}\ln{\left(2\right)}-\frac23\alpha^{3}-\alpha\operatorname{Li}_{2}{\left(e^{-2\alpha}\right)}+\int_{0}^{\alpha}\mathrm{d}\tau\,\operatorname{Li}_{2}{\left(e^{-2\tau}\right)}\\
&=\alpha^{2}\ln{\left(\sinh{\left(\alpha\right)}\right)}+\alpha^{2}\ln{\left(2\right)}-\frac23\alpha^{3}-\alpha\operatorname{Li}_{2}{\left(e^{-2\alpha}\right)}-\frac12\operatorname{Li}_{3}{\left(e^{-2\alpha}\right)}+\frac12\operatorname{Li}_{3}{\left(1\right)}\\
&=\frac12\zeta{\left(3\right)}-\frac12\operatorname{Li}_{3}{\left(e^{-2\alpha}\right)}-\alpha\operatorname{Li}_{2}{\left(e^{-2\alpha}\right)}+\alpha^{2}\ln{\left(2\sinh{\left(\alpha\right)}\right)}-\frac23\alpha^{3}.\\
\end{align}$$","['integration', 'definite-integrals', 'special-functions', 'polylogarithm', 'closed-form']"
4760777,"Are angles of Pythagorean triples, e.g. $\tan^{-1}\frac34$, transcendental multiples of $\pi$?","Simple question, but I don't know the answer and can't easily find good resources. Sometimes we can give quite surprising exact forms to the circular functions at peculiar arguments through quite nontrivial methods, so I wasn't sure whether this was possible. Are the non-right angles corresponding to Pythagorean triples transcendental multiples of $\pi$ , or does it depend on the specific case? Let's say as a simple example we take the Pythagorean triple $(3,4,5)$ . One angle in this triangle would be $\tan^{-1}\frac34$ . Is this a transcendental multiple of $\pi$ ? If so, or if not, can we generalise? Edit: Original phrasing asked whether the angles were transcendental. This was just a mistake in typing due to tiredness, and attempts to correct this were also phrased badly due to more tiredness. Thanks for answering the question as intended rather than the literal interpretation.","['number-theory', 'trigonometry', 'transcendental-numbers']"
4760799,Show that a group of order $2010$ with an abelian normal subgroup of order $6$ is abelian,"Let $G$ be group of order $2010$ with $N$ an abelian normal subgroup of order $6$ ; show: $N\le Z(G)$ where $Z(G)$ is the center of $G$ Show that there exist a unique $5$ -Sylow subgroup of $G$ Conclude that $G$ is abelian For the first question, note that there exists an element $x$ of $N$ with order $3$ and an element $y$ with order $2$ ; we can show that both $\left\langle x \right\rangle,\left\langle y \right\rangle$ are characteristic in $N$ (so normal in $G$ ); this implies that $N=\left\langle x \right\rangle \times \left\langle y \right\rangle$ and we conclude by observing that $x,y \in Z(G)$ (by an argument related to conjugacy classes). But I'm stuck on the second question; how would you show that $P\in \text{Syl}_{5}(G)$ is unique? For the third part, I think it's fairly straightforward as all Sylow subgroups $(2, 3, 5, 67)$ are normal, so $G$ is the direct product of such subgroups.","['group-theory', 'abstract-algebra', 'sylow-theory']"
4760815,Evaluating $\left(\sum_{n=1}^{\infty}\frac{1}{(2n+1)^2-1}\right)-\left(\sum_{k=2}^{\infty}\sum_{n=1}^{\infty} \frac{1}{(2n+1)^{2k}-1}\right)$,"This essentially translates to $\left(\dfrac{1}{3^2-1}+\dfrac{1}{5^2-1}+\dfrac{1}{7^2-1}+\cdots\right)-\left(\dfrac{1}{9^2-1}+\dfrac{1}{25^2-1}+\dfrac{1}{27^2-1}\cdots\right)$ I used a method I'm not too sure about.
I assumed the trigonometric series: $\cos(x)\cos(x/3)\cos(x/5)\cos(x/7)\cos(x/9)\cdots$ The coefficients of $x^2$ in the entire series is $\left(\dfrac{-1}{2}\right)\left(\dfrac{1}{1^2}+\dfrac{1}{3^2}+\dfrac{1}{5^2}+\cdots\right)=\dfrac{-\pi^2}{16}$ Deprecating the cos series into infinite products: $$\begin{split}&\cos(x)\cos(x/3)\cos(x/5)\cos(x/7)\cos(x/9)\cdots\\ &=(\cos(x/1))(\cos(x/3)\cos(x/9)\cos(x/27)\cos(x/81)\cdots)\\
&\quad\,\,(\cos(x/5)\cos(x/25)\cos(x/125)\cdots)(\cos(x/7)\cos(x/49)\cos(x/343)\cdots)(\cdots)\cdots\end{split}$$ Individually the coefficients of $x^2$ are $\cos(x)$ gives $\dfrac{-1}{2}$ ; $\cos(x/3)$ series gives $\dfrac{-1}{2(3^2-1)}$ (Infinite GPs, $\dfrac{1}{3^2}+\dfrac{1}{9^2}+\cdots$ ) $\cos(x/5)$ series gives $\dfrac{-1}{2(5^2-1)}\cdots$ So the summation of the coefficients should equal $\dfrac{-\pi^2}{16}$ (Since $\cos(x)$ expansion is $1+a2x^2+\cdots$ , the coefficients should be added individually) So, $\dfrac{1}{1}+\dfrac{1}{3^2-1}+\dfrac{1}{5^2-1}+\dfrac{1}{7^2-1}+\dfrac{1}{11^2-1}+\dfrac{1}{13^2-1}\cdots=\dfrac{\pi^2}{8}\simeq1.233$ The value manually calculated upto $n=10$ gives $\simeq1.2125$ However I'm not sure if this method is valid or correct. I would greatly appreciate if anyone could rectify the errors or errenous assumptions in the derivation.","['trigonometry', 'sequences-and-series']"
4760819,Does the Diophantine equation $z_1^3 +z_2^3+z_3^3=\alpha^3$ have a solution for every integer $\alpha>0$?,"I should have asked this question before the other one since Moore's tables for $5$ th powers have radius $<17700$ , while Wrobleski's tables for $3$ rd powers go up to one million , hence more data to work with. And since this is Diophantine, then it is not required that the $z_k$ be positive integers. I. Form $x_1^3+x_2^3 = x_3^3+x_4^3$ We focus on primitive solutions and consider $(x_1,x_2) = (x_3,x_4)$ as trivial. The first ones are, \begin{align}
1^3 & + 12^3 \,=\, 10^3 + 9^3\\ 
2^3 & + 16^3 \,=\, 15^3 + 9^3\\ 
3^3 & + 60^3 \,=\, 59^3 + 22^3\\
4^3 & + 110^3 = 101^3 + 67^3\\ 
5^3 & + 76^3 \,=\, 69^3 + 48^3\\
6^3 & + 552^3 = 551^3 + 97^3
\end{align} and so on, for all $x_1<3000$ (which is quite a long stretch). Note that for some $x_1 = 13m$ , it seems terms are a bit larger, $$13^3 + 5288^3  = 5148^3 + 2253^3$$ II. Form $x_1^3+x_2^3+x_3^3=x_4^3$ Likewise, we disregard trivial solutions $x_i = x_4$ . Hence, \begin{align}
1^3 & + \,6^3\, + \,8^3 \,=\, 9^3\\
2^3 & + 17^3 + 40^3 = 41^3\\
3^3 & + 10^3 + 18^3 = 19^3\\  
4^3 & + 17^3 + 22^3 = 25^3\\ 
5^3 & + \,4^3\, + \,3^3\, =\, 6^3\\
6^3 & + 32^3 + 33^3 = 41^3
\end{align} and so on, also for all $x_1<3000$ . Again, for some $x_1 = 13m$ , terms are a bit larger, $$(4\times13)^3 + 321^3 + 3327^3 = 3328^3$$ P.S. Note that for squares, we have the identity, $$n^2 + (n + 1)^2 + (n^2 + n)^2 = (n^2 + n + 1)^2$$ which proves every positive integer $n$ appears at least once. It may be true for cubes as well. III. Questions For any positive integer $x_1$ , is it true there are primitive positive solutions to both forms? If we can't prove it in general, how high can Wroblewski's tables extend the confirmed range to a bound $x_1>3000$ ? (Less than $10^6$ for sure.)","['computational-mathematics', 'number-theory', 'diophantine-equations']"
4760856,Prove this trigonometry equation in geometry setting,"Equilateral triangle $BCD$ is constructed out of $\triangle ABC$ . Let $AA^\prime$ be diameter of $(ABC)$ . If $\angle BDA^\prime=\alpha$ , $\angle CDA^\prime=\beta$ , prove that
\[\frac{\sin(B+\alpha)}{\sin(C+\beta)}=\frac{\cos(B-A)}{\cos(C-A)}.\] I thought of constructing angles that equal to either of $B+\alpha$ , $C+\beta$ , $|B-A|$ , or $|C-A|$ , but cannot do anything meaningful.","['triangles', 'trigonometry', 'geometry']"
4760887,Double Limits When Defining Improper Integral,"In my textbook , an improper integral is defined as shown below, where $F$ is a primitive function of a function $f$ . $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}F(b)-\lim_{a\to-\infty}F(a) \tag{1}$$ Could I rewrite it in one of the following ways? $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to+\infty}\int_{-a}^af(x)\mathrm{\ d}x \tag{2}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{a\to-\infty}\lim_{b\to+\infty}\int_a^bf(x)\mathrm{\ d}x \tag{3}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{b\to+\infty}\lim_{a\to-\infty}\int_a^bf(x)\mathrm{\ d}x \tag{4}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\underset{b\to+\infty}{\lim_{a\to-\infty}}\int_a^bf(x)\mathrm{\ d}x \tag{5}$$ $$\int_{-\infty}^{+\infty}f(x)\mathrm{\ d}x\mathrel{\overset{\text{def}}{=\mathrel{\mkern-3mu}=}}\lim_{(a,b)\to(-\infty,+\infty)}\int_a^bf(x)\mathrm{\ d}x \tag{6}$$ What would be the differences between each one of these notations (talking about the limits)? Actually, are any of them even valid?","['measure-theory', 'improper-integrals', 'real-analysis', 'notation', 'limits']"
4760890,eigenvalues of a first order differential operator on a manifold,"I have the following, maybe naive question: Given a smooth vector field $X$ on a smooth manifold $M$ with $\dim M \geq 1$ .
Then this defines a linear operator $$
 D: C^\infty(M,\mathbb C) \to C^\infty(M,\mathbb C) ; f \mapsto X.f\,.
$$ What I would like to know is: What is known about the eigenvalues of this linear map $D$ ? It is clear to me that constant maps are always mapped to $0$ , so $0$ is always an eigenvalue. It is also clear that if $X=0$ , then $D=0$ and there is no other eigenvalue. But assume, $X$ is not the constant $0$ -vector field. Does this imply that $D$ has a complex non-zero eigenvalue ? Does this depend on whether $M$ is compact or not? EDIT: It looks to me as if $X$ being not constantly zero is not enough to conclude the existence of a non-zero eigenvalue.
Motivated by the comment of Ben Grossmann I looked at the real line $\mathbb R$ and the circle $\mathbb R/\mathbb Z$ and it seems that $X(x)=\cos^2(x)$ should be an example of a vector field where $D$ has only the eigenvalue $0$ -- if my calculus was correct. So, maybe the correct question is: Assume that $X$ is everywhere non-zero, does this imply that we have a non-zero eigenvalue ? The comment of Ben Grossmann answeres this in the affirmative for $M=\mathbb R$ and I think a very similar argument should work for the circle $M=\mathbb R/\mathbb Z$ . The comment of Eric solves this in the affirmative locally but it is not clear how the global geomety of $M$ comes into play here.","['vector-fields', 'smooth-manifolds', 'linear-algebra', 'functional-analysis', 'differential-geometry']"
4760903,Supremum of the random process,"Let $X_1(t),...,X_N(t)$ be $N$ independent, mean zero random processes indexed by points $t \in T$ .  Let $\epsilon_1,...,\epsilon_N$ be independent symmetric Bernoulli random variables. Prove that $$
\frac{1}{2} \mathbb{E} \sup_{t \in T} \sum_{i=1}^N \epsilon_i X_i(t) \leq \mathbb{E} \sup_{t \in T} \sum_{i=1}^N  X_i(t) \leq 2 \mathbb{E} \sup_{t \in T} \sum_{i=1}^N \epsilon_i X_i(t)
$$ This is the problem from the high dimensional probability 7.1.9(Vershynin) I'm trying to solve this problem and already show the upper bound. To show the lower bound, \begin{align*}
\mathbb{E} \sup_{t \in T} \sum_{i=1}^N \epsilon_i X_i(t) &= \mathbb{E} \sup_{t \in T} \sum_{i=1}^N \epsilon_i [X_i(t) + \mathbb{E} -X_i'(t))] \\
&\leq \mathbb{E} \sup_{t \in T} \sum_{i=1}^N \epsilon_i [X_i(t) - X_i'(t)] \\
&= \mathbb{E} \sup_{t \in T} \sum_{i=1}^N [ X_i(t)- X_i'(t)]
\end{align*} To end the proof of the lower bound, I think I need to show that \begin{align}
\mathbb{E} \sup_{t \in T} \sum_{i=1}^N [ X_i(t)- X_i'(t)] \leq \mathbb{E} \sup_{t \in T} \sum_{i=1}^N X_i(t)+ \mathbb{E} \sup_{t \in T} \sum_{i=1}^N X_i'(t). \;\;\;\;\;\;(1)
\end{align} $X_i'(t)$ is the independent copy of the random process $X_i(t)$ Does (1) inequality hold?","['stochastic-processes', 'inequality', 'probability-theory', 'random-walk']"
4760935,prove by induction $\sqrt{1 \sqrt{2 \sqrt{3.....\sqrt n }}} < 2$,"My attempt: let $$p(n-1) = \sqrt{1 \sqrt{2 \sqrt{3.....\sqrt{n-1} }}}$$ now let us assume that $p(n-1)$ is less than 2 for $n = 1$ (base case) $ \sqrt 1 = 1 < 2$ now $$\frac{p(n)}{p(n-1)}  = \frac{\sqrt{1 \sqrt{2 \sqrt{3.....\sqrt{n}}}}}{ \sqrt{1 \sqrt{2 \sqrt{3.....\sqrt{n-1} }}})} = n^{(1/2^n)}$$ now $$\sqrt{1 \sqrt{2 \sqrt{3.....\sqrt n }}} < \sqrt{1 \sqrt{2 \sqrt{3.....\sqrt{n-1}  }}} \cdot n^{(1/2^n)} $$ as $p(n-1)$ is less than 2 so we can write $$\sqrt{1 \sqrt{2 \sqrt{3.....\sqrt n }}} < 2n^{(1/2^n)} $$ as n tends to infinity $2n^{(1/2^n)}$ converges to 2 so by taking limits on both sides we have $$\lim_{{n \to \infty}} \sqrt{1 \sqrt{2 \sqrt{3.....\sqrt n }}} < 2$$ now $\frac{p(n)}{p(n-1)}$ equals $n^{(1/2^n)}$ which is always greater than 1 this means that $p(n)$ is strictly increasing. So if p(n) is increasing and $\lim_{{n \to \infty}} p(n) $ < 2 then this means that p(n) is always lesser than 2 for all n. This was my argument, but I wasn't given credit for this solution. So is there anything wrong with my solution.","['induction', 'discrete-mathematics']"
4760989,Unconditional convergence implies convergence of sum of norm squares in hilbert space,"Let $(H, (.,.))$ be a Hilbertspace and consider the definition of unconditional convergence, i.e. for a set $I \neq \emptyset$ and a family of vectors $(x_i)_{i \in I}$ in $H$ the series $$ \sum_{i \in I}x_i$$ is said to be unconditional convergent to a value $x \in H$ , if for any $\epsilon > 0$ there is a finite $I_0 \subseteq I$ such that $$ \left\lVert \sum_{i \in \tilde{I}}x_i - x\right\rVert<\epsilon$$ for all finite $\tilde{I} \supseteq I_0$ . Using this definition I wanted to prove that the unconditional convergence of a series $ \sum_{n \in \mathbb{N}} x_n$ in $H$ implies the convergence of $\sum_{n=1}^{\infty} \lVert x_n \rVert^2$ . Using unconditional convergence of $ \sum_{n \in \mathbb{N}} x_n =\colon y$ and the fact that the inner product is linear and continuous in each argument we deduce that $$(y,y) = (\sum_{n \in \mathbb{N}}x_n, \sum_{m \in \mathbb{N}}x_m) = \sum_{n \in \mathbb{N}}\sum_{m \in \mathbb{N}}(x_n, x_m)$$ and analogously for the sums interchanged. Here, the sums on the right side converge unconditionally. From there I want to infer, and I don't know how to do that step, that $$ \sum_{(n,m) \in \mathbb{N} \times \mathbb{N}}(x_n,x_m)$$ converges unconditionally, which would then lead me to the unconditional convergence of $$ \sum_{(n,m) \in \mathbb{N} \times \mathbb{N}}\lvert (x_n,x_m) \rvert.$$ Hence we get $$\sum_{n=1}^{\infty} \lVert x_n \rVert^2 = \sum_{n \in \mathbb{N}}(x_n, x_n) \leq \sum_{(n,m) \in \mathbb{N} \times \mathbb{N}}\lvert (x_n,x_m) \rvert.$$ Thus my question is if is this is correct and if so, how can the step which I highlighted with ""I don't know how to do"" be justified? Thanks in advance.","['hilbert-spaces', 'functional-analysis', 'sequences-and-series']"
4761019,8 planes tangent 3 spheres in the space,"I know it might seem like a trivial question, but I think the result is very long and I wanted a consultation to find a ""smart"" way to solve it without wasting hours of time on unnecessarily long calculations. I was looking for a ""compact"" algorithm for calculating the equations of the $8$ tangent planes $3$ generic spheres in space. I found the $2D$ version (i.e. $4$ tangent lines $2$ circles) $$\begin{cases} (x-x_1)^2+(y-y_1)^2=r_1^2\\(x-x_2)^2+(y-y_2)^2=r_2^2\end{cases}\qquad{\color{gray}{\left[\begin{matrix}d_x=x_2-x_1\qquad d_y=y_2-y_1\\d=\sqrt{e^2+f^2} \end{matrix}\right.}}$$ $$r: y-y_0=m(x-x_0)$$ Where $$\begin{array}{|c||c|c|}\hline&\text{Internal}&\text{External}\\\hline(x_0,y_0)&\left(\dfrac{x_2 r_1+x_1 r_2}{r_1+r_2},\dfrac{y_2 r_1+y_1 r_2}{r_1+r_2}\right)&\left(\dfrac{x_2 r_1-x_1 r_2}{r_1-r_2},\dfrac{y_2 r_1-y_1 r_2}{r_1-r_2}\right)\\m&\dfrac{d_xd_y\pm(r_1+r_2)\sqrt{d^2-(r_1+r_2)^2}}{d_x^2-(r_1+r_1)^2}&\dfrac{d_xd_y\pm(r_1-r_2)\sqrt{d^2-(r_1-r_2)^2}}{d_x^2-(r_1-r_2)^2}\\\hline\end{array}$$ I wanted to generalize to $$\begin{cases} (x-x_1)^2+(y-y_1)^2+(z-z_1)^2=r_1^2\\(x-x_2)^2+(y-y_2)^2+(z-z_2)^2=r_2^2\\(x-x_3)^2+(y-y_3)^2+(z-z_3)^2=r_3^2\end{cases}$$ The fact that they are 8 makes me think that the result depends on a kind of ""binary code"" understood as the alternation of signs of 3 elements (something like $\pm a_1\pm a_2\pm a_3$ ,seeing the formula for the $2D$ case I guess it could be the radius) I was thinking that maybe a ""vectorial"" version of the problem could be useful too, ie $$\begin{cases}
\Vert\mathbf{x}-\mathbf{x}_1\Vert=r_1\\
\Vert\mathbf{x}-\mathbf{x}_2\Vert=r_2\\
\Vert\mathbf{x}-\mathbf{x}_3\Vert=r_3
\end{cases}$$ and the result expressed as a dot product $$\langle \mathbf{x}-\mathbf{x}_0,\mathbf{a}\rangle=0$$ Where $\mathbf{x}:=(x,y,z)$ , $\mathbf{x}_0:=(x_0,y_0,z_0)$ and $\mathbf{a}:=(a,b,c)$","['spheres', '3d', 'tangent-line', 'geometry', 'algorithms']"
4761021,Intuitive difference between optimal transport distance and Fisher information distance,"Let me start by saying I'm not a mathematician but a biologist with an interest in mathematics. I have a set of covariance matrices and I am interested in studying their geometry in the Symmetric Positive Definite Matrices (SPDM) manifold. I'm particularly interested in relating the geometry to statistical properties of the centered Gaussians defined by those matrices (e.g. how 'discriminable' the data generated by the different Gaussians is). There exist a plethora of metrics that are used with this manifold. Two of these metrics (at least) are related to probabilistic/statistics concepts. One is the Affine Invariant (AI) distance, which according to this source *Up to a constant, it is known as the Fisher information metric"". The other one is the Wasserstein distance , which is the optimal transport distance. The AI distance between $A$ and $B$ is given by: $$d(A,B) = ||log(A^{-1/2} B A^{-1/2})||_F$$ The Wasserstein distance is given by: $$d(A,B) = tr(A) + tr(B) - 2tr((A^{1/2} B A^{1/2})^{1/2})$$ These two metrics behave very differently. For one, it is my understanding that the AI manifold has negative curvature, while the Wasserstein manifold has non-negative curvature. But I'm still not clear on the differences between the two from a probability/statistical conceptually point of view. What do the two distances tell me with regards to the statistical problem of discriminating between classes that generate data according to the Gaussians of those SPDM? What complementary information about this problem do the two metrics give? This gives some intuition of the difference between KL divergence and Optimal transport distance, but I'm not sure how much translates to the AI/Fisher information metric.","['statistics', 'information-geometry', 'symmetric-matrices', 'positive-definite', 'differential-geometry']"
4761043,Conformal mapping from a wedge to the upper half complex plane,"I found (L. J. Laslett, "" On Intensity Limitations Imposed by Transverse Space-Charge Effects in Circular Particle Accelerators "", Proceedings of The 1963 Summer Study on Storage Rings, Accelerators and Experimentation at Super-High Energies , Lawrence Radiation Laboratory, 1963 pp. 324-367) a conformal mapping formula which is supposed to map a wedge (coordinate $z=-X$ ) with an angle of $2\alpha$ at the sharp corner into the upper half plane. The sharp corner of the wedge is mapped to $z=0$ . $$z' =ic'\left(\frac{z}{X} +1\right)^{\frac{\pi}{2\alpha}}$$ Does this conformal transformation indeed map a wedge on the upper half plane? If I parametrize the points on the upper flank of the wedge  by (I represent here the complex numbers as vector $ z=x+iy = (x,y)^T$ ): $$z = \left(\begin{array}{c} -X \\ 0\end{array}\right) + t\left(\begin{array}{c} 1 \\ \tan(\alpha)\end{array}\right)$$ with $t=[0,  \infty]$ I have difficulties to see that $z'$ ends up to be a real number -- i.e. the lower limit of the upper plane. Moreover I wonder of the role of the parameter $c'$ . Is it  free or does it depend on $X$ or $\alpha$ ? Thank you for any help.","['complex-analysis', 'conformal-geometry']"
4761048,Possible connection between prime numbers in binary and $\pi$,"I just realized I asked a question with a very similar title a while back. This is not a duplicate. It is another conjecture though. Conjecture: $\lim\limits_{x\to\infty}\mu \{ f(2), f(3), f(5), ... , f(x) \}=\frac{\pi}{2}-1$ where $x\in\mathbb{P}$ , $f(x)$ equals the amount of $1$ s divided by the total number of digits in the binary representation of $x$ . $\mu$ is the arithmetic mean and $\mathbb{P}$ represents the set of all primes. Example of $f(x)$ because of my poor explanation (I tried my best): $f(5)\rightarrow 101\rightarrow \frac{\text{number of $1$s (or the digit sum)}}{\text{total number of digits}}\rightarrow\frac{2}{3}$ I have no idea why this conjecture would be true but it appears that it may converge, at least for the first $2000$ primes. In the picture, the green line is $y=\frac{\pi}{2}-1$ and the black points are $\mu \{ f(2), f(3), f(5), ... , f(x) \}$ . My question is whether or not my conjecture is true? Am I missing something obvious here? Also, I find it very interesting that it looks similar to a quasiperiodic function.","['binary', 'conjectures', 'pi', 'limits', 'prime-numbers']"
4761049,How to prove that there is a unique line between 2 distinct points using linear algebra?,"I want to prove using the parametric equation of line that any 2 distinct points $\ P,Q \in \mathbb{R}^2 $ is connected by a unique line. We know that a such a line can be formed by fixing one of the points as the end point of the starting vector and the other vector as a direction vector. We can represent the line as $ L = \{ P + \lambda (Q-P) \}$ or $ L = \{ Q + \mu (P-Q) \}$ where $\lambda,\mu \in \mathbb{R}$ . How to prove that the two forms are equivalent to each other?","['parametric', 'linear-algebra', 'geometry']"
4761051,Why can't a base be negative in an exponential function?,"The function $f(x) = a^x$ is generally taught to only allow $a > 0$ . This is usually justified by giving a few examples of complex points in cases where $a < 0$ . For example: $f(x) = (-2)^x$ , if $x = 0.5, f(x)=sqrt(-2)$ , which of course is $0+sqrt(2)i$ , a complex number. That being said, can't any number be approximated using odd denominators, and thus be put in the function?
Example: $x=3/4$ can be approximated using increasingly large powers of 3 in the denominator $2/3<x<3/3$ $6/9<x<7/9$ $20/27<x<21/27$ and so on, in this way a real value of $f(x)$ can be always determined for any real $x$ , even if $a<0$ . So why do teachers and graphing software refuse to consider $f(x)=(-e)^x$ (or similar) as functions?","['partial-functions', 'functions', 'exponential-function', 'domain-theory']"
4761053,Find $f(x)$ such that $\sum_{k=0}^{\infty}f^k\left(x\right)=\sum_{k=0}^{\infty}f^{\left(k\right)}\left(x\right)$,"I wonder how we can find all $f(x)$ that satisfies $$\sum_{k=0}^{\infty}f^k\left(x\right)=\sum_{k=0}^{\infty}f^{\left(k\right)}\left(x\right)\tag{1}$$ Here $f^{(k)}(x)$ means the $k$ th derivative of $f(x)$ . We could tell that this DE is not a linear differential equation, let alone has a finite order. For the LHS to converge, $|f(x)|<1$ . So we could use geometric series to write this as $$\frac1{1-f(x)}=\sum_{k=0}^{\infty}f^{\left(k\right)}\left(x\right)$$ Replacing $f(x)=y$ and integrating with respect to $y$ , we get $$-\ln|1-y|=\frac{y^2}2+\int\sum_{k=1}^\infty y^{(k)}dy+C$$ I was thinking of substituting the LHS of $(1)$ into the integrand, but that just gives $-\ln|1-y|=-\ln|1-y|$ . How can we solve this differential equation? If we don't have a closed form, can we write the equation differently (not something obvious like adding and subtracting the same number), obtain asymptotics for $f(x)$ , or find approximations?","['ordinary-differential-equations', 'asymptotics', 'calculus', 'closed-form', 'derivatives']"
4761057,Is my proof of Markov Property for Reflected BM correct?,"I want to show that $|B_{t}|$ is a Markov Process where, $B_{t}$ is a Standard Brownian Motion. I have seen the proof here and here . But I don't understand why the method below might fail (or if it's correct). I have the following Lemma at my disposal: (Ref Rene Schilling Brownian Motion Lemma $A.3
$ ) Let $X:(\Omega,\mathcal{F})\to (D,\mathcal{D})$ and $Y:(\Omega,\mathcal{F})\to(A,\mathcal{E})$ be measurable maps such that $X$ is $\mathcal{X}$ measurable and $Y$ is $\mathcal{Y}$ measurable and $\mathcal{X}$ and $\mathcal{Y}$ are independent sub-sigma algebras of $\mathcal{F}$ . Then for any $\Phi:D\times E\to \Bbb{R}$ be measurable and bounded , we have that $E(\Phi(X,Y)|\mathcal{X})=E[\Phi(x,Y)]\vert_{x=X}=E(\Phi(X,Y)|X)$ . So if we use the above Lemma, if $u$ be a Bounded measurable function and if I set $\phi(X,Y)=u(|X+Y|)$ where $X=B_{s}$ and $Y=B_{t+s}-B_{t}$ and we set $\sigma(B_{t}:t\leq s)=\mathcal{X}$ and $\mathcal{Y}=\sigma(B_{t+s}-B_{s})$ Then, by the above lemma, we have that $E(\Phi(X,Y)|\mathcal{X})=E\bigg(u(|B_{t+s}|)\bigg|\mathcal{X}\bigg)=E\bigg(u(|B_{t+s}|)\bigg|B_{s}\bigg)$ which is the criteria for a process $X_{t}$ to be a Markov Process. Where and how am I wrong in arguing this way? Any help is appreciated.","['conditional-expectation', 'markov-process', 'solution-verification', 'brownian-motion', 'probability-theory']"
4761068,Is every connected complete locally compact metric space proper?,"The Hopf-Rinow theorem for metric spaces states that a length metric spaces if complete and locally compact then it is proper, i.e. every closed bounded subset is compact. I wonder whether there is a counter example if one drops the length metric requirement and adds connectedness. More precisely: If a metric space is connected complete and locally compact, then is it proper ?","['general-topology', 'metric-spaces']"
4761075,Integrals Involving GCD Function,"Recently I got interested in Integrals involving GCD. I was wondering if I could get more such Integrals, Here are some examples: $$\int_0^\pi \sin^2(ab x) \cot(ax)\cot(bx)dx=\left(\frac{2\color{red}{\gcd(a,b)}-1}{2}\right)\pi$$ $$\int_0^{\pi/2}\ln{\lvert\sin(mx)\rvert}\cdot \ln{\lvert\sin(nx)\rvert}\, dx=\frac{\pi^3}{24}\frac{\color{red}{\gcd^2(m,n)}}{mn}+\frac{\pi\ln^2(2)}{2}$$ $$\int_{0}^{\pi}\arctan\left(\cot\left(mt\right)\right)\arctan\left(\cot\left(nt\right)\right)dt=\frac{\pi^{3}}{12}\cdot\frac{\color{red}{\gcd^2\left(m,n\right)}}{mn}$$ $$\int_{0}^{1}\lfloor{ax}\rfloor \lfloor{bx}\rfloor 
dx=\frac{\color{red}{\gcd^2\left(a,b\right)}}{12ab}-\frac{a}{4}-\frac{b}{4}-\frac{b}{12a}-\frac{a}{12b}+\frac{ab}{3}+\frac{1}{4}$$ Here is a Conditional One: If $c$ is not a multiple of $\gcd(a,b)$ then, $$\int_{0}^{\pi/2}\cos^{2}\left(ct\right)\ln\left(\left|\sin at\right|\right)\ln\left(\left|\sin bt\right|\right)dt=
\frac{\pi^{3}}{48}\frac{\color{red}{\gcd^{2}\left(a,b\right)}}{ab}+\frac{\pi\ln^{2}\left(2\right)}{4}$$ This is not a question per say but I am just curious about more of these Integrals. You can answer with a new Integral and maybe its solution. I will also keep adding more such Integrals.","['integration', 'calculus', 'definite-integrals']"
4761150,Where was the formal definition of a limit first published?,"I've read that Bolzano was one of the first to formalize a limit with the epsilon-delta definition, but I can't find a text of his that shows his definition. Does anyone know what publication the first epsilon-delta definition of a limit can be found in?","['epsilon-delta', 'real-analysis', 'mathematicians', 'limits', 'math-history']"
4761221,"When picking a delta to show a function is continuous, does that delta need to rely on epsilon?","Generally, when we want to show continuity, we are given a small $\varepsilon$ and we pick a $\delta$ according to this $\varepsilon.$ Heuristically, we can consider this $\delta$ to be a function of $\varepsilon,$ which must decrease as $\varepsilon$ decreases, for example with the function $x \mapsto x$ , we have $\delta(\varepsilon) = \varepsilon.$ I also see that with a constant function, we can pick $\delta$ to be any number we like, so in a way our function $\delta$ does not depend on $\varepsilon.$ My question is: Let $(X,d_x),(Y,d_Y)$ be metric spaces. If $f: X \to Y$ is a non-constant continuous function, must $\delta$ depend on $\varepsilon$ ? I can see that if $X$ is infinite the answer is yes. Say we choose a $\delta>0$ , and pick $x,x' \in X : d_X(x,x') < \delta,$ we can pick $\varepsilon  = d_Y(f(x),f(x')),$ which means that this $\delta$ does not work for this $\varepsilon.$ I think the infinite set means that we can always pick $x,x'$ such that $f(x)\ne f(x')$ , and $d_x(x,x')<\delta.$ I see that this argument does not work does not work if $X$ is finite though, as we can pick $\delta = \min_{x,x' \in X}\{d_X(x,x')\},$ so I worry that there is some weird finite set where we can pick a $\delta$ that works for all $\varepsilon.$ I know that thinking of $\delta$ as a function of $\varepsilon$ in this way isn't necessarily accurate, as it may imply that only one $\delta$ works for a $\varepsilon$ , which is not true, but this isn't the point of my question. EDIT: I think I have thought of a counter-example. Let $X = Y = \{0,1\}$ with the discrete metric. Let $f(x) = x.$ Then choosing $\delta = 1/2$ implies continuity, but $f$ is not constant.","['continuity', 'analysis', 'epsilon-delta']"
4761237,Probability of student not taking course,"An elementary school is offering 3 language classes: one in Spanish, one in French, and one in German. These classes are open to any of the 87 students in the school. There are 34 in the Spanish class, 31 in the French class, and 19 in the German class. There are 14 students that in both Spanish and French, 5 are in both Spanish and German, and 6 are in both French and German. In addition, there are 2 students taking all 3 classes. If two students are chosen randomly, what is the probability that neither of them are taking Spanish? My thought process was to first find the probability that the first and second student was taking Spanish, which would be 34/87 * 33/86. We have to find the complement of this, which would be 1-(187/1247) = 1060/1247. Is this correct? If not, can someone tell me how to go about the problem?",['statistics']
4761260,Proving the limit $\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6}$ using the epsilon-delta definition.,Prove $\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6}$ . Solution Attempt: $\left| f(x) - L \right| < \epsilon$ $\left|f(x)-L \right|=\left| \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2} - \frac{\sqrt[3]{2}}{6} \right|$ $=\left| \frac{6\left( \sqrt[3]{x}+\sqrt[3]{2}\right)-\sqrt[3]{2} (x+2)}{6(x+2)} \right|$ $=\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{6(x+2)}\right|$ $=\frac{1}{6}\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right|$ $\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right|<6\epsilon$ I am now stuck at this part of the solution. Can someone assist me?,"['limits', 'epsilon-delta']"
4761267,Summation $\sum_{i\neq j}^{\infty}\frac{1}{(i^2-j^2)^2}$,"I am stuck at this summation. $$\sum_{i\neq j}^{\infty}\frac{1}{(i^2-j^2)^2}$$ $i, j$ start from $1$ . I am kind of expecting an answer in terms of $\pi^4$ but that might not be true. Also this is a sum I came across while researching something so I do not expect it to have an answer.","['summation', 'sequences-and-series']"
4761274,Are $3^{2k}-3^k+1$ and $3^{2k}+3^k+1$ ever prime powers at the same time?,"Let $q=3^k$ with $k\geq 2$ .  I'd like to know whether $q^2-q+1$ and $q^2+q+1$ are ever prime powers at the same time. I've checked up to $k=50$ , and it seems that the total number of prime divisors of $q^2-q+1$ and $q^2+q+1$ is increasing, but I don't know how to prove this. Any help/references appreciated!","['number-theory', 'polynomials', 'prime-numbers']"
4761279,Clarification on treating a variable as constant in partial derivatives vs. allowing it to vary in total differentials,"I am taking a course in mathematics for physicists and in one lesson we discussed total differentials ( $df=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy$ ). There is a point that's puzzling me (I know that it is basic, sorry): the apparent contradiction in the treatment of a variable as both constant and varying in different contexts. Specifically, I'm trying to reconcile how, in the calculation of partial derivatives, we treat a variable (let's say y) as constant, yet in the context of total differentials, we allow the same variable (y) to vary (as indicated by its non-zero differential dy). This seems contradictory to me, as we're simultaneously treating y as both a constant and a variable. How can we justify treating a variable as constant in one scenario while allowing it to vary in another scenario? Could someone provide an explanation that clarifies this situation, other than the ""different perspectives"" or ""two contexts"" explanation? Thank you for any insights or references you can provide!","['multivariable-calculus', 'calculus']"
4761287,Finding new solutions to $a^5+b^5+c^5+d^5+e^5=0$?,"This post discussed the more symmetric equation, $$x_1^5+x_2^5+x_3^5 = y_1^5+y_2^5+y_3^5$$ where we assume all terms $\in \mathbb{Z},$ solutions as primitive , and $(x_1, x_2, x_3) = (y_1, y_2, y_3)$ as trivial. Jyrki Lahtonen pointed out that for such equations, if one term happens to be a multiple of $11$ , then another term must also be a multiple of $11$ . For the three known solutions of the special case $x_1 = 0$ (a counterexample to Euler's Sum of Powers conjecture), it seems to be unmentioned that all three have terms with $11m$ . I. Solution 1 While not the first discovered, this has a lot of structure, $$11^5\,(\color{red}{0^5} + 20^5) + 14132^5 = 11^5(457^5 + 567^5) + 14068^5$$ $$14132-14068 = 2^6\quad$$ $$\; 457+567 = 2^{10}$$ Having only two odd terms, it must obey a general congruence discused in this post . II. Solution 2 Likewise, this also has just two odd terms, and follows the aforementioned congruence. $$11^5\,(\color{red}{0^5} + 10^5) + 27^5 + 84^5 + 133^5 = 144^5\quad$$ $$27+133\equiv 0 \text{ mod }2^5$$ This was the first discovered (by accident) by Lander & Parkin in 1967. Incidentally, $$-144+133\equiv 0 \text{ mod }11\;\;$$ a property shared by the other solution below. III. Solution 3 Last one to be discovered (1996). This has four odd terms, so, $$11^5\,(\color{red}{0^5} + 5^5) + 3183^5 + 28969^5 + 85282^5 = 85359^5$$ $$55+28969\equiv 0 \text{ mod }2^5$$ $$-3183+85359\equiv 0 \text{ mod }2^5\;$$ though it is uncertain if both pairs of odd terms will always obey the congruence. And, $$-85282+85359\equiv 0 \text{ mod }11\;\;$$ IV. Questions If $x_1=0,$ is it a congruence requirement that one term be a multiple of $11$ ? As a long shot, can we use the symmetric structure of Solution 1 to find similar solutions?","['computational-mathematics', 'number-theory', 'modular-arithmetic', 'diophantine-equations']"
4761289,"Does $\lim_{n \rightarrow \infty} \int_{-\infty}^\infty\frac{\cos x}{1 + x^{2n}} dx$ exist, and if so what is it?","Let $$f(n) = \int_{-\infty}^\infty\frac{\cos x}{1 + x^{2n}} dx$$ Question: I'm curious if $\lim_{n \rightarrow \infty} f(n)$ exists and if so, if the exact value is known. According to Desmos at least, we have: $$\begin{align*}
f(1) & = \pi / e \approx 1.15572734979\\
f(2) & \approx 1.54427604502 \\
f(3) & \approx 1.63476626339 \\
f(4) & \approx 1.6604332697 \\
f(5) & \approx 1.67010286509 \\
& \vdots \\
f(98) & \approx 1.68291615825 \\
f(99) & \approx 1.68291667745 \\
f(100) & \approx 1.68291718113 \\
& \vdots \\
f(998) & \approx 1.68294172092 \\
f(999) & \approx 1.68294172142 \\
f(1000) & \approx 1.68294172191 \\
\end{align*}$$ So on the surface, it looks like it could converge, but I can't distinguish that from a slowly growing function like $\log x$ . Computing $f(1)$ is actually an exercise I did in my complex analysis course many years ago now, and I tried using the same approach to solve this, but I ran into an issue that I'll explain below. Here's what I tried... We use a domain $D_R$ which for a given radius $R$ is bounded by $\Gamma_R \cup [-R, R]$ , where $\Gamma_R$ is the upper semicircle with radius $R$ centered at the origin, and $[-R,R]$ is a line segment on the real axis. The function inside the integral, $\frac{\cos z}{1 + z^{2n}}$ , appears to have simple poles at the roots of unity, only rotated by $\pi/4n$ . As a result of this rotation, exactly half of the poles are in the upper half-plane. When we substitute $e^{iz}$ for $\cos z$ here, the residues look like $$\text{Res }\left[\frac{e^{iz}}{1 + z^{2n}},z_j\right] = \lim_{z \rightarrow z_j} (z - z_j)\frac{e^{iz}}{1 + z^{2n}} = \frac{e^{iz_j}}{\prod_{k=1,k \neq j}^{2n}(z_j - z_k)}$$ Note that for points $z \in \Gamma_R$ , $|1 + z^{2n}| \leq R^{2n} - 1$ (since $|z^{2n}| = R^{2n}$ and so $|1 + z^{2n}|$ can be at most one lower than $|z^{2n}|$ ). Since $|e^{iz}| \leq 1$ in the upper half plane, by the ML-estimate, we have $$\left|\int_{\Gamma_R} \frac{e^{iz}}{1 + z^{2n}} dz \right| \leq \frac{\pi R}{R^{2n} - 1}$$ where the right side vanishes as $R \rightarrow \infty$ . In other words, $$\lim_{R \rightarrow \infty}\int_{\Gamma_R} \frac{e^{iz}}{1 + z^{2n}} dz = 0.$$ By the Residue Theorem, we have $$\begin{align*}
\int_{\partial D_R} \frac{e^{iz}}{1 + z^{2n}} dz & = \int_{\Gamma_R} \frac{e^{iz}}{1 + z^{2n}} dz + \int_{-R}^R \frac{e^{iz}}{1 + z^{2n}} dz \\
& = 2 \pi i \sum_{j = 1}^n\left(\text{Res }\left[\frac{e^{iz}}{1 + z^{2n}},z_j\right] \right) \\
& = 2 \pi i \sum_{j = 1}^n\left(\frac{e^{iz_j}}{\prod_{k=1,k \neq j}^{2n}(z_j - z_k)} \right) \\
\end{align*}$$ where I'm assuming $j \in \{1, 2, \dots, n\}$ represent the half of the poles which are in the upper half plane. When we take $R \rightarrow \infty$ this gives $$\begin{align*}
\int_{-\infty}^\infty \frac{e^{iz}}{1 + z^{2n}} dz & = 2 \pi i \sum_{j = 1}^n\left(\frac{e^{iz_j}}{\prod_{k=1,k \neq j}^{2n}(z_j - z_k)} \right) - \lim_{R \rightarrow \infty}\int_{\Gamma_R} \frac{e^{iz}}{1 + z^{2n}} dz \\
& = 2 \pi i \sum_{j = 1}^n\left(\frac{e^{iz_j}}{\prod_{k=1,k \neq j}^{2n}(z_j - z_k)} \right).\end{align*}$$ This is normally where I would take the real part of the right side to get the value of $f(n)$ , but I really don't even know how to compute that. Even if I try taking $n \rightarrow \infty$ , I don't see a way to simplify it. Any insight would be much appreciated. :) Have a wonderful day!","['complex-analysis', 'limits', 'calculus', 'definite-integrals']"
4761295,Why the study of spanning tree is important in graph theory?,"A spanning tree of a graph is a subgraph obtained by deleting only edges of the graph and which is also a tree. Why does one study ""spanning tree"" in graph theory? What are ""spanning trees"" real life application? We know the number of spanning trees of a graph is equal to any cofactor of the Laplacian matrix of the graph. Why is one interested in knowing number of spanning trees of a given graph? I would like to know enough in order to motivate a undergraduate or graduate student.","['graph-theory', 'trees', 'soft-question', 'discrete-mathematics']"
4761316,Is the series $ \sum_{n=1}^{\infty} \frac{\cos (\frac{1}{n^2})}{\sqrt{2n+1}}$ divergent or convergent?,"I want to know if my reasoning is correct about the divergence of the series $\displaystyle \sum_{n=1}^{\infty} \frac{\cos (\frac{1}{n^2})}{\sqrt{2n+1}}$ . First, we know that $\displaystyle \frac{1}{n^2} \in (0,1], \forall n \in \mathbb{N}$ . The function $\cos x$ is decreasing and positive on the interval $(0,1]$ and so $\cos 1 >0$ . Also, we have $\displaystyle 1 \geq \frac{1}{n^2}, \forall n \in \mathbb{N} \implies \cos 1 \leq \cos(\frac{1}{n^2}), \forall n \in \mathbb{N}$ . So, $\displaystyle a_n = \frac{\cos (\frac{1}{n^2})}{\sqrt{2n+1}} \geq \frac{\cos 1}{\sqrt{2n+1}} = b_n$ . Because, $\displaystyle \sum_{n=1}^{\infty}b_n$ diverges, then by the comparison test $\displaystyle \sum_{n=1}^{\infty}a_n$ diverges too. Thanks for any advices!","['divergent-series', 'convergence-divergence', 'sequences-and-series']"
4761342,Differentiating a pullback along a family of curves,"Say I have a family of curves $x_s : [0,1] \longrightarrow M$ where $s \in (-\epsilon, \epsilon)$ is my family's parameter, and $M$ is a manifold (which, for all purposes being, we can assume to be $\mathbb{R}^n$ ). Additionally, let $\zeta := \dfrac{\mathrm{d}}{\mathrm{d}s}x_s|_{s = 0}$ , which ends up being a vector field along the curve $x$ . (More precisely, it is a function $[0,1] \longrightarrow x^*TM$ with appropriate regularity that I care about, but I don't believe it is necessary to go into such details). I am trying to calculate: $\dfrac{\mathrm{d}}{\mathrm{d}s}|_{s = 0}\displaystyle\int_{0}^1 x_s^{*}\lambda$ Where $\lambda$ is an arbitrary (integrable) $1$ -form defined on my manifold $M$ (which we assume to be $\mathbb{R}^n$ ). The source I am reading makes the following claim: $\boxed{\dfrac{\mathrm{d}}{\mathrm{d}s}|_{s = 0}\displaystyle\int_{0}^1 x_s^{*}\lambda = \displaystyle\int_0^1 x^{*}\mathcal{L}_\zeta\lambda}$ where $\mathcal{L}_\zeta$ denotes the Lie derivative with respect to the vector $\zeta$ defined above. But I'm not entirely sure how to prove it. Here is where my head is at so far: \begin{align}
\dfrac{\mathrm{d}}{\mathrm{d}s}|_{s = 0}\displaystyle\int_{0}^1 x_s^{*}\lambda &= \displaystyle\int_{0}^1 \dfrac{\mathrm{d}}{\mathrm{d}s}|_{s = 0} \, x_s^{*}\lambda \\
&= \displaystyle\int_0^1 \lim\limits_{\varepsilon \to 0} \frac{1}{\varepsilon}(x_\varepsilon^{*}\lambda - x^{*}\lambda) \textit{     (cause } x_0 = x)
\end{align} However, I'm not sure how to formally identify $\lim\limits_{\varepsilon \to 0} \dfrac{1}{\varepsilon}(x_\varepsilon^{*}\lambda - x^{*}\lambda)$ with $x^{*}\mathcal{L}_\zeta \lambda$ . I can see ""philosophically"" why these should be the same ( $\zeta$ is a vector field along the curve $x$ , and it is generated by the ""curve of curves"" $x_s$ . So intuitively, the expression on the left sort of corresponds to a derivative along the flow of $\zeta$ , which is exactly what the Lie derivative is). But then, I'm not sure how to prove it formally because the vector field $\zeta$ is only defined along $x$ , so what does it even mean to take $\mathcal{L}_\zeta \lambda$ ? Do we somehow extend $\zeta$ beyond $x$ in an arbitrary way? And then, since we don't care about this extension,  maybe that's the reason we take the pullback along $x$ , and end up with the expression $x^{*}\mathcal{L}_\zeta\lambda$ ? Any help or progress on the question would be much appreciated. Thank you :)","['lie-derivative', 'functional-analysis', 'differential-forms', 'differential-geometry']"
4761400,Composition series with non isomorphic quotients,"Question Let $1\lhd G_1 \lhd \ldots \lhd G_n=G$ be a composition series of the group $G$ . If for every $i\not= j$ the quotients $G_{i+1}/G_i$ and $G_{j+1}/G_j$ are non isomorphic, then show that every two normal subgroups of $G$ are non isomorphic. Attempt If $H,K$ are two normal subgroups of $G$ , then the series $1\lhd G_1\cap H \lhd \ldots \lhd G_n\cap H=H$ and $1\lhd G_1\cap K \lhd \ldots \lhd G_n\cap K=K$ are two composition series of $H,K$ . I can not see how to combine them in order to show that $H\not\cong K$ . Definition: a composition series of a group $G$ is a series $1\lhd G_1\lhd\ldots \lhd G_n=G$ such that each quotient group $G_{i+1}/G_i$ is simple.","['quotient-group', 'group-theory', 'normal-subgroups']"
4761454,Graph Theory / combinatorics question,"Here a problem from Graph Theory I can not solve. There are $2023$ viewpoints on an island. We know that each viewpoint has line of sight to at least $42$ other viewpoints. You should note that, if a viewpoint $A$ has a line of sight with the viewpoint $B$ , then also $B$ with $A$ . Furthermore, for two different viewpoints $a$ and $b$ , there is a positive integer $n$ and viewpoints $A_1, A_2,\dots , A_{n+1}$ so that $A_1=a$ and $A_{n+1} = b$ and $A_1$ with $A_2$ , $A_2$ with $A_3$ and $\dots A_n$ with $A_{n+1}$ has line of sight. We call the smallest such number $n$ the viewing distance between $a$ and $b$ . The question is to determine the greatest viewing distance that two different viewpoints can have under these conditions. My approach: it seems as if I could be able to solve it with the pigeonhole principle, because I already solved a similar problem, but I do not know how, and it is a problem from Graph Theory.","['graph-theory', 'pigeonhole-principle', 'combinatorics']"
4761505,Similarities in the digits of the powers of 2 and 5,"Many may have noticed that the negative powers of 5 contain the same digits as the positive powers of 2: This pattern intrigued me. I started to wonder if it exists in different number bases. I soon realized that, for a base n, such a pattern seems to exist when you pick a pair of factors of n. As an example, in base 6, the digits will be the same for powers of 2 and 3: For number bases where n is a square number, that means there is a symmetry in the digits for the powers of √n. For example, let's look at the powers of 3 in base 9: And, finally if n has multiple pairs of factors, the pattern works for each pair individually. Let's take base 12, with pairs of factors (2,6) and (3,4): I wonder why this pattern seems to universally emerge in these scenarios - and if the pattern is real at all!
Is there anyone who has an idea on this?","['algebra-precalculus', 'binary', 'number-systems', 'decimal-expansion']"
4761545,Is every finite scheme affine?,"$\def\sF{\mathcal{F}}
\def\sO{\mathcal{O}}$ Today I wondered is every finite scheme affine? and I found a proof using Serre's criterion on affiness ; however, I don't know if I may be using a sledgehammer to crack a nut. My questions are: Is there an easier way of proving this? Does this result show up somewhere in the literature? (I could not find anything on google.) If one deals with finite discrete schemes, then it's easy (one uses that singleton schemes are affine + a finite union of disjoint open affine subschemes is affine), but in the general case I don't know if there's an simpler argument. [EDIT 1/9/23: thanks to Alex Kruckman for spotting the mistake on my proof (see his comment). The argument below (I edited it slightly) now shows ""if $X$ is a finite scheme with $n$ points, then $H^i(X,\mathcal{F})=0$ for all $i\geq n$ and all quasi-coherent sheaves $\mathcal{F}$ over $X$ ."" (This is a weaker form of 0A3G (1); note that $\dim X< n$ .)] Here's how the proof goes: Suppose $X$ is a finite scheme. We proceed by induction on the cardinal $|X|=n$ . For $n=1$ it's trivial, so suppose $n>1$ . If there is $x\subset X$ which has a unique open neighborhood (namely, $X$ ), then we are done (by the vanishing of quasi-coherent sheaves on affines). So suppose there is no such $x$ . Let $\sF$ be a quasi-coherent sheaf of $\sO_X$ -modules. Using the induction hypothesis and Mayer-Vietoris , to show $H^i(X,\sF)=0$ for all $i\geq n$ , it suffices to construct open cover $X=U\cup V$ , where $U,V\neq X$ . For each $x\in X$ , denote $U_x$ to the intersection of all open neighborhoods of $x$ (which is open). We assumed $U_x\neq X$ for all $x\in X$ . One has $$
U_x=\{
y\in X\mid y\rightsquigarrow x
\}
$$ (this formula holds in all topological spaces). Now, let $x_1,\dots,x_r$ be the closed points of $X$ (i.e., the maximal points in the specialization order) and call $U_i=U_{x_i}$ . Then $x_r\notin U_1\cup\cdots\cup U_{r-1}=:V$ . Calling $U=U_r$ , we have that $X=U\cup V$ is the desired open cover. $\square$","['alternative-proof', 'algebraic-geometry', 'schemes', 'reference-request']"
4761575,What can we say about $f$ if $\int_0^1 f(x)p(x)dx=0$ for all polynomials $p$?,"This question was motivated by another question in this site. As explained in that problem (and its answers), if $\displaystyle f$ is continuous on $\displaystyle [0,1]$ and $\displaystyle \int_0^1 f(x)p(x)dx=0$ for all polynomials $\displaystyle p$, then $\displaystyle f$ is zero everywhere. Suppose we remove the restriction that $\displaystyle f$ is continuous. Can we conclude from $\displaystyle f\in L^1([0,1])$ that $\displaystyle f$ is zero almost everywhere? (This should be terribly standard. My apologies, I am rusty of late.)",['real-analysis']
4761585,Example of a vector space with different addition/scalar multiplication operators,"I have learned that a vector space is a set of elements called vectors , on which are defined an addition operation and a scalar multiplication operation with the scalars in some field $F$ . However, all the examples of vector fields I know ( $\mathbb{R}^n$ , $\mathbb{C}^n$ , $\mathbb{F}^{(n, n)}$ , etc) all use the ""normal"" operations of addition and scalar multiplication. As an example, let's consider the set $S = \{(x, y) \in \mathbb{R}^2 \lvert x + y = 10\}$ , for instance. With the intuitive definitions of addition and scalar multiplication ( $(a, b) + (c, d) = (a + c, b + d)$ and $k(a, b) = (ka, kb)$ ), I can verify that this is a vector space, but I can't think of any other definitions of the addition/scalar multiplication operation that still makes this set a vector space. Could someone provide and explain a nonobvious example of addition/scalar multiplication operations that still keeps the set as a vector space?","['vectors', 'linear-algebra', 'vector-spaces']"
4761593,Differentiable function with differentiable inverse must be continuously differentiable?,"I was wondering, if there is a function $f$ that is differentiable everywhere in $\mathbb{R}$ (but not continuously differentiable ), with a differentiable inverse . Since it needs to be bijective and it is continuous, it is w.l.o.g. strictly increasing and thus has nonnegative derivative everywhere. I have tried looking at the standard pathological functions that I know of, but I couldn‘t find any example. If the restriction to invertible functions with differentiable inverse wasn’t there, then there are some good examples of such pathological functions.","['continuity', 'derivatives', 'examples-counterexamples', 'real-analysis']"
4761638,Why pick this definition of measurability over this almost identical one?,"In our measure theory classes, we are going through the construction of measures from outer measures (any $\sigma$ -additive monotonic function on a power set). In that, the following definition of measurability is thrown at us out of the blue: Given an outer measure $\pi^*$ on a set $X$ , a subset $A\subseteq X$ is called measurable iff for every $E\subseteq X$ , we have $$
\pi^*(E) = \pi^*(E\cap A) + \pi^*(E\setminus A)\text.
$$ Now, I could have demanded that for each $E$ , $A$ gets partitioned such that $\pi^*(A) = \pi^*(A\cap E) + \pi^*(A\setminus E)$ . But the above definition instead focuses on the partition of $E$ induced by $A$ . Question: Is there an ""easy"" way to ""see"" why measurability is not defined in this alternate way? Of course, I am not looking for an answer like, ""It makes the proofs work."".",['measure-theory']
4761640,Is there a known explanation for the Feynman point?,"The Feynman point is a mathematical coincidence . It states that from position 762, there are six consecutive nines in the decimal expansion of pi. Some mathematical coincidences have an explanation, like Ramanujan's constant being close to an integer . Is there a known explanation for the Feynman point? Update: the ‘special thing’ about this string of six 9s is that is occurs so early. According to Wikipedia: For a normal number sampled uniformly at random, the probability of a specific sequence of six digits occurring this early in the decimal representation is about 0.08%.
The early string of six 9's is also the first occurrence of four and five consecutive identical digits. If we regard the strings 000000, 111111, until 999999 ‘equally important’, then we should immediately multiply this probability by 10. As with every mathematical coincidence, it could be an ‘actual coincidence’, meaning that there is no ‘explanation’. However, maybe there is in fact an ‘explanation’. This question gives an example of a similar, but more extreme situation. In that case, there is a clear explanation.","['pi', 'probability', 'decimal-expansion']"
4761663,Order statistics of uniform random variables and exponential random variables,"Let $(U_{(1)},\dots,U_{(n)})$ be the first order statistic of $n$ i.i.d. uniform random variables in $(0,1)$ . Let $(E_i)_i$ be $n$ i.i.d. exponential random variables of parameter 1, also independent from everything else. Is it possible to write a formula for $\mathbb{P}(E_1 U_{(i)}\geq E_i U_{(1)}, \forall i\geq2)$ ? Is there a nice formula at least when $n$ goes to infinity?","['statistics', 'order-statistics', 'probability-theory', 'probability', 'random-variables']"
