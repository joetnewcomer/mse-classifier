question_id,title,body,tags
4658613,Evaluate the integral $\int \frac{\cos^2(x)}{1-\beta \sin(x)}\mathrm{d}x$,"Evaluate the integral $\int \frac{\cos^2(x)}{1-\beta \sin(x)}\mathrm{d}x$ Using Wolfram, I get a very complex result. Obviously the integration for $$
\int \frac{\cos^2(x)}{1-\sin(x)} \mathrm{d}x= x-\cos(x) + C
$$ However, when there is a constant $\beta$ that is greater than $1$ in my case, in front of the sine term, it becomes very complex. I'm wondering if there is a better approach to solving this problem. And this is also a definite integral and $x$ is in a certain interval. So my problem is, $$
\int_{x_1}^{x_2} \frac{\cos^2(x)}{1-\beta \sin(x)}\mathrm{d}x
$$ where $\beta > 1 $ .","['integration', 'trigonometric-integrals', 'definite-integrals']"
4658684,"Can we evaluate the integral using Feynman’s, Frullani’s or double integral without Beta functions?","Background When I first met the integral $$I=\int_0^{\infty} \frac{\tan ^{-1}(a x)-\tan ^{-1}(x)}{x} d x,$$ I tried to use Feynman’s Technique by considering the integral with parameter $a$ $$ I(a)=\int_0^{\infty} \frac{\tan ^{-1}(a x)-\tan ^{-1}(x)}{x} d x, \quad \textrm{ where } a>0
$$ Differentiating $I(a)$ gives $$
\begin{aligned}
I^{\prime}(a) & =\int_0^{\infty} \frac{1}{1+a^2 x^2} dx\\
& =\frac{1}{a}\left[\tan ^{-1}(a x)\right]_0^{\infty} \\
& =\frac{\pi}{2 a}
\end{aligned}
$$ Integrating $I^{\prime}(a)  $ from $a=1$ to $e$ gives $$
\begin{aligned}
I(e)-I(1) & =\int_0^e \frac{\pi}{2 x} d x =\frac{\pi}{2}[\ln x]_1^e \\I=I(e)& =\frac{\pi}{2}
\end{aligned}
$$ Then I discovered that $I$ can be also evaluated as a double integral, Frullani’s integral etc., I started to try similarly but failure to evaluate the following general integral $$I(a)=\int_0^{\infty} \frac{\tan ^{-1}(e x)-\tan ^{-1}(x)}{x^a} d x,$$ where $0<a<1.$ Investigation by Beta function In order to convert $I$ into a Beta function, I use integration by parts on $\frac{1}{x^a}$ as below: $$
\begin{aligned}
I(a) & =\frac{1}{-a+1} \int_0^{\infty}\left(\tan ^{-1}(e x)-\tan ^{-1} x\right) d\left(x^{-a+1}\right) \\
& =\frac{1}{-a+1}\left[x^{-a+1}\left(\tan ^{-1}(e x)-\tan ^{-1} x\right)\right]_0^{\infty} -\frac{1}{-a+1} \int_0^{\infty}\left(\frac{x^{-a+1} e}{1+e^2 x^2}-\frac{x^{-a+1}}{1+x^2}\right) d x\\&= \frac{e}{1-a} \int_0^{\infty} \frac{x^{1-a}}{1+e^2 x^2} d x+\frac{1}{1-a} \int_0^{\infty} \frac{x^{1-a}}{1+x^2} d x\\& \stackrel{ex\mapsto x}{=}  -\frac{1}{(1-a) e^{1-a}} \int_0^{\infty} \frac{x^{1-a}}{1+x^2} d x+\frac{1}{1-a} \int_0^{\infty} \frac{x^{1-a}}{1+x^2} d x\\&= \frac{1}{1-a}\left(1-\frac{1}{e^{1-a}}\right) \int_0^{\infty} \frac{x^{1-a}}{1+x^2} d x
\end{aligned}
$$ Using the result in the post ,we have $$
\boxed{I(a)=\frac{\pi(1-e^{a-1})}{2(1-a)}\csc \left(\frac{\pi a}{2}\right)}
$$ For examples, $$
\begin{aligned}
I\left(\frac{1}{2}\right) & =\frac{\pi}{2 \times \frac{1}{2}}  \csc \left(\frac{\pi}{4}\right)\left(1-\frac{1}{\sqrt{e}}\right) =\pi \sqrt{2}\left(1-\frac{1}{\sqrt{e}}\right) \\
I\left(\frac{1}{3}\right) & =\frac{3 \pi}{4} \csc \left(\frac{\pi}{6}\right)\left(1-\frac{1}{e^{\frac{2}{3}}}\right) =\frac{3 \pi}{2}\left(1-\frac{1}{e^{\frac{2}{3}}}\right)
\end{aligned}
$$ Fortunately, the original integral $I=\lim _{a \rightarrow 1} I(a)=\frac{\pi}{2}.$ My Question : Can we evaluate $I(a)$ using Feynman’s, Frullani’s or double integral without Beta functions?","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'beta-function']"
4658708,Integral inequality implies majorization by solution of ODE,"Let $f:[0, \infty)\to [0, \infty)$ be non-decreasing (and not necessarily differentiable nor continuous) and satisfy $$f(t)\leq f(0)+C\int_{0}^{t}f(s)^{2}ds,$$ where $C>0$ . Is it true then that $$f(t)\leq g(t)\quad \text{for all}~t<t_{*},$$ where $t_{*}>0$ and $g$ is a differentiable function on the interval $[0,t_*)$ such that $$g'(t)=Cg(t)^{2}\quad\forall\,0<t<t_*, \quad g(0)=f(0)?$$ Obviously if we would have equality in the above integral inequality and differentiability of $f$ , we would have $f=g$ , but is it still true that $f\leq g$ under these weaker assumptions? My idea is to discretize it (by the monotonicity of $f$ ), so that we have $$f_{n}\leq f_{0}+C\sum_{k=1}^{n}{f^{2}_{k}}$$ and by the assumption on $g$ , $$g_{n}= g_{0}+C\sum_{k=1}^{n}{g^{2}_{k}}, \quad g_{0}=f_{0},$$ and show that $f_{k}\leq g_{k}$ for all $1\leq k\leq n$ but the inductive step does not seem to be working.","['integration', 'ordinary-differential-equations', 'real-analysis']"
4658712,Confusion on the signs involving two square roots of a complex number,"I am confused on dealing with the positive or negative sign when finding the two square roots of a complex number. For example I am solving a similar question to the one here: link In the answer it is written: $$z^{1/2}=\sqrt{9\left(\cos\frac{\pi}{3}+i\sin \frac{\pi}{3}\right)}$$ Then: $$=\color{red}3\left(\cos\left(2k\pi+\frac{\pi}{3}\right)+i\sin \left(2k\pi+\frac{\pi}{3}\right)\right)^{1/2}$$ Wouldn't $\sqrt{9}$ result in $\pm 3$ ?  Why did he take the positive root only? I understand that converting a number into polar form and using De Moivre's formula to find the square root would yield this general formula: $$(r(\cos(\theta)+ i \sin(\theta)))^{1/2} = ±\sqrt{r}(\cos(\theta/2) + i \sin(\theta/2))$$ And that is my source of confusion, I can't apperantly see how to deal with the $\pm$ sign. Considering that in the linked answer it was not there from the first place. I would love to be corrected on what I am misunderstanding.","['algebra-precalculus', 'complex-numbers']"
4658759,"Lognormal Distribution, mean and variance of logarithm of distribution","Let Y be a lognormally distributed random variable with mean $\mu_Y$ and standard deviation $\sigma_Y$ . Assume $\ln(Y)$ is normally distributed with mean $\u$ and variance $\sigma^2$ . According to my source, the mean of the lognormal distribution satisfies $\mu_Y=\ln(\frac{\mu\sigma}{\sqrt{1+w}})$ and variance $\sigma^{2}_Y=\ln(1+w)$ where $w=(\sigma_Y/\mu_Y)^2$ . I am questioning the correctness of this result. I have attempted to derive this formula as follows. We know that if Y is the logarithm of the normal distribution with mean $\mu$ and variance $\sigma^2$ , then the mean of Y is given by (1) $\mu_Y=e^{\mu + \frac{1}{2}\sigma^2}$ , and (2) $\sigma_Y^2=(e^{\sigma^2} -1)\mu_Y^2$ Setting $w$ the same value as above, the second equation implies that $w=e^{\sigma^2} -1$ so that $\sigma^2=\ln(w+1)$ just as claimed above. On the other hand, now that we have $\sigma^2$ , we substitute this value into equation (1) we get $\mu_Y=e^{\mu + \frac{1}{2}\ln(w+1)}$ , which, to my algebra, yields $\mu =\ln(\frac{\mu_Y}{\sqrt{w+1}})$ . Hoping that somebody here can verify my sanity, or explain why the formula for the mean given in my source is correct.","['statistics', 'probability']"
4658786,Convex function with a parameter H,"Let us take $s_1, s_2, t_1, t_2 >0$ such that $ s_1 < t_1 < s_2 < t_2 $ . Let us also assume that $H \in (0 , 1)$ and define $$ a_1 = t_2 - s_1, \quad a_2 = t_2 - t_1, \quad b_1 = s_2 - s_1, \quad b_2 = s_2 -t_1 . $$ We define a function $f$ as $$ f(x) = x^{2H}, \ x \geq 0 . $$ and we study the sign of the expression $$ \frac{1}{2} \bigg( f(a_1) - f(a_2) - \Big( f(b_1) - f(b_2) \Big) \bigg). $$ We would like to show that for $H \in (0, 0.5)$ the sign is negative and for $H \in (0.5, 1 )$ the sign is positive. My approach is the following. In the expression above, we compare two differences of the function $f$ on two intervals $[b_2, \ b_1 ]$ and $[a_2,  a_1 ]$ of the same length, which might intersect, but the second one is shifted to the right. Since $f$ is increasing for all $H$ and for $H \in (0, 0.5)$ $f$ is concave, the change on $[a_2, \ a_1]$ must be smaller that the change on $[b_1, \ b_2]$ . Hence, the inequality. The picture below gives the idea. However, I do not know how to put it together formally. I would appreciate any advice.","['jensen-inequality', 'parametric', 'functions', 'convex-analysis']"
4658800,Proving a combinatorial quantity is uniformly distributed,"Shorter Version. Pick a list $\mathbf{a}$ of length $n$ , taking in values in $\{1,2,\ldots,n+1\}$ at random. Then sort $\mathbf{a}$ in increasing order to obtain $\mathbf{a}' = (a_{(1)}, \ldots, a_{(n)})$ , and count the number of times $a_{(k)} \leq k$ holds. Conjecture: The resulting count takes in values $0, 1, \ldots, n$ equally likely. How do we prove or disprove this conjecture? Longer Version. The setting is as follows: $\mathcal{A}_n = \{1, 2, \ldots, n+1\}^n$ is the collection of all lists of length $n$ taking integer values between $1$ and $n+1$ . For each list $\mathbf{a} \in \mathcal{A}_n $ , we sort $a$ in increasing order to obtain $\mathtt{sort}(\mathbf{a}) = (a_{(k)})_{k=1}^{n}$ . Equivalently, $a_{(k)}$ denotes the $k$ th smallest element in $\mathbf{a}$ . For example, $$ \mathtt{sort}(5, 1, 2, 7, 5, 2) = (1, 2, 2, 5, 5, 7). $$ Then, define the function $\mathtt{below}(\mathbf{a})$ as the number of $k$ 's for which $a_{(k)} \leq k$ holds, that is, $\mathtt{below}(\mathbf{a}) = \sum_{k=1}^{n} \mathbf{1}[a_{(k)} \leq k]$ . For example, $$ \mathtt{below}(5, 1, 2, 7, 5, 2) = \mathtt{below}(\underline{1}, \underline{2}, \underline{2}, 5, \underline{5}, 7) = 4. $$ While playing with this setting with computer software, I noticed some patterns that eventually led me to formulate the following conjecture: Conjecture. Sample $A$ from $\mathcal{A}_n$ uniformly at random. Then $\mathtt{below}(A)$ is uniformly distributed over the set $\{0, 1, \ldots, n\}$ . How do we prove or disprove this? Discussion. For example, if $n = 2$ , then $$ \begin{array}{c}
\mathbf{a} \\ 
(1, 1) \\ (1, 2) \\ (1, 3) \\ (2, 1) \\ (2, 2) \\ (2, 3) \\ (3, 1) \\ (3, 2) \\ (3, 3)
\end{array}
\quad
\longrightarrow
\quad
\begin{array}{c}
\mathtt{sort}(\mathbf{a}) \\ 
(1, 1) \\ (1, 2) \\ (1, 3) \\ (1, 2) \\ (2, 2) \\ (2, 3) \\ (1, 3) \\ (2, 3) \\ (3, 3)
\end{array}
\quad
\longrightarrow
\quad
\begin{array}{c}
\mathtt{below}(\mathbf{a}) \\ 
2\vphantom{,)} \\ 2\vphantom{,)} \\ 1\vphantom{,)} \\ 2\vphantom{,)} \\ 1\vphantom{,)} \\ 0\vphantom{,)} \\ 1\vphantom{,)} \\ 0\vphantom{,)}\\ 0\vphantom{,)}
\end{array}
$$ and hence $\mathtt{below}(A)$ takes values $0, 1, 2$ with equal probabilities. Also, using computer software, I tested this conjecture for $n \leq 7$ . However, brute-force search goes quickly out of hand as $n$ gets larger. And I was unable to come up with a nice explanation of this observation, letting alone a proof. Context. This can be thought of as a discrete analog of the fact that, for the Brownian bridge $(B_t)_{t\in[0,1]}$ joining $(0, 0)$ to $(1, 0)$ , the occupation time $\int_{0}^{1} \mathbf{1}[B_t \geq 0] \, \mathrm{d}t$ is uniformly distributed over $[0, 1]$ . Indeed, a version of invariance principle tells that $$t \mapsto n^{-1/2}(\lfloor nt\rfloor - a_{(\lfloor nt\rfloor)})$$ tends to the Brownian bridge as $n \to \infty$ in an appropriate sense. So, it is reasonable to expect that $\mathtt{below}(A)$ is approximately uniformly distributed. What is quite surprising is that $\mathtt{below}(A)$ seems exactly uniformly distributed.","['combinatorics', 'probability', 'uniform-distribution']"
4658818,Searching for a bound for the integral of a 1-form along a loop.,"Consider the submanifold $M$ of $\mathbb R^8$ , with coordinates $(x_1,y_1,x_2,y_2,x_3,y_3,x_4,y_4)$ , defined by the following equations $$x_1^2+y_1^2+x_2^2+y_2^2=1,$$ $$x_3^2+y_3^2+x_4^2+y_4^2=1,$$ $$x_1x_2+y_1y_2+x_3x_4+y_3y_4 = 0.$$ Consider a loop (closed curve) $\gamma$ and assume that $\gamma$ is contractible in $M$ . What I want to prove is the following: The integral $$\int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4$$ is bounded by a constant not dependent on $\gamma$ . I believe that such an integral is bounded by $\pi$ . I am not sure if such a constant exists. This integral appeared in a research problem where I was computing the holonomy of certain vector bundles, where it does seem that such a bound exists. I also believe there is some trick using symplectic geometry to find this bound. If $\Gamma$ is a disc with boundary $\gamma$ , then $$\int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4 =\int_\Gamma dx_1dx_2+dy_1dy_2+dx_3dx_4+dy_3dy_4,$$ by the Stokes theorem, the integral of a symplectic form. Any ideas are welcome. Thank you in advance.","['symplectic-geometry', 'submanifold', 'multivariable-calculus', 'stokes-theorem', 'differential-forms']"
4658823,Does matrix multiplication require an inner product space?,"Does matrix multiplication require an inner product space? It would seem to me that since multiplying a vector by a matrix is simply a linear map, and multiplying two matrices the composition of a linear maps, these must be defined in any vector space, and should not depend on an inner product. Yet, any definition of matrix multiplication I can consider immediately induces an inner product.  I understand that you can multiply a vector by a matrix without using an inner product, but, once you do so, you immediately define an inner product. So: Can matrix multiplication exist in vector spaces without inner products? What is the relationship between the two?","['inner-products', 'vector-spaces', 'matrices', 'linear-algebra', 'linear-transformations']"
4658853,Is the L2 norm positive definite?,"I am learning the concept of $L^2_w[a,b]$ space - space of square-integrable functions on the interval $[a,b]$ - in the context of Hilbert spaces. After struggling to show that the space of continuous functions is not complete, I came to wonder if the $L^2_w$ norm is in fact a well-defined norm. The definition of a norm, by my understanding, is any function from a vector to a scalar satisfying: $|| a || \ge 0$ with $||a||=0$ implying $a=\mathbb{0}$ , $||\lambda a|| = |\lambda| \cdot ||a||$ , and $||a+b|| \le ||a|| + ||b||$ . My problem is with the first axiom. It feels like there are plenty of functions in $L^2_w[a,b]$ that are nonzero yet have zero norm. One such example is $f(x)=\begin{cases} 0 & (x \ne \frac{a+b}{2}) \\ 1 & (x = \frac{a+b}{2}) \end{cases}$ . These examples seem terrifying to me since, in case of Cauchy sequences in such space, $\lim_{n \rightarrow \infty}{||f_n-f||=0}$ might not imply $\lim_{n\rightarrow\infty}f_n = f$ . Am I missing something or understanding something horribly wrong?
Thank you.","['normed-spaces', 'functional-analysis']"
4658956,Spoons card game probability problem,"We have 12 cards with four different suits: hearts, diamonds, spades, and clovers. Each suit has three cards with numbers from 1 to 3. We randomly deal these cards to three people, giving each person four cards. What is the probability that at least one person has four of a kind? I came across this probability problem while playing 'Spoons' with my friend. I tried to solve the problem and got the results, but I couldn't check if this is the right answer, so I'm asking here. My answer is $$\frac{3\cdot 3{}\cdot \left({\binom{8}{4}-2}\right)+6}{\binom{12}{4}\cdot \binom{8}{4}}$$","['card-games', 'probability']"
4658959,Algebraic varieties fulfilled by solutions of polynomial ODEs,"Let's assume we have a two dimensional polynomial vector field of degree $d$ $$F: \mathbb{R}^{2}\rightarrow\mathbb{R}^{2}, \quad (x,y)\mapsto \begin{pmatrix}P(x,y), \\ Q(x,y)\end{pmatrix}$$ and we are given a solution $\gamma: I\subset \mathbb{R} \rightarrow \mathbb{R}^2$ . Let's define a polynomial of the same degree $d$ $$G:\mathbb{R}^{2}\rightarrow\mathbb{R}, \quad (x,y)\mapsto G(x,y).$$ but which can have completely different coefficients and also other monomials. I.e. $x^5 \cdot y$ could be a monomial in $G$ which is not a monomial of $F$ . Do we know if there exists an initial condition $x_0$ such that the solution curve $\gamma(t,x_0)$ fulfills $$G(\gamma_x(t,x_0),\gamma_y(t,x_0)=0 \quad \forall t?$$ As an example, given $F=(-y,x)$ and $x_0=(1,0)$ , the solution fulfills $$G(x,y)=x^2+y^2-1=0$$ But $G$ is a degree higher than $F$ . But that is the only example I currently was able to come up. I am interested in any result and any new example of this kind.","['real-algebraic-geometry', 'ordinary-differential-equations', 'real-analysis']"
4658963,high-water mark distribution,"Given a sequence of values ${a_k}_{k=1}^n$, the high-water marks are the values at which the running maximum increases. For example, given a sequence $(3,5,7,8,8,5,7,9,2,5)$ with running maxima $(3,5,7,8,8,8,9,9,9)$, the high-water marks are $(3,5,7,8,9)$, which occur at $k=1, 2, 3, 4, 8$. For every sequence $a_k$ there is a number of high-water marks $N_{a_k}$ Consider a set $\sigma$ of all permutations of $n$ numbers $(1, \dots, n)$. 
Does anybody knows the analytical expression for the distribution of the  number of high-water marks ($N_{a\in \sigma}$)? For example, if $n=3$ $1,2,3 \rightarrow 3$ $1,3,2 \rightarrow 2$ $2,1,3 \rightarrow 2$ $2,3,1 \rightarrow 2$ $3,1,2 \rightarrow 1$ $3,2,1 \rightarrow 1$ and the distribution is $\left( \frac{2}{3!},\frac{3}{3!},\frac{1}{3!} \right)$. Any references would be appreciated.","['permutations', 'probability-distributions', 'reference-request']"
4658970,Intuition of $D\leftarrow XC^{T}\text{diag}(C1_n)^{-1}$ update rule in matrix factorization,"I am reading this paper where they use Matrix Factorization over Attention mechanism in their Hamburger model. In section 2.2.2 they say, Vector Quantization (VQ) (Gray & Neuhoff, 1998), a classic data compression algorithm, can be formulated as an optimization problem in term of matrix decomposition: $$
\min _{\boldsymbol{D}, \boldsymbol{C}}\|\boldsymbol{X}-\boldsymbol{D} \boldsymbol{C}\|_F \quad \text { s.t. } \mathbf{c}_i \in\left\{\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_r\right\}\tag1
$$ where $e_i$ is the canonical basis vector, $\mathbf{e}_i=[0, \cdots, 1, \cdots, 0]^{\top}$ . The solution to minimize the objective in Eq. (1) is K-means (Gray & Neuhoff, 1998). However, to ensure that VQ is differentiable, we replace the hard arg min and Euclidean distance with softmax and cosine similarity, leading to Alg. 1, where $\operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X})$ is a similarity matrix whose entries satisfy $\operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X})_{i j}=\frac{\mathbf{d}_i^{\top} \mathbf{x}_j}{\|\mathbf{d}\|\|\mathbf{x}\|}$ , and softmax is applied column-wise and $T$ is the temperature. Further, we can obtain a hard assignment by a one-hot vector when $T \rightarrow 0$ . I didn't understand the last line, ""...and softmax is applied column-wise and $T$ is the temperature. Further, we can obtain a hard assignment by a one-hot vector when $T \rightarrow 0$ ."" What they mean by $T$ (Temperature) here? In fact, I couldn't get the justification for the replacement of $\arg \min$ with softmax here. And what is their update rule doing for $D\leftarrow XC^{T}\text{diag}(C1_n)^{-1}$ And it seems like their VQ is similar to traditional non-negative matrix factorization with regularization on $C$ . Or am I confused between these two? Thanks in advance. update I remove some questions which might be not going with M.SE rules. And convert the thread with a single math-based question only. Hope it will get better reach now.","['matrices', 'optimization', 'nonnegative-matrices', 'matrix-decomposition']"
4658980,Proof Verification: $2^n+1 \leq 3^n$,"Claim: $$\forall n \in \mathbb{N}, 2^n+1 \leq 3^n $$ Proof (Induction): Base: Let $$n = 1, 2^1 + 1 \leq 3$$ so this is true. Inductive Step: Suppose $$n \geq 1$$ and assume inductive hypothesis holds for all values less than or equal to n. $$2^{n+1} + 1 = 2*2^n + 1$$ recall $$2^n \leq 3^n - 1$$ So, $$2(2^n) + 1 \leq 2(3^n -1) + 1$$ Now, we want to show $$\forall n \in \mathbb{N}, 2(3^n) - 1 \leq 3^{n+1}$$ This inequality becomes $$-1 \leq 3^n$$ which is obviously true $\forall n \in \mathbb{N}$ . Therefore, $$2^{n+1} + 1 \leq 2(3^n - 1) + 1 \leq 3^{n+1} \implies \forall n \in \mathbb{N}, 2^{n+1}+1 \leq 3^{n+1}$$ by induction. This is one of the first inequality induction proofs I have done, and usually in an inductive proof, you construct the right hand side using the left hand side. I did not do this here, I just pulled the $3^{n+1}$ out of nowhere and tested if the simpler inequality $2(3^n) - 1$ was always less than or equal to $3^{n+1}$ . Is this rigorous enough? Thanks.","['proof-writing', 'solution-verification', 'induction', 'discrete-mathematics']"
4659023,Why is the decomposition of a function into odd and even parts interesting?,"For all functions $f:\mathbb{R}\to\mathbb{R}$ one can find a unique decomposition $f(x)=E(x)+O(x)$ where $E(-x)=E(x)$ and $O(-x)=-O(x)$ . Is there any branch of mathematics where analysing the decomposition of a function into its odd and even parts has an important role? The even-odd decomposition of $e^x$ is $\cosh(x)+\sinh(x)$ , and $e^{ix}$ has a decomposition into even and odd parts $\cos(x)+i\sin(x)$ . Are there other well-known functions whose decompositions are other well-known functions? My first question has been answered already here in detail, but it doesn't provide any answers for the second half of my question, that is examples of well-known functions with an interesting decomposition.","['even-and-odd-functions', 'functions', 'soft-question']"
4659055,Products of antiharmonic forms (or functions) with harmonic forms,"Let $X$ be a compact Kähler manifold, with fixed Kähler form $\Omega$ .
Then, the wedge product of two harmonic forms is not necessarily harmonic, as explained for instance here .
This prompts the question of whether the product of a $\mathcal{C}^\infty$ -antiharmonic form $\alpha \in \mathcal{A}^{p,q}(X)$ with a harmonic form $\beta \in \mathcal{H}^{r,s}(X)$ is antiharmonic.
Here, a $\mathcal{C}^\infty$ -differential form $\alpha \in \mathcal{A}^{p,q}(X)$ is said to be antiharmonic if $\alpha$ is orthogonal to any harmonic form, with respect to the Hodge inner product $(\alpha,\beta) := \int_X \alpha \wedge (\ast \beta)$ , where $\ast$ denotes the Hodge $\ast$ -operator. In particular, thanks to the Hodge decomposition, we know that the group of antiharmonic forms coincides with $\mathrm{Im}(d) \oplus \mathrm{Im}(d^\ast)$ , where $d$ is the usual exterior differential, and $d^\ast$ is its adjoint with respect to the $\ast$ -operator. Such a statement is clearly true if $\alpha = d(\gamma)$ , since the product of a $d$ -exact form with a $d$ -closed one is again exact, thanks to Leibniz's rule. Therefore, by the Hodge decomposition the only case left to consider is $\alpha = d^\ast(\gamma)$ . Is it reasonable to expect that $d^\ast(\gamma) \beta \subseteq \mathrm{Im}(d) \oplus \mathrm{Im}(d^\ast)$ , or is this not true? I would be particularly interested in this property when $p = q = 0$ , i.e. when $\alpha = f$ is a function such that $\int_X f \cdot \Omega^n = 0$ , where $\Omega \in \mathcal{A}^{1,1}(X)$ is the Kähler form associated to the Kähler metric on $X$ . Analogously, one can say that $f = \Delta(g)$ where $g = G(f)$ and $G$ is Green's operator. Therefore, the previous question amounts to ask whether $\int_X \Delta(g) \beta_1 \wedge (\ast \beta_2) = 0$ for every smooth function $g$ and every pair of harmonic forms $\beta_1,\beta_2 \in \mathcal{H}^{p,q}(X)$ , where $\ast$ denotes the Hodge $\ast$ -operator. Finally, note that this question is intimately related to the behaviour of the orthogonal projection $H \colon \mathcal{A}^{p,q}(X) \to \mathcal{H}^{p,q}(X)$ . More precisely, is it true that $H$ is a map of $\mathcal{C}^\infty(X)$ -modules, i.e. that $H(f \alpha) = H(f) H(\alpha)$ for every smooth function $f$ and every $\alpha \in \mathcal{A}^{p,q}(X)$ ? Note that $H(f)$ is a constant, so this property might be too much to be true. Moreover, note that an analogous statement is not true in general if $f$ is replaced by a differential form of higher degree, unless we are in a situation where the wedge product of two harmonic differential forms is harmonic.","['hodge-theory', 'complex-geometry', 'kahler-manifolds', 'differential-geometry']"
4659068,"""Simpler"" proof of general Nullstellensatz for Jacobson rings.","Assume all rings/algebras are commutative with identity. "" $\hookrightarrow$ "" means injection and "" $\twoheadrightarrow$ "" means surjection. $A_f$ is the localization of $A$ on the point $f$ . We slightly abuse $\mathfrak p\cap R$ to denote the retraction $f^{-1}(\mathfrak p)$ of the prime ideal $\mathfrak p\subset S$ in the ring map $f:R\to S$ . $A$ is a Jacobson ring if every prime ideal is the intersection of the maximal ideals containing it. The word ""Simple"" refers to that we can apply the standard techniques (see ""Proof of $(\color{RoyalBlue}2)$ "") used to prove the Nullstellensatz for fields. The key is to prove $\operatorname{Im} (A/\mathfrak p\to (A/\mathfrak p)_f/\mathfrak m)$ is a field and construct a maximal ideal as $\ker (A/\mathfrak p\to (A/\mathfrak p)_f/\mathfrak m)$ in $A/\mathfrak p$ in the arrow compositions below: $$k\to A/\mathfrak p\hookrightarrow (A/\mathfrak p)_f\twoheadrightarrow (A/\mathfrak p)_f/\mathfrak m$$ Here is the form of Nullstellensatz I would like to prove If $R$ is a Jacobson ring, then so is any finite generated $R$ -algebra $A$ . Here is my proof: Assume $A$ is not Jacobson and choose a prime ideal $\mathfrak p\subset A$ and an element $f$ such that $f\notin \mathfrak p$ but $f$ is contained in all maximal ideals containing $\mathfrak p$ . We have composition arrows $$R/(\mathfrak p\cap R)\to A/\mathfrak p\hookrightarrow (A/\mathfrak p)_f\twoheadrightarrow(A/\mathfrak p)_f/\mathfrak m_{\mathfrak p}$$ where $\mathfrak m_{\mathfrak p}$ is a maximal ideal of $(A/\mathfrak p)_f$ . Inspired by the classic Nullstellensatz for fields, we will need $\mathfrak p\cap R$ to be large enough to make $R/(\mathfrak p\cap R)$ a field. To be precise, we make a hypothesis and assume it is true. Hypothesis. There is a prime ideal $\mathfrak q\supset \mathfrak p$ such that $f\notin \mathfrak q$ and $\mathfrak q\cap R$ is maximal in $R$ . Then looking at the composition arrows again for $\mathfrak q$ : $$R/(\mathfrak q\cap R)\to A/\mathfrak q\hookrightarrow (A/\mathfrak q)_f\twoheadrightarrow(A/\mathfrak q)_f/\mathfrak m_{\mathfrak q}$$ $(A/\mathfrak q)_f/\mathfrak m$ is a finite field extension of the field $R/(\mathfrak q\cap R)$ . As an intermediate extension, the image of $A/\mathfrak q$ in $(A/\mathfrak q)_f/\mathfrak m$ is also finite over $R/(\mathfrak q\cap R)$ , and must then be a field. Thus $\ker(A/\mathfrak q\to (A/\mathfrak q)_f/\mathfrak m)$ is a maximal ideal in $A/\mathfrak q$ , and corresponds to a maximal ideal in $A$ containing $\mathfrak p$ but not containing $f$ , contradiction. The hypothesis is true, for example this gives a sharper result, showing $(A/\mathfrak q)_f$ is a field. And then use the fact that $R/(\mathfrak q\cap R)\hookrightarrow (A/\mathfrak q)_f$ is a finite type extension from a Jacobson ring to a field to deduce $R/(\mathfrak q\cap R)$ is a field, as shown here . Is there a easier way through to prove the hypothesis above without showing $(A/\mathfrak q)_f$ is a field, but rather studying the prime/maximal correspondence between $R\to A$ ? Update: I am looking at a particularly interesting result which will prove our hypothesis: If $R$ is Jacobson, then for any finite type ring map $f: R\to A$ , $f^{-1}(\mathfrak m)$ is maximal in $R$ if $\mathfrak m$ is maximal in $A$ . To prove our hypothesis using this result, form the composition ring map $R\to A\to (A/\mathfrak p)_f$ , choose a maximal ideal of $(A/\mathfrak p)_f$ and look at the corresponding prime ideal in $A$ and in $R$ . Update: The converse of the result is also true, and strongly relies on this result. I believe that is why stacks project uses the sharper result to prove the Nullstellensatz for Jacobson rings. $R$ is Jacobson if and only if for any finite type ring map $R\to A$ , inverse image of maximal ideals are maximal as well.","['maximal-and-prime-ideals', 'algebraic-geometry', 'abstract-algebra', 'ideals', 'commutative-algebra']"
4659077,A problem on compositions of functions,"$$f(x) = \begin{cases} x-1 & x \geq 0 \\ 1-x & x <0 \end{cases}$$ $$g(x) = \begin{cases} x& x\geq 0 \\ x^2 & x < 0 \end{cases}$$ It asks me for the compositions $f(g(x))$ . What I did: it seemed easy to me, since $$f(g(x)) = \begin{cases} x-1 & x \geq 0 \\ 1-x^2 & x < 0 \end{cases}$$ $$g(f(x)) = \begin{cases} x-1 & x \geq 0 \\ (1-x)^2 & x < 0 \end{cases}$$ Instead in the solution there is: $$g(f(x)) = \begin{cases} 1-x & x < 0 \\ (x-1)^2 & 0 \leq x < 1 \\ x-1 & x \geq 1 \end{cases}$$ I don't get that solution. Some clarifications?","['elementary-functions', 'calculus', 'analysis']"
4659086,"If $f$ is a smooth real valued function on real line such that $f'(0)=1$ and $|f^{(n)} (x)|$ is uniformly bounded by $1$ , then $f(x)=\sin x$?","Let $f : \mathbb R \to \mathbb R$ be a smooth ( infinitely differentiable everywhere ) function such that $f '(0)=1$ and $|f^{(n)} (x)| \le 1 , \forall x \in \mathbb R , \forall n \ge 0$ ( as usual denoting $f^{(0)}(x):=f(x)$) ; then is it true that $f(x)=\sin x , \forall x \in \mathbb R$ ?","['complex-analysis', 'calculus', 'harmonic-analysis', 'real-analysis']"
4659115,"How to calculate the expected correlation between N normally distributed variables which are all coupled by the same correlation, given a sample set?","I have N normally distributed variables, each with the same known standard deviation and mean. They are not independant, and are each correlated with the other with a Pearson's correlation coefficient of P, which we do not know. If we take a single sample of each of the N variables, I would like to derive a MLE of the underlying correlation P. It stands to reason that we should be able to etimate the value P, given the clustering of the sampled values and how close they are to the sample mean (a high value of P would result in a high level of clustering etc). I imagine some form of MLE would work here, but I'm coming a little unstuck on how to derive a PDF. My current train of thought would be to derive a PDF for the RMSE of each of the N samples relative to the mean sample value (which should be highly dependant on P as stated), and interpret this as a function of P and then derive the MLE formualtion etc but this seems like it would be an incredibly hard PDF to derive. Is there something I'm missing? Is there a better way of doing this?","['statistical-inference', 'statistics']"
4659131,Why $24-504x+4416x^2+24(-1-2x)(1-x-x^2)^{23}\ge 0$ for $x\ge 0$,"How can I rigorously prove that $$24-504x+4416x^2+24(-1-2x)(1-x-x^2)^{23}\ge 0$$ for all real $x\ge 0$ ? I tried using root finding algorithms but I can't come up with a rigorous proof. But I noticed that at $x=0$ , the polynomial is exactly zero.","['algebra-precalculus', 'polynomials', 'inequality', 'real-analysis']"
4659162,$\lim_{n\to\infty} \left(\frac{e}{n}\right)^n \int_0^n |x(x-1)(x-2)\dots(x-n)|dx$,"What is $$
\lim_{n\to\infty} \left(\frac{e}{n}\right)^n 
\int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right| \, dx?
$$ Context: I was trying to find an asymptotic expression for the total area of the regions enclosed by $y = x(x-1)(x-2)(x-3)\cdots(x-n)$ and the $x$ -axis. Through trial and error, it seems that $$
\int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right|\, dx 
\approx 2\left(\frac{n}{e}\right)^n
$$ for large $n$ , but I don't know how to prove this.","['integration', 'asymptotics', 'real-analysis', 'gamma-function', 'calculus']"
4659166,Chain rule mismatch,"Let $$
g\bigl(h(t)\bigr) = \cos(\sqrt{t})
$$ and $$
g(t) = \cos(t) 
$$ and $$
h(t) = \sqrt{t}.
$$ Verify that $$
\frac{dg}{dh} = \frac{\;\frac{dg}{dt}\;}{\frac{dh}{dt}}.
$$ I tried doing this but I got $$
\frac{dg}{dh} = -\sin(\sqrt{t}) \cdot \frac{1}{2\sqrt{t}}
$$ but that is not equal to $$
\frac{\frac{dg}{dt}}{\frac{dh}{dt}} = -2\sin(t) \cdot \sqrt{t}
$$ Where did I make a mistake?
Edit: I see the mistake but I tried another method and I don't know why that is wrong. $$\frac{dg}{dh}=-sin\left(h\right)\cdot \frac{dh}{dh}=-sin\left(\sqrt{t}\right)$$","['calculus', 'derivatives', 'chain-rule']"
4659200,"Lebesgue measurable function in $\mathbb{R}^d$ is almost everywhere pointwise limit of step functions (Stein & Shakarchi, Real Analysis)","Please see this SE post for a full quote of Theorem 4.3, which, in summary, states that a measurable function is an almost everywhere pointwise limit of a sequence of step functions. I have a question about this excerpt of the theorem: To this end, we recall part (iv) of Theorem 3.4, which states that for every $\epsilon$ there exist cubes $Q_1, \ldots, Q_N$ such that $m(E \Delta \bigcup_{j=1}^N Q_j) \leq \epsilon$ . By considering the grid formed by extending the sides of these cubes , we see that there exist almost disjoint rectangles $\tilde R_1, \ldots, \tilde R_M$ such that $\bigcup_{j=1}^N Q_j = \bigcup_{j=1}^M \tilde R_j$ . By taking rectangles $R_j$ contained in $\tilde R_j$ , and slightly smaller in size, we find a collection of disjoint rectangles that satisfy $m(E \Delta \bigcup_{j=1}^M R_j) \leq 2 \epsilon$ . For context, this idea of ""extending the sides of these cubes"" comes from the proof of Lemma 1.1 in the same textbook: (Lemma 1.1) If a rectangle is the almost disjoint union of finitely many other rectangles, say $R = \bigcup_{k=1}^N R_k$ , then $|R| = \sum_{k=1}^N |R_k|$ . The relevant part of the proof is: We consider the grid formed by extending indefinitely the sides of all rectangles $R_1, \ldots, R_N$ . This construction yields finitely many rectangles $\tilde R_1, \ldots, \tilde R_M$ , and a partition $J_1, \ldots, J_N$ of the integers between 1 and $M$ , such that the unions $R = \bigcup_{j=1}^M \tilde R_j$ and $R_k = \bigcup_{j \in J_k} \tilde R_j$ , for $k = 1,\ldots, N$ , are almost disjoint. My issue is: While I understand this grid extension idea, it seems a bit overkill. In theorem 4.3, all we care about is that we can take a slightly smaller subset of the almost disjoint union of closed cubes so that we have a strictly disjoint union of closed cubes. But there are much simpler ways to achieve this compared to the rectangular grid extension. For example, because we are starting from an almost disjoint union of cubes, then we can just shrink each side of cube $Q_j$ ""towards its midpoint,"" so that the boundary of every cube no longer intersects. E.g., in 1-D, $[2, 3]$ and $[3, 4]$ are almost disjoint cubes. Suppose I want to decrease the total volume by $\epsilon = 0.4$ . If I scale each side length by $1 - \epsilon/2 = 0.8$ ""towards its midpoint"" (so, now the cubes are $[2.1, 2.9]$ and $[3.1, 3.9]$ ), then the total volume is $(1 - \epsilon/2) * 2 = 2 - \epsilon$ , which is exactly the desired reduction in volume. This shrinkage extends to $\mathbb{R}^d$ because we know that cube $Q_j$ has sides defined by $[a^{j}_i, b^{j}_i], i = 1,\ldots, d$ , with side length $s_j := b^{j}_i - a^{j}_i > 0$ , so there always exists some scaling factor $c_j > 0$ so that the resulting closed cubes are well-defined (i.e., non-empty interior and strictly disjoint). In summary, why bother with the grid extension idea? We kind of get for free an easy way to get a strictly disjoint union of closed cubes with a precise reduction in volume, since the closed cubes are almost disjoint to begin with. Is my thinking correct, or is the grid extension idea necessary to conclude the proof of Theorem 4.3 ? Thanks","['measure-theory', 'real-analysis']"
4659203,Well-definedness of a certain function defined on the rationals,"Let $A$ be an irrational, countable dense subset of $[0,1]$ , e.g, $A=\{q\sqrt 2|q\in\mathbb Q\} \cap[0,1] $ . Let $\{a_i\}_{i\in \mathbb N} =A$ be an enumeration of $A$ . Let $f:\mathbb Q\cap[0,1] \to \mathbb R$ be defined by: $$f(x)=\sum_{i\in\mathbb N} \frac{\varepsilon_i}{|x-a_i|}.$$ Can the sequence $\{\varepsilon_i\}_{i\in\mathbb N}$ be chosen such that $f$ is well-defined at every point, i.e., the sum is finite? I came up with this example in trying to construct a continuous function on a dense subset of the unit interval that has no continuous (domain) extension to any neighbourhood (in the unit  interval) of any point, but I don’t know how to make to ensure it is well-defined","['analysis', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'general-topology']"
4659251,Why do cross products not change their sign under an inversion of coordinates?,"I always understood that the direction of a cross product was determined by the right-hand rule regardless of the handedness of the coordinate system we're working in. The only way a cross product wouldn't change its sign under a coordinate inversion is if we were to assume a left-hand rule for computing cross products in a left-handed coordinate system. But if we were to do this then we would have to specifically mention that the right-hand rule only holds true in a right-handed coordinate system (which almost no one ever does). Also, this means that the directions of quantities like angular momentum depend upon the coordinate system being used which just seems wrong. Also also, if we were to use a left-hand rule for left-handed coordinate systems, $\hat{i} \times \hat{j} = \hat{k}$ even in a left-handed coordinate system which is incorrect. This question was prompted by Problem 1.10 from Griffiths and its solution .","['coordinate-systems', 'electromagnetism', 'linear-algebra']"
4659256,Kernel and Differential Operator,"I hope your day is great so far.
I have a question. I am given a Cauchy-Euler ODE $$\big((x+2)^3\mathbf{D}^3+4(x+2)^2\mathbf{D}^2+3(x+2)\mathbf{D}+1\big)y=\frac{1}{x+2},$$ where $\mathbf{D}^n=d^n/dx^n$ . Letting $e^t=x+2$ transforms this ODE, such that the coefficients are constants, giving a new ODE $$(\mathsf{D}^3+\mathsf{D}^2+\mathsf{D}+1)y=e^{-t},$$ where $\mathsf{D}^n=d^n/dt^n$ . I can't use the formula $$\frac{1}{p(\mathsf{D})}[e^{at}]=\frac{1}{p(a)}e^{at},p(a)\neq0,$$ since $p(a)=0$ . How can I find the particular solution then? I actually find that we can use $$y_p=\frac{1}{p_{n-k}(a)}\bigg(\frac{1}{k!}t^k+\text{ker}\,\mathsf{D}^k\bigg)e^{at},$$ for this case. However, I don't understand this formula and was hoping someone could explain that. Thanks in advance!","['differential-operators', 'linear-algebra', 'ordinary-differential-equations']"
4659267,"Exercise 19 on p.39 in Exercises 2B in ""Measure, Integration & Real Analysis"" by Sheldon Axler. Is my solution ok? How to simplify these conditions?","I am reading ""Measure, Integration & Real Analysis"" by Sheldon Axler. The following exercise is Exercise 19 on p.39 in Exercises 2B in this book. Exercise 19 Suppose $X$ is a nonempty set and $\mathcal{S}$ is the $\sigma$ -algebra on $X$ consisting of all subsets of $X$ that are either countable or have a countable complement in $X$ . Give a characterization of the $\mathcal{S}$ -measurable real-valued functions on $X$ . My solution is here: Let $f$ be an $\mathcal{S}$ -measurable real-valued function on $X.$ Let $A:=\{a\in\mathbb{R}:f^{-1}((a,\infty))\text{ is countable}\}.$ We prove that $A$ is not empty. Assume that $A=\emptyset.$ Then, $f^{-1}((a,\infty))$ is uncountable for any $a\in\mathbb{R}.$ Then, $X\setminus f^{-1}((a,\infty))=f^{-1}((-\infty,a])$ is countable for any $a\in\mathbb{R}.$ Since $X=f^{-1}(\mathbb{R})=f^{-1}(\bigcup_{n\in\mathbb{Z}} (-\infty,n])=\bigcup_{n\in\mathbb{Z}}f^{-1}((-\infty,n])$ , $X$ is countable. Let $a\in\mathbb{R}.$ Since $f^{-1}((a,\infty))\subset X$ , $f^{-1}((a,\infty))$ is countable. This is a contradiction. So, $A$ is not empty. Let $a_1\in A.$ Let $a_2$ be any element of $\mathbb{R}$ such that $a_1<a_2.$ Then, $f^{-1}((a_2,\infty))\subset f^{-1}((a_1,\infty)).$ Since $f^{-1}((a_1,\infty))$ is countable, $f^{-1}((a_2,\infty))$ is countable. So, if $a_1\in A$ , then $a_2\in A$ for any $a_2\in\mathbb{R}$ such that $a_1<a_2.$ So, if $A$ is not bounded below, $A=\mathbb{R},$ if $A$ is bounded below, $A=(b,\infty)$ or $A=[b,\infty)$ for some $b\in\mathbb{R}.$ First, we consider the case $A=(b,\infty)$ for some $b\in\mathbb{R}.$ $f^{-1}((b,\infty))$ is uncountable since $b\notin A.$ So, $f^{-1}((-\infty,b])=X\setminus f^{-1}((b,\infty))$ is countable. Let $\epsilon$ be any positive real number. Then, $f^{-1}((b+\epsilon,\infty))$ is countable since $b+\epsilon\in A.$ Therefore, $f^{-1}((b,b+\epsilon])=f^{-1}((b,\infty))\setminus f^{-1}((b+\epsilon,\infty))$ is uncountable. Next, we consider the case $A=[b,\infty)$ for some $b\in\mathbb{R}.$ $f^{-1}((b,\infty))$ is countable since $b\in A.$ Let $\epsilon$ be any positive real number. $f^{-1}((b-\epsilon,\infty))$ is uncountable since $b-\epsilon\notin A.$ $f^{-1}((-\infty,b-\epsilon])=X\setminus f^{-1}((b-\epsilon,\infty))$ is countable. $f^{-1}((b-\epsilon,b])=f^{-1}((b-\epsilon,\infty))\setminus f^{-1}((b,\infty))$ is uncountable. Conversely, let $f$ be a real-valued function on $X$ which satisfies one of the following conditions: $f^{-1}((a,\infty))$ is countable for any $a\in\mathbb{R}.$ There exists $b\in\mathbb{R}$ such that for any positive real number $\epsilon$ , $f^{-1}((b+\epsilon,\infty))$ is countable  and $f^{-1}((b,b+\epsilon])$ is uncountable and $f^{-1}((-\infty,b])$ is countable. There exists $b\in\mathbb{R}$ such that for any positive real number $\epsilon$ , $f^{-1}((b,\infty))$ is countable  and $f^{-1}((b-\epsilon,b])$ is uncountable and $f^{-1}((-\infty,b-\epsilon])$ is countable. If $f$ satisfies 1, then $f$ is an $\mathcal{S}$ -measurable real-valued function on $X.$ Suppose that $f$ satisfies 2. Then, $f^{-1}((c,\infty))$ is countable if $b<c$ since there exists a positive real number $\epsilon$ such that $b+\epsilon<c.$ Then, $X\setminus f^{-1}((c,\infty))=f^{-1}((-\infty,c])$ is countable if $c\leq b.$ So, $f$ is an $\mathcal{S}$ -measurable real-valued function on $X.$ Suppose that $f$ satisfies 3. Then, $f^{-1}((c,\infty))$ is countable if $b\leq c.$ Then, $X\setminus f^{-1}((c,\infty))=f^{-1}((-\infty,c])$ is countable if $c<b$ since there exists a positive real number $\epsilon$ such that $c<b-\epsilon.$ So, $f$ is an $\mathcal{S}$ -measurable real-valued function on $X.$ Is my solution ok? Even if my solution is ok, the conditions 1, 2 and 3 are not simple. How to improve this?","['measure-theory', 'solution-verification', 'real-analysis']"
4659278,Integration a zero valued function product other function,"Suppose I have a function $f(x) = 0$ for all $x$ except $0$ . Now, $f(x) g(x)$ will also be zero for all $x$ except $0$ (or may be at $0$ ). Now, if try to find the value of $\int_{-\infty}^{\infty} f(x)g(x) \, \mathrm{d}x$ , since this will give area of the graph, then I think it must be $0$ . But, is it possible for the integral to get some finite value?","['integration', 'functions', 'definite-integrals']"
4659301,Prove that the solution to this differential equation must become constant once it hits 1.,"I have the following conjecture which I hope to prove. Conjecture. Let $f :[a,b]\to \mathbb R$ be differentiable on $[a,b]$ that satisfies $$
f'(x) = {\left(1-f(x)^3\right)}^{\frac 1 3}
$$ for all $x\in [a,b]$ .
Then, for all $x_0 \in [a,b]$ , if $f(x_0) = 1$ , then $f(x) = 1$ for all $x\in[a,b]$ and $x > x_0$ . I tried to do the following: Proof. Let $x_0 \in [a,b]$ such that $f(x_0) = 1$ . Let $x>x_0$ . Then, either $f(x)>1$ or $f(x) = 1$ or $f(x) < 1$ . We use proof by contradiction to eliminate the other $2$ cases. First, assume towards contradiction that $f(x) > 1$ . We construct a sequence $\{x_n\}_{n\in\mathbb N}$ as follows. By Mean Value Theorem, there exists $x_1 \in (x_0,x)$ such that $f'(x_1) = \frac{f(x) - f(x_0)}{x-x_0} > 0$ . Hence, $f(x_1) < 1$ . For all $n> 1$ , let $x_n \in (x_{n-1}, x)$ be such that $f'(x_n) = \frac {f(x)-f(x_{n-1})}{x-x_{n-1}} > 0 $ . Then it is clear that $\{x_n\}_{n\in\mathbb N}$ is increasing and bounded above, so it converges. Let $L = \lim_{n\to \infty}x_n$ . Then, since $f$ is continuous, $f(L) = \lim_{n\to\infty} f(x_n) \leq 1$ .(because $f'(x_n) > 0$ for all $n\in \mathbb N$ , which means $f(x_n)< 1$ for all $n\in\mathbb N$ ). And I stopped there, not knowing how should I proceed.","['derivatives', 'ordinary-differential-equations', 'real-analysis']"
4659320,Proving the sum of length of a unique path in a tree is less than equal to $n$ choose $2$.,"I am having trouble on trying to prove this statement using induction. Given a tree with $n$ vertices with $n \geq 2$ . $x$ is a fixed vertex, for each $v$ in the vertex set, $d(v,x)$ is the length of the unique path from $v$ to $x$ in the tree. I know that a tree with at least $2$ vertices has at least $2$ leaves, meaning at least $2$ vertices with degree $1$ . Every tree has $n - 1$ edges. If I remove $1$ leaf, I now have $n - 2$ edges. That's as far as I got. I'm not sure how to proceed. I know my goal is to reach ${k + 1 \choose 2}$ . $$\sum d(v, x) \leq {n \choose 2}$$","['graph-theory', 'trees', 'induction', 'combinatorics']"
4659337,Calculate probability of flipping coins with probabilities dependent on previous results,"Let's say I have a biased coin that has a probability of 0.2 for heads and 0.8 for tails. If you flip it 6 times without getting a heads however, the probability of heads increases by 0.2 per non-heads flip afterwards, so the 7th flip without heads will have a probability of 0.4 for heads and 0.6 for tails, the 8th flip will have a probability of 0.6 for heads, and the 10th flip will have a guaranteed chance for heads. If you get heads at any point, the probability resets back to 0.2, and you would have to roll 6 tails again to raise the probability. In this scenario, what is the best way to calculate the probability of getting X amount of heads with an arbitrary number of coin flips? For example, how might I calculate the probability of getting 2 heads from 10 flips with this coin? So far, I've tried brute-force simulation and dynamic programming, which certainly work, but I just wondered if there was a more direct and efficient approach. I've looked into using discrete Fourier transforms on rolling multiple dice, but I've found that the problem doesn't exactly translate one-to-one with the one here. I'm not too knowledgeable about probability and math in general, so I hope someone more knowledgeable could offer some suggestions or insights. Thanks for the help!","['programming', 'dice', 'dynamic-programming', 'probability']"
4659358,Ji Chen's lemma,"https://artofproblemsolving.com/community/u797276h2556237p27284705 In the solution of the VMF member, Ji Chen's lemma was mentioned. I asked VMF but have not received an answer yet. Could you provide me with some information about this theorem? I find it quite interesting. I have searched on AoPS and StackExchange but haven't found anything yet. Thank you!","['algebra-precalculus', 'reference-request', 'inequality']"
4659360,"Find the Green’s function $G(x, t)$ using a Laplace transform in $t$","Given the wave equation: $G_{tt} - G_{xx} = \delta(x-x_{0})\delta(t-t_{0})$ , with conditions: $-\infty < x < \infty,t>0$ , $G(x,0) = 0, G_{t}(x,0) = 0$ . Use Laplace transform to find the Green's function for this equation. -- After applying the Laplace transform to the equation in $t$ , I get: $$s^{2}\bar{G} - sG(x,0) - G_{t}(x,0) - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0})$$ By the initial condition: $$s^{2}\bar{G} - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0})$$ But I am struggling here, after solving the homogenous equation $s^{2}\bar{G} - \bar{G''} = 0$ , I got $\bar{G} = A(s)e^{xs}+B(s)e^{-xs}$ . And $\bar{G}$ makes the left hand side of the above equation zero. Are there better ways to deal with the left hand side?","['greens-function', 'laplace-transform', 'ordinary-differential-equations', 'partial-differential-equations']"
4659391,How to solve $\tan x=x^3$,"I was playing with functions I just learned, like sin, cos and tan, then I saw that the graph of $ y = \tan x$ and $y = x^3$ are pretty similar, that's how I thought of the equation $\tan x = x^3$ .
No matter how hard I try, I can't find a way to even start this problem.
(Please consider the fact that I am a highschool student, if it isn't possible to solve this problem with my current knowledge please let me know.)",['trigonometry']
4659472,Green function jump conditions for second order differential equation,"I been trying to find the Green's function for a particular problem.
For the equation $$ 
q(x) \frac{\mathrm{d}^2u(x)}{\mathrm{d}x^2} + p(x) u(x) =0 \tag{1}
$$ where $q(x)$ and $p(x)$ are some functions of $x$ . I have solved these and have two linearly independent results. $$
u(x) = c_1 u_1(x) + c_2 u_2(x)
$$ with $c_1, c_2$ being arbitrary constants and both $u_1, u_2$ solve equation (1). Now for the boundary conditions. At $x\to +\infty$ we require that the solution goes to zero, only $u_2$ satisfies, another condition is at $x\to1_+$ . Only $u_1$ satisfies this.
Thus our Green's function can be written as $$
G(x,y) = c_1 \mathcal{H}(y-x) u_1(x) + c_2 \mathcal{H}(x-y) u_2(x).
$$ Where $\mathcal{H}$ is the Heaviside step function and $y>1$ . Now the Green's function must be continous at $x = y$ , thus we have condition for (for example) $c_1$ . But what is the other condition? Does the condition $$
\lim_{x\to y_-} \frac{\partial G(x,y) }{\partial x} -\lim_{x\to y_+} \frac{\partial G(x,y) }{\partial x} = 1
$$ hold even for our special ODE (1)?","['greens-function', 'ordinary-differential-equations']"
4659489,Dual operator of Markovian operator,"I am trying to incorporate the definition of dual operator in a Markovian setting.
Say I have a Markov kernel $K:S\times S\to[0,1]$ i.e., a mapping such that for every $A\in S$ $x\to K(x,A)$ is a measurable mapping, and for every $x \in S$ , $A\to K(x,A)$ is a probability measure. I can see it as an operator acting on measures $(\mu K)(A)= \int K(x,A) d\mu(x)$ and as an operator acting on functions $Kf(x) = \int dK(x,y) f(y)$ . Now, given a reference measure $\nu$ , I want to study the dual operator $K^\star_\nu$ .
Consider $K$ as a mapping form $L^p(\nu K)$ to $L^p(\nu)$ , then $K^\star_\nu$ is a mapping from $(L^p(\nu))^\star$ to $(L^p(\nu K))^\star$ i.e., $L^q(\nu)$ to $L^q(\nu K)$ with $q=p/(p-1)$ . By definition I should have that $(K^\star_\nu f)(h) = f(Kh)$ where $h\in L^p(\nu K)$ . Now I should show that $(\nu K)(gK^\star_\nu(f))= \nu(K(g)f)$ but I am not sure how to derive this. Passing to integrals: $$ (\nu K)(gK^\star_\nu(f)) = \int d\nu K(y) g(y) (K^\star_\nu f)(y) 
 = \int d\nu K(y) g(y) f(K?(y)) $$ my question is, what does ""?"" represent there? I do know that $K_\nu^\star f\in L^q(\nu K)\approx (L^p(\nu K))^\star$ so the first piece makes sense formally if I see it as an operator acting on $L^q(\cdot)$ however, when I use the definition of dual operator, then $(K_\nu^\star f)(y)=f(K?y)$ and I don't know how to fill the question mark and how to bridge the duality between the operators and the duality between the spaces as $K$ should act on $L^p(\nu K)$ and not on the field.","['measure-theory', 'operator-theory', 'markov-chains', 'lp-spaces', 'dual-spaces']"
4659492,"What exactly should the solution to $y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}$ be?","(Hey everyone, I'm new to Stack Exchange and this is actually my first question, so if I'm doing anything inappropriate regarding asking questions, please point that out for me.) So I've been taking an ODE course lately, and I encountered a question in my homework, which is basically an Initial Value Problem that goes by $$y^\prime = (1 - 2x)y^2, \ y(0) = -\frac{1}{12}.$$ This is a pretty standatd IVP, so I just attempted by dividing both sides by $y^2$ (while noting the trivial solution $y = 0$ ): $$\frac{dy}{y^2} = (1-2x)dx$$ Integrating both sides gives me $$-\frac{1}{y} = -x^2 + x + C$$ Plugging in the initial conditions gives $C = 12$ , so after doing some algebraic manipulations, the final answer I obtained is $y = 0$ and $y = \frac{1}{x^2 - x -12}.$ But here's the thing: the solution given by my teacher is $$y = \frac{1}{x^2 - x - 12}, \ -3 < x < 4,$$ about which there are several things I'm not so sure.
First, I don't really understand why the trivial solution is missing in the suggested solution. Also, I'm not sure whether it's necessary at all to include the domain of the function in the answer (since the solutions to problems as such on textbooks don't seem to bother including the domains), although in this case since the polynomial with $x$ terms is in the denominator, there is certain constraints on the interval on which $y$ is defined--which brings out my third confusion: why is the domain here not simply $x \neq -3, x \neq 4$ , but an open interval $(-3, 4)$ ? One possible reasoning I came up with is that, given the initial point $(0, -\frac{1}{12})$ , we only care about the part of the function close to that point. But if the answer is given solely by the suggested solution, isn't the trivial solution completely ignored? My concerns here are really just about subtlties, but as a beginner in ODE, I find these minor issues especially convoluted and needs clarifications, so I'd be appreciative if anyone can help with this! $\textbf{EDIT}$ : $y^2$ is not a trivial solution in this case, as it does not satisfy the initial condition, so I've made a dumb mistake...It is better though, to explicitly write down the domain of the function (see Anne's comment below). Thank you guys for helping!","['initial-value-problems', 'ordinary-differential-equations']"
4659498,How to calculate $\lim_{t \to \infty} \sum_{c=1}^{t-1} \frac{q^c}{t-c}$,How can I find the value of the infinite sum: $$\lim_{t \rightarrow \infty} \sum_{c=1}^{t-1} \frac{q^c}{t-c}$$ where $0 <q <1$ . I tried putting the expression into WolframAlpha for some numerical experiments. It saw some behaviors that the infinite sum actually converges to 0. I am interested to know the reason behind this. WolframAlpha also gave me this closed form expression for the summation involving Lerch Transcendent which I don't understand. Is there a simpler way to derive this result? Any help is appreciated.,"['calculus', 'analysis']"
4659508,Family of spin Dirac operators coming from path of metrics,"Let $\mathbb{R}\ni t\mapsto g_t$ be a continuous path (in the $C^{\infty}$ -topology of $\Gamma(\text{Sym}^2(T^*M))$ ) of smooth metrics on a compact spin manifold $M$ . For each $t\in \mathbb{R}$ , we can define the Dirac operator $D_t$ associated to the metric $g_t$ over a spinor bundle $S_t\to M$ , in the usual way. I'm trying to understand under which conditions one can perceive these Dirac operators as defined on the same spinor bundle $S$ , with the final goal of showing that the map $t\mapsto D_t \in \mathcal{B}(W^1(S),L^2(S))$ is norm-continuous, where $W^1(S)$ denotes the first Sobolev space. I already know that for two conformally related metrics one can find an isometry between their spinor bundles, so this would definitely be enough, but too restrictive. I can see on Dependence of spinor bundle on choice of metric that the answer seems to be positive if one picks two metric $g_0$ and $g_1$ and looks at the line segment along them, but I cannot seem to be able to prove it. Any help on that would be appreciated!","['elliptic-operators', 'spin-geometry', 'differential-geometry']"
4659512,Formula for the ratio of Numbers not divisible by the first n primes,"I was playing around with prime numbers, and specifically how many numbers we can exclude from an interval from being primes. It is easy to see, that after $2$ , the ratio of numbers is $1-\frac{1}{2}$ , as we know every second number to be divisible by $2$ and, as such be impossible to be a prime. After we include $3$ , the ratio would be $1-\frac{1}{2}-\frac{1}{3}$ , except that every number divisible by $6$ would be excluded once by the $\frac{1}{2}$ and a second time by the $\frac{1}{3}$ . Therefore in the actual ratio we have to add every sixth number again. So the actual ratio is $1-\frac{1}{2}-\frac{1}{3}+\frac{1}{6}$ . And naturally I wondered how this sequence continues once we include $5$ . However, the number of terms grows rapidly. This is because 1: We have to add back every tenth and fiveteenth number, again, as we would otherwise subtract them twice by the $\frac{1}{2}$ and $\frac{1}{5}$ or the $\frac{1}{3}$ and $\frac{1}{5}$ respectively. 2: We have to subtract every $30$ eth Number, as we subtract them thrice from the $2,3,5$ and add them back thrice from the $6,10,15$ , so as to not ignore these, we subtract them. This is the sequence I have arrived at, although I could be missing something $1-\frac{1}{2}-\frac{1}{3}+\frac{1}{6}-\frac{1}{5}+\frac{1}{10}+\frac{1}{15}-\frac{1}{30}$ Conjectured sequence If we look at the number of terms, the sequence $1,2,4,8$ appears, furthermore, if we look at the latter half of our sequence for the first 3 prime numbers $(-\frac{1}{5}+\frac{1}{10}+\frac{1}{15}-\frac{1}{30})$ , it is just the first half $(1-\frac{1}{2}-\frac{1}{3}+\frac{1}{6})$ multiplied by $-\frac{1}{5}$ . So here is my conjecture stated a bit more mathematically: Let $p_n$ be the $n$ -th prime number, $r(n)$ the ratio of natural numbers divisible by the first $n$ prime numbers to the natural numbers, then \begin{multline}
r(0)=1,\\
r(n+1)=r(n)-\frac{1}{p_{n+1}}r(n)\\
=r(n)\cdot\left(1-\frac{1}{p_{n+1}}\right)\\
=r(0)\cdot\left(1-\frac{1}{p_1}\right)\left(1-\frac{1}{p_2}\right)...\left(1-\frac{1}{p_{n+1}}\right)\\
=\prod_{i=1}^{n+1}\left(1-\frac{1}{p_i}\right)
\end{multline} My question is: is this true? It seems true but again, as the number of terms grow so rapidly I could have missed something. I am also very certain, that I was not the first to have this idea, but I failed to find any theorem like this on google.","['number-theory', 'conjectures', 'prime-numbers', 'sequences-and-series']"
4659570,Stokes Theorem on manifolds with dense corners,"I am currently working on a project, where I would (ideally) like to apply Stokes theorem on a Manifold with corners. I have found various sources, which justify this application. Except one thing: In the book ""Stokes's Theorem and Whitney Manifolds"" by Anthony W. Knapp, on page 108, the author uses an indicator function $I_k(x)$ which is 1 on the subset $D(x,E)\geq2^{-k}$ and 0 elsewhere, where E is the set of ""exceptional points"" such as corners, and D is the distance function (exceptional points are the points on the boundary, which make the boundary non-differentiable). He later arrives at the conclusion, that, if E has the condition $$\operatorname*{lim}_{\delta\downarrow0}\;\delta^{-1}\vert\{x\in\mathbb{R}^{m}\mid D(x,E)\lt \delta\}=0,$$ then Stokes theorem $\int_{B-E}\omega=\int_{U}d\omega$ holds. The problem is that this proof doesn't work, if E is dense, due to the fact that the set of points which satisfies $D(x,E)\geq2^{-k}$ is empty. So, my question is: Is Stokes theorem applicable on manifolds with dense corners (or better, with a dense set of exceptional points)? If so, why/why not?","['integration', 'stokes-theorem', 'differential-geometry']"
4659601,"Show if $f:\mathbb{Z}_p\to\mathbb{Q}_p$ is continuous such that $f(n)=(-1)^n$, then $p=2$.","Let $\mathbb{Q}_p$ denote the p-adic rationals and same for the integers. Suppose $f:\mathbb{Z}_p\to\mathbb{Q}_p$ is continuous, with the additional property that at $n\in\mathbb{Z}^{\geq 0}$ , $f(n)=(-1)^n$ . I wish to conclude that $p=2$ . I'm a little stuck with how to proceed. By way of contradiction, suppose that $p>2$ . By continuity, we must have that for all $x,y\in\mathbb{Z}_p$ , $$|f(x)-f(y)|_p<\epsilon$$ if $x,y$ is sufficiently close (the definition of continuity). Now, I'd like to choose integers which are close but such that $(-1)^n$ and $(-1)^{n'}$ are opposites, and then somehow use this to derive a contradiction, but I'm stuck making a rigorous argument. Perhaps I'm thinking about $p$ adic continuity wrong?","['number-theory', 'p-adic-number-theory', 'continuity']"
4659622,Total sets for $L^p$ for every $1\leq p < \infty$.,"Consider $L^p[ 0,1]$ for $1\leq p < \infty$ or, if you prefer, $L^p(\mu)$ where $\mu$ is a finite Borel measure with compact support. Let $(\phi)_{i\in I}$ be a subset of measurable functions that is contained in $L^p$ for every $1\leq p < \infty$ and assume that it is total for $L^{p_0}$ for some $1\leq p_0< \infty. $ Could you deduce that $(\phi_i)_{i\in I}$ is total for $L^p$ for every $1\leq p < \infty$ ? If $1\leq p_1 \leq p_0$ it is enough to recall that the set of linear combinations of step functions is dense in every $L^p$ , and argue as it follows. Let $f \in L^{p_1}$ and $\varepsilon > 0.$ Then, there exists $g \in L^{p_1}\cap L^{p_0}$ , which is a linear combination of step functions, such that $||f-g||_{p_1} < \varepsilon.$ Moreover, since $(\phi)_{i\in I}$ is total in $L^{p_0},$ there exists $h$ in its linear span such that $||g-h||_{p_0} < \varepsilon.$ Since the measure is finite, it follows that $||g-h||_{p_1} \leq C ||g-h||_{p_0},$ with $C > 0.$ Finally, $$||f-h||_{p_1} \leq ||f-g||_{p_1} + ||g_h||_{p_1} < \varepsilon (1+C),$$ as we wanted. What does it happen with the rest of exponents $p > p_0$ ?","['measure-theory', 'lp-spaces', 'real-analysis']"
4659664,Show that $\textbf{A}\cdot\nabla G=0$ for given $G$ and $\bf{A}$,"I have solved $$yu_x+u_y=u^2$$ using $x=-\frac{s^2}{2},\,y=s>0,\,u=1$ and obtained the solution $$u(x,y)=\frac{1}{\sqrt{\frac{y^2}{2}-x}+1-y}$$ Now I am trying to show $\textbf{A}\cdot\nabla G=0$ where $$G(x,y,u)=\sqrt{\frac{y^2}{2}-x}+1-y-\dfrac{1}{u}=0, \qquad\textbf{A}=(y,1,u^2)$$ Therefore, as $$\nabla G = \left(-\frac{1}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+\dfrac{u_x}{u^2},\,\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2},\,\frac{1}{u^2}\right)$$ I find that \begin{align}
\textbf{A}\cdot{\nabla}G &= -\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+y\dfrac{u_x}{u^2}+\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2}+1 \\
&= y\frac{u_x}{u^2}+\frac{u_y}{u^2} \\
&= 1
\end{align} This should be equal to zero though so where have I gone wrong?","['multivariable-calculus', 'solution-verification', 'partial-differential-equations']"
4659681,"When does the equations have $1,2,3$ solutions?","There is given an equation, $$ \frac{x^2-x+1}{x^2+x+1}=kx+1$$ When does this solution have one, two, three real solution(s) in $x$ . My Approach: The above equation can be rewritten as, $$\frac{-2x}{x^2+x+1}=kx$$ This forms a cubic equation in $x$ where $0$ is a universal solution. So, then canceling $x$ gives a quadritic equation and which can add either $0,1,2$ more solutions according to value of discriminant of equation ( $D<0,D=0,D>0$ ) respectively. But this gave, $k\in \bigg(-\infty,\frac{-8}{3}\bigg)\cup[0,\infty)$ for one solution $k=\frac{-8}{3}$ for two solutions $k\in\bigg(\frac{-8}{3},0\bigg)$ for three solutions But my textbook has different answer for second and third part. Where Am I wrong? And if you know different method to this problem please post that, I am glad to know alternatives.","['functional-equations', 'systems-of-equations', 'functions']"
4659723,"Do there exist countinuously differentiable functions $f,g$ such that $f(x)g(x) = x$ and $f(0) = g(0) = 0$?","Do there exist continuously differentiable functions $f,g : \mathbb{R} \rightarrow \mathbb{R}$ such that $f(0) = g(0) = 0$ and for every $x {\in} \mathbb{R}, \,f(x)g(x) = x$ ? My solution: No. Since $f$ and $g$ are differentiable functions and $f(x)g(x) = x$ , we have $f'(0)g(0) + f(0)g'(0)= 1$ , which is not possible if $f(0) = g(0) = 0$ . Why does the exercise specify that $f$ and $g$ must be CONTINUOUSLY differentiable? It seems weird to add that if we don't need it; did I miss something?","['derivatives', 'real-analysis']"
4659733,What can we say about the pre-image of the support of the pushforward measure in relation to some original measure?,"I have a (probability) measure $\mu$ on space $X$ , a function $f: X \rightarrow Y$ , and a pushforward measure $\nu$ on $Y$ induced by applying $f$ to $\mu$ . Suppose we have $\text{supp}(\nu)$ . Can we say anything about the set $f^{-1}(\text{supp}(\nu))$ in relation to a set involving $\mu$ ? My guess is that it needn't equal $\text{supp}(\mu)$ . [I have read this post .] If additionally $f$ is continuous, then it seems like we should have $\text{supp}(\mu) = f^{-1}(\text{supp}(\nu))$ ? Is this right? (edit: it is not! see comments)","['pushforward', 'measure-theory', 'probability-theory', 'probability']"
4659750,Sum of signed permutations of digits equals zero,"After playing around with signed permutations lately, (as part of studying properties of antisymmetric tensors and wedge products which are not really related to what I want to ask about), I noticed that apparently for every integer with $3$ digits or more, the signed alternating sum of its digits permutations is equal to zero. In order to avoid abusing notation, I'll first define $P_{\pi}(n)$ to be the signed and permuted version of an $N$ digits integer via a given permutation $\pi\in{S_N}$ : $$ P_{\pi}(n) = \text{sign}(\pi)\sum_{i=1}^N{n_i10^{\pi_{i}-1}}$$ Where $n_i$ is assumed to be the $i$ 'th digit, and I define $n_0$ to be the least significant digit, just to ensure that the original number $n$ maintains its original sign (This way, it is ""hit"" with the identity permutation). With the above, I can more clearly restate the observation, that for any $n\in\mathbb{N}$ with $N\ge3$ digits: $$\sum_{\pi\in{S_{N}}}{P_{\pi}(n)=0}$$ Where it is assumed $\pi$ ranges over all possible elements in $S_N$ . In case this is confusing, a concrete example follows: $$123-132+312-321+231-213=0$$ My question is whether this is a known result, and if so where can I find more information about this property? BTW, I haven't proved this, so I should also ask whether this even holds true or not (although at this point I am rather convinced it is). It should be straightforward to prove via mathematical induction I think, although it may be more enlightening to see a more direct proof (I tried to write one, but those anti symmetrized sums get really cumbersome really quickly and I gave up for now :)).","['permutations', 'number-theory', 'natural-numbers']"
4659758,Determine the radius of convergence of the series,"I'm trying to solve an exercise in the Complex Analysis textbook, but I can't use the hint given by the author. Here is the exercise: Determine the radius of convergence of the series $\sum _{n=1}^{\infty } a_{n} z^{n}$ when $a_{n} =\frac{( n!)^{3}}{( 3n) !}$ . Hint: Use Stirling’s formula, which says that $n!\sim \ cn^{n+\frac{1}{2}} e^{-n}$ for some $c >0$ . I figured it out using the ratio test, but the answer here should be using the Cauchy-Hadamard theorem with $\limsup$ because I couldn't find a discussion of the ratio test before the exercise. Textbook is by Stein and Shakarchi, Chapter $1$ , exercise $16$ . Edited: I think I solved this problem by trying another form of the Stirling formula from the internet, but is it correct? $$
Γ( x) \sim  x^{x -1} e^{- x}\sqrt{2\pi x}
$$ \begin{align*}
\frac{( n!)^{3}}{( 3n) !} &\sim \frac{\left(n^{n -1} e^{-n}\sqrt{2\pi n}\right)^{3}}{( 3n)^{( 3n) -1} e^{-( 3n)}\sqrt{2\pi ( 3n)}}\\
\\
&=\frac{n^{3n -3} e^{- 3n}\sqrt{2\pi n}^{3}}{( 3n)^{( 3n) -1} e^{-( 3n)}\sqrt{2\pi ( 3n)}}\\
\\
&=\frac{n^{3n -1} n^{2}\sqrt{2\pi  n}^{3}}{( 3n)^{( 3n) -1}\sqrt{2\pi ( 3n)}}\\
\\
&=\frac{ n^{3n -1} n^{2}( 2\pi n)}{( 3n)^{( 3n) -1} 3^{1/2}}\\
\\
&=\frac{n^{3} 2\pi }{3^{( 3n) -1} 3^{1/2}}
\end{align*} Then $$\frac{1}{R} = \lim _{n\rightarrow \infty }\left(\frac{n^{3} 2\pi }{3^{( 3n) -3/2}}\right)^{1/n}=\frac{1}{3^{3}}
$$","['complex-analysis', 'calculus', 'power-series']"
4659762,Does the implicit function theorem if it holds give a curve in $\mathbb{R}^2$ (or in $\mathbb{R}^3$)?,"Prove that the equations $\begin{cases}e^x+2e^{y}+e^z=4\\
x+y+z=0\end{cases}$ define a curve in space and the curve doesn't cross itself. I let $z=-x-y$ and define $f(x,y)=e^x+2e^y+e^{-x-y}=4$ . My idea is to show that the graph of $f$ is a curve that doesn't cross itself. Then projecting it onto the plane $x+y+z=0$ will be a curve in $\mathbb{R}^3$ that also doesn't cross itself. That the graph of $f$ is a curve can supposedly be shown by the implicit function theorem. If I haven't understood the following incorrectly Does statement of Implicit Function Theorem imply that level set is a curve? We have that $f_x=e^x-e^{-x-y}$ and $f_y=2e^y-e^{-x-y}$ . These are zero only when: $\begin{cases}
e^x=e^{-x-y}\\
2e^y=e^{-x-y} \end{cases}\iff \begin{cases}-2x=y\\
\ln 2+2y=-x\end{cases}$ . The first equations has solutions $(x,-2x)$ , $x\in\mathbb{R}$ and putting it in the second yields $\ln2=3x$ which gives the critical point $\left(\frac{\ln2}{3},\frac{-2ln2}{3}\right)$ . Since $f\left(\frac{\ln2}{3},\frac{-2ln2}{3}\right)\neq 4$ by the implicit function theorem the graph of $f$ is a injective curve(it doesn't cross itself). Does the implicit function theorem actually give a curve? Consider $x^2-y^2=2$ , the critical point for this equation is $(0,0)$ but the ""curve"" is a hyperbola. Now with the above in mind, projecting the hyperbola onto the plane gives two split curves in $\mathbb{R^3}$ , is this still considered a curve and if not how can I be sure in my solution? Edit: Not cross itself means no self-intersection. I don't know anything else this is a question from my exam in calc 3. The solution given (it literally says advice on how to solve it). Let $f(x, y, z) = e^x + 2e^y + e^z$ och $g(x, y, z) = x + y + z$ . We see that $x=z=y=0$ is a solution, thus the suspected curve contains atleast a point. We have that $\nabla f(x,y,z,)=(e^x,2e^y,e^z)$ and $\nabla g(x,y,z)=(1,1,1)$ . The vector product of the gradients is then $\nabla f \times \nabla g=(2e^y − e^z, e^z-e^x,e^x-2e^y)$ . This vector is 0 if and only if $e^x=2e^y=e^z$ . If $(x,y,z)$ is such a point with $g(x,y,z)=0$ then $x=z=(1/3)\ln2$ and $y=-(2/3)\ln2$ . Then $f(x,y,z)\neq 4$ . This shows that $\nabla g$ and $\nabla f$ thever are parallel if $g(x,y,z)=0$ and $f(x,y,z)=4$ and therefore by the implicit function theorem a curve in space .","['multivariable-calculus', 'differential-geometry']"
4659790,Non equivalent norms on $L^2$,"This might seem silly as the main issue is the question is vague, but it did lead to an interesting question. So an old exam paper asks the following Let $|| . ||$ be a norm on $L^2(\mathbb{R})$ s.t. the space remains complete when endowed with this norm (as opposed to the usual $L^2$ norm). Assume further, every $|| . ||$ - convergent sequence  has an a.e. (pointwise) convergent subsequence. Prove $|| . ||$ is equivalent to the usual $L^2$ norm. Now previous parts of the question hint at using the Closed Graph Theorem, but I'm not sure I'm reading the question right. If their statement means "" whenever $f_n$ is a sequence that converges in the $|| . ||$ norm to some $f$ , then there is a subsequence $f_{n_k}$ that converges pointwise a.e to $f$ "" then I can see the CGT gives almost immediately the identity map between the two spaces ( or rather the same space endowed with the two different norms) is continuous. However, if they just mean ""whenever $f_n$ is a sequence that converges in the $||. ||$ norm to some $f$ , there is some $g$ in $L^2$ and a subsequence $f_{n_k}$ s.t $f_{n_k}$ converges a.e to g"" then it's not really obvious to me how to get the conclusion, or indeed if the conclusion is actually true. So my question is, can you put such a norm on $L^2$ - i.e that makes the space complete, and every norm-convergent has an a.e. convergent subsequence, but possibly to a different a.e limit than the norm-limit - that is not equivalent to the usual $L^2$ norm? (Or indeed equivalently s.t there is a sequence that converges to $0$ in norm, but no subsequence converges to $0$ a.e) The more I think about it, the more it seems like it should be true, but I don't quite have an example of such norm","['measure-theory', 'lp-spaces', 'functional-analysis']"
4659807,Expected number of elements for the first of $n$ many hash tables to be filled?,"This is a generalization of the questions: question 1 and question 2 . There are $n$ many hash tables each of size $m$ . Each turn a random element in one of the hash tables is filled. If the element selected for that turn has already been filled then we've wasted that turn, and we go to the next turn as usual. What is the expected number of turns needed for one of the hash tables (whichever is first) to be filled? Another way of asking this problem is: If we have an $n*m$ - sided die and we partition the possible values into $n$ equal sized sets of $m$ elements, what is the expected number of rolls before we complete a set? For example, if we have a 15-sided die and we split the numbers into 3 sets of 5: $\{1,2,3,4,5\}, \{6,7,8,9,10\}$ , and $\{11,12,13,14,15\}$ , then $m = 5$ and $n=3$ . On average, how many times must we roll the die before we've seen all $5$ rolled values for one of the sets (whichever is first)? In question 1 WhatsUp gave a recursive equation for a two hash table scenario: $$E(a, b) = 1 + \frac a {2m}E(a - 1, b) + \frac b {2m} E(a, b - 1) + \left(1 - \frac{a + b}{2m}\right)E(a, b)$$ with $E(a,b)$ defined as the expected number of elements to fill one table, if there are still $a$ and $b$ empty entries in the two tables. Can this be modified for a 3 hash table scenario as: $$E(a, b, c) = 1 + \frac a {3m}E(a - 1, b, c) + \frac b {3m} E(a, b - 1, c) + \frac c {3m} E(a, b, c - 1) + \left(1 - \frac{a + b + c}{3m}\right)E(a, b, c)$$ and if so how do we go about solving for $E(m,m,m)$ ? Could we generalize this equation to deal with $n$ many hash tables, and if so how? In question 2 there are some great answers involving Markov chains, but if $n$ and $m$ get very large, the transition matrix gets very large and impractical to fill out or work with. (At least that's how it seems to me. Are there are tricks to deal with this?) So another approach is necessary. Thomas Andrews gave a very detailed answer which included a generalized solution which should work here $$\sum_{i_1}^{a_1}\cdots\sum_{i_n=1}^{a_n}(-1)^{\sum (i_j-1)}\binom{a_1}{i_1}\cdots\binom{a_n}{i_n}\frac{a}{\sum i_j}$$ but isn't intuitive and I'm struggling to see how it came about. He mentioned, and I can see, that it stems from an inclusion-exclusion argument, but I'm failing to see how to start there and end up with his equation.","['inclusion-exclusion', 'combinatorics', 'coupon-collector', 'discrete-mathematics', 'probability']"
4659849,How do I solve $ \int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}dx$?,"I had to solve this integral: $$
\int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}\mathrm{d}x
$$ by the substitution $x=\sqrt{3}t$ the indefinite integral can be written as: $$
\int \frac{x^2}{\sqrt{3+x^2}}\;\mathrm{d}x = \int \frac{3t^2}{\sqrt{1+t^2}}\; \mathrm{d}t
$$ at this stage I performed the substitution $\displaystyle t=\frac{e^u-e^{-u}}{2}$ , but the computations are definitely too tough for being a high school exercise.
I'd like to know if there's a simpler method, by substitution, that allows to solve it.","['integration', 'analysis']"
4659876,Direct sum decomposition of Hilbert spaces,"Let $A$ be a self-adjoint operator on a Hilbert space $\mathscr{H}$ with dense domain $D(A)$ . From the continuous functional calculus, for every continuous function $f \in C(\sigma(A))$ we can associate a bounded operator $f(A)$ on $\mathscr{H}$ . Moreover, if $\psi \in \mathscr{H}$ , the functional: $$\omega(f) := \langle \psi, f(A)\psi\rangle$$ is a positive linear functional, so by Riesz Representation Theorem there is an isomorphism between $\mathscr{H}$ and the set of regular Borel measures on $\sigma(A)$ , denoted by $M(\sigma(A))$ , such that each $\psi \in \mathscr{H}$ is uniquely associated to a measure $\mu_{\psi}$ , with $$\langle \psi, f(A)\psi\rangle = \int_{\sigma(A)}f(\lambda)d\mu_{\psi}(\lambda).$$ I now that every Borel measure $\mu$ has a decomposition $\mu = \mu_{pp}+\mu_{ac}+\mu_{sc}$ , which is a consequence of the Lebesgue Decomposition Theorem . Here $\mu_{pp}$ is a pure point measure, $\mu_{ac}$ is absolutely continuous with respect to the usual Borel measure on $\mathbb{R}$ and $\mu_{sc}$ is singular with respect to the latter. Hence, there is an identification $\psi \to \mu_{\psi} = \mu_{\psi,pp}+\mu_{\psi,ac}+\mu_{\psi,sc}$ . This shows that $\mathscr{H}$ can be written as $\mathscr{H}_{pp}+\mathscr{H}_{ac}+\mathscr{H}_{sc}$ , with $\mathscr{H}_{pp} := \{\psi \in \mathscr{H}: \mbox{$\mu_{\psi}$ is pure point}\}$ , $\mathscr{H}_{ac} := \{\psi \in \mathscr{H}: \mbox{$\mu_{\psi}$ is absolutely continuous with respect to the usual Borel measure}\}$ and $\mathscr{H}_{sc} := \{\psi \in \mathscr{H}: \mbox{$\mu_{\psi}$ is singular with respect to the the usual Borel measure}\}$ . My question is: why this sum is actually a direct sum ? That is, how to prove further that: $$\mathscr{H} = \mathscr{H}_{pp}\oplus \mathscr{H}_{ac}\oplus \mathscr{H}_{sc}?$$","['spectral-theory', 'functional-analysis', 'mathematical-physics']"
4659973,"Optimal positions to place 3 coins in a line such that when you keep rolling the die and increment your position, you reach a coin.","You have 3 coins and a die. There is a line with positions from 1 to 1000. you need to place the coins in any of these positions in the line. from position 0,  you keep rolling the die and you jump that many steps in the line. if you reach any position with a coin in it, you win. If you cross 1000 without reaching any positions with a coin, you lose. What are the optimal positions to place the coins in the line to maximize your chance of winning the game?","['game-theory', 'optimization', 'combinatorics', 'probability']"
4660018,Induced connections,"Let $E\rightarrow M$ be a be a vector bundle. From this bundle, we can construct various other bundles such as $E^*$ , and $E\otimes E$ . I know that if we have a connection, or covariant derivative $\nabla:\Gamma(E)\rightarrow \Omega^1(M,E)$ , then there are induced connection on these bundles given by: $$(\nabla^*\Xi)(\Phi)=\nabla(\Xi(\Phi))-\Xi(\nabla\Phi)$$ $$\nabla^\otimes(\Phi\otimes \Psi)=\nabla\Phi\otimes \Psi+\Phi\otimes\nabla\Psi$$ where $\Xi\in\Gamma(E^*)$ , and $\Phi,\Psi\in \Gamma(E)$ . My question is then this, I can show that these are indeed connections on the aforementioned vector bundles, but is there a reason, other than verifying some version of the product rule, that these should be the induced connections on the vector bundles $E^*$ and $E\otimes E$ ? In other words, are these connections somehow uniquely/naturally/canonically determined given $\nabla$ ? It seems these definitions are pulled out of thin air, while, for example, the definition of the Lie derivative naturally extends to deducing relations such as the ones above.","['connections', 'smooth-manifolds', 'vector-bundles', 'differential-topology', 'differential-geometry']"
4660022,Proving $\sum_{k=1}^{2n-1}\frac{\sin(\frac{\pi k^2}{2n})}{\sin(\frac{\pi k}{2n})}=n$,"I wander on the internet and found this problem (from Quora) this link The problem is proving the identity: $$\sum_{k=1}^{2n-1}\frac{\sin\left(\frac{\pi k^2}{2n}\right)}{\sin\left(\frac{\pi k}{2n}\right)}=n$$ I only can transform this sum to $$2\sum_{k=1}^{n-1}\frac{\sin\left(\frac{\pi k^2}{2n}\right)}{\sin\left(\frac{\pi k}{2n}\right)}+\sin\left(\frac{n\pi}{2}\right)$$ And from this step, I don't know how to process further.
Thank you for reading, any hint or another approach is welcome.","['trigonometry', 'summation']"
4660035,Show that a dynamical system is not ergodic (finding the invariant sets),"Given $(\Omega, \mathcal F, \mathbb P)$ a probability space. Consider the iid sequence of random variables $(X_n)_{n\in \mathbb N}$ defined on $(\Omega, \mathcal F, \mathbb P)$ : $$X_n \overset{iid}{\sim} N(0,1).$$ Let $B:(\Omega, \mathcal F, \mathbb P)\to \{\hbox{head},\hbox{tail}\}$ be a Bernoulli random variable: $B \sim \hbox{Bernoulli}(p)$ . Define $(Y_n)_{n\in \mathbb N}$ as follows. Flip a coin according to $B$ and: If it is head, set $Y_n=X_n$ , for all $n$ ; If it is tail, set $Y_n=0$ , for all $n$ . Notice that $(Y_n)_{n\in \mathbb N}$ is defined on $(\Omega, \mathcal F, \mathbb P)$ with trajectories in $(\mathbb R^\mathbb N, \mathcal B^\mathbb N, P_Y)$ , where $P_Y$ is the law of the stochastic process $(Y_n)_{n\in \mathbb N}$ . Moreover, it is straightforwar to show that $(Y_n)_{n\in \mathbb N}$ is stricty stationary. According to Chapter V, Probability , Albert Shiryaev, $(Y_n)_{n\in \mathbb N}$ generates a dynamical system $(T, P_Y)$ as follows. First, define $T: \mathbb R^\mathbb N \to \mathbb R^\mathbb N $ as: $$T(x_1,x_2,...)= (x_2,x_3,...).$$ Since $(Y_n)_{n\in \mathbb N}$ is stricty stationary, we have that $P_Y$ is $T$ -invariant in the following sense: $$P_Y(A)=P_Y(T^{-1}(A)), \quad \forall \, A \in \mathcal{B}^\mathbb N$$ We say that $(T, P_Y)$ is ergodic if: $$P_Y(A)\in \{0,1\}, \quad \forall A \in \mathcal{I}_T$$ where $\mathcal{I}_T$ is the class of $T-$ invariant sets (indeed, a $\sigma-$ algebra), defined as : $$\mathcal{I}_T:=\{A \in \mathcal B^\mathbb N : A = T^{-1}(A)\}$$ I want to show that $(Y_n)_{n\in \mathbb N}$ is not ergodic. For this, I want to: Find $\mathcal{I}_T$ for the dynamical system generated by $(Y_n)_{n\in \mathbb N}$ ; Show that $P_Y(A)\notin \{0,1\}$ for some $A \in \mathcal{I}_T$ . Your help in these two points can help me to show non-ergodicity in other more complex cases. Update question I am indeed interested in these two points above, but I would like to focus on showing whether it is true that $\mathcal{I}_T \subset \mathcal B^\mathbb N$ is related (in some sense) to $\sigma(B) \subset \mathcal F$ . Here, $\sigma(B)$ is the sigma algebra generated by the Bernoulli $B$ .","['stochastic-processes', 'ergodic-theory', 'probability-theory', 'dynamical-systems']"
4660039,Catalan interpretation,"I have question and I think the answer is equal with Catalan numbers. The question as follows: The number of the sequence $a_0a_1a_2\cdots a_na_{n+1}$ of integers with $a_i\geq 2, a_0=a_{n+1}=1$ and $a_i \vert (a_{i-1}+a_{i+1})$ , where $1\leq i\leq n$ is equal Catalan numbers. I obtained some values of these sequences: For $n=1$ we have $121$ , For $n=2$ , we have $1231, 1321$ , For $n=3$ , we have $12341, 12531, 13231, 13521, 14321$ For $n=4$ , we have $$123451,123741,125341,125831,127531,132341,132531,135231,135721,138521,143231,143521,147321,154321.$$ I try to define bijection between these sequence and other interoperation of Catalan numbers. Please let me know if you have any comments to help.
Thanks.","['catalan-numbers', 'combinatorics', 'sequences-and-series']"
4660105,"How many subsets $S$ of $\{1, 2, \dots , n\}$ satisfy $S \cup (S + 1) = \{1, 2, \dots ,n\}$?","For a set $S$ of integers, define $S + 1$ to be $\{x + 1 : x \in S \}$ . How many subsets $S$ of $\{1, 2, \dots , n\}$ satisfy $S \cup (S + 1) = \{1, 2, \dots ,n\}$ ? Can you generalize this? What I think is that we need always the condition that $1$ and $n-1$ are in the set $S$ as that's the only way for $1$ and $n$ to be in $S \cup (S+1)$ . For $n=1$ there is no such set as $S = \{1\}$ , but $S+1 = \{2\}\Longrightarrow S \cup (S + 1) = \{1, 2\}\neq\{1\}$ . For $n=2$ there is only one subset that works i.e. $S = \{1\}$ . For $n=3$ I think that only $S =\{1,2\}$ works. And lastly for $n=4$ I haven't computed all of the $16$ subsets, but I guess that the only ones working would be $S=\{1,3\}$ and $S=\{1,2,3\}$ . Is this a correct guess for the problem?",['combinatorics']
4660108,"If $G$ is a locally compact group and $L^1(G)$ is unital, then $G$ is discrete.","The book Principles of Harmonic Analysis by Anton Deitmar and Siegfried Echterhoff outlines the following proof to show that if $G$ is a locally compact group and $L^1(G)$ is unital, then $G$ is discrete. I have questions about certain steps. Proof. Assume that $A = L^1(G)$ possesses a unit $\phi$ and $G$ is non-discrete. The latter fact implies that any unit neighborhood $U$ has at least two points. This implies by Urysohn’s Lemma (A.8.1) that for every unit-neighborhood $U$ , there are two Dirac functions $\phi_U$ and $ψ_U$ , both with support in $U$ , such that the supports of $\phi_U$ and $ψ_U$ are disjoint, hence in particular, $\|\phi_U - \psi_U\|_1 = 2$ for every $n\in \mathbb N$ . How does Urysohn's lemma guarantee the existence of the required Dirac functions? Details would be great. The statement "" $\|\phi_U - \psi_U\|_1 = 2$ for every $n\in \mathbb N$ "" doesn't seem to make sense, there is no $n$ involved at all! What does the author mean? The function $\phi$ being a unit means that we have $\phi ∗ f = f ∗ \phi = f$ for every $f ∈ L^1(G)$ . There exists a unit-neighborhood $U$ , such that one has $\|\phi_U \ast \phi - \phi\|_1 < 1$ and $\|\psi_U \ast \phi - \phi\|_1 < 1$ . Hence $2 = \|\phi_U − ψ_U\|_1 ≤ \|\phi_U − \phi\|_1 + \|\phi − ψ_U\|_1 < 2$ , a contradiction! Hence the assumption is false, and $G$ must be discrete. $\square$ Why does there exist a unit-neighborhood $U$ such that $\|\phi_U \ast \phi - \phi\|_1 < 1$ and $\|\psi_U \ast \phi - \phi\|_1 < 1$ ? $\color{blue}{\text{(Resolved.)}}$ Thanks a lot! Note: All groups in this discussion are Hausdorff. By a Dirac function we mean a function $f\in C_c(G)$ satisfying $f\ge 0$ , $\int_G f(x)\, dx = 1$ , and $f(x) = f(x^{-1})$ for all $x\in G$ . Update $1$ : $Q3.$ is resolved by the following lemma from the text (take $\epsilon = 1$ ): Lemma $1.6.6.$ Let $\epsilon > 0$ . For every $f\in L^1(G)$ there exists a unit-neighborhood $U$ such that for every Dirac function $\phi_U$ with support in $U$ one has $$\|f \ast \phi_U - f\|_1 < \epsilon, \quad\quad \|\phi_U \ast f - f\|_1 < \epsilon$$","['harmonic-analysis', 'proof-explanation', 'banach-algebras', 'general-topology', 'locally-compact-groups']"
4660110,Geometric interpretation of Prime Avoidance Lemma,"I'm doing Gathmann's commutative algebra notes Exercise 2.10: Let $R$ be a ring. (a) Let $I_1,..., I_n\unlhd R$ and $P\unlhd R$ be a prime ideal. If $P\supset I_1\cap\cdots\cap I_n$ , prove that $P\supset I_k$ for some $k=1,...,n$ . (b) Let $I\unlhd R$ , let $P_1,...,P_n\unlhd R$ be prime ideals. If $I⊂ P_1\cup\cdots\cup P_n$ , prove that $I⊂ P_k$ for some $k=1,...,n$ . (c) Show that the statement of (b) still holds if $P_1$ is not necessarily prime (but $P_2,...,P_n$ still are). Can you give a geometric interpretation of these statements? I proved the two statements, but I'm not sure about the geometric interpretations. For (a), since intersection of ideals correspond to union of subvarieties, prime ideals correspond to irreducible subvarieties, it can be interpreted as, if an irreducible subvariety is contained in the union of some subvarieties, then it is contained in one of them. This one is easy to imagaine geometrically, since an irreducible subvariety is, by definition, not a union of subvarieties. For (b), $P_1\cup\cdots\cup P_n$ generates $P_1+\cdots+P_n$ , and the sum corresponds to the intersection of some subvarieties, so the statement can be interpreted as, if a subvariety contains the intersection of some irreducible subvarieties, then it contains one of these subvarieties. I'm not sure about this one because I can's see why it is geometrically true. Suppose we have two circles in $\mathbb A_\mathbb R^2$ which intersect at two points $(a_1,a_2)$ and $(b_1,b_2)$ , then $V(x-a_1,y-a_2,x-b_1,y-b_2)$ is a subvariety contains the two points but not contains any of the circle, so I think my interpretation is wrong. I have seen the comment in this post which seems to be right, but can someone explain more? In particular, why is my translation of the prime avoidance lemma wrong? I'm only learning commutative algebra, so I'm not familiar with algebraic geometry.","['algebraic-geometry', 'commutative-algebra']"
4660114,Why is there no negative infinity in the extended complex plane?,"I'm reading Ravi Agarwal's ""Introduction to Complex Analysis"". He says this: It is often convenient to add the element $\infty$ to $\mathbb{C}$ . The enlarged set $\mathbb{C} \cup \{\infty\}$ is called the extended complex plane. Unlike the extended real
line, there is no $-\infty$ . He uses infinity to discuss the idea of a ""neighbourhood of infinity"", but he defines them without the need for infinity as a set of complex numbers $z$ following $\{z:|z-z_0|>r>0\}$ . What's the point? Why is adding the element $\infty$ convenient? Is there not a better way to discuss the concept of ""neighbourhoods of infinity""? And why not add $-\infty$ ?","['complex-analysis', 'recreational-mathematics', 'elementary-set-theory']"
4660158,Set notation iterating two variables over different lengths,"How does one interpret the following set notation: $$
\{(x_i,y_j): i=1,...,4,j=1,...,6\}
$$","['elementary-set-theory', 'notation']"
4660175,Integrating with respect to Lebesgue measure,"I have taken a course on measure theory, learned all the important limit theorems, construction of the Lebesgue integral and so forth, but what I have never encountered is, how I actually compute a Lebesgue integral w.r.t. a specific given measure.  I.e., how the definition of the measure with respect to which we integrate comes into play when computing the integral. On $\mathbb{R}$ , for example, consider the Borel Lebesgue measure $\lambda((a, b])=b - a$ or the counting measure $\nu(A)=|A|$ (if $A$ is a measurable set). How do you compute the integral of a function $f$ with respect to these measures and where and how do their definitions come into play? I am aware that on compact intervals integrating a function $f$ w.r.t. the Borel-Lebesgue measure is the same as just computing the Riemann Integral, but that is not what I mean. I mean how do you use the specific definition of your measure in the (Lebesgue) integral to compute that integral without resorting to the Riemann Integral (which is anyway only possible if your measure is the Borel-Lebesgue one)? Also, the Borel-Lebesgue measure is defined for intervals, but how do you compute the integral w.r.t. the Borel-Lebesgue measure for an arbitrary measurable set?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4660177,How to find the surface area of the spout of a teapot?,"I am trying to find the surface area of a teapot. I found the surface area of the body of the teapot by taking a picture of it, graphing the picture and then calculating the surface area of revolution. However, I don't know how to find the surface area (or an estimate of the surface area) of the teapot's spout. Here are some pictures of teapots with spouts that looks like the one that my teapot has: I would seriously appreciate it if someone can give me details about how to find the surface area of the spout, or provide links to articles or papers that shows how to do this. Thank you in advance.","['multivariable-calculus', 'calculus', 'mathematical-modeling', 'geometry']"
4660183,A topological Ehresmann's theorem,"A proper local homeomorphism is a covering map (assuming some mild conditions on the involved spaces). I want to know about the following generalization, which I believe is false but cannot come up with a counterexample to. Suppose $f : E \to B$ is proper and locally of the form $U\times V \to V$ , ie every point $e \in E$ has a neighborhood $W \subset E$ of $e$ , some $U = U_e$ and a neighborhood $V \subset B$ of $f(e)$ such that $f(W) \subset V$ and there is a homoeomorphism $W \cong U \times V$ that takes $f$ to the projection. Then $f$ is a fiber bundle. Here I assume $B$ is locally compact, Hausdorff and locally connected (or locally contractible, or a manifold). $B$ should also be connected if your definition of fiber bundle insists that the fibers are constant (and not just locally constant). The map $f$ being proper means: $f^{-1}(K)$ is compact for $K \subset B$ compact. $f$ is separated : $\{(x, y) \mid f(x) = f(y)\}$ is closed in $E \times E$ (or $E$ is Hausdorff). Without this the line with doubled origin mapping to the line is a counterexample to even the covering space case. I am also happy to assume some niceness of $f^{-1}(b)$ for $b \in B$ (it is of course compact by properness). As noted above if $f^{-1}(b)$ is discrete then the statement is true. Even with the strongest variations of the assumptions above (ie $B$ compact manifold, $E$ Hausdorff, fibers of $f$ smooth manifolds), I believe this is false because it would reduce the proof of Ehresmann's theorem (for a proper submersion $f: E \to B$ ) to: Use the ""submersion theorem"" ( $\impliedby$ the implicit function theorem) to get that $f$ is a projection locally in $E$ . Use properness to upgrade that to $f$ being a projection locally in $B$ , ie a fiber bundle. This two step process, decoupling the submersion/smoothness and proper/topological parts, is actually how I think of Ehresmann's theorem, but all proofs I have seen leverage differentiability in patching together the different product-like neighborhoods. Except in the covering space case, where the (local) connectivity assumptions are enough to match up nearby fibers. I did find this MO post about Ehresmann's theorem with a comment by Ryan Budney which seems to say that there is a possibly-long-winded proof that uses not much more than the implicit function theorem. Note that if $f$ is a smooth map of manifolds that is locally (smoothly) a projection then it is automatically a submersion, so the smooth version of the statement is actually equivalent to Ehresmann's theorem. I am looking for either a proof (possibly with stronger topological assumptions on $B$ , $E$ etc) or a counterexample.","['general-topology', 'proper-maps', 'fiber-bundles']"
4660186,"Given a symmetrical function, how to solve so symmetrical point lands on $x$? [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last year . Improve this question Say I have the following curve: $$y = k_1 + \dfrac{k_2}{1-x} \left\{0\leq x\leq1\right\}$$ If I say that: $k_1=0.038$ $k_2=0.002$ I get this: How do I? a) Find the point of symmetry (somewhere in the red area)?´ b) Pick some $k_1$ (like $0.038$ ) Pick some $x$ (like $0.9$ ) Solve for $k_2$ , so that the symmetry point lands on $x$ ?","['calculus', 'functions']"
4660212,What does 1/(2 + 3/(4 + 5/(6 + 7...))) converge to?,I'd like to find a closed-form expression for $$ x = \frac{1}{\displaystyle2 + \frac{3}{\displaystyle4 + \frac5{6+\cdots}}}$$ My own attempt implies $ x = \frac{1}{2} $ . But by numerical approximation the sequence seems to converge to $ \approx 0.379731955 $ .,['sequences-and-series']
4660234,Dirac delta distribution in the complex plane,"In the complex plane, one can write $$\delta(x) \delta(y) = \dfrac{1}{\pi} \partial_z \dfrac{1}{\bar{z}}.$$ How to prove this relation has been answered before and identifies $$ 2\pi \delta(x) \delta(y) = \partial_x \left(\dfrac{x}{x^2 + y^2}  \right) + \partial_y \left(\dfrac{y}{x^2 + y^2}  \right).  $$ However, I have not been able to show that this relation indeed holds. How can I prove that it is indeed true?","['complex-analysis', 'dirac-delta', 'distribution-theory']"
4660267,Measure as a dual of $C_0$,"Let $\mu$ be a nonnegative finite measure on $\Omega \subset \mathbb{R}^n,$ i.e. $\mu(\Omega) < \infty.$ Clearly for any $f \in C_0(\Omega)$ we have $\int\limits_{\Omega}fd{\mu} \leq ||f||_{L^{\infty}(\Omega)} \mu(\Omega).$ Do we also have the following? \begin{equation}
\mu(\Omega)= \sup\limits_{ f\in C_0(\Omega), ||f||_{L^{\infty}}=1 }  \int \limits_{\Omega} f d \mu.
\end{equation} More generally for a signed measure $\mu$ on $\Omega\subset{\mathbb{R}^N}$ (bounded subset), do we have $|\mu|(\Omega)=||\mu||_{\Omega},$ where $||.||_{\Omega}$ dentes the norm on $\mathcal{M}(\Omega)$ as a dual of $C_0(\Omega)?$","['measure-theory', 'functional-analysis', 'analysis']"
4660281,Is the change of base scheme $\operatorname{Spec} K \to \operatorname{Spec} k$ relevant?,"Let $X$ be a $K$ -scheme (affine if necessary), $K|k$ an extension of fields (separable, finite, ... as necessary).
Then $k \hookrightarrow K$ is unique as a homomorphism of $k$ -algebras and so $\operatorname{Spec} K \to \operatorname{Spec} k$ is unique as a morphism of $k$ -schemes. In this way, $X$ has the unique associated structure morphism $X \to \operatorname{Spec} K \to \operatorname{Spec} k$ , which makes $X$ into a $k$ -scheme. Is this construction helpful for anything or do you know of a place where it is used? I have not seen it used so far, but am new to the language of schemes and may be missing something.","['algebraic-geometry', 'schemes']"
4660289,Placement of critical points of Morse function on the sphere,"On a similar note to my previous question , I am still thinking about functions on the sphere. Assume I have a $C^2$ function on the sphere $ f : S^{d-1} \mapsto \mathbb{R} $ which is Morse (i.e. all critical points are non-degenerate and thus all critical points are isolated) and assume that this function has at least one saddle point. Now, if we say $ d = 3 $ , if I am not mistaken, then we will actually have at least two saddles, since by the Poincare-Hopf theorem, the sum of the indices of any vector field must sum up to its Euler characteristic, which in this case is 2. Now, my question is, are there any restrictions regarding where these critical points are exactly on the sphere (when $ d = 3 $ , but also in general) ? Can they be arbitrarily close to each other? If there are some restrictions, what exactly is causing them? Again, apologies if my question is really basic, as I am absolutely unknowledgeable about differential topology. Any references or comments are appreciated!","['differential-topology', 'morse-theory', 'smooth-manifolds', 'differential-geometry']"
4660319,Are inf and sup continuous functionals in general?,"Let $X$ be any topological space and $\bar{\mathbb{R}} = [-\infty, \infty]$ with the standard topology. Is it true, in general, that the functionals $$\inf: C(X,\bar{\mathbb{R}}) \to \bar{\mathbb{R}}, f \mapsto \inf f$$ $$\sup: C(X,\bar{\mathbb{R}}) \to \bar{\mathbb{R}}, f \mapsto \sup f$$ are continuous, if $C(X,\bar{\mathbb{R}})$ is equipped with the compact-open topology? If $X$ is compact, it should be true, if the following argument is correct (wlog. proof only for $\inf$ and $X \neq \emptyset$ ): Let $a \in \mathbb{R}$ arbitrary. Then $\inf^{-1}([-\infty,a)) = \bigcup_{x\in X} \langle x,[-\infty,a) \rangle$ is open. On the other hand $\inf^{-1}((a,\infty]) = \bigcup_{\varepsilon > 0}\langle X, (a+\varepsilon, \infty] \rangle$ , which is open, because $X$ is compact. $[-\infty,a)$ and $(a,\infty]$ build a subbase of $[-\infty,\infty]$ . Therefore $\inf$ is continuous. What happens for arbitrary spaces $X$ , or locally compact, etc.?","['continuity', 'general-topology', 'supremum-and-infimum']"
4660329,Example of graph with strange property,"The question is now also published in MathOverflow (here) . Note: Whenever I mention a coloring of a graph I'm referring to a proper coloring over its vertices using the least amount of colors. Pondering on graph coloring I came across a strange class $\mathcal G$ of problematic graphs. A graph $G$ is in $\mathcal G$ if it has the following property: For any vertex $v$ , any coloring $c$ of $G$ and any color $\alpha\in c[N(v)]$ there is a coloring $k$ of $G$ such that There is no vertex $u\neq v$ such that $k(u) = k(v)$ $k^{-1}[\alpha]\cap N(v) = c^{-1}[\alpha]\cap N(v)$ That is, given a coloring, we can always find another one that preserves a chosen color in the neighborhood of a chosen vertex while making the color of this vertex unique in the graph. The thing is: I don't know any graph with this property, except by the complete ones. It's easy to show that, if such a graph has chromatic number one, two or three, then it must be complete. Is it also true for higher chromatic numbers? Can you give me an example of a graph in this class that is not complete? One possible strategy to approach this problem is trying to determine properties of such graphs (to hopefully prove they must be complete or filter searches for an example). It's not very hard to see that, if $G$ is in $\mathcal G$ and $v$ is a vertex of $G$ , then $\chi(G-v) = \chi(G)-1$ . and $e$ is an edge of $G$ , then $\chi(G-e) = \chi(G)-1$ . that is, $G$ is vertex and edge minimal. Those seem to be the most natural properties to find. As pointed out in a comment, it is also possible to show that, if $\chi(G)>2$ , then each vertex of $G$ belongs to a triangle. This result can be improved: given a vertex $v$ of $G$ , there can not be a set of more than $\deg(v)-\chi(G)+2$ independent vertices in $N(v)$ . Upon closer inspection, I realized the interesting class of problematic graphs is actually (probably) narrower then I previously thought. Let's call it $\mathcal H$ . A graph $G$ is in $\mathcal H$ if it has the following property: For any vertex $v$ and any nonempty set $S\subset N(v)$ of independent vertices there is a coloring $c$ of $G$ such that There is no vertex $u\neq v$ such that $c(u) = c(v)$ $S = c^{-1}[\alpha]\cap N(v)$ for some color $\alpha$ It's clear that $\mathcal H\subset \mathcal G$ , so the previously mentioned properties must still apply. It's not clear if $\mathcal H\neq \mathcal G$ . However, I'm only interested in knowing if there is a non-complete graph in $\mathcal H$ .","['graph-theory', 'combinatorics', 'coloring']"
4660355,How to Check monotonicity of a function,"Given a function, $f$ be defined on $[0,1]$ by $$f(x) = \begin{cases} 0 & x=0 \\ \frac1{2^{n-1}} & \frac1{2^n}\lt x\le\frac1{2^{n-1}}, n\in\Bbb N\end{cases}$$ If I take $n =1$ , then $f(x) = 1$ and $\frac{1}{2} \lt x \le 1$ If $n =2$ , $f(x) = \frac{1}{2}$ and $\frac{1}{2^{2}} \lt x \le \frac{1}{2}$ If $n=3$ , $f(x) = \frac{1}{2^{2}}$ and $\frac{1}{2^{3}} \lt x \le \frac{1}{2^{2}}$ How should I find if the function is monotonically  increasing or decreasing? Really don't know!","['functions', 'monotone-functions', 'real-analysis']"
4660393,Series of polynomials very nearly follows binomial coefficients but doesn't quite,"I'm modelling a system using a Markov chain and by a few iterations of the transition matrix I can see a pattern emerging in the resulting polynomial that really looks like Pascal's triangle, but isn't quite. I've been out of the mathematics world for a little while now so I may be missing quite obvious, but this feels like the sort of thing that should be able to wrap very neatly up into a little summation or something. Ideally I want a simple expression in terms of $n$ and $\mu$ . $$T=\begin{bmatrix}
\mu^2-2\mu+1 & \mu-\mu^2 & \mu-\mu^2 & \mu^2\\ 
0 & 1-\mu & 0 & \mu\\ 
0 & 0 & 1-\mu & \mu\\
0 & 0 & 0 & 1
\end{bmatrix}$$ $$i=\begin{bmatrix}
1\\ 
0\\ 
0\\ 
0
\end{bmatrix}$$ Basically what I've done so far is take my transition matrix $T$ and raise it to the powers 1-4, then left multiplying by the initial state vector $i$ . The first term of the resulting state vector $p$ is pretty clearly $(\mu^2-2\mu+1)^n$ , but the other ones are much bigger. At the moment I'm only interested in the final term $p_4$ , so I'll only show that one. Here's the polynomials in expanded form: $$\mu^2\\
\mu^4-4\mu^3+4\mu^2\\
\mu^6-6\mu^5+15\mu^4-18\mu^3+9\mu^2\\
\mu^8-8\mu^7+28\mu^6-56\mu^5+68\mu^4-48\mu^3+16\mu^2$$ There's clearly a pattern here, the powers are going up by two each time, and each coefficient alternates sign, but what I'm trying to figure out is why the last few coefficients of each polynomial don't seem to follow the Pascal's triangle pattern. In each equation the middle coefficient is 2 smaller than the binomial coefficient should be, and then the ones after that are different in a way I can't seem to find a pattern for. I was hoping someone might be able to point me in the right direction on this, I feel like there must be a way to write a general expression for this but I can't put my finger on it.","['markov-chains', 'matrices', 'binomial-coefficients', 'polynomials', 'stochastic-matrices']"
4660401,Mazur's theorem for general number fields,"Mazur's theorem completely classifies the possibilities of $E_{tors}(\mathbb{Q})$ for an elliptic curve $E/\mathbb{Q}$ , including the fact that only finitely many groups occur. What happens with $E_{tors}(k)$ for $E/k$ with $k$ a general number field? A bit of cursory searching suggests that at least the quadratic case is well-understood, but I don't know what the current state of the art is or how the proof of that case works. More specifically, my understanding of the proof of Mazur's theorem is that it involves proving that (1) $Y_1(N)$ parametrizes elliptic curves containing points of order exactly $N$ ; (2) these points are actually defined over $\mathbb{Q}$ ; (3) these rational points exist iff $X_1(N)$ has genus $0$ . Do those ideas extend to more general cases?","['number-theory', 'algebraic-geometry', 'elliptic-curves', 'arithmetic-geometry']"
4660430,Is this statement correct : $\frac{dv}{dt}\times dx=\frac{dx}{dt}\times dv$,"So I just came across a physics derivation where the treat the $dv$ and $dx$ operators like fractions while I have always heard it's a mistake. But so far what I came up with: $$\begin{align*}
\frac{dv}{dt}\times dx &= \lim_{h\to0} \frac{v(t+h)-v(t)}h \times (x(t+h)-x(t)) \\
&= \lim_{h\to0} \frac{x(t+h)-x(t)}h \times (v(t+h)-v(t)) \\
&= \frac{dx}{dt}\times dv
\end{align*}$$ Is my reasoning correct? If not please explain what is the correct way to think about this.","['differential', 'calculus', 'limits', 'algebra-precalculus', 'derivatives']"
4660475,Prove that $J=JH=HJ$,"The question asks for the idempotency of \begin{align*}
H-\frac{1}{n}J
\end{align*} where $H$ is the hat matrix for multilinear regression, $H = X(X'X)^{-1}X'$ , and $J$ is the matrix of all 1's. I proceeded by definition, i.e., wanting to show that $(H-\frac{1}{n}J)(H-\frac{1}{n}J)= H-\frac{1}{n}J$ , expanding it I got: \begin{align*}
H-\frac{1}{n}(HJ+JH)+\frac{1}{n}J
\end{align*} where we can find that $HJ=JH$ , knowing that any element of the i-th row of $HJ$ is the sum of i-th row of $H$ ; any element of j-th column of $JH$ is the sum of j-th column of $H$ , combining with the fact that $H$ is symmetric. How can I proceed from up here? It seems necessary that $HJ=J$ , but how do we prove it?","['matrices', 'statistics', 'linear-regression']"
4660567,Non-unitary isometry and a norm equality,"I am looking at a paper which asserts the following equality relating a non-unitary isometry. There is no explanation given for this, and I cannot figure out why this is true: Here is the proposition: Let $A$ be a unital $C^*$ algebra (some norm closed subalgebra of $B(H)$ where $H$ is a Hilbert space, containing the identity), and let $v$ be a non-unitary isometry. Moreover let $\lambda, \rho$ be positive scalars satisfying $0< \lambda, \rho < 1$ . Then we have the following claim, Claim: $\left|\left|{\rho v - \lambda } \right|\right|= \rho + \lambda$ . I can't seem to find out why this is true (and it certainly does not seem obvious to me). I cannot even show this for the unilateral shift (i.e. the map that sends $e_i \rightarrow e_{i+1}$ for $i \in \mathbb{N}$ on $\ell^2(\mathbb{N})$ ). Here are some things that I tried: First that the inequality $\lvert\lvert {\rho v - \lambda } \rvert\rvert \leq \rho + \lambda$ is obvious. Thus if our $C^*$ algebra is isometrically isomorophic to some subalgebra of $B(H)$ for some Hilbert space $H$ , then if we choose normalized $x \in ran(v)^{\perp}$ then $\lvert\lvert {\rho v(x) - \lambda(x) } \lvert \lvert = \sqrt{\rho^2+\lambda^2}$ , but this is not enough to show the equality. Another hope is to use the $C^{*}$ property and write $\lvert\lvert {\rho v - \lambda } \lvert \lvert ^2 = \lvert\lvert ({\rho v - \lambda })^{*} ({\rho v - \lambda })\lvert \lvert = \lvert\lvert \rho^2+ \lambda^2 - \rho \lambda (v+v^*)\lvert \lvert $ and say something about this quanity and maybe use spectral properties (i.e. use the spectral theorem in a meaningful way) of the self adjoint operator $v+v^*$ , but I am unsure how to proceed.","['von-neumann-algebras', 'operator-algebras', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
4660584,Differentiable function: where am I wrong?,"Consider the function $$f(x, y) = \frac{x^4y^3}{x^8 + y^4}$$ not at the origin, but $0$ at the origin. I already provd the continuity of the function at the origin. I have to show if it's differentiable. Considering some paths, I keep getting $$\lim_{(x, y) \to (0, 0)} \frac{1}{\sqrt{x^2+y^2}} \frac{x^4y^3}{x^8 + y^4} =0$$ Yet when proving in serious way the differentiability, I get stuck. Here is what I did: \begin{equation}
\begin{split}
\vert \frac{x^4y^3}{(x^8+y^4)\sqrt{x^2+y^2}} \vert & \leq \frac{x^4y^2|y|}{(x^8+y^4)\sqrt{x^2+y^2}}\\\\
& \leq \frac{x^4y^2|y|}{\sqrt{x^2+y^2}2x^4y^2}
\\\\
& = \frac{|y|}{\sqrt{x^2+y^2}}
\\\\
& \leq \frac{\sqrt{y^2}}{\sqrt{x^2+y^2}}
\\\\
& \leq \frac{\sqrt{x^2+y^2}}{\sqrt{x^2+y^2}}
\\\\
& = 1
\end{split}
\end{equation} From line two to line three I used AM/GM: $x^8 + y^4 \geq 2\sqrt{x^8y^4} = 2x^4y^2$ and I reversed So, am I wrong or is the function not differentiable at zero?","['multivariable-calculus', 'solution-verification', 'derivatives', 'real-analysis']"
4660593,Closed form of $\begin{align}\int_{0}^{\infty}x^{1-x^{2-x^{3-...}}}dx\end{align}$.,"I want to find the closed form of $\begin{align}\int_{0}^{\infty}x^{1-x^{2-x^{3-...}}}dx\end{align}$ . Quick disclaimer: I have no reason to believe one actually exists Using Desmos, the closest I have gotten is $1.2421832267$ . I have noticed that when it is repeated an odd amount of times, for example $x^{1-x^{2-x^3}}$ the integral does not converge and I would have to change it to $x^{1-x^{|2-x^3|}}$ . Is there any way to avoid having to do this (perhaps with a slightly altered equation)? My question is still for the closed form. I first tried to find the closed form by putting it in Desmos and then plugging the decimal it gave me into WolframAlpha in hopes of getting a closed form but it didn't give me anything, even after I wrote a Python script so I could copy and paste 200 repetitions into Desmos in an attempt to get a more accurate decimal. Here is my other approach: The closed form of $\begin{align}\int_{0}^{\infty}x^{1-x^{2-x^{3-...}}}dx\end{align}$ is equal to $\begin{align}\lim_{a\to\infty}\int_{0}^{a}x^{1-x^{2-x^{3-...}}}dx\end{align}$ . This is where I am stuck. Thanks in advance for the help!","['integration', 'power-towers', 'improper-integrals', 'recursion']"
4660598,PDE inequalities to show a gradient is integrable,"I'm having trouble understanding the two inequalities in page 9 , quoted below. From the fact that $u\in C^{0,\alpha}(M)$ , apply $W^{2,p}$ estimate followed by Sobolev embedding onto $w_r(x)$ , where $r>0$ fixed, in a (conic) annulus $A(1)$ around $p$ , we have $$|\nabla u|_{C^0(A(r))}\leq |\nabla\nabla u|_{L^p(A(r))}\leq Cr^{\alpha-1}.$$ $|\nabla u|$ is therefore integrable on $\partial M$ and also on $M$ . Unless I missed it, I don't think the author defines $w_r(x)$ , which is one minor area of confusion. In any case, we're estimating on the boundary of $M$ , and $\dim\partial M=2$ . Note that we can't immediately get an $L^1$ bound on $\nabla u$ by GNS since $$|\nabla u|_{L^1}\leq |\nabla\nabla u|_{L^p}\implies 1=\frac{2p}{2-p}\implies p=2/3.$$ I'm not sure what the author means by ""apply $W^{2,p}$ estimate followed by Sobolev embedding"", which I assume gives the first inequality. I record a few thoughts below. I assume the domains are $A(r)$ , which I will omit from the notation. We can use GNS inequality for $1\leq p<2$ , to relate $\nabla u$ to its derviative. For $n=2$ , we get $$|\nabla u|_{L^{\frac{2p}{2-p}}}\leq |\nabla\nabla u|_{L^p}.$$ I am unsure how to proceed from here. I definitely don't think it's true that $$|\nabla u|_{C^0}\leq C|u|_{L^\frac{2p}{2-p}}.$$ We want to apply one of the following argument to $v=\nabla u$ , and this is possible if $u\in W^{3,2}$ . But $u\in W^{3,p}$ on the interior (see Lemma 1.1, page 3 ), not boundary, of $M$ . By the general Sobolev inequalities , for $n=2$ , we know that $v\in W^{2,p}$ with $p=2,3,4,...$ , then for any $\alpha\in (0,1)$ , we have $$|v|_{C^{0,\alpha}}\leq C|v|_{W^{2,i}}.$$ Alternatively, for $1\leq p< 2$ , for $\alpha=2-2/p$ , we get $$|v|_{C^{0,\alpha}}\leq C|v|_{W^{2,p}}.$$ Alternatively, for $p>2$ not an integer, we have for $\alpha=1-2/p$ , $$|v|_{C^{0,\alpha}}\leq C|v|_{W^{2,p}}.$$ For the second inequality, I'm assuming this comes from the domain being a conic annulus, and somehow $u\in C^{0,\alpha}$ should enter. But I'm not clear how this is done.","['partial-differential-equations', 'differential-geometry']"
4660689,How do you know whether this graph is 3-colorable or not?,"How do you know that this graph is or is not 3-colorable? If it is not, I don't understand how I would prove that either. I was told the chromatic number is 4 but I am still unsure on how to prove that.","['graph-theory', 'combinatorics', 'coloring']"
4660713,Calculus operators in Laplace space?,"I was playing with Laplace transforms and found something curious. Suppose we have an expression $4x+3$ that we want to take the derivative of or the integral of. If we instead took its Laplace transform, we have $$\mathcal{L} [4x+3](p)= \frac{3p+4}{p^2}$$ Now, let's multiply this and divide this by $p$ , and then invert the transform. We can see that $$\frac{3p+4}{p^2}\cdot p = \frac{3p+4}{p}$$ $$\mathcal{L}^{-1}\left[\frac{3p+4}{p}\right](x) = 3\delta(x)+4$$ $$\frac{3p+4}{p^2}\cdot \frac1p = \frac{3p+4}{p^3}$$ $$\mathcal{L}^{-1}\left[\frac{3p+4}{p^3}\right](x) = 2x^2+3x$$ I noticed an odd thing. The results are quite similar to taking the integral in the divide case, and taking a derivative in the multiplying case. There's an extra dirac delta function which gives me doubts however, so I tried with something different. Using $\sin(2x)+3$ gives me a similar result. If we take its Laplace transform, multiply/divide by $p$ , and then invert the transform, we get $3\delta(x)+2\cos(2x)$ from multiplying $p$ , and $3x-\frac12\cos(2x)+\frac12$ from dividing by $p$ , which in the vein of integration can be said to be the same as the indefinite integral since we just collect constants. Clearly, there seems to be some relationship between dividing in Laplace space and integrating in real space, and vice versa, but it doesn't seem it's exactly the same either, since there's extra constants and Dirac delta functions. I'm guessing I'm supposed to account for some type of initial condition? but i'm not sure. Any ideas on what's going on?","['integration', 'calculus', 'derivatives', 'laplace-transform']"
4660716,Does every extension of a finite group by $\mathbb{R}^n$ split?,"Suppose $G$ is a topological group containing a closed normal subgroup $N$ isomorphic to $(\mathbb{R}^n, +)$ such that $G/N$ is finite. Is $N$ a semidirect factor? Equivalently, does $G$ contain a finite subgroup with the same cardinality as $G/N$ ? This would be enough, because $N$ is torsion-free. I believe this is true because, more generally, any extension of a compact Lie group by a vector group splits (Iwasawa 1949). But I wonder if one can avoid using Haar measure etc. when the quotient is finite.","['topological-groups', 'group-extensions', 'group-cohomology', 'group-theory', 'lie-groups']"
4660727,"How to find this LCM sum function? $ \text{lcm}(m,n) +\text{lcm}(m+1,n) +\cdots+\text{lcm}(n,n)$","Problem: $$S=\text{lcm}(m,n) + \text{lcm}(m+1,n) +\ldots+ \text{lcm}(n,n)$$ There is already a question on math.stackexchange for $[1,n]$ range . I am trying to generalise it further for $[m,n]$ range. My thoughts: $\operatorname{lcm}(m, n) + \operatorname{lcm} (n-m, n) = \frac{ mn } { \gcd(m, n)} + \frac{ (n-m)n} { \gcd(n-m, n)} = \frac{ n\times n} { \gcd(m,n) }$ .  should hold here too We can write the required series as, $S = \sum_{i=m}^{n} \frac{i.n}{gcd(i,n)}=n\sum_{i=m}^{n} \frac{i}{gcd(i,n)}$ $ A332049(n) = \sum_{i=1}^{n-1} \frac{i}{gcd(i,n)}$ $\implies A332049(n) + 1 = \sum_{i=1}^{m-1} \frac{i}{gcd(i,n)} +  \sum_{i=m}^{n} \frac{i}{gcd(i,n)} $ $\implies A332049(n) + 1 = \sum_{i=1}^{m-1} \frac{i}{gcd(i,n)} +  S/n $ $\implies A332049(n) + 1 - \sum_{i=1}^{m-1} \frac{i}{gcd(i,n)} =    S/n $ A332049 above refers to this oeis series As evident we didn't benefit much from this substitution. As splitting additively into two didn't help, My hunch is we might need to split the series into two or more multiplicatively/recursively. And also I feel sum or difference of totients might come into play rather than mere totients for this. I am smelling possibility of principle of inclusion exclusion. Another way I am thinking is, for all $d$ in divisors of $n$ $S=$ $ \sum_{d|n}(n/d)[ (\varphi(n/d) - \sum_{i=1}^{m-1}|(gcd(i,n)=d)| ]$ I don't know how to evaluate the second part efficiently Summarise: Be it $\sum_{i=1}^{m-1} \frac{i}{gcd(i,n)} $ or $\sum_{i=1}^{m-1}|(gcd(i,n)=d)| $ whicheverso closed form you can suggest would do the job.","['number-theory', 'summation', 'totient-function', 'sequences-and-series']"
4660754,Can an arbitrary 3-d shape be fitted inside a cube so it touches all the sides?,"In 2-d space, it is possible to take any shape and fit it inside a square such that it touches all the sides of the square. In other words, its projection on the x-axis is the same as its projection on the y-axis. To see this, we can consider a function which is the ratio of its projection on the x-axis to its projection on the y-axis. As we rotate by $\frac{\pi}{2}$ , this ratio will go from one side of $1$ to the other. Because it is a continuous function, it must cross $1$ . I was wondering about extending this result to 3-d space. I have a general 3-d object and would like to rotate it such that its projection along all three axes becomes the same. Can we prove this is always possible or otherwise? And if it isn't possible in general, any non-trivial special cases where it will be?","['geometry', 'rotations']"
4660755,Proving $(a → (b → c)) ∧ (∼ c) ≡ (a → ∼ b) ∧ (∼ c)$ confusion.,"I have the following statement that I want to prove: $(a → (b → c)) ∧ (∼ c) ≡ (a → ∼ b) ∧ (∼ c)$ I think I can prove this using the law of equivalences, however I also noticed that both statements, the LHS and the RHS has a ∧ (∼ c) at the end. So is it fine that I conclude $(a → (b → c)) = (a → ∼ b)$ ? Can someone please explain why or why not I can do this? I think I have done this before in normal algebra but not with statement variables.",['discrete-mathematics']
4660806,Justifying an inequality,"I am reading Banach Spaces of Analytic Functions by Hoffman. In Page 52, the authors claim this inequality as passing remark: Let $f \in L^1 (\mathbb T)$ and $f(e^{i0})\ne 0$ and suppose that $$\frac{1}{2\pi}\int \log (|f| +\varepsilon) d\theta \ge \log |f(e^{i0})| \qquad  \forall \varepsilon >0$$ We can then show that: $$\frac{1}{2\pi}\int \log (|f|) d\theta \ge \log |f(e^{i0})|$$ I initially thought of using Fatou's inequality to prove this but I do not think that will work here. Is there any other way passing the limit inside the integral here? EDIT I think I have found the proof but I would like some verification. Note that: $$1 = \log e \le\log (|f| + e) \le |f| +e$$ Hence, $\log (|f| + e)$ is integrable. We have that $\log(|f| + 1/n) \le \log (|f|+e)$ for every $n \in \mathbb N$ by assumption. Thus, we have that by Dominated Convergence theorem that $$\int \log(|f|) \frac{d\theta}{2\pi}= \lim_{n\to \infty} \int \log(|f| + 1/n) \frac{d\theta}{2\pi}$$","['measure-theory', 'solution-verification', 'proof-writing', 'inequality']"
4660828,"If $a_n=\sin(n^2)$, how to construct (if possible) a subsequence such that $\lim a_{n_k}\rightarrow 0$?","For $a_n=\sin(n)$ , it can use the continued fraction method to construct the subsequence, so that $\lim a_{n_k}\rightarrow 0$ . But for $a_n=\sin(n^2)$ , how to construct (if possible) a subsequence such that it converges to $0$ ?",['analysis']
4660844,Is $xf'(x)/f(x)$ increasing if $f$ is increasing and convex?,"Let $f:\mathbb{R}^+\to\mathbb{R}^+$ be such that $f(0)=0$ , $f'(x)>0$ and $f''(x)>0$ (increasing and convex). For $K\geq 0$ let \begin{align}
g(x) = \frac{xf'(x)}{K+f(x)}.
\end{align} Is the function $g$ increasing? It is increasing in standard cases like $f(x)=x^{\alpha}$ for $\alpha\geq 1$ , and $f(x)=e^{x}-1$ . Are there obvious counterexamples? If $K=0$ , then $g'(x)>0$ is equivalent to \begin{align}
x\frac{f''(x)}{f'(x)} \geq x\frac{f(x)}{f'(x)} - 1.
\end{align} Is this an interpretable condition? I have seen $x\frac{f''(x)}{f'(x)}$ referred to as the curvature of a function.","['convex-analysis', 'derivatives', 'real-analysis']"
4660852,What is the arc length formula in a metric space?,"Let $f(x):[a,b]\longrightarrow \mathbb{R}^n$ be injective and continuously differentiable curve. Then the arc length is given by $$\int_a^b |f'(t)|dt$$ . What will be the arc length formula if $\mathbb R^n$ is replaced by a metric space $(X,d)$ . I found the definition of metric derivative here . Is the arc length formula given by $$\int_a^b \lim_{s\to0}\frac{d(f(t+s),f(t)}{|s|}dt$$ ,
if the limit and integral exists? Does it make sense at all?","['metric-geometry', 'differential-geometry', 'metric-spaces', 'real-analysis']"
4660854,Understanding an the last step in proving the Poincaré inequality for smooth functions on $\mathbb{R}^n$.,"I'm looking at the following: Let $u:X \to \mathbb{R}$ be an integrable and smooth function on a subspace $X \subseteq \mathbb{R}^n$ equipped with the Lebesgue measure $\lambda_n$ , and let $B \subseteq X$ be an open ball. $C$ and $R$ are constants. $$
\int_B \int_B |u(x)-u(y)|\mathrm{d}y\mathrm{d}x \leq C R^{n+1} \int_B |\nabla u(x)| \mathrm{d}x
$$ Supposing that this inequality is true (I've been able to derive this), how does one deduce the Poincaré inequality for smooth functions on balls in $\mathbb{R}^n$ , namely $$
\frac{1}{\lambda_n(B)} \int_B |u(x)-u_B|\mathrm{d}x \leq C(n) \cdot \operatorname{diam}(B) \cdot \frac{1}{\lambda_n(B)} \int_B |\nabla u(x)| \mathrm{d}x
$$ where $u_B := \frac{1}{\lambda_n(B)}\int_B u(y) \mathrm{d}y$ is the mean value of $u$ in $B$ , and $C(n)$ is some positive real constant depending only on $n$ ?","['integration', 'measure-theory', 'lebesgue-measure', 'metric-spaces', 'inequality']"
4660860,Does $\partial\bar{\partial}+\bar{\partial}\partial=0$ imply integrability of the almost complex structure?,"Let $X$ be an almost complex manifold. A well-known result says that $\bar{\partial}^{2}=0$ implies the integrability of the almost complex structure. My question is what about $\partial\bar{\partial}+\bar{\partial}\partial=0$ ? Does it imply the integrability? I have proved that $\partial\bar{\partial} f+\bar{\partial}\partial f=0$ always holds for any function $f$ , but does it hold for any form?","['complex-geometry', 'differential-geometry']"
4660861,Is there analog of angle in complex domain?,"Given a real vector space with an inner product, i. e. a positive definite symmetric bilinear form $\beta$ , I can define angle between any two nonzero vectors $v_1$ , $v_2$ as $\arccos\frac{\beta(v_1,v_2)}{\sqrt{\beta(v_1,v_1)\beta(v_2,v_2)}}$ . The angle will not change if I multiply the vectors by nonzero real numbers of the same sign, and will become replaced by the complementary angle if these numbers have opposite signs. The angle is also invariant under the action of the group $\operatorname{O}(\beta)$ of $\beta$ -preserving linear transformations. What are analogs of all this for complex vector spaces? I suppose that one has to take a Hermitian form for $\beta$ but the above expression under $\arccos$ will now be a complex number. Shall I take its argument? What happens under the action of the unitary group?","['euclidean-geometry', 'inner-products', 'hilbert-spaces', 'linear-algebra', 'complex-numbers']"
4660889,Evaluating $\int \sqrt{\frac{x^2+1}{x^2(1-x^2)}}dx$,"Today I came across the following integral $$\int \sqrt{\frac{x^2+1}{x^2(1-x^2)}}dx$$ Here's my work: $$\begin{align}\int \sqrt{\frac{x^2+1}{x^2(1-x^2)}}dx& = \int\frac{1}{x} \sqrt{\frac{1+x^2}{1-x^2}}\ dx \\ & =  -\int \sqrt{\frac{1+\cos(2\theta)}{1 - \cos(2\theta)}} \tan(2\theta) \ d\theta\tag{$*$}\\& = -\int \sqrt{\frac{2\cos^2(\theta)}{2\sin^2(\theta)}}\cdot \frac{\sin(2\theta)}{\cos(2\theta)} \ d\theta\\& = - \int\frac{\cos(\theta)}{\sin(\theta)}\cdot \frac{2\sin(\theta)\cos(\theta)}{\cos^2\theta - \sin^2\theta}\ d\theta\\& = - \int \frac{(\cos^2\theta + \sin^2\theta) + (\cos^2\theta - \sin^2\theta)}{\cos^2\theta - \sin^2\theta}\ d\theta\\& = - \int \sec(2\theta) + 1 \ d\theta\tag{1}\\& =-\frac{1}2 \ln|\sec(2\theta) + \tan(2\theta)| + \theta + C\tag{2}\\& = - \frac{1}{2} \ln\left|\sec(\cos^{-1}(x^2)) + \tan(\cos^{-1}(x^2))\right|  + \frac12 \cos^{-1}(x^2) + C\\& = - \frac12\ln\left|\frac1{x^2} + \frac{\sqrt{1 - x^4}}{x^2}\right| + \frac12 \cos^{-1}(x^2) + C\\& = - \frac12\ln\left|\frac{1 + \sqrt{1 - x^4}}{x^2}\right| + \frac12 \cos^{-1}(x^2) + C\end{align}$$ $(*)$ Here I've made a substitution $x^2 = \cos(2\theta)$ so that $\dfrac{dx}{x} = - \tan(2\theta)\ d\theta$ . But this answer seems to be wrong! I differentiated my answer but didn't get the original integrand. Wolframalpha results this expression and I'm not sure about it. I'm just unable to figure out what's actually wrong with my method. Edit: With Bob Dobbs's comment, I got that I did wrong moving from step $(1)$ to step $(2)$ .","['integration', 'indefinite-integrals', 'calculus']"
