question_id,title,body,tags
3500079,Is the nth time derivative of position ever really zero?,"Change in velocity comes from acceleration, but that acceleration was also a leap that started from nothing. It was zero, then positive, and then something else. So it's application must have been pushed by some other, more interior, and higher-derivative force. But the force that pushed this positive acceleration into existence, it also arose from zero. So it must have a higher-order parent that produced it. And so on. So is any nth time derivative of position ever really zero? Doesn’t any change in constant velocity rest finally on an infinite time derivative of position?",['derivatives']
3500125,"ways in which $A,B$ refuse to be the member of same team","Consider a  class of $5$ Girls and $7$ boys . The number of different teams consisting of $2$ girls and $3$ boys that can be formed from this class , If there are two specific boys $A$ and $B,$ who refuse to be the member of same team, is what i try Method $(1)$ Ways in which $A,B$ not a member = Total -ways in which both $A,B$ included $$=\binom{5}{2}\cdot \binom{7}{3}-\binom{5}{2}\cdot \binom{5}{1}=300$$ Method $(2)$ Ways in which $A,B$ not a member $$=\binom{5}{2}\cdot \binom{5}{3}=100$$ above we have excluded $2$ boys because they are not a member But answer given is $300$ please explain me How i am wrong in $(2)$ Method",['combinatorics']
3500150,"Relationship between $\mathbb P_{x \sim \mathcal N(x_0,\sigma^2 I)}(x \in A)$ and distance of $x_0$ to $A$","Let $A$ be a non-empty measurable subset of euclidean $\mathbb R^n$ and $x_0 \in \mathbb R^n\setminus A$ . Define $d(x_0,A) := \inf_{a \in A} d(x_0,a)$ , the distance of $x_0$ from $A$ . Finally, let $\sigma > 0$ . Question 1. Are there any interesting relationships (functional equalities, inequalities, etc.) between $d(x_0,A)$ and $p(x_0,A) := \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A)$ ? Notes I'm fine with partial answers to Question 1 which make ""reasonable"" assumptions on $A$ (convex, closed, smoothness, curvature, etc.) Ultimately, I'm interested in estimating $p(x_0,A)$ . The sought-for connection with $d(x_0,A)$ is just a rough guess (inspired by the case of half-spaces worked-out below). It may turn out that other quantities are petinent for the estimation are in fact, not $d(x_0,A)$ in general. Motivating example: half-spaces Say $b \in \mathbb R$ , $u \in \mathbb R^n$ with $\|u\| = 1$ , and $A:=\{x \in \mathbb R^n \mid x^Tu \le b\}$ , a half-space, and let $x_0 \in \mathbb R^n\setminus A$ . Note that $d(x_0,A)=x_0^Tu-b$ . Let $\Phi$ be the CDF of the standard 1D Gaussian distribution. Then one easily computes $$
\begin{split}
p(x_0,A) &= \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A) = \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x^Tu \le b)\\
&= \mathbb P_{z \sim \mathcal N(0,I)}(z^Tu \le (b-x_0^Tu)/\sigma)=\Phi(-d(x_0,A)/\sigma),
\end{split}
\tag{*}
$$ or equivalently, $d(x_0,A) = -\sigma\Phi^{-1}(p(x_0,A))=\sigma\Phi^{-1}(1-p(x_0,A))$ . Edit: rough upper-bound on $p(x_0,A)$ via closed convex hulls Suppose $A$ is closed convex nonempty subset of $\mathbb R^n$ .
Let $H_0$ be the half-space containing $A$ , bordered by the hyper-plane with outward normal  pointing in the direction of $x_0-a_0$ , where $a_0$ is the (unique!) point of $A$ such that $d(x_0,A) = d(x_0,a_0)$ . In other words, $$
H_0 := \{x \in \mathbb R^n \mid (x-a_0)^Tu_0 \le 0\} = \{x \in \mathbb R^n \mid x^Tu_0 \le b_0\},
$$ where $u_0 := (x_0-a_0)/d(x_0,A)$ , a unit vector in $\mathbb R^n$ and $b_0 := a_0^Tu_0 \in \mathbb R$ . Note that $d(x_0,A) = d(x_0,H_0) = x_0^Tu_0-b_0$ . Now, if $x \sim \mathcal N(x_0,\sigma^2 I)$ , we have $$
\mathbb P(x \in A) \le \mathbb P(x \in H_0) \overset{*}{=} \Phi(-d(x_0,A)/\sigma),
$$ i.e $p(x_0,A) \le \Phi(-d(x_0,A)/\sigma)$ , or equivalent, $d(x_0,A) \ge \sigma\Phi^{-1}(1-p(x_0,A))$ . The following theorem follows. Lemma. Let $A$ be a nonempty measurable subset of $\mathbb R^n$ , and let $\overline{co}(A)$ denote is the closure of it's convex hall. Let $x_0 \in \mathbb R^n\setminus \overline{co}(A)$ . Then $$
d(x_0,A) \ge d(x_0,\overline{co}(A)) \ge \sigma\Phi^{-1}(1-p(x_0,\overline{co}(A))).
$$","['convex-geometry', 'statistics', 'probability', 'hypothesis-testing']"
3500158,L'Hopital's Rule seemingly not giving graphical result [duplicate],"This question already has answers here : L'Hôpital's rule does not apply?! (3 answers) Closed 4 years ago . I'm trying to evaluate the following limit... $$\lim\limits_{x \to \infty} \frac{x-3}{\cos x+x}$$ My first thought was to use L'Hôpital's rule. The numerator obviously tends towards infinity, and so, I believe, does the denominator, because of the additive $x$ term. Applying the rule, then, we arrive at the following result: $$\lim\limits_{x \to \infty} \frac{1}{-\sin x+1}$$ It would therefore seem that the limit does not exist, since the denominator is an alternating trig function with periodic asymptotes. However, if we graph our original function, it's quite clear that the limit actually tends to 1. This can be shown be squeezing the function between $\frac{x-3}{-1+x}$ and $\frac{x-3}{1+x}$ respectively. Why, then, does L'Hôpital's rule not seem to apply to this function? Is there some nuance or precondition for the rule that I'm missing?","['limits', 'calculus']"
3500163,Prove by limit definition $\lim _{x\to \infty }\left(\frac{-7x^2+9x}{4x^2+8}\right)=\frac{-7}{4}$,Prove by limit definition $$\lim _{x\to \infty }\left(\frac{-7x^2+9x}{4x^2+8}\right)=\frac{-7}{4}$$ let $\epsilon > 0$ need to find $M$ such that for every $x>M \implies |f(x) - L|<\epsilon$ $\left|\frac{-7x^2+9x}{4x^2+8}+\frac{7}{4}\right| = \frac{9x+14}{4\left(x^2+2\right)} \le \frac{23x}{4x^2}=\frac{23}{4x} < \epsilon $ so I choose $M=\frac{23}{4\epsilon}$ My question is that I assumed that $x>0$ do I need to check when $x\le0$ or this is enough since $x \to \infty $ ? and does this prove the limit ? thanks,"['limits', 'calculus', 'functions']"
3500206,Points division in a game (addition and subtraction and division only) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 4 years ago . Improve this question We three are playing a game and within each trial of a game two people lose certain amount of points and one of them gains the points lost by the other two. Here is a preview of the excel file for first few trials: I got the total of the win and loss for each person and then divide the loss of each and then give to the winning person which I later realised was wrong. I just want to know how to proceed in excel. The winning points can be found using the autosum but the major issue is when I autosum the loss of each inidividual, from that loss how much should I give to both others. As it is clear to me now it shouldn't be divided by half as in each trial loss of points is varying. Any help is appreciated. I dont know how to attach an excel file here: Otherwise I could have done that. I just want to know the procedure with some detail.","['recreational-mathematics', 'discrete-mathematics']"
3500271,How to find number of integers not divisible by 2 nor 3 in a range?,"I'm trying to come up with a formula to find the number of numbers in a range that are neither divisible by 2 nor 3. For example between 20 and 30 their are 3, namely 23, 25 and 29. I had the formula $f(a, b)=b−⌊b/2⌋−⌊b/3⌋+⌊b/6⌋- a-⌊a/2⌋+⌊a/3⌋-⌊a/6⌋$ where $a<b$ but it doesn't always work. For example for $a=5,b=10$ it gives 1 but should give 2 (because 5 and 7 are in the range of 5 to 10). The formula should be inclusive of $a$ and $b$ in the sense they are counted if they are not divisible by 2 or 3. I don't mind if it's a piecewise function.","['number-theory', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
3500276,"Are there polynomials $P, Q$ with degree no less than 2018 and with integer coefficients, such that $P(Q(x))=3Q(P(x))+1$ for all real $x$?","Determine whether or not two polynomials $P, Q$ with degree no less than 2018 and with integer coefficients exist such that $$P(Q(x))=3Q(P(x))+1$$ for all real numbers $x$ . Attempt: I found a different construction $P(x) = \frac{3}{2}[(2x + 1)^{n+1}-1], Q(x) = \frac{3}{2}[(2x + 1)^n-1]$ , which came from $3x[(2x+1)^n + (2x+1)^{n-1} + ... + 1]$ . I guessed this construction from the small cases I did, and I feel that if (P, Q) works then (something, P) probably works, so I tried to inductively construct a pair of polynomials. Any more complete and elegant solution?","['algebra-precalculus', 'polynomials']"
3500295,"$f(x,y) = \frac{x^3y}{x^4 + y^2}$ is not differentiable at $(0,0)$.","The directional derivative of $f: U \to \Bbb{R}^m$ at $p \in U$ in the direction $u$ is the limit, if it exists, $$\nabla_p f(u) = \lim_{t\to 0}\frac{f(p + tu) - f(p)}{t}.$$ (Often one requires that $|u| = 1$ .)} (a) If $f$ is differentiable at $p$ , why is it obvious that the directional derivative exists in each direction $u$ ?} (b) Show that the function $f : \Bbb{R}^2 \to \Bbb{R}$ defined by $$f(x,y) = \begin{cases}
\frac{x^3y}{x^4 + y^2},& (x,y) \neq (0,0)\\
0,& (x,y) = (0,0)
\end{cases}.$$ has $\nabla_{(0,0)}f(u) = 0$ for all $u$ but is not differentiable at $(0, 0)$ . My only question about that problem is about the differentiability of $f$ . I cannot see how to prove that $f$ is not differentiable. I tried to show that the partials derivatives is not continuous, show that $\frac{f(x,y)}{|(x,y)|} \not\to (0,0)$ as $(x,y) \to (0,0)$ , but nothing worked. I appreciate any hints.","['frechet-derivative', 'derivatives', 'real-analysis']"
3500339,"Integral with dm(x,y) with respect to measure: what is the difference between $\int f(x)dx$ and $\int f(x) dm(x)$?","What is the difference between $\int f(x)dx$ and $\int f(x) dm(x)$ ? In an exercise I was asked to compute $\int\limits_{[0,1]\times[0,1]}xe^{xy}dm(x,y)$ and I did it using the definition $\int f(x,y)dm(x,y)=\int\limits_0^\infty m(\{(x,y): f(x,y)>t\})dt$ but then the TA told me that $dm(x,y)$ is the same as $d\vec{(x,y)}$ and by using Tonelli (since $f(x,y)>0$ in this case) the integral is simply $\int\limits_0^1\int\limits_0^1 f(x,y)dydx$ So my question is : why do we use two different notations for the same thing? And what exactly is $dm(x,y)$ ?","['integration', 'measure-theory', 'lebesgue-integral']"
3500461,Bank's Board of Executive Officers,"A Bank's Board of Executive Officers is composed of a Director, a Deputy Director and four Heads of Sector. The Director decides to install a new vault. Have several locks made and distribute the keys so that: • Each key opens exactly one lock. • The safe is only opened if all its locks are opened. • The Director can open the safe by himself. • The Deputy Director may only open the safe with one of the Heads of Sector. • Heads of Sector can only open the vault in groups of three. a) What is the minimum number of locks that must be placed in the safe to make this scheme possible? b) If so, how many keys should each have? Attemp:
The problem is that I have no feedback, I would like to know if you agree with my answer:
A) 5 Locks B) Director (E) 5, Deputy Director (v) 4, Heads of Sector (a, b, c, d) 2 Lock1 ABVE Lock 2 BCVE Lock3 CDVE 4 DAVE lock 5 ABCDE lock 1 to 4 are to lock the bosses to 3 to open together, 5 is to lock the vice to open alone. What do you think?",['combinatorics']
3500486,Square root of matrix over $F_p$,"Let $M \in F_p^{n\times n}$ be a square matrix of dimension $n\times n$ with entries over $F_p$ . Then we want to obtain $M$ from $M^2=M\cdot M$ . Initially I think of the old diagonalisation procedure of solving the square root of a matrix by: $$M^2 = P\cdot D \cdot P^{-1}$$ $$M = P\cdot D^{\frac{1}{2}} \cdot P^{-1}$$ Where $D$ is a diagonal matrix with the eigenvalues as entries defined over $F_p$ . So $2^{-1} \pmod p$ must be a unit on $F_p^{*}$ , this is $2$ must be an unit on $F_p^{*}$ so it has an inverse. Moreover, when taking $F_2$ as the field, the diagonalisation method doesn't yield the square root matrix as $2$ is not an unit on $F_2^{*}$ . I know that if $M^k \in GL(n,F_2)$ then $\gcd(k,\vert GL(n,F_2) \vert)$ must be equal to $1$ so $M^{k\cdot d}=M$ where $d$ is the multiplicative inverse of $k$ modulo the order of the given general linear group. It works also with the multiplicative order of $M$ in $GL$ . But what if $k=2$ and the field is $F_2$ ? Is there any method for computing the square root matrix of $M^2$ over $F_2$ ?","['finite-fields', 'matrices', 'linear-algebra', 'group-theory', 'diagonalization']"
3500727,How can you verify that a 3 by 3 unimodular matrix generates an infinite number of Fermat near misses?,"I am interested in finding 3 by 3 Ramanujan-Hirschhorn matrices. By definition, a Ramanujan-Hirschhorn matrix is a 3 by 3 matrix which produces an infinite number of Fermat near misses. Ramanujan in his ""lost notebook"" makes the amazing claim that if the integers a_n, b_n, c_n  are defined by: $$\sum_{x\ge0}{a_n x^n}=\frac{1 + 53x + 9x^2}{1 − 82x − 82x^2 + x^3}$$ $$\sum_{x\ge0}{b_n x^n}=\frac{2 - 26x - 12x^2}{1 − 82x − 82x^2 + x^3}$$ $$\sum_{x\ge0}{c_n x^n}=\frac{2 + 8x - 10x^2}{1 − 82x − 82x^2 + x^3}$$ then $a_n^3 + b_n^3 = c_n^3 + (-1)^n$ Two proofs of this claim and a plausible explanation of how Ramanujan may have been led to it have been given by Michael Hirschhorn (1993 -94 ). Indeed, Hirschhorn showed that the sequences  {a_n} , {b_n} and  {c_n}  are given by $$ \begin{array}{} 
\begin{bmatrix}
    a_n \\\
    b_n \\\
    c_n 
    \end{bmatrix} & = {\begin{bmatrix}
                   63 & 104 & −68 \\\
                   64 & 104 & −67 \\\
                   80 & 131 & −85 
                   \end{bmatrix}}^n & \cdot &  \begin{bmatrix}
                                         1 \\\
                                         2 \\\
                                         2 
                                        \end{bmatrix} \end{array}
$$ Notice that the matrix above is unimodular and it produces an infinite number of triples of Fermat near misses when multiplied on the right by the column vector (1 2 2 ). 
In 2013 Tito Piezas , with the help of Mathematica's GeneratingFunction command was able to find: $$
\begin{aligned}
\sum_{n=0}^\infty a_n x^n &= \frac{-9(417-5602x+x^2)}{R_2}\\\
\sum_{n=0}^\infty b_n x^n &= \frac{8(-566-11315x+x^2)}{R_2}\\\
\sum_{n=0}^\infty c_n x^n &= \frac{-6(877+6898x+x^2)}{R_2}
\end{aligned}
$$ where $R_2 = -1+184899x-184899x^2+x^3$ and, $a_n^3+b_n^3 = c_n^3 + 1$ which is a sum of cubes identity analogous to Ramanujan's. I used a method very similar to Michael Hirschhorn's method to derive a 3 by 3 unimodular matrix associated with the above Tito Piezas sum of cubes identity. 
The unimodular matrix that i have found is: $$ \begin{array} {}
\begin{bmatrix}
    a_n \\\
    b_n \\\
    c_n 
    \end{bmatrix} &=& {\begin{bmatrix}
                   156625 & 115992 & −79656 \\\
                   189000 & 139969 & −96120 \\\
                   219624 & 162648 & −111695 
                   \end{bmatrix}}^n \cdot \begin{bmatrix}
                                         3753 \\\
                                         4528 \\\
                                         5262 
                                        \end{bmatrix} 
\end{array} $$ As you can see the above matrix is also unimodular and according to Tito Piezas generates an infinite number of Fermat near miss triples when multiplied on the right by the column vector (3753 4528 5262 ).
But how can i verify the claim that the above matrix produces an infinite number of Fermat near miss triples?  Any suggestions or ideas ?
More importantly, how can I derive or compute another Ramanujan-Hirschhorn matrix different from the one given above ? Is there a systematic way of deriving these matrices ? There is probably an infinite number of them !!
Does anyone know any other Ramanujan-Hirschhorn matrices ?","['matrices', 'number-theory', 'computational-mathematics', 'unimodular-matrices']"
3500816,Spivaks Calculus. Conics Section. Change of coordinates system,"I have a question regarding the conics section of Spivak calculus and  the way he derives the final equation. I understand everything that he is doing up until: Now we have to choose coordinate axes in the plane P. We can choose L as the first axis, measuring distances from the intersection Q with the horizontal plane (Figure 5); for the second axis we just choose the line through Q parallel to our original second axis. If the first coordinate of a point in P with respect to these axis is x, then the first coordinate of this point with respect to the original axes can be written in the form $$\alpha x+\beta$$ for some $\alpha$ and $\beta$ Now, I understand everything before that, and why he is changing coordinates from the 'standard' ones, xyz, to coordinates within the plane, so he can express the intersection of the cone and the plane in a equation. But the thing is how does he justify that there is some $\alpha$ and some $\beta$ that satisfy this condition for all x ? He just claims that such values exist, with no proof. For anyone not familiarized with this chapter, i leave it down here so you can read it. One of the simplest subsets of this three-dimensional space is the (infinite) cone illustrated in Figure 2; this cone may be produced by rotating a ""generaling line,"" of slope C say, around the third axis. For any given first two coordinates x and y, the point (x,y,0) in the horizontal plane has distance $$\sqrt{x^2+y^2}$$ from the origin, and thus $$\tag1 (x,y,z)\text{ is in the cone if and only if }z=\pm C\sqrt{x^2+y^2}$$ .
We can descend from these three-dimensional vistas to the more familiar two-dimnensional one by asking what happens when we intersect this cone with some plane P (Figure 3). If the plane is parallel to the horizontal plane, there's certainly no mystery--the intersection is just a circle. Otherwise, the plane P intersects the horizontal plane in a straight line. We can make things a lot simpler for ourselves if we rotate everything so that this intersection line points straight out from the plane of the paper, while the first axis is in the usual position that we are familiar with. The plane P is thus viewed ""straight on,"" so that all we see (Figure 4) is its intersection L with the plane of the first and third axes; from this view-point the cone itself simply appears as two straight lines. In the plane of the first and third axes, the line L can be described as the collection of all points of the form $$(x,Mx+B)$$ ,
where M is the slope of L. For an arbitrary point (x,y,z) it follows that $$\tag2 (x,y,z)\text{ is in the plane }P\text{ if and only if }z=Mx+B.$$ Combining (1) and (2), we see that (x,y,z) is in the intersection of the cone and the plane if and only if $$\tag{$*$} Mx+B=\pm C\sqrt{x^2+y^2}.$$ Now we have to choose coordinate axes in the plane P. We can choose L as the first axis, measuring distances from the intersection Q with the horizontal plane (Figure 5); for the second axis we just choose the line through Q parallel to our original second axis. If the first coordinate of a point in P with respect to these axis is x, then the first coordinate of this point with respect to the original axes can be written in the form $$\alpha x+\beta$$ for some $\alpha$ and $ \beta$ . On the other hand, if the second coordinate of the point with respect to these axes is y, then y is also the second coordinate with respect to the original axes. Consequently, (*) says that the point lies on the intersection of the plane and the cone if and only if $$M(\alpha x+\beta)+B=\pm C\sqrt{(\alpha x+\beta)^2+y^2}.$$ Although this looks fairly complicated, after squaring we can write this as $$\alpha^2C^2y^2+\alpha^2(M^2-A^2)x^2+Ex+F=0$$ for some E and F that we won't bother writing out. Dividing by $\alpha^2$ simplifies this to $$C^2y^2+(C^2-M^2)x^2+Gx+H=0.$$","['algebra-precalculus', 'conic-sections']"
3500868,Complex skew-symmetric matrices,"Horn and Johnson's matrix analysis makes the following interesting statements about the Jordan canonical forms of symmetric and skew-symmetric matrices. Note: I am concerned here with matrices that have complex entries, and I am considering the entrywise transpose rather than the conjugate-transpose. Regarding symmetric matrices: Theorem 4.4.24: Each $A \in M_n$ is similar to a complex-symmetric matrix. Regarding skew-symmetric matrices: 4.4.P34: Although a symmetric complex matrix can have any given Jordan canonical form (4.4.24), the Jordan canonical form of a skew-symmetric complex matrix has a special form. It consists of only the following three types of direct summands: (a) pairs of the form $J_k(\lambda) \oplus J_k(-\lambda)$ , in which $\lambda \neq 0$ ; (b) pairs of the form $J_k(0) \oplus J_k(0)$ , in which k is even; and (c) $J_k(0)$ , in which k is odd. Explain why the Jordan canonical form of a complex skew-symmetric matrix $A$ ensures that $A$ is similar to $−A$ ; also deduce this fact from [similarity of a matrix to its transpose]. In the above, $J_k(\lambda)$ denotes the Jordan block of size $k$ associated with eigenvalue $\lambda$ . The exercise given is easy enough, but I'd like to prove the leading assertion. To that end, I have found a useful trick: if $A$ is skew-symmetric and $B$ is symmetric, then $A \otimes B$ is skew-symmetric (where $\otimes$ denotes a Kronecker product).  With this trick together with the theorem above, it is easy to find examples of skew-symmetric matrices similar to summands (a) and (b).  However, that's as far as I got, which leaves me with two questions. Questions: How can we construct a skew-symmetric matrix that is similar to $J_k(0)$ , where $k$ is odd? Why are there no skew-symmetric matrices similar to $J_k(0)$ , where $k$ is even? Thank you for your consideration. An update: one way to answer question 2 is as follows.  We have the following result: Corollary 4.4.19: Let $A \in M_n$ be skew-symmetric.  Then $r = \operatorname{rank}(A)$ is even, the non-zero singular values of $A$ occurs in pairs $\sigma_1 = \sigma_2 = s_1 \geq \sigma_3 = \sigma_4 = s_2 \geq \cdots \geq \sigma_{r-1} = \sigma_r = s_{r/2} \geq 0$ , and $A$ is unitarily congruent to $$
0_{n-r} \oplus \pmatrix{0&s_1\\-s_1 & 0} \oplus \cdots \oplus \pmatrix{0&s_{r/2}\\-s_{r/2} & 0}.
$$ By the way: $A$ is unitary congruent to $B$ means that $A = UBU^T$ for some unitary matrix $U$ ; note that this is not necessarily a matrix similarity. Because $A$ has singular values that occur in pairs, we can preclude the possibility that $A$ is similar to any matrix of odd rank.  For even $k$ , $J_k(0)$ is such a martix. I would still be interested in an argument that doesn't use this fact though; perhaps there is an easy way to see that a skew-symmetric matrix must have even rank. Possibly useful observations: The rank of $A$ is the same as that of the Hermitian matrix $A^*A = \overline{A^T}A = - \bar A A$ . Due to the above corollary, we will necessarily be able to write a matrix that is similar to $J_3(0)$ in the form $$
A = U\pmatrix{0&1&0\\-1&0&0\\0&0&0}U^T = u_1u_2^T - u_2u_1^T
$$ where columns $u_1,u_2$ of $U$ are orthonormal.","['matrices', 'skew-symmetric-matrices', 'linear-algebra', 'complex-numbers']"
3500874,Defining an Antiderivative for Monotone Functions,"I'm reading through a paper and I'm having trouble following the logic of the following step. ""Suppose $\psi:(0,\infty) \to [0,\infty)$ is a non-negative, non-decreasing function. Let $\Psi$ be the primitive function of $\psi$ , i.e. $\Psi' = \psi$ ."" Can we actually do this? I believe monotonicity of $\psi$ means that an antiderivative can indeed be defined, and would be continuous, but wouldn't it only be differentiable almost everywhere? In particular, at points of discontinuity of $\psi$ (the set of which I believe will have measure zero, again by monotonicity) we cannot say $\Psi' = \psi$ ? In summary, is it instead true that $\Psi' = \psi$ a.e.? Thank you for your time!","['integration', 'lebesgue-integral', 'monotone-functions', 'real-analysis']"
3500898,Understanding the least squares regression formula?,"I've seen the following tutorial on it, but the formula itself had not been explained ( https://www.youtube.com/watch?v=Qa2APhWjQPc ). I understanding the intuition behind finding a line that ""best fits"" the data set where the error is minimised (image below). However, I don't see how the formula relates to the intuition? If anyone could explain the formula, as I can't visualise what it's trying to achieve. A simple gradient is the dy/dx , would't we just do $\sum(Y - y) \ ÷ \sum (X - x)$ where Y and X are the centroid values (average values). By my logic, that would be how you calculate the average gradient? Could someone explain this to me?","['linear-regression', 'statistics', 'least-squares']"
3500921,Why Abel-Plana formula does not work for exponent?,"Abel-Plana formula : $$\sum _{n=0}^{\infty }f(n)=\int _{0}^{\infty }f(x)\,dx+{\frac {1}{2}}f(0)+i\int _{0}^{\infty }{\frac {f(it)-f(-it)}{e^{2\pi t}-1}}\,dt$$ If we take $f(x)=e^{-x}$ , the right-hand side is $3/2$ , the left-hand side is $\frac{e}{e-1}$ .","['integration', 'complex-analysis', 'sequences-and-series']"
3500967,Log of product of 3 matrix exponentials,"I'm working on a certain problem that involves the following question: Let $A,B$ be two self-adjoint operators, and define $C=e^{-A}e^{-B}e^{-A}$ . Is there a ""convenient"" way to express $log(C)$ ? I'm not entirely sure what ""convenient"" is, looking for anything that could be useful. I tried two things - the first was using the Campbell-Baker-Hausdorff formula, but I got very complicated expressions that I do not know how to deal with (since the two operators do not necessarily commute). The second was assuming that one of the operators is diagonal (I simply calculate everything up to a conjugation), but still, I couldn't find a fairly simple way to express what I'm looking for. Does anyone know of some useful identities/methods that could be useful for this? In the problem I'm working on, $A,B$ depend on two parameters which vary in a certain region, and I want to find a ""simple"" expression for $log(C)$ , so that I can change the parameters and immediately see what happens. Thanks in advance.","['matrices', 'operator-theory', 'linear-algebra', 'functional-analysis']"
3500969,Is $\ln(\pi(e^x)) \sim x?$,"Q: Is $\ln(\pi(e^x))\sim x?$ $\DeclareMathOperator{\Li}{Li}$ $\pi(x)$ is the prime counting function. Here's my attempt. I think to prove or disprove this, I have to show whether or not $$ \lim_{x\to \infty} \frac{\ln(\pi(e^x))}{x}=1.$$ Since $$ \ln(\pi(e^x)) \sim \ln(\Li(e^x)) $$ I will substitute that into the limit: $$ \lim_{x\to \infty} \frac{\ln(\Li(e^x))}{x}. $$ Now I'll use L'Hopitals rule repeatedly because we have some forms of $\frac{\infty}{\infty}.$ $$ \lim_{x\to\infty} \frac{e^x}{x\Li(e^x)}=\lim_{x\to \infty} \frac{e^x}{\Li(e^x)+e^x}=\lim_{x\to \infty} \frac{e^x}{e^x\left(1+\frac{1}{x}\right)}=\lim_{x\to \infty}\frac{1}{1+\frac{1}{x}}=1. $$ Is this correct?","['number-theory', 'limits', 'asymptotics', 'prime-numbers']"
3500992,Does the Lie derivative of a harmonic form with respect to a killing vector field vanish,"On the wikipedia page for Killing vector fields under 'properties' it is stated that if $X$ is a Killing vector field and $\omega$ is a harmonic differential form then $\mathcal{L}_{X}\omega=0$ . https://en.wikipedia.org/wiki/Killing_vector_field#Properties It seems like this should be true but I can't find it stated anywhere else except for the online previews of a couple of books which I don't have access to. I have tried to show this using the formula for the Lie derivative of a form $\mathcal{L}_{X}\omega=d(i_{X}\omega)+i_{X}(d\omega)$ along with the co-closure condition on $\omega$ and Killing's equation but I'm not really sure where to go. So, my question is: is this true and if so, what direction should I go in to try and show it?","['hodge-theory', 'riemannian-geometry', 'differential-geometry']"
3500995,Approximate recursion defined by alternating summation?,"Problem & Question Given a constant $k\ge 2$ , we define the sequence $a_k(0)=0,a_k(1)=1$ and for $n\ge2$ : $$
a_k(n)=\sum_{i=1}^{\left[\frac{n}{k}\right]} (-1)^{n-i+1} a_k(i)
$$ Where $\left[\frac{n}{k}\right]$ gives integer part of $\frac{n}{k}$ . (The $\left[\space\right]$ represents truncation.) For example, for $k=2$ we have the following scatterplot: [Click to see animated gif] . It appear as two interchanging curves repeat [following shape] in larger forms as we increase $n$ . Given $k$ , can we find a real equation that would approximate ""the curves"" given by $a_k(n)$ ? That is, the plot given by $a_k(n)$ can be defined as $\Gamma_1\cup\Gamma_2$ where $\Gamma_1,\Gamma_2$ are those ""the curves"". Specifically, $\Gamma_1,\Gamma_2$ are graphs of two real functions $\gamma_1(x),\gamma_2(x)$ where $\gamma_1(x)=-\gamma_2(x)$ as they are symmetric over $y$ -axis. The goal is to define them for $x\in\mathbb R$ . Currently, you can say that $a_k(n)$ approximates $\gamma_1,\gamma_2$ at points $x=n\in\mathbb N$ . If it is not clear what I mean, clicking on the previous link ""[following shape]"", you can see the curve above the $y$ -axis as $\Gamma_1$ given by $\gamma_1(n)$ , and the curve below $y$ -axis as $\Gamma_2$ given by $\gamma_2(n)$ . For simplicity, we can restrict ourselves to $k=2$ if needed. My idea The curves $\gamma_1(n)\approx-\gamma_2(n)$ generated by $a_k(n)$ remind me of a sine function whose period is being stretched by some function $g(n)$ , and whose amplitude is being increased by some function $h(n)\ne 0$ . Is it possible to find such $g,h$ such that the following ""sine form"": $$f_k(x):=h_k(x)\sin\left(\pi g_k(x)\right)\approx \gamma_1(x)=-\gamma_2(x)\space ?$$ To answer my question? This needs to be defined for all real $x\gt 0$ . Notice the above $f_k(x)$ form would have roots (be zero) at $x=g_k^{-1}(m),m\in\mathbb N$ . That is, lets observe ""near-zeros"" of the sequence $a_k(n)$ - The points where the curves pass over the $y$ -axis and and are closest to it. Here are first couple ""near-zeros"" for $k=2$ : $$a_2(n)\approx 0 \text{ at } n\approx 2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots$$ Meaning that the $f_k$ (the $\gamma_1,\gamma_2$ ) should be zero (have a root) somewhere near these ""near-zeros"". In the context of my ""sine form"" $f$ mentioned above, and for $k=2$ , this means that we want to find $g_2$ such that for $m=1,2,3,\dots$ we have: $$ g_2^{-1}(m)=2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots $$ For example, here is the plot of $a(n)$ for $n\le215$ showing a near-zero around $n\approx204.5$ , where we have $a(204)=-a(205)=-40\approx0$ . (Hence I've taken $n\approx \frac{204+205}{2} = 204.5$ ) But, the problem here is: How to determine at which $n$ will $a_k(n)$ reach a ""near-zero""? The other problem that needs to be solved, is to find $h_k(n)$ . For starters we need to know the growth of the absolute value of the sequence, $|a_k(n)|$ ? Then, the question remains: Can we interpolate (approximate) a ""closed form"" of $h_k(n)$ , to get the ""amplitude"" of $f_k$ ? Alternatively, is there a better $f_k$ form to search for, than my ""sine form"" $f_k$ ?","['interpolation', 'approximation', 'recurrence-relations', 'real-analysis', 'recreational-mathematics']"
3501001,Use Fubini's theorem to reverse the order of the double integral,"Use Fubini's theorem to reverse the order of the double integral (the reversed integral may split into a sum of multiple pieces): a) $\int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx$ b) $\int_0 ^2 \int_x ^{3} f(x,y) dy dx$ c) $\int_{-1} ^2 \int_0 ^{1-y^2} f(x,y) dx dy$ In part $c)$ be careful about signs! My attempt a) Boundary of number at $x$ : $0\leq x\leq 4$ Boundary of function at $y$ : $0\leq y\leq \sqrt{x}.$ Now consider, boundary of number at $y$ : $0\leq y\leq 2$ boundary of function at $x$ : $y^2\leq x\leq 4.$ So by the Fubini's $\int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx=\int_0 ^2 \int_{y^2}^4 f(x,y) dx dy.$ b) Boundary of number at $x$ : $0\leq x\leq 2$ , boundary of function at $y$ : $x\leq y\leq 3.$ Now, consider boundary of number at $y$ : $0\leq y\leq 3$ boundary of function at $x$ : $0\leq x\leq y$ So, by the Fubini's theorem: $\int_0 ^2 \int_x ^{3} f(x,y) dy dx=\int_0 ^3 \int_0 ^{y} f(x,y) dy dx.$ c) I couldn't draw graph, may you draw and add here? May you check $a), b)$ and may you help for $c)$ ?","['multivariable-calculus', 'calculus', 'real-analysis']"
3501034,Understanding measurable functions and their definition based on pre-images?,"I have recently started learning a bit more about measure theory, but I've been stuck on the definition of measurable functions. I'm comfortable with the formal definition that says a function $f:X\to Y$ is measurable if the pre-image of any measurable set is measurable. What I don't understand is why this definition has been chosen, i.e. the ""intuition"" on the meaning of being measurable. I haven't learned about $\sigma$ -algebras due to the book I am using, but I'm aware that measurable functions preserve the structure of the measure spaces. In that case, I'm would like to know why pre-images do the trick and not the images of functions. If I wanted to know if $f$ preserved the structure, then my first idea would be to make sure that measurable sets are mapped into/to measurable sets, not to look at pre-images. Continuity has almost the same definition. However, this comes from the generalization of the $\epsilon$ - $\delta$ definition of continuity from analysis/metric spaces. Therefore, I don't think the same rational can be used to explain why we use pre-images to define measurable functions. I have read through a fair amount of StackExchange answers on the topic, and some responses clarified why this definition is useful. For one , if $Y$ does not have a measure an $X$ has $\mu$ , then we can pull-back to get $\mu\circ f$ . However, this issue doesn't arise when both spaces are measurable. The second thread that helped explained that being measurable is necessary for the Lebesgue integral. Taken together is that all there is to it? Is this defined so that we can pull-back real functions to properly define Lebesgue integration? Any sort of insights or alternative perspectives would be welcome.","['measure-theory', 'lebesgue-measure']"
3501135,Fitting a plane to points using SVD,"I am trying to find a plane in 3D space that best fits a number of points.  I want to do this using SVD.  To calculate the SVD: Subtract the centroid of the points from each point. Put the points in an mx3 matrix. Calculate the SVD (e.g. [U, S, V] = SVD(A)). The last column of V, (e.g. V(:,3)), is supposed to be a normal vector to the plane.  While the other two columns of V (e.g. V(:,1) and V(:,2)) are vectors parallel to the plane (and orthogonal to each other).  I want to find the equation of the plane in ax+by+cz+d=0 form.  The last column of V (e.g. V(:,3)) gives ""a"", ""b"", and ""c"", however, in order to find ""d"", I need a point on the plane to plug in and solve for d.  The problem is that I don't know what are valid points to use to plug in. My question is: does the centroid of the points necessarily lie on the fitted plane? If so, then it's easy to just plug in the centroid values in the equation (along with the  from the norm) and solve for ""d"".  Otherwise, how can I calculate ""d"" in the above equation? The matrix U apparently gives the point values but I don't understand which values to take.","['statistics', 'svd', 'principal-component-analysis']"
3501169,Conditional expectation of multivariate normal,"Note: In trying to formalize my question, I think I found one answer to it.  I have still posted the question, in part because I hope that someone else has a less algebraically intensive solution. The question: I'm reading Lamperti's Probability , Second Edition.  I'm trying to understand Example 3 of Section 4 of Chapter 1 (page 25 of my book), which has to do with conditional expectations applied to multivariate normal distributions.  He makes a leap of logic that I don't follow. Lamperti says that, given random variables $X_0, X_1, ..., X_n$ with positive and continuous joint density $f(t_0, t_1, ..., t_n)$ , we can write $E[X_0 | X_1, ..., X_n]$ as the random variable $g(X_1, ..., X_n)$ , with $g(t_1,...t_n)$ defined as $$g(t_1, ..., t_n) := \frac{\int s f(s, t_1, ..., t_n) ds}{\int f(s, t_1, ..., t_n)ds}$$ So far, so good.  He also says that $X_0, ..., X_n$ follow a multivariate normal distribution as long as they have a joint density of the form $$ f(t_0, ..., t_n) = K \exp(-\frac{1}{2} \sum^n_{i,j=0}d_{ij}t_it_j)$$ with $K$ a normalizing constant and $[d_{ij}]$ a symmetric, positive-definite matrix.  Also fine.  Then, however, he says that, from the two facts above, we can deduce that $$E[X_0 | X_1, ..., X_n] = - \sum^n_{k=1} \frac{d_{k0}}{d_{00}}X_k$$ but I do not know how. Do you?","['conditional-expectation', 'probability-theory', 'normal-distribution']"
3501195,Intermediate value theorem for functions $f:\mathbb R \to \mathbb R^2 $,"Let $f:[a,b]\subset \mathbb R\to\mathbb R^2$ be a continuous function on $[a,b]$ , and differentiable on $(a,b)$ , such that $f´(t)\neq 0 $ for all $t\in(a,b)$ . Prove that there exists $\xi \in (a,b) $ and $\lambda\in\mathbb R$ such that: $$f(b)-f(a)= \lambda f´(\xi)$$ What I have so far is that by applying the one dimensional case, i get that: $$f(b)-f(a)=(b-a)(f_1´(\xi_1), f_2´(\xi_2))$$ For some $\xi_1,\ \xi_2 \in \mathbb R$ . Thus one way to prove it would be by showing that $\xi_1=\xi_2$ , and this is where I´ve no clue how to proceed.","['multivariable-calculus', 'calculus', 'real-analysis']"
3501200,Expected number of rounds played in a game,"Suppose one plays the following game: Start with $n$ rounds. Each round, flip a weighted coin with probability $p$ of coming up heads. Each time we obtain heads, we get $m$ additional rounds added to our current total. We play until we run out of rounds.
The expected number of rounds played is: $$
E[\# Rounds]= n + nmp + n(mp)^2 + n(mp)^3 +\dots 
$$ My question is how does one derive this expectation formula?
(This is a question for work, not school).","['expected-value', 'probability-theory']"
3501227,Central idempotent of a group representation,"Given a finite group $G$ with representation $\rho: G \mapsto V$ , and let $\chi$ be the character of an irreducible representation. It is well known that the following map $$\pi = \frac{\chi(1)}{|G|} \sum_g \chi(g) \rho(g)$$ is the projection into the isotypical component of $\rho$ corresponding to $\chi$ , but I cannot find a self-contained proof of this fact. In particular, is there a direct way to show that $\pi^2 = \pi$ ?","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
3501247,Show that $\operatorname{Pr}(Z-X \geq 0)$ converges to one,"Suppose that $V_i$ , for $i \in \mathbb{N}$ , are i.i.d. standard normal random variables and $Y_i = \sum_{k=1}^i V_k$ for $i \in \mathbb{N}$ with $Y_0 = 0$ . Let $X_n = (\sum_{i=1}^n V_i Y_{i-1})^2 Y_n^2$ and $Z_n = (\sum_{i=1}^n Y_{i-1}^2)^2$ . How can I show that $\lim_{n \rightarrow \infty} p(Z_n - X_n \geq 0) = 1$ ? From simulations, I have seen that this probability converges to 1 very fast. Intuitively, this makes sense because the we have $E[Z_n] = \Theta(n^4)$ and $\sigma(Z_n) = \Theta(n^4)$ but $E[X_n] = \Theta(n^3)$ and $\sigma(Z_n) = \Theta(n^3)$ . Chebyshev inequality implies that $X_n$ samples are mostly around $n^3$ but $Z_n$ samples are mostly around $n^4$ . Thus, there is a higher chance that $Z_n$ is greater than $X_n$ .","['statistics', 'probability-limit-theorems', 'normal-distribution', 'upper-lower-bounds', 'probability']"
3501287,"For What $n, m$ is $f(x)$ Differentiable? Continuously Differentiable?","Let $f(x) = \begin{cases} |x|^m\sin{\left(\frac{1}{|X|^n}\right)} & x\neq 0\\ 0 & x=0\end{cases}$ . Questions: For what values of $m,n \in \mathbb{R}$ is $f$ continuous at $0$ ? Differentiable at $0$ ? Continuously Differentiable at $0$ ? My (partly successful) attempt: $f$ continuous at $0$ precisely when $0<m$ or $n\lt m\leq0$ . This is because if $m>0$ then $\lim_{x\to 0}{|x|^m}=0$ and $\sin{\left(\frac{1}{|X|^n}\right)}$ is bounded. Else, if $n<m\leq 0$ then $|x|^{m-n} \frac{\sin{\left(\frac{1}{|X|^{n}} \right)}}{\frac{1}{|X|^n}}$ converges to $0$ as well. I know I need to check when the limit $\lim_{x\to 0}{\frac{f(x)}{x}}$ exists. This is too difficult and divided into too many subcases. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case? Same as 2. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case?","['limits', 'derivatives', 'continuity']"
3501308,"Without using a calculator can you say, which number is greater, $65^{1662}$ or $33^{1995}$?","Without using a calculator can you say, which number is greater, $65^{1662}$ or $33^{1995}$ ? first i thought we can say which is bigger by differences. Like Here difference of base : 65–33=32 . Here difference of exponent : 1995–1662=333 . Difference of exponents are bigger than difference of base. so I think I should consider the biggest number by bigger difference. So i considered $33^{1995}$ as bigger number and I was right. $33^{1995}$ is bigger then $65^{1662}$ . But then i took another example $21^{6}$ and $81^{4}$ . Here difference of base : 81–21=60 . Here difference of exponent : 6–4=2 . Then as before here also i considered bigger number by bigger difference. So here base has the bigger difference than exponent. So according to the bigger difference of base i considered bigger number by bigger difference and that was 81^3. But I was wrong at this point. $21^{6}$ is bigger than $81^{3}$ Then What is the correct way of finding the bigger number ?","['exponential-function', 'discrete-mathematics']"
3501380,$\int_a^b\frac{|\gamma'(t)|}{1-|\gamma(t)|^2}dt$ (from a calculation of Poincare distance)?,"This answer requires only 2 new definitions and elementary complex analysis. I have been reading these notes titled Invariant Pseudodistances and Pseudometrics in Complex Analysis in Several Variables
(PDF link via diva-portal.org) . On page 13, section 1.4.3, they define the Poincare distance. Can you help me with the calculations where they compute $L_{{\beta}_\mathbb{D}}$ I know that the portion marked in yellow is a typo. But Can you tell me how $L_{{\beta}_\mathbb{D}}$ Is equal to the expression marked in red with a $dt$ . And also why ${\beta}^i_\mathbb{D}(0, \phi_z(w))$ is equal to ${\beta}^i_\mathbb{D}(0, |\phi_z(w)|)$ ?","['complex-analysis', 'hyperbolic-geometry', 'mobius-transformation']"
3501392,Show that $SO_{3}$ contains the free group of rank 2 as a subgroup.,"I'm trying to prove that $SO_{3}$ contains the free group of rank 2, $F_{2}$ , as a subgroup, by showing there are two rotations in $SO_{3}$ that are independent and hence generate $F_{2}$ . I came across the following proof of this online: $$ A(x,y,z) := (\frac{3}{5}x + \frac{4}{5} y, -\frac{4}{5} x + \frac{3}{5} y, z ); \quad
B(x,y,z) := (x, \frac{3}{5}y + \frac{4}{5} z, -\frac{4}{5} y + \frac{3}{5} z ).$$ These are easily seen to be rotation matrices with inverses $$ A^{-1}(x,y,z) := (\frac{3}{5}x - \frac{4}{5} y, \frac{4}{5} x + \frac{3}{5} y, z ); \quad
B^{-1}(x,y,z) := (x, \frac{3}{5}y - \frac{4}{5} z, \frac{4}{5} y + \frac{3}{5} z ).$$ Now we claim that no non-trivial composition of $A, B, A^{-1}, B^{-1}$ gives the identity.  It suffices to show that no non-trivial composition
  of the operators $5A$ , $5B$ , $5A^{-1}$ , $5B^{-1}$ gives a linear operator whose coefficients are all divisible by 5.
  We now work in the finite field geometry $F_5^3$ , where $F_5 = \mathbb{Z}/5\mathbb{Z}$ is the field of order 5.  Then we have $$ 5A(x,y,z) := (3x + 4y, -4x + 3y, 0); \quad 5B(x,y,z) := (0, 3y + 4z, -4y + 3z)$$ and $$ 5A^{-1}(x,y,z) := (3x - 4y, 4x + 3y, 0); \quad 5B^{-1}(x,y,z) := (0, 3y - 4z, 4y + 3z).$$ Each of these operators are rank one operators in $F_5^3$ : \begin{align*}
\text{range}(5A) &= \text{span}( (3,-4,0) ) = \ker(5A^{-1})^\perp\\
\text{range}(5A^{-1}) &= \text{span}( (3,4,0) ) =  \ker(5A)^\perp\\
\text{range}(5B) &= \text{span}( (0,3,-4) ) = \ker(5B^{-1})^\perp\\
\text{range}(5B^{-1}) &= \text{span}( (0,3,4) ) = \ker(5B)^\perp.
\end{align*} From this we see that any non-trivial combination of $5A$ , $5A^{-1}$ , $5B$ , $5B^{-1}$ (in which $5A$ and $5A^{-1}$ are never
  adjacent, and $5B$ and $5B^{-1}$ are never adjacent) will always be a non-zero operator, as desired, because the ranges and kernels are skew. I can't quite understand the proof from 'Each of these operators are rank one operators in $F_5^3$ ' onwards. I would really appreciate it if someone could explain the proof in a way that is relatively easy to understand. Thanks for any help.","['proof-explanation', 'matrices', 'linear-algebra', 'group-theory', 'rotations']"
3501468,Differentiating term by term in a Banach space: how to justify it?,"After looking at this question , I am now wondering if the theorem proven in the first answer below can be generalized to a Banach space. See here for my attempt. But before doing that, I have the following problem: NOTE: I use the notations in the first answer below that question. I don't know how to justify why the series $\sum a_kf_k$ can be differentiated term by term, i.e. why $\partial^\alpha (\sum a_kf_k)=\sum a_k(\partial ^\alpha f_k) $ , and why the convergence of $\sum a_k\partial ^\alpha f_k$ implies the existence of $\partial^\alpha (\sum a_kf_k)$ In Banach space, the multi-index notation does not mean anything, so I should write $\sum a_kD^nf_k$ .","['frechet-derivative', 'banach-spaces', 'functional-analysis', 'separable-spaces']"
3501510,How does the surface of a sphere break the parallel postulate?,"I'm currently studying non-Euclidean geometry and recently learned that a sphere is in 3-dimensional Euclidean space, but its surface is not. According to my findings, non-Euclidean spaces are ""spaces where the parallel postulate does not hold."" ( History of manifolds and varieties - Wikipedia ) meaning that if there was a line and we were to draw two lines such that the inner angles are less than 90 degrees, the two lines would never meet. Please correct me if I'm understanding this fundamentally incorrectly. If I have understood it correctly, then does the surface of a sphere satisfy this postulate? If I were to draw a line on the surface of a sphere and draw two lines such that the inner angle they make with the first line is less than 90 degrees, I'm pretty sure that they would meet somewhere on the surface. Would anyone be kind enough to help me understand this concept? Thanks in advance. P.S., I've taken a look at this Math Stack Exchange question: Spherical Geometry and Playfair's Axiom but it didn't help so much.","['noneuclidean-geometry', 'geometry']"
3501591,"Is there another name for this ""power of two"" sequence?","I was recently asked how to call a number sequence, and since I am not sure about naming conventions, I am grateful for any help. There is a sequence of real numbers, where: the next element always equals the previous element, multiplied by 2 1 is included A slice of this sequence will look like that: 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32. Is there a well-known name for this sequence? I see powers of two, starting from 1, but is there a name like “integers” or “Fibonacci numbers”? Are there separate names for an integer part and fractional part?
I saw several names in OEIS , but what about fractional part?",['sequences-and-series']
3501607,Verify $\cos(x)=\frac{1-t^2}{1+t^2}$ with $t=\tan(\frac{x}{2})$,"I was requested to verify, with $t=\tan(\frac{x}{2})$ , the following identity: $$\cos(x)=\frac{1-t^2}{1+t^2}$$ I'm quite rusty on my trigonometry, and hasn't been able to found the proof of this. I'm sure there may be some trigonometric property I should know to simplify the work. Could someone hint me or altotegher tell me how to solve this problem? I tried to simplify the RHS looking to get $\cos(x)$ out of it but failed.",['trigonometry']
3501616,Minimum value of the given function-,Find the minimum value of- $$\frac{(x+\frac{1}{x})^6-(x^6+\frac{1}{x^6})-2}{(x+\frac{1}{x})^3+(x^3+\frac{1}{x^3})}$$ I tried opening the brackets and trying to cancel the terms and using AM-GM.,"['contest-math', 'functions']"
3501627,linear subspaces of a projective space,Definition :  a linear subspace of $\mathbb{P}^n$ is a closed subspace defined by linear homogeneous equations. Claim :  a linear subspace  of $\mathbb{P}^n$ of dimension $d$ is isomorphic to $\mathbb{P}^d$ . I have tried building the isomorphism by brute force and failed. But I think there is a more elegant way of proving this. I'll be happy for some help.,"['algebraic-geometry', 'affine-geometry', 'projective-space']"
3501629,Find a perpendicular vector in 3D to another 3D vector with same length?,"Is there a fast way to find such a 3d vector that is perpendicular to another 3d vector given $x,y,z$ and is same length that is efficient (no sqrt or $\cos$ / $\sin$ )? Like how in 2d such a vector is $(-y, x)$ .","['geometry', '3d']"
3501715,Odd and even number such that polynomial is odd,"Somebody can help me with this polynomial problem? I tried something but i am not really sure if i can finish with that. Thank you! Let $P$ a polynomial with integer coefficients for which exists $2$ integer numbers, one odd, one even such that values of the polynomial in these values are odd. Show that the polynomial cannot have integer zeros. I tried to use the contradiction and purpose we have zeros integer numbers. But i don't know how to elaborate that. I used $P=a_nX^n+\cdots+a_0$ and $a,b\in Z,a=2k,b=2k+1,k\in Z$ . Then we have $P(a)=2k+1$ and $P(b)=2k+1$ . If we say $P$ has integer zeros let $a,b$ to be zeros. But actually $P(a)$ and $P(b)$ are even, contradiction?","['algebra-precalculus', 'polynomials']"
3501737,"$50\cos^2 x + 5\cos x = 6\sin^2 x$, find $\tan x$","$50\cos^2 x + 5\cos x = 6\sin^2 x$ Find $\tan x$ I used $\cos^2 x + \sin^2 x = 1$ to get the equation $$56\cos^2 x + 5\cos x -6 = 0$$ I then solve this to get $\cos x = \dfrac27, -\dfrac38$ Then I used generic trig ratios to get $\tan x = \pm\dfrac{3\sqrt5}{2}, \pm\dfrac{\sqrt{55}}{3}$ Are these $\pm$ signs correct? My reasoning came for a CAST diagram, can you confirm if this is correct and if not, why?",['trigonometry']
3501748,Prove that the set of accumulation points is circle. Find it center and radius [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I have the following sequence $a_n= \prod_{k=1}^n (1+\frac{i}{k})$ I need to prove that the set of accumulation points is circle. Also I need to find it center and radius. I don’t know how to attempt, could someone give any hints ? So first I was trying to understand what is the center and radius of circle. Also I think , that I need to somehow rewrite $a_n$ ’s",['complex-analysis']
3501788,Isomorphism of $\mathbb{Z}\ltimes_A \mathbb{Z}^m$ and $\mathbb{Z}\ltimes_B \mathbb{Z}^m$,"Let $A$ and $B$ be matrices of finite order with integer coefficients. Let $n\in\mathbb{N}$ and let $G_A=\mathbb{Z}\ltimes_A \mathbb{Z}^n$ be the semidirect product, where the action is $\varphi(n)\cdot (m_1,\ldots,m_n)=A^n (m_1,\ldots,m_n)$ , and similarly with $B$ . It is easy to construct an isomorphism between $G_A$ and $G_B$ if $A$ is conjugate in $GL(n,\mathbb{Z})$ to $B$ or $B^{-1}$ . But, this is also a necessary condition? I mean, does $G_A\cong G_B$ implies $A\cong B$ or $A\cong B^{-1}$ in $GL(n,\mathbb{Z})$ or is there a counterexample? I've seen in A necessary condition for two semi-direct products to be isomorphic. that it is true if A and B are hyperbolic, i.e none of their eigenvalues have module 1, but it isn't the case. Thank you!","['group-isomorphism', 'group-extensions', 'semidirect-product', 'abstract-algebra', 'group-theory']"
3501821,will in any case $\frac{\partial f}{\partial x}$ is Reciprocal of $\frac{\partial x}{\partial f}$,If $y$ is a Single Variable function of $x$ we have: $$\frac{dy}{dx}=\frac{1}{\frac{dx}{dy}}$$ Now coming to partial Derivatives i have tested for Polar Coordinates $$x=r \cos t$$ $$y=r \sin t$$ We have: $$\frac{\partial x}{\partial r}=\cos t=\frac{x}{\sqrt{x^2+y^2}}$$ Where as: $$\frac{\partial r}{\partial x}=\frac{\partial \left(\sqrt{x^2+y^2}\right)}{\partial x}=\frac{x}{\sqrt{x^2+y^2}}$$ Which are actually same? So is it not true about reciprocal relation in Partial derivatives or in which situations: $$\frac{\partial f}{\partial x}=\frac{1} {\frac{\partial x}{\partial f}}$$,"['multivariable-calculus', 'derivatives', 'chain-rule']"
3501847,Differentiating an integral depending on a parameter,"Consider the following double integral depending on a parameter $x \in \mathbb{R}$ : $$
I(x) := \int_{x}^{\infty} f ( z) e^{-(z-x)} \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt \ dz,
$$ where $f(z)$ is continuous and vanishes at infinity, meaning that $$
\lim_{ | z | \rightarrow \infty} f ( z ) = 0
$$ It is known that under this condition $I(x)$ is also continuous and vanishes at infinity . I am trying to find out whether $I(x)$ is necessarily differentiable
  in $x$ without any additional assumptions on $f$ . If we denote by $$
J(x,z) := \int_{0}^{\infty} \frac{e^{-(z-x)t}}{10+\ln^2{t}} \ dt
$$ then a naive ""differentiation"" of $J(x,z)$ w.r.t. $x$ may lead to $$
\frac{d}{dx} J(x,z)  \overset{?}{=} \int_{ 0 }^{ \infty} \frac{te^{-(z-x)t}}{10 + \ln^2 t} \ dt,
$$ which does not converge if we set $z=x$ . Can this imply that $I(x)$ is not necessarily differentiable? One could also try a change of variables through $y :=z-x$ and get $$
I( x ) := \int_{0}^{\infty} f( y + x ) e^{-y} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ dt \ dy,
$$ and it seems that this would hardly lead anywhere, since we know nothing about the differentiability of $f$ w.r.t. $x$ . Another thing, which may or may not be helpful, is that $$
\int_{0}^{\infty} \int_{0}^{\infty} \frac{e^{-yt}}{10+\ln^2{t}} \ \color\red{dy} \ dt = \frac{ \pi }{ \sqrt{10}},
$$ as computed here by wolframalpha. In summary, knowing that $I(x)$ is continuous and vanishing at infinity, are there any criteria which can be used to show that $I(x)$ is necessarily differentiable without additional assumptions on $f$ , and, if so, how does the derivative look like? Alternatively, is there some $f$ which is continuous and vanishing at infinity, but such that $I(x)$ is not differentiable?","['integration', 'improper-integrals', 'analysis', 'real-analysis', 'calculus']"
3501900,"Finding CDF of $Y = \min(X, X^2)$ and $Z=\max(X,X^2)$ when $X$ is Uniform on $[0,2]$","Let X be a random variable with uniform distribution on $[0,2]$ . Find CDF of $Y = \min({X, X^2})$ and $Z = \max(X, X^2)$ . $F_Y(t) = \mathbb{P}(Y \leq t) = \mathbb{P}(\min(X, X^2) \leq t) = \dots$ If $t < 0$ then $\mathbb{P}(\min(X, X^2) \leq t) = \mathbb{P}(X \leq t) = 0 $ If $t \geq 2$ then $\mathbb{P}(\min(X, X^2) \leq t) = 1 $ because $ \mathbb{P}(X \leq t) = 1 $ What about when $t \in [0, 2)$ ? How can I find $F_Z(t)$ ?","['probability-theory', 'probability']"
3501943,Solve $\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}e^{-5x^2-5y^2+8xy}dxdy$,I have a problem in evaluating  the double integral $$\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}e^{-5x^2-5y^2+8xy}dxdy$$ I tried to use polar coordinates $e^{-(r^2(5-8\sin(\theta)\cos(\theta))}$ but still unsolvable Any help please,"['integration', 'calculus']"
3501961,Homeomorphism Between Closed Riemann Surfaces Homotopic to Quasiconformal Mapping,"I'm re-reading a paper of Bers and for the second time, and I am yet again confused about the claim in the title, which Bers declares to be easy to prove. For context, I'll lay out some terminology. We call a pair $(S, \alpha)$ a marked Riemann surface when $S$ is a Riemann surface and $\alpha$ is an equivalence class of generators of $\pi_1S$ , each of which $\{a_i\}_{i=1}^{2g}$ satisfies the standard one relation $1= \prod_{j=1}^{2g} a_{2j-1} a_{2j} a_{2j-1}^{-1}a_{2j}^{-1}$ . A map between marked Riemann surfaces $f:(S, \alpha) \rightarrow (S',\alpha')$ is a homeomorphism that respects the markings. Finally, call a pair of marked Riemann surfaces $(S, \alpha), (S', \alpha')$ to be similarly oriented if there is an orientation-preserving homeomorphism $f: S \rightarrow S'$ . Now, Bers goes on to make the following claim: If $(S, \alpha)$ and $(S', \alpha')$ are similarly oriented marked Riemann surfaces, then there is a quasiconformal mapping $f: (S, \alpha) \rightarrow (S', \alpha')$ . Ok, this sounds plausible enough to me. On the other hand, it is by no means obvious. Here, I should mention Bers has earlier stated the equivalence of a few (standard) definitions of quasiconformality for a mapping $f: S \rightarrow S'$ , including the following: (i) $\hat{f}_\bar{z} = \mu \hat{f}_{z}$ , in terms of weak derivatives, for any coordinate representation $\hat{f}$ , where $\mu \in L^{\infty}$ (ii) any coordinate representation $\hat{f}$ has bounded dilatation across all quadrilaterals. Specifically, Bers goes on to say Had we demanded that the homeomorphism $f$ be continuously differentiable everywhere the proof would be somewhat laborious. Since we use a very general definition of quasiconformality, the proof presents no difficulties and may be omitted. My question is thus, what is the idea that Bers' has in mind for the proof? By hypothesis, we've got our hands on a homeomorphism $f$ that respects the desired marking, but a priori $f$ might have unbounded dilatation. So, we need some clever way to homotope $f$ so that its dilatation becomes bounded across the whole surface, at which point we are done. However, I see no intuitive way to make this work. Technical details aside, I am very interested to see what ideas anyone has, or better yet, what idea they imagine Bers thought would easily yield a proof. Bers, Lipman , Quasiconformal mappings and Teichmüller’s theorem, Princeton Math. Ser. 24, 89-119 (1960). ZBL0100.28904 .","['riemann-surfaces', 'surfaces', 'complex-analysis', 'teichmueller-theory', 'quasiconformal-maps']"
3502025,Tangent bundle for smooth algebraic variety,"I was discussing with some friend about how to define the tangent bundle for a smooth variety since this is very natural in the manifold setting and we couldn't find references discussing in detail. For example, if $X\subset\mathbb{A}^n$ is an affine algebraic variety we can define the tangent bundle $TX\subset\mathbb{A}^{2n}$ , where first $n$ coordinates respect de equations for $X$ and the other the equations for tangent spaces: $\sum_{i=1}^n{\frac{\partial f}{\partial x_i}}(x)y_i=0$ . These imply that $TX$ is an algebraic variety, but my problem is: is this a vector bundle? The problem is that I don't see how it will be locally trivial. Zariski open sets are very large and it will impose a lot of rigidity; also there is no implicit function theorem so we can get trivializations for $TX$ like in complex analytic viewpoint. If this is locally trivial, how can it be shown? Otherwise, can an example be easily found?","['affine-varieties', 'algebraic-geometry', 'tangent-bundle']"
3502090,Prove a trigonometric inequality,"Let $a,b,c$ reals with sum 0. Prove that $\left|\sin{a}+\sin{b}+\sin{c}\right| \leq \frac{3\sqrt{3}}{2}.$ My idea was to set $z_1=\cos{a}+i\sin{a}$ and also $z_2,z_3$ . Then I tried replacing the sines in the inequality with these complex numbers. However, I didn’t get to anything interesting that could help me prove this. Thanks in advance!","['trigonometry', 'cauchy-schwarz-inequality', 'absolute-value', 'a.m.-g.m.-inequality']"
3502198,Why is the axiom of countable choice constructively valid?,"I am reading Andrej Bauer's Five stages of accepting constructive mathematics . Theorem 1.3 proves that the axiom of choice implies excluded middle. Shortly afterwards Bauer implies that the axiom of countable choice is constructively valid. However I can't see why the same proof doesn't show that countable choice also implies excluded middle. The proof of Theorem 1.3 goes as follows: For an arbitrary proposition $P$ define $A = \{ x \in \{0,1\} | P \vee (x=0) \}$ and $B = \{ y \in \{0,1\} | P \vee (y=1) \}.$ Since each of $A$ and $B$ is inhabited (by 0 and 1 respectively), by choice there is a function $f \colon \{ A, B\} \rightarrow A \cup B$ such that $f(A) \in A$ and $f(B) \in B$ . Since $A, B \subseteq \{0,1\}$ , we have exhaustive cases: $f(A) = 1$ . Then $1 = f(A) \in A$ , so $P \vee (1=0) $ , which is equivalent to $P$ . $f(B) = 0$ . Then $0 = f(B) \in B$ , so $P \vee (0=1) $ , which is equivalent to $P$ . $f(A) = 0$ and $f(B) = 1$ . Then we have $\neg P$ , for if $P$ were true, then $A = B= \{0,1\}$ so $0 = f(A) = f(B) = 1$ , which is absurd. In each case we have decided either $P$ or $\neg P$ , so choice implies excluded middle. Why doesn't the same argument go through with countable choice (``every countable family of inhabited sets has a choice function'')? The set of sets we index over here, $\{A, B\}$ , is finite . What's to stop us defining $A_0$ and $A_1$ to be $A$ and $B$ above and then setting, e.g., $A_2 = \{2\}$ , $A_3 = \{3\} \ \dots $ and using the choice function given to us by countable choice to run the same argument again looking at the values $f(A_0)$ and $f(A_1)$ ?","['elementary-set-theory', 'constructive-mathematics', 'logic']"
3502205,Proving that a group of order $p^nq$ for primes $p$ and $q$ is not simple.,Prove that a group of order $p^nq$ for primes $p$ and $q$ is not simple. I've been able to prove the theorem holds for $p=q$ and $p>q$ . If $p<q$ the best I've been able to do is use Sylow to show: $$p^n+p^{n-1}-1\leq q$$ Yet I seem to be stuck. I would appreciate any help.,"['finite-groups', 'simple-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
3502206,Higman’s Lemma for Matrices?,"It is well-known that the set of finite sequences on a finite alphabet is well-quasi-ordered by the subsequence relation. Question : Is the set of finite matrices on a finite alphabet well-quasi-ordered by the submatrix relation? How to prove or disprove? In other words, if $(M_1, M_2,\dots)$ is an infinite sequence of finite matrices on a finite alphabet, is it necessary that there exist $i<j$ such that $M_i$ is a submatrix of $M_j$ ? (For matrices $A$ and $B$ , $A$ is said to be a submatrix of $B$ iff $A$ is obtainable by deleting some or no rows and/or columns of $B$ .) A general version of this question is posed (unanswered) as ""Exercise 1.12 (Higman’s Lemma for Matrices?)"" on p. 17 in Algorithmic Aspects of WQO Theory by Schmitz & Schnoebelen. (I have no idea how to approach this.)","['reference-request', 'matrices', 'order-theory', 'combinatorics', 'sequences-and-series']"
3502266,How do I solve the quintic $n^5-m^4n+\frac{P}{2m}=0$ for $n$?,"I want to solve the following equation for $n$ in terms of $P$ and $m$ . $$n^5-m^4n+\frac{P}{2m}=0$$ I've bought and read many books, including ""Beyond The Quartic Equation"" but I've either missed something or do not have enough background or they said,  'such-and-such is used' but did not show how to use such-and-such to solve what I gather is a Bring-Jerrard quintic equation. I'm just a forklift mechanic 40 years removed from academia with a math hobby. I've been writing a math paper on Pythagorean triples for about $10$ years and, with help, I thought I was almost done with, ""On Finding Pythagorean Triples."" Then, I thought of a new way to find ""Triples On Demand"", i.e. how to find a Pythagorean triple, if it exists, given only the product $(P)$ of A,B,C. Using Euclid's formula: $$A=m^2-n^2\qquad B=2mn\qquad C=m^2+n^2$$ the product is $2m^5n-2mn^5=P$ . The best I've been able to understand is that the first equation above is in Bring-Jerrard form. The only thing I can add is that $P$ is a multiple of $60$ such as $60, 480, 780$ ,etc.  and $m$ will be one of a range of values to test where $\lfloor\sqrt[6]{P}\rfloor\le m\le \lceil\sqrt[5]{P}\space\rceil$ . How do I find the group and know if it is solvable? How does symmetry and/or permutations apply to this equation if at all? How does this equation correspond to an icosahedron? Is there a trig approach like the one here for a cubic equation? $$mn^3-m^3n+D=0$$ Almost any approach would be appreciated. I have so much to learn but none of the answers or comments have been useful so far – the approaches have been self-referential. How do I solve this quintic for $n$ if $P$ and $m$ are known? Update: I changed an $f$ in the OP to a $P$ so don't be confused by some of the comments. Also, I'm starting a bounty but not a large one for fear it will be wasted on the less-than-useful answers that have been upvoted already. Hurry, if you have an answer. I'd prefer to award the bounty rather than have it given away by an algorithm. A comment mentioned I should be more specific about what I want to do. I'm looking for inputs to Euclid's formula (shown above) and which we define here as $F(m,n)$ ––note capitol F. I want one-to-five functions $n_x=f_x(P,m )$ such that, given a number like $4200$ and, knowing $$\lfloor\sqrt[6]{4200}\rfloor=4\le m\le \lceil\sqrt[5]{4200}\space\rceil=6$$ I can discover $$f(4200,4)=3\Rightarrow F(4,3)=(7,24,25)\qquad f(4200,5)\notin\mathbb{N}\qquad f(4200,6)\notin\mathbb{N}$$ If an integer were not found for any of the $[5]$ solutions in the specified range of $m$ -values, then we would know that no Pythagorean triple exists for that value of $P=A\times B\times C$ . Now, I'm told, specific cases are needed before we can find a group. Here are the smallest sample equation values and the ""correct"" solution of $f(P,m)=n$ for each. $$n^5-16n+15=0\rightarrow f(60,2)=1\quad
n^5-81n+80=0\rightarrow f(480,3)=1\quad
n^5-81n+130=0\rightarrow f(780,3)=2\quad
n^5-256n+255=0\rightarrow f(2040,4)=1\quad
n^5-256n+480=0\rightarrow f(3840,4)=2\quad
n^5-256n+525=0\rightarrow f(4200,4)=3\quad
n^5-625n+624=0\rightarrow f(6240,5)=1\quad
n^5-625n+1218=0\rightarrow f(12180,5)=2\quad
n^5-625n+1476=0\rightarrow f(14760,5)=4\quad
n^5-1296n+1295=0\rightarrow f(15540,6)=1\quad
n^5-625n+1632=0\rightarrow f(16320,5)=3\quad
n^5-1296n+2560=0\rightarrow f(30720,6)=2\quad
n^5-2401n+2400=0\rightarrow f(33600,7)=1\quad
n^5-1296n+3355=0\rightarrow f(40260,6)=5\quad
n^5-1296n+3645=0\rightarrow f(43740,6)=3\quad
n^5-1296n+4160=0\rightarrow f(49920,6)=4\quad
n^5-4096n+4095=0\rightarrow f(65520,8)=1\quad$$ Are these sample equations enough to associate with a Galois group? Once we find the group, how do we proceed?","['galois-theory', 'abstract-algebra', 'quintics', 'trigonometry', 'solvable-groups']"
3502272,Conceptual Clarification: Rudin's Definition of a Differential Form of Order k,"Broad Context As the title suggests, I'm reading Rudin's POMA (3e) and have gotten to chapter 10. Miscellaneous online resources have not been as useful as always, since Rudin (notably, I think) defines differential forms without making any mention whatsoever of manifolds, tensor products, or the exterior algebra (which are all things I've found via. google, and which I do not yet know about). Presumably, Rudin's definition should be understandable without any of these concepts, or else he would have had to introduce them. Narrow Context I expect half of the problem will be notation. Here are the definitions I'm working with. $\mathbf{\mathscr{C}'}$ -Mapping: Let $X$ and $Y$ be normed vector spaces over $\mathbb{C}$ . Let $B(X,Y)$ denote the set of all bounded linear mappings $X \to Y$ . Let $A$ be an open subset of $X$ (with distances measured in terms of the norm of $X$ ) such that every $a \in A$ is a limit point of $A$ . Let $f : A \to Y$ be a map. $f$ is said to be $\mathscr{C}'$ in $A$ if $f$ is Frechet differntiable on $A$ and (letting $\mathcal{D} f(x)$ denote the derivative) the map $A \to B(X,Y) : x \mapsto \mathcal{D} f(x)$ is continuous on $A$ (with distances in $B(X,Y)$ measured in terms of the operator norm). Full disclose, I've expanded the above definition to a slightly more general context than Rudin presents (definition 9.20 POMA). $\mathbf{k}$ -Surface in $\mathbf{A}$ : Let $n$ and $k$ be positive integers. Let $A$ be an open subset of $\mathbb{R}^n$ . Rudin defines a $k$ -surface in $A$ to be a $\mathscr{C}'$ mapping $K \to A$ , where $K$ is a compact subset of $\mathbb{R}^k$ . In particular, Rudin says we are to confine our attention to the situation in which $K$ in the above definition is either a $k$ -cell (by which it is meant a Cartesian product of $k$ non-degenerate closed intervals of real numbers), or is the standard $k$ -simplex. Note, the above is Def 10.10 and the below is Def 10.11 in POMA. Differential Form of Order $\mathbf{k \geq 1}$ : Let $A \subset \mathbb{R}^n$ be open. Let $\Omega(A)$ denote the set of all $k$ -surfaces in $A$ (this is my own notation). Let $\phi_1,\ldots,\phi_n$ denote the component functions of a given $\Phi\in\Omega(A)$ . A differential form of order $k$ is a function $\omega : \Omega(A)\to\mathbb{R}$ determined by the rule $$
  \Phi \mapsto \int_{\text{dom}(\Phi)} 
         \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}\big(\Phi(\mathbf{u})\big) 
         \det \big(
             \mathcal{D} (\phi_{i_1}(\mathbf{u}),\ldots,\phi_{i_k}(\mathbf{u}))
         \big) \mathrm{d} \mathbf{u}
$$ where the integral is a classical (what I would call iteratively defined) multiple integral over the compact subset dom $(\Phi)$ of $\mathbb{R}^k$ (that is, the domain of $\Phi$ ). Also, the indices $i_1,\ldots,i_k$ ""range independently from $1$ to $n$ ,"" and ""the functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k}$ are assumed to be real and continuous"" on $A$ .
  Moreover, Rudin says that the above rule is ""symbolically represented by the sum"" $$
  \omega = \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}(\mathbf{x})\hspace{1mm}
           \mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k}.
$$ Rudin does not provide any sort of definition of "" $x \wedge y$ ."" The Question Did I happen to get anything wrong in the above definitions? Is the summation taken over $i\in\{1,\ldots,n\}$ , over $(i_1,\ldots,i_k)\in\{1,\ldots,n\}^k$ , or over some other indexing set? In other words, how many of these weirdly indexed functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k} : A \to \mathbb{R}$ are necessary in order to completely determine $\omega$ ? What the heck is this wedge notation supposed to (as Rudin says) ""symbolically represet?"" It would almost seem to me that $\omega$ is entirely determined by the set $A$ and the functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k}$ , is it not? Do the $\mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k}$ specify a subset of $\{1,\ldots,n\}^k$ over which the summation is to be taken, or something like that? Again, I'm not familiar with manifolds, nor with tensor products, and Rudin has not developed these subjects, at all. Normally, I prefer rigor to intuition, but as a first step, I'll take what I can get. Suggested supplementary readings are also welcome although, realistically, for the foreseeable future I won't have time to read, e.g., a whole book on a subject which is not part of my school work. Thanks for your time.","['differential-forms', 'analysis']"
3502330,Lax-Milgram as a corollary of Stampacchia theorem [Brezis book],"I'm reading Brezis functional analysis,sobolev spaces and pdes book, where at page 140 of the 2010 Springer edition there is the following corollary 5.8 , that is a corollary of Stampacchia theorem (theorem 5.6.) : Assume $a(u,u)$ is a coercive bilinear form on $H$ (where $H$ is a Hilbert space over $\mathbb{R}$ ); then for every $\phi \in H^*$ there exists an element $u$ such that $a(u,v)=\langle \phi,v \rangle,$ for every $v \in H.$ Moreover, if $a$ is symmetric, $u$ is characterized by the property $\mathcal{P}:$ $$u\in H \ \ \text{and} \ \  \frac{1}{2}a(u,u)-\langle \phi,u \rangle = \text{min}_{v \in H}\big\{\frac{1}{2} a(v,v)-\langle \phi,v \rangle \big\}$$ Unfortunately, Brezis gives a very sketchy proof of the corollary, saying that one should just apply the reasoning of a previous corollary (5.4) , which say that if $M$ is a closed linear subspace of $H.$ For $x\in H,$ $y=P_Kx$ is
  characterized by the property that for all $m \in M$ $$ y\in M \ \text{and} \ \langle x-y, m \rangle =0$$ question Can you provide me a more detailed proof of this fact? Either a proof given as an answer or a reference to a detailed proof in some other book is good. Plese read The only request is that I would like to follow the approach of Brezis of deriving it from Stampacchia theorem.","['reference-request', 'hilbert-spaces', 'real-analysis', 'bilinear-form', 'functional-analysis']"
3502331,Fourier transform (The 1-D neutron diffusion equation),"The $1-D$ neutron diffusion equation with a (plane) source is $-D\frac{\mathrm{d^{2}}\varphi (x) }{\mathrm{d} x^{2}}+K^{2}D\varphi (x)=Q\delta (x)$ where $\varphi (x)$ is the neutron flux, $Q\delta (x)$ is the (plane) source at $x = 0$ and $D$ and $K^2$ are
constants. Apply a Fourier transform. Solve the equation in transform space. Transform your solution back into x-space. ANS : $\varphi (x)=\frac{Q}{2KD}e^{-|Kx|}$","['fourier-transform', 'ordinary-differential-equations']"
3502337,characterization of multivariate normal distribution by its one-dimensional marginals,"Let $X$ be a random vector in $\mathbb R^n$ . we say $X$ has the normal distribution $N(\boldsymbol{\mu},\boldsymbol{V})$ if it has the density function: $$\frac{1}{(2\pi)^{D/2}|\boldsymbol{V}|^{1/2}} \exp \left( -\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T \boldsymbol{V}^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \right) $$ Using this definition, I wonder how to show that $X$ has a multivariate normal distribution if and only if every one dimensional marginal $\langle X,\theta \rangle$ is normal (in one-dimensional sense). I have trouble in both directions: (1) For the forward direction, I am not sure how to find the density of $\langle X,\theta \rangle$ for a fix vector $\theta$ so that I can show the one-dimensional marginals are normal. (2) For the backword direction, a hint says we may use the Cramer-Wold device, which says the sequence $X_n$ converges to $X_0$ in distribution if $\langle X_n,\theta \rangle$ converges to $\langle X_0,\theta \rangle$ in distribution for all fix vector $\theta$ . But I am not sure how to use this theorem in the proof (where is the convergence?). This is the Exercise 3.3.4 in Vershynin's high dimensional probability book. But I can't find it in some other prbability books, say Durrett.",['probability-theory']
3502384,how does $(A \cap B) \cap (B \cap C)$ lead to $A \cap B \cap C$?,I was reading the proof of inclusion-exclusion principle for three events. I am wondering how does $$(A\cap B)\cap(B\cap C)$$ lead to $A\cap B \cap C$ ? Is it by associativity so that we can rewrite as $A \cap (B \cap  B) \cap C$ so then becomes $A \cap B \cap C$ ?,"['elementary-set-theory', 'inclusion-exclusion']"
3502385,Solving $2^x + 3^x = 6^x$,"How would I solve $2^x + 3^x = 6^x$ ? So far, I've been able to simplify the equation into $$(2^x - 1)(3^x - 1) = 1,$$ but I'm not quite sure where to go from here. Additionally, letting $y = 2^x$ yielded $$(y-1)(y^{\log_23} - 1) = 1,$$ however as with before, I'm pretty much stuck at this step. WolframAlpha says that $x \approx 0.787885$ , but I'd like a closed form for $x$ if possible.",['algebra-precalculus']
3502395,"Why do mathematicians say ""map"" and ""mapping"" when talking about functions?","I don't know if this is the right place to ask this, but I realized I've been using the word ""map"" to mean ""function"" for some time now and I have no idea why. When did people start using ""map"" this way, and how is it connected to the usual meaning of ""map"" (a visual representation of an area)? The only thing I can think is that charts on a manifold are very much like maps in the traditional sense, but most functions aren't manifold charts.","['functions', 'math-history', 'terminology']"
3502425,"Existence of a decomposition of an arbitrary rotation into three rotations about the $x,y,z$ axis respectively.","In reading about Euler Angles from various sources on the internet, it seems the treatment of this subject usually assumes that for an arbitrary rotation $3 \times 3$ rotation matrix $R$ with real entries, that there exists various decompositions of $R=ABC$ where $A,B,C$ are rotations of three angles respective to three co-ordinate axes, and then proceeds to show how to find the angles. Examples include, for three angles in radians, say $\psi, \theta, \phi$ , a decomposition $R=R_x(\psi)R_y(\theta)R_z(\phi)$ , i.e. rotations around the $x,y,z$ axis respectively. Wikipedia also includes in their description here , Proper Euler Angles a decomposition using these rotation axis': Proper Euler angles (z-x-z, x-y-x, y-z-y, z-y-z, x-z-x, y-x-y), where the first decomposition reuses the z axis. In the case where we wish to express $R=R_z(\psi)R_y(\theta)R_x(\phi)$ , I am trying to write an existence proof for such a decomposition. If we assume this is true, then we can solve $\small R = \begin{bmatrix} R_{11} & R_{12} & R_{13} \\ R_{21} & R_{22} & R_{33}\\R_{31} & R_{32} & R_{33}\end{bmatrix} = \begin{bmatrix} \cos \psi & -\sin \psi & 0\\ \sin \psi & \cos \psi & 0 \\ 0 & 0 & 1  \end{bmatrix} \begin{bmatrix} \cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta\\ \end{bmatrix} \cdot \begin{bmatrix}1 & 0 & 0 \\ 0 & \cos \phi & -\sin \phi \\ 0 &  \sin \phi & \cos \phi \end{bmatrix}$ Giving $R = \begin{bmatrix} \cos \theta \cos \phi & \sin \psi \sin \theta \cos \phi - \cos \psi \sin\phi & \cos \phi \sin \theta \cos \phi + \sin \psi \sin \phi\\ \cos\theta \sin\phi & \sin \psi \sin\theta \sin \phi + \cos \psi \cos \phi & \cos \psi \sin \theta \sin \phi - \sin \psi\cos \phi & \\-\sin \theta & \sin \psi \cos \theta & \cos \psi \cos \theta \end{bmatrix}$ . Then assuming existence, we can solve for each angle, where for example $\theta = - \sin ^{-1}(R_{31})$ . I am not sure why such a decomposition exists, a priori. Any insights appreciated.","['linear-algebra', 'geometry']"
3502508,Prove $\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^2=\frac{\pi^2}{24}$,"how to prove that $$\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^2=\frac{\pi^2}{24}\ ?$$ where $\overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k}$ is the alternating harmonic number. This problem is proposed by a friend on a Facebook group and I managed to prove the equality using only integration but can we prove it using series manipulations? Here is my work, In page $105$ of this paper we have $$\overline{H}_n-\ln2=(-1)^{n-1}\int_0^1\frac{x^n}{1+x}dx$$ Therefore $$(\overline{H}_n-\ln2)^2=\int_0^1\int_0^1\frac{(xy)^n}{(1+x)(1+y)}dxdy$$ $$\Longrightarrow \sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^2=\int_0^1\int_0^1\frac{dxdy}{(1+x)(1+y)}\sum_{n=0}^\infty(-xy)^n$$ $$=\int_0^1\int_0^1\frac{dxdy}{(1+x)(1+y)(1+xy)}=\int_0^1\frac{1}{1+x}\left(\int_0^1\frac{dy}{(1+y)(1+xy)}\right)dx$$ $$=\int_0^1\frac{1}{1+x}\left(-\frac{\ln\left(\frac{1+x}{2}\right)}{1-x}\right)dx=-\int_0^1\frac{\ln\left(\frac{1+x}{2}\right)}{1-x^2}\ dx,\quad x=\frac{1-u}{1+u}$$ $$=\frac12\int_0^1\frac{\ln(1+u)}{u}du=\frac12(-\operatorname{Li}_2(-1))=\frac12(\frac12\zeta(2))=\frac{\pi^2}{24}$$ Regarding series manipulation, we know that $$\overline{H}_{2n}=H_{2n}-H_n$$ but we need $\overline{H}_n$ , so is this identity helpful? any other ideas? thank you","['integration', 'alternative-proof', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
3502530,Construct a field having 49 elements,"I am working on this problem from Herstein's book, Abstract Algebra, 3rd edition.
One question asks to construct a field of 49 elements. It gives a hint to use the ring of Gaussian integers and a maximal ideal. This is what I have so far.
All rings are assumed to have a unit and be commutative. Let R=Z[i] , M:= {a+bi| 7|a and 7|b} Its pretty easy to show M is an ideal of R. To show M is maximal, suppose $\exists$ N $\subset$ R, where N is an ideal and M $\subsetneq$ N. So $\exists$ a+bi $\in$ N s.t 7 $\not|$ a or 7 $\not|$ b. 
so using modular arithmetic we get that $$a^2 + b^2\equiv 1,2,3,4,5,6\pmod7$$ .
Therefore letting t = $a^2 + b^2 $ , $\exists$ p,q $\in$$\mathbb{Z}$ such that  7p + tq = 1 Since N is an ideal and 7 and t are in N, 1 is also in N so it is the whole ring. So M is maximal and R/M is a field. To show that it has 49 elements, I dont know if what I'm doing is correct. I constructed a group homomorphism from the additive group property of R/M to the group $\mathbb{Z}$$_7$$\times$$\mathbb{Z}$$_7$ $\phi$ :R $\rightarrow$$\mathbb{Z}$$_7$$\times$$\mathbb{Z}$$_7$ a+bi $\rightarrow$ ( $\bar a$ , $\bar b$ ). This map is surjective and a group homomorphism
The kernel of this map is also M. So by the first homomorphism theorem for groups I get that R/M $\cong$ $\mathbb{Z}$$_7$$\times$$\mathbb{Z}$$_7$ as groups, therefore |R/M| = 49 Is it correct to do this, view the field as a group and construct a function to find out how many elemnts it has?",['abstract-algebra']
3502672,relation variance and covariance,"Let’s assume a (weakly) stationary process $\{u_t\}$ , such that the mean of $u_t$ and
the covariance $(u_t, u_{t+h})$ do not depend on $t$ . For simplicity, we assume that $E(u_t) = 0$ . (Don't know how whether this is relevant.). Then my book makes the following step: $$var(\sum^T_{t=1}u_t)=\sum^T_{t=1}\sum^T_{s=1}cov(u_s,u_t).$$ I don't understand this step. Isn't the variance the covariance with itself?","['statistics', 'variance']"
3502683,I need help proving a result regarding multipliers for the HK integral,"A function $g:[a,b]\rightarrow \mathbb{R}$ is said to be of bounded variation on $[a,b]$ if $$
\sup\left\{\sum_{i=1}^n|g(x_i)-g(x_{i-1})|:a = x_0 <\ldots<x_n = b\right\}<+\infty.
$$ If $g:[a,b]\rightarrow\mathbb{R}$ is of bounded variation on $[a,b]$ and $f:[a,b]\rightarrow \mathbb{R}$ is HK integrable on $[a,b]$ then $fg$ is HK integrable on $[a,b]$ . I have found proof of this that I am content with. I need help in proving or disproving the following statement: Let $g:[a,b] \rightarrow \mathbb{R}$ be a function. Then $fg$ is HK integrable on $[a,b]$ for all HK integrable functions $f:[a,b]\rightarrow \mathbb{R}$ if and only if $\exists
g_0:[a,b]\rightarrow\mathbb{R}$ such that $g_0$ is of bounded variation on $[a,b]$ and $g_0 = g$ everywhere except for a set of Lebesgue measure $0$ . If my claim is wrong, please let me know.","['integration', 'analysis', 'real-analysis', 'functional-analysis', 'gauge-integral']"
3502685,"$f^{-1}:W(f) \to [1,\infty)$ of $f:[1, \infty) \to \mathbb{R}, x \mapsto f(x) = x^2-2x+3$","Let $f:[1, \infty) \to \mathbb{R}, x \mapsto f(x) = x^2-2x+3$ How can one find the inverse function $f^{-1}:W(f) \to [1,\infty)?$ Without the interval I know that for $y$ we get $y=1+\sqrt{x-2}, y = 1 - \sqrt{x-2}$ for the inverse function, but I don't know how it's done when an interval is given. After graphing the function one knows that the codomain $W(f)$ is given by $W(f) = f([1,\infty)) = [2,\infty)$ . But how can one show that?","['functions', 'analysis', 'inverse-function']"
3502704,Smoothness of general divisors in a pencil with base locus,"Everything is over $\mathbb C$ . Let $f:X \to \mathbb P^2$ be a flat projective morphism and $x_0\in \mathbb P^2$ be a fixed point in $\mathbb P^2$ , and all the lines ( $\mathbb P^1$ ) through it are denoted by $\{L_\alpha\}_{\alpha \in \mathbb P^1}$ . We assume that $X$ is smooth, and all the fibers $f^{-1}(x)$ except for $x=x_0$ are smooth. I would like to know if the following is true: There exists at least one $L_\alpha$ such that $f^{-1}(L_\alpha)$ is smooth. I tried to use some sort of Bertini theorem, which says if a linear system has no base point, then a general divisor would be smooth. However, here $\{f^{-1}(L_\alpha)\}$ certainly contains base locus $f^{-1}(x_0)$ . So I guess the statement is not true. But I want an example to see how this fails. Thanks in advance.","['complex-geometry', 'divisors-algebraic-geometry', 'algebraic-geometry', 'intersection-theory']"
3502752,How does $\tan 70^\circ - \sec 10^\circ$ have the exact value of $\sqrt{3}$?,"Recently, while trying to solve a problem posed by the Youtube video here , I found the following relation: $$\tan 70^\circ - \sec 10^\circ = \sqrt{3}$$ This relation is exact, and that can be proved by combining the purely geometric arguments presented in the video linked above and some basic trigonometry. But as far as I know, the trigonometric ratios of $10^\circ$ and $70^\circ$ cannot be expressed in exact form (using only square roots). In fact, Gauss proved that the sine or cosine of any angle ${360^\circ}\over n$ cannot be expressed in terms of fractions and square roots unless $n = 2^m\cdot\prod p_i$ where every $p_i$ is a Fermat prime : neither $10^\circ$ nor $70^\circ$ is even constructible. So how does the stated relation hold? I have been unable to prove why it should ... any insights appreciated!","['trigonometry', 'proof-writing']"
3502909,Partial derivative of $(x^3+y^3)^{1/3}$,"Let $f(x,y) = (x^3+y^3)^{1/3}$ . Computing the partial derivative with respect to $x$ , we get $$
\frac{\partial f}{\partial x} = \frac{x^2}{(x^3+y^3)^{2/3}}.
$$ This can't be evaluated at $(0,0)$ , and the limit doesn't exist at $(0,0)$ either (approaching the origin along the $x$ -axis and the $y$ -axis give different values). However, if we use the definition of the partial derivative at $(0,0)$ we get $$
\frac{\partial f}{\partial x} (0,0) = \lim_{h \to 0} \frac{f(0+h,0) - f(0,0)}{h} = \lim_{h \to 0} \frac{(h^3)^{1/3}}{h} = 1,
$$ so the definition seems to tell us that this partial derivative is $1$ . Why does differentiating with respect to $x$ and then taking a limit not give the same answer?","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
3502916,Why does $\cos^{-1}\left(\cos(30)\right)$ (using radians) give the digits of $\pi$ after $3$?,"I was messing around with my calculator when in radian mode I entered the following $$\cos^{-1}\left(\cos(30)\right)$$ and it gave back $1.4159$ . Basically, the digits of pi after $3$ . Is this merely a coincidence or is there something more to this? It seems kinda interesting.",['trigonometry']
3502925,Gradient of a function on a Kähler manifold,"Given two real functions $f, h$ on a Kähler manifold $(X, \omega)$ , I am trying to make sense of the following equality: $\Delta_{\omega}(fh) = (\Delta_{\omega} f) h + f (\Delta_{\omega}h) + 2 \nabla_{w} f \cdot \nabla_{w}h$ . In local coordinates $\Delta_{\omega}(fh)  = g^{i\bar j} \partial_i \partial_{\bar j} (fh) = g^{i\bar j}\partial_i f \partial_{\bar j}h +g^{i \bar j} (\partial_i \partial_{\bar j} f)  h + g^{i \bar j} \partial_{\bar j} f \partial_{i} h + g^{i \bar j} (\partial_i \partial_{\bar j} h)  f = (\Delta_{\omega} f) h + f (\Delta_{\omega}h) + g^{i\bar j}\partial_i f \partial_{\bar j}h + g^{i \bar j} \partial_{\bar j} f \partial_{i} h$ How are the remaining two terms assembled into forming the dot product of the gradient? How is the gradient define on a kahler manifold? Thoughts: Since we are dealing with real functions, can potentially use the induced Riemannian metric of a Kähler metric to define the gradient in the usual sense. Is $\nabla_{\omega}$ intepreted like so? What would be a local expression in terms of coordinates?","['kahler-manifolds', 'differential-geometry', 'riemannian-geometry', 'real-analysis']"
3502957,What are are some illustrative (non-)examples of proper morphisms?,"""Proper"" is an adjective used to describe a morphism of spaces— topological spaces, schemes, locales, etc —that is sufficiently nice and has some neat properties . Between topological spaces a morphism is proper if the preimage of compact set is compact, which is a clean definition, but it doesn't give me any geometric intuition that generalizes to schemes or beyond. Similarly the definitions I've seen for a proper morphisms of schemes hasn't helped improve my intuition. What are some flags/heuristics/features of a morphism that I should look out for to decide whether or not it's proper? What are some illustrative examples or non-examples of proper morphisms, in whatever setting, that could help build my geometric intuition here? This answer over on MathOverflow helps a bit, but only a bit,","['algebraic-geometry', 'intuition', 'topos-theory', 'locales', 'schemes']"
3502964,"Find a function $f$ such that $\gcd(f(x)-f(y),x-y)\mid 2$ For all integers $x,y$.","Find a function $f$ such that $\gcd(f(x)-f(y),x-y)\mid 2$ For all integers $x,y$ . This was a question proposed to me by my teacher to attempt, I tried to find a function such that $f(x)-f(y)=x-y+1 $ but there are no solutions for that because $f(y)-f(z)=y-z+1$ therefore $f(x)-f(z)=x-z+2$ which is a contradiction Excuse any bad english please, it is not my first language","['contest-math', 'gcd-and-lcm', 'functions']"
3502980,Width and thickness of the Samsung Odyssey G9 monitor,"The width and thickness of the upcoming Samsung monitor have not been released yet. However, we know it's a part of a circle of 1m radius and we know the length of that part of the circle. I'm guessing we should be able to get the width and thickness of the monitor right? Here's where I'm at: Thanks for helping me to see if it will fit my desk :)","['trigonometry', 'linear-algebra']"
3503003,Special name for two variables that sum to one?,"I'm curious if there is any special name for the variables in the following function: $\alpha+\beta=1$ , such that each can be determined by subtracting 1 from the other. Sort of like saying $\alpha$ is the 'reciprocal' of $\beta$ ...but through subtraction by 1 instead of division by 1.","['convention', 'algebra-precalculus', 'terminology']"
3503007,Matrix right multiplication orbit invariant,"Let M and N be two square matrices such that they have an equal image. Show that there exists an invertible matrix P, such that N = MP I have managed to prove the converse...but this is tougher because I'm guessing we need to build a linear map which then identifies with a matrix P.","['matrices', 'group-actions', 'linear-algebra', 'linear-transformations']"
3503019,Well-definedness of the pullback on covectors.,"Basic definitions in question: Let $M,N$ be smooth manifolds, and consider a smooth map $\phi : M \rightarrow N$ . The push-forward map is the map: $$\begin{align}
\phi_* : & \ TM \rightarrow TN \\
& \ X \mapsto \phi_*(X)
\end{align}$$ $$\text{with} \ \phi_*(X)f = X(f\circ \phi) \ \forall f\in C^{\infty}(N)$$ The pull-back map is the map: $$\begin{align}
\phi^* : & \ T^*N \rightarrow T^*M \\
& \ \omega \mapsto \phi^*(\omega)
\end{align}$$ $$\text{with} \ \phi^*(\omega)(X) = \omega(\phi_*(X)), \ X \in TM$$ Easy to see that the image of a fibre over $p$ , $T_pM$ , of the tangent bundle $TM$ under the push-forward $\phi_*$ is contained in the fibre over $\phi(p)$ in the corresponding tangent bundle $TN$ : $$\phi_*(T_pM) \subseteq T_{\phi(p)}N.$$ However, it was also claimed that the pull-back of a generic covector $\omega \in T_{\phi(p)}^*N$ will be a covector $\phi^*(\omega) \in T^*_pM$ , where I particularly emphasise the $p$ in $T^*_pM$ . The question: Given that $\phi$ is not known to be injective, isn't it impossible for this definition to guarantee that a covector $\omega$ defined at $x=\phi(p)\in N$ will necessarily be pulled back to a covector at the point $p$ of $M$ ? Patently, if $\phi$ is not injective, there could exist $q\not=p$ with $x=\phi(p)=\phi(q)$ -- so would the pullback of $\omega$ lie in $T^*_pM$ or $T^*_qM$ ? Worse still, what does one do with covectors defined at points in $N$ that don't lie in the image of $\phi$ ? But, if injectivity/surjectivity of $\phi$ is indeed the minimal requirement to have a well-defined pullback of this type, this would consequently impose constraints on the dimensions of $M,N$ . This was certainly not discussed (although I can see this working better in the case of embedding a lower dimensional manifold in one of higher dimension, for example). It's more a question of, what am I missing here? I note that this is a map between cotangent bundles as opposed to the spaces of sections of the cotangent bundles. Perhaps, this is an acceptable definition when acting on forms/covector fields ? I stumbled upon this ( Definition of pullback. ), which states ""This situation with forms is different. For differential forms the pull-back is well-defined even if the function is not injective."" in the top answer.",['differential-geometry']
3503073,"When solving an SAS triangle, why do you have to use the law of sines to find the angle opposite of the shortest remaining side?",In my math class (precalc) my book states the following rules for solving an SAS triangle using law of cosines. Solving an SAS Triangle Use the Law of Cosines to find the side opposite the given angle. Use the Law of Sines to find the angle opposite the shorter of the two given sides. This angle is always acute. Find the third angle by subtracting the measure of the given angle and the angle found in step 2 from 180. Why do you need to follow step #2?,"['trigonometry', 'algebra-precalculus', 'triangles', 'geometry']"
3503079,Complete Induction on two variables,"How should I go about proving P(x,y) $\forall x,y$ using complete induction? I couldn't find an example for this kind yet. (Some posts in here used normal induction, something like. a) P(0,0), b) for some x, y: $P(x,y) \rightarrow P(x+1,y)$ and c) for some x, y: $P(x,y) \rightarrow P(x,y+1)$ . ) In my case, with P(x,y) alone I can not conclude P(x+1,y) or P(x, y+1). Thus, I wish to assume P(u,v) $\forall u,v < x, y$ in order to prove P(x,y). Would the following scheme valid? prove P(0,0) prove P(0,y) $\forall y$ prove P(x,0) $\forall x$ assume P(u,y) $\forall u < x$ and $\forall y$ , prove P(x,y) assume P(x,v) $\forall x$ and $\forall v < y$ , prove P(x,y) then conclude from 1-5 that P(x,y) $\forall x,y$ . Thanks","['induction', 'discrete-mathematics']"
3503086,Why does $\int_C\frac{dz}{z}=2\pi i$? [duplicate],"This question already has answers here : Cauchy's Theorem on Path Integrals (3 answers) Closed 4 years ago . I was thinking that I don't really understand why $$\int_{|z|=1}\frac{dz}{z}=2\pi i$$ I do understand how to calculate it and it's importance for residues, Cauchy etc... The question is that, if $f(z)=z^{-1}$ , then $f(z)=\overline{z}$ when $|z|=1$ , somehow there must be an asymmetry in $f(z)$ along $|z|=1$ in order for the integral to be $2\pi$ units up in the imaginary axis. If $C$ is the unit circle traversed in the natural (anticlockwise) direction at ""1 m/s speed""(i.e. $e^{i\theta}$ for $\theta\in[0,2\pi]$ ) then $f(C)$ is again the unit circle traversed at ""1 m/s"" speed but in a clockwise manner. Then again, I don't see any asymmetry: you will pass both by $z$ and $-z$ and those should cancel giving $0$ as the value of the integral. I tried to think of the line integral in terms of area: $ie^{i\theta}$ is the tangent vector to the curve at $z=e^{i\theta}$ and an infinitesimal triangle (or circle sector) should have area $\frac{1}{2}ie^{i\theta}e^{i\theta}$ . Couldn't get any further because what meaning does ""complex number value area"" mean? Is there some interpretation? I am not convinced that the $\gamma'(t)$ factor appearing in the calculations really makes the difference (or how it would).
Just to add to my controversy, the function $f(z)=z^{-2}$ takes the unit circle into the unit circle but ""changes speeds"" by which I mean that it gives somehow ""more weight"" to some points (it stretches the upper semicircle into the whole circle) Maybe the question sounds stupid and I hope it does have a simple explanation. Thanks!","['integration', 'complex-analysis']"
3503132,Distributing a Combination of Identical and Distinct Objects in distinct boxes,"Q1 . How many ways are there to distribute 4 distinct oranges and 6 distinct apples into 5 distinct boxes? Approach 1 : Let us assume the two tasks disjoint. So oranges go in $5^4$ ways and apples in $5^6$ ways. So total ways in which both tasks happen together is $5^4*5^6=5^{10}$ Approach 2 : Assume there are together 10 distinct objects to be distributed. Answer is $5^{10}$ Q2 . How many ways are there to distribute 4 identical oranges and 6 distinct apples into 5 distinct boxes? Approach 1 : The two tasks are disjoint. orange task can be done in $\binom{4+5-1}{4}$ Using Sticks and Stones for Counting number of Ways. apple task is as before. so total number of ways $=\binom{4+5-1}{4}*5^6$ Approach 2 : But if i combine both tasks as in Q1, first assume that oranges are different then i have total number of ways is $5^4*5^6=5^{10}$ . Now since oranges are identical, so total number of ways should be $5^{10}/4!$ . This is not even a whole number. So $\binom{4+5-1}{4}*5^6\neq 5^{10}/4!$ Why is approach 2 going wrong in Q2 and not Q1?","['combinatorics', 'discrete-mathematics']"
3503143,Difference between Adjoint of a matrix and its transpose,"For simplicity let $T:R^{n}\rightarrow R^{n},$ be a linear operator and $[A]_T$ be the matrix of  operator $T.$ Then matrix of adjoint $T^{\times}$ operator of $T$ is given as $$[A]_{T^{\times}}=[A]_{T}^t$$ where $t$ denotes transpose of a matrix. My Question is that then from above equality the notions of transpose of a matrix and adjoint of a matrix are same, they why we use separate names?? while in some textbooks adjoint of a matrix is referred to the transpose of coefficient matrix.","['transpose', 'linear-algebra', 'linear-transformations']"
3503172,"$(M_f, X \cup Y)$ has the homotopy extension property","Let $X,Y$ be spaces and $f:X \to Y$ a continuous map. I want to show that $(M_f, X \cup Y)$ has the homotopy extension property. In the proof of Whitehead's theorem (Theorem 4.5 in Hatcher's Algebraic Topology), Hatcher says that this is obvious, but I can't see this even in intuition. How do I  have to show this? Here, we are regarding $X$ and $Y$ as subspaces of the mapping cylinder $M_f$ .","['general-topology', 'homotopy-extension-property', 'homotopy-theory', 'algebraic-topology']"
3503176,Mode of a dataset having distinct numbers,"I am actually a bit confused... about the definition of a mode . According to the definition I read, it came to my notice that the number with highest frequency has to be a mode for a given data set , but then what if I have all the numbers as distinct... In that scenario we won't have a particular number having a frequency more than other elements in the data set...
  Now if I consider a case when we have 2 numbers in a dataset with same max number of occurrences like: $$2,3,4,5,3,2$$ Here 2, 3 both happen to have same maximum frequency and thus we say there are 2 modes...
  The above is stated similar in case we have 3 modes or multi modes ...
  So if there are all distinct numbers then we would have each number having the same maximum frequency as 1 ..so we can say all the numbers are modes ...for that dataset...But then I have seen on some websites claiming that such data sets have ""NO MODE"". So again my thought process is contradicting...what claims I am seeing on websites...
So plz some one help me get this basic concept resolved ...","['statistics', 'combinatorics']"
3503224,Proving $\mathbb{P}(X=Y)=0$ for independent random variables,"If I assume that $\mathbb{P}(X=x)=\mathbb{P}(Y=x)=0$ for all $x\in\mathbb{R}$ with $X$ and $Y$ independent random variables on the same probability space, does it follow that $\mathbb{P}(X=Y)=0$ ? My reasoning went as follows: $\mathbb{P}(X=Y)=\sum_{x\in\mathbb{R}}\mathbb{P}(X=x \space\cap\space Y=x)=\sum_{x\in\mathbb{R}}\mathbb{P}(X=x)\mathbb{P}(Y=x)=\sum_{x\in\mathbb{R}}0=0$ . However, I'm not sure that every concept that I have used here is applicable in the most general setting. Could anyone verify whether this proof is correct for all probability spaces?","['measure-theory', 'probability-theory', 'probability']"
3503245,Find $\max(x^2y+y^2z+z^2x+xyz)(x^2z+y^2x+z^2y+xyz)$ subject to $x+2y+3z=4$,"Let $x$ , $y$ and $z$ be non-negative numbers such that $x+2y+3z=4.$ Find: $$\max(x^2y+y^2z+z^2x+xyz)(x^2z+y^2x+z^2y+xyz).$$ I took this problem here: https://dxdy.ru/topic18767-30.html a last post. This problem is a similar to many contests problems. On one of Canadians olimpiads was $x^2y+y^2z+z^2x\leq4$ for non-negatives $x$ , $y$ and $z$ such that $x+y+z=3.$ Also, $x^2y+y^2z+z^2x+xyz\leq4$ with the same conditions. My attempts: For $(x,y,z)=(2,1,0)$ we get a value $8$ , which looks as a maximal value. I solved this problem for $x=\min\{x,y,z\}$ and for $y=\min\{x,y,z\}$ . But for $z=\min\{x,y,z\}$ we need to prove that $$(x+2y+3z)^6\geq512(x^2y+y^2z+z^2x+xyz)(x^2z+y^2x+z^2y+xyz),$$ which after substitution $x=z+u$ , $y=z+v$ gives something very hard: $$38464z^6+64(473u+1202v)z^5+16(447u^2+3068uv+4092v^2)z^4+$$ $$+32(7u^3+234u^2v+1044uv^2+952v^3)z^3+$$ $$+4(7u^4-200u^3v+808u^2v^2+3040uv^3+2032v^4)z^2+$$ $$+4(9u^5-38y^4v-152u^3v^2+208u^2v^3+592uv^4+288v^5)z+$$ $$+(u-2v)^2(u^4+16u^3v+120u^2v^2+64uv^3+16v^4)\geq0.$$ Thank you!","['contest-math', 'inequality', 'buffalo-way', 'multivariable-calculus', 'algebra-precalculus']"
3503269,Octonion Algebras Over Number Fields,"Is there any textbook or paper about Arithmetic of Octonion Algebras or Octonion Algebras constructed over number fields?
I know J. Voight book and K. Martin notes about quaternion algebras but I was thinking about a generalization using Octonions. More than that: Is it possible to define Orders and Maximal Orders over an octonion algebra? Or these concepts just make sense over associative algebras?","['abstract-algebra', 'quaternions', 'octonions']"
3503310,Rigorous proof of the Jordan-Hölder Theorem,"As far as I am aware, the standard books on abstract algebra (Lang, Dummit, Rotman, Grillet, etc.) do not give a rigorous proof of the Jordan-Hölder Theorem. Here are two examples from Lang and Grillet: As you can see, there's a lot of prose in these proofs. I do not have any issues with these ""prose-laden"" proofs per se; however, I do like to replace them with their formal counterparts whenever possible. To some of you this may seem rather trivial, but to me it isn't: I genuinely do not understand these sorts of proofs of the Jordan-Hölder Theorem. Let me explain why. Let $G$ be a group (finite or otherwise) with identity $e$ . To say that two normal series $(H_i)_{0\leq i\leq n}$ , $(K_j)_{0\leq j\leq m}$ of $G$ are equivalent means that $n=m$ , and $(\forall i)(i\in[0,n-1]\Rightarrow \frac{H_i}{H_{i+1}}\,\simeq\,\frac{K_{\phi(i)}}{K_{\phi(i)+1}})$ , where $\phi:[0,n-1]\rightarrow[0,n-1]$ is a permutation of the interval $[0,n-1]$ . Now go back to the proofs of Lang and Grillet: Is it at all obvious how these proofs satisfy (1) and (2)? (They don't even bother to construct a candidate for $\phi$ .) Logically speaking, I should be able to give a formal proof of (1) and (2) being satisfied for two composition series, no? Ok—Let $(H_i)_{0\leq i\leq n}$ , $(K_j)_{0\leq j\leq m}$ be two composition series of $G$ . By Schreier, we know that these two series have equivalent refinements: $(H'_i)_{0\leq i\leq n'}$ , $(K'_j)_{0\leq j\leq n'}$ . This means that we have a permutation $$\psi:[0,n'-1]\rightarrow [0,n'-1]$$ such that $$(\forall i)(i\in[0,n'-1]\Rightarrow \frac{H'_i}{H'_{i+1}}\,\simeq\,\frac{K'_{\psi(i)}}{K'_{\psi(i)+1}}).$$ Now I have to show that (1) and (2) hold for our original series. That partly means I should have to prove (either by construction or by contradiction) that their exists a permutation of the interval $[0,n-1]$ for which (2) holds: this means that I should be able to assume $i\in[0,n-1]$ and show that the consequent must also hold. Let us fix some notation: \begin{align*}
\mathcal{A}&=\{X\ |\ (\exists i)(i\in[0,n'-1]\ \land\ X=H'_i/H'_{i+1})\}\\
\mathcal{B}&=\{X\in\mathcal{A}\ |\ K\simeq\{e\}\}\\
\mathcal{C}&=\{X\ |\ (\exists i)(i\in[0,n-1]\ \land\ X=H_i/H_{i+1})\}
\end{align*} Then $\mathcal{A}-\mathcal{B}=\mathcal{C}$ . Let $\mathcal{A'},\mathcal{B'},\mathcal{C'}$ denote the same sets except with $K'_{\psi(i)}/K'_{\psi(i)+1}$ , $K_i/K_{i+1}$ and $m$ replacing $H'_i/H'_{i+1}$ , $H_i/H_{i+1}$ and $n$ . Then also $\mathcal{A'}-\mathcal{B'}=\mathcal{C'}$ . You can say that this shows that the non-trivial factors of each refinement are the factors of its original series. This is roughly what Grillet says mid-paragraph. But why should $m=n$ ? I should be able to derive a contradiction by supposing $n\ne m$ , no? Moreover, how does this prove the existence of a permutation $\phi$ satisfying the formula $$(*)\ \ \ (\forall i)(i\in[0,n-1]\Rightarrow \frac{H_i}{H_{i+1}}\,\simeq\,\frac{K_{\phi(i)}}{K_{\phi(i)+1}})?$$ The part of the proof involving the permutation is essentially ignored by both Lang and Grillet. Is it because it is unimportant? Then why include it in the definition of equivalence ? In any case, I would appreciate any help I can get w.r.t the proof of the implication $(*)$ . EDIT: Two votes have been cast to close this question; I am not sure why. I will restate my request in order to once again establish that I have asked this question in good faith. What I am looking for is very simple: (a) a proof that deduces $H_i/H_{i+1}\simeq K_{\phi(i)}/K_{\phi(i)+1}$ from the premise $i\in[0,n'-1]$ (essentially a demonstration of a logical implication). I would appreciate it if you could try to only invoke Schreier as an existential result (i.e. not putting further assumptions on the form of the equivalent refinements); (b) a proof that shows (and not merely says) why $m=n$ . This has to be a formal set theoretic proof of equinumerosity. Please do not give me a prose proof. I already have Lang et al. for that. Also, I have eyes and I can read. Therefore, please, do not give me chewed up versions of Lang's proof. I think this is a reasonable demand. If you think that I have violated the rules, please leave a comment explaining why. Thank you. EDIT 2: Consider the following lists: \begin{align*}
&0,1,\ldots,n-1 &(a)\\
&H_0/H_1,\ldots, H_{n-1}/H_n &(b)\\
&H'_0/H'_1,\ldots, H'_{n'-1}/H'_{n'} &(c)\\
&K'_{0}/K'_{1},\ldots, K'_{n'-1}/K'_{n'} &(d)\\
&K_0/K_1,\ldots, K_{m-1}/K_m &(e)\\
&0,1,\ldots,m-1 &(f)
\end{align*} Using the notation employed above, I will try to explain my difficulty with the proof 3.0 (maybe the next patch giving us version 4.0 will be the lucky one?), submitted below, in its author's favourite medium for doing mathematics: prose. The strategy is very simple: if you can show that you can put list $(a)$ and list $(f)$ in one to one correspondence by only using the lists $(b)-(e)$ , then you can be said to have shown that $m=n$ . The lists $(a)$ and $(b)$ are in a one to one correspondence through the rule $i\mapsto H_i/H_{i+1}$ . Moreover, $(b)$ and its surrogate list in $(c)$ are also bijectively connected, e.g. by the map which sends each $H_i$ to the $H'_j$ such that $H_i=H'_j$ where $j$ is the maximal of the set of all such indices (you can probably do this in some other way too); denote it by $r_i$ . One can connect the lists $(f)$ , $(e)$ and $(d)$ in a similar manner. That leaves us with $(c)$ and $(d)$ . I don't see how these two lists can be connected other than by using the identifications sanctioned by $\psi$ . For example: I will send $H_i/H_{i+1}$ to $H'_{r_i}/H'_{r_{i}+1}$ which is itself sent to $K'_{\psi(r_i)}/K'_{\psi(r_i)+1}$ etc. Hence, if we can put surrogates of $(b)$ in $(c)$ and those of $(e)$ in $(d)$ in one-to-one correspondence, we are done.—But.—As the author of the submitted answer has already admitted, it is possible, by using $\psi$ , to take two different elements of list $(a)$ to the same element of list $(d)$ . Well then, better say goodbye to our bijection showing $m=n$ . That $\psi$ only allows nontrivial to nontrivial is irrelevant to this particular issue: because it can do so—wait for it—trivially, e.g. by allowing us to send every nontrivial element of $(c)$ to one particular nontrivial element of $(d)$ (since one can identify different members of a series with each other). Incidentally, this shows why the ""identifications"" don't give us a very robust method of counting in this case. The question is whether the correspondence between the surrogates of the elements of $(b)$ in $(c)$ and those of the elements of $(e)$ in $(d)$ is, according to $\psi$ , unique. In the absence of uniqueness of the partial correspondence between $(c)$ and $(d)$ , how else can one use $\psi$ to connect $(b)$ and $(e)$ ? Merely asserting ""just use $\psi$ !"" won't do; you have to specify the manner in which one has to use $\psi$ exactly. This sums up the lack clarity present in Lang's proof and the post below. This is what I wanted cleared up with a standard set theoretic proof. We seem to have so many testimonies, but, strangely, very little actual evidence. I am sure that such a bijection exists (cumbersome or not), though a little more honesty would perhaps make us admit that it is not near at hand. Quite simply, the method  suggested  is inadequate. A last word: I do not approve of recycling the constructions involved in Schreier. You can invoke Schreier as an existential theorem. That should be enough.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'simple-groups']"
3503311,"Do non-compact spaces always contain ""big"" closed subsets?","I was wondering if the following is true or not: Let $X$ be a non-compact topological space. Does there exist a closed non-compact subset $C$ such that $C\neq X$ ? The result is obvious if, for example, $X$ has an isolated point $x$ because then $X\setminus\{x\}$ is closed. But what if that is not the case? Thank you.","['general-topology', 'compactness']"
3503320,A Measure Theoretic formulation of Bayes' Theorem,"I am trying to find a measure theoretic formulation of Bayes' theorem, when used in statistical inference, Bayes' theorem is usually defined as: $$p\left(\theta|x\right) = \frac{p\left(x|\theta\right) \cdot p\left(\theta\right)}{p\left(x\right)}$$ where: $p\left(\theta|x\right)$ : the posterior density of the parameter. $p\left(x|\theta\right)$ : the statistical model (or likelihood ). $p\left(\theta\right)$ : the prior density of the parameter. $p\left(x\right)$ : the evidence . Now how would we define Bayes' theorem in a measure theoretic way? So, I started by defining a probability space: $$\left(\Theta, \mathcal{F}_\Theta, \mathbb{P}_\Theta\right)$$ such that $\theta \in \Theta$ . I then defined another probability space: $$\left(X, \mathcal{F}_X, \mathbb{P}_X\right)$$ such that $x \in X$ . From here now on I don't know what to do, the joint probability space would be: $$\left(\Theta \times X, \mathcal{F}_\Theta \otimes \mathcal{F}_X, ?\right)$$ but I don't know what the measure should be. Bayes' theorem should be written as follow: $$? = \frac{? \cdot \mathbb{P}_\Theta}{\mathbb{P}_X}$$ where: $$\mathbb{P}_X = \int_{\theta \in \Theta} ? \space \mathrm{d}\mathbb{P}_\Theta$$ but as you can see I don't know the other measures and in which probability space they reside. I stumbled upon this thread but it was of little help and I don't know how was the following measure-theoretic generalization of Bayes' rule reached: $${P_{\Theta |y}}(A) = \int\limits_{x \in A} {\frac{{\mathrm d{P_{\Omega |x}}}}{{\mathrm d{P_\Omega }}}(y)\mathrm d{P_\Theta }}$$ I'm self-learning measure theoretic probability and lack guidance so excuse my ignorance.","['measure-theory', 'probability-theory', 'probability']"
3503353,Derive a general result for the $n$th derivative of $\ln(1+x)$,"I'm stuck on this problem: write down the four first derivatives of $f(x)= \ln(1+x)$ and hence derive a general expression for the nth derivative of $f$ . The first four derivatives  I found are respectively: $$\frac{1}{(1+x)},\; \frac{-1}{(1+x)^2},\;\frac{2}{(1+x)^3},\; \frac{-6}{(1+x)^4}. $$ I know I am missing something simple, but I can't see a pattern... any help would be great!","['calculus', 'functions', 'derivatives']"
3503375,Measure on the power set of the natural number,"The problem is the following. Let $X = \mathbb{N}$ , and $A = P(X)$ (A is the power set of the natural number). Fix some sequence $(a_n)_{n=1}^{\infty} \subset [0, \infty)$ such that $\sum^\infty_{n=1} a_n < \infty$ . Define $$\mu(A) = \sum_{n \in A} a_n$$ Show that $\mu$ is a measure. The condition I am having trouble with is the countable additivity. So I would have to show that for disjoint collection $(A_n)_{n = 1}^\infty \subset P(X)$ , we have $$\mu(\cup A_n) = \sum_{n \in \cup A_n} a_n = \sum_{i = 1}^\infty \mu(A_i) = \sum_{i = 1}^\infty  \sum_{n \in A_i} a_n$$ Intuitively, this seems obvious as $A_i$ forms a partition, so summing over the partition and then adding them again should yield the same result. But I don't know how to write this intuition out mathematically, and I also know that intuition can be wrong with things like infinite sums. So anyone could help me on writing the proof out, I would be really grateful. Thank you!",['measure-theory']
3503436,Inverting a function: proof of $W(x) = \ln\frac{x}{\ln\frac{x}{\ddots}}$,"for $|W(x)|>1$ , $W$ The Lambert W-function: $$ W(x) =\ln\cfrac{x}{\ln\cfrac{x}{\ln\cfrac{x}{\ddots}}}  $$","['lambert-w', 'numerical-methods', 'inverse-function', 'sequences-and-series']"
3503448,Differential Equations µ substitution,"My professor has removed the lecture notes which were our only resource (no textbook) for the class, so I do not have much to go on except for a class I took years ago. He never named it in class, but I remember calling the solution technique µ sub. I can't find any tutorials that I recognize as being this kind of problem or using the equation he provides. Solve: $v'=5-0.1v$ with $v(0)=44$ The hint provided is to use the equation: $y(x)=e^{-µ(x)}\int{e^{-µ(x)}q(x)dx}$",['ordinary-differential-equations']
3503450,Enumerating All Cases Where Two Pairs of Dates Overlap and Don't Overlap,"Given two pair of dates I would like to compute all the different ways the dates can overlap (see attached picture for some examples). To be slightly more rigorous, given two pair of dates $(t_1, t_2), (t_3,t_4)$ what are all the ways that the date pairs can overlap as well as not overlap? There are no constraints on the system so $(t_1)$ can be less than, equal to or greater $(t_3)$ or $(t_4)$ . Similarly for $(t_2)$ . I could enumerate all the cases for two pairs of dates but the situation gets complicated for more than two pairs of dates. What would be a mathematically rigorous approach to enumerate all the cases when there are two pairs of dates and how would that approach extend for cases where there are three or even four pairs of dates? I am looking for a rigorous approach to this problem so that I don't inadvertently miss a scenario. The analysis will be used to produce software for a critical process. UPDATE There is one constraint which I did not mention when posting first. This constraint is that for each pair of dates $(t_m, t_n)$ , $t_m \leq t_n$ .","['combinatorics', 'discrete-mathematics']"
3503454,"Equal in distribution, linear combination of random variables","I have a problem that I am working on, and I am trying to figure out if there is more information I can say about it. I have that $X, Y, Z$ are iid and that $\frac{X + Y+ Z}{\sqrt{3}} \overset{d}{=} X$ , and $E(X^2) = 1$ . I need to find the distribution of X. It seems clear that $$\frac{X + Y+ Z}{\sqrt{3}} \overset{d}{=} X$$ $$\Rightarrow X + Y+ Z \overset{d}{=} \sqrt{3}X$$ Hence, for expected value we have $$E(X) = \mu = E(Y) = E(Z)$$ $$E(X+Y+Z) = \sqrt{3} E(X)$$ $$3\mu= \sqrt{3} \mu \Rightarrow \mu = 0$$ And for the Variance of X we can see that $$Var(X) = E(X^2) - (E(X))^2$$ $$Var(X) = 1 - \mu^2 = 1$$ $$Var(X) = Var(Y) = Var(Z) = 1$$ $$Var(\frac{X+Y+Z}{\sqrt{3}}) = \frac{1+1+1}{3} = 1$$ Therefore $E(X) = 0,$ and $Var(X) = 1.$ It seems like $X$ could be a standard normal distribution, but clearly we do not know that for sure.  Is there anything else that I can say about it?","['probability-distributions', 'normal-distribution', 'probability']"
3503461,Is there a way to tell if a limit is positive infinity or negative infinity without graphing?,If I have such limit $\displaystyle\lim_{x \to -\infty} \frac{2x^2-4x}{x+1}$ to calculate. How can I know if the result if $-\infty$ or $\infty$ if I don't have a way to graph this function and I don't know how this graph looks like? Because the direct substitution will be like this: $\dfrac{2(-\infty)^2-4(-\infty)}{-\infty+1}$ I am always confused when calculating the limit when it is approaching $-\infty$ because it is not as easy as ones that approach $\infty$,"['differential', 'limits', 'calculus']"
