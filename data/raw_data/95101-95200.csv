question_id,title,body,tags
1304089,Differentiating both sides of an inequality with monotonic functions,"If $f(x)\le g(x)$ for all real $x$ for monotonic functions $f$ and $g$ (say, both increasing), does it follow that $f'(x)\le g'(x)$? (Note: I've seen several questions asking the same thing without the condition of monotonicity, but the counterexamples given always involve a non-monotonic function, and it seems to me that this condition might be sufficient; I haven't been able to come up with any counterexamples myself.) If not, is the stronger condition that $f^{(n)}(x)$ and $g^{(n)}(x)$ are monotone for either all natural $n$ or all $n\le N$ for some $N$ sufficient?","['derivatives', 'calculus', 'inequality']"
1304107,Characterization of 1-dimensional manifolds. [duplicate],"This question already has answers here : The only 1-manifolds are $\mathbb R$ and $S^1$ (2 answers) Closed 9 years ago . My intuition tells me that the only connected 1-dimensional topological manifolds are the real line $\mathbb{R}$ and the circle $S^1$. Is this true? If yes, is it possible to prove it from first principles, or is it something that needs some highly technical theorems? If no, do we have an example of connected 1-dimensional manifold not homeomorphic to $\mathbb{R}$ or $S^1$?","['manifolds', 'general-topology']"
1304113,"Is ""A Short Elementary Proof of the Unprovability of the Collatz Conjecture"" really as claimed? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 7 years ago . Improve this question The paper ""A Short Elementary Proof of the Unprovability of the Collatz Conjecture"", from a self-proclaimed peer reviewed journal (although the publisher, ScienceDomain International, appears on Beall's list ), purports to prove that the Collatz Conjecture is unprovable. If it's valid, why has it not received more attention? If it’s invalid, what is the flaw, and why has it not been pointed out since initial publication?","['collatz-conjecture', 'number-theory', 'soft-question']"
1304119,A complemented lattice satisfying de Morgan's laws is an ortholattice?,"Suppose you have a bounded, complemented lattice $\mathfrak{L} = \left<L, \vee, \wedge, \neg, 1, 0\right>$ that satisfies De Morgan's laws. I want to prove that this is an ortholattice. The first condition $a \leq b \implies \neg b \leq \neg a$ was straightforward, but I am struggling a lot with the second condition. That is, given $\neg (a \vee b) = \neg a \wedge \neg b$ and $\neg (a \wedge b) = \neg a \vee \neg b$ for all $a, b \in L$, I'd like to show that $$\neg\neg a = a$$ for all $a, b, \in L$. When I looked this up in a textbook, what I saw was:
$\neg \neg a = \neg (\neg a \vee \neg a) = \neg \neg a \wedge \neg \neg a = a \wedge a = a $ I can't, however, understand how one gets $\neg \neg a \wedge \neg \neg a = a \wedge a$ without assuming $\neg\neg a = a$. I also tried proving this in some other way but failed. I need one of two things: An alternative proof, or An explanation why $\neg \neg a \wedge \neg \neg a = a \wedge a$ doesn't assume $\neg\neg a = a$","['lattice-orders', 'elementary-set-theory']"
1304154,Finding extreme values of a variable on an intersection of a sphere and a plane,"Determine the minimum and maximum value of the variable $z$ defined by the curve given by:
\begin{cases} x^2+y^2+z^2=1 \\ x+2y+2z=0 \end{cases} So do I need to find a function $z=f(x,y)$ or just find, implicitly, the derivatives that satisfy $f'_x =0, f'_y=0$? I don't know if it is possible to explicitly parametrize the curve with $z=t$, because that is probably how I would usually solve this kind of problem. Any advice?","['optimization', 'multivariable-calculus']"
1304165,Name of Jordan Canonical Form in infinite dimensions?,"I tend to think of Jordan canonical form as the generalized spectrum theorem.  I read it as saying, every matrix cannot be diagonalized, but they can be ""jordanized"".  In functional, I've seen the spectral theorem again.  The situation there however is much more complex.  What does the Jordan canonical form go by in infinite dimensions, or does it really not hold there well?  Maybe just Hilbert Spaces would be easiest to consider so I can get a feeling for what is out there. Thanks!","['linear-algebra', 'functional-analysis']"
1304167,"Integral of $\frac{\sin^2(nx/2)}{\sin^2(x/2)}$ over $[-\pi,\pi]$.","I would like to show that $$\frac{1}{n\pi}\int_{-\pi}^\pi \frac{\sin^2(nx/2)}{2\sin^2(x/2)} dx = 1$$ My attempt is very similar to the accepted answer to this question. $$\int_{-\pi}^\pi \frac{\sin^2(nx/2)}{2\sin^2(x/2)} dx = \frac{1}{2} \int_{-\pi}^\pi {1-\cos(n x)\over 1-\cos(x)}dx$$ I have $\int_{-\pi}^{\pi}{1-\cos(n x)\over 1-\cos(x)}dx$, then 
$$\cos\bigl((n+1)x\bigr)+\cos\bigl((n-1)x\bigr)=2\cos x\, \cos(nx)\ ,$$
that gives
$$1-\cos\bigl((n+1)x\bigr)=2(1-\cos x)\cos(nx)+ 2\bigl(1-\cos(nx)\bigr)-\bigl(1-\cos((n-1)x\bigr)\ .$$
Except $$\int_{-\pi}^{\pi}\cos(nx)\ dx= \frac{1}{n} \sin(n\pi) \ne 0$$ does not go away. So I do not get the right conclusion in the recursion at the end. Where is my mistake?","['trigonometry', 'calculus', 'definite-integrals', 'integration']"
1304171,Does $\mu(x+B)= \mu(B)$ for all balls $B$ imply that $\mu$ equals the Lebesgue measure (up to scaling)?,"Suppose that $\mu$ is a measure on $(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$ such that $\mu(K)<\infty$ for any compact set $K$ and $$\mu(x+B) = \mu(B) \tag{1}$$ for all $x \in \mathbb{R}^d$ and open balls $B$ . Does this imply that $\mu$ equals up to a constant the Lebesgue measure $\lambda^d$ , i.e. does it hold that $\mu = c \lambda^d$ for some constant $c \in [0,\infty]$ ? My attempts: It is well-known that if $(1)$ holds for all ( $d$ -dimensional) rectangles, then the claim holds true. So if we knew that any rectangle can be covered by disjoint balls, then we would be done. However, as far as I know, this is actually not possible. (See this question ) Define $$\mathcal{D} := \{A \in \mathcal{B}(\mathbb{R}^d); \forall x: \mu(x+A) = \mu(A)\}$$ and show that $\mathcal{D}$ is a Dynkin system. If we knew this, then the fact that the balls are contained in $\mathcal{D}$ would imply $\mathcal{D} = \mathcal{B}(\mathbb{R}^d)$ . My problem: Since $\mu$ is (in general) not a finite measure, I don't see how to prove the implication $A \in \mathcal{D} \implies A^c \in \mathcal{D}$ . I also considered defining $$\mathcal{D}_R := \{A \in \mathcal{B}(B(0,R)); \forall x: \mu(x+A) = \mu(A)\};$$ then it is easy to show that $\mathcal{D}_R$ is a Dynkin system, but, unfortunately, we cannot conclude that the balls are contained in $\mathcal{D}_R$ (since the intersection $B \cap B(0,R)$ is in general not a ball). Any ideas, counterexamples,...? Edit: @NateEldredge suggested as a counterexample the measure $$\mu(A) := \sum_{q \in \mathbb{Q}} \delta_q(A)$$ in case that $\mu$ does not need to be finite on compact sets.","['lebesgue-measure', 'measure-theory']"
1304218,Proof that repeated sum equals binomial formula,"Let $s, d$ be positive integers. Can you prove the following general formula for the repeated sum? I developed this problem on my own, but is it a well known result?
$$\sum_{i_1 = 0}^s \sum_{i_2 = 0}^{i_1} \sum_{i_2 = 0}^{i_2} \cdots \sum_{i_{d} = 0}^{i_{d-1}} 1 = \binom{s + d}{d}$$
E.g. if $d = 1$ then the sum is
$$\sum_{i_1 = 0}^s 1 = s+1 = \binom{s+1}{d}.$$
If $d = 2$ then the sum is
$$\sum_{i_1 = 0}^s \sum_{i_2 = 0}^{i_1} 1 = \sum_{i_1 = 0}^s (i_1+1) = \frac{(s+1)(s+2)}{2} = \binom{s+2}{d}.$$","['sequences-and-series', 'binomial-coefficients', 'combinatorics']"
1304235,Set theory: cardinality of a subset of a finite set.,Suppose $A$ is a finite set of cardinality $n$. And Let $B$ be a subset of $A$ and the cardinality of $B$ equals $n$. Then $B=A$. Many texts use this fact very frequently but it seems that they just take it for granted. How can I prove this rigorously? Any help will be appreciated.,['elementary-set-theory']
1304244,"What is meant by ""The Lie derivative commutes with contraction""?","This was stated recently in a GR course I am taking, and I found it also stated on Wikipedia (second paragraph ). I simply don't know what is meant by this. For a vector $X$ and 1-form $\eta$, I would define the contraction as $$ \eta(X) $$ Whilst the Lie derivative of this quantity is $$ \mathcal{L}_Y(\eta(X)) = (\mathcal{L}_Y\eta)(X) + \eta(\mathcal{L}_Y X) $$ By the Leibniz rule. So I can't see what ""commutes"" could mean in this context.","['differential-geometry', 'lie-derivative']"
1304258,Extreme values of a two-variable polynomial,"Is it possible to find a two-variable polynomial which has only two extreme values on the whole plane, one is a local maximum, another is a local minimum, and the local maximum is less than the local minimum?","['calculus', 'real-analysis', 'functions', 'multivariable-calculus', 'derivatives']"
1304260,Prove that: $S_{XYZ}\geq \frac{1}{4}S_{ABC}$,"$\triangle ABC$. Let $X\in BC; Y\in CA, Z\in AB$ such that $\angle YXZ= \angle BAC, \angle XZY=\angle ACB, \angle ZYX=\angle CBA$. Prove that: $S_{XYZ}\geq \frac{1}{4}S_{ABC}$ P/s: I have proved that the length of circumscribed circles of $\triangle AYZ,\triangle BXZ,\triangle CXY,\triangle XYZ$ are the same",['geometry']
1304261,Show $x^p-t$ has no root in the field $\mathbb{F}_p(t)$,"I don't think I fully understand. Let's say there is a root $x_0 \in K=\mathbb{F}_p(t)$, where $p$ is a prime number. Then $x_0 = \frac{P(t)}{Q(t)}$ for some polynomials $P,Q \in \mathbb{F}_p[t]$. We can assume $\gcd(P,Q)=1$ and $x_0^p-t= \frac{(P(t))^p}{(Q(t))^p}-t= \frac{P(t)^p-tQ(t)^p}{Q(t)^p} =0$, so coefficients of $P(t)^p, tQ(t)^p$ must be identical, which contradicts $\gcd(P,Q)=1$, hence such $x_0$ does not exist and the polynomial has no root in $K$. am I about right? anyway, I would appreciate an explanation about $\mathbb{F}_p(t)$, what is $t$? what is the meaning of a variable which does not belong to any specific ""world""?
I cannot use $t$ as if it was a member of $\mathbb{F}_p$ and hence cannot assume $t^{p-1} = 1 \pmod p$...","['extension-field', 'abstract-algebra', 'field-theory', 'irreducible-polynomials']"
1304283,$\overline{\mathrm{Im} (T^*T)} = \overline{\mathrm{Im} T^*}$,"I need to prove that in a Hilbert space, $\overline{\mathrm{Im}(T^*T)} = \overline{\mathrm{Im}T^*}$.
I have already shown that $\ker (T^*) = (\mathrm{Im} T)^\perp$ and have so far concluded that $[\mathrm{Im}(T^*T)]^{\perp \perp} = [\mathrm{Im} T^*]^{\perp \perp}$ by proving $\ker (T^*T) =\ker (T)$. How do I obtain the final step? Can I just use the fact that for any subspace $A$, $A^{\perp \perp}= \overline{ \mathrm{Sp} (A)}$ but since the image is always a linear subspace, it is equal to its span?","['hilbert-spaces', 'operator-theory', 'linear-algebra', 'functional-analysis']"
1304293,What is an exact characterization for the functions $f$ such that $xf'(x) \leq 2f(x)$?,"What is an exact characterization for the functions $f$ such that $xf'(x) \leq 2f(x)$? I know, for instance, that the inequality holds for all functions $f(x) = c_0 + c_1x + c_2x^2$, with $c_0, c_1, c_2 \geq 0$. But it does not hold for $f(x) = x^2 - x$ or $f(x) = e^x$. I'm interested in the broadest possible class of functions that it holds for. Note 1: For my application, we can also assume that $f(x), f'(x), f''(x) \geq 0$ for all $x \geq 0$ (i.e., it is positive, non-decreasing, and convex over $[0, \infty)$). Note 2: I am interested in a slightly more general solution, $xf'(x) \leq bf(x)$, where $b \geq 1$ is a constant. But I should be able to generalize any solution for the $b = 2$ case.","['calculus', 'inequality', 'functions']"
1304296,Complex analysis integration with logs,"$$\int_C \operatorname{Log}\left(1-\frac 1 z \right)\,dz$$ where $C$ is the circle $|z|=2$ I don't even know how you would begin doing this. I understand $\operatorname{Log}(z)=\ln|z|+i\arg(z)$, but I don't think it helps in this case.","['contour-integration', 'logarithms', 'complex-analysis', 'integration']"
1304330,2D Brownian Motion -- Does this argument work?,"Consider a 2D Brownian Motion $(X_1(t),X_2(t))$ starting at $(x_1,x_2) \in \mathbb{R}^2$.
For every $s\geq0$, let $$\tau_s = \inf \left\{t \geq 0 \mid X_1(t) - x_1 > s \right\}\qquad Y_s = X_2(\tau_s)$$ The problem is to show that $Y=(Y_s)$ has the stationary, independence property. Here's what I've tried: Stationary: $Y_t - Y_s = X_2(\tau_t) - X_2(\tau_s) = X_2(\tau_t - \tau_s) = X_2(\tau_{t-s}) = Y_{t-s}$ This uses the fact that $\tau$ is stationary and the strong Markov property on $X_2$ Independence: Know $P(\tau_s < \infty) = 1$ for every $s$. By simple Markov property, I can reshift $X_2$ to a new process $\tilde{X}_2$ so that $\tilde{X}_2(0) = 0$. So by Strong Markov Property, $ E^{x_2}[Y_{\tau_s} \circ \theta_{\tau_s} | F_{\tau_s}] = E^{0}[Y_{\tau_s}] = 0$. But I get stuck at the independence part here since I'm not sure if I'm using the strong Markov property correctly. I want to essentially argue that because I know $X_2$ has the independent increment properties, that $Y$ should have it as well but I'm struggling to make it work.","['probability-theory', 'stochastic-processes', 'brownian-motion', 'stationary-processes', 'stopping-times']"
1304361,multinomial hypothesis testing,"Suppose we have data $(X_1, X_2, X_3)$ (I'll refer the categories as 1, 2, 3) that has a multinomial distribution with parameters $n$ and $(p_1, p_2, p_3)$ and we want to test the hypothesis that $p_1>p_2>p_3$.  I am trying to figure out an exact testing procedure for this. One idea I had would be to first condition $X_3$ on $X_1$ i.e. see how many times 3 was realized when the options were 3 and 2 only.  According to the hypothesis, this should be less than 1/2 and we could construct a simple binomial test for this since $X_3|X_1$ has a binomial distribution with parameters $n-X_1$ and $\frac{p_3}{p_2+p_3}$ and reject if the realization of 3 exceeded some critical value $k$. Secondly, we could perform a similar test for $X_2|X_3$ and if the hypothesis is true we should see 2 chosen less than half the time when the options are only 1 or 2.  A similar binomial test would work for testing this.  Overall, we could reject the original hypothesis if either of these tests reject.
However, controlling size seems difficult to me in this case since the tests are not independent (at least they obviously seem to not be). Is there a better approach to testing this hypothesis?  I have considered confidence intervals for the $p$'s, but not sure if I should use two-side or one-sided.  I have seen procedures for confidence intervals for $p_i-p_j$ for $i\neq j$, but these rely on asymptotic approximations. Anyway, any help or suggestions or references would be greatly appreciated.","['hypothesis-testing', 'probability', 'statistics']"
1304368,Proving that every vector space has a norm.,"I am trying to prove that every vector space $X$ has a norm. I have some silly questions, but it's better to ask them now instead of later. I think I'm having a bit of trouble getting intuition about basis in infinite dimensional spaces. Fix a Hamel basis ${\cal B} = ({\bf e}_i)_{i \in I}$. Then for all ${\bf x} \in X$, we write: $${\bf x} = \sum_{i \in F}a_i{\bf e}_i,$$ for some $F \subset I$ finite. I understand that this combination is unique in the sense that: $$\sum_{i \in F_1}a_i{\bf e}_i = \sum_{i \in F_2}b_i{\bf e}_i,$$ for some $F_1,F_2 \subset I$ finite, implies that $a_i = b_i$ for all $i \in F_1 \cap F_2$ and $a_i = 0$ for all $i \in F_1 \setminus F_2$, and $b_i = 0$ for all $i \in F_2 \setminus F_1$. Does this mean that $F_1 = F_2$? Assuming that yes, although I'm not sure, the idea would be to define some kind of max norm, which would be well-defined: $$\|{\bf x}\| = \max\{|a_i| \mid i \in F  \}.$$ This idea seems good, it even showed up in another answer . The properties $\|{\bf x}\| \geq 0$ for all ${\bf x}\in X$, $\|{\bf x}\| = 0 \implies {\bf x}=0$ and $\|\lambda{\bf x}\| = |\lambda|\|{\bf x}\|$ are all clear. I'm having trouble getting the triangle inequality. How to make a sum here is a bit confuse to me. If ${\bf x},{\bf y}\in X$, then there are $F_1,F_2 \subset I$ finite such that: $${\bf x} = \sum_{i \in F_1}a_i{\bf e}_i, \quad \text{and}\quad {\bf y}=\sum_{i\in F_2}b_i{\bf e}_i,$$ so that $${\bf x}+{\bf y} = \sum_{i \in F_1 \setminus F_2}x_i{\bf e}_i + \sum_{i\in F_1 \cap F_2}(a_i+b_i){\bf e}_i + \sum_{i \in F_2 \setminus F_1}b_i{\bf e}_i.$$ I wanted to write this as $\sum_{i \in \text{ something}}c_i{\bf e}_i,$ but the only thing I could think of was: $$\sum_{i \in F_1 \cup F_2}c_i{\bf e}_i, \quad c_i = \begin{cases} a_i, \text{ if }i \in F_1 \setminus F_2 \\ a_i+b_i, \text{ if }i \in F_1 \cap F_2 \\ b_i, \text{ if }i \in F_2 \setminus F_1\end{cases}$$ But again: Is this combination unique in the sense that the only possible combination for the vector ${\bf x}+{\bf y}$ will be indexed by $F_1\cup F_2$? I think I am overcomplicating things. I can get the triangle inequality with this, it seems, but things don't look well-defined enough for me. Can someone address these questions and give me a small explanation about it? Thanks. Edit: to confirm what I understood from gerw's answer: since the combination is unique, if I write
$${\bf x}=∑_{i∈F_1}a_i{\bf e}_i=∑_{i∈F_2}b_i{\bf e}_i,$$
for $F_1,F_2⊂I$, finite sets, then:
$\max\{|a_i|∣i∈F_1\}=\max\{|b_i|∣i∈F_2\}$,
and this ensures that $\|\cdot\|$ is well defined, right?","['vector-spaces', 'linear-algebra', 'functional-analysis']"
1304373,"If $A$ is a matrix, and $A^2=I$, then can I say that $|A|= \pm1$?","$A^2=I$ Take determinant on both sides:
$$|A^2|= |I| $$
$$|A|^2= 1$$
$$|A| = +1 \text{  or  } -1$$ Is this proof correct?","['determinant', 'matrices']"
1304398,Third-order differential equation with initial values using Euler method,"The problem I have is the initial value problem $$y''' = x + y$$ with 
$$ y(1) = 3, y'(1) = 2, y''(1) = 1$$ that should be solved with Eulers method using the step length, $h = \frac{1}{2}$. The iteration step for Eulers method is $y_{n+1} = y_n + hf(x_n, y_n)$. So I should need a system of equations of my initial values I have. 
I started with substituting: $$U_1 = y, U_2 = y', U_3 = y''$$ and then $$U_1' = U_2, U_2' = U_3, U_3' = x + U_1$$ 
And it's here I'm stuck, I don't understand how I should start iterate with the step-length from here, I have only encountered first-order problem with Eulers method so I would love if someone could point me in the right direction.","['numerical-methods', 'ordinary-differential-equations']"
1304402,What is the result of $\bigcap_{n=1}^{\infty}{(-1/n; 1/n)}$,"I would like to know the intersection of $(-1/n ; 1/n), \forall n \in N$. I am in trouble thinking it could be $\{0\}$ or $\emptyset$. Can anyone help me?","['elementary-set-theory', 'discrete-mathematics']"
1304520,$\sin x + c_1 = \cos x + c_2$,While working a physics problem I ran into a seemingly simple trig equation I couldn't solve. I'm curious if anyone knows a way to solve the equation: $\sin(x)+c_1 = \cos(x)+c_2$ (where $c_1$ and $c_2$ are constants) for $x$ without using Newton's method or some other form of approximation.,['trigonometry']
1304529,Intuition of upper & lower bound of sequence of subsets.,"I have come across these while studying the limsup & liminf of sequence of subset of a set. In order to understand that, I have to understand what least upper bound & greatest lower bound of a sequence of subset mean. I would be grateful if anyone helps me comprehend this concept intuitively as I am new & novice to this topic.",['elementary-set-theory']
1304539,What are higher derivatives?,"From Wikipedia : Higher derivatives can also be defined for functions of several variables, studied in multivariable calculus. In this case, instead of repeatedly applying the derivative, one repeatedly applies partial derivatives with respect to different variables. For example, the second order partial derivatives of a scalar function of n variables can be organized into an n by n matrix, the Hessian matrix. One of the subtle points is that the higher derivatives are not intrinsically defined, and depend on the choice of the coordinates in a complicated fashion (in particular, the Hessian matrix of a function is not a tensor). Nevertheless, higher derivatives have important applications to analysis of local extrema of a function at its critical points. For an advanced application of this analysis to topology of manifolds, see Morse theory. In multivariable calculus, I was told that higher derivatives were tensors and that was the reason we never went beyond Hessians (none of us had studied tensors before).  If higher derivatives aren't tensors, then what are they?  Where can I learn more about them?","['multivariable-calculus', 'tensors', 'derivatives']"
1304574,Example of a limit question requiring infinite applications of L'Hospital's rule to get a result,"I'm looking for a limit of the form $\lim_{x \to ?}\frac{f(x)}{g(x)}$ such that any arbitrary number of iterations of L'Hospital's rule results in an indeterminate form and the limit that could (most easily?) be calculated by taking an infinite number of iterations -- ie calculating $$\lim_{x \to ?} \left(\lim_{n \to \infty} \frac{f^n(x)}{g^n(x)}\right)$$ I've seen questions like When does L' Hopital's rule fail? -- but the examples from this question either don't have an 'infinite derivative' (because it just alternates) or else just obviously simplify (I'm not sure how to formally rule this case out, maybe by requiring the function $\frac{f^n(x)}{g^n(x)}$ to not 'have a hole' at the limit evaluation point for all $n$). Is this even possible? The direction of my own work on this problem has been looking for a trig function that alternates between two values when the limit is taken at infinity, and each successive derivative decreases the distance between the alternating values. After an infinite number of derivative operations, the distance between the alternating values would essentially be 0 and a limit at infinity would exist. I believe a ratio of two such functions could satisfy the requirements of my question.","['calculus', 'limits']"
1304587,How many possible shuffles can be won perfectly?,"It is known that the possible shuffles of a deck of cards is $52!$, or ~$80658175170943878571660636856403766975289505440883277824000000000000$ different combinations. I have become aware of a game known as Idiot's Delight. The game is played in this manner: A deck is shuffled, and held in your left hand, face down. You begin pulling cards from the back of the deck, flipping them face up on top of the deck. You continue this process. When you have at least 4 cards, begin looking at the last 4 cards. If the first and last cards of the last four cards are the same suit, you may remove the two cards between them. If the first and last cards of the last four cards are the same value, you may remove all four cards. If the last four cards do not satisfy these conditions, continue drawing from the back of the deck, until you run out of face-down cards. Look at this example [Face Down Cards][$A$ ♠][$2$ ♣][$3$ ♦][$4$ ♠] Here, the $2$♣ and $3$♦ can be removed, and then play continues. You can win a game by correctly ""matching"" all the cards, or removing all cards from the face down section and containing no cards face up. This forces the last play to have the first and fourth card match in number. Correct me if I'm wrong, but I believe that this game is possible to be won, albeit if difficult. Assuming it is possible to be won, how would I calculate how many potential shuffles out of $52!$ would be possible to be won, if the game is played perfectly, and how could I design a proof for this? Perfect play, for the sake of this question, encompasses not knowing the order of the deck, but making the decision to remove cards whenever there is a possibility of such. EDIT: Assume for the sake of this question that a shuffle is a completely random selection of one of $52!$ permutations.","['sequences-and-series', 'probability', 'card-games']"
1304589,An identity involving the Bessel function of the first kind $J_0$,"Today, computing $\int_{0}^{\pi/2}\sin^2(\sin^2 x)\,dx$, I found an interesting identity:
$$\sum_{k\geq 0}\frac{(-1)^k(4k)!}{(2k)!^3 4^k}=\cos(1)\cdot\sum_{k\geq 0}\frac{(-1)^k}{k!^2 4^k}.$$
How would you prove it?","['bessel-functions', 'sequences-and-series']"
1304594,Prove that $\sin (\theta) + \cos(\theta) \ge 1$,"Let $\theta$ be an arbitrary acute angle. Prove that  $\sin (\theta) + \cos(\theta) \ge 1$. $$\big(\sin (\theta) + \cos (\theta)\big)^2 = 1 + 2 \sin(\theta)\cos(\theta)\ge 0$$ so, \begin{align*}\big(\sin(\theta)) + \cos(\theta)\big)^2 &> 1\\ 
\big(\sin(\theta)+ \cos(\theta)\big)^2 &\ge 1\end{align*}","['inequality', 'trigonometry']"
1304597,"Existence of a minimizer for $\int_0^1|P(t)|\,{\rm d}t$.","Let $m > 0$ be a fixed integer. Show that among all the polynomials $P \in \Bbb C[X]$ with degree $\leq m$ and with $P(0)=1$, there is one that makes minimum the value $\int_0^1|P(t)|\,{\rm d}t$. My attempt: Let ${\cal P}^m$ be the set of all the polynomials in $\Bbb C[X]$ with $P(0)=1$. I am aware that $$\|P\| = \int_0^1 |P(t)|\,{\rm d}t$$ is continuous because it is a norm. Since ${\cal P}^m$ and $\Bbb R$ have finite dimension, and $T: {\cal P}^m \to \Bbb R$ given by $T(P) = P(0)$ is linear, $T$ is continuous, so: $$ \{ P \in {\cal P}^m \mid P(0) =  1 \} = T^{-1}(\{1\})$$ is a closed set. If I could show that it is bounded, we would have compactness (because of finite dimension) and I could conclude the result by Weierstrass' extremum theorem. But if you make $m=1$ and look at $T$ as a projection, it is clear that the set is not compact, so I don't think I am in the right way. Is there a way to save my work so far? If someone can give me hints it's also ok. Thanks.","['analysis', 'functional-analysis']"
1304633,Solution to $y'=y^2-4$,"I recognize this as a separable differential equation and receive the expression: $\frac{dy}{y^2-4}=dx$ The issue comes about when evaluating the left hand side integral: $\frac{dy}{y^2-4}$ I attempt to do this integral through partial fraction decomposition using the following logic: $\frac{1}{(y+2)(y-2)} = \frac{A}{y+2}+\frac{B}{y-2}$ Therefore,
$1=Ay-2A+By+2B$. Since the coefficients must be the same on both sides of the equation it follows that: $0=A+B$    and $1=-2A+2B$.
Hence, $A=-B$, $B=\frac14$, $A=-\frac14$. Thus the differential equation should be transformed into: $-\frac{1}{4} \frac{dy}{y+2} + \frac14 \frac{dy}{y-2} = x+C$ Solving this should yield: $-\frac14 \ln|y+2| + \frac14 \ln|y-2| = x+C$ which simplifies as: $\ln(y-2)-\ln(y+2)=4(x+c)$ $\ln[(y-2)/(y+2)]=4(x+c)$ $(y-2)/(y+2)=\exp(4(x+c))$ $y-2=y*\exp(4(x+c)+2\exp(4(x+c))$ $y-y\exp(4(x+c))=2+2\exp(4(x+c))$ $y(1-\exp(4(x+c)))=2(1+\exp(4(x+c)))$ $y= 2(1+\exp(4(x+c)))/(1-\exp(4(x+c)))$ However, when done in Mathematica/Wolfram Alpha the result is given as (proof in the attached image) $\frac14 \ln(2-y) -\frac14 \ln(2+y) = x + C$ and returns an answer of: $y= 2(1-\exp(4(x+c)))/(1+\exp(4(x+c)))$. Can anyone figure out where I have made an error? The only thing I can think of is something with evaluating the absolute values of the natural logarithms.","['calculus', 'differential']"
1304646,Prove that $\sin(a)$ + $\cos(a)\leq\sqrt{2}$ [duplicate],"This question already has answers here : Prove: $|a\sin x+b \cos x|\leq \sqrt{a^2+b^2}$ (6 answers) Closed 5 years ago . $$\begin{align*}
\sin (a) + \cos(a) &\leq \sqrt{2}\\
(\sin(a)+ \cos(a))^2 &\leq (\sqrt{2})^2\\
\sin^2(a) + 2\sin(a)\cos(a) + \cos^2(a) &\leq \text{2}
\end{align*}$$
Am I doing it right? I need help.","['inequality', 'trigonometry']"
1304659,Show that the empty set is independent of $A$ for any $A$,I am somewhat stumped as to how to approach this. The only thing I can remotely think of is $$P(A\cap \emptyset) = P(\emptyset)$$ but nothing else comes to mind. Suggestions?,"['probability-theory', 'probability']"
1304668,How to Show RV Related to Poisson Random Measure is a.s. Finite?,"I'm really new to this area of random measures, and I'm a bit confused on how to get started on this problem. Let $\mu$ be a measure on $\mathbb{R}$ with
  $\mu(\left\{0\right\}) = 0$ and $\int_\mathbb{R} |x| \wedge 1 \mu(dx)
 < \infty$. Define $\nu = \lambda \times \mu$. Let $M$ be a Poisson Random Measure with mean $\nu$. Let $Z_t =
 \int_{[0,t] \times \mathbb{R}} M(ds,dx)$. Prove that $Z_t < \infty$ a.s., i.e. $\mathbb{P}(Z_t<\infty)=1$. I'm not really sure how to approach this problem. I managed to show that $\nu$ must be $\sigma$-finite. I also know the following: $$P(Z_t < \infty) = lim_{\alpha \rightarrow 0} E[e^{\alpha Z_t}]\ \text{and}\ Z_t = lim_{m \rightarrow \infty} \int_{[0,t] \times (\frac{1}{m}, \infty]} M(ds,dx).$$ Normally the way I usually show something is finite a.s. is to show it has finite expectation a.s., but I don't see a way to make that work in this case.","['probability-theory', 'lebesgue-measure', 'levy-processes', 'measure-theory']"
1304672,Deriving the Quadratic Polynomials Defining the Twisted Cubic,"I've recently been reading about rational normal curves and how they may be represented and have come to the following question:  (for simplicity's sake, the problem is stated in terms of the RNC in $\mathbb{P}^3$, i.e., the twisted cubic) The twisted cubic is the image of the map
$$ v : \mathbb{P}^1 \to \mathbb{P}^3 $$
which sends $$[X_0 : X_1] \mapsto [X_0^3 : X_0^2 X_1 : X_0 X_1 ^2 : X_1 ^3]$$
In Harris' book on Algebraic Geometry it is stated that the twisted cubic is also the set of common zeros of the three homogeneous quadratic polynomials
$$ F_0 (Z) = Z_0 Z_2 - Z_1 ^2$$ 
$$F_1 (Z) = Z_0 Z_3 - Z_1 Z_2$$
$$ F_2 (Z) = Z_1 Z_3 - Z_2 ^2 $$ I have filled in the blanks as Harris (implicitly) suggests: that is, I have verified that any point on the twisted cubic is a point on each of the quadrics $F_i$ and that any common zero of the quadrics lies on the twisted cubic.  What continues to mystify me is the question: where do these quadratic polynomials come from? When they are given to me, I can certainly verify that they have the prescribed properties, but given the first description of the twisted cubic, how does one arrive at the quadrics defining it as a variety? Any guidance will be greatly appreciated! EDIT: I have found the quadrics simply by ""playing around:"" Denote by $C$ the twisted cubic in $\mathbb{P}^3$.  Let $P = [Z_0 : Z_1 : Z_2 : Z_3] \in C$ be any point.  Then we know that 
$$Z_0 = X_0^3, \ Z_1 = X_0^2 X_1, \ Z_2 = X_0 X_1^2, \ Z_3 = X_1 ^3 $$
Observing, for example, that 
$$(X_0 ^3)(X_1^3) - (X_0 ^2 X_1)(X_0 X_1^2) = 0 $$
Implies that 
$$ Z_0 Z_3 - Z_1 Z_2 = 0$$
and yields the quadratic polynomial $F_1 (Z)$.  The same sort of trick yields the other two polynomials $F_0 (Z)$ and $F_2 (Z)$. Now I've come to (perhaps) a more sophisticated set of questions: (1) How did we know to search for quadratic polynomials? (2) How did we know to search for three of them? I suppose we could proceed in a very ad hoc way: Well, it wasn't that bad to come up with the polynomial equations, and they happened to be quadratic (how fortuitous!)  Supposing I stopped after only finding two of the quadratics I could have manually checked ""are these enough to get all the points of the twisted cubic?""  If yes, great.  If no, search for more polynomials and repeat. Sure.  The above procedure would have worked just fine.  But what about when we try to generalize to the rational normal curve in $\mathbb{P}^n$?  Certainly we can (and should) be smarter about things...",['algebraic-geometry']
1304679,Proving that a polynomial of the form $(x-a_1)\cdots(x-a_n) + 1$ is irreducible over $\mathbb{Q}$,"I want to prove that for any set of distinct integers $a_1,\ldots,a_n$, the polynomial
$$h = (x-a_1)\cdots(x-a_n) + 1$$ is irreducible over the field $\mathbb{Q}$, except for the following special cases which are reducible: $$\left.\begin{cases}
a_1 = a\\
a_2 = a+2
\end{cases}\right\} \implies h = (x-a-1)^2$$ and
$$\left.\begin{cases}
a_1 = a\\
a_2 = a+1\\
a_3 = a+2\\
a_4 = a+3
\end{cases}\right\} \implies h = ((x-a-1)(x-a-2)-1)^2$$","['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
1304682,Categories for the working mathematician exercises III 1,"I'm currently reading Mac Lane's Categories for the working mathematician and I'm having some trouble with the two following exercises from part III. Find (from any given object) an universal arrow to the forgetful functor $\mathbf{Rng}\rightarrow\mathbf{Ab}$ that forgets multiplication (it is important to stress that $\mathbf{Rng}$ means rings with units) Prove the second isomorphism theorem for groups, that is $SN/N\simeq S/S\cap N$ for $S,N\subset G$, $N$ normal in $G$, using only universality. For the first one, it is just that I am not aware of the name of the mathematical construction: I can guess that the ring $R_{G}$ constructed from $G$ is a kind of ring with a copy of $\mathbb{Z}$ plus all products of elements of $G$, with equivalence relations $(a+b)c=ac+bc$ and $(na)b=a(nb)$ for integer $n$. (I know for example that if we forget addition instead of multiplication then we get the ring algebra $R[G]$) For the second one I don't know how to characterise $SN$ and $S\cap N$ in an element-free fashion. I did manage to prove the third isomorphism theorem.","['abstract-algebra', 'group-rings', 'group-isomorphism', 'category-theory']"
1304698,Existence of a non-abelian group of order $p^n$.,"Question: Let $p$ be any prime and $n \geq 3$. Show that there exists a non-abelian group of order $p^n$. Attempt: Take $n = 3$. Writing $\mathbb Z_p \times \mathbb Z_p = \{e, \alpha_1, \ldots, \alpha_{p^2 - 1}\}$ and considering $$\begin{align}\tau_{jk} : \mathbb Z_p \times \mathbb Z_p&\to \mathrm {Aut} (\mathbb Z_p)\,\\e &\mapsto id\\\alpha_j &\mapsto id \\\alpha_i &\mapsto \rho_k  \end{align}$$ $\forall i \neq j$ where $\rho_k : \mathbb Z_p \to \mathbb Z_p$ is such that $\rho_k (1) = k$, with $k \in \{2, \ldots, p-1\}$. Then we take the group $(\mathbb Z_p \times \mathbb Z_p) \ltimes_{\tau_{jk}} \mathbb Z_p$. Then $$(\alpha_i , 2) \ltimes_{\tau_{jk}} (\alpha_i, 1) = (2\alpha_i, 1  + \tau_{jk} (\alpha_i)(2)) = (2\alpha_i , 1  +2 k)$$ and $$(\alpha_i , 1) \ltimes_{\tau_{jk}} (\alpha_i, 2) = (2\alpha_i, 1  + \tau_{jk} (\alpha_i)(2)) = (2\alpha_i , 1  +k)$$ taking $k = 1$ for example we have that the group $(\mathbb Z_p \times \mathbb Z_p) \ltimes_{\tau_{jk}} \mathbb Z_p$ is not abelian. I was wondering if this is a good aproach. If not, is there an easier way to reach the result? If this is the ""best"" way then would the higher cases ($n > 3$) be more of a notation play game?","['abstract-algebra', 'group-theory', 'semidirect-product']"
1304708,Prove or disprove $\int_{-\infty}^\infty \frac{dx}{\cos x+\cosh x}=\frac{1512835691 \pi}{1983703776}$,"In this question, Evaluating the integral $\int_{-\infty}^\infty \frac {dx}{\cos x + \cosh x}$ , robjohn evaluates the integral to a nice summation with an approximate value. When plugged into W|A, it gives a possible closed form as $\dfrac{1512835691 \pi}{1983703776}$, correct to at least 20 decimal digits. When subtracting the two in W|A, it gives a nice result of $0$. ( 1 ) Can we prove that it equals the conjectured closed form?","['closed-form', 'improper-integrals', 'integration']"
1304731,Computing the $n^{\textrm{th}}$ permutation of bits.,"I've seen this post about the $n^{\textrm{th}}$ permutation of a set but that is not what I need. If you have a bit string (ones and zeros only) there are algorithms to quickly permute the NEXT lexicographically ordered bit permutation. For example take $$000111 \rightarrow 001011 \rightarrow 001101\rightarrow\cdots$$ etc. If the string is long then there are going to be approximately one Bajillion of these things. And I want to know how to compute the $n^{\textrm{th}}$ guy without doing the exhaust. Backgound: This is for a parallel computing job where I need to farm out the search space of elements of a set. Foreground: I own a copy of Knuth's 4th volume of ""The Art of Computer Programming"" and I think the answer is in there but I can't seem to find it. (It's like 900 pages). I'm posting here in the hopes that someone has knowledge of this (obviously). Even if you can point me to a source, say the part in Knuth's book where he describes this problem I would be most grateful.","['computer-science', 'combinatorics']"
1304737,What is a pullback in simple calculus context?,"The definition of a pullback provided by my text is quite accessible Let $\phi : M \to N$, $f:N \to \mathbb{R}$, then $f\circ \phi: M \to \mathbb{R}$, where $\phi^*f = f\circ\phi$ and $\phi^*$ is the pullback of
  $f$ by $\phi$ But when I look up some examples, things like category theory and sheafs and differential forms pop up! I only have a background in calculus, can someone provide a simple examples from calculus to illustrate the idea of a pullback? and why is pullback problematic/pathological/not desired??","['differential-geometry', 'calculus']"
1304742,Next book in learning Differential Geometry,"I have just finished the book ""Manfredo P. do Carmo - Differential Geometry of Curves and Surfaces"". My aim is to reach to graduate level  to do research, but articles are not only too advanced to study after Carmo's book, but also I don't think that they are readable by just studying Carmo's book at all for a self-learner like me. Please someone tell me a book for Differential Geometry more advanced than Carmo's book but readable esp. for self-learning . Thanks a lot.","['differential-geometry', 'reference-request']"
1304744,Probability of Independent Events individual vs in series,"I understand that independent events (such as a fair coin flip) should not be viewed in succession. For example, if you flip heads 10 times in a row, the odds of flipping the next coin heads is still 50%. However, there is another way of looking at the coin flips. What is the probability of flipping 11 heads in a row And I already know that to be 0.5^11 ~= 0.1% So if you were making a bet on coin flips and 10 heads came up, would you still base a bet on the 50% fact, or on the odds of getting heads 11 times in a row (.1%) fact? If you consider the 50% fact or 0.1% fact, please explain why?","['probability-theory', 'probability', 'statistics', 'gambling']"
1304746,A function with midpoint-linear derivative is a quadratic polynomial,"Suppose $f:\mathbb{R}\to \mathbb{R}$ is a differentiable function such that $$f'\left(\frac{a+b}{2}\right) = \frac{f'(a)+f'(b)}2,\quad \forall a,b\in\mathbb{R}$$ Prove that $f$ is a polynomial of degree at most $2$ . In other words: if the derivative of a differentiable function is midpoint-linear, then it is linear. (In general, being midpoint-linear, i.e., $g((a+b)/2)=(g(a)+g(b))/2$ , is weaker ). I'm looking for a solution without Baire category tools, e.g., on the level of Rudin's Principles of Mathematical Analysis . Motivation The problem isn't hard to solve if one uses the fact that $f'$ , being a Baire- $1$ function, has a point of continuity. Then the rest is elementary: here's a proof that a midpoint-linear function $g$ with a point of continuity is linear: Subtract a  linear function to arrange $g(0)=g(1)=0$ . Apply the midpoint-linear property to conclude that $g=0$ at all dyadic rationals. If $g$ is continuous at $a$ , then $g(a)=0$ by above. For every $\epsilon>0$ there is a neighborhood of $a$ in which $|g|<\epsilon$ . From midpoint-linearity, and $g$ being $0$ on a dense set, it follows that $|g|<\epsilon$ everywhere. Thus, $g\equiv 0$ . But invoking that fact about Baire- $1$ function looks like using a sledgehammer and makes me think I'm missing some approach here.","['real-analysis', 'derivatives']"
1304756,How do you convert different bases?,"I know how to convert any number into base 10 by using the below method. Write (6712)base 8 in base 10.
Ans: $6 \times 8^3 + 7 \times 8^2 + 1 \times 8^1 + 2 \times 8^0 = 3530_{10} $ However, I am not sure how to convert a number in base 10 or a different base into a number in a different base (other than 10). For example, write (101)base 2 in base 8. Is there a formula to solve such questions? Help would be appreciated. Thank you.","['number-theory', 'elementary-number-theory']"
1304792,Partial sums of falling factorials,"I want to know if there exists some way, approximate or exact, to do a partial sum of falling factorials of the kind: $$\sum_{k=i}^{n}(a+k)_{h}$$ where all are constants (here $(r)_s:=r(r-1)\cdots (r-s+1)$ represent a falling factorial). And I'm interested too in some partial sum like this $$\sum_{k=i}^{n}(a+k)_{h}r^k$$ In particular I want a closed form to this formula: $$\sum_{m=0}^{3}\left(\sum_{k=1}^{19-m}(19-k)_m\right )^{-1} \left(-(19)_m+\sum_{k=0}^{19-m}(19-k)_m q^k\right )$$ Possibly there is not a closed form but I don't know. I started to read about hypergeometric series but this topic is completely new to me so I don't have a clear way to approach to my question by now. I will appreciate any help. If you can show me via some link or bibliography is fine too. Thank you in advance. UPDATE Ok, I was reading the book of Graham that @ncmathsadist said to me and I have a partial answer. The question is close to some general topics on discrete maths (that I unfortunately forget). The point is that an analogue to $\int_{a}^{b}x^n dx=\frac{x^{n+1}}{n+1}\Big|_{a}^{b}\ $ on difference calculus is $$\sum\nolimits_{a}^{b}(k)_n\delta k =\frac{(k)_{n+1}}{n+1}\bigg|_{a}^{b}$$ For the second case I can use an analogue to integration by parts that is named summation by parts: $$\sum f(k)\Delta g(k) \delta k=f(k)g(k)-\sum \Delta f(k) g(k+1)\delta k$$ But I dont get any closed form, so I assumed these formulas haven't closed forms.","['factorial', 'summation', 'closed-form', 'combinatorics']"
1304819,How to use Bayes's rule with mixed distributions?,"On page 81 of The Likelihood Principle by Berger and Wolpert (1988) I find the following claim (which references example 20 on page 75). We consider a certain statistical problem from a Bayesian perspective. Suppose we have an infinite sequence of i.i.d. random variables with distribution $N(\theta,1)$. Consider the stopping rule that stops when $n$ is such that $|\overline{X_n}\ge Kn^{-1/2}|$, where $K$ is some fixed constant and $\overline X_n$ is the sample mean. It can be shown this stopping time is finite with probability 1. Then the likelihood function is proportional to a $N(\overline{X_n}, n^{-1/2})$ density. We also assume we have a prior that places half its mass at $0$, and half its mass on a normal distribution centered at zero with very large variance (so the distribution is mixed). Let us agree the wave our hands and say the normal distribution with very large variance can be approximated with an improper uniform prior over the real line. Recall that the likelihood principle says the stopping rule is irrelevant when making our Bayes's rule calculations. Treating $n$ as fixed in advance, Berger and Wolpert make the following claim about the posterior: $$\pi(0\,|\, \overline{X_n}=Kn^{-1/2})=[1+(1+n^2)^{-1/2}\exp[(K^2n)/(2(1+n))]]^{-1}.$$ I cannot figure out how this was derived. In particular, I am clueless how Bayes's rule should be applied in the case of a mixed prior and continuous likelihood. Any help would be greatly appreciated.","['probability-theory', 'bayesian', 'statistics', 'bayes-theorem']"
1304835,An invertible sparse matrix?,"I'm not entirely certain about how to tackle this problem.... I hope you ladies and gents can help :) If $M\in M_{n\times n}(\mathbb{R})$ be such that every row has precisely tow non-zero entries, one is precisely equal to $1$ and the other is found in the diagonal and is strictly greater than one.  Must $M$ be invertible? My thoughts to date :) I believe the answer to be yes; reasoning: Intuition : for $n\leq 2$ $M$ can be readily calculated directly. Proof sketch idea: For arbitrary large $n$, I was thinking using the mini-max theorem to obtain a lower-bound on the smallest eigenvalue; and since all the non-zero entries are sufficiently large (at least 1); I would be done since then all eigenvalues must be strictly positive....
(But is the matrix Hermitian and how can I calculate this explicitly?)","['eigenvalues-eigenvectors', 'operator-theory', 'linear-algebra', 'matrices']"
1304844,A question on Characteristic function on Cantor like set,"Consider a Cantor like set $C$ with measure $1>\epsilon>0$ on the interval $[0,1]$. Is it possible to find a measurable set $F \subset [0,1]$ with $m(F)=1$ such that $\displaystyle \chi$$_c|_F$ is continuous. Here  $\displaystyle \chi$$_c$ is the characteristic function on the Cantor like set and $\displaystyle \chi$$_c|_F$ is the restriction of the function to $F$ . I have a feeling that it is not possible but I am not sure how to approach this. If somebody could help it would be great. Thanks.","['lebesgue-measure', 'cantor-set', 'measure-theory']"
1304848,Proving a corollary of a corollary of the Mean Value Theorem (corollary-ception),"This is will a wordy question but here it goes: My analysis book states the mean-value theorem and then a corollary which we will label as (1): Let $f$ be a differentiable function on $(a,b)$ such that $f'(x) = 0$
for all $x \in (a,b)$. Then $f$ is a constant function on $(a,b)$. My book then goes on to state an additional corollary which we will label as (2): Let $f$ and $g$ be differentiable functions on $(a,b)$ such that
$f'=g'$ on $(a,b)$. Then there exists a constant $c$ such that $f(x)
   = g(x) + c$ for all $x \in (a,b)$. This corollary also makes sense. I interpreted it as two functions $f$ and $g$ with parallel tangent lines at every point so that one function is an exact copy of the other except it is at a different height. However, in the proof of (2), the book simply states ""Apply [1] to the function $f-g$. I don't see how (1) completely proves (2) because (1) only deals with functions with $f'(x) = 0$ and (2) deals with functions with f' = g' on a whole interval. Can anyone explain this? Thank you!","['continuity', 'real-analysis', 'derivatives']"
1304855,Prove this limit $\lim \limits_{x\to\infty}f(x)=0$,"I have this problem in real analysis. I think it needs integral factor or knowledge of ODE to prove, but not sure how to it. Here is the question: Let $f$ be a real valued continuous function on $[0,\infty]$ such that
  $$
\lim \limits_{x\to\infty}\left(f(x)+\int_{0}^{x}f(t)dt\right)
$$
  exists. Prove that
  $$
\lim \limits_{x\to\infty}f(x)=0
$$","['calculus', 'real-analysis', 'limits', 'improper-integrals']"
1304871,The inverse of a matrix in which the sum of each row is $1$,Let $A$ be an invertible $10\times 10$ matrix with real entries such that the sum of each row is $1$. Then choose the correct option. The sum of the entries of each row of the inverse of $A$ is $1$. The sum of the entries of each column of the inverse of $ A$ is $1$. The trace of the inverse of $A$ is non-zero. None of the above. If the matrix is given we can find its inverse but how can we find its inverse if the matrix itself not given? Any idea on how to find the answer?,"['linear-algebra', 'matrices']"
1304886,The derivative of $x!$ and its continuity,"is the factorial of fractions and negative numbers defined? If yes, then what is its graph? Also please find its domain. Our teacher said the factorial of a fraction is the fraction itself. He also said the graph is continuous but could not determine the derivative of $x!$.","['factorial', 'continuity', 'gamma-function', 'derivatives']"
1304910,clarification of a doubt over a defined result in ODE,"I was going through the topic of Wronskian in ODE came up with the following result: I have a little doubt. Can we say the same if we interchange $y_1$  and $y_2$ i.e. between consecutive zeroes of $y_2$ there is exactly one zero of $y_1$. Or, between successive zeroes of $y_1~(or~y_2)$ there is exactly one zero of $y_2~(or~y_1$)
Kindly help.",['ordinary-differential-equations']
1304916,Why does $\frac{49}{64}\cos^2 \theta + \cos^2 \theta$ equal $\frac{113}{64}\cos^2 \theta $?,I have an example: $$ \frac{49}{64}\cos^2 \theta + \cos^2 \theta = 1 $$ Then what happens next: $$ \frac{113}{64}\cos^2 \theta = 1 $$ Where has the other cosine disappeared to? What operation happened here? Any hints please.,"['arithmetic', 'fractions', 'algebra-precalculus', 'trigonometry']"
1304924,Non Existence of a proper holomorphic map from the punctured unit disc to an Annulus,Show that there is no proper holomorphic map from the punctured unit disc to an annulus  $A_r=\{z \in \mathbb C:1 <|z| < r \}$. Def :A map $f: X \to Y$ is called proper if $f^{-1}(K)$ is compact for every compact set $K$ in Y. please give some hints/ideas to prove this.Can someone please give a reference for reading about construction of proper maps between different domains in $ \mathbb C$ ?,['complex-analysis']
1304934,Difficult inverse tangent identity,"Prove that: $$\arctan\left(\frac{\sqrt{1 + x} - \sqrt{1-x}}{\sqrt{1 + x} + \sqrt{1-x}} \right) = \frac{\pi}{4} - \frac{1}{2}\arccos(x), -\frac{1}{\sqrt{2}} \le x \le 1$$ I'd multiply the inside of $\arctan$ by the conjugate of the denominator. I get: $$\arctan\left(\frac{1 - 1\sqrt{1 - x^2}}{x} \right)$$ But that is still very difficult. Any HINTS, no solutions?","['contest-math', 'algebra-precalculus', 'combinatorics', 'trigonometry']"
1304936,"Prob. 7, Sec. 3.8, in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: The dual space of a Hilbert space is a Hilbert space.","Here's Prob. 7, Sec. 3.8 in Introductory Functional Analysis With Applications by Erwine Kreyszig: Show that the dual space $H^\prime$ of a Hilbert space $H$ is a Hilbert space with inner product $\langle \ \cdot \ , \ \cdot \ \rangle_1$ defined by 
  $$\langle f_z, f_v \rangle_1 \ = \ \overline{\langle z, v \rangle} \ = \ \langle v, z \rangle,$$
  where $f_z(x) = \langle x, z \rangle$ for all $x \in X$. Now I know that each bounded linear functional $f \in H^\prime$ can be written as $f = f_z$, for a unique $z \in H$ with $\Vert z \Vert = \Vert f \Vert$. The sum of two bounded linear functionals on any normed space is again a bounded linear functional, and so is the scalar multiple of any bounded linear functional. Moreover, for each $z \in H$ and for each $w \in H$, we have 
$$\left( f_z + f_w \right) (x) = f_z(x) + f_w(x) = \langle x, z \rangle + \langle x, w \rangle = \langle x, z+w \rangle = f_{z+w}(x)$$ 
for all $x \in H$. So $f_{z+w} = f_z + f_w$. For $z \in H$ and for any scalar $\alpha$, we have 
$$f_{\alpha z} (x) = \langle x, \alpha z \rangle = \overline{\alpha} \langle x, z \rangle = \overline{\alpha} f_z (x) \ \mbox{ for all } \ x \in H.$$
So, $f_{\alpha z} = \overline{\alpha } f_z$. Thus the mapping $z \mapsto f_z$ of $H$ into $H^\prime$ is surjective, isometric, and conjugate linear. Moreover, the set of all bounded linear functionals on any normed space is itself a normed space, rather a Banach space. But how to show that the inner product given by Kreyszig is the (only) natural one (i.e. that which fits into what we already know from the normed space theory)?","['inner-products', 'real-analysis', 'functional-analysis', 'hilbert-spaces', 'analysis']"
1304952,Derivation of the Exponential Nature of $e^x$,"Presumably, the transcendental number $e$ was first found by taking the power series solution to the (arguably most fundamental) differential equation $f'(x)=f(x)$, with the initial condition $f(0)=1$ and then plugging in $x=1$. My question is, is there any way, other than using the equivalent of the multinomial theorem for power series to demonstrate that $f(nx)=f^n(x)$ for all $n$ and $x$ (i.e., that $f(x)$ is an exponential function) other than this laborious and unaesthetic method? Intuitively, I would expect the solution to be an exponential function but is there a better, somewhat rigorous demonstration of this simple fact?","['elementary-functions', 'ordinary-differential-equations', 'exponential-function']"
1305001,Weak convergence of sequence of measures iff every subsequence in sequence of distribution functions contains an a.e. convergent subsequence,"I'm trying to prove the first part of Proposition 8.1.8 in V.I.Bogachev , Measure Theory 2 : A sequence of signed measures $\mu_n$ on the interval $[a,b]$ converges weakly to a measure $\mu$ precisely when $\sup_n\lvert|\mu_n|\rvert<\infty$ and every subsequence in the sequence of the distribution functions $F_{\mu_n}$ of the measures $\mu_n$ contains a further subsequence convergent to $F_\mu$ at all points, with the exception of points of an at most countable set. I'm struggling with the direction $\Leftarrow$ and already asked a question about this here , but I'm not sure if I did mention everything needed so let me cite the first few lines of the proof: Suppose that the measures $\mu_n$ are uniformly bounded and satisfy the indicated condition with subsequences, but do not converge weakly to $\mu$. Since every continuous function $f$ can be uniformly approximated by smooth functions, we obtain, taking into account the boundedness of $\lvert|\mu_n|\rvert$, that there exists a smooth function $f$ such that the integrals of $f$ against the measures $\mu_n$ do not converge to the integral of $f$ against $\mu$. Passing to a subsequence, we may assume that the difference between the indicated integrals remains greater than some $\delta>0$. Passing to a subsequence once again we can assume that $\lim_{n\to\infty}F_{\mu_n}=F_\mu$ everywhere, with the exception of finiteley or countably many points. No problems for me so far. Please notice $F_{\mu_n}(t):=\mu_n([a,t))$. The functions $F_\mu$ and $F_{\mu_n}$ are constant on $(b,+\infty)$, hence $\mu([a,b])=\lim_{n\to\infty}\mu_n([a,b])$. Then the integration by parts formula yields that the right-hand side of the equality
  $$\int_a^bf(t)\mu_n(dt)=f(b)F_{\mu_n}(b+)-\int_a^bf'(t)F_{\mu_n}(t)dt$$ converges to
  $$ f(b)F_{\mu}(b+)-\int_a^bf'(t)F_{\mu}(t)dt=\int_a^bf(t)\mu(dt)$$
  which leads to a contradiction. My problem here is I don't see why the last integral in the first line converges to the first integral in the second line. I don't see any possibility to apply monotone or dominated convergence, so I have no idea what to do. Any help would be highly appreciated, I'm really stuck here.
Thank you very much in advance. Edit: If this is trivial, please just give me a short hint. This is driving me crazy!","['measure-theory', 'integration']"
1305002,Solve for $x$: $x =\ln(x)^4$,"I plotted the functions on both sides and it shows the equations has at least three solutions. Is there some non-interative (not sure if i used this term correctly - i mean the way you would solve, for instance, $x = x^2$) method to solve this equation?","['algebra-precalculus', 'logarithms']"
1305004,"What does ""ordering of sets by inclusion"" mean?","I was recently introduced to partial order relations. Although I understand the concept of a relation, I do not understand how subsets can have a sequence, or what that has to do with the phrase ""ordering of sets by inclusion."" Can anyone help me clarify the concept with an intuitive example?",['elementary-set-theory']
1305034,Linear combination of matrices over finite and infinite fields,"Let $F\subset K$ be the fields. Let $A_1,\ldots, A_m$ be the $n\times n$ matrices over the field $F$, and  $c_1,\ldots,c_m\in K$  such that $c_1A_1+\cdots+c_mA_m$ is invertible. How to prove that for infinite $F$ there exists $f_1,\ldots,f_m\in F$ such that $f_1A_1+\cdots+f_mA_m$ is invertible and for finite $F$ it does not hold?","['extension-field', 'field-theory', 'vector-spaces', 'matrices', 'linear-algebra']"
1305035,What exactly is antieigenvalue analysis?,"I found a book in the library about antieigenvalue analysis and it is possibly the most unreadable piece of literature I have ever made an effort to understand. Unfortunately, every other resource I try inevitably takes you back to the same author. I should apologize for a lack of greater research effort, but beyond the line on the wikipedia page , The antieigenvectors $x$ are the vectors most turned by a matrix or operator $A$ I can't make heads or tails of anything else. Could someone explain why (or why not) this topic is useful or interesting? I've previously read about topics such as fractional calculus, harmonic analysis, non-standard analysis, product integration, quantum probability, higher order fourier analysis and other topics by dusting off a rarely read book off a shelf. I've always found something cool or interesting. I enjoy linear algebra quite a lot. The name antieigenvalue is very enticing to me. I really want to think that this topic is going to be neat. What is in antieigenvalue analysis that should excite me?",['linear-algebra']
1305047,Can a function which is periodically undefined have a limit as x goes to infinity?,"I'm currently preparing for a calculus test. I was trying to solve the exercises of the test of last year, and one of the questions was: Give a full limit research of this function: $$f:(e,\infty) \setminus \{e^{e^{k\pi}}\mid k \in \mathbb N \} \to \mathbb R $$
  $$x \mapsto \frac{\cot(\ln(\ln(x)))}{\sqrt{x-e}}$$ Now, I was wondering if it is even possible that this function has a limit as $x$ approaches $+\infty$. If I insert the function in Maple, I get $0$ as the limit for $x \to +\infty$. But I'm not really sure if that's correct since the function is periodically not defined. So my question is: Can a function that is periodically not defined have a limit for $x$ approaching infinity? (The question wasn't formulated in English, so I'm sorry if I mistranslated it)","['calculus', 'limits', 'functions']"
1305070,$L^2$ convergence of this sequence,"I am given the following sequence of functions $(f_m)_{m \in \mathbb{N}}$. They are defined by
$$ f_m(x):=\left( \frac{e^{-ix}-1}{-ix} \right)^m \left( \sum_{l \in \mathbb{Z}} \frac{\left|e^{-ix}-1 \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-\frac{1}{2}}$$ for $x \neq 2 \pi l$ with $l \in \mathbb{Z}$ and $f_m(2 \pi l)=1$. Now, I want to show that this sequence converges to the characteristic function $\chi_{[-\pi,\pi]}$ in the $L^2$ sense? So $\|f_m- \chi_{[-\pi,\pi]} \|_2 \rightarrow 0.$ The idea could be to use the dominated convergence theorem. For this we need to find a uniform upper bound and be sure that pointwise convergence is satisfied. Regarding the upper bound it may be useful to see that $$|f_m(x)|^2 = \left( \sum_{l \in \mathbb{Z}} \frac{\left|x \right|^{2m}}{|x+ 2 \pi l |^{2m}} \right)^{-1}.$$","['analysis', 'calculus', 'real-analysis', 'lebesgue-integral']"
1305135,How many orthogonal matrices are there,"this might sound like a stupid question, but what I mean is: You need $n \times n$ elements to define a square matrix $\in R^{n \times n}$. How many element do I need to define an orthogonal matrix? I have the feeling that it should be $n$ or $2n$ but cannot find a clean mathematical formulation for that. Thanks!","['orthogonality', 'orthonormal', 'matrices']"
1305136,"Definitions of $\mathrm{Hom}(V,W)$","I have the definition of a homomorphism as map such that $\varphi(g_1g_2)=\varphi(g_1)\varphi(g_2)$ I have the definition of $\mathrm{Hom}(V,W)$ as $$\begin{align}\mathrm{Hom}(V,W) &= \{\mathbb{C}-\text{linear maps }\varphi:V \rightarrow W \} \\
&\cong \{n \times m \text{ matrices } \}
\end{align}$$ Does $\mathrm{Hom}$ stand for homomorphisms because I cannot see how it relates to the definition of homomorphism I have the definition of $\mathrm{Hom}_G$ as $$\begin{align}\mathrm{Hom}_G(V,W) &= \{\varphi \in \mathrm{Hom}(V,W) \mid g\varphi(v)=\varphi(gv), \forall g \in G, \forall v \in V\} \\
&=\{\varphi \in \mathrm{Hom}(V,W) \mid \rho(g)\cdot\varphi(v)=\varphi(\rho(g)v) , \forall g \in G, \forall v \in V\}
\end{align}$$ I then have the question: Prove that if $\rho$ is an irreducible representation, then for an element $g \in G$ $$g \in Z(G) \iff \rho(g)=\lambda I$$ In the solution I have that: $(``\implies"")$ If $g \in Z(G)$ then $gh=hg \ \forall h \in G$. By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. But \rho is irreducible so $\mathrm{Hom}_G(V,V)$ consists of scalar matrices by Schur's Lemma. I do not understand how ""By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. "" How is this the definiton of $\mathrm{Hom}_G$? I cannot see why this is equivalent.","['representation-theory', 'characters', 'group-theory', 'finite-groups', 'linear-algebra']"
1305138,"Do $J$, the all-ones matrix of even order, always have eigenvectors consisting of entries $-1, 1$ only?","Do $J$ , the all-ones matrix of even order, always have eigenvectors consisting of entries $-1, 1$ only? It seems so, vector having all its entries $1$ is one eigenvector for larest eigenvalue $n$ and any vector having half its entries as $1$ and half as $-1$ is eigenvector for ev $0$ . For $n=4k$ we have Hadamard matrices there, so those vector will work. But I don't know how to show existence of such $n$ linearly independent vectors.","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1305143,What is the value of $a^4+b^4+c^4$?,"Consider $a,b,c$ such that $a+b+c =1, a^2+b^2+c^2=2$ and $a^3+b^3+c^3=3$. Find the value of $a^4+b^4+c^4$, if possible. Trial: I observe that 
\begin{align}
a^4+b^4+c^4 &=(a^2+b^2+c^2)^2-2(a^2b^2+b^2c^2+c^2a^2)\\&=2^2-2[(ab+bc+ca)^2-2abc(a+b+c)]\\&=4-2[(ab+bc+ca)^2-2abc]
\end{align} 
Then 
\begin{align}
(a+b+c)^2 &=a^2+b^2+c^2+2(ab+bc+ca)\\\implies 1^2= 2+ 2(ab+bc+ca)\\\implies ab +bc+ca &=-\dfrac12
\end{align}
Then I am stuck. Please help.","['algebra-precalculus', 'problem-solving']"
1305151,Proving that $\cos(2\pi/n)$ is algebraic,I want to prove this without using any of the properties about the field of algebraic numbers (specifically that it is one). Essentially I just want to find a polynomial for which $\cos\frac{2\pi}{n}$ is a root. I know roots of unity and De Moivre's theorem is clearly going to be important here but I just can't see how to actually construct the polynomial from these facts.,"['abstract-algebra', 'linear-algebra', 'algebraic-number-theory']"
1305165,Escaping a stampede in Buffalo Country,"A person is standing at a random point in square ABCD (vertices labelled clockwise) of side length 10 units. The person is capable of instantly reaching and running at 1 unit/second. A herd of buffaloes forming a straight front 10 units wide is charging in a straight line through the square ABCD, at uniform speed 2 units/second. They enter through AB exactly and exit through CD exactly. Assume the person becomes aware of the stampede when the buffaloes first enter the square, and that the person takes the path which gives them the best chance of running out of the way of the stampede. What is the chance the person escapes the stampede? Will the best escape path always be straight in this case? What is the general strategy for solving these types of pursuit/escape problems?","['geometry', 'ordinary-differential-equations']"
1305203,Clustering elements according to covariance matrix,"I'm doing a little bit of topic modelling (which is not really my area) with twitter tweets.
The situation is the following: I have a (sort of) covariance matrix where the entrie $C_{ij}$ corresponds to the frequency of the words $i$ and $j$ occuring together in a tweet. Given this Matrix $C$ I would like to automatically cluster words into different topics. However, since my background isn't statistics nor data analysis, I think I might need the correct terms to search for. I'm not sure if k-means or PCA is what I need. In the optimal case, I end up with a not prior specified number of topics, that gathers that combines only the words that really correlated. Especially, I don't want all words to be assigned, given that some words only correlate very little.","['data-analysis', 'data-mining', 'reference-request', 'statistics']"
1305228,Proving using AM-GM inequality,"If $x,y\in \mathbb{R}$, and $x>y$, how to show
$(x-y)^{((x-y)/(2x-y))}\times (x+y)^{((x)/(2x-y))}>x$? I know I have to use AM-GM inequality, but it is not clear how.","['analysis', 'real-analysis', 'inequality']"
1305235,Convexity of the complex ellipsoid,"Let $p_1,\dotsc,p_n$ be positive integers.
  Define the complex ellipsoid $$\Omega(p_1,\dotsc,p_n)=\left\{(z_1,\dotsc,z_n)\in\Bbb C^n:\sum\limits_{i=1}^n{\left|z_i\right|^{2p_i}}<1\right\}.$$ I want to prove that the set $\Omega(p_1,\dotsc,p_n)$ is convex . I have already proved that $\Omega(p_1,\dotsc,p_n)$ is convex for $p_1=p_2=\dotsb=p_n$ using the Minkwoski's inequality. But I don't find any way to prove the convexity for distinct $p_i$'s. Help is appreciated.","['several-complex-variables', 'convex-analysis', 'complex-analysis']"
1305243,Difficult to understand difference between the estimates on E(X) and V(X) and the estimates on variance and std.dev. on lambda-hat,"I'm having a very hard time to separate estimates on population values versus estimates on sample values. I'm struggling with this exercise (not homework, self-study for my exam in introductionary statistic course): a) Calculate the maximum likelihood estimate on $\lambda$ and name this $\hat{\lambda}$ I did this. First I calculated the maximum likelihood estimator for $\lambda$, which is $\hat{\lambda}=\frac1n \sum_{i=0}^n k_i$ And the maximum likelihood estimate was $\hat{\lambda_e}=\frac{15}6=2.5$ Hopefully this is right. But now I'm asked to calculate the ""estimates on $E(X)$ and $E(Y)$ and afterwards I'm asked to calculate the estimate on both the variance and standarddeviation on the estimate, $\hat{\lambda}$? I'm really struggling to separate the two from each other, so any hint or guidance would be much appreciated.","['estimation', 'poisson-distribution', 'statistics', 'parameter-estimation']"
1305245,Questions on Erdős–Ginzburg–Ziv theorem for primes and understanding related lemmas and their applications.,"While trying to prove the prime case of Erdős–Ginzburg–Ziv theorem : Theorem: For every prime number $p$, in any set of $2p-1$ integers, the sum $p$ of them divisible by $p$. I came across with the following lemma: Lemma 1: If $p$ is a prime number and $A,B$
   are subsets of $\mathbb{Z}_p$ with $\varnothing\neq A\neq \mathbb{Z}_p
 $ and $\left |B\right | =2$, then $\left |A+B \right | \geq\left
 |A\right |+1  $, where $ A+B=\{a+b: a \in A, b \in B \} $. Proof: Without loss of generality $B=\{ 0,\beta\}$, where $\beta \neq 0$ and then $A\subseteq A+B$. Let us assume by contradiction that $A+B=A$. Let $a\in A$. Therefore $a+\beta\in A+B$, and thus $a+\beta\in A$. By this manner we can deduce that $a+k\beta\in A$ for every $k$ and from here that $$\mathbb{Z}_p=\{a,a+\beta, a+2\beta,\dots , a+(p-1)\beta\}\subseteq A $$ But we assume $A\neq \mathbb{Z}_p$.■ Question 1: How we deduce that  $\left |A+B \right | \geq\left |A\right |+1$ by such contradiction? Here I decided to prove this lemma by my own, and I came up with some more general result: Lemma 2: If $p$ is a prime number and $A,B$ are subsets of
   $\mathbb{Z}_p$ with $\varnothing\neq A,B$, then $\left |A+B \right |
 \geq\left |A\right |+\left |B\right | -1 $ Proof: Since the set of integer numbers is ordered set, we can translate $A$ and $B$ to new sets $A'$ and $B'$, so that $\sup(A')=0$ and $\inf(B')=0$ and still preserve the cardinalities of these two sets, thus $\left |A\right|=\left |A'\right |$ and $\left |B\right|=\left |B'\right |$. Now, since $0\in A',B'$ we can deduce that $A\cup B\subseteq A+B$. Small observation about the nature of $A'$ and $B'$ revivals that $\left |A\cup B\right|=\left |A\right|+\left |B\right|-1$. Combining the last result with $\left |A\cup B\right|\leq\left |A+B\right |$ we get the desired result. ■ Now, returning to text where lemma 1 is used, Proof(EGZ for primes): Without loss of generality let $$(1) \ \  \ \ \ \ \      0\leq a_1\leq a_2\leq\dots \leq a_{2p-1}<p$$ If $a_i=a_{i+p-1}$ for certain $1\leq i\leq p$, then $$\sum^{i+p-1}_{j=i}=pa_i$$ And that is what we looked for. Now suppose that $a_i\neq a_{i+p-1}$ for all $1\leq i\leq p$, then we define $A_1=\{ a_1\}$ and $A_i=\{a_i,a_{i+p-1}\}$for $1\leq i\leq p $, then by repeating the lemma above one can conclude that in $\mathbb{Z}_p$: $$\left|A_1+A_2+\cdots +A_p \right|=p$$ Thus, every element in $\mathbb{Z}_p$ is the sum of exact $p$ terms of our ordered set, and that the sum of $p$ elements of that set divisible by $p$. 
■ Question 2: How really one can deduce $\left|A_1+A_2+\cdots +A_p \right|=p$ from the lemma, and how can one conclude that every element in $\mathbb{Z}_p$ is the sum of exact $p$ terms of our ordered set, and that the sum of $p$ elements of that set divisible by $p$? Searching for more information, I came up with Noga Alon and Moshe Duniner article , which proved EGZ theorem in five different ways. The first proof that they begin with is the related one. They states the Cauchy-Davenport theorem: Cauchy-Davenport theorem: If $p$ is a prime number and $A,B$ are two nonempty subsets of $\mathbb{Z}_p$, then $\left | A+B  \right|\geq \min\{p,\left|A  \right|+ \left | B \right |-1 \}$ And proving in the same matter the prime case of EGZ. Question 3: Do we really need the Cauchy-Davenport theorem to prove EGZ theorem for primes? Thank you.","['discrete-mathematics', 'number-theory', 'additive-combinatorics', 'arithmetic-combinatorics', 'combinatorics']"
1305251,What is the smallest prime of the form $n^n+8$?,"Is there a prime of the form $n^n+8$ , $n\in \mathbb N$ ? If yes, what is the smallest one ? It is clear, that $n$ must be odd and cannot be a multiple of $3$ (otherwise
$n^n+8$ is of the form $x^3+8=(x+2)(x^2-2x+4)$ and both $x+2$ and $x^2-2x+4=(x-1)^2+3$ are greater than $1$, so $n^n+8$ is not prime). Furthermore, I verified that $n^n+8$ is composite for all natural numbers
below $4000$.","['prime-numbers', 'number-theory']"
1305261,"Trying to prove that a function got no limit at $(0,0)$","Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$,   defined by:
$$
f(x,y)=\begin{cases}
1 & y=x^{2}\\
0 & \text{otherwise}
\end{cases}
$$
How can I show that this function got no limit at $(0,0)$?
Can I define a sequence $x_n = \frac{1}{n}$ and get: $\lim_{n\to\infty}f(x_n,(x_n)^2)=1$ so the limit is $1$, but if I will take $\lim_{n\to\infty} f(x_n,0) = 0$ ? Is this a valid proof? Thank you!","['continuity', 'multivariable-calculus', 'limits']"
1305280,"if two space are homotopy equivalent and one is connected, prove that the other is connected as well","I've tried using the definition of homotopy equivalent spaces which states that X and Y are homotopy equivalent if: There are continuous functions $f:X \rightarrow Y,g:Y \rightarrow X$ such that $f \circ g$ is homotopic to the identity map of Y and $g \circ f$ is homotopic to the identity map of X. I don't see how this definition can help us. I know that X and Y have the same fundamental groups as well, can that help me? Kees","['homotopy-theory', 'connectedness', 'general-topology']"
1305316,Differentiation under integral sign (arctan-function),"I have the integral $$ F(s) = \int_{0}^{\infty} \frac{\arctan(sx)}{x(1+x^2)} dx$$
and am supposed to solve it by finding $F'(s)$. So we get 
$$ F'(s) = \int_{0}^{\infty} \frac{\partial F}{\partial s} \frac{\arctan(sx)}{x(1+x^2)} dx =...=  \int_{0}^{\infty} \frac{1}{1+s^2x^2+x^2+s^2x^4} dx $$
and I don't see how I can solve this integral. Should I try another approach?",['multivariable-calculus']
1305322,Conjugation of permutations: left-to-right versus right-to-left.,"In the group $S_n$ I usually use the fact that if $(a_1 a_2 \dots a_r) \in S_n$ is an r-cycle and $\sigma \in S_n$ then $\sigma (a_1 a_2 \dots a_r)\sigma^{-1} = (\sigma(a_1)\sigma(a_2) \dots \sigma(a_r))$. My question is: does this use the convention that permutations act from left-to-right, or right-to-left? Here's a proof for this identity: Let $\rho \in S_n$ be such that $\rho (a_i) = \rho(a_{i+1 \text{ (mod } r)})$, in other words $\rho = (\sigma(a_1) \sigma(a_2) \dots \sigma(a_r))$. Then $a_i \overset{\sigma}{\longmapsto} \sigma(a_i) \overset{\rho}{\longmapsto} \sigma(a_{i+1}) \overset{\sigma^{-1}}{\longmapsto} a_{i+1} \implies \sigma^{-1}\rho\sigma=(a_1 a_2 \dots a_r)$ and $\sigma(a_1 a_2 \dots a_r)\sigma^{-1} = (\sigma(a_1) \sigma(a_2) \dots \sigma(a_r)) = \rho$. Here I have used the convention that permutations act from right-to-left. However, in most other situations I prefer to read permutations from left to right - this is probably the most common convention among group theorists (see here ). For consistency, it seems I should be using $\sigma(a_1 a_2 \dots a_r)\sigma^{-1} = (\sigma^{-1}(a_1) \sigma^{-1}(a_2) \dots \sigma^{-1}(a_r))$ instead of the other version. Am I correct, or have I made a misunderstanding somewhere?","['convention', 'group-theory', 'symmetric-groups', 'permutations']"
1305324,Bound on first derivative $\max \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \max |f''(x)|$,"I want to show that for a function $f \in C_c^2((a,b))$ non-negative, the inequality 
$$\sup \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \sup |f''(x)|$$
holds. I noticed that the left term is equal to  $2 |\sqrt{f(x)}'|^2.$ So the question is equivalent to: Can I bound this term just by the second derivative? Currently, I don't see how this could work. The problem I am also having with the exercise is that we get problems if $f(x)=0$ cause then we may divide zero by zero on the left-hand side and it is not immediate to me that the limit is finite. If anything is unclear, please let me know.","['analysis', 'calculus', 'real-analysis']"
1305325,On an Integral inequality.,I am following a proof and I am having troubles with the last inequality stated Specifically could I have some extra passages on this? $$\int_{\delta}^{\pi} [f(w+u) - f(w)] \frac{\sin^2(nu/2)}{2 \sin^2(u/2)}du \le \frac{1}{2\sin^2(\frac{\delta}{2})}\int_{0}^{\pi} [f(w+u) - f(w)] du $$,"['inequality', 'trigonometry', 'real-analysis', 'integration']"
1305339,Circular definition of tangent line and derivative,"I'm trying to understand the deep relations between the tangent line to the graph of a function $f$ at a given point $P$, and the derivative of $f$ at the same point. Indeed, in many books the derivative is often defined as the slope of the ""tangent"", with only an intuitive definition of what a tangent line is. Then, once the concept of derivative is well assimilated, books define more precisely the tangent as... the line passing through $P = (a, f(a))$ with slope $f'(a)$ ! This seems a bit circular. So some authors use an other approach, starting by giving a geometric definition of a tangent, and then showing the connection with derivatives. That's what one can read in the paper "" What a tangent line is when it isn't a limit "", by Irl Bivens. But I am missing something in the proof of Theorem 1, when the author writes: it suffices to show that $|f(x)-L(x)|\leq (1/2)|L(x)-K(x)|$ where $f$ is a differentiable function, $L$ is the line of equation $L(x) = f'(a)(x-a)+f(a)$ and $K$ an other line, of equation $K(x)=p(x-a)+f(a)$ with $p \neq f'(a)$. So, basically, the goal is to show that $L$ is a better approximation of $f$ than any $K$. But where does the right-hand side come from ? It looks like, locally, $f(x) = 1/2 (L(x)+K(x))$, but why?","['calculus', 'real-analysis', 'derivatives']"
1305343,"Prove that if $p$ is prime greater than $3$ ,then: $p^2+2015$ is multiple of $24$? [duplicate]","This question already has answers here : For any prime $p > 3$, why is $p^2-1$ always divisible by $24$? (21 answers) Closed 6 years ago . Prove that if $ p $ is prime number $(p >3)$, then the number $p^2+2015$ is multiple of $24 $? Thank you for any help","['prime-numbers', 'number-theory', 'divisibility']"
1305345,an exercise about changing the measure and convergence in $L^1$,"this is exercise 17.12 from probability essentials written by jacod & protter. Suppose $lim_{n→∞} X_n = X$ a.s. Let $Y = sup_n |X_n − X|$. Show $Y < ∞$
a.s. , and define a new probability measure $Q$ by
$Q(A) = \frac{1}{c}
E (1_A \frac{1}{1 +Y})$  , where   $c = E (\frac{1}{1 +Y})$  .
Show that $X_
n$ tends to $X$ in $L_1$ under the probability measure $Q$. i've proved that $Y < ∞$
a.s. cam anyone help me with the second part? thanks.","['probability-theory', 'probability', 'random-variables', 'measure-theory']"
1305351,limit question - $\lim_{x\rightarrow a}g(f(x))=c$,"Can i say that if  $\lim_{x\rightarrow a}f(x)=b$ and $\lim_{x\rightarrow b}g(x)=c$
then $\lim_{x\rightarrow a}g(f(x))=c$ ? I don't think so but don't know how to prove it. Thanks.","['calculus', 'limits']"
1305385,Defining a bounded operator on $l^p$,"Let $(c_{jk})_{j,k \in \mathbb{N}} \subset \mathbb{C}$ be such that $a:=\sup_{k \in \mathbb{N}} \sum_{j \in \mathbb{N}}|c_{jk}|<\infty$ and $b:=\sup_{j \in \mathbb{N}} \sum_{k \in \mathbb{N}}|c_{jk}|<\infty$ Prove that $$T:l^p \to l^p,(Tx)_j:=\sum_{k \in \mathbb{N}}c_{jk}x_k$$ defines a bounded linear map with $\|T\|\leq a^{\frac{1}{p}}b^{\frac{1}{q}}$ where $p \in (1,\infty)$ and $q$ is its Hölder conjugate I have spent a couple of hours trying to prove this inequality but nothing seems to be working . I can't even prove that T is a bounded operator. Any hints on how could I go about solving this ? I started by writing the definition of the norm as $\|T\|=\sup_{\|x\|_{p}=1}\big(\|Tx\|\big)=\sup_{\|x\|_{p}=1}\big(\sum_{j \in \mathbb{N}} \big(\sum_{j \in \mathbb{N}}c_{jk}x_k \big)^{p} \big)^{1/p}$
Then I tried to use Holder's inequality  inside bracket but nothing seems to be working . I mean I can try to write down what i tried to do, but none of my attempts seem to go anywhere
Any hint would be appreciated.","['lp-spaces', 'functional-analysis', 'inequality']"
1305386,Why do all parabolas have an axis of symmetry?,"And if that's just part of the definition of a parabola, I guess my question becomes why is the graph of any quadratic a parabola? My attempt at explaining : The way I understand it after some thought is that any quadratic can be written by completing the square as a perfect square + a constant, for ex: $f(x) = x^2 + x$ can be written as $f(x) = (x+\frac{1}{2})^2 - \frac{1}{4}$. So essentially, any quadratic is a displaced version of $f(x) = x^2$, and it's pretty obvious why $f(x) = x^2$ has an axis of symmetry and why it's at the vertex. Is my reasoning correct? And if you have a different way to think about it and explain it, whether geometric, algebraic or other, I would love to see it.","['algebraic-geometry', 'algebra-precalculus']"
1305408,"Prove without Liouville's theorem: $f$ is entire, $\forall z \in \mathbb C: |f(z)| \leq |z|$, then $f=a \cdot z$, $a \in \mathbb C, |a| \leq 1$","Prove without Liouville's theorem: $f$ is entire, $\forall z \in \mathbb C: |f(z)| \leq |z|$, then $f=a \cdot z$, $a \in \mathbb C, |a| \leq 1$ What I tried so far: $f$ is entire, so $f(z)= \Sigma _{n=0}^\infty a_nz^n$, and then $|\Sigma _{n=0}^\infty a_nz^n| \leq |z| \Rightarrow |\frac {\Sigma _{n=0}^\infty a_nz^n}{z}| \leq 1 \Rightarrow |\Sigma _{n=0}^\infty a_nz^{n-1}| \leq 1$ How can I continue from here? Thank you in advance for your assistance!",['complex-analysis']
1305410,Wave equation $u_{xx}+u_{xt}- u_{tt}=0$,"Does anybody know how we can solve the equation $u_{xx}+  u_{xt}-  u_{tt}=0$ with $u(x,0):=g(x)$ and $u_t(x,0):=h(x)?$ I mean it is known how to do this for the wave equation see here but I don't know how to do this in the more general case with the mixed term in it.","['calculus', 'wave-equation', 'real-analysis', 'analysis', 'partial-differential-equations']"
1305420,Using spherical coordinates to find volume of a region,"Use spherical coordinates to find the volume of the region lying above $z = \sqrt{3x^2+3y^2}$ and within the $x^2+y^2+z^2=2az$, $a>0$. So far I know that the first graph is a cone and the second one is some kind of sphere. I have completed the square so that the new equation is: $$x^2+y^2+(z-a)^2=a^2$$
I know how to convert to spherical coordinates but the $a$ is throwing me a bit.","['volume', 'spherical-coordinates', 'calculus', 'multivariable-calculus']"
1305427,limit of a function involving infinite nested roots,"I was given the following problem :
 $$\lim \limits_{x \to \infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ The following is my approach: $= \sqrt{ \lim \limits_{x \to \infty} \frac{x}{x+\sqrt{x+\sqrt{x+...}}} } $ I divide by x: $= \sqrt{ \lim \limits_{x \to \infty} \frac{1}{1+\frac{\sqrt{x+\sqrt{x+...}}}{x}} } $ I then reasoned that obviously:
$ \sqrt{x+\sqrt{x+...}}<x$ that is  $ \frac{\sqrt{x+\sqrt{x+...}}}{x}  = \frac{1}{x^y} : y>0 $ hence  $\lim \limits_{x \to \infty} \frac{\sqrt{x+\sqrt{x+...}}}{x} =0$ I concluded that the original limit equals $ \sqrt{\frac{1}{1+0}} = 1$ Was my reasoning correct? And is this a legitimate mathematical approach to solve this limit?",['limits']
1305450,Why does ${\lambda _i}(A) \ge {\lambda _i}(B)$?,"Let $A,B \in {M_n}$ are Hermitian and $A-B$ has only nonnegative eigenvalues.Why does ${\lambda _i}(A) \ge {\lambda _i}(B)$  (for $i=1,2,\ldots,n$) ?","['linear-algebra', 'matrices']"
1305451,All clubs have a member among $n$ people,"Let $n \geq 14$ be a positive integer. In a city there are more than $n$ clubs, all of them have exactly 14 members. At each group of $n+1$ clubs there is a person who is member of  at least 15 of these $n+1$ clubs. Show that it is possible to select $n$ people such that all clubs have a member among these people. This question is from Hungary.
I tried to proceed by induction on the number $n$ and I could prove the base case $n=14$.
However I don't know how to go on.
Any help is welcome.","['graph-theory', 'algorithms', 'combinatorics']"
1305466,Commutative property of matrix multiplication (or lack thereof),"Assuming $A$ and $B$ are invertible matrices and are of proper dimensions to be multiplied (say, $2\times2$), is the following expression correct for all examples of matrices $A$ and $B$? $$(A^{-1}B)(AB^{-1}) = A^{-1}BAB^{-1} = A^{-1}AB^{-1}B = I^2 = I$$ My understanding is that for matrices $A$ and $B$, $AB$ doesn't necessarily equal $BA$ as matrix multiplication is not commutative. I'm trying to simplify the below expression: $$(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1}$$ Nothing is given about the matrices $A$, $B$, $C$, or $D$ beyond that they are invertible and of correct dimensions such that any matrix multiplication is possible. My process is as below: $$\begin{align}(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1} &= (A^{-1}B^{-1})(AC^{-1})(DC)D^{-1}\\
&= A^{-1}B^{-1}AC^{-1}DCD^{-1}\\
&= B^{-1}A^{-1}AC^{-1}CDD^{-1}\\
&= B^{-1}I^3\\
&= B^{-1}\end{align}$$ You'll notice the error I've made here: $(AB)^{-1} = (B^{-1}A^{-1})$, not $(A^{-1}B^{-1})$, but in the end it doesn't change the answer. The answer in the textbook is indeed $B^{-1}$. According to the above: $$(B^{-1}A^{-1}) = (A^{-1}B^{-1})$$ But matrices are not commutative? Why does the algebra suggest they are?","['linear-algebra', 'matrices']"
1305479,What are the direct summands in $\mathbb{Z}^n$?,"I am interested in knowing the rank $k$ submodules of the $\mathbb{Z}$-module $\mathbb{Z}^n$ which are are also direct summands. I know that $\mathbb{Z}^k$ sitting in $\mathbb{Z}^n$ in the standard way, i.e., $(z_1, \dots, z_k) \mapsto (z_1, \dots, z_k, 0, \dots, 0)$ is one such summand. So if I look at all the different ways in which $\mathbb{Z}^k$ is embedded in $\mathbb{Z}^n$ will that give me all the required summands or will there be more? If so, how do I characterize all the embeddings, i.e., injective $\mathbb{Z}$-module homomorphisms from $\mathbb{Z}^k$ into $\mathbb{Z}^n$? Thank you.","['abstract-algebra', 'modules']"
1305494,Invertible matrix over a ring and its eigenvalues,"Eigenvalues and invertible matrices for fields and vector spaces: Let $K$ be a field (so $K^n$ is a $K$-vector space) and let $A \in K^{n\times n}$ be an $n\times n$-matrix. Then we have the following definition and theorem: Definition: An element $\lambda\in K$ is called an eigenvalue of $A$ iff there is a $v\in K^n\setminus\{0\}$ such that $Av=\lambda v$. Theorem: The matrix $A$ is invertible iff all of its eigenvalues are invertible (i. e. 0 isn't an eigenvalue). ""Eigenvalues"" and invertible matrices for rings and modules: Let $R$ be a commutative ring with a multiplicative identity (so $R^n$ is an $R$-module) and let $B\in R^{n\times n}$ be an $n\times n$-matrix. Then let's define what an eigenvalue of $B$ is: Definition: An element $\mu\in R$ is called an eigenvalue of $B$ iff there is a $v\in R^n\setminus\{0\}$ such that $Bv=\mu v$. Clearly, it is possible for an $R^{n\times n}$-matrix to have only invertible eigenvalues and not to be invertible itself. Take for example $\begin{pmatrix}
3 & -13 \\
1 & -3\end{pmatrix}\in \mathbb{Z}^{2\times 2}$. All of its eigenvalues are invertible (simply because it doesn't have any eigenvalues in $\mathbb{Z}$); however the matrix is not invertible (because its determinant is 4 which is not a unit in $\mathbb{Z}$). So the following is for sure false: Not a theorem! The matrix $B$ is invertible if all of its eigenvalues are invertible (i. e. are units in $R$). Now, I wonder if the following is true: Theorem? If the matrix $B$ is invertible , then all of its eigenvalues are invertible (i. e. are units in $R$). If it is true, how can it be proved? If it is false, what would be a counter-example?","['eigenvalues-eigenvectors', 'inverse', 'linear-algebra', 'modules']"
