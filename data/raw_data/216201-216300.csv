question_id,title,body,tags
4394144,nonsingularity assumption in Lemma V.1.3 in Hartshorne,"Lemma V.1.3 in Hartshorne states that if $C$ is an irreducible nonsingular curve on a surface $X$ , and $D$ any curve meeting transversally with $C$ , then $\#(C \cap D) = \deg_C (\mathscr{L}(D) \otimes \mathcal{O}_C)$ . The proof is based on tensoring the short exact sequence $0 \rightarrow \mathscr{L}(-D) \rightarrow \mathcal{O}_X \rightarrow \mathcal{O}_D \rightarrow 0$ with $\mathcal{O}_C$ to obtain the short exact sequence $0 \rightarrow \mathscr{L}(-D)\otimes \mathcal{O}_C \rightarrow \mathcal{O}_C \rightarrow \mathcal{O}_{C \cap D} \rightarrow 0$ . The exactness on the left of the second sequence follows from the transversality assumption. With this, one identifies $\mathscr{L}(D) \otimes \mathcal{O}_C$ with the invertible sheaf on $C$ associated to the divisor $C \cap D$ . Since the intersection has been assumed transversal, one concludes that the degree of the divisor $C \cap D$ must be equal to $\#(C \cap D)$ . My question is: where does the assumption on the nonsingularity of $C$ come into play? And one more question: If two curves $C, D$ , not necessarily nonsingular, meet transversally at a finite number of points, is their intersection number not always equal to $\#(C \cap D)$ ? Why when defining the intersection number of curves meeting transversally it is required that the curves are nonsingular? (e.g., as in the discussion above Theorem V.1.1 at page 357)","['algebraic-geometry', 'intersection-theory']"
4394175,Rudin RCA ch. 1 ex. 7: Monotone convergence theorem for decreasing sequence,"Suppose $f_n:X\rightarrow[0,\infty]$ is measurable for $n=1,2,3,\dots$ , $f_1\geq f_2\geq f_3\geq\cdots\geq 0$ , $f_n(x)\rightarrow f(x)$ as $n\rightarrow\infty$ , for every $x\in X$ , and $f_1\in L^1(\mu)$ . Prove that then $$\tag{*}\lim_{n\rightarrow\infty}\int_X f_n\,d\mu=\int_X f\,d\mu$$ and show that this conclusion does not follow if the condition "" $f_1\in L^1(\mu)$ "" is omitted. Let $E$ consist of the points $x\in X$ at which $f_1(x)<\infty$ . By the dominated convergence theorem, $$\int_E f_n\,d\mu\rightarrow \int_E f\,d\mu\mbox{.}$$ Since $f_1\in L^1(\mu)$ , $\mu(E^c)=0$ , and hence (*) follows. Let $X=\{1,2,3,\dots\}$ , and let $\mu$ be the counting measure. For each $n$ , define $f_n:X\rightarrow[0,\infty]$ by $$f_n(x)=\left\{\begin{array}{ll}\infty&(x\geq n)\\0&(x<n).\end{array}\right.$$ Then $\lim f_n=0$ , and $\int_X f_n\,d\mu=\infty$ for all $n$ . Is this correct?","['measure-theory', 'solution-verification']"
4394201,"Let $M=2\times 3^2\times 5^3\times 7^4$. Let $a_1,a_2,...,a_{120}$ be the positive divisors of $M$. Calculate $\prod_{i=1}^{120} a_i$","Let $M=2\times 3^2\times 5^3\times 7^4$ . Let $a_1,a_2,...,a_{120}$ be the positive divisors of $M$ . Calculate $$\prod_{i=1}^{120} a_i$$ . My Attempt: I am aware how to find the sum of these $120$ divisors. $$\sum_{i=1}^{120}a_i=(2^0+2^1)(3^0+3^1+3^2)(5^0+5^1+5^2+5^3)(7^0+7^1+7^2+7^3+7^4)$$ But I have never come across a problem asking for product of divisors before. In the product of divisors $2^0$ and $2^1$ will occur $(1+2)(1+3)(1+4)=60$ times each. Similarly, $3^0$ , $3^1$ and $3^2$ will occur $(1+1)(1+3)(1+4)=40$ times each. $5^0, 5^1, 5^2, 5^3$ will occur $(1+1)(1+2)(1+4)=30$ times each. $7^0, 7^1, 7^2, 7^3, 7^4$ will occur $(1+1)(1+2)(1+3)=24$ times each. So product of divisors equals $$(2^0)^{60}.(2^2)^{60}.(3^0)^{40}.(3^1)^{40}.(3^2)^{40}.(5^0)^{30}.(5^1)^{30}.(5^2)^{30}.(5^3)^{30}.(7^0)^{24}.(7^1)^{24}.(7^2)^{24}.(7^3)^{24}.(7^4)^{24}=2^{60}.3^{120}.5^{180}.7^{240}$$ Is this correct. Can there be a shorter or more direct way to do it.","['elementary-number-theory', 'algebra-precalculus', 'combinatorics', 'factoring']"
4394250,How prove this inequality with $\int_{0}^{1}(f(x))^2dx\le\frac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx$,"let $f(x)$ is smooth function，and such $$f(0)=f'(0)=f(1)=f'(1)=0$$ show that $$\int_{0}^{1}(f(x))^2dx\le\dfrac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx$$ where the $\theta $ is  smallest positive root $$\cos{\theta}\cosh{\theta}=1$$ ,and the $\dfrac{1}{\theta^4}$ is best coefficient. I know that this form of inequality is commonly called Wirtinger's inequality： links ,but this  my question conditions are quite many, and it doesn’t seem feasible to use a proof method like that Wirtinger's  inequality,","['inequality', 'real-analysis']"
4394284,"Directional derivatives, continuity at $0$ and differentiability of $f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases}$","I have solved the following problem and I would appreciate some feedback on my solution, thanks: Let $f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases}$ . (a) Find all the directional derivatives of $f$ at $(0,0)$ . (b) Is f continuous at $(0,0)$ ? (c) Is f differentiable at $(0,0)$ ? My solution: (a) $$D_{\mathbf{v}}f=\lim\limits_{t\to 0}\frac{f(\mathbf{0}+t\mathbf{v})-f(\mathbf{0})}{t}=\lim\limits_{t\to 0}\frac{tv_1t^6v_2^2}{t^4v_1^4+t^8v_2^8}\frac{1}{t}=\lim\limits_{t\to 0} t^2\frac{v_1v_2^6}{v_1^4+t^4v_2^8}=0,$$ where $\mathbf{v}=(v_1,v_2)\neq (0,0).$ (b) $f(x,mx)=x^3\frac{m^6}{1+m^8x^4}\xrightarrow{(x,y)\to (0,0)}0$ but $f(x,\sqrt{x})=\frac{1}{2}$ so $\lim\limits_{(x,y)\to (0,0)}f(x,y)$ does not exist and $f$ is thus not continuous at $(0,0).$ (c) $f$ cannot be differentiable at $(0,0)$ since it is not continuous at $(0,0).$","['continuity', 'multivariable-calculus', 'derivatives']"
4394289,"Osculating Plane of $(t,t^2,t^3)$ and a result","Find the osculating plane of curve parametric form $(t,t^2,t^3)$ . Also prove that intersection of three osculating planes on any three points of curve lies on plane containing those three points . My attempt :: I solved first part of question and found equation of osculating on a point on curve whose parameter is t $$3t^2X-3tY+Z-t^3=0$$ Now second part of question is where I am stuck . My thoughts are let three points on curve be whose parameter are a,b and c. Then write equation for their osculating plane ,Find intersection point and  find equation of plane containing three points . And finally satisfie the intersection point in plane . This process seems quite lengthy . Is there a smart way to do it ?","['plane-geometry', '3d', 'differential-geometry']"
4394352,"Multivariable chain rule for $f:\mathbb{R}^2\to\mathbb{R},\ f\ \ \mathcal{C}^2$ function","I am trying to solve the following problem. Suppose $f:\mathbb{R}^2\to\mathbb{R}$ is $\mathcal{C}^2.$ Let $F\begin{pmatrix}r\\\theta\end{pmatrix}=f\begin{pmatrix}r\cos\theta\\r\sin\theta\end{pmatrix}$ . Show that $\frac{\partial^2F}{\partial r^2}+\frac{1}{r}\frac{\partial F}{\partial r}+\frac{1}{r^2}\frac{\partial^2 F}{\partial \theta^2}=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2},$ here the lefthand side is evaluated at $\begin{bmatrix}r\\\theta\end{bmatrix}$ and the righthand is evaluated at $\begin{bmatrix}r\cos\theta\\r\sin\theta\end{bmatrix}.$ What I have done: $DF\begin{pmatrix}r\\\theta\end{pmatrix}=Df\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)D\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}=\left[\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\ \ \frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\right] \begin{bmatrix}\cos\theta & -r\sin\theta\\ \sin\theta & r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\cos\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\sin\theta & -\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\sin\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial F}{\partial r} & \frac{\partial F}{\partial\theta}\end{bmatrix}.$ It is not clear to me now how I should apply the chain rule to compute the left hand side terms like $\frac{\partial^2F}{\partial r^2}$ so I would appreciate an hint about how to do this.","['multivariable-calculus', 'derivatives', 'chain-rule']"
4394404,Spectrum of sum of bilateral shift operator and a compact operator,"Let $T$ be the bilateral shift operator, that is: $T: \ell^2(\mathbb{Z}) \to \ell^2(\mathbb{Z})$ such that $(T(x))_k=x_{k-1}$ (where $x_{k-1}$ means the $k-1$ coordinate of the sequence. I have been able to prove that its spectrum is the unit circle (set of all $\lambda$ s with unit euclidean norm) and I am then asked to show that I can find a compact operator $K$ such that the spectrum of $T+K$ is the unit ball, i.e. the set of all complex numbers with norm not greater than 1. Previously I have been asked to prove that in general, if $T$ and $K$ are bounded and compact operators (respectively) from a Banach space $X$ to itself, then if $\lambda$ is in the spectrum of $T$ but is not an eigenvalue of finite multiplicity it follows that $\lambda$ is also in the spectrum of $T+K$ . Could someone please help me connect the dots? Thanks.","['spectral-theory', 'compact-operators', 'functional-analysis']"
4394409,Solution explanation Durrett probability 5th edition exercise 2.3.12,"Let $X_1,X_2,...$ be a sequence of random variables on $(\Omega,\mathcal{F},P)$ where $\Omega$ is a countable set and $\mathcal{F}$ consists of all subsets of $\Omega$ . Show $X_n \rightarrow X$ in probability implies $X_n \rightarrow X$ a.s. Here is the solution from the book: We can pick $\delta_n \rightarrow 0$ so that $P(|X_n-X|>\delta_n) \rightarrow 0$ . Let $\omega \in \Omega$ with $P(\omega)=p>0$ . For large $n$ we have $P(|X_n-X|>\delta_n)  \leq p/2$ so $|X_n(\omega)-X(\omega)| \leq \delta_n \rightarrow 0$ . If $\Omega_0=\{\omega:P(\{w\})>0\}$ then $P(\Omega_0)=1$ and done. I don't follow the argument, starting ""For large $n$ we have $P(|X_n-X|>\delta_n)  \leq p/2$ so $|X_n(\omega)-X(\omega)| \leq \delta_n \rightarrow 0$ ."". Can someone walk me through? Thanks.","['proof-explanation', 'probability']"
4394413,Questions about the Bochner integral,"I'v seen a proposition saying that: Let $f: \Omega \rightarrow \mathcal{X}$ be a Bochner integrable function and $\mu$ a $\sigma$ -finite measure. Let $\left(A_{n}\right)_{n \in \mathbb{N}}$ be a sequence of disjoint measurable sets and $A:=\bigcup_{n=1}^{\infty} A_{n}$ , then $$
\int_{A} f \mathrm{~d} \mu=\sum_{n=1}^{\infty} \int_{A_{n}} f \mathrm{~d} \mu
$$ and the sum is absolutely convergent. I think the condition ' $\mu$ is $\sigma$ -finite is unnecessary', what's wrong with my proof? My proof: $$
\sum_{n=1}^{\infty}\left\|\int_{A_{n}} f \mathrm{~d} \mu\right\| \leqslant \sum_{n=1}^{\infty} \int_{A_{n}}\|f(\omega)\| \mathrm{d} \mu=\int_{A}\|f(\omega)\| \mathrm{d} \mu<\infty
$$ thus the sum absolutely converges.
Then we just need to verify $$
\left\|\int_{A} f \mathrm{~d} \mu - \sum_{n=1}^{m} \int_{A_{n}} f \mathrm{~d} \mu \right\|\rightarrow 0 (m\rightarrow 0)
$$ Obviously, $$
\left\|\int_{A} f \mathrm{~d} \mu - \sum_{n=1}^{m} \int_{A_{n}} f \mathrm{~d} \mu \right\| = \left\|      \int_{\bigcup_{n=m+1}^{\infty}  A_{n}   }                   f \mathrm{~d} \mu\right\| \leq   \int_{\bigcup_{n=m+1}^{\infty} A_{n}   }                  \| f \mathrm\|{~d}\mu = \sum_{n=m+ 1}^{\infty} \int_{A_{n}}\|f\| \mathrm{d} \mu \rightarrow 0
$$ which completes the proof.","['integration', 'measure-theory', 'bochner-spaces', 'real-analysis']"
4394446,"What exactly do delta method estimates of moments for $1/\bar X_n$, $\bar X_n\sim\mathcal N(\mu,\sigma^2/n)$ approximate? (not as simple as you think)","Let me start with the excerpt out of Casella & Berger's Statistical Inference (2nd edition, pg. 470) that inspired this question. Definition 10.1.7 For an estimator $T_n$ , if $\lim_{n\to\infty}k_n\mathrm{Var}T_n=\tau^2<\infty$ , where $\{k_n\}$ is a sequence of constants, then $\tau^2$ is called the limiting variance or limit of the variances . Example 10.1.8 (Limiting variances) For the mean $\bar X_n$ of $n$ iid normal observations with $\mathrm EX=\mu$ and $\mathrm{Var}\,X=\sigma^2$ , if we take $T_n=\bar X_n$ , then $\lim n\mathrm{Var}\bar X_n=\sigma^2$ is the limiting variance of $T_n$ . But a troubling thing happens if, for example, we are instead interested in estimating $1/\mu$ using $1/\bar X_n$ . If we now take $T_n=1/\bar X_n$ , we find that the variance is $\mathrm{Var}\,T_n=\infty$ , so the limit of the variances is infinity. But recall Example 5.5.23, where we said that the ""approximate"" mean and variance of $1/\bar X_n$ are $$
\mathrm E\left(\frac{1}{\bar X_n}\right)\approx\frac{1}{\mu},
$$ $$
\mathrm{Var}\left(\frac{1}{\bar X_n}\right)\approx\left(\frac{1}{\mu}\right)^4\mathrm{Var}\bar X_n,
$$ and thus by this second calculation the variance is $\mathrm{Var}\,T_n\approx\frac{\sigma^2}{n\mu^4}<\infty$ . This example points out the problems of using the limit of the variances as a large sample measure. Of course the exact finite sample variance of $1/\bar X$ is $\infty$ . However, if $\mu\neq 0$ , the region where $1/\bar X$ gets very large has probability going to $0$ . So the second approximation in Example 10.1.8 is more realistic (as well as being much more useful). It is this second approach to calculating large sample variances that we adopt. Now, I disagree that $\mathrm{Var}\left(1/\bar X_n\right)=\infty$ . While I do agree that $\mathrm E\left(1/\bar X_n^2\right)=\infty$ , the first negative moment $\mathrm E\left(1/\bar X_n\right)$ is clearly undefined and thus so is the variance. Given that both $\mathrm E\left(1/\bar X_n\right)$ and $\mathrm{Var}\left(1/\bar X_n\right)$ are strictly undefined, I have the following question. Question: What exactly do the delta method approximations for $\mathrm E\left(1/\bar X_n\right)$ and $\mathrm{Var}\left(1/\bar X_n\right)$ approximate if the moments of $1/\bar X_n$ are undefined? My thoughts: Let $\bar X_n\sim\mathcal N(\mu,\sigma^2/n)$ and define for $t\in\Bbb R$ $$
\mathcal H[f_{\bar X_n}](t)=\lim_{\epsilon\to 0^+}\int_{\Bbb R\setminus(t-\epsilon,t+\epsilon)}\frac{f_{\bar X_n}(x)}{x-t}\,\mathrm dx,
$$ which is the Hilbert transform of the density function for $\bar X_n$ . Since the Hilbert transform commutes with derivatives, i.e. $$
\mathcal H[\partial_t^k u]=\partial_t^k\mathcal H[u],
$$ the expression $\mathcal H[f_{\bar X_n}](t)$ represents a sort of generating function for the negative moments of $\bar X_n$ , which do not exist in the traditional sense.  We define $$
\mathrm E\bar X_n^{-k}:=\frac{1}{(k-1)!}\partial_t^{k-1}\mathcal H[f_{\bar X_n}](t)\Big|_{t=0}.
$$ For our particular example $$
\mathcal H[f_{\bar X_n}](t)=\frac{\sqrt 2}{\sigma/\sqrt n}\mathcal D\left(\frac{\mu-t}{\sqrt 2\,\sigma/\sqrt n}\right),
$$ with $\mathcal{D}(z)=e^{-z^{2}}\int_{0}^{z}e^{t^{2}}\,\mathrm{d}t$ being Dawson's integral; thus $$
\mathrm E\bar X_n^{-1}=\frac{\sqrt 2}{\sigma/\sqrt n}\mathcal D\left(\frac{\mu}{\sqrt 2\,\sigma/\sqrt n}\right)
$$ and $$
\mathrm{Var}\bar X_n^{-1}:=\mathrm E\bar X_n^{-2}-(\mathrm E\bar X_n^{-1})^2
$$ with $$
\mathrm E\bar X_n^{-2}=\frac{\sqrt 2\,\mu}{(\sigma/\sqrt n)^3}\mathcal D\left(\frac{\mu}{\sqrt 2\,\sigma/\sqrt n}\right)-\frac{1}{(\sigma/\sqrt n)^2}.
$$ Now, the Dawson integral admits the following asymptotic expansion for $x\to\infty$ $$
\mathcal D(x/\sqrt 2)\sim\frac{1}{\sqrt 2\,x}\sum_{k=0}^\infty (2k-1)!!\frac{1}{x^{2k}},
$$ so letting $n\to\infty$ we have for the first moment $$
\begin{align}
\mathrm E\bar X_n^{-1}
&\sim\frac{1}{\mu}\sum_{k=0}^\infty (2k-1)!!\left(\frac{\sigma/\sqrt n}{\mu}\right)^{2 k}\\
&=\frac{1}{\mu}+\mathcal O\left(\frac{1}{n}\right),
\end{align}
$$ which is the first-order delta method approximation for the mean of $1/\bar X_n$ . Likewise, for the second moment we find $$
\mathrm E\bar X_n^{-2}
\sim\frac{1}{(\sigma/\sqrt n)^2}\sum_{k=1}^\infty (2k-1)!!\left(\frac{\sigma/\sqrt n}{\mu}\right)^{2 k},
$$ which upon combining with the expansion for $\mathrm E\bar X_n^{-1}$ gives the asymptotic approximation for $n\to\infty$ $$
\begin{align}
\mathrm{Var}\bar X_n^{-1}
&=\frac{1}{\mu^2}+3\frac{(\sigma/\sqrt n)^2}{\mu^4}+\mathcal O\left(\frac{1}{n^2}\right)-\left(\frac{1}{\mu^2}+2\frac{(\sigma/\sqrt n)^2}{\mu^4}+\mathcal O\left(\frac{1}{n^2}\right)\right)\\
&=\frac{(\sigma/\sqrt n)^2}{\mu^4}+\mathcal O\left(\frac{1}{n^2}\right),
\end{align}
$$ which is the first-order delta method approximation for the variance of $1/\bar X_n$ . So in this context, the delta method moment ""approximations"" are equal to the first term in the asymptotic expansions for our exact generalized moments in the case $n\to\infty$ . To ask my question from a different perspective: If we are to assign any value to the moments of $1/\bar X_n$ , should we instead assign these generalized values for $E\bar X_n^{-k}$ with the delta method moment estimates simply being an approximation of these ""exact"" moments? Indeed, the generalized moments seem to provide a more accurate picture of the moments for $1/\bar X_n$ whenever $|\mu/(\sigma/\sqrt n)|\to\infty$ ( $n$ does not necessarily have to be large), which implies $f_{\bar X_n}(0)$ is vanishingly small. Consider the case $\mu=6$ , $\sigma=1$ , and $n=1$ so that $\bar X_n=X\sim\mathcal N(\mu,\sigma^2)$ . I simulated $X$ in MATLAB with the following code: mu = 6;
sigma = 1;
Y = 1./normrnd(mu,sigma,1e8,1);

EY = sqrt(2)/sigma*dawson(mu/(sqrt(2)*sigma));
[EY 1/mu mean(Y)]

EY2 = sqrt(2)*mu/sigma^3*dawson(mu/(sqrt(2)*sigma))-1/sigma^2;
[EY2 1/mu^2 mean(Y.^2)]

EY3 = (sqrt(2)*(mu^2-sigma^2)*dawson(mu/(sqrt(2)*sigma))-mu*sigma)/(2*sigma^5);
[EY3 1/mu^3 mean(Y.^3)]

VarY = EY2-(EY)^2;
[VarY sigma^2/mu^4 var(Y)]

fY = @(y) normpdf(1./y,mu,sigma)./y.^2;
ax = linspace(0.05,0.35,512);

figure
hold on;
histogram(Y,linspace(0.05,0.35,40),'Normalization','pdf')
plot(ax,fY(ax),'Color',[0 0 0],'LineWidth',2) Here are the results showing that the generalized moments more accurately reflect the sample statistics. $$
\begin{array}{cccc}
 &\text{generalized} &\text{delta} &\text{sample}\\
\mathrm EX^{-1} &0.1718 &0.1667 &0.1718 \\
\mathrm EX^{-2} &0.0305 &0.0278 &0.0305 \\
\mathrm EX^{-3} &0.0056 &0.0046 &0.0056 \\
\mathrm{Var}X^{-1} &0.0010 &0.0008 &0.0010
\end{array}
$$","['singular-integrals', 'asymptotics', 'delta-method', 'cauchy-principal-value', 'probability-theory']"
4394451,Convergence in probability and convergence in distribution in the context of OLS,"What I am missing here? Let there be three iid variables $(W_i, D_i, U_i)$ where $U_i$ may have an error interpretation in a traditional OLS context. That is $E[W_iU_i]=0$ and $E[D_iU_i]=0$ . Moreover, everything is properly defined, second moments and covariances across all three variables exist and for simplicity the expectations of all three variables equal $0$ . Consider now the following problem: We are interested in identifying the limiting behavior (as n grows to $\infty$ ) of these two expressions $\frac{1}{n}\sum{W_i^2}\frac{1}{\sqrt{n}}\sum{D_iU_i}$ which should be completely identical to $\frac{1}{\sqrt{n}}\sum{W_i^2}\frac{1}{n}\sum{D_iU_i}$ . This kind of settings are very common in OLS consistency proofs. These problems are usually tackled by invoking some sort of CLT on the error component (which will be distributed as a $N(0, E[D_i^2U_i^2])$ and some convergence in probability on the first term. We apply Slutsky Theorem and we are essentially done. However, and even if I strongly belive I am wrong, I don't manage to understand why the opposite does not lead to the same result. That is, apply the CLT over the $W_i^2$ component (assuming the existence of fourth moments) and then use the convergence in probability of $\frac{1}{n}\sum{D_iU_i}$ to $E[D_iU_i] = 0$ . But, and this is the key problem. Then we will obtain deterministic result of the form $N(0,0)$ rather than $N(0,\sigma_{w}^2E[D_i^2U_i^2])$ . What I am missing here? Thank you for your help!","['statistics', 'probability-limit-theorems', 'central-limit-theorem', 'probability-distributions', 'probability']"
4394466,Why doesn't this differential technique work?,"Suppose we try to solve the ODE $y' = y,$ we can rearrange this into $y'/y = 1,$ integrate and obtain $\ln(y) = x.$ However, let's try this on $y'' = xy$ , then we rearrange this into $y''/y = x,$ but the solution is Ai $(x)$ and Bi $(x)$ , the airy functions. Can this technique of rearranging then integrating be salvaged to explain this result or does it fail in some way? Is there a known way to integrate to derive a function $f$ in terms to $y'$ and $y$ ?",['ordinary-differential-equations']
4394493,Does there exists another approach to solve for the product of such expression?,"Problem: if the real roots of $x^3-3x+1$ are $\alpha , \beta $ and $\gamma,$ then what is the value of cyclic $(\alpha^2-\gamma)\;?$ Here is my approach, using trigonometry. Is my work correct?","['trigonometry', 'solution-verification', 'polynomials']"
4394522,"If $X\subset\Bbb R^n$ is a topological ball, is $\partial X$ a topological sphere?","Given a set $X\subset\Bbb R^n$ (I am most interested in $n=4$ ) that is homeomorphic to the $n$ -ball $B^n:=\{x\in\Bbb R^n\mid \|x\|\le 1\}$ . Is it true that the boundary $\partial X$ is homeomorphic to the $(n-1)$ -sphere $S^{n-1}:=\partial B^n$ ? This sounds like an inverse of the generalized Schönflies theorem , but I haven't found anything on this.","['geometric-topology', 'general-topology', 'spheres']"
4394545,Minimum swaps necessary for every person to meet every other person at every location,"The setting: There are 9 people. There are 3 locations. At any time, there are 3 people at each location. People can only swap locations at the same time. The goal: Every person visits each location once. Every person meets each person at least once. At each swap, one person stays at the location. Example: The numbers in the table below represent which person is at which location. e.g. in iteration 1, persons 1, 2, and 3 are at location 1. All goals are met in this example, but can it be done in fewer iterations? Iteration Location 1 Location 2 Location 3 1 1, 2, 3 4, 5, 6 7, 8, 9 2 4, 2,  9 1, 5, 7 3,  8,  6 3 4, 1, 8 2, 6, 7 3,   9, 5 4 4, 3, 7 2, 5, 8 1, 9, 6 5 5, 6, 7 3, 8, 9 1, 2, 4","['discrete-optimization', 'combinatorial-designs', 'combinatorics']"
4394552,"Assume $X_1,...,X_n$, are i.i.d, $E(X^k)=\mu_k, k=3,4$ Then sample variance satisfies $\sqrt{n}(\hat\sigma^2-\sigma^2)\to_D N(0,\alpha^2)$","Assume $X_1,...,X_n$ , are i.i.d, $E(X_i)=\mu, Var(X_i) = \sigma^2, E(X^k)=\mu_k, k=3,4$ Then sample variance satisfies $\sqrt{n}(\hat\sigma^2-\sigma^2)\to_D N(0,\alpha^2)$ , find $\alpha$ the question doesnt say but I assume it means sample variance $\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2$ , not divided by $n-1$ . So Central Limit Theorem gives me that $\sqrt{n}(\frac{1}{n}\sum_{i=1}^n X_i - \mu)\to_D N(0,\Sigma)$ And so from here I want to use delta method, to find some function $f$ , to take $f(\bar X)-f(\mu)=\hat\sigma^2-\sigma^2$ and then use the delta method to compute $\alpha$ I expanded $\hat\sigma^2=\frac{1}{n}\sum_{i=1}^n (X_i^2 - 2X_i \bar X +\bar X^2)=\frac{1}{n}\sum_{i=1}^n X_i^2 - \bar X^2$ I know that $\sigma^2=E(X^2)-\mu^2$ From here I'm not sure what to do, I also don't see where the condition that $E(X^k)=\mu_k, k=3,4$ would be needed.","['statistics', 'central-limit-theorem', 'probability']"
4394579,"What are some ""easy, transcendental"" constructions of finite nilpotent groups?","I considered this question in teaching abstract algebra.  I am trying to impress upon my students about the messiness of studying nilpotent groups generally.  More specifically, I am looking for ways of constructing families of $2$ -groups which demonstrate this in a fairly transparent and somewhat transcendental manner. For example, is there some construction which uniquely yields a non-trivial $2$ -group for every graph (likely with some special properties); this is what I mean by ""transparent"".  When I say ""transcendental"", I mean the that the idea should transcend the usual things one sees in typical graduate sequence textbook in abstract algebra.  It is easy, for example, to form groups of unipotent matrices over fields of characteristic $2$ , but I am looking for something that displays more chaos and disorder.  Graphs seem like a good place to start because they are so intuitive. I hope this question is not too vague.  I would personally like to better understand $p$ -groups generally, so I am asking this with a somewhat selfish motive.  Of course, it would be nice to know something that my students would be able to grasp.  Thank you.","['graph-theory', 'finite-groups', 'nilpotent-groups', 'abstract-algebra', 'discrete-mathematics']"
4394581,"$O_1, O_2, O_3, O_4$ circumcenters for triangles $AQM , MBN, NCP, PDQ$.","Let ABCD a parallelogram and $M, N, P, Q$ on $AB, BC, CD, DA$ and $O_1, O_2, O_3, O_4$ circumcenters for triangles $AQM , MBN, NCP, PDQ$ . Show that $O_1O_2,O_3,O_4$ is a parallelogram. In a rectangle it's easy to prove.
I tried to costruct line bisectors but I didn't find anything.","['euclidean-geometry', 'geometry']"
4394588,Convergence of a system of ODE's (replicator dynamics),"I have to find the points of convergence (i.e. $\lim_{t\to\infty} v^k(t)$ ) of the following replicator dynamics, given by a system of $4$ linear ODE's: $$\frac{\dot{v}^k(t)}{v^k(t)}=\alpha[(Av)^k-v^TAv]\quad if\quad v^k(t)>0$$ $$\dot{v}^k(t)=0\qquad if\quad v^k(t)=0$$ given an initial $v^k(0)>0$ for $k=1,2,3,4$ . For what matters $A$ is the following matrix: $$
A=\begin{bmatrix}
4 & 5 & 4 & 3\\
3 & 4 & 5 & 4\\
2 & 3 & 4 & 5\\
1 & 2 & 3 & 4
\end{bmatrix}
$$ I have never studied dynamical systems, and I only know how to study those of the very simple form: $$\dot{x}=Ax$$ via the exponential matrix of $A$ . So I am clueless. Any help would be precious, also, I would be very interested in an handy reference about this kind of problems.","['dynamical-systems', 'reference-request', 'ordinary-differential-equations', 'real-analysis']"
4394649,tough GRE subject problem,"what is the smallest value for $n$ for which the following limit exists for all $r \geq n$ ? $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{\vert x \vert^2+\vert y \vert^2}$$ I thought of using LHopitals rule but get stuck differentiating the absolute values. I wonder if there's a trick I am unaware of. So the comments are saying to rewrite as $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{x^2+ y^2}$$ Then by LHopitals Rule (twice) I get $$\frac{r(r-1)x^{r-2}}{4}$$ But I get stuck here.","['limits', 'multivariable-calculus']"
4394761,Cauchy's theorem and mathematica disagree? Integral involving branch points.,"Consider the following integral: $$\int_{-\infty}^{\infty} \frac{dx}{\sqrt{x^2-2i\epsilon x -1}(x^2+1)}$$ where $\epsilon$ is an infinitesimal positive number. In the complex $x$ -plane, the integrand has two poles and two branch points. The poles are at $x=\pm i$ and the branch points are at $x=\pm 1 - i\epsilon$ . You can see that for $\epsilon>0$ the branch points are shifted away from the contour. The square-root function is taken to be the standard principal square-root function: the branch cut for $\sqrt{x}$ runs along $(-\infty,0]$ , and $$\sqrt{e^{i\left(\frac{\pi}{2}\pm\delta \right)}}=\pm i$$ This integral may be illustrated as follows: The green line represents the branch cut, across which the phase of the integrand is discontinuous. Because the integration contour (red line) does not pass through it, we should be able to solve this integral by closing the contour in the upper-half plane, picking up only the reside at $x=i$ . $$\begin{align}
\int_{-\infty}^{\infty} \frac{dx}{\sqrt{x^2-2i\epsilon x -1}(x^2+1)}&=2\pi i \,\textrm{Res}\left(I(x);x=i\right)\\
&=2\pi i\frac{1}{i\sqrt{2} (2i)}\\
&=-\frac{i\pi}{\sqrt{2}}\\
&\approx -i\,2.22144
\end{align}$$ However when I integrate via Mathematica, I get a totally different answer. NIntegrate[1./(Sqrt[x^2 - 2.*I*0.0001*x - 1]*(x^2 + 1)), {x, -\[Infinity],\[Infinity]}, MaxRecursion -> 20, Method -> ""LocalAdaptive""]

> 1.24641 - 8.75132*10^-7 I So the answer Mathematica gives is approximately $1.246$ , which completely doesn't agree with the answer previously derived via the residue theorem. What's going on here? Which answer is wrong? Or are both answers wrong?","['integration', 'complex-analysis', 'complex-integration', 'mathematica']"
4394788,Phase portrait of the simple pendulum. How much time does it take to get from one unstable equilibrium to the other?,"Consider a simple pendulum. I am trying to understand the phase portrait of its dynamics. I read that the ""time"" in which the dynamical system goes from the the unstable equilibrium $\theta = - \pi$ to $\theta = \pi$ , is infinite. I do not understand how to interpret, let alone formally prove this fact; I read that it should follow from Cauchy's existence and uniqueness theorem. Can you explain to me both at the intuitive and formal level this fact?","['mathematical-physics', 'classical-mechanics', 'ordinary-differential-equations', 'dynamical-systems']"
4394822,Notation for Lebesgue integration.,"I haven't seen this explicitly stated anywhere, so I just wanted to confirm my understanding of the standard notation for Lebesgue integration. Say we have a measure space $(X, \mathcal{A}, \mu)$ and measurable function $f: X \rightarrow \mathbb{R}$ be a real-valued measurable function.
Is one of the below preferred over the other to denote the Lebesgue integral? $\int_X f(x) d\mu(x)$ $\int_X f(x) \mu(dx)$","['notation', 'measure-theory']"
4394841,"Are Cantor-like sets disjoint for $\xi,\eta$ with no common power?","Let $\xi,\eta \in (0,\frac{1}{2})$ . Let $C_\xi$ (and analogously for $C_\eta$ ) be the perfect symmetric set built by iterating the transformation $$[0,1] \to [0,\xi]\cup [1-\xi, 1].$$ Will the sets $C_\xi$ and $C_\eta$ be disjoint, assuming $\xi^n\neq\eta^m$ for all $n,m\ge1$ ? My approach has reduced the problem to this other question .","['summation', 'cantor-set', 'real-analysis', 'general-topology', 'geometric-series']"
4394848,How to show that $P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n$?,"The question is : Suppose that for each positive integer $n$ , $X_{n1},X_{n2},\dots,X_{nn}$ are independent, identically distributed random variables taking only the values $-\sqrt{n}$ , $0$ , $\sqrt{n}$ , such that \begin{equation*}
P(X_{n1}= -\sqrt{n})= P(X_{n1}= \sqrt{n})=\frac{1}{2n^2}, P(X_{n1}=0)= 1- \frac{1}{n^2}.
\end{equation*} i) Show that $P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n$ ? and use this to show that $X_{n1}+X_{n2}+\dots+X_{nn} \Rightarrow \delta_0 $ as $n \rightarrow \infty$ , where $\delta_0$ is degenerate probability measure defined by $\delta_0(\{0\})=1$ and $\delta_0(\{R-0\})=0$ . I tried to solve this by using the Lindeberg central limit theorem and I have shown that $E(X_{nk})=0$ , $\operatorname{Var}(X_{nk})=1/n$ and $\operatorname{Var}\left(X_{n1}+X_{n2}+\dots+X_{nn}\right)=1$ . Can anyone give me some hints to solve this question?","['statistics', 'central-limit-theorem', 'probability-theory', 'probability']"
4394893,"Using related rates, why can we ignore dimensions and consider rates of change, when in seemingly identical situations, we must consider both?","So I was preparing a lesson on related rates for the calc 1 class I am a TA for and I realized that the two problems below in the photo are basically identical: Given a right triangle, x, x', y, y' are known, Find z' (or s'). Problem #1 and $4 are solved identically, but in problem #4, we can use a ""cheat"" and just consider a right triangle with legs x'=25 and y'=60 and hypotenuse=s'. Solving for $s'... \\s'=\sqrt{x'^2+y'^2}=\sqrt{25^2+60^2}=\sqrt{4225}=65 $ This implies the distance between the cars is changing at constant rate, independent of the location of the cars. But this method does not work for the seemingly identical problem #1. I am conflicted... why is this ""cheat"" only viable for some instances of these problems and not all? I checked back in my own notes from calc 1 and this ""cheat"" could be used on other problems too, so it's not something unique with the numbers in #4.","['related-rates', 'physics', 'calculus', 'derivatives']"
4394930,"Definition of ""separate"" in Diestels‘s Graph Theory","In Dietels Graph theory , he defined ""separate"" in chapter 1.4 as If $A, B \subseteq V$ and $X \subseteq V \cup E$ are such that every $A-B$ path in $G$ contains a vertex or an edge from $X$ , we say that $X$ separates the sets $A$ and $B$ in $G$ . Note that this implies $A \cap B \subseteq X$ . So vertex in A or B is allowed to be in X? However, in chapter 3.3, he stated the following theorem: A set of $a-B$ paths is called an $a-B$ fan if any two of the paths have only $a$ in common. Corollary 3.3.4. For $B \subseteq V$ and $a \in V \backslash B$ , the minimum number of vertices separating $a$ from $B$ in $G$ is equal to the maximum number of paths forming an $a-B$ fan in $G$ . Here is the contradiction: isn't {a} itself a separator of $a$ and B? Which makes the corollary wrong in most cases. This is such a fundamental and important concept yet I can't find explicit explanation in the book, could someone clarify it? Thanks!","['graph-theory', 'graph-connectivity', 'combinatorics', 'discrete-mathematics', 'terminology']"
4395022,"Is there a bijection from $[1, \infty)$ to half a unit sphere?","So I'm trying to find a bijection from $[1, \infty)$ to the surface of the unit hemisphere $U$ (without a flat bottom) in spherical coordinates. I define this below: $$U = \{(\theta, \psi).\, \theta \in [0,2\pi) \wedge \psi \in [0,\pi/2)\} - \{(\theta, 0).\, \theta \in (0,2\pi)\}$$ I defined $U$ like this because the coordinates $(0,0)$ and $(1,0)$ for example represent the same point on the surface ( $\theta$ is the annulus and $\psi$ is the polar angle.) What I'm thinking of doing is defining a function $f:[1,\infty) \to U$ where I partition the domain such that each element $e\in P \subseteq [1,\infty)$ where $P$ is a partition can be mapped to a point in one of the unique circular cross-sections of U. $P$ should also have the same number of elements as the number of points in this cross section. I also want $f(1) = (0,0)$ . I'm not sure if wording $f$ like this is sufficient to show that it is a bijection. Could I please have some help?",['functions']
4395079,"What is the sum of natural numbers that have $5$ digits or less, and all of the digits are distinct?","$1+2+3+\dots+7+8+9+10+12+13+\dots+96+97+98+102+103+104+\dots+985+986+987+1023+1024+1025+\dots+9874+9875+9876+10234+10235+10236+\dots+98763+98764+98765=$ The only thing I can do is to evaluate a (bad) upper bound by evaluating the sum of natural numbers from $1$ to $98765$ , that is equal to $98765 \times (98765+1)/2=4,877,311,995$ . So the desired answer is less than that. Any help would be appreciated. Thanks.","['summation', 'decimal-expansion', 'natural-numbers', 'sequences-and-series']"
4395125,Optimal speed for approaching red light to maximize velocity with non-uniform probability,"Problem statement When I cross red lights, my goal is to being going as fast as possible when the light turns green. I am at distance $D$ from a traffic light when it turns red. Let the time length of the red light be $t_{red}$ with probability density $ p$ on $[0,T_{max}]$ . My velocity $v(t)$ must be such that $\int_{0}^{T_{max}} v(t)dt \leq D$ . ( I can't cross the light while it's red.) Given any $C$ , I would like to find $v$ to maximize $E(v(t_{red}))$ given $Var(v(t_{red})) < C$ . My best solution so far What follows is my best family of solutions so far. I can show that it is optimal for $C=0$ and $C=\infty$ I would appreciate your help confirming or denying it is optimal for other values of $C$ . Let $F(a) = \int_0^{T_{max}}p^a(t)dt$ Note that $F(0)=T_{max}$ and $F(1)=1$ Let $v_a(t) = \frac{D}{F(a)}{p^a(t)}$ . We have $\int_0^{T_{max}}v_a(t)dt=D$ $E(v_a(t_{red}))=\frac{D}{F(a)}\int_0^{T_{max}}p^{a}pdt=D\frac{F(a+1)}{F(a)}$ $E(v_a(t_{red})^2) =\frac{D^2}{F(a)^2}\int_0^{T_{max}}(p^a)^2pdt= D^2\frac{F(2a+1)}{F(a)^2} $ $Var(v_a(t_{red}))= D^2\frac{F(2a+1)-F(a+1)^2}{F(a)^2}$ To get a feel for the expected value and variance of this solution, I plotted it out for a simple example: I can show you that this is optimal for at least two values of $a$ . $a=0$ if $a=0$ then $E(v_0(t_{red})) = D\frac{F(1)}{F(0)}=\frac{D}{T_{max}}$ . Also $Var(v_0(t_{red})) = D\frac{F(1)-F(1)}{F(0)}=0$ . This is optimal for $C=0$ since if $C=0$ then $v$ is constant, and must satisfy $\int_{0}^{T_{max}} v(t)dt \leq D$ while trying to maximize $E(v)$ . Side note: This is also the solution of the problem if you assumed uniform probability distribution from the get-go (which for practical purposes is a fair assumption). $a=\infty$ I can also show that this is optimal for $a=\infty$ in this case $C=\infty$ and this case this is simply the extremal Holder's equality Holder's inequality says: $D||p||_{\infty}=sup(\int_0^{T_{max}} vpdt: ||v||_1=D)$ and since $v_{\infty}$ essentially approaches a delta function around the supremum with weight $D$ we hit this upper bound at $v_{\infty}$ Side note: this solution is hilariously nonphysical. The driver waits for the moment where the light is most likely to turn and accelerates to an astronomic velocity towards the light for a split second and the halts to a dead stop. Thats what happens when we allow infinite variance. Question Of course I have just proved the two easiest cases. I would love to show that for any given $C$ the $v_a: Var(v_a) = C$ will maximize $E(v)$ for all $Var(v)\leq C$ What I've tried One method I have been able to think of for proving optimality is to show
if $g: E(g(t))=0, \int_0^{T_{max}}g = 0$ then $E(v_a^2) \leq E((v_a+g)^2) \implies Var(v_a)\leq Var(v_a+g)$ . not much came out of this approach for me. One promising approach considering instead something like $E(v_ag) \geq E(v_a)$ and $ \int_0^{T_{max}}v_ag = \int_0^{T_{max}}v_a$ the problem turns into the following: if $  \int_0^{T_{max}}p^ag = \int_0^{T_{max}}p^a$ and $  \int_0^{T_{max}}p^{a+1}g \geq \int_0^{T_{max}}p^{a+1}$ then does $  \int_0^{T_{max}}p^{2a+1}g^2 \geq \int_0^{T_{max}}p^{2a+1}$ ? this is nice as: $p^{a}gp^{a+1}g= p^{2a+1}g^2$ and $p^{a}p^{a+2}= p^{2a+1}$ . So we almost want a rule like $\int a \geq \int b, \int c = \int d \implies \int ac \geq \int cd$ under our assumptions.","['statistics', 'ordinary-differential-equations', 'optimal-control', 'optimization', 'holder-inequality']"
4395177,Measure of correlation between two random variables,"I have two random variables, $X$ and $Y$ that are jointly distributed according to CDF $F(.,.)$ and a pdf $f(.,.)$ with support $[0,1]\times [0,1]$ . Moreover, the marginal distributions, $F_X(x):= \int_0^1 f(x,y) dy$ and $F_Y(y)$ (defined analogously) coincide. I want to define the following notion of correlation or interdependence between $X$ and $Y$ . $(X,Y)$ are more correlated than $(\hat X,\hat Y)$ if, The marginal distributions of $X,Y,\hat X,\hat Y$ coincide. And, for a.e. $x$ and interval $I$ such that $x \in I$ , \begin{align*}
\mathbb P(Y \in I \vert X = x) \ge \mathbb P(\hat Y \in I \vert \hat X = x) 
\end{align*} I am unable to find any order that says something like this. Does anyone know if this notion has been used by anyone? Any references would be great.","['correlation', 'statistics', 'probability']"
4395191,Every boolean function is multiplicative with probability greater than $1/2$,"Let $f:\left\{-1,1\right\}^n\to\left\{-1,1\right\}$ . How to show that $$
P_{{x,y,z}} \{f(xyz)=f(x)f(y)f(z)\} \ge 1/2?
$$ where $x,y,z$ are distributed uniformly and independently on $\left\{-1,1\right\}^n$ . Equivalently, the set $$
\{(x,y,z)\in (\left\{-1,1\right\}^n)^3 \,|\,f(xyz)=f(x)f(y)f(z)\}
$$ has at least $2^n\times 2^n\times 2^{n-1}$ elements. I think there should be a simple argument that I am missing. Edit: I forgot to define: $$
(xy)_i:=x_iy_i, \,\,\, (xyz)_i:=x_iy_iz_i,
$$ so the product is component-wise.","['boolean-algebra', 'combinatorics', 'discrete-mathematics', 'probability', 'computer-science']"
4395231,Calculating the following MLE,"Q. A biased die favours the number 2. It's rolled 4 times and 2 comes up twice. It's rolled again 4 times and 2 comes up once. Calculate the Likelihood and find the MLE. My working out: $L(\theta;x) = \binom{8}{3}\theta^3(1-\theta)^5$ , then taking the first derivative to find the local maximum $\binom{8}{3}\theta^2(1-\theta)^4(3(1-\theta) - 5\theta)=0 \implies \theta = \frac{3}{8}$ we take the second derivative to check the turning points. $\frac{\partial L(\theta;x)}{\partial \theta}=2\binom{8}{3}\theta(1-\theta)^3(32\theta^2-25\theta+3)$ plugging in for $\theta = \frac{3}{8}$ we finally get $2 \cdot 56 \cdot 3/8 \cdot (2/8)^3(32 \cdot (3/8)^3-25\cdot 3/8 + 3) = -2.06$ therefore, $\theta = \frac{3}{8}$ is a global maximum. Have i derived the MLE correctly?","['statistics', 'maximum-likelihood']"
4395250,Two roots between bounds,"Let $a$ and $b$ satisfy $a \geq b>0, a+b=1$ . Prove that if $m$ and $n$ are positive integers with $m<n$ , then $a^{m}-$ $a^{n} \geq b^{m}-b^{n}>0$ . For each positive integer $n$ , consider a quadratic function $$g_{n}(x)=x^{2}-b^{n} x-a^{n} .$$ Show that $g(x)$ has two roots that are in between $-1$ and $1$ . Hint: Use derivatives. Question was given to me by my teacher. I have successfully proved $(1)$ by considering a function $p(x) = a^x - b^x$ then working with its derivative involving the $\log$ function and provided data to reach the conclusion. (I admit that this idea is not original and is borrowed from the proof of $e^{\pi} > {\pi}^e$ ) I couldn't figure out $(2)$ . Writing $g(x) = x^2 - bx - a$ then its discriminant is $b^2 + 4a > 0$ by provided data so it does have distinct real roots. Checking the bounds $g(1) = 1 - b - a $ which is $0$ (by provided data) which doesn't lead anywhere and hence any attempt of IVT (or rather, Bolzano's Th.) is not possible. So nothing conclusive is obtained but even if IVT held would it show that the roots are within $-1$ and $1$ ? $\star$ Is there a theorem which shows that a polynomial must have two roots in some interval or is there some trivial deduction that I am missing on ? Any hints/comments on how to proceed will suffice. Sorry if something is incorrect in the above reasoning.","['calculus', 'polynomials', 'algebra-precalculus']"
4395251,Tiling of Klein quartic with 56 equilateral triangles,"I am interested in the regular hyperbolic 14-gon associated with the Klein quartic (Klein's famous ""Hauptfigur""). This 14-gon, or more precisely the genus-3 surface obtained by identifying its edges in appropriate ways, can be tiled by 56 equilateral $2\pi/7$ triangles, as beautifully illustrated by Greg Egan here: https://www.gregegan.net/SCIENCE/KleinQuartic/KleinQuartic.html On the other hand, I know the Klein 14-gon can be tiled by 336 Schwarz triangles (the smallest triangles in Egan's picture). This tiling is described by the (2,3,7) triangle group $\Delta(2,3,7)$ : $\Delta(2,3,7)=\langle a,b,c\mid a^2=b^2=c^2=(ab)^2=(bc)^3=(ca)^7=1\rangle$ , where $a,b,c$ are reflections across the 3 edges of a given Schwarz triangle. The edge identifications of the 14-gon are described by a torsion-free index-336 normal subgroup $\Gamma\triangleleft \Delta(2,3,7)$ . The factor group $G=\Delta(2,3,7)/\Gamma$ is the order-336 group of automorphisms of the Klein surface (here including both conformal and anti-conformal automorphisms). Elements of $G$ ""translate'' one Schwarz triangle to another, and thus tile the entire surface starting from any given Schwarz triangle. Now, observing that each $2\pi/7$ triangle consists of 6 Schwarz triangles, here is what I want to know: Analogously to $\Delta(2,3,7)$ , is there an infinite (Fuchsian) group $\Delta'$ which tiles the hyperbolic plane with $2\pi/7$ triangles? If so, can $\Delta'$ be expressed as a (normal?) index-6 subgroup of $\Delta(2,3,7)$ ? What are its generators/relators in terms of $a,b,c$ , assuming it is finitely generated/presented? Assuming $\Delta'$ above exists, is $\Gamma$ realized as a torsion-free index-56 normal subgroup of $\Delta'$ ? Correspondingly, is there a finite group $G'$ of order 56 which tiles the Klein 14-gon with $2\pi/7$ triangles? If so, can $G'$ be expressed as a (normal?) index-6 subgroup of $G$ ?","['automorphism-group', 'riemann-surfaces', 'algebraic-geometry', 'hyperbolic-geometry', 'group-theory']"
4395261,"(Fake proof) Bounded linear operators $\mathcal{L}(X,Y)$ between separable Banach spaces $X$, $Y$ is itself a separable Banach space?","Fake proof: Let $X$ and $Y$ be separable Banach spaces. We know that the space $\mathcal{L}(X,Y)$ of bounded operators $X \to Y$ endowed with the operator norm is itself a Banach space. (Cf. [1] [2] [3] .) So we need to show that $\mathcal{L}(X,Y)$ is separable. It is known (cf. p.14 of Alexander S. Kechris, Classical Descriptive Set Theory ) that the closed unit ball of $\mathcal{L}(X,Y)$ with the subspace topology inherited from the strong operator topology of $\mathcal{L}(X,Y)$ is separable. (Cf. [4] [5] .) However, if the unit ball of a normed vector space is separable, then the entire vector space must be separable. (Cf. [6] [7] .) Therefore $\mathcal{L}(X,Y)$ is separable (in the strong operator topology), so "" $\mathcal{L}(X,Y)$ is a separable Banach space"". Why proof is fake (?): The main issue is that the operator norm topology on $\mathcal{L}(X,Y)$ is being conflated with the strong operator topology on $\mathcal{L}(X,Y)$ . Is that correct? The strong operator topology on $\mathcal{L}(X,Y)$ is separable but need not be (complete) metrizable. In contrast, the operator norm topology on $\mathcal{L}(X,Y)$ is (complete) metrizable but need not be separable. (Cf. [8] [9] .) Is that correct? E.g. in general the strong operator topology on $\mathcal{L}(X,Y)$ is generated by semi-norms [10] in some sense related to the operator norm, but in general is not even metrizable [11] and thus clearly not induced by the operator norm and not a Banach space. Cf. also this question [12] .","['functional-analysis', 'fake-proofs']"
4395276,Same bound on derivative of function and its inverse,"Imgine $f$ is a function (real or complex, doesn't matter) and $|f'(a)|\leq 1$ , $|(f^{-1})'(a)|\leq 1$ for a given $a$ in the domain. Does this mean that $|f'(a)|=1$ ? Why?","['functions', 'inverse', 'analysis']"
4395311,Finding the variance of a function,"In an attempt to find the variance of a function continuous in the interval $S = [a,b]$ . I have proposed a solution. But am afraid that I have something wrong over here. And before any further explanation I need to clarify that I am currently in high school year 11 so please consider my range of knowledge! without any further a do here is my attempt. For any continuous function $f(x)$ we can define $$
\tag{*}
M\{f\}(a, b) = \frac{1}{b-a} \int_{a}^{b}f(x)\, dx.
$$ From here we will use a theorem that states: $$
\tag{**}
\sigma^2 = (\frac{1}{N}\sum_{i}^N x_i^2) - x_{avg}^2.
$$ Here note that $\frac{1}{N}\sum_{i}^N x_i^2$ is just the average of the set $S$ where $S = \{x_i|i\in[1, N]\land i\in N\}$ now if we need to find the average of some function $f$ from the theorem $(*)$ we can find it. So let's plug all of that in: $$
\tag{***}
(*)(**) \implies \sigma^2\{f\}(a, b) = M\{x^2\}(a, b)-[M\{f\}(a, b)]^2 = \frac{1}{b-a} \int_{a}^{b}x^2 dx - [\frac{1}{b-a} \int_{a}^{b}f(x) dx]^2.
$$ Now when I input the function $\sigma^2\{\sin x\}(-3, 3)$ wolfram alpha outputs 3 but on the other hand I have written a code which divides an interval into pieces and computes the value of the function at each slicing point and then it computes the variance of the collected data which it claims to be about $0.523254$ here is the code: def mean(data):
    return sum(data) / len(data)

def variance(data):
    mean_flt = mean(data)
    return sum([(i ** 2) / len(data) for i in data]) - mean_flt ** 2

def split_interval(interval, section):
    step = (interval[1] - interval[0]) / section
    array = []
    i = interval[0]
    while i <= interval[-1]:
        array.append(i)
        i += step

    return array[:]

function = lambda x : math.sin(x)
dataset = [function(i) for i in split_interval([-3, 3], 100)]
var = variance(dataset)

print(""var-rep-100 : "", var) please help me see my mistakes whether it is in the code or it is in the math! Thank you in advance!","['integration', 'statistics', 'python', 'definite-integrals', 'calculus']"
4395335,"Why isn't the set V, as defined in the question body, a vector space?","Let $V$ denote the set of ordered pairs of real numbers and define our operations as follows: For $(a_1, a_2)$ , $(b_1, b_2)$ $\in$ $V$ and $c \in R$ , $(a_1, a_2) + (b_1, b_2) = (a_1 + b_1, a_2b_2)$ and $c(a_1, a_2) = (ca_1, a_2)$ Now I've determined that this isn't a vector space, but I thought it was only because it fails one of the distributive rules, namely $(a+b)x \neq ax + bx$ for $x \in V$ and $a,b \in R$ . However, I'm told it also fails the additive inverse rule: $\forall x \in V$ , $\exists y \in V$ such that $x + y = 0$ . The potential $0$ vector in this set would be $(0, 1)$ , since $\forall x \in V$ , $x + (0, 1) = x$ . But now why can't we simply define an additive inverse as follows: $\forall x \in V$ , define $y = (-x_1, \frac{1}{x_2})$ $\Rightarrow x + y = (x_1, x_2) + (-x_1, \frac{1}{x_2}) = (x_1 + (-x_1), x_2(\frac{1}{x_2})) = (0, 1) = 0$ Are we not allowed to use division here, or something, since it's not defined as an operation in a simple vector space like this? But it is certainly defined on $R$ , which is the field we are assuming for this potential vector space, right? What am I missing?","['elementary-set-theory', 'linear-algebra', 'vector-spaces', 'inverse']"
4395365,existence of a bounded $C^\infty$ function,"Prove that there exists a bounded $C^\infty$ function (i.e. infinitely differentiable) $f:\mathbb{R}\to\mathbb{R}$ so that $\lim\limits_{n\to\infty} f^{(n)}(0) = \infty$ . The function $g:(-1,1)\to \mathbb{R}$ given by $g(x)=1/(1-x)$ has derivatives satisfying $g^{(n)}(0) = n!$ , so we can let $f(x) = g(x)h(x)$ for $|x| \leq \frac{1}2$ and $f(x) = 0$ for $|x| \ge 1/2$ , provided we can find a function $h : \mathbb{R}\to [0,1]$ that is in $C^\infty$ with $h(x) = 1$ for $|x|\leq \frac{1}4$ and $h(x) = 0$ for $|x|\ge \frac{1}2$ . Then $f'(0) = g'(0)h(0) + g(0) h'(0) = g'(0) = 0!$ , and one can show by induction that $f^{(n)}(x) = \displaystyle\sum_{i=0}^n {n\choose i} g^{(n-i)}(x)h^{(i)}(x),$ from which it follows that $f^{(n)}(0) = g(0) = n!$ for each x. How do I find $h$ ? I tried using $\arctan$ but I can't even get a differentiable function (at least on all of $\mathbb{R}$ satisfying the constraints.","['limits', 'calculus', 'derivatives', 'real-analysis']"
4395375,Minimal assumption on points of discontinuity of $f$ so that $\liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1$,"Suppose we are given a function $f(x)$ . We want to show the following claim: \begin{align}
\liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1,
\end{align} almost everywhere $(x,t)$ (in Lebesgue measure) . Question: What is a minimal assumption on the points of discontinuity of $f$ that will guarantee this to be true? For example, is it sufficient to assume that the set of points of discontinuity has Lebesgue measure zero? But then there are sets that are dense and have measure zero. I am somewhat confused about this.","['limits', 'continuity', 'limsup-and-liminf', 'real-analysis']"
4395422,Trying to understand the differences between $\mathbb{Z}_2 * \mathbb{Z}_2$ vs $\mathbb{Z}_2 \times \mathbb{Z}_2$,"I’m trying to understand the differences between free products and direct products with an example: $\mathbb{Z}_2 * \mathbb{Z}_2$ vs $\mathbb{Z}_2 \times \mathbb{Z}_2$ . If I understand correctly, the latter should be finite whereas the former is not, is that correct?","['direct-product', 'group-theory', 'free-product']"
4395429,Prove that LM is parallel to AB,"the question is the following: The problem is equivalent to show that $M$ is a midpoint of arc $\widehat{AB}$ . I made a diagram on geogebra (points $C$ and $P$ got confused, sorry): It is not hard to prove $AMYX$ is cyclic. I was hoping to find some congruent triangles that would help, but it didn't quite work: $AM=MB$ and $MY=MT$ so I put my hope of finding congruence there. I didn't want to work with the second meeting of $PB$ and the left circle, because it seems unnecessary. Got a lot of stuff going on here: $C$ is the circumcenter of $\triangle MTY$ , $AT \perp TX$ EDIT: we got a Protassov configuration! I think the incenter of $\triangle ABC$ (in the top image, $\triangle ABP$ in the bottom) has a nice alignment with $T$ and a midpoint of arc $AB$","['euclidean-geometry', 'homothety', 'circles', 'geometry']"
4395440,"Calculate $\gcd(2a+4b, 2a +8b)$, if $a\equiv b \pmod{\! 5},\ 6a+11b = 5$","Given: $\ \ a$ is even $\ \ \color{#90f}{6a+11b=5}$ $\ \ \color{#0af}{a\equiv b\pmod{\! 5}}$ Q: Calculate $\gcd(2a+4b,2a+8b)$ My try: We know there is some $i$ such that $a=2i$ , plus from 3 we know there is some $j$ such that: $a-b=5j$ which means $b=a-5j=2i-5j$ . From 2, we get: $34i-55j=5$ So, $$\gcd(2a+4b,2a+8b)=\gcd(12i-20j,20i-40j).$$ I'm stuck here, how to continue?","['elementary-number-theory', 'gcd-and-lcm', 'discrete-mathematics']"
4395589,Definition of setting the integration constant equal to zero.,"In Calculus books, we often hear of setting the integration constant equal to zero. But what does that actually mean? For example, the set of antiderivatives of $2x$ is $x^2 + C$ . If we set the constant equal to zero, we get simply $x^2$ . However, I can also write the set of antiderivatives as $x^2 + \pi + C$ . In that case, setting the constant equal to zero would give us $x^2 + \pi$ . So, my question is, what does setting the constant equal to zero in an antiderivative actually mean?","['indefinite-integrals', 'calculus']"
4395593,Correlation of two random variables as functions,"Consider 2 random variables $X_1$ and $X_2$ : $X_1 = Y_1 \sqrt{\alpha + \beta {X_0}^2}$ $X_2 = Y_2 \sqrt{\alpha + \beta {X_1}^2}$ $ \alpha>0 $ , $\beta \ge 0$ where $Y_1$ and $Y_2$ are independent and both follow a normal standard distribution $N (0, 1)$ . Also, $Y_1$ is independent of $X_0$ and $Y_2$ is independent of $X_1$ . Find the correlation between $X_1$ and $X_2$ . Based off these informations, I know that $X_1 | X_0 = x_0$ ∼ $N(0, \alpha + \beta x_0^2)$ and $X_2 | X_1 = x_1$ ∼ $N(0, \alpha + \beta x_1^2)$ . Also, the equation for $X_2$ tells me that $X_1$ and $X_2$ are positively correlated. I thought of applying this formula: $Cov(X_1,X_2) = E[Cov(X_1,X_2|Z)] + Cov[E(X_1|Z),E(X_2|Z)]$ but I tried using $Z=X_0$ and $Z=X_1$ it seems like it doesn't work. Am I choosing the wrong formula?","['conditional-probability', 'statistics', 'covariance', 'probability']"
4395766,Given diagonals in a parallelogram. Find sides.,"Given a parallelogram with $d_1 = AC = 26$ cm, $d_2 = BD = 18$ cm and $\sin \displaystyle \angle AOD = \frac{12}{13}$ . Find $AB = a$ and $AD = b$ . What I did in order to solve it Using the formula for the area $S = \frac{d_1d_2\sin \displaystyle \Phi}{2} = \frac{26 \times 18 \times 12/13}{2} = 216$ Using Heron's formula for $\triangle BOC$ to find $b$ Defining $OC = a = 26 / 2 = 13$ , $OB = b = 18 / 2 = 9$ , $BC = c$ $p = \frac{a + b +c}{2} = \frac{13 + 9 + c}{2} = \frac{22 + c}{2}$ $S = \sqrt{p(p-a)(p-b)(p-c)}$ I believe when I find $b$ , I will able to find $a$ too, but kinda stuck at that point.",['geometry']
4395801,"For Ax=B, where all variables are matrixes, If A is constant, and B'=1.05B, does x'=1.05x for Ax'=B'?","I feel like the answer to this is is quite a simple yes, but I was asked to prove it on a MATLAB script by solving for x on matrices of increasing sizes, and to my surprise, it was never exactly a 5% difference, and it seems the discrepancy grows as the number of elements in the array increases.
What's going on here? Is it some sort of computing estimation quirk or is my understanding incorrect? Taking the question further, I also tested if all elements in B were given a maximum of a 5% error, multiplying them by random values between 0.95 and 1.05. My thought was that the error in x would be bounded by 5% as well, because a sort of weighted average of every error in B would be bounded between 1.05 and 0.95, but the error in this test grew exponentially with the size of the matrix, much faster than the previous test.
Any explanation about what's going on here would be very appreciated.","['systems-of-equations', 'matlab', 'matrices', 'linear-algebra', 'error-propagation']"
4395809,What is $\frac{\binom{n}{1}}{1} + \frac{\binom{n}{2}}{2} + \frac{\binom{n}{3}}{3} + \cdots + \frac{\binom{n}{n}}{n}$,"Good Day Today, I learnt that $$\frac{\binom{n}{0}}{1} + \frac{\binom{n}{1}}{2} + \frac{\binom{n}{2}}{3} + \cdots + \frac{\binom{n}{n}}{n + 1} = \frac{2 ^ {n + 1} - 1}{n + 1}$$ I changed it a little and tried to find out $$\frac{\binom{n}{1}}{1} + \frac{\binom{n}{2}}{2} + \frac{\binom{n}{3}}{3} + \cdots + \frac{\binom{n}{n}}{n}$$ Going with a similar approach, I figured out the expression is $$n \cdot {(\frac{\binom{n - 1}{0}}{1} + \frac{\binom{n - 1}{1}}{4}} + \frac{\binom{n - 1}{2}}{9} + \cdots \frac{\binom{n - 1}{n - 1}}{n ^ 2})$$ Similarly, then I integrated $$(1 + x) ^ {n - 1} = \binom{n - 1}{0} + \binom{n - 1}{1}x + \cdots + \binom{n - 1}{n - 1}x ^ {n - 1}$$ from $0$ to $x$ . Divide both sides by $x$ . Integrate from $0$ to $x$ again. Substitute $x = 1$ But, in the final step, I got stuck on $$\int_{0}^{x}{\frac{(1 + x) ^ {n} - 1}{xn}}dx$$ Putting in WolframAlpha gives a very non-elementary answer. There are two scenarios. I am wrong in these calculations. This expression does not have a simple value. I'd appreciate if somebody could help me figure this out. Sorry if I am making a silly mistake, I don't know calculus very well. Thanks.","['calculus', 'binomial-coefficients', 'combinatorics', 'sequences-and-series', 'binomial-theorem']"
4395830,Check my example: does a function satisfying properties $1-4$ exist?,"As an exercise, I need to find a $C^2$ function $f:\mathbb{R}^*\to\mathbb{R}$ such that $\eta>0$ exists such that $$ 1.\quad |f^{\prime}(x)|\le \eta\quad \mbox{ as } x\to +\infty;$$ $$ 2.\quad f^{\prime}(x)\ge \frac{1}{x^2}\quad \mbox{ as } x\to 0;$$ $$3. \quad a\in\mathbb{R}^* \mbox{ exists such that } f^{\prime}(a)=a \mbox{ (i.e. $f^{\prime}(a)$ has a fixed point)};$$ $$4.\quad f^{\prime\prime} (a)<-1. $$ I am thinking about the function $f(x) =-\frac{1}{x}$ ; in fact $\lim_{x\to+\infty} f(x)=0$ and $2.$ is satisfied with the equality. Moreover, $f^{\prime}$ has the fixed point $a=1$ and $f^{\prime\prime}(x)=\frac{-2}{x^3}$ so that $4.$ holds true. Could someone please tell me if my computations hold true? If possible, could someone please give me another (or more than another one) example of function satisfying these assumptions. Thank you in advance!","['calculus', 'derivatives', 'fixed-points', 'real-analysis']"
4395868,"Relation between the homotopy classes of map $S^n \to S^2$ and $T^n \to S^2$ for $n=2,3$","I wonder if there is any relation between the homotopy classes of map $S^n \to S^2$ , and the homotopy classes of map $T^n \to S^2$ , where $n=2,3$ . In particular, I want to understand the below quotes from the physics paper https://arxiv.org/abs/1407.3427 : The classification of the map $T^2 \to S^2$ is equivalent to the
homotopy group $\pi_2(S^2)$ classifying the map $S^2\to S^2$ , because the nontrivial cycles on the torus do not affect the homotopy classes
of the map. It is well known that the winding number of this map is
the same as the first Chern number. However, there is a subtle difference between the map $T^3 \to S^2$ and the map $S^3 \to S^2$ . The maps from $T^3\to S^2$ is more
complicated than the maps $S^3\to S^2$ , because there are 3
independent 2D torus inside $T^3$ and each of them may has nonzero
Chern numbers. For $\mu = x,y,z$ , let $C_\mu$ be the Chern number when
taking $k_\mu$ to be constant. It is pointed out by Pontryagin that if $C_\mu \neq 0$ , then the Hopf index, defined as $$\mathcal H =\frac{1}{32\pi^2} \int_{T^3} A \wedge dA,$$ (I will not quote the
definition of $A$ ) is no longer integer-valued and take values in the
finite group $\mathbb Z_{2\times \mathrm{gcd}(C_x, C_y, C_z)}$ . Could you explain why the bold part holds? Also, (mathematical) references on this topic is appreciated. I have basic knowledge of algebraic topology and differential geometry.","['physics', 'reference-request', 'algebraic-topology', 'differential-geometry']"
4395888,Is this a viable technique for integration that I should persue exploring? Thoughts?,"I started with the integral of: $$\int \frac{\ln(x)}{x}dx = \frac{\ln^2(x)}{2} + C$$ Which was very easy to integrate.
Then, I moved to a more difficult problem: $$\int \frac{\ln(x-t)}{x} dx$$ I started by letting $t$ tend to some arbitrary, zero-like variable $O_t$ .
Where some $a_0 - O_t = a_0$ , but $O_t$ is not technically equal to zero. In other words, expressions containing $O_t$ cannot be simplified and compacted, but it behaves like zero when the cyclical nature of the integral pops up. Using it, the DI method yields: $$\int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x)}{x-O_t}dx$$ Since this $O_t$ is technically equal to zero, then by definition: $$\int \frac{\ln(x-O_t)}{x}dx = \int \frac{\ln(x)}{x-O_t}dx$$ Plugging this is, you find that: $$\int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x-O_t)}{x}dx$$ You can add the integral to both sides, and divide by two: $$\int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-O_t)\ln(x)}{2}+C$$ Then, in order to complete the problem, you can let $O_t$ re-approach $t$ : $$\int \frac{\ln(x-t)}{x}dx = \lim\int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-t)\ln(x)}{2} + C$$ In order to test this, you can plug in $t=0$ to see this conclusion in action: $$\int \frac{\ln(x-0)}{x} dx = \frac{\ln(x-0)\ln(x)}{2} + C = \frac{\ln^2(x)}{2} + C$$ Which indeed is correct.
Is this new ""zero-substitution"" technique for integration viable? Or does it have major gap holes. Should I pursue it and apply it to other integrals? Thank you for your help.","['integration', 'calculus', 'analysis', 'substitution']"
4395901,Integrating factor of $x^2ydx-(x^3+y^3)dy=0$,"$x^2ydx-(x^3+y^3)dy=0$ I have to find an integrating factor $\mu$ . Denote $P=x^2y,Q=-(x^3+y^3)$ The correct answer for getting an integrating factor is $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy}$ or $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy}$ ? In my post Solve $ye^ydx+(1+xe^y)dy=0$ , I got a comment that the correct answer is $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{P}}dy}.$ Then , $\int\frac{x^2+3x^2}{x^2y}dy=4ln|y| \implies \mu=e^{ln|y|}=y^4$ , But the correct integrating factor is $y^{-4}$ . How is it possible ? maybe $\mu=e^{\int\frac{P_y-Q_x}{\color{red}{-P}}dy}$ ? Thanks !",['ordinary-differential-equations']
4395920,Solve the ODE $y y''=3(y')^2$ using Reduction of Order,"For the reduction of order method, we are supposed to guess a solution, $y_1$ , and assume that the second solution (this will be second order as the title shows) is of the form $y_2=uy_1$ .  But I'm having difficulty finding a solution by inspection.  I've tried the following, which seemed legit at first, only to find that the constant 3 is making the problem more difficult. $$y=e^3x \quad y=e^{\sqrt{3}x} \quad y=ax^2 \quad y=\sqrt{x}$$ Previously the problems given, it was pretty straight forward but this one is really hurting as I can't find a solution by inspection.  It could be that we can maybe use some ""product rule"" manipulation, since $$y y''=3(y')^2 \quad \rightarrow \quad y y''-3(y')^2=0$$ looks like it could be of the form $(y y')'$ , but again, that 3 is messing me up.  Once I find it, I want to try and find the other solution using Reduction of Order.  So for anyone answering, please don't solve the ODE, just how I can maybe posit a guess given the information provided in the problem.","['reduction-of-order-ode', 'ordinary-differential-equations']"
4395924,Space of probability measures with total variation norm is not separable,"Let $S$ be the space of probability measures on $[0,1]$ and equip it with the distance induced by the total variation norm: for $\mu,\nu\in S$ : $$\|\mu-\nu\|= \sup_{P\in \mathcal P} \sum_{A\in P}|\mu(A)-\nu(A)|$$ where $\mathcal P$ is the space of all finite partitions of $[0,1]$ , e.g. $P=\{[0,\frac{1}{3}], [\frac{1}{3}, \frac{5}{8}],[\frac{5}{8}, 1]\}$ . We can show that $S$ is not separable: if the probability measures may have atoms, then the space $D$ of all Dirac masses each  concentrated on a number in $[0,1]$ is such that all probability measures are at distance $2$ from each other (pairwise) and are uncountable. Hence, $S$ is not separable. What about the space of all atomless probability measures $S_{\text{AL}}\subset S$ ? Can we make this uncountable? Intuition: if we ""make"" the Dirac masses continuous by e.g. putting a very thin Gaussian bell (with a small standard deviation) around each $a\in\mathbb R$ , these Gaussian bells will not be at a fixed distance from each other anymore. Someone else’s suggestion: consider the Z-order curve $\zeta(\cdot)$ , apparently this is a  continuous measurable map which is one-to-one from a line to the square. As a result, you can map the Dirac masses continuously to the square inducing a probability distribution that is measurable and continuous. This would give us an uncountable set. I do not understand this line of reasoning, however","['measure-theory', 'metric-spaces', 'real-analysis', 'general-topology', 'probability-theory']"
4395949,"$f(x)$ and $g(x)$ are monic cubic polynomials, with $f(x)-g(x)=r$. If $f$ has roots $r+1$ and $r+7$, and $g$ has roots $r+3$ and $r+9$, then find $r$.","Let $f(x)$ and $g(x)$ be two monic cubic polynomials, and let $r$ be a real number. Two of the roots of $f(x)$ are $r+1$ and $r+7$ . Two of the roots of $g(x)$ are $r + 3$ and $r + 9,$ and $$f(x) - g(x) = r$$ for all real numbers $x.$ Find $r.$ So far, I have $$f(x)=(x-r-1)(x-r-7)(x-p)$$ and $$g(x)=(x-r-3)(x-r-9)(x-q).$$ From $f(x)-g(x)=r$ , I know that their constant terms differ by $r$ . I expanded the two functions but it was too complicated. I also plugged in $x=r+1,r+7,r+3,r+9$ into $f(x)-g(x)=r$ , but it didn't do much. Thanks in advance!!!!!","['cubics', 'roots', 'functions', 'polynomials', 'algebra-precalculus']"
4395997,Definition of bipartite graph from Murty-Bondy book,"I was reading reading the definition of bipartite graph and one moment is confusing me.
Due to this definition we can consider any graph $G=(V,E)$ as a bipartite, if we take $X=\varnothing$ and $Y=V$ . However, I am sure that my reasoning is false. Am I missing something here?","['graph-theory', 'definition', 'combinatorics', 'discrete-mathematics', 'bipartite-graphs']"
4396003,Closed form of $\sum_{i=0}^{\infty} x^i (i!(i+1)!)^{-1/2}$,"I am looking for a closed form of this series $$
\sum\limits_{i=0}^{\infty} \frac{x^i}{\sqrt{i!(i+1)!}},\:x \geq 0.
$$ The main problem is that this series contains roots of factorials. But many special functions just contain factorials, not their roots. Is it possible to express the sum of this series in some well-known (or little known) functions?","['power-series', 'closed-form', 'sequences-and-series']"
4396007,Characterising functions $\mathbb{F}_2^n \to \mathbb{F}_2$ satisfying special equations,"Let $\mathbb{F}_2=\{0,1\}$ be the field with two elements, and let $u:\mathbb{F}_2^n\rightarrow \mathbb{F}_2$ be a function. Is it possible that $$
\sum_{x \in \mathbb{F}_2^n}(-1)^{u(x)+u(x+a)}= 0,
$$ for every $a \neq 0$ in $\mathbb{F}_2^n$ ? If so, can we characterise the functions satisfying property? Comment: I treat the sum here as natural (or real) number, not as an element of $\mathbb{F}_2$ . This question arose in the context of this question . (Trying to derive a lower bound on the approximate multiplicativity of a Boolean function). If $u$ is linear, i.e. $u(x+y)=u(x)+u(y)$ , then the sum above is never zero. Edit: Jyrki Lahtonen gave a nice example for all even $n$ . It turns out that there are no such functions for odd $n$ .","['boolean-algebra', 'finite-fields', 'combinatorics', 'discrete-mathematics', 'computer-science']"
4396019,Definition of orientation on manifolds and global continuous frame,"In Tu and Lee books on smooth manifolds the definition of orientation on a manifolds $M$ is given by We first assign a pointwise orientation : for every $p \in M$ we choose a class of oriented basis of $T_pM$ .
Next we say that this pointwise orientation is continuous if for every $p \in M$ there is a local frame on neighbourhood $U$ of $p$ that is positively oriented at each point of $U$ .
So an orientation of $M$ is a continuous pointwise orientation . My question is why don't we use a global frame on $M$ to define orientation ? Is there example of manifolds that doesn't admit global frame but are orientable ? Is there a like with parallelizable manifolds ?","['smooth-manifolds', 'definition', 'orientation', 'manifolds', 'differential-geometry']"
4396020,"What does the Outer automorphism group act on? If an automorphism preserves conjugacy classes, is it inner?","(Let $x^G:=\{gxg^{-1}:g\in G\}$ denote the conjugacy class of $x$ ) Am I right in thinking $\text{Out }G := \text{Aut }G/\text{Inn }G$ acts on the conjugacy classes of G, $\{x^G:x\in G\}$ ? What else could I think of it acting on (trying to get some intuition)? In addition to that, if $f\in \text{Aut }G$ $$f\in\text{Inn }G\iff(\forall x\in G)f(x^G) = x^G$$ I feel this must be true, but couldn't find anything on it. If it is true, can we find a weaker condition on the right hand side to check if $f\in\text{Inn }G$ ? eg. we only require $f(x^G) = x^G$ for $x$ in a set of generators for this to be true.","['automorphism-group', 'group-theory', 'group-actions']"
4396033,A question on invariant Borel measurable partitions,"Imagine $X=[0,1)$ and $T:X \to X$ . Let $\mu$ be any $T$ -invariant probability measure on $X$ . Let $A_1, \cdots, A_m$ is a T-invariant Borel measurable partition of $X$ . Set for $0<\epsilon<\frac{1}{2}$ : \begin{align}
&B_i=\{J \in \mathcal{P}: \mu(A_i \cap J)>(1-\epsilon)\mu(J)\} \\
&B_0=X \setminus \bigcup_{i=1}^mB_i
\end{align} Where $\mathcal{P}$ is the set of atoms.
Then $B_0, B_1, \cdots, B_m$ is a measurable partition of $X$ . By elementary measure theory, \begin{align}
&\mu(A_i \Delta B_i)< \epsilon \\
&\mu(B_0)<m.\epsilon
\end{align} I couldn't see the reason of the above relations. could anyone help?","['measure-theory', 'real-analysis']"
4396035,Interchanging a limit and a parametric improper integral,"Suppose I have the limit: $$\lim _{t\to 0^+}\int _0^{\sqrt{\sqrt{t}+4}}\sqrt{1+\frac{t}{4\sqrt{x}}}\:dx$$ How can I prove that the limit is $2$ ? It is easy to prove that for all $t > 0$ the improper inegral converges by using the limit comparison test (the problem is at x = 0). I also see that the upper bound approaches $2$ . If I try to write the improper integral as a limit of a proper integral I get: $$\lim _{t\to 0^+}\:\left(\lim _{a\to 0^+}\int _a^{\sqrt{\sqrt{t}+4}}\sqrt{1+\frac{t}{4\sqrt{x}}}\:dx\right)$$ However, I don't know how to deal with an iterated limit. When can I switch the order? Is it even the proper way to tackle the limit? Note that I haven't learned measure theory so I would prefer a solution which doesn't require it.","['improper-integrals', 'limits', 'definite-integrals', 'leibniz-integral-rule']"
4396046,"Find the number of all sequences $\{ a_{n}\}$ in $\{-5,-4,-3,...,0,1,...,100\}$ such that $ |a_{n}| < |a_{n+1}|$.","Find the number of all sequences $\{ a_{n}\}$ in $\{-5,-4,-3,...,0,1,...,100\}$ such that $ |a_{n}| < |a_{n+1}|$ . I think we can find a unique sequence for every non empty subset of $\{0,1,\ldots ,100\}$ so we have at least $2^{101}-1$ .","['combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4396058,Can the limit of a complex number have multiple values?,"I have been studying ""Complex analysis"" for a while. In this ""limit of complex function"" question, I get an answer that's a $4$ th root of $-1.$ I change  that form to polar coordinate form and then see that there's more than one value. This is the work that I have done so far: So, Can the limit of a complex function has more than one value? If not, do I need to change an answer that's a fractional exponent to polar form?","['complex-analysis', 'limits']"
4396070,How close is the median of a uniformly chosen subsample to the median of the full set?,"Take $N$ a set of n numbers, sample s numbers from $N$ uniformly and with replacement giving us the set $S$ . What is the relationship between the median of $N$ and the median of $S$ ? I want a result that looks like: With probability ... the median of $S$ has a rank in $N$ in $\left[\left(\frac12 - d\right)n, \left(\frac12 + d\right) n\right]$ . I am actually interested in a proof of Lemma 13 of https://dl.acm.org/doi/pdf/10.1145/3409964.3461790 (which comes without a proof) Thanks :)","['median', 'sampling', 'discrete-mathematics', 'algorithms', 'probability']"
4396120,Compute $\int_{-\infty}^{+\infty}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x$,"I would like to compute the following integral: $$ \int_{-\infty}^{+\infty}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x\quad,\quad a\in\mathbb{R}$$ Where I extend the sqrt function to complex number such that $\sqrt{a^2-x^2} = i\sqrt{x^2-a^2}$ when $|x|>|a|$ . So I can split the integral in three: $$ \int_{-\infty}^{+\infty}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x =  \int_{-\infty}^{-a}\frac{e^{-\sqrt{x^2-a^2}}}{i\sqrt{x^2-a^2}}\mathrm{d}x + \int_{a}^{+\infty}\frac{e^{-\sqrt{x^2-a^2}}}{i\sqrt{x^2-a^2}}\mathrm{d}x + \int_{-a}^{a}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x$$ And then by parity: $$ \int_{-\infty}^{+\infty}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x =  -2i \int_{a}^{+\infty}\frac{e^{-\sqrt{x^2-a^2}}}{\sqrt{x^2-a^2}}\mathrm{d}x + 2\int_{0}^{a}\frac{e^{i\sqrt{a^2-x^2}}}{\sqrt{a^2-x^2}}\mathrm{d}x$$ These two integrals are convergent, for $x\rightarrow +\infty$ it is obvious, for $x\rightarrow a$ we have: $$\frac{1}{\sqrt{x^2-a^2}}=\frac{1}{\sqrt{(x-a)(x+a)}}\sim_{a}\frac{1}{\sqrt{2a(x-a)}} $$ and $1/\sqrt{u}$ is convergent as $u\rightarrow 0$ .
I tried to compute them but failed, what I tried: Complex analysis, but because of the sqrt I'm not sure on how to handle it, if ou have any document that explain how to compute this kind of integral, I'll be happy to see it. I tried to introduce a new variable $t$ and see the integral as: $$ \int_{a}^{+\infty}\frac{e^{-\sqrt{x^2-a^2}}}{\sqrt{x^2-a^2}}\mathrm{d}x = \int_{a}^{+\infty}\int_{+\infty}^{1}\frac{e^{-t\sqrt{x^2-a^2}}}{\sqrt{x^2-a^2}}\mathrm{d}t\mathrm{d}x$$ and then using substitution $u^2=x^2-a^2$ , then I ended up with: $$ \int_{0}^{+\infty}\int_{+\infty}^{1}\frac{e^{-tu}}{\sqrt{u^2+a^2}}\mathrm{d}\mathrm{d}x $$ , I gave the $\mathrm{d}t$ integral to sympy that returned me an expression dependant of Meijer function taht I wasn't able to integrate ... If you have any idea, let me know.
Thank you.","['integration', 'improper-integrals', 'special-functions', 'complex-analysis', 'indefinite-integrals']"
4396134,Show that $x = \sqrt{ab}$ in the figure below,"For reference: If $ABCD$ is a square and $BC$ is a diameter. Show that $x = \sqrt{ab}$ . Progress:
Let $h$ = height $\triangle TPQ$ $$\triangle TPQ \sim \triangle TAD$$ $$
\frac{h+a+b+x}{h} = \frac {a+b+x}{x} \iff \frac{a+b + x}{h} = \frac{a+b}x$$ ....?","['euclidean-geometry', 'geometry', 'plane-geometry']"
4396151,Fibonacci Like Sequence,"I'm working on this problem but I'm not sure how to solve it. Consider a sequence of positive integers $1, a_2, a_3, \dots , a_k=55$ where each term in the sequence is the sum of any two previous (not necessarily distinct) terms. What is the smallest value of $k$ such that the sequence exists? Just for clarification, a possible sequence would be $1, 2, 4, 8, 16, 32, 48, 52, 54, a_{10}=55$ , where the second through sixth terms are created by adding the previous term twice, the seventh term is created by adding 16 to the previous term, the eighth term is created by adding 4 to the previous term, the ninth term is created by adding 2 to the eighth term, and 55 is created by adding 1 to the ninth term. This sequence would have $k=10$ . I found a possible sequence for $k=9$ , which is $1, 2, 4, 6, 12, 24, 48, 54, 55$ , and I'm pretty sure I can't find one for $k=8$ . However, I'm not sure how to prove that $k=9$ is the lowest I can go. Thanks for any help on this problem.","['elementary-number-theory', 'optimization', 'sequences-and-series']"
4396181,$L_q$ is always dense in $L_p$?,"From Theorem 3.13 on page 69 of Rudin's real and complex analysis, I learned that the set $S$ of all simple functions with finite support is dense in $L_p$ for $1 \leq p<\infty$ . Obviously $S\subset L_q$ for all $q>0$ . Then it seems obvious to conclude that $L_q \cap L_p$ is dense in $L_p$ simply because S is dense in $L_p$ and $S \subset L_q \cap L_p$ . I think the argument is quite simple here, but I couldn't find any resource to confirm this claim. Am I missing something in my argument?","['lp-spaces', 'functional-analysis', 'analysis']"
4396182,Is it possible to find the $n$-derivative of $\csc(m\pi)?$,"I am trying to find the $n$ -th derivative of $\csc(m\pi)$ , so I took few cases: for simplicity let $x=\cot(m\pi)$ and $y=\csc(m\pi)$ , $$\frac{d^0}{dm^0}\csc(m\pi)=\pi^0(\color{red}{1}x^0y^1)$$ $$\frac{d^1}{dm^1}\csc(m\pi)=-\pi^1 (\color{red}{1}x^1y^1)$$ $$\frac{d^2}{dm^2}\csc(m\pi)=\pi^2(\color{red}{1}x^2y^1+\color{red}{1}x^0y^3)$$ $$\frac{d^3}{dm^3}\csc(m\pi)=-\pi^3(\color{red}{1}x^3y^1+\color{red}{5}x^1y^3)$$ $$\frac{d^4}{dm^4}\csc(m\pi)=\pi^4(\color{red}{1}x^4y^1+\color{red}{18}x^2y^3+\color{red}{5}x^0y^5)$$ $$\frac{d^5}{dm^5}\csc(m\pi)=-\pi^5(\color{red}{1}x^5y^1+\color{red}{58}x^3y^3+\color{red}{61}x^1y^5)$$ $$\frac{d^6}{dm^6}\csc(m\pi)=\pi^6(\color{red}{1}x^6y^1+\color{red}{179}x^4y^3+\color{red}{479}x^2y^5+\color{red}{61}x^0y^7)$$ and saw that \begin{align}
\frac{d^n}{dm^n}\csc(m\pi)&=(-\pi)^n\sum_{k=0}^{\lfloor{n/2}\rfloor}\color{red}{a_k} x^{n-2k}
y^{2k+1}\\
&=(-\pi)^n\sum_{k=0}^{\lfloor{n/2}\rfloor}\color{red}{a_k} \cot^{n-2k}(m\pi)\csc^{2k+1}(m\pi)\\
&=(-\pi)^n\csc^{n+1}(m\pi)\sum_{k=0}^{\lfloor{n/2}\rfloor}\color{red}{a_k} \cos^{n-2k}(m\pi)
\end{align} If we replace $n$ by $2n$ then separate the last term we have $$\frac{d^{2n}}{dm^{2n}}\csc(m\pi)=\pi^{2n}\csc^{2n+1}(m\pi)\left[a_n+\sum_{k=0}^{n-1}\color{red}{a_k} \cos^{2n-2k}(m\pi)\right]$$ In the cases mentioned above, we notice that when the order of the derivative is $0, 2, 4, 6$ , the coefficients of the last terms are $1, 1, 5, 61$ which match the absolute value of the Euler numbers : $$E_0=1, E_2=-1, E_4=5, E_6=-61$$ and so $$\frac{d^{2n}}{dm^{2n}}\csc(m\pi)=\pi^{2n}\csc^{2n+1}(m\pi)\left[|E_{2n}|+\sum_{k=0}^{n-1}\color{red}{a_k} \cos^{2n-2k}(m\pi)\right]$$ By the way, if we take the limit to both sides of the last result letting $m$ approach $1/2$ , we have $$\lim_{m\to \frac12}\frac{d^{2n}}{dm^{2n}}\csc(m\pi)=\pi^{2n}(1)\left[|E_{2n}|+0\right]=|E_{2n}|\pi^{2n}.$$ Question : Is it possible to find $\color{red}{a_k}$ ?","['trigonometry', 'derivatives', 'sequences-and-series']"
4396191,"To find the number of $10$ digit numbers, where the sum of digits is divisible by $10$.","To find the number of $10$ digit numbers, where the sum of digits is divisible by $10$ . The sum of digits is divisible by $10$ implies the sum has to be a multiple of $10$ . Since the number is $10$ digited, the first term has to take values greater than $0$ . Some of the numbers are all ones, $11\cdots11, 12\cdots10, 13\cdots100$ . I am finding it difficult to count the number of such numbers.","['algebra-precalculus', 'combinatorics', 'discrete-mathematics']"
4396218,How do you go about Gaussian elimination modulo p?,"The problem at hand is specifically to find the inverse of a matrix using Gaussian elimination modulo 29. I am familiar with the process of regular Gaussian elimination and modular arithmetic but not the combination of the two. The matrix you are given is: $$A=\begin{pmatrix}
12 & 3 & 23 \\
28 & 1 & 2 \\
11 & 0 & 7
\end{pmatrix}$$ I first start out the way I would when doing a normal Gaussian elimination to find the inverse by writing out the following matrix: $$A=\begin{pmatrix}
12 & 3 & 23 & 1 & 0 & 0 \\
28 & 1 & 2 & 0 & 1 & 0\\
11 & 0 & 7 & 0 & 0 & 1
\end{pmatrix}$$ From here, I first tried solving it how I would normally and then taking the elements modulo 29 at the end but unfortunately you end up with fractions at the end. After that I've tried working with only integers and tried to reduce numbers using modulo 29 as I go but haven't found success and I'm not sure if that is how you're supposed to do it. My question more generally would be how you go about Gaussian elimination modulo p for any prime number. Thanks in advance!","['matrices', 'gaussian-elimination', 'modular-arithmetic']"
4396224,A conjecture about prime test,"Conjecture If $\varphi(m)<\varphi(n)$ for all $m<n$ ,then $n$ is a prime number. I tried to find a counterexample when $n=pq$ ( $p,q$ are prime), then we have to find a prime between $(p-1)(q-1)$ and $pq$ , but I don't know if this is an open problem.","['number-theory', 'prime-numbers']"
4396236,Show that if $\epsilon\to0$ then $(1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0)$,"I'm trying to prove that for a function $\varphi\in C_c^\infty(\mathbb{R}^n)$ and $\alpha(\epsilon)$ , the surface of the ball $B_\epsilon (0)$ \begin{align*}\underset{\epsilon\to0}{\lim} (1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0). \end{align*} At some point, it's necessary using the fact that $\varphi$ is continuous. I was thinking about rewriting $\alpha(\epsilon)$ like an integral but I don't achieve anything with it. Please, ¿can you give me some hint?","['integration', 'multivariable-calculus']"
4396238,Problem 2.4.8 rom Tao-Vu book,"For each $j=1,2,3,$ let $G_j$ be a $K_j$ -approximate group in an
ambient group $Z$ . Using the Ruzsa triangle inequality, show that $$|G_1+G_2+G_3|\leq K_2\dfrac{|G_1+G_2||G_2+G_3|}{|G_2|}.$$ Conclude
that $$d(G_1+G_2, G_1+G_2+G_3)\leq d(G_2, G_2+G_3)+\log K_1K_2.$$ Similarly for permutations. Conclude from this and the preceding
exercise that $$d(G_1,G_2)\leq d(G_1+G_3, G_2+G_3)+2\log K_1K_2K_3.$$ This is a problem 2.4.8 from Tao-Vu book and I was able to prove the first two inequalities but cannot prove the last one. The last inequality can be written equivalently as $$|G_1+G_2|^2|G_1+G_3||G_2+G_3|\leq(K_1K_2K_3)^4 |G_1+G_2+2G_3|^2|G_1||G_2|.$$ I have tried many ways to prove it but failed.
I'd be grateful for any help! EDIT (Possible counterexample to the initial inequality): We claim that the inequality $$d(G_1,G_2)\leq d(G_1+G_3, G_2+G_3)+2\log K_1K_2K_3 \Leftrightarrow$$ $$|G_1+G_2|^2|G_1+G_3||G_2+G_3|\leq(K_1K_2K_3)^4 |G_1+G_2+2G_3|^2|G_1||G_2|$$ is wrong. Let's take a look at the example which you've suggested. We consider the integer lattice $\mathbb{Z}
^2$ and let $G_1$ be an interval $[-N,N]$ on $x$ -axis of $\mathbb{Z}^2$ , $G_2$ be an interval $[-N,N]$ on $y$ -axis of $\mathbb{Z}^2$ and $G_3=[-N,N]\times [-N,N]$ on $\mathbb{Z}^2$ . We see that each of them are approximate groups with $K_1=2$ , $K_2=2$ and $K_3=4$ . Easy to see that $G_1+G_2\equiv G_3=[-N,N]\times [-N,N]$ , $G_1+G_3=[-N,N]\times [-2N,2N]$ , $G_2+G_3=[-2N,2N]\times [-N,N]$ and $G_1+G_2+2G_3=[-3N,3N]\times[-3N,3N]$ . Our inequality becomes $$(2N+1)^6(4N+1)^2\leq 16^4(2N+1)(6N+1)^4$$ and this inequality is obviously wrong because the LHS $\sim N^8$ and the RHS is $\sim N^5$ .","['additive-combinatorics', 'combinatorics', 'discrete-mathematics']"
4396260,"How to calculate the entropy of a random vector $X = (C_1,\dots,C_n,D_1,\dots,D_m)$ where $C_i$ are continuous margins and $D_i$ are discrete margins?","Let $X := (C,D) = (C_1,\dots,C_n,D_1,\dots,D_m)$ be a random vector from a mixed continuous-discrete distribution, meaning that $X$ takes values in $$X \in \mathbb{R}^n \times \mathbb{N}^m$$ with $C = (C_1,\dots,C_n) \in \mathbb{R}^n, D = (D_1,\dots,D_m) \in \mathbb{N}^m$ You can think of $X$ as a row from a generic dataset. Maybe it's storing some data about individuals, and contains some discrete values like occupation , sex , residence , country of birth , etc. and continuous values like date of birth , height , weight , salary , etc. How to calculate (or estimate) the entropy of $X$ , $H(X)$ ? If $X$ was a purely discrete random vector, then its entropy would be $$H_{\text{discrete}}(X) = -\sum_{x \in \text{Dom}(X)} \mathbb{P}(X=x) \log_2(\mathbb{P}(X=x))$$ Or if $X$ was a purely continuous random variable, then its entropy would be $$H_{\text{continuous}}(X) = -\int_{\text{Dom}(X)} f_X(x) \log_2(f_X(x))dx$$ (Where $f_X(x)$ is the p.d.f. of $X$ .) It's not as simple as ""adding up a sum and an integral"" (something like $-\sum \mathbb{P}(C) \log_2(\mathbb{P}(C)) - \int f_D(x) \log_2(f_D(x))dx$ ), because correlations between $C_i$ and $D_j$ wouldn't be taken into account. I have found the following study that deals with the case of $X \in \mathbb{R} \times \mathbb{N}$ , or the case where $n=1$ and $m=1$ . Is there a study that deals with the general case? Edit: I'm surprised this doesn't exist in the literature, because databases with such rows are extremely common. In fact it's hard to find databases that only have discrete or continuous columns.","['entropy', 'information-theory', 'probability-theory', 'random-variables']"
4396328,Geometry Question: A property of a convex polyhedron.,"I'm trying to interpret a verified solution for the following problem. Show that $v_3+f_3>0$ .
Here, $v_n$ denotes the number of vertices of a convex polyhedron that meet with $n$ edges, and $f_n$ denotes the number of faces that have $n$ edges. The solution that I'm reading uses the property that the sum of angles (between two consecutive edges) around any vertex is less than $2\pi$ , and it proceeds as follows. Let $F,V,E$ be # of faces, vertices, and edges of a convex polyhedron. And, assume that $v_3+f_3=0$ . As we already know that the sum of angles around a vertex must be less than $2\pi$ , we get a following inequality: $\sum \text{angles} < 2\pi V$ . But, $\sum \text{angles} = \sum (n-2)f_n \pi$ because the sum of angles of an $n$ -gon is $(n-2)\pi$ . i.e. $V>\sum (n-2)f_n$ . (This is where I'm having problem.) Note that $E=\frac{1}{2}\sum nf_n$ . So, $2 = V + F - E > \sum (\frac{n}{2}-1)f_n$ . It is clear to see that $v_3+f_3 \geq 0$ , but assuming $v_3+f_3=0$ gives us a contradiction because when $n \geq 4$ , $2>\sum f_n$ , which is not possible. The problematic part: $V>\sum (n-2)f_n$ First, I thought this was a typo, and I thought it was supposed to be $2V>\sum (n-2)f_n$ as we can cancel $\pi$ out on both sides of the previous inequality. However, then this inequality only gives me that $2>0$ , which doesn't imply anything. $$2V>\sum (n-2)f_n \\ \Rightarrow V>\sum(\frac{n}{2}-1)f_n \\ \Rightarrow 2=V+F-E>0$$ So, now I'm trying to observe for any weird convex polyhedron so that I can find a counterexample of $V>\sum (n-2)f_n$ , but the inequality seems right. I am wondering where did this inequality come from, and if possible, I would like to know details behind here that are probably omitted because of the obvious reasons that I'm missing here. Any help would be greatly appreciated.","['convex-geometry', 'polyhedra', 'geometry']"
4396364,How is $\cos(x) + \sin(x) = a\cos(x + b)$?,I was reading Fourier Analysis - An Introduction And it says at some point One can easily verify that there exists $A > 0$ and $\varphi \in \mathbb{R}$ such that $$a \cos ct + b \sin ct = A \cos (ct - \varphi)$$ I was confused that $\cos x + \sin x$ can be expressed by another $\cos$ because if it’s true then it looks so trivial that it should be taught in elementary trigonometry but I don’t remember it from anywhere. I even tried plotting $\cos x + \sin x$ graph and it looked awfully identical to $\cos x$ just slightly shifted and scaled. Is it true that $\cos x + \sin x$ is just another $\cos$ ?,['trigonometry']
4396395,Proving Cauchy Theorem,"This theorem says that a function $f$ is defined in the interval (a, + $\infty$ ) and it is bounded in every finite interval (a,b). Then it holds that: (assume that the limit of the RHS exists) $$\lim_{x\to \infty} \frac{f(x)}{x} = \lim_{x\to \infty}[f(x+1) - f(x)];$$ $$\lim_{x\to \infty} [f(x)]^{1/x} = \lim_{x\to \infty}\frac{f(x+1)}{f(x)}.$$ My question is how to prove the above equations. Any help will be appreciated.",['analysis']
4396439,"Is $f:[a,b] \times \Omega \to E$ measurable?","Problem: Suppose that $[a,b] \subset \mathbb{R}$ , $(\Omega, \mathcal{F})$ is a measure space and $E$ is a topological space. Suppose $f : [a,b] \times \Omega \to E$ is such that: $\forall t$ $f(t, \cdot): \Omega \to E$ is measurable. (Here the $\sigma$ -fields of $E$ is the one generated by open sets) $\forall \omega \in \Omega$ $f(\cdot, \omega) : [a,b] \to E$ is right-continuous. How can I conclude that $f$ is $\mathcal{B}([a,b])\otimes\mathcal{F} - \mathcal{B}(E)$ measurable? Attempt: I tried in the following way. Let us define $f_n : [a,b] \times \Omega \to E$ in the following way, $f_n ([\frac{k}{n}, \frac{k+1}{n}), \omega)=f(\frac{k+1}{n}, \omega)$ for all $0 \leq k \leq n-1$ . Since $f_n$ is measurable in $[\frac{k}{n}, \frac{k+1}{n}) \times \Omega$ bye the first hypothesis on $f$ we get that $f_n$ is measurable and right-continuous. Now I noticed that $f_n$ point wise converge to $f$ . In fact we have that for a fixed $(t,\omega)$ we obtain $t\in [\frac{k}{n}, \frac{k+1}{n})$ and thus $f_n(t,\omega)=f(\frac{k+1}{n},\omega) \to f(t,\omega)$ by the fact that $f$ is right-continuous. Now I proved the following lemma: Lemma: if $(X, \mathcal{X})$ is a measure space and $E$ is a metric space we have that if $f_n : X \to E$ are measurable and point wise converge to $f$ that $f$ is measurable. Proof: If $A$ is an open set then: $$f^{-1}(A)=\bigcup\limits_{n} \bigcap\limits_{k \geq n} \{ x \in X: dist(f_k(x), A^c) > \frac 1 n \}$$ is a measurable set $\Box$ . Is this correct? Does this lemma hold also for general $E$ topological space? Are there different hypotheses to make on $E$ ?","['stochastic-processes', 'measure-theory', 'measurable-functions', 'probability']"
4396529,"If $(X,Y)$, $(X',Y')$ are iid. real-valued RV, is $\mathbb{E}(X \cdot Y) = \mathbb{E}(X' \cdot Y')$?","I need a bit of help with the statement in the title that I have encountered in a proof of Hoeffding's covariance equality in my statistics course. We begin by assuming that we can generate an independent copy of $(X,Y)$ , let's call it $(X',Y')$ . An independent copy is coinciding in distribution by definition, so $\mathbb{P}(X \leq x, Y \leq y) = \mathbb{P}(X' \leq x, Y \leq y) = F_{(X,Y)}(x,y)$ . That is exactly the definition of iid random variables, if I am not mistaken. The step where I am stuck at is why $2\mathbb{E}(XY) - 2\mathbb{E}(X) \mathbb{E}(Y) = \mathbb{E}(X-X')(Y-Y')$ . I have already shown that any pair of mixed variables is independent of each other, so $\mathbb{E}(XY')= \mathbb{E}(X'Y) = \mathbb{E}(X) \mathbb{E}(Y)$ by noticing $\mathbb{P}_X = \mathbb{P}_{X'}$ and $\mathbb{P}_Y = \mathbb{P}_{Y'}$ (using continuity of measures). The only thing missing now is the questioned equality in the title. I don't know if it is true, but multiplying out the right side strongly suggests equality. I have thought about rewriting $\mathbb{P}(XY \leq x)$ for $x \in \mathbb{R}$ as a countable union in a way, so that variables can be pulled apart as seperate conditions and using the definition of the pair $(X',Y')$ , but as of now I am still in the dark on how to approach this. Have a good day!","['statistics', 'probability-theory', 'random-variables']"
4396535,"Is there a measurable function from $[0,1]$ to $ω_1$?","Does there exist a measurable function from $[0,1]$ (with the Lebesgue measure) to $ω_1$ that induces the Dieudonné measure? Definitions: $ω_1$ is the set of all countable ordinals, equipped with its Borel $σ$ -algebra. The Dieudonné measure $\nu$ assigns measure $1$ to each subset of $ω_1$ that contains an unbounded closed set. For a measurable function $f : [0,1] \to ω_1$ , the induced measure assigns measure $\mu(f^{-1}(E))$ to each measurable set $E ⊆ ω_1$ . So to restate the question, in two parts: first, is there any measurable surjection from $[0,1]$ to the set of countable ordinals? Second, is there such a function $f$ such that for each Borel set of countable ordinals $E$ that contains an unbounded closed set, $\mu(f^{1}(E)) = 1$ ? Intuitively, it doesn't seem like this should be possible. For one thing, $f$ would have to map every subset $[0,1]$ with a measure other than $0$ or $1$ to a non-Borel subset of ordinals, which seems pretty pathological. The Lebesgue measure is non-atomic and regular, while the Dieudonné measure is neither. But I don't think I know enough analysis to prove it either way.","['measure-theory', 'ordinals', 'borel-measures', 'real-analysis']"
4396588,How is the last equation derived in this equality?,"From Inequalities A Mathematical Olympiad Approach , page 122 Observe that, $$\sum_{i = 1}^n \sqrt{a_i}-(n-1)\sum_{i = 1}^n \dfrac{1}{\sqrt{a_i}} = \sum_{i = 1}^n \dfrac{1}{1+a_i}\sum \sqrt{a_i} - \sum_{i = 1}^n \dfrac{a_i}{1+a_i} \sum_{i = 1}^n \dfrac{1}{\sqrt{a_i}}\\ = \sum_{i,j} \dfrac{a_i-a_j}{(1+a_j)\sqrt{a_i}} = \sum_{i>j} \dfrac{(\sqrt{a_i}\sqrt{a_j}-1)(\sqrt{a_i}-\sqrt{a_j})^2(\sqrt{a_i}+\sqrt{a_j})}{(1+a_i)(1+a_j)\sqrt{a_i}\sqrt{a_j}}$$ I get that one $1+a_j$ factor in the denominator is from the given summation condition and the we applied $a^2-b^2$ for square root in the numerator to get two factors. But where do other factors come from? Also in the summation in the last step, how did we say $i>j$ ?",['algebra-precalculus']
4396621,How many ways are there for three persons to register in five classes?,"I want to find the number of ways that $3$ people register in $5$ classes. Each of them can register for as many classes as they want but they must register in at least one class. I thought that for each class, a person has two choices. The person can register or not. So there are $2^5-1$ ways for a person to  register in the classes. (as each person must register for at least in one class). So given $3$ persons, total number of ways would be $(2^5-1)^3$ . Is it correct? But on the other hand, I thought that for each class, each person can register or not so for each class there are $2^3-1$ ways to be registered. So the final number would be $(2^3-1)^5$ , Which one is correct?",['combinatorics']
4396729,How close is an odd power of two to a perfect square?,"Let $n \ge 1$ be an odd natural number. Define $$f(n)=\min \{\,\, |k| \,\,\, | \, k+2^n \,\,\,\text{is a square}\,\,,k \in \mathbb{Z}\}.$$ That is $f(n)$ measures how close is the power $2^n$ to a perfect square. I guess that this notion was studied somewhere, but I couldn't find it naively on google. Question: (a bit soft) Does this function has a known name in the literature? Has it been studied somewhere? Is there a closed form formula for it, or at least some nice lower bounds on its values?","['number-theory', 'soft-question', 'elementary-number-theory', 'perfect-powers']"
4396743,Prove that $\det(A^{-1}-A)+\det(A^{-1}+A)\geq 6$,"Let $A$ be a $3 \times 3$ matrix with real numbers, with $\det(A)=1$ and $\operatorname{Tr}(A)=-1$ . Prove that $$\det(A^{-1}-A)+\det(A^{-1}+A)\geq 6.$$ This is supposed to be a 11th grade problem. Attempts: I used the formula $\det(A+xB)=\det(A)+ax+bx^{2}+\det(A)x^{3}$ and then
I tried expanding the determinant $\det(A^{-1}-A)=\det(A^{-1})-a+b-\det(A)$ , with $\det(A)=\frac{1}{\det A}$ , and the same with $\det(A^{-1}+A)=\det(A^{-1})+c+d+\det(A)$ but then I got stuck. I also used the equation of the matrix $A_{3 \times 3}$ : $A^{3}-\operatorname{Tr}(A)A^{2}+\operatorname{Tr}(A^{*})A-\det(A)I_3=O_3$ , I also know that $\operatorname{Tr}(A^{*})=\frac{\operatorname{Tr}(A)^{2}-\operatorname{Tr}(A^{2})}{2}$ but I couldn't find $\operatorname{Tr}(A^2)$ . I am missing something or is there another way of solving maybe involving matrix ranks and Sylvester’s Theorem? I could't find a way to apply it. Any help would be appreciated.","['contest-math', 'determinant', 'matrices', 'linear-algebra', 'inequality']"
4396812,Observing the limiting behavior of the ODE $z' = z(z-a)(1-z)$,"Consider the ODE given by $$\left\{
\begin{aligned}
z'(t) &= z(t) \cdot (z(t) - a) \cdot (1-z(t)) && t>0\\
z(0) &= z_0
\end{aligned}
\right.$$ where $z_0 \in (0,a)$ and $a \in (0,1)$ . I would like to show that $$\lim_{t\to \infty} z(t) = 0$$ in this scenario - preferably whilst avoid solving the ODE explicitly (since I have reason to believe it should be an ""obvious"" result). As context, this comes up in solving the nonlinear PDE $$\left\{
\begin{aligned}
&u_t - \Delta u = u(u-a)(1-u) &&(x,t) \in \Omega \times (0,\infty) \\
&\partial_n u \equiv 0 &&(x,t) \in \partial \Omega \times (0,\infty) \\
&u(x,0) = \varphi(x) &&x \in \Omega
\end{aligned}
\right.$$ wherein: $\Omega \subseteq \mathbb{R}^n$ $\varphi(x) \ge 0$ $\varphi\not \equiv 0$ $\| \varphi\|_\infty < a$ which is Example $3.4$ in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). It is claimed without proof that $$ z_0 < a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 0$$ and $$
z_0 > a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 1
$$ but I cannot for the life of me see why. Clearly, $z_0 < a$ gives $z'(t) < 0$ at $t=0$ , and similarly $z_0 > a$ gives $z'(t) > 0$ there. However, I'm not sure what to do with this information; what does it imply, if anything, about the future times? Ideally we'd be able to bound $z$ above/below as needed and use some sort of monotone convergence theorem to establish the desired result. If we do calculate an explicit solution to the ODE ( Wolfram link ), then we get a result that is implicit and uses logarithms: without accounting for $z_0$ , we'd have $$C + t = \frac{1}{a(a-1)} \Big( a \ln(1 - z(t)) - (a-1) \ln(z(t)) - \ln(z(t) - a) \Big)$$ and if we desire this to be well-defined over the reals, then we see that $$0 < a < z(t) < 1$$ in the scenario I wish to focus on (where $z_0 < a$ ). This at least gives upper and lower bounds on $z$ , but it's not clear if those are the tightest (or if, indeed, some amount of information is lost in solving the ODE). Does anyone have any ideas? Hopefully I'm just overlooking something obvious.","['limits', 'uniform-convergence', 'ordinary-differential-equations', 'partial-differential-equations']"
4396825,Is this a typo in Brezis's Ex 3.24?,"I'm doing Ex 3.24 in Brezis's book of Functional Analysis. The purpose of this exercise is to sketch part of the proof of Theorem 3.29 , i.e., if $E$ is a Banach space such that $B_{E}$ is metrizable with respect to $\sigma\left(E, E^{\star}\right)$ , then $E^{\star}$ is separable. Let $d(x, y)$ be a metric on $B_{E}$ that induces on $B_{E}$ the same topology as $\sigma\left(E, E^{\star}\right)$ . Set $$
U_{n}=\left\{x \in \color{blue}{B_{E}} ; d(x, 0)<\frac{1}{n}\right\}
$$ Let $V_{n}$ be a neighborhood of $0$ for $\sigma\left(E, E^{\star}\right)$ such that $\color{blue}{V_{n} \subset U_{n}}$ . We may assume that $V_{n}$ has the form $$
V_{n}=\left\{x \in \color{blue}{E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\}
$$ with $\varepsilon_{n}>0$ and $\Phi_{n} \subset E^{\star}$ is some finite subset. Let $D=\cup_{n=1}^{\infty} \Phi_{n}$ and let $F$ denote the vector space generated by $D$ . We claim that $F$ is dense in $E^{\star}$ with respect to the strong topology. Suppose, by contradiction, that $\overline{F} \neq E^{\star}$ . Prove that there exist some $\xi \in E^{\star \star}$ and some $f_{0} \in E^{\star}$ such that $$
\left\langle\xi, f_{0}\right\rangle>1, \quad\langle\xi, f\rangle=0 \quad \forall f \in F, \quad \text{ and }\quad\|\xi\|=1 .
$$ Let $$
W=\left\{x \in B_{E} ;\left|\left\langle f_{0}, x\right\rangle\right|<\frac{1}{2}\right\} .
$$ Prove that there is some integer $n_{0} \geq 1$ such that $V_{n_{0}} \subset W$ . Prove that there exists $x_{1} \in B_{E}$ such that $$
\left\{\begin{array}{l}
\left|\left\langle f, x_{1}\right\rangle-\langle\xi, f\rangle\right|<\varepsilon_{n_{0}} \quad \forall f \in \Phi_{n_{0}} \\
\left|\left\langle f_{0}, x_{1}\right\rangle-\left\langle\xi, f_{0}\right\rangle\right|<\frac{1}{2}
\end{array}\right.
$$ Deduce that $x_{1} \in V_{n_{0}}$ and that $\left\langle f_{0}, x_{1}\right\rangle>\frac{1}{2}$ . Conclude. If $E$ is finite-dimensional the the weak topology coincides with the norm topology. Now consider the case $E$ is infinite dimensional. Then each weakly open set is unbounded , so the set $V_n$ defined by the author above is unbounded and thus $V_{n}$ can not be a subset of $U_{n}$ . Hence I think it should be $$
V_{n}=\left\{x \in \color{blue}{B_E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\}
$$ Could you confirm if my observation is correct? I solve 3. as follows. It follows from $f_0 \notin \overline F$ that $f_0$ is linearly independent of any finite subset of $F$ . This implies $\bigcap_{f\in \Phi_{n_0}} \ker f \not \subseteq \ker f_0$ . This implies there is $0 \neq a \in \bigcap_{f\in \Phi_{n_0}} \ker f$ such that $a \notin \ker f_0$ . The we can pick $t\in \mathbb R$ such that $x_1 := ta$ satisfies the requirement. Could you confirm if this argument is fine?","['banach-spaces', 'functional-analysis', 'separable-spaces']"
4396929,"""Winding number"" of the map $g: T^{2n+1} \to U(N)$","For $n\in \mathbb Z_+$ , let $T^{2n+1}$ be the torus and $U(N)$ be the unitary group, where $N$ is sufficiently large. A physics paper on topological insulators claims the following: For a map $g: T^{2n+1} \to U(N)$ , we can associate an integer-valued winding number defined as $$  \frac{(-1)^n n!}{(2n+1)!} \left(\frac{i}{2\pi}\right)^{n+1} \int_{T^{2n+1}} \mathrm{Tr}\left( (g^{-1} dg)^{2n+1}\right) \in \mathbb Z.$$ Why is it integer-valued? How can we interpret the above quantity as the winding number? (The dimensions of $T^{2n+1}$ and $U(N)$ are different, but what I know from the de Rham cohomology theory is that the degree of map is defined for two connected and compact manifolds with the same dimension.) Is the above invariant related to the fact that $\pi_{2n+1}(U(N)) \simeq \mathbb Z$ ? (The physics paper also mentions this fact, but not explicitly states how the above invariant is related to $\pi_{2n+1}(U(N))$ .) Is the above formula valid if we replace the domain of $g$ to $S^{2n+1}$ ?","['winding-number', 'algebraic-topology', 'differential-geometry']"
4396937,Find maximum likelihood estimator of $f(x;\theta)=\frac{1}{\theta^2}xe^{\frac{-x}{\theta}}$,Find maximum likelihood estimator of $\hat{\theta}$ given the pdf $f(x;\theta)=\frac{1}{\theta^2}xe^{\frac{-x}{\theta}}$ I'm stuck on solving this one out. The answer is given as $\frac{\bar{X}}{2}$ in the answer sheet. I think I'm suppose to get to $\hat{\theta}=\frac{\left(\sum _{i=1}^nx_i\:\right)}{2n}=\frac{\bar{X}}{2}$ . I'm having a hard time getting there though. I've done a few problems like this and I found the estimator successfully but this one is giving me a hard time. I think I'm making some computational error somewhere which is causing the problem. Here are the equations I get when I solve them out: (I think I'm doing a computation wrong and it's causing issues maybe) $L(\theta)=\frac{1}{\theta ^{2n}}\cdot e^{\sum _{i=1}^n-\frac{x_i}{\theta }\:}\cdot \left(x_1\cdot x_2\cdot .....\cdot x_n\right)$ $ln\left(L\left(\theta \right)\right)=ln\left(\frac{1}{\theta ^{2n}}\cdot e^{\sum _{i=1}^n-\frac{x_i}{\theta }\:}\cdot \left(x_1\cdot x_2\cdot .....\cdot x_n\right)\right)$ $=-2nln\left(\theta \right)-\sum _{i=1}^n\frac{x_i}{\theta }+ln\left(x_1\cdot x_2\cdot ....\cdot x_n\right)\:$ $\frac{d}{d\theta }\left(L\left(\theta \right)\right)=\frac{d}{d\theta }\left(-2nln\left(\theta \right)-\sum _{i=1}^n\frac{x_i}{\theta }+ln\left(x_1\cdot x_2\cdot ....\cdot x_n\right)\right)\:=0$ $\frac{d}{d\theta \:}\left(L\left(\theta \right)\right)=-\frac{2n}{\theta }-\frac{d}{d\theta }\left(\sum _{i=1}^n\:\frac{x_i}{\theta }\right)+\frac{1}{\left(x_1\cdot x_2\cdot ...\cdot x_n\right)}=0$ $-\frac{2n}{\theta }-\left(\sum _{i=1}^n\:\frac{x_i}{\theta }\right)+\frac{1}{\left(x_1\cdot \:x_2\cdot \:...\cdot \:x_n\right)}=0$ $-\frac{2n}{\theta }-\left(\sum _{i=1}^n\:\frac{x_i}{\theta }\right)=-\frac{1}{\left(x_1\cdot \:x_2\cdot \:...\cdot \:x_n\right)}$ $-\frac{1}{\theta }\left(2n+\sum \:_{i=1}^n\:x_i\right)=-\frac{1}{\left(x_1\cdot \:x_2\cdot \:...\cdot \:x_n\right)}$ $\left(2n+\sum \:_{i=1}^n\:x_i\right)=\frac{\theta }{\left(x_1\cdot \:x_2\cdot \:...\cdot \:x_n\right)}$ $\left(2n+\sum \:_{i=1}^n\:x_i\right)\left(x_1\cdot \:\:x_2\cdot ...\cdot \:\:x_n\right)=\theta $ And this is not what I'm suppose to get. I'm a bit out of practice with calculus so I'm not really sure where the error is and how to fix it.,"['statistics', 'parameter-estimation', 'solution-verification', 'maximum-likelihood', 'probability']"
4396959,"Proof explanation: Bounding of $\mathcal{C}^1[a,b]$ functions which vanish at $a,b$ (among other conditions)","Claim: Let $f,g \in \mathcal{C}^1[a,b]$ be such that: $f(a) = f(b) = g(a) = g(b) = 0$ $g(x) > 0$ on $(a,b)$ $g'(a) > 0$ $g'(b) < 0
\newcommand{\o}{\Omega}
\newcommand{\oo}{\overline{\Omega}}
\newcommand{\pp}{\partial_{\mathbf{n}}}
\newcommand{\p}{\partial}
\newcommand{\CC}{\mathcal{C}}
$ Then $\exists k > 0$ whereby $f(x) \le k \cdot g(x)$ for all $x \in [a,b]$ . Context: This claim arises as Lemma $3.7$ in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). The original claim and proof are made for $n$ -dimensional space; I'm just trying to intuit my way around the one-dimensional case for now. Below, define $\partial_{\mathbf{n}} f$ as follows: $\mathbf{n}$ represents the outward normal vector for $f$ , in the case of $\partial \Omega$ (where $\Omega \subseteq \mathbb{R}^n$ is a bounded domain); then $\partial_{\mathbf{n}} f$ represents the derivative with respect to this vector. The claim and proof from the text go (essentially) as so: Lemma $3.7$ : Let $\o$ be of class $\CC^1$ , let $u,v \in \CC^1(\oo)$ be such that $u,v \equiv 0$ on $\p \o$ $v(x) > 0$ on $\o$ $\pp v < 0$ on $\p \o$ Then $\exists k > 0$ where $u(x) \le k \cdot v(x)$ in $\oo$ . Proof ( quoted verbatim ): Owing to $u,v \in \CC^1(\oo)$ and $\pp v < 0$ on $\p \o$ , there exists $k_1 > 0$ for which $$
\pp u - k_1 \pp v > 0
$$ on $\p \o$ . Noting that $u - k_1 v = 0$ on $\p\o$ , we can find a $\o$ -neighborhood $V$ of $\p\o$ such that $u-k_1 \le 0$ in $V$ . Because $u,v \in \CC^1(\o\setminus V)$ and $v > 0$ in $\o\setminus V$ , there is a positive constant $k_2$ such that $u(x) \le k_2 v(x)$ in $\o\setminus V$ . Take $k := k_1 + k_2$ . Then the desired conclusion holds. A translation to the one-dimensional case would essentially be this: Lemma $3.7$ (in $\mathbb{R}$ ): Let $\o = [a,b] = \oo$ , let $f,g \in \CC^1[a,b]$ be such that $f(x) = g(x) = 0$ for $x \in \{a,b\}$ $g(x) > 0$ on $(a,b)$ $g'(a) > 0$ and $g'(b) < 0$ Then $\exists k > 0$ where $f(x) \le k \cdot g(x)$ in $[a,b]$ . Proof: Owing to $f,g \in \CC^1[a,b]$ and the conditions on $g'$ , there exists $k_1 > 0$ for which: $f'(a) - k_1 g'(a) > 0$ $f'(b) - k_1 g'(b) < 0$ Interjection: Why does such a $k_1$ exist? And are these the correct derivative conditions in the bullets? (I'm not totally comfortable with the outer normal derivative thing.) Proof ( cont. ): Noting that $f(x) - k_1 g(x) = 0$ for $x = a,b$ , we can find an $\o$ -neighborhood $V$ of $\p \o = \{a,b\}$ where $f(x) - k_1 g(x) \le 0$ in $V$ . Interjection: So essentially we may take a ball of radius $\varepsilon$ at each of $x=a,x=b$ , and the text essentially claims that $f(x) - k_1 g(x) \le 0$ in these balls for appropriately small $\varepsilon > 0$ . How is this ensured? Why can we never have a positive value instead? Proof ( cont. ): Because $f,g \in \CC^1([a,b] \setminus V)$ and $g(x) > 0$ in $(a,b) \setminus V$ , then $\exists k_2 >0$ such that $f(x) \le k_2 g(x)$ for all $x \in (a,b)  \setminus V$ . Interjection: How is this $k_2$ 's existence justified? I'm guessing just take $\inf \{ v(x) \mid x \in (a,b)  \setminus V \}$ and scale accordingly, since $v$ is positive? But that seems too easy... Proof ( cont. ): Take $k := k_1 + k_2$ ; then the desired result holds. Can anyone enlighten me on these details?","['proof-explanation', 'real-analysis', 'continuity', 'upper-lower-bounds', 'derivatives']"
4397009,Evaluate $\int_{0}^{1}K(x)^2\text{d}x -\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x$,"Recently, I found this identity on a mathematical site(seems true): $$\int_{0}^{1}K(x)^2\text{d}x
-\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x
=\frac{\Gamma\left ( \frac{1}{4}  \right )^4 }{64}$$ where $K(x)=\int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t$ . A problem that resemble the above identity is here . Which has the power $3$ . But so far, I still don't really know how to relate those two integrals.","['integration', 'definite-integrals', 'special-functions', 'real-analysis', 'elliptic-integrals']"
4397017,Some calculation about expectation of empirical CDF,"Given iid random samples $X_1,\dots, X_n$ with CDF $F(x)$ and $f(x)$ that is continuous and symmetric around $0$ . (Then $E X=0$ and 50% quantile is $0$ ). Define the empirical CDF as $F_n(x)=\frac{1}{n}\sum_{i=1}^n I(X_i\le x)$ . How to simplify the following expectation in terms of $\mu_k=E|X_1|^k$ for $k=1,2,...$ ? (1) $$
 E\left[|X_1|^2(2F_n(0)-1)^2\right]
$$ (2) $$
 E[X_1|X_1|(2F_n(0)-1)]
$$ For (1),
This becomes $$
E\left[4|X_1|^2F_n(0)^2+|X_1|^2-4F_n(0)|X_1|^2\right]=E[4|X_1|^2F_n(0)^2]+E[|X_1|^2]-4E[F_n(0)|X_1|^2]
$$ where the third term is $4E[I(X_1\le 0)|X_1|^2]=4*0.5*\mu_2=2\mu_2$ , and the first term is $$
4E\left[|X_1|^2\left(\frac{1}{n^2}\sum_{i,j=1}^{n}I(X_i\le 0, X_j\le 0)\right)\right]
$$ but I am stuck here... For (2), since $E[X_1|X_1|]=0$ , it becomes $$
E[2X_1|X_1|F_n(0)]=2E[X_1|X_1|I(X_1\le 0)]=-2\int_0^\infty x|x|dF(x)=-\mu_2
$$ Does this one correct?","['statistics', 'analysis', 'probability']"
4397045,Are these generalizations known in the literature?,"By using $$\int_0^\infty\frac{\ln^{2n}(x)}{1+x^2}dx=|E_{2n}|\left(\frac{\pi}{2}\right)^{2n+1}\tag{a}$$ and $$\text{Li}_{a}(-z)+(-1)^a\text{Li}_{a}(-1/z)=-2\sum_{k=0}^{\lfloor{a/2}\rfloor }\frac{\eta(2k)}{(a-2k)!}\ln^{a-2k}(z)\tag{b}$$ I managed to find: $$\int_0^\infty\frac{\ln^{2n}(x)}{1+yx^2}dx=\frac{\left(\frac{\pi}{2}\right)^{2n+1}}{\sqrt{y}}\sum_{k=0}^n\binom{2n}{2k}|E_{2n-2k}|\pi^{-2k}\ln^{2k}(y)\tag{c}$$ $$\int_0^\infty\frac{\ln^{2n-1}(x)}{1+yx^2}dx=\frac{-\left(\frac{\pi}{2}\right)^{2n-1}}{2\sqrt{y}}\sum_{k=0}^{n-1}\binom{2n-1}{2k+1}|E_{2n-2k-2}|\pi^{-2k}\ln^{2k+1}(y)\tag{d}$$ $$\int_0^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx=|E_{2n}|\left(\frac{\pi}{2}\right)^{2n+1}\zeta(2a+1)$$ $$-\frac{\left(\frac{\pi}{2}\right)^{2n+1}}{(2a)!}\sum_{k=0}^n \binom{2n}{2k}|E_{2n-2k}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)\tag{e}$$ $$\int_0^\infty\frac{\ln^{2n-1}(x)\text{Li}_{2a}(-x^2)}{1+x^2}dx=\frac{-\left(\frac{\pi}{2}\right)^{2n-1}}{2(2a-1)!}*$$ $$\sum_{k=0}^{n-1} \binom{2n-1}{2k+1}|E_{2n-2k-2}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)\tag{f}$$ $$\int_0^1\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx=\frac12|E_{2n}|\left(\frac{\pi}{2}\right)^{2n+1}\zeta(2a+1)$$ $$-\frac{\left(\frac{\pi}{2}\right)^{2n+1}}{2(2a)!}\sum_{k=0}^n \binom{2n}{2k}|E_{2n-2k}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)$$ $$+2\sum_{k=0}^a\frac{(2k+2n+1)!}{(2k+1)!}4^k \eta(2a-2k)\beta(2k+2n+2)\tag{g}$$ $$\int_0^1\frac{\ln^{2n-1}(x)\text{Li}_{2a}(-x^2)}{1+x^2}dx=$$ $$\frac{-\left(\frac{\pi}{2}\right)^{2n-1}}{4(2a-1)!}\sum_{k=0}^{n-1} \binom{2n-1}{2k+1}|E_{2n-2k-2}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)$$ $$+\sum_{k=0}^a\frac{(2k+2n-1)!}{(2k)!}4^k \eta(2a-2k)\beta(2k+2n)\tag{h}$$ $$\sum_{k=1}^\infty\frac{(-1)^k H^{(2a+1)}_k}{(2k+1)^{2n+1}}=\frac1{2(2n)!}|E_{2n}|\left(\frac{\pi}{2}\right)^{2n+1}\zeta(2a+1)$$ $$-\frac{\left(\frac{\pi}{2}\right)^{2n+1}}{2(2n)!(2a)!}\sum_{k=0}^n \binom{2n}{2k}|E_{2n-2k}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)$$ $$+\frac{2}{(2n)!}\sum_{k=0}^a\frac{(2k+2n+1)!}{(2k+1)!}4^k \eta(2a-2k)\beta(2k+2n+2)\tag{i}$$ $$\sum_{k=1}^\infty\frac{(-1)^k H^{(2a)}_k}{(2k+1)^{2n}}=\frac{\left(\frac{\pi}{2}\right)^{2n-1}}{4(2n-1)!(2a-1)!}*$$ $$\sum_{k=0}^{n-1} \binom{2n-1}{2k+1}|E_{2n-2k-2}|\pi^{-2k}(2a+2k)!(2^{2k+2a+1}-1)\zeta(2k+2a+1)$$ $$-\frac{1}{(2n-1)!}\sum_{k=0}^a\frac{(2k+2n-1)!}{(2k)!}4^k \eta(2a-2k)\beta(2k+2n)\tag{j}$$ Question : Are the results of $(c)$ to $(j)$ known in the literature? If the reader is curious about the correctness of the results above and wants to verify them on Mathematica, the Mathematica command of $|E_r|$ is Abs[EulerE[r]] Thanks, Proof of (a) : By using Euler's reflection formula $$\Gamma(m)\Gamma(1-m)=\pi \csc(m\pi),\quad m\notin\mathbb{Z}$$ and beta function $$\operatorname{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}=\int_0^\infty \frac{x^{a-1}}{(1+x)^{a+b}}dx$$ with $a=m$ and $b=1-m$ , we have $$\pi\csc(m\pi)=\int_0^\infty\frac{x^{m-1}}{1+x}dx$$ differentiate both sides $2n$ times with respect to $m$ then let $m$ approach $1/2$ $$\int_0^\infty\frac{\ln^{2n}(x)}{1+x^2}dx=\frac{\pi}{2^{2n+1}}\lim_{m\to \frac12}\frac{d^{2n}}{dm^{2n}}\csc(m\pi)$$ the proof completes on using $$\lim_{m\to \frac12}\frac{d^{2n}}{dm^{2n}}\csc(m\pi)=|E_{2n}|\pi^{2n}$$ which is explained here . Proof of (b) : Divide both sides of the common dilogarithm identity $$\text{Li}_2(-z)+\text{Li}_2(-1/z)=-\frac{\ln^2(z)}{2}-2\eta(2)$$ by $z$ then integrate repeatedly. Proof of (c) : Let $yx^2=t^2$ , $$\int_0^\infty\frac{\ln^{2n}(x)}{1+yx^2}dx=\frac{1}{\sqrt{y}}\int_0^\infty\left(\ln(\sqrt{y})-\ln(t)\right)^{2n}\frac{dt}{1+t^2}$$ $$=\frac{1}{\sqrt{y}}\int_0^\infty\left(\sum_{k=0}^{2n} \binom{2n}{k}(-\ln(t))^{2n-k}\ln^k(\sqrt{y})\right)\frac{dt}{1+t^2}$$ $$=\frac1{\sqrt{y}}\sum_{k=0}^{2n}\binom{2n}{k}\ln^k(\sqrt{y})\int_0^\infty\frac{(-\ln(t))^{2n-k}}{1+t^2}dt$$ Since we care for only the even powers of $\ln(t)$ as the odd powers make the integral zero, we have $$\sum_{k=0}^{2n}f(k)=\sum_{k=0}^n f(2k)$$ and so $$\int_0^\infty\frac{\ln^{2n}(x)}{1+yx^2}dx=\frac1{\sqrt{y}}\sum_{k=0}^{n}\binom{2n}{2k}\ln^{2k}(\sqrt{y})\int_0^\infty\frac{\ln^{2n-2k}(t)}{1+t^2}dt$$ The proof completes on using the result in $(a)$ . Proof of (d) : We follow the same steps in proof $(c)$ : $$\int_0^\infty\frac{\ln^{2n-1}(x)}{1+yx^2}dx=\frac{-1}{\sqrt{y}}\int_0^\infty\left(\ln(\sqrt{y})-\ln(t)\right)^{2n-1}\frac{dt}{1+t^2}$$ $$=\frac{-1}{\sqrt{y}}\int_0^\infty\left(\sum_{k=0}^{2n-1} \binom{2n-1}{k}(-\ln(t))^{2n-k-1}\ln^k(\sqrt{y})\right)\frac{dt}{1+t^2}$$ $$=\frac{-1}{\sqrt{y}}\sum_{k=0}^{2n-1}\binom{2n-1}{k}\ln^k(\sqrt{y})\int_0^\infty\frac{(-\ln(t))^{2n-k-1}}{1+t^2}dt$$ Since we care for only the even powers of $\ln(t)$ , we have $$\sum_{k=0}^{2n-1}f(k)=\sum_{k=0}^{n-1} f(2k+1)$$ and so $$\int_0^\infty\frac{\ln^{2n-1}(x)}{1+yx^2}dx=\frac{-1}{\sqrt{y}}\sum_{k=0}^{n-1}\binom{2n-1}{2k+1}\ln^{2k+1}(\sqrt{y})\int_0^\infty\frac{\ln^{2n-2k-2}(t)}{1+t^2}dt$$ The proof completes on using the result in $(c)$ . Proof of (e) : Using the integral representation of the polylogarithm function: $$\text{Li}_{a}(z)=\frac{(-1)^{a-1}}{(a-1)!}\int_0^1\frac{z\ln^{a-1}(t)}{1-zt}dt$$ We have $$\int_0^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx=\int_0^\infty\frac{\ln^{2n}(x)}{1+x^2}\left(\frac{1}{(2a)!}\int_0^1\frac{-x^2\ln^{2a}(y)}{1+yx^2}dy\right)dx$$ $$=\frac{1}{(2a)!}\int_0^1\ln^{2a}(y)\left(\int_0^\infty\frac{-x^2\ln^{2n}(x)}{(1+x^2)(1+yx^2)}dx\right)dy$$ $$=\frac{1}{(2a)!}\int_0^1\frac{\ln^{2a}(y)}{1-y}\left(\int_0^\infty\frac{\ln^{2n}(x)}{1+x^2}dx-\int_0^\infty\frac{\ln^{2n}(x)}{1+yx^2}dx\right)dy$$ use $(a)$ and $(c)$ for the inner integrals $$=\frac{\left(\frac{\pi}{2}\right)^{2n+1}}{(2a)!}\left(|E_{2n}|\int_0^1\frac{\ln^{2a}(y)}{1-y}dy-\sum_{k=0}^n\binom{2n}{2k}|E_{2n-2k}|\pi^{-2k}\int_0^1\frac{\ln^{2k+2a}(y)}{\sqrt{y}(1-y)}dy\right)$$ The proof completes on using: $$\int_0^1\frac{\ln^a(x)}{1-x}dx=(-1)^aa!\zeta(a+1)$$ $$\int_0^1\frac{\ln^a(x)}{\sqrt{x}(1-x)}dx=(-1)^aa!(2^{a+1}-1)\zeta(a+1)$$ Proof of (f) : Following the same steps in proof $(e)$ : $$\int_0^\infty\frac{\ln^{2n-1}(x)\text{Li}_{2a}(-x^2)}{1+x^2}dx=\int_0^\infty\frac{\ln^{2n-1}(x)}{1+x^2}\left(\frac{1}{(2a-1)!}\int_0^1\frac{x^2\ln^{2a-1}(y)}{1+yx^2}dy\right)dx$$ $$=\frac{1}{(2a-1)!}\int_0^1\ln^{2a-1}(y)\left(\int_0^\infty\frac{x^2\ln^{2n-1}(x)}{(1+x^2)(1+yx^2)}dx\right)dy$$ $$=\frac{1}{(2a-1)!}\int_0^1\frac{\ln^{2a-1}(y)}{1-y}\left(\int_0^\infty\frac{\ln^{2n-1}(x)}{1+yx^2}dx-\int_0^\infty\frac{\ln^{2n-1}(x)}{1+x^2}dx\right)dy$$ substitute $(d)$ and notice that the second inner integral is zero $$=\frac{-\left(\frac{\pi}{2}\right)^{2n-1}}{2(2a-1)!}\sum_{k=0}^{n-1}\binom{2n-1}{2k+1}|E_{2n-2k-2}|\pi^{-2k}\int_0^1\frac{\ln^{2k+2a}(y)}{\sqrt{y}(1-y)}dy$$ The proof completes on using $$\int_0^1\frac{\ln^a(x)}{\sqrt{x}(1-x)}dx=(-1)^aa!(2^{a+1}-1)\zeta(a+1)$$ Proof of (g) : $$\int_0^1\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx=\int_0^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx-\underbrace{\int_1^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx}_{x\to1/x}$$ $$=\int_0^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx-\int_0^1\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-1/x^2)}{1+x^2}dx$$ add the integral to both sides $$2\int_0^1\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx=\int_0^\infty\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx$$ $$+\int_0^1\frac{\ln^{2n}(x)[\text{Li}_{2a+1}(-x^2)-\text{Li}_{2a+1}(-1/x^2)]}{1+x^2}dx$$ the first integral is given in $(e)$ . For the second integral, replace $a$ by $2a+1$ in $(b)$ then use $\sum_{k=m}^na_k=\sum_{k=m}^{n}a_{n-k+m}$ $$\text{Li}_{2a+1}(-z)-\text{Li}_{2a+1}(-1/z)=-2\sum_{k=0}^a \frac{\eta(2a-2k)}{(2k+1)!}\ln^{2k+1}(z)$$ and so $$\int_0^1\frac{\ln^{2n}(x)[\text{Li}_{2a+1}(-x^2)-\text{Li}_{2a+1}(-1/x^2)]}{1+x^2}dx=-4\sum_{k=0}^a\frac{\eta(2a-2k)}{(2k+1)!}4^k\int_0^1\frac{\ln^{2k+2n+1}(x)}{1+x^2}dx$$ and the proof completes on using $$\int_0^1\frac{\ln^a(x)}{1+x^2}dx=(-1)^a a! \beta(a+1)$$ Proof of (h) : We follow exactly the same steps in proof $(g)$ but here we use $$\text{Li}_{2a}(-z)+\text{Li}_{2a}(-1/z)=-2\sum_{k=0}^a \frac{\eta(2a-2k)}{(2k)!}\ln^{2k}(z)$$ Proof of (i) and (j) : Using the generating function $$\sum_{k=1}^\infty H_k^{(a)} x^k=\frac{\text{Li}_a(x)}{1-x}$$ and the fact that $$\int_0^1 x^{k}\ln^n(x)dx=\frac{(-1)^nn!}{(k+1)^{n+1}}$$ we have: $$\sum_{k=1}^\infty\frac{(-1)^k H^{(2a+1)}_k}{(2k+1)^{2n+1}}=\frac1{(2n)!}\int_0^1\frac{\ln^{2n}(x)\text{Li}_{2a+1}(-x^2)}{1+x^2}dx$$ $$\sum_{k=1}^\infty\frac{(-1)^k H^{(2a)}_k}{(2k+1)^{2n}}=\frac{-1}{(2n-1)!}\int_0^1\frac{\ln^{2n-1}(x)\text{Li}_{2a}(-x^2)}{1+x^2}dx$$ These two integrals are given in $(g)$ and $(h)$ . Edit 3/26/2024 More generalized polylogarithmic integrals can be found in this preprint .","['integration', 'reference-request', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
4397081,Find Conditional expectation of uniform variables ...,"Let $\xi,\eta$ be independent random variables, both with uniform distribution on $[0,2]$ . Find $E[\eta^2|\xi/\eta]$ . My attempt to solve the problem is in the attached file.
I believe I solved it, but made a mistake on the way.
Range of $u$ becomes $[0,\infty]$ , so if I integrate joint probability $1/8$ , I get $\infty$ . My solution","['conditional-expectation', 'uniform-distribution', 'probability-theory', 'random-variables']"
4397099,Transfinite limit of an orbit of a dynamical system and rationalizability of symmetric games.,"Consider a dynamical system and let $v(t)=(v^1(t),\dots,v^n(t))$ be an orbit. Suppose we are able to prove, for some $k=1,\dots,n$ : $$\lim_{t\to\infty} v^k(t)=0$$ Of course, we cannot be sure that $\exists T\ v^k(T)=0$ . In any arbitrary large, but finite time, the limit point can only be approximated. Intuitively, it takes $\omega$ steps to reach the limit point. Then, we can imagine of ""placing the system in its limit point"" and study its behavior iterating from there. In a sense, study the $\omega+\omega$ -limit of the system. My first question is: Is there something as the ""transfinite limit"" of a dynamical system? Can we formalize the previous idea, maybe using nets associated to the orbits of the system? I am asking this because in (evolutionary) game theory an interesting result hold, in words: considering a symmetric normal form game, we can associate to it a dynamical system describing how it would be played by a population of players interacting by imitation. This is a replicator dynamics, and the orbits of the system describe the evolution of of the frequency with which an action is played by the population. Then, one can prove that in the limit, the frequency with which a dominated action is played tend to $0$ .  An interesting set of actions is the set of rationalizable or equivalently iteratively undominated actions. Is there a ""limit"" in which the population plays only rationalizable actions? If the previous idea can be formalized, I would say that the answer is positive: in the first $\omega$ steps dominated actions are played with frequency $0$ , hence eliminated. Then, $\omega+\omega$ steps the  dominated actions of the resulting subgames are played with frequency $0$ and hence deleted, and so on until in some $\omega+\dots+\omega$ steps only rationalizable actions are played with positive probability. My second curiosity is the: If the ""transfinite limit"" of  a dynamical system is studied, are there general results linking the behavior at various ordinal levels?","['ordinary-differential-equations', 'ordinals', 'real-analysis', 'game-theory', 'dynamical-systems']"
4397112,"If $|\phi| \approx |\psi|$, then Is $|| \mathcal{F}^{-1} [\phi \hat{f}] ||_p \leq C || \mathcal{F}^{-1} [\psi \hat{f}] ||_p$?","$\underline{\textbf{Background:}}$ I am currently trying to estimate the following $L^p$ norm: \begin{align}
& \Big{|}\Big{|} A \Big{|}\Big{|}_p \\
& =
\int_{\mathbb{R}^d} \Big{|} \int_{|\xi|<2t^{1/2}} 
\exp\Big{(} ix\cdot\xi  \Big{)}
\Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \big{(}1+i\sqrt{ \frac{4t}{|\xi|^2} -1  }\big{)}\Big{)} \text{d}\xi \Big{|}^p \text{d}x  \Big{\} }^{1/p} ,
\end{align} where $t > 0$ , and $d \geq 2$ . The above complicated function was obtained from part of an inverse Fourier transform (which needs to be split into small and large $|\xi|$ , as the $\sqrt{\frac{4t}{|\xi|^2} - 1}$ part blows up at $|\xi| = 2t^{1/2}$ ). As such, I would like to treat it as a Fourier transform still (formally), by rewriting as \begin{align}
\Big{|}\Big{|} A \Big{|}\Big{|}_p = \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p ,
\end{align} where \begin{align}
\hat{f}(t,\xi) := \Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}\Bigg{|}_{|\xi|<2t^{1/2}}
\end{align} (I hope to get around the issue of smoothness by taking the limit of $\hat{f}$ times some test function.) $\underline{\textbf{Problem:}}$ I would like to simplify the problem further. Note that the multiplier in $\hat{f}$ is pointwise bounded above and below: \begin{align}
C^{-1} \frac{t^{1/2}}{|\xi|} \leq  \Big{|}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{|} \leq C \frac{t^{1/2}}{|\xi|}, \text{ for all } C \geq 4.
\end{align} I am wondering if this permits the following estimate: \begin{align}
& \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p \\
& \leq 
C
\Big{|}\Big{|} \mathcal{F}^{-1}\Big{[}   \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \frac{t^{1/2}}{|\xi|} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}  \Big{]}  \Big{|}\Big{|}_p.
\end{align} In more general terms, if we have pointwise boundedness above and below for a Fourier multiplier \begin{align}
C^{-1}\psi(\xi) \leq \phi(\xi) \leq C\psi(\xi), \text{ for all } \xi \in \mathbb{R}^d,
\end{align} can we then say \begin{align}
\Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \phi \hat{g} \Big{]} \Big{|}\Big{|}_p \leq C \Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \psi \hat{g} \Big{]} \Big{|}\Big{|}_p,
\end{align} for sufficiently well behaved $\hat{g}, \phi, \psi$ ? \begin{align}
\end{align}","['fourier-analysis', 'fourier-transform', 'complex-analysis', 'lp-spaces', 'functional-analysis']"
4397249,Name for connected spaces homeomorphic to all of their open proper connected subsets? (Besides $\mathbb{R}$?),"Because $\mathbb{R}$ is homeomorphic to any open interval, and a subset of $\mathbb{R}$ is connected if and only if it is an interval , it has the unusual property of being homeomorphic to any of its proper open connected subsets. In particular, given any point in $\mathbb{R}$ , all of its proper connected open neighborhoods are homeomorphic to $\mathbb{R}$ itself. Additionally, any space with the trivial topology vacuously has this property, since it has no proper open subsets. Question: Are there any other nontrivial spaces besides $\mathbb{R}$ with this unusual/idiosyncratic property? If so, has anyone ever studied this class of topological spaces and given them a name? If so, what is the name? Note that $\mathbb{R}^n$ for $n \ge 2$ does not have this property (cf. this question or this one ), $\mathbb{R}^2 \setminus \{0 \}$ is a counterexample (and one that shows we have to get algebraic topology involved for $n \ge 2$ ).","['general-topology', 'terminology', 'reference-request']"
