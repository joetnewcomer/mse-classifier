question_id,title,body,tags
3109494,Is one sheafification enough for the module inverse image?,"Let $\newcommand{\G}{\mathcal{G}}\newcommand{\O}{\mathcal{O}} f: (X, \O_X) \to (Y, \O_Y)$ be a morphism of locally ringed spaces and $\G$ a sheaf of $\O_Y$ modules. The module inverse image of $\G$ is defined as $$
f^* \G = f^{-1} \G \otimes_{f^{-1} \O_Y} \O_X
$$ There are three sheafifications involved here: one for the tensor product one for $f^{-1} \G$ and finally one for $f^{-1} O_Y$ , since the inverse image $f^{-1} \G$ is defined as the sheafification of $f^+ \G$ , where $$
f^+ \G (U) = \varinjlim_{V \supset f(U)} \G(V)
$$ My question is whether $f^* \G$ is equal to the sheafification of the presheaf $f^+ \G \,\hat\otimes_{f^+ \O_Y} \O_X$ (where $\hat\otimes$ denotes a tensor product without sheafification. In particular I would be interested in some ""high-level"" argument which maybe applies to more general situations.","['algebraic-geometry', 'category-theory', 'commutative-algebra', 'sheaf-theory']"
3109497,Derivation of $\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$ [duplicate],"This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Closed 3 years ago . I would like to see a detailed, step-by-step derivation of the following identity $$\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$$ where $x, y \in \mathbb R^n$ and $A \in \mathbb R^{n \times n}$ . I thought it would be easy to do using Einstein notation, but I am messing up with the reciprocal basis.","['matrices', 'scalar-fields', 'matrix-calculus', 'derivatives']"
3109513,What is a rational section of an invertible sheaf?,"I am studying Cartier divisors, and I am confused about exactly how they correspond to rational sections of a line bundle, or what a rational section of a line bundle even is. Let $X$ be an integral scheme with generic point $\eta$ . Let $\mathscr{L}$ be an invertible sheaf. What is the precise definition of a rational section in this context? I have read the following three: 1) A section $s$ of $\mathscr{L}$ over a dense open set (since $X$ is integral, this is just any open set), 2) An equivalence class of sections of $\mathscr{L}$ over open sets with the obvious equivalence relation, 3) A global section $s \in \Gamma(X, \mathscr{L} \otimes_{\mathcal{O}_{X}} \mathcal{K})$ , where $\mathcal{K}$ is the constant sheaf of rational functions on $X$ . I have been operating under the assumption of the second definition, which seems to me to just be saying that it's an element of the stalk $\mathscr{L}_{\eta}$ at the generic point. Is this correct? I also want to be able to say that 2) and 3) are equivalent, but I am not able to actually prove this. I was going to use the universal property for sheafifiation of the tensor product sheaf, but that argument seems to be broken because the line bundle may not have any global sections. This is a precursor question to a question about Cartier divisors, but I wanted to clarify this first to make sure I can frame the question properly and iron out any confusion before I ask it.","['divisors-algebraic-geometry', 'algebraic-geometry', 'schemes']"
3109545,"Directional derivative confusion - why does independently evaluating partial changes, then adding them, work?","I apologize for both my crude math grammar, and what is probably an obvious question - I am a novice. I am confused as to why, when taking the directional derivative, the gradient is evaluated by plugging in a fixed set of dependent variables, and this gradient is then dotted with the desired unit direction; as opposed to only the FIRST component in the gradient being evaluated by plugging in the fixed set of dependent variables, and then subsequent partial derivative components being evaluated at the new position/modified set of dependent variables , e.g. for a 2-d gradient, partial x being evaluated at (x,y) and partial y evaluated at (x+dx,y)? In more detail: As I understand it, the directional derivative for a function z=f(x,y) is calculated by the gradient dotted with a unit vector in the desired direction. Each component of the gradient is the ratio, at a point (x,y) , of the change in z to an “extremely small” independent change in x, or y. Dotting the x-component of the gradient with the x-component of our unit direction vector thus gives the change in z from taking an independent step in the x-direction by “ratio of change in z to extremely small change in x, scaled down by x-component of unit direction vector”. The same is independently done for y. The two are then added together to get the result of the dot product, apparently the ratio of the change in z to an “extremely small change” in the desired direction. My confusion lies in why the two partial derivatives that make up the gradient are evaluated at (x,y) before dotting with the desired unit direction, instead of partial x being evaluated at (x,y), and partial y being evaluated at ( x+(dx*x-component of unit direction vector) , y) (or vice versa) ? What if independent changes in x and y at a given point (x,y) would each result in increases in z, but changed together result in a decrease in z? Geometrically I can simply imagine a slope that increases in the positive x and y directions over a “small distance”, but drops into a pit when moving diagonally in <“small x * sqrt(2)/2”, “small y*sqrt(2)/2”> direction. The only way to reach this pit would be by first moving in one direction, then subsequently in the next direction, aka first evaluating the change in z from (x, y) to (x+dx*sqrt(2)/2, y), and then adding the change from (x+dx*sqrt(2)/2, y) to (x+dx*sqrt(2)/2, y+dy*sqrt(2)/2). This seems like an very obvious and trivial observation. Am I misunderstanding the directional derivative? Or is this issue simply “circumvented” by pointing to the partial changes in x and y being “infinitely small”? If so, how does that work when applied in the real, discrete world? Thank you very much for your time.","['multivariable-calculus', 'calculus', 'partial-derivative', 'gradient-descent', 'derivatives']"
3109633,Polylogarithm inequality: $(s+1)\frac{-\operatorname{Li}_{s+1} (-x)}{-\operatorname{Li}_s(-x)} > \log(x)$,"For $s \geq 0$ and $x > 0$ define $$ f_s (x) = - \operatorname{Li}_s (-x) \stackrel{s > 0}{=} \frac{1}{\Gamma(s)} \int \limits_0^\infty \frac{x t^{s-1}}{\mathrm{e}^t + x} \, \mathrm{d} t \, .$$ I would like to prove the inequality $$ \tag{1} (s+1) \frac{f_{s+1}(x)}{f_s(x)} > \log(x) $$ for $s \geq 0$ and $x \geq 1$ (it is obviously correct for $x \in (0,1)$ ). It can be used to show that the entropy of an ideal Fermi gas is always non-negative. Here's what I have come up with so far: Since $f_s (1) = \eta (s) > 0$ for $s \geq 0$ ( $\eta$ is the Dirichlet eta function), $(1)$ trivially holds for $x = 1$ . Note that we have $x f_{s+1}'(x) = f_s (x)$ for $s \geq 0$ and $x > 0$ by definition. Now suppose that $(1)$ is true for some $s \geq 0$ . Then $$ \frac{\mathrm{d}}{\mathrm{d}x} [(s+2)f_{s+2}(x) - \log(x) f_{s+1}(x)] = \frac{1}{x} [(s+1)f_{s+1}(x) - \log(x) f_{s}(x)] > 0$$ follows for $x \geq 1$ and integration yields $(1)$ with $s$ replaced by $s+1$ . Therefore, it is sufficient to prove the inequality for $s \in [0,1)$ . The case $s=0$ is trivial, as $f_0 (x) = \frac{x}{1+x}$ and $f_1(x) = \log(1+x)$ . Now let $s \in (0,1)$ . The inequality $f_{s+1}(x) > f_{s}(x) , \, x > 0 ,$ confirms $(1)$ for $x \in [1,\mathrm{e}^{s+1}]$ . The asymptotic expansion $$ f_{s} (x) \stackrel{x \to \infty}{\sim} 2 \log^s (x) \sum \limits_{k=0}^\infty \frac{\eta(2k)}{\Gamma(s+1-2k) \log^{2k}(x)} $$ leads to $$ \frac{(s+1) f_{s+1}(x)}{\log(x) f_s(x)} \stackrel{x \to \infty}{\sim} 1 + \frac{\pi^2 s}{3 \log^2 (x)} + \mathcal{O}(\log^{-4}(x)) \, , $$ which shows that $(1)$ holds for sufficiently large $x$ . These results and numerical experiments clearly suggest that the inequality is true for all $x \geq 1$ , but I have not found a proof yet. Integration by parts reveals that $(1)$ is equivalent to $$ \tag{2} \int \limits_0^\infty \frac{u^{s+1} \mathrm{e}^{\alpha (u-1)}}{(\mathrm{e}^{\alpha (u-1)} + 1)^2} \, \mathrm{d} u > \int \limits_0^\infty \frac{u^s \mathrm{e}^{\alpha (u-1)}}{(\mathrm{e}^{\alpha (u-1)} + 1)^2} \, \mathrm{d} u \, , \, \alpha = \log(x),$$ but I am stuck here as well, so my question is: How can we prove $(1)$ or $(2)$ for $s \in (0,1)$ and $x > 1$ ?","['integration', 'special-functions', 'polylogarithm', 'inequality', 'mathematical-physics']"
3109642,Evaluating $\int_0^1 \frac{\mathrm dx}{(x^2+ax+1)^{n+1}}$ with real methods,"I would like to know if there are any (preferably easier) methods of evaluating $$Q_n(a)=\int_0^1 \frac{\mathrm dx}{(x^2+ax+1)^{n+1}}$$ With real methods. Here's the way I did it. Complete the square: $$Q_n(a)=4^{n+1}\int_0^1\frac{\mathrm dx}{((2x+a)^2+4-a^2)^{n+1}}$$ Then $u=2x+a$ gives $$Q_n(a)=2^{2n+1}\int_a^{a+2}\frac{\mathrm du}{(u^2+4-a^2)^{n+1}}$$ Then consider the indefinite integral $$F_n^{w}(x)=\int\frac{\mathrm dx}{(x^2+w)^{n+1}}$$ Integration by parts yields the recurrence relation (for $n\in\Bbb Z\geq 1$ ) $$F_n^{w}(x)=\frac{x}{2wn(x^2+w)^n}+\frac{2n-1}{2wn}F_{n-1}^{w}(x)$$ With the base case $$F_0^{w}(x)=\frac1{\sqrt{w}}\arctan\frac{x}{\sqrt{w}}$$ And since this recurrence is in the form $$f_n=\alpha_n+\beta_nf_{n-1}$$ the solution to which is $$f_n=f_0\prod_{k=1}^{n}\beta_k+\sum_{k=0}^{n-1}\alpha_{n-k}\prod_{j=1}^{k}\beta_{n-j+1}$$ We have (I omit the simplification steps) $$F_n^{w}(x)=\frac{{2n\choose n}}{2^{2n}w^{n+1/2}}\arctan\frac{x}{\sqrt{w}}+S_n^w(x)$$ With $$S_n^w(x)=\frac{x}{2w}\sum_{r=0}^{n-1}\frac{(x^2+w)^{r-n}}{(2w)^r}R_r^{(n)}$$ and $$R_r^{(n)}=\frac1{n-r}\prod_{j=1}^{r}\frac{2n-2j+1}{n-j+1}$$ So we have that $$Q_n(a)=2^{2n+1}\left[F_n^{4-a^2}(a+2)-F_n^{4-a^2}(a)\right]$$ Which is, after some simplification, $$\begin{align}
Q_n(a)=&\frac{2{2n\choose n}}{(4-a^2)^{n+1/2}}\left[\arctan\sqrt{\frac{2+a}{2-a}}-\arctan\frac{a}{\sqrt{4-a^2}}\right]\\
&+\frac1{4-a^2}\sum_{r=0}^{n-1}\frac{2^rR_r^{(n)}}{(4-a^2)^r}\left[(2+a)^{r-n+1}-a\right]
\end{align}$$","['integration', 'definite-integrals']"
3109679,How to show the solution of $\dot{x} = Ax $ is an invariant subspace?,"Consider the linear dynamical system $\dot{x} = Ax $ in $V$ a finite dimensional vector space. The definition of an invariant subspace $U$ is as follows: For all $x_0 \in U$ , (initial condition), the solution $x(t)$ with initial condition $x(0)=x_0$ lies in $U$ , i.e. $x(t) \in U$ for all $t \in \mathbb{R}$ .
Intuitively, it means if we start in a subspace we never leave that subspace. Prove that a subspace $U \subset V$ is an invariant subspace if and only if $AU \subset U$ .","['vector-spaces', 'ordinary-differential-equations', 'dynamical-systems']"
3109685,Marginal convergence with independence implies jointly convergence,"If $X_n$ and $Y_n$ are independent random vectors for every $n$ , then $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to}Y$ imply that $(X_n,Y_n) \overset{d}{\to} (X,Y)$ where $X$ and $Y$ are independent. I know the statement is true for $X_n, Y_n$ converges to $X, Y$ in probability without the assumption of independence. When I tried to prove this, I used the characteristic function, but I got stuck to show $X$ and $Y$ are independent.","['probability-theory', 'weak-convergence']"
3109731,Is this subset of $M_4(\mathbb R)$ connected?,"Let us consider an affine structure $\star$ of $M_4(\mathbb R)$ which has following form \begin{align*}
\begin{pmatrix}
0 & * & 0 & * \\
1 & * & 0 & * \\
0 & * & 0 & * \\
0 & * & 1 & *
\end{pmatrix},
\end{align*} where $*$ can assume any real number. It is also clear for any monic $4^{th}$ degree real polynomial, we can at least find one realization in $\star$ since the upper left block and lower right block can be considered as in companion form. We further concern this set \begin{align*}
\mathcal E = \{ A \in \star: \max_i \left( \lambda_i(A) \right) < 0 \}.
\end{align*} In other words, all elements in $\mathcal E$ has above defined structure and all eigenvalues on the left open half plane. I am trying to determine whether the set $\mathcal E$ is connected. My first try was to determine for a fixed monic polynomial, whether all realizations in $\star$ is connected. If this is true, for any $A_1, A_2 \in \mathcal E$ , we may first connect them to a companion form in $\star$ (by making the $32$ entry to be $1$ and other entries in the second column to be $0$ ) without changing the eigenvalues, and then the companion forms are connected as a consequence of property of polynomials. But as the question I asked a while ago, this is only true if it has at least one real eigenvalue. I strongly believe the set is connected. The question is related but I am not sure it is that related. Because the condition there is much more strict. I was asking to connect all realizations in $\star$ yielding the same characteristic equation without changing its eigenvalues. Here we allow this to vary inside $\mathcal E$ .","['general-topology', 'path-connected', 'linear-algebra']"
3109732,"A group has odd number of elements iff each element is a square, $a=b^2$.","Let $G$ be a finite group having order $n$ , prove that $n$ is odd if and only if for each $a\in G$ , there is $b\in G$ such that $a=b^2$ . I first assume $n$ is odd, then let $a$ be an element in $G$ , since $a^n=e$ , we have $$a=a^{n+1}=(a^{(n+1)/2})^2$$ which tells $a$ is the square of $a^{(n+1)/2}$ . To prove the converse, I use contraposition, which I assume $n$ is even, then I want to find $a\in G$ such that it is not a square. My intuition is, ""assume"" we can always find an element $a\in G$ such that the smallest positive integer $k$ satisfy $$a^k=e$$ is $k=n$ . Then it is not true that $a$ can be expressed as a square, indeed if $a=b^2$ for some $b\in G$ , then the smallest $k$ satisfy $b^k=e$ will be $k=2n$ , but this contradicts the smallest $k$ cannot exceed $n$ . My problem is, is the assumption I made always true? If it is true then it should be in the very beginning of the textbook, but I have forgotton it. Any help? Source: Apostol Analytic Number Theory.","['group-theory', 'abstract-algebra']"
3109832,Little question regarding bijection from $ \mathbb N \times \mathbb N$ to $\mathbb N$,"I want to make explicit (by a formula) the function $σ : ℕ^2 → ℕ$ , defined by the pattern $σ(0,0) = 0, σ(1,0) = 1, σ(0,1) = 2, σ(2,0) = 3, σ(1,1) = 4, σ(0,2) = 5,…$ , and prove that σ is bijective. Looking at the sequence I thought that $σ(i,j) = M(i+j) + j$ with $M(m) = m(m+1)/2 $ should do, but proving that this is a bijection turned out to be more complicated than I expected. Any suggestion?","['elementary-set-theory', 'discrete-mathematics']"
3109870,Is there a sum of an uncountable set of Real numbers? [duplicate],"This question already has answers here : Does uncountable summation, with a finite sum, ever occur in mathematics? (3 answers) Closed 5 years ago . The community reviewed whether to reopen this question 10 months ago and left it closed: Original close reason(s) were not resolved The addition of Real numbers is commutative, so instead of saying we can find the sum of a sequence $\{a_1,...,a_n\}$ of real numbers that are pairwise not equal, we can say that there is a sum of a finite set $A=\{a_i:1\leq i \leq n\}$ of real numbers, since the order of summation does not matter. The same thing is true for infinite sequence. Instead of saying we can find the sum of a infinite sequence(series) $\{a_1,...\}$ (i.e. $\sum_{i=1}^\infty a_i$ ) of real numbers that are pairwise not equal, we can say that there is a sum of an infinite set $A=\{a_i: i \in \mathbb{N}\}$ of real numbers, since the result of summation is NOT changed if we put $a_i$ in different order. Question: Can we define a sum of a uncountable subset of real numbers? Does the order of summation matter in this case?","['real-numbers', 'summation', 'sequences-and-series', 'real-analysis']"
3109913,To find the inverse of a special kind of matrix.,"In a matrix analysis problem, I encountered the following special kind of matrix $$  
    \begin{bmatrix}
    0 & 1 & a & a & a & a \\
    1 & 0 & a& a& a& a \\
    a& a &0 & 1& a& a \\
    a& a &1 & 0 & a& a \\
    a & a & a & a &0 & 1\\
    a & a & a & a &1 & 0
    \end{bmatrix}
$$ where $a$ is a positive integer.But this matrix is not a circulant matrix. It is not in my knowledge if this is any known form. We can also search for inverses for the general form of the matrix for even order.","['analysis', 'matrices', 'inverse', 'linear-algebra', 'matrix-analysis']"
3109964,Determine the number of surjective functions $f$ from $A$ to $B$ such that $f(a_1)≤f(a_2)≤...≤f(a_{100})$.,"Let $A = \{a_1,a_2,...,a_{100}\}$ and $B= \{1,2,...,50\}$ . Determine
  the number of surjective functions f from A to B such that $f(a_1)≤f(a_2)≤...≤f(a_{100})$ . What if $f$ does not need to be
  surjective? My approach: Number of Surjective functions:
First $f(a_1)=1$ (otherwise f is not surjective). Of $a_2,..., a_{100}$ , choose any $49$ . Order them based on increasing order of index and assign values $2$ to $50$ to the $a_i$ 's based on their order. For example if I chose $a_2, a_5, a_{10},..a_{96}$ (some $49$ elements from $A$ ) then, $f(a_2)$ is the first element that equals 2 (first based on ordering) $f(a_5)$ is the first element that equals 3... $f(a_{96})$ is the first element that equals 50 The remaining 50 elements are fixed. Answer: $_{99}C_{49}$ . The problem can be recast as the block walking problem so the answer is $_{149}C_{49}$ .","['functions', 'combinatorics', 'discrete-mathematics', 'proof-verification']"
3109975,"If for some sequence $a_n\to \infty$ the limit $\lim_{n\to\infty} f(a_nx)$ exists for all $x\in\mathbb R$, then $\lim_{x\to\infty} f(x)$ exists","A little bit of context. I was given a problem which went like if $X_n$ is normally distributed with mean $a_n$ and is converging in distribution to $X$ , then $a_n\to a$ for some $a\in\mathbb R$ and $X$ is normally distributed. The question is quite doable using characteristic functions, I guessed, until I ended up with the problem if both limits $$\lim_{n\to\infty}\cos(a_nt) \ \ \ \text{ and } \lim_{n\to\infty} \sin(a_nt)$$ for all $t\in\mathbb R$ then the limit $$\lim_{n\to\infty} a_n$$ exists as well and is finite. If I assume $a_n$ is bounded, then I don't have any problem proving the statement. But if $a_n$ is unbounded, then there is, without loss of generality, a subsequence $a_{n_k}\to\infty$ . Therefore my question which is a bit general: Question. Let $f:\mathbb R\to\mathbb R$ be a continuous function and $a_n$ be a sequence diverging to $\infty$ , if $$\lim_{n\to\infty} f(a_nt)$$ exists for all $t\in\mathbb R$ , do we have $\lim_{x\to\infty} f(x)$ exists as well? The result is well-known if $a_n=n$ . If we have proven this statement then my original claim is easy to prove.  I attempted to mimic the proof in this post , but I stopped where they say $(f(nmx))_{n\in\mathbb N}$ is a subsequence of $(f(nx))_{n\in\mathbb N}$ since in my particular case I don't have $(f(a_na_mx))_{n\in\mathbb N}$ is a subsequence of $(f(a_nx))_{n\in\mathbb N}$ . However deep inside, I believe there is a possibility for mimicing.","['baire-category', 'functional-analysis', 'analysis', 'real-analysis']"
3109988,Understanding the notation when finding action-angle coordinates,"I'm trying to learn the basics of KAM theory and I wanted to first get to grips with Liouville integrability for Hamiltonian systems but I'm getting rather confused by the notation which seems to be universally assumed. Suppose we have a time independent Hamiltonian system in a $2n$ dimensional phase space, we can express the system in terms of generalised coordinates and momenta ${\bf{x}} = ({\bf{p}}, {\bf{q}})$ , then the equations read $$\dot{{\bf{x}}} = J\nabla H= \{{\bf{x}}, H \}$$ With $J$ the $2n \times 2n$ symplectic matrix $\left( \begin{array} { c c } { 0 } & { I_n } \\ { -I_n } & { 0 } \end{array} \right)$ and Poisson bracket $\{F,H\} := \nabla F^T J \nabla H$ The system is integrable in the Liouville sense if there exists $n-1$ additional ( $H$ taken as the first one) constants of motion, functions $f_i({\bf{p}}, {\bf{q}})$ such that for all $i,j = 1,\ldots n$ we have $\{f_i,f_j\} = 0$ and the gradients $\nabla f_i$ are linearly independent almost everywhere. If this is satisfied we can define so called action-angle variables $(I,\theta)$ such that under this coordinate system the Hamiltonian is a function of $I$ alone, so the equations become $$\displaystyle \dot{\theta}_i = \frac{\partial H}{\partial I_i} = \Omega_i(I) \hspace{10mm} \dot{I_i} = \frac{\partial H}{\partial \theta_i} = 0 $$ My confusion begins here, the formula given by pretty much every source I find is just stated as: $$I_i = \frac{1}{2\pi}\oint p_idq_i \hspace{10mm}\theta_i = \frac{\partial}{\partial I_i} \int p_i dq_i $$ What does this mean and can anyone provide a concrete example of doing this line integral? When I see a line integral it is usually in the form $\displaystyle \oint_\Gamma {\bf{F}} \cdot d{\bf{r}}$ , where ${\bf{F}} = {\bf{F}}({\bf{x}})$ is a vector field and we integrate along a curve $\Gamma$ which we can parametrise. I don't think this is a scalar line integral either since this worked example on page 12 claims to use Stoke's theorem. This post on physics stack exchange was somewhat helpful for understanding $p_i$ is really meant by the equation obtained by rearranging on a level set of $H$ , but I am not sure really whats going on with the path being integrated along, or what you do for the angle variable, since $p$ is just a coordinate and can only be identified as a function of $q$ through $H$ , how do you differentitate with respect to $I_i$ ?. Furthermore, at what point do we use all the Poisson-commuting constants of motion that we had to have to establish integrability in the first place, they do not seem to appear in this formula?","['classical-mechanics', 'integrable-systems', 'ordinary-differential-equations', 'dynamical-systems']"
3110027,When does $P\text{SO}P^{-1} \subseteq \text{SO}$?,"Let $P \in \text{GL}_n^{+}(\mathbb {R})$ . Suppose that $P\cdot \text{SO}(n)\cdot P^{-1} \subseteq \text{SO}(n)$ . Is it true that $P \in \lambda \text{SO}(n)$ for some $\lambda \in \mathbb{R}$ ? I know that every matrix that commutes with $\text{SO}(n)$ must be in $\lambda \text{SO}(n)$ (and for $n >2$ it must be a multiple of the identity), but this is not the same question. Edit: In a previous version, I only required $P \in \text{GL}_n(\mathbb {R})$ instead of $\text{GL}_n^{+}(\mathbb {R})$ . In that case any matrix in $\text{O}(n)$ would satisfy the requirements, so $P$ is not necessarily a multiple of special orthogonal matrix (in even dimensions). I guess that a morally equivalent question would be to assume only $P \in \text{GL}_n(\mathbb {R})$ , but to require $P\cdot \text{O}(n)\cdot P^{-1} \subseteq \text{O}(n)$ . Then is it true that $P \in \lambda \text{O}(n)$ for some $\lambda \in \mathbb{R}$ ?","['orthogonal-matrices', 'group-theory', 'symmetry', 'lie-groups']"
3110086,Find the general solution of $4y′′+ 3y′−y=e^{−x}+x$,"Given $$4y′′+ 3y′−y = e^{−x} + x$$ Find the general solution I identified this as a non-homogeneous linear equation
so I assumed that my solution, $$ y = y_h  + y_p$$ $$ 4\lambda^2 + 3\lambda - 1 = 0$$ $$ \lambda = -1 \quad\text{and}\quad \lambda =1/4$$ $$ y_h  = Ae^{x/4} + Be^{-x}$$ However, I am not too sure what should I do to find $y_p$ I do know that my $G(x) = e^{-x} + x$ but am not too sure how to find $y_p$ .","['calculus', 'ordinary-differential-equations']"
3110145,How the way of finding roots of a complex number works,"I would like to describe my doubt with a question Finding the sixth roots of $$ 16 - 16\sqrt{3}i $$ One can solve this question by obtaining $$ z^6 = r^6\exp(i\theta) = 32\exp((-\pi/3 + 2\pi k)/6)$$ Then if you substitute consecutive numbers you get the six roots $$ k = -1, \theta = -7\pi/18$$ $$ k = 0, \theta = -\pi/18$$ $$ k = 1, \theta = 5\pi/18$$ $$ k = 2, \theta = 11\pi/18$$ $$ k = 3, \theta = 17\pi/18$$ $$ k = 4, \theta = 23\pi/18$$ $$ k = 5, \theta = -7\pi/18$$ $$ k = 6, \theta = -\pi/18$$ and so on, where you can see a pattern, i.e. six numbers in a row And there's my first question, why times $\exp(2\pi k)$ with different ks can
obtain different solutions? Isn't that times $\exp(2\pi k)$ to a complex number just rotate the complex number anti-clockwise to the origin position? To my view, the 6 in the denominator seems performing magic here. For my second question, why there's pattern? Is it somehow related to trigonometric property or modular arithmetic?","['complex-numbers', 'discrete-mathematics']"
3110158,Show that $a_{n}M_{n} - a_{n}^{2}\xrightarrow{d} Q$,"Let $(X_{i})_{i}$ be IID standard normal random variables. Define $M_{n}:=\max{(X_{1},...,X_{n})}$ and $a_{n}=\sqrt{2\log(n)-\log(\log{(n)})-\log{(4\pi)}}$ Show that $a_{n}M_{n} - a_{n}^{2}\xrightarrow{d} Q$ where $Q$ is a probability measure on $\mathbb R$ with the distribution function $F_{Q}(c)=\exp{(-e^{-c}})$ for $c \in \mathbb R$ . My ideas: Let $c$ be an arbitrary continuous point for $F_{a_{n}M_{n} - a_{n}^{2}}$ and $F_{Q}$ respectively. It follows that $F_{a_{n}M_{n} - a_{n}^{2}}(c)=P(a_{n}M_{n} - a_{n}^{2}\leq c)=P(M_{n}\leq\frac{c+a_{n}^{2}}{a_{n}})=P(X_{1}\leq \frac{c+a_{n}^{2}}{a_{n}},...,X_{n}\leq \frac{c+a_{n}^{2}}{a_{n}})=(P(X_{1}\leq \frac{c+a_{n}^{2}}{a_{n}}))^{n}=\phi(\frac{c+a_{n}^{2}}{a_{n}})^{n}=\phi(\frac{c+2\log(n)-\log(\log{(n)})-\log{(4\pi)}}{\sqrt{2\log(n)-\log(\log{(n)})-\log{(4\pi)}}})^{n}=\frac{1}{\sqrt{2\pi}}(\int_{-\infty}^{\frac{c+2\log(n)-\log(\log{(n)})-\log{(4\pi)}}{\sqrt{2\log(n)-\log(\log{(n)})-\log{(4\pi)}}}}e^{-\frac{x^2}{2}}dx)^{n}$ And then I get stuck.","['probability-distributions', 'probability-theory', 'probability']"
3110159,Symmetry in function given by double sum,"I had to deal with this function: $$ f_n(x_1,x_2)=(x_2-x_1)^{n-1}\sum_{m=0}^{n-1}\sum_{j=0}^{n-m-1}C(n,m,j)\left(\frac{x_2}{x_2-x_1}\right)^m\left(\frac{x_2(1-x_1)}{x_2-x_1}\right)^j $$ where $$C(n,m,j)=\frac{(-n+1)_m}{m!}\frac{(-n+m+1)_j(n)_j}{j!(m+2)_j}$$ or $$C(n,m,j)=\frac{(-1)^{m+j}(n+j-1)!(m+1)}{j!(n-m-j-1)!(m+j+1)!}$$ (Here $(x)_j=x(x+1)\cdots (x+j-1)$ is the Pochhammer symbol or rising factorial). I computed several cases and it seems that $f_n(x_1,x_2)$ is a symmetic function for every $n$ , even though that is not apparent at first sight. Can one bring the function to another form that is explicitly symmetric? I have tried playing with hypergeometric identites, but there are so many of them and I didn't get anywhere. EDIT: Playing with particular cases, it seems the function is given by $$f_n(x_1,x_2)=\sum_{i=1}^{n}\sum_{j=1}^iA_{i,j}x_1^{n-1-i+j}x_2^{n-j}$$ and some of the coefficients are recognizable: $$A_{i,1}=A_{i,i}={2n-i-1\choose n-1}{n-1\choose n-i}\frac{1}{(n-i+1)}$$ and $$A_{n,j}={n-1\choose j-1}{n\choose j-1}\frac{1}{j}$$","['summation', 'functions', 'polynomials', 'hypergeometric-function']"
3110161,Approximating orthonormal basis in Hilbert space by orthonormal basis from dense subset,"Consider a Hilbert space $H_2$ with norm $\|\cdot\|_2$ . Consider a linear space $H_1 \subset H_2$ so that $H_1$ is dense in $H_2$ . Consider an orthonormal sequence $(h_k)_{k \in \mathbb{N}}$ in $H_2$ . Can I always find an orthogonal sequence $(g_k)_{k \in \mathbb{N}}$ in $H_2$ so that, for any $k \in \mathbb{N}$ , $$g_k\in H_1$$ and $$\|h_k - g_k\|_2 \leq \frac{1}{1+k^2}\; ?$$","['hilbert-spaces', 'orthonormal', 'functional-analysis']"
3110166,"Prove that, $\int_{0}^{2\pi}\frac{\cos x+2}{5+4\cos x} dx=\pi$","I have tried to solve this which goes as follows- $$\begin{align*}
\int\frac{\cos x+2}{5+4\cos x} dx
&=\int\frac{(1/4)(5+4\cos x)+(3/4)}{5+4\cos x} dx\\
&={1\over4}\int dx+{3\over4}\int\frac{dx}{9\cos^2 {x\over 2}+\sin^2{x\over2}}\\
&=({x\over4}+C)+{3\over4}\int\frac{\sec^2{x\over2}}{9+\tan^2{x\over2}}dx\\
&=({x\over4}+C)+{3\over2}\int\frac{dy}{9+y^2}\qquad \text{substituting $\tan {x\over2}=y$}\\
&={x\over4}+{1\over2}\arctan({y\over3})+C'={x\over4}+{1\over2}\arctan({\tan{x\over2}\over3})+C'
\end{align*}$$ So, $$\int_{0}^{2\pi}\frac{\cos x+2}{5+4\cos x} dx=\left[{x\over4}\right]_{0}^{2\pi}+{1\over2}\left[\arctan({\tan{x\over2}\over3})\right]_{0}^{2\pi}
\\={\pi\over2}+{1\over2}\left[\arctan({\tan\pi\over3})-0\right]={\pi\over2}+{1\over2}\left[0-0\right]={\pi\over2}$$ So, I can't get $\pi$ !! Can anybody find out where is my fault? Thanks for assistance in advance.","['integration', 'trigonometric-integrals', 'definite-integrals']"
3110168,equation with exponential functions 2,Solve the following equation over the real numbers: $$ (3+ \sqrt{5})^x + (3- \sqrt{5})^x=7 * 2^x $$,"['algebra-precalculus', 'exponential-function', 'radicals']"
3110212,Weak Convergence Lemma - Is Banach needed?,"Lemma. Let $X$ be a normed space. If $x_n \rightharpoonup x$ in $X$ and $x_n^* \to x^*$ in $X^*$ , then $\lim_{n \to \infty} x_n^*(x_n) = x^*(x)$ . If $X$ is even Banach, then $x_n \to x$ in $X$ and $x_n^* \overset{*}\rightharpoonup x^*$ in $X^*$ implies $\lim_{n \to \infty} x_n^*(x_n) = x^*(x)$ . The lecture notes I work with, had this wrong at first - it didn't include Banach for the second statement. (The proof uses boundedness of $(x_n^*)$ which relies on Banach-Steinhaus!) Is Banach really needed, though? Does anyone have a counterexample?","['banach-spaces', 'functional-analysis', 'weak-convergence']"
3110213,Showing that only $(n+1)^{n-1}$ of all the possible $n^n$ choices assure a full car park,"This exercise is taken from the site of Queen Mary University of London: A car park has $n$ spaces, numbered from $1$ to $n$ , arranged in a row. $n$ drivers each independently choose a favourite parking space in the park. As each driver enters the park, he drives to his favourite space. If it is empty, he parks there. Otherwise, he continues along the line and parks in the first free space. If all spaces between his favourite and $n$ are taken, he leaves in disgust. Show that, out of the $n^n$ choices of favourite spaces that the drivers can make, just $(n+1)^{n-1}$ lead to all drivers parking successfully. Now, my question is whether my (attempt of ) solution is wrong and what a different approach could be. Here's my idea: proving the claim by (a kind of) induction. It is easy to verify truthfulness for $n=1$ and $n=2$ , so having this base case, we shall prove that if the claim holds for all $1\le n\le k$ , then it does also for $n=k+1$ . Let us assume WLOG there is an already established order of appearance of the drivers, since a different order yields the same number of successful choices: it just suffices to swap the drivers. Let us call $d_m$ the space chosen by the $m$ -th driver. Now, consider the cases in which $d_1=1$ . By inductive hypothesis, the other drivers have $n^{n-2}$ successful choices. In fact, the same goes for all the other $n-1$ choices of the first driver, so that in total, as a ""first outcome"", we have $\color\red{n^{n-1}}$ successful choices. We somewhat can do the same reasoning with the second driver, but we must keep in mind that for each of his choices, we have already counted those in which the first driver occupies the other spaces. For example, we need to subtract the successful cases in which at the same time $d_2=1$ and $d_1=2$ : that is $(n-1)^{n-2}$ . This happens for all the spaces not chosen by the second driver, and so we actually need to subtract $(n-1)^{n-1}$ . Finally, letting vary $d_2$ yields the ""second outcome"" $\color\red{n^{n-1}-n(n-1)^{n-1}}$ . And so on, and so on. The final outcome would be the sum of all the red expressions. However I'm afraid no nice power would be a result of this. Is there a mistake in my reasoning?","['alternative-proof', 'proof-verification', 'combinatorics']"
3110230,Open problems in Cellular Automata field,"here there is a link on Wolfram about 20 open problems of CA theory . Has anyone of them been solved or tested?
I'm searching for literature.","['automata', 'cellular-automata', 'computational-mathematics', 'discrete-mathematics', 'computational-complexity']"
3110240,Are matrices of different sizes associative?,"I have three matrices, respectively A, B, C. Matrices A and B are both 3x3 matrices, and C is a 3x1 matrix. Is ABC associative? In general if the product (AB)C and A(BC) is well defined then does associativity always hold?","['matrices', 'linear-algebra', 'associativity']"
3110267,"Study convergence of $x_{n+1} = a\left(x_n + {1\over x_n}\right)$, for $x_1 = a$ and $a \in (0, 1)$","Given a recurrence relation: $$
x_{n+1} = a\left(x_n + {1\over x_n}\right) \\
x_1 = a\\
n\in\Bbb N
$$ Show that: $$
\begin{align*}
a \ge 1 &\implies \lim_{n\to\infty} x_n =+\infty \tag1\\
a \in (0, 1) &\implies \lim_{n\to\infty} x_n =\sqrt{\frac{a}{1-a}} \tag2\\
\end{align*}
$$ I think I've been able to prove case $(1)$ but faced difficulties in case $(2)$ . Here is what i've done for $a\ge 1$ . Suppose a limit exists, then it must equal to one of the fixed points: $$
\exists \lim_{n\to\infty}x_n = L \implies L = aL + {a\over L}\\
\iff L = \frac{\pm \sqrt{4a(1-a)}}{2(a-1)} = \mp \sqrt{\frac{a}{1-a}}
$$ Since $x_n > 0$ the only possible finite limit is $L = \sqrt{\frac{a}{1-a}}$ . Consider the following expression: $$
x_{n+1} - x_n = a\left(x_n - {1\over x_n}\right) - x_n =\frac{x_n^2(a-1)+a}{x_n} > 0
$$ Therefore $x_n$ in monotonically increasing: $$
x_{n+1} \ge x_n
$$ Also for $a\ge1$ : $$
L = \sqrt{\frac{a}{1-a}}\notin \Bbb R
$$ Thus by monotonicity of $x_n$ the only possible options left is: $$
\lim_{n\to\infty} x_n = +\infty
$$ I've tried to apply a similar reasoning to $(2)$ but unfortunately it lead nowhere. The difficulty is also in the fact that the sequence is not necessarily monotone. Here are two sandboxes i've been using while solving the problem. How can I show that the sequence has a limit when $a\in(0,1)$ ?","['limits', 'sequences-and-series', 'recurrence-relations', 'real-analysis']"
3110284,Is the Pisot Triangle series known?,"The Kepler triangle is built with powers of $\sqrt\phi$ to make a right triangle. The supergolden ratio can make a 120° triangle. It turns out that most Pisot numbers ( Mathworld , Wilkipedia ) 1 to 4 ( $\rho, \chi, p_3, \psi$ ) and $p_9$ can also make 120° triangles, as can $t$ , the tribonacci constant . Is this triangle series known?  I just realized the last triangle is in the tribonacci Rauzy fractal . I wonder if nice Rauzy fractals exist for $p_3$ and $p_9$ . These are related to New Substitution Tilings Using 2, φ, ψ, χ, ρ .","['algebraic-geometry', 'abstract-algebra', 'triangles', 'polynomials', 'constants']"
3110306,"Finding all real $x$ such that $1+\sum_{j=1}^n\sin{\frac{j\pi x}{n+1}} = 0$, where $n=18$.","The task is to find all $ x \in \mathbb R $ such that $$ 1 + \sum_{j=1}^n \sin{\frac{j\pi x}{n + 1}} = 0, \qquad n = 18 $$ What I have tried Using the following formulas: $$1. \sin{x} + \sin{y} = 2\sin{\frac{x + y}{2}}\cos{\frac{x - y}{2}} $$ $$2. \cos{x} + \cos{y} = 2\cos{\frac{x + y}{2}}\cos{\frac{x - y}{2}} $$ $$3. \sin{x}\sin{y} = \frac{1}{2} (\cos{(x - y)} - \cos{(x + y)}) $$ First approach $$ 1 + \sin{\frac{\pi x}{19}} + \sin{\frac{18\pi x}{19}} + \sin{\frac{2 \pi x}{19}} + \sin{\frac{17\pi x}{19}} + ... + \sin{\frac{9 \pi x}{19}} + \sin{\frac{10 \pi x}{19}} = 0 $$ Using formula 1: $$ 1 + 2\sin{\frac{\pi x}{2}} (\cos{\frac{17\pi x}{19}} + \cos{\frac{15\pi x}{19}} + ... + \cos{\frac{\pi x}{19}})$$ Skipping some steps, we get: $$ 1 + 2\sin{\frac{\pi x}{2}}\cos{\frac{9\pi x}{38}}(2\cos{\frac{3\pi x}{38} + 1})(2\cos{\frac{\pi x}{38}} + 1) = 0 $$ From here I couldn't find a way to go further Second approach Multiply both sides by $ \sin{\frac{\pi x}{38}} $ : $$ \sin{\frac{\pi x}{38}} (1 + \sin{\frac{\pi x}{19}} + \sin{\frac{2\pi x}{19}} + \sin{\frac{3 \pi x}{19}} + ... + \sin{\frac{17 \pi x}{19}} + \sin{\frac{18 \pi x}{19}}) = 0 $$ Using formula number 3 $$ \sin{\frac{\pi x}{38}} + \frac{1}{2}(\cos{\frac{\pi x}{38}} - \cos{\frac{3\pi x}{38}} + \cos{\frac{3\pi x}{38}} - \cos{\frac{5\pi x}{38}} + ... + \cos{\frac{35\pi x}{38}} - \cos{\frac{37\pi x}{38}}) = 0$$ $$ \sin{\frac{\pi x}{38}} - \sin{\frac{\pi x}{2}}\sin{\frac{18\pi x}{38}} = 0 $$ From here I couldn't find any way to go further Any hints will be appreciated! EDIT: The upper-most formula had a mistake, but now it is up to date. PS The problem is supposed to be solvable assuming you have high-school level math (i.e only simple trigonometric formulas are allowed, no complex numbers, no derivatives for the sine and cosine functions). Also, this equation on Wolfram Alpha shows the following solution: $ x = \frac{19}{2}(4n - 1), n \in \mathbb Z $ Update 2 : Using @Doug M's answer, I managed to conclude that ( $ \sin{\frac{\pi x}{19}} = 1 $ and $ \cos{\frac{\pi x}{19}} = 0 $ ), which equalates to $ \frac{\pi x}{19} = \frac{\pi}{2} + 2\pi n, n \in \mathbb Z $ is one of the solutions to the equation","['trigonometric-series', 'trigonometry']"
3110313,$n|a^n-b^n\Rightarrow n|\frac{a^n-b^n}{a-b}$ [duplicate],"This question already has answers here : $n \mid (a^{n}-b^{n}) \ \Longrightarrow$ $n \mid \frac{a^{n}-b^{n}}{a-b}$ (2 answers) Closed 5 years ago . Let $n\in\mathbb{N}$ and $(a,b)\in \mathbb{Z}^2$ . Show that: $$n|a^n-b^n\Longrightarrow n|\frac{a^n-b^n}{a-b}$$ I’ve tried an induction, but I gave up. Is there a direct proof? To admin: Please open this post and wait one day before closing it since we are looking for new perspectives.","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
3110316,Product of a and b should be equal to the sum of all numbers in the sequence excluding a and b,"I came across the below question in codewars. Regarding to the question, all I know is the sum of numbers ranging from 1 to n is $n(n+1)/2$ . And I have no idea how to solve this question further. There were plenty of code solutions available but I wanted to understand mathematically. Can someone help me to solve this? Given a sequence of numbers ranging from 1 to n, pick two numbers a and b 
   from that sequence such that the product of a and b should be equal to the 
   sum of all numbers in the sequence, excluding a and b",['sequences-and-series']
3110400,Closure of $AB$,"I'm trying to understand what the closure of $AB$ looks likes... $AB = \{ab: a \in A, b\in B\}$ So I know the closure of $AB = AB \cup (AB)'  = \{ab: a \in A, b\in B\}\cup\{ab: a \in A', b\in B'\} $ . But is this equal to $\{ab: a \in A\cup A', b\in B\cup B'\}$ ?  If yes, is this my properties of sets or just because of the closure?","['elementary-set-theory', 'topological-groups', 'analysis']"
3110405,Birthday Problem without using complement,"I have a solution to the birthday problem without using complements that is arriving at the wrong answer. I'd like to understand what I am doing wrong. I am not looking for alternate solutions to the problem. Problem Assuming there are only 365 days (ignore leap year), and each day is equally likely to be a birthday, what is the probability that at least 2 people have the same birthday in a room of N people? Sample Space: $365^N$ Event Space ${N\choose 2 }$ pairings for people with the same birthday for each pair, $365$ possible birthdays for remaining $N-2$ people, $365^{(N-2)}$ permutations which we can basically ignore (but still must be counted since they are part of the event space) So I would expect the answer to be: $$\frac{{N\choose 2 } * 365 * 365^{(N-2)}}{365^N} = \frac{{N\choose 2 }}{365}$$ With $N=23$ , I get 69% chance of $2$ people having same birthday, but correct answer is ~50%. So where am I over-counting?",['probability']
3110453,Why are subsets of compact sets not compact?,"So much of the properties of compact sets are motivated by finite sets, to the point that thinking of compact sets as topologically finite sets may yield some deeper understanding. But finite sets have the intuitive property that every subset of a finite set is also compact(also finite), why is it that compact sets give this up? It is easy to state that they just do and provide the example $I=(0,1)$ and $K=[0,1]$ as an example, but the problem with that is it really only helps illuminate how compact sets work in the euclidean spaces. It isn't generally true in all spaces that open sets aren't compact, or that closed and bounded sets are compact. So there is a problem generalizing the ideas. The heart of my question is : Given a subset $E$ of a compact set $K$ why isn't it compact? Simple Answer : Because there is an open cover of $E$ which has no finite subcover The deeper question: Why?","['general-topology', 'compactness', 'real-analysis']"
3110464,Continuous map from subset of $\mathcal{C}$ (Cantor) to non-measurable set.,"I've come across the following proposition and its proof. Given $f(x)=x+V(x)$ , where $V(x)$ is the Cantor-Vitali function on $[0,1]$ : $f(x)$ is an homeomorphism from $[0,1]$ to $[0,2]$ . $\lambda(f(\mathcal{C}))=1$ , where $\lambda$ is Lebesgue measure and $\mathcal{C}$ is Cantor set. Setting $g=f^{-1}$ , there is a measurable set $A\subset[0,1]$ whose preimage under $g$ is non-measurable. Proof for the first two points is straightforward. About the third one, it says $f([0,1]\setminus \mathcal{C})$ is a countable union of disjoint open intervals, hence $f(\mathcal{C})$ is a countable union of disjoint closed intervals $T_j$ . There exists a $T_k$ such that $\lambda(T_k)>0$ and, since $T_k$ is a closed interval, there's a non-measurable subset $E\subset T_k$ . Then $A=g(E)\subset\mathcal{C}$ is a measurable set because $\lambda$ is complete and $\lambda(\mathcal{C})=0$ . Although this seems reasonable to me, wouldn't it imply that $g(T_k)\subset \mathcal{C}$ ? But, being $g$ continuous, it would mean that $\mathcal{C}$ contains an interval, and that's false. Where am I wrong? Or is the excerpt from the proof incorrect? If so, how would I prove the third point?","['measure-theory', 'cantor-set', 'proof-explanation', 'real-analysis', 'measurable-sets']"
3110553,Associated Graded Algebra,"I'm trying to work through Exercise III.27 of Lang's Algebra : Let $A$ be a filtered algebra, $A=\bigcup_{j\geq 0}A_{j}$ . For $j\geq 0$ , define $R_{j}=A_{j}/A_{j-1}$ , with $A_{-1}=\{0\}$ . Let $R=\bigoplus_{j\geq 0}R_{j}$ . Define a natural product on $R$ making $R$ into a graded algebra. My Attempt : I believe that the product is defined by $$(x+A_{n-1})(y+A_{m-1})=xy+A_{n+m-1}$$ for all $x\in A_{n}$ and all $y\in A_{m}$ . Then $R$ is clearly a direct sum of subspaces and $R_{j}R_{k}\subset R_{j+k}$ for all $j$ and $k$ , since $$
\frac{A_{j}}{A_{j-1}}\cdot\frac{A_{k}}{A_{k-1}}\subset\frac{A_{j+k}}{A_{j+k-1}}$$ (in the above containment, we use the fact that $xy\in A_{j+k}$ if $x\in A_{j}$ and $y\in A_{k}$ ; Also, $A_{j-1} \cdot A_{k-1} \subset A_{j+k-1}$ ). Hence, $R$ is a graded algebra. My Questions: Does this argument look okay? In particular, how can I show that the product is well-defined? Thanks in advance for any suggestions.","['abstract-algebra', 'graded-algebras', 'algebras']"
3110589,Does there exist a group that is both a free product and a direct product of nontrivial groups?,"Do there exist such nontrivial groups $A$ , $B$ , $C$ and $D$ , such that $A \times B \cong C \ast D$ ? I failed to construct any examples, so I decided to try to prove they do not exist by contradiction. If such groups exist, then $C$ and $D$ are disjoint subgroups of $A \times B$ . Suppose $w \in F[x, y]\ \{e\}$ , where $F[x, y]$ is the free group with generators $x$ and $y$ . Suppose $(a_c, b_c) \in C$ , $(a_d, b_d) \in D$ and $h: F[x, y] \rightarrow A \times B$ is a homomorphism, that maps $x$ to $(a_c, b_c)$ and $y$ to $(a_d, b_d)$ . Then, by definition of free product $h(w) \neq e$ . Thus, either $\pi_A(h(w)) \neq e$ or $\pi_B(h(w)) \neq e$ , where $\pi_A$ and $\pi_B$ are projections on $A \times B$ onto $A$ and $B$ respectively. Thus every group word is not an identity either for $A$ or for $B$ and that results in $\{A, B\}$ generating the variety of all groups. And here I am stuck, failing to determine anything else. Or do the examples actually exist?","['direct-product', 'combinatorial-group-theory', 'abstract-algebra', 'free-product', 'group-theory']"
3110652,How to calculate number of matrices contains 2-zeroes lines,"Consider a matrix $A \in Mat_{n \times n}(\{0,1\})$ . Now we want to calculate the amount of 2-zeroes lines in matrix, i.e. consider  a matrix $A : $ \begin{pmatrix}
  1& 1 & 1 & \dots & 1\\
  0 & 0 & 0  &\dots & 0\\
  0 & 0 & 0  &\dots & 0\\
  1& 1 & 1 & \dots & 1\\
  \dots & \dots & \dots &\dots &\dots
\end{pmatrix} This matrix has two consecutive lines containing zeroes. We need to find number of matrix from $Mat_{n \times n}(\{0,1\})$ containig such propertie. Also I need to mention that this matrices are appropriates : $\begin{pmatrix}
  1& 1 & 1 & \dots & 1\\
  0 & 0 & 0  &\dots & 0\\
  0 & 0 & 0  &\dots & 0\\
    0 & 0 & 0  &\dots & 0\\
  \dots & \dots & \dots &\dots &\dots
\end{pmatrix}$ , $\begin{pmatrix}
  1& 1 & 0 & \dots & 1\\
  0 & 0 & 0  &\dots & 0\\
  0 & 0 & 0  &\dots & 0\\
    1 & 1 & 0  &\dots & 1\\
  \dots & \dots & \dots &\dots &\dots
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 0 & 1& \dots & 1\\
  1 & 0 & 0 &1  &\dots & 1\\
  1 & 0 & 0  &1&\dots & 1\\
    1 & 0 & 0 &1 &\dots & 1\\
  \dots & \dots &\dots& \dots &\dots &\dots
\end{pmatrix}$ , So it doesn't matter where should be the 2-lines of zeroes in row or in column, or it doesn't matter it 3-lines the main property that there should be at least one 2-zeroes line. I've tried to calculate it by step, i.e. : $\displaystyle \sum_{k=2n}^{4n-5}2(n-1)\binom{n^{2}}{k-2n} + \sum_{k=4n-4}^{...}\binom{n^{2}}{k-2n}-\dots$ this dotes arise from repeating combination. This is the problem I've stuck. Any ideas ? Edit :
All valid matrices for $n = 3$ $\begin{pmatrix}
  1& 1 & 1 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 1 & 1 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 1 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 1 & 0 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 1 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 0 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 1 & 0 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 1 \\
  0& 0 & 1 \\
  0& 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 1 \\
  0& 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 1 \\
  0& 0 & 0 \\
  0& 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 1 \\
  0& 0 & 1 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  0& 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 1 \\
  0& 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  1 & 1 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  0 & 1 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  1 & 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  1 & 1 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  0 & 0 & 1 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  0& 0 & 0 \\
  0 & 1 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 0 \\
  1& 0 & 0 \\
  1 & 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  1& 0 & 0 \\
  1 & 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 0 \\
  0& 0 & 0 \\
  1 & 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  1& 0 & 0 \\
  1& 0 & 0 \\
  0 & 0 & 0 \\
\end{pmatrix}$ , $\begin{pmatrix}
  0& 0 & 0 \\
  1& 0 & 0 \\
  0 & 0 & 0 \\
\end{pmatrix}$ . I guess that's all for $n =3$ .","['matrices', 'combinatorics']"
3110659,How to calculate $\sum\limits_{n=1}^{\infty}\frac{1}{3^n - 2^n}$,"By the test of reason, this series converges. 
The problem is figuring out which technique to use to calculate your sum. Thanks for any help.",['sequences-and-series']
3110665,Area of Mandelbrot set: Uniform convergence in Laurent series method,"I am reading Ewing and Schober's paper on analytically computing the area of the Mandelbrot set and I hope, in a shred of such, that someone might have an idea why swapping an integral and sum is justified. The paper can be foud on https://link.springer.com/article/10.1007%2FBF01385497 , p.69. A diffeomorphism $$\phi(z)=\sum_{m=-1}^{\infty} b_mz^{-m}$$ maps the exterior of the unit disk to the exterior of the Mandelbrot set.  The image $\phi(C_r)$ of a circle of radius $r>1$ forms a region $M( R)$ and we wish to find the area of $M(1)$ . Using Green's Theorem, they give the following expression for the area $$ -\frac12 \int \sum_{n,m\geq -1} m\overline{b_n}b_m(1/r)^{n+m}e^{(m-n)i\theta} d\theta. $$ I am trying to understand: Why does the sum $$\sum_{n,m\geq -1} m\overline{b_n}b_m(1/r)^{n+m}e^{(m-n)i\theta}$$ converges uniformly so that swapping the integral and the sum is justified? I tried to bound it by a geometric series but the trouble is that the coefficients are not constant and also $r>1$ . The first coeffients are $b_0=-1/2,b_2=-1/4,b_3=15/128,b4=0,b_5=-47/1024$ but computing these coefficients seems to be a delicate matter. Does the sum converge because these coefficients converge very quickly to zero?","['complex-analysis', 'complex-dynamics']"
3110669,What does constant along characteristic mean,"When we have linear or quasilinear first order pde $$ a(x,y,u) u_x + b(x,y,u) u_y = c(x,y,u) $$ And suppose we have found characteristics with prescribed Cauchy data $u |_{\text{curve}} = \phi$ I always see on my notes that the solution $u$ is always constant on the characteristics curves. What do they really mean by that? What is the geometric intuition ..?","['ordinary-differential-equations', 'parabolic-pde', 'pseudo-differential-operators', 'linear-pde', 'partial-differential-equations']"
3110670,Proving logical equivalences in the statements $(∃𝑥)(𝑃(𝑥) → 𝑄(𝑥))$ and $(∀𝑥)𝑃(𝑥) → (∃𝑥)𝑄(𝑥)$,For this I must show that the two statements $(∃𝑥)(𝑃(𝑥) → 𝑄(𝑥))$ and $(∀𝑥)𝑃(𝑥) → (∃𝑥)𝑄(𝑥)$ are logically equivalent. The issue I'm coming up with is that I'm unsure about the proper methods of proving predicate logic. Any help would be appreciated.,"['proof-writing', 'logic', 'predicate-logic', 'discrete-mathematics']"
3110691,"prove that : $\dfrac{a^2}{2}+\dfrac{b^3}{3}+\dfrac{c^6}{6} \geq abc$ for $a ,b ,c \in \mathbb{R}^{>0}$","prove that : $\dfrac{a^2}{2}+\dfrac{b^3}{3}+\dfrac{c^6}{6} \geq abc$ for $a ,b ,c \in \mathbb{R}^{>0}$ . I think that must I use from $\dfrac{a^2}{2}+\dfrac{b^2}{2} \geq ab$ but no result please help me .!","['algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
3110697,Delannoy numbers in 3 dimensions,"Is anyone aware of something analogous to Delannoy numbers in 3 dimensions? In addition to going left, right and diagonally in the plane, movement is possible to adjacent squares above (see the diagram).",['combinatorics']
3110751,Approximate Borel set by compact set,"I have proven the following: Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $\mathcal{A}$ an algebra such that $\sigma(\mathcal{A})=\mathcal{F}$ . Then for every $\varepsilon>0$ and $B\in\mathcal{F}$ we got that there is a set $A\in\mathcal{A}$ such that $\mathbb{P}(A\Delta B)\leq \varepsilon$ . (here $\Delta$ is the symmetric difference) I want to show: Let $\mathbb{P}$ be a probability measure on $(\mathbb{R}^{n},\mathcal{B}(\mathbb{R}^{n}))$ , where $\mathcal{B}(\mathbb{R}^{n})$ is the Borel algebra of subsets of $\mathbb{R}^{n}$ . Using the previous fact, show that for any $\varepsilon>0$ and $B\in\mathcal{B}(\mathbb{R}^{n})$ , that there is a compact set $A\in\mathcal{B}(\mathbb{R}^{n})$ such that $A\subseteq B$ and $\mathbb{P}(B\setminus A) \leq \varepsilon$ . Any suggestions?","['measure-theory', 'probability-theory', 'probability']"
3110764,Is a linear functional which is positive on linearly generating set of projections positive?,"Let $A$ be a unital C $^*$ algebra, and suppose there is a set of projections $P \subset \mathcal{P}(A)$ whose linear span is dense in $A$ . If $\varphi \in A^*$ has $\varphi(p) \ge 0$ for all $p \in P$ , does it follow that $\varphi \ge 0$ ? Note that this does hold if every element of $A$ can be approximated in norm by a linear combination of mutually orthogonal projections in $P$ (given any $x^*x \in A_+$ , such an approximation for $x$ will lead to an approximation of $x^*x$ by a linear combination with positive coefficients), but is there any reason to believe it in general?","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
3110782,"Spivak, Calculus on Manifolds 3-40","Looking for a hint to the following question: If $g: \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable and $\det g'(x) \neq 0,$ prove that in some open set containing $x$ we can write $g = T \circ g_n \circ \ldots \circ g_1,$ where $g_i$ is of the form $g_i(x) = (x^1, \ldots, f_i(x), \ldots, x^n),$ and $T$ is a linear transformation. Since $g$ is $C^1$ we can apply the inverse function theorem, and obtain an open set $A$ containing $x$ where $g$ is one to one. Since this problem is in the section on change of variables, I assume it can be used in the solution. I state it below as Spivak states it. Let $A \subset \mathbb{R}^n$ be an open set and $g: A \to \mathbb{R}^n$ be a 1-1, continuously differentiable function such that $\det g'(x) \neq 0$ for all $x \in A$ . If $f: g(A) \to \mathbb{R}$ is integrable, then $$\int_{g(A)}f = \int_Af \circ g|detg'|.$$ I've managed to produce the conditions on $g$ necessary for change of variables but I have no clue as to how to apply it. Considering just the representation of $g$ stated in the problem, it seems to be a composition taking $x$ to $g(x)$ coordinate by coordinate. Can anyone give me a hint?","['integration', 'real-analysis', 'multivariable-calculus', 'change-of-variable', 'linear-algebra']"
3110828,Relation between the eigenvalues of matrices conjugated by a homeomorphism.,"Let $A, B$ be $2\times 2 $ matrices satisfying: The eigenvalues $\lambda,\mu$ of $A$ satisfy $|\lambda|<1<|\mu|$ . The eigenvalues $\lambda',\mu'$ of $B$ satisfy $|\lambda'|<1<|\mu'|$ . There exists a homeomorphism $h:\mathbb{R}^2 \to \mathbb{R}^2$ such that $$h(A x) = B h(x), \  \forall \ x \in \mathbb{R}^2.$$ I'm reading the paper ""Generic Singularities of 3D Piecewise
Smooth Dynamical Systems"", and the author ""kinda says"" that that 1) + 2) + 3) implies that $$\frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}.  $$ My question: Does someone know if 1)+2)+3) $\Rightarrow$ $\frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')} $ ? Just some commentaries. I might be confusing something. However, my assumption is based on this phrase The references listed on the picture above are: Moreover, on Proposition 9, the author uses (in my view) the fact of existing a conjugation between two diffeomorphisms $\phi$ and $\phi_0$ , to conclude that $P(\phi) = P(\phi_0)$ and then construct a homeomorphism. Can anyone help me? EDIT: the author at no time says that $1) + 2) +3) \Rightarrow \frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}$ , I understood what was written in the wrong way, it was my mistake.","['matrices', 'diffeomorphism', 'dynamical-systems']"
3110836,Billiards in a holey square,"Suppose you start a point-billiard (or light ray) in a square at a random location, shooting off at a random angle, reflecting with angle-of-incidence equals angle-of-reflection. In general, because the point coordinates and direction vector are irrational with probability $1$ , the path will fill the square. Left: Starting from blue point, $100$ bounces; Right: $500$ bounces. Now suppose you remove all rational points from the boundary.
If the square is $[-1,1]^2$ , remove points $(\pm 1, r)$ and $(r, \pm 1)$ for every rational $r \in [-1,1]$ .
So now the boundary has an infinite (but countable) number of holes:
there is a hole at $(\frac{1}{2},1),(\frac{1}{32},1),(\frac{171}{541},1)$ , etc. Q1 . Is it the case that the probability that the billiard / light ray escapes through a boundary hole is zero? I believe the answer is Yes , but it certainly strains intuition, so I want to be certain. Q2 . Under what conditions on the starting position and initial ray direction will the escape probability be positive, presumably $1$ ?","['billiards', 'random', 'geometry', 'infinity', 'dynamical-systems']"
3110848,"If $\cos\theta=\frac{\cos\alpha+\cos \beta}{1+\cos\alpha\cos\beta}$, then prove that one value of $\tan(\theta/2)$ is $\tan(\alpha/2)\tan(\beta/2)$",If $$\cos\theta = \frac{\cos\alpha + \cos \beta}{1 + \cos\alpha\cos\beta}$$ then prove that one of the values of $\tan{\frac{\theta}{2}}$ is $\tan{\frac{\alpha}{2}}\tan{\frac{\beta}{2}}$ . I don't even know how to start this question. Pls help,"['algebra-precalculus', 'trigonometry']"
3110864,Proving that a subset of $l^{2}$ is compact,"Define $B \subset \ell^{2}$ by $$B = \{x \in \ell^{2}: \sum_{n=1}^{\infty}n|x_{n}|^{2} \leq 1 \}. $$ Show that $B$ is compact. I found this question while studying for an exam. I tried proving that $B$ was sequentially compact by taking an arbitrary sequence $\{ x^{(k)} \}_{k \geq 1} \in \ell^{2}$ , and using a diagonalization argument to extract a subsequence $\{ x^{k(j)} \}_{j \geq 1}$ with the property that $$\lim_{j \to \infty}x_{n}^{k(j)} = x_{n} \in \left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}} \right],$$ for each $n \in \mathbb{N}$ . I proved that $x = (x_1,\dots,x_{n},\dots) \in B$ , but can't find a way to prove that my original sequence converges to $x$ in the $\ell^{2}$ norm. Is this approach correct so far, or would it be easier to show that $B$ is complete and totally bounded?","['sequences-and-series', 'functional-analysis', 'compactness', 'real-analysis']"
3110870,"Provide a counterexample: If $n^2-1$ is divisible by $5$, then $n$ is divisible by $2$ or $3$","Provide a counterexample: If $n^2-1$ is divisible by $5$ , then $n$ is divisible by $2$ or $3$ My book doesn't have an answer to this question, but I think it's $n=6$ . 
Since $6^2-1=35$ , which is divisible by $5$ but not by $2$ or $3$ . Is this the right way to solve these counterexample problems? Are these problems solved by just plugging in numbers until you get to the right one?","['examples-counterexamples', 'discrete-mathematics']"
3110976,A Challenging Integral: $\int_{0}^{1} \frac{x \arcsin(x/2) \log(x)}{x^2-1} \ dx=\frac{5 \pi^3}{1296}$,"I wish to evaluate the integral $$I=\int_{0}^{1} \frac{x \arcsin(x/2) \log(x)}{x^2-1} \ dx.$$ I used Mathematica's Rationalize command to see that it is equal to $5 \pi^3/1296,$ but I do not know how to prove it analytically. Context The integral appears as part of an alternative expression for the triple integral $$J=\int_{0}^{1} \int_{0}^{1}\int_{0}^{1} \frac{x^2y}{\sqrt{4-x^2}{\sqrt{4-x^2y^2} \sqrt{4-x^2y^2z^2}}} \ dz \ dy \ dx.$$ It is easy to evaluate $J$ directly and show it is $\frac{\pi^3}{1296}.$ However, if we reverse the order of integration and integrate with respect to $y$ first, we get $$J=- \int_{0}^{1} \int_{0}^{1} \frac{\log \left(\frac{\sqrt{4-x^2 z^2}+\sqrt{4-x^2} z}{2 z+2}\right)}{\sqrt{4-x^2} z} \ dz \ dx.$$ The integral can be expanded into the triple integral $$- \int_{0}^{1} \int_{0}^{1} \int_{0}^{z}\frac{x^2}{z \left(x^2 \left(t^2 \sqrt{4-x^2}+\sqrt{4-t^2 x^2}\right)-4
   \left(\sqrt{4-t^2 x^2}+\sqrt{4-x^2}\right)\right)} \ dt \ dz \ dx.$$ Reversing the order of integration and integrating with respect to $x$ first, we can deduce \begin{align*}
J &= \int_{0}^{1} \int_{t}^{1}\frac{\frac{\pi }{6} t-\sin ^{-1}\left(\frac{t}{2}\right)}{\left(t-t^3\right) z} \ dz \ dt  \\
&= -\int_{0}^{1}\frac{\frac{\pi}{6} t \log(t) -\sin ^{-1}\left(\frac{t}{2}\right) \log(t)}{t-t^3} \ dt  \\
&=\frac{\pi}{6} \int_{0}^{1}\frac{\log(t)}{t^2-1} \ dt + \int_{0}^{1} \frac{\sin ^{-1}\left(\frac{t}{2}\right) \log(t)}{t-t^3} \ dt \\
&= \frac{\pi^3}{48} + \int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt - \int_{0}^{1} \frac{t \log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t^2-1} \ dt,
\end{align*} in which we used the well-known result $$\int_{0}^{1} \frac{\log(t)}{t^2-1} \ dt = \frac{\pi^2}{8}$$ and partial fractions on the second integral term in the second to last equality. Recalling $J=\pi^3/1296,$ rearranging gives $$
\begin{align*}
-\frac{13 \pi ^3}{648} &=\int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt - \int_{0}^{1} \frac{t \log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t^2-1} \ dt \\
&=\int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt - I.
\end{align*}$$ Question Upon using integrating by parts, it turns out $$\int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt= -\int_{0}^{1} \frac{\log^2(t)}{2\sqrt{4-t^2}} \ dt.$$ Further substituting $t=2 \sin(\theta)$ and using the answers in either Interesting log sine integrals $\int_0^{\pi/3} \log^2 \left(2\sin \frac{x}{2} \right)dx= \frac{7\pi^3}{108}$ or Finding $\int^{1}_{0}\frac{\ln^2(x)}{\sqrt{4-x^2}}dx$ , we can deduce $$\int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt = -\frac{7 \pi^3}{432}.$$ Hence, assuming this, we can get $$I= \frac{5 \pi^3}{1296}.$$ Can we evaluate $I$ without knowing the value of the integral $\int_{0}^{1} \frac{\log (t) \sin ^{-1}\left(\frac{t}{2}\right)}{t} \ dt$ ? Can $I$ be evaluated with real methods ?","['integration', 'calculus']"
3111000,"Show that $\lim\limits_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4}$ does not exist.","Show that $$\lim_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4}$$ does not exist. I'm not even sure how to approach this. I tried factoring out $xy$ in the numerator to get $xy(x^2 - y^2)$ , but I don't think that gets me anywhere with the denominator.","['multivariable-calculus', 'limits', 'calculus']"
3111005,Probability of getting 3 balls in 1st box if 12 balls are distributed randomly among 3 boxes,"$12$ balls are distributed at random among $3$ boxes.The probability that the 1st box will contain $3$ balls is_______? My Approach: $\quad \quad \quad \quad \quad \quad \text{Method}1$ [Considering all balls different] : Tatal no. of ways= $3^{12}$ No. of ways in which 1st box will contain $3$ balls: $$\binom{12}{3} * 2^9$$ Therefore,required probability= $$\frac{\binom{12}{3} * 2^9}{3^{12}}$$ $$=0.2119$$ $\quad \quad \quad \quad \quad \quad \text{Method}2$ [Considering all balls identical] : Tatal no. of ways: $$x_1 + x_2 + x_3=12$$ ,where $\,\, 0 \leq x_1 \leq 12,\,\, 0 \leq x_2 \leq 12,\,\, 0 \leq x_3 \leq 12$ $$= \binom{3+12-1}{12}$$ $$=\binom{14}{12}$$ No. of ways in which $1st$ box will contain $3$ balls $=$ No. of ways in which $2nd$ and $3rd$ boxes will contain $9$ balls (as the remaining $3$ balls will be given to $1st$ box) so, $$ x_2 + x_3=9$$ ,where $\,\, \,\, 0 \leq x_2 \leq 9,\,\, 0 \leq x_3 \leq 9$ $$=\binom{2+9-1}{9} =\binom{10}{9}$$ Therefore,required probability= $$\frac{\binom{10}{9} }{\binom{14}{12}}$$ $$=0.11$$ so, which method is the correct approach for the given problem and why the other is wrong,please explain...","['combinatorics', 'probability']"
3111026,Does the set of all finite subsets of positive integers form a group under set intersection?,"Let A be a set of all finite subsets of positive integers. I have proved closure and associativity under intersection. I am kind of confused about existence of identity. Originally, I was thinking that the set of all positive integers also lives in A and when any set M in A intersects with that set, we will get M back. However, I am not sure if the set of all positive integers even lives in A cause it is not finite, is it? Any help would be great.","['elementary-set-theory', 'group-theory']"
3111054,"Show that no matter how $12$ points are put on a plane, there are $3$ among them forming an angle not greater than $18^o$.","Problem : Show that no matter how $12$ points are put on a plane, there are $3$ among them forming an angle not greater than $18^o$ . I am not getting any ideas in solving this problem. So, there will be $\binom{12}{3}= 220 $ triangles which means there will be a total of $660$ angles. We need to show that at least one of out of these $660$ angles will be less than or equal to $18^o$ degrees. How should we proceed now?","['pigeonhole-principle', 'triangles', 'combinatorics', 'geometry']"
3111075,"Let $A,B,X$ be finite sets. Prove that $|A \triangle X| + |X \triangle B| = |A \triangle B| \iff A \cap B \subseteq X \subseteq A \cup B$","I'm trying to do the following exercise: Let $A,B,X$ be finite subsets of a set $U$ . Prove that $|A \triangle X| + |X \triangle
 B| = |A \triangle B| \iff A \cap B \subseteq X \subseteq A \cup B$ I am allowed to assume that the cardinality of the symmetric difference of finite sets is a metric.
So I can use the following properties without proving them: Let $A,B,C$ be finite subsets of a set $U$ , then: P.1) $|A \triangle B| \ge 0$ P.2) $|A \triangle B| = 0 \iff A=B$ P.3) $|A \triangle B| = |B \triangle A| $ P.4) $|A \triangle C| \le |A \triangle B|  + |B \triangle C|$ This is how I tried to prove the implication $A \cap B \subseteq X \subseteq A \cup B \implies |A \triangle X| + |X \triangle
 B| = |A \triangle B|$ I'm assisting myself with the following two lemmas: Lemma 1: $A \cap B \subseteq X \subseteq A \cup B \implies (A \triangle X) \cap (X \triangle B) = \emptyset$ Proof: Let $A,B,X$ be finite sets such that $A \cap B \subseteq X \subseteq A \cup B$ Let's assume there exists an element $t \in (A \triangle X) \cap (X \triangle B)$ There are four possible cases: i) $t \in (A-X) \ \land \ t \in (X-B)$ ii) $t \in (A-X) \ \land \ t \in (B - X)$ iii) $t \in (X-A) \ \land \ t \in (X-B)$ iv) $t \in (X-A) \ \land \ t \in (B-X)$ Assuming i) : $t \in (A-X) \ \land \ t \in (X-B) \implies (t \in A \ \land \ t \notin X) \land (t \in X \ \land t \notin B) \implies t \notin X \land t \in X$ , which is a contradiction. Assuming ii) : $t \in (A-X) \ \land \ t \in (B - X) \implies (t \in A \ \land t \notin X) \land (t \in B \ \land \ t \notin X)$ $\implies (t \in A \ \land \ t \in B) \land t \notin X \implies t \in A \cap B \ \land \ t \notin X$ , which contradicts the hypothesis $A \cap B \subseteq X$ . Assuming iii) : $t \in (X-A) \ \land \ t \in (X-B) \implies (t \in X \ \land \ t \notin A) \land (t \in X \ \land \ \ t \notin B)$ $\implies t \in X \ \land \ t \notin A \ \land t \notin B \implies t \in X \ \land \ t \notin A \cup B$ , which contradicts the hypothesis $X \subseteq A \cup B$ . Assuming iv) : $t \in (X-A) \ \land \ t \in (B-X) \implies (t \in X \ \land \ t \notin A) \land (t \in B \ \land \ t \notin X) \implies t \in X \land t \notin X$ , which is a contradiction. We get a contradiction in all four cases, so there does not exist an element $t$ such that $t \in (A \triangle X) \cap (X \triangle B)$ . Therefore $(A \triangle X) \cap (X \triangle B) = \emptyset$ $ \blacksquare$ Lemma 2: If $A,B,X$ are finite subsets of $U$ then $(A \triangle X) \cup (X \triangle B) = (A \cup X \cup B) - (A \cap X \cap B)$ Proof: By definition of symmetric difference we have: $(A \triangle X) \cup (X \triangle B) = [(A \cup X) \cap \overline{(A \cap X)}] \cup [(X \cup B) \cap \overline{(X \cap B)}]$ Applying the distributive laws of union and intersection: $(A \triangle X) \cup (X \triangle B) = [((A \cup X) \cap \overline{(A \cap X)} \ ) \cup (X \cup B)] \cap [((A \cup X) \cap \overline{(A \cap X)} \ ) \cup \overline{(X \cap B)}]$ $= [((A \cup X) \cup (X \cup B)) \cap ( \ \overline{(A \cap X)} \  \cup (X \cup B))] \cap [((A \cup X) \cup \overline{(X \cap B)} \ ) \cap ( \ \overline{(A \cap X)} \ \cup \ \overline{(X \cap B)} \ )]$ By de Morgan laws and idempotence and associativity of union and intersection: $(A \triangle X) \cup (X \triangle B) = [(A \cup X \cup B) \cap ( ( \ \overline{A} \cup \overline{X} \ ) \cup (X \cup B))] \cap [((A \cup X) \cup ( \ \overline{X} \cup \overline{B} \ )) \cap ( \overline{A} \cup \overline{X} \cup \overline{B})]$ $ = [(A \cup X \cup B) \cap (  \ \overline{A} \cup ( \overline{X} \  \cup X ) \cup B)] \cap [(A \cup (X \cup  \ \overline{X}) \cup \overline{B} \ ) \cap ( \overline{A} \cup \overline{X} \cup \overline{B})]$ By complementation and absorption laws: $(A \triangle X) \cup (X \triangle B) = [(A \cup X \cup B) \cap (  \ \overline{A} \cup U \cup B)] \cap [(A \cup U \cup \overline{B} \ ) \cap ( \overline{A} \cup \overline{X} \cup \overline{B})]$ $= [(A \cup X \cup B) \cap U] \cap [U \cap ( \overline{A} \cup \overline{X} \cup \overline{B})]$ $= (A \cup X \cup B) \cap ( \overline{A} \cup \overline{X} \cup \overline{B})$ $= (A \cup X \cup B) \cap ( \overline{A \cup X \cup B} )$ $= (A \cup X \cup B) - ( A \cup X \cup B )$ $\blacksquare$ Now, going back to the original implication we have: $A \cap B \subseteq X \subseteq A \cup B \implies A \cup B \cup X = A \cup B \ \land \ A \cap B \cap X = A \cap B$ $\implies (A \cup B \cup X ) - (A \cap B \cap X) = (A \cup B) -(A \cap B)$ $\implies (A \cup B \cup X ) - (A \cap B \cap X) = A \triangle B$ So, applying lemma 2: $(A \triangle X) \cup (X \triangle B) = A \triangle B \implies |(A \triangle X) \cup (X \triangle B) | = |A \triangle B|$ $\implies |(A \triangle X)| + |(X \triangle B) | - |(A \triangle X) \cap (X \triangle B)|= |A \triangle B|$ And finally, by lemma 1: $|(A \triangle X)| + |(X \triangle B) | - |\emptyset|= |A \triangle B|$ $|(A \triangle X)| + |(X \triangle B) | - 0= |A \triangle B|$ $|(A \triangle X)| + |(X \triangle B) |= |A \triangle B|$ $\blacksquare$ Is this part of the proof correct? And how do I prove that $|A \triangle X| + |X \triangle
 B| = |A \triangle B| \implies A \cap B \subseteq X \subseteq A \cup B$ ?? I don't know what to do on that part.","['elementary-set-theory', 'proof-writing']"
3111128,Expected value of falling factorials from axioms of Poisson process,"Falling factorial, $(x)_n$ , is the product of biggest $n$ terms in factorial, $(x)_n = x(x-1)(x-2)\cdot \ldots \cdot (x-n+1)$ . Or the number of ways to color the set of $n$ objects into different colors if you have $x$ possible colors. Using formula for probabilities of Poisson random variable $X$ one can find that $E((X)_n)=\lambda^n$ . Derivation with $\lambda=1$ . One may also use probability generating function. But I guess the answer is too beautiful to obtain it by boring summation :) Starting from three axioms one may find $E(X)=\lambda$ without calculating probabilities. Just divide the interval $[0;1]$ into $n$ parts and make $n$ goes to infinity. I wonder whether it is possible to find all the expected values $E((X)_n)$ directly from the defining axioms of Poisson process (without derivation of probabilities)??? not a homework — just curiosity :)","['poisson-distribution', 'probability']"
3111155,Weak$^*$ convergence in $L^1$,"Suppose we have a sequence $\{f_n\}$ of $L^1$ functions such that $||f_n||_1 \leq K_1$ , then viewing $L^1(\mathbb{R}) \subset \mathcal{M}(\mathbb{R})$ where $\mathcal{M}(\mathbb{R})$ is the space of Radon measure which is isomorphic to the dual space of $C_c(\mathbb{R})$ , we can extract a subsequence $\{{f_n}_k\}$ which converges to a Radon measure in the weak $^*$ topology on $\mathcal{M}(\mathbb{R})$ .
Suppose in addition we have $$\int_{\mathbb{R}}f_n=K_2, \forall n \in \mathbb{N}$$ and $f_n \rightarrow f$ pointwise almost everywhere in $\mathbb{R}/\{0\} $ Then can we say that $\exists C \in \mathbb{R}$ such that $$\int_{\mathbb{R}} {f_n}_k\Phi(x)dx=\int_{\mathbb{R}} f(x)\Phi(x)dx+ C\Phi(0),$$ i.e the weak $^*$ limit is of the subsequence is of the form $f+C\delta_{0}$ ?","['weak-convergence', 'measure-theory', 'functional-analysis', 'analysis']"
3111169,"King on reduced chessboard $2\times 2$ moving randomly, what is the probability that it ends up in one of the corners after $1000$ moves?","As mentioned in the title, we have a chessboard $2\times2$ , the king moves with equal probability to each square on the chessboard. King begins from the left upper corner. What is the approximate probability that the king will be standing in the bottom right corner after a thousand moves? I know how to solve it when the number of steps goes to infinity, which would make the probability $1/4$ . But is there any trick to do it for a $1000$ moves or is it just ""relatively"" large number so that I would use my method to solve it as $n$ goes to infinity?","['markov-chains', 'probability']"
3111184,"Please prove $n! > (n/3)^n$ is true, without using mathematical induction","Please prove $n! > \left(\frac{n}{3}\right)^n$ is true, without using mathematical induction.
I've proved it using mathematical induction, but our teacher asked us to derive it using limits $n$ pre-calculus. I tried, but I'm stuck.",['limits']
3111193,Intuition for a limit found using L'Hôpital (geometry),"$OPR$ is a sector with central angle $\theta$ . $A(\theta)$ is the area of the segment bounded by the line $PR$ and the arc $PR$ and $B(\theta)$ is the area of the triangle $PQR$ . The ratio $$\frac{A(\theta)}{B(\theta)} = \frac{r^2(\theta-\sin\theta)}{2\frac{r^2(1-\cos\theta)\sin\theta}{2}}$$ If I use L'Hôpital to find $\lim_{\theta \rightarrow 0} \frac{A(\theta)}{B(\theta)}$ , then the answer is $\frac{1}{3}$ . I was wondering if there is any intuition for this limit and if this should be the answer you 'expect'... I originally thought it would be $0$ and also not sure why this is wrong. As a smaller note, most books with this example write $\theta \rightarrow 0^+$ , but is it OK to still write $\theta \rightarrow 0$ ?","['limits', 'geometry', 'intuition']"
3111198,Shouldn't tan(x) be a continuous function,"If a 'function ' is continuous it must have its limit at $a$ equal to $f(a)$ . 
Considering tan(x) one may say that it is continuous for its domain but not a continuous function for all real numbers $\mathbb{R}$ . But isn't saying that wrong?  Simply because if we take $\mathbb{R}$ as the input of our function our function is no longer a function because a function is defined when 'each' input value has a well defined output which is not the case for all inputs in $\mathbb{R}$ . And thus our function is no more a function and we cannot say if it's a non-continuous 'function'","['continuity', 'trigonometry']"
3111199,Problem in Ergodic theory,"Let $(X,T,\mu)$ be a classical dynamical system, where $(X,\mu)$ is a probability measure space and $T$ is a measure preserving invertible transformation. Let $U$ be the unitary on $L^{2}(X,\mu)$ defined by $U(f)(s)=f(T^{-1}s)$ . If $T$ is ergodic and not weak mixing then why is it true that $U$ has at least an eigenvalue other than $1$ ? P.S. Somewhere I found that the underlying Hilbert space can be decomposed into the space generated by eigen vectors and the weak mixing part. I would be delighted someone can give me some suggestions on that.","['eigenvalues-eigenvectors', 'operator-theory', 'ergodic-theory', 'functional-analysis', 'dynamical-systems']"
3111220,How to find $\lim \limits_{x \to 0} \frac{\sqrt{x^3+4x^2}} {x^2-x}$ when $x\to 0^+$ and when $x\to 0^-$?,"I'm trying to find: $$ \lim \limits_{x \to 0} \frac{\sqrt{x^3+4x^2}} {x^2-x} $$ Since there is a discontinuity at $x=0$ I know that I have to take the limits from both sides, $x \to 0^+$ and $x \to 0^-$ , and check if they're equal. If I factor it I get: $$ \lim \limits_{x \to 0} \left(\frac{\sqrt{x+4}} {x-1}\right) = - 2$$ Is this the same as $x \to 0^+$ ? If so, how do I approach the problem for $x \to 0^-$ ? If not, how do I do I do it from both sides?","['limits', 'calculus']"
3111260,Error evaluating $ \lim_{x\to 0}\frac{x-\tan x}{x^3} $,"Evaluate the limit: $$
\lim_{x\to 0}\frac{x-\tan x}{x^3}
$$ I solved it like this, $$
\lim_{x\to 0} \left({1\over x^2} - \frac{\tan x}{x^3}\right)
=\lim_{x\to 0}\left({1\over x^2} - {\tan x\over x}\cdot {1\over x^2}\right)
$$ Now using the property $$
\lim_{x\to 0} \frac{\tan x}{x}=1
$$ we have: $$
\lim_{x \to 0} \left(\frac{1}{x^2} - \frac{1}{x^2}\right)=0
$$ Please explain my error! How can I avoid such errors?
I have it's correct solution. All I want to know is what I did wrong here? Note: English is my second language.","['limits', 'calculus', 'trigonometry', 'infinity']"
3111265,Prove convergence rate to zero increases as n increases,"I am trying to prove that in the equation: $$F_{i,n} =
 \prod_{0}^{n-1}𝑃_{𝑖−(𝑛+1)}𝑃_{𝑖−𝑛}$$ as $i > n$ , $i > 1$ , $n \ge 1$ , $n \longrightarrow
 \infty$ and $𝑃_{𝑖−𝑛} \longrightarrow 0$ , $$F \longrightarrow 0$$ as
   well. In the aformentioned equation, P is a real number (actually probability of something), raning from 0 to 1. Any idea how I might be able to prove the theory in mathematics apart from proving it based on empirical results? P.S. I am a Research Scientist with Computer Science background, lacking knowledge in theoretical mathematics. So any help related to this topic would help. Thank you in advance. With Regards, http://somdipdey.co.uk/","['proof-explanation', 'proof-verification', 'discrete-mathematics']"
3111272,Computation of completion of a local ring,"Let $X=\mathrm{Spec}(\mathbb{R}[a,b]/(a^2+b^2+1))$ and consider the closed point $p=(a)$ . 
  I would like to compute the completion of $\mathcal{O}_{X,p}$ w.r.t. to its maximal ideal $\mathfrak m$ . Since $(\mathcal{O}_{X,p},\mathfrak{m}) $ is a noetherian regular local ring of dimension one with residue field $\mathbb{C}$ , so will be $(\widehat{\mathcal{O}}_{X, \, p},\widehat{\mathfrak m})$ and by Cohen's structure theorem this should yield that $\widehat{\mathcal{O}}_{X, \, p}\cong \mathbb{C}[[t]]$ (I copied the argument from this MO answer ). I tried to write out the explicit isomorphism, but I fail to see how to construct a map $\mathbb{C}[[t]]\to (\mathbb{R}[a,b]/(a^2+b^2+1))_\mathfrak m)/a^n=\mathcal{O}_{X,p}/\mathfrak m^n$ for $n\geq 3$ .","['formal-completions', 'ring-theory', 'algebraic-geometry', 'commutative-algebra']"
3111273,Consequence of local duality,"Let $(R,\mathfrak m)$ be a local Cohen-Macaulay ring of dimension $d$ with a canonical module $\omega.$ Let $M,N$ be maximal Cohen-Macaulay $R$ -modules. Then local duality implies $$\mathrm{Ext}^i(N,\mathrm{Hom}(M,\omega))=\mathrm{Ext}^i(M,\mathrm{Hom}(N,\omega)).$$ It will be really helpful if someone explains the above equality using local duality. (Here by local duality I mean $\mathrm{Hom}(\mathrm{Ext}^i(M,\omega),E)\cong H_{\mathfrak m}^{d-i}(M)$ where $E$ is the injective hull of residue field of $R.$ )","['local-rings', 'algebraic-geometry', 'commutative-algebra']"
3111285,Bounded 3 values random walk,"I have an array of size M, for each shift one random variable $X_i$ enter and one exit. The $X_i$ r.v. are iid and $X_i = \pm1$ with $p=\frac{1}{2}$ .
Assuming that the sum of the $M$ $X_i$ random variables starts from $0$ , what is the probability that after $n$ shifts has ""revisited"" $0$ ? I've tried to solve this considering the random walk $\sum_{i}\varepsilon_i$ where $\varepsilon_i=\pm2$ with $p=\frac{1}{4}$ and $\varepsilon_i=0$ with $p=\frac{1}{2}$ , could you please help me? As a start I can ignore that the array is bounded. Any hint is well accepted (if there is a solution that uses the generating functions it's even better, as these are more familiar to me), thank you all.","['random-walk', 'probability']"
3111296,Difference between normal functions and discontinuous functions?,"For the function, $$y=\frac{x^2-1}{x-1}$$ The denominator cannot be zero. So $$\lim_{x\to1}\frac{x^2-1}{x-1}=\lim_{x\to1}(x+1)=2$$ "" $y=\frac{x^2-1}{x-1}$ is discontinuous at $x=1$ since $y$ is undefined at that point. This leaves a gap in the curve. The limit tells us that $y\to2$ as $x\to1$ , so the gap is at $(1,2)$ ."" This is a bit from my maths textbook (Maths In Focus) about discontinuous functions. This is cool and good. However, what I'm having a bit of trouble with is understanding how can $y=\frac{x^2-1}{x-1}$ be equal to $y=x+1$ when they generate different graphs. The graph $y=\frac{x^2-1}{x-1}$ is discontinuous while $y=x+1$ is continuous. What I don't understand is why does the graph of $y=x+1$ change when it is multiplied by $\frac{x-1}{x-1}$ , which is essentially multiplication by one. How can you change a value/graph when all you do is multiply by one? I have searched over the internet and there isn't a single article/video explaining this specifically, which probably means I'm misunderstanding something or overlooking something fundamental. Any clarification on what exactly is going on would be deeply appreciated.",['functions']
3111299,"If $\tan(x_1) \cdots\tan(x_n)=1$ for acute $x_i$, then does it follow that $\cos(x_1)+\cdots+\cos(x_n) \leq n\sqrt{2}/2$?","It is easily seen that if $x,y\in[0,\pi/2)$ satisfy $\tan(x)\tan(y)=1$ , then $$\cos(x)+\cos(y)\le\sqrt 2$$ A much more delicate fact is that if $\tan(x)\tan(y)\tan(z)=1$ (while $0\le x,y,z<\pi/2$ ), then $$\cos(x)+\cos(y)+\cos(z)\le \frac{3\sqrt 2}2$$ I can prove this, but the proof is a little complicated; can anybody suggest a nice, simple proof? As a generalization, suppose that $n\ge 4$ , $x_1,\dotsc,x_n\in[0,\pi/2)$ , and $\tan(x_1)\dotsb\tan(x_n)=1$ . Does it follow that $$ \cos(x_1)+\dotsb+\cos(x_n) \le \frac{n\sqrt 2}2? $$","['inequality', 'substitution', 'cauchy-schwarz-inequality', 'optimization', 'trigonometry']"
3111304,An open cover of $\mathbb{R}^n$ and $\mathbb{C}^n$,"Consider the following subset of $\mathbb{R}^n$ : \begin{eqnarray}V_i:=\{(p_1, \cdots, p_n)\in\mathbb{R}^n|x^n-p_1x^{n-1}+p_2x^{n-2}-\cdots+(-1)^np_n=0\text{ has at least one root with multiplicity at least }i\}\end{eqnarray} Then one can see that $V_i$ is a real affine subvariety of $\mathbb{R}^n$ , and that there is this string of inclusions \begin{eqnarray}V_n\subset V_{n-1}\subset\cdots\subset V_1=\mathbb{R}^n\end{eqnarray} Question : Is there a finite open cover $\{U_j\}_{j=1}^m$ of $\mathbb{R}^n$ such that the following holds: For any subset $J\subset\{1, 2, \cdots, m\}$ , let $i_J$ be the largest number such that $\left(\bigcap_{j\in J}U_j\right)\cap V_{i_J}\neq\varnothing$ . Then for any $i$ satisfying $1\leq i\leq i_J$ , $\left(\bigcap_{j\in J}U_j\right)\cap V_i$ deformation retracts to a point $v_{i_J}$ in $V_{i_J}$ . Does such a finite open cover exist if $\mathbb{R}^n$ is replaced by $\mathbb{C}^n$ ? For $n=2$ and $\mathbb{R}^2$ , the question is easy: in this case, $V_2$ is the parabola $p_1^2-4p_2=0$ . We may simply take the open cover $\{U_1:=\mathbb{R}^2\}$ and both $U_1\cap V_1=\mathbb{R}^2$ and $U_1\cap V_2=V_2$ deformation retract to $v_2:=(0, 0)$ in $V_2$ .","['algebraic-geometry', 'roots', 'polynomials', 'algebraic-topology']"
3111327,"Is it true, that for any two non-isomorphic finite groups $G$ and $H$ there exists such a group word $w$, that $|V_w(G)| \neq |V_w(H)|$?","Is it true, that for any two non-isomorphic finite groups $G$ and $H$ there exists such a group word $w$ , that $|V_w(G)| \neq |V_w(H)|$ ? Here $V_w(G)$ stands for the verbal subgroup of $H$ , generated by the group word $w$ . Initially, the question I wanted to ask was: “Is it true, that for any two non-isomorphic finite groups $G$ and $H$ there exist such a one-word generated group variety $\mathfrak{U}$ , such that $G$ is in $U$ and $H$ is not?” However, then I found an obvious counterexample: $C_2$ and $C_2 \times C_2$ . So, I decided to require a stronger condition. For the statement of the main question that counterexample already fails. Moreover, if $H$ and $G$ are counterexamples, then they are required to have following properties: 1) They are both non-abelian: If one of the groups is abelian the other group is not, then their commutator subgroups have different orders. If both tho both are abelian, then by the classification of finite abelian groups they can be decomposed into direct products of primary cyclic groups. $$G = (C_2^{g_2} \times ... \times C_{2^i}^{g_{2^i}} \times ...) \times ... \times (C_{p_j}^{g_{p_j}} \times ... \times C_{{p_j}^i}^{g_{{p_j}^i}} \times ...) \times ... $$ $$H = (C_2^{h_2} \times ... \times C_{2^i}^{h_{2^i}} \times ...) \times ... \times (C_{p_j}^{h_{p_j}} \times ... \times C_{{p_j}^i}^{h_{{p_j}^i}} \times ...) \times ... $$ where $$g_{{p_j}^i} = \log_p|V_{{p_j}^{i-1}}(G)| - 2\log_p|V_{{p_j}^{i}}(G)| + \log_p|V_{{p_j}^{i+1}}(G)|$$ $$h_{{p_j}^i} = \log_p|V_{{p_j}^{i-1}}(H)| - 2\log_p|V_{{p_j}^{i}}(H)| + \log_p|V_{{p_j}^{i+1}}(H)|$$ It is not hard to see, that if they satisfy the condition, they are isomorphic. 2) They have the same order: $$|G| = |V_x(G)| = |V_x(H)| = |H|$$ 3) They have the same exponent: $$exp(G) = min\{n \in \mathbb{N}: |V_{x^n}(G)| = 1\} = min\{n \in \mathbb{N}: |V_{x^n}(H)| = 1\} = exp(H)$$ 4) $var(G) = var(H)$ : A group $G$ satisfies an identity $w$ iff $|V_w(G)| = 1$ . 5) $\forall w \in F_\infty  \text{ } V_w(G) = G \iff V_w(H) = H$ Moreover, if $G$ and $H$ are counterexamples with the least possible order, they have to satisfy the additional condition: For every group word $w$ , if $V_w(G)$ is a non-trivial proper verbal subgroup, then $V_w(G) \cong V_w(H)$ and $\frac{G}{V_w(G)} \cong \frac{H}{V_w(H)}$ . If there is a group word $w$ , such that $V_w(G)$ and $V_w(H)$ are non-trivial proper verbal subgroups of the corresponding groups and not isomorphic to each other, then they are the counterexample of lesser order, as $V_{u(x_1, ... , x_m)}(V_{w(x_1, ... , x_n}(G)) = V_{w(u(x_{11}, ... , x_{m1}), ..., u(x_{1n}, ... , x_{mn}))}(G)$ . If for every group word $w$ , if $V_w(G)$ is a non-trivial proper verbal subgroup, then $V_w(G) \cong V_w(H)$ and there is a group word $w$ , such that $V_w(G)$ and $V_w(H)$ are non-trivial proper subgroups of the corresponding groups and $\frac{G}{V_w(G)}$ and $\frac{H}{V_w(H)}$ are not isomorphic to each other, then $\frac{G}{V_w(G)}$ and $\frac{H}{V_w(H)}$ are a counterexample as $V_u(\frac{G}{V_w(G)}) \cong \frac{V_u(G)}{V_w(G) \cap V_u(G)}$ However, even with all those facts in my hands, I still failed to get the contradiction.","['universal-algebra', 'finite-groups', 'verbal-subgroups', 'abstract-algebra', 'group-theory']"
3111343,"Proving $\int_0^\infty x^ne^{-tx}\frac{\sin x}xdx=\frac{\sin n\theta}{(1+t^2)^{n/2}}(n-1)!$, where $\theta=\arcsin\frac1{\sqrt{1+t^2}}$","I have stumbled upon the PDF by Leo Goldmakher from University of Toronto, Canada, named Differentiation Under The Integral Sign (PDF link) . In that pdf, he gave a theorem (his Theorem 1, at the end of the note) which states as below: For any real number $t\geq0$ and any integer $n\geq 1$ we have $$\int_0^\infty x^n e^{-tx}\frac{\sin{x}}{x} dx = \frac{\sin{n\theta}}{(1+t^2)^{\frac{n}{2}}} (n-1)!$$ where $\theta =\arcsin {\frac{1}{\sqrt{1+t^2}}}$ Now I didn't understand how to prove this theorem? I know $$\displaystyle\int_0^\infty x^n e^{-tx}\sin{x} \frac{dx}{x} =\frac{(n-1)!}{(1+t^2)^n}\left(\frac{i}{2}\left[(t-i)^n-(t+i)^n\right]\right)$$ The Author  says substituting the above into (1) and tidying up a bit leads to this theorem. I didn't understand how did he arrive at this theorem?","['self-learning', 'improper-integrals', 'calculus', 'trigonometry', 'derivatives']"
3111369,Degrees of freedom of the set of positive definitive matrices,"If I am not wrong, the set of definite positive matrices with real coefficients is a convex cone without the vertex, which is the null matrix. What is the number of degrees of freedom for this set of matrices? Please apologize me for not being formal. I will try to explain better my question. In statistics, a positive definite matrix has a special meaning, since it is a variance-covariance matrix. In statistical inference, we want to have a number of samples that it is at least equal to the number of degrees of freedom of the parameter space.
Suppose to have a set of observation from a multivariate normal model $N(\mathbf{0}, \mathbf{V})$ , and that you want to do inference on $\mathbf{V}$ . You need a number of samples at least equal to the number of degrees of freedom for the matrix $\mathbf{V}$ . So, my question, how many degrees of freedom has the matrix $\mathbf{V}$ ? How to formalize this concept? Is ""number degrees of freedom"" a synonym for ""dimension of a manifold"" or something similar? Thank you for the clarification.","['statistics', 'positive-definite']"
3111398,Do we need the axiom of replacement (ZFC) to define a product of structures (Universal Algebra)?,"Do we need the axiom of replacement (ZFC) to define a product of structures (Universal Algebra)? Here $\mathrm{V}$ is the class of all sets. From my perspective here is how the product of structures is defined. Definitions: Let $ X $ be a set. An arity-relation in $ X $ is a 2-tuple $ \left(\vphantom{n}\smash{\breve{n}},\asymp\right) $ , where $ \vphantom{n}\smash{\breve{n}}\in \mathbb{N} $ and $ {\asymp}\subseteq X^{\vphantom{n}\smash{\breve{n}}} $ . Let $ X $ be a set. An arity-operation on $ X $ is a 2-tuple $ \left(\vphantom{n}\smash{\dot{n}},\odot\right) $ , where $ \vphantom{n}\smash{\dot{n}}\in \mathbb{N} $ and $ {\odot}:X^{\vphantom{n}\smash{\dot{n}}}\to X $ . A structure is a 3-tuple $ \left(X,(\vphantom{n}\smash{\breve{n}},\asymp),(\vphantom{n}\smash{\dot{n}},\odot)\right) $ , where $ X $ is a set and $ \{(\vphantom{n}\smash{\breve{n}}_{(\cdot)},\asymp_{(\cdot)})\} $ is a family of arity-relations in $ X $ and $ \{(\vphantom{n}\smash{\dot{n}}_{(\cdot)},\odot_{(\cdot)})\} $ is a family of  arity-operations on $ X $ . Let $ \{(X_\lambda,(m,\asymp^{\lambda}),(n,\odot^{\lambda}))\}_{\lambda\in\Lambda} $ be a family of structures.
Fix nonempty $ I\subseteq \Lambda $ .
The product of $ (X_i,(m,\asymp^i),(n,\odot^i)) $ for $ i\in I $ , denoted $ \prod_{i\in I}(X_i,(m,\asymp^i),(n,\odot^i)) $ , is the 3-tuple $ \left(Y,(m,\vphantom{\asymp}\smash{\bar{\asymp}}),(n,\vphantom{\odot}\smash{\bar{\odot}})\right) $ , where $ Y=\prod_{i\in I}X_i $ and $ \vphantom{\asymp}\smash{\bar{\asymp}}_{(\cdot)}:\operatorname{dom}\left(m\right)\to \mathrm{V}:\alpha\mapsto \{y^{(\cdot)}\in Y^{m_\alpha}:\left(\forall i\in I\right)[\pi_i\circ y\in ({\asymp}^i)_\alpha]\} $ and $ \vphantom{\odot}\smash{\bar{\odot}}_{(\cdot)}:\operatorname{dom}\left(n\right)\to\mathrm{V}:\alpha\mapsto \{(y^{(\cdot)},z)\in Y^{n_\alpha}\times Y:\left(\forall i\in I\right)[(\odot^i)_\alpha(\pi_i\circ y)=z_i]\} $ . Question: Can the codomain of $\bar{\asymp}_{(\cdot)}$ or $\bar{\odot}_{(\cdot)}$ , currently written as $\mathrm{V}$ , be described without the axiom of replacement? Context: I am interested in the axiom of replacement because of how little it is required in everyday mathematics (embedded in ZFC). Originally I was looking into Birkhoff's HSP Theorem in Universal Algebra where no relations beyond equality is in a structure. I simply extended the definition to include relations because that is what Model Theory studies. I provide this context because I received some downvotes on this question without explanation. EDIT1: I want to add that I am not commited to the definition above. If you know of a different definition that can work in ZFC (minus replacement), then I will consider that an answer to the titular question. For example, the definition of an ordered pair $(a,b):=\{\{a\},\{a,b\}\}$ commonly used has the benefit of the separation axiom and power set axiom (among others) being enough to define products of sets. If one insists to define ordered pairs in an unknown fashion that so that $(a,b)=(x,y)\iff a=x\text{ and }b=y$ , then the replacement axiom would be required to do much of the same things. For example, see $\text {dom}(R)$ and $\text {ran}(R)$ exists for any definition of order pair.","['elementary-set-theory', 'definition', 'universal-algebra']"
3111421,Subsets of $\mathbb Z/n\mathbb Z$ disjoint with some of its shifts,"Are there any descriptions of all subsets $X$ of $\mathbb Z/n\mathbb Z$ with the following property:  there exists $a\ne 0$ in $\mathbb Z/n\mathbb Z$ such that $X$ is disjoint with $X + a = \{x + a \pmod n\mid x \in X\}$ ? For example, for $n=5$ , any set with 1 or 2 elements satisfies this property. I used to believe that the same holds for any subsets of size less than or equal to $\frac{n}{2}$ , but for $n=6$ , a counterexample can be easily constructed. Probably there’s some well-known theorem about it?","['additive-combinatorics', 'finite-groups', 'abstract-algebra', 'sumset', 'group-theory']"
3111432,Is the infinitely distributed lag process strong mixing?,"Suppose the process $\{X_{t}:t\in Z\}$ is absolutely continuous distributed and strong mixing with the coefficient $\alpha(s)$ defined as \begin{equation}
\alpha(s) \equiv \sup \{ |P(A\cap B) - P(A)P(B)| : -\infty < t<\infty, A \in X^{t}_{-\infty}, B\in X^{\infty}_{t+s} \}.
\end{equation} Here $\alpha(s) \rightarrow 0$ at a polynomial rate. Now, consider a new process of infinitely distributed lags as $Y_{t} = \lambda(L)X_{t} \equiv X_{t} + \lambda_{1}X_{t-1} + \lambda_{2} X_{t-2} + \ldots$ , where $\lambda_{k} = \rho^{k}$ with $|\rho|<1$ . My question is: is the process $Y_{t}$ strong mixing (or under what conditions)? If $Y_{t}$ can be strong mixing,  what is the mixing coefficient of $Y_{t}$ ? The same as $\alpha(s)$ ？","['stochastic-processes', 'probability-theory', 'mixing']"
3111444,Find out the value of the integral $\int_{-2}^{2} \lfloor x^2-1\rfloor dx$,"Find out the value of the integral $$\int_{-2}^{2} \lfloor x^2-1\rfloor dx$$ where $[x]$ denotes the floor function (i.e., $[x]$ is the greatest integer $\le x$ .) My attempt ..... $$\int_{-2}^2 \lfloor x^2 – 1\rfloor dx = 2\int_0^2 \lfloor x^2-1\rfloor dx$$ Because $\lfloor x^2 – 1\rfloor$ is even. $$2\int_0^2 \lfloor x^2-1\rfloor dx =\\ 2\int_0^1 \lfloor x^2-1\rfloor dx+2\int_1^{\sqrt{2}} \lfloor x^2-1\rfloor dx +2\int_{\sqrt{2}}^{\sqrt{3}} \lfloor x^2-1\rfloor dx + 2\int_{\sqrt{3}}^2 \lfloor x^2-1\rfloor dx$$ But how to evaluate this or am I wrong in the whole assumption?","['integration', 'ceiling-and-floor-functions', 'definite-integrals']"
3111505,Second-order non-linear differential equation containing sgn function encountered in literature on the SYK model.,"I'm trying to solve the following differential equation: $$J^2\frac{1}{2^{q-1}}\operatorname{sgn}(\tau)e^{g(\tau)}=\partial_\tau^2\Big(\frac{1}{2q}\operatorname{sgn}(\tau)g(\tau)\Big).$$ Here $J^2$ and $q$ are constants and I want to solve for $e^{g(\tau)}$ . I encountered this equation in a paper by Gábor Sárosi et al . called ""AdS $_2$ holography and the SYK model"" (p.38) and ""Comments on the Sachdev-Ye-Kitaev model"" (p.12) by Juan Maldacena and Douglas Stanford.
According to them the general solution to this differential equation is given by: $$e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))}$$ where $$\mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}}$$ I am unable to reach the same result unfortunately. What I tried to do is solve the equation for $$|\tau|>0$$ so that I loose the sgn function (this could potentially be the problem) since I'm not sure how to deal with it otherwise. I get: $$\frac{d^2g(\tau)}{d\tau^2}=J^2\frac{q}{2^{q-2}}e^{g(\tau)}$$ It would be nice if we could make this a first order differential equation so
multiplying both sides by $$\frac{dg(\tau)}{d\tau}$$ and integrating w.r.t. $\tau$ gives: $$\int\frac{dg(\tau)}{d\tau}\frac{d^2g(\tau)}{d\tau^2}d\tau=J^2\frac{q}{2^{q-2}}\int e^{g(\tau)}\frac{dg(\tau)}{d\tau}d\tau\\$$ $$\frac{1}{2}\Big(\frac{dg(\tau)}{d\tau}\Big)^2=J^2\frac{q}{2^{q-2}}(e^{g(\tau)}+c_1)$$ Here I used integration by parts to rewrite the lhs. Rearranging a bit gives: $$\frac{dg(\tau)}{d\tau}=J\sqrt{\frac{q}{2^{q-3}}}\sqrt{(e^{g(\tau)}+c_1)}$$ Now that the equation is a separable first order differential equation we can integrate to find the general solution: $$\int\frac{dg(\tau)}{\sqrt{(e^{g(\tau)}+c_1)}} =J\sqrt{\frac{q}{2^{q-3}}}\int d\tau$$ $$-\frac{2}{\sqrt{c_1}} \operatorname{artanh}\Big(\frac{\sqrt{e^{g(\tau)}+c_1}}{\sqrt{c_1}}\Big) =J\sqrt{\frac{q}{2^{q-3}}}(\tau+c_2)$$ So that: $$e^{g(\tau)} =c_1\bigg(\tanh^2\Big(\mathcal{J}\sqrt{c_1}(\tau+c_2)\Big)-1\bigg)$$ where again $$\mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}}$$ Obviously this is not the same as the general solution they state in their paper. Hopefully someone can help me see where I go wrong. If I check the solution they give in the paper $\Big(e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))}\Big)$ it does work out: $$\frac{d^2g(\tau)}{d\tau^2}=\mathcal{J}^2e^{g(\tau)}$$ $$\frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))}=\frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))}$$ But I can think of no way of finding the constants of integration ( $c_1$ and $c_2$ ) using boundary conditions $$g(0)=g(\beta)=0$$","['calculus', 'ordinary-differential-equations']"
3111517,Unbiased real vector with respect to arbitrary orthonormal basis for a finite Hilbert space,"Here I use Dirac notation to denote vectors. I would like to show that for an arbitrary orthonormal basis $\{ |\psi_k\rangle \}_{k=1}^n \subset \mathbb C^n$ , $$\langle \psi_i | \psi_j \rangle = \begin{cases} 1 & i = j \\ 0 & i \neq j \end{cases}$$ there exists phases $\{\theta_k\}_{k=1}^n \subset [0, 2\pi]$ such that $\sum_{k=1}^n e^{i\theta_k}|\psi_k\rangle \in \mathbb R^n$ . Or equivalently, there exists a real vector $|v\rangle \in \mathbb R^n (|v\rangle \neq 0)$ such that $$|\langle v|\psi_1 \rangle| = \cdots = |\langle v|\psi_n \rangle|$$ The case $n=2$ is easy. Let $$| \psi_1 \rangle = \begin{pmatrix} \alpha_1 \\ \beta_1 \end{pmatrix} \ \ \ \ 
  | \psi_2 \rangle = \begin{pmatrix} \alpha_2 \\ \beta_2 \end{pmatrix} \ \ \ \ 
  | v \rangle = \begin{pmatrix} x \\ y \end{pmatrix}$$ It is easy to check that the quadratic equation of $x, y$ $$|\langle v|\psi_1 \rangle|^2-|\langle v|\psi_2 \rangle|^2 = (|\alpha_1|^2 - |\alpha_2|^2)x^2 + (|\beta_1|^2 - |\beta_2|^2)y^2 + 2\operatorname{Re}(\alpha_1\bar\beta_1 - \alpha_2\bar\beta_2)xy= 0$$ has non-trivial real roots. However, it seems that the high dimensional cases are more serious. Are there any suggestions to me?","['vector-spaces', 'probability']"
3111529,Probability on Hilbert spaces,"I have been looking to generalize a result concerning random variables with values in $\mathbb{R}^d$ to random variable with values in function spaces (in particular a space of smooth functions). I noticed that expressing many of my conditions in the $\mathbb{R}^d$ -case as norms and inner-products made it very easy to translate my results into the functional case. It seemed very well-behaved, so I pondered if maybe further generalization was possible. Does anyone have a reference about probability theory on Hilbert spaces in general, i.e. how to define/calculate expectations, variances, covariances of Hilbert space valued random variables? I'm very familiar with measure theory and probability theory but not very solid on heavy functional analysis / topology. I'd like to avoid thinking of stochastic processes as indexed families of regular random variables but instead view them as points in some function space (Hilbert space) and then use the tools of the function space to do probability. Maybe this isn't possible and it is just wishful thinking from me but a solid book or two would be much appreciated!","['hilbert-spaces', 'probability-theory', 'reference-request']"
3111547,Finding $\int^{\frac{\pi}{8}}_{0}\ln(\tan x)\mathrm dx$,"Finding $\displaystyle \int^{\frac{\pi}{4}}_{0}\ln(1+\cos x)\mathrm dx$ What I tried Put $\displaystyle I =\int^{\frac{\pi}{4}}_{0}\ln(1+\cos x)\mathrm dx$ \begin{align*}
I&=\int^{\frac{\pi}{4}}_{0}\ln(1+\cos x)\mathrm dx\\
&=\int^{\frac{\pi}{4}}_{0}\ln\bigg(2\cos^2\frac{x}{2}\bigg)\mathrm dx\\
&=\frac{\pi}{4}\ln(2)+4\int^{\frac{\pi}{8}}_{0}\ln(\cos x)\mathrm dx\\
&=\frac{\pi}{4}\ln(2)+4\left[\left[x\ln(\cos x)\right]^{\frac{\pi}{8}}_{0}+\int^{\frac{\pi}{8}}_{0}\ln(\tan x)\mathrm dx\right]\\
&=\frac{\pi}{8}\ln(2)+\frac{\pi}{2}\ln\left(\cos \frac{\pi}{8}\right)+4\int^{\frac{\pi}{8}}_{0}\ln(\tan x)\mathrm dx
\end{align*} How do I solve it? Help me, please.","['integration', 'definite-integrals', 'closed-form']"
3111616,"How do I evaluate $\int\limits_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{x^\alpha(1+x^\alpha)} \mathrm dx$, where $\alpha \in \mathbb{R}$","\begin{align*}
\int_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{x^\alpha(1+x^\alpha)} \mathrm dx &= 
\int_{0}^{+\infty} e^{-\frac{1}{x}}\left(\frac{1+x^\alpha}{x^\alpha(1+x^\alpha)}  -  \frac{x^\alpha}{x^\alpha(1+x^\alpha)}\right)dx\\ &= \int_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{x^\alpha} \mathrm dx - \int_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{1+x^\alpha} \mathrm dx
\end{align*} So I get : $$\int_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{x^\alpha} \mathrm dx = \Gamma(\alpha-1)$$ But I don´t know how to evaluate : $$- \int_{0}^{+\infty} \frac{e^{-\frac{1}{x}}}{1+x^\alpha} \mathrm dx$$ Thanks in advance for your replies.","['integration', 'calculus']"
3111627,Is the solution to the Poisson equation an analytic function in general?,I am wondering this because I just came from my PDE's 2 class and we talked about the regularity of the Laplace equation and elliptic functions DE's. My professor the stated the following which we didn't prove: If $\bigtriangleup u = 0$ then $u$ is analytic $\Rightarrow$ we can express $u$ as a Taylor Series I found this very interesting and I was wondering if the same fact would hold for the Poisson Equation ( $\bigtriangleup u=f$ ) does this give that $u$ must also be analytic ? If so I would love to see a proof as I haven't been able to find a proof showing it is or is not true.,"['harmonic-analysis', 'poissons-equation', 'functional-analysis', 'partial-differential-equations']"
3111650,Are neural networks with bounded parameters a compact subset of the Banach space of continuous functions?,"Let $d, n \in \mathbb{N}$ . Moreover, let $D \subset \mathbb{R}^d$ be compact and denote with $\mathcal{C}(D, \mathbb{R}^n) $ the set of continuous functions from $D$ to $\mathbb{R}^n$ . Then $\mathcal{C}(D, \mathbb{R}^n) $ is a Banach space with the usual maximum norm $\lVert f\rVert_{\infty} := \max_{x \in D} \lVert f(x) \rVert $ . Now let $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous (activation) function, $K>0$ and $\Gamma$ be a set of fixed hyperparameters for an artificial neural network such that the input layer has $d$ nodes and the output layer has $n$ nodes. Denote with $\mathcal{H}_{\sigma, \Gamma, K} \subset \mathcal{C}(D, \mathbb{R}^n)$ the set of all neural networks with activation function $\sigma$ and fixed architecture $\Gamma$ for which all $l \in \mathbb{N}$ parameters $\theta_1,...,\theta_l$ (the biases and edgeweights) are smaller or equal than $K$ in absolute value, i.e. $\lvert \theta_i \rvert \leq K$ for all parameters $\theta_i$ of the neural network. I want to rigorously show, that $\mathcal{H}_{\sigma, \Gamma, K}$ forms a compact topological subspace of the Banach space $(\mathcal{C}(D, \mathbb{R}^n), \lVert . \rVert_{\infty}) $ . One idea I have is to show that the mapping $j:[-K,K]^l \rightarrow \mathcal{H}_{\sigma, \Gamma, K}$ which maps a set of parameters $\Theta$ to a neural network $\Phi_{\Theta}$ is continuous. Since $[-K,K]^l$ is compact, this would imply compactness of $j([-K,K]^l) = \mathcal{H}_{\sigma, \Gamma, K}$ . So far, however, I have not been successful. I am grateful for any advice! Kind regards and thank you very much, Joker","['banach-spaces', 'neural-networks', 'artificial-intelligence', 'functional-analysis', 'compactness']"
3111687,Spaces of submanifolds,"Let $M$ and $N$ be smooth manifolds with $\dim M<\dim N$ . The spaces $\mathrm{Emb}(M,N)\subset\mathrm{Imm}(M,N)$ of smooth embeddings and immersions $f:M\to N$ , respectively, are infinite dimensional Frechet manifolds. They are open subsets of the space $C^{\infty}(M,N)$ of smooth maps $M\to N$ , or equivalently, of the space $\Gamma(M,M\times N)$ of smooth sections of the trivial bundle $M\times N\to M$ . The diffeomorphism group $\mathrm{Diff}(M)$ acts naturally on these spaces by precomposition. My question is what structure the quotient spaces $\mathrm{Emb}(M,N)/\mathrm{Diff}(M)$ and $\mathrm{Imm}(M,N)/\mathrm{Diff}(M)$ have. These spaces seem to arise naturally in geometric problems, where only the image $f(M)\subset N$ of a map $f:M\to N$ is of concern, not the actual map itself. 1) Do $\mathrm{Emb}(M,N)/\mathrm{Diff}(M)$ and $\mathrm{Imm}(M,N)/\mathrm{Diff}(M)$ have a smooth manifold structure? 2) What are their topological properties, e.g. homology/homotopy? Can we compute these via some kind of ""Morse"" functions?","['differential-topology', 'smooth-manifolds', 'algebraic-topology', 'differential-geometry']"
3111719,What is the distribution of $(1+X^2)e^{-X^2/2}$ when $X$ is Cauchy?,"If $X$ is a Cauchy random variable with $f(x)=\frac{1}{\pi}\frac{1}{1+x^{2}}$ , what is the distribution of $Y= (1+X^2)e^{-X^2/2}$ ? What I tried: I was thinking I may be able to use Jacobian, but I am unable to invert the function $Y=f(X)$ to find solutions. Any Idea how to go about this problem?","['statistics', 'probability-distributions', 'probability']"
3111732,A line segment joins the middle points of each diagonal of a quadrilateral with known sides. What is sum of the squares of the diagonals?,"$|DF|=|FB|$ $|AE|=|EC|$ What is $|AC|^2 + |BD|^2= \ ?$ I asked one of my classmates for the solution, he said: $$|AC|^2+|BD|^2+4 \cdot3^2 = 7^2 +4^2 +5^2+6^2$$ He didn't tell me how he obtained it, either because he wouldn't want to tell me (I remotely know him) or he just memorized the formula. Provided that this is a high school geometry problem, can you show me how above equation is obtained in simple terms?","['quadrilateral', 'euclidean-geometry', 'geometry']"
3111752,"Find the total number of 20 digit codes that can be formed using the numbers {0,1,2,3,4}, such that consecutive digits have a difference of 1?","To start with an example of such a code can be: $34321210123212343210$ I have no clue how this property can be mathematically counted. I actually even have a short solution of this question which I could not make any sense of. I will type the solution at the bottom of this body, one may choose to solve it themselves first and then refer the solution. I could notice some obvious properties that if there occurs the digit $4,$ it must have $3$ on both sides. And similarly the digit $0$ must have $1$ on both sides. I am not able to think any way to solve this problem. Please help me with an explanatory solution. Thank you in advance. . . . . . . . . . . . . . . . . . . . ............................................................................................................................................... SOLUTION:
Let $A_n$ denote number of codes which end in $0$ or $4,$ $B_n$ denote number of codes which end in $1$ or $3$ and $C_n$ denote number of codes which ends with $2.$ $A_{n+1} = B_n = C_{n+1}, B_{n+1} = A_n + 2C_n$ Thus $N= 2^3\cdot 3^9$","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
3111773,General form of elements of $SU(2)$,"$SU(2)$ is the set of $2\times 2$ complex matrices $A$ satisfying $AA^*=I$ and $\det(A)=1$ where $A^*$ denotes the conjugate transpose of $A$ and $I$ is the identity matrix. I've seen everywhere that the elements of $SU(2)$ can be represented as $\begin{pmatrix}\alpha&\beta\\-\bar\beta&\bar\alpha\end{pmatrix}$ where $\vert\alpha\vert^2+\vert\beta\vert^2=1$ , but I've been having a stupidly hard time working out the arithmetic for this claim. I was able to show that if $\begin{pmatrix}\alpha&\beta\\\gamma&\delta\end{pmatrix}$ is in $SU(2)$ (or even just $U(2)$ ), then $\vert\alpha\vert=\vert\delta\vert$ and $\vert\beta\vert=\vert\gamma\vert$ , but I'm stuck from there. I know I must use the equation $\alpha\delta-\beta\gamma=1$ to obtain the desired result, but I'm failing to do so.","['linear-algebra', 'lie-groups']"
3111774,What is the inverse operation of a gradient?,"I notice that the function $$f(x,y,x;a,b,c) = ke^{-a/x-b/y-c/z}$$ has partial derivatives $$\nabla f = \begin{bmatrix}
\partial f / \partial x
\\ 
\partial f / \partial y
\\ 
\partial f / \partial z
\end{bmatrix} = \begin{bmatrix}
a f / x^2
\\ 
b f / y^2
\\ 
c f / z^2
\end{bmatrix}$$ I want an operation, let's call it $\beta(\nabla f)$ , that does this: $$\beta(\nabla f)=ke^{-a/x-b/y-c/z}$$ I.e., takes the gradient and returns the original function. I was thinking along the lines of $$\beta(\nabla f)=\int\int\int \nabla f\cdot \mathbf{n} \:\:dx \:dy\:dz \:\:; \: \mathbf{n} = [1,1,1]$$ But it seems pretty arbitrary to throw that $\mathbf{n}$ in there. Plus, this would basically be an indefinite volume integral, which I'm not sure exists (volume integrals always have to be definite integrals, right?).","['indefinite-integrals', 'multivariable-calculus', 'inverse-function', 'vector-analysis']"
3111788,Ramanujan's Master Theorem relation to Analytic Continuation,"$\DeclareMathOperator{Re}{Re}$ To provide some background, this is a question based on establishing the identity $$\int_0^\infty \frac{v^{s-1}}{1+v}\,dv=\frac{\pi}{\sin \pi s},\qquad 0<\Re s<1$$ using Ramanujan's Master Theorem (RMT). See the question asked here for the full details related to it. In this answer by user @mrtaurho, we use the geometric series expansion $$\frac{1}{1+v}=\sum_{k=0}^\infty (-v)^k.$$ However, the geometric series clearly isn't valid for $|v|\geq 1$ , and it was explained that the radius of convergence didn't play a role in the proof of RMT and so doesn't matter too much here, only that the underlying structure is revealed when considering the geometric series representation. This made me think of analytic continuation since we only need an identity to be valid in some region to be able to extend a function outside of that region; it seems like the geometric series is only valid for $|v|<1$ but this allows us to extend the connection beyond just $(0,1)$ . So my question is this: Is there some connection between RMT and analytic continuation? Or perhaps the connection is finer than RMT and is only a small part of some detail in its proof?","['integration', 'complex-analysis', 'power-series', 'improper-integrals']"
3111813,What is the smallest number of $45^\circ$–$60^\circ$–$75^\circ$ triangles in non-trivial substitution tiling?,"Let base = $45^\circ$ – $60^\circ$ – $75^\circ$ triangle. Over at What is the smallest number of base s that a square can be divided into? it was determined that 23 base were needed to make a $45^\circ$ – $45^\circ$ – $90^\circ$ triangle. How about non-trivial dissections of base into similar triangles?  Start with base and divide it into smaller copies of base . Ideally the method should be specific to base and wouldn't work with other triangles. Also, at least one of the internal triangles should have no edges parallel to the original triangle. What are the simplest non-trivial dissections of base into similar triangles?","['geometry', 'triangles', 'recreational-mathematics', 'tiling', 'fractals']"
3111823,Is $[F(x)]^{-1}=F(-x)$ obvious for the given matrix $F(x)$?,$$F(x)=\begin{bmatrix}\cos x&-\sin x&0\\\sin x&\cos x&0\\0&0&1\end{bmatrix}$$ Is it very obvious (in the sense that without any calculations) that $[F(x)]^{-1}=F(-x)$ ? My book directly writes this without any explanation. How is this evident without calculation?,['matrices']
3111830,Prove that the covariant derivative commutes with musical isomorphisms,"Suppose I have a covector field $\omega$ and a covariant derivative $\nabla_{X}$ for some vector field $X$ on a Riemannian manifold $(M, g)$ . Define $X^{\flat} \in \mathfrak{X}^{*}(M)$ as $X^{\flat}(Y) = g(X, Y)$ , and $\omega^{\sharp}$ as the unique vector for which $\omega(X) = g(\omega^{\sharp}, X)$ ; it's easily checked that these musical maps are mutually inverse, so they're isomorphisms between $\mathfrak{X}(M)$ and $\mathfrak{X}^{*}(M)$ . Anyway, I want to prove that, for the Levi-Civita connection $\nabla$ on $(M, g)$ , we have $\nabla_{X}(\omega^{\sharp}) = (\nabla_{X}\omega)^{\sharp}.$ In several books I've been reading, it says that this follows immediately from $\nabla g = 0$ . However, I keep getting that this is wrong, for example if $(E_{1},...,E_{n})$ is a local orthonormal frame on $M$ , I get that $\nabla_{E_{i}}(E_{j}^{*}{^{\sharp}})$ does not equal $(\nabla_{E_{i}}E_{j}^{*})^{\sharp}$ . Here's what I've been trying: 1. Finding $\nabla_{E_{i}}(E_{j}^{*}{^{\sharp}})$ : I start by finding the components of $E_{j}^{*}{^{\sharp}}$ $$(E_{j}^{*}{^{\sharp}})^{k} = \sum_{l} g^{kl} (E_{j}^{*})_{l} = \delta_{kj}.$$ Therefore, $E_{j}^{*}{^{\sharp}} = E_{j}$ . Now, I want to find $\nabla_{E_{i}} E_{j}^{*}{^{\sharp}} = \nabla_{E_{i}} E_{j}$ . $$\nabla_{E_{i}} E_{j} = \sum_{k} \Gamma_{ij}^{k}E_{k},$$ where $\Gamma_{ij}^{k}$ are the Christoffel symbols with respect to this frame. 2. Finding $(\nabla_{E_{i}}E_{j}^{*})^{\sharp}$ : again, I begin by finding the components, this time of $(\nabla_{E_{i}}E_{j}^{*})^{\sharp}$ : $$ ((\nabla_{E_{i}}E_{j}^{*})^{\sharp})^{k} = \sum_{l} g^{kl} (\nabla_{E_{i}}E_{j}^{*})_{l},$$ so I want to find $\nabla_{E_{i}}E_{j}^{*}(E_{l})$ . Since $\nabla_{E_{i}}$ is a tensor derivative, we have: $$ 0 = \nabla_{E_{i}}(E_{j}^{*}(E_{l})) = \nabla_{E_{i}}E_{j}^{*}(E_{l}) + E_{j}^{*}(\nabla_{E_{i}}E_{l}),$$ so $\nabla_{E_{i}}E_{j}^{*}(E_{l}) = - E_{j}^{*}(\sum_{k}\Gamma_{il}^{k}E_{k}) = - \Gamma_{il}^{j}$ . Finally, $$ ((\nabla_{E_{i}}E_{j}^{*})^{\sharp})^{k} = \sum_{l} g^{kl} (\nabla_{E_{i}}E_{j}^{*})_{l} = -\Gamma_{ik}^{j},$$ so $(\nabla_{E_{i}}E_{j}^{*})^{\sharp} = \sum_{k} (-\Gamma_{ik}^{j}E_{k}) = -\sum_{k}\Gamma_{ik}^{j}E_{k}$ . Since I don't know that $\Gamma_{ik}^{j} + \Gamma_{ij}^{k} = 0$ , I don't see why these two are equal. Did I make a mistake somewhere; is there an easier proof of this fact?","['vector-fields', 'connections', 'tensors', 'differential-geometry']"
