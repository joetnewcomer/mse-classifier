question_id,title,body,tags
3606592,Proof of some Riemann problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 4 years ago . Improve this question I was wondering if anyone is aware of this proof about the Riemann Hypothesis. https://arxiv.org/pdf/1004.4143.pdf What I like about it is his analytic approach using the behaviour of general series at singular points. Since I haven't heard anything about it, I presume there must be some flaw somewhere, but I can not find it. To me it seems legit. In particular I want to point out his main result. I added short variations of the proofs which should be sufficient to capture the entire essence. The paper with that regard is somewhat cumbersome. He resums the $\zeta$ -function, compares it with its integral representation and obtains an expression valid for $\sigma>0$ ( $s=\sigma+it$ ) $$(s-1)\zeta(s) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \tag{0}$$ where $S_n(s)=-\frac{1}{n} \sum_{k=1}^n \binom{n}{k} (-1)^k \, k^{1-s}$ . Then he defines a function $\zeta(s,x)$ valid for $|x|<1$ by $$(s-1)\zeta(s,x) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \, x^n$$ which converges to (0) for $\sigma>0$ as $x\rightarrow 1$ . He uses a result for the asymptotic behaviour of $S_n(s)$ as $n$ gets large $$\frac{S_n(s)}{n+1} \sim \frac{1}{n(n+1)(\log n)^{1-s} \Gamma(s)} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.1}$$ which probably inspired him to look for functions $\phi_s(x)=\sum_{n=0}^\infty \phi_n(s) \, x^n$ whose coefficients $\phi_n(s)$ have the same asymptotic behaviour with respect to $n$ . Such a function is $$\phi_s(x)=(1-x)\left(\frac{\log(1-x)}{-x}\right)^{s}$$ whose coefficients obey $$\phi_n(s) \sim \frac{-s}{n^2 (\log n)^{1-s}} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.2}$$ as $n$ gets large. Since $\phi_s'(x)$ diverges to $\infty$ as $x\rightarrow 1$ it follows that $\zeta'(s,x)$ diverges to $\infty$ as $x\rightarrow 1$ . This allows him to establish the result (after a lot of details) $$\lim_{x\rightarrow 1} \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} = \lim_{n\rightarrow \infty} \frac{\frac{nS_n(s)}{n+1}}{n\phi_n(s)} = \frac{-1}{\Gamma(s+1)} \, . \tag{1}$$ In a similar but different way as in the pre-print this can be easily seen as follows. proof: Define $a_n\equiv\frac{nS_n(s)}{n+1}$ and $b_n\equiv n\phi_n(s)$ , so that $\lim_{n\rightarrow \infty}\frac{a_n}{b_n} = \frac{-1}{\Gamma(s+1)} \equiv c$ . This is equivalent to saying $\frac{a_n}{b_n}=c+\epsilon_n$ for some complex sequence $\epsilon_n$ that vanishes as $n\rightarrow \infty$ . From the asymptotics (as.1) and (as.2) it is seen that for large $n$ $$|\epsilon_n| = {\cal O}\left(1/\log n\right) \, .$$ To establish the result, $a_n = cb_n + \epsilon_n b_n$ is used to construct $$\sum_{n=1}^\infty a_n x^n = c \sum_{n=1}^\infty b_n x^n + \sum_{n=1}^\infty \epsilon_n b_n x^n$$ from which it then follows if we can show that $\sum_{n=1}^\infty |\epsilon_n b_n|$ converges. But from the asymptotics for $|\epsilon_n|$ and $|b_n|$ this sum can be bounded $$\sum_{n=2}^\infty |\epsilon_n b_n| \leq C|s| \sum_{n=2}^\infty \frac{1}{n(\log n)^{2-\sigma}}$$ for a suitable constant $C<\infty$ . The convergence of the right sum follows by the integral test for $0<\sigma<1$ . From this it can be deduced: If $s$ is a non-trivial zero, then $$\lim_{x\rightarrow 1} \frac{(s-1)\zeta(s,x)}{\phi_s(x)} = \frac{-1}{\Gamma(s+1)} \, . \tag{2}$$ proof: Define $\delta(x)\equiv \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} + \frac{1}{\Gamma(s+1)}$ , so that the previous result is compactly written as $\lim_{x\rightarrow 1} \delta(x) = 0$ . Let $s$ be a non-trivial zero and integrate (the previous proof showed that $\zeta'(s,x)$ has the same singularity at $x=1$ as $\phi_s'(x)$ which is manifestly integrable in a neighbourhood $\leq 1$ ) $$\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t = -(s-1)\zeta(s,x) - \frac{\phi_s(x)}{\Gamma(s+1)}$$ or $$-\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)} = \frac{(s-1)\zeta(s,x)}{\phi_s(x)} + \frac{1}{\Gamma(s+1)} \, .$$ It suffices to show that the LHS vanishes absolutely as $x\rightarrow 1$ , for this $$\left|\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)}\right| \leq \frac{\int_x^1 |\delta(t)| \, |\phi_s'(t)| \, {\rm d}t}{|\phi_s(x)|} \, .$$ The nominator ( $f(x)$ ) and denominator ( $g(x)$ ) on the RHS are differentiable on the open interval $(x,1)$ and finite at $x<1$ , so Rolle's theorem (for the function $h(t)=f(t) - \frac{f(x)}{g(x)} \, g(t)$ ) or equivalently L'Hospital can be applied. The RHS becomes in the limit $x\rightarrow 1$ $$\lim_{x\rightarrow 1} |\delta(x)| \, \frac{|\phi_s'(x)|}{-|\phi_s(x)|'} = 0 \cdot 1 = 0 \, .$$ In the final step he considers the ratio of (2) for the zero $s$ and $1-s$ $$ \lim_{x\rightarrow 1} \left|\frac{(s-1)\zeta(s,x)}{\phi_s(x)}\frac{\phi_{1-s}(x)}{(-s)\zeta(1-s,x)}\right| = \left|\frac{\Gamma(2-s)}{\Gamma(s+1)}\right| \tag{3}$$ or $$ \lim_{x\rightarrow 1} \left|\frac{\phi_{1-s}(x)}{\phi_s(x)}\right| = \lim_{x\rightarrow 1} \left( \frac{\log(1-x)}{-x} \right)^{1-2\sigma} = \left|\frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)}\right| $$ which is an equation only possible when $\sigma =1/2$ in which case the LHS is 1, which is indeed true for any real $t$ on the RHS when using the functional equation for the $\zeta$ -function $$\left| \frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)} \right| = \left| \frac{(2\pi)^{1-s}}{2\sin(\pi s/2)\Gamma(s)} \right| = 1 $$ and $\left|\Gamma(1/2+it)\right|^2 = \frac{\pi}{\cosh(\pi t)}$ . Wrong... As most probably have expected, the proof is fundamentally flawed. I've been thinking about this for some time now and am somewhat embarassed to say that the solution is so trivial. It would have saved me lots of time, if somebody here would have pointed it out earlier, since it could have been seen from my given exposition, but here we go... The problem with the proof lies within its formulation. It utterly overcomplicates matter where not necessary. This distracts from the relevant point and superficially makes it look as if it were correct, just like shenanigans try to distract their prey. While I have already simplified the steps significantly (I believe), it already more or less starts with Equation (1). It is not even wrong, but should have been more properly written as $$(s-1)\zeta'(s,x) = -\frac{\phi_s'(x)}{\Gamma(s+1)} + {\cal O}(1)$$ for $x\rightarrow 1$ , in order to keep track of error-terms. This shows that both functions share the same logarithmic singularity at $x=1$ . Now when integrating this Equation from $x$ to $1$ , it is not yet assumed that $s$ is a zero. This gives rise to the identity $$(s-1)\zeta(s,x)=(s-1)\zeta(s) - \frac{\phi_s(x)}{\Gamma(s+1)} + {\cal O}(1-x)$$ as $x\rightarrow 1$ , which is just the expansion of $(s-1)\zeta(s,x)$ about the singular point $x=1$ . Now if $s=s_0$ is a zero with $0<\Re(s_0)<1$ , then the constant term of the expansion vanishes identically, so that Equation (3) becomes $$\lim_{x\rightarrow 1} \left|\frac{(s_0-1)\zeta(s_0,x)}{(-s_0)\zeta(1-s_0,x)}\right| = \lim_{x\rightarrow 1} \left| \frac{\phi_{s_0}(x)}{\phi_{1-s_0}(x)} \frac{\Gamma(2-s_0)}{\Gamma(s_0+1)}\right| \\ \stackrel{!}{=} \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s,1)}{(-s)\zeta(1-s,1)} \right| = \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s)}{(-s)\zeta(1-s)} \right| \, .$$ This is where the flaw becomes apparent, because the interchange of limits is generally wrong and would have to be crucially justified. Evidently it would be the main part of the proof! I should also mention, that Equation (0) - even though he might have found it independently - can actually be traced back to a version from H. Hasse (1930).","['riemann-zeta', 'divergent-series', 'analysis']"
3606632,$\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0$ for a $C^\infty$ function $f:\Bbb R^n\to \Bbb R^n$,"Let $f,g:\Bbb R^n \to \Bbb R^n$ be two $C^\infty$ functions. I am trying to prove the following statements: (1) $\displaystyle\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0$ $(1\leq i\leq n)$ , where $Df$ is the derivative of $f$ (with $ij$ -entry given by $\frac{\partial f_i}{\partial x_j}$ ), and $\text{cof}(A)$ is the cofactor matrix of $A$ . (2) If $U$ is a bounded open connected subset of $\Bbb R^n$ having smooth boundary, and if $f=g$ on $\partial U$ , then $\int_U \det(Df)dx=\int_U \det (Dg)dx$ . For (1), by definition of the cofactor matrix, we have $(\text{cof}(Df))_{ij}= (-1)^{i+j} \frac{\partial f_i}{\partial x_j}\det(M_{ij})$ , where $M_{ij}$ is the $ij$ -minor of $\text{cof}(Df)$ . But I can't see how to proceed. For (2), I think I should use some kind of Stoke's theorem, but I have no idea. Any hints for these? Thanks in advance.","['integration', 'smooth-functions', 'analysis', 'real-analysis', 'linear-algebra']"
3606699,Modern definition of unimodality for multivariate distributions and characterizations thereof,"Let $X$ be a random variable on $\mathbb R^n$ with density $f$ (w.r.t Lebesgue). Question. What is a generally agreed upon definition of unimodularity (of the distribution) of $X$ ? Are there any known charaterizations of this property ? Notes In the special case $n=1$ , unimodality is agreed in the literature to mean that There is a point $c \in \mathbb R$ (called the mode of $X$ ) such that $f$ is increasing on $(-\infty, c)$ and decreasing on $(c,\infty)$ . In this case, it is well-known (Khintchine, 1938) that $X$ has unimodal distribution iff $X \overset{d}{=} UZ$ , where $U$ is the uniformly distributed r.v on $[0, 1]$ and $Z$ a r.v on $\mathbb R$ which is independent of $U$ . Update: $\alpha$ -unimodularity The following definition is an interesting candidate for the concept of multivariate modularity. Definition. Let $\alpha \in [1,n]$ . A random variable $X$ on $\mathbb R^d$ is said to $\alpha$ -unimodular about the point $c \in \mathbb R^d$ iff for every bounded measurable function $g:\mathbb R^n \rightarrow [0,\infty)$ the function $t \mapsto t^{n-\alpha}\mathbb E[g(t(X-c))]$ is non-decreasing on $[0,\infty]$ . The unimodularity index of $X$ is defined to be the smallest value of $\alpha \in [1,n]$ such that $X$ is $\alpha$ -unimodular. The following is a characterization of multivariate unimodularity which parallels the univariate case. Theorem (Dharmadhikari et Joag-Dev, 1988). $X$ is $\alpha$ -unimodular iff $X \overset{d}{=} U^{1/\alpha}Z$ , where $U$ is the uniformly distributed r.v on $[0, 1]$ and $Z$ a r.v on $\mathbb R^d$ which is independent of $U$ .","['statistics', 'probability-distributions', 'definition', 'probability']"
3606707,Cubic roots and difference of cubes in limits $\lim\limits_{n\to\infty} (\sqrt[3]{n^6-6n^4+1} - n^2)$,Find the limit: $$\lim\limits_{n\to\infty} (\sqrt[3]{n^6-6n^4+1} - n^2)$$ I applied the identity: $$a^3 - b^3 = (a-b)(a^2+ab+b^2)$$ by multiplying the numerator and denominator by the complementary part. $$\lim\limits_{n\to\infty} \frac{(\sqrt[3]{n^6-6n^4+1} - n^2)(\sqrt[3]{(n^6-6n^4+1)^2} + n^2\sqrt[3]{n^6-6n^4+1}+ n^4)}{(\sqrt[3]{(n^6-6n^4+1)^2} + n^2\sqrt[3]{n^6-6n^4+1}+ n^4)}=\lim\limits_{n\to\infty} \frac{n^6-6n^4+1 - n^6}{(\sqrt[3]{(n^6(1-6/n^2+1/n^6))^2} + n^2\sqrt[3]{n^6(1-6/n^2+1/n^6)}+ n^4)}=\lim\limits_{n\to\infty} \frac{-6n^4+1}{(n^4\sqrt[3]{(1-6/n^2+1/n^6)^2} + n^4\sqrt[3]{(1-6/n^2+1/n^6)}+ n^4)}=\lim\limits_{n\to\infty} \frac{-6+1/n^4}{(\sqrt[3]{(1-6/n^2+1/n^6)^2} + \sqrt[3]{(1-6/n^2+1/n^6)}+ 1)}=\frac{-6}{3}=-2$$ Is there any more elegant way to approach the problem?,"['limits', 'calculus', 'radicals']"
3606757,A result in the solution of wave equation,"Let $u$ be a smooth solution of the initial-value problem $$
\left\{
\begin{aligned}
u_{tt} - u_{xx} &= 0\qquad \text{in}~ \mathbb{R}\times (0,\infty)\\
u=g,\quad u_t&=h\qquad \text{on}~\mathbb{R}\times \{t=0\}
\end{aligned}
\right.
$$ Suppose that $g$ and $h$ are sufficiently smooth and have compact supports. \
  Define $p(t) := \frac{1}{2} \int\limits_{-\infty}^\infty u_x(x,t)^2\,d x \quad\text{and}\quad k(t) := \frac{1}{2}\int\limits_{-\infty}^\infty u_t(x,t)^2\,d x\,.$ Prove that $E(t):= k(t) + p(t)$ is constant in $t\geq 0$ . Prove that $p(t)=k(t)$ for all large enough times $t$ . My Attempt: For the part 1, I was trying to prove that $\frac{d}{dt}E(t)=0$ . Hence we have, \begin{align}
E^{'}(t)&=k^{'}(t)+p^{'}(t)\\
&=\int\limits_{-\infty}^\infty u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{tt}\\
&=\int\limits_{-\infty}^\infty  u_x.u_{xt}+\int\limits_{-\infty}^\infty u_t.u_{xx}
\end{align} But after that what should I do.. Also for part 2, I know that from the D'Alambert's Formula, we have $$u(x,t)=\frac{1}{2}[g(x+t)-g(x-t)]+\frac{1}{2}\int\limits_{x-t}^{x+t}h(y)dy \tag{1}$$ So I'm trying to prove that when $t\to\infty$ , $p(t)-k(t)=\frac{1}{2}\int\limits_{-\infty}^{\infty}u_x^2-u_t^2=0$ . But I'm having a hard time in differentiating $(1)$ to get the suitable values. Appreciate your help","['multivariable-calculus', 'partial-differential-equations']"
3606761,"Linear algebra - Prove that for any isomorphism there is an ""identity basis(?)""","I'd appreciate if you could help me with this question.
What I thought about so far is showing that for any basis $B$ , we know that $[T]_B$ is invertible so there are $k$ elementary matrices such that $E^k*...*E^1*[T]_B=I$ . I'm not sure if it's a good direction. Here is the question: Let $V$ be a vector space over $F$ . Prove that for any isomorphism $T : V \rightarrow F^n$ , there exists a basis $B$ for $V$ such that $T=Is_B$ , meaning $T(v)=[v]_B$ Note: if you see a solution I'd be happy if you could write a hint in addition to your solution, so i can try it myself first. Also, I'm not a native english speaker so if anything is not clear please tell me and I'll fix it.","['linear-algebra', 'vector-space-isomorphism']"
3606781,Annihilator is a smooth submanifold of the cotangent bundle,"Let $X$ be a smooth manifold and let $Y$ be a smooth submanifold. Denote by $$
TY^0=\{(q,p)\in T^*X\colon q\in Y,p|_{T_qY}=0\}\subseteq T^*X
$$ The annihilator of $Y$ . Is is it true that the annihilator is a smooth Lagrangian submanifold of $T^*X?$ , and how to see this? The given hint is too use a suitable atlas of charts for $X$ but I don't see how this helps me. Do I have to starts which charts on $X$ , restrict them to $Y$ and from there on construct charts for $TY^0$ ?","['manifolds', 'general-topology', 'differential-geometry']"
3606898,Isomorphism between the actions of $G$ on $G/H$ and $G/gHg^{-1}$?,"I'm doing Aluffi's Chapter 0, and Exercise 9.13 asks: Prove that for all subgroups $H$ of a group $G$ and for all $g\in G$ , $G/H$ and $G/(gHg^{-1})$ (endowed with the action of G by left-multiplication) are isomorphic in G-Set. My plan is to first find a bijection between $G/H$ and $G/(gHg^{-1})$ , then prove that the map commutes with the action. The first map I thought of was just $\varphi:G/H \longrightarrow G/(gHg^{-1})$ defined by $aH \mapsto a(gHg^{-1})$ , but the problem is that this is not well defined, since we can have $aH = a'H$ but $a(gHg^{-1}) \neq a'(gHg^{-1})$ . What would be well-defined is $aH \mapsto g(aH)g^{-1}$ . This works as a bijection between $G/H$ and $G/(gHg^{-1})$ , but the problem is that is does not commute with the action of $G$ . If the action wasn't strictly left-multiplication, then I could make the map work (by making it conjugate instead), but Aluffi is specifying that the action must be left multiplication. So now I don't really know what to do. On one hand, my gut feeling tells me that left-multiplication is just not ""compatible"" with conjugation, and maybe there's no way to make $\varphi$ commute with the action of $G$ . On the other hand, Aluffi's exercises are pretty well done, and I might be just missing something?","['group-actions', 'group-theory', 'abstract-algebra']"
3606917,An application of Serre Duality,"This is problem 2 of Chapter 5 in Voisin's book Hodge Theory and Complex Algebraic Geometry I . Let $X$ be a connected compact complex manifold of dimension $n$ and let $L$ be a holomorphic line bundle on $X$ . We assume that there exists an integer $N>0$ , such that $$H^0(X,L^{\otimes N})\neq 0.$$ Show that if $H^n(X,L\otimes K_X)\neq0$ , the line bundle $L$ is trivial. The question is named after An application of Serre duality but I did not figure out how.","['complex-geometry', 'algebraic-geometry', 'line-bundles']"
3606920,Sum of normal vectors to the surfaces of a tetrahedron is zero.,"Show that the sum of some outer-pointing vectors perpendicular on the faces of a tetrahedron
  which are proportional to the areas of the faces is the zero vector. Can somebody give me some advice about how to start? I don't this this is something hard to prove with the cross product.","['area', 'vectors', 'cross-product', 'geometry', 'solid-geometry']"
3606955,True or false?: $\exists x\in\Bbb{R}\;\forall y\in\Bbb{R}(x^2>y^2\to x>y)$,"Determine whether $$\exists x\in\Bbb{R}\;\forall y\in\Bbb{R}(x^2>y^2\to x>y)$$ is true or false. My attempt: First answer? Is true. Pick $x=1\in\Bbb{R}$ . Then for all $y\in\Bbb{R}$ , we have that $1^2=1>y^2$ . This implies $|y|<1$ , so $-1<y<1$ , so $y>-1$ and $y<1$ . By reduction we have $y<1$ , and we are done. Second answer? We will prove that this holds for any $x,y$ . From $x^2>y^2$ we have $|x|>|y|$ . That is, $|y|<x$ . Then, $-x<y<x$ , so $y>-x$ and $y<x$ . Again by simplification, we end up with $y<x$ , which is the same as $x>y$ . Questions Are both answers a correct way to solve the problem? In the first answer, could I pick any value of $x$ ? Can I pick $x=0$ ? In that case I should have $0>y^2$ , which is false, so the statement is true. In the second answer did we prove this statement: $\forall x\in\Bbb{R}\;\forall y\in\Bbb{R}(x^2>y^2\to x>y)$ ?","['logic', 'discrete-mathematics']"
3607075,Variance of the stopping time. Matching Problem,"I have a following problem: $N$ guests leave their hats in a heap and collect them in random order. Those who by chance get back their own hats happily go home. The remaining ones yet again throw their hats in a heap and collect them randomly. Those who get back their own hats happily go home. ...This continues until the stopping time $T$ , when all gentlemen go home with their own hats on. Find $E[T]$ and $Var[T]$ My progress: I've successfully proved that if $X_n$ is the number of guests present after the $n$ -th round, then $X_n$$+$$n$ is a martingale. 
According to the optimal stopping theorem $E$ [ $X_T$$+$$T$ ]= $E$ [ $X_0$ ]= $N$ and therefore $E[T]$ = $N$ But I am now stuck on finding the variance. My guess is that I should consider something like $X_n^2$ + (something that depends on n), prove that it's a martingale too and then somehow find the variance from this. If someone can help me with this last part I would be very grateful.","['stochastic-processes', 'martingales', 'probability-theory', 'probability']"
3607118,A high road to the Kähler identities?,"Let $(X, \omega)$ be a compact Kähler manifold. The Kähler identities express the commutator relations between the operators $$\partial, \ \ \overline{\partial}, \ \ L,$$ and their adjoints. To be clear, $L : \Lambda^k \to \Lambda^{k+2}$ is the Lefschetz operator $\alpha \mapsto \alpha \wedge \omega$ . Although the identities are not hard to prove, remembering them is a challenge that I am yet to conquer. This leads me to ask whether there is a more enlightened perspective to be had when it comes to the Kähler identities. Edit: This is also discussed in the following MO post: https://mathoverflow.net/questions/64520/global-algebraic-proof-of-the-kahler-identities","['hodge-theory', 'complex-geometry', 'algebraic-geometry', 'kahler-manifolds']"
3607231,Intuitively predict the direction of the acceleration vector of a single parameter vector valued function,"The velocity vector of a Vector valued function (or position function), (for example $f(t)= \lt x(t),y(t) \gt$ )
 seems to always be tangent to the curve of motion, its pretty intuitive why that is. However intuition about what the acceleration vector $f ^{\prime \prime}(t)$ looks like or where does it point is hard to find. Can I somehow predict or get an intuition on what the acceleration vector might be or where does it point to? how?.","['physics', 'derivatives', 'vector-analysis', 'differential-geometry']"
3607285,Question about the proof of Fubini's 'other' theorem on the term by term differentiation of infinite sum of increasing functions,"This is problem 25.15 Fubini's 'other' theorem from Rene Schilling's Measures, Integrals, and Martingales. Let $(f_n)_n$ be a sequence of monotone increasing functions $f_n: [a,b] \to \mathbb{R}$ . If the series $s(x):= \sum_{n=1}^\infty f_n(x)$ converges, then $s'(x)$ exists a.e. and is given by $s'(x)=\sum_{n=1}^\infty f_n'(x)$ a.e. Below is the solution to this problem. I can follow all steps of the proof except the last, where it says the first part of the proof applied to this series implies that we can differentiate this series term by term and $\sum_k (s'(x)-s_{n_k}'(x)) $ converges.
I can't figure out which part of the proof justifies term by term differentiation here. I would greatly appreciate any help.","['measure-theory', 'derivatives', 'analysis', 'real-analysis']"
3607375,"$G$ is a finite group and $a \in G$ s.t. $a$ has exactly $2$ conjugates. Then $G$ contains a non-trivial, proper normal subgroup.","$G$ is a finite group and $a \in G$ s.t. $a$ has exactly $2$ conjugates. Then $G$ contains a non-trivial, proper normal subgroup. This question proved to be more difficult  than I had expected, and as a result I had to do a lot of reading. By reading various proofs, I compiled the full proof into the following and I have some questions about it: Proof: Given $2$ conjugates of $a$ means $|G|/|N(a)| = 2$ where $N(a)$ is the normalizer (or centralizer) of $a$ . So $N(a) \neq G$ which means $N(a)$ is a proper subgroup of $G$ . Note : Suppose if $N(a) = \{e\}$ . For any $g \in G$ $gag^{-1}$ is either $a$ or the other conjugate. If $gag^{-1} = a \implies ga = ag \implies g \in N(a) \implies g = e$ . So let's consider $x,y \in G$ s.t. $x \neq y$ . Then $xax^{-1} = yay^{-1} \implies a(x^{-1}y) = (x^{-1}y) a \implies (x^{-1}y) \in N(a) \implies (x^{-1}y) = e \implies x=y$ which is a contradiction. So $N(a)\neq \{e\}$ . Since $[G:N(a)]=2$ then $N(a)$ is normal in $G$ . $\Box$ Now I have a problem with this proof. I don't like that there are 2 hypothesis floating around. That is, $N(a) = \{e\}$ and $x \neq y$ especially because I didn't really "" use "" the fact $x \neq y$ in an effective way. My only reason for writing ""... $x=y$ which is a contradiction"" is that I just didn't want that to be true. Beyond that, I don't see how I can effectively argue it. Can I please get some help with filling holes in my proof?","['normal-subgroups', 'group-theory', 'proof-writing', 'solution-verification']"
3607398,Chess tournament combinatorics,"There are $2n$ players in a chess tournament. The first round consists of pairing the players to
participate in $n$ matches with every player playing one match. In terms of $n,$ how many ways can
this pairing take place? The first pair of people can be chosen from $\binom{2n}{2}.$ Since we chose the first pair, the next pair will be chosen from an amount of $\binom{2n-2}{2}.$ This pattern continues till $\binom{2}{2}$ which is for the $n$ th pair. Hence, the pairings will be $$\binom{2n}{2}\cdot\binom{2n-2}{2}\cdot...\cdot\binom{2}{2}.$$ Dividing by $n!$ gets the ways $2n$ people can be chosen to play $n$ games. How do I simplify this?",['combinatorics']
3607423,Finding all Lines Passing Through a Point Given the Product of Intercepts,"How does one find the equation of all lines passing through a point (Ex. $(6, -1)$ ), satisfying the condition that the product of their $x$ and $y$ intercepts must equal some number $c$ (Ex. $3$ )? As far as I understand this can be conceptualized as finding the equation of the line containing the points $(6,-1), (a,0), (0,b)$ where $a$ is the $x$ -intercept, $b$ is the $y$ -intercept and $ab=3$ . I've tried finding the slope with $m=\frac{y_{2}-y_{1}}{x_{2}-x_{1}}$ and have gotten $m=\frac{-1}{6-a}$ and $m=\frac{-1-b}{6}$ . I've also solved for $a$ and $b$ in terms of $m$ and tried susbstituting these values into the equation of a line ( $y=mx+b$ ) but I just cant eliminate enough variables to solve for anything useful. Feel like I'm missing something obvious, I'm not even sure if there is more than one equation that satisfies the conditions.","['coordinate-systems', 'algebra-precalculus', 'geometry']"
3607474,On Spivak's introduction to one parameter group of diffeomorphisms,"In Chapter 5 of M. Spivak's A Comprehensive Introduction To Differential Geometry he states the existence and uniqueness of integral curves $\alpha_x:(-b,b)\rightarrow U $ of an ODE with initial condition $$\alpha_x'(t)=f(\alpha(t)) $$ $$\alpha_x(0)=x $$ This is proven for a map $f:U\rightarrow \mathbb{R}^n$ defined in an open set $U$ and every $x\in B_a(x_0)$ for some $a>0$ and $x_0\in U$ such that the closed ball $\overline{B}_{2a}(x_0)\subset U$ , we also ask $f$ to be locally Lipschitz (there are as well some restriction on $b$ ). Next he introduces the flow $$\alpha:(-b,b)\times B_a(x_0)\rightarrow U\quad \;\text{as }\quad (t,x)\mapsto\alpha_x(t) $$ Ok! this is all great and fairly straightforward, but the next discussion has an important step that I do not follow, here it is: He argues that $\alpha$ is continuous (proven, no problem!) and since every $\alpha_y$ satisfies the condition $\alpha_y(0)=y$ , then we have $$\alpha:\{0\}\times \overline{B} _{a/2}(x_0)\rightarrow \overline{B} _{a/2}(x_0) $$ Then by continuity of $\alpha$ and compactness of $\{0\} \times  \overline{B} _{a/2}(x_0)$ , then there exists $\epsilon>0$ such that $$\alpha:(-\epsilon,\epsilon) \times B_{a/2}(x_0)\rightarrow B_a(x_0) $$ This last step I do not get, it does not seems trivial to me the existence of such $\epsilon$ . My attempt of give a proof of such statement is the following: First, since $B_a(x_0)$ is an open neighborhood of $x_0$ and $\alpha$ is continuous we have an open set $V=(-\epsilon,\epsilon)\times B_\delta(x_0)$ for some $\epsilon,\delta>0$ , such that $$\alpha(V)\subset B_a(x_0)$$ Here compactness should play an important role to bound $\delta$ , but I cannot see this. Can someone please give me a hint or full proof of this last statement? Thanks in advance!","['ordinary-differential-equations', 'differential-geometry']"
3607538,Is square root a function?,"I just began the topic of functions in my Mathematics textbook and in the first paragraph itself, two conditions about a relation being a function were mentioned. They were : 1) For every a∈A, there exists b∈B such that (a,b)∈f
    I think  that is another way of saying that both the input and output must lie in the given sets
2) If (a,b)∈f and (a,c)∈f, then b = c
    And I think that this is another way of saying that an input has a unique output Let me know if I have misunderstood these statements. Now, let's think of the square root function... The square root of 4 is ±2, which means that there are two outputs 2 and -2 for one input 4, so there are two images of the same input, so it defies the second condition that states that a function must have a unique output/image of some input Let R represent the set of real numbers, then if square root is a function, it means that both the input and output must lie in the set R. In the case of square root of -1, the output will be i (iota) which is not a member of the set R but is a member of C (the set of complex numbers), this defies the first condition that makes a relation a function. Please let me know if I have misunderstood some concept related to functions and if square root is really a function or not. Thank You...","['elementary-set-theory', 'functions']"
3607564,Find the sum $\sum _{n=1}^{\infty}a_1a_2a_3...a_n $ where $a_{n+1}=\ln\frac{e^{a_n}-1}{a_n}$.,"Let $\{a_n\} $ be defined as follows: $a_1 \gt 0$ and $$a_{n+1}=\ln\frac{e^{a_n}-1}{a_n}$$ for $n\ge 1.$ Then the sum $$\sum_{n=1}^{\infty}a_1a_2...a_n $$ is _____________ My attempt:(By using hint ) By Given condition $a_{n+1}-a_n=\ln\frac{e^{a_n}-1}{a_n}-lne^{a_n} $ $\Rightarrow a_{n+1}-a_n=\ln\frac{1-e^{-a_n}}{a_n}       
\quad    (1) $ Now $e^x \gt 1+x \space \forall x \in R \quad (2)$ $\Rightarrow  e^{-a_n} \gt 1-a_n $ From (1) $ a_{n+1}-a_n \lt \ln\frac{1-1+a_n}{a_n}=0  $ Also $a_{n+1} \gt \ln \frac{a_n}{a_n}=0 $ using (2) for $x=a_n$ So $\{a_n\} $ is a nonotone decreasing sequence bounded below. Let $lim_{n\rightarrow \infty}a_n=l $ . Then by the given recurrence relation $l=\ln \frac{e^l-1}{l} $ $\Rightarrow e^l(l-1)+1=0 $ is satisfied by $l=0$ Hence $ \exists k\in N$ s.t $ \forall n\ge k$ ,we have $a_n \lt 1$ Let $a_1a_2...a_{k-1}=p $ Then $\sum_{n=k}^{\infty}a_1a_2...a_n=p\{a_k+a_ka_{k+1}+...\} $ $\Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p\{a_k+a_k^2+...\}$ since $a_n$ is monotone decreasing . $\Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p \frac{a_k}{1-a_k} $ So $\sum_{n=1}^{\infty}a_1a_2...a_n \lt S+p\frac{a_k}{1-a_k} $ where $ S=\sum_{n=1}^{k-1}a_1a_2...a_n $ So the given sereis is convergent. I would like to know if my workings are correct.I don't know how to find the actual sum.Also I have seen an answer here  ( What is $\sum_{n=1}^{\infty}a_1 a_2...a_n$? ) but unfortunately I can't understand it.Please help me in finding the sum.Thanks in advance.","['convergence-divergence', 'analysis', 'sequences-and-series']"
3607697,Proof that $\pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2}$,"In this post, the symbol $\sim$ means asymptotically equivalent . The relationship between $\pi$ and factorials hinges on Stirling's formula : $$n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n\implies n!^2\sim 2\pi n\left(\frac{n}{e}\right)^{2n}.$$ In a similar manner, $$(2n)!\sim \sqrt{4\pi n}\left(\frac{2n}{e}\right)^{2n}.$$ Then, by taking the ratio of the two quantities and simplifying one gets $$\frac{(2n)!}{n!^2}\sim \frac{2^{2n}}{\sqrt{\pi n}}.$$ By squaring both sides and further manipulation: $$\pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2}.$$ Also, note that the last limit for $\pi$ above and $$\frac{n!^2}{2n}\left(\frac{e}{n}\right)^{2n}$$ (which can be obtained as a limit for $\pi$ from Stirling's formula dircetly) have a different rate of convergence. My question is, how can one justify the manipulations (e.g. squaring both sides) when the expressions are not equivalent but only asymptotically equivalent? When I used this I ran into problems: $$\ln n!\sim n\ln n-n\, \text{(true)},$$ $$n!\sim e^{n\ln n-n}\, \text{(false)}.$$","['factorial', 'asymptotics', 'real-analysis', 'pi', 'limits']"
3607752,"Show $\int^{\pi/2}_0 \cos^{\mu}(x)\sin^{v}(x)dx= \frac{1}{2}B(\frac{1+\mu}{2},\frac{1+v}{2})$","I have the following task: The Beta integral $B(p,q)= \int^1_0t^{p-1}(1-t)^{q-1}dt$ is convergent for all $p,q \gt 0$ Check that $\int^{\pi/2}_0\cos^{\mu}(x)\sin^{v}(x)dx= \frac{1}{2}B(\frac{1+\mu}{2},\frac{1+v}{2})$ for $\mu,v \gt -1$ What I tried so far: First of all I calculated the RHS of the equality: $$\frac{1}{2}B(\frac{1+\mu}{2},\frac{1+v}{2})= \frac{1}{2}\int^1_0t^{\frac{1+\mu}{2}-1}(1-t)^{\frac{1+v}{2}-1}dt=\frac{1}{2}\int^1_pt^{\frac{\mu-1}{2}}(1-t)^{\frac{v-1}{2}}dt=\frac{1}{2}\int^1_0t^{\frac{\mu-1}{2}}dt-\frac{1}{2}\int^1_0t^{\frac{\mu+v-2}{2}}dt=\frac{1}{2}\frac{t^{\frac{\mu-1}{2}+1}}{\frac{\mu-1}{2}+1}\Big|^1_0-\frac{1}{2}\frac{t^{\frac{\mu+v}{2}}}{\frac{\mu+v}{2}}\Big|^1_0=\frac{t^{\frac{\mu+1}{2}}}{\mu+1}\Big|^1_0-\frac{t^{\mu+v}}{\frac{\mu+v}{2}}\Big|^1_0=\frac{1}{\mu+1}-\frac{1}{\mu+v}=\frac{\mu+v-\mu-1}{(\mu+v)(\mu+1)}=\frac{v-1}{(\mu+v)(\mu+1)}$$ But when I try to calculate $\int^{\pi/2}_0\cos^{\mu}(x)\sin^{v}(x)dx$ , whatever I try, I can't seem to get the result. I've looked on Wikipedia for the integral, and the value of it is: \begin{aligned} \int\left(\sin ^{n} a x\right)\left(\cos ^{m} a x\right) d x &=-\frac{\left(\sin ^{n-1} a x\right)\left(\cos ^{m+1} a x\right)}{a(n+m)}+\frac{n-1}{n+m} \int\left(\sin ^{n-2} a x\right)\left(\cos ^{m} a x\right) d x \quad \,(\text { for } m, n>0) \\ &=\frac{\left(\sin ^{n+1} a x\right)\left(\cos ^{m-1} a x\right)}{a(n+m)}+\frac{m-1}{n+m} \int\left(\sin ^{n} a x\right)\left(\cos ^{m-2} a x\right) d x \quad \,(\text { for } m, n>0) \end{aligned} And I don't think this is even close to what I'm trying to get. What am I doing wrong? Any help is appreciated!","['integration', 'beta-function', 'trigonometry', 'real-analysis']"
3607808,Growth of holomorphic functions,"Let $f$ be an entire function and define $$M(r)=\max\limits_{|z|\le r}|f(z)|=\max\limits_{|z|=r}|f(z)|.$$ Similarly, also define $$m_0(r)=\min\limits_{|z|\le r} |f(z)|,\hspace{5mm} m(r)=\min\limits_{|z|=r}|f(z)|.$$ I was thinking about what we can say about the functions $M(r), m_0(r)$ and $m(r)$ as $r\to\infty.$ For example, by the maximum modulus principle, we know that $M(r)$ is actually increasing. Therefore the limit $\lim\limits_{r\to\infty}M(r)$ exists. It is also clear that if $M(r)\to c<\infty$ then (Liouville's theorem) $f$ must be a constant.It follows that $M(r)\to \infty.$ In fact, for polynomials it is each to check that $M(r)\approx r^n$ as $r\to \infty.$ Moreover, if $\frac{M(r)}{r^n}\to c<\infty$ then using Cauchy's estimate we can show that $f$ must be a polynomial. If $f$ is non-polynomial entire function then it is clear that $M(r)/r^n\to \infty$ for every $n\ge 0.$ My question is can we strengthen it furthre? For example can we say that for a non-polynomial entire function $f,$ we must have $M(r)\approx e^{r}$ or $M(r)\geq Ce^{r^{1-\epsilon}}$ for every $\epsilon>0?$ Now coming to $m_0(r)$ and $m(r).$ We note that $m_0(r)$ is decresing and hence the limit exists, but it is not very interesting. If $f$ has any zero in the plane then $m_0(r)=0$ for all $r$ sufficiently large and therefore the limit will be zero. On the other hand, if $f$ does not have any zero and $f$ is non-constant then $f$ must go arbitrary close to $0$ by picard's theorem. It follows that $m_0(r)\to 0.$ In other words, $m_0(r)\to c<\infty,$ and $c\neq 0$ if and only if $f$ is a constant. The most interesting one is $m(r).$ Let us start with a simple case. If $f$ is a polynomial (of degree $n\ge 1$ ) then $m(r)\approx r^n$ for suffiently large $r.$ Therefore, the limit $m(r)\to \infty.$ Moreover, $\frac{m(r)}{r^n}\to c\neq 0.$ If $f$ is not a polynomial and $f$ does not have a zero then it is very similar to $m_0(r)$ and $m(r)\to 0.$ In general, for a non-polynomial entire function $f,$ we know that the infinity is an essential singularity. In particular, there exists a sequence $z_n\to \infty$ such that $|f(z_n)|\to 0.$ This tells us that $m(|z_n|)\to 0.$ In particular, if $\lim m(r)$ exists, then it must be $0.$ But, I am not able to establish the existence of limit of $m(r).$ Does the limit $\lim\limits_{r\to \infty}m(r)$ always exist? Can we make a more refined statement about the behavior of $m(r)?$ For example, if $f$ has $n$ zeroes in the complex plane can we say that $m(r)\to 0$ like $r^{-n}?$ (I am not hoping this statement to be true, it is just for the illustration of the kind of statement I want to make about $m(r).$ )","['complex-analysis', 'entire-functions', 'analysis']"
3607912,Functorial morphisms on the identity functor,"Given a category $\mathcal{C}$ , the functorial morphisms $\phi:\rm{id}_{\mathcal{C}}\rightarrow \rm{id}_{\mathcal{C}}$ define a monoid via $(\phi\circ \psi)_X := \phi_X\circ\psi_X$ for $X\in \rm{Ob}(\mathcal{C})$ , where the unit is $\rm{id}_{\rm{id}_\mathcal{C}}:\rm{id}_{\mathcal{C}}\rightarrow \rm{id}_{\mathcal{C}}$ , $ (\rm{id}_{\rm{id}_\mathcal{C}})_X := \rm{id}_X$ . The goal is to compute this monoid for the categories $\rm{Set}$ , $\rm{Ring}$ and $R-\rm{Mod}$ for a given commutative ring $R$ . How do I approach this? For $\rm{Set}$ , let $\phi$ be such a functorial morphism, $X,Y$ be any two sets. Then there is a commutative diagram such that $f\circ \phi_X = \phi_Y \circ f$ for every map $f:X\rightarrow Y$ . I want to conclude some necessary properties of $\phi_X$ from this, but I don't know how yet.","['abstract-algebra', 'category-theory', 'commutative-algebra']"
3608100,Find the average of a function defined on a Fat Cantor Set,"Suppose we have $P:A\to[0,1]$ , where $A$ is the fat cantor set denoted as $C$ . We produce $C$ by removing $1/4$ of $[0,1]$ around mid-point $1/2$ $$C_{1}=[0,3/8]\cup [5/8,1]$$ $$C_{1,1}=[0,3/8] \ \ \ \ \ \ C_{1,2}=[5/8,1]$$ We repeat, removing $1/16$ of $[0,1]$ around each midpoint of remaining intervals $C_{1,1}$ and $C_{1,2}$ . $$C_{2}=[0,5/32]\cup[7/32,12/32]\cup[5/8,25/32]\cup[27/32,1]$$ $$C_{2,1}=[0,5/32] \ \ \ \ \ \ \ C_{2,2}=[7/32,12/32] \ \ \ \ C_{2,3}=[5/8,25/32] \ \ \ C_{2,4}=[27/32,1]$$ Repeat the process until $\lim\limits_{n\to\infty}C_{n}=C$ , where $\lim\limits_{n\to\infty} 1/4^n$ of $[0,1]$ around each midpoint of remaining intervals are removed. I want to find the average such that $$\lim_{n\to\infty}\sum_{i=1}^{2^{n}} P(t_{n,i})(1/2^n)$$ Where $\bigcup\limits_{i=1}^{2^{n}}C_{n,i}=C_n$ and $t_{n,i}\in C_{n,i}$ How would we use mesh and list manipulation to solve this? Would we get $\frac{1}{2}\int_{0}^{1} P dx$ ? EDIT: I found when $P(x)=x^2$ , the average I want should be $4/11$ but according to this answer , the average should be $38/105$ ? Which answer is correct?","['programming', 'measure-theory', 'cantor-set', 'average', 'probability-theory']"
3608175,Weeks Manifold - the Heegaard splitting and presentations of its fundamental group,"I'm working on the Heegaard splitting of some 3-manifold and I'm currently stucked on the Weeks manifold. It is a closed orientable hyperbolic 3-manifold with the smallest volume, obtained by (5, 2) and (5, 1) Dehn surgeries on the Whitehead link. I found a presentation of the fundamental group of the Weeks manifold, it is $$\pi_1(M_W)=\langle a,b\mid a^2b^2a^2b^{-1}ab^{-1}=1,a^2b^2a^{-1}ba^{-1}b^2=1\rangle.$$ What is the rank of this group? Do you know any better presentation of the Weeks manifold's fundamental group? Or do you know the better ways to compute the Heegard genus of this manifold?","['group-presentation', 'fundamental-groups', 'group-theory', 'low-dimensional-topology', 'algebraic-topology']"
3608187,Function of a confidence interval,"If we have a confidence interval for a given parameter $\theta$ given as $[\theta_l, \theta_u]$ ( $l$ is for lower and $u$ is for upper) at confidence level $\gamma$ , and we have a monotone (Borel) measurable function $f(\cdot)$ , can we claim that $[f(\theta_l), f(\theta_u)]$ is a confidence interval for the transformed parameter $f(\theta)$ at the same level $\gamma$ ?
Can we say anything about the confidence interval if $f$ is not monotone?","['confidence-interval', 'statistics', 'parameter-estimation', 'probability']"
3608212,"If $A,B \in \mathcal{M_2}(\mathbb{R})$ and $A^2+B^2=AB$, does it follow that $A$ and $B$ commute?","Let $A,B \in \mathcal{M_2}(\mathbb{R})$ such that $A^2+B^2=AB$ . Is it necessary that $AB=BA$ ? I could easily show that such matrices have the property that $(AB-BA)^2=O_2$ (this was actually the question I was asked, then I started wondering if it would be true that the matrices actually commute) by considering the matrix $M=A+\epsilon B$ and computing the determinat  of $M \cdot \overline{M}$ ( $\epsilon \in \mathbb{C}\setminus \mathbb{R}$ is a cubic root of unity), but that's all I got. I tried to find some counterexamples, but I have a hard time finding any matrices with that property. EDIT: To see that $(AB-BA)^2=O_2$ we do the following : by direct computations $$|\det M|^2=\det M \det \overline{M}=\epsilon^2 \det(AB-BA)$$ and this is a real number if and only if $\det(AB-BA)=0$ . From the Cayley-Hamilton theorem we now get that $(AB-BA)^2=O_2$ .","['matrices', 'linear-algebra']"
3608328,"Suggestions for $ \lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2} $?","I'm trying to evaluate $$ 
\lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2}
$$ over the domain $x>0$ , $y>0$ . ============ My attempt: $f(x,x^2)\to +\infty$ ; so if the limit exists it must be $+\infty$ . I tried to evaluate the limits ""near"" $(x,x)$ where, I thought, there's may be some problems: $f(x, x-x^2)\to +\infty$ . Then I convinced myself the limit could be $+\infty$ : since $f(x,y)>0$ over the domain, I had to find such $g(x,y)$ that: 1. $f(x,y) \ge g(x,y)$ 2. $ \lim_{(x,y)\to (0,0)} g(x,y)\to +\infty $ $$
f(x,y)=\frac{x-\sqrt{xy}}{x^2-y^2}=\frac{x-\sqrt{xy}+y-y}{x^2-y^2}=\frac{x-\sqrt{xy}+y}{x^2-y^2}-\frac{y}{x^2-y^2}=\frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2}
$$ Where the last step follows by $(x-\sqrt{xy}+y) \ge 0$ with $x>0$ , $y>0$ . $$
\frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2} = \frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2}.
$$ From $\left[3\left(x-\sqrt{xy}+y\right)^2\right] \ge \left[x+xy+y^2\right]$ , for every $(x,y)$ with $(x>y)$ : $$
\frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(x^2-y^2)}+\frac{y}{y^2-x^2}.
$$ From here I observated that $ \left[\lim_{(x,y)\to (0,0)} g(x,y)\to +\infty \right]$ and eventually $ \left[\lim_{(x,y)\to (0,0)} f(x,y)\to +\infty \right]$ for $(x>y)$ . I thought that for $(y>x)$ , the inequality was formally equivalent when I replace $(x)$ with $(y)$ and viceversa: $$
\frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(y^2-x^2)}+\frac{x}{x^2-y^2}.
$$ However I could see, through an online grapher, that it is false!! So I remained without any chance to conclude the limit. ============ Is there anybody who knows why the last inequality isn't correct? And also, has anybody some hints to evaluate the limit?","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
3608336,"Find the max $\{\sqrt{n^3}\lg n, \sqrt[3]{n^4}\lg^5n \}$","*Note: the logs are with base 2 (computer scince question). Let $$
f(n) = \max\{\sqrt{n^3}\lg n, \sqrt[3]{n^4}\lg^5n \} = \max\{f_1(n),f_2(n)\}
$$ I want to find which function is the max between the two functions such that the bigger will satisfies: $$
\exists n_0 \in N, \forall n > n_0: f_1(n) > f_2(n)
$$ What i tried: I tried to start with comparing the two functions, hopinf to find the cross point, than just checking what happend after and before the cross point. Therefore, i try to solve: $$
\sqrt{n^3}\lg n =\sqrt[3]{n^4}\lg^5n 
$$ Solving this i got to: $$
\sqrt[3]{n} = \lg^4n
$$ Maybe i miss something, but i dong know how to solve this... I would like a hint more than a solution. Thank you very much for the answers.","['algebra-precalculus', 'functions', 'upper-lower-bounds']"
3608380,Parallel vector fields orthonormal at a point implies manifold is euclidean,"Let $(N, g)$ be a complete Riemannian $n$ -manifold such that there is a compact set $K$ such that $N\setminus K$ is diffeomorphic to $\mathbb{R}^n \setminus D^n$ .
Suppose there exist $n$ parallel vector fields $V_1, ..., V_n$ that are orthonormal at a point.
I want to prove that the manifold is globally isometric to $\mathbb{R}^n$ with the euclidean metric. First part of the proof: As $\forall X \in \mathfrak{X}(N)$ $$
		\nabla_X \langle V_i, V_j \rangle = \langle \nabla_X V_i, V_j \rangle +\langle V_i, \nabla_X V_j \rangle = 0
		$$ $\langle V_i, V_j \rangle$ is constant.
        Moreover, as it is equal to $\delta_{ij}$ a a point, $\{V_i\}$ are orthogonal everywhere by continuity.
        Now, as the Levi-Civita connection is torsion-free $$
		[V_i, V_j] = \nabla_{V_i} V_j - \nabla_{V_j} V_i = 0
		$$ and hence they are integrable locally to a chart which is an isometry because of the equation $\langle V_i, V_j \rangle = \delta_{ij}$ . Missing part: The goal is to use Killing-Hopf theorem to prove the following lemma Lemma: Let $(M,g)$ be a complete flat Riemannian manifold such that there is a compact set $K \subset M$ with $M\setminus K$ diffeomeorphic to $\mathbb{R}^n \setminus D^n$ . Then $(M,g)$ is isometric to Euclidean space. but I am not really sure how to do this. Does anybody have a hint? (In case someone is wondering I need this to give a rigorous proof of the positive mass rigidity in the positive mass theorem).","['riemannian-geometry', 'differential-geometry']"
3608394,Find all roots of the polynomial equation $p(p(x)) - x = 0$,"Let $p(x)$ be a quadratic polynomial such that for distinct reals $\alpha$ and $\beta$ , $$p(\alpha)=\alpha\ \&\ p(\beta)=\beta$$ Show that $\alpha$ and $\beta$ are the roots of the following equation $$p(p(x))-x=0$$ Also find the remaining roots. The first part was very simple to prove, in order to find the remaining roots, I assumed $t$ to be a root of the second equation with $p(t)=u.$ Hence it immediately follows that $u$ is also the root of the second equation with $p(u)=t$ . Now the task is to find such $u$ and $t$ . We have $$at^2+bt+c=u \ \ \ \ (1)$$ and $$au^2+bu+c=t\ \ \ (2)$$ Thus, taking (1) - (2) and cancelling $u-t$ we get $$u+t=\frac{-(1+b)}{a}$$ Now, taking $u^2*(1) - t^2*(2)$ and cancelling $u-t$ again, we get $$ut=\frac{1+b+ac}{a^2}$$ With this, we see that $u$ and $t$ are the roots of the following equation $$a^2x^2+a(1+b)x+(1+b+ac)=0$$ And thus the roots can be computed using the quadratic formula. First of all, I want to know whether my answer is correct or not, as the book that I use does not provide any answer to this problem, and if it is incorrect, I would like to know the correct answer. If my answer is correct, can the answer be better in anyway? (as I only came up with an equation for the roots... and writing down the final answer using the quadratic formula looks crazy!!) Thanks for any answers!! Edit: Here I have assumed $p(x)=ax^2+bx+c$","['algebra-precalculus', 'quadratics', 'roots', 'polynomials']"
3608484,Does the existence of the second derivative implies the existence of the first?,"It can be a Spivak's ""Calculus"" question, but I have not found anywhere so far the answer. The second derivative seems to be defined in terms of the first, but is there any way to formally prove that in general the existence of the second derivative implies the existence of the first?.","['integration', 'calculus', 'derivatives']"
3608506,Distance between two curves,"Consider the following subsets of the plane: $$
C_1=\{(x,y): x>0, y=\frac{1}{x}\}
$$ and $$
C_2=\{(x,y):x<0,y=-1+\frac{1}{x}\}.
$$ Given any two points $P=(x,y)$ and $Q=(u,v)$ of the plane, their distance $d(P,Q)$ is defined by $$
d(P,Q)=\sqrt{(x-u)^2+(y-v)^2}.
$$ Show that there exists a unique choice of points $P_0\in C_1$ and $Q_0\in C_2$ such that $$
d(P_0,Q_0)\leq d(P,Q) \mbox{ for all } P\in C_1 \mbox{ and } Q\in C_2.
$$ Here if I take two points $p(x,\frac{1}{x}) \in C_1$ and $Q(-x, -1-\frac{1}{x}) \in C_2$ for $x>0$ , then I take their distance and take the derivative and prove that there exists a unique point at which the minima occurs.
But I cannot justify my choice of the point $Q \in C_2$ , the point could have been $Q(-x^{'} ,-1-\frac{1}{x^{'}})$ . But my intuition says that the minimum will occur when I take the same variable for both the points and also it has something to do with the symmetry of the curves. 
My problem is I am not being able to give a mathematical proof of my intuition.","['plane-curves', 'curves', 'calculus', 'derivatives']"
3608539,Summing series containing $e^{an} \pm 1$ term in the denominator,"I was looking over Ramanujan's first letter to Hardy and came across several series of a similar form: $$ \sum_{n=1}^{\infty} \frac{n^{13}}{e^{2\pi n}-1} = \frac{1}{24} $$ $$ \sum_{n=1}^{\infty} \frac{\coth(n\pi)}{n^7} = \frac{19 \pi^7}{56700} $$ $$ \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^5 \cosh \left(\frac{(2n+1)\pi}{2} \right)} = \frac{\pi^5}{768} $$ Does anyone know what method(s) Ramanujan used or is likely to have used to compute these, or where I can find this information? More generally I am wondering what are some techniques of summing series in which the denominator of the general term contains an exponential $\pm 1$ , with perhaps another exponential or rational function in the numerator? Series of the form $$ \sum_{n=1}^{\infty} \frac{p(n)}{q(n)} \frac{1}{e^{an} \pm 1} \hspace{0.5cm} \text{or} \hspace{0.5cm} \sum_{n=1}^{\infty} \frac{p(n)}{q(n)} \frac{1}{\cosh(an)} \hspace{0.5cm} \text{or} \hspace{0.5cm} \sum_{n=1}^{\infty} \frac{p(n)}{q(n)} \coth(an) $$ where $p$ and $q$ are polynomials and $a>0$ is a constant.",['sequences-and-series']
3608559,Find the values of $x$ in terms of $a$ in $x^2+\frac{(ax)^2} {(x+a)^2} =3a^2$,"The question is as follows. Find the values of $x$ in terms of $a$ in $x^2+\dfrac{(ax)^2}{(x+a)^2} =3a^2 $ My solution: Multiply both sides by $(x+a)^2$ and expand. On rearranging we get $x^4+2ax^3-a^2x^2-6a^3x-3a^4=0$ Now dividing by $a^4$ we and taking $\dfrac{x}{a}=y$ , we get $y^4+2y^3-y^2-6y^3-3=0$ Now we can write the above equation as $(y^2+ay+b)(y^2+cy+d)=0$ Now expanding it and comparing coefficient with the equation we get $a=-1;b=-1;c=3;d=3$ So we get $(y^2-y-1)(y^2 +3y+3)=0$ The second bracket has no real roots and by solving 1st bracket we get $y=\dfrac{x}{a} =\dfrac{1\pm\sqrt5}{2}$ So $x=\dfrac{a(1\pm\sqrt5) }{2}$ My question is 1: As you can see the method is long and tedious, is there any other or elegant way to solve it? 2:Can any step in my solution or the solution itself be improved upon or be replaced by another easier step (like but not limited to factorising the equation we got in $y$ directly into two quadratics or solving the quatic directly) Thanks! I got the question from a preparatory book for olympiads.","['contest-math', 'roots', 'factoring', 'algebra-precalculus', 'quadratics']"
3608570,weak star and strong convergence of net in Banach spaces,"In Banach spaces, the following result is well-known: (1) Let $X$ be a Banach space. Let $\{x_n\}\subset X$ and $\{x^*_n\}\subset X^*$ be such that $x_n \rightarrow x$ (convergence with respect to strong topology on $X$ ) and $x^*_n\overset{\ast}{\rightharpoonup} x^*$ (convergence with respect to weak star topology on $X^*$ ). Then we have, $\langle x^*_n, x_n\rangle\rightarrow \langle x^*,x\rangle$ . The proof for the above result is based on the fact that if $x^*_n\overset{\ast}{\rightharpoonup} x^*$ then $\{x^*_n\}$ is bounded. We have known that this fact does not hold if we replace the sequence by net (based on the following counterexample: Must a weakly or weak-* convergent net be eventually bounded? ) My questions are: 1) Whether or not the result (1) still holds if the sequence is replaced by the net (see the following for definition: https://en.wikipedia.org/wiki/Net_(mathematics) )? 2) In the case (1) is false for the net, could we construct a counterexample? And what more assumptions are added such that (1) is true for the net. Thank you for all discussions on this topic.","['weak-convergence', 'functional-analysis', 'weak-topology', 'general-topology', 'nets']"
3608614,"Method of Adjoints, Neural ODEs","I've been trying to understand the gist behind the Chen et. al paper on neural ODE's ( https://arxiv.org/pdf/1806.07366.pdf ). 
It seems like the main trick here is to be able to take derivatives of functions of an ODE solver, with respect to neural network parameters. This is done by the adjoint sensitivity method, where we solve a differential equation in order to obtain the gradients of the loss function. To understand this technique, I wanted to implement a simple version. $$
\frac{d z(t)}{dt} = f(z(t), t, \alpha) = \alpha z(t)
$$ With conditions: start time $t_0$ , stop time $t_1$ , initial position $z(t_0)$ . Now based on our parameter and initial condition, we then have that our solved ode will give us $z(t) = e^{\alpha(t-t_0)}z(t_0)$ . Suppose I want to minimize the loss function: $$L = (z(t_1) -1)^2 /2$$ . That is, I only care about the value of the solved ODE at time $t_1$ , and i want its value to be 1. I can do this analytically here, so I wanted to solve using the adjoint method, and confirm that the two methods match. According to the adjoint method described in the paper, we then need to solve for the adjoint: $a(t) = \partial L/ \partial z(t)$ . We do this by solving the differential equation which $a$ satisfies: $$
\frac{d a}{d t} = -a \partial f/\partial z
$$ we can do this and obtain $$
a(t) = e^{\alpha (t-t_1)} (z(t_1)-1)
$$ Which we can easily see matches our boundary condition of $a(t_1) = (z(t_1)-1)$ Now my goal was to find $d L / d \alpha$ , which is given by equation (51) in the paper: $$
\frac{d L}{d \alpha} = - \int_{t_1}^{t_0} a(t) \frac{\partial f}{\partial \alpha} = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) z(t) = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) e^{\alpha(t-t_0)}z(t_0)
$$ $$
= (z(t_1)-1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha
$$ However, we can determine $dL/d\alpha$ analytically here: $$
\frac{dL}{d \alpha} = \frac{dL}{d z(t_1)}\frac{d z(t_1)}{d \alpha} = \big[(e^{\alpha(t_1-t_0)}z(t_0) -1)\big] \big[ (t_1-t_0)e^{\alpha(t_1-t_0)}z(t_0) \big]
$$ If I plug the form for $z(t_1)$ into the adjoint result, these two should match. But I get the following Adjoint result: $(e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha$ Analytic result: $(e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0)(t_1-t_0)e^{\alpha(t_1-t_0)}$ But these do not match ! If somebody could explain why this is, I would really appreciate it. Neural ODE's seem interesting, but if I can't understand an incredibly simple toy model, I dont see how I can use them. Thanks for your time","['numerical-methods', 'neural-networks', 'ordinary-differential-equations']"
3608651,Prove that: $\left|\int\limits_a^b\frac{\sin x}{x}dx\right|\le \frac{2}{a}$,"Prove that: $$\left|\int\limits_a^b\frac{\sin x}{x}dx\right|\le \frac{2}{a}$$ $0<a<b$ I'm not asking for a complete solution to this, just a hint or a push in the right direction. I've been struggling with this and haven't really made any progress.","['integration', 'calculus']"
3608660,If $A^2 = A$ then $A$ is diagonalizable,"I've stumbled upon this question in my assignment: Prove if $A_{nxn}(\mathbb C)$ with $A^2 = A$ , then $A$ is diagonalizable My first thought is to solve for $p(A)$ where $p(x)  = x^2 - x$ and you get real roots.
Would that be sufficient in showing that $A$ is diagonalizable given you get real roots?","['eigenvalues-eigenvectors', 'idempotents', 'matrices', 'linear-algebra', 'diagonalization']"
3608728,Number of paths between points in a grid with blocked nodes?,"I'm working on a research project and I came across this problem. I was curious if there is a strategy to count the number of paths from A to B, using free nodes while avoiding the blocked nodes. The starting point is always the bottom left-hand side corner, and the endpoint is always the top right-hand side corner. We can only move up or to the right. Blocked nodes always appear on the top left-hand side corner of the grid. There is no free node on the left-hand side, or above any forbidden node. I can find the answer using search algorithms, but that's not efficient, especially, with large scale problems. I was curious if there is a better mathematical expression/strategy for this problem. Thank you! 
This is an example:","['combinatorics', 'recurrence-relations']"
3608761,Is the one-point compactification terminal in the „category of compactifications“?,"Let $X$ be a noncompact $KC$ -Space, i.e. a topological space in which every compact set is closed.
Call further $\iota_Y\colon X\to Y$ a compactification if $Y$ is compact and $f$ is an embedding with dense image.  Lastly, let $X^\ast := X\cup \{∞\}$ denote the one-point compactification of $X$ , with corresponding embedding $\iota_\ast$ . (Note that since $X$ is not necessarily Hausdorff nor locally compact, $X^\ast$ does not have to be Haussdorff, which is why we weakened the definition of “compactification” accordingly) Question : Is there a unique continuous map $h\colon Y\to X^\ast$ which respects the embeddings, i.e. for which $h\circ \iota_Y = \iota_\ast$ ?
Or, in other words: If $\mathscr C$ is the category with objects $(Y,\iota_Y)$ as above, and morphisms continuous inclusion-respecting maps $Y\to Y^\prime$ , is $(X^\ast, \iota_\ast)$ a terminal object? Note that if $h$ is continuous, it must be surjective: Since $h$ commutes with the embeddings, we have $$
  X^\ast\setminus\{∞\} = \mathrm{Im}(\iota_\ast)\subseteq \mathrm{Im}(h) \subseteq X^\ast.
$$ however, since $\mathrm{Im}(h)$ is the image of a compact set and hence compact itself, but $\mathrm{Im}(\iota_\ast) \simeq X$ is noncompact, only $\mathrm{Im}(h)=X^\ast$ remains a possibility. I've managed to show that – assuming $h$ sends all the new points in $Y$ to $∞$ – neighborhoods around $\iota_\ast(x)\in X^\ast$ have a neighborhood as a preimage, and that preimages of open neighborhoods around $∞$ are open if and only if every compact closed $C\subseteq X$ maps to a closed $\iota_Y(C)$ – but I don't see how that's necessarily the case since $h$ does not have to be closed.
However, I didn't manage to show that all such $h$ necessarily have to map all „new“ points (those in $Y\setminus \mathrm{Im}(\iota_Y)$ ) to $∞$ .","['closed-map', 'general-topology', 'compactification', 'category-theory']"
3608780,How both definitions of a lie group are equivalent,"I can't proof that these definitions of lie groups are equivalent.
Can anyone prove it directly?. A Lie group is a set which carries the algebraic structure
of a group and the differentiable structure of a smooth manifold such that the
mapping $$ G\times G \rightarrow G \hspace{1cm} (a,b) \mapsto ab^{-1}$$ is smooth A Lie group is a set which carries the algebraic structure
of a group and the differentiable structure of a smooth manifold such that the
mappings $$ G\times G \rightarrow G \hspace{1cm} (a,b) \mapsto ab$$ and $$ G \rightarrow G \hspace{1cm} g \mapsto g^{-1}$$ are smooth","['smooth-manifolds', 'differential-geometry']"
3608847,Exact sequence of topological groups induces exact sequence on the fundamental group,"I think it’s a general result, but if you like you can assume compact Lie Groups. Here is the situation: We have a exact sequence of path connected topological groups $$ 1 \rightarrow  A \rightarrow B \rightarrow C \rightarrow 1$$ Name the continuous homomorphisms $f: A \rightarrow B$ and $g: B \rightarrow C$ . I would like to know if the following sequence is exact: $$ 1 \rightarrow \pi_1(A) \rightarrow \pi_1(B) \rightarrow \pi_1(C) \rightarrow 1$$ With the induced maps $f_*$ and $g_*$ . I already proved $\operatorname{Im} f_* \subset \ker g_*$ , but I can’t conclude the other inclusion. If you have any other suggestion to conclude it I would be very happy. Remark: In general it’s not true that a injective map implies that the induced map on the fundamental group is injective. Example: inclusion of $\mathbb{S}^1$ in $\mathbb{B}^1$ .","['general-topology', 'exact-sequence', 'fundamental-groups', 'topological-groups']"
3608890,Find solutions of $F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right)$,"I need to construct a continuous function $F:\mathbb R\to \mathbb R$ with $F(0)=0$ and $F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right)$ for every $x\in\mathbb R$ . I found $F(x)\equiv0$ is a solution. Moreover, if F is a polynomial, I could prove that $F$ must be zero function. My question is: Is there another such function? I think it must be have, but I could not construct them.","['functional-equations', 'calculus', 'analysis']"
3608943,Calculating Norm in a Reproducing Kernel Hilbert Space (RKHS),"I see a lot of texts describing properties of Reproducing Kernel Hilbert Spaces (RKHS), but I can't seem to find one that explains how to actually calculate the norm in the RKHS. From Wikipedia, it says: A RKHS is a space of functions in which point evaluation is a
  continuous linear functional. Roughly speaking, this means that if two
  functions $f$ and $g$ in the RKHS are
  close in norm, i.e., $\|f-g\|$ is
  small, then $f$ and $g$ are also
  pointwise close, i.e., $ |f(x)-g(x)|$ is small for all $x$ . The reverse need not
  be true. Questions: It seems a RKHS is a space full of functions, so each element in it is a function? But I can treat these elements as points nonetheless? How does the concept of ""norm"" make sense in this context? A
Euclidean norm is the distance between two points in space, but this is the distance between two functions in a space? Assuming I understand question 2, how can I go about actually calculating this norm? An example or formula would be most appreciated.","['hilbert-spaces', 'vector-spaces', 'functional-analysis', 'reproducing-kernel-hilbert-spaces']"
3608956,Is this a correct proof of injectivity of right inverse function by contraposition?,"I'm trying to prove the injectivity of $f: X \to Y$ in $g \circ f = id_X$ (with $g : Y \to X$ ). This is what I came up with and I'd appreciate someone pointing out any mistakes. 1 $$x \neq x' \implies id_X(x) \neq id_X(x')$$ 2 $$\implies (g \circ f) (x) \neq (g \circ f )(x')$$ 3 $$\implies g(f(x)) \neq g(f(x'))$$ 4 $$\implies f(x) \neq f(x')$$ The surjectivity of $g$ permits the implication in (4), right? Thanks in advance.","['elementary-set-theory', 'solution-verification']"
3608959,Showing $1-2z^2-2z^3-2z^4-2z^5$ has a unique root inside the disk of radius 0.6,"I'd like to show $P(z)=1-2z^2-2z^3-2z^4-2z^5$ has a unique root inside the disk $|z|<0.6$ . I tried using Rouche's theorem, which worked for polynomials of this form $1-2z^2-2z^3-2z^4...-2z^n$ but of higher degrees, but the same method did not work on $n=5$ or smaller $n$ . For higher degrees, I used it as follows, multiplying by $z-1$ we obtain the polynomial $-2z^{n+1}+2z^2+z-1$ . By defining $f(z)=2z^2+z-1=2(z+1)(z-\frac{1}{2})$ and $g(z)=-2z^{n+1}$ one can show (using the regular and reverse triangle inequality) that on $|z|=0.6$ we have $|f(z)|>|g(z)|$ for $n\geq6$ . However for $n=3,4,5$ this method failed. I'm wondering if there's another way besides Rouche's theorem, or maybe a different use of Rouche here, or even some idea on why Rouche doesn't work on those values. Just note that $0.6$ is not especially important, it's what I thought to use myself to find a proof of a certain claim. Similar radii (not far from $0.6$ ) that guarantee a unique root inside it would also be helpful.","['roots', 'complex-analysis', 'rouches-theorem', 'polynomials', 'inequality']"
3609021,"If $M$ and $N$ are divisible, abelian groups, show that their tensor product $M \otimes_\mathbb{Z} N$ is uniquely divisible","Recall that an abelian group $M$ is divisible if for each $m \in M$ and $r \in \mathbb{Z}$ , there is an $m' \in M$ such that $rm' = m$ . It is uniquely divisible if that $m'$ is unique. If $M$ and $N$ are divisible, abelian groups, show that their tensor product $M \otimes_\mathbb{Z} N$ is uniquely divisible. Conclude $\mathbb{Q}/\mathbb{Z} \otimes \mathbb{Q}/\mathbb{Z} = 0$ . I am new to the subjects of tensor products, modules, and exact sequences. Here are my thoughts so far: $M$ is divisible, so the map $\phi_1: M \longrightarrow M : m' \longmapsto rm'$ is surjective for any $r \in \mathbb{Z}$ . Similarly, $N$ is divisible, so the map $\phi_2 : N \longrightarrow N : n' \longmapsto sn'$ is surjective for any $s \in \mathbb{Z}$ . To show that $M \otimes_\mathbb{Z} N$ is uniquely divisible, my idea was to construct the map $\phi: M \otimes_\mathbb{Z} N \longrightarrow M \otimes_\mathbb{Z} N : m' \otimes n' \longmapsto rm' \otimes sn'$ and show that it is bijective for any $r, s \in \mathbb{Z}$ . I believe that surjectivity would follow easily from the fact that $\phi_1$ and $\phi_2$ above are both surjective. But, how can I show that this map is injective ? Am I on the right track ? I haven't yet used that both $M$ and $N$ are abelian. As far as $\mathbb{Q}/\mathbb{Z}$ , I'm aware of the purely algebraic way to see that $\mathbb{Q}/\mathbb{Z} \otimes \mathbb{Q}/\mathbb{Z} = 0$ . However, how can I use the claim in this problem to show this ? $\mathbb{Q}/\mathbb{Z}$ is a divisible abelian group, so the claim is relevant. Upon proving it, it tells us that $\mathbb{Q}/\mathbb{Z} \otimes \mathbb{Q}/\mathbb{Z}$ is uniquely divisible. But why does this tell us that $\mathbb{Q}/\mathbb{Z} \otimes \mathbb{Q}/\mathbb{Z} = 0$ ? Can we possibly involve an exact sequence to do this ? A relevant exact sequence might be $0 \longrightarrow \mathbb{Z} \longrightarrow \mathbb{Q} \longrightarrow \mathbb{Q}/\mathbb{Z} \longrightarrow 0$ . Tensoring is right exact, so we can get another exact sequence by tensoring each object in this sequence (over $\mathbb{Z}$ ) with $\mathbb{Q}/\mathbb{Z}$ . Is that the right idea ? Thanks for all of your help.","['group-theory', 'exact-sequence', 'abstract-algebra', 'tensor-products', 'divisible-groups']"
3609041,Contour Integral involving Zeta function,"I'm trying to compute the contour integral $$\frac{1}{2 \pi i} \int_{c - i \infty}^{c + i \infty} \zeta^2(\omega) \frac{8^\omega}{\omega} \ d \omega$$ where $c > 1$ , $\zeta(s)$ is the Riemann zeta function. Using Perron's Formula and defining $D(x) = \sum_{k \leq x} \sigma_0(n)$ , where $\sigma_0$ is the usual divisor counting function, one can show that $$D(x) = \frac{1}{2 \pi i} \int_{c - i \infty}^{c + i \infty} \zeta^2(\omega) \frac{x^\omega}{\omega} \ d \omega.$$ So, to that end one can just compute $D(8)$ and call it a day. However, for my own purposes I want to redefine $D(x)$ by the above integral instead. Hence, why I state the problem for a specific case $x = 8$ , for example. I have made some progress. By considering a modified Bromwich contour that avoids the branch cut and $z = 0$ , lets call it $\mathcal{B}$ , we can apply Cauchy's Residue Theorem: $$\oint_{\mathcal{B}} \zeta^2(\omega) \frac{8^\omega}{\omega} d \omega = 2 \pi i \operatorname*{Res}(\zeta^2(\omega) \frac{8^{\omega}}{\omega}; 1) = 8(-1 + 2 \gamma + \ln 8)$$ where $\gamma$ is the Euler-Mascheroni constant. I obtained this by expanding $\zeta^2(\omega) \frac{8^\omega}{\omega}$ into its Laurent series. To get the desired integral we would then need subtract from this value the portions of the contour that are not the vertical line $c - iR$ to $c + iR$ , and then take the limit as $R \to \infty$ and $r \to 0$ where $C_r$ is the circle of radius $r$ where the $\mathcal{B}$ dodges the origin. Feel free to modify this contour in any shape or form, or consider a different positive integer value of $x$ . PS: I’ve had to move this post around between sites. I think here is the most appropriate place, sorry if you’ve seen this before. This is the post’s final resting place.","['number-theory', 'analytic-number-theory', 'contour-integration', 'riemann-zeta', 'complex-integration']"
3609047,Limit of $\frac{n + (-1)^n n}{n+2}$,"I'm taking a university real analysis course and I was tasked with demonstarting a proof on regaurding the limit (or lack there of) of $\frac{n + (-1)^n n}{n+2}$ . Here's my attempt at a proof that no limit exists. First use algerbra of limits to seperate $\frac{n + (-1)^n n}{n+2}$ $\implies$ $\frac{1}{1+\frac{2}{n}} + \frac{(-1)^n}{1 + \frac{2}{n}}$ Limit of $\frac{1}{1+\frac{2}{n}}$ as $n \rightarrow \infty$ is simply 1. Using the squeeze prinicible. Assuming $\frac{(-1)^n}{1 + \frac{2}{n}}$ does have a limit $y$ then sequences $x_n,y_n,z_n$ such that $x_n \leq y_n \leq z_n$ all sequences must converge on $y$ . Take... $x_n = \frac{-1}{1+\frac{2}{n}}$ $ y_n = \frac{(-1)^n}{1 + \frac{2}{n}}$ $z_n = \frac{1}{1+\frac{2}{n}}$ There is no possible $x_n$ or $z_n$ that converge to similar limits and obeys $x_n \leq y_n \leq z_n$ . Therefore there is no limit $y$ . Since $x_n = \frac{(-1)^n}{1+\frac{2}{n}}$ has no limit, by the algebra of limits neither does $\frac{n + (-1)^n n}{n+2}$ . Any feedback would be much appreciated!","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3609101,Why is the geometric mean less sensitive to outliers than the arithmetic mean?,"It’s well known that the geometric mean of a set of positive numbers is less sensitive to outliers than the arithmetic mean. It’s easy to see this by example, but is there a deeper theoretical reason for this? How would I go about “proving” that this is true? Would it make sense to compare the variances of the GM and AM of a sequence of random variables?","['descriptive-statistics', 'statistics', 'means']"
3609123,Prove if $A\Delta B\subseteq A\Delta C$ then $A\cap C\subseteq B\subseteq A\cup C$,"I'm a bit stuck on the following problem: Prove if $A\Delta B\subseteq A\Delta C$ then $A\cap C\subseteq B\subseteq A\cup C$ . At first, I write the statement like that: $$(A\Delta B\subseteq A\Delta C)\implies(A\cap C\subseteq B)\land(B\subseteq A\cup C)$$ And then handle the two parts separated by $\land$ independently. On each part I use proof by contradiction, so I will try to prove the two following statements: I. $A\cap C\not\subseteq B$ II. $B\not\subseteq A\cup C$ In case of $A\cap C\not\subseteq B$ : It means that every $x$ will meet the requirement of $(x\in A)\land(x\in C)\land(x\not\in B)$ . Now there is the problem that I can't understand how to proceed from here. I know that I should find a statement which conflicts with the if requirement, but it looks tricky. I would appreciate your help.",['elementary-set-theory']
3609149,Expected value and Gambler's fallacy,"Betting on a fair coin has expected value 0 dollars. Suppose we win 1 dollar for each win and lose the same for each loss. Suppose we have lost 100 dollars so far. Then it's right to say that this loss has to be balanced out by the winnings somewhere in the future tosses of the coin? That's because the expected value is 0, so we can't remain at -100 dollars till infinity. But that also implies that the set of future tosses of the coin are overall biased towards winning, which is Gambler's fallacy. Please help.","['expected-value', 'probability', 'gambling']"
3609219,"MLE of Uniform on $(\theta, \theta +1)$ and consistency/bias","I see there were a few questions on SE about MLE of Uniform already but none of them helped me with this one: We are to compute MLE of $U(\theta, \theta +1)$ and check if it is biased and consistent . I tried by making a spin-off of an example with $U(0, \theta)$ but I am not sure if it is correct. Suppose there's $X_1, X_2, \dots, X_n$ i.i.d with $U(\theta, \theta +1)$ , $T(X_1, \dots, X_n)$ is the statistic and $(x_1, \dots, x_n)$ a sample from that statistic. I start of with computing $L(\theta)$ $$
L(\theta)=\prod_{i=1}^n\mathbb{1}_{[\theta, \theta +1]}(x_i) = \mathbb{1}_{(-\infty, X(1)]}(\theta)\cdot\mathbb{1}_{[X(n),\infty)}(\theta+1)
$$ Since $P(x_i \geq \theta) = 1$ this is just $$
L(\theta)=\mathbb{1}_{[X(n),\infty)}(\theta+1) = \begin{cases}
      1, & \text{if}\ \theta + 1 \geq X(n) \\
      0, & \text{otherwise}
    \end{cases}
$$ The smallest value of $\theta = 1$ is then $\frac{X(n) - 1 + X(1)}{2}$ and this is our MLE. As @StubbornAtom pointed out in comments, this is not the only MLE possible. How can I calculate bias and consistency of the $\hat{\theta}^{MLE}$ of my choosing?","['statistics', 'parameter-estimation', 'probability', 'maximum-likelihood']"
3609232,Derivative of function with the Kronecker product of a Matrix with respect to vech,"I have $\Sigma$ a symmetric $2 \times 2$ matrix, and $\Sigma^{-1}$ is its inverse. Now, $\tilde{\Sigma}^{-1}=\Sigma^{-1} \otimes I_{n \times n}$ (Kronecker product). I have a function $Y=f(\tilde{\Sigma}^{-1})$ that gives a value in $\mathbb R$ . Let's define $\Phi_{\Sigma}=vech(\Sigma)$ Now, I am trying to get $\frac{\partial Y}{\partial \Phi_{\Sigma}}$ . So far I have $\frac{\partial Y}{\partial \Phi_{\Sigma}^T} = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T \Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ I've been working on this an got that $\Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ is a vector with $n \times n$ elements. 
Now, working with the first part of the derivative $\Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T  = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial vec(\Sigma)^T}  D_2 \Bigg)^T  = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma}\Big)  D_2 \Bigg)^T = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma^{-1}} \frac{\partial \Sigma^{-1}}{\partial \Sigma} \Big)  D_2 \Bigg)^T$ $= \Bigg( vec \Big( (I_2 \otimes I_n) (-\Sigma^{-1} \Sigma^{-1}) \Big)  D_2 \Bigg)^T$ where $D_2$ is the duplication matrix However, the matrices $(I_2 \otimes I_n)$ and $-\Sigma^{-1} \Sigma^{-1}$ are not conformable. So it is wrong. Also, since $\Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ is a vector with $n \times n$ elements, and $\frac{\partial Y}{\partial \Phi_{\Sigma}}$ is $3 \times 1$ , so $\Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T$ should be $3 \times (n \times n)$ .
May I ask for advice on solving this task?","['vectorization', 'matrices', 'matrix-calculus', 'kronecker-product', 'derivatives']"
3609245,Proving a piecewise-defined map $\mathbb Z\to\mathbb N$ is onto,"Let $F: \mathbb Z \rightarrow \mathbb N$ be defined by $F(n) =
\begin{cases}
2x,  & x\ge 0 \\
-2x-1, & x\lt 0
\end{cases}
$ .
Prove that $F$ is onto. I Considered $y \in \mathbb N$ . To find $x$ for which $F(x) =y$ , $y=2x$ , $x=\frac{y}{2}$ and then $F(x)=y$ and now I 
need to consider other  2 cases – $y$ even and $y$ odd. (stuck) Can you help me out?","['contest-math', 'functions', 'integers']"
3609297,Specific example of infinite series that converges to $0$,"I am looking for an infinite series that all its terms must be distinct integers so that it converges to $0$ . Without the condition specified, there are a lot of examples like $0+0+0+\dots$ $1+(-1)+0+0+\dots$ $(-2)+1+\frac{1}{2}+\frac{1}{4}+\dots$ If the two conditions must be satisfied, the example I can think of is $$1+(-1)+2+(-2)+3+(-3)+\dots$$ But I do not think this series is a convergent series.","['integers', 'real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
3609300,What is the functional derivative?,"I do not understand, if the functional derivative is a function a generalized function (distribution) a functional itself something different (see Euler-Lagrange) To clarify my question, I have seen multiple instances of functional derivative definitions Functionals When the Functional gets Taylor expanded (here using a ""good"" $\eta(x)$ ) we get $$F[y(x)+\epsilon \eta(x)] = F[y(x)] + \frac{dF[y(x) + \epsilon \eta(x)]}{d\epsilon}\Big|_{\epsilon=0}\cdot \epsilon + ...$$ as I understood, the term on the RHS is the functional derivative. But since the LHS is a functional and the RHS is a functional + a real number ( $\epsilon$ ) times the functional derivative, I conclude that the functional derivative must also be a functional. Functions/Distributions The english wikipedia page [2] states, that the functional derivative is defined as $$\int{\frac{\delta F}{\delta \rho} (x)\phi(x)dx}=\frac{dF[\rho(x) + \epsilon \phi(x)]}{d\epsilon}\Big|_{\epsilon=0}$$ notice that the RHS is equivalent to the functional derivative defined above. However, it is $$\frac{\delta F}{\delta \rho} (x)$$ that is defined to be the functional derivative, and not the RHS (as I concluded above). Therefore I may as well assume that the functional derivative is a function/distribution. Something else The solution to the Euler-Lagrange Equation (one dimensional for simplicity) given an Energy Functional $J[y] = \int_{a}^{b}{L(x,y,y')}$ is $$\frac{\delta J}{\delta y} = \frac{dL}{dy} - \frac{d}{dx}(\frac{dL}{dy'}) = 0$$ here, $\frac{\delta J}{\delta y}$ is supposedly the fractional derivative of the integral, which has to be stationary. RHS tells me that the functiona derivative is a differential equation - which has a function as a solution - but I am now completely unsure what the functional derivative in itself actualy is. I have seen multiple viewpoints, each and every one cluttering my intuition even more. For instance the wikipedia article claims that $\frac{\delta F}{\delta \rho} (x)$ has to be seen as a ""gradient"" (which is a vector in multivariate calculus), while $\int{\frac{\delta F}{\delta \rho} (x)\phi(x)dx}$ has to be thought of like a directional derivative (which is the inner product of the gradient and the direction vector). But since there are no bounds on the integral the ""directional derivative"" is also a function, or am I mistaken? [1] http://lab.sentef.org/wp-content/uploads/2016/11/Tutorial_02.pdf page 4 [2] https://en.wikipedia.org/wiki/Functional_derivative","['functional-analysis', 'calculus-of-variations']"
3609310,What is a generating set of the canonical module?,"Let $R = k[x_1,...,x_n]$ be a standard graded polynomial ring over a field $k$ . Let $I$ be an ideal of $R$ such that $R/I$ is Cohen-Macaulay. Then the canonical module $\omega_{R/I}$ can be identified with an ideal in R, say $J$ . For any such identification, $\omega_{R/I}$ is either an ideal of height 1 or equals $R/I$ (by Proposition 3.3.18, ""Cohen-Macaulay rings"" Bruns-Herzog). Now, my question is that what will be a generating set for $J$ ?  The proof of proposition 3.3.18 does not say anything about the generating set. Any ideas/hints are welcome.","['algebraic-geometry', 'commutative-algebra']"
3609412,The Arnol'd conjecture for symplectomorphisms sufficiently close to the identity,"In studying symplectic geometry, a relatively easy corollary of Weinstein's Lagrangian Neighbourhood theorem is the following. On a closed symplectic manifold $(M,\omega)$ with $H^1_{\text{dR}}(M)=0$ , every symplectomorphism sufficiently $C^1$ -close to the identity has at least $\text{Crit}(M)$ fixed points. In the proof of this, it is claimed that if $\phi$ is sufficiently $C^1$ close to the identity, the graph of $\phi$ is contained in a given open neighbourhood $\mathcal{U}$ of the diagonal $\{(x,x)| x\in M\}$ . Why is this claim actually true?
For filling in the details, I would like to find a $C^1$ neighbourhood $\mathcal{N}$ of the identity such that if $\phi\in\mathcal{N}$ , then $\text{Graph}(\phi)\subset \mathcal{U}$ . Intuitively, it is very clear that for $\phi(x)$ sufficiently close to $x$ , we have that $\text{Graph}(\phi)$ is contained in $\mathcal{U}$ . However, how can I choose such an open neighbourhood $\mathcal{N}$ such that it actually works that the graph of each $\phi$ in this open neighbourhood is contained in $\mathcal{U}$ ? I know from Page 37 in Hirsch, Differential topology that the group of $C^1$ diffeomoprhisms $M\to M$ is open in $C^1(M)$ , but I could not find the result in there any further.","['symplectic-geometry', 'fixed-point-theorems', 'manifolds', 'general-topology', 'differential-geometry']"
3609574,Whether a square can be traversed in finite time,"You are at point $C$ inside square $ABCD$ in the Cartesian plane, in which $A=(0,0), B=(0,1), C=(1,1), D=(1,0)$ . You want to get to vertex $A$ . However, your “speed” in $\frac{\text{units}}{\text{sec}}$ is everywhere equal to your y-coordinate. Can you get from $C$ to $A$ in finite time? If you can, what is the minimal time required for the journey? A friend asked me this as a challenge recently out of what I think was a calculus textbook. I haven’t found any concrete way of resolving the question one way or another (or even modeling it properly), but most of my intuition says that the voyage should not be possible in finite time. Specifically, it seems to me that this problem is somehow related to the fact that the harmonic series, as well as the integral of the harmonic series, diverges. On the other hand, perhaps this problem is like Zeno’s paradoxes- an infinite number of decreasing steps adding up to something finite. On solving the problem itself, I know that one can simplify whether it can be done in finite time to whether going straight down from $B$ to $A$ can be done in finite time. On minimizing the time taken (if it exists), I have no idea how to determine how to test infinite functions from $(1,1)$ to $(0,0)$ for their “speed”s, although I conjecture ones that are nowhere concave up should always be faster. $y=\sqrt{x}$","['optimization', 'definite-integrals', 'ordinary-differential-equations', 'sequences-and-series']"
3609592,Why is every smooth quartic in $\Bbb{P}^3$ a K3 surface?,"Usually the first example of a K3 surface presented to us is the Fermat quartic $x_0^4+x_1^4+x_2^4+x_3^4=0$ in $\Bbb{P}_\Bbb{C}^3$ . But I've just found out that actually any smooth quartic in $\Bbb{P}^3$ is K3, and I'm trying to understand why. I know that since $S\subset\Bbb{P}^3$ is a smooth quartic, then  in terms of linear equivalence we have $S\sim 4H$ , where $H\subset \Bbb{P}^3$ is a hyperplane. By the adjunction formula: $$K_S=(K_{\Bbb{P}
^3}+S)\big|_S\sim(-4H+4H)\big|_S=0$$ Now, to prove that $h^1(S,\mathcal{O}_S)=0$ , I have no idea what to do. I imagine that the Fermat quartic has nothing special, and we should be able to prove $h^1=0$ for any smooth quartic without additional difficulties. But I really don't know how to do that.","['complex-geometry', 'algebraic-geometry', 'k3-surfaces']"
3609620,Doubt on property of norm on $L_1(\mathbb{R})$,"we define the space of absolutely integrable functions $$L_1\left( \mathbb{R} \right):=\left \{ f:\mathbb{R} \rightarrow \mathbb{C}~ \bigg{|}~\int_{-\infty}^{\infty}|f(x)|\mathrm{d}x <+\infty\right\}$$ We define the norm on $L_1 \left( \mathbb{R} \right)$ as $$\| f\|_1:=\int_{-\infty}^{\infty}|f(x)|\mathrm{d}x$$ My doubt is that is this really a valid norm ? As this does not satisfy the property that $\|f\|=0 \iff f=0$ . Its clearly satisfies the reverse direction, but not the forward direction. We can consider an example $$\tilde{f}(x): = \left\{\begin{aligned}1 ~~&\text{if}~~x=0\\0 ~~&\text{if}~~x\in \mathbb{R}\setminus\{0\}\end{aligned}\right.$$ This satisfies $\|\tilde{f}\|=0$ even if $f$ is not entirely the zero function. My instructor said this has something to do with ""measure theory"" which i'm not much familiar with. I don't really $\textit{grok}$ this over the top explanation. Will appreciate any help understanding this concept.","['measure-theory', 'normed-spaces', 'functional-analysis']"
3609706,Questionable Proof in Visual Complex Analysis [duplicate],"This question already has answers here : Uniqueness of Powerseries in an arbitrarily small neighborhood of $0$ in $\mathbb C$ (3 answers) Closed 4 years ago . I am currently reading the book ""Visual Complex Analysis"". It's a great book so far, but already in the beginning the proof of the identity theorem seems dubious. I mean, it's known from high-school algebra, that you're not allowed to divide by z if you set z = 0. Isn't this proof completely wrong? Some friends I've asked even told me that this theorem is wrong in the reals, so wouldn't a correct proof have to use properties of the complex numbers? Is it even possible to prove this with elementary methods? I've seen proofs that use properties of holomorphic functions but haven't gotten that far in my book yet, so I have no experience with holomorphic functions.","['complex-analysis', 'solution-verification', 'alternative-proof']"
3609856,Explicitly Understanding a Case of Dolbeault's Theorem.,"If $\Omega^p$ is the sheaf of holomorphic $p$ -forms on a complex manifold $M$ , then Dolbeault's theorem states $$
H^{p,q}(M) \cong H^q(M,\Omega^p)
$$ Setting $p=0,q=1$ , we get $$
H^{0,1}(M) \cong H^1(M,\cal O) \cong \check{H}^1(\cal O)
$$ where $\cal O$ is the structure sheaf, and $\check{H}$ is Cech cohomology. Is there an explicit way to see the isomorphism between Cech cohomology and Dolbeault cohomology in this case?","['complex-geometry', 'algebraic-geometry', 'sheaf-cohomology']"
3609857,Derivatives of $G(x)=\int^{e^x}_1(\log(t))^2dt$ and $H(x)=\int^{x^2}_{-x^2}e^{-t^5}dt$,"I have $2$ tasks: To evaluate $G(x)=\int^{e^x}_1(\log(t))^2dt$ for $x\gt 0$ and $H(x)=\int^{x^2}_{-x^2}e^{-t^5}dt$ for $x \in \Bbb R$ So by the fundamental theorem of calculus: If $F(x)=\int^x_af$ is differentiable at $c$ , then $F'(c)=f(c)$ And by Newton's FTC: $\int^b_af(x)dx=F(b)-F(a)$ So, what I do is : $G'(x)=(\log(e^x))^2-(\log(1))^2=x^2$ And $H'(x)=e^{-x^{10}}-e^{x^{10}}=\frac{1-e^{2x^{10}}}{e^{x^{10}}}$ But , in the answer sheet, the result is: $G'(x)=\log^2(e^x)e^x=x^2e^x$ And $H'(x)=2x(e^{-x^{10}}+e^{-x^{10}})=4xe^{-x^{10}}$ What am I doing/interpreting wrong? Any help is appreciated!","['integration', 'derivatives', 'real-analysis']"
3609901,Sum Infinite Random Variables,"Let's say we generate $n$ samples independently from two independent distributions $X$ and $Y$ . We know that the following is true from Jensen's Inequality: $$\
E\left[\min\left(\sum_{i=1}^{n}X_i, \sum_{i=1}^{n}Y_i\right)\right] \leq \min\left(\sum_{i=1}^{n}E[X_i], \sum_{i=1}^{n}E[Y_i]\right)
$$ I was wondering what happens if $n \to \infty$ . Precisely, $$\
\lim_{n\to \infty}E\left[\min\left(\sum_{i=1}^{n}X_i, \sum_{i=1}^{n}Y_i\right)\right] = \lim_{n\to \infty}E\left[n\times \min\left(\frac{\sum_{i=1}^{n}X_i}{n}, \frac{\sum_{i=1}^{n}Y_i}{n}\right)\right]
$$ From Strong Law of Large Numbers we have, \begin{align}
\lim_{n\to \infty}E\left[n\times \min\left(\frac{\sum_{i=1}^{n}X_i}{n}, \frac{\sum_{i=1}^{n}Y_i}{n}\right)\right]&=\lim_{n\to \infty}E[n\times \min(\mu_X, \mu_Y)]\\&=\lim_{n\to \infty}n\times \min(\mu_X, \mu_Y)
\end{align} For the second term, $$\
\min\left(\sum_{i=1}^{n}E[X_i], \sum_{i=1}^{n}E[Y_i]\right)=n\times \min(\mu_X, \mu_Y)
$$ I know that the above results don't mean much as we have $\lim_{n\to \infty}n\times constant$ . However, my intuition says that for large values of $n$ both the terms should be pretty close. Can you please explain if my intuition is actually correct? Can you please give a proof for the same or a counter-example?","['jensen-inequality', 'expected-value', 'law-of-large-numbers', 'probability-theory', 'probability']"
3609912,Expansion about $x=1$ for $\sum_{n=2}^\infty \frac{x^n}{n\log n}$,"$$\sum_{n=2}^N \frac{1}{n\log n}$$ diverges as $N\rightarrow \infty$ , because the integral $$\int_2^N \frac{{\rm d}t}{t\log t}=\log(\log N) - \log(\log 2)$$ diverges. The singularity is double logarithmic and I therefore expect the series $$f(x)=\sum_{n=2}^\infty \frac{x^n}{n\log n}$$ to have a double logarithmic singularity at $x=1$ i.e. $$f(x) \sim \log \left(-\log\left(1-x\right)\right)$$ as $x\rightarrow 1$ . Is there a simple way to derive the expansion about $x=1$ ? Since $\frac{x^n}{n\log n}$ is monotonic, it may be useful/effective to deploy Euler-Maclaurin Expansion i.e. calculate the integral $$\int_2^\infty \frac{e^{n\log x}}{n\log n} \, {\rm d}n \, ,$$ but no anti-derivative seems to exist. Ultimately this question is related to the existence of these types of integrals $$\int_2^\infty \frac{e^{-tx}}{\log x} \, {\rm d}x$$ for $t>0$ and $t\rightarrow 0$ in terms of (semi)-elementary functions. I found the following asymptotic expansion for any $a>0$ \begin{align}
\int_a^\infty \frac{e^{-xt}}{\log x} \, {\rm d}x &\stackrel{u=xt}{=} \frac{-1}{t\log t} \int_{at}^\infty \frac{e^{-u}}{1 - \frac{\log u}{\log t}} \, {\rm d}u \\
&= \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{1}{\log^n t} \int_{at}^\infty e^{-u} \log^n u \, {\rm d}u \\
&\stackrel{t\rightarrow 0}{=} \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{\Gamma^{(n)}(1)}{\log^n t} \, . \tag{1}
\end{align} The limiting integral for $t\rightarrow 0$ converges and the error by extending the range to $0$ is ${\cal O}\left(t\log^n(t)\right)$ . Therefore the asymptotic expansion follows. Integrating the asymptotic expansion (1) with respect to $t$ then yields $$\int_a^\infty \frac{e^{-xt}}{x\log x} \, {\rm d}x = \log(-\log(t)) - \sum_{n=1}^\infty \frac{\Gamma^{(n)}(1)}{n\log^n t} + C$$ and it can be shown $C=0$ .","['complex-analysis', 'power-series', 'analysis']"
3610063,"If $A_n B_n$ converge to the identity matrix, is it true that $A_n^{1/2} B_n^{1/2} \to I$ as well?","If two sequences of positive definite symmetric matrices $(A_n)$ and $(B_n)$ are such that $A_n B_n \to I$ , is it necessarily true that $A_n^{1/2} B_n^{1/2}$ also converge to the identity? If not, what sort of additional assumptions are needed? (I can see that if $A_n$ and $B_n$ are simultaneously diagonalizable, the implication holds. Are there weaker conditions that also guarantee this?)","['matrices', 'linear-algebra']"
3610075,Evaluate $\int_0^\pi \frac{\sin\frac{21x}{2}}{\sin \frac x2} dx$ (from MIT Integration Bee),"I recently watched the MIT Integration Bee ( $2006$ ) video and stumbled upon this unusual integral: $$\int_0^\pi \frac{\sin\frac{21x}{2}}{\sin \frac x2} dx$$ I thought multiplying up and down by $\cos \frac x2$ would help, after which I got $$ \int_0^\pi \frac{\sin11x + \sin10x}{\sin x}dx = I$$ Now using $\int_0^a f(x) dx = \int_0^a f(a-x) dx$ , $$I=\int_0^\pi \frac{\sin 11x -\sin 10x}{\sin x}$$ and on adding the two we get $$I= \int_0^\pi \frac{\sin 11x}{\sin x}$$ Now there are two paths I could take, either write $\sin 11x$ entirely in terms of $\sin x$ (which is a daunting task) or apply the sine addition rule as $\sin 11x = \sin(10x + x)$ . Doing the latter gives $$I= \int_0^\pi \frac{\sin 10x}{\sin x} \cos x \space dx + \int_0^\pi \cos 10xdx$$ $$= \int_0^\pi \frac {\sin 10x}{\sin x} \cos x\space dx$$ Do I keep going from here by using the sine addition rule again? Or is there a better way? There probably is.","['integration', 'trigonometric-integrals', 'definite-integrals', 'contest-math']"
3610204,How was this circle formula be simplified?,"I am currently doing some questions for loci of complex numbers and this question stumped me. I did some algebra, and got to here: $$128(x^2 - 2x) + 144y^2 = 1024.$$ However, the answer scheme then simplifies this to: $$128(x-1)^2 + 144y^2 = 1152.$$ How did this happen? Shouldn't the $1024$ become $1025$ instead (from the $(x-1)^2$ )?","['analytic-geometry', 'proof-explanation', 'algebra-precalculus', 'complex-numbers']"
3610219,"Is $Tf(x)=\frac{1}{x}\int_{0}^{x}f(y)dy$ bounded as operator on $L^2((0,1);\mathbb{R} )$?","Given the operator $T:L^2((0,1);\mathbb{R} )\rightarrow L^2((0,1);\mathbb{R} )$ defined by $Tf(x)=\dfrac{1}{x}\displaystyle\int_{0}^{x}f(y)\,\mathrm dy$ , say if it is well defined and discuss its boundedness. This is part of an exercise which asked the same thing for operators of the form $T_{\alpha}f(x)=\dfrac{1}{x^\alpha}\displaystyle\int_{0}^{x}f(y)\,\mathrm dy$ with $\alpha >0$ . We find that $T_\alpha$ is bounded for $\alpha \in (0,1)$ and not even well defined for $\alpha >1$ . For $\alpha=1$ we tried to exhibit a sequence $f_n\in L^2$ for which $\|Tf_n\|^2_2/\|f_n\|^2_2$ diverges, but we find that truncated funtions $h(x)\chi(x)_{(1/n,1)}$ with $h(x)=x^\beta$ or $\dfrac{\ln(x)}{x}$ don't make the trick. Another attempt was to write $f_n(x)=\Sigma_kf_{nk}(x)\chi(x)_{(s(k),s(k+1))}$ for some ""partitioning function"" $s$ and hope to balance the growth of $f_{nk}$ with the speed of interval $(0,1)$ subdivision. However calculations are very heavy, and led us nowhere. Another thought was that $T$ could maybe by bounded over some dense subspace, which would answer to the question in a certain sense. Thank you for reading, hope the best.","['operator-theory', 'real-analysis', 'hilbert-spaces', 'functional-analysis', 'unbounded-operators']"
3610265,A stronger version of Cauchy-Goursat Theorem,"Claim: $C$ is a simple closed contour, $f$ is continuous at all points interior and on $C$ , and $f$ is analytic at all points interior to $C$ , then $\int_C f(z)dz = 0$ . To prove this, I suppose we may somehow approach $\int _{C}f(z) dz$ by a sequence contour integrals on closed curve inside $C$ , which has value zero by the usual Cauchy Goursat theorem, but can't quite finish the proof. I saw a similar quesiton Here , but not quite satisfied with the answer, in particular, the step of bounding the difference of integrals around two different contours. I think the post refers to the ML theorem to bound each contour integrals, but I am not sure how the separate bound for each can be put together to give a bound for the difference. And ideas or comments are much appreciated.","['complex-analysis', 'contour-integration']"
3610269,"If a tournament graph has no cycles of length $3$, prove that it is a partial order.","If a tournament graph has no cycles of length $3$ , prove that it is a partial order. I was thinking that perhaps a proof by contradiction might helpful. Could I start with a tournament graph $G$ that has a cycle of $3$ , assume that it is a partial order, and then find some contradiction/violation of the properties of a partial order (i.e. show that $G$ does not fulfill antisymmetry or transitivity)?","['directed-graphs', 'graph-theory', 'order-theory', 'combinatorics', 'discrete-mathematics']"
3610297,domain and range of trigonometric function,"I have a question relating to finding domain and range. If you have something like $f(x) = \sqrt{\sin x}$ , or $\sqrt{\sin 3x}$ , how to you work out the domain for this. I have viewed an answer to see if I could work it out myself, but I am struggling to understand it. I realise this function is an indefinite disjointed interval, however I struggling to understand how to express the interval when the endpoints are fixed and infinite?? The domain is expressed as the following: $0 \leq 3x - 2\pi n \leq \pi$ and $n\in\mathbb{Z}$ . I am unsure where the n has come from? Also, how is it determining the regular value pie. Is $\mathbb{Z}$ referring to imaginary numbers. I am only just getting into mathematics again. Thank you.",['trigonometry']
3610320,How to find $\int \frac {(1+x^2)(2+x^2)}{(x \cos x+\sin x)^4}dx$?,"I came across this integral while studying indefinite integrals. So far I have had many unsuccessful attempts which include - trying by parts - but I could not find a way to proceed with it. I even tried dividing both numerator and denominator by $x^4$ to yield $$\int \frac {(1+x^{-2})(1+2x^{-2})}{( \cos x+ \frac \sin x)^4}dx.$$ But I am still not able to move forward. I even tried to cheat a little bit by taking derivative of options, still nothing. And many more.. So the problem still stands. Can someone tell me how to proceed? (Note:  this is a problem from very elementary calculus course so no contour integrals, no multivariable and such stuff, however I think differentiation under integration would be fine. Also it would help if answers are one of those present in options (see image).) Edit: Thanks to comments now I know answer is C but I was wondering if anyone could show me a straightforward way to do it thanks!
Edit 2 : turns out B is also correct.","['integration', 'indefinite-integrals', 'trigonometric-integrals']"
3610341,How to show :$\int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt \frac {2018}{2020}$,I tried to solve this problem as $$0\lt x \lt 1 \implies 0\lt x^{2018} \lt 1 \implies 0\lt (1-x^{2018}) \lt 1$$ this means $$\int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt   \int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}\right] dx \lt \int_0^1 1^{{1\over 2020}} dx$$ as inequality can be integrated. So I have come as far as proving the given  expression is less than 1 but I cannot proceed further. Can someone show me how to proceed? (Please try give answers which can be understood by those in elementary calculus courses),"['integration', 'calculus', 'definite-integrals', 'inequality']"
3610375,Prove $ \sum\limits_{k = 1}^{n} ( - 1)^{\lfloor k\alpha \rfloor}$ is unbounded where $\alpha$ is irrational.,"Obviously, value of $ ( - 1)^{\lfloor k\alpha \rfloor}$ depends on $ \{k\alpha\}$ . This sequence is uniformly distrubuted $ \mod 1$ . We have $ \sum\limits_{k = 1}^{n} ( - 1)^{\lfloor k\alpha \rfloor} = \sum\limits_{k = 1}^{n} f\left(k\frac {\alpha}{2} \right)$ , where $ f$ is defined by $ f(x) = \left\{\begin{array}{cc} 1 & 0\leqslant x < \frac {1}{2} \\
-1 & \frac {1}{2}\leqslant x < 1 \end{array} \right.$ How to go on with this?","['limits', 'calculus', 'sequences-and-series']"
3610413,Differential equation $\frac{dy}{dx}-2(3\cos x+5)y=-1$,How can i solve the differential equation $$\frac{dy}{dx}-2(3\cos x+5)y=-1$$ What i have try It represent a linear differential equation of degree and order $1$ So compare with $\frac{dy}{dx}+Py=Q$ We have $P=-2(3\cos x+5)$ and $Q=-1$ And Integrating factor $\text{(I.f)} =e^{\int 2(3\cos x+5)dx}=e^{-2(3\sin x+5x)}$ So solution is $$ y=\int Q\text{(I.f)}dx=-\int e^{-2(3\sin x+5x)}dx$$ How do i solve it Help me please or How can i write its solution . Thanks Update: wolframalpham alpha show as How can i  write  it solution  in that form. !,['ordinary-differential-equations']
3610470,Complex numbers algebra problem involving cyclic summation,"Let $a_1$ , $a_2$ , $a_3\in \mathbb{C}$ and $|a_1|=|a_2|=|a_3|=1$ . If $\sum\frac{a_1^{2}}{a_2 a_3}=-1$ , find $|a_1 + a_2 + a_3|$ What I have done till now: First, I tried to attack the required sum directly. Let $\alpha=|a_1 + a_2 + a_3|$ , then squaring both sides we get , $$\alpha^{2}=(a_1 + a_2 + a_3)\left(\frac{1}{a_1} + \frac{1}{a_2} + \frac{1}{a_3} \right)$$ since $|a_1|=|a_2|=|a_3|=1$ and $|z|^{2}= z\overline{z}$ , but it did not yield much as i perceived. Then in the given sum, $\sum\frac{a_1^{2}}{a_2 a_3}=-1$ , i tried taking LCM on left side and on solving I got, $a_1^{3} + a_2^{3} + a_3^{3} = -a_1 a_2 a_3$ . On manipulations, we get $$(a_1 + a_2 + a_3)(a_1^{2} + a_2^{2} + a_3^{2} - a_1 a_2 - a_2 a_3 - a_3 a_1) = 2a_1 a_2 a_3.$$ Here I am facing a dead end. I even tried taking conjugate of $\sum\frac{a_1^{2}}{a_2 a_3}=-1$ , and add the 2 equations but it does not seem to be helping much . Please help me with this problem.","['algebra-precalculus', 'summation', 'complex-numbers']"
3610550,Determine eigenvalues of $B:=\left(\begin{smallmatrix}0&A\\A^T&0\end{smallmatrix}\right)$ in terms of the singular values of $A$,"Let $A\in\mathbb R^{m\times n}$ and $$B:=\begin{pmatrix}0&A\\A^T&0\end{pmatrix}.$$ How can we determine the eigenvalues of $B$ ? Let $r:=\operatorname{rank}A$ . By the singular value decomposition, there are $\sigma_1,\ldots,\sigma_r>0$ ( not necessarily distinct) and orthonormal systems $(v_1,\ldots,v_r)$ and $(u_1,\ldots,u_r)$ of $\mathbb R^n$ and $\mathbb R^m$ with $$A=\sum_{i=1}^r\sigma_iv_i\otimes u_i\tag1.$$ Now, clearly, \begin{align}Av_i&=\sigma_iu_i,\\A^Tu_i&=\sigma_iv_i\tag2\end{align} for all $i\in\{1,\ldots,r\}$ and hence $$B\begin{pmatrix}u_i\\v_i\end{pmatrix}=\sigma_i\begin{pmatrix}u_i\\v_i\end{pmatrix}\;\;\;\text{for all }i\in\{1,\ldots,r\}.\tag3$$ So, $\sigma_1,\ldots,\sigma_r$ are eigenvalues of $B$ . How do we determine the other eigenvalues? And is there a formula for $\operatorname{rank}B$ in terms of $r$ ?","['eigenvalues-eigenvectors', 'operator-theory', 'linear-algebra', 'functional-analysis', 'spectral-theory']"
3610557,Mean value theorem with integrals?,"Here is the question: Let $f:[0, 1]\rightarrow \mathbb{R}$ be a continuous function satisfying $$\int_0^1 (1-x)f(x) \,dx = 0$$ Show that there exists $c\in (0, 1)$ such that $$\int_0^c xf(x)\,dx = cf(c)$$ I'm pretty sure that the problem wants me to use the mean value theorem of some type. So I tried to consider a function $F(t)$ that would give the form $$F'(t) = \int_0^t xf(x)\,dx -tf(t)$$ so that I would be able to say $F'(c) = 0$ for some $c\in (0, 1)$ , using Rolle's theorem. But this gave me $$F(t) = \int_0^t ((t-1)x-x^2)f(x)\,dx$$ which didn't really help me proceed any further. I have also tried setting $$F(t) = \int_0^t (t-x)f(x)\,dx$$ in hopes of using Rolle's theorem, since $F(0)=F(1)=0$ . But $F'(t)$ wasn't really the required form. I've also tried other different forms so that I could apply mean value theorem for integrals, or Cauchy's mean value theorem. But I couldn't derive the correct form to solve the problem. Maybe I have missed something? Or can someone provide me a different approach to this problem? Thanks in advance.",['calculus']
3610610,Does specifying two elements in a set imply they are not the same element?,"Pretty basic question regarding set theory notation; Suppose I have a set $A$ . If I then write $a,b\in A$ , does that automatically imply $a\neq b$ due to the fact I've used two separate dummy variables? Is it possible to have $a=b$ even if the value of $a=b$ is unique in $A$ ? Thanks","['elementary-set-theory', 'notation', 'discrete-mathematics']"
3610629,$\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx$ and $\lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx$,"Like the title says, I'm curious if anyone has any insight on trying to compute these limits. Numerical investigations seem to indicate that $$\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx = 2$$ and $$\lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx = \pi$$ For the first one, it seems to almost follow from dominated convergence since $$\lim_{n\to\infty} \int_{-1}^1 \cos(x^{2n}) \:dx = \int_{-1}^1 \cos(0) \:dx = 2$$ All there is left to prove is that $$\lim_{n\to\infty} \int_1^\infty \cos(x^{2n})\:dx = 0$$ I've tried integrating by parts and a Fourier transform argument, but nothing seems to definitively pin this limit as being zero in a rigorous way. For the other one I am completely at a loss as to where the $\pi$ would come from in a dominated convergence style argument since the usual trick would give some multiple of $\sin(1)$ . Granted, the limit may not be $\pi$ , but I am having even less luck with this limit than the other. Any tips are appreciated.","['integration', 'limits', 'calculus', 'improper-integrals']"
3610670,Polynomials and the Generalized Mean Value Theorem,"I want to prove the following proposition: Proposition. There exist real polynomials $f$ and $g$ such that $g(0)\neq g(1)$ and $$ \frac{f(1)-f(0)}{g(1)-g(0)}\neq \frac{f^{'}(\xi)}{ g^{'}(\xi)}$$ for all $\xi\in (0,1)$ . It seems directly related to the generalized mean value theorem (MVT) but am not sure how to prove it. The negation of the statement is: For all real polynomials $f$ and $g$ such that $g(0)\neq g(1)$ there exist $\xi\in (0,1)$ such that $$ \frac{f(1)-f(0)}{g(1)-g(0)}= \frac{f^{'}(\xi)}{ g^{'}(\xi)}$$ which is true if $g^{'}(\xi)\neq 0$ by the generalized MVT. Any help is greatly appreciated.","['calculus', 'derivatives', 'real-analysis']"
3610690,Finding the domain of ODE without calculating a solution,"We have two ODEs: \begin{cases}
    x'(t) = 2x^2-t \\
    x(1) = 1
  \end{cases} and \begin{cases}
    x'(t) = t+e^x \\
    x(1) = 0
  \end{cases} I am to find(without calculating, only using theorems and slope fields) an interval on which the solution exists. They both are not Lipschitz so Picard–Lindelöf's doesn't work, but Peano does. We know that a solution exists in $[1 - \eta, 1 + \eta]$ . I drew a slope field which seems to be very steep for points outside $[-2, 2]$ for the first one and very steep for $x > 1$ for the second one. What can I do next?","['ordinary-differential-equations', 'real-analysis']"
3610735,Find without L'Hospital's rule: $\lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}}$,"$A = \sqrt{17 - 2x^{2}} \sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} \\ 
\Rightarrow \lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}} \\ 
=  \lim\limits_{x \to 2} \frac{A^{6} - 9^{6}}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\ 
=  \lim\limits_{x \to 2} \frac{(x - 2)^{2}(-72x^{10} - 192x^{9} + 940x^{8} + 2576x^{7} + 874x^{6} + 1992x^{5} - 24543x^{4} - 73908x^{3} - 82540x^{2} - 200414x - 102154)}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\ 
=  \frac{-2.11.3^{10}}{6.9^{5}} = \frac{-11}{3}.$ Am I right? Is there a simple way?","['limits', 'functions', 'limits-without-lhopital']"
3610756,"Is $(C^1[0,1],\|\cdot\|)$ with $\|f\|:=|f(0)|+\sup_{0\le{t\le{1}}}{|f'(t)|}$ a Banach space?","Let $(C^1[0,1],\|\space{}.\|)$ be a normed space where $C^1[0,1]$ is the set of functions with continuous derivatives and let $\|\space{}.\|$ be the norm on this set defined by: $$\|f\|:=|f(0)|+\sup_{0\le{t\le{1}}}{|f'(t)|}.$$ Is this space Banach? My attempt: I don't think it is. Here is my counter example: Let $(f_n)_{n=1}^{\infty}$ be a sequence of functions defined by $f_n(t)=\sqrt{(t-\frac{1}{2})^{2}+\frac{1}{n}}$ .
This clearly belongs to $C^1[0,1]$ but its limit does not, namely $f(t)=|t-\frac{1}{2}|$ . My issue however is this hasn't shown the sequence converges to $f$ with respect to the norm $\|\space{}.\|$ . I don't know how to use this example, since the norm doesn't make sense with $f$ , since f is not differentiable on t = 1/2. But does this counter example work?","['banach-spaces', 'sequence-of-function', 'real-analysis', 'continuity', 'functional-analysis']"
3610995,"For all real $x, y$ that satisfy $x^3+y^3=7$ and $x^2+y^2+x+y+xy=4$","I started with $x+y=a$ and $xy=b$ and I rewrote the equations with a and b.
I got $$b=a^2+a-4$$ $$x^3+y^3=2a^3+3a^2-12a^2=7$$ $$f(a)=-2a^3+3a^2-12a^2-7=0$$ I factorised it to get $$f(a)=-(a-1)(a-1)(2a+7)=0$$ So $a=1,b=-2$ or $a=\frac{-7}{2},b=\frac{19}{4}$ Now I know that values of a and b but how do I get x and y? Note:
The textbook solution says that if we consider a=1 and b=-2 then x and y are the roots of $$t^2+t-2=0$$ But shouldn't it be $$t^2-t-2=0$$ from Vieta's formulas?
I would like a clarification for this or any other solutions would be fine too.","['contest-math', 'systems-of-equations', 'factoring', 'symmetric-polynomials', 'algebra-precalculus']"
3611010,Homotopy type of $\Bbb CP^2 - \Bbb RP^2$,"Consider the complex projective plane $\Bbb CP^2$ . We can embed $\Bbb RP^2$ in $\Bbb CP^2$ in a natural way, namely, $[x_0:x_1:x_2]\in \Bbb RP^2 \mapsto [x_0:x_1:x_2] \in \Bbb CP^2$ . We can thus consider $\Bbb RP^2$ as a subspace of $\Bbb CP^2$ . On the other hand, let $Q=\{[z_0:z_1:z_2]\in \Bbb CP^2 :  z_0^2+z_1^2+z_2^2 =0\}$ . Clearly $Q$ does not intersect $\Bbb RP^2$ , so we have $Q \subset \Bbb CP^2-\Bbb RP^2$ . Does $\Bbb CP^2 - \Bbb RP^2$ deformation retracts onto $Q$ ? Actually I want to show that $\Bbb CP^2-\Bbb RP^2$ is homotopy equivalent to $S^2$ in two steps, by first showing that $\Bbb CP^2-\Bbb RP^2$ is homotopy equivalent to $Q$ and then showing $Q$ is homotopy equivalent to $S^2$ . Though I have no idea with the latter one, I want to manage the former one first. What I know about these is, that if we let $K=\{(z_0,z_1,z_2)\in \Bbb C^3-0:z_0^2+z_1^2+z_2^2=1\}$ , then the canonical projection $K\to \Bbb CP^2-Q$ is a surjective, two-to-one, covering map, and that $K$ is homeomorphic to $TS^2$ and hence deformation retracts onto $S^2$ . Thus $\Bbb CP^2-Q$ is homotopy equivalent to $\Bbb RP^2$ . (But I'm not sure that these informations is helpful for the above questions) Any hints or ideas please? (I hope some elementary approaches, if possible.)","['general-topology', 'homotopy-theory', 'algebraic-topology']"
3611032,"Taylor polynomial: the higher the degree, the better the approximation?","Let $f$ be an infinite times differentiable function. Is it true that: the higher the degree $n$ of the Taylor polynomial $T_{n,f,x_0}$ of $f$ around $x_0$ , the better the approximation? Some thoughts . Given $n$ , polynomial $T_{n,f,x_0}$ is the best approximation of $f$ near $x_0$ that fulfils the requirement of equal derivatives with $f$ at $x_0$ . So regarding polynomials of degree  at most $n$ , $T_{n,f,x_0}$ is the winner. On the other hand, although one would like $T_{n,f,x_0}$ to fit better function $f$ , as $n$ grows larger, it seems to me that there is no reason for this to happen. Of course, one should define what ""fit better"" means. In our case, it would be something like: $$\sup_{x\in I}|T_{n+1,f,x_0}(x)-f(x)|\leqslant \sup_{x\in I}|T_{n,f,x_0}(x)-f(x)|$$ in a neighborhood $I$ of $x_0$ . Of course, I must admit that the cases I see graphically most of the times, fulfill the last requirement, by fitting better and better the graph of $f.$ Thank in advance for the help.","['calculus', 'taylor-expansion', 'analysis', 'real-analysis']"
3611070,Bound on matrix product $\begin{bmatrix} 1+\frac{1}{n} & -1 \\ 1 & 0 \end{bmatrix}\cdots\begin{bmatrix} 1+\frac{1}{2} & -1 \\ 1 & 0 \end{bmatrix}$,"Let matrices $$A_n = \begin{bmatrix} 1+\frac{1}{n} & -1 \\
1 & 0 \end{bmatrix}$$ and the matrix product $$X_n = A_n A_{n-1} \cdots A_3 A_2 \quad .$$ Determine a bound on $|(X_n)_{1,1}|$ , independent of $n$ . The actual form of the product gets pretty involved.  It appears from numerical simulations that $|(X_n)_{1,1}| \le |(X_2)_{1,1}| = 1.5$ but I wouldn't know how to prove it. A less tight bound, independent of $n$ , would be fine too.","['matrices', 'algebra-precalculus', 'inequality', 'sequences-and-series']"
3611100,Show that function is a diffeomorphism,"Consider two non-empty, bounded and convex domains $A \subset B \subset \mathbb{R}^d$ with $C^2$ -boundaries.
Define a function $\phi:\partial A \to \partial B$ by $x\mapsto y$ such that $y-x=\lambda_x\vec{n}_A(x)$ for some $\lambda_x >0$ where $\vec{n}_A(x)$ is the outer normal of $\partial A$ at $x$ . So roughly speaking this function maps any boundary point of $A$ to its ""orthogonal counterpart"" on the boundary of $B$ . I expect that this function is homeomorphic (this should follow from the fact that any boundary of a convex set is homeomorphic to the unit disk) and I also guess that it is a diffeomorphism due to the $C^2$ -boundaries but I don't know how to show this. In particular I am interested in a explicit expression for $|\det (D\phi(x))|$ as I want to use the integration by substitution formula. Can anyone help me with these two problems? I appreciate any help.","['geometry', 'analysis', 'real-analysis', 'calculus', 'differential-geometry']"
3611196,The product topology of weak topologies is the same as the weak topology of the product space?,"I am reading Brezis' Functional Analysis. On page 62, in Theorem 3.10, it says ... $E \times F$ equipped with the product topolgy $\sigma(E, E^\star) \times \sigma(F, F^\star)$ , which is clearly the same as $\sigma(E\times F, (E \times F)^\star)$ . Here, $E$ and $F$ are Banach spaces and $\sigma(E, E^\star)$ and $\sigma(F, F^\star)$ are their weak topologies respectively. Actually, I showed that $\sigma(E, E^\star) \times \sigma(F, F^\star) \subset \sigma(E\times F, (E \times F)^\star)$ in the following way. Let $\pi: E \times F \to E$ with $\pi(x,y) = x$ and $f_1 \in E^\star$ . Then, since $f_1 \circ \pi \in (E \times F)^\star$ , $(f \circ \pi)^{-1}(O_1) = \pi^{-1}(f_1^{-1}(O_1)) = f_1^{-1}(O_1) \times F \in \sigma(E\times F, (E \times F)^\star)$ for an open set $O_1$ in $\mathbb{R}$ . In the same manner, if $f_2 \in F^\star$ , then $f_2^{-1}(O_2) \times F \in \sigma(E\times F, (E \times F)^\star)$ for an open set $O_2$ in $\mathbb{R}$ . Therefore, $f_1^{-1}(O_1) \times f_2^{-1}(O_2) \in \sigma(E\times F, (E \times F)^\star)$ . Since $f_1^{-1}(O_1) \times f_2^{-1}(O_2)$ is a basis member of $\sigma(E, E^\star) \times \sigma(F, F^\star)$ , $\sigma(E, E^\star) \times \sigma(F, F^\star) \subset \sigma(E\times F, (E \times F)^\star)$ . However, I cannot show $\sigma(E, E^\star) \times \sigma(F, F^\star) \supset \sigma(E\times F, (E \times F)^\star)$ . How can we show this? Any help would be really appreciated!","['general-topology', 'functional-analysis', 'weak-topology']"
3611197,Proving that a rational map is an endomorphism of an elliptic curve,"I recently posted Morphisms between projective varieties but haven't yet had a response, so I've tried to address the problem from a different perspective (namely, following the definitions given the 1st chapter of Silverman's Arithmetic of Elliptic Curves). Consider the curve (defined over an algebraically closed field) in $\mathbb{P}^2,$ given by $E:\;y^2z=x^3-xz^2.$ Problem. I want to show that the rational map $\phi=(xz:yz:-x^2)$ is actually a morphism $E\to E.$ Attempt. It's clear that $\phi$ is regular everywhere except at $P=(0:0:1)$ and $Q=(0:1:0).$ We show it's regular at these points too as follows: Observe that $$(xz:yz:-x^2)=(x^2z:xyz:-x^3)=(x^2z:xyz:-y^2z-xz^2)=(x^2:xy:-y^2-xz)$$ and this last term is clearly regular at $Q;$ in particular, we have $\phi(Q)=P.$ Similarly, observe that $$(xz:yz:-x^2)=(xyz:y^2z:-x^2y)=(xyz:x^3-xz^2:-x^2y)=(yz:x^2-z^2:-xy)$$ and this last term is clearly regular at $P$ ; in particular, we have $\phi(P)=Q.$ This shows that $\phi$ is indeed a morphism $E\to E.$ I'd really like to know whether what I've done is right! Moreover, I'd be very interested to know how this matches up with the seemingly more abstract proof given by Mumford in his red book (see link at top of question).","['algebraic-geometry', 'solution-verification']"
3611207,Embedding Sobolev $H^1$ into $L^\infty$ space,"My question is to eventually prove the following inequality: for $f\in H^1(\mathbb{R})=\{f,f'\in L^2\}$ $$\|f\|_{L^\infty}\leq a\|f\|_{L^2}+\frac{1}{a}\|f'\|_{L^2}, \forall a>0.$$ Here are my thoughts: 1). Sobolev embedding tells us: $$ H^1(\mathbb{R})\hookrightarrow C^{0,1/2}(\mathbb{R}),$$ for $\Omega$ being a compact subset of $\mathbb{R}$ . 2). Coverage in $C^{0,1/2}$ implies Converge in $L^\infty$ (still need compact support?) 3). It seems that we may have now $\|f\|_{L^\infty}\leq C\|f\|_{H^1}$ over any compact subset. My confusion is how to remove the compactness requirement (Lebesgue Dominated convergence theorem?) and how to prove the final version of that inequality? New thoughts : suppose $f_n=f \chi_{[-n.n]}$ , then via FTC, we have $$f_n(z)=\int^z_af'(s)ds+f(a),$$ here we can choose $a$ such that $|f(a)|\leq \frac{1}{2n}\int_{-n}^n|f(s)|ds$ , then by Cauchy-Schwartz inequality, $$|f_n(z)|\leq \sqrt{2n}\|f'\|_{L^2}+\frac{1}{\sqrt{2n}}\|f\|_{L^2}.$$ Then I try to apply DCT, which is not allowed in this case.","['complex-analysis', 'sobolev-spaces', 'functional-analysis', 'integral-inequality']"
3611218,Does Lebesgue Change of Variables only hold for Borel functions?,"In Bogachev, Measure Theorem , Theorem 3.7.1 states: If $F : \mathbb{R}^n \supseteq U \to \mathbb{R}^n$ is $C^1$ and injective, then for any $A \subseteq U$ Lebesgue measurable and any Borel function $g \in L^1(\mathbb{R}^n)$ , on has the equality: $\int_A g(F(x)) |JF(x)|dx = \int_{F(A)} g(y) dy$ , where $JF(x)$ is the Jacobian determinant of $F$ at $x$ . I am wondering about if/how this can be extended to Lebesgue functions $g$ . Since if $g$ is Lebesgue, $g=h$ $\lambda$ a.e. for some $h$ Borel, where $\lambda$ is Lebesgue measure, we would like to be able to just put $h$ in for $g$ and show that the equality holds for all Lebesgue measurable functions. This may not work though: $g=h$ $\lambda$ a.e. doesn't clearly imply $g \circ F = h \circ F$ , in fact for $F$ a constant function one can easily find $g,h$ s.t. $g=h$ a.e. but $g\circ F(x) \neq h \circ F(x)$ for all $x$ . But we have the condition that $F$ is injective and $C^1$ . In this circumstance can we prove that $g \circ F = h \circ F$ a.e. (I'm assuming not, otherwise the theorem would have been stated to include Lebesgue measurable functions). If not, what is a counterexample? Does this provide a counterexample to the above theorem but for Lebesgue measurable functions? Note that for $F$ a diffeomorphism onto its image, we have a well defined $F^{-1}$ , which is $C^1$ by the inverse function theorem, hence locally Lipschitz. Locally Lipschitz functions preserve null sets, hence $F \circ g = F \circ h$ $\lambda$ a.e. provided $g=h$ $\lambda$ a.e. I am looking for a strengthening of this result.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3611256,Is there a geometry where the distance between two points can be complex?,This tweet contained this image which is of course complete nonsense but it got me thinking -- is there such a thing where the distance of two points is a complex number? Ps. it seems such questions are fit for this SE for example Can there be a geometry where angles can be infinite?,['geometry']
