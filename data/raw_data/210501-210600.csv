question_id,title,body,tags
4231135,Semi-simple elements of reductive linear algebraic group is dense.,"I have seen several claims of the fact that the set of semi-simple elements of a connected, reductive linear algebraic group over an algebraically closed field is dense,
moreover that the set of regular semi-simple elements is open dense.
How can I show these facts? For simplicity it is enough to assume characteristic zero if it simplifies the argument.","['algebraic-geometry', 'algebraic-groups']"
4231158,Is there an infinite topological space with only countably many continuous maps to itself?,"Now cross-posted to Mathoverflow. Is there an infinite topological space $X$ with only countably many continuous functions to itself? Such a space would have only countably many points because the constant functions are continuous. A space with countably many points such that only the constant functions and the identity are continuous would work. I wasn't able to find something: all the topological spaces I can think of have $2^{\aleph_0}$ continuous maps to themselves. We cannot find a counterexample among the metrizable spaces. Let $X$ be an infinite countable metric space with metric $d$ . If $X$ is discrete, it is clearly false, so suppose that there is some point $x$ which is not isolated. Let $(r_i)_{i \in \mathbb{N}}$ be a strictly decreasing sequence of real numbers converging to $0$ such that for any $n \in \mathbb{N}$ , there is no point $y$ with $d(x,y) = r_n$ , and such that there is a point $y$ with $r_n > d(x,y) > r_{n+1}$ . We define $$B_{n+1} = \{y \in X \mid r_n > d(x,y) > r_{n+1} \}\space \text{and} \space B_0 = \{y \in X \mid d(x,y) > r_0 \}$$ For any integer $n$ , we choose such a point $y_n$ in $B_n$ . Then, we can define the continuous function $$f : X \to X \space \text{as} \space f(x) = x \space \text{and} \space f(y) = y_n \space \text{for}\space  y \space \text{in}\space  B_n$$ Then, for each $n \in \mathbb{N}$ , you can choose to swap $y_{2n}$ and $y_{2n+1}$ or not, giving you $2^{\aleph_0}$ continuous maps. Another large class of examples that I know of are Alexandrov topologies , however, each Alexandrov topology corresponds to a preorder, and the continuous maps between two Alexandrov topologies correspond to the morphisms between the preorders. An infinite countable preorder has always $2^{\aleph_0}$ endomorphisms, hence I cannot find a counterexample there either. I looked for other examples in the Counterexamples in topology book, but nothing looked promising (not that I proved it for every countable space in the book.) More generally, for any infinite cardinal $\kappa$ , is there a topological space with $\kappa$ points and exactly $\kappa$ -many continuous maps to themselves? An obvious example for $2^{\aleph_0}$ is $\mathbb{R}$ with the euclidean topology. In fact, for any infinite cardinal $\kappa$ , seeing $\kappa$ as a discrete space, we can define the space $X = \kappa^{\aleph_0}$ with the product topology, and this space has $2^\kappa$ continuous functions from itself to itself, because the continuous functions $f : X \to X$ are determined by their values on the sequences which are eventually constant. Hence, for any cardinal $\kappa$ such that $2^\kappa = \kappa^{\aleph_0}$ , we know that the answer is positive for $2^\kappa$ . Edit: Using the $\pi$ -Base , an online database of topological spaces inspired by the book Counterexamples in topology and expanding it, I obtained this list of possible spaces . I proved for every one of these spaces that there were too many continuous maps, except for the Relatively prime integer topology (also known as the Golomb space) and the Prime integer topology. The first one was proved to have too many continuous maps , and the second one is very similar to the first one, so I don't place much hope on it. We need to look somewhere else. Here's my last idea: if we take $F$ a filter on $\mathbb{N}$ , adding the empty set, we obtain a topological space. Could some space obtained this way have only countably many continuous functions to itself? Could it be always true if the filter is an ultrafilter? I don't know how to answer this question.","['general-topology', 'cardinals']"
4231168,Fourier coefficients of $\mathrm{e}^{\mathrm{e}^{\mathrm{i}x}}$?,"The question is to find the complex Fourier  coeff. of $ f(x)=\mathrm{e}^{\mathrm{e}^{\mathrm{i}x}}$ This leads to an integral $\int_{0}^{2\pi} \mathrm{e}^{\mathrm{e}^{\mathrm{i}x}-\mathrm{i}kx} \,dx$ ,
which I have no idea how to solve.","['integration', 'fourier-series']"
4231184,Calculate the volume between $z=\sqrt{x^2+y^2}$ and $z=x^2+y^2$.,"Calculate the volume between $z=\sqrt{x^2+y^2}$ and $z=x^2+y^2$ . Attempt We project on the $xy$ plane the intersection between $z=\sqrt{x^2+y^2}$ and $z=x^2+y^2$ , which is the circle $x^2+y^2=1, z=1$ . We can conclude that the region between $z=\sqrt{x^2+y^2}$ and $z=x^2+y^2$ can be described by $$-1\leq x\leq 1, -\sqrt{1-x^2}\leq y \leq \sqrt{1-x^2}, x^2+y^2\leq z \leq \sqrt{x^2+y^2}$$ The volume is given by $$V=\iiint_W dxdydz=\int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \int_{x^2+y^2}^{\sqrt{x^2+y^2}} dxdydz$$ When I try to solve this, I get a difficult expression and cannot calculate it. So I think, everything I have done is wrong.","['multivariable-calculus', 'multiple-integral', 'volume']"
4231230,Derivative of $\cos^{-1}\sqrt{\frac{1+x}2}$ using substitution,"Find the derivative of $$\cos^{-1}\sqrt{\frac{1+x}2}.$$ I'm learning differentiation and calculus for the first time. I can easily find the derivative of the given expression by chain rule. But the book from which I'm learning calculus encourages finding derivatives of inverse trigonometric functions of algebraic functions with substitution rather than using chain rule. So, I want to find the derivative of this function with substitution. Here is my attempt to do that: Let $x=\cos2\theta$ , then $\theta=\frac{\cos^{-1}x}2$ . Now, $\begin{align}\cos^{-1}\sqrt{\frac{1+x}2} &= \cos^{-1}\left(\frac1{\sqrt2}\sqrt{1+\cos2\theta}\right)\\ &= \cos^{-1}\left(\frac1{\sqrt2}\sqrt{1+\cos^2\theta-1}\right)\\ &= \cos^{-1}\left(\frac1{\sqrt2}\cos\theta\right)
\end{align}$ I can't proceed further from here. Can we write this as $\frac1{\sqrt2}\theta$ ? And please do not give a solution using chain rule, as this would be of no help to me.","['inverse-function', 'substitution', 'calculus', 'trigonometry', 'derivatives']"
4231265,Proving $\pi$ is irrational with recursive integrals,"Given $$I_n = \int_{-1}^1 (1-x^2)^n\cos(\theta x)dx,$$ I have proven the relation $$\theta^2I_n = 2n(2n-1)I_{n-1} - 4n(n-1))I_{n-2}.$$ I am supposed to now notice that $$\theta^{2n+1}I_n = n!(P_n(\theta)\sin(\theta) + Q_n(\theta)\cos(\theta)),$$ where $P_n$ and $Q_n$ are polynomials of degree at most $2n$ with integer coefficients, then use this relation to prove that $\pi$ is irrational. I can sort of see the relation, but can't see a clear way to prove it besides ''the coefficients seem to decrease by one each time so the $n!$ makes sense, and $I_1,I_0$ have the desired $\sin\theta$ and $\cos\theta$ , and the plus $1$ makes sense since we apply the relation either $n$ or $n-1$ times on the terms (giving us $\theta^{2n}$ or $\theta^{2n-2}$ ) but then $I_1 = \frac{4}{\theta^3}(\sin\theta - \theta\cos\theta)$ and $I_0 = \frac{2}{\theta}\sin\theta$ ''. So, I can vaguely see how all the parts could fit together but am not convinced with my argument and don't see an obvious way to prove it carefully by actually applying the relation by hand (I've tried expanding a few of the term by hand to see if a particularly nice pattern appears, but I don't see one). Can someone explain how to do this a bit more carefully? I see how to prove that $\pi$ is irrational given this relation (plug in $\theta = \pi/2$ and find an integer between $0$ and $1$ assuming $\pi$ is rational). I got the problem from this practice sheet: [https://www.dpmms.cam.ac.uk/study/IA/AnalysisI/2020-2021/aI_4_21.pdf]","['integration', 'pi', 'polynomials', 'real-analysis']"
4231276,Proof of Stewart's Theorem using elementary geometry,"I was reading Stewart's Theorem which states that Given a triangle with side lengths $a$ , $b$ , $c$ and a cevian of length $d$ which divides $a$ into two segments $m$ and $n$ as shown in the figure we must have $$b^2m+c^2n=a(d^2+mn)$$ which is often remembered using the funny mnemonic, ""A man and his dad put a bomb in the sink"" which gives $$man+dad=bmb+cnc$$ Now, this can be proved quite easily using The Law of Cosines on $\theta$ and $\theta^\prime$ as marked in the figure. You can find the proof here . But, I was thinking if there is a more elementary geometric argument (without using trignometry and without using a lot of algebraic manipulation) to see this clearly in front of our eyes. I tried giving an attempt, but it seems extremely difficult using only elementary geometry since there is no trivial symmetry immediately observable in this diagram. Also, the terms involved in the formula, i.e., $b^2m$ , $c^2n$ , $ad^2$ and $amn$ , are all of dimension $[L^3]$ - so, it seems difficult to even draw areas on sides and geometrically manipulate them. Any elegant geometric insight or an intuitive idea would be appreciated. Note: as pointed out in the comments, and as you can see here as well, the Cosine Law can be very well proved using elementary geometry. While that is of course an answer, we all agree that it's cheating at some level. There's another answer as well, but this one's so much algebraic jargon, I would prefer trigonometry instead. Also, Stewart's book, Some general theorems of considerable use in the higher parts of mathematics seems to contain a completely geometric argument. But the absense of diagrams in Google Scans combined with the length of this solution makes it very difficult to follow. I would also love to see a summarized version of this in more understandable way without loosing the geometric essence.","['euclidean-geometry', 'big-picture', 'geometry', 'intuition']"
4231370,Characterizations of local topology of $\mathbb{R}^n$,"What topological properties uniquely characterize the local topology of $\mathbb{R}^n$ ? I know $\mathbb{R}$ is the unique complete ordered field, but that's partly algebraic. Better if the characterization avoids sneaking in the topology of $\mathbb{R}$ through the side door by saying it has a metric which is continuous, for example. And certainly we don't want to sneak it in by assuming the notion of a manifold. A start: a similar question on MathStack, and a 1965 paper giving an axiomatic characterization of $S^n$ . (Chapter II of the paper is the meat.)","['real-numbers', 'manifolds', 'general-topology']"
4231379,Do simple paths have unique endpoints?,"Let $Y \subset \mathbb{R}^n$ be a simple path. That is there exists a continuous injective function $f : [0, 1] \rightarrow \mathbb{R}^n$ such that the image of $f$ is $Y$ . Note, by this definition $f(0)$ cannot equal $f(1)$ . Now assume there exists another continuous injective function $g : [0, 1] \rightarrow \mathbb{R}^n$ whose image is also $Y$ . Can $\{f(0), f(1)\} \neq \{g(0), g(1)\}$ ?","['general-topology', 'real-analysis']"
4231381,Recurrence relation of L'Hôpital's rules,"Today I woke up with this thought. Imagine I have a limit that is given by: $$\lim_{x\to x_0} {[f(x)/g(x)]}$$ If $f(x_0)=g(x_0)=0$ then I can solve it using L'Hôpital's rule. If $f'(x_0) = g'(x_0)=0$ , then I should perform another derivation and keep doing that many times as I need. Imagine that after $n$ L'Hôpital derivations I get: $${[f^{(n)}(x)/g^{(n)}(x)]=\alpha[f(x)/g(x)]}.$$ Does that necessarily means that the limit does not exist in $x=x_0$ ?","['limits', 'calculus', 'derivatives']"
4231391,Normalizers/centralizers of Sylow 2 subgroups of subgroups of a finite solvable group with elementary abelian sylow subgroup,"Let $G$ be a finite solvable group with an abelian Sylow $p$ subgroup $S$ . A classic theorem of Burnside says that $S$ has a normal complement if and only if the Centralizer of $S$ in $G$ is equal to the Normalizer of $S$ in $G$ . Since the centralizer is contained in the normalizer we can just look at the index.  So for any group $G$ let ${\rm icn}_p(G)$ be the index of the centralizer of a Sylow $p$ subgroup in its normalizer. First just consider $p=2$ , but much of this, if not all, could be independent of $p$ . I would like to know how ${\rm icn}$ behaves w.r.t. subgroups. If $G$ is solvable with abelian Sylow $2$ subgroup $S$ is it true for every subgroup $H \subset G$ that ${\rm icn}(H) \le {\rm icn}(G)$ ? Moreover is it true that ${\rm icn}(H)$ is a divisor of ${\rm icn}(G)$ ? These seem like they should be elementary, but I don't see how to approach. A quick computation shows that if solvability and abelian are not required, then the second statement about divisors is false, but the first inequality still seems to hold.
An example of this is the symmetric group $S_4$ . Here the Sylow 2 subgroup is not abelian, but for each subgroup of $K$ of $S_4$ , $icn(H)$ is still not greater than 4. In fact $icn(S_4) = 4$ , and $A_4 \subset S_4$ gives $icn(A_4) = 3$ Is this true for any  finite group $G$ and subgroup $H \subset G$ that ${\rm icn}(H) \le{\rm icn}(G)$ ? Thanks for your help","['group-theory', 'sylow-theory', 'finite-groups', 'solvable-groups']"
4231395,Difference between these two definitions of limits,This is a definition from WikiBooks : This is another definition from LibreTexts : The first definition says except possibly at $x=c$ while the second definition says $f(x)$ is defined for all $x\neq a$ . Why do these two definitions define $f(x)$ differently?,"['limits', 'calculus', 'definition']"
4231409,Parallelogram law functional equation: $ f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big) $,"I'm trying to solve the following functional equation, called the parallelogram law : $$ f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big) $$ It can be easily proven that we have the family of solutions $ f ( x ) = c x ^ 2 $ . Is there other type of solution apart from this? I've proven that this are all the solutions in the case $ f : \mathbb Q \to \mathbb Q $ this is the only solution. To prove this, first let $ x = y = 0 $ . We then get $$ 2 f ( 0 ) = 4 f ( 0 ) \implies f ( 0 ) = 0 $$ Now will prove by induction that $ f ( n x ) = n ^ 2 f ( x ) $ , $ \forall n \in \mathbb N $ . Statement hold for $ n = 1 $ . Now, supposing the induction hypothesis holds, let $ x = n x $ and $ y = x $ . Then $$ f \big( ( n + 1 ) x \big) + f \big( ( n - 1 ) x \big) = 2 \big( f ( n x ) + f ( x ) \big) $$ $$ f \big( ( n + 1 ) x \big) + ( n - 1 ) ^ 2 f ( x ) = 2 \left( n ^ 2 + 1 \right) f ( x ) $$ $$ f \big( ( n + 1 ) x \big) = ( n + 1 ) ^ 2 f ( x ) $$ Now letting $ x = 0 $ , we get $$ f ( y ) + f ( - y ) = 2 f ( y ) \implies f ( - y ) = f ( y ) $$ $$ f ( n x ) = n ^ 2 f ( x ) \text , \ \forall n \in \mathbb Z $$ Also notice that $$ f ( x ) = f \left( n \frac x n \right) = n ^ 2 f \left( \frac x n \right) $$ $$ f \left( \frac x n \right) = \frac { f ( x ) } { n ^ 2 } \text , \ \forall n \in \mathbb Z \setminus \{ 0 \} $$ We can conclude that $ f ( r x ) = r ^ 2 f ( x ) \forall r \in \mathbb Q $ . The above reasoning is valid also when $ f : \mathbb R \to \mathbb R $ . If we assume continuity  in some interval or monotonicity in some interval $ [ a , b ] $ , the solution is of form of a quadratic polynomial. If the solution is not unique, what are the minimal assumptions one has to make to ensure the statement hold (injectivity, positivity, Riemann integrability, measurability, not being dense in a region of $\mathbb R ^ 2 $ )?","['functional-equations', 'recreational-mathematics', 'real-analysis']"
4231410,Meaning of linearity in statistics vs linearity in linear algebra,"I've noticed that in Statistics, when the topic of ""linearity"" comes up, what is usually meant is that a fit can be made with a function of the form $\mathbb{R} \mapsto \mathbb{R}: y(x)= mx+q$ Now in linear algebra this is obviously not true (equations of the form y=mx+q are not necessarily linear). Am i mistaking something or is this just a point where statisticans and mathematicians use the same words for different things?","['statistics', 'linear-algebra']"
4231417,Can you string together inequalities and equalities into a single statement?,"If I write a statement like $$|f(x) - f(y)| \leq K|x - y| < K \delta = K \cdot \frac{\varepsilon}{K} = \varepsilon,$$ is that appropriate? Context: I have a classmate who insists that ""mixing"" the equalities on the end of a statement that begins with inequalities somehow makes the statement inappropriate. I'm not sure if he thinks the above statement is literally wrong or (more likely) thinks it is somehow just stylistically unpleasant to the point where it should be avoided. My contention is that the statement is read from left to right and if it consists of a string of individually true statements, then the statement is true; and there isn't any other convenient way to write the statement, so it must be stylistically acceptable as well. Any rules, thoughts, or opinions?","['inequality', 'proof-writing', 'analysis']"
4231433,"How is $P(A|B') = P(A|B)$ the definition for independence of events $A,B$?","Notations: $P(XY) := P(X \cap Y)$ and $X' := X^{\small \complement}$ . I read from a comment on MSE that $P(A | B) = P(A)$ and $P(A | B) = P(A | B')$ are equivalent definitions of independence of two events and they both lead to $P(AB) = P(A)P(B)$ . The first definition now seems rather natural to me, thanks to @lulu and @Ryan G . We assume that the probabilities $0 < P(A), P(B) < 1$ . Derivation from the first def.: $P(A|B) = P(A) \iff \frac{P(AB)}{P(B)} = P(A) \iff P(AB) = P(A)P(B)$ . Derivation from the second def.: $P(A|B) = P(A|B') \iff \frac{P(AB)}{P(B)} = \frac{P(AB')}{P(B')}$ . Now if they are equivalent, then $\frac{P(AB')}{P(B')} = P(A) \iff P(A | B') = P(A)$ which is not necessarily the case. Can anyone please confirm this or tell me what exactly is the statement/assumption that I am missing?","['conditional-probability', 'independence', 'probability-theory']"
4231490,Does there exist a connected metric space $X$ all of whose nonempty open subsets $U\neq X$ are not connected?,"Question is the title. Is there an infinite connected metric space $X$ such that if $U\subseteq X$ is nonempty, open, and isn't all of $X$ , then $U$ is not connected? I've tried to prove that no such space can exist using the uniqueness of dispersion points/explosion points, but haven't had much luck. Rephrasing the question: Must every infinite connected metric space contain a nontrivial (not empty or the space itself) connected open subset? EDIT: in light of Alessandro Codenotti's comment showing that such space does exist, let me up the ante somewhat. Is there a connected metric space that that has no infinite subspace with the above property? That is, given an infinite metric space $(X,d)$ can I always find a connected subspace $Y\subseteq X$ that contains a nontrivial connected open subset with respect to itself?","['general-topology', 'connectedness']"
4231501,"Is it true that $d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y))$?","In arbitrary metric space $(M, d)$ , is it true that $d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y))$ ? Clearly, in the simple case where $M = \mathbb{R}$ and $d(x,y) = | x - y |$ , we have $$\begin{align}
d((f+g)(x), (f+g)(y)) &= |(f+g)(x) - (f+g)(y)| \\
&= |(f(x) + g(x)) - (f(y) + g(y))| \\
&= |(f(x) - f(y)) + (g(x) - g(y))| \tag{1} \\
&\leq |f(x) - f(y)| + |g(x) - g(y)| \\
&= d(f(x), f(y)) + d(g(x), g(y)),
\end{align}$$ as desired. So basically, this proof requires the ability to rearrange the values within the absolute values sign, as indicated in line (1), which is certainly possible. But when I try this same type of proof using just the properties of an arbitrary metric, I get $$\begin{align}
d((f+g)(x), (f+g)(y)) &= d(f(x) + g(x), f(y) + g(y)) \\
&(=?) \ |d(f(x), f(y)) -  d(g(x), g(y))| \tag{2}\\
&\leq d(f(x),f(y)) + d(g(x), g(y)),
\end{align}$$ and I'm not sure I can do the same type of rearrangement in an arbitrary metric space, which is why I've put the question mark in line (2). I feel like I'm missing something obvious! (For what it's worth, I'm trying to use this to prove that the sum of uniformly continuous functions is uniformly continuous.)","['metric-spaces', 'analysis', 'real-analysis']"
4231542,Relation between eigenvalues and size of geometrically shaped data in PCA,"I want to measure the orientation and size of objects in a binary image using PCA. As I have a picture the data points are gridded (pixels). The object can be either an ellipse or a rectangle that I do not know beforehand. For the centered data points I calculate the eigenvalues and eigenvectors of the covariance matrix and then want to derive the object size from the eigenvalues. The problem is that the scaling of the eigenvalues depends on the shape of the data points. Example Let's assume I have an ellipse with full axes lengths (from one end to other end) $a=100,b=50$ and a rectangle with same dimensions. Both objects are rotated by $\theta=45$ deg. from the axis aligned state, see figure. The normalized eigenvectors direct as expected in the principal directions: $e_1\approx\dfrac{1}{\sqrt{2}} \begin{align}
     \begin{bmatrix}-1 \\ 1  \end{bmatrix}\end{align}$ , $e_2\approx\dfrac{1}{\sqrt{2}} \begin{align}
     \begin{bmatrix}1 \\ 1  \end{bmatrix}\end{align}$ . The corresponding eigenvalues $\lambda_1,\lambda_2$ are related to object sizes by $a=k \sqrt{\lambda_1},b=k \sqrt{\lambda_2}$ . However $k$ depends on the shape. By testing many combinations of $a,b,\theta$ I can reproduce the  dimensions of the ellipse if $k\approx4$ and for the rectangle if $k\approx2\sqrt{3}\approx3.46$ . How can I derive $k$ from my original data as I do not know beforehand if my object is an ellipse or rectangle? For completeness I give the covariance matrices of the depicted example: ellipse (3927 data points): $\begin{align}  \begin{bmatrix}
390.84770    &  -234.51809 \\
-234.51809   &    390.84770
  \end{bmatrix}\end{align}$ rectangle (4994 data points): $\begin{align}  \begin{bmatrix}
 518.98143  &    -309.86476\\
-309.86476   &    518.98143
  \end{bmatrix}\end{align}$","['covariance', 'linear-algebra', 'image-processing', 'eigenvalues-eigenvectors']"
4231547,Find the determinant of a particular matrix without a calculator.,"I would like to know the trick to solving this determinant problem. Find the determinant of the matrix $$
\begin{bmatrix}
283&5&\pi&347.86\times10^{15^{83}}\\
3136 & 56 & 5 & \cos(2.7402)\\
6776 & 121 & 11 & 5\\
2464 & 44 & 4 & 2
\end{bmatrix}
$$ Hint: do not use a calculator. This is a problem here (section 3.2). According to it, the answer is 6. I have no idea how to do this. Obviously some of the entries in this matrix are red herrings, and I need to perform some trick. But the only manipulations I know towards computing the determinant is adding one row to another and multiplying a row by a scalar, and nothing in this matrix suggests that I do that. I've tried decomposing the matrix as follows, $$
\begin{bmatrix}
283 & 5 & \pi & 347.86\times10^{15^{83}}\\
56^2 & 56 & 5 & \cos(2.7402)\\
11^2\cdot 56 & 11^2 & 11 & 5\\
4\cdot11\cdot 56 & 4\cdot 11 & 4 & 2
\end{bmatrix},$$ though I'm not sure what I accomplished (I could also note that $56 = 14\cdot 4, 4 = 2^2$ etc. and do more decomposing). The problem is not all entries in any column/row are nice, and the nicest looking ones are the first and second column (or third and fourth row). Maybe the matrix is the product of two nice ones?","['determinant', 'linear-algebra']"
4231549,Random bipartite graph with lower bounded degrees,"Let $i$ be a positive integer and $V_1, V_2$ be the vertex sets of a bipartite graph. We want to sample uniformly at random a bigraph in the set of bigraphs with vertex set $V_1 \cup V_2$ such that for each $v \in V_1$ , it has at least $i$ neighbors in $V_2$ and for each $u \in V_2$ , it has at least one neighbor in $V_1$ . Is there any efficient way to do this?","['graph-theory', 'random-graphs', 'bipartite-graphs', 'discrete-mathematics']"
4231552,Differentiation of a matrix expression with respect to a vector,"I am interested in the expression: $$ \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right]  $$ where $\bf{p}, \bf{x} $ are $S \times 1$ vectors and $D_1,P_1$ are $S \times S$ matrices. In this case, only the entries of $P_1$ directly depend on $\bf{p}$ so I was thinking of writing: $$ \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right] = \left(D_1 +P_1 \right)^{-1} \frac{dP_1}{d\bf{p}}  \left(D_1 +P_1 \right)^{-1} \left(P_1\bf{x} \right) + \left(D_1 +P_1 \right)^{-1} \frac{dP_1\bf{x}}{d\bf{p}}. $$ I don't have much confidence that this is correct though. I don't even know if it makes total sense, since $P_1 \bf{x}$ is a vector and $\frac{dP_1}{d\bf{p}}$ is a tensor? Any help is appreciated.","['matrices', 'matrix-calculus', 'derivatives']"
4231564,"How to calculate the series $ \sum_{n=1}^{+\infty}\frac{1}{x_n^2} $ if $ x_1, x_2, ..., x_n, ... $ are all positive roots for $ 2020\tan x=2021x $.","If $ x_1, x_2, ..., x_n, ... $ are all positive roots for $ 2020\tan x=2021x $ , it is easy to verify that $ \sum_{n=1}^{+\infty}\frac{1}{x_n^2} $ is convergent. However I can not calculate the exact value for the limit. What I can do is to estimate the order of $ \frac{1}{x_n^2} $ , which is not useful for the calculation. I know that the equation $ 2020\tan x=2021x $ may have some relations with the problem of eigenvalues for the wave equation, but I cannot use it to solve this problem either. Can you give me some hints or references?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4231572,Find a IVP associated with $y' = y - y^{2}$,"The function $g(x, c) = \frac{1}{1-c e^{-x}} $ is a family of solutions (of one parameter) of the first order DE $y'(x) = y(x) - y^{2}(x)$ . Find a IVP associated with this differential equation and find the solution corresponding to the initial condition $y(0) = \frac{-1}{3}$ I did the next: The IVP associated with $y' = y- y^{2}$ is: $y' - y + y^{2} = 0$ and $y(0) = \frac{-1}{3}$ this is defined in $(- \infty,1) $ I'm not sure if that's right. It's seems incomplete to me.","['initial-value-problems', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4231630,How do I propagate errors back to the previous layers for convolutional neural network?,"I'm trying to figure out the backward propagation for convolutional neural network. I drew the following figure to illustrate the forward propagation of a 3-layer model. Someone may argue the flatten should be counted as another layer, well, that would make me write a few more lines of derivation without giving any good. To simplify the whole process, I decided to consider binary classification and just one training example which is also an input image, so the output of the 1st layer would be $a^{[1]} = ReLU (x * w^{[1]} + b^{[1]}) \tag{1}$ where the $*$ denotes the convolution operation, $w^{[1]}$ denotes the kernel/filter used in this convolution operation, $b^{[1]}$ denotes the bias/intercept for layer [1] . Given the input image $x$ is size of (8, 8), the filter size of (3, 3), so the output of layer [1] $a^{[1]}$ is size of (6, 6). $m$ denotes the number of training examples, so $m=1$ here, and the last dimension of (m, 8, 8, 1) means the color channel which is also 1 here. There are 10 learnable params in layer [1] , 9 for the kernel, 1 for the bias and the activation function is ReLU in this model. There is no learnable params or activation function in layer [2] though, I still use $a$ to denote the output of this layer. So $a^{[2]}$ is size of (3, 3), as the kernel size = (2, 2). $a^{[3]}$ denotes the output of the last layer of the model and could be computed using this formula $a^{[3]} = \sigma{(z^{[3]})} \tag{2}$ where $\sigma{(\cdot)}$ denotes the sigmoid function and $z^{[3]} = a^{[2]} \cdot w^{[3]} + b^{[3]} \tag{3}$ so, there are another 10 params to learn, 9 for $w^{[3]}$ and 1 for $b^{[3]}$ . the loss function could be $$ \mathcal{L}(a^{[3]}, y) =  - [y \log a^{[3]} + (1-y)  \log(1-a^{[3]})] \tag{4}$$ I clearly understand part of the backward propagation for this model, which is $$
\dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[3]}} = -\dfrac{y}{a^{[3]}} + \dfrac{1-y}{1-a^{[3]}} \tag{5}
$$ $$
\dfrac{da^{[3]}}{dz^{[3]}} = a^{[3]}\,(1-a^{[3]}) \tag{6}
$$ \begin{align}
\frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}}
= a^{[3]} - y \tag{7}
\end{align} \begin{align}
\frac{d\mathcal{L}(a^{[3]}, y)}{da^{[2]}}
= \frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}} \cdot w^{[3]T}
\tag{8}
\end{align} the part of equation (3) could be rewritten as $
a^{[2]} \cdot w^{[3]} 
= a^{[2]}_{1,1} \ w^{[3]}_{1} + a^{[2]}_{1,2} \ w^{[3]}_{2}+ a^{[2]}_{1,3} \ w^{[3]}_{3} \\
+ a^{[2]}_{2,1} \ w^{[3]}_{4} + a^{[2]}_{2,2} \ w^{[3]}_5+ a^{[2]}_{2,3} \ w^{[3]}_{6} \\
+ a^{[2]}_{3,1} \ w^{[3]}_{7} + a^{[2]}_{3,2} \ w^{[3]}_{8}+ a^{[2]}_{3,3} \ w^{[3]}_{9} \tag{9}
$ where $
a^{[2]}_{1,1} = \max(a^{[1]}_{1,1}, a^{[1]}_{1,2}, a^{[1]}_{2,1}, a^{[1]}_{2,2}) \tag{10}
$ $
a^{[2]}_{p,r} = \max(a^{[1]}_{2p-1,2r-1}, a^{[1]}_{2p-1,2r}, a^{[1]}_{2p,2r-1}, a^{[1]}_{2p,2r}) \tag{11}
$ How do I propagate errors back to layer [2] and [1] ? I'm now blocked at the step of propagating errors from $a^{[2]}$ to $a^{[1]}$ . If $a^{[1]}_{1,1}$ is the output of equation (10), is it correct to use the formula below to compute $
\dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[1]}}
$ ? $
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[1]}_{1,1}} = 
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[2]}_{1,1}} =
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial z^{[3]}} \ w^{[3]}_1 = (a^{[3]} - y)\ w^{[3]}_1
$ If yes, what about the other 3 guys? I went through a post for this though, I still don't know what should I do to complete the formulas after (8). Could someone give me a hint? Note: I know what the convolution operation and cross-correlation are in deep learning. I'd just like to know the backward propagation part.","['machine-learning', 'calculus', 'derivatives', 'convolution']"
4231634,Solving the equation $\{a^{-1}\}=\{a^2\}$,"Let $\{x\}$ be the fractional part of a real number $x$ . If $a$ is a positive real number with $\{a^{-1}\}=\{a^2\}$ and $2<a^2<3$ , find $a^{12}-144a^{-1}$ . This is probably an AIME problem. But I don't know from which year it is. Here is my solution of the problem: Because $2<a^2<3\implies\frac1{\sqrt3}<a^{-1}<\frac1{\sqrt2}$ , we have $\lfloor a^{-1}\rfloor=0$ and $\lfloor a^2\rfloor=2$ . So the equation is $\begin{align}a^{-1}=a^2-2 &\implies a^3-2a-1=0\\ &\implies (a+1)(a^2-a-1)=0\\ &\implies a=\frac{1+\sqrt5}2.\end{align}$ Now with some boring calculations, we can find $a^{12}-144a^{-1}$ . But this is very much dependant on computations. Is there some other way to find $a^{12}-144a^{-1}$ that is not much dependant on computational skills? Also as we obtained $a=\phi$ , can we use Fibonacci numbers here?","['contest-math', 'algebra-precalculus', 'systems-of-equations', 'fibonacci-numbers']"
4231641,Is a smooth Lie group an analytic Lie group?,"Maybe related : Why are Lie groups automatically analytic manifolds? I'm seraching for a precise statement. I propose two theorems. Theorem 1 Let $G$ be a smooth manifold (the transitions functions are $C^{\infty}$ as maps between open subsets of $R^n$ ). We suppose that $G$ is a group for which the mulliplication and the inverse are smooth. Then the transition functions, the multiplication and the inverse are analytic. Theorem 2 Let $G$ be a smooth manifold (the transitions functions are $C^{\infty}$ as maps between open subsets of $R^n$ ). We suppose that $G$ is a group for which the mulliplication and the inverse are smooth. Then $G$ accepts an analytic atlas for which the multiplication and the inverse are analytic.
Moreover every two such atlas are analytically diffeomorphic. My guesses: Theorem 1 is wrong because the atlas may contain smooth charts which are not analytic. Theorem 2 is true. QUESTIONS: is theorem 1 wrong ? is theorem 2 true ? can I have a link (online; I do not have access to books) to a precise statement and a proof ? My aim is to be sure that I can always suppose every Lie groups to be analytic (transition charts, multiplication, inverse) without loss of generality.","['lie-groups', 'differential-geometry']"
4231658,Counting the number of similar matrices over finite fields,"Among $3 \times 3$ invertible matrices with entries from the field $ \mathbb{Z/3Z}$ , how many matrices are similar to the following matrix? \begin{pmatrix}
2 & 0 &0 \\ 
 0&2  &0 \\ 
 0&0  &1 
\end{pmatrix} Things I'm familiar with:  I know similar matrices have the same determinant and trace. The number of invertible matrices over $ \mathbb{Z/3Z}$ is $(3^3-1)(3^3-3)(3^3-3^2)$ Please give me a hint to proceed from here.","['matrices', 'finite-fields', 'similar-matrices', 'linear-algebra']"
4231666,Deriving the Formula for the Surface Area of a Sphere [duplicate],"This question already has answers here : Why is this way of deriving surface area of sphere wrong when a similar method can be used to derive volume? (2 answers) Closed 2 years ago . I was trying to derive the surface area of a sphere, and I was hoping someone could possibly explain why this one method I am trying does not work. I've seen and done other ways of deriving the surface area so I'm not asking for a different way, but I'm specifically wondering why this one wouldn't work. I started with a sphere centered at the origin with radius $r$ . I then imagined taking narrow, verticle slices out of the sphere (that are parallel with the z-axis and perpendicular to the x-axis) to create rings. I would then unroll each ring and find its area. The area would be $2\pi \sqrt{r^{2}-x^{2}} * dx$ (where $dx$ is the width of each ring) It seems to me that if I did this for all x values on $[-r, r]$ , and summed them, I should get the surface area of the sphere. I wrote this as the integral : $ \int_{-r}^{r} 2\pi\sqrt{r^{2}-x^{2}} \,dx $ This integral is equal to $\pi^{2}r^{2}$ , which is obviously not the right answer. I'm thinking that maybe I need to account for the curvature in some way that I'm not, but I'm really not sure. If anyone could explain why this doesn't work, I would appreciate it greatly.","['integration', 'calculus', 'geometry']"
4231710,"Constructive proof of: If $\lim_{x \to a}(f \cdot g) (x) = L$ and $\lim_{x \to a}f (x) = \frac{L}{K}$ for $K \neq 0$, then $\lim_{x \to a}g (x)=K$.","To improve my comfort with $\delta-\epsilon$ proofs, I aim to constructively prove the following claim: If $\displaystyle \lim_{x \to a}(f \cdot g) (x) = L$ and $\displaystyle \lim_{x \to a}f (x) = \frac{L}{K}$ for $K \neq 0$ , then $\displaystyle \lim_{x \to a}g (x)=K$ . I tried some new tricks, so I just wanted to make sure that everything looked okay. Useful Lemma $\dagger$ : $\left|f(x)g(x)-\frac{L}{K}K \right|=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right|$ By assumption, we know the following statements: $$\forall \varepsilon_{f\cdot g}\gt 0 \ \exists\delta_{f\cdot g} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f\cdot g} \rightarrow \left|f(x)g(x)-L\right| \lt \varepsilon_{f\cdot g}\right]$$ $$\forall \varepsilon_{f}\gt 0 \ \exists\delta_{f} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f} \rightarrow \left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f}\right]$$ We want to prove that: $$\forall \varepsilon\gt 0 \ \exists\delta \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta \rightarrow \left|g(x)-K\right| \lt \varepsilon\right]$$ Choose the following error terms: $$\varepsilon_{f\cdot g}=\frac{\varepsilon |L|}{2\cdot2|K|}$$ $$\text{For } \varepsilon_1=\frac{\varepsilon |L|}{2|K|\cdot2|K|} \text {and for } \varepsilon_2=\frac{L}{2|K|} \text{, let }\varepsilon_{f}=\min(\varepsilon_1,\varepsilon_2)$$ For each error term, we have a corresponding $\delta_{f\cdot g}$ and $\delta_{f}$ . Let $\delta = \min(\delta_{f\cdot g},\delta_{f})$ . Let $x$ be an arbitrary element that satisfies $0 \lt |x-a| \lt \delta$ . If the above conditions are satisfied, we can make the below argument: First, note that $L$ can be rewritten as $\frac{L}{K}K$ . By $\dagger$ and our first assumption, we have: \begin{align}
\left|f(x)g(x)-\frac{L}{K}K \right|&=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right| \\
&=\left|\left[g(x)-K \right]\cdot\left[\left(f(x)-\frac{L}{K} \right)+\frac{L}{K} \right]+ K\left[f(x)-\frac{L}{K} \right]\right| \\
&=\left|\left[g(x)-K \right]\cdot\left[f(x)\right]+ K\left[f(x)-\frac{L}{K} \right]\right| \lt \varepsilon_{f\cdot g}  \\
\end{align} Applying the reverse triangle inequality, we then have: \begin{align}
\left|\left[g(x)-K \right]\cdot\left[f(x)\right]\right|-\left|K\left[f(x)-\frac{L}{K} \right]\right|=|g(x)-K||f(x)|-|K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}\\
\end{align} Then: $$|g(x)-K||f(x)| \lt \varepsilon_{f\cdot g}+ |K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}+|K|\varepsilon_{1}$$ Dividing through by $|f(x)|$ we have: \begin{align}
|g(x)-K| \lt \frac{\varepsilon_{f\cdot g}+|K|\varepsilon_{f}}{|f(x)|}&=\frac{1}{|f(x)|}\cdot \left(\frac{\varepsilon |L|}{2\cdot2|K|}+|K|\frac{\varepsilon |L|}{2|K|\cdot2|K|} \right)\\
&=\frac{1}{|f(x)|}\cdot \left ( \frac{|L|}{2|K|}\right) \left( \frac{\varepsilon}{2}+\frac{\varepsilon}{2}\right)\\
&=\frac{\varepsilon\left(\frac{|L|}{2|K|}\right)}{|f(x)|}
\end{align} To complete this proof, we need to show that $$\frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1$$ From our definition of $\varepsilon_2$ , we know that $\left|f(x)-\frac{L}{K} \right|=\left|\frac{L}{K}-f(x) \right| \lt \frac{|L|}{2|K|}$ . Applying the reverse triangle inequality, we have: $$\frac{|L|}{|K|}-|f(x)|\lt\frac{|L|}{2|K|} \implies \frac{|L|}{2|K|} \lt |f(x)| \implies \frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1 \implies \varepsilon \cdot \frac{\frac{|L|}{2|K|}}{|f(x)|} \lt \varepsilon$$ Therefore, we can conclude that $|g(x)-K| \lt \varepsilon$ so long as $0 \lt |x-a| \lt \delta$ . Any suggestions or easier paths (...with the exception of proving it by contradiction...) are greatly appreciated. Edit : Although it is implied by the construction of the different $\varepsilon$ 's, I see that I should have explicitly stated $L$ must take on non-zero values. Secondly, I should have explicitly demonstrated that division by $|f(x)|$ does not result in division by $0$ . Looking at $\varepsilon_2$ , you can actually show that $0\lt \frac{|L|}{2|K|} \lt |f(x)| \lt \frac{3|L|}{2|K|}$ ...so we have no issue of division by $0$ .","['limits', 'solution-verification', 'epsilon-delta']"
4231713,Has anyone ever attempted to find all splits of a rectangle into smaller rectangles?,"I'm enamored by the games Unproportional and Unproportional2 . The only problem is that they have way too few levels. :) But, I'm a computer programmer, so I could make my own! With blackjack and hookers the ability to upload custom pictures and randomly generate grids! Most everything is straightforward there, except for the generation of the grids. How do I take a rectangle and partition it in smaller rectangles? And not only that, I want to be sure that my algorithm can produce ANY possible split within my limitations. (I'll need to add some limitations to make the grids playable. My first idea is to limit the minimum size of rectangle sides, but that will come from playtesting) Searching for an answer to this I found the CS.SE question "" Randomly partition an rectangle into N smaller rectangles "". This suggests an algorithm: Take a rectangle Randomly do one of the following: Split the rectangle into two rectangles with a line parallel to its sides Split the rectangle into this pattern (with randomize sides and possibly mirrored): Take each one of the new rectangles and go to point 1. (Recursion) It's something. But I had doubts if it could produce ALL possible splits. So I took a look at the games, and sure enough, the first level I clicked already had a split which cannot be produced with the above algorithm: This also gave me inspiration to produce this split, which I think is a minimal example of another split that cannot be produced by the above algorithm: OK, so if that doesn't work, what does? It feels to me that a problem like this (partitioning a rectangle into smaller rectangles) is something that should have been explored mathematically, but I don't know where how and what to look for. So, my question is - is there any mathematical research which, either as its primary focus or as a side effect, has developed an algorithm for enumerating all possible splits of a rectangle into smaller rectangles?","['geometry', 'algorithms']"
4231723,Comparing prime spectra of rings with prime spectra of monoids,"Given a (pointed or unpointed) monoid $M$ , one defines an ideal of $M$ to be a subset $I$ of $M$ such that if $a\in I$ and $r\in M$ , then $ra\in I$ (and such that $0\in I$ if $M$ is pointed). A prime ideal $\frak p$ of $M$ is then an ideal such that if $ab\in\frak p$ , then $a\in\frak p$ or $b\in\frak p$ . The set of all prime ideals of $M$ can is written $\mathrm{MSpec}(M)$ , and it can be made into a topological space equipped with a sheaf of monoids, giving a variant of algebraic geometry where monoids play the role of rings. Good references for this are Sections 1--2 of this paper and Ogus's book . Lately I've been trying to compile some examples of such prime spectra, and wondering about how these relate to the usual $\mathrm{Spec}$ of a ring. So far, I've found or read about the following examples (the first five examples below come from Martin Brandenburg's answer here ): $\mathrm{MSpec}(\mathbf{N},+)=\{0,\mathbf{N}_{>0}\}$ . $\mathrm{MSpec}(\mathbf{Z},+)=\{0\}$ . $\mathrm{MSpec}(\mathbf{N},\cdot)=\mathrm{MSpec}(\mathbf{Z},\cdot)=\mathcal{P}(\mathbf{P})$ , the powerset of the set $\mathbf{P}$ of all prime numbers. $\mathrm{MSpec}(\mathbf{N}\otimes_{\mathbf{N}_+}\mathbf{N})\cong\mathcal{P}(\mathbf{P}\times\mathbf{P})$ . $\mathrm{MSpec}(\mathbf{Z}\otimes_{\mathbf{F}_1}\mathbf{Z})\cong\mathcal{P}(\mathbf{P})\times\mathcal{P}(\mathbf{P})$ . $\mathrm{MSpec}(K)=\{(0)\}$ whenever $K^\times=K\setminus\{0\}$ . In particular this applies to $\mathbf{F}_1=\{0,1\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x])=\{(0),(x)\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x,y])=\{(0),(x),(y),(x,y)\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x_1,...,x_n])=\mathcal{P}(\{x_1,...x_n\})$ . $\mathrm{MSpec}(\mathbf{F}_1[t,t^{-1}])=\{(0),(t),(t^{-1})\}$ . Now, any ring $R$ has an associated monoid, given by keeping only multiplication. I've noticed that there seems to be some relation between $\mathrm{Spec}(R)$ and $\mathrm{MSpec}(R)$ when applicable in the above examples; e.g. $\mathrm{Spec}(\mathbf{Z})=\{(0)\}\cup\mathbf{P}$ naturally injects into $\mathrm{MSpec}(\mathbf{Z},\cdot)=\mathcal{P}(\mathbf{P})$ , at least as a set. Are there any results relating the monoidal spaces $\mathrm{Spec}(R)$ and $\mathrm{MSpec}(R)$ ? Are there for $\mathrm{MSpec}(M)$ and $\mathrm{Spec}(\mathbf{N}[M])$ or $\mathrm{Spec}(\mathbf{N}_+[M])$ ? Finally, do we always have a natural morphism of monoidal spaces $\mathrm{Spec}(R)\to\mathrm{MSpec}(R)$ ?","['monoid', 'algebraic-geometry']"
4231739,"What is the cardinality of $K$, where $L_\infty[0,1]=C(K)$?","By Gelfand representation of (real) C $^*$ -algebras, it is known that $L_\infty[0,1]$ is isometrically isomorphic to $C(K)$ , for some compact Hausdorff $K$ . By looking at the proof, $K$ is actually defined in the following way: $$K=\{\mu\in L_\infty^*:\|\mu\|=\mu(e)=1\text{ and }\mu(fg)=\mu(f)\mu(g)\text{ for all }f,g\in L_\infty\},$$ where $e$ is the unit of the C $^*$ -algebra (that is, in this case the 1-constant function). Moreover, it is proved that $K$ is non-empty since it contains the extreme points of the set $\{\mu\in L_\infty^*:\|\mu\|=\mu(e)=1\}$ . Can we say anything about the cardinality of this set $K$ ? So far I've been able to show the following topological lemma: $\textbf{Lemma:}$ A compact Hausdorff countable space is metrizable. Since $C(K)=L_\infty$ is non-separable, it is known that this implies that $K$ must be non-metrizable. This, combined with the lemma, shows that $K$ is uncountable. Can we do better than this?","['banach-spaces', 'general-topology', 'functional-analysis', 'gelfand-representation']"
4231746,Does a bijection preserves set difference?,Suppose that $\phi: Y \to Y$ be a bijection. Suppose that $B\subseteq A\subseteq Y$ and $\phi(A)\subseteq A$ and $\phi(B)\subseteq B$ . Is it true that $\phi(A\setminus B)\subseteq A\setminus B$ ? I took $x\in A\setminus B$ . So $x\in A$ but $x\notin B$ . So $\phi(x)\in A$ . But I coulnt conclude that $\phi(x) \notin B$ . when does $\phi$ preserves set difference? What more assumption do we need?,['elementary-set-theory']
4231750,Solving the equation of the type $g\left( x \right) = \int\limits_0^x {f\left( t \right)dt} $,"Let $f:\left[ {0,1} \right] \to R$ be a differentiable function. Let $g\left( x \right) = \int\limits_0^x {f\left( t \right)dt} $ with $g\left( 1 \right) = 0$ . Which of these equations must have at least one solution for x in the interval $(0,1)$ ? (A). $g\left( x \right) = f\left( x \right)$ (B). $xg\left( x \right) = \left( {1 - x} \right)f\left( x \right)$ (C). $f\left( x \right) = f'\left( x \right)g\left( x \right)$ (D). $f\left( x \right) = xg\left( x \right)$ This question has one or more than one correct My approach is as follow $g\left( 0 \right) = \int\limits_0^0 {f\left( t \right)dt}  = 0$ Given $g(1)=0$ On differentiating we get $g'\left( x \right) = f\left( x \right)$ , hence $g'\left( c \right) = f\left( c \right)$ where $c\in (0,1)$ . How we will use the Rolle's theorem for checking and verifying each option choice.","['functions', 'rolles-theorem']"
4231813,Differentiating real valued vector functions.,"If $v(a,b,c): \mathbb{R}^3 \to \mathbb{R}^3 $ and $ f: \mathbb{R}^3 \to \mathbb{R}$ by $f(v) = v \cdot v $ (dot product) , what is $\frac{\partial f}{\partial a}$ ? Chain rule attempt: $\frac{df}{dv} = 2v$ and so $\frac{\partial f}{\partial a} = 2v \cdot \frac{\partial v}{\partial a} $ ? I dont think this is correct unfortunately. Could someone give me some pointers please :)","['multivariable-calculus', 'derivatives', 'vector-analysis', 'real-analysis']"
4231894,Understanding Brouwer Separation Theorem with an easier proof,"I'd like to undersand better Brouwer separation theorem given in Massey (Proposition 6.5 p.215) since has some smoky parts to me.
To lighten the notation we set $D^n := \mathbb{D}^n, S^n := \mathbb{S}^n$ Lemma $6.2$ would be cited so I'm going to cite the statement: Lemma: Let $Y$ be a subset of $S^n$ which is homeomorphic to $I^k$ where $0 \leq k \leq n$ . Then $\tilde{H}_i(S^n-Y) = 0$ for all $i$ . Furthermore let's denote $C_0,C_1$ the two connected components of the follwing: Proposition: Let $A \subset S^{n}$ homeomorphic to $S^{n-1}$ , then $A$ is the boundary of each components of $S^n-A$ . Proof: In order to prove the Proposition we must show that : The boundary of each component is a subset of $A$ . Any point $a \in A$ is a boundary point of each component of $S^n-A$ . To prove the first, Massey asserts that it's sufficient to know that $S^n-A$ is open (but I don't understand why, any help would be appreciated). I'd like to give my explanation and ask if it's correct: Let's suppose for the sake of contradiction that $\exists p \in \overline{C_0}^{S^n} \cap C_1$ . Since $p$ belongs to the first set, for every neighborhood $U$ of $p$ exists a sequence $(x_n)_n \in C_0$ such that $x_n \to p$ and $x_n \in U$ definitely. Since $C_1$ is open in $S^n$ , exists $V$ neighborhood, with $p \in V \subset C_1$ . This leads to contradiction since we then would have $C_0 \cap C_1 \ne \varnothing$ . Is this reasoning correct? The proof prooceds as follows to prove the second point: Let N be any open neighborhood of $a$ in $S^n$ ; we must show that $N \cap C_i \ne \varnothing$ for $i=0,1$ . Note that $N \cap A$ is an open neighborhood of $a \in A$ . Since $A$ is homeomorphic to $S^{n-1}$ , we can find a decomposition $$A = A_1 \cup A_2$$ where $A_i$ is homeomorphic to $D^{n-1}$ and $A_1 \cap A_2 \simeq S^{n-2}$ , with $A_2 \subset N \cap A$ . We know from Lemma $6.2$ that $S^n - A_1$ is arcwise connected. Let $p_0 \in C_0,p_1 \in C_1$ and choose an arc $ f: I \longmapsto S^n - A_1$ that joins them. Since $f(I) \cap A \ne \varnothing$ we must have $f(I) \cap A_2 \ne \varnothing.$ Consider the subset $f^{-1}(A_2) \subset I$ ; this is a compact subset of $I$ and hence it munst have a least a point $t_0$ and a greatest point $t_1$ . Obviously $t_0,t_1$ are boundary points of $f^{-1}(A_2)$ , and $f^{-1}(N)$ is an open subset of $I$ which cointains both. From this follows by an easy arguments that $f^{-1}(N) \cap f^{-1}(C_i) \ne \varnothing$ for $i=0,1$ which completes the proof. Question : Here I don't understand what's the easy argument and why we have the necessity to use $t_0,t_1$ which arises from compactness of $f^{-1}(A_2)$ . My argument was the following: Since $I = f^{-1}(N) \cup f^{-1}(C_0) \cup f^{-1}(C_1)$ those are three open subset of $I$ which is connected, so we must have intersection, but $f^{-1}(N)$ necessarly $f^{-1}(N)$ has to lie in the middle of $I$ since $f^{-1}(C_1)$ and $f^{-1}(C_0)$ can't intersect; so we have $f^{-1}(N) \cap f^{-1}(C_i) \ne \varnothing$ for $i=0,1$ as requested. Where this fails? This argument doesn't use that $f^{-1}(A_2)$ is compact since it only uses that $A_2 \subset N$ , hence $f^{-1}(A_2) \subset f^{-1}(N)$ . I don't really see where this argument fails when $f^{-1}(A_2)$ is not compact. Any help to understand the necessity of the assumptions and correctness of my arguments would be appreciated.","['spheres', 'solution-verification', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
4231903,SDE's driven by compound Cox processes and their compensated versions,"Is it possible to define a stochastic differential equation of the form $$
dX(t) = f(t,X)dJ(t)
$$ Where $J(t)$ is a compound Cox process defined like $$
J(t) = \sum_{j=1}^{N(t,\Lambda)} \eta_j
$$ With $N(t,\Lambda)$ - Poisson process with stochastic intensity $\Lambda(t)$ , i.e. a Cox process; $\Lambda(t)$ - another stochastic process, that apparently (from reading a few articles) needs to have $\Lambda(0)=0$ , $\Lambda(t)<\infty$ a.s., non-decreasing and right-continuous; $\eta$ - i.i.d. random variables, independent from everything else, that have values on some set $A$ (just to try and be more general and assume point processes). If I'm not mistaken, this would imply the existence of a stochastic integral of the type $$
\int_t^T f(s,X_{-})dJ(s)=\int_{(t,T]\times A} f(s,X_{-}) \mu_{J}(ds, d\eta)=f(t,X_{-})\sum_{s\in(t,T]} \Delta J(s)
$$ Where the compound Cox jump sizes $\Delta J(s)$ are as implied by its expression, previously written. Does an integral of this type exist for this type of processes, and can we numerically simulate them, as if they were more common compound Poisson process? Or is there some property that limits what we can do with them because they explode/diverge somewhere? Furthermore, is it possible to define a compensated compound Cox process $\tilde{J}(t) = J(t) - \mathbb{E}[J(t)]$ that is a martingale, similar to what we can do with compound Poisson processes? In this case, if I additionaly assume, for simplicity and illustration, that we know the distribution of the increments of $\Lambda(t)$ with some $\Lambda(t) - \Lambda(s) \sim U_{\Lambda}(t-s)$ , we would have for the expectation of $J(t)$ that $$
\mathbb{E}\left[J(t)\right]
= \mathbb{E}\left[ N(t, \Lambda)\right] \mathbb{E}\left[\eta\right]
= \mathbb{E}\left[\eta\right] \int_0^{\infty} k \frac{\lambda^k t^k e^{-\lambda} }{k!}\mu_{\Lambda}(d\lambda)
$$ Where we know that $\mu_{\Lambda}$ will depend on $t$ because of the previous assumption. This looks like we will have a sort of convolution of the chosen $\mu_{\Lambda}$ by the gamma function. Would computing this then assure us that $\tilde{J}(t)$ would be a martingale? EDIT: I tried to add a few more details from my line of reasoning and tried to expand on my thoughts on calculating the compensated version of this process.","['stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4231911,Relations from quotient of free product,"This question arose from an exercise which asks you show that the fiber coproduct exists in the category of groups. I was eventually able to (mostly) solve the problem by “gluing” the images of elements under the two homomorphisms(i.e letting $\phi(h)\psi^{-1}(h)=1$ where $\phi : H \rightarrow G$ and $\psi : H \rightarrow G’$ are homomorphisms.) I looked further into this concept (amalgamation) on the Wikipedia page and it says that in order to obtain the relation $\phi(h)\psi^{-1}(h)=1$ on the free product $G * G’$ you must quotient out by the smallest normal subgroup (i.e. the intersection of all normal subgroups) containing the words $\phi(h)\psi^{-1}(h)$ . So my questions are: why isn’t it sufficient to simply adjust the definition of the binary operation so that whenever we have a product ( $a_1a’_1…a’_n\phi(h)$ )( $\psi(h)b_1…b_kb’_k$ ) it is equivalent to $a_1a’_1…a_nb_1…b_kb’_k$ and how do we know that the normal subgroup isn’t “too big”(that is, it establishes an unwanted relation)? I am familiar with relations established by quotients of free groups, however I don’t quite see why this works.","['combinatorial-group-theory', 'quotient-group', 'free-groups', 'free-product', 'group-theory']"
4232000,"$(1+x+x^2)^n=P_0+P_{1}x + P_{2}x^2+ \cdots +P_{2n}x^{2n}$ Prove that,$ P_0+P_{3}+P_{6}+ \cdots =3^{(n-1)}$ [duplicate]","This question already has answers here : A Binomial Expansion (Sum of Coeffients) (2 answers) Closed 2 years ago . Let's say $$ S_n = (1+x+x^2)^n $$ n=1 $$S_1=1+x+x^2$$ n=2 $$S_2=1+2x+3x^2+2x^3+x^4$$ n=3 $$S_3=1+3x+6x^2+7x^3+6x^4+3x^5+x^6$$ n=4 $$S_4=1+4x+10x^2+16x^3+19x^4+16x^5+10x^6+4x^7+x^8$$ By taking coefficients of the $S_n$ we can form this type of triangle similar to Pascal's Trinagle $$\begin{matrix}
&&&&&&&&&1\\
&&&&&&&1&&1&&1\\
&&&&&1&&2&&3&&2&&1\\
&&&&1&&3&&6&&7&&6&&3&&1\\
&&1&&4&&10&&16&&19&&16&10&&4&&1
\end{matrix}$$","['number-theory', 'algebra-precalculus']"
4232009,Transform ODE $(u-x)u_x + u + x = 0$ to polar coordinates,"According to Peter Olver in his book “Applications of Lie Groups to Differential Equations”, p. 104, Example 2.32, the ODE: $$(u-x)u_x + u + x = 0 \tag{1}$$ is transformed in polar coordinates with $x = r\cos \theta$ and $u = r\sin\theta$ to: $$\frac{dr}{d\theta} = r.$$ How do I transform $u_x=\partial_x(r\sin\theta)$ to the new coordinates? I tried the following, but the equation I got is very far from $\frac{dr}{d\theta} = r$ . I rewrote $\partial_x$ in terms of $\partial_r$ and $\partial_\theta$ using the chain rule. Supposing $f = f(r,\theta)$ is a function of $r$ and $\theta$ , we should have: $$\begin{aligned} \partial_x f(r,\theta) &= (\partial_x r)\partial_r f + (\partial_x \theta)\partial_\theta f\\
&=\cos\theta\partial_r f + \left(\partial_x \arctan\frac ux\right) \partial_\theta f\\
&=\cos\theta\partial_r f + \left(\frac {1}{1+\theta^2}2\theta\frac{xu_x - u}{x^2}\right) \partial_\theta f.\end{aligned}$$ Thus: $$\begin{aligned} u_x &=\cos\theta\partial_r (r\sin\theta) + \left(\frac {1}{1+\theta^2}2\theta\frac{xu_x - u}{x^2}\right) \partial_\theta (r\sin\theta)\\
&=\cos\theta\sin\theta + \left(\frac {2\theta}{1+\theta^2}\frac{xu_x - u}{(r\cos\theta)^2}\right) (-r\cos\theta + r_\theta \sin\theta)\\
&=\cos\theta\sin\theta - \left(\frac {2\theta}{1+\theta^2}\frac{r(u_x\cos\theta - \sin\theta)}{r\cos\theta}\right) + \left(\frac {2\theta}{1+\theta^2}\frac{\cos\theta u_x - \sin\theta}{r\cos^2\theta}\right) r_\theta \sin\theta.\end{aligned}$$ Rearranging: $$\begin{aligned} u_x\left[1 + \frac{2\theta}{1+\theta^2}\left(1+\frac{r_\theta}{r}\tan\theta \right)\right] &= \cos\theta\sin\theta + \frac {2\theta}{1+\theta^2}\left(\tan\theta - \frac{r_\theta}{r}\tan^2\theta\right).\end{aligned}$$ So: $$ u_x = \frac{\cos\theta\sin\theta + \frac {2\theta}{1+\theta^2}\left(\tan\theta - \frac{r_\theta}{r}\tan^2\theta\right)}{1 + \frac{2\theta}{1+\theta^2}\left(1+\frac{r_\theta}{r}\tan\theta \right)}$$ Then, rewriting (1) as $u_x = \frac{x+u}{x-u}$ , I obtained: $$\frac{\cos\theta\sin\theta + \frac {2\theta}{1+\theta^2}\left(\tan\theta - \frac{r_\theta}{r}\tan^2\theta\right)}{1 + \frac{2\theta}{1+\theta^2}\left(1+\frac{r_\theta}{r}\tan\theta \right)}=\frac{\cos\theta+\sin\theta}{\cos\theta-\sin\theta}.$$",['ordinary-differential-equations']
4232019,Expressing the $n-$th derivative of $f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5}$,"In an attempt to express the $n-$ th derivative of the rational function $f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5}$ , I split it into $\left( \frac{1+2i}{(x-2-i)} + \frac{1-2i}{(x-2+i)} + \frac{1}{(x-1)} \right)$ , then using $y= \frac{1}{ax+b} \Rightarrow y_{n}=\frac{(-1)^n n! a^n}{(ax+b)^{n+1}}$ I ended up with: $$(-1)^n n! [(1+2i)(x-2-i)^{-n-1}+(1-2i)(x-2+i)^{-n-1} +(x-1)^{-n-1}]$$ Now I don’t know how to get back to $\mathbb{\mathbb{R}}$ from $\mathbb{\mathbb{C}}$ . What I'd like to get is a form that doesn't make use of trigonometric functions but uses the binomial theorem. Can anyone help me achieve this? Thank you so much in advance.","['analysis', 'functions', 'binomial-theorem', 'derivatives', 'complex-numbers']"
4232055,Meaning of vector equations,"I'm taking a linear algebra class now, and I was introduced to vector equations. Consider the system $$
\left\{ \begin{array}{rcl}
x-4y &=& 8\\
2x+3y &=& 6\\
\end{array}
\right.$$ I want to understand why I can factor out the x and y variables to create the vector equation $$x\begin{bmatrix} 1 \\ 2 \end{bmatrix}+y\begin{bmatrix} -4 \\ 3 \end{bmatrix}=\begin{bmatrix} 8 \\ 6 \end{bmatrix}$$ are $x$ and $y$ scalars here? Is the column vector $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ the same as the vector $\left< 1, 2\right>$ ? And lastly, how does this notation help me solve for solutions?","['linear-algebra', 'vectors']"
4232081,Is the differential of a functional norm-dependent?,"A functional $J: (\text{normed function space}) \to \mathbb{R}$ is said to be differentiable at $f$ if there exists a linear functional $\phi$ such that $$\forall h: J(f + h) - J(f) = \phi(h) + r(h) ||h||$$ where $r(h)$ is a functional which goes to $0$ as $||h|| \to 0$ . My questions is: does the existence, and value of, the derivative depend on the norm $||\cdot||$ ? My intuition is that both the existence and value are norm-dependent. As a special case though, I think that if two norms are equivalent, then the existence and value of the derivatives match for both norms. That is, suppose $||\cdot||_0$ and $|| \cdot ||_1$ are two norms with $$c|| \cdot ||_1 \le || \cdot ||_0 \le C || \cdot ||_1$$ for some positive constants $c, C$ and suppose that $J$ is a functional differentiable at $f$ w.r.t. $|| \cdot ||_1$ . Then we can write $$\forall h: J(f + h) - J(f) = \phi(h) + r(h) ||h||_1$$ with $\phi$ linear and $r(h) \to 0$ as $||h||_1 \to 0$ . Fix $\epsilon > 0$ . There exists a $\delta > 0$ such that $||h||_1 < \delta \implies ||r(h)||_1 < \frac{\epsilon}{C}$ . But then $\frac 1C ||r(h)||_0 \le \frac {\epsilon}{C}$ , so $||r(h)||_0 \le \epsilon$ . Thus $J$ is differentiable at $f$ w.r.t. $|| \cdot ||_0$ and has the same derivative. Another special case is that a linear functional $J$ has derivative $J$ at every point, w.r.t. any norm.","['functional-analysis', 'analysis', 'calculus-of-variations']"
4232122,Maximum likelihood estimation intuition for continuous distributions,"I was just revisiting the fundamentals and rationale of maximum likelihood estimation when I realised I can't rationalise the continuous case as opposed to the discrete case. For a discrete random variable, say $X$ , with PMF $p_X(x\mid\theta)$ , suppose we have an i.i.d sample $(X_1, X_2, \ldots, X_n)$ from this distribution. Then, the joint PMF will be given by: $$p_{X_1,\ldots,X_n}(x_1,\ldots,x_n\mid\theta) = \prod_{i=1}^n p_X(x_i\mid\theta).$$ We can use the joint PMF here to calculate probabilities of specific vectors, like the probability that $(X_1, \ldots, X_n)$ actually took on observed values $(x_1, \ldots, x_n)$ . However, this depends on the fixed $\theta$ . The MLE methodology suggests that we look for a value of $\theta$ that maximises the probability that $(X_1, \ldots, X_n) = (x_1, \ldots, x_n)$ . This is why we have $$ L(\theta \mid x_1,\ldots,x_n) = \prod_{i=1}^n p_X(x_i\mid\theta),$$ which outputs the probability of the observed values occurring, depending on the value of $\theta$ we select. This function makes sense to maximise as it directly corresponds to probability values (though $L$ is not a PMF/PDF itself). My confusion arises when we have $X$ continuous, with PDF $f_X(x\mid\theta)$ . We have the joint PDF $$ f_{X_1,\ldots,X_n}(x_1,\ldots,x_n\mid\theta)=\prod_{i=1}^n f_X(x_i\mid\theta). $$ Now differently, this function evaluated on the observed data $(x_1,\ldots,x_n)$ does NOT correspond to the probability of the sample occurring under the distribution $f$ . Of course, the probability that the vector $ (X_1, \ldots, X_n) $ is equal to the particular sample is $0$ since the distribution is now continuous. So why is the next step to maximise the joint density's value evaluated on the data instead? Is this because the volume under the joint PDF around the observed point $(x_1,\ldots,x_n)$ would increase? I'm trying to conceptualise the trivial case where $n=1$ and MLE suggests I simply select a $\theta$ to maximise $f_{X_1}$ at the observed point. But I can't figure out why that would maximise the probability of observing $x_1$ itself. This question might be a little trivial, but I'm struggling to find any answers on this specific concept. Any help is appreciated! Thank you :)","['statistics', 'estimation', 'intuition', 'maximum-likelihood', 'probability']"
4232147,Probability that my set of dice 'wins',"My question is about probabilities. Given are two sets of regular, 6-sided, fair dice of at least one die (but no upper bound on the number of dice). Now, 1 of the sets is considered to be 'the winner'. This is determined with the following method: Sort both sets on the number of eyes. Compare the dice of both sets 1-by-1 starting at the highest by pairing up the dice from both sets (compare 1st with 1st, 2nd with 2nd etc.). As soon as 1 die is higher than the other, then the respective set is the winner and the other dice are ignored. If both sets have an equal number of dice and all dice are equal, then it's a tie. If it's a tie except that one set still has dice left (is a bigger set), then that set is the winner. Examples: A: 6 5 5 (winner)
B: 6 5 4

A: 4 (winner)
B: 1 1 1

A: 6 5 2 2
B: 6 5 2 2
(tie)

A: 6 5 1 (winner)
B: 6 5 Question: How can I calculate the probability that a certain set wins after you throw all dice, given only the sizes of both sets? Edit: The answer can either be a function with 2 inputs, or if this is not possible, an algorithm that calculates this. In practice, the number of dice will be very small (usually smaller than 5). Edit 2: Context : These are the rules for resolving a speed roll to determine initiative in a fight between gladiators in the board game called Spartacus. I'm programming a hobby project where I'm simulating such fights and I want to use the probability of either gladiator winning initiative. The probability is used in a Minimax algorithm to generate child positions of the starting position. Disclaimer: I'm not a mathematician and I have little mathematical knowledge but I do know algebra etc. I'm looking for an answer in the form of an equation with 2 unknowns: sizes of both sets or an algoritm that accepts those inputs","['dice', 'probability']"
4232149,Counterexample to the Hasse principle of the form $x^3 + y^3 + z^3 + nt^3$,"Selmer's cubic is a counterexample to the Hasse principle for ternary cubic forms. We also know that the Hasse principle does not hold for quaternary cubic forms, as $$
5x^3 + 12y^3 + 9z^3 + 10t^3
$$ represents zero over every completion of $\mathbb{Q}$ , but does not represent zero over $\mathbb{Q}$ . My question is: Is there a known explicit counterexample to the Hasse principle of the form: $$
x^3 + y^3 + z^3 + nt^3,
$$ for some $n \in \mathbb{Z}$ ,
and would a demonstration that the Hasse principle holds for cubic forms of this form not trivially imply the ""sums of three cubes conjecture""? By this I of course mean that $x^3 + y^3 + z^3$ is universal over $\mathbb{Q}_p$ for every $p \neq 3$ , and hence represents $-n$ over $\mathbb{Q}_p$ for every $p \neq 3$ . The representation of $0$ by $x^3 + y^3 + z^3 + nt^3$ over $\mathbb{Q}_3$ follows from the congruence conditions on $n$ mod 9, and thus the representation of $0$ by $x^3 + y^3 + z^3 + nt^3$ over $\mathbb{Q}$ follows from the Hasse principle. I am, of course, aware that $$\{
x^3 + y^3 + z^3 + nt^3 \mid n \in \mathbb{Z} \}
$$ is a very artificially constructed class of cubic forms. Many thanks.","['cubics', 'number-theory', 'p-adic-number-theory', 'reference-request', 'abstract-algebra']"
4232230,Laplace-de Rham operator acting on vector fields?,"I am (still) trying to get through the old paper of Dohrn and Guerra . It introduced a Laplace-Kodaira-de Rham operator $\Delta$ acting on a vector field $V$ , in its equation (14), as follws: ... given by the Laplace-Kodaira-de Rham operator $\Delta$ [10]. On vector fields $\Delta$ acts as \begin{equation*}\tag{14}
  (\Delta V)^i = g^{jk} \nabla_j\nabla_kV^i + R_{\,j}^i V^j.
\end{equation*} where $R_{\,j}^i$ is the curvature tensor associated to $g$ (the choice of the sign is opposite to that found in the mathematical literature). ...... [10] G. de Rham, Varietes differentielles, Actualites Sci. Ind. 1222 (1955), Paris. As I understand, the equation (14) adopts the abstract index notations. It yields that \begin{equation*}
g^{jk} \nabla_j\nabla_kV^i = g^{jk} (\nabla^2_{\partial_j,\partial_k} V)^i = [\mathrm{tr} (\nabla^2 V)]^i.
\end{equation*} So $g^{jk} \nabla_j\nabla_kV^i$ is nothing but the $i$ -th component of the connection Laplacian of the $(1,0)$ -tensor $V$ . The tensor $R_{\,j}^i$ should be, I guess, the minus of the contraction of the usual Riemann curvature tensor , that is, \begin{equation*}
R_{\,j}^i = - g^{kl} R^i_{kjl}
\end{equation*} which is exactly a type change of the Ricci curvature . So basically, the Laplace-Kodaira-de Rham operator for vector fields introduced here is just differed from the connection Laplacian by a type change of the Ricci curvature, which is very similar to the Weitzenböck identity for those Laplacians acting on forms. But, to my best of knowledge, the Laplace-de Rham operator $\Delta = d\delta + \delta d$ can only act on forms. How could it act on vector fields? The reference [10] here is the French version of G. de Rham's book, its English version can be found here . I looked all over this book, but I can only find the definition for the Laplace-de Rham operator acting on forms therein. Could anyone find more reference or provide an explanation for this? TIA...","['laplacian', 'riemannian-geometry', 'differential-geometry']"
4232239,What is the maximum amount of shuffled a deck of cards can be?,"I had this question pop into my head a few days ago and I've been thinking about it since. It has a fairly simple set up: you have 52 cards. Take the amount of ""shuffling"" to be the average distance between where each card started, and where it ended up after the shuffle. For example, given $N$ cards, the maximum shuffling would be decided as: $$\text{maximum shuffling} = \max \Bigg\{ \sum_{n=1}^{N} \frac{|\text{new}_n - \text{old}_n|}{N} \Bigg\}.$$ The old positioning of the card is arbitrary, they must simply be different, so I came up with: $$\text{maximum shuffling} = \max \Bigg\{ \sum_{n=1}^{N} \frac{|f(n, N) - n|}{N} \Bigg\}.$$ Such that $f(n, N):(\mathbb{N} \leq N)\times \mathbb{N} \to \mathbb(\mathbb{N} \leq N)$ is bijective. For $N=1$ , the maximum shuffling is $0$ .
For $N=2$ , the maximum shuffling is $1$ .
For $N=3$ , the maximum shuffling is $\frac{5}{3}$ . I do not know what it is for $N=4$ . My goal is to find $N=52$ . How would I go about this?",['statistics']
4232284,"Integral form(s) of a general tetration/power tower integral solution: $\sum\limits_{n=0}^\infty \frac{(pn+q)^{rn+s}Γ(An+B,Cn+D)}{Γ(an+b,cn+d)}$","In many tetration/power tower integrals, one sees a general form of the following. Let this new function be notation used to show the connection between the general result and special cases using types of Incomplete Gamma functions . The goal is to find an integral representation of the general case or a special case. If this is not possible, then maybe a special case of it has an integral representation. Note there are ways to put the summand into other functions, but this way is simple. Please note that I will use a made up general “T” function to show how each integral below it is a special case of the following: $$T_{p,q}^{r,s}\left(_{\ \ a,b,c,d}^{A,B,C,D}\right)=\sum_{n=0}^\infty \frac{(pn+q)^{rn+s}Γ(An+B,Cn+D)}{Γ(an+b,cn+d)}$$ Here is motivation that integral representations are possible. Note that I will use the primitive for simplicity: $$\int (cx)^{ax^b}dx=\sum_{n=0}^\infty\frac{(-a)^n Q(n+1,-(bn+1)\ln(cx))}{c^{bn+1}(bn+1)^{n+1}}= \frac{1}{-ac^{b-1}}\sum_{n=0}^\infty\frac{ (-ac^bbn-ac^b)^{n+1}Γ(n+1,-bn\ln(cx)-\ln(cx))}{Γ(n+1,0)} =  -\frac{1}{ac^{b-1}}  T_{-abc^b,-ac^b}^{1,1}\left(_{\ \ 1,1,0,0}^{1,1,-b\ln(cx),-\ln(cx)}\right) $$ $$\int a^{ta^t}dt=t+\frac{1}{\ln(a)}\sum_{n=0}^\infty \frac {(-1)^n Q(n+1,-nt\,\ln(a))}{n^{n+1}}= t-\frac{1}{\ln(a)}\sum_{n=0}^\infty \frac {(-n)^{-n-1} Γ(n+1,-nt\,\ln(a))}{Γ(n+1,0)} =t-\frac{1}{\ln(a)} T_{-1,0}^{-1,-1}\left(_{\ \ 1,1,0,0}^{1,1,-t\,\ln(a),0}\right) $$ $$\int \frac{dx}{xe^x-1}=\sum_{n=0}^\infty \frac{Γ(n+1,-nx)}{n^{n+1}}= \sum_{n=0}^\infty n^{-n-1} Γ(n+1,-nx)= T_{1,0}^{-1,-1}\left(_{0,1,0,0}^{1,1,-x,0}\right) $$ $$\int \text W(\ln(x))dx=\text W(\ln(x))(x-1)+\sum_{n=1}^\infty\frac{(-1)^n Q(n+1,-n\,\text W(\ln(x))}{n^{n+1}}= \text W(\ln(x))(x-1) -\sum_{n=1}^\infty\frac{(-n)^{-n-1}Γ(n+1,-n\,\text W(\ln(x))}{Γ(n+1)} = T_{-1,0}^{-1,-1}\left(_{1,1,0,0}^{1,1, -\,\text W(\ln(x)),0}\right) $$ Miscellaneous sums of interest. Subfactorial : $$\sum_{n=2}^\infty \frac{1}{!n}=e\sum_{n=0}^\infty \frac{1}{Γ(n+2,-1)}=e\,T_{p,q}^{0,0}\left(_{1,2,0,-1}^{0,1,0,0}\right)$$ $$\sum_{n=-\infty}^{-1} Γ(n,n)=\sum_{n=0}^\infty Γ(-n-1,-n-1)= T_{p,q}^{0,0}\left(_{-1,-1,-1,-1}^{\quad 0,1,0,0}\right) $$ I already know about the Abel-Plana formula , but it offers no new insights. There are other theorems that could possibly be used. How can the integral representations for the goal sum be found? If the integral representation is indeed impossible, then what is an integral  representation for a special case? This will help us solve similar problems. Please correct me and give me feedback!","['integration', 'summation-method', 'gamma-function', 'power-towers', 'tetration']"
4232306,What is the range of the Gamma function,"I know from the product definition $\Gamma(z)=\frac{e^{-\gamma z}}{z}\prod_{n=1}^{\infty}\left(1+\frac{z}{n}\right)^{-1}e^{z/n}$ that it has no zeros. Is $0$ the only value omitted by $\Gamma$ ? I guess another way of looking at the question is that $\frac{1}{\Gamma(z)}$ is an entire function, so (by little Picard) omits at most one complex value; so the question is whether this function is surjective or whether it omits one complex value (if the latter, do we know which one)? I have seen a similar question for Riemann's zeta function answering that $\zeta$ is surjective, but I haven't been able to find anything for the Gamma function (my motivation is that I just learnt little Picard so I'm reviewing several basic functions from complex analysis, like all the trig functions, exponential, etc and trying to know the ranges of more of these functions).","['complex-analysis', 'analysis', 'gamma-function']"
4232324,maximum-likelihood function for the SST distribution,"The SST distribution is a reparametrization of $ST3$ , which is the skew t-student type 3. Below is some information. Let $Z_0 \sim ST3 (0, 1, \nu, \tau)$ and $Y = \mu + \sigma\left(\frac{Z_0 - m}{s}\right)$ , where, \begin{equation}
m = E(Z_0) = \frac{2\tau^\frac{1}{2}(\nu-\nu^{-1})}{(\tau - 1)B\left(\frac{1}{2},\frac{\tau}{2}\right)}
\end{equation} \begin{equation}
s^2 = Var(Z_0) = \frac{\tau}{\tau - 2}(\nu^2 + \nu^{-2} - 1) - m^2
\end{equation} Hence, $Y = \mu_0 + \sigma_0Z_0$ , where $\mu_0 = \mu - \frac{\sigma m}{s}$ e $\sigma_0 = \frac{\sigma}{s}$ , and so $Y \sim ST3(\mu_0, \sigma_0, \nu, \tau)$ with $E(Y) = \mu$ and $Var(Y) = \sigma^2$ for $\tau > 2$ . Let $Y \sim SST(\mu, \sigma, \nu, \tau) = ST3(\mu_0, \sigma_0, \nu, \tau)$ for $\tau > 2$ . The pdf of the skew Student t distribution, denoted by $Y \sim SST(\mu, \sigma, \nu, \tau)$ , is given by: \begin{equation}
f_Y(y|\mu, \sigma, \nu, \tau) = \left\{ \begin{array}{rcl}
\frac{c}{\sigma_0}\left(1 + \frac{\nu^2 z^2}{\tau} \right)^{\frac{-(\tau + 1)}{2}} & \mbox{if}
& y<\mu_0 \\ \frac{c}{\sigma_0}\left(1 + \frac{z^2}{\nu^2 \tau} \right)^{\frac{-(\tau + 1)}{2}} & \mbox{if} & y\geq \mu_0 
\end{array}\right.
\end{equation} for $-\infty < y < \infty$ , where $-\infty < \mu < \infty$ , $\sigma >0$ , $\nu > 0$ and $\tau > 2$ and where $z=\frac{(y-\mu_0)}{\sigma_0}$ , $\mu_0 = \mu - \frac{\sigma m}{s}$ , $\sigma_0 = \frac{\sigma}{s}$ and $c = 2\nu [(1+\nu^2)B\left(\frac{1}{2},\frac{\tau}{2}\right)\tau^{\frac{1}{2}}]^{-1}$ . Note that $E(Y) = \mu$ and $Var(Y) = \sigma^2$ and the moment based skewness and excess kurtosis of $SST(\mu, \sigma, \nu, \tau)$ are the same as for $ST3(\mu_0, \sigma_0, \nu, \tau)$ , and hence the same as for $ST3(0, 1, \nu, \tau)$ , depending only on $\nu$ and $\tau$ . How would the likelihood function of this distribution be expressed, according to the parameters?
It is known that the link functions for $(\mu, \sigma, \nu, \tau)$ is the $identity$ , $log$ , $log$ , and $log-2$ .","['statistics', 'regression', 'probability-distributions', 'maximum-likelihood', 'probability']"
4232366,Recursive definition of hyperbolic functions,"I have a question about the recursive definition of functions, given as follows. Two sequences of functions, $\{C_k\}, \{S_k\}$ satisfy the following. $$S_k(x)=\int_0^x C_k(t)\,\mathrm dt$$ $$C_{k+1}(x)=1+\int_0^x S_k(t)\,\mathrm dt$$ with $C_1(x)=1$ . Prove that there exist two functions $C$ and $S$ on $\mathbb{R}$ , such that $\{C_k\}$ converges to $C$ , and $\{S_k\}$ converges to $S$ . As $C_1(x)$ is a polynomial, $C_k, S_k$ are polynomials for all $k$ . I wrote down some terms, and I got the following. $k$ $C_k(x)$ $S_k(x)$ $1$ $1$ $x$ $2$ $1+\frac{1}{2}x^2$ $x+\frac{1}{6}x^3$ $3$ $1+\frac{1}{2}x^2+\frac{1}{24}x^4$ $x+\frac{1}{6}x^3+\frac{1}{120}x^5$ I noticed that there are only even degrees in $C_k(x)$ , and odd degrees in $S_k(x)$ , so they are each even functions and odd functions, respectively. Given the pattern, I suppose $C_k$ and $S_k$ are given as $$C_k(x)=\sum_{n=0}^{k-1}\frac{x^{2n}}{(2n)!}$$ $$S_k(x)=\sum_{n=0}^{k-1}\frac{x^{2n+1}}{(2n+1)!}$$ (I can prove it by induction, but I haven't tried yet. I'm pretty sure that this is the right expression.) This is exactly the Taylor polynomial for $\cosh x$ and $\sinh x$ . So I suspect that $C=\cosh x$ , and $S=\sinh x$ . At this point, I have two questions. If I could show that $C_k$ is the $(k-1)$ th Taylor polynomial of $\cosh x$ for all $k$ , would that mean that $C=\cosh x$ ? (The same question goes to $S_k$ , too.) Whether this is a valid way or not, I think this way can give great insight, but not an elegant proof. I would like to prove the existence of $C$ and $S$ by only using the recursion expression, and that we can make $\|C_k-C\|_\infty <\epsilon$ for all $k>k_0$ (i.e the definition of uniform convergence). How could I do that? Any other proofs are welcome. Thanks in advance.","['functions', 'recursion', 'real-analysis']"
4232373,Combinatorics question: N people selecting k objects (without replacement) from field of K total objects. Odds all objects are selected?,"Title is a bit verbose so here's a scenario: Suppose 4 friends go out to a restaurant, and each of them will order 3 items from a total list of 9 items. Each item ordered by a single person is different. If each person's selection of items is completely random, what is the probability that between the 4 friends, all 9 items on the menu are sampled at least once? This type of problem would be simmed pretty easily, but I am very interested in the theoretical method that would be used to arrive at the answer. Here's what I think so far: If each person has (9 choose 3) item combinations (orders), then the total number of order combinations should be ((9 choose 3) ^ 4) / (4!) (since we don't care about who orders what, just that all items are covered). So all that's left is to quantify those order combinations in which every item is covered, and then divide by that total. However, this is where I'm stuck. It has occurred to me that one way to solve this would be to just add up all of the possible ways for the scenario to succeed. If the items are numbered 1-9 and each {} represents an order, this would look like:
P({1,2,3}, {1,2,3}, {4,5,6}, {7,8,9}) + P({1,2,3},{3,4,5},{5,6,7},{7,8,9}) + ...
But that type of breakdown seems like a nightmare especially at higher numbers. It feels like there should be a way to make clever use of a choose function for a relatively neat solution. Thanks for taking the time to read this, and I appreciate any help that may be offered toward finding a solution. Edit: After mulling this over, I think this can actually be solved with just straight probability without even worrying about the # of combinations. The probability that all items are covered would be 1 - the sum of 1 item, 2 items... 6 items not covered = 1 - Sum(i = 1 to i = 6) {((9-i) (8-i) (7-i)/(9x8x7))^4}. Doing that yields 76.8909%, which seems like a reasonable outcome (I haven't checked this for correctness). Even if the above approach works, I would be interested in knowing if there are any clever ways to quantify the number of combinations where every item is covered. Thanks!","['combinatorics', 'probability']"
4232379,The remainder when dividing an arbitrary polynomial by a quadratic,"I'm currently working through the introductory materials for an online math olympiad class, and I saw this problem: Given a polynomial $P(x)$ , compute the remainder of $P(x)$ when divided by $(x-a)(x-b)$ in terms of $a,b,P(a),P(b)$ I didn't really know any algorithm for doing this, so I just used some intuition about dividing a polynomial by a linear term. If I was just dividing $P(x)$ by $(x-a)$ , I know the remainder is $P(a)$ , and we might see this expressed as $P(x)=(x-a)Q(x)+P(a)$ for some polynomial $Q(x)$ . So I tried to generalize this to dividing by a quadratic term. I figured we'd have something in the form $P(x)=(x-a)(x-b)Q(x)+R(x)$ , where $R(x)$ is a linear polynomial, and also the remainder. I figured, since we have two degrees of freedom for $R(x)$ (since it must be linear.) I guessed I could determine $R(x)$ by making sure the equation $P(x)=(x-a)(x-b)Q(x)+R(x)$ was satisfied at $x=a$ and $x=b$ . This means we want $P(a)=R(a)$ and $P(b)=R(b)$ . The gives us exactly one line for $R(x)$ , which is: $$R(x)=\frac{P(a)-P(b)}{a-b}(x-a)+P(a)$$ And I'm pretty sure this is the correct remainder, but I'm not satisfied with my answer for two reasons. First, it looks really messy. And furthermore, I feel like there must be a simpler way to do this. I have no idea how I'd generalize this to division by cubics or quartics (Not that it's part of the problem, but it's a natural extension of it,) which usually means there's a simpler approach. My question: Is there more to this? Am I missing a more general method? Thank you! Note: Some may complain that this is a homework question, but I'd like to point out that 1. I already have the answer, I just want to know if there's a better solution, and 2. This problem isn't even required and I'm not getting a meaningful grade out of this.","['contest-math', 'algebra-precalculus', 'polynomials']"
4232391,Continuity of volume of balls as function of radius (metric measure spaces),"Let $(X,d,m)$ be a proper metric measure space, i.e. closed ( $d$ -)balls are compact $m$ is a Borel atomless measure, finite and non-zero on every ball of positive radius (If necessary, we can assume that $(X,d)$ is complete.) Question: What are (mild) sufficient conditions such that for every $x_0\in X$ there exists $r_0=r_0(x_0)$ such that $F_{x_0}\colon r\mapsto m B_r(x_0)$ is continuous on $[0,r_0)$ Comments: It is clear that the assertion does not hold on arbitrary metric measure spaces (see here ). Furthermore, it is always true that for every $x_0\in X$ the function $F_{x_0}$ has up to countably many discontinuity points (see here ). Also, $F_{x_0}$ is continuous at $0$ , since $m$ is assumed atomless The property is stable under multiplication by $L^p_{\rm loc}(m)$ densities ( $p>1$ , but maybe not $p=1$ ), in the sense that if the assertion is true for $m$ , then it is true for $fm$ for any density $f$ as above It seems reasonable to expect the property to hold for $\mathsf{CD}(K,N)$ spaces, $\mathsf{MCP}(K,N)$ spaces, and maybe spaces satisfying a Bishop–Gromov-type or a Brunn–Minkowski-type inequality, but I would very much like some simpler assumptions: e.g. doubling (and, if needed, weak (1,1)-Poincaré , however I do not see why it could be needed). 5'. Bishop–Gromov seems a reasonable assumption because in that case $x\mapsto m B_r(x)$ is continuous for each fixed $r$ (see here , Lemma A.1) A proof/reference for $\mathsf{MCP}(K,N)$ spaces would also be much appreciated. While this seems reasonable to expect, I am not able to show the sought property for the Hausdorff measure on Ahlfors regular spaces (definition here )","['measure-theory', 'optimal-transport', 'metric-spaces', 'geometric-measure-theory']"
4232428,Hitting a target with a die: finding a better closed form of the recursive formula,"Roll a k -sided die over and over and sum the results. What's the probability that the result will eventually hit exactly n ? The recursive formula is: $$
p_{k,n}=
\begin{cases}
\begin{array}{cc}
 0 & n<0 \\
 1 & n=0 \\
 \sum _{x=1}^k \frac{p_{k,n-x}}{k} & n>0 \\
\end{array}
 \\
\end{cases}
$$ Through extremely tedious trial and error, I found the closed form: $$
p_{k,n}=
\frac{(k+1)^{n-1}}{k^n}+\sum_{x=1}^{\lfloor{n/(k+1)}\rfloor}(-1)^x\frac{n\cdot (kx+x)^{n-kx-x-1}\cdot x^{kx+x-n}\cdot(n-kx-1)!}{k^{n-kx}\cdot(x-1)!\cdot(n-kx-x)!}
$$ Mathematica: closed[k_,n_]:=(k+1)^(n-1)/k^n+Sum[(-1)^y*n*(k*y+y)^(n-k*y-y-1)*y^(k*y+y-n)*(n-k*y-1)!/k^(n-k*y)/(y-1)!/(n-k*y-y)!,{y,1,Floor[n/(k+1)]}] Does a cleaner closed form exist? Is there a general approach that works well on recursive formulas with multiple base cases?","['recurrence-relations', 'closed-form', 'probability', 'recursion']"
4232439,Inverse limit and group completion are isomorphic,"It's been a while I get stuck on proof from Lang's Algebra book, which states that the completion and the inverse limit $\varprojlim G/H_{r}$ are isomorphic where $\{H_r \}$ is a sequence of normal subgroups in $G$ with $H_r 	\supset H_{r+1}$ for all $r$ . Theorem 10.1 : The completion and the inverse limit $\varprojlim G/H_{r}$ are isomorphic under natural mappings. Proof : We give the maps. Let $x=\{x_n\}$ be a Cauchy sequence. Given $r$ , for all $n$ sufficiently large, by the definition of Cauchy sequence, the class of $x_n \mod H_r$ is independent of $n$ . Let this class be $x(r)$ . Then the sequence $(x(1),x(2),…)$ defines an element of the inverse limit. Conversely, given an element $(\overline{x}_1,\overline{x}_2,…)$ in the inverse limit, with $\overline{x}_n \in G/H_n$ , let $x_n$ be a representative in G. Then the sequence $\{x_n\}$ is Cauchy. We leave to the reader to verify that the maps we have defined are inverse isomorphisms between the completion and the inverse limit. Actually, I don't understand this proof at all. Given $r$ and $x=\{x_n\}$ be a Cauchy sequence, what is exactly $x(r)$ ? By definition of Cauchy sequence, there is exits $N$ such that for all $m,n \geqslant N $ we have $x_nx_m^{-1} \in H_r$ , so is $x(r)$ the set $\{ x_i \mid i  \geqslant N\}$ ? In the proof, he said that the sequence $(x(1),x(2),…)$ defines an element of the inverse limit. Why does $(x(1),x(2),…) $ modulo the null sequences give us a single element? To see that $\{x_n \}$ is a Cauchy sequence from $(\overline{x}_1,\overline{x}_2,…)$ , by definition of limitt inverse, we have $f^{n}_{m}(\overline{x}_n)=\overline{x}_m$ , i.e $x_m, x_n$ are equal in $G/H_m$ which means $x_m x_n^{-1} \in H_m$ . Therefore, given $r$ , for any $m,n \geqslant r$ we have $x_m x_n^{-1} \in H_m \subset H_r$ so $\{x_n \}$ is indeed a Cauchy sequence. But how does two representative sequence $\{x_n \}$ and $\{x'_n \}$ of $(\overline{x}_1,\overline{x}_2,…)$ give us a single element in completion? Any helps would be appreciated! Thanks.","['group-theory', 'abstract-algebra']"
4232476,"How to find $ \iint_{D_R}e^{-x}\arctan(\frac{y}{x})\,dx\,dy\,$?","How to find $$ \iint_{D_R}e^{-x}\arctan\frac{y}{x}\,dx\,dy\,,$$ where $$ D_R=\left\{(x,y) \mid \frac{R}{2}\leq x\leq R, 0\leq y \leq \frac{2}{R}x-1 \right\}\,?$$ I know that $$\int \arctan \frac{y}{x}\, dy =y \arctan \frac{y}{x} -\frac{x}{2} \ln\left(1+\left(\frac{y}{x}\right)^2\right)+C$$ So how to find $$\int_{R/2}^R e^{-x} \left(\frac{2}{R}x-1 \right) \arctan \frac{\frac{2}{R}x-1}{x} -\frac{x}{2} \ln\left(1+\left(\frac{\frac{2}{R}x-1}{x}\right)^2\right)\,dx$$ and it's difficult to calculate.","['multivariable-calculus', 'multiple-integral']"
4232504,How can there be an infinite number of nesting finite sets?,I'm working through the book Understanding Analysis by Stephen Abbott and encountered this problem: And I can't really wrap my head around how there could be an infinite series of nesting sets each containing finite elements? And how their intersection will be non-empty? Since they have finite elements wouldn't it mean that at one point they would 'run out' of elements and become an empty set? I would really appreciate if someone could also explain to me the logic behind why (a) is false. I'm self studying Analysis and everything has been really confusing.,['elementary-set-theory']
4232506,Function to grab specific digits from a number?,"Recently I’ve been thinking a lot about grabbing digits from numbers, for things like calculating multiplication persistence, i.e. turning $1234$ into a $1\times2\times3\times4$ . I’ve been able to come up with $$\lfloor logx \rfloor +1$$ to output the number of digits of the given number $x$ , as well as $$\left\lfloor \frac{x}{10^{ \lfloor logx \rfloor -n+1}} \right\rfloor$$ which outputs the first $n$ digits of any number x. But that’s about as far as I’ve been able to think up. Is there any way to construct a function that retrieves the $n^{th}$ digit of a given number? Forgive me if my notation or comprehension is poor, I have no formal education in this field of maths.","['combinatorics', 'decimal-expansion']"
4232543,Show that $H_{DR}^2(\mathbb{R}^2-P-Q)=0$ without using Mayer-Vietoris Sequence,"I'm reading Bott & Tu's Differential Forms in Algebraic Topology and its Exercise 1.7 asks me to compute $H_{DR}^*(\mathbb{R}^2-P-Q)$ where $P$ and $Q$ are two points in $\mathbb{R}^2$ . I've figured out how to compute this for degree $0$ and $1$ , but get stucked on showing that $H_{DR}^2(\mathbb{R}^2-P-Q)=0$ . I know how to show this using Mayer-Vietoris sequence, but this exercise is put just before the section of Mayer-Vietoris sequence, so I really wonder a solution without the use of that. I attempted to show that every $2$ -form $f(x,y)dxdy$ is exact using the integration on path to find $\alpha dx+\beta dy$ such that $d(\alpha dx+\beta dy)=fdxdy$ , but it seems that with two holes in $\mathbb{R}^2$ there is not a legal choice of pathes that may work. To find $\alpha$ and $\beta$ is exactly to solve the differential equation $\frac{\partial \alpha}{\partial y}= -\frac{f}{2} $ and $\frac{\partial \beta}{\partial x}=\frac{f}{2}$ . Is there any result for this in PDE? Thanks in advance for any hint, solution or reference that contains a solution.","['differential-topology', 'partial-differential-equations', 'differential-forms', 'differential-geometry']"
4232581,Differentiating a Vector and a Matrix w.r.t. a Vector [Matrix Calculus],"I am studying matrix calculus for linear regression and machine learning and I would like to know exactly if the following calculations are correct: Let $y=\sin(x+yz)$ and $r=\begin{bmatrix}x\\y\\z\end{bmatrix}$ Then the following is the gradient of $y$ i.e., the derivative of $y$ with respect the vector $r$ in denominator layout: $$\frac{\partial y}{\partial r}=\frac{\partial\sin(x+yz)}{\partial r}=\begin{bmatrix}\frac{\partial \sin(x+yz)}{\partial x}\\\frac{\partial\sin(x+yz)}{\partial y}\\\frac{\partial \sin(x+yz}{\partial z}\end{bmatrix}=\begin{bmatrix} \cos(x+yz)\\z\cos(x+yz)\\y\cos(x+yz)\end{bmatrix}$$ In numerator layout it would be: $$[\cos(x+yz), z\cos(x+yz), y\cos(x+yz)]$$ Now, let $$\mathbf{y}=\begin{bmatrix} e^{xyz}\\x^2z\\yx\end{bmatrix}$$ So in numerator layout: $$\frac{\partial\mathbf{y}}{\partial r}=\begin{bmatrix} \frac{\partial e^{xyz}}{\partial x} & \frac{\partial e^{xyz}}{\partial y} & \frac{\partial e^{xyz}}{\partial z}\\ \frac{\partial x^2z}{\partial x} & \frac{\partial x^2z}{\partial y} & \frac{\partial x^2z}{\partial z}\\ \frac{\partial yx}{\partial x} & \frac{\partial yx}{\partial y} & \frac{\partial yx}{\partial z}\end{bmatrix}\\ =\begin{bmatrix} yze^{xyz} & xze^{xyz} & xye^{xyz}\\2xz& 0 & x^2 \\y & x & 0\end{bmatrix} $$ In denominator layout it would be the transpose of the above matrix? Now, let $$Y=\begin{bmatrix} x^2yz & xy^2z \\xyz^2 & \ln(xyz) \end{bmatrix}=\begin{bmatrix} Y_{11} & Y_{12} \\Y_{21} & Y_{22} \end{bmatrix}$$ Then $$\frac{\partial Y}{\partial r}=\begin{bmatrix} \frac{\partial Y_{11}}{\partial x} & \frac{\partial Y_{12}}{\partial x} & \frac{\partial Y_{11}}{\partial y} & \frac{\partial Y_{12}}{\partial y} & \frac{\partial Y_{11}}{\partial z} & \frac{\partial Y_{12}}{\partial z} \\ \frac{\partial Y_{21}}{\partial x} & \frac{\partial Y_{22}}{\partial x} & \frac{\partial Y_{21}}{\partial y} & \frac{\partial Y_{22}}{\partial y} & \frac{\partial Y_{21}}{\partial z} & \frac{\partial Y_{22}}{\partial z}\end{bmatrix}$$ Would this be correct? I'm worrying I mixed up the shape and/or the positions of the members of the matrices $\frac{\partial\mathbf{y}}{\partial r}$ and $\frac{\partial Y}{\partial r}$ .","['vectors', 'multivariable-calculus', 'calculus', 'matrix-calculus', 'derivatives']"
4232589,Fit cube in a circular hole (only 50% of the time),"QUESTION: You have a hole in the shape of a circle with radius R and a cube with side 1. If the probability that the cube will enter the hole is 50%, what is the  radius R of the circle? Examples of three different orientations of the cube exactly (if one would connect the center of the cube and the center of the hole, they would form a vertical line) above the hole: What I've tried: I know the longest straight line distance in the cube will be √3. And then I thought, starting with that distance representing the diameter of a circle and now rotate it either clock-wise or counter clock-wise (don't matter obv). But here I'm stuck. It seems that I should rotate it x many degrees so that it will fit exactly only 50% of the time. But can't come up with the answer. Maybe it's not even the right approach and you have a different approach and can come up with an answer. Thanks.","['geometry', 'probability']"
4232594,Finds the point of discontinuity of $x[x]$ and $[\sin{x}]$,"Find the points of discontinuity of $f(x)$ = $x[x]$ and $g(x)$ = $[\sin{x}]$ , where $[]$ stands for greatest integer function. For $f(x)$ , as we know that $[x]$ is discontinuous at integers therefore I tried to check if $f$ is discontinuous at integers. I determined the left-hand and right-hand limits of $f$ at $a$ (Say, $a$ $\in$ $\mathbb Z$ ), both are found to be different. The left-hand limit of $f$ at $a$ is $a(a-1)$ and right hand limit is $a^{2}$ . Then f should be discontinuous at integers same as $[x]$ and if we take $a=0$ then it will be continuous at $0$ So $f(x)$ will be discontinuous at integers except $0$ . But If I want to draw the graph of this function then I have to combine the graph of $x$ and $[x]$ (I guess?). And this graph is confusing me. Please correct me if I am wrong. Also, how can I confirm the same with the help of the graph? For $g(x)$ I drew the graph of $\sin{x}$ and with its help, I drew the graph of $[\sin{x}]$ for [0, $4\pi$ ] and found that the points of discontinuities for this function are such as: $\pi/2$ , $\pi$ , $2\pi$ , $5\pi/2$ , $3\pi$ , $4\pi$ then for $\mathbb R$ , the set of points of discontinuities for $g(x)$ will be { $(4$ n $-3)\pi/2$ , $n\pi$ : $n\in\mathbb Z$ }.But I am not completely sure if I am correct. Please correct me if I am wrong somewhere. Any hints will be appreciated. Thanks in advance.","['limits', 'functions', 'continuity']"
4232623,Is the inequality in Hoeffding's lemma ever tight?,"The Hoeffding Lemma asserts that $X$ is a random variable bounded between $[a,b]$ then $$\mathbf{E}[e^{\lambda (X - \mathbf{E}[X])}] \leq e^{\lambda^2(b-a)^2/8}$$ A typical example which asks us to show tightness of the above bound is using symmetric random variables. $X$ s.t. $X$ takes value $a$ w.p. $1/2$ and $b$ w.p. $1/2$ . WLOG Lets take $a$ and $b$ to be $-1$ and $1$ . Thus a symmetric rademacher variable.
The LHS of the Hoeffding inequality gives us $\frac{e^\lambda + e^{-\lambda}}{2}$ and the RHS gives us $e^{\lambda^2/2}$ . I dont see any way how these expressions can be same. More generally except for constant random variables I dont see anyway this inequality could hold as an equality. Are there non trivial examples where the above inequality holds with equality","['inequality', 'probability-theory', 'probability', 'random-variables']"
4232624,Connected graph of $f$ and $g$ implies connected graph of $f\circ g$?,"Is it true that if two functions $f,g:\mathbb R\to\mathbb R$ both have connected graph, then so does their composition $f\circ g$ ? I think, the answer is negative, and I would like to find an explicit example. What we know: If $f$ or $g$ is continuous, then then graph of $f\circ g$ is connected. If the graph of a function $f:\mathbb R\to\mathbb R$ is PATHconnected, then $f$ is continuous. Any Darboux function which is Baire 1 has a connected graph There is a counterexample to the above question in a (subtopology of) $\mathbb R^2$ . Any help is highly appreciated, thank you very much in advance.","['connectedness', 'general-topology', 'real-analysis']"
4232631,Two types of averages for ratios of two things,"It was brought to my attention in this video that, where you have some attribute of your data that is the ratio of two other attributes, there are two ways of generating the mean ""average"" of this ratio. Taking the example from the beginning of the video, suppose that $x_i$ is the total sales (in \$) for a given line item, and $k_i$ is the quantity sold: Item ID Sales Qty sold Accessory #1 $x_1$ $k_1$ Accessory #2 $x_2$ $k_2$ Accessory #3 $x_3$ $k_3$ For each individual line item, you might define ""Sales per unit"" as $u_i=\frac{x_i}{k_i}$ . However, as the video points out, there are two ways one might then define the average Sales per unit for an Accessory. Using the video's terminology: $$\text{AGG}=\frac{x_1+x_2+x_3}{k_1+k_2+k_3}$$ $$\text{AVG}=\frac{1}{3}\left(\frac{x_1}{k_1}+\frac{x_2}{k_2}+\frac{x_3}{k_3}\right)$$ So $\text{AVG}$ is just $\bar{u}$ , whereas $\text{AGG}$ creates a composite Accessory item $\left(X, K\right)$ , whose sales and quantity are the sum of the individual line items', and calculates $U=\frac{X}{K}$ for that composite item. Plugging in example numbers for the $x_i$ and $k_i$ suggests that $\text{AGG}$ and $\text{AVG}$ do not in general give the same value. My question is: what are the qualitative differences in what $\text{AGG}$ and $\text{AVG}$ are doing? Which provides the better view of what average sales per unit for an Accessory are?","['descriptive-statistics', 'statistics']"
4232702,Examples of prime spectra of monoids,"Given a (pointed or unpointed) monoid $M$ , one defines an ideal of $M$ to be a subset $I$ of $M$ such that if $a\in I$ and $r\in M$ , then $ra\in I$ (and such that $0\in I$ if $M$ is pointed). A prime ideal $\frak p$ of $M$ is then an ideal such that if $ab\in\frak p$ , then $a\in\frak p$ or $b\in\frak p$ . The set of all prime ideals of $M$ can is written $\mathrm{MSpec}(M)$ , and it can be made into a topological space equipped with a sheaf of monoids, giving a variant of algebraic geometry where monoids play the role of rings. Good references for this are Sections 1--2 of this paper and Ogus's book . Lately I've been trying to compile some examples of such prime spectra, and wondering about how these relate to the usual $\mathrm{Spec}$ of a ring. So far, I've found or read about the following examples (the first five examples below come from Martin Brandenburg's answer here ): $\mathrm{MSpec}(\mathbf{N},+)=\{0,\mathbf{N}_{>0}\}$ . $\mathrm{MSpec}(\mathbf{Z},+)=\{0\}$ . $\mathrm{MSpec}(\mathbf{N},\cdot)=\mathrm{MSpec}(\mathbf{Z},\cdot)=\mathcal{P}(\mathbf{P})$ , the powerset of the set $\mathbf{P}$ of all prime numbers. $\mathrm{MSpec}(\mathbf{N}\otimes_{\mathbf{N}_+}\mathbf{N})\cong\mathcal{P}(\mathbf{P}\times\mathbf{P})$ . $\mathrm{MSpec}(\mathbf{Z}\otimes_{\mathbf{F}_1}\mathbf{Z})\cong\mathcal{P}(\mathbf{P})\times\mathcal{P}(\mathbf{P})$ . $\mathrm{MSpec}(K)=\{(0)\}$ whenever $K^\times=K\setminus\{0\}$ . In particular this applies to $\mathbf{F}_1=\{0,1\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x])=\{(0),(x)\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x,y])=\{(0),(x),(y),(x,y)\}$ . $\mathrm{MSpec}(\mathbf{F}_1[x_1,...,x_n])=\mathcal{P}(\{x_1,...x_n\})$ . $\mathrm{MSpec}(\mathbf{F}_1[t,t^{-1}])=\{(0),(t),(t^{-1})\}$ . Now, any ring $R$ has an associated monoid, given by keeping only multiplication, and these should give more examples. For the usual prime spectrum of a ring, we have \begin{align*}
\mathrm{Spec}(\mathbf{C}[x])   &= (0)\cup\bigcup_{a\in\mathbf{C}}(x-a),\\
\mathrm{Spec}(\mathbf{C}[x,y]) &= (0)\cup\bigcup_{a,b\in\mathbf{C}}(x-a,y-b)\cup\{p(x,y):\text{$p\in\mathbf{C}[x,y]$ irreducible}\}\\
\mathrm{Spec}(\mathbf{Z}_p)    &= (0)\cup(p)
\end{align*} and $\mathrm{Spec}(\mathbf{Z}[x])$ is known and described in Spectrum of $\mathbb{Z}[x]$ . Do we know what are the $\mathrm{MSpec}$ of the underlying pointed monoids of these four rings?","['monoid', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4232720,Continuous Poisson Distribution,"Is there a Continuous analogous of the Poisson Distribution? Under the analogous, I mean such a distribution that: It is a one-parameter distribution Its distribution function is similar to the Poisson one","['poisson-distribution', 'probability-distributions', 'probability']"
4232730,"Prove that $\mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y)$, for $x<y.$","Let $X_1, \cdots,X_n $ be iid random variables with distribution $F. T_n(x)$ denotes the number of elements $\le x; x \in \mathbb R$ . Prove that $\mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y)$ , for $x<y$ . Attempt Let $j = T_n(y) ,i = T_n(x)$ . Then $\mathbb E [T_n(x) ~ T_n(y)] = \sum_{j = 0}^n ~\sum_{i = 0} ^j ~ ^jC_i~ [F_X(x)]^i [F_X(y) - F_X(x)]^{j-i} \cdot j \cdot i$ $= \sum_{j = 0}^n j \cdot [F_X(y) - F_X(x)]^{j}~~\sum_{i = 0} ^j ~ ^jC_i~ \dfrac {[F_X(x)]^i} {[F_X(y) - F_X(x)]^{i}} \cdot i$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j}~ \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} ~~\sum_{i = 0} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\sum_{i = 1} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~~\sum_{m = 0} ^{j-1} ~ ^{j-1}C_m~ \dfrac {[F_X(x)]^m} {[F_X(y) - F_X(x)]^m}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} +1 \big)^{j-1}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(y)]} {[F_X(y) - F_X(x)]} \big)^{j-1}$ $= \sum_{j = 0}^n j^2 \cdot {[F_X(x)]}~{[F_X(y)]}^{j-1} $ $= \dfrac{F_X(x)}{F_X(y)}\sum_{j = 0}^n j^2 \cdot ~{[F_X(y)]}^{j} $ This doesn't lead to the result being proved. Could someone advise?","['statistics', 'probability-distributions', 'order-statistics']"
4232744,Why does $f'(x_{(1)})=-r_{(1)}$ in conjugate gradient?,I am reading through the paper Painless Conjugate Gradient by Jonathan Shewchuk and have reached the following passage: Everything in there makes sense up until he says $f'(x_{(1)})=-r_{(1)}$ . How did he derive that? I'm having a lot of trouble visualizing why that would be the case.,"['derivatives', 'gradient-descent', 'linear-algebra']"
4232747,The operation rule of the semidirect product $C_m\rtimes C_k$,"It is known that the semidirect product $C_m\rtimes C_k$ is defined by presentation $$
C_m\rtimes C_k=\langle a, b\mid a^m=1, b^k=1, b^{-1}ab=a^e\rangle,
$$ where $e^k\equiv1\pmod{m}$ . Since every element in $C_m\rtimes C_k$ is of the form $a^{\alpha}b^{\beta}$ , where $\alpha\in\{0,1,\ldots,m-1\}$ and $\beta\in\{0,1,\ldots,k-1\}$ , there exist integers $x,y$ for which $$
(a^{\alpha}b^{\beta})(a^{\gamma}b^{\delta})=a^{x}b^{y}
$$ How to express the integers $x,y$ using $\alpha,\beta,\gamma,\delta$ ? If the elements where of the form $b^{\beta}a^{\alpha}$ is was easier to use the relation $b^{-1}ab=a^e$ , But here I can not see how to use it to ""shift"" the product to the form $a^xb^y$ . Thanks!","['group-presentation', 'finite-groups', 'combinatorial-group-theory', 'semidirect-product', 'group-theory']"
4232753,Let G be a 3-regular plane graph with 12 faces. How many vertices does G have?,This would be pretty easy to solve if I knew that G is connected by using Eulers formula $|V| - |E| + |F| = 2$ . But I don't know how to show that G is connected. Am I on the wrong path? Or is there some combinatorial argument to count the vertices?,"['graph-theory', 'graph-connectivity', 'discrete-mathematics', 'planar-graphs']"
4232775,Second derivative of a log function,"I am trying to calculate the Fisher Information (covariance matrix) for categorical random variable. I can calculate the first and the second derivatives of the following function, but to calculate their joint second derivative, I got lost. Can anyone explain how to derive off diagonal entries of covariance matrix? $\frac{dev^2L}{dev_pdev_(1-p)}$ $\log(p) + \log(1-p)$ =","['calculus', 'statistics']"
4232820,What method was used to solve this non-linear differential equation?,"Consider the following first-order non-linear differential equation: $$\left(\frac{dx}{dt}\right)^2+1=\frac{a}{x}\,,$$ where $a\in \mathbb{R}_{>0}$ . I have been reading a book where they provide the following solution, assuming that $x(t)$ has a local maximum at $t=0$ , in the parametric form: $$
\left\{ \begin{aligned}t\left(\eta\right) & =\frac{1}{2}x_{max}\left(\eta+\sin\eta\right)\,,\\
x\left(\eta\right) & =\frac{1}{2}x_{max}\left(1+\cos\eta\right)\,.
\end{aligned}
\right.
$$ My question is how was this solution found? I've been playing around with the equation, but I cannot find a way to solve it.",['ordinary-differential-equations']
4232829,Estimating $\tan(\cos(\sin1))$,"The precise question through which I came across this particular estimation is as follows: If $x=\alpha$ is the maximum value of $x$ for which $ \left \lfloor{\sin^{-1}(\cos^{-1}(\tan^{-1}x))} \right \rfloor = 1$ , then $\alpha$ lies in the interval: A) $\left(0,\dfrac{1}{\sqrt3} \right)$ B) $\left(\dfrac{1}{\sqrt3},1 \right)$ C) $\left(1,\sqrt3 \right)$ D) $(\sqrt3, \infty)$ Finding the value of $\alpha$ isn't hard. The part where I'm stuck in is the estimation of its interval. $$ \begin{align} &\ \left \lfloor{\sin^{-1}(\cos^{-1}(\tan^{-1}x))} \right \rfloor = 1 \\
  \implies &\ \sin^{-1}(\cos^{-1}(\tan^{-1}x)) \in \left[1,\frac{\pi}{2}\right] \\
\implies &\ \cos^{-1}(\tan^{-1}x) \in \left[\sin1,1\right] \\
\implies &\ \tan^{-1}x \in [\cos1,\cos(\sin1)] \\
\implies &\ x \in [\tan(\cos1),\tan(\cos(\sin1))] \end{align} $$ From here, it's very clear that $\alpha =\tan(\cos(\sin1))$ but now I am supposed to predict the interval in which it lies. The options seem to represent the numerical values of $\tan0, \tan\frac{\pi}{6},\tan\frac{\pi}{4},\tan\frac{\pi}{3},\tan\frac{\pi}{2} $ . So the problem essentially comes down to predicting whether $\cos(\sin1)$ lies in $\left(0, \frac{\pi}{6}\right), \left(\frac{\pi}{6}, \frac{\pi}{4}\right),\left(\frac{\pi}{4}, \frac{\pi}{3}\right), \left(\frac{\pi}{3}, \frac{\pi}{2}\right)  $ Through rough estimation, I was able to arrive at (B), which is also the correct answer. $$  \frac{\pi}{4} < 1 < \frac{\pi}{3}  $$ $$ \frac{1}{\sqrt2} < \sin1 < \frac{\sqrt3}{2} $$ $$ \begin{align} \cos(\sin1) &\ \in (\cos0.866, \cos0.707) \\
&\ \in (\cos49.6^{\circ}, \cos40.5^{\circ}) \\
&\ \approx \cos45^{\circ} \\
&\ = \frac{1}{\sqrt2} \in \left(\frac{\pi}{6}, \frac{\pi}{4}\right) \end{align}$$ I guess my approximations were a bit too rough. But anyway, what I am looking for is a concrete mathematical way to prove that $\cos(\sin1)$ lies in the interval $\left(\frac{\pi}{6}, \frac{\pi}{4}\right)$ without any inconclusive approximations.","['trigonometry', 'interval-arithmetic', 'estimation']"
4232859,Evaluate : $S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7}+\frac{1}{9\cdot10\cdot11}+\cdots$,"Evaluate: $$S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7} + \frac{1}{9\cdot10\cdot11}+\cdots$$ to infinite terms My Attempt: The given series $$S=\sum_{i=0}^\infty \frac{1}{(4i+1)(4i+2)(4i+3)} =\sum_{i=0}^\infty \left(\frac{1}{2(4i+1)}-\frac{1}{4i+2}+\frac{1}{2(4i+3)}\right)=\frac{1}{2}\sum_{i=0}^\infty \int_0^1 \left(x^{4i}-2x^{4i+1}+x^{4i+2}\right) \, dx$$ So, $$S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4} + \frac{x^2}{1-x^4}\right)dx=\frac{1}{2} \int_0^1 \left(\frac{1+x^2}{1-x^4}-\frac{2x}{1-x^4}\right)\,dx = \frac{1}{2} \int_0^1 \left(\frac{1}{1-x^2}-\frac{2x}{1-x^4}\right)\,dx$$ $$=\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\int_{0}^{1}\frac{2x}{1-x^4}dx=\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\frac{1}{2}\int_{0}^{1}\frac{1}{1-y^2}dy=0(y=x^2)$$ which is obviously absurd since all terms of $S$ are positive. But if I do like this then I am able to get the answer, $$S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4}+\frac{x^2}{1-x^4}\right)dx=\frac{1}{2}\int_{0}^{1}\frac{(1-x)^2}{1-x^4}dx=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1+x}-\frac{x}{1+x^2}\right)dx=\frac{\ln2}{4}$$ What is wrong with the previous approach","['integration', 'summation', 'definite-integrals', 'real-analysis']"
4232880,Finding a missing angle in the picture containing regular hexagon and square,"I want to find $\angle AGM=\theta$ in the following picture: Here $ABCDEF$ and $BAGH$ are regular hexagon and square respectively and $M$ is the midpoint of $FH$ . I found a trigonometric solution. I'm providing key ideas of the solution: Let $AB=1$ . Now we can apply cosine rule on $\triangle AHF$ to find $HF$ and $HM$ . Now in $\triangle MGH$ , we can find $GM$ using cosine rule again and then find $\angle MGH$ by sine rule. This gives $\theta=15^{\circ}$ . (I'm not providing the calculations as they are not nice and I did most of them with calculator.) But I believe there are some beautiful synthetic solution to the but didn't find one. So, I need a synthetic solution to the problem.","['contest-math', 'euclidean-geometry', 'geometric-construction', 'angle', 'geometry']"
4232899,Inverting an equation involving a sum of shifted functions,"I have the following system: \begin{equation}
f(x) = \sum_{n=-\infty}^{\infty} A_n(x-n x_0) g(x-nx_0)
\end{equation} where system functions $A_n(x)$ are known, and I am trying to express output $g(x)$ in terms of the input $f(x)$ . We may assume that $A_n(x) = 0$ for $n > N$ , such that the summation consists of finitely many terms. Also, the range of $x$ of interest can be restricted to a finite interval. However, I am having trouble trying to invert the expression to determine $g(x)$ . This is what I have attempted so far: I suspect the answer will take the form \begin{equation}
g(x) = \sum_{n=-\infty}^{\infty} B_n(x) f(x+nx_0)
\end{equation} So, it looks like I want to determine what coefficients $B_n(x)$ are, and I believe these coefficients will be dependent only on the coefficients $A_n(x)$ : If I change the input, I expect the output to change while $A_n(x)$ aren't changed. By symmetry in the target inverse expression, $B_n(x)$ should also not change, i.e. it is not dependent on the particular values of the input and output functions, but only dependent on $A_n(x)$ . Please let me know if you think this assumption is incorrect. By substituting one equation into another, we obtain \begin{equation}
f(x) = \sum_{j=-\infty}^{\infty}\sum_{k=-\infty}^{\infty} A_j(x-jx_0) B_k(x - jx_0) \, f(x -jx_0 + kx_0)
\end{equation} We can use the Kronecker delta to compact this equation into \begin{equation}
\sum_{j=-\infty}^{\infty}\sum_{k=-\infty}^{\infty} \Big(A_j(x-jx_0) B_k(x - jx_0) - \delta_{jk}\Big)\, f(x -jx_0 + kx_0) = 0
\end{equation} $A_n(x)$ and $B_n(x)$ should not depend on the input, and so we can consider $f(x) = 1$ to get \begin{equation}
\sum_{j=-\infty}^{\infty}\sum_{k=-\infty}^{\infty} A_j(x-jx_0) B_k(x - jx_0) = 1
\end{equation} This is as far as I've got, but I feel like an appropriate choice of $f(x)$ might reveal more relationships between $A_n(x)$ and $B_n(x)$ that would allow them to be solved. Simpler(?) case If the general problem cannot be practically solved, then the case I am particular interested in is when $A_n$ is constant for $n \ne 0$ : \begin{equation}
f(x) = A_0(x) g(x) + \sum_{n \ne 0} A_n g(x-nx_0)
\end{equation} UPDATE In the paper Sandberg, H., & Mollerstedt, E. (2005). Frequency-domain analysis of linear time-periodic systems. IEEE Transactions on Automatic Control, 50(12), 1971-1983. the concept of a Harmonic Transfer Function is used to relate functions of frequency that follow the same relationship as above. In the terms used in the question, an infinite dimension vector is defined, whose elements consist of a function that is incrementally shifted as one goes down the vector: \begin{equation}
\mathbf{f}(x) = 
\begin{bmatrix}
\dots & f(x+2x_0) & f(x+x_0) & f(x) & f(x-x_0) & f(x-2x_0) & \dots
\end{bmatrix}^T
\end{equation} \begin{equation}
\mathbf{g}(x) = 
\begin{bmatrix}
\dots & g(x+2x_0) & g(x+x_0) & g(x) & g(x-x_0) & g(x-2x_0) & \dots
\end{bmatrix}^T
\end{equation} That is, we have vectors whose elements are defined as $f_n(x) = f(x-nx_0)$ , and $g_n(x) = g(x-nx_0)$ , where $-\infty < n < \infty$ . Therefore, we can express the following: \begin{equation}
f_j(x) = \sum_{j = -\infty}^{\infty} A_{jk}(x) g_k(x)
\end{equation} where $A_{jk}(x) = A_{k-j}(x-kx_0)$ . That is, we an infinite linear matrix equation: \begin{equation}
\mathbf{f}(x) = \mathbf{A}(x) \mathbf{g}(x)
\end{equation} Now, this looks like something we can invert: \begin{equation}
\mathbf{B}(x) = \mathbf{A}^{-1}(x) \quad\text{such that}\quad
\mathbf{g}(x) = \mathbf{B}(x) \mathbf{f}(x)
\end{equation} where $B_{jk}(x) = B_{j+k}(x-jx_0)$ . However, we have two problems: the matrix elements are functions of $x$ , and the vectors and matrix are of infinite dimension! Is there a practical way to invert an infinite matrix, and would the inversion need to be done separately for each individual value of $x$ ? I get the feeling it might be possible to choose a matrix that is large enough, and it would be sufficient to invert that. Numerical example For the simplest case where $A_n(x) = A_n$ (i.e. constant), then $A_{jk} = A_{k-j}$ . Let's consider the following example: \begin{equation}
A_n = \delta[n+1] + 3\delta[n] + \delta[n-1]
\end{equation} where \begin{equation}
\delta[n]
=
\begin{cases}
1 & \text{for $n=0$}\\
0 & \text{for $n\ne 0$}\\
\end{cases}
\end{equation} That is, \begin{equation}
f(x) = g(x-x_0) + 3g(x) + g(x+x_0)
\end{equation} I obtain matrix $\mathbf{A}$ , but I truncate it to a $201 \times 201$ matrix with the index ranging in $-100 \le n \le 100$ . Then I take the inverse of the matrix, and I extract the row at index $n=0$ , which is $B_{0n} = B_n$ . Using MATLAB to perform this inverse, I get \begin{equation}
B_{n} = \dots + 0.0652\delta[n+2] - 0.1708\delta[n+1] + 0.4472\delta[n]\\ - 0.1708\delta[n-1] + 0.0652\delta[n-2] + \dots
\end{equation} In other words, \begin{equation}
g(x) = \dots + 0.0652f(x + 2x_0) - 0.1708f(x + x_0) + 0.4472f(x)\\ - 0.1708f(x - x_0) + 0.0652f(x - 2x_0) + \dots
\end{equation} This appears to be what I want. Furthermore, it seems to show no discernible change whenever I increase the size of the matrix at truncation. This is good, but I feel like this is still going to be quite clunky for the case where $A_0(x)$ is a function while $A_n$ is constant for $n \ne 0$ . I don't want to have to invert a matrix for every value of $x$ ! Is there a way to extend the above numerical case to this slightly more complex case? Aside Interestingly, if I change the value of $A_0$ from 3 to a different value $m$ , we get some interesting behaviour... For $m = 2.5$ : we see how the $B_n$ coefficients remain large over a larger span of index $n$ . This is particularly noticeable for $m=2.1$ : As long as the coefficients drop off to zero before getting to the ""edge"" of the matrix, I think it is reasonable to assume these coefficients are the inverse that I'm looking for. However, for $m=2$ , the coefficients do not die down sufficiently fast, only reaching zero right at the edge, and we see that the height of the coefficient $B_0$ is on the order of the size of the matrix: If fact, $B_0$ seems to scale in proportion to the size of the matrix, suggesting that the untruncated infinite matrix would cause $B_n$ to blow up to infinity. This suggest certain systems of equations are singular, and cannot be inverted...","['functions', 'inverse']"
4232912,How can I find the uncertainty of derivatives?,"Suppose I have a quadratic (weighted) least-square fit result obtained from a given set of data: $$
f(x) = \underbrace{-0.243(\pm0.3324)}_{\text{quad}_a}x^2\underbrace{{}-0.921(\pm0.061)}_{\text{quad}_b}x \underbrace{{}-2.12(\pm0.0223)}_{\text{quad}_c}
$$ If I'm taking the derivative of $f(x)$ to have $f'(x) = Ax+B$ , I wonder how can I figure out the uncertainties on $A$ and $B$ ? I also have the correlations C(quad_a, quad_c) = -0.422
C(quad_a, quad_b) = -0.278 Thanks!","['statistics', 'confidence-interval', 'regression', 'linear-regression', 'error-propagation']"
4232945,General techniques for showing $G$ doesn't have a subgroup of some order,"While taking a practice qual I realized I didn't know how to show that a simple group of order $168$ has no elements of order $14$ . Thankfully mse does know , and I don't feel bad about not coming up with this idea. It did get me thinking, though. I know a lot of techniques for showing subgroups of certain orders do exist (Sylow/Hall Theorems, etc.) but almost no techniques for showing subgroups of certain orders don't exist. Obviously Lagrange's Theorem is the main obstruction, and even the linked question we take a purported copy of $C_{14}$ in $G$ and build a counterexample to Lagrange. But at least at present the technique in that answer, as well as techniques to similar nonexistence theorems I've found, feel somewhat ad hoc. So my question is Are there general techniques for showing $G$ has no subgroup of order $k$ , even if $k \mid |G|$ ? I'm interested both in ways to extend the reach of Lagrange's Theorem, as well as in other invariants (even if they might be complicated). It looks like this has been asked before , but the answer isn't very informative. Maybe that's because there's no great answer? I hope that isn't the case! Thanks in advance ^_^","['group-theory', 'abstract-algebra', 'finite-groups']"
4232984,Number of solutions of a Diophantine equation,"I'm currently studying about the solutions of the $x_1\pm x_2\pm\cdots\pm x_m=0$ , where the $x_i$ 's are positive integers. They started with a trigonometric identity \begin{equation}
\cos(x_1) \cos(x_2)\cdots\cos(x_m)=\dfrac{1}{2^{m-1}}\sum \cos(x_1\pm x_2\pm\cdots\pm x_m)
\end{equation} where the sum is taken over all possible choices of signs $+$ and $-$ . Later, they arrived to the conclusion that the number of solutions of the equation $x_1\pm x_2\pm\cdots\pm x_m=0$ (all the possible choices of signs) is given by the integral \begin{equation}
\dfrac{2^{m-1}}{2\pi}\int_{-\pi}^{\pi}\cos(x_1t) \cos(x_2t)\cdots\cos(x_mt)dt
\end{equation} which of course can be simplified further (by using the fact that the integrant is an even function) as \begin{equation}
\dfrac{2^{m-1}}{\pi}\int_{0}^{\pi}\cos(x_1t) \cos(x_2t)\cdots\cos(x_mt)dt
\end{equation} How they did arrived to that integral representation? I can't figure out this. Any help given will be appreciated.","['number-theory', 'combinatorics', 'diophantine-equations']"
4232990,"Without loss of generality, what may we validly assume?","So I came across the question where it was asked:
Given that $a,b,c $ are positive integers, in a proof to the theorem $a^3 + b^3 + c^3 \geq a^2b + b^2c + c^2a$ , we may assume without loss of generality that: $a \geq c, b \geq c$ $a \geq b, a \geq c$ $a \geq b, b \geq c$ $a \geq c, c \geq b$ I deduced that the correct answer to the question would be (3) since those are the possible cases and if that's not true, we could reorder $a,b,c$ around. Is my idea of it correct?","['proof-explanation', 'proof-writing', 'discrete-mathematics']"
4232995,Flip a Coin 100 times - Heads then Tails,"If I flip a fair coin 100 times, and each time I get a heads immediately followed by tails, I win $5. How much am I expected to win during the game? I know that the probability of getting a head = probability of getting a tail = $\frac{1}{2}$ , so the probability of a heads then a tails = $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$ . But this is the same probability as any combination of two flips (i.e. two heads, two tails, or tails then a heads). So how can I determine how many of the desired combination would be expected in 100 flips?","['statistics', 'probability-distributions', 'probability']"
4233047,Expressing the $n$–th derivative of $y=\frac{1}{(1+x^2)^2}$,"In an attempt to find a way to express the n-th derivative of $y=\frac{1}{(1+x^2)^2}$ using the binomial theorem I got stuck in the last computation. I'll explain what I did: $$f(x)=\frac{1}{(1+x^2)^2}=\frac{1}{(x+i)^2(x-i)^2}=\\
\frac{i}{4}\left( \frac{1}{x+i} \right)-\frac{1}{4}\left( \frac{1}{(x+i)^2} \right)-\frac{i}{4}\left( \frac{1}{x-i} \right)-\frac{1}{4}\left( \frac{1}{(x-i)^2} \right)$$ Then I used $y=\frac{1}{ax+b} \Rightarrow y_{n}=\frac{(-1)^nn!a^n}{(ax+b)^{n+1}}$ and $y=\frac{1}{(ax+b)^2} \Rightarrow y_{n}=\frac{(-1)^n(n+1)!a^n}{(ax+b)^{n+2}}$ to get: $$\frac{(-1)^n}{4}\left( in!\left( \frac{(x-i)^{n+1}-(x+i)^{n+1}}{(x^2+1)^{n+1}} \right)-(n+1)!\left( \frac{(x-i)^{n+2}+(x+i)^{n+2}}{(x^2+1)^{n+2}} \right) \right)$$ Then I stopped for a second and I analized $(x-i)^{n+1}-(x+i)^{n+1}$ and $(x-i)^{n+2}+(x+i)^{n+2}$ using the binomial theorem: - $(x-i)^{n+1}-(x+i)^{n+1}$ = $$\sum_{k=0}^{n+1}\binom{n+1}{k}x^{n+1-k}\cdot i^k((-1)^k-1)$$ $i^k((-1)^k-1) =0$ for $k=2n$ ,whereas for $k=2n+1$ $\longrightarrow$$-2i(-1)^{\frac{k-1}{2}}$ - $(x-i)^{n+2}+(x+i)^{n+2}$ = $$\sum_{k=0}^{n+2}\binom{n+2}{k}x^{n+2-k}\cdot i^k((-1)^k+1)$$ $i^k((-1)^k+1) =0$ for $k=2n+1$ , whereas for $k=2n$ $\longrightarrow$$2(-1)^{\frac{k}{2}}$ . And now I got stuck sorting out the indeces $n,k$ of summations; what I obtain is: $$\frac{d^n}{dx^n}(f(x))=\\
 \frac{(-1)^nn!}{2(x^2+1)^{n+1}}\cdot \sum_{k=1}^{n+1}\binom{n+1}{k}x^{n+1-k}(-1)^{\frac{k-1}{2}} - \frac{(-1)^n(n+1)!}{2(x^2+1)^{n+2}}\cdot \sum_{k=2}^{n+2}\binom{n+2}{k}x^{n+2-k}(-1)^{\frac{k}{2}}$$ But testing it I can see it’s wrong.
Can anyone help me please sorting the $n,k$ indices and perhaps have a look at my computations also?
Thank you","['complex-analysis', 'binomial-theorem', 'analysis', 'real-analysis']"
4233057,Prove a matrix has non-zero determinant,"While working on a problem, I managed to reduce it to showing that the matrix $A = [a_{ij}]$ , where $2a_{ij} = \frac{1}{i+j-1} - \frac{(-1)^{i+j-1}}{i+j-1}$ , has non-zero determinant, for whatever size you choose. Here are a few cases: $$\det\begin{bmatrix} 1 & 0 & \frac{1}{3} \\ 0 & \frac{1}{3} & 0 \\ \frac{1}{3} & 0 & \frac{1}{5} \end{bmatrix} = \frac{4}{135};$$ $$\det\begin{bmatrix} 1 & 0 & \frac{1}{3} & 0 \\ 0 & \frac{1}{3} & 0 & \frac{1}{5}\\ \frac{1}{3} & 0 & \frac{1}{5} & 0 \\ 0 & \frac{1}{5} & 0 & \frac{1}{7} \end{bmatrix} = \frac{16}{23625}$$ I worked on a few cases using online calculators, but explicit determinant calculation in arbitrary sizes is very cumbersome. It seems to tend to $0$ , but to always be positive. In fact, I suspect this is a positive-definite matrix, but, again, couldn't quite prove it - and I recall having seen it in a computational setting before, maybe numerical integration, though I'm not sure... Anyhow, is there any technique I can use to show this has non-zero determinant for any size I pick? Thanks in advance!","['matrices', 'positive-matrices', 'determinant', 'linear-algebra']"
4233087,"Implicit differentiation , leibniz notation- problem differential operator notation in multivariable calc","here are three examples of what im finding troublesome in understanding, i can workout the basics by myself but when it comes to iterating the differential operator i find the notation and the process convoluted. My biggest problem is i dont know in which logical step im making the mistake and so i dont know what to search so i can clear my confusion. example one change the equation $y'' +\frac{2}{x}y'+y=0 $ by taking "" $x$ as a function and $t=xy$ for a free variable"" my work- i take $\frac{\mathrm dy}{\mathrm dx}=\frac{\frac{\mathrm dy}{\mathrm dt}}{\frac{\mathrm dx}{\mathrm dt}}=\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}$ but my book solution says $\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{1}{x^2}$ and i have no idea why since even if they used the chain rule $\frac{\mathrm d (\frac{t}{x})}{\mathrm dt}$ it would not give the book answer.even if we swapped in $y=\frac{t}{x}$ and then using the chain rule i dont get and extra $t$ as in $\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{t}{x^2}$ example two find $\mathrm d z , \mathrm d^2 z$ given implicit function (1) $\frac{x}{z}=\ln(\frac{z}{y})+1$ so i use the total differential on (1) to get $$\frac{z\mathrm dx-x \mathrm dz}{z^2}=\frac{y}{z}\frac{y \mathrm dz - z \mathrm dy}{y^2}$$ from which by rearranging we can get $\mathrm d z$ $$\mathrm d z=\frac{z(y\mathrm dx+z\mathrm dy)}{y(x+z)}$$ $(x\neq-z)$ by simple algebra manipulation, now we use total derivative on (2) $$ zy\mathrm dx-xy \mathrm dz -yz \mathrm dz-z^2 \mathrm dy=0$$ to get (book answer gives this) $$y(x+z)\mathrm d^2 z=z\mathrm dx \mathrm dy+(z \mathrm dy - x \mathrm dy)\mathrm dz -y^2 \mathrm dz^2$$ and i dont understand this process at all as by going by partial derivatives i dont get the second derivative to look like that(i dont know how i would use chain rule on for example $yz \mathrm dx=z\mathrm dx\mathrm dz+y\mathrm dx\mathrm dz$ ?) and im confused as to what $\mathrm dz^2$ is supposed to mean rigorously at this point. after rearranging and using $\mathrm dz$ the book answer gives $$\mathrm d^2 z=-\frac{z^2(y\mathrm dx-x\mathrm dy)^2}{y^2(x+z)^3}$$ example three transform $y'''=\frac{6y}{x^3}$ if new variable is $t=\ln(|x|)$ so i begin to calculate $y'$ as follows $$\frac{\mathrm dy}{\mathrm dx}=\frac{\mathrm dy}{\mathrm dt}\frac{\mathrm dt}{\mathrm dx}=\frac{1}{x}\frac{\mathrm dy}{\mathrm dt}$$ now to calculate the second derivative by t i do as follows $$\frac{\mathrm d^2y}{\mathrm dx^2}=\frac{1}{x}\frac{\mathrm d}{\mathrm dx}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x}\frac{\mathrm dt}{\mathrm dx}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x^2}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right)$$ and i thought that was it but the book solution gives this for the $\frac{\mathrm d}{\mathrm dt}(\frac{\mathrm dy}{\mathrm dt})=\frac{\mathrm d^2 y}{\mathrm dt^2}-\frac{\mathrm dy}{\mathrm dt}$ and i dont get how we got to this by using the  chain rule given the first derivative like so $\frac{(\mathrm dy)'\mathrm dt-\mathrm dy (dt)'}{(\mathrm dt)^2}$ im not sure what im getting wrong and i think i need a bit of ""hand holding"" so i would beg   please explain where im making the mistake or link me to a book/theory for this. i have no formal knowledge in differential forms or wedge products as im a second year math student.","['differential-operators', 'multivariable-calculus', 'implicit-differentiation', 'derivatives']"
4233103,Proving a complex function is surjective,"Is this a valid proof of surjectivity? $g: \mathbb{C} \rightarrow \mathbb{C}
\\
g(z) = z^2 + z
$ let $
z = a + bi 
\\
w = z^2 + z
\\
w = (a + bi)^2+(a+bi)
\\
Rearranging:
\\
w - a^2 + b^2 - 2abi = a + bi = z
$ It follows: $g(w - a^2 + b^2 - 2abi) = z$ Therefore surjective.","['functions', 'complex-numbers']"
4233165,Derive the Jacobi equation from a specific geodesic variation,"Construction of a specific geodesic variation: Let $\tau = \{\exp_{x_0}(tX):t\in[0,1]\}$ be a geodesic on a complete Riemannian manifold $(M,g)$ , starting from $x_0\in M$ with velocity $X\in T_{x_0} M$ . Let $V$ be a vector attached to $x_0$ that is not parallel to $X$ . Suppose that both $X$ and $V$ have unit length, i.e., $g_{x_0}(X,X) = g_{x_0}(V,V) = 1$ . Now we introduce the geodesic starting from $x_0$ with velocity $V$ by $$y_s := \exp_{x_0}(sV).$$ Denote the parallel transport of $X$ along $\gamma = \{y_s\}$ by $$X(s) := \Gamma(\gamma)_0^s (X).$$ Then we get a family of geodesics starting from $y_s$ with velocity $X(s)$ , $$\tau^s := \{ \exp_{y_s} (tX(s)):t\in[0,1]\},$$ which forms a variation of the geodesic $\tau$ . So the following vector field along $\tau$ should be the Jacobi field , $$J(t) := \frac{\partial}{\partial s}\bigg|_{s=0} \exp_{y_s} (tX(s)),$$ which means \begin{equation}\tag{1}
\frac{D^2}{dt^2} J(t) + R(J(t), \dot \tau(t))\dot \tau(t) =0,
\end{equation} where $D$ denotes the covariant derivative with respect to the Levi-Civita connection, $R$ the Riemann curvature tensor. The question is, how to derive the Jacobi equation (1) from this contruction? I tried a lot but failed. This question acturally arises from this paper at its equation (26). Could anyone figure this out? TIA...","['connections', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
4233167,Slice theorem to give manifold structure,"I'm looking for some reference for the following statement I noticed on wiki: It says that the slice theorem can be used to prove that the orbit space is a manifold when the group is compact and action is free. Where I can find a proof of this? Also, is it necessary that the group is compact? Don't we also need to require the action to be proper (for quotient space to be Haursdorff)? Is the group or the manifold necessarily finite dimensional to make the quotient a manifold? What is the general statement of this one? Any comment is appreciated.","['manifolds', 'group-actions', 'lie-groups', 'differential-geometry']"
4233224,"Solving for the value of $a,b$ in $f(x)=(ax+b)(x^5+1)-(5x+1)$ s.t. $(x^2+1)|f(x)$","$Q.$ If $f(x)=(ax+b)(x^5+1)-5x-1$ is divisible by $x^2+1$ . Then the value of $2a+3b$ $?$ MY APPROACH : We have , $(x^2+1)|f(x)$ then $(x-i)|f(x)$ and $(x+i)|f(x)$ . So by Factor Theorem we have $f(-i)=0$ and $f(i)=0$ $$f(i)=-a+bi+(a-5)i+b-1=0$$ $$f(-i)=-a-bi-(a-5)i+b-1=0$$ By this I concluded that $a-b=-1$ . But this is not enough information to solve the problem .","['functions', 'polynomials']"
4233289,Solving these matrix equations,"Let $A, B$ be two invertible matrices of order $r$ such that $$ABA=BA^2B, \qquad A^3=I, \qquad B^{2n-1}=I \text{ for some positive integer } n$$ I am interested in checking if $A$ and $B$ are commutative. Also if $B$ is idempotent ( $B^2=B$ ) or involutory ( $B^2=I$ ). My approach As $$A^3=I, A^2=A^{-1}$$ $$ABA=BA^2B=BA^{-1}B$$ $$A^{-1}B^{-1}A^{-1}=B^{-1}AB^{-1}$$ Now I am unable to proceed from here. I thought $$B^{-1}A^{-1}=AB^{-1}AB^{-1}$$ But this also doesn't yield any fruitful result. If anyone can share some alternate ways to these type of problems for faster solving or if anyone can spot how to proceed with this, it would be a great help. Thank You.","['matrices', 'inverse']"
4233304,"Given matrix $X$, how to find elementary matrices $E_1$, $E_2$ and $E_3$ such that $X = E_1 E_2 E_3$?","Given $$X = \begin{bmatrix} 0 & 1\\ -2 & -18\end{bmatrix}$$ find elementary matrices $E_1$ , $E_2$ and $E_3$ such that $X = E_1 E_2 E_3$ . My attempt I did 3 row operations from $X$ to get to $I_2$ Swapping row 1 and row 2 Row 1 becomes $-\frac12$ of row 1 Row 1 becomes Row 1 - 9 Row 2 So then $$E_1 = \begin{bmatrix}
0 & 1\\
1 & 0
\end{bmatrix}, \qquad 
E_2 = \begin{bmatrix}
-1/2 & 0\\
0 & 1
\end{bmatrix}, \qquad 
E_3 = \begin{bmatrix}
1 & -9\\
0 & 1
\end{bmatrix}$$ However, when I multiply the $E_1$ , $E_2$ and $E_3$ it doesn't give $X$ . Can someone please tell me where I have made a mistake or if I've approached this question incorrectly?","['matrices', 'gaussian-elimination', 'linear-algebra']"
4233328,Proving $A \cup (A \setminus B) = A$,"I am trying to prove $A \cup (A \setminus B) = A$ . Here is my attempt. \begin{align*}
x \in A \cup (A \setminus B) & \iff x \in A \vee x \in (A \setminus B) \\
& \iff x \in A \vee \left(x \in A \wedge x \not \in B\right) \\
& \iff \left(x \in A \vee x \in A\right) \wedge \left(x \in A \vee x \not \in B\right) \\
& \iff x \in A \wedge \left(x \in A \vee x \not \in B\right) \\
& \iff \left(x \in A \wedge x \in A\right) \vee \left(x \in A \wedge x \not \in B\right) \\
& \iff x \in A \vee \left(x \in A \wedge x \not \in B\right) \\
& \iff x \in A 
\end{align*} I feel that there are some extraneous steps here. How does this look?","['elementary-set-theory', 'solution-verification']"
4233389,How to calculate normals or surfaces in 3D using angles?,"For example in 2D space: It is possible to calculate unit normal, $\vec{n}$ , using $\theta$ and a bit of the unit-circle application: $$\vec{n} = (\sin{\theta}, -\cos{\theta})$$ I want to know is the same rules apply for 3D: If it looks confusing: Plane a has no elevation Plane b is elevated on plane a by $\theta$ $\phi$ is the angle made between the x-axis and Plane a Think about the 2D cartesian graph turning into the 3D cartesian graph above, but now the entire slope is shifted by angle of $\phi$ in the horizontal direction (from the x-axis). Now, to calculate $\vec{n}$ in 3D graph, do the same rules apply as the 2D graph (ie, $$\vec{n} = (\sin{\theta}, -\cos{\theta}, z)$$ , and if so, how would we calculate z? Basically, I want to calculate the normal of plane b using its angle of inclination. Any help is appreciated! EDIT: Continuing on @user 's answer: I know $\angle X$ (Angle made with the y-axis / its rotation around the x-axis) I know $\angle Y$ (Angle made with the x-axis / its rotation around the y-axis) I know $\angle Z = \theta$ (Angle made with the x-axis / its rotation around the z-axis) Is it now possible to calculate $\phi$ ? ... so that I can calculate the normal.","['trigonometry', 'angle', 'vectors', '3d']"
4233492,$\int\frac{dx}{\sqrt{\alpha+\beta x+\gamma x^2}}=\frac{1}{\sqrt{-\gamma}}\arccos\bigl(-\frac{\beta+2\gamma x}{\sqrt{q}}\bigr)$?,"I'm studying Kepler's laws from Classical Mechanics, 2nd ed. Goldstein. In page 95 there is given an indefinite integral $$\int\frac{dx}{\sqrt{\alpha+\beta x+\gamma x^2}}=\frac{1}{\sqrt{-\gamma}}\arccos\biggl(-\frac{\beta+2\gamma x}{\sqrt{q}}\biggr).$$ However, when I took a look the source given in a book (A Short Table of Integrals), there is the result $$\int\frac{dx}{\sqrt{\alpha+\beta x+\gamma x^2}}=-\frac{1}{\sqrt{-\gamma}}\arcsin\biggl(\frac{\beta+2\gamma x}{\sqrt{q}}\biggr).$$ Then, I tried relations of $\arcsin$ and $\arccos$ , so $\arcsin(x)=\frac{\pi}{2}-\arccos(x)$ and also negative argument $-\arcsin(x)=\arcsin(-x)$ , but just ended up to result $$-\frac{1}{\sqrt{-\gamma}}\arcsin\biggl(\frac{\beta+2\gamma x}{\sqrt{q}}\biggr)=-\frac{1}{\sqrt{-\gamma}}\arccos\biggl(-\frac{\beta+2\gamma x}{\sqrt{q}}\biggr)+\frac{\pi}{2\sqrt{-\gamma}}.$$ So is there something I don't see or understand, or is there just a misprint in the book? For the clarification, the result is used to solve this equation: $$\varphi=\varphi_{0}-\int\frac{du}{ \sqrt{\frac{2mE}{l^2}+\frac{2mku}{l^2}-u^2}}$$ and the book ends up to result $$\varphi=\varphi'-\arccos\left(\frac{\frac{l^2u}{mk}-1}{\sqrt{1+\frac{2El^2}{mk^2}}}\right)$$ and by solving the $u=1/r$ we got final result: $$\frac{1}{r}=\frac{mk}{l^2}\left(1+\sqrt{1+\frac{2El^2}{mk^2}}\cdot \cos(\varphi-\varphi')\right)$$","['indefinite-integrals', 'trigonometry']"
4233544,What number is written?,"In each of the 28 blank hexagons in the figure real numbers are written  in such a way
that the number in each inner hexagon is equal to the arithmetic mean of the numbers in the six adjacent ones. Find the value in the central hexagon (where ?). I solved that problem head-on by composing a system of 28 linear algebraic equations and solving it with help of Maple. My answer is $\frac {4986619541155196219}{4026303401170889720}=1.238510625\dots$ (if I am not  mistaken).  This is a training problem from the course of discrete complex analysis which is used to model magnetism and seepage in porous media. The comments to the problem hint that there is another solution that does not require cumbersome calculations.",['complex-analysis']
4233609,Show that $f:\mathbb R^n\to \mathbb R$ defined as $f(x)=\inf_{y\in K}||x-y||$ continuous on $\mathbb R^n$,"A question from my analysis course reads Let $K$ be a compact subset of $\mathbb R^n$ . Define $f:\mathbb R^n\to \mathbb R$ as $$f(x)=\text{d}(x,K)=\inf_{y\in K}||x-y||$$ Show that $f$ is continuous on $\mathbb R^n$ and $f^{-1}\left(\{0\}\right)=K$ . Is the compactness of $K$ necessary? Now, I can very well understand that the described function has to be continuous because $f(x)=0$ when $x\in K$ and as $x$ slowly moves out of $K$ , the distance increases continuously. But, of course, that's quite far away from a real mathematical proof, and as far as I understand, we need to use the property If $f:\mathbb A\to \mathbb B$ is continuous and $O\subset B$ is an open set, then $f^{-1}\left(O\right)$ is open in $\mathbb A$ to arrive at the formal proof. But, I'm quite new to functions whose domain is not $\mathbb R$ . Until now, I have only studied functions $f:\mathbb R\to \mathbb R$ . So, I'm a little confused about how to construct the proofs. Please help me to do it. I was asked in the comments whether I'm allowed to use the $\epsilon-\delta$ property to prove continuity. Well, it wasn't explicitly mentioned in the exercise that I need to use the preimage property, but this question came just a couple of pages after this property was introduced. That's why I assumed they expect me to use that technique. Although I will also appreciate a detailed $\epsilon-\delta$ proof, it would be better if I can have a proof using the preimage method .","['analysis', 'real-analysis', 'continuity', 'calculus', 'compactness']"
4233635,The number of monotonic Boolean functions of $n$ variables,"Our professor gave us this task: let $T_0^{(n)},T_1^{(n)},S^{(n)},L^{(n)},M^{(n)}$ be the sets of boolean functions $\{0,1\}^n\rightarrow\{0,1\}$ which preserve $0$ , preserve $1$ , are self-dual, are linear and are monotonous, respectively. Find $|T_0^{(n)}|,|T_1^{(n)}|,|S^{(n)}|,|L^{(n)}|,|M^{(n)}|.$ It was easy to calculate $|T_0^{(n)}|=2^{{2^n}-1}=|T_1^{(n)}|$ and $|L^{(n)}|=2^{n+1}$ using algebraic normal forms. $|S^{(n)}|=2^{2^{n-1}}$ was not difficult to compute. But what about $|M^{(n)}|$ ? I see that we shall find the number of free distributive lattices on $n$ generators but do not know, how to do this. I've counted that for $n=0,1,2,$ and $3$ $|M^{(n)}|$ equals $2,3,6$ and $20$ , respectively. Any help is appreciated! :-)","['lattice-orders', 'logic', 'monotone-functions', 'discrete-mathematics']"
