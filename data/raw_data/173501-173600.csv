question_id,title,body,tags
3090855,How to find limit $\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2}$,How can I find limit $$\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2}$$ I solve subproblem: $$\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h} = \lim_{h\rightarrow0^-}\frac{1}{e^{1/|h|}\cdot h} =\lim_{y\rightarrow -\infty}\frac{y}{e^{|y|}}=0$$ but I have no idea how to apply that for main target,"['limits', 'limits-without-lhopital']"
3090891,Let $F \subset \mathbb{R}^n$ be a nonempty countable or finite closed set. Prove that $F$ must have an isolated point.,"I tried to think of the problem using contradiction that if $F$ does not have any isolated point (that is, $F$ contains only limit points) then $F$ is uncountable. But then I was stuck into how to prove that $F$ is uncountable. Can anyone provide a method to proving this proposition?","['analysis', 'real-analysis']"
3090897,Why does PageRank use 0.85 as a standard damping factor?,"I understand that for a low damping factor, some nodes will rarely be reached and a high damping factor will slow down the algorithm and cause the random walk to be stuck in 'sinks', and as such a middle ground is preferable. However, is there a precise reason for a 0.85 factor ? Does 0.8, 0.9, or 0.75 work just as well ?","['page-rank', 'linear-algebra', 'probability']"
3090912,Is there a well known geometrical interpretation of the power reducing formula for $\sin^2 \theta?$,"The wikipedia provides geometrical interpretations for the angle addition identities , but not for the power reducing identities. My students and I have been playing around with them and arrived at a geometric interpretation of the power reduction for $\sin^2\theta$ that we've never seen before. What would be a good place to search to find out whether our geometric interpretation is novel? Essentially the argument goes like this: Draw two congruent right triangles such that their right angles are coincident, their smaller angle $\theta$ and their hypotenusae $1$ . Construct a square, $A$ whose side length is $\sin\theta$ and another square $B$ whose side length is $\cos\theta$ . The gnomon inside $B$ but not inside $A$ has area $\cos2\theta$ by the angle addition identity. Draw a square $C$ of side length $1$ circumscribing $B$ . The area inside $C$ but not inside $B$ has area $\sin^2\theta$ by the Pythagorean identity. Thus, $\sin^2\theta + \sin^2\theta + \cos2\theta$ must equal $1$ . Is that a well known argument?","['trigonometry', 'geometry']"
3090916,Evaluate $\int_{0}^{1}\frac{1+x+x^2}{1+x+x^2+x^3+x^4}dx$,Evaluate $$I=\int_{0}^{1}\frac{(1+x+x^2)}{1+x+x^2+x^3+x^4}dx$$ My try: We have: $$1+x+x^2=\frac{1-x^3}{1-x}$$ $$1+x+x^2+x^3+x^4=\frac{1-x^5}{1-x}$$ So we get: $$I=\int_{0}^{1}\frac{1-x^3}{1-x^5}dx$$ $$I=1+\int_{0}^{1}\frac{x^3(x^2-1)}{x^5-1}dx$$ Any idea from here?,"['integration', 'definite-integrals']"
3090920,What does it really mean for a wave equation to be critical?,"I am trying to understand intuitively the concept of criticality in general for Wave equations.
For example, consider the cauchy problem of semi-linear equation \begin{equation}
\begin{cases}
\phi_{tt}+\Delta\phi +|\phi|^{p-1}\phi=0; \quad x\in \mathbb{R}^n\\
\phi(x,0)=f(x)\in H^1(\mathbb{R}^n), \quad \phi_t(x,0)=g(x) \in L^2(\mathbb{R}^n)
\end{cases}
   \end{equation} I am trying to understanding what does it mean for the exponent $p$ defined in the equation above to be critical exponent, thereby making it critical. I understand that $p$ must satisfy certain condition for it to be critical (subcritical or even supercritical) at the level of certain Sobolev space $H^s$ say, if the Cauchy problem above stays invariant under some scalling transformation. And that does not mean criticality for blow-up, but rather for minimal smoothness condition for the existence (local) of solution. My questions: (a) Why do we need to study criticality of certain wave equation? (b) What are the differences between the criticality for blow-up and that of the equation. (c) What condition must $p$ satisfy to be considered as a critical exponent at least for blow-up. Any hint that clear the way to understand the difference is highly welcome.","['dispersive-pde', 'functional-analysis', 'wave-equation', 'partial-differential-equations', 'regularity-theory-of-pdes']"
3090927,Why is Convolution Well-Defined (Simple Example),"I am having trouble understanding why convolution is well-defined. Let's take a simple example: $(\Omega, \mathcal{F}, P)$ probability space and $X_{1}, X_{2}$ two real random variables where $P(X_{1}=3)=\frac{1}{2},P(X_{2}=2)=\frac{1}{4}$ And $P(X_{1}=1)=\frac{1}{5},P(X_{2}=4)=\frac{1}{3}$ Then my understanding of convolution is $P_{X_{1}+X_{2}}\circ A^{-1}$ where $A: X_{1} \times X_{2}\to \mathbb R,A(x_{1},x_{2})=x_{1}+x_{2}$ So surely, if, for instance $X_{1}+X_{2}=5$ , I get more than one preimage, and hence how can convolution be well-defined? In the above case, I would get: $P_{X_{1}+X_{2}}\circ A^{-1}(5)=P_{X_{1}}(3)P_{X_{2}}(2)=\frac{1}{2}\times\frac{1}{4}=\frac{1}{8}$ while $P_{X_{1}+X_{2}}\circ A^{-1}(5)=P_{X_{1}}(1)P_{X_{2}}(4)=\frac{1}{5}\times\frac{1}{3}=\frac{1}{15}$ I do not know where I am going wrong in my understanding of convolution. Any help is greatly appreciated.","['convolution', 'probability-theory', 'probability', 'random-variables']"
3090985,Cauchy a.e. implies existence of a measurable function to which this sequence converges a.e,"I have seen a similar exercise in Royden (if I recall correctly), but the statement included Cauchy convergence in measure. In this case, there are measurable functions $f_n:X\rightarrow\mathbb{R}$ such that $\{f_n\}$ is Cauchy a.e. I need to prove that there exists a measurable function $f$ for which $f_n\to f$ a.e. Every Cauchy sequence has a finite limit then, intuitively, it seems that we can find a function $f$ which is finite-valued so that $f_n \to f$ a.e. I can't, however, justify this in a rigorous way. Can you provide a hint, please?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3091075,Show compactness of an operator,"Let $T: C^0[0,1] \rightarrow l^1$ , $(Tf)_n=a_n \int_0^{1/n} f(x)dx$ , for an $f$ in $C^0[0,1]$ . Prove that $T$ is compact when $\{\frac{a_n}{n} \}_n \in l^1$ . I know the definition of compact operator, but in this case I'd like to state that if $\{ \frac{a_n}{n} \} \in l^1$ , then $T$ is a finite rank operator. Indeed, $||(Tf)_n||_{l^1} = \sum_n |\frac{a_n}{n} \int_0^1 f(\frac{t}{n})dt|$ . But $f$ is continuous in $[0,1]$ , then this integral is finite, and, particularly, it's bounded from $||f||_{\infty}$ , thus the last sum is less or equal than $||f ||_{\infty} \sum_n |\frac{a_n}{n}|$ , thus it's convergent, and then $T$ has finite rank. Is it okay? EDIT Set $T_m(f)=(a_1 \int_0^1f(x)dx,\ldots,a_m \int_0^{1/m}f(x)dx, 0, 0, \ldots)$ . I want to show that $|| T_m-T|| \rightarrow _m 0$ in the operator norm. I have to compute $\sup \{ ||T_m(f) - T(f)||_{l^1}: ||f||_{\infty} \leq 1 \}=\sup \{ \sum_{k=m+1} |a_k \int_0^\frac{1}{k}f(x)dx|: ||f||_{\infty} \leq 1 \}$ Now, $\sum_{k=m+1} |a_k \int_0^\frac{1}{k}f(x)dx| \leq \sum_{k=1}^{\infty} \frac|{a_k}{k} \int_0^1 f(\frac{t}{n})dt| \leq ||f||_{\infty} \sum_k |\frac{a_k}{k}|$ . Thus, by taking the supremum I have that $||T-T_m|| = \sum_{k=m+1}^\infty |\frac{a_k}{k}|$ . But for $\{ \frac{a_k}{k} \} \in l^1$ , this is the remainder of a convergent series, and then the limit over $m$ goes to $0$ . So $T$ is compact, since it's limit of compact (they're finite rank) operators","['operator-theory', 'compact-operators', 'functional-analysis']"
3091098,Higher moments of linear regression residuals?,"Background In the following linear regression with i.i.d $\epsilon_i$ $(i = 1, \cdots, n)$ with mean 0 finite variance $\sigma^2$ , \begin{align*}
Y_i = X_i^\intercal\beta + \epsilon_i
\end{align*} we know the least-squares estimator for $\beta$ is \begin{align*}
\hat{\beta}=(X^\intercal X)^{-1}X^\intercal Y
\end{align*} where $X = (X_1, \cdots, X_n)^\intercal \in \mathbb{R}^{n\times p}$ , $Y = (Y_1,\cdots, Y_n)^\intercal \in \mathbb{R}^n$ . Now, we can easily derive the variance of $\hat{\epsilon} = Y - X\hat{\beta}$ with \begin{align*}
\text{Var}(\hat{\epsilon}) &= \text{Var}((I-H)Y) \\
&\overset{\heartsuit}{=} (I-H)\text{Var}(Y)(I-H)^\intercal \\
&= \sigma^2(I-H)(I-H)^\intercal \\
&\overset{\spadesuit}{=} \sigma^2(I-H)
\end{align*} where $H = X(X^\intercal X)^{-1}X^\intercal$ is a projection matrix of rank $p$ , thus justifying equality $(\spadesuit)$ . Hence, this allows us to find an unbiased estimator for $\sigma^2$ , since \begin{align*}
\text{Trace}(\text{Var}(\hat{\epsilon})) = \sigma^2\text{Trace}(I-H) = \sigma^2(n-p) \implies \widehat{\sigma^2}=(n-p)^{-1}\|Y - \hat{Y}\|_2^2
\end{align*} An attempt for higher moments How would we generalize this computation in coming up with unbiased estimators for the third centralized moments \begin{align*}
\mu_3 \overset{\text{def}}{=} \mathbb{E}\epsilon^3_i 
\end{align*} or beyond? Here's my attempt. Define \begin{align*}
\mathcal{S}(\hat{\epsilon}) = \mathbb{E}(\hat{\epsilon}_i - \mathbb{E}\hat{\epsilon}_i)^{\otimes 3}
\end{align*} I don't understand tensor products quite well, but I'm guessing \begin{align*}
\mathcal{S}(\hat{\epsilon}) = (I-H) \underset{(I-H)}{\mathcal{S}(Y)}(I-H)^\intercal = \mu_3(I-H) \underset{(I-H)}{I_{n\times n \times n}}(I-H)^\intercal
\end{align*} in the same way as equality $(\heartsuit)$ under ""Background"".  I included the additional underset $(I-H)$ to elicit the idea that we are now ""matrix"" multiplying along the additional dimension induced from the 3-dimensional tensor. But, I don't quite understand what it means to multiply in this direction. Am I on the right track? Are there other matrix tricks I could use? Edit: I found that that tensor product is simply a 3-mode product, resulting in \begin{align*}
[\mathcal{S}(\hat{\epsilon})]_{ijk} = \mu_3 \sum_{v=1}^{n}M_{iv}M_{jv}M_{kv}
\end{align*} where $M = I - H$ is still a projection matrix. How to proceed from here?","['statistics', 'projection-matrices', 'linear-algebra', 'tensor-products']"
3091106,$p$-adic-valuation of an expression involving Bernoulli numbers,"Let $p = 43, 67$ or $163$ (three primes such that $h(\mathbb{Q}(\sqrt{-p})) = 1$ ) and consider $k = (p+1)/2$ .  I'm interested in computing the $p$ -adic valuation of the expression \begin{equation}
1+\frac{l}{B_l},
\end{equation} where $l = k + m(p-1),~m\geq 0$ . Note that, by the Kummer congruences and the fact that $h(-p) \equiv -2B_k~(\mathrm{mod}~p)$ , we have \begin{equation}
v_p(1+l/B_l)\geq 1.
\end{equation} In fact, I have observed by numerical computations that this inequality is an equality for many value of $m$ . I have also observed that if $m$ is such that $2m+1 = dp$ for an integer $d$ , then the valuation of $1+l/B_l$ seems to be equal to $2$ . These observations seem very mysterious for me, so I was wondering if anybody had an idea to explain this? Thanks !","['number-theory', 'p-adic-number-theory', 'bernoulli-numbers']"
3091108,"""Milk"" the integral $\int_0^\infty\left(\frac{x^2}{x^4+2ax^2+1}\right)^r\frac{x^2+1}{x^2(x^s+1)}\mathrm dx$","I found the following integral in chapter $13$ of Irresistible Integrals , and I would like to see which conclusions you can reach from it. My goal in asking this question is to see which methods I can employ in the future to generalize/""milk"" cool integrals like this. I admit this post is very similar to the original ""Integral Miking"" post, but since this post is concerning a specific integral, it is not a duplicate. \begin{align}
\int_0^\infty\left(\frac{x^2}{x^4+2ax^2+1}\right)^r\frac{x^2+1}{x^2(x^s+1)}\mathrm  dx&= \int_0^1\left(\frac{x^2}{x^4+2ax^2+1}\right)^r\frac{x^2+1}{x^2}\mathrm dx\\
&=\frac12\int_0^\infty\left(\frac{x^2}{x^4+2ax^2+1}\right)^r\frac{x^2+1}{x^2}\mathrm dx\\
&=\int_0^\infty\left(\frac{x^2}{x^4+2ax^2+1}\right)^r\mathrm dx\\
&=\sqrt{\frac{\pi(a+1)}{2}}\frac{\Gamma(r-\frac12)}{(2a+2)^r\Gamma(r)}
,\end{align} Which works for $r>\frac12$ and all(?) $s$ , because as the authors showed, the integral is independent of $s$ . This question wouldn't be complete without my attempts: Setting $a=1$ , we have $$\int_0^\infty\left(\frac{x}{x^2+1}\right)^{2r}\mathrm dx=\frac{\sqrt{\pi}\,\Gamma(r-\frac12)}{2^{2r}\Gamma(r)}.$$ Taking $\frac{d}{dr}$ on both sides, $$\int_0^\infty\left(\frac{x}{x^2+1}\right)^{2r}\log\left(\frac{x}{x^2+1}\right)\mathrm dx=\frac{\sqrt{\pi}}{2}\frac{d}{dr}\frac{\Gamma(r-\frac12)}{2^{2r}\Gamma(r)}.$$ And it can be shown, in a somewhat similar way, that $$\int_0^\infty\left(\frac{x}{x^2+1}\right)^{2r}\log^n\left[\frac{x}{x^2+1}\right]\frac{\mathrm dx}{(x^2+1)^2}=\frac{\sqrt\pi}{2^{n+4}}\left(\frac{d}{dr}\right)^n\frac{\Gamma(r+\frac32)}{4^r\Gamma(r+2)}.$$ Unfortunately, I feel as if my creative well has run dry, and I would like to see what you can get from this integral. Have fun! Edit: Context The authors of Irresistible Integrals called this integral a ""Master Formula"" because it apparently could produce a plethora of identities. I would like to see which identities you can derive from said integral.","['integration', 'big-list', 'soft-question']"
3091126,"Is there a theory of ""almost symmetry"" generalizing group theory?","Apologies for the inescapably soft question. Does there exist a theory that aims to develop tools analogous to those of group theory, except for the study of objects that are merely almost symmetrical? I am not sure what exactly it would look like, and I haven't been able to find what I'm looking for via internet searches. Here are the types of objects of study I have in mind: A convex polyhedron that is a slight perturbation of a regular icosahedron (perhaps I realized the icosahedron as the intersection of half-spaces, and moved one of the half-spaces a tiny bit) is not $A_5$ -symmetrical, but e.g. there will be an action of $A_5$ on the ambient space such that all the images of my almost-regular icosahedron will be contained in a thin tubular neighborhood of its original position. If I know my object has this type of ""almost symmetry"" then I know something about it constraining its geometry, even though it is not actually symmetrical. Consider a rooted tree consisting of a root with 10 branches, each of which has 100 leaves, except one which has 101. If all the branches had 100 leaves, then the tree would have an automorphism group isomorphic to $S_{100}\wr S_{10}$ , acting transitively on the leaves. As it is, the automorphism group is $(S_{100}\wr S_9) \times S_{101}$ , with 900 leaves falling into one orbit and 101 falling into the other. There's no room in these considerations for the fact that 100 and 101 are close-together numbers. In some contexts, this is clearly as it should be, but in others, one might want a more permissive way to capture the almost-symmetrical nature of the graph. For example, if the tree is the setting for some sort of game, and the players begin the game on the leaves, and move about the tree as the game progresses, then an automorphism group transitive on the leaves expresses a kind of ""fairness"" with respect to the initial positions of the players. In this context, depending on the rules of the game, it might be that the game in which one of the branches has 101 instead of 100 leaves is ""still fair enough"", and one would then want a more permissive description that allows to capture this ""near equality of initial states"" than is captured by the actual graph automorphism group.","['group-theory', 'symmetry', 'soft-question', 'approximation-theory']"
3091133,How to prove $\mathbb{R}^n < \mathbb{N}^\mathbb{R}$,"I know that the cardinality of $\mathbb{R}^n$ is equal to $\mathbb{R}$ . I also know that the cardinality of $\mathbb{N}^n$ is equal to $\mathbb{N}$ , but how do I prove that the cardinality of $\mathbb{R}^\mathbb{N}$ is smaller than the cardinality of $\mathbb{N}^\mathbb{R}$ .","['elementary-set-theory', 'cardinals']"
3091152,How can you solve and equation with inverse functions?,$$\arctan(x) - \arctan(2/x) = \arctan(7/9)$$ where $x$ is positive . The answer should be 3. Thanks,['trigonometry']
3091153,Closed form expression for infinite series,"I was given the following function: $$ f(x) =  x + \frac{2x^3}{1\cdot3} + \frac{2\cdot4x^5}{1\cdot3\cdot5} + \frac{2\cdot4\cdot6x^7}{1\cdot3\cdot5\cdot7}...   $$ $$ \forall x    \in [0,1) $$ And then I was asked to find the value of $ f(\frac{1}{\sqrt{2}}) $ , which obviously requires me to compute the closed form expression of the infinite series. I tried 'Integration as a limit of sum' but I was unable to modify the expression accordingly. How do I approach the problem?","['calculus', 'sequences-and-series']"
3091162,Sufficient conditions on the isomorphism of two groups,"Let $G_1, G_2$ be two groups with at least one nontrivial proper subgroup each. Let $S_1, S_2$ be the sets of proper subgroups of, respectively $G_1, G_2$ . Suppose there exists a bijective function $f: S_1 \rightarrow S_2$ such that $\forall A\in S_1, f(A)$ is isomorphic to $A$ . When can I conclude that $G_1, G_2$ are isomorphic? I think that, if $G_1$ and $G_2$ are finite and abelian we can conclude that they are isomorphic, but I can't prove It.
Moreover, I haven't found any counterexample for nonabelian finite groups.",['group-theory']
3091176,Finding an $\epsilon-\delta$ proof for a multivariable limit.,"Suppose $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is defined as $$(x,y) \longmapsto
\left\{
\begin{array}{cl}
      \dfrac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2} & \mbox {if } (x,y) \neq (0,0) \\
\\
      0 & \mbox {if } (x,y) = (0,0)
   \end{array}
\right.
 $$ I have shown that $f$ is continuous at $(0,0)$ using polar coordinates, but I am really trying to improve my $\epsilon-\delta$ proofs for such limits. I always end up getting confused with the inequalities. (Sorry if the question is a bit repetitive here) So for $\epsilon > 0$ I need to find a $\delta$ such that $\|(x,y)\| =\sqrt{x^2 + y^2} < \delta$ implies $|f(x,y)| < \epsilon$ I have tried to work backwards using the inequality $(x^2+y^2)^2 \geq 4x^2y^2$ $$\begin{align*}
\left|\frac{4x^2y^3 +x^4y - y^5}{(x^2+y^2)^2}\right|&=\left|\frac{4x^2y^3}{(x^2+y^2)^2} + \frac{x^4 y}{(x^2+y^2)^2} - \frac{y^5}{(x^2+y^2)^2}\right|\\
&\leq\left|\frac{4x^2y^3}{(x^2+y^2)^2}\right| + \left|\frac{x^4 y}{(x^2+y^2)^2}\right| + \left|\frac{y^5}{(x^2+y^2)^2}\right|\\
&\leq\left|\frac{4x^2y^3}{4x^2y^2}\right| + \left|\frac{x^4 y}{4x^2y^2}\right| + \left|\frac{y^5}{4x^2y^2}\right|\\
&= \left|y\right| + \left|\frac{x^2}{4y}\right| + \left|\frac{y^3}{4x^2}\right|
\end{align*}$$ Got stuck here and not sure if my approach is correct.","['limits', 'multivariable-calculus', 'epsilon-delta', 'real-analysis']"
3091195,Show that $(y_n)\in l^q$,"The problem I'm trying to solve is: Let $1\leq p, q\leq\infty$ such that $\frac{1}{q}+\frac{1}{p}=1$ . If $$\sum_{n=1}^{\infty} |x_n||y_n|<\infty$$ for all $(x_n)\in l^p$ , then $(y_n)\in l^q$ . $\bullet$ If $p=\infty$ , then picking $(x_n)=(1,1,1,...)\in l^\infty$ , we have $$\sum_{n=1}^{\infty} |y_n|<\infty$$ i.e. $(y_n)\in l^1$ . but how can I prove for $1\leq p<\infty$ ?","['lp-spaces', 'functional-analysis']"
3091197,Show that a certain set of invertible matrices is a normal subgroup of another set of invertible matrices (triangular),"$U_n=$$\left( \begin{array}{rrrr}
1 & * & \cdots & * \\
0 & \ddots & * & \vdots \\
\vdots & 0 & \ddots & * \\
0 & \cdots & 0 & 1 \\
\end{array}\right) $$T_n=$$\left( \begin{array}{rrrr}
* & * & \cdots & * \\
0 & \ddots & * & \vdots \\
\vdots & 0 & \ddots & * \\
0 & \cdots & 0 & * \\
\end{array}\right) $ I have to Show that $U_n$ is a normal subgroup of $T_n$ To this end I have to show (after I have verified that $U_n$ is a subgroup of $T_n$ ): $$\forall_{x\in T_n}\forall_{a\in U_n}\exists_{b\in U_n}xa=ba$$ I have looked at the case n=2 the results are $\begin{pmatrix}
a & b \\
0 & c \\
\end{pmatrix}$ $\begin{pmatrix}
1 & d \\
0 & 1 \\
\end{pmatrix}$ $=\begin{pmatrix}
a & ad+b \\
0 & c \\
\end{pmatrix}$ $=\begin{pmatrix}
1 & adc^{-1} \\
0 & 1 \\
\end{pmatrix}$ $\begin{pmatrix}
a & b \\
0 & c \\
\end{pmatrix}$ Now the Question is how can I choose the coefficients for my Matrix $b$ in Dependance to $a$ ? I suspect that the coefficients of $b$ can be determined this way (Note that I use the notation $b_{kj}$ which translates to: I look at the coefficient in the $k$ -th row at the $j$ -th column of the Matrix $b$ ) $$b_{kj}\begin{cases}1 &\mbox{if } j=k \\ 0 & \mbox{if } j>k\\a_{(k-1)j}a_{kj}a_{k(j+1)} & \mbox{otherwise}  \end{cases}$$ How can I prove this idea?","['matrices', 'proof-writing', 'linear-algebra']"
3091251,How can I calculate the probability of two independent events with only the union and the intersection?,"Suppose two events A and B are two independent events with $P(A) > P(B)$ and $P(A ∪ B) = 0.626$ and $P(A ∩ B) = 0.144$ , determine the values of P(A) and P(B). So far I have: $$P(A ∩ B) = P(A) + P(B) - P(A U B)$$ $$0.144 = P(A) + P(B) - 0.626$$","['statistics', 'probability']"
3091270,Where did the mean estimator come from?,"What was the motivation behind the definition of the mean estimator  : $$\hat{\mu}=\frac{1}{N}\sum_{i=1}^N X_{i}$$ Did we come up with this very form through trial and error ? I do know that it's unbiased but aren't there other estimators that are also unbiased , so why did we favor this particular one ?","['statistics', 'math-history', 'probability']"
3091364,differential equation of the explicit RMS function,"This is my first time posting on any math forum, let alone stackexchange, so I do hope I'm doing everything correct! Some Background I'm an engineer, and not a mathematician, although I do enjoy maths which is the reason for this question. In my own time I've been looking at RMS - DC converters within the field of electronics, essentially they take an input of an AC signal, and output a DC voltage. The signal should, ideally, have a mean equal to the RMS of the input signal. For example, a 1Vrms sine wave input should produce a DC voltage of 1V. Due to the nature of the conversion, the output is a signal with a small amount of AC content (ripple) and also a DC offset (due to attenuation, resulting from a low pass filter used to implement the moving average). I've been attempting to derive this error mathematically - which is certainly nothing new. However, it seems i'm struggling with the maths and I hope someone here can help! The Problem Determine the error from an RMS-DC converter. There is a paper available on IEEE Xplore (which I have and can upload here if that is allowed?) that details one method of determining this error. There are two methods of determining RMS, implicit and explicit. The explicit is easier for me to understand so I hope someone can help me with the following: $V_{RMS}(t) = \sqrt{AVG(V^2_{in}(t))}$ In electronics, the average is done with an operational amplifier, which gives: $V_{RMS}(t) = \sqrt{-\frac{1}{\tau}\int_{0}^{t}V_{in}(t)^2\delta t}$ The minus sign is due to the practical implementation of the operational amplifier being setup in an inverting operation (input signal is entering the inverting pin (-) on the operational amplifier) Conventionally, $V_{RMS}()$ is used as the signal, representing voltage. However, the paper uses $e_o(t)$ , presumably representing the error, so I will continue with their nomenclature from now on: $V_{RMS}(t) == e_o(t)$ and $V_{in}(t) == e_i(t)$ The paper shows an example circuit: Example circuit To which it goes on to say that ""it is easy to show that the differential equation for $e_o^2(t)$ can be written as..."" (unfortunately, not so easy for me!): $\tau \frac{\delta e_o^2(t)}{\delta t} + e_o^2(t) = e_i^2(t)$ The paper continues to derive the DC error (through, as I understand, solving the differential equation, and then using some series expansion, and taking the DC term). however, I fail at the first hurdle and can't seem to get the differential equation. My Workings So Far From the definition of RMS, I derived what I thought was the differential equation by (I believe it's called) implicit differentiation as follows: $e_{o}(t) = \sqrt{- \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t}$ Where $t$ is the time-varying quantity, and $\tau$ is the Resistor-Capacitor time constant, introduced by the R and C in the circuit. Squaring both sides: $e^2_{o}(t) = - \frac{1}{\tau}\int^t_0{e_i^2(t)}\delta t$ and then (implicit?) differentiation: $\frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} \frac{\delta}{\delta t}[\int^t_0{e_i^2(t)}\delta t] $ giving: $\frac{\delta}{\delta t} e^2_{o}(t) = - \frac{1}{\tau} {e_i^2(t)} $ $\tau \frac{\delta e^2_{o}(t)}{\delta t} = - e_i^2(t) $ Which, of course is not the same as the author in the paper. Can anyone see where I have gone wrong? I understand this is quite a simple question and probably didn't need so much detail, but I thought I would include just in case it's an assumption that's causing the wrong answer. Thanks in advance!","['integration', 'definite-integrals', 'derivatives', 'ordinary-differential-equations']"
3091383,Show that the cardinal of $ A := \left\{ k \in \mathbf{Z} | 0 \leq k \leq n \text{ and } \binom{n}{k} \text{ is odd} \right\} $ is a power of 2 [duplicate],"This question already has answers here : Number of odd binomial coefficients is a power of $2$ (2 answers) Closed 5 years ago . Let $ A := \left\{ k \in \mathbf{Z} | 0 \leq k \leq n \text{ and } \binom{n}{k} \text{ is odd} \right\}  $ . I must show that the cardinal of $A$ is a power of 2. I have tried to show that there exist a bijection between $A$ and the set of subparts of another set, but unsuccessfully. I also thought about trying to show that the cardinal of $A$ must divide the cardinal of $ P(\left\{ k \in \mathbf{Z} | 0 \leq k \leq n \right\}) $ (it is $ 2^{n+1} $ ), which would ensure the result, but I do not think this a good path. Are there simple arguments to show that ?","['cardinals', 'binomial-coefficients', 'combinatorics']"
3091386,integration by substitution of multiple variables,"I have an integral \begin{equation}
\int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x}
\end{equation} where $f: \mathbb{R}^m \rightarrow \mathbb{R}$ and $\mathbf{B}\in\mathbb{R}^{m\times n}$ . I also know \begin{equation}
\int_{\mathbb{R}^m}f(\mathbf{u})\mathrm{d}\mathbf{u}.
\end{equation} For $n=m$ , we have \begin{equation}
\int_{\mathbb{R}^n}f(\mathbf{B}\mathbf{x})\mathrm{d}\mathbf{x} = \frac{1}{\det\mathbf{B}} \int_{\mathbb{R}^n}f(\mathbf{u})\mathrm{d}\mathbf{u}. 
\end{equation} What do I do for $n\neq m$ ?","['integration', 'multivariable-calculus', 'calculus', 'substitution']"
3091423,Singular value inequality for sum of 2 matrices,"I found a theorem mentioned in a couple of places, but could not find a proof. 
The theorem states the following: Let $A, B \in \mathbb{F^{m,n}}$ , $p=min(m,n)$ with singular values $\sigma_1(A) \geqslant...\geqslant \sigma_p(A)$ and $\sigma_i(B) \geqslant...\geqslant \sigma_p(B)$ respectively,
then $\sigma_{i+j-1}(A+B) \leqslant \sigma_i(A) + \sigma_j(B)$ . I am looking for a proof of the above. Thanks in advance.","['inequality', 'linear-algebra', 'singular-values']"
3091545,$f_n$ converge uniformly to $f$ then $\mathrm{d}f_n(x_n)$ converges to $\mathrm{d}f(x)$,"Let $f_n : \mathbb{R}^p \to \mathbb{R}$ such that the $f_n$ are $C^1$ and such that the sequence $(f_n)_{n \in \mathbb{N}}$ converges uniformly to a function $f : \mathbb{R}^p \to \mathbb{R}$ which is $C^1$ . Then prove that for all $x \in \mathbb{R}^n$ there is a sequence $(x_n)_{n \in \mathbb{N}}$ which converge to $x$ such that $\mathrm{d}f_n(x_n)$ converges to $\mathrm{d}f(x)$ . I must say that I don't know at all how to do and don't have any intuition of what is really going on here. So we might look at the case qhere $p= 1$ . So we can write : $$f(a+h) = f(a)+ f'(a)h +o(h)$$ $$\forall n \in \mathbb{N}, f_n(a+h) = f_n(a) + f'_n(a)h +o(h)$$ Hence we have : $$\mid f'(a)h - f'_n(a)h \mid \leq \mid f(a+h)-f(a) \mid +\mid f(a)-f_n(a) \mid + \mid o(h) \mid$$ Since the function $f_n$ converge uniformly to $f$ , we have : $$\mid f'(a)h - f'_{\infty}(a) \mid \leq \mid o(h) \mid$$ And now using we let $h \to 0$ so that : $$\mid f'(a) -f_\infty'(a) \mid = 0 $$ I don't know if this works, but it feels strange to me since in the case the sequence $x_n$ is just the constant sequence... and moreover if this is correct I don't see at all how to generalise to higher dimensions. Thank you !","['real-analysis', 'multivariable-calculus', 'calculus', 'uniform-convergence', 'sequences-and-series']"
3091563,give 5 other equivalent iterated triple integrals,"I am given the following integral: $$\int_0^2\int_0^{y^3}\int_0^{y^2}f(x,y,z) dzdxdy $$ I was successfully able to rewrite this in its dzdydx, dxdzdy, and dxdydz forms, but I'm having a hard time understanding the iterated triple integrals where dy is the inner integral. The solution for dydzdx is as follows: $$\int_0^8\int_0^{x^{2/3}}\int_{x^{1/3}}^2f(x,y,z)dydzdx\,+\,\int_0^8\int_{x^{2/3}}^4\int_{z^{1/2}}^2f(x,y,z)dydzdx $$ I don't understand why the integral was split. How do I figure out what this shape looks like in the zx plane in order to even know that it needed to be split?","['integration', 'multivariable-calculus', 'calculus']"
3091565,Geometry as a Group Action,At 38:45 in this lecture by Thurston he defines a geometry as a an action by a group $G$ on a simply connected topological space $X$ such that the action is transitive and the stabilizer of a point $x\in X$ is compact.  How would you construct Euclidean and or spherical geometry using Thurston's definition?,"['manifolds', 'group-theory', 'low-dimensional-topology']"
3091593,About the definition of curvature,"In Do Carmo's differential geometry book, he says for a curve $\alpha: I=(a,b)\rightarrow\mathbb{R}^3$ parametrized by arc length, ""since the tangent vector $\alpha'$ (s) has unit length, the norm $|\alpha''(s)|$ of the second derivative measures the rate of change of the angle which neighboring tangents make with the tangent at $s$ . Why does the unit length of the tangent vector imply this geometric meaning of $|\alpha''(s)|$ ?","['multivariable-calculus', 'differential-geometry']"
3091631,Perfect matchings in bipartite graphs.,"Question: $G$ is a bipartite graph where $|X| = |Y| = n$ and $|E| \geq n^2 - \frac{2n}{3} + 3$ . $X$ and $Y$ are the set of vertices and $E$ is the set of edges. Prove that $G$ has a perfect matching. My approach: I tried solving the question but when you plug in values like $n=3$ , $|E| \geq 9-2+3 \implies |E|\geq10$ . How can this be? When $n = 3$ , every vertex can be connected to only $3$ other vertices. Therefore maximum number of edges should be $9$ but this is clearly not the case here.","['graph-theory', 'matching-theory', 'discrete-mathematics', 'bipartite-graphs', 'probability']"
3091649,All these theorems say that conditional distributions tend to be more concentrated. Are they all really one theorem?,"Consider a probability distribution, which I'll call the ""prior distribution"", and some functional of that distribution. Also consider that same functional, but applied to the probability distribution after conditioning on the value of a random variable. Since what we're conditioning on is random, this functional is itself random. I'll call the conditioned distribution the ""posterior distribution."" When this functional measures concentration or dispersion of the distribution, we get theorems like these ones: The expected value of the entropy of the posterior distribution is less than or equal to the entropy of the prior distribution The expected value of the variance of the posterior distribution is less than or equal to the variance of the prior distribution The expected value of the Euclidean norm of the posterior distribution is greater than or equal to the Euclidean norm of the prior distribution (If it's not clear that Euclidean norm measures the concentration of a distribution, consider that $||p||^2 = \sum_i p_i^2$ , which is the probability of drawing the same element twice.) Is there one theorem, that has all three of these facts as special cases?","['conditional-probability', 'bayesian', 'information-theory', 'probability-theory', 'probability']"
3091673,Find the UMVUE of $b^{\mu}$,"Let $X_1,X_2,..X_n$ be a random sample from Cauchy $(\mu,1)$ population.
Find the UMVUE of $b^{\mu}$ where $b$ is any positive real number.
Now actually calculating sample mean won't work here because its distribution is also Cauchy $(\mu,1)$ ,so expectation won't exist.And all I know is that the sample median asymptotically goes to $\mu$ ,so it won't work here either.Is there any trick other than the normal procedures like Lehmann Scheffe to do this?","['statistical-inference', 'statistics', 'probability-distributions', 'parameter-estimation']"
3091682,Proving that $\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)}$,"I conjecture that : $$\forall b\in\mathbb{N}\setminus\lbrace0,1\rbrace,\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)}$$ Which is well verified through numerical simulations. Maybe I'm missing something obvious here, but I have absolutely no idea as of how to prove it. Uniform convergence is of course of no help here, the series $\sum\limits_{n=0}^{\infty}1$ being trivially divergent. Any insight ?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3091706,Is a definite integral just a summation?,I am learning about definite integrals and found the formula for finding an average of a function over a given interval: $$\frac{1}{b-a}  \int_{a}^{b} f\left(x\right) dx$$ If we look at the average function for a set of numbers: $$\frac{1}{n} \sum_{i=1}^{n} x_i$$ It would seem as if the integral is essentially a summation of all values from $a$ to $b$ . Is it correct to think of it this way?,"['calculus', 'definite-integrals', 'means']"
3091713,"Formula for the sequence 0,3,8,15,24 ...","Out of my own interest I've been practicing finding formula for for sequences and I've been having trouble finding one for the nth term for this sequence. 0,3,8,15,24 ... Clearly you add 5,7,9,11 ... to the previous number but if anyone had some insight about how to express this in a formula that would be much appreciated.",['sequences-and-series']
3091721,Lie derivative of Almost complex structure $J$ along a vector field $X$.,"I'd like some guidance in computing the Lie derivative of an almost complex structure $J$ on a smooth closed Riemannian manifold $M$ in terms of the Levi-Civita connection $\nabla$ . The formula I'm trying to obtain is the following $$ (\mathcal{L}_X J)(Y)=(\nabla_X J)Y-\nabla_{JY}X+J\nabla_YX $$ (Ex 2.1.1 Salamon-McDuff J-Holomorphic curves and Symplectic Topology). here is what I did. I can think of $J$ as a $(1,1)$ -tensor field $J(\alpha,Y)$ and by definition the Lie derivative is $\begin{align}(\mathcal{L}_X J)(\alpha,Y) &= X(J(\alpha,Y))-J(\mathcal{L}_X\alpha,Y)-J(\alpha,\mathcal{L}_XY)\\ &= X(J(\alpha,Y))-J(\mathcal{L}_X\alpha,Y)+J(\alpha,[X,Y])\\ &= X(J(\alpha,Y))-J(\mathcal{L}_X\alpha,Y)+J(\alpha,\nabla_YX)-J(\alpha,\nabla_XY) \end{align}$ The term $+J(\alpha,\nabla_YX)$ somehow resemble $J\nabla_YX $ even though with this (1,1)-tensor I'm not sure. Can someone help me with these computations?","['riemannian-geometry', 'complex-geometry', 'connections', 'almost-complex', 'differential-geometry']"
3091809,Harvard To Mit Math Competition [2004],"Zach has chosen five numbers from the set $\{1,2,3,4,5,6,7\}$ . If he told Claudia what the product of the chosen numbers was, that would not be enough information for Claudia to figure out whether the sum of the chosen numbers was even or odd. What is the product of the chosen numbers?. Consider a set, $S$ = $\{N>1|N≠$ p_1•p_2.....p_k $\}$ where $p_1•p_2.....p_k$ are primes. If I write the statement for above expression then I would write that I considered a set "" $S$ "" such that all the elements in "" $S$ "" are greater than $1$ and can't be factorised into primes. So, from above statement, $N$ can't be a prime number, because every          prime "" $p$ ""can be written as $p$ itself. Let, $M$ =min( $S$ ) So by Construction, $M$ = $a$ • $b$ , where $a$ , $b$ are composite numbers. Here, $1$$\lt$$a$$\leq$$b$$\lt$$M$ The above expression contradicts the fact that $M$ is minimal, because $a$ , $b$$\lt$$M$ . $\Rightarrow$ $a$ , $b$$\notin$ S So, a,b can be factorised into primes. Let, $a=p_1•p_2....p_k$ , $b=q_1•q_2....q_m$ Hence, $M=p_1•p_2..p_k•q_1•q_2..q_m$ So, $M$$\notin$$S$ This implies that $S$ = $\varnothing$ Hence, FTA is true $\forall$ N $\gt$ 1 For $a$ , $b$$\in$$R$ , $a$$>0$ , $\exists$$n$$\in$$N$ , $n$$a$$>$$b$ $$P=\displaystyle\prod_{k=1}^{n}\left[1–tan^2\left(\frac{2^kπ}{2^n+1}\right)\right]$$","['contest-math', 'number-theory']"
3091818,Invertible elements in $l^1(\mathbb Z)$,"The vector space $l^1(\mathbb Z)$ with $\|x\| = \sum_{n \in \mathbb Z} |x_n|$ and $x * y(t) = \sum_{k \in \mathbb Z} x(k)y(t-k)$ forms a unital complex Banach algebra, with the unit being $\mathbf 1(0) = 1$ and $\mathbf 1(z) = 0$ for all $z \neq 1$ . I need to find the invertible elements of this Banach algebra. The first thing we do is note that if $\|h\| < 1$ then $1-h$ is invertible. Furthermore, $f$ is invertible if and only if $\alpha f$ is invertible for some scalar $\alpha$ non-zero. Therefore, combining these, if there is some non-zero $\alpha \in \mathbb C$ and $h \in l^1(\mathbb Z)$ with $\|h\| < 1$ such that $f = \frac{1}{\alpha} (\mathbf 1 - h)$ then $f$ is invertible. This simplifies to $\mathbf 1 - \alpha f$ being of norm $<1$ for some $\alpha$ non-zero. By definition of the norm , $$\sum_{k \in \mathbb Z} |(\mathbf 1(k) - \alpha f(k))| < 1 \iff |1-\alpha f(0)| + \sum_{k \neq 0\in \mathbb Z} |\alpha||f(k)| < 1 $$ It is not clear to me how I should proceed further on from this point : this gives some condition on $f$ in terms of $\alpha,h$ and I want to claim that this is sufficient, but no progress has been possible in the other direction because $f * g = \mathbf 1$ , from the assumption of $f$ being invertible is not workable because of too many equations in the unknowns $f(k)$ . I believe that this is down to which elements  in the Banach algebra don't have zero in their spectrum, so if there is any result in that direction (i.e. results about the spectrum) I would like to know about that as well.","['banach-algebras', 'spectral-theory', 'functional-analysis']"
3091832,Is $1111111111111111111111111111111111111111111111111111111$ ($55$ $1$'s) a composite number?,"This is an exercise from a sequence and series book that I am solving. I tried manipulating the number to make it easier to work with: $$111...1 = \frac{1}9(999...) = \frac{1}9(10^{55} - 1)$$ as the number of $1$ 's is $55$ . The exercise was under Geometric Progression and Geometric Mean. However, I am unable to think of a way to solve this problem using GP. How do I proceed from here?","['repunit-numbers', 'sequences-and-series', 'algebra-precalculus', 'geometric-series', 'geometric-progressions']"
3091900,Is $\mathbb{Q}^n$ a vector space over $\mathbb{Z}$ or $\mathbb{Q}$?,"Is $\mathbb{Q}^n$ a vector space over $\mathbb{Z}$ or over $\mathbb{Q}$ ? $\mathbb{Q}^n$ is clearly not a vector space over $\mathbb{R}$ , because scalar multiplication of some $q \in \mathbb{Q}$ by $\pi$ renders $\pi q$ , which is irrational.","['vectors', 'linear-algebra', 'vector-spaces']"
3091913,closed form expression for $\sin 10^o$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question As we know that $\sin 15^o$ , $\sin 30^o$ , $\sin 45^o$ have simple closed form expressions as these are multiples of 3, but i have never seen any simple closed form expression for $\sin 10^o$ or simply sine for any non-multiple of 3, if there exists a closed form expression, do  help me, PS. I know $\sin 10^o$ is solution of $8x^3-6x+1=0$ but i can't solve it as its too tedious . Why is being multiple of 3 such a great thing for an angle???","['trigonometry', 'angle', 'closed-form']"
3091953,Putting socks and shoes on a spider,"A spider needs a sock and a shoe for each of its eight legs. In how many ways can it put on the shoes and socks, if socks must be put on before the shoe? My attempt: If I consider its legs to be indistinguishable, then it's exactly the $8^{\text{th}}$ term of the Catalan sequence. However the legs are distinguishable. So the total number of ways equals $\frac1{8+1} \binom{16}{8} (8!)^2$ . Is this correct? Is there another way of doing it? Edit: All socks and shoes are distinguishable .","['permutations', 'contest-math', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
3092017,Nested powers of $\sqrt 2$ has a solution different from its limit. What does this mean?,"The infinitely nested power expression below has a limit of $2$ : $$x=\sqrt2^{\sqrt2^{\sqrt2^{...}}}$$ In finding this limit, we may use: $$x=(\sqrt2)^x$$ But this expression has two solutions, $2$ and $4$ . We know that $2$ is the right answer by evaluating some finite truncations, but this $4$ is bothering me. What does $4$ mean in this expression? Is it significant in some way?","['limits', 'power-towers']"
3092020,Construction of product measures - self study,"I am struggling with section 18 of the textbook ""Probability and Measure"" by Billingsley. In particular with the construction of product measures. The contruction uses two measure spaces $(X,\mathcal X,\mu)$ and $(Y,\mathcal Y,\nu)$ where $\mu$ and $\nu$ are assumed finite . In short, the argument is the following: since the section of $E \in \mathcal X \times \mathcal Y$ is measurable, the measure of the section, i.e. $\nu[y:(x,y) \in E]$ is a well-defined function of x. the class of $E \in \mathcal X \times \mathcal Y$ for which such function is measurable is a $\lambda$ -system and contains the $\pi$ -system of measurable rectangles, therefore it coincides with $\mathcal X \times \mathcal Y$ (because the latter is generated by measurable rectangles) it is stated that $\pi'(E)=\int_X \nu[y:(x,y) \in E] \mu(dx)$ is a finite measure on $\mathcal X \times \mathcal Y$ . For the other section the measure is denoted by $\pi''(E)$ For measurable rectangles $\pi'(A\times B) = \pi''(A\times B) = \mu(A)\nu(B)$ , therefore the class of $E \in \mathcal X \times \mathcal Y$ for which $\pi'(E) = \pi''(E)$ contains the measurable rectangles, and since it is a $\lambda$ -system it contains $\mathcal X \times \mathcal Y$ . The result is then extended to $\sigma$ -finite measures.
I know that the $\sigma$ -finite assumption is necessary (popular example here ) but I can't see where the finiteness assumption is required in the argument above: could you please help? Edit: Perhaps the problem lies with proving that the class $\mathcal L$ of $E \in \mathcal X \times \mathcal Y$ for which $\nu[y:(x,y) \in E]$ is measurable is a $\lambda$ -system. It is required that, if $A \in \mathcal L$ , $A^C \in \mathcal L$ , i.e., $\nu[y:(x,y) \in A^C]$ is measurable $\mathcal X$ . I would try to prove measurability by showing that, for $c\in \mathbb R$ , $\{x: \nu[y:(x,y) \in A^C]\leq c\} \in \mathcal X$ . To do that, I would use: $\nu[y:(x,y) \in A^C] = \nu[Y] - \nu[y:(x,y) \in A]$ (1) and then rearrange the inequality that determines the set. However, if the measure is not finite, (1) is not lecit. I am not sure, however, if this is a real problem or there is a different way to prove measurability that I just can't see.
Can you confirm/suggest anything?","['proof-explanation', 'measure-theory', 'probability-theory', 'real-analysis']"
3092023,subgroup of a quotient group,"If $K\subset G$ is a normal subgroup of a group $G$ , and if $H\supset K$ is a subgroup of $G$ , then we have that $H/K$ is a subgroup of $G/K$ . But I don't see how the elements of $H/K$ can be part of $G/K$ at all. For we have $$
H/K \ni \bar h =\{a\in H\mid a^{-1}h\in K\}
$$ and $$
G/K \ni \bar g =\{a\in G\mid a^{-1}g\in K \}.
$$ So it seems to me we can't have $H/K\subset G/K$ , because the elements of $G/K$ actually contain elements of $H/K$ , and aren't equal to it; $H/K\ni \bar a_{H/K}\subset \bar a_{G/K}\in G/K$ . But given that $H/K$ is considered a subgroup, it seems to me then that we would have an equality. Is this the case? Or am I confusing some definitions here. Note: I don't want to use cosets here; just the definition of an element of $G/K$ or $H/K$ using the equivalence relation $a\equiv b$ iff $a^{-1}b\in K$ .",['group-theory']
3092059,Is it necessary to prove uniqueness of Peano addition?,"If we define addition as follows: Define $a+0=a$ . For all $a,b\in\mathbb N$ such that $a+b$ is defined, define $a+S(b)=S(a+b)$ . It's easy to show through induction that this defines addition for all $a,b\in\mathbb N$ . A few mathematical papers on Peano axioms such as this one include a proof of the uniqueness of addition. I don't particularly have any problem with that, but I'm curious as to what circumstance needs this uniqueness. For example, will associativity, commutativity and all other properties of addition be proved without having to ensure uniqueness?","['elementary-set-theory', 'arithmetic', 'peano-axioms']"
3092061,Existence and construction of isomorphism between finite groups,"Assume I have two finite groups $G$ and $H$ of equal order. Further assume I have found minimal generating sets $A$ and $B$ for a the two groups respectively (of equal size) and additionally (see comments) I know at least one decomposition into generating elements of all $g \in G$ . I now want to find out if the two groups are isomorphic give this extra information of the minimal generating sets. Is there an approach along these lines: Define a bijective function $f: A\to B$ . Extend the function to all of $G$ in the following manner: For $g \in G$ find a decomposition of $g = a_1 + \cdots + a_m$ where $a_i \in A$ and define $f(g) = \sum_{i=1}^m f(a_i)$ . If this extension fulfills the homomorphism property $f(g_1 + g_2) = f(g_1) + f(g_2)$ for all $g_1, g_2 \in G$ then it is an isomorphism. Question 1 . Is the above statement correct? Do I need to check $f(G) = H$ or is this already implicitly true? Question 2 . Here I need to expand the function for all $g \in G$ and check the homomorphism property for all pairs of elements from $G$ . Given the information of two minimal generating sets, can I reduce the amount of checking I have to do? (Checking only the pairs of generators is obviously not enough since the extension fulfills the homomorphism property for elements from $A$ by construction)","['group-theory', 'group-isomorphism', 'finite-groups']"
3092079,Computing singularities of a surface,"Let $Y$ be the abelian variety $\mathbb{C}/\mathbb{Z}[i] \times \mathbb{C}/\mathbb{Z}[i] $ .  Let $X$ be the quotient of $Y$ by action of the group generated by the map $\eta(x,y)=(ix,iy)$ . This group generated is of order 4, and is given by $\{e, -e, \eta, -\eta\}$ where $e$ is the identity map. How can we show that $X$ is in fact a rational surface and has 10 singularities? I do know we have to look at the fixed points by the subgroups generated by $\eta$ but I have very little idea on how to proceed. Any hints given or links to papers describing this particular construction would be greatly appreciated! Edit: I have been told that I need to look at the fixed points of the orbit, which in this case is simply the 0 class. How can I proceed?","['complex-geometry', 'algebraic-geometry', 'surfaces', 'singularity']"
3092189,Schoen & Yau's proof of the positive-mass theorem: Why is the surface S homeomorphic to $\mathbb{R}^2$?,"I'm currently reading through Schoen & Yau's 1979 proof of the positive-mass theorem and am trying to understand the following statement on p. 55 of the publication (page 11 of the proof / PDF): Remark 2.1: The Cohn-Vossen inequality says that $\int_S K \leq 2\pi \chi(S)$ , where [ $K$ is the Gauss curvature and] $\chi(S)$ is the Euler characteristic of $S$ . Combining this with (2.18) we see immediately that $S$ is homeomorphic to $\mathbb{R}^2$ Here, (2.18) refers to the inequality $\int_S K > 0$ derived just before. How can I see that the surface $S$ is homeomorphic to $\mathbb{R}^2$ ? The immediate implication of (2.18) obviously is that $\chi(S) > 0$ , so $\chi(S) \geq 1$ . Apart from that, I know that $S$ is non-compact but I'm unsure about other properties $^\dagger$ of $S$ as my understanding of its construction is somewhat limited so far. Is it those properties that imply that the Betti numbers $b_i$ vanish for $i \geq 1$ ? -- $^\dagger$ For instance, I suspect that $S$ is boundaryless, connected & simply connected and also a closed subset of the ambient manifold $N$ but since I only have a rough idea of how $S$ is constructed as a limit of surfaces $S_\sigma$ (namely by representing the minimal surfaces $S_\sigma$ locally as graphs in the tangent space (using normal coordinates) and using Arzelà-Ascoli for finding the limit), I'm having trouble coming up with rigorous proofs of any properties of $S$ that could help prove the claim.","['general-relativity', 'algebraic-topology', 'differential-geometry']"
3092190,Asymptotics of Hypergeometric $_2F_1(a;b;c;z)$ for large $|z| \to \infty$?,"I found this list of asymptotics of the Gauss Hypergeometric function $_2F_1(a;b;c;z)$ here on Wolfram's site for large $|z| \to \infty$ In particular there is a general formula for $|z| \to \infty$ $$
_2F_1(a;b;c;z) \approx \frac{\Gamma(b-a)\Gamma(c)}{\Gamma(b)\Gamma(c-a)} (-z)^{-a} +\frac{\Gamma(a-b)\Gamma(c)}{\Gamma(a)\Gamma(c-b)} (-z)^{-b}
$$ How is this derived? Also, is this always true (meaning, for all $a$ , $b$ , $c$ )? There are no sources on the site I linked. Is there also a way to determine the next-order terms?","['complex-analysis', 'power-series', 'asymptotics', 'hypergeometric-function']"
3092236,Existence of a strictly increasing transformation between two functions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Assume $f$ and $g$ are two differentiable functions defined on a compact interval $X \subseteq \mathbb{R}$ mapping into $\mathbb{R}$ . I want to proof or disproof the following statement $ \forall x \in X: \operatorname{sign}(f'(x))=\operatorname{sign}(g'(x))\;\;  \implies \exists \;\; m: \mathbb{R} \to \mathbb{R}$ , strictly increasing s.t. $f=m \circ g$ My attempts raised the elementary question which conditions on arbitrary $f,g$ are in general sufficient for the existence of an $m$ such that $f=m \circ g$ . Any suggestions?","['monotone-functions', 'derivatives', 'transformation']"
3092287,Conformal automorphism of unit disk that interchanges two given points,"Let $a$ and $b$ be distinct points in the unit disk $D$ . Show that there exists a conformal automorphism $f$ of $D$ that interchanges $a$ and $b$ ; that is, $f(a) = b$ and $f(b) = a$ . Idea: we know that $g(z)=\frac{\alpha-z}{1-\bar{\alpha}z}$ interchanges $0$ and $\alpha$ and by composition we can find out the map $f(a) = b$ for any $a$ and $b$ in the unit disk $D$ . But how can I get the other way by the same map? Thanks.","['complex-analysis', 'conformal-geometry', 'automorphism-group', 'mobius-transformation']"
3092340,Why is Euler's number $2.71828$ and not anything else? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question Why is Euler's number $\mathtt 2.71828$ and not for example $\mathtt 3.7589$ ? I know that $e$ is the base of natural logarithms. I know about areas on hyperbola xy=1 and I know its formula: $$e =\sum_{n=0}^\infty \frac{1}{n!} \approx 2.71828$$ And I also know it has many other characterizations.
But, why is $e$ equal to that formula (which sum is approximately $\mathtt 2.71828$ )? I googled that many times and every time it ends in having "" $e$ is the base of natural logarithms"". I don't want to work out any equations using $e$ without understanding it perfectly. Summary: I'm looking for the origin of $e$ , if $\pi$ came from the radius of a circle with a unit diameter, then what is $e$ ???","['eulers-number-e', 'calculus', 'irrational-numbers', 'logarithms']"
3092369,"""Numbers"" bigger than every natural number","In the book Understanding analysis , by Abbot, when discussing the Archimedean property, the author states that there are ordered field extensions of $\mathbb{Q}$ that include ""numbers"" bigger than every natural number. Could someone provide examples and and explanation why this could be the case?","['extension-field', 'examples-counterexamples', 'analysis']"
3092403,Number of possible zero entries in orthogonal matrices,"It's easy to check that in an orthogonal  matrix $Q$ dimension $2 \times 2$ if there is entry $0$ in the matrix then necessary one additional zero must be present and the total number of zeros is $2$ . In an orthogonal matrix  dim. $3 \times 3$ number of zeros can be (if they are present) , I suppose from observations, only $4$ or $6$ - once again we obtain an even  number of possible zeros. Examples: $ \begin{bmatrix}   
0.6 & -0.8  & 0 \\ 0.8 & 0.6 & 0 \\ 0 & 0 & 1 \\ 
\end{bmatrix} \ \ $ , $ \ \ \begin{bmatrix}   0 & 0 & 1 \\
1 & 0  & 0 \\ 0 & 1 & 0 \\
\end{bmatrix}$ Can this observation be extended for other orthogonal matrices of
greater dimensions?  The number of zeros is always even? How to prove this? Maybe, it is known the explicit formula for the  number of possible zeros in orthogonal matrices of any dimension?","['matrices', 'orthogonal-matrices', 'linear-algebra']"
3092406,"Second derivative operator closed on $AC^2[0,1]$","Consider the Hilbert space $\mathcal H=L^2(0,1)$ and let $D:=AC^2[0,1]$ be the space of functions $f\in C^1[0,1]$ such that $f'$ is absolutely continuous and the weak derivative $f''$ is in $ L^{2}(0,1)$ . Define the operator $$T:D\to \mathcal H; \, T= -\frac{d^2}{dx^2}.$$ I want to prove that $T$ is closed, i.e. the graph of $T, \Gamma(T)$ is closed. To that end, let $(\varphi, \psi) \in \overline{\Gamma(T)}$ and $(\varphi_n)_{n\in \mathbb N}$ such that $\varphi_n \stackrel{n\to \infty}{\longrightarrow} \varphi$ and $T\varphi_n  \stackrel{n\to \infty}{\longrightarrow} \psi$ in $L^2$ . I need to prove $\varphi \in D$ and $T\varphi = \psi$ . Here I am stuck. I tried a lot playing with integration by parts and other stuff. I am also not sure what I can say about the first derivative of $\varphi$ . Any help appreciated!","['operator-theory', 'functional-analysis', 'mathematical-physics']"
3092412,prove $\int_0^\infty \frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}{8}$ with real methods,"Context: I looked up ""complex residue"" on google images, and saw this integral. I, being unfamiliar with the use of contour integration, decided to try proving the result without complex analysis. Seeing as I was stuck, I decided to ask you for help. I am attempting to prove that $$J=\int_0^\infty\frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}8$$ With real methods because I do not know complex analysis. I have started with the substitution $x=\tan u$ : $$J=\int_0^{\pi/2}\log^2(\tan x)\mathrm dx$$ $$J=\int_0^{\pi/2}\log^2(\cos x)\mathrm dx-2\int_{0}^{\pi/2}\log(\cos x)\log(\sin x)\mathrm dx+\int_0^{\pi/2}\log^2(\sin x)\mathrm dx$$ But frankly, this is basically worse. Could I have some help? Thanks. Update: Wait I think I actually found a viable method $$F(\alpha)=\int_0^\infty \frac{x^{\alpha}}{x^2+1}\mathrm dx$$ As I have shown in other posts of mine, $$\int_0^\infty\frac{x^{2b-1}}{(1+x^2)^{a+b}}\mathrm dx=\frac12\mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{2\Gamma(a+b)}$$ so $$F(\alpha)=\frac12\Gamma\left(\frac{1+\alpha}2\right)\Gamma\left(\frac{1-\alpha}2\right)$$ And from $$\Gamma(1-s)\Gamma(s)=\frac\pi{\sin \pi s}$$ we see that $$F(\alpha)=\frac\pi{2\cos\frac{\pi \alpha}{2}}$$ So $$J=F''(0)=\frac{\pi^3}8$$ Okay while I have just found a proof, I would like to see which ways you did it.","['integration', 'calculus', 'real-analysis']"
3092448,What to multiply by to get correct form ODE,"Suppose $y'' + f(x)y = 0$ where $M \geq f(x) \geq m > 0$ on some interval $[a,b]$ , then the number zeros $N$ of a non trivial solution is $\lfloor\frac{(b-a)\sqrt{m}}{\pi}\rfloor \leq N \leq \lceil\frac{(b-a)\sqrt{M}}{\pi}\rceil$ Simple. Now suppose I have an equation $y''+4y'+\frac{8x+\sin(x)}{x+1}y = 0$ and I want to estimate the number of zeros of a non trivial solution. I can't use the theorem as is, because the ODE is not in the correct form, to fix this, we can multiply by $e^{2x}$ and get $y''e^{2x}+4y'e^{2x}+\frac{8x+\sin(x)}{x+1}ye^{2x} = 0$ Now if we let $ye^{2x} = z$ we have an ODE $z'' + (\frac{8x+\sin(x)}{x+1}-4)z = 0$ which is in the correct form How did the professor know to multiply by $e^{2x}$ ? Is there a method to this or was this just a lucky guess","['calculus', 'ordinary-differential-equations']"
3092499,Difference between $d\mu(x)$ and $\mu(dx)$,"In my lecture notes of probability course I found two different notations involving $d,\mu$ and $x$:
is there any difference between $\mu(dx)$ and $d\mu(x)$? For example I read $\mu(dx) = \frac{1}{\sqrt{2\pi}}\exp\{-\frac{1}{2}x^2\}dx$ (the density of $\mu=\mathcal{N}(0,1)$, the standard normal distribution) and $\varphi(t) = \mathbb{E}\left[\exp\{itX\}\right] = \int \exp\{itx\}\mu_X(dx)$ but also $\int f(x)d\mu(x)$: Is it the same if I write down $\int f(x)d\mu(x)$ or $\int f(x) \mu(dx)$ ?","['notation', 'measure-theory', 'lebesgue-integral', 'probability-theory']"
3092512,How to show that this short exact sequence does not split,"Consider the Short Exact sequence $0\rightarrow C_0((0,1))\rightarrow C([0,1])\rightarrow \mathbb{C}\bigoplus\mathbb{C}\rightarrow 0$ where the map from $C([0,1])\rightarrow \mathbb{C}\bigoplus\mathbb{C}$ sends $f$ to $(f(0),f(1))$ . How does one show this sequence does not split in terms of C* algebras. i.e. there does not exist a *-homomorphism inducing a right split. I consider the map $(a,b)$ to the continuous function starting at $a$ and ending at $b$ by Tietzes extension theorem. Why can this not be made to be a *homomorphism?","['k-theory', 'c-star-algebras', 'functional-analysis']"
3092518,On calculating sigma algebras generated by specific functions.,"I started (again!) with the intention to build an interesting example of a computation of conditional expectation with respect to $\sigma(X) $ when $X $ is not a step function. My first example, choosing $\Omega = [-1, 1], $ ${\cal F} = {\cal B}(\Omega), $ and $P = (1/2)\lambda $ with $X: \Omega \mapsto R: \omega \mapsto \omega^2 $ or $|\omega | $ worked fine, since $\sigma(X) = \{A \in {\cal B}(\Omega): A= -A\}. $ When I tried to have a function that is not symmetric, say $Y(\omega) = -\omega\cdot I_{[-1, 0]}(\omega) + 2\omega \cdot I_{[0, 1]}(\omega), $ not such luck. Now, I can see the overall structure of $\sigma(Y), $ but cannot really write it down in an elegant manner: I see that intervals like $(a, b) $ with $a= -1/2*b $ should be part of $\sigma(Y), $ if $b > 0, $ but I was hoping to get an elegant expression like in the case of $\sigma(X) $ above. Is that possible? Thank you. Maurice","['conditional-expectation', 'probability-theory']"
3092527,An intriguing pattern in Ramanujan's theory of elliptic functions that stops?,"I. Define the ff integrals, $$K(k)=K_2(k)=\int_0^{\pi/2}\frac{1}{\sqrt{1-k^2 \sin^2 x}}dx=\large{\tfrac{\pi}{2}\,_2F_1\left(\tfrac12,\tfrac12,1,\,k^2\right)}$$ $$K_3(k)=\int_0^{\pi/2}\frac{\cos\left(\frac13\,\arcsin\big(k\sin x\big)\right)}{\sqrt{1-k^2 \sin^2 x}}dx=\large{\tfrac{\pi}{2}\,_2F_1\left(\tfrac13,\tfrac23,1,\,k^2\right)}$$ $$K_4(k)=\int_0^{\pi/2}\frac{\cos\left(\frac12\,\arcsin\big(k\sin x\big)\right)}{\sqrt{1-k^2 \sin^2 x}}dx=\large{\tfrac{\pi}{2}\,_2F_1\left(\tfrac14,\tfrac34,1,\,k^2\right)}$$ $$K_6(k)=\int_0^{\pi/2}\frac{\cos\left(\frac23\,\arcsin\big(k\sin x\big)\right)}{\sqrt{1-k^2 \sin^2 x}}dx=\large{\tfrac{\pi}{2}\,_2F_1\left(\tfrac16,\tfrac56,1,\,k^2\right)}$$ These are Ramanujan's theory of elliptic functions for alternative bases of signature $2,3,4,6$ , respectively. There are only 4 signatures. II. Then, using Wolfram , I observed the closed-forms of the ff definite integrals, $$\int_0^1 K_2(k)\, dk = {\tfrac{\pi}{2}\,_3F_2\left(\tfrac12,\tfrac12,\tfrac12;1,\tfrac32;1\right)}=2G$$ $$\int_0^1 K_3(k)\, dk = {\tfrac{\pi}{2}\,_3F_2\left(\tfrac12,\tfrac13,\tfrac23;1,\tfrac32;1\right)}=\tfrac{3\sqrt3}2\, \ln2$$ $$\int_0^1 K_4(k)\, dk = {\tfrac{\pi}{2}\,_3F_2\left(\tfrac12,\tfrac14,\tfrac34;1,\tfrac32;1\right)}=2\ln(1+\sqrt2)$$ $$\int_0^1 K_6(k)\, dk = {\tfrac{\pi}{2}\,_3F_2\left(\tfrac12,\tfrac16,\tfrac56;1,\tfrac32;1\right)}=\tfrac{3\sqrt3}4\, \ln(2+\sqrt{3})$$ where $G$ is Catalan's constant . (Curiously, other than the first, Wolfram didn't recognize the closed-form of those hypergeometrics. I had to use the Inverse Symbolic Calculator .) III. Questions Does the generalized hypergeometric function, $$H(n)=\,_3F_2\left(\tfrac12,\tfrac1n,\tfrac{n-1}{n};1,\tfrac32;1\right)$$ have a closed form only for $n=2,3,4,6$ ? (I tried $n=5,7,8$ , etc, and it doesn't seem to have a ""neat"" form using elementary functions.) If so, is it connected to why there are only 4 signatures of alternative bases?","['integration', 'definite-integrals', 'elliptic-functions', 'closed-form', 'hypergeometric-function']"
3092603,Is $\mathbb{R}^n$ a vector space or a metric space?,"In my various courses, for instance, linear algebra and vector calculus, I am somewhat confused with what precisely $\mathbb{R}^n$ is. From the definition of the Cartesian product, I would conceptualise $\mathbb{R}^n$ as the metric space with some distance operator, where all the points are just $n$ -tuples. This is surely a distinct notion from vectors as isn't the point $A = (1,2,3)$ , for instance, different from the vector $\vec{a} =\begin{pmatrix}
1\\ 
2\\ 
3
\end{pmatrix}$ ? But if we were to consider the points in $\mathbb{R}^n$ as vectors then clearly it is a vector space. However I don't know whether these two conceptions of $\mathbb{R}^n$ are actually equivalent. Surely the vectors do not correspond to a specific point in space, unlike the points in $\mathbb{R}^n$ . Forgive me if this is a silly question, or if my question seems garbled. Also please help me with tags if they are inappropriate.","['multivariable-calculus', 'linear-algebra', 'vector-spaces', 'metric-spaces']"
3092659,But what is a continuous function?,"I have a very basic problem. I am confused about ""continuous function"" term. What really is a continuous function? A function that is continuous for all of its domain or for all real numbers? Let's say: $\ln|x|$ - the graph clearly says it's continuous for all real numbers except for $0$ which is not part of the domain. So is this function continuous or not? I could say same about $\tan{x}$ or $\frac{x+1}{x}$ And also what about: $\ln{x}$ - the graph clearly says it's continuous for all of its domain: $(0; \infty)$ - so is this $f$ continuous or not? Thanks for clarification.","['limits', 'continuity', 'real-analysis']"
3092734,Determining elliptic curve analytic rank even/odd,"For an elliptic curve over Q that is defined with large coefficients, it can take mathematical software (such as Sage) a long to time calculate the analytic rank. However, it seems to quickly know if the rank is even or odd. I would like to understand how they determine this so quickly. This is hinted at in a PlanetMath article as the ""root number"" obtained from the sign of the functional equation: $$\Lambda(E,s) = \pm \Lambda(E,2 - s)$$ Where $\Lambda$ is related to the $L$ function by: $$\Lambda(E,s) = N^{s/2} (2\pi)^{-s} \Gamma(s) L(E,s)$$ where $N$ is the conductor of $E$ over $\mathbb{Q}$ . (I'm still a little confused on the detailed definitions. Another reference, with slightly different definition relating $\Lambda$ and $L$ , http://www.math.harvard.edu/~gross/preprints/ell2.pdf ) Anyway, the expansion definition of $L$ does not look like it would be valid for both sides of the functional equation, which would prevent just evaluating $\Lambda$ to check the sign. And due to the speed, I'm guessing Sage isn't evaluating $L$ at all here (or can immediately tell just by looking at a couple terms in the expansion?). Is there some trick that allows extracting the sign without evaluating $L$ ?","['number-theory', 'l-functions', 'elliptic-curves']"
3092785,Population Cumulative Distribution Function with Simple Random Sampling,"Link to textbook example: http://puu.sh/CEgk2/b5d08453fb.png Context to Example 5.4.1: http://puu.sh/CEgmt/c6390a8bba.png Hi, I was reading up on my textbook readings for my intro to stats class, and I got confused over an example that was provided in the textbook. I was wondering if I could get some help with understanding it. In the first picture, there is an equation that is equivalent to CDF. Please correct me if I'm understanding this wrong, but the first part of that equation says ""the probability of the measurement X of the first individual in the population being less than x is equal to..."". Then, the second part of the equation says ""the absolute value of the number of individuals from the population set such that the measurement of the population is less than or equal to x over 20"". Then, the third part is just the CDF. The thing that confuses me is that if the probability is being asked of the first individual of the population set being less than x (which is stated in the first part), why does the second part require me to account for other possible individuals from the population set which satisfy the X(pi) <= x?",['statistics']
3092795,Calculating a cumulative series,"I could do this on a calculator but I thought there would be a more efficient way to do this than punching on my calculator the following: $3.5 \cdot 1.03 + 3.5 \cdot 1.03^2 + 3.5 \cdot 1.03^3 + \ldots + 3.5 \cdot 1.03^{10}$ So basically, $xy + xy^2 + xy^3 + ... + xy^{10}$ How can I simplify this to make it easier to calculate on a calculator?","['power-series', 'algebra-precalculus', 'sequences-and-series']"
3092815,Definition: what is the difference between generating a random variable versus generating a random number?,"Can someone clarify to me the difference between: generating a random variable (according to a certain distribution)
versus generating a random number (according to a certain
distribution)? I am thoroughly confused by this concept, because my goal is simply to generate a set of data points according to a certain distribution, and every single reference out there is about generating random variables according to a certain distribution. Mathematically speaking, A random variable is a function. A random number is a scalar. Totally different thing. For example, this pdf ( http://opim.wharton.upenn.edu/~sok/papers/s/rv.pdf ) is titled ""Generating a random variable"" and starts an example with ""the most widely used method of generating pseudo-random numbers are the congruential
generator"". But the notation for generating the pseudo-random number follows the convention of random variables. In this reference, it talks about generating random variables with the rejection-sampling method. http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf But I thought this method was used for generating random numbers (which is the end-goal for everyone)? What is the distinction between these two concepts?","['statistical-inference', 'statistics', 'mathematical-modeling', 'definition', 'probability']"
3092848,Identifying a multiplicative measure,"I am trying to solve the next problem: Let $\mu:\mathcal{B}_{[0,1]}\rightarrow\mathbb{R}$ be a multiplicative measure, i.e. $$\int fg d\mu=\left(\int f d\mu\right)\left(\int g d\mu\right)$$ for all continuous $f,g:[0,1]\rightarrow\mathbb{R}.$ Identify $\mu.$ I was thinking in two possibly cases. One of them involves the Lebesgue measure and the other the trivial measure: If we consider the constant function $f=g=1$ we have $\mu([0,1])=\mu([0,1])^{2}$ so $\mu([0,1])=0$ or $\mu([0,1])=1.$ If $\mu([0,1])\neq 0,$ I'm not sure $\mu$ be the Lebesgue measure, because in general, for Lebesgue measure, doesn't satisfy the equality $\mu(A\cap B)=\mu(A)\mu(B).$ Also I was considering the case $f=g$ to get $\int f^2 d\mu=\left(\int f d\mu\right)^2$ but it seems a little useless. Any kind of help is thanked.",['measure-theory']
3092860,Defining a pseudo-gradient field for a $1$-form,"I'm reading Audin and Damian's Morse Theory and Floer Homology ; they say there is an analogous way to define nondegenerate critical points for 1-forms as well as pseudo-gradient fields but don't discuss how. The goal is to define such a vector field for a 1-form $\alpha$ and then lift it to a pseudo-gradient field for a function on a covering space. Here's the context. Suppose I have a smooth closed 1-form $\alpha$ on a manifold $M$ . If I consider a map $\phi:\pi_1(M) \to \mathbb{R}$ which is simply integrating $\alpha$ along a loop in $M$ , then it is in fact a homomorphism. I can then consider $\ker \phi \subset \pi_1(M)$ and find a smooth covering space $p: \hat{M} \to M$ such that $p_*(\pi_1(\hat{M}))=\ker \phi$ . This means that for all loops $\hat{\gamma} \in \pi_1(\hat{M})$ , $$\int_{\hat{\gamma}}p^* \alpha = 0
$$ by construction. Thus, $p^* \alpha$ is exact ( $= df$ ) for some function $f$ . We also observe that $(df)_y = 0 \Leftrightarrow \alpha_{p(y)}=0$ . Thus, they say that $f$ and $p^* \alpha$ share the same critical points which themselves share properties such as nondegeneracy and index. It seems a critical point for $\alpha$ is simply where it vanishes. My questions: How are the notions of critical points, nondegeneracy, and pseudogradients defined for a 1-form? Can this be done for $k$ -forms? This paper by Latour is referenced but I can't read French: http://www.numdam.org/article/PMIHES_1994__80__135_0.pdf","['morse-theory', 'differential-geometry']"
3092865,Computing maps in Leray spectral sequence,"Let $f:X_{s1} \to X_{s2}$ be morphism of sites ( Here $X$ is some scheme $X_{s1}$ refers to the site on $X$ ). Now using the Leray spectral sequence one gets the following exact sequence $0 \to H^1(X_{s2}, f_{*}\mathcal{F} ) \to H^1(X_{S1}, \mathcal{F}) \to H^{0}(X_{s2},R^{1}f_{*}\mathcal{F}) \to H^2(X_{s2}, f_{*}\mathcal{F})$ . Is there any example where the maps of the above sequence has been explicitly computed? Say for example suppose I want to compute the map $H^{0}(X_{s2},R^{1}f_{*}\mathcal{F}) \to H^2(X_{s2}, f_{*}\mathcal{F})$ explicitly. Is there any example where I can find such a computation?","['spectral-sequences', 'algebraic-geometry', 'sheaf-cohomology']"
3092866,Understanding a proof concerning the loci of zeros of a polynomial curve,"I am trying to understand following proof. One definition the author uses: The locus of zeros of a function $f(z, K)$ with respect to $K > 0$ is the set of all points $z$ such that for some $K_0$ , $f(z, K_0)=0$ . I can understand every sentence of the proof. But my confusion is: it seems to me it only proves necessity. Am I understanding wrong?","['complex-analysis', 'locus', 'abstract-algebra', 'polynomials']"
3092889,How To solve This Perfect Square Word Problem,"Here's a problem about perfect squares and it's very hard for me. I tried to solve but I got stuck. Last year, the town of Whipple had a population that was a perfect square. Last month, 100 enlightened people moved to Whipple, making the population one more than a perfect square. Next month, 100 more people will move to Whipple, making the population a perfect square again. What was the original population of Whipple? Here's what I did: Let the population last year be n, so n = x^2 and x = √n
Last month: n + 100 = x^2 + 1
Next Month: n + 200 = x^2 ... and i Got stuck there. I don't know where I am going ... Your help is appreciated","['algebra-precalculus', 'square-numbers']"
3092892,When the element-wise product of two ideals produces an ideal,"Consider the ring $R=\mathbb C[X,Y]$ . For every two ideals $I,J$ of $R$ , define $I*J:=\{ij : i\in I, j\in J\}$ . Now definitely, $I*J=J*I$ always holds. If $I$ is principal, then actually $I*J$ is an ideal of $R$ . My question is: If $I$ is a proper, non-zero ideal of $R=\mathbb C[X,Y]$ such that for every ideal $J$ of $R$ , $I*J$ is also an ideal of $R$ (i.e. $I*J=IJ$ ), then does it imply that $I$ is principal ? Or at least $I$ is contained in a principal prime ideal ? If neither of these are true, then can we characterize all ideals $I$ with the said property in some other way ? Some thoughts towards possibly showing $I$ is principal : To show $I$ is principal, enough to show $I$ is free, then by Quillen-Suslin, enough to show $I$ is projective, and since we are in Noetherian, finitely generated case, enough to show $I$ is flat over $R$ . So it is enough to show $I \otimes_R J \cong IJ =I*J$ for every ideal $J$ . No idea how to show that though ...","['ring-theory', 'algebraic-geometry', 'polynomials', 'ideals', 'commutative-algebra']"
3092935,Surface of an open ball,"In $\mathbb{R}^3$ , Euclidean space. Suppose each point is either blue or red. Let $R>0$ be the largest number such that an open ball $B(x_0,R)$ contains only blue points. Is it true that there is at least one limit point of the red points on the surface of this ball? This is trivial if we are considering real line, but here I'm not so sure. The converse is that for all points on surface we can find an open blue ball centered around it. But there is no way to find an uniform lower bound of the radius of all such balls for all the points--so that I can claim if the statement is not true then I can find $R'>R$ , i.e. blue ball can be enlarged. This difficulty does not exist for 1D since there  are only two boundary points. What can I do?","['general-topology', 'analysis', 'real-analysis']"
3092947,Books for Advanced algebra and Advanced geometry(AMC 12) [duplicate],This question already has answers here : What's a good book for a beginner in high school math competitions? (4 answers) Closed 5 years ago . I will be taking the American Math Contest. Various topics are covered in this test including advanced geometry and algebra. It would be great if any of you could provide books/references which contain challenging or hard problems related to advanced geometry and algebra.,"['geometry', 'book-recommendation', 'reference-request', 'abstract-algebra', 'algebra-precalculus']"
3092986,Efficient computation of incremental standard deviation (removing first value),"I found this link about incremental standard deviation where it computes the standard deviation every time a new element was added to the dataset. Is there a similar method when adding a new value and removing the first value from the dataset? For example, [45, 26, 78, 45, 34, 56] - initial data set 

[26, 78, 45, 34, 56, 74 *(new data)*] - first value 45 was removed","['computational-mathematics', 'statistics', 'standard-deviation', 'algorithms']"
3092989,Is this linear operator on polynomials with sup-norm bounded?,"Question: Let $\mathcal{P}$ be the space of all polynomials (with real coefficients) on the real line, endowed with sup-norm (i.e., $\|p\| = \sup_{0\le x\le 1}|p(x)|$ ). For any fixed $n \in \mathbb{N}$ , consider the linear functional $\ell_n \colon \mathcal{P} \to \mathbb{R}$ , where $\ell_n(p)$ is equal to the coefficient of $x^n$ in $p$ . Is $\ell_n$ a bounded linear functional on this normed (but incomplete) space? Attempt: Well I can see that $\ell_0$ is a bounded linear functional with norm 1, but I don't know the answer in general. Thanks for your help!","['normed-spaces', 'functional-analysis']"
3093008,Does there exist a partition of an L to create a square?,"Background: Consider the following collection of tiles. These can be arranged to form a ""difference of two squares"" which I call an ""L"" (shown above), or a ""square"" (shown below). In this particular example, we have $3a = c$ . When the lengths of the L satisfy this ratio, one can partition the L in such a way that rearranging the pieces produces a square. What if the side lengths do not satisfy this ratio? More precisely, my question is as follows. Question: Suppose you are given an arbitrary L with outer side lengths $c$ and inner side lengths $a$ . Is there a procedure that one can follow to create a finite partition of the L which can be reassembled to form a square of side lengths $b = \sqrt{c^2 - a^2}$ ? Here $a, c \in \mathbb R$ are arbitrary with $0 < a < b$ . The term ""procedure"" is left intentionally vague. I believe this is difficult to achieve. What if $a,b,c$ are Pythagorean triples? Can the above problem then be solved? If this second problem is also too difficult, is there some other class of ratios for which this is problem to solve?","['euclidean-geometry', 'puzzle', 'geometry', 'plane-geometry', 'tiling']"
3093025,Smallest subgroup generated by a subset of a group.,"If $\ G$ is a group, and the set $S=\{a,b\}$ is a subset of $\ G$ , can we say that the smallest subgroup of $\ G$ generated by $\langle a,b\rangle$ will always be either $\langle a \rangle$ , $\langle b\rangle$ , or in the case that $\langle a\rangle$ does not generate $b$ and $\langle b\rangle$ does not generate $a$ , then $\langle a,b\rangle$ = $G$ ? I'm having difficulty trying to think of a counterexample to this, particularly a finite group whose elements can be easily enumerated (e.g. $\ U(n)$ the multiplicative group of integers modulo $n$ ).","['cyclic-groups', 'finite-groups', 'examples-counterexamples', 'abstract-algebra', 'group-theory']"
3093030,What is the probability of SPECIAL PIRATE KNIVES GAME ??,"https://www.google.com/search?q=pirate+knife+game&rlz=1C1CHZL_enTH705TH708&source=lnms&tbm=isch&sa=X&ved=0ahUKEwirheygt5TgAhUYSX0KHbQ5AacQ_AUIDigB&biw=1536&bih=762 The link above is what I refer to as "" Pirate Knives game "". Traditionally, it is the game which your objective is to find a one and the only spot hit the pirates. So there are 15 spots and 15 knives. Each turn player 1 and 2 will stab a knife in one spot and the person who stab to the right spot wins! But I am a lazy person so I create a new game and bet with my friends. **The betting is fixed 10 if he cannot win within 10 knives.
EDITED: The game is 20 dollar bet 10 is fixed and 10 is marginally decreased per round.
IF he wins the first round he will get 20 but if he fails at first round the bet decrease to 19 dollars Winning bet is 10 + (10-k+1)
The faster he wins the higher he gets.** So basically, I change this game to be one player game. Which I found after watching my friends smile and sad, before and after the game. I realized that it is an unfair game for him. X denotes the random variable that stands for the round that he wins. 
This game is no-replacement event then it is sample space of each round decrease by one. The probability of each round 
Pr(X= $x_{i}$ ) = 1/15 , 1/14 , 1/13 , ... , 1/7 , 1/6. Even though he got many chances to stab that thing he still has the chance only of 1/6 at most. That made me realize that the game is designed not to win easily it is a famous game the play with the fear and thrill of children. It must be more than 10 for an expected round that the knife will hit that spot. So here is the favor I would like to ask that what is the probability of my friends will win.","['statistics', 'probability']"
3093074,Prove there exists a matrix $B$ such that $AB=BA$ and $B^2 = -I$.,Let $A$ be a real $n \times n$ matrix without real eigenvalues. Prove there exists a real matrix $B$ such that $AB=BA$ and $B^2 = -I$ . I understand that $n$ is even and $A$ is a nonsingular  matrix.,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3093126,Integration by parts questions. Work check.,I have a couple of problems that I'm trying to work through. I'm a tad stuck on 2. Here is what I have? $\int t \cdot e^{-3t} dt$ so let's say: $$u = t \quad \text{and} \quad du = dt$$ $$dv = e^{-3t} \quad \text{and} \quad v = \frac{e^{-3t}}{-3}$$ so according to integration by parts: $$\begin{align*} \int t \cdot e^{-3t} dt &= t \cdot \frac{e^{-3t}}{-3} - \int \frac{e^{-3t}}{-3} dt \newline &= \frac{t}{-3} \cdot e^{-3t} - \frac{-1}{3} \frac{e^{-3t}}{-3} \newline &= \left(\frac{t}{3} - \frac{1}{9} \right) e^{-3t} \end{align*}$$ Is this right? $\int t^2 \sin (\beta t) dt$ Is $\beta $ a constant? What is this notation? $\int \ln \sqrt{x} dx$ $$\int \ln \sqrt{x} dx = \int \ln x^{\frac{1}{2}} dx$$ so let's try: $$u = \ln{x^{\frac{1}{2}}} \quad \text{and} \quad du = \frac{1}{x^{\frac{1}{2}}} \cdot \frac{1}{2} \frac{1}{x^{\frac{1}{2}}} = \frac{1}{2x}$$ and $$dv = dx \quad \text{and} \quad v = x$$ so $$\begin{align*} \int \ln \sqrt{x} dx = \ln x^{\frac{1}{2}} \cdot x - \int x dx = x \ln{x^{\frac{1}{2}}} - \frac{x^2}{2} \end{align*}$$ How does that look?,"['integration', 'calculus', 'proof-verification']"
3093152,What is the difference between a free Abelian group (of say a finite basis) and a finitely generated Abelian group?,Is the difference that the generating set of the finitely generating Abelian group need not be linearly independent? I understand that both the basis and the finite generating set span the Abelian group. Thanks for clarifying.,"['abelian-groups', 'abstract-algebra']"
3093160,how to solve this first order nonlinear differential equation,"I'm reading nonlinear control systems book. The author provides this example $$
\dot{x} = r + x^2, \quad r < 0.
$$ I would like to compute the analytical solution for the proceeding ODE. My attempt is $$
\begin{align}
\frac{dx}{dt} &= r + x^2 \\
\frac{dx}{r+x^2} &= dt \\
\int^{x(t)}_{x_0} \frac{1}{r+x^2} dx &= \int^{t}_{t_0} d\tau \\
\frac{\tan^{-1}\left(\frac{x}{\sqrt{r}}\right)}{\sqrt{r}} \Big|^{x(t)}_{x_0} &= (t-t_0)
\end{align}
$$ Now the problem with the assumption that $r<0$ , how I can handle the substitution for the left side? I need to reach the final step where $x(t)$ is solely in the left side.","['nonlinear-system', 'ordinary-differential-equations']"
3093218,Expected value of outer product of multivariate normal random vector with itself,"Let's say I have a random vector $\boldsymbol{t}$ that is distributed according to a multivariate normal distribution: $$
\boldsymbol{t} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Psi})
$$ I now want to find the expected value of the outer product of this random vector: $$
\mathbb{E}\left[\boldsymbol{t}\boldsymbol{t}^\intercal\right]
$$ Is there a closed-form solution to this problem? In my studies, I have stumbled across the Wishart distribution . Might this be a way to tackle this problem?","['normal-distribution', 'expected-value', 'outer-product', 'probability', 'random-variables']"
3093225,An efficient approach to combinations of pairs in groups without repetitions?,"Before I start, I need to admit that I am not a mathematician and if possible would need this explained in laymen terms. I appreciate your patience with this. The problem: A class consisting of n students whom I'd like to pair up throughout the school year. The number of pairs is n/2. I'd like to maximize students working with new people as much as possible, and so exhaust all possible combinations. Permutations don't matter -- student 1 + student 2 is same as student 2 + student 1. What is an efficient way to build all non-repeating combinations of pairs? One way, (suggested by Aleksei Malushkin), is to find all unique pairs, then all combinations of n/2 pair groups by brute-forcing out all non-valid groups. Finding all unique pairs is trivial in Ruby: [1,2,3,4,5,6,7,8].combination(2).to_a provides an array of all 2-item pairs. What I need, however, is to produce all groups consisting of 4 pairs each, each group without repetition of students. If the class consists of 20 students, I will need all groups of 10 pairs. The bruteforce approach creates all combinations of pairs and throws away the ones containing repeating pairs, but this falls apart very quickly with higher numbers of students. Here's the ruby code which solves for 8 students: # Ruby 2.5.3

students = [1,2,3,4,5,6,7,8]
possible_pairs = students.combination(2).to_a  # https://ruby-doc.org/core-2.4.1/Array.html#method-i-combination

puts ""Possible pairs: (#{possible_pairs.size}) #{possible_pairs}""
puts ""Possible pair combinations: #{possible_pairs.combination(a.size/2).size}""

groups_without_repetition = possible_pairs.
combination(students.size/2).                     # create all possible groups with students.size/2 (4) members each
each_with_object([]) do |group, accumulator|      # with each of these groups, and an ""accumulator"" variable starting as an empty array

  next unless group.flatten.uniq.length == (students.size)  # skip any group with repeating elements
  next if accumulator.find {|existing_group| existing_group & group != []}      # skip any group that may be found in the accumulator

  accumulator << group  # add any non-skipped group to the accumulator
end # returnn the value of the accumulator and assign it to groups_without_repetition

puts ""actual pair combinations without repetition (#{groups_without_repetition.size}):""

groups_without_repetition.each_with_index do |arr, i|
  puts ""#{i+1}: #{arr}""
end When run, this returns: Possible pairs: (28) [[1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]]
Possible pair combinations: 376740
actual pair combinations without repetition (7):
1: [[1, 2], [3, 4], [5, 6], [7, 8]]
2: [[1, 3], [2, 4], [5, 7], [6, 8]]
3: [[1, 4], [2, 3], [5, 8], [6, 7]]
4: [[1, 5], [2, 6], [3, 7], [4, 8]]
5: [[1, 6], [2, 5], [3, 8], [4, 7]]
6: [[1, 7], [2, 8], [3, 5], [4, 6]]
7: [[1, 8], [2, 7], [3, 6], [4, 5]] It is however not efficient. With only 12 students, the possible pairs are 66, and the possible pair combinations 90858768. I am looking to apply this to a class with 80+ participants so this approach is clearly not going to work. Question 1: What would be an efficient approach to construct these combinations? Looking at the results, it seems to me that the number of valid groups is n/2 - 1, as a student can only belong to one of n/2 possible pairs. My sense is that it would be more efficient to construct only the valid groups_without_repetition instead of creating all possible groups and throwing out the non-valid ones. I am not sure how to approach this process, and would appreciate your help. Question 2: How to approach this with an odd number of students? This may need to be a separate discussion so I would not worry about it unless it has a known solution. a. In a case where one of the students will have to participate twice to accommodate the odd number b. In a case where one of the pairs would become a trio In each of these cases, the students who were a part of the above non-conventional pairings, should be excluded from further exceptions for as many rotations as possible. Edit: Posting a Ruby-based solution, based on Alexander Burstein's answer class SafeArray < Array

  # make it possible to return items from the beginning of the array when
  # the item's index exceeds the size of the array. 
  # Ruby does this automatically for negative indexes.

  def [](i)
    if i > size-1
      super(i % size)
    else
      super(i)
    end
  end

end


class AllGroups

  attr_reader :items, groups

  def initialize(items)
    @items = items
  end

  def generate!
    @groups = []

    return if items.size < 2
    return @groups = [[ items[0], items[1] ]] if items.size < 4

    n = items.size
    infinity_point = items[0]

    candidates = SafeArray.new( items[1..-1] )


    (0..n-2).each do |i|
      group = []

      group << [infinity_point, candidates[i]]  # first pair

      (1..(n/2 - 1)).each do |k|
        current_pair = [ candidates[i+k], candidates[i-k] ]
        group << current_pair
      end

      @groups << group

    end
  end

end

# run with 8 ""students""

students = %w{1 2 3 4 5 6 7 8}

all_groups = AllGroups.new(students)
all_groups.generate!

all_groups.groups.each_with_index do |g, i|
  puts ""#{i+1}: #{g}""
end

# results:

# 1: [[""1"", ""2""], [""3"", ""8""], [""4"", ""7""], [""5"", ""6""]]
# 2: [[""1"", ""3""], [""4"", ""2""], [""5"", ""8""], [""6"", ""7""]]
# 3: [[""1"", ""4""], [""5"", ""3""], [""6"", ""2""], [""7"", ""8""]]
# 4: [[""1"", ""5""], [""6"", ""4""], [""7"", ""3""], [""8"", ""2""]]
# 5: [[""1"", ""6""], [""7"", ""5""], [""8"", ""4""], [""2"", ""3""]]
# 6: [[""1"", ""7""], [""8"", ""6""], [""2"", ""5""], [""3"", ""4""]]
# 7: [[""1"", ""8""], [""2"", ""7""], [""3"", ""6""], [""4"", ""5""]] ```","['combinations', 'combinatorics', 'generating-functions']"
3093234,Is Wiener measure a Borel measure?,"I'm not sure that I understand the Wiener measure in right way. Here's what I think. Wiener measure is the measure on the classical wiener space C([0,1]) with sup norm. And it satisfies the properties of Wiener process or Brownian motion.(Like $B_t-B_s \sim N(0,t-s)$ ) My question is, what is the corresponding $\sigma$ -algebra of Wiener measure on C([0,1])? Is it Borel $\sigma$ -field? Or at least containing every open set of C([0,1]) ?","['stochastic-processes', 'measure-theory', 'probability-theory']"
3093288,Colouring a sequence,"Define a 2 coloring of $\left \{ \left. 0,1 \right \}^* \right.$ to be a function $\chi:\left \{ \left. 0,1 \right \}^* \right. \rightarrow \left \{ \left. red,blue \right \} \right. $ e.g. if $\chi(1101)=red$ , we say that 1101 is red in the coloring of $\chi$ . Prove: For every 2 coloring $\chi$ of $\left \{ \left. 0,1 \right \}^* \right.$ and every (infinite) binary sequence $S \in \left \{ \left. 0,1 \right \}^\infty \right.$ There is a sequence: $$w_{0},w_{1},w_{2},...$$ of strings $w_{n} \in \left \{ \left. 0,1 \right \}^* \right.$ such that: i) $S= w_{0}w_{1}w_{2}...$ and ii) $w_{1},w_{2},...$ are all the same color. (The string $w_{0}$ may not be this color). I have tried making arguments that you can split any sequence and define each 'half' as the colorings. This then fails for an infinite sequence. I am not sure how to 'prove' such a result. I'm not sure if you can split it into cases where the infinite sequence is repeating or non-repeating etc. Thanks for any help","['combinatorics', 'coloring']"
3093359,Polar plots of $\sin(kx)$,"The plots of $\sin(kx)$ over the real line are somehow boring and look essentially all the same: For larger $k$ you cannot easily tell which $k$ it is (not only due to Moiré effects): But when plotting $\sin(kx)$ over the unit circle by $$x(t) = \cos(t) (1 + \sin(kt))$$ $$y(t) = \sin(t) (1 + \sin(kt))$$ interesting patterns emerge, e.g. for $k = 1,2,\dots,8$ Interlude: Note that these plots are the stream plots of the complex functions $$f_k(z)=\frac{1}{2i}(z^k - \overline{z^k})z $$ on the unit circle (if I didn't make a mistake). Note that $f_k(z)$ is not a holomorphic function. You  may compare this with the stream plot of $$g_k(z)=\frac{1}{2i}(z^k - \overline{z^k}) = f_k (z)/z$$ with $g_k(e^{i\varphi}) = \sin(k\varphi) $ : [End of the interlude.] Even for larger $k$ one still could tell $k$ : Furthermore you can see specific effects of rational frequencies $k$ which are invisible in the linear plots. Here are the plots for $k=\frac{2n +1}{4}$ with $n = 1,2,\dots,8$ : The main advantage of the linear plot of $\sin(kx)$ is that it has a simple geometrical interpretation resp. construction: It's the plot of the y-coordinate of a point which rotates with constant speed $k$ on the fixed unit circle: Alternatively, you can look at the sine as the projection of a helix seen from the side. This was the idea behind one of the earliest depictions of the sine found at Dürer : Compare this to the cases of cycloids and epicycles. These also have a simple geometrical interpretation - being the plots of the x- and y-coordinates of a point on a circle that rolls on the line resp. moves on another circle with constant speed My question is: By which geometrical interpretation resp. construction (involving circles or
  ellipses or whatsoever) can the polar plots of $\sin$ be seen resp. generated? Which construction relates to the construction of $\sin$ by a rotating point on a circle in the way that the construction of epicycles relates to the construction of cycloids? Just musing: Might this question have to do with this other question on Hidden patterns in $\sin(kx^2)$ ? (Probably not because you cannot sensibly plot $\sin(kx^2)$ radially, since there is no well-defined period.)","['trigonometry', 'visualization', 'geometry']"
3093384,Monotone Likelihood ratio and Karlin-Rubin test,"I'm studying the Karlin-Rubin test on the book Statistical Inference by Casella and Berger. There the MLR property for a family of pdf is defined as: $\forall \theta_2>\theta_1\:\: \frac{g(t,\theta_2)}{g(t,\theta_1)}$ is a monotone function of $t$ on the union of the supports. Then Karlin-Rubin test says that if $T$ is a sufficient statistic for $\theta$ and his family of densities has the MLR, then a UMP level $\alpha$ test for testing $H_0: \theta\leq\theta_0$ vs $H_0: \theta>\theta_0$ has a rejection region with shape $\{T>t_0\}$ (for an appropriate $t_0$ ). My question: If I choose $-T$ then I will still have a sufficient statistic for $\theta$ with the MLR. However, using the theorem I will get a region $\{-T>t_1\}$ which is equivalent to $\{T<-t_1\}$ that has got an opposite shape to the one above. Is this correct? I believe that the definition of MLR given there is wrong ... ... and that one should require that the ratio is increasing, but I'm not sure about that. Also I'm not sure about the contradiction in what I got.","['statistical-inference', 'statistics', 'hypothesis-testing']"
3093400,Is it simply a coincidence that if you differentiate the formula for the volume of sphere you get the formula for the surface area of sphere? [duplicate],"This question already has answers here : Why is the derivative of a circle's area its perimeter (and similarly for spheres)? (8 answers) Closed 5 years ago . So my question is this: $$V=\frac{4}{3}\pi r^3$$ And, $$\frac{dV}{dr}=4\pi r^2=SA$$ Is this a coincidence or are there some mathematical hoodoos that I'm unaware of? P.S. are there any more tags that I should use?","['calculus', 'geometry', 'volume']"
3093466,Evaluate $\lim\limits_{n\to \infty} \left( \cos(1/n)-\sin(1/n) \right) ^n $?,"How to calculate $\lim\limits_{n\to \infty} \left( \cos(1/n)-\sin(1/n) \right) ^n $ ? Since $\lim\limits_{n\to \infty} \frac {\cos(1/n)-\sin(1/n) }{1-1/n} = 1  $ , I guess that the limit above is $\frac{1}{e}$ , but since the form $(\to 1)^{\to \infty}$ is indeterminate, I don't know how to prove it formally. Thanks!","['limits', 'calculus']"
3093489,Solving a PDE for determining a metric,"A friend is studying some Finsler metrics and she has come upon the following partial differential equation: $$\begin{cases}
y^i\frac{\partial b(x,y)}{\partial x^i}=0\\
y^i\frac{\partial b(x,y)}{\partial y^i}=1\\
\end{cases},$$ where $b:\mathbb{R}^{2n}\to\mathbb{R},\ x=(x^1,\dots,x^n)\in\mathbb{R}^n, \ y=(y^1,\dots,y^n)\in\mathbb{R}^n.$ I have managed to find a family of solutions: $$b_k(x,y)=\sqrt{|y|^2-k(|x|^2|y|^2-\langle x,y\rangle^2)},\ k\in\mathbb{R}.$$ but I don't see how to solve the equation in general.","['partial-differential-equations', 'differential-geometry']"
3093502,Estimating a parameter from a sequence of signals with non i.i.d. noise,"I have the following statistics/probability question: Let $A$ and $B$ be two finite sets of real numbers. Let $n \geq 1$ . Let $T_1,...,T_n$ be random variables with values in $A$ (not necessarily independent). Let $Q$ be a random variable with values in $B$ . 
Let $\epsilon_1, \epsilon_2,...,\epsilon_n$ be a sequence of independent random variables which are identically normally distributed, with mean 0 and variance $\sigma>0$ . Assume that
for all $1 \leq i \leq n$ , $\epsilon_i$ is independent of $T_i$ and of $Q$ (note that $\epsilon_i$ may be correlated with $T_{i-1}$ for instance). Let \begin{equation*}
X_i=T_i+Q+\epsilon_i
\end{equation*} I would like to construct a random variable $Q_n$ that is $(X_1,X_2,...,X_n)$ -measurable, and such that for some constant $C$ and $d>0$ that depend only on $A$ , $B$ and $\sigma$ , we have \begin{equation*}
\left\|Q_n-Q\right\|_{L^2} \leq C n^{-d}.
\end{equation*} In words, knowing $X_1,X_2,...,X_n$ , I would like to approximate $Q$ in an efficient way. Many thanks!","['statistics', 'parameter-estimation', 'probability-theory', 'normal-distribution']"
3093529,How to proof variance of the sum $x+y$ is the sum of variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$?,"Show that the variance of the sum $x + y$ of the random variable $x$ and the random variable $y$ is the sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ ? My attempt: Is my first time doing statistics and there are alot of terms i am quite unfamiliar with. Random variable: Value of a variable result from a random experiment: e.g. (the score when a die is rolled once) Variance( $\sigma^2$ ) : Average of squared deviations from the mean or square of standard deviation? I know variance of sum $x + y$ means $var(x+y) = $ E(x+y)^2 - $(E(x+y))^2$ But what does sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ means? Edited:
My 2nd attempt:
Var(x + y ) = E $(x+y)^2$ - $(E(x+y)^2)$ = E( $x^2$ + 2xy + $y^2$ ) - [( $Ex)^2$ + 2ExEy + ( $Ey)^2$ ] = [E $x^2$ - (E $x)^2$ ] + [E $y^2$ - (E $y)^2$ ] + 2[Exy - ExEy] = Var x + Var y + 2cov (x,y) Now i assume(not sure is it correct) sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ = var ( $\sigma_x^2$ ) + var( $\sigma_y^2$ ) 
= E( $\sigma_x^2$ ) -[E( $\sigma_x)]^2$ +  E( $\sigma_y^2$ ) -[E( $\sigma_y)]^2$ Since var(x) = $\sigma_x^2$ , =E(E( $x^2)$ - $[E(x)]^2$ ) - [ $E^2$ ((E( $x^2)$ - $[E(x)]^2$ )) + E(E( $y^2)$ - $[E(y)]^2$ ) - [ $E^2$ ((E( $y^2)$ I did not further solve it because i realise is wrong as the above does not have an expression that has an xy expression. So where have i done wrong?","['statistics', 'variance', 'standard-deviation']"
3093547,"If $(A+B)^n$ is binomial for some $n$, does that imply $AB = BA$?","We say that a matrix power $(A+B)^n$ is binomial iff it satisfies the matrix equality $$(A+B)^n = \sum \limits_{j\,=\,0}^n \binom{n}{j}A^jB^{n-j}.$$ If two matrices have a binomial power for some $n$ , does that imply that $AB = BA$ ? My approach was the most obvious (I think):  I tried to find a non-commutative expression for $(A+B)^n$ to finally get all those ugly permutations with the same degree equal to something that adds up to the term with the respective degree on the other side, but they could ""chaotically"" add up to that without being commutative, so it's not interesting. I also tried to use a ""inductive-like"" thinking to get the simplest case of commutativity, i.e., the case where $n=2$ , so, by the Euclidean algorithm, $n = 2q + r$ . I didn't manage to advance much, though.","['matrices', 'algebra-precalculus', 'binomial-theorem', 'linear-algebra']"
3093638,How to solve following linear differential-difference equation?,"How to solve following linear differential-difference equation? $$\frac{da_{n}(t)}{dt}=i k na_{n}(t)+G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots~~~~~(1)$$ where, k and G is a constant. And $i$ is the imaginary unit. The initial conditions are $$a_{n}(0)=\delta_{mn}=\begin{cases}
C_{0}~~(n=m)
\\
0~~(n\neq m)
\end{cases}$$ where, $C_{0}$ is a constant. $a_{0}(t)$ and $a_{1}(t)$ is unknown function. I want to find $a_{n}(t)$ . If $k=0$ , then equation $(1)$ reduce to following equation: $$
\frac{da_{n}(t)}{dt}=G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots.~~~~~(\mathrm{A})
$$ The general solution of equation $(\mathrm{A})$ is $$
a_{n}(t)=C_{0}\frac{1}{\sqrt{\mathstrut 2\pi}}\left(\frac{n}{2} \right)!~\sqrt[]{\mathstrut 2^{n}}\left\{1+(-1)^{n+1} \right\}(\cosh{2Gt})^{-\frac{3}{2}}(\tanh{2Gt})^{-\frac{1}{2}(n-1)}.~~~~~(\mathrm{B})
$$ Equation $(\mathrm{B})$ satisfy following initial conditions: $$
a_{n}(0)=\begin{cases}
C_{0}~~(n=1)
\\
0~~~~~(n\neq 1)
\end{cases}
~~~~~~~~~(\mathrm{C})
$$ I have tried Laplace transform to equaton $(1)$ . Multiplying both sides of equation $(1)$ by $\mathrm{e}^{-st}$ and then Integrating  for the interval $0$ to $\infty$ to obtain $$
\int_{0}^{\infty}dt~\mathrm{e}^{-st}
\frac{da_{n}(t)}{dt}=i k n\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t)+G\left\{n(n-1)\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n-2}(t)-\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n+2}(t) \right\}.~~~~~(2)
$$ We define the $U_{n}(s)$ $$
U_{n}(s):=\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t).~~~~~(3)
$$ Then equation $(2)$ to be $$
sU_{n}(s)-a_{n}(0)=iknU_{n}(s)+
G\left\{n(n-1)U_{n-2}(s)-U_{n+2}(s) \right\}.~~~~~(4)
$$ where, we use following integration by parts $$
\int_{0}^{\infty}dt~\mathrm{e}^{-st}\frac{d a_{n}(t)}{dt}=sU_{n}(s)-a_{n}(0).
$$ Let's solve equation $(4)$ by using Z-transform. First, we define unilateral Z-transform $W(s,z)$ as follows: $$
\mathcal{Z}[U_{n}(s)]=
W(s,z):=\sum_{n=0}^{\infty} U_{n}(s)z^{-n}.~~~~~(5)
$$ Noting following relations Differentiation & Time delay $$
\mathcal{Z}[n(n-1)U_{n-2}(s)]=
2z^{-2}W(s,z)-2z^{-1}\frac{\partial W(s,z)}{\partial z}+\frac{\partial^{2}W(s,z)}{\partial z^{2}},
~~~~~~~~(6)
$$ Time advance $$
\mathcal{Z}[U_{n+2}(s)]=z^{2}W(s,z)-z^{2}U_{0}(s)-zU_{1}(s),
~~~~~~~~~(7)
$$ Using propaty of $a_{n}(0)=\delta_{mn}$ $$
\mathcal{Z}[a_{n}(0)]=C_{0}z^{-m},
~~~~~~~~~~(8)
$$ we can transform equation $(4)$ as follows: $$
Gz^{2}\frac{\partial^{2}W(s,z)}{\partial z^{2}}+(-2G-ikz^{2})z\frac{\partial W(s,z)}{\partial z}
+\left\{z^{2}(-Gz^{2}-s)+2G \right\}W(s,z)+Gz^{3}\left\{zU_{0}(s)+U_{1}(s) \right\} +C_{0}z^{-m+2}=0.
~~~~~~(9)
$$ Equation $(9)$ is some kind of Bessel equation. I'm trying to solve equation $(9)$ .","['delay-differential-equations', 'recurrence-relations', 'ordinary-differential-equations']"
3093668,An example of a two dimensional integrable distribution on $SO(3)$,"I've been reading Manifolds, Tensor Analysis, and Applications recently, and have a question about how to construct a two dimensional integrable distribution on $SO(3)$ . Let $M$ be a manifold, the Local Frobenius Theorem says that a subbundle $E$ of $TM$ is involutive if and only if it is integrable. So I'm trying to construct two vector fields $ X, Y $ defined on open sets of $SO(3)$ such that $[X, Y]$ take values in the distribution generated by $X, Y$ . This two vector fields $X, Y$ can not be left-invariant at the same time, otherwise they can be moved to the identity, and $[X,Y]$ can be computed directly by the Lie-bracket of $\mathfrak{so(3)}$ . Identifying $\mathfrak{so(3)}$ with $R^3$ by $$\left (
\begin{array}
&0 & -c & b \\
c & 0 &-a \\
-b & a & 0\\
\end{array}
\right ) \cong 
\left (
\begin{array}
& a\\b\\c\\
\end{array}
\right ) ,
$$ it can be verified that for any two linearly independent vectors $v_1, v_2$ in $R^3$ , $v_1 \times v_2$ can not be in $span\{v_1, v_2 \}$ , which means that $[X, Y]$ can not take values in the distribution generated by $X, Y$ . And I got stuck here. My questions are: For any given vector fields $ X, Y $ , how to compute $[X, Y]$ directly, where $X, Y$ are not assumed to be left-invariant; Do such a two dimensional integrable distribution exist? If yes, how can I construct it? If not, how to prove? Any hint or comment will be appreciate.","['smooth-manifolds', 'differential-geometry']"
