question_id,title,body,tags
4000248,Finding the limits of integration for the volume of a region inside a cube,"Premise We are restricted to the region $x, y, z \in [f, 1]$ where $0 \leq f < 1$ . The surface $y^2=4xz$ divides this cube into two regions. We are interested in finding the volume of the region where $y^2 - 4xz \geq 0$ . My Attempt Let $t=\frac{y^2}{4}$ for brevity. When $f=0$ , it is relatively easy to figure out the limits. We can separate the region into two where $x \leq t$ and $x > t$ . In the first case, we have $y^2 - 4xz \geq 0$ for all $z \in [0, 1]$ because $xz \leq x\cdot 1 \leq t$ . Similarly, in the second case, we have $y^2 - 4xz \geq 0$ for all $z \in [0, \frac{t}{x}]$ because $xz \leq x\cdot \frac{t}{x} = t$ . Applying these limits, we have $$
\begin{align}
V &= \int_{0}^{1}{dy\int_{0}^{t}{dx\int_{0}^{1}{dz}}} + \int_{0}^{1}{dy\int_{0}^{t}{dx \int_{0}^{\frac{t}{x}}{dz}}}\\
  &= \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot (1)}} + \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot \frac{t}{x}}}\\
  &= \frac{1}{12} + \frac{1 + 3\log{2}}{18} \\
  &\approx 0.254
\end{align}
$$ However, when $f > 0$ , I cannot seem to understand the proper limits for $z$ . Is it $f$ to $\frac{t}{x}$ or is the lower limit $f^2$ ? What happens when $f$ is less than or equal to $\frac{y^2}{4}$ ? I tried a few times, but the results ended up being negative, which is clearly wrong as volumes are nonnegative. Specific Questions Primary objective I would like help in figuring out the limits in the case where $f>0$ . Secondary objectives Is this approach sensible? Perhaps there is a better way to solve this problem? Are there any glaring oversights? What books/courses/reference should I follow to be able to solve these types of problems?","['integration', 'definite-integrals', 'volume', 'limits', 'inequality']"
4000258,Relation between Rogers Ramanujan continued fraction and $j$-invariant,"While going through this answer I found an interesting but slightly complicated relation between Rogers-Ramanujan continued fraction and the j-invariant. I would like to know an elementary proof of the same. Before proceeding let me define all the necessary terms and symbolism to set the proper context. Let $\tau$ be a complex number with positive imaginary part and $q=\exp(2\pi i\tau) $ so that $|q|<1$ . Below I define functions and the relations which I am aware of. The Rogers-Ramanujan continued fraction is given by $$R(q) =\cfrac{q^{1/5}}{1+\cfrac{q}{1+\cfrac{q^2}{1+\cfrac{q^3}{1+\dots}}}}\tag{1}$$ Ramanujan studied this function in great detail and obtained the following fundamental identities $$\frac{1}{R(q)}-1-R(q)=\frac{\eta(q^{1/5})} {\eta(q^5)}\tag{2}$$ and $$\frac{1}{R^5(q)}-11-R^5(q)=\left(\frac{\eta(q)}{\eta(q^5)}\right)^6\tag{3}$$ where $\eta(q) $ is Dedekind eta function defined by $$\eta(q) =q^{1/24}\prod_{n=1}^{\infty} (1-q^n)\tag{4}$$ To define the $j$ -invariant we need to introduce Ramanujan's version of Eisenstein series denoted by $L, M, N$ (symbols $P, Q, R$ are typically used but we want to avoid conflict with Rogers-Ramanujan continued fraction $R(q) $ ) \begin{align}
L(q) &= 1-24\sum_{n=1}^{\infty}\frac{nq^n}{1-q^n}\tag{5a}\\
M(q)&=1+240\sum_{n=1}^{\infty} \frac{n^3q^n}{1-q^n}\tag{5b}\\
N(q) &=1-504\sum_{n=1}^{\infty} \frac{n^5q^n}{1-q^n}\tag{5c}
\end{align} It should be observed that $L$ is related to $\eta$ via $$L(q) =24q\frac{d}{dq}(\log\eta(q))\tag{6}$$ The $j$ -invariant is defined as $$j(q) =\frac{1728M^3(q)}{M^3(q)-N^2(q)}\tag{7}$$ Ramanujan obtained a system of differential equations connecting $L, M, N$ : \begin{align}
q\frac{dL(q) } {dq} &=\frac{L^2(q)-M(q)}{12}\tag{8a}\\
q\frac{dM(q)}{dq}&=\frac{L(q)M(q)-N(q)}{3}\tag{8b}\\
q\frac{dN(q)} {dq} &=\frac{L(q) N(q) - M^2(q)}{2}\tag{8c}
\end{align} Using $(6)$ and $(8a)$ it is evident that $M(q) $ can also be expressed in terms of $\eta(q) $ . On the other hand the above differential equations allow us to prove that $$M^3(q)-N^2(q)=1728\eta^{24}(q)\tag{9}$$ and thus we have some expression for $j(q) $ in terms of $\eta(q) $ . The following complicated relation holds between Rogers Ramanujan continued fraction $R(q)$ and $j(q) $ : $$ R^5 (R^{10}+11 R^5-1)^5j+(R^{20}-228 R^{15}+494 R^{10}+228 R^5+1)^3 = 0\tag{10}$$ I checked Wikipedia and found that this is derived from another identity $$j(q) =\frac{(x^2+10x+5)^3} {x} \tag{11}$$ where $$x=125\left(\frac {\eta(q^5)}{\eta(q)}\right)^6\tag{12}$$ Using $(11),(12)$ and $(3)$ we can deduce $(10)$ with a little algebra. Thus the problem boils down to a proof of equation $(11)$ . I don't know if this can be derived using algebraic manipulation of the identities given above or does it need some specific modular equation. Any proofs or suggestions for proof are welcome. I don't understand the machinery of modular forms properly and would prefer an approach more in the spirit of Ramanujan. The question is however tagged ""modular-forms"" to get the attention of experts from that tag. Update : I have finally managed to give a proof based on modular equation of degree $5$ given by Ramanujan and posted it as an answer. The proof is more of a verification and a more natural proof utilizing some transformation formula of eta function is desired. There is a related question which assumes $(10),(11)$ and proves $(3)$ , but the approach uses Mathematica.","['q-series', 'modular-forms', 'sequences-and-series']"
4000296,What does the null hypothesis $H_0:\beta_2=0$ mean?,For the model $$Y=\beta_1+\beta_2X_2+\beta_3X_3+\epsilon$$ what does the null hypothesis $H_0:\beta_2=0$ mean? I think that then $X_2$ would not affect the expected value of $Y$ since the coefficient os then $0$ . But is that correct?,"['statistics', 'hypothesis-testing']"
4000304,Lipschitz implies bounded gradient with any norm?,"Assume I have a differentiable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ that is $L$ -Lipschitz with respect to a norm $\|\cdot\|$ on $\mathbb{R}^n$ : $$ \forall x, y \in \mathbb{R}^n, |f(x) - f(y)| \le L\|x-y\|.  $$ I know that if $f$ is further assumed to be convex, and $\|\cdot\|$ is the euclidian norm, then convexity implies $$ L\|\nabla f(x)\|_2 \ge f(x + \nabla f(x)) - f(x) \ge \langle \nabla f(x), \nabla f(x)\rangle = \| \nabla f(x)\|_2^2, $$ thus $\|\nabla f(x)\|_2 \le L$ . But does $\|\nabla f(x)\| \le L$ still hold if $\|\cdot\|$ is any norm on $\mathbb{R}^n$ ? And if $f$ is only assumed differentiable?","['derivatives', 'lipschitz-functions', 'real-analysis']"
4000319,Integration along fibers,"The following statements are  from the book heat kernel and dirac operator chapter 1. "" Let $\pi : M \rightarrow B $ be a fiber bundle with n-dimensional fiber, such that both M and B are oriented. If $\alpha \in {A}_c^k(M)$ is a compactly-supported differential form on M, its integral over the fibers of $ M \rightarrow B $ is the differential form $\int\limits_{M/B} \alpha \in A^{k-n}(B)$ such that \begin{equation} 
\int\limits_B (\int\limits_{M/B} \alpha) \wedge \beta = \int\limits_M \alpha \wedge \pi^* \beta \quad  ... (1.15)
\end{equation} for all differential forms $\beta$ on the base B. We sometimes write $ \pi_* \alpha $ instead of $ \int\limits_{M/B} $ . It follows easily from the (1.15) that \begin{equation} 
\pi_*(\alpha \wedge \pi^* \beta ) = \pi_* \alpha \wedge \beta \quad ...(1.16)
\end{equation} for all $\alpha \in A_c(M) $ and $\beta \in A(\beta).$ "" My questions are the following: what is the intuition behind defining the integral of $\alpha$ over the fibers by the equation (1.15), and why the notion of integration along fiber  is important ? how to prove that equation 1.15 implies equation 1.16 ?","['integration', 'fiber-bundles', 'differential-forms', 'differential-geometry']"
4000405,find the number of possible positive integer solutions when the inequality is $a \times b \times c \lt 180$,"As most of you know there is a classical question in elementary combinatorics such that if $a \times b  \times c = 180$ , then how many possible positive integer solution are there for the equation $?$ The solution is easy such that $180=2^2 \times 3^2 \times 5^1$ and so , for $a=2^{x_1} \times 3^{y_1} \times 5^{z_1}$ , $b=2^{x_2} \times 3^{y_2} \times 5^{z_2}$ , $c=2^{x_3} \times 3^{y_3} \times 5^{z_3}$ . Then: $x_1+x_2+x_3=2$ where $x_i \geq0$ ,  and $y_1+y_2+y_3=2$ where $y_i \geq0$ and $z_1+z_2+z_3=1$ where $z_i \geq0$ . So , $C (4,2) \times C(4,2) \times C(3,1)=108$ . Everything is clear up to now.However , i thought that how can i find that possible positive integer solutions when the equation is $a \times b  \times c \lt 180$ instead of $a \times b  \times c = 180$ After , i started to think about it. Firstly , i thought that if i can calculute the possible solutions for $x_1+x_2+x_3 \lt2$ where $x_i \geq0$ ,  and $y_1+y_2+y_3 \lt 2$ where $y_i \geq0$ and $z_1+z_2+z_3 \lt1$ where $z_i \geq0$ , then i can find the solution.However , there is a problem such that when i calculate the solution , i do not include the prime numbers and their multiplicites which is in $180$ . For example , my solution does not contain $1 \times 1 \times 179 \lt 180$ My question is that how can we solve these types of question . Is there any $\color{blue}  {\text{TRICK}}  $ for include all possible ways ? Moreover ,this question can be generalized for $a \times b  \times c \leq 180$ , then what would happen for it ? Thanks for helps..","['combinations', 'combinatorics', 'problem-solving', 'discrete-mathematics']"
4000464,The Hilbert function of a set of general points in projective $r$-space.,"A set of $d$ points $X\subset \mathbb P^r$ is said to be in general position if there are no more than $2$ points of $X$ on any line, no more than $3$ points on any $2$ -plane,..., no more than $r$ points in an $(r-1)$ -plane. Let $X$ be the union of $d$ general points in $\mathbb P^r$ . Is it true that $$H_X(s)=\min\left\{d,\binom{r+s}r\right\}\tag{$\star$}\label{star}?$$ For example, if $X$ consists of $3$ non-collinear points in $\mathbb{P}^2$ , then $H_X(s)=3$ for all $s\in\mathbb N$ [1] . If $X$ consists of $4$ points that are not collinear, then $$H_X(s)=\begin{cases}3 & s=1\\ 4& s \geq 2\end{cases}.$$ One can also see that $(\ref{star})$ holds for a set of $7$ (or less than $7$ ) general points in $\mathbb P^3$ . Now let $X\subset \mathbb P^3$ be the union of $8\,(\gt 2\cdot3+1)$ general points. According to $(\ref{star})$ , we should have $H_X(2)=8$ . But I don't see it and I suspect it is not true. Probably it has to do with the following fact: Let $X$ be a set of $d\le 2r+1$ points in $\mathbb{P}^r$ in linearly
general position. Then the space of quadratic forms vanishing on $X$ is $\binom{r+2}{r}-d$ dimensional (i.e. $H_X(2)=d$ ) [2] . By the way, I saw $(\ref{star})$ in the following paper: Generators for the Homogeneous Ideal of $s$ General Points in $\mathbb P^3$ . Can someone tell me what's going on? Is $(\ref{star})$ true? Edit : I think I'm misinterpreting generic $d$ -position as general position. This paper discusses the two notions on page number 534. So it looks like there are counterexamples to the above equality.","['algebraic-geometry', 'commutative-algebra']"
4000479,"Is the Arabic ""4"" a value, numeral, or numerical expression?","What I understand: A number is an abstract concept. We use numerals such as names (i.e. cuatro), symbols (i.e. Arabic ""4""), and numerical expressions (i.e. ""9 - 5"") to represent numbers. Where I get tripped up: ""Value"" is defined as the number a numeral represents, where ""4"" is a value of ""3 + 1"". Doesn't this contradict the author's earlier definition of ""4"" being a symbol ? Is ""4"" a number or a numeral? Bonus Question ""4"" is then defined as a numerical expression.
Is a numerical expression a type of numeral, or are they synonymous to one another? I value your insight!","['real-numbers', 'algebra-precalculus', 'terminology']"
4000485,How to express the covariant derivative on $TM$ by the covariant derivative on $M\times M$?,"Let $(M,g,\Gamma)$ be a Riemannian manifold with the Levi-Civita connection. $M\times M$ and $TM$ have natural metric structrures inherited from $(M,g,\Gamma)$ . Let $\phi:TM \rightarrow M \times M$ be defined as follows: $$ \phi(z,u) = \big(\exp_z(u),\exp_z(-u)\big)$$ where $\exp_z$ is the exponential map $\exp_z : T_zM \rightarrow M $ . At least in some neighbourhood of the section $u=0$ it is invertible. Let $F$ be a tensor field on $M\times M$ . Let $G$ be a tensor field on $TM$ defined as $$ G = \phi^*F$$ where $\phi^*$ is the pullback/pushfoward of a tensor field (at least in the domain where $\phi$ is invertible). Let $ v \in T_{(z,u)}TM$ . My question is: how to write the covariant derivative $\nabla_{v} G$ in terms of the covariant derivative of $F$ ? My first thought was to just write $$ \nabla_{v} G = \phi^*(\nabla_{d\phi(v)} F) $$ but since $\phi$ doesn't seem to be a homomorphism of the metric structures on $TM$ and $M\times M$ , the covariant derivative on $TM$ is not a pullback of the covariant derivative on $M\times M$ , so I don't think that's the correct formula. Still, the metric structures on $TM$ and $M\times M$ both originate from the same structure on $M$ , so I think there must be some relation. EDIT: I was able to conclude that $$ \nabla_{v} G - \phi^*(\nabla_{d\phi(v)} F) = -\frac{d}{dt}\Big|_{t=0} \Big( (\phi^*\tilde P^{\phi(\gamma)}_t) (P^\gamma_t)^{-1} G\Big) $$ where $\gamma$ is any curve on $TM$ such that $\gamma'(0) = v$ , $P^\gamma_t$ is the parallel transport on $TM$ along $\gamma$ from point $\gamma(t)$ to point $\gamma(0)$ , and $\tilde P^{\phi(\gamma)}_t$ is the parallel transport on $M\times M$ along the curve $\phi(\gamma)$ from point $\phi(\gamma(t))$ to $\phi(\gamma(0))$ . I still don't know how to calculate this derivative $\frac{d}{dt}$ and express it in terms of, for example, the Riemann tensor, or at least the Synge's function.","['riemannian-geometry', 'connections', 'tensors', 'derivatives', 'tangent-bundle']"
4000523,Right versus left derivative by increasing one-sided derivatives,"Condider a map $f:D\to\mathbb{R}$ , $D$ open interval.
Let be $y,z\in D$ , $y<z$ . Suppose that $f$ has everywhere in $D$ both left and right derivative (this implies that $f$ is continuous), both increasing .
I want to show that $$
f'(y^+)\le f'(z^-).
$$ I have found a way, but I'm not completely convinced that it is formally correct: $$
f'(y^+)
=\lim_{\varepsilon\to 0^+}\frac{f(y+\varepsilon)-f(y)}{\varepsilon}
\overset{\forall\delta}{\le} 
  f'((z-\delta)^+)
= \lim_{\varepsilon\to 0^+}\frac{f(z-\delta+\varepsilon)-f(z-\delta)}{\varepsilon}
\overset{\varepsilon=\delta}{=} \lim_{\varepsilon\to 0^+}\frac{f(z)-f(z-\varepsilon)}{\varepsilon}
=f'(z^-)
$$ where $\delta>0$ and $y<z-\delta$ . Is there a way to formalise my proof?","['limits', 'solution-verification', 'derivatives', 'real-analysis']"
4000537,Terence Tao Analysis 1 Excercise 8.3.2 Proof Verification,"I am working on excercise 8.3.2 in Tao Analysis 1. This is my answer to the excercise. The question statement is: Let $A$ , $B$ , $C$ be sets such that $A \subseteq B \subseteq C$ , and suppose that there is an injection $f : C → A$ . Define the sets $D_0$ , $D_1$ , $D_2$ ,... recursively by setting $D_0 := B\setminus A$ , and then $D_{n+1} := f(D_n)$ for all natural numbers $n$ . Prove that the sets $D_0$ , $D_1$ ,... are all disjoint from each other (i.e., $D_n ∩ D_m = ∅$ whenever $n \neq m$ ). Also show that if $g : A → B$ is the function defined by setting $g(x) := f^{−1}(x)$ when $x \in \cup_{n=0}^{\infty}D_n$ , and $g(x) := x$ when $x \not \in \cup_{n=0}^{\infty}D_n$ , then $g$ does indeed map $A$ to $B$ and is a bijection between the two. In particular, $A$ and $B$ have the same cardinality. My work: The first step is to prove all the sets are disjoint from each other. Assume there are two sets $D_{n_0}$ and $D_{m_0}$ are not disjoint. The aim is to prove this contradicts the injectivity of $f$ .
Assume without loss of generality that $n_0 > m_0$ . Define the two sequences $\{n_{k}:k \in \mathbb{Z}^{+}$ } and $\{m_{k}:k \in \mathbb{Z}^{+}$ } as follows: $n_k = n-k$ , $m_k = m-k$ . Consider $D_{n_1}$ and $D_{m_1}$ . If they are disjoint, stop here, if not, consider $D_{n_2}$ and $D_{m_2}$ and then $D_{n_3}$ and $D_{m_3}$ , etc... until for some $k$ , we find that $D_{n_k}$ and $D_{m_k}$ are disjoint (and such a k does exist, because if $D_{n_k}$ and $D_{m_k}$ are not disjoint for all $k<m$ , then for $k=m$ , we have the two sets $D_{n_m}$ and $D_0$ which are certainly disjoint). Now we have arrived at the following: For the $k$ that was found, $D_{n_{k-1}} \cap D_{m_{k-1}} \neq \phi$ and $D_{n_{k}} \cap D_{m_{k}} = \phi$ . So we have an $x$ such that $x \in D_{n_{k-1}}$ and $x \in D_{m_{k-1}}$ and so $x = f(y_1) = f(y_2)$ for some $y_1 \in D_{n_k}$ and $y_2 \in D_{m_k}$ . Since $D_{n_{k}} \cap D_{m_{k}} = \phi$ , $y_1 \neq y_2$ . We have found that $y_1 \neq y_2$ with $f(y_1) = f(y_2)$ , contradicting the injectivity of $f$ . So all the sets are piecewise disjoint. Next, given the function $g: A → B$ , need to prove $g$ is both injective and surjective. For injectivity, if $x_1 \neq x_2$ , then there are four possible cases: $\quad$ 1) $x_1 \in \cup_{n=0}^{\infty} D_n$ and $x_2 \in \cup_{n=0}^{\infty} D_n$ . $\quad$ 2) $x_1 \in \cup_{n=0}^{\infty} D_n$ and $x_2 \not \in \cup_{n=0}^{\infty} D_n$ $\quad$ 3) $x_1 \not \in \cup_{n=0}^{\infty} D_n$ and $x_2 \in \cup_{n=0}^{\infty} D_n$ $\quad$ 4) $x_1 \not \in \cup_{n=0}^{\infty} D_n$ and $x_2 \not \in \cup_{n=0}^{\infty} D_n$ In the first case, $g(x_1) = f^{-1}(x_1)$ and $g(x_2) = f^{-1}(x_2)$ . Clearly, $g(x_1) \neq g(x_2)$ or else the same element would map to two different images under $f$ . In the second case, $g(x_1) = f^{-1}(x_1) \in \cup_{n=0}^{\infty} D_n$ and $g(x_2) = x_2 \not \in \cup_{n=0}^{\infty} D_n$ so $g(x_1) \neq g(x_2)$ . The third case is similar to the second case, so its omitted. In the last case, $g(x_1) = x_1$ and $g(x_2) = x_2$ so $g(x_1) \neq g(x_2)$ . This proves that $x_1 \neq x_2 \implies g(x_1) \neq g(x_2)$ and $g$ is injective. To prove $g$ is surjective, let $x \in B$ . Then $x \in B \setminus A$ or $x \in A$ . If $x \in B \setminus A = D_0$ , then $\exists y_1 \in D_1$ such that $f(x)=y_1$ and $x = f^{-1}(y_1)$ so $x = g(y_1)$ . If $x \in A$ , then $x \in \cup_{n=0}^{\infty} D_n$ or $x \not \in \cup_{n=0}^{\infty} D_n$ . If $x \in \cup_{n=0}^{\infty} D_n$ , then $x \in D_n$ for some  and there exists a $y \in D_{n+1}$ such that $y = f(x)$ so that $x = f^{-1}(y)$ and $x = g(y)$ . If $x \not \in \cup_{n=0}^{\infty} D_n$ , then $g(x) = x$ . This shows that for every $x \in B$ , there exists a corresponding $y \in A$ such that $x = g(y)$ , and $g$ is surjective. Is this correct? Thanks for your time.",['elementary-set-theory']
4000602,Order of calculation about nested absolute values,"In case of nested absolute values, is there a difference between working first on the outer abs value and the opposite? $$ \frac{|2x^2-3|x|+1|}{|x|+1} = 1 $$ Should I consider first the inner one? My real solutions for this equation are $\left \{0, \pm 2\right \}$ , and I've found those working first with the inner one.","['algebra-precalculus', 'absolute-value']"
4000723,Problem in the theory of finding the unique solution of $-u'' = \delta_0$ and $-u'' + cu= \delta_0$,"I have theese two problems: (I) Find the unique distribution $u \in H^1(-1,1)$ such that $-u'' = \delta_0$ and $u(-1) = u(1) = 0$ . (II) Let $c > 0$ , find the unique distribution $u \in H^1(\mathbb{R})$ such that $-u'' + cu= \delta_0$ Because we want to use Lax-Milgram's theorem, solving theese problem we began like this: $(I)$ $u \in H^1(-1,1)$ satisfies $-u'' = \delta_0$ in the distributional way if and only if for all $\varphi \in C_c^\infty(-1,1)$ $$\begin{cases}
u \in H^1_0(-1,1)\\[8pt]
\int_{-1}^1 u' \varphi' \ dx = \varphi(0)
\end{cases}$$ $(II)$ $u \in H^1(\mathbb{R})$ satisfies $-u'' +cu= \delta_0$ in the distributional way if and only if for all $\varphi \in H^1(\mathbb{R})$ $$
\begin{cases}
u \in H^1(\mathbb{R}) \\[8pt]
\int_{-1}^1 u'\varphi' +cu\varphi \ dx = \varphi(0)
\end{cases}$$ My questions are: Why in $(I)$ we took $\color{red} {u \in H^1_0(-1,1)}$ and not $u \in H^1(-1,1)$ ? Why in $(II)$ we took $\color{red}{\varphi \in H^1(\mathbb{R})}$ and not $\varphi \in C_c^\infty(\mathbb{R})$ ? What is the general approach to be sure to choose ""the right space for the right function"" in this kind of problems? I'm studying a course of a master's degree (Italy) in mathematics and as you may observe in my last questions on this site, this course was really linked to the solutions of the Dirichlet problem. Introducing the ""weak derivateive"" of a function we mentioned that distributions live in spaces where there is not a normal topology to define continuity (taken for example from a norm) but we did not go deeper. We defined the Sobolev space $W^{k,p}(\Omega)$ as follows: $$W^{k,p}(\Omega) := \{u\in L^p(\Omega) \ : \ D^\alpha u \in L^p(\Omega) \ \text{for all}\  \alpha : |\alpha| \le k\}$$ where $D^\alpha u$ is used as a notion of the distributional derivative of $u$ , that in our definition is the distribution that acts on every $\varphi \in C^\infty_c(\Omega)$ as $$\langle D^\alpha u , \varphi \rangle := (-1)^{|\alpha|}\langle u , D^\alpha \varphi \rangle$$ We define $H^k(\Omega) := W^{k,2}(\Omega)$ and $W^{k,p}_0(\Omega)$ is the closure of $C^\infty_c(\Omega)$ inside $W^{k,p}(\Omega)$ Here I state Lax-Milgram theorem: Let $H$ be an Hilbert real space and let $a:H\times H \to \mathbb{R}$ be a bilinear, continous and coerced form. Then for all $F\in H^*$ there exists one and one only $\bar{u} \in H$ such that $$a(\bar{u},v) = F(v) \qquad \forall v \in H$$ If $a$ is also symmetric then $\bar{u}$ satisfies $$\frac{1}{2}a(\bar{u},\bar{u}) - F(\bar{u}) = \min_{v\in H} \Big\{\frac{1}{2}a(v,v) + F(v)\Big\}$$","['sobolev-spaces', 'ordinary-differential-equations', 'distribution-theory']"
4000724,Limit to $e^2$.,"I have to show $$
\lim_{x \to \infty} {\left(\frac{x^2 + 1}{1 - x^2}\right)}^{x^2} = e^2,
$$ but I don't get the trick to see it, I suppose I can use something like $$
{\left(\lim_{x \to \infty} {\left(1 + \frac1x\right)}^x\right)}^2 = e^2,
$$ but I do not see how I can apply to the problem. On the other hand, $$
\frac{x^2 + 1}{1 - x^2} = 1 + \frac{2 x^2}{1 - x^2}.
$$ Any hint?",['limits']
4000814,"Everyone is passed everything exactly once, but never from the same person","Say 4 or more people are sitting around a table. Each has a sheet of paper. Devise an algorithm to pass these papers between these people that guarantees: Each person passes and signs every piece of paper exactly once, no more, no less Every time the papers are passed, each person receives a paper from a person they have not received it from before The papers must all be passed the exact same number of times as there are people (i think this follows from 1, just making sure) People can only have one paper at a time; in other words, no person can be signing two papers at the same time. Questions: Is there a mathematical name for this problem? Or can anyone find internet pages where this problem has been discussed before? Is this possible for every number of players? If so, is there an algorithm to generate this? If not, is there an algorithm that can at least minimize the number of times people receive a paper from someone they have received it from before? This has real world implications!!! I would like to use this theoretical algorithm in my game Drawphone, more info here about how it fits in. Thanks! :)","['graph-theory', 'combinatorial-designs', 'combinatorics', 'permutations']"
4000842,"How few $(42^\circ,60^\circ,78^\circ)$ triangles can a regular hexagon be divided into?","While working on an answer to this excellent post by Edward H. on dissections of an equilateral triangle into similar triangles with angles of $42^\circ, 60^\circ,$ and $78^\circ$ , I wondered about a related question: what if we instead try to tile a regular hexagon? Obviously, any solution for the triangle with $k$ tiles gives a solution of size $6k$ . However, we can do much better than this. At the moment, the best solution I know of makes uses of a shape first discovered by Anders Kaseorg while working on the triangular case, namely that of a $60^\circ-120^\circ$ rhombus constructed from 72 tiles: Putting three of these parallelograms together yields a hexagon with $\textbf{216}$ triangles. However, I think it may well be possible to do substantially better than this. I tried a few methods and haven't yet been able to beat $216$ , but just to demonstrate that arrangements of similar size can be achieved with more asymmetric configurations, here is a $274$ -triangle arrangement I found: It seems quite plausible to me that more clever arrangements of triangles could get below $216$ , perhaps substantially so. I highly recommend reading the previous question and its answers for useful background information and construction strategies before embarking on this problem.","['recreational-mathematics', 'puzzle', 'geometry', 'tiling']"
4000867,Are elliptic curve irreducible?,"Suppose we have a typical elliptic curve (smooth, projective curve with a distinguished point $\mathcal{O} = [0,1,0]$ )  over some algebraically closed field $\overline{K}$ $$E: F(X,Y,Z) = Y^2Z + a_1XYZ + a_3YZ^2 - X^3 - a_2X^2Z - a_4XZ^2 - a_6Z^3 =0$$ is $F$ irreducible in $\overline{K}[X,Y,Z]$ ? I cant find anything definitive online.","['number-theory', 'algebraic-geometry', 'geometry', 'curves']"
4000916,Divisibility by n for 3- digit number,"We have 3 numbers: $\; \overline{xyz}=100x+10y+z$ , $\; \overline{yzx}$ and $\; \overline{zxy}$ , all 3 divisible by the same number $n$ . Prove that the number $P=x^3+y^3+z^3-3xyz$ is also divisible by n. $n \mid 100x+10y+z$ $n \mid 100y+10z+x$ $n \mid 100z+10x+y$ $n \mid 111x+111y+111z \Leftrightarrow n \mid 111(x+y+z)$ but since 111 is not a prime, we can't say for sure if $n \mid (x+y+z)$ . Also $P = \frac{1}{2}(x+y+z)((x-y)^2+(y-z)^2+(z-x)^2)$ . So if I managed to prove that $n \mid (x+y+z)$ I think it would be sufficient, right? Can you help me?","['number-theory', 'divisibility']"
4000944,Multiple integration with Dirac delta,"I am reading a paper and trying to solve collision integral. In Appendix A, there is an integral, $$ I^{(n)} = \int \frac{p^2_1\,dp_1 \,p^2_3\,dp_3\,p^2_4\,dp_4}
{2E_12E_32E_4} 2\pi \delta (p_1 − p_3 − p_4)e^{−E_1/T} I_\Omega,$$ where \begin{align}
I_\Omega 
&=\left( 4\pi \right)^3 \int d^3\lambda \, \frac{\sin p_1\lambda }{p_1\lambda}\frac{\sin p_3\lambda }{p_3\lambda}\frac{\sin p_4\lambda }{p_4\lambda} \\ 
&=\dfrac{32\pi^5}{p_1p_3p_4}\Bigg[\dfrac{\left( p_1-p_3+p_4\right) }{\left| p_1-p_3+p_4\right|}+\frac{(p_1+p_3-p_4) }{| p_1+p_3-p_4|}-\frac{(p_1-p_3-p_4) }{|p_1-p_3-p_4|}-\frac{(p_1+p_3+p_4) }{|p_1+p_3+p_4|}\Bigg].
\end{align} They get a result $$I^{(n)}=\int \frac{16\pi^6 p_1^2}{E_1}e^{-E_1/T} \, dp_1$$ after integrating out dirac delta. I can't seem to understand how do they remove both $p_3$ and $p_4$ using one delta integral. Also, since all the terms in brackets of $I_\Omega$ are positive, it seems that term goes to zero. Can anyone help? Note: $E_i$ and $p_i$ are energy and momentum of $i^{th}$ particle and here it is assumed that $m_3 =0$ and $m_4=0$ hence $E_3 = \sqrt{p_{3}^{2}+m_{3}^{2}}=p_{3}
$ & similarly $E_4 = p_4$ .","['integration', 'dirac-delta', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4000979,Composition of inverse trigonometric functions with trigonometric functions,"I found in this Wikipedia article a useful table showing the algebraic expressions for the composition of trigonometric functions with inverse trigonometric functions, along with a picture explaining where they come from. Among others, $$\begin{align}
\sin({\arccos{x}})=\sqrt{1-x^2}, \quad \sin({\arctan{x}})=\frac{x}{\sqrt{1+x^2}}\\
\cos({\arcsin{x}})=\sqrt{1-x^2}, \quad \cos({\arctan{x}})=\frac{1}{\sqrt{1+x^2}}\\
\tan({\arcsin{x}})=\frac{x}{\sqrt{1-x^2}}, \quad \tan({\arccos{x}})=\frac{\sqrt{1-x^2}}{x}\\
\end{align}$$ What would be, instead, the algebraic expressions for the composition of inverse trigonometric functions with trigonometric functions, such as the followings? Could someone provide them or give a source where they can be obtained? $$\begin{align}
\arcsin({\cos{x}})=?, \quad \arcsin({\tan{x}})=?\\
\arccos({\sin{x}})=?, \quad \arccos({\tan{x}})=?\\
\arctan({\sin{x}})=?, \quad \arctan({\cos{x}})=?\\
\end{align}$$","['trigonometry', 'inverse-function', 'function-and-relation-composition']"
4001006,Elementary proof of Power Rule For differentiation $f'(x)=rx^{r-1}$ for $f(x)=x^r$,"I could  not find any elementary proof of the Power Rule for differentiation: Given $x,r \in R$ , $x>0$ and a function $f(x)=x^r$ , then its derivative is $f'(x)=rx^{r-1}$ Defintion :Let a sequence of rational numbers { $r_n$ } tend to $r$ then $x^r=\lim_{r_n \to r}x^{r_n}$ All the proofs I had seen utilized the derivative of $logx$ but I dont think this is  elementary because one has to show that for $e=\text{Euler's number}$ , $t \in R$ and $n \in N$ $\lim_{n \to \infty}(1+\frac{1}{n})^n=\lim_{t \to 0}(1+t)^\frac{1}{t}=e$ which can be proved using the Power Rule Ofcourse the proof for the Power Rule is straight forward if one is familiar with the result of Power Rule if $r$ is a rational number and theorem  regarding  derivative of a sequence of derivatives that are uniformly convergent.","['derivatives', 'real-analysis']"
4001012,"Find geometric derivation of $\rho(a,b)=\frac{2|a-b|}{\sqrt{1+|a|^2}\sqrt{1+|b^2|}}$ for stereographic projection.","When the complex plane is projected to the spherical surface, we can brute force the formula for the the distance between the two image points $a,b$ on the sphere $d(a,b)=\frac{2|a-b|}{\sqrt{1+|a|^2}\sqrt{1+|b^2|}}$ In retrospect, we see the factors appearing in this distance formula represent lengths. For instance $\sqrt{1+|a|^2}$ is the distance from North Pole of the unit sphere to $a$ in the complex plane. Can we derive this formula by pure geometric argument?","['geometry', 'trigonometry', 'stereographic-projections', 'problem-solving', 'complex-numbers']"
4001034,Calculate the dimensions of a rotated rectangle inside a bounding box,"I'm trying to figure out if it's possible to calculate the width & height of the gray rectangle if I only know the width & height of the dotted red rectangle and the angle of the rotation. I've tried substituting values for the width and height and using $x \cos(\alpha) + y \sin(\alpha)$ and attempting to figure out a scaling factor, but I'm not really getting anywhere.",['trigonometry']
4001059,Which number is greater? $2\sqrt{2}$ or $e$,"I have to determine which number is greater, $2\sqrt{2}$ or $e$ . I had a similar question as well, it was $2^\sqrt{2}$ compared to $e$ . For that one I managed to prove the inequality by using the increasing sequence converging to $e$ : $(1+\frac1n )^n $ So I just searched for a value to assign to n such that $(1+\frac1n )^n \gt 2^\sqrt2$ I tried to proceed in a similar way with $2\sqrt{2} \gt \lt e$ , but it seems I can't get nowhere (I used the sequence decreasing and converging to $e$ : $(1+\frac1n )^{n+1}$ ) Is there another way to prove the inequality without the use of the calculator and maybe using derivatives? The question was in a derivatives file, so I'm wondering is there's a way to get to the end using them. Any hint would be much appreciated, thanks.","['constants', 'calculus', 'derivatives', 'inequality']"
4001078,Counting chessboard rectangles,"I am looking into the puzzle count the number of rectangles in a regular $8*8$ chessboard. For a 1 by 1 chessboard there are 0 rectangles For a 2 by 2 chessboard there are 4 rectangles (2 by 1) For a 3 by 3 chessboard there are 6 rectangles of 1 by 2, 3 rectangles of 1 by 3, 6 rectangles of 2 by 1 and 3 rectangles of 3 by 1 i.e. total 22 For a 4 by 4 rectangles there are 56 rectangles in total (12 of 1 by 2, 12 of 2 by 1, 8 of 2 by 4, 8 of 4 by 2, 4 of 1 by 4, 4 of 4 by 1, 2 of 2 by 4, 2 of 4 by 2, 2 of 3 by 4, 2 of 4 by 3). So we have the following sequence (4, 22, 56, ...): 2 $\times$ 2 3 $\times$ 3 4 $\times$ 4 5 $\times$ 5 6 $\times$ 6 7 $\times$ 7 8 $\times$ 8 4 22 56 I can't see a pattern in the sequence. Is there one that I am missing?","['puzzle', 'chessboard', 'discrete-mathematics', 'sequences-and-series']"
4001084,Regular Lagrangian flows on a domain of $\mathbb{R}^d$ with a boundary,"I'm looking for some references about the theory of regular Lagrangian flows on a smooth domain $\Omega$ of $\mathbb{R}^d$ (say a smooth bounded open set of $\mathbb{R}^d$ or a half space). Here, regular Lagrangian flows refer to the ones developped in the theory of Ambrosio which intends to explore the link between continuity equations : $\partial_t \rho +\mathrm{div}_x(b \rho)=0$ and ordinary differenrial equations : $\frac{d}{d t} X(t,x)=b(t,X(t,x))$ with $X(0,x)=x $ , for a vector field $b=b(t,x)$ which is not smooth (namely with Sobolev or BV regularity in space). See for instance Definition 13 p.13 in http://php.math.unifi.it/users/cime/Courses/2005/02/CIME-2005-Ambrosio-Lecture_Notes.pdf It actually extends the famous results of Di Perna and Lions of 1989 on transport and continuity equations. But all of these theories actually take place in $\mathbb{R}^d$ , namely the fixed vector field $b$ (and then $\rho=\rho(t,x)$ and $X=X(t,x)$ ) are defined (for the space variable) on the whole space i.e. $b : \mathbb{R}^+ \times\mathbb{R}^d \rightarrow \mathbb{R}^d$ . However, the theory of Di Perna and Lions can be extended to domains included in $\mathbb{R}^d$ and which have a boundary , by constructing renormalized solutions to these equations where there is of course an additional boundary condition (see for instance https://hal.archives-ouvertes.fr/hal-00004420/document for results on a bounded domain with an absorption boundary condition) I am therefore wondering if regular Lagrangian flows can be also defined and used on a domain with a boundary.","['transport-equation', 'measure-theory', 'ordinary-differential-equations', 'partial-differential-equations']"
4001180,What does this: $\{a\}\doteq\emptyset$ mean to you? Is this notation acceptable in any sense?,"I'm kind of embarassed for asking this question, but... The problem: I'm having a hard time trying to establish a consistent notation for the spectral decomposition of the elements of a second-order cone $L_m\doteq \{(x_0,\bar{x})\in \mathbb{R}\times \mathbb{R}^{m-1} \colon x_0\geq \|\bar{x}\| \}$ when $m=1$ . I could not find a standard treatment for this case anywhere. Context: It is quite well-known that every $x\in L_m$ can be decomposed as $x=\lambda_1(x)v_1(x) + \lambda_2(x)v_2(x)$ , where $\lambda_i(x)=x_0+(-1)^i\|\bar{x}\|$ and $v_i(x)=\frac{1}{2}(1,(-1)^iw)$ , with $$w=\left\{\begin{array}{ll}\bar{x}/\|\bar{x}\| & \text{ if } \bar{x}\neq 0\\ \text{anything s.t. }\|w\|=1 & \text{ if } \bar{x}=0\end{array}\right.$$ However, I don't see how to denote this when $m=1$ . In this case, it seems that the only reasonable way of defining these guys is to set $\lambda_1(x)=x$ and $v_1(x)=1$ , since a defining property of $v_1$ and $v_2$ is their ""orthonormality"". And from the point of view of Euclidean Jordan Algebras, $L_1$ has rank 1. But what about $\lambda_2$ and $v_2$ ? Actually, I want to write that $v_2$ is not defined, in such a way that I can talk about $v_1$ and $v_2$ and it's clear for everyone that they should simply ignore $v_2$ . In other words, what I want to know is: How to define $v_2(x)$ such that $\{v_1(x),v_2(x)\}=\{1\}$ when $m=1$ , while keeping $v_1(x)v_2(x)=0$ ? Even a clever notation would do the job, as long as it is not outrageous... My (terrible) attempt to solve the problem: I am very inclined to make an abuse of notation and define $v_2(x)$ such that $\{v_2(x)\}\doteq\emptyset$ and $v_1(x)\doteq 1$ , so I would have $\{v_1(x),v_2(x)\}=\{v_1(x)\}\cup\{v_2(x)\}=\{1\}$ and some sort of ""vacuous orthogonality"", but it makes me feel bad, and I have never seen this kind of abuse before. What do you think about it? Any suggestion is welcome!","['elementary-set-theory', 'second-order-cone-programming', 'notation', 'jordan-algebras']"
4001203,When is normalizer of wreath product the wreath product of normalizers?,"I set out wondering about the $p$ -Sylow subgroups of symmetric group $S_n$ . First, if $n=p^k$ , the $p$ -Sylow is the iterated wreath product $C_p^{\wr k}:=C_p\wr C_p\wr\cdots\wr C_p$ . This is the automorphism group of the $k$ -level $p$ -ary rooted tree (the leaves may be labelled with the numbers $1,\cdots,p^k$ ). More generally if $n=a_kp^k+\cdots+a_1p+a_0$ is $n$ written in base- $p$ then $$ P=\prod_{j=0}^k a_j\, C_p^{\wr j} $$ where we interpret $aG=\underbrace{G\times\cdots\times G}_a$ in the above. We can show $P$ is a $p$ -Sylow of $S_n$ by simply calculating its order. The number of $p$ -Sylow subgroups is $S_n/N_{S_n}(P)$ , by the orbit-stabilizer theorem, so more generally I was curious what this normalizer is. I think we can say $N_{S_n}(P)=S_{a_0}\times\prod_{j=1}^k a_j N_{S_{p^j}}(C_p^{\wr j})$ using this lemma: Lemma . If $G\curvearrowright Z$ and $H\le G$ then $Z^H$ is $N_G(H)$ -stable. Thus if $G=H\times K$ with $H\le S_X,K\le S_Y$ and $X^H,Y^K$ empty then we can say $N_{S_{X\times Y}}(H\times K)=N_{S_X}(H)\times N_{S_Y}(K)$ . Edit : it seems I have this wrong, should be $(C_p^{\wr j})\wr S_{a_j}$ ? It remains to determine $N_{S_{p^k}}(C_p^{\wr k})$ . For this, I came to the following question: Suppose $A\curvearrowright X,B\curvearrowright Y$ and we consider the imprimitive action $A\wr B\curvearrowright X\times Y$ . That is, $A^Y\rtimes B$ acts on $X\times Y$ by having $\alpha\in A^Y$ act by $\alpha\cdot(x,y)=(\alpha(y)x,y)$ and $b\in B$ by $b\cdot(x,y)=(x,by)$ . Essentially, if we think of $X\times Y$ as an array of ordered pairs with $X$ - and $Y$ -coordinates, then $A^Y$ has a copy of $A$ for each row to act on the elements within the rows independently, and $B$ permutes the rows themselves. Question . Under what conditions can we say $N_{S_{X\times Y}}(A\wr B)=N_{S_X}(A)\wr N_{S_Y}(B)$ ?","['wreath-product', 'abstract-algebra', 'symmetric-groups', 'group-theory', 'group-actions']"
4001251,Best books to self study number theory and especially diophantine equations?,"I'm an undergraduate math student. I've already passed linear algebra and abstract algebra and recently passed a course in elementary number theory and I really enjoyed it. (Our source was Daniel Flath's ""Introduction to Number Theory"" and Kenneth Rosen's ""Elementary Number Theory"".) Now, I want to self-study more books this semester and I'm searching for good books with exercises. My main interest is in solving diophantine equations and their applications. Thanks for your suggestions and sorry for my bad English. :)","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'reference-request']"
4001252,Integral for Biot-Savart Law on a box,"I'm trying to make a basic computer model of a bar magnet. In the process I came across this question and answer that appears to have an appropriate equation for me to use. I say appears because my maths knowledge is only barely past high-school level, and the answerer stops here: you can simply use the Biot-Savart law to calculate the magnetic field: $$\mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3}$$ I believe you can take it from here. So I understand that in this case I can use the fact that the net magnetic field (the thing I want to model) is composed of the sum of all the magnetic fields produced in this situation. Which is to say I need to add up the field produced by each of the faces with respect to each point I want to model. I also figure that that is what the part of the equation that I don't understand is trying to express, namely the section $\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x')$ So in the above example a bar magnet is being modeled like so: You can model a bar magnet by a rectangular box with a constant magnetization in one direction. Let's take the box $[0,a]\times[0,b]\times[0,c]$ , with a constant magnetization $\mathbf M(\mathbf x) = M_0 \ \hat{\mathbf k}$ , where $\hat{\mathbf k}$ is the unit vector in the $z$ direction. The bound volume and surface current densities are: $$\mathbf J_b(\mathbf x) = \boldsymbol{\nabla}\times\mathbf M(\mathbf x)$$ $$\mathbf K_b(\mathbf x) = \mathbf M(\mathbf x) \times \hat {\mathbf n}$$ The volume current density is zero because $\mathbf M$ is constant. For the surface current density, the top and bottom faces don't contribute since $M_0 \hat{\mathbf k}\times\hat {\mathbf k}=0$ . For the other four faces we have: $$\mathrm{x=0 \ face:} \ \mathbf K_1 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf i}) = -M_0 \ \hat{\mathbf j}$$ $$\mathrm{x=a \ face:} \ \mathbf K_2 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf i} = M_0 \ \hat{\mathbf j}$$ $$\mathrm{y=0 \ face:} \ \mathbf K_3 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf j}) = M_0 \ \hat{\mathbf i}$$ $$\mathrm{y=b \ face:} \ \mathbf K_4 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf j} = -M_0 \ \hat{\mathbf i}$$ Now that you know the bound current distribution, you can simply use the Biot-Savart law to calculate the magnetic field: $$\mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3}$$ I believe you can take it from here. My question is how do I evaluate the integral portion of this equation? I'm looking to turn this into a piece of computer code, and my background is pretty shallow when it comes to this level of maths. Edit: I understand Matlab has an integrate function, but I would prefer not to buy a license for that if possible. Edit2: After thinking about this some more, and with the help of Ian's comments I have determined what I think I need to do, which is best expressed graphically by the diagram I have just drawn:","['integration', 'physics', 'multivariable-calculus']"
4001305,Proof if $f$ is continuous then the preimage is closed.,"I want to show that if $f: \mathbb{R}\to\mathbb{R}$ is continuous then $f^{-1}(l) = \{x\in\mathbb{R}|f(x) = l\}$ is closed. I am not sure of my proof as I feel like I am missing a step: I decided to use sequential continuity and consider a convergent sequence $(x_n)\subset f^{-1}(l)$ . By sequential continuity if $x_n \to L \implies f(x_n) \to f(L) = L'$ I want to say that I am now done - but I think I need to prove that $L \in f^{-1}(l)$ , or $L' \in Imf$ .","['elementary-set-theory', 'functions', 'sequences-and-series']"
4001335,Proving $\prod_{n=1}^{\infty}\left(1+\frac{1}{n^{3}}\right)=\frac{1}{\pi}\cosh\frac{\pi\sqrt{3}}{2}$,"First I rewrite $\prod_{n=1}^{\infty}\left(1+\frac{1}{n^{3}}\right)$ as $\prod_{n=1}^{\infty}\left(\frac{1+n^{3}}{n^{3}}\right)$ , then by factor out polynomial I get $\prod_{n=1}^{\infty}\left(\frac{(1+n)(n^{2}-n+1)}{n^{3}}\right)$ which is a problem because I can't factor any further which makes me stuck on this step I would hope for any help.","['infinite-product', 'limits', 'calculus', 'real-analysis']"
4001372,"In $\mathcal{P}(\mathbb{Z})$, $A \equiv B \iff \exists_{r \in \mathbb{Z}} ( \forall_{x \in A} (x + r \in B) ∧ \forall_{y \in B} (y - r \in A))$...","We define an equivalence relation $≡$ in the set $\mathcal{P}(\mathbb{Z})$ as: $$A \equiv B \iff \exists_{r \in \mathbb{Z}} ( \forall_{x \in A} (x + r \in B) ∧ \forall_{y \in B} (y - r \in A))$$ Determine $|\mathcal{P}(\mathbb{Z})/_≡|$ and $∣[\mathbb{N}]_≡∣ $ . I know that for $r = 0$ : $A = B$ , for $r = 1$ : $A$ and $B$ lay next to each other, have $1$ common element and both cover a part of $X$ axis that is $1$ wide. Similar facts are true for $r = 2$ , $r =3$ ... That would mean that: $|\mathcal{P}(\mathbb{Z})/≡| = |\mathbb{N}|$ . However, I don't know what to do with $∣[\mathbb{N}]_≡∣ $ .",['elementary-set-theory']
4001395,Continuity of Functions with Vertical Tangents,"I'm running into some confusion regarding properties of continuous functions. I'm comfortable with the epsilon-delta definition of limits and the basic definition of continuity at a point $a$ ( $\lim_{x\rightarrow a}f(x)$ must exist, $f(a)$ must exist, and the two must equal one another), but I'm frequently encountering the statement that a function is continuous if and only if ""a small change in $x$ produces a small change in $f(x)$ "". This seems reasonable enough, but continuous functions with vertical tangents seem to present a contradiction to this assertion. For example, the function $f(x)=\sqrt[3]{x}$ is continuous at $x=0$ by the definition of continuity, but its derivative at $x=0$ is undefined because the line tangent to the curve $y=f(x)$ is vertical. Shouldn't this mean, then, that at $x=0$ a small change in $x$ produces an infinite change in $y$ ? I'm not sure if $\left.\frac{dy}{dx}\right|_{x=0}=\infty$ constitutes an abuse of notation, but certainly it is true that $f^\prime(0)$ is undefined and $$\lim_{x\rightarrow 0}f^\prime(x)=\infty\text{.}$$ So I'm not sure how this ""small change in $x$ /small change in $f(x)$ "" description of continuity holds in this particular case.  Can anyone help resolve this confusion for me?","['continuity', 'calculus', 'limits', 'derivatives', 'infinitesimals']"
4001423,The differential of the exponential map on a Riemannian manifold,"Let $M$ be a Riemannian manifold and $exp_{x}: T_{x}M \to M$ the usual exponential map. It is an important fact that $D_{0}exp_{x}(v) = v$ for any $v \in T_{0}(T_{x}M)$ . The proof goes like this: $D_{0} exp_{x}(v) = \frac{d}{dt} exp_{x}(tv)|_{t = 0} =  \gamma(t)'|_{t = 0} = v$ , where $\gamma(t)$ is the geodesic starting at $v$ . I am struggling to understand this proof because I find it hard to interpret $\frac{d}{dt} exp_{x}(tv)|_{t = 0}$ . Is it supposed to be understood as $\lim_{t \to 0} \frac{exp_{x}(tv) - exp_{x}(0)}{t}?$ This difference expression does not make sense since we cannot take difference on a manifold without extra structure. I guess my problem is most of the texts I encountered tend to treat calculations like this like they are in the Euclidean space rather than on a manifold. I suppose it should be correct to do so, but is it trivially true without any justification? The way I would prove this is to take $f \in C^{\infty}(M)$ , and try to figure out what is the action of $D_{0}exp_{x}(v)$ on $f$ , but none of the reference I find take this approach.","['riemannian-geometry', 'differential-geometry']"
4001464,Induced module and surjective morphism,"I am trying to solve the following question: Let $G$ be a finite group and $H$ a subgroup of $G$ . Let $A$ be a $G$ -module. Show that $\pi: I^{H}_{G}(A)\to A$ defined by $\pi(f)=\sum_{g\in G/H}g\cdot f(g^{-1})$ is surjective morphism. Here, $I^{H}_{G}(A)$ is the induced module from $H$ to $G$ . It is $Hom_{H}(\mathbb{Z}[G],A)$ . I tried to understand the image of $\pi$ , but I could only conclude that it is invariant by $H$ . I don't have any other idea what to do.","['group-theory', 'abstract-algebra', 'group-cohomology', 'modules']"
4001476,Non-uniqueness of MLE of multivariate Laplace distribution?,"Suppose $\{X\}_{i=1}^n\overset{i.i.d}{\sim}X$ , and $X\in\mathbb{R}^d$ has density, $$f_{\theta}(x)=c\exp\left\{-||x-\theta||\right\},\theta\in\mathbb{R}^d,$$ where $||\cdot||$ denotes the Euclidean norm. Show that the MLE $\hat{\theta}$ exists but is not unique when $n$ is even. I know how to prove that the MLE $\hat{\theta}$ exists, by noticing $\underset{\theta\rightarrow\partial\mathbb{R}^d}{\lim} \log f(\theta)=-\infty$ , however I don't know how to prove it is not unique. I understand that the log-likelihood function, $l(\theta)=-\sum_{i=1}^n||x_i-\theta||$ is a convex, but not strictly convex, function, but this does not guarantee the nonuniqueness. I also tried taking the first and second gradient of $l(\theta)$ . But even I have a non-negative definite $l(\theta)$ , I still don't have the non-uniqueness... ---Update--- The original question is here. This is from Mathematical Statistics (Bickel).","['multivariable-calculus', 'statistics', 'optimization', 'maximum-likelihood']"
4001485,"Show $T$ is compact operator if $\langle Te_n,e_n \rangle$ tend to zero.","Suppose $\mathcal{H}$ is a Hilbert space, and $T\in B(\mathcal{H})$ . If for each orthonormal (norm 1) basis $\{e_n\}\subseteq \mathcal{H}$ , we have $\langle Te_n, e_n \rangle \rightarrow 0$ . Can we deduce $T$ is compact? I guess it may use spectra decomposition. Take adjoin, use the fact that $\langle Ae_n,e_n\rangle \rightarrow 0$ iff $\langle (A+A^*)e_n,e_n \rangle \rightarrow 0$ and $\langle (A-A^*)e_n,e_n\rangle \rightarrow 0$ , we can assume $T$ is self-adjoint. So $T$ can be viewed as a multiple operator(multiple by a $L^{\infty}(X,d\mu)$ function) on $L^2(X,d\mu)$ , via unitary equivalence. Here $d\mu$ is an abstract $\sigma$ -finite Borel measure on X. But I don’t know how to use the condition.. And, another way, if we use the spectra decomposition via $T=\int z dE$ , E is the corresponding spectra measure. To show $T$ is compact, it is sufficient to prove the projection $E(-\infty,-\epsilon)$ and $E(\epsilon, +\infty)$ are all finite rank, for every $\epsilon>0$ . And I feel the condition may can be use together with something like dominate converge theorem? But I fail.
Any help or hint? Thanks.","['hilbert-spaces', 'spectral-theory', 'compact-operators', 'functional-analysis']"
4001510,"Showing a biconditional statement about function lim sups in $\Bbb R^n$, and codifying the intuition into a proof","$
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\ve}{\varepsilon}
\newcommand{\para}[1]{\left( #1 \right)}
\newcommand{\set}[1]{\left\{ #1 \right\} }
$ The Statement: Let $f : E \subseteq \R^n \to \R$ . The claim to prove: $$\beta = \limsup_{x \to x_0} f(x) \iff \text{conditions (i) & (ii) below hold}$$ The conditions: (i) $\exists \set{x_n}_{n \in \N} \subseteq \R^n$ a sequence such that $f(x_n) \to \beta$ (ii) $(\forall b > \beta)(\exists \delta > 0)(f(x) < b \text{ for } x \in B^* \cap E)$ Notable Definitions & Notations: I think the more relevant definitions to go over would be these two: We denote the ""punctured ball"" centered at $x_0$ with radius $\delta$ by $B^*$ , and more explicitly may also write $$B^* := B^*(x_0;\delta) := B(x_0;\delta) \setminus \set{x_0}$$ (Some of you may be more familiar with the notation $B_\delta(x_0)$ to denote a ball of radius $\delta$ centered at $x_0$ .) Essentially the punctured ball $B^*$ is the usual ball with the center removed. We define the limit supremum of $f$ as $x \to x_0$ as so: $$\limsup_{x \to x_0} f(x) = \lim_{\delta \to 0} \left( \sup_{x \in B^* \cap E} f(x) \right)$$ Context & Attempts: Ultimately this is a homework problem, so I would prefer to not have full proofs given out; moreso just nudges and such. Now, in the senses of geometry, visuals, and intuition, I think I have some ideas as to what is going on, though not quite the whole picture. In the forward direction: let $\beta$ be $\limsup f(x)$ . Then that means two things. Firstly, we can find a sequence $x_n$ in the domain where that sequence under $f$ , $f(x_n)$ , converges to $\beta$ . I think that helps get $\beta \le \sup f(x)$ on the relevant set. Moreover, whenever we have a number $b$ larger than $\beta$ , then condition (ii) essentially constricts $b$ and $\beta$ in a sense. The closer $b$ and $\beta$ , the smaller $\delta$ is, and eventually I think that will help us get to equality. In the backwards direction: we know there is a sequence $\set{x_n}$ where $f(x_n) \to \beta$ , and whenever $b > \beta$ , $x \in B^*(x_0;\delta) \cap E$ have $f(x) < b$ (though where $\beta$ lies between the two is not known). I think this would give that $\sup f(x) \le b$ for the relevant set, and as $\delta \to 0$ , $b$ and $\beta$ get closer and closer together until we somehow get equality. My issues with these are more like ... trying to codify my intuitions. I have written down some things, but I'm not too certain they're on the right track. For instance, for the forward direction: Attempt at the Forward Direction: Suppose $f : E \subseteq \R^n \to \R$ and $\beta := \displaystyle \limsup_{x \to x_0} f(x)$ . Then by definition, $$
\beta = \lim_{\delta \to 0} \para{ \sup_{x \in B^*(x_0;\delta) \cap E} f(x) }
$$ Let $\delta_k := 1/k$ . Then define $\BB_k := B^*(x_0;\delta_k)$ . Since $\delta_k \to 0$ , it holds that $$
\sup_{x \in \BB_k \cap E} f(x) \to \beta
$$ Thus, define $x_k$ by $x_k \in \BB_k \cap E \subseteq E$ , and then $\set{x_k}_{k \in \N}$ is one such that $f(x_k) \to \beta$ , satisfying condition (i). Suppose $b > \beta$ . To see condition (ii) holds, suppose otherwise, that $\forall \delta > 0$ , $f(x) \ge b$ for $x \in B^* \cap E$ . This would mean that $$
\sup_{x \in B^*(x_0;\delta) \cap E} f(x) \ge b > \beta
$$ for all $\delta > 0$ and in particular the case $\delta \to 0$ . This would give that $ \displaystyle\limsup_{x \to x_0} f(x) = b$ instead from the definition, contradicting that $\beta$ is the lim sup. Thus, a contradiction is reached and condition (ii) holds. Attempt at the Backwards Direction: (which is moreso scratch work that stalled hard) We're going to suppose (i) & (ii) hold. Since $\exists \set{x_k}_{k \in \N} \subseteq E$ such that $f(x_k) \to \beta$ , from definition, $\forall \ve > 0$ , $\exists N \in \N$ such that, $\forall n \ge N$ , $|f(x_n) - \beta| < \ve$ . In particular, we can consider the elements of $\set{x_k}$ which are within a radius $\delta > 0$ of $x_0$ and are different from $x_0$ ; these elements define a subsequence $\set{x_{n_k}}_{k \in \N} \subseteq B^* \cap E$ . From the fact that limits are unique, and if a sequence $z_k \to L$ , then all subsequences of $\set{z_k}_{k \in \N}$ converge to $L$ as well. Thus, $\set{x_{n_k}}_{k\in\N}$ is a sequence in $B^* \cap E$ . If we take the limit as $\ve,\delta \to 0$ , it then follows from these and the definition of supremum that $$
\lim_{\delta \to 0} \para{ \sup_{B^* \cap E} f(x) } \le \beta
$$ Consider condition (ii) and that for a $b > \beta$ , $\exists \delta > 0$ such that $f(x) < b$ for $x \in B^* \cap E$ . (That's where I get lost though...) Questions & Concerns: Is my intuition for what's going on correct? If not, what's the correct intuition and the general approach I should use? Is there anything remotely salvageable from my approaches? If there are errors with my approach, what are they? How might I rectify them? Thanks for any insight you can give me! Edit: (thoughts for converse as of 1/27/2021) I had a thought for the approach that seems to neatly tie conditions (i) and (ii) together. Let $\set{b_k}_{k \in \N}$ be a monotone decreasing sequence with limit $\beta$ , i.e. $b_k > \beta$ for every $k$ , and $b_k \searrow \beta$ . Define $\ve_k := b_k - \beta$ . Consider the preimage $f^{-1}(\beta,\ve_k)$ . This may consist of a set of disjoint sets, so consider only the one $\mathcal{A_k}$ the point $x_0$ lies in. For each preimage determined by $\ve_k$ , we can get a $\mathcal{A_k}$ , containing a ball $B(x_0;\delta_k)$ . Thus, each $\ve_k$ determines a preimage, which determines a ball (a subset of the preimage), which determines a $\delta_k$ . Define a set of points $x_k$ by being in that preimage and different from $x_0$ , i.e. take $x_k \in B^*(x_0;\delta_k)$ . Obviously, as $b_k \to \beta$ , then $\delta_k \to 0$ . Moreover, $x_k \to x_0$ , and $f(x_k) \to \beta$ . For each $k \in \N$ , we have that $$\sup_{x \in B^*(x_0;\delta_k) \cap E} f(x) = b_k$$ and, as $\delta_k \to 0$ , we have $b_k \to \beta$ , giving the lim sup as desired. (I'm sure there are details to iron out in this, but I feel there's a grain of truth at the solution in there somewhere...)","['limsup-and-liminf', 'proof-writing', 'metric-spaces', 'real-analysis', 'sequences-and-series']"
4001520,Why is $d(x^2)= 2xdx$?,"This seems a bit obvious, but I'm not clear on one part. My teacher had told us $d(x^2) =2xdx$ .
However on asking why, we were told $d(x^2)= (x+dx)^2-x^2= 2xdx+(dx)^2$ . We were told that we can ignore the $(dx)^2$ because its small.
For obvious reason I feel this step is wrong, why can we ignore $(dx)^2$ , is their any other way of getting the result?","['integration', 'derivatives']"
4001536,Interesting Identity: $\lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{nk^{-j}\Phi_{j}}=e^{\gamma}$,"Define the following identities: $$
\phi_{n}:=\frac{4}{\pi}\int_{0}^{\infty}\frac{\coth(nx^{-1})-xn^{-1}}{n(1+x^{2})^{2}}\;dx \qquad
\text{and}\qquad 
\Phi_{n}:=\frac{\cos(n\pi)}{n}
$$ $\forall n\in\mathbb{N}$ . I want to show that : $$
\lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{ nk^{-j}\Phi_{j}}=e^{\gamma}
$$ Where $\gamma$ is the Euler - Mascheroni constant. What I did is that I used the linearity of integrals for $\phi_{n}$ $$
\frac{\pi}{4}\phi_{n}=\int_{0}^{\infty}\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\;dx-\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})^{2}}\;dx
$$ The first one I define it as $I_{1}$ and the second one I define it as $I_{2}$ . Afterwards I defined: $$
\lambda_{n}(x):=\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\leq\left|\frac{1}{n(1+x^{2})^{2}}\right|\leq\left|\frac{1}{2n(1+x^{2})}\right|=:V_{n}(x)
$$ so that: $$
\lim_{n\to\infty}\int_{0}^{\infty}V_{n}(x)\;dx=\lim_{n\to\infty}\frac{\pi}{4n}=0
$$ Applying the Lebesgue Dominated Convergence Theorem I get: $$
\lim_{n\to\infty}\int_{0}^{\infty}\lambda_{n}(x)\;dx=0
\tag 1$$ Now note that: $$
I_{2}=\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})}\;dx=\frac{1}{2n^{2}}\quad\implies\quad\lim_{n\to\infty}I_{2}=0
\tag 2$$ $(1)$ and $(2)$ verify that $\lim_{n\to\infty}(1+\phi_{n})^{n}\leq\lim_{n\to\infty}(1+I_{1}-\displaystyle I_{2})^{n}=\lim_{n\to\infty}\left(1+\frac{1}{n}-\frac{2}{n\pi^{2}}\right)^{n}=e$ Now all I have to evaluate $\displaystyle\exp\left(\lim_{m\to\infty}\sum_{k=1}^{\infty}\sum_{j=2}^{\infty}\frac{\cos(j\pi)}{k^{j}j}\right)$ but this seems a bit confusing and misleading when I try it which is what I am stuck on.","['limits', 'calculus', 'real-analysis']"
4001563,Area of shaded triangular region from absolute value functions,"So, I got this question from my teacher. I've tried solving it but to no avail. I cannot work out a method either, which can help me solve this. So, it gives us two absolute value functions $g(x) = 4|x-3|+3$ $f(x) = -6|x-3|+9$ There are two points, A & B as shown in the figure given below.
I don't understand, when I find the points of intersection by solving $g(x) = f(x)$ then what are those $x$ values. Are they the coordinates of point A and B?
And what would be the next steps leading to the complete solution of this question. I'd appreciate if anyone points out the steps or just solves it for my ease. Question FIGURE","['area', 'absolute-value', 'functions', 'triangles', 'trigonometry']"
4001572,Strategy of a game,"Alice and Bob are playing a game. They write down the integers between $1$ and $100$ (inclusive). First, Alice chooses a number she likes and deletes it, and then Bob chooses to delete another number among the divisors and multiples of the one Alice deleted. Then Alice repeats what Bob has done, choosing another number among the divisors and multiples of the one Bob deleted. The game continues until one of them has no choices and the other wins. For example, Alice chooses $94$ . Bob chooses $47$ . The only choice left for Alice now is $1$ . And then Bob chooses $97$ to win the game. To make the question interesting, we add the restriction on Alice that she can only begin with an even number. The question is: Does Alice have a winning strategy? It can be interpreted as a graph. The graph has $100$ vertices and has edges between every divisor-multiple pair. When a point is deleted, if every point with an edge to the deleted point has no winning strategy for Alice, then the deleted point has a winning strategy for Bob.   But I can't run the program on my PC. Maybe there is a wiser strategy?","['graph-theory', 'number-theory', 'game-theory']"
4001589,finding the mean in normal distribution,"I am struggling on this question where I need to find the mean given the standard deviation.
The question is the following: A certain brand of flood lamps has a length of life that is normally distributed, 
with a standard deviation of 392 hours.
If 8 % of the lamps last more than 5600 hours, what is the mean length of life? This is what I've tried so far let $X$ = length of light $X\sim\mathcal{N}(\text{mean = unknown }(u), SD = 392)$ $P(X > 5600) = 0.8$ $1 - P(X < 5600) = 0.8$ (stuck after this part) $1 - P(5600-u/392 < 5600) = 0.8$ ( incorrect) I am having troubles working backwards to the find the mean.
Could someone point me in the right direction.","['statistics', 'normal-distribution']"
4001592,Topological conjugacy between linear maps in dimension $1$,"Let $A_{\alpha}: \mathbb{R} \to \mathbb{R}$ denote the linear map \begin{align}
A_{\alpha}(x)=\alpha x.
\end{align} By definition two homeomorphisms $f:X \to X$ and $g:X \to X $ are topologically conjugate to each other if there is a homeomorphism $h:X \to X $ such that $hf=gh$ . I wish to prove that if $ 0 < \alpha < 1$ and $ 0 < \beta < 1 $ then $A_{\alpha}$ and $A_{\beta}$ are topologically conjugate. Could anyone help me find this homeomorphism?","['topological-dynamics', 'general-topology', 'analysis', 'dynamical-systems']"
4001599,"What is the motivation behind the definition of the ""Neighbourhood Space""?","Bert Mendelson's ""Introduction to Topology"" (3ed, 1975, Dover pubs) spends a section in both his chapter on metric spaces and that on general topological spaces (chapters 2 and 3 respectively) hammering out the axioms for a neighborhood space . As follows: A neighborhood space is a set $S$ such that, for each $x \in S$ , there exists a set of subsets $\mathcal N_x$ of $S$ satisfying the following conditions: $\text N 1$ : There exists at least one element in $\mathcal N_x$ $\text N 2$ : Each element of $\mathcal N_x$ contains $x$ $\text N 3$ : Each superset of $N \in \mathcal N_x$ is also in $\mathcal N_x$ $\text N 4$ : The intersection of $2$ elements of $\mathcal N_x$ is also in $\mathcal N_x$ $\text N 5$ : There exists $N' \subseteq N \in \mathcal N_x$ which is in $\mathcal N_y$ for each $y \in N'$ He goes on to establish that a neighborhood space is exactly the same as a topological space: the neighborhood space induced by the topological space induced by a neighborhood space is that neighborhood space, and so on. So the neighborhood space axioms define exactly the same space as the well-known open set axioms of topology. Proving that a collection of subsets of a set adhere to the neighborhood space axioms is fiddly and tedious, and conceptually non-intuitive. Particularly axiom $N5$ , which is horrible. The question is: is there a good reason for a ""neighborhood space"" to be defined? Does it have any particular uses that make it essential or worthwhile to study in detail? Or is it just an interesting backwater that Mendelson (and also Willard, it seems, by taking a glance at the Wikipedia page on ""Neighborhood system"") defines for the fun of it? Hence, how much effort would one be expected to exert in an attempt to master fluency in handling such constructs? (No doubt there may be some difficult-to-analyse spaces which are analysed more easily using neighborhood axioms rather than open set axioms -- but are they sufficient bang for the buck?)","['general-topology', 'soft-question']"
4001601,Help solving $\int \:\frac{dx}{x+\sqrt{9x^2-9x+2}}$,"Reading an article online I came across this integral I've been trying to solve since yesterday morning and I literally tried every integration method I know and I still can't solve it. I am a beginner to integrals so help will be appreciated! I tried substitution, integration by parts and I can't seem to find the correct way! $$\int \:\frac{dx}{x+\sqrt{9x^2-9x+2}}$$ Does anyone know how to proceed from here? What to do? Because I am clueless. Thanks!","['integration', 'indefinite-integrals', 'calculus']"
4001619,How to calculate the limit of zeta function,"Suppose $f(x)>0$ , $f''(x)\leqslant0$ ,and $\lim\limits_{x\to+\infty}f(x)=+\infty$ on $[0,+\infty)$ .prove that $$\lim\limits_{s\to0^+}\sum\limits_{n=0}^{\infty}\dfrac{(-1)^n}{f^s(n)}=\frac{1}{2}.$$ I tried to do it ,first of all we have $$\sum\limits_{n=0}^{\infty} \frac{(-1)^n}{f^s(n)}=\sum\limits_{n=0}^{\infty} \left( \frac{1}{f^s(2n)}-\frac{1}{f^s(2n+1)} \right).$$ Use MVT we have $$ \frac{1}{f^s(2n)}-\frac{1}{f^s(2n+1)}=-\frac{sf'(\xi_n)}{f^{s+1}(\xi_n)}(\xi_n\in(2n,2n+1)).$$ But next, I don't know how to deal with it. I want to ask that this problem can be solved by the property that the function is concave.","['limits', 'sequences-and-series', 'real-analysis']"
4001621,Is the matrix $A^4+A^3-3A^2-3A$ invertible?,"Let $A$ be a real $5 \times 5$ matrix satisfying $A^3-4A^2+5A-2I=O$ . Is the matrix $A^4+A^3-3A^2-3A$ invertible? Consider the polynomial $t^3-4t^2+5t-2=(t-1)^2(t-2)$ . The minimal polynomial of $A$ divides $(t-1)^2(t-2)$ . Let $p(t)$ be the minimal polynomial of $A$ . If $p(x)=t-1$ or $t-2$ , then $A$ is a scalar multiple of the identity and $A^4+A^3-3A^2-3A=-4I \text{ or }6I$ , so $A^4+A^3-3A^2-3A$ is invertible. If $p(t)=(t-1)^2(t-2)$ or $(t-1)(t-2)$ , then $A$ is similar to a upper triangular matrix $J$ with diagonal entries $1,2$ . Then $A^4+A^3-3A^2-3A=P(J^4+J^3-3J^2-3J)P^{-1}$ and $\det(A^4+A^3-3A^2-3A)=\det(J^4+J^3-3J^2-3J)$ . Since $J^4+J^3-3J^2-3J$ is upper triangular and the diagonal entries do not vanish, $\det(J^4+J^3-3J^2-3J) \ne 0$ . If $p(t)=(t-1)^2$ , then by the same reasoning, $A^4+A^3-3A^2-3A$ is invertible. Is there more efficient way to solve this kind of problems or I have to discuss any possible situations every time?","['solution-verification', 'linear-algebra']"
4001683,"If $p \in U\subseteq M$ is an open subset, then $C^\infty_p(U) = C^\infty_p(M)$.","Let $M$ be a (smooth) manifold and $p \in M$ . We define $C^\infty_p(M)$ to be the set of equivalence classes of smooth functions $M \to \mathbb{R}$ that are identified when they agree on some  neighborhood of $p$ . In Tu's book ""Introduction to manifolds"", p87, I read that if $U$ is an open subset of $M$ containing $p$ , then $$C_p^\infty(M) = C_p^\infty(U)$$ Questions : Evidently, this is not a strict equality because on the right we have (classes of) functions $U \to \mathbb{R}$ and on the left we have functions $M \to \mathbb{R}$ . Hence, I guess that they mean that there is a canonical isomorphism (of $\mathbb{R}$ -algebras) $$C_p^\infty(M) \to C_p^\infty(U): f \mapsto f\vert_U$$ Clearly this is injective, because if $f$ and $g$ agree on $U$ , then they are in the same class so $f= g$ in $C_p^\infty(M)$ . Why is this mapping surjective? Don't we need that a smooth function $U \to \mathbb{R}$ can be smoothly extended to a smooth function $M \to \mathbb{R}$ for this?","['germs', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
4001762,show that the set $ A'=\left\{ x:x\in\partial(A\setminus\{x\}\right\} $is closed,"I'm studying Calculus 3, and our teacher tends to give us extremely difficult questions.
one of those questions was this:
""let $A \subseteq \Bbb R^k $ . show that the set $ A' = \{ x \in \Bbb R^k\mid x \in \partial(A \setminus \{ x \})\}$ is closed"".
I've tried to show using sequence characterization or showing that $(A')^c$ is open, but I don't have a clue of how to start, or even if I'm in the right direction.
I could really use some help or a guiding hand with this.","['multivariable-calculus', 'calculus', 'general-topology']"
4001780,Compute $\int_{-\infty}^{\infty}\frac{x\sin(\pi x)}{(x-3)(x-2)}dx$ using residue thoerem,"I am trying to compute $\int_{-\infty}^{\infty}\frac{x\sin(\pi x)}{(x-3)(x-2)}dx$ using the residue theorem. To do so, I am integrating the function $f(z)=\frac{ze^{i\pi z}}{(z-3)(z-2)}$ over a the frontier of $\{z:|z|<R,Im(z)>0\}-(\{z:|z-2|<r\}\cup\{z:|z-3|<r\})$ (a semicircle that ""avoids"" the singularities of $f$ ). I have used Jordan's lemma to prove that the integral over the big semicircle is null when $R\longrightarrow \infty$ . My problem is I do not know how to compute the integral over the small semicircles when $r\longrightarrow 0$ . How could I approach this? Thanks in advance.","['complex-analysis', 'residue-calculus', 'improper-integrals']"
4001813,A question regarding the power of a sinus function.,"My calculus book says the following: $\sin^{n}{x}$ is short for $(\sin{x})^n$ , $n\neq -1$ . This is obviously trivial, however I don't get it why it says $n\neq -1$ . The above condition should for $n=-1$ , right? Thank you very much for your help!
Regards,
Charlie","['notation', 'trigonometry']"
4002013,How to calculate the length of arcs in the given periodic function?,"I have this function $$f(x)=-11 \cos x+11 \cos 2 x+6$$ with period $2\pi$ . The plot of function is on the left below. Then, I want to know how I can calculate the lengths of the four arcs for which $-2<f(x)<6$ that is the plot on the right. Any comments or hints are appreciated.","['arc-length', 'periodic-functions', 'calculus', 'trigonometry', 'algebra-precalculus']"
4002019,Integration of $\int_0^{\infty}\frac{e^{(\lambda +is)t}-e^{-(\lambda +is)t}}{e^{\pi t}-e^{-\pi t}}dt$,Integration of $$\int_0^{\infty}\frac{e^{(\lambda +is)t}-e^{-(\lambda +is)t}}{e^{\pi t}-e^{-\pi t}}dt$$ I am thinking about this integral from last half and hour but still can't figure it out how to solve this. what i was think is to make this in form $$\int_{0}^{\infty}\frac{\sinh(\lambda+is)t}{\sinh (\pi t)} dt$$ but still this will get me nowhere. please just give me a hint how to proceed this. Thank you.,"['integration', 'fourier-analysis']"
4002020,Balanced $2019\times 2019$ grids (BMO $2020$ round $2$),"A $2019 \times 2019$ square grid is made up of $2019^2$ unit cells. Each cell is coloured either black or white. A colouring is called balanced if, within every square subgrid made up of $k^2$ cells for $1 \le k\le 2019$ , the number of black cells differs from the number of white cells by at most one. How many different balanced colourings are there?
( Two colourings are different if there is at least one cell which is black in exactly one of them. ) Let $f(n)$ be the number of different balanced colorings for an $n\times n$ grid. A few unfruitful ideas: $\mathbf{1) Inclusion-Exclusion}$ Call the event that the $i^{th}$ subgrid is balanced, $B_i$ . Then $$ f(n) = \left | \bigcap B_i \right | = \left | \left( \bigcup (B_i)’ \right)’ \right | = 2^{n^2}-\left | \bigcup (B_i)’ \right|$$ but the latter term isn’t easily calculated. $\mathbf{ 2) Recursion}$ $f(n)$ will include the number of ways where the top left $(n-1)\times (n-1)$ grid is balanced, but there doesn’t seem to be a fixed $t(n)$ so that $$f(n) = t(n) f(n-1) \ \text{or} \ f(n-1) + t(n) $$ $ \mathbf{3) Manual \ Counting } $ I found $$f(1)=2 \\ f(2) = 6 \\ f(3) = 10 \\ f(4) = 18 \\ f(5)=26 $$ from where I guess $$f(2n+1)= 2^{n+3} -6 $$ but that’s just a guess.","['contest-math', 'combinatorics']"
4002042,Summing $1+\cos(\theta)+\cos(2\theta) +\cdots + \cos(n\theta)$,"First of all, I'd like to acknowledge that there are already solutions to this question on this forum. However, I'm repeating the question here because my solution doesn't quite get me what those other solutions are and I'm wondering if I'm doing something wrong. I know that taking $z\in \mathbb{C}$ s.t. $|z|=1$ and $\arg(z) = \theta$ , we get $z=\cos(\theta)+i\sin(\theta)$ . Then by DeMoivre's Theorem we get $z^n= (\cos(\theta)+i\sin(\theta))^n=\cos(n\theta)+i\sin(n\theta)$ , and we see that the sum $1+\cos(\theta)+\cos(2\theta) +\cdots + \cos(n\theta)$ is equal to $\operatorname{Re}(z^0+z^1+\cdots+z^n)$ (call it $S$ ). From there, using geometric sums, $S=\frac{z^{n+1}-1}{z-1}$ . In order to find $\operatorname{Re}(S)$ , I am multiplying the top and the bottom by $\overline{z-1}$ to get $S=\frac{(z^{n+1}-1)*(\overline{z-1})}{(z-1)*(\overline{z-1})}$ . This is where I'm running into trouble. Expanding $z$ into $\cos(\theta)+i\sin(\theta)$ I get that the denominator is equal to $2-2\cos(\theta)$ (and whatever other variants of this using trig identities). However, this doesn't seem to line up with any of the other standard answers I'm finding online (which mostly involve a $\sin(\frac{\theta}{2})$ in the denominator). I was wondering if anyone could point out if/where I've gone wrong in my process. Is there another way to handle this sum (while still using DeMoivre's Theorem)?",['complex-analysis']
4002082,Is Bayes' Theorem really that interesting?,"I have trouble understanding the massive importance that is afforded to Bayes' theorem in undergraduate courses in probability and popular science. From the purely mathematical point of view, I think it would be uncontroversial to say that Bayes' theorem does not amount to a particularly sophisticated result. Indeed, the relation $$P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(B\cap A)P(A)}{P(B)P(A)}=\frac{P(B|A)P(A)}{P(B)}$$ is a one line proof that follows from expanding both sides directly from the definition of conditional probability. Thus, I expect that what people find interesting about Bayes' theorem has to do with its practical applications or implications. However, even in those cases I find the typical examples being used as a justification of this to be a bit artificial. To illustrate this, the classical application of Bayes' theorem usually goes something like this: Suppose that 1% of women have breast cancer; 80% of mammograms are positive when breast cancer is present; and 10% of mammograms are positive when breast cancer is not present. If a woman has a positive mammogram, then what is the probability that she has breast cancer? I understand that Bayes' theorem allows to compute the desired probability with the given information, and that this probability is counterintuitively low. However, I can't help but feel that the premise of this question is wholly artificial. The only reason why we need to use Bayes' theorem here is that the full information with which the other probabilities (i.e., 1% have cancer, 80% true positive, etc.) have been computed is not provided to us. If we have access to the sample data with which these probabilities were computed, then we can directly find $$P(\text{cancer}|\text{positive test})=\frac{\text{number of women with cancer and positive test}}{\text{number of women with positive test}}.$$ In mathematical terms, if you know how to compute $P(B|A)$ , $P(A)$ , and $P(B)$ , then this means that you know how to compute $P(A\cap B)$ and $P(B)$ , in which case you already have your answer. From the above arguments, it seems to me that Bayes' theorem is essentially only useful for the following reasons: In an adversarial context, i.e., someone who has access to the data only tells you about $P(B|A)$ when $P(A|B)$ is actually the quantity that is relevant to your interests, hoping that you will get confused and will not notice. An opportunity to dispel the confusion between $P(A|B)$ and $P(B|A)$ with concrete examples, and to explain that these are very different when the ratio between $P(A)$ and $P(B)$ deviates significantly from one. Am I missing something big about the usefulness of Bayes' theorem? In light of point 2., especially, I don't understand why Bayes' theorem stands out so much compared to, say, the Borel-Kolmogorov paradox, or the ""paradox"" that $P[X=x]=0$ when $X$ is a continuous random variable, etc.","['statistics', 'motivation', 'bayes-theorem', 'probability']"
4002087,"Prove if $f : \mathbb{R}\to\mathbb{R}$ is continuous and $S \subset \mathbb{R}$ is compact, then the image is also compact.","I am not 100% sure about my proof, ideally I would like some hints/confirmation on my proof. Proof: Since I want to show $f(S)$ is compact I need to show it is closed and bounded. Closure Consider a sequence $(x_n) \subset S$ where $x_n \to L \in S$ (by compactness). Since $L \in S$ we have that since the function is continuous $f(L) \in f(S)$ . Then using sequential continuity on this sequence $f(x_n) \to f(L) \in f(S)$ . Since we can represent any sequence in $f(S)$ as the image of any given sequence in $S$ the set $f(S)$ is closed. Bounded Since $S$ is bounded and $f$ continuous, suppose $s^*$ maximises the function $f$ on $S$ - if $f(s^*) \to \infty$ $f$ is not continuous. Since $max(S)$ and $min(S)$ are finite numbers, we cannot have divergence to infinity of the function over the elements of $s$ , so $f(s^*)$ is the upper bound of $f(S)$ . Repeating for the lower bound I have that S is bounded. Is my proof correct. I have concerns that my proof for bounded is not very well worded, also I feel like my proof for closure may miss certain cases.","['continuity', 'general-topology', 'functions', 'compactness']"
4002088,"Evaluating $I(z,s)=\int_0^1\int_0^1\left(1-\frac{(1-x)(1-y)}{(1-(1-z)x)(1-(1-z)y)}\right)^{s-2}\,\mathrm dx\mathrm dy$","I came across the following double integral in a statistics problem: For $z>0$ $$
I(z,s)=\int_0^1\int_0^1\left(1-\frac{(1-x)(1-y)}{(1-(1-z)x)(1-(1-z)y)}\right)^{s-2}\,\mathrm dx\mathrm dy.
$$ All I was interested in was the values of $s$ for which $I$ converges $(s>0)$ . That said, the integral looked interesting enough to try and evaluate.  I was successful in evaluating the special case $z=1$ (see below) but could not solve for the general solution. Any ideas on how to solve it? Case for $z=1$ : We have $$
I(1,s)=\int_0^1\int_0^1\left(1-(1-x)(1-y)\right)^{s-2}\,\mathrm dx\mathrm dy.
$$ Substituting $(u,v)=(1-(1-x)(1-y),x)$ , the resulting integral in $v$ is easily evaluated in terms of the natural logarithm; hence, $$
I(1,s)=-\int_0^1 u^{s-2}\log(1-u)\,\mathrm du.
$$ Observe that $-u^{s-2}\log(1-u)\sim u^{s-1}+\mathcal O(u^s)$ as $u\nearrow 0$ , which tells us the integral should converge for $s>0$ . Using the series expansion for the logarithm we then integrate termwise to find $$
I(1,s)=\sum_{k=1}^\infty\frac{1}{k(k+s-1)}=\frac{H_{s-1}}{s-1},
$$ where $H_{s-1}=\psi(s)+\gamma$ is the generalized harmonic number, $\psi(z)=\partial_z\log\Gamma(z)$ is the digamma function and $\gamma=0.577\dots$ is the Euler–Mascheroni constant.","['integration', 'digamma-function', 'gamma-function', 'multivariable-calculus', 'sequences-and-series']"
4002090,How to Taylor expand a scalar quantity about a unit sphere?,"Consider a vector field $\mathbf{v} (r,\theta)$ expressed in the system of (axisymmetric) spherical coordinates with $r$ denoting the radial distance and $\theta$ the polar angle.
We consider a surface $S$ defined by $r = 1 + \epsilon f(\theta)$ where $\epsilon \ll 1$ .
We denote by $\mathbf{n}$ the vector normal to that surface.
Specifically, $$
\mathbb{n} = \mathbb{e}_r - \epsilon f'(\theta) \, \mathbb{e}_\theta \, , 
$$ wherein $\mathbb{e}_r$ and $\mathbb{e}_\theta$ are the basis unit vectors. Using perturbation analysis, the field $\mathbb{v}$ can be expressed to leading order as $$
\mathbb{v} = \mathbb{v}^{(0)} + \epsilon \mathbb{v}^{(1)} \, .
$$ Accordingly, $$
\mathbb{v} \cdot \mathbb{n} = 
v_r^{(0)} + \epsilon\left( v_r^{(1)} - f'(\theta) v_\theta^{(0)} \right) \, 
$$ wherein $v_r := \mathbb{v} \cdot \mathbb{e}_r$ and $v_\theta := \mathbb{v} \cdot \mathbb{e}_\theta$ . I was wondering how one can Taylor expand the dot product $\mathbb{v} \cdot \mathbb{n}$ about the unit sphere $r=1$ .
Specifically, $$
\bigg. \mathbb{v} \cdot \mathbb{n} \bigg|_{r = 1+\epsilon f(\theta)}
= 
[\bigg. \mathbb{v}^{(0)} \cdot \mathbb{e}_r + \epsilon \left( \text{something} \right) ] \bigg|_{r = 1} 
+ \mathcal{O}(\epsilon^2) \, .
$$ Any help is highly appreciated! Thank you","['geometry', 'real-analysis', 'taylor-expansion', 'perturbation-theory', 'power-series']"
4002119,Show $\cos\frac\gamma2=\sqrt{\frac{p(p-c)}{ab}}$,"Show that the equality $$\cos\dfrac{\gamma}{2}=\sqrt{\dfrac{p(p-c)}{ab}}$$ holds for a $\triangle ABC$ with sides $AB=c,BC=a, AC=b$ , semi-perimeter $p$ and $\measuredangle ACB=\gamma$ . I have just proved that $$l_c=\dfrac{2ab\cos\dfrac{\gamma}{2}}{a+b}$$ Is this a well known formula for angle bisectors? Can we use it somehow? Thank you in advance!","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4002122,Proof of optimal transport map for 1-d Wasserstein,"I'm following some code for implementing Wasserstein distance. They provide a link to this paper https://arxiv.org/pdf/1509.02237.pdf On page 10 they state proposition 1, namely
The $p$ -Wasserstein distance between two probability measures $P$ and $Q$ on $\mathbb{R}$ with $p$ -finite moments can be written as $$W_{p}^p(P,Q) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt $$ where $F^{-1},G^{-1}$ are the quantile functions of $P$ and $Q$ respectively. Now the proof is provided on page 17. I sort of follow most of it but how do they arrive the last result $$\int_{supp \pi^* } |x-y|^p d \pi^*(x,y) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt$$ My guess is they made the substitution $F(x)=G(y) = t$ so then the substitution works for $|x-y| = |F^{-1}(t) - G^{-1}(t)|$ but i can't get the rest to work out. Can somebody add the steps in logic in?","['convex-optimization', 'multivariable-calculus', 'optimal-transport', 'metric-spaces']"
4002130,Frobenius theorem to solve PDE v.s. other techniques,"When is the Frobenius theorem used to prove existence for PDE on manifolds, as opposed to more analytical techniques? I apologize that my question is pretty vague, but it stems from confusion about what techniques are generally used in geometric PDE. Sometimes I see more analytic machinery (from, for instance, Evans' textbook on PDE) applied in coordinates. Others (Lee's books on manifolds/Riemannian manifolds) prove existence to PDE using the Frobenius theorem. Could someone shed some light on when one technique is used versus another, or any similarities/differences between them? (Perhaps I am making a distinction where there really isn't one?)","['frobenius-method', 'partial-differential-equations', 'differential-geometry']"
4002152,Legendre's three-square theorem,"Problem: If $ n \in \mathbb{N} $ can be represented as $ n = n_1^2 + n_2^2 + n_3^2 ,\quad n_1, n_2, n_3 \in \mathbb{Z}, $ show that then $ n \neq 4^a(8m+7)$ for $a,m \in \mathbb{Z}.$ My solution: Since this is the beginning of a math course in Discrete Mathematics, I don't think we're supposed to use the quadratic reciprocity law, Dirichlet's theorem on arithmetic progressions nor the equivalence class of the trivial ternary quadratic form. as listed on Wikipedia . It also states on Wikipedia (Why?) that every square is congruent to either $0,1,4$ in $\mathbb{Z}_8$ . So the sum of three squares $n \in \{0,1,2,3,4,5, 6\} \subset \mathbb{Z}_8.$ My initial plan was to show that $ 4^a(8m+7) \in \{6,7\}$ . This not true though, as $ 4^a \equiv 4 $ or $ \equiv 0 $ for $ a \geq 1 $ . Thus $ 4^a(8m+7) \equiv 0 $ or $4$ . Any hints? Muchos gracias!","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
4002166,Is the matrix $\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$ significant?,"Is the matrix $\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$ significant? The reason I am asking is as follows. As a 'challenge' question at the end of my textbook it asked  to prove by induction that if $$\textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$$ then for $n\in\mathbb N$ $$\textbf{A}^n=\textbf{A}=\begin{pmatrix} \cosh^2 x & \cosh^2 x\\ -\sinh^2x&-\sinh^2x\end{pmatrix}$$ I proved this easily, but it struck me that this matrix behaves in exactly the same way as the identity matrices. Of course, this result is equivalent to $$\begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix}^n=\begin{pmatrix} \cos^2 x & \cos^2 x\\ \sin^2x&\sin^2x\end{pmatrix}$$ So, are  these results used in  more advanced linear algebra or university level maths? It just seems too coincidental not to be used in some more advanced results or topics.","['matrices', 'algebra-precalculus', 'linear-algebra', 'trigonometry']"
4002199,sheaf inclusion of line bundle,"Suppose $(X,\mathcal O _X)$ is a complex manifold, $L$ a line budle with nontrivial global section $s\in H^0(X,L)$ . Then in sheaf category, one has a inclusion $\mathcal O_X\rightarrow L$ . Notice that it is equivalent to for every open set $U\in X$ , $\mathcal O_X(U)\rightarrow L(U)$ is injective. So does it mean that if $0\neq s\in H^0(X,L)$ , $s_{|U}\neq 0$ ? I know for holomorphic functions, there exist identity theorem. But in my opinion, there exists some differences between the two notions (take open covering...) So, how should I prove the sheaf inclusion? Thanks in advance.","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory', 'differential-geometry']"
4002252,Proving $\sum_{n=0}^\infty\frac{x^n}{n!}>0$ for all $x\in\Bbb R$ without invoking any Taylor series knowledge base.,"While studying basic Taylor expansions, I came across this expansion of $$e^x=\sum_{n=0}^\infty\frac{x^n}{n!}.$$ We already know this function is positive for all $x$ . But let us assume we don't know about the Taylor expansion , and try to prove this in the expanded form only, how to will we prove it ? It's already clearly  visible for $ x > 0$ as well as for $ -1<x<0 $ . But how to prove it for $x<-1$ ? I have tried much but not able to build up a strong convincing proof. Any help would be highly appreciated. thanks! P.S : prove it 'WITHOUT' using $e^{x}$ function.","['calculus', 'functions']"
4002259,"Proving that $\frac{\nabla^2 u}{ds \, dt} - \frac{\nabla^2 u}{ds \, dt} = k_{\nabla}(\frac{d\gamma}{ds}, \frac{d\gamma}{dt})(u)$","I am currently trying to make Exercise 20 of these lecture notes .
It should be a very simple exercise, but I am swamped due to the amount of bookkeeping. I fear I am missing a very simple nuance that makes it impossible for me to solve this problem. I'll briefly mention the exercise below with the relevant notes from the lecture notes. The Exercise Exercise 20. Let $E \to M$ be a vector bundle equipped with the connection $\nabla$ . Let $\gamma = \gamma(t,s) \colon I_1 \times I_2 \to M$ be a smooth map defined on the product of two intervals $I_1, I_2 \subset \mathbb{R}^2$ and $u = u(t,s) \colon I_1 \times I_2 \to E$ a smooth covering. Show that $$
\frac{\nabla^2 u}{ds \, dt} - \frac{\nabla^2 u}{ds \, dt} = k_\nabla \left( \frac{d\gamma}{ds}, \frac{d\gamma}{dt} \right)(u),
$$ where $k_\nabla$ in the induced curvature. Clarifying the notation Let me elucidate the notation $\frac\nabla{dt}$ and $\frac\nabla{ds}$ .
If $\gamma \colon I \to M$ is a path, then we can use sections $s \in \Gamma(E)$ to produce smooth coverings $u = s \circ \gamma \colon I \to E$ .  This implies that the notation $$
\nabla_{\dot \gamma}(s)(\gamma(t)) 
$$ makes sense as you can view $\dot \gamma$ as a sort of vector field. We denote this as $$
\frac{\nabla(s \circ \gamma)}{dt}.
$$ So more generally, we can define the operator $\frac{\nabla}{dt}$ to define a new path for smooth coverings of $\gamma$ $$
\frac{\nabla u}{dt} \colon I \to E.
$$ Locally with respect to a frame $e = (e_j)_j$ , we may write $u(t) = \sum_j u^je_j(\gamma(t))$ . Then using formula (1.18) from these lecture notes, one may write the $i$ 'th component of $\frac{\nabla u}{dt}$ with respect to this frame as $$
\left( \frac{\nabla u}{dt} \right)^i = \frac{d u^i}{dt}(t) + \sum_j(t) \omega^i_j(\dot \gamma(t)).
$$ Here $\omega = (\omega^i_j)$ represents the connection matrix, representing the one-forms that you obtain from the equation $\nabla_X(e_j) = \sum_i \omega_i^j(X) e_i$ . My strategy, my work and what I find hard For this exercise I had the following strategy in mind. I was planning to use the last formula twice:  twice to calculate $\frac{\nabla^2 u}{ds \, dt}$ and twice to calculate $\frac{\nabla^2 u}{dt \, ds}$ . From there I hoped it would resemble the local form of the curvature. What I find hard is the following: By applying that formula, you get a lot of terms, and I guess that my bookkeeping skills can't keep up at the moment. Maybe I am just going about it the wrong way. Here is my work until now. I also found it hard to properly express the local form of the curvature in terms of the connection: $k = d \omega + \omega \wedge \omega$ , where $(\omega \wedge \omega)^i_j = \sum_k \omega^i_k \wedge \omega^k_j$ is the local connection matrix. I was never able to prove this (maybe I should open a separate thread for this, but so be it). So am I missing an important nuance? Is this the right strategy? It this just really a matter of bookkeeping and am I just messing up? Thanks in advance for the feedback!","['vector-bundles', 'curvature', 'differential-geometry']"
4002279,Cardinality of set of functions,"Let $\mathbb{N}=\{0,1,2,\ldots\}$ and we consider the following set: $$\{f:\mathbb{N}\longrightarrow\mathbb{N} \mid\forall n\in\mathbb{N} \ , f(n)=f(n+1)+f(n+2)\}$$ I can tell that this set has at least one element $f:\mathbb{N}\to\mathbb{N}$ such that $f(n)=0$ but can we say more about its cardinal? and what would happen in the case of decreasing functions","['elementary-set-theory', 'discrete-mathematics']"
4002291,Question on proof of fundamental theorem of calculus,"Let be $F:[a,b]\to\mathbb{R}$ , where $F(x):=\int\limits_a^xf(t)dt$ . We assume $f$ to be continuous at point $x_0$ and Riemann-integrable over $[a,b]$ . Then we know that $F'(x_0)=f(x_0)$ . My (edited) approach (we assume $x<x_0$ ): Let's define the function $M:[a,x_0]\to\mathbb{R}$ , where $M(x):=\sup\{\left|f(t)-f(x_0)\right|\mid t\in[x,x_0]\}$ . If we choose an arbitrary $\frac{\epsilon}{2}>0$ then by continuity of $f$ at $x_0$ there exists a $\delta>0$ such that for all $t\in [x,x_0]$ with $|t-x_0|<\delta$ we have $|f(t)-f(x_0)|<\frac{\epsilon}{2}$ . By definition of the supremum there exists a $t'\in[x,x_0]$ such that $M(x)-\frac{\epsilon}{2}<|f(t')-f(x_0)|$ and therefore $M(x)<\frac{\epsilon}{2}+|f(t')-f(x_0)|<\epsilon$ . So if we only admit $x\in [a,x_0]$ such that $|x-x_0|<\delta$ we see that $M(x)$ is continuous at $x_0$ . Keeping $|x-x_0|<\delta$ in mind, allows us to conclude: $$
\left|\frac{\int\limits_x^{x_0}f(t)dt}{x-x_0}-f(x_0)\right|=\left|\frac{\int\limits_x^{x_0}f(t)-f(x_0)dt}{x-x_0}\right|\leq \left|\frac{M(x)(x_0-x)}{x-x_0}\right| = M(x) <\epsilon.
$$ The case "" $x>x_0$ "" is basically is the same. Hence, $F'(x_0)=f(x_0)$ . Is this proof o.k.?","['integration', 'limits', 'solution-verification', 'real-analysis']"
4002317,Question on convolution in measure theory to probability theory and the need for independence,"In a classical measure theory context, with two measures $\mu_{1},\; \mu_{2}$ on $\mathcal{B}(\mathbb R)$ as the image of the product measure $\mu_{1}\otimes \mu_{2}$ under the map $+:\mathbb R\times \mathbb R \to \mathbb R$ is defined as a convolution. When switching over to probability theory, I notice that we also introduce the notion of independence, i.e. for independent random variables $X$ and $Y$ the convolution is the distribution of the sum $X+Y$ . Question: Why do we need the notion of independence for the random variables? Surely by the definition of the convolution in measure theory it immediately follows that we are evaluating the sum of the random variables? Or am I missing something obvious? I guess the notion of not knowing what the product measure looks like could be the pitfall. Thanks in advance!","['measure-theory', 'real-analysis', 'stochastic-processes', 'functional-analysis', 'probability-theory']"
4002347,Why do we need that $f$ is continuous at $L$ to have $\lim \limits_{n \to \infty} f(a_{n}) = f(L)$ when $\lim \limits_{n \to \infty} a_{n} = L$?,"If $$\lim \limits_{n \to \infty} a_{n} = L $$ and the function $f$ is continuous, then $$\lim \limits_{n \to \infty} f(a_{n}) = f(L)$$ I do not understand why do we have to indicate that $f$ is continuous at $L$ . Can't we just say that $f$ is defined at $L$ ?","['limits', 'calculus', 'continuity', 'sequences-and-series']"
4002357,If a sequence $f_{n}\neq0$ converges uniformly and $\frac{1}{f_{n}}$ converges pointwise then it also converges uniformly,"I have this question I'm unable to solve (you need to prove this or give a counterexample: If a sequence $f_{n}\neq0$ ( $f_{n}\left(x\right)\neq0,\forall n\geq N\in\mathbb{N}$ for some N) converges uniformly and $\frac{1}{f_{n}}$ converges pointwise then it also converges uniformly. I tried proving $$\lim_{n\to\infty}\sup\left|\frac{1}{f_{n}\left(x\right)}-\frac{1}{f\left(x\right)}\right|=0$$ but it didn't work. I tried looking at function of the form $\frac{1}{n}$ but $\frac{1}{\frac{1}{n}}=n$ doesn't converge. I tried looking at $$f_{n}\left(x\right)=\begin{cases}
\frac{1}{n} & x\leq\frac{1}{n}\\
0 & x>\frac{1}{n}
\end{cases}$$ but it doesn't fit the demand of $f_{n}\neq0$ . So I wasn't able to prove nor provide a counterexample","['functions', 'pointwise-convergence', 'uniform-convergence']"
4002495,The formulas of prostapheresis: memorization technique,"This question is related purely for my students of an high school and indirectly for me. The formulas below are the formulas of prostapheresis, \begin{cases}
\sin\alpha+\sin\beta=2\,\sin \dfrac {\alpha+\beta}{2}\, \cos \dfrac {\alpha-\beta}{2} \\
\sin\alpha-\sin\beta=2\sin \dfrac {\alpha-\beta}{2} \,\cos \dfrac {\alpha+\beta}{2}\\
\cos\alpha+\cos\beta=2\cos \dfrac {\alpha+\beta}{2}\,\cos \dfrac {\alpha-\beta}{2}\\
\cos\alpha-\cos\beta=-2 \,\sin \dfrac {\alpha+\beta}{2} \,\sin \dfrac {\alpha-\beta}{2}
\end{cases} and while I am able to find them, I am not able to find a technique to memorize them. Is there a technique to be able to memorize them?","['mnemonic', 'trigonometry', 'education', 'algebra-precalculus', 'soft-question']"
4002510,Prove this formula $\frac{1-r\cos(x)}{1-2r\cos(x)+r^2}= 1+\sum_{n=1}^{+\infty}r^{n}\cos\left(nx\right)$,"I am trying to use prove, by just simple algebraic manipulation, to prove the equality of this formula. $$\dfrac{1-r\cos(x)}{1-2r\cos(x)+r^2}= 1+\sum_{n=1}^{+\infty}r^{n}\cos\left(nx\right)$$ I have been given hints and instructions from this thread Write using Euler’s identity $$\cos(x)=\frac{e^{ix}+e^{-ix}}{2}$$ Factor the denominator Find the partial fractions decomposition, expand the parts as geometric series, and convert from exponential functions back to trigonometric functions (Euler’s identity again). This is how I have done: RHS: $$\dfrac{1-r(\dfrac{e^{ix}+e^{-ix}}{2})}{1-2r(\dfrac{e^{ix}+e^{-ix}}{2})+r^{2}}=\dfrac{1-r(\dfrac{e^{ix}-e^{-ix}}{2})}{1-re^{ix}-re^{-ix}+r^2}$$ Then from this thread, I have learnt how to factorize the denominator (See Jack D'Aurizio answer, first answer of the thread, first line). The trick is to write $1=e^0$ . $$\dfrac{1-r(\dfrac{e^{ix}+e^{-ix}}{2})}{(re^{ix})(r-e^{-ix})}$$ The third step is to decompose the fraction: $$\dfrac{A}{r-e^{ix}}+\dfrac{B}{r-e^{-ix}}=\dfrac{1-r(\dfrac{e^{ix}+e^{-ix}}{2})}{(r-e^{ix})(r-e^{-ix})}$$ $$A(r-e^{-ix})+B(r-e^{ix})=1-r(\dfrac{e^{ix}+e^{-ix}}{2})$$ I am stuck here, I notice that let $x=0$ , we will have $$A(r-1)+B(r-1)=1-r(\dfrac{e^{ix}+e^{-ix}}{2})$$ I am stuck here, I don't know to find $A$ and $B$ Also, could you provide in details how to finish the part ""expand the parts as geometric series, and convert from exponential functions back to trigonometric functions (Euler’s identity again)"". My symbolic manipulation skill is not very good, so a detailed answer is great! Thanks!","['summation', 'calculus', 'sequences-and-series', 'power-series', 'complex-numbers']"
4002605,Can I use a z test to compare two samples if there is a significant difference in the size? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question I have to compare two unequal size samples from a population of 53k. One has a proportion of 56% and size of 50k and the other has a proportion of 50% and size of 3k. Can I do a two proportion z test to compare to make a case that the proportion of sample 1 is greater than sample 2?
Does the difference in the sample size pose an issue? Some more info:
Here is the situation: Lets say I had 53k people and 50k received treatment A and 3k received treatment B. From the 3k, 50% showed positive results and from the 50k, 56% showed positive effect. How can I conclude that my treatment A was more effective than treatment B? I am looking for a way to make that case.","['statistics', 'probability']"
4002634,MGF bound for non-centered random variables?,"So I was trying to prove the following fact: if $|X| \leq 1$ is a random variable with mean $\mu$ , then $\mathbb{E} e^{t (X - \mu)} \leq e^{t^2/2}$ , for any real $t$ . I can show this in the special case $\mu = 0$ :
for $x \in [-1, 1]$ we note by convexity $$
e^{tx} \leq \frac{1-x}{2} e^{-t} + \frac{1 + x}{2} e^{t}. 
$$ Hence, taking expectations: $\mathbb{E} e^{t (X -\mu)} \leq \cosh(t) \leq e^{t^2/2}$ . Any ideas how to extend this argument to general $\mu$ ?","['moment-generating-functions', 'concentration-of-measure', 'probability-theory', 'upper-lower-bounds']"
4002742,ADE singularity and Milnor lattice,"I am reading this paper: https://arxiv.org/abs/0810.2687 by A. J. de Jong and Robert Friedman. In the proof of Theorem 4.10, a singularity of the following type shows up $$y^2=x^3+z^{6d-1}.$$ When $d=1$ ,  this is exactly the type of $E_8$ in Du Val singularities. And the Milnor lattice and Dynkin diagram can be obtained by blowing up. In general, looks like the corresponding Milnor lattice is isomorphic to $(2d-2)U\oplus d(-E_8)$ . I am wondering how to verify this in this case. In particular, can this be deduced from the case when $d=1$ ? Thanks!","['number-theory', 'singularity', 'singularity-theory', 'algebraic-geometry', 'algebraic-topology']"
4002755,Meaning of a formula for the $y$-intercept?,"Suppose a line goes through the points $A = (x_1, y_1)$ and $B = (x_2, y_2)$ . One can easily check (as I did today while doodling) that $$
b = - \frac{x_1 y_2 - x_2 y_1}{x_2 - x_1}
$$ where $b$ is the y-coordinate of the y-intercept. This can be written more suggestively as $$
b = -\frac{\det(A, B)}{\Delta x} \label{b}\tag{1}
$$ The presence of $\det(A,B)$ suggests a geometrical interpretation, but I couldn't think of one.
This reminds me of Cramer's Rule, but I couldn't make that connection explicit either. Can anyone give an interpretation for equation \ref{b}?","['algebra-precalculus', 'determinant', 'intuition']"
4002812,Proving the existence/non-existence of prime elements,"No answers, please. Hints only. Given that $n$ is a positive integer and $x=(n+1)!+2$ , I want to prove whether or not the sequence $x, x+1, \cdots, x+(n-1)$ contains no primes. I did a lot of algebra to find that $x+(n-1)$ is no more than just $(n+1)[n!+1]$ . But I'm not sure what to do with this, I'm still unsure what this says about the conclusion.","['factorial', 'number-theory', 'proof-writing', 'discrete-mathematics', 'prime-numbers']"
4002820,Prove that the probability there exists at least 10 triangles in the random graph is greater in RG(p) than RG(q) if p > q,I came across this question and I don't know where to begin: Assume you have a random 100-vertex graph and define the probability of a graph G to be $p^{E(G)}(1-p)^{100 choose 2 - E(G)}$ where $E(G)$ is the number of edges and p is the parameter (the probability a given edge exists). Prove that the probability there exists at least 10 triangles in the random graph is greater in RG(p) then RG(q) if p > q Does anyone have any suggestions on how to start?,"['graph-theory', 'random-graphs', 'discrete-mathematics', 'probability']"
4002840,Is this the normal derivative of a measure? What does it mean?,"I'm reading Filippo Santambrogio's Optimal transport for Applied Mathematicians and I've come across the following (on page 123). The context is that $\mathbf{v}_t$ is a vector-valued velocity function and $\varrho_t$ is a density measure. We say that a family of pairs of measures/vector fields $(\varrho_t, \mathbf{v}_t)$ with $\mathbf{v}_t \in L^1(\varrho_t; \mathbb{R}^d)$ and $\int_0^T ||\mathbf{v}_t||_{L^1(\varrho_t)} dt < +\infty$ solves the continuity equation $$ \partial_t \varrho_t \, + \, \nabla \cdot (\varrho_t\mathbf{v}_t) = 0 $$ on $(0,T)$ in the distributional sense if, for any bounded and Lipschitz test function $\phi \in C_c^1((0,T)\times \overline{\Omega})$ , we have $$ \int_0^T \int_\Omega (\partial_t\phi) d\varrho_tdt + \int_0^T \int_\Omega \nabla \phi \cdot \mathbf{v}_td\varrho_t dt = 0$$ This formulation includes no-flux boundary conditions on $\delta \Omega$ for $\mathbf{v}_t$ , i.e. $\varrho_t \mathbf{v}_t \cdot \mathbf{n} = 0$ My question is this: what does the boundary condition at the end ( $\varrho_t \mathbf{v}_t \cdot \mathbf{n} = 0$ ) mean? It seems like we are multiplying a measure by a two vector fields. Does this give us a measure? Do we find the dot product of the vector fields first?","['measure-theory', 'optimal-transport', 'distribution-theory', 'functional-analysis', 'partial-differential-equations']"
4002847,Cartesian product of two collections of sets?,"If we have $\mathcal{G} = \{\phi, \{0\}, \{1\}, \{1,2\} \},$ would $\mathcal{G} \times \mathcal{G}$ have the form $\{\phi, (\{0\}, \{0\}), (\{0\}, \{1\}), (\{0\}, \{0, 1\}), \dots \}$ ? In that case, what would an example of $(w_1, w_2) \in B \in \mathcal{G} \times \mathcal{G}$ be? My working: Take for example $B = (\{0\}, \{0, 1\})$ . This is not a set; so, I don't understand how one could take an element $(w_1, w_2) \in B$ . P.S.: I am taking a Probability Theory class now and even though I did a Measure Theory class last term, I am finding things very difficult. But I really want to get good at this stuff. Would there be any reading material you'd recommend? (I find the lectures hard to follow.) EDIT:
I encountered this statement in the book Theory of Probability and Random Processes by Koralov. In Definition 2.1, he states: A finite-dimenstional cylinder is a set of the form $$A = \{\omega: (\omega_{t_1}, \dots, \omega_{t_k}) \in B \},$$ where $t_1, \dots, t_k
\geq 1$ , and $B \in \mathcal{G} \times \dots \times \mathcal{G}$ (k
times).","['measure-theory', 'probability-theory']"
4002861,Understanding Corollary 2.3.2 from Bickel's Mathematical Statistics.,"I am trying to understand the Corollary 2.3.2 from Bickel's book mathematical statistics . The corollary says Consider the exponential
family $$p(x,\theta)=h(x)\exp\left\{\sum_{j=1}^kc_j(\theta)T_j(x)-B(\theta)\right\},x\in\mathcal{X},\theta\in\Theta.$$ Let $C^0$ denote the interior of the range of $(c_1(\theta),c_2(\theta),\dots,c_k(\theta))^T$ and let $x$ be the
observed data. If the equations $$E_\theta T_j(X)=T_j(x),j=1,\dots,k$$ have a solution $C(\hat{\theta})\in C^0,$ then it is the unique MLE of $\theta$ . I think the conclusion of this corollary is useful, as it is not easy to show the existence and uniqueness of a curved exponential family in general. However, the book does not given any example of using this corollary. I am trying to understand this corollary. Can someone give an example showing the usefulness of it?","['statistics', 'probability-theory', 'maximum-likelihood']"
4002898,Solving $\cos(2\theta)=\cos(\theta)$,"The question : Solve Equation $\cos(2\theta)=\cos(\theta)$ for $0 \le \theta \le 2\pi$ in terms of $\pi$ . My solution: - Using trigonometry identity , I quickly get $$2\cos^2(\theta)-1=\cos(\theta)$$ replcae $$\cos(\theta)=a.$$ I'll get quadratic equation of $$2a^2-a-1=0,$$ where $a$ is $$a = -\frac12 , 1.$$ The result I got is $$\theta=0,{2\over3}\pi,2\pi.$$ The scheme said I'm missing is $\frac43\pi$ . Where am I missing? How to achieve it?",['trigonometry']
4002930,Subgroups of semidirect product of two abelian groups,"Let $G= N \rtimes_{\varphi} Q$ be a semidirect product coming from a short exact sequence $$ 1 \rightarrow N \rightarrow G \rightarrow Q \rightarrow 1 $$ and assume that $N$ and $Q$ are finitely generated abelian groups. Suppose that there is $N_{0}$ a finite index subgroup in $N$ such that $\varphi_{|N_{0}}$ is the  trivial homomorphism. That is, $Q$ acts trivially on $N_{0}$ by conjugation. My question is whether $N \rtimes_{\varphi} Q $ is VIRTUALLY abelian. My attempt is the following: Suppose that $N= N_{0}g_{1} \dot{\cup} \cdots \dot{\cup} N_{0} g_{n}$ for some $g_{1},\dots,g_{n}\in N$ (this comes from the fact that $N_{0}$ has finite index in $N$ ). Since $N_{0}$ is not necessarily normal in $G$ , we cannot take the product $N_{0}Q$ . Nevertheless, we can take the subgroup generated by $N_{0}$ and $Q$ , $\langle N_{0}, Q \rangle$ . I would like to show that $\langle N_{0}, Q \rangle$ has finite index in $G$ . Note that $G= NQ= QN$ . Thus, for any $g\in G$ , $$g= qn,$$ for some $q\in Q, n\in N$ . But since $n\in N$ , $n= n_{0}g_{i}$ for some $n_{0}\in N_{0}$ , $i\in \{1,\dots,n\}$ . Thus, $$G= \langle N_{0},Q \rangle g_{1} \cup \dots \cup \langle N_{0}, Q \rangle g_{n}.$$ In conclusion, $\langle N_{0}, Q \rangle$ has finite index in $G$ . In addition, $\langle N_{0}, Q \rangle$ is abelian because $N_{0}$ and $Q$ are abelian and $Q$ acts trivially on $N_{0}$ .","['exact-sequence', 'semidirect-product', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4002970,What Is This Measure Theory Rule Called? Is it Just the Substitution Rule in Disguise?,"Let $(\Omega,\mathcal{F},\mu)$ be a measure space. For a $\mathcal{B}(\mathbb{R})$ -measurable function $f \geq 0 : \Omega \rightarrow \mathbb{R}$ one can define a measure $\nu$ by: $$
\nu(A) : = \int_A f d \mu = \int f \mathbb{1}_{A} d \mu
$$ $\nu$ is provably a measure according to this answer . Now suppose we have some $\mathcal{B}(\mathbb{R})$ -measurable function $g : \Omega \rightarrow \mathbb{R}$ . And suppose $g$ is integrable w.r.t. $\nu$ . Question: Does this identity have a standard name in measure theory: $$\int gf d \mu = \int g d \nu$$ Note: maybe the exact identity above isn't a named result in measure theory, but can be shown via the application of one or two standard measure theory results? I think I can prove the identity using first principles of measure theory, but I'd rather use more standard results and my common sense is telling me the above is so simple it must have a standard name in measure theory that I'm just missing. The proof of the identity I imagine is to use the approximation theorem for integrals, which asserts that a sequence of increasing simple functions approximates $\int g d \nu$ . That is, we approximate with functions of the form: $$\sum_{i=1}^n a_i \nu(A_i) = \sum_{i=1}^n a_i \int_{A_i} f d \mu$$ We can then invoke the approximation theorem again for the integrals inside the sum: $$\sum_{i=1}^n a_i \sum_{j=1}^m b_{i,j} \mu(B_{i, j}) = \sum_{i=1}^n \sum_{j=1}^m a_ib_{i,j} \mu(B_{i, j})$$ Where $\bigcup_{j=1}^m B_{i, j} = A_i$ . Since all the $B_{i,j}$ are disjoint, then the above is a simple function. Moreover, our invocations of the approximation theorem guarantee us that: $\forall \omega \in B_{i,j} \subseteq A_i : a_i \leq g(\omega)$ $\forall \omega \in B_{i,j} : b_{i,j} \leq f(\omega)$ Therefore, we have: $$\forall \omega \in B_{i,j} : a_ib_{i,j} \leq gf(\omega)$$ And so the term on the RHS is the integral of a simple function which is bounded by $gf$ . And since the integral of $gf$ is the supremum over all such integrals of simple functions we have the bound: $$\int gf d \mu \geq \int g d \nu$$ OK... so this is just an inequality rather than an identify. But I believe the proof can be fixed & completed by invoking the approximation theorem more fully: this will give us a sequence of simple functions converging to $f$ and another sequence converging to $g$ . Then we can show that the resulting simple functions constructed above converges to $gf$ and so by the monotone convergence theorem the integrals will converge also. But rather than complete this result in my own clunky way, I'd rather first see if there is some known result or results that can be applied here rather than go right back to first principles and simple functions. Note: The induced measure $\nu$ is a measure on the original measurable space $(\Omega,\mathcal{F})$ - not a pushforward measure to $\mathcal{B}(\mathbb{R})$ . So I can't see how something like change of variables / substitution can be applied. Unless I'm just not being creative enough... Credit to this question for borrowed text.","['integration', 'measure-theory', 'reference-request']"
4002975,Prove that the following statement involving Buchstab function is true,"I came across this problem in this book called Cambridge studies in advanced mathematics 97, page 217-218. I wad reading this line where the author starts the inductive steps and apply the inductive hypothesis and yields $$\Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \bigg(\frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})}+O\bigg(\frac{p}{\ln(p)}\bigg)+O\bigg(\frac{x}{p\ln^2(x)}\bigg)\bigg)$$ which indeed is $$\Phi(x,x^{\frac{1}{u}}) = \frac{U\omega(U)x}{\ln(x)}+O\bigg(\frac{x}{\ln^2(x)}\bigg)+O\bigg(\frac{x^{\frac{2}{U}}}{\ln^2(x)}\bigg)+O\bigg(\frac{x}{\ln^2(x)}\bigg)+\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{u_p\omega(u_p)x}{p\ln(\frac{x}{p})}$$ since the author wrote that the sum over $p$ of the first error term is $\ll x^{\frac{2}{U}}/\ln^2(x)$ , and the sum over $p$ of the second is $\ll x/\ln^2(x)$ . The author then estimate the contribution of the main term in the sum by writing the Prime Number Theorem in the form $\pi(t)=\mathrm{li}(t)+R(t)$ , then apply Riemann-Stieltjes integration and yields $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+f(t)R(t)\bigg|_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}}-\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} R(t)\,df(t)$$ where $$f(t)=\frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)}$$ which in fact it is $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^2(t)}\,dt+\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln(t)}\,dR(t) $$ before applying Riemann-Stieltjes integration. I noticed that for large $t$ , we have $$dR(t)=\bigg(\frac{\ln(t)-1}{\ln^2(t)}-\frac{1}{\ln(t)}\bigg)\,dt=-\frac{dt}{\ln^2(t)}$$ I am also reading this paper on Dickman function and noticed the similar situation $$\sum_{x^d<p\leq x^{u_1}} \rho\bigg(\frac{\ln(p)}{\ln(x)-\ln(p)}\bigg)\frac{x}{p}=x\int_{x^d}^{x^{u_1}} \rho\bigg(\frac{\ln(t)}{\ln(x)-\ln(t)}\bigg)\,dF(t)$$ where $F(t)=\sum_{p\leq t} \frac{1}{p}$ , therefore I guess that $\frac{1}{p\ln(p)}$ could also be expressed as such and my conjecture would be $$\sum_{x^{\frac{1}{u}}\leq p<x^{\frac{1}{U}}} \frac{\omega(\frac{\ln(x)}{\ln(p)}-1)x}{p\ln(p)}=\int_{x^{\frac{1}{u}}}^{x^{\frac{1}{U}}} \frac{(\ln(t)-1)\omega(\frac{\ln(x)}{\ln(t)}-1)x}{t\ln^3(t)}\,dt$$ The difficulty that I am facing is to prove the conjecture I made above, but currently I have no idea how this can be achieved. Is this conjecture correct and if it is correct, then how do I show it? Therefore I hope someone could explain the things that I am missing.","['analytic-number-theory', 'number-theory', 'analysis', 'real-analysis']"
4002979,Proof of inequality $0<\sum_{k=1}^n\frac{\sin kx}{k}<\int_{0}^{\pi}\frac{\sin x}{x}dx$。,"I give a proof of a question in AoPSonline https://artofproblemsolving.com/community/c7h492604_sum_k0nfracsinkxk $\textbf{What I want to know is: is there some mistake in my proof, any help will welcome!}$ The LHS is just Fejer-Jackson inequality.
Let $$f_n(x)=\sum_{k=1}^n\frac{\sin kx}{k},\quad x\in[0,\pi]$$ For the RHS, we can deal with as follows:
consider $$f'_n(x)=\sum_{k=1}^{n}\cos kx=\frac{\cos\frac{n+1}{2}x\sin\frac{n}{2}x}{\sin \frac{x}{2}},x\in(0,\pi),$$ then $$f'_n(x)=0\iff \cos\frac{n+1}{2}x\sin\frac{n}{2}x=0,\quad x\in(0,\pi),$$ the critical points contained in $(0,\pi)$ of $f_n$ are listed below: $$\text{critical points}:\frac{\pi}{n+1}<\frac{2\pi}{n}<\frac{3\pi}{n+1}<\cdots.$$ Remark：The number of critical points of $f_n$ is $n$ if $n$ is odd, and is $n-1$ if $n$ is even.
It can be shown that $$f_n\left(\frac{\pi}{n+1}\right)=\sum_{k=1}^n\frac{\sin\left(\frac{k\pi}{n+1}\right)}{k}
=\max_{x\in[0,\pi]}f_n(x).$$ Note that: $\frac{\pi}{n+1}$ is the smallest critical point of $f_n$ .
We claim that: (1) sequence $\left\{f_n\left(\frac{\pi}{n+1}\right)\right\}$ is strictly increasing（can refer $\sum_{k=1}^{n}\frac1{k}\sin(\frac{k\pi}{n+1})$ is increasing ）; (2) $$\lim_{n\to\infty}\sum_{k=1}^n\frac{\sin\left(\frac{k\pi}{n+1}\right)}{k}
          =\int_{0}^{\pi}\frac{\sin x}{x}dx;$$ (3) $$f_n(x)<\int_{0}^{\pi}\frac{\sin x}{x}dx\quad x\in(0,\pi).$$ Due to $f_n$ is strictly increasing on $[0,\frac{\pi}{n+1}]$ ,
we know $$f_{n+1}\left(\frac{\pi}{n+2}\right)>f_{n+1}\left(\frac{\pi}{n+1}\right)
=f_{n}\left(\frac{\pi}{n+1}\right).$$ $$\sum_{k=1}^n\frac{\sin\left(\frac{k\pi}{n+1}\right)}{k}
=\frac{1}{n+1}\sum_{k=1}^n\frac{\sin\left(\frac{k\pi}{n+1}\right)}{\frac{k}{n+1}}\to \int_{0}^{\pi}\frac{\sin x}{x}dx
\quad (\text{Riemann\ sum})$$ So we conclude: $$f_n(x)\leq f_n\left(\frac{\pi}{n+1}\right)<\sup_{n}f_n\left(\frac{\pi}{n+1}\right)=\lim_{n\to\infty}\sum_{k=1}^n\frac{\sin\left(\frac{k\pi}{n+1}\right)}{k}
          =\int_{0}^{\pi}\frac{\sin x}{x}dx.$$","['integration', 'inequality', 'solution-verification', 'sequences-and-series', 'trigonometry']"
4002985,Find a solution to $tx''+x'+2x = 0$,"Find a nontrivial (no need to be general) solution to $$tx''(t)+x'(t)+2x(t) = 0$$ W/A spells the solution to some Bessel function. I tried to use the series method so I derive $$
x = \sum_{i=0}^\infty a_nx^n, \quad a_{n+1} = \frac{-2}{(n+1)^2} a_n
$$ well it is not the cleanest solution for this, so I'd like to ask if a explicit solution (non-Bessel function) is available.",['ordinary-differential-equations']
4002992,Can $n!$ start with the digits 2020...,"Can anyone give me any hints on how to tackle this? I assume the answer is no otherwise why ask the question. I am aware that $k \mid n!$ for all $k\leq n$ , and I am aware that divisibility by $3$ , $9$ and $11$ in particular have nice and easy tests involving sums of digits etc. I know that $2020 = 2^2 \cdot 5 \cdot 101$ written as a product of primes. However I am not seeing how these things help.. Thanks Edit: Plot twist... $384!$ begins with the digits $2020$ so now the question is how do we prove this can happen (let's pretend we don't have computers)?","['number-theory', 'factorial', 'elementary-number-theory']"
4003001,"If the base change of varieties are isomorphic, are they birationally equivalent?","Let $X,Y$ be smooth projective varieties over a field $k$ . Assume that there is a finite Galois extension $L/k$ such that there is an isomorphism $f : X \times_k L \cong Y \times_k L$ . Is it true that $X$ and $Y$ are birationally equivalent over $k$ ? Notice that $f$ does not need to be the base change of a morphism $X \to Y$ . Also, it does not need to be Galois-equivariant. I don't know if one could see $X$ as a Zariski-dense subscheme of $X \times_k L$ , because I only know that fiber products provide projection maps $X \times_k L \to X$ (so it does not go in the right direction...).","['extension-field', 'algebraic-geometry', 'birational-geometry']"
4003002,"If $f=g$ almost everywhere, then $\int_E f = \int_E g$.","One task in the book Analysis 2 by Tao is to prove the following Proposition 8.2.6: Let $(E,\mathcal{E},\mu)$ a measure space and $f,g \colon E \to [0,\infty]$ $\mathcal{E}$ -measurable functions on $E$ . If $f=g$ almost everywhere, then $\int_E f = \int_E g$ . My problem is not to prove the claim. For example let $A = \{f \neq g\} \in \mathcal{E}$ , then $f = \chi_A f + \chi_{A^c}f$ , $g = \chi_A g + \chi_{A^c}g$ and $\chi_{A^c}f = \chi_{A^c}g$ . So we have $\int_E f = \int_A f + \int_{A^c} f$ , when we know that interchanging sum and integral is possible. Then it's easy to show the rest. ( $\chi$ is indicator function) But the only properties I have available are: $\int_E f = \sup \left\{ \int_E g : g \text{ is simple, } 0 \leq g \leq f \right\}$ $f_E f = 0 \Leftrightarrow f=0$ almost everywhere $\int_E cf = c \int_E f$ , $c \in \mathbb{R}$ , $c \geq 0$ $f \leq g \Rightarrow \int_E f \leq \int_E g$","['measure-theory', 'lebesgue-integral', 'real-analysis', 'measurable-functions', 'almost-everywhere']"
4003006,Is it true that any uncountably generated sigma algebra can be generated by a countable subset?,"Let $X$ be a set and let $\mathcal C$ be a non-empty collection of subsets of $X$ From my classes I was asked if the following statement is always true: For any $A ∈ σ(\mathcal C)$ , there exists a countable
sub-collection $\mathcal D ⊂ \mathcal C$ such that $A ∈ σ(\mathcal D)$ . This was not difficult to prove true, but I also considered a slight modification to the statement: There exists a countable sub-collection $\mathcal D ⊂ \mathcal C$ such that, for any $A ∈ σ(\mathcal C)$ , $A ∈ σ(\mathcal D)$ The second statement is equivalent to saying: There exists a countable $\mathcal D ⊂ \mathcal C$ such that $σ(\mathcal C) = σ(\mathcal D)$ . My intuition is that this is false but I can't think of any examples at least in the case for $X=\mathbb R$ . So my question is this, is the modified statement true for $X=\mathbb R$ and if yes then is it also true for arbitrary $X$ .",['measure-theory']
4003013,Length of the intersection between a sphere and a cylinder,"Note: while writing this question I realized what I was missing so there is no question here, but I thought it is a nice exercise to share, and I'd like to see more solutions. I am trying to calculate the length of the curve given by $$x^2+y^2+z^2=1, x^2+y^2=x$$ The curve is the intersection between a sphere and a cylinder, and we can  notice is is symmetric around the $xy$ and $xz$ planes, so I tried to calculate only the part where $y>0, z>0$ and multiply by 4. For this part I used the parameterization: $\gamma(t)=(t, \sqrt{t-t^2}, \sqrt{1-t}), t \in [0, 1]$ And then the length should be $l=4\int_0^1 ||\gamma'(t)|| dt = 4\int_0^1 \sqrt{1+\frac{(1-2t)^2}{4(t-t^2)} +\frac{1}{4-4t}} dt = 4\int_0^1 \sqrt{\frac{t+1}{4t-4t^2}} dt$ And I'm not sure how to solve this integral but using an online calculator we can see the length is $\approx 7.64$ Different solutions, or suggestions for ways to solve this integral are welcome!",['multivariable-calculus']
4003019,Is a topology a set or a family?,"So I've just started learning topology and a lot of the definitions confuse me.
My main problem is that some of the definitions are quite inconsistent to me for example in topology without tears the author says that a topology $\tau$ is a set of subsets of $X$ and then he proceeds with the axioms.
However I've also seen a lot from other sources that a topology is a family of subsets of $X$ and then they proceeds to describe the same axioms. However they do not refer to the topology $\tau$ as being a set at all which confuses me. I think my confusion is the definition on what a family actually is ive seen lots of confusing definitions on what a family is like it is a surjective function however I cannot seem to grasp the idea. So why do they define them differently and what is a family? Thanks in advance.","['elementary-set-theory', 'self-learning', 'general-topology', 'set-theory']"
4003020,"Can a group be thought of as some kind of ""rule book"" telling how symmetries interact with each other?","I'm new to Abstract Algebra and I'm trying to explain myself what a group really is. So far I've looked into a couple of books, read a little about groups here and there and this is what I've understood so far. Symmetry: A symmetry of an object $O$ is a bijective function $s:O \to O$ i.e. $s(O)=O$ Intuitively, they are transformations that leave the shape of the object unchanged, they just shuffle around the pieces of the object. Here, I've taken the object $O$ to be the set $O$ since it is my understanding that everything is a set. Group: A group $G$ is collection of symmetries of the object $O$ satisfying: It has a symmetry which does nothing to the object $O$ Every symmetry applied to $O$ can be reversed. Any sequence of symmetries applied to $O$ is also a symmetry of $O$ present in $G$ Given a sequence of symmetries i.e. $s_1,s_2, s_3$ , the order in which you apply this sequence of symmetries to $O$ doesn't matter i.e. $(s_1 \cdot s_2) \cdot s_3 = s_1 \cdot (s_2 \cdot s_3)$ Hence a Group is merely a ""map"" or some kind of ""guide book"" that tells you ""how"" the symmetries interact with each other and you can just throw away the object, you don't care about it. Is my intuition so far correct? If there are any flaws in it, please point out. If you can improve on this intuition, please by all means. Now my questions are, Why do you not care about the object? Given a group, how can I find the object that this group describes the symmetries of? Does every group describe symmetries of some object?","['group-theory', 'abstract-algebra', 'intuition']"
4003027,Every Brownian Motion is the result of Levy's Construction,"I want to show that any Brownian Motion $(B(t))$ on a Probability space $(\Omega,\mathscr{A},\mathbb{P})$ is the a.s. result of a Levy's Construction, i.e. interpolation using a sequence of independent normally distributed random variables $\{Z_d\}_{d\in \mathcal{D}}$ with mean $0$ and variance $1$ where $\mathcal{D} = \{\text{dyadic numbers}\} = \{2^{-n}k: 1\leq k \leq 2^n,n\geq 1\}$ . I think this is true but I would like to have this verified. My attempted proof: We use notation from the book Brownian Motion by Mörters and Peres. Let $\mathcal{D}_n = \{k2^{-n}: 1\leq k \leq 2^n\}$ and let $(\tilde{B}(t))$ be a Brownian Motion attained via Levy's construction using $\{\tilde{Z}_d\}_{d\in \mathcal{D}}$ , where $\{\tilde{Z}_d\}_{d\in \mathcal{D}}$ are independent random variables with Gaussian distribution with mean $0$ and variance $1$ . So that for $d\in \mathcal{D}_n$ $$\tilde B_n(d) = \sum_{i=0}^{n}\tilde F_i(d) = \sum_{i=0}^{\infty}\tilde F_i(d)$$ where for $n>1$ $$\tilde F_n(t) = \begin{cases}2^{-(n+1)/2}\tilde Z_t& \text{ for $t\in \mathcal{D}_n\setminus \mathcal{D}_{n-1}$} \\
0& \text{ for $t\in \mathcal{D}_{n-1}$} \\
\text{linear interpolation between}\end{cases}$$ and $\tilde F_0(t) = t\tilde Z_1$ . Then $$\tilde B(d) = \frac{\tilde B(d-2^{-n})+\tilde B(d+2^{-n})}{2}+\frac{\tilde Z_d}{2^{(n+1)/2}}$$ and hence for every $n$ there is a $2^n\times 2^n$ invertible matrix $A_n$ such that \begin{equation*}
			\begin{pmatrix}
				\tilde{Z}_{d_1} \\
				\tilde{Z}_{d_2} \\
				\vdots \\
				\tilde{Z}_{d_{2^n}}
			\end{pmatrix} = A_n \begin{pmatrix}\tilde{B}_{d_1} \\
				\tilde{B}_{d_2} \\
				\vdots \\
				\tilde{B}_{d_{2^n}}
			\end{pmatrix}.
		\end{equation*} where $d_k = k2^{-n}$ and where for any $m\leq n$ and $d_k\in \mathcal{D}_m$ the random variable $\tilde{Z}_{d_k}$ only depends on those $\tilde{B}_d$ where $d\in \mathcal{D}_m$ . If we set \begin{equation*}
				\tilde{X} := \begin{pmatrix}\tilde{B}_{d_1} \\
				\tilde{B}_{d_2} \\
				\vdots \\
				\tilde{B}_{d_{2^n}}
			\end{pmatrix} \quad \text{and } \quad X:= \begin{pmatrix}{B}_{d_1} \\
				{B}_{d_2} \\
				\vdots \\
				{B}_{d_{2^n}}
			\end{pmatrix}
		\end{equation*} then we can define $A_nX = Z$ . And since $\mathbb{P}_{X} = \tilde{\mathbb{P}}_{\tilde{X}}$ and $Z$ is a Gaussian random vector we conclude that the entries of $Z$ are independent with normal distributions with mean $0$ and variance $1$ . Furthermore $X = A_n^{-1}Z$ and hence the Levy construction attained from $Z$ at step $n$ i.e. $$\sum_{i=0}^{n}F_i(d) = \sum_{i=0}^{\infty}F_i(d)$$ where we let $F_i$ be constructed as $\tilde F_i$ but changing $Z_t$ for $\tilde{Z}_t$ , agrees with the Brownian Motion $(B(t))$ at all dyadic rationals of order less than $2^{-n}$ : $\mathcal{D}_n$ . Since $(B(t))$ are continuous a.s. the limit of the interpolation obtained via the Levy construction is precisely $(B(t))$ . Is there anything subtle I'm missing here? The reason I wan't to have this result is that it is sometimes easier to prove a result about Brownian Motions as the limit of Levys Construction.","['solution-verification', 'brownian-motion', 'probability-theory']"
4003163,A function with period 1 and $\alpha$ (irrational),"I'm looking for a non-constant function $f(x)$ , which there is a irrational number $\alpha$ such that $$
f(x)=f(x+1)=f(x+\alpha)
$$ for all $x\in\mathbb{R}$ . Note that though the function $f(x)$ do not have a smallest period, but that doesn't mean $f(x)$ must be a constant (the Dirichlet function is a good example, but it failed in this situation).","['functional-equations', 'functions']"
