question_id,title,body,tags
2751108,"Nonlinear ODE $y''+y'+\frac {1}{y}=0$, $\int yy''dy$ and $\int yy''dt$","$$y''(t)+y'(t)+\frac {1}{y(t)}=0$$ (by y' and y'' I mean $\frac{dy}{dt}$ and $\frac{d^2y}{dt^2}$ ) First, I only know basic-ish Calculus but I'm willing to do some reading. Second, I'd appreciate some help with my (extra?) doubts (marked with ""‡""). Third, this and this are kinda similar and I understand how they work but I want the function that multiplies $y''$ to be the same (or a multiple but unless that makes it easier, let's stick with this) as the one that multiplies $y'$ . A square in the first derivative $y'$ actually makes sense for what I was trying to do but I'm not sure if it'll simplify things or if these are helpful here: $$(yy')'=yy''+y'y'$$ $$(y'/y)=\frac{y''}y-\frac{y'^2}{y^2}$$ My failed attempt(s)--- I know that not all nonlinear DEs are [nicely?] solvable but I just wanna make sure. My issue with this one is that if we integrate both sides in function of $t$ we get $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dt+\int\frac{dy}{dt}dt+\int\frac {1}{y}dt=0\Rightarrow$$ $$y'+y+\int\frac {1}{y}dt=0$$ and I can't get rid of the last bit ‡[I've heard that you should be cautious with simplifying differentials like dt and dy algebraically but I don't understand why. Can you give me some examples?]. In function of y: $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\int\frac {1}{y}dy=0\Rightarrow$$ $$\int \frac{d}{dt}\left(\frac{dy}{dt}\right) dy+\int\frac{dy}{dt}dy+\ln|y|=0$$ and I can't get rid of the first two bits.
If we multiply the equation by $y$ : $$yy''+yy'+1=0$$ Now, to successfully integrate in function of something we need to be able to solve either $$\int yy''+yy' dt$$ or $$\int yy''+yy' dy$$ . Let's try in function of $t$ first. $$\int yy' dt=\int y\frac{dy}{dt}dt=\frac {y^2}{2}+C$$ , so there's that. (integrating by parts with the integral I want on the left side and yes, it might be weird but I think it's so much simpler) $$\int yy'' dt=yy'-\int y'y' dt$$ or $$\int yy'' dt=y''\int y dt-\int (y'''\int y dt)dt$$ $\int y'y'dt=y'y-\int y''y dt$ Substituting this into the nicer one above: $$\int yy'' dt=yy'-\bigg(y'y-\int y''y dt\bigg)$$ , which cancels out and makes me sad Trying to integrate by parts like this also doesn't work: $\int yy''*1 dt$ . Let's try in function of $y$ .
‡[Now, for the stuff above, Wolfram Alpha didn't spit out anything useful but it gave me some results for the integrations by parts below I don't think are right. Is my boi WA wrong?] $$\int yy' dy=y\int \frac{dy}{dt}dy-\int 1(\int \frac{dy}{dt}dy)dy$$ or $$\int yy' dy=y'\frac{y^2}{2}-\int \frac{d}{dy}\frac{dy}{dt}\frac{y^2}{2}dy=y'\frac{y^2}{2}-\int \frac{y^2}{2}d\frac{dy}{dt}$$ $$\int \frac{y^2}{2}*1d\frac{dy}{dt}=\frac{y^2}{2}y'-\int \frac{d(y^2/2)}{dy'}y'dy'=\frac{y^2}{2}y'-\int yy' dy$$ Substituting above: $$\int yy' dy=y'\frac{y^2}{2}-(\frac{y^2}{2}y'-\int yy' dy)$$ , which leads us to the stunning conclusion that $0=0$ .
Nevertheless, Wolfram Alpha says that $\int yy' dy=\frac{y^2}{2}y'+C$ but if $y=t^3$ , $$\int yy' dy=\int t^3(3t^2)d(t^3)=\int t^3(3t^2)(3t^2)dt=\frac{9t^8}{8}+C\neq\frac{3t^8}{2}$$ . Kinda close, though? It also says that $$\int yy'' dy=\frac{y^2}{2}y''+C$$ . Using that info to try to solve our equation that's multiplied by $y$ , we can only further complicate the problem. I've tried doing $u=\frac{1}{y}$ and $u=x'$ but those didn't seem to lead anywhere. [PS: Also, this doesn't come from a textbook or something, I just thought about a Physics thing (which I'm not even sure is reasonable) and got stuck on the math part. I've become more interested in the mathematics behind it than on whether or not it's useful, so the original situation doesn't matter anymore. But do see the comments if you're curious. PPS: Formatting math expressions is a thing that rhymes with ""itch"". But thanks for reading!]","['calculus', 'integration', 'nonlinear-system', 'ordinary-differential-equations', 'solution-verification']"
2751200,"Find all positive integers $x$ such that $[\frac{x}{5}]-[\frac{x}{7}]=1$, where, for any real number $t$, $[t]$ denotes the greatest integer $\le t$","I have tried the following steps: $[\dfrac{x}{5}]-[\dfrac{x}{7}]=1$ @Berci suggested this: $[\dfrac{x}{5}]\le\dfrac{x}{5}$ $-[\dfrac{x}{7}]<-\dfrac{x}{7}+1$ $\implies [\dfrac{x}{5}]-[\dfrac{x}{7}]<\dfrac{x}{5}-\dfrac{x}{7}+1 $ $\implies x>0$ $\implies x> 0$ $\implies x \in [1,.... \infty]$ Next is a manual observation, we will see that numbers lets say $n\equiv1(mod 5)\implies n\equiv 5k+1 $ where $k\in \{1,2,3,4\}$ why $k\in \{1,2,3,4\}$? why not $k\in \mathbb{N}$? Lets see: Hence if we try numbers like $6,11,16,21$ we will see that 
$[\dfrac{x}{5}]-[\dfrac{x}{7}]=1$ But the moment one tries with $x=26,36$ we will see $[\dfrac{x}{5}]-[\dfrac{x}{7}]=2$ Again with $x=41,46,51,56$ we see $[\dfrac{x}{5}]-[\dfrac{x}{7}]=2$ And so on as will proceed and this $[\dfrac{x}{5}]-[\dfrac{x}{7}]>1$ and increases more and more. Again if we try $x=12,13$,we get $[\dfrac{x}{5}]-[\dfrac{x}{7}]=1$ Also $x=5$ is a solution. So is it the answer that $x \in {5,6,11,12,13,16,21}$?","['inequality', 'functions']"
2751245,Non-example of a lemma on continuity [duplicate],"This question already has an answer here : Gluing Lemma for Closed Sets: Infinite Cover Counter-Example (1 answer) Closed 6 years ago . Lemma: Let $f:(X,\mathcal{T})\longrightarrow(X',\mathcal{T'})$ be a mapping  between topological spaces where $$X=\cup_{j=1}^m F_j$$
  $ \text{ with }F_j\text{ closed for }\mathcal{T},\, f_{|F_j} \text{ continuous },\forall j=1,\ldots,m.$ Then $f$ is continuous. I'd like to find a non-example to that when $X$ is a countably infinite union of closed sets. My course already provides one that I find counterintuitive as I've mostly worked so far on metric spaces (the identity between $\mathbb{N}=\cup\{n\}$ with the cofinite topology and $\mathbb{N}$ with the discrete topology). Is it possible to find one where both spaces are $\mathbb{R}$ with the standard topology ? I've considered disjoint closed $X_n$ that increasingly cover $\mathbb{R}$ but in vain.","['continuity', 'general-topology', 'examples-counterexamples']"
2751247,How can I show that $(1+2\cos 2\theta)^3=7+2(6\cos 2\theta+3\cos4\theta+\cos6\theta)$ using the tensor product and the Clebsch Gordan Theorem? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can I show that $$(1+2\cos2\theta)^3=7+2(6\cos2\theta+3\cos4\theta+\cos6\theta)$$
using the tensor product and the Clebsch Gordan Theorem?","['tensor-products', 'trigonometry', 'lie-algebras', 'lie-groups']"
2751248,Understanding this theorem and how to go about proving it surjectivity?,"I don't follow understand what this theorem is saying: ""Every function $f: A \to{\cal P}(A)$ is not surjective."" Is this saying so for example let A = {1,2,3} then the power set of A is {},{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3} Is this saying that there is no surjective function that takes every element in each of its power sets to atleast 1 element in A? Is my understanding correct?","['elementary-set-theory', 'functions', 'discrete-mathematics']"
2751296,"Prove $(X_n, F_n) $ Martingale $\iff \int_{F} X_{n+1} = \int_{F} X_{n} \forall F \in F_n$","I have some additional questions to this exercise: Let $(\Omega, F, F_n, P)$ filtered probability space. Let $(X_n)_{n\in\mathbb{N}} \in \mathcal{L}^1(P)$, which is adopted to $(F_n)_{n\in\mathbb{N}}$. Show $(X_n, F_n) $ Martingale $\iff \int_{F} X_{n+1} = \int_{F} X_{n} \forall F \in F_n$. I think I have proved the following: ""$\Rightarrow$""
$$\int_F X_{n+1} dP = E[1_{F}X_{n+1}] = E[E[1_{F}X_{n+1}|F_n]] = E[1_FE[X_{n+1}|F_n]] = E[1_F X_{n}] = \int_F X_{n} dP $$ Is this correct? Im not sure about this:
$$ E[E[1_{F}X_{n+1}|F_n]] = E[1_FE[X_{n+1}|F_n]] $$ And how should I show the other implication? Any help is appreciated?","['probability-theory', 'filtrations', 'martingales', 'measure-theory', 'conditional-expectation']"
2751306,Top-down proof of a lemma used in Schröder-Bernstein theorem,"Please have a check whether it is fine or contains any error! Thank you so much! Lemma: Suppose that $Z \subseteq Y \subseteq X$ and that $f:X \to Z$ is bijective, then there exists a bijection $g : X \to Y$. Proof: Without loss of generality, we assume $Y \subsetneq X$. Let $\mathcal{F}=\{ V\subseteq X | (X-Y) \subseteq V \text{ and } f(V) \subseteq V \}$ and $A=\bigcap_{V\in \mathcal{F}}V$ 1. $A \in \mathcal{F}$ $\forall V \in\mathcal{F}, X-Y\subseteq V\implies X-Y\subseteq \bigcap_{V\in\mathcal{F}}V\implies X-Y\subseteq A$. $x\in A\implies \forall V \in\mathcal{F},x\in V \implies \forall V \in\mathcal{F},f(x)\in f(V)\subseteq V \implies \forall V\in\mathcal{F},f(x)\in V$ $\implies f(x)\in\bigcap_{V\in\mathcal{F}}V\implies f(x)\in A\implies f(A)\subseteq A$. To sum up, $X-Y\subseteq A$ and $f(A)\subseteq A \implies A\in\mathcal{F}$. Furthermore, $\forall V\in \mathcal{F}: A \subseteq V\implies A$ is the minimal element of $\mathcal{F}$. 2. We prove $A \neq \varnothing$ $X-Y \subseteq X$ and $f(X)=Z \subseteq X \implies X \in \mathcal{F} \implies \mathcal{F} \neq \varnothing.$ As a result, $f(A) \subseteq A$ and $(X-Y) \subseteq A$. $\mathcal{F} \neq \varnothing$ and $\forall V \in \mathcal{F}, (X-Y) \subseteq V \implies (X-Y) \subseteq \bigcap_{V\in \mathcal{F}}V \implies (X-Y) \subseteq A \implies$ $A \neq \varnothing.$ 3. We prove $f(A)=A \cap Y$ (Here I figure out two ways to prove this and I present both of them) a. Approach 1 Let $B=f(A) \cup (X-Y)$ $f(A) \subseteq A$ and $(X-Y) \subseteq A \implies B \subseteq A$. $f(A) \subseteq A \implies f(f(A)) \subseteq f(A)$; $(X-Y) \subseteq A \implies f(X-Y) \subseteq f(A)$. First, $B=f(A) \cup (X-Y) \implies (X-Y) \subseteq B$. Second, $f(B)=f(f(A) \cup (X-Y))=f(f(A)) \cup f(X-Y) \subseteq f(A) \subseteq B.$ Finally, $(X-Y) \subseteq B$ and $f(B) \subseteq B \implies B \in \mathcal{F},$ but $B \subseteq A$. From the minimality of $A$, $B=A$. $A \cap Y=B \cap Y= (f(A) \cup (X-Y)) \cap Y=(f(A) \cap Y) \cup ((X-Y) \cap Y)=f(A) \cup \varnothing = f(A)$. b. Approach 2 $f(A) \subseteq A$ and $f(A) \subseteq Z \subseteq Y \implies f(A) \subseteq (A \cap Y)$. Assume $(A\cap Y) \not\subseteq f(A) \implies \exists p\in (A\cap Y)$ such that $p \notin f(A) \implies p \in Y$. Let $B=A-\{p\}$. First, $p \in Y \wedge (X-Y) \subseteq A \implies X-Y \subseteq A-\{p\} \implies (X-Y) \subseteq B$. Second, $f(B)=f(A-\{p\})=f(A)-f(\{p\})$ [Since $f$ is injective] $\subseteq f(A) \subseteq f(A)-\{p\}$ [Since $p \notin f(A)$] $\subseteq A-\{p\}$ [Since $f(A) \subseteq A$]$=B$. To sum up, we have $(X-Y) \subseteq B$ and $f(B)\subseteq B$, then $B \in \mathcal{F}$, but $B \subsetneq A$. This contradicts to the minimality of $A \implies (A \cap Y) \subseteq f(A)$. $f(A) \subseteq (A \cap Y)$ and $A \cap Y \subseteq f(A) \implies A \cap Y=f(A)$. 4. $f(A) \cup (X-A)=Y$ and $f(A) \cap (X-A)=\varnothing$ $X-Y \subseteq A \implies X-A \subseteq X-(X-Y)=Y$. $f(A) \cup (X-A)=(A \cap Y)\cup (X-A)=(A\cup (X-A)\cap (Y\cup (X-A))=X\cap (Y\cup (X-A))=Y\cup (X-A) \subseteq Y \cup Y=Y \implies f(A) \cup (X-A)=Y$. $f(A) \cap (X-A)=(A \cap Y) \cap (X-A)=Y \cap (A \cap (X-A))=Y \cap \varnothing=\varnothing.$ 5. We generate $g$ as follows $$
g(x) =
\begin{cases}
\ f(x)      & \text {if $x \in A$} \\
x   & \text {if $x \in X \setminus A$} \\
\end{cases}
$$","['proof-writing', 'elementary-set-theory', 'proof-verification']"
2751319,Why is the opposite Borel subgroup used?,"When reading some papers on flag varieties, I sometimes find some remarks mentioning opposite Borel subgroup. It seems that people do so when they consider algebraic group. To my understanding, it is just a convention but it's very confusing to me because I don't understand the motivation. For instance, the following is from the page 3 of Line bundles on Bott-Samelson varieties by Lauritzen and Thomsen. Here, $G$ is a connected semisimple, simply connected linear algebraic group over an algebraically closed field, and $B$ is a Borel subgroup. It is well known, that $\mathcal{L}_{G/B}(\lambda)$ is globally generated exactly when $\lambda$ is dominant with respect to the Borel subgroup opposite to $B$ (or equivalently $\langle \lambda, \alpha^\vee \rangle \geq 0$ for all simple roots $\alpha \in S$). I thought that $\lambda$ is dominant if and only if $\langle \lambda, \alpha^\vee \rangle \geq 0$. But why respect to the Borel subgroup opposite to $B$? Is this because of the convention regarding the opposite Borel subgroups, or is it just a typo? I am more familiar with Lie groups and practically know nothing about algebraic groups. Is there a basic reference which explains this kind of convention in detail? Why is such convention necessary or has advantage?","['algebraic-groups', 'root-systems', 'representation-theory', 'algebraic-geometry']"
2751388,"Prove that there is a unique probability on $[0,1]$ s.t. $\mathbb P([0,x])=x$.","I want to define a probability space $([0,1],\mathcal F,\mathbb P)$ where $\mathbb P([0,x])=x$. I would like to prove that there is no probability measure on $2^{[0,1]}$, and that there is a unique probability on $[0,1]$ s.t. $\mathbb P([0,x])=x$, and for such a space $\mathcal F=\sigma (\{[0,x]\mid x\in [0,1])$. My teacher said that it's a hard exercise, but do you have a reference for such a proof ?","['probability-theory', 'measure-theory']"
2751431,Infinite Sequence of Coin Tossings,"I have a question about an exercise problem from Durrett's probability book and its solution sketch in the solution manual. On page 53, 2.1.18, it says ""If we want an infinite sequence of coin tossings, we do not have to use Kolmogorov's theorem. Let $\Omega$ be the unit interval $(0,1)$ equipped with the Borel set $\mathcal{F}$ and Lebesgue measure P. Let $Y_n(\omega) = 1 $ if $[2^n \omega]$ is odd and $0$ if $[ 2^n\omega]$ is even. Show that $Y_1, Y_2, ...$ are independent with $P(Y_k=0)=P(Y_k=1)=1/2.$"" In the solution manual, the solution is given in two lines as follows: Let $i_1, i_2, ...\in \{0,1\}$ and $x = \sum_{m=1}^n i_m 2^{-m}$, then $P(Y_1 = i_1,..., Y_n = i_n) =P(\omega \in [x,x+2^{-n})) = 2^{-n}.  \quad*$ I understand that this solves the problem, but I cannot see how to justify the first equation in $*$. Could someone give a hint? Note: To my knowledge, $[2^n \omega]$ is defined by $[x] = \max\{a \in \mathbb{Z}, a \leq x\}$ for $x\in \mathbb{R}.$","['independence', 'probability-theory', 'random-variables']"
2751439,Prove the following determinant,"Prove the following:
$$\left|
\begin{matrix}
(b+c)&a&a \\
b&(c+a)&b \\
c&c&(a+b) \\
\end{matrix}\right|=4abc$$ My Attempt: $$\left|
\begin{matrix}
(b+c)&a&a \\
b&(c+a)&b \\
c&c&(a+b) \\
\end{matrix}\right|$$ Using $R_1\to R_1+R_2+R_3$
$$\left |
\begin{matrix}
2(b+c)&2(a+c)&2(a+b) \\
b&(c+a)&b \\
c&c&(a+b) \\
\end{matrix}\right|$$
Taking common $2$ from $R_1$
 $$2\left|
\begin{matrix}
(b+c)&(a+c)&(a+b) \\
b&(c+a)&b \\
c&c&(a+b) \\
\end{matrix}\right|$$ How do I proceed further?","['matrices', 'algebra-precalculus', 'linear-algebra', 'determinant']"
2751450,"If there exists an irreducible open covering, then the space is irreducible.","If a topological space $X$ has an open covering $\{ U_i \}$ such that every $U_i$ is irreducible and that for each $i,j$, $U_i \cap U_j \neq \emptyset$, then is $X$ irreducible? I found Space admitting an irreducible connected open covering is irreducible , but I think this is wrong.: If this answer were correct, every scheme would be affine.","['zariski-topology', 'general-topology', 'algebraic-geometry']"
2751451,Number of ways to invite 7 friends for dinner in triplets.,"In how many ways can you invite 7 friends to dinner in triplets throughout a period of 7 days so that no 3 friends are invited 2 times? It means that for each day you invite 3 of your friends. What I thought about was to allocate one friend to all the days and another the same way so that I have to allocate friends to as little as possible places, in order to have a maximum of 2 friends invited more than 1 time. But after I did I saw that it is impossible to do so as I have invited 2 friends the maximum number of times I can and I have to allocate the 5 remaining friends in the 7 remaining places that have to be filled. Therefore at least one of the remaining has to repeated more than once. This leads me to the conclusion that this is impossible and that the question means that no 3 friends have to be repeated exactly 2 times instead of 2 times or more. If this is the case could anyone provide me with a solution. The answer is 151 200. Any help would be greatly appreciated as this my homework for tomorrow.",['combinatorics']
2751491,Congruence properties of global units,"Let $K$ be a number field, $O$ its ring of integers with global unit group $O^\times$. Further, let $M\subset O^\times$ be a subgroup of finite index. My question is: Is there a non-zero ideal $\frak{n}$ of $O$, such that $(1+\frak{n})\cap$ $O^\times \subset M$? By Dirichlet, $O^\times$ and hence $M$ are finitely gen. $\mathbb{Z}$-modules, even free (up to roots of unity). I feel that this should be almost enough to compare the topologies of $\widehat{O^\times}$ and $\widehat{O}^\times$, where $\widehat{}$ is profinite completion. But playing around with this hasn't amounted to anything. I feel, like I'm missing something obvious here... P.S: Minkowski's ""geometry of numbers"" was also not helpful in attacking this.","['number-theory', 'algebraic-number-theory']"
2751525,$Sp_{2g}(\Bbb Z) \rightarrow Sp_{2g}(\Bbb Z/m) $ is surjective?,"How to show that the mod $m$ map $Sp_{2g}(\Bbb Z) \rightarrow Sp_{2g}(\Bbb Z/m) $ is surjective without using some deep structure theorem (like strong aprroximation)? Where $Sp_{2g}$ means the symplectic group. Motivation: for $SL_n$ one can prove the surjectivity by noticing that $SL_n(A)$ is generated by elementary matrices for every local ring $A$. This answer gives a proof using elementary generators in the case $m$ is a prime. However, he didn't give any reference for such calculations.","['number-theory', 'abstract-algebra', 'group-theory', 'algebraic-geometry']"
2751531,The continuity of linear functionals with respect to uniform convergence of entire functions on balls,"Let $X$ be a Banach space and $H_b (X)$ be the algebra of complex-valued entire functions on $X$ which are bounded on bounded sets, with the topology of uniform convergence on bounded sets. Let $\varphi \in H_b^* (X)$ (which is the dual space of $H_b (X)$). Each $\varphi \in H_b^* (X)$ is continuous with respect to the norm of uniform convergence on some ball in $X$. Define the radius function $R$ on $H_b^* (X)$ by declaring $R(\varphi) $ to be the infimum of all $r >0$ such that $\varphi$ is continuous with respect to the norm of uniform convergence on the ball $r B$. I don't understand the above bold statement. I am a bit confused because I think $\varphi \in H_b^* (X)$ is continuous on every ball in $X$ (so, the radius of $\varphi$ is always $0$). I want to know what I missed.","['banach-spaces', 'uniform-convergence', 'banach-algebras', 'functional-analysis', 'analyticity']"
2751540,"If two functions are defined by the same formula, but have different codomains, are they equal?","It is said for two functions $f,g$ to be equal they must have same domain and codomain and for each $x\in X$, $f(x)=g(x)$. But shouldn't functions such as $f:\Bbb R \to \Bbb C$ where $f(x)=x^2$ and $g:\Bbb R \to \Bbb R$ where $g(x)=x^2$ still be considered equal functions for example? Even if codomain is different.",['functions']
2751541,Regular surface and self-intersections,"So the definition of a regular surface is the following: A topological subspace $S  \subset \mathbb{R^3}$ is a regular surface if $\forall p \in S$ there exist open sets $U \subset \mathbb{R^2}$ , $V \subset S, p \in V$ and a function $\phi:U \rightarrow \mathbb{R^3}$ with $\phi \in C^{\infty}(U, \mathbb{R^3})$ such that: $\phi:U \rightarrow V$ is a homeomorphism. $d\phi_q:\mathbb{R^2} \rightarrow \mathbb{R^3}$ is injective $\forall q \in U$ ( $d\phi_q$ is the differential of $\phi$ in q). Regular surfaces cannot have self-intersections. I guess this has to follow from the definition of regular surface given above, probably the condition of $\phi$ being an homeomorphism, but I can' t see exactly why.","['differential-geometry', 'surfaces']"
2751549,"Let $G$ be a group, $G'=[G,G]$ and $G''=[G',G']$ the first and second derived subgroups and assume $G''$ is cyclic. Prove that $G''\subset Z(G')$.","I'm trying to prove the following, but I'm stuck and I don't see how to continue. Any help is much appreciated! Let $G$ be a group, $G'=[G,G]$ and $G''=[G',G']$ the first and second derived subgroups and assume $G''$ is cyclic. Prove that $G''\subset Z(G')$. My work this far: Since $G''$ is cyclic, it is abelian. Since it is the commutator subgroup of $G'$, it is also known that $G''$ is normal. Thus (using a theorem from my syllabus), there exists a homomorphism $g:G'/G''\to\mathrm{Aut}(G'')$ such that $g(aG'')=\phi_{a|G''}$ for $a\in G'$, where $\phi_{a|G'}:G''\to G'':x\mapsto axa^{-1}$, thus the conjugation map by $a$. Now it suffices to proof $\phi_{a|G'}=Id_{G'}$ for all $a\in G'$. But how to prove this? I don't see it. The work I did this far follows a hint that was given for the question.","['abstract-algebra', 'group-theory']"
2751551,Prove or contradict: Between each two solutions of $\arctan x = \sin x$ exists a solution for $1-\cos x = x^2 \cos x$,"Prove or contradict: Between each two solutions of $\arctan x = \sin x$ exists a solution for $1-\cos x = x^2 \cos x$ I have this question in a sample exam and I don't even know what would be a good way to approach this. I though about finding the ranges where the two difference functions have different slopes or something, but I'm not quite sure..","['trigonometry', 'calculus']"
2751560,What is the Topology of the Iwasawa Manifold?,"Let
$$
H=
\left\{
\begin{pmatrix}
1 & z_1 & z_3 \\
0 &   1 & z_2 \\
0 &   0 &   1
\end{pmatrix}
:
z_1,z_2,z_3 \in \mathbb{C}
\right\}
$$
be the complex Heisenberg group and denote by $G$ the subgroup of $H$ where the entries $z_1,z_2,z_3 \in \mathbb{Z}[i]$ are in the Gaussian integers.
Then $G$ acts on $H$ from the left and the quotient $W=H/G$ is a compact manifold with 6 real dimensions (3 complex dimensions, but $W$ is not Kähler).
$W$ is called Iwasawa manifold . Note that
$$
\begin{pmatrix}
1 &   a & c \\
0 &   1 & b \\
0 &   0 &   1
\end{pmatrix}
\begin{pmatrix}
1 & z_1 & z_3 \\
0 &   1 & z_2 \\
0 &   0 &   1
\end{pmatrix}
=
\begin{pmatrix}
1 & z_1+a & z_3+a z_2+c \\
0 &   1 & z_2+b \\
0 &   0 &   1
\end{pmatrix},
$$
i.e. every element in $W$ has exactly one representative with elements $z_1,z_2,z_3 \in [0,1)\times [0,1)i$.
It is mentioned e.g. in Daniel Huybrecht: Complex Geometry that $W$ is a non-trivial $2$-torus bundle over a $4$-torus (the non-triviality is checked by computing homology groups). Question 1:
  Consider the map
  $$\Phi: ([0,1]/_{\{0,1\}} \times [0,1]/_{\{0,1\}} i)^3 \rightarrow W, (z_1,z_2,z_3) \mapsto 
\begin{pmatrix}
1 & z_1 & z_3 \\
0 &   1 & z_2 \\
0 &   0 &   1
\end{pmatrix}.$$
  This is a well-defined (i.e. $\Phi(0,0,0)=\Phi(1,0,0)=\Phi(0,1,0)=\dots$), bijective (statement about representatives for elements in $W$ above), continuous map.
  Thus it is a homeomorphism.
  However it can't be, because it has different homology groups.
  What is the problem with my ""homeomorphism""? And Question 2:
  What is a good way to define functions on $W$?
  I was hoping to use above bijection, but there is something wrong with it, so it seems like a bad idea.","['complex-geometry', 'differential-geometry']"
2751589,Suppose that $Z \subseteq Y \subseteq X$ and that $f:X \to Z$ is bijective. Then $A$ is the minimal element of $\mathcal{F}$,"Please have a look at the below proof and check whether it contain any error. I'm very thankful for your help! Theorem: Let $Z \subseteq Y \subseteq X, \space f:X \to Z$ is bijective, $\mathcal{F}=\{ V\subseteq X \mid (X-Y) \subseteq V \text{ and } f(V) \subseteq V \}$, and $A=A_0\cup A_1\cup A_2\cup\cdots$ where $A_0=X-Y$ and $A_{n+1}=f(A_n)$. Then $\forall V\in\mathcal{F},A\subseteq V$. Proof: 1. $A \in \mathcal{F}$ $f(A)=f(A_0\cup A_1\cup A_2\cup\cdots)=f(A_0)\cup f(A_1)\cup f(A_2)\cup\cdots=A_1\cup A_2\cup A_3\cup\cdots \implies$ $f(A) \subseteq A$. Furthermore, $A=A_0\cup A_1\cup A_2\cup\cdots \implies A=A_0 \cup f(A)=(X-Y)\cup f(A)$ $\implies (X-Y)\subseteq A$. Thus, $f(A) \subseteq A$ and $(X-Y)\subseteq A \implies A \in \mathcal{F}$. 2. $\forall A'\subsetneq A, A'\notin \mathcal{F}$ $A_0 =X-Y \implies A_0 \cap Y=\varnothing. A_{n+1}=f(A_n) \implies A_{n+1} \subseteq Y \space \forall n \in \mathbb{N}\implies A_0\cap A_n =\varnothing\space\forall n>0 \text{ (and we also know that } f \text{ is injective) }\implies f^m(A_0) \cap f^m(A_n)=\varnothing\space\forall m\in\mathbb{N}\text{ and } n>0\implies A_m \cap A_{m+n} =\varnothing \space \forall m \in \mathbb{N} \text{ and } n>0 \implies A_m \cap A_n =\varnothing \space \forall m \neq n.$ Thus $A$ is the union of disjoint sets. As a result, if $x\in A$, then $x$ only belongs to a unique $A_n$. For $A'\subsetneq A$, let $i=\min \{n \in \mathbb{N} \mid \exists x\in A_n \text{ such that } x\notin A'\}$. It's clear that $\exists y\in A_i \text{ such that } y\notin A'$ and that $A_n \subseteq A' \space\forall n<i$. We have two cases in total. a. $i=0$ $\implies y \in A_0 \implies y \in X-Y \implies X-Y \not \subseteq A' [\text{ since } y\notin A'] \implies A' \notin \mathcal{F}.$ b. $i>0$ $\implies i=t+1 \implies A_t \subseteq A' \implies f(A_t) \subseteq f(A') \implies A_{t+1} \subseteq f(A') \implies y \in f(A')$. We have that $y \in f(A')$ and $y \notin A' \implies f(A') \not \subseteq A' \implies A' \notin \mathcal{F}.$ 3. $\forall V'\in\mathcal{F},A\cap V' \in \mathcal{F}$ $A\in\mathcal{F}\implies X-Y\subseteq A$ and $f(A)\subseteq A$. $V'\in\mathcal{F}\implies X-Y\subseteq V'$ and $f(V')\subseteq V'$. Thus $X-Y\subseteq A\cap V' \text{ and } f(A\cap V')=f(A)\cap f(V')$ [Since $f$ is injective] $\subseteq A\cap V'.$ This implies $A\cap V' \in \mathcal{F}$. 4.$\forall V\in\mathcal{F},A\subseteq V$ Assume the contrary, i.e. $\exists V'\in\mathcal{F},A\not\subseteq V' \implies \exists a\in A, a\notin V' \implies A\cap V'\subsetneq A$ and $A\cap V' \in\mathcal{F}$. But this contradicts the fact that $\forall A'\subsetneq A, A'\notin \mathcal{F}$. Thus $\forall V\in\mathcal{F},A\subseteq V$, or equivalently $A$ is the minimal element of $\mathcal{F}$. $$\tag*{$\blacksquare$}$$","['proof-writing', 'elementary-set-theory', 'proof-verification']"
2751593,Finding solutions for $\sin(z)=\sin(2)$,"having some difficulty wrapping my head around how to methodically do this style of question: Find all solutions $z\in ℂ$ for $\sin(z)=\sin(2)$ I attempted to solve this by using the identity, $\sin(x)\cosh(y)+i\cos(x)\sinh(y)=\sin(2)$, so that, $\sin(x)\cosh(y)=\sin(2)$ and $\cos(x)\sinh(y)=0$, but this seemed to lead to a dead end, so I'm unsure as to what the next step should be... Any help is much appreciated","['complex-analysis', 'complex-numbers']"
2751617,Odd values for Dirichlet beta function,"I would like to find a proof for the generating formula for odd values of Dirichlet beta function, namely: $$\beta(2k+1)=\frac{(-1)^kE_{2k}\pi^{2k+1}}{4^{k+1}(2k)!}$$ My try was to start with the cosine infinite product $$\cos{(\pi x)}=\prod_{n\ge 1} \left(1-\frac{4x^2}{(2n-1)^2}\right)\;,$$ Logarithmate and differentiate it, in order to get: $$-\pi\tan{(\pi x)}= \sum_{n\ge 1} \frac{\frac{-8x}{(2n-1)^2}}{1-\frac{4x^2}{(2n-1)^2}}$$ Then multiplying by $x$ and expanding the denominator into a geometric series, yields: $$\pi x\tan{(\pi x)}= 2\sum_{n\ge 1} \sum_{k\ge 0} \left(\frac{2x}{(2n-1)}\right)^{2k}\frac{4x^2}{(2n-1)}=2\sum_{n\ge 0} \sum_{k\ge 0} \left(\frac{2x}{(2n+1)}\right)^{2k+1}$$ Now, already in this post it has been shown how to expand $\tan(x)$ into power series, but I have $x\tan(x)$ and I don't know how to equate the coefficients this way. Can I get some help with this? EDIT : Related: Euler numbers grow $2\left(\frac{2}{ \pi }\right)^{2 n+1}$-times slower than the factorial? However I am still interested in how to show this using my idea.","['generating-functions', 'infinite-product', 'dirichlet-series', 'sequences-and-series']"
2751633,Sum to $n$ terms of the given series [duplicate],"This question already has answers here : Expression generating $\left( \frac{3}{10}, \, \frac{3}{10} + \frac{33}{100}, \, \frac{3}{10} + \frac{33}{100} + \frac{333}{1000}, \dots \right)$ (4 answers) Closed 6 years ago . Find the sum to $n$ terms of the given series: $$0.3+0.33+0.333+0.3333+\cdots$$ My Attempt:
Let 
$$S=0.3+0.33+0.333+0.3333+\cdots \text{ to $n$ terms}$$ 
$$=\frac {3}{10}+\frac {33}{100}+\frac {333}{1000} + \frac {3333}{10000}+\cdots$$
$$=\frac {3}{10} \left[1+\frac {11}{10}+\frac {111}{100}+\frac {1111}{1000}+\cdots \text{ to $n$ terms}\right]$$ How do I continue from here?","['algebra-precalculus', 'summation', 'sequences-and-series']"
2751638,Find the derivative of $f$ at $2$,"Let $g$ be a real valued function defined on the interval $(-1,1)$ such that $$e^{-x}(g(x)-2e^x)=\int_0^x \sqrt{y^4+1}\,dy$$ for all $x\in (-1,1)$ and $f$ be another function such that $$f(g(x))=g(f(x))=x.$$ Then find the value of $$f'(2).$$ So, first I tried alot a then we tried it in an integration calculator but $\int_0^x \sqrt{y^4+1} \, dy$ is not possible. Next, since $f(g(x))=g(f(x))=x$ is given, therefore $f^{-1}=g$ and the vice versa. But I could not even find the dumbest idea how to do this question. Please help me out guys. Thanks!! Cheers!!","['derivatives', 'integration', 'inverse-function', 'functions']"
2751645,On the solution of one matrix differential equation,"We have the next equation 
$$\dot K=AK+KA^T+HH^T,$$
where $K(t),A(t),H \in R^{n \times n}, K-$is a symmetric matrix. How one can prove that the solution of this equation is as follows:
$$K(t)=\int_0^te^{As}HH^T e^{A^Ts}ds$$
on $ [0,t]$, provideed that $K(0)=\boldsymbol{0}$?","['matrices', 'matrix-equations', 'ordinary-differential-equations', 'matrix-calculus']"
2751650,Proving two functions are equal with their derivatives,"I just post a question to check if my proof to the following is true :
Let $f$ and $g$ be two differentiable functions on $E=[a,b],$ then if $f'(t)=g'(t)$ and $f(a)=g(a)$ then $f=g, \forall t \in E$ (1)we admit that $f$ and $g$ are differentiable, thus they are continuous thus integrable. (2) we also admit that $f'(t)=g'(t) \implies \int_a^b f'(t) dt =\int^b_a g'(t) dt $ (2) We'll admit the fundamental theorem of calculus then
let $x \in E$ $$\int_a^x f'(t) dt =f(x)-f(a)$$ $$\int_a^x g'(t) dt=g(x)-g(a)$$ so $$f(x)-f(a)=g(x)-f(a)$$ i.e. $$f(x)=g(x), \forall x \in E$$ QED
Is there any way to prove this using only the derivate (and no integral) ?
Thanks
TMD","['derivatives', 'alternative-proof', 'functions', 'solution-verification']"
2751661,Limit of $L^p$ Norms and Essential Supremum,"Assume $m(E) < \infty$. For $f \in L^\infty(E)$, show that $\lim_{n \to \infty}||f||_n= ||f||_\infty$ This problem comes from Royden and Fitzpatrick's Real analysis. I think the statement is false, but hopefully someone will correct me if I am mistaken. Let $E=[0,1/2]$, and consider $f(x)=x \in L^\infty(E)$. Since $f$ is continuous, $||f||_\infty = ||f|||_{max} = 1/2$. However, $$||f||_n \le [m(E)]^{1/n} ||f||_\infty = (1/2)^{1/n}||f||_\infty,$$ which implies $$\lim_{n \to \infty} ||f||_n \le ||f||_\infty \lim_{n \to \infty} (1/2)^n = 0,$$ and therefore $\lim_{n \to \infty} ||f||_n = 0 \neq 1/2$. What am I missing? EDIT: Made a stupid error: the ""$n$"" should be replaced with ""$1/n$"". I've been working on this problem for quite some time now; I could use a gentle prod in the right direction.","['real-analysis', 'measure-theory']"
2751722,The $2$-category of monoids,"People sometimes say that monoids are ""categories with one object"". In fact people sometimes suggest that this is the natural definition of a monoid (and likewise ""groupoid with one object"" as the definition of a group). But categories naturally form a $2$-category $\mathbf{Cat}$. So if we took the above definition seriously then we would view monoids as forming a $2$-category $\mathbf{Mon}$. The objects would be monoids and the morphisms would be monoid homomorphisms, but there would also be $2$-morphisms between homomorphisms. A $2$-morphism between $f,g:M\to N$ is an $n\in N$ such that $nf(m)=g(m)n$ for all $m\in M$. If one takes the principle of equivalence seriously then this poses a problem because we lose the ability to talk about the ""underlying set"" of a monoid. There's no $2$-functor $U:\mathbf{Mon}\to\mathbf{Set}$ (treating $\mathbf{Set}$ as a $2$-category with no nontrivial $2$-morphisms) that sends each monoid to its underlying set and each homomorphism to its underlying function. In the $1$-category of monoids this would be given by applying the functor $\mathrm{Hom}(\Bbb N,-)$. But in the $2$-category $\mathbf{Mon}$ two homomorphisms $f,g:\Bbb N\to M$ are isomorphic whenever $f(1)=mg(1)m^{-1}$ for some $m\in M$, so this construction only gives us the set of conjugacy classes of $M$ rather than its set of elements. Clearly this poses a problem if we want to work with monoids and groups. In particular proofs involving finite groups often require the ability to count the number of elements in some subset of a group. It becomes impossible to state Lagrange's Theorem. We also lose the ability to talk about the free group on a set, since we can't construct the adjoint to the nonexistent functor $U$. In light of this, I want to know if it's actually possible to take ""category with one object"" as our definition of monoid, and still be able to prove things in a practical way. I can see two ways to do this: 1) Recover the $1$-category of monoids from $\mathbf{Mon}$ in some natural way or 2) Show that we can reconstruct group theory in a way that never uses concepts like ""order of a group"" or ""free group on a set"" Does anybody know a way to do either of these?","['monoid', 'abstract-algebra', 'higher-category-theory', 'soft-question', 'category-theory']"
2751743,"Evaluate the integral$ \int_{-\infty}^{\infty}\frac{b\tan^{-1}\big(\frac{\sqrt{x^2+a^2}}{b}\big)}{(x^2+b^2)(\sqrt{x^2+a^2})}\,dx$.","I am attempting to evaluate $$\int_{-\infty}^{\infty}\dfrac{b\tan^{-1}\Big(\dfrac{\sqrt{x^2+a^2}}{b}\Big)}{(x^2+b^2)(\sqrt{x^2+a^2})}\,dx. $$ I have tried using the residue formula to calculate the residues at $\pm ib,\pm ia,$ but it got messy very quickly. Then I tried to use a trigonometric substitution $x=a\tan(\theta)$; $dx=a\sec^{2}(\theta)\,d\theta$ which led me to the integral $$\int_{-\infty}^{\infty}\dfrac{b\tan^{-1}\Big(\dfrac{a\sec(\theta)}{b}\Big)\sec(\theta)}{(a^2\tan^{2}(\theta)+b^2)}\,d\theta.$$ The bounds for this integral seem incorrect, but I am more worried about the actual expression before I deal with the bounds, which may have to be changed into a double integral where $0\leq\theta\leq2\pi$ and the second bound would range from $-\infty$ to $\infty$. I am wondering if there is some kind of substitution I have missed, but I have hit the wall. The OP of this problem said there were cases that would come into play, but when I asked him whether or not those cases arose from $b<0$ and $b>0$ he told me they did not. The cases most likely arise from whether $a$ and $b$ are positive or negative, because the case where $b=0$ is trivial, and in the case where $a=0$ I used wolframalpha and the integral evaluates to $\dfrac{\pi\ln(2)\lvert b \rvert}{b^2}$ for $\Im(b)=0 \land \Re(b)\neq0.$ Contour integration may be necessary. I am stuck on this problem and I would greatly appreciate the help. Thank you for your time.",['integration']
2751772,Converting Differential Operator to Integral Equation,"The Green's function of following differential operator $$(\mathcal{L}y)(x)=\frac{d}{dx}\left(x\,\frac{dy}{dx}\right)-\frac{n^2}{x}\,y(x), \:0<x<1$$ with boundary conditions
$$y(0)=y(1)=0$$
 can be the form of 
$$G(x,t)=  \begin{cases} 
     \dfrac{1}{2n} \left(\dfrac{x}{t}\right)^n(1-t^{2n})& x\leq t\\
      \dfrac{1}{2n} \left(\dfrac{t}{x}\right)^{\!n}(1-x^{2n}) &  x>t
   \end{cases}.
$$ Now using this, I would like to convert the following ODE to an integral equation for nonzero number $n$ (which I got stuck in)
$$x^2y''+xy'+(\lambda x^2-n^2)y=0$$
$$y(0)=y(1)=0. $$
We also can write the above equation in $\mathcal{L}y(x)+\lambda xy=0$ relavant to $y(0)=y(1)=0$, so it may be correct to say
$$\int_0^1G(x,s)(\mathcal{L}y(s)+\lambda sy(s))\,ds=0$$ So now it equals $\displaystyle y(x)+\lambda \int_0^xG(x,s)sy\,ds+\lambda \int_x^1G(x,s)sy\,ds=0$. I'm confused here, since I assumed $\displaystyle y(x)=\int_0^1G(x,s)Ly(s)\,ds $ from this http://www.nada.kth.se/~annak/greens1d_odes.pdf However from Zeyman, I think we have $\displaystyle Ly(x)=\int_0^1G(x,s)Ly(s)\,ds$, since $G(x,y)$ is a kernel. Anyways, I'm not sure how to solve this question; I appreciate any help.","['functional-analysis', 'ordinary-differential-equations', 'integral-equations']"
2751793,"Verify that certain statistic is complete (namely, $X_{(1)}$ for $f_\theta(x)=e^{-(x-3\theta)}I_{(3\theta,\infty)}(x)$)","Given the density function of a certain population $$f_\theta(x)=e^{-(x-3\theta)}I_{(3\theta,\infty)}(x)$$ I am asked to find a complete statistic for a random simple sample $(X_1,X_2,\ldots,X_n)$ of it. From the nature of the given density function, I would like to prove that $X_{(1)}$ is complete. To do so, the density of $X_{(1)}$ is given by $$g(x)=ne^{-n(x-3\theta)}$$ And if $\operatorname E[f(S)]=0$, where $S=X_{(1)}$, then $$\operatorname E[f(S)]=\int_{3\theta}^\infty f(x)ne^{-n(x-3\theta)}\, dx=0$$ How to proceed from this point, and prove that $f=0$ almost everywhere? Thanks in advance.","['statistical-inference', 'probability-theory', 'statistics', 'definite-integrals', 'probability']"
2751842,Sum of binomial coefficient,"My question is the following. Calculate the value of: $$\sum_{u=0}^{22} u(u-1) \binom {22}u$$ I am not too sure I'm allowed to perform each of these steps, but what I thought was the following: $$\sum_{u=0}^n \binom nu = 2^n.$$ $$2^{22}\sum_{u=0}^{22} u(u-1)$$ $$2^{22}\sum_{u=0}^{22} (u^2-u)$$ $$2^{22}\sum_{u=0}^{22} u^2- 2^{22}\sum_{k=0}^{22}u$$ My thought is to use the following equations afterwards. However, I don't get the correct answer. $$\sum_{k=0}^{n} k^2 = n(n+1)(2n+1)/6$$ $$\sum_{k=0}^{n} k = n(n+1)/2$$","['combinatorics', 'summation', 'binomial-coefficients']"
2751883,Completion commutes with finite direct sums,"I have been reading a couple of different places, that completion of module (or the $I$-adic completion to be more exact) commutes with finite direct sums. I was told, that it follows from the fact, that completion is an additive functor, but as I haven't worked with functors before, that doesn't make much sense to me, and I was wondering if it could be proven in another way (maybe from using the definition of completion only). I've been trying to work it around my self, but doesn't seem to get anywhere.","['abstract-algebra', 'commutative-algebra']"
2751893,Continuous random variable is a mathematical trick,"Measuring real (physical) events, a random variable can assume only discrete values. Is the use of continuous random variables a mathematical trick cause: it's easier to work with real number ? If yes, are you able to explain this in more detail ?","['statistics', 'probability']"
2751922,I have a problem to solve this inequality $\sin(2x)-2\cos x\geq 0$,"I have a problem to solve this inequality:
$$\sin(2x)-2\cos x\geq 0$$ I have tried at similar mode: $$2\sin x\cos x-2\cos x\geq 0 \Longleftrightarrow 2\cos x(\sin x-1)\geq 0$$ My result is $$-\frac \pi2+2k\pi\leq x \leq \frac \pi2+2k\pi, \quad k\in\mathbb{Z}$$
because $\cos x\geq 0,-\frac \pi2+2k\pi\leq x \leq \frac \pi2+2k\pi, \quad \text{and} \quad \sin x-1\geq 0\,\,$  if $\,\,\,\,\,x=\pi/2+2k\pi$. The solution of my book is: $$\frac \pi2+2k\pi\leq x \leq \frac 32\pi+2k\pi, \quad k\in\mathbb{Z}$$
Why?","['algebra-precalculus', 'inequality', 'calculus']"
2751936,What exactly does $g^2(x)$ mean,"I know there's (kind of?) some controversy as to what $g^2(x)$ means — it could be $g(g(x))$, or $(g(x))^2$ or even $g''(x)$. I'm taking a calculus course and assumed that $g^2(x)$ meant $g''(x)$, but it turns out that the solution saw this as $(g(x))^2$. Here's the problem if it will clarify things: If $P(x) = g^2(x)$, then $P'(x)$ equals... (of course, there's a table given with values of g and g' accompanying the problem and answer choices, but I'm not including them here because of copyright.) So in general, is there any way I can determine which one exactly the problem means when $g^2(x)$ appears?","['derivatives', 'calculus', 'functions']"
2751962,Interesting property of sum of powers of integers from 1 to 114.,"When I have this list of specific X values: $X: 1, 2, 3, 4, \ldots, 112, 113, 114.$ $$\sum_{n=1}^{114}n = 6555$$ $$6555/19 = 345$$ The sum of these $X$ values divided by $19$ is an integer. Then I square each $X$ value: $X^2: 1, 4, 9, 16, \ldots, 12544, 12769, 12996.$ The sum of these $X^2$ values divided by $19$ is also an integer. Then I cube each $X$ value, and the pattern continues. Is there any reason for this property mathematically? Or does it just exist? Many thanks.",['sequences-and-series']
2751968,"Functions with bounded derivatives, closed under composition","This is a follow-on of sorts to this question , but is self-contained. Let $F_1 := \{f \in C^\infty(\mathbb{R}) \mid \|\frac{df}{dx}\|_\infty \le c_1\}$ ($c_i > 0$ throughout). Given $f, g \in F_1$, we have by the chain rule that $g \circ f \in F_1$ if $\|\frac{dg}{df}\|_\infty \|\frac{df}{dx}\|_\infty \le c_1$ and this is automatically true if $c_1 \le 1$. Now let $F_2 := \{f \in F_1 \mid \|\frac{d^2f}{dx^2}\|_\infty \le c_2\}$. Given $f, g \in F_2$, we have by the generalized chain rule ( Faà di Bruno's formula ) that $g \circ f \in F_2$ if it is in $F_1$ and $\|\frac{d^2g}{df^2}\|_\infty \|\frac{df}{dx}\|_\infty^2 + \|\frac{dg}{df}\|_\infty \|\frac{d^2f}{dx^2}\|_\infty \le c_2$ and this is automatically true if $c_1, c_2$ satisfy the corresponding inequality $c_2 c_1^2 + c_1 c_2 \le c_2$, i.e. if $c_1^2 + c_1 \le 1$ (in which case $c_1 \le 1$ also). In general, let $F_n := \{f \in F_{n-1} \mid \|f^{(n)}\|_\infty \le c_{n} \}$. Given $f, g \in F_n$, we have by the generalized chain rule that $g \circ f \in F_n$ if it is in $F_{n-1}$ and $\sum_{\pi\in\Pi(n)} \|g^{(|\pi|)}\|_\infty \prod_{B\in\pi} \|f^{(|B|)}\|_\infty \le c_n$ (notation from the Wikipedia article ) and this is automatically true if the $c_i$ satisfy the corresponding inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \{1, ..., n\}$. My question then is: can $F_\infty := \bigcap_{n \in \mathbb{N}} F_n$ be closed under composition without being trivial? In other words: Does there exist a sequence of values $c_i > 0$ which satisfy the inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \mathbb{N}$?","['real-analysis', 'inequality', 'limits', 'functional-analysis', 'sequences-and-series']"
2752014,Bernoulli trials - proving that a sequence will occur infinitely many times,"Let's consider the infinite numbers of Bernoulli trials. The probability of the success ($S$ - success, $F$ - failure) is equal to $p$. I am to prove that the possibility of getting the infinite amount of the following occurrence - $SSSFFF$ is equal to $1$. My attempt: 1) I tried to solve the problem by redefining the success. Now for me it would be not $S$ but the sequence $SSSFFF$. The probability of such sequence would be equal to $p^3(1-p)^3 > 0$. $E[X = \text{success at k-th trial}] = \frac{1}{p^3(1-p^3)}$ so I would need $E[X]$ trials to get my ""new"" success. Of course after infinite numbers of them I would get infinite numbers of successes. 2) $P(\text{getting at least one redefined success in n trials}) = 1 - P(\text{only F in n trials}) = 1 - (1 - p^3(1-p^3))^n \rightarrow 1$. Thus because of the ""no memory rule"" we will get infinite numbers of ""new"" successes. Are my solutions correct? If no, why?","['probability-theory', 'probability']"
2752046,Hitting time of Brownian motion against a square root curve,"$B$ denotes Brownian motion and the hitting time I am interested in is
$$\tau = \inf\{t \geq 0: B_t = b\sqrt{a+t}\}$$
where $a,b >0$. I first want to show that $\tau < \infty$ almost surely. I am going to use Khintchine's law of the iterated logarithm (LIL). Define $$\Omega := \left\{\omega: \limsup_{t\to\infty } \frac{B_t(\omega)}{\sqrt{2t\log\log t}} = 1 \right\}$$
The LIL states that $P(\Omega) = 1$. I will show that $\tau(\omega) < \infty$ for every $\omega \in \Omega$. Fix $\omega \in \Omega$ and some small $\varepsilon > 0$. Then there exists $T(\omega,\varepsilon) > 0$ such that
$$\left\lvert\sup_{t > T } \frac{B_t(\omega)}{\sqrt{2t\log\log t}} -1\right\rvert < \varepsilon$$
This in turn implies that
$$\sup_{t > T } \frac{B_t(\omega)}{\sqrt{2t\log\log t}} > (1-\varepsilon)$$
Then there must exist some $u > T$ such that
$$B_u(\omega)> (1-\varepsilon)\sqrt{2u\log\log u}$$ Since $\sqrt{2t\log\log t}$ is greater than $b\sqrt{a+t}$ for $t$ large enough, I can choose $\varepsilon$ small enough to make sure $\sqrt{2u\log\log u} > b\sqrt{a+u}$ so that $\tau(\omega) < u$. I got a bit sloppy towards the end but I hope the idea is correct. I just wanted to have this verified. My main concern is in the part where I dropped the supremum. My second question is about $E[\tau]$. It is easy to show that $E[\tau] = \infty$ whenever $b \geq 1$ by Wald's identity. But I am trying to show that $E[\tau] < \infty$ whenever $b < 1$. The hint in the book is that $E[\tau\wedge n] \leq \frac{ab^2}{1-b^2}$ for $n \geq 1$. Obviously, if I can show this inequality then I would be done immediately by monotone convergence but I haven't been able to prove that this inequality holds. I made some efforts below but I find it very hard to believe that $E[\tau] < \infty$ whenever $b < 1$. For a hitting time against a constant boundary, i.e. $\inf\{t\geq 0: B_t = x\}$, the expected value is not finite. How can it possibly be finite for a growing boundary? Anyway, here is what I tried so far. I know that $B_{\tau \wedge n}$ is lower and upper bounded by the running minimum and running maximum Brownian motion at time $n$. So $E[B_{\tau \wedge n}^2] < \infty$. Therefore, I can use Wald's identity to write $$E[\tau \wedge n] = E[B_{\tau \wedge n}^2]$$
and focus on $E[B_{\tau \wedge n}^2]$. However, I don't see how I can make use of the fact that $b < 1$. By time scaling property of Brownian motion I have
$$\tau \stackrel{\text{d}}{=} b^{2n} \inf\left\{t \geq 0: B_t = b\sqrt{\frac{a}{b^{2n}}+t}\right\}$$
If I denote $f(a,b) := E[\tau]$, I can write 
$$f(a,b) = b^{2n}f\left(\frac{a}{b^{2n}},b\right)$$ 
Letting $n \to \infty$ does not help because I don't know what happens to $f\left(\frac{a}{b^{2n}},b\right)$ in the limit (it is increasing for sure but beyond that I don't have much else to say on it). Applying the time scaling property in a slightly different way I get $$\tau \stackrel{\text{d}}{=} ab^2 \inf\{t \geq 0: B_t = \sqrt{1+b^2t}\}$$
The stopping time on the RHS looks easier to study but I don't see how this brings me closer to the answer.","['stochastic-processes', 'probability-theory', 'proof-verification', 'stopping-times', 'brownian-motion']"
2752056,End behavior of $(\sin k)^k$ where $k \in \mathbb{N}$,"It's clear that the set $\{\sin k \mid k \in \mathbb{N}\}$ is dense in $[0, 1)$. What I'm having trouble determining is how the sequence $\{(\sin k)^k \mid k \in \mathbb{N}\}$ behaves for large $k$. My main question is: does there exist $N \in \mathbb{N}$ and $\epsilon \in \mathbb{R}, 0 \leq \epsilon < 1$ such that for all $k \geq N, k \in \mathbb{N}$, $(\sin k)^k \leq \epsilon$? Or equivalently, does there exist $N \in \mathbb{N}$ such that $\sup(\{(\sin k)^k \mid k \in \mathbb{N}, k \geq N\}) \neq 1$?","['analytic-number-theory', 'sequences-and-series']"
2752098,"Does $\int_0^\infty f(x+\theta)g(x) \, dx=0 \ , \ \forall \theta \in \mathbb{R}$ imply $f=0$ almost everywhere if $g$ is smooth and strictly positive? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be an integrable function, and $g:(0,\infty)\rightarrow(0,\infty)$ a smooth, strictly positive function. If $$\int_0^\infty f(x+\theta)g(x)\,dx=0\qquad\forall\theta\in\mathbb{R}$$ Does that imply that $f=0$ almost everywhere? This problem is part of a statistics problem in which I would like to prove that if $$\int_0^\infty f(x+3\theta)ne^{-nx} \, dx=0$$ For all $\theta\in\mathbb{R}$, then one should have $f=0$ almost everywhere; you can find the main question here , in which I propose $X_{(1)}$ as a complete statistic for a given density function - it has been solved, nevertheless I still am interested in a more global and rich treatment of the problem. Some collegues are suggesting to use fourier analysis and the convolution theorem, but I am not familiar with these techniques and I would like to avoid those methods to solve the problem. Thanks in advance.","['real-analysis', 'fourier-analysis', 'integration', 'definite-integrals', 'measure-theory']"
2752145,What are applications of etale cohomology and Abelian varieties? (And what is arithmetic geometry?),"First, I apologize for my poor English. I like number theory such as ""when can prime $p$ be written as $x^2 + y^2$?"" and ""find the integer solutions of this equation.""
Because I've heard that these problems can be solved by arithmetic geometry, I want to study it. So I've read Hartshorne's Algebraic Geometry and Neukirch's Algebraic Number Theory and so on. (I've heard that these are fundamental for arithmetic geometry.)
And next I’m about to study Abelian varieties and etale cohomology for the same reason.
But after reading list of contents of these books, I feel like that these are very abstract, and very distant from my first purpose. So the question: what are applications of abelian variety and etale cohomology? I know the theory of etale cohomology solved the Weil's conjecture. But only this? I know this is the great achievement, but I want to know more applications. (and I do not know applications of the theory of abelian varieties at all.) And the second question is: what is arithmetic geometry? What books should I read to study it's mainstream? I want, for example, books which tells me not only the definitions and fundamental properties of etale cohomology, but also interesting and elementary (like ""find the integer solutions of this equation"") applications. Or books which tells me many theories and techniques which are used in frontier researches. Please help.","['abelian-varieties', 'algebraic-geometry', 'reference-request', 'number-theory', 'arithmetic-geometry']"
2752169,How many $3 \times 3$ non-symmetric and non-singular matrices $A$ are there such that $A^{T}=A^2-I$?,How many $3 \times 3$ non-symmetric and non-singular matrices $A$ are there such that $A^{T}=A^2-I$ ? Note: $I$ denotes the identity matrix of size $3 \times3$ and $A^{T}$ represents the transpose of the matrix $A$ . I took transpose on both sides in given equation to get $A=(A^T)^2-I$ and then I put value of $A^T$ in this equation using $A^{T}=A^2-I$ to get $A^3-2A-I=0$ which ultimately gave $(A^T-A)(A+I)=0$ . How to deal the problem from here? And is there any better approach to tackle this problem? The given answer is $0$ .,"['matrices', 'linear-algebra']"
2752170,Rolling two dices n-times. Probabillity of getting doubles,"I started learning the basics of probabillity theory by myself and did some practicing and I was doing the following exercise which i found on the internet. Two dices are rolled n-times, determine the probabillity of getting each double (1,1),(2,2),.....,(6,6)
atleast one time. I used the binominal distribution, since all four conditions i) finte number of trials ii)trials are independent iii)each trial has two outcomes and at each trial probability doesn't change. Consider getting a double is a sucess P(getting a double on a roll)=$\frac { 1 }{ 6 } $ P( not getting a double)=$\frac { 5 }{ 6 } $ since we are looking for 6-succes we get from the binominal distribution formula that (n,6)
$P(x=6)=(n,6){ p }^{ 6 }{ (1-p) }^{ n-6 }$ $P(x=6)=(n,6){ \frac { 1 }{ 6 }  }^{ 6 }{ (1-\frac { 5 }{ 6 } ) }^{ n-6 }$ ((n,6) denotes the binominal coeff.)
I'm a little concerned if this is right,since i did not consider the atleast one time thing furhter my modell does not consider that if a pair comes twice it still counts as success even tho its none.
Could you help me to finish this correctly",['probability']
2752185,show that $x^2+x+1 \in F[x]$ is irreducible,"So this is in regards to exam revision. The complete question is to construct a field of order 32, then show that $x^2+x+1\in F[x]$ is irreducible. So I began by constructing the field $$\begin{align}
F[x]=&Z_{2}[x]/(x^5+x^2+1)\\
=&\{a+b\alpha+c\alpha^2+d\alpha^3+\alpha^4;a,b,c,d,e\in Z_2\}
\end{align}$$ and $\alpha=x\in F$. I do not know how I would proceed to show that the $x^2+x+1$ is irreducible over $F$. Any help would be appreciated.","['irreducible-polynomials', 'finite-fields', 'discrete-mathematics']"
2752186,Function is open iff every element of a subbasis maps to open set?,"I recently read that, given topological spaces $S,T$ and a map $f:S\rightarrow T$, for $f$ to be open it is sufficient to show that for a certain subbasis $C$ of $T$ and all (open) sets $A\in C$ holds $f(A)$ is open. (The converse holds by definition.) This fact was stated as obvious, however I'm struggling with following it through. Before I prove the statement for any open set, I start with the open sets $A\in B$, where $B$ is the basis of $T$ obtained from $C$ (i.e. $B$ contains all finite intersections of Elements of $C$). What I know: $\exists A_1,\ldots,A_n\in C: A = \bigcap A_i$ since $A_i$ is open and in $C$, $f(A_i)$ is open by assumption for all $i=1,\ldots,n$ therefore $\bigcap f(A_i)$ is open What I want to show: $f(A)$ is open If I had $f(A)=\bigcap f(A_i)$ I would be finished, but this doesn't hold generally. I don't know how to continue from here. Getting from $C$ to any open set is easy, because for $A$ open exist index set $I$ and sets $A_i\in C$ with $i\in I$ such that $A=\bigcup A_i$, furthermore $f(A)=\bigcup f(A_i)$ always holds.","['general-topology', 'elementary-set-theory', 'open-map']"
2752206,How to pullback a form,"I am trying to make sense of pullbacks of forms as described in Guillemin and Pollack. If $f:X\rightarrow Y$ is a smooth map and $\omega$ is a $p$-form on $Y$, we define a $p$-form $f^*\omega$ on $X$ as follows. If $f(x)=y$, then $f$ induces a derivative map $df_x:T_x(X)\rightarrow T_y(Y)$. Since $\omega(y)$ is an alternating $p$-tensor on $T_y(Y)$, we can pull it back to $T_x(X)$ using the transpose $(df_x)^*$. We then define the pullback of $\omega$ by $f$ by $f^*\omega(x)=(df_x)^*\omega[f(x)]$. (Note: In general, we define the transpose as $A^*T(v_1,...,v_p)=T(Av_1,...,Av_p)$) I am having trouble actually applying this definition to real examples. Guillemin and Pollack give none. For example, if $f:(\pi/2,\pi/2)\rightarrow \mathbb{R}$ is given by $f(t)=\sin t$ and $\omega=dx/\sqrt{1-x^2}$, then I begin as follows. We have that $df_t=f'(t)=\cos t$. Then $f^*\omega(t)=(\cos t)^*\omega$. Is this correct? How do I proceed?","['multivariable-calculus', 'differential-geometry', 'differential-topology']"
2752323,General form for $\sin(kx)$ in terms of $\sin(x)$ and $\cos(x)$,"Identities for $\sin(2x)$ and $\sin(3x)$, as well as their cosine counterparts are very common, and can be used to synthesize identities for $\sin(4x)$ and above. Given some integer $k$, is there an equation to find $\sin(kx)$ in terms of $\sin(x)$ and $\cos(x)$? For example, if I wanted to find the identity for $\sin(1000x)$, what would it be? Identies used:
$$\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)\\\cos(a+b)=\cos(a)\cos(b)+\sin(a)\sin(b)$$",['trigonometry']
2752353,Can the Sum of Two Tensor Products Be Written as a Single Tensor Product?,"In general, if I have $|\Psi\rangle = (|\Psi_{1_1}\rangle \otimes |\Psi_{1_2}\rangle + |\Psi_{2_1}\rangle \otimes |\Psi_{2_2}\rangle)$,  can I find $|\Psi_{3_1}\rangle$ and $|\Psi_{3_2}\rangle$, such that $|\Psi\rangle = |\Psi_{3_1}\rangle \otimes |\Psi_{3_2}\rangle$? Here $\otimes$ means tensor product and $|\Psi\rangle$ and means a vector. No assumption is made about any relationship between the $|\Psi_{i_j}\rangle$, except that they are all the same dimension and their components are complex numbers. The motivation is the quantum double slit experiment, where the wave state, $|\Psi\rangle$, between the slits and the detector, is the sum of two interfering waves, and $|\Psi\rangle$ is still in a ""pure state"", which means that $|\Psi\rangle$ can also be written as a tensor product","['tensor-products', 'linear-algebra']"
2752359,Prove the limit property of a 'random' walk/gambling problem,"Suppose a person plays a sequence of independent games. At the $n$th game, he plays with equally with $n$ other people, gaining $n$ units of money with probability $\frac{1}{n+1}$, losing $1$ unit of money otherwise. Now we can formulate it as follows. Suppose $(X_n)_{n\geq1}$ are independent random variables with $\mathbb{P}(X_n=n)=\frac{1}{n+1}$ and $\mathbb{P}(X_n=-1)=\frac{n}{n+1}$. Obviously it's a fair game. Note total gaining as $G_n=\sum_{i=1}^n X_i$, prove that $\liminf_n G_n=-\infty$ and $\limsup_n G_n=+\infty$. I can prove the second one with the help of Borel-Cantelli Lemma, by showing that $\{X_n=n\}$ happens infinitely many times, but I feel stuck when dealing with the first one. Can anyone help? No matter by what methods. I hope that it can be generalized in some sense. I don't know if we can use knowledges of martingale since we are trying to prove some 'bad' property (divergence) of $G_n$.","['gambling', 'random-walk', 'probability-theory', 'borel-cantelli-lemmas', 'martingales']"
2752378,"If $x^n \equiv a \space (31)$, how many values of n and a are there s.t there are only 10 solutions?","If $x^n \equiv a \space (31)$, how many values of n and a are there s.t there are only 10 solutions? I have no idea of how to do this - would really appreciate all help.","['number-theory', 'elementary-number-theory']"
2752407,Find E[X] and V[X],"Let the random variable X satisfy $$E\left[(X −1)^2\right] = 10$$ and $$E\left[(X −2)^2\right] = 6.$$ No clue how to start this one... To find E[X] do I integrate both?
Would appreciate some hints on how to start this one. Kinda lost on how to start this one since I never encountered this problem before.","['probability-theory', 'probability', 'variance', 'probability-distributions']"
2752447,Closed form for $\sum_{n=0}^\infty {1\over (xn)!}$,"There seem to be rather nice values for low integer values for $x$, as follows: $$\begin{array}{|c|c|}
\hline
x & \displaystyle\sum_{n=0}^\infty {1\over (xn)!} \\[1ex]
\hline
1 & e \\[2ex]
2 & \cosh(1) \\[2ex]
3 & \displaystyle{e^{3/2} + 2\cos\frac{\sqrt 3}{2} \over 3 \sqrt e} \\[2ex]
4 & \displaystyle\frac12(\cos1+\cosh1)\\
\hline
\end{array}$$ (These were verified with WolframAlpha.) Is there any closed form for this, or some other function that it can be expressed in? This is just out of interest by the way; I have no dire need for an answer, although one would certainly be appreciated!","['taylor-expansion', 'sequences-and-series', 'closed-form']"
2752526,Geodesics of the orthogonal group,"Problem. I'm working on an exercise from a text in Riemannian geometry, that tells me to do this: Let $\mathbf{O}(n)=\{A\in\mathbb{R}^{n\times n}:A^t A=I\}$ be equipped with its usual (left-invariant!) Riemannian metric $g_p(X_p,Y_p)=\operatorname{tr}(X_p^t Y_p)$ for $p\in \mathbf{O}(n)$ and $X_p,Y_p\in T_p\mathbf{O}(n)$ . Show that for a $C^2$ -curve $\gamma\colon I\to \mathbf{O}(n)$ , it holds that $\gamma$ is geodesic if and only if $\gamma^t \ddot{\gamma}=\ddot{\gamma}^t \gamma$ . I think I solved it while trying to explain where I was stuck (it's always surprising how helpful it can be to just try to formulate a question), so I will post my suggested solution as an answer to this post, in case somebody else needs it or somebody else has something to add.","['geodesic', 'riemannian-geometry', 'differential-geometry', 'lie-groups']"
2752579,"finding the coefficient k of a joint pdf $f(x,y)$ under the multivariate normal distribution","given the joint pdf $f(x,y) = $ $$k e^{-\frac12(x^2-2xy+5y^2)},$$ find $k.$ I understand that there is some trick to computing $k$ using properties of the multivariate normal distribution, but have no clue where to start.","['statistics', 'probability', 'probability-distributions']"
2752583,Calculation of convolution of two indicator functions,"So the problem I'm working on states... Let $f_k=\mathbf{1}_{[-1,1]}\ast\mathbf{1}_{[-k,k]}$, where $\ast$ is a convolution. Compute $f_k(x)$ explicitly.
$\\$ I know that the value of the function is dependent on x. However, I've been working on this for the better part of the day and can't come up with anything reasonable. I know that the function is nonzero only when $x-1\leq y\leq x+1$ and $-k\leq y\leq k$. I have that (only because I saw something similiar here ): \begin{eqnarray*}
f_k(x) &=& \int_\mathbb{R}\mathbf{1}_{[-1,1]}(x-y)\cdot\mathbf{1}_{[-k,k]}(y)\ dy\\
&=& \int_{-k}^k\mathbf{1}_{[-1,1]}(x-y)\ dy = \int_{-k}^k\mathbf{1}_{[-1,1]}(y-x)\ dy \\
&=& \int_{-k}^k\mathbf{1}_{[x-1,x+1]}(y)\ dy
\end{eqnarray*} Will this work? I still don't know how to break $\mathbb{R}$ into whatever intervals of $x$. Could someone give me any hints or suggestions on how to proceed with this problem?","['real-analysis', 'convolution', 'measure-theory']"
2752597,How can I show that the polynomial $t^n-x$ is irreducible in $F(x)[t]$?,"In the setup of the problem, it is given that $F$ is a field, $F(x)$ is the field of rational functions with coefficients in the field $F$, and $n$ is a positive integer.  I also understand that $F(x)[t]$ is the ring of polynomials in the variable $t$ with coefficients given by the elements in $F(x)$. I began to approach this problem by setting up a proof by contradiction: Suppose that $t^n-x$ is reducible in $F(x)[t]$.  Then, $t^n-x=a(t)*b(t)$ for some irreducible, non-units $a(t)$, $b(t)$ in $F(x)[t]$.  We know that the degrees of $a(t)$, $b(t)$ are strictly less than $n$. But after this, I am stumped. Any help is much appreciated! Edit: I also want to work this out a bit on my own, so instead of a full answer or proof maybe I'd prefers a few hints or maybe observations that I'm missing?","['irreducible-polynomials', 'abstract-algebra', 'maximal-and-prime-ideals', 'field-theory']"
2752602,A case where the weak law of large number holds while SLLN dose not,"$(X_i)_{i \geq 2}$ is a sequence of independent random variables. Their probability measure is defined as $P(X_i=i)=\frac{1}{i\log i}$ and $P(X_i=0)=1-\frac{1}{i \log i}$. How can we show that $\frac{1}{n}\sum\limits_{i=2}^n(X_i -E(X_i))$ converges in probability to zero but not almost surely. To prove the weak law of large number, I can just use Chebyshev inequality. But I don't know how to show that this doesn't converge to zero almost surely.","['probability', 'probability-distributions']"
2752650,"Solving $u_t=ku_{xx}$ for $\ t\ge 0,-\infty<x<\infty$","Solve $$u_t=ku_{xx}\\u(x,0)=g(x)$$ for $t\ge 0, -\infty<x<\infty$, where $$g(x) = \begin{cases}
1, \quad \lvert x \rvert < 1 \\
0, \quad \lvert x \rvert > 1
\end{cases}$$ Solution. We have that the solution is given by 
\begin{align}
u(x,t) &= \frac{1}{\sqrt{4\pi kt}}\int_{-1}^1e^{-(x-y)^2/4kt} \cdot 1 dy \\
&= \frac{-1}{\sqrt\pi}\int_{\frac{x-1}{\sqrt{4kt}}}^{\frac{x+1}{\sqrt{4kt}}} e^{-p^2}dp
\end{align}
if we consider  $p=\frac{(x-y)}{\sqrt{4kt}}$ in the solution formula. Is my solution correct so far? If yes, how do I integrate the integral with the $p$ variable? I was thinking to separate the integral like $$\int_{-a}^a=\int_{-a}^0+\int_0^a$$ but I'm not sure that $$\frac{x-1}{\sqrt{4kt}}<0$$","['partial-differential-equations', 'calculus', 'integration', 'ordinary-differential-equations', 'initial-value-problems']"
2752657,Simulating a fair die with a 5-card hand,"This question is inspired by an earlier post: Card game and dice rolling: Finding random variables with similar probability mass function I want to simulate a fair 6-sided die using 5-card hands from a regular deck of 52 cards.  Cards are drawn without replacement, so there are ${52 \choose 5}$ possible hands, and since this number is divisible by 6, it is possible to partition the ${52 \choose 5}$ possible hands into 6 equal-sized disjoint subsets $A_1 \cup A_2 \cup A_3 \cup A_4 \cup A_5 \cup A_6$.  Then, when I draw a 5-card hand, if it belongs to $A_j$ then I pretend I rolled the number $j$, and this process simulates a fair 6-sided die.  This is entirely possible, but my question is: What is the easiest / most elegant / simplest way to do this? If the order of drawing the 5 cards can be used (which is the question asked by the earlier post), then an easy / elegant / simple answer is: $A_1 = $ the first non-King you draw is Ace or 2, $A_2 =$ the first non-King you draw is 3 or 4, etc., up to $A_6=$ the first non-King you draw is Jack or Queen.  These sets are clearly of equal size and disjoint, and since you draw 5 cards you must draw a non-King, so their union also cover all $52 \times 51 \times 50 \times 49 \times 48$ possible draws.  However, this method requires that you distinguish based on the order of drawing those 5 cards. My question in this post requires that you do not distinguish based on the order of draw. @RossMillikan and I had a brief discussion of several attempted solutions in the comments of the earlier post, but now that I have more time to think, I think none of the solutions actually work.  Here are the (IMHO wrong) attempts: 1st Attempt: Ignore Kings, assign Ace = 1; 2 through 10 = face value; J = 11; Q = 12.  Add the 5 card values mod 6. My thoughts: This would have obviously worked if you were drawing with replacement (and can somehow ""magically"" avoid the 5-King draw), since every card is uniformly distributed mod 6.  But because you are drawing without replacement, if your first card is e.g. a 4, then the next card is no longer randomly distributed mod 6 because you have one fewer 4.  Maybe this still simulates a fair die, but I don't see an obvious proof. 2nd Attempt: Again ignore kings (until the end). If you have distinct ranks, add them mod 6. If you have at least one pair, take the rank with maximum quantity (most repeated occurrences) mod 6. This handles all draws except $xxyyz$ and $Kxxyy$. For the first, take $z$ mod 6. For the second, if the king is red take the higher rank of $x$ or $y$, if the king is black take the lower. My thoughts: This would obviously work if each subcase is partitioned evenly into the 6 subsets.  E.g. all hands of type $xxyyz$ certainly seems to be partitioned evenly.  However consider hands of type $KKKxy$ (3 Kings, 2 other different ranks).  Because we're constrained by $x \neq y$ (in this subcase), $Prob(x+y \ is\ odd) = 6/11$.  So clearly this subcase is not partitioned evenly into the 6 subsets.  Again, the overall scheme might still simulate a fair die, but I don't see an obvious proof. To summarize: the question is not whether this partition is possible - it is, simply because ${52 \choose 5}$ is divisible by 6.  The question is what is an easy / elegant / simple way to do this, i.e. to describe the partition. (Bonus if your solution generalizes to any $N$-sided die, where $N$ divides ${52 \choose 5}$.)","['combinatorics', 'probability', 'card-games']"
2752670,Derive Dirichlet test from Abel test,"This is for improper integrals, but I believe the one for series would be similar. Dirichlet test requires $f(x)$ to be monotone, $\lim_{x \to \infty}f(x) = 0$ and $\int_{a}^{b}g(x)$ to be bounded by constant $B$. Then $\int_{a}^{+\infty}f(x)g(x)dx$ converges. Meanwhile, Abel test requires $f(x)$ to be monotone and bounded while $\int_{a}^{\infty}g(x)dx$ converges. Then $\int_{a}^{+\infty}f(x)g(x)dx$ converges. I see how the two are very similar, except with the tighter/looser conditions on $f$ and $g$. How do we derive the Abel test from the Dirichlet test?","['improper-integrals', 'real-analysis', 'integration', 'sequences-and-series']"
2752715,Integrating factor of $xy'+ 4y= x^2-x+1$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How do I solve this linear differential equation? $$xy'+ 4y= x^2-x+1.$$ I am supposed to use the integrating factor to solve it but I do not really understand how to implement it. I do know however, a linear first order O.D.E has the form: $$\frac{\mathrm dy}{\mathrm dx} + P(x) y = Q(x).$$",['ordinary-differential-equations']
2752731,Higher derivatives $\wp^{(2n)}$ of the Weierstrass $\wp$-function,"I'm interested in considering the Weierstrass $\wp$-function denoted $\wp(\tau, z)$ as well as its higher derivatives $\wp^{(2n)}$, for all $n \geq 0$.  The derivatives are with respect to the $z$ variable, and I refer to this highly related question ( Calculating derivatives of the Weierstrass $\wp$-function in terms of $\wp$ and $\wp '$ ) for definitions and such. In an answer to the above referenced question, a user points out that using the relation $$(\wp^{(1)})^{2} = 4\wp^{3} - \wp g_{2} - g_{3},$$ one can easily produce the following result $$\wp^{(2)} = 6 \wp^{2} - \frac{1}{2}g_{2},$$ and one could then show that all higher derivatives are a polynomial in $\wp, \wp^{(1)}, g_{2}$, and $g_{3}$.  It's easy to convince oneself that for even derivatives $\wp^{(2n)}$ there is no dependence on $\wp^{(1)}$, so we should be able to write it as a polynomial in $\wp, g_{2}$, and $g_{3}$. My question is: do we have a general formula for this polynomial for all $n \geq 0$?  I tried coming up with it myself and it seems to quickly become unbearable.  If there isn't a closed form, is there anything theoretically notable (aside from the obvious) about the polynomial?  Is there anything we can really say about it?  I feel like this must exist somewhere out there, and I'm just unable to find it.","['number-theory', 'complex-analysis', 'elliptic-curves', 'elliptic-functions']"
2752754,Is the base case $m=0$ in this proof by induction correct?,"Let $\Bbb J(n)$ the set of bounded intervals in $\Bbb R^n$ , that is, if $I\in\Bbb J(n)$ there are bounded intervals $I_k\in\Bbb R$ such that $I=\prod_{k=1}^n I_k$ , where the product is the cartesian product (we consider the empty set an interval in $\Bbb R$ and consequently also in $\Bbb J(n)$ ). Then let any finite collection $I,J_1,J_2,\ldots, J_m\in\Bbb J(n)$ , and I want to prove using strong induction that $$I\setminus\left(\bigcup_{k=1}^m J_k\right)\text{ can be written as a union of finite pairwise disjoint intervals}\tag1$$ If I choose the base case $m=1$ then the proof becomes messy, but if I choose the base case $m=0$ then I find that $(1)$ is just $I$ , so the base case holds trivially, and the rest of the proof is very simple. Proof: assume that a base case $m=0$ or $m=1$ holds, then we choose $(1)$ as hypothesis for all $1\le k\le m$ for some $m$ . Then the induction step gives $$I\setminus\left(\bigcup_{k=1}^{m+1}J_k\right)=\left(I\setminus\left(\bigcup_{k=1}^m J_k\right)\right)\setminus J_{m+1}\\=\left(\bigsqcup_{j=1}^r H_j\right)\setminus J_{m+1}=\left(\bigsqcup_{j=1}^r H_j\right)\cap J_{m+1}^\complement\\=\bigsqcup_{j=1}^r(H_j\cap J_{m+1}^\complement)=\bigsqcup_{j=1}^r(H_j\setminus J_{m+1})$$ for $H_j\in\Bbb J(n)$ , and using again the hypothesis we can write $H_j\setminus J_{m+1}$ as a union of disjoint intervals, so we are done. There is a huge difference in difficulty in this proof choosing $m=0$ or $m=1$ as the base case, so I'm not sure if the proof for $m=0$ is completely right. Can someone confirm that we can choose $m=0$ in this proof without harm? Also, I would like to know if someone knows how to handle easily the base case $m=1$ . I tried but I lost myself in a mountain of intersections, unions and abstract thinking. Of course, if someone knows how to prove $(1)$ in a different way I would like to know it. Equivalently $(1)$ can be stated as: for any finite collection $J_1,J_2,\ldots,J_m\in\Bbb J(n)$ then $\bigcup_{k=1}^m J_k$ can be written as a pairwise disjoint union of intervals. However, with this assertion, the induction step is as hard as the base case of the above when $m=1$ . EDIT: Amazingly it seems that the base case $m=0$ is valid. Observe what happens with this reformulation of $(1)$ : Let $I\in\Bbb J(n)$ and $\mathcal S\subset\Bbb J(n)$ finite. Then there is some finite $\mathcal B\subset\Bbb J(n)$ such that $$I\setminus\left(\bigcup\mathcal S\right)=\bigsqcup\mathcal B\tag2$$ Then from $(2)$ we can do induction over $|\mathcal S|$ and start with zero, so it seems that the base case of $m=0$ on $(1)$ is perfectly valid.","['induction', 'alternative-proof', 'elementary-set-theory', 'proof-verification']"
2752835,Find probability that Rahul examines fewer policies than toby,"$$ \underline{ \bf Attempt} $$ Let $X$ and $Y$ be the policies examined by rahul and toby, respectively. By the manner this problem is phrased we have two Bernoulli r.v. For instance, $X$ has probability success of $0.1$, thus $$ p_X(x) = {n \choose x} 0.1^x 0.8^{n-x}$$ and similarly $$ p_Y(y) = {n \choose y} 0.2^y 0.9^{n-y} $$ Now, since $X$ and $Y$ are given to be independent, we have $$ p_{XY}(x,y) = {n \choose x} {n \choose y}  0.1^x 0.8^{n-x} 0.2^y 0.9^{n-y} $$ Now, we want to find $$ P(X<Y) $$ But, for discrete case how do we compute this? Do we compute $$ \sum_{x<y} p_{XY}(x,y) $$ Perhaps my solution is not really the right to do this problem?",['probability']
2752909,Why does a linear homogeneous ODE have only a solution of summed exponentials?,"There is a linear homogeneous ODE (let's pick a second order one, but it can be in any order): \begin{align*}
af'' + bf' + cf = 0
\end{align*} We know, that \begin{align*}
f(t)=e^{\lambda t}
\end{align*} is a solution, and we need to find two $\lambda$'s, so the general solution is (if $\lambda$'s are real and distinct): \begin{align*}
f(t)=c_1e^{\lambda _1 t} + c_2e^{\lambda _2 t}
\end{align*} My question is, why do the only solution is in the form of summed exponentials? What is the proof, that there is no other solution in some other form, a non-exponential one? (I understand, that if $f_1$ and $f_2$ are solutions, then $c_1f_1+c_2f_2$ is a solution too, but I don't understand, why $f_*$ have to be exponential)",['ordinary-differential-equations']
2752937,$\mathrm{Aut}(\mathrm{Aut}(G))\cong\mathrm{Aut}(G)$ for $G$ a non-abelian simple group,"For the last couple of days I've been doing a lot of group theory problems, but I found the following particularly difficult, and quite interesting. My level is up to an introductionary course in group theory excluding Sylow Theory. I've made a big edit to include all the work done this far and exclude non-relevant parts. Let $G$ be a non-abelian group such that all normal subgroups are trivial (i.e. the only normal subgroups are $\{e\}$ and $G$ itself). Prove the following. a. $G\cong\mathrm{Inn}(G)$ b. If $\psi\in\mathrm{Aut}(G)$ and $\forall\varphi\in\mathrm{Inn}(G), \ \psi\circ\varphi=\varphi\circ\psi$ , then $\psi=\mathrm{id}_G$ . c. If $N\lhd\mathrm{Aut}(G)$ such that $N\cap\mathrm{Inn}(G)=\{\mathrm{id}_G\}$ , then $N=\{\mathrm{id}_G\}$ . d. $\mathrm{Inn}(G)\ char \ \mathrm{Aut}(G)$ e. $\mathrm{Aut}(\mathrm{Aut}(G))=\mathrm{Inn}(\mathrm{Aut}(G))\cong\mathrm{Aut}(G)$ a. Since $Z(G)\lhd G $ , either $Z(G)=\{e\}$ or $Z(G)=G$ . In the latter case $G$ is abelian, which is not the case, thus $Z(G)=\{e\}$ , and it follows that $\mathrm{Inn}(G)\cong G/Z(G)\cong G$ . b. Elaborating on the comments by Max and Servaes, we see that the given identity yields $\psi(g)\psi(a)\psi(g^{-1})=g\psi(a)g^{-1}$ and from there $g^{-1}\psi(g)\psi(a)=\psi(a)g^{-1}\psi(g)$ . Since $\psi\in\mathrm{Aut}(G)$ , $\psi$ is bijective (thus surjective), and thus $\forall g\in G \ g^{-1}\psi(g)\in Z(G)=\{e\}$ , thus $g=\psi(g)$ for all $g\in G$ , so $\psi=\mathrm{id}_G$ c. Suppose $N\lhd\mathrm{Aut}(G)$ such that $N\cap\mathrm{Inn}(G)=\{\mathrm{id}_G\}$ . Since $\mathrm{Inn}(G)\lhd\mathrm{Aut}(G)$ and the intersection between both normal subgroups is trivial, $\forall\varphi\in\mathrm{Inn}(G)\ \forall \psi\in N, \ \ \psi\circ\varphi=\varphi\circ\psi$ , thus by applying part b conclude that $\psi=\mathrm{id_G}$ if $\psi\in N$ , thus $N=\{\mathrm{id}_G\}$ indeed. d. Is worked out by Servaes. e. The last part that remains. Some conjectured that part e) might contain a typo, but I don't think so. I managed to proof the following: we know from part b) that if $\psi\in\mathrm{Aut}(G)$ commutes with all $\chi\in\mathrm{Aut}(G)$ , then certainly $\psi$ commutes with all $\varphi\in\mathrm{Inn}(G)\subset\mathrm{Aut}(G)$ . So $Z(\mathrm{Aut}(G))=\{\mathrm{id}_G\}$ , and thus $\mathrm{Inn}(\mathrm{Aut}(G))\cong\mathrm{Aut}(G)/Z(\mathrm{Aut}(G))\cong\mathrm{Aut}(G)$ . The only thing that remains to be proven is that $\mathrm{Aut}(\mathrm{Aut}(G))=\mathrm{Inn}(\mathrm{Aut}(G))$ , for a simple non-abelian group the automorphism group is complete. According to Wikipedia , this should be true, but I've no idea how to prove this, but I guess it might follow quite easily. I would appreciate it a lot if someone could write a proof for this very last part.","['abstract-algebra', 'group-theory', 'automorphism-group']"
2752940,"Show that $\operatorname{E}[\beta_i\mid Y_i,\tau_i] = (1-u_i)Y_i$.","Let\begin{align}
Y\mid\beta,\sigma^2&\sim N(X\beta, \sigma^2I)\\
\beta\mid\sigma^2,\tau_1^2,\ldots,\tau_p^2&\sim N(0,\sigma^2D), \,\,D = diag(\tau_1^2, \ldots, \tau_p^2)\\
\tau_j^2\mid\lambda&\stackrel{iid}{\sim}\operatorname{Exp}(\lambda^2/2)
\end{align} and suppose that $X = I, \sigma^2 = 1$ and $\lambda = 1$. I want to show that $$E[\beta_i\mid Y_i, \tau_i] = (1-u_i)Y_i$$ with $u_i = (1+\tau_i^2)^{-1}$. I think I need to compute the posterior density of $\beta_i$ by multiplying the prior of $\beta_i$ and the likelihood of $Y_i$, but I'm having some difficulties. I think we have the following prior of $\beta_i$ and likelihood of $Y_i$: $$p(\beta_i\mid\tau_i^2) = \dfrac{1}{\sqrt{2\pi}}\exp\bigg(-\dfrac{\beta_i^2}{2\tau_i^2}\bigg), \,\,p(Y_i\mid\beta_i\sigma^2) = \dfrac{1}{\sqrt{2\pi}}\exp\bigg(-\dfrac{(Y_i -\beta_i)^2}{2}\bigg)$$ If this is correct, the posterior probability density function looks like this: $$p(\beta_i\mid Y_i,\tau_i) = \dfrac{1}{\sqrt{2\pi}}\exp\bigg(-\dfrac{\beta_i^2}{2\tau_i^2}\bigg)\dfrac{1}{\sqrt{2\pi}}\exp\bigg(-\dfrac{(Y_i -\beta_i)^2}{2}\bigg) \\= \dfrac{1}{2\pi}\exp\bigg(-\dfrac{\beta_i^2}{2\tau_i^2} - \dfrac{Y_i^2 - 2Y_i\beta_i + \beta_i^2}{2}\bigg)$$ This is the point at which I'm stuck; I don't see how the distribution corresponding to this density would have an expected value of $(1 - u_i)Y_i$... Question: How do I solve this exercise? Am I going in the right direction? Edit: I was wondering whether I'm allowed to just consider $$\dfrac{1}{2\pi}\exp\bigg(-\dfrac{\beta_i^2}{2\tau_i^2} - \dfrac{Y_i^2 - 2Y_i\beta_i + \beta_i^2}{2}\bigg)\propto\exp\bigg(-\dfrac{\beta_i^2}{2\tau_i^2} + Y_i\beta_i - \dfrac{\beta_i^2}{2}\bigg)$$
I don't think I am; the expectation of a constant times a normal distribution is not the same as the expectation of the normal distribution itself right? I thought I could mention it anyway since I might be mistaken.","['probability-theory', 'probability', 'probability-distributions']"
2752945,Why was the word “confidence” introduced into statistical practice? What does it convey that “probability” does not?,"Edit :Hagen von Eitzen also asks about the word ""likelyhood"".","['statistics', 'probability', 'discrete-mathematics']"
2752952,Find $\lim_{n \to \infty } a_n$,"\begin{cases} a_1=\sqrt 3 \\ a_2 = \sqrt {3\sqrt 3}\\ a_n = \sqrt {3a_{n-1}} \quad \text{for } n\in\mathbb Z^+\end{cases} This sequence is bounded above by $3$ and is monotone increasing, so by monotone bounded sequence theorem, the sequence converges. But, the question asks to find $\lim_\limits{n \to \infty } a_n$. I guess the limit is $3$, but don't know how to prove it. Could you give some hint? Thank you in advance.","['real-analysis', 'sequences-and-series']"
2752958,"Bott & Tu, Exercise 4.3.1","I have some trouble computing the integral in Exercise 4.3.1 in Bott and Tu; Differential Forms in Algebraic Topology, and I was wondering if someone could help me with that. So we have the $n$-form $$\omega= \sum_{i=1}^{n}(-1)^{i-1}x_idx_1\ldots \widehat{dx_i}\ldots dx_{n+1}$$ (hat means omit) on the $n$ unit sphere $S^n$ and we have to integrate it over $S^n$. So, i did it over $S^1$ using the usual parametrization but higher dimensional analogues seem too complicated to compute. Is there an easier way? Thank you.","['algebraic-topology', 'differential-forms', 'integration', 'differential-geometry']"
2752962,Finding probability of joint discrete rv,"Let $X$ be poisson distribution with parameter $\lambda$ and $Y$
  geometric with parameter $p$. Find $P(Y>X)$. ($\bf independent$) Attempt We know $p_X(x) = \frac{ e^{-\lambda} \lambda^x }{x!} $ and $p_Y(y) = (1-p)^{y-1} p$. Now, we have the region $\{(x,y) : y>x \}$ for integers $x,y$. We want to sum over this region. My thought is to first sum $y$ from $1$ to $\infty$ and $x$ is between $y$ and $\infty$. Thus, $$ P(Y>X) = \sum_{y=1}^{\infty} \sum_{x=y}^{\infty} \frac{ e^{-\lambda} \lambda^x }{x!} (1-p)^{y-1} p = pe^{-\lambda} \sum_{y=1}^{\infty} (1-p)^{y-1} \sum_{x=y}^{\infty} \frac{ \lambda^x}{x!} = p e^{-\lambda} \frac{1}{1-1+p} \sum_{x=y}^{\infty} \frac{ \lambda^x}{x!} = e^{- \lambda} \sum_{x=y}^{\infty} \frac{\lambda^x}{x!}$$ Here is where I get stuck. My answer key says it should $e^{- \lambda p }$. But I cant get to this answer. Any help?",['probability']
2752969,The time complexity of solving an ODE,"I'm a teacher assistant in an ODE course, and I was thinking about the different methods to solve a linear ODE of order $n$ with constant coefficients, and how long they can get. For example you need to invert a matrix (a very costly operation) and do a bunch of integrals if you want to use variation of parameters, or if you want to use undetermined coefficients, finding the coefficients can be a long process (I don't know the actual computational cost though). I'd like to tell the students that you can't solve the equations faster than this, or that you can actually do it faster, at least theorically, so my question is: is there an upper/lower bound for the time complexity of analytically solving a linear ODE?","['computational-complexity', 'ordinary-differential-equations']"
2752972,Does the confidence interval that maximizes the parameter estimate have nominal coverage?,"Assume there are two confidence intervals for parameters $\theta_1$ and $\theta_2$, $C_1$ and $C_2$ with upper and lower bounds $[l_1,u_1]$ and $[l_2,u_2]$, respectively. Assume the intervals have nominal coverage $1-\alpha$. The parameter estimates are $\hat{\theta}_1$ and $\hat{\theta}_2$. Now I always (in replications of the experiment) choose the interval with the higher estimate. Does this interval still have nominal coverage? Formally define $$g:=\begin{cases} 1 \quad \text{if} \ \hat{\theta}_1 > \hat{\theta}_2 \\
2 \quad \text{if} \ \hat{\theta}_1 \le \hat{\theta}_2
\end{cases}$$ with interval  $C_g=[l_g,u_g]$. What is $P(\theta_d \in [l_g,u_g])$? Remark 1 : This seems to hold for two intervals, but fails for generalizations to more than two. I am looking for a proof of this result or a starting point. Remark 2 : I would also be interested if this problem is discussed anywhere in the literature and under which keyword / topic. Remark 3 : The problem can be understood in two ways. 1) The one population case where data $X_n$ are sampled from the same population and two different parameters are estimated on $X_n$ (e.g. mean and median, see BruceET's answer). 2) The two population case where $X_{n_1}$ and $X_{n_2}$ are sampled from two populations with $\theta_1$ and $\theta_2$ the same type of parameters (e.g. both means of potentially different values; this was my original intention with this question). Edit: Based on BruceET's answer I programmed a simulation. It shows good nominal coverage of two-sided intervals. This simulation concerns the (1) scenario in remark 3. n=15
reps=10^5
true_mean = 100 # = true_median in case of normal distribution

set.seed(12345)
cover_m = cover_med = cover_g = numeric()
for(i in 1:reps){
  x   = rnorm(n, true_mean, 10)
  m   = mean(x)
  med = median(x)
  g   = med > m
  CI_m   = t.test(x)$conf.int[1:2]
  CI_med = wilcox.test(x, conf.int=T)$conf.int[1:2]
  CI_g   = CI_m * (1-g) + CI_med * g
  cover_m[i]   = CI_m[1] <= true_mean & CI_m[2] >= true_mean
  cover_med[i] = CI_med[1] <= true_mean & CI_med[2] >= true_mean
  cover_g[i]   = CI_g[1] <= true_mean & CI_g[2] >= true_mean
}

apply( cbind(cover_m,cover_med,cover_g), 2,mean)

>cover_m cover_med   cover_g 
>0.95278   0.95097   0.95197","['statistics', 'confidence-interval', 'statistical-inference']"
2752997,A spherical ball of salt is dissolving in water,"A spherical ball of salt is dissolving in water in such a way that the rate of decrease in volume at any instant is proportional to the surface. Prove that the radius is decreasing at a constant rate. My Approach: $$\dfrac {dV}{dt}\propto Surface (S)$$
$$\dfrac {dV}{dt}=k.S$$
where $k$ is a proportionality constant. 
$$\dfrac {dV}{dt}=k.4\pi r^2$$ How do I proceed?","['derivatives', 'calculus']"
2753018,Integrating Factor Initial Value Problems,How do I solve for the initial value with the given equation: $xy'+y = e^{\sin x}\cos x$ for y $\left(π\right)$ = 1 I have tried to isolate the $y'$ by multiplying the whole equation by $\frac 1x$ and I got: $y' + \frac 1x$$y= \frac 1x$$e^{\sin x}\cos x$ I thus got my integrating factor as: $I\left(x\right)$ = $e^{∫ \frac 1x}$ = $e^{\log x}$ = x I'm not really sure how do I continue from here though.,['ordinary-differential-equations']
2753038,What is an intuitive explanation of a complex slope of a real line?,"On thinking about what real slopes and imaginary slopes are I have become a bit confused. We have one coordinate $x$-$y$-$z$ system that we use to specify position. I suppose the coordinates in this system will have only real values. But then again in the argand plane the $x$ axis is real and $y$ axis is imaginary. What is a complex slope of a real line? How do we find it?My book has this sentence: ""If $A(z_1)$ and $B(z_2)$ are two points in the argand plane then the complex slope of line $AB$ is $\frac{z_1-z_2}{\bar z_1-\bar z_2}$."" Where did they get this from? I'm not very sure whether this formula is correct either.  I do not have extensive knowledge of complex numbers, I have just studied the basics.","['coordinate-systems', 'complex-analysis', 'complex-numbers', 'polar-coordinates']"
2753141,Find $f_{10}$ of the sequence defined by $f_{n+1}=\frac{8f_n}{5}+\frac{6 \sqrt{4^n-f_n^2}}{5}$,"Find $f_{10}$ of the sequence defined by $$f_{n+1}=\frac{8f_n}{5}+\frac{6 \sqrt{4^n-f_n^2}}{5}$$ given $f_0=0$ My Approach: Letting $f_n=2^n b_n$ we get $$b_{n+1}=\frac{4b_n}{5}+\frac{3 \sqrt{1-b_n^2}}{5}$$ Now letting $b_n=\cos(x_n)$ we get $$\cos(x_{n+1})=\cos(x_n-\theta)$$ where $\cos(\theta)=\frac{4}{5}$ Now Since $f_0=0$ we have $b_0=0$ and $x_0=\frac{\pi}{2}$ Now we have $$x_{n+1}=x_n-\theta$$ Putting $n=0,1,2,3 \cdots 10$ and adding all we get $$x_{10}=\frac{\pi}{2}-10\theta$$ Hence $$b_{10}=\cos\left(\frac{\pi}{2}-10\theta\right)=\sin(10\theta)=\sin\left(10\arcsin\left(\frac{3}{5}\right)\right)$$ How to proceed further?","['algebra-precalculus', 'trigonometry', 'sequences-and-series', 'inverse-function']"
2753210,When can we say that $A^{\mathrm T} B = B^{\mathrm T} A$?,"I was looking at the derivation of the normal equation from here . Now, the author has used the fact that $A^{\mathrm T} B = B^{\mathrm T} A$ to reach the step shown in the below image. Can anyone provide some information like, when is it true, or how we can prove it? $$J(\theta) = ((X\theta)^{\mathrm T} -y^{\mathrm T})(X\theta -y)$$ $$J(\theta) = (X\theta)^{\mathrm T} X\theta -\color{blue}{(X\theta)^{\mathrm T} y \color{black}{-} y^{\mathrm T} (X\theta)} +y^{\mathrm T} y$$ Note that $X\theta$ is a vector, and so is $y$ . So when we multiply one by another, it doesn't matter what the order is (as long as the dimensions work out). So we can further simplify: $$J(\theta) = \theta^{\mathrm T} X^{\mathrm T} X \theta -\color{blue}{2(X\theta)^{\mathrm T} y} +y^{\mathrm T} y$$","['matrices', 'transpose', 'linear-regression', 'matrix-calculus']"
2753211,Log Likelihood vs Chi Squared- what's the difference?,"A program I have used for a linguistics analysis offers the option of using log likelihood or chi-squared to calculate the keyness of a word
(Keyness being the actual frequency of a word in a text in comparison to the expected frequency based upon its frequency in a reference text)
The program has a default setting of using log likelihood, so I happily left it at that and let it compute the statistics I needed. 
I do however, need to briefly explain the difference between the two methods, and possibly why LL is suggested as the better option. I haven't studied maths in a long time. Can someone explain the difference between chi-squared and log likelihood, in simpleton terms for me?",['statistics']
2753223,"Elementary proof of ""irreducible polynomials remain irreducible over transcendental extension""?","Consider the following statement Let $K$ be a field, $L/K$ a purely transcendental extensions of fields (i.e. $K$ is relatively algebraically closed in $L$). Let $F \in K[X_1, \ldots, X_n]$ be an irreducible polynomial over $K$. Then $F$ remains irreducible over $L$. Is there an elementary way to show this statement, accesible to undergraduate students? I will give a proof of which I consider at least part 1 to be non-elementary. (source:  Irreducibility of Polynomials over Global Fields is Diophantine, Philip Dittmann, arxiv ) Special case $K$ and $L$ algebraically closed . In this special case, the statement follows by quantifier elimination in algebraically closed fields. Indeed, irreducibility of a polynomial $F$ can be written as a firs-order formula with the coefficients of $F$ as parameters. By quantifier elimination this formula is equivalent to a formula without quantifiers, which then holds in $L$ as soon as it holds in $K$. General case. Let $\overline{L}$ be an algebraic closure of $L$ and $\overline{K}$ the algebraic closure of $K$ in $\overline{L}$. After a change of coordinates we may assume the constant coefficient of $F$ is $1$. Suppose $F$ factors as a product of irreducible polynomials $F_1, \ldots, F_n$ over $\overline{K}$ with constant coefficient $1$. By the special case each of these factors (in $\overline{K}[X_1, \ldots, X_n]$) remains irreducible over $\overline{L}$. Suppose for the sake of a contradiction that $F$ were reducible over $L$, then we would have $F = G \cdot H$ for certain $G, H \in L[X_1, \ldots, X_n]$ with constant coefficient $1$. By unique factorisation in $\overline{L}[X_1, \ldots, X_n]$, both $G$ and $H$ would have to be products of certain $F_i$'s, whereby their coefficients would lie in $\overline{K} \cap L = K$, contradicting the assumption that $F$ is irreducible over $K$.","['irreducible-polynomials', 'model-theory', 'abstract-algebra', 'extension-field', 'commutative-algebra']"
2753267,Evaluate the integral $\int_{-\infty}^{\infty} e^{-x^2} e^{ix} dx$,I tried common techniques like integration by parts but did not succeed. The $e^{ix}$ also is causing some trouble. I'm guessing this is a Fourier transform in disguise but I'm not sure how the evaluate this. Any help will be appreciated!,"['complex-analysis', 'fourier-analysis', 'analysis']"
2753294,Convergence concerning the $\alpha$th derivative of $f(x)=x^\alpha-\alpha^x$,"Let $x_\alpha$ be the stationary point of the $\alpha$ th derivative of the function $f(x)=x^\alpha-\alpha^x$ , and let $$\lambda_\alpha=\frac{f(-x_\alpha)}{-x_\alpha}.$$ For $\alpha\in\Bbb N$ , does the limit $$\lim_{\alpha\to\infty}\bigg|\frac{\lambda_{\alpha+1}}{\alpha\lambda_\alpha}\bigg|$$ exist, and if so, what is its value? We write $$\begin{align}f(x)=x^{\alpha}-\alpha^x&\implies f'(x)=\alpha x^{\alpha-1}-\alpha^x\ln\alpha\\&\implies\quad\quad\qquad\cdots\\&\implies f^{(\alpha)}(x)=\left[\prod_{i=0}^{\alpha-1}(\alpha-i)\right]-\alpha^x\ln^\alpha\alpha=0\end{align}$$ for stationary points. Then $$-x_\alpha=\frac1{\ln\alpha}\left[\ln\ln^\alpha\alpha-\sum_{i=0}^{\alpha-1}\ln(\alpha-i)\right]\tag{1}$$ so $$\begin{align}&\lambda_{\alpha}=\frac{\frac1{\ln^\alpha\alpha}\left[\ln\ln^\alpha\alpha-\sum_{i=0}^{\alpha-1}\ln(\alpha-i)\right]^\alpha-\frac{\ln^\alpha\alpha}{\prod_{i=0}^{\alpha-1}(\alpha-i)}}{\frac1{\ln\alpha}\left[\ln\ln^\alpha\alpha-\sum_{i=0}^{\alpha-1}\ln(\alpha-i)\right]}\\\implies&\lambda_{\alpha}=\frac{\left[\ln\ln^\alpha\alpha-\sum_{i=0}^{\alpha-1}\ln(\alpha-i)\right]^\alpha\prod_{i=0}^{\alpha-1}(\alpha-i)-\ln^{2\alpha}\alpha}{\ln^{\alpha-1}\alpha\left[\ln\ln^\alpha\alpha-\sum_{i=0}^{\alpha-1}\ln(\alpha-i)\right]\prod_{i=0}^{\alpha-1}(\alpha-i)}\tag{2}\end{align}$$ The expression for $|\lambda_{\alpha+1}/\alpha\lambda_{\alpha}|$ is extremely complicated so I will not give it here. How should I continue? It seems that the limit does exist , but I would like a rigorous proof of it.","['derivatives', 'stationary-point', 'convergence-divergence', 'limits']"
2753304,"If a tree has $n$ vertices, then it must have a path with at least $k$ vertices or at least $n/k$ leaves","Prove the following statement: If a tree has $n$ vertices, then it must have a path with at least $k$ vertices or the tree must contain at least $\dfrac{n}{k}$ leaves. My thoughts: I kind of get why this is true, since if the longest path has $k-1$ vertices, then at best you can have one vertex in the middle with paths of length $\dfrac{k-1}{2}$ going out from it in $\dfrac{2n}{k-1}$ directions for a total of $\dfrac{2n}{k-1}$ leaves. However, I can't seem to give a rigorous proof of this - any hints?","['graph-theory', 'trees', 'discrete-mathematics']"
2753330,How to find the trace of exponential of a matrix,"Given $$A=\begin{bmatrix}-3&2\\-1&0\end{bmatrix}$$ how to find $e^{trA}$ ? $e^A=pe^Dp^{-1}$ where p is invertible matrix and D is diagonal matrix whose diagonal entries are the eigen value of A. Should i proceed with this concept. Please suggest me.
Thank you.","['matrices', 'matrix-exponential']"
2753371,"Does inscribing a circle, then a triangle, then ..., inside of an initial triangle telescope to some ""center"" of that triangle?","Start with a triangle, and construct its inscribed circle. Take the three points where the inscribed circle is tangent to the triangle, and construct a new triangle with those points as the vertices. Then construct the inscribed circle of this new triangle. Continuing this construction indefinitely gives us a sequence of inscribed circles and triangles that telescope down to a limit point . This is just the construction from this question , but we start with a triangle instead of a circle (and so no choice of points is being made). In the comments Steven Stadnicki brought up a good question: does this construction telescope down to a defined center of the original triangle? By ""defined center"", I mean any of the many many points that have been given a name  like prefix center of a triangle. It's naturally not the incenter, since the limit point will literally be the limit point of the incenters of the sequence of nested triangles, which isn't constant. It can't be the orthocenter or circumcenter since those could easily lie outside the triangle, and I checked that it's not the centroid. I suppose an equivalent question would be: does there exist an alternative way to construct this limit point using only finitely many steps that maybe someone has considered before?","['recreational-mathematics', 'euclidean-geometry', 'triangles', 'geometry']"
2753375,"Prove For $(X_n)_{n \geq1}$ independent RVs, $ X_n \rightarrow X \ \text{ a.s.} \Rightarrow \sum _{n\geq1} P(|X_n -X| \gt \varepsilon) \lt \infty$","I would like to show the following: For $(X_n)_{n \geq1}$ independent RVs, $$ X_n \rightarrow X \ \text{ a.s.} \Rightarrow\ \forall \varepsilon \gt0, \ \sum _{n\geq1} P(|X_n -X| \gt \varepsilon) \lt \infty$$ We don't know that $|X_n -X| \gt \varepsilon$ are independent so second Borel-Cantelli cannot be applied. Any hint is appreciated.","['probability', 'borel-cantelli-lemmas']"
2753435,Is there an elementary proof of Cayley-Bacharach that holds in the case where points are not necessarily distinct?,"Silverman and Tate in Rational Points On Elliptic Curves (and Tao here ) discuss a proof of the Cayley-Bacharach Theorem in the case where two conics intersect at nine distinct points. The proof uses Bézout's theorem, but other than that is elementary. Is there a way to modify the aforementioned proof (or, present some other simple proof) that can prove Caley-Bacharach in the case where the nine intersection points of the conics are not necessarily distinct (that is, the number of intersection points counting multiplicity is nine, but not counting multiplicity is less than nine)?","['algebraic-geometry', 'abstract-algebra', 'intersection-theory', 'elliptic-curves', 'conic-sections']"
2753446,What is minimum speed needed to throw object over a wall that height h and at distance d?,"(I am not expert in English. I will write as well as I can.) I ask this question because I want to solve ( What is minimum speed needed to jump over sphere object that has radius R and at distance d? ) But I think I have to this question before. This question is like that question but condition to find u minimum is less complex. If I know how to solve this question, I may be able to solve that question. To understand this question easier, lets see this picture. From this picture, what is minimum initial speed to throw red ball over this blue wall? (assume red ball is very small and throw form h=0 and blue wall is very thin) d and h can be any positive real number. g is a gravitational acceleration(approximately 9.80665 $m/s^2$). This is a mathematical-physics question but mainly in maths. I can do physics part but have problem in maths part. Physics Part : Let red ball in all pictures is at origin point and y is height The relation between x and y for projectile motion is $y(x) = xtan\theta - \frac{gx^2}{2u^2cos^2\theta}$ , $0 < \theta < \frac{\pi}{2}$ Condition of the wall is $y(d) \geq h$ but for $u_{min}$, $y(d) = h$ $dtan\theta - \frac{gd^2}{2u^2cos^2\theta} = h$ $tan\theta - \frac{gd}{2u^2cos^2\theta} = \frac{h}{d}$ $\frac{gd}{2u^2cos^2\theta} = tan\theta - \frac{h}{d}$ $\frac{1}{u^2} = \frac{2cos^2\theta}{gd}( tan\theta - \frac{h}{d} )$ $u = \sqrt{ \frac{gd}{2cos^2\theta( tan\theta - \frac{h}{d} )} }$ At this point, I don't how to find $u_{min}(d,h)$ from this equation. (If you give value of d and h (for example, d = 2m and h = 1m), it is possible to find $\theta$ that minimize u.) I know only that y(x) (parabola curve) for $u_{min}(d,h)$ look like. Case : d $\leq$ c h (c is a constant. There is a ratio that if d > c h , y(x) won't have maximun point at the top of the wall.) Case : d > c*h Please help me.","['classical-mechanics', 'trigonometry', 'calculus', 'projectile-motion']"
2753555,Which statistical test should I use in this situation (sample size question)?,"Assume I have 5 millions oranges. Now I want to test a hypothesis. If there is a black dot on an orange, then the orange is BAD. Within these 5 millions oranges, I have been told that which are bad. However, I cannot check all bad oranges to see whether there is black dot on each. I want to find the sample size that I need to prove my hypothesis is right at 95% sure.",['statistics']
2753585,Is the infinite product of $-1 \times -1 \times -1 \times\dots = -i$?,"So I woke up this morning and I was thinking about the infinite product $-1 \times -1 \times -1 \times\dots$, and what it equals. I came to the conclusion that it equals $-i$. Alternatively stated, $
{\displaystyle \prod_{i}^{\infty} (-1)} = -i
$ Here's how I reached this: $${\prod_{i}^{\infty} (-1)} = e^{\ln({\displaystyle \prod_{i}^{\infty} (-1)})} = e^{\displaystyle \sum_{i}^{\infty}{\ln(-1)}}= e^{\displaystyle \sum_{i}^{\infty}{i\pi}}=e^{i\pi\displaystyle \sum_{i}^{\infty}{1}}$$ Now, here's where I'm a little hesitant. I want to say that, from $\zeta(0)=-\frac{1}{2}$, we can conclude that $$e^{i\pi\displaystyle \sum_{i}^{\infty}{1}} = e^{-\frac{1}{2}i\pi} = -i$$. I have been told before that the sum $\displaystyle \sum_{i}^{\infty}{1}$ is not actually $-\frac{1}{2}$, but I'm not really sure why. It would seem that if this is the case, then my product would in fact not be $-i$. Though, I must say that $-i$ sort of makes sense, because multiplying complex numbers is essentially rotating them, and so rotating by $180$ every time will get you $180+180+180+...$ is the same as $180*(1+1+1+...)$ which is (if my premise is right) $180*(-\frac{1}{2})=-90$. $-90$ degrees on the complex plane turns out to be $-i$. So my question is, is there a hole in my logic? I know what not accounting for $\zeta(0)=-\frac{1}{2}$, the sum $1+1+1+...$ is divergent, but taking that into account, can I say with confidence that $-1 \times -1 \times -1 \times\dots = -i$?","['complex-analysis', 'infinite-product', 'riemann-zeta']"
2753594,Understanding solution of PDE using method: separation of variables.,"Could someone please help me to understand the doubts I have about the solution of this pde problem and to check the things that I've added to the solution? Oscillations of the beam are described by equation $u_{tt}+Ku_{xxxx}=0, \ with\ 0\le x\le L,K>0$. If both ends clamped, then the boundary conditions are $u(0,t)=u_x(0,t)=0\\u(L,t)=u_x(L,t)=0.$ Use separation of variables to find eigenvalues and their corresponding eigenfunctions. Solution. First off consider the change of coordinates $[0, L]\ to \ [-L,L],L=1/2.$ Separating variables $u(x,t)=X(x)T(t)$ we arrive to $T''+\lambda T=0$ and $X^{iv}-\frac{\lambda}{k}X=0 $. If $w^4=\lambda/k$ then $$X^{(iv)}=w^4X\\X(L)=X'(L)=0\\X(-L)=X'(-L)=0\\T''+Kw^4T=0$$ w>0 Questions I understand the change from $[0, L]\ to \ [-L,L],L=1/2$, but why use it in the solution? Why not to use the original interval? From comments, Leucippus mentioned that conditions for $T$ are missing, how do I find such conditions? I don't understand the '..up to a constant factor $C=1$,..' in (4.17) red square, why to take $C=1$ ? Finally in (4.19) and (4.20) we arrive to the solutions $X(t)$ in which I don't see what is the eigenvalue (In my book eigenvalue was given like $\beta=\frac{\pi x}{l}$ and it came from applying the conditions). I think it's $w$ but how do I get $w$ to have the form $\lambda_n=\frac{n\pi }{l}$? I also think the solution is not complete, we also need solution of $T''+Kw^4T=0$ am I right? Could someone help me to understand please? Now suppose eigenvalue $\lambda=0$, then $kw^4=0.$ $$\implies X^{iv}=0,\ T''=0$$ Applying condition $x=0$ in the solution gives $X(x)=A$ and $T(t)=B$, thus $\lambda=0 $ is an eigenvalue and the eigenfunction is any constant. Now suppose eigenvalue $\lambda<0$, then $kw^4<0.$ I won't continue here since I don't know If I'm doing everything right or wrong. Note: An alternative solution is very welcome especially if it's easier than mine :) Thank you.","['alternative-proof', 'partial-differential-equations', 'proof-verification', 'boundary-value-problem', 'ordinary-differential-equations']"
2753607,Why is $+1$ necessary in the definition of the $\lt$-minimum element of a set of ordinals,"My question pertains to this paragraph in Schimmerling's ""Intro to Set Theory,"" page 34. If $A$ is a non-empty set of ordinals, then $A$ has an $\lt$-minimum element. To justify the definition, use that fact that $A\subseteq \text{sup}(A)+1$ and $(\text{sup}(A)+1,\lt)$ is a wellordering. Why do you need the $+1$? My guess is to deal with, for example, $A=\{1, 2,\dots\}$, where $\text{sup}(A)=\omega$, but $\omega\notin A$. If this is a correct guess, why is $A$ any more of a subset of $\omega+1$ than $\omega$, and why is $(\omega+1,\lt)$ any more of a wellordering than $(\omega,\lt)$? Thanks",['elementary-set-theory']
2753618,"Verify a certain statistic is complete (namely, $\bar X$ for $\frac{x+1}{\theta(\theta+1)}e^{-\frac{x}{\theta}} \quad \text{for} \ \ x>0, \ \theta>0$)","Given the density function of a certain population+ $$f_\theta(x)=\frac{x+1}{\theta(\theta+1)}e^{-\frac{x}{\theta}} \qquad \text{for } x>0, \ \theta>0$$ Since $f_\theta(x)$ is part of the exponential k-parametric family, from its factorization one immediately knows that $\bar X$ is a minimal sufficient statistic. Moreover, it is complete; this is a fact you may find in several statistical inference books. However, I cannot use this result freely in my statistics course, and I may prove by definition that $\bar X$ is complete. Here is my attempt: If one wants to prove that if $E[f(\bar X)]=0$, then one must have $f=0$ almost everywhere, first one has to explicitly give the density of $\bar X$ to compute the expectancy of $f(\bar X)$. However, it is not trivial what this density may be, as $f_\theta(x)$ does not correspond to the density of any of the well-known continuous distributions for which $\bar X$ is easily obtainable. So, I may define the following change of variables: $$(Y_1,Y_2,\ldots,Y_{n-1},Y_n)=(X_1,X_2,\ldots,X_{n-1},\bar X)$$ Which has as inverse function $$(X_1,X_2,\ldots,X_n)=(Y_1,Y_2,\ldots,nY_n-(Y_1+Y_2+\cdots+Y_{n-1}))$$ And therefore, its jacobian matrix is $$J=\begin{bmatrix}
    1       & 0 & 0 & \dots & -1 \\
    0       & 1 & 0 & \dots & -1 \\
   \vdots & \vdots & \vdots & & \vdots \\
    0       & 0 & 0 & \dots & n
\end{bmatrix}$$ Which gives $|\det(J)|=n$. Therefore, one has $$f_{(Y_1,\ldots,Y_n)}(y_1,\ldots,y_n)=nf_{(X_1,\ldots,X_n)}(y_1,\ldots,y_{n-1},ny_n-(y_1+\cdots+y_{n-1}))$$ where the latter equality is nothing but the joint sample density $f_\theta(x_1,\ldots,x_n)$ at the point $(y_1,\ldots,y_{n-1},ny_n-(y_1+\cdots+y_{n-1}))$ Subtituding one has \begin{align} & f_{(Y_1,\ldots,Y_n)}(y_1,\ldots,y_n) \\ = {} & n\frac{(y_1+1)(y_2+1)\cdots(ny_n-(y_1+y_2+\cdots+y_{n-1}))}{\theta^n(\theta+1)^n}e^\frac{-(y_1+\cdots+y_{n-1}+ny_n-y_1-\cdots-y_{n-1})}{\theta} \\ = {} & n\frac{(y_1+1)(y_2+1)\cdots(ny_n-(y_1+y_2+\cdots+y_{n-1}))}{\theta^n(\theta+1)^n} e^{-\frac{ny_n}{\theta}}
\end{align} To finally obtain $f_{\bar X}(\bar X)$ one must calculate $$f_{\bar X}(\bar X)=\int_{C_{Y_1}}\int_{C_{Y_2}}\cdots\int_{C_{Y_{n-1}}} f_{(Y_1,\ldots,Y_n)}(y_1,\ldots,y_n) \ dy_1 \, dy_2 \ldots dy_{n-1}$$ Which can be rewritten as $$\frac{n}{\theta^n(\theta^n+1)}e^{-n\frac{y_n}{\theta}} \int_0^\infty \int_0^\infty\cdots\int_0^\infty(y_1+1)(y_2+1)\cdots(ny_n-y_1-\cdots-y_n-1) \ dy_1 \, dy_2\cdots dy_n$$ However, I don't know how to compute that integral, nor if my previous calculations are correct. I think this density can also be found via obtaining the characteristic function, but I am not sure if that method is any better than finding the marginal distribution as I have just tried. After all that one should still compute $\operatorname E[f(\bar X)]$; I hope its an easier task. Thanks in advance for your collaboration.","['statistical-inference', 'probability-theory', 'statistics', 'integration', 'probability']"
2753619,Average coverage of circles,"Suppose we are given an infinite sequence $(\mathbf{r}_n)_{n=1}^\infty$ of 2D points, spread uniformly over space, with a given density $\rho$. What is the supremum of the percentage of surface covered by the circles with centers $\mathbf{r}_n$ and radii $a_n$, such that the circles do not overlap? The answer will obviously not depend on the sequence $(\mathbf{r}_n)_{n=1}^\infty$. I also think that $\rho$ will just become a multiplicative constant. Clarification : $(\mathbf{r}_n)_{n=1}^\infty$ is a fixed sequence of 2D points distributed uniformly (constant density) over the 2 dimensional plane. Not all sequences of radii $(a_n)_{n=1}^\infty$ are such that the circles do not overlap, but some do. Therefore, the supremum over this set of the percentage of the plane covered by the circles exists. My question is whether this percentage can be calculated or how to approach this problem. An image is worth a thousand words : Since the 2D surface is infinitely large, no probability theory is required and the solution will not depend on $(\mathbf{r}_n)_{n=1}^\infty$. Problem statement as a limit, ($\rho=1$ for simplicity) : Given $N$ uniformly distributed random 2D vectors on a square of size $\sqrt{N}\times\sqrt{N}$, called $\mathbf{r}_1,\cdots,\mathbf{r}_N$. Define: $$f(\mathbf{r}_1,\cdots,\mathbf{r}_N)=\sup_{a_1,\cdots,a_N}\left\{\frac{1}{\underbrace{N}_{\text{total area}}}\underbrace{\sum_{i=1}^N\pi a_i^2}_{\text{filled area}}:\underbrace{\forall i\forall j\neq i:\|\mathbf{r}_i-\mathbf{r}_j\|\leq a_i+a_j}_{\text{no circles overlap}}\right\}$$ What is $\lim_{N\to\infty}f(\mathbf{r}_1,\cdots,\mathbf{r}_N)$? I hope everybody agrees that this number will not depend on the choice of $(\mathbf{r}_n)_{n=1}^\infty$, since the 2D surface is infinitely large, but I am afraid my question is ill-posed (does taking the $\mathbb{E}$ help?).",['geometry']
2753625,Find the limit of some roots raised to n,"let $x_1,x_2,x_3$ be the roots of $x^3-x^2-1=0$. If $x_1$ is a real root of the equation, compute: $\lim_{n\to\infty}(x_2^n+x_3^n).$ First I find these relations using viete: $x_1+x_2+x_3=1$ $x_1^2+x_2^2+x_3^2=1$ $x_1^3+x_2^3+x_3^3=4$ Now, we can find a recurrence of the other sum of roots using the inial equation and we get this sequence: $$a_n=a_{n-1}+a_{n-3}$$ where $$a_1=1, a_2=1, a_3=4$$ And I tried something like this to try and solve this problem but couldn't work it out until the end... I'm actually stuck here.","['real-analysis', 'polynomials', 'limits']"
2753630,"Are there any other fields other than $\mathbb{R},\mathbb{C}$, rich enough to have analysis built on them?","I've been thinking about this, I don't know how to look up anything similar, so here I am asking a question. Specifically, is there any space $X$ with the following properties: Algebraic structure: The first thing we demand is that $X$ is a commutative ring . Later on, I'll explain why the field structure is also demanded. Topology. Obviously we need a notion of closeness on our space $X$. What is more is that we want to correlate the distance of points $a,b$ to the element $a-b$ of $X$. That being said, we demand that there exists something like a norm on $X$ (note: $X$ is not necessarily a vector space over $\mathbb{R}$), namely $\|\cdot\|$, that has the following properties: (a) $\|x\|=0$ iff $x=0$. (b) $\|-x\|=\|x\|$ (c) $\|x+y\|\leq\|x\|+\|y\|$ So $\|\cdot\|$ actually induces a metric on $X$: set $d(x,y):=\|x-y\|$ . Short proof:  1) $d(x,y)=0$ iff $\|x-y\|=0$ 
 iff $x-y=0$ iff $x=y$. 2) $d(x,y)=\|x-y\|=\|y-x\|=d(y,x)$. 3) $d(x,y)=\|x-y\|=\|x-z+z-y\|\leq\|x-z\|+\|z-y\|=d(x,z)+d(z,y)$. We demand the ability to differentiate some functions $f:X\to X$, so we want a field structure, since we have to deal with limits of the form $\displaystyle{\frac{f(x_n)-f(x)}{x_n-x}}$. I am not adding anything extra for integration, since measures can be attached to any set. Before you comment or answer about the Frobenius theorem , note that there is no need for $X$ to be a vector space over $\mathbb{R}$ ,or, even if it was, no need of being of finite dimension over $\mathbb{R}$. I also know some basics about differentiable manifolds, so any info on this is not necessary. I'm looking for an exact answer on the question, and, differentiable manifolds, although close (in the sense that there can be tons of analysis performed on them), are far from having those properties. Part 2 : (refering to the members of the community with experience on research) If there are known examples of such spaces, are they interesting at all? P.S.
I couldn't think of any reason this question is trivial or silly, but if it is, feel free to say so! Also, if anyone has a better idea for the tags, please suggest an edit.","['derivatives', 'general-topology', 'field-theory', 'analysis']"
