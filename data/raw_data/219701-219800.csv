question_id,title,body,tags
4494251,Prove that the intersection of two open sets is open.,"I am just starting to learn proofs and my question mainly has to do with the structure. Let A and B be open sets. If the statement $A \cap B \neq \emptyset \implies A \cap B$ is open is true, then its contrapositive $A \cap B$ not open $\implies A \cap B = \emptyset$ is also true. However, we know that the empty set is open as well as closed, so the first statement can't be true. The problem is, I know that this is a true statement. Where am I going wrong?","['proof-writing', 'logic', 'real-analysis', 'solution-verification', 'elementary-set-theory']"
4494262,A random walk on half the number line,Consider a symmetric random walk on the number line where steps are size $1$ .  If a step from $0$ tries to go to $-1$ you stay at $0$ instead. You start at $0$ and we want to compute the expected time to reach $x>0$ . From numerical experiments it seems to be $x(x+1)$ .  How can this be proved?,['probability']
4494289,Derive a formula $\sum_{j=1}^{n-1}j^a \sim \zeta(-a)+\frac{n^{a+1}}{a+1}\sum_{s=0}^\infty \binom{a+1}{s}\frac{B_s}{n^s} $,"Detailed derivations on this formula : There is an exercise problem (Ex. 3.2) in Olver's book, on page 292. (or you can find it on this website: Eq.2.10.7 ). $$\sum_{j=1}^{n-1}j^a \sim \zeta(-a)+\frac{n^{a+1}}{a+1}\sum_{s=0}^\infty \binom{a+1}{s}\frac{B_s}{n^s} \tag{*}$$ I try to derive it. I begin with: $$\sum_{j=n_0}^n f(j)=\int_{n_0}^n f(x) dx+\frac{f(n_0)+f(n)}{2}+\sum_{s=1}^{m-1}\frac{B_{2s}}{(2s)!}\left( f^{(2s-1)}(n)-f^{(2s-1)}(n_0) \right)+R_m(n) $$ let $n_0=1$ and $f(x)=x^a$ , for left-hand-side: $$\sum_{j=n_0}^n f(j)=\sum_{j=1}^n j^a$$ for right-hand-side: $$\begin{align}
\int_{n_0}^n f(x) dx&=\int_1^n x^a dx=\frac{1}{a+1}x^{a+1}|_1^n=\frac{n^{a+1}}{a+1}-\frac{1}{a+1}\tag{1}\\
\\
\frac{f(n_0)+f(n)}{2}&=\frac{f(1)+f(n)}{2}=\frac{1+n^a}{2}\tag{2}\\
\\
f^{(2s-1)}(x)&=a(a-1)...(a-2s+2)x^{a-2s+1}=\frac{a!}{(a-2s+1)!}x^{a-2s+1}\\
\\
f^{(2s-1)}(n)&=\frac{a!}{(a-2s+1)!}n^{a-2s+1}\\
\\
f^{(2s-1)}(n_0)&=f^{(2s-1)}(1)=\frac{a!}{(a-2s+1)!}
\end{align}$$ $$\begin{align}
\sum_{s=1}^{m-1}\frac{B_{2s}}{(2s)!}\left( f^{(2s-1)}(n)-f^{(2s-1)}(n_0) \right)&=\sum_{s=1}^{m-1}\frac{B_{2s}}{(2s)!}\frac{a!}{(a-2s+1)!}\left(n^{a-2s+1}-1\right)\\
\\
&=\frac{1}{a+1}\sum_{s=1}^{m-1}\frac{B_{2s}}{(2s)!}\frac{(a+1)!}{(a-2s+1)!}\left(n^{a-2s+1}-1\right)\\
\\
&=\frac{1}{a+1} \sum_{s=1}^{m-1}B_{2s}\binom{a+1}{2s }\left(n^{a-2s+1}-1\right) 
\end{align}$$ Next, substitute: $s'=2s$ and use the fact $B_{2k+1}=0$ for $k=1,2,3,...$ $$\begin{align}
\sum_{s=1}^{m-1}\frac{B_{2s}}{(2s)!}\left( f^{(2s-1)}(n)-f^{(2s-1)}(n_0) \right)&=\frac{1}{a+1} \sum_{s'=2}^{2m-2}B_{s'}\binom{a+1}{s'}\left(n^{a-s'+1}-1\right)\\
\\
&=\frac{n^{a+1}}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}\frac{B_{s'}}{n^{s'}}-\frac{1}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}B_{s'}\tag{3}
\end{align}$$ Combine $(1)(2)(3)$ , $$\begin{align}
\sum_{j=1}^n j^a&=\frac{n^{a+1}}{a+1}-\frac{1}{a+1}+
\frac{1+n^a}{2}+\frac{n^{a+1}}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}\frac{B_{s'}}{n^{s'}}-\frac{1}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}B_{s'}+R_m(n)\\
\\
\sum_{j=1}^n j^a&=n^a+\frac{n^{a+1}}{a+1}-\frac{n^a}{2}+\frac{n^{a+1}}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}\frac{B_{s'}}{n^{s'}}\\
&~~~~~~~~~-\frac{1}{a+1}+\frac{1}{2}-\frac{1}{a+1} \sum_{s'=2}^{2m-2}\binom{a+1}{s'}B_{s'}+R_m(n)\\
\end{align}$$ Use the fact $B_0=1,~B_1=-\frac{1}{2}$ : $$\begin{align}
\sum_{j=1}^n j^a=n^a+\frac{n^{a+1}}{a+1}\sum_{s'=0}^{2m-2}\binom{a+1}{s'}\frac{B_{s'}}{n^{s'}}-\frac{1}{a+1}\sum_{s'=0}^{2m-2}\binom{a+1}{s'}B_{s'}+R_m(n)\tag{4}
\end{align}$$ The remainder becomes: $$\begin{align}
R_m(n)&=\int_{n_0}^n \frac{B_{2m}-B_{2m}(x-\lfloor x\rfloor)}{(2m)!}f^{(2m)}(x)dx\\
\\
&=\color{red}{\int_{n_0}^\infty \frac{B_{2m}-B_{2m}(x-\lfloor x\rfloor)}{(2m)!}f^{(2m)}(x)dx}\color{blue}{-\int_{n}^\infty \frac{B_{2m}-B_{2m}(x-\lfloor x\rfloor)}{(2m)!}f^{(2m)}(x)dx}\tag{5}\end{align}$$ where $$f^{(2m)}(x)=a(a-1)...(a-2m+1)x^{a-2m}=\frac{a!}{(a-2m)!}x^{a-2m}$$ Since $|B_{2m}-B_{2m}(x-\lfloor x\rfloor)|\le 2|B_{2m}|$ , the $\color{blue}{\text{blue-colored term}}$ is bounded by $$\begin{align}\left|-\int_{n}^\infty \frac{B_{2m}-B_{2m}(x-\lfloor x\rfloor)}{(2m)!}f^{(2m)}(x)dx\right|&\le 2|B_{2m}|\int_{n}^\infty \frac{1}{(2m)!}|f^{(2m)}(x)|dx\\
\\
&=\left|\frac{2B_{2m}}{(2m)!}\cdot\frac{a!}{(a-2m)!}\cdot \frac{-1}{a-2m+1}\cdot n^{a-2m+1}\right|\\
\\
&=\left|\frac{2B_{2m}}{a+1}\cdot\binom{a+1}{2m}\right|\cdot \frac{n^{a+1}}{n^{2m}}\\
\\
&=\mathcal{O}\left( \frac{n^{a+1}}{n^{2m}}\right)\tag{6}\end{align}$$ The $\color{red}{\text{red-colored term}}$ becomes $$
\begin{align}
\int_{n_0}^\infty \frac{B_{2m}-B_{2m}(x-\lfloor x\rfloor)}{(2m)!}f^{(2m)}(x)dx&=\frac{B_{2m}}{(2m)!}f^{(2m-1)}(x)\bigg|_{n_0}^\infty-\frac1{(2m)!}\sum_{k=n_0}^\infty\int_{k}^{k+1} B_{2m}(x-k)f^{(2m)}(x)dx\\
\\
&=-\frac{B_{2m}}{(2m)!}f^{(2m-1)}(n_0)-\frac{1}{(2m)!}\sum_{k=n_0}^\infty I_{k,2m}\tag{7}
\end{align}$$ where $$I_{k,p}=\int_{k}^{k+1} B_{p}(x-k)f^{(p)}(x)dx$$ perform the integration by part, and we get $$I_{k,p}=B_p(1)f^{(p-1)}(k+1)-B_p(0)f^{(p-1)}(k)-\int_{k}^{k+1} B'_{p}(x-k)f^{(p-1)}(x)dx$$ Use the property: $B'_s(t)=s\cdot B_{s-1}(t),~~s=1,2,3,\dots$ , we get recursive equation $$I_{k,p}=B_p(1)f^{(p-1)}(k+1)-B_p(0)f^{(p-1)}(k)-p\cdot I_{k,p-1}\tag{8}$$ Note the following properties between Bernoulli polynomial and Bernoulli number: $$B_p(1)=B_{p}(0)=B_p,~~p=2,3,\dots~~~~~\text{But}~~~~B_1(1)=-B_1=\frac12,~~ B_1(0)=B_1=-\frac12$$ hence, we need to deal with the $p=1$ cases separately. Then, eq.(8) becomes $$I_{k,p}=B_p\cdot\left[f^{(p-1)}(k+1)-f^{(p-1)}(k)\right]-p\cdot I_{k,p-1},~~~~p=2,3,\dots\tag{9}$$ Evaluate eq.(9) recursively and we get $$I_{k,p}=\sum_{i=2}^p (-1)^{p-i}\cdot \frac{p!}{i!}\cdot B_i\cdot\left[f^{(i-1)}(k+1)-f^{(i-1)}(k)\right]+(-1)^{p-1}\cdot p!\cdot I_{k,1},~~p\ge2\tag{10}$$ Not done yet, I will update it later. Not done yet, I will update it later. Not done yet, I will update it later.","['integration', 'divergent-series', 'summation', 'asymptotics', 'sequences-and-series']"
4494296,1955 Miklós Schweitzer Problem 3 - concentration inequality,"Source : 1955 Miklós Schweitzer Problem 3 Let the density function $f(x)$ of a random variable $\xi$ be an even function; let further $f(x)$ be monotonically non-increasing for $x > 0$ . Suppose that $$D^2= \int_\mathbb{R} x^2 f(x)\;dx$$ exists. Prove that for $\lambda > 0$ , $$P(\left|\xi\right| \geq \lambda D)\leq \frac{1}{1+\lambda^2}.$$ Attempt : As $f(\cdot)$ is even, $\mathbb{E}(\xi)=0$ . I apply Cantelli's inequality to obtain $$P(|\xi| \geq \lambda D)\leq \frac{2}{1+\lambda^2}.$$ Well, I guess it wasn't meant to be that easy haha... so I assume I must do something using monotonicity of $f(\cdot)$ . Alternatively, I wanted to apply some exponential tail bound - for instance ideally yielding something similar to the following: $$P(|\xi| \geq \lambda D)\leq^* e^{-g(\lambda)}\leq \frac{1}{1+\lambda^2},$$ where $\leq^*$ would follow by showing $\xi$ is sub-Gamma/Gaussian/etc. but I didn't put much thought into this approach. Question : Is there a way to just slightly change the initial lazy approach I took - i.e. maybe another concentration inequality in a clever way? Or is the approach totally different? I appreciate any help - thanks!","['contest-math', 'inequality', 'probability-theory', 'probability']"
4494312,"How to ""feel"" semidirect products","Sorry if the question makes no sense, but I've always had a problem ""feeling"" semidirect products though I understand them. Unlike direct products, I can easily point it out when I'm working with a group. This happens when some of the elements seem independent of other elements. Like when I'm adding complex numbers, I can see the imaginary part go with the imaginary part, and the real with the real. Or when multiplying, the magnitude multiplies by the magnitude, and the angle adds with the angle. But I am not able to develope an intuitive feeling to know when a group is a semidirect product of 2 subgroups. The most thing I've went to so far is to see some elements independent but interact in special cases. Like in the dihedral group, where rotations are rotations, and a reflection is a reflection unless there are 2 reflections which might affect a rotation. But other than that, pointing a semidirect product is not as obvious as direct products. Can someone help me quickly notice when a group is a semidirect product of 2 subgroups?","['permutations', 'semidirect-product', 'abstract-algebra', 'intuition', 'group-theory']"
4494346,Existence of a fixed point to a vector-valued mapping,"I have the following system: $$\begin{cases}
F_1(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{11}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{12}} = x_1 \\ 
F_2(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{21}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{22}} = x_2
\end{cases}$$ where $c_i, a_{ij}\geq 0$ and $c_1 + c_2 = a_{11}+a_{12} = a_{21} + a_{22} = 1.$ I strongly suspect the above system has a solution $(x_1,x_2)$ with $x_i>0,$ but not quite sure how to prove it. The Jacobian after taking $\ln$ from both sides is positive definite which can be seen easily. But I am not sure how that would help if we wanted to use Implicit Function Theorem, for instance. I thought about using Banach's fixed point theorem but the map $F = (F_1, F_2)$ does not seem to be a contraction since the derivative will be unbounded due to the exponents $a_{ij}\leq 1.$ I think a vector version of the Intermediate Value Theorem is probably most promising but all I can find is Poincare-Miranda Theorem , which I am not sure is immediately applicable. For what its worth we can assume the domain I am interested is: $$D = \{(x_1,x_2)\vert\,  a_{1i}x_1 + a_{2i}x_2 < c_i  \}\subseteq\mathbb{R}^2_{>0}$$ Finally, its one-dimensional version is an immediate consequence of the Intermediate Value Theorem","['fixed-point-theorems', 'real-analysis', 'multivariable-calculus', 'calculus', 'nonlinear-system']"
4494362,Method of energy functions to show boundedness of 2nd Order ODE,"I made the following system from the typical $x_1=x, x_2=x'$ transformation: $x_1'=x_2\\x_2'=\sin^2(x_1)-x_2\cos^2(x_1)$ I then used the general energy function $E(t,x_1,x_2)=\frac{1}{2}(x_2)^2+\int_{0}^{x_1}g(s)ds$ when $x''+f(x)x'+g(x)=0$ . Here, $E(t,x_1,x_2)=\frac{1}{2}(x_2)^2-\int_{0}^{x_1}\sin^2(s)ds$ And $\frac{dE}{dt}=-(x_2)^2\cos^2(x_1)≤0$ after applying the FTC, substituting, and cancelling terms. So the system is always dissipating energy, that is, for any time t, $E(t)≤E(0)$ . I want to show the boundedness of $x_2$ , that is, $|x_2(t)|≤K,$ for some constant $K$ and $t≥0$ . I get to the inequality: $\frac{1}{2}(x_2(t))^2-\int_{0}^{x_1(t)}\sin^2(s)ds≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0))$ Or if you prefer: $\frac{1}{2}(x_2(t))^2-\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0))$ . I cannot figure out how to remove the terms with $x_1(t)$ to isolate $|x_2(t)|$ while also keeping the ≤ inequality... Note: $-\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))<0$ , for all $x_1(t)$ Any help? Update: I've since shifted my efforts to first showing $x_1$ is bounded and then moving terms around in the inequality to show $x_2$ is bounded. I think this might be the only way to do it, but I'm not confident my method was correct...","['bounded-variation', 'ordinary-differential-equations', 'upper-lower-bounds']"
4494391,How is a set an event,"For problems where sets are described as events such as: In a group of 12 friends, 8 study French, 4 study french only and 2 study neither French nor Cantonese.  Let A be the event 'studies French' and let B be the event 'studies Cantonese'. To my mind, A and B are sets and the elements are people (friends).  How are they then also events?  Is it because the 12 friends comprise the ""sample space"" and the event is ""sampling""? A set is any collection of objects (mathematical or not).  An event is ""an outcome or defined collection of outcomes of a random experiment"" (well, according to Statistics.com).  I don't think it's a contentious subject so I'll quote wikipedia here for the definition of an event: ""In probability theory, an event is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned."" Where is the experiment in the initial statements?  Or am I confusing the sets defined by the text with my own e.g. Set A={a,b,c,d,e,f,g,h}
Set B={e,f,g,h,i,j}
compliment of the sets = {k,l} Perhaps the categories are the 'elements' of the sets in this instance?
Set A={any friend who studies French}
Set B={any friend who studies Cantonese}
compliment of the sets = {any friend who studies neither} I am new to this but I've heard of set builder notation.  I'm wondering how to describe ""Let A be the event 'studies French' and let B be the event 'studies Cantonese'."" in set builder in that case.",['elementary-set-theory']
4494400,"Intuitively, why does it make sense to go second in dice game to maximize chance of winning?","Here's a question from a probability book I am working through: Let's add more fun to the triplet game. Instead of fixed triplets for the two players, the new game allows both to choose their own triplets. Player $1$ chooses a triplet first and announces it; then player $2$ chooses a different triplet. The players toss the coins until one of the two triplet sequences appears. The player whose chosen triplet appears first wins the game. If both player $1$ and player $2$ are perfectly rational and both want to maximize their probability of winning, would you go first (as player $1$ )? If you go second, what is your probability of winning? There's $8$ possible triplets sequences for each player: HHH, HHT, HTH, HTT, THH, THT, TTH, TTT The players can't have the same triplet, hence there being $64 - 8 = 56$ probability outcomes to calculate for player $2$ winning. After spending half an hour tediously calculating all $56$ , it turns out that player $2$ can always choose a triplet, dependent on what player $1$ picked, as to win with probability at least ${2\over3}$ . However, I am wondering if there is an intuitive way to see that without tediously doing all $56$ computations. Or if seeing that player $2$ can always win with probability at least ${2\over3}$ is too much to ask for of an intuitive heuristic, how can we see that player $2$ can always win with probability at least ${1\over2}$ ? Edit: Since the problem statement is referring to earlier parts of the problem, I am reproducing those problem statements here as well: Part A. If you keep on tossing a fair coin, what is the expected number of tosses such hat you have $HHH$ (heads heads heads) in a row? What is the expected number of tosses to have $THH$ (tails heads heads) in a row? Part B. Keep flipping a fair coin until either $HHH$ or $THH$ occurs in the sequence. What is the probability that you get an $HHH$ subsequence before $THH$ ?","['gambling', 'markov-chains', 'dice', 'combinatorics', 'probability']"
4494402,How to find the sum of constants given the following systems of equations?,"Given that $q,r,s$ , and $t$ are different constant values in the following systems of equations containing $a,b,c$ , and $d$ . Find the sum $q+r+s+t$ . $\frac{1}{qa+1} + \frac{1}{qb+1} + \frac{1}{qc+1}+ \frac{1}{qd+1} = 1$ $\frac{1}{ra+1} + \frac{1}{rb+1} + \frac{1}{rc+1}+ \frac{1}{rd+1} = 1$ $\frac{1}{sa+1} + \frac{1}{sb+1} + \frac{1}{sc+1}+ \frac{1}{sd+1} = 1$ $\frac{1}{ta+1} + \frac{1}{tb+1} + \frac{1}{tc+1}+ \frac{1}{td+1} = 1$ I tried using a simpler version of the problem but I can't find the pattern. I am thinking that there might be a pattern that I can get from finding the sum $q+r$ from this simpler version of the problem. $\frac{1}{qa+1} + \frac{1}{qb+1} = 1$ $\frac{1}{ra+1} + \frac{1}{rb+1} = 1$ I also tried adding the four equations that leads me to factor out the sum $q+r+s+t$ but it becomes more complicated. But I noticed that the sum of the each set of denominators per equation is equal to the product of all those denominators. However, I am stuck at looking for the pattern where I can use that fact. Thanks in advance for your comments and suggestions on how to solve this particular algebra problem.","['algebra-precalculus', 'systems-of-equations', 'polynomials']"
4494416,Finding an $x$-intercept of a parabola from given information,"The parabola in the figure below has a negative intercept and a positive intercept $(a,0)$ on the $x$ -axis, and the equation of the axis of symmetry at $x = 2$ . What are the all possible values for the $x$ -intercept $(a,0)$ ? I know that the equation of a parabola opening up will have the form $(x-h)^2=4p(y-k)$ . Since the axis of symmetry is $x=2,$ then $x$ value of the vertex is $2$ . So, $h=2$ . Then $x$ -intercept $(a,0)$ it is a point in the parabola. I thought to then substitute those values into the equation, but I don't know what else to do. $(x-h)^2=4p(y-k)$ $(a-2)^2=4p(0-k)$ I have like three incognita there. I don't think I can assume that the focus is the point $(2,0)$ so I can find $p.$ . I'm a bit lost here; will appreciate any help.","['algebra-precalculus', 'conic-sections']"
4494439,"Find the number of permutations of the word ""AUROBIND"" in which vowels appear in alphabetical order?",I thought of 2 ways to solve this Firstly put all vowels in alphabetical order A_I_O_U and for the remaining $5$ gaps each consonant has $5$ choices to go to. So total choices becomes $5^4 \cdot 4!$ . Firstly put all vowels in alphabetical order A_I_O_U and for the remaining 5 gaps we need to solve the equation $g_1+g_2+g_3+g_4+g_5=4$ . And the number of solutions to this equation is the answer. That is $8C4 \cdot 4!$ . I don't know which one is correct and why other one is wrong.,"['permutations', 'combinatorics', 'discrete-mathematics']"
4494459,"Probability of not playing an opponent in a contest with $2^n$ players, from Ross Introduction to Probability","In a certain contest, the players are of equal skill
and the probability is $\frac{1}{2}$ that a specified one of
the two contestants will be the victor. In a group
of $2^n$ players, the players are paired off against
each other at random. The $2^{n-1}$ winners are again
paired off randomly, and so on, until a single winner
remains. Consider two specified contestants, $A$ and $B$ , and define the events $A_i$ , $i\leq n$ , $E$ by $A_i$ : $A$ plays in exactly $i$ contests: $E$ : $A$ and $B$ never play each other. We have to calculate the probability of $P(A_i)$ and $P(E)$ . Though an elegantly simple approach has been stated here https://math.stackexchange.com/a/2481789/496972 but I wanted to know the flaw in my reasoning My attempt $P(A_i)$ is pretty simple. After every elimination round $2^{n-i}$ contestants remain $i=\{0,1,2..n\}$ . By symmetry every contestant has an equal chance to make to the $i^{th}$ round. Hence, $$
P(A_i)= \dfrac{2^{n-i}}{2^n}= \left(\frac{1}{2} \right)^i
$$ If $A$ is eliminated after $i^{th}$ round, he has played  against $i+1$ players. Then the probability of not playing $B$ is same as choosing $i+1$ players out of $2^n- 2$ players. Then, $$
\begin {align*}
P(E)&= \sum_{i=0}^{n-1} P(E|A_i)P(A_i)\tag{because $A_i$'s are disjoint}\\
&= \sum_{i=0}^{n-1} \dfrac{\binom{2^n-2}{i+1} }{\binom{2^n-1}{i+1}}\times \left(\frac{1}{2} \right)^i\\
&=\sum_{i=0}^{n-1} \dfrac{2^n-2-i}{2^n-1} \times \left(\frac{1}{2} \right)^i\\
&=\dfrac{2^n-2}{2^n-1}\sum_{i=0}^{n-1}\left (\dfrac{1}{2} \right)^i-\frac{1}{2(2^n-1)}\sum_{i=1}^{n-1}i \left(\frac{1}{2} \right)^{i-1}
\end{align*}
$$ When I simplify this expression, it is not remotely close to $1- \dfrac{1}{2^{n-1}}$ which is the answer. Where am I going wrong?","['discrete-mathematics', 'probability']"
4494472,What is the expectation of the exponential distribution multiplied by indicator function?,"I am reading research paper [A New Bayesian Lasso] , where $u$ has the distribution The expectation of $u_j$ is given by $$\frac{1}{\lambda}+|\beta_j|$$ . I know that the term $\frac{1}{\lambda}$ is the expectation of the exponential distribution but where did the term $|\beta_j|$ come from? I have tried to calculate $$\int _{|\beta_j|}^\infty u_j \lambda \text{Exp}(-\lambda u_j)du_j=\text{Exp}(-\lambda |\beta_j| )\left(\frac{1}{\lambda}+|\beta_j|\right)$$ which gives $\frac{1}{\lambda}+|\beta_j|$ multiplied by $\text{Exp}(-\lambda |\beta_j| )$ .","['expected-value', 'statistics', 'probability']"
4494505,Minimize $\mathrm{tr}(R^{-1}B^TXB)$ over $R$ subject to $X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA$,"Given $A\in\mathbb{R}^{n\times n}$ and $B\in\mathbb{R}^{n\times 2}$ , I am interested in solving the following problem: \begin{array}{ll} \underset{R \in \mathbb{R}^{2\times 2}}{\text{minimize}} & \mathrm{tr} \left( R^{-1}B^T X B \right)\\ \text{subject to} & X=A^TXA-A^TXB(R+B^TXB)^{-1}B^TXA.\end{array} Here $X$ is unique stabilizing solution to DARE , thus X is positive definite. It is required that $R>0$ (i.e. positive definite) and $B^TXB$ to be full rank. EDIT: For a fixed $A,B,R$ , we can get unique $X$ by solving DARE, for example by using matlab ""idare"" or ""dare"" command. However, here $R$ is not fixed, it is a variable, thus for each $R$ , there is corresponding $X$ . My attempt: I wanted to start with simpler case when we put additional constraints on $R$ . Assume that $R$ is diagonal and positive definite. WLOG we can assume that $R=\mathrm{diag}\{r_1,r_2\}$ , such  that $r_1+r_2=1$ and $1>r_i>0$ for $i=1,2.$ A=[3 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2];
B=rand(4,2);
Q=zeros(4,4);    
r1=linspace(0.01, 0.99);
for i=1:100  
    R=[r1(i) 0; 0 1-r1(i)];
    [X,~,~] = idare(A,B,Q,R);
    T(i)=trace(inv(R)*B'*X*B);
end 
plot(T) It looks like that as we increase $r_1$ from $0$ to $1$ , then $\mathrm{tr} \left( R^{-1}B^T X B \right)$ is continuous, moreover, it is convex. However, I am unable to prove it.","['convex-optimization', 'optimization', 'trace', 'linear-algebra']"
4494591,How do I handle equations like $f(x)-x = f'(x)$?,"How do I handle equations like $f(x)-x = f'(x)$ ? Integrating both sides gets me nowhere... I have been trying to guess an example, but with no avail. I cannot think of any other approaches ... Edit: ok i noticed $f(x) =x+1$ ... can someone guide be to how i can solve for the other solutions?","['calculus', 'ordinary-differential-equations']"
4494597,The closure of certain subspace of $\ell_\infty$,"What is the  closure of the following subspace $A$ of $\ell_\infty$ with the standard sup norm of $\ell_\infty$ : $$A=\{(a_n)\in \ell_\infty\mid (A_n)=a_1+a_2+\ldots+a_n\; \text{belongs to} \;\ell_\infty \}$$ Is it true to say that $\bar{A}$ is the space of  all $(a_n)\in \ell_\infty$ such that the  Cesaro sum $\frac{a_1+a_2 +\ldots +a_n}{n}$ goes to zero? If it is the case, what is a proof?","['sequences-and-series', 'functional-analysis', 'cesaro-summable', 'real-analysis']"
4494618,"Is there always a subsequence, whose intersection has a positive measure?","I have the following problem: A number $0<C<1$ is given. On the interval $[0,1]$ there is a sequence of
compacts with measures at least $C$ . Is it true that there is always a
subsequence, whose intersection has a positive measure? I took some examples: $$a_n = \left[\frac{1}{2n+1}, \frac{1}{2n+1}+\frac{1}{2}\right]$$ The measure here is Lebesgue measure, that means: $\mu([a, b]) = b-a$ , if $b \geq a$ $\mu([a, b]) = 0$ , if $b \leq a$ Here $C = \frac{1}{2}$ . But it seems here the intersection of each subsequence has the positive measure. How can I solve this problem?","['functional-analysis', 'real-analysis']"
4494634,Roll a pair of 6-sided dice. $S=$ total score. What is the expected number of pair-rolls to get the same value of $S$ consecutively?,"Roll a pair of unbiased 6-sided dice. Let $S=$ total score. So the possible values of $S$ are 2, 3, 4, ..., 12. Let $X$ = number of pair-rolls up to and including the first time we get the same value of $S$ consecutively. For example, if the values of $S$ are {9, 7, 12, 3, 8, 5, 5} then $X=7$ . Question: What is the value of $E(X)$ ? What makes this question difficult for me is that the values of $S$ have different probabilities, so I cannot directly use a geometric distribution. And I have not been able to find a useful recurrence relation. I made a simulation on excel with 1000 trials, and $E(X)$ seems to be approximately $10$ . (Context: No special context; I've been looking at various questions involving dice n MSE, and I came up with this question.)","['expected-value', 'statistics', 'dice']"
4494637,How to find subgroups of a product group containing the diagonal?,"Given a non-abelian group $G,$ I am curious about subgroups of $G\times G$ containing the diagonal $\{(a, a)\mid a\in G\}.$ As a starting point, I thought of looking at the simplest examples $S_3$ and $D_8$ . Direct computations quickly become tedious as the product group is a bit large. Also, I think Goursat's lemma could be helpful, but don't know how to use it for an explicit calculation. Hope someone with more experience in group theory can help me to find them for $S_3$ (and if possible $D_8$ and $Q_8$ ). At least if you can give me a list of all such groups and a hint to find them, that would be great.","['direct-product', 'finite-groups', 'group-theory', 'symmetric-groups', 'abelian-groups']"
4494669,roll until lose game with changing probability,"Start with probability p $= 1$ Keep rolling until you get a failure. Modify p after each roll, multiplying it by k Count the number of successes. event_count = 0
    p = 1
    while random() < p:
        event_count += 1
        p *= k $E$ = expected value of event_count at the end I'm looking for a formula for k in terms of $E$ . (This would be the inversion of the function for $E$ , given k .) random() gives a continuous uniform distribution $[0, 1)$ The probability p changes at each iteration (multiplied by k ). event_count will always be at least $1$ , since p starts at $1$ , so the first random roll will always be a success. If k $>= 1$ , event_count is infinity. If k $<= 0$ , event_count is $1$ . So the interesting values of k are $(0, 1)$ Some samples give these numbers: E: k
            1.0: 0.0,
            1.01: 0.01,
            1.1: 0.099,
            1.25: 0.238,
            1.5: 0.42,
            2.0: 0.645,
            3.0: 0.833,
            4.0: 0.904,
            5.0: 0.938,
            6.0: 0.957,
            7.0: 0.9685, Most of these digits are significant. I found $0.968$ to be pretty reliably giving $E < 7$ and $0.969$ to reliably give $E > 7$ What's the formula to relate $E$ and k ?","['expected-value', 'probability']"
4494717,determine when all cyclic shifts are distinct,"Prove that if $a,b$ are coprime positive integers, then for any path from $(0,0)$ to $(b,a)$ where each step involves going up or to the right one unit, then all cyclic shifts of $P$ are distinct. For instance, if $R$ denotes a right step and $U$ denotes an up step and if $a=2,b=3,$ then the cyclic shifts of $RU$ are $UR$ and $RU$ . Prove or disprove the converse (i.e. if for any path from $(0,0)$ to $(b,a)$ where each step involves going up or to the right one unit, all cyclic shifts of $P$ are distinct, $a$ and $b$ are coprime). I think distinct cyclic shifts can only be equal if in some sense, they're ""multiples of each other."" That is, if there's two repetitions of the same pattern. So if there are two distinct cyclic shifts that are identical, one should be able to prove that $a$ and $b$ have a common factor. I think one can prove that if a string has only two letters and each occurs an odd number of times, then no distinct cyclic shifts of the string are the same (distinct as in the shifts involve different indices of the string). To prove this, it would probably be useful to induct on the number of occurrences of the letter with the fewest occurrences, with the base case being one occurrence. The inductive step would involve listing a cyclic shift as $s_i s_{i+1}\cdots s_{i+n-1}$ and another as $s_j s_{j+1}\cdots s_{j+n-1}$ such that the two are equal. Here we're assuming we have a string of length n and that we're using cyclic indices (so $s_{i+n} = s_i\,\forall i$ ). There is some $1\leq k < n$ with $s_{i+k} = s_j = s_{j+k}$ . Replacing $s_{j}$ and $s_i$ with the other character however doesn't seem to help much. But I'm stuck as to what to do next as $j+k$ might be a different index from $i$ . I think the converse might also hold, and to show it, it might be useful to prove the contrapositive. Let $d = \gcd(a,b).$ Construct a path consisting of $\frac{a}d$ R steps followed by $\frac{b}d $ U steps. Repeat this path $d > 1$ times. Then note that cyclically shifting the path by $\frac{a}d$ indices always yields the same path, because the letters representing the path are periodic with period $\frac{a+b}d$ (more formally if $D_i$ represents the ith direction of the path, then using cyclic indices, $D_{i+(a+b)/d} = D_i$ ).","['elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4494776,Why is the Spectral Density of a stationary sequence real?,"I am trying to understand (and prove) why the spectral density $\Phi_s \in L^1([- \pi, \pi])$ of a stationary sequence $s = \{s_n\}_{n \in \mathbb{Z}}$ is real . I wanted to argue via the auto-correlation function but got stuck somewhere in between. I know that the auto-correlation in this case is defined as $R_s(n) = \langle s_n, s_0 \rangle = \frac{1}{2 \pi} \int_{- \pi}^{\pi} \Phi_s(\theta) \cdot e^{- i n \theta} d \theta$ and must be non-zero and real-valued since the inner product is given this way. Further, I know that the spectral density (or any function $\Phi \in L^1$ ) is uniquely determined by its Fourier Coefficients. But how do I use this information to show that $\Phi_s$ is real-valued? Maybe the solution is very obvious and I just cannot seem to see it! Thanks in advance! :-)","['fourier-analysis', 'stationary-processes', 'fourier-transform', 'signal-processing', 'probability-theory']"
4494779,What is the number of different sums of the items of a set of consecutive natural numbers?,"I have been playing around with sets of consecutive natural numbers like say $S=\{1,2,3,...,10\}$ and I have come up with this problem for which however I have not yet found an answer and I don't know if there is one. My problem is the following: Considering a set $S=\{1,2,3,...,n\}$ where $n$ is an arbitrary natural number, can we derive a formula that will indicate the number of different sums that will be produced by summing the items of the set $S$ in all different combinations? For example for the set $S=\{1,2,3,4,5\}$ we get: $$1+2=3\\
1+2+3=6\\
1+2+3+4=10\\1+3+4+5=13\\1+2+4+5=12\\1+2+3+4+5=15\\
1+2+3+5=11\\1+4+5=10\\1+3+5=9\\1+3+4=8\\1+2+5=8\\1+2+4=7\\1+3=4\\1+4=5\\1+5=6\\
2+3=5\\2+4=6\\2+5=7\\
2+3+4=9\\2+3+5=10\\2+4+5=11\\
2+3+4+5=14\\
3+4=7\\3+5=8\\
3+4+5=12\\
4+5=9\\$$ So we have $26$ different summations and $13$ different sums(I hope I didn't make any mistake)","['number-theory', 'combinatorics', 'elementary-number-theory']"
4494783,"$p^k\mid |G|$ for $k=1,\dots,\alpha$, and $n_k=\#$ of subgroups of order $p^k$. Is $n_k\equiv 1\pmod p$ for every $k=1,\dots,\alpha$?","Let $p$ be a prime divisor of the order of a finite group $G$ , and $\alpha$ the greatest power of $p$ dividing $|G|$ . Let $n_k$ be the number of subgroups of $G$ of order $p^k$ (so, $n_\alpha$ is the number of $p$ -Sylow subgroups of $G$ , usually referred to as "" $n_p$ ""). The following three facts are compatible with the subsequent (conjectured) claim: $n_\alpha\equiv 1\pmod p$ , by Sylow III; $n_k\ge 1$ , for every $k=1,\dots,\alpha$ : this is, e.g. , the Theorem 2.12.1 in Herstein's Topics in algebra ; $n_1\equiv 1\pmod p$ : this is a corollary of this result : in fact, in the notation of the link, $p\mid |X|=n_1(p-1)+1=n_1p-(n_1-1)$ if and only if $p\mid n_1-1$ if and only if $n_1\equiv 1\pmod p$ . Claim . $n_k\equiv 1\pmod p$ for every $k=1,\dots,\alpha$ . According to this and this , the claim holds true for the particular cases $|G|=2^33$ and $|G|=2^43$ , respectively. Q : Is the claim true?","['group-theory', 'sylow-theory', 'finite-groups']"
4494795,If the gradient of two functions are related by a matrix can we say anything about these two functions?,"Say we have two functions $f, g: \mathbb{R}^n \rightarrow \mathbb{R}$ and we have a matrix $M \in \mathbb{R}^{n \times n}$ which is constant, i.e. not a function of $x$ . Say we know that $\nabla f = M \nabla g$ , then can we say anything about the relationship between $f$ and $g$ ? What about in the case that $M$ is a function of $x$ ? I have not been able to discover any relation between $f$ and $g$ in either case, but I feel like there should be something. I know that we can apply the chain rule in the case where we have $\nabla f = \lambda(g(x)) \nabla g$ and $\lambda: \mathbb{R} \rightarrow \mathbb{R}$","['multivariable-calculus', 'scalar-fields']"
4494804,A weighted count of Egyptian fraction representations,"Given a positive rational $\alpha$ and a natural number $k$ , let $N_k(\alpha)$ be the number of Egyptian fraction representations of $\alpha$ with smallest element ${1\over k}$ (see e.g. the survey ""Paul Erdős and Egyptian Fractions"" by Graham) . For $s>0$ let $$S_s(\alpha):=\sum_{k\in\mathbb{N}}{N_k(\alpha)\over k^s}$$ and let $$t(\alpha)=\inf\{s: S_s(\alpha)<\infty\}.$$ My first question is what the above ""threshold"" value is, in at least one particular natural case: Q1 : What is $t(1)$ ? Or, failing that, can we compute any concrete examples of $t(\alpha)$ ? Tentatively, I suspect that $t(\alpha)=1$ for every positive rational $\alpha$ , but I don't see how to prove that. Basically, I need an appropriate bound on the number of Egyptian fraction representations of a given rational with a given smallest term, but I don't see how to get a bound which is good enough for this question. My second question is about what happens, specifically, at $s=2$ : Q2 : What is $S_2(1)$ ?","['number-theory', 'recreational-mathematics', 'egyptian-fractions', 'sequences-and-series']"
4494806,Painting the board by selecting $2\times 2$ type squares from the $m\times n$ type board,"Problem: Initially we have an $m\times n$ board with all unit squares white. As a result, we want to paint any two squares with a common edge, one black and one white. (That is, we want to get it like a chessboard pattern after the final step.)
In each step of the painting process, a $2\times 2$ square is selected on the board and its white unit squares are painted black and its black unit squares painted white. For which $(m, n)$ pairs can the board be painted as desired? Note: I solved some parts of the problem. I'm adding them to the post. Remaining part of the problem is at the bottom of the page. Thanks. My attempts and some parts about the solution: For $4\mid m$ and $4 \mid n$ , it is possible. Firstly, let's take $m=4, n=4$ . $
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \text{ } & \text{ } & \text{ } \\ \hline
 \text{ } & \text{ } & \text{ } & \text{ } \\ \hline
 \text{ } & \text{ } & \text{ } & \text{ } \\ \hline
 \text{ } & \text{ } & \text{ } & \text{ } \\ \hline
\end{array}
\to 
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \blacksquare & \text{ } & \text{ } \\ \hline
 \blacksquare & \blacksquare & \text{ } & \text{ } \\ \hline
 \text{ } & \text{ } & \blacksquare & \blacksquare \\ \hline
 \text{ } & \text{ } & \blacksquare & \blacksquare \\ \hline
\end{array}
\to
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \text{ } & \blacksquare & \text{ } \\ \hline
 \blacksquare& \text{ } & \blacksquare & \text{ } \\ \hline
 \text{ } & \blacksquare & \text{ } & \blacksquare \\ \hline
 \text{ } & \blacksquare & \text{ } & \blacksquare \\ \hline
\end{array}
\to
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \text{ } & \blacksquare & \text{ } \\ \hline
 \text{ } & \blacksquare & \text{ } & \blacksquare \\ \hline
 \blacksquare & \text{ } & \blacksquare & \text{ } \\ \hline
 \text{ } & \blacksquare & \text{ } & \blacksquare \\ \hline
\end{array}
$ That is, a $4 \times 4$ board can be paint as desired. Easily, we expand that, if $4\mid m$ and $4\mid n$ then a $m \times n$ board can be paint as desired. If $m=2$ or $n=2$ , $m \times n$ board can not be paint as desired. Let $m=2$ . \begin{array}{|c|c|c|c|c|}
\hline
 \text{x} & \text{ } & \text{ } & \text{ }& \text{ } & \text{ } \\ \hline
 \text{x} & \text{ } & \text{ } & \text{ }& \text{ } & \text{ } \\ \hline
\end{array} Let's examine the first column of the $2\times n$ table (Cells with x in them). Since the colors of these cells will always be the same, a desired format cannot be done. If $m$ or $n$ is an odd number, a painting in the desired format can not be done. Suppose that $m \geq 3$ odd number. Let's examine difference of number of white squares and number of black squares in the first two columns. Let $w, b$ be number of white squares and number of black squares in the first two columns. Difference of they is $D = w- b$ . $
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \text{ }  \\ \hline
 \text{ } & \text{ }  \\ \hline
 \text{ } & \text{ }  \\ \hline
 \text{ } & \text{ }  \\ \hline
\text{ } & \text{ }  \\ \hline
\end{array}
$ Initially $D = 2m - 0 = 2m$ . $
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \blacksquare  \\ \hline
 \blacksquare & \text{ }  \\ \hline
 \text{ } & \blacksquare  \\ \hline
 \blacksquare & \text{ }  \\ \hline
\text{ } & \blacksquare  \\ \hline
\end{array}
$ After the final step $D= m - m = 0$ . We will prove that $D$ is an invariant in $\mod 4$ . Because, $
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \text{ }  \\ \hline
 \text{ } & \text{ }  \\ \hline
\end{array}
\to
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \blacksquare   \\ \hline
 \blacksquare  & \blacksquare   \\ \hline
\end{array}
$ After the step, $D$ will decrease $8$ . $
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \text{ }  \\ \hline
 \text{ } & \text{ }  \\ \hline
\end{array}
\to
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \blacksquare   \\ \hline
 \blacksquare  & \blacksquare   \\ \hline
\end{array}
$ After the step, $D$ will decrease $4$ . $
\begin{array}{|c|c|c|c|c|}
\hline
 \blacksquare & \blacksquare  \\ \hline
 \text{ } & \text{ }  \\ \hline
\end{array}
\to
\begin{array}{|c|c|c|c|c|}
\hline
 \text{ } & \text{ }   \\ \hline
 \blacksquare  & \blacksquare   \\ \hline
\end{array}
$ After the step, $D$ will same...etc. Thus, $D$ is an invariant in $\mod 4$ .
When $m\geq 3$ odd number, initially $D = 2m \equiv 2 \pmod{4}$ and after the final step $D\equiv 0 \pmod{4}$ . Since $2\not\equiv 0 \pmod{4}$ , there is a contradiction. That is, when $m$ or $n$ is an odd number, a painting in the desired format can not be done. Remaining Problem: Let $m,n>2$ even numbers. (a) If $m\equiv 2 \pmod{4}$ and $n\equiv 2 \pmod{4}$ , can we paint in the desired format? (b) If $m\equiv 2 \pmod{4}$ and $n\equiv 0 \pmod{4}$ , can we paint in the desired format?",['combinatorics']
4494846,"What does the dual parallel transport, or conjugate affine connection, actually do?","I'm reading both Frank Nielsen's An Elementary Introduction to Information Geometry as well as Amari's Information geometry and its applications , and confused by something they use (I'll focus on AEITIG here). He first introduces the concept of an affine connection $\nabla$ , which is basically a covariant derivative on vector fields, so if we have vector fields $X, Y$ , it tells us how to calculate $\nabla_X Y$ , i.e., how $Y$ changes as we move in the $X$ direction, at each point. If we have the basis $(\partial_i) = (\frac{\partial}{\partial x_i})$ for a smooth chart $(U, x)$ , then exactly what the connection does is defined in terms of the Christoffel symbols: $\nabla_{\partial_i} \partial_j = \Gamma_{ij}^k \partial_k$ This all makes sense to me. Then, from what I understand, if we want to transport a vector from the tangent plane at one point to another, we could do some sort of integration of that derivative over the path taken, and that would give the final vector. He then introduces (section 3) the concept of a conjugate connection $\nabla^\ast$ that's conjugate to a given connection $\nabla$ . He mentions a few properties of it, like an identity that must be satisfied for it to be conjugate, but never what it actually does or how to figure it out. Here's that identity: What I don't understand is the following: What does the conjugate connection actually do to a vector ? I.e., for the regular connection, it's clear that it's saying how it's changing the basis vectors. Given a connection $\nabla$ , how does one actually find the conjugate $\nabla^\ast$ ? I have a couple guesses but they're only that. My guess for (1) is that if the primal connection tells us how the basis vectors change, $\nabla_{\partial_i} \partial_j$ , then maybe the conjugate one tells us how the reciprocal basis vectors $\partial^i$ change, i.e., $\nabla_{\partial^i} \partial^j$ . This is somewhat complicated by the fact that (like I asked about in my previous question ) his dual basis isn't the covector dual basis $dx_i$ I'm used to from earlier differential geometry, it's the dual (covector) basis with the ""sharp"" operator applied to it, making it a vector again, such that we have two ways of expressing the same vector . For (2), I'm even less sure, but he notes that if we have a pair of conjugate connections $\nabla, \nabla^\ast$ that are coupled with the metric connection $g$ , then the mean connection must equal the Levi-Civita connection, so $\nabla^{LC} = \overline \nabla = \frac{\nabla + \nabla^\ast}{2}$ Since we can figure out the Christoffel symbols of the LC connection from the metric $g$ , it seems like given the symbols defining $\nabla$ , we could figure out an explicit form for $\nabla^\ast$ . Is this right, or is there an easier way?","['information-geometry', 'differential-geometry']"
4494907,"Can you always decompose an $f: [0,\infty) \to \mathbb{R}$ into the sum of a monotonic function and a periodic function?","Is there a nice class of functions, like smooth or smooth and bounded, such that you can always decompose an $f: [0,\infty) \to \mathbb{R}$ into the sum of a monotonic function and a periodic function? It seems true to me but I can't find a reference. An equivalent question is if you can decompose a sufficiently nice odd $g: \mathbb{R} \to \mathbb{R}$ into the sum of a monotonic function and a periodic function.","['periodic-functions', 'functions', 'monotone-functions', 'real-analysis']"
4494943,Converse of Stone-Weierstrass Theorem for Complex Continuous Functions,"Stone-Weierstrass Theorem for complex continuous functions says: Let $K$ be a compact Hausdorff space and $\mathcal{A} \subseteq C(K, \mathbb{C})$ be a subalgebra. If $\mathcal{A}$ separates points and is closed under conjugation, then $\mathcal{A}$ is dense in $C(K, \mathbb{C})$ . I am trying to figure out whether the converse holds, at least in case $K$ is a metric space. It is obvious that $\mathcal{A}$ separates points if $\mathcal{A}$ is dense in $C(K, \mathbb{C})$ , but it seems difficult to prove or disprove that $\mathcal{A}$ is closed under conjugation if $\mathcal{A}$ is dense. Is there any proof or counterexample for this?","['complex-analysis', 'weierstrass-approximation', 'real-analysis']"
4494952,"Spivak, Ch. 15, Trigonometric Functions"", Proof of $\arcsin{\alpha}+\arcsin{\beta}=\arcsin{(\alpha \sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2})}$.","The following problem is from Chapter 15 ""Trigonometric Functions"" from Spivak's Calculus Prove that $$\arcsin{\alpha}+\arcsin{\beta}=\arcsin{(\alpha
 \sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2})}$$ indicating any restrictions on $\alpha$ and $\beta$ . My question is about the restrictions on $\alpha$ and $\beta$ . I will show the solution from the solution manual first, and then specify my question. Here is the solution manual solution From the addition formula for $\sin$ we obtain, for $|\alpha|\leq 1$ and $|\beta|\leq 1$ , $$\sin{(\arcsin{\alpha}+\arcsin{\beta})}=\sin{(\arcsin{\alpha})\cos{(\arcsin{\beta})}}+\cos{(\arcsin{\alpha})\sin{(\arcsin{\beta})}}$$ $$=\alpha\sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2}$$ Note that though it is not mentioned a significant step is taken in showing that $$\cos{(\arcsin{x})}=\sqrt{1-x^2}$$ This is done by computing the derivative of $\sin(x)$ as the reciprocal of the derivative of $\arcsin$ at $\sin{x}$ . Consequently $$\arcsin{\alpha}+\arcsin{\beta}=\arcsin{(\alpha\sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2})}\tag{1}$$ provided that $-\pi/2\leq\arcsin{\alpha}+\arcsin{\beta}\leq\pi/2$ . [If $\pi/2<\arcsin{\alpha}+\arcsin{\beta}\leq \pi$ , the right side must be
replaced with $\pi-\arcsin{(\alpha\sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2})}$ , and if $-\pi\leq \arcsin{\alpha}+\arcsin{\beta}\leq -\pi/2$ , replaced with $-\pi-\arcsin{(\alpha\sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2})}$ .] My question is about the last paragraph. Let's start at the point where we have $$\sin{(\arcsin{\alpha}+\arcsin{\beta})}=\alpha\sqrt{1-\beta^2}+\beta\sqrt{1-\alpha^2}$$ and we want to take the $\arcsin$ of each side to obtain $(1)$ . Now, $\arcsin{\alpha}$ and $\arcsin{\beta}$ are each in $(-\pi/2, \pi/2)$ . Their sum is in $(-\pi, \pi)$ , and the whole left expression is thus in $[-1,1]$ . $\pm 1$ occur when $\arcsin{\alpha}+\arcsin{\beta}=\pm \frac{\pi}{2}$ . If one of these two cases occurs, then $\arcsin$ isn't defined for $\sin{(\arcsin{\alpha}+\arcsin{\beta})}=\pm 1$ . EDIT: the sentence above is is incorrect. I confused $\arcsin$ with its derivative $\arcsin'$ . So my first question is about the following snippet of the solution manual solution provided that $-\pi/2\leq\arcsin{\alpha}+\arcsin{\beta}\leq\pi/2$ where the values $\pi/2$ and $-\pi/2$ are included in the possibilities. Are these values really allowed? EDIT: given the first edit above, yes, these values are allowed. My second question is about the discussion when $\pi/2<\arcsin{\alpha}+\arcsin{\beta}\leq \pi$ or $-\pi\leq \arcsin{\alpha}+\arcsin{\beta}\leq -\pi/2$ . What is happening in these cases, I don't know what Spivak's solution manual did there.","['integration', 'proof-explanation', 'inverse-function', 'calculus', 'derivatives']"
4494955,"If $X$ is an integrable random variable, then $\left|\int_{A}X\mathrm{d}\mathbb{P}\right| \leq \int_{A}|X|\mathrm{d}\mathbb{P}$","Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. If $X$ is an integrable random variable, then \begin{align*}
\left|\int_{A}X\mathrm{d}\mathbb{P}\right| \leq \int_{A}|X|\mathrm{d}\mathbb{P}
\end{align*} Here is my attempt: \begin{align*}
\left|\int_{A}X\mathrm{d}\mathbb{P}\right| & = \left|\int_{A}(X^{+} - X^{-})\mathrm{d}\mathbb{P}\right|\\\\
& = \left|\int_{A}X^{+}\mathrm{d}\mathbb{P} - \int_{A}X^{-}\mathrm{d}\mathbb{P}\right|\\\\
& \leq \int_{A}X^{+}\mathrm{d}\mathbb{P} + \int_{A}X^{-}\mathrm{d}\mathbb{P}\\\\
& = \int_{A}(X^{+} + X^{-})\mathrm{d}\mathbb{P}\\\\
& = \int_{A}|X|\mathrm{d}\mathbb{P}
\end{align*} Can someone criticize my solution?","['expected-value', 'solution-verification', 'probability-theory', 'random-variables']"
4494961,If $\mu(B_n) \to 1$ then $\int_{B_n} f \mathrm d \mu \to \int_{X} f \mathrm d \mu$,"I'm trying to prove this convergence result. Could you please have a check on my attempt? Let $(X, \mu)$ be a probability space and $f:X \to \mathbb R$ such that $f \in L_1 (\mu)$ . Assume there is a sequence $(B_n)$ of measurable sets such that $\mu(B_n) \to 1$ . Then $$
\int_{B_n} f \mathrm d \mu \to \int_{X} f \mathrm d \mu.
$$ Proof: Because $(1_{B_n} f)_n$ does not necessarily converges to $f$ $\mu$ -a.e., we are unable to apply dominated convergence theorem . It suffices to prove that $$
\int_{B^c_n} f \mathrm d \mu \to 0.
$$ We need the following lemma whose proof is given at the end. Lemma: For each $\varepsilon>0$ , there is $\delta >0$ such that if $\mu (B) < \delta$ then $\int_{B} |f| \mathrm d \mu < \varepsilon$ . Given $\varepsilon >0$ , there is $\delta>0$ such that $\int_{B} |f| \mathrm d \mu < \varepsilon$ for all $\mu(B) <\delta$ by our Lemma . Given $\delta >0$ , there is $N >0$ such that $\mu(B_n^c) < \delta$ for all $n>N$ . It follows that for each $\varepsilon>0$ there is $N>0$ such that $\int_{B^c_n} |f| \mathrm d \mu <\varepsilon$ for all $n>N$ . Notice that $|\int_{B^c_n} f \mathrm d \mu| \le \int_{B^c_n} |f| \mathrm d \mu$ . This completes the proof. Proof of the lemma: We have $f \in L_1 (\mu)$ implies $|f| \in L_1 (\mu)$ , so there is a simple function $0 \le g \le |f|$ such that $$
\int_X (|f| -g) \mathrm d \mu < \frac{\varepsilon}{2}.
$$ We assume $g$ has a form $$
g = \sum_{i=1}^m a_i 1_{A_i}
$$ where $a_i \ge 0$ for $i = 1, \ldots, m$ and $(A_i)_{i=1}^m$ is pairwise disjoint sequence of measurable sets. We have $$
\int_B g \mathrm d \mu = \sum_{i=1}^m a_i \mu (A_i \cap B)  \le \mu(B) \sum_{i=1}^m a_i.
$$ If $\mu(B) \le \dfrac{\varepsilon}{2 \sum_{i=1}^m a_i}$ , then $$
\begin{align}
\int_B |f| \mathrm d \mu &= \int_B (|f| - g) \mathrm d \mu +\int_B g \mathrm d \mu \\
&\le \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
&= \varepsilon.
\end{align}
$$",['measure-theory']
4495022,Countable vs Uncountable Intersection,"I have a specific question regarding the difference between countable and uncountable intersection. Specifically, let $B(x,r)$ be the open ball centered at $x\in \mathbb{C}$ of radius $r>0$ . It is clear that $$\bigcap_{\theta\in \mathbb{R}}B(e^{2\pi i\theta},1.5)=B(0,0.5).$$ However, does $$\bigcap_{\theta\in \mathbb{Q}}B(e^{2\pi i\theta},1.5)=B(0,0.5)?$$",['complex-analysis']
4495044,Proof of a limit of a function. Is it correct?,"Edit: There is an answer at the bottom by me explaining what is going on in this post. Define a function $f : R \to R$ by $f(x) = 1$ if $x = 0$ and $f(x) = 0$ if $x \ne 0$ . I was attempting to prove that $\lim_{x \to 0; x\in R}f(x)$ is undefined. The following is my proof. Proof: Suppose that $\lim_{x\to 0; x \in R}f(x)=L$ . Then for every $\varepsilon > 0$ there exists a $\delta > 0$ such that for all those $x \in R$ for which $|x-0|<\delta$ we have that $|f(x)-L|<\varepsilon$ . But $|0| < \delta$ and by the Archimedean property we know that for $\delta > 0$ there exists an integer $n>0$ such that $0<|\frac 1 n| < \delta$ . This is a contradiction, as $f(0) = 1$ and $f(\frac 1 n)= 0$ and both are less than $\delta$ . Is the proof correct? Edit: Here a limit is defined using adherent points and not limit points. If we were to use limit points then, $\lim_{x \to 0; x\in R\setminus \{0\}}f(x)=0$ . I have updated the question with correct notation. Edit 2: Most textbooks define limits using limit points. In which case you we would have that $\lim_{x \to 0}f(x)=\lim_{x \to 0; x\in R\setminus \{0\}}f(x)$ . We are considering the definition of the limit where limits are defined using adherent points. Where it really matters whether we are considering $lim_{x\to 0; x\in R\setminus \{0\}}f(x)$ or $lim_{x\to 0; x\in R}f(x)$ . Edit 3: This is the definition of convergence of a function at a point in the book Analysis 1 by Terence Tao. Let $X$ be a
subset of $R$ , let $f : X → R$ be a function, let $E$ be a subset of $X$ , $x_0$ be an adherent point of $E$ , and let L be a real number. We say that f
converges to $L$ at $x0$ in $E$ , and write $\lim_{x \to x_0;x\in E} f(x) = L$ , iff $f$ , after
restricting to $E$ , is ε-close to $L$ near $x_0$ for every $\varepsilon > 0$ . If $f$ does not
converge to any number $L$ at $x_0$ , we say that $f$ diverges at $x0$ , and leave $\lim_{x\to x_0;x\in E} f(x)$ undefined.
In other words, we have $lim_{x\to x_0;x\in E} f(x) = L$ iff for every $\varepsilon > 0$ ,
there exists a $\delta > 0$ such that $|f(x) − L| ≤ ε$ for all $x \in E$ such that $|x − x0| < \delta$ . There are a two other things he defines that are used in this definition. That of $\varepsilon$ closeness and local $\varepsilon$ closeness. For those wanting to read those, Here is the link . It is on page 221. Edit 4: It might be useless defining limits without limit points. But that is the definition for which I am trying to prove this.","['limits', 'solution-verification', 'analysis', 'real-analysis']"
4495086,Fourier transform is continuous using $\epsilon$-$\delta$ definition of continuity,"Let $f\in L^1(\mathbb{R}^n)$ . Then the Fourier transform $\hat{f}$ is given by $$\hat{f}(\xi):=\int_{\mathbb{R}^n} e^{-2\pi i x\cdot \xi} f(x) dx.$$ The dominated convergence theorem confirms that the Fourier transform is a continuous function. Indeed, if the sequence $\xi_n\rightarrow \xi_0$ then $\hat{f}(\xi_n)\rightarrow\hat{f}(\xi_0)$ since, for every $n\geq 1$ , $|e^{-2\pi i x\cdot \xi_n} f(x)|\leq |f(x)|\in L^1(\mathbb{R}^n)$ . I want to prove that $\hat{f}$ is continuous using the $\epsilon$ - $\delta$ definition. My attempt: Fix $\xi_0$ in $\mathbb{R}^n$ and let $\epsilon>0$ . We need to prove that there exists $\delta>0$ such that $$|\xi-\xi_0|<\delta \implies |\hat{f}(\xi)-\hat{f}(\xi_0)|<\epsilon.$$ We have $$\hat{f}(\xi)-\hat{f}(\xi_0)=\int_{\mathbb{R}^n} (e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}) f(x) dx.$$ Define $$g(t):=e^{-2\pi i x\cdot ((1-t)\xi+t\xi_0)}.$$ Then $g$ is a smooth function. By the mean-value theorem $$e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}=g(1)-g(0)= g^{\prime}(s)=
2\pi i e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x\cdot (\xi-\xi_0)$$ for some $s\in (0,1)$ . Therefore $$\hat{f}(\xi)-\hat{f}(\xi_0)=2\pi i (\xi-\xi_0)\cdot\int_{\mathbb{R}^n} e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x f(x) dx.$$ Now, if $f$ has compact support, or decay faster than $1/|x|$ as $|x|\rightarrow \infty$ we are done since, in that case, $$|\hat{f}(\xi)-\hat{f}(\xi_0)|\leq C |\xi-\xi_0|$$ with $C=2\pi \int_{\mathbb{R}^n} |x| |f(x)| dx$ . If $f$ is just an $L^1$ function we must use the cancellation due to the oscillatory exponential factor (may be using Riemann-Lebesgue lemma).","['epsilon-delta', 'complex-numbers', 'analysis', 'real-analysis']"
4495136,Position of crease-point on right edge of $40\times40$ square upon folding top-left corner to midpoint of bottom edge?,"I was doing the following problem and my intuition was as follows. If point P touched Point R, the crease would be no distance from Q. When point P touches S, then the crease would be 20 cm away from point Q. So when point P touches point M, then the crease should be 10 cm away from point Q because M is the midpoint. But when I actually folded it out, I found the answer is actually 5cm from point Q. How could I work this out without folding paper in an exam, what is the intuition behind it?","['euclidean-geometry', 'problem-solving', 'geometry', 'origami']"
4495174,A difficult integral for the Chern number,"The integral $$
I(m)=\frac{1}{4\pi}\int_{-\pi}^{\pi}\mathrm{d}x\int_{-\pi}^\pi\mathrm{d}y \frac{m\cos(x)\cos(y)-\cos x-\cos y}{\left( \sin^2x+\sin^2y +(m-\cos x-\cos y)^2\right)^{3/2}}
$$ gives the Chern number of a certain vector bundle [1] over a torus. It can be shown using the theory of characteristic classes that $$
I(m) = \frac{\mathrm{sign}(m-2)+\mathrm{sign}(m+2)}{2}-\mathrm{sign}(m) = \begin{cases}1 & -2< m < 0 \\ -1 & 0 < m < 2 \\0 & \text{otherwise}\end{cases}.
$$ Is there any way to evaluate this integral directly (i.e. without making use of methods from differential geometry) to obtain the above result? I should mention that the above integral can be written as ( $1/4\pi$ times) the solid angle subtended from the origin of the unit vector $\hat{\mathbf{n}}$ , $$
I(m)=\frac{1}{4\pi}\int_{-\pi}^{\pi}\mathrm{d}x\int_{-\pi}^\pi\mathrm{d}y\, \hat{\mathbf{n}}\cdot\left(\partial_x \hat{\mathbf{n}}\times \partial_y \hat{\mathbf{n}}\right),
$$ where $\mathbf{n}(m)=(\sin x, \sin y, m- \cos x-\cos y)$ . While this form makes it very straightforward to evaluate $I(m)$ , I am interested in whether there is a way to compute this integral using more standard techniques. [1] B. Bernevig Topological Insulators and Topological Superconductors Chapter 8","['integration', 'definite-integrals', 'multivariable-calculus', 'characteristic-classes', 'differential-geometry']"
4495183,$\int f \;d\mu=\int g \;d\mu=1$ and $f \log g \in L^1(\mu)$ implies $\int f \log g \; d\mu\leq \int f \log f \; d\mu$,"I'm trying to show the following inequality. Let $f,g:X\to (0,\infty)$ be measurable functions with $\int f\; d\mu=\int g \;d\mu=1$ . Show that if $f \log g \in L^1(\mu)$ , then $$\int f \log g \;d\mu\leq \int f \log f \;d\mu$$ This seems like an application of Jensen's inequality but I'm having difficulty applying it correctly. I defined a measure $$d\nu=f\;d\mu$$ and so I need to show that $$\int  \log g \;d\nu\leq \int  \log f \;d\nu$$ Note that $(X,\nu$ ) is a probability space. I need help recognizing the convex function $\phi$ and the $\nu-$ integrable function $F$ onto which I can apply Jensen's inequality: $$\phi(\int F\;d\nu)\leq \int \phi(F)\;d\nu$$ Any help with this is appreciated. Thanks for your time.","['lebesgue-integral', 'jensen-inequality', 'real-analysis', 'convex-analysis', 'probability-theory']"
4495193,Find the integral of $\frac{x^n}{(\exp(x)+1)(\exp(-x)+1)}$,"I need to find the integral $$I = \int_{-\infty}^\infty \frac{x^n}{(\exp(x)+1)(\exp(-x)+1)}dx.$$ I think this should be possible with the residue theorem, but I got stuck.
So first of all the integral is zero if n is odd, since then it is an odd integrand integrated over a symmetric interval. My choice for the complex contour is a rectangle with sidelength $2R$ (on the real axis) and the other $2\pi i$ , because the denominator is the same for $x \rightarrow x+2\pi i$ and $I = \lim_{R\rightarrow \infty} \int_{\gamma_1}f(z)dz$ . I define the contour $\gamma = \gamma_1 + \gamma_2 + \gamma_3 + \gamma_4$ , where $\gamma_1$ and $\gamma_3$ are the two long, horizontal sides (with length $2R$ ) and $\gamma_2$ and $\gamma_4$ the two vertical ones. Then there is one pole of order 2 inside the region of integration at $z_0 = i\pi$ . $\int_{\gamma_2} f(z)dz + \int_{\gamma_4}f(z)dz = \int_0^{2\pi i} \frac{(t+R)^n}{(\exp(t+R)+1)(\exp(-t-R)+1)}dt - \int_0^{2\pi i} \frac{(-t+2\pi i-R)^n}{(\exp(-t-R)+1)(\exp(+t+R)+1)}dt = 0 + 0$ individually when $R\rightarrow \infty$ . Now this is where I got stuck. I want to relate the integral over $\gamma_3$ to the integral over $\gamma_1$ . For this I tried to rearange $$\begin{align}
\int_{\gamma_3}f(z)dz  &= -\int_{-R}^{R}\frac{(-t+2\pi i)^n}{(\exp(t) + 1)(\exp(-t)+1)}dt \\
\\
 &= -\sum_{k=0}^n \binom{n}{k}\cdot\int_{-R}^R\frac{(-t)^k(2\pi i)^{n-k}}{(\exp(t)+1)(\exp(-t)+1)}dt \\
\\
&=  
-\sum_{k=0}^n \binom{n}{k}\cdot(2\pi i)^{n-k}\cdot I_k
\end{align}$$ where I defined $I_k = \int_{\gamma_1}f(z)dz$ . Now I don't know how to proceed from here. Is there a way I can solve this recursive formula or do I is there maybe a completely different way of solving this?","['integration', 'complex-analysis', 'contour-integration', 'residue-calculus']"
4495204,Who wins in this simple coin-placing game?,"We have an $m\times n$ rectangle. Two players take turns placing down diameter- $1$ coins that don't overlap with previously placed coins. Whoever cannot place a coin loses. Who wins? (Note: coins are not required to fit neatly on a grid, so most turns will have uncountably many options.) I'm fairly certain we are guaranteed a winner, even though the game tree has infinitely many vertices, because it's not infinitely deep: there's an upper bound to the length of a game, and if I recall correctly there's a general theorem that implies one player must have a winning strategy in such a case. I don't know how to determine what that winning strategy is, though, or who has it. Circle packings are pretty messy, so I'm not quite sure where to begin. (For definitiveness, I will say that coins meeting at only one point - coins that are tangent - are allowed, though I am curious how the game changes if this rule is changed.)","['game-theory', 'circles', 'geometry']"
4495242,An olympiad number theory problem asking for prime numbers with a certain property,"Let $\displaystyle T_k=\sum^{k}_{j=1} \frac{1}{j2^j}$ , where $k$ is a positive integer. Find all prime numbers $p$ such that $$\displaystyle\sum_{k=1}^{p-2}\frac{T_k}{k+1}\equiv 0 \pmod{p},$$ where $\frac{a}{b} \pmod{p}$ means $ab^{-1} \pmod{p}$ and $b^{-1}$ is the multiplicative inverse of $b \pmod{p}$ . This is an olympiad  problem asked by my previous teacher in my high school. It is so hard to me that the only thing I can do is to check whether the desired property holds for small numbers.","['contest-math', 'number-theory', 'modular-arithmetic', 'prime-numbers']"
4495243,How to integrate products of exponential of exponential (or polynomials) and Gaussian functions,"Gaussian functions often appear in conditional expectation integrals. I am interested to know if analytic integrals of the following forms are available $$
\int \exp(-\exp(a x))   \exp(\frac{-(x-\mu)^2}{\sigma^2}) \; dx \quad\quad\quad (*)
$$ or \begin{equation}
\int \exp( P(a x))   \exp(\frac{-(x-\mu)^2}{\sigma^2}) \; dx
\end{equation} where $P$ is a polynomial in $x$ , and $a$ , $\mu$ and $\sigma$ are real constants. While the first integral is the one I would like to solve, I can rewrite the double exponential as a series \begin{eqnarray}
\text{Series}[\exp (-\exp (a x)),\{x,0,8\}] &=& \exp\left( -1-a x-\frac{a^2 x^2}{2}-\frac{a^3 x^3}{6}-\frac{a^4 x^4}{24}-\frac{a^5 x^5}{120}-\frac{a^6 x^6}{720}-\frac{a^7 x^7}{5040}+O\left(x^8\right)   \right)      \\
&=& \frac{1}{e}-\frac{a x}{e}+\frac{a^3 x^3}{6 e}+\frac{a^4 x^4}{24 e}-\frac{a^5 x^5}{60 e}-\frac{a^6 x^6}{80 e}-\frac{a^7 x^7}{560 e}+O\left(x^8\right)
\end{eqnarray} and this allows the integral in equation $(*)$ to be written as a sum of integrals of the form. \begin{equation}
\sum_{n=1}^{\infty} c_n \int(ax)^n  \exp(\frac{-(x-\mu)^2}{\sigma^2}) \; dx
\end{equation} so that each term looks like it is related to a moment-like integral (as listed on https://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions ). $\int \phi(x) \; dx=\Phi(x)+C$ $\int x \phi(x) \;dx=−\phi(x)+C$ $\int x^2 \phi(x)dx=Φ(x)−x\phi(x)+C$ $\int x^{2k+1} \phi(x)\;dx=−\phi(x)\sum_{j=0}^{k}\frac{(2k)!!}{(2j)!!} x^{2j}+C$ $\int x^{2k+2} \phi(x)\;dx=−\phi(x)\sum_{j=0}^{k}\frac{(2k+1)!!}{(2j+1)!!}x^{2j+1}+(2k+1)!!\Phi(x)+C$ While practically it may be possible to truncate the series and obtain numerical results, it would be much nicer if the definite integral for the exponent of exponent form exists. I would be grateful if anyone recognises the form of the integral in equation $(*)$ and could point me to its definite integrated form. I have not been able to locate it in some of the standard references.","['integration', 'statistics', 'conditional-expectation', 'expected-value', 'gaussian']"
4495248,"What is the need of Borel Set, instead of calling those sets as subsets of Real Numbers?","I am new to measure theory. I have good grasp on statistics and probability concepts, and not I am trying to learn it from the measure theory perspective. I encountered Borel Sets. The definition (taken from wikipedia) is as follows, Borel set is any set in a topological space that can be formed from open sets (or, equivalently, from closed sets) through the operations of countable union, countable intersection, and relative complement. Now, what I don't understand is, why we have a different definition, instead of just calling these sets as subsets of $\mathbb{R}$ .
For example,
Say I have two sets, $(- \infty,b] , [b,\infty)$ . Then by $(-\infty,b]\cap [b,\infty)$ we have ${b}$ . Then by, $(-\infty,b]\cap \{b\}^c$ we have $(-\infty,b)$ and so on. The point I am making here is, with enough open interval and closed interval subsets, we can achieve any subsets on the Real Numbers. So why even bother with the definition? Is there any advantage of having Borel sets be defined in such a way, instead of just calling those subsets of $\mathbb{R}$ ?","['measure-theory', 'statistics', 'probability']"
4495273,Evolution of the energy density under the harmonic map heat flow (The Ricci Flow in Riemannian Geometry),"I am reading now the book The Ricci Flow in
Riemannian Geometry by Ben Andrews and Christopher Hopper. It has been a while since I did not use pullback bundles and other objects, and they still look complicated to me. I have trouble regarding sections 3.2.1 and 3.2.2. First let me introduce some notations and context: They consider a vector subbundle of $T(M \times I)$ , where $I \subset \mathbb{R}$ is a time interval, consisting of vector tangents to the first factor $M$ in the sense that $(\pi_2)_* = 0$ . On that bundle, a connection $\nabla$ is defined via the one on $M$ denoted by $^M\nabla$ augmented by its effect on the time component $\nabla_{\partial_t} u = [\partial_t, u]$ for vectors on this bundle, and extended by zero outside the bundle. We are given a smooth map between Riemannian manifolds $f : (M, g) \to (N, h)$ and consider its time dependent variant $f : M \times  I \to N$ . The map induces the pullback bundle $f^*TN$ over $M \times I$ , and consider on it the pullback connection. The Energy $E$ is defined as the total density $$E(f) = \frac{1}{2}\int_M e(f) \: d\mu(g) = \frac{1}{2}\int_M\langle f_*, f_*\rangle \: d\mu(g)$$ We use coordinates $\{x_i\}$ on $M \times I$ and $\{y^\alpha\}$ on $N$ . We also use the indices to denote components. For instance in coordinates, we have $f_* = (f_*)_i^\alpha dx^i \otimes (\partial_\alpha)_f$ . I have a question regarding the motivation behind the gradient descent flow :
By a direct computation as done in section 3.2.1, the authors show that $$ \frac{d}{dt} E(f) = -\int_M \langle f_*\partial_t, \Delta_{g,h}f\rangle \: d\mu(g). $$ Then they go on like this Note that $f_*\partial_t$ is the variation of $f$ . Hence the gradient of E, with respect to the inner product of $f^*TN$ , is $-\Delta_{g,h}f$ and the gradient descent flow is $$ f_*\partial_t = \Delta_{g,h}f.$$ First, I do not understand why is the gradient of $E$ is $-\Delta_{g,h}f$ . Second, how does this imply the following of the statement? I have another question regarding the evolution of the energy density : The authors show that the evolution equation for $f_*$ is given by $$ (\Delta_{\partial_t} f_*)(\partial_i) = (\Delta_* f)(\partial_i) + g^{kl}R^N(f_*\partial_k, f_*\partial_i)(f_*\partial_l) - g^{kl}f_*(R^M(\partial_k, \partial_l)).$$ From that, they infer that $$\frac{d}{dt}e = \langle f_*, \Delta f_*\rangle + g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_i, f_*\partial_l, f_*\partial_j) - g^{kl}h_f\left(f_*\left((^M\mathrm{Ric})_k^{\:\,m}\partial_p\right), f_*\partial_l\right). $$ Here is my attempt for the second term :
By unravelling the definition for the scalar product, we get \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f(f_*)_i^\alpha[R^N(f_*\partial_k, f_*\cdot)(f_*\partial_l)]_j^\beta = g^{kl}g^{ij}h_f\left((f_*)_i,R^N(f_*\partial_k, f_*\partial_j)(f_*\partial_l)\right) = g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_j, f_*\partial_l, f_*\partial_i).\end{multline} For the third term. Here is my attempt :
By unravelling again, we get \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f (f_*)_i^\alpha[f_*(R^M(\partial_k, \cdot)\partial_l)]_j^\beta = g^{ij}h_f\left((f_*)_i, g^{kl}f_*((R^M)_{kjl}^{\:\:\:\:m}\partial_m\right) \\ = - g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jkl}^{\:\:\:\:m}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jk}\!\,^m_{\:\:\,l}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*((^M\mathrm{Ric})_j^{\:\,m}\partial_m)\right) = g^{kl}h_f\left(f_*\partial_l, f_*((^M\mathrm{Ric})_k^{\:\,m}\partial_m)\right). \end{multline} Are those two computations correct?","['riemannian-geometry', 'ricci-flow', 'heat-equation', 'solution-verification', 'differential-geometry']"
4495300,$x^2-px+p^2-4<0$ for at least one $x<0$,"Find the set of all values of $p$ , for which $x^2-px+p^2-4<0$ for at least one $x<0$ . My work: Let $x=k<0$ be a root of this inequality. So our inequality becomes $$k^2-pk+p^2-4<0$$ Here $k^2$ is $>0$ , $-pk$ is $>0$ So for the inequality to hold $p^2-4$ has to be $<0$ . After solving I got the range as, $p\in(-2,2)$ . Now consider a quadratic equation $$f(y)=y^2-py+p^2-4=0$$ where $f(x)<0$ So here the $\triangle\ge0$ So $$p^2-4p^2+16\ge0$$ or $$p\in\left(-\infty,\frac{-4}{\sqrt{3}}\right]\cup\left[\frac{4}{\sqrt{3}},\infty\right)$$ Doing the intersection we get $$p\in\phi$$ But the answer is given as $p\in\left(\frac{-4}{\sqrt{3}},2\right)$ And I'm not sure about the correctness of the answer But if it is correct, Where did I go wrong $?$","['interval-arithmetic', 'functions', 'inequality', 'quadratics', 'quadratic-forms']"
4495301,Lower and upper bound of discrete gaussian curvature.,"In a triangle 2-manifold mesh, or symplicial complex (but I'll stick with the former terminology) the discrete gaussian curvature is usually defined $$
K(v_i) = \frac{1}{A(i)}\left(2\pi - \sum_{(v_i,v_j,v_k)} \theta_i^{jk}\right)
$$ Where $v_i$ is a vertex of the triangle mesh, $(v_i,v_j,v_k)$ is a triangle of the mesh (identified with the triple of vertices) and $\theta_i^{jk}$ is the angle at $v_i$ of the triangle $(v_i,v_j,v_k)$ . Moreover $A(i)$ is the voronoi area around $v_i$ , namely one third of sum of all triangles area, incident to $v_i$ . However for example in this C++ library ( Geometry Central ) it seems that the definition is simply $$
K(v_i) = 2\pi - \sum_{(v_i,v_j,v_k)} \theta_i^{jk}
$$ For this last version (which I don't know whether or not its correct but it doesn't matter too much I guess) the question I have is what is the lower bound and upper bound of such formulas? I would've guessed that $$0 \leq K(v_i) \leq 2\pi$$ But because I see sometimes negative values as well maybe I am wrong? I was able to construct by hand few examples of negative $K$ but the ones I managed are only if the vertex $v_i$ is a boundary vertex. A general proof would be useful, or a reference I can look up.
To summarize then What are the bounds of $K(v_i)$ ? What is the proof of such bounds?","['computer-science', 'meshing', 'geometry', 'differential-geometry']"
4495303,"The dual of $L^p(X,\mu)$, where $1<p<\infty$ and $\mu$ is a positive measure","Note. The functions are intended to have real values. Theorem. Let $(X,\mathcal{A},\mu)$ a $\sigma-$ finite measure space and let $p\in [1,\infty)$ . For all $F\in \left(L^p(X)\right)^*$ exists a unique function $f\in L^q(X)$ , where $q$ is the conjugate exponent of $p$ , such that $$F(g)=\int_X gf\;d\mu\quad\forall g\in L^p(X);$$ moreover we have $$\rVert F \lVert_{(L^p(\mu))^*}=\rVert f \lVert_q.$$ If $\mu$ is only a positive measure, then the above theorem also holds for $1<p< \infty$ . I want to prove this using the following steps. Definition A set $E\in\mathcal{A}$ is said $\sigma-$ finite if exists a sequence $\{E_n\}\subseteq \mathcal{A}$ such that $E=\cup E_n$ and $\mu(E_n)<\infty$ for all $n\in\mathbb{N}$ . Let $$\Sigma=\left\{E\in\mathcal{A}\;|\; E\;\text{is}\;\sigma-\text{finite}\right\}$$ Claim $1$ . Let $F\in (L^p)^*$ , for all $E\in\Sigma$ exists a unique function $f_E\in L^q$ with $f_E|_{X\setminus E}=0$ such that $F(g)=\int_X gf_E\;d\mu$ for all $g\in L^p(X)$ . My proof for claim $1$ . Consider the measure $\mu_E(A):=\mu(A\cap E)$ for $A\in\mathcal{A}$ , then $\mu_E$ is $\sigma-$ finite measure on $\mathcal{A}\cap E$ and for the Theorem, exists a unique $f\in L^q(E)$ such that $F(g)=\int_E gf\;d\mu_E$ for all $g\in L^p (E)$ . Defining $$f_E(x)=f(x)\chi_E$$ we have that $$F(g)=\int_X gf_E\;d\mu\quad \forall g\in L^p(X)$$ Claim 2. If $E,E'\in\Sigma$ and $E\subseteq E'$ , then $f_E=f_E'$ a.e in $E$ . My proof for claim $2$ . It doesn't exist because I can't come to a conclusion. Let $$\lambda (E)=\int_X\lvert f_E \rvert^q\;d\mu\quad\forall E\in\Sigma:$$ Claim 3. $\lambda$ is an increasing function with respect to inclusion. My proof for claim $3$ . For the Claim $2$ we have that $$\lambda(E)=\int_X\lvert f_E\rvert^q\;d\mu=\int_E \lvert f_E\rvert^q\;d\mu=\int_E \lvert f_E'\rvert^q\;d\mu\le \int_{E^{'}} \lvert f_E\rvert^q\;d\mu=\lambda(E')$$ Claim 4. $\lambda$ it is limited from above My proof for claim $4$ . It doesn't exist because I can't come to a conclusion. We choose a sequence $\{E_n\}\in\Sigma$ such that $\lambda(E_n)\to m:=\sup_\Sigma \lambda(E)$ : Claim 5. $H=\bigcup_{n=1}^\infty E_n\in \Sigma$ and then $\lambda(H)=m$ . My proof for claim $5$ . It doesn't exist because I can't come to a conclusion. Claim 6. Defining $f=f_H$ we have that $f=f_E$ a.e in $E$ for all $E\in\Sigma$ . My proof for claim $6$ . It doesn't exist because I can't come to a conclusion. Claim 7. If $g\in L^q$ , defininf $N=\{x\in X\;|\;g(x)\ne 0\}$ , result that $N\in\Sigma$ and $F(g)=\int_X gf_{N\cup H}\;d\mu=\int_X gf\;d\mu.$ My proof for claim $7$ . It doesn't exist because I can't come to a conclusion. Question. I am aware of the fact that I have done little, but I would be grateful for any valuable hints you will give me. Thank you.","['measure-theory', 'proof-explanation', 'proof-writing', 'real-analysis', 'functional-analysis']"
4495321,Elementary doubts on limits and absolute value functions,"I have elementary questions about the definition of the absolute value functions and their limits. It should be remarked that I have also searched and read previous posts of Mathstackexchange. However, these did not help me with the questions to be presented here. Owing to the fact that there are few references on the subject, I think that these questions, and their possible answers, may be of great interest to those that subscribe to mathstacksexchange. With that said, let me recall first the definition of the absolute value of a real function. It is well-known that we may define a modulus function as follows: $f(x)=|x|=\left\{ \begin{array}{rcl}
x & \mbox{if}
& x \geq 0 \\ -x & \mbox{if} & x < 0 .
\end{array}\right.$ In the light of the foregoing, I ask: 1.Suppose that we have a function $\psi (x)$ that shows the possibility to diverge to positive infinity when $\psi >0$ as $x\rightarrow x_{0}$ or diverge to negative infinity when $\psi <0$ as $x\rightarrow x_{0}$ .  Here, $x_{0}$ is real a constant. On account of the definition expressed above, is it correct to write that $\displaystyle\lim_{x\rightarrow x_{0}}\psi \rightarrow \pm \infty \Leftrightarrow |\psi|\rightarrow \infty \quad\mbox{in which}\left\{ \begin{array}{rcl}
\psi \rightarrow \infty & \mbox{if}
& \psi > 0 & \mbox{as} & x \rightarrow x_{0}\\ \psi \rightarrow -\infty & \mbox{if} & \psi < 0 & \mbox{as} & x \rightarrow x_{0}   \quad ?
\end{array}\right.$ Second, is there a more formal way to express this from the standpoint of mathematics? In addition, may you suggest references on the limits of absolute value functions that diverge to infinity? Ps: It is worth emphasizing that I am not very well familiar with the correct notation of absolute value functions and limits. Hence, correct me if the above is not right.","['limits', 'algebra-precalculus', 'absolute-value']"
4495384,"If two matrices A and B have the same determinant, are AB and BA similar?","If two matrices A and B have the same determinant, are AB and BA similar? How would I prove it?
I'm learning this topic for a data science course, and I don't have a mathematical background. We haven't covered eigenvalues and characteristic polynomials yet. It would be helpful if you can answer without using those concepts.
Thanks","['matrices', 'linear-algebra']"
4495407,Total derivative of $e^{xy}(x^2+y^2)$,"Let $f:\mathbb{R^2} \rightarrow \mathbb{R} \longmapsto e^{xy}\cdot (x^2+y^2)$ Show for which $(x,y)\in \mathbb{R^2}$ the function is totally differentiable. A function is totally differentiable if a) $\lim \limits_{h \to 0}\frac{f(x+h)-f(x)-A\cdot h}{\Vert h \Vert}$ or b) f is continuously partially differentiable I first calculated the partial derivatives for both x and y: $$ \frac{\partial f}{\partial x}=e^{xy}\cdot(yx^2+y^3+2x)$$ $$ \frac{\partial f}{\partial y}=e^{xy}\cdot(xy^2+x^3+2y)$$ Thus the jacobian Matrix is $\begin{array}{rrr} 
e^{xy}\cdot(yx^2+y^3+2x) & e^{xy}\cdot(xy^2+x^3+2y) \\
\end{array}$ The partial derivatives are continuous as they are a product of two continuous functions. Therefore according to b) f is totally differentiable $\forall x,y\in \mathbb{R}$ Is this correct or did I make a mistake somewhere and f actually isn't totally differentiable for every point? Should this rather be done via a) or is that unnecessearily complex?","['partial-derivative', 'multivariable-calculus']"
4495411,Which interval is correct here$?$,"The equation $$2\textrm{sin}^2\theta x^2-3\textrm{sin}\theta x+1=0$$ where $\theta \in \left(\frac{\pi}{4},\frac{\pi}{2}\right)$ has one root lying in the interval $(0,1)$ $(1,2)$ $(2,3)$ $(-1,0)$ I know that if $f(a)$ and $f(b)$ are of opposite signs then at least $1$ or in general odd number of roots of the equation $f(x)=0$ lie between $a$ and $b$ . But I am not able to use this piece of information here, maybe because of two variables. I also tried to assume $\textrm{sin}\theta x$ as $y$ but nothing good came out. Any help is greatly appreciated. EDIT Answer given is $(1,2)$","['roots', 'interval-arithmetic', 'functions', 'trigonometry', 'quadratics']"
4495429,How can I invert a complicated bivariate polynomial?,"Here is the function I want to invert: rho(t, p) =             
        + 998.20123 * p ** 0
        - 192.9769495 * p ** 1
        + 389.1238958 * p ** 2
        - 1668.103923 * p ** 3
        + 13522.15441 * p ** 4
        - 88292.78388 * p ** 5
        + 306287.4042 * p ** 6
        - 613838.1234 * p ** 7
        + 747017.2998 * p ** 8
        - 547846.1354 * p ** 9
        + 223446.0334 * p ** 10
        - 39032.85426 * p ** 11
        - 0.20618513 * (t - 20) ** 1
        - 0.0052682542 * (t - 20) ** 2
        + 0.000036130013 * (t - 20) ** 3
        - 0.00000038957702 * (t - 20) ** 4
        + 0.000000007169354 * (t - 20) ** 5
        - 0.000000000099739231 * (t - 20) ** 6
        + 0.1693443461530087 * p ** 1 * (t - 20) ** 1
        - 10.46914743455169 * p ** 2 * (t - 20) ** 1
        + 71.96353469546523 * p ** 3 * (t - 20) ** 1
        - 704.7478054272792 * p ** 4 * (t - 20) ** 1
        + 3924.090430035045 * p ** 5 * (t - 20) ** 1
        - 12101.64659068747 * p ** 6 * (t - 20) ** 1
        + 22486.46550400788 * p ** 7 * (t - 20) ** 1
        - 26055.62982188164 * p ** 8 * (t - 20) ** 1
        + 18523.73922069467 * p ** 9 * (t - 20) ** 1
        - 7420.201433430137 * p ** 10 * (t - 20) ** 1
        + 1285.617841998974 * p ** 11 * (t - 20) ** 1
        - 0.0119301300505701 * p ** 1 * (t - 20) ** 2
        + 0.2517399633803461 * p ** 2 * (t - 20) ** 2
        - 2.170575700536993 * p ** 3 * (t - 20) ** 2
        + 13.53034988843029 * p ** 4 * (t - 20) ** 2
        - 50.29988758547014 * p ** 5 * (t - 20) ** 2
        + 109.635566657757 * p ** 6 * (t - 20) ** 2
        - 142.2753946421155 * p ** 7 * (t - 20) ** 2
        + 108.043594285623 * p ** 8 * (t - 20) ** 2
        - 44.14153236817392 * p ** 9 * (t - 20) ** 2
        + 7.442971530188783 * p ** 10 * (t - 20) ** 2
        - 0.0006802995733503803 * p ** 1 * (t - 20) ** 3
        + 0.01876837790289664 * p ** 2 * (t - 20) ** 3
        - 0.2002561813734156 * p ** 3 * (t - 20) ** 3
        + 1.02299296671922 * p ** 4 * (t - 20) ** 3
        - 2.895696483903638 * p ** 5 * (t - 20) ** 3
        + 4.810060584300675 * p ** 6 * (t - 20) ** 3
        - 4.672147440794683 * p ** 7 * (t - 20) ** 3
        + 2.458043105903461 * p ** 8 * (t - 20) ** 3
        - 0.5411227621436812 * p ** 9 * (t - 20) ** 3
        + 0.000004075376675622027 * p ** 1 * (t - 20) ** 4
        - 0.00000876305857347111 * p ** 2 * (t - 20) ** 4
        + 0.000006515031360099368 * p ** 3 * (t - 20) ** 4
        - 0.00000151578483698721 * p ** 4 * (t - 20) ** 4
        - 0.00000002788074354782409 * p ** 1 * (t - 20) ** 5
        + 0.00000001345612883493354 * p ** 2 * (t - 20) ** 5 I usually use Python for my problems, so I tried sympy.solve, but it says that ""No algorithms are implemented to solve the equation. "" Are you aware of a tool that could do it? Or do you even know if there's a way to check if this function is invertible at all? This function is from https://www.oiml.org/en/publications/bulletin/pdf/oiml_bulletin_july_2015.pdf , page 11 (numbered 9 in the doc) I want to be able to invert it with respect to either variable. The rho function is monotonic for a fixed t : But not for a fixed p: For most of my use cases, the problem is much simpler since t is always 20. The polynom to inverse then becomes rho(p) =            
    + 998.20123 * p ** 0
    - 192.9769495 * p ** 1
    + 389.1238958 * p ** 2
    - 1668.103923 * p ** 3
    + 13522.15441 * p ** 4
    - 88292.78388 * p ** 5
    + 306287.4042 * p ** 6
    - 613838.1234 * p ** 7
    + 747017.2998 * p ** 8
    - 547846.1354 * p ** 9
    + 223446.0334 * p ** 10
    - 39032.85426 * p ** 11 with p in [0; 1]. This is strictly increasing on the intervalle, so it must have a solution for the inverse? sympy.solve returns the following roots : [
    1.30502635519079, 
    -0.282825358240998 - 0.174919557643158*I, 
    -0.282825358240998 + 0.174919557643158*I, 
    -0.0229786659460121 - 0.441023703545107*I, 
    -0.0229786659460121 + 0.441023703545107*I,
    0.434132824352787 - 0.596683531853141*I, 
    0.434132824352787 + 0.596683531853141*I, 
    0.877616891070605 - 0.549674307713161*I,
    0.877616891070605 + 0.549674307713161*I, 
    1.20382263269017 - 0.326905440295712*I, 
    1.20382263269017 + 0.326905440295712*I
 ] of which only the first one is real. What do i do with it?","['functions', 'inverse', 'inverse-function']"
4495447,Is self-adjoint operator in a Hilbert space is always positive operator?,"Let me first define the self-adjoint operator. Let $A$ be a bounded operator in a Hilbert space $H$ , then $A$ is said to be a self-adjoint operator if $A^*=A$ . And $A$ is known as a positive operator if $\langle Ax,x \rangle \geq 0$ . What I know is that a positive operator is not always a self-adjoint operator for example we can take the rotation operator in $\mathbb{R}^2$ . My question is whether the self-adjoint operator is always a positive operator? According to me it is not because if $A$ is a self-adjoint operator then $\langle Ax,x \rangle$ is real and when this is greater than or equal to zero then we say it is a positive operator but I am finding it difficult to construct an example.","['hilbert-spaces', 'self-adjoint-operators', 'functional-analysis', 'positive-semidefinite']"
4495528,"Dynamic dice game, how to reasonably estimate answer by hand without laboriously calculating","Here's a question from my probability textbook: A casino comes up with a fancy dice game. It allows you to roll a dice as many times as you want unless a $6$ appears. After each roll, if $1$ appears, you will win $\$1$ ; if $2$ appears, you will win $\$2$ ; $\ldots$ ; if $5$ appears, you win $\$5$ , but if $6$ appears all the money you have won in the game is lost and the game stops. After each roll, if the dice number is $1$ - $5$ , you can decide whether to keep the money or keep on rolling. How much are you willing to pay to play the game (if you are risk neutral)? It's been asked before on MSE multiple times: When to stop rolling a die in a game where 6 loses everything Dynamic dice game: optimal price to enter A dynamic dice game Here's the answer to the question in my book: Assuming that we have accumulated $n$ dollars, the decision to have another roll or not depends on the expected profit versus expected loss. If we decide to have an extra roll, our expected payoff will become $${1\over6}(n + 1) + {1\over6}(n + 2) + {1\over6}(n + 3) + {1\over6}(n + 4) + {1\over6}(n + 5) + {1\over6} \times 0 = {5\over6}n + 2.5.$$ We have another roll if the expected payoff ${5\over6}n + 2.5 > n$ , which means that we should keep rolling if the money is no more than $\$14$ . Considering that we will stop rolling when $n \ge 15$ , the maximum payoff of the game is $\$19$ (the dice rolls a $5$ after reaching the state $n = 14$ ). We then have the following: $f(19) = 19$ , $f(18) = 18$ , $f(17) = 17$ , $f(16) = 16$ , and $f(15) = 15$ . When $n \le 14$ , we will keep on rolling, so $E[f(n) \mid n \le 14] = {1\over6} \sum_{i = 1}^5 E[f(n + i)]$ . Using this equation, we can calculate the value for $E[f(n)]$ recursively for all $n = 14, 13, \ldots, 0$ . After laboriously calculating or writing a program, we get $E[f(0)] = 6.15$ , and so we are willing to pay at most $\$6.15$ for this game. However, I'm wondering if there's a quick way by hand to get a reasonable/""good enough"" estimate for $E[f(0)]$ without having to do multiple ""average-five-numbers-and-repeat"" as the book suggests. Is there?","['estimation', 'expected-value', 'dynamic-programming', 'game-theory', 'probability']"
4495534,Why is the domain of a circle with a defined radius the set of all real numbers?,"I'm working through a textbook and a problem asked about the domain and co-domain of $x^2 + y^2 = 1$ . My understanding is that the domain of a function is the set of possible numbers $x$ can take on to make the original expression valid. Because the left-hand side is the sum of two squared terms, the lowest value $x$ can take, while remaining a true expression, is $-1$ and the highest is $1$ . Given this, I think the correct answer for the domain is $\{-1 < x < 1\}$ , but the textbook says the domain is the set of all real numbers; why is this the case? Here is a screenshot of the textbook:","['algebra-precalculus', 'relations', 'discrete-mathematics']"
4495556,Finding conformal map from a region with a slit to upper half plane,"Find a conformal map from $F=\{z=x+iy\in\mathbb{C}:-\pi<y<\pi\}\setminus(-\infty,0]$ to the upper half plane $\mathbb{H}=\{z=x+iy\in\mathbb{C}:y>0\}$ To be honest, I never dealt with strips with slits before so I don't really have an idea where to start. The first idea that came to mind is maybe use the logarithm function (since its' principal branch is when we take $\mathbb{C}\setminus(-\infty,0]$ . Other than that I don't know how to continue. What is the general way to think on such questions?","['complex-analysis', 'conformal-geometry', 'analysis', 'mobius-transformation']"
4495565,$I_n = \int_1^e (\ln x)^n dx$. Compute $\lim_{n\to\infty} nI_n$,"If $$I_n = \int_1^e (\ln x)^n dx$$ Compute $$\lim_{n\to\infty} nI_n$$ One solution would be to determine the recurrence relation of $I_n$ (using integration by parts), namely $$I_{n+1} + (n+1)I_n = e$$ From the recurrence relation and from the fact that $I_n$ is decreasing we can determine that $$e = I_{n+1} + (n+1)I_n \ge I_{n+1} + (n+1)I_{n+1} = (n+2)I_{n+1}$$ which means that $$I_{n+1} \le \frac{e}{n+2}$$ so $$\lim_{n \to \infty} I_n = 0\tag{1}\label{1}$$ The recurrence relation can be rewritten as $$nI_n = e - I_{n+1} - I_{n}$$ and using $\eqref{1}$ we can conclude that $$\lim_{n\to\infty} nI_n = e$$ Is there another ""more elegant"" solution to this problem? Thank you!","['limits', 'definite-integrals']"
4495572,Solving the first-order non-linear differential equation $y' = y^2 - 2 x$,"I am trying to solve this Cauchy's problem: $$
y' = y^2 - 2x
$$ with condition $y(0) = 2$ It's very similar to Bernoulli equation $$
y' + a(x)y = b(x)y^2$$ however doesn't contain $a(x)y$ . I also tried substitution like $y = tx$ . but it hasn't helped, because i've got this $$
t'x + t = (tx)^2 + 2x
$$ I've tried using computer algebra resourses, and it gave me solution in closed form which looks very awfull  with Bessel function of first kind and Gamma function. So let's make it easier, is there any way to find a series $y(x) = \sum a_n x^n$ that formally solve this differential equation (i calculated some first coeffitients, but i can't solve infinite system of linear equations on coeffitients and i can't find a good formula for $a_n$ )?","['cauchy-problem', 'airy-functions', 'special-functions', 'ordinary-differential-equations']"
4495686,Why $\det\begin{pmatrix} 1&1&0&...&0 \\ -1 & 1 & 0 &...&0 \\ -1 & 0 & 1 &...&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots \\ -1&0&0&...&1\end{pmatrix}=2$?,"Let $$A = \begin{pmatrix}
1 & 1 & 0 & 0 &0 &... & 0
\\ -1 & 1 & 0 & 0 &0&...&0
\\ -1 & 0 & 1 & 0 &0&...&0
\\ -1 & 0 & 0 & 1 &0&...&0
\\ -1 & 0 & 0 & 0 &1&...&0
\\ \vdots & \vdots & \vdots & \vdots &\vdots&\ddots&\vdots
\\ -1 & 0 & 0 & 0 &0&...&1\end{pmatrix}$$ be an $n \times n$ matrix consisting of $1$ 's on its diagonal, a $1$ in the entry located on row $1$ and column $2$ , and $-1$ 's from  the second entry of the first column all the way to $n$ . Prove that the determinant of $A$ is always $2$ . How should I begin this? I tried taking the determinants of the matrix when $n=2, 3, $ and $ 4$ and saw that they were all $2$ , but am not sure how to proceed.","['matrices', 'determinant', 'linear-algebra']"
4495687,Does strict convexity imply continuity?,"Let $E$ be a normed space and $f:E \to \mathbb R$ convex. If $E = \mathbb R^d$ then $f$ is locally Lipschitz-continuous. If $E$ is infinite-dimensional then $f$ is not necessarily continuous. There exists discontinuous linear functional on an infinite-dimensional normed space. Now we assume more that $f$ is strictly convex, i.e., $$
f(tx + (1-t)y) < tf(x)+(1-t)f(y) \quad \forall t \in (0, 1), \forall x,y\in E \text{ s.t. } x\neq y.
$$ Strict convexity does not imply differentiability. Does strict convexity of $f$ imply that $f$ is continuous?","['continuity', 'convex-analysis', 'functional-analysis']"
4495691,A Simple Two-State Markov Chain : How to Explain a Strange Result?,"Context : After reading answers to my previous post (you don't need to read , this post is self sufficient)  , I obtained a strange result which I'm unable to explain . Problem : An opera singer is due to perform a long series of concerts. Having a bad temper, singer is liable to pull out each night with probability $f(a)\in (0,1]$ . Once this has happened singer will not sing again until the promoter convinces singer of the promoter’s high regard. This the promoter does by sending flowers every day until the singer returns. Flowers costing $b$ thousand GBP , $0 ≤ b ≤ 1$ ,
bring about a reconciliation with probability $g(b) \in (0,1] $ .The promoter stands to
make $a$ thousand GBP from each successful concert , $0 ≤ a ≤ 1$ . How much should promoter spend on flowers? This's equivalent to finding the expected eventual daily profit then maximizing . The later is irrelevant here . This could be solved by (thanks to user Sharky Kesa ): Suppose the opera singer has $n$ performance days . Let $$
U_i = \left\{ \begin{array}{cc}
1 & \text{ if performance occurs on day } i \\
0 & \text{ else }
\end{array} \right.
$$ for $i\in \{1,...,n\} $ . $(U_i)_{i\ge 0}$ is a Markov chain on state space $\{0,1\}$ .
The profit is $M = (a + b)(U_1+...+U_n) - nb $ . We have transition matrix $$
P^n = \begin{bmatrix}
1 - g(b) & g(b) \\
f(a) & 1 - f(a) 
\end{bmatrix}^n
= \begin{bmatrix}
1 & 1 \\
1 & - \frac{f(a)}{g(b)} 
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & (1-f(a)-g(b))^n
\end{bmatrix}
\begin{bmatrix}
\frac{f(a)}{f(a)+g(b)} & \frac{g(b)}{f(a)+g(b)} \\
\frac{g(b)}{f(a)+g(b)} & \frac{-g(b)}{f(a)+g(b)}
\end{bmatrix}
$$ Let the distribution of $U_i$ be $u_i$ , we have $$
u_1 = \begin{bmatrix}\frac{1}{2} &  \frac{1}{2} \end{bmatrix}
$$ $$
\implies u_i = u_1 P^{i-1} = 
\begin{bmatrix}
\frac{1}{2}\frac{ 2f(a) - (f(a)-g(b) )(  1 -f(a) - g(b) )^{i-1}   }{f(a) + g(b) } &
\frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)}
\end{bmatrix}
$$ Let $U = \lim_{i\to\infty} U_i  $ and the eventual daily profit $D =  (a+b)U - b $ , so $$
ED  = 
\lim_{i\to \infty} (a+b)EU_i - b = 
\lim_{i\to \infty} (a+b)\left( \frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)} \right) - b
$$ $f(a) , g(b) \in (0,1] \implies | 1-f(a)-g(b) | < 1$ or $f(a)-g(b) = 0 $ so $$
 = (a+b)\left( \frac{1}{2}\frac{ 2g(b)  }{f(a)+g(b)} \right) - b
$$ $$
 =  \frac{ ag(b)  - bf(a)  }{f(a)+g(b)}  
$$ $$
 =  \frac{ a\frac{1}{f(a)}  - b\frac{1}{g(b)}  }{\frac{1}{f(a)}+\frac{1}{g(b)}} 
$$ $$
=  \frac{ aEX - bEY  }{EX+EY} 
$$ with $X\sim Geo(f(a)) , Y \sim Geo(g(b))$ and there's no restriction on their dependency . You could also arrive here by solving for invariant distribution and applying theorem 1.10.2 here . How to explain this result ? e.g. Could it be explained by the 2 approaches I mentioned or does it suggest a third approach to the problem ? A possible third approach : (thanks to user Jafego ) Instead of viewing $(U_{n})_{n\ge 0}$ as a ""straight line"" , warp it to form numerous cycles according to some rules  (like a spring except size of each cycle is random)  . Each cycle has 2 components : A consecutive number of days $X\sim Geo(f(a)) $ where the promoter makes a profit $a$ thousand GBP per day . A consecutive number of days $Y\sim Geo(g(b))$ where the promoter losses $b$ thousand GBP per day . A cycle is of length $X + Y$ . So the ""sample mean"" of daily profit in a cycle is $ \frac{aX-bY}{X+Y}$ .  I think this ""sample mean"" is also i.i.d for each cycle .   However , it has variable sample size so I'm unable to proceed from here . This's as close as I could get to explaining the result .","['markov-chains', 'discrete-mathematics', 'game-theory', 'probability-theory', 'probability']"
4495697,"Four of a kind combinatorics, why is my logic incorrect?","I'm going through a probability textbook and getting stuck on a simple question. From a standard deck of 52 cards, what is the probability of getting a 4 of a kind? I've seen many of the answers, and I understand how to get there. The straightforward way of answering the question is just to realize there's $ 13$ four of a kinds with $48$ choices for the 5th card so the solution is just $$ \frac {13 *48} {52 \choose 5} = \frac {624} {52 \choose 5}$$ I understand how to get to this answer. My question is, why is the following approach incorrect? For the first card we can draw any of the 52 cards, we must draw the remaining cards with the same rank. This equates to : $$52 * 3 * 2 * 1$$ Then for the 5th card we have 48 remaining cards so we have: $$52*3*2*1 * 48$$ Now, furthermore, these 5 cards can be arranged in any manner. So for the total number of ways we have: $$ \frac {52 *3 *2 *1 * 48} {5!} $$ Then we simply divide by ${52 \choose 5}$ to get our final probability. Now it turns out this answer is wrong, but I cannot figure out why? The mistake is obviously in this step: $$ \frac {52 *3 *2 *1 * 48} {5!} $$ But for the life of me I cannot figure out why. It seems the correct answer is to divide by $4!$ factorial instead of $5! $ , but why? We have 5 cards, they can be arranged in $5!$ ways","['solution-verification', 'combinatorics', 'probability']"
4495718,"Similar matrices and conjugation outside of the center of GL(n,R) (a basic linear algebra question)","I'm teaching an introductory linear algebra course and have confused myself about something that is probably very straightforward; I would love someone to help me see where I'm going wrong. We have just started talking about eigenanalysis and have recently shown that two matrices are similar if and only if they represent the same linear transformation. That is, we have $A = P B P^{-1}$ if and only if $A$ and $B$ are both matrix representations of $T$ in the bases $\mathcal{B}_A$ and $\mathcal{B}_B$ . This is all fine; the proof is straightforward and totally unobjectionable. My confusion comes from the fact that conjugation ( $\phi_g: a \mapsto gag^{-1}$ ) is a nontrivial action for most subgroups of linear transformations. For an example, let's look at $r$ conjugated by $s$ in the dihedral group $D_n$ . I have $r^{-1} = s r s$ , and $r$ and $r^{-1}$ are not the same linear transformation for any $n > 2$ . But $s = s^{-1}$ , so we certainly have $r^{-1} = P r P^{-1}$ (matrix similarity) for a matrix representation of $D_n$ in whatever basis we might choose. It seems like these would then be a pair of similar matrices that represent different linear transformations, which is obviously not possible. I must be operating under some kind of silly misconception or misunderstanding some basic definition; can somebody tell me what it is?","['matrices', 'similar-matrices', 'linear-algebra', 'linear-transformations']"
4495726,Motion along a circle,"The problem below is from the textbook I'm reading from. I'm also stating the solution to the problem after the problem statement. Someone kindly verify the legitimacy of my solution. Problem Statement : Show that the vector valued function $$
 \textbf{r}(t)=2\textbf{i}+2\textbf{j}+\textbf{k}+\cos
 t\left(\frac{1}{\sqrt{2}}\textbf{i}-\frac{1}{\sqrt{2}}\textbf{j}\right)+\sin
 t\left(\frac{1}{\sqrt{3}}\textbf{i}+\frac{1}{\sqrt{3}}\textbf{j}+\frac{1}{\sqrt{3}}\textbf{k}\right)
 $$ describes the motion of a particle moving in the circle of radius $1$ , centered at $(2,2,1)$ . My Solution : Part 1 : Proving $\textbf{r}(t)$ represents a circular path. Let us for now assume this function indeed defines a circular path and let the center be $(a,b,c)$ . Let $\textbf{R}(t)$ be the radius vector from the center to any point on the curve $\textbf{r}(t)$ . Then $$
\textbf{R}(t)=\textbf{r}(t)-\langle a,b,c \rangle=\left\langle 2-a+\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}}, 2-b-\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}},1-c+\frac{\sin t}{\sqrt{3}} \right\rangle
$$ The tangent vector to $\textbf{r}(t)$ is obtained by differentiating $\textbf{r}(t)$ . Let it be $\textbf{v}(t)$ . $$
\textbf{v}(t)=\textbf{r}'(t)=\left\langle -\frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}}, \frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}},\frac{\cos t}{\sqrt{3}} \right\rangle
$$ If $\textbf{r}(t)$ represents a circular path then $\textbf{R}(t)\cdot\textbf{v}(t)=0$ . This gives us $$
\textbf{R}(t)\cdot\textbf{v}(t)=\left(\frac{a-b}{\sqrt{2}}\right)\sin t + \left(\frac{5-a-b-c}{\sqrt{3}}\right)\cos t=0
$$ only if $$
a=b \text{ & } a+b+c=5
$$ We can see that $(a,b,c)$ exist that can satisfy this condition and hence $\textbf{r}(t)$ represents a circular path. Part 2 : Finding the center. Since $\textbf{r}(t)$ is a circular locus, the radius vector's magnitude would be constant, i.e $|\textbf{R}(t)|=k$ . This means $$
\frac{d}{dt}|\textbf{R}(t)|=0 \implies \frac{d}{dt}|\textbf{R}(t)|^2=0
$$ Doing differentiation and some algebra gives us $$
|\textbf{R}(t)|\frac{d}{dt}|\textbf{R}(t)|=\left(\frac{a-b}{\sqrt{2}}\right)\sin t+\left(\frac{4-a-b}{\sqrt{3}}\right)\cos t=0
$$ From part 1, we know that $a=b$ and $a+b+c=5$ . Using this we get $$
\left(\frac{4-2a}{\sqrt{3}}\right)\cos t=0 \implies a=2 \implies b=2 \implies c=1.
$$ Part 3 : Finding radius Substituting values of $a$ , $b$ and $c$ into the expression for $|\textbf{R}(t)|$ gives $|\textbf{R}(t)|=1$ . I've spared the algebra. This completes my solution/proof. My main question In Part 1, I have assumed $\textbf{r}(t)$ to be a circular path and found that real numbers $a$ , $b$ and $c$ exist that satisfy the condition for $\textbf{r}(t)$ to be circle: that radius is perpendicular to tangent. If $\textbf{r}(t)$ wasn't a circle, then $a$ and/or $b$ and/or $c$ wouldn't exist. Is this argument watertight? I think so because there is no other curve that satisfies this condition, at least as far as I know.","['multivariable-calculus', 'solution-verification', 'vectors', 'vector-analysis']"
4495730,"Permutation property of ""law of cosines"" stated more compactly through abstract algebra","The law of cosines, stated: $c^2 = a^2 + b^2 - |a||b|\cos(\gamma)$ is a result that still holds if we swap the side-lengths and the corresponding angle. Is there a way to capture this symmetry from a group-theoretic perspective? Or some other abstract algebraic perspective? I apologize, I'm not entirely sure how to ask this due to significant lack of knowledge. I think one could use Galois theory to state this more elegantly -- by defining this to be a quadratic polynomial in some sense. Or we could not use Galois theory at all and think about a corresponding permutation group. I'm also unaware if this is already a fairly common sidenote in the study of, say, quadratic forms for a particular geometry. Motivation: Wherever I see this elementary result stated, either the other 2 corresponding equations are stated alongside it, or it's stated that the equation would still work if we swapped the variables. Having learned aspects of group theory and abstract algebra, I feel there is a better way to state this. And I'm hoping this more abstract way of interpreting this could lead to an elementary example of a more advanced concept -- and as a bonus, potentially open the door to better understanding applications of quadratic forms and of defining a geometry through a group-theoretic perspective.","['analytic-geometry', 'group-theory', 'trigonometry', 'permutation-cycles']"
4495731,Find the value of $\int_{0}^{1}f(x)dx$ given the following conditions without using CS inequality,"We are asked to solve for the value of $\int_{0}^{1}f(x)dx$ given that $$ f(1) = 0 \tag{1}  $$ $$ \\ \int_{0}^{1}[f'(x)]^2dx = 7 \tag{2} \\ $$ $$ \int_{0}^{1}x^2f(x)dx = \frac{1}{3} \tag{3} $$ We can solve this using Cauchy-Schwartz Inequality: From $(3)$ we can use integration by parts to obtain $$ \int_{0}^{1}x^3f'(x)dx = -1 \tag{4}$$ With $(2)$ and $(3)$ , and from Cauchy-Schwartz inequality $$ \Big[ \int_{0}^{1}x^3f'(x)dx \Big]^2 \leq \underbrace{\Big( \int_{0}^{1}[x^3]^2dx \Big)}_{\frac{1}{7}} \underbrace{\Big( \int_{0}^{1}[f'(x)]^2dx\Big)}_{7 \text{ from } (2)}$$ Therefore we have, $$ 1 \leq 1 $$ Thus an equality, which in turn implies $f'(x) = ax^3$ where $a$ is just a scalar. From $(1)$ we have, $$f(x) = \frac{a}{4}x^4 + C \rightarrow f(1) = \frac{a}{4} + C  = 0 $$ and from $(2)$ we have, $$ \int_{0}^{1}[f'(x)]^2dx = \int_{0}^{1}(ax^3)^2dx = a^2\underbrace{\int_{0}^{1}x^6dx}_{\frac{1}{7}} = 7 \rightarrow a = \pm 7$$ $$f(x) = \pm \frac{7}{4}x^4 \mp \frac{7}{4}$$ and finally from $(3)$ we can see $$\int_{0}^{1}x^2\big(-\frac{7}{4}x^4 +\frac{7}{4} \big) dx = \frac{1}{3} $$ but $$\int_{0}^{1}x^2\big(\frac{7}{4}x^4 - \frac{7}{4} \big) dx = - \frac{1}{3} $$ Therefore we can conclude $$ \boxed{f(x) = -\frac{7}{4}x^4 +\frac{7}{4}} $$ and $$\boxed{\int_{0}^{1}f(x)dx = \int_{0}^{1}-\frac{7}{4}x^4 +\frac{7}{4}dx = \frac{7}{5}}$$ This concludes the solution using CS inequality. But are there other methods to solve this, preferably only using high-school/AP level calculus?","['calculus', 'cauchy-schwarz-inequality', 'analysis']"
4495758,How to show analytically that $x^4-4x^3-2x^2+16x+24=0$ has no solutions in $\mathbb{R}$?,"I'm trying to study a function and I need to prove that $x^4-4x^3-2x^2+16x+24=0$ has no solutions in $\mathbb{R}$ in a simple way. I drew the polynomial function $f(x)=x^4-4x^3-2x^2+16x+24$ with Wolfram and saw that the graph doesn't intersect the $x$ axis but I'd like to know if there is any simple method to prove that without Wolfram or any other ""external supports"".","['algebra-precalculus', 'functions', 'polynomials']"
4495769,A question on glueing modules,"$\require{AMScd}$ Consider the following commutative diagram of integral domains: \begin{CD}
R_0 @>>> R_1\\
@VVV @VVV\\
R_2 @>>> R_3,
\end{CD} where $R_3 \simeq R_1 \otimes_{R_0} R_2$ . Let $M_i$ be a finitely generated module over $R_i$ for $i=1,2$ such that $M_3 \simeq R_3 \otimes_{R_2} M_2 \simeq R_3 \otimes_{R_1} M_1$ . Under what minimal assumptions on morphisms between rings (e.g. faithful flatness etc.?) is it always possible to obtain a finitely generated $R_0\textrm{-module}$ $M_0$ such that $R_1 \otimes_{R_0} M_0 \simeq M_1$ and $R_2 \otimes_{R_0} M_0 \simeq M_2$ .","['finitely-generated', 'algebraic-geometry', 'commutative-algebra', 'modules']"
4495798,Two hard integrals: $\int_{0}^{1}\frac{\log{(x)}\log{(1-x)}\log{(1+x^2)}}{x}dx$ and $\int_{0}^{1}\frac{\log^2{(x)}\log{(1+x^2)}}{1-x}dx$,"I found two integrals that seem hard to evaluate: $$I_1=\int_{0}^{1}\frac{\log{(x)}\log{(1-x)}\log{(1+x^2)}}{x}dx$$ $$I_2=\int_{0}^{1}\frac{\log^2{(x)}\log{(1+x^2)}}{1-x}dx$$ I am just a beginner in logarithmic integral. So, I searched to find substitution like $x=\frac{1}{1+x}$ , $x=\frac{1}{1-x}$ , or $x=\frac{1-x}{1+x}$ , but they didn't work.
Can I ask some ideas from every one? Thank you. EDIT: After using Mathematica, with MZIntegrate paclet gives closed-form: $$\begin{align}I_1&=\int_{0}^{1}\frac{\log{(x)}\log{(1-x)}\log{(1+x^2)}}{x}dx\\&=G^2+\frac{5 \text{Li}_4\left(\frac{1}{2}\right)}{4}+\frac{35}{32} \zeta (3) \log (2)-\frac{119 \pi ^4}{5760}+\frac{5 \log ^4(2)}{96}-\frac{5}{96} \pi ^2 \log ^2(2)\\I_2&=\int_{0}^{1}\frac{\log^2{(x)}\log{(1+x^2)}}{1-x}dx\\&=2 G^2+\frac{35}{16} \zeta (3) \log (2)-\frac{199 \pi ^4}{5760}\end{align}$$ where $G$ is Catalan's constant.","['integration', 'calculus', 'improper-integrals', 'special-functions']"
4495807,CDF of the distance from origin to the hyperplane passing through $d$ i.i.d. points in $\mathbb{R}^d$,"I am stuck with a problem in multivariable statistics. The problem can be stated as follows: For a spherically symmetric distribution in $\mathbb{R}^d$ , it can be specified completely by the function $F(r)=\Pr(\|X\|>r)$ . For example, 2d standard Gaussian distribution has $F(r)=\exp(-r^2/2)$ .
Now $X_1, X_2, \dots, X_d$ i.i.d. follow this distribution.
These $d$ random points form a hyperplane in $\mathbb{R}^d$ . What is the probability $H(p)$ that the distance from the origin to this hyperplane is larger than $p$ for $p>0$ ? The answer should be written as integration about $F$ . I found the solution for $d=2$ in a German paper [1], which says $$
H(p)=\frac{2}{\pi} \int_p^{\infty} \arccos\frac{p}{r} |d(F^2(r))|
$$ I successfully derive the formula for $d=3$ as: \begin{align}
H(p)= \int_p^{+\infty}\frac{2p }{\pi r \sqrt{r^2-p^2}}G_3^{3}(r)dr
\end{align} where \begin{align}
G_3(r)=\int_r^{+\infty} \sqrt{1-\frac{r^2}{y^2}}
    |d F(y)|
\end{align} which represents $P(x_1^2+x_2^2>r^2)$ . In $d=3$ , I can write $H(p)$ in a similar form as $d=2$ : \begin{align}
H(p)=\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_3^{3}(r))|
\end{align} Then I encounter problem to obtain $H(p)$ for $d>3$ , since the derivation for $d=2,3$ relies on geometric insights. Any suggestions on this problem? Now I guess for $d\geq 2$ : \begin{align}
G_d(r) &= \int_r^{+\infty} (1-\frac{r^2}{y^2})^{\frac{d}{2}-1}|dF(y)|\\
H(p) & =\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_d^{d}(r))|
\end{align} However, I found difficulty to prove this conjecture. [1] Carnal, Henri. ""Die konvexe Hülle von n rotationssymmetrisch verteilten Punkten."" Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 15.2 (1970): 168-176.","['multivariable-calculus', 'integral-geometry', 'multivariate-statistical-analysis']"
4495813,Word length function on subgroup,"Let $G$ be a group finitely generated by a set $S$ with $S=S^{-1}$ . The word length of an element $g \in G$ (with respect to $S$ )  is defined as \begin{eqnarray}
|g|_S:= \min \{n \in \mathbb{N} \mid s_1...s_n =g \text{ for some }s_1,...,s_n\in S\}.
\end{eqnarray} For a subgroup $H \leq G$ , can we always find a finite generating set $T \subseteq H$ such that the restriction of $| \cdot |_S$ to $H$ coincides with the word length coming from $T$ ? If no, what about the case where $H$ is a (finite index) normal subgroup?","['combinatorics-on-words', 'group-theory']"
4495893,An invertible matrix is orthogonal if and only if the inverse is equal to the transpose on nonzero elements,"Let $A$ be an invertible real matrix, and suppose that $(A^{-1})_{i,j} = (A^{T})_{i,j}$ whenever $(A^{T})_{i,j}\ne 0$ . Is it true that $A$ is orthogonal? I found this statement in a paper without proof. The converse is true for obvious reasons. Trying to write $AA^{-1} =  A^{-1}A= I$ one can find that $A$ has all columns and rows of unitary norm, but not much more. This is sadly not enough to conclude that $A$ is orthogonal (there are explicit counterexamples).
Can you help? For those who asked, notice that for all $i,j$ , $$(A^{-1})_{i,j}(A)_{j,i} = (A^{-1})_{i,j}(A^{T})_{i,j} = (A^{T})^2_{i,j}$$ so $$1 = (A^{-1}A)_{i,i} = \sum_j (A^{-1})_{i,j}(A)_{j,i} = \sum_j (A^{T})^2_{i,j}$$ meaning that the norm of any column of $A$ is 1. From $AA^{-1}=I$ you get that also the norm of the rows is 1.","['hadamard-product', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'inverse']"
4495917,Why are meromorphic functions on a smooth projective curve rational?,"Let $C \subset \mathbb P^n$ be a smooth connected projective curve over $\mathbb C$ . Then the function field $k(C)$ consists of all functions $f$ which can locally (in the Zariski topology) be written as quotients of polynomial functions on $C$ . On the other hand, we can also think of $C$ as a connected compact Riemann surface. Then the field of meromorphic functions $\mathcal M(C)$ consists of all $f$ which can locally (in the analytic topology) be written as quotients of holomorphic functions. Some answers claim $k(C) = \mathcal M(C)$ . Clearly, $k(C) \subset \mathcal M(C)$ . How can I see the other inclusion?","['algebraic-curves', 'riemann-surfaces', 'projective-varieties', 'complex-geometry', 'algebraic-geometry']"
4495981,Lagrange Remainder multivariable functions,"Let $f: \mathbb{R^2}\rightarrow \mathbb{R}$ be $f(x,y)=y^2\cdot cos(2\pi x)$ Calculate the 2nd degree taylor polynomial at $(x_0,y_0)=(1,1)$ : $$
T_2[f,(1,1)]=f(x,y)+\langle \nabla f(x,y),\xi\rangle+\frac{1}{2}\langle\xi,Hessf\cdot\xi\rangle=-y^24\pi^2cos(2\pi x)\xi_1^2-4\pi\cdot y\cdot sin(x)\xi_1\xi_2-4\pi\cdot y\cdot sin(2\pi x)\cdot\xi_1\xi_2+2cos(2\pi x)=1+2\xi_2-2\pi^2\xi_1^2+\xi_2^2
$$ I believe my calculated Taylor Polynomial should be correct.
Now I am supposed to calculate the error.
Let $I=[0,2]x[0,2]$ , show that the following estimate is true: $$
\forall x\in I: \lvert f(x)-T_2[f(1,1)](x-(1,1))\rvert\leq\frac{16}{3}\pi^3+8\pi^2+2\pi
$$ There was a tip given to use triangle inequality but I decided to ignore that one for now.
I calculated the error: $$
R_2=\lvert \sum_{\lvert \alpha\rvert =3}\frac{D^\alpha f(\xi)}{\alpha !}\cdot(x-(1,1))^\alpha\rvert
$$ $$
\frac{\xi_2^2 8\pi^3sin(2\pi\xi_1)}{6}\cdot(x_1-1)^3+4\pi^2cos(2\pi\xi_1)\xi_2(x_1-1)^2\cdot(x_2-1)-2\pi sin(2\pi\xi_1)\cdot(x_1-1)\cdot(x_2-1)^2
$$ $\xi$ is between $1$ and $x_i$ and I want the term to reach its maximum.
Let $x_1=2$ and $x_2=2$ We now have: $$
\frac{\xi_2^2\cdot8\pi^3sin(2\pi\xi_1)}{6}-4\pi^2cos(2\pi\xi_1)-2\pi sin(2\pi\xi_1)
$$ This can't ever reach $\frac{16}{3}pi^3+8\pi^2+2\pi$ right? So where did I go wrong, is it my derivation or something else? I don't really know how to go further and can't find any examples for the lagrange remainder for more than one variable.","['partial-derivative', 'multivariable-calculus', 'taylor-expansion']"
4496066,Conditions for $A\cos^2(x)+B\sin^2(x)+C\sin(x)\cos(x)+D\cos(x)+E\sin(x)+F = 0$ to have (a) real root(s),I am doing research on Linear Algebra and encounter a function in the form like below: $$A\cos^2(x)+B\sin^2(x)+C\sin(x)\cos(x)+D\cos(x)+E\sin(x)+F = 0$$ What are the conditions that the coefficients of the equation need to satisfy for it to have (a) real root(s)? I am just wondering if there are some known results for the problem.,['trigonometry']
4496095,Different homomorphisms giving isomorphic semidirect product,"Let $H$ and $N$ be two groups, $\phi$ and $\psi$ be two homomorphisms $H \to \operatorname{Aut}(N)$ , and $G_1 = N \rtimes_\phi H$ and $G_2 = N \rtimes_\psi H$ corresponding semidirect products. What are conditions on $\phi$ and $\psi$ for $G_1 \cong G_2$ ? If for some automorphisms $h \in \operatorname{Aut}(H)$ and $n \in \operatorname{Inn}(\operatorname{Aut}(N))$ we have $$\phi = n \circ \psi \circ h\tag1$$ then $G_1 \cong G_2$ . At least for infinite groups, this condition is not necessary: let $X^\omega$ be direct sum of countable number of copies of $X$ , let $N = S_3^\omega \times \mathbb Z_3^\omega \times \mathbb Z_3^\omega$ and $H = \mathbb Z_2^\omega \times \mathbb Z_2^\omega$ , then trivial homomorphism and homomorphism that sends $i$ -th copy of $\mathbb Z_2$ of the first component of $H$ to non-trivial automorphism of $i$ -th copy of $\mathbb Z_3$ of the second component of $N$ , and sends all elements from the second component of $H$ to trivial automorphism,
both give group $\mathbb S_3^\omega \times \mathbb Z_6^\omega$ , but there are no automorphisms on $\operatorname{Aut}(N)$ and $H$ that transforms one of this homomorphisms to the other. Is condition $(1)$ necessary for finite groups? Is there some ""good"" general condition?","['group-theory', 'semidirect-product']"
4496099,The mice problem,"Problem. You have $2$ mice, and $9$ bottles, exactly one of which is poisoned. Each mouse can taste any combination of bottles at once, and if it is poisoned it will die after exactly one minute. Once a mouse is dead, it can't taste any more bottles. How many minutes do you need to determine exactly which bottle is poisoned? This question has me in a complete bind. Anyone who can point me in the right direction as to how to get started with this problem would be greatly appreciated.","['permutations', 'combinations', 'probability']"
4496139,"A question regarding Skorohod's representation theorem and joint weak convergence of $(X_{n-i},Y_n)$","Let $(X_n,Y_n)$ be random variables with $(X_n,Y_n) \rightarrow (X,Y)$ in distribution for $n \to \infty$ . By Skorohod's representation theorem , there exists a probability space with random variables $X_n',Y_n'$ , $X',Y'$ : $$(X_n,Y_n) \sim (X_n',Y_n'), \qquad (X,Y) \sim (X',Y'), \qquad (X_n',Y_n') \to (X',Y') \quad \text{a.s.}$$ Fix $i \in \mathbb{N}$ . What can we say about the distribution of $(X_{n-i},Y_n)$ ? E.g. can we get random variables from Skorohod's theorem, such that also: $$(X_{n-i},Y_n) \sim (X_{n-i}',Y_n')$$ Q: Is there a counterexample, where $(X_n,Y_n) \rightarrow (X,Y)$ in distribution, but $(X_{n-i},Y_n)$ fails to converge in distribution (to some limit)? Comment: If $(X_n,Y_n)$ are jointly Markov, then I think we get that all $(X_{n-i},Y_n)$ converge whenever $(X_n,Y_n)$ does.","['probability-theory', 'weak-convergence']"
4496143,If I have a seperable Hilbert space does any countably infinite set of linearly independent vectors span the space?,"If I have a seperable Hilbert space does ANY countably infinite set of linearly independent vectors span the space? Span in the sense that the set of all linear combinations of these vectors is dense in the Hilbert space. My intuition says yes, but I am concerned there is some subtlety regarding convergence that I am missing.","['quantum-mechanics', 'linear-algebra', 'functional-analysis']"
4496201,How does the Axiom of Choice plays role in this proof?,"I was reading a proof of the following theorem: Let $(X,d)$ be a metric space, if $(X,d)$ is complete and totally bounded then is compact (in the sequencial sense). Proof: Let $(x^{(n)})_{n=1}^{\infty}$ be a sequence in $X$ , the metric space is totally boundend then we can find $y_1,\dots, y_n\in X$ such that $$X= \bigcup_{i=1}^n B(y_i, 1)$$ notice that there must exists $1\leq k\leq n$ , such that for all $N\geq 1$ , there exist $n\geq N$ , and $x^{(n)}\in B(y_k,1)$ and we set: $$(1;1):= \min\{m\in\mathbb{N} : x^{(m)}\in B(y_k, 1)\}$$ $$(n; 1):= \min\{m\in\mathbb{N} : m > (n-1, 1), \ x^{(m)}\in B(y_k, 1)\} \quad\forall n\geq2$$ then $(x^{(n;1)})_{n=1}^{\infty}$ is a subsequence of $(x^{(n)})_{n=1}^{\infty}$ and $(x^{(n;1)})_{n=1}^{\infty}$ is contained in a $1$ -ball; we repeat this procedure with the sequence $(x^{(n;1)})_{n=1}^{\infty}$ using $1/2$ -balls to cover $X$ , then we obtain a subsequence $(x^{(n;2)})_{n=1}^{\infty}$ of $(x^{(n;1)})_{n=1}^{\infty}$ , such that $(x^{(n;2)})_{n=1}^{\infty}$ is contained in a single $1/2$ -ball; we repeat this procedure infinte times, then for all $j\geq 1$ we get a subsequence $(x^{(n;j)})_{n=1}^{\infty}$ of $(x^{(n)})_{n=1}^{\infty}$ , such that for each $j$ , the elements of $(x^{(n;j)})_{n=1}^{\infty}$ are contained in a single ball of radius $1/j$ , and also that each sequence $(x^{(n;j+1)})_{n=1}^{\infty}$ is a subsequence of the previous one $(x^{(n;j)})_{n=1}^{\infty}$ , finally we take the sequence $(x^{(n;n)})_{n=1}^{\infty}$ , this sequence is a subsequence of $(x^{(n)})_{n=1}^{\infty}$ , also is a Cauchy sequence and is convergent, since, the space is complete, then every sequence in $X$ has a convergent subsequence. My doubt is in the way we construct the sequence $(x^{(n,n)})_{n=1}^{\infty}$ , How we can use more rigorous arguments to construct that sequence? I suspect that we need Axiom of choice, so my attempt was: Let $\mathcal{C}$ be set of all subsequence $(y^{(n)})_{n=1}^{\infty}$ of $(x^{(n)})_{n=1}^{\infty}$ and define $\Omega:= \mathcal{C}\times \mathbb{N}$ with the relation $\sim$ defined as follows: $\big((y^{(n)})_{n=1}^{\infty}, \ k\big) \sim \big((z^{(n)})_{n=1}^{\infty}, \ j\big)$ iff $j=k+1$ and $(z^{(n)})_{n=1}^{\infty}$ is a subsequence of $(y^{(n)})_{n=1}^{\infty}$ such that $d(z^{(m)}, z^{(n)}) < 2/j$ for all $n,m\geq 1$ , with a similar prodecure to we used at the beginning of the proof we can show that for all $a\in \Omega$ there exist $b\in\Omega$ such that $a\sim b$ , for each $a\in\Omega$ we define $R(a):=\{b\in\Omega : a\sim b\}$ , with the axiom of choice we can find a function: $$f : \Omega\rightarrow \bigcup_{a\in\Omega} R(a)$$ such that $f(a)\in R(a)$ for all $a\in\Omega$ , so consider the sequence $\big[\big(f^k((x^{(n)})_{n=0}^{\infty},0\big)\big]_{k=1}^{\infty}$ , for each $j\geq 1$ we define the sequence $(x^{(n:j)})_{n=1}^{\infty}$ as the sequence of the first entry of $f^j((x^{(n)})_{n=0}^{\infty},0\big)$","['axiom-of-choice', 'set-theory', 'general-topology', 'analysis']"
4496284,Inequality $(1-e^x)\ln(1-xe^{-x})\leq x^2$,"This inequality, which appears to hold for all $x\in\Bbb R$ , $$(1-e^x)\ln(1-xe^{-x})\leq x^2$$ arose when I tried to prove that the positive root of $x-\frac{x^2}{a}-\log(x+1)$ is bounded above by $a-\log(a+1)$ . Note that for the question linked, it suffices to prove the inequality for $x>0$ , but I found it holds for $x<0$ too. The LHS is neither even nor odd so we will need to consider each case separately. Near zero, the bound is very tight; for example, expressing the inequality as an integral $$\int_0^x\left(1-3t-\frac{(t-1)^2}{e^t-t}-e^t \ln\left(1-t e^{-t}\right)\right)\,dt<0$$ fails since both $1-3t-\frac{(t-1)^2}{(e^t-t)}$ and $e^t\ln(1-te^{-t})$ are strictly decreasing. Can the inequality be analytically proven?","['calculus', 'inequality']"
4496356,Image of non-linear function $\mathbb{R}^2 \to \mathbb{R}^2$,"I have the function $F:\mathbb{R}^2 \to \mathbb{R}^2$ defined $F(x,y)=(x^2-y-1,y)$ and wish to find its image (range, whatever): the subset $F(\mathbb{R}^2)=\{F(x,y)|(x,y) \in \mathbb{R}^2\} \subseteq \mathbb{R}^2.$ Well, $\{F(x,y)|(x,y) \in \mathbb{R}^2\}=\{(x^2-y-1,y)|(x,y) \in \mathbb{R}^2\},$ so I am looking for ordered pairs $(u,v)$ satisfying $(u,v)=(x^2-y-1,y)$ . I got that $(x,y)=(\pm \sqrt{u + v + 1},v),$ but am not sure where to go from here, or if these first steps are even correct. I have also noticed that $F(-x,y)=F(x,y)$ , but I'm not sure what to do with this. What is the general procedure for these problems? When things are linear I know exactly what to do.","['multivariable-calculus', 'calculus', 'functions']"
4496364,What motivation is there for Sylow's Theorems?,"I want to know what kind of results or consequences which motivate Sylow's Theorems.
I mean for one who doesn't even know about statement of Sylow's Theorem, can motivate for it by some other consequences, like in linear algebra $1$ dimensional invariant subspace will directly motivate one to eigen values.","['group-theory', 'abstract-algebra', 'finite-groups']"
4496396,"Getting the area between $y$-axis, $f(x)$, $f(2-x)$ when both function is given by their subtraction?","For a polynomial $f(x)$ , let function $g(x) = f(x) - f(2-x)$ . $g'(x) = 24x^2 - 48x + 50$ What is the area between $y=f(x)$ , $y=f(2-x)$ , and $y$ -axis? My approach: $g(1) = f(1) - f(1) = 0$ . From $g'(x)$ , $g(x) = 8x^3 - 24x^2 + 50x + C$ . From 1 and 2, $C = -34$ . Since $f(x)$ is a cubic function, let $f(x) = ax^3 + bx^2 + cx + d$ and compare coefficients of $g$ and $f(x) - f(2-x)$ . $a = 4$ , $2b + c = 1$ . And I'm stuck. I can't see how I can get more info about $f(x)$ using these conditions and get areas out of it.","['cubics', 'definite-integrals', 'derivatives']"
4496397,What type of math should I use for this puzzle?,"I could use help modeling this puzzle. I'm not looking for a solution but I need help in phrasing the problem mathematically. I need a change in paradigm. My friend asked me for help with this puzzle but frankly, I'm not sure where to begin. I'm thinking maybe of a discrete math or maybe even a linear algebra approach but I've not taken either class in many years and am not sure. I tried discrete math but the expressions I got were too long to be reasonable. Linear algebra struck me only because it's matrix-based but I couldn't find a way to apply it (with further thought seems very much the wrong tool). Is there a way to set the problem up so that it can be solved in $O(1)$ time? I am looking up Sudoku algorithms at the moment and hopefully that'll help. Any suggestions to a posable algorithm or model would be great! Edit: I know I used a lot of buzz words in my original post. These edits should have fixed some of that. I don’t like the name of the puzzle either but I wanted to be somewhat academic in adding a source. To clarify for @Blitzer I just what to know what type of math to use. I spoke of big O notation because I didn’t know if it was possible to solve in a non-algorithmic manner. The responses I’ve got have been amazing and incredibly detailed. A big thank you to everyone for perpetuating such a helpful community!","['sudoku', 'puzzle', 'discrete-mathematics']"
4496409,Evaluating $\lim _{x \rightarrow 0} \frac{12-6 x^{2}-12 \cos x}{x^{4}}$,"$$
\begin{align*}
&\text { Let } \mathrm{x}=2 \mathrm{y} \quad \because x \rightarrow 0 \quad \therefore \mathrm{y} \rightarrow 0\\
&\therefore \lim _{x \rightarrow 0} \frac{12-6 x^{2}-12 \cos x}{x^{4}}\\
&=\lim _{y \rightarrow 0} \frac{12-6(2 y)^{2}-12 \cos 2 y}{(2 y)^{4}}\\
&=\lim _{y \rightarrow 0} \frac{12-24 y^{3}-12 \cos 2 y}{16 y^{4}}\\
&=\lim _{y \rightarrow 0} \frac{3(1-\cos 2 y)-6 y^{2}}{4 y^{4}}\\
&=\lim _{y \rightarrow 0} \frac{3.2 \sin ^{2} y-6 y^{2}}{4 y^{4}}\\
&=\lim _{y \rightarrow 0} \frac{ 3\left\{y-\frac{y^{3}}{3 !}+\frac{y^{5}}{5 !}-\cdots \infty\right\}^{2}-3 y^{2}}{2 y^{4}}\\
&=\lim _{y \rightarrow 0} \frac{3\left[y^{2}-\frac{2 y^{4}}{3 !}+\left(\frac{1}{(3 !)^{2}}+\frac{2}{3 !}\right) y^{4}+\cdots \infty\right)^{2}-3 y^{2}}{2 y^{4}}\\
&=\lim _{y \rightarrow 0} \frac{3\left\{y^{2}-\frac{2 y^{4}}{3 !}+\left\{\frac{1}{(3 !)^{2}}+\frac{2}{5 !}+y^{4}+\cdots \infty\right)-3 y^{2}\right.}{2 y^{4}}\\
&=\lim _{y \rightarrow 0}\left[\frac{-\frac{6}{3 !}+3\left\{\frac{1}{(3 !)^{2}}+\frac{2}{5 !}\right\} y^{2}+\cdots \infty}{2}\right]\\
&=\lim _{y \rightarrow 0}\left[\frac{-1+3\left\{\frac{1}{(3 !)^{2}}+\frac{2}{5 !}\right\} y^{2}+\cdots \infty}{2}\right]\\
&=-\frac{1}{2} \text { (Ans.) }
\end{align*}
$$ Doubt Can anyone please explain  the 5,6,7 equation line? Thank you",['limits']
4496419,Find that of $\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}$ equals $= \frac{\partial^2 f}{\partial u^2} + ...$,"Given: $$\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = \frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2}$$ where $$u = x \cos \theta + y \sin \theta$$ $$v = -x \sin \theta + y \cos \theta$$ I am trying to show how we can get: $$ \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} = \frac{\partial f}{\partial u} \sin \theta+ \frac{\partial f}{\partial v} \cos \theta$$ Now, we can see that if we take the derivative of $u = x \cos \theta + y \sin \theta$ with respect to $y$ we can get why $\frac{\partial v}{\partial y} = sin(\theta)$ and same for $\frac{\partial u}{\partial y} = sin(\theta)$ . Question : how we can get that $ \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} $ in the first place please as if we summed $ \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} $ we will get $ 2 \frac{\partial f}{\partial y}  $ and not $ \frac{\partial f}{\partial y}  $ ?","['partial-derivative', 'laplacian', 'multivariable-calculus', 'chain-rule']"
4496457,Prove that $\sum_{k = r}^{n} k(k - 1)(k - 2)\cdots(k - r+ 1)D_n(k) = n!$,"Prove that $$\sum_{k = r}^{n} k(k - 1)(k - 2)\cdots(k - r+ 1)D_n(k) = n!$$ where $D_n(k)$ represents the number of permutations of length $n$ with exactly $k$ fixed points. Hello, I am trying this problem. We can re-arrange the LHS to look like $$\sum_{k=r}^{n}\binom{k}{r}r!D_n(k)$$ Now, the combinatorial approach becomes apparent: This is just counting the number of pairs of type $$(\text{ordered array of length }r, \text{Permutations where these } r \text{ points are fixed points})$$ Another way to count this is by first picking the array in $\binom{n}{r}r!$ ways, and then multiplying this with the number of permutations where these $r$ points are fixed points: $(n -r)!$ . So, total = $$\binom{n}{r}r!(n-r)! = n!$$ I am also looking for algebraic approach. Does anybody have any idea how to algebraically prove this? Thanks.","['summation', 'fixed-points', 'binomial-coefficients', 'combinatorics', 'sequences-and-series']"
4496458,The convergence of the disintegrations of a sequence of measures,"Let $C = \{c : [0,1] \to \mathbb R ^n \mid c \text{ is continuous and } c(0)=0 \}$ be endowed with the Wiener measure $P$ . Consider an exhaustion $\mathbb R^n = \bigcup _{k \ge 0} U_k$ where each $U_k \ni 0$ is a connected open subset with smooth boundary. Let $C_k = \{ c \in C \mid c([0,1]) \subseteq \overline {U_k} \}$ . Clearly $C = \bigcup _{k \ge 0} C_k$ , an exhaustion. Endow each $C_k$ with the intrinsic Wiener measure $P_k$ (notice that $P_k$ is not the restriction of $P$ to $C_k$ !). It is known that $P_k (A \cap C_k) \to P(A)$ for every Borel $A \subseteq C$ . If $h : (0, \infty) \times \mathbb R^n \to (0, \infty)$ is the heat kernel on $\mathbb R^n$ , there exists a disintegration $(\mu_x) _{x \in \mathbb R^n}$ of $P$ such that $$P(A) = \int _{\mathbb R^n} \mathrm d x \, h(1, x) \, \mu_x (A)$$ for every Borel $A \subseteq C$ . ( $\mu_x$ is concentrated on the curves that end at $x$ for almost every $x \in \mathbb R^n$ .) Similarly, if $h_k : (0, \infty) \times \overline{U_k} \to (0, \infty)$ is the heat kernel on $\overline {U_k}$ , there exists a disintegration $(\mu_x ^k) _{x \in \overline{U_k}}$ of $P_k$ such that $$P_k(A) = \int _{\overline {U_k}} \mathrm d x \, h_k(1, x) \, \mu_x ^k (A)$$ for every Borel $A \subseteq C_k$ . I believe that if $A \subseteq C$ is Borel, then $\mu_x ^k (A \cap C_k) \to \mu_x (A)$ for almost every $x \in U_0$ ( $U_0$ is the smallest domain in the exhaustion). Is this true? Where can I find a proof?","['conditional-probability', 'measure-theory', 'probability-theory', 'wiener-measure']"
4496474,Topology Study Books,"I am interested in studying topology and in particular maps of the sphere to itself. However, I have no background in topology and I would prefer a book that avoids very technical proofs. Something like Topology for Physcists, Albert S. Schwarz . Any recommendations? Thank you in advance.","['general-topology', 'differential-topology', 'algebraic-topology']"
4496491,Calculation of the Acceleration of a Stepper Motor,I'm working on a stepper motor as part of my studies and while I was doing research I came across this paper I tried to walk my way through the equations and tried to find the proof for the Eq. 13. But I'm having a hard time going from Eq. 2 to Eq. 3. The author calculated the acceleration between c1 and c2. The derivation of the speed gives us the acceleration. But since we don't have the function as a variable of time I cannot calculate the derivative from the equation. Also I tried to calculate it from the change on the axes but it doesn't have the information on the time axes. This is the graph for the velocity against time The paper gives the velocity equation as: $$w = \frac{αf }{c}$$ It doesn't define the velocity as a function of time. And we need the get to the equation 3 the derivative of w. $$w' = \frac{2αf^2 (c_1-c_2) }{c_1c_2(c_1+c_2)}$$ I appreciate all your help me making this calculation. Thank you.,"['calculus', 'derivatives']"
4496494,Morphism of free groups that induces isomorphism on abelianizations,"I came up with the following question, that I'm not able to prove or disprove. Let $\phi: F_I \to F_J$ a morphism between the free groups generated by the sets $I$ and $J$ . This induces a morphism between the abelianizations $\phi_\mathrm{ab}: \oplus_I \mathbb{Z} \to \oplus_J \mathbb{Z}$ . Suppose that $\phi_\mathrm{ab}$ is an isomorphism. Does this imply that $\phi$ is an isomorphism as well? I am able to see that $|I| = |J|$ $\phi$ is injective But I'm not able to prove surjectivity nor to find a counterexample.","['free-groups', 'group-theory', 'group-isomorphism', 'abelian-groups']"
4496520,"Solving the Diophantine system $pqr=a^4$, $p+q+r=b^4$","I am trying to find solutions of the following system of diophantine equations: $$\left\{\begin{array}{rcl}pqr&=&a^4\\p+q+r&=&b^4\end{array}\right.$$ where $a$ , $b$ , $p$ , $q$ ans $r$ are positive integers such that $\gcd(p,q,r)$ is not divisible by $\theta^4$ , $\theta>1$ . I found the following solutions $(p,q,r)$ with a computer program : $(3\,;6\,;72)$ , $(25\,;60\,;540)$ , $(72\,;576\,;648)$ and $(162\,;448\,;686)$ . The system has infinitely many solutions : take $(p\;q\,;r)=\left(A^4\,;B^4\,;C^4\right)$ , where $A^4+B^4+C^4=D^4$ and $A$ , $B$ , $C$ are coprime (see this article ). But can we prove that there are infinitely many solutions using more elementary ways ? Thank you for your help !","['number-theory', 'elementary-number-theory', 'diophantine-equations']"
4496538,"Exercise 6, Section 3.2 of Hoffman’s Linear Algebra","Let $T$ be a linear transformation from $\Bbb{R}^3$ into $\Bbb{R}^2$ , and let $U$ be a linear transformation from $\Bbb{R}^2$ into $\Bbb{R}^3$ . Prove that the transformation $U\circ T$ is not invertible. Generalize the theorem. Potential approach: Assume towards contradiction, $U\circ T$ is invertible. By exercise 4 section 2 of Munkres’ topology, $T$ is injective and $U$ is surjective. We’ll only use $T$ is injective. By this post, $|\Bbb{R}|$ $=|\Bbb{R}^2|$ $=|\Bbb{R}^3|$ . Is the following lemma hold : If $f:A\to B$ is injective and $|A|=|B|$ , then $f$ is surjective ? Let assume for a moment lemma holds, $T$ is surjective. So $T$ is a bijective linear map. So $\Bbb{R}^3\cong \Bbb{R}^2$ . By exercise 6 section 3.3 , $\mathrm{dim}(\Bbb{R}^3)$ $= \mathrm{dim}(\Bbb{R}^2)$ $=3=2$ . Thus we reach contradiction. Hence $U\circ T$ is not even injective. Can you please verify lemma? Generalize theorem: Let $V,W$ be $n,m$ -dimensional vector space over field $F$ such that $n\gt m$ . If $T\in L(V,W)$ and $U\in L(W,V)$ , then $U\circ T$ is neither injective nor surjecctive. In particular, $U\circ T$ is not bijective. Approach(1): Assume towards contradiction, $U\circ T$ is injective. By exercise 4 section 2 of Munkres’ topology, $T$ is injective. So $N_T=\{0_V\}$ . $V$ is finite dimensional vector space. By rank nullity theorem, $n=\mathrm{dim}(V)$ $= \mathrm{dim}(N_T)+ \mathrm{dim}(R_T)$ $= \mathrm{dim}(R_T)$ . Since $R_T\leq W$ and $\mathrm{dim}(W)=m$ , we have $\mathrm{dim}(R_T)\leq \mathrm{dim}(W)=m$ . So $n= \mathrm{dim}(R_T)\leq m$ . But $n\gt m$ . Thus we reach contradiction. Hence $U\circ T$ is not injective. Assume towards contradiction, $U\circ T$ is surjective. By exercise 4 section 2 of Munkres’ topology, $U$ is surjective. So $R_U=V$ . $W$ is finite dimensional vector space. By rank nullity theorem, $m=\mathrm{dim}(W)$ $=\mathrm{dim}(N_U)+ \mathrm{dim}(R_U)$ $= \mathrm{dim}(N_U)+ \mathrm{dim}(V)$ $= \mathrm{dim}(N_U)+n$ . So $m= \mathrm{dim}(N_U)+n$ . which implies $\mathrm{dim}(N_U)$ $=m-n\lt 0$ . Thus we reach contradiction. Hence $U\circ T$ is not surjective. So $U\circ T$ is neither injective nor surjective. Approach(2): Assume towards contradiction, $U\circ T$ is injective. By exercise 4 section 2 of Munkres’ topology, $T$ is injective. Let $B_V=\{\alpha_1,,…,\alpha_n\}$ be basis of $V$ . So $(T(\alpha_1),…,T(\alpha_n))$ is sequence in $W$ . If $T(\alpha_i)=T(\alpha_j)$ , for some $i,j\in J_n$ , then $(T(\alpha_1),…,T(\alpha_n))$ is dependent. If $T(\alpha_i)\neq T(\alpha_j)$ , $\forall i\neq j$ , then $(T(\alpha_1),…,T(\alpha_n))$ is dependent, since $|\{ T(\alpha_1),…,T(\alpha_n)\}|=n\gt m$ . So $\exists (c_1,…,c_n)\in F^n$ such that $c_i\neq 0_F$ for some $i\in J_n$ and $\sum_{i=1}^nc_i\cdot_W T(\alpha_i)=0_W$ . Since $\{\alpha_1,…,\alpha_n\}$ is independent, we have $x=\sum_{i=1}^nc_i\cdot_V \alpha_i \neq 0_V$ . So $T(x)$ $=T(\sum_{i=1}^nc_i\cdot_V \alpha_i)$ $= \sum_{i=1}^nc_i\cdot_W T(\alpha_i)=0_W$ . So $0\neq x\in N_T$ . Which implies $T$ is not injective. Thus we reach contradiction. Hence $U\circ T$ is not injective. Assume towards contradiction, $U\circ T$ is surjective. By exercise 4 section 2 of Munkres’ topology, $U$ is surjective. Let $B_W=\{\beta_1,…,\beta_m\}$ be basis of $W$ . So $(U(\beta_1),…,U(\beta_m))$ is sequence in $V$ . $|(U(\beta_1),…,U(\beta_m))|\leq m\lt n$ . By theorem 4 corollary 2 section 2.3, $\mathrm{span}(U(\beta_1),…,U(\beta_m))\neq V$ . So $\exists x\in V$ such that $x\notin \mathrm{span}(U(\beta_1),…,U(\beta_m))$ . By exercise 8 section 3.1 , $\mathrm{span}(U(\beta_1),…,U(\beta_m))=R_U$ . Which implies $U$ is not surjective. Thus we reach contradiction. Hence $U\circ T$ is surjective. So $U\circ T$ is neither injective nor surjective.","['proof-writing', 'functions', 'linear-algebra', 'solution-verification', 'linear-transformations']"
4496576,Study the convergence of $\sum_{n=1}^{\infty}\frac{\sin (n\sqrt{n})}{\sqrt{n}}.$,"I try to use the following result: If $f(x)\in C^1[1,+\infty)$ and $\displaystyle\int_1^{+\infty} |f'(x)|{\rm d}x$ is
convergent, then $\displaystyle\sum_{n=1}^{\infty} f(n)$ has the same convergence
or divergence as $\displaystyle\int_1^{+\infty} f(x) {\rm d}x$ . Now, define $f(x):=\dfrac{\sin x^{3/2}}{x^{1/2}}$ . Then \begin{align*}  \int_1^{+\infty}|f'(x)|{\rm d}x&=\int_1^{+\infty}\left|\frac{3}{2}\cos x^{3/2}-\frac{\sin x^{3/2}}{2x^{3/2}}\right|{\rm d}x\\ &=\int_1^{+\infty}\left|\frac{\cos x}{x^{1/3}}-\frac{\sin x}{3x^{4/3}}\right|{\rm d}x\\ &\ge \int_1^{+\infty}\frac{|\cos x|}{x^{1/3}}-\frac{|\sin x|}{3x^{4/3}}{\rm d}x=+\infty, \end{align*} which is not convergent.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4496596,A stopping time problem for a random walk with transition probabilities dependent on states,"The Problem: In a one-dimensional random walk, at position $n > 0$ , the probability of moving to $(n-1)$ is $\frac{n+2}{2n+2}$ , and the probability of moving to $(n+1)$ is $\frac{n}{2n+2}$ . Starting at position $n$ , what is the average time to reach position $0$ ? Question 1: Let $v(n)$ be the expected time of moving from $n$ to $0$ . Then we have the following recursive equation: $$v(n) = 1 + \frac{n+2}{2n+2} v(n-1) + \frac{n}{2n+2} v(n+1)$$ And obviously $v(0) = 0$ , but to solve the equation we also need to know $v(1)$ , which I cannot compute. So my first question is: How to compute $v(1)$ ? p.s. Using random simulation on a computer, I have estimated that $v(1) = 3$ . Then the recursive equation can be solved to obtain $v(n) = n(n+2)$ . But I do not know how to compute $v(1)$ theoretically. Question 2: Another way to solve this problem is to use martingale.
Let $X_t$ be the random position at time $t$ , and $X_0 = n$ .
Let $$Y_t = (X_t+1)^2 + t$$ then $\{Y_t, t \ge 0\}$ is a martingale with respect to $\{X_t, t \ge 0\}$ .
Let $T$ be the first visit time from $n$ to $0$ (stopping time). If the optional stopping theorem is applicable , then $$E(Y_T)= (0+1)^2+E(T) = E(Y_0) = (n+1)^2 + 0$$ So we have $$E(T) = n(n+2)$$ However, for the optional stopping theorem to be applicable, I have to firstly prove $E(T) < \infty$ . So my second question is: How to prove the expected stopping time $E(T) < \infty$ ? Thanks","['random-walk', 'markov-process', 'martingales', 'stopping-times', 'probability']"
4496646,Is there an algorithm to check that a subgroup of a CAT$(0)$ group is *not* quasiconvex?,"Let $G$ be a finitely generated CAT $(0)$ group and $H$ a subgroup. If $H$ is quasiconvex then it is finitely generated, so we can immediately conclude that any non-finitely generated subgroup of $G$ is not quasiconvex. Let us assume then that $H$ is finitely generated, and suppose $H$ is given as a finite set of generators $X\subset G$ . Question: Is there an algorithm which takes as input the group $G$ (in some suitable format) and the set $X$ and outputs whether $H=\langle X\rangle$ is not quasiconvex? I am specifically interested in the case of a right-angled Coxeter system $(W,S)$ which is CAT $(0)$ as seen by making it act on their Davis complex - a CAT $(0)$ cube complex. In [1], Pallavi Dani and Ivan Levcovitz give a procedure ( standard completion sequence ) which takes in a finite set $X\subset W$ and which terminates after finitely many steps if and only if $\langle X\rangle$ is quasiconvex, see Theorem 8.4. I want to know if there is an algorithm, ie a procedure which terminates after finitely many steps, which tells me that a subgroup is not quasiconvex. I have searched on MSE and MO, as well as Googling the literature on the subject but found nothing. I know there are certain invariants which can help prove in certain cases that a subgroup is not quasiconvex (eg growth rate), but this does not amount to a generally applicable algorithm. I have asked the question in the very general setting of CAT $(0)$ groups, but would be happy with an algorithm which works in a restricted setting, eg CAT $(-1)$ groups; groups which act on a CAT $(0)$ cube complex (compare Theorem 8.4 mentioned above with Theorem 5.1 in [2]); right-angled Coxeter groups, etc. If it turns out that this problem is undecidable, that would also be interesting. [1] Pallavi Dani and Ivan Levcovitz Subgroups of right-angled Coxeter groups via Stallings-like techniques , 2021. [2] Michael Ben-Zvi, Robert Kropholler, and Rylee Alanza Lyman Folding-like techniques for CAT $(0)$ cube complexes , 2020. Edit: The definition of quasiconvex subgroup I am using is as follows: a subgroup $H$ of $G$ is $M$ -quasiconvex, for $M \ge 0$ , if any geodesic path in the Cayley graph of $G$ with endpoints in $H$ lies in the $M$ -neighborhood of $H$ . $H$ is quasiconvex if it is $M$ -quasiconvex for some $M$ . Whether or not a subgroup is quasiconvex does not depend on which finite generating set of $G$ is used to construct the Cayley graph.","['finitely-generated', 'coxeter-groups', 'algorithms', 'geometric-group-theory', 'group-theory']"
4496677,Help learning and understanding polynomial factorizations,"A number theory book I'm reading used this factorization as a main step for a proof: Given $m > n$ , integers, $$\left( a^{2^{m}} - 1 \right) = \left( a^{2^{m-1}} + 1 \right)\left( a^{2^{m-2}} + 1 \right) \left( a^{2^{m-3}} + 1 \right) \cdots \left( a^{2^n} + 1 \right) \left( a^{2^n} - 1 \right)$$ Original from the book (in Portuguese): I'm having a bit of trouble trying to understand how to derive it systematically (in a simple way, without lots of calculations). If this a known common factorization? Im I missing something?
If so (or not) could one suggest source to learn such factorizations? Sorry for the possibly relatively low effort question. I just tried to find material on such factorizations and could not find. Thanks in advance.","['number-theory', 'factoring', 'polynomials']"
