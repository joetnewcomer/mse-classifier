question_id,title,body,tags
4256639,"If $f(x)=\frac{e^x}{x}$, evaluate $\lim\limits_{n\to\infty}\frac{f^{(n)}(1)}{n!}$.","If $f(x)=\frac{e^x}{x}$ , evaluate $$\lim_{n\to\infty}\frac{f^{(n)}(1)}{n!}$$ where $f^{(n)}(x)$ is the $n$ -th derivative of $f(x)$ . This is from a local contest held in my city last year. I tried by calculating first few derivatives of $f(x)$ to have an estimation of $f^{(n)}(x)$ and got the following results: $$\begin{align} f'(x) &= \frac{e^x(x-1)}{x^2}\\ f''(x) &= \frac{e^x(x^2-2x+2)}{x^3}\\ f'''(x) &= \frac{e^x(x^3-3x^2+6x-6)}{x^4}
\end{align}$$ As it can be seen, the derivatives are getting complicated in each step and I can't expect to have a nice expression for $f^{(n)}(x)$ . Then how do I solve the problem? Also, I would like to share my current knowledge of the topic to receive an answer that I can understand. I'm currently a high schooler and am learning calculus as an amateur. I have done almost all elementary concepts of differentiation and limits. I have not learnt integration yet. I don't know about Taylor and Maclaurin series but I know the expansions of $e^x$ , $\ln(1+x)$ etc. I hope the answerers will consider these and provide a detailed solution if needed. Edit: As figured out in the comments, the limit should be $$\lim_{n\to\infty}\bigg|\frac{f^{(n)}(1)}{n!}\bigg|.$$ Sorry for the inconveniences.","['contest-math', 'limits', 'calculus', 'limits-without-lhopital']"
4256691,When does a contractible simple loop have a contraction where every intervening loop is simple?,"If $\gamma:S^1\to X$ is a simple contractible loop, when can we say there must by a contraction, $H(s,t)$ such that $\gamma_t:s\mapsto H(s,t)$ is a simple loop for all $t<1?$ (1) It seems like you should be able to do this if $X$ is a manifold. (Of dimension $>1,$ but there are no retractable simple loops in manifolds of dimension $1.$ ) Maybe it requires dimension $>2?$ (2) I’d like to find a Hausdorff counterexample, or a proof that we can always find $H$ in those cases. When there is a counterexample, the base case would be a quotient of the closed disk, with no more than one point on the boundary in each equivalence class. With $\sim$ defined cleverly enough, then $D^2/\sim$ with the loop $S^1\to D^2\to D^2/\sim $ would be an example. A simple non-Hausdorff counterexample is $u\sim v$ if both $|u|,|v|<1/2.$ This quotient is non-Hausdorff, since an equivalence class is not closed. The singleton in $D^2/\sim$ consisting of the equivalence class of $0$ is open in $D^2/\sim.$ So you can’t cross $[0]$ to retract to another point - $\gamma_{t}^{-1}([0])$ must be an open set. And you can’t retract to $[0]$ point since $H^{-1}(\{[0]\})$ would have to be open, so must contain an $(s,t)$ with $t<1.$ If $X$ and $\gamma$ are a counterexample, then this means in particular that there is a $P:D^2\to X$ with $P_{|S^1}=\gamma.$ If $X$ is Hausdorff, then when we define $u \sim_P v$ iff $P(u)=P(v),$ then $Y=D^1/\sim_P$ is homeomorphic to $P(D^2),$ since $Y$ is compact and $P(D^2)$ is Hausdorff. If we can’t find a proper retraction in all of $X,$ we can’t find one in $P(D^2)$ and hence not in the case of $\gamma’:S^1\to D^2/\sim_P.$ So any Hausdorff counterexample gives us a Hausdorff counterexample of the form $D^2/\sim$ for some equivalence relation $\sim.$","['manifolds', 'general-topology', 'homotopy-theory', 'retraction']"
4256692,"Solve the differential equation. $y^\prime = y^2+\frac{1}{x^4}$, $y=\frac{1}{x^2}\text{ctg}(\frac{1}{x}+c) - \frac{1}{x}$ and $y(+\infty)=0$","Solve the differential equation. $$y^\prime = y^2+\frac{1}{x^4}.$$ I am given this as a solution.Need to choose one which satisfies given condition.( $y(+\infty)=0$ ) $$y=\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x}$$ and $$y(+\infty)=0.$$ We need to find $c$ and show that it is solution for differential equation. So $$\lim_{x\to +\infty}\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x} =0
\\
\lim_{x\to+\infty}\frac{1}{x^2}\frac{\cos(\frac{1}{x}+c)}{\sin(\frac{1}{x}+c)} =0$$ From here I don't know how to find $c$ . Will be glad if you can help me.","['integration', 'definite-integrals', 'ordinary-differential-equations', 'real-analysis', 'calculus']"
4256727,The umbral calculus proof of the higher order product rule,"unfortunately, I seem to be quite unable to come up with the correct umbral calculus proof of the identity $$
\frac{\mathrm{d}^{n}\left(fg\right)}{\mathrm{d}x^{n}}\left(x\right) = \sum_{k=0}^{n}{\binom{n}{k} f^{\left(k\right)}\left(x\right) g^{\left(n-k\right)}\left(x\right)}.
$$ I tried to write $\frac{\mathrm{d}}{\mathrm{d}x}$ as an element of a ring in which $f$ and $g$ might be idempotents, but the problem is that we are multiplying and not adding $f$ and $g$ . I then tried and failed to find the proof on the internet. I'd be enourmously grateful for any courteous hints.","['derivatives', 'calculus', 'umbral-calculus', 'combinatorics']"
4256734,Expectation of a multivariate Gaussian after going through a Softmax,"Let $\varepsilon\sim N(0, I_D)$ be a $D$ -dimensional random vector, distributed normally with mean $0$ and covariance given by the identity matrix of size $D$ . In some computations I'm doing in my research, the following expectation arises: $$\mathbb{E}_{\varepsilon\sim N(0, I_D)}\left[\frac{\exp(\varepsilon_i b_i)}{\sum_{k=1}^D \exp(a_k + \varepsilon_kb_k) )}\right],$$ where $a, b\in\mathbb{R}^D$ are some $D$ -dimensional vectors. I wonder if this expectation has a closed form . It looks gnarly, though. The fact that passing a Gaussian through a sigmoid also has to be approximated gives me even less hope.","['expected-value', 'probability']"
4256741,Uniform probability measure on integers and arithmetic progressions,"Does there exist a probability measure on the integers such that, the probability of any two arithmetic progressions with the same difference part, is the same? We assume the probability measure is defined over the power set of the integers. Hence, a probability measure corresponds to a sequence of positive (non-negative) numbers $\{p_z\}_{z \in \mathbb{Z}}$ which sum up to one.","['measure-theory', 'puzzle', 'statistics', 'arithmetic-progressions', 'probability']"
4256761,$\int_\gamma\frac{1}{\sqrt z}dz$ where $\gamma$ is the lower half of the unit circle from $+1$ to $-1$,"To solve this I put $\gamma(t)=e^{-it}$ with $t\in(0,\pi)$ . Then $$
\int_\gamma z^{-\frac{1}{2}}dz=-\int_0^\pi e^{\frac{it}{2}}ie^{-it}dt=-i\int_0^\pi e^{-\frac{it}{2}}dt=2(e^{i\pi/2}-e^{0})=-2-2i.
$$ But if I use $\sigma(t)=e^{it}$ , $t\in(\pi,2\pi)$ , then $$
\int_\gamma z^{-\frac{1}{2}}dz=-\int_\sigma z^{-\frac{1}{2}}dz=-\int_\pi^{2\pi}e^{-\frac{it}{2}}ie^{it}dt=-i\int_\pi^{2\pi}e^{\frac{it}{2}}dt=-2(e^{\pi i}-e^\frac{\pi i}{2})=2+2i.
$$ I cant see where is the problem. I'll thank you for any help.",['complex-analysis']
4256773,"Suppose $M\leq P$ such that $|M|=p^m$, and both $M$ and $P/M$ are cyclic, then there exists $N\leq P$ such that $P=M+N$ and $M\cap N=\{0\}$.","Question: Let $P$ be an abelian $p$ -group with exponent $p^m$ .  Suppose $M\leq P$ such that $|M|=p^m$ , and both $M$ and $P/M$ are cyclic, then there exists $N\leq P$ such that $P=M+N$ and $M\cap N=\{0\}$ . Thoughts: I've wrestled with trying to get the ""trivial"" cases out of the way first, but, for instance, if $M=P$ , then $N$ couldn't even exist, so $M\neq P$ .  I've tried thinking about the orders of the groups $M$ and $N$ and tried playing with them, but I must not be doing something right, because I am never using that $M$ and $P/M$ are cyclic, which I'm sure (I suppose) are essential in the question... any help is greatly appreciated!","['cyclic-groups', 'abstract-algebra', 'p-groups', 'group-theory', 'abelian-groups']"
4256774,"Why multiplication of 142857 with 2,3,4,5,6 gives the same digits shifted?","I was reading about a not so practical way to determine the divisibility of a number by $7$ . At some point the following number is mentioned: $142857$ (which is the result of $\frac{999999}{7}$ ) and apparently this number as I have verified if multiplied by $2,3,4,5, 6$ it gives the same digits in different order e.g. $142857 \cdot 3 = 428571$ Why does this number have this property? I see that the numbers $2,3,4,5,6$ are all remainders if we divide $100, 10, 10000, 100000, 1000$ by $7$ respectively but I am not sure if there is any correlation with the property. I'd like to understand the intuition behind this ""trick"" Note: I have found a similar post but I don't see any explanation on this","['algebra-precalculus', 'arithmetic', 'recreational-mathematics']"
4256781,Proof that $f_{n} = \chi_{\Omega_{n}}f \to f$ in $L^{2}$,"Let $f: \mathbb{R} \to \mathbb{C}$ be an unbounded measurable function and let $\mu$ be a finite measure on the Borel $\sigma$ -algebra $\mathbb{B}(\mathbb{R})$ such that: $$\int_{\mathbb{R}}|f(x)|^{2}d\mu(x) < +\infty$$ For each $n \in \mathbb{N}$ , let $\Omega_{n} := \{x \in \mathbb{R}: |f(x)| \le n\}$ and $\chi_{\Omega_{n}}$ its characteristic function, i.e. $\chi_{\Omega_{n}}(x) = 1 $ if $x \in \Omega_{n}$ and zero otherwise. I'm trying to prove that the sequence of functions $f_{n} := \chi_{\Omega_{n}}f$ is Cauchy and converges in $L^{2}(\mathbb{R},\mu)$ to $f$ . My work so far is as follows: $$||f_{n}-f_{m}||^{2} = \int_{\mathbb{R}}|\chi_{\Omega_{n}}f-\chi_{\Omega_{m}}f|^{2}d\mu = \int_{\mathbb{R}}|\chi_{\Omega_{n}}-\chi_{\Omega_{m}}|^{2}|f|^{2}d\mu$$ Now, assuming $m \le n$ , we notice that $\Omega_{m}\subseteq \Omega_{n}$ , so that $|\chi_{\Omega_{n}}-\chi_{\Omega_{m}}| = \chi_{\Omega_{n}}-\chi_{\Omega_{m}} = \chi_{\Omega_{n}\setminus \Omega_{m}}$ . Hence, $$||f_{n}-f_{m}||^{2} = \int_{\mathbb{R}}\chi_{\Omega_{n}\setminus\Omega_{m}}|f|^{2}d\mu = \int_{\Omega_{n}\setminus\Omega_{m}}|f|^{2}d\mu$$ But I'm stuck here. How can I prove this is Cauchy?","['measure-theory', 'lp-spaces', 'functional-analysis', 'analysis']"
4256801,"Find $arg \min_{f} \mathbb{E}_{xy}(y - f(x))^2$ where x, y are random variable","$x, y : \mathbb{X} \rightarrow \mathbb{R} $ - are random variable I need to find function f, that minimize $\mathbb{E}_{xy}(y - f(x))^2 $ - mathematical expectation of the joint density of random variables x, y. My solution is $$(\mathbb{E}_{xy} y^2 - 2\mathbb{E}_{xy}yf(x) + \mathbb{E}_{xy}f^2(x))|_{f}' = $$ $$ = ( \mathbb{E}_{xy} y^2 - 2 \mathbb{E}_{x} \left[ \mathbb{E} (yf(x) | x) \right] + \mathbb{E}_{x} \left[ \mathbb{E} (f^2(x) | x) \right])|_{f}'= $$ $$=\left|f(x) \ is \ x-measurable \right| = $$ $$= (-2 \mathbb{E}_x(f(x) \mathbb{E}(y | x)) + \mathbb{E}_{x} \left[ f^2(x) \right])|_{f}'=  $$ $$  = (-2 \mathbb{E}_x(f(x) \mathbb{E}(y | x)) + \mathbb{E}_{x} \left[ f^2(x) \right])|_{f}'= $$ $$ = \mathbb{E}_x \left[ (-2f(x)\mathbb{E(y |x)} +f^2(x))|_f' \right] =  $$ $$  = \mathbb{E}_x \left[ -2\mathbb{E(y |x)} + 2f(x) \right] = 0 \ (a.e.) $$ $$\Rightarrow f(x) = \mathbb{E}(y |x)$$ But i'm not sure if it is possible to differentiate by function like that. Please tell me am i right?","['expected-value', 'derivatives', 'probability-theory', 'random-variables']"
4256828,Trying to prove the Pythagorean theorem using Picks theorem.,"Picks Theorem Let A be the area of a simply closed lattice square. Let B denote the number of lattice points on the square edges and I the number of points in the interior of the square. Then $\large A=I + \frac{B}{2}-1$ Define three squares with areas $\large A_{a} = I_{a} + \frac{B_{a}}{2}-1 = a^2$ $\large A_{b} = I_{b} + \frac{B_{b}}{2}-1 = b^2$ $\large A_{c} = I_{c} + \frac{B_{c}}{2}-1 = c^2$ Theorem For $B_{c}=4$ , $A_{c} = A_{a} + A_{b}$ . Proof Case $B_{c}=4$ , $\large A_{c} = I_{c} + \frac{4}{2}-1$ $\large A_{c} = I_{c} + 1$ Observe that $\large I_{c} = A_{a} + A_{b} - 1$ Substituting $I_{c}$ $\large A_{c} = (A_{a} + A_{b} - 1) + 1$ $\large A_{c} = A_{a} + A_{b}$ $\therefore \large c^2 = a^2 + b^2 $ Questions Is that a valid proof for specific case $B_{c}=4$ ? For general cases, can Picks be applied to prove Pythagoras? Thanks.","['euclidean-geometry', 'solution-verification', 'geometry']"
4256829,Mid-point convex measurable subset of $\mathbb{R}$ with positive Lebesgue measure is an interval,"The question is right as the title: Let $E$ be a measurable subset of $\mathbb{R}$ w.r.t. Lebesgue measure, and has positive measure. For any $x,y\in E$ , $\frac{x+y}{2}\in E$ . Prove that $E$ is an interval(like $[a,b],[a,b),(a,b)$ etc., possibly infinity endpoint). The hint is to find some function on it, and I tried looking at its characteristic function, but I have no clue.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4256850,Gluing schemes to get $x_0^2+x_1^2-x_2^2=0$,"Exercise 4.5.A from Vakil asks to think through how to define a scheme that should be
interpreted as $x_0^2+x_1^2-x_2^2=0$ ""in $\mathbb P^2_k$ "". According to the hint, I need to consider the following three affine schemes (here $x_{0/2}=x_0/x_2$ and other indices are defined similarly), assuming I didn't mess up anything: $X_0=\operatorname{Spec} k [x_{1/0}, x_{2/0}]/(x_{2/0}^2-x_{1/0}^2-1)$ $X_1=\operatorname{Spec} k [x_{0/1},x_{2/1}]/(x_{2/1}^2-x_{0/1}^2-1)$ $X_2=\operatorname{Spec} k [x_{0/2},x_{1/2}]/(x_{0/2}^2+x_{1/2}^2-1)$ From what I understand, to say how we glue them, we need to define the following: open subschemes $X_{ij}\subset X_i$ such that $X_{ii}=X_i$ isomorphisms $f_{ij}:X_{ij}\to X_{ji}$ such that $f_{ii}$ is the identity and then prove that $C.\text{   }f_{ik}|_{X_{ij}\cap X_{ik}}=f_{jk}|_{X_{ji}\cap X_{jk}}\circ f_{ij}|_{X_{ij}\cap X_{ik}}$ The first question is whether what I wrote above is correct (""ideologically"" at least), and the second question is what should $X_{ij}$ and $f_{ij}$ be?","['algebraic-geometry', 'abstract-algebra', 'schemes', 'projective-space']"
4256882,"Are $\sin x$, $\sin(x + \frac{\pi}{6})$, and $\sin(x + \frac{\pi}{3})$ linearly independent?","I'm specifically interested in the dimensionality of the subspace spanned by $\{\sin x, \sin(x + \frac{\pi}{6}), \sin(x + \frac{\pi}{3})\}$ in $C^0(\mathbb{R}, \mathbb{R})$ , the vector space of continuous functions from $\mathbb{R}$ to $\mathbb{R}$ . I think the answer is, yes, they are linearly independent, but I'm not confident about the best way to demonstrate this. Would a good approach be to do Taylor expansions at $x_0 \in \{0, \frac{\pi}{6}, \frac{\pi}{3}\}$ , and examine the polynomial coefficients? This seems messy. Wondering if there's a simpler way to think about this.","['function-spaces', 'trigonometry', 'linear-algebra']"
4256930,If the derivative of $f:R^m\to R^m$ is isometric then $f$ is isometric,"I can't finish this problem Let $f:R^m\to R^m$ be a $C^1$ function such that for all $x\in R^m$, $f'(x):R^m\to R^m$ is an isometry i.e. $|f'(x)v|=|v|$ for all $v\in R^m$. Prove that $f$ is an isometry, i.e. that $|f(x)-f(y)|=|x-y|$ for all $x,y\in R^m$. Conclude that there is a linear isometry $T:R^m\to R^m$ and $a\in R^m$ such that $f(x)=Tx+a$ for all $x\in R^m$. This is what I've done:
Since $|f'(x)v|=|v|$ for all $x,v$ we have that $|f'(x)|=1$. The mean value inequality then says that $|f(x)-f(y)|\le 1\cdot |x-y|=|x-y|$. Ideally one would like to apply the previous inequality with $f^{-1}$ in place of $f$ but we don't know that $f$ is bijective. I tried to use the inverse function theorem but it only assures the inverse of $f$ when $f$ is restricted to some open sets. So how would one prove that $f$ is bijective?. After that I think i can finish the problem.","['derivatives', 'analysis']"
4256951,Is the random graph ideal an $F_\sigma$-ideal?,"Let $\{X_n: n \in \omega\}$ be an independent family of subsets of $\omega$ such that $n\in X_m$ iff $m\in X_n$ , for all $n,m \in \omega$ . The random graph can be defined by $\mathcal{R}=(\mathbb{N}, E)$ where $E:=\{\{n,m\} : n \in X_m\}$ . The random graph ideal $I_{\mathcal{R}}$ is the ideal generated by cliques and free sets in the random graph, i.e., $A\in I_{\mathcal{R}}$ if and only if there exists $A_1, \dots, A_n$ sets homogeneous for $c:[\omega]^2 \rightarrow \{0,1\}$ defined by $c(\{n,m\})=0$ iff $\{n,m\} \in E$ such that $A \subseteq \bigcup_{i=1}^n A_i$ . My question is:
Is $I_{\mathcal{R}}$ an $F_\sigma$ subset of $2^\omega$ ?","['general-topology', 'descriptive-set-theory', 'set-theory']"
4256954,"Derivation of Laplace transform of Bessel function, where does initial condition go?","I'm trying to understand the derivation of the Laplace transform of $J_0(t)$ given in Spiegel's ""Laplace Transforms"" book, on p. 23 of my copy. One way is to use the power series representation for $J_0(t)$ . The other way is to take the Laplace transform of Bessel's differential equation $tJ_0''(t) + J_0'(t) + t J_0(t) = 0$ , using the initial conditions $J_0(0) = 1$ , $J_0'(0)=0$ , however there's something that's seriously confusing me. When we do the second method, using the properties of the LT, we arrive at $$
-\frac{d}{ds} (s^2 y -s -0) +(sy -1) - \frac{dy}{ds} = 0.
$$ Then, doing algebra and solving the separable ODE, we get $y(s) = \frac{c}{\sqrt{s^2+1}}$ , and then use the initial-value theorem to get $c=1$ . The thing I don't understand is that the initial condition $Y'(0) = 0$ is not registered by this method, since it disappears when we take the derivative above, also we don't use it in the initial-value theorem, we use instead $Y(0) = 1$ . But I thought two initial conditions were required to uniquely determine a 2nd order ODE like this? Also, doesn't the power series solution use $Y'(0) = 0$ , and then they agree? I have a feeling that this has something to do with the fact that there are two solutions to this ODE, the solutions of the first and second kind, but I don't really understand well what is happening. Is anyone able to enlighten me? Greg",['ordinary-differential-equations']
4256974,Is my process of proving this problem correct when $2x$ & $3x$ are not acute angles?,"Problem: Prove $$\cot^{-1}(\tan 2x)+\cot^{-1}(-\tan 3x)=x$$ My proof: $$\begin{align}\text{L.H.S}&=\cot^{-1}(\tan 2x)+\cot^{-1}(-\tan3x) \\
&=\cot^{-1}(\cot (\frac{\pi}{2}-2x))-\cot^{-1}(\cot(\frac{\pi}{2}-3x))\\
&=3x-2x\\
&=x\\
&=\text{R.H.S(proved)}\end{align}$$ Will my proof still be valid if $2x$ or $3x$ is in any quadrant other than the 1st quadrant? What will be the correct proof that will hold true for any quadrant? Is this identity only true when 2x & 3x are in the first quadrant?",['trigonometry']
4257002,a regular curve with $\kappa(t)>0$ is helix if and only if $\frac{\kappa}{\tau}$ is constant,"A curve is said to be helix if its tangent line have a constant angle with a fixed direction. i.e. $\langle T(t),u\rangle$ is constant for some unit vector $u$ . I am trying to prove: a regular curve with $\kappa(t)>0$ is helix if and only if $\frac{\kappa}{\tau}$ is constant. I can show the sufficient part： if $\alpha(t)$ is helix, then $\langle T'\!, u\rangle=0$ and therefore $\langle N,u\rangle=0$ , which implies $\langle-\kappa(t)T(t)+\tau(t)B(t), u\rangle=0$ . We then have $$
\frac{\kappa(t)}{\tau(t)} 
= \frac{\langle T(t),u\rangle}{\langle B(t),u\rangle},
$$ which is constant, since $\langle T(t),u\rangle,\langle B(t),u\rangle$ are constant. But I have no idea in proving the reverse direction.","['frenet-frame', 'curvature', 'differential-geometry']"
4257003,"Given $n>2$ and $1<k<n$. Is it possible to exist $n$ events(probability>0), s.t. for any $k$ among them are independent while any $k+1$ are not?","For example when $k=2$ , consider $n+1$ disjoint events : $E=\{ e_0,e_1,...,e_n\}$ with the probability that $P(e_0)=a,P(e_i)=b,1\leq i\leq n.$ We define $n$ events $E_1,E_2,...,E_n$ such that $E_i=\{e_0\} \cup\{ e_i \}$ .For any $1\leq i<j<k\leq n$ we obtain: $P(E_i)P(E_j)P(E_k)=(a+b)^3,P(E_i)P(E_j)=(a+b)^2,P(E_i \cap E_j)=a,P(E_i\cap E_j\cap E_k)=a.$ We can easily prove the existence of  (a,b) satisfying $(a+b)^2=a,0<a,b<1,a+nb<1$ for any given n, which means that for any $2$ events are independent while any $3$ are not. So a more genreal question is that given $n>2$ and $1<k<n$ . Is it possible to exist $n$ events, s.t. for any $k$ among them are independent while any $k+1$ are not? Beased the proof above, we know that k=2 is OK.","['elementary-set-theory', 'discrete-mathematics', 'probability-theory', 'probability']"
4257025,The series $\sum_{n=1}^{\infty} x^{\sum_{k=1}^{n} 1/k}$ converge?,"The series $\sum_{n=1}^{\infty} x^{\sum_{k=1}^{n} 1/k}$ converge?
For $x>0$ I was looking at some things and I noticed that when $x>1$ $$\lim x^{\sum_{k=1}^{n}1/k} = \infty$$ And therefore, the series in is divergent. The problem is that I do not know very well if the series converges or diverges when $0<x<1$ . Appreciate your help!","['power-series', 'sequences-and-series']"
4257045,$p = (u + v)^2 + uv$ for primes $p = 5k \pm 1$? [duplicate],"This question already has answers here : Primes for >1 (good expression) (2 answers) Closed 2 years ago . I came up with the following conjecture which I am unable to prove. For all primes $p$ which can be represented as either $5k + 1$ or $5k - 1$ for some positive integer $k$ , we can find positive integers $u$ , $v$ such that $p = (u + v)^2 + u \times v$ For example, $31 = 5 \times 6 + 1 = (2 + 3)^2 + 2 \times 3$ Using a computer, I have verified it for $p < 10000$ .
Is there a general proof for this?","['number-theory', 'conjectures', 'prime-numbers']"
4257075,"Characterization of nonabelian group $G$ such that for all $x,y\in G$, $xy\neq yx\implies x^2=y^2$.","Let $G$ be a nonabelian group such that for all $x,y\in G$ , $xy\neq yx\implies x^2=y^2$ . (Or equivalently, for all $x,y\in G$ , either $xy=yx$ or $x^2=y^2$ .) Let $g\in G\setminus Z(G)$ . Then $gh\neq hg$ for some $h\in G$ . Therefore $gh^{-1}\neq h^{-1}g$ and $g^{-1}h\neq hg^{-1}$ . By the assumption, we get $g^2=g^{-2}=h^2=h^{-2}$ . This implies that $g^4=1$ . Next let $z\in Z(G)$ . Since $(zg^{-1})h\neq h(zg^{-1})$ , it follows that $(zg^{-1})^2=h^2$ , which implies that $z^2=g^2h^2=1$ . In short, $G$ is a $2$ -group of exponent $4$ . Such a group exists as the quaternion group $Q_8$ fulfils the property. Are there any characterization on groups with this property? Or are there any other group that fulfils the property?","['group-theory', 'abstract-algebra']"
4257104,"Assume $f,g, fg\in L^1(\mathbb{R}^n)$ and $\widehat{f}\in L^1(\mathbb{R}^n)$. Prove that $\widehat{fg}=(2\pi)^{-n}\widehat{f}*\widehat{g}$","To demonstrate this exercise we will use the following theorem: Theorem: Let $f\in L^1(\mathbb{R}^n)$ and assume that $\widehat{f}\in L^1(\mathbb{R}^n)$ . Then $f$ is equivalent to a continuous function. Therefore, we assume with no loss of generality that $f$ is continuous. Then for $x\in\mathbb{R}^n$ , $$f(x)=(2\pi)^{-n}\displaystyle{\int_{\mathbb{R}^n}\widehat{f}(\xi)e^{ix\cdot\xi}\,d\xi}.$$ by the previous theorem and by definition of Fourier transform \begin{align*}
\widehat{fg}(\xi)&=\int_{\mathbb{R}^n}f(x)g(x)e^{-ix\cdot\xi}\,dx\\ &= \int_{\mathbb{R}^n}\left[(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]g(x)e^{-ix\cdot\xi}\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}g(x)e^{-ix\cdot\xi}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix(\xi-\eta)}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\left[\int_{\mathbb{R}^n}g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\widehat{g}(\xi-\eta)\,d\eta\\&=(2\pi)^{-n}\left(\widehat{f}*\widehat{g}\right)(\xi).
\end{align*} Remark: Note that Fubini's theorem has been applied to the fifth line of the chain of equalities above, which is possible since $\widehat{f}(\eta)g(x)\in L^1(\mathbb{R}^n\times\mathbb{R}^n)$ which implies that $\displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g}\hspace{.2cm}$ exists. Indeed: \begin{equation}
\left|\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g\right|\leq\int_{\mathbb{R}^n\times\mathbb{R}^n}|\widehat{f}g|=\|\widehat{f}g\|_{L^1}\leq\|\widehat{f}\|_{L^1}\|g\|_{L^1}<\infty\qquad (1).
\end{equation} My concerns about the above proof are: The chain of equalities immediately above is correct to justify that $\displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g}$ exists? Also I'm not sure if the last inequality in $(1)$ is true. Did I apply Fubbini's theorem correctly?","['fourier-analysis', 'fourier-transform', 'real-analysis', 'lp-spaces', 'functional-analysis']"
4257147,What are books/notes on coordinate-free statistics?,"There are some books here , but all focus on linear models. Do we have books/notes with broader coverages? In case of linear models, “coordinate-free” basically means “matrix-free” and uses the theory of vector space, and the viewpoint is geometric. (Outside of linearity, would we go into something like Information Geometry?) Edit : It seems Information Geometry and Topological Data Analysis are related to this question. But books on IG and TDA tend to collect recent research results, and the problems solved are specialized. Rather, I’d like to see coordinate-free reconstructions of main stream statistical methods (maybe also advanced ones), and this would also help to learn IG and TDA, proper.","['statistical-inference', 'statistics', 'book-recommendation', 'reference-request', 'soft-question']"
4257155,Perceptron Mistakes,"In this problem, we will investigate the perceptron algorithm with different iteration ordering. Consider applying the perceptron algorithm through the origin based on a small training set containing three points: $x^{(1)} =[-1,-1], y^{(1)}=1$ $x^{(2)} =[1,0], y^{(2)}=-1$ $x^{(3)} =[-1, 1.5], y^{(3)}=1$ Given that the algorithm starts with $θ^{(0)}=0$ , the first point that the algorithm sees is always considered a mistake. The algorithm starts with some data point and then cycles through the data (in order) until it makes no further mistakes. Now assume that $x^{(3)}=[−1,10]$ . How many mistakes does the algorithm make until convergence if cycling starts with data point x(1)? Also provide the progression of the separating plane as the algorithm cycles in the following list format: $[[θ^{(1)}1,θ^{(1)}2],…,[θ^{(N)}1,θ^{(N)}2]]$ , where the superscript denotes different θ as the separating plane progresses. For example, if $θ$ progress from $[0,0]$ (initialization) to $[1,2]$ to $[3,−2]$ , you should enter $[[1,2],[3,−2]]$ number of mistakes of Perceptron algorithm if the algorithm starts with $x^{(1)}$ = $2$ the progression of the separating hyperplane of the Perceptron algorithm if the algorithm starts with $x^{(1)}$ . = $[[-1,-1], [-2, 9]]$ However, my answer seems wrong and input is a matrix of incorrect shape Does anyone has an idea what I am doing wrong does I make mistake while calculating the perceptron mistakes?","['machine-learning', 'discrete-mathematics', 'computer-science']"
4257177,Is this statement about inverse $\cot$ valid to write?,"I was curious if one can bring the negative sign out as we do in case of $\sin^{-1}(x)$ , $\tan^{-1}(x)$ , & $\csc^{-1}(x)$ . Can one do the same for $\cos^{-1}(x)$ , $\sec^{-1}(x)$ , & $\cot^{-1}(x)$ ? For example, $$\cot^{-1}(-\tan(x))$$ $$=-\cot^{-1}(\tan(x))...(i)$$ Can I write (i)? I'm asking this because if $x=45^{\circ}\text{(any acute angle)}$ , $$\cot^{-1}(-\tan(45^{\circ}))$$ $$=-\cot^{-1}(\tan(45^{\circ}))$$ $$=-\cot^{-1}(1)$$ $$=-45^{\circ}$$ Now, $-45^{\circ}$ is outside the restricted range of $\cot^{-1}(x)$ : $(0,\pi)$ . So, is (i) valid to write?",['trigonometry']
4257229,"Evaluating $\int e^{-\sqrt{a^2 - b^2} \cosh(x)} \, dx$","I've been trying to solve the definite integral \begin{align*}
    I = \int_{0}^{u} e^{-\sqrt{a^2 - b^2} \cosh(x)} \, dx \, ,
\end{align*} with $u = \mathrm{arctanh}(\frac{b}{a})$ and $a > b, \, a > 0$ . Performing the change of variables $x = \mathrm{arccosh}(y)$ , we can rewrite the integral \begin{align*}
    I = \int_{1}^{\cosh(u)} \frac{1}{\sqrt{y^2 - 1}} e^{-\sqrt{a^2 - b^2} \, y} \, dy \, .
\end{align*} I feel like it should be possible to evaluate this, and my intuition tells me that the indefinite integral will come out as an error function since it looks very similar to \begin{align*}
    \int \frac{1}{\sqrt{x}} e^{-\alpha x} \, dx = \sqrt{\frac{\pi}{\alpha}} \mathrm{erf}(\sqrt{\alpha x})
\end{align*} for $\alpha > 0$ . I've looked through Gradshteyn and Ryzhik, and Prudnikov, Brychkov, and Marichev, but haven't found anything quite what I'm looking for. Does anyone else know how to approach this integral? P.S. The original integral was \begin{align*}
    \int_0^\infty dx \, e^{-\sqrt{a^2 - b^2} \cosh(x - u)} \, ,
\end{align*} which I split up into something that gives a modified Bessel function of the second kind, and the other term is the integral I have above.","['integration', 'definite-integrals', 'elementary-functions', 'indefinite-integrals', 'error-function']"
4257238,"Show $(\mathbb{E}[Z\mid\mathcal{F}_t],\mathcal{F_t})_{t \geq0}$ is an *uniformly integrable* martingale.","Suppose $Z \in \mathcal{L}^1(P)$ . I want to show that $(\mathbb{E}[Z\mid\mathcal{F}_t],\mathcal{F_t})_{t \geq0}$ is an uniformly integrable martingale. I have managed to show to it is a martingale but I am stuck trying to show is uniformly integrable as well. I have tried looking at the sufficient condition that $$
\sup_{t\geq 0} \mathbb{E}[(\mathbb{E}[Z\mid\mathcal{F}_t])^2] < \infty
$$ and my idea was to use Jensens inequality but that doesn't seem to work. I would also need to somehow use that $Z \in \mathcal{L}^1(P)$ as I haven't used that yet.","['martingales', 'uniform-integrability', 'probability-theory']"
4257264,Presentation of Grothendieck-Witt group $GW(\mathbb{F})$ in terms of generators and relations.,"Let $\mathbb{F}$ be a field, which for the sake of this discussion, is such that char $\mathbb{F} \neq 2$ . By Corollary 9.4 in Scharlau's Quadratic and Hermitian Forms, the Grothendieck-Witt group $GW(\mathbb{F})$ is generated by elements $\langle \alpha \rangle, \alpha \in \mathbb{F}^{\times}$ , subject to the relations $\langle \alpha \rangle = \langle \alpha \beta^{2} \rangle$ for all $\alpha , \beta \in \mathbb{F}^{\times}$ . $\langle \alpha \rangle + \langle \beta \rangle = \langle \alpha + \beta \rangle + \langle (\alpha + \beta)\alpha \beta \rangle$ . I understand the proof of this result as presented in Scharlau. However, in Morel's $\mathbb{A}^{1}$ Algebraic topology over a field, lemma 2.9, he says that the second relation may be obtained from the first relation and the relation $\langle \alpha \rangle + \langle -\alpha \rangle = 1 + \langle -1\rangle$ . I understand the motivation behind this relation (matrices of the form on the LHS are congruent to the hyperbolic plane when char $\mathbb{F} \neq 2$ ), but cannot formally derive the second relation using this and relation 1). I am probably just missing a trick. Any help would be much appreciated!","['algebraic-k-theory', 'group-presentation', 'group-theory', 'algebraic-topology', 'quadratic-forms']"
4257317,The infinity Wasserstein distance $W_\infty$ and the weak topology,"Let $X$ be a compact metric space. The $\infty$ -Wasserstein distance $W_\infty$ on the space $P(X)$ of Borel probability measures on $X$ can be described as $$W_\infty(\mu,\nu) = \inf\{r>0 \mid \mu(U)\le\nu(U_r)\,\forall \text{ open } U\subseteq X\},$$ where $U_r=\{x\in X \mid d(x,U)\le r\}$ . The topology induced by this distance is in general finer than the weak (or vague) topology. For example, if $X=[0,1]$ and $\delta_x$ denotes the point mass at $x$ , the measures $\mu_n=\frac{n-1}{n}\delta_0+\frac{1}{n}\delta_1$ converge weakly to $\delta_0$ , but $W_\infty(\mu_n,\delta_0)=1$ for every $n$ . On the other hand, $W_\infty$ dominates the Levy-Prokhorov distance, so $W_\infty$ -convergence does imply weak convergence. Question : is $W_\infty$ -convergence equivalent to weak convergence if one restricts to fully supported measures? (Edit: by fully supported I mean those measures $\mu$ such that $\mu(U)>0$ for every nonempty open set $U\subseteq X$ .) I believe this is the case at least for $X=[0,1]$ , but I just want to make sure it is not obviously ridiculous!","['measure-theory', 'metric-spaces']"
4257324,$\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx$ [duplicate],"This question already has an answer here : Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? [closed] (1 answer) Closed 2 years ago . The first problem I encountered $$\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx$$ It $\displaystyle\int_{-\infty}^{\infty} e^{-x^2} dx$ I tried to make it variable interchangeable, like in your problem, but I didn't get any results. How can you prove that a function has no closed form integral? Then I learned a lot through this post. But still $$\int_{a}^{b} f(x)^{g(x)} dx$$ $$\begin{array}{I|l|l|I}f(x)&g(x)\\ \hline \sin{(x)}&\cos{(x)} \\ \cos{(x)}&\sin{(x)} \\ \tan{(x)}&\cot{(x)} \\ \arcsin{(x)}&\arccos{(x)} \\ \tan{(x)} &\sin{(x)} \\ ...&...\end{array}$$ I realized I didn't know how to calculate specific integrals of its type. I'm looking for a solution to your first problem, can you help me? Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? It's the same problem here, but it looks like there's been no answer. That's why I wanted to ask you again. $WolframAlpha$ although he says there is a value to a particular integrale $$\lim_{x\to\pi^{-}} \sin{(x)}^{\cos{(x)}}=\exp{(-\log{0})}=+\infty$$ is.","['integration', 'definite-integrals']"
4257345,Convergence in probability in metric spaces,"Let $X,X_1,X_2\dots$ be random variables defined on a probability space $(\Omega,\mathcal{F},P)$ and taking values in a metric space $(S,d)$ . We write $X_n\overset{p}{\to}X$ if $P[d(X_n,X)\geq \epsilon]\to 0$ for all $\epsilon>0$ . Question: Do we need $S$ to be separable for this to make sense? Wikipedia indicates yes, but this answer indicates no. The definition on page 27 of Billingsley's Convergence of Probability Measures puts no restriction on $S$ , but the definition on page 287 of Dudley's Real Analysis and Probability imposes separability. The distance function $d:S\times S\to \mathbb{R}$ is continuous with respect to the product topology on $S\times S$ , and so is $\mathcal{B}(S\times S)$ measurable. On the other hand if $X,X_n:\Omega\to S$ are both Borel measurable, then the product map $(X,X_n):\Omega\to S\times S$ is $\mathcal B(S)\otimes \mathcal B(S)$ measurable. So it seems we need separability to ensure $\mathcal{B}(S\times S)=\mathcal B(S)\otimes \mathcal B(S)$ so that the composition $d(X_n,X)$ is measurable. However if $X=c$ a constant, then I believe this restriction is not needed. Is this correct? Thanks for your help.","['measure-theory', 'separable-spaces', 'metric-spaces', 'measurable-functions', 'probability-theory']"
4257498,Composite of uniform distributions,"If $X_1$ is uniform $(0,1)$ and $X_2$ is uniform $(0, X_1+1)$ ,
what is $X_3$ , which is characterized as uniform $(0, X_2+1)$ ? I simulated $X_3$ and got the following graph but found it hard to compute the precise distribution:","['probability-distributions', 'uniform-distribution', 'probability']"
4257524,Is $\varnothing\in\{\varnothing\}$ and $\varnothing\subseteq \{\varnothing\}?$,"Is $\varnothing\in\{\varnothing\}$ and $\varnothing\subseteq \{\varnothing\}?$ As per me yes they are true because, in the first statement, the set $\{\varnothing\}$ contains one single element, $\varnothing$ , the empty set. In the second statement because of the fact that an empty set is a subset of every set What about $\{\varnothing\}\subseteq \varnothing$ ?",['elementary-set-theory']
4257532,Hartshorne Problem II.8.1 b,"Here is the problem statement: Let $B$ be a local ring containing a perfect field $k$ such that $B$ is the localization of a finitely generated $k$ -algebra. Let $k(B)$ be the residue field of $B$ . Then show that $B$ is regular if and only if $\Omega_{B/k}$ is free of rank $\text{dim}B+\text{tr.d.}k(B)/k$ . I can shof the if part but not the only if part. Here is hat I've done: From part (a) we have a s.e.s of $k(B)-$ vector spaces $$0\to m/m^2\to \Omega_{B/k}\otimes_B k(B)\to \Omega_{k(B)/k}\to 0.
$$ We get $$\text{dim}_{k(B)}(\Omega_{B/k}\otimes_B k(B))=\text{dim}_{k(B)}(m/m^2)+\text{dim}_{k(B)}(\Omega_{k(B)/k}).$$ But by theorem 8.6A the second term on the right is $\text{tr.deg}(k(B)/k)$ . So we get $B$ is regular $\iff\dim_{k(B)}(m/m^2)=\dim B\iff   \dim_{k(B)}(\Omega_{B/k}\otimes_B k(B))=\dim B+\text{tr.deg}(k(B)/k)$ . Now assume $\Omega_{B/k}$ is free of rank $\text{dim} B+\text{tr.deg}(k(B)/k)$ . Then $\text{dim}_{k(B)}(\Omega_{B/k}\otimes_B k(B))=\text{dim} B+\text{tr.deg}(k(B)/k)$ and $B$ is regular by the equivalence above. For the converse this is how I started: assume $B$ is regular. Then by the above $\text{dim}_{k(B)}(\Omega_{B/k}\otimes_B k(B))=\text{dim} B+\text{tr.deg}(k(B)/k)$ . Also, $B$ is an integral domain and if $K$ is its field of fraction then (by proposition 8.2A) $$\Omega_{B/k}\otimes_B K\cong \Omega_{K/k}.$$ As $B$ is a localization of a finitely generated $k$ -algebra $K$ is a finitely generated extension of $k$ . Since $k$ is perfect we get $K$ is separably generated over $k$ . Therefore (by theorem 8.6A) $\text{dim}_K(\Omega_{K/k})=\text{tr.d}(K/k)$ .
Now my idea here was to show that $\text{tr.d}(K/k)=\text{dim}B+\text{tr.d}(k(B)/k)$ and then use lemma 8.9 from Hartshorne to conclude but I don't know how to prove this. If anyone could help me finish this approach or show me how to solve it in some other way I would be very grateful!","['algebraic-geometry', 'commutative-algebra']"
4257619,"Is curvature independent of ""domain scaling""?","$\def\vv#1{\mathbf{\vec{#1}}}
\def\unitvv#1{\mathbf{\hat{#1}}}
\def\derivative#1#2#3{\frac{d^{#3}#1}{{d#2^{#3}}}}$ I'm not sure if ""domain scaling"" is the correct word, but I had a problem: Find the curvature of the curve $\vv r(t)= (9 + \cos 6t - \sin 6t)\unitvv i + (9 + \sin 6t + \cos 6t)\unitvv j+ 4 \unitvv k$ To do this, I used the formula that I had already derived for the curvature: $\kappa := \frac{d\vv T}{ds} = \frac{\|{\vv{r'}\times\vv{r''}}\|}{{{\|\vv{r'}}\|}^3}$ It was an absolute mess with all sorts of factors of $6$ flying around the page, and I kept losing track of my work, so I decided to try and rescale by using $ u := \frac{t}{6}$ into: $\vv r(u) = (9+\cos u - \sin u)\unitvv i + (9 + \sin u + \cos u) \unitvv j + 4 \unitvv k$ . I justified that the curvature would be the same by lemma: Lemma 1: $ u(t) = \alpha t ; \alpha \in \mathbb{R \implies}\kappa (\vv r(t)) = \kappa(\vv r(u))$ Proof: $\kappa (\vv r(u)) = \frac{\|{{\derivative{\vv r}{u}{}}\times{\derivative{\vv r}{u}{2}}}\|}{{{\|\derivative{\vv r}{u}{}}\|}^3} = 
\frac{\|({\derivative{t}{u}{}\derivative{\vv r}{t}{} )\times((\derivative{t}{u}{})^2(\derivative{\vv r}{t}{2}) + (\derivative{t}{u}{2})(\derivative{\vv r}{t}{}))}\|}{{\|\derivative{t}{u}{}\derivative{\vv r}{t}{}\|}^3} 
= \frac{\|({\alpha^{-1}\derivative{\vv r}{t}{} )\times(\alpha^{-2}(\derivative{\vv r}{t}{2}) + 0(\derivative{\vv r}{t}{}))}\|}{{\|\alpha^{-1}\derivative{\vv r}{t}{}\|}^3} 
= \frac{\|\alpha^{-3}\|\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\alpha^{-3}\|\|\derivative{\vv r}{t}{}}\|}^3} 
= \frac{\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\derivative{\vv r}{t}{}}\|}^3} = \kappa(\vv r(t))$ It got me the right answer of $\frac{\sqrt2}{2}$ in my example problem, but I can't seem to wrap my head around it conceptually from what my idea of $\kappa$ represents. Perhaps I made an error in my lemma that made this only work because the curvature was a constant? If not, maybe my idea of $\kappa$ is wrong and someone can give be a better intuition of it.","['curves', 'multivariable-calculus', 'vector-analysis']"
4257622,How to solve $\sum_{n=-\infty}^\infty\frac{y^2}{[(x-n\pi)^2+y^2]^{3/2}}$?,"I need to solve this sum: $$\sum_{n=-\infty}^\infty\frac{y^2}{[(x-n\pi)^2+y^2]^{3/2}}.$$ Do you have any ideas for how I could do this? I know that this sum: $$\sum_{n=-\infty}^\infty\frac{y}{(x-n\pi)^2+y^2},$$ can be simplified by using the following identities (see this Wikipedia article ): $$\cot(x+iy) = \sum_{n=-\infty}^\infty\frac{1}{x-n\pi + iy},$$ $$\sum_{n=-\infty}^\infty\frac{y}{(x-n\pi)^2+y^2}=\frac{i}{2}\sum_{n=-\infty}^\infty\frac{1}{x-n\pi+iy}-\frac{1}{x-n\pi-iy}.$$ Hence, $$\begin{aligned}
\sum_{n=-\infty}^\infty\frac{y}{(x-n\pi)^2+y^2}&=\frac{i}{2}[\cot(x+iy)-\cot(x-iy)] \\
&=\frac{\sinh(2y)}{\cosh(2y)-\cos(2x)}.
\end{aligned}$$ Do you know if a similar trick can be used to solve the original sum?","['summation', 'electromagnetism', 'complex-analysis', 'trigonometry', 'complex-numbers']"
4257623,Regularity conditions for the approximation of the mean of order statistics (i.e. when is $E[X_{(pn)}] \approx F^{-1}(p) $),"Let $X_{(1)} \le X_{(2)}\le ... \le X_{(n)}$ be a sequence of order statistics generate from an i.i.d. sequence $X_1, X_2, ... , X_{n}$ . We are interested in expected value of $X_{(pn)}$ , for $p \in (0,1)$ , that is \begin{align}
E[X_{(pn)}],
\end{align} we assume $pn$ is always an integer or just consider $E[X_{( \lceil pn \rceil)}]$ . A commonly used, large $n$ , approximation of expectation  is \begin{align}
E[X_{(pn)}] \approx F^{-1}(p) 
\end{align} where $F$ is cdf of the original distribution and $F^{-1}$ the quantile function (inverse cdf). Question: Under what regularity conditions  is this approximation valid?  In other words, under what regularity conditions do we have that \begin{align}
\lim_{n \to \infty} | E[X_{(pn)}] - F^{-1}(p) |=0?
\end{align} What Have I done: I have looked at books like ""First Course on Order Statistics"" by Bary Arnold and other references. While these approximations appear there and in other sources, I was not able to find conditions under which this approximation is valid. The proof that I was able to find  goes as follows: \begin{align}
E[X_{(pn)}]=&E[F^{-1}(U_{(pn)})] \text{ where  $U_{(pn)}$ is order statistics of uniform distribution}\\
&=E[F^{-1}(p) -\frac{d}{du}F^{-1}(u)|_{u=D} (p-U_{(pn)})]
\end{align} where in the above we used Taylor's reminder theorm and where the random variable $D$ is between $p$ and $U_{(pn)}$ .  This leads to \begin{align}
E[X_{(pn)}]
&=F^{-1}(p) - E \left[\frac{1}{f(F^{-1}(D))} (p-U_{(pn)})\right]
\end{align} where we used that $\frac{d}{du}F^{-1}(u)=\frac{1}{f(F^{-1}(u))}$ and where $f$ is pdf of $F$ . At this point we need to show that \begin{align}
\lim_{n \to \infty} E \left[\frac{1}{f(F^{-1}(D))} (p-U_{(pn)})\right]=0.
\end{align} However, I was not able to find conditions that guarantee this.  The key is to flip the expectation and limit, but I was not able to find a dominating random variable for this.","['expected-value', 'probability-theory', 'order-statistics']"
4257659,Notation for sum over element wise multiplication,"Im looking for  a  typical notation  for  the  sum  over the elements after an element-wise  multiplication  of two matrices $A$ , $B$ (hadamard  product). Is  it  correct  to  write $\sum  A \odot B$ without  further  specifiying  what $\sum$ is doing  or  do  i  need  to  use  row  and column  indices?
Thanks!","['matrices', 'hadamard-product']"
4257689,Perturbation of the spectrum of a matrix by adding small decaying coefficients.,"BACKGROUND & MOTIVATION: We consider in a complex infinite-dimensional Hilbert space a bounded operator $T$ . We pick a Hilbert basis $(e_n)_{n\in\mathbb{Z}}$ and project $T$ onto it: for all $N\in\mathbb{N}^*$ , we let \begin{align*}
T_N&:=(\langle e_m,Te_n\rangle)_{|m|,|n|\leq N}
\end{align*} where $\langle\cdot,\cdot\rangle$ is the inner product of the Hilbert space. The main motivation is to determine whether $0$ is in the spectrum of $T$ or not. We assume that $T$ is of the form $\mathrm{1}+K$ where $K$ is trace-class, so that $T$ is Fredholm of index $0$ : in particular, its spectrum consists in eigenvalues and $0$ is an eigenvalue if and only if the Fredholm determinant $\det(\mathrm{1}+K)$ cancels. Since $T_N\to T$ as $N\to+\infty$ in the operator norm topology, we have: \begin{align*}
\lim_{N\to+\infty}\det((\langle e_m,Te_n\rangle)_{|m|,|n|\leq N})&=\det(T).
\end{align*} Since $K$ is compact, we do know that the non-zero eigenvalues of $K_N:=(\langle e_m,Ke_n\rangle)_{|m|,|n|\leq N}$ converge to non-zero eigenvalues of $K$ . Now the goal is to give an estimate on the rate of convergence of the eigenvalues of $K_N$ with, say, modulus greater than $1/4$ , to their limits in the spectrum of $K$ . In the literature, such an estimate relies on bounding the resolvent $(K-\lambda)^{-1}$ for $\lambda$ on a small contour enclosing an eigenvalue of $K$ , which is not feasible in practice. Fix $N\in\mathbb{N}^*$ and $N'>N$ . To simplify notations and put the problem into a general form, let $A_N\equiv A\in\mathcal{M}_{2N+1}(\mathbb{C})$ (so $A$ plays the role of $K_N$ above) and \begin{align*}
A'&:=\begin{pmatrix}E_1&E_2&E_3\\E_4&A&E_5\\E_6&E_7&E_8\end{pmatrix}\in\mathcal{M}_{2N'+1}(\mathbb{C})
\end{align*} where $E_1,E_3,E_6,E_8\in\mathcal{M}_{N'-N}(\mathbb{C})$ , $E_2,E_7\in\mathcal{M}_{N'-N,2N+1}(\mathbb{C})$ and $E_4,E_5\in\mathcal{M}_{2N+1,N'-N}(\mathbb{C})$ have $\|\cdot\|_1$ and $\|\cdot\|_\infty$ norms $\leq\varepsilon$ ; we also have that $\sum_{k}|E_j|_{kk}\leq\varepsilon$ by the trace-class property of $K$ . Typically, the coefficients of $A'$ decay as ""we move to the exterior"" of $A$ ', that is: \begin{align*}
&|(E_2)_{j,k}|\lesssim\frac{1}{(N'+j)^2};&&|(E_7)_{j,k}|\lesssim\frac{1}{(N+j)^2};\\
&|(E_4)_{j,k}|\lesssim\frac{1}{(N'+k)^2};&&|(E_5)_{j,k}|\lesssim\frac{1}{(N+k)^2};\\
&|(E_1)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N'+k)^2};&&|(E_6)_{j,k}|\lesssim\frac{1}{(N'+k)^2},\frac{1}{(N+j)^2};\\
&|(E_3)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N+k)^2};&&|(E_8)_{j,k}|\lesssim\frac{1}{(N+k)^2},\frac{1}{(N+j)^2}.
\end{align*} Using the formula \begin{align*}
\lambda\mathrm{1}_{2N+1}&=\frac{1}{2\pi\mathrm{i}}\oint(A-\mu)^{-1}\mu\mathrm{d}\mu
\end{align*} for all eigenvalue $\lambda$ of $A$ , we know that there exists $\delta(\varepsilon)>0$ such that \begin{align*}
\mathrm{Spec}(A')\setminus\overline{D(0,1/4)}\subset\mathrm{Spec}(A)\setminus\overline{D(0,1/4)}+D(0,\delta(\varepsilon))\tag{$\star$}
\end{align*} where $\mathrm{Spec}$ denotes the spectrum of the corresponding matrix; here we removed the closed discs $\setminus\overline{D(0,1/4)}$ as we do not interest ourselves in the spectrum that accumulate at 0 (by compactness of $K$ ). I would like to get an explicit estimate of $\delta(\varepsilon)$ . The matrix $A$ is not normal . One way I see to prove the statement for the largest (in modulus) eigenvalues is to use the definition of the spectral radius as $\lim_{n\to+\infty}\|A^n\|^{\frac{1}{n}}$ ; but this implies to compute the coefficients of $A^n$ : we can see that we obtain a matrix whose each line consists in a sum of $3^n$ products of the matrices $E_j$ and $A$ , and indeed the term $A^n$ is obtained on the line $k$ for $N'+1\leq k\leq N'+2N+1$ . I do not see a clean way to proceed then as terms containing $A^\ell$ with $\ell\leq n-1$ do not vanish at the limit $n\to+\infty$ (and could blow in norm as $n\to+\infty$ ). Another thing I tried without success is the following: let $u\in\mathbb{C}^{2N+1}$ and $\lambda\in\mathbb{C}$ such that $Au=\lambda u$ with $|\lambda|>1/4$ . We look for $u'\in\mathbb{C}^{2N'+1}$ and $\lambda'\in\mathbb{C}$ such that $A'u'=\lambda'u'$ of the form \begin{align*}
u'&=\sum_{k=0}^{+\infty}\varepsilon^ku_k,\qquad\qquad\lambda'=\sum_{k=0}^{+\infty}\varepsilon^k\lambda_k
\end{align*} where $u_0=(0,u,0)$ and $\lambda_0=\lambda$ . The idea was then to eliminate terms of order $\varepsilon^k$ with an appropriate choice of $u_k$ and $\lambda_k$ -- perhaps by setting $\varepsilon^{2k}\lambda_k$ in the series defining $\lambda'$ instead of $\varepsilon^k\lambda_k$ . I could not manage to do it so far.","['matrices', 'eigenvalues-eigenvectors']"
4257705,Chern character,"I am trying to read the book “Fourier-Mukai transforms in algebraic geometry”. Around the end of the page 126 of this book, it is written that the Chern character is from $K(X)$ (Grothendieck group) to $H^*(X,\mathbb{Q})$ . However, according to Fulton’s intersection theory book or Hartshorne’s algebraic geometry book, the Chern character is from $K(X)$ to $A(X)$ (Chow ring). Now, my question is that how is this possible? Indeed, how can we look at the Chern character with codomain $H^*(X,\mathbb{Q})$ ? One answer to this question could be that we consider the composition with cycle map, but then, if we do this, why do we again have Grothendieck-Riemann-Roch theorem or indeed Theorem 5.26, page 127, “Fourier-Mukai transforms in algebraic geometry”?",['algebraic-geometry']
4257779,Probability of One Event Less Than Probability of Second Event,"I am having a bit of trouble proving some cases when one probability is smaller than the other probability for all positive integers $a, b$ , so some suggestions would be appreciated. Here is the problem: For all $a, b \in \mathbb{Z}^+$ , if $P(A) = \dfrac{a^2 - a + b^2 - b}{a^2 + 2ab + b^2 - a - b}$ and $P(B) = \dfrac{a^2 + b^2}{a^2 + 2ab + b^2}$ , prove that $P(A) < P(B)$ . So I attempted using cases. Case 1: If $a = b \neq 0$ , then we want to show that $P(A) < P(B)$ \begin{align*}
P(A) = \dfrac{a^2 - a + b^2 - b}{a^2 + 2ab + b^2 - a - b} = \dfrac{2a^2 - 2a}{4a^2 - 2a} < \dfrac{a^2 + b^2}{a^2 + 2ab + b^2} = \dfrac{2a^2}{4a^2} = \dfrac{1}{2} = P(B)
\end{align*} Thus, this implies that $P(A) < P(B)$ for all $a, b \in \mathbb{Z}^+$ The next two cases are the cases I am having trouble with. Case 2: If $a > b > 0$ , then we want to show that $P(A) < P(B)$ . Case 3: If $b > a > 0$ , then we want to show that $P(A) < P(B)$ . I am not sure how to approach cases 2 and 3. But case 3 should follow from case 2. So some assistance would be appreciated. Thanks","['proof-writing', 'solution-verification', 'probability']"
4257814,There exists a positive integer $s$ for which the number of divisors of $sn$ and of $sk$ are equal.,"IMO 2018 SL N1: Determine all pairs $(n, k)$ of distinct positive integers such that there exists a positive integer $s$ for which the number of divisors of $sn$ and of $sk$ are equal. I am stuck. Here is my progress. If $k|n$ then it is not possible. Let $k={p_1}^{\alpha_1}\cdot{p_2}^{\alpha_2}\dots \cdots {p_m}^{\alpha_m}. $ Let $n={p_1}^{\beta_1}\cdot{p_2}^{\beta_2}\dots \cdots {p_m}^{\beta_m}. $ Where $\alpha_i, \beta_i\ge 0.$ Then the number of divisors of $k$ is $ (\alpha_1+1)\dots(\alpha_m+1)$ and of $n$ is $(\beta_1+1)\dots(\beta_m+1).$ Now there will be a exponent $\alpha_i$ such that $\alpha_i>\beta_i.$ If not then $k|n.$ Say WLOG $\alpha_1>\beta_1.$ Similarly there will be a exponent $\alpha_i$ such that $\alpha_i<\beta_i.$ If not then $m|n.$ Say WLOG $\alpha_2<\beta_2.$ Now we can probably let $S=p_1^a\cdot p_2^b.$ We want $(\alpha_3+1)\dots(\alpha_m+1)(a+\alpha_1+1)(b+\alpha_2+1)=(\beta_3+1)\dots(\beta_m+1)(a+\beta_1+1)(b+\beta_2+1).$ We can treat $(\alpha_3+1)\dots(\alpha_m+1)=Y$ as a constant and $(\beta_3+1)\dots(\beta_m+1)=Z$ as constant. I am not sure on how to proceed. We just have to construct those $a,b.$","['contest-math', 'number-theory', 'elementary-number-theory']"
4257836,The sum of distances from a vertex in a Tree to all other vertex have the same parity.,"Let $G$ be a tree on $n$ vertices when $n$ is even. Then for each vertex, the sum of distances from it to all other vertex is computed. It is interesting to note that all of them have the same parity. I saw some examples of trees and the above fact holds true. But I am unable to prove that. I was thinking if we can use the fact that there is always a unique path from one vertex to another in a tree.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4257875,Minimal sufficient statistic for normal bivariate is complete?,"Let $\mathbf{Z}_1, \mathbf{Z}_2, \ldots, \mathbf{Z}_n$ be iid random sample of size $n$ where $\mathbf{Z}_i = (X_i,Y_i)^T$ is a normal bivariate distribution $\mathcal{N}_2(\mathbf{0},\Sigma)$ such that $$ \Sigma = \begin{pmatrix}1&\rho\\ \rho &1\end{pmatrix} \qquad \text{and} \qquad \mathbf{0}=(0,0)^T$$ I want to find a minimal sufficient statistic for $\rho$ . Here, I denote $\underline{Z}=(\mathbf{Z}_1,\mathbf{Z}_2,\ldots, \mathbf{Z}_n)$ . By the Theorem 5.3 in this e-book , I compute the ratio of the likelihood function for two samples $\underline{Z}$ and $\underline{W}$ . Since $f_\mathbf{Z}(\mathbf{z}\mid\rho)=(2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{1}{2}[\mathbf{z}^T\Sigma^{-1}\mathbf{z}]\right)$ , then we have \begin{align*}
f_\underline{Z}(\underline{z}\mid\rho) &= \prod_{i=1}^n f_\mathbf{Z}(\mathbf{z}_i\mid\rho)=\prod_{i=1}^n (2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i}{2}\right)\\
&=(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right)
\end{align*} Thus, \begin{align*}
\frac{f_\underline{Z}(\underline{z}\mid\rho)}{f_\underline{Z}(\underline{w}\mid\rho)}&=\frac{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right)}{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)}\\
&=\exp\left(-\frac{1}{2}\left(\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)\right)
\end{align*} The ratio turns to be a constant for $\rho$ if and only if \begin{equation}
\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i=0 \quad \cdots \quad(\dagger)
\end{equation} Therefore, \begin{align*}
\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i&=\frac{1}{1-\rho^2}\sum_{i=1}^n \left\{x_i^2-2\rho x_iy_i + y_i^2\right\}\\
&= \frac{1}{1-\rho^2}\left(\sum_{i=1}^n x_i^2-2\rho \sum_{i=1}^n x_iy_i + \sum_{i=1}^ny_i^2\right)
\end{align*} If we denote $S_X=\sum_{i=1}^n X_i^2$ , $S_Y=\sum_{i=1}^n Y_i^2$ and $S_{XY}=\sum_{i=1}^n X_iY_i$ Here, I define $T(\underline{z})=(S_X, S_Y, S_{XY})$ . Therefore, if $T(\underline{Z}) = T(\underline{W})$ , then $(\dagger)$ satifies. This proves that $T$ is a minimal sufficient statistic for $\rho$ . The second task is checking if $T$ is a complete statistic... but I can't find a way to prove if is or not complete. The complete statistic definition that I have just take any function $g$ and doesn't take in consideration the ""measurable function"" as in the definition of complete statistic from Wikipedia .","['statistical-inference', 'statistics']"
4257918,A linear recurrence relation with no primes,"Let $r(m) = m^2 + m -1$ and $q(m) =(r(m) +2)^2 - 2$ where $m$ is a positive integer. For a particular $m$ ,
let $A_1=m^3+2m^2 +2m$ $ A_2 = q(m) A_1 + r(m) $ $A_n = q(m) A_{n-1}-A_{n-2}+r(m)$ It appears $A_n$ is always composite for all positive integers $m$ and $n$ except when $m=1$ and $n=1$ in which case $A_1=5$ is prime. Is this observation true? My attempt: For a particular $m$ , we have $A_1 \equiv 0 $ (mod $m$ ) $A_2 \equiv - 1$ (mod $m$ ) $A_3 \equiv 0 $ (mod $m$ ) $A_4 \equiv 0 $ (mod $m$ ) $A_5 \equiv - 1$ (mod $m$ ) By induction, it can be shown that $A_n \equiv - 1$ (mod $m$ ) if $n \equiv 2 (\ $ mod $3$ ) and $A_n \equiv 0 $ (mod $m$ ) if $n \not\equiv 2 (\ $ mod $3$ ) If $m>1$ and $n \not\equiv 2 (\ $ mod $3$ ) , $A_n$ is composite because $m \ | \ A_n$ . The remaining cases to prove are $m=1$ and $n \equiv 2 (\ $ mod $3$ ) NB: All terms of the sequence $A_n$ appear to be positive integer  solutions in $z$ to the diophantine equation $(xz+1)(yz+1)=z^4+z^3 +z^2 +z+1$ . More on this Diophantine equation https://mathoverflow.net/questions/403542/positive-integer-solutions-to-the-diophantine-equation-xz1yz1-z4z3-z/404278#404278 I found out a paper that considers a more general equation $(xz+1)(yz+1)=z^4+c_1z^3 +c_2z^2 +c_3z+1$ where $c_1, c_2, c_3$ are known integers. https://www.worldscientific.com/doi/abs/10.1142/S1793042117500129 .  However, it uses Mathematics  am not familiar with. Also am more interested in the case when $z$ is prime. The paper provides an algorithm for constructing all solutions $x, y, z$ to the more general equation but does not discuss the primality of solutions in $z$ . If the claim about $A_n$ is true, then to test $n = p^4 +p^3 +p^2 +p+1$ , $p$ prime, for primality, one simply checks if $n$ passes the Fermat test and if $b^{(n-1)/p} \not\equiv 1$ ( mod $n$ )  for some base $b$ https://arxiv.org/abs/2005.02327 ( General purpose promality test - Theorem 2.2)","['number-theory', 'recurrence-relations', 'prime-numbers']"
4257933,What were Poincaré's fundamental groups? (a motivation request),"In this math.overflow question: https://mathoverflow.net/questions/143116/where-does-the-notation-pi-1x-x-for-the-fundamental-group-first-appear , an answer posits that Poincaré regarded the fundamental group ""as a group of permutations of fundamental regions"". In this History of Math & Science question: https://hsm.stackexchange.com/questions/2369/how-did-poincar%C3%A9-discover-the-fundamental-group , an answer posts Poincaré's writing from his Analysis Situs on the definition of the fundamental group. However, I do not understand what Poincaré is saying (or what ""group of permutations of fundamental regions"" means); so I wonder if someone can explain what Poincaré is trying to say in this passage. Another issue I have is about the chronology. In the Wikipedia article for the Poincaré conjecture: https://en.wikipedia.org/w/index.php?title=Poincar%C3%A9_conjecture&oldid=1044860877#Poincar%C3%A9's_question , it makes it seem like Poincaré developed the theory of the fundamental group after his discovery of the Poincaré homology sphere in 1904. However, Analysis Situs was published/written in 1895, in which the above excerpt on the fundamental group is taken. Did Poincaré develop the theory of the fundamental group in response to the homology sphere, and if not, was there a specific problem that motivated his discovery of the fundamental group? I am aware of a similar MSE question: What was Poincaré's original idea of the fundamental group? , but the answers were ""link-only"" and didn't answer my questions. Finally, even if people do not have specific answers to the above questions, I would appreciate if someone knows what could have motivated Poincaré to consider the idea of the fundamental group, i.e. a priori why would it be a good avenue of investigation to study loops/paths in a space to better understand the topology of that space.** EDIT: I will try to better explain my last question. I know for example that in math studying maps out of or in to a space illuminates some properties of that space; but for the fundamental group we are considering maps from $[0,1]$ in to the space (or for homotopies maps from $[0,1]\times [0,1]$ in to the space). This to me seems somewhat arbitrary: why should the unit interval and square be the “basis” on which we study other spaces?","['fundamental-groups', 'homotopy-theory', 'general-topology', 'math-history', 'algebraic-topology']"
4257964,Linear independence vs probabilistic independence [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to solve the following exercise yet I do not know from which angle to attack it. Therefore, I need a hint or two to get me started on both ""if"" and ""only if"" implications. Let $V$ be a finite-dimensional vector space over a finite field $F$ , and let $X$ be a random variable drawn uniformly at random from $V$ . Let $\langle, \rangle: V \times V \rightarrow F$ be a non-degenerate bilinear form on $V$ , and let $v_1,\dots,v_n$ be non-zero vectors in $V$ . Show that the random variables $\langle X, v_1 \rangle, \dots, \langle X, v_n \rangle$ are jointly independent if and only if the vectors $v_1,\dots,v_n$ are linearly independent.","['independence', 'algebraic-independence', 'linear-algebra', 'probability-theory', 'random-variables']"
4258035,Problem while calculating the area of $S^2$ using differential forms,"I am trying to calculate the area of the semi-sphere $S^2_-$ using differential forms, I have the only local chart $(A,\phi,S^2_-)$ given by $A=\{(x,y)\in\mathbb{R}:x^2+y^2< 1\}$ and $\phi(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})}$ . Also I have the unitary normal vector field $N(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})}$ , so that $\{N,\frac{\partial{\phi}}{\partial{x}},\frac{\partial{\phi}}{\partial{y}}\}$ form an oriented basis of $\mathbb{R}^3$ . Now if I calculate by hand $\phi^*(\iota(N)dx\wedge dy\wedge dz)$ I get \begin{align}
&\phi^*(-xdy\wedge dz-y dx\wedge dz-\sqrt{1-x^2-y^2}dx\wedge dy)\\
&=-(-x)dy\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\
&\qquad-yd(-x)\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\
&\qquad-\sqrt{1-x^2-y^2}d(-x)\wedge dy\\
&=\left(\frac{-x^2}{\sqrt{1-x^2-y^2}}+\frac{y^2}{\sqrt{1-x^2-y^2}}+\sqrt{1-x^2-y^2}\right)dx\wedge dy
\end{align} so $\int_{S^2_-}\iota(N)dx\wedge dy\wedge dz=\int_{A}\phi^*(\iota(N)dx\wedge dy\wedge dz)=\int_A \frac{1-2x^2}{\sqrt{1-x^2-y^2}}dxdy$ . At this point I know there is a mistake because last integral should be $\int_A \frac{1}{\sqrt{1-x^2-y^2}}dxdy$ but I really can't figure out where this mistake is.","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
4258036,"Proving that $2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x)=0$, for $N>1$ and $\delta\in[0,1]$, has real solutions","We have the equation \begin{equation}
2\delta\sin(x)-\sin((N+1)x)+\delta^2\sin((N-1)x) = 0,
\end{equation} where $N$ is a positive integer greater than $1$ and $\delta\in[0,1]$ . Now, numerical studies indicate that the solutions to this equation are always real, but we have not been able to prove it. One idea was to use Rouche's Theorem , but that would require us to know about the number of real solutions in e.g. the interval $[0,\pi]$ , and it seems hard to implement anyway since the equation has three terms. Another idea was to rewrite the equation in a convenient way, let $x = a+bi$ and see that the only way to have the right hand side be a real number, was to put $b=0$ , but that just generated another equation that was hard to solve. Finally, we tried to argue in the following way: We know that for $\delta=0$ , the equation has only real solutions, and we know the number of them. If we increase $\delta$ and show that the maxima of the curve $\sin((N+1)x)+\delta^2\sin((N-1)x)$ in the interval $[0,\pi]$ lie above the curve $2\delta\sin(x)$ for all $\delta$ (or precisely on the curve for $\delta=1$ ), we will not ""lose"" any real solutions. But analyzing the maxima was not so straightforward either.","['complex-analysis', 'trigonometry']"
4258096,Why is a stopping time measurable w.r.t. to her associated sigma-algebra?,"Using the following definition I want to proof that a stopping time $\tau$ is measureable w.r.t. its associated $\sigma$ -Algebra $\mathcal{F}_\tau$ . Definition. Let $T$ be a totally ordered set, $t^*:= \sup T$ and $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t \in T},\mathbb{P})$ a filtered probability space. A map $\tau :\Omega \to \overline{T}$ is called $(\mathcal{F}_t)_{t \in T}$ -stopping time , if $\{\tau \leq t\} \in \mathcal{F}_t$ for all $t \in T$ . Furthermore, the $\sigma$ -Algebra $\mathcal{F}_\tau$ associated with $\tau$ is defined by $\mathcal{F}_\tau = \{F \in \mathcal{F}_{t^*} | F \cap \{\tau \leq t\} \in \mathcal{F}_t \text{ for all } t \in T\}$ . I think I can show the statement for real-valued stopping times. $\tau$ is $\mathcal{F}_\tau$ - $\mathscr{B}_{\mathbb{R}}$ -measurable if $\tau^{-1}(A) \in \mathcal{F}_\tau$ for all $A \in \mathscr{B}_{\mathbb{R}}$ . To show this I use two statements I remember from Probability Theory: It suffices to show $\tau^{-1} (A) \in \mathcal{F}_\tau$ for all $A \in \mathcal{A}$ if $\sigma(\mathcal{A}) = \mathscr{B}_{\mathbb{R}}$ and The set of all intervals $(-\infty,b]$ for $b \in \mathbb{R}$ generates $\mathscr{B}_{\mathbb{R}}$ . Because I got $\tau^{-1}((-\infty,b]) = \{ \tau \leq b\} \in \mathcal{F}_b \subseteq \mathcal{F}_{t^*}$ and $\tau^{-1}((-\infty,b]) \cap \{\tau \leq t\} = \begin{cases} \{\tau \leq b\} & \text{for } b \leq t & \in \mathcal{F}_b \subseteq \mathcal{F}_t,\\ \{\tau \leq t\} & \text{for } b > t & \in \mathcal{F}_t \end{cases}$ for all $b \in \mathbb{R}$ it follows with the two statements above that every $\mathbb{R}$ -valued stopping time is $\mathcal{F}_\tau$ - $\mathscr{B}_{\mathbb{R}}$ -measurable. I think my problems with this proof are because of some gaps in the fundamentals concerning measurability. My questions would be: First of all, is my proof for real-valued stopping times correct? Why can I ""assume"" that $\mathcal{F}_\tau$ -measurable means $\mathcal{F}_\tau$ - $\mathscr{B}_\mathbb{R}$ -measurable? Do the statements 1. and 2. hold for a totally ordered $T$ (and therefore can the proof be done similiarly in the more general setting)? Thank you in advance for your help!","['measure-theory', 'stochastic-processes', 'stopping-times', 'probability-theory', 'stochastic-calculus']"
4258129,"How to show the following inequality $_2F_1\left(5.5, 1, 5;-x^2\right)>0$?","Consider the function $_2F_1\left(5.5, 1, 5;-|x|^2\right)$ for $x\in \mathbb{R}^n.$ I want to show that this function is positive. I checked that it does not have any roots so can I conclude the inequality by using continuity in $x$ of the function $_2F_1\left(5.5, 1, 5;-|x|^2\right)$ ?","['proof-writing', 'roots', 'analysis', 'inequality', 'hypergeometric-function']"
4258161,Why do two half-toruses add up to the same volume?,"We have a small torus $A$ with $R=\frac{11}{2}+0.0005$ and $r=0.0005$ . Look at it from the top, and cut along the circle $R$ traces when spun around the center of the torus, and we get the inner and outer half of a torus, inner half clearly with less volume compared to the outer half. Another similar torus $B$ with $R=\frac{13}{2}$ and $r=0.0005$ is also cut in a similar manner. The outer half clearly has more volume than the inner half. (I have noted that the different halves are not equal to the volume of half cylinders since one side is flat and thus the volume displacement will not be equal to the original.) Now, I calculate the volumes of the inner half of A, and the outer half of B using shell integration by setting the center of the torus at $(0,0)$ and rotating the equation of a circle about the line $x=0$ , and get $2\cdot\int^{5.5}_{5.4995}\left(2\pi\cdot x\cdot\sqrt{.0005^2-\left(x-\frac{11}{2}\right)^2}\right)\text{ d}x=\frac{\pi(8250\pi-1)}{6000000000}$ and $2\cdot\int^{6.5005}_{6.5}\left(2\pi\cdot x\cdot\sqrt{.0005^2-\left(x-\frac{13}{2}\right)^2}\right)\text{ d}x=\frac{\pi(9750\pi+1)}{6000000000}$ respectively. I then calculated the same volumes assuming they were equal to half cylinders with the height equal to the circumference of a circle with radius $R$ , and got $\frac{11\pi^2}{8000000}$ and $\frac{13\pi^2}{8000000}$ respectively. I notice something really odd - the volume(assuming half cylinders) for the inner half of A is $\frac{\pi}{6000000000}$ larger than the result I got from shell integration, and the outer half of B was exactly $\frac{\pi}{6000000000}$ smaller. Why is this? These two toruses clearly have different $R$ , and I don't see any reason why the volume inaccuracies for the inner half of $A$ and the outer half of $B$ (when you calculate them as half cylinders) would be the same and cancel each other out if we add them? Thanks,
Max0815","['integration', 'geometry', '3d']"
4258166,How can we know when the limit given result is wrong if we are trying to prove it by its definition?,"I am currently learning Real Analysis and we learned about the definition of a convergent limit and did some exercises to apply the definition in order to prove the limit's result (i.e: ""Use the definition of a limit to show $\lim_{x\rightarrow\infty}f(x)=L$ ) However, my concern is that what if the given result (i.e: $L$ ) is wrong? It looks to me that I can prove any limit converges to anything this way (I am not worried about this in an exam context or just, that is just some general thoughts). To examine this, I tried proving a wrong result myself, took this: Use the definition of a limit to show that $\lim_{n\rightarrow\infty}\frac{1}{n}=10$ I worked it out just like I did for the real result. Solution: Note: $n,n_0\neq 0$ Let $\epsilon>0$ be arbitrary. We search for $n_0 \in \mathbb{N}$ s.t. $n\geq n_o \Longrightarrow |\frac{1}{n}-10|<\epsilon$ $\frac{1-10n_0}{n_0}<\epsilon \iff n_0>\frac{1}{10+\epsilon}$ We can take, $n_0=\left \lfloor{\frac{1}{10+\epsilon}+1}\right \rfloor $ Done. Obviously, I don't think that I actually proved that this limit converges to $10$ , I just think I am missing something in terms of my understanding or the way this was presented to me was wrong which is why I am posting here.","['limits', 'real-analysis']"
4258197,"Show that $x_n \notin \overline { \text{span} (x_1,\dots, x_{n-1},x_{n+1},x_{n+2}, ...)}$","Let X be a banach spaces and $(x_n)_{n\geq1}$ be a Schauder basis in $X$ . $1)$ $\forall x\in X,$ $x=\sum_{n=1}^\infty \lambda_n x_n$ and for all $n\geq 1$ , we define $x_n^*=\lambda_n$ , show that $x_n^* \in X^*$ . $2)$ And show that $x_n \notin \overline { \text{span} (x_1,\dots, x_{n-1},x_{n+1},x_{n+2}, ...)}$ I can manage $(1)$ but help me for $(2)$ please thank you.","['banach-spaces', 'functional-analysis']"
4258236,Functional that leads to hollow,"Consider a field $\phi(x,y)$ in the 2D plane. The minimum of the following Functional leads to formation of a 2D disk in which $\phi(x,y)=1$ deep inside the disk and $\phi(x,y)=0$ outside the disk.The boundary of the disk has a thickness so that $\phi$ continuously gors from 1 to 0. $$\DeclareMathOperator{\Dm}{\operatorname{d\!}}
F=\int \Dm x \Dm y \big(\phi^2(1-\phi)^2 + K (\nabla \phi)^2 \big)
$$ with $K>0$ and a constraint on $\phi$ as following: $\int \phi dx dy =c$ where $c>>1$ . Is it possible to add a term to the above functional so that the minimum of $F$ becomes a disk with a hollow circle at the centre? I thought about considering a term which favours negative curvature like $-\nabla^2 \phi$ , but since this term can be written as a divergence, its integral is zero and would not change the $F$ . I would appreciate it if some one could clarify if this question has a solution or not.","['variational-analysis', 'functional-equations', 'calculus-of-variations', 'functional-analysis', 'optimization']"
4258244,Find hazard rate of $X$ from its residual lifetime survival function,"Given a random variable $X$ with survival function $S_X(x)$ , I know that I can find the survival function of the residual lifetime rv $T_t=X-t|X>t$ by $S_{T_t}(x)=\frac{S_X(t+x)}{S_X(t)}$ . But can I go the other way around, and find the survival function of $X$ starting with knowing the survival function of $T_t$ ? Here's the problem: Given a random variable $X>0$ , the associated residual lifetime random variable is defined as $T_t=X-t|X>t$ for a constant $t\geq 0$ . Given that $S_{T_t}(y)=e^{-y^2-(t+5)y}$ , $y>0$ , determine the hazard rate of $X$ , i.e. $h_X(s)$ . Here's what I tried: I attempted to get the survival function of $X$ by setting $t=0$ and using that to calculate the hazard rate. But when I set $t=0$ I get $S_X(y)=e^{-y^2-5y}$ . When plugging that back into the formula for the residual lifetime rv survival function I get $$S_{T_t(y)}=\frac{S_X(t+y)}{S_X(y)}=\frac{e^{-(t+y)^2-5(t+y)}}{e^{-t^2-5t}}=\frac{e^{-t^2-2ty-y^2-5t-5y}}{e^{-t^2-5t}}=e^{-y^2-2yt-5y}$$ This no longer matches the original problem where $S_{T_t}(y)=e^{-y^2-(t+5)y}$ . Where am I going wrong?",['statistics']
4258309,"What does it means ""orthonormal"" vector?","my teacher's slides say that ""The columns [u_1, u_2, . . . , u_m] of an orthogonal matrix are orthonormal"". The slides provide a rigorous definition of orthogonal matrix but they say nothing about orthonormal vectors. What does it means?","['matrices', 'orthogonal-matrices', 'orthonormal', 'terminology']"
4258342,To cover the edges of $5 \times 5$ grid graph with $5$ paths of $8$ edges each.,Consider the grid graph $5 \times 5$ with $25$ vertices and $40$ edges. $1.$ cover its edges with $8$ paths of $5$ edges each. $2.$ cover its edges with $5$ paths of $8$ edges each. I am able to do the first part. But not able to do the 2nd part.,"['graph-theory', 'combinatorics', 'discrete-mathematics']"
4258431,"Let $p(x)$ and $q(x)$ be two polynomials such that $p(2)=5$, $p(3)=12$ and $p(q(x))=p(x)q(x)-p(x)$. Find the value of $q(10)$.","Let $p(x)$ and $q(x)$ be two polynomials such that $p(2)=5$ , $p(3)=12$ and $p(q(x))=p(x)q(x)-p(x)$ . Find the value of $q(10)$ . The question is from a local mock contest that took place a few days ago. Here is my attempt to solve the problem: Since $p(2)=5$ and $p(3)=12$ , we have that $$p(x)=(x-2)r(x)+5\\ p(x)=(x-3)s(x)+12$$ where $r(x)$ and $s(x)$ are two polynomials. Then I substituted $x=10$ in the constraint $$p(q(x))=p(x)q(x)-p(x)$$ and replaced $p(10)$ in terms of $r(10)$ and $s(10)$ . But that makes the problem more complicated as there gets too many polynomials involved. So how do I solve the question?","['contest-math', 'roots', 'functions', 'polynomials', 'algebra-precalculus']"
4258457,Limit in two variables using epsilon-delta,"In a class assignment, I was asked to find the following limit: $$\underset{(x,y)\to (0,0)}{\lim}\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}$$ I computed it to be $0$ numerically and as such, I wanted to prove it using the $\varepsilon-\delta$ definition, but I have not been able to find a $\delta$ such that $\left\vert{\sqrt{x^2+y^2}}\right\vert < \delta$ implies $\left\vert{\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}}\right\vert < \varepsilon$ . I also tried using the squeeze theorem, but I have not been able to find an upper-bound for $\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}$ that does not go to infinity as $(x,y)$ goes to $(0,0)$ . Any help or clue would be greatly appreciated. Thank you in advance!","['multivariable-calculus', 'limits', 'calculus', 'epsilon-delta']"
4258488,Subsystem of infeasible system of linear inequalities,"Suppose that we have a system of linear inequalities described by $$Ax \leq b$$ where $A$ is $m \times n$ matrix. I want to show that if the system is infeasible then it has a subsystem of at most $\text{rank}(A)+1$ inequalities such that the subsystem is also infeasible. My is initial thought was to use Farkas Lemma on the original system and modify the certificate $y$ to obtain $y'$ as a certificate for a subsystem of at most $\text{rank}(A)+1$ inequalities, but I am stuck from there, maybe my initial thought it wrong?","['linear-programming', 'linear-algebra', 'discrete-mathematics', 'optimization', 'inequality']"
4258536,Integral of weird function,"I am trying to solve the following integral \begin{equation}
{\cal I}=\int_{-\infty}^{\infty}\frac{d\omega}{2\pi}\frac{2\tau}{1+\omega^{2}\tau^{2}}\frac{\sin^{2}\left[\left(\omega+\Omega\right)t/2\right]}{\left[\left(\omega+\Omega\right)/2\right]^{2}}
\end{equation} for finite $\Omega$ . I could only solve it for $\Omega=0$ , yielding \begin{equation}
{\cal I}=2 \left[\tau^{2}\left(e^{-t/\tau}-1\right)+\tau \thinspace t\right]
\end{equation} I also tried to solve using the fact below \begin{equation}
{\cal I}=\int_{0}^{t}\int_{0}^{t}dt_{1}dt_{2} e^{-|t_1-t_2|/\tau}e^{i\Omega\left(t_{1}-t_{2}\right)}=\int_{-\infty}^{\infty}\frac{d\omega}{2\pi}\frac{2\tau}{1+\omega^{2}\tau^{2}}\frac{\sin^{2}\left[\left(\omega+\Omega\right)t/2\right]}{\left[\left(\omega+\Omega\right)/2\right]^{2}}
\end{equation} although this did not help much. Any thoughts?","['integration', 'definite-integrals']"
4258544,"If for every $x>0,P(|X_k|>x) \leq P(Y>x)$ then $\lim_k E[X_k|\mathcal{G}]=E[X|\mathcal{G}]$ a.s.?","Consider on a probability space $(\Omega,\mathcal{F},P),$ a sub- $\sigma$ -algebra $\mathcal{G}$ and a sequence of random variables $(X_k)_k$ converging a.s. to $X$ . Let $Y \in L^1.$ ​ Prove that if $|X_k| \leq Y$ then $E[X_k|\mathcal{G}]$ converges a.s. to $E[X|\mathcal{G}]$ . Does it follow that $\lim_k E[X_k|\mathcal{G}]=E[X|\mathcal{G}]$ a.s. a. if for every $x>0,P(|X_k|>x) \leq P(Y>x)?$ b. if for every $x>0,P(|X_k|>x|\mathcal{G}) \leq P(Y>x|\mathcal{G})$ a.s. ? Attempt: The result follows by applying Fatou's lemma: $E[\liminf_k(2Y-|X_k-X|)|\mathcal{G}] \leq \liminf_k E[2Y-|Y_k-Y||\mathcal{G}]$ a.s. which implies that $2E[Y|\mathcal{G}]\leq 2 E[Y|\mathcal{G}]-\limsup_kE[|X_k-X||\mathcal{G}]$ a.s. so that $\limsup_kE[|X_k-X||\mathcal{G}]=0$ a.s. concluding the proof. Any ideas for part 2. are welcomed!","['conditional-expectation', 'measure-theory', 'probability-theory']"
4258560,Optimal rate of convergence of nonparametric density estimators,"Suppose that $X_1, X_2, \dots, X_n$ forms an independent and identically distributed sample from some $d$ -dimensional probability distribution with unknown probability density function $f$ . Let $x$ be some point in the interior of $f$ 's support. The goal is to estimate $f$ based on the observed sample using some nonparametric estimator $\hat f$ . Nonparametric estimators suffer from the curse of dimensionality; Stone [1] is often given as a reference to the claim that the optimal rate of convergence for any nonparametric estimator is $\mathcal O\big(n^{-\frac{p}{2p+d}}\big)$ , where $p$ is the degree of smoothness. I don't understand how Stone's setting applies to a general class of nonparametric estimators. The setting stated in his paper appears very artificial. I would be glad if someone could shed some light on this. [1] https://projecteuclid.org/journals/annals-of-statistics/volume-8/issue-6/Optimal-Rates-of-Convergence-for-Nonparametric-Estimators/10.1214/aos/1176345206.full","['statistics', 'asymptotics']"
4258597,Why is the measure determined by the Stieltjes inversion formula?,"$f$ is a Herglotz-Nevanlinna function if it is analytic in the open upper half plane and the imaginary part $\Im f (z) \geq 0$ for $\Im z >0$ . $f$ has the following integral representation $$f(z) = a + bz + \int_\mathbb{R} \frac{1}{t-z} - \frac{t}{1+t^2} d \mu(t)$$ where $a \in \mathbb{R}, \ b \geq 0$ and $\mu$ is a Borel measure on $\mathbb{R}$ such that $$\int_\mathbb{R} \frac{d \mu(t)}{1+t^2} < \infty$$ The Stieltjes inversion formula gives, that for any interval $(t_1, t_2) \subset \mathbb{R}$ $$\mu((t_1, t_2)) + \frac{\mu(\{t_1\}) + \mu(\{t_2\})}{2} = \lim_{y \to 0^+} \frac{1}{\pi} \int_{t_1}^{t_2} \Im f(t+iy) d t \tag{1}$$ My question is, why is the measure $\mu$ determined by $f$ ? Can the left hand side of $(1)$ be interpreted as a measure of an interval? The Wikipedia article has $\mu((t_1, t_2])$ on the left hand side. If this is the case, then I understand why the measure would be determined, but what happened to the point masses $\mu(\{t_1\})$ and $\mu(\{t_2\})$ ?","['complex-analysis', 'measure-theory', 'stieltjes-integral']"
4258619,"Is L^2(0,T;H^1(\Omega)) compactly embedded in L^2(0,T;L^2(\Omega))?","I know by Rellich-Kondrachov theorem, $H^1(\Omega)$ is compactly embedded $L^2(\Omega)$ . Also, $L^2(0,T;H^1(\Omega))$ is continuously embedded in $L^2(0,T;L^2(\Omega))$ . However, is this embedding compact? Is there a reference related to it?","['sobolev-spaces', 'functional-analysis', 'compactness']"
4258620,"What are relations of $S_4$, if generators are $a=(12)$, $b=(1234)$?","I think, relations could be $a^2=e$ , $b^4=e$ , $ababab=e$ , $b^2 a b^2 a b = b a b^2 a$ . By two first relations and $b^3 = ababa$ (it is result of our relations),  we can present any element that is generated by $a$ and $b$ , as a sequence, where there are not more than one consecutive $a$ and not more than two consecutive $b$ . By the last relation we can put all squares of $b$ to the beginning of sequence. We can remove  3 or more consecutive $ab$ by third relations. And also we have $ab^2ab^2ab^2ab^2=e$ as result of relations, so we can remove 4 or more of consectutive $ab^2$ from the beginning of sequence. But, I'm not sure that we need exactly 4 relations, that there are not  3 relations that would be sufficient.","['permutations', 'group-presentation', 'finite-groups', 'symmetric-groups', 'group-theory']"
4258626,Structure sheaf of $\operatorname{Spec} A$ with $A$ an integral domain,"Let $A$ be an integral domain, $K = \operatorname{Quot}(A)$ the field of fractions of $A$ and $X=\operatorname{Spec}(A) = \{p\subset A \mid \ p \ \text{prime ideal of }A \}$ the spectrum of $A$ with the Zariski topology.
For every $p\in X$ the localization $A_p$ is a subring of $K$ . We take $\mathcal{O}_X$ the structure sheaf of $X$ ; for every open subset $U\subset X$ , $\mathcal{O}_X(U)$ is the set of sections $s:U\rightarrow \bigsqcup_{p\in U}A_p $ such that $s(p)\in A_p$ for all $p\in U$ and locally a quotient of elements of $A$ (i.e. for each $p\in U$ there exists a neighborhood $V$ of $p$ contained in $U$ and elements $a,f\in A$ such that for each $q\in V$ , $f\notin q$ and $s(q)=\frac{a}{f}\in A_q$ ) We want to show that $$
\mathcal O_X(U) = \bigcap_{p\in U}A_p \subset K
$$ My attempt I found here a similar question, but there is not an answer. I think that the key point is the density of $\eta = (0)$ in $X$ thanks to the fact that $A$ is a domain. I tried to show that a section $s\in \mathcal O_X(U)$ is not only locally a quotient of elements of $A$ , but globally (on $U$ ) a quotient of elements of $A$ . To do that, consider a neighborhood $V$ of $\eta$ and $a_1,f_1\in A$ such that for all $q\in V$ , $s(q) = \frac{a_1}{f_1}$ and $f_1\notin q$ for all $q\in V$ . Suppose that $V\neq U$ . Hence there exists $p\in U \setminus V$ and a neighborhood $W$ of $p$ and $a_2,f_2\in A$ with the same properties as above. Since $\eta$ is dense, $\eta\in W$ and we have $\frac{a_1}{f_1} = \frac{a_2}{f_2}$ in $A_\eta = K$ . From here, I can find that $a_1\in p$ , but I can't go any further if I don't add more hypothesis on $A$ . Could this be the right approach? Thanks in advance","['affine-schemes', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
4258648,How can I calculate the following double limit? Or does it diverge?,"$$\lim_{(x, y)\to(0,2)}\left(\frac{y\sin(xe^y)}{x}\right)$$ Here we deal with $\frac{0}{0}$ indeterminate form. What I could see is that $$\left|\frac{y\sin(xe^y)}{x} \right| \leq \left|\frac{y}{x}\right|$$ but what to do with that I don't know. If the $y \to 0$ I would know that it diverges.","['limits', 'multivariable-calculus']"
4258658,criteria for flatness of map $f:A \to A[X]/P(X)=:B$,"Let $A$ commutative ring with $1$ and $P(X) \in A[X]$ a non constant polynomial. Can it be exactly characterized when the canonical map $f:A \to A[X]/P(X)=:B$ is (1) flat or (2) induces surjective map $Spec(f): Spec(B) \to Spec(A), \mathfrak{p} \mapsto \mathfrak{p} \cap A$ Note: in case when (1) and (2) hold at the same time $f$ is called faithfully flat. But in general there is no connection between (1) and (2), therefore these can be studied separately","['algebraic-topology', 'ring-theory', 'algebraic-geometry', 'flatness', 'commutative-algebra']"
4258696,ODE of continuous multivariate function,"This question is pretty similar to the one discussed here: ODE of multivariate function . The problem that we have is: $$
\frac{df(x, t)}{dt} = p(x) + c\int_{y}g(x, y)f(y,t) \, dy + cf(x,t)
.
$$ But this time, both $x$ and $t$ are continuous. The context of the problem and the background of me, as the author of both questions are the same, except that now I know how we can approach the previously mentioned problem. My questions are: Is this any specific general form (like Linear Separable ODEs) to which this problem belongs? Is this problem solvable? If so, how? Thanks in advance!","['multivariable-calculus', 'calculus', 'ordinary-differential-equations']"
4258701,Burnside's Lemma to count the number of ways to write an integer as a product of three positive integers [duplicate],"This question already has answers here : What is the number of unordered triples $a,b,c$ such that $abc = n$? (2 answers) Closed 1 year ago . I am asked to find the number of distinct ways to express 1,000 as a product of three integers using Burnside's Lemma. By distinct ways, I mean that the order of the factors is not important, e.g., $2 \cdot 4 \cdot 125$ is not distinct from $125 \cdot 4 \cdot 2$ . I am aware that there are several similar questions to this, but none that I see use Burnside's Lemma: $$|X / G| = \frac{1}{|G|} \sum_{g \in G} |X^{g}|.$$ Here $|X / G|$ is the number of $G$ orbits and $X^{a}$ is the number of points fixed by a permutation $a$ in $G$ . I understand the lemma and its proof, but I am not sure what group and group action is relevant to the number 1,000 for this problem. I'm fairly certain that I have to consider the prime factorization: $2^{3}5^{3}$ . Any help on making sense of this question is greatly appreciated.","['permutations', 'group-theory']"
4258708,"A multivariable function that is not differentiable only at $(-1<x<1, y=0)$","I have to think of a function with two variables( $f: R^2 \rightarrow R)$ that is continuous at every points in $R^2$ but not differentiable only at $B= \{ (x,y)|-1<x<1, y=0 \}$ . At first I came up with this function: $$
f(x,y)= \begin{cases} \sqrt{(x+1)^2+y^2},& x<-1\\\\ |y|,&-1\le x \le 1 \\\\ \sqrt{(x-1)^2+y^2},&x>1 \end{cases}
$$ But I realized that this function is not differentiable in points $(-1,0)$ and $(1,0)$ which are not included in $B$ . Can anybody help me with this problem?","['multivariable-calculus', 'derivatives']"
4258724,Exact ODE: Why is $du = 0$?,"The following form $$M(x,y)dx + N(x,y)dy$$ can be derived from the total differential of a bivariate function $u(x,y)$ , which is $$du = u_x(x,y)dx + u_y(x,y)dy.$$ Therefore, $du = M(x,y)dx + N(x,y)dy$ . This is perfectly understandable so far. Then we somehow set $u(x,y) = c$ , which makes the differential of $u$ equal to $0$ . I'm confused about this. If $u(x,y) = c$ , then $y$ is no longer an independent variable. The value of $y$ is now dependent on $x$ . Simply put, $u(x,y) = c$ is equal to $u(x,y(x)) = c$ . It's really just an implicit form of a single-variable function. So how is $du$ even defined in this case? Why is $du$ equal to $0$ when there's no tangent plane to approximate the value of $\Delta u$ in the first place?","['differential-forms', 'ordinary-differential-equations']"
4258737,"Spivak, Calculus, Chapter 1, Problem 19b: how to gain intuitive understanding of this proof of the Schwarz Inequality?","In the Prologue section of Spivak's Calculus, Ch.1 problem 19b, we are asked to prove Schwarz inequality starting from the inequality $$2xy\leq x^2+y^2\tag{1}$$ with $$x=\frac{x_i}{\sqrt{x_1^2+x_2^2}}\tag{2}$$ $$y=\frac{y_i}{\sqrt{x_1^2+x_2^2}}\tag{3}$$ My solution is simply to substitute $(2)$ and $(3)$ into $(1)$ , first with $i=1$ then with $i=2$ , and then adding the inequalities. However, that was a mechanical exercise for me, and I haven't been able to form an intuitive understanding of what those expressions for $x$ and $y$ are. My initial attempt was to imagine two unit vectors $\vec{x} = \frac{\langle x_1,x_2 \rangle}{\sqrt{x_1^2+x_2^2}}$ and $\vec{y} = \frac{\langle y_1,y_2 \rangle}{\sqrt{y_1^2+y_2^2}}$ . $$(\vec{x}-\vec{y})^2\geq 0\tag{4}$$ $$\implies\vec{x}^2-2\vec{x}\cdot\vec{y}+\vec{y}^2\geq0\tag{5}$$ $$\implies 2\vec{x}\cdot\vec{y}\leq \vec{x}^2+\vec{y}^2\tag{6}$$ If I interpret $(4)$ as a dot product of $x-y$ with itself (and I realize that I may be using the exponential quite sloppily here, but its a sketch), it says such a dot product (the square of the norm of $x-y$ ) is larger than 0, which is true for any two vectors. By substituting the vectors into $(6)$ , we almost immediately obtain the Schwarz Inequality. In the context of these vectors, I take that to mean that for any two unit vectors $x$ and $y$ , $\vec{x}\cdot\vec{y}\leq 1$ . So this particular proof of the Schwarz Inequality is basically saying: ""given two unit vectors, their dot product is smaller than or equal to one"". Does this vector interpretation make sense? Is there a better way to interpret this particular proof? Why does it not work if we start with the equally true relationship $(\vec{x}+\vec{y})^2=0$ . Note that I am aware of other perhaps more enlightening proofs. I am interested in this particular proofs interpretation.",['algebra-precalculus']
4258761,finding derivative in Gronwall's lemma integral form proof,"taken from answer here and wiki proof Let: $v(s) = \exp\biggl({-}\int_a^s\beta(r)\,\mathrm{d}r\biggr)\int_a^s\beta(r)u(r)\,\mathrm{d}r,\qquad s\in I.$ Using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, we obtain for the derivative $[\exp(-\int_a^s\beta(r)\,dr)(\int_a^s\beta(r)u(r)dr)]'=$ applying $(uv)'=u'v+uv'$ $(e^{-\int_a^s\beta(r)\,dr})'(\int_a^s\beta(r)u(r)dr)+(e^{-\int_a^s\beta(r)\,dr})(\int_a^s\beta(r)u(r)dr)'=$ Chain rule on the first summand exp term, Newton Leibniz formula to the right term $(e^{-\int_a^s\beta(r)\,dr})(-\int_a^s\beta(r)\,dr)'(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))=$ Newton Leibniz formula for the derivative of integral on the first summands factor $(e^{-\int_a^s\beta(r)\,dr})(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))=$ pulling out exp term $(e^{-\int_a^s\beta(r)\,dr})[(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad\beta(s)u(s)-\beta(a)u(a)]=$ $\beta(s),\beta(a)$ out $(e^{-\int_a^s\beta(r)\,dr})[\beta(s)(u(s)-\int_a^s\beta(r)u(r)dr)+\beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr)]$ However, wiki has no term with $\beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr)$ $v'(s) = \biggl(\underbrace{u(s)-\int_a^s\beta(r)u(r)\,\mathrm{d}r}_{\le\,\alpha(s)}\biggr)\beta(s)\exp\biggl({-}\int_a^s\beta(r)\mathrm{d}r\biggr),
\qquad s\in I,$ are there any assumptions or given conditions so that the second summand vanishes?","['integration', 'proof-explanation', 'calculus', 'derivatives', 'chain-rule']"
4258808,Isn't the book wrongly taking $\sin^{-1}$ on both sides here?,"Question: If $\sin(\pi\cos\theta)=\cos(\pi\sin\theta)$ , then show that $\theta=\pm\frac{1}{2}\sin^{-1}\frac{3}{4}$ . My book's solution: $$\sin(\pi\cos\theta)=\cos(\pi\sin\theta)$$ $$\sin(\pi\cos\theta)=\sin(\frac{\pi}{2}\pm\pi\sin\theta)\ [\text{Formula:}\cos\theta=\sin(\frac{\pi}{2}\pm\theta)]$$ $$\sin^{-1}(\sin(\pi\cos\theta))=\sin^{-1}(\sin(\frac{\pi}{2}\pm\pi\sin\theta))...(i)$$ $$\pi\cos\theta=\frac{\pi}{2}\pm\pi\sin\theta...(ii)$$ $$\cos\theta=\frac{1}{2}\pm\sin\theta$$ $$\cos\theta\pm\sin\theta=\frac{1}{2}$$ $$\cos^{2}\theta\pm2\sin\theta\cos\theta+\sin^{2}\theta=\frac{1}{4}$$ $$1\pm\sin2\theta=\frac{1}{4}$$ $$\sin2\theta=\pm\frac{3}{4}$$ $$\sin^{-1}(\sin2\theta)=\sin^{-1}(\pm\frac{3}{4})$$ $$2\theta=\pm\sin^{-1}(\frac{3}{4})$$ $$\theta=\pm\frac{1}{2}\sin^{-1}(\frac{3}{4})\ \text{(showed)}$$ This solution is good and all, but if we input the value of $\theta$ in line (i) we will see something interesting:- $$\sin^{-1}(\sin(\pi\cos\theta))=\sin^{-1}(\sin(\frac{\pi}{2}\pm\pi\sin\theta))$$ $$[\text{Let's input $\theta=\frac{1}{2}\sin^{-1}(\frac{3}{4})$}]$$ $$\sin^{-1}(\sin(164.058809^{\circ}))=\sin^{-1}(\sin(164.058809^{\circ})$$ $$164.058809^{\circ}=164.058809^{\circ}$$ This is what my book did essentially. However, isn't $164.058809^{\circ}$ outside the restricted range of $\sin^{-1}(x)$ : $[\frac{\pi}{2},-\frac{\pi}{2}]$ ? So, is the line (ii) in the solution of the book valid?",['trigonometry']
4258812,A solution using 'Lifting the exponent' lemma to IMO 1990 P3,"Question: Find all positive integers $n$ such that $$n^2\mid 2^n+1$$ My solution: Lemma (Lifting the exponent): Let $v_p(n)$ denote the highest power of a prime $p$ that divides $n$. That is, $v_p(n)=k$ such that $p^k \mid n$ and $p^{k+1} \nmid n$ . We have the following relation: $$\large v_p(x^n-y^n) = v_p(x-y)+v_p(n)$$ for any odd prime $p$ and , $$\large v_2(x^n-y^n)=v_2(x-y)+v_2(x+y)+v_2(n)-1$$
If $n$ is odd then replacing $y$ by $-y$ we obtain $$ \large v_p(x^n+y^n)=v_p(x+y)+v_p(n)$$ The original problem: It is easy to see that $n=3$ satisfies the solution. We will prove that this the only solution. Suppose for the sake of contradiction that $p \geqslant 5$ is a prime divisor of $n$. The original problem reduces to finding $n$ such that $$\large v_p(n^2) \leqslant v_p(2^n+1)$$
Observe that $n$ is odd and since $v_p(x^n+y^n)=v_p(x+y)+v_p(n)$, for odd $n$, $$\large v_p(2^n+1)=v_p(2+1)+v_p(n)$$ Since by our assumption $p \geqslant 5$ therefore $v_p(1+2)=v_p(3)=0$. And hence $$\large v_p(2^n+1)=v_p(n)$$
But this implies $$\large v_p(n^2) > v_p(n)$$ which is absurd. Therefore the only solution is $n=3$. Is my solution valid? I'm quite new to lifting the exponent lemma. I read it in Evan Chen's handout titled 'Orders Modulo a Prime' and was pretty fascinated at the fact that it could be used to solve at least easy-moderate Olympiad problems.
Please verify and see if I missed out something obvious. Also, I have not included a proof of the lemma as I think it would be pretty useless here :)
Thank you.","['contest-math', 'elementary-number-theory', 'divisibility', 'prime-numbers']"
4258818,Find number of arrangements of a cube if sum of numbers on each face must be same,"Each vertex of a cube is to be labelled with an integer 1 through 8, without repetition, such that sum of numbers of the four vertices of a face  is the same for each face. Arrangements that can be obtained through rotations of the cube are considered to be the same. How many different arrangements are possible? My attempt: Fix 8 on E. Then, D+C=F+G (Since H is common to both faces) Similarly, C+H=A+F and D+A=H+G We can extend this analogy to other sides, and we see that D+E=B+G, E+H=A+B, E+F=B+C. This means that {1,2}, {1,3}, {7,8}, {8,6} cannot be on one line segment since there are no other distinct numbers which add up to give 3,4,15 or 14 respectively. How can I get the other conditions? Edit: As @John and @Alexander mentioned in the comments, the sum of numbers on each face should be 18. The possible sets of numbers on opposite faces will be {8,7,1,2}, {3,4,5,6}; {8,6,1,3}, {4,5,7,2}; {8,5,2,3}, {1,4,7,6} and {8,5,1,4}, {2,3,6,7}. Case 2 will not have any possibilities since there are no 2 numbers except for 8,3 which add up to 11. In case 4, 4 must be opposite to 8 and in case 1, 7 must be opposite to 8. Now we can try to count it on a case by case basis, but I can't think of a foolproof method without repetitions. Can someone help :)","['permutations', 'combinations', 'geometry', 'combinatorics', 'combinatorial-geometry']"
4258834,$L^p$ spaces of tangent sections: reference request,"What sorts of books would address $L^p$ spaces of sections of the tangent space of a Riemannian manifold ( $C^\infty$ , compact, ideally with boundary)? In particular, I'd like to show that smooth sections are dense in the set of $L^2$ -measurable or $L^\infty$ sections, in the $L^2$ norm or $L^\infty$ norm. I could do this using local orthonormal frames and a partition of unity argument, but I'd prefer a reference which does this directly.","['lp-spaces', 'reference-request', 'riemannian-geometry', 'differential-geometry']"
4258844,Each solution of ODE approaches constant given inequality,"given $x'(t)=f(x,t)$ , if $|f(x,t)|\leq A(t)|x|$ and $\int_a^\infty A(s)ds=C\neq\infty$ each solution of IC approaches a constant value. Approach:
Let $x(T)=x_0$ integrating $x'(t)=f(x,t)$ over $t$ in [T; $\infty)\implies \lim\limits_{t\to\infty}x(t)-x_0=\int_{T}^{\infty}f(x,s)ds$ $\lim\limits_{t\to\infty}x(t)=x_0+\int_{T}^{\infty}f(x,s)ds$ $\lim\limits_{t\to\infty}x(t)\leq |x_0|+|\int_{T}^{\infty}f(x,s)ds|\\ \leq |x_0|+\int_{T}^{\infty}|f(x,s)|ds\\ \leq|x_0|+\int_{T}^{\infty}A(s)|x|ds$ from condition on $A(s)$ can conclude that $\lim\limits_{t\to\infty}x(t)<\infty$ but does it exist? function x may have no limit (like $\sin x$ ) Correct me if im wrong P.S. was also thinking about more general condition what will happen if $|f(x,t)-f(y,t)|\leq A(t)|x-y|$ if, suppose two different x,y solutions of ODE with different IC's, must these solutions converge to different values? Using the same approach led to: $x-y\leq|x-y|\leq|x_0-y_0|+\int_{T}^\infty A(s)|x-y|ds<\infty$ but again
If I suppose the contrary that two different solutions converge to the same constant their difference at infinity must be $=0$ but it I'm stuck with applying limit to make use of condition on $A(t)$ to inequality above (just as in the special case checking one solution converging to constant). Proof after hint of Lutz Lehmann $x'(t)=f(x,t)\leq |f(x,t)|\leq A(t)|x|\\ |x'(t)|\leq A(t)|x|\\$ Applying Gronwalls inequality $x'\leq f(x,t)x\implies x\leq x_0 e^{\int_\tau^{\infty}f(s)ds}$ $$|x(t)| \leq |x_0| e^{\int_{\tau}^{\infty} A(s)ds}$$ Due to $\int_{\tau}^{\infty} A(s)ds=C<\infty$ , means function $x(t)$ is bounded from both sides by the constant factor $e^{\int_{\tau}^{\infty} A(s)ds}$ mentioned above. However question still remains: what happens when $t\to \infty$ with $x(t)$ as it is bounded from both sides due to derivation, but may oscillate at infinity.","['solution-verification', 'ordinary-differential-equations']"
4258884,A question about skew-symmetric matrix.,"I have no idea how to prove/disprove this Statement. Statement: Let $K$ be a field of arbitrary characteristics. Let $X$ be a skew-symmetric $(n \times n)-$ matrix over $K$ and $H=(h_{ij})$ be skew-symmetric with $$h_{ij} =
\begin{cases}
0,  & i=j \\[2ex]
1, & i >j \\[2ex]
-1 & i<j \\
\end{cases}$$ Then there is $Q$ such that $X=QHQ^T.$ Any help is appreciated. Thank you!","['matrices', 'skew-symmetric-matrices', 'linear-algebra']"
4258987,"X,Y ~ Unif(0,1) not necessarily independent, can P(X+Y>1)>1/2?","My set up is the following: $X,Y \sim \text{Unif}(0,1)$ but their joint distribution is not constrained. My question is whether there exists a joint dependence between them (that preserves the marginals) such that $\operatorname{Prob}(X+Y>1)>1/2$ . I can show it is = 1/2 for independence (via integrating the joint PDF), but am wondering whether there is a simple argument or counterexample either way for the cases where the joint distribution is not constrained. I have tried conditioning on one of the variables but couldn't make progress. All the simiulation evidence I have suggests it is = 1/2 for a variety of dependencies.
Thanks!","['uniform-distribution', 'probability', 'random-variables']"
4258997,Solving the equation $e^{x}-2^{x}=1$,"I'm trying to solve $e^{x}-2^{x}=1$ This came from one of my HS students, who was hoping to solve it algebraically. We graphed the first term vs the 2nd and third and see - Which helped to understand there is a solution, and only one. By simply substituting 1 and then 2 for X, we had already concluded, but graphed for confirmation. Aside from staring at this for the last few days, we are not finding even a first step beyond this. The student is a sophomore, but our curriculum goes to AP Calculus, so if the answer is that there is no algebraic solution, but we need calculus to solve, that's fine.
He got this from a friend, and offered no background beyond that.",['algebra-precalculus']
4259005,Simple proof that the Catalan numbers are integers,"As the Catalan numbers are defined as $$C_n = \frac{1}{n+1} \binom{2n}{n},$$ it is not immediately clear that they are integers. To show that they are, there's a relatively basic approach involving some binomial identities, but I wanted to avoid most of these, so I tried the following. To show that $C_n$ is integer, it obviously suffices to show that $n + 1 \mid \binom{2n}{n}$ .
Given $n$ , we can see that $$ \binom{2n}{n+1} = \frac{n}{n+1} \binom{2n}{n} $$ by manipulating the fractions a little. Thus, $$ \binom{2n}{n+1} (n+1) = n \binom{2n}{n},$$ and therefore $n + 1 \mid n \binom{2n}{n}$ .  Since $n$ and $n+1$ are coprime, $n+1 \mid  \binom{2n}{n}$ , which should complete the proof. Is this proof correct? For some reason, it feels like there's something off with it, although I can't see any mistakes.","['catalan-numbers', 'solution-verification', 'combinatorics']"
4259009,Matching a minimal amount of entries to a set of requirements (Graph theory),"I have a set $R$ of requirements that have to be fulfilled with a certain amount of ""points"", and a set $E$ of entries that can fulfill multiple requirements with a certain amount of ""points"" each. For example: In order to get your degree, you have to acquire 50 credits in Mathematics, 20 credits in Physics and 10 credits in Philosophy. You also have a set of lectures that you have visited, while one lecture can be assigned to one or more subjects, and has a certain amount of credit points (e.g ""Philosophy of Science"" gives 5 Philosophy credits and 3 Physics credits). Now, let us assume you have completed enough lectures to get your degree (but way more than necessary). Let us also assume that your university is greedy and getting a completed exam accredited on your final report always costs a fixed amount of money. From all the lectures, how do you get a subset $E' \subseteq E$ of completed lectures such that all requirements in $R$ are fulfilled and $|E'|$ is minimal? I have started by modelling the problem as a weighted bipartite graph of two disjoint vertex sets $R \cup E$ . If some $e \in E$ fulfills a requirement $r \in R$ with $p$ credits, an edge of weight $p$ is drawn between $e$ and $r$ . Also, each $r \in R$ is weighted (as a vertex) with the amount of credits needed to fulfil it. We are looking for a minimal subset $E' \subseteq E$ such that for each $r \in R$ , the sum of weights of edges from $$r_{E'}^+ = \{e \in E' | e \text{ is connected to } r\}$$ is greater or equal than the weight of $r$ itself. This is done assuming such an $E'$ exists. As you see, I have worked the modelling out, but am at a loss when it comes to some kind of solution. It looks like some mix of a matching and flow problem to me, but this is unlike any problem I have seen. All help is appreciated.","['graph-theory', 'discrete-mathematics', 'algorithms', 'discrete-optimization', 'optimization']"
4259038,"Raspberries are 85% water, raspberry jam is 30% water, and sugar contains no water. What weight of jam can I make with 2.8 kg of raspberries?","By weight, raspberries are 85% water, raspberry jam is 30% water, and sugar contains no water. I make raspberry jam by mixing equal weights of raspberries and sugar and then boiling them to evaporate off some of the water. What weight of jam can I make with 2.8 kg of raspberries? If $x$ is the final amount of jam produced, then final amount of water $w = 0.30x$ Final amount of water $w$ is some percentage $n$ of the water in raspberries $= (2.8*0.85)n\\ ⇒ 0.30x = 2.38n$ Final amount of jam = amount of raspberries without water + amount of water from raspberries + sugar. so, $x = 2.8*0.15 + 2.38n + 2.8\\ ⇒ x = 2.38n + 3.22$ Solving, $x = 2.38 * \frac{0.30}{2.38}x + 3.22\\⇒x = 4.6$ So 4.6 kg of jam can be produced from 2.8kg of raspberries. Is that correct and using an efficient technique? I mention ""efficient technique"" because i often spend a lot of time going round in circles trying to relate facts in algebra.","['algebra-precalculus', 'percentages']"
4259103,Why is squaring both sides not producing extraneous roots here?,Process $1$ : $$2x-1=0$$ $$2x=1$$ $$x=\frac{1}{2}$$ Process $2$ : $$2x-1=0$$ $$(2x-1)^2=0$$ $$(2x-1)(2x-1)=0$$ $$2x-1=0$$ $$2x=1$$ $$x=\frac{1}{2}$$ Why aren't we getting extraneous roots in process $2$ ?,"['algebra-precalculus', 'roots', 'polynomials']"
4259114,What is the maximum number of warriors one can put on a chess board so that no two warriors attack each other?,"In chess, a normal knight goes two steps forward and one step to the side, in some orientation. Thanic thought that he should spice the game up a bit, so he introduced a new kind of piece called a warrior . A warrior can either go three steps forward and one step to the side, or two steps forward and two steps to the side in some orientation. Given a $2020\times2020$ chess board. Find, with proof, the maximum number of warriors one can put on its cells such that no two warriors attack each other. The question is a modified version of a problem from Bangladesh Mathematical Olympiad 2019. For more clarity, here is a picture that shows example moves of a warrior : This is my first time solving this kind of problem. I've made the following progress in solving the question: We place the warriors in each cell of $n$ -th column where $n\equiv1\ (\bmod 4)$ . The following picture shows this strategy in an $8\times8$ board: It can be seen that no two warriors can
attack each other. Hence, the answer to our original problem should be $2020\times505$ . Though this result matches with the original answer, I have still some confusions. Firstly, the optimal strategy is that in the $2020\times2020$ board, we place a warrior in each cell of $n$ -th column. But what if we don't place them with that strategy or we just randomly place the warriors so that they cannot attack each other? How will I know other strategies would not give a result greater than $2020\times 505$ ? More specifically, how do I write a formal proof for this kind of problems?","['contest-math', 'proof-writing', 'combinatorics', 'chessboard', 'optimization']"
4259135,Dirac function squared?,"I am trying to perform the following limit \begin{equation}
\lim_{t\rightarrow\infty}\int\frac{d\omega}{2\pi}S\left(\omega\right)\frac{\sin^{2}\left[\left(\omega+\Omega\right)t/2\right]}{\left[\left(\omega+\Omega\right)/2\right]^{2}}
\end{equation} Ideally, I was thinking about using the relation $\lim_{t\rightarrow\infty}\frac{\sin\left(\pi xt\right)}{\pi x}=\delta\left(x\right)$ to simplify the equation above, and actually this kind of works, but leads to the following  integral of a Dirac delta function squared, \begin{align*}
\int\frac{d\omega}{2\pi}S\left(\omega\right)\lim_{t\rightarrow\infty}\frac{\sin^{2}\left[\left(\omega+\Omega\right)t/2\right]}{\left[\left(\omega+\Omega\right)/2\right]^{2}} & =\int\frac{d\omega}{2\pi}S\left(\omega\right)\lim_{t\rightarrow\infty}\frac{\sin\left[\pi\left(\frac{\omega+\Omega}{2\pi}\right)t\right]}{\pi\left(\frac{\omega+\Omega}{2\pi}\right)}\frac{\sin\left[\pi\left(\frac{\omega+\Omega}{2\pi}\right)t\right]}{\pi\left(\frac{\omega+\Omega}{2\pi}\right)}\\
 & =\int\frac{d\omega}{2\pi}\delta\left[\left(\frac{\omega+\Omega}{2\pi}\right)\right]\delta\left[\left(\frac{\omega+\Omega}{2\pi}\right)\right]\\
 & =\int\frac{d\omega}{2\pi}2\pi\delta\left(\omega+\Omega\right)\times2\pi\delta\left(\omega+\Omega\right)\rightarrow\infty
\end{align*} I was reading about the square of the Dirac delta function, it turns out this is not even a distribution. [See: https://math.stackexchange.com/questions/2221429/why-is-the-square-of-dirac-delta-function-not-a-distribution] ; or even if treated as would diverge after the integration [See: https://physics.stackexchange.com/questions/47934/dont-understand-the-integral-over-the-square-of-the-dirac-delta-function] In a related old post, one of our peers mentioned that is possible to prove \begin{equation}
\lim_{t\rightarrow\infty}\frac{\sin^{2}\left[\left(\omega+\Omega\right)t/2\right]}{\left[\left(\omega+\Omega\right)/2\right]^{2}}=2\pi\delta\left(\omega+\Omega\right)t
\end{equation} Limit of $ \frac{1}{2}\frac{\sin^2(\omega t/2)}{(\omega/2)^2}$ which resolves easly my problem. However, I do not know how to prove that. It seems one only applies the relation above once.","['integration', 'definite-integrals', 'dirac-delta', 'probability-distributions', 'distribution-theory']"
4259192,Why special fiber of Neron model of elliptic curve is group variety?,"Let $E$ be an elliptic curve over local field $K$ , whose ring of integers is $R$ .
Let $ε$ be an Neron model of $E$ , and $ε'$ be it's special fiber. Then, why $ε'$ is a group variety ? Does this depend on some special condition of neron model?
Or is the 'Special fiber of group scheme is always group variety'holds in general? Thank you in advance.","['algebraic-groups', 'elliptic-curves', 'algebraic-geometry', 'neron-model', 'schemes']"
4259262,Angle chasing to show three points are collinear.,"Let $ABC$ be an acute triangle with circumcenter $O$ and let $K$ be such that $KA$ is tangent to
the circumcircle of $\triangle ABC$ and $\angle KCB = 90 ^{\circ}$ . Point $ D$ lies on $ BC$ such that $KD || AB.$ Show
that $DO$ passes through $A.$ This is a problem from EGMO, eg.1.32. My approach: I created a dummy point D' such that D'O passes through A.
Now I just have to show that BA and D'A are || which will solve the problem. I think I have all the necessary theorems and I think this problem can be solved with angle chasing, but I can't really figure out how to proceed. Can you please guide me?",['geometry']
4259382,"Selecting distinct $A,B,C⊆\{1,2,3,4\}$ such that $A\cup B\cup C=S$","Let $S$ be the set $\{1,2,3,4\}$ . Now how many ways are there to select three distinct subsets $A$ , $B$ and $C$ such that $A\cup B\cup C=S$ ? I am confused how to approach this question. Only thing that came to my mind is to make cases but it's gonna take so much time. Is there another way of doing it?",['combinatorics']
4259413,"Ramanujan: If $\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx$, then $\frac\pi2\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty\psi(q,x)\psi(p,nx)dx$.","Apparently, the following appeared in Ramanujan's first letter to GH Hardy. If $$\psi(p,n)=\int_0^a\phi(p,x)\cos(nx)dx,$$ then $$\frac{\pi}{2}\int_0^a\phi(p,x)\phi(q,nx)dx=\int_0^\infty \psi(q,x)\psi(p,nx)dx$$ I tried proving this, but with not much success. First, I wrote $$\psi(q,x)\psi(p,nx)=\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudv$$ Then I guess we have $$\int_0^\infty\psi(q,x)\psi(p,nx)dx=\int_0^\infty\int_0^a\int_0^a\phi(q,u)\phi(p,v)\cos(xu)\cos(nxv)dudvdx,$$ but this looks just awful. It's hard to believe that expression would somehow reduce to the desired one. If there is a way to go about it like this perhaps it is via some clever substitution or change of variables, but seeing as so little is known about $\phi$ , I don't imagine there'd be much to work with. Does anyone know how to prove Ramanujan's result?","['integration', 'multivariable-calculus', 'iterated-integrals', 'real-analysis']"
4259470,How to prove that a group action is continuous iff actions of each group element are homeomorphisms,"Let $X$ be a topological space and $G$ a group, equipped with the discrete topology, with an action on $X$ , and define $\phi:G\times X\rightarrow X$ by $\phi(g,x) = g\cdot x$ . I'm trying to show that $\phi$ is continuous if and only if each map $\ell_g:X\rightarrow X:x\mapsto g\cdot x$ is a homeomorphism of $X$ . The forward direction is fairly straightforward. The reverse direction (i.e. showing each $\ell_g$ is a homeomorphism) is proving more difficult, and I am starting to wonder if it is even true. We know that each $\ell_g$ is a bijection, and continuity of $\ell_g^{-1}$ follows from continuity of $\ell_g$ for all $g\in G$ by $\ell_g^{-1}$ = $\ell_{g^{-1}}$ .  The trouble is that it seems very difficult to extract information about a particular $\ell_g$ from the continuity of $\phi$ . In particular, if $U$ is open in $X$ , we can write $$\phi^{-1}(U) = \bigcup_{g\in G}\{g\}\times\ell_g^{-1}(U) = \bigcup_{g\in G}\{g\}\times\ell_{g^{-1}}(U) $$ but I see no clear way to move from this to the fact that $\ell_{g^{-1}}(U)$ is open in $U$ , which would complete the proof. Any help getting unstuck (or a counterexample) would be appreciated!","['general-topology', 'group-actions', 'group-theory']"
4259539,"How to formally argue that concentration bounds are valid for dependent samples that are drawn ""better than"" i.i.d.","I'm looking for references that help me formally argue that, in a special case of non-independent sampling, we can apply concentration bounds as if the sample was i.i.d..  Below is a simple concrete example to explain my question. Suppose we are interested in some statistic (e.g., the expected value) of a probability distribution represented by the rolls of two 6-sided weighted dice.  With probability $p$ , dice A is thrown, and we get a number in $\{1, 2, \dotsc, 6\}$ (with some unknown probabilities).  With probability $1-p$ , dice B is thrown, and we get a number in $\{1, 2, \dotsc, 6\}$ (with some unknown probabilities that are different from A).  So the result is always an integer in $\{1, 2, \dotsc, 6\}$ . Now suppose we are given an i.i.d. sample of size $n$ (assume $n$ is even, for reasons that will become apparent soon), and then apply something like Hoeffding's inequality to get a probabilistic bound on the mean of the distribution (or some other concentration bound/statistic), but we are forced to discard half of our sample first ( for reasons that are not important for the purposes of this question ). We could of course, randomly partition our sample into 2 equally-sized sets, and throw away one of those sets, which results in the remaining set being an i.i.d. sample, to which we can straightforwardly apply our concentration bounds (let's call this the vanilla procedure ).  Notice that the vanilla procedure is equivalent to drawing an i.i.d. sample of size $n/2$ . Alternatively, if we know which samples are from A and which are from B, we could split things so we throw away half of the rolls from A and half of the rolls from B.  For example, following this procedure, if $n=100$ and we got 60 rolls from A and 40 from B, we would throw away 30 randomly selected rolls from A, and 20 randomly selected rolls from B, leaving us with a 30 A rolls and 20 B rolls to estimate our statistic.  We'll call this the alternative procedure below. In other words, instead of randomly throwing away half of the data, the alternative procedure does something similar to (but not exactly the same as) stratification when deciding what data to discard, in an attempt to reduce variance of the estimator. Intuitively, while a sample from alternative procedure is not sampled independently, it is clearly ""more representative of the distribution in expectation"" than an i.i.d. sample obtained from the vanilla procedure (sorry for the informal terms, this is the core of the question and I don't know the correct terms). (It is trivial to show that the resulting sample results in an unbiased estimator of the mean.  Intuitively, it may also result in a lower variance estimator of the mean compared to the vanilla procedure, since we avoid situations where we discard a disproportionate number of the rolls from one of the dice. In other words, we are more likely to get rolls of the dice that more closely match the proportions $p$ and $1-p$ .) So, since this sampling method is ""better than i.i.d."", it seems very intuitive that we should be able to use concentration bounds as if the sample from the alternative procedure was sampled i.i.d..  How can I formally argue this idea?  What limits are there on this argument in terms of applicability? (I am convinced that this method applies to estimating statistics like the mean, but am not sure if it is generally applicable, e.g., the expected sample variance might be higher compared to that of an i.i.d. sample.)  Thank you! Things I've looked at/considered already (reading this section may be helpful, but is not necessary to answer the question): I've noticed that the alternative procedure can be categorized as producing an exchangeable sequence of random variables ( https://en.wikipedia.org/wiki/Exchangeable_random_variables ).  I can't find any sources that help me exploit this fact. I've seen several papers like https://arxiv.org/pdf/1309.4029.pdf , which are closely related to this idea, but do not directly help since this is not a simple ""sampling without replacement"" scenario.","['statistics', 'sampling']"
4259561,"For all diagonals $a$ of Pascal's triangle (figurate numbers), $\sum_{k=a}^\infty {k\choose a}\frac{1}{2^k}=2$? [duplicate]","This question already has answers here : Generalized binomial identity that converges to 2 [duplicate] (2 answers) Closed 2 years ago . I am looking for the derivation of the closed form along any given diagonal $a$ of Pascal's triangle, $$\sum_{k=a}^n {k\choose a}\frac{1}{2^k}=?$$ Numbered observations follow. As for the limit proposed in the title given by: Observation 1 $$\sum_{k=a}^\infty {k\choose a}\frac{1}{2^k}=2,$$ when I calculate the sums numerically using MS Excel for any $a$ within the domain ( $0\le a \le100$ ) the sum approaches 2.000000 in all cases within total steps $n\le285$ . The first series with $a=0$ is a familiar geometric series, and perhaps others look familiar to you as well: $$\sum_{k=0}^\infty {k\choose 0}\frac{1}{2^k}=1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+... =2,$$ $$\sum_{k=1}^\infty {k\choose 1}\frac{1}{2^k}=\frac{1}{2}+\frac{1}{2}+\frac{3}{8}+\frac{1}{4}+\frac{5}{32}... =2,$$ $$\sum_{k=2}^\infty {k\choose 2}\frac{1}{2^k}=\frac{1}{4}+\frac{3}{8}+\frac{3}{8}+\frac{5}{16}+\frac{15}{64}... =2,$$ but it is both surprising and elegantly beautiful that these sums across all diagonals appear to approach the same value. Some additional observations from the numerically determined sums: Observation 2 The maximum value of any term ${k\choose a}\frac{1}{2^k}$ within a diagonal $a$ for the domain $(a>0)$ is attained at $k=2a-1$ and repeated for the term immediately following ( $k=2a$ ). Observation 3 $$\sum_{k=a}^{2a} {k\choose a}\frac{1}{2^k}=1$$ Observation 4 $$\sum_{k=a}^{n} {k\choose a}\frac{1}{2^k} + \sum_{k=n-a}^{n} {k\choose n-a}\frac{1}{2^k}=2$$ It's very likely that the general closed form has been derived before, but searching for the past several days has produced no results. It appears that setting up the appropriate generating function may play a role, but I am at a loss as to how to proceed. Looking forward to the responses.","['summation', 'binomial-coefficients', 'sequences-and-series']"
