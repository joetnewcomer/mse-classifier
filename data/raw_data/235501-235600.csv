question_id,title,body,tags
4928775,Is this an equivalence of connectedness in $\mathbb{R}^{n}$?,"Let $A$ be a subset of $\mathbb{R}^{n}$ with the following property: for every $B\subseteq \mathbb{R}^{n}$ , $A\cap Bd(B)\neq \emptyset$ (i.e., $A$ has non-empty intersection with the boundary of $B$ ) whenever $A\cap B \neq \emptyset \neq A\cap (\mathbb{R}^{n}\setminus B)$ . Must $A$ be connected? This is related to this question: Is this an equivalence of connectedness? As Moishe Kohan showed, the property above does not imply connectedness in arbitrary topological spaces. Now I am interested in which cases there is a positive answer and it seems to me that a good candidate is the Euclidean space $\mathbb{R}^{n}$ . For example, $A$ is connected provided that $A\subseteq \mathbb{R}$ . Indeed, if this were not the case, then there would exist $x,y\in A$ and $z\in \mathbb{R}$ with $x<z<y$ and $z\not\in A$ . Therefore, $B:= (-\infty,z)$ satisfies $x\in A\cap B$ , $y\in A\cap (\mathbb{R}\setminus B)$ and $A \cap Bd(B) = \emptyset$ . What happens in higher dimensions?","['general-topology', 'connectedness']"
4928779,Would like to validate whether the AUC equation is correct or not,"I found a paper ""Chapi, Kamran, et al. ""A novel hybrid artificial intelligence approach for flood susceptibility assessment."" Environmental modelling & software 95 (2017): 229-245"". The paper mentioned the equation of area under the ROC curve (AUROC) is: $$
AUROC = \frac{ \sum TP+\sum TN }{P+N}
$$ Here, I don't want to argue anything since it is published in a good journal but I would like to have more discussion if the AUC can be easily calculated with the equation. What I know to calculate AUC is by using integral for area calculation. I tried to find the source of the equation, but I could not find the original one who proposed the equation and well-widely recognized. I would like to validate if the equation is correct to calculate AUC? I would be glad to appreciate any comments and thoughts, big thanks.","['machine-learning', 'statistics', 'signal-processing', 'neural-networks']"
4928808,Definition of Ricci flow,"My undergraduate thesis is related to the Ricci flow, and I have a number of basic questions. Let $M$ be a smooth manifold. At the start of Chapter 2.3 of Peter Topping's Lectures on the Ricci flow , he supposes that $g$ is a smooth family of Riemannian metrics on $M$ defined on an open interval. Question 1. Does Peter mean that $g$ is a map of the form $g:I \times M \rightarrow T^{(0,2)}TM$ , where $I\subseteq \mathbb R$ is an open interval? Does smoothness here mean that $g$ is smooth as a map between the smooth manifolds $I \times M$ and $T^{(0,2)}TM$ ? I'm trying to write the definition of the Ricci flow in my own words. This is what I have: Let $I$ be an open interval, and let $g:I \times M \rightarrow T^{(0,2)}TM$ be a smooth (in the sense of Question 1) family of metrics. For each $p \in M$ , we have a smooth curve $g(\cdot,p):I \rightarrow T^{(0,2)}T_p M$ given by $t \mapsto g(t,p)$ . For each $t \in I$ , define $(\partial_t g)(t,p) := g(\cdot,p)'(t)$ . Since $T^{(0,2)}T_p M$ is a vector space, we can view $g(\cdot,p)'(t)$ as an element of $T^{(0,2)}T_p M$ . We say that $g$ is a solution to the Ricci flow if $$(\partial_t g)(t,p) = -2 \text{Ric}(g(t,p)) \qquad \forall (t,p) \in I \times M.$$ Question 2. Is my definition correct? Does the solution of the Ricci flow need to be smooth in the sense of Question 1? Theorem A.15 of The Ricci Flow: Techniques and Applications Part I states the following: Theorem 1. If $(M,g_0)$ is a closed Riemannian manifold, then there exists a unique solution $g(t)$ to the Ricci flow defined on some positive time interval $[0, \varepsilon)$ such that $g (0) = g_0$ . Question 3. In the theorem above, what does it mean for the solution $g:[0,\varepsilon) \times M \rightarrow T^{(0,2)}TM$ to be smooth? Does it mean that $g$ is smooth as a map between smooth manifolds, where $[0,\varepsilon) \times M$ is considered as a manifold with boundary? Or does it mean that $g$ is continuous, and smooth on $(0,\varepsilon) \times M$ ? Or does it mean something else? On the other hand, the wikipedia page on the Ricci flow states that Hamilton proved the following theorem: Theorem 2. If $(M,g_0)$ is a closed Riemannian manifold, then there exists a positive number $T$ and a Ricci flow $g_t$ parameterised by $t \in (0,T)$ such that $g_t$ converges to $g_0$ in the $C^\infty$ topology as $t \rightarrow 0$ . Question 4. What is the relationship between Theorem 1 and Theorem 2? Does one follow from the other? I tried looking at the original paper by Hamilton , but I wasn't able to understand it.","['partial-differential-equations', 'ricci-flow', 'riemannian-geometry', 'differential-geometry']"
4928844,"Solving the system $x^4+y^4+z^4=a$, $xy+xz+yz=b$, $xyz=c$","I am trying to solve the following system of equations: $$
\begin{cases} x^4+y^4+z^4=a\\[4pt] xy+xz+yz=b\\[4pt] xyz=c\end{cases} 
$$ where $a$ , $b$ and $c$ are constants and $x$ , $y$ and $z$ are the variables that need to be solved. The approach that appears to be the most promising is to isolate for 1 variable in equation 3, substitute it into equations 1 and 2, then do something with the substituted equations 1 and 2 (now in 2 variables) that results in an equation in 1 variable (likely a cubic in $variable^4$ ) which can then be solved. However, I have messed around with this for a long time and could not find a solution. Is this system even solvable at all? If so, how can it be done?","['nonlinear-system', 'algebra-precalculus', 'systems-of-equations', 'polynomials']"
4928863,Upper semicontinuity of sequence of Hausdorff measures,"This is exercise 12.2 of Measure theory and integration of Michael Taylor.
Let $(d_j)$ be a sequence of metrics on a compact space $X$ such that there exists $c,C>0$ with $$ cd_0(x,y) \leq d_j(x,y) \leq Cd_0(x,y) $$ for all $x,y\in X$ .
Suppose also that $(d_j)$ converge uniformly to a metric $d$ .
I want to show that, for any $S$ Borel subset, $\delta > 0$ and $\varepsilon > 0$ , $$ \mathcal{H}_{d_j,\lambda\delta}^r(S) \leq \mathcal{H}_{d, \delta}^r(S) + \varepsilon$$ with $j$ big enough and $\lambda = C/c$ and $$ \mathcal{H}^r_{d,\delta}(S) = \inf\{ \sum (\operatorname{diam}_d U_i)^r \mid S \subset \cup U_i, \operatorname{diam} U_i \leq \delta \} $$ the approximate $\delta$ -Hausdorff measure.
However when I try I always get a factor $(c/C)^r$ and I don't know how to get ride of it as it is not clear if we can rescale our cover. $$
\begin{aligned}
\mathcal{H}^r_{d,\delta}(S) + \varepsilon &\geq
\sum (\operatorname{diam}_d U_i)^r \\
&\geq \sum \left( \frac{c}{C} \operatorname{diam}_{d_j} U_i\right)^r
\end{aligned}
$$","['measure-theory', 'hausdorff-measure', 'metric-spaces', 'analysis', 'real-analysis']"
4928870,Product of two iterated commutators and their length.,"Given the free group $F_n$ on $n$ generators,
I strongly suspect and want to show that if we are given two iterated commutators $w_1,w_2$ which are not inverses of each other, then the length of their product (as a reduced word) is longer than the minimum of the lengths of the two, i.e. $$ \ell(w_1w_2) \geq \min\{\ell(w_1),\ell(w_2)\}$$ This problem reduces to showing that the multiplication can‘t reduce more than „half“ of both commutators, so if $w_1=u_1^{-1}v_1^{-1}u_1v_1$ then we can’t reduce more than the $u_1v_1$ part if the $u_2^{-1}v_2^{-1}$ part was also reduced in $w_2=u_2^{-1}v_2^{-1}u_2v_2$ . This is the only thing I need to prove, the other cases fulfill the inequality easily. Is this a known problem? Maybe the inequality is false, but I already know from a proven result that it cannot be far off. Here are some ideas I had: I tried to do an induction: Assuming more than half gets reduced in both, them we can use induction hypothesis on $v_1u_2^{-1}$ to see that one of them is not reduced more than half. Lets assume $v_1$ gets completely absorbed. How would I know what $u_1$ does to the rest of $u_2^{-1}$ ? The problem is that even if a commutator gets eaten completely, it wasn't necessarily a full commutator when we made the other commutator, $$ [a,b][b,[b^{-1},a]]=[a,b]\underbrace{b^{-1}[b^{-1},}_{\text{gets eaten}}a]^{-1}b[b^{-1}a]=a^{-1}b^{-1}abb^{-1}a^{-1}bab^{-1}b[b^{-1},a]=\underbrace{b^{-1}}_{\text{part of }[b^{-1},a]^{-1}}b[b^{-1},a]$$ What I mean is that a commutator gets cancelled without explicitly looking like a commutator in the other word, which makes things more complicated. If we have equality, as in my example, then we probably get another commutator again. The idea is to ultimately show that iterated commutator can't generate elements shorter than an iterated commutator (assuming they have to contain a certain number of generators). I guess we could also assume by induction that what remains of $u_2^{-1}$ is just a product of iterated commutators (like no more than three)? I would also be happy with showing this inequality if $w_1$ and $w_2$ both contain all $n$ generators in some way (maybe we could break down the commutators until the set of generators in two subcommutators are not contained in each other).","['combinatorial-group-theory', 'group-theory', 'abstract-algebra']"
4928899,"Does it make sense to consider function, that has finite zero value at limit point as ""infinitely small""?","Studying the calculus, I am currently reading the chapter ""Comparison of infinitely small functions"". It is started to overview ratios such as $$\lim_{x->x_0}\dfrac{f(x)}{g(x)}=0 \space,  \space \lim_{x->x_0}\dfrac{f(x)}{g(x)}=\infty$$ , where $f(x)$ and $g(x)$ are infinitely small functions. I assume, that by ""infinitely small at limit point $x_0$ function"" meant $\lim_{x->x_0}f(x)=0$ . Reading, I started to think about examples, and I could provide only $f(x)=\dfrac{a}{x}$ at infinity. But then, I see examples of infinitely smalls like $f(x)=x$ at $x->0$ . According to the limit definition, since $\delta$ -neighborhood of $x_0$ considered as deleted , i.e. $0<|x-x_0|<\delta(\epsilon)$ , it seems, like, formally, there is no requirement for function value at limit point itself. I understand why - for cases like $1/x$ , but exactly because of this, as I understand, the idea of infinitely small was invented - to describe the smallest, yet non-zero value , and so called ""infinitely small (function)"" is what mathematics such as Euler, Cauchy, and other invented. If it is true, is not consider functions such $x^3$ as infinitely small intuitively wrong, or I missing some ideas? Maybe historically, the idea of limit of function as argument tends to limit point - was the closest function value to function value at limit point? But the method of finding the closest value to some point was not found, and instead, we have vice-versa: we found a point, that is called an accumulating point, which any $\epsilon$ -neighourhood contains at least one value of function at some another accumulating point, called limit point. So it still makes sense to find what is the closest value of $0$ for $x^3$ , but calculus does not provide such tool, instead providing the tool to compare that infinitely small value to another, for example $x^2$ , because, how it may be non-logical, we see that ""infinitely closest"" to $0$ of $x^3$ is different from ""infinitely closest"" to $0$ of $x^2$ ?","['limits', 'calculus']"
4928961,"How can I check that for a quasi-compact morphism $f:Z\to X$, the kernel of $\mathcal{O}_X\to f_*\mathcal{O}_Z$ is quasi-coherent?","If $f:Z\rightarrow X$ is a quasi compact morphism of schemes and $\mathcal J  :=\ker(\mathcal O_X \rightarrow f_* \mathcal{O}_Z)$ , then $(\operatorname{Supp}( \mathcal O_X/ \mathcal J)$ , $i^{-1}(\mathcal O_X/\mathcal J))$ is a scheme. Where $i: Supp( \mathcal O_X/ \mathcal J)\rightarrow X$ denotes the inclusion map. How to prove this statement? I think it is enough to check that the ideal sheaf $\mathcal J$ is quasi-coherent. That is, for any $D(g)\subset U=\operatorname{Spec} A$ affine open, we need to check $J(D(g))=I A_g$ where $I=\mathcal J(U)$ . That is what I think : $$\mathcal J(D(f))=\ker(\mathcal O_X(D(f))\rightarrow \mathcal O_Z(f^{-1}(D(f))))\\
=\ker(A_f\rightarrow \mathcal O_Z(\cup_{i=1}^nU_i))\\
=\ker(A_f\rightarrow  \mathcal O_Z(\cup_{i=1}^nU_i) \rightarrow \bigoplus_{i=1}^nB_{if_i})$$ The second ""="" is because we assume $f$ is quasi compact and $D(f)$ is quasi compact, hence its preimage is quasi compact and hence can be covered by finitely many affine open $U_i=\operatorname{Spec} B_i$ . The last ""="" is because for any sheaf $\mathscr F$ and for any open cover $U=\cup_i U_i$ , the map $\mathscr F(U)\rightarrow \Pi_i\mathscr F(U_i)$ is always injecive. Hence, $\mathcal O_Z(\cup_{i=1}^nU_i) \rightarrow \Pi_{i=1}^n\mathcal O_Z(U_i)$ is injective and we can assume $U_i=D(f_i)$ are principal open for $f_i\in B_i$ then $\mathcal O_Z(U_i)=B_{i,f_i}$ . We can replace product by direct sum since it is finite. Composing a injective map does not change the kernel. Am I going in the right direction? I do not know how to continue, I am thinking maybe I am completely wrong.","['affine-schemes', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
4928963,Dimension of the center of a block,"Let $G$ be a finite group and let $F$ be an algebraically closed field of characteristic $p$ . I've been studying some modular character theory from Navarro's ""Characters and blocks of finite groups"" and came across the following question: Let $B$ be a block of a finite group $G$ . Then, show that the number of ordinary irreducible characters belonging to $B$ is the dimension of $Z(B)$ over $F$ . Just for some context, we can decompose the algebra $FG$ as $$FG = \bigoplus_{B \in \operatorname{Bl}(G)} B$$ where the blocks $B$ are indecomposable ideals of $FG$ . Each of them is an algebra with unity $e_B$ , where this is a primitive central idempotent associated to the decomposition. Then, a module $M$ belongs to $B$ if the idempotent acts as a unity in $M$ ; otherwise, $e_B M = 0$ . If $p \nmid |G|$ , then these blocks are just the matrix algebras given in the Wedderburn-Artin decomposition of $FG$ , and each of them is associated to a unique and distinct irreducible representation of $G$ . Also, the center of each is one-dimensional, meaning the problem is true in this case. This is actually how we get that the dimension of the center of $Z(FG)$ as an $F$ -vector space is the number of distinct irreducible representations of $G$ up to equivalence. However, if $p \mid |G|$ , the argument above doesn't work anymore, as $FG$ is no longer semi-simple. In that case, I don't know how to analyze $\dim_FZ(B)$ ... I tried to use the standard basis of $Z(FG)$ , which are the class sums of $G$ , but couldn't get a basis for $Z(B)$ from that... Does anyone have a hint as for how to solve the question above? Thanks in advance! EDIT: fixed a small mistake. Also, just to clarify, I put some definitions and the like in the question, but feel free to use more than that!","['group-theory', 'representation-theory', 'idempotents', 'characters']"
4928975,Isomorphism of topological groups,"Question : Let $\mathbb{Q}(\sqrt{\mathbb{Q}})$ be the subfield of $\overline{\mathbb{Q}}$ generated by $\{\sqrt{x} : x \in \mathbb{Q}\}$ . Prove that $\mathbb{Q} \subset \mathbb{Q}(\sqrt{\mathbb{Q}})$ is Galois, and that the map $\psi : \text{Gal}(\mathbb{Q}(\sqrt{\mathbb{Q}}) / \mathbb{Q}) \rightarrow \text{Hom}(\mathbb{Q}^{*}, \{\pm 1\}) : \sigma \mapsto (a \mapsto \sigma(\sqrt{a})/\sqrt{a})$ is an isomorphism of topological groups, if $ \text{Hom} (\mathbb{Q}^{*}, \{ \pm 1 \} )$ has the relative topology inside $\{ \pm 1 \}^{\mathbb{Q}^{*}}$ . Prove also that this Galois group is isomorphic to the product of a countable infinite collection of copies of $\pm 1$ . My attempt : First I thought we could recognize the field as $\mathbb{Q}(\sqrt{\mathbb{Q}}) = \mathbb{Q}(\sqrt{-1}, \sqrt{2}, \sqrt{3}, \sqrt{5}, \dots)$ . This field extension is clearly algebraic, also separable since the characteristic is 0. Furthermore this extension is normal because for any $\alpha \in \mathbb{Q}(\sqrt{\mathbb{Q}})$ we can consider $f^{\alpha}_{\mathbb{Q}} = X^{2} - \alpha^{2}$ to see that this polynomial splits into linear factors in $\mathbb{Q}(\sqrt{\mathbb{Q}})[X]$ . Hence, it is a Galois extension. To show that $\psi$ is a group morphism, let $\sigma, \tau \in \text{Gal}(\mathbb{Q}(\sqrt{\mathbb{Q}}) / \mathbb{Q})$ and let $a \in \mathbb{Q}^{*}$ . then we have that $\frac{\sigma(\sqrt{a})}{\sqrt{a}} \cdot \frac{\tau(\sqrt{a})}{\sqrt{a}} = \frac{\sigma(\sqrt{a} \cdot \frac{\tau(\sqrt{a})}{\sqrt{a}})}{\sqrt{a}} = \frac{(\sigma \circ \tau)(\sqrt{a})}{\sqrt{a}}$ . Hence, $\psi$ is a group morphism. For injectivity, suppose that $\sigma \in \ker\psi$ . Then $\sigma$ must be de identity on all $\sqrt{a}$ for $a \in \mathbb{Q}^{*}$ . But then $\sigma$ is the identity on all the generators of $\mathbb{Q}(\sqrt{\mathbb{Q}})$ , so $\sigma$ must be the identity. Hence, $\ker\psi$ is trivial and thus $\psi$ is injective. Here I am a bit stuck, I am struggling to finish the proof that $\psi$ is an isomorphism of topological groups. If anybody has hints on how to proceed I would appreciate that. Furthermore, I would like to hear if my following argument is correct for proving that the Galois group is isomorphic to the product of a countable infinite collection of copies of $\{ \pm 1\}$ : The way we identified $\mathbb{Q}(\sqrt{\mathbb{Q}})$ as the field $\mathbb{Q}(\sqrt{-1}, \sqrt{2}, \sqrt{3}, \dots)$ gives us an isomorphism $\text{Gal}(\mathbb{Q}(\sqrt{\mathbb{Q}}) / \mathbb{Q}) \cong \{\pm 1\} \times \prod_{p \text{ prime }} \{\pm 1\}$ since we can flip the sign of any $\alpha$ being a generator. Each one of these non-trivial automorphisms of order 2 will correspond with a copy of $\{\pm 1\}$ . Furthermore it is an infinite amount of copies since each $\sqrt{p_{i}}$ and $\sqrt{p_{j}}$ are linearly independent. Edit : I think the idea is to endow $\{ \pm 1 \}^{\mathcal{P}}$ where $\mathcal{P} = \{-1\} \cup \{p : p \text{ prime}\}$ with the product topology and then show that $\psi$ is continuous, since a continuous bijection from a compact space to a Hausdorff space is a homeomorphism.","['group-isomorphism', 'topological-groups', 'field-theory', 'galois-theory', 'abstract-algebra']"
4929038,Implicit function equation $f(x) + \log(f(x)) = x$,"Is there a function $f \colon \mathbb{R}_{>0} \to \mathbb{R}_{>0}$ such that $$
f(x) + \log(f(x)) = x
$$ for all $x \in \mathbb{R}_{>0}$ ? I have tried rewriting it as a differential equation by introducing $g(x) := f(x) + \log(f(x)) - x$ , which is constant $=0$ , so $$
0 = g'(x) = f'(x) + \frac{f'(x)}{f(x)} - 1
$$ which is equivalent to $$
f'(x) \left( \frac{f(x)+1}{f(x)}\right) = 1,
$$ but when I solve that, I get back the original implicit function.","['implicit-function', 'exponential-function', 'ordinary-differential-equations', 'logarithms']"
4929072,"Closed form of $\int_{(0,1)^5} \frac{xyuvw}{(1-(1-xy)u)(1-(1-xy)v)(1-(1-xy)w))}\, dx\,dy\,du\,dv\,dw$","I need closed form for the integral $$I:=\int_{(0,1)^5} \frac{xyuvw}{(1-(1-xy)u)(1-(1-xy)v)(1-(1-xy)w))} \, dx\,dy\,du\,dv\,dw$$ Then $$0<I<\int_{(0,1)^5} \frac{1}{(1-(1-xy)u)(1-(1-xy)v)(1-(1-xy)w))} \,dx\,dy\,du\,dv\,dw$$ So $$0<I<\int_{(0,1)^2} \frac{-\log^3(xy)}{(1-xy)^3} \, dx\,dy$$ By Wolfram alpha $$0<I<27.4126$$ So the given integral converges. How do we find a closed form of this integral may be in terms of Riemann zeta function? Any help would be highly appreciated. Thank you.","['integration', 'number-theory', 'analytic-number-theory', 'multiple-integral', 'riemann-zeta']"
4929115,What are some examples of algebraic curves with maximum number of real components,"Harnack's curve theorem states that for any algebraic curve of degree $m$ in the real projective plane, the number of components $c$ is bounded by $\frac{1-(-1)^m}{2} \le c \le \frac{(m-1)(m-2)}{2}+1$ and that any number of components in this range can be attained. The Trott curve is an example of a degree 4 curve with the 4 components. What are some examples of degree 5 curves with 7 components and degree 6 curves with 11 components?",['algebraic-geometry']
4929124,"What is the product topology on $\{ 1, \ldots, k \}^{\mathbb R}$? How does Tychonoff’s theorem imply that it is compact?","I'm not well-versed in topology and encountered this as part of a larger argument about coloring the real numbers: ""An arbitrary Cartesian product of compact topological spaces is compact (Tychonoff's Theorem), and in particular, the space $M$ of all mappings $f: \mathbb{R} \rightarrow [k]$ is compact."" (Here, $k$ is a positive integer and $[k] = \{1, \ldots, k\}$ .) The topology they define on this space is ""that of the Cartesian power $[k]^{\mathbb{R}}$ ""; I am not familiar with this topology. I don't see the connection between Tychonoff's Theorem and the statement. In other words, in what sense is $M$ a Cartesian product of compact topological spaces? This confuses me because the elements of $M$ are mappings , and it is not clear to me how a mapping itself could be an element of a Cartesian product.","['general-topology', 'real-analysis']"
4929130,How to solve $\int\frac{e^{x/2}\cos x}{\sqrt[3]{3\cos x+4\sin x}}dx$?,"How do I solve the integral: $\int\frac{e^{x/2}\cos x}{\sqrt[3]{3\cos x+4\sin x}}dx$ . It was a question from the graduation prank at my school. I found it interesting and tried like substituting the third root in the denominator but it didn't work out. My second try was to think from the answer that WolframAlpha provided : $\frac{6}{25}e^{x/2}\sqrt[3]{(3\cos x+4\sin x)^2} +C$ , because $\frac{6}{25} = \frac{2}{5}*\frac{3}{5}$ and tried that the inside of the third root was substituted because $\sqrt[3]{u^2}$ integrates to $\frac{3u\sqrt[3]{u^2}}{5}$ . But than I had no idea.",['integration']
4929169,Randomly select only $10$ values out of numbers $1-30$ without replacement & keeping the order they are drawn,"I have values 1 through 30 and I want to randomly select only 10 of these values without replacement. Once a value is selected it is locked into that order placement from when it was chosen and cannot be selected again. I can't figure out the math behind this, but I attempted to visualize the sequence process in Excel. For example in the screenshot of Excel, 16 was the first value to be randomly selected from 1-30. This fixed 16 to be the first value in the sequence for the remainder of the iterations. It also removed the value 16 from the selection choices and dropped the range to 1-29. This continues until 10 values are randomly selected and placed in the order in which they were drawn. At first, I thought it would be like $C(n,r) = C(30,10) = 30,045,015$ combinations, but upon further examination, I'm not too sure that's the correct route to go with since this is a sequence-type problem. Can you help me understand this as a theory, equation, or formula? I'm hoping to make a code that can handle this random process, but first I want to better understand what I am dealing with at the moment. Thanks in advance!","['combinatorics', 'discrete-mathematics']"
4929189,Give X a differentiable structure such that the inclusion map is smooth,"Let $X = ¥{(x,y,z) ¥in R^3: z^4 = x^2+y^2, z ¥geq 0¥}$ . First I'm trying to prove that this is not a smooth submanifold of $R^3$ . Clearly the problematic point is $(0,0,0)$ however I haven't been able to prove formally that this point fails. For reference I'm trying to do an elementary proof of this, so this should basically be a result of the definition of a submanifold. I tried to asume the existence of a chart $¥varphi$ of a neighborhood of $(0,0,0)$ to $R^2$ and with this try to consider the projection $¥pi$ from $X$ to $R^2$ and consider the map $¥pi ¥circ ¥varphi^{-1}$ and use inverse function theorem, but the derivative of this function is not invertible, so I didn't know how to proceed. Second for the differential structure I took the atlas generated by the chart $¥phi(x,y,z) = (x^{1/3},y^{1/3})$ . Now with this atlas the inclusion map is differentiable but not smooth, so something like this should work, but I don't know exactly what chart to take. Any help would be greatly appreciated.","['smooth-functions', 'smooth-manifolds', 'differential-topology', 'derivatives', 'differential-geometry']"
4929205,"General form of finite partitions of $\{0,1\}^{\mathbb{Z}}$.","Is the following statement true? If so, how does one go around proving it? ""Let $\alpha$ be a finite partition of $\{0,1\}^{\mathbb{Z}}$ , then there exists a finite subset of the integers $K \subset \mathbb{Z}$ such that $\alpha$ can be written as the collection of sets with the following structure: $A = \{ (x_i)_{i \in \mathbb{Z}}|\, \forall k \in K, x_k = a_k\}$ , with $a_k$ a given sequence of elements of $\{0, 1\}$ , save for sets of measure zero."" The measure is the uniform Bernoulli measure, that is the product measure defined on the cylinders by $\mu(\{ (x_i)_{i \in \mathbb{Z}}|\, \forall k \in K, x_k = a_k\}) = 2^{-|K|}$ . It ocurred to me this could be true, and it would help me prove a subsequent result, but I don't know how it could be proven.","['measure-theory', 'combinatorics', 'set-partition']"
4929233,How would I calculate the flux of a given vector field through a surface?,"I'm trying to work out a problem where I need to calculate the flux of the vector field $A= \langle xy, yz, zx \rangle$ through the shape pictured below. So far I've set up the following integral... $\oint_{S} \mathbf{A} \cdot \mathrm{d} \mathbf{a}$ ; with $\mathrm{d}\mathbf{a}$ being an infintesimally small area. And I've worked it out to the following result: $$\begin{align*}
\oint_{S} \mathbf{A} \cdot \mathrm{d}\mathbf{a} 
&= \int_S \left( xy \, \mathrm{d}y \, \mathrm{d}z + yz \, \mathrm{d}x \, \mathrm{d}z + zx \, \mathrm{d}x \, \mathrm{d}y \right) \\
&= \int_0^c \int_0^b xy \, \mathrm{d}y \, \mathrm{d}z + \int_0^c \int_0^a yz \, \mathrm{d}x \, \mathrm{d}z + \int_0^b \int_0^a xy \, \mathrm{d}x \, \mathrm{d}y
\end{align*}$$ Is there no way around evaluating these three double integrals, or did I just do the problem wrong? I know I can get the same answer by evaluating $\int \nabla \cdot \mathbf{A}\, \mathrm{d}\tau$ over the volume of the shape, but I specifically want to know how to setup the problem using the surface integral.","['vector-fields', 'surface-integrals', 'multivariable-calculus', 'calculus', 'vector-analysis']"
4929262,"Minimizing $\left(\frac{c}{a} + \frac{c}{b}\right)^2$, where $c$ is the hypotenuse of a right triangle with legs $a$ and $b$","This question is regarding the following problem Given that $a, b, c$ are the sides of the $\triangle ABC$ which is right angled at $C$ , then what is the minimum value of the following expression? $$\left(\frac{c}{a} + \frac{c}{b}\right)^2$$ My approach to solving the above problem was as follows, i factored out $c$ and we get the following $$c^2\left(\frac{1}{a} + \frac{1}{b}\right)^2 \tag1$$ $$c^2\left(\frac{1}{a^2} + \frac{1}{b^2} + \frac{2}{ab}\right) \tag2$$ Now using the inverse pythagorean theorem we get $$c^2\left(\frac{1}{c^2} + \frac{2}{ab}\right) \tag3$$ $$1 + \frac{2c^2}{ab} \tag4$$ Now by using the $AM > GM$ inequality $$\frac{a+b}{2} \geq \sqrt{ab} \tag5$$ $$\frac{c^2+2ab}{4} \geq ab \tag6$$ $$c^2 \geq 2ab \tag7$$ $$\frac{c^2}{ab} \geq 2 \tag8$$ Now using this fact at the equation we obtained earlier we find that the minimum value of the expression in the question happens to be $5$ . However the answer to the above question is given to be $8$ in my book. Am I wrong, or the book? And if I am wrong, then where? Any help would be highly appreciated.","['triangles', 'inequality', 'trigonometry', 'a.m.-g.m.-inequality']"
4929278,I found a bound for roots in polynomial equation. Didn’t know if it is original.,"One day I was trying to solve cubic equation without cubic formula(because it is computationally complicated and no algebraic way to denest cubic root). So I tried rational root theorem, which works when there is actually a rational root obviously. But sometimes the finite options that the theorem yield is a bit too much. So I have been working on ways to decrease the number of possibilities of roots. And one of the ways is to find the bound of roots. And I’ve discover 2 bounds on my own. One of which is Cauchy’s bound, and the other… I can’t found it anywhere on the internet. And the proof is really easy unless I mess something up. So it would be very weird if no-one has found it yet. Anyways, here’s the bound: For a degree n polynomial $$P(x)= a_n x^n + a_{n-1} x^{n-1} + … + a_1 x + a_0,$$ every non zero roots of $P(x) = 0$ satisfies the boundary $$ \frac 1 m \le |x| \le M $$ where $$ M = \max_{1 \leq i \leq n} \sqrt[i\,]{\left | \frac{na_{n-i}}{a_n} \right | } $$ and $$ m = \max_{1 \leq i \leq n} \sqrt[i\, ]{\left | \frac{na_i}{a_0} \right |}  $$ Proof : Rewrite $P(x) = 0$ into $$ n = -(\frac{na_{n-1}}{a_n} \frac 1 x + \frac{na_{n-2}}{a_n} \frac 1 {x^2} + … + \frac{na_{1}}{a_n} \frac 1 {x^{n-1}} + \frac{na_{0}}{a_n} \frac 1 {x^n})$$ by putting $x^n$ on LHS and everything else on RHS and divide both side by $x^n/n$ And if every terms on RHS is less than 1, then $P(x) \ne 0$ .
Any M larger than $$ \max_{1 \leq i \leq n} \sqrt[i\, ]{\left | \frac{na_{n-i}}{a_n} \right | } $$ does that. $$ n = \sum_{k=1}^n{1} = \sum_{k=1}^n{\frac{-na_{n-k}/{a_n}}{-na_{n-k}/{a_n}}} > \sum_{k=1}^n{\frac{-na_{n-k}/{a_n}}{M^k}} $$ Any $x$ larger or equal to M would be equality destroyer. Thus deriving the upper bound in the proof. For lower bound, use $x=1/y$ substitution and apply upper bound to $y$ ’s polynomial. Easy proof(if not wrong), the bound may seem bad at first. But you can optimize it in a few ways. Like vanishing the term that has the same sign as $a_n$ . And cancel terms by grouping. But that isn’t the point today. New theorem, old theorem, or a false one? I am sure it wasn’t trivial. As it is quite useful. Also I am just an high schooler, I haven’t learnt to write proof formally. And it is the first time of me using latex. Edit: If you feel like this might be new, how do I publish the result to let everyone know? How can I strengthen the bound?(using maybe like the second largest coefficient or smth else)","['algebra-precalculus', 'solution-verification', 'polynomials', 'upper-lower-bounds']"
4929279,Proving 2 triangles are congruent,"Given $\Delta ABC, \Delta A'B'C'$ s.t $\widehat{BAC}=\widehat{B'A'C'}, BC=B'C', AD=A'D'$ $(AD, A'D'$ are internal bisectors of $\widehat{BAC}, \widehat{B'A'C'}$ respectively). Prove that $\Delta
 ABC=\Delta A'B'C'$ . My attempt: Let $E\in\vec{AB}, F\in\vec{AC}$ s.t $AE=A'B', AF=A'C'$ .
Then $\Delta AEF=\Delta A'B'C'\Rightarrow EF=B'C'$ . On the other hand $\Delta AED=\Delta A'B'D'\Rightarrow ED=B'D'$ . Similarly, $DF=D'C'$ . So, $ED+DF=B'D'+D'C'=B'C'=EF\Rightarrow  E,D,F$ are collinear and $D$ is between $E,F$ . I'm stuck here :( . Could someone help me? Here's a picture: Thanks in advance.","['congruences-geometry', 'triangles', 'geometry']"
4929331,Equivalent definitions of affine function: Does $f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y)$ on convex set $S$ imply linearity of $f$ on $S$?,"For a function $f\colon\mathbb{R}^n\to\mathbb{R}$ the following two are equivalent \begin{align*}
&\text{(i) there exist $a\in\mathbb{R}^n$, $b\in\mathbb{R}$ so that $f(x)=a^{t}x+b$}\newline
&\text{(ii) for all $x,y\in\mathbb{R}^n$ and $\gamma\in(0,1)$ it holds $f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y)$}.
\end{align*} See for example here for a proof I want to argue that the statement still holds if we consider functions $f:X\to\mathbb{R}$ where $X\subset\mathbb{R}^n$ is a convex set. My intuitive idea: Obviously $(i)\implies (ii)$ still holds in that case. On the other hand, if we assume that (ii) is true, then we could consider the affine space spanned by $X$ . Let us call this affine space $Y$ . I think it should be true, that the interior of $X$ is open in $Y$ . So we could probably define an affine function $\tilde{f}:Y\to\mathbb{R}$ that extends $f$ first. And then extend $\tilde{f}$ to all of $\mathbb{R}^n$ (somehow). I am hoping that someone knows how to do this rigorously.","['convex-analysis', 'analysis', 'real-analysis']"
4929356,Is there a website that has all the special functions? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 18 days ago . Improve this question There are a lot of special functions, and I wonder if there is a website that collects all of them, similar to how the Encyclopedia of Triangle Centers collects information on triangle centers. Another question is: What are the criteria for defining a new special function? One can define anything if they want to, so is there any established criteria for what would be considered and accepted as a new special function? For example I  can just define a function like $\Theta(x)$ to be the inverse function for $e^{\sin(x)}\cos(x)+ 5x^x-\Gamma(\gamma x) $ where $x\in [0, \frac \pi 2 ]$ Now this is a new function, what one should do to make similar functions an ""acceptable"" special function? I think  a function to be considered a new one is that it can't be obtained via definite combinations of the known functions. In geometry if one discovers a new triangle center he can just submit it to ETC I wonder if  a similar thing exist for special functions? It seems like the such websites don't exist but I think a websites  that collects special functions like how ETC collect triangle centers is a good idea that I want to make this website but I don't have enough knowledge in mathematics or programming  to make such thing I wonder whether I can request such Idea in MSE or MO for the community ton do.","['special-functions', 'analysis', 'real-analysis', 'complex-analysis', 'soft-question']"
4929400,Find the sum $PC^2+PD^2$ in the trapezoid inscribed below,"If there is a semicircle of diameter $AB$ in which an isosceles trapezoid $ABCD$ , ( $AB \parallel CD$ ) is inscribed. On $AB$ , we take a point "" $P$ "" such that $PA^2 + PB^2 = 5^2$ .
Calculate: $PC^2 + PD^2$ . I made the drawing and tried it. $\triangle ADP: PD^2 = l^2+PA^2 - 2lPA\cos\theta\ (I)$ $\triangle PCB: PC^2 = l^2+PB^2 - 2lPB\cos\theta\ (II)$ $(I)+(II):\ PC^2+PD^2 = 2l^2+PA^2+PB^2-2l\cos\theta(PA+PB)$ $\underbrace{PC^2+PD^2} =2l^2+ 25-2l\cos\theta(PA+PB)$ $\underbrace{PC^2+PD^2} =2l^2+ 25-2l\cos\theta(2R)$ I couldn't finish it....","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'plane-geometry']"
4929402,"Estimation of $H_{n,\frac{1}{2}}$","Question : Determine the value of $\lfloor{ H_{120,\frac{1}{2}} }\rfloor$ My Attempt: Let $$S=\sum_{j=1}^{120}\frac{1}{\sqrt{j}} $$ then we can use the following inequality $$\int_{0}^{120}\frac{1}{\sqrt{x}}dx>S>\int_{1}^{121}\frac{1}{\sqrt{x}}dx$$ $$2\sqrt{120}>S>20$$ $$22-\Delta>S>20$$ so this approximation was not enough redoing this for $$S-1=\sum_{j=2}^{120}\frac{1}{\sqrt{j}}$$ $$\int_{1}^{120}\frac{1}{\sqrt{x}}dx>S-1>\int_{2}^{121}\frac{1}{\sqrt{x}}dx$$ $$2\sqrt{120}-1>S>23-2\sqrt{2}$$ $$21-2\Delta>S>20\implies \lfloor{ H_{120,\frac{1}{2}} }\rfloor=20$$ I was asking if there a better method for this which would do this in one go rather than 2 times...","['algebra-precalculus', 'sequences-and-series']"
4929414,Drawing an equilateral triangle on lattice square grid,"I know that it's not possible to construct an equilateral triangle on lattice square grid if its vertices must be lattice points. But, suppose you only have a lattice square grid and a straightedge (and a tool like pencil or pen to draw). Is it possible to construct several lines that will then, in fact, create an equilateral triangle?","['euclidean-geometry', 'geometry', 'geometric-construction']"
4929434,Why is the magnitude of the cross product equal to the parallelogram spanned by the two vectors?,"Given vectors $a, b \in \mathbb{R}^3$ , the cross (vector) product of $a \times b = c$ is defined as a vector orthogonal to both $a$ and $b$ such that the right hand rule is satisfied, with the additional stipulation that $\|c\|=\|a\|\|b\|\sin(\theta)$ where $\theta$ is the angle between $a,b$ . But any vector $c’ \in \mathbb{R}^3$ such that \begin{align*}
a \cdot c’ &= 0, \\
b \cdot c’ &=0
\end{align*} satisfies the orthogonality condition. For non-parallel $a,b$ , there are infinitely many vectors $c’$ that do this (by the rank theorem). Albeit, these $c’$ lie on the same line. My questions are: What is the purpose of wanting the cross product to have magnitude equal to the parallelogram spanned by $a$ and $b$ ? How does the determinant formulation of the cross product $$
c = \mathrm{det} \begin{bmatrix}
\hat{\imath} & \hat{\jmath} & \hat{k} \\
a_x & a_y & a_z \\
b_x & b_y & b_z
\end{bmatrix}
$$ ensure that $\|c\|=\|a\|\|b\|\sin(\theta)$ ? I understand how to work backwards, i.e. to show that given this formulation, $\|c\|=\|a\|\|b\|\sin(\theta)$ . I am more interested in working forwards: why does this particular formulation give the desired magnitude?","['cross-product', 'multivariable-calculus', 'vectors', 'vector-analysis']"
4929443,Linearity of differential forms,"I put my hands on ""Linear Algebra"" by Serge Lang, second edition, and I noticed that it contains some sections that were later removed in the following third one. In one of the removed parts from that edition there was a section about tensor products and differential forms. Here, assuming that $\{e^1,e^2,e^3\}$ is a basis of $V$ and $\{dx,dy,dz\}$ is a basis of $V^*$ , it is stated [XIII, §3, p.314]: [...] By a differential form on $\mathbb R^3$ of degree $2$ , one means a map ( not necessarily linear !) $$\omega: \mathbb R^3\rightarrow V^*\land V^*$$ form $\mathbb R^3$ into the alternating product of the dual space with itself. [...] Now, I am a bit confused as I thought differential forms were linear, am I wrong? Can you make one/some examples?","['tensors', 'linear-algebra', 'tensor-products', 'differential-forms', 'differential-geometry']"
4929488,"A circle is inscribed in a triangle, with three other circles in the corner regions. The radii are integers. Possible values of the largest radius?","Consider four circles with integer radii inscribed in a triangle as shown. That is, a circle with integer radius $R$ is inscribed in a triangle, and three other circles with integer radii $a,b,c$ are each tangent to the large circle and two sides of the triangle. Given an arbitrary positive integer, how can we determine if it is a possible value of $R$ ? (As an analogy, here is a similar question, and its answer: Given an arbitrary positive integer, how can we determine if it is the hypotenuse of a right triangle with integer side lengths? The answer is by checking if it has a prime factor of the form $4k+1$ .) My attempt It turns out that $R=\sqrt{ab}+\sqrt{bc}+\sqrt{ac}$ . So we require $R=\sqrt{ab}+\sqrt{bc}+\sqrt{ac}$ and $R>\max(a,b,c)$ where $R, a,b,c\in\mathbb{Z^+}$ . If we temporarily relax the requirement that $R>\max(a,b,c)$ , then it can be shown that $R$ can be any positive integer except $1,2,4$ , as follows (but I'm not sure if this is useful). $R$ can be any odd number greater than $1$ , because if $a=1,\space b=1,\space c=n^2$ , then $R=2n+1$ . As @Zoe Allen noted in the comments, if $a,b,c$ give $R$ , then $2a,2b,2c$ give $2R$ , so $R$ can be $2^k(2n+1)$ , so it only remains to consider the powers of $2$ . As @John Omielan noted, if $a=1,\space b=4,\space c=4$ then $R=8$ , so $R$ can be any power of $2$ larger than $4$ . Clearly $R$ cannot equal $2$ . It is easy to show that $R$ cannot equal $4$ (if $R\le 4$ then at least two of $a,b,c,$ must equal $1$ , but then the third cannot make $R=4$ ). So $R$ can be any positive integer except $1,2,4$ . But with the requirement that $R>\max(a,b,c)$ , there are many other numbers that cannot be $R$ values. As @Will Jagy noted, the numbers below $100$ that cannot be $R$ values are $1,2,4,7,13,14,17,23,28,34,37,43,46,49,53,67,73,97,98$ . (As an example, $R=11$ is not on this list because $11=\sqrt{1\cdot 4}+\sqrt{4\cdot 9}+\sqrt{1\cdot 9}$ .) This seems like an almost random sequence of numbers. The OEIS does not have this sequence (but the OEIS has a conjectured sequence of numbers not of the form $ab+bc+ac$ ). Edit : Posted on MO .","['elementary-number-theory', 'triangles', 'circles', 'geometry']"
4929527,Can I apply taylor expansion to exponents?,"If I have a limit in this form: $$\lim_{x\to x_0}f(x)^{g(x)}$$ can I expand $f(x)$ and $g(x)$ to end up with a limit like: $$\lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2 + \cdots\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2 + \cdots}$$ then cut up the series and write: $$\lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2}$$ and work on this one instead? Or is this not allowed for some obscure reason? $x_0$ is within the radius of convergence of both expansions. EDIT: I'll add an example which I've seen today on this website: $$\lim_{\theta\to0^+}(\sin\theta)^{\sin\theta-\sin^2\theta}$$ Can I use the fact that $\sin\theta\sim\theta$ and say that this limit is equivalent to the following? $$\lim_{\theta\to0^+}\theta^{\theta(1-\theta)}$$ The result is the same, but it could be a coincidence. Is this procedure correct here? In the general case? Thanks in advance.","['limits', 'calculus', 'taylor-expansion']"
4929544,Fiber product of schemes and tensor calculations,"In Vakil's notes, starting from 10.2.A , he listed a few tensor identities to help calculating fiber products of schemes, for example (supposing $B\to A$ a ring map, giving $A$ the structure of a $B$ -algebra): Adding an extra variable: $A \otimes_B B[t] \cong A[t]$ , Base change by closed embeddings: $A/I^e \cong A \otimes_B (B/I)$ , Base change of affine schemes by localization $S^{-1}A\cong A\otimes_B S^{-1}B$ . With these basic tools we're able to compute various tensor products (hence fiber product of schemes), and I'm trying to apply this to an example made up by myself. Consider the projections of $\Bbb A^2=\operatorname{Spec} k[x,y]$ to the diagonal line $Z=\operatorname{Spec} k[x,y]/(x-y)$ from the horizontal and vertical directions, namely: How do we calculate the fiber product? Topologically the result is a $3$ -dimensional thing, it is the subset of the direct product of $\Bbb A^2\times \Bbb A^2$ as \begin{align*}
\Bbb A^2\times_Z \Bbb A^2 &= \{(p, q)\in \Bbb A^2\times \Bbb A^2\mid \pi_1(p)=\pi_2(q)\}\\
&= \{(x, y, z, w)\in \Bbb A^2\times \Bbb A^2\mid y=z\}\\
&\cong\operatorname{Spec} k[x, y, z, w]/(y-z).
\end{align*} My question is, how do I use the above basic tools to calculate the tensor product and get the same result? Namely how do I show the following identity (if it is correct) step by step? \begin{align*}
k[x,y]\otimes_{\frac{k[x,y]}{(x-y)}} k[x, y] \cong \frac{k[x, y, z, w]}{(y-z)}.
\end{align*}","['algebraic-geometry', 'abstract-algebra', 'tensor-products', 'algebraic-topology']"
4929562,How to Handle a Singularity: Seeking Solutions for a Differential Equation System,"I am trying to solve the following differential equation system: $\begin{aligned} & r^2 K^{\prime \prime}=K(K-1)(K-2)+\frac{1}{4} h^2 K \\ & r^2 h^{\prime \prime}=\frac{1}{2} h K^2+\left(\lambda / g^2\right)\left(h^2-g^2 V_0^2 r^2\right) h\end{aligned}$ I have already tried Rk4 but it does not give me satisfying results (the problem is the singularity at r = 0). And before I try other methods, I wanted to ask if this type of differential equation is known or if it can at least be put into a known form so that I can think of a separate method. Normally the following procedure should let me solve the system numerically, or not? $\begin{aligned}\frac{du_1}{dr} &= u_2 \\
\frac{du_2}{dr} &= \frac{u_1(u_1-1)(u_1-2)}{r^2} + \frac{\frac{1}{4}u_3^2 u_1}{r^2} \\
\frac{du_3}{dr} &= u_4 \\
\frac{du_4}{dr} &= \frac{\frac{1}{2}u_1^2u_3 + \left(\frac{\lambda}{g^2}\right)(u_3^2 - g^2V_0^2r^2)u_3}{r^2}\end{aligned}$ with: $\begin{aligned} u_1 &= K  \\
u_2 &= K' = \frac{du_1}{dr} \\
u_3 &= h \\
u_4 &= h' = \frac{du_3}{dr}\end{aligned}$","['nonlinear-system', 'numerical-methods', 'ordinary-differential-equations']"
4929665,Reverse engineering a property for Geometric Brownian Motion,"During a lecture on financial mathematics we were given a side (non-homework) question to hone the SDE solving skills. Setting $\left(\Omega, \mathcal{F}, \mathbb{P},\left(\mathcal{F}_t\right)_{t \in[0, T]}\right)$ — filtered probability space. $\left(Z_t\right)_{t \in[0, T]}$ is a Brownian motion. We are given a geometric Brownian motion: $d X_t = \mu X_t \, dt + \sigma X_t \, dZ_t$ and $X_0 \in(0, \infty).$ Problem Let $f: \mathbb{R}_{+} \rightarrow \mathbb{R}$ be a bounded and smooth function.
There exists a function $F:[0, T] \times \mathbb{R}_{+} \rightarrow \mathbb{R}$ s.t. $$
\mathbb{E}_{\mathbb{P}}\left[f\left(X_T\right) \mid \mathcal{F}_t\right]=F\left(t, X_t\right). \tag{$\star$}
$$ Find this function. Attempt The property that's supposed to hold is akin to the martingale property—for that I'd be able to show that the geometric Brownian motion $X$ is a martingale with respect to the underlying Brownian motion $Z.$ But then $(\star)$ isn't obviously the exact martingale property but rather asks for something of an integrated and 2-variable relative of $f$ . I don't know how to wrap/reverse engineer it to satisfy the condition. I would appreciate any help.","['expected-value', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4929685,What is the frequentist's Bayesian prior for a coin with unknown bias,"A ""coin"" has a fixed unknown bias $0\le p\le1$ for heads, and out of $n\ge0$ tosses it yielded $0\le h\le n$ heads. Note that this occurs with probability $P(h\;|\;p,n)=\binom{n}{h}p^h(1-p)^{n-h}$ . We would like a ""best guess"" for $p$ . The frequentist view is that $p$ should be the maximum-likelihood-estimate $\frac hn$ . Indeed $\frac{d}{d\rho}\binom{n}{h}\rho^h(1-\rho)^{n-h}=0$ occurs at $\rho=\frac hn$ . The uniform Bayesian view is that $p$ should be $\frac{h+1}{n+2}$ . Indeed it has prior distribution $f(p)=1$ and the posterior distribution conditional on $(n,h)$ is then a Beta distribution $f(p\;|\;n,h)=\frac{P(h\;|\;p,n)f(p)}{\int_0^1P(h\;|\;\rho,n)f(\rho)d\rho}=\frac{(n+1)!}{h!(n-h)!}p^h(1-p)^{n-h}$ hence $\mathbb E[p\;|\;n,h]=\frac{(n+1)!}{h!(n-h)!}\int_0^1\rho^{h+1}(1-\rho)^{n-h}d\rho=\frac{(n+1)!}{h!(n-h)!}\frac{(h+1)!(n-h)!}{(n+2)!}=\frac{h+1}{n+2}$ . I don't yet have intuition for why these two viewpoints are the same if and only if $n=2h$ , let me know! But my main question is: What is the frequentist's prior , i.e. what distribution $f$ satisfies $\mathbb E_f[p\;|\;n,h]=\frac hn$ for all pairs $\lbrace(n,h)\in\mathbb Z^2\;|\; 0\le h\le n\rbrace$ ? Rephrased, $n\int_0^1\rho^{h+1}(1-\rho)^{n-h}f(\rho)d\rho=h\int_0^1\rho^h(1-\rho)^{n-h}f(\rho)d\rho$ . Taking $(n,h)=(1,0)$ forces $f$ to obey $\int_0^1\rho(1-\rho)f(\rho)d\rho=0$ and so under some natural assumptions on the non-negative $f$ this should mean $f$ is almost-everywhere zero and not a normalized sum of Dirac-deltas. I believe this is Qiaochu's answer below. This would be poetic and intuitive: a frequentist by construction would have no a priori guess, consistent with the fact that the vacuous 0 heads out of 0 tosses has undefined quotient $\frac00$ (whereas the uniform Bayesian invokes symmetry to guess $\frac12=\frac{0+1}{0+2}$ ).","['statistical-inference', 'probability-distributions', 'probability']"
4929708,Need intuition for dice betting problem,"I'm having a little trouble gaining some intuition for this dice betting question: A and B each take turns rolling a fair six-sided dice. A player wins if they roll a 4 which is followed by a 1 from the other player. Would you prefer to be A or B? My intuition said A because if I went first, I could roll a 4 and then B could roll a 1, and I would have the potential to win at the very beginning. If I'm B, I'm, in turn, subjecting myself to this possibility. But the answers say that it is right to be B. I'm not sure why this is the case.","['dice', 'probability-theory', 'probability']"
4929732,Proof of a Limit related to Gauss' Convergence test,"So this is the question:
if the series $\sum_{n=1}^{\infty} a_n$ is such that $$\frac{a_n}{a_{n+1}} = 1 + \frac pn  + \alpha_n$$ and the series $\sum_{n=1}^{\infty} \alpha_n$ converges absolutely, then $a_n\sim \frac{c}{n^p}$ and that $\sum_{n=1}^{\infty} a_n$ and it converges absolutely for $p>1$ and diverges for $p \leq 1$ Here is my failed attempt $$\text{let} \:c_n=\ln(\frac{b_n}{b_{n+1}})=\ln(\alpha_n+\frac{p}{n}+1)\le \alpha_n+\frac{p}{n} $$ $$\implies \sum_{i=1}^{n}c_i={\ln(b_1)}-{\ln(b_{n+1})}=\ln\prod_{i=1}^{n}(\alpha_i+\frac{p}{i}+1)$$ $$\le\sum_{i=1}^n\alpha_i+p\sum_{i=1}^n \frac 1i$$ Although I applied the inequality, it seems that it doesn't work.
And I have absolutely no idea how to continue this? Could you help me? Edit: I also noticed that a similar question has already been asked here Gauss' test for Convergence And nobody answered.","['real-analysis', 'calculus', 'absolute-convergence', 'limits', 'convergence-divergence']"
4929740,Calculate the volume of intersection sphere and cone using triple integral,"Compute the volume under sphere $x^2+y^2+z^2=4$ above $xy$ -plane and above cone $z=\sqrt{x^2+y^2}$ using triple integral. I try to plot as follows: I try using triple integral $$\int\limits_{-\sqrt{2}}^{\sqrt{2}} \int\limits_{-\sqrt{2-x^2}}^{\sqrt{2-x^2}} \int\limits_{\sqrt{x^2 +y^2}}^{\sqrt{4-x^2-y^2}} dzdydx$$ But too difficult to solve that integral. Now, I think to transform to cylindrical coordinates or sphere coordinates but I confused to determine lower bound and upper bound of triple integral. Anyone can explain the ways to solve this problem?","['integration', 'multivariable-calculus', 'volume']"
4929860,"The space $C_c$ of real-valued compactly supported, continuous functions is not a Banach space under any norm","This answer showed the space $c_{00}$ of compactly supported sequences, is not a Banach space under any norm. I wonder if the same is true for the space $C_c$ of real-valued compactly supported, continuous functions: $C_c$ is not a Banach space under any norm. From this question the space $C_c$ is a complete space with some topology which is not metrizable.","['complete-spaces', 'banach-spaces', 'normed-spaces', 'functional-analysis']"
4929864,Sufficient conditions for uniform convergence in probability,"I have a sequence of continuous random variables $\{X_n\}$ , with density $f_n(x \mid \theta)$ w.r.t. the Lebesgue measure. $X_n = o_{p_\theta}(1)$ for any $\theta \in \Theta$ . For a fixed $\theta_0$ in the interior of the parameter space $\Theta$ , let $B_\gamma(\theta_0)$ denote the closed ball of radius $\gamma$ centered at $\theta_0$ . I want to understand sufficient conditions for uniform convergence in probability in the following sense: there exists $\gamma>0$ , such that for any $\epsilon>0$ , $\delta>0$ , there exists $N$ large, for all $n>N$ , $$\sup_{\theta \in B_\gamma(\theta_0)} Pr(|X_n| > \epsilon \mid \theta) < \delta.$$ What I have tried/ thought about so far: I came across this paper which gives sufficient and necessary conditions for uniform convergence in probability. However, its setup is a bit different from mine. It discusses cases like: say $X_n(\theta)$ 's are random functions of $\theta$ , and then trying to see when $\sup_\theta X_n(\theta) = o_p(1)$ . I've thought about it, but I don't see how I can transform my setup into such situation. I also saw this post . The second equation listed there would be what I'm interested in. But the post didn't further discuss conditions for it to hold. An equivalent question here is, I want to find a sequence of positive numbers $\epsilon_n \to 0$ , and $$\sup_{\theta \in B_\gamma(\theta_0)}Pr(|X_n| > \epsilon_n \mid \theta) \to 0.$$ Another sufficient condition for this uniform convergence in probability is if the following holds: $$\sup_{\theta \in B_\gamma(\theta_0)} f_n(x \mid \theta) \le \sum_{j=1}^K M_j f_n(x \mid \theta_j)$$ for some finite $K$ , $M_j$ 's in $\mathbb{R}$ and fixed $\theta_1, \dots, \theta_K$ . I'm interested to see whether there are other sufficient (or even better, sufficient and necessary) conditions for this uniform convergence in probability to hold. I've tried but didn't seem to find many relevant references. Any thoughts, comments, references would be greatly appreciated!! p.s. Here are two more articles that I found relevant: Niemiro, W., & Zieliński, R. (2012). Remarks on uniform convergence of random variables and statistics. Demonstratio Mathematica, 45(2), 265-274. Philippou, A. N. (1978). A Note on Convergence in Probability Uniformly in a Parameter. Δελτίο της Ελληνικής Μαθηματικής Εταιρίας, 19(19B), 260-264.","['measure-theory', 'probability-theory', 'uniform-convergence', 'real-analysis']"
4929939,How can I fit a circle into the gap between an ellipse and the coordinate axes?,"Consider an ellipse with the equation $\frac{\left(x-ar\right)^{2}}{a^{2}}+\frac{\left(y-br\right)^{2}}{b^{2}}=r^{2}$ . How would I fit a circle with an equation $\left(x-k\right)^{2}+\left(y-k\right)^{2}=k^{2}$ in the small gap between the ellipse and the coordinate axes? When the ellipse is a circle, e.g. $a=5, b=5, r=25$ , the circle equation $\left(x-k\right)^{2}+\left(y-k\right)^{2}=k^{2}$ works with $k=\left(3-2\sqrt{2}\right)ar$ . But as soon as $a$ or $b$ or $r$ is changed, it breaks. How would I fit a circle in such space? Desmos link: https://www.desmos.com/calculator/pifjys6a1h (play around with the $a$ and see) Edit: Thanks to @Blue 's answer, I created this Desmos graph for future reference: https://www.desmos.com/calculator/dgrpsygsnn","['analytic-geometry', 'algebra-precalculus', 'conic-sections', 'geometry']"
4929995,Best quantitative version of open mapping theorem in complex analysis,"There is a ""quantitative"" version of the Open Mapping Theorem in complex analysis , saying: Let $f$ be holomorphic on the closure of a disc $V$ around $c$ , and assume $m := \min_{z\in \delta V} |f(z)-f(c)|$ is not $0$ . Then $f(V)$ contains the open disc $B_{m/2}(f(c))$ . This is e.g. proved in section 8.5 of Remmert's textbook , and the proof is essentially reproduced in the accepted answer to this question , where, possibly for pedagogical reasons, the weaker $m/3$ instead of $m/2$ is used. Remmert says (p. 258) this proof goes back to Carathéodory's Theory of Functions of a Complex Variable , pp. 139/140. I see that in this proof , the radius of the disc around $f(c)$ cannot be made bigger than $m/2$ . However, if I draw (quite possibly too simple) sketches of the geometric situation, it looks as if the image should actually always contain $B_m(f(c))$ . Question : Is there an example of a function $f$ , analytic on the closure of some disc $V$ around $c$ , with $m := \min_{z\in \delta V} |f(z)-f(c)| > 0$ but $f(V)$ does not contain $B_m(f(c))$ ? If so, is $m/2$ indeed the best upper bound for the radius of an open disc around $f(c)$ guaranteed to be in $f(V)$ ?","['complex-analysis', 'open-map']"
4930004,A peculiar problem on geometry relating to finding the angle between the diagonals of a cyclic quadrilateral,"A quadrilateral with side lengths $a$ , $b$ , $c$ and $d$ can be inscribed in a circle such that $a=\frac{1}{c}$ and $b=\frac{1}{d}$ . If $∆A$ represents the area of the quadrilateral. Prove that the angle between the diagonals of the quadrilateral is $\sin^{-1}(∆A).$ My Attempt: I tried to solve this problem by first understanding the given relationships between the sides of the quadrilateral. Given that $a = \frac{1}{c}$ and $b = \frac{1}{d},$ I wrote down these relationships to keep track of them. Next, I reminded myself that since the quadrilateral is inscribed in a circle, it is a cyclic quadrilateral. For such quadrilaterals, I know the sum of the opposite angles is $$180^\circ.$$ I also looked up Brahmagupta's formula for the area of a cyclic quadrilateral, which is: $$\Delta A = \sqrt{(s-a)(s-b)(s-c)(s-d)}$$ where (s) is the semiperimeter: $$s = \frac{a+b+c+d}{2}$$ Given the relationships between the sides, I wrote: $a=\frac{1}{c}$ and $b=\frac{1}{d}$ and let $c=y$ and $d=x.$ Then I substituted these into the formula for the semiperimeter: $$s = \frac{\frac{1}{x} + \frac{1}{y} + x + y}{2}$$ From here, I attempted to express the area $$\Delta A$$ using these relationships, but the expressions got pretty complicated. I wasn't quite sure how to simplify them. I then recalled that the area can also be expressed in terms of the diagonals and the sine of the angle between them. I tried to set up the equation for the area using the diagonals $d_1$ and $d_2$ : $$
\Delta A = \frac{1}{2} d_1 d_2 \sin(\theta)
$$ However, I got stuck trying to relate this to the given side lengths and couldn't figure out how to prove it. I suspect it involves more advanced properties of cyclic quadrilaterals or a specific theorem that I might not be aware of. I’m sure there's a clever way to connect all these pieces, but I couldn’t find it. Please anyone help me to complete the prove.","['trigonometry', 'geometry']"
4930025,"Mistake in Proof ""Every unique factorization domain is a principal ideal domain""","While doing my Algebra HW, I  ""proved"" that every unique factorization domain (UFD) is a principal ideal domain (PID). I know that this is not true, however I fail to see  where exactly is the mistake.  Could someone please pinpoint  whats wrong ? Firstly, to avoid confusion, as a UFD I denote such commutative ring $R$ that there is a subset $\mathbb{P} \subset R:$ All elements of $\mathbb{P}$ are prime,  each prime element $p \in R$ has an associated element $p' \in \mathbb{P}: (p)   = (p')$ and each element $x \in R\setminus ( \{0\} \cup R^*) $ can be written as $x = \varepsilon  \cdot p_1 ^{\nu_1 } \cdot ... \cdot p_m ^{\nu_m}$ , where $\varepsilon \in R^*$ invertable, $p_1, ...,p_m \in \mathbb{P}$ and $\nu_1, ..., \nu_m \in \mathbb{N} \cup \{  0\} $ . Moreover, such factorization is unique up to permutations. Firstly I would like to prove the following statement: Claim 1: $R$ is a UFD and $a, b \in R\setminus \{0\} : a \mid b $ , and $b$ is factorized as $b= \varepsilon _1 p_1^{\nu_1}\cdot ...\cdot p_m^{\nu_m} , p_1,.., p_m \in \mathbb{P}$ are primes and $\varepsilon \in R^*$ - an invertable element, then $a$ is factorized as $a = \varepsilon _2 p_1^{\nu'_1}\cdot ....\cdot p_m^{\nu_m' }$ , where $0 \leq \nu_k ' \leq \nu_k , \forall k \in\{1,..., m\} $ Proof: Let $a \mid b \Leftrightarrow \exists c\in R\setminus \{0\} : b = a\cdot c$ . W.l.o.g assume $c \notin R^* $ (otherwise $q$ is factorized as $q := q \cdot p_1^0 \cdot... \cdot p_m^0$ ). Then, if $a = \varepsilon_1 \cdot q_1 ^{l_1} \cdot ...\cdot q_ n^{l_m}, c = \varepsilon _2 h_1 ^{j_1}\cdot ...\cdot h_k^{j_k}$ , $b = a\cdot c = \varepsilon_1 \varepsilon _2 q_1 ^{l_1} \cdot ...\cdot q_ n^{l_m}h_1 ^{j_1}\cdot ...\cdot h_k^{j_k}$ would be a factorization of $b$ . But factorizations are unique up to permutations, so $q_1 ,.., q_n, h_1,.., h_k \in \{p_1, .., p_n\}$ so (w.l.o.g) $n=m , q_i = p_i, i = 1,.., n$ and $k = m , h_1 = p_1 ,..., h_m = p_m$ . Morever, $p_i^{l_i + j_i } = p_i^{\nu_i}, l_i, j_i \geq 0 \Rightarrow l_i \leq  \nu _i $ $\blacksquare$ Now, the ""proof"" itself: Claim 2: Each UFD is PID Proof: Assume for contradiction, i.e there is $I \subset R, I \neq \emptyset: \forall x \in R: I \neq (x) $ .  Consider $\mathcal{P} := \{ P \subset I : P \text{ is a principal ideal}\}$ . The partial order on $\mathcal{P}$ is given by $""\subset""$ relation. I will show that $\mathcal{P}$ has a maximal element - that is, a principal ideal $P\subset I: \forall P' \subset I$ principal ideals : $P'\subset P$ . For that I wold like to use Zorn's lemma - then, all I need to show is that every chain $\{ P_{\alpha }\} _{\alpha \in A } \subset \mathcal{P}$ is bounded from above: i.e there is an element $P \in \mathcal{P}: \forall \alpha \in A: P_{\alpha} \subset P$ . Firstly, $\{ P_{\alpha}\}_{\alpha \in A}$ is a chain means that $A$ is a totaly ordered set ( $\forall \alpha_1, \alpha _2\in A: \alpha_1 < \alpha_2 \lor \alpha_1 =\alpha_2 \lor \alpha_2 > \alpha_1$ ) and $\alpha_1 \leq \alpha_2 \Rightarrow P_{\alpha_1}\subset P_{\alpha_2}$ . Since $\{P_\alpha\}_{\alpha \in A}$ are principal ideals, we can associate this chian with another chain $\{ d_\alpha \}_{\alpha \in A}: \forall \alpha \in A: P_{\alpha} = (d_{\alpha})$ . Since $(d_{\alpha_1}) \subset (d_{\alpha_2}) \Leftrightarrow d_{\alpha_2} \mid d_{\alpha_1}$ , the relations translates to $\alpha_1 \leq \alpha_2 \Rightarrow d_{\alpha_2} \mid d_{\alpha_1} $ . Now I would like to show that $P := \bigcup\limits_{\alpha \in A }P_{\alpha}$ is an upper boundary of the chain 1. $P$ is an ideal : Let $x \in P$ . Then, $\exists \alpha \in A: x \in P_{\alpha}$ . Then, $\forall r \in R: rx \in P_{\alpha}$ since $P_{\alpha }$ is an ideal $\Rightarrow rx \in P$ . Let $x_1 , x_2 \in P$ . Then $\exists \alpha_1, \alpha_2 \in A : x_1 \in P_{\alpha_1 }, x_2 \in P_{\alpha_2 }$ . W.l.o.g $\alpha_1 \leq \alpha_2 \Rightarrow P_{\alpha_1} \subset P_{\alpha_2} \Rightarrow x_1, x_2 \in P_{\alpha_2} \Rightarrow x_1 + x_2 \in P_{\alpha_2} \Rightarrow x_1 +x_2 \in P$ . Also its clear that $P \subset I$ 2. $P$ is a principal ideal : If $|A|< \mathbb{N}$ the statement is obvious. Otherwise consider $\alpha \in A $ . Then, there are two options: Either $P_\alpha = P$ (and $\forall \alpha' \in A : \alpha ' \leq a $ ), and we are done (since $P_{\alpha}$ is a principal ideal), or there is $\alpha ' \in A : \alpha ' > \alpha$ and $P_{\alpha } \subset P_{\alpha'}$ is a proper subset. However, per note above, $(d_\alpha) = P_{\alpha} \subset P_{\alpha '} = (d_{\alpha'})$ implies $d_{\alpha'} \mid d_{\alpha}$ . We can consider $d_{\alpha }, d_{\alpha'} \neq 0$ since if $\{ P_{\alpha}\} = \{ \{0\} \}$ , it is trivialy bounded from above. So, per claim 1, if $p_{\alpha} = \varepsilon p_1^{\nu_1} \cdot ... \cdot p_n^{\nu_n}$ is a factorization of $p_{\alpha}$ , then $p_{\alpha'} = \varepsilon' p_1^{\nu_1'} \cdot ... \cdot p_n^{\nu_n'}$ , $0 \leq \nu_1 ' \leq \nu_1 ,...., 0 \leq \nu_n' \leq \nu _n$ is a factorization of $p_{\alpha'}$ . Moreover, $(d_\alpha) $ is a proper subset $(d_{\alpha'})$ , i.e $p_{\alpha'}$ and $p_{\alpha}$ are not associated, so there must be at least one $k \in \{1 ,..., n\} : \nu_k' < \nu_k$ . Now repeat the considerations for $P_{\alpha'}$ : Either $P_{\alpha'} = P$ and the statement is true, or there is a proper overset $P_{\alpha''} \supset P_{\alpha'}$ . For $d_{\alpha''}$ the factorization is given by $p_{\alpha'} = \varepsilon' p_1^{\nu_1''} \cdot ... \cdot p_n^{\nu_n''}$ , where $\nu_k '' \leq \nu _k', \forall k \in\{ 1,..., n\}$ and for at least 1 $k \in \{1,...,n\}: \nu_k '' < \nu_k' $ . Then consider $P_{\alpha''}$ and so on. However this process cant go on forever - indeed, if it continues for at most $\nu_1 + \nu _2 + ... + \nu_n :=N$ steps we would get an ideal $P_{\alpha^{(N)}} = (d_{\alpha^{(N)}})$ , where $d_{\alpha^{(N)}}$ can be factorized as $d_{\alpha^{(N)}}  = \varepsilon^{(N)}p_1^0 \cdot ... \cdot p_n^0 \in R^* \Rightarrow (d_{\alpha^{(N)}}) = (1) = R$ , however we assumed that $P_{\alpha^{(N)}} \subset I$ where $I$ is not a principal ideal, so a contradiction. This means that the process breaks down after finitely many steps, and we would get that $P = P_{\alpha^{(K)}} = (d_{\alpha^{(K)}}) $ , so its a principal ideal Now we have shown that $P \subset I$ and $P$ is a principal ideal $\Rightarrow P \in \mathcal{P}$ and $\forall \alpha \in A : P_{\alpha } \subset P$ , so its an upper boundary. Lemma of Zorn now yields that there is such $P \in \mathcal{P}: \forall P' \in \mathcal{P}: P' \subset P$ . But this gives us a contradiction: On one hand, $P \subset I$ is a proper subset, since we assumed that $I$ is not a principal ideal. On the other hand $\forall x \in I: x \in (x) \subset I$ , where $(x)$ is a principal ideal, so $(x) \subset P \Rightarrow x \in P \Rightarrow I \subset P$ . $\blacksquare$ I fail to see whats exactly worng here - I guess that would be either the way I use Zorn's lemma (which is still a new concept to me), or the part where is show that $P$ is a principal ideal. Would be very grateful if someone could say which part is wrong.","['principal-ideal-domains', 'unique-factorization-domains', 'fake-proofs', 'abstract-algebra', 'ideals']"
4930038,Is this provable? $\lim_{x\to 0} \frac{\sin (\pi \cos^2 x)}{x^2}=\pi $ [duplicate],"This question already has an answer here : What is $\lim\limits_{x \to 0} \frac{\sin(\pi \cos^2x)}{x^2}= \ ?$ [closed] (1 answer) Closed 20 days ago . I came across this question $$\lim_{x\to 0} \frac{\sin (\pi \cos^2 x)}{x^2}=\pi $$ I tried following method simplify it into a $x\to0$ , $\sin x / x$ type limit $$\lim_{x\to 0}\frac{\sin(\pi \cos^2 x)}{\pi \cos^2 x}\cdot\frac{\pi \cos^2 x}{x^2} $$ But, still no luck. Any suggestions or tips to solve this. (the exercise book told you shouldn't use L'hopitals)","['limits', 'algebra-precalculus', 'limits-without-lhopital', 'trigonometry']"
4930047,How to conclude the non-existence of limit,"Let $f(x, y) = \dfrac{x^n + y^n}{x^2 - y^2}$ , $n > 2$ , be defined in $U = \left\{(x, y) \in \mathbb{R}^2 : y \neq x, y \neq -x\right\}$ . My problem is to determine the existence of $\lim\limits_{\substack{(x, y) \rightarrow (0, 0) \\ (x, y) \in U}} f(x, y)$ . The solution offered by a professor is the following: Let $V = \left\{(x, x + x^p) : x > 0\right\}$ , $p > n - 1$ . If $(x, y) \in V$ then $$f(x, y) = f\left(x, x + x^p\right) = \dfrac{x^n + \left(x + x^p\right)^n}{x^2 - \left(x + x^p\right)^2} = -\dfrac{x^n + x^n + nx^{n - 1 + p} + \cdots + x^{np}}{2x^{p + 1} + x^{2p}}.$$ As $p > n - 1$ , if $x \rightarrow 0$ then $f\left(x, x + x^p\right) \rightarrow \infty$ . Thus, $f$ is not convergent when $(x, y) \rightarrow (0, 0)$ in $U$ . My question is how could I come up with this idea. Firstly, I'd have to think about the non-existence instead of the existence, and still I wouldn't know how to get to the set $V$ in order to prove it. Moreover, is there another way to do it?","['real-analysis', 'calculus', 'functions', 'limits', 'convergence-divergence']"
4930115,$| |x + y|^p - |x|^p | \leq \epsilon |x|^p + C |y|^p$,"I want to demonstrate that: Let $1 < p < \infty$ ; for any $\epsilon > 0$ , there exists $C = C(\epsilon) \geq 1$ such that for all $x, y \in \mathbb{R}$ , we have $$ | |x + y|^p - |x|^p | \leq \epsilon |x|^p + C |y|^p.$$ I encountered this question while reading an article by Brézis, where, under certain conditions, $$\lim_n \, ( ||f_n||_p^p - ||f_n - f||_p^p ) = ||f||_p^p.$$ However, the inequality above was simply stated, and I'm struggling to understand why it holds. My thoughts are that it might be derived from some inequalities involving the derivatives of monotone functions. Thank you for any insights.","['real-numbers', 'real-analysis', 'calculus', 'lp-spaces', 'derivatives']"
4930136,Calculating the fibres of a scheme morphism are proper but the morphism is not proper,"$\newcommand{\Spec}{\operatorname{Spec}}\newcommand{\C}{\mathbb C}\newcommand{\A}{\mathbb A}$ Let $f:\mathbb A^1_\mathbb C\rightarrow \mathbb A^1_\mathbb C$ be induced by the ring homomorphism $t\mapsto t^2$ , and consider $f|_{U}:U\rightarrow \mathbb A^1_\mathbb C$ , where $U$ is $\mathbb A^1_\mathbb C\smallsetminus \{\langle t-1\rangle\}$ . I am trying to calculate the fibres of this morphism over closed points, and show that these are proper but that $f|_U:U\rightarrow \A^1_\C$ is not proper. Via some tensor product calculations, and by knowing that $U$ is the distinguished open corresponding to $t-1$ , I can calculate the fibres to be: $$f|_U^{-1}(z)=\begin{cases}
V(t^2-z)\cong \Spec \C\times \C&\text{if }z\neq 1,0\\
V(t^2)\cong \Spec \C[t]/\langle t^2\rangle&\text{if }z=0\\
V(t+1)\cong \Spec \C&\text{if }z=1
\end{cases}$$ which follow because $\C[t]\otimes_{\C[t]}\otimes \C\cong \C[t]/\langle t^2-z\rangle$ , so the fibre of $f$ over $z$ is just $V(t^2-z)$ , and so the fibre of $f|_U$ is just $U_{t-1}\cap V(t^2-z)$ . For $z\neq1$ we have that this completely contained in $U_{t-1}$ , so the fibre are the same as they would be in the non restricted case, that is just $V(t^2-z)$ . With $z=1$ , we have that $V(t^2-1)$ is not completely contained in $U_{t-1}$ , so $V(t^2-1)=V'(t^2-1)$ , where $V'(t^2-1)$ is the vanishing locus of the ideal $\langle t^2-1\rangle\subset \C[t]_{t-1}$ . Since $t-1$ is invertible, this is the vanishing locus of $V(t+1)$ in $\Spec C[t]_{t-1}$ , and the so this is isomorphic to $\Spec \C$ . I guess these are also proper because if we write $g_z$ as $f|_U$ restricted to the fibre over $z$ then these are all closed embeddings, and closed embeddings are proper. What I am confused about is how to prove that $f|_U:U\rightarrow \A^1$ is not proper. The morphism is not closed becasue every closed subset of $\A^1$ , and thus of $U$ is a finite set of closed points, and $f|_U$ takes closed points to closed points. We thus have to go to the base change, but I am not sure what I should base change by because the simple base changes don't really change anything or leave me with closed embeddings as above. Any help would be appreciated.","['affine-schemes', 'algebraic-geometry', 'schemes', 'fibre-product']"
4930154,What explains this $n \times n$ determinant pattern?,"$\mathbf{SETUP}$ I've been studying a physics model that involves inverting an $n \times n$ matrix of this form: $$
    \mathbf{A}^{-1}_n(x)=
    \begin{pmatrix}
        1 & -x & -x & -x & ... \\
        -x & 1 & -x & -x & ... \\
        -x & -x & 1 & -x & ... \\
        -x & -x & -x & 1 & ... \\
        ... & ... & ... & ... & ...
    \end{pmatrix}
^{-1}
$$ which made me interested in when it is not invertible, i.e. $\det(\mathbf{A}_n(x)) = 0$ . I managed to derive a nice reason for why: $$
\det(\mathbf{A}_n(x=\tfrac{1}{n-1})) = 0
$$ which is because the determinant is equivalent to the polynomial: $$
\det(\mathbf{A}_{n+1}) = (x+1)^n (1-nx)
$$ (Try it! It works :) - Incidentally, would this be a common result in some textbook?) $\mathbf{QUESTION}$ But then, I playfully decided to just make a single (non-diagonal) matrix entry equal to zero, keeping all the others still at $\tfrac{1}{n-1}$ , and stumbled upon this other pattern: $$
    \det
    \begin{pmatrix}
        1 & 0 & \tfrac{1}{1-n} & \tfrac{1}{1-n} & ... \\
        \tfrac{1}{1-n} & 1 & \tfrac{1}{1-n} & \tfrac{1}{1-n} & ... \\
        \tfrac{1}{1-n} & \tfrac{1}{1-n} & 1 & \tfrac{1}{1-n} & ... \\
        \tfrac{1}{1-n} & \tfrac{1}{1-n} & \tfrac{1}{1-n} & 1 & ... \\
        ... & ... & ... & ... & ...
    \end{pmatrix}
    =
    \frac{n^{n-2}}{(n-1)^n}
$$ (The above works regardless of which non-diagonal entry you choose to make zero, of course.) Can anyone help me understand why? Might this be related to MacMahon's Master Theorem? Or is there some more obvious way to think about this result?","['determinant', 'linear-algebra', 'combinatorial-proofs', 'combinatorics']"
4930212,Let a real measurable function $f$ map every open set to the whole real line. Is there always a restriction to a set of measure zero doing the same?,"Given $f:\mathbb{R}\rightarrow\mathbb{R}$ mapping each non-empty open set to the whole real line, is there always a set $A$ of measure zero such that the function $g:\mathbb{R}\rightarrow\mathbb{R}$ defined as $g(x)=f(x)$ on $A$ and $0$ everywhere else also maps every non-empty open set to the whole real line? Edit: My idea is to use the sigmoid function to convert it into the equivalent problem about functions with domain $(0,1)$ and work in binary. Consider the set $B \subseteq (0,1)$ defined as the union over all $n$ of values in $(0,1)$ whose $n$ th to $n+k$ th binary places are 0, for some $k$ depending on $n$ . This strategy lets us restrict to a set of arbirtrarily small positive measure. Can this idea be extended by incorporating the actual $f$ to get a let us restrict to a set of measure $0$ ?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4930277,Zeros of a reciprocal polynomial,"Let $k$ be a positive even integer and consider the polynomial $$
f_k(z)=z^k(z+1)^k+z^k+(z+1)^k.
$$ Numerical computation suggests that the zeros of this polynomial behave as follows: If $\mbox{Re}(z)\gt -1/2$ , then the zeros lie on the unit circle. If $\mbox{Re}(z)\lt -1/2$ , then all the zeros lie on the unit circle centered at $-1$ . Otherwise, the rest of the zeros lie on $\mbox{Re}(z)= -1/2$ . Here is a picture of the zeros for $k=120$ : I would like to prove formally that this is true, but I don't know where to start. I tried for instance assuming that $\mbox{Re}(z)\gt -1/2$ and then trying to show that $|z^k|=1$ by expressing $z^k$ in terms of a polynomial in $(z+1)^k$ , but this didn't work. I also noticed that $f_k(z)$ is a reciprocal polynomial, so I tried looking at results on this direction, but all the results say something like: if some condition holds then all the zeros lie in the unit circle, but of course in this case not all of them lie in the unit circle, and I don't know how to adapt those results in this case. I would also like to know the separation of the zeros, I'm thinking that this is $O(1/k)$ .","['number-theory', 'roots', 'polynomials']"
4930280,"Proving $\Bbb{E}[\frac{\mu-X}{(\mu+X)^\lambda}]>0$, for $X$ a positive random variable with compact support, $\mu=\Bbb{E}[X]$, and $\lambda\geq 1$","Let $X$ be a positive random variable with compact support and let $\mu=\mathbb{E}\left[X\right]$ .
Consider the following inequality involving the parameter $\lambda\geq 1$ : $$\mathbb{E}\left[\frac{\mu-X}{(\mu+X)^\lambda}\right]>0$$ Is there a straightforward way to prove this inequality?","['expected-value', 'measure-theory', 'probability', 'inequality']"
4930294,Corollary to the Three Series Theorem,"Problem Suppose $X, X_1, X_2, \ldots$ are iid and in $L^p$ for some $1 < p < 2$ . Show that the series of $Y_n=\frac{X_n − \mathbb{E}(X)}{n^{1/p}}$ converges. Partial solution Here, we do not have variance, which rules out the use of Khintchine-Kolmogorov Convergence Theorem . We then move on to the necessary and sufficient Kolmogorov 3 Series Theorem . I cannot really understand how to verify these, though, as for example $$
\sum \mathbb{P}(|Y_k|>1)= \sum \mathbb{P}(|X_k - \mathbb{E}[X]|> n^{1/p}) \leq \sum \mathbb{E}[(X-\mathbb{E}[X])] \frac{1}{n}
$$ where I have used Chebychev with $g(x) = x^p$ (there is not much else that comes to my mind), and where the right hand side obviously diverges. What kind of upper bound could I use here (and in the other 2 series) to ensure convergence?","['probability-theory', 'probability', 'random-variables']"
4930387,Finding the maximum of an integral,"I am wondering if it is possible to find the maximum ( $x_{\text{max}}$ , $v_{\text{max}}$ ), also an approximate one, of this integral as function of $x$ : $$v(x)=\int_0^{x+x_0} \frac{\exp \left[-\frac{(x-\tau )^2}{2}\right]}{\sqrt{\tau }} \, d\tau$$ Integrate[
 1/Sqrt[\[Tau]] Exp[-((x - \[Tau])^2/2)], {\[Tau], 0, x + x0}] where $x_0>0$ and $x > -x_0$ . For example, for $x_0=4$ , we have varying $x$ : ListPlot[Table[{x, 
   NIntegrate[
    1/Sqrt[\[Tau]] Exp[-((x - \[Tau])^2/2)], {\[Tau], 0, x + x0} /. 
     x0 -> 4]}, {x, -4, 15, 0.1}], Frame -> True, 
 FrameLabel -> {""x"", ""v(x)""}] From one side, I tried to find where $dv(x)/dx=0$ , using the Leibniz integral rule, for which $$\frac{dv}{dx}=\frac{e^{-\frac{x_0^2}{2}}}{\sqrt{x+x_0}}+\int_0^{x+x_0} \frac{e^{-\frac{1}{2} (x-\tau )^2} (\tau -x)}{\sqrt{\tau }} \, d\tau$$ 1/(E^(x0^2/2)*Sqrt[x + x0]) + 
 Integrate[(\[Tau] - x)/(E^((1/2)*(x - \[Tau])^2)*
     Sqrt[\[Tau]]), {\[Tau], 0, x + x0}] but is seems that things get more complicated. On the other side, having checked that the maximum is near $t=0$ , I tried to sobstitute to the integrand in $v(x)$ its Taylor expansion around $t=0$ . In this case, the integral can be performed but the result is again much complicated depending on the number of terms kept from Taylor expansion. If $x_0\rightarrow \infty$ , the integral from $0$ to $\infty$ of the Taylor expansion of $$ \frac{\exp \left[-\frac{(x-\tau )^2}{2}\right]}{\sqrt{\tau }}$$ around $x=0$ truncated to the third order term is $$v(x)\approx -\frac{1}{6} \left(x^2-4\right) \left(2^{3/4} x \, \Gamma \left(\frac{7}{4}\right)+3 \sqrt[4]{2} \, \Gamma \left(\frac{5}{4}\right)\right)$$ The last function has a maximum of $v_\text{maxapprox}=2.52714$ at $x_\text{maxapprx}=0.651579$ with $$x_\text{maxapprox}=\frac{\frac{\sqrt{\sqrt{2} \Gamma \left(\frac{5}{4}\right)^2+\frac{8}{3} \sqrt{2} \Gamma \left(\frac{7}{4}\right)^2}}{2^{3/4}}-\frac{\Gamma \left(\frac{5}{4}\right)}{\sqrt{2}}}{\Gamma \left(\frac{7}{4}\right)}$$ (Numerically, for $x_0 \rightarrow \infty$ , the result should be $x_\text{max}=0.7649$ and $v_\text{max}=2.5596$ .) Any help or suggestion, also about the $x_0 \rightarrow \infty$ case, is really welcome.","['integration', 'optimization', 'functions']"
4930396,Mismatching Euler characteristic of the Torus,"Why is it that when I try to compute the Euler characteristic for the Torus using a drawing like the following , then the number that I get is not the number that the Torus should have? Which is $0$ ? I mean, if I understand correctly, given any polygon and counting $V-E+F$ then I get the Euler characteristic for that specific volume. But in my case, $V-E+F=16-24+10=2$ . Obviously, this torus is not the same as the sphere which has the same apparent Euler characteristic. I know that there exists a way for creating a torus from a simple sheet of paper by identifying the corners of such, and in this case the Euler characteristic comes up being $0$ as it should. But I just wanted to know what was the flaw in my argument.","['polyhedra', 'geometric-invariant', 'geometry', 'polygons', 'general-topology']"
4930409,About countable family of sets in R satisfying a hitting condition,"I was working on a problem when I hit a little snag about a geometric problem. To properly describe it, let me introduce some notions. We say that a point $p$ hits a set $X$ if $p\in X$ . We say that a point $p$ hits a family of sets $\mathcal F$ if $p$ hits $B$ for all $B\in \mathcal F$ . We say that a family $\mathcal F$ is finitely hittable if there is a finite set of points $P$ such that for any set $B\in\mathcal F$ , there is a point $p\in P$ such that $p$ hits $B$ . Now call a countable family $\mathcal F$ of sets in $\mathbb R$ good if it satisfies the following property : $\mathcal F$ is not finitely hittable implies there exists a countable subfamily of $\mathcal F$ made of disjoint sets. I want to look at when a family is good. So far I have found the following. If the sets in $\mathcal F$ are compact and connected, then $\mathcal F$ is good. If the sets in $\mathcal F$ are not necessarily compact, then there is a family that is not good. If the sets in $\mathcal F$ are compact but can have arbitrarily many number of components (even if restricted to finite but unbounded number of components), then there is a family that is not good. What I am stuck at is proving (or finding a counterexample of) the goodness of $\mathcal F$ when all the sets in the family are compact and the number of connected components is bounded (even for at most 2 connected components). I have tried a couple of proof ideas but none really worked. One of them that I thought would work but did not was via induction on the bound on the number of connected components. The base case of 1 components is handled by the result of bullet point 1. Assuming it holds for at most $k$ connected components, I am looking not at families with at most $k+1$ connected components. Roughly, my idea was to look at one component from each set, use the result from the base case, get a disjoint set of components, consider the subfamily of sets from which those components belong to, throwing away those components and if they are not disjoint then induct. But the condition of not being finitely hittable may not hold for this family. So I am stuck (If it's finitely hittable, then one can look at the whole family minus this subfamily which, but then that doesn't decrease the number of components so induction doesn't help right now. If I can show this process terminates in a non-finitely hittable subfamily after finitely many iterations, then we are done, but I can't even do that). I have tried my hand at counterexamples too but nothing really came of it. My gut feeling says it's false, but no dice really. Any help is appreciated.","['general-topology', 'combinatorial-geometry', 'geometry', 'real-analysis']"
4930480,Cantor-Bendixson theorem and AC,"For context, the Cantor-Bendixson theorem states that a closed subset $A$ of a Polish space can be written as the union of a perfect subset and a countable set $A=P\cup C$ . Now, I know two proofs of this theorem: one of them uses condensation points, and the other uses Cantor-Bendixson rank; but in the both cases the proof has a step that says "" $C$ is a countable union of countable sets, and therefore countable..."", which famously relies on AC $_\omega(^\omega\omega)$ . But is this really necessary? If this is the case, one reasonable attempt is to construct a Polish space from a countable family of countable sets, but I don't see immediately how that can be carried out.","['axiom-of-choice', 'general-topology', 'analysis']"
4930489,Is a small or a large population more likely to be dominated by its outliers?,"Prior to 1947, American baseball was racially segregated, with the so-called Major Leagues open only to white players. Black players played in smaller leagues called the ""Negro Leagues"".  Teams in each league played games only against other teams in the same league. Until this year , it was the policy of Major League Baseball that statistics from Negro League play were not incorporated into the official statistics.  For example, a Negro League player who had accumulated a large number of home runs would not have been included on the list of players with the most home runs, regardless of how their total compared with those of players in the Major Leagues.  And if a player had played in both Negro and Major leagues, only their Major League performance was counted in the official statistics. One rationale given for this policy decision was that the Negro Leagues were much smaller than the Major Leagues. The argument was that hitting a large number of home runs in the Negro Leagues might not be as difficult as doing the same in the National League, because the pitchers in the National League would overall have been better than those in the Negro Leagues. My question is whether this is mathematically plausible. The Major League populations of pitchers were certainly larger than those of Negro Leagues, and so might include most of the really excellent pitchers.  But by the same argument a larger league would also have included more sub-par pitchers.  In fact, the great majority of professional baseball players are below-average performers, because talent (however measured) is heavily right-skewed.  I can't guess which effect will dominate the other. (Perhaps neither.) In this question I am only interested in the mathematics , and not in any sort of historical or policy issues. For concreteness: Let's suppose that baseball talent in any particular area (home run hitting or what have you) is normally distributed in the general population but professional baseball players are drawn from the rightmost tail of this distribution, say the people who are $4.5$ or more standard deviations above the mean.       (This is why talent distributions are right-skewed.) Suppose that the population of baseball players has been arbitrarily (uniformly randomly) divided into a small group with fraction $f$ of all players, and a large group with $1-f$ fraction of the players.  (Historically, I think around $f\approx \frac18$ of players played in the Negro Leagues.) My question is : Are the outliers in the small group more extreme, relative to the rest of their group, than the outliers in the large group are relative to the rest of the large group? I imagine that probability theory has a standard definition of an ""outlier"" and a way to measure how extreme a particular outlier is.  Is this correct? Perhaps the answer is as simple as comparing the expected standard deviations of the two groups?  I don't know.",['statistics']
4930604,Missing solutions from $y'(x)^2 = 2 y(x) y''(x)$,"I am looking into this ODE: $y'(x)^2 = 2 y(x) y''(x)$ , and I find some solutions through the following steps: $$ \frac{y'(x)}{y(x)} = 2 \frac{y''(x)}{y'(x)}$$ This suggests that by integration I should have $$ \ln( y(x)) = 2 \ln( y'(x) ) + C_0 $$ $$ y(x) = \lambda y'(x)^2 $$ with $\lambda = e^{C_0}$ Again, this suggests 2 possible pairs of signs: $$ \pm \lambda^{-\frac{1}{2}} = \frac{y'(x)}{ \pm \sqrt{y(x)}} $$ By integrating, I get 2 possible solutions: $$ \pm \lambda^{-\frac{1}{2}} x + C_1 = \ln{ \left| \sqrt{y(x)} \right| }$$ $$ y(x) = e^{\pm 2 \lambda^{-\frac{1}{2}} x + C_1}$$ where $C_1$ is allowed to be complex. These are certainly solutions, but apparently this procedure does not capture all possible solutions. In fact, if $y(x) = x^n$ , then we have $$ n \frac{x^{n-1}}{x^{n}} = 2 \frac{ (n - 1) x^{n-2}}{x^{n-1}} $$ which is a solution if $n = 2n - 2$ , which has a solution for $n=2$ So my question is here: why are these polynomial solutions missed from my 1st attempt at solving the ODE? If the procedure misses polynomial solutions, what other solutions might be also amiss?",['ordinary-differential-equations']
4930622,Calculus: Integral of a function [duplicate],"This question already has an answer here : Integral $\int^{\infty}_{- \infty} e^{z(a+t)} \frac{1}{(e^z + 1)^{a+b}} dz$ (1 answer) Closed 18 days ago . For all $-1<k<1$ , I'm trying to solve the following integral: $$\int_{-\infty}^\infty z\frac{e^{z(k+1)}}{(1+e^z)^2}dz.$$ My firt attempt was to use change of variables: $t=e^z\implies z=\ln(t)\implies dz/dt=1/t$ . Hence, \begin{equation}
\int_{-\infty}^\infty z\frac{e^{z(k+1)}}{(1+e^z)^2}dz=\int_{0}^\infty \ln(t)\frac{t^{k}}{(1+t)^2}dt. \quad (1)
\end{equation} But the expression on the RHS of $(1)$ is not easy to solve. WolframAlpha asserts that \begin{equation}
\int_{0}^\infty \ln(t)\frac{t^{k}}{(1+t)^2}dt=\pi(1-\pi k \cot(\pi k))\csc(\pi k), \quad (2)
\end{equation} which I'm unable to show. Can someone help me with the steps to show that $(2)$ is valid?","['integration', 'indefinite-integrals']"
4930690,"Finding MLE given dependent observations from uniform distribution $U(0,\theta)$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 18 days ago . Improve this question Suppose we are given random variables $X_1,...,X_n$ that are uniformly distributed on the interval $[0,\theta]$ , with $\theta >0$ unknown.  I know that if the $X_1,...,X_n$ are furthermore independent, we can conclude that the Maximum Likelihood Estimation is given as $\hat{\theta} = \max\limits_i x_i$ . I was wondering if the same conclusion also holds if the random variables $X_1,...,X_n$ are not independent. My main problem is that I am unable to determine the Likelihood function, which I do not know if it is possible if the $X_i$ are not independent. I would appreciate any help.","['statistics', 'parameter-estimation', 'maximum-likelihood', 'probability-theory', 'probability']"
4930697,I need help in understanding the alternative solution provided to solve this geometry question of calculating area of quadrilateral,"Question: Solution provided: I understand this part that equal chords of a circle subtend equal angles at the center, but after this the faculty transformed this whole diagram to one shown below in the second picture. I am not able to understand how did the two sides get interchanged here ?","['area', 'quadrilateral', 'circles', 'geometry', 'solution-verification']"
4930743,Can you define a group with only one equality for an inverse? [duplicate],"This question already has answers here : Right identity and Right inverse in a semigroup imply it is a group (7 answers) Closed 19 days ago . One of the group axioms is the existence of an inverse, which is usually stated as: For each $a\in G$ there exists an element $b$ in $G$ such that $ab=ba=e$ where $e$ is the identity element of the group. When we only deal with matrix multiplication, we know that it suffices to show $ab=e$ in order to deduce that both matrices are invertible and that $b$ is the inverse of $a$ . My question is: Is it possible to write the group axiom of inverses with only one equality? I.e., to say that for each $a\in G$ there exists an element $b$ in $G$ such that $ab=e$ where $e$ is the identity element of the group. Will this equality always imply that also $ba=e$ as in matrices? Alternatively, is it possible to construct a set $G$ with an operation $\star$ that will satisfy associativity, existence of identity and this new definition of existence of inverses but will fulfill the original statement $ab=ba=e$ (only $ab=e$ )? Thank you",['group-theory']
4930764,Existence and Uniqueness of Equilibrium points for Concave N-Person Games,"I am reading a paper . I have a problems with understanding their lemma Lemma: The nonzero elements of every vector $u \in U(x)$ are given by a vector $\bar{u} \in E^k, \bar{k} \leqslant k$ , where $\bar{u}=-\left(\bar{H}^{\prime} \bar{H}\right)^{-1} \bar{H}^{\prime} g(x, \bar{r}) \geqslant 0$ .
The $m \times \bar{k}$ matrix $\bar{H}=\bar{H}(x)$ consists of $\bar{k}$ linearly independent columns of $H(x)$ selected from $\nabla h_j(x)$ for $j \in J$ . Let me recall some useful notations We define an $m \times k$ matrix $H(x)$ whose $jth$ column is $\nabla h_j(x)$ $$H(x)=\left[\begin{array}{lll}
\nabla h_1(x) & \nabla h_2(x) \ldots \nabla h_k(x)
\end{array}\right]$$ $g(x,r)$ is a mapping of $E^m$ into itself $$g(x, r)=\left[\begin{array}{c}
r_1 \nabla_1 \varphi_1(x) \\
r_2 \nabla_2 \varphi_2(x) \\
\vdots \\
r_n \nabla_n \varphi_n(x)
\end{array}\right]$$ We define the mapping $f(x,u,r)$ of $E^{m+k} \to E^m$ for each fixed $\bar{r}>0$ as follows $$f(x, u, \bar{r})=g(x, \bar{r})+H(x) u$$ where $u \in U(x) \subset E^k$ such that $$U(x)=\left\{u \mid\|f(x, u, \bar{r})\|=\min _{\substack{v_j \geqslant 0, j \in J \\ v_j=0, j \neq J}}\|f(x, v, \bar{r})\|\right\}$$ $$J=J(x)=\left\{j \mid h_j(x) \leqslant 0\right\}$$ Thank you very much for your help. I appreciate it a lot","['karush-kuhn-tucker', 'convex-optimization', 'analysis', 'optimization', 'convex-analysis']"
4930813,Uniqueness of module structure,"Question : Let $E$ be a torsion abelian group. Prove that $E$ has exactly one $\hat{\mathbb{Z}}$ -module structure, and that the scalar multiplication $\hat{\mathbb{Z}} \times E \rightarrow E$ defining this module structure is continuous, if $E$ is given the discrete topology. Attempt : So I have already shown that $\hat{\mathbb{Z}} \times E \rightarrow E : ((a_{n})_{n > 0}, e) \mapsto a_{m}e$ defines a module structure where $\text{ord}(e) = m$ by verifying the axioms. Also, I have already shown that the module structure is continuous if $E$ is given the discrete topology. However I am struggling with proving the uniqueness. If anybody has a hint for this I would appreciate that. This is the first question I have ever encountered about module structures, so I am unaware of any theorems that could be applicable. This question has been asked before but there have been no responses about the uniqueness part of the question.","['torsion-groups', 'modules', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4930833,Combinatorics - find number of ways to distribute students,"In a course, there are $3n$ male students, $3n$ female students and $2n$ lecturers: The task is to distribute the students among the lecturers, such that in every lecturer's class there are exactly $3$ students, at least one is a male and one is a female. In how many ways can we distribute the students ?. My approach : First choose $2n$ males and $2n$ females to make sure every lecturer has one male and one female - ${3n\choose 2n}\cdot {3n\choose 2n}$ . Then, freely distribute the remaning ones - $(2n)!$ . Finally, cancel the order of choosing by dividing by $3!$ for each
triplet - so $(3!)^n\ldots$ The final result is ${3n\choose 2n}\cdot {3n\choose 2n}\cdot (2n)!\cdot
\frac{1}{(3!)^n}$ . I know this is wrong because if $n=1$ we get $3$ which is obviously incorrect. But I need some help understanding the incorrect combinatorical arguments I used, and how to solve this correctly. Cheers !.",['combinatorics']
4930899,Number of homomorphisms from $\DeclareMathOperator{\Z}{\mathbb Z}\Z\oplus \Z$ to $S_3$,"I read that a homomorphism is fully determined by where its generators are mapped to. Let $f\colon\, \Z\oplus \Z\to S_3$ be a group homomorphism. Generators of $\Z\oplus \Z$ are $(1,0)$ and $(0,1)$ . I can map $(1,0)$ to any permutation I want. I just need to make sure that $f(1,0)$ and $f(0,1)$ commute. Suppose: $f(1,0) = id$ (6 options for $f(0,1)$ ) $f(1,0) = (12), (23) \text{ or } (13)$ (2 options each for $f(0,1)$ ) $f(1,0) = (123) \text{ or } (132)$ (3 options each for $f(0,1)$ ) Thus, total of $6 + 2×3 + 3×2 = 18$ homomorphisms. Note: I counted the options by using the fact that the size of centralizer of an element is equal to order of the group divided by size of conjugacy class of the element. It looks like, this can be generalized. From $\Z\oplus \Z$ to $S_n$ , there are $n!\times k$ homomorphisms where $k$ is the number of disjoint conjugacy classes in $S_n$ . Is my conjecture true? I raise this doubt because I assumed that each pair of commuting permutations $f(1,0)$ and $f(0,1)$ determines a homomorphism. I don't know for certain if this is true.","['group-homomorphism', 'abstract-algebra', 'solution-verification', 'symmetric-groups', 'group-theory']"
4930927,Rotating and scaling an arbitrary triangle such that the new triangle has its vertices on the sides of the original one,"Given $\triangle ABC$ , and a scale factor $r \lt 1 $ , I want to find the necessary rotation (center and angle) such that the rotated/scaled version of the triangle has its vertices lying on the sides of $\triangle ABC$ . To clarify what I want.  Suppose the vertices $A,B,C$ are in counter-clockwise order.  I want to scale these points (homothety) about a certain point $P$ by a scale factor $r < 1$ , and then rotate them by an angle $\theta$ about the SAME point $P$ , such that the images of homothety $A',B',C'$ simultaneously land (after rotation) on $AB, BC, CA$ respectively, or simultaneously land on $AC, BC, AB$ respectively. How to do this ?  Are there known results on this ?","['analytic-geometry', 'geometry', 'triangles', 'trigonometry', 'rotations']"
4930944,The sum $s(k)=\sum_{n=1}^\infty\frac{\Gamma(\frac{1}{2^n}+1)}{\Gamma(\frac{1}{2^n}-k)}$ gives weird fractions,"I stumbled across a series while playing around with a functional equation, it looks like this: $$s(k)=\sum_{n=1}^\infty\frac{\Gamma(\frac{1}{2^n}+1)}{\Gamma(\frac{1}{2^n}-k)}$$ Mathematica gives interesting fractions for the first couple of values of $k$ . Starting with $k=0$ and incrementing by $1$ , they are: $$1, -\frac{2}{3},\frac{8}{7}, -\frac{328}{105}, \frac{1088}{93}, -\frac{108608}{1953}, \dots$$ But Mathematica wasn't able to evaluate the sum for a generic positive integer $k$ . So, can you $\Gamma$ -wizards tell me if there's a way of getting those fractions without numerically evaluating the sum? EDIT: I should say where I got that sum from! I was thinking about a series $$f(x)=\sum_{n=1}^\infty(x^{1/2^n}-1)$$ which satisfied the equation $f(x^2)=f(x)+x-1$ . I noticed that this is similar to $\ln(x^2)=\ln(x)+\ln(x)$ , expecially because $\ln(x) \approx x - 1$ near $1$ . So this function $f(x)$ is kind of like a undercooked logarithm or something. So I wanted to find a Taylor series of $f(x)$ near $x = 1$ and after a some algebra I got: $$f(x)=\sum_{k=0}^\infty\sum_{n=1}^\infty\frac{\Gamma(\frac{1}{2^n}+1)}{(k+1)!\cdot\Gamma(\frac{1}{2^n}-k)}(x-1)^{k+1}$$ which is how I got to the sum","['gamma-function', 'sequences-and-series']"
4930953,Proving differentiability by testing continuity of the partials?,"This is question 54 from Section 2.3 of Susan Jane Colley's Vector Calculus. I need to determine whether the function is differentiable at the point $(a,b) = (1, 0)$ $$
f(x,y) = \big((x-1)y\big)^{2/3}
$$ So my attempt at this began with checking the partial derivatives for continuity. If the partial derivatives are not continuous then $f$ fails to be differentiable. $$
f_x(x, y) = \frac{2y}{3\big((x-1)y \big)^{1/3}}\\\\
f_y(x, y) = \frac{2(x-1)}{3\big((x-1)y \big)^{1/3}}
$$ Given the both partials are not defined at $(a,b) = (1,0)$ , then I determined $f$ is not differentiable. However, I took a look at the solution manual it asserts that the function is indeed differentiable. I have provided her proof of this below. Solutions The solution manual asserts that $$
f(x,0) \equiv 0 \Rightarrow f_x(1, 0) = 0\\\\
f(1,y) \equiv 0 \Rightarrow f_y(1, 0) = 0
$$ The tangent plane $h(x, y) = f(1,0) + f_x(1, 0)(x-1) + f_y(1, 0)(y-0) = 0$ . Thus, for $(x,y) \neq (1,0)$ we have that $$
0 \leq \frac{|f(x,y) - h(x,y)|}{||(x,y) - (1,0)||} = \frac{|f(x,y)|}{\sqrt{(x-1)^2 + y^2}}
$$ Then $$
|f(x,y)| = |x-1|^{2/3}|y|^{2/3} \leq ((x-1)^2 + y^2)^{1/3}((x-1)^2 + y^2)^{1/3}\\\\
= ((x-1)^2 + y^2)^{2/3}
$$ Thus, $$
\frac{|f(x,y)|}{\sqrt{(x-1)^2 + y^2}} \leq \frac{((x-1)^2 + y^2)^{2/3}}{((x-1)^2 + y^2)^{1/2}} = ((x-1)^2 + y^2)^{1/6}
$$ Because the last expression approaches zero as $(x,y) \rightarrow (0,0)$ then $f$ must be differentiable at $(1,0)$ I have several questions Could you explain why despite the fact the partial derivatives are not continuous at $(a,b) = (1,0)$ , we go on to assert differentiability? Why does $f(x,0) \equiv 0$ mean that $f_x(1, 0) = 0$ ? The argument on the tangent plane is confusing. We say $h(1,0) = 0$ but the proof goes on to say ""For $(x,y) \neq (1,0)$ "" and then proceeds to use $h(x)$ at $(1,0)$ In the line beginning $|f(x,y)$ , what identity is this being used? In the final argument, is this some variation of the squeeze theorem in that if $\lim_{(x,y) \rightarrow (1,0)}((x-1)^2 + y^2)^{1/6} = 0$ then since this is greater than or equal to $|f(x,y)| = |x-1|^{2/3}|y|^{2/3}$ , then its limit must also be zero? Apologies for the long question but I want to ensure I understand this.","['calculus', 'derivatives']"
4931022,Showing that the union of two families of disjoint sets is disjoint?,"I misread a theorem in my textbook and ended up with the following statement: Let $\mathcal{F}$ and $\mathcal{G}$ be nonempty families of sets, where every element of $\mathcal{F}$ is disjoint from every element of $\mathcal{G}$ . Prove that $\bigcup \mathcal{F}$ and $\bigcup \mathcal{G}$ are disjoint. Here is a draft of my attempted proof: Let $\mathcal{F}$ and $\mathcal{G}$ be nonempty families of sets, and suppose every element of $\mathcal{F}$ is disjoint from every element of $\mathcal{G}$ . Let $A$ be an arbitrary set in $\mathcal{F}$ , and choose an arbitrary element $a \in A$ . So, $a \in \bigcup \mathcal{F}$ . Then for all $B \in \mathcal{G}$ , $A \cap B = \emptyset$ . In particular, $a \notin B$ , and $a \notin \bigcup \mathcal{G}$ . Since $a$ was an arbitrary element in $\bigcup \mathcal{F}$ , $\bigcup \mathcal{F} \cap \bigcup \mathcal{G} = \emptyset$ . I feel like something is wrong at the end of my proof, and I don't even know if the theorem I wrote here is correct. I've seen many counterexamples on MSE involving $\{\emptyset\}$ for similar questions, but I didn't grasp them enough to apply them here. Could someone provide clarification on whether the theorem is correct and, if not, explain why? Also, if my proof is flawed, could you point out where it goes wrong and how to fix it? Thank you in advance for your help!","['elementary-set-theory', 'solution-verification']"
4931115,Prove that the sequence defined by $a_{n + 1} = 2a_n^2 - 1$ is always negative iff $a_1 = -\frac{1}{2}$,"I want to prove that the sequence defined by $a_{n + 1} = 2(a_n)^2 - 1$ is always negative if and only if $a_1 = -\dfrac{1}{2}$ . Here are some observations I've made so far. However, I haven't been able to consolidate them into a full proof: With $a_1 = -\dfrac{1}{2}$ , all terms in the sequence are $-\dfrac{1}{2}$ . So, $-\dfrac{1}{2}$ is a fixed point of the sequence. The only other fixed point of the sequence (found by solving the equation $a_n = a_{n + 1} \iff a_n = 2(a_n)^2 - 1$ ) is $1$ , but that is not negative. Now, say that $a_n \leq 0$ . If $(a_n)^2 > \dfrac{1}{2}$ (iff $a_n < \dfrac{\sqrt{2}}{2}$ ), then we are guaranteed that $a_{n + 1} > 0$ , and we are done. Otherwise, if $\dfrac{\sqrt{2}}{2} \leq a_n \leq 0$ , we have $-\dfrac{1}{2} \leq a_{n + 1} \leq 1$ , and so $\dfrac{1}{4} \leq (a_{n + 1})^2 \leq 1$ , and so $-\dfrac{1}{2} \leq a_{n + 2} \leq 1$ . This is the same constraint we found on $a_{n + 1}$ , so continuing this way gives us no progress. Finally, I've observed that when I test different values for $a_1$ , the values seem to oscillate around $-\dfrac{1}{2}$ (getting farther away every time, unless we started at $-\dfrac{1}{2}$ ) until we find a value whose square is more than $\dfrac{1}{2}$ . I'm not sure what this implies, though. Can anyone provide hints or a solution to this problem?","['contest-math', 'proof-writing', 'sequences-and-series']"
4931120,"Flux of $\mathbf{F}=(z+\sin(y), xyz, 1)$ across the boundary of $V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\}$","I have calculated the outward flux of $\mathbf{F}=(z+\sin(y), xyz, 1)$ across $V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\}$ directly and using the divergence theorem and the answers I have got seem to coincide but I am still unsure if I have done the calculations with the right orientation: I should calculate the outward flux but it seems to me that $\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}$ is pointing inside $V$ which is a contradiction; so, I would be grateful if someone would check out my work and explain to me how to verify that I have chosen the right orientation. Thanks. What I have done: Given that $\partial V=\partial V_{top}\cup\partial V_{lat}+\partial V_{bottom}$ we parametrize the top surface boundary of $V$ by $\mathbf{g}:(0,\sqrt{2})\times (0,2\pi)\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}r\\ \theta\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ r^2\end{bmatrix}$ so $\frac{\partial\mathbf{g}}{\partial r}=(\cos(\theta),\sin(\theta), 2r)$ , $\frac{\partial\mathbf{g}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0)$ , $\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}=(-2r^2\cos(\theta), -2r^2\sin(\theta), r)$ , the lateral surface boundary by $\mathbf{h}:[0,2\pi]\times [0,2]\to\mathbb{R}^3, \mathbf{h}\begin{pmatrix}\theta\\ z\end{pmatrix}=\begin{bmatrix}\cos(\theta)\\\sin(\theta)\\ z\end{bmatrix}$ , $\frac{\partial\mathbf{h}}{\partial\theta}=(-\sin(\theta),\cos(\theta),0), \frac{\partial\mathbf{h}}{\partial z}=(0,0,1), \frac{\partial\mathbf{h}}{\partial\theta}\times \frac{\partial\mathbf{h}}{\partial z}=(\cos(\theta),\sin(\theta),0)$ , the bottom surface by $\mathbf{f}:(0,2\pi)\times(0,\sqrt{2})\to\mathbb{R}^3,\ \mathbf{f}\begin{pmatrix}\theta\\ r\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ 0\end{bmatrix}$ so $\frac{\partial\mathbf{f}}{\partial r}=(\cos(\theta),\sin(\theta), 0)$ , $\frac{\partial\mathbf{f}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0)$ , $\frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}=(0, 0, -r)$ we get \begin{align}
\text{flux}_{\partial V}(\mathbf{F})&=\text{flux}_{\partial V_{top}}(\mathbf{F})+\text{flux}_{\partial V_{lat}}(\mathbf{F})+\text{flux}_{\partial V_{bottom}}(\mathbf{F})\\ &=\int_{\partial V_{top}}\mathbf{F}\left(\mathbf{g}(r,\theta)\right)\cdot\left(\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\right)drd\theta+\int_{\partial V_{lat}}\mathbf{F}(\mathbf{h}(\theta, z))\cdot\left(\frac{\partial\mathbf{h}}{\partial\theta}\times\frac{\partial\mathbf{h}}{\partial z}\right)d\theta dz\\ &+\int_{\partial V_{bottom}}\mathbf{F}(\mathbf{f}(\theta, r))\cdot\left(\frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}\right)d\theta dr=\\&=\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}\left( (r^2+\sin(r\sin(\theta)))(-2r^2\cos(\theta))+r^4\cos(\theta)\sin(\theta)(-2r^2\sin(\theta))+1\cdot r \right)dr\right)d\theta\\& +\int_{z=0}^{z=2}\left(\int_{\theta=0}^{\theta=2\pi}(z+\sin(\sin(\theta))\cos(\theta)+(\cos(\theta)\sin(\theta)z)\sin(\theta)+1\cdot 0)d\theta\right)dz\\& +\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}[(0+\sin(r\sin(\theta)))0+0\cdot 0+1\cdot (-r)]d\theta\right)dr\\&=\int_{r=0}^{r=\sqrt{2}} \left(\int_{\theta=0}^{\theta=2\pi}(-2r^4\cos(\theta)-2r^2\cos(\theta)\sin(2\sin(\theta))-2r^6\cos(\theta)\sin^2(\theta)+r)dr\right)d\theta+0-2\pi\\ &=\int_{r=0}^{r=\sqrt{2}}(2\pi r)dr-2\pi=2\pi-2\pi=0.
\end{align} Applying the divergence theorem we get: $$\text{flux}_{\partial V}(\mathbf{F})=
\iiint_V \text{div}(\mathbf{F})\,dxdydz=\int_{\theta=0}^{\theta=2\pi}\int_{r=0}^{r=\sqrt{2}}\int_{z=0}^{z=2}r\cos(\theta)r^2\cdot rdr d\theta=0.$$","['divergence-theorem', 'multivariable-calculus', 'solution-verification']"
4931151,Probability of the Maximum of $6$ die rolls,"Someone please explain to me why this is wrong. Here is the question: If I roll $6$ different fair $6$ -sided dice, what is the probability that the maximum of the rolls is $5$ ? My approach was to find the the PDF of the Maximum, then evaluate at $x = 5$ : Let $f (\text{Max})$ be the PDF of the maximum, then $f (\text{Max}) = \frac{6!}{5!} \frac{1}{6} F (\text{Max})^5$ , which is $\left( \frac{\text{Max}}{6} \right)^5$ . Evaluating at $\text{Max} = 5$ , gives us $\left( \frac{5}{6} \right)^5$ . I know this is actually the CDF of the maximum here, but where did I go wrong in my derivation of the PDF?","['actuarial-science', 'statistics', 'combinatorics', 'order-statistics', 'probability']"
4931164,Is there a matrix with rational entries similar to a given matrix?,"I am working on the following problem. Is there a matrix $A$ with rational entries similar to the matrix $\begin{pmatrix} \sqrt{2} & 0\\0 & \sqrt{2}\end{pmatrix}$ ? What about $\begin{pmatrix} \sqrt{2} & 0\\0 & -\sqrt{2}\end{pmatrix}$ ? Now, in the first case, there is no such matrix because its trace would be $2 \sqrt{2}$ . But in the second case, all I can answer is ""maybe"". The trace is zero, so I can't rule it out. I know the eigenvalues will be $\pm \sqrt{2}$ , but this doesn't tell me anything. Is there a systematic way to solve this? Can I ""undiagonalize"" a matrix in such a way that I can tell directly whether this matrix $A$ exists or not? Any help is appreciated.","['matrices', 'similar-matrices', 'linear-algebra']"
4931188,Is the Residue Theorem applied correctly?,"Can you please tell me if I've done this exercise correctly? Given the function $\displaystyle g(z)=\frac{ze^{1/z}}{z^{2} + 3}\,,\,$ find all the singularities and compute the integral $$I = \oint_{|z|=1} g(z)dz$$ First of all I found the singularities, namely $z=0$ , $z=\sqrt{3}I$ and $z=-\sqrt{3}I$ . The first on is an essential singularity while the other two are 1-poles (poles of order 1).
In order to find the integral I want to apply the Residue Theorem as follows: $$I=\int_{|z|=1} g(z)dz = 2\pi iRes(g,0)$$ Since the direct computation of the Laurent series of g around z=0 seems too difficult, I tried to compute $Res(g,0)$ as $$Res(g,0)=-Res(g,\infty)-Res(g,\sqrt{3}i)-Res(g,-\sqrt{3}i)$$ If we set $h(\omega)=-\frac{1}{\omega^2}g(\frac{1}{\omega})$ , we can compute $Res(g,\infty)=Res(h,0)$ $$-\frac{1}{\omega^2}g(\frac{1}{\omega})= -\frac{1}{\omega^2}\left(\frac{\frac{1}{\omega}e^\omega}{\frac{1}{\omega^2}+3}\right)=-\frac{1}{\omega}\frac{e^\omega}{1+3\omega^2}$$ Now if I consider a suitable area around $\omega=0$ I can develop $\frac{1}{1+3\omega^2}$ as a geometric series: $$\frac{1}{1+3\omega^2} = \sum_{n=0}^{+\infty}(-1)^n(3\omega)^n$$ So since $$-\frac{1}{\omega^2}g(\frac{1}{\omega})=-\frac{1}{\omega}\frac{e^\omega}{1+3\omega^2} =- \frac{1}{\omega}(1+\omega+\frac{\omega^2}{2}+\dots)(1-3\omega^2+\dots)=-\frac{1}{\omega}+\dots$$ Thus $Res(g,\infty)=-1$ and after some computation I found that $I=2\pi i( 1-cos(\frac{\sqrt{3}}{3}))$ . So, my question is: are my passages legit?
Is there another way to compute $Res(g,0$ )? Thank you!","['complex-analysis', 'complex-integration', 'solution-verification']"
4931221,"If $a$, $b$, $c$ are the roots of $x^3-6x^2+3x+1=0$, find all posible values of $a^2b + b^2c + c^2a$ and ""hence"" find $|(a-b)(b-c)(c-a)|$ . [duplicate]","This question already has an answer here : Using Vieta's theorem for cubic equations to derive the cubic discriminant (1 answer) Closed 18 days ago . Let $a$ , $b$ , $c$ be the roots of $$x^3 - 6x^2 + 3x + 1 = 0$$ Find all the possible values of $a^2b + b^2c + c^2a$ and hence find $|(a-b)(b-c)(c-a)|$ . My try : Let $P(x) = x^3-6x^2+3x+1 = (x-a)(x-b)(x-c)$ Then, $\frac{P(x)}{x-a} = (x-b)(x-c) => (a-b)(a-c) = \lim{x → a} \frac{x^3-6x^2+3x+1}{x-a} = 3(a^2-4a+1) = 3(a-(2+√3))(a-(2-√3))$ . Similarly, $(b-a)(b-c) = 3(b^2-4b+1) = 3(b-(2+√3))(b-(2-√3))$ and $(c-a)(c-b) = 3(c^2-4c+1) = 3(c-(2+√3))(c-(2-√3))$ . Combining them, we get $(a-b)^2(b-c)^2(c-a)^2 = 27P(2+√3)P(2-√3)$ . How to proceed from here ??","['algebra-precalculus', 'polynomials']"
4931296,$(1-x)^{n+a} \sum_{j=0}^\infty \binom{n+j-1}{j}\binom{n+j}{a} x^j = \sum_{j=0}^a \binom{n}{a-j}\binom{a-1}{j} x^j$,"Let $n$ and $a$ be natural numbers. How to prove the following for $x \in [0, 1)$ ? $$
(1-x)^{n+a} \sum_{j=0}^\infty \binom{n+j-1}{j}\binom{n+j}{a} x^j = \sum_{j=0}^a \binom{n}{a-j}\binom{a-1}{j} x^j
$$ Context: This came up while solving this question . Numerical evidence on Desmos . I tried expanding the $(1-x)^{n+a}$ with binomial formula. Or maybe since we're after a sum from $0$ to $a$ , expand $(1-x)^{n}$ and $(1-x)^{a}$ separately?","['binomial-coefficients', 'combinatorics', 'generating-functions', 'power-series', 'probability']"
4931300,How to calculate $\int_{0}^{\infty} x^n(1+x)^n e^{-ncx^2}\text{d}x$,"As the title mentioned, I want to calculate \begin{equation}
\int_{0}^{\infty} x^n(1+x)^n e^{-ncx^2}\text{d}x,
\end{equation} where $n$ is a positive integer, $c$ is a positive real number in the maginitude of 1 (valued around 1). This integral is difficult because of the term $(1+x)^n$ , if we expand this term then the result can be written as \begin{equation}
\frac12 \sum_{k=0}^{n} \binom nk (nc)^{-\frac{n+1+k}2}\Gamma(\frac{n+1+k}2),
\end{equation} where the sum of Gamma function is also difficult to deal with.
If the exact result is difficult, a strict upper bound is also welcomed, or an asymtotic bound w.r.t. $n\to \infty$ .","['integration', 'definite-integrals', 'gamma-function', 'exponential-function', 'hypergeometric-function']"
4931308,Constructing Paths in a Connected Graph with Odd-Degree Vertices,"I am working on a problem and would appreciate some insights or suggestions on how to approach it. Problem:
Let G = (V, E) be a connected graph where n is the number of vertices in V that are of odd degree. How can one construct n/2 trails in G such that each edge in E is contained in exactly one of these trails? (edit: replaced path with trail) My solution:
By the Handshaking Lemma, the number of vertices of odd degree is even; thus, n/2 will always be a positive integer. Construct pairs of vertices of odd degree such that no vertex of odd degree is left unpaired. Construct a path between each pair such that the path contains only vertices of even degree between the start and end vertices. According to the theorem discussed in the lecture, it follows that this path is Eulerian. Now, remove the edges of the Eulerian path from the graph and repeat the process with all remaining pairs of vertices.","['graph-theory', 'discrete-mathematics', 'eulerian-path']"
4931463,"Bounds of Pearson correlation coefficients for (X,Z) knowing those for (X,Y) and (Y,Z)","Assume $X,Y,Z$ are three variables over a set of data (say, a finite set of data to avoid discussions of convergence).  Suppose we know the Pearson correlation coefficient $r_{X,Y}$ and $r_{Y,Z}$ : given these data, what bounds (ideally sharp) can we put on $r_{X,Z}$ ? If I am not mistaken, the question is equivalent to the following: for a positive semidefinite matrix with a diagonal of $1$ , if we know the coefficients $(i,j)$ and $(j,k)$ in the matrix, what (ideally sharp) bounds can we put on the coefficient $(i,k)$ ? Surely this is a classical problem and has been considered before, but I don't know what terms to search for.","['semidefinite-programming', 'statistics', 'inequality']"
4931487,"Does the sequence $1, 2, 3, 4, 5, 6$ appear in the number of groups of order $n$ up to isomorphism?","Let $\mathrm{gnu}(n)$ denote the number of groups of order $n$ up to isomorphism. $\mathrm{gnu}(1), \mathrm{gnu}(2), \mathrm{gnu}(3), \dots$ is now a sequence of integers, and we may ask if and where the subsequence $1, 2, 3, \dots, k$ appears successively, for some integer $k$ . Just looking through the first few terms, it is not hard to find an instance of $1, 2, 3, 4$ , namely $\mathrm{gnu}(73), \mathrm{gnu}(74), \mathrm{gnu}(75), \mathrm{gnu}(76)$ . But $\mathrm{gnu}(77)=1$ , so this is not a $1,2,3,4,5$ . To find $1,2,3,4,5$ is significantly harder. There are necessary and sufficient conditions on $n$ which determine whether $\mathrm{gnu}(n)=1,2,3,4$ , listed for example in this paper on page 6. I implemented these to find the integers $n$ such that $\mathrm{gnu}(n+i)=i$ for $1 \leq i \leq 4$ . I've written a computer program to find these - the sequence begins $n = 72, 20664, 66600, 84744, 89784, 141240, 175032, 232680, 271272, 288072, 378984,...$ . Hence our candidates for where we want to find a $5$ are at $77, 20669, 66605, 84749, 89789, 141245, 175037, 232685, 271277, 288077, 378989,...$ . I've tested these manually and, hoping that my calculations and program are correct, the earliest instance of a $1,2,3,4,5$ occurs at $\mathrm{gnu}(2814121), \mathrm{gnu}(2814122), \mathrm{gnu}(2814123), \mathrm{gnu}(2814124), \mathrm{gnu}(2814125)$ . But $\mathrm{gnu}(2814126)=24$ , so this is not a $1,2,3,4,5,6$ . The obvious next challenge is to find a $1,2,3,4,5,6$ . That's a lot harder as: I've struggled to find a necessary and sufficient condition for $\mathrm{gnu}(n)=5$ anywhere online (and am not sure on how to begin deriving it myself!) As $n$ gets large, the tests for whether $\mathrm{gnu}(n)=1,2,3,4$ tend to get longer, since they're based on the prime factorization of $n$ So, I'm wondering if anyone has any observations that'd make finding a $1,2,3,4,5,6$ easier: Is there a necessary and sufficient condition for $\mathrm{gnu}(n)=5$ ? Is there some smarter method for how we may search for this?","['group-theory', 'finite-groups', 'groups-enumeration']"
4931514,Computing pullback of the one form $dx$,"I have the following 1-form in $\mathbb{R}^2$ which is $\omega=dx$ , and it is given the map $$\varphi: \mathbb{R}^2 \to S^2 \setminus \lbrace N \rbrace  $$ $$ (x,y)\mapsto \left( \frac{2x}{x^2+y^2+1}, \frac{2y}{x^2+y^2+1}, 1 - \frac{2}{x^2+y^2+1} \right) $$ where N is the north, and I want to compute the pullback of $(\varphi^{-1})$ , where I found that is given by $$ \varphi^{-1}: S^2 \setminus \lbrace N \rbrace \to \mathbb{R}^2   $$ $$ (x,y,z) \mapsto \left( \frac{x}{1-z}, \frac{y}{1-z} \right)  $$ but using the definition that Do Carmo gives I don't understand how to compute the pullback, where it is said that if we have $f: \mathbb{R}^n \to \mathbb{R}^m$ a differentiable map, then the pullback is: $$ (f^{*}w)(p)(v_1, \ldots, v_k) = \omega(f(p))(df_p(v_1), \ldots, df_p(v_k)) $$ where $p \in \mathbb{R}^n$ , $v_1, \ldots, v_k \in \mathbb{R}_{p}^{n}$ , any help would be appreciated.","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
4931629,Understanding Newtonian mechanics using concepts from differential geometry,"In a book I'm reading ( Friedrich and Agricola ), I encountered the following definition of a ""Newtonian system"": An autonomous Newtonian system is a triple ( $M^m$ , $g$ , $X$ ) consiting of a manifold $M^m$ , a Riemanian metric $g$ , and a vector field on the space $TM$ such that $d\pi \circ X = Id_{TM^m}$ , where $\pi$ is the canonical projection from $TM$ to $M$ . Rationale for definition is that ""not all vector fields are allowed, since a force can act only in space"". I'm having trouble understanding the definition, because the composition of $d\pi$ and $X$ being identity is equivalent to $X$ having only components of the form $\frac{\partial}{\partial x^i}$ (and no components $\frac{\partial}{\partial v^i}$ ). The book then provides the following example on $TM \cong \mathbb{R^2}$ where $M=\mathbb{R}$ : $X = \dot{x} \frac{\partial}{\partial x} + \frac{1}{m}(-k^2x-\rho\dot{x})\frac{\partial}{\partial \dot{x}}$ which does not seem to fit!","['physics', 'classical-mechanics', 'differential-forms', 'differential-geometry']"
4931631,Algorithm for finding intersection of two groups from generators,"Say I have two subgroups of $S_n$ defined from their generators.
E.g. $G_1 = \langle (0 3 4 1), (0 3 2 1 4)\rangle$ and $G_2 = \langle (4)(0 2 3 1), (0 4 3 2 1)\rangle$ .
Their intersection can be written in terms of three generators: $G_1\cap G_2 = 
    \langle (0 1 4 3),
    (0 4)(1 3),
    (0 3 4 1)\rangle$ . Of course one can find the intersection by generating all elements of $G_1$ and $G_2$ . But is there a faster way, assuming each group has just a small number of generators?","['permutations', 'group-theory', 'finite-groups', 'algorithms']"
4931672,On a probability density generated by the sum of reciprocols of divisors of integers,"For positive integers $n$ , define $H(n)$ as the sum of the reciprocals of divisors of $n$ : $$ H(n) = \sum_{k: k|n} 1/k$$ where a divisor includes both $1$ and $n$ .  For $t\geq 0$ define \begin{align}
f(t) &= \limsup_{n\rightarrow\infty} \frac{1}{n}\sum_{i=1}^n1_{\{H(i)\geq t\}}
\end{align} I would like to understand the $f(t)$ function (as well as perhaps its $\liminf$ version).  In particular, I would be interested in comments or strengthened forms of the following conjecture I just made (based on a previous question linked below). My conjecture uses only a Markov inequality, but perhaps there is some stronger Gaussian approximation that can be used. Or perhaps the conjecture is false? Conjecture: $f(t) \leq \frac{\pi^2}{6t} \quad \forall t >0$ Intuition: For all positive integers $N$ we have $$ H(N) = \sum_{k=1}^{\infty} 1_{\{k|N\}}\frac{1}{k} \quad (*)$$ We can imagine that for every positive integer $k$ , the ""probability"" that a ""randomly selected integer $N$ "" is divisible by $k$ is $1/k$ . This can be made precise by observing $$ \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{i=1}^n 1_{\{\mbox{$k$ divides $i$}\}} = 1/k$$ However, for the sake of this intuitive idea, let's keep the notion of ""randomly selected integer"" and ""probability"" imprecise.
So if we imagine $N$ as a ""randomly selected large integer"" then from (*) $$ E[H(N)] = \sum_{k=1}^{\infty} P[\mbox{$k$ divides $N$}]\frac{1}{k}= \sum_{k=1}^{\infty}\frac{1}{k^2}=\frac{\pi^2}{6}$$ and by using the Markov inequality (imagining that it still works in this imprecise probability world) we obtain for all $t>0$ : $$ P[H(N)\geq t] \leq \frac{E[H(N)]}{t} = \frac{\pi^2}{6t}$$ I suspect that a variation of this argument can be made rigorous by carrying out a sample path version of the Markov inequality. This was inspired by comments on the question here: $\lim_{n \to \infty} ​ H(a_n)=\infty$ where $H(a_n)$ is the sum of the inverses of the divisors of $a_n$",['number-theory']
4931679,Finding the number of zeros on a half plane of $z^4+3z^2 + z + 1$,"I found this problem in Berkeley problems in Mathematics : How many roots has the polynomial $z^4+3z^2 + z + 1$ in the right half $z$ -plane? To this point, I have tried using the method prescribed by this answer in which we can compute what parts of the contour contribute to the winding number, but it seems we get less lucky here that the ""real part of $p(it)$ along the contour $[-iR, iR]$ has 4 zeros instead of being all positive. I see how this could contribute two to the winding number, but that isn't definitive since we have $$
\Re(f(it)) = \mathcal{O}(z^4) \quad \text{and} \quad \Im(f(it)) = \mathcal{O}(z).
$$ When you think about it, the contour along $Re^{it}, t \in [-\pi/2, \pi/2]$ contributes a winding number of 2, so one should rule out the possibility of the imaginary line contributing anything. In general, I'm looking for more strategies to Rouche Theorem problems, and any suggestions would help, thanks!","['complex-analysis', 'rouches-theorem']"
4931694,Problematic limit $\epsilon \to 0 $ for combination of hypergeometric ${_2}F_2$ functions,"In an earlier question , the integral $$I_n(c)=\int_0^\infty x^n (1+x)^n e^{-n c x^2} dx$$ was considered with particular focus on its behavior for positive integer $n$ . In trying to analyze this, it appears that Mathematica runs into issues in taking certain limits. At risk of being a CAS-centric question, I will dispense with the original integral and instead focus on a specific example arising from it (namely, that of $n=c=1$ ). After extensive offline manipulation, the discrepancy arises as follows. Let $$f(\epsilon )={_2}F_2 \left(\begin{array}{c}1/2,-\epsilon/2\\ 3/2,-\epsilon \end{array}\Bigg{\vert} -1\right)+\frac16~ {_2}F_2 \left(\begin{array}{c}3/2,1+\epsilon/2\\ 5/2,2+\epsilon \end{array}\Bigg{\vert} -1\right)$$ (Here I've let $\epsilon=n-1$ , so as to focus on a zero limit.) Empirically (i.e., having Mathematica compute for $\epsilon \approx 0$ ) it appears that $f(0)\to 1$ as $\epsilon \to 0$ . This is also consistent with the original integral, which can be computed by elementary means in the case of $n=1$ (though verifying $f(0)=1$ by this route requires the extensive manipulations alluded to earlier). However, if Mathematica directly evaluates $f(0)$ it instead obtains $$f(0)\underset{?}{=}\frac{3}{2}-\frac{\sqrt{\pi}}{4}\operatorname{erf}(1)\approx 1.12659$$ where $\text{erf}(x)$ is the error function. So for some reason Mathematica behaves as though $f(\epsilon)$ is well-defined but discontinuous at $\epsilon=0$ . What I haven't been able to suss out is if this is a software issue or something more substantive. The limit is not entirely trivial, since when $\epsilon \to 0$ the hypergeometric function picks up integer parameters which can introduce complications. So what I'm looking for is a direct proof that $f(0)=1$ , and hopefully some means of clarifying Mathematica's error.","['summation', 'pochhammer-symbol', 'calculus', 'limits', 'hypergeometric-function']"
4931736,Stability of Hamiltonian system on degenerate critical point.,"I'm trying to find information on the stability of the following ODE: $$ x'' = x^4-x^2.$$ We know that it has a Hamiltonian $H(x,y) = \dfrac{y^2}{2} - (\dfrac{x^5}{5} - \dfrac{x^3}{3})$ . The orbits are given by the equation $$ y(x) = \pm \sqrt{\alpha(3x^5 - 5x^3 - \beta)}$$ for constants $\alpha = 2/15$ and $15\beta = H(x,y)$ . Now, we know that $3x^5 - 5x^3 - \beta$ is positive when the graph of $P(x) = 3x^5 - 5x^3$ is above the line $y=\beta$ . Looking at the graph we see that the domain of definition is disconnected when the line is in between the local extrema of $P(x)$ and only becomes connected when the line goes above or below the local extrema of $P(x)$ . From this information, we can conclude that the critical points $(-1,0)$ and $(1,0)$ obviously have to be a centre and a saddle point. However, we can't get any information on the critical point at $(0,0)$ . Looking at the direction field We kind of expect it to be a centre, but I really don´t know how to prove this. Geometrical reasoning aside, it easy to check that the Hessian has positive eigenvalues at $(-1,0)$ and that the Jacobian has a positive and a negative eigenvalue at $(1,0)$ and thus these points are a centre and a saddle point, but what do I do at zero?","['ordinary-differential-equations', 'stability-theory', 'hamilton-equations', 'nonlinear-dynamics', 'dynamical-systems']"
4931742,"Size of the set of all $f:${$1,2,3,4,5,6$} $\rightarrow$ {$1,2,3,4,5,6$} s.t. $\forall n \in${$1,2,3,4,5,6$} $f(n) \neq n$ and $f(f(f(f(f(f(n))))))=n$","I considered the smaller problem where the set instead has $4$ elements and $f(f(f(f(n))))=n$ , and after listing all of the possibilities I counted a total of $9$ . That immediately tells me that the answer for a question of this kind will not necessarily be a factorial number, which is perhaps what a naïve first instinct might make one think (in fact, everyone in the comments section of the social media post I ripped this problem from keeps insisting that the answer is $5!$ or $6!$ , likely reasoning too hastily and uncarefully). I repeated this process for the case where we have a set with $5$ elements and $f(f(f(f(f(n)))))=n$ , and I counted a total of $44$ possibilities. It's also worth noting that in the case where we have $3$ elements and $f(f(f(n)))=n$ we of course only have $2$ possibilities. Thus, the sequence seems to go: $2,9,44$ Based on an intuition regarding symmetry, I'm willing to bet that if $m$ is the number of elements in our set then the answer as to the cardinality of the set of $f$ will always be divisible by $m-1$ Thus, I'm willing to bet that the answer to this question is divisible by $5$ . Unfortunately, this is the closest I've gotten to making progress. I really don't even know how to start. However, after running my partial sequence ( $A000166$ ) through the On-Line Encyclopedia of Integer Sequences, it seems to make sense to believe that what we're dealing with here are in fact the subfactorial numbers. It makes so much sense, in fact, that I'm almost certain this is the case, in which case the answer to this question would be $!6=265$ . If that ends up being the case, maybe this problem could be a good way for me to be introduced to the subfactorial numbers and the theory behind them. As noted in the comments, I made some mistakes in my intuition-building examples. Most importantly, I should make it clear that we're actually dealing with this sequence ( $A261531$ ) instead. In the case with $5$ elements, the two integer sequences are no longer identical and it already starts to become important to realize that the subfactorial number $!5=44$ of permutations I initially counted includes permutations that fail to satisfy $f(f(f(f(f(n)))))=n$ . This makes the cases of $5$ or more elements fundamentally different from the cases of $4$ or fewer elements.",['combinatorics']
4931780,Show that $\frac{1}{\sqrt{n}} S_n$ does not converge in probability. [duplicate],"This question already has answers here : Convergence in Central Limit Thorem (3 answers) Closed 16 days ago . Let $(X_n)$ be a sequence of stochastically independent and identically distributed random variables with $\mathbb{E}X_1 = \mu < \infty$ and $\text{Var} \, X_1 = \sigma^2 < \infty$ , where $\sigma^2 > 0$ . Let $Y_n = X_n - \mu$ and $S_n = \sum_{k=1}^{n} Y_k$ for all $n \in \mathbb{N}$ . The central limit theorem now gives the convergence $$ \frac{1}{\sqrt{n}} S_n \xrightarrow{d} X, \quad n \to \infty, $$ for an $X \sim \mathcal{N}(0, \sigma^2)$ . Show that $\frac{1}{\sqrt{n}} S_n$ does not converge in probability. Hint : Consider $Z_{2n}$ , where $Z_n = \frac{1}{\sqrt{n}} S_n$ . I don't understand why the claim should hold, so what is going wrong that it doesn't converge in probability?","['convergence-divergence', 'central-limit-theorem', 'probability-theory', 'probability']"
4931793,Hamiltonian Paths and Cycles in Graphs,"Can someone verify if my proof is correct and rigorous? Thank you in advance! Proposition : Let $G$ be a graph with at least 3 nodes such that every node has at least $n/2$ neighbors. If $G$ has a Hamiltonian path, then $G$ contains a Hamiltonian circuit. I am attempting to prove this without using Dirac's or Ore's theorem. Proof :
Assume $G$ contains a Hamiltonian path $P = v_0, v_1, \ldots, v_n$ . By assumption, $v_0$ and $v_n$ each have at least $n/2$ neighbors. However, each can have at most $n-2$ possible neighbors excluding themselves and each other. Thus, considering both $v_0$ and $v_n$ , we have $n/2 + n/2 = n$ potential connections, but only $n-2$ possible distinct neighbors. By the pigeonhole principle, there must be at least one vertex that is a common neighbor of both $v_0$ and $v_n$ . Connecting $v_0$ and $v_n$ through this common neighbor forms a Hamiltonian circuit.","['graph-theory', 'pigeonhole-principle', 'discrete-mathematics', 'hamiltonian-path']"
4931820,Why isn't continuity necessary for existance of partial derivatives?,"Consider the graph of a function. $$f(x,y) =\begin{cases}
\frac{xy}{x^2+2y^2},  & \text{(x,y) $\neq$ (0,0)} \\
0, & \text{elsewhere}
\end{cases}$$ It is discontinuous at $(0,0)$ . As on path $y = mx$ , the function evaluates to $\frac{m}{1+2m^2}$ limit depends on the path and hence is not continuous. But its partial derivatives exist at $(0,0)$ . What's the difference between derivates and partial derivates that gave rise to this situation? Isn't a partial derivative much like a derivate of a 3D curve in a plane? Even the procedure to find PD becomes the same once we consider other variables constant.","['real-analysis', 'multivariable-calculus', 'calculus', 'partial-derivative', 'derivatives']"
4931854,The notion of orientation in vector spaces,"Do Carmo's Curves and Surfaces book states that ""two ordered bases $e = \{e_i\}$ and $f = \{f_i\}$ , $i = 1,\ldots,n$ , of an $n$ -dimensional vector space $V$ have the same orientation if the matrix of change of basis has positive determinant."" This same concept of ""orientation"" is the subject of another question on stack, but I am concerned with a purer intuitive understanding. What is the idea behind assigning this notion of same orientation to this particular circumstance with positive determinant? What properties of a matrix with positive determinant motivate this definition?","['curves', 'linear-algebra', 'intuition', 'differential-geometry']"
4931859,An unintuitive expected value in a coin toss sequence,"Originally I was investigating how to calculate the expected number of flips to obtain $n$ consecutive heads and $m$ consecutive tails. I played with $n=3$ , $m=2$ for a fair coin, and arrive at an expected value that seemed unintuitive. From now on let $n=3$ , $m=2$ , and let $X$ be the number of flips to achieve the goal. For clarity also let $M$ (resp. $N$ ) be the number of flips to obtain $m$ consecutive tails (resp. $n$ ) consecutive heads. Also assume the coin lands on its head with probability $1/2$ . Define $H$ (resp. $T$ ) as the event that the first flip is a head (resp. tail). Conditioning on the first flip, we get $$E[X]=1+\frac{1}{2}(E[X|H]+E[X|T]).$$ We can also condition $E[X|H]$ on the first position of a tail in the next $(n-1)$ flips. Since any tail in the next (n-1) flips resets the state to an initial $T$ , we get $$E[X|H]=\frac{1}{2}\cdot(1+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[X|T])+\left(\frac{1}{2}\right)^2\cdot(2+E[M]).$$ Let me note that the term $\left(\frac{1}{2}\right)^2\cdot(2+E[M])$ is for the next two flips being heads. From there we only need to obtain $m$ consecutive tails. Similarly, we can condition $E[X|T]$ on the position of the first head in the next $(m-1)$ flips. We have $$E[X|T]=\frac{1}{2}\cdot(1+E[X|H])+\frac{1}{2}\cdot(1+E[N]).$$ Now let me use the fact that $E[M]=6$ and $E[N]=14$ to arrive at \begin{equation*}
\begin{split}
E[X|H] =& 3+\frac{3}{4}E[X|T],\\
E[X|T] =& 8+\frac{1}{2}E[X|H].
\end{split}
\end{equation*} We can then solve $E[X|H]=\frac{72}{5}$ and $E[X|T]=\frac{76}{5}$ , resulting in $$E[X] = \frac{79}{5}.$$ It seems to me a bit surprising that $$E[X|H]<E[X|T],$$ where there are less consecutive tails required. I simply want to check that my calculation was  correct.",['probability']
4931864,How to force a product of two i.i.d. random variable to be gaussian,"This question is related to this other question of mine : I realized that my original question was maybe too abitious, and I would like to discuss a much more limited version of it. Consider two real random variables indipendent and identically distributed ( i.i.d. ) $X,Y$ . Now consider the random variable $Z$ given by: $$Z=XY, \tag{1}$$ how should X and Y be (identically) distributed to have $Z$ be destributed as a gaussian (normal distribution) with mean equal to $0$ and variance equal to $\sigma ^2$ ? I found no theory on the topic: some bibliographical references, if they exist, will be also much apreciated.","['statistics', 'independence', 'normal-distribution', 'probability', 'random-variables']"
4931889,Proof of second partials test for saddle points - why does Hessian need to be nonsingular?,"Suppose $f:U \subseteq \mathbb{R}^n \to \mathbb{R}$ is $C^2$ on $U$ and $\nabla f(x)=0$ at $x \in U$ . If $Hf(x)$ is indefinite and nonsingular—that is, if it has both positive and negative eigenvalues and none of its eigenvalues are $0$ , then $f$ has a saddle point at $x$ by the Second Partial Derivative Test. I am trying to prove this result, but I do not understand where the part about $Hf(x)$ being nonsingular comes in. My attempt is as follows. With a change of coordinates, we can write $h^THf(x)h$ as $y^TDy$ , where $D$ is a diagonal matrix with all the eigenvalues of $Hf(x)$ as its entries, and $y$ being the coordinates of $h$ with respect to the eigenbasis. Let $\{v_1,...v_n\}$ be the eigenbasis, all of unit length. Then $$h^THf(x)h=y^TDy=\lambda_1y_1^2+\cdots+\lambda_ny_n^2.$$ Suppose $\lambda_1>0$ . Then $h^THf(x)h>0$ for any $y_1$ as long as $y_2,...y_n=0$ . Taking $h=y_1v_1$ , we have $$f(x+h)-f(x)=\frac{1}{2} hHf(x)h+E(h)=\frac{1}{2}\lambda_1y_1^2+E(h)$$ where $E(h)$ is the error of the quadratic approximation. If $E(h)\ge 0$ , then $f(x+h)-f(x)>0$ . Therefore assume $E(h)<0$ . Fix $\epsilon<\lambda_1/2$ . Then for all small enough $h$ , i.e., all small enough $y_1$ , $|E(h)|<\epsilon \lVert h \rVert^2=\epsilon y_1^2$ . We thus have $$f(x+h)-f(x)=\frac{1}{2}\lambda_1y_1^2-|E(h)|\ge\frac{1}{2}\lambda_1y_1^2-\epsilon y_1^2>0.$$ Assuming that $\lambda_2<0$ and employing a similar argument, we thus see that along $v_1$ , $f$ is growing, but along $v_2$ , $f$ is decreasing. This tells us $f$ has a saddle point at $x$ . Is my reasoning correct? Where does nonsingularity come in? As long as there is one positive eigenvalue and one negative eigenvalue the argument holds; why do we need to ensure that all eigenvalues are nonzero?","['multivariable-calculus', 'linear-algebra', 'real-analysis']"
4931915,A problem in L1 space,"Problem: Let $(X, \mathcal{A}, \mu)$ be a measure space. Let $f: X \to [0, \infty)$ be measurable. Then define the set $$A_f = \left\{g \in L^1 (\mu)\ |\ |g| \leq f\mbox{ a.e.} \right\}.$$ Prove the following: $A_f$ is closed. If $A_f$ is norm bounded, then $f \in L ^1 (\mu)$ . I proved the first item, but I'm having trouble with the second. I have a feeling that I have to use Lebesgue's dominated convergence theorem, but I have no clue how to set it up. This is probably really easy, but I just couldn't find the solution for the last couple hours. Any help is appreciated. P.S. This question is from an old exam, it's not a homework question or anything like that.","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
4931981,An inequality about index in finite group,"Some denotations Let G be a finite group. Let N be a normal subgroup of G .(Which means $\forall n \in N, \forall g \in G, gng^{-1}\in N$ ) Let M be an arbitary subgroup of N . Let H be the normalizer of M in G .(i.e. $H = $ { $g\in G: \forall m\in M, gmg^{-1}\in M$ }) For any finite group W , denote the order of W by $|W|$ . Question Is the following inequality always true? $ |H|/|M| \geq |G|/|N| $ I found that the inequality is true for some group, including the Dihedral Group $D_4$ , the Quaternion Group $Q_8$ , the symmetric group $S_4$ . I also found that the inequality is true if $M$ is a Sylow subgroup of $N$ . I can provide details if required. It will be a quite beautiful conclusion if the inequality is true for any group. I will be rather appreciated if anyone may help me with this problem!","['group-theory', 'normal-subgroups']"
4932083,"What statement is true for every $a$, $b$ and $c$?","I have this task in mathematical logic for which I don't really have a tool for solving. What statement is true for every a, b and c? $a \in b \wedge b \in c \rightarrow a \in c$ $a \in b \wedge b  \subseteq c \rightarrow a \in c$ $a \subseteq b \wedge b \in c \rightarrow a \in c$ $a \subseteq b \wedge b \subseteq c \rightarrow a \subseteq c$ I need to determine which of these is true and need to explain my answer for every single one of these. Now, intuitively, all of these would be true (1. is trivial, 2. if a from b, b is a subset of c and then a in c is true because a is in b and b is a subset of c, 3. a subset of b, b from c then a is from c as well also should make sense (unless here $\in$ is meant to represent at least one element like for example theres element from b in c but doesnt necessarily need to include whole area of b? I consider $\in$ to represent an entire set belonging in another set, basically like a strict inclusion)). I think my intuition is off and I'm definitely missing a tool required to solve this. What tools are required for this and how to, after solving, explain my reasonings?","['elementary-set-theory', 'boolean-algebra', 'logic', 'discrete-mathematics']"
