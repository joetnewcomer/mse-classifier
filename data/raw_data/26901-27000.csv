question_id,title,body,tags
247385,"Prove that $ f(x) $ has at least two real roots in $ (0,\pi) $","Let $ f $ be a continuous function defined on $ [0,\pi] $. Suppose that $$ \int_{0}^{\pi}f(x)\sin {x} dx=0,   \int_{0}^{\pi}f(x)\cos {x} dx=0 $$ Prove that $ f(x) $ has at least two real roots in $ (0,\pi) $",['calculus']
247404,Open mapping theorem and second category,"This seems like a fundamental result but I can not solve it of find an solution:
Let $M:X\rightarrow U$ be a bounded linear map between Banach spaces. Show that if the 
range of M is a set of second category of U; then the range is all of U.","['operator-theory', 'functional-analysis']"
247422,What's the expected value of a lottery ticket? [duplicate],"This question already has answers here : Which is the correct way to calculate the expected value of a shared lottery jackpot? (4 answers) Closed last year . Suppose there's a lottery. Each ticket sold has probability $p$ of winning, and they are all independent of each other. The size of the jackpot is $j$. If there are $n$ winners, each winner gets a payoff of $j/n$ dollars. The total number of tickets sold is $t$. What is the expected value of a lottery ticket? Also, given that I win, what is the probability that I have to share the jackpot with at least one other person? PS - I think I know the answer, but have failed to convince someone else, so I'm looking for a third-party to give an answer.",['probability']
247425,Is there a vector space that cannot be an inner product space?,"Quick question: Can I define some inner product on any arbitrary vector space such that it becomes an inner product space? If yes, how can I prove this? If no, what would be a counter example? Thanks a lot in advance.","['vector-spaces', 'linear-algebra', 'inner-products']"
247434,What is a good metric to compare matrices?,"I have a matrix that I obtained from theoretical computation and I have another matrix which I obtained by actual data collection. How do I compare the two matrices? How do I state that one matrix is significantly different from the other? I have looked into: Frobenius Norm but I don't know what should be my threshold for comparison. Looking at max and min deviation of one matrix from the other. But again, what's a good threshold? I thought that to say one matrix varies significantly from the other, I would need to quantify the variance of those terms and compare how the deviation compares to the variance (or SD) but sadly, I don't have data for that and neither can I theoretically estimate any variances. I am kind of lost in what would be a good metric.","['matrices', 'linear-algebra']"
247435,Hilbert basis of vector space,"Let $V$ a vector space with inner product and $X\subset V$ orthonormal. Prove that exists a Hilbert basis (an orthonormal set of vectors with the property that every vector in $V$ can be written as an infinite linear combination of the vectors in the basis) such that $X\subset B$. I can consider $B=X\cup X^{\perp}$ and this set will be maximal, but I am not sure about this, is it correct? Another idea is using the Zorn lemma, but I need a ""chain"", how can I build this chain? Thanks for your help.","['linear-algebra', 'functional-analysis', 'analysis']"
247476,Calculating probability of some event using geometric considerations,"I want to estimate exponentially the following probability: Let $\bf{U}\in\mathbb{R}^n$ be a random vector uniformly distributed on the $n$-dimensional hypersphere, centered at the origin with radius $R$, and let $A\in\mathbb{S}_{++}^{n\times n}$ be a positive definite and deterministic matrix. I want to estimate the probability that 
$$
\text{Pr}(||\bf{v}-A\bf{U}||^2\approx n\alpha) = ?\ \ \ (1).
$$ As example, when $A$ is an identity matrix, we have 
$$
\text{Pr}(||\bf{v}-\bf{U}||^2\approx n\alpha)
$$ 
which can be interpreted as the probability that a randomly vector $\bf{U}$ on the hypersphere ($R$) will fall on the hypersphere centered around $\bf{v}$ with radius $\approx\sqrt{n\alpha}$. Equivalently, we want to find the probability that a randomly vector $\bf{U}$ on the hypersphere ($R$) would have some ""correlation"" coefficient (Pearson product), $\beta$, with $\bf{v}$. This probability can be calculated as the fraction between the surface area of the $n-2$ dimensional ""circle"" with radius $R\sin(\gamma)$ (in which $\beta = \cos(\gamma)$), and the $n$ dimensional sphere of radius $R$. It is easily can be shown that as $n\to\infty$ this probability is given by $\exp(n\ln(1-\beta^2)/2)$. When we introduce the matrix $A$, then $\bf{Y} = AU$ lives on the the $n$-dimensional hyper-ellipsoid . Accordingly, we want to find the probability that a randomly vector $\bf{Y}$ on the hyper-ellipsoid will fall on the hypersphere centered around $\bf{v}$ with radius $\approx\sqrt{n\alpha}$. So we need to find the fraction between the surface area of the interaction of this two shapes, and the surface area of the hyper-ellipsoid. Any suggestions how to accomplish that, or, solving the problem in different way? Thank you!","['probability-theory', 'probability-distributions', 'probability']"
247525,"Checking if one ""special"" kind of block matrix is Hurwitz","I have the next block matrix $$
J = \begin{bmatrix}A & B \\ K &0\end{bmatrix}
$$
all matrices are square, where $A < 0$ (definite negative), $B$ has all its eigenvalues with positive real part being $A = - (B + B^T)$, and $K$ is a diagonal matrix. What I see from numerical simulation, is that if $$K < 0 \iff J \text{ is Hurwitz}$$. Any clue about how can I prove this? Another question, I have seen many times that $$
M = \begin{bmatrix}A & B \\ -B^T &0\end{bmatrix}
$$ is Hurwitz if $A < 0$, but I can not find the proof for it. I guess it is really
related to the former question. Many thanks in advance. Edit More ideas related to the second question. $M$ is Hurwitz and its characteristic polynomial is det$(\lambda^2I - A\lambda + BB^T)=0$. The cp of J is det$(\lambda^2I + (B + B^T)\lambda - BK) = 0$. Looking at the two cps, $-A > 0$ and $B+B^T > 0$, and $BB^T > 0$ and $-BK$ has its Eigenvalues with positive real part $\iff BK + K^TB^T < 0$. Is this fact related to being Hurwitz ? I mean, the block matrix is Hurwitz if its cp det$(\lambda^2I + V\lambda + W)=0$ has $V > 0$ and $W > 0$ ? Edit 2 Thank you very much for your response in Answer1. You are right, and also I have found several counterexamples to this conjecture. However changing the condition (which has been proved wrong with a counter example) I have not found a counter example (yet).
Let $K = -cI$, with $c>0$ being a scalar. In other words, 
$$BK + K^TB^T < 0 \iff J \text{ is Hurwitz}$$. Finally, I have found a counter example (third comment to the answer 1). So this conjecture is wrong too. However, it seems that for $c$ sufficiently small, $J$ is Hurwitz, now I have to found the condition in $c$ for that. Any clues or suggestions? Edit 3 Finally, it has been found with counterexamples, that the last conjecture is false too. Then, the only way (as far as I know) that I have for assessing the stability of $J$ is to check the next linear matrix inequality. $$
J \quad\text{is Hurwitz if} \quad \exists K \quad \text{s.t.} \quad J+J^T \prec 0 
$$","['eigenvalues-eigenvectors', 'hurwitz-matrices', 'matrices', 'linear-algebra', 'block-matrices']"
247538,Stochastic integrals and new probability measures,"Let $B$ be a standard Brownian motion on $(\Omega, \mathcal{F}, P, ({\mathcal{F}_t})_{t\ge0})$, where the filtration is the one generated by $B$. Fix a time interval $[0,T]$. Define the process $X$ as the solution to the SDE
$$
\mathrm dX_t = \sigma X_t\,\mathrm dB_t,\quad X_0 = 1.
$$ Define, for each real number $\alpha$, a measure $P_{\alpha}$, such that $X$ under $P_{\alpha}$ solves the equation
$$
\mathrm dX_t = \alpha  X_t\,\mathrm dt + \sigma X_t\,\mathrm dB^{\alpha},
$$ where $B^{\alpha}$ is a Brownian motion under $P_{\alpha}$. Give an explicit expression for the Radon-Nikodym derivative (likelihood process)
$$
L^{\alpha} = \frac{\mathrm dP_{\alpha}}{\mathrm dP_0}, 
$$
on $\mathcal{F}_t$. So this is the question. And I guess you're supposed to use the Itô formula. But I've had a hard time grasping the question. Some guidance on how I could think and where I should begin would be more than appreciated! (This is my first post on this site and also the first time i use TeX so might not look very good, hopefully you'll understand anyway!)","['probability-theory', 'stochastic-integrals', 'brownian-motion']"
247542,Projection is an open map,Let $X$ and $Y$ be (any) topological spaces. Show that the projection $\pi_1$ : $X\times Y\to X$ is an open map.,"['general-topology', 'open-map', 'product-space']"
247544,Why does this identity equal the number of primes?,"Can someone explain why this identity gives the number of primes?  I don't understand it. $D_{0,a}(n) = 1$ $D_{k,a}(n) = \displaystyle\sum_{j=1}^{k} \binom{k}{j}\sum_{m=a+1}^{\lfloor n^{\frac{1}{k}}\rfloor}D_{k-j,m}(\frac{n}{m^{j}})$ $\pi(n) = \displaystyle\sum_{j=1}^{\log_2 n}j^{-1}\mu(j)\sum_{k=1}^{\log_2 n^\frac{1}{j}}{-1}^{k+1} k^{-1}D_{k,1}(n^{\frac{1}{j}})$ $\pi(n)$ is the number of primes function here.  I tried translating all this into Mathematica : Dx[0,a_,n_]:=1  
Dx[k_,a_,n_]:=Sum[Binomial[k,j] Dx[k-j,m,n/m^j],{m,a+1,Floor[n^k^-1]},{j,1,k}]  
NumberOfPrimes[n_]:=Sum[j^-1 MoebiusMu[j](-1)^(k+1) k^-1 
    Dx[k,1,n^j^-1],{j,1,Log[2,n]},{k,1,Log[2,n^j^-1]}] and sure enough NumberOfPrimes[n] is giving me exactly what PrimePi[n] does, so it seems like it works.  But I don't understand why. The place I found this said it was $O(n)$ time and constant space, but it only mentioned the identity in passing and didn't give a derivation.","['prime-numbers', 'number-theory']"
247556,How to find the critical points of a polynomial? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I need help finding the critical points of the function $x^5+x^4-2x^2$, I don't understand, can someone help show me how to find the critical points please","['calculus', 'derivatives']"
247566,Proving that a composite number $n$ has a factor $k \leq \sqrt{n}$,Prove that a composite number $n$ has a factor $k \leq \sqrt{n}$. Do we prove by proof by contrapositive? Prove that a prime number $n$ has no factor $k \leq \sqrt{n}$. Any tip?,"['elementary-number-theory', 'discrete-mathematics']"
247569,Expected number of trials to see x unique values out of N total values,"I'm new to these forums so please forgive me if my question is poorly worded/phrased. Suppose I have a list of N unique integers that I'm drawing from, one at a time, with replacement. Let x be the number of non-repeated integers I've drawn thus far (or, put another way, the number of trials thus far whose outcome was distinct from every trial before it). Finally, let n be the amount of non-repeated integers I'm seeking, n being (obviously) between [0, N ]. How many trials will it take, on average, before x=n ? For example, take a deck of cards. If I'm drawing with replacement how many trials will it take until I've seen 26 different cards.",['probability']
247577,Problem about convex quadrilateral,"Given $a, b​​, c, d>0$. ¿What is the necessary and sufficient condition so that it can form a convex quadrilateral with sides $a, b​​, c, d$?",['geometry']
247581,expectation of a process of a multidimensional brownian motion,"Let $B(t)=(B_{1}(t),B_{2}(t),B_{3}(t))$ be a standard three dimensional Brownian motion
(i.e. it has independent components and starts at the origin). Now let $a=(a_{1},a_{2},a_{3})\neq(0,0,0)$ be a point in $\mathbb{R}^{3}$ and define $X(t)=\frac{1}{||a+B(t)||}$. Now compute the expected value of $X(t)$. Im kind of stuck on this one. I think I should use Girsanov theorem or some Ito calculus but im not getting anywhere. Any ideas?","['probability-theory', 'stochastic-integrals', 'brownian-motion']"
247586,Summing well ordered sets,"$A$ and $B$ are well-ordered sets. $A, B \subseteq \mathbb{R}$ $$
  C := \{ n+m : n \in A , m \in B \}
  $$ How do i now prove that $C$ is well ordered?
It seem logical to me, but I have to prove that ever $S \subset C$ has a minimum.",['elementary-set-theory']
247593,Quasiconformal map between the complex plane and a disk,"According to the Poincaré-Koebe theorem, it is known that the unit disk $\mathbb D$ and the complex plane $\mathbb C$ aren't conformally equivalent. My question is maybe naive, but I was wondering if this statement is still true in the quasiconformal sense.
More precisely, does there exist a quasiconformal map from $\mathbb D$ onto $\mathbb C$ ?","['riemannian-geometry', 'complex-analysis', 'quasiconformal-maps']"
247595,Construct a continuous monotone function $f$ on $\mathbb{R}$ that is not constant on any segment but $f'(x)=0$ a.e.,"This is one exercise from Rudin's book, Construct a continuous monotone function $f$ on $\mathbb{R}^1$ that is not constant on any segment but $f'(x)=0$ a.e.","['measure-theory', 'real-analysis']"
247600,Probability of 15 consecutive green lights,"Introduction Upon a trip home, my mother and I were noticing a very peculiar occurrence: Traffic lights were almost continuously green. Indeed, exactly fifteen different traffic lights were green consecutively. Now, I am bad at probability, but this seems unlikely. Probability Application I reasoned that since there are three different options for all fifteen traffic lights, the probability of fifteen traffic lights being consecutively green was one in $_{15}P_{3}$. This is because there is only one sequence of traffic light configurations where all of them are green and we must count all of the possible traffic light configurations using a permutation since a sequence such as G,R,Y is not the same as Y,R,G. With that said, the probability of the event comes out to be approximately $0.0366\%$. Question Is this application of probability correct, incorrect, or somewhere in the middle? To make this question very precise, 'somewhere in the middle' means that my application of probability makes many underlying assumptions and ignores many factors. I am not aware what these specific assumptions and factors may be (if they exist), but that's why I ask this question. In what sense am I correct, and in what sense am I incorrect?","['applications', 'probability']"
247609,Using a choice function to find an inverse for $F\colon A\to P(B)$,"Let $A$ and $B$ be arbitrary non-empty sets and let $F\colon A\to P(B)$, be an arbitrary function which covers $B$ in the sense that $\forall b \in B$, $\exists a \in A$ such that $b \in F(a)$ holds. Using axiom of choice show there exists a function $G: B \rightarrow A$ ,such that for all $b\in B$, $b \in F(G(b))$","['elementary-set-theory', 'axiom-of-choice']"
247610,hausdorff space and continuous function,"Here is the question I could not solve even though I have been thinking it since couple days. Let $f$ be a continuous function from $(X, \mathcal{T})$ to $(X, \mathcal{T})$. Show that if $X$ is a Hausdorff space, then
  $$\{x \in X : f(x)=x\}$$
  is closed. If $X$ is a $T_1$ space can we still have the same conclusion? If not, provide a counterexample.",['general-topology']
247619,"Prove that (M,+,*) is a field","Prove that the multiplication $*:M \times M \to M$ defined by this table: * | 0  1 -------- 0 | 0 0 1 | 0 1 together with the commutative group (M,+), is a field (M,+,*). Group axioms: 1) Closure: $0*0=0 \in M$ $0*1=0 \in M $ $ 1*0=0 \in M$ $ 1*1=1 \in M $ 2) Inverse element: $1*1=1  $ $ 1*1=1 $ $\forall i,a,e \in M : a*i=i*a=e$ here the inverse element is i=e=1. 3) Identity element: $ 0*1=0 $ $1*0=0$ $1*1=1$ $\forall e,a\in M: e*a=a*e=a $ with $e=1$ 4) Associativity: 4.1) $0*(0*0)=0 \leftrightarrow (0*0)*0=0$ 4.2) $0*(0*1)=0 \leftrightarrow (0*0)*1=0$ 4.3) $0*(1*0)=0 \leftrightarrow (0*1)*0=0$ 4.4) $0*(1*1)=0 \leftrightarrow (0*1)*1=0$ 4.5) $1*(0*0)=0 \leftrightarrow (1*0)*0=0$ 4.6) $1*(0*1)=0 \leftrightarrow (1*0)*1=0$ 4.7) $1*(1*0)=0 \leftrightarrow (1*1)*0=0$ 4.8) $1*(1*1)=1 \leftrightarrow (1*1)*1=1$ $\forall a,b,c\in M : a*(b*c)=(a*b)*c$ the addition $+:M \times M \to M$ defined by: $+ | 0$ $1$ ------------ $0 | 0$  $1$ $1 | 1$  $0$ Field condition: 1F) Commutativity: $0*0=0=0*0$ $ 1*0=0=0*1 $ $ 1*1=1=1*1 $ 2F) Distributivity: 2.1F) $0*(0+0)=0 \leftrightarrow (0*0)+(0*0)=0$ 2.2F) $0*(0+1)=0 \leftrightarrow (0*0)+(0*1)=0$ 2.3F) $0*(1+0)=0 \leftrightarrow (0*1)*(0*0)=0$ 2.4F) $0*(1+1)=0 \leftrightarrow (0*1)+(0*1)=0$ 2.5F) $1*(0+0)=0 \leftrightarrow (1*0)+(1*0)=0$ 2.6F) $1*(0+1)=1 \leftrightarrow (1*0)+(1*1)=1$ 2.7F) $1*(1+0)=1 \leftrightarrow (1*1)+(1*0)=1$ 2.8F) $1*(1+1)=0 \leftrightarrow (1*1)+(1*1)=0$ $\forall a,b,c\in M : a*(b+c)=(a*b)+(a*c)$ Could you please tell me if I have made a mistake? And what about 2) the inverse element is that correct?(because in the ""usual multiplication"" the inverse element i is $i=a^{-1}$ defined?!)","['axioms', 'analysis']"
247634,Derivatives vs Integration,"Given that the continuous function $f: \Bbb R \longrightarrow \Bbb R$ satisfies
  $$\int_0^\pi f(x) ~dx = \pi,$$
  Find the exact value of
  $$\int_0^{\pi^{1/6}} x^5 f(x^6) ~dx.$$ Let
  $$g(t) = \int_t^{2t} \frac{x^2 + 1}{x + 1} ~dx.$$
  Find $g'(t)$. For the first question: The way I understand this is that the area under $f(x)$ from $0$ to $\pi$ is $\pi$. Doesn't this mean that the function can be $f(x)=1$? Are there other functions that satisfy this definition? The second line in part one also confuses me, specifically the $x^6$ part! For the second question: Does this have to do something with the Second Fundamental Theory of Calculus? I see that there are two variables, $x$ and $t$, that are involved in this equation.","['calculus', 'integration', 'derivatives']"
247640,"laplacian of a function, relation to the function","Suppose for some function $\Phi$ we have: $$
\nabla^2 \Phi(\mathbf{r})=\phi(\mathbf{r})
$$ where $\phi(\mathbf{r})$ is some well-behaved smooth function, which is finite everywhere. Does this mean that $\Phi(\mathbf{r})$ itself doesn't have any singularities? Could you please point me out any useful theorems?","['multivariable-calculus', 'vector-analysis']"
247645,Intersection of nested compact subspaces in non-Hausdorff space,Why is it that if a space is not Hausdorff and you take the intersection of nested compact subspaces the intersection could be empty? Could you give me an example?,"['general-topology', 'compactness']"
247658,Monotonicity and Derivatives,"It is quite easy to see that there are plenty of functions $f$ for which $$ \frac{d^n}{dx^n} f(x) \geq 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ For starters, it holds for any polynomial with positive or zero coefficients. The stronger condition, with a strict inequality $$(1) \;\;\;\frac{d^n}{dx^n} f(x) > 0 \;\;\;\forall n \in \mathbb{N}, x \in \mathbb{R}_+$$ holds in less cases. However, sums of exponentials of the form $$ \;\;\; f(x) = \sum_i A_i e^{B_i x} \;\;\; A_i, B_i > 0$$ satisfy this stronger condition. But there are others My questions are: 1) Does (1) imply $O(f(x)) \geq O(\exp(x))$, (in the sense $O(a(x)) >,=,< O(b(x))$ if $\lim_{x\rightarrow\infty} \frac{a(x)}{b(x)} = \infty,\text{constant},0$, respectively) 2) Is there a simple test for (1), and/or, does this condition have an established name?","['real-analysis', 'limits']"
247689,How do you solve the following a - b(mod5) = a + b(mod5)?,attempted solution: a - b(mod5) = a + b(mod5) 2a(mod5) = 0 a = 5 5 + b mod 5 = 5 - b mod 5 b mod 5 = -b mod 5 b = 0,"['modular-arithmetic', 'discrete-mathematics']"
247695,"Behaviour of $r'=r-r^3 , \theta'=(\sin\theta)^2+a$",What are the local and global behavior of solutions of $r'=r-r^3$ $\theta'=(\sin\theta)^2+a$ at the bifurcation value $a=-1$?,['ordinary-differential-equations']
247696,Does $\mathbb R^2$ contain more numbers than $\mathbb R^1$? [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$ Do the real numbers and the complex numbers have the same cardinality? Does $\mathbb R^2$ contain more numbers than $\mathbb R^1$? I know that there are the same number of even integers as integers, but those are both countable sets. Does the same type of argument apply to uncountable sets? If there exists a 1-1 mapping from $\mathbb R^2$ to $\mathbb R^1$, would that mean that 2 real-valued parameters could be encoded as a single real-valued parameter?","['cardinals', 'elementary-set-theory']"
247699,Is a morphism of schemes which is proper at every fiber proper?,"Let $f\colon X \rightarrow Y$ be a separated morphism of finite type of schemes.
Suppose $f^{-1}(y)$ is proper over $Spec(k(y))$ for every $y \in Y$, where $k(y)$ is the residue field of $y$.
Is $f$ proper?
If not, what conditions (in addition to the above one) should $Y$ (or $X$) satisfy to make $f$ proper?",['algebraic-geometry']
247710,Notation for repeated application of function,"If I have the function $f(x)$ and I want to apply it $n$ times, what is the notation to use? For example, would $f(f(x))$ be $f_2(x)$, $f^2(x)$, or anything less cumbersome than $f(f(x))$? This is important especially since I am trying to couple this with a limit toward infinity.","['notation', 'functions']"
247733,What is the meaning of Chebychev's result and why is PNT stronger?,"I saw the proof by Chebychev that there are constants $c_1,c_2$: $$c_1 \frac{x}{\log(x)} < \pi(x) < c_2 \frac{x}{\log(x)}$$ and the Prime Number Theorem states $$\lim_{x \to \infty} \frac{\pi(x)}{x/\log(x)} = 1$$ I don't understand what exactly PNT is telling us, and why is it stronger than Chebychev ? My understanding of Chebychev is that $\pi(x)$ is always between two different constant multiples of the same function ($\frac{x}{\log(x)}$) so it must have the same asymptotic growth as them (I was thinking for example, there's no constants $c_1$, $c_2$ such that $c_1 x^3 < e^x < c_2 x^3$ because they have different asymptotic growth). I don't really undestand what PNT is telling us though, at first I thought it might be saying that $\pi(x)$ converges towards the cuve $y = \frac{x}{\log(x)}$ but I saw this graph (with $\pi(x)$ in red/blue and $x/\log(x)$ in green) so then I thought maybe there's some constant $C$ such that $\pi(x)$ converges towards $x/\log(x) + c$ but this might be wrong too.. I will be very grateful to understand the true meaning of PNT. Thank you.","['prime-numbers', 'number-theory']"
247748,Find a convex combination of scalars given a point within them.,"I've been banging my head on this one all day! I'm going to do my best to explain the problem, but bear with me. Given a set of numbers $S = \{X_1, X_2, \dots, X_n\}$ and a scalar $T$, where it is guaranteed that there is at least one member of $S$ that is less than $T$ and at least one member that's greater than $T$, I'm looking for an algorithm to create a Convex Combination of these scalars that equals $T$. For example, for the set $\{2,4\}$ and the scalar $3$, the answer is: $$.5 \cdot 2 + .5 \cdot 4 = 3.$$ I believe in many cases there are infinite infinitely many solutions. I'm looking for a generalized algorithm/formula to find these coefficients. Additionally, I would like for the coefficient weights to be distributed as evenly as possible (of course while still adding up to 1.)
For instance, for the set $\{1,2,4\}$ and the scalar $3$, a technically valid solution would be the same as the first example but with the coefficient for $1$ assigned a weight of 0 - but it would be prefferable to assign a non-zero weight.
I may not be thinking through this last part very clearly :)","['convex-analysis', 'linear-algebra', 'linear-programming', 'algorithms']"
247767,Finding an idempotent that satisfies certain conditions in a matrix ring.,"I've been stuck on a problem, and I was wondering if anyone could help me out. The problem is: Let $R$ be the $2 \times 2$ matrix ring over the reals $\mathbb{R}$ of the form
  $$ \begin{bmatrix}a & b \\0 & c\end{bmatrix}, $$
  where $a, b, c \in \mathbb{R}$. Find an idempotent $e$ in $R$ such that $eRe$ is a field, but the right ideal $eR$ is not minimal. I was thinking of using $e=\begin{bmatrix}0 & 1 \\0 & 1\end{bmatrix}$, which is idempotent.
I also showed $eRe$ is a field, but I'm not sure how to show the right ideal $eR$ is not minimal. If this $e$ doesn't work, I also tried $e=\begin{bmatrix}1 &0 \\0 & 0\end{bmatrix}$, but once again, I'm not sure how to show $eR$ is not minimal. Any help would be greatly appreciated. Thanks!","['matrices', 'ring-theory', 'ideals', 'abstract-algebra']"
247777,Why is this the answer?,"A fair coin is tossed $n$ times by Adam and $n$ times by Andrew. What is the probability that they get the same number of heads? Now since there are a total of $2n$ flips, $n$ from each person, we would need to choose $k$ flips that they both have heads correct? So since this is a binomial$\sim (2n,\frac12)$ I came to find that we have $${2n \choose k }\left(\frac12\right)^{2n}$$ However my textbook says it differently. I'm wondering why the book says it is:  $${2n \choose n }\left(\frac12\right)^{2n}$$ Is it the same thing? I'm just confused as to why it would say choose $n$. Should I be assuming assume that half of the total flips are are going to be the same?","['probability-theory', 'probability']"
247795,What is the covariance of mixture Bernoulli distribution?,"For mixture of multivariate Bernoulli distribution we have that, $$p(x|\mu,\pi) =\sum_{k=1}^{K}\pi_kp(x|\mu_k)$$ where $$p(x|\mu_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}$$ I read it from the book that $$E[x] = \sum_{i=1}^{K}\pi_k\mu_k$$ $$\operatorname{cov}[x] = \sum_{k=1}^{K}\pi_k(\Sigma_k+\mu_k\mu_k^T) - E[x]E[x]^T$$ The mean is trivial to prove, however I can't find proof for the covariance and I don't know how to prove it. Can anyone help?","['linear-algebra', 'probability-distributions', 'probability', 'covariance']"
247799,Prove the set of which sin(nx) converges has Lebesgue measure zero (from Baby Rudin Chapter 11),"I am trying to work through Rudin. This is a question from chapter 11: Suppose that $\{n_k\}$ is an increasing sequence of positive integers and $E$ is the set of all $x$ in $(-\pi,\pi)$ at which $\sin(n_kx)$ converges. Prove that $E$ has Lebesgue measure zero. Hint: For every subset $A$ of $E$, $\int_A \sin (n_kx) dx$ tends to zero, and $2\int_{A} \sin^2 (n_k x)dx$ tends to the measure of $A$. So, if I can use the hint, I'm pretty sure I can get the question. The thing is, I have no idea how to prove the hint. Using the hint, here's how I think you do the question: Define $f$ on $E$ as 
\begin{align*}f(x)= \lim_{k\rightarrow \infty} \sin(n_kx) \end{align*}
Notice that $\sin(n_kx)\leq 1$, and $1\in L$ on $A\subset E$ for every measurable $A$ (since $E$ has finite measure), so Theorem $11.32$ in Rudin (the Lebesgue dominated convergence theorem) says
\begin{align*} \int_A f(x) dx = \int_A \lim_{k\rightarrow \infty} \sin(n_kx) =  \lim_{k\rightarrow \infty} \int_A \sin(n_kx) = 0 \end{align*}
(by the first hint). But this was true for all $A\subseteq E$, so by one of the questions on the last assignment, $f(x)=0$ almost everywhere on $E$ and so $f^2(x)=0$ almost everywhere on $E$. Using Theorem $11.32$ again, we get
\begin{align*} \mu(A) = \lim_{k \rightarrow \infty} \int_A 2\sin^2(n_kx) = 2\int_A \lim_{k\rightarrow \infty}  \sin^2(n_kx)= 2 \int_A f^2(x)=0 \end{align*}
Therefore $\mu(E)=0$. Does anybody know how to prove the hint? Thanks!","['measure-theory', 'analysis']"
247844,Is following system of nonlinear ODEs recognized?,"The following system of ODEs – is it recognized as distinct system, with meaningful background and uses? $$\frac{dx}{dt} = - [x(t)]^2 -  x(t)y(t)$$
$$\frac{dy}{dt} = - [y(t)]^2 -  x(t)y(t)$$ This is probably not needed, but initial conditions: $x(t=0) = x_0, \space y(t=0) = y_0$","['ordinary-differential-equations', 'reference-request']"
247866,"Show that $\int_0^ \infty \frac{1}{1+x^n} dx= \frac{ \pi /n}{\sin(\pi /n)}$ , where $n$ is a positive integer.","Using residues, try the contour below with $R \rightarrow  \infty$ and  $$\lim_{R \rightarrow  \infty }  \int_0^R  \frac{1}{1+r^n}  dr \rightarrow \int_0^\infty  \frac{1}{1+x^n}  dx$$ I've attempted the residue summation, but my sum did not converge.","['residue-calculus', 'complex-analysis']"
247891,Show this function is convex.,"Could someone point me in the right direction for proving the following? Given that $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is an affine map given by $f(x)=A\mathbf{x}+\mathbf{b}$, $g:\mathbb{R}^m\rightarrow \mathbb{R}$ is convex, and that $g(A\mathbf{x}+\mathbf{b})$ is also convex, prove that $h(x)=\max \{\mathbf{a}_1^T\mathbf{x}+b_1,\mathbf{a}_2^T\mathbf{x}+b_2,\dots,\mathbf{a}_m^T\mathbf{x}+b_m\}$ is a convex function. I know that $\max\{x_1,x_2,...,x_m\}$ is a convex function, but I am not sure how to use it to prove this. I previously proved that the composition $g\circ f$ is convex.","['optimization', 'convex-analysis', 'real-analysis', 'analysis']"
247911,When does $V = \ker(f) \oplus \operatorname{im}(f)$?,"For a vector space $V$ and a linear operator $f : V \to V$, under what conditions does $V = \ker(f) \oplus \operatorname{im}(f)$? Is it always true, or only in special cases? Edit : $V$ is finite dimensional.",['linear-algebra']
247916,Composition Rule for Laplacian,Is there a easy composition Rule for Laplacian? Assume that $$u : \mathbb{R}^2 \rightarrow \mathbb{R}$$ $$I : \mathbb{R}^2 \rightarrow \mathbb{R}^2$$ so what will be $$Δu(I(x))=...$$,['multivariable-calculus']
247919,Pigeon Hole Problem,"Prove that of any 100 different twelve digit numbers (first digit cannot be zero) there are two of them with the same first and fifth digit. I'm new to this principle and need some assistance. I've been trying to understand how to approach this problem, but no dice. I guess I understand that there are two ""pigeons"" in the same ""box"", but still need help going through this.","['pigeonhole-principle', 'discrete-mathematics']"
247925,Is there any connection between this matrices,"Matrices I discuss are all $N\times N$ hermitian matrices. Define two positive (semi)definite matrices $H_1$ and $H_2$. Define the following matrices 
\begin{align}
P_1&=H_1+(I+H_2)^{-1} \\
P_2&=(I+H_1)^{-1}+H_2
\end{align}
I was just curious if there are any connections between them. It can be from any perspective, eigenvalues, rank, eigenbasis, simultaneous diagonalization or any such concept. Even special cases are welcome, for instance, say they are rank-one matrices, Does it make any difference?","['matrices', 'linear-algebra']"
247948,"Is $[0,1]$ a union of family of disjoint closed intervals?","According to this question , $[0,1]$ cannot be written as union of countable disjoint closed sets, is the same true about (uncountable) family of disjoint closed intervals ?","['general-topology', 'real-analysis', 'analysis']"
247976,$\mathrm{Spec}(R)\!=\!\mathrm{Max}(R)\!\cup\!\{0\}$ $\Rightarrow$ $R$ is a PID,"Is the following true: If $R$ is a commutative unital ring with
  $\mathrm{Spec}(R)\!=\!\mathrm{Max}(R)\!\cup\!\{0\}$, then $R$ is a PID. If yes, how can one prove it? Since $0$ is a prime ideal, $R$ is a domain. Thus we must prove that every ideal is principal. I'm not sure if this link (first answer) helps.","['homological-algebra', 'commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
247984,What is the probability distribution on integers?,"Assume that in the group of integers $\mathbb{Z}$, I randomly choose two integers $a$ and $b$ and I would like to ask whether they generate the group $\mathbb{Z}$, and then, what is the probability for this event? However, to explain clearly ""probability meaning"", we need to know about probability distribution. Could any one give  me an exact definition for this then? Or here, we just reduce our consideration on each quotient group $\mathbb{Z}/n\mathbb{Z}$ for each $n$? Thanks in advance.","['probability-theory', 'group-theory', 'probability-distributions', 'probability']"
247999,What is Gamma density's differential equation?,What is Gamma density's differential equation?,"['ordinary-differential-equations', 'probability-distributions']"
248012,Lower Bound of countable order-types of linearly ordered sets,"On Page 44, Set theory, Jech(2006) (Show that) there are at least $\mathfrak{c}$ countable order-types of linearly ordered sets. [For every sequence $a = \{ a_n : n \in N\}$ of natural numbers consider the order-type $\tau_a=a_0+\xi+a_1+\xi+a_2+\dots$, where $\xi$ is the order type of the integers. Show that if $a \ne b$, then $\tau_a \ne \tau_b$. ($\mathfrak{c}$ is the cardinality of $R$) ] On Page 19, two sets have the same order type, if and only if they are isomorphic in the sense that there is a bijective function$f$, and both $f$ and $f^{-1}$ are order-preserving. So I speculate that the answer should equal the cardinality of the set of initial segments of the least uncountable ordinal, which is $\omega_1$. I must have misunderstood the problem somehow. Even more confusing to me is the hint. 
Allow me to follow the convention in Jech's textbook: $1+\omega=\omega \ne \omega+1$ $2·\omega=\omega \ne \omega·2 =\omega+\omega$ Since the order-type of integers is the sum of that of negative integers and that of natural number, which is $\omega·2$ . Then we have: $\tau_a = (a_0+\omega)+\omega+(a_1+\omega)+……=\omega^2$, which is a constant function, just contrary to injective function as shall be shown.","['ordinals', 'elementary-set-theory', 'order-theory']"
248024,"Drawing two cards from 52, what is the probability that the second card has a higher face value than the first?","So if I draw two cards from 52, what is the probability that the second card has a higher face value than the first? The values of the cards are Ace = 1, Two = 2,..., King = 13. I got as far as $$\frac{13 \choose 1}{52 \choose 2}$$.. i.e. choosing the first card from 13 cards. But I don't how I am supposed to take account of choosing the second card.","['probability', 'combinatorics']"
248028,Proving that $\sum_{|j| < n} (n-|j|) \exp(ij\lambda)= \frac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)}$,"I want to show that $\sum_{|j| < n} (n-|j|) \exp(ij\lambda)=  \dfrac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)}$. I know from Proving $\sum\limits_{k=0}^{n}\cos(kx)=\frac{1}{2}+\frac{\sin(\frac{2n+1}{2}x)}{2\sin(x/2)}$ \begin{equation}
(1)\sum_{j=1}^{n-1} cos(j\lambda) = -\frac 1 2 + \frac{\sin(\frac{2n-1} 2 \lambda)}{2\sin(\frac \lambda 2)}.
\end{equation} and from the Hint in the first answer in How to show that $\frac{1}{\tan(x/2)}=2 \sum_{j=1}^{\infty}\sin(jx)$ in Cesàro way/sense? $$
(2)\frac d {dx} \sum_{j=1}^{n-1} sin(j\lambda) =\frac d {d\lambda} \frac{\cos({\frac \lambda 2})-\cos(\frac{n+1}2 \lambda)}{2\sin(\frac\lambda 2)} = \frac{n\sin(\frac{n+1} 2 \lambda)\sin(\frac \lambda 2)+\cos(\frac{n\lambda}2) - 1}{4\sin^2(\frac \lambda 2)}.
$$ This is what i did so far:
\begin{align}
\sum_{|j| < n} (n-|j|) \exp(ij\lambda) &= n + 2\sum_{j=1}^{n-1}(n-j) cos(j\lambda) \\
&=n + 2n \sum_{j=1}^{n-1} cos(j\lambda) - \frac d {d\lambda}\sum_{j=1}^{n-1} sin(j\lambda)\\
&= \frac{n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2)}{\sin^2(\frac \lambda 2)}.
\end{align} If this and the proposition is true, it should be true that
$$
n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2) - \sin^2(\frac {n\lambda} 2) = 0.
$$ But for $n=2$, i get
$$
3\sin(\frac {3\lambda} 2) \sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 \cos(\lambda) - \sin^2 \lambda \ne 0.
$$
(checked with Wolframalpha) So there is probably something wrong with my calculations. But i checked everything twice and don't see my mistake. Do you see it? EDIT Trying to prove it as proposed in the second answer. Let $z:=\exp(i\lambda)$ and $p(z):=\sum_{j=0}^{n-1} z^j$.
First i want to show that $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$. It is
\begin{align}
\sum_{j=1}^{n-1} z^{j}&= p(z) - 1 = \frac{1-z^n}{1-z} - 1.\\
\sum_{j=1}^{n-1} z^{-j}&= p(z^{-1})-1 = \frac{1-z^{-n}}{1-z^{-1}} - 1.\\
\sum_{j=1}^{n-1} jz^{-j} &= i \frac d {d\lambda} p(z^{-1}) = \frac{-nz^{-n}+(n-1)z^{-n-1}+z^{-1}}{(1-z^{-1})^2}.\\
\sum_{j=1}^{n-1} jz^{j} &= -i \frac d {d\lambda} p(z) = \frac{-nz^{n}+(n-1)z^{n+1}+z}{(1-z)^2}.\\
\end{align}
So i have
\begin{align}
\sum_{|j|<n} (n-|j|) z^j = n + n(\sum_{j=1}^{n-1} z^{j} + \sum_{j=1}^{n-1} z^{-j}) - \sum_{j=1}^{n-1} jz^{j} - \sum_{j=1}^{n-1} jz^{-j}.
\end{align}
If i now put in the values from above and substract $p(z)p(z^{-1})$ i should get $0$. But Wolfram Alpha doesn't agree, so i am again on the wrong track. What did i wrong? The last step $p(z)p(z^{-1})= \frac{(z^{n/2}-z^{-n/2})^2}{(z^{1/2}-z^{-1/2})^2}$ was easy to show. EDIT2 I did it now.
It was a lot easier to show directly $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$ without the geometric series partial sums values.","['complex-analysis', 'analysis']"
248032,"Is ""product"" of Borel sigma algebras the Borel sigma algebra of the ""product"" of underlying topologies?","A Borel sigma algebra is the smallest sigma algebra generated by a topology. The ""product"" of a family of Borel sigma algebras is to first take the Cartesian of the Borel sigma algebras, and then generate the smallest sigma algebra. Similarly, the ""product"" of a family of topologies is to first take the Cartesian of the topologies, and then generate the smallest topology. Is the ""product"" of some Borel sigma algebras the Borel sigma algebra for the ""product"" of their underlying topologies? Thanks!","['general-topology', 'measure-theory', 'borel-sets', 'product-space']"
248033,set of all points where $|f(x)|>\epsilon$ is finite,"$f:\mathbb{R}\rightarrow \mathbb{R}$ is function such that $\forall\epsilon>0$, the set $\{x:|f(x)|>\epsilon\}$ is finite. We need to show $\{x:f(x)=0\}$ is uncountable. Could any one give me hints? Thank you","['elementary-set-theory', 'real-analysis']"
248045,Losing at Spider Solitaire,"Spider Solitaire has the property that sometimes none of the cards in the final deal can ""go"" and so you lose, regardless of how much progress you have made beforehand. You would have known that you would lose had you seen the final ten cards before the game started. I wonder if we can calculate the probability of this happening. To be clear, I want to find the probability that the final ten cards out of two packs of well-shuffled cards comprise cards no two of which are exactly one away from each other numerically (only the values matter, not the suits). Note: there are several variants of Spider solitaire. I'm primarily interested in the standard 104-card, four-suit game.","['card-games', 'inclusion-exclusion', 'probability', 'combinatorics']"
248063,"Functions of bounded variation as the dual of $C([a,b])$","I am trying to understand this proposition about the dual of $C([a,b])$. I would like some help with the following: (1) What does the integral with respect to a function of bounded variation mean? (2) According to Riesz Representation Theorem, the dual of $C([a,b])$ is the space of regular Borel measures (Radon measures) on $[a,b]$. So what is the relation between these measures and the bounded variation functions? It would be nice to get some explanation in terms of distributions.","['bounded-variation', 'functional-analysis']"
248077,Order topology and partial order,"Let $(X,\leq)$ be an ordered space and define on $X$ the order topology form the subbasis $\{(x,y), (x,\to), (\leftarrow,x) : x,y \in X \}$. Can you read the fact that $\leq$ is a total order on the topology of $X$? I hoped it was equivalent to a separation property of $X$ (for example, if $\leq$ is total then $X$ is Hausdorff, but there exist Hausdorff partial ordered spaces; a more difficult result is that a linearly ordered space is completely normal, and here I didn't find a counter-example). Do you know something about that?","['general-topology', 'order-theory']"
248085,Symmetry in ordinary differential equations,"Suppose I am  given an ode $${dy\over dx}={1\over x^2}f(xy)$$ where $f$ is some arbitrary function. How then does doing the following help solve the equation? : First I have a vector field $$V=x\partial_x-y\partial_y$$ I see that this corresponds to the transformation $$(x,y)\to (x\exp(p), y\exp(p))$$ where $p$ is a parameter. Then I sought invariant coordinates $q_1,q_2$ of the vector field such that $$V(q_1)=1, V(q_2)=0$$ In this case I could take $q_1=xy$ and $q_2=\log|x|$. Though there might be other choices that are more suitable? Please help!","['vector-spaces', 'ordinary-differential-equations']"
248089,There are $n$ horses. At a time only $k$ horse can run in the single race. How many minimum races are required to find the top $m$ fastest horses?,There are $n$ horses. At a time only $k$ horses can run in the single race. How many minimum races are required to find the top $m$ fastest horses? Please explain your answer. PS: There is no timer.,['combinatorics']
248105,$(a_{2n})$ and $(a_{2n+1})$ converges then $(a_n)$ converges,"Whe had the following theorem in class: If $(a_{2n})_{n\in\mathbb N}$ and $(a_{2n+1})_{n\in\mathbb N}$ are convergent sequences with the same limit $a$ , then the sequence $(a_{n})_{n\in\mathbb N}$ converges. Proof: $(a_{2n})_{n\in\mathbb N}$ converges, so $\forall\varepsilon>0\exists   N_1\in\mathbb N:\forall n\geq N_1: |a_{2n}-a|<\varepsilon$ It follows $|a_m-a|<\varepsilon$ for all even $m\geq2N_1$ . $(a_{2n+1})_{n\in\mathbb N}$ converges, so $\forall\varepsilon>0\exists  N_2\in\mathbb N:\forall n\geq N_2: |a_{2n+1}-a|<\varepsilon$ It follows $|a_m-a|<\varepsilon$ for all odd $m\geq2N_2+1$ . Thus for all $m\in\mathbb N$ with $m\geq M:=\max\{2N_1,2N_2+1\}$ is $|a_m-a|<\varepsilon$ $\;\;\;\;\;\;\;\;\;\;\;$ q.e.d. Now, after trying to proove it myself again, I've just wondered myself about one thing: Why can't you choose $M=\max\{N_1,N_2\}$ and leave the second line in each number out? Why do you have to choose $2N_1$ and $2N_2+1$ ? Thanks for helping!","['sequences-and-series', 'real-analysis', 'analysis']"
248108,How do Flatlanders represent Möbius strips?,"There are 3D representations of Klein bottles that give people in our 3D universe a pretty good idea of how one is constructed: We can sort of see how this thing needs to be 'twisted' in the fourth dimension. But how do Flatlanders create 2D representations of Möbius strips?  The only thing I can imagine them representing a Möbius strip as is a line segment. Or can they somehow represent that it's twisted in the third dimension? And if not, why not?  That is, why is there not an analogue in Flatland of our representations of Klein bottles in the 3D world?","['general-topology', 'mobius-band', 'klein-bottle']"
248116,arithmetic mean of a sequence converges,"We had a theorem that the means of a sequence also converges: Let $(a_n)_{n\in\mathbb N}$ be a convergent sequence. Then $\displaystyle \overline a_n=\sum_{k=1}^n \frac{a_k}n$ also converges. I've tried to prove it: $|\overline a_n-a|=\frac1n|\sum_{k=1}^n(a_k-a)|\leq\sum_{k=1}^{M-1}|a_k-a|+\sum_{k=M}^n|a_k-a|$ The second sum is $<\frac{\varepsilon}2$ , because there is an $M\in\mathbb N$ so that $(a_n)_{n\in\mathbb N}$ converges. Now you can consider all $n\geq\max\{M,\frac2{\varepsilon}\sum_{k=1}^{M-1}|a_k-a|\}$ and so $|\overline a_n-a|<\varepsilon$ . But can you also say that there is a $K\in\mathbb N$ such that $\frac1n\sum_{k=1}^{M-1}|a_k-a|<\frac{\varepsilon}2$ for all $n\geq K$ ? And do I have to take the first sum from $k=1$ to $M$ or can you do it as above? Thanks for helping.","['sequences-and-series', 'real-analysis', 'analysis']"
248134,Hom between 2 schemes,"Why is the set $Hom(X,Y)$ between 2 schemes $X$ and $Y$ a scheme as well? Where can I read the construction? For example, $Hom(\mathbb{A}^1,\mathbb{A}^1)$ is the set of all polynomial, and what is the structure of scheme?","['algebraic-geometry', 'schemes']"
248196,Line integral and integration of differential forms,"The definition of integral of a $k$-form $\omega$ over a parametrized manifold $ Y_\alpha $ is $ \int_{Y_\alpha}\omega = \int_A\alpha^*\omega$ where $ \alpha\colon A \to R^n $. Let $ \gamma:(a, b) \to R^3$.  $ C_\gamma $ is a curve in $ R^3 $ and the line integral over that curve is written as $ \int_{C_\gamma}Pdx + Qdy + Rdz$ where $P$, $Q$ and $R$ are component function of $\gamma $. By applying the definition of integration of a $k$-form over a manifold we should get the formula 
$$ \int_a^bP(\gamma(t)) \frac{d\gamma_1}{dt} + Q(\gamma(t)) \frac{d\gamma_2}{dt} + R(\gamma(t)) \frac{d\gamma_3}{dt}dt.$$ However, I seem to be missing something. Here's my computation:
$$ \int_{C_\gamma}Pdx + Qdy + Rdz = \int_a^bP(\gamma(t))dx(\gamma(t))(\alpha_*(t; v)) + ... \\= \int_a^bP(\gamma(t))dx(\gamma(t))(\gamma(t);D\gamma(t) v) + ... $$ where $v$ is some vector, in this case just a scalar, since it has only one component. Obviously, the disappearance of $v$ would give the expected result, but I can't see why it should disappear.
I know I'm missing something quite simple, but could someone point that out for me, cause I'm feeling quite helpless right now... For reference: I'm using the book ""Analysis on manifolds"" by Munkres.","['differential-forms', 'manifolds', 'integration', 'analysis']"
248203,What is the derivative of a summation with respect to its upper limit?,"For the moment, consider the corresponding problem involving integration. Let $s(x)$ be the explicit solution to the following integral. $
\displaystyle s(x)=\int_a^x f(t) \, dt
$ The function $s'(x)$ is equivalent to the derivative of the integral with respect to it's upper limit and may be expressed in integral form. $
\displaystyle s'(x)=\partial _x\left(\int_a^x f(t) \, dt\right)=f(a)+\int_a^x f'(t) \, dt
$ Now let $s(x)$ be the explicit solution to the following summation. $
\displaystyle s(x)=\sum _{t=a}^x f(t)
$ The function $s'(x)$ is equivalent to the derivative of the summation with respect to it's upper limit. What is the derivative of $s(x)$ expressed in summation form? $
\displaystyle s'(x)=\partial _x\left(\sum _{t=a}^x f(t)\right)=\ ?
$","['asymptotics', 'summation', 'calculus']"
248209,Surfaces of revolution - Problem,"Here is the problem: Let $S$ $\subset$ $\mathbb{R^{3}}$ be a regular surface with the property that all normal lines meet the z-axis. Prove that S is contained in a revolution surface around z. I first tried to prove that every curve contained in this surface with a fixed z-coordinate is a circle (by showing that all normal lines to this curve meet at a point, it's a previous exercise), but the normal vector to the surface and to the curve are not the same, and I didn't go further... can you help me?","['surfaces', 'differential-geometry']"
248239,How do I apply an epsilon delta proof to the following problem?,"Any help in solving the following problem would be greatly appreciated: Let $f, g_1, g_2$ be functions from $\mathbb R$ to $\mathbb R$, with
  $g_1(x) \leq f(x) \leq  g_2(x)$, for all $x \in \mathbb R$. Suppose
  that, for some $p \in \mathbb R$, we have $\lim_{x \rightarrow p}
g_1(x) = \lim_{x \rightarrow p} g_2(x) = c$. Show that $ \lim_{x\rightarrow p} f(x) = c,$
   as well. I've spent hours on it and haven't come up with anything worth posting.","['continuity', 'limits']"
248245,Exactly half of the elements of $\mathcal{P}(A)$ are odd-sized,"Let $A$ be a non-empty set and $n$ be the number of elements in $A$, i.e. $n:=|A|$. I know that the number of elements of the power set of $A$ is $2^n$, i.e.  $|\mathcal{P}(A)|=2^n$. I came across the fact that exactly half of the elements of $\mathcal{P}(A)$ contain an odd number of elements, and half of them an even number of elements. Can someone prove this? Or hint at a proof?","['elementary-set-theory', 'combinatorics']"
248266,Limit $\left(\frac{1}{n!}\right)^{1/n}$ [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: $ \lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite I want to prove the following limit: $$\lim_{n \to \infty}  \left(\frac{1}{n!}\right)^{1/n} = 0$$
 Rewriting into exponentials, this becomes $\displaystyle \exp\left(\frac{1}{n}\log\frac{1}{n!}\right) = \exp\left(-\frac{\log{n!}}{n}\right)$, so it suffices to prove that
$$\lim_{n \to \infty} \frac{\log{n!}}{n} = \infty$$
How can I proceed to show this is true?","['factorial', 'radicals', 'limits']"
248267,Prove that Christoffel symbols transformation law via the metric tensor,"It is known that the transformation rule when you change coordinate frames of the Christoffel symbol is: $$ \tilde \Gamma^{\mu}_{\nu\kappa} = {\partial \tilde x^\mu \over \partial x^\alpha} \left [ \Gamma^\alpha_{\beta \gamma}{\partial x^\beta \over \partial \tilde x^\nu}{\partial x^\gamma \over \partial \tilde x^\kappa} + {\partial ^2 x^\alpha \over \partial \tilde x^\nu \partial \tilde x^\kappa} \right ]$$ Is there any way to prove this rule using only the definition of the Christoffel via the metric tensor? That is, using: $$ \Gamma^\mu _{\nu\kappa} = \frac{1}{2}g^{\mu\lambda}\left(g_{\lambda\kappa,\nu}+g_{\nu\lambda,\kappa}-g_{\nu\kappa,\lambda} \right)$$ All proofs have I've seen of the transformation law involve another method.","['tensors', 'riemannian-geometry', 'connections', 'differential-geometry']"
248269,How is the derivative of the CDF of a random variable $X$ its PDF?,"For a continuos random variable $X$ , if we have its p.d.f. $f(x)$ , then the cumulative densitity function (c.d.f.) of $X$ , $F(x)$ , is $$F(x) = \int_{-\infty}^{x}f(t)dt$$ We also have $$F'(x) = \frac{d}{dx}F(x)= f(x)$$ Why does the last step in the following equation yields $f(x)$ ? $$\frac{d}{dx}F(x) = \frac{d}{dx} \int_{-\infty}^{x}f(t)dt = \int_{-\infty}^{x} \frac{\partial}{\partial x} f(t)dt $$","['calculus', 'random-variables', 'probability-theory', 'probability-distributions', 'probability']"
248292,"How to prove that $E(X\mid\sigma\,(\mathcal{G},\mathcal{M}))=E(X\mid\mathcal{M})$ if $\mathcal{G}$ is independent of $X$ and $\mathcal{M}$?","I want to prove the following: Let $X\in L^1(\Omega,\mathcal{F},\mathbb{P})$ be a random variable and suppose $\mathcal{G,M}\subset\mathcal{F}$ are sub–$\sigma$–algebras such that $\mathcal{G}$ is independent of $X$ and $\mathcal{M}$. Prove that $$\mathbb{E}(X\mid\sigma\,(\mathcal{G},\mathcal{M}))=\mathbb{E}(X\mid\mathcal{M}).$$ Following hints from my textbook, I have proved that for all $G\in\mathcal{G}$ and $M\in\mathcal{M}$ $$\int_{G\cap M}\mathbb{E}(X\mid\mathcal{M})\,\text{d}\mathbb{P}=\int_{G\cap M}X\,\text{d}\mathbb{P}.$$
However, there's number of issues I do not understand and I hope you could clarify them to me. I know that $\mathbb{E}(X\mid\sigma\,(\mathcal{G},\mathcal{M}))$ is the only r.v. $\eta$ such that (i) $\eta$ is $\sigma\,(\mathcal{G},\mathcal{M})$–measurable, (ii) $\forall_{A\in\sigma\,(\mathcal{G},\mathcal{M})} \int_A\eta=\int_AX.$ By showing that RHS of the equality in the problem satisfies (i) and (ii), I'm done. However, my questions are: $1)$ Why is $\mathbb{E}(X\mid\mathcal{M})$ a $\sigma\,(\mathcal{G},\mathcal{M})$–measurable r.v.? $2)$ So far, I proved (ii) for $A$'s of the form $A_1\cap A_2$ where $A_1\in\mathcal{G}$, $A_2\in\mathcal{M}$. Does the claim follows from that? If so, why? Thanks for help! Edit: Ad question 2): I've just found that it is sufficient to verify (ii) for every set $A$ in some $\pi$–system that contains $\Omega$ and generates $\sigma\,(\mathcal{G},\mathcal{M})$. Is it true that $\{G\cap M\mid G\in\mathcal{G}, M\in\mathcal{M}\}$ is a generator of $\sigma\,(\mathcal{G},\mathcal{M})$? I will strongly appreciate any stories that may possibly improve my understanding of $\sigma$–algebras.",['probability-theory']
248304,estimation of transition probabilities from aggregate data,"Please, O mathematicians, help me understand the approach to the problem of estimating transition probabilities given only aggregate data in Kalbfleisch & Lawless' 1984 paper ""Least-Squares Estimation of Transition Probabilities from Aggregate Data"". (Or some other approach—it's the estimation that interests me, not that approach in particular.) Forgive the somewhat janky but hopefully comprehensible notation. Here's my particular problem. We have some number of people; each person $i$ is in a state $X_i(t)$ at time $t$ and can move at time $t+1$ into any state $0 \ldots X_i(t)+1$ inclusive. At time $t=0$ everyone is in state $0$. The individual state transitions are unobserved; all we observe are the counts of people in a given state $j$ at time $t$: $\mathbf{M}_{t,j} = \#\{i : X_i(t) = j\}$. Because everyone starts in state $0$ at time $0$, and because you can only move to at most state $k+1$ if you were formerly in state $k$, this generates an upper-triangular matrix of counts. We want to know the probabilities of the possible state transitions $\mathbf{p}_{k,j} = \Pr(X_i(t+1) = j \mid X_i(t) = k)$ given the counts in $\mathbf{M}$. Kalbfleisch & Lawless's paper seems to be about exactly this . (The paper is available here if you want to look at it.) However, I can't figure out how to apply their approach at all. (Doubtless because of my more general ignorance.) They give a couple of examples so I wanted to work through them and hit a wall pretty quickly. Some things in particular I am failing to understand: On p. 171 several equations are defined prefaced by the clause ""if there are no structural $0$'s, so that $r = k(k-1)$"" or ""if $r = k(k-1)$"". However, in the first example, there are structural zeroes and the answers given to the examples presume that (if you dont' take the structural zeroes into account the dimensions of several matrices will differ from what they are claimed to be in the text). There are also structural zeroes in my particular problem, so I would like to know if there's a straightforward way to proceed in that case. Also on p. 171 the matrices $\mathbf{B}_t$ are defined for $t = 1, \ldots, m$, but later equations involve the summation from $t = 1 \ldots m$ of terms involving $\mathbf{B}_{t-1}$, meaning that $\mathbf{B}_0$ must be defined. Perhaps more fundamentally I don't really understand why the last column of their matrix $\mathbf{P}$ is cut off—don't we want to know e.g. $p_{13}$ as well as $p_{12}$? I suspect I am missing some very basic things, but I know very little about the subject (I'm interested in it for a particular practical problem). If anyone can help me understand how to compute what I'm after, or the mechanism described in the paper, that would be immensely helpful. Thanks.","['matrices', 'markov-process', 'markov-chains', 'probability']"
248310,How to find the number of roots using Rouche theorem?,"Find the number of roots $f(z)=z^{10}+10z+9$ in $D(0,1)$. I want to find $g(z)$ s.t. $|f(z)-g(z)|<|g(z)|$, but I cannot. Any hint is appreciated.",['complex-analysis']
248312,$G/N$ is nilpotent $\Leftrightarrow$ $N$ contains the intersection of all subgroups in the lower central series.,"I want to solve the following exercise: Let $G$ be a group. $G^{0}\geq G^{1} \geq G^{2}\dots$ is a lower central series, s.t. $G^{0} = G$ and $G^{i+1} = [G,G^{i}]$ . Let $N$ be a normal subgroup of $G$ . a) Then $G/N$ nilpotent $\Rightarrow$ $\bigcap\limits_{i\geq 0}G^{i} \subset N$ . b) With $G$ finite, $\bigcap\limits_{i\geq 0}G^{i} \subset N$ $\Rightarrow$ $G/N$ nilpotent. So far I just have ideas to a): I found a Corrolary that says: $N$ normal, $G/N$ nilpotent and $N\leq Z_{i}(G)$ for some $i$ (where it notates the upper central series), then $G$ is nilpotent. I thought, if I could show that $N\leq Z_{i}(G)$ , then with $G$ nilpotent, the lower central series terminates and the intersection would be trivial (? is that right?) and so contained in $N$ . Is that the right way? Or should I better approach that exercise differently? Thanks and best,
Sara","['finite-groups', 'group-theory', 'abstract-algebra']"
248313,Convergence of finite differences to zero and polynomials,"Assume that $f:\mathbb R \rightarrow \mathbb R$ is continuous and $h\in \mathbb R$.  Let $\Delta_h^n f(x)$ be a finite difference of $f$ of order $n$, i.e $$
\Delta_h^1 f(x)=f(x+h)-f(x),
$$
$$
\Delta_h^2f(x)=\Delta_h^1f(x+h)-\Delta_h^1 f(x)=f(x+2h)-2f(x+h)+f(x),
$$
$$
\Delta_h^3 f(x)=\Delta_h^2f(x+h)-\Delta_h^2f(x)=f(x+3h)-3f(x+2h)+3f(x+h)-f(x),
$$
etc.
There is an explicite formula for $n$-th difference: 
$$
\Delta_h^n f(x)=\sum_{k=0}^n (-1)^{n-k}\frac{n!}{k!(n-k)!} f(x+kh).
$$ Assume now that $n\in \mathbb N$ and  $f:\mathbb R \rightarrow \mathbb R$ are such that for each $x \in \mathbb R$:
$$
\frac{\Delta_h^n f(x)}{h^n} \rightarrow 0 \textrm{ as } h \rightarrow 0.
$$
Is it then $f$ a polynomial of degree $\leq n-1$? It is clear if $n=1$, because then $f'(x)=0$ for $x\in \mathbb R$. Edit. Without continuity assumption about $f$ it is not true, because for $n-1$-additive function $F$ which is not $n-1$-linear we have $\Delta_h^nf(x)=0$, where $f(x)=F(x,...,x)$.",['analysis']
248363,A function whose value is either one or zero,"First I apologize in advance for I don't know math's English at all and I haven't done math in almost a decade. I'm looking for a function whose ""domain/ensemble of definition"" would be ℝ (or maybe ℤ) and whose ""ensemble/domain of variation"" would be ℕ{0, 1}
that would look like something this awesome ascii graph... ^f(x)
          |          
          |
1________ | __________
0________\./__________>x
         0|
          |
          |
          | f(x) is always 1 , but 0 when x = 0 Actually I need this for programming, I could always find other ways to do what I wanted with booleans so far but it would be much better in most cases to have a simple algebra formula to represent my needs instead. I just hope it's not too complicated a function to write/understand and that it exists of course. Thanks in advance guys.","['boolean-algebra', 'functions']"
248372,A function cannot be included in its own domain,For what set theoretic reasons can a function not be included in its own domain? Thanks,"['elementary-set-theory', 'functions']"
248377,Probability that z precedes both a and b in the permutation?,"What is the probability of the event that z precedes both a and b when we randomly select a permutation of the 26 lowercase letters of the English alphabet? Currently, my thoughts are: P(25,25)x24      There are p(25,25) ways to get certain letters of the alphabet 24 ways to place z, it can't be in the last 2 spots since it has to precede a and b .",['probability']
248390,What is the difference between a poisson process and a poisson point process?,"What is the difference between a poisson process and a poisson point process???? in terms of properties and definition ? If possible, please also provide an explanation in layman terms.","['probability-theory', 'stochastic-processes']"
248391,Dirac Operators on $S^1$,"I am trying to understand the Dirac operators associated to the 2 spinor bundles on $S^1.$ I have been getting very confused about why one bundle has nontrivial harmonic spinors and the other doesn't.(Harmonic spinors are solutions $s$ to the equation $Ds = 0$ where $D$ is the Dirac operator and $s$ is a section.) Here is my argument (which must be wrong somewhere). We have 2 spin structures given by the connected 2-fold covering and the disconnected 2-fold covering. Since the tangent bundle $TS^1$ is trivial, we can choose the trivial connection on it given by $f \rightarrow df.$ When considered as a connection on the principal bundle of frames (also isomorphic to $S^1$), i.e. as a Lie algebra valued one form on $S^1,$ it must be the zero form. (As a quick aside, the Lie algebra of $SO(1)$ is just the $0$ Lie algebra, so it seems like there is only one connection on the tangent bundle of $S^1$ since we could only ever have the $0$-form as the connection form on our frame bundle. But this is not true, we can add a 1-form to any connection and get another connection. How can this be?) (EDIT: Answer provided by Eric: because we have implicitly reduced the structure group of the frame bundle to $SO(1),$ an $so(1)$ valued one form corresponds to a connection compatible with the given metric, and there is only one of these since the torsion of any connection on $S^1$ is zero.) Ok, so now given either spin structure, the connection must lift to the $0$ connection. Furthermore, any complex line bundle over the circle is trivial, so both cases look exactly the same, and the Dirac operator appears to be $f \rightarrow i\frac{df}{dx}.$ However, I am told that in the case of the connected double cover we should have an additional condition on our $f,$ namely that it should satisfy $f(-x) = -f(x).$ Where have I gone wrong? (2nd EDIT) I think I know where my confusion stems from. Given a spin structure $P$ on a manifold $M^n$ we can identify sections of the spinor bundle with smooth maps $f: P \rightarrow \mathbf{C}^n$ such that $f(pg) = g \cdot f(p)$ as follows. Take any discontinuous section $s: M \rightarrow P.$ This gives a section $t:M \rightarrow P \times_{Spin(n)} \mathbf{C}^n$ of the spinor bundle by the formula $t(m) = [s(m), f \circ s(m)],$ and by the compatibility condition on $f$ this is independent of the choice of section $s.$ It is via this identification that I understand how sections of the spinor bundle for the connected double cover must be functions $f: S^1 \rightarrow \mathbf{C}$ such that $f(-x) = f(x).$ And, as luck would have it, in this case the Spin structure on $S^1$ is itself, as a space, $S^1,$ so the description of the Dirac operator translates easily via this identification. But in the case of $S^2,$ for example, sections can be identified with maps from $S^3$ to $\mathbf{C}^2$ but the local description of the Dirac operator I see in books (like Lawson/Michelson's Spin Geometry) tells me how to differentiate maps $U \subset S^2 \rightarrow \mathbf{C}^2$ in a trivializing nbhd $U.$ How do I translate the local description of the Dirac operator so that it tells me how to differentiate the map $S^3 \rightarrow \mathbf{C}^2$?","['riemannian-geometry', 'algebraic-topology', 'differential-geometry']"
248396,How do I prove a combinatorial statement about the change-making problem when using the greedy algorithm?,"Let $D$ be set of denominations and $m$ the largest element of $D$. We say that $c$ is a counterexample if the greedy algorithm gives an answer different from the optimal one. Now, apparently, if for a given set $D$, the greedy algorithm does not return an optimal solution, then the smallest $c$ will be smaller than $2m-1$. Is this really true? How can we prove it? If not, is there a relatively small range to look for the smallest counterexample?","['optimization', 'elementary-number-theory', 'algorithms', 'combinatorics']"
248410,Can we give a definition of the cotangent based on a functional equation?,I've recently learned that the cotangent satisfies the following functional equation: $$\dfrac1{f(z)}=f(z)-2f(2z)$$ (true for $f(z)\neq 0$). Can we solve this equation for real or complex functions $f?$ Can we give additional conditions such that $\cot$ is the only real or complex function satisfying these conditions and the equation? Or is there perhaps a different functional equation better suited for this purpose? I'm asking this because I know about such a characterization of the real function $\exp$. Please note that I know very little about functional equations. I've only seen two examples dealt with in my courses.,"['definition', 'functional-equations', 'trigonometry', 'real-analysis', 'complex-analysis']"
248411,"The inequality of the form $\mathbb{P}_x(\sigma=k)\leq (1-p)^k$, where $\sigma$ is a stopping time","Let $X$ be a metric space and $\Omega=X^{\mathbb{N}}$. Consider the sequence $(X_n)_{n\geq 0}$ given by $$X_n(\omega)=x_n\;\;\;(\omega=(x_0,x_1,...),\;n\in\mathbb{N_0}).$$
Let $P:X\times\mathcal{B}(X)\to\left[0,1\right]$ be a stochastic kernel. Then, for every $x\in X$ we can define a probability measure $\mathbb{P}_x$ on $\mathcal{B}(X^{\mathbb{N}})$ such that $(X_n)_{n\geq 0}$ is a homogeneus Markov chain on the probalility space $(X, \mathcal{B}(X^{\mathbb{N}}), \mathbb{P}_x)$ starting at the point $x$ and $P$ is its transition function. Let $A\in \mathcal{B}(X)$ and define the stopping time $\tau= \inf\{n\geq 1: X_n\in A\}$ and $$\tau(1)=\tau,\;\;\; \tau(k+1)=\tau(k) + 1_{\left\{\tau(k)<\infty\right\}}(\tau \circ T_{\tau(k)})\;\;\;(k\in\mathbb{N}),$$
where $T_n(x_1,x_2,...)=(x_n,x_{n+1},...)$. Now, let 
et $B\in \mathcal{B}(X^{\mathbb{N}})$ and assume that there is $p>0$ such that $\mathbb{P}_x(B)\geq p$, for $x\in A$. Define $$\widehat{\tau}=\inf\{n\geq 1: X_n\in A,\; (X_{k+n})_{k\geq 0}\in B\}.$$
Finally, define $$\sigma=\inf\{k\geq 1: \widehat{\tau}=\tau(k)\}.$$ My question is: How to prove that $\mathbb{P}_x(\sigma=k)\leq (1-p)^{k-1}$, for $k\in \mathbb{N}$? I have tried to use the Strong Markov Property. Is it necessary to assume that $\tau<\infty$ $\mathbb{P}_x$ a.e.? I will be grateful for any help.","['probability-theory', 'probability']"
248441,Lebesgue integrable function?,"If $\displaystyle f(x)=\frac{1}{x^p}$ $(0 < x \leq 1)$ then $f \in L[0,1]$ if $p<1$ and $$\int_{0}^1 f= \frac{1}{1+p} $$ I know that non negative measurable function f is Lebesgue integrable on [a,b] if $$\int_{a}^b f=\lim_{n \to \infty} \int_{a}^b f^n$$ 
If this limit is finite then function is Lebesgue integrable.
but how can i find $f^n$ for this function? please help me.Thanks in advance.","['lebesgue-integral', 'measure-theory', 'real-analysis']"
248449,How to find all Laurent series for $\frac{1}{z^2 - z}$ centered at $1$?,"I have two questions.  The definition I learned defined a laurents for a function analytic in some annalus?   I guess this question is asking annali centered at $1$?  How do I determine all the annali? Also, is there a quicker way to find coefficients $a_n$ of the laurent series without applying the definition directly (i.e $a_n = 1/2\pi i \int f(\zeta)/(\zeta - 1?)^{n+1} d\zeta$)",['complex-analysis']
248461,Is there any research in mathematical biology that isn't heavy in differential equations?,"I'm near the end of my pure math undergrad trying to decide what sort of math I'm interested in for graduate school. I've always thought the idea of mathematical biology was cool, but it seems like a lot of it is steeped in differential equations, which I'm really not too big on. I know there's some genetics research related to statistics, too, but I'm more into pure math. I guess the question is, is there any area of mathematical biology that doesn't rely heavily on differential equations or statistics (and if so, could you provide reference material)?","['ordinary-differential-equations', 'biology', 'soft-question']"
248462,Why this random variable is uniformly distributed?,"Here is an exercise. Suppose that people arrive at a bus stop in accordance with a Poisson
  process with rate $\lambda$. The bus departs at time $t$. Let $X$
  denote the total amount of waiting time of all those who get on the
  bus at time $t$. Let $N(t)$ denote the number of arrivals by time $t$. Now we want to calculate $E[X\;|\;N(t)]$. The solution says that the waiting time of each person, $T$, is uniformly distribute in $(0,t)$. So $E[X\;|\;N(t)=n]=n\int_0^t(t-s)\frac{1}{t}ds$. My question is, why $T$ is uniformly distribute? If people arrivals obeying Poisson process, shouldn't $T$ obeying exponential distribute with parameter $\lambda$?","['probability-theory', 'probability']"
248472,Expectation on 1/X,In general can one say that for a random variable X: $E[\frac{1}{X}] = \frac{1}{E[X]}$ ? I've worked out a few examples where this works but I'm not sure how widely this is useful...,['probability']
248473,A question about functions in $L^p(E)$,"I've been working on a problem. I found a couple of related questions on the site, but I was having a little bit of trouble clarifying everything. The goal is to show that given $f\notin L^p(E)$, there exists a $g\in L^{p'}(E)$ such that $fg\notin L^1(E)$. Note this is the second part of Wheeden and Zygmund's Introduction to Real Analysis textbook. To show that for $f\notin L^p(E)$, there exists a $g\in L^{p'}(E)$ such that $fg\notin L^1(E)$, consider the following. Without loss of generality, we can assume all functions are nonnegative. Now, suppose there is a sequence $\{g_k\}_{k=1}^\infty\subseteq L^{p'}(E)$ with $\Vert g_k\Vert_{p'}=1$ and
    $$
        \int_E fg_k>3^{k}.
    $$
    Set 
    $$
        g=\sum_{k=1}^\infty 2^{-k}g_k
    $$
    and observe that, by Minkowski's inequality, $\Vert g\Vert_{p'}\leq 1$. Note that
    $$
        \int_E fg=\int_E f\sum_{k=1}^\infty 2^{-k}g_k>\int_E\sum_{k=1}^\infty \left(\frac{3}{2}\right)^{k}=\infty.
    $$
    Thus, $g\in L^{p'}(E)$ but $fg\notin L^1(E)$.
    Hence, we have reduced the problem to showing that such a sequence exists. First note that if $f=\infty$ on any set $A$ of positive measure, then we can simply take $$g=\frac{1}{|B|}\text{ when }x\in B$$
where $B\subseteq A$ has positive, finite measure, and $g=0$ otherwise. Thus $g$ has the desired properties. This implies we can assume $f$ is finite a.e., and in particular, for any positive real number $c$, we can find a set $F$ with finite measure such that $\int_F f=c$. With this in mind, we can find a nested sequence of sets $\{E_k\}_{k=1}^\infty$, each with finite measure, such that $\bigcup_{k=1}^\infty E_k=E$ and $\int_{E_k}f>3^k$. Now, take
    $$g_k=\frac{1}{|E_k|^{\frac{1}{p'}}} :\ x\in E_k$$
    By construction, $\Vert g_k\Vert_{p'}=1$, and it is easy to check that $\int fg_k\longrightarrow\infty$ as $k\to\infty$. Hence, we have the existence of the sequence, and we've demonstrated the existence of such functions $g$.
Does this seem sufficient? The last part seems to be a little bit lacking, but I'm not quite sure why I feel it's unjustified. Thanks in advance!","['measure-theory', 'lp-spaces', 'real-analysis']"
248492,Statement not provable from ZF,"I'm doing the following exercise from Just/Weese: Some thoughts: To show that the statement is not provable from $ZF$ I could either show that it implies the axiom of choice or Tychonoff or I could assume that it is provable from $ZF$ and produce a contradiction. Since I'm give a statement that is known not to imply $AC$ and I'm told to use it, I will do so. The idea is that if the second statement in the exercise is provable from $ZF$ then the first statement implies Tychonoff's theorem which is the desired contradiction. Proof: Let (*)""For every indexed family of non-empty sets there is a family of compact Hausdorff topologies."" be provable from $ZF$. Assume that (**)""The product of compact Hausdorff families is compact."". Let $X_i$ be an arbitrary collection of sets compact spaces . Then by (*) we can turn them into a compact Hausdorff family and by (**) the product is compact. Hence Tychonoff's theorem holds which is equivalent to $AC$. But then (**) would be equivalent to $AC$.$\Box$ Some more thoughts: Given a single arbitrary set it is always possible to endow it with a compact Hausdorff topology: First put the discrete topology on it and then use one-point compactification to make it into a compact space. The reason why this cannot be applied to an infinite family of sets is that for each set one needs to choose a bijection between the set and the set union one point. And to do so one needs $AC$. Can you tell me if I have it right? Thank you.","['elementary-set-theory', 'axiom-of-choice']"
248506,What does $m\mathbb Z/n\mathbb Z$ look like?,"Someone asked me about what is really the quotient $m\mathbb Z/n\mathbb Z$ looks like when $m|n$. Unfortunately, I did some handy calculations for him to convince him, but it didn't work. May someone help me to have a solid explanation about this group? Thank you!",['group-theory']
248510,Probability density under new measure,"Let $P$ and $Q$ be two equivalent probability measures, and let $dP=ZdQ$ for some $Z$. If I know the probability density of a random variable $X$ under measure $P$, how can I calulate the density of $X$ under measure $Q$? In other words, if I know that
$$P(X\le x)=\int\limits_{-\infty}^x f(s)\, ds,$$
for all real $x$, is it possible to find a function $g$ such that
$$Q(X\le x)=\int\limits_{-\infty}^x g(s)\, ds.$$","['probability-theory', 'measure-theory']"
248513,Finding all groups with $\text{Aut}(G)=\{1\}$ [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: $|G|>2$ implies $G$ has non trivial automorphism I am doing this exercise: Find all groups $G$, with $\text{Aut}(G)=\{1\}$. What has been clear to me is the group $G$ should be abelian group. Because we will have $G=Z(G)$ and I see that at least all $\phi_g(x)=g^{-1}xg$ are just identity. Any help is appreciated!","['group-theory', 'abstract-algebra']"
248532,Why are vector spaces sometimes called linear spaces?,"I have never come across the term 'linear space' as a synonym for 'vector space' and it seems from the book I am using (Linear Algebra by Kostrikin and Manin) that the term linear space is more familiar to the authors as opposed to using vector space. This book was translated from the Russian edition into English so it seems that the term linear space is/was more predominant in the Russian speaking countries? So I was wondering what is the intuition/motivation behind choosing such a term for the concept of a vector space. 
  Why have the word 'linear space' for vector spaces? 
  What is so ""linear"" about vector spaces?
  Is it possible to have a ""non-linear"" vector space? 
  Why should we distinguish between ""linear"" and ""non-linear"" if such a term non-linear space exists? I know that I have not had enough linear algebra and exposure to higher mathematics to have a feel for why such a term is used for vector spaces and it would be great if someone could give an exposition.","['vector-spaces', 'definition', 'math-history', 'linear-algebra', 'terminology']"
248539,"Why is AC needed for $|\bigcup X_i|=|\bigcup Y_i|$, $\forall i$ $|X_i|=|Y_i|$, $\{X_i\}_{ i\in I}$, $\{Y_i\}_{i\in I}$ pairwise disjoint?","On Page 60, Set Theory, Jech(2006), 5.9 If $\{X_i : i \in I\}$ and $\{Y_i : i \in I\}$ are two disjoint families such that $|X_i| = |Y_i|$ for each $i \in I$, then $|\cup_{i \in I}X_i| = |\cup_{i \in I}Y_i|$ [Use AC ] Here's how far I goes: $|X_i| = |Y_i|$ implies there exists a bijective function $f_i: X_i \to Y_i$ for each $i \in I$ ex ante. Since $\{X_i : i \in I\}$ is a disjoint family, for each $x \in \cup_{i \in I}X_i$, there exists exactly one $i \in I$, such that $x \in X_i$. So we could define a bijective function $f:\cup_{i \in I}X_i \to \cup_{i \in I}Y_i$, by $f(x)=f_i(x)$, if  $x \in X_i$. I don't see any usefulness of AC in problem 5.9, as opposed to problem 5.10, in which $|\cup_{i \in I}X_i| = |\cup_{i \in I}Y_i|$ is replaced by $|\prod_{i \in I}X_i| = |\prod_{i \in I}Y_i|$. The reason is that without AC, the cardinality of a cartesan product of non-empty sets is arbitary.","['elementary-set-theory', 'axiom-of-choice']"
248547,Proving equivalences of statements equivalent to AC,"I'm doing the following exercise from Just/Weese: Show in ZF that (WO) implies (IC) and that (IC) implies (SC). where (WO) Every set can be well-ordered. (IC) For any two sets $X,Y$ either there is an injection $X \hookrightarrow Y$ or $Y \hookrightarrow X$. (SC) For any two sets $X,Y$ either there is an surjection $X \twoheadrightarrow Y$ or $Y \twoheadrightarrow X$. (WO) $\rightarrow$ (IC): Let $X,Y$ be two sets. Then by (WO) they can be well-ordered. Therefore each is in bijection with an ordinal $\alpha$ (and $\beta$, respectively): Claim: Every well-ordered set is isomorphic to an ordinal. Proof: Let $\langle, X,W \rangle$ be a well-ordered set. Let $\alpha$ be an ordinal with $|\alpha| \ge |X|$. Define an injective map $f: X \hookrightarrow \alpha$ as follows: (i) Let $x_0$ be the $W$-minimal element. Then $x_0 \mapsto \varnothing$. (ii) Assume $f$ has been defined for $x \in I_W (x')$. Define $x' \mapsto \sup^+ f(I_W (x'))$. Let $\tilde{f} = f: X \to \mathrm{im}f$. Then $\tilde{f}$ is a bijection and $\mathrm{im}f$ is an initial segment of an ordinal hence also an ordinal.$\Box$ Either $\alpha \in \beta$ or $\beta \in \alpha$. Hence either $X \hookrightarrow Y$ or $Y \hookrightarrow X$. (IC) $\rightarrow$  (SC): Let $X,Y$ be sets. Then either $X \hookrightarrow Y$ or $Y \hookrightarrow X$. Given $X \hookrightarrow Y$ it is easy to construct a surjection $Y \twoheadrightarrow X$, similarly for $Y \hookrightarrow X$. Can you tell me if these proofs are correct? Thanks. I also wanted to prove (IC) $\rightarrow$ (WO), but I'm stuck. I thought of something like if $X$ is a set and $\alpha$ is an ordinal then either $X \hookrightarrow \alpha$ or $\alpha \hookrightarrow X$ but the latter case seems to be a dead end.","['elementary-set-theory', 'axiom-of-choice']"
248554,Inverse spectral problem: How to recover the function $ q(x) $?,"The forward problem is a second order Sturm-Liouville operator $$ - \frac{d^{2}}{dx^{2}}y(x)+q(x)y(x)=zy(x) $$ with the boundary conditions $ y(0)=0=y(\infty) $. If I know the spectral measure function $ \sigma (x) =\sum_{\lambda_{n} \le x}1 $, then can I reconstruct the inverse of the potential $ q^{-1}(x) $? My question is, how do I use the Gelfand-Levitan-Marchenko theory to reconstruct the potential $ q(x) $?","['ordinary-differential-equations', 'inverse-problems', 'sturm-liouville']"
248567,Conditional Expectation: What happens if you take conditional expectation on trivial sigma field?,"Consider for the trvial $\sigma$ - field $\mathcal{F}_0 = \{\emptyset , \Omega\}$,
What is Conditional expectation of the following in the following cases when $A = \emptyset$ and $A = \Omega$ ??? ? Can someone please help me fill in the ?? below, as this would help improve my understanding a lot ? $$\int_? E[X | \mathcal{F}_0]1_A dP =  ?    \;\; \forall A \in \mathcal{F}_0$$ Question 2: And What if I just condition on the $\sigma$-field, $$ \int_? E[X | \mathcal{F}] dP = ? $$ For the second question, I guess it is = X right? Since X is already $\mathcal{F}$- measurable by definition of random variable, if given the $\sigma$ - feld $\mathcal{F}$, everything is known, there is no randomness in X.","['probability-theory', 'stochastic-processes', 'probability']"
248570,"Determine whether this series is absolutely convergent, conditionally convergent or divergent?","The series $ \sum_{n=1}^{\infty} \frac{(-1)^n n}{n^2 + 1} $; is it absolutely convergent, conditionally convergent or divergent? This question is meant to be worth quite a few marks so although I thought I had the answer using the comparison test, I think I'm supposed to incorporate the alternating series test.","['divergent-series', 'convergence-divergence', 'sequences-and-series', 'analysis']"
