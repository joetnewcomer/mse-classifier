question_id,title,body,tags
1127959,$ \sum_{n=1}^{\infty}a_{n} $ diverges but $ \sum_{n=1}^{\infty}\frac{a_{n}}{1+a_{n}^{2}} $ sometimes converges and sometime diverges.,"Let $ \lbrace a_{n}\rbrace $ be a sequence of positive terms such that $ \sum \limits_{n=1}^{\infty}a_{n} $ diverges. I am going to show that the series $$ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $$ sometimes converges and sometime diverges. My Attempt: For the divergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $, I defined the sequence $ \lbrace a_{n}\rbrace $ as follows. For each $ n\in \mathbb{N} $, $ a_{n}=1 $. Then $ \sum \limits_{n=1}^{\infty}a_{n}=\sum \limits_{n=1}^{\infty}1 $  and  $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}}=\sum \limits_{n=1}^{\infty}\dfrac{1}{2} $. Hence both  $ \sum \limits_{n=1}^{\infty}a_{n} $ and $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $ are divergent. But I am having trouble finding an example for the convergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $. Can any one please give me a hint or an idea?","['sequences-and-series', 'convergence-divergence', 'calculus', 'real-analysis', 'analysis']"
1127998,Understanding averaging of symplectic matrices via Haar measure,"In McDuff and Salamon's Intro. to Symplectic Topology (2nd edition), there's a proof that  $U(n)$ is a maximal compact subgroup of $Sp(2n)$ which I'm trying to understand. The proof uses the Haar measure to average over matrices in a way that's unclear to me. The statement I'm having difficulties with is the following: Let $G\subset Sp(2n)$ be a compact subgroup. There exist a symmetric and positive definite matrix $P\in Sp(2n)$ such that $$\Psi^TP\Psi=P, \forall\Psi\in G$$
According to the authors, such a matrix can be obtained by averaging the matrices $\Psi^T\Psi$ over $\Psi\in G$ using the Haar measure $C(G,\mathbb R)\rightarrow \mathbb R$ for a compact Lie group. So, the question is, in its broadest sense, how to understand the averaging process here? (Bear with me, clearer question follows) Here are some details on my progress in making sense of this before submitting this question: So far I have came across these Notes on Compact Lie Groups by Salamon which detail the construction of said Haar measure. This does not explain how to obtain a measure, in the measure theoretic sense, from the functional built in Salamon's notes. So, further searching lead me to Royden's Real Analysis (3rd edition), where he explains in detail how to build a measure out of a functional ( Daniell Integral ) in such a way that the integral over the measure agrees with the functional. All of this is fine and well, but ultimately remains in the realm of functionals, where I'm looking for a process that'll result in the matrix $P$ as above. In a more intuitive manner, what I gather should happen is somewhere along these lines: Let $M:C(G,\mathbb R)\rightarrow\mathbb R$ be a Haar measure following Salamon's notations, and let $\mu$ be the measure corresponding to the functional $M$, constructed like in Royden. Put $P=\int_G \Phi^T\Phi\operatorname{d}\mu$, then we can calculate that $$\Psi^TP\Psi=\Psi^T\int_G\Phi^T\Phi\operatorname{d}\mu\Psi=\int_G\Psi^T\Phi^T\Phi\Psi\operatorname{d}\mu=\int_G\left(\Phi\Psi\right)^T\Phi\Psi\operatorname{d}\mu=\int_G \Phi^T\Phi\operatorname{d}\mu=P$$where the second to last transition follows from $\mu$ being right invariant. So, now my question boils down to this - In light of all that's said, seeing as the Daniell integral $M$ assigns a real number to a function, where we need to assign a matrix to a function, how can $\int_G\Phi^T\Phi\operatorname{d}\mu$ be understood? Also, I'm guessing the second transition in the calculation follows from some means of limiting and applying the equation to the limits. If I'm mistaken, I would very much appreciate clarification on that as well, assuming the entire maneuver makes sense. Note : I am aware of an errata for this edition which fixes the error here, namely, $P$ is not guaranteed to be symplectic, but that's besides the point.","['matrices', 'measure-theory', 'integration', 'symplectic-linear-algebra']"
1128036,Example of a linear algebraic group which is not a Lie group,"I am trying to reconcile the notions of algebraic groups, linear algebraic groups, Lie groups, and Lie algebras, along with their notions of root systems, maximal tori, etc. To begin, I am trying to draw a sort of Venn diagram relating algebraic groups, linear algebraic groups, and Lie groups. Certainly all linear algebraic groups are algebraic groups. I know that the universal cover of $SL(2, \mathbb{R})$ is an example of a Lie group which is not a linear algebraic group. Are all linear algebraic groups Lie groups? I know that they can all be realized as subgroups of some $GL(n, k)$, which is a Lie group, but I believe that a subgroup of a Lie group must be closed to be a Lie group itself. Thanks for the help!","['algebraic-geometry', 'representation-theory', 'abstract-algebra', 'lie-groups', 'group-theory']"
1128058,How to figure out how many possible sequences contain a specific criteria,"If a 6-sided die is rolled 5 times and each roll is recorded as an element of set A (|A| will be 5 after all rolls), How many results out of all the possible results will have exactly two 4's as elements. First step was obviously to figure out how many possible results there are to begin with,
Since the result will be a sequence of 5 items from 6 choices, with repeats allowed I used the formula (n+k-1) C k, so here it will be 10 C 5, 10!/(5!5!) = 252 possible results. I'm stumped how to figure out how many of these results contain exactly two 4's....... I feel like the answer is staring me in the face but I can't quite see it lol. Just looking for a nudge in the right direction!","['permutations', 'discrete-mathematics', 'combinatorics']"
1128104,Can anything be learned about a probability distribution *directly* from its characteristic function?,"Some preliminaries: I know that one can take the inverse Fourier transform to get back the pdf...that is not what I am after. My question is whether the characteristic function, qua function, tells us anything about the underlying random variable. As an example: The moment generating function can be directly used in Chernoff's Lemma: $$P(X\geq0)\leq MGF_{X}(t),\; \forall t$$ Is there anything that can be similarly done with a Characteristic Function? Does it's complex modulus or absolute square have any meaning? Can we directly interpret its real or complex parts separately? From the little I have used the Characteristic Function, it seems to be used for showing convergence in distribution, and relies simply upon recognizing the forms of various characteristic equations. However, this narrow view is likely just from inexperience. I would be happy to have my view expanded :-)","['probability-theory', 'probability-distributions', 'probability', 'characteristic-functions']"
1128135,Theorem 3.1 from Milnor's Morse Theory,"Milnor is in the business of proving that if $f: M \to \mathbb{R}$ is a smooth function, $a < b$, and $f^{-1} ([a,b])$ is a compact subset of $M$ containing no critical points, then $M^a$ is diffeomorphic to $M^b$, where $M^x = f^{-1} (-\infty, x]$. In the proof, he starts by equipping $M$ with a Riemannian metric $g$ and then considering a smooth vector field $X$ such that $X = g(\nabla f, \nabla f)^{-1} \nabla f$ on $f^{-1}([a,b])$ and $X$ is compactly supported.  He then generates a maximal flow $\theta :\mathbb{R} \times M \to M$ for $X$.  He then notes that if $\theta_q(t) \in f^{-1}([a,b])$, then $\frac{d (f \circ \theta_q(t)}{dt}$ = 1, and hence the diffeomorphism $\theta_{b-a}$ takes $M^a$ diffeomorphically onto $M^b$. I do not understand how this follows.  I understand essentially that what I'm supposed to see is that if $f(q) = a$ then I can write $f \circ \theta_q(t) = t + a$ and then taking $t = b - a$. I do not see why I'm justified in getting the above expression for $f \circ \theta_q(t)$, despite knowing that $(f \circ \theta_q)' (0) = 1$ (wouldn't I need to know that the derivative is 1 on some neighborhood of 0 to deduce that $f \circ \theta_q$ is locally given by the linear function?  Maybe $\theta_q$ intersects $f^{-1}([a,b])$ only at $t = 0$).","['differential-topology', 'real-analysis']"
1128265,"Equivalent condition for $ (X_n, \mathcal{F}_n) $ to be a martingale","I've encountered an interesting problem and am not quite able to solve it. It is to prove the following statement ($ X_n $ denotes a sequence adapted to a filtration $\mathcal{F}_n) $: $$ (X_n,\mathcal{F}_n) \text{ is a martingale} \iff \text{ for any bounded stopping moment } \tau ~ \mathbb{E} X_\tau = \mathbb{E}X_0$$ I'm not quite sure where to begin with - it's possible to prove that for a martingale it holds that $$ \mathbb{E}\left( X_n - X_{n-1}\right) = 0 $$ but $ \tau $ is a random variable, so I can't think of a way to skip that; I would appreciate some help","['probability-theory', 'martingales']"
1128279,"Induction Proof Check: For a binary tree T, Prove that the number of full nodes in T is always one less than the number of leaves in T.","This is a slight variant on a very common beginner's problem. I think I've got it figured out, but I wanted to make sure I actually proved what's being asked. We define a binary tree $T$: (a) A tree with a single root $r$ is in $T$ (b) From $r$ branches two trees: $T_1$ and $T_2$ A node is full if it contains a non-empty left child and a non-empty right child. Prove (using induction) that for any tree, the number of full nodes is one less than the number of leaves. This recurrence relation describes the relationship between a the number of full nodes $F(n)$ and the number of leaves $n$: $$
F(n) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if $n = 2$}\\
    F(n-1) + 1 & \quad \text{if $n > 2$}
  \end{array} \right.
$$ I will now represent this recurrence relation in what I suspect is its closed form: $$ G(n) = n - 1 $$ Now, I if can prove that $F(n) = G(n)$, then I have proved that the number of full nodes is always one less than the number of leaves . Basis: $ G(2) = 2 - 1 = 1 $ $\checkmark$ Inductive Hypothesis: $F(k) = G(k) = k - 1$, where $n=k$ , to show: $F(k+1) = k+1-1 = k$ Inductive Step: $$\begin{align}
\ F(k+1) & = F((k+1)-1) + 1 \\
& = F(k) + 1 \\
& = G(k) + 1 \\
& = (k - 1) + 1 \\
& = k
& \qquad\square
\end{align}$$ It looks like I've proved that $F(n) = G(n)$, but I feel like I haven't properly associated the original recurrence relation $F(n)$ with the tree itself. I feel like I did, but what do proofs care about my feelings? Does my proof hold?","['trees', 'induction', 'discrete-mathematics', 'computer-science']"
1128300,System of equations with 2 parameters,"I have no idea how even to start!
\begin{align*}
(u^2+v^2)(u+v)&=15uv \\
(u^4+v^4)(u^2+v^2)&=85u^2v^2
\end{align*}","['symmetric-polynomials', 'algebra-precalculus', 'systems-of-equations']"
1128301,Does the limit of this double sequence exist?,"Consider 
$$a_{mn}=\frac{m^2n^2}{m^2+n^2}\left(1-\cos\left(\frac{1}{m}\right)\cos\left(\frac{1}{n}\right)\right)$$ Does $\lim_{m,n\to\infty}a_{mn}$ exist? It can be seen that $$\lim_{m\to\infty}\left(\lim_{n\to\infty}a_{mn}\right)=\lim_{n\to\infty}\left(\lim_{m\to\infty}a_{mn}\right)=\frac{1}{2}$$ However, I still cannot determine whether the limit exists or not. Any one can help? Thanks!","['sequences-and-series', 'real-analysis']"
1128314,Why upperbound $|x-a|$ by 1 in the proof of continuity?,"In most (all?) proofs of continuity of polynomials ($x^2, x^3$, etc), for example in Max Rosenlicht's book ( http://www.math.pitt.edu/~frank/pittanal2121.pdf , page 97), the usual trick is to get to the expression 
$$
|x-a||x+a|
$$ 
and then bound $|x-a|$ by 1, which is then used to bound $|x+a|$ and then obtain $\delta = \min \{1, \frac{\epsilon}{2a +1} \}$. This baffles me even after numerous attempts. My questions are: 1) Why 1? What will change if I chose a different value? 2) Why do we need this other 'smaller' bound at all? I realize the question is not particularly challenging, but afte numerous attempts I still can't get my head around it.","['continuity', 'real-analysis', 'limits']"
1128323,"Are $\emptyset$ and $X$ closed, open or clopen?","It is indeed a very basic question but I am confused: (1) In an 2013 MSE posting under general topology here , I was told that $\emptyset$ is an open set and therefore I assume $X$ must be open too. (2) But in Wikipedia page on clopen set here , it says ""In any topological space $X$, the $\emptyset$ and the whole space $X$ are both clopen."" (3) And yet in another Wikipedia page on closed set here , ""The $\emptyset$ is closed, the whole set is closed."" I must have missed something. Can you help me with a supreme verdict , once and for all, as sure as the sun rises from the east each morning, if $X$ and $\emptyset$ are open, closed or clopen. Of course I am talking about topology, thanks for your time.",['general-topology']
1128330,Calculation of determinant of an arrowhead matrix,"Is there any easier way to make sure the determinant of the following $n \times n$ matrix is $n$ ? $$\begin{vmatrix}
  1 & -1 & -1 & -1 & \cdots & -1 \\
  1 &  1 &  0 &  0 & \cdots &  0 \\
  1 &  0 &  1 &  0 & \cdots &  0 \\
  1 &  0 &  0 &  1 & \cdots &  0 \\
  \vdots & \vdots &  \vdots & \vdots  & \ddots &  \vdots \\
  1 & 0 & 0 & 0 &\cdots  & 1
 \end{vmatrix} = n$$ I figured it with a smaller dimension and it indeed produces the determinant that is the size of dimension. I tried to do a cofactor expansion with the first row, and each term produces the determinant of $1$ and if you sum them up, then the total determinant will be $n$ . But the sign change for each cofactor is confusing, and it is not easily seen that each cofactor term is actually positive $1$ .","['matrices', 'determinant']"
1128336,The square of a standard Normal random variable,I am having a bit of trouble with this: Let $U=Z^2$ where Z is a standard Normal random variable with pdf: $$f_z(z) = \frac{1}{\sqrt{2\pi}} e^{\frac{-z^2}{2}}$$ I want to use the inversion method but have thus far only learned to use this when functions are strictly increasing or decreasing. Since a standard normal distribution function is strictly increasing and then strictly decreasing I thought perhaps I could find some way to use this method. I have the final answer as $f_u(u)= \frac{1}{\sqrt{2\pi}\sqrt{u}}e^{\frac{-u}{2}}$ for $u>0$ But I am not very comfortable with the process of getting to that answer. I used a bit of a walk through and made some assumptions about what was happening. Could anyone help me understand how I should use the above information to reach this answer?,"['statistics', 'probability']"
1128350,"If $x_n \rightarrow 0$ and $\{y_n\}$ is a bounded sequence, then $x_ny_n \rightarrow 0$.","Let $(x_n)$ be a sequence such that $x_n \to 0 $ and let $(y_n)$ be a sequence such that $(y_n)$ is bounded. Show that $(x_ny_n) \to 0 $ My try Since $(y_n)$ is bounded, we can find some $\alpha \in \mathbb{R}$ such that $|y_n| < \alpha $ for all $n$. Next, let $\epsilon > 0 $ be given and find some $N > 0$ such that $|x_n| < \frac{ \epsilon}{ \alpha } $ for all $n > N $. Notice, $$|x_ny_n| = |x_n||y_n| < \frac{ \epsilon}{\alpha} \cdot \alpha = \epsilon$$ To show that $(x_ny_n) \to 0 $, is it enough to take the same $N$ as before? so that $|x_ny_n| < \epsilon $ for all $n > N $","['proof-verification', 'real-analysis', 'limits']"
1128351,Deduce the conclusion from the premise.,"Use the valid argument form to deduce the conclusion
from the premises, giving a reason for each step. A. ~p v q ➵ r B. s v ~q C.~t D.  p ➵ t E. ~p Λ r ➵  ~s F. (conclusion) ~q So Far this is my work. p➵   t  ( p implies t, if p then t, modus tolltens) ~t conclusion ~p ~p ➵  q (conclusion) ~p v q (generalization) ~p v q ➵  r ~ p v q r This is where I get stuck. Does anyone know what to do next and why?",['discrete-mathematics']
1128352,References for hemicontinuity?,"Let $X$ be a real vector space, $K\subset X$ be a nonempty and convex set. 
The mapping $f:X\rightarrow\mathbb{R}$ is said to be hemicontinuous if for every $u,v\in K$,
the mapping $g(t):[0,1]\rightarrow\mathbb{R}$ given by $g(t)=f(tu+(1-t)v)$ is continuous. I would like to find references and properties for this function. Thank you for all kind help and comments.","['vector-spaces', 'convex-analysis', 'convex-optimization', 'functional-analysis']"
1128360,How do I prove these biconditional statements?,"I keep getting stuck when I get to (not p or q) and (p or not q) for number 3 and for number 4 I get stuck in relatively the same place. Edit: I want to prove them with using equivalence laws, not truth tables","['logic', 'discrete-mathematics']"
1128367,Integration help - question: $e^{-\sin(x)}$,"I would really like some help with the integration of $e^{-\sin(x)}$. Thanks to anyone who will help :) Given that $\sin(x) > \frac{2x}{\pi}$ for $0 < x < \frac{\pi}{2}$, where $$\int_0^{\pi/2}e^{-\sin x}\,dx<\int_0^{\pi/2}e^{-2x/\pi}\,dx$$ RTS: $$\int_0^{\pi/2}e^{-\sin x}\,dx=\int_{\pi/2}^{\pi}e^{-\sin x}\,dx$$",['integration']
1128380,"Find $f_{1}, f_{2}$ in $\mathbb{Z_{6}}[x]$ such that deg$(f_{1})$ = deg$(f_{2}) = 2$ and deg$(f_{1}+f_{2})=1$","Find $f_{1}, f_{2}$ in $\mathbb{Z_{6}}[x]$ such that deg$(f_{1})$ = deg$(f_{2}) = 2$ and deg$(f_{1}+f_{2})=1$ Find $g_{1}, g_{2}$ in $\mathbb{Z_{6}}[x]$ such that deg$(g_{1})$ = deg$(g_{2}) = 1$ and deg$(g_{1}.g_{2})=1$","['modular-arithmetic', 'abstract-algebra']"
1128396,Random sums of iid Uniform random variables [duplicate],"This question already has answers here : Choose a random number between $0$ and $1$ and record its value. Keep doing it until the sum of the numbers exceeds $1$. How many tries do we need? (8 answers) Closed 4 years ago . Let $\{X_r : r\ge 1\}$ be independently and uniformly distributed on $[0,1]$. Let $0<x<1$ and define $$N=\min\{n\ge 1 : X_1 + X_2 +\ldots+X_n> x\}$$ Show that $$P(N>n) =  \frac{x^n}{n!}$$ and hence find the mean and variance of $N$. I cant see how $N$ can be greater than $n$, as each $X_r$ is less than or equal $1$, so if each $X_r= 1$, then the sum would equal $n$, but not be greater. I am obviously interpreting this wrong but I cant see where.","['statistics', 'probability-distributions', 'probability-theory']"
1128403,What is the difference between Hom and Sheaf Hom?,"I'm reading Hartshorne's book, and in 3.6 he begins to go into detail about Ext and sheaf Ext, which are derived functors of Hom and sheaf Hom respectively. Let $\mathcal{F,G}$ be sheaves of $\mathcal{O}_X$ modules on a scheme $X.$ $Hom(\mathcal{F,G})$ is the set of $\mathcal{O}_X$ module homomorphisms between $\mathcal{F}$ and $\mathcal{G}$. Hence, if $\varphi \in Hom(\mathcal{F,G})$ then for every open set $U \subset X,$ we have a map $\varphi|_{U}$. On the other hand, the sheaf $\mathcal{Hom(F,G)}$ assigns to each open set $U\subset X$ the set of $\mathcal{O}_U$ module homomorphisms $Hom \mathcal{(F}(U),\mathcal{G}(U))$ (where $\mathcal{O}_U$ is considered as a ring, not a ringed space). It seems to me that both Hom and sheaf Hom encode the same information in different ways, so I don't understand why their derived functors seem different. EDIT: Actually I think Hom and sheaf Hom might be different in the following way: for Hom, we are looking at ""global maps,"" i.e. maps of $\mathcal{O}_X$ modules, which we can then restrict to an open set. On the other hand, sheaf Hom assigns to each open set a hom-set of maps, and some of those maps may not arise from restriction of a ""global map."" But since sheaf Hom is a sheaf, not a presheaf, this probably shouldn't be a problem, and all maps are indeed restrictions of global maps since we can glue them together. Am I right about this? -Hom is just the global sections of sheaf Hom. Thanks Qiaochu",['algebraic-geometry']
1128416,Equality on functions in $ \mathbb{R}^n $,"Let $ f,g : M \subset \mathbb{R}^p \to \mathbb{R}^q $ continuous. Given $ a \in M $, supose that all open ball centered in $a$ contains a point $x$ such as $f(x) = g(x) $. Show that $ f(a) = g(a) $. Use this to show that if $f,g$ are  continuous real functions and $ f(x) = g(x) $ for all $x \in \mathbb{Q}$  then $ f \equiv g $. Demonstration: Since $f,g$ are continuous  $ x \in B(a,\delta) \implies f(x) \in B (f(a),\epsilon) $ and $ x \in B(a,\delta) \implies g(x) \in B (g(a),\epsilon) $ (in this case I'm already considering minimum delta that satisfies this). By the global continuity we have $ A = M \cap f^{-1}(B(f(a), \epsilon) $ and $ B = M \cap g^{-1} (B ( g(a), \epsilon ) $. Supose that for all $ x \in B(a,\delta) $ we find a $f(x)  \in B(f(a),\epsilon)$ and $ g(x) \in B(g(a),\epsilon ) $ such as $f(x) =  g(x)$. Since they are open balls and continuous we have a neigborhood in $f(a)$ (and also in $g(a)$ ) such as all points on image are equal in particular $ f(a) = g(a) $. My question is:
That proves the first proposition? 
Why there is need to restrict the second statement to rationals only? Isn't it valid to all reals? (can we find a counter example?) How can we prove this? (i could not find any way besides saying that is valid for all real function therefore is valid to rationals.","['general-topology', 'functions', 'continuity', 'real-analysis', 'metric-spaces']"
1128436,Prove that vector space and dual space have same dimension,"As an exercise in my textbook, I need to prove that if $V$ is a finite dimensional vector space with dual space $V^*$ over $\mathbb{R}$, then dim$(V)$=dim$(V^*)$. Let $\omega\in V^*$ and let $\{e_1,...,e_n\}$ be a basis for $V$. Define $e^i\in V^*$ by $e^i(e_j)=\delta_{ij}$. We show that $\{e^1,...,e^n\}$ spans $V^*$. $\omega(v)=\omega(v_1e_1+...+v_ne_n)=v_1\omega(e_1)+...+v_n\omega(e_n).$ If $\omega(e_1)=\lambda_1,...,\omega(e_n)=\lambda_n$, then $\omega(v)=v_1\lambda_1e^1(e_1)+...+v_n\lambda_ne^n(e_n)$=$\lambda_1e^1(v)+...+\lambda_ne^n(v)$. To show $\{e^1,...,e^n\}$ is linearly independent, suppose that $0=c_1e^1+...+c_ne^n$ is the zero mapping to $\mathbb{R}$. Consider the image of $e_1:$  $0(e_1)=c_1*1+...+c_n*0=c_1$ Hence, $c_1=0$. Repeating the procedure for $e_j$, $2\leq j\leq n$, we see that $c_1=c_2=...=c_n=0$. Does this proof look correct? If the proof is correct, I have an additional small question. It seems like this proof is dependent on the fact that $V$ is a real (or complex) vector space, since we define the covectors to have the image set $\{0,1\}$. Is there a proof that works for vectors spaces over general fields?",['linear-algebra']
1128457,Find the length of the chord given that the circle's diameter and the subtended angle,A chord of a circle subtends an angle of 89 degrees at its centre. Find the length of the chord given that the circle's diameter is 11.4 cm. The problem I have here is that I can't visualise this question. I've tried drawing it but gets all messy and I get confused so can someone please draw the diagram for me? That's all I need because I can work from there on after.,"['geometry', 'triangles', 'trigonometry', 'circles']"
1128474,Does $A\setminus B = A\setminus C$ imply $B=C$?,"Let $A, B, C$ be sets with $B \subset  C$ and $C \subset  A$. Does $A\setminus B = A\setminus C$ imply $B=C$? I am not sure what the ""\"" means, so I don't know how to solve this.","['notation', 'elementary-set-theory']"
1128476,How can I square $-1 < x < 1$?,"If I square $-1 < x < 1$, I get $1 < x^2 < 1$ which doesn't make any sense. What additional algebraic steps do I need to apply in order to get the proper inequality $0 < x^2 < 1$? And if we are going backwards, how do I algebraically solve $x^2 < 1$ to obtain $-1 < x < 1$? If I take the square root of both sides, I get $x < \pm 1$ which would mean that $x < 1$ and $x < -1$ so that's just $x < -1$ which is obviously not true. The solution would be to flip the inequality for the negative sign, but how do you know when to flip the inequality sign or not besides when you divide/multiply by opposite signs?",['algebra-precalculus']
1128509,"Show $\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$ if $|z_1| ,|z_2| < 1$ [duplicate]","This question already has answers here : Show that $\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$ when $|\alpha|,|\beta| < 1$ (4 answers) Closed 6 years ago . Show $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ if $|z_1| <1$ and $|z_2| < 1$ Consider: $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right|^2$$
$$={\frac{|z_1-z_2|^2}{|1-z_1 \overline{z_2}|^2}}$$
$$=\frac{(z_1-z_2)(\overline{z_1-z_2})}{{(1-z_1\overline{z_2})(\overline{1-z_1\overline{z_2}})}}$$
$$=\frac{(z_1-z_2)(\overline{z_1}-\overline{z_2})}{{(1-z_1\overline{z_2})(1-\overline{z_1}z_2)}}$$
$$=\frac{z_1\overline{z_2}+z_2\overline{z_2}-z_1\overline{z_2}-\overline{z_1}z_2}{1+z_1\overline{z_2}\overline{z_1}z_2-\overline{z_1}z_2-z_1\overline{z_2}}$$
$$=\frac{|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2})}{1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})}$$ Now, I need to show that: $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ So, $$1+\frac{|z_1|^2}{|z_2|^2} < \frac{1}{|z_2|^2}+|z_1|^2$$ $$\rightarrow 1-|z_1|^2 < \frac{1}{|z_2|^2} - \frac{|z_1|^2}{|z_2|^2}$$ $$\rightarrow 1-|z_1|^2 < \frac{1 -|z_1|^2}{|z_2|^2}$$ This shows $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ So, $$|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}| < 1$$ Is this proof correct? However I am also curious as to the following ""proof"". $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$
$$\rightarrow |z_1|^2 - |z_1|^2|z_2|^2 < 1-|z_2|^2$$
$$\rightarrow |z_1|^2(1-|z_2|^2) < 1-|z_2|^2$$
$$\rightarrow |z_1|^2< 1$$ I am curious because since $|z_1|<1$, then this implies $|z_1|^2 < 1$. So, is it sufficient to show that  $|z_1|^2 < 1$ which implies $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ which implies $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ which implies $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ ?","['complex-numbers', 'complex-geometry', 'proof-verification', 'complex-analysis']"
1128520,Maximum / Minimum Question with 3 Variables?,"I seem to be stuck in this problem, would need your help! Question: Assume I have : 147 of x, 174 of y, 238 of z A different amount of x, y and z are being used to produce 3 different products A, B and C, satisfying the 3 equations below. A = 10x + 5y + 3z, B = 3x + 10y + 5z, C = 5x + 3y + 10z The question is how do I find out which combination of A, B and C I should produce in order to maximize the usage of the current resources and produce the maximum amount of A, B and C combined. (A + B + C will be maximum.) Thanks for the help!","['multivariable-calculus', 'linear-programming']"
1128525,"if $f(x)$ is even and can be infinitely differentiable, how about $f(\sqrt{x})$","I have a question $f(x)$ is even and can be infinitely differentiable, how about $f(\sqrt{x})$ in [0,$\infty$)? can we say that the  $f(\sqrt{x})$ also can be infinitely differentiable in $[0,\infty)$. My thoughts: let $g(x)=f(\sqrt{x})$, and then I have proved $g(x)$ can be differentiable and continuous in $[0,\infty)$, and I supposed this conclusion still holds for $n=k$($k$ is an integer),which means $g(x)$ can be $k$-times differentiable and continous in $[0,\infty)$, so I want to use mathematical induction to prove it. But I am stuck here, since I have no idea how to prove g can be $(k+1)$ times differentiable using the assumption that $g$ can be $k$-times differentiable and continuous. Can someone tell me whether it is true or not?","['calculus', 'real-analysis', 'analysis']"
1128541,Finding conditional expectation from system of equations,"I have three equations: $$X_m = \beta_0 + \beta_1 \cdot X_I + \varepsilon_{BL}$$ $$W_M = X_M + \varepsilon_{MBL}$$ $$W_I = \gamma_0 + \gamma_1 \cdot X_I + \varepsilon_{RDI}$$ The $\varepsilon$'s are normally distributed with mean 0, variance $\sigma^2$. I need to find E[$X_I\mid W_I$] and E[$X_I\mid W_I,W_M$]. The first is straightforward because I have an equation that relates $X_I$ and $W_I$, but how do find the latter expectation?","['statistics', 'conditional-expectation']"
1128549,Getting equations from a network graph,"I am learning about Network Analysis in Linear Algebra and I need help figuring out how to get the equations from this graph: The arrows represent how many particles are going in a given direction. The back of the book tells me they are: $$x_1 + x_2 = 20$$
$$x_3 + 20 = x_4$$
$$x_2 + x_3 = 20$$
$$x_1 + 10 = x_5$$
$$x_5 + 10 = x_4$$ From my understanding, if we look at first equation, there are twenty particles going to the $1$ and they can split in two directions. So if you add up $x_1$ and $x_2$, they should equal twenty as it is impossible to go higher than that. Even if I am correctly analyzing the first equation, I don't understand the rest of the equations that the book gives. For example, how does $x_3 + 20 = x_4$? What do $x_3$ and $20$ have to do with $x_4$? Any help would be appreciated. Thank you!","['graph-theory', 'linear-algebra']"
1128570,Proving that $\sin^7\theta + \cos^7\theta <1$ using basic trigonometry and identities [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How do I prove $\sin^7\theta + \cos^7\theta < 1$ for an angle between $(0,\pi/2)$?","['inequality', 'trigonometry', 'algebra-precalculus']"
1128621,How can I show that $f$ must be zero if $\int fg$ is always zero?,"Let $f(x)$ be continuous on $[a,b]$ and suppose $\int_a^b f(x)g(x)dx = 0$ for every continuous function $g$ on $[a,b]$. Prove that $f(x)=0$ on $[a,b]$. I understand that $f(x)$ must be zero otherwise the integral can't possibly be zero for all possible $g$. But, I am not sure how to approach this systematically. Any suggestions would be appreciated. Thanks!","['calculus', 'integration']"
1128629,"Find $E(|X-Y|^a)$ where $X$ and $Y$ are independent uniform on $(0,1)$","Let $X,Y$ be independent $Uniform(0,1)$ random variables. Find $E(|X-Y|^a)$ where $a>0$ . My working: Define $W=1$ if $X>Y$ and $W=0$ if $X<Y$ . We seek $E(|X-Y|^a)=E[E(|X-Y|^a|W)]=E(|X-Y|^a|W=1)P(W=1)+E(|X-Y|^a|W=0)P(W=0)=E((X-Y)^a)P(X>Y)+E((Y-X)^a)P(X<Y)$ . Now $P(X<Y)=\dfrac{1}{2}$ and $P(X>Y)=\dfrac{1}{2}$ by symmetry. $E((X-Y)^a)=\int_{0}^{1}\int_{0}^{x}(x-y)^af_{X,Y}(x,y)dydx=\int_{0}^{1}\int_{0}^{x}(x-y)^adydx=\dfrac{1}{(a+1)(a+2)}$ . Similarly $E((Y-X)^a)=\dfrac{1}{(a+1)(a+2)}$ by symmetry when $Y>X$ . Required expectation= $\dfrac{1}{(a+1)(a+2)}\dfrac{1}{2}+\dfrac{1}{(a+1)(a+2)}\dfrac{1}{2}=\dfrac{1}{(a+1)(a+2)}$ But correct answer is $\dfrac{2}{(a+1)(a+2)}$ . Where am I going wrong?","['probability-theory', 'uniform-distribution', 'probability-distributions', 'probability']"
1128631,Can anyone solve a stochastic differential equation - related to neuroscience research?,"I'm a neuroscience grad student, and I'm hoping one of ya'll could help me solve this problem regarding particle diffusion. It relates to my research on molecular-level neural plasticity, but I've framed the problem in general terms below. This will probably require you to have some experience with stochastic differential equations. Any insight is much appreciated. Thanks! The problem in a nutshell I am ultimately interested in the average steady-state particle density that can be expected in a defined region of space, when the particles are diffusing with random motions -- assume particle diffusion is Brownian motion , with zero drift, and have a constant diffusion rate coefficient. For example I'm looking for a numerical solution for the average steady-state particle density in two circular subregions with a diameter of 0.8 arbitrary units (au) contained inside a rectangular 3x6 au flat surface (these subregions are equal distance from the edges - see below). Here, let's define 1 au of distance as 1 micron (µm). In this 18 µm² space the particle diffusion coefficient is 0.1 µm²/s, except in the circular subregions; in subregion-1 the diffusion rate is 0.05 µm²/s, in subregion-2 the diffusion rate is 0.01 µm²/s. The boundary conditions can be considered 'rebound', and there are 200 total particles in this closed system. The solution should be able to generalize to any number of particles, but solving this particular situation would be great for starters. See the question visually represented here in Fig 1 along with two graphs described below. Each data-point in the graphs represent the average steady-state particle density for 10 independent simulation trials. Monte Carlo Simulation Results Figures (Fig 1) I've simulated the above scenario using Matlab. Particles diffused along the 2D surface of a closed rectangular environment that contained two equal-area subregions with a diffusion rate slightly lower than the global diffusion rate of 0.1 µm²/s. The diffusion coefficient (Dcoeff) of the bottom circular subregion was set to 0.01 µm²/s while the top subregion was set to 0.05 µm²/s. There were 500 total time-steps in each individual simulation. The heat-map seen on the right highlights regions of relatively high particle density at the end of the simulation (aka at steady-state ); which was done using matrix convolution of particle locations with a Gaussian-shaped mask (top right). Matlab code for this simulation (Fig 2) The diffusion coefficients mentioned above resulted in average steady-state particle density values that were vastly different between the two subregions, with little variance (CI envelopes reflect noise for 10 iterations). The top subregion averaged ~15 particles at steady-state (green), while to bottom region averaged ~60 particles (red). The third line (blue) represents the steady-state particle density in a circular region the same size as the two subregions, but was set to the global diffusion rate. (Fig 3) This figure shows the effects of holding the Dcoeff of one subregion constant at 0.01 µm²/s while changing the other Dcoeff from 0.01 µm²/s to 0.05 µm²/s (at 0.01 µm²/s increments). (left panel) The Dcoeff ratio at each step (calculated by simply dividing the pre-set diffusion rates of the two subregions) and the resulting steady-state particle density in each subregion are almost exactly proportional. (right panel) Interestingly, particle availability minimally affects this outcome - as the upper subregion transitioned from 0.01 µm²/s to 0.05 µm²/s it ultimately made an additional 25-30 particles available globally, but only a small portion of them accumulated in the lower subregion (which had a stable Dcoeff of 0.01 µm²/s throughout this ancillary simulation). The math question and proof of concept I believe that the particle diffusion can be summarized as Wiener process / Brownian motion with zero drift, and the Kolmogorov forward equation (aka Fokker Plank equation) will describe the time-evolution of the PDF for a random process. However, it's not immediately clear to me how to define the sODE (or sPDE); consequently, I don't even have a good sense of the difficulty-level of this question. Ultimately what I'm looking for, is an equation where I can enter these constants: Co (Outer box xy dimensions) Ci (Inner circle radius) Do (diffusion coefficient for Co) Di (diffusion coefficient for Ci) N (total number of particles in the closed system) And the output will provide the expected density of particles in Co and Ci at steady-state. Note that I'm also interested in the rate of change in these values if suddenly D changes from 0.01 µm²/s to 0.05 µm²/s , but first things first. I believe the steady-state diffusion equation will be of use: ∇D(r)∇n(r)=0 with spatially dependent diffusion coefficient D(r) and prescribed total particle number N=∫n(r)dr If you have an answer, please use an example showing the input/output. I would like to test it against the Monte Carlo simulation to be convinced. If the fact that the two-dimensional geometry has no special symmetry (which prevents a closed-form solution), feel free to just cut the system in half, such that we are only considering the bottom 3x3 square with the circular subregion directly in the center. In fact, if it makes it significantly easier, the outer container can be circular as well. Again, any help is much appreciated! Cheers for sharing your skills.","['stochastic-processes', 'differential-geometry', 'heat-equation', 'brownian-motion', 'surfaces']"
1128636,Prove that if $E(X\log X)<\infty$ then $E(\sup_n |S_n|/n)<\infty$.,"This is part 2 of a two part question. In the first part, we were asked to show that if you had a non-negative sub martingale $M_n$ then $$\sup_n E(\sup_{k\leq n} M_k)\leq \sup_n 2E(M_n \log M_n)+2$$ We need to use the above fact to Prove that if $E(X\log X)<\infty$ then $$E(\sup_n |S_n|/n)<\infty,$$ where $S_n=X_1+\dots+X_n$, with $X_i$ iid with distribution $X$. I am unsure of how to introduce a sub-martingale into the problem since $S_n/n$ is not a martingale. Any help/hints would be appreciated.","['probability-theory', 'martingales']"
1128657,Gosper's unusual formula connecting $e$ and $\pi$,Wolfram MathWorld quotes (see equation $(26)$) Gosper gives the unusual equation connecting $\pi$ and $e$ $$\sum_{n = 1}^{\infty}\frac{1}{n^{2}}\cos\left(\frac{9}{n\pi + \sqrt{n^{2}\pi^{2} - 9}}\right) = -\frac{\pi^{2}}{12e^{3}}\tag{1}$$ I did not find any references to a paper in which Gosper proved the above formula. I am perplexed by this cosine term in the formula and it does not seem to resemble any familiar series. A direct proof of this formula would be greatly appreciated. Or if someone knows Gosper's paper where this formula is established then do provide a link to that paper. Update : We can see that $$\frac{9}{n\pi + \sqrt{n^{2}\pi^{2} - 9}} = n\pi - \sqrt{n^{2}\pi^{2} - 9}\tag{2}$$ and noting that $\cos(n\pi - \alpha) = (-1)^{n}\cos \alpha$ we can see that the formula of Gosper can be written as $$\sum_{n = 1}^{\infty}\frac{(-1)^{n}}{n^{2}}\cos\sqrt{n^{2}\pi^{2} - 9} = -\frac{\pi^{2}}{12e^{3}}\tag{3}$$ I think that the formula given by Gosper is probably a special case of a more general formula for the sum $$\sum_{n = 1}^{\infty}\frac{(-1)^{n}}{n^{2}}\cos\sqrt{n^{2}\pi^{2} + a^{2}}\tag{4}$$ (the formula $(3)$ corresponds to $a = 3i$). Looks like Gosper is also fond of strange formulas like Ramanujan.,"['pi', 'sequences-and-series']"
1128664,Intuition of coset of a subgroup,"Hey guys I am trying to form the intuition that distinct left coset of subgroups are  actually disjoint. I understand the proof constructed but I don't think I get the intuition behind why that the case if someone could maybe explain to me that would be great. I think this is very important to understand concretely since Lagrange theorem which has very intuitive proof is based on it. Definition of coset: Let H be a subgroup of the group G. For any $a \in G$ , $aH =\{x \in G : x = ah \quad \text{for some} \quad h \in H\}$ is a left coset of $H \in G$ .
Similarly, $Ha$ is called a right coset of $H \in G$ .","['group-theory', 'abstract-algebra']"
1128681,Group of order 112 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $G$ be a finite group of order $2^4\times 7$ and Sylow $7$-subgroup of $G$ is not normal. Prove that Sylow $2$-subgroup of $G$ is abelian. I am very grateful for any help in this problem.","['abelian-groups', 'abstract-algebra', 'sylow-theory', 'finite-groups', 'group-theory']"
1128690,Prove there exists $a\in \Bbb{R}$ such that $f'(a)=0$.,"Let $f$ be differentiable on $\Bbb{R}$ and let $\lim_\limits{x\to \infty}f(x)=\lim_\limits{x\to -\infty}f(x)=0$. Prove there exists $a\in \Bbb{R}$ such that $f'(a)=0$. Attempt: If $f$ is constant we are done. Otherwise, $f$ is not constant. Suppose there is no such $a$. Then, in particular, $f$ has no extremum $\implies$ $f$ has no point where is goes from increasing to decreasing or from decreasing to increasing. $f$ is continuous since it is differentiable. Hence, since $\lim_\limits{x\to \infty}f(x)=\lim_\limits{x\to -\infty}f(x)=0$, $f$ must be constant. A contradiction. Therefore either $f$ is constant and $f'(x)=0$ everywhere, or $f$ has an extremum at $x=a$ and $f'(a)=0$. I feel like my attempt is not formal enough, any correction? 
I would appreciate your help.","['calculus', 'proof-verification', 'real-analysis', 'limits']"
1128712,integral cohomology ring of real projective space,"What is the cohomology ring 
$$
H^*(\mathbb{R}P^\infty;\mathbb{Z})?$$
$$
H^*(\mathbb{R}P^n;\mathbb{Z})?$$ for mod 2 coefficient, the answer is on Hatcher's book and Proving that the cohomology ring of $\mathbb{R}P^n$ is isomorphic to $\mathbb{Z}_{2}[x]/(x)^{n+1}$ . For the graded module structure, it is obtained from the homology struture.","['projective-space', 'homology-cohomology', 'algebraic-geometry', 'algebraic-topology', 'cw-complexes']"
1128717,How do I prove that $S_A\cong S_B\implies |A|=|B|$?,"Let $A,B$ be infinite sets such that $S_A\cong S_B$. (Symmetric groups are group isomorphic) How do I prove that $|A|=|B|$? The only proof I know uses Axiom of choice. (That is, using AC to give orders to infinite family of finite sets) Is there a way to prove this not invoking AC?","['abstract-algebra', 'axiom-of-choice']"
1128719,Convergence for Conjuguate gradient method,"I am trying to probe this corollary in a numerical PDE book: If $A\in \mathbb{R^{n\times n}}$  is symmetric and positive definite, then the conjugate gradient method reaches the exact solution in at most $n$ iterations.","['numerical-linear-algebra', 'linear-algebra', 'numerical-methods', 'analysis']"
1128755,How many 0's are in the end of this expansion?,How many $0's$ are in the end of: $$1^1 \cdot 2^2 \cdot 3^3 \cdot 4^4.... 99^{99}$$ The answer is supposed to be $1100$ but I have absolutely NO clue how to get there. Any advice?,"['divisibility', 'contest-math', 'elementary-number-theory', 'number-systems', 'combinatorics']"
1128777,Alternative matrix representation for translation,"The ''usual"" way to write translation for $\textbf{v}\in \mathbb{R}^2$ is with the following $3\times3$ matrix
$$  \left( {\begin{array}{ccc}
   1 & 0 & x_{0} \\
   0 & 1 & y_{0} \\
   0 & 0 & 1 \\
  \end{array} } \right) \cdot \left( {\begin{array}{c}
   x \\
   y \\
   1 \\
  \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\\1\end{array}}\right)$$ But also it seems to be possible to write this transformation with the following $2\times2$  matrix:
$$  \left( {\begin{array}{cc}
   1 & \frac{x_{0}}{y} \\
   \frac{y_{0}}{x} & 1 \\
   \end{array} } \right) \cdot \left( {\begin{array}{c}
   x \\
   y \\
    \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\end{array}}\right)$$ I didn't find a source using this as a version of translation, what are the problems associated with this representation?","['linear-transformations', 'matrices', 'linear-algebra']"
1128825,"Writing random variable formulas with set notations, What is the problem?",Is it wrong to write $\displaystyle P(X \mid Y) = \frac{P(X \cap Y)}{P(Y)}$ when $X$ and $Y$ are random variables? As I know a random variable is a function and therefore has a range and the two ranges may intersect.,"['probability-theory', 'probability', 'random-variables']"
1128862,Is it possible that a finitely generated ring has an ideal that is not finitely generated,"Sorry if this is duplicated. I couldn't find an exact answer of my question. One definition of Noetherian ring is: A ring $R$ is Noetherian if all its ideals are finitely generated. I know there are other definitions. I am just wondering why this definition has to state ""all its ideals"". Can there be a ring $R$ that is finitely generated as its own ideal, and has a proper ideal that is not finitely generated? Are there examples? Thank you!","['ring-theory', 'group-theory', 'abstract-algebra', 'noetherian']"
1128917,Prove that the circumcenter of $\triangle PIQ$ is on the hypotenuse $AC$.,"In right angled $\triangle ABC$ with $\angle B=90 ^{\circ}$, $BD$ is an altitude on $AC$. $P,Q,I$ are the incenters of $\triangle ABD,\triangle CBD$ and $\triangle ABC$ respectively. Prove that the circumcenter of $\triangle PIQ$ lies on the hypotenuse $AC$. This question was asked today in the Indian National Mathematics Olympiad (INMO) 2015 in which I appeared.I totally had no idea and infact have no idea on how to proceed. Any help would be appreciated.","['geometry', 'triangles']"
1128930,Matrix norm question,"Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm, if $$||A^*A+AA^*||=||A^*A||$$ does it imply that $AA^*=0$. If not, could you give a counter-example?",['matrices']
1128932,no. of real roots of the equation $ 1+\frac{x}{1}+\frac{x^2}{2}+\frac{x^3}{3}+............+\frac{x^7}{7} = 0$,"The no. of real roots of the equation $\displaystyle 1+\frac{x}{1}+\frac{x^2}{2}+\frac{x^3}{3}+............+\frac{x^7}{7} = 0 $ $\bf{My\; Try::}$ First we will find nature of graph of function $\displaystyle 1+\frac{x}{1}+\frac{x^2}{2}+\frac{x^3}{3}+............+\frac{x^7}{7}$ So $$\displaystyle f(x) = 1+\frac{x}{1}+\frac{x^2}{2}+\frac{x^3}{3}+............+\frac{x^7}{7}.$$ Then Differentiate both side w . r to $x\;,$ We get $$\displaystyle f'(x)=1+x+x^2+x^3+..........+x^6$$ Now for max. and Minimum Put $$f'(x) = 0\Rightarrow 1+x+x^2+x^3+x^4+x^5+x^6 = 0$$ We can write $f'(x)$ as  $$\displaystyle \left(x^3+\frac{x^2}{2}\right)^2+\frac{3}{4}x^4+x^3+x^2+x+1$$ So $$\displaystyle f'(x) = \left(x^3+\frac{x^2}{2}\right)^2+\frac{1}{4}\left[3x^4+4x^3+4x^2+4x+4\right]$$ So $$\displaystyle f'(x) =  = \left(x^3+\frac{x^2}{2}\right)^2+\frac{1}{4}\left[\left(\sqrt{2}x^2+\sqrt{2}x\right)^2+(x^2)^2+2(x+1)^2+2\right]>0\;\forall x\in \mathbb{R}$$ So $f'(x) = 0$ does not have any real roots. So Using $\bf{LMVT}$ $f(x) = 0$ has at most one root. In fact $f(x) = 0$ has exactly one root bcz $f(x)$ is of odd degree polynomial and it will Cross $\bf{X-}$ axis at least one time. My question is can we solve it any other way, i. e without using Derivative test. Help me , Thanks",['algebra-precalculus']
1128948,Evaluate $\lim_{x\to\infty} ((x^5+x^4)^{1/6}-(x^5-x^4)^{1/6})$,"I've been struggling with the following: $$\lim_{x\to\infty} ((x^6+x^5)^{1/6}-(x^6-x^5)^{1/6})$$ Tried factoring out $x^{5/6}$ and then using L'hopital- which got me nowhere, tried multiplying by the conjugate, but it got messy- so either I'm scared of the algebra or there's a better way.","['calculus', 'limits']"
1128995,Differentiating a multivariable function,"Knowing that $$z(x,y)=f(\frac{x}{y})$$I'm supposed to find $$x\frac{\partial z}{\partial x} + y\frac{\partial z}{\partial y}$$ . This problem makes no sense to me, can anyone help with the differentiation ? I really don't get how to apply the chain rule this time .","['multivariable-calculus', 'partial-derivative']"
1129004,Create the most 'stressful' tennis match ever!,"Some sports, such as tennis, use a complicated points system (point, game, set, match; with deuces and tie-breaks) for what would otherwise be an extremely simple and monotonous sport. The main reason, I suppose, is to make the sport more stressful, by making some parts of the match more crucial than others. It should be possible to make it a lot more 'stressful' mathematically. Mathematical scenario: There is an event, played by 2 players, worth 1 point. The probability of winning the entire match, if they (a) win (b) lose the next point is calculated. The difference between these values is the 'stress' value of that point. The sum of all the stress values is called the 'total stress' value of the match, and this number divided by the number of points played is the 'average stress'. For players of equal skill, and stress-handling, the probability of either player winning any given point is $\frac12$. The 'skill' of a player is a randomly generated number from $0\%$ to $100\%$, and the probability of a player winning a point is given by $\frac{Skill(own)}{Skill(own)+Skill(opponent)}$ Your job is to create a points system, however simple or complicated, using any kind of conditions, limits, etc, that produces the maximum possible: (a) Total stress (b) Average stress for a match. You are allowed to create your own definitions of 'point', 'set' and so on, however, the 'points' I am referring to is a single valid serve (double faults to be ignored) with a single winner. The only constraint is that the total number of points played should be between $1000$ and $1500$ for at least $90\%$ of the scenarios (9 in every 10 matches should not have the players play less than $1000$ or more than $1500$ points).","['probability-distributions', 'probability', 'combinatorial-game-theory']"
1129014,Polynomials and difference operator,"Let's consider a difference operator $\triangle f(n)= f(n+1)-f(n)$. How to prove that $f$ is a polynomial so that $deg(f) \leq d$ if and only  if $\triangle f ^{d+1} =0$. First step of the solution might be like this:
$\triangle ^{d+1} f(n)=  \binom {d+1}{0} f(n+d+1)- \binom {d+1}{1} f(n+d) + \binom {d+1}{2} f(n+d-1)- \ldots - \binom {d+1}{2} f(n+1)+ \binom {d+1}{1} f(n) $, but i have no reasonable ideas about how to work with it. The hardest part is to show the inverse statement: if $f$ satisfies   this condition, so it's a polynomial. Could somebody give me  a hint?","['discrete-mathematics', 'binomial-coefficients', 'combinatorics']"
1129036,Evaluating the Definite Integral $\int_0^{\pi}\cos^{2n} \theta d\theta$,"$$\int_0^{\pi}\cos^{2n} \theta d\theta$$ $$u=\cos \theta \implies du= -\sin \theta d\theta \implies d\theta= -\frac{du}{1-u^2} $$ $$\int_{-1}^1 \frac{u^n}{1-u^2} du=\int_{-1}^1 \frac{u^n}{(1-u)(1+u)}du$$ I have no idea what to do next, any guidance is appreciated!","['definite-integrals', 'trigonometric-integrals', 'calculus', 'integration']"
1129047,"Does there exist a space $W$ s.t. $X\to W,\ Y\to W$ are covering spaces when $Z\to X,\ Z\to Y$ are covering spaces?","Suppose $Z\to X,\ Z\to Y$ are covering spaces. Does there exist a space $W$ s.t. $X\to W$ and $Y\to W$ are covering spaces? Reference: Exercise 1.3.11 in page 80 from Allen Hatcher's book Algebraic topology gives an answer. Exercise 1.3.11 Construct finite graphs $X_1$ and $X_2$ having a common finite-sheeted covering space $\widetilde X_1 = \widetilde X_2$ , but such that there is no  space having both $X_1$ and $X_2$ as covering spaces.","['general-topology', 'covering-spaces', 'algebraic-topology']"
1129062,Probability involving chess board,"if 2 cells are chosen at random on a chess board what is the probability that they will have a common side
i tried solving the question by considering different cases for the cells on:
1. corner
2. edge other than corner
3. cell in middle but i guess the cases might be repeating 
so please help","['permutations', 'probability', 'combinatorics']"
1129063,How to solve this ordinary differential equation?,I am just trying to find general solution $$\frac{dy}{dx} = 1 + \sqrt{1 - xy}$$,['ordinary-differential-equations']
1129070,"$a^2 + b^2 + c^2 = 1 ,$ then $ab + bc + ca$ gives =?","In a recent examination this question has been asked, which says: $a^2+b^2+c^2 = 1$ , then  $ab + bc + ca$ gives = ? What should be the answer? I have tried the formula for $(a+b+c)^2$, but gets varying answer like $0$ or $0.25$, on assigning different values to variables. How to approach such question?",['algebra-precalculus']
1129118,Counting squarefree 3-almost primes. (Solved!),"Right so we're gonna define a 3-almost prime as $n = p_1\cdot p_2\cdot p_3$ and a squarefree 3-almost prime is a 3-almost prime such that $p_1\neq p_2 \neq p_3$. My question is this; For a given number $N$ how many squarefree 3-almost primes are there less than or equal to $N$? I'm looking for an exact answer like the one found in this answer for squarefree 2-almost primes. I've also found this paper which looks promising but I have yet to wrap my head around it. Can anybody point me in the right direction? Edit #1: right so having looked around on the internet I've discovered that: $$\pi_3(n) = \sum _{i=1}^{\pi \left(\sqrt[3]{n}\right)} \sum _{j=i}^{\pi
   \left(\sqrt{\frac{n}{p_i}}\right)} \left(\pi \left(\frac{n}{p_i
   p_j}\right)-j+1\right)$$
gives the number of 3-almost primes equal to or less than $n$. I also know that the only way a 3-almost prime can fail to be square free is if one of its prime factors is repeated. So I would think that $f(n) = \pi_3(n) - \pi(\sqrt{n})$ get me somewhere close to what I want because $\pi(\sqrt{n})$ is the number of squarefull 2-almost primes. Edit #2: I have a feeling that I'm going to end up with a function $f(n) = \pi_3(n) - \sum q(m)\pi(\sqrt{m})$ can't quite figure out what it should be yet. Edit #4: I think I've found a working formula. It appears that $$f(n) = \pi_3(n)-\sum _{i=1}^{\frac{n}{6}} \pi
   \left(\sqrt{\frac{n}{p_i}}\right)$$ works. But I'm a little iffy on the upper bound for the summation. I've checked it against precalculated values of f(n) and it appears to work. But I feel that its way to high. Can anybody see an easy a way to reduce it? Edit #5: It appears I can use PrimePi[n] as an upper bound for the sum. Still think its kinda big.",['number-theory']
1129155,Computation of the support of a sheaf,"Shafarevich defines the support of a sheaf $\mathcal{F}$ as the set $X \setminus M$ where $M=\bigcup_{V \subset X} V$ ($V$ open) with $\mathcal{F}(U)=0$ for all nonempty $U \subset V$. After some pages, he asks the following question: is $\overline{\mathcal{F}}$ a coherent sheaf on $Y$, or even a sheaf of $\mathcal{O}-$modules? (where $Y$ is a reduced closed subscheme of $X$ and $\overline{\mathcal{F}}$ is the sheaf obtained after the devissage process) He says no, providing a counterexample; that's to say if $X=\operatorname{Spec}(\mathbb{Z})$ and $\mathcal{F}$ is the coherent sheaf corresponding to the module $\mathbb{Z}/p^2\mathbb{Z}$, with $p$ prime. At this point he writes that the support of this sheaf is the prime ideal generated by $p$ and the corresponding reduced subscheme is $\operatorname{Spec(\mathbb{Z}/p\mathbb{Z})}$.
But I don't know how his calculations work!! Can you help me, please?","['sheaf-theory', 'algebraic-geometry']"
1129170,Invertible Matrices and Linear independence,"If a matrix is invertible, what does this tell us application wise? I am familiar with what it implies in regards to the properties of the matrix, i.e:
the determinant is non-zero, and for a matrix $A$, $Ax=0$ implies $x=0$. However, during discussions in my lectures for an optimization class, a class mate always brings up non-invertible and invertible matrices and what they imply. I can't be too specific as I tend to get lost in discussion whenever this happens, but could anyone clue me in on what the significance of invertibility and linear independence (of which the definitions I am aware of) is? Kind regards.","['matrices', 'linear-algebra']"
1129171,Prove that $f(z)=z^2$ is continuous.,"Prove that $f(z)=z^2$ is continuous for all complex and real values of $z$. What I've got so far is: Given $ \epsilon >0$ and $|z-z_0|<\delta$ after some calculations (which I've checked with the answer key)
$$ |f(z)-f(z_0)|<\delta(\delta+2|z_0|) $$ Beyond this things get difficult when trying to create $\epsilon$ as a function of $\delta$, the answer reads: $$\delta(\delta+2|z_0|)\leq \frac{\epsilon}{3|z_0|}(|z_0|+2|z_0|)=\epsilon $$ and I have no clue how to get there.","['epsilon-delta', 'complex-numbers', 'continuity', 'complex-analysis']"
1129200,Find $\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)dt}{x-\pi}\;$,"$$\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)\,dt}{x-\pi}$$ I don't understand why the limit is not $\infty$ How is the limit: $1$?","['calculus', 'integration', 'limits']"
1129210,Proving that the iterated limit and the two dimensional limit are same,"If $\lim_{(x,y)\rightarrow (a,b)} f(x,y) = L$ and if the one dimensional limits : $\lim_{x \rightarrow a}f(x,y)$ and $\lim_{y \rightarrow b}f(x,y)$ both exist, prove that : $$\lim_{x \rightarrow a} [~\lim_{y \rightarrow b} f(x,y)] =  \lim_{y \rightarrow b} ~[~\lim_{x \rightarrow a }f(x,y)~]$$ Attempt: $\lim_{(x,y)\rightarrow (a,b)} f(x,y) = L$ means limit exists from any direction, whether we take a curved path first along $x=a$ and then come up to $y=b$. Or, we move along $y=b$ and then come upto $x=a$. Hence, we should be able to infer that $\lim_{x \rightarrow a} [~\lim_{y \rightarrow b} f(x,y)] =  \lim_{y \rightarrow b} ~[~\lim_{x \rightarrow a }f(x,y)~]$. However, I wanted to write down a more rigorous proof. Here's how I started off : $\lim_{(x,y)\rightarrow (a,b)} f(x,y) = L$ means, $\forall \epsilon>0, \exists ~\delta $ neighborhood such that $|f(x,y) - l|<\epsilon$ whenever $(x,y) \in \delta$ neighborhood. $\implies l-\epsilon<f(x,y) <l +\epsilon$ whenever $(x,y) \in \delta$ neighborhood. Could anyone please tell me how do I proceed from here? Thank you very much for your help in this regard.","['multivariable-calculus', 'limits']"
1129213,Simplify $\frac{\Gamma(n)}{\Gamma(n+a)}$ with $a\in\mathbb C$.,"How can simplify the following expression?
$$\frac{\Gamma(n)}{\Gamma(n+a)}\sim \cdots\text{ ?}$$
Where $a\in\mathbb C$, $n\in \mathbb N$. Any suggestions please? I propose the following. We have the classical Stirling's
  approximation formula for the Gamma-Function in the form: $$  
\Gamma(z)=\sqrt{2\pi}e^{-z}z^{z-1/2}\left(1+O\left(\frac{1}{|z|}\right)\right)
 $$ for $|\arg(z)|<\pi$ as $|z|\to\infty$. And, from another question here there is also the shifted
  Stirling's approximation for the Gamma-Function due to C. Rowe that
  says: $$  
\Gamma(z+a)=\sqrt{2\pi}e^{-z}z^{z+a-1/2}\left(1+O\left(\frac{1}{|z|}\right)\right)
$$ Therefore $$  
\frac{\Gamma(z)}{\Gamma(z+a)}=\frac{\sqrt{2\pi}e^{-z}z^{z-1/2}\left(1+O\left(\frac{1}{|z|}\right)\right)}{\sqrt{2\pi}e^{-z}z^{z+a-1/2}\left(1+O\left(\frac{1}{|z|}\right)\right)}=\frac{\left(1+O\left(\frac{1}{|z|}\right)\right)}{z^a\left(1+O\left(\frac{1}{|z|}\right)\right)}
$$ and: $$   \frac{\Gamma(n)}{\Gamma(n+a)}\sim n^{-a} \ \ \
(n\rightarrow \infty) $$ I would appreciate some corrections on this procedure. Thanks.","['special-functions', 'complex-analysis']"
1129245,Show that $f(x) = \frac{x^3}{1+x^2}$ is bijective,"This seems like a simple question, but I'm stuck: how do I show that $f: \mathbb{R} \rightarrow \mathbb{R}$ defined as $f(x) = \frac{x^3}{1+x^2}$ is bijective? I want to demonstrate that it is both injective and surjective. To show that it's injective, I need to show that $f(x) = f(y)$ implies $x = y$. However, I can't see a way to reduce $\frac{x^3}{1+x^2} = \frac{y^3}{1+y^2}$ to $x = y$ (since there are no like terms to combine). I'm also unsure of how to prove surjectiveness.","['functions', 'real-analysis']"
1129264,Fatou for weak convergence,"I want to do exercise 3.2.4 from Rick Durett, Probability: Theory and Examples page 86. $$\text{Let } g \geq 0 \text{ be continuous. If }X_n \Rightarrow X_{\infty} \text{ then } \liminf_{n\rightarrow \infty} \mathbb E(g(X_n))\geq \mathbb E(g(X_{\infty}))$$ My attempt: Because $X_n\Rightarrow X_{\infty}$ there exists a random variabel $Y_n$ (with the same distribution as $X_n$ ) which converges to another random variable $Y_{\infty}$ almost surely. So, we have $\liminf_{n \rightarrow \infty}\mathbb E(g(X_n))=\liminf_{n \rightarrow \infty}\mathbb E(g(Y_n))\geq \mathbb E(g(Y_{\infty}))$ by fatou and the continuity of g. But can we say that $\mathbb E(g(Y_{\infty}))=\mathbb E(g(X_{\infty})$ , if yes, then the prove would be finish.","['probability-theory', 'weak-convergence', 'probability']"
1129275,Closed immersions and base change,"Consider a field extension $L\subseteq K$ and two $L$ schemes $X_L$ and $Y_L$ with an embedding $j:X_L\longrightarrow Y_L$. Now take the base changes 
$$X:=X_L\times_{\text{Spec L}}\text{Spec} K$$ 
$$Y:=Y_L\times_{\text{Spec L}}\text{Spec} K$$ We have the following commutative diagram: If we know by hypotesis that $j\times\text{id}:X\longrightarrow Y$ is a closed embedding , then can we conclude that also $j$ was a closed embedding ?","['algebraic-geometry', 'schemes']"
1129312,Second derivative of a convex function in the Itō–Tanaka formula,"This is the form of the Itō–Tanaka formula I have (Revuz and Yor): For $f$ a convex function and $X$ a continuous semimartingale, $$f(X_t)=f(X_0) +\int_0^tf_{-}'(X_s)dX_s+\frac{1}{2}\int_{\mathbb{R}}L_t^a f''(da).$$ What confuses me is how to make sense of $f''$ or rather how to make sense of the second integral. I don't know much functional analysis, but I believe that this 'second derivative' is to be interpreted as a distribution. In which case $\int_{\mathbb{R}}L_t^a f''(da)$ really means $f''(L_t^a)$, where $f''$ is a distribution. But, what's to say $L_t^a$ is a suitable test function?","['stochastic-calculus', 'functional-analysis']"
1129342,Is AR(p) strictly stationar?,"Good evening. Is it true that model AR(p) is a strictly stacionar random sequence? Model AR(p) is given by $X_{t} = \varphi_{1} X_{t-1} + \ldots + \varphi_{p} X_{t-p} + Y_{t}$ where $\{Y_{t}\}_{t \in {Z}}$ is white noise and strict stacionarity means that all finite-dimensional distributions are invariant with respect to shift. If so, can you prove it?","['statistics', 'random-variables']"
1129353,Cauchy riemann eq for anti-holomorphic function.,"I face a problem asking for CR equation for anti-holomorphic function. They ask for three forms: in rectangle coordinate, polar coordinate and complex coordinate. My approach is that : Let $f = u + iv$ be a function which is anti-holomorphic. Let $ \bar{f} = g = s + it$. So $g$ satisfies normal CR in rectangle coordinate, that is, $s_x = v_y, s_y = -v_x$. So $u_x = -v_y, u_y = v_x$ is CR for anti-holomorphic $f$. Also, $rU_r = -V_\theta, rV_r = U_\theta$ is a polar CR for anti-holomirphic. But I do not know what is CR for complex coordinate. Actaully, I am not sure if CR for rectangle coordinate is the same as CR for complex coordinate since complex coordinate can be expressed as xy-coordinate.",['complex-analysis']
1129367,"$X_n \stackrel{d}{\to}X$, $Y_n \stackrel{d}{\to} c \implies X_n+Y_n \stackrel{d}{\to} X+c$","Let $X_n\Rightarrow X$ and $Y_n\Rightarrow c$. Show that $X_n+Y_n\Rightarrow X+c$. Prove: There exists sequences of random variables $(X^{(*)}_n)$ and $(Y^{(*)}_n)$ such that $(X^{(*)}_n)$ and $X_n$ have the same distribution and $X_n\rightarrow X_{\infty}$ a.s. $(Y^{(*)}_n)$ and $Y_n$ have the same distribution and $Y_n\rightarrow Y_{\infty}=c$ a.s. Hence we can conclude that $X_n+Y_n\rightarrow X+c$ almost surely, which implies convergence in distribution. Can someone take a look at it?","['probability-theory', 'weak-convergence', 'probability']"
1129376,Cotangent Fields: Exactness vs. Conservation,"Given a smooth manifold. Then a cotangent field is exact iff conservative:
$$\alpha\in\mathcal{X}^*(M):\quad\alpha=\mathrm{d}h\iff\oint\alpha=0$$
How to prove this properly?","['differential-forms', 'differential-geometry']"
1129381,Injectivity of Homomorphism in Localization,"Let $\alpha:A\to B$ be a ring homomorphism, $Q\subset B$ a prime ideal, $P=\alpha^{-1}(Q)\subset A$ a prime ideal. Consider the natural map $\alpha_Q:A_P\to B_Q$ defined by $\alpha_Q(a/b)=\alpha(a)/\alpha(b)$. Suppose that $\alpha$ is injective. Then is $\alpha_Q$ always injective? I think so, but I'm clearly being too dense to prove it! My argument goes as follows. Let $\alpha(a)/\alpha(b)=0$. Then $\exists c \in B\setminus Q$ s.t. $c\alpha(a)=0$. If $B$ is a domain we are done. If not we must exhibit some $d\in A\setminus P$ s.t. $da=0$. Obviously this is true if $c =\alpha(d)$. But I don't see how I have any information to prove this! Am I wrong and this is actually false? If so could someone show me the trivial counterexample I must be missing? Many thanks!","['examples-counterexamples', 'ring-theory', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
1129382,Lie Derivative of a section on a vector bundle,"I'm still trying to figure out how to do the Lie derivative of a Jacobian. (c.f. earlier unanswered post ). If I know how how to do Lie derivatives on section of vector bundles, that would be sufficient. In my case the vector bundle $\pi:E\rightarrow X$ is given by the dual of the pullback bundle $E=(\phi^*TY)^*$, where $\phi:X\rightarrow Y$ is a smooth map between smooth manifolds. So let $\alpha$ be a section of that bundle. How do I define $\mathcal L_v\alpha$ ? And it would be helpful also to know it in local coordinates. The reason why this would help me to define the Lie derivative of the Jacobian is that the Jacobian is a linear map from $TX\otimes (\pi^*TY)^*$ to $\mathbb R$, i.e.
\begin{equation}
(w,\alpha)_x\mapsto \langle D\phi\, w,\alpha\rangle_x\;
\end{equation}
where locally $w=\sum_iw^i(x)\frac{\partial}{\partial x^i}$ and $\alpha=\sum_b\alpha^b(x)\,dy^b$ ($x^i$ a coords on $X$ and $y^b$ are coords on $Y$) and the pairing is $\langle\cdot,\cdot\rangle_x: T_{\phi(x)}Y \times T^*_{\phi(x)}Y\rightarrow \mathbb R$. Then I could define $\big(\mathcal L_vD\phi\big) (w,\alpha)=\mathcal L_v \big(D\phi(w,\alpha)\big)-D\phi(\mathcal L_v w,\alpha)-D\phi(w,\mathcal L_v\alpha)$, knowing $\mathcal L_v\alpha$.","['multivariable-calculus', 'lie-derivative', 'vector-bundles', 'differential-geometry']"
1129393,"Does this argument suffice to show a ""record"" occurs at time n with probability 1/n?","I think it does, but, in addition to checking for correctness, I'd like to know what other argument we might use. Let $X_1, X_2,...X_n$ be be a sequence of independent identically distributed continuous random variables.  We say a record occurs at time $n$ if $ X_n \gt \max(X_1, .\dots ,X_{n−1})$ . Show $P$ {Record occurs at time n}= $1/n$ . Since the distributions are identical and independent, then all the $n!$ ways to order the $n$ $X_i's$ are equally as probable. Since there are exactly $(n-1)!$ ways to order the $n$ $X_i's$ such that the $n$ th element is the largest, then $P$ {Record occurs at time n}= $(n-1)!/n!=1/n$ .","['probability-theory', 'probability-distributions', 'probability', 'combinatorics']"
1129398,What $n^{\frac{1}{\log_2n}}$ means?,I was confused with about the $n^{\frac{1}{\log_2n}}$ expression. I am not sure how to make mathematical sense of it - i.e. express it in another way for easier understanding. I tried to plug in some numbers like $4$ and $8$. I got the answer $2$ in both cases. Is it what this expression means? $2$?,"['exponentiation', 'logarithms', 'algebra-precalculus']"
1129409,Is the unit ball in this sequence space compact?,"I have a set $X=\{\text{complex sequences } \{x_n\}: \sup\limits_{n}\sqrt{n}\left|x_n\right|\leq 1\}$ equipped with a metric $d(\{x_n\},\{y_n\})=\sup\limits_{n}|x_n-y_n|+\sup\limits_{n}\sqrt{n}|x_n-y_n|$. I want to show that the closed unit ball centered around $\{0\}$ is either compact or not compact, i.e. $N_1=\{\{x_n\}\in X: d(\{x_n\}, \{0\})\leq 1\}$. Closed unit balls are typically not compact (the only case of this is when $X$ is finite dimensional). I'd like to find a sequence in $N_1$, that is a sequence of sequences, that does not have a convergent subsequence. Alternatively, I can show that it isn't complete or totally bounded. The abundance of possibilities, as well as my unfamiliarity with sequence spaces, is confounding me. Can anyone help?","['sequences-and-series', 'functional-analysis', 'real-analysis', 'compactness']"
1129412,How can I prove that a continuous injective function is increasing/decreasing? [duplicate],"This question already has answers here : A continuous, injective function $f: \mathbb{R} \to \mathbb{R}$ is either strictly increasing or strictly decreasing. (5 answers) Closed 4 years ago . If I have a continuous, injective function mapping the real numbers, then it is either increasing or decreasing. This seems intuitively obvious but I can't come up with a neat proof for it.","['functions', 'continuity', 'real-analysis']"
1129415,Wedge product and its dual,"I am learning about differential forms and exterior algebra, and I am trying to get more familiar with the wedge product of vectors. A differential form is an element of $\left( \bigwedge^k (A)\right)^*$, but when they are written out, they are written as $\bigwedge^k (A^*)$, and I'm trying to get a better handle on this correspondence. Suppose $A, B, C,$ and $D$ are finite-dimensional vector spaces. Then there is an isomorphism 
$$
\hom(A, C) \otimes \hom(B,D)\xrightarrow{\sim}\hom(A \otimes B, C \otimes D)\\
\phi \otimes \psi \mapsto (a \otimes b \mapsto \phi a \otimes \psi b).
$$
Choosing $C,D = \mathbb{F}$ gives us a statement about the dual of a tensor product: $(A \otimes B)^* = A^* \otimes B^*$, and in particular, $(A^{\otimes n})^*= (A^*)^{\otimes n}$. We could have known that they were isomorphic just by counting dimensions, but the above theorem seems to give the isomorphism more context and make it somehow natural and basis-free. Now it also happens to be true that $\left(\bigwedge^k (A)\right)^* \cong \bigwedge^k (A^*)$. This is true just by counting dimensions, but I'm trying to understand the correspondence a little better. I'm wondering if there is a theorem like the above to put this correspondence in a broader context. A friend pointed out to me that a dimension-counting argument implies that such a theorem couldn't be much like the form above. Anyway, I'm just interested in understanding the correspondence $\left(\bigwedge^k (A)\right)^* \cong \bigwedge^k (A^*)$ better, and so better grokking how to work with differential forms.","['linear-algebra', 'differential-geometry']"
1129420,$f$ is continuous and open implies $f$ injective,"Question: Let $f: \mathbb R \to \mathbb R$ be continuous and open, that is if $A \subset \mathbb R$ is open then $f(A) \subset \mathbb R$ is open. Prove that $f$ is injective. Attempt: Suppose $f$ is not injective then there exit $x, y \in \mathbb R$ such that $$x < y \implies f(x) = f(y) = c$$ Take the closed inteval $[x,y] \subset \mathbb R$, as $f$ is continuous then $\displaystyle {f|_{[x,y]}}$ is continuous, by Weierstrass Theorem we have that $f$ has a maximum or a minimum point. Let's assume $m = \max \{f(a) ; a \in [x,y]\}$. Now there is  $x' \in [x,y]$ such that $f(x') = m$. If we take the open $(x'-\delta, x'+\delta)$ centered at $x'$ and  we have (1)  If $m = c$ then $f$ is constant on the interval $[x,y]$ and $f((x'-\delta, x'+\delta)) = \{c\}$ which is closed, thus a contradiction; (2)  If $m \neq c$ then we would have $f((x'-\delta,  x'+\delta)) = (b, m]$, where $b$ can also be $b = \infty$. Again a contradiction. Well, this is my least embarassing attempt. I'm not sure how to show $(2)$ $100 \%$. I have also tried to show $f^{-1}f(A) = A$ for any $A \subset \mathbb R$, tried to work on the connected space  $\mathbb R$ by finding a contradiction using the intervals $E_{[f > c]}$ and  $E_{[f < c]}$ open when $A$ is open. Any thoughts? Note: I've already seen this to try something out, but the fact that $f$ is monotone on this exercise comes as a consequence.","['continuity', 'real-analysis', 'analysis']"
1129428,"Find $\int_{-1}^3xf(x)\,dx$ where $f(x)=\min(1,x^2)$","Find: $$\int_{-1}^3xf(x)\,dx,$$ where $f(x)=\min(1,x^2)$. I thought about solving it like this: $$\int_{-1}^1 x^3\,dx + \int_{1}^3x\,dx = \cdots = 4.$$ But the solution is $\frac{26}{3}$
and I don't understand how they got it.","['definite-integrals', 'calculus']"
1129459,What is a fewnomial?,"I came across the theory of ""fewnomials"" (by Khovanskii), which (I guess) are related to polynomials. However, I was surprised that there is no single question on stackexchange concerning fewnomials, and few in mathematical research in general (there are some papers on arXiv). Does anyone know something more about the concept and can please explain me the idea behind fewnomials? Thanks.","['terminology', 'polynomials', 'reference-request', 'analysis']"
1129469,An example of a nilpotent group,"Is there an example of a nilpotent group such that $G/G'$ is (non-trivial) torsion-free while $G$ is not? I cannot think of any example of this kind and I think that it is not proved any result like this, am I wrong?","['examples-counterexamples', 'nilpotence', 'group-theory', 'abstract-algebra']"
1129510,Showing the Existence of Total Derivatives,"I was presented with the following problem regarding a function that has discontinuous partial derivatives:
$$ f(x,y) =\begin{array}{lr} x y \sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0\\ 0 : (x,y) = 0\end{array}$$
I showed the existence of the $x$ partial derivative using two cases when $y$ constant is $0$ and when $y \neq 0$ in conjunction with product, chain, and quotient rule. This results in:
$$\begin{align} \frac{\partial f}{\partial x} (x,y) &= -\frac {2x^2 y \cos(\frac{1}{x^2 + y^2})} {(x^2 + y^2)^2} - y\sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0 \\ \frac{\partial f}{\partial x} (0,0) &= 0 \end{align}$$
I can see that the first part of this piece-wise function does not converge to 0 as $(x,y) \rightarrow 0 $ (in fact it doesn't seem to converge to anything, including $-\infty$). This would imply that the function $f(x,y)$ is not continuously differentiable . However, I am asked to show that this function is differentiable at every point and I have no idea how to do this. At the continuous points I believe I can use chain rule to maybe show differentiability. Would this be a good strategy? I am thinking about setting:
$$ f(x,y) = g(h(x,y)) $$
where :
$$ h(x,y) = (x,y,xy\sin(\frac{1}{x^2+y2})) $$
$$ g(x,y,z) = xyz $$
However, its not clear to me exactly how easy it is to show differentiability of $h$ and $g$ and address the discontinuous point. Sorry if I am missing something trivial. I don't often work with not-so-nice functions. But I would be very grateful if someone could push me in the right direction.","['multivariable-calculus', 'derivatives', 'real-analysis']"
1129545,"If A is connected, is $\bar{A}$ connected?","If A is connected, is $\bar{A}$ connected?
Here $\bar{A}$ is the closure of $A$. Here's my attempt at trying to prove this: Suppose that $\bar{A}$ is disconnected. Then, there exists open, disjoint, non empty subsets $U, V$ such that $U \cup V = \bar{A}$ (This is the definition of disconnectedness that I've learned) Then we can write A as $A = (U \cap A) \cup (V \cap A)$ $U \cap A$ and $V\cap A$ are open in A. Also, they are disjoint since $U$ and $V$ are disjoint. Now if I show that $U \cap A$ and $V \cap A$ are nonempty then I get a contradiction and proof is complete. However, I am not quite sure how to do this? Or is it not true that $\bar{A}$ is connected?",['general-topology']
1129548,Continuity of functions inside a open ball,"Let $ f: X \subset \mathbb{R}^p \to \mathbb{R}^q $ and $ a \in X$. Supose that for all $ \epsilon > 0 $ exists $ g: X \to \mathbb{R}^q $ continuous at $a$ such as $ \| f(x) - g(x) \| < \epsilon $ for all $x \in X$, prove that $f$ is continuous in $a$. Demonstration: Since $g$ is continuous we have: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \ < \delta \implies \| g(x) - g(a) \| < \epsilon $. We know by hypothesis that $ \| f(x) - g(x) \| < \epsilon \; \forall \; x \in X $. We wanto to prove: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \| < \delta \implies \| f(x) - f(a) \| < \epsilon $. My intuition was that the norm/distance (in the metric space ) from $ \| f(a) - f(x) \| \le \| g(x) - g(a) \| - \frac{\epsilon}{2} = r $. But I could not follow up since the distance do no implies that $f(x)$ is inside the ball on $g(x)$","['general-topology', 'metric-spaces', 'continuity', 'real-analysis']"
1129559,"Matrix norm question, normal matrices","Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm (operator norm), if
$$\|A^*A+AA^*\|=2\,\|A^*A\|$$
prove/disprove that $A$ is normal.",['matrices']
1129574,Power Set Mathematics,"If given the following sets $A = \{1,2,3\}$ and $B = \{3,4,5\}$, the power sets of each are the following:
$$\mathfrak P(A) = \{\emptyset,(1),(2),(3),(1,2),(1,3),(2,3),(1,2,3)\}\\
\mathfrak P(B) = \{\emptyset, (3),(4),(5),(3,4),(3,5),(4,5),(3,4,5)\}$$ What is $\mathfrak P(A) \cap \mathfrak P(B)$ $|\mathfrak P(A \cup B)|$ $|\mathfrak P(A) \cup \mathfrak P(B)|$ My guess for the intersection one the first one is just $\{3\}$ and ${\emptyset}$ because it is only used elements in both. I'm not sure about the next two. I do understand how power sets work but im curious on what the difference is between the $\mathfrak P(A \cup B)$ and $\mathfrak P(A) \cup \mathfrak P(B)$. Is there a difference? How do we approach these types of problems?",['elementary-set-theory']
1129597,Problem on Integration: $\Bbb R-\Bbb C$ split and pull back of forms,"This post is not short. However I'm sure that a guy who good handle these concepts, could read and answer in five minutes. I only want to write my attempt, in order to understand where I'm wrong. Let $\Omega\in\Bbb C$ be a domain; $\varphi:\Omega\to[-\infty,+\infty[$ upper semicontinous (i.e. $\varphi(z_0)\ge\limsup_{z\to z_0}\varphi(z)\;\;\forall z_0\in\Omega$). I need to show, given $\bar\Delta_{z_0,r}\Subset\Omega$ (the unitary disk of radius $r$) that
$$
\varphi(z_0)\le\frac1{2\pi r}\int_{\partial\Delta_{z_0,r}}\varphi(s)\;ds
$$
implies
$$
\varphi(z_0)\le\frac i{2\pi r^2}\int_{\Delta_{z_0,r}}\varphi(z,\bar z)\;dz\wedge d\bar z\;\;.
$$ Now what everybody would do is to rewrite the first inequality as $$
(2\pi t)\varphi(z_0)\le\int_{\partial\Delta_{z_0,t}}\varphi(s)\;ds
$$
and then integrate over $]0,r]$ wrt the variable $t$. In this way LHS become easily $\pi r^2\varphi(z_0)$. My problem is with RHS. Roughly I'd write
$$
\int_0^r\int_{\partial\Delta_{z_0,t}}\varphi(s)\,ds\,dt
=\int_{\Delta_{z_0,r}}\varphi(z)\,dz
$$ but I'm not sure it has some sense.
So I started form the other side, and this is what I got:
\begin{align*}
\int_{\Delta_{z_0,r}}\varphi(z,\bar z)\;dz\wedge d\bar z
&=-2i\int_{\Delta_{z_0,r}}\varphi(x,y)\,dx\wedge dy\\
&=-2i\int_{\Delta_{z_0,r}}\varphi(x,y)\,dx\wedge dy
\end{align*} this could be seen in two ways: writing $dz\wedge d\bar z=(dx+idy)\wedge(dx-idy)=-2i(dx\wedge dy)$ or changing variable via the isomorphism $\beta:\Bbb R^2\stackrel{\simeq}{\to}\Bbb C$ defined by $(x,y)\mapsto(x+iy,x-iy)$. What we have, then, is the integral of a $2$-form on a $2$-parametric manifold. So I consider now
$$
\alpha:]0,r[\times[0,2\pi[\longrightarrow\Delta_{z_0,r}\setminus\{z_0\}
$$
defined by
$$
(t,\theta)\longmapsto (\Re z_0+t\cos\theta,\Im z_0+t\sin\theta)
$$ from which we have
$$
-2i\int_{\Delta_{z_0,r}}\varphi(x,y)\,dx\wedge dy=\\
=-2i\int_{]0,r[\times[0,2\pi[}\varphi (\Re z_0+t\cos\theta,\Im z_0+t\sin\theta)
\underbrace{[\partial_t\alpha_1\partial_{\theta}\alpha_2-\partial_t\alpha_2\partial_{\theta}\alpha_1]}_{=\partial\alpha_1\wedge\partial\alpha_2(\partial_t\alpha,\partial_{\theta}\alpha)=t}\,dt\,d\theta\\
=-2i\int_0^rt\underbrace{\int_0^{2\pi}\varphi (\Re z_0+t\cos\theta,\Im z_0+t\sin\theta)\,d\theta}_{=:A(t)}\,dt
$$ And till here it seems (to me!) to have made no mistakes. Now I thought I could work on $A(t)$. Using $\beta^{-1}$ I got
$$
A(t)=\frac i2\int_0^{2\pi}\varphi(z_0+te^{i\theta})\,d\theta
$$
The problem comes now: how to proceed?
I would change variable to this last integral $s=z_0+te^{i\theta}$: in this way it's ALMOST (and almost in Mathematics means wrong) equal to the wanted integral, except for a $ie^{i\operatorname{arg}(s)}$ or something similar (I erased it from my notebook). Where am I wrong? How can I conclude? Many thanks!","['differential-forms', 'integration', 'complex-analysis']"
1129603,In what sense is $p$-adic Hodge theory related to ordinary (complex) Hodge theory?,"I started reading about $p$-adic Hodge theory in the notes of Brinon and Conrad. I quote (page 7): The goal of p-adic Hodge theory is to identify and study various “good” classes of
  $p$-adic representations of $G_K$ for p-adic fields $K$, especially motivated by properties of $p$-adic representations arising from algebraic geometry over $p$-adic fields. Here $G_K$ is the absolute Galois group of the field $K$. This is nice but still a bit vague. It would be nice to know what we are heading for more or less. It is clear to me that from the action of the absolute Galois group on the étale cohomology groups we can obtain $p$-adic representations, and this seems to be the geometric motivation behind the theory. Therefore my question for you: Does $p$-adic Hodge theory provide a splitting of the étale cohomology groups in an analogous way as ordinary Hodge theory does for singular homology of smooth varieties over $\mathbb{C}$? And if so, is this what the theory is set up to do, or is there more to it?","['etale-cohomology', 'homology-cohomology', 'algebraic-geometry', 'hodge-theory', 'galois-representations']"
1129607,What does it mean to extend a function?,What does it mean to extend a function? Can someone please give an example? Thanks in advance!,"['functions', 'real-analysis']"
1129625,Continuity of $\mu \mapsto \mu(E)$ for $\mu$ probability measure and $E$ Borel subset,"Let $X$ be a topological space endowed with the Borel sigma-algebra, let $\mathcal{P}(X)$ be the set of all Borel probability measures on $X$, endowed with the weak* topology. Fix $E$ Borel subset of $X$. Is the function $f\colon \mathcal{P}(X) \to [0,1]$, defined by $f(\mu)=\mu(E)$, a continuous function? Or, at least, is it measurable (wrt the Borel sigma algebra of $\mathcal{P}(X)$)?","['probability-theory', 'measure-theory', 'functional-analysis']"
1129628,Similarity of real matrices over $\mathbb{C}$,"$A  \underset{\mathbb{C}}{\sim} B \overset{\text{def}}{\iff} A=C^{-1}BC, \space C\in M_{n}(\mathbb{C})$ and similarly for $\underset{\mathbb{R}}{\sim}$. I want to prove that $ A \underset{\mathbb{C}}{\sim} B$ for $A,B \in M_{n}(\mathbb{R})$ therefore $A \underset{\mathbb{R}}{\sim} B$. My idea is that elementary divisors of $A,B$ over $\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\overline{z})^k$ is also elementary divisor $\implies$ $A,B$ have same elementary divisors over $\mathbb{R}$. But i think it's not clear.","['matrices', 'linear-algebra']"
1129683,Abelian groups whose automorphism group is a $p$ group,"$\def\Aut{\operatorname{Aut}}$
Let $G$ be a finite abelian group such that $\Aut(G)$ is an $p$ group ,that is, $|\Aut(G)|=p^n$ . Then can we determine the cyclic decomposition of $G$ or at least the order of $G$ ?","['abelian-groups', 'abstract-algebra', 'group-theory', 'finite-groups', 'p-groups']"
1129702,A simple floor function conditional proof,"I have to prove the following: There exists a positive real number $a$ so that for all real numbers $x$ if $x$ -$\lfloor x \rfloor < a$, then $\lfloor 3x \rfloor = 3 \lfloor x \rfloor$. I have attempted to do this in several ways. I got the furthest when I attempted to use three separate cases, where $x$ is an integer, $x$ is a positive real number that is not an integer, and $x$ is a negative real number that is not an integer. In all cases, $x$ was a particular but arbitrarily chosen value of the given type. This approach failed when $x$ was assumed to be a negative real number that was not an integer. Intuitively, I know that $x - \lfloor x \rfloor $ is going to be greater than 0 because $\lfloor x \rfloor$ will always be less than $x$, but I have no idea where to go from there in proving the property. I feel like my attempted approach might have been the wrong one from the start, but I don't know how else to prove this. Thanks in advance for any help.",['discrete-mathematics']
1129709,Why is the graph of 4 nodes and 2 edges not self-complementary?,"I am having some trouble seeing why a graph of 4 nodes and 2 edges is not self-complementary such that $G$ is isomorphic to $\overline{G}$ (G complement) (please see the attachment below). I know that the number of edges in a self-complementary graph must be $\frac{1}{4}n(n-1)$ , (half of that of a $K_n$ graph) and so for a graph of 4 nodes, the number of edges must be 3. However, why is the graph attached not self-complementary? The number of edges, nodes, the degree sequence, length of cycles, etc., are the same, and could I not map $G$ and $G$ complement like: $f(d) = b$ ; $f(b) = d$ ; $f(a) = a$ ; $f(c) = c$ ? I must be missing something about isomorphism here - if someone could point it out, that'd be great - thanks so much!","['graph-theory', 'discrete-mathematics', 'graph-isomorphism']"
1129711,Compactness of the moduli space of bundles with fixed determinant,"The moduli space of semistable holomorphic vector bundles of fixed rank and fixed determinant line bundle on a compact Riemann surface is known to be compact itself.  (In particular, when the rank is $1$, this space is just a single point, in the Picard group.) When the rank is $2$ or more, is there a direct way to see that the moduli space is compact? By ""direct"", I mean I'd like to be able to see this without appealing to flat connections, representations of the fundamental group, or Yang-Mills theory.","['riemann-surfaces', 'algebraic-geometry', 'holomorphic-bundles']"
