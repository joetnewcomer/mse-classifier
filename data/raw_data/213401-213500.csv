question_id,title,body,tags
4314880,Compute the probability that the first $k$ draws are red and the next $n-k$ are green,"At time $0$ there is one red ball and one green ball in the urn. At time $n$ , we draw out a ball chosen at random. We return it to the urn and add one more ball of the color chosen. Compute the probability that the first $k$ draws are red and the next $n-k$ are green. Generalize from this to conclude that if $X_n$ is a fraction of red balls at time $n$ then $P(X_n=k/(n+2))=1/n+1$ for $1\leq k\leq n+1$ . Now suppose we start with $5$ green and $1$ red. Use the stopping theorem to find an upper bound on the probability of $A=\{X_n>1/2\text{ for some } n\geq0\}$ Let $R_n=$ {the $n$ -th ball is red} and $G_n=$ {the $n$ -th ball is blue}. Then we know $P(R_n)=\frac{R}{R+G}=\frac{1}{2}$ and $P(G_n)=\frac{G}{G+R}=\frac{1}{2}$ (where $R$ and $G$ represents the initial number of red and green balls respectively). Then the probability that the first $k$ draws are red would be $\frac{1}{2}^k$ and the probability that the rest are green is $\frac{1}{2}^{n-k}$ . Putting this together, we have $\frac{1}{2}^{k}\frac{1}{2}^{n-k}=\frac{1}{2}^n$ . The Optional Stopping Theorem: Suppose $X_n$ is a submartingale and $N$ is a stopping time. Assume that $X_n$ is bounded or $N$ is bounded. That is, for some $M<\infty$ , $|X_n|\leq M$ with probability $1$ , or $N<M$ with probability $1$ . Then $E[X_1]=E[X_N]$ . If $N\leq n$ with probability $1$ then $E[X_1]\leq E[X_N]\leq E[X_n]$ . I'm not sure where to proceed with this. I am having trouble with this problem. I'd appreciate it if I could get some pointers on this. Thank you.","['stochastic-processes', 'statistics', 'martingales', 'probability']"
4314946,Confused about elements of a set,"Here is an exercise from this book . Suppose that $$A \in B$$ and $$B \in C$$ . Does this mean that $$A \in C$$ ?
Give an example to prove that this does NOT always happen (and explain
why your example works). You should be able to give an example where $$|A| = |B| = |C| = 2$$ . I could not find an example of 3 sets proving this statement. With my beginner understanding of sets, I think this cannot happen. If the set $b$ is an element of the set $c$ , then the elements of $b$ can never be related to $c$ because $b$ is not a subset of $c$ . So $a$ cannot possibly be an element of $c$ . However, from the wording of the question (prove this does NOT always happen), this implies that the statement is usually true but there are cases where this is false. Can anyone help me think of an example where this statement is true or false?",['elementary-set-theory']
4314997,Exterior tensor product of complexes of coherent sheaves,"I am reading the book “Fourier-Mukai transforms in algebraic geometry” by Daniel Huybrechts. In page 119, in Exercise 5.13, with the assumption $P_i\in D^b(X_i\times Y_i)$ , $i=1,2$ , he considers the exterior tensor product $P_1\boxtimes P_2\in D^b((X_1\times X_2)\times (Y_1\times Y_2))$ . My question is that what is this exterior tensor product?","['derived-categories', 'algebraic-geometry']"
4315017,Charaterization of the component of exterior bundle,"Let $(M,J)$ be an almost complex manifold,consider the complexified tangent bundle it has two subbundle $T^{(0,1)}M$ and $T^{(1,0)}M$ ,where $T^{(0,1)}M$ is the eigenbundle of eigen value $i$ associcated to $J$ . Then we can consider the decomposition of the exterior bundle $$\Lambda^{1,0} M:=\left\{\xi \in \Lambda_{\mathbb{C}}^{1} M \mid \xi(Z)=0 \forall Z \in T^{0,1} M\right\}\\\Lambda^{0,1} M:=\left\{\xi \in \Lambda_{\mathbb{C}}^{1} M \mid \xi(Z)=0 \forall Z \in T^{1,0} M\right\}$$ Furthure more we have, $\Lambda^{p,q} = \Lambda^{p, 0} \otimes \Lambda^{0, q}$ Then I need to prove the following charaterization (which appears on Moroianu's Kahler geometry note): $\left.\omega \text { is a section of } \Lambda^{k, 0} M \text { if and only if } Z\right\lrcorner \omega=0 \text { for all } Z \in T^{0,1} M \text {. }$ $\omega$ is a section of $\Lambda^{p, q} M$ if and only if it vanishes whenever applied to $p+1$ vectors from $T^{1,0} M$ or to $q+1$ vectors from $T^{0,1} M$ given $\omega\in \Lambda^{1,0}M$ the $(0,2)$ component of $d\omega$ vanish if and only if for all $Z,W \in T^{0,1}M$ , $d\omega (Z,W) = 0$ My attempt:If we write under the local coordinate the only if part is easy to check,I don't know how to check the if part.","['complex-analysis', 'smooth-manifolds', 'differential-geometry']"
4315044,Can you give me an example of a non compact topological space with compact dense subset?,"Can you give me an example of a non compact topological space with compact dense subset? I know a Hausdorff topological space $(X, \tau)$ with compact dense subset must be compact. Hence, my intuition suggests that there may be a non compact topological space with compact dense subset. And such space will not be Hausdorff space anymore. But it is difficult for me to cite this example. Please explain it in details. Thanks.","['general-topology', 'examples-counterexamples', 'compactness']"
4315048,"If a diffusion is Gaussian, what does it imply to its drift and volatility?","Let $(Y_t)$ be a stochastic process solution to the SDE $$dY_t= \lambda(Y_t,t) dt + \sigma(Y_t,t) dB_t. $$ If we know that $(Y_t)$ is a Gaussian process, what does it inform us on the drift $\lambda$ and the volatility $\sigma$ ? To my knowledge, the only link between the distribution of $(Y_t)$ and its drift/volatility is Fokker-Planck equation: if $p(x,t)$ is the density function of $(Y_t)$ , we have $$\frac{\partial}{\partial t} p(x, t)=-\frac{\partial}{\partial x}[\lambda(x, t) p(x, t)]+\frac 1 2 \frac{\partial^{2}}{\partial x^{2}}[\sigma^{2}\left(x, t\right)p(x, t)]$$ I tried ""brut-forcing"" by plugging in a Gaussian density in the PDE and try to see what contraints on the drift and volatility we get, without much success. EDIT: you can assume $(Y_t)$ to be one-dimensional for simplicity, but a general answer would be more than welcome.","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4315081,Inverse Rouché problem?,"The problem on one of my mock-exams is as follows: Suppose both $f$ and $g$ are entire with both a finite amount of zeros that all lie in a disk $D_r(0)$ . The zeros are counted by their multiplicity. Suppose that the amount of zeros of both function is is equal. Prove that there exists an entire function $h$ that has no zeros such that there is a sufficiently large $R > r$ for which it holds that $|f(z) - h(z)g(z)| < |f(z)|$ for all $|z|=R$ . A sub question I have to this is why do we need the condition that the amount of zeros is equal? Why can $h$ and $R>r$ not exist if the number of zeros aren't equal? I suppose this has to to with some kind of inverse of Rouché's theorem, because we're starting from to fact that the number of zeros is equal and we want to prove an inequality. $\textbf{Rouché's Theorem:}$ Suppose $f$ and $g$ are holomorphic in an open set containing a circle $C$ and its interior. If $|f(z)| > |g(z)|$ for all $z \in C$ then $f$ and $f+g$ have the same amount of zeros inside the circle $C$ . Could anyone give a hint? Thanks! This problem is similar to Inverse statement to Rouché's theorem in complex analysis.",['complex-analysis']
4315122,Probability for any ball to be picked X times.,"My team member and I came across a problem in a non-Math-related research area. I am paraphrasing the problem here in order to seek your help: There are 10000 different numbered balls in a bowl. Each time, 10
balls are picked at random from the bowl and put back into the bowl.
This process is repeated 5 times. What is the
probability that at least one numbered ball is picked at least 3
times? Our first idea is to use cumulative Binomial probability to find the probability that a particular ball is picked more than 2 times. But this only gives the probability from the ""perspective of a single ball"". How can we expand this to seek the answer we need? We highly appreciate any help regarding this.","['permutations', 'combinations', 'probability']"
4315143,The anti-derivative of any matrix function,"If we have some differentiable function $f:\mathbb{R}^n\mapsto\mathbb{R}^m$ , we can always calculate the Jacobian of this function, i.e., $$ \frac{df}{dx}(x) = \begin{pmatrix}\frac{\partial f_1 }{\partial x_1 } & \cdots & \frac{\partial f_1 }{\partial x_n } \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m }{\partial x_1 } & \cdots & \frac{\partial f_m }{\partial x_n } \end{pmatrix} $$ So, for example if $f=\begin{pmatrix} x_1+x_2^2 \\ \sin(x_1 x_2) \end{pmatrix}$ , we have $$ F(x) = \frac{df}{dx}(x) = \begin{pmatrix} 1 & 2 x_2 \\ x_2 \cos(x_1x_2 ) & x_1 \cos(x_1x_2 ) \end{pmatrix}. $$ My question is, can we find an $f$ for any $F(x)$ ? In other words, is any matrix function of the form $$F(x_1, \ldots, x_n) = \begin{pmatrix} F_{1,1}(x_1, \ldots, x_n) & \cdots & F_{1,n}(x_1, \ldots, x_n) \\ \vdots & \ddots & \vdots \\ F_{m,1}(x_1, \ldots, x_n) & \cdots & F_{m,n}(x_1, \ldots, x_n) \end{pmatrix}, \quad (F_{i,j}\text{ is assumed to be continuous}) $$ a Jacobian matrix for some mapping $f$ ? On one hand this seems trivial, on the other hand I cannot find anything useful. I already tried to have it row by row, but finding the integral basically from a freaky row-vector is not really insightful... (my concern is that if I take $F(x)$ as some super weird matrix function there will not exist an $f$ ...) (NB: please add some reference or keywords in your answer)","['integration', 'multivariable-calculus']"
4315197,Projection onto the convex hull of the set of permuted values of a given vector,"Let $v$ be a vector in $\mathbb{R}^n$ , such that $v_i \geq 0$ . I am interested in the convex hull of $\{v_{\sigma} : \sigma \in \Sigma_n\}$ where $\Sigma_n$ is the set of permutations of $\{1,...,n\}$ and $v_{\sigma} = (v_{\sigma(1)},... ,v_{\sigma(n)})$ . I know that the convex hull of permutation matrices is well-studied (since it is the Birkhoff polytope) and that projections onto it are possible via the Sinkhorn-Knopp algorithm. What about $C = \text{conv}\{v_{\sigma} : \sigma \in \Sigma_n\}$ ? We can write $C$ as $C=\{Qv : Q \text{ is doubly stochastic}\}$ , but are there simpler ways than existing algorithms to project onto $C$ (w.r.t. the Euclidean distance)? Thanks in advance!","['matrices', 'projection', 'convex-analysis', 'permutations']"
4315275,I just have some questions in regards to how equations of sets work.,"I've been looking at these systems of set equations and the way they work has been progressively more confusing especially with how I keep on recieving information that seems to be all over the place. Example: I'm currently solving a system of 2 set equations: $$X \cup (A\cap B) = A$$ $$B\backslash X = B\backslash A$$ (Note: I'm supposed to find all X that will solve this equation) Anyway, I've done some steps, and my equation looks like spaghetti so if I make a mistake whilst typing this out, or if there's something wrong in general I apologise. We were given some pointers how to solve this, and this is the exact reason why I'm lacking some knowledge since we didn't actually solve any of these problems in class for some reason, but yeah:\ Apparently first I'm supposed to change the equation L = R to an equation of the form $L\oplus R = \emptyset $ \ And then I'm supposed to add all equations together so that all equations of form $S_i=\emptyset$ end up in the equation: $\cup_{i}S_i = \emptyset $ I'm supposed to put the equation into a form of unions of intersections and/or their complements. (I won't list more, as this is about where I run into problems) $$X\cup (A\cap B) = A \rightarrow (X\cup(A\cap B))\oplus A = \emptyset$$ and $$B\backslash X = B\backslash A \rightarrow (B\cap X^c)\oplus (B\cap A^c) = \emptyset$$ I don't really understand why we do this next thing, but essentially, when you have that $\oplus$ symbol, you can do the following: $$(X\cup(A\cap B))\oplus A = \emptyset \rightarrow ((X\cup (A\cap B))^c\cap A)\cup ((X\cup(A\cap B))\cap A^c)$$ and the same for the other one $$(B\cap X^c)\oplus (B\cap A^c) = \emptyset \rightarrow ((B\cap X^c)^c\cap(B\cap A^c))\cup ((B\cap X^c)\cap(B\cap A^c)^c)$$ Then from there we have to make it into a single equation $$((X\cup (A\cap B))^C\cap A)\cup((X\cup(A\cap B))\cap A^c)\cup ((B\cap X^c)^c\cap(B\cap A^c))\cup ((B\cap X^c)\cap(B\cap A^c)^c) =\emptyset $$ And then comes the part where I tend to get stuck, which is simplification: $$((X^c\cap (A^c \cup B^c))\cap A)\cup ((X\cup(A\cap B))\cap A^c)\cup ((B^c\cup X)\cap(B\cap A^c))\cup ((B\cap X^c)\cap (B^c\cup A))$$ From this point out I tend to run into a lot of problems as this wasn't properly explained to us because as I've said we haven't really solved any problems of this type, for example, just looking at the first bracket I get: $$((X^c \cap A^c)\cup (X^c \cap B^c) \cap A)$$ (at least this is what I think I get, I'm not really sure about it) Here are my questions (finally): First: Can I move the $\cap A$ into the 2nd bracket of this expression I obtained without messing anything up? Second; is my distribution of $X^c\cap (A^c\cup B^c)$ even correct? Third: How do I change the 2nd bracket (from the super long equation) in a way that I end up with intersections inside brackets instead of unions?
(Bracket I'm referring to:) $$(X\cup (A\cap B))\cap A^c$$ Honestly if someone provides the entire solution that'd be the best possible outcome, but after typing this out I really don't wish that on anyone.\ Anyway, those are all the actual questions I have in regards to this specific problem, I had a couple more before this, but I figured them out whilst writing this.
If by any chance you made it to the bottom thanks I guess. I have an exam tomorrow, I've been studying for like 2 weeks with like 6 hours a day and I just can't seem to figure out some things I probably should've figured out by now, so I'll be posting multiple questions, so if you have time please check out the rest as well when they are up.",['elementary-set-theory']
4315323,Sufficient and necessary conditions for a smooth function to have a Fourier transform with rapid decay,Let $f \in C^\infty(\mathbb R) \cap L^1(\mathbb R)$ . I want to learn necessary and sufficient conditions for the Fourier transform $$\hat f (y) := \int_{\mathbb R} f(x) \exp(-2 \pi i xy) dx$$ to have rapid decay. Meaning that we have $$\lim_{|y| \to \infty} y^k \hat f(y) = 0$$ for all $k \in \mathbb N_0$ . Using the Riemann–Lebesgue lemma and the fact $$\hat{f^{(k)}}(y) = (2 \pi i y)^k \hat f(y)$$ in case $f^{(k)} \in L^1$ we get $\hat f$ has rapid decay if all derivatives of $f$ are in $L^1$ . So this condition is sufficient . Is it also necessary ? Which other sufficient and necessary conditions are there?,"['derivatives', 'fourier-transform', 'fourier-analysis', 'real-analysis']"
4315471,"$f(X)$ measurable, but $f$ not measurable","Let $(\Omega,F,P)$ be a probability space.
Suppose $f\colon \mathbb R\to\mathbb
R$ is some function such that $f(X)$ is measurable for every real valued random variable $X$ . I am curious if $f(X)$ is necessarily $\sigma(X)$ -measurable. I tried to conclude with Doob-Dynkin lemma but to do we would need $f$ to be $\mathcal{B}(\mathbb R)$ measurable. Does someone has an idea or is this is false in general?","['measure-theory', 'probability-theory']"
4315489,Prove that $\ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|}$ is always positive,"I have to study when the function $$f'(x) = \ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|}$$ is positive. I tried to use the inequality $\ln(1 + t) < t$ , $\forall t > -1$ but this could not help me ( $\frac{1}{|x|} - \frac{1}{1 + |x|}$ is always a positive quantity but I cannot say anything about $f'(x)$ ). Is there a quick method? Thanks.","['inequality', 'derivatives']"
4315512,Bound L^2 norm of gradient by L^infinity norm,"For $u\in H^1_{loc}(\mathbb{R}^2)$ a weak solution to $$-div(a\cdot \nabla u) = 0$$ with $a_{ij}$ constant and strongly ellipctic, we showed that $$\int_{B(x_0,s)} |\nabla u|^2 dx \leq \left(\frac{2s}{r}\right)^\alpha \int_{B(x_0,r)} |\nabla u|^2 dx $$ for some constant $\alpha > 0$ . Our goal is now to show a version of Liouvilles theorem i.e. that if in addition $u\in L_\infty(\mathbb{R}^2)$ , $u$ must already be constant. My idea is to bound $\int_{B(x_0,r)} |\nabla u|^2 dx$ somehow and then let $r\to\infty$ . This way we would get for all $x_0$ and $s$ that $u$ is constant on $B(x_0,s)$ etc. The problem is now to bound the integral. I thought to this end we want to show $$||\nabla u ||_{L^2(V)} \leq C$$ for some constant $C$ and all compact $V$ , where I assumed that it would be something like $c\cdot ||u||_\infty$ for some other constant $c$ . But as pointed out in the answers, there are counterexamples. It feels like I‘m mentally blocked (or as we would say in Germany „ich hab ein Brett vorm Kopf“). Thanks for the help!","['normed-spaces', 'sobolev-spaces', 'functional-analysis']"
4315572,Multiplicity and set of zeros.,"exercise: Let us assume that the function f has derivatives of all orders. Suppose that all zeros of $f$ have finite multiplicity. Let $a$ and $b$ be points of $A$ , such that $a<b$ and neither point is a zero. Show that $f$ has at most finitely many zeros in $] a, b[$ (We say that a point $c$ is a root of $f(x)=0$ with multiplicity $m$ , if $f^{(k)}(c)=0$ for $k=0, \ldots, m-1$ and $f^{(m)}(c) \neq 0 .$ As usual $f^{(0)}$ denotes $f$ .) lemma: A zero of finite multiplicity is an isolated point of the set of zeros proof: Considering Taylor polynomial $E(h)=f(c+h)-\left(f(c)+\frac{1}{1 !} f^{\prime}(c) h+\frac{1}{2 !} f^{\prime \prime}(c) h^{2}+\cdots+\frac{1}{m !} f^{(m)}(c) h^{m}\right)$ and using L’Hopital’s Rule we can show that $\lim\limits_{h \rightarrow 0} \frac{E(h)}{h^{m}}=0$ . Because of the multiplicity and for $h \neq 0$ we can write $$
\frac{f(c+h)-f(c)}{h^{m}}=\frac{1}{m !} f^{(m)}(c)+\frac{E(h)}{h^{m}}
$$ and from this we can deduce the proof of the lemma. So if we know if all zeros of $f$ have finite multiplicity then all zeros are isolated. Meanwhile we have bounded interval we can use Bolzano–Weierstrass theorem to show that if f have infinitely many zeros then at least one of the zeros should be limit point which contradicts to be isolated. But why I do need $f(a)$ , $f(b)$ not being zero which was declared in the exercise.
I am suspicious about using intermediate value theorem and taylor polynomial with remainder. But I don't know how to do it?? Remark: ( May be it can be some kind of hint which I cannot figure out: next exercise asks ::: In the previous exercise, if f (a) and f (b) have the same sign, show that the number of zeros in ]a, b[, counted by multiplicity, is even. If $f(a)$ and $f(b)$ have opposite signs, show that the number of zeros in ]a, b[, counted by multiplicity, is odd)","['analysis', 'real-analysis', 'calculus', 'functions', 'derivatives']"
4315659,Question about an exercise in my Measure Theory book.,"There's an exercise in my book that states the following: Let $\mu_0 (A) = \mu(A \cup A_0),\,A \in \mathcal{F}$ be a measure. Show that if $$\int f \,d\mu$$ exists, then $$\int_{A_0} f\,d\mu = \int f\,d_{\mu_0}$$ However, I think there's a typo in this exercise. $\mu_0$ isn't even a measure when $\mu(A_0) \neq 0$ , since $\mu_0(\varnothing) = \mu(A_0)$ . If it were $\mu_0(A) = \mu(A \cap A_0)$ , which I think that's what the author meant, I could prove it in the following way: For simple functions, $$\int_{A_0}f\,d\mu = \int f\mathbf{1}_{A_0}\,d\mu = \sum_{i=1}^{n} f_i \mathbf{1}_{A_0}\mu(A_i) =\sum_{i=1}^{n} f_i\mu(A_i \cap A_0) = \sum_{i=1}^{n} f_i \mu_0(A_i) = \int f\,d\mu_0$$ For a non-negative measurable function $f$ , there's a non-decreasing sequence of simple functions $(s_n)_{n \in \mathbb{N}}$ such that $s_n \rightarrow f$ , then $$\int_{A_0} f\,d\mu= \int f\mathbf{1}_{A_0}\,d\mu = \lim_{n \rightarrow \infty} \int s_n\mathbf{1}_{A_0}\,d\mu = \lim_{n \rightarrow \infty} \int s_n\,d\mu_0 = \int f\,d\mu_0$$ And finally, for any measurable function, we just use $f = f^{+} - f^{-}$ . So, which one is it, $\mu(A \cup A_0)$ or $\mu(A \cap A_0)$ ?","['measure-theory', 'lebesgue-integral']"
4315675,there are infinitely many numbers $n$ so that $2^n$ ends in $n$,"Prove that there are infinitely many numbers $n$ so that $2^n$ ends in $n$ . I was thinking of using induction. I know that $2^{36}$ ends in $36$ and $2^{736}$ ends in $736$ . Indeed, one can see that $2^{100}\equiv 1\pmod{125}$ by the Euler Fermat theorem so $2^{103}\equiv 8\pmod{1000}$ . Hence $2^{736} = (2^{103})^{7}\cdot 2^{15} \equiv 8^7\cdot 2^{10}\cdot 2^5\equiv (2^{10})^2\cdot 2\cdot 24\cdot 32\equiv 24^3\cdot 64\equiv 824\cdot 64\equiv 736\pmod{1000}$ . Thus the proof will be complete if I can show that if $2^{n}$ ends in $dn$ , where $d$ is the digit before $n$ , then $2^{dn}$ ends in $dn$ , but I'm not sure how to show this inductive step. Perhaps the Euler-Fermat theorem or Fermat's Little theorem might be useful? Edit: Yes, it seems like this question might be a duplicate but I think that this question needs more justification than the one linked in the first comment below. In particular, I don't see enough justification in the answers for that question as to why certain numbers of the form $n=100l+36$ satisfy $2^n$ ends in $n$ .","['contest-math', 'number-theory', 'induction', 'elementary-number-theory']"
4315679,"$\mathbf{E}[\max\{X, a\}] \geq \max\{\mathbf{E}[X], a\}$","Let $X$ be a random variable with finite mean, (i.e., $\mathbf{E}[X] < \infty$ ), and $a \in \mathbb{R}$ . Prove that $$ \mathbf{E}[\max\{X, a\}] \geq \max\{\mathbf{E}[X], a\}. $$ My notions are to use either $\mathbf{E}[X] = \int_{-\infty}^{\infty} xf(x) \, \mathrm{d}x$ where $f$ is the probability density function of $X$ and $\mathbf{P}(X \leq a) = \int_{-\infty}^{a} f(x) \, \mathrm{d}x$ , or use the indicator method $\mathbf{E}[I_X] = \mathbf{P}(X)$ to substitute into the original inequality. However, I got stuck midway in both approaches. Thanks in advance for your help. I would highly appreciate it if a proof based on basic theorems of probability/expectation is provided.","['integration', 'probability-theory']"
4315751,$\lim\limits_{x \to \infty} \frac{\ln x}{x} =0$,"I wanted to prove $\lim\limits_{x \to \infty} \frac{\ln x}{x} =0$ by the squeeze theorem. I know $$e^x =1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\dots$$ by Taylor expansion, so $e^x \ge 1+x$ and thus $x \ge \ln(1+x)$ and finally, $x \ge \ln(x)$ . I also know $\ln x \ge 0$ for $x\geq 1$ . So I tried to apply the squeeze theorem $0 \le \ln x \le x$ , and then dividing throughout by $x$ I get $0 \le \frac{\ln x}{x} \le 1$ and then if I apply limit $x \to \infty$ , but I'm unable to apply the squeeze theorem here. Can anybody help me understand what is it that I'm doing wrong?
I do not want to use L'Hospital's theorem.","['limits', 'calculus', 'limits-without-lhopital', 'real-analysis']"
4315858,"Seeking for help to find a formula for $\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{n}}$, where $a>1.$","When tackling the question , I found that for any $a>1$ , $$
I_1(a)=\int_{0}^{\pi} \frac{d x}{a-\cos x}=\frac{\pi}{\sqrt{a^{2}-1}}.
$$ Then I started to think whether there is a formula for the integral $$
I_n(a)=\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{n}}, $$ where $n\in N.$ After trying  some substitution and integration by parts, I still failed and got no idea for reducing the power n. After two days, the Leibniz Rule for high derivatives come to my mind. Differentiating $I_1(a)$ w.r.t. $a$ by $(n-1)$ times yields $$
\displaystyle \begin{array}{l}
\displaystyle \int_{0}^{\pi} \frac{(-1)^{n-1}(n-1) !}{(a-\cos x)^{n}} d x=\frac{d^{n-1}}{d a^{n-1}}\left(\frac{\pi}{\sqrt{a^{2}-1}}\right) \\ \displaystyle 
\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{n}}=\frac{(-1)^{n-1} \pi}{(n-1) !} \frac{d^{n-1}}{d a^{n-1}}\left(\frac{1}{\sqrt{a^{2}-1}}\right) \tag{*}\label{star}
\end{array}
$$ I am glad to see that the integration problem turn to be merely a differentiation problem. Now I am going to find the $(n-1)^{th} $ derivative by Leibniz Rule. First of all, differentiating $I_1(a)$ w.r.t. $a$ yields $$
\left(a^{2}-1\right) \frac{d y}{d a}+a y=0 \tag{1}\label{diffeq}
$$ Differentiating \eqref{diffeq} w.r.t. $a$ by $(n-1)$ times gets $$
\begin{array}{l}
\displaystyle \left(a^{2}-1\right) \frac{d^{n} y}{d a^{n}}+\left(\begin{array}{c}
n-1 \\
1
\end{array}\right)(2 a) \frac{d^{n-1} y}{d a^{n-1}}+2\left(\begin{array}{c}
n-1 \\
2
\end{array}\right) \frac{d^{n-2} y}{d a^{n-2}}+x \frac{d^{n-1} y}{d a^{n-1}}+(n-1) \frac{d^{n-2} y}{d a^{n-2}}=0
\end{array}
$$ Simplifying, $$
\left(a^{2}-1\right) y^{(n)}+(2 n-1) ay^{(n-1)}+(n-1)^{2} y^{(n-2)}=0 \tag{2}\label{diffrec}
$$ Initially, we have $ \displaystyle y^{(0)}=\frac{1}{\sqrt{a^{2}-1}}$ and $ \displaystyle y^{(1)}=-\frac{a}{\left(a^{2}-1\right)^{\frac{3}{2}}}.$ By \eqref{diffrec}, we get $$
y^{(2)}=\frac{2 a^{2}+1}{\left(a^{2}-1\right)^{\frac{5}{2}}}
$$ and $$
\displaystyle  y^{(3)}=-\frac{3 a\left(2 a^{2}+3\right)}{\left(a^{2}-1\right)^{\frac{7}{2}}}
$$ Plugging into \eqref{star} yields $$
\begin{aligned}
\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{3}} &=\frac{\pi}{2} y^{(2)}=\frac{\pi\left(2 a^{2}+1\right)}{2\left(a^{2}-1\right)^{\frac{5}{2}}} \\
\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{4}} &=-\frac{\pi}{6} \cdot \frac{3 a\left(2 a^{2}+3\right)}{\left(a^{2}-1\right)^{\frac{7}{2}}} =-\frac{\pi a\left(2 a^{2}+3\right)}{2\left(a^{2}-1\right)^{\frac{7}{2}}}
\end{aligned}
$$ Theoretically, we can proceed to find $I_n(a)$ for any $n\in N$ by the recurrence relation in $(2)$ . By Mathematical Induction, we can further prove that the formula is $$
\int_{0}^{\pi} \frac{d x}{(a-\cos x)^{n}}=\frac{\pi P(a)}{\left(a^{2}-1\right)^{\frac{2 n-1}{2}}}
$$ for some polynomial $P(a)$ of degree $n-1$ . Last but not least, how to find the formula for $P(a)$ ?  Would you help me?","['integration', 'trigonometry', 'induction', 'trigonometric-integrals']"
4315926,Least-squares solution to system of equations of $4 \times 4$ matrices with $2$ unknown matrices,"This question is in the context of a robotics problem. The goal is to track a robot using both its onboard odometry system and a VR system (HTC Vive Pro) using a VR controller mounted to the robot. What is known is the transformation between odometry origin and the robot (measurements $A_n$ ) and between the VR origin and the controller (measurements $C_n$ ). Ignoring both systems' inaccuracies for now, we can also assume the robot's pose according to both systems is identical ( $I$ ). Driving around will result in many pairs of measurements ( $A_n$ , $C_n$ ). What is unknown is the fixed transformation $B$ between the two coordinate systems and the fixed transformation $D$ between the robot's origin and the VR controller. All transformations involved are proper rigid. The chain of transformation looks as follows. This leaves us with an equation system of $4 \times 4$ homogeneous transformation matrices $$A_n \cdot B \cdot C_n \cdot D = I$$ with $I$ being the $4 \times 4$ identity matrix and $(A_n, C_n)$ being (many) pairs of measurements. I am looking for the optimal (least-squares, I suppose) solution for $B$ and $D$ , so that the equation holds approximately true for all pairs of measurements.","['robotics', 'matrices', 'least-squares', 'linear-algebra', 'optimization']"
4315929,Show that Newton update in two variables converges,"I am having two update rules of a two variable function $f(x, y)$ , i.e., $$x \mapsto h_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x,y)} \tag{1}$$ $$y \mapsto h_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x,y)} \tag {2}$$ We know that $(1)$ and $(2)$ converge when applied seperately. My goal is to prove that
when these rules are applied alternatively they do find a root for $f(x,y)$ and converge to a stationary point. In [1] , it is shown that $(1)$ and $(2)$ are contractions mappings. I am trying to understand if $$H(x,y) = \begin{bmatrix} h_1(x,y) \\ h_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x, y)}{\partial_y f(x,y)} \end{bmatrix}$$ is a contraction mapping. Following the same procedure as in [1] we have $$\begin{aligned}\|H(x,y) -H(x',y') \| = &  \left\| \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x,y)}{\partial_y f(x,y)} \end{bmatrix} - \begin{bmatrix} x' \\ y'\end{bmatrix} + \begin{bmatrix} \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix}\right\|  \\ = & \left\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} -  \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x,y)}{\partial_y f(x,y)} - \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix} \right\|. \end{aligned}\tag{3}$$ Following the same procedure as in [1] , I am trying to apply mean value theorem with two variables but I can not show that $$\| H(x,y) - H(x',y')\| \leq k~ \| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{4}$$ for some appropriate $k$ . Could you please someone help to show that $H(x, y)$ is (or it is not) a contraction mapping? EDIT1 : If we use the modified Newton method we have $$x \mapsto c_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x^0,y^0)} \tag{5}$$ $$y \mapsto c_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x^0,y^0)} \tag {6}$$ By mean value theorem we have $$\| C(x,y) - C(x',y')\| \leq \| \nabla C(x,y)^T \|\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{7}$$ where $$C(x,y) = \begin{bmatrix} c_1(x,y) \\ c_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x^0,y^0)} \\ \frac{f(x, y)}{\partial_y f(x^0,y^0)} \end{bmatrix}\tag{8}$$ Also, I have that $\partial_x f(x, y)$ and $\partial_y f(x, y)$ are decreasing and positive. Thus, there exists $m1,m_2$ and $M_1, M_2$ such that $$0 < m_1 < \partial_x f(x,y) < \frac{M_1}{2} \tag{9}$$ and $$0 < m_2 < \partial_x f(x,y) < \frac{M_2}{2}, \tag{10}$$ where $M_1 = \max_x \{ \partial_x f(x,y) \}$ and $M_2 = \max_y \{ \partial_x f(x,y) \}$ . Thus, we need to show that $ \| \nabla C(x,y)^T \|<1$ in order $C(x,y)$ to be a contraction mapping. We have $$ \| \nabla C(x,y)^T \| = \| I - \begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix} \| \leq 1 + \| \underbrace{\begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix}}_{A} \|.\tag{11}$$ The eigenvalues of $A$ are the roots of $$det(A -\lambda I) = 0.\tag{12}$$ Doing computations, we conclude that $$\lambda (\lambda - \partial_x f(x,y) / M_1 - \partial_y f(x,y) / M_2 ) = 0 \tag{13}$$ which gives $$\lambda = 0 \tag{14} \Rightarrow \| \nabla C(x,y)^T \|<1 $$ and $$\lambda = \partial_x f(x,y) / M_1 + \partial_y f(x,y) / M_2 < 1 \Rightarrow \| \nabla C(x,y)^T \|< 2 \tag{15}$$ where $(9)$ and $(10)$ are used in the last inequality. If we keep $\lambda = 0 $ , $C(x,y)$ is a contraction by not in the case where $\lambda < 1$ . Not a satisfactory result. Can we do better? Any comments are highly appreciated. EDIT2: Please let me add some more information of the problem I am facing. Let $t \colon \mathbb{R}^2 \to \mathbb{R}$ . I am trying to find a way to get the roots of the nonlinear equation $$f (x,y )= 1/ \| t (x, y)\| - 1/ y \tag{16}$$ subject to $x \geq x_0$ and $y \geq y_0$ . As far as I understand this is a single equation with two unknowns. Maybe this is the reason the mapping is not a contraction. Any information to understand the general behavior of $(16)$ along with the aforementioned discussion is highly appreciated.","['newton-raphson', 'contraction-operator', 'multivariable-calculus', 'linear-algebra', 'optimization']"
4315954,Prerequisites for Condensed Mathematics and Analytic Geometry,I'm a student in Algebraic Geometry. I've read chapter 2 and 3 of Hartshorne. I want to study the theory of Condensed Mathematics and Analytic Geometry by Scholze and Clausen. What are the basic prerequisites for understanding the theory? How much of Topos Theory is needed? Just the basic definitions? Can you give me some references? Is the theory of $\infty$ -Categories and/or Derived Algebraic Geometry needed? Many Thanks,"['p-adic-number-theory', 'algebraic-geometry', 'nonarchimedian-analysis', 'reference-request']"
4315961,"To prove that the set of all polynomials with coefficient of $x^2$ equal to $0$ is dense in $C([0,1])$.","Let $C([0, 1])$ denote the metric space of continuous real valued functions on $[0, 1]$ under
the supremum metric - i.e., the distance between $f$ and $g$ in $C([0, 1])$ equals $\sup\{|f(x) − g(x)| : x \in [0, 1]\}.$ Let $Q \subseteq C([0, 1])$ be the set of all polynomials in $\mathbb{R}[x]$ in which the coefficient of $x^2$ is $0$ . Then $Q$ is dense in $C([0, 1])$ . The set of all polynomials is dense in $C([0,1])$ using Stone-Weierstrass theorem. If I prove that the set of all polynomials with coefficient of $x^2$ equal to $0$ is dense in the set of all polynomials then I'll be done. How to prove this?","['functional-analysis', 'analysis']"
4315967,Prove/Disprove: $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ is a probability density function,"Question :
Assume that $f_X(x)$ is the probability density function of a random variable $X$ such that $X\in[a,b]$ for $b\gt a$ . Does there exist another random variable $Y\in[a,b]$ having $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ as its
probability density function? Note: There reason I'm asking this question is that I want to treat $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y)$ as the pdf of a new random variable like $Y$ and somehow change the measure I'm working with (cause dealing with that $Y$ is much more simple for me in some situation). I believe the "" Radon–Nikodym derivative (density) "" should be related to my question. However, I'm not very familiar with the probability measures. My try : I know that the integral of the probability density function over the whole support of the variable should be equal to $1$ . So I tried to prove that: $$
\int_{a}^{b} \dfrac{e^{2y}}{\mathbb{E}[e^{2X}]} f_X(y) dy =1
$$ However, I do not know how to deal with the fraction $\dfrac{e^{2y}}{\mathbb{E}[e^{2X}]}$ in the integral. We know that $\int_a^b f_X(x)dx=1$ . I cannot see what happens when the fraction is multiplied, and how it affects the whole integral.","['probability', 'density-function']"
4315970,Poker and Combinatorics (Don't Mix): How to solve this problem?,"Does anyone know the solution to this problem? I'm not sure if it's asking to find the cardinality first, and whether I should use the additive counting principle or the multiplicative one? Would help me a lot, thanks! Given is a standard $52$ -card poker game. A hand is a non-ordered selection of five Cards from the game. Consider the following question and the answer to it: Question: How many hands are there that have all four colors? Answer: We choose a card from each color, so there are $(13^4)$ possibilities for this. $48$ possibilities remain when we choose the fifth card. So there are a total of $(13^4) \times 48$ hands that contain all four suits. Is the solution correct? If not, what is wrong with the answer and what is the correct solution?","['permutations', 'logic', 'combinatorics', 'discrete-mathematics']"
4315972,A doubt about a proof of chain rule for smooth functions between smooth manifolds,"I'm reading Theorem 1.1 in this lecture notes. Theorem 1.1 (Chain Rule for Manifolds). Suppose $f: X \rightarrow Y$ and $g: Y \rightarrow Z$ are smooth maps of manifolds. Then: $$
\mathrm{d}(g \circ f)_{x}=(\mathrm{d} g)_{f(x)} \circ(\mathrm{d} f)_{x}
$$ Proof. If $\varphi$ is a local parameterisation of $x, \psi$ is a local parameterisation of $y=f(x)$ , and $\eta$ is a local parameterisation of $z=g(f(x))$ , then this is evident from: $$
g \circ f=\left[\eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1}\right] \circ\left[\psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}\right]
$$ and the usual chain rule from multivariate analysis (as we can differentiate the RHS as before using the usual chain rule). In the proof, $g \circ f = \Psi \circ \Phi$ with \begin{align}
\Psi &:= \eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1} \\
\Phi &:= \psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}.
\end{align} The composition of smooth maps is also smooth, so $\Psi, \Phi$ are smooth. However, $\operatorname{dom} (\Psi) = \operatorname{dom} (\psi^{-1})$ which is open in $Y$ but not necessarily open in its ambient Euclidean space. The same situation holds for $\Phi$ . So $\Psi, \Phi$ are not differentiable in the usual sense, so we can not apply the chain rule on $\Psi \circ \Phi$ . However, the author said ""...we can differentiate the RHS as before using the usual chain rule..."". Could you please elaborate on my confusion?","['proof-explanation', 'chain-rule', 'smooth-manifolds', 'differential-geometry']"
4315974,How do we describe maps of line bundles on $\mathbb{P}^1$?,"I am trying to get a very down to earth description of the sections of the holomorphic line bundles over the complex projective line. We have that for line bundles $\mathcal{O}(d)$ of non-negative degree $d\geq 0$ one has $$
H^0(\mathcal{O}(d)) \simeq \mathbb{C}[z_0,z_1]^d,
$$ the space of homogeneous polynomials in two variables of degree $d$ . For $d<0$ , the line bundle $\mathcal{O}(d)$ only has the zero global section. Another relevant feature is that for different degrees $m,n$ we have $\mathcal{O}(m)\otimes\mathcal{O}(n)\simeq \mathcal{O}(m+n)$ , and $\mathcal{O}(n)^*\simeq \mathcal{O}(-n)$ . This is particularly useful so as to interpret the bundle of homomorphisms $$Hom(\mathcal{O}(n),\mathcal{O}(m)) \simeq \mathcal{O}(m)\otimes \mathcal{O}(n)^* \simeq \mathcal{O}(m-n)$$ For positive $m\geq n\geq 0$ my intuition tells me that a global homomorphism from $\mathcal{O}(n)$ to $\mathcal{O}(m)$ sends degree- $n$ polynomials to degree- $m$ polynomials via multiplication with a degree- $(m-n)$ polynomial. However, if $m-n\geq 0$ but $n<0$ I cannot describe this homomorphism in terms of global sections of $\mathcal{O}(n)$ , as there are no nontrivial ones! My question : how can I describe, fiberwise, this homomorphism $\mathcal{O}(n)\rightarrow \mathcal{O}(m)$ in terms of the degree- $(m-n)$ polynomial, when $n<0$ ? Some preliminary thoughts : for negative degree bundles, I can try to describe them as the tensor power of the hyperplane bundle, with fibers given by $$\mathcal{O}(-1)|_{[z_0:z_1]} = \{([z_0:z_1],\phi):\phi\in(\mathbb{C}^2)^*,\phi(z_0,z_1)=0\}$$ My maybe-not-so-useful guess is that for $d<0$ I would get fibers $$\mathcal{O}(d)|_{[z_0:z_1]} = \{([z_0:z_1],\sum_i\alpha_i \phi^i_1\otimes\dots\otimes \phi^i_{-d}):\phi^i_j\in(\mathbb{C}^2)^*,\alpha_i\in\mathbb{C},\phi^i_j(z_0,z_1)=0\}$$","['complex-geometry', 'vector-bundles', 'algebraic-geometry']"
4316026,Derivative of matrix w.r.t. itself,"This sounds like a joke, but i am actually interested and would like an answer, but what is the derivative of a matrix $C$ w.r.t. itself? $$
\text{What is: } \frac{\delta C}{\delta C}\text{?}
$$ Is it a matrix with shape equal to $C$ and filled with ones?","['matrices', 'calculus', 'matrix-calculus', 'derivatives']"
4316040,Exact Solution form Fundamental Solution PDE,"Let $f$ be a function on $\mathbb{R}_+\times\mathbb{R}^d$ . Let $L$ be some differential operator, like $L=\frac{\partial}{\partial t}+\frac{\partial^2}{\partial x^2}$ . Consider for some function $g$ the PDE $$ L[f]=g$$ equipped with the initial condition that $f(0,x)=h(x)$ . If we are told the fundamental solution ( i.e the solution when $f(0,x)=\delta(x_0)$ ) how do I from this get the solution for when $f(0,x)=h(x)$ ? As an example I am thinking of eq (12.27) the Langevin equation from the book Elements of Nonequilibrium Statistical Mechanics by V. Balakrishnan. Im confused when comparing it to the wikipedia https://en.wikipedia.org/wiki/Fundamental_solution#Proof_that_the_convolution_is_a_solution because one is relating fundamental solutions about the RHS of the PDE and the other is relating them to initial conditions.","['partial-differential-equations', 'initial-value-problems', 'parabolic-pde', 'fundamental-solution', 'derivatives']"
4316070,If $f'(x) − f'(y) ≤ 3|x − y|$ then show that $|f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2$,"Let $f : \mathbb R \rightarrow \mathbb R $ be a twice differentiable function. Suppose that for all $x, y \in \mathbb R$ the function $f$ satisfies $f'(x) − f'(y) ≤ 3|x − y|$ then show that for all $x$ and $y$ , we must have $$|f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2$$ Setting $y=0$ gives $c-3x≤f'(x)≤c+3x$ , where $c=f'(0)$ $\Rightarrow \int_0^t c-3x≤\int_0^t f'(x)≤\int_0^t c+3x$ $\Rightarrow ct-\frac{3}{2}t^2-c≤f(t)-c_1≤ct+\frac{3}{2}t^2-c$ Where $c_1=f(0)$ . Now let $c_1-c=c_0$ $\Rightarrow ct-\frac{3}{2}t^2+c_0≤f(t)≤ct+\frac{3}{2}t^2+c_0$ For the given expression to be maximum, $f(x)$ must be maximum, $f(y)$ must be minimum and for $x≥y$ $f'(y)$ must also be minimum (we will look at the case $x<y$ separately). Substituting all these values, we get $(cx+\frac{3}{2}x^2+c_0)-(cy-\frac{3}{2}y^2+c_0)-(c-3y)(x-y)$ $\implies \frac{3}{2}(x^2+y^2)+6xy-3y^2$ $\implies \frac{3}{2}(x-y)^2+3y(2x-y)$ Under assumption $x>y$ , for all $y>0$ this expression is clearly $>\frac{3}{2}(x-y)^2$ . So where did I go wrong, and what will be the correct solution?","['functional-inequalities', 'calculus', 'functions', 'solution-verification', 'inequality']"
4316072,Do full rank matrices in $\mathbb Z^{d\times d}$ preserve integrals of functions on the torus?,"Let $\mathbb T:=[0,1]/(0\sim 1)$ .  Its easy to see that if $f:\mathbb T\to \mathbb R$ and $k\in\mathbb Z$ is not zero, then $f_k(x):=f(kx)$ (1) defines a map $ f_k:\mathbb T\to \mathbb R$ , and $$ \int_{\mathbb T} f_k(x) dx = \int_{\mathbb T} f(x)dx$$ Indeed: identifying with 1-periodic functions on $\mathbb R$ , one has $$ \int_{\mathbb T} f_k(x) dx = \int_0^1 f_k(x) dx = \frac1k\int_0^k f(y)dy \overset{\star}= \frac1k\sum_{j=0}^{k-1}\int_0^1f(y+j)dy = \int_0^1 f(y)dy. $$ Question Is the following $d$ dimensional analogue true?
Let $f:\mathbb T^d\to \mathbb R$ and $K\in\mathbb Z^{d\times d}$ with $\det K\neq 0$ . We well-define a map $f_K:\mathbb T^d\to R$ by $f_K(x) := f(Kx)$ . Is it true that $$ \int_{\mathbb T^d} f_K(x) dx = \int_{\mathbb T^d} f(x)dx?$$ The above proof only extends to diagonal matrices $K$ . The issue for me is in getting a replacement for the identity marked $\star$ . For an example matrix where the above isn't enough, consider $M=\begin{pmatrix} 2 & 1 \\ -1 & 2 \end{pmatrix}$ . $M$ sends $[0,1]^2$ (green) to $M[0,1]^2$ (red): and the associated change in coordinates (corresponding to the inverse map) looks like this: Note that $\det M=5$ and there are five small (rotated, translated) cubes in one big cube. So I suspect the calculation should go like this. Presumably, $$ \int_{\mathbb T^d} f(Kx)dx = |\ker K|\int_{\mathbb T^d/\ker K}f(Kx)dx\tag{A}$$ where we identify $K\in\mathbb Z^{d\times d}$ with the map $K:\mathbb T^d\to\mathbb T^d$ . Then, it seems that $$ |\ker K| = |\det K|\tag{B}$$ Since the First Isomorphism Theorem tells me that $K$ is an isomorphism from $\mathbb T^d/\ker K$ to $\mathbb T^d$ , change of variables $y=Kx$ (with Jacobian $1/|\det K|$ ) gives $$\int_{\mathbb T^d/\ker K}f(Kx)dx = \frac1{|\det K|}\int_{\mathbb T^d} f(y) dy,$$ which finishes the proof. So now the question is, how does one prove $(A),(B)$ ? The appearance of $\mathbb Z^{d\times d}$ reminds me (OK, it reminded a friend) of the fundamental theorem of finitely generated abelian groups, but I have forgotten this almost completely. It also seems from googling that a key thing im missing is the ability to identify a ‘fundamental domain’? All thoughts welcomed","['integration', 'periodic-functions', 'ergodic-theory', 'linear-transformations', 'group-theory']"
4316073,Infinite abelian group whose all nontrivial subroups have finite index,"D.J.S. Robinson, A Course in the Theory of Groups, 2d edition, exercise 4.1.3, p. 98, asks for a proof of the following statement : Statement 1 . If $G$ is an infinite abelian group all of whose proper quotient groups (I understand : quotients by nonzero subgroups) are finite, then $G$ is (infinite) cyclic. I can prove it, but only by use of the following theorem, which is proved later in the book (4.2.10, p. 103) : Statement 2. If an abelian group is finitely generated, it is a direct sum of finitely many cyclic (finite or infinite) subgroups. (I give a proof of Statement 1 by Statement 2 below.) My question is: Is it possible to prove Statement 1 without using Statement 2 in a more or less explicit manner? Thanks in advance for the answers. Here is my proof of Statement 1 by use of Statement 2. Let $G$ be as in Statement 1 . Note it additively. For every nonzero element $x$ of $G$ , $\mathbb{Z}x$ is a nontrivial subgroup of $G$ , thus, by hypothesis, (1) the quotient $G/ \mathbb{Z}x$ is finite. Thus, since $G$ is infinite by hypothesis, $\mathbb{Z}x$ is infinite. Since this is true for every nonzero element $x$ of $G$ , (2) $G$ is torsion-free. Choose a nonzero element $x$ of $G$ (it is possible, since $G$ is infinite by hypothesis). By (1), there exists a finite set $\{a_{1}, \ldots , a_{n} \}$ of elements of $G$ such that $a_{1} + \mathbb{Z}x, \ldots , a_{n} + \mathbb{Z}x$ are all the elements of $G/ \mathbb{Z}x$ . Thus $G$ is generated by $x, a_{1}, \ldots, a_{n}$ , thus $G$ is finitely generated, thus, by Statement 2, (3) $G$ is a direct sum $H_{1} \oplus \cdots \oplus H_{k}$ of finitely many nontrivial cyclic subgroups. Since $G$ is nontrivial, we have $k \geq 1$ . By (2), $G$ is torsion-free, thus every $H_{i}$ is torsion-free. Thus, since $H_{i}$ is nontrivial, (4) every $H_{i}$ is infinite. Since $H_{1}$ is not trivial, $G / H_{1}$ is finite (by hypothesis of the exercice). But, by (3), $G / H_{1}$ is isomorphic to $H_{2} \oplus \cdots \oplus H_{k}$ , thus $H_{2} \oplus \cdots \oplus H_{k}$ is finite. Thus, in view of (4), $k = 1$ , thus, by (3), $G = H_{1}$ . Thus, by (3), $G$ is (infinite) cyclic and we are done. Edit (January 15, 2022). If I am not wrong, the proof of satement 1 can easily be extracted from Robinson's proof of statement 2. Let $G$ be as in statement 1. As noted, $G$ is torsion-free. Since $G$ is infinite, we can choose a nonzero element $a$ of $G$ . Then $<a>$ is a nontrivial subgroup of $G$ , thus, by hypothesis, $<a>$ is of finite index $m > 0$ in $G$ . Then $x \mapsto mx$ defines a homomorphism from $G$ to $<a>$ . Since $G$ is torsion-free, this homomorphism is injective, thus $G$ is isomorphic to a subgroup of $<a>$ , thus $G$ is cyclic.","['group-theory', 'abelian-groups', 'infinite-groups']"
4316081,Seemingly conflicting notions of a function,"Throughout my mathematical education, I have seen a few, seemingly, different and conflicting notions of what a function is: A function is a a type of mathematical object that maps every element of a set to some member of another set. E.g. $f:\mathbb{R} \rightarrow \mathbb{R},\ x \mapsto 2x$ is a function that maps every real number to another real number. Namely, $f$ maps every $t \in \mathbb{R}$ to $2t$ or $f(y) = 2y, \ y \in \mathbb{R}$ . If the value of a quantity, $a$ , is written in terms of other quantities, $b$ and $c$ ; then $a$ is called a function of $b$ and $c$ . E.g. If mass is constant, the force in $F=ma$ is called a function of acceleration and is written $F(a)=ma$ . How does this relate to the notion of function in 1? The input $a$ refers to the value of a physical quantity here whereas in 1, the input is just a placeholder for an arbitrary real number so $f(a)=2a, \ a \in \mathbb{R}$ is the same as $f(m)=2m, \ m \in \mathbb{R}$ . There appears to be a dependence on the symbols used to represent the inputs when taking partials derivatives of functions. For example, $f(x)=2x$ and $f(y)=2y$ is the same mapping according to 1 but the partial with respect to $x$ of both of these are quite different. How do you reconcile all of these ideas? What am I missing? Thanks in advance","['applications', 'functions', 'intuition', 'partial-derivative', 'algebra-precalculus']"
4316109,About $\mathring{A_1}\cup\mathring{A_2}\cup\mathring{A_3}=\mathring{\overbrace{A_1\cup A_2\cup A_3}}$,"If $\operatorname{Bd}(A_1)\cap\operatorname{Bd}(A_2)\cap\operatorname{Bd}(A_3)=\emptyset$ , show that $\operatorname{int}(A_1)\cup\operatorname{int}(A_2)\cup\operatorname{int}(A_3)=\operatorname{int}(A_1\cup A_2\cup A_3)$ ? With $\operatorname{int}(A)$ is  the interior if $A$ ,
and $\operatorname{Bd}(A_1)$ is the boundary of $A_1$ Proof Suppose $x\in\operatorname{int}(A_1\cup A_2\cup A_3)$ but $x\notin \operatorname{int}(A_1)\cup\operatorname{int}(A_2)\cup\operatorname{int}(A_3);$ I claim that $x\in\operatorname{Bd}(A_1)\cap\operatorname{Bd}(A_2)\cap\operatorname{Bd}(A_3).$ I will show that $x\in\operatorname{Bd}(A_1);$ the proof of $x\in\operatorname{Bd}(A_2)$ and $x\in\operatorname{Bd}(A_3)$ is similar. Since $x\notin\operatorname{int}(A_1),$ it will suffice to show that $x\in\operatorname{Cl}(A_1).$ Let $U$ be any neighborhood of $x.$ Then $V=U\cap\operatorname{int}(A_1\cup A_2\cup A_3)$ is a neighborhood of $x,$ and $V\subseteq A_1\cup A_2\cup A_3,$ but $V\not\subseteq A_2\cup A_3$ (since $x\notin\operatorname{int}(A_2))\cup\operatorname{int}(A_3)),$ so $V\cap A_1\ne\emptyset$ and therefore $U\cap A_1\ne\emptyset.$ is my Proof correct? Any help will be appreciated.","['general-topology', 'solution-verification']"
4316118,Domain in proof of Euler's theorem for homogeneous functions,"$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\ra}{\rightarrow}
\newcommand{\d}{\text{d}}
$$ Most statements of Euler's theorem for homogeneous functions I found restrict the domain of the considered function, and/or the homogeneity degree, and/or the rescaling parameter. For example Wikipedia states the theorem for positively homogeneous functions $f: \R^n \setminus {0} \ra \R$ Apostol at pag. 287 considers only positive rescaling parameters. I don't see what goes wrong with this generality: Definition Let $k$ be an integer. A differentiable function $f: \R^n \ra \R$ is $k$ -homogeneous if $$
f(ax) = a^kf(x)
$$ for all $x \in \R^n$ and all $a \in \R \setminus {0}$ . Euler's Theorem Let $k$ be an integer and let $f: \R^n \ra \R$ be a differentiable function. Then $f$ is $k$ -homogeneous if and only if $$
x \cdot \d{f}(x) = kf(x)
$$ for all $x \in \R^n$ . Proof Let $f$ be $k$ -homogeneous. Then for any nonzero real $a$ , $f(ax) = a^k f(x)$ . Differentiate with respect to $a$ : $$
\d{f}(ax) \cdot x =  ka^{k-1}f(x)
$$ In particular this must be true for $a$ = 1, so $$
\d{f}(x) \cdot x = kf(x)
$$ Conversely, let the condition above hold; then for any real $a$ , $ax \cdot \d{f}(ax) = k f (ax)$ . Define $g(a) = f(ax)$ for any fixed $x \in \R^n$ . I guess here it is actually necessary to consider $g(a) = f(ax)$ for any fixed $x \in \R^n \setminus {0}$ . Differentiate $g$ with respect to $a$ , and assume $a \neq 0$ : $$
\frac{\d{}}{\d{a}}g(a) = \d{f}(ax) \cdot x = \frac{\d{f}(ax) \cdot ax}{a} = \frac{k}{a}f(ax) = \frac{k}{a}g(a)
$$ The solution of this ODE is $g(a) = g(1) a^k$ , i.e. $f(ax) = a^k f(x)$ for all nonzero $a$ . To wrap up I'm not sure whether it is necessary to state the theorem excluding the origin of $\R^n$ from the domain of $f$ ; The only constraint on $a$ seems to be $a \neq 0$ ; I see no constraint on the homogeneity degree $k$ , namely the theorem seems to hold for $k>0$ , $k=0$ and $k<0$ . Is this correct?",['multivariable-calculus']
4316124,On the proof that positive linear functionals are continuous,"Am reading the proof from here reproduced below: Let $X$ be any real or complex Banach space of functions or equivalence classes of functions such that with $f\in X$ also $|f|\in X$ and $\||f|\| = \|f\|$ and let $\phi$ be a positive linear functional on $X$ . It is easily seen that $|\phi(f)|\le 2\phi(|f|)$ for all $f\in X$ . Suppose that $\phi$ is unbounded. Then there exists a sequence $(f_n)\subset X$ with $\|f_n\|=1$ and $|\phi(f_n)|\ge 2\cdot 4^n$ . Put $g_n := |f_n|\ge 0$ . Then $\|g_n\|=1$ and $\phi(g_n)\ge 4^n$ for $n\in\mathbb N$ . Define the function $h := \sum_{n=1}^\infty2^{-n}g_n\in X$ . Then $h\ge 2^{-n}g_n$ for each $n\in\mathbb N$ and so $\phi(h)\ge 2^{-n}\phi(g_n)\ge 2^n$ for all $n\in\mathbb N$ , which is impossible. Question: Why do we have $h\ge 2^{-n}g_n$ for each $n\in \mathbb N$ ? This doesn't seem to follow from the assumptions made. Do we need $X$ to be an $L^p$ space with $1\leq p< \infty$ ? It looks like we are using the property that if $f_n\geq 0$ for each $n$ and $f_n\to f$ with respect to $\|\cdot \|$ then $f\geq 0$ . But why is this true? Am I missing something? Thanks a lot for your help. EDIT: For the real case it seems that the natural requirement is to let $X$ be a Banach lattice . This will ensure that $|x|:=\sup\{x,-x\}\in X$ whenever $x\in X$ , that $\||x|\| = \|x\|$ , and that the positive cone $P := \{ x \in X : x\geq 0\}$ is closed (see here ). In the case where $X=L^p$ for some $1\leq p< \infty$ I believe we can argue as follows: Choose a representative for $h$ and $\sum_{i=1}^n2^{-i}g_i$ for each $n$ . Then $\sum_{i=1}^n2^{-i}g_i \to h$ in $L^p$ as $n\to \infty$ . We can extract a subsequence $(n_k)$ with $\sum_{i=1}^{n_k}2^{-i}g_i \to h$ a.s. as $k\to \infty$ . This implies that $h\ge 2^{-n}g_n$ a.s. for each $n\in \mathbb N$ .","['banach-spaces', 'measure-theory', 'normed-spaces', 'lp-spaces', 'functional-analysis']"
4316132,Is this sequence bounded or unbounded?,"Let $\{ a_n \}_{n=0}^\infty$ be the sequence given by $a_0 = 3$ and $$a_{n+1} = a_n - \frac{1}{a_n}, \quad n\ge 0.$$ (you can easily check that the sequence is well defined). Question : Is this sequence bounded or not? I was only able to show that there are infinitely many $n \geq 0$ such that $a_n > 0$ and infinitely many $n \geq 0$ such that $a_n < 0$ in this way: suppose for example that we have eventually $a_n > 0$ . But then $a_n$ is eventually strictly decreasing and so, using the fact that eventually $a_n > 0$ together with the monotone convergence theorem for sequences we get that $a_n$ is convergent, which is absurd because this would imply, together with the fact that $a_{n+1} a_n = a_n^2 - 1$ for all $n$ , that it's limit $L \in \mathbb{R}$ satisfies the equation $L^2 = L^2 - 1$ . Using the same reasoning you can show that $a_n < 0$ eventually leads to a contradiction. I think the boundedness question is much harder to understand because this sequence behaves in a very chaotic way.","['real-numbers', 'calculus', 'sequences-and-series', 'real-analysis']"
4316154,Do the elements of this series converge to zero?,"Let $\{a_n\}\subset [0,\infty )$ . It is well know that if $$\sum_{n \in \mathbb{N}}a_n < \infty$$ then we must have: $a_n \to 0$ as $n \to \infty$ . Now suppose that $a_n=a_n(u)$ with $\{a_n\}\subset [0,\infty )$ and $u \in U$ fixed in  some metric space $U$ (possibly infinite dimensional). Asssume the following condition: there exists $C>0$ such that $$\sum_{n \in \mathbb{N}}a_n(u) < C$$ for every $u \in U$ . I remark that $C$ is independent of $u$ . Can I conclude that $\sup_{u} |a_n(u)| \to 0$ ? Or do I need to assume that $U$ is compact?","['divergent-series', 'functional-analysis', 'uniform-convergence', 'sequences-and-series']"
4316167,Differntial inclusions that are single valued near hyperbolic rest point: Hartman Grobman still fine?,"Suppose for $x\in \mathbb{R}^m$ we have a well behaved differential inclusion $\dot x \in F(x)$ , where $F(x)$ is a Marchaud map, i.e. u.s.c. with compact convex values and linear growth condition:  there exists $c>0$ s.t. $$\sup\{\|y\|:\, y\in F(x) \}\le c(1+\|x\|)$$ for all $x$ . So absolutely continuous solutions exist, but may not be unique. We also assume $F$ is $C^1$ whenever it is single-valued. Now suppose there is a rest point $x^*$ in a neighborhood $U$ , s.t. $F(x)$ is single valued on that neighborhood, $F(x^*)=0$ , and $x^*$ is hyperbolic. Can we still apply Hartman-Grobman in a neighborhood around that rest point? I would think yes, since the construction is local, but I'm not sure if somehow the non-uniqueness of solutions may give us an issue. Could someone give me a hint please?","['set-valued-analysis', 'ordinary-differential-equations']"
4316185,"Evaluate $\int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z \,d y \,d x$","$$
\text { Evaluate } \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} \,d z\, d y\, d x
$$ How to solve this ? I tried this $\begin{aligned} & \int_{0}^{1} \int_{0}^{1-x} \int_{0}^{1-x-y} \sqrt{\frac{x}{y z}} d z d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}}[2 \sqrt{z}]{0}^{1-x-y} d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} \frac{\sqrt{x}}{\sqrt{y}} 2 \sqrt{1-x-y} d y d x \\
=& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \frac{\sqrt{1-x-y}}{\sqrt{y}} d y d x \\=& \int_{0}^{1} \int_{0}^{1-x} 2 \sqrt{x} \sqrt{\frac{1-x-y}{y}} d y d x \end{aligned}$","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4316194,What should/can be learnt in a reading course on Elliptic Curves?,"Currently, I am organizing a potential reading course on Elliptic Curves and have gotten some reference suggestions (I will be a student participating in said course), but do not have a really clear idea of what one might hope to cover or gain from it. For motivation, I am interested in elliptic curves due to potentially seeing Algebraic Geometry in action (and I am somewhat interested in Algebraic Number Theory, but am only taking a first graduate course in the material this semester). I have gotten some reference suggestions such as: ""Introduction to Elliptic Curves and Modular Forms"" by Koblitz, the text by Silverman and Tate, and Silverman's ""The Arithmetic of Elliptic Curves"". There were also the notes by Milne. After an inspection of Silverman's ""The Arithmetic of Elliptic Curves"", Chapters V on Elliptic Curves over Finite Fields and Chapter VIII with the Mordell-Weil Theorem for $\mathbb{Q}$ shine to me. However, having such a narrow goal like proving the Mordell-Weil Theorem means I will inevitably omit many other sections. What are topics in Elliptic Curves that are essential to be aware of and should be covered in a reading course? Potentially, this may include material outside of Silverman's text.","['algebraic-curves', 'elliptic-curves', 'number-theory', 'reference-request', 'algebraic-geometry']"
4316218,What is the easier way to find the circle given three points?,"Given three points $(x_1,y_1)$ , $(x_2,y_2)$ , and $(x_3,y_3)$ , if $$\frac{y_2-y_1}{x_2-x_1} \neq \frac{y_3-y_2}{x_3-x_2} \neq \frac{y_1-y_3}{x_1-x_3},$$ then there will be a circle passing through them. The general form of the circle is $$x^2 + y^2 + dx + ey + f = 0.$$ By substituting $x = x_i$ and $y = y_i$ , there will be a system of equation in three variables, that is: \begin{align*}
    \begin{pmatrix}
        x_1 & y_1 & 1 \\
        x_2 & y_2 & 1 \\
        x_3 & y_3 & 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        d \\
        e \\
        f
    \end{pmatrix} &=
    \begin{pmatrix}
        -\left(x_1^2+y_1^2\right) \\
        -\left(x_2^2+y_2^2\right) \\
        -\left(x_3^2+y_3^2\right)
    \end{pmatrix}.
\end{align*} Solving this system gives the solution \begin{align*}
    d &= \frac{(x_3^2 + y_3^2 -x_1^2+y_1^2)}{x_1 - x_3} - e\left(\frac{y_1 - y_3}{x_1 - x_3}\right) \\
    e &= \frac{(x_3^2 + y_3^2 -x_2^2+y_2^2)(x_1-x_3) - (x_3^2 + y_3^2 -x_1^2+y_1^2)(x_2-x_3)}{(y_2-y_3)(x_1-x_3) - (y_1-y_3)(x_2-x_3)} \\
    f &= \frac{-(x_3^2 + y_3^2)(x_1-x_3) - (y_1-y_3)x_3}{x_1 - x_3} - e\left(\frac{y_3(x_1 - x_3) - x_3(y_1 - y_3)}{x_1 - x_3}\right)
\end{align*} As there are a lot of things going around, the solution is prone to errors. Maybe this solution also has an error. Is there a better way to solve for the equation of the circle?","['conic-sections', 'algebra-precalculus', 'circles']"
4316271,Age of a Renewal Process as a Markov Chain,"I've been working through Stochastic Processes by Sheldon Ross and am stuck on example 4.3(c) in the book. The example is as follows Age of a Renewal Process Initially an item is put into use, and when it fails it is replaced at the beginning of the next time period by a new item. Suppose that the lives of the items are independent and each will fail in its i th period of use with probability $P_{i}, i \geq 1$ , where the distribution { $P_{i}$ } is aperiodic and $\sum_{i}iP_{i}<\infty$ . Let $X_{n}$ denote the age of the item in use at time $n$ - That is, the number of periods (including the n th) that it has been in use. Then if we let $$
\lambda(i)=\frac{P_{i}}{\sum_{j=1}^{\infty}P_{j}}
$$ be the probability that an i unit old item fails, then { $X_{n},n\geq 0$ } is a Markov chain with transition probabilities given by $$
P_{i,1}=\lambda(i)=1-P_{i,i+1},\quad i\geq 1
$$ Hence the limiting probabilities are such that $$
\pi_{1}=\sum_{i}\pi_{i}\lambda(i) \\
\pi_{i+1}=\pi_{i}(1-\lambda(i)), \quad i\geq1
$$ Iterating yields $$
\begin{equation} \label{1}
\begin{split}
\pi_{i+1} & =\pi_{i}(1-\lambda(i)) \\
& =\pi_{i-1}(1-\lambda(i-1))(1-\lambda(i)) \\
& =\pi_{1}(1-\lambda(1))(1-\lambda(2))\dots (1-\lambda(i)) \\
& =\pi_{1}\sum_{j=i+1}^{\infty}P_{j} \\
& =\pi_{1}P\{X\geq i+1\}
\end{split}
\end{equation}
$$ where $X$ is the life of an item. Using $\sum_{i=1}^{\infty}\pi_{i}=1$ yields $$
1=\pi_{1}\sum_{i=1}^{\infty}P\{X\geq i\}
$$ or $$
\pi_{1}=1/E[X]
$$ and $$
\pi_{i}=P\{X\geq i\}/E[X]
$$ It seems like basic probability but I get to the part where he's iterating to get the expression for $\pi_{i+1}$ and am completely lost. I get that $1-\lambda(i)=P\{X\neq i\}$ and $P\{X\geq i+1\}=P\{X\neq 1,X\neq 2, \dots, X\neq i\}$ . So isn't he claiming that $P\{X\neq 1,X\neq 2, \dots, X\neq i\}=P\{X\neq 1\}P\{X\neq 2\}\dots P\{X\neq i\}$ but surely that's not true? Thanks","['stochastic-processes', 'markov-chains', 'probability-theory', 'probability']"
4316300,Probability a random spherical triangle has area $> \pi$,"From Michigan State University's Herzog contest : Problem 6, 1981 Three points are taken at random on a unit sphere. What is the probability that the area of the spherical triangle exceeds the area of a great circle? I assume we always take the unique proper spherical triangle, with sides and angles not greater than $\pi$ . I assume ""the area of a great circle"" is the plane area $\pi$ , since a proper spherical triangle never has area larger than the spherical area $2 \pi$ on each side of a great circle. I assume the random distribution is uniform, such that the probability each point is within a given measurable subset of the sphere is proportional to the spherical area of that subset. First, given triangle vertices $A,B,C$ on the unit sphere, we can rotate and/or mirror the coordinate system so that $C = (0,0,1)$ , $A$ is on the $x \geq 0, y=0$ half-plane, and $B$ is on the $y \geq 0$ hemisphere. Label the side lengths opposite $A$ , $B$ , and $C$ as $a$ , $b$ , and $c$ respectively. Label the (dihedral) angles at $A$ , $B$ , and $C$ as $\alpha$ , $\beta$ , and $\gamma$ respectively. Ignore a degenerate triangle case with any of these variables exactly equal to $0$ or $\pi$ ; this happens with probability zero. Then all six variables are in the interval $\{a,b,c,\alpha,\beta,\gamma\} \subset (0,\pi)$ . We can parameterize the probability distribution as: $$ \begin{align*}
 P(a < a_0) &= \frac{1-\cos a_0}{2} \\
 P(b < b_0) &= \frac{1-\cos b_0}{2} \\
 P(\gamma < \gamma_0) &= \frac{\gamma_0}{\pi}
\end{align*}$$ Variables $a$ , $b$ , and $\gamma$ are independent, so the overall probability density function is $$ dP = \frac{1}{4\pi} \sin a \sin b\, da\, db\, d\gamma $$ The spherical area $\sigma$ of the triangle is $$ \sigma = \alpha + \beta + \gamma - \pi $$ The outcome $\sigma > \pi$ is only possible if $\gamma > \frac{\pi}{2}$ , since the spherical triangle covers a subset of the region between the planes containing $\{O,A,C\}$ and $\{O,A,B\}$ with area, $2 \gamma$ . Similarly $\sigma > \pi$ implies $\alpha > \frac{\pi}{2}$ and $\beta > \frac{\pi}{2}$ . We also have Napier's analogy $$ \tan \frac{\alpha+\beta}{2} = \frac{\cos \frac{a-b}{2}}{\cos \frac{a+b}{2}} \cot \frac{\gamma}{2} $$ Since $\{a,b\} \subset (0, \pi)$ , $|a-b|<\pi$ and $\cos \frac{a-b}{2} > 0$ . If $\sigma > \pi$ then $\{\alpha, \beta, \gamma\} \subset (\frac{\pi}{2}, \pi)$ which implies $\tan \frac{\alpha+\beta}{2} < 0$ and $\cot \frac{\gamma}{2} > 0$ . Therefore $\cos \frac{a+b}{2} < 0$ , and $\frac{a+b}{2} > \frac{\pi}{2}$ . So given $\{\alpha, \beta, \gamma, \frac{a+b}{2}\} \subset (\frac{\pi}{2}, \pi)$ , all these inequalities are equivalent: $$ \begin{align*}
\sigma &> \pi \\
 \alpha + \beta + \gamma &> 2\pi \\
 \pi - \frac{\gamma}{2} &< \frac{\alpha+\beta}{2} \\
 \tan \left(\pi - \frac{\gamma}{2}\right) &< \tan \frac{\alpha+\beta}{2} \\
 -\tan \frac{\gamma}{2} &< \frac{\cos \frac{a-b}{2}}{\cos \frac{a+b}{2}} \cot \frac{\gamma}{2} \\
\tan^2 \frac{\gamma}{2} &> - \cos \frac{a-b}{2}\, \sec \frac{a+b}{2} \\
\gamma &> 2\tan^{-1} \sqrt{- \cos \frac{a-b}{2}\, \sec \frac{a+b}{2}} = \gamma_L(a,b)
\end{align*}$$ where the function $\gamma_L(a,b)$ is defined where $0 \leq a \leq \pi$ , $0 \leq b \leq \pi$ , and $a+b>\pi$ .
The probability in question is \begin{align}P(\sigma > \pi) &= \int_0^\pi \int_{\pi-a}^\pi \int_{\gamma_L(a,b)}^\pi \frac{1}{4\pi} \sin a \sin b\, d\gamma\, db\, da\\&= \frac{1}{4\pi} \int_0^\pi \int_{\pi-a}^\pi [\pi - \gamma_L(a,b)] \sin a \sin b\, db\, da\end{align} I've found we can also write $$ \gamma_L(a,b) = \pi - \cos^{-1}\left(\cot \frac{a}{2} \cot \frac{b}{2}\right) $$ Either way, integration by parts can get rid of the inverse trig function in the integrand, leaving a square root, but I haven't been able to make much progress beyond that in solving the definite integral. Wolfram Alpha gives the numeric answer as apparently $\frac{1}{6}$ (1) (2) . But it seems it can't solve it symbolically either. Given this was in a limited-time competition, maybe there's some other simpler way to go about this. Some sort of symmetry grouping related points or triangles? How can we prove the exact probability?","['integration', 'spherical-trigonometry', 'contest-math', 'definite-integrals', 'spherical-geometry']"
4316307,"Coefficient extraction,show: $[z^n]\frac{1}{(1-z)^{\alpha + 1}} \log \frac{1}{1-z} = \binom{n + \alpha }{n} (H_{n+\alpha} - H_{\alpha})$","I want to show: \begin{equation*}
  [z^n]\frac{1}{(1-z)^{\alpha + 1}} \log \frac{1}{1-z} = \binom{n + \alpha }{n} (H_{n+\alpha} - H_{\alpha}).
\end{equation*} where $[z^n]$ means the $n$ -th coefficient of the power series and \begin{equation}
H_{n+\alpha} - H_{\alpha} = \sum^{n}_{k=1}{\frac{1}{\alpha + k}}
\end{equation} So far I got \begin{equation*}
    \frac{1}{(1 - z)^{\alpha + 1}} = (1-z)^{-(\alpha + 1)} = \sum_{n \geq 0}{\binom{-\alpha - 1}{k}(-1)^k z^k} =  \sum_{n \geq 0}{\binom{\alpha + n }{n} z^n}
  \end{equation*} where I used $\binom{-\alpha}{k} = (-1)^k \binom{\alpha + k - 1}{k}$ and \begin{equation*}
    \log \frac{1}{1 - z} = - \log 1- z = \sum_{n \geq 1}\frac{z^n}{n} = z\sum_{n \geq 0}\frac{z^{n}}{n+1}
  \end{equation*} therefore \begin{align*}
    \frac{1}{(1-z)^{\alpha + 1}} \log \frac{1}{1-z} &= z\left( \sum_{n \geq 0}{\binom{\alpha + n }{n} z^n} \right) \left( \sum_{n \geq 0}\frac{z^{n}}{n+1} \right) \\
    &= z \sum_{n \geq 0}\sum^{n}_{k=0}\binom{\alpha + k}{k} \frac{1}{n - k + 1} z^{n} \\
    &= \sum_{n \geq 0}\sum^{n}_{k=0}\binom{\alpha + k}{k} \frac{1}{n - k + 1} z^{n + 1}.
  \end{align*} Now the $n$ -th coefficient is \begin{align*}
    \sum^{n}_{k=0}\binom{\alpha + k}{k} \frac{1}{n - k + 1} &= \binom{\alpha + n}{n} \sum^n_{k = 1}{\frac{n!}{(n-k)! k!} \frac{1}{\binom{\alpha + n}{n - k}}    \frac{1}{n - k + 1}} +  \frac{1}{n+1}\\
    &=  \binom{\alpha + n}{n} \sum^n_{k = 1}{\binom{n}{k} \frac{1}{\binom{\alpha + n}{n - k}}    \frac{1}{n - k + 1}}  +   \frac{1}{n+1}\\
    &= \binom{\alpha + n}{n} \sum^n_{k = 1}{\binom{n}{k - 1} \frac{1}{\binom{\alpha + n}{n - k - 1}} \frac{k-1}{n - k -1}  \frac{1}{\alpha + k + 1} } + \frac{1}{n+1}\\
    &= \binom{\alpha + n}{n} \sum^{n+1}_{k = 2}{\binom{n}{k - 2} \frac{1}{\binom{\alpha + n}{n - k}} \frac{k- 2 }{n - k}  \frac{1}{\alpha + k} } + \frac{1}{n+1}
  \end{align*} but from now on I can't see how to proceed.
I also know that \begin{equation}
[z^n] \frac{1}{1-z} \log \frac{1}{1-z} = H_n
\end{equation} somehow I also tried to use some sort of transformation law for
coefficient extraction, but I am not aware of any kind of transformation law for coefficient extraction. Using $1-u = (1-z)^{\alpha + 1}$ or $z = 1 - (1-u)^{1/(\alpha + 1)}$ gives \begin{equation}
\frac{1}{\alpha + 1}\frac{1}{1-u} \log \frac{1}{1 - u}
\end{equation} and for this I get the coefficient $\frac{1}{\alpha + 1} H_n$ . Another approach I did was the following: Since \begin{equation*}
    (1-z)^{-m} = \exp(-m \ln(1-z)) 
  \end{equation*} I get \begin{equation*}
    \frac{\partial }{\partial m} \exp(-m \ln(1-z))  = -\ln(1-z)  \exp(- m \ln(1-z)) = - \ln(1-z) \frac{1}{(1-z)^m} = \frac{1}{(1-z)^m} \ln \frac{1}{1-z}
  \end{equation*} therefore \begin{align*}
    \frac{\partial }{\partial m} (1-z)^{-m} &= \sum_{n \geq 0}{\frac{\partial }{\partial m} \binom{m + n - 1}{n} z^n} \\
    &= \sum_{n \geq 0}{\frac{\partial }{\partial m} \frac{\Gamma(m + n)}{n!\Gamma(m)} z^n} 
\end{align*} But I don't know any identities for the derivative of the gamma function.","['harmonic-numbers', 'calculus', 'binomial-coefficients', 'generating-functions', 'algorithms']"
4316329,Morse functions invariant under diffeomorphisms,"Let $f:M \to \mathbb{R}$ be a Morse function of a compact manifold $M$ . Assume $\sigma:M \to M$ is a diffeomorphism such that $f$ is invariant under $\sigma$ , i.e. $f(\sigma x)=f(x)$ for all $x \in M$ . I am trying to understand how $\sigma$ induces homomorphisms of the Morse homology groups $H_k(M)$ and how to describe them explicitly. If we denote the boundary maps by $\partial_k$ , the elements of $H_k(M)$ are linear combinations of cosets of the form $x+\text{im }\partial_{k+1}$ where $x$ is a critical  point of index $k$ with $x \in \ker \partial_k$ . Question: Does $\sigma$ act on $H_k(M)$ by application to representatives of the cosets? That is: Is the map $$x+\text{im }\partial_{k+1} \mapsto \sigma x+\text{im }\partial_{k+1}$$ well defined and does it extend to an automorphism of $H_k(M)$ ? I think I am able to see that $\sigma$ permutes the critical points of a fixed index. This would show that $\sigma$ is an automorphism of the Morse chain groups $C_k(M)$ . However, I don't know how to proceed. The standard approach would be to show that $\sigma$ commutes with the boundary maps $\partial_{k+1}$ but I have no ideas how to do this. I tried to analyze the action of $\sigma$ on the trajectories between critical points but up to now, this didn't result in anything useful for me. Not sure if this is a good strategy here...","['morse-theory', 'homology-cohomology', 'differential-geometry']"
4316338,Derivative of dual isogeny is pullback on $H^1$,"Let $X$ and $Y$ be elliptic curves (over an algebraically closed field, but no assumptions on the characteristic) with Jacobians $J_X$ and $J_Y$ respectively. Suppose $f:X\to Y$ is an isogeny, with dual isogeny $\widehat{f}:J_Y\to J_X$ . How can I show that the map on tangent spaces $d\widehat{f}:T_0J_Y\to T_0J_X$ is $f^*:H^1(Y,\mathcal{O}_Y)\to H^1(X,\mathcal{O}_X)$ ? I already know that the tangent space to the identity of $J_X$ is $H^1(X,\mathcal{O}_X)$ via considering maps $\operatorname{Spec} k[\varepsilon]/\varepsilon^2\to J_X$ and using the universal property of the Jacobian variety, but I'm a little stumped on how to rigorously show the statement about the map. Background: I'm trying to connect the two characterizations of the Hasse invariant of an elliptic curve in terms of the action of the Frobenius on $H^1$ and the separability of the dual of the Frobenius. Knowing this statement would finish the problem by the link between separability and the map on tangent spaces (plus maybe an argument that the map on tangent spaces has to be the same everywhere? I'm actually realizing I might need a little help with that too, as I'm writing this).","['isogeny', 'algebraic-geometry', 'elliptic-curves']"
4316347,How to calculate the side of a right triangle from the coordinates of points and the length of one side?,"I have the line AB . And I need to calculate the coordinates of point D . I know the coordinates of points A , B and C . If I make this an imaginary right triangle, I just need to know the length of the CD line ( a in the picture) Since I can easily calculate the line length AC ( d on the picture) from the coordinates, I only need the line AD to calculate the CD using the Pythagorean theorem. I know the coordinates of points A and B , so I can easily calculate the length of line AB from this. But how do I calculate the length of the AD line so that I can then calculate the length of the CD ? Or is it possible in another way? Unfortunately, I don't know the angles either. Please help Thank you","['triangles', 'pythagorean-triples', 'geometry']"
4316374,Bayes theorem problem example,"In an urn we have 3 fair dice and 1 unfair. The fair ones are cubic and the unfair is a tetrahedron with all 6s.
We put our hand inside the urn and pick one die at random. Assuming we can't tell from its shape if it's fair or not, with eyes blindfolded, we roll the die we picked. Find the probability we bring a 6.
Then, given we got a 6, find the probability we had picked the unfair die.
Lastly, given that we got a 6 in the previous case, find the probability we get one more 6, by re-rolling this same die. In the first question, since the sample space is $(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,6,6,6,6)$ , the probability of getting a 6 is $\frac{7}{22}$ . In the second question, since we have 4 6s from the unfair die and 3 6s from the 3 fair dice, given that we got a 6, the probability of having picked the unfair die is $\frac{4}{7}$ . I am not sure about the last question: Clearly we must use Bayes' theorem but I am not very familiar with it. $P(A|B) = \frac {P(B|A)(P(A))}{P(B)}$ . $A$ is the event of picking the unfair die and $B$ the event of rolling a second 6 after the first roll is a 6. $P(B|A) = 1$ , $P(A) = \frac {1}{4}$ since the unfair die is 1 in 4. $P(B)$ So $P(A|B) = \frac {P(B|A)(P(A))}{P(B)}$ . Can you help me finish the last question? Thanking you in advance!","['bayes-theorem', 'probability']"
4316512,Why does integration by parts give me the wrong answer?,"I'm solving a first order differential equation. $$\frac{dy}{dx}+2xy=x$$ I multiplied by the integration factor to get the equation in the form $f\left(x\right)\frac{dy}{dx}+f'\left(x\right)y=f\left(x\right)Q$ $$f\left(x\right)=e^{x^{2}}$$ so $$e^{x^{2}}\frac{dy}{dx}+2xe^{x^{2}}y=xe^{x^{2}}$$ Rearranged $$e^{x^{2}}y=\int{xe^{x^{2}}}dx$$ I know the easiest and correct way to do this is by using a u substitution for $e^{x^{2}}$ but I tried using integration by parts and I got a completely different answer to using a u sub. I got $$y=\frac{1}{2}-\frac{1}{4x^{2}}+\frac{c}{e^{x^{2}}}$$ The correct answer using u sub is $$y=\frac{1}{2}+\frac{c}{e^{x^{2}}}$$ I don't understand why I'm getting a different answer. If I take $u = x$ and $\frac{dv}{dx} = e^{x^{2}}$ Using integration by parts I get $$\int{xe^{x^{2}}}=x\left(\frac{1}{2x}e^{x^{2}}\right)-\int\frac{1}{2x}e^{x^{2}}dx$$ Can anyone explain what am I doing wrong, or why this is getting me different answer? Or why can I not use to it to get the same answer? Thank you to whoever can tell me what I'm doing wrong.","['integration', 'substitution', 'ordinary-differential-equations']"
4316540,Intuition on the higher order tangent bundles,"The first order tangent bundle $TM$ can be thought of as the set of velocities at each point on the manifold. It can be formally defined in one of two ways: it can be the set of derivations on the manifold or the set of equivalence classes of tangent curves. I’ve read that the tangent bundle is itself a smooth manifold so you can then take the tangent bundle of that. My question would be how that would work. When I think of the tangent bundle, I think of a bunch of tangent spaces and I can’t grasp how they make a smooth manifold. Given the smooth structure, how would a higher order tangent space be defined and what would it mean? I’ve seen $T^2 M$ be defined as the set of second order derivations, as equivalence classes of curves that agree to to their second derivative, and recursively as $T^k M \equiv T(T^{k-1} M)$ . How are they related, what would the dimensionality of these tangent bundles be, and what is the geometric intuition behind it? How is the tangent bundle a smooth manifold? What is the definition of higher order tangent bundles? What is the geometric interpretation of them? What math can be done with them? Bonus: Could you explain the same for the cotangent bundle and its higher analogues along with mixed bundles like $T^6 T^{3*} M$ ?","['riemannian-geometry', 'smooth-manifolds', 'derivatives', 'tangent-bundle', 'differential-geometry']"
4316558,Trouble with the Jacobian for a camera projection matrix.,"I am using the Gauss Newton method to update a camera projection matrix. $$
x = \frac{PX}{P_\text{last row} X}
$$ where x is a known $3\times k$ matrix, X is a known $4\times k$ matrix, and $$
P = 
\begin{bmatrix}
p_1 & p_2 & p_3 & p_4\\
p_5 & p_6 & p_7 & p_8\\
p_{9} & p_{10} & p_{11} & p_{12}\\
\end{bmatrix}
$$ I have a loss function I want to minimize L, where $$
R(P) = \frac{PX}{P_\text{last row} X} - x
$$ and unroll r into $$
r(P) = 
\begin{bmatrix}
R(P)_{1,1} \\ R(P)_{2,1} \\ \vdots \\ R(P)_{2,k} \\ R(P)_{3,k}
\end{bmatrix}
$$ then $$
L(P) = r(P)^T \cdot r(P)
$$ I unroll P into a vector p $$
p = 
\begin{bmatrix}
p_1 \\ p_2 \\ \vdots \\ p_{11} \\ p_{12}\\
\end{bmatrix}
$$ I know $$
\frac{\partial L}{\partial P} =  2 \frac{\partial r}{\partial 
P} r(P)
$$ and then I have my Jacobian where I want to find $$
\frac{\partial r(P)}{\partial p} = J =
\begin{bmatrix}
\frac{\partial r(P)_{1,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,1}}{\partial p_{12}}\\
\frac{\partial r(P)_{2,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,1}}{\partial p_{12}}\\
\frac{\partial r(P)_{1,2}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,2}}{\partial p_{12}}\\
\vdots & \ddots & \vdots\\
\frac{\partial r(P)_{2,k}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,k}}{\partial p_{12}}\\
\end{bmatrix}
$$ I removed the rows that don't depend on P, every third row, because the last row of the output is always 1. $$
\frac{\partial R(P)_{1,i}}{\partial P_{1, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\
\frac{\partial R(P)_{1,i}}{\partial P_{2, j}} = 0 \\
\frac{\partial R(P)_{1,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{1, 1} + X_{2, i}P_{1, 2} + X_{3, i}P_{1, 3} + X_{4, i}P_{1, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}a_i}{c_i^2}\\
\frac{\partial R(P)_{2,i}}{\partial P_{1, j}} = 0 \\
\frac{\partial R(P)_{2,i}}{\partial P_{2, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\
\frac{\partial R(P)_{2,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{2, 1} + X_{2, i}P_{2, 2} + X_{3, i}P_{2, 3} + X_{4, i}P_{2, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}b_i}{c_i^2} \\
\vdots
$$ Doing some substitution I get $$
J =
\begin{bmatrix}
\frac{X_{1,1}}{c_1} & 0 & \frac{X_{1, 1}a_1}{c_1^2} & \cdots & \frac{X_{4, 1}a_1}{c_1^2}\\
0 & \frac{X_{1,1}}{c_1} & \frac{X_{1, 1}b_1}{c_1^2} & \cdots & \frac{X_{4, 1}b_1}{c_1^2}\\
\frac{X_{1,2}}{c_2} & 0 & \frac{X_{1, 2}a_2}{c_2^2} & \cdots & \frac{X_{4, 2}a_2}{c_2^2}\\
0 & \frac{X_{1,2}}{c_2} & \frac{X_{1, 2}b_2}{c_2^2} & \cdots & \frac{X_{4, 2}b_2}{c_2^2}\\
\vdots & \ddots & \ddots & \ddots & \vdots\\
0 & \frac{X_{1,k}}{c_2} & \cdots & \cdots & \frac{X_{4, k}b_k}{c_k^2}\\
\end{bmatrix}
$$ But when plugging this into the Gauss-Newton Method: $$
p_{n+1} = p_n - (J^TJ)^{-1}J^Tr(P_n)
$$ It just gives me wildly incorrect results. Where did I go wrong?","['jacobian', 'linear-algebra', 'partial-derivative', 'derivatives', 'total-variation']"
4316629,$100$ numbers are written around the circle,"$100$ positive numbers are written in a circle. The sum of any two
neighbors is equal to the square of the number following them
clockwise. Find all such sets of numbers. Let us introduce the notation $n=100$ , not forgetting that $n$ is even. Let the numbers start in clockwise order $a_{1}, a_{2}, \ldots, a_{n}$ . The sum of the conditions of the problem gives $(a_{1}-1)^{2}+(a_{2}-1)^{2}+\ldots+(a_{n}-1)^{2}=n$ . Suppose that two adjacent numbers can simultaneously be at least $2$ , and at least one of them is strictly greater than $2$ . Then all subsequent numbers in the circle are strictly greater than $2$ , which contradicts the obtained equality. The case is considered similarly when two adjacent numbers can simultaneously be no more than $2$ , and at least one of them is strictly less than $2$ . Thus, the numbers ""alternate"": no less than two and no more than two. Suppose WLOG that $a_{1}+a_{2} \geq 4$ . Then $$ \begin{gathered} a_{1}+a_{2} \geq 4, a_{3}+a_{4} \geq 4, \ldots, a_{n-1}+a_{n} \geq 4, \\ 4 \geq a_{2}+a_{3}, 4 \geq a_{4}+a_{5}, \ldots, 4 \geq a_{n}+a_{1}. \end{gathered} $$ Adding all the inequalities, we get the inequality $0≥0$ . Therefore, $$ a_{1}+a_{2}=a_{3}+a_{4}=\ldots, a_{n-1}+a_{n}=a_{2}+a_{3}=a_{4}+a_{5}=\ldots=a_{n}+a_{1}=4. $$ So all numbers are equal to two. QUESTION : If we add up all the inequalities and get the identity, then we cannot draw any conclusions beyond the fact that this situation is possible. How was the consequence obtained that the sums of all pairs are equal to $4$ ?","['number-theory', 'elementary-number-theory']"
4316648,Injectivity of $I\leadsto V_I(-)$ and relation to Hilbert's Nullstellensatz,"The functor $\mathbb A^n_k(-):\mathbf {Alg}_k\to \mathbf{Set}$ is represented by $\operatorname{Hom}_k(k[x_1,\dots ,x_n],-)$ ; for every ideal $I\subseteq k[x_1,\dots ,x_n]$ , there is a subfunctor $V_I(-)\subseteq \ A^n_k(-)$ that sends a $k$ -algebra $K$ to the set (in $K^n$ ) of the solutions of (all the polynomials in) $I$ . Clearly $V_I(-)$ is represented by $\operatorname{Hom}_k(k[x_1,\dots ,x_n]/I,-)$ , meaning that the association $I\leadsto V_I(-)$ is injective. However if we restrict the domain of our functors to $\mathbf {Alg}^{\mathbf{red}}_k\subseteq \mathbf {Alg}_k$ , i.e. the reduced $k$ -algebras, we lose the injectivity: for example, $V_{(x)}(-)$ and $V_{(x^2)}(-)$ become the same functor. My first question is: if I only consider radical ideals, then the association $I\leadsto V_I(-)$ returns injective? I would say yes because if $I=\sqrt{I}$ , then $k[x_1,\dots ,x_n]/I$ is reduced and so $V_I(-)$ is representable also as a functor on $\mathbf{Alg}^{\mathbf{red}}_k$ . My second question is: does Hilbert's Nullstellensatz link in any way with these observations? They remind me of the bijection between radical ideals and algebraic varieties, but I can't see clearly the connection, if it exists. Thanks for any clarification.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4316714,Is D a borel subset?,"I need to show that $D=\{(x,x)\in \mathbb{R}^2 :x \in [0,1]\}$ is a closed set, and conclude that $D \in B(\mathbb{R}^2)$ (Borel subset). I am a bit struggling to show that D is a closed set. Is it allowed to state $D=[0,1]\times[0,1]$ and $[0,1]$ is closed in $\mathbb{R}$ , thus the cartesian product $[0,1]\times[0,1]$ is also closed? And then I come to the next part. It is not really clear for me what a Borel sigma algebra means. I thought we have that $D \in B(\mathbb{R}^2)$ , because since $D$ is open it is the complement of an open set. And since the Borel sigma algebra is the smallest sigma algebra containing all open sets, it must also contain all closed sets (by the properties of a sigma algebra.) Is this correct?","['measure-theory', 'real-analysis']"
4316778,Behavior of the roots of an infinite series.,"I have the polynomial $P_n(z)=1-\sum_{k=1}^{n}z^k$ . We know that this polynomial has exactly $n$ roots in $\mathbb{C}$ . Let $\rho$ be the number of roots of $P_n$ , thence if $n\to\infty$ then $\rho$ must tend to $\infty$ too. Though, if we interpret the sum as a geometric series, we get that $$P_n(z)=1-\sum_{k=1}^{n}z^k=\frac{z^{n+1}-2z+1}{1-z}$$ And if we make $n\to\infty$ it only converges for $|z|<1$ , becoming $P_\infty(z)=-\frac{2z-1}{1-z}$ , that has only one real root for $z=\frac{1}{2}$ . So, where did the other roots go? I plotted $P_n$ variating $n$ and noted that the roots tend to accumulate on the unit disk. ( Interactive Mapping ). So, can we say that all the roots will accumulate on the unit disk as $n\to\infty$ for $z\neq1$ ? How can we prove this? Thanks! Here I have some screenshots of the mapping. Respectively, $n=5$ , $n=10$ , $n=100$ . You can see that the roots tend to accumulate towards the unit circle. | | Thanks.","['complex-analysis', 'power-series', 'polynomials', 'complex-numbers']"
4316780,"$||x|| \le ||x+ry||$ for all $r \ge 0 \implies \langle j(x), y \rangle \ge 0$, where $j$ is the duality map.","Let $X$ be a real Banach space. Let $J \colon X \to 2^{X^*}$ be its (normalized)  duality map, $$ J(x) = \{ x^* \in X^* \colon \langle x^* , x \rangle =||x|| \ ||x^*||, \ || x^* ||=||x||   \} , \ x \in X.$$ Assume that $X$ is smooth, so that $J(x)= \{j(x)\}$ is a singleton. Fix $x,y \in X$ . It is known that, if $$||x|| \le ||x+ry||,  \tag 1$$ for every $r \in \mathbb R$ , then $\langle j(x),y \rangle =0$ .   What happens if we only take $r \ge0$ in $(1)$ ? I expect that $ \langle j(x),y \rangle  \ge 0$ , since that is the case for Hilbert spaces. Indeed, if $X$ is a Hilbert space with inner product $(\cdot,\cdot) $ , then $J$ is just the identity map, and $(1)$ implies that $$ r^2 ||y||^2 + 2r (x,y) \ge 0, $$ for every $r \ge 0$ .  Dividing by $r$ and then letting $r \to 0$ we obtain that $(x,y) \ge 0$ .","['orthogonality', 'functional-analysis']"
4316868,Confused about adding cardinalities of sets as elements,"Here is an exercise from this book . Consider the sets $A$ and $B$ , where $$A = \{3, |B|\}$$ and $$B = \{1, |A|, |B|\}$$ . What are the sets? Here is the solution from the book: We need to be a little careful here. If $B$ contains 3 elements, then $A$ contains just the number 3 (listed twice). So that would make $$|A| = 1$$ , which would make $$B = \{1, 3\}$$ , which only has 2
elements. Thus $$|B| \ne 3$$ . This means that $$|A| = 2$$ so $B$ contains at least the elements 1 and 2. Since $$|B| \ne 3$$ , we must
have $$|B| = 2$$ which agrees with the definition of $B$ . Therefore
it must be that $$A = \{2,3\}$$ and $$B = \{1, 2\}$$ . I do not understand how the solution ended. If at the end $A$ now has 2 elements, $B$ should be updated as well. Previously $B$ knows that $A$ is just $\{3\}$ with $$|A| = 1$$ , but since $A$ now has two elements 2 and 3, then $|A| = 2$ and $B$ must be $\{1, 2, 3\}$ . The last $3$ is the new cardinality of $B$ . Of course, if I go this route, then $A$ must be updated as well because now $$|B| = 3$$ , making $A$ go back to $\{3\}$ . Thus we go back to the original forms of the sets, namely $$A = \{3\}$$ and $$B = \{1,2\}$$ and so on. It seems to me, finding both cardinalities leads to never ending transformations. There must be something that I am missing. Why is it correct to end the sets such that $$A = \{2,3\}$$ and $$B = \{1,2\}$$ . It seems to only respect the cardinality of $B$ but did not continue to respect the cardinality of $A$ . But continuing on this reasoning will lead to an infinite loop. Can someone explain what is wrong in my thinking?","['elementary-set-theory', 'discrete-mathematics']"
4316965,Is there a known simple characterization of the category $\mathbf{Grp}^{\text{op}}$?,"For some common categories, there are nice characterizations of their opposite category: ie any group is isomorphic to it's opposite, $\mathbf{Set}^{\text{op}}$ is equivalent to the category of complete atomic boolean algebras and $\mathbf{CRing}^{\text{op}}$ is equivalent to the category of affine schemes. This question also asks about a description of the opposite category of $\mathbf{Top}$ , also has some interesting answers. Is there a nice characterization of $\mathbf{Grp}^{\text{op}}$ (ie as some simple concrete category)? Has this category been studied, and if yes, what are some references about its properties?","['abstract-algebra', 'soft-question', 'category-theory', 'reference-request']"
4316971,"In a Banach Algebra, can the product of two elements be invertible if we know one of the element is non invertible?","In a Banach Algebra, can the product of two non-invertible element be invertible? Basically in linear algebra, we know that if the product of two matrix is invertible, then both matrix must be invertible. The proof is very simple if we use determinants. Now for general Banach space with infinite dimensions, there is no such thing as a determinant, so I was wondering if we have two elements whose product is invertible, do we know if both elements are invertible? And what if the elements commute?","['linear-algebra', 'functional-analysis']"
4317061,A $\sigma$-algebra that is complete as a Boolean algebra?,"This MO thread has several examples of complete Boolean algebras that are not isomorphic to $\sigma$ -algebra of sets. But what is a non-trivial example of a complete Boolean algebra that is isomorphic to a $\sigma$ -algebra of set as Boolean algebras? Equivalently, is there a $\sigma$ -algebra of set that is complete as a Boolean algebra? A trivial example is the power set algebra $\mathcal{P}(X)$ , or any algebra of set that is atomic and complete as Boolean algebra. Also, from the above post it seems a non-trivial example is probably not ccc. I am wondering if the tree algebra on some tree such as $2^{<\aleph_1}$ can help, but the tree algebra itself doesn't seem complete, or even $\sigma$ . Maybe we can consider its completion? Clarification: By a complete Boolean algebra I mean a Boolean algebra whose any subset has supremum. An algebra of set (on $X$ ) is a nonempty subset of $\mathcal{P}(X)$ closed under union and complementation. An algebra of set can be viewed as a Boolean algebra in an obvious way. Edit: Now I have the feeling that the Boolean completion of the poset $(2^{<\aleph_1},\supseteq)$ should does it. This is the poset of partial map from $\aleph_1$ to $\{0,1\}$ with countable domain, under inverse inclusion (the largest element being the empty map). Every branch in $2^{<\aleph_1}$ , equivalently every element in $2^{\aleph_1}$ , determines an $\sigma$ -complete ultrafilter in the Boolean completion because the branch has uncountable cofinality. The Boolean completion seems to be the regular-open algebra on $2^{\aleph_1}$ with countable support product topology. I'm not sure if it is extremally disconnected; probably not.","['boolean-algebra', 'measure-theory', 'set-theory']"
4317065,Is the map sends $T$ to $T^*$ adjoint of $T$ surjective?,"Let $B(X)$ denotes the set of all bounded linear operators from $X$ to $X$ , where $X$ is a Banach space. Same is defined for the set $B(X^*)$ , where $X^*$ denotes the set of all bounded linear functionals of $X$ , that is all bounded linear functionals from $X$ to the field $\mathbb F$ . In other words, $X^*$ is the dual space of $X$ . Let us now define a map $\alpha :B(X) \to B(X^*)$ by. $$\alpha(T)=T^*, \quad T \in B(X)$$ where $T^*$ is the Banach space adjoint of the operator $T$ .
That is, for Banach spaces $X,Y$ and $T\in B(X,Y)$ the adjoint operator $T^*: Y^* \to X^*$ is defined as $$T^*(y^*)(x)=y^*(T(x)),~~~y^* \in Y^*,~~x \in X.$$ Now note that, since $\|T\|=\|T^*\|$ , the map $\alpha$ is an isometry and also injective. Can I show that $\alpha$ is a linear isomorphism? Linearity of $\alpha$ comes from the linearity of adjoint operators but I am not able show that $\alpha$ is surjective. One of my friends told me $\alpha$ may not be surjective but can't give any argument.","['banach-spaces', 'operator-theory', 'functional-analysis', 'dual-spaces', 'adjoint-operators']"
4317095,Limiting distribution CLT problem,"Let $X_1, X_2, ....$ be iid RV with mean 0, variance 1, and $E(X_i^4)$ is finite, Show that the limiting distribution of $Z_n = \sqrt{n} \frac{X_1X_2 + X_3X_4 +..... + X_{2n-1}X_{2n}}{X_1^2+X_2^2 + .... + X_{2n}^2}$ converges to N(0, 1) in distribution. I think this problem uses CLT to prove. But I'm just stuck on where to start. Any help will be appreciated.","['probability-distributions', 'probability-theory', 'central-limit-theorem']"
4317114,Is there a graph with all vertices having degree 3 or greater that doesn't have a hamiltonian path?,"Is there an example of a simple, undirected graph with all vertices having degree 3 or greater that doesn't have a hamiltonian path? I've seen this question appear in the title of this post, but then the author proceeds to talk about cycles and chords rather than hamiltonian paths. I haven't been able to come up with any simple, undirected graphs where all the vertices are of degree 3 or greater and there doesn't exist a hamiltonian path.","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
4317116,Reference Request: Explicit formula for the exponential of a triangular matrix in terms of convolutions,"In this previous question it came up that there's a pretty explicit formula for the exponential of a triangular matrix. In the below, let $*$ denote convolution and define $$
G_{\lambda_i}(t) =
\begin{cases}
\exp(\lambda_i t) & t \geq 0 \\
0 & \text{otherwise.}
\end{cases}
$$ Then, given a lower triangular matrix $$
L = \begin{pmatrix}
\lambda_1 &  &  &  \\
l_{21} & \lambda_2 &  & \\
\vdots& & \ddots & \\
l_{n1}&l_{n2} &\dots &\lambda_n
\end{pmatrix}
$$ for $t \geq 0$ , the $(a,b)$ entry of the matrix exponential is $$
\left(\exp(tL)\right)_{ab}
=
\sum_{a = i_1 > i_2 > \dots > i_k = b} l_{i_1i_2}\cdots l_{i_{k-1}i_k} (G_{\lambda_{i_1}} * \dots * G_{\lambda_{i_k}})(t).
$$ For example, $$
\left(\exp(tL)\right)_{31}
=
l_{31} (G_{\lambda_3} * G_{\lambda_1})(t) + l_{32} l_{21} (G_{\lambda_3} * G_{\lambda_2} * G_{\lambda_1})(t).
$$ This is fairly straightforward to see by induction. We're building up a solution to a differential equation $d/dt \exp(tA) = A \exp(tA)$ . At each induction step there is an inhomogenous linear equation for the new entries, whose homogenous part has Green's function $G_{\lambda_n}$ , so the inhomogenous solution is a combination of convolutions of the previous bits with that. I suspect there's also some combinatorial interpretation and argument. Probably something in terms of Laplace transforms too. This is a bit different from the way explicit formulas for exponentials of matrices are usually discussed: Usually, we reduce things all the way to Jordan blocks rather than merely triangular matrices. Usually, we decompose $G_{\lambda} * G_{\mu}$ into a linear combination $(\lambda - \mu)^{-1} G_\lambda - (\lambda - \mu)^{-1} G_\mu$ , and handle the $\lambda = \mu$ case separately: $(G_\lambda * G_\lambda)(t) = t G_\lambda(t).$ (Avoiding both of these was useful for the previous question linked above; the goal was to find uniform bounds, and both the JNF and the decomposed-into-linear-combinations expressions are sensitive to small changes in the matrix entries.) This direct formulation in terms of convolutions of Green's functions feels like a ""textbook"" result from some textbook somewhere, but I'm not familiar enough with the topic to know where. Does anyone have a reference in the literature that states more or less this?","['ordinary-differential-equations', 'linear-algebra', 'combinatorics', 'reference-request']"
4317159,Pullback of $n$-sphere volume form via Gauss map,"Let $M \subset \mathbb{R}^{n+1}$ be a Riemannian hypersurface, and let $N$ be a smooth unit normal vector field along $M$ . Denote by $\nu : M \to \mathbb{S}^n$ the Gauss map associated to $N$ . Show that $$\nu^*\mathrm{vol}_{\mathbb{S}^n} = (-1)^n K \mathrm{vol}_M, $$ where $K$ is the Gaussian curvature of $M$ , $\mathrm{vol}_{\mathbb{S}^n}$ is the standard volume form of $\mathbb{S}^n$ , and $\mathrm{vol}_M$ is the volume form of $M$ . I am not particularly sure how to prove this statement. I tried proving this locally, in a coordinate chart, but the computations get messy. More precisely, I am not sure how to relate the volume form on $\mathbb{S}^n$ with the Gaussian curvature of $M$ and its volume form.","['curvature', 'riemannian-geometry', 'differential-geometry']"
4317160,Combinatorial design to compress a Boolean lattice without confusing small sets,"Is there a kind of combinatorial design that controls the sizes of small unions of the blocks? I'm looking for a set $B$ of $|B|=b$ blocks, where $b\sim100$ , which are subsets of a set $X$ of $|X|=v$ points, $v=32$ or less preferably, $v=64$ . The blocks don't need to have a uniform size, but they should all be nonempty. More generally, given a union of a ""small"" set of blocks, it should be possible to infer the number of blocks in the union. To be precise, there should be an integer $\ell\geq 2$ , ideally as large as possible, such that $B$ has the following property: If $P$ and $Q$ are subsets of $B$ with $|P|<|Q|\leq \ell$ , then $\left|\bigcup P\right|<\left|\bigcup Q\right|$ . Note that if $b\leq v$ then this would be trivial. Just let $B$ be a set of $b$ distinct singleton subsets of $X$ . Then for all $P\subset B$ we get $\left|\bigcup P\right|=|P|$ , so the highlighted property is guaranteed for all $\ell$ . So the whole point of this question is asking if we can achieve some compression, i.e. $b>v$ . Note that in the $b>v$ regime we can't possibly achieve $\ell=b$ anymore, but I'd still like for $\ell$ to be large. For example, perhaps there is a design with $b=128$ , $v=32$ , and $\ell=3$ , such that every block has size $4$ , every union of two blocks has size $6$ , $7$ , or $8$ , and every union of three blocks has size $9$ or more. Arguments that limit $\ell$ or construct nice designs would be helpful. References to what these things are called would also be helpful! Edit: According to https://www.win.tue.nl/~aeb/codes/Andw.html we have these nice constant-weight binary codes to work with: $A(64, 6, 4) = 336$ citing Brouwer et al (1990) "" A new table of constant weight codes "" $A(64, 8, 5) = 192$ citing a Steiner system $S(2,5,65)$ . (How does that work?) $A(63, 10, 6) = 126 $ citing Smith et al (2006) "" A New Table of Constant Weight Codes of Length Greater than 28 "" who list it on page 12, Table 7, bottom row as ""CC"" meaning a construction from a cyclic permutation These constructions give us blocks that intersect each other at at most one point. From that, we can achieve $\ell=3$ . In fact, the last construction of weight $6$ almost gets us to $\ell=4$ : Each block has weight $6$ . A union of two blocks has weight $11$ or $12$ . A union of three blocks has weight $15$ to $18$ . A union of four blocks has weight $18$ to $24$ . If there were some argument why a union of four blocks could be forced to have weight $\geq 19$ , then we'd get $\ell=4$ after all. Given those results, my questions are, more specifically: Are these sorts of codes the best bet for satisfying the inequality that I want? Is there a way to get to $\ell=4$ with $b\geq100$ and $v\leq64$ ? Is there a literature on this question that I've missed?","['combinatorial-designs', 'coding-theory', 'combinatorics', 'reference-request']"
4317193,Is is mathematically correct to write $\frac{h>2}{l<4}\neq\frac{1}{2}$ therefore $\sin^{-1}\frac{h>2}{l<4}\neq30^{\circ}$?,"The goal is to say that an h>2 dived by an l<4 will never be 1/2 therefore the inverse sine of an h>2 dived by an l<4 can never be 30 degrees. Can I express is like so? $\frac{h>2}{l<4}\neq\frac{1}{2}$ therefore $\sin^{-1}\frac{h>2}{l<4}\neq30^{\circ}$ Or is that mathematically ""illegal""?","['algebra-precalculus', 'trigonometry']"
4317220,Convergence of partial sum in $L^q$,"Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of real independent random variables on a probabolity space $(\Omega, F, P)$ such that $X_n^2 \in L^1$ and $\mathbb{E}[X_n]=0$ for every $n$ . Assume $\sum_{i=1}^{\infty}\mathbb{E}[X_n^2]< \infty$ . Put $S_n= \sum_{i=1}^nX_i$ . As a classical result, we have $S_n \to S$ almost surely and in $L^2$ as $n \to \infty$ , where $S=\sum_{i=1}^{\infty}X_i$ . I want to prove the following inequality which is supposed to hold for every $p \in (1, \infty)$ : \begin{equation*}
        (\mathbb{E}[\sup_{n \geqslant 1} |S_n|^{2p}])^{\frac{1}{p}} \leqslant \frac{p}{p-1} (\mathbb{E}[|S|^{2p}])^{\frac{1}{p}}
    \end{equation*} Here is my attempt: I was able to prove that the following holds for every $t>0$ : \begin{equation*}
        P(\sup_{n \geqslant 1}S_n^2 > t) \leqslant \frac{1}{t} \mathbb{E}[S^2; \sup_{n \geqslant 1}S_n^2 > t]
    \end{equation*} We can assume without loss of generality that $S \in L^{2p}$ . Truncate with $Y_k= \chi_{\sup_{n \geqslant 1} |S_n|^{2} \leqslant k}\sup_{n \geqslant 1} |S_n|^{2}$ . Here $\chi$ is the indicator function. It suffices to prove the following holds: \begin{equation*}
(\mathbb{E}[Y_k^p])^{\frac{1}{p}} \leqslant \frac{p}{p-1} (\mathbb{E}[|S|^{2p}])^{\frac{1}{p}}
\end{equation*} First of all, as a classical result: \begin{equation*}
    \mathbb{E}[X] = \int_{0}^{\infty}P(X > t)dt
\end{equation*} If $p>1$ , then by a simple change of variable: \begin{equation*}
    \mathbb{E}[X^p] = \int_{0}^{\infty}P(X > t^{\frac{1}{p}})dt = p \int_0^{\infty}t^{p-1}P(X > t)dt
\end{equation*} We can apply this to convert the expectations to Lebesgue integrals and apply 1: \begin{equation*}
    (\mathbb{E}[Y_k^p])^{\frac{1}{p}} = (p \int_0^{k}t^{p-1}P(Y_k > t)dt)^{\frac{1}{p}} \leqslant (p \int_0^{k}t^{p-2} \mathbb{E}[S^2; \sup_{n \geqslant 1}S_n^2 > t]dt)^{\frac{1}{p}}
\end{equation*} On the other hand, we can do the same for the right side of the inequality: \begin{equation*}
    \frac{p}{p-1} (\mathbb{E}[|S|^{2p}])^{\frac{1}{p}} = \frac{p}{p-1} (p\int_0^{\infty}t^{p-1}P(S^2>t)dt)^{\frac{1}{p}}
\end{equation*} And I am stuck as I do not have a way to connect the expectation of $S^2$ with the probability and the order of $t$ does not match. As a second attempt, I notice that the inequality looks very similar to Hardy's. Observe that $\frac{p}{p-1}=\int_0^1x^{\frac{1}{p}}dx $ . I was trying to use Fubini's theorem like in the proof of Hardy's but I did not end up getting anything. Hints will really be appreciated. Thanks in advance for the help!",['probability-theory']
4317227,"Closed form of $\sum\limits_{n=1}^\infty \frac{γ(n+a,bn)n^k}{(n+c)!}$ with the Lower Gamma function?","$$\Large{\text{Goal:}}$$ One goal is to find better ways of expressing: $$\sum_{n=0}^\infty \frac{(pn+q)^{rn+s}Γ(An+B,Cn+D)}{Γ(an+b)}$$ $$\Large{\text{Special Case:}}$$ Here are some closed form special cases using the Regularized Lower aand upper Incomplete Gamma function and Lower Incomplete  gamma function $$γ(a,z)=\Gamma(a)-\Gamma(a,z)\\\text P(a,z)=\frac{γ(a,z)}{\Gamma(a)}\\Q(a,z)=\frac{\Gamma(a,z)}{\Gamma(a)}\\\text P(a,z)+Q(a,z)=1\\\sum_{n=1}^\infty \frac{γ(n+a,bn)}{n! n^{a+1}}\mathop=^{0\le b\le1}\frac{b^{a+1}}{a+1}\implies\sum_{n=1}^\infty \frac{\Gamma(n+a,bn)}{n! n^{a+1}}= -\frac{b^{a+1}}{a+1}+\sum_{n=1}^\infty \frac{(n+1)_{a-1}}{ n^{a+1}} =? $$ The $\sum\limits_{n=1}^\infty \frac{(n+1)_{a-1}}{ n^{a+1}}$ seems to be a sum of Riemann Zeta function values since you can try the following for $a\in\Bbb N$ to get a closed form of the simpler sum: $$\sum_{n=1}^\infty \frac{\Gamma\left(n+4,\frac n2\right)}{n! n^{5}}=\frac{\pi^2}{6}+\frac{11\pi^4}{90}+6ζ(3)+6ζ(5)-\frac1{160}= 26.9781475874885318135350777164… $$ where appears the Pochhammer symbol $(A)_x$ . The restriction on $b$ is loose since: $$\sum_{n=1}^\infty \frac{γ\left(n+i,\frac n3\right)}{n!n^{i+1}}=\frac{3^{-1-i}}{1+i}= \frac{1-i}{6}(\cos(\ln(3))-i\sin(\ln(3)))=-0.072624103140189550674560360495022601835... -
0.2242349107490594690170820750267034298651... i$$ Conjectures: $$\sum_{n=0}^\infty \frac{Q(n+1,n)}{n^3}\mathop=^? ζ(3)-\frac13$$ $$\sum_{n=0}^\infty \frac{Q(n+1,n)}{n^2}\mathop=^?\frac{\pi^2}6-\frac12$$ Please correct me and give me feedback! $$\Large{\text{Mini Goal:}}$$ Using the above notation, I have noticed a closed form for the following $γ(a,z)=\int_0^z t^{a-1} e^{-t} dt$ series: $$\sum\limits_{n=1}^\infty \frac{γ(n+a,bn)n^k}{(n+c)!}\\\\ \sum_{n=1}^\infty \frac{γ\left(n+2,\frac n5\right)n^3}{n!}=\frac{3982173}{10485760}\\ \sum_{n=1}^\infty \frac{γ\left(n+1,\frac n5\right)}{n!} =\frac1{32}\\ \sum_{n=1}^\infty \frac{γ\left(n,\frac n4\right)}{(n-3)!}=\frac{1717}{2916}\\ \sum_{n=1}^\infty \frac{γ\left(n,\frac n2 \right)}{n!}=\ln(2)\\ \sum_{n=1}^\infty \frac{γ\left(n,\frac n2 \right)}{n^3 n!}=\frac{119}{288} \\ \sum_{n=1}^\infty \frac{γ\left(n+2,\frac n3 \right)}{n^2 (n-2)!} =\ln\left(\frac23\right)+\frac{89}{192}$$ What is a closed form for: $\sum\limits_{n=1}^\infty \frac{γ(n+a,bn)n^k}{(n+c)!}$ ? The easiest cases would be those with $a,-c\in\Bbb N \text{ or } 0,\frac1b\in\Bbb N,k\in\Bbb Z$ , but these are just a suggestion if there is no closed form outside of these restrictions.","['special-functions', 'lambert-w', 'closed-form', 'sequences-and-series', 'recreational-mathematics']"
4317277,How to calculate odds in finance,"I'm studying both Financial Math and Statistics , and it came to my mind how could one interrelate them. So I thought in an example and started to scribble: Given two business operation, the first one with 80% of sucess and 5% interest rate per month ; the second one with 95% of sucess and 1,5% interest rate per month . Considering that if the operation fails, no money is lost , which one is more likely profitable at medium term (10 months)? The first one: $$M = x\cdot(1+0,05)^{10}$$ $$M = 1,63x$$ $$Odds = (\frac{80}{100})^{10} = 10,47\%$$ The second one: $$M = x\cdot(1+0,015)^{10}$$ $$M = 1,16x$$ $$Odds = (\frac{95}{100})^{10} = 59,87\%$$ In the first operation , the most probable case is 10,47% of earning 63% income ; while in the second operation , the most probable case is 59,87% of earning 16% income . But it seems that analysis is not fair, as both have different chances, so I thought that in order to make it pair, I should calculate the odds of the first operation (as it haves more income) to have same income as the second one. $$(1+0,05)^x = 1,16$$ $$log\;1,05^x = log\;1,16$$ $$x = \frac{log\;1,16}{log\;1,05}$$ $$x = 3,042 \approx 3$$ It has to be a success 3 times to produce the same result, and the odds are: $$Odds = (\frac{80}{100})^{3} = 51,2\%$$ My questions are: 1 - Is this analysis logical and coherent? 2 - Is it right to assume that the second case is statistically a better choice? 3 - How can this extend into a case that considers money lost at failure?","['statistics', 'finance', 'probability']"
4317287,$S^1$-valued function on $T^n$,"Let $f:T^n\to S^1$ be a smooth function on the $n$ -torus $T^n=S^1\times \cdots \times S^1$ . The differential $df$ can be viewed as a closed 1-form on $T^n$ (not exact).
Moreover, it should give a nonzero cohomology class $[df]$ in $H^1(T^n;\mathbb Z)$ in $\mathbb Z$ -coefficients. Concerning the natural isomorphism $H^1(T^n;\mathbb Z)\cong \mathrm{Hom}(\pi_1(T^n),\mathbb Z)$ , this should mean the map sending a loop $\sigma$ in $T^n$ to the integer $\mathrm{deg}(f\circ \sigma)$ . (Note that $f\circ \sigma: S^1\to S^1$ ) I believe all these are standard, but I fail to find a reference or write down enough details to convince myself. Could you help me to make this clear? Or, maybe I was mistaken somewhere?","['differential-forms', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4317307,Solution to the functional equation $f(2x) = f(x)\cdot\sin(x)$?,"Solution to the functional equation $f(2x) = f(x)\cdot\sin(x)$ ? At first I believe that finding an answer to that equation it was going to be an easy problem, since this other equation $g(2x) = g(x)\cdot\cos(x)$ could be solved by $g(x) = \text{sinc}(x) = \frac{\sin(x)}{x}$ , but I have already tried many different ansatz unsuccessfully for $f(x)$ (real and complex valued functions), so this kind of innocent equation have already beaten me. Hope you can explained how to solve it, but first of all, please display the solution to the problem (I need it to test solutions to other related problems), this because I am starting to believe that maybe it don't have a solution, or at least a simple one through standard functions. Added later: Reading your answers I understand now is far from be an easy question as I believe at first sight... For some of you that ask me about additional characteristics for obtain a solution, I am aiming to find a solution $f(x) \in \mathbb{C}$ analytic, since I was thinking about $f(x)$ as a Fourier transform... but reading the answers it looks I will never find its inverse neither... As @Sangchul Lee explain in the comments, the equation: $$s(2x) = s(x)\cdot\frac{\sin(x)}{x}$$ have been shown here to have a difficult solution: $$s(x) \propto \prod\limits_{p=1}^\infty \frac{\sin(x/2^p)}{x/2^p}$$ Now, by letting $s(x) = f(x)\cdot q(x)$ such as: $$ s(2x) = f(2x)\cdot q(2x) = f(x)\cdot q(x)\cdot\frac{\sin(x)}{x}$$ And following Wolfram-Alpha here if I make the split as: $$ q(2x) = q(x)/x \Rightarrow q(x) \propto \sqrt{x}\,e^{-\frac{\log^2(x)}{\log(4)}}$$ So given there is an existent $q(x) \in \mathbb{R}$ :... It is possible to replace $q(2x)/q(x) = 1/x$ in $ f(2x)\frac{q(2x)}{q(x)} = f(x)\sin(x)\cdot\frac{1}{x}$ and simplify it to $f(2x) = f(x)\sin(x)$ ?? Does it means that the main question have at least as solution $f(x) = c\,\frac{\displaystyle{e^{\log^2(x)/\log(4)}}}{\sqrt{x}}\prod\limits_{p=1}^\infty \frac{\sin(x/2^p)}{x/2^p}$ ??? (with "" $c$ "" an arbitrary constant)","['functional-equations', 'functions', 'fourier-analysis', 'real-analysis']"
4317316,"Show that if $|z|=2$, $\text{Im}(1-\bar{z}+z^2)\le 7$.","Show that if $|z|=2$ , $|\text{Im}(1-\bar{z}+z^2)|\le 7$ . My attempt: Let $z=x+yi$ , $|z|=\sqrt{x^2+y^2}$ , $|z|^2=(\sqrt{x^2+y^2})^2=x^2+y^2$ . Since $|z|=2$ , then $|z|^2=2^2=4$ . So $x^2+y^2=4$ . Also, $(x-y)^2\ge0$ $\implies x^2+y^2\ge2xy$ ; $$xy\le2 $$ So $|y|\le2$ and $|x|\le 2$ . Now, $$\begin{aligned}
1-\bar{z}+z^2&=1-(x-yi)+(x+yi)^2\\
&=1-x+yi+x^2+2xyi-y^2\\
&=1-x+x^2-y^2+(2xy+y)i
\end{aligned}$$ Thus, $$\begin{aligned}
\ |\text{Im}(1-\bar{z}+z^2)|&=|y+2xy|\\
&\le|y|+|2xy|\\
&\le |y|+|x^2+y^2|\\
&\le2+4=6
\end{aligned}$$ How about the case when it is equal to $7$ ?",['complex-analysis']
4317353,How to find the sum $\sum_{k=1}^{\lfloor n/2\rfloor}\frac{2^{n-2k}\binom{n-2}{2k-2}\binom{2k-2}{k-1}}{k}$,"Let $n$ be positive integer, find the value $$f(n)=\sum_{k=1}^{\lfloor n/2 \rfloor}\dfrac{2^{n-2k}\binom{n-2}{2k-2}\binom{2k-2}{k-1}}{k}. $$ I have found $$ f(2)=1, \quad f(3)=2, \quad f(4)=5, \quad f(5)=14, \quad f(6)=42, \quad f(7)=132. $$ It seem OEIS (A000108) : $$ f(n)=\dfrac{1}{n}\binom{2n-2}{n-1},$$ but how to prove it?","['contest-math', 'summation', 'catalan-numbers', 'binomial-coefficients', 'combinatorics']"
4317354,Inscribe a triangle in a circle to produce four areas. Can the four areas be integer values simultaneously?,"Inscribe a triangle in a circle to produce four areas (three sectors and a triangle). Can the four areas be integer values simultaneously? My current processes: Let radius be $r$ , $\angle{AOB}=\alpha$ , $\angle{BOC}=\beta$ , $\angle{AOC}=\theta$ Then the area of the disk is $A=\pi r^2  \in \mathbb{Z}$ $\therefore r=\sqrt{\dfrac{A}{\pi}}$ $\therefore$ Area of minor segment $AB=\dfrac{1}{2} r^2(\alpha-\sin\alpha) = \dfrac{A}{2\pi} (\alpha-\sin\alpha)\in \mathbb{Z}$ So the problem is congruent to: does an $\alpha$ exists so that $(\alpha-\sin\alpha)$ is divisible by $2\pi$ ? Three of such $\alpha$ form a set of $(\alpha,\beta,\theta)$ sum to $2\pi$ .","['number-theory', 'geometry']"
4317359,$PQ ∥ BC$ for isosceles $\triangle ABC$ and inscribed equilateral $\triangle PQR$ with $R$ being midpoint of $BC$,Triangle $ABC$ is isosceles. An equilateral triangle $PQR$ is inscribed in it with $R$ being the midpoint of $BC$ . How can you prove $PQ \parallel BC$ ?,"['congruences-geometry', 'triangles', 'geometry']"
4317376,Prediction interval method explanation,"For making prediction intervals, the book ( Applied statistics and probability for engineers by montgomery and runger 5th edition ) creates a new random variable called prediction error and based on that (after translation and scaling to standard $T$ ) it creates a PI for $X_{n+1}$ . My question is this:
Why not use $\overline{X}$ as an estimate for mean of $X_{n+1}$ and use sample standard dev as estimate for std of $X_{n+1}$ and basically create a PI based on that like we do for confidence interval. Why use prediction error? What is wrong with my method? Thank you","['statistics', 'confidence-interval', 'normal-distribution']"
4317408,Using R for Introductory Statistics - Verzani Problem 8.6,"I am trying to solve the Verzani problem 8.6 in his book, but I have no idea how to. I have to use the confidence intervals to solve the problem.
Here is the picture of the problem: Problem 8.6 A student wishes to find the proportion of left-handed people at her college. She surveys 100 fellow students and finds that only 5 are left-handed.
If she computed a 95% confidence interval would it contain the value of
p = 1/10? I have tried to use the confidence interval formulas, I have already solved some of these problems using the confidence interval formula, but I don't know how to solve the last part - would it contain the value of
p = 1/10? Formula I am using by using this formula, lower is $-0.1410372$ and upper is $0.2410372$ . If someone would help me get an answer so I can learn from it or guide me to an answer. Thank you so much...","['statistics', 'confidence-interval']"
4317415,Convergence of $\sum_{n=1}^{\infty}\frac{(\frac{2}{3}+\frac{1}{3}\cdot \sin(n))^n}{n}$,"My friend asked me this problem: Source : Problem 35 from a 2004 book by Borwein, Bailey, and Girgensohn [1] Determine whether the series $$\sum_{n=1}^{\infty}\frac{(\frac{2}{3}+\frac{1}{3}\cdot \sin(n))^n}{n}$$ converges. Firstly I want to use the root test. But later I found that it just didn't work because $2/3+1/3\cdot \sin(n)$ can't be smaller than any given constant that is smaller than 1. Then I have no other ways to deal with it. [1] Jonathan M. Borwein, David H. Bailey, and Roland Girgensohn. Experimentation in Mathematics: Computational Paths to Discovery. CRC Press, 2004.","['analysis', 'sequences-and-series']"
4317564,How do i find the value for the series $ \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)}$? [duplicate],"This question already has answers here : Find the sum of the series $\sum \frac{1}{n(n+1)(n+2)}$ (10 answers) $\sum_1^\infty{\frac{1}{n(n+1)(n+2)}}$? [duplicate] (1 answer) Closed 2 years ago . I have a general problem understand how to solve these kind of questions where you have a series and you need to find the value of it: $$  \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)} $$ I know that $  \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)} $ = $ \frac{a_1}{n}+\frac{a_2}{n+1}+\frac{a_3}{n+2} $ which can be rewritten as: $ n^2(a_1+a_2+a_3)+n^1(3a_1+2a_2+a_3)n^0(2a_1)=1$ Using a LGS i come up with the solutions: $ a_1 = \frac{1}{2}$ ; $ a_2 = -1 $ ; $ a_3 = \frac{1}{2} $ ; Okay, So i put that into my initial formula: $  \sum_{n=1}^{\infty} \frac{0.5}{n}+\frac{-1}{n+1}+\frac{0.5}{n+2} $ Nice, but here i need a clear guidance whats next as this step is confusing me. It would be really great so see a solution for this so that i can study it further. Thanks in advance everyone!","['calculus', 'analysis', 'sequences-and-series']"
4317590,6 Heads Followed by 6 Tails (Coin Flipping),"You throw a fair coin one million times. What is the expected number of strings of 6 heads followed by 6 tails? The answer given is: There are $1,000,000 - 11$ possible slots for the sequence to occur. In each of these slots, the probability is $2^{-12}$ . Due to linearity of expected value, the answer is therefore $(1,000,000 - 11)\times2^{-12}$ . I don't understand why this solution works. Shouldn't there be any consideration of the fact that at most only $\dfrac{1,000,000 - 11}{12} \approx 83332$ strings of 6 heads followed by 6 tails can occur. Can anyone please help me understand the solution?","['probability-distributions', 'combinatorics', 'recreational-mathematics', 'probability-theory', 'probability']"
4317597,Metrically Cauchy vs. Topologically Cauchy,"Let $X$ be a topological vector space. A family $x_i \in X, i\in I,$ is called a topologically Cauchy family if for every NH $U$ of zero, there exists $i_0\in I$ such that $$ i,j\geqslant i_0 \Rightarrow x_i-x_j \in U $$ If the topology on $X$ is induced by a metric $d$ , then the family is called metrically Cauchy if for every $\varepsilon >0$ , there exists $i_0\in I$ such that $$ i,j\geqslant i_0 \Rightarrow d(x_i,x_j)<\varepsilon. $$ Allegedly, neither implies the other in general, which I'm having a hard time understanding. The notions are equivalent if the metric is shift invariant, e.g a norm. Suppose $\tau _X$ is induced by a metric $d$ . Let $x_i\in X, i\in I,$ be metrically Cauchy. Let $B := B(0,\varepsilon)$ be a NH of zero. We can find $i_0\in I$ such that the metric condition is satisfied. But if the implication ""Metrically Cauchy $\Rightarrow$ Topologically Cauchy"" fails in general, that must mean $x_i-x_j\in B$ is not necessarily true for some $i,j\geqslant i_0$ . Conversely, if $x_i$ is topologically Cauchy and $\varepsilon >0$ , then $x_i-x_j\in B(0,\varepsilon)$ , but then again $d(x_i,x_j)< \varepsilon$ has to fail sometimes. What are some examples of non-shift-invariant metrics and families (sequences, even?) for which either implication fails? Is shift invariance necessary for the notions to be equivalent? Are there such topological vector spaces (whose topology is induced by a metric) such that exactly one implication is true?","['topological-vector-spaces', 'functional-analysis', 'examples-counterexamples', 'real-analysis']"
4317629,"A finite correspondence in $Cor_k(X,\mathbb A^1)$ is a principal Weil divisor","I am trying to understand Lemma 4.4 of ""Lectures on Motivic Cohomology"" by Mazza-Weibel-Voevodsky. In the following, $X$ is a smooth scheme over a field $k$ .
In the proof of the lemma, the authors say "" If $Z$ is an element of $Cor(X,\mathbb A^1)$ , then $\exists ! \ f$ , a rational function on $X\times \mathbb P^1$ and an integer $n$ such that $\operatorname{div}f=Z$ and $f/t^n=1$ on $X\times \{\infty\}$ ."" I am struggling to figure out why this is true. It suffices to show this for an elementary correspondence. So let $Z$ be an irreducible closed subset of $X\times \mathbb A^1$ which is finite and surjective over $X$ . I want to show that $Z=\operatorname {div}(f)$ for some rational function $f$ . If I just concentrate on $X=\operatorname{Spec}A$ an affine $k$ -domain, then $Z=V(p)$ where $A\rightarrow A[t]/p$ is injective and finite. But I am not quite sure how this implies $p$ is a rational Weil-divisor. Any help will be highly appreciated. EDIT: It seems to prove the lemma I only need to understand the group $Cor_k(X,\mathbb G_m)$ where $G_m=\mathbb A^1-0$ . So if $X=\operatorname{Spec}A$ , then an elementary finite correspondence in $Cor_k(X,\mathbb G_m)$ is a prime ideal $p\in\operatorname{Spec}A[t,t^{-1}]$ such that $A\hookrightarrow \frac{A[t,t^{-1}]}{p}$ is finite and injective. Now $p$ has a polynomial in $A[t]$ with leading coefficient unit as well as a polynomial in $A[t]$ with constant term a unit. Let $f_+$ be the monic polynomial in $p\cap A[t]$ of min degree and $f_-$ is the polynomial in $p\cap A[t]$ of min degree with constant term $1$ . Then it can be easily seen that the two polynomials have same degree $n$ . Not sure if this helps.","['divisors-algebraic-geometry', 'algebraic-geometry']"
4317671,Do the invariants & covariants completely characterize a projective variety up to projective equivalence?,"Let $V \subseteq \mathbb{CP}^n$ be a projective variety embedded in complex projective n-space. By the nullstellensatz it is the zero-set of a finite number of homogeneous polynomials, $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ , in variables $x_1,...x_{n+1}$ , which are coordinates in $\mathbb{CP}^n$ . Let $PGL(n)$ be the group of projective transformations of $\mathbb{CP}^n$ , whose action on $\mathbb{CP}^n$ is induced, as usual, by the action of $GL(n+1)$ on $\mathbb{C}^{n+1}$ . We say that two such varieties $V,U \subseteq \mathbb{CP}^n$ are projectively equivalent if one can be transformed to the other by a projective transformation (i.e. they are in the same orbit of the action of $PGL(n)$ on subsets of $\mathbb{CP}^n$ ). As explained for example here and here , given a finite set of homogeneous polynomials $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ , the algebra of invariants of these polynomials is the (graded) algebra of all those polynomials in the coefficients of the original polynomials, which are invariant, up to a multiplicative factor, under the action of $GL(n+1)$ on $\mathbb{C}^{n+1}$ . In addition, that constant factor has to be a power of the determinant of the transformation. In other words, the invariants are polynomial functions in the coefficients of the original polynomials, such that when one substitutes in the original polynomials a linear change of variables, the invariants stay the same, up to a factor which is a power of the determinant of the linear change of variables. Similarly (as explained in the references), one also has the algebra of covariants of a set of polynomials, which is defined similarly except that the covariants are polynomial functions in the coefficients of the original polynomials and in the variables themselves. According to Hilbert's Basis Theorem, both algebras are finitely generated. Now, invariants of a set of homogeneous polynomials are not enough to characterize the corresponding embedded projective variety up to projective equivalence. For example (as described in both references), for a binary cubic, i.e. a homogeneous polynomial of degree 3 in 2 variables, the algebra of invariants is generated by the discriminant, but the discriminant is 0 if the cubic has a double root and a single root, or if the cubic has a triple root. The first case corresponds to the cubic representing a variety in $\mathbb{CP}^{1}$ consisting of two distinct points, while the second case corresponds to the cubic representing a variety in $\mathbb{CP}^{1}$ consisting of a single point. These two are clearly not projectively equivalent. However, the above book by Olver also mentions that the invariants and covariants together are enough to characterize the corresponding sets of points up to projective equivalence. Specifically, a binary cubic has its hessian as a covariant; while one cannot speak of the value of the hessian of a binary cubic (since it is a function of $x_1,x_2$ as well as being a function of the coefficients), one can speak of the hessian being identically zero or not - and more generally, of two hessians being equal, up to a factor (by which I mean that one hessian is a multiple of the other by a number). Olver shows that the hessian vanishes identically iff the cubic has a triple root; thus the discriminant and hessian together completely characterize the set of zeros of a binary cubic up to projective equivalence - if the roots are distinct then both are nonzero; if there is a double root then the discriminant vanishes but the hessian does not; and if there is a triple root then both vanish. My question, roughly, is whether this idea generalizes to arbitrary embedded varieties. Are the algebras of invariants and covariants of the defining polynomials, together, enough to characterize the embedded variety up to projective equivalence? To state the question precisely, we need to first introduce absolute invariants - these are invariants that do not change at all under a projective transformation of the coordinates (i.e. even a multiplication by a power of the determinant is now not allowed), but they are allowed to be rational functions of the coefficients of the original polynomials - not only polynomials in them. The first kind of invariants that we discussed are now called relative invariants. It is easy to realize and see that the field of absolute invariants of a finite set of polynomials is generated by certain quotients of powers of the generators of their algebra of relative invariants (basically you take quotients of two generators, each raised to a power, such that the resulting quotient is unchanged by a projective transformation because the determinantal powers in the numerator and denominator cancel each other). Similarly one has absolute covariants and their field, with its generators. Now, say we are given two finite sets of homogeneous polynomials over $\mathbb{C}$ , $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ and $q_1(x_1,...x_{n+1}),...,q_k(x_1,...x_{n+1})$ , both having the same number of polynomials, and with the polynomials having the same degrees $d_1,...d_k$ . Take a generating set $b_1,...b_m$ for the field of absolute invariants of a set of $k$ polynomials of degrees $d_1,...d_k$ , and take a generating set $c_1,...c_l$ for the field of absolute covariants of a set of $k$ polynomials of degrees $d_1,...d_k$ . Suppose that every invariant in $b_1,...b_m$ has the same value for the polynomials in $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ and for those in $q_1(x_1,...x_{n+1}),...,q_k(x_1,...x_{n+1})$ (that is, when the coefficients of these polynomials are substituted in the rational functions $b_1,...b_m$ , the same complex numbers are obtained). Suppose also that every covariant in $c_1,...c_l$ is equal for the polynomials in $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ and for the polynomials in $q_1(x_1,...x_{n+1}),...,q_k(x_1,...x_{n+1})$ (equality here is not equality of complex numbers, but equality in a certain field of rational functions). Does it follow that the two embedded varieties defined by $p_1(x_1,...x_{n+1}),...,p_k(x_1,...x_{n+1})$ and $q_1(x_1,...x_{n+1}),...,q_k(x_1,...x_{n+1})$ are projectively equivalent? (technically one should also consider the case when some of the absolute invariants and covariants are undefined since the substitution causes some denominators of the rational functions which define them to be zero, but this is already complicated enough so let's just forget about this case for now.) Another related question that naturally follows from this is whether the invariants only are enough, if we restrict ourselves to irreducible varieties. In the above exmaple of a binary cubic, the covariant (the hessian) is only needed to distinguish between the two degenerate cases that result from the defining equation being reducible (a double root and a triple root). If we restrict ourselves to varieties defined by irreducible polynomials, are the values of the invariants in a generating set $b_1,...b_m$ for the field of absolute invariants enough to determine the variety up to projective equivalence? Further related questions are what happens when we look at varieties/polynomials above $\mathbb{R}$ instead of $\mathbb{C}$ ? Above other fields? And what happens when we look at equivalence relative to subgroups of $GL(n+1)$ instead of the full group (such as affine equivalence or euclidean equivalence) - since as is well known, such subgroups also have their corresponding algebras of invariants? I welcome any partial answer or direction to answering any of these questions. Resources on classical invariant theory are notoriously difficult to find, and I was somewhat surprised that this question is not addressed in any resource that I know (even though Olver's book above comes very close to discussing this question, but then backs away). It seems like a question that would have been pretty interesting to the projective geometers of the 19th century. For this reason, if anyone knows a nice reference that discusses the relation of classical invariant theory to projective geometry, I'd also like to know (Salmon's classic treatises don't seem to answer this question).","['projective-geometry', 'computational-algebra', 'algebraic-geometry', 'invariant-theory', 'commutative-algebra']"
4317688,Random walks on symmetric groups,"Let $S_n$ be the symmetric group on $n$ elements. Now, we pick a random transpositions to generate random walks on $S_n$ (also assume the probability of picking each transposition is equal of course). There are $n(n-1)/2$ transpositions. How many does is need on average (expectation) to get back to the identity $(1)$ ? Namely, what is the expectation step of a random walk on $S_n$ first hit the identity? I could compute the case for 2,3, and 4. The expectations are simply 2, 6, 24. So I guess it is $n!$ .","['symmetric-groups', 'combinatorics', 'random-walk', 'probability']"
4317715,Understanding numerator/denominator layout in matrix-calculus,"This is a distilled version of this question. Consider the following: $$
\begin{align}
z & = f(\mathbf{y}) \\
\mathbf{y} & = g(\mathbf{x}) \\
\text{where, } & z \in \mathbb{R} \text{, and} \\
& \mathbf{y}, \mathbf{x} \text{ are two $(1, m)$ dimensional vectors, i.e. row-vectors}
\end{align}
$$ Using numerator-layout , what is the dimension of the derivative $\frac{\mathrm{d}z}{\mathrm{d}\mathbf{y}}$ ? Should it be a column-vector of dimension $(m, 1)$ , because $\mathbf{y}$ is a row-vector of dimension $(1, m)$ ( Source ) But, using this notation causes issues while computing the derivative $\frac{\mathrm{d} z}{\mathrm{d} \mathbf{x}} = \frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}} \frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}}$ ; since, $\frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}}$ would be an $(m, m)$ matrix, while $\frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}}$ is an $(m, 1)$ vector. However, this notation does serve well when computing the derivatives of the form $\frac{\mathrm{d} h(\mathbf{X})}{\mathrm{d}\mathbf{X}}$ , where $\mathbf{X}$ is a matrix of dimension $(m, n)$ ; and $h(\mathbf{X})$ is a scalar-valued function. Or should it be a row-vector, because according to the numerator-layout the derivative has the dimensions --> $\text{numerator-dimension} \times (\text{denominator-dimension})^\intercal = (1,1)\times(m, 1)$ ( Source ) Also, (for this point) is my understanding even correct? PS: also, is there any definitive guide from which I can learn matrix-calculus from the first principals. Although, the following sources are good, they still leave a lot of gaps: Matrix-Calculus The Matrix-cookbook Old and New Matrix Algebra Useful for Statistics , by T. P. Minka The Matrix Calculus You Need For Deep Learning Matrix Differentiation Matrix Calculus The Guide for Matrix Calculus , by R.P. Pacelli.","['neural-networks', 'machine-learning', 'matrix-calculus', 'vector-analysis', 'derivatives']"
4317738,When can one use the Leibniz rule for integration?,"If I want to determine determine the following expression \begin{align*}
    \frac{\partial }{\partial t} \int_{a}^{b} f(x, t)\mathrm{d}x 
\end{align*} is it sufficient that $f_n\left(x, t+\frac{1}{n}\right)$ is uniformly converging to $f(x, t)$ in order to be allowed to apply the Leibniz rule, i.e. \begin{align*}
    \frac{\partial }{\partial t} \int_{a}^{b} f(x, t)\mathrm{d}x =
\int_{a}^{b}  \frac{\partial }{\partial t} f(x, t)\mathrm{d}x
\end{align*} given that $a, b$ are independent of $t$ ? If not, which conditions are sufficient for being able to use Leibniz' rule (I have not learned about the dominated convergence theorem yet)?","['integration', 'derivatives', 'partial-derivative', 'real-analysis']"
4317803,How can I extend a finitely additive measure?,Let $B$ be a boolean algebra. Suppose we have a finitely additive measure $\mu$ defines on a subalgebra $A\subseteq B$ . Is it possible to extend $\mu$ to a finitely additive measure $\nu$ on $B$ ? Is there a related theorem about extension of measures?,"['boolean-algebra', 'measure-theory']"
4317811,Two numbers with a given difference having the same number of divisors,"So, it is required to prove that for each natural $k$ there are two natural numbers with a difference $k$ having the same number of divisors. For example, for the case $k=27$ , the pair $(18,45)$ is suitable. One of the special cases of this problem can be solved quite easily, namely, if in the factorization of the number $k$ into factors there is an equal number of twos and triples (in particular, if there are none at all), then the numbers $2k$ and $3k$ will have the same number of divisors. Please help to solve this problem in the general case.","['divisibility', 'number-theory', 'elementary-number-theory', 'arithmetic-functions', 'arithmetic']"
4317891,"Exercise 1.5.xi from Emily Riehl's ""Category Theory in Context"" on properties of some functors","I've been going through Emily Riehl's textbook on categories, and struggle with the exercise 1.5.xi. Consider the functors $Ab \to Grp$ (inclusion), $Ring \to Ab$ (forgetting the multiplication), $(-)^\times: Ring \to Grp$ (taking the group of units), $Ring \to Rng$ (inclusion), $Fld \to Ring$ (inclusion) and $Mod_R \to Ab$ (forgetful). Determine which functors are full, which are faithful, and which are essentially surjective. Do any define an equivalence of categories? (Warning: A few of these questions conceal research-level problems,
but they can be fun to think about even if full solutions are hard to come by.) As mentioned, some of the problems are pretty hard, so I skipped some of them. For now, I've determined that properties only for $Ab \to Grp$ , $Ring \to Ab$ and $Fld \to Ring$ . This are my solutions: $Ab \to Grp$ : almost obviously, this functor is not essentially surjective, whileas it's fully faithful: if two groups are abelian, then any abelian homomorphism between them is just a group homomorphism, so it's faithful, and if two groups in $Grp$ are in fact abelian groups, then they have the same set of morphisms as objects in $Ab$ , so it's fully faithful, and really $Ab$ is a full subcategory of $Grp$ . $Ring \to Ab$ : this functor is not essentially surjective(because e.g. $\mathbb{Q}/\mathbb{Z}$ is not a ring); it's faithful, since different ring homomorphisms are going to be different group homomorphisms under functor action. But this functor is not full: consider rings $\mathbb{Z}/n\mathbb{Z}$ and $\mathbb{Z}$ . If $U: Ring \to Ab%$ , then consider $\varphi: U(\mathbb{Z}/n\mathbb{Z}) \to U(\mathbb{Z}): x \mapsto 0$ . There's no $\psi: \mathbb{Z}/n\mathbb{Z} \to \mathbb{Z}$ such that $U(\psi) = \varphi$ , so this functor is not full. $Fld \to Ring$ : in some sense same as for $Ab \to Grp$ : $Fld$ is full subcategory of $Ring$ and this functor is not obviously essentially surjective. I'd like to know whether I am correct. Seems like there was nothing hard, but I have doubts on my solutions. Also I'm interested in the properties of another functors. I was thinking about $Mod_R \to Ab$ : since it's forgetful, it's faithful. But is it full? Or essentially surjective? Obviously, not both(otherwise, this is an equivalence of categories), and the answer is somehow depends on the ring(because if $R = \mathbb{Z}$ , then this is the equivalence of categories). Probably I can show that such functor is essentially surjective, since I kinda can create module via tensor product $G \otimes_\mathbb{Z} R$ (which is kinda left-adjoint to $Mod_R \to Ab$ ), but I'm not sure in it. So, can anyone elaborate the situation with $Mod_R \to Ab$ and the other functors? Or maybe give a tip on it. Concretely, I have no clue on: $(-)^\times: Ring \to Grp$ (taking the group of units) $Ring \to Rng$ (inclusion) Thanks!","['functors', 'category-theory', 'abstract-algebra', 'solution-verification', 'adjoint-functors']"
4317911,"Poisson Process with $\lambda = 3$, suppose $6$ cars arrive after $2$ hours, what is the probability only $1$ arrived in the first hour?","Suppose the number of cars arriving at a gas station per hour follows a Poisson Process with $\lambda = 3$ , Suppose $6$ cars arrive after $2$ hours, what is the probability only $1$ arrived in the first hour? So I know that $P(X_t=k)=e^{\lambda t}\frac{(\lambda t)^k}{k!}$ represents the probability the process is at k at time t. So because the states are independent, I should just have the $P(X_1=1)P(X_1=5)$ ? Since the probability of $5$ cars arriving in the 2nd hour should be the same as the probability $5$ arrive in the first hour? Which gives me $P(X_1=1)P(X_1=5)=(3e^3)(\frac{3^5e^3}{5!})$ ? Or does independence mean knowing the total after $2$ hours doesn't matter? And I only need to compute $P(X_1=1)$","['statistics', 'poisson-process']"
4317937,Energy Distance between Multivariate Gaussian Distributions,"The square of energy Distance between CDFs $F$ and $G$ , of $X$ and $Y$ resp., is defined here as $$d^2(F, G) = E||X-Y|| - E||X-X'|| - E||Y-Y'||$$ where $(X, X')$ and $(Y, Y')$ are IID pairs. I am looking for an explicit form for multivariate Gaussians $F = N(0, C_F)$ and $G = N(0, C_G)$ , in terms of $C_F$ and $C_G$ I found something similar ( this paper's Theorem 2.2), which gives a result for the Wasserstein metric under some other distance function called the Riemannian metric(in the above formulation we used Euclidean distance). However, the paper's theory was too complicated for me to follow. Any help in this direction would be really appreciated. Thanks!","['riemannian-geometry', 'probability-distributions', 'optimal-transport', 'gaussian-measure', 'probability-theory']"
4317983,Choosing at least 2 women from 7 men and 4 women,"In how many different ways can we choose six people, including at least two women, from a group made up of seven men and four women? Attempt: As we have to have at least two women in the choices, then $\displaystyle\binom{4}{2}$ , leaving a total of $4$ out of $9$ people to be chosen. So $$\binom{9}{4} \cdot \binom{4}{2}=756$$ The answer is $371$ . Where is my error?",['combinatorics']
4318010,Are rings of power series over a local field complete?,"Let $K$ be a finite extension of $\mathbb{Q}_p$ , and let $D$ be some disk $D = \{ x\in \overline{K} \mid |x| < c < 1\}$ . Is the set of power series in $K[[T]]$ which converge on $D$ and are bounded on $D$ complete with respect to the supremum norm $\|f\|_\infty=\sup_{x\in \overline{K}, |x|<c} |f(x)|$ ?","['local-field', 'number-theory', 'p-adic-number-theory', 'analysis']"
4318023,Proving ${\mathbb{P}}^n$ is Hausdorff,"I am trying to understand and complete the proof that the real projective space ${\mathbb{P}}^n$ is Hausdorff.In my notes it is modeled as ${\mathbb{R}}^{n+1}\setminus \{0\}/\sim  $ and it goes like this: It is enough to construct, given two different points $[a]$ and $[b]$ a continuous function $f:{\mathbb{P}}^n \rightarrow {\mathbb{R}} $ such that $f[a] \neq f[b] $ (why ?.......(1)) We fix $\omega$ in ${\mathbb{R}}^{n+1}\setminus \{0\}$ and define $f[\nu]$ as the squared of the distance from $\omega$ to the vector line $R\nu$ generated by $\nu$ . Since $f\circ \pi(\nu) = f[\nu] = |\omega|^2-(\omega . \nu)^2/|\nu|^2$ , it follows that $f \circ \pi $ is continuous and hence $f$ is continuous...(why?........(2)) ( $\pi$ is the canonical projection $\pi : {\mathbb{R}}^{n+1}\setminus \{0\} \rightarrow {\mathbb{P}}^{n} $ ) It is then enough to take $w \sim a$ to have $f[a] = 0 \neq f[b]$ ...(*) I have two questions: 1)why is (1) true ? At first I thought I could justify (1) like this: Since ${\mathbb{R}}$ is Hausdorff, $\exists $ open sets $A,B$ such that for $f[a] \neq f[b] $ , $f[a] \in A$ and $f[b] \in B$ , with $A \cap B = \emptyset$ . By hypothesis then $[a] \neq [b] $ and since $f$ is continuous, $f^{-1}(A) $ and $f^{-1}(B) $ are open sets of ${\mathbb{P}}^n$ containing $[a]$ and $[b]$ respectively such that $f^{-1}(A) \cap f^{-1}(B) = f^{-1}(A\cap B)=f^{-1}(\emptyset)=\emptyset $ . Then ${\mathbb{P}}^n$ is Hausdorff But I don't think my proof is correct since  for it to work we need $f$ to have the property stated at (1) but the function $f$ that they propose later has that property only when taking one of the points fixed, say $a$ as $a \sim w$ as done in (*), and not for any two points as needed for the definition of Hausdorff space. Besides if w is the center of a circle (in the 2-dimensional case for instance there are two lines that have the same distance to w, that is the tangent lines, so (1) does not hold for any two different $[a] $ and $[b]$ ) 2)why $f \circ \pi $ continuous implies $f$ is continuous? don't think  I can compose with a continuous $\pi^{-1}$ , since that function is not well defined since $\pi$ is not injective","['quotient-spaces', 'general-topology', 'projective-space', 'differential-geometry']"
4318060,"Does this ""factory of binary sequences"" group have a standard name?","I was thinking about ways to associate binary sequences with elements of a group. I came up with a group, and I was wondering if it has a standard name, or if it can be described easily in terms of well-studied objects. (The construction seems interesting to me, but I would rather not reinvent the wheel!) The construction is as follows. Let $T$ be the set of all binary sequences, so any element $t \in T$ is a map $t: \mathbb{Z} \to \{0,1\}$ . Define a map $c: T \to T$ (which I think of as ""cycle"") that changes the zeroth element of a sequence from 0 to 1, or from 1 to 0. That is, for a sequence $t \in T$ , we have: $$
[c(t)](0)= 
\begin{cases}
0 & \textrm{if } t(0) = 1\\
1 & \textrm{if } t(0) = 0
\end{cases} \\
\textrm{and }[c(t)](n) = t(n) \textrm{ for any } n \neq 0.
$$ Next, define a map $s: T \to T$ (which I think of as ""shift"") that shifts all the elements of a sequence to the right, so: $$
[s(t)](n) = t(n-1), \textrm{ for any } n \in \mathbb{Z}.
$$ My reason for considering these maps is that, by using them together, it seems one can construct binary sequences (say, by setting the zero sequence as a starting point and then specifying a list of instructions like ""do cycle, then shift, then cycle""). One then can ""compose"" two binary sequences by concatenating the lists of operations used to generate them. This gives a way of combining binary sequences different from element-wise operations. Notice that these maps $c$ and $s$ are invertible. We have $c = c^{-1}$ , as switching the zeroth element twice of any sequence leaves the sequence unchanged. The inverse of $s$ acts by shifting all elements of an input sequence to the left.
Define the ""binary sequence factory group"", $F$ , as the group generated by these operations $c$ and $s$ , with group element composition given by function composition. This group has an infinite number of elements, corresponding to the fact that there are an infinite number of binary sequences. In addition, this group is not commutative (for example, $ccs$ is different from $csc$ ). However, it is generated by two pretty simple operations. Does this group $F$ have a standard name? Or can it be constructed in some standard way from well-known groups? Can this group be simply described in terms of some presentation?","['finitely-generated', 'group-theory', 'reference-request', 'sequences-and-series']"
4318082,Is this a rigorous derivation? What justifies this step? $\lim_{h\to 0} \frac{e^h -1}{h}$,"I'm trying to find the easiest way to prove that $\lim_{h\to 0} \frac{e^h-1}{h}=1$ . I found the following derivation, which seems the most straightforward: $$\lim_{h\to 0} \frac{e^h-1}{h} \overbrace{=}^* \lim_{h\to 0} \frac{((1+h)^\frac{1}{h})^h-1}{h}=\lim_{h\to 0}\frac{1+h-1}{h}=1$$ The step I'm curious about is the first one. I realize that $e=\lim_{h\to 0} (1+h)^\frac{1}{h}$ , but what is it that fully justifies substituting this limit inside the outer limit? If I remember correctly sometimes making such substitutions inside limits can lead to false results? Naively I would say that all I can say for sure is that $$\lim_{h\to 0} \frac{e^h-1}{h}=\lim_{h\to 0} \frac{(\lim_{k\to 0} (1+k)^\frac{1}{k})^h-1}{h}$$ Am I missing something trivial? [Added: I define $e = \lim_{n\to\infty} (1+\frac{1}{n})^n$ and then $e^x$ is defined per the usual way of exponentiation of real numbers. Therefore the above limit is a stepping stone in proving that $(e^x)'= e^x$ and so we can't use this derivative (or its consequences, such as the Taylor series of $e^x$ ) in order to prove this limit, so that we avoid a cyclic argument.]","['limits', 'calculus', 'substitution']"
