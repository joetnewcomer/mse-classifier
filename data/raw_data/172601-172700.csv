question_id,title,body,tags
3066218,Is restriction of a function necessarily limited to a subset of the domain?,"A pretty basic question but I cannot find it either here or on the net. The usual definition of restriction of a function $f : X \to Y$ , to a set $X'$ (denoted by $f{\restriction_{X'}}$ ) assumes that $X' \subset X$ , i.e. the restricted domain is a subset of the original domain . See for example the Wikipedia definition of restriction . However, in certain contexts it may be simpler and very natural to extend this concept to any set $X'$ , simply by considering $X \cap X'$ ; thus writing $f{\restriction_{X'}}$ directly instead of $f{\restriction_{X' \cap X}}$ , because: the intent is clear even if $X'$ is not a subset of the domain; it saves some characters; if the domain of the function has no established name, than this operation requires even more characters, (something like $f{\restriction_{X'  \cap {\operatorname {dom} f} }}$ ); if the function is constructed as an expression (like $g\circ h$ ), then this becomes even more complex and requires unnecessary repetition $(g \circ h){\restriction_{X' \cap \operatorname {dom}(g \circ h) }}$ . Naturally, if such expressions become abundant, it is possible to add a short remark on the extended usage. However, I am still curious whether such extension would be considered (even a minor) abuse of notation without any remark? there is some deeper reason why the ""usual"" definition assumes that the restriction is limited to subsets of the domain? This constraint seems completely unnecessary. Note: Despite using the ""usual"" definition, even the Wikipedia article hints at the possibility of the ""extended"" one: Informally, the restriction of $f$ to $A$ is the same function as $f$ , but is only defined on $\displaystyle A\cap \operatorname {dom} f$ . because if we had really assumed $A \subset \operatorname {dom} f$ then it would have been enough to write ""... but is only defined on $A$ "".","['elementary-set-theory', 'functions', 'notation']"
3066230,"$H$ Hilbert space, $T$ symmetric bounded linear, when is $H=R(T) \oplus N(T)$?","I just saw in an exercise that if I have a prehilbert space $H$ and $T$ a linear, bound and symmetric operator then $R(T)=N(T)^{\perp}$ . Now I was asking myself whether $H=R(T) \oplus N(T)$ .
On wiki I saw that if $X$ is closed in $H$ then $H=X \oplus X^{\perp}$ .
So the question is, is $R(T)$ or $N(T)$ closed in general? Is $N(T)$ closed if it is separable?","['operator-theory', 'orthogonality', 'direct-sum', 'hilbert-spaces', 'functional-analysis']"
3066253,solving logarithmic expression:$x= \frac{(\log_2(1+2x)) \cdot (1+2x)}{4}-5 $,I need to solve logarithmic equation that contains a variable inside and outside the logarithm. I want to solve the following equation for x: $$x= \frac{(\log_2(1+2x)) \cdot (1+2x)}{4}-5 $$ Note1: The base of logarithm is $2$ . Note2: The equation is supposed to be closed form. Note3: The constant values are not important and can be changed.,"['calculus', 'functions', 'algebra-precalculus', 'logarithms']"
3066268,Does this sequence of product of two primes always exist,"Consider a sequence $A: a_1, a_2, a_3 .. a_n$ such that each element of this sequence is product of two primes from $p_1, p_2 .. p_m$ so that: Each $a_i$ is distinct. $m$ is the least integer so that $\frac{m(m-1)}{2} \ge n$ Any three consecutive elements of sequence are coprime, but any two are not. For example, for $n = 3$ , we have $m = 3$ and so taking the 3 primes as $a,b,c$ , we can say the sequence $A$ is $ab, bc, ac$ For $n = 4$ , $m = 4$ and $A: ab, bc, cd, da$ For $n = 6$ , $m = 4$ and $A: ab, bd, da, ac, cd, db$ The question is to show that such a sequence exists for any $n$ . If this sequence exists for all $n$ , is there a simple pattern for $A$ ?\ Edit There was an error in example 3, no such sequence exists for $n=6$ .","['prime-numbers', 'algebra-precalculus', 'discrete-mathematics', 'sequences-and-series']"
3066273,Is there a neat way to find the sign of a real function with radicals like this?,"This simple function: $$f(x) = \sqrt{x+2} - 2\sqrt{x+1} + \sqrt{x}$$ is negative for $x>0$ (just checked out with a graphic calculator). But how to ""prove"" this algebraically? The function comes out from the problem of finding which of these numbers $\sqrt{12} - \sqrt{11}$ and $\sqrt{11} - \sqrt{10}$ is bigger. [BTW, given that $f(x)$ is negative, it should be that $\sqrt{11} - \sqrt{10}$ is bigger than $\sqrt{12} - \sqrt{11}$ !] Thanks!","['functions', 'real-analysis']"
3066311,"Does an element of a set, that can't be in a list, make that set uncountable?","We can map the infinite subsets of $\mathbb N$ to the finite subsets of $\mathbb N$ The finite subset will be a prefix, of the infinite subset (that it is paired with), that has not yet been used. $1 \mapsto \{ \color{red}{1} ,2,3,4,5,6,7,8 \dots \} \mapsto \{ \color{red}{1} \}$ $2 \mapsto \{ \color{red}{2} ,4,6,8,10,12,14 \dots \} \mapsto \{ \color{red}{2} \}$ $3 \mapsto \{ \color{red}{1,3} ,5,7,9,11,13,15 \dots \} \mapsto \{ \color{red}{1,3} \}$ $4 \mapsto \{ \color{red}{1,2} ,3,7,9,19,27,31 \dots \} \mapsto \{ \color{red}{1,2} \}$ $5 \mapsto \{ \color{red}{1,2,3} ,4,21,22,25,32 \dots \} \mapsto \{ \color{red}{1,2,3} \}$ $6 \mapsto \{ \color{red}{2,3} ,4,6,7,8,21,55,58 \dots \} \mapsto \{ \color{red}{2,3} \}$ $7 \mapsto \{ \color{red}{2,3,4} ,6,7,8,9,21,55,58 \dots \} \mapsto \{ \color{red}{2,3,4} \}$ $8 \mapsto \{ \color{red}{2,3,4,6} ,7,9,21,55,58 \dots \} \mapsto \{ \color{red}{2,3,4,6} \}$ $9 \mapsto \{ \color{red}{2,3,4,6,7} ,6,7,8,21,55,58 \dots \} \mapsto \{ \color{red}{2,3,4,6,7} \}$ $\dots$ We can then find an infinite set that is not in this list (by diagonalization). $N \mapsto \{ \color{red}{4,5,8,9,\dots}   \} \mapsto \{ \color{red}{4} \} \lor \{ \color{red}{4,5} \} \lor \{ \color{red}{4,5,8} \} \lor\{ \color{red}{4,5,8,9} \} \dots$ We can then pair our infinite set, that can't be in our list, with $\{ \color{red}{4} \}$ if it is not been used or $\{ \color{red}{4,5} \}$ or $\{ \color{red}{4,5,8} \}$ etc... until we find some finite set that has not been used. Since we need sets with a finite number of elements, and we have sets with an infinite number of elements to choose from, we will always find a finite prefix from each infinite set. So, if we can find a finite set for every infinite set IN or NOT IN our list, and our set of finite sets is countable then can we still say that our set of infinite sets is uncountable based on there being an element that can't be in a list?",['elementary-set-theory']
3066313,Prove that $\sqrt{3\pm\sqrt{7}} \not\in \mathbb{Q}(\sqrt{3\mp\sqrt{7}})$.,"I'm currently solving a fairly long exercise related to Galois theory in which I've come across having to prove that $\sqrt{3+\sqrt{7}} \not\in \mathbb{Q}(\sqrt{3-\sqrt{7}})$ and $\sqrt{3-\sqrt{7}} \not\in \mathbb{Q}(\sqrt{3+\sqrt{7}})$ . So far I haven't been able to find an ""easy"" or simple and understandable way to do so given that this isn't the main part of the problem. Any help is appreciated!","['galois-theory', 'algebraic-number-theory', 'abstract-algebra']"
3066327,i know application but want proof of following definite integration property .,"Question: prove (a) If $f(x)=f(-x)$ and $f(x+\pi)=-f(x)$ then, $$\int_{0_{+}}^{\infty} f(x)\dfrac{\sin x}{x}dx=\int_{0_{+}}^{\dfrac{\pi}{2}}f(x) \cos x\  dx$$ Then, use this identity in proving result (b): $$\int_{0_{+}}^{\infty}\dfrac{\tan x}{x}dx=\dfrac{\pi}{2}$$ My attempt : I easily solved part (b) as follows $$\int_{0_{+}}^{\infty}\dfrac{\tan x}{x}dx=\int_{0_{+}}^{\infty}\sec x\ \dfrac{\sin x}{x}dx$$ Now using identity (a) because $\sec(-x)=\ \sec x$ and $\sec(x+\pi)=-\sec x$ we get: $$\int_{0_{+}}^{\infty}\sec x\ \dfrac{\sin x}{x}dx=\int_{0_{+}}^{\pi/2}\sec x\cos x\  dx=\dfrac{\pi}{2}$$ But I don't know how to prove identity (a) and how can I use identity to evaluate some famous definite integrals.","['integration', 'contest-math', 'improper-integrals', 'definite-integrals', 'real-analysis']"
3066330,"Choosing the vertices of a regular hexagon, how many ways are there to form four triangles such that any two triangles share exactly one vertex?","In other words, how many ways are there to choose  four $3$ -combinations of a $6$ -set, such that each two combinations contain exactly one common element, modulo dihedral group $D_6$ ? For example, $\{1,2,3\}, \{1,4,5\},\{2,4,6\},  \{3,5,6\}$ and $\{2,3,4\}, \{2,5,6\},\{3,5,1\},  \{4,6,1\}$ belong to the same class, but $\{1,2,3\}, \{1,4,5\},\{2,5,6\},  \{3,4,6\}$ does not. Context: I am considering partitions of a $4$ -set and partially ordering them by refinement. The result is a lattice which I am trying to visualize by a $3$ D graph. The most interesting part of the graph involves a square and a regular hexagon. For example: $\{1,2,3\}, \{1,4,5\},\{2,4,6\},  \{3,5,6\}$ leads to this visualization. Edit: I have categorized all $30$ possibilities into $5$ orbits: (three edges of the hexagon missing, $4$ elements) $124.135.256.346, 125.134.246.356, 135.146.236.245, 136.145.235.246.$ (two edges, one long diagonal missing, $6$ elements) $123.145.246.356, 124.135.236.456, 125.136.246.345, 126.135.245.346, 134.156.235.246, 135.146.234.256.$ (one edge, two short diagonals missing, $12$ elements) $123.145.256.346, 123.146.245.356, 124.136.235.456, 124.136.256.345, 124.156.235.346, 125.134.236.456, 125.146.234.356, 125.146.236.345, 126.134.245.356, 126.145.235.346, 134.156.236.245, 136.145.234.256.$ (two short and one long diagonal missing, $6$ elements) $123.146.256.345, 123.156.245.346, 124.156.236.345, 125.136.234.456, 126.134.235.456, 126.145.234.356.$ (three long diagonals missing, $2$ elements) $123.156.246.345, 126.135.234.456.$","['graph-theory', 'combinatorics']"
3066342,Smooth vector fields form a $C^\infty (M)$-module,"I am watching these lectures and am confused at this point about the counterexample to the module of smooth vector fields having a basis. The speaker invokes the Hairy Ball Theorem to say that every vector field on $S^2$ must vanish, but it is not clear to me why that prevents any vector field from being a part of a basis.","['algebraic-topology', 'differential-geometry']"
3066343,Example where The Lebesgue Integral is Better,"What is an example that involves a fuction on an interval of the real numbers where the Lebesgue integral is better than the Riemann integral. By better , it probably means that the Lebesgue intregral is defined while the Riemann integral is not. By an example , I mean an example that has importance in mathematics for reasons other than just showing that the Lebesgue integral is more general than the Riemann integral. If the importance of the example is not clear, then please explain why it is important. A possible answer could be an example where the use of the dominated convergence theorem plays a role.","['integration', 'riemann-integration', 'measure-theory', 'lebesgue-integral']"
3066407,Taylor expansion of imaginary part?-Doable or not?,"I have a number $z = a+re^{i(\pi-\varepsilon)}$ and $\varepsilon>0$ is small, $a,r>0.$ You can assume furthermore that $r\le a+2.$ I then define the expressions $$z_{\pm}:=\frac{1}{2} \left(z\pm \sqrt{z^2-4} \right).$$ The question is: Can one find a Taylor expansion of the imaginary part of $z_{\pm}$ in terms of $\varepsilon$ . I would like to know at least what the leading order terms are for $\varepsilon$ small. Let me finish with a quote of encouragement: Mark Twain — 'They did not know it was impossible so they did it'","['complex-analysis', 'calculus', 'functional-analysis', 'real-analysis']"
3066478,Definition of smooth functions on arbitrary subsets of $\mathbb{R}^n$ and partial derivatives,"Let $A$ be an arbitrary subset of $\mathbb{R}^n$ , and let $f:A\to \mathbb{R}$ be a function. We say that $f$ is smooth if for each point $p$ in $A$ there exists an open subset $U$ of $\mathbb{R}^n$ containing $p$ and a smooth map $g:U\to \mathbb{R}$ such that $f$ and $g$ agrees on $A \cap U$ . Now let $U$ be an open subset of $\mathbb{H}^n$ , with $\mathbb{H}^n=\{x\in \mathbb{R}^n:x^n\ge0 \}$ Let $f:U\to \mathbb{R}$ be a smooth function in the above sense.
Let $p\in U \cap \partial\mathbb{H}^n$ . Thus there are $\tilde U$ open subset of $\mathbb{R}^n$ and a smooth map $g:\tilde U\to \mathbb{R}$ such that $g$ and $f$ agrees on $U \cap \tilde U$ . I want to show that the partial derivatives of $f$ at $p$ are determined by their values in Int $\mathbb{H}^n$ , and therefore in particular are indipendent of the choice of exstension i.e. of the choice of $g$ . Here is my argument. $$\lim_{t\to0^+}\frac{f(p+te_i)-f(p)}{t}=\lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}\qquad [1]$$ beacuse $f(p)=g(p)$ and for sufficient positive small $t$ we have $p+te_i \in U \cap \tilde U$ and so $f(p+te_i)=g(p+te_i)$ . Now, since $g$ is smooth at $p$ by hypothesis, we have that the right hand side of the above equation exists, and we have $$\lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}=\lim_{t\to0^-}\frac{g(p+te_i)-g(p)}{t}=:\partial_i|_p g$$ So I can define $\partial_i|_p f:=\partial_i|_p g$ and this value is indipendent of the choice of $g$ by $[1]$ . So I have shown the claim in the above yellow box. Is my argument right? In John M. Lee Textbook Introduction to smooth manifolds, on page 27, he says that By continuity , all partial derivatives of $f$ at points of $U\cap \partial\mathbb{H}^n$ are determined by their values in Int $\mathbb{H}^n$ , and therefore in particular are indipendent of the choice of exstension. But it seems I dont'use the continuity of any function, so I'm not convinced of having understood the matter.","['multivariable-calculus', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3066514,Matrix quadratic form expansion question,"I'm trying to do a question and within it, I need to expand a matrix quadratic form: $\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x})$ In my working out, I think that the following is correct: $$
\begin{align}
\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) & = \frac{1}{2}(\vec{y}^{T} - \vec{x}^{T}) \Sigma (\vec{y} - \vec{x}) \\
& = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \frac{1}{2} \vec{y}^{T}\Sigma\vec{x} - \frac{1}{2} \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x}
\end{align}
$$ However, in the answers, it says that the answer is $\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \vec{y}^{T}\Sigma\vec{x} - \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x}$ so the middle two cross product terms do not have a half multiplied to them. Can anyone explain this? Or are the answers wrong? Thanks in advance!","['matrices', 'matrix-equations']"
3066520,Compactness of a specific set in weak topology,"I have the following question: Let $E$ be a polish space (that is, a topological space, which is separable and metrizable, such that $E$ would be complete if equipped with this metric). Consider the space of probability measures on $E$ , denoted by $\mathcal M (E)$ equipped with the topology of weak convergence. The claim I am struggling with is the following: For every $\ell\in\Bbb N$ let $K_\ell$ be a compact set of $E$ . Then the set $$\bigcap_{\ell\in\Bbb N} \{ m \in \mathcal M (E): m(K^{\operatorname{c}}_\ell) \leq \frac 1 \ell \}$$ is compact.","['measure-theory', 'weak-convergence', 'topological-vector-spaces', 'functional-analysis', 'general-topology']"
3066546,How to show that $\dim\ker(AB) \le \dim \ker A + \dim \ker B $?,"I want to show that $$ \dim \ker(AB) \le \dim \ker A + \dim \ker B. $$ My problem I thought that I can do that in this way: Let consider $x \in\ker B$ $$Bx  = 0$$ Let multiply this from left side by $A$ and we get: $$ABx = 0$$ so $$\ker B \subset\ker AB $$ so $$\dim \ker(B) \le \dim\ker AB$$ We can do the same thing with $\ker A$ let consider $ \vec{y} \in \operatorname{im}(AB) $ so $$ y = (AB)x $$ what is equivalent to $$ \vec{y} = A(B\vec{x}) = A\vec{w} $$ So $$ \vec{y} \in \operatorname{im}(AB) \rightarrow \vec{y} \in \operatorname{im}(A)$$ so $$ \operatorname{rank} AB \le  \operatorname{rank} A \leftrightarrow  \dim \ker A \le \dim \ker AB $$ But I am not sure what I should do later... edited I have seen this post $A, B$ are linear map and dim$null(A) = 3$, dim$null(B) = 5$ what about dim$null(AB)$ but I haven't got nothing like $\operatorname{im}(A|_{\operatorname{im}(B)})$ on my algebra lecture and I can't use that so I search for another proof (or similar without this trick)","['linear-algebra', 'vector-spaces', 'linear-transformations', 'function-and-relation-composition']"
3066572,Motives and representations,"Assuming standard conjectures, the category of motives is equivalent to the category of representations of a certain group over $\mathbb Q$ , but I don't understand the abstract construction. Now, if we start with a nonsingular projective variety $X$ we get a motive $(X, \Delta_X, 0)$ , so we should also get a representation $\rho$ . I think it would help me to understand concretely what representation arises this way. As far as I understood, the space of the representation should be $\bigoplus_{i=0}^{2 \dim(X)} H^i(X)$ , and it should probably factor through some simpler group. I would be grateful to anyone explainig me how to explicitely build/ describe $\rho$ from $X$ .","['algebraic-geometry', 'representation-theory', 'category-theory']"
3066603,How orbits of $G$-sets and these characters are related,"I've been learning about induced representations recently and I've come across something which I'm very confused about; For any $G$ -set $X$ , the number of orbits is equal to $(1_G, \chi_{\mathbb{C}[X]})$ where $1_G$ denotes the character of the trivial representation and $\chi_{\mathbb{C}[X]}$ denotes the character of the permutation representation defined below; $\mathbb{C}[X]$ is the natural permutation representation with basis ${\{e_x : x \in X}\}$ and the action of $g$ being $ge_{x} = e_{gx}$ . I have no idea why this is true. I know that $Ind_{H}^{G} \mathbb{C} \simeq \mathbb{C}[G/H]$ and the explanation I've seen goes like this; If $X$ is transitive, then $(1_G, \chi_{\mathbb{C}[X]}) = 1$ , which can be shown through Frobenius Reciprocity. If $X$ isn't transitive, then $X = \bigcup_{i=1}^{n} X_i$ where the $X_i$ are transitive. Then, $(1_G, \chi_{\mathbb{C}[X]}) = (1_G, \chi_{\mathbb{C}[X_1] \oplus \cdots \oplus \mathbb{C}[X_n]})= (1_G, \chi_{\mathbb{C}[X_1]}) + ... + (1_G, \chi_{\mathbb{C}[X_n]}) = n$ . I mean, I understand every line of the proof and can follow it, but why does the value $(1_G, \chi_{\mathbb{C}[X]})$ give you the number of orbits of a $G$ -set? What does this value have to do with orbits of $G$ -sets?","['representation-theory', 'group-rings', 'abstract-algebra', 'group-theory', 'group-actions']"
3066631,An identity about Killing vector field,"I am studying Kähler geometry and I started reading these lecture notes and this is the first exercise (p.2). If $X$ is a Killing vector field on a Riemannian manifold and $Y,Z$ be two vector fields, then \begin{equation}
	\nabla^2X(Y,Z)+R(X,Y)Z=0
\end{equation} I thought by writing everything in local coordinates would work but I really want some kind of neat proof. Can someone give some suggestion? Thank you in advance.","['curvature', 'riemannian-geometry', 'differential-geometry']"
3066648,"Compute $\mathbb{E}\big[\exp(XY+X)\big]$ where $X, Y$ are independent uniformly distributed over $[0,1]$ r.v's.","Compute $\mathbb{E}\big[\exp(XY+X)\big]$ where $X, Y$ are independent uniformly distributed over $[0,1]$ r.v's. I first computed $\mathbb{E}\big[\exp(XY+X)\mid X\big]$ . Because $X$ and $Y$ are independent then $$\mathbb{E}\big[\exp(XY+X)\mid X\big] = \phi(X)\,,$$ where $$\phi(x) =\mathbb{E}\big[\exp(xY+x)\big] = \int_0^1 \exp(xy+x)dy = \frac{\exp x(\exp x - 1)}{x}.$$ so $$\mathbb{E}\big[\exp(XY+X)\big] =  \int_0^1 \frac{\exp x(\exp x - 1)}{x} dx\,,$$ which I don't know how to compute. Could someone check if I'm good till now and maybe help me continue? Thanks!","['integration', 'conditional-expectation', 'expected-value', 'probability-theory', 'random-variables']"
3066684,Is the following matrix defined by the roots of Chebyshev polynomial invertible?,"Let $x_0, \dots , x_n$ be the roots of the Chebyshev polynomial $T_{n+1}(x)$ . We define: $$A=\begin{pmatrix}
 \frac{1}{\sqrt2}T_0(x_0) & \cdots & \frac{1}{\sqrt2}T_0(x_n) \\
 T_1(x_0) & \cdots & T_1(x_n) \\
 \vdots  & \vdots& \vdots \\
 T_n(x_0) & \cdots & T_n(x_n) \\
 \end{pmatrix}$$ Is $A$ invertible? If so, calculate $A^{-1}$ . I have tried to solve it for the roots of $T_2(x)$ and I found that the matrix is invertible. How can I generalize this?","['matrices', 'orthogonal-polynomials', 'linear-algebra', 'chebyshev-polynomials']"
3066697,What does an entire function whose images are normal matrices look like?,"I'd like to ask about the properties of a matrix-valued function $A:\mathbb C\to M_n(\mathbb C)$ where each entry of $A$ is an entire function and $A(z)$ is invertible and normal for every $z\in\mathbb C$ . I have difficultly to even come up with an interesting example of such a matrix-valued function. If $A$ is always Hermitian or always unitary, $A$ must be constant. Every non-constant example that I could find turned out to be uninteresting: it could be written in the form of $$
A(z)=UD(z)U^\ast,\tag{1}
$$ where $U$ is some constant unitary matrix and $D(z)$ is some diagonal matrix function. This includes the cases where $A=f(z)M$ or $A=e^{f(z)K}$ , in which $f$ is an entire scalar function, $M$ is a constant normal matrix and $K$ is a constant skew-Hermitian matrix. Must $A$ be in the form of $(1)$ ? If not, is there any complete characterisation of $A$ ? Answers with the invertibility requirement removed are also welcomed.","['matrices', 'complex-analysis', 'linear-algebra']"
3066739,find adjoint operator of an operator A,"How to find adjoint operator of an operator A $$A \in B(C^1[0,1], C[0,1])$$ $$ (Ax)(t) = x'(t)?$$ In answer : for any functional $f_y$ originated by function $y \in BV_0[0,1]:A(f'_y) = g_z$ , where functional $g_z$ originated by couple of function $z(t) = y(t)$ and number zero. Have no idea how to find. Can you help me with this?",['functional-analysis']
3066746,"average length of ""harmonic walk"" mod $n$ is $\ln(n)$ -- why?","Let a harmonic walk be a stochastic process where at turn $t \ge 2$ the probability of ending the walk is $\frac{1}{t}$ and the first turn never ends the walk. A harmonic walk can be viewed as a distribution over $\mathbb{N}_{\ge 1}$ without finite mean. Empirically, it seems to be the case that the average length of a harmonic walk mod $n$ is $\ln\left(n\right)$ . I'm wondering why this should be the case. The definition of the harmonic series (1) looks similar to the integral definition of the natural log, $\ln(x) \stackrel{\text{def}}{=} \int_{1}^{x} 1/s \;\text{ds} $ , so there might be reason to suspect that a relationship might exist, but that resemblance isn't remotely convincing. The harmonic series $H$ is given below (1) with the symbol "" $H$ "" referring to the formal sum, not the value. $$ H \stackrel{\text{def}}{=} \sum_{k=1}^{\infty} \frac{1}{k} \;\;\;\;\;\;\;\;\text{and $H$ diverges} \tag{1} $$ We can write an equivalent formal sum of partial products $H'$ (2a). I don't know the exact term for the relationship between $H$ and $H'$ as formal expressions, but $\frac{1}{k}$ is finite and definitely equal to $\prod_{l=2}^{k} \frac{l-1}{l} $ . $$ H' \stackrel{\text{def}}{=} \sum_{k=1}^{\infty} \prod_{l=2}^{k} \frac{l-1}{l} \tag{2a} $$ $$ H' = 1 + \left(\frac{1}{2}\right) + \left(\frac{1}{2} \times \frac{2}{3} \right) \dots \tag{2b} $$ Squinting at the definition of $H'$ , we can come up with a stochastic process $\Psi$ . At every turn in $\Psi$ , we either stop immediately or add 1 to $t$ . If $t = 1$ , then advance to the next turn with probability $1$ . If $t \ne 1$ , then stop with probability $\frac{1}{t}$ and continue to the next turn with probability $\frac{t-1}{t}$ . If we take the output of $\Psi \;\;\text{mod}\;\; n$ for various values of $n$ , it seems to match $\ln\left(n\right)$ . n        Ψ mod n     ln(n)
2          0.693     0.693
3          1.099     1.099
4          1.386     1.386
5          1.609     1.609
6          1.793     1.792
7          1.947     1.946 Why should this be the case? Here is the python code used to produce samples: import random
import math
import os

STOP = ""STOP""
GO   = ""GO""

def step(n):
    assert (n > 1)
    if random.randint(1, n) == 1:
        return STOP
    else:
        return GO


def walk_length():
    n = 2
    len_ = 1
    while GO == step(n):
        len_ += 1
        n += 1
    return len_


for x in range(1000000):
    print(walk_length() % int(os.getenv(""MODULUS""))) and the summary script from here : #! /bin/sh

Rscript -e 'summary (as.numeric (readLines (""stdin"")));' A sample run. > env MODULUS=7 python process_steps.py | summary 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   1.000   1.000   1.947   3.000   6.000","['probability', 'sequences-and-series']"
3066755,Dual of $\mathbb{Q}$[x] is not isomorphic to $\mathbb{Q}$[x],Denote by $\mathbb{Q}$ the set of the rational numbers. Denote by $\mathbb{Q}[x]$ the vector space over $\mathbb{Q}$ of the polynomials with rational coefficients. Denote by $(\mathbb{Q}[x] )^{\star}$ the dual of $\mathbb{Q}$ [x] . I am trying to show that $(\mathbb{Q}[x] )^{\star}$ and $\mathbb{Q}[x]  $ are not isomorphic (that is: does not exist a linear transformation which is bijective). I really don't know how to start. Someone could help me ? Thanks in advance,['linear-algebra']
3066793,Dominating sets in tournaments; is $2^{n+1}-2$ tight?,"A tournement is a directed graph such that for every pair of distinct vertices $\{x,y\}$ , there is either an edge from $x$ to $y$ or from $y$ to $x$ , but not both. I will use "" $x\to y$ "" to mean ""there is an edge from $x$ to $y$ ."" A dominating set of a directed graph is a subset $S$ of vertices such that for every $t\notin S$ , there exists $s\in S$ so $s\to t$ . It can be shown $^*$ that every tournament on $2^{n+1}-2$ vertices has a dominating set of size $n$ . My question is whether this result is tight. Does there exist a tournament on $2^{n+1}-1$ vertices with no dominating set of size $n$ ? If not, what is the smallest tournament with no dominating set of size $n$ ? My thoughts: A necessary condition for a graph on $2^{n+1}-1$ vertices with no dominating set of size $n$ is that every vertex must have an out-degree of exactly $2^n-1$ , so exactly half of its edges are outgoing. The answer is yes when $n=1,2$ . The ""rock-paper-scissors"" graph on three vertices has no dominating set of size $1$ . The graph on $\mathbb Z/7\mathbb Z$ where each $x$ has directed edges to $x+1,x+2$ and $x+4\pmod7$ has no dominating set of size $2$ . For $n\ge 3$ , the possibilities get too large, and I cannot come up with a clever solution. Can anyone see a pattern? I came up with this problem while thinking about this puzzle . $^*$ Consider a vertex $s$ with maximal out-degree. By the hand-shaking lemma, this degree must be at least $(2^{n+1}-3)/2$ , so at least $2^n-1$ . Include $s$ in $S$ , then ignore $s$ and the vertices $t$ for which $s\to t$ . What remains is tournament of size $(2^{n+1}-2)-1-(2^{n}-1)=2^n-2$ . Proceed by induction.","['directed-graphs', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
3066820,Does a zero conditional expectation imply pairwise covariance is 0?,"Suppose in econometrics, $$ y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{k}x_{k} + u$$ In Gujarati's book, it says that the following equation (1) $$ E[u | x_{1}, x_{2},..., x_{k}] = E[u] = 0 \tag{1}$$ given that $$ x_{1}, x_{2},..., x_{k}$$ are independent from each other
implies (with bilinearity property of covariance) $$ Cov(u, x_{1} + x_{2} + ... +x_{k}) = cov(u, x_{1}) = cov(u, x_{2}) = ... = cov(u,x_{k}) = 0$$ How can we derive this result? Book's content: Thank you very much.","['statistical-inference', 'statistics', 'covariance', 'conditional-probability', 'conditional-expectation']"
3066825,Isn't this proof of a theorem about the closedness of a set wrong?,"I was reading a proof of the following theorem in my textbook: A set $A$ is closed iff $A' \subseteq A$ . Proof: Suppose $A$ is closed and $x \in A'$ . If $x \notin A$ , then $x\in A^c$ , an open set. Thus $\mathcal{N}(x, \delta)\subseteq A^c$ for some positive $\delta$ . But then $\mathcal{N}(x, \delta)$ can contain no points of $A$ . Thus $x$ is not an accumulation point of $A$ and so $x\notin A'$ , a contradiction. We conclude that $x\in A$ . Therefore $A'\subseteq A$ . Now suppose $A'\subseteq A$ . To show that $A$ is closed, we show $A^c$ is open. If $A^c$ is not open, there is $x\in A^c$ that is not an interior point of $A^c$ . Therefore, no $\delta$ -neighborhood of $x$ is a subset of $A^c$ ; that is, each $\delta$ -neighborhood of $x$ contains a point of $A$ . This point must be different from $x$ , since $x\in A^c$ . Thus $x\in A'$ . But $A'\subseteq A$ , so $x\in A$ . This is a contradiction. We conclude that $A$ is closed. Somewhere along the proof, the complement of $A$ is proven to be open. In order to achieve this, the author first assumes that it's not open and produces a contradiction. But isn't that wrong since it's possible for a set to be neither open nor closed?",['general-topology']
3066866,How to show that $\sum_{n=1}^{\infty}\frac{H_n}{n}\cos\left(\frac{n\pi}{3}\right)=-\frac{\pi^2}{36}$,"I learnt on www.pi314.net that $$\sum_{n=1}^{\infty}\frac{H_n}{n}\cos\left(\frac{n\pi}{3}\right)=-\frac{\pi^2}{36}$$ This result is hard to verify using Wolfram Alpha since the series converges very slowly. I do not know how to prove this result. I tried to rewrite the original series as $$\sum_{k\geq1} \frac{1}{k}\sum_{n\geq k}\frac{\cos\left(\frac{n\pi}{3}\right)}{n}$$ I know that $$\sum_{n\geq 1}\frac{\cos\left(\frac{n\pi}{3}\right)}{n}=0$$ But since $n$ starts from $k$ , I cannot continue from here.
Any hint?",['sequences-and-series']
3066884,Find the domain and range of a function,"The question is: find the domain and range of $f(x) = \sqrt{{(1-x)}/x}$ . For the domain, I took $x ≠ 0$ and $(1-x)/x ≥ 0$ , which gave me $1 ≥ x$ . So the domain I found is $(-∞,0)∪(0,1]$ . However, the proposed solution says that the domain is just $(0,1],$ without explaining, so I wonder why. Also, any quick tip to find the range (without taking monotonicity, roots, limits to infinity etc.)? I tried inverting the function and the domain of the inverse is $\Bbb R$ , which is clearly not the range of $f$ . Thanks in advance!","['algebra-precalculus', 'functions']"
3066911,Does the improper integral $\int_{\mathbb{R}^2} e^{-xy}\sin(x)$ converge?,"Does the improper integral $\large\int_{\mathbb{R}^2} e^{-xy}\sin(x)\mathrm{d}x\mathrm{d}y$ converge? I know the when the integral is only on $\mathbb{R}^2_+ \times \mathbb{R}^2_+$ then it converges to $\frac{\pi}{2}$ . However here the boundaries are different. Also it is not a positive function so I need to show it on the absolute value. It means I need to show that $$\int_{\mathbb{R}^2} e^{-xy}|\sin(x)|\mathrm{d}x\mathrm{d}y$$ converges. I'm pretty sure it doesn't converge, since I can't bound it from the right by a converge-bale function (I tried with $e^{-xy}$ , also, Wolfram Alpha said so too). However I can't think of a function that won't converge and will bound it from the left. Help would be appreciated.","['integration', 'improper-integrals', 'multivariable-calculus', 'convergence-divergence']"
3066982,Simplify $\sqrt{\dfrac{\sqrt[3]{64} + \sqrt[4]{256}}{\sqrt{64}+\sqrt{256}}}$ into $\dfrac{\sqrt{3}}{3}$,"I am on the final question of a textbook chapter on radicals and this question feels more challenging, perhaps that's the idea. If you view my post history I typically make a effort to provide some working to simplify the expression to an extent, but here I am very confused about where to go or my first steps. I am to simplify: $$\sqrt{\frac{\sqrt[3]{64} + \sqrt[4]{256}}{\sqrt{64}+\sqrt{256}}}$$ The solution is $\displaystyle \frac{\sqrt{3}}{3}$ The expression makes me ""feel"" like there is a rule when dividing radicals with the same radicand but different index' with different index'. Is that true? In this case, how to divide $\frac{\sqrt[3]{64}}{\sqrt{64}}$ ? I know that the 3rd root and sq roots are 4 and 8 which would leave me with 1/2. using a calculator I can see that the 4th root of $256$ is 4 but I think I'm to arrive at the solution without a calculator. Is there a prescribed approach or order of operations to simplifying an expression like this? How can I arrive at $\dfrac{\sqrt{3}}{3}$",['algebra-precalculus']
3066989,"Filling a gap in triangle inequality proof for $d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert $.","For the function $d:\mathbb{R} \times \mathbb{R} \to \mathbb{R}$ defined as $$d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert. $$ I want to prove this is a metric but Im having issues proving the triangle inequality. I know this problem has been discused before and the one idea im trying to prove this is using the fact that $f(x)=\frac{x}{1+x}$ is increasing and also satisfies the subadditivity property. This is $f(a+b) \leq f(a) + f(b)$ for every $a,b \geq 0$ . Im stuck proving the subadditivity, someone mention this can be done with intermediate value theorem but I dont see how. Thanks for reading and helping.","['calculus', 'derivatives', 'metric-spaces', 'real-analysis']"
3066991,Power Series of an Operator,"I'm working through some functional analysis problems and  am having trouble with the following. Let $f(z)=\sum_{n=0}^\infty a_n z^n$ be a power series with radius of convergence $R>0$ . Let $A\in\mathcal{B}(\mathcal{H})$ with $||A||<R.$ Show the following: (a) There is a $T\in\mathcal{B}(\mathcal{H})$ with $\langle Tx, y\rangle=\sum_{n=0}^\infty a_n\langle A^nx,y\rangle$ for $x,y\in\mathcal{H}.$ Here we denote $T$ by $f(A)$ . (b) That the partial sums of $f(A)$ converge to $T$ in the operator norm. (c) That $f(A)B=bf(A)$ whenever $B\in\mathcal{B}(\mathcal{H})$ with $BA=AB.$ (d) For $f(z)=e^z$ for $A$ self-adjoint we have $f(iA)$ is unitary. Solution attempts: (a) I think $T = \sum_{n=0}^\infty a_n A^n$ ,  but am  having trouble showing this is within $\mathcal{B}(\mathcal{H})$ . In particular, I have for $h\in\mathcal{H}$ with $||h||=1,$ $|| \sum_{n=0}^\infty a_nA^n h||\leq |a_0|||h||+|a_1|||Ah||+|a_2|||A^2h||+\cdots$ My thought was to continue this chain of inequalites until I got a series that converged, but since I am taking the absolute values of the coefficients I don't think that this will work. (b) I think I could get this part if I saw how part (a) is done. (c) $$
    f(A)B = (\sum a_n A^n ) B = (a_0+a_1A+a_2A^2+\cdots)B\\
    =a_0B+a_1AB+a_2AAB+\cdots = Ba_0+Ba_1A+Ba_2AA+\cdots \\
    =B(\sum a_n A^n) = Bf(A)
    $$ (d) $f(iA)=\sum \frac{1}{n!} (iA)^n\Rightarrow f(iA)^*= (\sum \frac{1}{n!}(iA)^n)^*=\sum \frac{1}{n!}(i^n)^*(A^n)^*=\sum \frac{1}{n!}(i^*)^n(A^*)^n=\sum \frac{1}{n!}(-i)^nA^n =f(-iA)$ . Since $A$ commutes with itself, we have $f(iA)f(-iA)=f(-iA)f(iA)=e^{iA}e^{-iA}=e^{0}=I$","['power-series', 'matrix-exponential', 'functional-analysis']"
3067034,Laurent Series of $\ln(|x|)$?,"I read in this question that no Laurent series exists for $\ln(x)$ , as Patch's answer shows: ""The problem with $\log z$ in the complex plane is that it is a ""multi-valued function"", so we must specify what range of values we are considering the function to have. Because we must make this choice, the function fails to be continuous in any (punctured) disk about $z=0$ , and thus is not (complex) differentiable in any neighborhood about the point."" For me, this raises the question of what if the function were differentiable on a punctured disk about $z=0,\;$ that is, is $\ln(|z|)$ holomorphic, and, if so, what is its Laurent series? I believe from Owen Biesel's answer here that $\ln(|z|)$ is holomorphic, but I am new to complex analysis, so I could be very wrong. If it is holomorphic,  how would I go about calculating its Laurent series? I understand that it involves taking a positively oriented closed contour integral, but I am having trouble understanding how to properly take contour integrals, as I keep obtaining answers of 0. Thank you in advance!","['complex-analysis', 'analytic-number-theory', 'complex-numbers']"
3067039,Sylow Subgroups and Conjugation,"Suppose that $G$ is a finite group whose order is divisible by a prime $p$ . Let $S$ be the set of Sylow $p$ -subgroups of $G$ ; let $H$ be an element of $S$ . $H$ acts on $S$ by conjugation. The fact that is the key to the proof of the Third Sylow Theorem is that there is only one $H$ -orbit of order $1$ . I was wondering if we could say more about this action. I first thought that maybe there are only two $H$ -orbits. But this turned out to be false in general. (I saw that it was false in the dihedral group $D_{5}$ .) Next I checked whether all the orbits other than $\{H\}$ has order $H$ . This is false in general, too. (This is false in $A_{5}$ .) After fiddling with some examples, I found that all the $H$ -orbits except $\{H\}$ seem to have the same order. But I can neither prove or disprove this claim. So here are my questions: Is the above claim correct? If so, why? Is there any other things we can say about the $H$ -orbits? Any help is appreciated. Thanks in advance!","['group-theory', 'abstract-algebra', 'finite-groups', 'sylow-theory']"
3067046,What algorithm can I use to find this ellipse inscribed in a quadrilateral?,"There's a certain drawing exercise designed to improve a drawing student's understanding of perspective and ability to draw shapes freehand. (The actual exercise is described by Irshad Karim on Drawabox.com, on the pages "" Ghosted Planes "" and "" Ellipses in Planes "".) The exercise consists of drawing a convex quadrilateral, then drawing a window-like structure inside the quadrilateral, and finally drawing the ellipse implied by this structure. Ideally, the result will look similar to this diagram: The exact steps of the exercise are: Draw any convex quadrilateral $ABCD$ . Draw the diagonals of the quadrilateral; call their intersection point $E$ . Draw a line segment passing through $E$ , which is concurrent to the edges $AD$ and $BC$ . (Line segments are concurrent if they are all parallel, or if, when they are extended to lines, the resulting lines all intersect at a single point.) One endpoint should lie on $AB$ , and be labeled $F$ ; the other endpoint should lie on $CD$ , and be labeled $G$ . Likewise, draw a line segment passing through $E$ , which is concurrent to the edges $AB$ and $CD$ . One endpoint lies on $AD$ and is labeled $H$ , the other lies on $BC$ and is labeled $J$ . Finally, draw the unique ellipse which is tangent to $ABCD$ at $F$ , $G$ , $H$ , and $J$ . Given the coordinates of $A$ , $B$ , $C$ and $D$ , what algorithm can be used to find the resulting ellipse? I'm not 100% sure that there is always a unique ellipse tangent to $ABCD$ at $F$ , $G$ , $H$ and $J$ , but it definitely seems like there is. There's always a unique ellipse which is tangent at $F$ and which passes through $G$ , $H$ , and $J$ ; and from experimentation, it looks like this ellipse is always tangent at $G$ , $H$ and $J$ as well. In the case where $ABCD$ is a square, everything is especially simple. The resulting ellipse is the circle inscribed in the square. I suspect that every case is simply the image of this case under some type of perspective transformation which preserves ellipses. But I don't know how to prove this, or how to make use of this fact.",['geometry']
3067099,$\{x\} \times B \subset \mathbb{R}^{n+m}$ is compact if $B \subset \mathbb{R}^m$ is compact and $x \in \mathbb{R}^n$,"This is from Spivak Calculus on manifolds page 14. 
I cannot use the fact that the cartesian product of two compact sets which are subsets of Euclidean space is compact because he uses what is written in the title to prove that fact. It seems that I am only allowed to use the definition of compactness here, which is that a set is compact if every open cover of it has a finite subvoer.
(The book only deals with Euclidean space here, and Spivak defines an open set $U$ to be such that for every element $x \in U$ , there is an open rectangle $A$ such that $x \in A \subset U$ .) As he says in the book, this must be very easy to see, but I just cannot figure out. Thanks in advance.","['general-topology', 'analysis', 'real-analysis']"
3067102,Understanding operator under a subtitution,"in my notes, I have the following phrase: With $x = e^t$ and $x \frac{d}{dx} = \frac{d}{dt}$ How, how come we get $x \frac{d}{dx} = \frac{d}{dt} $ ? I know if I diferentiate with respect to $t$ I obtain $$ \frac{d}{dt} x = x $$ I do not understand how this operators are defined? maybe I am misunderstading the notation?",['calculus']
3067109,Images and Inverses,"Definition 0.2.10 Let $A$ and $B$ be sets, and $f: A \rightarrow B$ be a function. Let $S \subseteq B$ . Then the set $f^{-1}(S)=\{ x\in A : f(x) \in S\}$ is called the inverse image of the set $S$ under the function $f$ . Definition 0.2.14 Let $A$ and $B$ be sets, and $f: A \rightarrow B$ be a function. Let $T \subseteq A$ . Then the set $f(T)=\{x\in B : x=f(t) \text{ for some }t \in T \}$ is called the image of the set $T$ under the function $f$ . How do these two definitions differ from traditional usage of $f$ and $f^{-1}$ as in a calculus course? My attempt to make any sense of it: They are similar in many ways. In the definitions, for a function $f$ that maps $A$ to $B$ is similar to plugging a value $x$ into $f(x)$ and getting an output $y$ . But, in the definitions given above, $f$ is invertible if it is one-to-one and onto, but how does this differ from the traditional $f$ used in calculus? I cannot seem to find the ties or correlations.",['elementary-set-theory']
3067148,"One lily pad, doubling in size every day, covers a pond in 30 days. How long would it take eight lily pads to cover the pond? [duplicate]","This question already has answers here : A lily pad doubles in area every second. After one minute, it fills the pond. How long would it take to quarter fill the pond ? (8 answers) Closed 5 years ago . A lily pad sits on a pond. It doubles in size every day. It takes 30
  days for it to cover the pond. If you start with 8 lily pads instead,
  how many days does it take to cover the pond? I think that the answer is $27$ , but I don't really think that makes sense intuitively. I think that, intuitively, the answer should be less than $30/4$ since it is increasing at an exponential rate.","['algebra-precalculus', 'sequences-and-series']"
3067188,Polynomial Long Division Confusion (simplifying $\frac{x^{5}}{x^{2}+1}$),"I need to simplify \begin{equation}
\frac{x^{5}}{x^{2}+1}
\end{equation} by long division in order to solve an integral. 
However, I keep getting an infinite series: \begin{equation}
x^{3}+x+\frac{1}{x}-\frac{1}{x^{3}}+...
\end{equation}","['algebra-precalculus', 'polynomials']"
3067189,Torricelli's law modelling Sphere,"Struggling connecting Water-volume to time. Could anyone be kind and guide me in the right direction? A spherical tank is filled with liquid. A tap in the bottom of the
  tank opens, and the liquid starts to drain. After one hour is The tank
  emptied halfway. How long does it take before it is completely empty?
  (The flow is assumed to follow Torricelli's law: its is proportional
  to the square root of the surface So far I'm mostly grasping at straws. I've come up with $V'(x) = -k\sqrt{h}$ Assuming r=1 => $V(h) = \int_0^h\pi(1-y^2)dy $ $V(h)$ is a primitive to $V'(h) = \pi (1-h^2)$ $V(h=1)= \frac{2\pi}{3}$ $V(h=\frac{1}{2})= \frac{2\pi}{6}$ $ t = 0 \iff h = 1$ , $ t = 1 \iff h = \frac{1}{2}$ Not sure how to connect this to time further to figure out when $\int_0^tV'(t)dt$ = 0","['integration', 'ordinary-differential-equations']"
3067228,A formula for the number of possible combinations of a $i\times j$ rectangle in a $m\times n$ grid such that they don't overlap?,"Suppose I have a grid of size $m\times n$ and a rectangle of length $i\times j$ where $i$ and $j$ are integers as shown here for where $m = 7$ , $n = 5$ , $i = 2$ , $j = 3$ : ￼ Does there exist a formula or equation that determines the number of combinations that are possible such that the grid is filled by the rectangle as much as possible and there is no overlap? perhaps in terms of $m,n,i,j$ ? A few combinations would be as shown here: ￼","['combinations', 'discrete-geometry', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics']"
3067266,"A counter-example for integration by parts when there are ""small"" singularities","I am looking for a ""counter-example"" to integration by parts of the following type: $\Omega \subseteq \mathbb R^n$ is an open, bounded, connected domain with smooth boundary. $u,v:\bar \Omega \to \mathbb R$ are real-valued functions, where $u$ is smooth and compactly supported in $\Omega$ , and $v$ is smooth on an open subset of $\bar \Omega$ whose complement is a closed subset of measure zero. I want $\int_{\Omega}(\partial_iu)v \neq -\int_{\Omega}u(\partial_iv)$ , i.e. to demonstrate failure of integration by parts. Edit: Does the answer change if we assume in addition that $v$ is continuous everywhere on $\bar \Omega$ ?. BigbearZzz gave here an example with a non-continuous $v$ . If $v$ was smooth on all $\bar \Omega$ , then integration by parts would work. The point is that I am limiting the singular set to be closed and of measure zero. I guess integration by parts still cannot be saved, but I don't have a concrete example.","['integration', 'measure-theory', 'definite-integrals', 'singularity', 'geometric-measure-theory']"
3067285,"Have I correctly proved that $\lim_{||(x,y)||\to\infty}\frac1{y-x}\int_x^y\exp(-1/|t|)dt$ equals $1$?","I should prove that, as long as $y\ne x$ , $$f(x,y)=\frac1{y-x}\int_x^y\exp(-1/|t|)dt\longrightarrow1 \ \text{as $||(x,y)||\to\infty$}$$ and I would like to do it without $\varepsilon-\delta$ reasoning. My idea was to let $h=y-x$ , and then separately consider the cases $|h|\to\infty$ and $|h|\not\to\infty$ , as $||(x,y)||\to\infty$ . We have $$f(x,y)=f_h(x)=\frac1{h}\int_x^{x+h}\exp(-1/|t|)dt=\frac1h\int_0^h\exp(-1/|t+x|)dt.$$ If $|h|\to\infty$ then at least one of $|x|$ and $|y|$ must also go to $\infty$ . In the former case, by the dominated convergence theorem, $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=1;$$ in the latter case, by De L'Hospital, $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\exp(-1/|h+x|)=\lim_{|y|\to\infty}\exp(-1/|y|)=1. $$ Finally, suppose $|h|\not\to\infty$ ; then $|x|\to\infty$ , as a consequence of \begin{align}\sqrt{x^2+y^2}=\sqrt{x^2+(x+h)^2}=\sqrt{2x^2+2hx+h^2}&\le\sqrt{4x^2+4|hx|+h^2} \\ &\le\sqrt{(2|x|+|h|)^2} \\ &=2|x|+|h|.\end{align} So we get once again $$\lim_{||(x,y)||\to\infty}\frac1h\int_0^h\exp(-1/|t+x|)dt=\lim_{||(x,y)||\to\infty}\frac1h\int_0^hdt=\lim_{||(x,y)||\to\infty}\frac{h}h=1.$$ Is my proof correct? Otherwise, how can I amend it?","['integration', 'proof-verification', 'real-analysis', 'alternative-proof', 'multivariable-calculus']"
3067342,Split up a double integral,"Is it true that $\int_a^b \int_c^d f(x)g(y)dydx = \int_a^b f(x)dx \cdot \int_c^dg(y)dy $ My intuition says it is true, but I also have the feeling that I am missing something, but I cannot prove it. In my application $a,b,c,d$ are length variables. $f(x)$ and $g(y)$ are sinusoidal functions. I would like to prove the general case, can somebody help me out?","['integration', 'multivariable-calculus']"
3067359,Minimal polynomial of a matrix having only 1s on the counter diagonal,"Consider the matrix $A=a_{ij}$ where $$a_{ij}=\begin{cases}1\ \ \text{if}\ \ i+j=n+1\\0\ \ \text{otherwise}\end{cases}$$ . Then, what can be said about the minimal polynomial of the matrix $A$ . Note that one eigenvalue is easily found by taking the eigenvector $\begin{pmatrix}1\\1\\1\\\ldots\\\ldots\\\ldots\\1\end{pmatrix}$ . Any hints. Thanks beforehand.","['matrices', 'minimal-polynomials', 'linear-algebra']"
3067383,Orthogonal matrices only defined for standard inner product?,"$\newcommand{\tp}[1]{#1^\mathrm{T}} \newcommand{\Id}{\mathrm{Id}} \newcommand{\n}{\{1,\ldots,n\}} \newcommand{\siff}{\quad\Leftrightarrow\quad} \newcommand{\ijth}[2][\tp{Q}Q]{[#1]_{#2}} \newcommand{\K}{\mathbb{K}}$ Let orthogonal matrices be defined as follows. A matrix $Q\in\mathcal{M}_{m\times n}(\mathbb{K})$ , where $\mathbb{K}$ is a field, is said to be orthogonal if $$ Q^\mathrm{T}Q = \mathrm{Id}_n$$ I'm not fully sure if I'm understanding the following fact correctly: A matrix $Q\in\mathcal{M}_{m\times n}(\K)$ is orthogonal iff
  the columns of $Q$ form an orthonormal set in $\K^m$ . Proof Let $q_i$ denote the $i$ -th column of $Q$ for all $i\in\{1,\ldots,n\}$ , and let $\ijth[A]{ij}$ denote the $(i,j)$ -th
  element of $A$ for any matrix $A$ . Then, $Q$ being an orthogonal
  matrix is equivalent to $$\tp{Q}Q = \Id_n \siff \ijth{ij} = \delta_{ij}\,,$$ where $\delta_{ij}$ is the Kronecker delta. On the
  other hand, by the definition of matrix multiplication, $$\ijth{ij} = \sum_{k=1}^{m} \ijth[\tp{Q}]{ik}\ijth[{Q}]{kj} = \sum_{k=1}^{m} \ijth[Q]{ki}\ijth[{Q}]{kj} \stackrel{\color{red}*}{=} \langle q_i, q_j\rangle\,.$$ Thus $Q$ is orthogonal iff $$ \langle q_i, q_j\rangle = \delta_{ij} \qquad\forall (i,j)\in\n\times\n\,, $$ which is true iff $(q_i)_{i\in\n}$ form an orthonormal set. Particularly, I'm suspicious of the equality marked with the red asterisk. Isn't that true only for the standard inner product (i.e. the dot product), defined as $
\langle u, v \rangle = \sum_i u_iv_i\ 
$ ? So, are orthogonal matrices only treated in the context of the standard inner product? If so, is there a ""generalization"" of orthogonal matrices for general inner product spaces?","['inner-products', 'orthogonal-matrices', 'linear-algebra']"
3067456,"Find a function which when applied to the inverse of an argument, only changes sign","So basically, a function $f$ with $f(\frac{1}{x}) = - f(x)$ . Additionally, it should also be strictly increasing. I know that the logarithm has this property, but I'm looking for a function with different boundary conditions. Namely: f(0) = -1 (and $f(x -> \infty) = 1$ ). I know one solution to this: $f(x) = \frac{x-1}{x+1}$ , but I am wondering: Is there a general method to find such functions? Is my given solution unique?",['functions']
3067562,Clarification on set definitions from How to Prove it (Velleman) Chapter 2.3,"I am reading the first two paragraphs of Chapter 2.3 in How to Prove It (Velleman) and I am unclear about why $x \in \{n^2 | n \in \Bbb{N}\}$ is the same as $\exists n \in \Bbb{N}(x = n^2)$ I will lay out Velleman's explanation and my understanding of it, I would love some feedback on if I'm thinking about this correctly. For the reader, up until this point in the book, we know two ways to define sets. list elements in brackets $$\{1, 2, 3, 4\}$$ use elementhood notation $$\{x \ |\ P(x)\}$$ Velleman suggests that it is common to replace the $x$ in front of the vertical line with a more complex expression. He uses the example of $S$ , the set of perfect squares $$S = \{n^2\ |\ n \in \Bbb{N}\}\tag{1}$$ He claims that (1) can be written as $$S = \{x \ |\ \exists n \in \Bbb{N}(x=n^2)\}\tag{2}$$ Thus $$S = \{n^2\ |\ n \in \Bbb{N}\} = \{x \ |\ \exists n \in \Bbb{N}(x=n^2)\}$$ At this point I'm still with Velleman. Replacing x before the vertical line with a more complex expression is intuitive. Putting the statement ""x is a perfect square"" into the form $\exists n \in \Bbb{N}(x=n^2)$ and defining the set S using that logical statement as an elementhood test - $S = \{x\ |\ \exists n \in \Bbb{N}(x=n^2)\}$ - makes sense ( this question is a good discussion. on eq 2 ) Velleman continues and therefore $x \in \{n^2\ |\ n \in \Bbb{N}\}$ means the same thing as $\exists n \in \Bbb{N}(x = n^2)$ I am missing the jump to this final conclusion, here are my thoughts so far. First thought Perhaps a step Velleman is taking (and assumes reader will make) is that $x \in \{n^2\ |\ n \in \Bbb{N}\}$ is the same as $x \in \{x\ |\ \exists n \in \Bbb{N}(x = n^2)\}$ . And then, using the following from Chapter 1.3, in general, the statement $y \in \{x\ |\ P(x)\}$ means the same thing as P(y)...a statement about y but not x. from this, we can see that $x \in \{x\ |\ \exists n \in \Bbb{N}(x = n^2)\}$ means the same thing as $\exists n \in \Bbb{N}(x = n^2)$ , and then arrive at the conclusion. Second thought (not entirely unrelated way of thinking about it) $\exists n \in \Bbb{N}(x = n^2)$ and $x \in \{n^2\ |\ n \in \Bbb{N}\}$ are both statements about x, where x is a free variable. These statement can be evaluated to true or false, but that will depend on the value of x that is used. They will always evaluate to the same truth/false value, therefore they mean the same thing.","['elementary-set-theory', 'proof-explanation', 'predicate-logic', 'logic']"
3067566,Coordinate basis and coordinate systems,"When we introduce coordinate systems, like spherical coordinates, one usually does it with respect to cartesian coordinates. What would be the right way to derive the (for example) spherical coordinate basis of the tangent space at a point of a manifold(without using cartesian coordinates at all)?
I mean, I have seen the definition of the tangent space and the coordinate basis, but how does one compute it in practice? And deduce the metric tensor from the coordinate basis?","['coordinate-systems', 'curvilinear-coordinates', 'tangent-spaces', 'manifolds', 'differential-geometry']"
3067569,Integro-differential equation including a convolution of the first derivative.,"I am having difficulty finding the right approach to solving the following differential equation, $$
y''(t)+\int_t^Tg(s-t)y'(s)\,ds=f(t),
$$ with the boundary conditions, $$y(0)=y_0\,,\quad y(T)=0.$$ I considered representing $y(t)$ using a Fourier series, but term-wise differentiation is not guaranteed because the periodic extension of $y(t)$ isn't 
differentiable at the boundaries $t=0,T$ . What approach is the smartest to solve this equation? Any help will be appreciated.","['boundary-value-problem', 'convolution', 'ordinary-differential-equations', 'integro-differential-equations']"
3067596,Assuming the existence of solutions in solving exercises,"A sizeable chunk of my first calculus course at university comprised of learning techniques to evaluate limits, such as this simple example, evaluating the limit: $$\lim_{x \to 7} \frac{x^2 -8x + 7}{x-7}.$$ A typical solution would be to identify that for $x \neq 7$ , $$\frac{x^2 -8x + 7}{x-7} = x-1,$$ so $$\lim_{x \to 7} \frac{x^2 -8x + 7}{x-7} = \lim_{x \to 7} x-1 = 6.$$ In my eyes, we have shown that if the limit exists, its value must be $6$ . We have not shown that the limit exists in the first place and is equal to $6$ , since we have presupposed the existence of the limit when writing $$\lim_{x \to 7} \frac{x^2 -8x + 7}{x-7} = \lim_{x \to 7} x-1,$$ since the existence of both objects on either side of an equality is a necessary condition for the equality to be true (right?). My main questions are: do such methods of evaluation serve as evidence that these limits in fact exist in the first place, or do they only tell us what the limit ought to be, and the only way we can be sure is to formally prove it using the $\epsilon$ - $\delta$ definition? Is this case similar to ""finding"" the derivatives of functions?","['limits', 'algebra-precalculus']"
3067649,Prove $\frac{dw}{dt} + aw \leq 0$ implies that $w(t)$ is a decreasing function,"Question: Suppose that $w(t)$ is a non-negative continuous function, and there exists a constant $a \in \Bbb R$ such that \begin{align}
\frac{dw}{dt} + aw & \leq 0 \qquad \qquad \text{for }t \geq 0 \\
w(0) & = 0
\end{align} Is it then true that $w(t)$ is a decreasing function (on the interval $[0,\infty)$ )? Attempt: If in addition it is known that $a \geq 0$ , then this would be easy, since we would have $$\frac{dw}{dt} + aw \leq 0 \implies \frac{dw}{dt} \leq -aw \leq 0$$ However, I want to prove this for general constant $a$ . I have a feeling that I can somehow compare this to the exponential function $Ae^{-at}$ as this is the solution of the differential equation $$\frac{dw}{dt} + aw =0$$ Gronwall's inequality also comes to mind, but I am not sure how to use it. Any help would be much appreciated. Thanks! Edit: It seems to me that $w$ must in fact be the zero function. Thinking about it in an intuitive/heuristic way, $$w'(0) + aw(0) \leq 0 \implies w'(0) \leq 0$$ since $w(0)=0$ , i.e. $w$ is decreasing at $t=0$ . However, $w$ is non-negative with $w(0)=0$ , so in fact $w'(0)=0$ . This implies that $w(0 + \delta) = 0$ and we can repeat the above argument.","['inequality', 'ordinary-differential-equations']"
3067653,Show that Kernel density estimate integrates to 1,"I'm trying to show that the Kernel density estimate $$ f(x) = \frac1{nb}\sum_{i=1}^nK \left(\frac {X_i - x}b\right) $$ actually integrates to 1. The $X_i$ are iid and K is a symmetric probability density function; in particular, K integrates to 1. What I get is $$\int_{-\infty}^{\infty} \frac1{nb}\sum_{i=1}^nK\left(\frac {X_i - x}b\right) dx$$ $$ = \frac1{nb}\int_{-\infty}^{\infty} \sum_{i=1}^nK\left(\frac {X_i - x}b\right) dx $$ $$=\frac1{nb}\sum_{i=1}^n\int_{-\infty}^{\infty} K\left(\frac {X_i - x}b\right) dx$$ $$=\frac1{b}\int_{-\infty}^{\infty} K\left(\frac {X_i - x}b\right) dx$$ Using as a substitution $$ u = \frac {X_i - x}b$$ $$ dx = -b du$$ we get $$=\frac1{b}\int_{-\infty}^{\infty} K(u) (-b) du $$ $$= (-b) \frac1{b}\int_{-\infty}^{\infty} K(u) du  = -1$$ Something must have gone wrong with the integration. I know that $f(x)$ is often defined differently, as $$ f'(x) = \frac1{nb}\sum_{i=1}^nK\left(\frac {x - X_i}b\right) $$ and with this version I can show that it integrates to 1. However $f'(x)$ and $f(x)$ should be equivalent as $K$ is symmetric. So there is some flaw in my calculations (my knowledge about integration with substitution is purely heuristic).","['integration', 'statistics', 'probability-distributions', 'improper-integrals']"
3067662,Is Neighborhood an open set?,"I am reading a book where it is written that , Let $(X,d)$ be any metric space $a \in X$ then for any $r \gt 0$ the set $S_r(a)$ ={ $x \in X$ : $d(x,a) \lt r$ } is called an open ball of radius $r$ centered at $a.$ & 
  Let $(X,d)$ be any metric space and $x \in X$ . A subset $N{(a)}$ of $X$ is called a neighborhood of a point $a$ , if there exist an open ball $S_r(a)$ centered at $a$ and contained in $N{(a)}$ i.e $S_r(a)$ $\subseteq$ $N{(a)}$ . But in Rudin ,it is given that in a metric space $X$ a neighborhood of $a$ is a set $N_r(a)$ containing of all q such that $d(a,q) \lt r$ , for some $r \gt 0$ ,the number $r$ is called the radius of $a$ .
According to the definition of Rudin every neighborhood is an open set.
But according to  the text which I am reading,  does it tell that every neighborhood is an open set?",['general-topology']
3067737,limit of function: $ \lim_{x \to \infty} \frac{[x]}{x}$,"This is my homework: Limit of functions: ( $[x]$ is the total part of $x$ ) $\displaystyle \lim_{x \to \infty} \frac{[x]}{x}$ $\displaystyle \lim_{x \to -\infty} \frac{[x]}{x}$ $\displaystyle \lim_{x \to 0} \frac{[x]}{x}$ For the first one, I used the inequality $$x -1 \leq [x] \leq x.$$ So, $$\frac{1-\frac{1}{x}}{1}=\frac{x-1}{x} \leq \frac{[x]}{x} \leq \frac{x}{x}=1.$$ So my answer is $\displaystyle \lim _{x \to \infty} \frac{[x]}{x} = 1$ .
Is that good?
I don't know what to use in other examples $\ldots$","['limits', 'calculus', 'ceiling-and-floor-functions']"
3067739,Calculating UMVUE for Poisson distribution,I've started to learn methods of finding UMVUE distribution. I found some nice examples on this site . I got stuck while evaluating UMVUE for Poisson distribution for a parametric function $g(\theta)$ . How did the author get $$\sum_{t=o}^\infty \frac{h(t) n^t}{t!} \theta^t = e^{n \theta}g(\theta)?$$ I thought $$\mathbb{E}(h(x)) = g(\theta)$$ should be considered.,"['statistics', 'parameter-estimation']"
3067765,What is the relationship between probability densities and mass densities?,"In statistics we have probability distribution functions which give us likelihood that some random variable ( $X$ ) will equal a particular value. Assuming this variable is continuous, the distribution satisfies: $$
\int f_{X}(x) \ dx = 1
$$ and can be used to obtain the moments of the variable \begin{align}
\mu'_i = \int X^i f_x(x) dx
\end{align} In physics we often work with the mass distribution function which describes the distribution of mass. For example the total mass across a system is given by: $$
M_{tot} = \int g_M(m) \ dm
$$ and like the statistical PDF we can also derive moments of this mass distribution function by integrating across the distribution function: $$
\mu'_i = \int m g_M(m) \ dm
$$ I am wondering what the connection between these two concepts is and if there is any way to move between them?","['physics', 'statistics', 'probability-distributions']"
3067779,Geometric proof/evidence for the 3x3 matrix determinant's formula?,"I'm struggling to find a visually intuitive proof for the formula of a 3x3 matrix. I searched it online for hours and it seems impossible to find a source that attempt at least to explain where does it come from. I'm aware that a very similiar question has been asked but it's a very old question and it probably didn't get the deserved attention (I'm referring to this ).
Moreover, I think that the increased popularity of the site since then may offer this  question the possibility to get  a better and more satisfactory answer which would turn helpful for a lot of students. This what a visual explanation for the determinant's formual of a 2x2 matrix: This makes it very clear why determinant for a 2x2 matrix is a d-b c and it visually explains how determinant is linked to the area of a parallelogram. I'm not looking necessarly for this kind of ""geometric"" proof. It would be helpful any intuitive explanation for the formula.","['determinant', 'linear-algebra', 'linear-transformations']"
3067795,Deriving the central Euler method and intuition,"My professor (Dutch) asked us to determine, among other things, the truncation error of the central Euler method. First of all, this is probably not the correct term, since there are very few results for ""Central Euler"", so that made looking things up a hassle. To determine the truncation error, I thought I had to first know how to derive this central Euler method, given by: $$
u_{k+1}=u_{k-1} + 2h\cdot f(u_k)
$$ (It also states $u_0$ and $u_1$ are given/known) In which $f(x)$ gives the slope at point $x$ (I think). I tried to derive this using Taylor expansions, but I didn't get close. I also thought I had to get an intuitive notion of what this means, and I figured out it's this: We add two times the slope (times the step size) at $x=u_k$ to the the y-coordinate at $u_{k-1}$ to get the y coordinate at $x=u_{k+1}$ . The two times is because the length between $k-1$ and $k+1$ is equal to $2h$ . So my question is: How do I derive this method, what's it called, and is the derivation a good step in figuring out the truncation error? Note: It was given that the truncation error is of order $h^3$ .","['truncation-error', 'numerical-methods', 'ordinary-differential-equations']"
3067798,Calculate $\lim_{n \to \infty} \int_{\mathbb{R_{+}}} \exp((\cos^n x) -x) d\lambda(x)$,"the exponential function being increasing we have $| \exp((\cos^n x) -x)| \leq \exp(1 -x) \in L^1([0,+\infty[) $ so $x \to \exp((\cos^n x) -x)$ is Riemann absolutely convergent therefore $l = \lim_{n \to \infty} \int_{\mathbb{R_{+}}} \exp((\cos x^n) -x) d\lambda(x) =\lim_{n \to \infty} \int_{0}^{+\infty} \exp((\cos^n x) -x) dx $ by the dominated convergence theorem  : $l = \int_{0}^{+\infty} \lim_{n \to \infty}  \exp((\cos^n x) -x) dx$ I don't know how to deal with this limit, as $x$ is in $\mathbb{R_{+}}$ I can't even use a taylor expression around $0$ any hints ?","['integration', 'measure-theory', 'lebesgue-measure', 'improper-integrals', 'lebesgue-integral']"
3067862,Probability Mass Function of Number of Draws with Replacement Until N Distinct Results are Obtained,"Let $X$ be the discrete random variable that corresponds to the number of draws (with replacement) from a population of $k$ distinct objects required until $n$ distinct results are obtained, where $1\le n\le k$ .  Each item is equally likely to be drawn. What is the probability mass function (PMF) of $X$ ?  Its support is $[n,\infty]\cap\mathbb{N}$ . The only portion I'm able to derive is this: $$P(X=n)=\prod_{i=k-n+1}^{k}\frac{i}{k}$$ But what about on the rest of the support?  I assume it's going to involve combinatorics, but I'm not sure how to derive it.","['discrete-mathematics', 'probability-distributions', 'combinatorics', 'probability']"
3067872,"If $X_n$ is Gamma $(n,\lambda)$ distributed then $(\lambda X_n -n)/\sqrt n\to N(0,1)$","Let $X_n$ be Gamma $(n,\lambda)$ distributed, and $Y_n = \dfrac{\lambda X_n -n}{\sqrt{n}}$ .
  Show that $Y_n \rightarrow N(0,1)$ . My idea to prove this is to use Lévys theorem with the characteristic functions. If I can show that the characteristic functions $\phi_{Y_n}$ of $Y_n$ converge to a function $\phi$ , which is continous in $0$ , then there exists a random variable $Y$ such that $\phi$ is the characteristic function of $Y$ . My aim is to get $\phi_Y (u) = e^{-u^2/2}$ since I have to show that the $Y_n$ converge to a standard normal random variable and the cF describes the distribution uniquely. I know that the cF of a Gamma $(n,\lambda)$ -distributed random variable $X_n$ is given by $$\phi_{X_n}(u) =\left(\frac{\lambda}{\lambda - iu}\right)^n $$ The cF rules for the linear transformation $$Y_n = \frac{\lambda X_n}{\sqrt{n}} +\frac{-n}{\sqrt{n}}$$ give $$\phi_{Y_n} (u)= e^{-in \frac{u}{\sqrt{n}}} \phi_{X_n}(\frac{\lambda u}{\sqrt{n}}) = e^{-iu \sqrt{n}} \left(\frac{\lambda}{\lambda - i \frac{\lambda u}{\sqrt{n}}}\right)^n$$ that is, $$\phi_{Y_n} (u) = e^{-iu \sqrt{n}} \left(\frac{1}{1 - i \frac{ u}{\sqrt{n}}}\right)^n = e^{-iu \sqrt{n}} \frac{1}{ (1 - i \frac{u}{\sqrt{n}})^n}$$ and now I don't know how to continue calculating. Since I need the limit of this, I thought about the representation of the exponential function in terms of the limit of a sequence, e.g. $$e^x = \lim_{n \rightarrow \infty} \left( 1 + \frac{x}{n}\right)^n$$ Some hints or a trick would be very nice.","['characteristic-functions', 'gamma-distribution', 'central-limit-theorem', 'probability-theory']"
3067909,Is $(\sum_{k=0}^n \sin(k!))_{n \in \mathbb{N}} $ bounded?,"It is a relatively easy exercice to show that $(\sum_{k=0}^n \sin(k))_{n \in \mathbb{N}} $ is bounded by considering complex exponentials $e^{ik}$ (that form a geometric sum) and then taking the imaginary part. But what happens when we replace $k$ by $k!$ ? I think that it is bounded by analogy. The terms somehow compensate one another... But I was unsucessfull at proving it. One of my attempts : I tried to prove that $n!$ is an equidistributed sequence mod $2 \pi$ (I'm not sure that's true), but even with that I struggle to conclude.","['sequences-and-series', 'real-analysis']"
3067954,Why is it that the surface integral of the flux of a vector field is the same as the surface integral of the vector field itself?,"In other words, this: http://www.math.ucla.edu/~archristian/teaching/32b-w17/week-7.pdf Is this just a definition because what we really care about is how much the vectors are ""pushing"" through the surface? Or is it an actual equality?","['multivariable-calculus', 'stokes-theorem']"
3068004,A hint for the entropy problem-entropy of one discrete variable is greater than the entropy of another one,I need a hint on how to start solving the following problem. Entropy of a discrete variable X is $H(X) = −\sum_{x\in \{x:P(X=x)>0\}}P(X=x)logP(X=x)$ . Let $f:R → R$ be any function.\ a) Show that entropy of a discrete variable X is greater than or equal to entropy of a discrete variable f(X).\ b) Show that equality occurs if and only if function f is injective on {x : P(X = x) > 0},"['entropy', 'probability']"
3068013,The total number of subsets is $2^n$ for $n$ elements,"In my probability book, it says that to count the total number of subsets of n elements is a process of $n$ stages with binary choice of either adding this element to the subset or not to add it. Therefore, the total number is $$2^n$$ But, for instance, we have 3 elements, according to this formula, there are 2 to the power of 3 elements, namely 8, which are $${\emptyset},A,B,C, AB, AC, BC, ABC$$ However, I have a hard time of imagining the process or N stages binary choice that form this many subsets. Can anyone explain/help me to understand it? I mean, ABC, if we are making the choice of A, put it in or do not put it in, exactly which subset are we choosing to put in or not? Thank you.","['permutations', 'combinations', 'probability-theory', 'probability']"
3068031,"Example of $a,~b\in G$ such that $ab\in H\leq G$ and $a^2b^2\notin H.$","Let $G$ be a group and $H$ be a subgroup of $G$ . Let also $a,~b\in G$ such that $ab\in H$ . True or false? $a^2b^2\in H.$ Attempt. I believe the answer is no (i have proved that the statement is true for normal subgroups, but it seems that there is no need to hold for arbitrary subgroups). I was looking for a counterexample in a non abelian group  of small order, such as $S_3$ , or $S_4$ , but i couldn't find a suitable combination of $H\leq S_n$ , $\sigma$ and $\tau\in S_n$ such that $\sigma \tau \in H$ and $\sigma^2 \tau^2 \notin H.$ Thanks in advance for the help.","['permutations', 'group-theory', 'abstract-algebra', 'examples-counterexamples']"
3068035,Hermite polynomial with brownian motion is martingale,"Let $(B_t)_{t\ge 0}$ be a standard brownian motion. I want to show that $(H_n(B_t,t))_{t\ge 0}$ is a martingale, where $$H_n(x,t)=\frac{d^n}{du^n}e^{ux-\frac{u^2}{2}t}\Big|_{u=0},$$ such that the Taylor-formula is $$e^{ux-\frac{u^2}{2}t}=\sum_{n\ge 0}H_n(x,t)\frac{u^n}{n!}$$ How do I calculate $E[|H_n(B_t,t)|]$ ? Is it possible to interchange integration and differentiation? I tried to calculate $H_n(x,t)$ first to find a formula, but I do not get a nice solution. Can someone give me a hint on this? Thanks in advance!","['stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3068049,"In the differential geometric view of Hamiltonian mechanics, why do the phase space coordinates come from the cotangent bundle $T^* Q$?","I have following question about the Hamiltonian mechanics from differential geometrical viewpoint: We start with a physical system parametrized by generalized (position) coordinates $(q^i)$ providing under given restrictions the configuration space $Q$ or more precisely a (smooth) manifold . The tangent bundle $TQ$ over $Q$ provides for the fixed coordinates $q^i$ the corresponding velocities $\dot q ^j$ . Geometrically the velocities $\dot q ^j$ at $q^i$ belong to the tangent space $TQ_{q_0}$ at fixed point $q_0^i$ of $Q$ . So $q^i$ and $\dot q ^j$ provide parameters for the Lagrangian $L[q(t), \dot q(t)]$ , a function on $TQ$ . On the other hand it's known that the Hamiltionian $H[q(t), p(t)]$ can be interpreted as a function on the cotangent bundle $T^*Q$ where for fixed $q_0^i \in Q$ the momenta $(p_0)_j$ are living in cotangent space $T^*Q_{q_0}$ . My question is why (mathematically) the phase space spanned by position $q^i$ and momentum coordinates $p_j$ come from the cotangent bundle $T^*Q$ while the position $q^i$ and velocity coordinates $\dot q_j$ come from tangent bundle ? Or in other words why the cotangent space $T^*Q_{q}$ corresponds to momenta while the tangent space $TQ_{q}$ to velocities from viewpoint of differential geometry? Remark: I know that elemenary physical approach always associates the velocity to the tangent space of the position but this doesn't answer concretely why the momenta spaces should belong exactly to the cotangents. Intuitively I guess that the velocities and momenta should behave differently under transformations in sense of co- and contravariant coordinates but I'm not sure if this is the real reason for the problem above...","['hamilton-jacobi-equation', 'mathematical-physics', 'differential-geometry']"
3068107,Find all polynomials $p(x)$ such that $(p(x))^2 = 1 + x \cdot p(x + 1)$ for all $x\in \mathbb{R}$,"Find all polynomials $p(x)$ such that $(p(x))^2 = 1 + x \cdot p(x + 1)$ for all $x\in \mathbb{R}$ . I assume that $O(p(x)=n)$ (Where O(p) denotes the order of p)
let $p(x)=a_nx^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2} \dots \dots +a_{1}x^{1}+a_{n-1}$ also $a_n \neq 0$ Then we have $a_n^2x^{2n} +\dots +a_0^2=1+a_nx^{n+1}+ \dots a_0x$ since $a_n \neq 0$ we must have $2n=n+1$ and hence $n=1$ hence the polynomial must be linear and hence there doesn't exist any solution(I proved assuming $p(x)=ax+b$ ) Is it correct?","['abstract-algebra', 'polynomials', 'real-analysis']"
3068112,"What is the expected number of randomly generated numbers in the range [a, b] required to reach a sum $\geq X$?","We are generating random numbers (integers) in the range $[a, b]$ . All values are equally likely. We will continue to generate random numbers in this range, and add up successive values until their combined sum is greater than or equal to a set number $X$ . What is the expected number of rolls to reach at least $X$ ? Example: a = 1000
b = 2000
X = 5000

Value 1: 1257 (total sum so far = 1257)
Value 2: 1889 (total sum so far = 3146)
Value 3: 1902 (total sum so far = 5048; all done) So it took $3$ rolls to reach $\geq5000$ . Intuitively, we can say that it will not take more than $5$ rolls if each roll is $1000$ . We can also say that it will not take less than $3$ rolls if each roll was $2000$ . So it stands to reason that in the example above, the expected number of rolls lies somewhere between $3$ and $5$ . How would this be solved in the general case for arbitrary values $[a, b]$ and $X$ ? It's been quite a while since I last took statistics, so I've forgotten how to work with discrete random variables and expected value.","['discrete-mathematics', 'probability', 'random-variables']"
3068113,"Do differential equations correspond to vector fields, or differential forms, or both?","Suppose we have two populations, one of predators, the other of prey. Let $x$ denote the number of prey and $y$ denote the number of predators. Suppose we've chosen to model the rise and fall in population numbers by the Lotka-Voltera equations : $$\frac{dx}{dt} = \alpha x - \beta xy, \qquad \frac{dy}{dt} = -\gamma y + \delta xy$$ It seems to me that there's both a vector field interpretation of these equations, and also a differential forms interpretation. For the differential forms viewpoint, we work on a three dimensional manifold with coordinate axes labelled $x$ , $y$ and $t$ , and we look for submanifolds whose cotangent bundles satisfy the following equations: $$dx = (\alpha x -\beta xy)dt, \qquad dy = (-\gamma y + \delta xy)dt.$$ Another viewpoint is that we work on a two-dimensional manifold with coordinate axes labelled $x$ and $y$ and we look for integral curves of the vector field $$(\alpha x - \beta xy)\frac{\partial}{\partial x} + (-\gamma y + \delta xy) \frac{\partial}{\partial y}$$ Question. Are differential equations vector fields or differential forms, or both? More generally, how does this all work? On Cartesian space, are both viewpoints equally valid? Why are they equivalent? When we work on smooth manifolds, are both viewpoints equally valid? Is there a sense in which they're equivalent, but not canonically equivalent, kind of like how a finite-dimensional vector space is isomorphic to its dual, but not in a canonical way?","['multivariable-calculus', 'differential-topology', 'ordinary-differential-equations', 'differential-geometry']"
3068141,Longest sequence of consecutive integers which are not coprime with $n!$,"For any integer $n$ , the factorial $n!$ is the product of all positive integers up to and including $n$ . Then in the sequence $$n!+2,n!+3,... ,n!+n$$ the first term is divisible by $2$ , the second term is divisible by $3$ , and so on. Thus, this is a sequence of $(n − 1)$ consecutive composite integers, which definitely not coprime with $n!$ . Question: Is this the longest sequence of consecutive integers which are not coprime with $n!$ (less than $n!$ )? On the other words, is $(n-1)$ is the length of the longest sequence of consecutive integers which less than $n$ factorial and not coprime with it? Or can we find longer?","['number-theory', 'factorial', 'elementary-number-theory']"
3068155,How to prove that $\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4}$,"How to prove that $$\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4}?$$ I have encountered this integral recently, and I am aware that one can show how this is true with a substitution $u=\ln{x}$ and then expanding it into a series of integrals of the form $\int_{0}^{\ln{(\sqrt{2}+1)}}ue^{-nu}du$ where each integral is then calculated individually, but it feels pretty brute-forced. Is there any other way to do this? Thanks.","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3068201,Closed form of $\int_0^\infty \sin(x)\sin\left(\frac{1}{x}\right)dx$?,"I have stumbled onto an interesting integral $$\int_0^\infty \sin(x)\sin\left(\frac{1}{x}\right)dx$$ which I noticed graphically that it appears to be $1$ , but I have no idea on how to evaluate it. Maybe it could be done with the use of Bessel Functions? Any help is appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'closed-form', 'bessel-functions']"
3068203,show this inequality with $\sum_{i=1}^{n}a_{i}=n$,"Let $n\ge 3$ be postive  number, $a_{i}>0,i=1,2,\cdots,n$ ,and $\displaystyle\sum_{i=1}^{n}a_{i}=n$ ,show that $$a^3_{1}a_{2}+a^3_{2}a_{3}+\cdots+a^3_{n}a_{1}+n\ge 2(a_{1}a_{2}\cdots a_{n-1}+a_{2}a_{3}\cdots a_{n}+a_{n}a_{1}\cdots a_{n-2})$$ it seem can use indution to prove it.when $n=3$ ,it must prove $$a^3_{1}a_{2}+a^3_{2}a_{3}+a^3_{3}a_{1}+3\ge 2(a_{1}a_{2}+a_{2}a_{3}+a_{3}a_{1})$$ it seem  use three shcur inequaliy $$a^3+b^3+c^3+3abc\ge \sum ab(a+b)$$ then we have $$a^2+b^2+c^2+3(abc)^{2/3}\ge 2(ab+bc+ca)$$","['multivariable-calculus', 'buffalo-way', 'a.m.-g.m.-inequality', 'inequality']"
3068232,"Prove that $\sup S=\sup R,$ if $S\subseteq\Bbb{R}$ is bounded from above and $R\subseteq S$.","Please, is this correct? Let $S$ be a subset of the real numbers bounded from above and let $R\subseteq S$ satisfy the following condition: $\forall\;x\in S,\;\exists\; r\in R\;\text{such that}\;x\leq r$ .
I want to prove that \begin{align} \sup S=\sup R.\end{align} PROOF Let $x\in S$ be arbitrary, then $\exists\; r\in R\;\text{such that}\;x\leq r.$ The number, $r$ , is an upper bound for the set $S.$ So, $\sup S\leq r,\;\text{for some} \; r\in R.$ By definition of $\sup,\;\;y\leq\sup R, \forall\; y\in R.$ In particular, $y=r,\;r\leq\sup R.$ Thus, \begin{align}\tag{1} \sup S\leq r\leq\sup R.\end{align} Since $R\subseteq S$ , we have \begin{align}\tag{2} \sup R\leq \sup S.\end{align} Thus, \begin{align} \sup S=\sup R.\end{align}","['analysis', 'real-analysis']"
3068253,Solving $v-\eta v' = vv''+v'^2$,"Question: How to solve the differential equation $$v - \eta \frac{dv}{d\eta} = v\frac{d^2v}{d\eta^2} + \bigg(\frac{dv}{d\eta}\bigg)^2$$ Attempt: This equation looks quite disgusting to me. I tried writing it in the form $$-\eta^2 \frac{d}{d\eta}\bigg(\frac{v}{\eta}\bigg) = \frac{d}{d\eta}\bigg( v\frac{dv}{d\eta}\bigg)$$ but that doesn't seem to help. Honestly, I am at a loss at how to tackle this equation (not even sure if there is an easy solution for this). Note: This differential equation actually arose as part of my attempt to find a similarity solution for the PDE $$\frac{\partial u}{\partial t} = \frac{\partial}{\partial x}\bigg(u\frac{\partial u}{\partial x}\bigg)$$ Letting $u(x,t) = t^a v(\eta)$ where $\eta = x/t^b$ and $a,b$ are constants to be specified later on, we get \begin{align}
& \frac{\partial u}{\partial t} = \frac{\partial}{\partial x}\bigg(u\frac{\partial u}{\partial x}\bigg)\\
\implies & \frac{\partial u}{\partial t} = u\frac{\partial ^2u}{\partial x^2} + \bigg(\frac{\partial u}{\partial x}\bigg)^2 \\
\implies & at^{a - 1}v - bxt^{a-b-1}v' = t^{2a-2b}vv'' + t^{2a-2b} v'^2 \\
\implies & at^{a - 1}v - b\eta t^{a-1}v' = t^{2a-2b}vv'' + t^{2a-2b} v'^2
\end{align} So we need $a=b=1$ in order for this to be a similarity solution. The equation thus becomes $$v - \eta \frac{dv}{d\eta} = v\frac{d^2v}{d\eta^2} + \bigg(\frac{dv}{d\eta}\bigg)^2$$ which I am unable to solve.","['ordinary-differential-equations', 'partial-differential-equations']"
3068317,Does connected components of a group scheme form a group scheme?,"Let $G$ be a group scheme locally finite type and smooth over a base scheme $S$ , and assume $S$ is normal and integral. Then does the set of (geometrical) connected components of a group scheme form a group? or even point of a group scheme over $S$ ? If $S=Spec k $ where $k$ is a field, then this is true and we have a theory of $\pi_0(G)$ using etale algebras over a field. I wonder what will happen for the generic case.","['general-topology', 'algebraic-geometry', 'algebraic-groups']"
3068318,"Operator norm of $T:l^{2}\rightarrow l^{1}$ where $Tx=(x_{1},x_{2}/2,x_{3}/3,x_{4}/4,...)$","As the title states, I need to compute the operator norm of a linear operator $T:l^{2}\rightarrow l^{1}$ , where $$Tx=\left(x_{1},\frac{x_{2}}{2},\frac{x_{3}}{3},\frac{x_{4}}{4},... \right)$$ Using Holder's inequality for any sequence $(x_{i})_{i\geq 1}\in l^{2}$ , we can show \begin{align}
|Tx|_{1}&=\sum_{i=1}^{\infty}=|x_{1}|+\left|\frac{x_{2}}{2}\right|+\left|\frac{x_{3}}{3}\right|+\cdots\\
&=|x_{1}||1|+|x_{2}|\left|\frac{1}{2}\right|+|x_{3}|\left|\frac{1}{3}\right|+\cdots \\
&\leq \left|x_{i}\right|_{2}\left|\frac{1}{i}\right|_{2} \\
&=\frac{\pi}{\sqrt{6}}|(x_{i})|_{2}
\end{align} Hence $$\displaystyle |Tx|_{1}\leq\frac{\pi}{\sqrt{6}}|(x_{i})|_{2}\implies||T||\leq\frac{\pi}{\sqrt{6}}$$ However, I am unable to find a sequence in $l_{2}$ which has norm $|(x_{i})|\leq 1$ so that I may use the property $||T||=\text{sup}_{|(x_{i})|_{2}=1}|Tx|_{1}$ . Any help is appreciated. Thank you.","['operator-theory', 'normed-spaces', 'linear-algebra', 'functional-analysis']"
3068381,An identity involving binomial coefficients and Stirling numbers of both kinds,"I calculated, using Mathematica, that for $4\leq k \leq 100$ , $$ \sum_{j=k}^{2k} \sum_{i=j+1-k}^j (-1)^j 2^{j-i} \binom{2k}{j} S(j,i) s(i,j+1-k) = 0,$$ where $s(i,j)$ and $S(i,j)$ are Stirling numbers of the first and second kinds, respectively. Here the code: F[k_] := Sum[(-1)^j 2^(j - i) Binomial[2 k, j] StirlingS2[j, 
       i] (StirlingS1[i, j + 1 - k]), {j, k, 2 k}, {i, j - k + 1, j}]; Table[F[k], {k, 4, 100}] How do I prove it holds for all $k \geq 4$ ?","['summation', 'combinatorics', 'stirling-numbers']"
3068414,Function has nth derivatives bounded by exponential,"Does there exist an infinitely differentiable function $f: (0,\infty) \to \mathbb{R}$ , other than a constant multiple of $e^{-x}$ , satisfying $|f^{(n)}(x)| \leq e^{-x}$ for all $n$ , $x$ ? Some counterexamples we have excluded are $e^{-kx}$ for $k \not = 1$ (if $k < 1$ then the inequality fails for large $x$ , if $k > 1$ then the inequality fails for large $n$ ) and the function $\frac{1}{1+e^{x}}$ (the higher derivatives blow up around $x=0$ ). We have tried using the Laplace transform to write $f(x) = \int_0^{\infty} g(t)e^{-tx}dt$ , which shows we should expect higher derivatives to blow up if $f$ is not $e^{-x}$ but since $g$ need not be nonnegative we can't get anything concrete this way.","['derivatives', 'exponential-function', 'real-analysis']"
3068424,Iwahori versus Bruhat decompositions,"I am faced with the following issue that I do not understand but seems contradictory, coming from the book of Roberts and Schmidt about $GSp(4)$ . Consider a local non-archimedean field $F$ , let $p$ be its maximal ideal, $\mathcal{O}$ its ring of integers and $G=GSp(4, F)$ . We are interested in the following Klingen congruence subgroup $$K = 
\left( 
\begin{array}{cccc}
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
p & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
p & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
p & p & p & \mathcal{O}
\end{array}
\right)
$$ (from now on all the subgroups written in this matrix-entries form is meant to be their intersection with $GSp(4, F)$ . I am interested in computing the index of this subgroup in the maximal compact subgroup $K_0$ (where all the entries are integers). Iwahori decomposition We have $$
K = 
\left( 
\begin{array}{cccc}
1 &  & &  \\
p & 1 & &  \\
p &  & 1 &  \\
p & p & p & 1
\end{array}
\right)
\left( 
\begin{array}{cccc}
\mathcal{O}^\times & &  &  \\
 & \mathcal{O} & \mathcal{O} &  \\
 & \mathcal{O} & \mathcal{O} &  \\
 &  &  & \mathcal{O}^\times
\end{array}
\right)
\left( 
\begin{array}{cccc}
1 & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
 & 1 &  & \mathcal{O} \\
 &  & 1 & \mathcal{O} \\
 &  &  & 1 
\end{array}
\right)
$$ so that in particular by decomposing the left subgroup we should obtain $$
\left( 
\begin{array}{cccc}
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O}
\end{array}
\right)
=
\bigsqcup_{a, b, c \in \mathcal{O}/p} 
\left( 
\begin{array}{cccc}
1 &  &  &  \\
a & 1  & &  \\
b & & 1 & \\
c & b & -a & 1
\end{array}
\right)
K
$$ (where the fact that the entries on the right are this way comes from the conditions of belonging to $GSp(4)$ ). So that in particular the index should be, writting $N(p)$ for the norm of $p$ , $$[K_0:K] =N(p)^3$$ Bruhat decomposition On the other hand, introducing the subgroup $$
Q = 
\left( 
\begin{array}{cccc}
\mathcal{O} & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
 & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
 & \mathcal{O} & \mathcal{O} & \mathcal{O} \\
 & & & \mathcal{O}
\end{array}
\right)
$$ the Bruhat decomposition yields that for any field $k$ , $$
GSp(4, k) = Q 
\sqcup Qx 
\left( 
\begin{array}{cccc}
1 & k &  & \\
 & 1 &  & \\
 & & 1 & k \\
 & & & 1
\end{array}
\right)
\sqcup 
Qxy 
\left( 
\begin{array}{cccc}
1 & & k  &  \\
 & 1 & k & k \\
 & & 1 &  \\
 & & & 1
\end{array}
\right)
\sqcup 
Qxyx
\left( 
\begin{array}{cccc}
1 & k & k & k\\
 & 1 &  & k\\
 & & 1 & k \\
 & & & 1
\end{array}
\right)
$$ where the transformations $x$ and $y$ are defined by $$x= 
\left( 
\begin{array}{cccc}
 & 1&  & \\
1 &  &  & \\
 & &  & 1 \\
 & &1 & 
\end{array}
\right)
$$ $$y =
\left( 
\begin{array}{cccc}
1 &  &  &\\
 &  & 1 & \\
 & -1 & &  \\
 & & & 1
\end{array}
\right)
$$ In particular if $k$ is the finite field with $N(p)$ elements, the index we search for is exactly the cardinality of $GSp(4,k) / Q$ , and this one is $(1+N(p))(1+N(p)^2)$ , so that we should say $$[K_0:K] =(1+N(p))(1+N(p)^2)$$ Here is the question following from this discussion: Both results are different, what is
  happening?","['number-theory', 'group-theory', 'proof-verification']"
3068427,What are trivial functions.,"In a book on probability theory there was a statement that: ""Probability function P[x] is a non trivial function of x."" I don't know the meaning of ""non-trivial"" functions? I tried searching on net but there were many problems discussing about non trivial function but nowhere its exact definition was given. I found this desmos link, here I thought that g(x) is a non trivial function of x. But g(x) can be written as g(x)=f(8x)=8x-1. Then whats so special about g(x) here. Kindly help me with the definition of trivial/non-trivial function. And how P[x] is a non-trivial function of x?","['functions', 'soft-question', 'probability']"
3068439,"Find a modified coupling $((X_n,\tilde Y_n))_{n∈ℕ_0}$ with the same coupling time $τ$ and $\tilde Y_n=X_n$ for $n≥τ$ in the coupling lemma","Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(E,\mathcal E)$ be a measurable space $(X_n)_{n\in\mathbb N_0}$ and $(Y_n)_{n\in\mathbb N_0}$ be independent $(E,\mathcal E)$ -valued time-homogeneous Markov chains on $(\Omega,\mathcal A,\operatorname P)$ with common transition kernel $\kappa$ and $$Z_n:=(X_n,Y_n)\;\;\;\text{for }n\in\mathbb N_0$$ $\mathcal F^X$ , $\mathcal F^Y$ and $\mathcal F^Z$ denote the filtraiton generated by $X$ , $Y$ and $Z$ , respectively It's easy to see that $$\tau:=\inf\left\{n\in\mathbb N_0:X_n=Y_n\right\}$$ is an $\mathcal F^Z$ -stopping time and hence $$\tilde Y_n:=1_{\left\{\:n\:<\:\tau\:\right\}}Y_n+1_{\left\{\:n\:\ge\:\tau\:\right\}}X_n\;\;\;\text{for }n\in\mathbb N_0$$ is $\mathcal F^Z$ -adapted. Moreover, $\mathcal F^Z=\mathcal F^X\vee\mathcal F^Y$ . How can we show that $\tilde Y$ is a time-homogeneous Markov chain with the same distribution as $Y$ ? I guess the basic idea is that $Z$ is clearly a time-homogeneous Markov chain with transition kernel $\pi$ satisfying $$\pi((x,y),B_1\times B_2)=\kappa(x,B_1)\kappa(y,B_2)\;\;\;\text{for all }x,y\in E\text{ and }B_i\in\mathcal E\tag1.$$ Since $\mathbb N_0$ is countable, $Z$ is strongly Markovian at $\tau$ and hence $$1_{\left\{\:\tau\:<\:\infty\:\right\}}\operatorname E\left[f\left(\left(Z_{\tau+n}\right)_{n\in\mathbb N_0}\right)\mid\mathcal F_\tau\right]=1_{\left\{\:\tau\:<\:\infty\:\right\}}(\pi f)(Z_\tau)\;\;\;\text{almost surely}\tag2,$$ where $\pi f:=\int\pi(\;\cdot\;,{\rm d}z)f(z)$ , for all bounded and $(\mathcal E\otimes\mathcal E)^{\otimes\mathbb N_0}$ -measurable $f:(E\times E)^{\mathbb N_0}\to\mathbb R$ . So, if $k\in\mathbb N_0$ , $n_0,\ldots,n_k\in\mathbb N_0$ with $0=n_0<\cdots<n_k$ and $B\in\mathcal E^{\otimes k}$ , we obtain \begin{equation}\begin{split}1_{\left\{\:\tau\:<\:\infty\:\right\}}\operatorname P\left[\left(\tilde Y_{\tau+n_1},\ldots,\tilde Y_{\tau+n_k}\right)\in B\mid\mathcal F_\tau\right]&=1_{\left\{\:\tau\:<\:\infty\:\right\}}\bigotimes_{i=1}^k\kappa^{n_i-n_{i-1}}(X_\tau,B)\\&=1_{\left\{\:\tau\:<\:\infty\:\right\}}\operatorname P\left[\left(Y_{\tau+n_1},\ldots,Y_{\tau+n_k}\right)\in B\mid\mathcal F_\tau\right]\end{split}\tag3\end{equation} almost surely. However, it's neither clear to me how we can conclude that $\tilde Y$ is Markovian (with respect to its generated filtration) nor why it has the same distribution as $Y$ . Clearly, the distribution of $Y$ is uniquely determined by the finite-dimensional distributions $\operatorname P\left[\left(Y_{n_1},\ldots,Y_{n_k}\right)\;\cdot\;\right]$ (and the same applies to $\tilde Y$ ). Moreover, we may write $$\operatorname P\left[\left(\tilde Y_{n_1},\ldots,\tilde Y_{n_k}\right)\;\cdot\;\right]=\operatorname P\left[n<\tau,\left(Y_{n_1},\ldots,Y_{n_k}\right)\;\cdot\;\right]+\operatorname P\left[n\ge\tau,\left(X_{n_1},\ldots,X_{n_k}\right)\;\cdot\;\right]\tag4.$$ Many pieces, I'm not able to combine.","['coupling', 'markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory']"
3068444,area of part of Archimedes's spiral,"Find the area of region inside the ""first loop"" of the Archimedes spiral (that is, the spiral for $0 \le \theta \le 2\pi$ ) and to the left of the $y$ -axis. The area the question wants is between $\theta = \pi/2$ and $\theta = 3\pi/2$ for the graph $r=\theta$ . Therefore, I computed the integral $\int_{\pi/2}^{3\pi/2} \theta \,d\theta = \pi^2$ . I even checked it with a graphing calculator to make sure that the integral was computed correctly. However, apparently, this is not a correct answer. Could someone help me see why?","['integration', 'calculus']"
3068482,Boundary of convex subset diffeomorphic to the sphere,"Suppose you have a strictly convex set $\Omega \subset \mathbb{R}^n$ , $n \geq 0$ , bounded, with non-empty interior. Without loss of generality I will assume $0 \in \text{Int} \Omega$ . Thus there exists a clear homeomorphism $x \to \frac{x}{\|x\|}$ between the boundary of $\Omega$ and the sphere. Seems to me that this map is actually a $C^1$ -diffeomorphism, is it true without further assumption ? What if we also assume the boundary of $\Omega$ to be $C^1$ ?","['convex-analysis', 'differential-geometry']"
3068492,Continuous functions vanishing at infinity on a non-locally-compact space,"Let $X$ be a topological space which is not locally-compact (e.g., an infinite-dimensional Hilbert space).
Let $C_{0}(X)$ denote the space of complex-valued, continuous functions vanishing at infinity on $X$ , that is, an element $f\in C_{0}(X)$ is a complex-valued, continuous function on $X$ such that,for every $\epsilon>0$ , there exists a compact $K\subset X$ such that $|f(x)|\leq \epsilon$ outside $K$ .
It is my understanding that $C_{0}(X)$ is a Banach space just as $C_{0}(Y)$ with $Y$ a locally-compact space. However, in the second page of this article , it is stated that $C_{0}(X)$ only contains the zero function when $X$ is not locally-compact , but the statement is not proved.
On the other hand, at the end of the first page of this article , it is said that $C_{0}(X)$ may be very small (which, I guess, means that there are non-vanishing elements in it), but the statement is also not proved.
Furthermore, in the accepted answer of this question , it is given an example of a non-vanishing functions vanishing at infinity on a non-locally compact space, but nothing is said about its continuity. Since I have not a strong background in functional analysis, I really do not know where to start to prove/disprove the previous statements, thus I would appreciate any hint, or any suggestion about references dealing with these matters explicitely (that is, by giving explicit proofs).","['banach-spaces', 'functional-analysis']"
3068499,"Let $A\in M_n(\mathbb{Q})$ with $A^k=I_n$. If $j$ is a positive integer with $\gcd(j,k)=1$, show that $ \operatorname{tr}(A)= \operatorname{tr}(A^j)$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $A\in M_n(\mathbb{Q})$ with $A^k=I_n$ . If $j$ is a positive
     integer with $\gcd(j,k)=1$ , show that $ \operatorname{tr}(A)=
   \operatorname{tr}(A^j)$ . I don't know how to start to prove that. I tried to find the matrix $B$ similar with $A$ , but I am stuck... Thank you.","['matrices', 'trace', 'linear-algebra']"
3068514,A linear map $T: \mathbb{R^3 \to \mathbb{R^3}}$ has a two dimensional invariant subspace.,Let $T: \mathbb{R^3 \to \mathbb{R^3}}$ be an $\mathbb{R}$ -linear map. Then I want to show that $T$ has a $2$ dimensional invariant subspace of $\mathbb{R^3}.$ I considered all possible minimal polynomial of $T$ and applying canonical forms I found some obvious $2$ dimensional invariant subspaces. I stuck when the minimal polynomial is of the form $(X-a)^3$ for some real number $a.$ In this situation since the minimal polynomial and the characteristic polynomial coincides $T$ has a cyclic vector. But I can't complete it further. I need some help. Thanks.,"['jordan-normal-form', 'matrices', 'abstract-algebra', 'linear-algebra', 'linear-transformations']"
3068567,A group with 4 elements,"I am reading ""An Introduction to Algebraic Systems"" by Kazuo Matsuzaka. And there is the following problem. Prove that there exists a group $G$ with $4$ elements which has two elements $\sigma, \tau$ such that $\sigma^2 = \tau^2 = e$ and $\sigma \tau = \tau \sigma$ and $G = \{e, \sigma, \tau, \sigma \tau \}$ , where $e$ is a unit element of $G$ . My 1st solution is the following: Let $G = \{e, a, b, c\}$ . Let $e e = e, e a = a e = a, e b = b e = b, e c = c e = c, a b = b a = c, b c = c b = a, c a = a c = b, a a = e, b b = e, c c = e.$ Then, $e$ is a unit element of $G$ and $e^{-1} = e, a^{-1} = a, b^{-1} = b, c^{-1} = c.$ If we define, $\sigma := a, \tau := b$ , then $\sigma^2 = \tau^2 = e$ and $\sigma \tau = c = \tau \sigma.$ But it is a little troublesome to prove the associative law holds. So I don't like this solution. My 2nd solution is the following: Let $S_n = \{1, 2, \cdots, n\}$ be the symmetric group degree $n$ which is greater than or equal to 4. Let $\sigma := (1 2), \tau := (3 4), e := \mathrm{id}$ . Let $G := \{e, \sigma, \tau, \sigma \tau \}$ . Then, $\sigma^2 = \tau^2 = e$ and $\sigma \tau = \tau \sigma$ . $G$ is a finite set and obviously $G$ is closed under multiplication on $S_n$ . So $G$ is a subgroup of $S_n$ . I don't think this solution is pure because I used $S_n$ . Please tell me other solutions.","['group-theory', 'abstract-algebra', 'finite-groups']"
3068605,Solve a PDE defining a limit cycle of a nonlinear DE,"When trying to identify a limit cycle of a 2nd order nonlinear DE $$\begin{cases}\dot{y}_1=y_2\\\dot{y}_2=-ky_2-\frac{b(a)sin(y_1)}{1+c_1(a)cos(y_1)+c_2(a)sin(y_1)}\end{cases},$$ where $b(a)$ , $c_1(a)$ , and $c_2(a)$ are some coefficients depending on the parameter $a$ ,
I arrived at the following 1st order PDE: $$\frac{\partial V}{\partial y_1}y_2-\frac{\partial V}{\partial y_2}ky_2-\frac{\partial V}{\partial y_2}f(y_1)=0.$$ I wish to find a solution to this equation in the form $V(y_1,y_2)=0$ such that the following conditions are satisfied: $V(y_1+2\pi,y_2)=V(y_1,y_2)$ ; $\frac{dy_2}{dy_1}\bigg|_{y_1=0}=0$ and $\frac{d^2y_2}{dy_1^2}\bigg|_{y_1=0}>0$ ; $\frac{dy_2}{dy_1}\bigg|_{y_1=\pi}=0$ and $\frac{d^2y_2}{dy_1^2}\bigg|_{y_1=\pi}<0$ . Item 1 comes from the fact that the DE is defined on a cylinder, while Items 2 and 3 say that the plot $y_2=y_2(y_1)$ of $V(y_1,y_2)=0$ has a minimum at $y_1=0$ and a maximum at $y_1=\pi$ . The latter conditions were obtained from numerical simulations. Furthermore, we know that $f(y_1+2\pi)=f(y_1)$ and $f(0)=0$ . Question: I wonder whether this problem is well defined and if ""yes"", how to solve it? Indeed, I want to find the limit set $V(y_1,y_2)=0$ since the problem -- as it is stated -- can have only one solution that corresponds to the limit cycle. My attempts. Let $y_1=0$ . If $\frac{\partial V}{\partial y_2}\bigg|_{y_1=0}\neq 0$ , Item 2 along with $f(0)=0$ implies that $y_2=0$ which contradicts the fact that the point $(0,0)$ is an isolated equilibrium. So, I conclude that $$\frac{\partial V}{\partial y_2}\bigg|_{y_1=0}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=0}= 0.$$ Following the same logic I conclude that the same holds for $y_1=\pi$ : $$\frac{\partial V}{\partial y_2}\bigg|_{y_1=\pi}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=\pi}= 0.$$ So, basically, that's it. I tried several candidates for $V(y_1,y_2)$ , but couldn't get any meaningful result. This plot makes me to believe that there is a limit cycle. The red trajectory (approximately) corresponds to the unstable solution of the sadle, two others are just computed for different initial values of $y_2$ .","['dynamical-systems', 'ordinary-differential-equations', 'partial-differential-equations']"
3068674,About a $C^\infty$ extension of a function defined on a closed set (or a $C^\infty$- version of Tietze's extension theorem),"Let a function $f$ defined on a closed subset $F$ of $\mathbf{R}$ which is potentially $C^\infty$ in this sense : to define the notion of potential derivative, let us say that $a\in \mathbf{R}$ is a potential derivative of $f$ at $x_0\in F$ if $f(x) = f(x_0) + a(x-x_0) + o(x-x_0)$ for $x\in F$ ( $a$ may not be unique because $x_0$ might be isolated in $F$ ). Let us say that $g$ is a potential derivative of $f$ if $g(x_0)$ is a potential derivative of $f$ at $x_0$ for all $x\in F$ . Then, a function $f$ is potentially $C^\infty$ on $F$ if there exists a sequence $(g_n)$ such that $f = g_0$ , $g_{n+1}$ is a potential derivative of $g_n$ for all $n\in \mathbf{N}$ .
Remark that the potential derivative is unique if $x_0 \in F$ is not isolated (it is just the limit of the newton difference quotient). This enables for instance to have a Taylor expansion of the function which approximates the function at all order : $f(x) = P_n(x-x_0) + o((x-x_0)^n)$ , where $P_n(X) = \sum_{k=0}^n g_k(x_0) X^k$ EDIT : I am sorry : this is false. So it is needed to suppose its existence, and also the exitence of the expansion of the $f^{(k)}$ . See the Whitney extension theorem for the exact hypothesis needed : https://en.wikipedia.org/wiki/Whitney_extension_theorem . I give a counter-example : $F = \{0\} \cup \cup_{n\in \mathbf{N}} [\frac{1}{4^n}, \frac{2}{4^n}]$ , let for $x\in F$ $\phi(x)$ the least element in the same connex component of $x$ , and put $f(x) = \phi(x)^2 + x$ . $f$ is ""potentially $C^\infty$ "" in the sense I've mentionned, but its ""potential Taylor expansion"" at 0 $f(x) = x$ is not compatible with f at the order 2. Does such a function potentially $C^\infty$ admits a $C^\infty$ extension $\phi$ on $\mathbf{R}$ such that the successive derivatives coincide with whatever potential derivatives $g_n$ fixed ?
It is well known, by Tietze's extension theorem, that a continuous function on a closed set admits a continuous extension. But what about a $C^\infty$ extension ? I have also the same question by replacing $\mathbf{R}$ by $\mathbf{R}^n$ , $\mathbf{R}^m$ with the natural definition of potential differentiation ; let $f$ a function $F \rightarrow R^m$ defined on $F$ a closed set of $\mathbf{R}^n$ . $f$ is said to be potentially differentiable at $x_0 \in F$ if there exist an endomorphism $u$ from $\mathbf{R}^n$ to $\mathbf{R}^m$ such that for $x\in F$ , $f(x) = f(x_0) + u(x-x_0) + o(x-x_0)$ . Then $u$ is said to be a differential of $f$ on $x_0$ . A function $u$ from $f$ to the set of endomorphisms from $\mathbf{R}^n$ to $\mathbf{R}^n$ is said to be a potential differential of $f$ if for all $x_0 \in F$ , $u(x_0)$ is a differential of $f$ at $x_0$ .
Then, $f$ is potentially $C^\infty$ if there exist $(u_n)$ such that $f = u_0$ and $u_{n+1}$ is a potential differential of $u_n$ for all $n\in \mathbf{N}$ . Then, the sequence $(u_n)$ is said to be a sequence of potential iterated differentials of $f$ . There is no unicity of the potential differential because $F$ might lack some directions. Nevertheless, it is possible to check that theese definitions enable to have an analogous Taylor expansion of the function $f$ . EDIT : this is again false. Does a potentially $C^\infty$ function in this sense admits an extension $\phi$ $C^\infty$ such that the successive differentials coincide with whatever potential iterated differentials fixed $(u_n)$ ? EDIT : I .
For a good notion of ""potential differentiability"", you need to suppose the existence of taylor expansions, which is not automatic.","['general-topology', 'real-analysis']"
3068684,A simple integral with one question,"Question is: For $x$ equals $4$ and $9$ , why is $t$ not $\pm2$ and $\pm3$ but just $2$ and $3$ ?",['integration']
3068691,Minimum value of $\frac{4}{4-x^2}+\frac{9}{9-y^2}$,"Given $x,y \in (-2,2)$ and $xy=-1$ Minimum value of $$f(x,y)=\frac{4}{4-x^2}+\frac{9}{9-y^2}$$ My try: Converting the function into single variable we get: $$g(x)=\frac{4}{4-x^2}+\frac{9x^2}{9x^2-1}$$ $$g(x)=\frac{4}{4-x^2}+1+\frac{1}{9x^2-1}$$ Using Differentiation we get: $$g'(x)=\frac{8x}{(4-x^2)^2}-\frac{18x}{(9x^2-1)^2}$$ $$g'(x)=2x\left(\frac{4(9x^2-1)^2-9(4-x^2)^2}{(4-x^2)^2(9x^2-1)^2}\right)$$ $$g'(x)=70x\frac{9x^4-4}{(4-x^2)^2(9x^2-1)^2}=0$$ So the critical points are: $x=0, x=\pm \sqrt{\frac{2}{3}}$ But $x \ne 0$ since $xy=-1$ $$g'(x)=70x \frac{(3x^2-2)(3x^2+2)}{(4-x^2)^2(9x^2-1)^2}$$ By using derivative test we get Minimum occurs when $x=\pm \sqrt{\frac{2}{3}}$ Hence $$x^2=\frac{2}{3}, y^2=\frac{3}{2}$$ Min value is $$\frac{4}{4-\frac{2}{3}}+\frac{9}{9-\frac{3}{2}}=\frac{12}{5}$$ Is there any other approach?","['maxima-minima', 'algebra-precalculus', 'derivatives']"
3068712,Is there math with non-commutative multiplication of real numbers?,"I'm wondering is there a math with non-commutative multiplication of real numbers. For example, we could define operator ⊗ for $ n, m ≥ 0$ : $$ n⊗ m = n\times m  $$ $$ n⊗ (-m) = n\times m  $$ $$ -n⊗ m = -(n\times m) $$ $$ -n⊗ (-m) = -(n\times m) $$ Or we can choose some other rules of multiplication. How could this math be applied?","['group-theory', 'noncommutative-algebra']"
3068732,Algorithm for finding the smallest integer that satisfies several modular congruence conditions?,"my first question! I work a lot with numbers (finance) but very much an amateur mathematician - please be gentle. I have the following problem that has come from discussions about cryptography: Given a set of prime numbers $p_1, p_2,...,p_n$ and sets $S_{p_1},S_{p_2},..., S_{p_n}$ containing a selection of modular residue classes with respect to each prime, find the smallest positive integer $q$ that is congruent to a residue class in each $S_i, i = 1,...,n$ . Example: $p_1 = 7$ , $p_2 = 11$ , $p_3 = 13 $ $S_1 = \{[0]_7,[1]_7,[6]_7\}$ $S_2 = \{[1]_{11},[5]_{11},[6]_{11},[10]_{11}\}$ $S_3 = \{[2]_{13},[5]_{13},[8]_{13},[11]_{13}\}$ By the Chinese Remainder Theorem (CRT) there will always be a solution if the $S_i$ s are non-empty. A brute force approach is to take all combinations of elements from the $S_i$ s, apply the CRT to each one in turn and keep track of the smallest. For example, if we take the first elements from each of the sets above, we can use CRT to solve the following system of equations for q: $$
q \equiv 0 \mod 7 \\
q \equiv 1 \mod 11 \\
q \equiv 2 \mod 13
$$ In this case q = 210. Repeating this process for all the other combinations, we can establish that the smallest possible q is actually 21 ( $\equiv [0]_7 \equiv [10]_{11} \equiv [8]_{13} $ ) Of course, this is fine to do when the number of primes and the size of the $S_i$ s is small, but blows up quite quickly if they get larger. I'm interested in establishing whether a better algorithm could do this more efficiently and ultimately what is the complexity of this problem in a computational sense. Any smart ideas? Literature recommendations or theorems in particular that might point me in the right direction would be most appreciated! Edit: @Yong Hao Ng gives a good improvement on brute force in his answer below, but I think the problem still blows up subexponentially as the number of primes grows, assuming that the number of residue classes for each prime p is roughly p/2. I'm wondering if that's anyone who can improve on this, or indeed suggest why you can't improve on this!","['modular-arithmetic', 'number-theory', 'chinese-remainder-theorem', 'computational-complexity', 'prime-numbers']"
