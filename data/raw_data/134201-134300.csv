question_id,title,body,tags
2112004,Prove the solution set to homogeneous equations with this form,"Let a homogeneous system be given by: $$\left(\begin{matrix}
0    & \pm1 & \pm1 & \ldots & \pm1 \\
\pm1 & 0    & \pm1 & \ldots & \pm1 \\
\pm1 & \pm1 & 0    & \ldots & \pm1 \\
&&\ldots \\
\pm1 & \pm1 & \pm1    & \ldots & 0 \\
\end{matrix}\right){\bf{x}}={\bf{0}}$$ Where this matrix is $n\times n$, where $n$ is odd, and the sum of each row is zero - that is, there are exactly $(n-1)/2$ ones, $(n-1)/2$ minus ones, and one zero in each row. I wish to prove that the solution set is the set of vectors $\bf{x}$ with all elements of $\bf{x}$ being equal.
Is there a nice matrix based way of proving this? I think this may be proved using induction, but a non-inductive method would be preferable.","['matrices', 'linear-algebra', 'systems-of-equations']"
2112012,Why the slope of $e^x$ at any point is it's value at that point?,I know that we can differentiate $e^x$ by using limits. But why is it that it is the derivative of it's own? Or is it the case that there must exist a function which should be it's own derivative and we have defined it to be $e^x$?,"['derivatives', 'intuition', 'exponential-function', 'calculus']"
2112065,"Why is $\text{dim ker}(T) = 0$ a requirement for invertibility of a finite dimensional operator, but not for an infinite dimensional operator?","I have read that Fredholm operators generalize the notion of invertibility, and that all finite dimensional operators are Fredholm with index 0. Also, if you have a Fredholm operator with index zero, then it is surjective if and only if it is injective. So if a Fredholm operator has index 0 and is injective then it will be invertible. So consider the finite dimensional operator $T: \mathbb{R}^n \to \mathbb{R}^n$ given by the matrix $A$ as follows: $$
A =
\begin{bmatrix}
1 & 0 & 0 & \dots & \dots & 0 \\
0 & 1 & 0 & \dots & \dots & 0 \\
0 & 0 & 1 & \dots & \dots & 0 \\
0 & 0 & 1 & \dots & \dots & 0 \\
\vdots & \dots & \ddots & \ddots & 0 & 0 \\
\vdots & \dots & \dots & 0 & 1 & 0 \\
0 & 0 & \dots & 0 & 0 & 0 \\
\end{bmatrix}.
$$ That is, the elements of the diagonal of $A$ are $1$, except for the final diagonal element which is $0$. $A$ has a kernel and cokernel (they are the same) both with dimension $1$ so it is Fredholm with index zero. However it is not injective and therefore not invertible. It seems that for any finite dimensional operator (matrix) I choose with non-trivial kernel, I will never have an injective operator as it can always be reduced to row echelon form in which it will have one or more rows that are all zero. So the only way an operator $T$ can be invertible in the finite dimensional case is if it has $\text{dim ker}(T) = \text{dim (coker}(T)) = 0$. On the other hand, from reading about infinite dimensional operators, it seems invertibility is often shown by first showing the operator is Fredholm with index 0, and then showing it is injective. I don't understand why having $\text{dim ker}(T) \neq 0$ prevents invertibility in the finite dimensional case, yet it doesn't seem to be an issue in the infinite dimensional case? For a finite dimensional operator with a non-trivial kernel we can never have injectivity, so how is it possible that an infinte dimensional operator with non-trivial kernel can be injective?","['functional-analysis', 'operator-theory', 'linear-transformations']"
2112077,Series shown up in Quantum Mechanics,"Here is the series, by some concepts in quantum mechanics, it should be 1, however, I don't know how to prove it. Could someone show me the proof? Thanks.
$$\lim_{k\to \infty}\sum_{n=1}^{k} \frac{4}{n^2\pi^2}(1-\cos\frac{n\pi}{2})^2
$$","['sequences-and-series', 'calculus', 'limits']"
2112108,Prove a convex and concave function can have at most 2 solutions,"Let $ a,b \in \mathbb{R}$ Let $f(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly convex in $ (a, b) $ and $g(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly concave in $ (a, b) $ How can I prove the intersection of $ f $ and $ g $ can have a maximum of two roots $f(x)-g(x)=0 $ ?","['derivatives', 'proof-writing', 'roots']"
2112138,Can one find a set of four positive integers in which the product of any two distinct integers is a perfect square plus one?,"This post is motivated by a famous IMO problem . It implies $\{2,5,13,x\}$ ($x$ denotes a positive integer) is not a set having this property. I am wondering if there really exists a set $\{a,b,c,d\}$ (each element is a positive integer) such that $ab-1$, $ac-1$, $ad-1$, $bc-1$, $bd-1$, $cd-1$ are all perfect squares. Could you help me?","['number-theory', 'square-numbers']"
2112140,Special points in the Jacobian of a hyperelliptic curve under Frobenius,"Good evening. Let $H/\overline{\mathbb{F}}_q$ be a hyperelliptic curve of genus 2 with a rational Weierstrass point and let $J$ be its jacobian, if $\phi\in \text{End}(J)$ is the $q$-Frobenius. I am trying to calculate the cardinality of the following set: $Z := \lbrace P\in H(\overline{\mathbb{F}}_q):[P-\infty]\oplus [P^\phi - \infty]\sim 0$ or $ [R-\infty] \rbrace$ for some $R\in H(\overline{\mathbb{F}}_q)$. Is easy to see that all the points $Q\in Twist^2(H)$ because $Q^\phi = \iota Q$, also the $\mathbb{F}_q$-rational $2$ and $3$ torsion points on the Jacobian of the form $D:= [T-\infty]$  as $2D=0$ and in case of being 3 torsion we have that $3D=[\iota P-\infty]$. So here we have that $\#Z \geq \#Twist^2(H)(\mathbb{F}_q) + \#W + \#T=2(q+1) - \#H(\mathbb{F}_q)+\#W+\#T$ Where $W$ are the $\mathbb{F}_q$-rational Weierstrass points on $H$ and $T$ are the three torsion points given by prime divisors. Are there more?, I don't know if Frobenius can do more Weird stuff in different extensions. It will be nice to know if there is something to study related to sums of elements in the same orbits by some action in abelian varieties Thanks","['finite-fields', 'cryptography', 'elliptic-curves', 'algebraic-geometry']"
2112146,Breaking Banach's Fixed Point Theorem,"In trying to see how Banach's fixed point theorem would break down in an incomplete space, I tried to come up with an example of a function: $f: \mathbb{Q} \longrightarrow  \mathbb{Q} \ \ $such that $ \forall x,y \in \mathbb{Q}$ our function is contracted with factor $\frac12$ and has no fixed points. i.e. $$ \left | {f(x)-f(y)} \right |\leq \frac12 \left | x-y \right | ,\\ \nexists \ \  x^* \in \mathbb{Q}  \ \ s.t. \  f(x^*)=x^* \ $$ 
Starting with $f(x)=x^2-2$ and using Newton-Raphson I was able to construct the following function which works $$ f(x) := \left\{\begin{array}{lr}
        \frac12 (x-1) + \frac32, & \text{for }  x < 1\\
        \frac12- x^{-2}, & \text{for } 1\leq x\leq 2\\
        \frac12 (x-2) + \frac32, & \text{for } x>2
        \end{array}\right\}$$ That led me to thinking if there was a bijection which satisfied the same criteria. I think I can see why one could exist but am not sure how to prove that one necessarily exists. I am also keen on an explicit example of a bijection that works.","['examples-counterexamples', 'banach-fixed-point', 'analysis']"
2112161,Explain why $(a−b)^2 = a^2 −b^2$ if and only if $b = 0$ or $b = a$.,"This is a question out of ""Precalculus: A Prelude to Calculus"" second edition by Sheldon Axler. on page 19 problem number 54. The problem is Explain why $(a−b)^2 = a^2 −b^2 $ if and only if $b = 0$ or $b = a$. So I started by expanding $(a−b)^2$ to $(a−b)^2 = (a-b)(a-b) = a^2 -2ab +b^2$. To Prove that $(a−b)^2 = a^2 −b^2 $ if b = 0 I substituted b with zero both in the expanded expression and the original simplified and I got $(a−b)^2 = (a-0)^2 = (a-0)(a-0) = a^2 - a(0)-a(0)+0^2 = a^2$ and the same with $a^2 -2ab +b^2$ which resulted in $a^2 - 2a(0) + 0^2 = 2a$ or if I do not substite the $b^2$ I end up with $a^2 + b^2$. That's what I got when I try to prove the expression true for $b=0$. As for the part where $b=a$, $(a−b)^2 =  (a-b)(a-b) = a^2-2ab+b^2$, if a and b are equal, let $a=b=x$ and I substite $a^2-2ab+b^2 = x^2-2(x)(x) + x^2 = x^2-2x^2+x^2 = 1-2+1=0$ I do not see where any of this can be reduced to $a^2-b^2$ unless that equals zero......I do see where it holds but I do not see how would a solution writting out look.After typing this it seems a lot clearer but I just can't see how to phrase a ""solution"". P.S: This is my first time asking a question here so whatever I did wrong I am sorry in advance and appreciate the feedback.",['algebra-precalculus']
2112165,Cover for Cech cohomology of the constant sheaf $\mathbb{Z}$ over $\mathbb{P}^n$,"First of all consider $\mathbb{P}^n$ as a complex analytic manifold. In Griffiths and Harris's Principles of Algebraic geometry p.145 it is stated
$$
H^2 (\mathbb{P}^n, \bf{\mathbb{Z}}) \cong \mathbb{Z},
$$
that is, the second Cech cohomology group of $\mathbb{P}^n$ over the constant sheaf $\mathbb{Z}$ is isomorphic to $\mathbb{Z}$. I wanted to check this using the usual algebraic cover of open sets: $\mathcal{U} = \{ U_i \}_{0 \le i \le n}$, where $U_i = \{ x_i \neq 0 \}$, but already for $\mathbb{P}^1$ this fails, meaning that $\mathcal{U}$ is not fine enough for computing Cech cohomology. Edit: As Rene remarked in the answer below, there is a theorem of Leray which states that given a sheaf $\mathcal{F}$ and a cover $\mathcal{U}= \{ U_i \}$ such that
$$
H^p(U_{i_0} \cap \dotsb \cap U_{i_k}, \mathcal{F}) = 0
$$
for all finite intersections of $\mathcal{U}$ and all $p$, then
$$
H^p(\mathcal{U},\mathcal{F}) \rightarrow H^p(X, \mathcal{F})
$$
is an isomorphism for all $p$. This still gives rise to questions: Is there a standard, or at least well known, cover of $\mathbb{P}^n$ satisfying this condition? Is there any intuition on how to choose such covers? Is there any other direct method of seeing the isomorphism $H^2 (\mathbb{P}^n, \bf{\mathbb{Z}}) \cong \mathbb{Z}$ ?","['homology-cohomology', 'complex-geometry', 'algebraic-geometry', 'sheaf-cohomology', 'algebraic-topology']"
2112177,How to evaluate $\int_{0}^{1}\frac{\log x}{1+x} dx$,Today When I was computing measure theoretic entropy of Gauss map I encountered this integral. Then I check asked Questions but I couldn't find same question  how to evaluate: $$\int_{0}^{1}\frac{\log x}{1+x} dx$$ Thanks for any hint.,['integration']
2112188,Proof of Killing's Equation,"The problem: I am trying to prove that, for a Riemannian manifold $(M,\langle\ , \ \rangle)$, $X \in \Gamma(TM)$ is a Killing field (i.e. one for which $\langle u,v \rangle_{p \in M} = \langle (d \phi^X_{t})_{\phi^{X}(t, p)} (u), (d \phi^X_{t})_{\phi^{X}(t, p)} (v)  \rangle_{\phi^{X}(t, p)}$, where $\phi^{X}_{t} = \phi^{X}(t,) $ is the local flow of $X$) if $\forall Y,Z \in \Gamma(TM)$ $$ \langle \nabla_Y X, Z \rangle + \langle \nabla_Z X, Y \rangle = 0, $$
where $\nabla$ is the Levi-Civita connection. My work so far: By using the compatibility and torsion-freeness of the connection, it is easy to reduce this assumption to $$ X \langle Y,Z \rangle = \langle [X,Z],Y \rangle + \langle [X,Y],Z \rangle. $$
Then, if we choose $Y,Z$ such that $Y(p) = u$, $Z(p) = v$, and remembering that $X(p) = \frac{d}{dt}\Big|_{t=0} \phi^{X}(t,p)$, our LHS evaluated at $p$ becomes $$ \frac{d}{dt} \Big|_{t=0} \langle Y(\phi^{X}(t, p)), Z(\phi^{X}(t, p)) \rangle_{\phi^{X}(t, p)}. $$ Our RHS at $p$ becomes $$ \langle [X,Z](p),Y(p) \rangle_p + \langle [X,Y](p),Z(p) \rangle_p . $$
At this point I start looking at the definition of the Lie bracket: $$ [X,Y](p) = \frac{d}{dt}\Big|_{t=0} d \phi^X_{-t}(Y(\phi^X(t,p))), $$ so our RHS becomes $$ \langle \frac{d}{dt}\Big|_{t=0} d \phi^X_{-t}(Z(\phi^X(t,p))), Y(p) \rangle_p + \langle \frac{d}{dt}\Big|_{t=0} d \phi^X_{-t}(Y(\phi^X(t,p))) , Z(p) \rangle_p,$$ but I am having a hard time completing the proof. My hunch is that we want to show the RHS at $p$ is zero (I could be wrong here). Any hints or pointers would be very welcome. Apologies if my notation is a bit untidy. There is also probably a much easier way to do this, so any hints away from my solution are also welcome.","['isometry', 'lie-derivative', 'riemannian-geometry', 'differential-geometry']"
2112221,Non-commutative formal groups- proof of non-commutativity,"I'm studying Hazewinkel's ""Formal Groups"" book, and example of a non-commutative group law is given by: on the ring $\mathbb{F}_p[c]/(c^2)$ we define
$$F(X,Y)=X+Y+cXY^p.$$
I see how to prove everything except non-commutativity- my problem is I tried to find example where it fails for $p=3$ and I keep getting that this is commutative (I am making mistake somewhere): I think that elements in $\mathbb{F}_3[c]/(c^2)$  can be seen as polynomials of degree $1$ or $0$, ie. elements are of form $a\cdot c+b$. Also, since characteristic of ring is $3$ we have $(a\cdot c+b)^3=a^3c^3+b^3=b$ so if $F(X,Y)=F(Y,X)$ than since $X+Y=Y+X$ we must have $cXY^p=cYX^p$ Let $X=ac+b$ and $Y=Ac+B$ then $cXY^3=c(ac+b)(Ac+B)^3=$(step 2)$=c(ac+b)B=bBc$ and $cYX^3=c(Ac+B)(ac+b)^3=$(step 2)$=c(Ac+B)b=bBc$ which leads to conclusion it is commutative. Which steps are wrong?","['finite-fields', 'finite-groups', 'abstract-algebra', 'elliptic-curves', 'group-theory']"
2112319,What is the most efficient series to calculate sine?,"What formula is used to calculate sine in modern computers? Is Taylor formula the best? What formulas converge faster, especially out of $2\pi$ range?","['numerical-methods', 'taylor-expansion', 'trigonometry']"
2112322,A new characterization of an annulus in the plane?,Let $K$ a connected compact subset of the Euclidean plane which has an infinite set of reflection symmetries. Does this imply that $K$ is an annulus ? Source : les dattes à Dattier,"['general-topology', 'recreational-mathematics', 'real-analysis', 'geometry']"
2112332,Finding a Galois extension of $\Bbb Q$ of degree $3$,"I want to find a Galois extension $K/\mathbb{Q}$ such that $[K:\mathbb{Q}]=3$. I thought about this for a while, but haven't been able to come up with one yet. What I tried so far: (i) Taking a separable polynomial $f\in\mathbb{Q}[x]$ of degree three and considering its splitting field. (ii) Looking at the splitting fields of primitive roots of unity. The second one doesn't work because the splitting field over such a root has as degree a value in the range of Euler's totient function, and this doesn't contain three. The first approach also didn't work. I tried polynomials of the form $(x-\sqrt{p})(x-\sqrt{q})(x-\sqrt{r})$ for primes, but those have degree $8$. I then tried 'third roots' $\alpha$, but the minimal polynomials of those have complex as well as real roots, so the simple extensions $K(\alpha)$ aren't normal unless they're trivial. Could anyone please give me a hint on what else to try.","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2112338,Objects are shuffled. What is the probability that exactly one object remains in its original position?,"We have a deck with $n$ cards enumerated $1,2,\ldots,n$. The deck is shuffled. What is the probability of exactly one card to remain on its original position? What is the limit as $n$ rises to infinity? $$
\begin{array}{rcl}
\{1\} & : & \dfrac 11 \\[6pt]
\{12,21\} & : & \dfrac 02 \\[6pt]
\{123,132,213,231,312,321\} & : & \dfrac 36 \\[6pt]
& \vdots
\end{array}
$$ At $n = 100 0$ and $10 000$ trials: $$
\begin{array}{rcl}
\text{value of }n & & \text{probability} \\
\hline
1000 & & 0.3739 \\
1001 & & 0.3689 \\
1002 & & 0.3722 \\
1003 & & 0.3638 \\
1004 & & 0.3707 \\
1005 & & 0.3664 \\
1006 & & 0.3616 \\
1007 & & 0.3728 \\
1008 & & 0.3702 \\
1009 & & 0.3801
\end{array}
$$ At $n = 100 000$ and $10 000$ trials: $\text{value of } n \quad \text{probability}$ $\quad 100000	\quad \quad 0.3659$ $\quad 100001	\quad \quad 0.3552$ $\quad 100002	\quad \quad 0.356$ $\quad 100003	\quad \quad 0.367$ $\quad 100004	\quad \quad 0.3738$ $\quad 100005	\quad \quad 0.3647$ $\quad 100006	\quad \quad 0.3654$ $\quad 100007	\quad \quad 0.3637$ $\quad 100008	\quad \quad 0.3718$ $\quad 100009	\quad \quad 0.3708$ Apparently, probability approaches $0.36-0.38$, but how can one derive it analytically?",['combinatorics']
2112360,"Prove that $x_1,x_2,x_3$ are all integers","This result has been disproven. Let $x_1,x_2,x_3,x_4$ be an arithmetic progression and suppose that $$x_1^3, \quad x_1^3 + x_2^3, \quad x_1^3 + x_2^3 + x_3^3, \quad x_1^3 + x_2^3 + x_3^3+x_4^4$$ are perfect squares. Prove that $x_1,x_2,x_3,x_4$ are all integers. Since these numbers are in arithmetic progression, we must have $x_1,x_1+d,x_1+2d,x_1+3d$ are our four terms. Also, $x_1$ must either be a perfect square, or the cubic root of a perfect square and $x_2,x_3$ must be integers or cubic roots. I thought about proving the latter is impossible by showing that the distance between the cubic roots of integers is unique ($\sqrt[3]{a}-\sqrt[3]b)$, so that $x_4^3$ can't be an integer. How do we continue from here?",['number-theory']
2112369,Find a value such that linear system has no solution,"Okay, so this is probably an elementary level question, but I am going to ask anyways since I cannot figure out what to do next. Given the system of equations: x + 2y - 5z = 1 x + y + 4z = 1 4x + 10y + kz = 5 Find a k such that there is no solution to the system. 
How I started on the problem: From looking at the first two equation I figured out that y=9z . I am not sure what to do with this information. My reasoning could be wrong but I feel that there would be many k such that there would be no solution and only one that there would be a solution. Am I wrong in my reasoning? I also know that if I could get a row in the matrix that was untrue there would be no solution.","['matrices', 'linear-algebra', 'systems-of-equations']"
2112375,Is there a meaningful example of probability of $\frac1\pi$?,"A large portion of combinatorics cases have probabilities of $\frac1e$. Secretary problem is one of such examples. Excluding trivial cases (a variable has a uniform distribution over $(0,\pi)$ - what is the probability for the value to be below $1$?). I can't recall any example where $\frac1\pi$ would be a solution to a probability problem. Are there ""meaningful"" probability theory case with probability approaching $\frac1\pi$?","['combinatorics', 'probability']"
2112428,Showing a Mobuis strip is a surface,"A Mobius strip is a surface which is diffeomorphic to the surface $S$ defined
below. Let $α : R → R^{3}$ be defined as $α(t) = (\cos{t},\sin{t}, 0)$. Let $ψ : R × (−1, 1) → R^{3}$ be defined as $ψ(t, s) = 5α(t) + s(\cos(\frac {t}{2})(0, 0, 1) +  \sin(\frac {t}{2})α(t))$. $(5\cos{t},5\sin{t}, 0) + s \sin(\frac {t}{2}) (\cos{t},\sin{t}, 0) + s \cos(\frac {t}{2})(0, 0, 1) $ In this form its allittle easier to view this is cylindrical coordinates. With some help i have finally been able to visualize this as a circle of radius 5 with a line segment on the end of radius of the circle moving around the entire circle while completing a flip of the line segment with one rotation about the axis. 
s appears to control the length of this line segment and seems to change the orientation of the line segment? as it transitions through s=0. Sketch the image $S$ of $ψ$.
Mobius band turning counter clockwise when s is negative clockwise when s is positive. appears to be a half turn each way. when s=0 we got a circle of radius 5. Show that S is a $C^{\infty}$ in $ \mathbb{R^{3}}$ i think i need to do this in a range from 0 to $\pi$ and $ \pi $ to 2 $ \pi $
where the area is s 2.5 $ \pi $ for each half? Bounty Award:
I would like an approach to show its a $C^{\infty}$ surface preferably by using patchs not the IFT any ideas how to set this up is sufficient for bounty. I am given in the Question that this is diffemorphic to mobius strip. Can i somehow use that and the fact that a mobius strip is a surface to show that this if a surface?","['general-topology', 'differential-geometry']"
2112442,A cubic nonlinear Euler sum,"Any idea how to solve the following Euler sum $$\sum_{n=1}^\infty \left( \frac{H_n}{n+1}\right)^3 =
 -\frac{33}{16}\zeta(6)+2\zeta(3)^2$$ I think It can be solved it using contour integration but I am interested in solutions using real methods.","['euler-sums', 'harmonic-numbers', 'sequences-and-series']"
2112457,Proving that $m(\{f^{-1}(S) : S \in A\}) = \{f^{-1}(S) : S \in m(A)\}$ (monotone classes),"Let $X,Y$ be sets, $A \subseteq \mathcal{P}(Y)$ and $f: X \to Y$. Then $$m(\{f^{-1}(S) : S \in A\}) = \{f^{-1}(S) : S \in
 m(A)\}$$ Where $m(A)$ denotes the monotone class generated by $A$. For proving the inclusion $\subseteq$ it is enough to show that $$\{f^{-1}(S) : S \in
 m(A)\}$$ is a monotone class. Hence we take an increaing sequence $(f^{-1}(S_n))_{n \in \mathbb{N}}$ where $S_n \in m(A)$ an look at its union. I mean trivially $$\bigcup_{n \in \mathbb{N}}f^{-1}(S_n) = f^{-1}\left( \bigcup_{n \in \mathbb{N}}S_n\right)$$ So if we could show $\bigcup_{n \in \mathbb{N}}S_n \in m(A)$ we are done. For this $(S_n)_{n \in \mathbb{N}}$ must be increasing, which is not per se the case, isn't it? I asked the one which wrote this exercise and he replied that it is the same as for $\sigma(A)$ ($\sigma$-algebra generated by $A$) no further assumptions on the nature of $f$ have to be made. But I mean, $\sigma$-algebras are a richer structure than monotone classes, hence it could be that we have to assert more in this case. My assumption would be that $f$ has to be surjective. So my question is: Is this assumption necessary or is it right that it is like in the case of $\sigma$-algebras? Does the proof work nevertheless and I do not see it?",['measure-theory']
2112506,"Exercise 15, chapter 9 of O'Neill's ""Semi-Riemannian geometry""","I am stuck with the following exercise (exercise 15, chapter 9) in O'Neill's book on semi-Riemannian geometry: ""Let $M$ be a complete and connected semi-Riemannian manifold of dimension $n$. Show that the following statements are equivalent: 1) The isometry group of $M$ has dimension $n(n+1)/2$. 2) The algebra of Killing vector fields of $M$ has dimension $n(n+1)/2$. 3) Given any two points $p,q\in M$ and any linear isometry $\Lambda:T_pM\to T_q M$, there exists an isometry $\sigma:M\to M$ such that $\sigma(p)=q$ and $d\sigma_p=\Lambda$."" I have no problem in showing 1) $\Rightarrow$ 2) and 3) $\Rightarrow$ 1), but I cannot show 2) $\Rightarrow$ 3). More specifically: given 2), it is clear to me that 3) with $p=q$ will hold whenever $\Lambda$ is connected to the identity, but I cannot see why it should hold also for $\Lambda$ not connected to the identity. Any ideas?","['differential-geometry', 'general-relativity']"
2112514,Simple Homogeneous ODE,$$y'-\frac{1-2x}{y}=1$$ If we look at the homogeneous part can we use $y_{h}=e^{-\int p(x)dx}$ or because it is in the form of $y'+p(x)y^{-1}=0$ we can not?,['ordinary-differential-equations']
2112553,Statistical significance of linear least squares,"Consider two data series, $X = (x_1, x_2, \dots, x_n)$ and $Y = (y_1, y_2, \dots, y_n),$ both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without fitting any intercept), as in $Y = aX + \epsilon$ where $\epsilon$ denotes a series of error terms. It can be shown that $a=\frac{\rho_{XY} \sigma_Y}{\sigma_X}$. Suppose that $\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically significantly different from $0$ at the $95\%$ level if: i. $n=10^2$ ii. $n = 10^3$ iii. $n = 10^4$ I know that I need to find a $p$-value for each of these, and I assume that it will be in turns of $\rho$ and $n$. However, everything I attempt leaves a lingering standard deviation somewhere from the definition of $a$. How do you test significance of a least squares in this case?","['statistics', 'least-squares']"
2112576,How to define a covariant derivative on a smooth vector bundle using only an Ehresmann connection?,"I'm trying to get a clear picture of covariant differentiation in my head. I'm looking for a definition of the covariant derivative using only the structure of an Ehresmann connection on a smooth vector bundle. The data of an Ehresmann connection on any submersion can be specified in the three usual equivalent ways of specifying a splitting of a short exact sequence: If $f:X\to Y$ is a submersion, there's the exact short sequence of Atiyah, of bundles over $X$ $$\mathrm VX\to \mathrm TX\to f^\ast \mathrm TY.$$ If we take a right splitting $\nabla:f^\ast \mathrm TY\to \mathrm TX$, the only new possibility seems to horizontally lift vector fields. How to define a covariant derivative on a smooth vector bundle $f:X\to Y$ using only an Ehresmann connection? Update following levap's great answer. If I understand correctly, here's the diagram describing the maps of levap's answer. $$\newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex}
\newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}}
\begin{array}{c}
\mathrm T Y & \ra{\mathrm d s} & \mathrm T X & \ra{K} & \mathrm VX & \ra{\Phi} & X\times _YX & \ra{\pi_2} & X\\
\da{} & & \da{} & & \da{} & & \da{\pi_1} & & \da{f}\\
Y & \ras{s} & X & \ras{=} & X & \ras{=} & X & \ras{f} & Y \\
\end{array}$$ Here, $K$ is a section of the bundle map $\mathrm VX\to \mathrm TX$ over $X$, which fiberwise projects from a tangent space to its vertical subspace. $\Phi$ is fiberwise $\mathrm T_pf^{-1}(y)\cong f^{-1}(y)$ given by identifying the vector space $f^{-1}(y)$ with its tangent space at $p\in f^{-1}(y)$. I still don't feel I understand the geometry so I'll try to describe what I do. The bundle I'm visualizing is the ""infinite Möbius strip"" over the circle. The differential $\mathrm ds$ is by functoriality a section of $\mathrm df$, which means fiberwise $\mathrm d_ys$ a section of $\mathrm d_pf$. Now, the fiber $(\mathrm d_pf)^{-1}(v)$ consists of tangents upstairs. The fiber of a nonzero vector in $\mathrm T_yY$ consists of tangents with a horizontal component , since they're not in the kernel. At any rate, $\mathrm ds$ smoothly chooses a subbundle of $\mathrm TX\to X$. For the infinite Möbius strip this amounts to drawing a single arrow on each of the fibers in a smoothly varying way. $\pi_2\circ \Phi \circ K$ then projects this subbundle onto the vertical bundle. For the infinite Möbius strip we project each arrow on a fiber to the fiber itself, identified with its tangent space. The picture is then a smooth array of vertical (in the direction of the fiber) arrows, one on each fiber. Finally, given a vector field $\mathcal Y$ downstairs, $\nabla_\mathcal{Y}s$ is simply precomposition of $\nabla s$ with $\mathcal Y$. Why is the projection to the vertical bundle capturing the same as differentiating the parallel transport? It looks like we're ignoring variation between fibers by projecting onto the vertical bundle - exactly the opposite of parallel transport. I don't understand the intuition here... Here's the best I have: the fact we're parallel along fibers amounts to saying we're moving vectors ""without changing them w.r.t the horizontal direction"". This is somehow analogous to the projection on the vertical bundle, which also ignores horizontal changes. The vertical changes are the ones intrinsic to the manifold upstairs because the fibers of $f$ are ""straight"", as opposed to general tangents which may ""point outside of the surface"". So is the covariant derivative of a section of a vector bundle $f:X\to Y$ sort of like ""partial differentiation along the directions of the fibers of $f$?""","['smooth-manifolds', 'differential-geometry', 'vector-bundles', 'connections', 'definition']"
2112592,Why do we generally require $n$ equations for $n$ unknowns?,"Ever since I wrote my first $x$, it was drilled firmly into my head that generally, to ""solve"" for $n$ variables $\{x_1, \ldots, x_n\}$ you need to specify $n$ functions $\{f_i : X^n \to R\}$ that vanish at all your $x_i$. This was a generally necessary and sufficient condition to get a finite (nonzero) number of solutions. Some years down the road, I learned linear algebra, and the case for systems of linear equations was clear: ""well-posed"" solve for x problems were identified by full rank. For rank-deficient matrices, either there were 0 or infinitely many solutions. The solution space in the latter case could still be quantified by dimension. Is there a way to formalize this idea a little bit more rigorously for continuous functions in general? I sort of understand the idea of dimension in algebraic geometry—my idea is that the ""effectiveness"" of a system of equations is measured by the dimension of their variety, as each time you mod the coordinate ring by one equation, this is equivalent to ""substituting"" one equation into the other like we all did when we were kids. Does this algebraic dimension agree with a linear-algebraic intuition for dimension (dimension of tangent space)? Is there a concrete (possibly differential) way to compute this dimension efficiently by hand?","['differential-geometry', 'algebraic-geometry', 'systems-of-equations']"
2112594,When does intersection commute with tensor product,"Given two submodules $U,V \subseteq M$ over a (commutative) ring $R$, and a flat $R$-module $A$, I can interpret $U \otimes_R A$ and $V \otimes_R A$ as submodules of $M \otimes_R A$. Is it necessarily true that $$(U \cap V) \otimes_R A \cong (U \otimes_R A) \cap (V \otimes_R A) ?$$ I think it should be true in many cases, with intuition coming from $\mathbb{Z}$-modules and $A = \mathbb{Q}$, but I'm unsure about what happens in general.","['tensor-products', 'modules', 'abstract-algebra', 'commutative-algebra', 'flatness']"
2112604,"How to make unbiased coin from potentially biased coin, is my reasoning correct?","Problem: Let's say you have a coin that might be (you don't know how biased, or even whether biased in the first place) biased, and you want to come up with a way to simulate an unbiased flip. My reasoning (took me a few steps to get here but I'll spare you the struggle) is that no matter how biased the coin is, the probability of observing HT is the same as the probability of observing TH (assuming P(H) and P(T) are independent). So we can assign for example H to HT and T to TH, and just wait for either sequence to take place. I think that makes sense, but my intuition has been proven wrong so many times in the past that I really want to make sure my reasoning here is correct. I'm also very interested to hear other ways to think about this and other kinds of intuition for this idea. EDIT : Based on @Mariuslp's answer, I'd like to understand why we can't use a ""sliding window"", meaning why we have to toss away the first two flips if neither HT nor TH happens. From my comment to his answer : The way I think about it, the probability of the entire sequence that happens before either HT or TH is the same for either HT or TH since P(H) and P(T) are independent, so it shouldn't matter. What's the problem with my thinking?",['probability']
2112605,Is there an explanation for the multiple large entries of those continued fraction expansions?,"I searched numbers $N$, such that the continued fraction of $N^{1/3}$ has very large entries. I only searched for a single large entry, but I was surprised that two continued fractions contained not only one large entry, but multiple large entries. Here the two amazing expansions : $$102175  [46, 1, 2, 1, 8741, 2, 186, 2, 13112, 1, 6, 1, 8, 2, 9, 2, 623, 1, 33, 1
, 9, 1, 2, 2, 17484, 14, 2, 2, 1, 4, 19021, 2, 1, 1, 1, 1, 1, 1, 3437888, 2, 2,
6, 21510, 2, 1, 2, 55063048, 1, 1, 1, 1, 1, 2, 8, 44, 2, 1, 4, 1, 4, 61, 2, 1666
1, 2, 1, 3, 1, 1, 23, 1, 4, 2, 2, 8, 3, 3, 1, 1, 2, 6, 3, 1, 1, 3, 5, 17, 21, 17
, 3, 168, 3, 1, 1, 17, 1, 3, 2, 3, 4, 3]   55063048$$
$$267090  [64, 2, 2, 31104, 1, 4, 64, 4, 1, 46657, 1288, 55545, 1127, 62210, 2, 2,
 40, 1, 1, 2, 1, 1, 4, 559, 8, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 101091, 3, 1,
 1, 8, 6, 10, 3, 1, 2, 2, 1, 1, 2, 17, 2, 1, 1, 2, 1, 4897902700, 1, 54, 1, 288,
 1, 1, 1, 20, 1, 1, 5, 31360929, 1, 15, 9, 1, 1, 30, 1, 5, 6, 2, 7, 16, 2, 3, 1,
 2, 3, 9935, 1, 3, 2, 1, 5, 4, 4, 2, 1, 28, 1, 27]   4897902700$$ Explanation : First, the number $N$ is displayed, then the first $100$ terms of the continued fraction of $N^{1/3}$ and finally the maximum of the entries. The cubic roots of the numbers $102175$ and $267090$ seem to have a very special continued fraction. In the second continued fraction, we have even $5$ consecutive large entries, and in both continued fractions we have an entry larger than $10^6$ besides the maximum entry. This is not at all what I expected, in particular because almost every real number has a continued fraction expansion that follows a special distribution mentioned here : Typicality of boundedness of entries of continued fraction representations The continued fraction expansions above are far away from this distribution (even if we do not consider the maximum entry). How can this phenomen be explained ?","['number-theory', 'continued-fractions']"
2112674,Proof involving modular and primes,"My Question Reads: If $a, b$ are integers such that $a \equiv b \pmod p$ for every positive prime $p$, prove that $a = b$. I started by stating $a, b \in \mathbb Z$. From there I have said without loss of generality,  $a \geq b$. Suppose $1 \leq a - b \in \mathbb Z$. From here I expressed $a - b$ as a prime factorization. $a - b = p_1 \times p_2 \times \ldots \times p_n$ From here I said to consider a prime $p$ such that $p$ does not equal $p_k$ for all $1$ less than or equal to $k$ less than or equal to $n$.
From here I am a bit lost as to how to continue.","['abstract-algebra', 'prime-factorization', 'proof-writing', 'modular-arithmetic', 'discrete-mathematics']"
2112675,Find normal vector without using cross product,"I have the points: $A(0,0,1), B(0,1,0), C(2,0,0)$ I am asked to find the scalar equation of the plane but I cannot use cross product to compute the normal vector. How do I do it without it? Let: $AB = [0,1,-1], AC = [2,0,-1]$ Let $n$ be the normal vector $[n_{1}, n_{2}, n_{3}]$ I know, $AB \cdot n = 0, AC \cdot n = 0 $. How can I find the normal vector?","['vector-spaces', 'linear-algebra', 'vectors', 'discrete-mathematics']"
2112679,Help with my proof that the union of two countably infinite sets is countably infinite,"Let $A, B$ be countably infinite sets. Prove that the union of $A$ and $B$ is also countably infinite. I'm aware that this question already exists on stack exchange, but I'm trying it a different way and am having difficulty finishing it. Let the union of $A$ and $B$ be denoted by $C$. For $C$ to be countably infinite, we need to find a bijection between $C$ and the natural numbers. Let $f(n)$ be the bijective function mapping the natural numbers to $A$, and $g(n)$ be the bijective function mapping the natural numbers to $B$. Define $h(n)$ to be the function mapping the natural numbers to $C$ given by: $$h(2n) = f(n)$$
$$h(2n-1) = g(n)$$ To show that $h(n)$ is bijective, we need to show that it is both surjective and injective. I was able to do the surjective part without problem, but am having trouble proving injectivity. Attempt: if $n_1, n_2$ are even, then $n_j = 2k_j$ for some other natural numbers $k_j$. Thus, $$h(2k_1) = h(2k_2)$$
$$f(k_1) = f(k_2)$$
$$k_1 = k_2$$
since $f(n)$ is bijective. Simlarly, if $n_1, n_2$ are odd, then $n_j = 2k_j - 1$ for some natural numbers $k_j$. So, $$h(2k_1 - 1) = h(2k_2 - 1)$$
$$g(k_1) = g(k_2)$$
$$k_1 = k_2$$
since $g(n)$ is bijective. However, we could also have the case that $n_1$ is even and $n_2$ is odd. Then, $$h(n_1) = h(n_2)$$
$$h(2k_1) = h(2k_2 - 1)$$
$$f(k_1) = g(k_2)$$ this is where I get stuck. If I can show this, I've shown that $h(n)$ is injective, and in turn, bijective. For reference, this site uses a very similar technique, but seems to completely disregard injectivity for some reason. http://planetmath.org/unionofcountablesets EDIT: Let $A' = A \text{\ } B$. It is clear that $A' \cup B = A \cup B$, as: if $c \in A' \cup B$, then $c \in A'$ or $c \in B$. If $c \in B$, then $c \in A \cup B$. If $c \in A'$, then, by definition, $c \in A$, so that $c \in A \cup B$. It is also clear that $A'$ must be a countable set, as $A$ is countable and $A'$ has, at most, the same number of elements as $A$. If $A'$ is finite, then $A' \cup B$ is countably infinite, as we have added only a finite number of elements to $B$ (is this obvious enough to take as fact without proof?). If $A'$ is countably infinite, then redefine $f(n)$ to be a bijective function from natural numbers to $A'$. The cases of surjectivity and injectivity above are unchanged. Now, if $$f(k_1) = g(k_2)$$ we have that for some $a \in A'$ and $b \in B$, $a = b$. This contradicts the fact that $A'$ and $B$ are disjoint, so we must have that $h(2k_1) \ne h(2k_2 - 1)$ for any natural numbers $k_j$. Therefore, $h(n)$ is bijective. QED.",['elementary-set-theory']
2112681,Let $G$ be a finite group with more than one element. Prove that $G$ has an element of prime order,"For this question why can't I just say since the identity is in every group, therefore the statement hold? Here is what I have so far:
Let $g$ be an element of a group $G$, such that $g ≠ e$, and the order of $g$ is $n$. Let $p$ be some prime that divides $n$, then
$$g^n = (g^{n/p})^p = e.$$ Therefore the the statement is true. Is this enough to prove the statement?",['group-theory']
2112685,"6 Red balls, 7 Blue balls, ways to choose 5 balls so there are more red balls chosen.","My thoughts are the following: P(Ways to choose 5 balls so there are more red balls) = P(3 red are chosen) + P(4 red are chosen) + P(5 red are chosen). My intuition says to do this problem considering each subproblem with conditional probability (eg, P(3 are chosen) = (6/13) (5/12) (4/11) (7/10) (6/9)). I feel like there is an easier combinatorial answer, but I haven't dealt with this type of problem in a few months.",['combinatorics']
2112707,A limit related to asymptotic growth of tetration,"The tetration is denoted $^n a$, where $a$ is called the base and $n$ is called the height, and is defined for $n\in\mathbb N\cup\{-1,\,0\}$ by the recurrence
$$
{^{-1} a} = 0, \quad {^{n+1} a} = a^{\left({^n a}\right)},\tag1$$
so that
$${^0 a}=1, \quad {^1 a} = a, \quad {^2 a} = a^a, \quad {^3 a} = a^{a^a}, \, \dots \quad {^n a} = \underbrace{a^{a^{{.^{.^{.^a}}}}}}_{n\,\text{levels}}.\tag2$$
Let $a$ be a real number in the interval $e^{-e} < a < e^{1/e}$. It is known that the following limit exists
$$L(a) = \lim_{n\to\infty} {^n a},\tag3$$
where $L(a)$ satisfies $a^{L(a)}=L(a)$. For example, $L\left(\!\sqrt2\right)=2$. It is also known that
$$\lim_{n\to\infty} \, \frac{L(a) - {^{n+1} a}}{L(a) - {^n a}} = \ln L(a).\tag4$$
Finally, it is known that the following limit exists
$$C(a) = \lim_{n\to\infty} \, \frac{L(a) - {^n a}}{\left(\ln L(a)\right)^n}.\tag5$$
Apparently, no closed form for the function $C(a)$ is known.
But the numerical evidence suggests the following conjecture (basically, this is the coefficient of the linear term in the Taylor series expansion of $C(a)$ near $a=1$): $$C'(1) = \lim_{a\to1} \, \frac{C(a)}{a-1} \stackrel?= 1.\tag{$\diamond$}$$ How can we prove it? Can we find values of some higher-order derivatives? Are they all integers? Is there a general formula, recurrence or an efficient algorithm to compute them? Related questions: [1] [2] [3] .","['conjectures', 'limits', 'asymptotics', 'sequences-and-series', 'tetration']"
2112712,Is $\mathbb{R}$ the disjoint union of finitely many congruent dense sets?,"Both the set of rational numbers $\mathbb{Q}$ and its complement are dense in $\mathbb{R}$, but the relationship between them is very asymmetric. For instance, the rationals are countable and have Lebesgue measure 0, whereas the irrationals are uncountable and have infinite Lebesgue measure. Is it possibly to decompose the real numbers into dense subsets in a more symmetric way, so that $\mathbb{R}$ can be written as a union of finitely many disjoint sets which can be mapped into each other by translation or reflection (i.e. are congruent)?","['general-topology', 'examples-counterexamples', 'elementary-set-theory']"
2112803,Is the function $A \mapsto \det(A)$ convex over the set of positive definite matrices?,"Let $\mathbb{P}^{n \times n}(\mathbb{R})$ denote the set of positive definite matrices. Is the following function convex? $$ \det: A\in \mathbb{P}^{n \times n}(\mathbb{R}) \to \det (A)$$ I think the answer is yes, but I cannot prove it directly using the definition of convex function. How can I do?","['matrices', 'convex-analysis', 'determinant']"
2112823,Explain how $\binom{n}{0}-\binom{n}{1}+\binom{n}{2}+...+(-1)^{n}\binom{n}{n} = 0$ [duplicate],"This question already has answers here : Alternating sum of binomial coefficients: given $n \in \mathbb N$, prove $\sum^n_{k=0}(-1)^k {n \choose k} = 0$ [duplicate] (7 answers) Closed 7 years ago . I understand how the sum of the bottom row of Pascal's triangle is equal to $2^n$, but how is this formula equal to zero?  Shouldn't it be 1? $\binom{n}{0}-\binom{n}{1}+\binom{n}{2}+...+(-1)^{n}\binom{n}{n} = 0$","['combinatorics', 'combinatorial-proofs']"
2112973,Probability of possessing a position among all.,"Suppose with staggered entry, $5$ children have come to a teacher to learn alphabets. The teacher assigned them ID according to their arrival, that is, the teacher assigned ID $1$ to the child who came first to her, and then ID $2$ to the next arrival child and so on. Now the teacher wants to give them prize according to their learning ability. That is, the child who learned the alphabets in the shortest time among the $5$ children will be given the first prize, then the child who learned the alphabets in the second shortest time among the $5$ children will be given the second prize, and so on. I know that any child can possess any prize. That is, ID $1$ can have the quickest learning ability or can have the 2nd quickest learning ability or he may be the slowest learner among all. The same applies for any ID. Does the probability that the $i$th arrival possesses the $j$th prize depend on previous $(j-1)$ prizes? That is, can  the probability that the 1st arrival gets ""the 1st prize"" and the probability that the 1st arrival gets ""the 3rd prize"" be different?",['probability']
2113001,Find length of intersection between 2 points and a sphere,"I have a sphere and 2 points. The points have (x,y,z) coordinates and the sphere is defined by its centre (0,0,0) and radius R. I am trying to find the length between the 2 points which intersects the sphere. How can I obtain the equation to describe this length? See below, my objective is Length, L:",['geometry']
2113006,Find coefficient of generating function f(x).,Find coefficient of generating function. $ f(x) = \frac{2x}{1-x^{2}} +x$ MY WAY OF SOLVING SIMILAR PROBLEM: 1) $ g(x) = \frac{2x}{1-x^{2}}$ 2) partial fraction $g(x) = \frac{A}{1-x} + \frac{B}{1+x} $ 3) $ g(x) =  \sum\limits_{n=0}^\infty Ax^{n} + \sum\limits_{n=0}^\infty B (-1)^nx^{n} = \sum\limits_{n=0}^\infty (A+(-1)^nB)x^{n} $ -solution But what can I do with $f(x)$? I can't use my method because: $f(x) = \frac{2x+x(1-x^2)}{1-x^2} $ $\frac{-x^3 +3x}{1-x^2} = \frac{A}{1-x} + \frac{B}{1+x}$ $ -x^3+3x = A(1+x) + B(1-x) $ $-x^3 = 0 \cdot x^3  $ $ -1 =0 $,['discrete-mathematics']
2113021,Prove an entire function is constant,"Let $f$ be an entire function such that $f(0)=1, f'(0)=0$, and
$$
0<|f(z)|\leq e^{|z|}
$$
for every $z\in \mathbb{C}$. Prove $f$ is constant $1$ on $\mathbb{C}$. I am going to use Cauchy estimate similar to this . but I found it does not work, can you give me some hint?",['complex-analysis']
2113106,"Convexity of $x\left(1+\frac1x\right)^x,\ x\ge 0$","This may turn out to be really simple but I do not see a quick way to the proof. How would one show $\displaystyle x\Big(1+\frac1x\Big)^x,\ x\ge 0$ is convex? I derived the second derivative. It has a negative term. I suppose I could combine certain terms to make the negativeness disappear. But I hope there is a really clever way to see it right away.","['real-analysis', 'inequality', 'exponential-function', 'convex-analysis', 'calculus']"
2113108,What I am missing in this simple equation from Nesterov's paper?,"In this paper by Prof Nesterov, First-order methods of smooth convex optimization
with inexact oracle , proof of Theorem 2, there is the following very simple equation which I think it is wrong,
\begin{align}
\Vert x_{k+1}-x^*\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k+1}-x^*\rangle -\Vert x_{k+1}-x_k\Vert^2
\end{align} 
(They defined norm as $\Vert x\Vert^2 =\langle Bx,x\rangle$)
Obviously, it must be 
\begin{align}
\Vert x_{k+1}-x^*\Vert^2 = \Vert x_k-x^*+x_{k+1}-x_{k}\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k}-x^*\rangle \\
+\Vert x_{k+1}-x_k\Vert^2
\end{align}
Am I missing something? I also, checked journal version, Mathematical Programming .","['optimization', 'convex-optimization', 'linear-algebra']"
2113131,Which Fourier series define a (tempered) distribution?,"Let ${\sum}_{-\infty}^{+\infty} a_k e^{ikx}$ be the Fourier series of a function. 
Which condition must be satisfied by the coefficients ${a_k}$ so that the series defines a distribution in $\mathcal{D'}$ or in $\mathcal{S'}$? I know that a series of functions that define a distribution in $\mathcal{D'}$ is a distribution in the same space if it converges uniformly on compact sets; a series of functions that define a distribution in $\mathcal{S'}$ is itself a tempered distribution if it converges uniformly on $\mathbb{R}$. How does this all apply to the Fourier series?","['distribution-theory', 'fourier-series', 'sequences-and-series']"
2113138,"What is the difference between variable, argument and parameter?",I'm sure that these terms should be different since there exists a difference between parameter and argument in computer science but I'm not sure about their differences in math.,"['terminology', 'soft-question', 'functions', 'definition']"
2113205,$\int_{-\infty}^{\infty}{e^x+1\over (e^x-x+1)^2+\pi^2}\mathrm dx=\int_{-\infty}^{\infty}{e^x+1\over (e^x+x+1)^2+\pi^2}\mathrm dx=1$,"Look-alike integrals $$I_1+I_2=\int_{-\infty}^{\infty}{e^x+1\over (e^x-x+1)^2+\pi^2}\mathrm dx=\int_{-\infty}^{\infty}{e^x+1\over (e^x+x+1)^2+\pi^2}\mathrm dx=1\tag1$$ I just wonder if these integrals $I_1$ and $I_2$ are the same in term of transforming is concern? Or they just only happened to give the same closed form? If I make a substitution of $u=e^x+1$, nothing much happened $$\int_{1}^{\infty}{u\over u-1}\cdot{\mathrm du\over (u-\ln{(u-1)})^2+\pi^2}=\int_{1}^{\infty}{u\over u-1}\cdot{\mathrm du\over (u+\ln{(u-1)})^2+\pi^2}\tag2$$ Note: I can't show it but I think $I_1$ and $I_2$ are not related in term of transforming into each other. Else How can we show that $I_1$ or $I_2$ has a value of one?","['integration', 'definite-integrals', 'calculus']"
2113226,Division of Distributions by Polynomials,"Let $P(z)$ be a non-null complex polynomial in $n$ variables $z=(z_1,\dots,z_n)$:
\begin{equation}
P(z)=\sum_{|\alpha| \leq N} c_{\alpha} z^{\alpha},
\end{equation}
where as usual for every $\alpha=(\alpha_1,\dots,\alpha_n) \in \mathbb{N}^{n}$ we set $|\alpha|=\alpha_1+\dots+\alpha_n$, and $z^{\alpha}=z_1^{\alpha_1}\dots z_n^{\alpha_n}$. Consider $P$ as a polynomial function from $\mathbb{R}^n$ into $\mathbb{C}$:
\begin{equation}
P(x)=\sum_{|\alpha| \leq N} c_{\alpha} x^{\alpha} \quad (x \in \mathbb{R}^n).
\end{equation}
Define the linear subspace $\mathcal{M}_{\mathcal{S}}$ of the Schwartz space $\mathcal{S}=\mathcal{S}(\mathbb{R}^n)$:
\begin{equation}
\mathcal{M}_{\mathcal{S}}= \{ \psi \in \mathcal{S}: \psi=P\phi, \phi \in \mathcal{S} \},
\end{equation}
and the linear continuous multiplication map $M_{P}:\mathcal{S} \rightarrow \mathcal{M}_{\mathcal{S}}$
\begin{equation}
M_{P}(\phi)=P\phi \quad (\phi \in \mathcal{S}).
\end{equation}
In his work On the Division of Distributions by Polynomials , Hörmander proved the following remarkable result (whose proof is unexpectedly very complicated). Theorem (1). The map $M_P$ has a linear continuous inverse $M_{P}^{-1}:\mathcal{M}_{\mathcal{S}} \rightarrow \mathcal{S}$. From this result we can easily deduce the following Theorem (2). Let $T \in \mathcal{S}'$. Then there exists $S \in \mathcal{S}'$ such that $P \cdot S=T$. Proof. The map $T \circ M_{P}^{-1}: \mathcal{M}_{\mathcal{S}} \rightarrow \mathbb{C}$ is a linear continuous functional, so by the Hahn-Banach Theorem it can be extened to a continuous linear functional $S$ on $\mathcal{S}$. $S$ satifies $S(P\phi)=T(\phi)$ for each $\phi \in \mathcal{S}$, so $P \cdot S = T$.
QED Hörmander says that an exactly analogous argument proves the following result. Theorem (3). Let $\Omega$ be an open set of $\mathbb{R}^n$, and $T \in \mathcal{D'}(\Omega)$. Then there exists $S \in \mathcal{D'}(\Omega)$ such that $P \cdot S=T$. Could you see some way of proving this theorem by using Theorem (1)? The fact is that if we define the subpspace of $\mathcal{D}(\Omega)$
\begin{equation}
\mathcal{M}_{\mathcal{D}}= \{ \psi \in \mathcal{D}(\Omega): \psi=P\phi, \phi \in \mathcal{D}(\Omega) \},
\end{equation}
and the linear continuous multiplication map $N_{P}:\mathcal{D}(\Omega) \rightarrow \mathcal{M}_{\mathcal{D}}$
\begin{equation}
N_{P}(\phi)=P\phi \quad (\phi \in \mathcal{D}(\Omega)),
\end{equation}
I see no way of deducing from Theorem (1) that $N_P$ has a linear continuous inverse. If we could do this, then of course we could prove Theorem (3) by using the same argument we used to prove Theorem (2). Any help is welcome.
Thank you very much in advance for your attention. NOTE. Let me notice that there is instead a way of proving Theorem (3) by using Theorem (2) (but of course this was not what Hörmander had in mind). Let $\Gamma$ be the collection of all open rectangles $\omega$, such that the closure of $\omega$ is a compact set contained in $\Omega$. Clearly $\Gamma$ is an open covering of $\Omega$. Let $\omega \in \Gamma$ and choose $\xi \in \mathcal{D}(\Omega)$ such that $\xi=1$ on $\omega$. Since $\xi \cdot T$ is a distribution with compact support, it defines a tempered distribution, so that by Theorem (2) there exists $V \in \mathcal{S}'$ such that 
\begin{equation}
V(P \phi)= T(\xi \phi) \quad (\phi \in \mathcal{S}).
\end{equation}
In particular, we have
\begin{equation}
V(P\phi)=T(\xi \phi)=T(\phi) \quad (\phi \in \mathcal{D}(\omega)).
\end{equation}
Let us denote with $S_{\omega}$ the restriction of $V$ to $\mathcal{D}(\omega)$. We have $S_{\omega} \in \mathcal{D}(\omega)$. Moreover, if 
$T_{\omega}$ is the restriction of $T$ to $\mathcal{D}(\omega)$, then we have $ H \cdot S_{\omega} = T_{\omega}$. In other terms, the equation $P \cdot S = T$ has a solution on $\omega$. Now, we know that there exists  a locally finite partition of unity $(\psi_j)_{j=1}^{\infty}$ in $\Omega$ subordinate to the open cover $\Gamma$ (see Rudin, Functional Analysis, Second Edition, Theorem (6.20)). This means that $(\psi_j)_{j=1}^{\infty}$ is a sequence in $\mathcal{D}(\Omega)$, with $\psi_j \geq 0$, such that: (i) each $\psi_j$ has its support in some member of $\Gamma$, (ii) $\sum_{j=1}^{\infty} \psi_j(x)=1$ for every $x \in \Omega$, (iii) to every compact $K \subset \Omega$ correspond an integer $m$ and an open set $W \supset K$ such that
\begin{equation}
\psi_1(x)+\dots+\psi_m(x)=1,
\end{equation}
for all $x \in W$. Let $\omega_j$ be the element of $\Gamma$ which contains the support of $\psi_j$ according to (i). Then define
\begin{equation}
S(\phi)= \sum_{j=1}^{\infty} S_{\omega_j}(\psi_j \phi) \quad (\phi \in \mathcal{D}(\Omega)).
\end{equation}
Since for each $\phi \in \mathcal{D}(\Omega)$ only finitely many of the functions $\psi_j \phi$ are different from zero, it is easy to see that $S$ is well defined, that $S \in \mathcal{D'}(\Omega)$ and that $P \cdot S = T$.
QED","['functional-analysis', 'distribution-theory']"
2113308,Prove the function $f(x)$ have an extreme on $a$,"If $$f'(a)=f''(a)=f'''(a)=0$$$$ f^{(4)}\ne0$$
Will the function have an extreme on $a$? I have the solution "" Yes because the taylor function will be:""$$f(x) = f(a) + \frac{1}{4!}f^{(4)}(a)(x − a)^4 + o((x − a)^4)$$ But I can't see why that proves there is an extreme.","['derivatives', 'taylor-expansion', 'maxima-minima']"
2113313,Cauchy-Riemann equations for $f(z)=\overline z$,"Show that the function $f(z) = \overline{z}$
is Not differentiable anywhere in the $z$-plane. I am thinking about Cauchy-Riemann theorem $ f(z) = x-iy $ Here
$$
\begin{cases}
u(x,y) = x \\
v(x,y) = -y
\end{cases}
$$
and both of them are differentiable and \begin{align}
\frac{\partial u}{\partial x} = 1 \ne
 \frac{\partial v}{\partial y} \\
\frac{\partial u}{\partial y} = 0 = \frac{\partial v}{\partial x}
\end{align} I really can't give an example to prove that the function isn't differentiable anywhere.","['derivatives', 'complex-analysis', 'ordinary-differential-equations']"
2113337,Proving a matrix inequality,"Let $A, B \in \mathbb R^{m\times m}$ be symmetric positive semi-definite matrices. Is it true that
$$\sup_{\|x\| = 1} \left| \|Ax\| - \|Bx\| \right| \geq c(m) \|A-B\|,$$
with $c(m) > 0$ and where $\|\cdot\|$ denotes the 2-norm? Here is how I approached the problem. Let us introduce the notation $\Delta = A -B$ for convenience.
Without loss of generality, we can assume that $B$ is diagonal, that $\| \Delta\|$ = 1, and that $\Delta$ has an eigenvalue equal to $+1$. We have
$$
\begin{align}
                \|Ax\| = \sqrt{x^T (B+\Delta)^2 x} &= \sqrt{x^T B^2 x + x^T (B \Delta + \Delta B) x + x^T \Delta^2 x} \\
                                          &= \sqrt{\left(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x}\right)^2 - 2 \left(\sqrt{x^T B^2 x} \sqrt{x^T \Delta^2 x} - x^T B \Delta x\right)}.
\end{align}
$$
        By Cauchy-Schwarz inequality, the expression in the second brackets is non-negative, so
$$
\begin{align}
         \sup_{\|x\| = 1} \, \left| \|Ax\| - \|Bx\| \right|   &\geq \sup_{\|x\| = 1}\frac{|2 \, x^T B \Delta x + x^T \Delta^2 x|}{2(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x})}
\end{align}
$$
From here things get more complicated and any help would be appreciated. Of course, there might be other approaches to the problem. Thank you. Bonus question: Is the statement true for positive self-adjoint operators on a Hilbert space?","['matrices', 'inequality', 'linear-algebra']"
2113373,"Is the function $f(x, y) :=$ $\sin(xy) \over x^2 + y^2$ Lebesgue-integrable?","Given the function $f: \Bbb R^2 \rightarrow \Bbb R$, $f(x, y) :=
\begin{cases}
\sin(xy) \over x^2 + y^2,  & \text{$(x, y) \in \Bbb R^2 \setminus ${0}$$} \\
0, & \text{otherwise}
\end{cases}$ decide whether it is Lebesgue-integrable or not. Hint: $\int_{(0, \infty)}$ $1 \over r$ $d\lambda(r) = \infty$. Where to start about something like that? I know that a function is Lebesgue integrable if it is measurable and if $f(x, y)_+$ and $f(x, y)_-$ are integrable. I think it is measurable since it is continuous. So now, I would have to determine what $f(x, y)_+$ is and prove (or disprove) that $\int_{\Bbb R} f(x, y)_+ < \infty$? Edit: Since I didn't receive further help in the comments, I am searching for an other approach. There is a similar problem to this: Prove that function is not Lebesgue integrable If you take a look at the answer with 8 upvotes, you'll find that the hint that I was given was applied there. So by switching to polar coordinates $(x, y) = (r \cos \phi, r \sin \phi)$, we would receive something like: $\int_0^{2\pi} \int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $dr \ d\phi.$ If there was a way to ""clear"" the numerator here, it would be fairly easy to apply the hint here too, which would yield that the function is not Lebesgue-integrable directly. But is there a way to do it? Edit 2: On the other hand, isn't $\int_0^{\infty}$ $\sin(r^2 \ \cos \phi \ \sin \phi) \over r$ $\le \int_0^{\infty}$ $1 \over r$?","['multivariable-calculus', 'integration', 'lebesgue-integral', 'polar-coordinates']"
2113399,"If $\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = 0$, show that $\lim_{n\to\infty}{a_n} =0$.","Problem: If $\lim_{n\to\infty} \frac{a_{n+1}}{a_n} = 0$, show that $\lim_{n\to\infty}a_n = 0$. Attempted Proof: Let $\epsilon > 0$. From the hypothesis, $\exists \ N \in \mathbb{P}$ such that if $n \geq N$, then $$\left|\ \frac{a_{n+1}}{a_n} - 0 \ \right| < \epsilon.$$ This implies $$\left| {a_{n+1}} \right| < \epsilon \left| a_n \right|.$$ Thus, let $n\geq N'$ such that $$\left|\ \frac{a_{n+1}}{a_n} - 0 \ \right| < \epsilon\left|a_n\right|.$$ Then we have $\left|a_{n+1}\right| < \epsilon,$ which implies $\lim_{n\to\infty}a_n = 0$. My main concern with my proof is that $\epsilon$ depends on $a_n$. Is this an issue?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2113491,Calculating $\int_{-\infty}^{\infty}\frac{x^i}{((x^2+a)^2+b^2)^{\frac{3}{2}}}dx$,"I am looking for whether the integral
$$\int_{-\infty}^{\infty}\frac{x^i}{((x^2+a)^2+b^2)^{\frac{3}{2}}}dx$$
where $i=0,\ldots, 4$ and $a,b>0$ are parameters can be calculated in an elementary way. I myself got stuck and online calculators gave quite horribly looking answers. It would be especially nice to see a connection to probability (i.e. interpreting the above as moments of some known distribution), as the background of the question, which is a PDE problem, would actually suggest there might be one.","['integration', 'probability', 'calculus']"
2113526,Analyzing $\biggl\lfloor{\frac{x}{5}}\bigg\rfloor=\bigg\lfloor{\frac{x}{7}}\bigg\rfloor$,"How many non negative integral values of $x$ satisfy the equation :$$\biggl\lfloor{\dfrac{x}{5}}\bigg\rfloor=\bigg\lfloor{\dfrac{x}{7}}\bigg\rfloor$$. My try: Writing few numbers and putting in the required equation. $\underbrace{0,1,2,3,4}_{\implies 0=0},\underbrace{5,6}_{\displaystyle\times},\underbrace{7,8,9}_{\implies 1=1},\underbrace{10,11,12,13}_{\displaystyle\times} ,\underbrace{14}_{\implies 2=2},\underbrace{15,16,17,18,19,20}_{\text{LHS gives 3 but RHS gives 2}},\underbrace{21,22,23,24}_{\text{LHS gives 4 but RHS gives 3}}$. The point which I wish to make here is that after $14$ we'll get $5's$ multiple before than $7's$, hence $\forall x \geq 15 \implies \biggl\lfloor{\dfrac{x}{5}}\bigg\rfloor>\bigg\lfloor{\dfrac{x}{7}}\bigg\rfloor$ Hence the only solution are: $\{0,1,2,3,4,7,8,9,14\}$. Does these make sense? I need to know other solutions that don't incorporate such analyzing, means using some properties and framing it to solve above.","['algebra-precalculus', 'ceiling-and-floor-functions', 'elementary-number-theory']"
2113539,Continuous branch for $\sqrt{z(z-1)(z-2)(z-3)}$,"I am studying complex analysis out of Gamelin.  I am reading section 1.4 on  roots and principal values, but there's not a lot of worked examples. Question:  For the function $F(z) = \sqrt{z(z-1)(z-2)(z-3)}$, does there exist a continuous branch $f$ for the domain $\mathbb{C} \setminus [0,2] \cup [3,\infty)$ for which $\text{Re}(f(i)) > 0$? This question is really simple to me if the domain were different (say if the excluded part was $[0,1] \cup [2,3]$ because these domain boundaries reflect the sign changes of $z(z-1)(z-2)(z-3)$).  Basically, if I assumed the inside of the square root was real, for which $z$ would the inside be negative, then I just use the branch for $\sqrt{z}$ over the domain excluding the negative real axis.  But I do not see how to apply this intuition for the above domain.  Any help would be appreciated!","['complex-analysis', 'branch-cuts']"
2113547,"Intuition Behind, or Canonical Examples of Finite Type Morphisms","I'm new to the world of schemes, so I'm still trying to grasp some of the basics.  I understand most of the simple topological properties of schemes, as well as some of the sheaf-theoretic properties like reducedness, integrality, normality, etc.  But I'm having a hard time digesting what finte and finite type morphisms really entail concretely.  Also, Hartshorne doesn't seem to include many nice examples from what I can tell. Pretty much the only example I have down at this point is what it means for a scheme $X$ to be of finite type over a field $k$.  In such a case, we'd have a morphism of schemes $f:X \to \rm{Spec}(k)$, and since the spectrum of a field is simply a point, the inverse image is all of $X$.  Hence, just applying the definitions, we see that $X$ has a finite open cover by affine schemes $\rm{Spec}A_{i}$, where each $A_{i}$ is a finitely generated $k$-algebra.  If we add integrality of $X$, we observe that a variety over $k$ is simply a integral scheme of finite type over a field $k$. So this is nice, but beyond this I'm failing to grasp any intuition.  Does anyone have any canonical examples that I should work through?  Or maybe any pointers on how to think about finite and finite type .  Eisenbud and Harris point out that pretty much all geometrically interesting morphisms are usually in these two regimes.  For example, I guess spectra of local rings are ""non-geometric"" as they aren't of finite type over a field.  This is a great example of something I don't understand well enough yet.",['algebraic-geometry']
2113577,What is the easiest way to draw trigonometric Graphs,"I need to draw a correct sketch of trigonometric graphs like $$y=2\sin \left(3x+\frac{\pi}{4}\right)-1$$ I usually transform this into $$Y=\sin X$$ , where $$Y=\frac{y+1}{2}$$ and $$X=3x+\frac{\pi}{4}$$ And draw the graph $Y$ vs $X$. Is there another method which will help me to draw the graph in one step ?","['trigonometry', 'soft-question']"
2113603,Inverses of polynomials in $R[[T]]$,"Let $R$ be a commutative ring with 1. Is there a nice way to characterise elements
$$f(T) = a_0 + a_1 T + a_2 T^2 + \dots \in R[[T]]$$ 
whose inverses are polynomials? Something like ""$f$ has invertible constant term $and$...""? If it helps, the ring $R$ can be specialised to any field $K$. A crude guess was that maybe $f$'s coefficients have to be periodic in some way, but examples like $f(T) = 1/(1+T)^2$ show that's false. It seems natural to me that such a nice property should have a correspondingly nice bearing on the coefficients of $f$, but maybe this is too much to hope for?","['combinatorics', 'ring-theory', 'recurrence-relations', 'power-series']"
2113634,Comparing two rotation matrices,"Problem I want to compare two rotation matrices $R_A$ and $R_B$ both representing the orientation of the same point cloud in space, but computed from different methods. The idea is to have an estimation of the error between those two matrices. Method My idea was to do it as follows: Compute the rotation $R_{AB}$ between $R_A$ and $R_B$ as $R_{AB} = R_A^TR_B$ Compute the axis-angle ($\omega$, $\theta$) representation of $R_{AB}$ using the following formula:
$$Tr(R_A) = 1 + 2cos(\theta)$$ Use the angle $\theta$ as the rotation error. In Python, I do: r_oa = import(R_A) // See R_A below, in ""Data""
r_ob = import(R_B) // See R_B below, in ""Data""

r_oa_t = np.transpose(r_oa)
r_ab = r_oa_t * r_ob

np.rad2deg(np.arccos((np.trace(r_ab) - 1) / 2)) That seems quite straightforward to me, but I get $\theta = 23.86\unicode{xb0}$, which seems really unrealistic. This intuition is confirmed by the use of a software comparing the matrices, as described below in ""Verification"". Am I doing something wrong there? Is it just not the right way to compare my matrices? Verification In a processing software, I have the possibility to ""compare"" two matrices (each being a 4x4 matrix defining a position and an orientation: [R | t]). The documentation does not explain exactly how it works, but the output of this comparison is: 4 values for the rotation [deg]: Omega, Phi, Kappa, Total 4 values for the translation: $\Delta$x, $\Delta$y, $\Delta$z, Total. I assumed that the ""Total"" angle for the rotation is my $\theta$ (as described above). For the same matrices $R_A$ and $R_B$ as above, the software outputs: Total = 0.036477551. That seems much more realistic, but then I don't know why it is not what I get. Just to check my computation of $\theta$, I tried to compute it on $R_A$ and $R_B$ (instead of $R_{AB}$) and to compare that to the output of the software when running its comparison between $R_A$ and $I$, and also between $R_B$ and $I$. Even though I don't get the exact same $\theta$ as the software, my result is really close (to the 4th decimal). Assuming that the software loses precision somewhere, I believe that my computation of $\theta$ is correct. And my computation of $R_{AB}$ is quite straightforward, so I don't understand why I don't find the same $\theta$ for $R_{AB}$. Data My matrices are: $R_A = \begin{bmatrix} -0.956395958000000 & 0.292073230000000 & 0.000014880000000 \\
-0.292073218000000 & -0.956395931000000 & 0.000242173000000 \\
0.000084963000000 & 0.000227268000000 & 0.999999971000000\end{bmatrix}$ $R_B = \begin{bmatrix} 
-0.956227882000000 & 0.292623030000000 & -0.000013768000000 \\
-0.292623029000000 & -0.956227882000000 & -0.000029806000000 \\
-0.000021887000000 & -0.000024473000000 & 0.999999999000000
\end{bmatrix}$ The software outputs, for the comparison between $R_A$ and $R_B$: ""Total = 0.036477551"". Following my method, I get $\theta = 23.86$, which is completely different. Related questions ""Change in rotation"" matrix","['rotations', 'linear-algebra']"
2113654,Integration with respect to counting measure when $X=\mathbb{R}$,"Consider the measure space $(X,\mathcal{M},\mu)$ where $X=\mathbb{R}$, $\mathcal{M}=\mathcal{P}(X)$ and $\mu$ is the counting measure. I want to show that for every measurable function $f : \mathbb{R} \mapsto [0,+\infty]$, given $x \in \mathbb{R}$, $\int_{\{ x\}} f \, d \mu = f(x) $. Generally speaking $\int_{X} f \, d \mu = \sum_{x \in X} f(x) $. 
I can do this in the case $X=\mathbb{N}$ with a monotone convergence argument, but when $X$ is uncountable I find some trouble. How can I fix things when $X$ is uncountable?","['integration', 'measure-theory']"
2113656,Prove $\sum_{i=0}^n (i+1)((i+1)!) = (n+2)!-1$,"I haven't had to prove something of this type before, and can't even figure out how to start a relevant proof. Induction has been the general go-to method, but the switch (and combination) from i to n is somehow messing with my ability to get anywhere. Any help would be appreciated.","['proof-writing', 'summation', 'sequences-and-series', 'discrete-mathematics']"
2113657,Burnside's Lemma applied to grids with interchanging rows and columns,"I've recently learned about Burnside's Lemma ( https://en.wikipedia.org/wiki/Burnside%27s_lemma ) and its applications to rotating necklaces, coloring cubes and such, but I fear my understanding of it isn't mature enough and am unable to apply it to the following situation: Suppose you have a 2x3 matrix, and a set of 3 distinct colors $R, Y, B$. How many non-equivalent ways are there to color the matrix? Note that two matrices $m_1$ and $m_2$ are considered equivalent if you can turn the former into the latter by swapping any rows and/or columns as many times as you want. So I got started by setting up the Burnside's equation as follows: \begin{align*}\frac{1}{|G|}\sum_{g \in G} |X^g| &= \frac{1}{H! W!} \sum_{\sigma \in S_H}\sum_{\tau \in S_W} |X^{(\sigma, \tau)}|.
\end{align*} where $|G|$ is the total no. of elements that act to permute the set $X$ of matrices with height and width $H\times W$. Since there are $H!$ ways to permute the rows and $W!$ ways to permute the columns, the total no. of ways to permute the matrix is $|G| = H!W!$. So we can iterate through every way to permute the rows and every way to permute the columns (hence the double summation above), and for each permutation, find out the value of |$X^{(\sigma, \tau)}$| -- which is the number of matrices that are fixed (a.k.a. not changed) when applying permutation $\sigma$ to the rows and $\tau$ to the columns. For example, say we have a 2x3 grid with the following indices for each cell: (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) There would be 2! = 2 ways to permute the rows and 3! = 6 ways to permute the columns, so the Burnside's equation becomes:
\begin{align*}\frac{1}{2! 3!} \sum_{\sigma \in S_2}\sum_{\tau \in S_3} |X^{(\sigma, \tau)}|
\end{align*} It is here that I hit a stumbling block. Given we have 3 distinct colors to work with, it isn't clear to me how to count up |$X^{(\sigma, \tau)}$| for each permutation. If someone could show me a step-by-step way to compute the answer for this specific example, I feel I could probably learn to apply it to a more general situation, with arbitrary values of $W$, $H$, and number of colors. Thanks!",['combinatorics']
2113767,"Find all integers $(x, y)$ such that $1 + 2^x + 2^{2x + 1} = y^2$ [duplicate]","This question already has answers here : Solve $1+2^x+2^{2x+1}=y^2$ for integers $x,y>0\,$ [IMO 2006 P4] (2 answers) Closed 5 years ago . Find all integers $$(x, y)$$ such that $$1 + 2^x + 2^{2x + 1} = y^2$$
So I basically used $$ f(x) = 1 + 2^x + 2^{2x + 1} = y^2$$ and created a table from 0 to 20. I got two pairs of integers: $$(0, \pm2)$$ and $$(4, \pm23)$$
I want to know if there's another method or if there are any other pairs of integers, or a generalisation.",['number-theory']
2113769,Frobenius norm of Hadamard product,"I have an $(n\times n)$ real matrix obtained through the Hadamard product, $H=A\circ B$, of two real $(n\times n)$ symmetric matrices $A,B$. All elements of $A$ are positive, while diagonal elements of $B$ are zero and the rest are non-negative. My question is if there is any way I can relate (possibly through some inequalities) the Frobenius norm of $H$ to the ones of $A, B$. The Frobenius norm of a real $(n\times n)$ matrix $M$ is defined as
$$
\| M \|_F = \sqrt{Tr\left(M^T M\right)} =\sqrt{\sum_{i,j=1}^n M_{i,j}^2} \,.
$$ Thanks!","['matrices', 'normed-spaces']"
2113770,Prove that there is a unique function of class $C^1$ such that for all $x \in \mathbb{R}$: $x^2f(x)^3 + 3x^3f (x)^2 + (5x^4 + 1) f (x) = \cos (f (x))$,"Prove that there is a unique function $f: \mathbb{R} \to \mathbb{R}$ of class $C^1$ such that for all $x \in \mathbb{R}$: $$x^2f(x)^3  + 3x^3f (x)^2 + (5x^4 + 1) f (x) = \cos (f (x))$$ My work: Let $y=f(x)$ and by using implicit theorem is idea to solve this:
$$F(x,y)=x^2y^3  + 3x^3y^2 + (5x^4 + 1)y - \cos (y)=0=F(x,f(x))$$
(F is class $C^1$) To show this we must show that: $\frac{\partial F}{\partial y}$ is regular matrix for all x. $$\frac{\partial F}{\partial y}(x,y)=3y^2x^2+6yx^3+5x^4+1-\sin(y)$$
$$\frac{\partial F}{\partial y}(x,y)=3y^2x^2+6yx^3+5x^4+1-\sin(y)=0$$ 
But for $(x,y)=(0,\pi/2)$ $F$ is 0 - which isn't good :/
What I did wrong?","['derivatives', 'partial-derivative', 'proof-verification', 'functional-analysis', 'multivariable-calculus']"
2113791,if : $abc=8 $ then : $(a+1)(b+1)(c+1)≥27$,"if : $$abc=8 :a,b,c\in \mathbb{R}_{> 0}$$ then : $$(a+1)(b+1)(c+1)\ge 27.$$ My try : $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+abc$$ $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+8, $$ then?","['inequality', 'a.m.-g.m.-inequality', 'multivariable-calculus', 'symmetric-polynomials', 'holder-inequality']"
2113802,Origin of diagrammatics for cumulant-moment relations?,"The exponential-log transformation of exponential generating functions (see OEIS A036040 and A127671 ) relate the classical cumulants to their associated moments. Who were some of the first to introduce diagrammatic illustrations of the relationship between the classical cumulants and moments? (A excellent discussion of the connections can be found in "" Three lectures on free probability "" by Novak and LaCroix.) Agata Fronczak in the "" The microscopic meaning of the grand potential ..."" mentions Mayer, Riddel, and Uhlenbeck in relation to the cluster expansion theorem.","['graph-theory', 'math-history', 'statistics', 'combinatorics', 'sequences-and-series']"
2113836,Find the limit of $\frac{n}{n^3+1}+\frac{2n}{n^3+2}+\dots+\frac{n\cdot n}{n^3+n}$,"Let $\{x_n\}_{n\geq 1}$ be defined as $x_n = \frac{n}{n^3+1}+\frac{2n}{n^3+2}+\dots+\frac{n\cdot n}{n^3+n}$. Then $\lim\limits_{n\to\infty} x_n$ is? I want to find limit of this problem by a very specific method . I am uploading the pic of that method and my attempt.
Please guide me as to how to take this method ahead.","['real-analysis', 'limits', 'calculus', 'limits-without-lhopital', 'analysis']"
2113877,What knot is this?,My headphone cables formed this knot: however I don't know much about knot theory and cannot tell what it is. In my opinion it isn't a figure-eight knot and certainly not a trefoil. Since it has $6$ crossings that doesn't leave many other candidates! What is this knot? How could one figure it out for similarly simple knots?,"['knot-theory', 'geometry']"
2113892,Pullback of a differential form by a local diffeomorphism,"Suppose I have to smooth oriented manifolds, $M$ and $N$ and a local diffeomorphism $f : M \rightarrow N$. Let $\omega$ be a differential form of maximum degree on $N$, let's say, $r$. How can I rewrite
  $$\int_N \omega$$
  in terms of the integral of the pullback of $\omega$ by $f$, $f^*\omega$? So, I know that when $f$ is a diffeomorphism, then
$$\int_N\omega = \pm \int_M f^*\omega$$
depending on whether $f$ preserves the orientation or not. But that's in part due to the fact that $f$ is bijective, but that condition is removed when assuming $f$ is a local diffeomorphism. So is there a nice way to write that integral in terms of the pullback?","['smooth-manifolds', 'differential-forms', 'pullback', 'integration', 'differential-geometry']"
2113914,"Space of linear operators complete, but target space not.","Let $X$, $Y$ be normed spaces and denote by $L(X,Y)$ the space of bounded linear operators from $X$ to $Y$. We know that $L(X,Y)$ is complete if $Y$ is. Could you provide me with an example where $L(X,Y)$ is complete, but $Y$ is not?",['functional-analysis']
2113928,Conditional probability and testing twice,"I am struggling with an interview quiz question which starts with a standard conditional probability part: To detect genetic defects, you are in charge of 
  performing a test. You know that: 1% of people have a genetic defect 99.9% of tests for the gene detect the defect (true positives) 5% of the tests give a positive result even though there is no defect (false positives) Given that the condition of a patient is known, the results of multiple tests are independent a)
  If a person gets a positive test result, what is the probability that he/she actually has the genetic defect? For a), I would argue that this is standard conditional probability and can be solved with Bayes Rule. Let's call the event ""$+$"" when a person tests positive, ""$d$"" when a person has the disease and ""$\bar{d}$"" when a person does not have the disease. Now we are looking for $P(d\mid+)$, so the conditional probability that someone actually has the disease when he tests positive. Given Bayes, that is \begin{align}
P(d\mid+) & = \frac{P(+\mid d)}{P(+)}\cdot P(d) = \frac{0.999}{P(+\mid d)\cdot P(d) + P(+\mid\bar{d})\cdot P(\bar{d})}\cdot 0.01 \\[10pt]
& = \frac{0.999}{0.999\cdot 0.01 + 0.05\cdot 0.99}\cdot 0.01 \approx 0.168.
\end{align} So the probability is roughly 17%. What is more complicated for me is b): b) If a person gets a positive result in his/her first test, what is the probability of having a positive result in his/her second test? I would argue that we are looking for $P(++|+)$, i.e. the probability that someone tests positive the second time under the condition that he tested positive the first time. So we can apply Bayes again and get $\frac{P(+|++)}{P(+)}\cdot P(++)$. I'd argue that $P(+|++)$ is always 1 and that $P(++) = P(+)\cdot P(+)$. We can cancel out one $P(+)$ and this leaves us with $P(++|+) = P(+)$. On the one hand this looks reasonable given the independence proclaimed, on the other hand it feels counter-intuitive that the probability would only be 6% (on the third hand, we are talking about statistics here and that never went well together with common sense for me :-)). Thoughts?","['bayesian', 'statistics', 'bayes-theorem']"
2113950,How come $\frac{1}{1-x}$ and $\frac{x}{1-x}$ have the same derivative?,"Maybe I'm just having a brain breakdown moment, but it seems weird to me that both functions have exactly the same derivative, namely $\frac{1}{(1-x)^2}$. Obviously I'm not disputing whether or not it's correct, but I'm looking for some sort of intuition/justification (geometric or otherwise) as to why this is indeed the case. I can take the derivatives manually so this is really not a question of not understanding the algebra, rather maybe having poor geometric intuition as to why the changing numerator as a function of $x$ doesn't impact the rate of change of the overall function.",['derivatives']
2113973,Vector spaces - Proving that intersection is distributive over summation of vector spaces,"The problem Define the sum of two vector spaces (often two subspaces of a common vector space) $A$ and $B$ as $A+B=\{a+b: a \in A, b \in B \}.$ Let $U,V,W$ be arbitrary vector spaces. I want to show that $$U \cap (V+W)=(U \cap V)+(U \cap W)$$ My approach By starting with $x \in (U \cap V)+(U \cap W)$ and applying the properties of vector spaces (closure under vector addition ) to end up with $x \in U \cap (V+W)$, I've shown that $(U \cap V)+(U \cap W) \subset U \cap (V+W)$. My problem is with the reverse, i.e. showing that $U \cap (V+W) \subset (U \cap V)+(U \cap W)$. Following the usual procedure, I started with: $x \in U \cap (V+W)$, which implies that $x \in U \land x \in V+W$. Now, $x \in V+W \implies x=y+z$ for some $y \in V$ and some $z \in W$. Again, $x=y+z \in U$. If we could show that $y$ and $z$ both comes from $U$, then we will be done. But unfortunately $y+z \in U$ doesn't necessarily imply that $y \in U$ and $z \in U.$ Am I missing some very basic properties of vector spaces that would make this trivial? Any help would be greatly appreciated. P.S. Just a dumb question. Does the result break down if we replace vector spaces by arbitrary sets in the above problem?","['linear-algebra', 'vector-spaces']"
2113981,Uniqueness Theorem,"I have a problem to the following exercise: Suppose that $f$ and $g$ are integrable functions that are $\mathcal F $  measurable. Suppose that we have a sequence of $\sigma$-fields {$\mathcal F_{n}$} such that $\mathcal F_{n} \subset \mathcal F_{n+1}$ and for which (1) $\int_{A}f(x)dx = \int_{A}g(x)dx$ for all A $\in \mathcal F_{n}$ Show that if $\mathcal F$ is the smallest $\sigma$-field that contains $\mathcal F_{n}$ for all n, then $f(x) = g(x)$ for all $x$ except a set of measure zero. There is also a Hint: Note that $\{x: f \gt g\} \in \mathcal F$. Next, it may be useful to show that equation (1) holds for all $A \in \mathcal F $. Here one may want to consider the set $\mathcal G$ of all A $\in \mathcal F$ for which we have equation (1) and then show that $\mathcal G$ is a $\sigma$-field. This was an Exercise given many years ago on my University but without a solution. But I'm interested in a proof of this problem. Can someone please give a nice proof. Thanks in advance","['probability-theory', 'calculus', 'integration', 'probability', 'measure-theory']"
2113999,What statistical methods would you recommend?,"I need to understand how a number of parameters contribute to decomposition in wood. The parameters in question are (1) temperature, (2) humidity and (3) exposure to sun light. We already know that the all contribute, but we don't onow exactly how they affect the specific decomposition process. My need to asses to what extent a 1% increase in either of these parameters will increase the decomposition process. At my disposal, I have a dataset covering some 30 datapoints where the degree of decomposition is recorded together with the three other parameters. What method would you recommend for this type of problem?","['complex-analysis', 'statistics']"
2114064,About the roots of cubic polynomial [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $\alpha, \beta, \gamma$ be the complex roots of the polynomial $P_3(x)=ax^3+bx^2+cx+d$. Is there any known formula for calculating
$\alpha^2 \beta+\beta^2 \gamma+ \gamma^2\alpha \; , \;
\alpha \beta^2+\beta \gamma^2+\gamma\alpha^2$
(in terms of $a,b,c,d$)? If no, can someone obtain it?","['algebra-precalculus', 'cubics', 'roots', 'polynomials']"
2114065,Limit $\lim_{n\to\infty} n^{-3/2}(1+\sqrt{2}+\ldots+\sqrt{n})=\lim_{n \to \infty} \frac{\sqrt{1} + \sqrt{2} + ... + \sqrt{n}}{n\sqrt{n}}$,"How do I find the following limit? $$
\lim_{n \to \infty} \frac{\sqrt{1} + \sqrt{2} + ... + \sqrt{n}}{n\sqrt{n}}
$$ The answer (from Wolfram ) is $\frac{2}{3}$, but I'm not sure how to proceed. Is this an application of the Squeeze theorem? I'm not quite sure.","['radicals', 'limits']"
2114107,Commutator of a matrix to the power of k,"The question asks me to show that $$[A,B^k] = \Sigma_{r=1}^k B^{r-1}[A,B]B^{k-r}$$ (A, B are nxn matrices) but I can't get even close. I suspect there's some definition of $B^k$ that I don't know but is required. I've tried expanding the RHS, to get
$$\Sigma_{r=1}^k B^{r-1}(AB-BA)B^{k-r}$$
$$= \Sigma_{r=1}^k B^{r-1}ABB^{k-r} - B^{r-1}BAB^{k-r}$$ So starting from the LHS, what can I do to $B^k$? Substituting in the diagonalised matrix such that $B^k = P\Lambda^k P^{-1}$ didn't get me anywhere and I'm not sure where the sum comes into it. The only sums I've seen in matrix calculations come from exp(B), but I don't think that's related. Any help or hints are much appreciated!","['matrices', 'abstract-algebra', 'ring-theory', 'noncommutative-algebra']"
2114115,How to integrate this arcsin integral? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How to integrate $$\int_a^1 \frac{\arcsin x}{\sqrt{x^2-a^2}} dx?$$ This integral appeared in Demkov Yu. N., Ostrovsky V. N., Berezina (Avdonina) N. B., ""Uniqueness of the Firsov Inversion Method and Focusing Potentials"", Sov. Phys. JETP 33, 867-70, (1971).",['integration']
2114119,Number theory/combinatorics problem?,"Given any set of $14$ (different) natural numbers, prove that for some $k$ ($1 ≤ k ≤ 7$)
there exist two disjoint $k$-element subsets $\{a_1,...,a_k\}$ and $\{b_1,...,b_k\}$ such that the sum of the reciprocals of all elements in each set differ by less than $0.001$, i.e. $|A−B| < 0.001$, where $A =$ the sum of the reciprocals of the elements of the first subset and $B =$ the sum of the reciprocals of the elements of the second subset. Note: this problem is from the $1998$ Czech and Slovak Math Olympiad","['number-theory', 'combinatorics']"
2114150,Solve the equation $2\arcsin x=\arcsin(\frac{3}{4}x)$,"$$2\arcsin x=\arcsin(\frac{3}{4}x)$$ so $x\in[-1,1]$
so we have: $2\arcsin x=y\Rightarrow\sin\frac{y}{2}=x$ and $\arcsin x=y \Rightarrow \sin y=\frac{3}{4}x\Rightarrow\frac{4}{3}\sin y=x$ ,
$y\in[-\frac{\pi}{2},\frac{\pi}{2}]$ $$\sin\frac{y}{2}-\frac{4}{3}\sin y=0$$$$\sin\frac{y}{2}-\frac{4}{3}\cdot 2\sin\frac{y}{2}\cos\frac{y}{2}=0$$ $$\sin\frac{y}{2}\cdot (1-\frac{8}{3}\cos\frac{y}{2})=0$$
Is it done properly at this point? 1.$\sin \frac{y}{2}=0\Rightarrow x=0$ And what else?",['trigonometry']
2114153,Proving statement about approximation of functions in completed measure spaces by functions belonging to the noncompleted space,"Let $(X,\mathcal{A},\mu)$ be a measure space,
  $(X,\mathcal{A}_\mu,\overline{\mu})$ be its completion and $f: X \to
 [-\infty,\infty]$ $\mathcal{A}_\mu$-measurable. Then there
  exist $\mathcal{A}$-measurable functions $g,h: X \to
 [-\infty,\infty]$ with $g \leq f \leq h$ and $g = h$
  $\mu$-a.e. Proof. First assume $f \in \Sigma^+$. Then $$f = \sum_{i = 1}^n a_i \chi_{A_i}$$
where $a_i \leq 0$ and $A_i \in \mathcal{A}_\mu$ for $i = 1,\dots,n$. Since $\mathcal{A}_\mu$ is the completion of $\mathcal{A}$, there exist $E_i,F_i \in \mathcal{A}$ such that $$E_i \subseteq A_i \subseteq F_i \qquad \text{and} \qquad \mu(F_i \setminus E_i) = 0$$
for $i = 1,\dots,n$. Then $$g := \sum_{i = 1}^n a_i \chi_{E_i} \qquad \text{and} \qquad h := \sum_{i = 1}^n a_i \chi_{F_i}$$
 have the desired properties. Now let $f$ be a $\mathcal{A}_\mu$-measurable nonnegative function. Then we find a sequence $(\varphi_n)_{n \in \mathbb{N}}$ of $\mathcal{A}_\mu$-measurable simple functions $0 \leq \varphi_n$ such that $\varphi_n \nearrow f$. By the first part we find sequences $(g_n)_{n\in\mathbb{N}}$ and $(h_n)_{n \in \mathbb{N}}$ of $\mathcal{A}$-measurable functions such that $$g_n \leq \varphi_n \leq h_n \qquad \text{and} \qquad g_n = h_n \>\mu\text{-a.e.}$$
for any $n \in \mathbb{N}$. Now I am a bit unsure how to proceed. Intuitively, we just take $$g := \lim_{n \to \infty} g_n \qquad \text{and} \qquad h := \lim_{n \to \infty} h_n$$ but I am not sure if this works. I mean it is not clear that $g_n$ and $h_n$ converge pointwise. Any help? Edit. Maybe better would be $$g := \limsup_{n \to \infty} g_n \qquad \text{and} \qquad h := \liminf_{n \to \infty} h_n$$",['measure-theory']
2114171,Expected Value and Variance of a Markov Chain,"Say I have a transition matrix $Q = \begin{bmatrix}
    1-p       & p \\
    p       & 1-p 
\end{bmatrix}$ where $0 < p < 1$ for a two state system with states $-1$ and $1.$ Define $X_i$ to be the value of the markov chain at time $i$ (so either $-1$ or $1$). If $\bar{X_i} = \frac{1}{n}\sum_{i = 1}^n X_i$ what is the $\mathbb{E}[\bar{X_i}]$ and $Var[\bar{X_i}]$? I've started off by tackling $\mathbb{E}[\bar{X_i}]$ but it seems to me that this answer depends on whether $n$ is even or odd. Note since this is a Markov chain there is not pairwise independence.","['markov-chains', 'probability']"
2114174,Roll an N sided die K times. Let S be the side that appeared most often. What is the expected number of times S appeared?,"For example, consider a 6 sided die rolled 10 times. Based on the following monte-carlo simulation, I get that the side that appears most will appear 3.44 times on average. n = 6
k = 10
samples = 10000
results = []

for _ in range(samples):
    counts = {s:0 for s in range(n)}
    for _ in range(k):
        s = randint(0, n-1)
        counts[s] += 1

    results.append(max(counts.values()))

print sum(results)/float(len(results)) But I can't figure out how to get this in a closed form for any particular N and K.","['monte-carlo', 'statistics', 'probability', 'expectation']"
2114230,Estimating the total cost of purchasing every item in a grocery store,"My friend and I were arguing for way too long the other night about how much it would cost you to buy every single thing in a grocery store. Our first go at it went something like this: Assume there are $N_{\text{items}}$ items per row in the grocery store, and let $p_{\text{avg}}$ be the average price for each item. Then say that there are $N_{\text{rows}}$ rows. Multiplying this out we get a total price $P_{\text{total}}$ as $$ P_{\text{total}} = N_{\text{items}}p_{\text{avg}}N_{\text{rows}}$$ The only issue is, there is a vast range of difference prices for items, and vast ranges of items per row, depending on what row you're in. For instance, if you go down the aisle with all the spices, there's a ton of items at very low cost, but the coffee aisle has a lot of items at very high cost; the meat aisle has relatively average number of items at a much higher cost, as well as the kitchen-utensils/kitchenware aisle etc. This got me thinking that there must be a better way to do an accurate estimation for a problem like this. Perhaps come up with some sort of intelligent distribution for prices (I was thinking maybe a log-normal distribution with a maximum around some arbitrary ""most-probable"" price, based on observation). And possibly do the same thing with the number of items per row? Estimating $N_{\text{rows}}$ is relatively straight forward since most grocery stores have somewhere between ten and twenty rows, so letting be $N_{\text{rows}}$ Gaussian centered at ten should take care of that, if we even want to get that fancy with that variable. Anyway, I'm not that savy with probability/statistics in the first place, so I thought I would ask you brilliant people: how would you most intelligently try to take a stab at this estimation?","['statistics', 'estimation', 'fermi-problems', 'recreational-mathematics']"
2114257,Complete the short exact sequence $0\to \mathbb Z\to A\to Z_n \to 0$,"I want to find all possible $A$ in the following SES: $$0\to \mathbb Z\to A\to Z_n \to 0$$ I know by structure theorem of finite generated abelian groups, $A\cong \mathbb Z\oplus \mathbb Z_d$. Then how can we find the relation between $d$ and $n$? What are two maps in the middle? Since this is an exercise in Hatcher's book at the beginning section of homology theory, a solution without using Ext will be much better.","['abstract-algebra', 'exact-sequence']"
2114319,Term-by-term differentiation of a sequence of functions without uniform convergence of derivatives,"Consider a sequence of functions $f_n(x)$, which converges to $f(x)$ pointwise on the interval $(0,1)$. The functions $f_n(x)$ and the limit function $f(x)$ are differentiable, with derivatives $f'_n(x)$ and $f'(x)$ respectively. Assume that the sequence $f'_n(x)$ converges to a function pointwise. Because convergence of the derivatives is not necessarily uniform, the usual theorem for term-by-term differentiability cannot be applied to give $\lim_{n \rightarrow \infty} f'_n(x) = f'(x)$. However, If it is known that both $f_n(x)$ and $x f'_n(x)$ converge uniformly on $[0,1]$, does that imply that $\lim_{n \rightarrow \infty} f'_n(x) = f'(x)$ on $(0,1)$? I think the answer is yes. But it strikes me a bit that I get the desired result side-stepping the need for uniform convergence of $f'_n(x)$. So I'll post my proof and please let me know if there's any mistake in it. Or if the result is well-known, can you point me to a relevant theorem?","['uniform-convergence', 'sequences-and-series', 'calculus', 'functions', 'convergence-divergence']"
2114326,Formalising false positives as conditional probability,"I'm having a hard time trying to undestand this. I'm doing an exercise where I have to formalise 'your test has a false-positive rating of 5%'. If $B$ means that the test is positive, and $A$ means that a given person has the disease, then is $P(A^{C}|B)=5\%$, or $P(B|A^{C})=5\%$? Intuitively, I would have picked the second option. A false-positive is when I'm testing a person who doesn't have the disease, but still get a positive result.
However, this website http://vassarstats.net/bayes.html , says otherwise. According to it, a false positive would mean the first. I don't get this. Could someone please elaborate. Edit: I thought that after reading Bram28's answer I understood it, but actually I'm no less confused than before. For reference, I am referring to exercise 3.6 on page 19 in this book: http://www.karlin.mff.cuni.cz/~lachout/Vyuka/O-Sem/JacodProtter2004.pdf . According to Bram28's answer, false positive means $P(A^{C}|B)$. But this fixes $P(A|B)=1-P(A^{C}|B)$, which is what is asked to be determined in the question, without needing the 'accuracy' which is also given in the exercise. Does somebody understand what is happening there?","['probability-theory', 'probability', 'statistics']"
2114366,Changing a Differential Form into Spherical Polars,"I am currently trying to integrate:$$\omega_{a,b,c} = \frac{(x-a)dy\wedge dz+(y-b)dz\wedge dx+(z-c)dx\wedge dy}{[(x-a)^2+(y-b)^2+(z-c)^2]^{3/2}}$$ over the unit sphere centered at $(a,b,c)$, with the standard orientation. I am supposed to show that this integral gives $-4\pi$. I am confused by how the spherical coord system of this differential form would look. I know that the form replacing $(x-a)$ with $x$, same with $y$ and $z$ gives: $$\sin\phi\, d\theta\wedge d\phi$$Am I allowed to use the same parametrization? Or would this give a completely different parametrization? I am looking for a way without using Stokes' Theorem, as this is required for another part of the question. Thank you.","['differential-forms', 'stokes-theorem', 'integration', 'manifolds', 'differential-geometry']"
2114382,Is this condition necessary for a function $f : \mathbb{R}^m \to \mathbb{R}^n$ to be differentiable?,"Consider the following definitions: A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if there exists a linear map $T : \mathbb{R}^m \to \mathbb{R}^n$ such that $$\lim_{v \to 0} \dfrac{f(a+v) - f(a) - T(v)}{\|v\|} = 0$$ The directional derivative of $f$ at $a$ in the direction of $v \in \mathbb{R}^m$ is $$\dfrac{\partial f}{\partial v}(a) = \lim_{t \to 0} \dfrac{f(a+tv) - f(a)}{t}$$ Now, it is easy to show that $\dfrac{\partial f}{\partial (\alpha v)}(a) = \alpha\dfrac{\partial f}{\partial v}(a)$ for any $v$ and $\alpha \neq 0$ (if it exists). It is also easy to show that if $f$ is differentiable at $a$, then $\dfrac{\partial f}{\partial v}(a)$ exists for all $v$, with the special property that $$\forall \, v,w \in \mathbb{R^m} \qquad \dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$$ So far so good. Now, there is an equivalent definition of differentiability at a point, as follows: (Alternative Definition) A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if all the following conditions hold: For all $v \in \mathbb{R}^m$ the directional derivative $\dfrac{\partial f}{\partial v}(a)$ exists. For all $v,w \in \mathbb{R^m}$, we have $\dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$. $\lim_\limits{v \to 0} \dfrac{f(a+v) - f(a) - \frac{\partial f}{\partial v}(a)}{\|v\|} = 0$ I also understand that this definition is equivalent to the first one. So far so good. But is the second bullet on the Alternative Definition necessary? Or does it follow from the first and third bullets?","['multivariable-calculus', 'real-analysis', 'partial-derivative', 'derivatives']"
2114395,Prove a function that is concave and convex is affine,"I am trying to prove the following: If a function $f$ is concave and convex, then it is affine. Since $f$ is convex we have $f(\lambda x +(1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y))$ and $f(\lambda x +(1-\lambda)y) \geq \lambda f(x) + (1-\lambda)f(y)) $.  Thus we know $f(\lambda x +(1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y))$. This shows that $f$ is linear and hence it is affine. Is this proof right? Could anyone tell me if there is another way to prove this? Thanks!","['optimization', 'functions']"
2114424,"Parametrization for intersection of sphere and plane. $x^2+y^2+z^2=81$, $x+y+z=15$.","How can I find the parametrization of the curve given by the intersection of the sphere $x^2+y^2+z^2=81$ and the plane $x+y+z=15$. Also it says that this intersection is a circle (clearly) with center $(5,5,5)$, and that $(7,4,4)$, $(4,7,4)$, $(4,4,7)$ are points on the curve (I don't know if this information is useful for the parametrization) My try was to substitute $z$ in the sphere equation with the expression obtained for $z$ in the plane equation, but I can't conclude this way.","['algebra-precalculus', 'parametric', 'multivariable-calculus']"
2114429,Show that $x^2+y^2+z^2=999$ has no integer solutions,"The question is asking us to prove that $x^2+y^2+z^2=999$ has no integer solutions. Attempt at a solution: So I've noticed that since 999 is odd, either one of the variables or all three of the variables must be odd. If I assume that only one variable is odd, I can label the variables like this: 
$$x=2k_1+1$$
$$y=2k_2$$
$$z=2k_3$$ By substituting, and doing some algebra, I can conclude that $k_1^2+k_2^2+k_3^2+k_1=249.5$, which is not possible since all $k_i\in\Bbb Z$. If all three are odd, I can rename the variables like this: 
$$x=2k_1+1$$
$$y=2k_2+1$$
$$z=2k_3+1$$
Eventually I conclude that $k_1^2+k_2^2+k_3^2+k_1+k_2+k_3 = 249$, but I don't know where to go from there. An alternative I've considered is brute-forcing it, but I'd rather avoid that if I can. Any assistance here would be greatly appreciated.","['diophantine-equations', 'square-numbers', 'abstract-algebra', 'sums-of-squares', 'elementary-number-theory']"
2114440,"Prove that two sets A and B with $A \cap B=\emptyset$, $\sup A = \sup B$, $\sup A \notin A$ and $\sup B \notin B$ cannot exist.","I have to show that it is either possible or impossible to have two such sets. I understand intuitively that they cannot exist (correct me if I'm wrong, please), but can't seem to figure out how to even put it in words, let alone prove it. I was thinking of assuming all of these things and showing that it leads to a contradiction, but I got stuck several times. Could someone please give me some pointers as to how to begin?","['real-analysis', 'proof-writing', 'supremum-and-infimum', 'elementary-set-theory']"
2114463,Function that satisfies $f(x+ y) = f(x) + f(y)$ but not $f(cx)=cf(x)$ [duplicate],"This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 7 years ago . Is there a function from $ \Bbb R^3 \to \Bbb R^3$ such that $$f(x + y) = f(x) + f(y)$$ but not $$f(cx) = cf(x)$$ for some scalar $c$? Is there one such function even in one dimension? I so, what is it? If not, why? I came across a function from $\Bbb R^3$ to $\Bbb R^3$ such that $$f(cx) = cf(x)$$ but not $$f(x + y) = f(x) + f(y)$$, and I was wondering whether there is one with converse. Although there is another post titled Overview of the Basic Facts of Cauchy valued functions, I do not understand it. If someone can explain in simplest terms the function that satisfy my question and why, that would be great.","['linear-algebra', 'nonlinear-system']"
2114509,Limit of a sequence by Cauchy second test,The sequence is $$ \left[ \bigg(1+\frac{1}{n}\bigg)\bigg(1+\frac{2}{n}\bigg)\bigg(1+\frac{3}{n}\bigg)\cdots\bigg(1+\frac{n}{n}\bigg) \right]^{1/n} $$ as $n$ goes to infinity. By Cauchy second test it's pretty clear that it's limit will be equal to $1+\frac{n}{n}$ which is $2$. But the answer is $4/e$. I don't know what part I'm doing wrong.,['limits']
