question_id,title,body,tags
1651957,Evaluating the limit $\lim_{x \to 0}\left(x+e^{\frac{x}{3}}\right){}^{\!\frac{3}{x}}$,"$$y=\left(x+e^{\frac{x}{3}}\right)^{\frac{3}{x}}$$ I'm looking at this equation, and need to solve for the limit as $ \to 0$. I've graphed it, and know the solution is $e^4$, but am clueless as to the steps to actually solve this. (Note, I am an adult working as a math aide in a high school. I help students at Algebra, Trig, Geometry, and Intro to Calculus. 35 years out of HS myself, there are clearly some things I need to brush up on. i.e. I know my limits. Pun intended.) Yes, L'Hopital is fine. The student is in a BC calc class)",['limits']
1652000,A Compact Hausdorff Space with no Manifold Structure? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is an example of a compact Hausdorff space that cannot be given the structure of a (i)   differential manifold (ii)  topological manifold?","['examples-counterexamples', 'differential-topology', 'smooth-manifolds', 'manifolds', 'general-topology']"
1652006,What is the derivative of $z^{-1}$ with respect to $\bar{z}$?,"I asked a question here a few days ago but it wasn't answered and, as often happens with me, in trying to answer it myself I just confused myself out of understanding what I thought I knew. What is the derivative of $z^{-1}$ with respect to $\bar{z}$ ie $\frac{\partial z^{-1}}{\partial \bar{z}}$. Does such a question even make sense? More specifically I have a one form on the complex place without the origin $z^{-1}dz$ and wish to get a two form by taking the exterior derivative. This form is clearly not closed so I should get a non zero two form... but what is it?","['homology-cohomology', 'complex-analysis', 'complex-numbers', 'complex-geometry']"
1652036,upper bounding alternating binomial sums,"So we know that
$\large\sum_\limits{i=0}^t\dbinom{m}{i}\dbinom{n-m}{t-i}=\dbinom{n}{t}$ by a simple counting argument. Now is there any bound on the quantity $\large\sum_\limits{i=0}^t(-1)^i\dbinom{m}{i}\dbinom{n-m}{t-i}$? Can we show any non trivial upper bound on this quantity other than $\dbinom{n}{t}$?","['combinatorics', 'inequality', 'summation', 'binomial-coefficients']"
1652039,"If a fair six-sided die is rolled four times, in how many outcomes is the value of each roll at least as large as the value of the previous roll?","Suppose you roll a fair 6-sided die four times. Let C be the event that the value of each roll is at least as large as the value of the previous roll. What is the probability of C? I know that $$\omega = 6^4 = 1296$$ I also know that to get P(C),  I need to divide C by $\omega$ But what is the fastest/simpler way to get C? I could write all the sets that would qualify but that would A: be time consuming. B: error prone. Is there a formula to find C in this case?",['probability']
1652072,What type of discontinuity is found in this graph?,"$$
f(x) = \begin{cases}
       \dfrac{1}{x} && \text{when $x > 0$}\\
       4 && \text{when $x < 0$}
       \end{cases}
$$ What type of discontinuity is present when $f(0)$ ? I'm currently taking calculus and from what I've learnt, there are three kinds of discontinuities. Point/removable discontinuity, infinite discontinuity and jump discontinuity. I know definitely that the graph above does not have a point discontinuity. However, I'm unsure of whether the graph above contains an infinite discontinuity or jump discontinuity. Can someone provide me an answer with an explanation as to why? To generalize this question, the situation could be:
As $x$ approaches $a$ from one side, there is a finite limit. As $x$ approaches $a$ from the other side, $y$ approaches positive or negative infinity. However, $f(a)$ is non-existent. What type of discontinuity can be found in $f(a)$?","['continuity', 'calculus', 'functions']"
1652080,Rigorous Derivation of Metropolis-Hastings Transition Density,"The Metropolis-Hastings MCMC algorithm is as follows.  Set $X_0$ to some initial value in the support of the target density $f$ and choose a proposal density $q(y \mid x)$; a density in $y$ for each $x$ in the support of $f$.  For each time $t \in \mathbb{N}$, do the following: Generate a candidate point $Y_t \sim q(\cdot \mid X_t)$ Set 
$$
X_{t+1} = 
\begin{cases}
Y_t, \text{ with probability } \rho(X_t, Y_t) \\
X_t, \text{ with probability } 1 - \rho(X_t, Y_t),
\end{cases}
$$
where 
$$
\rho(x,y) = \min\left\{\frac{f(y)q(x \mid y)}{f(x)q(y \mid x)}, 1\right\}.
$$ These two steps define a Markov chain $\{X_t\}$ on an uncountable state space $\mathcal{S}$.  Our professor wrote the transition kernel $\pi$ for this Markov chain as
$$
\pi(x,y) = q(y \mid x)\rho(x,y) + (1 - r(x))\delta_x(y),
$$
where $r(x) = \int_{\mathcal{S}} q(y \mid x )\rho(x,y)\, dy$. I was unable to verify for myself this was indeed the transition density.  Is there a rigorous derivation/proof of this available? I know I need to show the following.  Let $f_t$ be marginal density of the chain at time $t$.  Denote the conditional distribution $P( \cdot \mid X_t = x) : \mathcal{B}(\mathcal{S}) \to [0,1]$; a probability measure on the state space for each $x \in \mathcal{S}$.  For any sets $A,B \in \mathcal{B}(\mathcal{S})$ the conditional distribution is defined so as to satisfy
$$
P(X_{t+1} \in A, X_t \in B) = \int_B f_t(x)P(X_{t+1} \in A \mid X_t = x) \, dx.
$$
Thus I need to show
$$
P(X_{t+1} \in A, X_t \in B) = \int_B f_t(x) \left(\int_A \pi(x,y) \,dy \right)\, dx.
$$
For this I am at sea.","['markov-chains', 'probability-theory', 'markov-process', 'probability']"
1652086,A question my proof about the line of curvature,"I am working on Exercise 2.4.4 of Differential Geometry and Its Application. The problem statement and my work is available at this link . At the end of my proof, I claimed that $ S_p(\alpha') $ is both on the plane and the tangent plane, so it must be $ k \alpha' $, at this point I am unsure if that is correct. I think so because $ \alpha' $ must be on the plane and on the tangent plane, but could that two planes be the same?",['differential-geometry']
1652096,Asymptotic distribution for non differentiable functions of estimators,"is there kind of a standard tool to derive the distribution of $f(\theta)$ if f is non differentiable (so no Delta Method available) and $\theta$ is asymptotically normal distributed?
Thanks a lot!","['derivatives', 'probability-theory']"
1652107,Convergence and value of infinite product $\prod^{\infty}_{n=1} n \sin \left( \frac{1}{n} \right)$?,"Since the limit $\frac{\sin(x)}{x}=1$ for $x \rightarrow 0$, I wondered about the infinite product: $$\prod^{\infty}_{n=1} n \sin \left( \frac{1}{n} \right)=\sin(1) \cdot 2 \sin\left( \frac{1}{2} \right) \cdot 3 \sin\left( \frac{1}{3} \right) \dots$$ By numerical experiment in Mathematica it seems to converge, even if very slowly (I mean to non-zero value): $$P(14997)= 0.755371783$$ $$P(14998)= 0.755371782$$ $$P(14999)= 0.755371782$$ $$P(15000)= 0.755371781$$ I can prove the convergence by integral test for the series: $$\sum^{\infty}_{n=1} \ln\left( n \sin \left( \frac{1}{n} \right) \right)$$ $$\int^{\infty}_{1} \ln\left( x \sin \left( \frac{1}{x} \right) \right) dx=\int^{1}_{0} \frac{1}{y^2} \ln \left( \frac{\sin (y)}{y} \right) dy=-0.168593$$ I think the integral test can work with negative function as long as it's monotone, otherwise I can just put the minus sign before the infinite sum. By the way, this is a related question about the convergence of the sum above. But I'm more interested in the infinite product itself. I'm not sure if the value of this infinite product can be found and how to go about it. Is it zero or not? Any thoughts would be appreciated","['infinite-product', 'sequences-and-series', 'limits']"
1652112,Definition of trajectory,"I am writing something that involves comparing the solutions of many different differential equations, and I need precise definitions of the terms trajectory and solution curve. Given a dynamical system $\dot{\textbf{x}}=F(\textbf{x})$ in $n$ dimensions, and an initial condition $\textbf{x}(t)=\textbf{0}$ Would the trajectory be a mapping from time to the phase plane? $T:\mathbb{R}\mapsto \mathbb{R}^n$?
Would the trajectory be a set of points on the phase plane?
Would the trajectory be a set of points in $\mathbb{R}^{n+1}$, containing both time $t$ and the vector $\textbf{x}(t)$? Also is the word trajectory a synonym of solution curve? how do they differ?","['ordinary-differential-equations', 'dynamical-systems', 'definition']"
1652121,Reduction formula for $\int \tan^n (2x) \ dx$,"Establish a reduction formula for $$\int \tan^n (2x) \ dx.$$ My attempt: let $I_{n}=\int \tan^n (2x) \ dx$ , $$=\int \tan^2 (2x) \tan^{n-2} (2x)dx$$ $$=\int (\sec^2 (2x)-1)\tan^{n-2}(2x)dx$$ $$=\int \sec^2(2x)\tan^{n-2}(2x)dx-\int \tan^{n-2}(2x)dx$$ Let $u=\tan (2x), du=2\sec^2 (2x) \ dx$ : $$=\frac{1}{2}\int u^{n-2}du-I_{n-2} =\frac{1}{2} \frac{u^{n-1}}{n-1}-I_{n-2} =\frac{1}{2n-2}\tan^{n-1}(2x)-I_{n-2}$$ Is it correct?","['integration', 'trigonometry', 'calculus', 'solution-verification']"
1652122,Let A be a square matrix such that $A^3 = 2I$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $A$ be a square matrix such that $A^3 = 2I$ i) Prove that $A - I$ is invertible and find its inverse ii) Prove that $A + 2I$ is invertible and find its inverse iii) Using (i) and (ii) or otherwise, prove that $A^2 - 2A + 2I$ is invertible and find its inverse as a polynomial in $A$ $I$ refers to identity matrix. Am already stucked at part i). Was going along the line of showing that $(A-I)([...]) = I$ by manipulating the equation to $A^3 - I = I$ and I got stuck... :(","['matrices', 'linear-algebra', 'inverse']"
1652142,Maximizing the probability of a poll prediction,"Using the central limit theorem, I was able to find out the first part of this question. However, part b is eluding me. How do I, in general, find a value for $n$ such that we can ensure the probability a poll is correctly predictive (ie. 95% certainty a majority would vote for candidate A?)? I'm sure it has something to do with CTL or LLN, but it's been elusive. The equations at the bottom were my best guess as to finding some relationship between the probability and $n$, but I cannot solve the equation. Situation : In a large voting population, 56% of the voters prefer candidate A to candidate B. The true percentage is not known to either candidate, and candidate A commissions a poll of 100 voters to determine whether or not he will win. a) Using the Central Limit Theorem, determine the probability that the poll will correctly predict his winning the election. \ answer. Let $X_i$, $i = 1,...,100$ represent the outcome of the vote of the $i$-th person polled, where $X_i = 1$ if they prefer candidate A (with probability 0.56) and $X_i = 0$ otherwise. Let $S_n$  be the sum of all $X_i$. Then the probability that the poll will correctly predict candidate A winning the election is 
\begin{align*}
P(S_n > 50) &= 1 - P(S_n \leq 50)
\end{align*}
It can be shown that $E[S_n] = n\mu = 56$ and $SD(S_n) = \sigma\sqrt{n} = 7.48$. Using the central limit theorem, we can approximate $Z = \frac{S_n - n\mu}{\sigma\sqrt{n}}$ as the Standard Normal, so 
\begin{align*}
P(S_n \leq 50) &= P(Z \leq \frac{50 - n\mu}{\sigma\sqrt{n}}) \\ 
&= P(Z \leq \frac{50 - 56}{7.48}) \\ 
&= \Phi(-0.8021) \\ 
&= 0.2119
\end{align*}
Finally, the probability that the poll will correctly predict candidate A winning the election is $P(S_n > 50) = 1 - P(S_n \leq 50) = 0.7881$. b) What sample size would be required to ensure at least a 95% chance of a correct prediction? answer. From part a, we can use the following inequality to solve for $n$: 
\begin{align*}
0.95 &\leq 1 - P(S_n \leq 50) \\
0.05 &\geq P(S_n \leq 50) \\
&\geq \Phi(\frac{50 - n(0.56)}{(0.784)\sqrt{n}})
\end{align*} Note: I  also attempted to use Markov's inequality to try to come up with a solution, but that proved just as difficult. EDIT: Thanks commenter Andre for pointing out that the last equation should use $P(S_n \leq n/2)$ instead of using 50. The equation is then 
$$
0.95 \leq 1 - P(S_n \leq n/2)
$$","['law-of-large-numbers', 'normal-distribution', 'statistics', 'central-limit-theorem', 'probability']"
1652182,Extrinsic Curvature of Surface of Codimension > 1,"We can define the extrinsic curvature of a codimension-one surface as $$K_{ab} = q_a^{\phantom{a}c} q_b^{\phantom{b}d} \nabla_c n_d,$$ where $n^d$ is the normal vector to the hypersurface and $q_{ab} = g_{ab} - n_a n_b$ is the induced metric (with $g_{ab}$ the ambient metric).  Here and below I'll assume the ambient geometry is Riemannian so that everything is spacelike.  Now, the above expression can also be written in terms of the Lie derivative as $$K_{ab} = \frac{1}{2}\mathcal{L}_n q_{ab}.$$ For a surface of codimension $n$, the extrinsic curvature can be defined in terms of the induced metric as $$K^c_{ab} = -q_a^{\phantom{a}d} q_b^{\phantom{b}e} \nabla_d q_e^{\phantom{d}c},$$ where now the induced metric can be written as $$q_{ab} = g_{ab} - \sum_i X^{(i)}_a X^{(i)}_b,$$ where the sum is over $n$ mutually orthogonal unit vectors normal to the surface.  It is relatively simple to show that for any such unit normal vector $X^a$, $$X_c K^c_{ab} = q_a^{\phantom{a}c} q_b^{\phantom{b}d} \nabla_c X_d.$$ My question is the following: can the above contraction be rewritten in terms of the Lie derivative, as it can in the codimension-one case?  Specifically, is it true that $$X_c K^c_{ab} = \frac{1}{2} \mathcal{L}_X q_{ab}?$$ I've been trying to derive the above expression for a while, but I've had no success.  I'd like to know if I've just been screwing up my algebra, or if it's just not true.","['curvature', 'differential-geometry', 'general-relativity']"
1652222,Find $\lim_{n\rightarrow\infty}\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n$,"Find the limit as $n\rightarrow\infty$ of $\left(1-(1-\exp(tn^{-\frac{1}{v}}))^v\right)^n$, where $t\in(-\infty,0)$, and $v\in(0,1)$. Remarks:
A non-trivial limit does exist! - verified numerically.
I would like to use a similar idea to $\lim_{n\rightarrow\infty}\left(1-\frac{t}{n}\right)^n=\exp(-t)$. This standard result can be proved, for example, by taking the logarithm and using l'Hopitals rule. The method does not seem to work in this case however due to problems differentiating.",['limits']
1652228,Show that $f(x):=\frac{2x^3+x^2+x\sin(x)}{(\exp(x)-1)^2}$ is continuously extendable to $x_0=0$.,"What I know If $\lim\limits_{x \to x_0}f(x) := r$ exists, we can create a new function $\tilde f(x) = \begin{cases} f(x) &\text{if }x\in\mathbb{D}\setminus x_0 \\ 
r & \text{if }x = x_0 \end{cases}$ which is then the continously extended version of $f$. What my problem is I am struggling with $\lim\limits_{x\to 0}\frac{2x^3+x^2+x\sin(x)}
{(\exp(x)-1)^2}:=r$. I tried using L'Hôpital's rule, because I noticed that  both denominator and numerator would equal to $0$ if I plug in $0$. This unfortunately didn't help at all, because you can derive those expressions as often as you want, without making your life easier. I deliberately phrased this question in regards of solving continuity problems like this, because I think that calculating the limit in this subtask of an actual first term exam is too hard. There has to be another way of solving this continuity issue, without having to calculate the limit. If there's no way around finding $r$, then there has to be an obvious trick that I am unaware of. Help is greatly appreciated!","['continuity', 'calculus', 'limits']"
1652236,Deducing the series expansion of $\arctan(x^2)$ via the series expansion of $\arctan(x)$ at $x=0$,Comparing the series expansion of $\arctan(x^2)$ and $\arctan(x)$ at $x=0$ it looks like one can take the result from $\arctan(x)$ and replace each $x$ with $x^2$ to deduce the series expansion of $\arctan(x^2)$. Is this just true in this specific case or is this approach generally valid? Do you have any other examples or counter-examples for this observation?,"['real-analysis', 'taylor-expansion', 'power-series', 'calculus']"
1652252,Strange behavior of infinite products $\prod^{\infty}_{n=1} \ln (1+ \frac{1}{n} )^n$ and $\prod^{\infty}_{n=1} \ln (1+ \frac{1}{n} )^{n+1}$,"There are two expressions marking the lower and upper bounds for number $e$: $$\left(1+\frac{1}{n} \right)^n \leq e \leq \left(1+\frac{1}{n} \right)^{n+1}$$ Naturally, I wanted to know if infinite products of their logarithms converge to the same value. I was greatly surprised to find that not only do they not converge to the same value, but one of them converges to zero and the other to infinity: $$\prod^{\infty}_{n=1} \ln \left(1+ \frac{1}{n} \right)^n=0$$ $$\prod^{\infty}_{n=1} \ln \left(1+ \frac{1}{n} \right)^{n+1} \rightarrow + \infty$$ On the other hand their product (or equally, the infinite product of their geometric means) converges, but not to $1$: $$\prod^{\infty}_{n=1} \ln \left(1+ \frac{1}{n} \right)^{n} \ln \left(1+ \frac{1}{n} \right)^{n+1}=\prod^{\infty}_{n=1} n(n+1) \ln^2 \left(1+ \frac{1}{n} \right) \rightarrow P$$ Mathematica gives the following values (since $P_n$ is decreasing, it's certainly less than $1$): $$P(14999)=0.921971686261$$
$$P(15000)=0.921971685920$$ The convergence (or divergence) can be proved using the corresponding series and the integral test: $$\sum^{\infty}_{n=1} \ln \ln \left(1+ \frac{1}{n} \right)^n $$ $$\int^{\infty}_{1} \ln \left( x \ln \left(1+ \frac{1}{x} \right)\right) dx =\int^{1}_{0} \frac{1}{y^2} \ln  \left( \frac{\ln \left(1+ y \right)}{y} \right) dy\rightarrow - \infty $$ This integral does not converge (according to Wolframalpha ) $$\sum^{\infty}_{n=1} \ln \ln \left(1+ \frac{1}{n} \right)^{n+1} $$ $$\int^{\infty}_{1} \ln \left( (x+1) \ln \left(1+ \frac{1}{x} \right)\right) dx =\int^{1}_{0} \frac{1}{y^2} \ln \left( \left(1+ \frac{1}{y} \right) \ln \left(1+ y \right) \right) dy\rightarrow + \infty $$ This integral also does not converge (according to Wolframalpha ) Finally, the 'mean' infinite product gives (see Wolframalpha ): $$\int^{1}_{0} \frac{1}{y^2} \ln \left( \frac{1}{y} \left(1+ \frac{1}{y} \right) \ln^2 \left(1+ y \right) \right) dy=-0.0569274$$ So, this infinite product converges, but not to $1$ according to Mathematica. Is there any explanation for all this? Is it connected to the special 
  properties of $e$?","['infinite-product', 'definite-integrals', 'sequences-and-series', 'limits']"
1652258,Finding $a^5 + b^5 + c^5$,"Suppose we have numbers $a,b,c$ which satisfy the equations
  $$a+b+c=3,$$
  $$a^2+b^2+c^2=5,$$
  $$a^3+b^3+c^3=7.$$ How can I find $a^5 + b^5 + c^5$? I assumed we are working in $\Bbb{C}[a,b,c]$. I found a reduced Gröbner basis $G$: $$G = \langle a+b+c-3,b^2+bc+c^2-3b-3c+2,c^3-3c^2+2c+\frac{2}{3} \rangle$$ I solved the last equation for $c$ and got 3 complex values. When I plug into the 2nd equation $(b^2+bc+c^2-3b-3c+2)$ I get a lot of roots for $b$, and it is laborious to plug in all these values. Is there a shortcut or trick to doing this? The hint in the book says to use remainders. I computed the remainder of $f = a^5 + b^5 + c^5$ reduced by $G$:
$$\overline{f}^G = \frac{29}{3}$$ How can this remainder be of use to me? Thanks. (Note: I am using Macaualay2)",['algebraic-geometry']
1652285,Maximal interval of solutions existence: $x'(t)=-x(t)+\sin x(t)+t^3$,"$x'(t)=-x(t)+ \sin x(t)+t^3$  in $\mathbb{R}$ I consider the function: $$ f(t,x)=-x+\sin x + t^3 $$
$$\frac{\partial f}{\partial x}=\cos x-1$$ I see that: $$\left| \frac{\partial f}{\partial x} \right| < 2$$ So, the function $f$ is a globally Lipschitz-function, because the partial derivative (considering the second variable) is bounded. For the global existence theorem, the maximal interval of  solutions existence  is $(-\infty,\infty)$ Is it correct? Thanks!","['real-analysis', 'ordinary-differential-equations', 'analysis']"
1652290,The knowledge of $n=n(s)$ can be used to determine the curvature $k(s)$ and the torsion $\tau (s)$,"Question: Show that the knowledge of the vector function $n=n(s)$ of a curve $\alpha:I\rightarrow \mathbb{R^3}$ with nonzero torsion everywhere, determines the curvature $k(s)$ and the torsion $\tau (s)$ of $\alpha$. Notes: $n$ is the normal versor to $\alpha$. Attempt: I tried using Frenet-Serret formulas, and then using the vector product between $n$ and $n'$, but it seems like I can't get to any result.",['differential-geometry']
1652310,Finding the Determinant of a particular Matrix,"I've come across the question of finding the determinant of the $(n\times n)$ matrix, given by $$A:=
\begin{pmatrix}
x & 1 & 1 & \dots & 1 \\
1 & x & 1 & \dots & 1 \\
\vdots && \ddots &  & \vdots \\
1  & \dots  & \dots & x & 1\\ 
1 &\dots & \dots & 1&x^2 
\end{pmatrix}$$ for all $x \in \mathbb R$ and for all $n\geq2$. I know that the answer is $$\det A =(x-1)^{n-1} \cdot (x^2+(n-1) x+n-1) \tag 1$$ and I can prove it by induction if I'm allow to make some hand-wavy assumptions, which I'm not allowed to do because it was an exam question of a pure maths class (linear algebra). However I think that I'm missing something because without having a computer algebra system, coming up with the equation $(1)$ is hard enough, let alone the two inductions I need to do in order to show the equation $(1)$ with some hand-wavy assumptions, which makes the question way too hard for a 2 hour exam. Is there an easier way to calculate this determinant?","['matrices', 'problem-solving', 'linear-algebra', 'determinant']"
1652314,Why is the image of an algebraic group by a morphism also an algebraic group?,"Let $K$ be a field and $G\subset K^m$ an (affine) algebraic group. If $\varphi:G\rightarrow (K^n,+)$ is a morphism of algebraic groups, why is $\varphi(G)$ is an algebraic group ? I would say for instance that $\varphi(G)$ is constructible by Chevalleys Theorem (if $K$ is algebraically closed, isn't it ?), and a group so closed... hence an algebraic variety, but that seems to involve to much technicalities, and does not hold if $K$ is not algebraically closed (does it ?). A more direct way to see this ?","['algebraic-groups', 'algebraic-geometry']"
1652322,Jump Diffusion Infinitesimal generator,"I have this difussion process $dX(t)=\mu X(t)dt+\sigma X(t)dW(t)+u X(t)  dN(t),\qquad X(0)=x > 0$ where $W(t)$ is a Brownian Motion and $N(t)$ is a Poisson process. And I need to know the infinitesimal generator but I can't . Can someone help me? $\mu,\sigma,u$ are constants. Thank you so much","['stochastic-processes', 'ordinary-differential-equations', 'stochastic-differential-equations']"
1652341,Convergent sequence of irrational numbers that has a rational limit.,Is it possible to have a convergent sequence whose terms are all irrational but whose limit is rational?,"['real-analysis', 'examples-counterexamples', 'sequences-and-series', 'irrational-numbers']"
1652349,The sum of the following infinite series $\frac{4}{20}+\frac{4\cdot 7}{20\cdot 30}+\frac{4\cdot 7\cdot 10}{20\cdot 30 \cdot 40}+\cdots$,"The sum of the following infinite series $\displaystyle \frac{4}{20}+\frac{4\cdot 7}{20\cdot 30}+\frac{4\cdot 7\cdot 10}{20\cdot 30 \cdot 40}+\cdots$ $\bf{My\; Try::}$ We can write the given series as $$\left(1+\frac{4}{20}+\frac{4\cdot 7}{20\cdot 30}+\frac{4\cdot 7\cdot 10}{20\cdot 30 \cdot 40}+\cdots\right)-1$$ Now camparing with $$(1+x)^n = 1+nx+\frac{n(n-1)x^2}{2!}+\cdots$$ So we get $\displaystyle nx=\frac{4}{20}$ and $\displaystyle \frac{n(n-1)x^2}{2}=\frac{4\cdot 7}{20\cdot 30}$ So we get $$\frac{nx\cdot (nx-x)}{2}=\frac{4\cdot 7}{20\cdot 30}\Rightarrow \frac{4}{20}\cdot \left(\frac{4-20}{20}\right)\cdot \frac{1}{2}x^2=\frac{4}{20}\cdot \frac{7}{30}$$ But here $x^2=\text{Negative.}$ I did not understand how can I solve it Help me, Thanks",['sequences-and-series']
1652355,Several questions about Riesz–Markov–Kakutani representation theorem,"This is a list of questions about Riesz–Markov–Kakutani representation theorem . 1)If $f\in L^1(\mu)$, is it true that $\phi(f)=\int_Xfd\mu$, where $\mu$ is given by the theorem? 
I am quite sure it is correct, since $C_c(X)$ is dense in $L^1(\mu)$. And we can extract a subsequence that pointwise converges to $f$. 2)If $X$ is not locally compact, does the theorem still stands? If not, does there exist a non-regular measure such that $\phi(f)=\int_Xfd\mu$? The property of locally compact is a must to use Urysohn lemma in the proof. The reason why Urysohn lemma (or partition unity) is widely used is because the way we construct the measure. So I am thinking if we can construct the same measure in another way that we can avoid Urysohn lemma. 3) If $X$ is not Hausdorff, what will happen as stated in 2)? 4)If $X$ is normal and not locally compact?","['riesz-representation-theorem', 'real-analysis', 'measure-theory']"
1652402,Projection maps are open,"I want to show  $p_x: X\times\ Y \to X$ is an open map. Here's my proof: Let $W \subset\ X\times\ Y$ be open subset, then $W = \bigcup U_\alpha \times\ V_\beta$, for $U_\alpha, V_\beta$ are open subsets of $X, Y$ respectively. Then $p_x(W) = p_x (\bigcup U_\alpha \times\ V_\beta)= \bigcup p_x (U_\alpha \times\ V_\beta) = \bigcup U_\alpha$ is open in $X$, so $p_x$ is an open map/","['general-topology', 'proof-verification']"
1652408,"The root system of $sl(3,\mathbb C)$","I want to determine the root-system of the lie algebra $sl(3,\mathbb C)$. Does someone know a good (and complete) reference for this problem? I know that the root-system is $A_2$ but I want to see a complete proof (calculation) for this. Thanks in advance.","['matrices', 'abstract-algebra', 'lie-algebras']"
1652413,Find (linear) transformation matrix using the fact that the diagonals of a parallelogram bisect each other.,"This is the first time I post something on this website. I'm on this question already for hours. I'm clearly not asking the community to do my homework, I'm hoping someone can explain me how I should solve the following question; Let $l$ be a line through the origin in $\mathbb{R}^2$ , $P_l$ the linear transformation that projects a vector onto $l$ , and $F_l$ the transformation that reflects a vector in $l$ . Draw diagrams to show that $F_l$ is linear. Diagrams? How does this look like? A standard matrix? Figure 3.14 (see image) suggests a way to find the matrix of $F_l$ , using the fact that the diagonals of a parallelogram bisect each other. Prove that $F_l = 2P_l(x) - x$ , and use this result to show that the standard matrix of $F_l$ is (see image). If the angle between $l$ and the positive $x$ -axis is $A$ , show that the matrix of $F_l$ is (see image). I attached the question as image Hopefully you can help. Thanks! Image: https://i.sstatic.net/vFkmM.jpg EDIT: Image shown here:","['matrices', 'linear-algebra']"
1652414,L2 Matrix Norm Upper Bound in terms of Bounds of its Column,"I need to find an upper bound for a matrix norm in terms of bounds of its columns. I have a vector $\varepsilon_i(x) \in R^{n\times1} $ such that  $||\varepsilon_i(x)||_2\le\gamma_0$. I also have a matrix $Z=[\varepsilon_1, \varepsilon_2, \varepsilon_3, ... ,\varepsilon_N] \in R^{n\times N}$. Using the information $||\varepsilon_i(x)||_2\le\gamma_0$, can I find an upper bound for $||Z||_2$? If this were to be a frobenius norm question, it would be quite easy to show. However, I couldn't find an inequality for L2 norm case. Thank you in advance for your help.","['matrices', 'normed-spaces', 'inequality']"
1652454,Prove $\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi$,"For a beginning calculus student , prove $\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi$ I'm guessing this means something like Allowed : Pre-university maths, precalculus, basic calculus up to techniques of integration, Bernoulli's rule for sequences, two police persons theorem for sequences, $\lim_{n \to \infty} f(a_n) = f(\lim_{n \to \infty} a_n)$ if $f$ is continuous Not allowed : Monotone Convergence Theorem, Taylor series, polar coordinates and advanced stuff in real and complex analysis and the like (eg limsup, liminf, Stolz–Cesàro theorem, Cauchy sequences) What I tried: $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \le \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi$$ To use the two police persons theorem, I need to find some $a_n$ s.t. $$\lim_{n \to \infty} a_n = 2\pi$$ $$\frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \ge a_n $$ $$a_n \ne \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n})$$ Questions: What $a_n$ can I use? How else can I approach this problem? Might I be able to say that $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \color{red}{=} \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi$$ because $\lim \sin(\pi/n) = \lim \pi/n$ and for the same reason that justifies step 3 here ? So it's like $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \lim_{n \to \infty} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = 2\pi (1) = 2\pi$$ ? What is the technique here exactly? Usually whenever I see a trigonometric function (eg $\sin$ or $\cos$ ), my instinct is to ignore the argument and focus on the range if possible (eg $[-1,1]$ ). However, that would seem to give me $-\infty < L < \infty$","['sequences-and-series', 'calculus', 'limits']"
1652480,Question about proof: Uniform cauchy $\Rightarrow$ Uniform convergence,"I have one quick question regarding the proof of a theorem contained in these notes. Theorem 5.13. A sequence $(f_n)$ of functions $f_n : A → R$ converges
uniformly on $A$ if and only if it is uniformly Cauchy on $A$ . Question: In the triangle inequality part $|f_n(x) - f(x)| \leq |f_n(x) - f_m(x)| + |f_m(x) - f(x)|$ Why do we know that $f_m(x) \to f(x)$ as $m \to \infty$ so that $|f_m(x) - f(x)| < \frac{\epsilon}{2}$ I mean, isn't $|f_m(x) - f(x)|$ basically the same as $|f_n(x) - f(x)|$ on the left hand side? Just one is indexed by $m$ instead $n$ ? Why do we know that $|f_m(x) - f(x)| < \epsilon$ but we do not know immediately whether $|f_n(x) - f(x)| < \epsilon$","['real-analysis', 'sequences-and-series', 'uniform-convergence']"
1652540,Pushforward and pullback of invertible sheaf on ruled surface,"Suppose $\pi:X\to C$ is a geometrically ruled surface, and $D$ a divisor on $X$. Then if $D.f=0$ for a fibre $f$, we know by Grauert's theorem that $\pi_{*}(\mathscr{L}(D))$ is a invertible sheaf on $C$. It is know that $\pi^{*}\pi_{*}(\mathscr{L}(D))\simeq \mathscr{L}(D)$, can anyone help to explain why this is true? Thanks!",['algebraic-geometry']
1652541,What does $-p \ln p$ mean if p is probability?,"In statistical mechanics entropy is defined with the following relation: $$S=-k_B\sum_{i=1}^N p_i\ln p_i,$$ where $p_i$ is probability of occupying $i$th state, and $N$ is number of accessible states. I understand easily what probability is: for a frequentist it's just average frequency of getting this result. But I have a hard time trying to intuitively understand what $-p_i \ln p_i$ means. In the case where $p_i=p_j\; \forall i\ne j$ this reduces to $\ln N$, i.e. logarithm of number of accessible states. But in general case of unequal probabilities, what does $-p_i\ln p_i$ really represent? Is it some sort of ""(logarithm of) average number of accessible states""? Or maybe it's more useful to try to understand what $p_i^{p_i}$ is (but this seems even harder)?","['probability', 'statistical-mechanics']"
1652545,Multivariate normal value standardization,"I am wonder how to standardize multivariate normal value . Normal standard multivariate distribution of $q$ variables is $z\sim N_q(0,I_q)$. I have found that $a+Bz\sim N_q(Ba,BB^T)$ and based on this fact normalization could be performed throught two ways: 1) Substracting mean vector and then taking $B=\Sigma^{-0.5}$ (as it gives $\Sigma^{-0.5} \Sigma (\Sigma^{-0.5})^T=I_q$) we get normalized value: $z=\Sigma^{-0.5}(x-a)$. 2) Using Cholesky decomposition $\Sigma=U^TU$ we get normalization rule $z=(U^T)^{-1}(x-a)$ But both strategies does not work! So you can check it using matlab code below: MU=[0.1333,-0.1];
SIGMA=[0.95,0.19;0.19,0.97];
x=[0.5, 0.7];
mvncdf(x,MU,SIGMA)
z=(SIGMA^(-0.5))*transpose(x-MU);
mvncdf(z)
z=transpose(chol(SIGMA))^(-1)*transpose(x-MU);
mvncdf(z) I also add code for R that gives the same results proving that the problem is not due to programm but due to incorrect appoach: SIGMA=matrix(c(0.95,0.19,0.19,0.97), ncol=2)
MU=c(0.1333, -0.1)
x=c(0.5,0.7)
pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=x)
z=as.numeric((solve(sqrtm(SIGMA)))%*%(x-MU))
pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=z) As the difference is rather big I am sure that such standardization approach is incorrect. But it is the only technicks I have found despite it is a hot and applied subject. Will be very greatful for help! PS: I also performed Jordan form approach but the result is still wrong : [v,j]=jordan(SIGMA)
B=v*j^(-0.5)*v^(-1)
z=B*transpose(x-MU);
mvncdf(z)","['multivariable-calculus', 'matlab', 'normal-distribution', 'probability-distributions']"
1652571,If $B\subset A$ and $f:A\to B$ is injective prove it's a bijection between $A$ and $B$,"I want to show that if $B\subset A$ and $f:A\to B$ is an injective function then there's a bijection between $A$ and $B$. I believe my ""proof"" is wrong, I probably use too much ""intuition"" when I try to solve it. But hopefully I will get a better feeling if someone tells me where/what I do wrong and help me. :) That said, a friend to me ""solved"" another problem ""if $f:A\to C$ is an injective function, and $g:C\to A$ is an injective function, then there is a bijection between $A$ and $C$"". She argued like this "" for all $a$ in $A$ we can find an element $f(a)$ in $C$ and for all $c$ in $C$ we have an element $g(c)$ in $A$. If f(a)=c then we must have g(c)=a. This holds for all $a$ and $c$. So each $a$ maps to exactly one $c$ and each $c$ maps to exactly one $a$"". That is the Schröder–Bernstein theorem though, I have seen the proof, so I could directly tell that it's a wrong proof. I would probably not argue in the exactly same way but probably in a similar fashion. On the other hand I cannot really tell why this doesn't prove the fact either. Oh well, here comes my proof, it's a similar argument so I guess I'm wrong :) Proof: Since I already know it's injective, I just have to show it's surjective. We have that $B\subset A$, that is, every element of $B$ is in $A$. Because of this we can for every element $b\in B$ find an element $a\in A$ such that $b=f(a)$. That is, $\forall b\in B\exists a\in A:\textbf{ }b=f(a)$. But that is the definition of surjection. Hence, there exists a bijection between $A$ and $B$ since f is injective. I bet I've forgot to mention something now, which I found important to mention, but unfortunately I have forgotten it. Hopefully I will remind myself. Thanks for your help :)","['real-analysis', 'functions', 'proof-verification']"
1652580,Prove that $f$ has a minimum,"Let $f$ be a positive and continuous function in $[0,\infty)$ , such that $\lim\limits_{x\to \infty} f(x)=2$ . Prove that if $f(0)<2$ , $f$ has  a minimum in $[0,\infty)$ . I am stuck in the final step, in my opinion. From the limit's definition, there is $M \in \Bbb R$ such that for every $x>M$ , $|f(x)-2| < \varepsilon$ In the closed interval, $[0,M]$ , the function has a minimum, according to Weierstrass theorem. I have to prove that it is the global minimum. It won't be a global minimum if there is some $x_0>M$ which is a minimum. I think that's where I have to reach a contradiction which I can't formalize. Thanks for you help.","['continuity', 'real-analysis', 'calculus', 'limits']"
1652602,"Determining when $\int_{0}^{\infty} \cos(\alpha x) \prod_{m=1}^{n} J_{0}(\beta_{m} x) \, \mathrm dx =0$ without using contour integration","Let $J_{0}(z)$ be the Bessel function of the first kind of order zero, and assume that $\alpha$ and $\beta_{m}$ are positive real parameters. $J_{0}(z)$ is an even function that is real-valued along the real axis. And when $z$ approaches infinity at a constant phase angle, $J_{0}(z)$ has the asymptotic form $$J_{0}(z) \sim \sqrt{\frac{2}{\pi z}} \cos \left(z-\frac{\pi}{4} \right), \quad |\arg(z)| < \pi. $$ So by integrating the entire function $$e^{i \alpha z} \prod_{m=1}^{n} J_{0}(\beta_{m}z) , \quad \sum_{m=1}^{n} \beta_{m} < \alpha,$$ around a contour that consists of the real axis and the infinitely large semicircle above it,  it would seem to follow that $$\int_{0}^{\infty} \cos(\alpha x) \prod_{m=1}^{n} J_{0}(\beta_{m} x) \, \mathrm dx =0 \, , \quad  \sum_{m=1}^{n} \beta_{m} < \alpha. \tag{1} $$ (For the cases $n=1$ and $n=2$ , you would need to appeal to Jordan's lemma.) Is there way to prove $(1)$ that doesn't involve contour integration? EDIT: A similar approach also shows that $$\int_{0}^{\infty} \frac{\cos(\alpha x)}{1+x^{2}} \prod_{m=1}^{n} J_{0}(\beta_{m} x) \, \mathrm dx = \frac{\pi e^{-a} }{2}   \prod_{m=1}^{n}I_{0}(\beta_{m}), \quad \sum_{m=1}^{n} \beta_{m} \le \alpha,$$ where $I_{0}(z)$ is the modified Bessel function of the first kind of order zero.","['bessel-functions', 'complex-analysis', 'improper-integrals', 'integration']"
1652612,Smooth sawtooth wave $y(x)=\cos(x-\cos(x-\cos(x-\dots)))$,"Consider an infinite recursive function $$y(x)=\cos(x-\cos(x-\cos(x-\dots)))$$ $$y=\cos(x-y)$$ Plotting the function $y(x)$ implicitly we get a smooth sawtooth-like wave: Was this function studied before? For example, its derivative, Fourier series or other properties. It may be useful in electronics or other applications. I can find the expression for its derivative in terms of $y$ and $x$, but I do not know how to plot it effectively (without tabulating it by recursive formula for $y$). $$y'(x)=-\frac{\sin(x-y)}{1-\sin(x-y)}$$ We obtain the correct expression for the maxima and minima of the function. (Note that the first positive maximum is at $x=1$ and the first minimum is at $x=\pi-1$, which is confirmed by numerical computation). Since the function is smooth, the derivative should be finite everywhere. Does this mean that the denominator in this expression can never be equal to $0$? By the way, the recursive formula itself is not a fluke - I checked its convergence numerically in Mathematica and it seems to converge for all values. However, for the values close to the 'vertical lines' the convergence is slower than for the rest. Edit The function is not smooth, because its derivative is not defined at $x=\frac{\pi}{2}+2\pi n$ and $y=0$ as cardboard_box pointed out and as can be seen from the expression. Oh, it's just vertical tangents apparently Update I wanted to illustrate the very helpful answers and comments I've been given so far. First, credit goes to Lucian for introducing me to Clausen function (see the first comment below). However, this function does not have vertical tangents anywhere, as you can see in the graph (I plotted Clausen using its Fourier series, since it's easier than the integral definition): So, the function I defined here is much more ""sawtooth-like"". And finally, davik gave the parametric form for the function and figured out its Fourier series, which I plot below: You may notice I displaced and reversed the original function when plotting these graphs. We can also use sine instead of cosine if needed. Now the final question that I wanted to ask: Has anyone seen this function anywhere? Maybe it's worth it to make a complete description of its properties and publish somewhere?","['signal-processing', 'periodic-functions', 'trigonometry', 'implicit-differentiation']"
1652619,How can one determine if a function should have parenthesis around their argument?,"I have noticed that there are a select few functions that are acceptable if their argument is not in parenthesis. For example, here are a few functions I noted do not require an arguement: Trig or hyperbolic functions: $\sin x,\coth x,\cdots$ The factorial function: $x!$ is an acceptable function of $x$ And logarithms, for example: $\log x$ So I was under the impression that all functions must have parenthesis $\sin (x)$ for example, in my mind, would be the correct notation, and so on. A few functions do actually require parens, like polylogs $\operatorname{Li}_2 x$ is not good. I can't really differentiate when a function must or doesn't need parenthesis around the argument. Thank you.","['notation', 'soft-question', 'functions']"
1652633,Shortest Path Via Dynamic Programming Formulation?,"We have a directed Graph $G=(V,E)$ with vertex set $V=\left\{ 1,2,...,n\right\}$. weight of each edge $(i,j)$ is shown with $w(i, j)$. if edge $(i,j)$ is not present, set  $ w(i,j)= + \infty $. for each vertex $i$ put $w(i,i)=0$. we want using Dynamic Programming to find shortest path between any two vertex in this graph. by using which of them following recursive relation $d[i,j,n]$ is equal to shortest path among vertexes $i$ and $j$? $I)$
$d[i,j,k]=\begin{cases}w(i,j) & k=1\\ min_{1 \leq r \leq n} \left\{ d[i,r,k-1] +w(r,j) \right\}& k>1 \end{cases}$ $II) d[i,j,k]=\begin{cases}w(i,j) & k=0\\ min \left\{ {d[i,j,k-1],d[i,k,k-1]+d[k,j,k-1]}\right\}  & k>0 \end{cases}$ $III) d[i,j,k]=\begin{cases}w(i,j) & k=1\\ min_{1 \leq r \leq n} \left\{ {d[i,r,\lfloor k/2\rfloor ]}+d[r,j, \lceil k/2\lceil ] \right\} & k>1 \end{cases}$ this is an exam on 2011 that solution is wrote all of them. Who Can
  Help this confusing question better understood?","['graph-theory', 'trees', 'computer-science', 'dynamic-programming', 'discrete-mathematics']"
1652661,Gradient descent: L2 norm regularization,"So I've worked out Stochastic Gradient Descent to be the following formula approximately for Logistic Regression to be: $
w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t)
$ $p(\mathbf{y} = 1 | \mathbf{x}, \mathbf{w}) = \sigma(\mathbf{w}^T\mathbf{x})$, where $\sigma(t) = \frac{1}{1 + e^{-t}}$ However, I keep screwing something with when adding L2 Norm Regularization: From the HW definition of L2 Norm Regularization: In other words, update $\mathbf{w}_t$ according to $l - \mu
 \|\mathbf{w}\|^2 $, where $\mathbf{\mu}$ is a constant. I end up with something like this: $
w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t + 2\mu w_t)
$ I know this isn't right, where am I making a mistake?","['gradient-descent', 'regularization', 'multivariable-calculus', 'numerical-optimization', 'linear-algebra']"
1652671,Vakil's definition of smoothness -- what happens at non-closed points?,"The following is definition 12.2.6 in Vakil's notes. A $k$-scheme is $k$-smooth of dimension $d$, or smooth of dimension
  $d$ over $k$, if it is pure dimension $d$, and there exists a cover by
  affine open sets $\operatorname{Spec} k[x_1,\dots , x_n]/(f_1,\dots, f_r)$ where the Jacobian matrix has corank $d$ at all points. The Jacobian matrix at a point $p$ is the usual matrix of partials evaluated at $p$. At closed points, the quotient ring is a field, so we have a map of vector spaces and the usual definition of rank applies. For a general prime $P$, what is meant here? For Vakil, a ring element $f\in R$ has the value $f\in R/P$ at the prime $P$. But in this general case we have only a map of domains, not fields, and its not clear to me what is meant by corank here. One way to interpret this is that certain minors vanish, but I'm not sure if that was what Vakil intended. He explicitly defines ""corank"" as dimension of the cokernel.","['algebraic-geometry', 'schemes', 'ring-theory', 'commutative-algebra', 'field-theory']"
1652691,How do conformal maps affect curvature?,"Let $(\overline{M}^{n+1}, \langle \cdot, \cdot \rangle)$ be a riemannian manifold with riemannian connection $\overline{\nabla}$ and consider $M^n \subset \overline{M}$ an orientable hypersurface with unit normal vector field $\nu: M \to T \overline{M}$. Given a conformal diffeomorphism $f: \overline{M} \to \overline{M}$ say, with conformal factor $\mu^2 \in C^{\infty}(\overline{M}, \mathbb{R}_+^*)$, i.e., \begin{align*}
\langle Df(p) \cdot v_1, Df(p) \cdot v_2\rangle = \mu^2(p) \langle v_1, v_2 \rangle, \quad \forall p \in \overline{M}, \, \forall v_1, v_2 \in T_p \overline{M},
\end{align*}
how can we relate the principal curvatures of $M$ at a point $p$ with those of $f(M)$ at the point $f(p)?$ If $\mu = 1$, i.e., if $f$ is an isometry, can we say that the correspondent principal curvatures are equal?","['conformal-geometry', 'riemannian-geometry', 'differential-geometry', 'curvature']"
1652720,Determining if there can be smooth closed geodesic given its curvature,In my class on differential geometry I have been given the following question on which I am stuck: Let S be a regular orientable surface in $ R^3 $ with Gaussian curvature $K$ (not necessarily constant) and we are to check if there can be a smooth closed geodesic on S in the following case: $ K>0 $ $ K=0 $ $ K<0 $ and we are to give an example when it is possible that the closed geodesic bounds a simply connected region I know for starters about the first one with positive curvature I may take the sphere and a great circle which is a geodesic and obviously bounds a simply connected subset of the sphere. But with the zero and negative curvatures I have no idea how to tell if a closed geodesic exists I think the Gauss Bonnet theorem has something to do with this but I cannot really proceed from there and I am posting here in the hope of getting help ******* Progress: I can work out what happens with Gauss Bonnet if I have the constraint that the geodesic bounds a simply connected region as this is simple but the fact of the matter is where I am stuck is the general existence or non existence if no constraint is given on the region bounded,"['curvature', 'differential-geometry', 'surfaces']"
1652730,"If $\sin A+\sin B+\sin C=\cos A+\cos B+\cos C=0$,","If $$\sin A+\sin B+\sin C=\cos A+\cos B+\cos C=0,$$ prove that
  $$\cos 3A+\cos 3B+\cos 3C=3\cos(A+B+C).$$ My solution: From the given, 
$$\cos^3A+\cos^3B+\cos^3C=3\cos A\cos B\cos C$$ Now, 
L.H.S$$=\cos3A+\cos3B+\cos3C$$
$$=4\cos^3A-3\cos A+4\cos^3B-3\cos B+4\cos^3C-3\cos C$$
$$=12\cos A \cos B \cos C.$$ My solution ends here. How should I complete? NOTE: I am not allowed to use complex numbers at my level. So, please help me solve this problem without using complex numbers.","['angle', 'euclidean-geometry', 'trigonometry']"
1652732,Find the minimum number of tickets to guarantee the win of a n-bit binary lottery?,"Here's the problem. I just don't know how to approach it. If the 'one error tolerance' were removed, then this would be a simple binomial distribution problem. But now I can't figure it out. In the game of n-bit Binary Lottery players try to correctly guess a randomly selected sequence of n zeros and ones. Players purchase lottery tickets, and for each ticket purchased, the player can select an n-bit sequence of zeros and ones. Then a sequence of n zeros and ones is selected at random. A prize is awarded to each player whose ticket is either an exact match of the randomly selected one, or differs from the randomly selected sequence in only one place. For example in 4-bit Binary Lottery, if the randomly selected sequence is 0100 then the following tickets win prizes: 0100 (the randomly selected sequence itself) as well as 1100, 0000, 0110, and 0101 (the four sequences obtained by changing the randomly selected sequence in one place). (a) In terms of n, what is the smallest number of tickets you need to purchase in n-bit Binary Lottery in order to guarantee that you match the randomly selected sequence exactly. (b) In terms of n, how many different winning tickets are there in n-bit Binary Lottery? (c) What is the smallest number of tickets you need to purchase to guarantee that you win in 4-bit binary lottery?
  Justify your answer. (d) What is the smallest number of tickets you need to purchase to guarantee that you win in 5-bit binary lottery?
  Justify your answer.","['combinations', 'number-theory', 'binomial-distribution', 'probability', 'discrete-mathematics']"
1652748,"Prove $\binom{3n}{n,n,n}=\frac{(3n)!}{n!n!n!}$ is always divisible by $6$ when $n$ is an integer.","Prove $$\binom{3n}{n,n,n}=\frac{(3n)!}{n!n!n!}$$ is always divisible by $6$ when $n$ is an integer. I have done a similar proof that $\binom{2n}{n}$ is divisible by $2$ by showing that $$\binom{2n}{n}=\binom{2n-1}{n-1}+\binom{2n-1}{n}=2\binom{2n-1}{n-1}$$ but I am at a loss for how to translate this to divisible by $6$. Another way to do this proof would be to show that when you shoot an $n$-element subset from $2n$ you can always match it with another subset (namely the $n$-elements that were not chosen). Again, no idea how to translate this to $6!$.","['combinatorics', 'multinomial-coefficients']"
1652754,Are all countable torsion-free abelian groups without elements of infinite height free?,"The height of an element $g$ in an abelian group $G$ is the largest $n\in \mathbb{N}$ such that there exist an element $h\in G$ such that $n*h=g$. If $g$ has no such largest integer than $g$ is of infinite height. Suppose that a group $G$ is countable, torsion free,and has no elements of infinite height, then must it be a free group? If we restrict to finitely generated groups this follows immediately from the classification of finitely generated abelian groups. If, on the other hand, we allow uncountable groups (of any size) then the Baer-Specker Group ($\mathbb{Z}^\mathbb{N})$ serves as a counterexample. I can't figure out a way to adapt the proof of the Baer-Specker Group to any countable group though.","['abelian-groups', 'abstract-algebra', 'infinite-groups', 'group-theory']"
1652758,Schwarz' lemma: Prove that the inequality is strict unless the function f is of a certain form.,"the question (not homework) I am trying to answer is, in part: Let $f$ be an analytic function that maps the open unit disk $D$ into
  itself and vanishes at the origin.  Prove that the inequality $$|f(z)| + |f(−z)| ≤ 2 |z^2| $$ is strict, except at the origin, unless f has the
  form $f(z) = λz^2$ for some $λ$ a constant of absolute value one . I applied Schwarz' lemma to obtain the inequality.  Below is my answer: It is clear that the inequality holds at the origin.  The hypotheses given for $f$ are the same as those required for Schwarz' lemma to apply to $f$; the lemma clearly applies to both $f(z)$ and $f(-z)$.  Thus I have
  $$|f(z) + f(-z)| \leq |f(z)| + |f(-z)| \leq |z| + |z| = 2|z|$$
  Divide both sides by $|2z|$ (I have assumed $z\neq 0$):
  $$\frac{|f(z) + f(-z)|}{|2z|} \leq 1$$
  This fact shows that the function $(f(z) + f(-z))/ 2z$ has a removable singularity at $z = 0$ (since it is bounded in a neighbourhood of that point).  Calling the analytic continuation $g(z)$, $g$ is a holomorphic map from $D$ to $D$ and vanishes at the origin; to see why, expand $f(z) + f(-z)$ into the sum of two power series, note that the first two terms vanish, and conclude that $g$ has a zero of order at least one at the origin.  So Schwarz' lemma applies to $g(z)$, and in particular $|g(z)| \leq |z|$.  But this fact directly implies the desired inequality. The problem is that I have come most of the way to proving the strict inequality, but cannot prove that the given form is the only possible form for $f(z)$.  I proved that if $|f(c) + f(-c)| = 2|c^2|$ for some $c$ not the origin, then the constructed function $g(z)$ is a rotation by Schwarz' lemma, which means that $f(z) + f(-z) = \lambda z^2$.  This means that all even-index coefficients of the power series of $f$ must be zero, but it does not rule out the possibility that there are odd-index coefficients.  None of the standard tricks like Cauchy inequalities work since the domain is the unit disc.  I also tried looking at the other implications of Schwarz (i.e., that $|f'(0)| < 1$) and that, too, led nowhere.  What am I missing here?",['complex-analysis']
1652776,What are some topics of advanced number theory every young geometers should know? (soft question),"By ""advanced number theory"", I mean topics like arithmetic/Diophantine geometry, modular/automorphic forms and Shimura varieties. I'm interested in derived/non-commutative algebraic geometry, some topology and higher category theory as well as string theory related things. According to a professor teaching my K-theory class, I only have to know (for topology) elliptic curves and algebraic number theory, both of which I have learned but nothing beyond them. However, as I'm trying to learn topics like geometric Langlands, which are closely related to modular/automorphic forms and other number theory topics, I've been increasingly aware of the influence of number theory in my field.","['arithmetic-geometry', 'soft-question', 'algebraic-geometry']"
1652781,Concentration inequality for sum of squares of i.i.d. sub-exponential random variables?,"Suppose $X_1, X_2, \ldots, X_n$ are independent and each has the same distribution with a sub-exponential random variable $X$ (for example, $X$ is the square of a standard normal Gaussian variable). Can I obtain a concentration inequality for the square of sub-exponential $X_i$ , say, $$\mathbb{P}\left( \frac{1}{n} \left( X_1^2+\cdots+X_n^2 \right) \ge \mathbb{E}\left[X^2\right] + t \right) \le C \exp\left( - n \cdot \min\left( C_1 t^2, C_2 t, C_3 \sqrt{t} \right) \right),$$ where $C, C_1, C_2, C_3$ are constants? This problem arises in my research. Remark: Actually, for i.i.d. sub-Gaussian random variables $Y_i\ (i=1,\ldots,n)$, I knew that $$\mathbb{P}\left( \frac{1}{n} \left(Y_1+\cdots+Y_n\right) \ge \mathbb{E}\left[Y\right] + t \right) \le \exp\left( - n\cdot C_1 t^2 \right).$$ Besides, since $Y_i^2\ (i=1,2,\ldots,n)$ are sub-exponential , I also knew that $$\mathbb{P}\left( \frac{1}{n} \left(Y_1^2+\cdots+Y_n^2\right)\ge \mathbb{E}\left[ Y^2 \right] + t \right) \le \exp\left( - n\cdot \min(C_1 t^2, C_2 t) \right).$$ These two inequalities can be proved by a Chernoff bound, since the moment generating functions of $Y$ (sub-Gaussian) and $Y^2$ (sub-exponential) both exist. However, I want to know whether there is an inequality like $$\mathbb{P}\left( \frac{1}{n} \left( Y_1^4+\cdots+Y_n^4 \right) \ge \mathbb{E}\left[Y^4\right] + t \right) \le C \exp\left( - n \cdot \min\left( C_1 t^2, C_2 t, C_3 \sqrt{t} \right) \right),$$ even though the moment generating function of $Y^4$ (square of sub-exponential) does not exist.","['probability-theory', 'probability', 'statistics']"
1652786,Fraction Sum Series,This question was asked in (selection) IMO for 8th graders. $1/2 + 1/6 + 1/12+ 1/20 + 1/30 + 1/42 +1/56 + 1/72 + 1/90 + 1/110 +1/132$ I have noticed that it can be written as $1/(1*2) + 1/(2*3) +1/(3*4) + 1/(4*5).... +1/(11*12)$ However I don't know how to continue..,"['algebra-precalculus', 'summation', 'fractions', 'arithmetic']"
1652812,A series to prove $\frac{22}{7}-\pi>0$,"After T. Piezas answered Is there a series to show $22\pi^4>2143\,$? a natural question is Is there a series that proves $\frac{22}{7}-\pi>0$? One such series may be found combining linearly the series that arise from truncating 
$$\sum_{k=0}^\infty \frac{48}{(4k+3)(4k+5)(4k+7)(4k+9)} = \frac{16}{5}-\pi$$
to two and three terms, namely $$\sum_{k=2}^\infty \frac{48}{(4 k+3) (4 k+5) (4 k+7) (4 k+9)} = \frac{141616}{45045}-\pi$$
and
$$\sum_{k=3}^\infty \frac{48}{(4 k+3) (4 k+5) (4 k+7) (4 k+9)} = \frac{2406464}{765765}-\pi$$
Solving
$$a\left(\frac{141616}{45045}-\pi\right)+b\left(\frac{2406464}{765765}-\pi\right)=\frac{22}{7}-\pi$$
for rational $a,b$ and some algebra manipulation yields the result $$\frac{16}{21} \sum_{k=0}^\infty \frac{1008 k^2+6952 k+12625}{(4 k+11) (4 k+13) (4 k+15) (4 k+17) (4 k+19) (4 k+21)}=\frac{22}{7}-\pi$$ It is interesting to note that the coefficients needed to multiply the two component series are both positive
$$a=\frac{113}{7·8·9}$$
$$b=\frac{391}{7·8·9}$$ because the truncation points have been chosen so that $$\frac{2406464}{765765}<\frac{22}{7}<\frac{141616}{45045}$$ This procedure yields a result that proves the claim with no need for further processing, and it is readily seen to prove $\frac{p}{q}-\pi>0$ for all fractions between $\pi$ and $\frac{16}{5}$. Now, in the light of this equivalent form of Lehmer's formula
$$\pi-3=\sum_{k=1}^\infty \frac{4!}{(4k+1)(4k+2)(4k+4)}$$ one may still ask Q1 Is there a series that proves $\frac{22}{7}-\pi>0$ with constant numerator? Q2 Is there a reason why $113$ is both the numerator of the $a$ coefficient and the denominator of the next convergent from above $\frac{355}{113}$? Edit: A similar series with smaller coefficients may be obtained by applying the method above to
$$\begin{align}
\sum_{k=0}^\infty \frac{960}{(4 k+3) (4 k+5) (4 k+7) (4 k+9) (4 k+11) (4 k+13)} 
&=
\frac{992}{315}-\pi \\
&=
\frac{3·333-7}{3·106-3}-\pi \\
\end{align}
$$
in order to obtain
$$\sum_{k=0}^\infty \frac{96 (160 k^2+422 k+405)}{(4 k+3) (4 k+5) (4 k+7) (4 k+9) (4 k+11) (4 k+13) (4 k+15) (4 k+17)} = \frac{22}{7}-\pi$$ Q3 What is the relationship between $\frac{992}{315}$ and the third convergent to $\pi$ $\frac{333}{106}$?","['inequality', 'diophantine-approximation', 'pi', 'sequences-and-series', 'approximation']"
1652837,Are all sets with finite measure measurable?,"In my textbook, it says: ""Let E be any set with m*(E) < $\infty$. Then E is measurable if and only if there exists a measurable set B with m(B) = m*(E)."" There always exists a measurable set of any given measure implying that there always exists a measurable set with measure m*(E). Thus this would imply that all sets with finite measure are measurable. I'm not sure if I'm thinking correctly. Is this true? Are all sets with finite measure measurable? Proof: Let $E$ be measurable. Then take $B = E$. Conversely, let there exist a measurable set $B \supset E$ with $m(B) = m$*$(E)$. Then, by Theroem 3.4.10 (b), we have $m$ * $(E-B) = m$ * $(E) - m$ * $(B) = 0$ Now, $E - B$ being a set of measure zero, is measurable and hence $ E = (E - B) \cup B $ is measurable","['lebesgue-measure', 'measure-theory']"
1652867,f: R → R and $|f'(x)| ≤ |f(x)|$ [duplicate],"This question already has answers here : Does $f(0)=0$ and $\left|f^\prime(x)\right|\leq\left|f(x)\right|$ imply $f(x)=0$? (4 answers) Closed 8 years ago . Let $f: R → R $ be a function such that $f'(x)$ is continuous and $|f'(x)| ≤ |f(x)|$ for all $x ∈ R$ , if $f(0)=0$ the maximum value of $f(5)$ is My Attempt: I proved that $f'(x)=0$ for $x ∈ [0,1]$ through LMVT but i am not able to prove anything further I also thought of a real life analogy of this that if magnitude of acceleration is always less than magnitude of velocity and initially the particle was at rest then it won't move for every $t>0$ and hence concluding $f(x)=0$ for every $x ∈ R$ Help Please.","['derivatives', 'ordinary-differential-equations', 'calculus', 'functions']"
1652881,Are planes in $3$-dimensions two-dimensional?,"Are planes in $3$-dimensions two-dimensional? The reason I ask is because mathematically the $xy$-plane exists in $3$D space but appears to be $2$D, but how can something $2$D be in $3$D space? I therefore conclude that a plane in $3$D space must have a thickness that is infinitesimal. Isn't this a contradiction, though, since planes have a thickness of that of a point and since a point has no thickness neither does the plane?","['analytic-geometry', 'geometry']"
1652905,Find $\lim_{h \to 0}\frac{\tan\sqrt{x+h}-\tan\sqrt{x}}{\log(1+3h)}$ without L'Hospital's rule,What is the limit of $$\lim_{h \to 0}\frac{\tan\sqrt{x+h}-\tan\sqrt{x}}{\log(1+3h)}$$ I am confused with square root function and L'Hospital's rule is prohibited . The professor requires a detailed solution. I don't know what to do. Thanks!,"['limits-without-lhopital', 'limits']"
1652931,Intuitive understanding of vector / matrix calculcation,"I am currently dealing with calculations done on vectors and matrices. For some parts I have gained an intuitive understanding, for others I didn't. E.g., when we are adding two vectors, you can imagine that this means adding two arrows. The result is a single arrow that reflects the combined forces of the two individual source vectors. The resulting vector will probably have a new direction, which is influenced by the two original ones. When we multiply a vector by a scalar, you can imagine that this means putting the very same arrow multiple times behind itself, to make it longer. I.e., the new vector has more force, but the direction stay the same. Now… if I want to multiply a matrix by a vector, what is the analogy for that? What does this mean in terms of geometry?","['matrices', 'vectors']"
1652936,Isomorphism between $G$ and $\mathbb{Q}^{*}$,"Let $\{G_{n}\}_{n\in \mathbb{N}}$ be a family of additive groups with $G_{1}=\mathbb{Z}_{2}$ and $G_{n}=\mathbb{Z}$ for $n\geq 2$
$$G=\bigoplus_{n\in \mathbb{N}}G_{n}$$ I want to prove that $G\cong \mathbb{Q}^{*}$ I can't find the correct function, can you give me a hint?
 I tried : $$(g_{1},g_{2},...)\longrightarrow -1^{g_{1}}\sum_{n\geq 2}\frac{g_{n}}{10^{n-2}} $$","['abelian-groups', 'group-theory']"
1652937,A good book for beginning Group theory,I am new to the field of Abstract Algebra and so far it's looking to me quite tough. So far I have encountered the following books in group theory - Contemporary abstract algebra by Joseph Gallian and Algebra by Michael Artin. But can someone suggest me a book which has theorems and corollaries explained using examples and not just mere proofs?,"['reference-request', 'abstract-algebra', 'book-recommendation', 'group-theory']"
1652984,Solution of the Beast in Disguise $y'=x-y^2$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question $y'=x-y^2$
Its a first-order nonlinear ordinary differential equation. The solution is given here http://www.wolframalpha.com/input/?i=solve+y%27%3Dx-y%5E2&x=0&y=0 . 
But how is it obtained? Whats happening here? Direct answers are appreciated, If not possible I will do with a few links to know more about these kind of equations. And some more personified help.",['ordinary-differential-equations']
1653006,How can I find the limit without using a closed form expression [duplicate],This question already has answers here : Value of $\lim_{n\to \infty}\frac{1^n+2^n+\cdots+(n-1)^n}{n^n}$ (3 answers) How should I calculate $\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$ [duplicate] (1 answer) Closed 8 years ago . I am trying to evaluate this limit without using the closed form expression for the sum of natural numbers raised to $k$th power.  $$\lim_{n \to \infty} \dfrac{ 1^n +2^n+\cdots +n^n}{n^n}$$ So far I have tried l'Hôpital which complicates it rather than simplifying and Cesaro Stolz doesn't seem to work either.,"['calculus', 'limits']"
1653015,Height and Distance problems,"A ladder rests against a wall at an angle $\alpha$ to the horizontal. When its foot is pulled away from the wall through a distance $a$, it slides a distance $b$ down the wall and makes an angle $\beta$ with the horizontal. Prove that :
$$\frac{a}{b}=\frac{\cos\alpha-\cos\beta}{\sin\alpha-\sin\beta}$$. I tried to solve this question as;
Let $AB$ be a wall and $AC$ be the ladder. Then $\angle ACB=\alpha$. Now, the question says, ""when its foot is pulled"", I doubt that whether the ladder is pulled in sideways direction or in downward direction.",['trigonometry']
1653061,Proving that $\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^\infty e^{A(t-s)}b(s)ds=\vec{0}$,"Consider $x'=Ax+b(t)$, a system of differential equations. Given that $A$ has negative real parts in all its eigenvalues, and that $\lim\limits_{t\to\infty} b(t) = \vec{0}$, I need to prove that $\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^t e^{A(t-s)}b(s)ds=\vec{0}$. Well, it's clear with $\lim\limits_{t\to\infty} e^{At}x_0 = \vec{0}$. But I'm struggling with $\lim\limits_{t\to\infty} \int\limits_0^t e^{A(t-s)}b(s)ds$. Can someone please give me a hint? I feel like I don't know some property which should be used here, such as a property of integrals.","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'calculus']"
1653079,Is there any palindromic power of $2$?,"My question is in the title: Is it possible to find $n≥4$ such that $2^n$ is a palindromic number (in base $10$)? A palindromic number is a number which is the same, independently from which side we read it (forwards and backwards), for example $121, 484, 55755$. My guess is ""no"". I know that a palindromic number $x$ with even length (i.e. the number of digits $\max\{n : 10^n \mid x\}$ is even) is a multiple of $11$: see here or here or here . In particular, a power of $2$ with an even length can't be a palindromic number. However, I don't know what to do with the case of an odd length. For instance, if $x=abcdcba$, then $abccba$ is a multiple of $11$, but I don't see how this can help. Here is a related question. On MathOverflow, related questions are: (1) and (2) . Maybe also this thread (since $(1)$ is focused on binary expansion). I tried with Mathematica and there is no palindromic power of $2$ with exponent $n<10000$: palindromeQ[n_] := IntegerDigits[n] === Reverse@IntegerDigits[n];
 For[i = 1, i < 10000, i++, If[PalindromeQ[2^i], Print[i]] ] Finally, I think that the answer will be the same if we replace $2$ by any integer $n>1$ which is not a multiple of $11$. I don't know how I could prove (even for the case of even length…) that $11^n$ is not a palindromic number for $n≥5$. Any hint will be helpful. Thank you!","['decimal-expansion', 'number-theory', 'elementary-number-theory', 'recreational-mathematics', 'open-problem']"
1653134,An example of smooth compactly supported function with everywhere non-vanishing Fourier transform,"I am trying to find an explicit example of smooth real-valued compactly supported function $u$ in $\mathbb R^n$, $n \geq 2$, such that its Fourier transform $\widehat u$ does not have zeros. By the Paley-Wiener characterization theorem it is sufficient to find an entire function $U$ on $\mathbb C^n$ such that: for any $N$ there exists $C_N$ such that
$$
   |U(z)| \leq C_N (1+|z|)^{-N} e^{B|\mathop{\mathrm{Im}} z|}
$$
for some $B > 0$; $U(z) \neq 0$ for $z \in \mathbb R^n$. $U(z) = \overline{U(-z)}$, $z \in \mathbb R^n$. On the other hand, by the Wiener $L^1$-approximation theorem it is sufficient to find a smooth compactly real-valued function $u$ in $\mathbb R^n$ such that the span of its shifts $u_a = u(\cdot-a)$, $a \in \mathbb R^n$, is dense in $L^1(\mathbb R^n)$. Please, help me.","['functional-analysis', 'examples-counterexamples', 'fourier-analysis']"
1653180,"$S_1 \subset S_2$. To show, $Span(S_1) \subset Span(S_2)$","Prove that if $S_{1} \subset S_{2}$, then $Span(S_{1}) \subset Span(S_{2})$ Approach: Suppose $S_{1} \subset S_{2}$ Let $x \in S_{1}$, then by definition of a subset, $x \in S_{2}$ All possible linear combinations of $x$ are $cx$ with $c \in \mathbb{R}$ So $cx \in Span(S_{1})$ But $x$ is also in $S_{2}$, therefore $Span(S_{2})$ contains all linear combinations of $x$ as well. So $cx \in Span(S_{2})$ We have $cx \in Span(S_{1})$, then $cx \in Span({S2})$  $\forall c \in \mathbb{R}$ We have shown: if $S_{1} \subset S_{2} \Rightarrow Span(S_{1}) \subset Span(S_{2})$ Is my approach to the question correct? If so, is this enough to answer the question or have I missed something?","['linear-algebra', 'elementary-set-theory']"
1653197,Riemann zeta-function functional equation proof,"I'm reading through Titchmarch's ""The Theory of the Riemann Zeta-Function"" and there's a part in the functional equation proof number 3 that I haven't figured out. He defines a function
$$\psi(x)=\sum_{n=1}^\infty e^{-n^2\pi x}$$
and next, for $x>0$ it is known that
$$
\sum_{n=-\infty}^\infty e^{-n^2\pi x}=\frac1{\sqrt{x}}\sum_{n=-\infty}^\infty e^{-\frac{n^2\pi}x},
$$
or 
$$2\psi(x)+1=\frac1{\sqrt{x}}\left( 2\psi\left(\frac1{x}\right)+1\right).$$
Where does the second equation come from exactly?","['complex-analysis', 'riemann-zeta', 'sequences-and-series']"
1653219,Is every topological space is measurable?,Actually I am learning about measure theory. But I have confusion between  topological space and measurable . Is there any relationship among them or not?,"['functional-analysis', 'real-analysis']"
1653225,Orthogonal trajectories - why is it necessary to isolate the parameter,"For orthogonal trajectory, I realized that I need to express the parameter of the given family of curves in terms of x and y, in order to get the right answer. e.g. in $y = kx$, $k$ is the parameter I was talking about in the preceding sentence above. 1) If I solve the orthogonal trajectory problem treating the parameter as if it is a constant. What will that constant mean in my final answer? 2) If I solve the problem properly (i.e. express $k$ in terms of $x$ and $y$), does $k$ changes as I move along any one of the orthogonal trajectories? Is this why I need to isolate for $k$ in the first place? Thanks for the help","['ordinary-differential-equations', 'calculus']"
1653227,"If a Stochastic Process has Variance linear with t, how to prove it is not Wide Sense Stationary?","For my study, as a part of a Matlab exercise, the following question is asked: Using the results of the estimated standard deviations of the random
  variable $x(k)$ for $k = 10^3; 10^4; 10^5$ conclude whether the random
  process is wide sense stationary (WSS) or not. The process we are studying in this exercise is Brownian motion, using the following difference equation: $x(k)+\beta_1x(k-1)+\beta_2x(k-2)=\beta_3w(k)$ Where $w(k)$ is modeled using samples from the Normal distribution, normalised by the time step $dt$. Now, from theory, I know that the variance of the Brownian motion (a Wiener process) increases linearly with time: $Var(x)=at=\sigma_x^2$ So, $\sigma_x=b\sqrt(t)$ This is indeed what I observe, when I plot these values; the function looks like a square root. When I plot the squared values, I get something looking very nicely linear. I also know that a Wiener process is not WSS from literature (the internetz). Now, it is up to me to prove this from the gained relation between the standard deviation (Variance), and the time. According to my course reader, 3 criteria are needed for a stochastic process to be WSS: The mean should be constant (time-invariant) Autocorrelation $t1$,$t2$ should only be dependent on the difference between two time-intervals, $t2-t1$ $c_x(0)<\infty$, (with $c_x$ the autocovariance function). i.e. The variance is finite. I think the criterium that needs to be checked is the 3rd one. Maybe the second one can also be used. (The first one is already met, since for this process the mean is zero, and therefore constant in time.) I was thinking along the lines of: $\lim_{t\to\infty} at=\infty$ (Apologies if this is not the correct way to write that) However, I'm not sure if this a valid proof, or not. In a real physical system, t will never reach infity, and the Variance will be very large, but not infinite. Maybe I can relate the Variance in some way to the Autocorrelation, and this way prove that for a variance linear with t, it is not possible to satisfy condition 2? I appreciate nudges in the good direction more than final answers, if possible.","['stationary-processes', 'statistics', 'covariance', 'brownian-motion']"
1653234,The set of integers is not open or is open,"Baby Rudin gives the example of the set of all integers being not open if it is a subset of $\mathbb{R}^2$. If we consider the set of integers in $\mathbb{R}$, is this set also not open? I can find a neighbourhood which will contain any point, $p$, however is it a requirement that a neighbourhood contains more than one point? I'm trying to understand this fully and have searched through the various posts that have a slight relation and can not find out specifically how these take interior and isolated points into account and how these relate to openess .","['real-analysis', 'analysis']"
1653248,"What is $\lim_{(x,y) \to (0,0)} \arctan(xy)/\sqrt{x^2+y^2}$?","The limit is this: $$\lim\limits_{(x,y) \to (0,0)} \frac{\arctan(xy)}{\sqrt{x^2+y^2}}$$ It's not necessary to give a whole solution, I want the path to see how to solve it. I tried both with sequences characterization and definition of limit but I don't know many things to do with $\arctan(xy)$. I only know it's bounded $-{\pi \over 2} < \arctan(xy) < {\pi \over 2}$, it's odd and strictly increasing.","['multivariable-calculus', 'real-analysis', 'calculus', 'limits']"
1653250,Is there a probability measure on the Cantor set?,I know that the Lebesgue measure of the Cantor set is $0$. Is there a finite positive regular measure on the Cantor set?,"['lebesgue-measure', 'measure-theory', 'cantor-set']"
1653272,"Determine the number of subgroups of $\Bbb Z_p \times\Bbb Z_p$, where $p$ is prime.","There are some answers online and we got one in our lecture. Unfortunately I have spent several hours trying to make sense of it and getting nowhere. I think it is mainly due to the fact of me being very very poor at groups of the type integers modulo $n$. I should note that I understand that the problem boils down to finding all the cyclic subgroups of $\Bbb Z_p \times \Bbb  Z_p$ of order $p$. Help would be much appreciated, thank you!","['finite-groups', 'modular-arithmetic', 'group-theory', 'cyclic-groups']"
1653300,For what $n$ is $\sum_{i=1}^\infty \frac{\cos (it)}{i^n}$ bounded and why doesn't a sine behave the same way?,"I've been looking at a parametric curve $$\pmatrix{X \\ Y}=\pmatrix{\sum_{i=1}^N \frac{\cos (it)}{i^n} \\ \sum_{i=1}^N \frac{\sin (it)}{i^n}}$$ where, for the plots below, $N$ runs from $1 \rightarrow 300$ and $n=1,2$, respectively. It seems that $X$ is unbounded/divergent in one, and bounded/convergent in the other, whereas $Y$ seems to be indifferent to $n$ and always be bounded. I use the term ""bounded"" to encapsulate that several different properties ($ \max (X),$ the area enclosed by $X,...$) could be a measure of this. My question: For which $n \in \mathbb{R}$ is $X,Y$ bounded, and why does the sine always seem to be bounded? I guess it could have something to do with this post, but I don't quite see how the argument in that post would be used for this problem, mostly because of my $t$, but perhaps it doesn't change anything? As a sidenote, I tried introducing a $(-1)^{i+1}$ in the sums, but all this did was to mirror the graph around the left-most point of the original graph, so that $X$ diverged to $-\infty$ instead. Any ideas of why this is the case? $n=1$ $n=2$ Oh, and here is a nice and wobbly version with $(-1)^{i}$: $n=1$ Any insights are much appreciated!","['parametric', 'limits']"
1653308,Differential equations with dense solutions,"Consider the differential equation $P(y',y'',y''',y'''')=0$ on $\mathbb R$, where $P(x,y,z,w)$ is the homogeneous polynomial of degree $7$ given by
$$
3x^4yw^2-4x^4z^2w+6x^3y^2zw+24x^2y^4w-12x^3yz^3-29x^2y^3z^2+12y^7.
$$
This example was given by Rubel in 1981 (Bulletin of the AMS), and he proved that for any continuous functions $f,g\colon\mathbb R\to\mathbb R$ with $g>0$ there is a solution $y$ of the differential equation satisfying
$$
|y(t)-f(t)|\le g(t),\quad \text{for all } t\in\mathbb R.
$$
Quite impressive. When one reads the proof one understands that all comes from the particular structure of the equation, but really impressive! My question is the following: Is there any polynomial of smaller degree leading to the same property? A perhaps more ambitious question would be: what is the smallest degree of a polynomial having this property? In fact one could also vary the number of variables of the polynomial.","['ordinary-differential-equations', 'dynamical-systems']"
1653317,"Probability and the ""out of"" thing""","I have quite an odd question: I am not able to fully understand the concept of ""out of"". If I roll a dice once, from a total of $6$ possible outcomes, I'll get 1. Why does that mean a fraction $1\over 6$ = approx $16.67 \%$ and why does that mean that on average one out of $6$ rolls, I will get for example ""$1$"" on dice. Where does the fraction say that for every $6$ rolls, I'll get on average one roll I wanted to get. Why does $5$ out of $7$ mean $5\over 7$, why does that mean that it's on average $5$ per every $7$ people? Because when I want to get $5\over 7$ of something, I divide something into $7$ parts and get $5$. Is that the second look at this matter, that it can be seen like, for example: for every $7$ (divide some number by $7$ to find out how many $7$s are there) and then multiply by $5$, because for every seven that is included in the number it will be $5$. Are my thoughts correct? How is the correct way of seeing these things? Thanks for help in advance. dont answer like Maths 90-page long thesis, I just want an answer that is en explanation in your own words. What I struggle is probably the fractions, what does out of mean and why... and you explain everything but this.","['probability', 'fractions']"
1653325,Can every separable differential equation be rewritten to potentially be exact (or NOT exact)?,"Let's say an ordinary linear DE is separable. Then
$$\frac{dy}{dx} = P(y)Q(x) \Leftrightarrow \frac{1}{P(y)}dy = Q(x)dx \Leftrightarrow Q(x)dx + R(y)dy = 0$$
is in exact form where $R(y)=-\frac{1}{P(y)}.$
This means that $M(x,y) = Q(x)$ and $N(x,y) = R(y)$ if we're using standard text book notation. To test for exactness, note that
$$\frac{\partial M}{\partial y} = 0 = \frac{\partial N}{\partial x},$$ meaning that any separable equation could be made to be exact. As a concrete example, consider
$$\frac{dy}{dx} = x\ln (y) + x$$
which is separable as
$$\frac{1}{\ln(y)+1}dy = xdx$$
and everything I wrote above follows (demonstrating that our equation is indeed exact). HOWEVER, if we instead write it as
$$dy= (x\ln (y) + x)dx \Leftrightarrow -(x \ln (y) + x)dx + dy = 0$$
then we have that $M(x,y) = -(x\ln(y) + x)$ and $N(x,y) = 1$ and our test for exactness yields
$$\frac{\partial M}{\partial y} = -\frac{x}{y} \neq 0 = \frac{\partial N}{\partial x}$$
showing that the SAME differential equation is NOT exact! What am I missing here?","['ordinary-differential-equations', 'calculus']"
1653336,difficult integral $\int_0^{\pi/2}\frac{x^2({1+\tan x})^2}{\sqrt{\tan x}({1-\tan x})}\sin{4x}dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question This is a complicated integral, the numerical value appears to me correct.Therefore how to prove this result?$$I=\int_0^{\pi/2}\frac{x^2({1+\tan x})^2}{\sqrt{\tan x}({1-\tan x})}\sin({4x})dx=\frac{\pi\sqrt{2}}{192}(35{\pi^2}-150+132\ln 2-84\ln^22)$$","['integration', 'definite-integrals']"
1653340,Find the domain of the given function.,"I have the function $f(x)=\cos^{-1}(\frac{1}{2\cos(x)})$ and I have to find its domain. What I know is that the domain of $\cos^{-1}(x)$ is $[-1,1]$ so I think that $\frac{1}{2\cos(x)}$ should be at least $-1$ and $1$ at most. So $-1=0.5/\cos(x)$ gives $x=\frac{2\pi}{3}+2\pi$ and $1=\frac{1}{2\cos(x)}$ gives $x=\pi/3+2\pi$ i.e according to me the domain of the above function $f(x)=\cos^{-1}(\frac{1}{2\cos(x)})$ is $[2\pi/3+2\pi,\pi/3+2\pi]$. Is that right or there is something wrong?",['functions']
1653388,How many ways we can choose items from different boxes,"I searched through the internet but couldn't find my answer, which can either be a very simple or a hard one. Assume there are $3$ boxes, which carry, respectively, $1$, $4$, $2$ items.
My question is how many ways we can select $3$ items from these boxes.
I am looking for a formula rather than a solution for these specific values. If I choose (take away) $3$ items by trying one by one.
\begin{array}{c c c}
0 & 3 & 1\\
0 & 2 & 2\\
0 & 4 & 0\\
1 & 1 & 2\\
1 & 3 & 0\\ 
1 & 2 & 1
\end{array} Items remain each time, so the answer seems to be $6$ different ways. But I am not sure.","['combinations', 'combinatorics', 'factorial', 'discrete-mathematics']"
1653403,"$A$ has more columns than rows and has full row rank, show there exist infinitely many $B$ s.t. $AB=I$","If A $\in M_{m\times n}(R)$ such that $n>m$. Prove that if $\text{rank} (A) = m$ then there are infinitely many matrices $B \in \ M_{n\times m} (R)$ such that $ AB = I_m$ So the question is defining a matrix $A$, in a set of matrices where the number of columns is always greater than the number of rows, and if $A$ has full row rank, then there are infinitely many matrices $B$ such that $AB$ creates the identity matrix. I'm not sure where to go with this question, could anyone please help? ETA Thank you to everyone for their answers, the question makes sense now.","['matrices', 'linear-algebra']"
1653406,Anti-derivative of continuous function $\frac{1}{2+\sin x}$,"I use tangent half-angle substitution to calculate this indefinite integral:
$$
\int \frac{1}{2+\sin x}\,dx = \frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}+\text{constant}.
$$ Wolfram Alpha also give the same answer. However, $\frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}$ is discontinuous on $(n+1)\pi$ where $n$ is any integer. Why is an anti-derivative of a continuous function discontinuous?","['integration', 'calculus']"
1653467,Question about non trivial zeros of Riemann zeta function,Riemann zeta function is $$\zeta(s)=\sum\frac{1}{n^s}$$ I read at wiki that the first nontrivial zero is located at $14.134725\ldots$ As far as I understand it means $$\zeta(s)=\sum\frac{1}{n^{0.5+i14.134725\ldots}}$$ Does it means $$\sum\frac{1}{n^{0.5+i14.134725\ldots}}$$ converges to $0$ for $n=1$ to infinity?,['complex-analysis']
1653477,"Show that $f$ is not differentiable at $(0,0)$ - $\frac{x_1^2x_2}{x_1^2+x_2^2}$","Let the function $f: \mathbb{R^2} \to \mathbb{R}$ defined as $$f(x) =   
\begin{cases}   
\frac{x_1^2x_2}{x_1^2+x_2^2},     
 & \quad \text{if } (x_1,x_2) \not= 0  \\ 
0,  & \quad \text{if } (x_1,x_2) = 0
 \\    \end{cases} $$ Show that $f$ is not differentiable at $(0,0)$. I think I can use the directionnal derivative or the polar coordinate, but I don't know how. Is anyone is able to explain to me how I can use these properties? If I can't, could you propose to me an alternative? P.S. Please, don't try to use a very specific analysis theory; I am only an undergraduate student (bachelor).","['derivatives', 'real-analysis']"
1653517,"Why does the Möbius function take its values so often in $\{0,+1,-1\}$?","The Möbius function of a locally finite poset $P$ is defined on its intervals $[x,y] \subseteq P$ recursively by
$$\mu([x,x])=1$$
$$\forall x < y : \mu([x,y])=-\sum_{x \leq z < y} \mu([x,z])$$
In many examples (see Wikipedia ) it turns out that $\mu$ takes its values in $\{0,+1,-1\}$. Is there any conceptual reason for this? Also, is there some characterization of those posets $P$ for which this holds?","['combinatorics', 'mobius-inversion', 'soft-question', 'order-theory']"
1653533,Central limit theorem for uncorrelated identically distributed random variable,"I have a sum of random variables as bellow
$$Y=\sum_n A_n=\sum_n B_n\times C_n$$
where $B_n$s are correlated Gaussian random variables with zero mean, variance $1$ and correlation $E\{B_nB^*_r\}=\frac{1}{N}\sum_{l-0}^{L-1}\sigma^2_le^{\frac{j2\pi l(n-r)}{N}}$ where $\sum_{l=0}^{L-1}\sigma^2_l=1$ and $L$ and $N$ are integers ($L<N$). $C_n$s are independent and identically distributed random variables with zero mean and variance $P$. Also $B_n$ is independent of $C_n$. My question is that can the distribution of $Y$ be approximated as Gaussian? I know that the central limit theorem is true for sum of iid random variables but $Y$ is sum of uncorrelated identically distributed random variable. I can't prove whether $A_n$s are independent or not. Does the central limit theorem hold for my case? Thanks in advance","['stochastic-processes', 'law-of-large-numbers', 'statistics', 'central-limit-theorem']"
1653536,"Show that we cannot have a prime triplet of the form $p$, $p + 2$, $p + 4$ for $p >3$","Show that we cannot have a prime triplet of the form $p$, $p + 2$, $p + 4$ for $p >3$ I was a bit lost with this proof until I found a similar looking proof-based question from a previous homework assignment in this class which said ""If $a$ is an integer, prove that one of the numbers $a$, $a + 2$, $a + 4$ is divisible by 3. These problems seem very similar to me and would lead me to assume that they would be proven similarly, although I am unsure as to how I would approach this for prime triplets as I am brand new to them. Any help is appreciated","['number-theory', 'prime-numbers', 'divisibility']"
1653544,Can hyperbolic sine and cosine be combined into a single function of shifted argument?,"For trigonometric functions we have a nice identity: $$A\cos x+B\sin x=\sqrt{A^2+B^2}\sin(x+\operatorname{atan2}(A,B)).\tag1$$ At the core of it is the well-known identity of $$\sin^2x+\cos^2y=1,\tag2$$ which allows us to view $\frac A{\sqrt{A^2+B^2}}$ as sine and $\frac B{\sqrt{A^2+B^2}}$ as cosine of some angle (same angle for both expressions). The analogous identity for hyperbolic functions is $$\cosh^2x-\sinh^2x=1.\tag3$$ But I can't seem to be able to use it to find hyperbolic analogue for $(1)$. The difference of squares seems to be not constraining enough to allow for it, unlike the sum. So is there any identity for hyperbolic functions, that would be analogous to $(1)$?","['hyperbolic-functions', 'trigonometry']"
1653554,For every normed space the norm map is not Fréchet differentiable at $0$.,Argue that for every normed space $\mathbb{X} \neq \{ 0 \}$ the norm map $\| \ldotp \|_\mathbb{X} : \mathbb{X} \to \mathbb{R}$ is not Fréchet differentiable at $0$. Not really sure where to start on this question. I know the absolute value function is not Fréchet differentiable at $0$.,"['derivatives', 'normed-spaces', 'real-analysis']"
1653611,Eigenvalues of $MA$ versus eigenvalues of $A$ for orthogonal projection $M$,"Suppose that $M$ is symmetric idempotent $n\times n$ and has rank $n-k$. Suppose that $A$ is $n\times n$ and positive definite. Let $0<\nu_1\leq\nu_2\leq\ldots\nu_{n-k}$ be the nonzero eigenvalues of $MA$ and $0<\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n$ be the eigenvalues of $A$. I'm trying to show that
  $$
\forall i=1,\ldots,n-k:\quad 0<\lambda_i\leq\nu_i\leq\lambda_{i+k}\tag{$*$}
$$ There will be a 300 bounty for the accepted answer. Can someone also please make all the (attempted) proofs below as spoilers? I can only do that for the first proof. Attempt : I have an attempt here using Durbin and Watson (1950) but I don't fully understand the authors' argument so the attempt is incomplete. Nonetheless, I'll present the attempt here. Step 3 below is where I am stuck. Step 1 : One can write $M$ as $M_kM_{k-1}\cdots M_1$ where $M_i=I_n-p_ip_i'$ and $\{p_1,\ldots,p_k\}$ is a set of $n\times 1$ mutually orthogonal vectors s.t. $||p_i||=1$. Proof. $M$, by assumption, can be written as $M=I_n-X(X'X)^{-1}X'$ where $X$ is $n\times k$ with full column rank. Let $P=(p_1,\ldots,p_k)$ (dimension $n\times k$) be the $Q$ bit of the QR decomposition of $X$. Step 2 : Let $T=(T_1,\ldots,T_n)$ be an $n\times n$ matrix of orthonormal eigenvectors of $A$ that corresponds to eigenvalues $\lambda_1,\ldots,\lambda_n$. Let $l_{1i}=T_i'p_1$. Then any nonzero eigenvalue $\theta$ of $M_1A$ satisfies
$$
\sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j)=0.\tag{$**$}
$$ Proof. For any eigenvalue (possibly $0$) $\theta$ of $M_1A$, we have
  $$
0=|I_n\theta-M_1A|=|I_n\theta-(I_n-p_1p_1')A|=|I_n\theta-(I_n-l_1l_1')\Lambda|
$$
  Here, $l_1$ is the $n\times 1$ column vector with entries $l_{1i}$ and $\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_n)$. Write out $I_n\theta-(I_n-l_1l_1')\Lambda$ in full. Subtract $l_2/l_1$ times the first row from the second row, $l_3/l_1$ times the first row from the third row, and so on, and then execute the Laplace expansion along the first row. The result is
  $$
0=|I_n\theta-(I_n-l_1l_1')\Lambda|=\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_{1i}^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j).
$$
  Plugging $\theta=0$ in the rightmost expression above gives $\sum_{i=1}^2l_{1i}^2=1$. Thus, 
  \begin{align*}
0&=\sum_{i=1}^nl_{1i}^2\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j)\\
&=\sum_{i=1}^nl_{1i}^2(\theta-\lambda_i)\prod_{j\neq i}(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j)
\end{align*}
  which can be simplified and, for $\theta\neq 0$, divided by $\theta$ to get ($**$). Step 3 : Let $0=\cdots =0<\theta_1^{(s)}\leq \theta_2^{(s)}\leq \theta_{n-s}^{(s)}$ be the eigenvalues of $M_sM_{s-1}\cdots M_1A$. Then,
$$
\forall s=1,\ldots,k:\quad \theta_i^{(s-1)}\leq\theta_i^{(s)}\leq \theta_{i+1}^{(s-1)},\quad i=1,\ldots,n-s.\tag{$***$}
$$
Here the $\lambda_i$'s are taken to be the $\theta_i^{(0)}$'s. Proof. Let's build the first step for the case $s=1$. Consider
  $$
f(\theta)=\sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j)
$$
  and consider $[\lambda_r,\lambda_{r+1}]$ for $r=1,\ldots,n-1$. Either $f(\lambda_r)=0$ or $f(\lambda_{r+1})=0$ or $f(\lambda_r)f(\lambda_{r+1})\neq 0$. It's easy to show that in general $f(\lambda_r)f(\lambda_{r+1})\leq 0$ and so if $f(\lambda_r)f(\lambda_{r+1})\neq 0$ then $f(\lambda_r)f(\lambda_{r+1})< 0$ and so by the Intermediate Value Theorem, there is an zero of $f$ between $(\lambda_r,\lambda_{r+1})$. In sum, there is a zero of $f$ in each $[\lambda_r,\lambda_{r+1}]$ for each $r=1,\ldots,n-1$. It follows that
  $$
0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n.
$$
  This proves ($***$) for $s=1$. Proceed with $M_2M_1A$ as $M_2(M_1A)$ to get ($***$) for $s=2$. And so on. Step 4 :
($*$) holds. Proof. By Step 3, for $i=1,\ldots,n-k$,
  $$
\nu_i=\theta_i^{(k)}\geq \theta_i^{(k-1)}\geq  \cdots \geq \theta_i^{(1)} \geq \theta_i^{(0)}=\lambda_i.
$$
  Similarly,
  $$
\nu_i=\theta_i^{(k)}\geq \theta_{i+1}^{(k-1)}\geq  \cdots \geq \theta_{i+k-1}^{(1)} \geq \theta_{i+k}^{(0)}=\lambda_{i+k}.
$$ Problem with Step 3 . The case for $s=1$ and $M_1A$ relies on $A$ being diagonalizable. I don't think the same argument works for $M_2(M_1A)$ because we don't know the diagonalizability of $M_1A$. So I don't think the induction step in Step 3 works (Durbin and Watson (1950) claim it does.). Moreover, while I'm confident in the Intermediate Value Theorem argument, I'm not confident about the subsequent claim: It follows that
$$
0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n.
$$","['eigenvalues-eigenvectors', 'inequality', 'matrices', 'proof-verification', 'linear-algebra']"
1653632,Differentiation as Rotation,I am trying to make a connection between linear algebra and the Fourier transform. Functions form a vector space and differentiation is an operator. Fourier transforming a function from what i understand is writing the function in the basis of complex exponentials. Differentiation in the Fourier domain is multiplication by the imaginary unit which is also the eigenvalue of the 90 degree rotation matrix of the 2x2 rotational matrix. Is it possible to say that the the differential operator is diagonalised in the Fourier domain? and Is the differential operator the equivalent of the rotation matrix in finite dimensional matrices?,"['functional-analysis', 'linear-algebra']"
1653636,"How to obtain $\lim_{(x,y) \to (0,0)} (x^2·y^3)/(x^4+y^6)$","I want to determine $$\lim\limits_{(x,y) \to (0,0)} \frac{x^2·y^3}{x^4+y^6}$$ I'm sure the limit exists, it's zero because I tried to find other different limits in line and parabola points ($(x,mx)$ and $(x,mx^2)$) but every time I got zero. So, I have to use the definition or sequences to confirm.","['multivariable-calculus', 'real-analysis', 'limits']"
1653643,Finding of $\hat{\theta}_{MLE}$ of $f(x; \theta) = (\theta + 1)x^\theta$,"Let $X_1, \cdots, X_n$ be a random sample from the PDF: $f(x;\theta) = (\theta + 1) x^{\theta}$ with $0<x<1$ and $\theta > -1$. The likelihood function is:
\begin{align}
L(\theta) &= f(x_1, \cdots, x_n; \theta) \mathbb{1}\{0<x<1\} \mathbb{1}\{\theta >-1\}\\
&= \prod_{i=1}^{n}{f(x_i ; \theta)}\mathbb{1}\{0<x<1\} \mathbb{1}\{\theta >-1\}\\
&= \prod_{i=1}^{n}{(\theta+1)x_i^{\theta}}\mathbb{1}\{0<x<1\} \mathbb{1}\{\theta >-1\} \\
&=(\theta+1)^n \left( \prod_{i=1}^n{x_i}\right)^\theta
\end{align} We look at the log likelihood $l(\theta)$ and take the derivative of this since it is easier to deal with and it is allowed because the log is monotonic: \begin{align}
l(\theta) = n\log(\theta+1) + \theta \left( \sum_{i=1}^n{\log(x_i)} \right)
\end{align} $\implies \frac{d}{d\theta} = \frac{n}{\theta+1} + \sum_{i=1}^n{\log(x_i)} = 0$ $\implies \hat{\theta}_{MLE} = -\frac{n}{\sum_{i=1}^n{\log(x_i)}} - 1$ However, this doesn't look right.","['maximum-likelihood', 'statistics', 'log-likelihood', 'parameter-estimation']"
1653681,"Which values of $p$, $f$ is it differentiable at the point $(0,0)$?","Let $p \geq 1$ and $f: \mathbb{R^2} \to \mathbb{R}$ defined as $$f(x) =   \begin{cases}   (\sin \|x\|)^p \cos \frac{1}{\|x\|},     
 & \quad \text{if } \|x\| \not= 0  \\ 0,  & \quad \text{if } \|x\| = 0
 \\    \end{cases} $$ (a) Show that $f$ is differentiable at point $x \not= (0,0)$ - (Done) (b) Which values of $p$, $f$ is it differentiable at the point $(0,0)$? We have seen in class that $f$ is differentiable at a point $\hat{x}$ is equivalent to find a function $f'(\hat{x}) \in \mathcal{L}(\mathbb{R}^n, \mathbb{R})$ such that $$\lim_{x \to \hat{x}} \frac{|f(x)-f(\hat{x})-f'(\hat{x})(x-\hat{x})|}{h}=0,$$ and we have discovered that $f'(\hat{x})(x-\hat{x}) = \nabla f(\hat{x}) \cdot (x-\hat{x})$. So far I tried to discover the partial derivatives of $f$ at $(0,0)$ : $$\frac{\partial f}{\partial x_1}(0,0) = \lim_{h \to 0} \frac{f(h,0)-f(0,0)}{h} = \lim_{h \to 0} \frac{(sin |h|)^pcos \frac{1}{|h|}}{h}$$ intuitively, I think $p$ has to be greater than $1$. We know that $(sin |h|)^pcos \frac{1}{|h|}$ is bounded between $-1$ and $1$. I am stucked at this point. Is anyone could help me to solve the problem? Is it a better way to find the values of $p$ for which $f$ is differentiable at the point $(0,0)$? A related question is this link . P.S. Please, don't try to use a very specific analysis theory; I am only an undergraduate student (bachelor).","['derivatives', 'real-analysis', 'partial-derivative']"
1653691,Why isn't every Hamel basis a Schauder basis?,"I seem to have tripped on the common Hamel/Schauder confusion. If $X$ is any vector space (not necessarily finite dimension) and $B$ is a linearly independent subset that spans $X$, then $B$ is a Hamel basis for $X$. If there exists a sequence $(e_n)$ such that for every $x \in X$ there exists a unique sequence of scalars $(\alpha_n)$ for which $\lim_{n \to \infty} || x - (\alpha_1e_1 + \cdots + \alpha_ne_n)|| = 0$, then $(e_n)$ is a Schauder basis for $X$. So I'm tempted to think that every Hamel basis is also a Schauder basis; just extened the finite linear combination into an infinite one by adding zero coeeficients. I know I'm wrong, but what am I missing?","['functional-analysis', 'schauder-basis', 'hamel-basis']"
