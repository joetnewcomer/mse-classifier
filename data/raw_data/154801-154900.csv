question_id,title,body,tags
2615511,Probability that two random numbers have a Sørensen-Dice coefficient over a given threshold,"The following problem came up at work and my probability knowledge isn't up to the task. Let $a, b \in \mathbb{Z}_{2^n}$ be two $n$ bit integers. Their Sørensen-Dice coefficient is the quantity
$$
DS(a, b) = \frac{2|a \wedge b|}{|a| + |b|}
$$
where $|a|$ denotes the population count of $a$ (number of 1 bits) and $a\wedge b$ is bitwise AND. I would like to know: Given $t \in [0, 1]$, what is the probability that $DS(a, b) \ge t$ for $a, b$ drawn uniformly at random from $\mathbb{Z}_{2^n}$? Call this probability $P_n(t)$.  Experimentally this probability appears to have the form $$P_n(t) = \frac{1}{1 + 2^{\alpha(t - 1/2)}}$$ where $\alpha$ is some function of $n$. I expect this to be the form of the final answer, albeit with a more precise description of $\alpha$. Edit: In practice $n$ is 1024 or, more generally, some even power of 512 (I expect the result to hold for general $n$, but maybe that case is easier to handle as a first step). I would like to be better at solving these kinds of problems, so I will gratefully accept any recommendations for further reading too.","['book-recommendation', 'statistics', 'probability', 'elementary-probability']"
2615531,geometric view of similar vs congruent matrices,"I'm trying to understand similarity $(A \sim B \iff A = SBS^{-1})$ and congruence $(A \cong B \iff A = PBP^T,\, P\in GL(n, \mathbb{R}))$ through geometric analogies. For triangles: affine transformations (i.e., uniform scaling, rotation, reflection, translation[?]) preserve similarity. Two similar triangles have equivalent angles but may have different side lengths (i.e., they are measured in a different basis) unitary transformations (i.e., rotation, reflection, translation[?]) preserve congruence For matrices: similarity preserves the idea of a linear transformation; most ""geometrically"", eigenvalues are preserved, but angles between vectors are not preserved congruence preserves the above DOESN'T preserve the spectrum and but preserves angles between vectors (my interpretation of the bilinear form property) and the number of positive / negative / zero eigenvalues However, we only require $P$ to be invertible for matrix congruence $A = PBP^T$. Why don't we require $P$ to be unitary (i.e., why do we allow the spectrum to change)? Why is the adjoint $P^T$ important? Also, is there an analogy for thinking about (1) how angles between vectors are preserved by congruence but not similarity with matrices and (2) how angles between triangle legs are preserved with both similarity and congruence, or am I reading too much into the naming conventions?","['intuition', 'linear-algebra', 'linear-transformations', 'geometry']"
2615611,Tannery's Convergence Theorem,"This appears as the 7th exercise of Chapter 10 in Apostol's book of Mathematical Analysis . $\{f_n\}$ is a sequence of functions and $p_n$ is an increasing sequence such that $p_n \rightarrow +\infty$. We have that The sequence of functions $\{f_n\}$ converges uniformly to $f$ on $[a, b]$ for every $b \ge a$. Each $f_n$ is Riemann-integrable on $[a, b]$ for every $b \ge a$. $|f_n(x)| < g(x)$ almost everywhere on $[a, +\infty)$ for some nonegative g, which is improper Riemann integratable on $[a, +\infty)$. Prove that $f$ and $|f|$ are improper Riemann-integrable on $[a, +\infty)$, the sequence $\{\int_a^{p_n}f_n(x)dx\}$ converges and
$$
\int_a^{+\infty}f(x)dx = \lim_{n\rightarrow+\infty}\int_a^{p_n}f_n(x)dx.
$$
It is easy to verify that the improper Riemann integral exists, but the convergence of $\{\int_a^{p_n}f_n(x)dx\}$ is frustrating.",['analysis']
2615671,Are Stock Market graphs differentiable?,"I read investing in Stock Market was about predicting the future of a graph. Suppose we know the value of the graph at all points before $a$ and also at $a$. Then one could use a very small number $h$ to calculate the approximate left hand derivative at $a$ by using the formula: $$f'(a)=\frac{f(a-h)-f(a)}{-h}$$ Similarly, the successive left hand derivatives could be calculated by the formulas: $$f''(a)=\frac{f(a-2h)-2f(a-h)+f(a)}{h^2}$$ I believe the general formula is: $$f^n(a)=\frac{\sum_{r=0}^n (-1)^r\binom{n}{r}f(a-(n-r)h)}{(-h)^n}$$ Then one assumes all Left Hand Derivatives=Right Hand Derivatives to plot the approximate future of the graph by using the formula: $$f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+.....$$","['derivatives', 'taylor-expansion', 'calculus', 'graphing-functions']"
2615770,Kind of heat equation,"How to solve the following equation or at least in what direction should I think?
$$\frac {\partial u}{\partial t}=\frac {\partial^2 u}{\partial x^2}+tu$$
It's almost exactly the heat equation, but this $tu$ makes things bad. I don't know how to deal with it.",['ordinary-differential-equations']
2615794,Change of basis with rotation matrices,"Let's work on $\mathbb{R}^3$ for the time being and rotate our standard basis as 
$$e_1 \to e_2 \\
e_2 \to -e_1 \\
e_3 \to e_3,$$
hence our transition matrix is
$$P = \begin{bmatrix}
    0       & -1 & 0 \\
    1       & 0 & 0 \\
    0       & 0 & 1
\end{bmatrix}$$
, and consider the point $a = (1, 0 ,0)$ in standard basis. Now since we have rotated our, so called, x-y plane $90^{\circ}$ degree counterclockwise wrt z-axis, our point wrt to the new coordinate system should $a'=(0,-1,0)$. However, when we multiply $a$ with the matrix $P$ (from right, i.e P$\cdot $a), we get $(0, 1, 0)$, which is a wrong result, so where is my mistake ? I mean I have checked my logic and calculation over and over again, but couldn't find the mistake that I'm doing.","['rotations', 'change-of-basis', 'linear-algebra']"
2615805,Relating the roots of quadratic to an inverse trigonometric functions' question,"If $\alpha$ and $\beta$ are the roots of the equation $x^2-4x+1=0$ then find the value of $$f(\alpha, \beta) =\frac {(\beta)^3}{2} \text{cosec}^2\left (\frac {1}{2}\arctan \frac {\beta}{\alpha}\right) +\frac {(\alpha)^3}{2} \sec^2\left (\frac {1}{2}\arctan \frac {\alpha}{\beta}\right)$$ My try : I wrote $$\arctan \frac {\beta}{\alpha}=\theta$$ hence we get $$\arctan \frac {\alpha}{\beta}=\frac {\pi}{2}-\theta$$
Using this I tried to simplify the expression but the cubes are messing up the method i tried.  Do this type of question have any other method.  If not someone please provide a better way to solve the problem using my approach. Thanks.","['trigonometry', 'inverse-function', 'functions', 'algebra-precalculus', 'quadratics']"
2615820,Evaluate $\sum\limits_{n=1}^{2016}\frac{1}{n!+(n+1)!+(n+2)!}$,"Evaluate the following sum:
  $$\dfrac{1}{1!+2!+3!}+\dfrac{1}{2!+3!+4!}+\dots + \dfrac{1}{2016!+2017!+2018!}$$ I was trying to rewrite the general term as: $$\frac{1}{n!+(n+1)!+(n+2)!}=\frac{1}{n!(n+2)^2}$$ However, this did not give any essential improvements.",['sequences-and-series']
2615844,Is the least upper bound of a set necessarily outside the set?,"I'm reading a book on Real Analysis, which has the following definition of least upper bound: The paragraph after the bullet points leads me to understand that the least upper bound of a subset of the reals is necessarily outside the set. The paragraph after that leads me to understand the least upper bound is inside the set. Is it necessarily one or the other? I'm thinking open sets like $(0,1)$ have a least upper bound of one, which is outside the set, and sets with closed bounds like $(0,1]$ have their least upper bound inside the set. Am I understand understanding the concept of least upper bounds correctly, at least so far?","['real-analysis', 'elementary-set-theory']"
2615861,"Understanding Leibniz's Formula for $\pi/4$, geometric proof",I was reading about Leibniz's geometric proof for $\pi/4$ in this wiki-page . I understand this proof almost completely with the exception of one part where it is stated that: $$dC=\bigtriangleup OPQ=\frac{OR\cdot PQ}{2}=\frac{OR\cdot ds}{2}$$ It wasn't immediately obvious to me why the two triangles I've drawn have the same area? Edit : I've added and edited a figure which illustrates how I'm looking at the two triangles.,"['area', 'triangles', 'geometry']"
2615882,Q of the QR decomposition is an upper Hessenberg matrix,"Let $A=\{a_{ij}\}_{i,j=1}^n$ be a nonsingular upper hessenberg matrix, i.e., $a_{i,j}=0$ for $i>j+1$. I want to show that if $A=QR$ is the QR-decomposition of $A$ then $Q$ is also an upper hessenberg matrix. $$$$ For the QR decomposition, we can apply the Gram-Schmidt Algorithm: 
$$\tilde{q}_1=a_1 \Rightarrow q_1=\frac{\tilde{q}_1}{\|\tilde{q}_1\|_2} \\ \tilde{q}_{k+1}=a_{k+1}-\sum_{i=1}^k(a_{k+1}, q_i)q_i \Rightarrow q_{k+1}=\frac{\tilde{q}_{k+1}}{\|\tilde{q}_{k+1}\|_2}  $$ That means that each column of the $Q$ matrix involves a linear combination of all the previous columns, right? We have that $A$ is an upper Hessenberg matrix, i.e. it holds that $a_{ij}=0$ for $i>j+1$. How could we continue? Could yoyu giive me a hint?","['matrices', 'numerical-methods', 'matrix-decomposition']"
2615883,"If a topological space is homeomorphic to a smooth manifold, then will it be a smooth manifold?",If I have already known a topological space $N$ is homeomorphic to a smooth manifold $M$ then will it be a smooth manifold? The atlas of $N$ is the preimage of the atlas of $M$ and the coordinate map is the composition of homeomorphism composites the coordinate map?,"['differential-geometry', 'differential-topology']"
2615885,How do I show that $F(h) = h \circ h$ is surjective?,"We are looking at a function $F:\mathbb{N}^\mathbb{N}\to \mathbb{N}^\mathbb{N}$ $$F(h)=h\circ h$$
I want to show that $F$ is not surjective. I have a solution, but I'm not quite sure if this is a proof or I just missed something.
Let's take the function $h(x)=x$ and feed it to $F$. We will get $F(h(x))=h(h(x))$. The function $h(h(x))$ is always equal to $x$. So I say that $F$ is not surjective, because if I feed $h(x)$ in, some values like for instance $t(x)=x^2$ will not be hit in the codomain, so $F$ is not surjective. I am really unsure whether this proves the statement. I feel like it's too easy. If it's wrong, I'd really appreciate some tipps!","['functions', 'proof-verification']"
2615892,Strategy in a waiting time game: fisherman's dilemma,"Suppose there is a fisherman out on the lake, who repeatedly casts his line looking for fish. Let $X_k$ denote the time it takes him to catch a fish on his $k$th cast, $k \in \mathbb{N}$, where the $X_k$ are iid, sampled from some distribution function $F$, and so that that $X_k > 0$ and $\mathbb{E}X < \infty$. The fisherman knows $F$, and has control over one thing: he can choose to re-cast his line at any time. What is his optimal strategy to catch the most fish in the long run? There are situations where it is advantageous to re-cast: if $F$ is bi-modal, with the two modes very far apart, then the fisherman will wait for the first mode to pass, and if he hasn't caught a fish, re-cast the line rather than waiting to hit the next mode. I can characterize the distribution functions where it is sometimes advantageous to re-cast via a direct computation, though I am curious if there is a more elegant condition. If the fisherman hasn't seen a fish up to time $t$, then his expected waiting time to see a fish is \begin{equation}
\mathbb{E}[X - t | X > t] = \int_{t}^\infty \frac{F(s)}{F(t)} ds,
\end{equation} where I'm using $F(x) = \mathbb{P}(X > x)$. On the other hand, the unconditional expected waiting time is \begin{equation}
\mu = \mathbb{E}[X] = \int_0^\infty F(s) ds.
\end{equation} Thus, it is advantageous to re-cast our line at time $t$ if the conditional expected waiting time is larger than the unconditional waiting time, i.e. any time $t > 0$ satisfying \begin{equation}
F(t) \int_0^\infty F(s) ds < \int_t^\infty F(s) ds.
\end{equation} $\Big($ EDIT 2 : @DaneiWeissman pointed out that this condition isn't always the right one to figure out the optimal strategy. Assuming that the right strategy is to pick a time $t$ and always re-cast at that time, the waiting time for that strategy is \begin{equation} \mathbb{E}[X|X \leq t] + \sum_{j \geq 0} F(t)^j(1-F(t)) jt = \frac{1}{1-F(t)} \int_0^t (F(s) - F(t)) ds + \frac{t F(t)}{1-F(t)} \end{equation} \begin{equation} =\frac{1}{1-F(t)}\int_0^t F(s)ds.
\end{equation} This is because we wait a geometrically-distributed number of times with failure probability $F(t)$ before there is a fish in $[0,t]$, and once this does occur, it takes time $\mathbb{E}[X|X \leq t]$ to catch it on average. So we should be instead be trying to solve \begin{equation} \frac{1}{F(t)} \int_t^\infty F(s) ds = \frac{1}{1-F(t)} \int_0^t F(s) ds, \hspace{5pt} (\star \star)\end{equation} or perhaps just minimize the RHS over $t \in \mathbb{R}$. $\Big)$ For example, if these expressions are equal for all $t$, then differentiating with respect to $t$ yields \begin{equation}
\mu F'(t) = -F(t),
\end{equation} and $F(t) = e^{-t/\mu}$ is the only distribution function on $[0,\infty)$ satisfying this ODE. So if the $X$'s are exponential then re-casting at any time never hurts or helps us. ( This shouldn't surprise anyone familiar with Poisson processes, and the memory-less property of the exponential distribution! ) One can also compute directly with $F_\alpha(x) = (1+x)^{-\alpha}$, for $\alpha > 1$. This gives \begin{equation}
\mathbb{E}[X - t | X > t] = \frac{t+1}{\alpha - 1},
\end{equation} while \begin{equation} \mathbb{E}X = \frac{1}{\alpha - 1}.
\end{equation} So the fisherman should recast his line at every positive time in this case! I am interested in the times $t$ where we have equality, namely times $t$ when \begin{equation}
\mathbb{E}[X-t|X > t] = \mathbb{E}[X] \hspace{10pt} (\star)
\end{equation} My questions are: 1) What possible sets can occur as solution sets to $(\star)$ or $(\star \star)$? For example, can there be a single unique solution? $N$ solutions for some $N \in \mathbb{N}$? Can it be satisfied for all $t$ in some interval? 2) Is there a nicer condition to determine whether or not there is some solution to $(\star$) or $(\star \star)$? (I wonder if there is some way to compare $F$ to an exponential distribution that would help.) 3) Can the integral equations $(\star)$ or $(\star \star)$ be reduced to a more tractable form? 4) Are there any other nice classes of distribution functions for which they can be solved explicitly? 5) How would one make sense of the case where $\mathbb{E}X = \infty$? The equations $(\star)$ and $(\star \star)$ don't make sense anymore. Should the fisherman ever re-cast in this case? Also, I am curious if this question has been studied in any other context: it seems like a very natural setup to me! P.S. There are many simple possible variants on the original question. I wonder what would happen if one imposed a time-cost on re-casting the line, or if the fisherman doesn't know how long has passed since he cast his line. What would the optimal strategy be? EDIT 1: Another distribution one can prove something about is $F(t) = e^{-t^2}$, which decays faster than exponentially. We have $\mathbb{E}X = \frac{\sqrt{\pi}}{2}$, while \begin{equation}
\mathbb{E}[X-t | X > t] = e^{t^2} \int_{t}^\infty e^{-s^2} ds.
\end{equation} It is straightforward to check that this function is strictly decreasing, and \begin{equation}
\lim_{t \to 0^+} \mathbb{E}[X-t | X > t] = \mathbb{E}X,
\end{equation} which holds for any distribution function $F$ (since $F$ has right limits and $F(0) = 1$). Thus, in this case the fisherman should never recast his line.","['statistics', 'probability', 'analysis']"
2615893,"f is a continuous function from (X,$\tau$) to {0,1} with discrete topology, if f non constant then (X,$\tau$) disconnected","Let $f$ be a continuous function such that $f : (X,\tau) \rightarrow (\{0,1\},\tau_1\}$. Where $(X,\tau)$ is a generic topological space and $\tau_1$ is the discrete topology. I want to prove that if f is non-constant then $(X,\tau)$ is disconnected. I started by describing $(\{0,1\},\tau_1\}$. This topological space is compact, totally disconnected and Hausdorff. However,from here I do not know how to continue. Any tips?","['general-topology', 'connectedness']"
2615909,The Wikipedia definition of an Antisymmetric relation,"The Wikipedia definition of an Antisymmetric relation says : R is antisymmetric precisely if for all $a$ and  $b$ in $X$ \begin{align}
\text{if } R(a,b) \text{ and } R(b,a) \text{ then } a=b.
\end{align} or, equivalently \begin{align}
\text{if } R(a,b) \text{ with } a\ne b  \text{ then } R(b,a) \text{ must not hold. }
\end{align} My Question : Shouldn't the contrapositive of the first statement say \begin{align}
\text{if } a\ne b \text{ then } R(a,b) \text{ or } R(b,a).
\end{align} and in considering the mathematical sense of ""or"", it might as well happen that even if $R(a,b)$ holds ; nothing stops from $R(b,a)$ NOT being true. Please help me resolve this.","['logic', 'elementary-set-theory']"
2615949,Scheme Theoretic Image (Hartshorne Ex.II.3.11.d),"The exercise asks to establish that given a morphism of schemes $f: Z \rightarrow X$ , there is a unique closed subscheme $Y \rightarrow X$ , such that 1) $f$ factors through $Y$ , and 2) whenever $Y'$ is another closed subscheme of $X$ such that $f$ factors through $Y'$ , then $Y \rightarrow X$ factors through $Y'$ . Most of the treatments of this problem i came across use a sheaf-of-ideals approach. Instead, i have been thinking about two more direct approaches (direct in the sense that Hartshorne poses this problem prior to introducing sheaves of ideals). First Aproach: As a topological space, let $Y$ be the topological closure in $X$ of the image of $f$ . Let $i: Y \rightarrow X$ be the inclusion map, and assign to $Y$ the sheaf $\mathcal O_Y:=i^{-1} \operatorname{im}f^{\#}$ , where $f^{\#}: \mathcal{O}_X \rightarrow f_* \mathcal{O}_Z$ . Then the morphism of sheaves $\mathcal{O}_X \rightarrow i_* \mathcal{O}_Y$ is the one given by composing $f^{\#}$ with the canonical morphism $\operatorname{im}f^{\#} \rightarrow i_* i^{-1} \operatorname{im}f^{\#}$ , and it is surjective by construction. Denoting by $f'$ the morphism $f$ with target $Y$ , one gives a morphism $ \mathcal{O}_Y \rightarrow f'_* \mathcal{O}_Z$ , by starting with the inclusion $\operatorname{im}f^{\#} \rightarrow f_* \mathcal{O}_Z$ , and then passing to $\mathcal{O}_Y=i^{-1}\operatorname{im}f^{\#} \rightarrow i^{-1} f_* \mathcal{O}_Z = f'_* \mathcal{O}_Z$ . One similarly checks the commutation of the diagram on the level of sheaves and the universal property of $Y$ . Do you agree with this approach? Second Aproach: If $X = \operatorname{Spec} A$ is affine, then we can cover $Z$ by open affines $\operatorname{Spec} B_i$ and the morphism $f$ is given locally by ring homomorphisms $\phi_i : A \rightarrow B_i$ . Then we can take $Y$ to be $\operatorname{Spec} (A/\cap_i\operatorname{ker} \phi_i)$ . If $X$ is not affine, then it is reasonable to cover it by open affines $X = \bigcup \operatorname{Spec} A_j$ , define $Y$ locally at each $\operatorname{Spec} A_j$ as above, and then glue the $Y_j$ . However, this might be problematic because the union of all $Y_j$ might not even be a closed set of $X$ . How can this difficulty be surpassed?","['schemes', 'algebraic-geometry']"
2615996,. There exists a nondiagonal matrix $A\in \mathcal {M}_n (\mathbb{R}) $ s.t. $A^{k+1}=I_n $ and $I_n-A $ invertible?,Let $k\in \mathbb {N} $. There exists a nondiagonal  matrix $A\in \mathcal {M}_n (\mathbb{R}) $ s.t. $A^{k+1}=I_n $ and  $I_n-A $ invertible?,"['matrices', 'abstract-algebra', 'linear-algebra']"
2616022,Derive a sufficient statistic for $\theta$.,"Exercise: Suppose that $X = X_1,\ldots,X_n$ are i.i.d. random variables with the $\operatorname{Ber}(\theta)$ distribution.
Derive a sufficient statistic for $\theta$. What I've tried: I know that by the Factorisation theorem $T(X)$ is a sufficient statistic if the likelihood function of $X$ can be written as $$f_X(X\mid\theta) = g_\theta(T(X))h(X)$$ where $h$ and $g_\theta$ are nonnegative Borel functions. So I've chosen to rewrite the likelihood function in the form of the factorisation theorem: $$f_X(X\mid\theta) = \prod_{i = 1}^n\theta^{x_i}(1-\theta)^{1-x_i} = \theta^{\sum_{i =1}^nx_i}(1-\theta)^{\sum_{i = 1}^n(1-x_i)}$$ However, I don't know how to proceed from here, as I can't see how this would been rewritten to the form used in the factorisation theorem. Question: How do I solve this exercise? (Preferably with the factorisation theorem) Thanks!","['statistics', 'statistical-inference', 'probability-distributions']"
2616025,Prove reflection property of parabola,"Problem from Calculus with Analytic Geometry 2nd ed. from Simmons, page 87/21(c). Let $p$ be a positive constant and consider the parabola $x^2 = 4py$ with vertex at the origin and focus at the point $(0, p)$. Let $(x_0, y_0)$ be a point on this parabola other than the vertex. Assume we've proved tangent at $(x_0, y_0)$ has $y$-intercept $(0, -y_0)$ triangle with vertices $(x_0, y_0), (0, -y_0), (0, p)$ is isosceles. Now suppose that a source of light is placed at the focus of a parabola, and assume that each ray of light leaving the focus is reflected off the parabola in such a way that it makes equal angles with the tangent line at the point of reflection (the angle of incidence equals the angle
of reflection). Use (b) to show that after reflection each ray points vertically upward, parallel to the axis. My analysis is here. But I don't know how to prove that angle $b = 90 \deg$:","['derivatives', 'conic-sections']"
2616046,Trying to figure out $\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n)$,"Let $(\Bbb{X}, \Sigma, \mu)$ be a measure space and Let $(A_n) \subset \Sigma$. The following property is satisfied: $\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n)$ where $\liminf_{n\to \infty}A_n = \displaystyle \bigcup_{n=1}^\infty \bigcap_{k=n}^\infty A_k$. I noticed that this property appears all the time (in Folland's book anyway) and I'm not sure I have grasped the notion correctly - or if to make it concrete and dumb - why aren't the two sides equal? Can anyone give an illustration (or a link to one), that will clarify this. Either a numerical illustration or a graphic one. Thanks.","['supremum-and-infimum', 'limsup-and-liminf', 'measure-theory', 'limits']"
2616061,Confusion about the conductor for Dirichlet characters,"I wanted to understand the definition of the conductor for Dirichlet characters and I read https://en.wikipedia.org/wiki/Dirichlet_character :
""We can formalize this differently by defining characters $\chi_1$ mod $N_1$ and $\chi_2$ mod $N_2$ to be co-trained if for some modulus $N$ such that $N_1$ and $N_2$ both divide $N$ we have $\chi_1(n) = \chi_2(n)$ for all $n$ coprime to $N$: that is, there is some character $\chi^*$ induced by each of $\chi_1$ and $\chi_2$. This is an equivalence relation on characters. A character with the smallest modulus in an equivalence class is primitive and this smallest modulus is the conductor of the characters in the class."" I am just really confused what they mean by it. Could someone possibly explain me what it means in a way that I can understand?","['number-theory', 'characters', 'elementary-number-theory']"
2616076,Ideal sheaf of points on a surface,"Let $S$ be a smoooth projective surface and $Z\subset S$ a zero-dimensional subscheme of $S$ corresponding to a bunch of distinct points $x_1,\dots,x_r\in S$ (i.e. each counted with multiplicity 1). Let $L$ be a line bundle on $S$, say very ample for the seek of simplicity. My question is: how should I be thinking of the sheaf $L\otimes\mathcal{I}_Z$, where $\mathcal{I}_Z$ is the ideal sheaf of $Z$ in $S$ ? For example, what is the geometric interpretation of the global sections $H^0(L\otimes\mathcal{I}_Z)$? If we were on a curve then I would see it as the subspace of divisors in $|L|$ passing through each of the $x_i$ (with multiplicity 1). But on a surface, I am not sure if this is still the correct interpretation. Any shared insight and/or concrete example would be greatly appreciated. Possible second question: how about if we add more multiplicities?",['algebraic-geometry']
2616092,Definition of Null-Sets using closed cuboids proof & Image of Null-sets under transformation.,"Let $Q$ be a closed, $n$-dimensional rectangle/cuboid and $N \subseteq Q$. Let $ U \supseteq Q $ be an open subset. Let $\varphi:U \to \mathbb{R}^n$ be a continuously differentiable function. First claim Show that there exists a constant $L \geq  0$ such that the image $\varphi(Q)$ is contained within a cuboid $Q_{\varphi}$, whose maximal side length is at most $L$ times as long as the longest side of $Q$ Second claim We call $Q$ cube-like if $$\max_{k = 1,..,n}(b_k-a_k)\leq 2 \min_{k = 1,..,n}(b_k-a_k)$$ ($a_k$ and $b_k$ are the coordinates of the $k$th side of the cuboid.) Show that Jordan-null-sets can also be defined with closed cube-like cuboids instead of open cuboids. Third claim Show that $\varphi(N)$ is a null-set if $N$ is a null set. Hints The exercise gives me the hint to show in 1. that $\varphi$ is Lipschitz continuous. My thoughts on how to solve this: I have no idea how I can show that $\varphi$ on $Q$ is Lipschitz continuous (I'd need your help here). no idea how to prove. We know that $N$ is a (Lebesgue) null set iff $\forall \epsilon > 0 \ \exists \{U_n \}_n (N \subset \bigcup_1 ^\infty U_n \ \land \ \sum_1^\infty vol(U_n) < \epsilon )$ where $U_i$ is an open cuboid, I need to show that closed cuboids work as well. I would do this using defining the closed cubes $\overline{U _i} \supseteq U_i \implies N \subset \bigcup_1 ^\infty \overline{U _i} $. When then need to somehow show that $ \sum _1 ^\infty vol(\overline{U _i}) < \epsilon$ Maybe we can use the fact that in 2. we have shown that open cuboids, as well as closed cube-like cuboids, can be used to define null sets. Your help is very appreciated since I don't even really know where to start.","['real-analysis', 'integration', 'measure-theory']"
2616113,Mean squared error of a sample covariance matrix,"I am trying prove a claim from some paper that the mean squared error of the sample covariance estimator $\bf{S}=\frac{1}{n}\bf{X}\bf{X}^T$ of the covariance matrix $\Sigma$ is $\mathrm{MSE}(\bf{S})=\frac{1}{n}[\mathrm{tr}(\Sigma^2)+\mathrm{tr}^2(\Sigma)]$. Here $\bf{X}$ are $p\times{}n$ matrices of samples drawn from the multivariate normal distribution $N_p(0,\Sigma)$. $\bf{S}$ and $\Sigma$ are symmetric matrices. The MSE is defined as $\mathrm{MSE}(\bf{S})= \mathbb{E}(\left\lVert \Sigma{}-S \right\rVert^2)$, where $\mathbb{E}()$ is the expectation value and $\left\lVert \bf{A} \right\rVert=\mathrm{tr}(\bf{A}^T\bf{A})^{1/2}$ is the Frobenius norm. Here is my attempt to solving this: 
\begin{align}\mathrm{MSE}\left(\bf{S}\right)
=&
\mathbb{E}(\left\lVert \Sigma{}-\bf{S} \right\rVert^2)
\\
=&
\mathbb{E}(\mathrm{tr}[(\Sigma{}-\bf{S})^T(\Sigma{}-S)])
\\
=&
\mathbb{E}(\mathrm{tr}[\Sigma^2-\Sigma{}\bf{S}-\bf{S}\Sigma{}+\bf{S}^2])
\\
=&
\mathrm{tr}[\Sigma^2]-2\mathbb{E}(\mathrm{tr}[\bf{S}\Sigma])+\mathbb{E}(\mathrm{tr}[\bf{S}^2])\\
=&
\mathrm{tr}[\Sigma^2]-2 \mathrm{tr}[\mathbb{E}(\bf{S})\Sigma]+\mathrm{tr}[\mathbb{E}(\bf{S}^2)]
\end{align} Now the problem has reduced to computing $\mathbb{E}(\bf{S})$ and $\mathbb{E}(\bf{S}^2)$. My initial thoughts were that $\mathbb{E}(\bf{S})=\Sigma$ and $\mathbb{E}(\bf{S}^2)=\Sigma^2$, but then the whole expression above is $0$... Could someone help by pointing out what am I doing wrong? Update: I found some problems with my reasoning. The expectation of $\bf{XX}^T$ is not $\Sigma$. Before going for the expectations, I am still struggling with computing the two necessary traces of the following type: $$\mathrm{tr}(\Sigma{}\bf{XX}^T)=\mathrm{tr}(\Sigma{}\Sigma{}^{1/2}\bf{YY}^T\Sigma{}^{1/2~T})$$ and $$\mathrm{tr}(\bf{XX}^T\bf{XX}^T)=\mathrm{tr}(\Sigma{}^{1/2}\bf{YY}^T\Sigma{}^{1/2~T}\Sigma{}^{1/2}\bf{YY}^T\Sigma{}^{1/2~T}),$$
where $\bf{Y}$ are samples from standard normal distributions and $\Sigma{}^{1/2}$ is a lower triangular matrix as obtained by Cholesky decomposition.","['expectation', 'statistics', 'linear-algebra']"
2616120,"$x_1,x_2\in \mathbb R^2 $ or $(x_1,x_2)\in \mathbb R^2 $ for this function?","Say I have the function $f(x_1,x_2)=x_1+x_2$ and I want to use vector notation. Q1: I set $\mathbf x=x_1\hat e_1+x_2\hat e_2=(x_1,x_2)$, so $f(x_1,x_2)=f(\mathbf x)$ and I now have $f(\mathbf x)=x_1+x_2$.
I guess this is correct? Q2 But should I write
$$
f(\mathbf x)=x_1+x_2, \quad x_1,x_2\in \mathbb R^2 \quad \tag 1
$$ 
or 
$$
f(\mathbf x)=x_1+x_2, \quad (x_1,x_2)\in \mathbb R^2 \quad \tag 2
$$","['multivariable-calculus', 'vectors']"
2616144,Maximum of a product of absolute values,"Let $f:[1008, 1009] \to \Bbb R$ be defined by $$f(x) =|x| \cdot |x-1| \cdot |x-2| \cdots |x-2017|$$ Find the maximum of $f(x)$ without using derivatives.","['a.m.-g.m.-inequality', 'optimization', 'functions', 'maxima-minima', 'absolute-value']"
2616180,Number of ways of filling $n \times n$ binary matrix,In how many ways can we fill $n \times n$ matrix with $+1$ or $-1$ such that the product of the entries in each row and each column equals $-1$?,"['matrices', 'combinatorics', 'combinations']"
2616214,Simplify the expression: $a^{\log {\sqrt \frac bc}}×b^{\log {\sqrt \frac ca}}×c^{\log {\sqrt \frac ab}}$,"My problem is Simplify the expression:$$a^{\log {\sqrt \frac bc}}×b^{\log {\sqrt \frac ca}}×c^{\log {\sqrt \frac ab}}$$ Here  $a,b,c \in \mathbb {R^+}$ My way: $$\begin{cases}
\frac bc=e^x \\
\frac ca=e^y \\
\frac ab=e^z \\
\end{cases} \Rightarrow e^{x+y+z}=1 \Rightarrow x+y+z=0 $$ $$a^{\log {\sqrt \frac bc}}×b^{\log {\sqrt \frac ca}}×c^{\log {\sqrt \frac ab}} = \sqrt {a^{\log e^x}} ×\sqrt {b^{\log e^y}}×\sqrt {c^{\log e^z}}=\sqrt {a^x×b^y×c^z}=\sqrt {a^x×\frac{a^y}{e^{yz}}×a^ze^{yz}}=\sqrt {a^{x+y+z}}=\sqrt {a^0}=1$$ Is this method correct and is there a better way? Thank you!","['radicals', 'logarithms', 'proof-verification', 'algebra-precalculus', 'proof-writing']"
2616215,Hidden variables in probability,"I came up with a mental problem in probability and I found out that it's either trivial or more complex than what I thought. The problem is this: Take a coin of diameter $D$ and radius $R$ , and toss it. Cover the table with a rectangular tablecloth made of small squares of side $\ell$ , with $\ell \geq D$ . What is the probability for the coin to land exactly inside a little square? If the problem has a trivial solution then I expect it to be this: the probability is the area of the coin over the area of the square: $$P = \frac{\pi R^2}{\ell^2}$$ But here I would a sort of $\theta$ function such that (in case this hypothesis is missing) the probability is zero when $D>\ell$ hence: $$P = \frac{\pi R^2}{\ell^2}\theta(\ell - 2R)$$ But here many problems arise: first of all my convention demands that $\theta (\ell - 2R) = 1$ for $2R = \ell$ , because as improbable it is, there could be a golden toss by which the coin lands exactly perfectly within the square the side of which is identical to the diameter. But now I strongly believe I shall consider the whole number of squares of the tablecloth. But more squares = more probability? I'm stuck on an either trivial or complicated reasoning. Any clarification?","['probability-theory', 'probability', 'geometric-probability']"
2616280,"If a compact set is covered by a finite union of open balls of same radii, can we always get a lesser radius?","This question seems obvious, but I'm not secure of my proof. If a compact set $V\subset \mathbb{R^n}$ is covered by a finite union of open balls of common radii $C(r):=\bigcup_{i=1}^m B(c_i,r)$, then is it true that there exists $0<s<r$ such that $V\subseteq C(s)$ as well? The centers are fixed. I believe this statement is true and this is my attempt to prove it: Each point of $v\in V$ is an interior point of least one ball (suppose its index is $j_v$), that is, there exists $\varepsilon_v>0$ such that $B(v,\varepsilon_v)\subseteq B(c_{j_v},r)$, so $v\in B(c_{j_v},r-\varepsilon_v)$. Lets consider only the greatest $\varepsilon_v$ such that this holds. Then defining $\varepsilon:=\inf\{\varepsilon_v\mid v\in V\}$ and $s=r-\varepsilon$ we get $V\subseteq C(s)$. But why is $\varepsilon$ not zero? I thought that considering the greatest $\varepsilon_v$ was important, but still couldn't convince myself. I would appreciate any help.","['general-topology', 'real-analysis', 'metric-spaces']"
2616298,Algebraic geometry and algebraic topology used in string theory,"I am looking for a comprehensive book or notes in algebraic geometry and algebraic topology techniques used in string theory compactifications covering topics like orientifolds, orbiolds, Calabi-Yau manifolds and toric geometry, divisors, resolution of singularities, fiber bundles etc. If it contains explicit examples and exercises it would be useful.","['algebraic-geometry', 'string-theory', 'mathematical-physics', 'book-recommendation', 'algebraic-topology']"
2616300,Exercise on integration of a function in two variables,"For which values of $a,b > 0$ is the function $f : (0, \infty)$ x $\mathbb{R} \to \mathbb{R}: (x,y) \mapsto \dfrac{1}{x^a(1+x^2+y^2)^b}$ integrable? I have tried to perform to change this function to a function in polar coordinates but then I get a $\int_{-\pi/2}^{\pi/2} \dfrac{1}{cos^a(\theta)}d\theta$ which is not integrable for any $a > 0$. Does somebody have an idea how I can solve this problem with another method or substitution?","['multiple-integral', 'real-analysis', 'integration', 'lebesgue-integral']"
2616313,Evaluate limit containing $\sum{n^6}$,"Evaluate: $$\lim_{n\to\infty}{\frac{1^6+2^6+3^6+\ldots+n^6}{(1^2+2^2+3^2+\ldots+n^2)(1^3+2^3+3^3+\ldots+n^3)}}$$ I can solve the denominator as: $$\frac{n(n+1)(2n+1)}{6}\cdot\frac{n^2(n+1)^2}{4}$$ $$n^7\cdot\frac{(1+\frac{1}{n})(2+\frac{1}{n})}{6}\cdot\frac{(1+\frac{1}{n})}{4}$$
$$=\frac{n^7}{12}$$ How can I reduce the numerator?","['limits-without-lhopital', 'limits']"
2616326,A and B are two matrices such that $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ then $ AB=BA$,"Let $A$ and $B$ be two invertible matrices in $M_2(\mathbb{R})$such that $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ then prove or disprove that $ AB=BA$ My working: $$(A+B)^3=A^3+3A^2B+3AB^2+B^3$$
$$\implies BA^2+B^2A+ABA+BAB =2A^2B+2AB^2$$ Now what should I do?",['matrices']
2616354,Generalization for Catalan number,"I know that the number of ways to open and close $n$ parenthesis is the $n$-th Catalan number. What about the number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis? Example: ))())() is a sequence of length 7 that contains exactly 2 well open and closed parenthesis. Basic fact: The number of sequences of open and closed parenthesis of length $k$ is equal to $2^k;$ The number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis is obviously greater that the the $n$-th Catalan number, but which is the precise value?","['combinatorics', 'probability', 'catalan-numbers']"
2616406,The set of all strictly increasing sequences of natural numbers,"The set of all strictly increasing sequences  ($a_n$) of natural numbers has cardinality $\mathbb{N}$, $\mathcal{P}(\mathbb{N})$ or $\mathcal{P}( \mathcal{P}(\mathbb{N}))$? I answered $\mathcal{P}(\mathcal{P}(\mathbb{N}))$ because as the set can be described by $X=\{\{\ldots, a_n, \ldots \},\{\ldots,b_n,\ldots\}, \ldots\}$ as it is not known if there are any limited sequences (so, assuming all have infinite elements), the cardinality of $X$ could not be the first option $\mathbb{N}$ because considering any function $\phi : \mathbb{N} \rightarrow X$ there would always be elements not listed from $X$. So I thought it would possible to say that $|X|\ge | \mathcal{P}(\mathbb{N})|$ and thinking that trying to build any bijection $\varphi: \mathcal{P}(\mathbb{N}) \rightarrow X$ would also not be possible as there would be elements from $X$ not listed too. So than I got $|X| = |\mathcal{P}( \mathcal{P}(\mathbb{N}))|$, but I would like to know if I answered correct and, if not, why?","['sequences-and-series', 'elementary-set-theory']"
2616517,Real numbers to real powers,"Suppose that $a,x,y\in\mathbb R$ with $a\ge1$ and $x,y\ge0$. We define the real power $a^x$ as follows: $$a^x=\sup\{a^r:0\le r\le x,r\in\mathbb Q\}$$ I've tried some ways to prove that $a^xa^y=a^{x+y}$, for example If we let $\alpha=a^x$, $\beta=a^y$ and $\gamma=a^{x+y}$, as a first stage I should prove that $\alpha\beta$ is an upper for the set $\{a^t:0\le t\le x+y,t\in\mathbb Q\}$. So I assume that $0\le t\le x+y$, so $-x\le t-x\le y$ which implies that $a^{t-x}\le\beta$. Since $a^x=\alpha$, this implies that $a^t\le\alpha\beta$. But I think that this proof has a problem, since $t-x$ can be in $\mathbb R\setminus \mathbb Q$. As a second stage, for $c<\alpha\beta$, I should find a $t\in \mathbb Q,0\le t\le x+y$ such that $a^t>c$. Since $c<\alpha\beta$, so $\frac c\alpha<\beta$, so there is an $s\in\mathbb Q,0\le s\le y$ such that $a^s>\frac c\alpha$. Similarly there is a $r\in\mathbb Q,0\le r\le x$ such that $a^r>\frac c\beta$. Therefore $a^{r+s}>\frac{c^2}{\alpha\beta}$. So from here I think we can't obtain anything. How can I solve these problems? Thanks.","['elementary-set-theory', 'real-analysis', 'supremum-and-infimum', 'analysis']"
2616536,Length of geodesic line equals distance between two points?,"Suppose that $M$ is a riemannian manifold, such that for any two distinct points there is a unique geodesic line connecting them. Does that imply that these geodesics are length-minimizing? That is to say, is the length of a geodesic between two points equal to the distance between the points? My effores led to being able to prove it, given the ""geodesic triangle inequality"" - the length of a geodesic between two points is $\leq$ the sum of distances of two geodesics connecting the two points to a third point. However I can't prove it.","['riemannian-geometry', 'differential-geometry', 'geodesic']"
2616572,How to solve the functional equation $f(x + f(x +y ) ) = f(2x) + y$?,"Find all functions $f:\mathbb{R}\rightarrow \mathbb{R}$ that satisfy the following equation: $$ f(x + f(x +y ) ) = f(2x) + y,\quad \forall x,y\in\mathbb{R}$$ The only function I have found is $f(x) = x$, but I think there are more.","['functions', 'functional-equations']"
2616621,"Omega limit set $\omega(x_0)$ for $r' = 2μr(5-r^2), \space θ' = -1$","Exercise : Given the dynamical system :
  $$x_1' = x_2+2μx_1(5-x_1^2-x_2^2)$$
  $$x_2' = -x_1+2μx_2(5-x_1^2-x_2^2)$$
  where $(x_1,x_2) \in \mathbb R^2$ and $μ>0$ a constant. Applying polar coordinates, determine the omega limit set $ω(x_0)$ for any given vector $(x_1,x_2)$. Discussion : I used the polar coordinates substitution : $$x_1 = r\cosθ$$
$$x_2 = r\sinθ$$ and via the expressions : $$rr' = x_1x_1' + x_2x_2'$$ $$θ' = \frac{x_1x_2' - x_2x_1'}{r^2}$$ I derived the system in polar coordinates : $$r' = 2μr(5-r^2)$$
$$θ' = -1$$ Now, what we can see is that the sign of $r'$ entirely depends on the factor $5-r^2$, since by it's definition $r>0$ and $μ>0$. This means that until $r=\sqrt5$ $r'>0$ and after that $r'<0$. Also, noting that $θ'=-1$ means that the direction of the phase portrait flow follows an anticlockwise flow. Other than that, defining an one dimension phase portrait for $r'$ and noting as a right arrow for where $r'>0$ and left for $r'<0$, we get : $$--- \quad 0 \quad \rightarrow \sqrt{5} \leftarrow$$ This means that there is a circle defined with $r=\sqrt{5}$. This implies that the omega limit set for any given values, will be : $$\text{For} \space x_0 \neq 0 \space \rightarrow \space ω(x_0) = S_{\sqrt5}$$ $$\text{For} \space x_0 = 0 \space \rightarrow \space ω(x_0) = \{0\}$$ Question : Is the above approach correct ?","['stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
2616648,How do I use Fubini to show a set has the measure 0?,"I am trying to show, that for fixed $v$ and $y \in \mathbb{R}^n$ the intersection of $L_y = \{y+tv ; t \in \mathbb{R}\}$ and a set E with  n-dimensional measure 0, has 1-dimensional measure 0 for almost every $y \in \mathbb{R}^n$.
Since $L_y$ is a line the intersection $L_y \cap E$ is one dimensional.
I am supposed to use Fubini's theoreme for the proof. My Idea was to define $\varphi : \mathbb{R}^n \to \mathbb{R}$ with $\varphi(y) := \lambda^1[L_y \cap E ] = \int_\mathbb{R} \chi_{E}(y+tv) dt$ 
 and  $\psi:\mathbb{R}^{n-1} \to \mathbb{R}$ with $ \psi(y) := \lambda^1[L_y \cap E] = \int_\mathbb{R} \chi_{E }(y+tv) \ dt.$ Than we know: $
0=\lambda^n[E] = \int_{\mathbb{R}^n} \chi_{E } \ d\lambda^n = \int_{\mathbb{R}^{n-1}}\int_\mathbb{R} \chi_{E}(y+tv)  dtdy = \int_{\mathbb{R}^{n-1}} \psi(y)  dy$. Now we know that $\psi = 0$ almost everywhere on $\mathbb{R}^{n-1}$. I want to use Fubini again to show that $\varphi = 0$ almost everywhere on $\mathbb{R}^{n}$ Edit: Therefore I define a function $h :\mathbb{R}^{n-1} \times \mathbb{R}$ by $h(x, z):= \varphi(y)$ for $(x, z) =y$ Now $\int_{\mathbb{R}^n} \varphi \ d\lambda^n = \int_{\mathbb{R}^n} h(x,z) \ d\lambda^n\overset{\text{Fubini}}{=} \int_\mathbb{R}\int_{\mathbb{R}^{n-1}}  h(x,z) \ dx dz = 0$ Because for fixed z $h(x,z) = \psi(x)$.","['functional-analysis', 'lebesgue-integral', 'measure-theory']"
2616673,"Inverse of a map $T_{(p,q)}(X \times Y) \to T_p X \times T_p Y$","I'm trying to show that the maps defined in https://math.stackexchange.com/a/413846/500094 are mutual inverses. I have problem with one direction: $(g\circ f)(v)=g(d(\pi_X)_{(p,q)}v,d(\pi_Y)_{(p,q)}v)=d(\iota_X)_p(d(\pi_X)_{(p,q)}v)+d(\iota_Y)_q(d(\pi_X)_{(p,q)}v)=\\d(\iota_X\circ\pi_X)_{(p,q)}v+d(\iota_Y\circ\pi_Y)_{(p,q)}v=v+v=2v\ne v$ At which point am I mistaken?","['real-analysis', 'function-and-relation-composition', 'functions']"
2616697,Show that $\Bbb Z\Big(\frac{1+\sqrt{d}}{2}\Big)$ is closed under multiplication.,"Here, we have that $d\equiv 1\bmod4$ and $$\Bbb Z\Big(\frac{1+\sqrt{d}}{2}\Big):=\Big\{u+v\Big(\frac{1+\sqrt{d}}{2}\Big): u,v\in\Bbb Z\Big\}.$$ Here is my attempt so far: Suppose $A:=u+v\Big(\frac{1+\sqrt{d}}{2}\Big)$ and $B:=w+x\Big(\frac{1+\sqrt{d}}
{2}\Big)$. Now, we consider the product $$\Big(u+v\Big(\frac{1+\sqrt{d}}{2}\Big)\Big)\cdot \Big(w+x\Big(\frac{1+\sqrt{d}}
{2}\Big)\Big)=uw+ux\Big(\frac{1+\sqrt{d}}
{2}\Big)+\frac{vx}{4}+\frac{vxd}{4}+\frac{vx\sqrt{d}}{4}.$$ Now, write $d=4k+1$ for some $k\in\Bbb Z$, then $$AB=uw+ux\Big(\frac{1+\sqrt{d}}
{2}\Big)+\frac{vx}{4}+\frac{vx(4k+1)}{4}+\frac{vx\sqrt{d}}{4}\\
=uw+ux\Big(\frac{1+\sqrt{d}}
{2}\Big)+\frac{vx}{4}+\frac{4vxk+vx}{4}+\frac{vx\sqrt{d}}{4}\\
=uw+ux\Big(\frac{1+\sqrt{d}}
{2}\Big)+\frac{vx}{2}+vxk+\frac{vx\sqrt{d}}{4}.$$ But this is where I'm stuck. Pointer on where to carry on please?","['abstract-algebra', 'algebraic-number-theory', 'algebra-precalculus', 'number-theory', 'ring-theory']"
2616708,"Show that $x_1' = x_2, \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)$ has a bounded solution","Consider the dynamical system :
  $$x_1' = x_2, \space \space x_2' = -2x_1 + x_2(2-5x_1^2 - 3x_2^2)$$
  where $x_1,x_2 \in \mathbb R$. Using the functional $V(x_1,x_2) = \frac{1}{2} x_1^2 + \frac{1}{2} x_2^2$, prove that for any given inital value, the initial value problem has a bounded solution. Attempt : Take the derivative over the solution curves for the given functional, which is : $$\dot{V}(x_1,x_2) = -x_1x_2 + x_2^2(2-5x_1^2-3x_2^2)$$ I will try ""increase"" this expression via inequalities (which is a standard way of handling such problems), so that I can provide a compact form with $V(x_1,x_2)$ in expressions which we know it's positive : $$\dot{V}(x_1,x_2)\leq-x_1x_2 + x_2^2(2-3x_1^2-3x_2^2)\leq\frac{1}{2}x_1^2 + \frac{1}{2}x_2^2 + x_2^2(2-x_1^2 - x_2^2)$$ Now, I have reduced the coefficients inside the parenthesis, since $x_1^2,x_2^2 \geq0$ which means that a smaller coefficient leads to a bigger quality. The inequality derived for the expression at front is a simple application of $(x_1+x_2)^2 \geq 0$. However, I cannot seem how I should continue from now on, since that $x_2^2$ in front of the parenthesis is bugging me and I do not know how to convert it while keeping the inequality in a form that would produce an expression of $V$. Any tips ?","['stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
2616743,"Does $H^2(X_{Zar},\mathcal{O}_X^\times)=0$ for $X$ a regular scheme?","Let $X$ be a regular Noetherian scheme. I read in Milne's book on Etale Cohomology that this implies that $H^2(X_{Zar},\mathcal{O}_X^\times)=0$. Can anyone explain the proof of this fact or give a reference to the proof?","['sheaf-cohomology', 'algebraic-geometry']"
2616744,Special formula for the permanent of the sum of two matrices,"Dear math stack exchange community, I was told that in the paper http://www.tandfonline.com/doi/abs/10.1080/03081088708817770 there was a formula for the permanent of the sum of two matrices $X$ and $Y$ via permanents of certain submatrices of $X$ and $Y$. Unfortunately, I don't have access to this paper (and thus, to this formula) at the moment. I would be very grateful. if somebody could give the formula here.","['matrices', 'abstract-algebra', 'algebra-precalculus', 'permanent', 'linear-algebra']"
2616758,Integral foliation identity,"I've been trying for days to solve the following identity, but feel like I really need a hint on how to start since I'm not yet much into how to work with submanifolds. Let $U\in \mathbb R^n $ and $f: U \to \mathbb R$ continuously differentiable with $df_x \neq 0$ for all $x\in U$ so that, for each $t \in \mathbb R$, $M_t := f^{-1} (\{ t \})$ is a $(n-1)$-dimensional submanifold of $\mathbb R^n$. Then, for every non-negative, $\mathcal B(U)$-measureable map $g: U \to \mathbb R$: $$\int_U g \,d\lambda_n = \int_\mathbb R \int_{M_t} \frac{g(x)}{\lVert \nabla f(x) \rVert} \omega^{M_t}(dx)\,dt,$$
  where $\omega^{M_t}$ is the surface measure on $M_t$ defined by
  $$\omega^{M_t}(A) := \omega^{\phi^*\sigma}(\phi ^{-1} (A)) := \int_{\phi ^{-1}(A) }\sqrt{\det (\phi^*\sigma)_x} \, \lambda_n(dx)$$
  for a parametrization $\phi$ and the standard inner product $\sigma$. My attempt so far: My first idea was to apply the implicit function theorem to find a unique function with $f(x, g(x)) = t$ for each $x$ and some neighbourhood $U_x$ and then apply Fubini to split the left hand side in two integrals. However, I lack technical knowledge to do that. My other attempt was to use a characterization of submanifolds: Fix $x \in M_t$. We can find a neighbourhood $x \in U_x \subseteq U$ and a $C^1$-diffeomorphism $F: U_x \to W_x$, $W_x \subseteq \mathbb R^n$ open neighbourhood of $0$. However, in this case, I don't know of suitable ways to split up the integral. Any help appreciated.","['real-analysis', 'integration', 'lebesgue-integral', 'manifolds', 'differential-geometry']"
2616763,"Convergence radius of power series $\sum_{n=0}^\infty c_nz^n $ where $c_0 = 0$, $c_1 = 1$, $c_n =\frac{c_{n-1} + c_{n-2}}{2}$","In my homework I'm supposed to find the convergence radius of the power set in the title. I'm allowed to assume $C = \lim_{n\to\infty} {c_{n+1}\over c_n}$ exists and we should try to figure out that $C$ has to be.
I found $${c_{n+1}\over {c_n}} = {1\over 2} + {c_{n-1}\over{2c_n}}  $$
so for $n\to\infty$: $$C = {1\over2} + {1\over{2C}}  \Leftrightarrow C=1$$ which doesn't really help me further. Investigating $c_n - c_{n-1}$ didn't help me out either. Any tips on what I might be missing here? I know how to find the radius once I've found the limit of the sequence, but can't seem to find it.","['power-series', 'sequences-and-series', 'convergence-divergence', 'limits']"
2616810,how to prove that the dual of a matroid satisfies the exchange property?,"In the Wikipedia entry for Dual matroid , an early section states An alternative definition of the dual matroid is that its basis sets are the complements of the basis sets of $M$. The basis exchange axiom, used to define matroids from their bases, is self-complementary, so the dual of a matroid is necessarily a matroid. There is also a survey paper by James Oxley, a well-known expert on matroids. In particular, on p. 11 of the paper, Oxley states that the dual of a matroid is itself a matroid, but points us to his book for the proof. I do not have access to the book, and I'm having a hard time seeing why the dual of a matroid satisfies the exchange property in one particular case. Suppose we have a matroid $M=(S, I)$. Suppose we define $I^\prime$ as the set of all $C^\prime \subseteq S$ such that $S-C^\prime$ contains some maximal subset $C \in I$. Now suppose further that we have $A^\prime \in I^\prime$ and $B^\prime \in I^\prime$, with $|A^\prime| < |B^\prime|$, and $S-A^\prime$ contains some maximal subset $A \in I$, and $S-B^\prime$ contains some maximal subset $B \in I$. Now if $B^\prime - A^\prime - A \neq \emptyset$, then it is easy to show that for any $x^\prime \in B^\prime - A^\prime - A$, we have $$
S - (A^\prime \cup \{x^\prime\} ) \supseteq A,
$$ resulting in $A^\prime \cup \{x^\prime\} \in I^\prime$, and hence the exchange property is satisfied. Next, if $B^\prime - A^\prime - A = \emptyset$, and $B\cap H=\emptyset$, where $H \equiv (S - B^\prime) - (S - A^\prime - A)$, then it is easy to show that for any $x^\prime \in B^\prime - A^\prime$, we have $$
S - (A^\prime \cup \{x^\prime\} ) \supseteq B,
$$ and hence the exchange property is again satisfied. I am stumped regarding the remaining case: if $B^\prime - A^\prime - A = \emptyset$, but $B\cap H \neq \emptyset$. In this case, it seems that for any $x^\prime \in B^\prime - A^\prime$, we are certain that $S - (A^\prime \cup \{x^\prime\} )$ does not contain fully either $A$ or $B$. Hence, we must prove that in this situation, there exists some $x^\prime \in B^\prime - A^\prime$ such that $S - (A^\prime \cup \{x^\prime\} )$ contains some other maximal subset $C \in I$. I have no clue how to do this.","['combinatorics', 'matroids', 'duality-theorems']"
2616814,infimum implies existence of a sequence,"I am following 'Introductory Functional Analysis' by Kreyszig. Theorem 3.3-1 says that Let $X$ be an inner product space and $M\neq\phi$ a convex subset which is complete (in the metric induced by inner-product). Then for every given $x\in X$ there exists a unique $y\in M$ such that 
$$\delta=\inf_{y'\in M}||x-y'||=||x-y||.$$ In the proof of this theorem, he has written that By the definition of infimum there is a sequence $(y_n)$ in $M$ such that
$\delta_n$ approaches $\delta$ where $\delta_n=||x-y_n||.$ I don't understand this statement because definition of infimum does not say anything about the existence of a sequence. For example, I can have a set {1} which has infimum equal to 1, but there is no sequence which converges to 1. So what does he mean in this proof? Am I missing something here?","['functional-analysis', 'real-analysis', 'supremum-and-infimum', 'proof-explanation']"
2616826,Homomorphic images of $p$-adic integers,"Let $J_p$ be the additive group of $p$-adic integers for a fixed prime $p$.
I would like to know if the structure of its homomorphic images is known. In particular, if $J_p/N$ is a homomorphic image with finite exponent, is $J_p/N$ also finite?","['abstract-algebra', 'group-theory', 'p-adic-number-theory']"
2616827,Eigenvalues of certain Hankel matrix,"For each $n\in\mathbb N$, let $A_n$ be the $n\times n$ matrix with entries
$$
A_n(k,j)=\begin{cases} 1,&\ k+j=n+1\\ 
1,&\ k+j=n+2,\\
0,&\ \text{ otherwise}
\end{cases}
$$
So, for instance, 
$$
A_4=\begin{bmatrix} 0&0&0&1\\ 0&0&1&1\\ 0&1&1&0\\ 1&1&0&0\end{bmatrix}.
$$
I'm looking for a proof of the following: If $n=2m$, then $A_n$ has $m$ positive and $m$ negative eigenvalues. if $n=2m+1$, then $A_n$ has $m+1$ positive and $m$ negative eigenvalues. Finding the eigenvalues explicitly is definitely not an option, judging by the formulas already in the case $n=3$. If $p_n$ denotes the characteristic polynomial of $A_n$, then we have the recursion
$$
p_{n+1}(t)=-t\,p_n(t)-p_{n-1}(t),
$$
but I don't know if one can obtain meaningful information from this.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'operator-theory']"
2616844,Realize the locus of homogeneous polynomials of degree $d$ as a projective variety.,"I am looking for clarification for the following problem. The field $k$ is algebraically closed and has characteristic not equal to $2.$ : Show that the locus of all homogeneous polynomials of degree $d$ in $n+1$ variables over $k$ is a projective variety. It seems to me that instead of considering the set $S$ of all homogeneous polynomials of degree $d$ in $n+1$ variables, we should consider $S/\sim$ where $f \sim g \in S$ if and only if there exists some nonzero scalar $a \in k$ so that $af = g.$ Here is what I have tried that has led me to this conclusion: Proof. Let $L$ consist of all homogeneous polynomials of degree $d$ in $n+1$ variables over $k$ as well as the zero polynomial. Then $L$ is an $\binom{n+d}{d}$ dimensional vector space. There is a vector space isomorphism $L \to k^{\binom{n+d}{d}}$ given by mapping a polynomial $f$ to the $\binom{n+d}{d}$-tuple consisting of the coefficients of $f$. This isomorphism identifies one dimensional linear subspaces of $L$ with one dimensional linear subspaces of $k^{\binom{n+d}{d}}.$ That is, $S/\sim \cong \mathbb{P}^{\binom{n+d}{d}-1}$ as topological spaces. Have I made an error in my reasoning, or am I right that the projective variety is $S/\sim$ instead of $S?$","['algebraic-geometry', 'projective-geometry', 'projective-space', 'commutative-algebra', 'linear-algebra']"
2616865,Can I extract common factor from a column in matrix?,Can I do the following for matrices: $$ A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} == 2^{1/3}\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = 2^{1/3}B \quad ??$$ I know I can do the following for matrices: $$ If \quad A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad then \quad 2A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$ and following for determinants: $$ |A| = \begin{vmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} == 2\begin{vmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} = 2|B| \quad \checkmark\checkmark$$,"['matrices', 'determinant']"
2616878,Question on proof of Erdos and Szekeres,"So my textbook has a specific proof of the theorem by Erdos and Szekeres We suppose that there is no increasing subsequence of length $n + 1$ and show that there must be a decreasing subsequence of length $n + 1$.
  For each $k = 1,2, ... ,n^2 + 1$ let $m_k$ be the length of the longest increasing subsequence that begins with $a_k$. Suppose $m_k\leq n$ for each $k = 1,2, ... ,n^2 + 1$, so that there is no increasing subsequence of length $n + 1$. Since $m_k \geq 1$ for each $k = 1,2, ... ,n^2 + 1$, the numbers $m_1, m_2, ... , m_{n^2+1}$ are $n^2 + 1$ integers each between 1 and $n$. By the strong form of the pigeonhole principle, $n + 1$ of the numbers $m_l, m_2, . .. , m_{n^2+1}$ are equal. Let $m_{k_{1}} = m_{k_{2}} = ... = m_{k_{n+1}}$, where $1\leq k_1<k_2<...<k_{n+1}\leq n^2+1$ My question is how can the strong pigeonhole principle be used to say that $n+1$ increasing subsequences are equal.  According to the pigeonhole principle if we have positive integers $q_1, q_2, ..., q_p$ and we distribute $q_1+q_2+...+q_p-p+1$ objects then either the 1st box has at least $q_1$ objects, or 2nd box has has at least $q_2$, etc.  So do the lengths of the sequences $m_1, m_2,..., m_{n+1}$ match up to the $q_1, q_2, ..., q_p$ integers. I have trouble seeing what the objects are and what the boxes are.  I thought initially that the objects would be the sequences, but I have trouble figuring out what the boxes are in that scenario.","['combinatorics', 'pigeonhole-principle', 'proof-explanation']"
2616881,"Study of a "" flow ""","First, sorry if my english does not correspond to the real/formal worlds that I should use for this domain in which i'm new. (logarithm in complex analysis and potential/flow) I know I can only ask for one question but here it is a part of a whole problem and I also want to know if my reasoning is good so ... I'm asked a small problem to study a potential defined by a function (I dont know what is $a$ ... )
$$
\Phi\left(z\right)=\ln\left(\frac{z-a}{z+a}\right)
$$
in which I guess ln is defined on $\mathbb{C} \setminus \mathbb{R}^{-}$ by
$$
\ln\left(z\right)=\ln\left(\left|z\right|\right)+i\text{arg}\left(z\right)=\ln\left(\rho\right)+i\theta
$$ 1] Find the domain $D$ where $\Psi$ is holomorphic and explicit $\Phi'$. 2] Write the velocity field (  which is $\mathscr{C}^{\infty}$ ) of this flow. Is it possible to define it on an open set which contains strictly $D$ ? 3] Show that the flows lines are given by circular wedges which extremities are $a$ and $-a$. 4] With $\gamma^{+}$ a Jordan contour, discuss of the value of the integral
$$
\int_{\gamma^{+}}\overline{V}\left(z\right) \text{d}z
$$
where $\overline{V}$ is the conjugate of $V$. My attempt is to find where $\displaystyle z \mapsto \frac{z-a}{z+a}$ is a negative real. With $z=x+iy$ i've found that $x$ and $y$ verify
$$
\frac{z-a}{z+a}=\frac{x^2+y^2-a^2+iya}{x^2+2ax+a^2y^2}
$$
Hence $y$ needs to be $0$ in order to be a real ( or $a=i \alpha$ )
Hence it is a negative real iif $x^2+y^2 \leq a^2$ then it would be defined on $\mathbb{C}$ minus a circle centered in $(0,0)$ of radius $a$ and  for $z \in D$ $$
\Phi'\left(z\right)=\frac{z+a-z+a}{\left(z+a\right)^2}\frac{z+a}{z-a}=\frac{2z}{\left(z+a\right)\left(z-a\right)}
$$ Should I calculate 
$$v_{x}=\frac{\partial \Psi}{\partial y}\left(x,y\right) \text{ and }v_{y}=-\frac{\partial \Psi}{\partial x}\left(x,y\right)
$$ $$
\frac{z-a}{z+a}=K \Leftrightarrow z=a\frac{1+K}{1-K}
$$
What can I conclude ? $z=cst \ \Rightarrow$ $x$ and $y \ \Rightarrow $ $\left|z\right|$ is constant ? No idea of how to proceed. Any ideas ? Thanks !","['complex-analysis', 'vector-fields']"
2616891,Combining strictly positive numbers to complex powers?,"Given complex numbers $z_1,z_2\in\mathbb{C}$ and strictly positive numbers $a,b>0$, are the following statements true in general? $$a^{z_1}b^{z_1}=(ab)^{z_1}~~~,~~~a^{z_1}a^{z_2}=a^{z_1+z_2}$$ If so, how to prove it? If not, which restriction would have to be applied to make it true? Thanks for any suggestion!","['complex-analysis', 'complex-numbers']"
2616895,Proving in different ways that $n^{n-1}-1$ is divisible by $(n-1)^2$.,"I have this amazing exercise which explicitly says prove in at least six different way that 
$n^{n-1}-1$ is divisible by $(n-1)^2$ where $n$ is an integer. So far I have only prove it as follows: By geometric sum we have $$\sum_{k=0}^{n-2}n^k=\frac{n^{n-1}-1}{n-1}$$
However, since $n\equiv 1\mod (n-1)$ we have 
$$\frac{n^{n-1}-1}{n-1}=\sum_{k=0}^{n-2}n^k\equiv \sum_{k=0}^{n-2}1^k\mod(n-1)$$
that is $$\frac{n^{n-1}-1}{n-1}=\sum_{k=0}^{n-2}n^k\equiv n-1\mod(n-1)\Longleftrightarrow  \frac{n^{n-1}-1}{n-1} \equiv 0\mod(n-1)$$ and this prove that $(n-1)^2$ divides $n^{n-1}-1$. Does anyone have another approach different from mine.?
  Note that I do necessary need one to give all six approaches as was asked in the exercise one or two is plainly enough for me.","['algebra-precalculus', 'arithmetic', 'geometric-series', 'elementary-number-theory']"
2616903,Area between two overlapping triangles,"The shaded part for 1 single triangle is $4/9$ths of the total area of the triangle. If this was considered to be 4 units, then the unshaded $5/9$ths would be 5 units. Thus the total area of the whole figure is 14 units and so $4/14$ths or $2/7$ths are shaded. I believe that's the right answer, but can someone tell me is there a more efficient or a method that utilises geometry? Thanks",['algebra-precalculus']
2616915,Compactness in $l^\infty$,"Let $A$ be a subset of $\ell^\infty(\mathbb R)$ defined by $$
A=\{(a_n)_{n\in\mathbb N}:|a_n|\leq 2^{-n}\},
$$ that is, each coordinate $a_n$ is bounded by $2^{-n}$ . Is the set $A$ compact in $l^\infty?$ My attempt: Let $a^{(m)}$ be a sequence in $A$ . Consider the set of all first coordinates $(a_1^{(m)})$ , which has a subsequence converging to $c_1$ . Next, consider the second coordinate, hence there is a further subsequence $a_2^{(m_k)}$ converging to $c_2$ . Continuing in this process, and then use the diagonal argument to pick a convergence subsequence. But I am not sure which subsequence I can pick.","['functional-analysis', 'real-analysis', 'compactness', 'sequences-and-series']"
2616926,Is this differential equation solvable when I cannot separate the variables?,"Suppose $(x+y+1)^2 \frac{dy}{dx}+(x+y+1)^2+x^3=0$. How do I express x in terms of y? My thoughts: I don't think there is a clean way of separating the variables, especially once I expand the terms. So I try to work through without expanding. I can try dividing the entire equation by $(x+y+1)^2$ and this might give me a cleaner expression, but I still cannot separate the x from the y. It doesn't seem there is a way to separate the x from the y, so is this still solvable?",['ordinary-differential-equations']
2616948,Powers of two in partial sums of binomial coefficients,"Let $f(m)$ be the sum of the first $m$ binomial coefficients, aka $f(m)$ produces a function $f_m$ where $$f_m(x) = \sum_{i=0}^{m} {\binom{x}{i}}$$ Another way to look at is is that $f(m)(x)$ is the sum of the first $m$ elements of the $x$ th row of pascal's triangle. I am interested in the set produced when a $f_m$ is mapped onto the natural numbers. $$S_m = \{f_m(a) \mid a \in \mathbb{N_0}\}$$ For example the first few lists are: $S_0 = [1, 1, 1, 1, 1, 1, 1, 1, ...]$ $S_1 = [1, 2, 3, 4, 5, 6, 7, 8, ...]$ $S_2$ $ = [1, 2, 4, 7, 11, 16, 22, 29, ...]$ $S_3$ $ = [1, 2, 4, 8, 15, 26, 42, 64, ...]$ $S_4$ $ = [1, 2, 4, 8, 16, 31, 57, 99, ...]$ In particular I am looking at when powers of two appear in a sequence. Using the power of brute force, I can see that for any sequence from 4 onwards, there is a simple pattern. $S_2$ produces $5$ powers of two, which are $[1, 2, 4, 16, 4096]$ $S_3$ produces $6$ powers of two, which are $[1, 2, 4, 8, 64, 2048]$ $S_4$ produces $6$ powers of two, which are $[1, 2, 4, 8, 16, 256]$ $S_5$ produces $7$ powers of two, which are $[1, 2, 4, 8, 16, 32, 1024]$ $S_6$ produces $8$ powers of two, which are $[1, 2, 4, 8, 16, 32, 64, 4096]$ $S_7$ produces $9$ powers of two, which are $[1, 2, 4, 8, 16, 32, 64, 128, 16384]$ $S_8$ produces $10$ powers of two, which are $[1, 2, 4, 8, 16, 32, 64, 128, 256, 65536]$ $S_9$ produces $11$ powers of two, which are $[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 262144]$ $S_{10}$ produces $12$ powers of two, which are $[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 1048576]$ From this data I conjecture that the sequence $S_m$ contains at least $m+2$ powers of two. I can show this lower bound using the fact that the sum of the elements in any row of pascal's triangle is a power of two. For each element of the sequence $S_m$ given by $f_m(a)$ where $0 \le a \le m$ , $f_m(a)$ is the sum of a full row of pascal's triangle (plus a couple of zeros), and thus is a power of two. This gives a lower bound of at least $m+1$ powers of two in $S_m$ . Furthermore $f_m(2m + 1)$ must be a power of two, as it is the sum of exactly half of a row of pascal's triangle. As the row is symmetrical, the sum of both halves are the same. For a hand-wavey proof, let $x$ be the sum of half the row. The sum of the full row is then $2x$ . Since the sum of the full row is also a power of two, $2x = 2^n$ for some $n$ . Therefore $x = 2^{n-1}$ , a power of two. This gives us one more guarenteed power of two in the sequence, bringing the lower bound of the number of powers of two in $S_m$ to be $m+2$ . Some questions Is there a proof that (for $m \ge 4$ ) the number of powers of two in $S_m$ is exactly $m+2$ ? For $m \le 2$ there are (countably) infinite powers of two in $S_m$ . What happened for $S_3$ and $S_4$ that it doesn't follow the pattern of infinite or $m+2$ ? (Sort of follow up to the last question) For $S_3$ the powers of two are well studied . Is there a well known solution for the powers of two in $S_4$ ?","['binomial-coefficients', 'sequences-and-series']"
2616955,Bounding tail conditional expectation of a random variable given variance,"Given a random variable $X$ with CDF $F(X)$, mean $E(X)=0$, and variance $Var(X) =\sigma^2$, I would like to bound the tail conditional expectation where $X$ is in the tail with probability $1-p$: $E(X|X\geq F^{-1}(p)) = \frac{1}{1-p}\int_{F^{-1}(p)}^\infty X dF(X)$ A nice way to get an upper bound is via a Chebyshev-type inequality: https://projecteuclid.org/download/pdf_1/euclid.aoms/1177697276 , which suggests: $E(X|X\geq F^{-1}(p)) \leq \sigma \sqrt{\frac{p}{1-p}}$ But how can we get a lower bound? Intuitively, since $E(X) = 0$, we must have: $E(X|X\geq F^{-1}(p)) \geq 0$ But that doesn't use the dispersion of the RV at all. Intuitively, if $\sigma$ is large and $E(X)=0$, the lower bound should be large, so the lower bound should grow with $\sigma$. But I have no idea how to formalize it. Any ideas would be greatly appreciated.","['probability-theory', 'probability', 'conditional-expectation', 'upper-lower-bounds', 'random-variables']"
2616988,Solving $ y'' - y' = -3$ via undetermined coefficients.,"I stumbled on a problem when solving this ODE. The method of undetermined coefficients say that I can suppose the solution in the form $y_p = k$.
Then:
$(k)''-(k)'=-3 \Rightarrow 0=3$, which is absurd.
When I suppose the solution in the form $kx$ it works, and I find the particular solution $3x$, but why I need to suppose a particular solution in a degree 1 if the RHS of the ODE is of degree 0?",['ordinary-differential-equations']
2616999,How does Prof D. J. H. Garling generalize recursion theorem?,"From textbook A Course in Mathematical Analysis by Prof D. J. H. Garling, I'm not able to understand how he generalizes recursion theorem. In the first step, he states the theorem: Here, mapping $s:P→P$ satisfies Peano’s axioms . In the second step, he reasons: Finally, he gets the theorem in a more general form: My confusion is that: In the original theorem, for each $\bar{a}$, there exists a unique mapping $a:P → A$ such that $a_{0}=\bar{a}$ and $a_{s(n)}=f(a_{n})$ $\forall n\in P$. We substitute $P$ by $Z^{+}$. As a result, for each $\bar{a}$, there exists a unique mapping $a:Z^{+} → A$ such that $a_{0}=\bar{a}$ and $a_{s(n)}=f(a_{n})$ $\forall n\in Z^{+}$. However, I don't understand how the author derives the mapping $f^{n}:A → A$ from the mapping $a:Z^{+} → A$. In the second step, the author lets $f^{n}(\bar{a})=a_{n}$. This means $f^{0}(\bar{a})=a_{0}$, $f^{1}(\bar{a})=a_{1}$, $f^{2}(\bar{a})=a_{2}$, ..., $f^{n}(\bar{a})=a_{n}$. From my understanding, he only defines $f^{0}, f^{1}, f^{2}, f^{3},..., f^{n}$ for only $\bar{a}$. I don't understand how the author defines $f^{0}, f^{1}, f^{2}, f^{3},..., f^{n}$ for the other elements in A. $(f^{n}(\bar{a})=a_{n}$ and it is possible that $ran(a)\subsetneq A) → ran(f^{n})\subsetneq A$. How can $f^{n}$ be defined for all $a\in A$? For the sake of clarity, there is a screenshot of full proof of recursion theorem by the author at Possible typos in the proof of Recursion Theorem . Many thanks for clarifying my doubt!",['elementary-set-theory']
2617015,What is Euler doing?,"In Euler's paper ""De Fractionibus Continuis Dissertatio"" ( English Translation ) he proves that the constant $e\approx 2.71828$ is irrational. 1 One step in the proof threw me for a loop, though. In the middle of paragraph 29, Euler solves the ODE $$
a\,\mathrm{d}q+q^2\,\mathrm{d}p=\mathrm{d}p
$$ by rearranging and integrating to get $$\begin{align}
\frac{a}{1-q^2}\mathrm{d}q&=\mathrm{d}p\\
\frac{a}{2} \log\left(\frac{1+q}{1-q}\right)&=p+C.
\end{align}$$ So far, so good. But now Euler says (quoting from Wyman's translation) ""the constant ought to be determined from this equation by setting $q=\infty$ when $p=0$. Wherefore there follows
\begin{equation}
\frac a2\log\left(\frac{q+1}{q-1}\right)=p.""
\end{equation} This is the part I feel icky about. My current interpretation of this is that when we choose the initial condition $q=\infty$ and $p=0$, we get $C=\frac a2\log(-1)$, so that $$\begin{align}
\frac{a}{2} \log\left(\frac{1+q}{1-q}\right)&=p+\frac a2\log(-1)\\
\frac{a}{2} \left(\log\left(\frac{1+q}{1-q}\right)+\log(-1)\right)&=p\\
\frac{a}{2} \log\left(\frac{1+q}{1-q}(-1)\right)&=p\\
\frac{a}{2} \log\left(\frac{q+1}{q-1}\right)&=p.
\end{align}$$
This interpretation leaves more than a little to be desired. For one thing, $\log(-1)$ doesn't even make sense in a purely real context, but if we decide to work in $\mathbb{C}$ then we can't just freely use the sum-to-product rule
\begin{equation}
\log(zw)=\log(z)+\log(w).
\end{equation} How should I understand this step in Euler's proof? 1) Ed Sandifer claims that this is the first rigorous proof that $e$ is irrational ( Link . Note that the linked PDF didn't display correctly in the two browsers I tried, but was fine when I downloaded it.)","['rationality-testing', 'math-history', 'ordinary-differential-equations', 'constants']"
2617025,$f$ is open iff $f(Int(A)) \subset Int(f(A))$,"Prove that $f:X \rightarrow Y$ is open iff $f(Int(A)) \subset Int(f(A)), \ A \subset X.$ Right direction: If $f$ is open then it transforms open sets into open sets. Let $A \subset X$. $Int(A)$ is open in $X$, so $f(Int(A))$ is open in $Y$, which means  $f(Int(A))=Int(f(Int(A))) \subset Int(f(A))$. I can't get to prove the other direction.",['general-topology']
2617077,Connection on a vector bundle in terms of sections,"Reference for this question is the book Geometry of Differential forms by Morita, page no 191. In last but one paragraph it says the following, Suppose we are given an arbitrary bundle $\pi:E\rightarrow M$  with a  connection $\nabla$. We think of $\nabla s$ as  a $1$ form on $M$ whose value on vector field $X$ on $M$  is the section $\nabla_Xs$. Connection on a vector bundle $\pi:E\rightarrow M$ is a bilinear map $\nabla:X(M)\times \Gamma(E)\rightarrow\Gamma(E)$ satisfying certain conditions where $X(M)$ is the collection of vector fields on $M$ and $\Gamma(E)$ is collection of  sections of vector bundle. I do not really understand the one that I have quoted above. $1$ form on $M$  is a $\omega:M\rightarrow \Lambda(TM)$. They are saying $\nabla_s$ is a $1$ form on $M$. We need to say what is $\nabla_s:M\rightarrow \Lambda(TM)$. Let $m\in M$ then, $\nabla_s(m):T_mM\rightarrow \mathbb{R}$. Let us fix some vector field $X$ on $M$. We need to define $\nabla_s(m)(X(m))$. Suppose we are in trivial vector bundle, where sections are just smooth maps on $M$ we can define $\nabla_s(m)(X(m))$ as $\nabla_X(s)(m)\in \mathbb{R}$. I do not see how we can think of $\nabla_s$ as a $1$ form  for a bundle which is not trivial. I see that they are trying to relate connection on a vector bundle to be something that takes a section on $E$ and gives a section on some bundle, definitely not a section on $M$ as they have written. 
I fail to understand this. A connection on a vector bundle is a map that takes a section on $M$ and gives a section on ..????","['connections', 'vector-bundles', 'differential-geometry']"
2617081,Why do we need the axiom of choice? [duplicate],"This question already has answers here : Axiom of Choice: What exactly is a choice, and when and why is it needed? (3 answers) Closed 6 years ago . I don't mean why is it important. I mean why can't we just define the ""selector function"" like $S\colon \mathbb{F} \to A, $ such that $S(X) = x \in X$, without an axiom? Why can't we do that but we can, for example, take some set that satisfies a condition from an uncountable family?","['axioms', 'calculus', 'analysis']"
2617121,Passage in the proof of Chern-Weil method in John Roe's Elliptic operators book,"I've problem understanding some passages in the proof of the Chern-Weil homomorphism: After defining what is an invariant polynomial $P$ on $\mathfrak{gl}_n(\Bbb C)$ we want to prove that given $K$ the curvature of a complex vector bundle $V$ over $M$ then $P(K)$ is a closed form, whose cohomology class is independent from the choice of the connection $\nabla$ on $V$. The author says that it's equivalent to prove closeness and well-definedness for the form $$\log \det(1+q\Omega)$$ where $q$ is just a parameter and $\Omega$ is the curvature $2$-form associated to the frame bundle associated to $V$. ($\Omega=d\omega+\omega^2$, where $\omega$ is the connection $1$-form). He then assume that $\omega$ depends on another parameter $t$ (I guess in order to prove well-definedness since the space of connections is affine) and after some algebra he gets $$(*) \ \ \ \ \ \ \ \ \frac{d}{dt}\log \det(1+q\Omega)= d\sum_{l=0}^{\infty}(-1)^lq^{l+1}\text{tr}\{\Omega^l \omega'\}$$ where $\omega'$ is the derivative w.r.t. $t$ of $\omega$. He then push everything to $M$ (we were working on $E$ now) and it concludes by saying: Now the result follows; for since any connection can be deformed locally to flatness, we see that $\log \det(1+q\Omega)$ is locally exact; that is closed; and since any two connections can be linked by a differentiable path, the cohomology class of $\log \det(1+q\Omega)$ is independent of the choice of connection The independence I think is obtained by integrating w.r.t. the parameter $t$ equation (*) and then switch the integral sign with the exterior derivative on RHS (can we do it?) Why do we have that the form is closed? where does flatness come into play?","['characteristic-classes', 'connections', 'differential-geometry']"
2617134,how to prove $\mathbb Z_n$ has divisors of zero if and only if $n$ is not prime,"A non-zero number $a \in \mathbb Z_n$ is called a divisor of zero if there is a non-zero number $b \in \mathbb Z_n$ such that $ab\equiv 0\pmod n.$ How can I prove $\mathbb Z_n$ has divisors of zero if and only if $n$ is not prime. I filled out the addition and multiplication tables for modulo 6 and 7 and tried to find out the relation, and it's definitely true. I know I need to prove it in both directions since it is an 'iff' question. But I still don't get it.","['abstract-algebra', 'group-theory']"
2617141,Why is this equation of what appears to be a circle not a function? How do show this algebraically.,"I know this is a circle (right?) and so for every x, there are 2 values of y (so y is not a function of x ), but how do I algebraically show this. This is the question: If I square root both sides, I get: $$y = \sqrt{4 - x^2} $$ But that seems to get rid of a y answer. If I reduce it, I get: $$y^2 = (2+x)(2-x)$$ But I still feel like I haven't algebraically shown this NOT to be a function.",['calculus']
2617173,"Existence of ""smallest"" non-empty measurable set","I am wondering if, for any $\sigma$-algebra $F$, there always exists some set $s\in F$ that satisfies the following properties, $s\ne \emptyset$. Except for $\emptyset$ and $s$ itself, there does not exist another set $t\in F$ that $t\subset s$. It is easy to see that this is true if the $\sigma$-algebra is induced by the topology induced by some metric because a set consisting of a single point satisfies this. But does it hold in general?",['measure-theory']
2617183,Whats the relationship between a presheaf and its sheafification?,"Given a presheaf $F'$ of rings, say, we have a morphism $F' \rightarrow F$ where $F$ is the sheafification of $F'$. I see no reason this morphism should be injective or surjective. Is that right?","['sheaf-theory', 'algebraic-geometry']"
2617207,How can I calculate the remainder of $3^{2012}$ modulo 17?,So far this is what I can do: Using Fermat's Little Theorem I know that $3^{16}\equiv 1 \pmod {17} $ Also: $3^{2012} = (3^{16})^{125}*3^{12} \pmod{17}$ So I am left with $3^{12}\pmod{17}$. Again I'm going to use fermat's theorem so:  $ 3^{12} = \frac{3^{16}}{3^{4}} \pmod{17}$ Here I am stuck because I get $3^{-4} \pmod{17}$ and I don't know how to calculate this because I don't know what $\frac{1}{81} \pmod{17}$ is. I know $81 = 13 \pmod{17}$ But I know the answer is 4. What did I do wrong?,"['number-theory', 'exponential-function', 'modular-arithmetic', 'factoring']"
2617325,What is the cardinality of the set of mathematical structures?,"In mathematical logic, there is a set $X$ that contains all mathematical structures given a certain symbol set $S$. E.g. lets say $S=(•,R)$ (a binary operator and a binary relation)
Let an $S$ structure $\mathfrak A$ be a tuple $(A, •,R)$ such that the two symbols have a corresponding relation that is defkned on $A$. What is the cardinality of $\{\mathfrak A|\mathfrak A \text{ is an $S$ structure} \}$? My suspicion is that it is huge. Perhaps it doesn't have a cardinality? My suspicion is: for any cardinality, we can construct another set of S structures whose domains have that cardinality, so that the set of such structures must have a cardinality greater than it. Hence the set of all such structures has an ""unbounded cardinality""? (???!)","['cardinals', 'logic', 'elementary-set-theory']"
2617340,"Let function $g:A\to A$, $\:\:\:g^{2016}=g\circ g\circ g...\circ g=id_A$ , Prove that $g$ is Inverse function","Let function $g:A\to A$,  $\:\:\:g^{2016}=g\circ g\circ g...\circ g=id_A$ , Prove that $g$ is Inverse function My attempted : $g$ is Inverse function if it's Bijection to prove $g$ is one-to-one function we need to show that $g$ is Left inverses : we get that :$$g^{2015}(g(a))=id_A$$ so $g$ is one-to-one function and to prove $g$ is Surjective  function we need to show that $g$ is right inverses : we get that :$$g(g^{2015}(a))=id_A$$ so $g$ is Surjective function $\implies$ $g$ is Bijection so it's Inverse function. Is this answer is correct and enough to prove it , if not how to prove it the right way ? thanks a lot.","['elementary-set-theory', 'functions']"
2617346,How can I calculate $\lim\limits_{x\rightarrow 0} \frac{(1+x)^{\frac{1}{x}}-e}{x}$? [duplicate],"This question already has answers here : Limit of $x\left(\left(1 + \frac{1}{x}\right)^x - e\right)$ when $x\to\infty$ (4 answers) Closed 4 years ago . How can I calculate this limit? $$\lim_{x\rightarrow 0} \frac{(1+x)^{\frac{1}{x}}-e}{x}$$ I thought about L'Hospital because case of $\frac{0}{0}$, but I don't know how to contiune from this point..","['real-analysis', 'taylor-expansion', 'limits', 'exponential-function', 'calculus']"
2617377,"Geometric inequality : In a equilateral , $ 4(PD^2 +PE^2 + PF^2) \ge PA^2 + PB^2 + PC^2 $","In a equilateral $ABC$ , 
Prove
$$ 4(PD^2 + PE^2 + PF^2 ) \ge PA^2 + PB^2 + PC^2 $$ $(PD, PE, PF   $ are distance to sides, and P is located same plane with $ABC$) I know that; $$PA + PB \ge PC , $$
$$ PB + PC  \ge  PA , $$
$$ PC + PA  \ge PB $$ and When P is located on the circumcircle , $'D, E, F$  is colinear.'  But these are not useful to solve the problem. I want some hints. Thank you.","['inequality', 'geometric-inequalities', 'euclidean-geometry', 'triangles', 'geometry']"
2617388,Probability of success and failure,"I've an interesting question on my hand which I've approach several others but all of them gives me different insights to this probability question. Here it is, The incidence of a suspicious transaction in a bank is 1 in 149 . They are able to correctly identify a legitimate transaction 92% of the time. However, this bank is also able to correctly pinpoint a suspicious transaction 92% of the time. One day, the bank identify a transaction as suspicious. What is the exact probability of the transaction actually being legitimate? From my personal point of view, if the question ask for the probability of the transaction actually being legitimate, states that the rate is  148⁄ 149 . The bank is able to correctly identify (which they fail to) a legitimate and suspicious transaction. Therefore, the failure % should be (8% * 8%) which is 0.08 * 0.08 = 0.0064. Hence, the probability of it actually being legitimate is 148⁄ 149 * 0.0064 = 0.00636 . However, i asked various people of their opinion and some states that the probability should be just 148⁄ 149 * 0.08 . Therefore, what should be the most probable answer to problems like this.",['probability']
2617391,"Trouble Understanding the Proof of the limit of Thomae's Function in $(0,1)$ is $0$","I am going through Spivak's Calculus. Thomae's Function is defined (though not named) in the book as: $$
f(x) = \begin{cases} 0,& x \text { irrational}, \text{ }0<x<1, \\ 1/q , & x = p/q \text{ in lowest terms}, \text{ }0<x<1.\end{cases}
$$ For any number $a$ so that $0<a<1$ $f$ approaches $0$ near $a$. The proof starts with letting $n$ be a natural number so large that $1/n <= \epsilon$. Now the only numbers x for which $|f(x) - 0| < \epsilon$ is false $x$ are: $$
\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{3}{4},\cdots,\frac{1}{n},\cdots,\frac{n-1}{n}.
$$ However, many of these numbers may be, there are only finitely many. Therefore of all these numbers, one is closest to $a$ meaning $|p/q - a|$ is smallest for one $p/q$.(If a is rational, then it might be one of these numbers, in that cases values $p/q$ such that $p/q \ne a$ should be considered.) This closes distance may be chosen as the $\delta$. For if $0<|x-a|<\delta$, then $x$ is not one of: $$
\frac{1}{2},\cdots,\frac{n-1}{n}.
$$ and therefore $|f(x) - 0| < \epsilon$ is true. Now there are few things I don't seem to understand. I can see why picking an $n$ such that $1/n <= \epsilon$ works but what I don't get is why $1/n < \epsilon$ or $1/n = \epsilon$ wouldn't work instead of $n$ such that $1/n <= \epsilon$. Also what is the reasoning of picking an $n$ such that $1/n <= \epsilon$? Is it because the function returns $\frac{1}{q}$ provided that $x$ is in the form $\frac{p}{q}$? And lastly could this also be considered a proof that shows that there is always an irrational between two reals?","['real-analysis', 'limits', 'functions', 'epsilon-delta', 'proof-explanation']"
2617427,Show that the posterior probability of $H_0$ equals $\Phi\left(\sqrt{n}\frac{\theta_o -\bar{X}_n}{\sigma}\right)$,"Exercise: Assume a Bayesian setup in which $X_1,\ldots,X_n\mid\Theta\stackrel{iid}{\sim}N(\theta,\sigma^2)$ and $\Theta$ gets assigned a prior distribution. Take a flat prior on $\Theta: f_\Theta(\theta)\propto 1$. We consider testing $H_0:\theta\leq \theta_0$ versus $H_1:\theta > \theta_0$. Show that the posterior probability of $H_0$ equals $$\Phi\bigg(\sqrt{n}\dfrac{\theta_o -\bar{X}_n}{\sigma}\bigg)$$ My approach: I need to show that $\mathbb{P}(\theta\leq \theta_0\mid X) = \Phi\bigg(\sqrt{n}\dfrac{\theta_o -\bar{X}_n}{\sigma}\bigg)$, and I know that $f_{\Theta\mid X}(\theta\mid x)\propto f_\Theta(\theta)\,f_{X\mid\Theta}(x\mid\theta) \propto\prod\limits_{i = 1}^n\exp\bigg(\dfrac{(x_i - \theta)^2}{\sigma^2}\bigg) = \exp\bigg(-\dfrac{\sum_{i = 1}^nx_i^2}{n\sigma^2} + \dfrac{2\theta\frac{1}{n}\sum_{i = 1}^n x_i}{2\sigma^2} - \dfrac{\theta^2}{\sigma^2}\bigg)$. If I'm not mistaken, I need to find $a$ and $b$ in $\Theta|X\sim N(a,b^2)$ so that I can normalise $\Theta$. Unfortunately I can't find a way to write $\exp\bigg(-\dfrac{\sum_{i = 1}^nx_i^2}{n\sigma^2} + \dfrac{2\theta\frac{1}{n}\sum_{i = 1}^n x_i}{2\sigma^2} - \dfrac{\theta^2}{\sigma^2}\bigg)$ as $\exp\bigg(-\dfrac{(\theta - a)^2}{b^2}\bigg)$. Question: How do I solve this exercise/what am I missing? Thanks in advance!","['statistical-inference', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2617429,"Existence and uniqueness of solution for the IVP : $y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0$","Exams Problem : Consider the IVP :
  $$y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0$$
  Examine if there exists a solution for the given IVP in an area of $x=0$. Furthermore, examine if conclusions can be made about the uniqueness of the solution. Attempt : As I've went over similar posts, you need to always determine how you define the powers/roots in this case, since it's a detail fact that plays a big role. Let's assume that we consider the real-valued $5th$ root in this particular case. Then, let's consider the function : $$f(x,y) = [x-\cos(y)]^{2/5}=\sqrt[5]{(x-\cos(y))^{2}}$$ Then, the function $f(x,y)$ will be continuous $\forall (x,y) \in \mathbb R^2$ since it is $(x-\cos(y))^2 \geq 0$. This means that the function $f(x,y)$ will also be continuous in a domain, such as : $$D=\{(x,y) \in \mathbb R^2 :|x|\leq a, |y| \leq b\} \space \text{where} \space a,b > 0$$ Obviously, the Picard/Peano theorems can be applied and then truly, there exists a solution for the IVP around $x=0$ with $y(0)=0$. My question is regarding the uniqueness though : One way to determine the uniqueness, is showing that the derivative of $f(x,y)$ with respect to $y$ is bounded or in other words (and approach), Lipschitz Continuous. Simply then (still considering the real valued root branch case) : $$\frac{\partial}{\partial y}f(x,y)= \frac{2\sin(y)}{5\sqrt[5]{(x-\cos(y))^3}}$$ But in such case, we see that $f_y(x,y)$ is not continuous for every value, since it's not always $(x-\cos(y))^3 \geq 0$. This means that the derivative $f_y(x,y)$ won't be continuous in the domain $D$ (which is a standard step for the existence) and thus no conclusions can be made about the uniqueness. In another way, the derivative $f_y(x,y)$ is not bounded (as I think), so still no conclusions can be made. Question : Is the above explanation and reasoning correct though ? Is there a more formal way of showing that $f_y(x,y)$ is not bounded or is my approach mistaken and the derivative could be bounded - defined ?","['stability-in-odes', 'roots', 'ordinary-differential-equations', 'dynamical-systems']"
2617453,What is the condition $|f(x)| \le f(|x|)$ called?,Suppose I have a function $f: \mathbb R \to \mathbb R$ so that for all $x\in \mathbb R$ it holds that $|f(x)| \le f(|x|).$ Does this condition have a name?,"['real-analysis', 'inequality', 'normed-spaces', 'terminology', 'analysis']"
2617472,Derive the posterior density of $\Theta$.,"Suppose that $X\sim \operatorname{Unif}(0,\theta)$ and assume a priori that $\Theta\sim Ga(2,1)$, that is $f_\Theta(\theta)\propto\theta e^{-\theta}\mathbb{1}_{[0,\infty)}(\theta)$. Exercise: Derive the posterior density of $\Theta$. What I've tried: I know that $$f_{\Theta\mid X}(\theta\mid x) = \dfrac{f_{X\mid\Theta}(x\mid\theta)\,f_\Theta(\theta)}{\displaystyle\int f_{X\mid\Theta}(x\mid\theta)\,f_\Theta(\theta)d\theta}\propto f_{X\mid\Theta}(x\mid\theta)\,f_\Theta(\theta)$$ What I'm unsure about is how to write $f_{\Theta\mid X}(\theta\mid x)\propto f_{X\mid\Theta}(x\mid\theta)\,f_\Theta(\theta)$, because this product of functions has two indicator functions in it, both with a different argument. We have (sort of?) $f_{\Theta\mid X}(\theta, x) \propto \dfrac{1}{\theta}\mathbb{1}_{(0,\theta)}(x)\,\theta e^{-\theta}\mathbb{1}_{[0,\infty)}(\theta)$ which makes me very uncomfortable. Furthermore when asked to give the posterior distribution, I'm not if I have to supply the entire fraction with the constant, or just the part to which it is proportional. Question: How do I solve this exercise? Thanks!","['statistical-inference', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2617477,"Hardy-Littlewood maximal function not integrable in B(0,1)","It is well known that if $f\in L^1(\mathbb{R}^n)$, then the Hardy-Littlewood maximal function:
$$
Mf=\sup_{r>0}\frac{1}{|B(x,r)|}\int_{B(x,r)}|f(y)|dy
$$
is not in $L^1(\mathbb{R}^n)$. Does there exist a $f$ such that $Mf$ is not integrable in $B(0,1)$?","['functional-analysis', 'harmonic-analysis', 'real-analysis', 'fourier-analysis']"
2617482,"A $G$-module admits a surjection from a $G$-module, which is free as an abelian group, such that the kernel is free","I am asking for a hint on how to prove the following result: Let $G$ be a group, $M$ a $G$-module (here “$G$-module” means $\mathbb{Z}[G]$-module). Show that there exists an exact sequence of $G$-modules $0\to K\to\tilde{M}\to M\to 0$ such that $K$ is a free $G$-module and $\tilde{M}$ is free as an abelian group. I understand how to prove the result when the roles of $K$ and $\tilde{M}$ are interchanged. From the associated long exact sequence, one sees that the higher cohomology groups of $M$ and $\tilde{M}$ are isomorphic. However, it is not clear to me how to construct such a module: the only natural way I know to construct a $G$-module which is free as an abelian group yields a projective module.","['group-cohomology', 'ring-theory', 'group-theory']"
2617513,"Embeddings between Hölder spaces $ C^{0,\beta} \hookrightarrow C^{0, \alpha} .$","Let $ \Omega \subset \mathbb R^n $ be an open subset and let $ 0 < \alpha < \beta \leq 1.$ We consider the space of Hölder continuous functions $C^{0, \alpha}$ which is a Banach space endowed with the norm $$ \| f\|_{C^{0, \alpha}} := \| f \|_{\infty} + \sup_{ x,y \in \Omega \\ x \neq y} \frac{ |f(x) -f(y)|}{|x-y|^\alpha}. $$ My questions has to do with the embedding $ C^{0,\beta} \hookrightarrow C^{0, \alpha} .$ If $ \Omega $ is bounded, then I can prove the estimate $ \| f\|_{C^{0, \alpha}} \leq \text{diam}(\Omega)^{\beta -\alpha} \| f\|_{C^{0, \beta}} ,$ which in turn implies that the embedding is bounded, i.e. continuous. Question: How can I show that the embedding is still continuous in the case where $ \Omega $ is unbounded ? Any help would be really appreciated.","['real-analysis', 'fractional-sobolev-spaces', 'functional-analysis', 'holder-spaces', 'analysis']"
2617515,How can we see a tuple as a set? [duplicate],"This question already has answers here : How can an ordered pair be expressed as a set? (3 answers) Closed 6 years ago . In the book ""mathematical logic"", it is said that $(a,b)$ is an abbreviation for $\{ \{a,a\},\{a,b \}\}$. I don't understand this, since firstly, $\{a,a\}=\{a\}$, and secondly, even if this weren't the case, how does  $\{ \{a,a\},\{a,b \}\}$ capture the ordered relation of $(a,b)$? i.e. how do we see a tuple as a set?",['elementary-set-theory']
2617569,"Let $A=\{1,2,…,6\} , B=\{1,2,…,20\}$. How many functions $f: B\to B$ are there, such that $I_A \subseteq f$ and $f$ is onto?","Let $A=\{1,2,…,6\} , B=\{1,2,…,20\}$ . How many functions $f: B\to B$ are there, such that $I_A \subseteq f$ and $f$ is onto? I know that there are $|B|^{|B|}$ options for functions, meaning ${20^{20}}$ .  Usually when I need to tell how many onto functions there are, I'm using the inclusion–exclusion principle, but I also know I need to take into consideration that $I_A \subseteq f$ and $|A| = 6.$ I do know that I will need to subtract the $6$ ""bad options"" when $I_A \subsetneq f$ but I'm not sure how to go from there. Any help is greatly appreciated.","['combinatorics', 'discrete-mathematics']"
2617575,Norm squared inequality,"I am reading a paper in optimization and confused about the following identity: Let $a,b,c$ be elements of a real Hilbert space $H$, $$
\|a-b\|^2 \leq 2\|a-c\|^2 + 2\|c-b\|^2.
$$ At first I thought it was just the triangle inequality and adding/subtracting $c$ inside the the first norm. However, since we have the norm squared instead of norm I am not sure that's the case. Can the inequality be true as stated?","['real-analysis', 'optimization', 'cauchy-schwarz-inequality', 'triangle-inequality', 'functional-analysis']"
2617606,difference of difference is constant question,"Why is the difference of the differences of squares is 2?
e.g.: 4, 9, 16, 25, 36 ... are all squares, 
the difference of two adjacent are: 5, 7, 9, 11 ... 
and the difference of that is always 2,
why is that happening? thanks",['algebra-precalculus']
2617621,"Solve $\lvert z \lvert = z^4, z \in \mathbb{C}$","$$z^4 =\lvert z \lvert ,    z \in \mathbb{C}$$ Applying the formula to calculate $ \sqrt[4]{z} $, I find that solutions have to have this form: $$z=\sqrt[4]{\lvert z \lvert}$$
$$z=\sqrt[4]{\lvert z \lvert} \ e^{i \frac{\pi}{2}}=i \ \sqrt[4]{\lvert z \lvert}$$
$$z=\sqrt[4]{\lvert z \lvert} \ e^{i \frac{3 \pi}{2}}=-i \ \sqrt[4]{\lvert z \lvert}$$
$$z=\sqrt[4]{\lvert z \lvert} \ e^{i \pi}=-\sqrt[4]{\lvert z \lvert}$$ Using the Cartesian form: $$(a+i b)^4=\sqrt{a^2+b^2}$$ $z=0$ is a solution If $a=0$ : $$(i b)^4=\lvert b \lvert $$
$$b^4=\lvert b \lvert$$ $-i$ and $i$ are solutions If $b=0$ : $$a^4=\lvert a \lvert $$ $-1$ and $1$ are solutions. Finally, these are the solutions of $z^4=\lvert z \lvert$: $$0,-i,i,1,-1$$ Is it correct? Thanks!","['complex-analysis', 'complex-numbers']"
2617676,"For $n \equiv 1 \pmod 3$,there exists $m<n$ such that $n+3k=m^2$","Let $n$ (greater than $1$) be of the form $3k + 1$. Let $k_1$ be the smallest positive integer such that $$n + 3k_1 = n^2.$$
Let $k_2$ and $m$ be the smallest non negative integers for which $$n + 3k_2 = m^2.$$
Prove that $k_2 < k_1$. I can verify that easily taking various values of $n$ satisfying $3k+1$ format, but am clueless as to how to prove it.
For example, when $n = 7$, adding $3$ fourteen times yields $49$ but adding it three times yields $16$.","['number-theory', 'elementary-number-theory']"
2617684,A Gift Problem for the Year 2018 [duplicate],"This question already has an answer here : The final number after $999$ operations. (1 answer) Closed 6 years ago . We had this problem in exam  class  yesterday on Combinatoric  and it was supposed to be the new year gift from our teacher. The exercise was entitled A Gift Problem for the Year 2018 Problem: The numbers  $1,\frac{1}{2},\frac{1}{3},\frac{1}{4},\cdots,\frac{1}{2018} $ are written on the blackboards. John chooses any two numbers say $x$ and $y$ erases them and writes the number $x+y+xy$.
  He continues to do so until there is only one number left on the board.
  What are the possible value of the final number? I understood the problem as follows for instance if John take $x=1$ and $y=\frac{1}{2}$ then $x+y+xy =2$ and the new list becomes $$2,\frac{1}{3},\frac{1}{4},\cdots,\frac{1}{2018} $$ 
 continuing like this and so on..... Please bear with me  that I do not want to propose my solution since I fell like it was wrong and I don't want to fail the exam before the result get out. but by the way I found, $2017$, $2018$ and $2019$ but I am still suspicious. You may help is you have an idea.","['finite-groups', 'combinatorics', 'summation', 'harmonic-numbers', 'group-theory']"
2617701,Monodromy representations and geodesics of singular flat metrics on $\mathbb{H}$,"Let $X$ be a closed Riemann surface of genus $g\ge 2$ and $\pi:\mathbb{H}\rightarrow X$ the holomorphic universal cover. Then $X$ is biholomorphic to the quotient $\mathbb{H}/\Gamma$, where $\Gamma$ is a Fuchsian group and $\rho:\pi_1(X)\rightarrow \Gamma$ is the monodromy representation. Given $x,y\in \mathbb{H}$ denote by $\sigma$ the hyperbolic geodesic from $x$ to $y$. For every $\gamma\in \pi_1(X)$ clearly (since $\Gamma$ is a group of isometries of $\mathbb{H}$) $\rho(\gamma)(\sigma)$ is the hyperbolic geodesic from $\rho(\gamma)(x)$ to $\rho(\gamma)(y)$. Now let $q$ be a quadratic holomorphic differential on $X$ and consider  $\mathbb{H}$ endowed with the flat singular metric $|\pi^*q|$. I am wondering if it is still true a statement similar to the preceding one. Given $x,y\in \mathbb{H}$ denote by $\tau$ the $|\pi^*q|$-geodesic from $x$ to $y$. Is it true that $\rho(\gamma)(\tau)$ (i.e. the monodromy action $\rho(\gamma)$ applied on $\tau$) is the $|\pi^*q|$-geodesic from $\rho(\gamma)(x)$ to $\rho(\gamma)(y)$?","['algebraic-topology', 'riemann-surfaces', 'riemannian-geometry', 'differential-geometry']"
2617705,Definition of matrix Lie group,"In Lie Groups, Lie Algebras and Representations by Brian C. Hall, one finds the following on page 3: Definition 1.4 A matrix Lie Group is any subgroup $G$ of $GL(n;\mathbb{C})$ with the following property: If $A_m$ is any sequence of matrices in $G$, and $A_m$ converges to some matrix $A$ then either $A\in G$, or $A$ is not invertible . Why do we have ""or $A$ is not invertible?"". I was expecting: ""If $A_m$ is any sequence of matrices in $G$, and $A_m$ converges to some matrix $A$ then $A\in G$."", without an ""or"" clause, to express the closure property of $G$. It seems that the ""or"" clause defeats the closure? I guess my problem is the difference between ""closed"" and ""complete"". In the definition of a ""complete"" metric space, we stipulate: ""a metric space $M$ is called complete (or a Cauchy space) if every Cauchy sequence of points in $M$ has a limit that is also in $M$"". But we also have ""In a topological space, a closed set can be defined as a set which contains all its limit points"". I naively drew an analogy between the two concepts, but that ""or"" clause seems to say that some sequences $A_m$ can have limit points that are not in $G$? Can we contrast ""closed"" and ""complete"" so that this becomes more intuitive?","['general-topology', 'group-theory', 'lie-algebras', 'lie-groups']"
2617708,"What is the difference between first order logic on a power set, and second order logic on the base set?","The question title doesn't fully capture my question, so I will clarify: Assume we have the structure of natural numbers: $$(\mathbb N, +,0,1)$$ We know from theorems in mathematical logic, that we cannot use first order predicate logic to fully characterize this structure with a set of axioms. Yet second order logic is incomplete (not all true statements can be proven). As far as I understand it, in axiomatic set theory we use first order logic, which is complete, but we quantify over a domain that contains both sets and urelements, thereby allowing us to state things in first order logic that would normally require second order logic (I am not saying that this is the purpose of axiomatic set-theory, but merely that it is true). But to simplify, we don't need to study the domain of all mathematical objects as set theory does. We can also formulate the structure: $$(\mathbb N,\mathcal P(\mathbb N), +,0,1,\in)$$ and then use first order logic on this structure. I assume that the standard proof calculus for this is complete, as it has been shown to be complete for first order logic? If so, then my question is: What is the difference between $$
\begin{align}
\text{applying second-order logic to } \quad &(\mathbb N, +,0,1) &\quad \text{ and}\\
\text{applying first-order logic to } \quad &(\mathbb N,\mathcal P(\mathbb N), +,0,1,\in)&
\end{align}
$$ Is there a difference? How do we make sense of first-order-logic-completeness, and second-order-logic-incompleteness, if there is no difference?","['second-order-logic', 'first-order-logic', 'logic', 'elementary-set-theory']"
2617765,Region of attraction and stability via Liapunov's function,"EXERCISE: Estimate the region of stability for the stationary point $O(0,0)$ given the differential system:
  $$x'=y$$
  $$y'=x^7-2\cdot x-y$$
  using the liapunov's funcion $V(x,y)=\dfrac{1}{2}\cdot x^2+\dfrac{1}{2}\cdot(x+y)^2$ Attempt :
We have that $V'=V_x \cdot x'+V_y \cdot y'$ So,$V'=(2x+y) \cdot y +(x+y)\cdot (x^7-2x -y)=x^8-2x^2+x^7y-xy $ So,how can i find the sign of V'.I think that i have to find it $V'<0$ everywhere outside the origin and the stationary point $O(0,0)$ will be asymptotic stable! After,that how can i find the region of attraction?","['basins-of-attraction', 'dynamical-systems', 'stability-theory', 'stability-in-odes', 'ordinary-differential-equations']"
