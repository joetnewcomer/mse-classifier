question_id,title,body,tags
4742290,Understanding the permutation representation,"I am learning about representation theory, and I would like to be sure that I have properly understood the permutation representation. I therefore hope that someone can read what I have written below and tell me if I have misunderstood something. Let $G$ be a group that acts on a set $S$ from the left, i.e., there exists a multiplication $G\times S\rightarrow S$ which is associative and such that $es=s$ , with $s\in S$ and $e$ being the identity in $G$ . We construct the permutation representation $\rho :G\rightarrow GL_n$ by defining a vector space $V$ by the span of the natural basis $\{\boldsymbol{e}_s\}_{s\in S}$ and then define each matrix $\rho(g)$ , by $$\tag{1}
\rho(g)\boldsymbol{e}_s:=\boldsymbol{e}_{gs},
$$ and extend linearly: $$\tag{2}
\rho(g)\sum_{s\in S}a_s\boldsymbol{e}_s=\sum_{s\in S}a_s\rho(g)\boldsymbol{e}_s.
$$ The associated module is defined by the vector space $V$ with the multiplication $g\boldsymbol{e}_s=\boldsymbol{e}_{gs}$ . If $S=G$ , then the permutation representation is also called the regular representation .","['permutations', 'representation-theory', 'modules', 'abstract-algebra', 'group-theory']"
4742308,"How should I revise my understanding of a ""number"" so that it makes sense for one number to represent three shapes in three different dimensions?","Take for instance the number 64. How is it that it can represent a line with a length of sixty-four units, a square with one side the length of eight units and a cube with one side the length of four units? I get that there are three different units of measurement involved; say, cm, cm2 and cm3. However, it doesn't seem to fully address my question. I'm currently reading Hung-Hsi Wu's Pre-Algebra to hone in my definitions before moving on to calculus and it struck me as odd that the author explained addition, subtraction and division of two fractions strictly on a one-dimensional number line but when it came to the multiplication of two fractions, he resorted to a two-dimensional plane with each side represented by one of the fractions, which got me to the question above (the two are not really related, but I want you to know the mental framework I'm currently in). The author defines a fraction as a point on a number line which is the definition of a number, too, and I am unable to grasp how something that represents a point on a number line can also be utilized to represent an area or a volume (I can understand a fraction as representing the distance between two points on a one-dimensional line as the point on which the bigger one of the two numbers would land had the two numbers been pulled back such that the smaller number was placed on the origin. )",['algebra-precalculus']
4742315,is $R(T_1+T_2)=R(T_1)+R(T_2)$ true?,"If $T_1,T_2\in\mathcal{L}(H)$ be two linear bounded operators on the Hilbert space $H= H_1\oplus H_2$ with $R(T_1)\subseteq H_1 $ and $R(T_2)\subseteq H_2 $ , is $R(T_1+T_2)=R(T_1)+R(T_2)$ true ?, where R(T) represents the range of T.
If no, then under what condition, my argument will be true. Here $R(T_1+T_2)\subseteq R(T_1)+R(T_2).$ I am unable to prove or disprove the converse.
Kindly provide  suggestions. Thanks.","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4742360,Solving $\log_x\left(\frac{\log_4(x)}{\log_4(x)-3}\right)^{\log_3(x)}= 2$,"I am trying to solve this equation: $$\log_x\left(\frac{\log_4(x)}{\log_4(x)-3}\right)^{\log_3(x)}= 2$$ I'd like some advice on what to go about it, so far I have made it into: $$\begin{aligned}\log_3(x)\cdot\log_x\left(\frac{\log_4(x)}{\log_4(x)-3}\right)=2
\\\log_3(x)\cdot\log_x(\log_4(x))-\log_x(\log_4(x)-3)=2\end{aligned}$$ And I'm unsure how to proceed from here?","['algebra-precalculus', 'logarithms']"
4742376,Suppose $G$ is a finite group and $|G:H|=n$ I want to show that $|H:H\cap H^g|\leq n$ for all $g\in G$.,"Suppose $G$ is a finite group and $|G:H|=n$ I want to show that $|H:H\cap H^g|\leq n$ for all $g\in G$ . If I let $g\in H$ then we know $H^g \leq N_G(H)$ which then we can apply the first isomorphism theorem and conclude $HH^g \leq G$ , $H\trianglelefteq HH^g$ and $H \cap H^g\trianglelefteq H$ and hence $$G/ H \geq HH^g/ H \trianglerighteq H/H\cap H^g$$ But for $g\notin H$ I have no clue. Since $g\notin H$ , I guess I can argue $H^g\leq G$ and hence $H^g\cap H \leq G$ and also $H\cap H^g \leq H$ but then these left cosets arent a group so I can't really say much. Maybe I can argue $H\cap H^g \subset H$ and hence $G/ H\cap H^g \supset H/ H\cap H^g$ and conlude it there? Any hint would be great appreciated. I want to fill the part knowledge I'm missing to complete this.","['group-theory', 'abstract-algebra', 'solution-verification', 'group-isomorphism']"
4742401,Show that $T$ is an unbiased estimator,"I'm given the following exercise;
Let $X_1$ , $X_2$ ,..., $X_n$ a sample from Uniform distribution $U(\theta,2\theta)$ , $\theta>0$ , with PDF $$f(x)=\frac{1}{\theta}, \theta<x<2\theta$$ and $$T=\frac{n+1}{2n+1}X_{(n)}$$ where $X_{(n)}=max{X_i}, i=1,2,....,n$ . I 'm instructed to show that $T$ is an unbiased estimator for $\theta$ , and determine its variance ( $Var(T)$ ). The only step I 'm sure of, is that I need to show $E[T]=\theta$ , the definiton of an unbiased estimator. From there, I am not sure how to continue. Normally I would have to simplify $E[T]$ , which might go like this; $$E[T]=E[\frac{n+1}{2n+1}X_{(n)}]=\frac{n+1}{2n+1}E[X_{(n)}]$$ ... and that's it. Even if I take $E[X_{(n)}]=E[X]=\frac{3\theta}{2}$ (where the first equality is probably not correct / the second equality derives from the fact that $E[X]=\frac{a+b}{2}$ for the uniform distribution in a domain $(a,b)$ ), I don't see the path that would lead me to $E[T]=\theta$ . Any help would be really appreciated, I 'm just getting started with statistics on Uni and only at the beginning of getting the hang of it.",['statistics']
4742446,Affine connection on non smooth vector fields,"The usual definition of an affine connection $\nabla_X Y$ requires $X,Y$ to be smooth vector fields on the ambiant differential manifold $M$ , i.e. $C^\infty$ . However at any point $p\in M$ , $(\nabla_X Y)_p$ means the differential of $Y$ at $p$ in the direction $X_p$ . So it seems enough that $Y$ is just differentiable, and we don't need any hypothesis on $X$ , not even continuity. Likewise, geodesics are required to be smooth curves, but their definition $\nabla_{\dot{\gamma}}\dot{\gamma}=0$ suggests that they are only $C^2$ (their acceleration is zero). And regarding the parallel transport of a tangent vector $u\in T_p M$ along a curve $\gamma$ , it seems that $\gamma$ just has to be piecewise differentiable : compose the parallel transports on each segment where $\gamma$ is differentiable. The speed $\dot{\gamma}$ does not need to exist at the junction points. Are there generalized definitions of connections for these non smooth cases ?","['vector-fields', 'connections', 'differential-geometry']"
4742489,"If $\alpha\wedge d\alpha$ is a volume form, there exists a vector field $X$ such that $i_X\alpha\equiv1$ and $i_X (d\alpha)\equiv0$.","I'm currently stuck on the following problem: Let $\alpha$ be a 1-form on a connected 3-manifold $M$ such that $\alpha\wedge d\alpha$ is a volume form. Show that there exists a vector field $X$ on $M$ such that $i_X\alpha\equiv1$ and $i_X(d\alpha)\equiv0$ . I believe there are a few routes to solve this problem, but I keep getting stuck at each step. Working locally, I first wrote $$\alpha=\alpha_1dx+\alpha_2dy+\alpha_3dz$$ and noticed that $\alpha$ is non-vanishing. Indeed, if not, there exists a point $p\in M$ such that $\alpha_i(p)=0$ for $i=1,2,3$ . Computing $\omega=\alpha\wedge d\alpha$ , it follows that $\omega_p=0$ , a contradiction. Thus, because $\alpha\neq0$ , we get (by a theorem about $\alpha\wedge d\alpha$ ) that $\ker\alpha$ is not involutive, but I'm really not sure what I can do from here. I suppose that if we choose appropriate vector fields $X,Y\in\ker\alpha$ , we do get that $$d \alpha(X, Y)=X(\alpha(Y))-Y(\alpha(X))-\alpha([X, Y])=-\alpha([X,Y])$$ is non-zero. Alternatively, there is the usual expansion $$i_X(\alpha\wedge d\alpha)=i_X\alpha\wedge d\alpha-\alpha\wedge i_Xd\alpha,$$ which connects both the conditions that $i_X\alpha\equiv1$ and $i_Xd\alpha\equiv0$ with the contraction of a volume form, but again I'm not able see where this leads me. I'd really appreciate any help towards a full solution. I'm reviewing my knowledge of smooth manifold theory, so this problem is not homework (although it's from an old exam at my university). Thank you!","['differential-forms', 'vector-fields', 'smooth-manifolds', 'differential-geometry']"
4742499,Why is integration by parts not working on this problem?,"I am trying to evaluate the following integral using integration by parts: $$\int\frac{x}{1+e^x}dx$$ However, using $u = x$ , $du = 1$ , $dv = \frac{1}{1+e^x}$ , $v = x-\log(e^x+1)$ , I keep getting that the  integral is $-\text{Li}_2(-e^x)+\frac{x^2}{2}-x\log(1+e^x)$ , but Wolfram Alpha says that the integral is $\text{Li}_2(-e^{-x})-x\log(e^{-x}+1)$ . Can anyone tell me what I'm doing wrong here?","['integration', 'indefinite-integrals', 'calculus']"
4742569,Strengthening the Log-Concavity of Binomial Coefficients: $\binom{n}{k-1}\binom{n}{k+1} < \binom{n}{k}^2 - \binom{n}{k}$,"In the following question: Log concavity of binomial coefficients: $ \binom{n}{k}^2 \geq \binom{n}{k-1}\binom{n}{k+1} $ It is proven via a combinatorial injective argument. However, by noticing that the set of identical pairs of choices cannot be in the output space of the transformed pair of selections, we can strengthen the inequality to $$\binom{n}{k-1}\binom{n}{k+1} \leqslant \binom{n}{k}^2 - \binom{n}{k}$$ From which the natural question follows: Is there a combinatorial argument that allows us to remove the equality case? (This has been checked numerically and holds) That is, how can we extend the argument even further to prove $$\binom{n}{k-1}\binom{n}{k+1} < \binom{n}{k}^2 - \binom{n}{k}$$","['combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'elementary-set-theory', 'inequality']"
4742574,Price of Option in Betting Game,"We both put 20 USD into a box. Then, we each generate a number in the interval (0,1) with uniform distribution. The person with the higher number wins and takes 40 USD, whilst the loser is left with 0 USD.
I offer to sell you an option that allows you to regenerate your number after you see both of our numbers. What is the price of the option? This is my approach. After we get our number, there is a 50% chance that yours is higher than mine. If this is the case, I choose to regenerate my number to try and win, which gives me another 50% chance of winning. However, if my number is initially higher than yours (50% chance), then I do not use the option. This gives the expected probability P of winning of: P = 0.5(1) + 0.5(0.5) = 0.75 Hence price of option would be (0.75*40) - 0.5(40) = $10 However, the answer is 20/3. May someone please explain where I am going wrong?","['statistics', 'gambling', 'expected-value', 'game-theory', 'probability']"
4742598,Sum of the Sines of the Ratios of Fibonnaci Numbers,"Saw an online a proof that $$\lim_{n\to \infty}\sum_{k=1}^{n}\sin\left(\frac{k}{n^2}\right)=\frac{1}{2}$$ that utilized the Squeeze Theorem and the fact that $x-x^3\leq \sin(x)\leq x$ for small $x$ . This got me thinking of the sum $$\lim_{n\to \infty}\sum_{k=1}^{n}\sin\left(\frac{F_k}{F_n}\right)$$ where $F_1=F_2=1$ and $F_n=F_{n-1}+F_{n-2}$ for $n>2$ . Testing partial sums on Wolfram Alpha confirms that such a series converges, and seems to approach around 2.4088209. I assume this might use similar tricks involving the golden ratio, but the same Squeeze Theorem trick doesn't seem to be useful here (it only really helped me show that the sum must be less that $\Phi^2$ ). Any one have any thoughts or methods on how to solve this?","['trigonometric-series', 'limits', 'fibonacci-numbers', 'sequences-and-series']"
4742599,Eigenvalue of diagonally dominant matrices,"I want to ask a question about eigenvalue  of diagonally dominant matrices. The question is : Assume $A=(a_{ij})_{n\times n}\in M_n(\mathbb{R})$ and $\lambda_1,\lambda_2,
\cdots,\lambda_n$ are the eigenvalues of $A$ . $A$ satisfies: $(1):$ $a_{ii}>0~~,i=1,2,\cdots,n$ . $(2):$ $a_{ij}<0~~,i\ne j$ . $(3):$ $a_{ii}>-\sum_{i\ne j}a_{ij}~~,i=1,2,\cdots,n$ . I would like to ask if there is $\mathrm{Re}(\lambda_i)\ge\vert \mathrm{Im}(\lambda_i)\vert,~~ i=1,2,\cdots,n$ ? I think it might have something to do with the estimation of the eigenvalues of the diagonal dominance matrix.
Any help or suggestions are appreciated. Maybe it is not true and I can't find an counterexample. Thanks!","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'eigenvalues-eigenvectors']"
4742638,Simplifying a Polynomial Expression with given information about parameter,"I have difficulty to solve the following question: Let $w \neq 1$ complex number that satisfies $w^{3} = 1$ , Let $P(z) = 3 + 2z - 3z^{2} + 5z^{3} + 6z^{5} - 9z^{6}$ Calculate $P(1) + P(w) + P(w^{2})$ I have attempted the following calculations: $P(1) = 4$ $P(w) = 3w^{2} + 2w - 1$ $P(w^{2}) = 2w^{2} + 3w - 1$ Hence, $P(1) + P(w) + P(w^{2}) = 5w^{2} + 5w + 2$ However, I believe there might be a parameter-less solution by utilizing the fact that $w^{3} = 1$ . Could someone kindly guide me on how to proceed with this approach? Thank you very much for your help.","['complex-analysis', 'complex-numbers']"
4742691,Proving $\sum\limits_{k=0}^{n-1}\frac{\left(-1\right)^{n-k-1}\binom{2n}{2k+1}\binom{2k}{k}\binom{2n-2k}{n-k}}{2^{2n}\binom{n}{k}} = (-1)^{n-1}$,"I found the below combinatorial identity seems true: $$\sum\limits_{k=0}^{n-1}\frac{\left(-1\right)^{n-k-1}\binom{2n}{2k+1}\binom{2k}{k}\binom{2n-2k}{n-k}}{2^{2n}\binom{n}{k}} = (-1)^{n-1}$$ but I have no idea how to prove it, anyone has some ideas? Here is where this problem comes from. I want to prove this Fourier series expansion $\log\left(2\cos\frac{\theta}{2}\right)=\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n-1}\cos n\theta}{n}$ then I try to calculate the coefficients by evaluating the following integral \begin{eqnarray}
a_{n}&=&\frac{2}{\pi}\int_{0}^{\pi}\cos\left(n\theta\right)\log\left(2\cos\frac{\theta}{2}\right)d\theta\\&=&\frac{2}{n\pi}\int_{0}^{\pi}\frac{\sin\left(n\theta\right)\sin\frac{\theta}{2}}{2\cos\frac{\theta}{2}}d\theta\\&=&\frac{\left(-1\right)^{n-1}}{n}\\A_{n}&=&\int_{0}^{\pi}\sin\left(n\theta\right)\tan\frac{\theta}{2}d\theta\\&=&2\int_{0}^{\frac{\pi}{2}}\sin\left(2n\theta\right)\tan\theta d\theta\\&=&2\sum_{k=0}^{n-1}\left(-1\right)^{n-k-1}\left(\begin{array}{c}
2n\\
2k+1
\end{array}\right)\int_{0}^{\frac{\pi}{2}}\left(\cos\theta\right)^{2k}\left(\sin\theta\right)^{2n-2k}d\theta\\&=&\sum_{k=0}^{n-1}\left(-1\right)^{n-k-1}\left(\begin{array}{c}
2n\\
2k+1
\end{array}\right)\frac{\Gamma\left(\frac{2k+1}{2}\right)\Gamma\left(\frac{2n-2k+1}{2}\right)}{\Gamma(n+1)}\\&=&\pi\sum_{k=0}^{n-1}\left(-1\right)^{n-k-1}\left(\begin{array}{c}
2n\\
2k+1
\end{array}\right)\frac{\left(\begin{array}{c}
2k\\
k
\end{array}\right)k!\left(\begin{array}{c}
2n-2k\\
n-k
\end{array}\right)(n-k)!}{2^{2n}n!}\\&=&\pi\sum_{k=0}^{n-1}\frac{\left(-1\right)^{n-k-1}\left(\begin{array}{c}
2n\\
2k+1
\end{array}\right)\left(\begin{array}{c}
2k\\
k
\end{array}\right)\left(\begin{array}{c}
2n-2k\\
n-k
\end{array}\right)}{2^{2n}\left(\begin{array}{c}
n\\
k
\end{array}\right)}\\&=&2\pi\left(-1\right)^{n-1}?
\end{eqnarray} therefore, I have two questions: (1) is there a direct way to evaluate the integral $A_n$ ? (2) how to prove the identity in a combinatoric way ?","['summation', 'combinatorics']"
4742718,Why exactly does this 2 get larger and smaller? (superimposed rotating patterns forming a random moiré),"While searching for something very different, I stumbled upon these links by Emin Gabrielyan whom I assume is also the author of the arXiv preprint The basics of line moire patterns and optical speedup also in 2007. Aperiodic random line moiré 2007-02-27 Examples of random moiré 2007-02-12 The Examples of random moiré page contains the GIF below with the following description. The figure below animates superposition of a base layer comprising randomly scattered copies of the symbol “2” with an opaque revealing layer comprising similarly placed tinny holes. In this animation, the revealing layer oscillates between –2 degree and +2 degree. The diameter of holes of this sample is about 4 to 5 times smaller than the size of the shapes of the base layer. What's happening reminds me of some of the demonstrations of Tadashi Tokieda in the Numberphile video Freaky Dot Patterns , especially after 04:00. How exactly does rotating one pattern ±2° relative to the other cause this huge zooming/de-zooming and inverting effect on the single big ""2"" in the center of the image?","['visualization', 'geometry']"
4742721,How to find a sparse lattice basis?,"I am working with lattice codes (see here , or here ) and facing the following problem: I have a set of $k$ vectors $\left\{v_1,\ldots,v_k\right\}$ which I know generate an $n$ -dimensional, full-rank lattice, in the sense that any lattice vector can be written as linear combination of the $v_j$ s with integer coefficients. This set is over-complete ( $k>n$ ) and has the property that the weight, i.e. the number of non-zero elements, of any of the $v_j$ s is low (bounded by a constant). Is there any way of constructing a lattice basis from an over-complete set such that the low-weight property is conserved? If that helps, we can assume that the lattice is obtained through construction $A$ , so thinking of the $v_j$ s as column vectors we can write $$ M = \left(v_1 , v_2, \ldots , v_k \right)  =  \left(v_1 , v_2, \ldots , v_{k-n}\ |\ 2\mathbb{I}_n \right) $$ with $\mathbb{I}_n$ the identity matrix, $v_1 , v_2, \ldots , v_{k-n} $ have entries in $\left\{0,1\right\}$ and our lattice would be $$ \mathcal{L} = \left\{Mz\in \mathbb{R}^n :\ z\in\mathbb{Z}^k \right\} .$$ In general, constructing a basis is reasonably easy using the Hermite normal form (HNF), but then the weight can become as large as $n$ . If we do assume we found a basis through the HNF, for example, then my problem could be stated as $$\text{minimize } f(U) = \left(\max_{j\in\left\{1,\ldots,n\right\}} \left\| (AU)(j) \right\|_0 \right)\ \text{subject to } U \text{ is unimodular} $$ where $(A U)(j)$ denotes the $j$ -th column of $A U$ and $\| v \|_0 = \sum\limits_j | v_j |^0$ is the zero ""norm"". I have spent some time looking into lattice basis reduction methods but it seems that most  algorithms aim at finding ""short"" bases (e.g., LLL), in the sense of Euclidean length, whereas I am specifically concerned with the ""sparsity"" of the basis. Intuitively, I think that this could also be phrased as basis reduction in the zero norm. Any pointer to algorithms to reduce the weight of lattice basis vectors would be much appreciated.","['unimodular-matrices', 'integer-lattices', 'matrices', 'linear-algebra', 'coding-theory']"
4742722,An etale morphism to a separated scheme is affine?,"I see the following statement in the proof of the local structure theorem of DM stacks from Jarod Alper's notes (Theorem 4.2.1). My question is why an etale from an affine scheme to a separated scheme is affine. Furthermore, is such an etale morphism still affine if $U$ is not affine or $\mathfrak{X}$ is only quasi-separated? I tried to prove the statement by using the cohomological criterion for affineness or by assuming the morphism $U\to\mathfrak{X}$ is standard etale. But neither seems to give a proof in an obvious way.","['algebraic-stacks', 'algebraic-geometry']"
4742745,Can I replace modulus inequalities with rooted square arguements?,"Suppose I want to show $|x-5|<|x+1|$ . One way (and the way my lecturer shows) to do it is look at the negative and positive regions and solve the inequality. But with the definition $\sqrt{x}\geq0$ , $\forall x\in \Bbb R$ , I can say that $|x|=\sqrt{x^{2}}$ . So could I, in general over $\Bbb R$ , use this definition to substitute the modulus argument? Obviously in some cases this won't do anything because I will end up with the same +/- situation, but at least here the task is (at least for my brain) more algebraically simple, because: $$
|x-5|<|x+1| \iff\sqrt{(x-5)^{2}} < \sqrt{(x+1)^{2}}\iff(x-5)^{2}<(x+1)^{2}$$ With dif. squares: $$(x-5)^{2}-(x+1)^{2}<0$$ $$(2x-4)(-6)<0$$ $$x>2$$ To me this seems extremely obvious because I can use the ""intuition"" $$a<b, c<d\implies ac<bd \quad\forall a,b,c,d \in \Bbb R_{\geq 0 },$$ but I'm new to real analysis and everything seems to have weird caveats that make nothing intuitive true in general haha. Is there a proof for this or is the intuition something that just logically extends from Peano's axioms and natural number theorems?","['inequality', 'absolute-value', 'real-analysis']"
4742763,Prove $\prod_{k = 1}^{n - 1}(x - e^{\frac{2\pi ik}{n}}) = \frac{x^n - 1}{x - 1}$,"I'm reading the solution to a math question, and part of the solution states this: $\prod_{k = 1}^{n - 1}(x - e^{\frac{2\pi ik}{n}})$ is equal to $\frac{x^n - 1}{x - 1}$ by the roots of unity. However, I'm not quite getting how these two are equal--not that I doubt it, I'm just stuck on how they got there by the roots of unity. In other words, could someone prove or further explain how they got from the first to the second expression?","['algebra-precalculus', 'proof-writing', 'roots-of-unity', 'complex-numbers']"
4742783,Is the Essential Spectrum the same as the Continuous Spectrum and Residual Spectrum,"Given a linear operator $A:D(A)\rightarrow X$ (where $D(A)$ is a dense subset of $X$ and $X$ is a Banach space or Hilbert Space), we define the spectrum to be $$
\sigma(A)=\{\lambda\in\mathbb{C}: A-\lambda \text{is not invertible}\}
$$ We note that $\sigma(A)$ will be a closed subset of $\mathbb{C}$ . Now there are a few ways that one may break up the spectrum. Firstly, we can decompose it into the discrete spectrum and the essential spectrum that is $\sigma(A)=\sigma_d(A)\cup \sigma_{ess}(A)$ . Where we have that the discrete spectrum, $\sigma_d(A)$ , consists of all eigenvalues of $A$ such that $A-\lambda$ has finite algebraic multiplicity. Then we define the essential spectrum $\sigma_{ess}(A)$ to be the complement of $\sigma_d(A)$ inside of $\sigma(A)$ . It is clear that this is a partition of the spectrum. Now there is another way in which we may break up the spectrum. We can decompose it as $\sigma(A)=\sigma_{eigenvalues}(A)\cup \sigma_{cont}(A)\cup\sigma_{res}(A)$ . In this decomposition, we have that $\sigma_{eigenvalues}(A)$ are the eigenvalues of $A$ consisting of those $\lambda\in\mathbb{C}$ such that $\ker(A-\lambda)\neq 0$ . Thus, the remaining cases what happens if $\ker(A-\lambda)=0$ , then this splits up into two cases depending upon the range of $A-\lambda$ . If $\text{Ran}(A-\lambda)$ is a dense subset of $X$ , then we have that $A-\lambda$ will have a densely defined inverse (which necessarily must be unbounded since if it were bounded then $A-\lambda$ would be invertible and $\lambda$ would not be in the spectrum) such $\lambda$ are a part of the continuous spectrum $\sigma_{cont}(A)$ . Then the last case is if $\ker(A-\lambda)=0$ , but $\text{Ran}(A-\lambda)$ is not dense, then put such $\lambda$ in the residual spectrum $\sigma_{res}(A)$ . Thus, we have these two different decompositions of the spectrum. My question is how related are these two decompositions? Since intuitively I would like to say that $\sigma_{eigenvalues}(A)=\sigma_d(A)$ , and then $\sigma_{ess}(A)=\sigma_{cont}(A)\cup\sigma_{res}(A)$ , but this seems wrong after thinking about it since there may be an eigenvalue with infinite algebraic multiplicity, so is all that we can say is that $\sigma_d(A)\subset\sigma_{eigenvalues}(A)$ and hence $\sigma_{cont}(A)\cup\sigma_{res}(A)\subset\sigma_{ess}(A)$ , or is there some sort of further decomposition relating these? Or maybe there is some property of $A$ (like maybe being normal or self-adjoint or something like that) which might make this true?","['spectral-theory', 'functional-analysis']"
4742823,"Another proof for $\,\cos(\beta-\alpha)=\sin\alpha\sin\beta+\cos\alpha\cos\beta\,$ formula","I was solving a vector’s problem : The question was: Find the length of $\vec{a} + \vec{b}$ vector in terms of $a,\,b,\,\alpha,\,\beta\,.$ Well, we know $\vec{a}$ equals to $\,(a\cos\beta,a\sin\beta)\,$ and $\vec{b}$ equals to $\,(b\cos\alpha,b\sin\alpha)\,.$ $\vec{a}+\vec{b}=(a\cos\beta+b\cos\alpha,a\sin\beta+b\sin \alpha)\,.$ So, the length of $\vec{a}+\vec{b}$ equals to : $\sqrt{(a\cos\beta+b\cos\alpha)^2+(a\sin\beta+b\sin\alpha)^2}\,.$ Another way of solving this problem is drawing  Cartesian coordinate system like this or using cosine rule which gives you: $\left|\vec{a}+\vec{b}\right|=\sqrt{a^2+b^2+2\!\cdot\!a\!\cdot\!b\!\cdot\!\cos(\beta-\alpha)}$ As a result, $\sqrt{(a\cos\beta+b\cos\alpha)^2+(a\sin\beta+b\sin\alpha)^2}=$ $=\sqrt{a^2+b^2+2\!\cdot\!a\!\cdot\!b\!\cdot\cos(\beta-\alpha)}\,.$ Now I'm trying to prove $\,\cos(\beta-\alpha)=\sin\alpha\sin\beta+\cos\alpha\cos\beta\,$ using this equality. I solved this equation for $\,\cos(\beta-\alpha)\,$ : $\cos(\beta-\alpha)=\dfrac{a^2\cos^2\!\beta+2ab\cos\beta\cos\alpha+b^2\cos^2\!\alpha+a^2\sin^2\!\beta+2ab\sin\beta\sin\alpha+b^2\sin^2\!\alpha-b^2-a^2}{2ab}$ The problem is that $a$ and $b$ still exist in the equality. I don't get why this happens. Does replacing $\,\sin^2\!\alpha\,$ and $\,\sin^2\!\beta\,$ with $\,1-\cos^2\!\alpha\,$ and $\,1-\cos^2\!\beta\,$ help to solve this problem ?",['trigonometry']
4742827,About differentiable dependence in a Cauchy problem,"Let $\epsilon >0$ , consider the Cauchy problem: $$\epsilon x' = x^2 + (1-\epsilon)t \quad , x(0)=1$$ If $x(t;\epsilon)$ denotes the solution (defined on the maximum interval) of the problem, I'm asked to verify that: $$\frac{\partial x}{\partial \epsilon} (t,1) = \frac{t}{(1-t)^2} \left ( \frac{t^2}{3} - \frac{t}{2} -1 \right )$$ I'm not able to get the previous expression and I'm getting kinda crazy xd. I know for a fact that the function $u = \partial_\epsilon (\cdot,1)$ solves the linear problem: $$u' = f_x (t,x(t;1),1) u + f_\epsilon (t,x(t;1),1) \quad , u(0)=0$$ In this case $f(t,x,\epsilon) = \frac{x^2}{\epsilon} + \frac{1-\epsilon}{\epsilon} t$ so: $$f_x (t,x,\epsilon) = \frac{2x}{\epsilon} \qquad f_\epsilon (t,x,\epsilon) = - \frac{x^2}{\epsilon^2} - \frac{t}{\epsilon^2}$$ , and the Cauchy problem I should work on is: $$u' = \frac{2}{1-t}u -t - \frac{1}{(1-t)^2} \quad , u(0)=0$$ , since $x(t;1) = (1-t)^{-1}$ . The solution to this IVP is: $$u(t) = - \frac{t (3t^3 - 8t^2 +6t+12)}{12 (1-t)^2}$$ (by Wolfram-Alpha), and it doesn't match with the solution given. Is there any mistake in my work?. ANY suggestion will be appreciated :)","['cauchy-problem', 'analysis', 'ordinary-differential-equations']"
4742870,Inverse of a Markov chain is always a Markov chain?,I'm aware that a Markov chain with a stationary distribution has an inverse which is a Markov chain. But I suspect this to be false in general. Can someone please provide a counterexample?,"['stochastic-processes', 'markov-process', 'probability-theory', 'markov-chains']"
4742875,"Intuition behind $Q_t=\sum \langle M^{\alpha},M^{\alpha}\rangle_t+\sum |A^{\alpha}|^3_t+|A^{\alpha}|_t+t$","Consider the semimartingale $Z$ , which by Doob decomposition can be written as $Z=M+A$ , where M is a martingale and A is the process of total bounded variation.
I am trying to make sense of the following definition of $Q$ , but I do not understand the intuition behind the author defining $Q$ in the following way. I know that something approximate would be derived considering $\langle Z,Z\rangle=\langle M+A,M+A\rangle$ , by using inner product properties, in which this inner product would be the quadratic variation. But in that sense the author does not really define the terminology being employed for example || stands for quadratic variation, but why not write $\langle M^{\alpha},M^{\alpha}\rangle_t$ as $|M^{\alpha}|_t$ instead? Let $Z$ be the canonical decomposition of semimartingale $Z$ into a local martingale and a process of locally bounded variation. Define $$Q_t=\sum \langle M^{\alpha},M^{\alpha}\rangle_t+\sum |A^{\alpha}|^3_t+|A^{\alpha}|_t+t$$ where $|A^{\alpha}|_t$ stands for the total variation of $A^{\alpha}$ on [0,t].Adding $t$ in the definiton makes $Q$ strictly increasing. Then he proves the following inequality and he uses $Q$ in order to create upper bounds with respect to the martingale and bounded variation integral. I have no idea where this is coming from. Lemma: There is a constant $C$ depending only on $m$ and $l$ such that for any $F*$ adapted, $M(m,l)$ valued continous process $F$ and an $F*$ stopping time $\tau$ : $$E \max_{0\leqslant t\leqslant\tau}|\int_{0}^{t}F_s dZ_s|^2\leqslant C E \int_0^{\tau}|F_s|^2dQ_s$$ In the proof the Ito isometry holds $E \left(\int_0^{\tau}|F_s|^2dQ_s\right)^2=\int_0^{\tau}|F_s|^2 dt$ I do not even know how $dt$ can be inferred from d $Q$ Question : What is the intuition behind this? Thanks in advance.","['stochastic-analysis', 'probability-theory', 'intuition']"
4742894,Any different way to solve an integral involving a trigonometric ratio?,"Recently I posted a question regarding computation of an interesting definite integral that involved trigonometric functions. Ever since, the topic of trigonometric integrals has quite attracted me, and I decided to sift through some calculus books that I have in search of interesting and challenging problems to solve. Yesterday I stumbled upon one that required me to compute the following integral: $$
\mathcal{I}(0, 2\pi) = \int\limits_0^{2\pi}\frac{\sin(x)+1}{\cos(x) +2}dx
$$ It was quite challenging for me to deal with this integral, so I decided to refer to WolframAlpha, which suggested I use the substitution $u=\tan(x/2)$ to attempt the integral. And it seems like the final answer should be $\mathcal{I}(0, 2\pi)=\dfrac{2\pi\sqrt{3}}{3}$ . Surely, since the function under the integral is defined everywhere in $\mathbb{R}$ (with period $\mathrm{lcm}(2\pi,2\pi)=2\pi$ ) the above substitution proposal seems valid. Yet I wonder whether there is some other “smarter” analytical approach to deal with this integral apart from tangential substitution. Any ideas or suggestions will be greatly appreciated.","['integration', 'trigonometric-integrals', 'definite-integrals', 'contest-math']"
4742899,"Does an undefined derivative always mean a vertical tangent line, and why do we define a tangent line when the derivative is undefined?","I am wondering whether an undefined derivative at a point implies that the tangent line to that point is vertical, and also how the tangent line could still exist if the derivative doesn't exist. For my first question, although I have read that vertical tangent lines will have an undefined derivative, I am wondering whether every single instance of a derivative being undefined at a point means that the tangent line at that point is vertical, and that there isn't another explanation for an undefined derivative. I am confused on this because my Professor intermediately concludes a vertical tangent line when the derivative is undefined, but it seems any situation where the derivative doesn't exist would lead to an undefined derivative, which may not necessarily have a vertical tangent line. For my second question, what is the reason why we regard the tangent line as existing when the derivative doesn't exist when we define the tangent line to a point to have slope equal to the derivative at that point? It seems that if a component of the tangent (its slope) doesn't exist, then the tangent line itself wouldn't exist. UPDATE:
I am now wondering whether the derivative being of the form $a \over 0$ implies that there is a vertical tangent line after reading the answers. Can a derivative of the form $a \over 0 $ eventualize for any other reason that a vertical tangent line?","['calculus', 'derivatives', 'tangent-line']"
4742936,Is Bing's discrete extension space realcompact?,"See here for definition of Bing's space. There's also Dan Ma's blog or Counterexamples in Topology by Steen and Sebach. Since the Michael's closed subspace $Y\subseteq X$ of Bing's space $X$ is metacompact but not paracompact, $Y$ is not countably compact. Therefore, $X$ is not countably compact. Since $X$ is normal, it's not pseudocompact. Is $X$ realcompact? The map $f:X\to \{0, 1\}^{2^\mathbb{R}}$ , $f(x) = x$ , is a continuous bijection. If we were to show that every subspace of $\{0, 1\}^{2^\mathbb{R}}$ is realcompact, then this would imply that $X$ is realcompact. Edit: We know that $Y$ is realcompact from other results (it's a $T_4$ metacompact space of non-measurable size). If we could show that $X$ is subparacompact, or more generally $\theta$ -refinable, then it would follow that $X$ is realcompact. This would hold if we could show that $X$ has a $\sigma$ -locally finite network, for then it'd be a $\sigma$ -space, hence subparacompact (Bing's space H is subparacompact for this reason). Edit 2: Note that the Bing's space H was specifically constructed so all the points in it are $G_\delta$ . This means we probably can't show that Bing's space G has such network after all, since all non-isolated points are not $G_\delta$ . Edit 3: In the literature, $\theta$ -refinable spaces are called submetacompact spaces. Here $X$ is a space which isn't collectionwise normal, so it's possible that it's submetacompact. Edit 4: Another author shows that if in the construction of Bing's space G, the set we use instead of $\mathbb{R}$ is of size $>$ continuum, then space such obtained won't be submetacompact, using some results by Erdos. This doesn't mean above example isn't submetacompact, since we are interested in construction based on taking $\mathbb{R}$ , a set of precisely size continuum. Edit 5: While I'm not certain if the Bing's space constructed from $\mathbb{R}$ is $\theta$ -refinable (or submetacompact), it's weakly $\theta$ -refinable and shrinking. In particular it's countably paracompact. Edit 6: I've missed this before, but $\{0, 1\}^{2^\mathbb{R}}$ is not hereditarily realcompact since $\omega_1$ embedds into it, and $\omega_1$ is not realcompact. It's not Borel-complete either since $\{0, 1\}^{\aleph_1}$ is not Borel-complete. Since $X$ is normal and countably paracompact, if $X$ is Borel-complete then $X$ is realcompact. Since $X$ is not countably compact, $\beta X$ is not Borel-complete. Perhaps one could show $X$ is Borel-complete?","['general-topology', 'paracompactness', 'realcompact-spaces']"
4742942,"""Show that T is an unbiased estimator"" follow-up","Yesterday I posted this question Show that $T$ is an unbiased estimator and with the help of others, I got to understand why $T=\frac{n+1}{2n+1}X_{(n)}$ is an unbiased estimator of $\theta$ , with regard to the PDF $f(x)$ in the question. Following the same reasoning, I proved the second part of the exercise, that $U=\frac{n+1}{5n+4}(2X_{(n)}+X_{(1)})$ is also an unbiased estimator of $\theta$ . So, we have $E[T]=E[U]=\theta$ . Now, onto the thrid part of the exercise, where I need a bit of help. I am asked to show that $Var(T)\geqslant Var(U)$ .
I choosed the straight-forward path (using $Var(X)=E[X^2]-(E[X])^2$ ) ; $$Var(T)\geqslant Var(U) \iff E[T^2]-(E[T])^2 \geqslant E[U^2]-(E[U])^2 \iff E[T^2] \geqslant E[U^2]$$ since $(E[T])^2 = (E[U])^2 = \theta$ . Moving on, $$E[\left(\frac{n+1}{2n+1}X_{(n)}\right)^2] \geqslant E[\left(\frac{n+1}{5n+4}(2X_{(n)}+X_{(1)})\right)^2]$$ $$\iff  \left(\frac{n+1}{2n+1}\right)^2 E[(X_{(n)})^2] \geqslant \left(\frac{n+1}{5n+4}\right)^2 E[(2X_{(n)}+X_{(1)})^2$$ $$\iff \left(\frac{1}{2n+1}\right)^2 E[(X_{(n)})^2] \geqslant \left(\frac{1}{5n+4}\right)^2 (E \left[4(X_{(n)})^2+ 4X_{(1)}X_{(n)}+ (X_{(1)})^2 \right]$$ $$\iff \left(\frac{5n+4}{2n+1}\right)^2 E[(X_{(n)})^2] \geqslant 4E(X_{(n)})^2 ]+ 4E[X_{(1)}X_{(n)} ]+ E[(X_{(1)})^2 ]$$ $$\iff \left( \left(\frac{5n+4}{2n+1}\right)^2 -4\right)E(X_{(n)})^2 ] \geqslant 4E[X_{(1)}X_{(n)} ]+ E[(X_{(1)})^2 ]$$ Now, we know that if $X\geqslant Y$ , then $E[X] \geqslant E[Y]$ . Since $X_{(n)}=max{X_i}, i=1,2,....,n$ , then it is obvious that $E(X_{(n)})^2 ] \geqslant E[X_{(1)}X_{(n)} ]$ and that $E(X_{(n)})^2 ] \geqslant E[(X_{(1)})^2 ]$ (all of the variables are positive, if that matters).
So, $$4E[X_{(1)}X_{(n)} ]+E[(X_{(1)})^2 ] \leqslant 5E[(X_{(n)})^2 ]$$ Therefore, if I manage to show that $$\left( \left(\frac{5n+4}{2n+1}\right)^2 -4\right)E[(X_{(n)})^2 \geqslant 5E(X_{(n)})^2 ]$$ the proof will be over. $$\left( \left(\frac{5n+4}{2n+1}\right)^2 -4\right)E[(X_{(n)})^2 \geqslant 5E(X_{(n)})^2 ]$$ $$\iff \left(\frac{5n+4}{2n+1}\right)^2 -4 \geqslant 5$$ $$\iff \left(\frac{5n+4}{2n+1}\right)^2  \geqslant 9=3^2$$ $$\iff \frac{5n+4}{2n+1} \geqslant 3$$ (the negative solutions are not acceptable, since $n$ is the number of the sample we get, hence positive) $$\iff 5n+4 \geqslant 6n+3 \iff n \leqslant 1$$ Now, clearly that cannot be, in fact I should get exactly the opposite inequality $n \geqslant 1$ . So, two things may be happening; either $a)$ The exercise has a typo, and $Var(T)\leqslant Var(U)$ and not "" $\geqslant$ "", or $b$ ) I have made a mistake, either in my reasoning or in the calculations. Once again, any help would be really helpful, to confirm that the exercise indeed has a typo or to   find out what the mistake is. Thanks in advance (and sorry for the long, long text)!","['statistics', 'variance']"
4743014,Coupon collector's problem with increasing number of coupons needed,"In the ordinary coupon collector's problem, there are $n$ types of coupons, and we pick coupons uniformly at random until we get at least one of each coupon. https://en.wikipedia.org/wiki/Coupon_collector's_problem It is well known that the expected time to collect all coupons $k$ times (for a fixed integer $k$ ) is $ n \log n + (k - 1) n \log\log n + O(n)$ . But what if we need to collect all coupons $n$ times? Is the expected time $\omega(n^2)$ ?","['coupon-collector', 'combinatorics', 'probability']"
4743036,Maximal set of independent random variables on a discrete probability space,"Suppose we have a finite discrete probability space $(\Omega, P)$ (say $|\Omega| = n$ ). What is the maximum number of random variables on this space that are mutually independent? With a bit of work we can find random variables $X_1, \ldots, X_n$ so that $\mathrm{cov}(X_i, X_j) = 0$ for $i \neq j$ . It is not the case, however, that covariance $0$ implies independence. -e- I changed the notation to be consistent with what it seems is more used.","['independence', 'discrete-mathematics', 'probability']"
4743083,Second De Rham Cohomology of $\mathbb{P}^2_\mathbb{R}$,"I'm trying to show that $H^2(\mathbb{P}^2_\mathbb{R})=0$ by pulling back closed 2-forms $\omega$ on $\mathbb{P}^2_\mathbb{R}$ to $S^2$ using the fact that $\pi^*\omega$ is exact (where $\pi:S^2\longrightarrow\mathbb{P}^2_\mathbb{R}$ is the canonical quotient map). If we write $\pi^*\omega=d\eta$ for $\eta\in\Omega^1(S^2)$ , I would like to ""average"" $\eta$ to produce a 1-form $\kappa\in\Omega^1(S^2)$ that descends to a 1-form $\tilde{\kappa}$ on $\mathbb{P}^2_\mathbb{R}$ , akin to my approach in 1 . However, I'm not entirely sure what conditions I need to check to prove that this is the case. In showing the analogous statement for 1-forms, I was able to just define a smooth function $g:S^2\longrightarrow S^2$ by $$g(p)=\frac{1}{2}\left(f(p)+f(-p)\right)=\frac{1}{2}\left(f(p)+(\alpha^*f)(p)\right)$$ if $\pi^*\omega=df$ for $f\in \Omega^0(S^2)$ , and checking that $g(p)=g(-p)$ . Returning to the 2-form case, I first tried to find a 1-form $\kappa$ on $S^2$ such that $\kappa_p(v)=\kappa_{-p}(v)$ , but I'm not even sure this statement makes sense: here, we'd want to regard $v$ as the ""same"" tangent vector in both $T_pS^2$ and $T_{-p}S^2$ . Anyway, I wasn't able to come up with a $\kappa$ that works, assuming this is the ""correct"" thing to check. Are there general conditions would I have to check to ensure this descent works (and similarly, a natural definition for $\kappa$ )? I'm hoping to establish that $H^2(\mathbb{P}^2_\mathbb{R})=0$ using this approach. Thank you for any help. References: 1 : De Rham Cohomology of $\mathbb{P}_\mathbb{R}^2$","['de-rham-cohomology', 'smooth-manifolds', 'differential-geometry']"
4743089,Evaluating the product $\prod\limits_{k=1}^{n}1+W \cos\left(\frac{2k\pi}{n}\right)$,"I am aware that there is a closed form for $\prod\limits_{k=1}^{n} \cos\left(\frac{k\pi}{n}\right)$ , (see Evaluating the product $\prod\limits_{k=1}^{n}\cos\left(\frac{k\pi}{n}\right)$ ). I ran into this sum in a problem with matrix determinants with period elements, and the result is of the form $$\prod\limits_{k=1}^{n}\left(1+ W \cos\left(\frac{2k\pi}{n}\right)\right).$$ I am wondering if there is a closed form for this product, or large $n$ approximations? I was able to find the expressions for $n=3,4,5,...,15$ in Mathematica, but I can't seem to be able to find a clear pattern. $
\frac{1}{4} (W-2)^2,1-W,\frac{1}{16} (W (W+2)-4)^2,-\frac{1}{16} (W-1) \left(W^2-4\right)^2,\frac{1}{64} (W ((W-4) W-4)+8)^2,-\frac{1}{4} (W-1) \left(W^2-2\right)^2,\frac{1}{256} (W-2)^2 \left(W^2 (W+6)-8\right)^2,-\frac{1}{256} (W-1) \left(W^4-12 W^2+16\right)^2,\frac{((W-2) W (W ((W-4) W-20)-8)-32)^2}{1024},-\frac{1}{256} (W-1) \left(3 W^4-16 W^2+16\right)^2,\frac{((W-2) W (W+2) (W (W (W+6)-20)-8)-64)^2}{4096},-\frac{(W-1) \left(W^6-24 W^4+80 W^2-64\right)^2}{4096},\frac{\left(W^3-8 W+8\right)^2 (W (W+2) ((W-10) W+4)+16)^2}{16384},-\frac{1}{256} (W-1) \left(W^2-2\right)^2 \left(W^4-8 W^2+8\right)^2,\frac{(W (W+2) ((W-2) W (W+2) (W (W (W+6)-48)+48)-64)+256)^2}{65536},-\frac{(W-1) \left(W^2-4\right)^2 \left(W^6-36 W^4+96 W^2-64\right)^2}{65536},\frac{(W (W (W ((W-2) W (W (W ((W-8) W-56)+48)+336)-448)+1024)+256)-512)^2}{262144},-\frac{(W-1) \left(5 W^8-80 W^6+336 W^4-512 W^2+256\right)^2}{65536}
$","['infinite-product', 'trigonometry', 'products']"
4743098,The universal property of subspace topology,"It is said in wikipedia , nlab and Terilla that subspace has a universal property. Universal property for the subspace topology. For every topological space $(Z, \tau_Z)$ and every function $f : Z \to Y$ , $f$ is continuous if and only if $i \circ f : Z \to X$ is continuous. Here's a picture $$
\begin{array}{ccc}
{} & {} & X \\
{} & \overset{i \circ f}{\nearrow} & \uparrow i \\
Z & \underset{f}{\to} & Y
\end{array}
$$ One should think of the universal property stated above as a property that may be attributed to a topology on $Y$ . At this point, you may think that some topologies have this property and some do not. Theorem $1$ means that the subspace topology on $Y$ , as previously defined, does have this universal property. Furthermore, the subspace topology is the only topology on $Y$ with this property. Let's prove it. But, as I know, universal property is some sort of 'initial' or 'final' property in another category such as object : $X\xrightarrow f Y, \forall Y, f$ and morphism $\sigma: Y\rightarrow Z$ makes the triangle $X\xrightarrow f Y$ and $X\xrightarrow g Z$ commute, thats to say $g = \sigma f$ . However, I cannot see any universal property in this triangle, since $i\circ f$ is not 'any' morphism, but have connections with what isn't known yet - the morphism $f$ . So, how can we understand this as a universal property ? Or, why a universal property can have connection with 'f'?","['general-topology', 'category-theory']"
4743100,Is $\csc^{-1}(u)=\tan^{-1}\left(\frac{1}{\sqrt{u^2-1}}\right)$ a valid trigonometric identity?,"I thinking about how to write inverse trig functions as each other, eg writing arccsc as some form of arctan, etc. Can anyone tell me if what I did was right? If not why? Here is what I did: Start with $$\csc^{-1}(u)=y$$ so $$\csc(y)=u$$ Draw a right-angled triangle from the acute reference angle $y$ , that has hypotenuse side $u$ and opposite side $1$ . So it follows that the adjacent side is $\sqrt{u^2-1}$ . From this we can see that: $$\tan(y)=\frac{1}{\sqrt{u^2-1}}$$ $$y=\tan^{-1}\left(\frac{1}{\sqrt{u^2-1}}\right)$$ And so we have the identity: $$\csc^{-1}(u)=\tan^{-1}\left(\frac{1}{\sqrt{u^2-1}}\right)$$ Is this identity I derived correct?","['algebra-precalculus', 'solution-verification', 'trigonometry']"
4743139,Can anyone help with seeing if this integral definition can be algebraically proven?,"Recently, I was messing around in Desmos and wanted to see if I could recreate $x^2$ by using an integral. I came across the following integral: $$\int_{0}^{x}\left\lfloor \frac{t}{x}+x \right\rfloor dt$$ When I plug the integral into WolframAlpha with a given $x$ value, it seems to return $x^2$ , and when graphed in Desmos it overlaps the graph of $x^2$ . Geometrically, it makes sense, as the space under the floor function to the $x$ -axis
with those parameters and bounds would just be a square with side lengths $x$ . But I wonder what the algebraic reasoning would look like. I currently can't solve it, but if anyone could help that would be amazing. Thank you!","['integration', 'proof-explanation', 'functions']"
4743159,Partial Differential when a variable is a function of itself,"I am given the following: $$ z = f(\frac{ny-mz}{nx -lz}) $$ And I have to show that $$ (nx-lz) \frac{\partial z}{\partial x} + (ny-mz) \frac{\partial z}{\partial y} = 0$$ The book I am referring to proceeds in the following way: $$ \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot(ny-mz)\cdot\frac{-n}{(nx-lz)^2}$$ But shouldn't it be $$ \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot[(ny-mz)\cdot\frac{-(n -l \frac{\partial z}{\partial x})}{(nx-lz)^2} + \frac{(-m)\frac{\partial z}{\partial x}}{(nx-lz)}]$$ According to my understanding if we are computing the differential change in $z$ wrt $x$ , then it cannot be treated as a constant anywhere it appears in the equation. Am I correct?","['multivariable-calculus', 'partial-differential-equations']"
4743187,"Let $f : \{1, 2, 3, 4, 5, 6, 7\} \to \{1, 2, 3, 4, 5, 6, 7\}$, then number of functions with $f(f(f(x))) =x$. [duplicate]","This question already has an answer here : Counting number of functions such that composition is inverse (1 answer) Closed 11 months ago . Let $f : \{1, 2, 3, 4, 5, 6, 7\} \to \{1, 2, 3, 4, 5, 6, 7\}$ , then number of functions with $f(f(f(x))) =x$ . The right answer comes to be $317$ .  I am unable to solve it.  This question comes from jee advanced preparation test and its solution is not given.","['functions', 'combinatorics']"
4743199,"Finding $2\sin x + 4\sin y$, given $\sin^2x+\cos^2y = \frac{11}{16}$ and $\sin\frac12(x+y) \cos\frac12(x-y) = \frac{5}{8}$","I think my basics are pretty weak, I am not able to solve this question. $$\begin{align}
\sin^2x+\cos^2y &= \frac{11}{16} \tag1 \\[4pt]
\sin\frac12(x+y) \cos\frac12(x-y) &= \frac{5}{8} \tag2\\
\\
\end{align}$$ Find the value of $$2\sin x + 4\sin y$$ (The answer is stated to be $4$ .) Sorry for the horrible $\LaTeX$ . MY APPROACH: I did not focus on simplifying the first equation, so I simplified the second equation by using the formula $2\sin(x)\cos(y) = \sin(x + y) + \sin(x - y)$ . From 2 \begin{align*}
\sin\left(\frac{x + y}{2}\right)\cos\left(\frac{x - y}{2}\right) & = \frac{5}{8}\\
2\sin\left(\frac{x + y}{2}\right)\cos\left(\frac{x - y}{2}\right) & = \frac{5}{4}\\
\sin x + \sin y = \frac{5}{4}
\end{align*} Squaring this we get equation 3, $$\sin^2x + \sin^2y + 2\sin x\sin y = \frac{25}{16}$$ Now I am adding equations 1 and 3 together \begin{align*}
\sin^2x + \sin^2y + 2\sin x \sin y + \sin^2x + \cos^2y & = \frac{25}{16} + \frac{11}{16}\\
\sin^2x + \sin^2y + 2\sin x \sin y + \sin^2x + \cos^2y & = \frac{36}{16}\\
\sin^2x + (\sin^2y + \cos^2y) + 2\sin x \sin y + \sin^2x & = \frac{9}{4}\\
2\sin^2x + 2\sin x \sin y & = \frac{9}{4}
\end{align*} Taking $2\sin x$ as common $$2\sin x(\sin x + \sin y) = \frac{9}{4}$$ But we know the value of $\boldsymbol{\sin x + \sin y}$ from equation number 2. Therefore, \begin{align*}
2\sin x(\sin x + \sin y) & = \frac{9}{4}\\
2\sin x \cdot \frac{5}{4} & = \frac{9}{4}\\
\sin x & = \frac{9}{10} 
\end{align*} Then I put this value in equation 2 and got the value of $\sin y$ as $7/10$ . But when I used these values to find the solution for $2\sin x + 4\sin y$ , I got a wrong answer $16/5$ .",['trigonometry']
4743208,Prove that $AB\cdot CD\geq 2S$.,"Question: Let $M$ be the centroid of triangle $ABC$ . $D$ is a point on the line passing through $A$ and parallel to $BC$ such that $\angle CMD=90^\circ$ . Let $S=[AMCD]$ . Prove that $AB\cdot CD\geq2S$ . This question was the last question from a mock exam a few days ago. To clarify, this exam is contest-style and for 7-8th graders. My approach: I immediately noticed that $2S=AC\cdot MD\sin\alpha$ , where $\alpha$ is the angle between $AC$ and $MD$ . Hence, it suffices to prove that $AB\cdot CD\geq AC\cdot MD$ , since $\sin\alpha\leq1$ . This is then equivalent to $\dfrac{AB}{AC}\geq\dfrac{DM}{DC}$ . However, I am not sure how do approach the problem from here. Any help on solving this using (preferably) synthetic geometry will be appreciated!","['contest-math', 'euclidean-geometry', 'geometry']"
4743217,Finding the height of the building using Trigonometric Ratio.,"Question: At a certain point in a large, level park, the angle of elevation to the top of an office building is ${30}^{\circ}$ . If you move ${400}ft$ closer to the building, the angle of elevation is ${45}^{\circ}$ . To the nearest ${10}ft$ . how tall is the building? My Initial Calculation : For ${30}^{\circ}$ Triangle: $$
\begin{array}{1}
=>\tan 30^{\circ}=\frac{h}{x+400}\\[0.1in]
=>h=0.577(x+400)\\[0.1in]
=>h=0.577x+230.8\\[0.1in]
\end{array}
$$ For ${45}^{\circ}$ Triangle: $$
\begin{array}{l}
=>\tan 45^{\circ} = \frac{h}{x-400}\\[0.1in]
=>\tan 45^{\circ} = \frac{0.577(x+400)}{x-400}\\[0.1in]
=>\tan 45^{\circ} = \frac{0.577x+230.8}{x-400}\\[0.1in]
=>x-400=0.577x+230.8\\[0.1in]
=>x-0.577x=230.8+400\\[0.1in]
=>0.423x=630.8\\[0.1in]
=>x=\frac{630.8}{0.423}\\[0.1in]
=>x=1491.25\\[0.1in]
\end{array}
$$ How tall is Building: $$
\begin{array}{1}
=>\tan 30^{\circ}=\frac{h}{x+400}\\[0.1in]
=>\tan 30^{\circ}=\frac{h}{1491.25+400}\\[0.1in]
=>h=1091.91ft\\
\end{array}
$$ The answer in the book is ${550ft}$ , What wrong did I do in my calculation? New Calculation : For ${30}^{\circ}$ Triangle: $$
\begin{array}{1}
=>\tan 30^{\circ}=\frac{h}{x+400}\\[0.1in]
=>h=0.577(x+400)\\[0.1in]
=>h=0.577x+230.8\\[0.1in]
\end{array}
$$ For ${45}^{\circ}$ Triangle: $$
\begin{array}{l}
=>\tan 45^{\circ} = \frac{h}{x}\\[0.1in]
=>\tan 45^{\circ} = \frac{0.577(x+400)}{x}\because h=0.577(x+400)\\[0.1in]
=>\tan 45^{\circ} = \frac{0.577x+230.8}{x}\\[0.1in]
=>x=0.577x+230.8\\[0.1in]
=>x-0.577x=230.8\\[0.1in]
=>0.423x=230.8\\[0.1in]
=>x=\frac{230.8}{0.423}\\[0.1in]
=>x=545.63\\[0.1in]
\end{array}
$$ How tall is Building: $$
\begin{array}{1}
=>\tan 30^{\circ}=\frac{h}{x+400}\\[0.1in]
=>\tan 30^{\circ}=\frac{h}{545.63+400}\because x=545.63\\[0.1in]
=>h=545.95ft\\
\text{Rounded to the nearest 10}ft\\
=>h=550ft
\end{array}
$$","['triangles', 'algebra-precalculus', 'solution-verification', 'trigonometry']"
4743223,Simplifying Coefficients of a Cubic Polynomial with Complex Roots,"I am currently encountering difficulties while trying to solve the following question, and I would greatly appreciate any assistance you can provide. Let $a,b,c$ be complex numbers. The roots of $z^{3} + a^{2} + bz +c = 0$ are: $z_1,z_2,z_3$ All the roots are non zero. I have to find cubic equation that has the following roots: $\frac{z_1}{z_2}, \frac{z_2}{z_3}, \frac{z_3}{z_1}$ I am aware that I need to apply the fundamental theorem of algebra and utilize symmetric elementary polynomials. Consequently, I have successfully computed the coefficients and exponents: The coefficient of $z^{3}$ is just $1$ The coefficient of $z^{2}$ : $-b = z_1 + z_2 + z_3$ The coefficient  of $z^{1}$ : $c = z_1z_2 + z_2z_3 + z_1z_3$ The coefficient  of $z^{0}$ : $d = z_1z_2z_3$ I have calculated: $q(z) = (z - \frac{z_1}{z_2})(z - \frac{z_2}{z_3})(z - \frac{z_3}{z_1}) = z^{3} + z^{2}(-\frac{z_2}{z_3} -\frac{z_1}{z_2} -\frac{z_3}{z_1}) + z(\frac{z_1}{z_3} +\frac{z_2}{z_1} +\frac{z_3}{z_2}) - 1$ I have explored several approaches in an attempt to simplify the coefficients using symmetric elementary polynomials. However, despite these efforts, I have not been successful in finding an expression that can be solely expressed in terms of the parameters b, c, and d as the coefficients of the new polynomial.","['complex-analysis', 'cubics', 'polynomials', 'complex-numbers']"
4743226,Is the domain of $g(f(x))$ always a subset of domain of $g(x)$?,"Let there be two functions $f(x)=\sqrt{x}$ and $g(x)=\sqrt{2-x}$ . So, $g(f(x))=\sqrt{2-\sqrt{x}}$ . As evident the domain of $g(x)$ is $(-\infty, 2]$ and the domain of $g(f(x))$ is $[0,4]$ . But from what I understand about composite functions, the domain of $g(f(x))$ must be a subset of domain of $g(x)$ . So according to that, the domain of $g(f(x))$ should be $[0,2]$ . Which one is the correct domain of $g(f(x))$ ?","['algebra-precalculus', 'functions', 'function-and-relation-composition']"
4743228,How to derive the jump condition at $x=0$ for a fourth-order ordinary differential equation involving a derivative of the Dirac delta function?,"Let's consider the fourth-order ordinary differential equation (ODE): $$
f^{(4)}(x) - 2 f^{(2)}(x) + f(x) = \delta'(x) , \tag{1}
$$ where $f^{(n)}$ denotes the $n$ th derivative with respect to $x$ . Given that the Dirac delta function is related to its derivative via $x\delta'(x) = -\delta(x)$ , we can rewrite Eq. (1) as [Wikipedia] : $$
x \left[ f^{(4)}(x) - 2 f^{(2)}(x) + f(x) \right] = -\delta(x) . \tag{2}
$$ Our objective is to determine the jump condition at the interface $x=0$ . To obtain the jump condition, we start by integrating both sides of Eq. (2) by parts. This leads to the following expression: $$
\left[ x \left( f^{(3)}-2f^{(1)}+f^{(-1)} \right) + f^{(2)}-2f+f^{(-2)} \right]_{x=-\epsilon}^\epsilon = 1\, ,
$$ where $\epsilon$ is a small positive value, approaching zero. How can we rigorously obtain the jump condition at $x=0$ ? It seems that the discontinuity in the second derivative might play a crucial role, but I'm struggling to establish a concrete connection. Any assistance or guidance on this matter would be greatly appreciated. Thank you very much!","['dirac-delta', 'ordinary-differential-equations', 'real-analysis', 'continuity', 'calculus']"
4743237,"If $p> \sqrt{|G|}$ divides $|G|$, then $G$ contains a normal subgroup of order $p$","Let $G$ be a finite group and let $p$ be a prime dividing the size $|G|$ of $G$ such that $p > \sqrt{|G|}$ .
Prove that $G$ contains a normal subgroup of order $p$ . I tried generalizing the proof of Cauchy's theorem (that there is a subgroup of order $p$ , not necessarily normal), but didn't seem to work. Any help appreciated! (Feel free to use group actions, but please avoid e.g. outsorcing to Sylow theorems, if possible.)","['group-theory', 'normal-subgroups', 'finite-groups']"
4743257,Inverse limit of the group $\mathbb{Q}_p$ of $p$-adic numbers,"I am studying $p$ -adic numbers and inverse (aka projective) limits.
I am interested in characterising the group $\mathbb{Q}_p$ of $p$ -adic numbers as an inverse limit. I already know the inverse system whose inverse limit is $\mathbb{Z}_p$ , but I get confused when negative powers of $p$ are involved, like in $p^{-|n|}\mathbb{Z}_p$ or in $\mathbb{Q}_p$ . I have found the following result in literature (book by L. Fuchs ""Abelian groups"", Section 2.5, pg. 65, Exercise (3)): the group $\mathbb{Q}_p$ is isomorphic to the inverse limit of the inverse system { $A_n (n\in\mathbb{N});\pi_n^{n+1}$ }, where $A_n=\mathbb{Z}(p^\infty)$ denotes the Prüfer $p$ -quasicyclic group and $\pi_n^{n+1}:\mathbb{Z}(p^\infty)\rightarrow \mathbb{Z}(p^\infty)$ is the multiplication by $p$ , for every $n\in\mathbb{N}$ .
I have problems in proving this result, can you help me? And which kind of sequences are there in the inverse limit group? Thank you very much in advance.","['group-theory', 'p-adic-number-theory', 'limits-colimits', 'reference-request']"
4743266,"$a,b,c>0:a+b+c=3.$ Prove that: $\sum\sqrt{\frac{ab+2}{ab+c}}\ge \frac{3\sqrt{6}}{2}.$","Problem. Let $a,b,c>0:a+b+c=3.$ Prove that: $$\sqrt{\frac{ab+2}{ab+c}}+\sqrt{\frac{bc+2}{bc+a}}+\sqrt{\frac{ca+2}{ca+b}}\ge \frac{3\sqrt{6}}{2}.$$ I tried a lot without success. By Holder, $$\left(\sum_{cyc}\sqrt{\frac{ab+2}{ab+c}}\right)^2\sum_{cyc}(ab+c)(ab+2)^2\ge (ab+bc+ca+6)^3,$$ which leads to wrong inequality at $a=b=0.96$ $$\left(\sum_{cyc}\sqrt{\frac{ab+2}{ab+c}}\right)^2\sum_{cyc}(ab+c)(ab+2)^2(a+b)^3\ge (\sum_{cyc}(ab+2)(a+b))^3=(3q-3r+12)^3$$ is not good enough. Also, by AM-GM $$\sum_{cyc}\frac{4(ab+2)}{3(ab+c)+2(ab+2)}=4\sum_{cyc}\frac{ab+2}{3c+5ab+4},$$ which implies $$\sum_{cyc}\frac{ab+2}{3c+5ab+4}\ge \frac{3}{4}.$$ The last inequality is not true when $a=b\rightarrow 1.5$ By homogenizing, we need to prove $$\sum_{cyc}\sqrt{\frac{13ab+4(ca+cb)+2(a^2+b^2+c^2)}{3ab+ca+cb+c^2}}\ge \frac{9\sqrt{2}}{2}$$ Is there an idea called ""isolated fudging"" to prove the OP? I really hope someone share it here. Thank you very much.","['algebra-precalculus', 'holder-inequality', 'inequality']"
4743299,Question Regarding Vershynin's Proof of Bernstein's Inequality,"I have been studying Vershynin's ""High-dimensional Probability,"" and  I have some confusion regarding the proof of Bernstein's inequality (Thm 2.8.2). It concerns the following step: (Perhaps note that $(X_i)_i$ is a finite sequence of independent, mean zero subexponential random variables) By a property of subexponential random variables, we can bound their MGF the following way for $|\lambda| \leq \frac{c}{\max \|X_i\|_{\phi_1}}$ : $$\mathbb{E}[\exp (\lambda X_i)] \leq \exp (C \lambda^2 \|X_i\|_{\phi_1}^2).$$ Thus we get (from a previous step, note $S := \sum X_i$ ) $$ \mathbb{P}(S \geq t) \leq \exp (-\lambda t + C \lambda^2 \sigma^2) \quad (*)$$ where $\sigma^2 = \sum_{i=1}^N \|X_i\|_{\phi_1}^2$ . Now we minimize this expression in $\lambda$ with respect to the constraint and get an optimal choice of $$\lambda = \min \left(\frac{t}{2C\sigma^2}, \frac{c}{\max_i \|X_i\|_{\phi_1}}\right).$$ Thus, with this we obtain $$ \mathbb{P}(S \geq t) \leq \exp \left(-\min \left(\frac{t^2}{4C\sigma^2}, \frac{ct}{2 \max_i \|X_i\|_{\phi_1}}\right)\right).$$ Now, my issue is with this last step. I get how we get the first term in the minimum simply by plugging in the first possible value of $\lambda$ into (*), and I would think to get the second one I just have to plug in the second possible value, but I don't see how I then get the second value... am I missing something? If you want to check the source material, the book is available online for free under https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf","['concentration-of-measure', 'distribution-tails', 'probability-theory', 'probability', 'random-variables']"
4743307,Is $\mathbb{R}^3$ with antipodal points identified a fiber bundle over $\mathbb{R}^2$?,"Consider the topological space arising as the quotient $ X:=\mathbb{R}^3/\sim $ where we identify $ x\sim \pm x $ . Does there exist a locally trivial fibre bundle with base space $ \mathbb{R}^2 $ and total space $ X $ ? The obvious projection $ \pi: X \to \mathbb{R}^2,\ (x,y,z)\mapsto (x,y) $ fails to give a fiber bundle since $ \pi^{-1}(\{p\}) $ is homeomorphic to $\mathbb{R}$ if $ p\neq 0 $ and to $ [0,\infty) $ if $ p=0 $ . I know that $X$ is not a manifold (but $X\setminus\{0\}$ is!)  and hence cannot be the total space of a locally trivial fiber bundle over $\mathbb{R}^2$ with manifold fibers.
But how to rule out any other fibers? I am interested in this space in the context of metric geometry where it is an important example of a ''non-manifold'' but I seem to lack the topological tools to tackle this problem.","['fiber-bundles', 'geometry', 'manifolds', 'general-topology', 'algebraic-topology']"
4743324,Sum of digits of a prime number in base 10,"Define $S(i)$ as the sum of the digits of the $i$ -th prime number $p_i$ . Furthermore, define $$T(i) := \frac{S(i)}{p_i}.$$ Is it true that $T(i) < T(i+1)$ infinitely many times? Response to comment:
I don't have much progression other than assuming by contradiction that there is a number $N$ for which $T(i) > T(i+1)$ for all $i>N$ . Then I want to show that this isn't true, my first idea was using Bertrand & Dirichlet (Bertrand for bounding $p_i$ and $p_{i+1}$ to eachother, and Dirichlet for constructing a prime of the form ending on $00001$ or something to lower its digit sum). I am fairly confident this statement is true, as surely you have primes of the form $10111101013201$ with a very low digit sum, and the next prime being of the form $10111101099999$ for example, with a much higher digit sum and therefore making our statement very probable. I created this problem myself, and context was purely random. I wanted to create an olympiad style problem with digit sum of a prime, but this problem seems too hard for an olympiad. I am assuming we need strong tools.",['number-theory']
4743375,Involutory matrix under prime modulo,"A matrix $A \neq I $ is called involutory modulo $m$ if $A^2≡I (\mod m).$ Prove or disprove that if $A$ is a $2 × 2 $ involutory matrix modulo $m$ , then $ \det(A) ≡
±1 (\mod m)$ . I already disprove this statement but I have doubt about what happens if $m=p$ be a prime. I assume a matrix $A$ is involutory modulo $p$ . Therefore it is the self-inverse matrix now, $\det(A)^2=\Delta ^2=1(\mod p)$ from that I got $\Delta =\pm1 (\mod p)$ Is this corect?","['matrices', 'number-theory', 'elementary-number-theory', 'prime-numbers']"
4743418,How to evaluate $\int_0^1\ln(1-x)dx$,I want to compute $\int_0^1\ln(1-x)dx$ . Using integration by parts we have: $\int_0^1\ln(1-x)dx=\int_0^1(x)'\ln(1-x)dx=x\ln(1-x)|_0^1-\int_0^1\frac{x}{1-x}dx$ but limit at $1$ of $x\ln(1-x)$ is $-\infty$ and $\int_0^1\frac{x}{1-x}dx$ does not converge. I know the answer is $-1$ because desired integral is equal to $\int_0^1\ln(x)dx$ or using integration by parts but with $-(1-x)$ not $x$ yields $-1$ but why first approach does not work and produce false equality?,"['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4743419,Mean value of the smallest selected number if we keep selecting numbers uniformly from 0 and 1 as long as they are decreasing,"I come across a problem that interests me a lot: Select numbers uniformly distributed between 0 and 1, one after one, as long as they keep decreasing: stop selecting when you obtain a number that is greater than the previous one you selected. Q: What is the average value of the smallest number you have selected? It is not that hard to show that the average number to select is e, but what is the average smallest value when stopping?","['expected-value', 'statistics', 'combinatorics', 'probability']"
4743458,Is every ccc space weakly Lindelöf?,"A space is separable if it has a countable dense subset. A space has the countable chain condition (ccc) if every collection of pairwise-disjoint open sets is countable. Finally, a space is weakly Lindelöf if every open cover has a countable subcollection whose union is dense in the space. Every separable space has the ccc. A separable space is also weakly Lindelöf: choose an open set from the cover for each member of the countable dense subset to obtain the desired countable subcollection. This is currently Theorem T129 of the pi-Base. Can T129 be improved to only assume ccc rather than separable?","['general-topology', 'lindelof-spaces']"
4743499,Algebraically why must a single square root be done on all terms rather than individually?,"Let's assume we know that $x+9=10$ .
I understand this is illegal: $$\sqrt[]{x} + \sqrt[]{9} = \sqrt[]{10}.$$ And this is correct: $$\sqrt[]{x + 9} = \sqrt[]{10}.$$ Is there an intuitive way to understand why this must be the case?","['sums-of-squares', 'algebra-precalculus', 'radicals', 'intuition']"
4743515,"Let $f$ be continuous function on $[a,b]$ and $g$ be Riemann integrable on [a,b], then the integral $fdg$ exists. [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 11 months ago . Improve this question Prove or disprove: Let $f$ be continuous function on $[a,b]$ and $g$ be Riemann integrable on [a,b], then the integral $fdg$ exists. My motivation to problem is studying the connections between the Riemann Integration and Riemann-Stieltjes integration, and trying to put conditions.","['analysis', 'real-analysis']"
4743517,Gradient of $x'\left(\sum_i x_i A_i\right)^{-1}x$,"I want to compute the gradient of $$f(x)=x'\left(\sum_i x_i A_i\right)^{-1}x$$ where $x$ is a $n\times 1$ column vector and $A_i$ is a square matrix. One option is to differentiate with respect to a single $x_j$ and then reconstitute the gradient but this gets very messy. The other option is to use matrix calculus rules. One can write $$ C:=\sum_i x_i A_i = (x'\otimes I_n)B$$ where $B$ is a block column vector: $B=\left[A_{1},\dots,A_{n}\right]'.$ Then using the rules for differentiating a matrix inverse $$
df=2C^{-1}xdx-x'C^{-1} \left(dC\right) C^{-1}x.
$$ Finally, $$
dC=(dx'\otimes I_n)B.
$$ But I am stuck there. How can I move from this to an expression for $\frac{df}{dx}$ ? EDIT: Corrected some mistakes that commenters noticed.","['matrices', 'multivariable-calculus', 'calculus', 'matrix-calculus']"
4743547,"Generating random, non-negative unit weight vectors.","Put it simply, how do I generate uniformly randomly a vector $w = (w_1,w_2,w_3)$ such that $w_1+w_2+w_3 = 1$ and $w_i\geq 0$ ? Essentially, I want the same thing as in Uniform distribution on the surface of unit sphere , except in the $L_1$ norm and also in the positive quadrant. Note I need this for a simulation, so a direct reference to a python library would work. Also, I decided it's better to post it here after searching on stackoverflow - even the more natural $L^2$ norm version of the question was better answered here. I am tempted to do the naive $X_1,X_2,X_3\sim Unif[0,1]$ i.i.d and then normalize by the $L^1$ norm, but the above answer for $L^2$ says this only works for $L^2$ if I do normal random variables, so I am a bit unsure.","['statistics', 'probability-distributions', 'uniform-distribution', 'probability']"
4743570,"What is the least number of concerts needed to be scheduled in order that each musician may listen, as part of the audience, to every other musician?","Six musicians gathered at a music festival. At each concert
some musicians played in the concerts while the others listened, as part
of the audience. What is the least number of concerts needed to be scheduled in order that each musician may listen, as part of the audience, to
every other musician? I have managed to find that $4$ concerts works if we let the players be $A,B,C,D,E$ and $F$ , then $A,B,C$ playing first, $B,E,F$ second, $C,D,E$ thirdly and $A,D,F$ last is a working construction. Now the issue I'm having is that I cannot prove that the cases with $3,2$ and $1$ concerts are not possible. Well the first one is easy since no two players are able to hear each other out ever, but how can I argue for the cases $3$ and $2$ ?","['contest-math', 'combinatorics']"
4743582,Tricky trig integral [duplicate],"This question already has answers here : Integrating $\int^0_\pi \frac{x \sin x}{1+\cos^2 x}$ (4 answers) Closed 11 months ago . I want to know how one should evaluate the following integral: $$\int_{0}^{\pi} \frac{x\cdot\text{sin}(x)}{1+\text{cos}^2(x)} dx$$ It doesn't lend itself to substitution or integration by parts; I also haven't been able to find any ""cheap shots"" using properties of odd and even functions. I tried using Feynman's trick with it, but it doesn't seem very helpful here (where to introduce the arbitrary variable other than in the power on cosine?). I want to know how to solve this integral, but also more broadly, if there is any transferrable skill/trick I can learn from it. I am also interested in how one can discern which integrals are good candidates for Feynman's trick.","['integration', 'trigonometric-integrals', 'definite-integrals']"
4743591,Weakly Lindelöf metrizable spaces are separable,"In Handbook of set-theoretic topology (Kunen & Vaughan, 1984), chapter 1 about Cardinal functions, Theorem 8.1 states that many of the cardinal functions are equal in the case of metrizable spaces.  Among them, $w(X) = d(X) = wc(X)$ , where: $w(X)$ is the weight of $X$ , $d(X)$ is the density of $X$ , $wc(X)$ is the weak covering number of $X$ , that is, the smallest infinite cardinal $\kappa$ such that every open cover of $X$ has a subcollection of cardinality $\le\kappa$ whose union is dense in $X$ . The corresponding special names when these cardinal functions are countable are: $X$ is second countable $X$ is separable $X$ is weakly Lindelöf In particular, a metrizable space is second countable iff it is separable iff it is weakly Lindelöf. For the proof, the equality $w(X)=d(X)$ for metrizable spaces is well-known. For the weak covering number, it is easy to see that the inequality $wc(X)\le w(X)$ holds in any topological space. For the reverse, the Handbook's Theorem 8.1 leaves the verification to the reader.  Can anyone provide a proof?","['general-topology', 'lindelof-spaces']"
4743593,"Is the approximation $\cos\frac23x \approx \frac12(1 + \cos x)$, for $x$ a first-quadrant angle, well known?","Please comment on whether the following approximation is well known $$\cos\frac23x \approx \frac12(1 + \cos x)$$ in the first quadrant, with a maximum error of $0.016$ .","['trigonometry', 'approximation']"
4743602,Area of $x^{10}+y^{10}\leq 1$,"Whilst looking at someone's vector calculus problem, they mentioned that, making use of Green's Theorem, they had to express the line integral of the boundary of $x^{10}+y^{10}\leq 1$ in terms of its area. The thing is they gave you the area as computed with Mathematica to be $4\Gamma^2(11/10)/\Gamma(6/5)\approx 3.943.$ And I was wondering how you'd prove this: $$\mathcal{A}=\iint_{x^{10}+y^{10}\leq1}dydx=2\int_{-1}^1\sqrt[10]{1-x^{10}}dx=\dfrac{4\Gamma^2(11/10)}{\Gamma(6/5)}.$$","['integration', 'multivariable-calculus', 'calculus', 'area']"
4743629,Prove that $\overline{DP} \cong \overline{ME}$ in $\triangle ABC$,"Let $\triangle ABC$ be isosceles with $\overline{AB} \cong \overline{AC}$ and altitudes $\overline{AD}$ , $\overline{BE}$ , and $\overline{CF}$ intersecting at $H$ , the orthocenter. $\bigcirc G$ , of diameter $CE$ , intersects $\overline{BC}$ and $\overline{CF}$ at $M$ and $N$ , respectively. $\overrightarrow{MN}$ intersects the altitude $\overline{AD}$ at $P$ . Prove that $\overline{DP} \cong \overline{ME}$ . My Diagram So far, I have found that $\angle CME$ is right, because it's an inscribed angle. Geogebra says that $MN=MC$ , but I am not sure why. I think the proof might involve the similar $\triangle AFE$ , then proving that $FE \parallel BC$ , but I really don't know how to get there. Any help would be really appreciated.","['euclidean-geometry', 'geometry']"
4743643,Is there an easy way to determine the order of the zero $z = 0$ of the function $f(z) = e^{\sin(z)} - e^{\tan(z)}$?,"I've found two ways to determine the order of the zero $z = 0$ of the function $f(z) = e^{\sin(z)} - e^{\tan(z)}?$ Both ways, described below, are unsatisfactory insofar as I need help either from a calculator like Wolfram Alpha or from a collection of mathematical formulas. (1) According to Wolfram Alpha the first and second derivatives of f evaluated at z = 0 are zero, while the third derivative is non-zero. Hence the zero has order three. (2) In a collection of mathematical formulas I've found the first few terms in the Maclaurin expansion of the tangent function. I've used that together with the easy to remember Maclaurin expansions of sin and exp, and have concluded that the Maclaurin expansion of f probably starts with a cubic term, which would mean that the zero has order three, a result which agrees with what we got with method (1). If I got a problem like this on an exam, I would be in big trouble since I wouldn't have access to any aids like the ones I've mentioned. It's not possible to calculate all of the first three derivatives of f by hand since repeated applications of the quotient rule would be necessary, which would be just too cumbersome. Also the Maclaurin expansion of tan is not an expansion which you could memorize due to the weird coefficients without a recognizable pattern. Is there an easy way to find the order of the zero z = 0 of f by hand, without any aids? I haven't been able to find one. I would like to add that I don't find it possible to calculate by hand the first few terms in the Maclaurin expansion of tan, since also that would require cumbersome, repeated applications of the quotient rule.",['complex-analysis']
4743683,Does convergence in distribution imply convergence in distribution of conditional expectation?,"Suppose that $X_N,X$ are defined on $(\Omega,\mathcal F, P)$ and that $X_N\to X$ in distribution. Assume $X_N,X$ are integrable. Let $\mathcal G$ be a sub sigma algebra. Suppose that we know $E[X_N\mid\mathcal G]$ converges in distribution. Does this imply that $E[X_N\mid\mathcal G]$ converge in distribution to $E[X\mid\mathcal G]$ ? It seems like an intuitive claim but I am struggling to prove it or find a counterexample. I know that if $A,B$ are continuity sets of $X$ then \begin{align}
P(X_N\in A\mid B)&=P(X_N\in A\cap B)/P(X_N \in B)
\\&\to P(X\in A\cap B)/P(X\in B)
\\&=P(X\in A\mid B).
\end{align} Can this conclude?","['conditional-expectation', 'convergence-divergence', 'probability-theory', 'weak-convergence']"
4743691,Are there any nice expressions for $\int_0^\infty e^{-x^2}\sqrt{x^2-k^2}\ \mathrm{d}x$?,"In some applied mathematics (ocean modelling) I was doing I came across the integral $$I(k)=\int_0^\infty e^{-x^2}\sqrt{x^2-k^2}\ \mathrm{d}x,$$ where $k\geq 0$ is a constant that depends on the parameters (e.g. flow speed) I wish to use. I was wondering whether anyone knows of a neat expression for this integral, even in terms of Bessel functions? I'm particularly interested in the real part of this integral, that is $$\Re(I(k))=\int_k^\infty e^{-x^2}\sqrt{x^2-k^2}\ \mathrm{d}x.$$ I was hoping for a solution similar this question but have had no luck. An asymptotic expression (for large $k$ ) for $I(k)$ or $\Re(I(k))$ would also be really useful.","['integration', 'definite-integrals', 'asymptotics', 'calculus', 'bessel-functions']"
4743736,Prove $\sum\limits_{\mathrm{cyc}} \sqrt{5a+5b+8ab}\ge 3\sqrt{2}+2\sqrt{5}$ for $ab+bc+ca=1$,"Let $a,b,c\ge 0: ab+bc+ca=1.$ Prove that $$\sqrt{5a+5b+8ab}+\sqrt{5c+5b+8cb}+\sqrt{5a+5c+8ac}\ge 3\sqrt{2}+2\sqrt{5}.$$ The equality case is $(0,1,1)$ but if we set the point $\left(\dfrac{\sqrt{3}}{3},\dfrac{\sqrt{3}}{3},\dfrac{\sqrt{3}}{3}\right),$ the $LHS-RHS \approx 0.$ I tried to square both side and obtain $$10(a+b+c)+8+2\sum_{cyc}\sqrt{5a+5b+8ab}\sqrt{5c+5b+8cb}\ge (3\sqrt{2}+2\sqrt{5})^2,$$ but I have no clue to work with the yield $\sum_{cyc}\sqrt{5a+5b+8ab}\sqrt{5c+5b+8cb}.$ I also tried to use Holder inequality $$(LHS)^2.\sum_{cyc}(5a+5b+8ab),$$ which did not help well. Does mixing variables technique help here? I hope we can find some brighter ideas. Thank you for your interest. Updated edit. We got some answers and progresses which seems not simple. The nice proof is teasing with us, isn't it ?","['contest-math', 'algebra-precalculus', 'holder-inequality', 'inequality']"
4743815,Measure spaces that stabilize with respect to the Carathéodory extension,"When learning measure theory, for sure one encounters the Carathéodory extension theorem with extends a premeasure defined over a semiring to a measure defined over a $\sigma-$ algebra. Let $X$ be a set, $\mathcal{R}$ be a semiring over $X$ and $\mu$ be a premeasure defined over $\mathcal{R}$ . For me, a natural question is: what if we apply the Carathéodory extension a second time to the measure space $(X,\mathcal{R}',\mu')$ itself obtained by the Carathéodory extension of $(X,\mathcal{R},\mu)$ ? Fortunately, the result stabilizes, because the outer measure induced by $\mu$ and $\mu'$ are the same: for any subset $S\subset X$ we have $$\mu^*(S)\overset{\operatorname{def}}{=}\inf\left\{\sum^{\infty}_{n=1}\mu(A_n):A_n\in\mathcal{R},\bigcup^{\infty}_{n=1}A_n\supset S\right\}=\inf\{\mu'(A):A\in\mathcal{R}',A\supset S\},$$ where $\mu^*$ is the outer measure induced by $\mu$ . Clearly we have "" $\ge$ "" since $\displaystyle\bigcup^{\infty}_{n=1}A_n\in\mathcal{R}'$ and that $\displaystyle\sum^{\infty}_{n=1}\mu(A_n)\ge\mu'\left(\displaystyle\bigcup^{\infty}_{n=1}A_n\right)$ . For "" $\le$ "", note that $\mu'(A)=\mu^*(A)\ge\mu^*(S)$ for every $A\in\mathcal{R}'$ . So I would like to ask: is there a name designed for the measure spaces with the property ""unable to be further extended by the Carathéodory process""? Are there any easy criteria to determine whether a measure space has such property?","['measure-theory', 'outer-measure']"
4743861,Exercise 3 point (g) in Stanley Combinatorics volume 1 chapter 1,"I'm having a hard time understanding the solution of the exercise 3 point (g) of Stanley Combinatorics volume 1 chapter 1. This exercise basically asks to prove in a combinatorial way the following equality $$\sum_{k=0}^n \binom{n}{k}^2 x^k=\sum_{j=0}^n \binom{n}{j}\binom{2n-j}{n}(x-1)^j.$$ The author's solution is the following: The LHS term is the number of ways to choose a triple $(S,T,f:S\to [x])$ such that $S\subseteq [n]$ , $T\subseteq [n+1,2n]$ and $|S|=|T|$ . Let $\mathcal{I}$ be the set of such triples. The RHS term is the number of ways to choose a triple $(A,B,g:A\to [x-1])$ such that $A\subseteq [n]$ and $B$ is a $n$ -subset of $[2n]-A$ . Let $\mathcal{J}$ be the set of such triples. I agree with these observations. Then the author constructs a bijection between $\mathcal{I}$ and $\mathcal{J}$ as follows $$(S,T,f)\mapsto (f^{-1}[x-1],([n]-S)\cup T,f|_{f^{-1}[x-1]}).$$ I agree that this is a well defined map, but after wrapping my mind around this a little bit I couldn't construct the inverse map or prove that this map is a bijection and the author doesn't justify this any further. Could you help me? Starting from $(A,B,g)$ if we could extrapolate $S$ , then finding $T$ and $f$ would be easy, because $T$ would be simply $B-([n]-S)$ and $f$ would be simply the map such that $f|_A=g, \ f|_{S-A}=x$ . I know that I should use the fact that $|S|=|T|$ but I don't know how. This exercise just feels artificial and convoluted to me.","['summation', 'combinatorics', 'discrete-mathematics']"
4743864,Evaluating $\int_1^e\left\{\left(\frac{x}{e}\right)^{2x}-\left(\frac{e}{x}\right)^x\right\}\ln x\;dx$,"How to evaluate this integral? $$\int_1^e\left\{\left(\frac{x}{e}\right)^{2x}-\left(\frac{e}{x}\right)^x\right\}\ln x\;dx$$ I am trying this question by substituting $\frac{x}{e} =t$ , and then $\frac{e}{x}$ will be $\frac{1}{t}$ . So I am getting the integral to be $$\int\left(t^{2x} -\frac{1}{t^x}\right)(1+\ln t)e\;dt$$ But how to approach further?","['calculus', 'definite-integrals']"
4743879,How to prove that the local minimum for $f(x)=\left|x\right|$ occurs at $x=0$?,"The first and second derivative tests don't work as $f\left(x\right)$ is not differentiable at $x=0$ . So, critical points can't be obtained to check for local maxima or minima or inflection. Plotting the graph for $f\left(x\right)=\left|x\right|$ does show that $f'\left(x\right)$ or $\frac{df\left(x\right)}{dx}$ goes from decreasing to increasing as $f\left(x\right)$ passes $x=0$ . Thus, $x=0$ can be termed as the point of local minimum. However, can this be done through another method in addition to curve sketching just like the first and second derivative tests?","['maxima-minima', 'derivatives']"
4743917,Proving that Coefficients of Homogeneous Linear Differential Equations have to be constant if $f^{\prime}$ and $f$ are solutions,"Let $a_0$ and $a_1$ be differentiable functions such that for each solution $f$ of the homogeneous linear differential equation $$
y^{\prime \prime}+a_1 y^{\prime}+a_0 y=0
$$ also $f^{\prime}$ is a solution. Then $a_0$ and $a_1$ are constant. Assuming $f$ is a solution, then $$
f^{\prime \prime}+a_1 f^{\prime}+a_0 f=0
$$ and $$
(f^{\prime})^{\prime \prime}+a_1 (f^{\prime})^{\prime}+a_0 (f^{\prime})=f^{\prime \prime}+a_1 f^{\prime \prime}+a_0 f^{\prime}=0.
$$ Hence we have got $$
f^{\prime\prime \prime}+(a_1-1)f^{\prime\prime}+(a_0-a_1)f^{\prime}-a_0f=0.
$$ and $$
f^{\prime\prime \prime}+a_1(f^{\prime\prime}-f^{\prime})+a_0(f^\prime-f)=0.
$$ But I do not know how this can be helpful. Any hint would be appreciated.",['ordinary-differential-equations']
4743924,Uniqueness of the root for a DoG function (Difference of Gaussian),"I am struggling with the following problem: Let $f$ be a real function such that: $f\in\mathcal{C}^\infty(\mathbb{R},\mathbb{R})$ , $f$ is strictly convex on $(-\infty,0)$ , strictly concave on $(0,\infty)$ , strictly increasing on $\mathbb{R}$ , (exponential growth ( $f(x) = o(e^{D|x|})$ )? just to suppose the following integral well defined). Let $a_1>a_2>0$ two real numbers. Can one prove (or give a counter-example to) the following statment: The function $g$ defined by $$g(x) := \int_\mathbb{R}\left(f(x+a_1s)-f(x+a_2s)\right)e^{-s^2/2}ds$$ has a unique $0$ on $\mathbb{R}$ . I am (numerically) convinced that the statement is true. Any hint, counter-example or help will be highly appreciated ! Thank you very much. Numerical example Here a picture showing the function $f = \arctan$ and the function $g$ . The difficulty is that $g$ is not increasing function... Note that the function $f$ is not necessarly odd. Possible hints The function $g$ can be as a DoG (difference of gaussians) function: $$g = f*(G_{a_1}-G_{a_2}).$$ DoG is known to be a possible approximation of gaussian laplacian, which is a smoothed version of the Laplacian. If there is one inflection point, we could imagine that the DoG function will have one zero point. One way to proceed is to show two points: If the Gaussian of Laplacian of $f$ (denoted $\text{LoG}(f)$ ) has two zeros, the Laplacian of $f$ $\Delta f$ has two zeros. ( This point is already proved ) If the $\text{DoG}$ function has two zeros, $\text{LoG}(f)$ must have two zeros. (This has to be proved).
If one manage to prove the second point, the result follows by contradiction (single 0 of $\Delta f$ ).","['integration', 'convex-analysis', 'convolution', 'real-analysis']"
4743937,Does $f=O(g)$ and $g=O(f)$ implies $\lim_{x\to\infty}f(x)/g(x)$ exists?,"Let $f,g:[0,\infty)\to [0,\infty)$ be continuous function, and recall that $f=O(g)$ if there exists $C>0$ such that for sufficiently large $x$ , we have $f(x)\leq Cg(x)$ , and define $g=O(f)$ similarly. I found a statement that says There exists $L\in \mathbb{R}, |L|<\infty$ such that $\lim_{x\to\infty}f(x)/g(x)=L$ if and only if $f=O(g)$ and $g=O(f)$ . Is this statement correct? It is easy to see that $\lim_{x\to\infty}f(x)/g(x)=L$ implies $f=O(g)$ and $g=O(f)$ . What about the other implication? Is it wrong or is it provable? If provable, I would appreciate any comments on how to prive it.","['limits', 'calculus', 'asymptotics', 'analysis']"
4743944,Are all bivariate polynomials of degree < 7 non-injective on rational numbers?,"In one of Alon Amit's interesting answers on the Quora website, he mentioned Don Zagier's conjecture that the bivariate polynomial $x^7 + 3y^7$ may be injective on rational numbers, that is, no two distinct pairs of rational values of $(x,y)$ produce the same value of $x^7 + 3y^7$ . Amit also referred to Bjorn Poonen's paper ""Multivariable polynomial injections on rational numbers"". The choice of this particular polynomial by Zagier for this conjecture makes me curious: What about even simpler polynomials with smaller exponents? Presumably they are expected to be non-injective on rational numbers, if Zagier chose $x^7 + 3y^7$ in particular as a candidate to be injective. But is there a proof that all polynomials of the forms $$
ax^3 + by^3,\\
ax^3 + by^5,\\
ax^5 + by^5,\\
ax^3 + by^7,\\
ax^5 + by^7
$$ are non-injective on rational numbers? Or at least, for small values of $a,b < 10$ , are there known distinct pairs of rational values of $(x,y)$ showing that each such polynomial is non-injective? Certain such polynomials have trivial or extremely simple examples showing that they are non-injective on rational numbers, such as $x^3 + 7y^3$ : $(1,1)$ and $(2,0)$ produce identical values. Also, any polynomial of the form $x^n + 2y^n$ , where $n$ is odd, has the trivial pair of rational values $(1,0)$ and $(-1,1)$ producing identical values. (Indeed any pair $(k,0)$ and $(-k, k)$ produce identical values of such polynomials). A less trivial example is $x^3 + 3y^3$ , for which $(3,-1)$ and $(0,2)$ both produce the value of $24$ . But other such polynomials do not appear to have such simple non-injective solutions. For example, what is a pair of rational values of $(x,y)$ that produce identical values of $x^5 + 3y^5$ ? By the way, this is equivalent to finding a sum or difference of two fifth powers of integers that is exactly 3 times another such sum or difference of two fifth powers. Likewise, Zagier's conjecture that $x^7 + 3y^7$ is injective on rational numbers is equivalent to the claim that there does not exist any sum or difference of two seventh powers that is exactly 3 times another sum or difference of two seventh powers. An example of a 5th degree polynomial with relatively small coefficients that I can show to be non-injective on rationals in a non-trivial way is $3x^5 + 22y^5$ , for which $(1,1)$ and $(3,-2)$ both produce the value of $25$ . Even better examples with very small coefficients are $x^5 + 4y^5$ , for which $(1,4)$ and $(5,3)$ both produce the value of $4097$ , and $x^5 + 8y^5$ , for which $(1,4)$ and $(-7,5)$ both produce the value of $8193$ . But it does not appear to be a simple matter to find such polynomials, and I imagine it would be quite difficult to show that all such 5th degree bivariate polynomials are non-injective on rationals. The existence of the simple examples for 5th degree polynomials cited above is related to the fact that all fifth powers of integers are equivalent to 1, -1, or 0 modulo 11. This creates numerous small ratios of the many sums and/or differences of fifth powers which have 11 as a common factor: $2^5 + 1^5$ , $3^5 - 1^5$ , $4^5 - 3^5$ , $5^5 - 1^5$ , $5^5 - 4^5$ , $7^5 + 1^5$ , etc. Perhaps this is one reason that Zagier did not conjecture that a simple 5th degree bivariate polynomial is injective on rationals. It appears that the sums and differences of seventh powers do not share nearly as many common factors. Some of them have the common factor 29, since all seventh powers of integers are equivalent to 1, -1, 12, -12, or 0 modulo 29, and some have the common factor 43, since all seventh powers are equivalent to 1, -1, 6, -6, 7, -7, or 0 modulo 43. But these common factors do not appear to be dense enough to produce small ratios akin to those of the sums and differences of fifth powers. Update: Robert Israel has found the following examples showing that certain additional 5th degree polynomials of the form $x^5 + by^5$ are non-injective on rational numbers: $11^5 + 7^5 = 177858 = 6 * (8^5 - 5^5)$ $2698^5 + 1052^5 = 144246898755300000 = 16 * (1685^5 - 1355^5)$ $23^5 + 11^5 = 6597394 = 17 * (15^5 - 13^5)$ $59^5 + 17^5 = 716344156 = 19 * (39^5 - 35^5)$ $65^5 - 63^5 = 167854082 = 22 * (22^5 + 19^5)$ $131^5 - 116^5 = 17576073075 = 25 * (59^5 - 26^5)$ The lack of such examples with 5th powers up to $3000^5 = 243000000000000000$ (243 quadrillion) for $x^5 + 3y^5$ , $x^5 + 5y^5$ , $x^5 + 7y^5$ , and multiples of $3, 5, 7$ except for $3*2 = 6$ and $5^2 = 25$ remains striking.","['algebraic-geometry', 'polynomials', 'rational-numbers']"
4743970,Exercise 10 in Stanley Combinatorics volume 1 chapter 1,"I'm again having some trouble understanding a solution of an exercise of chapter 1 of Stanley Combinatorics volume 1. It's the exercise 10 that requires to prove that the number of ways we can choose a subset of $[2n]$ satisfying the following properties: It contains exactly $s$ even elements and $r$ odd ones, It doesn't contain consecutive elements, is $\binom{n-r}{s}\binom{n-s}{r}$ . The author states that we can construct a bijection between the valid subsets and the multisets that contain exactly $s$ even elements and $r$ odd elements (counted with multiplicity) of the set $[2(n-r-s+1)]$ . One direction of the bijection is stated clearly: given a valid subset $S=\left\{a_1<...<a_{r+s}\right\}$ we define its associated multiset as $$\{a_1,a_2-2,a_3-4,...,a_{r+s}-2(r+s-1)\}.$$ Then the author essentially says that an inverse can be constructed. How?","['multisets', 'combinatorics', 'discrete-mathematics']"
4743971,Statistics of list permutations - expected similarity of rankings?,"I created a website where users create rankings, so for each user I have an ordered list of distinct elements $[e_1, e_2, ..., e_n]$ I calculate the similarity of two users by summing up the absolute index differences for each element and dividing the result by the maximum possible total difference ( $ \lfloor n^2/2 \rfloor $ ), then subtracting that from one. Users with similarity 1 have the exact same ranking. Now it seems that for most users the average difference to all other users isn't 50%, but below that. I calculated the difference for all permutations for $n=10$ from one specific ranking and got this result: and this seems to confirm this phenomenon - assuming that user rankings (element permutations) are random, the average similarity to other users is indeed less than 50%. But why? What kind of distribution is this and what is its most common value for a given $n$ ? Since with $n$ elements, there are $n!$ possible permutations, it isn't possible to ""brute-force-plot"" this for large n, unless (randomly) sampling a subset which I've done here:","['permutations', 'statistics']"
4744065,Ladder Operations and Rodriguez formula,"During my attempt to prove the Rodriguez Formula for Hermite Polynomials by using the Ladder Operators, $H_n(x) = (-1)^n e^{x^2}\frac{d^n}{dx^n}e^{-x^2}$ , I arrived to the formula $H_n(x) =  e^{x^2/2}(-\frac{d}{dx}+x)^ne^{-x^2/2}$ .Where $(-\frac{d}{dx}+x)^n$ represents the operator being applied n times. Now I have no idea on how to prove these two to be equivalent. Any hints or solutions would be appreciated","['hermite-polynomials', 'operator-theory', 'derivatives']"
4744138,Value of the infinite series $\sum_{n= 0}^{\infty}\frac{(-1)^n}{(2n+1)(n+1)}$ [duplicate],"This question already has an answer here : Find the sum of series $\displaystyle \sum_{n=0}^{+\infty}\frac{(-1)^n}{2(n+1)(2n+1)}$ (1 answer) Closed 11 months ago . I attempted to evlaute the integral $I=\int_1^\infty \log(1+\frac{1}{x^2})dx$ by using the series expansion of log which gave me the following series as an answer. $\sum_{n= 0}^{\infty}\frac{(-1)^n}{(2n+1)(n+1)}$ having no real experience in evaluating such series, I compared this result with $I$ on MATLAB and concluded that they both converged to $\frac{\pi}{2}-$ log $2$ . Despite my lack of experience, I gave it a try anyways. Here is how my attempt went: I began by expressing the sum in two parts. $$a_n=\frac{(-1)^n}{(2n+1)(n+1)}$$ so that $S=S_1+S_2$ where $$S_1=a_1+a_3+a_5+\cdot\cdot\cdot \space \space and  \space S_2 =a_0+a_2++a_4+\cdot \cdot \cdot $$ which would imply $S_1=\sum_{n=0}^{\infty}\frac{1}{(4n+1)(2n+1)}$ and $S_2=-\sum_{n=0}^{\infty}\frac{1}{(4n+3)(2n+2)}$ here, I've replaced $n$ by $2n$ in the original series to get $S_1$ and $2n+1$ to get $S_2$ . Then, I broke these series up into two pieces each by using partial fractions. $$S=s_1+s_2+s_3+s_4$$ where the series denoted by $s_i$ are given by the corresponding terms of the sum $$\sum_{n\ge0}\frac{1}{4n+1}-\frac{1}{2n+1}-\frac{2}{4n+3}+\frac{1}{2n+2}$$ In theory, I should be able to compute all four of these sums by making use of the geometrics series. But somehow that didn't work out.My question is: What went wrong with my solution? Are there any other more practical and elegant solutions to this problem as well?","['definite-integrals', 'sequences-and-series', 'real-analysis']"
4744187,A proof to Holder Inequality,"I'm  trying to proof Holder Inequality in metric spaces context. Here, we are in the $l^p$ space, every $x=(x_i)$ is a sequence such that $\sum |x_i|^p $ converges. The metric is given by $$d(x,y)=\left( \sum |x_i-y_i|^p \right) $$ First, I prooved the Young inequality. Set $p\geq 1$ , q is defined as $\frac{1}{p}+\frac{1}{q}=1$ then, for $\alpha,\beta$ positive real numbers, we have $\alpha \beta\leq\frac{\alpha^p}{p}+\frac{\beta^q}{q}$ No problem with that. Setting $\alpha=|x_i|$ and $\beta=|y_i|$ we have $$ |\sum x_iy_1|\leq \frac{1}{p}+\frac{1}{q}=1$$ if $\sum |x_i|^p=\sum|y_i|^q=1$ . I'm stucked here. How can I define a sequence in that space that converges, but the value is not 1 and get the Holder Inequality? Thanks for the help.","['holder-inequality', 'functional-analysis']"
4744218,What is the standard deviation of the distribution,"Suppose that you have a uniform distribution in the Interval $I_0$ where $$ I_0 \in [0,1] $$ With this as a starting interval, now you take another interval $I_1$ which is a subset of $I_0$ but is exactly half the length. You repeat this multiple times. So if your interval $m = n-1$ , then $$ I_n \subset I_m $$ and the length of interval $n$ is half to that of interval $m$ All the intervals are continuous. Question -> If $n$ tends to infinity, the interval will converge on a point. Now if you repeat this experiment infinitely many times, you will get infinite such points, which will form a distribution. What is the standard deviation of that distribution? Note - Since $I_0$ is of length 1, $I_1$ needs to be of length 0.5. So $I_1$ cannot start from (0.5,1],  as that would mean that $I_1$ will not be subset of $I_0$ . All the possible selections of $I_1$ are equally likely. So, choosing [0.25,0.75], [0.2,0.7],[0,0.5]  etc are all equally likely candidates for $I_1$","['limits', 'normal-distribution', 'standard-deviation']"
4744225,Integrating a multidimensional minimum function?,"Back some time ago, a friend challenged me to find this integral $$I = \int_0^1\int_0^1\cdots\int_0^1\min(x_1+x_2+\cdots+x_n, 1)\text{ d}x_1\text{ d}x_2\cdots\text{ d}x_n$$ I started off by trying to find a pattern by solving smaller integrals, which led me to find that each integral is basically the multidimensional ""volume"" of a $n$ -dimensional ""cube"" with side length $1$ , except with a small region sliced off, and this sliced region tends to $0$ as $n$ goes to infinity. So, I can say when $n\to\infty$ , the integral is $1$ , but this isn't helpful at all lol. I have not made any further progress this way, so I just tried numerically bashing everything. After iterating up to $x_{15}$ , I found what seems to be a pattern. I've made the following conjecture $$I \stackrel{?}{=} \frac{(n+1)!-1}{(n+1)!}$$ However, I am unable to prove this. Anyone able to help?","['integration', 'definite-integrals', 'factorial']"
4744325,A question on Beta function,"I need an asymptotic expansion/closed form for $$\sum_{k=1}^{\infty}\int_{0}^{\infty}(B(x+n+k,n+1))^2\ dx$$ where $B(m,n)$ is the Beta function and $n\in\mathbb{N}$ . Denote $$I_n=\sum_{k=1}^{\infty}\int_{0}^{\infty}(B(x+n+k,n+1))^2\ dx$$ By definition of Beta function $$I_n=\sum_{k=1}^{\infty}\int_{0}^{\infty}\left(\frac{\Gamma(x+n+k)\Gamma(n+1)}{\Gamma(x+2n+k+1)}\right)^2\ dx$$ $$I_n=(n!)^2\sum_{k=1}^{\infty}\int_{0}^{\infty}\left(\frac{\Gamma(x+n+k)}{\Gamma(x+2n+k+1)}\right)^2\ dx$$ Now interchanging the summation and integral (which needs to be justified) we get $$I_n=(n!)^2\int_{0}^{\infty}\sum_{k=1}^{\infty}\left(\frac{\Gamma(x+n+k)}{\Gamma(x+2n+k+1)}\right)^2\ dx$$ Now the infinite sum above is see here $$I_n=(n!)^2\int_{0}^{\infty} \left(\frac{\Gamma(x+n+1)}{\Gamma(x+2n+2)}\right)^2{}_3F_2\left ( 1,x+n+1,x+n+1;x+2n+2,x+2n+2;1 \right)dx$$ where ${}_3F_2$ represents the hypergeometric function. Thank you! Edit: @Gary and @TymaGaidash have in their elegant answers found that $$\sum_{k=1}^{\infty}\int_{0}^{\infty}\operatorname B^2(x+n+k,n+1)\ dx =\binom{2n}n\gamma-\sum_{j=0}^n\binom nj^2(2(H_{n-j}-H_j)\ln((j+n)!)+H_{n+j})$$ So if we define $$a_n:= \sum_{j=0}^n\binom nj^2(2(H_{n-j}-H_j)\ln((j+n)!)    $$ I need an asymptotic expansion for $a_n$ as $n\to \infty$ . I believe that we can apply Laplace's method of asymptotic expansion of integrals ( see here p. $322$ ) Question: I need to prove that the limit $$ \lim_{n\to \infty} \frac{n\ 4^{2n}}{e^{2n}}\{-d_{2n} a_n\}\leq \frac{3}{4}$$ where $d_{2n}=\text{LCM}(1,2,...,2n)$ , $\{x\}$ is the fractional part of $x$ and $a_n$ is defined as above. Edit Can we at least simplify the above fractional part $\{-d_{2n} a_n\}$ ? Can we use the (anti) symmetry in $a_n$ to get a simpler form of $\{-d_{2n} a_n\}$ ? I would really appreciate an answer which I can accept.  Thank you!","['integration', 'special-functions', 'gamma-function', 'sequences-and-series', 'hypergeometric-function']"
4744349,"""Vieta's-formulas"" on matrices","Let matrices $X $ and $ Y$ satisfy the equality $Z^2+AZ+B=0$ where $A , B$ are real matrices and $\det(X - Y)\ne 0$ Prove $ \operatorname{tr} X + \operatorname{tr} Y  = - \operatorname{tr} A$ $\det X \det Y = \det B $ Proof 1) $(X+Y)(X-Y)+XY-YX+A(X-Y)=0\Rightarrow X+Y+(XY-YX)(X-Y)^{-1}+A=0$ As $ \operatorname{tr} PQ= \operatorname{tr} QP$ we have $\operatorname{tr} (XY-YX)(X-Y)^{-1}=0$ entailing the result. We have $(X+A)X=-B$ and $(Y+A)Y=-B$ . The result follows if $\det XY=0.$ If $\det X\ne 0$ and $\det Y\ne0$ then $X-Y=B(Y^{-1}-X^{-1})$ Assuming that $X$ and $Y$ are permutable, we can easily obtain the result. Is it possible to achieve this result without such an assumption, or is it necessary to look for a counterexample?",['matrices']
4744351,Combinatorial Proof of the identity $\sum_{k=1}^{n}k\binom{n}{k} = n2^{n-1}$. [duplicate],"This question already has answers here : How to prove this binomial identity $\sum_{r=0}^n {r {n \choose r}} = n2^{n-1}$? (10 answers) Combinatorial Proof with Summation Identity: $\sum_{i=0}^n i{n \choose i} = n2^{n-1}$ (1 answer) Closed 11 months ago . I want to show $$\sum_{k=1}^{n}k\binom{n}{k} = n2^{n-1}$$ by way of combinatorial proof. Here is my attempt:
Let there be $n$ people.
We will count the number of ways in which we can choose a committee of any size and its chairperson, in two ways. First, we will find a committee and then find a chairperson for that committee. Let the size of the committee be $k\leq n.$ There are $\binom{n}{k}$ of choosing this committee. Then, from the $k$ members chosen, there will be one chairperson, which gives us $k$ choices for the chairperson. By the basic principle of counting, there are $$k\binom{n}{k}$$ possible choices for a committee and its chairperson to be chosen (of size $k$ ). Since $k$ can be any size from 1 to $n$ , with each size mutually exclusive, there are $$\sum_{k=1}^{n}k\binom{n}{k}$$ choices for committees and their chairpersons. The second way by which we count is to first find the chairperson and then find the rest of the members of the committee. Again, denote the size of the committee by $k.$ We first choose  a chairperson, having $n$ choices for the same. After choosing the chairperson, we need $k-1$ members from the remaining $n-1$ members (other than the chairperson since only one member can be the chairperson). This gives $$n\binom{n-1}{k-1}$$ choices for a committee of size $k.$ Note that since the chairperson is present, there is at least one member, so $k\geq 1.$ Now, to find all choices, we note that the sizes are mutually exclusive and range from 1 to $n$ giving $$\sum_{k=1}^{n}{n}\binom{n-1}{k-1} = n\sum_{k=0}^{n-1}\binom{n-1}{k} = n2^{n-1}.$$ This shows $$\sum_{k=1}^{n}k\binom{n}{k} = n2^{n-1}$$ . Please see if my proof is correct and there are ways to improve it.","['solution-verification', 'combinatorics', 'discrete-mathematics']"
4744413,A calculus problem from electrostatics,"Since this problem consists of multiple parts and one needs to see all of them to understand the problem i'm going to list out all of them: Consider a uniformly charged spherical shell of radius $R$ with surface charge density $\sigma_0$ . Compute the potential outtside the shell, $r>R$ . Now consider we slighttly squzee the sphere (without changing the surface charge density) along one axis so that the surface is now located at $$r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}}$$ where $b$ is a number such that $0<b^2-1 \ll 1$ and $\beta$ is the angle between a point on the surface and the symmetry axis of the system. Find the potential for $r<R$ neglecting terms $O(\epsilon^2)$ where $\epsilon=b^2-1$ . Hint: Recall the potential in terms of the charge volume density $\rho$ is $$\Phi(\vec{r})=\frac{1}{4\pi\epsilon_0}\sum_{n=0}^{\infty}\frac{1}{r^{n+1}}\int \mathrm d^3r'(r')^n\rho (\vec{r}')P_n(\cos\alpha)$$ where $\alpha$ is the angle between $\vec{r}$ and $\vec{r}'$ . The first question is simple enough, but I don't really understand the second question at all. Appreciate any help! EDIT : I shall provide the (my) solution to the first part of the question: The potential of a uniformly charged spherical shell of radius $R$ with surface charge density $\sigma_0$ outside of $R$ is simply $V=\frac{1}{4\pi\epsilon_0}\frac{Q}{r}$ , for $r > R$ . To formulate my question a bit better: Since the potential in electrostatics is generally given by the equation: $$V(r)=\frac{1}{4\pi\epsilon_0}\int \frac{dq}{r}$$ or in this case (using spherical coordinates): $$V(r)=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{dA}{r}=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{1}{r}R^2d\theta d\phi$$ Now if the sphere is ""slightly squeezed"", I can no longer use the above equation, and the shape of the sphere becomes somewhat of two (if I can assume again) U-shaped magnets with their open ends closed together. But then i don't understand the part where it says ""the surface is now located at $$r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}}$$ ""
Is this $r$ referring to the center of the object?","['physics', 'multivariable-calculus', 'electromagnetism']"
4744434,There exists an injection from $X$ to $Y$ if and only if there exists a surjection from $Y$ to $X$.,"Theorem. Let $X$ and $Y$ be sets with $X$ nonempty. Then (P) there exists an injection $f:X\rightarrow Y$ if and only if (Q) there exists a surjection $g:Y\rightarrow X$. For the P $\implies$ Q part, I know you can get a surjection $Y\to X$ by mapping $y$ to $x$ if $y=f(x)$ for some $x\in X$ and mapping $y$ to some arbitrary $\alpha\in X$ if $y\in Y\setminus f(X)$. But I don't know about the Q $\implies$ P part. Could someone give an elementary proof of the theorem?","['elementary-set-theory', 'functions']"
4744437,Partial fraction decomposition of $\frac{1}{(x(x+1)(x+2)...(x+n))^2}$,"In view of this question , I am trying to find the partial fraction decomposition of $$\frac{1}{(x(x+1)(x+2)...(x+n))^2}$$ where $n\in\mathbb{N}$ Since every $k$ , $k=-n,...,-2,-1,0$ is a pole of order two of the given fraction so its decomposition looks like $$\frac{1}{(x(x+1)(x+2)...(x+n))^2}=\sum_{k=0}^{n} \frac{a_k}{x+k}+\sum_{k=0}^{n} \frac{b_k}{(x+k)^2}  $$ So we get on multiplication $$1=a_0 x((x+1)...(x+n))^2+a_1x^2(x+1)((x+2)...(x+n))^2+...+a_n(x(x+1)...(x+n-1))^2(x+n)+b_0((x+1)...(x+n))^2+b_1(x(x+2)...(x+n))^2+...+b_n(x(x+1)...(x+n-1))^2 $$ Now we let $x=0$ , we get $b_0=\frac{1}{(n!)^2}$ and $x=-1$ gives $b_1=\frac{1}{((n-1)!)^2}$ and $x=-2$ gives $b_2=\frac{1}{(2(n-2)!)^2}$ I need to find a general formula for $a_k$ and $b_k$ . Can we use residues? Thank you!","['analysis', 'complex-analysis', 'binomial-coefficients', 'partial-fractions', 'residue-calculus']"
4744474,Find ratio formed by intersection of segments in a triangle,"Question: Points $P_1, P_2, \cdots, P_{29}$ are drawn, in that order, on side $AB$ of triangle $ABC$ so that $$AP_1=P_1 P_2=P_2 P_3=\cdots=P_{29} B$$ Let $M$ be the midpoint of $BC$ and $Q_i$ be the intersection of segments $CP_i$ and $AM$ for $1 \leq i \leq 29$ . Compute $\frac{Q_8 Q_{20}}{Q_{12}Q_{24}}$ . I began by letting $P_i$ be $(2i,0)$ , $A$ be $(0,0)$ , and $B$ be $(60,0)$ . Because the problem suggests that the ratio is constant, I let $\Delta ABC$ be equilateral, which means that $C$ has coordinates $(30, 30\sqrt 3)$ . It follows that $M$ has coordinates $(45, 15\sqrt 3)$ . From here, we can see $AM$ has equation $y=\frac{x}{\sqrt 3}$ and $CP_i$ has equation $$y=30\sqrt3 +\frac{900\sqrt 3}{2i-30}-\frac{30\sqrt 3 x}{2i-30}$$ Solving for the intersection, we see $Q_i$ has coordinates $\left(\frac{90i}{i+30}, \frac{90\sqrt 3 i}{i+30} \right)$ , and it follows that the required ratio is $$\frac{\frac{216\sqrt 3}{19}}{\frac{200}{7\sqrt 3}}=\frac{567}{475}$$ However, while this approach gets the answer, it is both ugly and only works for one very specific case. So, I am looking for a nicer synthetic solution. I have attached a diagram with $P_8$ and $P_{20}$ .","['contest-math', 'geometry']"
4744496,"Residual Subgroup, Conditional Expectations, and Central Sequences","Throughout, let us assume that $G$ denotes a group with the so-called infinite conjugacy class (ICC) property; i.e., the property that that the conjugacy class of any non-trivial element is infinite. A subgroup $H$ of $G$ is residual if there exists a subset $S \subseteq G \setminus H$ and elements $g_1 , g_2 \in G$ such that $G \setminus H = S \cup g_1^{-1} S g_1$ and $S, g_2^{-1}S g_2$ , and $g_2 S g_2^{-1}$ are disjoint subsets of $G \setminus H$ . Given a group $G$ , let $L(G)$ denote its group von Neumann algebra, which is generated by the left regular representation $\lambda : G \to U(\ell^2(G))$ . Since $\delta_e$ is a cyclic and separating for $L(G)$ , $x \mapsto x \delta_e$ is a linear isometry with dense range. In particular, using this fact, given any $x \in L(G)$ , we can write $x = \sum_{g \in G} x_g \lambda_g$ for some scalars $x_g \in \mathbb{C}$ (Fourier coefficients) satisfying $\sum_{g \in G} |x_g|^2 < \infty$ and $\lambda_g$ denotes the unitary induced by $g \in G$ from $G$ 's left regular representation. The von Neumann algebra $L(G)$ comes with a unique trace $\tau : L(G) \to \mathbb{C}$ given by $\tau (x) = \langle x \delta_{e} , \delta_e \rangle_{\ell^2(G)}$ , making $L(G)$ is a type $II_1$ factor (because $G$ is ICC). Given a subgroup $H \le G$ , let $E_{H} : L(G) \to L(H)$ be defined by $E_{H}(\sum_{g \in G} x_g \lambda_g) = \sum_{g \in H} x_g \lambda_g$ is a (the) conditional expectation of $L(G)$ onto $L(H)$ . Let $M$ be a $II_1$ factor von Neumann algebra with unique trace $\tau : M \to \mathbb{C}$ . We can use $\tau$ to get a norm on $M$ defined via $||x||_2 = \sqrt{\tau (x^*x)}$ . A sequence of elements $(x_n)_{n \in \mathbb{N}} \subseteq M$ bounded with respect to the trace norm (2-norm) is a central sequence if $||x_n y - y x_n||_2 \to 0$ as $n \to \infty$ for every $y \in M$ . I am trying to prove the following: If $H$ is a residual subgroup of $G$ , then any central sequence in $L(G)$ is equivalent to a central sequence whose elements lie in $L(H)$ . Given a central sequence $(x_n)_{n \in \mathbb{N}}$ in $L(G)$ , it's pretty clear that $(E_{H}(x_n))_{n \in \mathbb{N}}$ is the desired central sequence. It's easy to show that $(E_{H}(x_n))_{n \in \mathbb{N}}$ is $2$ -norm bounded, because $E_{H}$ is norm one projection, and it is easy to show that $(E_{H}(x_n))_{n \in \mathbb{N}}$ asymptotically commutes with everything in $L(H)$ in the $2$ -norm. However, I am having trouble showing it commutes with everything in $L(G)$ and it's equivalent to $(x_n)_{n \in \mathbb{N}}$ .","['von-neumann-algebras', 'group-theory', 'conditional-expectation', 'operator-algebras']"
4744515,why do we put equal to zero when trying to solve the polynomial inequality?,"I'm taking a course on algebra, the method my instructor showed to solve polynomial inequalities, is to put factors equal to zero to get ""points of interest""?? When you reach step like following $$(x+7)(x-4)>0$$ we put each factor equal to zero and get x, put them into number line and check each part, whether that part satisfies the inequality. But why are we putting, factors equal to zero? This is my question. I tried asking my instructor, it didn't make any sense to me - he said something like (not verbatim, and probably not accurate)  - polynomial functions are continuous, and all polynomials will cross the x-axis, and when they do $x$ will be zero, something like this. He mentioned intermediate value theorem.","['algebra-precalculus', 'polynomials', 'inequality']"
4744559,"Generalized log integral $\int_{0}^{\frac{\pi}{2}}\log(a+\sin(x))\,dx$","I was trying to evaluate the following integral, and wanted a solution verification. If I did something wrong please explain why it was wrong and how it can be fixed! Here is my method: $$I:=\int_{0}^{\frac{\pi}{2}}\log(a+\sin(x))\,dx$$ $$=\int_{0}^{\frac{\pi}{2}}\log(a+\cos(x))\,dx$$ Use: $$\cos(x)\equiv 2\cos^2(\frac{x}{2})-1$$ And define: $$c:=a-1$$ So $$I=\int_{0}^{\frac{\pi}{2}}\log(c+2\cos^2(\frac{x}{2}))\,dx$$ Let $$\frac{x}{2}\longrightarrow{x}$$ $$I=2\int_{0}^{\frac{\pi}{4}}\log(c+2\cos^2(x))\,dx$$ $$=2\int_{0}^{\frac{\pi}{4}}\log(c\sec^2(x)+2)\,dx+4\int_{0}^{\frac{\pi}{4}}\log(\cos(x))\,dx$$ The second integral is evaluated by my post: Solving integrals through “integral systems.” For the first, let: $$\tan(x)\longrightarrow{x}$$ $$I=2G-\pi\log(2)+2\int_{0}^{1}\frac{\log(c(x^2+1)+2)}{x^2+1}\,dx$$ $$I=2G-\pi\log(2)+2J$$ $$J=J(c):=\int_{0}^{1}\frac{\log(c(x^2+1)+2)}{x^2+1}\,dx$$ Now we perform Feynman’s technique on $J(c)$ . $$J’(c)=\int_{0}^{1}\frac{1}{cx^2+c+2}\,dx$$ $$J’(c)=\frac{\tan^{-1}\sqrt{\frac{c}{2+c}}}{\sqrt{c(2+c)}}$$ $$J(c)=J(c)-J(0)+\frac{\pi}{4}\log(2)=\frac{\pi}{4}\log(2)+\int_{0}^{c}J’(x)\,dx$$ $$J=\frac{\pi}{4}\log(2)+\int_{0}^{c}\frac{\tan^{-1}\sqrt{\frac{x}{2+x}}}{\sqrt{x(2+x)}}\,dx$$ Let: $$\sqrt{\frac{x}{2+x}}\longrightarrow{x}$$ $$J=\frac{\pi}{4}\log(2)+2\int_{0}^{\sqrt{\frac{a-1}{a+1}}}\frac{\tan^{-1}(x)}{1-x^2}\,dx$$ By another one of my posts: A generalized integral. $\int_{0}^{t}\frac{\arctan(x)}{1-x^2}\,dx$ And with mulch simplification we arrive at: $$I=\frac{\pi}{2}\cosh^{-1}(a)-\frac{\pi}{2}\log(2)+2Ti_2(a-\sqrt{a^2-1})$$ Where in my computations, $G$ denotes Catalans constant, and $Ti_2(x)$ denotes the inverse tangent integral.","['integration', 'solution-verification', 'definite-integrals', 'real-analysis']"
4744598,Expected Number of Dice Rolls to See All Sides,"I want to check if the solution to the following question is an application of the property $E[X + Y] = E[X] + E[Y]$ . The question: What is the expected number of rolls needed to see all six sides of a fair die? The solution:
We find that as we continue to make rolls and as we continue to see new values, the probability of seeing a new value changes overtime, from 1 to $\frac{5}{6}$ to $\frac{4}{6}$ and so on until we get to $\frac{1}{6}$ .  By treating each roll as a Geometric Random Variable, we find that the expected value of each of these rolls is given by $\frac{1}{p}$ , so for example after the first roll, the second roll's expected value would be $\frac{6}{5}$ . By adding up all these expectations, we find that the expected number of rolls needed to see all six sides of a fair die is 14.7 I'm asking because I'm having difficulty reconciling this solution with my understanding of the definition of expectation for discrete random variables, which is $E[X] = \Sigma xp(x)$ .","['expected-value', 'statistics', 'means', 'probability']"
4744620,How is the damping equation obtained?,"I modified some online notes in the internet and prepared this illustration for later use. Why does the formula fail? If $B=0$ (it is a sinusoidal system with no exponential components, undamped oscillation), it gets the form $x'' = -Cx$ . This is a sinusoidal movement with $\omega_0^2$ as C. Everything is okay here. If $C=0$ (it is an exponential system with no sinusoidal components, damped without oscillation for positive B), it gets the form $x' = -Bx + k$ after an integral is applied. This has a decay rate $B$ (not $\frac{B}{2}$ ! Here the formula fails) with $\lambda = -\lambda_0 = B$ . $$x'' + 2\zeta\omega_0 x' + \omega_0^2 x = 0$$ Is not this a general formula applicable to any possible situation? 2. $$\frac{\sqrt{B^2 - 4\omega_0^2}}{2} = \pm\omega_0i$$ $\omega_0^2 = C$ also produces mistaken results when $B \ne 0$ and I put it into the complex component of the quadratic formula. With some manipulation, it will take the form (multiplied by 2i): $$\sqrt{-B^2 + 4\omega_0^2} = \pm2\omega_0$$ $$-B^2 + 4\omega_0^2 = 4\omega_0^2$$ $$B=0$$ But this contradicts our very assumption that $B \ne 0$ . What am I confusing here? Why define $\zeta$ (the Damping Ratio) as $\frac{\lambda}{\omega_0}$ ? I think this comes from Euler's equation for a complex number as an exponent of $e$ : $$e^{(\lambda_0 \pm \omega_0i)t} = e^{\lambda_0t}e^{(\pm\omega_0t)i}$$ Here the exponential with the imaginary power produces an oscillating, sinusoidal factor, while the other is about decay. So decay divided by oscillation should overall say us how neatly the system is stabilizing. A high $\zeta$ means more damping and less oscillation ( $\zeta = 1$ being the best and more than it is overdamped). But now is not the formula Wikipedia gives ( $\frac{\lambda}{\sqrt{\lambda^2 + \omega_0^2}}$ ) more formal and logical? For that, it compares the decay rate (the real component of $r$ ) to the magnitude of $r$ . I am quite a bit confused. Hopefully not too many questions. Thanks! I have this unfortunate habit to not progress in my course until every detail makes sense to me.","['control-theory', 'ordinary-differential-equations']"
4744625,Evaluating the series $\displaystyle{\sum_{n=1}^\infty\binom{n}{n/2}\frac{(-1)^{n}}{n(2a)^n}}$,"I encountered the following series while evaluating an integral \begin{equation}
\sum_{n=1}^\infty\binom{n}{n/2}\frac{(-1)^{n}}{n(2a)^n}, \quad a\in\mathbb{R}
\end{equation} and am looking for a closed form. I have looked up generating functions and found some close candidates such as, \begin{equation}
\sum_{n=1}^\infty\binom{2n}{n}\frac{x^n}{4^nn}
= 2\log\left(\frac{2}{1+\sqrt{1-x}}\right)
\end{equation} but I can't figure out how to deal with the $\binom{n}{n/2}$ term. Looking at the power series for $(1+x)^n$ , I assume finding such a function may be difficult, but I hope a closed form exists. I will be happy to add my evaluation of the original integral up to the point where the series appears, if that may be necessary. Thank you in advance. Edit: Here is the integral I was evaluating, $$J(a)=\int_0^{\pi/2}\log\left(1+\frac{\sin x}{a}\right)\ dx,\quad a\in\mathbb{R}$$ which comes from this post, and the derivations of the series are in my answer there. I hope this helps @ClaudeLeibovici.","['generating-functions', 'closed-form', 'sequences-and-series']"
4744629,Geometric interpretation of a non-symmetric matrix having only real eigenvalues?,"Is there a geometric interpretation of a non-symmetric matrix having only real eigenvalues? It appears that multiplying random matrices with IID random entries eventually produces a matrix with only real eigenvalues, wondering if this can be turned into a statement about how random linear maps transform a vector. For instance, code below multiples $10\ 2\times 2$ random matrices, eigenvalues are almost always real. n = 2;
depth = 10;
dist = NormalDistribution[];
sample := RandomVariate[dist, {n, n}]/Sqrt[n];
sampled := Nest[sample . # &, sample, depth - 1];
Eigenvalues[sampled] (* {-0.859186, 0.613002} *) Some trajectories of 1,1 vector evolving in accordance with random 2D linear transformation. It appears that power iteration converges to a line for all matrices with real eigenvalues but not for complex-valued ones ( code ) Real-valued eigenvalues Complex valued eigenvalues",['linear-algebra']
4744637,Seeking reasoning and verification that $\sum_{n=1}^\infty (\frac{1}{(n+1)\ln^2 (n+1)})^r$ diverges for $0<r<1$.,"Seeking reasoning and verification that $$\sum_{n=1}^\infty \left(\frac{1}{(n+1)\ln^2 (n+1)}\right)^r$$ diverges for $0<r<1$ . Referring to a thread from 2016: Is there a series satisfies $a_n>0,\sum a_n$ converges but $\sum a_n^{r}$ diverges for all $0\leq r<1$ I verified that the sum converges for $r=1$ using the integral test, but am having a difficult time verifying its divergence for $0<r<1$ . My apologies if this is obvious. My available resources and textbooks lack clarity on the subject.","['sequences-and-series', 'real-analysis']"
4744649,What are the general procedures for simplifying a trigonometric expression using Euler's formula?,"What are the general procedures for simplifying a trigonometric expression using Euler's formula? As an example, this is how we can simplify the following trigonometric expression: $$\sin{x}\cos{x}$$ $$\sin{x}\cos{x} = \dfrac{e^{ix}-e^{-ix}}{2i} \times \dfrac{e^{ix}+e^{-ix}}{2}$$ $$ = \dfrac{(e^{ix}-e^{-ix})(e^{ix}+e^{-ix})}{4i}$$ $$ = \dfrac{(e^{2ix}-e^{-2ix})}{4i}$$ $$ = \dfrac{1}{2} \times \dfrac{(e^{i(2x)}-e^{-i(2x)})}{2i}$$ $$ = \dfrac{1}{2} \times \sin(2x)$$ However, how would you approach other expressions? What general steps can you take to simplify any trigonometric expression, such as the following: $$ \tan^{-1}(\dfrac{\cos{x}-\sin{x}}{\cos{x}+\sin{x}})$$ Where $\dfrac{-\pi}{4} < x < \dfrac{\pi}{4}$","['trigonometry', 'exponential-function', 'complex-numbers']"
4744651,The curve $y^2=x^3+Ax+B$ can be parameterized if and only if $x^3+Ax+B$ has a repeated root.,"I'm working through Shafarevich's Basic Algebraic Geometry, and one of the problems asks the reader to prove the problem in the title. I found the ""if"" direction fairly straightforward, but I can't finish my proof of the converse. If anyone can help me finish it, that'd be greatly appreciated! Suppose we have the rational parametrization $$(\frac{f(t)}{g(t)})^2=(\frac{p(t)}{q(t)})^3+A\frac{p(t)}{q(t)}+B,$$ yielding the identity $$f(t)^2q(t)^3=g(t)^2[p(t)^3+Aq(t)^2p(t)+Bq(t)^3].$$ Assuming $f$ and $g$ are coprime, we find that $p(t)^3+Aq(t)^2p(t)+Bq(t)^3$ must have a square factor. From here, I know that either two of the $(p(t)-aq(t)), (p(t)-bq(t)),$ and $(p(t)+(a+b)a(t)$ are the same, or, as @Aphelli pointed out, there are repeated factors. I cannot, for the life of me, figure out how to show this second case isn't so. If anyone can help me finish this, or give an alternative solution, that'd be greatly appreciated! Edit: Completely changed the question to account for the fact that the proof is, in fact, incomplete, and I cannot finish it.","['algebraic-geometry', 'parametrization']"
