question_id,title,body,tags
4175886,Methods for solving nonhomogeneous system of ODEs,"I have the system $$ \mathbf{x}'=\begin{pmatrix}2&1\\1&2\end{pmatrix}\mathbf{x}+\begin{pmatrix}e^t\\t\end{pmatrix}. $$ The eigenvalues of the matrix are 1 and 3 with eigenvectors $(1,-1)^T$ and $(1,1)^T$ respectively. Thus, we have the fundamental matrix $$
\Psi(t)=\begin{pmatrix}e^t & e^{3t}\\-e^t & e^{3t}\end{pmatrix}.
$$ From here I used the method of variation of parameters to assume there's a solution $\mathbf{x}=\Psi(t)\mathbf{u}(t)$ , where $\mathbf{u}(t)$ satisfies $\Psi(t)\mathbf{u}'(t)=\mathbf{g}(t)$ . I solved this matrix equation and multiplied to find $$
\mathbf{x}=c_1\begin{pmatrix}1\\-1\end{pmatrix}e^t+c_2\begin{pmatrix}1\\1\end{pmatrix}e^{3t}+\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}te^t-\frac{5}{12}\begin{pmatrix}1\\1\end{pmatrix}e^t-\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}t+\frac{1}{9}\begin{pmatrix}4\\-5\end{pmatrix}.
$$ I wanted to do this same problem using the Laplace transform given $\mathbf{x}(0)=\mathbf{0}$ .
When I take the Laplace transform of the system and solve for $\mathbf{X}(s)$ I get $$
\mathbf{X}(s)=
\begin{pmatrix}
s-2&-1\\
-1&s-2
\end{pmatrix}^{-1}
\begin{pmatrix}
(s-1)^{-1}\\
s^{-2}.
\end{pmatrix}.
$$ Performing this computation and taking the inverse Laplace transform, I get $$
\mathbf{x}=\begin{pmatrix}
\frac{1}{8}e^t+\frac{1}{8}e^{5t}-\frac{1}{4}e^t+t\\
e^t+\frac{2}{5}t+\frac{1}{2}e^{-t}+\frac{1}{50}e^{5t}-\frac{13}{25}.
\end{pmatrix}
$$ At this point I realized that there's no way this is the same answer. Does someone see where I'm going wrong or what I misunderstood about one of the methods?","['laplace-transform', 'ordinary-differential-equations']"
4175896,"Why is $C_0(X)$ not $C_b(X)$ for $X$ locally compact the ""prototypical"" abelian $C^*$ Algebra","Take $X$ to be a locally compact Hausdorf space, $C_b(X)$ to be bounded continuous functions and $C_0(X)$ to be bounded continuous functions that vanish at infinity. If $X$ is compact these notions coincide but in the case of say $\mathbb{R}$ the latter is a subalgebra of the former. As far as I can tell, both of these are abelian C* algebras. However, $C_0(X)$ seems to be preferred as the prototypical example. From wikipedia on C* algebras: Another important class of non-Hilbert C*-algebras includes the algebra $C_0(X)$ of complex-valued continuous functions on X that vanish at infinity, where X is a locally compact Hausdorff space. From https://math.dartmouth.edu/~dana/bookspapers/cstar.pdf The Banach algebras $C_0(X)$ (for X locally compact Hausdorff) will be our favorite
example of a commutative Banach algebra. Is this simply because $C_b(X)$ is a bit more unruly? It seems like the ""goal"" of this it to show via the Gelfand Representation that all abelian C* Algebras are in some sense functions on a compact space, is the point that $C_0(X)$ is ""the same as"" $C(X\cup{\infty})$ (the one point compactification), but the same is not true for $C_b(X)$ ?","['c-star-algebras', 'functional-analysis']"
4175912,Is it true that the derivative of an integral function is integrable?,"Let $f(x)$ be a Riemann integrable function (not neccesarily continuous) defined on some interval I. Let $F(x) = \int_a^xf(t)dt$ be it's integral function. Suppose that $F(x)$ is differentiable in the interval where it is defined, and let $F'(x)$ be its derivative. Is it true that $F'(x)$ is Riemann integrable on the interval I? I was wondering if this proposition is true, since the symmetric proposition is true by Barrow's Rule (FTC) Let $F(x)$ be a differentiable function defined on some interval I. Let $f(x) = F'(x)$ be it's derivative. Suppose that $f(x)$ is integrable in the interval where it is defined. Then $\int_a^x f(t)dt = F(x) + c$ , where $c = -F(a)$ If the first proposition its true, applying the second one we would have $\int_a^x F'(t)dt = F(x) - F(a)$ I would be grateful if the the proof is ""elementary"" and does not use any Measure Theory or Lebesgue integral results.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'derivatives']"
4175974,Contour integral of $\frac{|x|^{\frac{1}{2}}}{(x-4i)(x+2i)}$,"I am trying to solve the following integral: $$
\int_{-\infty}^{\infty}\frac{|x|^{\frac{1}{2}}}{(x-4i)(x+2i)}\,\mathrm dx.
$$ There are poles at $4i$ , $-2i$ , and a semi-circle in upper half plane contour of radius $R$ will enclose $4i$ . However, the function is non-analytic in $x=0$ , so I assumed I needed to evade this point with my contour, by using a small semi-circle of radius r around $x=0$ in the upper half plane. Residue theorem then gives us: $$
I+I_r+I_R = 2\pi i \operatorname{Res}\left(4i,\frac{|x|^{\frac{1}{2}}}{(x-4i)(x+2i)}\right).
$$ When taking the limit $R\rightarrow \infty$ , $I_R \rightarrow 0$ , and $I$ becomes the integral of interest. I am unsure if $I_r\rightarrow 0$ when $r\rightarrow 0$ , but I assumed this was the case. Calculating the residue gave me the result $-\frac{i}{3}$ , giving me the final result: $$
I = \int_{-\infty}^{\infty}\frac{|x|^{\frac{1}{2}}}{(x-4i)(x+2i)} dx = \frac{2\pi}{3}.
$$ According to online calculators, this result is wrong, and would like any suggestions as to how this integral can be solved. Thanks in advance!","['complex-analysis', 'contour-integration', 'definite-integrals', 'residue-calculus']"
4176009,Real Analysis Covid lectures using Rudin?,Just wondering if someone can point me to any intro real analysis video lectures that used Rudin. Because of Covid I thought that there might be a lot more out there but a cursory glance doesn't seem to give results.,"['reference-request', 'real-analysis']"
4176015,Mean and Variance of norm of multivariate normal,"Let $a\in\mathbb{C}^n$ be some complex vector, and let $Y \sim \mathbb{C}\mathcal{N}(0, \sigma^2I)$ , i want to find the mean and variance of this random variable: $$
X = \left\lVert Y \right\rVert_2^2 - \left\lVert Y - a \right\rVert_2^2.
$$ This is a random variable describing the distance of $Y$ to its mean versus another specific point. I was able to calculate the mean of $X$ as $$
-\left\lVert a \right\rVert_2^2.
$$ But the variance computations has proven to be quite complicated. Can someone help with this? Edit: Work I've done so far To compute $E[X^2]$ , we have that: $$ X^2 = \left\lVert Y \right\rVert^4 + \left\lVert Y - a \right\rVert^4 - 2\left\lVert Y \right\rVert^2 \left\lVert Y - a \right\rVert^2.$$ Computing the terms one at a time, we start with: \begin{align}
 \left\lVert Y - a \right\rVert^4 = {} & ( \left\lVert Y\right\rVert^2  +  \left\lVert a \right\rVert^2 - 2\Re \left\langle Y, a \right\rangle)^2 \\
 = {} & \left\lVert Y\right\rVert^4 + \left\lVert a\right\rVert^4 + 4\left(\Re \langle Y,a\rangle\right)^2 \\
 &+ 2\left\lVert Y\right\rVert^2\left\lVert a\right\rVert^2 \\
 &- 4 \left\lVert Y\right\rVert^2 \Re\left\langle Y, a \right\rangle \\
 &\underbrace{ {} - 4 \left\lVert g\right\rVert^2 \Re\left\langle Y, a \right\rangle}_{E[\cdot]= 0}
\end{align} Moreover, we have that: \begin{align}
\left\lVert Y\right\rVert^2 \left\lVert Y - a \right\rVert^2 
&= \left\lVert Y\right\rVert^4  +  \left\lVert Y \right\rVert^2\left\lVert a \right\rVert^2 - 2\left\lVert Y \right\rVert^2\Re \langle Y,a \rangle \\
\end{align} Subtracting $E[X]^2 = \left\lVert a \right\rVert_2^4$ from $E[X^2]$ , and removing all the terms that cancel out: \begin{align}
\operatorname{Var} &= E[X^2] - E[X]^2 \\
&= E\left[\left\lVert a\right\rVert_2^4 + 4(\Re\langle Y,a\rangle)^2\right] -  \left\lVert a\right\rVert_2^4 \\
&= E\left[4(\Re \langle Y,a\rangle )^2\right] \\
&= E\left[\big(\langle Y,a \rangle + \langle a,Y\rangle\big)^2\right]
\end{align} I'm stuck on what to do with the expression above.","['statistics', 'normed-spaces', 'normal-distribution', 'probability']"
4176025,The polar coordinates change of variables,"In my textbook, the author states that $$
\iint_R f(r\cos\theta,r\sin \theta)r\,dr\,d\theta =\iint_{G(R)} f(x,y)\,dy\,dx,
$$ where $G(r,\theta)=(r\cos\theta, r\sin \theta)$ and he says that this is the formula for the change of variables between rectangular coordinates and polar ones. But for me, it doesn't look right, if we want to change coordinates then the formula should be like this $$
\iint_R f(x,y)\,dy\,dx=\iint_{G(R)} f(r\cos\theta,r\sin \theta)r\,dr\,d\theta.
$$ Am I right? or are both formulas the same thing? This is a picture from the textbook, it says that the map $G$ takes rectangular to polar","['multivariable-calculus', 'change-of-variable', 'polar-coordinates']"
4176042,Prove that a set is the convex hull of another set,"Consider a set $\mathcal{P}$ defined as $$ \mathcal{P} := \left\{ (x,y) \mid x\in[0,\bar{x}], y\in[0,\bar{y}], xy=0 \right\}.$$ I am trying to prove that $\operatorname{conv}(\mathcal{P}) = \tilde{\mathcal{P}}$ where $\tilde{\mathcal{P}}$ is defined as $$ \tilde{\mathcal{P}} := \left\{ (x,y) \, \bigg| \, x\geq 0, y\geq 0, \frac{x}{\bar{x}} + \frac{y}{\bar{y}} \leq 1 \right\}.$$ I am sure that my hypothesis is correct because simply drawing the two sets on a piece of paper shows that $\tilde{\mathcal{P}}$ should indeed be $\operatorname{conv}(\mathcal{P})$ . However, I am struggling to prove it analytically. The approach I'm using is to show that $\mathcal{\tilde{P}} \subseteq \operatorname{conv}(\mathcal{P})$ and $\operatorname{conv}(\mathcal{P}) \subseteq \mathcal{\tilde{P}}$ both hold simultaneously. However, I have not been able to make much headway, and some help would be appreciated. (edit) Managed to show that $\tilde{\mathcal{P}} \subseteq \operatorname{conv}(\mathcal{P})$ since $\{ (0,0),(0,\bar{x}),(\bar{y},0) \} \in \mathcal{P}$ and every point in $\tilde{\mathcal{P}}$ belongs to $\operatorname{conv} \{ (0,0),(0,\bar{x}),(\bar{y},0) \}$ .","['convex-hulls', 'convex-optimization', 'convex-geometry', 'geometry', 'convex-analysis']"
4176051,"Why is this a dense set? (Lemma 6.13, Lee ISM)","In Lemma 6.13 of Lee's 'Introduction to Smooth Manifolds' Lee is showing that the images of two functions $\kappa, \tau$ are subsets of measure zero in $\mathbb{RP}^{N-1}$ . It follows then that their union $\operatorname{im}(\kappa) \cup \operatorname{im}(\tau)$ is also a measure zero set and finally he uses a small lemma which shows that the complement of a measure zero set is a dense set, that is $\mathbb{RP}^{N-1}\setminus(\operatorname{im}(\kappa) \cup \operatorname{im}(\tau))$ is dense in the projective space. I don't understand how it then follows that the set of vectors in $\mathbb{R}^N$ whose equivalence classes are contained in $\mathbb{RP}^{N-1}\setminus(\operatorname{im}(\kappa) \cup \operatorname{im}(\tau))$ are a dense subset of $\mathbb{R}^N$ . What is the formal reason that this set is dense in $\mathbb{R}^N$ ? Intuitively, i understand that the equivalence class of any vector is an uncountable set and so if they are dense, surely the vector in plain Euclidean space are dense too. But i fail to find a really formal argument for the claim. Any help would be nice. The cited proof can be found in Google Books: https://books.google.de/books?id=xygVcKGPsNwC&lpg=PA125&hl=de&pg=PA132#v=onepage&q&f=false","['general-topology', 'smooth-manifolds']"
4176057,How should exercise 1-5.7(B) from Stoll be read?,"I asked a question about this exercise in the past ( Is there a standard way to represent a general set theoretical equation in one variable. ), and don't mean to ""re-ask"".  But I'm unclear as to how I should read the phrase ""only complements of individual sets,"" in Stoll's outline of a proof. Specifically, does it mean that both an individual set and its complement may appear; or does it mean that only the complements of the given constant sets should appear? I believe it means the former, but the wording is a bit nebulous (to me). This is exercise 1-5.7(B) from Stoll, Robert R.. Set Theory and Logic (Dover Books on Mathematics) (Kindle Location 787). Dover Publications. Kindle Edition. Prove that an equation in $\mathcal{X}$ with righthand member $\emptyset$ can be reduced to one of the form $\left(\mathcal{A}\cap\mathcal{X}\right)\cup\left(\mathcal{B}\cap\mathcal{\overline{X}}\right)=\emptyset$ . (Suggestion: Sketch a proof along these lines. First, apply the DeMorgan laws until only complements of individual sets appear. Then expand the resulting lefthand side by the distributive law 3 so as to transform it into the union of several terms $\mathcal{T}_{i}$ , each of which is an intersection of several individual sets. Next, if in any $\mathcal{T}_{i}$ neither $\mathcal{X}$ nor $\mathcal{\overline{X}}$ appears, replace $\mathcal{T}_{i}$ by $\mathcal{T}_{i}\cap\left(\mathcal{X}\cup\mathcal{\overline{X}}\right)$ and expand. Finally, group together the terms containing $\mathcal{X}$ and those containing $\mathcal{\overline{X}}$ and apply the distributive law 3.) I'm not asking for a solution. My intuitive understanding is that, for any solution $\mathcal{X},$ we can exclusively group the $\mathcal{T}_{i}$ into those which are disjoint with $\mathcal{X},$ and those which are subsets of $\mathcal{X}.$","['elementary-set-theory', 'proof-explanation', 'logic']"
4176064,Neural ODEs - Adjoint Sensitivity Method for multiple time points,"I'm trying to understand the math behind the backward pass in Neural ODEs . I understand it boils down to solving an IVP backwards in time, for a state $\mathbf a(t)$ (the so called adjoint state) such that $$\mathbf a(t) = \frac{dL}{d\mathbf z(t)},$$ where $L(.)$ is some loss function. From this, it's possibe to derive $$\frac{d\mathbf a(t)}{dt} = -\mathbf a(t)\frac{\partial f(\mathbf z(t), t, \theta)}{\partial t}.$$ Assuming $L$ only depends on the hidden state at the final time point, z( $t_N$ ), the gradients w.r.t $z(t_N)$ are trivial. To get gradients w.r.t $z(t_0)$ , we must have stored $z(t_N)$ from the forward pass, and we solve the backwards IVP: $$\mathbf a(t_0) = \mathbf a(t_N) + \int_{t_N}^{t_0} \frac{d\mathbf a(t)}{dt}dt = \mathbf a(t_N) - \int_{t_N}^{t_0} \mathbf a(t)^T\frac{\partial f(\mathbf z(t), t, \theta)}{\partial \mathbf z(t)}dt. $$ To get gradients w.r.t $t$ and $\theta$ , the adjoint state is augmented with $\theta(t)$ (constant), and $t(t)$ , which have trivial derivatives $\frac{d\theta(t)}{dt} = 0$ and $\frac{dt(t)}{dt} = 1$ . We obtain (omitting some steps, which can be found in Appendix B in the paper): $$ \frac{d\mathbf a_{aug}}{dt} = -[\mathbf a(t) \ \mathbf a_\theta(t) \ \mathbf a_t(t)]\frac{\partial f_{aug}}{\partial [\mathbf z, \theta, t]}(t) = - [\mathbf a \frac{\partial f}{\partial \mathbf z} \  \mathbf a \frac{\partial f}{\partial \theta} \  \mathbf a \frac{\partial f}{\partial t}],$$ Where the first element is our adjoint state before augmentation, the second element gives us gradients w.r.t $\theta$ and the third element gives us gradients w.r.t $t$ . This is all straightforward to me, as long as $L$ only depends (directly) on $\mathbf z(t_N)$ . If instead we have a loss that depends directly on $\mathbf z(.)$ at additional intermediate time points, the authors state that one should simply repeat this procedure for each pair of consecutive time points and sum up the obtained gradients. My intuition is a bit lost as to why this is the case. I think part of it is the fact that future points can't influence past time points, but from Figure 2 in the paper it looks as though when we're going (backwards) from $\mathbf z(t_{i+1})$ to $\mathbf z(t_{i})$ and then ""jump"" there to repeat the procedure from $\mathbf z(t_{i})$ to $\mathbf z(t_{i-1})$ , we ""keep"" $\mathbf z_{i-1}$ from influencing $\mathbf z_{i+1}$ . Bluntly and to summarize: the paper states that ""If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation."", but I lack some intuition as to why that is the case.","['neural-networks', 'ordinary-differential-equations']"
4176084,An interesting binomial summation,"Recently, a student asked this summation $$
S= \sum_{k=0}^n \frac{{n \choose k}}{ {m + n-1 \choose k}}
$$ without putting any effort. Interestingly, opening of the binomial coefficient doesn't seem to help, yet it is doable. Use $$
{N\choose k}^{-1}=(N+1)\int_{0}^{1} x^k(1-x)^{N-k}\, dx.
$$ Then $$
S= \sum_{k=0}^n \frac{{n \choose k}}{ {m + n-1 \choose k}}= (m+n) \sum_{k=0}^{n}\int_{0}^{1} {n \choose k} x^k (1-x)^{m+n-1}\, dx
$$ $$\implies S=(m+n)\int_{0}^{1}\, dx~
(1-x)^{m+n-1}\sum_{k=0}^{n} {n \choose k}\left(\frac{x}{1-x}\right)^k$$ $$\implies S=(m+n)\int_{0}^{1}(1-x)^{m-1}dx=\frac{m+n}{m}.$$ The question is: How to do it otherwise?","['integration', 'summation', 'binomial-coefficients', 'sequences-and-series']"
4176151,Undesrstanding a proof related to Fibonacci numbers.,"I was going through the following proof of the book Introductory Combinatorics by Richard A. Brualdi. Theorem. The Fibonacci numbers satisfy the formula $f_n = \frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^n - \frac{1}{\sqrt{5}}(\frac{1-\sqrt{5}}{2})^n$ My doubt: In the proof they have written that we consider the Fibonacci recurrence relation in the form $f_n - f_{n-1} - f_{n-2} = 0$ One way to solve this recurrence relation is to look for a solution of the form $f_n =q^n$ ,
where q is a nonzero number. How and why we consider the solution of the form $f_n =q^n$ ? Maybe I am missing something very basic here. Unable to recall. The rest of the proof was understandable. Kindly help in this regard. Thank you so much for the help.","['fibonacci-numbers', 'proof-explanation', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4176176,Calculate $\sum\limits_{i=0}^\infty(2^{2^{(-i)}}-1)$,"Calculate $\sum\limits_{i=0}^\infty(2^{2^{(-i)}}-1)$ By a Python program showed below, I can calculate that it is about $1.7818$ ( $1.7818386318393353172743493971315840477111345642771002580086952600435098253141880734976160498115058888973$ by a comment). I've worked on this problem for days, but I can't solve it. Can anyone help me? Update: I'm a student who is interested in math. I came out with this problem about one year ago, but I think it's too difficult for me. Also, $2^{2^{(-i)}}-1\le2^{(-i)}$ , so it is a convergent serie. Update 2: Why $2^{2^{(-i)}}-1\le2^{(-i)}$ ? Let $x$ be $2^{(-i)}$ , $i\ge0$ , so $x\ge1$ , then $2^{2^{(-i)}}-1-2^{(-i)}=2^x-1-x$ . When $x=1$ , $2^x-1-x=0$ , and $\frac{d(2^x-1-x)}{dx}=2^x\ln-1>0$ for $x\ge1$ , so $2^{2^{(-i)}}-1-2^{(-i)}=2^x-1-x>0$ , that means $2^{2^{(-i)}}-1\le2^{(-i)}$ . Update 5 on 2021-07-02: Anyone who can show whether the sum is rational can win the bounty. from math import *
s = 0
a = 2
while a != 1:
    s += a - 1
    a = sqrt(a)
print(s)","['limits', 'exponentiation', 'sequences-and-series']"
4176179,Counter example that a convex set is not necessarily an interval or ray .,"The example I am trying to frame is like this : I am considering the dictionary order on $\mathbb{R}^2$ , now I know that the set $S=\{x^2+y^2<1\}$ is a convex set(I am confused that it is with respect to which order).It doesn't look like that $S$ is an interval or ray. I need some help to frame this question properly to give an example.","['general-topology', 'examples-counterexamples', 'analysis']"
4176189,Finding the integral curve of a vector field on cylinder,"Find the integral curve of the vector field $X = \{-y, x, 1\}$ that is given on circular cylinder. I have looked through the definition of integral curve . But I cannot find any beginner-friendly explanation of finding the curve itself. After a some research on MSE I found this post but still I don't know what to do with the cylinder. I quite new to all of these and self-learner. Some hints and explanations are greatly appreciated. So, am I to solve just $$\begin{cases}\dot x = -y \\ \dot y = x \\ \dot z = 1\end{cases}$$ ?","['vector-fields', 'ordinary-differential-equations', 'differential-geometry']"
4176195,Combinatorial interpretation of $\frac{1}{1-e^{x+1}}$ or $\frac{1}{1-e^{x-1}}$,"After like 100 hours searching info to unravel a mystery about expressions on $e$ like $\frac{e^{6} A_{5}\left(e^{-1}\right)}{e^{6}\left(1-e^{-1}\right)^{6}} \approx 5 !$ talked by me in here ,  user Jair Taylor redirected me to this generating functions $\frac{1}{1-e^{x-1}}$ as they generate exactly the coefficients I'm interested in. So I'm asking for help hopping to get a true final combinatorial (or even probabilistic) interpretation  of this coefficients (so I can stop searching for esoteric papers and maybe focus on my other subjects). Anyway sadly although $\frac{1}{1-e^{x+1}}$ is related to $Seq(Set^*(z+1))$ a very natural looking construction from Flajolet and Sedgewick's classic book, this has problems. In fact it's a very exact variation of the surjection generating function $$\frac{1}{2-e^z}$$ first the surjection evaded the set with no elements (it was $Seq(Seq(Z)-1)$ ). My expression on the other hand even has an extra empty element $Z+1$ entering the set construction, or $Seq(Set(Z+\varepsilon)$ . I have also imagined maybe an $e$ ""cost"" factor (rewriting $\frac{1}{1-e^{x+1}} = \frac{1}{1-e*e^{x}}$ ) but with the problem of having the empty ""urn"" going this way doesn't seem promising enough. For a maybe less contrived approach this seems to be related to seemingly niche problem called Simon Newcomb's problem : A counting problem about compositions with $k$ falls, in generic multisets. The expression that pertains me the most is $$\sum_{d=0}^{\infty} \frac{A_{d}(t) x^{d}}{(1-t)^{d+1} d !}=\frac{1}{1-t e^{x}}$$ where replacing $t$ follows the patterns I see in wolfram-alpha for the two functions in the title, and going with $t=1$ gives an identity in wikipedia, but as both expressions don't seem to have any justification,  it's just a formula for now. Trying to see the connection with the general problem and reading some of the papers the only thing I found was a curious $q-$ nomial identity $e_q(x^{-1}) = (x ; q)_{\infty}=\prod_{n=0}^{\infty}\left(1-x q^{n}\right)$ that in the limit would give a crazy equality suggested in that post $$e^{-x}= \prod_{n=0}^{\infty}\left(1-x\right)$$ The $q$ version property appears in the Wikipedia page for the so called $q$ -exponential and the quantum dilogarithm but along ""polytopes"",""matrix generating functions"" or other scary names in papers of this Newcomb problem, suggest me I'm maybe not ready to solve this problem.","['combinatorial-proofs', 'analytic-combinatorics', 'q-analogs', 'combinatorics', 'generating-functions']"
4176325,Closed form for $ a_n = 1-2^4 +3^4 - 4^4 + ... \pm (-1)^n \cdot n^4 $ using Generating Functions,"Problem : Find closed form expression for $ a_n = 1-2^4 +3^4 - 4^4 + ... \pm (-1)^n \cdot n^4 $ using Generating Functions. Hint: Calculate what function you need to multiply in order to get partial sums with alternating signs. I don't really know... , I found that $ G(x) = \sum_{n=0} (-1)^n(n+1)^4 x^n =\sum_{n=0} (-1)^n \cos(\pi n) x^n  $ . I get the same results when trying to solve the recurrence relation $ a_{n+1} = a_n + (-1)^n (n+1)^4 $ which describes the sequence above, so I get no progress. I don't really see how the hint helps me, do you have any ideas? Thanks in advance! Note: I found Closed form for $1-2+3-4+\cdots(-1)^{n-1}n$ and some more questions that relate to the partial sum above, but I can't see how to relate the partial sum to generating functions.","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4176330,How does the Cameron-Martin space of a Gaussian Markov process with initial condition $0$ change when we consider a random initial condition,"Suppose we are given a real-valued Gaussian Markov process with initial condition zero $$Y_t = \int_0^t f(s) dW_s \quad t \in [0,T], f\in L^2([0,T].$$ Following [Example 4.5, 1], $Y$ is distributed according to Gaussian measure on the canonical path space $C[0,T]$ with Cameron-Martin space given by $$H_Y =\left\{ h: h(t)= \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]) \right\}$$ I would like to show that for a process $$Y_t = Y_0 +\int_0^t f(s) dW_s \quad t \in [0,T]$$ with initial condition $Y_0 \sim \mathcal N(0, \sigma), \sigma >0$ , the Cameron-Martin space changes as one would expect $$H_Y =\left\{ h: h(t)= y_0+ \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]), y_0\in \mathbb R \right\}$$ This holds true in a couple of examples, however, I cannot seem to prove it in the general case. Any help is appreciated! [1] Lifshits (2012). Lectures on Gaussian processes.","['probability-distributions', 'stochastic-processes', 'markov-process', 'gaussian-measure', 'functional-analysis']"
4176366,Solution verification of $\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p}$?,"I need to find the sum for any $p \in \mathbb{R}$ , $$S=\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p}$$ My attempt: I will use the idea that $\lim\limits_{n \to ∞} f \cdot g = \lim\limits_{n \to ∞} f \cdot \lim\limits_{n \to ∞} g$ if and only if $f\cdot g$ is not an indeterminate form as $n \to ∞$ , $$S= \lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{n^p(1+r/n)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}=\lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \lim\limits_{n \to ∞} \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \int_{1}^{2} \dfrac{1}{x^p} \, \mathrm{d}x$$ If $p≠1$ then, $$S= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \left[\left(\dfrac{2^{-p+1}}{-p+1}\right)-\left(\dfrac{1^{-p+1}}{-p+1}\right) \right]$$ If $p=1$ then, $$S= \lim \limits_{n\to ∞} \dfrac{1}{n^{1-1}} \cdot \ln 2$$ Conclusion: If $p>1$ then $S=0$ , if $p=1$ then $S= \ln 2$ and if $p<1$ then $S = +\infty$ Is the reasoning correct at each and every step? Secondly, are there any alternative methods to do this? I couldn't find this question on the site.","['real-analysis', 'calculus', 'solution-verification', 'sequences-and-series', 'limits']"
4176385,For which $r \in \mathbb R$ is the series $S(r)$ finite?,"For each $r \in \mathbb R$ we let $$L_r := \left\{ \begin{pmatrix} a+cr \\ b+dr \\ c \\d \end{pmatrix} : a,b,c,d \in \mathbb Z \right\},
\quad W:= \left\{ \begin{pmatrix} 0 \\ 0 \\ x_3 \\ x_4 \end{pmatrix} : x_3,x_4 \in \mathbb R \right\}
$$ and $$ L^*_r := L_r \setminus W.$$ Now we consider the series $$S(r):=  \sum_{x \in L^*_r} E_1(2 (x_1^2+x_2^2)) \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2)).$$ Here $E_1 : (0, \infty) \to (0,\infty)$ is the exponential integral $$E_1(s):=\int_1^\infty \exp(-ts) \frac{dt}{t} = \int_s^\infty \exp(-t) \frac{dt}{t}.$$ My question: For which $r \in \mathbb R$ is $S(r)$ finite? My partial answer: It's quite easy to see that $S(r)$ is fintie for $r \in \mathbb Q$ . But that's all I've found out so far. Happy: I would already be happy if you could show for a single $r \in \mathbb R \setminus \mathbb Q$ (you pick it!) whether $S(r)$ converges or diverges. Some facts: We have $$ S(r) = \int_1^\infty \left( \sum_{x \in L^*_r}  \exp \left( (1-2t)(x_1^2+x_2^2) - (x_3^2+x_4^2) \right)\right) \frac{dt}{t}.$$ Further, we have $L_r^*=L_r \setminus \{0\}$ iff $r \in \mathbb R \setminus \mathbb Q$ . Estimates for $E_1$ : In case they help: For $s>0$ we have $$\frac{e^{-s}}{s+1} < E_1(s) < \frac{s+1}{s+2} \frac{e^{-s}}{s} < \frac{e^{-s}}{s}.$$ Proof for rational $r$ : Since I was asked in the comments to provide a proof for rational $r$ here it is. If $r$ is rational the set $$\{ a+br : a,b \in \mathbb Z \}$$ is a discrete subset of $\mathbb R$ . Hence $$\varepsilon := \min(\{2(x_1^2+x_2^2) : x \in L_r^*\}) > 0$$ exists. Now we make use of $$E_1(s) \le \frac{\exp(-s)}{s}.$$ We have $$S(r)
\le \sum_{x \in L^*_r} \frac{\exp(-2 (x_1^2+x_2^2))}{2 (x_1^2+x_2^2)} \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2))\\
\le \frac{1}{\varepsilon} \sum_{x \in L^*_r} \exp(-(x_1^2+x_2^2+x_3^2+x_4^2))\\
\le \frac{1}{\varepsilon} \sum_{x \in L_r} \exp(-||x||^2) < \infty.
$$","['divergent-series', 'integer-lattices', 'real-analysis', 'exponential-sum', 'exponential-function']"
4176386,Universal property of tensor product and fibre product - how to convert the diagrams,"We have two diagrams for the universal property of tensor product of rings and the fibre product for schemes. See here for fibre product , which is a square, and here for tensor product, which is a triangle (this is the ""for any $R$ -bilinear map ..."" diagram). However, I also know that the tensor product is a ""fibre coproduct"", so they should have diagrams in the same shape. How can I convert the triangular diagram for tensor product into a square? Where does the bilinear map go?","['algebraic-geometry', 'tensor-products', 'schemes']"
4176400,Find the number of unbeatable integers between $256$ and $1024$ inclusive,"Problem Mugdho and Snigdho are playing a game against each other. Mugdho has a red computer. If the computer screen displays the number $x$ , Mugdho can make a move and choose to: Add $1$ to $x$ . Multiply $x$ by $2$ . Snigdho has a blue computer. If his screen displays the number $y$ , Snigdho's moves are: Add $1$ to $y$ . Multiply $y$ by $4$ . They both start with a $0$ on their screen. Given an integer $n$ , Mugdho and Snigdho are trying to reach $n$ from $0$ on their computer in the minimum number of moves. But some integers are unbeatable - Mugdho and Snigdho will reach them in the same number of moves! Find the number of unbeatable integers between $256$ and $1024$ inclusive. (BdMO 2020) My approach Mugdho's case: We take the numbers in binary to solve the problem. We consider the case when Mugdho tries to reach $0$ from a number $n$ . For minimum number of moves, we divide a number by $2$ if it is even and we subtract $1$ from the number if it is odd until we reach $0$ . Dividing a number by $2$ removes the last bit of the string. Now, some observations lead that the number of moves $=$ number of bits $+$ number of 1s $-1$ . Snigdho's case: We do the same thing as before. This time we subtract $1$ from a number until we get a number divisible by $4$ and divide a number by $4$ if it is divisible by $4$ to reach $0$ . Dividing a number by $4$ removes the last two bits of the string. But this time, I can't find a closed form of the number of moves as I did in the previous case. I got the number of moves $=$ number of pairs of $0+$ number of $1$ s $-1$ . But this doesn't work. I need to complete the solution. And some other ways to solve the problem are also welcome.","['contest-math', 'number-theory', 'elementary-number-theory', 'game-theory', 'recreational-mathematics']"
4176425,"Evaluate $\lim_{n\to\infty}\dfrac{[1^2x]+[2^2x]+[3^2x]+...[n^2x]}{n^3}$, where $[.]$ denotes the greatest integer function $x\in R$","Evaluate $\lim_{n\to\infty}\dfrac{[1^2x]+[2^2x]+[3^2x]+...[n^2x]}{n^3}$ , where $[.]$ denotes the greatest integer function $x\in R$ $\lim_{n\to\infty}\dfrac{[1^2x]+[2^2x]+[3^2x]+...[n^2x]}{n^3}=\lim_{n\to\infty}\dfrac{\sum_{r=1}^n[r^2x]}{n^3}$ Without $[.]$ , I could have written $\sum r^2=\dfrac{n(n+1)(2n+1)}6$ . Not sure how to approach now. If we try to convert it into integration, we have $\dfrac1n$ which could be written as $dx$ , we need $\dfrac rn$ to be written as varibale (maybe $y$ here, since $x$ is already given), but we have $r^2$ in $[.]$ , I don't think we can take $n^2$ inside. Can the squeeze theorem be applicable here? But now sure how.","['integration', 'limits', 'summation', 'definite-integrals']"
4176442,"Prove that if $A\subset S^{n-1}$ with $\mu(A) = \frac12$, then $\mu(A_\epsilon) \ge 1 - e^{-n\epsilon^2/2}$","Background : On Pg. $44$ of these notes , it is stated that: In the case of a sphere $\Omega = S^{n-1}$ , we have the following pair of properties. If $A\subset\Omega$ with $\mu(A) = \frac12$ , then $\mu(A_\epsilon) \ge 1 - e^{-n\epsilon^2/2}$ . If $f:\Omega\to\mathbb R$ is $1$ -Lipschitz there is a number $M$ for which $$\mu(|f-M| > \epsilon) \le 2e^{-n\epsilon^2/2}$$ We have seen how the second can be deduced from the first. The reverse implication also holds (apart from the precise constants involved). To see why, apply the second property to the function given by $f(x) = d(x,A)$ . Theme of this post: I need help deducing $(1)$ from $(2)$ . Notation: $S^{n-1}$ is the Euclidean sphere of unit radius in $\mathbb R^n$ . $\mu$ refers to the rotation invariant probability measure on the sphere $S^{n-1}$ . The measure of the entire sphere is $1$ , that of a hemisphere is $\frac12$ , etc. $d(x,A) = \inf\{\|x-y\|: y\in A\}$ where $A \subset\mathbb R^n$ is compact and $x \in \mathbb R^n$ . A function $f: S^{n-1}\to\mathbb R$ is $1$ -Lipschitz, if for any $x,y\in S^{n-1}$ , $$\|f(x) - f(y)\| \le \|x-y\|$$ $A_\epsilon$ is the $\epsilon$ -thickening (or neighborhood) of $A$ , that is $A_\epsilon = A + \epsilon B^n_2$ where $B^n_2 = \{x: \|x\| \le 1\}$ . My thoughts: The author suggests to put $f(x) = d(x,A)$ in $(2)$ . First we would have to check that this function is $1$ -Lipschitz. Consider $x,y\in S^{n-1}$ , i.e. $\|x\|=  \|y\| = 1$ and $A$ , a compact subset of $S^{n-1}$ .
Thanks to @OliverDiaz's comment, I now see that $$|d(x,A) - d(y,A)| \le \|x- y\|$$ Consider $a\in A$ , then $d(x,A)\le \|x-a\| \le \|x-y\| + \|y-a\|$ . So, $d(x,A) - \|x-y\|\le \|y-a\|$ . Taking $\inf_{a\in A}$ , we have $d(x,A) - d(y,A) \le \|x-y\|$ . Interchanging $x$ and $y$ , we get $d(y,A) - d(x,A) \le \|x-y\|$ and hence the required inequality. Since $f = d(\cdot, A)$ is Lipschitz, there is some $M$ for which $$\mu(|d(\cdot, A)-M| > \epsilon) \le 2e^{-n\epsilon^2/2}$$ i.e. $$\mu(|d(\cdot, A)-M| \le  \epsilon) \ge 1 - 2e^{-n\epsilon^2/2}$$ It is easy to check that $A_\epsilon = \{x: d(x,A) \le \epsilon\}$ . Due to the similarity in expression of what I have arrived at, and what I need to prove - I think I'm pretty close, but don't know what to do next. Thank you!","['convex-geometry', 'measure-theory', 'real-analysis']"
4176451,Showing $\frac{\sin((2n+1)\theta)}{\sin\theta}=(2n+1)\prod_{k=1}^n\left(1-\frac{\sin^2\theta}{\sin^2\frac{k\pi}{2n+1}}\right)$,"A problem I solved previously I get $$\frac{\sin(n\theta)}{\sin(\theta)} = 2^{n-1}\prod_{k=1}^{n-1}\bigg(\cos(\theta)-\cos{\bigg(\frac{k\pi}{n}}\bigg)\bigg)$$ So considering $z^{4n+2} = 1$ leads to $$\frac{\sin((2n+1)\theta)}{\sin(\theta)} =  2^{2n}\prod_{k=1}^{2n}\bigg(\cos(\theta)-\cos\bigg(\frac{k\pi}{2n+1}\bigg)\bigg)......................(1)$$ Now $$\cos\bigg(\frac{2n\pi}{2n+1}\bigg) = \cos\bigg(\pi-\frac{\pi}{2n+1}\bigg)
=-\cos\bigg(\frac{\pi}{2n+1}\bigg)$$ $$\cos\bigg(\frac{(2n-1)\pi}{2n+1}\bigg) = \cos\bigg(\pi-\frac{2\pi}{2n+1}\bigg) =-\cos\bigg(\frac{2\pi}{2n+1}\bigg)$$ $$...........................................................$$ $$\cos\bigg(\frac{(n+1)\pi}{2n+1}\bigg) = \cos\bigg(\pi-\frac{n\pi}{2n+1}\bigg) =-\cos\bigg(\frac{n\pi}{2n+1}\bigg)$$ multiplying terms $k = 1$ with $k = 2n, k =2$ with $k = 2n-1\dots k = n$ with $k = n+1$ terms, from $......(1)$ I get $$\frac{\sin((2n+1)\theta)}{\sin(\theta)} = 2^{2n}\cdot\bigg(\cos(\theta)+\cos\bigg(\frac{\pi}{2n+1}\bigg)\bigg)\cdot \bigg(\cos(\theta)-\cos\bigg(\frac{\pi}{2n+1}\bigg)\bigg)\hspace{82pt}\cdot\bigg(\cos(\theta)+\cos\bigg(\frac{2\pi}{2n+1}\bigg)\bigg)\cdot\bigg(\cos(\theta)-\cos\bigg(\frac{2\pi}{2n+1}\bigg)\bigg)........\hspace{56pt}\bigg(\cos(\theta)+\cos\bigg(\frac{n\pi}{2n+1}\bigg)\bigg)\cdot\bigg(\cos(\theta)-\cos\bigg(\frac{n\pi}{2n+1}\bigg)\bigg)$$ $$ \hspace{35pt}= 2^{2n}\prod_{k = 1}^{n} \bigg(\cos^{2}(\theta)-\cos^{2}\bigg(\frac{k\pi}{2n+1}\bigg)\bigg).............(2)$$ How can I prove $$\frac{\sin((2n+1)\theta)}{\sin(\theta)} = (2n+1)\prod_{k = 1}^{n}\bigg(1-\frac{\sin^2(\theta)}{\sin^2\big(\frac{k\pi}{2n+1}\big)}\bigg)$$ from my calculation? Is my way of approaching this problem is ok or not? If it isn't give me one or a few hints.","['complex-analysis', 'trigonometry', 'sequences-and-series']"
4176464,Recursive factorization of words,"Let $\Sigma$ be an alphabet of cardinal $n$ . Let $T$ be the set of ordered binary tree whose nodes are labeled by words over $\Sigma$ , such that each leaf is labeled by a letter $a\in \Sigma$ and the parent of two nodes is labeled by the concatenation of their labels. Given such a tree $t\in T$ , we write $W_t$ for the set of words that appear as labels in $t$ , and $|W_t|$ for its cardinal. Given $u\in\Sigma^*$ , let $T_u$ be the subset of $T$ consisting of trees whose root is labeled by $u$ . I am interested in bounds on $$f(n,l)=\max_{u\in\Sigma^l}\min_{t\in T_u}|W_t|$$ in terms of $l$ when $n$ is constant, or in terms of both $l$ and $n$ . This quantity is useful because if we replace the tree by the corresponding directed acyclic graph, $|W_t|$ corresponds to the number of nodes in the DAG, and the quantity therefore correspond to the maximal number of nodes one may need to represent a word of length $l$ . I use this DAG structure in this answer . For now I only know that: $\boxed{l\le n \Rightarrow f(n,l)\ge l}$ If $l\le n$ then by taking a word $u$ that uses $l$ distinct letters, we get $|W_t|\ge l$ for any $t\in T_u$ by counting the leaves. $\boxed{l=2^k\Rightarrow f(1,l)=\log_2(l)+1}$ If $n=1$ and $l=2^k$ , then we can build a tree $t\in T_u$ that only uses the labels $a^{2^i}$ for $1\le i\le k$ , and we have $|W_t|=k+1=\log_2(l)+1$ words. Ideally, I would like a bound that either proves or disproves $f(2, l)=o(l)$ .","['combinatorics-on-words', 'combinatorics']"
4176508,Bounding sum of quartic deviations from sample mean,"I came across the following statement in The Jackknife and Bootstrap, Shao and Tu , p. 87: $$\sum_i(X_i-\bar X)^4\leq16\sum_i(X_i-\mu)^4,$$ where $\bar X$ and $\mu$ are the sample mean and expected value of the $X_i$ , respectively. Even without the term 16, the statement is well-known (for any $a$ , not only $\mu$ when we replace 4 by 2. My hunch is that the proof uses Pascal's triangle, as there are 16 terms in the l.h.s. when adding and subtracting $\mu$ and multiplying out: $$\sum_i(X_i-\bar X)^4=\sum_i(X_i-\mu)^4+4(X_i-\mu)^3(\mu-\bar X)+4(X_i-\mu)(\mu-\bar X)^3+6(X_i-\mu)^2(\mu-\bar X)^2+(\mu-\bar X)^4$$ My attempts at bounding the terms on the r.h.s. (other than the first one, which is already in the ""right"" format), including Hölder's inequality, however have not led anywhere useful.","['statistics', 'probability']"
4176515,How to prove the following $L^2$ convergence,"I have encountered this several times, but have no idea why its true. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $H(s,\omega): [0,t]\times \Omega\rightarrow \mathbb{R}$ be $\mathscr{B}[0,t]\times \mathcal{F}$ measurable. Moreover, for each fixed $\omega\in \Omega$ , $H(s,\omega)$ is continuous on $[0,t]$ . We assume that \begin{align*}
E\int_0^t H(s,\omega)^2\ ds < \infty
\end{align*} Define $H_n(s,\omega)$ as \begin{align*}
\textstyle H_n(s,\omega)= H(\frac{kt}{n}, \omega), \qquad \frac{kt}{n}\leq s< \frac{(k+1)t}{n}, 0\leq k\leq n-1
\end{align*} Then we have \begin{align*}
\lim_{n\rightarrow\infty}E\int_0^t (H_n(s,\omega)- H(s,\omega))^2\ ds= 0
\end{align*} It's clear that $H_n\rightarrow H$ a.s. We can't apply dominated convergence since this is Riemann type approximation. I tried to prove the sequence $H_n$ is cauchy in $L^2$ , but it seems does not work. Does anyone have any idea or comments?","['measure-theory', 'stochastic-calculus', 'probability']"
4176582,How to find the Skolem form of the following formula?,"I'm confused about finding a Skolem form of the following formula: F = $(\forall x)(P(x)\rightarrow(\forall y)((\forall z)Q(z,y)\rightarrow\neg(\forall z)R(y,z)))$ So following the algorithm I've done these transformations: F = $(\forall x)(\neg P(x)\vee(\forall y)((\exists z)\neg Q(z,y)\vee(\exists z)\neg R(y,z)))$ = $(\forall x)(\neg P(x)\vee(\forall y)((\exists z)\neg Q(z,y)\vee(\exists m)\neg R(y,m)))$ = $(\forall x)(\forall y)(\neg P(x)\vee(\neg Q(f(x,y),y)\vee\neg R(y,g(x,y))))$ = $\neg P(x)\vee(\neg Q(f(x,y),y)\vee\neg R(y,g(x,y)))$ But the solution on the textbook gives a different answer which is: F = $(\forall x)(\forall y)(\forall z)(\forall v)(\neg P(x)\vee\neg Q(z,y)\vee\neg R(y,v)))$ = $\neg P(x)\vee\neg Q(z,y) \vee \neg R(y,v)$ I can't quite understand. What's wrong with my own solution? And why here the $z$ is replaced with a constant instead of a function like $g(x,y)$ or $f(x,y)$ ?","['predicate-logic', 'first-order-logic', 'logic', 'solution-verification', 'discrete-mathematics']"
4176594,Is my method for computing $\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6}$ valid?,"I know that if $\lim_{x \to a}g(x)$ exists and $f(x)$ is continuous at $\lim_{x \to a}g(x)$ , then we can interchange the limit with $f(x)$ . That is, $$
\lim_{x\to a}f(g(x))=f(\lim_{x\to a}g(x)).
$$ So onto the limit in question: I'd like to show that $$
\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6}.
$$ I start off by letting $$
L = \lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}.
$$ Next, I apply the natural logarithm to both sides of the equation to give $$
\ln L = \ln\left(\lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}\right).
$$ But now, I would like to interchange the natural log with the limit on the right side of the equation. However, this seems not justified because I don't know, a priori, that $\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2}$ exists and if it does exist, that the natural log is even defined at the limit (what if it's $0$ or negative?). Thus, my method of interchanging the natural log with the limit seems circular. I need to assume the limit exists and is neither $0$ nor negative before proceeding with my computation. So here are my questions : Is it actually justified to interchange the natural log with the limit in this type of computation? Is it commonplace and accepted to assume the limit exists when going about these types of computations? If not, can someone propsose an alternative method for computing this limit without using the epsilon-delta definition?","['limits', 'calculus']"
4176596,"If $f:B\to\mathbb{R},\ B\subset\mathbb{R}$ is increasing then there is a sequence of strictly increasing functions whose pointwise limit is $f$","I have proved the following statement and I would like to know if my proof is correct and/or how it could be improved. Suppose $B\subset\mathbb{R}$ and $f:B\to\mathbb{R}$ is an increasing function. Prove that there exists a sequence $f_1, f_2,\dots$ of strictly increasing functions from $B$ to $\mathbb{R}$ such that $f(x)=\lim_{k\to\infty}f_k(x)$ for every $x\in B$ . My proof: The sequence defined by $f_k(x):=f(x)+\frac{x}{k},\ k\geq 1, x\in B$ satisfies the requirements for if we let $k\geq 1$ and $x,y\in B,\ x<y$ then $f_k(y)-f_k(x)=f(y)-f(x)+\frac{y-x}{k}>0$ so $f_k(y)>f_k(x)$ ie $f_k$ is strictly increasing and also, for any $x\in B$ we have $\lim_{k\to\infty}f_k(x)=\lim_{k\to\infty}(f(x)+\frac{x}{k})=\lim_{k\to\infty}f(x)+\lim_{k\to\infty}\frac{x}{k}=f(x)+0=f(x)$ . $\square$","['sequence-of-function', 'solution-verification', 'monotone-functions', 'real-analysis']"
4176613,Linear optimization for functions,"I have the following linear optimization problem. $$
\max \int_0^1 w(t) dt
$$ subject to $$
\int_0^1 w(t) \, x_i(t) \, dt \geq 0, \quad i=1,\dots,n 
$$ and $$
0 \leq w(t) \leq 1 \quad \text{for all} \quad t\in[0,1].
$$ Here, $x_1(t), \dots, x_n(t)$ are some given functions $x_i:[0,1]\to{\mathbb R}$ , and the optimization is over all measurable functions $w:[0,1]\to{\mathbb R}$ satisfying the constraints. I would like to prove that there exists a maximiser $w^*(t)$ such that for every $t\in[0,1]$ either $w^*(t)=0$ or $w^*(t)=1$ . This is ""obvious"" from the intuition that optimal solution for the linear program should be on the ""corner"" of the feasibility set. However, in this example we have uncountably many variables ( $w(t)$ for each $t\in[0,1]$ ) and uncountably many constraints. Can anyone suggest a good reference for study of such linear programs, and/or provide a direct solution for this problem?","['convex-optimization', 'lagrange-multiplier', 'linear-programming', 'functional-analysis', 'optimization']"
4176643,Is the $\sigma$-algebra generated by weak topology of (non-separable) Banach space equal Borel $\sigma$-algebra?,"Does the $\sigma$ -algebra generated by weak topology of Banach space equal Borel $\sigma$ -algebra for the norm topology? This question is useful in showing that weak measurability in the sense of Bochner is equivalent to measurability in the usual way when we equip the Banach space with the Borel $\sigma$ -algebra induced by the norm. I can cover the case where the Banach space is separable. Let $X$ our Banach space and denote the weak topology $\tau'$ and the $\sigma$ -algebra it generates $\sigma(\tau')$ . Since a closed ball is closed in the weak topology , all closed balls are in $\sigma(\tau')$ . Let $B(x,r)$ an open ball. Then $$B(x,r) = \bigcup_n \overline{B}(x,r-n^{-1}) \in \sigma(\tau').$$ Thus all open balls are in $\sigma(\tau')$ , and since the open balls generate the Borel sigma algebra (using here separability), $\sigma(\tau')$ contains the Borel $\sigma$ -algebra. The other inclusion is trivial, as the weak topology is weaker than the norm topology. I'm unsure whether the result holds when $X$ is not separable.","['banach-spaces', 'measure-theory', 'functional-analysis']"
4176646,Is this function differentiable? Is this directional derivative correct?,"I need to find the directional derivatives for all vectors $u=[u_1\ \ u_2]\in \mathbb R^2$ with $\|u\|=1$ at $P_0=(0,0)$ , and determine whether $f$ is differentiable at $P_0$ . $$f(x,y)=\begin{cases}
1 & y=x^2,x\neq 0\\
0 & \text{else}
\end{cases}$$ First of all, if $f$ is not continuous then can I always say it isn't differentiable? And my attemp was this: $$\lim_{t\rightarrow 0} \frac {f(P_0+tu)-f(P_0)} t = \lim_{t\rightarrow 0}
\begin{cases}
\frac{1}{t} & \text{else}\\
0 & u_1=0 \text{ or } u_1^2\neq u_2\\
\end{cases}$$ Does the fact that $\lim_{t\rightarrow 0}\frac {1}{t}$ does not exist say anything about f being differentiable? Because $D_if(P_0)$ both exist for $i=1,2$ . So I'd like to know if my calculation is correct, and if the continuous statement is true. Thanks!","['multivariable-calculus', 'calculus', 'solution-verification']"
4176683,"How many conjugacy classes of elements of order 7 are there in $GL(6, \mathbb{F}_2)$?","How many conjugacy classes of elements of order 7 are there in $GL(6, \mathbb{F}_2)$ ? I've been thinking the following, every element of order 7 must satisfy the next equation: $$x^7-1=0$$ We can express $$x^7-1=(x-1)(x^3+x+1)(x^3+x^2+1)$$ My intuition is that there are two conjugacy classes determined by the roots of the polynomials $(x^3+x+1)$ and $(x^3+x^2+1)$ , i.e, $Q=\{A\in GL(6, \mathbb{F}_2) : A^3+ A + I=0\}$ and $P=\{A\in GL(6, \mathbb{F}_2): A^3+A^2+I=0\}$ are the two conjugacy classes with elements of order 7. Also, I know that if some element $A\in GL(6, \mathbb{F}_2)$ is in $P$ (or in $Q$ respectively) then, every element in the conjugacy class of $A$ is in $P$ (or $Q$ ). Is my intuition right? Is there any other idea to solve this?","['matrices', 'general-linear-group', 'group-theory', 'abstract-algebra']"
4176704,"Is the set $S=\{ p(x) e^{-\alpha \|x\|^2} : p(x) \in \mathcal{P}, \alpha >0 \}$ dense in $L^2$ where $\mathcal{P}$ is a set of polynomials.","Let $\mathcal{P}$ be the set of all realvalued polynomials on $\mathbb{R}^2$ and define $$S=\{ p(x) e^{-\alpha \|x\|^2}, x\in \mathbb{R}^2 : p \in \mathcal{P}, \alpha >0 \}.$$ I have two questions, one minor and one major: (Minor) Is $S$ is a dense subset of $L^2(\mathbb{R}^2)$ ? (Major) What are some minimal assumptions on $\mathcal{P}$ that guarantee that $S$ is dense? For example, is it enough to consider only all homogeneous polynomials plus a constant. I think the answer to the first question follows from here where this result was shown for a one-dimensional case.","['hilbert-spaces', 'dense-subspaces', 'polynomials', 'functional-analysis']"
4176737,Proving exponential inequality without calculus,"Letting $a$ be a strictly positive parameter, one can use calculus to show that $x^a < \frac{1}{1+a-ax}$ for $x \in [0,1)$ . Is there a more direct way of showing this? Perhaps some well known inequality that I am not aware off? Thanks for any tips.",['algebra-precalculus']
4176748,How does the limit law $\lim_{x \to a}f\bigl(g(x)\bigr)=f\left(\lim_{x \to a}g(x)\right)$ work?,"I have $2$ questions regarding the following limit law: Suppose that $\lim_{x \to a}g(x)$ exists and is equal to $l$ , and that $f$ is
continuous at $l$ . Then, $\lim_{x \to a}f\bigl(g(x)\bigr)$ exists, and $$
\DeclareMathOperator{\epsilon}{\varepsilon} 
\lim_{x \to a}f\bigl(g(x)\bigr)=f(l)=f\left(\lim_{x \to a}g(x)\right) \, .
$$ My questions are: Have I stated this limit law correctly? How do you prove this limit law? Here is my attempted proof: Let $\epsilon>0$ . We wish to find a $\delta>0$ such that, for all $x$ , If $0<|x-a|<\delta$ , then $|f(g(x))-f(l)|<\epsilon$ . We are given that $f$ is continuous at $l$ , i.e. that there exists a $\delta'>0$ such that, for all $y$ , If $|y-l|<\delta'$ , then $|f(y)-f(l)|<\epsilon$ . We are also given that $\lim_{x \to a}g(x)=l$ , meaning that there is a $\delta>0$ such that If $0<|x-a|<\delta$ , then $|g(x)-l|<\delta'$ Since $g(x)$ is a number $y$ satisfying $|y-l|<\delta'$ , we get that $|f(g(x))-f(l)|<\epsilon$ . Hence, if $0<|x-a|<\delta$ , then $|f(g(x))-f(l)|$ , and so $\lim_{x \to a}f(g(x))=f(l)=f\left(\lim_{x \to a}g(x)\right)$ , completing the proof.","['real-analysis', 'continuity', 'calculus', 'solution-verification', 'limits']"
4176755,How to calculate $\lim_{x\to 0}\frac{4}{x}\big[\frac{x}{3}\big]$,"Question:-Calculate the following limit $$L=\lim_{x\to 0}\frac{4}{x}\bigg[\frac{x}{3}\bigg]$$ where, [ ] represents greatest integer function. There is also a similar limit which can be solved using squeeze theorem as follows $$ \lim_{x\to 0}\frac{x}{3}\bigg[\frac{4}{x}\bigg]$$ As $$x-1< [x] \leq x$$ $$\implies \frac{4}{x} -1< \bigg[\frac{4}{x}\bigg ] \leq \frac{4}{x} $$ $$\implies \frac{x}{3}\bigg(\frac{4}{x} -1\bigg)<\frac{x}{3} \bigg[\frac{4}{x}\bigg ] \leq \frac{x}{3}\bigg(\frac{4}{x}\bigg) $$ By squeeze theorem, we have $$ \lim_{x\to 0}\frac{x}{3}\bigg[\frac{4}{x}\bigg]=\frac43$$ But Squeeze theorem is not applicable on the given Limit So How can I do this? By desmos this limit  also equals $\frac43$ Thank you for your reply","['limits', 'calculus']"
4176766,Prove that $\sigma_{\text {ess}} (A)$ is a closed subset of $\mathbb R.$,"Let $A$ be a self-adjoint operator on a Hilbert space $\mathcal H.$ Let $E_A$ be the unique spectral measure associated to $A$ obtained from spectral theory for self-adjoint operators defined on the Borel- $\sigma$ -algebra of subsets of $\left [-\|A\|, \|A\| \right ]$ i.e. $$A = \int_{\left [-\|A\|, \|A\| \right ]} t\ dE_A(t).$$ Let $\sigma (A)$ denote the spectrum of $A.$ Then what I know is that for any self-adjoint operator $A$ on a Hilbert space $\mathcal H$ we have $\sigma (A) = \text {supp} (E_A),$ where $\text {supp} (E_A)$ denotes the support of $E_A.$ This shows that if $\lambda \in \sigma (A)$ then $E_A (\lambda - \varepsilon, \lambda + \varepsilon) \neq 0,$ for every $\varepsilon \gt 0.$ This leads us to the following subdivision of the spectrum $\sigma (A)$ of $A.$ An element $\lambda \in \sigma (A)$ is said to be an essential spectrum of $A$ if the range of the projection $E_A (\lambda - \varepsilon, \lambda + \varepsilon)$ is infinite dimensional for every $\varepsilon \gt 0.$ Otherwise we say that $\lambda$ is a discrete spectrum of $A.$ The collection of all essential spectrum of $A$ is denoted by $\sigma_{\text {ess}} (A)$ and the collection of all discrete spectrum of $A$ is denoted by $\sigma_{\text {disc}} (A).$ Now two results have been left as ( easy ) exercises which are the following $:$ $(1)$ $\sigma_{\text {ess}} (A)$ is a closed subset of $\mathbb R$ for any self-adjoint operator $A$ on a Hilbert space $\mathcal H.$ $(2)$ If $\lambda \in \sigma_p(A)$ has infinite multiplicity then $\lambda \in \sigma_{\text {ess}} (A),$ where $\sigma_p (A)$ denotes the point spectrum (or the collection of eigenvalues) of $A.$ But I find it difficult to prove the first one. I have tried by taking a sequence $\{\lambda_n\}_{n \geq 1}$ in $\sigma_{\text {ess}} (A)$ converging to $\lambda.$ Then the range of the projection $E_A (\lambda_n - \varepsilon, \lambda_n + \varepsilon)$ is infinite dimensional for every $\varepsilon \gt 0$ and for all $n \geq 1.$ But how does it guarantee that the range of the projection $E_A (\lambda - \varepsilon, \lambda + \varepsilon)$ is also infinite dimensional for all $\varepsilon \gt 0\ $ ? I have asked about it to our instructor. He told me that it is an one line argument . But I don't know why can't I able to see the proof. Also I don't have any idea about the second one. May be I am so stupid. Would anybody give me some suggestion here? I am totally confused at thus stage about how to proceed further. Any help regarding this will be warmly appreciated. Thanks! EDIT $:$ Finally I am able to prove the first one. Let us take $\varepsilon \gt 0$ arbitrarily. Let $\{\lambda_n\}_{n \geq 1}$ be a sequence in $\sigma_{\text {ess}} (A)$ converging to $\lambda \in \mathbb R.$ So there exists $N \geq 1$ such that $\lambda_n \in (\lambda - \varepsilon, \lambda + \varepsilon),$ for all $n \geq N.$ In particular $\lambda_N \in (\lambda - \varepsilon, \lambda + \varepsilon).$ Choose $\delta \gt 0$ small enough so that $(\lambda_N - \delta, \lambda_N + \delta) \subseteq (\lambda - \varepsilon, \lambda + \varepsilon).$ This implies that $E_A ((\lambda_N - \delta, \lambda_N + \delta)) \leq E_A ((\lambda - \varepsilon, \lambda + \varepsilon)).$ But this in turn implies that $$\text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right ) \subseteq \text {Range} \left ( E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right ).$$ Now since $\lambda_N \in \sigma_{\text {ess}} (A)$ it follows that $\text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right )$ is infinite dimensional and hence we have $\text {Range} \left (E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right )$ is infinite dimensional. This completes the proof. Now how do I prove the second one? Do anybody give any idea about it? Thanks!","['hilbert-spaces', 'self-adjoint-operators', 'spectral-theory', 'functional-analysis']"
4176784,"How group action of $G$ on set $\Omega$ acts on the vector space induced by $\Omega$, $\mathcal{X}(\Omega)$?","A left group action of $G$ on a set $\Omega$ is mapping $G \times \Omega \to \Omega$ . We define a space of $\mathcal{C}$ valued signals as $\mathcal{X}(\Omega.\mathcal{C}) = \{x:\Omega \to \mathcal{C}\}$ has a vector space property. As a typical illustration, take $\Omega = \mathbb{Z}_n \times \mathbb{Z}_n$ as a two dimensional $n \times n$ grids, $x$ is RGB image(i.e. a signal $x: \Omega \to \mathbb{R}^{3}$ ). Now if we want to obtain the group action of $G$ on the space $\mathcal{X}(\Omega)$ we have: \begin{align}
   (g \cdot x)(u) = x(g^{-1}u) \tag{1}
\end{align} where $g \in G, x \in \mathcal{X}, u \in \Omega$ . I got the equation $1$ from the paper, Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges (page 14 equation 3). The authors of this paper did not give any intuitive understanding of the equation $1$ also did not mention where to find the proof. Can anyone please help me to understand how this formula actually work?","['group-theory', 'group-actions', 'lie-groups']"
4176803,Evaluating $\int_0^1\frac{\ln(x)\ln(1-x)\ln(1+\sqrt{1-x^2})}{x}\mathrm{d}x$,"While I was working on computing $\sum_{n=1}^\infty\frac{{2n\choose n}H_{2n}^{(2)}}{4^n n^2}$ , I came across the integral: $$I=\int_0^1\frac{\ln(x)\ln(1-x)\ln(1+\sqrt{1-x^2})}{x}\mathrm{d}x.$$ I tried $x=\sin(u)$ then $\tan(u/2)=t$ and got $$I=\int_0^{\pi/2}\cot(u)\ln(\sin u)\ln(1-\sin u)\ln(1+\cos u)\mathrm{d}u$$ $$=\int_0^1\frac{1-t^2}{t(1+t^2)}\ln\left(\frac{2t}{1+t^2}\right)\ln\left(\frac{(1-t)^2}{1+t^2}\right)\ln\left(\frac{2}{1+t^2}\right)\mathrm{d}t.$$ Any idea? Thanks","['integration', 'harmonic-numbers', 'calculus', 'closed-form']"
4176846,Let's find solutions to two functional differential equations,"In order to prove the following Probability result: ""Find a family of random variables $X$ , having pdf $f$ , such that $X$ and $Y=f(X)$ have the same distribution."" Let's call $F(x)$ the CDF of the random variable $X$ and $f(x)$ its PDF.
Furthermore, the random variable $Y$ has CDF $K(y)$ and PDF $k(y)$ . I know that $y$ belongs to a set of values in which it can be considered valid the relation $y=f(x)$ . I'd like to ""investigate"" when the random variables $X$ and $Y$ have the same distribution, in the case of $f$ stictly increasing for $x\leq m$ and stricly decreasing for $x \geq m$ (where $m$ is the mode of the distribution). So we have to prove when $F(y)=K(y)$ . From a previous valid result, I know that, under these assumptions, the CDF of the random variable $Y$ is: $K(y)=2 F(f^{-1}(y))=2F(x)$ or, equivalently, $K(y)=2(1-F(f^{-1}(y))=2-2F(x)$ . In order to find the explicit expression of the PDF $f$ , using the relation $F(y)=K(y)$ (our thesis), I should prove that: in the case $f$ strictly increasing for $x \leq m$ , $$K(y)=2F(x)=F(y) \Leftrightarrow 2F(x)=F(f(x)) \Leftrightarrow 2F(x)=F(F'(x)).$$ So the first functional differential equation that should be solved is: $$2F(x)=F(F'(x))$$ in order to find the expression of $F$ and then obtain $f$ , derivating $F$ . in the case of $f$ strictly decreasing for $x \geq m$ $K(y)=2-F(x)=F(y) \Leftrightarrow 2-F(x)=F(f(x)) \Leftrightarrow 2-F(x)=F(F'(x))$ . The second functional differential equation to be solved is the following one: $$F(F'(x))+F(x)=2.$$ The solutions of the aforesaid equations should be in a certain way ""symmetrical"". I should solve those functional differential equations, but I have NO IDEA on HOW to solve them because I've never met this kind of equations before in my career. Could you help me, please?","['functional-equations', 'statistics', 'functional-analysis', 'probability', 'density-function']"
4176853,"Differentiating vector by matrix, optimization problem","I am currently writing a simulation, which includes an loss function at the end for optimization. In order to perform backpropagation I need to calculate some derivatives. My forward pass is the following function: $$
s(\alpha, \beta)=N - \sum_{i=1}^{N}\prod_{j=1}^{E_{max}}1-sigm(\alpha*I_{i,j}+\beta*D_{i,j})
\\ ´\\ \text{with: } N, E_{max} \in \mathbb{N_0}, \\ \alpha, \beta \in \mathbb{R}\\ I, D \in \mathbb{R}^{N\times E_{max}}
$$ 1. Is there a more concise way to calculate the row product of a matrix? While applying the chain rule I ran into the following issue: $$
f_{i,j}(\alpha,\beta) :=  sigm(\alpha*I_{i,j}+\beta*D_{i,j})\\
g_{i,j}(f):=sigm(f)\\
m_i(g):= \prod_{j=1}^{E_{max}}1-g_{i,j} \\
s(m):= N-\sum_{i=1}^{N}m_i\\
L(s):= \frac{1}{2}*(y-s)^2
$$ If I want to optimize for $\alpha$ , I need to calculate $\frac{dL}{d\alpha}$ So far I got: $\frac{dL}{ds}=-(y-s)$ $\frac{ds}{dm}=[-1, \dots, -1]$ 2. But how can I get $\frac{dm}{dg}$ ?
If I am not mistaken I would have to differentiate a vector function $m$ by a matrix $g$ ? 3. Is there a better way to get the derivative of $s$ directly?","['multivariable-calculus', 'matrix-calculus', 'optimization', 'gradient-descent', 'derivatives']"
4176865,Find the associated matrix to a linear transformation,"Let $D:\mathbb{P}_3[x]\to\mathbb{P}_2[x]$ be the linear transformation $f(x)\rightarrow f'(x)$ . I.e, the derivative of $f(x)$ . $\mathbb{P}_n[x]$ denotes the vector space of real polynomials of degree less or equal to $n$ . Find the associated matrix to $D$ with respect to the bases: $$\mathcal{B}=\{1,x,x^2,x^3\}  ~~\text{and}~~  \mathcal{C}=\{1,x,x^2\}$$ Find the associated matrix to $D$ with respect to the bases $\mathcal{B}'$ of $\mathbb{P}_3[x]$ and $\mathcal{C}'$ of $\mathbb{P}_2[x]$ $$\mathcal{B}'=\{x^3+1,x^2+x,x^2-x,x^3-1\}$$ and $$\mathcal{C}'=\{\frac{1}{2}x(x+1),1-x^2,\frac{1}{2}x(x-1)\}.$$ I solved part 1. and I obtained  the associated matrix, which is $$\begin{pmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{pmatrix}$$ However, I couldn't solve part 2. I don't really know how to proceed. Any help will be greatly appreciated.","['matrices', 'linear-algebra', 'polynomials', 'linear-transformations', 'derivatives']"
4176879,Technical Details in do Carmo's Riemannian Geometry,"I am confused about a proof in do Carmo's Riemannian Geometry . The following side-to-side screenshots are pages 227 - 228 in Section 4 of Chapter 10. I have included them so as to keep the question self-contained. The main post is directly below the pictures. If it helps, we may assume that $N$ is an embedded submanifold (the case I care about). For reference's sake, I will also include a screenshot of the ""Weingarten equation"" theorem from Chapter 6 of the same book. I am confused about do Carmo's proof of property (ii). do Carmo writes $$
\frac{DA}{ds}(0) = \left.\overline{\nabla}_{J(0)}A(s)\right|_{s = 0}.
$$ The right-hand side doesn't make a lot of sense to me. $A(s)$ is a vector field defined along a curve and not on an open subset of $M$ . If $A(s)$ is extendible to a vector field $X$ in a neighborhood of $p$ with $X(\alpha(s)) = A(s)$ for small enough $s$ , then the definition of $DA/ds(0)$ implies that $DA/ds(0) = \overline{\nabla}_{J(0)}X$ , and then we could continue the proof just fine. I cannot think of any other possible interpretation for the right-hand side of the above equation. But what if $A(s)$ is not extendible? Is there something that guarantees it in this case, or is this a missing assumption? Question 1: How do we choose such a vector field $X$ on $M$ near $p$ , such that for small enough $s$ , $X(\alpha(s)) = A(s)$ ? (Are we necessarily able to in this case, or is do Carmo omitting an important hypothesis?) If, for example, $\alpha'(0) = J(0) \neq 0$ , then $\alpha$ is locally an embedding, and so I can find local coordinates $(x^1,\dots,x^n)$ for $M$ with $\alpha(s) = (0, \dots, 0, s)$ , and then extend $A$ in the obvious way, But what if $J(0) = 0$ ? I do not see what to do in this case. It suffices in this case to prove that $J'(0) \in (T_pM)^\perp$ , but I don't know how to show this. Now, let's assume that we've chosen such an $X$ . I interpret the last equation as saying that $$
\left.\left\langle\left(\overline{\nabla}_{J(0)}A(s)\right)^\top,v\right\rangle\right|_{s = 0} = \left\langle \left(\overline{\nabla}_{J(0)}X\right)^\top,v\right\rangle = \left\langle -S_{\gamma'(0)}(J(0)), v \right\rangle,
$$ where the last equality supposedly follows from the Weingarten equation (reference above). In order to apply this, we have to choose our local extension $X$ in such a way that $X$ is normal to $N$ . Question 2: As a follow-up to Question 1, how can we choose $X$ normal to $N$ ? I apologize for the very long post. Any insights or help would be appreciated. I haven't been able to find anything on this online or in other textbooks, which is why I've decided to post this. If anyone could share any references with proofs of the fact do Carmo is proving, that would also be very helpful. (Of course, if I am making a fundamental misunderstanding and overcomplicating something, I would also welcome that as an answer.) Edit: Inspired by this post, I noticed that Question 1 can be whittled down to a specific case. If $\alpha'(0) \neq 0$ , then, as I remarked above, we can extend $A(s)$ . (The question of how to choose the extension normal to $N$ still stands.) If $\alpha'(s) = 0$ for all $s$ sufficiently close to $0$ , then $\alpha(s) \equiv p$ for small $s$ . In this case, $A(s)$ is a curve in the vector space $(T_pN)^\perp$ , so it follows that $J'(0) = DA/ds(0) = dA/ds(0) \in (T_pN)^\perp$ , which proves (ii) directly. It only remains to figure out what happens when we have a sequence $s_n \to 0$ with $\alpha'(s_n) \neq 0$ but $\alpha'(0) = 0$ . This, I have made no progress on.","['riemannian-geometry', 'differential-geometry']"
4176888,Typo in introductory statistics book,Mistake in book. This is an example from an indtroduction to the central limit theorem written by a professor at my university. Is this a typo?,['statistics']
4176893,Do irreducible sums span the same space?,"Irreducible decompositions Say that a decomposition $f(x,y) = \sum_i U_i(x)V_i(y)$ is irreducible if the $U_i$ are all linearly independent, as are the $V_i$ . The rank of a decomposition is the number of terms in the sum. In a previous question, I established that any two irreducible decompositions $f(x,y)=\sum_i U_i(x)V_i(y) = \sum_i P_i(x)Q_i(y)$ have the same rank. Moreover, the set $\{U_1, \ldots, U_n, P_1, \ldots, P_n\}$ is linearly dependent , as is the set $\{V_1,\ldots,V_n,Q_1,\ldots, Q_n\}$ . ( Linear independence when writing a function as a sum of functions. ) Question: I am trying to establish whether/when a stronger result might hold, namely that if the decompositions are irreducible, then the $\{U_i\}$ and the $\{P_i\}$ necessarily span the same space— that each $P_i$ is a linear combination of $U_i$ : If $f(x,y)=\sum_i U_i(x)V_i(y) = \sum_i P_i(x)Q_i(y)$ and both  decompositions are irreducible, then $\text{span}(\{U_i\}) = \text{span}(\{P_i\})$ . I know that if $n$ functions $f_i$ are independent, then there exist $n$ points $x_i$ such that the matrix $[f_i(x_j)]$ is invertible. So I can prove this result if there is a set of points $x_i$ such that both $[U_i(x_j)]$ and $[P_i(x_j)]$ are invertible. Or if the statement is false, perhaps there's an easy example of a particular $f$ and two decompositions where the conjecture fails. Alternatively, there might be a way to represent $f(x,y)$ as a decomposition involving both the $U_i$ and $P_i$ , then using the minimality property to winnow it down—but I haven't had much luck there. Any help is appreciated. I also tried defining $$D(\vec{\alpha})\equiv \text{det}([U_i(\alpha_j)]_{i,j}\cdot [P_i(\alpha_j)]_{i,j})$$ which, because of the multiplicative property of determinants, is identically zero unless there exists a collection of $n$ points $\alpha_1,\ldots,\alpha_n$ which simultaneously makes both matrices invertible. In short: Just how unique are irreducible decompositions? Are any two irreducible decompositions $\sum_i U_i(x)V_i(y) = \sum_i P_i(x)Q_i(y) $ linearly related to each other with $\text{span}(U_i)=\text{span}(P_i)$ , or are there decompositions that are significantly different from one another?","['nomography', 'linear-algebra', 'functional-analysis', 'wronskian']"
4176894,"Simple probability: $m$ balls are placed independently in container A with probability $a$, and B otherwise. Probability A has fewer than x balls?","Each of $m$ balls is placed independently into a container A with probability $0 <a<1$ , or in container B otherwise. What is the probability that container A will have fewer than $x$ balls, where $0<x\leq m$ ? Now immediately to me this looks like a binomial setup. So I would be tempted to say that the probability is $$P = \sum_{i=0}^{x-1} {m \choose i} a^i (1-a)^{m-1}$$ This seems correct from everything I know about the binomial distribution. Nevertheless I am uncomfortable with it because it seems to be summing up the probabilities of each of the events ""this specific set of $i<x$ balls is in A, and all other balls are in B$."" This makes me uncomfortable because the events are of course dependent in general. If you take one set of size $i>2$ containing ball number one, and another different set of size $i$ containing ball number one, then the events that the first set is the set of balls in A, is not independent from the event that the second set is the set in A. If someone could help to clarify where I am not understanding that would be much appreciated.","['binomial-distribution', 'probability']"
4176913,Trigonometric Simplification Question,"I was working through a trigonometry problem, and was having some difficulty so I decided to look at the solution. Here are the steps: $$\frac{\sin(2x+50^\circ)+\sin(150^\circ)}{\sin(2x+50^\circ)-\sin(150^\circ)}=\frac{\cos(50^\circ)-\cos(2x+50^\circ)}{\cos(50^\circ)+\cos(2x+50^\circ)}$$ $$\frac{\sin(2x+50^\circ)}{\sin150^\circ}=\frac{-\cos50^\circ}{\cos(2x+50^\circ)}$$ (Image that replaced text.) I am not exactly how the solution got from the first step to the second one. I would just like some clarification on the intermediate step.","['algebra-precalculus', 'trigonometry']"
4176935,Do Carmo's discussion of how $|b'(s)|$ measures how rapidly a curve pulls away from the osculating plane at $s$,"I am reading Do Carmo's Differential Geometry of Curves and Surface s, and a passage on page $18$ is confusing me. He writes: In what fallows we shall restrict ourselves to curves $\alpha(s)$ parametrized by arc length without singular points of order $1$ . We shall denote $t(s) = \alpha'(s)$ , the unit tangent vector of $\alpha$ at $s$ . Thus $t'(s) = k(s)n(s)$ (where $k(s)$ is the curvature of $\alpha$ ). The unit vector $b(s) = t(s) \times n(s)$ is normal to the osculating plane and will be called the binormal vector at $x$ . Since $b(s)$ is a unit vector, the length of $|b'(s)|$ measures the rate of change of the neighboring osculating planes with the osculating plane at $s$ ; that is, $b'(s)$ measures how rapidly the curve pulls away from the osculating plane at $s$ in a neighborhood of $s$ . I see that because $t(s), b(s)$ are both of norm $1$ and perpendicular that their cross product has length $1$ . I am not sure why since $b(s)$ is a unit vector, that the length $|b'(s)$ | mesasures how rapdidly the curve pulls away from the osculating plane at $s$ . How does the norm of $b'(s)$ play into things? If I look at what $b'(s)$ is, well its $\frac{d}{dt} (t(s) \times n(s)) = \frac{dt}{ds}\times n(s) + t(s) \times \frac{dn}{ds} $ , and well from here I'm not really sure how to proceed. Any insights appreciated.",['differential-geometry']
4176980,Lebesgue integral : Changing the $\sum$ and $\int$,"I want to calculate $\displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x)\ dx$ in Lebesgue integral theory. I calculated this using the theorem below. Theorem Let $\{f_n \}_{n=1}^{\infty}$ be a sequence of functions that are Lebesgue-measurable and non negative. And let $f:=\sum_{n=1}^{\infty} f_n$ . Then, $\displaystyle\int f dx=\sum_{n=1}^{\infty} \displaystyle\int f_n (x) dx.$ My calculation \begin{align}
\displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x) \ dx
&=\displaystyle\int \dfrac{x}{1-x}(-\log x) \cdot \chi_{(0,1)} (x) \ dx\\
&=\displaystyle\int \bigg(\sum_{k=1}^{\infty} x^k\bigg)\cdot (-\log x) \cdot \chi_{(0,1)} (x) \ dx\\
&=\displaystyle\int \sum_{k=1}^{\infty} \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx\\
&=\sum_{k=1}^{\infty}  \displaystyle\int \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx \ (\text{ using the Theorem) }\\
&=\sum_{k=1}^{\infty}  \displaystyle\int_0^1 x^k (-\log x)  \ dx \\
&=\sum_{k=1}^{\infty} \Bigg(\bigg[\dfrac{x^{k+1}}{k+1} (-\log x)\bigg]_0^1+ \displaystyle\int_0^1 \dfrac{x^{k}}{k+1} \ dx\Bigg) \ (\text{Integration by parts})\\
&=\sum_{k=1}^{\infty} \Bigg(0+ \dfrac{1}{(k+1)^2}\Bigg)
=\sum_{k=1}^{\infty}\dfrac{1}{(k+1)^2}.
\end{align} But I wonder if I can change $\sum$ and $\displaystyle\int.$ For each $k$ , $x^k(-\log x)\chi_{(0,1)}(x)$ is non negative but is it Lebesgue-measurable? And why?","['integration', 'calculus', 'lebesgue-integral', 'real-analysis']"
4176991,"If all circles have constant geodesic curvature, does the surface have constant curvature?","Let $(S, g)$ be a connected two dimensional smooth manifold which is equipped with a smooth Riemannian metric. Let $\nabla$ be the associated Levi-Civita connection. Consider a smooth unit-speed curve $\gamma$ on $S$ which parameterizes the boundary of a compact simply connected set with smooth boundary, and denote by $J_{\gamma}$ the unique inward pointing vector field along $\gamma$ such that $\{ \dot{\gamma}, J_\gamma\}$ is an orthonormal frame. Let $D_t$ denote covariant differentiation along $\gamma$ . Since $g(\dot{\gamma}, \dot{\gamma}) = 1$ , compatibility of the Levi-Civita connection shows $g(D_t \dot{\gamma}, \dot{\gamma}) = 0$ . Hence orthonormal expansion shows $D_t \dot{\gamma} = g(D_t \dot{\gamma}, J_\gamma) J_\gamma$ . And so the signed geodesic curvature at a point of the unit-speed curve $\gamma$ is defined to be the value of $g(D_t \dot{\gamma}, J_\gamma)$ at that point, and this expression shows the unsigned geodesic curvature , by definition $|D_t \dot{\gamma}|$ , equals the absolute value of the signed geodesic curvature. See [0] for background on these concepts and this thread for some interesting history on curvature of curves. There are at least two distinct concepts in Riemannian geometry which generalize the concept of a circle in Euclidean geometry. Note that a geodesic ball is the diffeomorphic (!) image of a disk that is centered at the origin under an exponential map at a point. A geodesic circle is the boundary of a geodesic ball. The Riemannian distance between two points in $S$ is the infimum of lengths of piecewise regular curves connecting them.
A metric circle is a locus of points in a surface which have constant Riemannian distance from a given point. Proposition 6.11 of [0] shows that a geodesic circle is a metric circle. A set is geodesically convex if every pair of points is connected by a unique minimizing geodesic. Since minimizing curves are geodesics up to reparameterization (Theorem 6.4 of [0]), and Riemannian manifolds are locally geodesically convex (Theorem 6.17 of [0]), small enough metric circles are geodesic circles. Therefore these two concepts are locally the same. Another concept which generalizes the Euclidean circle is that of a closed curve with constant geodesic curvature; I specify closed curve because the hyperbolic plane has constant geodesic curvature curves which are not closed curves. My question is about characterizing the case that this concept locally agrees with the previous two. There are several questions on this website asking to show that geodesic circles on constant curvature surfaces have constant geodesic curvature. So my specific question is a kind of converse to this. \begin{align*}
&\textrm{If every point in $S$ has a neighborhood such that all geodesic circles within the neighborhood } \\
&\textrm{and centered the point have constant geodesic curvature, does $S$ have constant curvature?}
\end{align*} [0] Lee, John M. , Introduction to Riemannian manifolds , Graduate Texts in Mathematics 176. Cham: Springer (ISBN 978-3-319-91754-2/hbk; 978-3-319-91755-9/ebook). xiii, 437 p. (2018). ZBL1409.53001 .",['differential-geometry']
4176996,How to find the general solution of $ \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t} $?,"How to find the general solution of $$ \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t} $$ My solution: I solve for the auxiliary equation which is $ m^2 + m -6 = 0$ so that the complementary equation $y_c$ is $C_1 e^{2x} + C_2 e^{-3x} $ where $C_1$ and $C_2$ are arbitrary constants. And the particular equation is $$y_p = 0 $$ since if we let $y_p= Ae^{2t} $ , take its first and second derivative, and substitute the result in our original equation, we we will get that $A=0$ Therefore, my answer is $y(t) = C_1 e^{2x} + C_2 e^{-3x} + 0 $ . Is this correct?","['ordinary-differential-equations', 'analysis', 'calculus', 'solution-verification', 'derivatives']"
4177009,A proof using mathematical induction,"By observing the base cases for $t=2,3$ , I conjectured the following general inequality which I don't know is true or false: $$\frac{(n_{1}+n_{2}+ \cdot \cdot \cdot n_{t})(n_{1}+n_{2}+ \cdot \cdot \cdot n_{t}-1)\cdot \cdot \cdot (n_{1}+n_{2}+ \cdot \cdot \cdot n_{t}-t+1)}{t!}+\frac{(n_{1}+n_{2}+ \cdot \cdot \cdot n_{t-1})(n_{1}+n_{2}+ \cdot \cdot \cdot n_{t-1}-1)\cdot \cdot \cdot (n_{1}+n_{2}+ \cdot \cdot \cdot n_{t-1}-t+2)}{(t-1)!}+\cdot \cdot \cdot+ \frac{(n_{1}+n_{2})(n_{1}+n_{2}-1)}{2!}+n_{1} +2 \leq \frac{(n_{1}+n_{2}+ \cdot \cdot \cdot+n_{t})^{t}}{t^{t-2}} $$ I tried to prove it using mathematical induction and at the last step of mathematical induction, I left with the following inequality to prove: $$\frac{(n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1})(n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1}-1)\cdot \cdot \cdot (n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1}-k)}{(k+1)!}+\frac{(n_{1}+n_{2}+\cdot \cdot \cdot n_{k})^{k}}{k^{k-2}} \leq \frac{(n_{1}+n_{2}+\cdot \cdot \cdot n_{k+1})^{k+1}}{(k+1)^{(k+1)-2}}$$ . I feel that the second term on the left-hand side will play a big role as its denominator is $k^{k-2}$ which could be helpful to obtain the denominator $(k+1)^{(k+1)-2}$ on the right-hand side of the equation. Further, the following obvious inequality related to first term on the left-hand side could also be useful: $$\frac{(n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1})(n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1}-1)\cdot \cdot \cdot (n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1}-k)}{(k+1)!} \leq \frac{(n_{1}+n_{2}+\cdot \cdot \cdot+n_{k+1})^{k+1}}{(k+1)!}$$ . Any hint or explicit proof will be a great help.","['elementary-number-theory', 'combinations', 'combinatorics', 'discrete-mathematics']"
4177022,Prove or disprove by induction that $a_n = b_n $ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have this task that gives me really a hard time: For $$ n \in \mathbb{N_0} $$ $a_n$ is defined as $$ a_n= (-1)^{(n+1)}+ 1 + -3n + 2n^2 $$ and $b_n$ as: $$ b_n =\begin{cases}
          a_n \quad &\text{for} \, 0 ≤ n ≤ 2, \\
          b_{n-1}+b_{n-2}-b_{n-3}+8 \quad &\text{for} \, 3 ≤ n \\
     \end{cases}
$$ Prove or disprove by induction that $ a_n = b_n $ for $n \in \mathbb{N} $ Can anyone help me with this? I have a hard time finding the right induction for this and its kinda important that i understand it. Would be great to see how to approach this properly. Thanks in advance!","['calculus', 'induction', 'discrete-mathematics', 'recursion']"
4177038,"Find the Jacobian of $f(x,y,z)=x^y$","Let $f(x,y,z)=x^y$ . I am trying to find the Jacobian $f’(a,b,c)$ without using partial derivatives, i.e. using the Chain Rule and several other elementary results on component functions etc.. Since $x^y=\exp(y\cdot\ln(x))$ , using the identity function $\pi$ we can write $$f(x,y,z)=\exp\left(\pi^2 \cdot \ln(\pi^1)\right)(x,y,z).$$ The proof then seems to require the Chain Rule, with $g(x)=\exp(x)$ and $h(x,y,z) =\pi^2 \ln(\pi^1)(x,y,z)$ , so that $f=g\circ h $ and $f’(a,b,c)= g’\left(h(a,b,c)\right)\cdot h’(a,b,c)$ . From here, I can calculate the solution $$\left[ba^{b-a}\; \; a^b \ln(a)\; \; 0\right].$$ However,in order to apply the Chain Rule, we require $h:\mathbb{R}^n\to \mathbb{R}^p$ and $g: \mathbb{R}^p \to  \mathbb{R}^m $ so that $f: \mathbb{R}^n \to \mathbb{R}^m$ is defined by $f= g\circ h$ . To get the right solution, however, I had to use $g’(h(a,b,c))=[a^b]$ . Question: How can we use the function $g:\mathbb{R}\to\mathbb{R}$ in the above proof for $f’(x)$ ?","['multivariable-calculus', 'derivatives', 'real-analysis']"
4177052,Does the order of nested quantifiers matter for $\exists x \forall y P(x) \vee Q(y)$?,"I fail to see difference between $\exists x \forall y P(x) \vee Q(y)$ and $\forall y\exists x  P(x) \vee Q(y)$ .
after all, there is no relation between x & y, and all we need to do is to check if one of the statements is true for the overall statement to be true since the or relation $\vee$ . like,
for every $x$ there is at least a $y$ that makes the statement $P(x) \vee Q(y)$ true for every $y$ there is at least an $x$ that makes the statement $P(x) \vee Q(y)$ true.","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
4177096,How well can a continuous distribution be approximated by a countable discrete distribution?,"Let $X$ be a random variable taking values on $\mathbb{R}^N$ , with probability density function $f:\mathbb{R}^N\rightarrow \mathbb{R}_{\ge 0}$ . Consider a new random variable $Y$ taking values in the countable set $(y_n)_{n\in\mathbb{N}}\subseteq \mathbb{R}^N$ , with respective probabilities $(p_n)_{n\in\mathbb{N}}$ , so $P(Y=y_n)=p_n$ and $\sum_{n=0}^\infty{p_n}=1$ . Without loss of generality, we may assume that $p_n$ is weakly decreasing in $n$ . By choosing $y_n$ and $p_n$ appropriately, can we ensure (non-trivial) error bounds of the something vaguely like one the following forms are satisfied? $$\forall x\in\mathbb{R}^N\setminus\{y_n|n\in\mathbb{N}\},\,\forall \epsilon>0,\,|P(\|X-x\|_2<\epsilon)-P(\|Y-x\|_2<\epsilon)|\le C \epsilon^{-\kappa},$$ for some $C>0$ and $\kappa>0$ , possibly depending on $N$ . AND/OR $$\forall x\in\mathbb{R}^N\setminus\{y_n|n\in\mathbb{N}\},\,\forall \epsilon>0,\,|P(\|X-x\|_2<\epsilon)-P(\|Y-x\|_2<\epsilon)|\le C P(\|X-x\|_2<\epsilon),$$ for some $C\in [0,1)$ , AND/OR $$\forall x\in\mathbb{R}^N\setminus\{y_n|n\in\mathbb{N}\},\,\lim_{\epsilon\rightarrow 0}{\frac{P(\|X-x\|_2<\epsilon)}{P(\|Y-x\|_2<\epsilon)}}=1.$$ I am not particularly attached to the precise form of these results. I am just curious if having countably many points means you can get in some sense ""close"" to the true, in a way that you can never with finitely many (as with finitely many, sufficiently small balls will contain $0$ $y_n$ ). I.e. is there any sense in which you can do better with countably many support points than you can with finitely many? For example, suppose we take $p_n={(n+1)}^{-1-\theta}{[\sum_{k=0}^\infty{{(k+1)}^{-1-\theta}}]}^{-1}$ for some small $\theta>0$ . Then it seems plausible that there could be an algorithm for constructing $y_n$ that proceeded by drawing points from $X$ and discarding them if they are too close to previously drawn points. My worry though is that the early points will end up with a kind of ""rain shadow"" around them. Sets close to $y_1$ , but not containing $y_1$ will end up with lower mass than they should have. Can this error be bounded?","['approximation', 'measure-theory', 'probability-theory', 'probability']"
4177124,"Question about improper integral, Munkres Problem 15.5","I have a question about improper integral from Munkres' Analysis on Manifold chapter 15. This is problem 5: What I have done was set up the double iterated integral over A and B and showed that the first one goes to infinity and the second one has a defined value and exist. My question is whether that is sufficient to prove that the first (improper) integral does not exist and the second one exist? The reason is because the integral may or may not exist, while the iterated integral may exist but be different than the integral. Both A and B are simple regions so Fubini's theorem can apply, but I think it needs to be a bounded region. So what I should do is to build a increasing sequence of compact rectifiable sets $U_n$ so that $U_1$ is in $U_2$ and so on and the union of all $U_n$ is the region A or B. How should I construct these sequence of sets?","['integration', 'multivariable-calculus', 'real-analysis']"
4177154,Find equation for a parabolic line that goes through two points in 3D space,"I'm currently working on some computer graphics program where I want to show a line that ""bounces"" from point $A$ to point $B$ , so I need an equation through which I can plug in values and interpolate the points that would trace the bounce. Given two points, $A$ and $B$ , in three dimensions, how do I determine the equation of a parabola that goes through $A$ and $B$ ? Some additional information that might be helpful: Assume that a point $C$ is the midpoint between $A$ and $B$ Assume that a point $D$ is the vertex of the parabolic line The length of line segment $\overline{CD}$ is a known variable/value","['geometry', '3d']"
4177181,Volume of a pyramid as a determinant?,"I have three given points, A, B and C, each of them is a corner of a pyramid. Another corner is located in origo. The task is to set up a determinant to describe the pyramids volume. Unfortunately, my book and Wikipedia won´t agree on how to do this, that´s why I´m asking you guys. PS. The follow up question is if the volume would be any different if the position vectors ( a , b and c ) and origo all where located in the same plane? Any help would be very appreciated!","['determinant', 'linear-algebra']"
4177193,"Is there a ""uniqueness theorem"" for non-normal ODE?","I say an ODE $F(x, y, y^{(1)}, \dots , y^{(n)})=0$ is normal if it can be written in a form $y^{(n)} = f(x, y, y^{(1)} , \dots , y^{(n-1)})$ . It is well-known for normal ODEs (satisfying good condition), there's a uniqueness theorem for initial value problem (Picard-Lindelof theorem). This result helps me completely determine the solutions of normal ODEs. Then, what about non-normal ODEs? For instance, I learned that $y = xy' - y'^2/4$ (Clairaut's form) has the general solution $y=Cx-C^2/4$ and singular solution $y=x^2$ in my textbook. However, in this case, general solution and singular solution can be ""connected"" continuously so that there can be another solutions, like $y=x^2(x>0), y=0(x \leq 0)$ . My textbook does not mention these ones. I feel weird about this. I want to determine all the possibility of the solutions of non-normal ODEs. The question is: (1) Is it possible?
(2) If so, how to do it? Is there a general result? I'm new to the theory of ODEs. Any help would be appreciated.",['ordinary-differential-equations']
4177215,Multidimensional version of inverse transform sampling,"I want to generalize the inverse transform sampling technique to a multidimensional setting.
I have only seen this for the circle, as described in the motivation below, but I can't seem to find any good resources on the generalized problem. The general setting I want to solve Let $A = \prod_{i=1}^n (a_i,b_i) \subseteq \mathbb R^n$ be a rectangular cuboid and let $$ \Phi \colon A \to M $$ be a diffeomorphism onto a bounded set $M \subseteq \mathbb R^n$ .
I want to find a measurable function $$ \Psi \colon ((0,1)^n, \mathcal B_{(0,1)^n}, \lambda_{(0,1)^n}) \to (A,\mathcal B_A)$$ such that the random variable $$ \Phi \circ \Psi \colon ((0,1)^n, \mathcal B_{(0,1)^n},\lambda_{(0,1)^n}) \to (M, \mathcal B_M)$$ has uniform distribution with respect to $\lambda_M$ . Note that for a Borel-measurable set $S \subseteq \mathbb R^n$ , we denote by $\mathcal B_S$ the Borel sigma-algebra on $S$ , and by $\lambda_S$ the Borel-measure or Lebesgue-measure on $S$ . [ Please correct me in the following if you find any errors, but it seems to be true. ] With the identity $f = \lvert \operatorname{Det}D\Phi \rvert / \int d\lambda_M$ , the question I ask above is the same as asking the following. Let $f \colon A \to [0,\infty)$ be a measurable function such that $\int f d\lambda_A = 1$ .  Define the measure $\mu_f : = \int f d\lambda_A$ on $(A, \mathcal B_A)$ .  We want to find a measurable function $$\Psi \colon ((0,1)^n, \mathcal B_{(0,1)^n}) \to (A, \mathcal B_A) \;,$$ such that the resulting random variable $$\Psi \colon ((0,1)^n, \mathcal B_{(0,1)^n}, \lambda_{(0,1)^n}) \to (A, \mathcal B_A) \;,$$ has uniform distribution with respect to $\mu_f$ .
This is the same as finding such a $\Psi$ such that $\lambda_{(0,1)^n} \circ \Psi^{-1} = \mu_f$ . For $n=1$ , this is exactly inverse transform sampling (as shown below). Motivation If we want to generate points in a circle $D^2$ randomly but uniformly on a computer, it is a bit tricky.
We cannot easily say ""create points uniformly on a circle"", as most programming languages only provide uniformly generated random points on an interval.
But we have the polar coordinate transformation $$ \Phi \colon (0,R) \times (0,2 \pi) \to D^2 \;,$$ so we can randomly choose points on those two intervals, right? Wrong! What we would get is the following. This polar coordinate transformation ""stretches and squashes"" space non uniformly, making the points denser for smaller radii and less dense for larger radii.
So we need to somehow transform the randomly generated points before we transform them to the disc in the end. The answer is to randomly generate points uniformly on $(0,1)^2$ , and then apply the function $$\Psi \colon (0,1)^2 \to (0,R) \times (0,2\pi)\;, \; (r,\theta) \mapsto (R\sqrt{r}, 2\pi\theta) \;,$$ and only then apply the polar coordinate transformation $\Phi$ , after which we get the desired result. I want to generalize this. My problem I totally understand the intuition behind it.
There are many resources that give a good explanation for this specific example, but I see no satisfying proof of the result in a way that I can generalize it . https://blogs.sas.com/content/iml/2016/03/30/generate-uniform-2d-ball.html (which is where I got the images) https://stats.stackexchange.com/questions/481543/generating-random-points-uniformly-on-a-disk https://stackoverflow.com/questions/5837572/generate-a-random-point-within-a-circle-uniformly/50746409#50746409 https://quantinterview.github.io/Random-Point-Circle/ I struggle to generalize this problem. My try to solve it The inverse transform sampling method allows the following.
(Note:  All sigma algebras are implied to be the borel/lebesgue sigma algebra.) Let $f \colon (a,b) \to [0,\infty)$ be a density function (i.e. it has to be measurable and $\int_{(a,b)} f d\lambda = 1$ ).
Define the probability density measure $\mu \colon = (E \mapsto \int 1_E f d\lambda_{(a,b)})$ . We can then find a measurable function $\Psi \colon (0,1) \to (a,b)$ , such that $\lambda_{(0,1)} \circ \Psi^{-1} = \mu$ , namely $\Psi = F^{-1}$ for $F = (x \mapsto \int 1_{(a,x)} f d\lambda_{(a,b)})$ , the cumulative distribution function. I tried to do multiple fubinis where I integrate every coordinate fully except for one, after which I have the one dimensional version of this problem.
But then it didn't seem to all tie up nicely in the end.
That solution to the problem, if it worked, also would mean that every coordinate gets transformed independently to $A$ , which doesn't feel right.
Or does that work?","['cumulative-distribution-functions', 'measure-theory', 'uniform-distribution', 'probability-theory']"
4177245,Characteristic functions inequality,"How to show that for any random variables $X,Y$ with characteristic functions $\phi_X, \phi_Y$ we have: $$\sup_{\xi \in \mathbb{R}} |\phi_X(\xi) - \phi_Y(\xi)| \leq 2P(X \neq Y)?$$ My attempt: First, I was considering an easier case, when $X,Y$ have densities $f_X, f_Y$ . In that case, we have: \begin{align}
|\phi_X(\xi)  - \phi_Y(\xi)| = \big|  \int_\mathbb{R} e^{i\xi t} (f_X(t)-f_Y(t)) \, dt \big| \leq \int_\mathbb{R} |f_X(t)-f_Y(t)| \, dt \leq\\ \leq \int_\mathbb{R} f_X(t) \, dt + \int_{\mathbb{R}} f_Y(t) \, dt = 2.
\end{align} But since $X,Y$ have densities $Z = X-Y$ does as well, so $P(Z=z)=0$ for any $z \in \mathbb{R}$ , so $P(Z \neq 0) = 1$ , so $2 P(X \neq Y) = 2.$ Is that correct? If so, how to generalise it for any random variables? Or maybe is there a completely different solution?","['characteristic-functions', 'probability-theory', 'probability', 'random-variables']"
4177249,The problem for conditional expectation.,"The joint probability density function for random variables $X$ , $Y$ is given by $$f(x, y)=\begin{cases}
2xy  & \text{if } 0\leq x\leq 2y\leq 2 \\ 0 &  \text{otherwise}
 & 
\end{cases}.$$ When the conditional expectation of $X$ is $E(Y | X=aY)=\frac{3}{10}$ , what is the real number $a(>1/2)$ ? My first solution is $$f(y \mid x=ay)=\frac{f(ay, y)}{f_{x}(ay)}=\frac{2ay^2}{\int_{0}^{\frac{1}{a}}2ay^2 dy}=3a^3y^2$$ $$\frac{3}{10}=E(Y\mid X=aY)=\int_{0}^{\frac{1}{a}} 3a^3y^3 dy=\frac{3}{4a} \Rightarrow a=\frac{5}{2}\,.$$ My second solution is Let $Z=\frac{X}{Y}, \;  W=Y$ .
Then, $X=YZ, \: \: Y=W, \;$ , the Jacobian is $W$ and $g(z,w)=f(wz, w)w=2zw^3 \; (\frac{1}{2}\leq z\leq \frac{1}{w}, \; 0\leq w\leq 2)$ $$g(w \mid z=a)=\frac{g(a, w)}{g_{Z}(a)}=\frac{2aw^3}{\int_{0}^{\frac{1}{a}}2aw^3 dw}=4a^4w^3$$ $$\frac{3}{10}=E(W\mid Z=a)=\int_{0}^{\frac{1}{a}} 4a^4w^4 dw=\frac{4}{5a} \Rightarrow a=\frac{8}{3}\,.$$ Why do I get two different values of $a$ ?","['conditional-expectation', 'probability-theory', 'probability']"
4177276,Problem book recommendations on complex manifolds,"I came across the book on Cauchy Riemann manifolds, ""CR manifolds and tangential Cauchy Riemann complexes"" . The book does not have a problem section. I would be grateful if anyone recommends some problem books on this topic ""Complex manifold"" and that would also serve as a supplement to this book.","['several-complex-variables', 'smooth-manifolds', 'complex-analysis', 'manifolds', 'complex-manifolds']"
4177323,Solving factorial equation $ x!=\left(x-1\right)! + 96 $,"I'm stuck with a problem that apparently is simple. I need to find $x$ in following equation: $$
x!=\left(x-1\right)! + 96
$$ How can I solve it? After some passage I've found: $$
\left(x-1\right)\left(x-1\right)! = 96
$$ But then I don't know how to continue...","['algebra-precalculus', 'factorial']"
4177329,Sequence of random variables is exchangeable iff every subsequence is equivalently distributed,"I'm trying to prove (or find a reference for) the following result: Let $E$ be a Polish space. A family $(X_n)_{n \in \mathbb N}$ of $E$ -valued random variables is exchangeable if and only if for every choice of natural numbers $1 \leq n_1 < n_2 < \ldots$ , we have $(X_1, X_2, \ldots) \stackrel{\mbox{$\mathcal{D}$}}{=} (X_{n_1}, X_{n_2}, \ldots)$ , where $\stackrel{\mbox{$\mathcal{D}$}}{=}$ denotes equivalence in distribution. A family of random variables as above is exchangeable if and only if given any finite permutation $\varrho : \mathbb N \to \mathbb N$ , the stochastic processes $(X_n)_{n \geq 1}$ and $(X_{\varrho(n)})_{n \geq 1}$ are equivalently distributed. I'm having trouble proving ""if"". I'm not sure how to extract a finite permutation from a choice of subsequences, so I'm guessing instead I'll have to use de Finetti's theorem. This requires me to find a random measure $\Xi$ for which, given $\Xi$ , we have that $X_1$ (or any other $X_n$ ) is $\Xi$ -distributed. One strategy would be to construct a suitable tight collection of measures $\left(\tilde\mu_i\right)_{i \in I} \subseteq \mathcal M_1\left(\mathcal M_1(E)\right)$ , apply Prohorov's theorem to find a weak limit $\tilde\mu$ , and choose a random variable $\Xi$ with values in $\mathcal M_1(E)$ so that $\mathbb P_\Xi = \tilde\mu$ . This is one way that de Finetti's representation theorem can be proven; in this proof, the tight collection of measures is $\tilde{\mathcal F} := \left(\mathbb P_{\xi_n(X)}\right)_{n \geq 1}$ , where given $x \in E^{\mathbb N}$ (note $X = (X_n)_{n \geq 1}$ is an $E^\mathbb N$ -valued random variable), $\xi_n(x)$ is the empirical distribution $\xi_n(x) = \frac 1 n \sum_{i=1}^n \delta_{x_i}$ . Proving that $\tilde{\mathcal F}$ is tight doesn't rely on exchangeability of $X$ , so we have the weak limit point $\tilde\mu \in \mathcal M_1(\mathcal M_1(E)$ of $\tilde{\mathcal F}$ , and a $\mathcal M_1(E)$ -valued random variable $\Xi$ so that $\tilde\mu = \mathbb P_{\Xi}$ . But in the proof of de Finetti's theorem that I'm trying to adapt, they show $\mathbb P_{(X_1, \ldots, X_k)} = \mathbb P_{\Xi^{\otimes k}}$ (and thus $X_1$ is $\Xi$ -distributed given $\Xi$ ) by showing that given $f_1, \ldots, f_k \in C_b(E)$ , letting $F(x) = f_1(x_1) \cdots f_k(x_k)$ , $$
\mathbb E\left[F(X) \right] = \lim_{i \to \infty} \mathbb E\left[\int f_1 \,d\xi_{n_i}(X) \cdots \int f_k\,d\xi_{n_i}(X)\right] = \mathbb E\left[\int F\,d\Xi^{\otimes k}\right] 
$$ which does require exchangeability of $X$ (specifically the first limit equality above). So my goal is to prove something similar to this limit assuming $(X_1, X_2, \ldots) \stackrel{\mbox{$\mathcal{D}$}}{=} (X_{n_1}, X_{n_2}, \ldots)$ for any subsequence. Does anyone have a suggestion for how to go about this? I'd accept a reference as well as an answer. Addendum: The proof of de Finetti's theorem that I was trying to adapt is in Section 13.4 of Achim Klenke's Probability Theory: A Comprehensive Course .","['stochastic-processes', 'random-variables', 'probability-theory', 'reference-request']"
4177347,How come $\sin(x^3)$ is treated like a composite function but $\sin(x)$ isn't?,"Preface: I'm not talking about sin in particular; any arbitrary trig function can be used. I just started learning the chain rule. I noticed for a certain example: when applying chain rule to $\sin(x^3)$ , it's treated as two nested functions, [1] $\sin(x^3)$ , and [2] $x^3$ . But when simply given some function, $\sin(x)$ , the derivative is just $\cos(x)$ . How come it's not treated as two functions, [1] $\sin(x)$ and [2] $g(x) = x$ ?","['calculus', 'derivatives', 'chain-rule']"
4177479,"Find a one-dimensional sufficient statistic for $\theta$ given that $f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x)$","Assume that $(X_1, X_2, X_3, \dots, X_n)$ is a random sample of the distribution having the following probability distribution function (PDF): $$f(x;\theta)=\frac{1}{\theta^2}x e^{-\frac{x}{\theta}} I_{(0,\infty)}(x), \quad \theta >0$$ where $I_{(0,\infty)}(x)$ is an indicator function on the set $(0,\infty)$ (meaning that if $x$ belongs to that set, the output of the function is $1$ , otherwise it's $0$ ). Question : Find a one-dimensional sufficient statistic for $\theta$ . I'm stuck at many things: (1) What is a ""one-dimensional"" sufficient statistic? I know that a sufficient statistic somehow summarizes the data such that if we only see that statistic, we will do the same as if the real data was shown to us. (2) Assuming that we know what a one-dimensional sufficient statistic for $\theta$ is, how should I proceed further? I mean, is there a systematic way to find a sufficient statistic? or we just guess it and try to prove that it is actually a sufficient statistic? Note: I also saw this post which seems relevant. However, I am not sure how to use the factorization theorem (if it can be useful in my case!). What are the factors here? And how should I get rid of the indicator function?","['statistics', 'probability-distributions']"
4177492,Finding a quadratic equation given its two roots. Does this method exist?,"This is my first post in a math exchange, however not my first post in an ""exchange"" forum. During one of my standardized testing preparation classes today, I was asked to find any quadratic equation given roots: $$
1-2\sqrt{3} , 1+2\sqrt{3}.
$$ This is a common problem in the particular standardized testing that I am preparing for. I came up with the following answer to the question in about 10 seconds: $$
(x-1)^2 = 12
$$ When solving: $$
\sqrt{(x-1)^2} = \sqrt{12}
$$ $$
x-1 = \pm 2 \sqrt{3}
$$ $$
x = 1 \pm 2 \sqrt{3}
$$ All it took to solve the problem was to realize that $\pm 2\sqrt 3$ was the same as $\pm\sqrt{12}$ . I like to think that it was a smart solution to the problem in question, and the reason why I bring up how much time it took me to do it is because once I thought of solving the problem in this way, it became very easy to think of solutions to similar problems. It is faster than factoring out. Now, my question is: How could I relate this method of finding the quadratic equation in a problem where both roots are rational numbers? For example: 2 and -3 Would it even be useful to think of problems in this way? And lastly, does something similar to this already exist? All answers are appreciated as long as they are informative and/or helpful. Thanks in advance.","['algebra-precalculus', 'quadratics']"
4177501,"If $2\sin\theta+\cos\theta=\sqrt3$, what is the value of $\tan^2\theta+4\tan\theta$?","If $2\sin\theta+\cos\theta=\sqrt3$ , what is the value of $\tan^2\theta+4\tan\theta$ ? $1)1\qquad\qquad2)2\qquad\qquad3)3\qquad\qquad4)5$ First I tried plugging in some values for $\theta$ like $0,\frac{\pi}4,\frac{\pi}3,...$ but neither of these known angles worked. But by doing it I realized that for $\theta=\frac{\pi}4+k\pi\quad$ , $\tan^2\theta+4\tan\theta=5\quad$ and $2\sin\theta+\cos\theta\neq\sqrt3$ Hence the fourth choice is wrong. Also tried to expanding, $$\tan^2\theta+4\tan\theta=\dfrac{\sin^2\theta}{\cos^2\theta}+\dfrac{4\sin\theta}{\cos\theta}=\dfrac{\sin^2\theta+4\sin\theta\cos\theta}{\cos^2\theta}$$ But can't continue even writing $4\sin\theta\cos\theta=2\sin2\theta$ doesn't help.","['algebra-precalculus', 'trigonometry']"
4177504,Confusion over Continuous Functional Calculus,"I have the following problem:  As far as I understand, the continuous functional calculus gives an isomorphism from the continuous functions over the spectrum of a normal element to the $C^*$ -algebra that the normal element and the unit generate. More specifically, if $A$ is a unital $C^*$ -algebra and $a\in A$ is normal, then we have a star-isomorphism $\Gamma_A:C(\sigma_A(a))\cong C^*(a,1_A)$ so that $\Gamma_A(1)=1_A$ and $\Gamma_A(z)=a$ , where $z$ is the identity map. Suppose now that $B\subset A$ is another unital $C^*$ -algebra (with unit $1_B$ ) that contains the above element $a$ . Then we get ""another"" continuous functional calculus, a star isomorphism $\Gamma_B:C(\sigma_B(a))\cong C^*(a,1_B)$ with $\Gamma_B(1)=1_B$ and $\Gamma_B(z)=a$ . By spectral invariance, we have that $\sigma_A(a)\cup\{0\}=\sigma_B(a)\cup\{0\}$ . By the above, we have that if $p$ is a polynomial in $z,\bar{z}$ with constant term $0$ , then $\Gamma_A(p)=\Gamma_B(p)$ . Now if $f$ is a continuous function defined on $\sigma_A(a)\cup\{0\}$ satisfying $f(0)=0$ , then by Weierstrass approximation we can find a sequence of polynomials $(p_n)$ in $z,\bar{z}$ such that $p_n\to f$ uniformly on $\sigma_A(a)\cup\{0\}$ . Thus $\Gamma_A(f)=\Gamma_B(f)$ . In other words, the continuous functional calculus when performed on functions that fix the origin is independent of the $C^*$ -subalgebra we are working in. Is this correct? If not, what is wrong? If it is correct, please look at the following. I feel very awkward about this result: Suppose that we have $C^*$ -algebras $B\subset A$ and suppose that $B$ is unital with unit $1_B$ . Now let $b\in B$ be a positive element with $\varepsilon1_B\leq b$ , for some $\varepsilon>0$ . Then $\sigma_B(b)\subset[\varepsilon,\|b\|]$ , so $b$ is invertible in $B$ , since $0$ is not in the spectrum. Moreover, $\sigma_A(b)\cup\{0\}=\sigma_B(b)\cup\{0\}$ . If $0\not\in\sigma_A(b)$ (which implies that $A$ is unital and that $b$ is invertible in $A$ ) then $\sigma_A(b)=\sigma_B(b)\subset[\varepsilon,\|b\|]$ . We can define $f(t)=1/t$ on $\sigma_A(b)=\sigma_B(b)$ and set $f(0)=0$ . This defines a continuous function on $\sigma_A(b)\cup\{0\}=\sigma_B(b)\cup\{0\}$ and by the above, $\Gamma_A(f)=\Gamma_B(f)$ , i.e. the inverse of $b$ in $B$ is the same as the inverse of $b$ in $A$ . More specifically, $1_B=1_A$ , which seems odd to me. We just proved that: $B\subset A$ is an inclusion of unital $C^*$ -algebras (the units being $1_B$ and $1_A$ respectively) and we have an element $b\in B$ with $\varepsilon1_B\leq b$ and also $b$ is invertible in $A$ . Then $1_B=1_A$ . IF my understanding of the continuous functional calculus is correct, can you give me a more compelling argument to convince me that the reasoning in the above proof is not flawed?","['c-star-algebras', 'spectral-theory', 'functional-analysis']"
4177601,Using dimension formula to prove subspace dimension formula,"I've always suspected the formulas $\dim F+G = \dim F + \dim G - \dim F\cap G$ and $\dim V = \dim  \mathrm{Ker} T + \dim \mathrm{Im} T$ were related someway. So I tried proving the former using the latter. It is easy to see that the direct outer product $F\times G$ has dimension the sum of dimensions of $F$ and $G$ . Define $\pi: F\times G \twoheadrightarrow F+G$ by $\pi(f,g) = f+g$ . It is clearly surjective. I claim that the kernel of $\pi$ is the subspace $H$ of $(F\cap G)^2$ of elements of form $(f, -f)$ , and therefore of dimension equal to the one of $F\cap G$ . Therefore the former result results from applying the dimension formula to $\pi$ . Is this correct?","['linear-algebra', 'linear-transformations']"
4177608,Arctan integral $ \int_{0}^{\infty}\frac{\arctan(x)}{x^{2}+k^{2}}$,"Is there a closed form for the integral $$ \int_{0}^{\infty}\frac{\arctan(x)}{x^{2}+k^{2}}$$ for $\forall  k \ge 1 $ ? Well, I was able to get the closed form for the case where $|k|\le1$ , and it is of the form $$ \int_{0}^{\infty}\frac{\arctan(x)}{x^{2}+k^{2}}=\frac{1}{2k}\left(\textrm{Li}_{2}(k)-\textrm{Li}_{2}(-k)-2\log(k)\tanh^{-1}(k)\right)$$ I would highly apperciate if any one could come up with a technique to solve the above mentioned problem","['integration', 'polylogarithm', 'definite-integrals', 'special-functions']"
4177609,"If sets $A, B$ are countable infinite then also $A \cup B$","This is my first „own“ proof I created. Is it ok? Are there things fishy? I‘d appreciate if someone could go thru my proof step by step and point out mistakes if there are any. $\Bbb N^{2n}$ means the even natural numbers and $\Bbb N^{2n+1}$ means the odd natural numbers. Theorem: If sets $A, B$ are countable infinite then also $A \cup B$ . Proof: We assume that sets $A,B$ are countable infinite, i.e. $|A| = |\Bbb N| = |B|$ , i.e. $f: A \to \Bbb N$ and $g: B \to \Bbb N$ are bijective. We also know that $|\Bbb N^{2n}| = |\Bbb N| = |\Bbb N^{2n+1}|$ . It follows $|A| = |\Bbb N^{2n}|$ and $ |B| = |\Bbb N^{2n+1}|$ , i.e. $f‘: A \to \Bbb N^{2n}$ and $g‘: B \to \Bbb N^{2n+1}$ are bijective. Now we assume a function $h: A \cup B \to \Bbb N$ thru the rule: if $x \in A$ then $f‘(x)$ , if $x \in B-A$ then $g‘(x)$ . The function $h$ is bijective. First it is injective because every preimage $x \in A \cup B$ has exactly one image due to the distribution of $f‘(x)$ and $g‘(x)$ . It is also surjective because every image in $\Bbb N$ has exactly one preimage due to $f‘$ , $g‘$ . But since $h$ is bijective it follows by definition $|A \cup B| = |\Bbb N|$ and that‘s by definition a case of countable infinity for $A \cup B$ .",['elementary-set-theory']
4177633,How to solve $a^x+b^x=1$ (solve for $x$),"I am sorry if this is too easy question for this site, but I really can't find the solution... $a^x+b^x=1$ Tyma Gaidash asked for context: I don't have much to add, I was trying to understand how fast population grow assuming that everyone born exactly same amount of time (for example everyone have 2 children 1 born 40 years after the parent born and the other 20 years after the parent born, is it faster than having a twins after 28 years) If I assume the population formula is $a^x$ and I need to find a
I get that the time it will take to the population multiply itself is the solution to the equation: $\sum_{i=1}^{n}2^{-t_i/x}=1$ where $t_i$ it after how many year children I born after his parent. I hoped that there is a way to solve this equation (at least for 2 children ) but I haven't found one.","['calculus', 'closed-form', 'transcendental-equations']"
4177654,Would it be possible to skip undergraduate college with self study/online courses [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 3 years ago . Improve this question Hi I was wondering if it were possible to skip undergraduate and apply to a graduate school. I have been taking some math courses through CTY a somewhat accredited program offered by Johns Hopkins university. They offer up to real analysis 1 and complex analysis 1. If I take up to there, would it be possible to skip undergraduate having self studied the higher level courses i.e topology/further analysis courses.",['analysis']
4177677,Does dropping off-diagonal elements from a positive semidefinite matrix preserve positive semidefinite property?,"Let $M$ be a symmetric positive semidefinite matrix of size $n \times n$ . Randomly drop any number of off-diagonal elements by setting them to zero on both the upper and lower triangle (keeping symmetry). Is the resulting matrix still positive semidefinite? For example: $$
\begin{pmatrix}
2.7732 & 0.670677 & 0.141774 \\
0.670677 & 2.50917 & 0.393801 \\
0.141774 & 0.393801 & 2.3329
\end{pmatrix}
\rightarrow
\begin{pmatrix}
2.7732 & 0.670677 & 0 \\
0.670677 & 2.50917 & 0 \\
0 & 0 & 2.3329 
\end{pmatrix}
$$ is still positive semidefinite. I observed this numerically by generating matrices following this algorithm , but it certainly may just be due to the distribution that those matrices came from. The number of elements to drop on one of the sides is $r \in \{ 1, 2, \dots, n(n-1)/2 \}$ . Thanks","['matrices', 'linear-algebra', 'positive-semidefinite']"
4177688,Is there an explicitly known Diophantine equation whose solvability is undecidable in ZFC?,"Reading the Wikipedia article on Diophantine sets , I was intrigued by the following remark near the end: Corresponding to any given consistent axiomatization of number theory, one can explicitly construct a Diophantine equation which has no solutions, but such that this fact cannot be proved within the given axiomatization. Unfortunately, no further references are provided concerning this particular statement. Now I cannot help but wonder: has such an 'undecidable' Diophantine equation ever been constructed for ZFC itself? In general (i.e., not specific to ZFC), is anything known about the minimal complexity of such undecidable equations (such as minimal number of variables, minimal degree)?","['number-theory', 'logic', 'diophantine-equations', 'computational-complexity', 'set-theory']"
4177700,Automorphisms of Cayley graphs of $\mathbb{Z}$,"My goal is to show that there are no finite generating sets $A$ and $B$ such that $\mathrm{Cay}(\mathbb{Z},A)$ is isomorphic to $\mathrm{Cay}(\mathbb{Z}\times \mathbb{Z}_2, B)$ . My idea for this is to start by noting that $\mathrm{Cay}(\mathbb{Z}\times \mathbb{Z}_2, B)$ has a non-trivial automorphism of order $2$ with exactly two fixed vertices, induced by the map $(a,b) \mapsto (-a,b)$ . Then I have the feeling that there should be no such automorphism of $\mathrm{Cay}(\mathbb{Z},A)$ . This is obvious if we pick the generating set $A = \{1\}$ ; in this case $\mathrm{Cay}(\mathbb{Z},A)$ is just the real line and if a graph automorphism has two fixed vertices then it is trivial. However, this is less clear when the generating set for $\mathbb{Z}$ is $\{n_1, \dots, n_k\}$ with $\gcd(n_1, \dots, n_k) = 1$ . Can anyone provide a proof that an automorphism of $\mathrm{Cay}(\mathbb{Z},A)$ with two fixed points must be trivial, or provide a counterexample? Different approaches to my original problem are also welcome. Edit. Some clarifications: my question is exactly as formulated in kabenyuk's comment. I want to show that there are no generating sets $A$ of $\mathbb{Z}$ and $B$ of $\mathbb{Z} \times \mathbb{Z}_2$ such that the Cayley graphs $\mathrm{Cay}(\mathbb{Z},A)$ and $\mathrm{Cay}(\mathbb{Z}\times \mathbb{Z}_2,B)$ are isomorphic. To fix things, here is the definition of the Cayley graph I am working with: for a group $G$ with generating set $S$ not containing the identity, the Cayley graph $\mathrm{Cay}(G,S)$ is the graph with vertex set $G$ and edge set $\{\{ g, gs \} : g \in G , s \in S \cup S^{-1} \}$ .","['graph-theory', 'group-theory', 'cayley-graphs']"
4177761,No polynomial with integer coefficients satisfies the conditions,"Prove that there is no polynomial with integer coefficients $P(x)$ such that $P(7)=5$ and $P(15)=9$ . In general; how to know if there exists a polynomial with integer coefficients $P(a)=b$ and $P(c)=d$ , where $a,b,c$ , and $d$ are all integers?","['integers', 'polynomials', 'diophantine-equations']"
4177770,Geometric reason why this determinant can be factored to (x-y)(y-z)(z-x)?,"The determinant $\begin{vmatrix}
    1 & 1 &1 \\ 
    x & y & z \\
    x^2 & y^2 &z^2 \\
  \end{vmatrix}$ can be factored to the form $(x-y)(y-z)(z-x)$ Proof: Subtracting column 1 from column 2, and putting that in column  2, \begin{equation*}
\begin{vmatrix}
1 & 1 &1 \\ 
x & y & z \\
x^2 & y^2 &z^2 \\
\end{vmatrix} 
= 
\begin{vmatrix}
1 & 0 &1 \\ 
x & y-x & z \\
x^2 & y^2-x^2 &z^2 \\
\end{vmatrix} 
\end{equation*} $
= z^2(y-x)-z(y^2-x^2)+x(y^2-x^2)-x^2(y-x)
$ rearranging the terms, $
=z^2(y-x)-x^2(y-x)+x(y^2-x^2)-z(y^2-x^2)
$ taking out the common terms $(y-x)$ and $(y^2-x^2)$ , $
=(y-x)(z^2-x^2)+(y^2-x^2)(x-z)
$ expanding the terms $(z^2-x^2)$ and $(y^2-x^2)$ $
=(y-x)(z-x)(z+x)+(y-x)(y+x)(x-z)
$ $
=(y-x)(z-x)(z+x)-(y-x)(z-x)(y+x)
$ taking out the common term (y-x)(z-x) $
=(y-x)(z-x) [z+x-y-x]
$ $
=(y-x)(z-x)(z-y)
$ $
=(x-y)(y-z)(z-x)
$ Is there a geometric reason for this? The determinant of this matrix is the volume of a parallelopiped with sides as vectors whose tail is at the origin and head at x,y,z coordinates being equal to the columns(or rows) of the matrix. $^{[1]}$ So is the volume of this parallelopiped equals $(x-y)(y-z)(z-x)$ in any obvious geometric way? References [1] Nykamp DQ, “The relationship between determinants and area or volume.” From Math Insight. http://mathinsight.org/relationship_determinants_area_volume","['determinant', 'geometry']"
4177783,"Comparing the product topology and dictionary order topology on $[0,1] \times [0,1]$.","This is a question from Munkres which has been answered a lot of times but my approach has been a bit different: I have been able to prove in the previous problem that $\mathbb{R}_d \times \mathbb{R}$ where $\mathbb{R}_d$ is the discrete topology  has the same topology as $\mathbb{R} \times \mathbb{R}$ in the dictionary order topology. Then shouldn't $I \times I$ in the dictionary order topology and $I_d \times I$ be same? If they are same then we start with the basis of $I_d \times I$ Where $I_d$ denotes the discrete topology. So the basis of $I_d \times I$ will be $\{x\} \cup (a, b)  $ . I am trying to show that the product topology is finer than the dictionary order topology. let $(x_0,y_0) \in \{x_0\} \times (a, b) $ .Then, $ a < y_0 < b$ . We see that, $(x_0,y_0) \in (x_0,x_0) 
\times (a, b) \subset \{x_0\} \times (a, b)  $ (**I am not sure about this because I think $ \{1/4\} \times (0,1)$ will not be open in the product topology but it is a basis of $I \times I$ in the dictionary order topology **) Now, the dictionary order topology is finer than the product topology : We see that, $(x_0,y_0) \in      \{x_0\} \times (a, b) \subset (x_0-\epsilon,x_0+\epsilon) 
\times (a, b) $ Where am I going wrong?","['general-topology', 'solution-verification', 'analysis']"
4177811,Show that $(k!)!$ is divisible by $(k!)^{(k-1)!}$,"Question : Show that $(k!)!$ is divisible by $(k!)^{(k-1)!}$ . Answer : Suppose we've $k!$ objects, where k objects of 1st kind, k objects of 2nd kind,...k objects of $(k-1)!$ kind (why?) . And from the well known formula we get : $$ \frac{(k!)!}{k!k!...k!((k-1) times)**(why?)**} = \frac{(k!)!}{(k!)^{(k-1)!}}$$ My question is from where we do we get the idea of having $(k-1)$ in the calculation. I'm confused. Please help me.","['combinatorics', 'discrete-mathematics', 'factorial']"
4177818,Taylor Series coordinate free form,"I am reading about the ""coordinate-free form"" of a Taylor series, it says, Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be differentiable, $x$ and $\delta r$ be vectors then $$f(x+\delta r)=f(x)+[\delta r \cdot \nabla f(x)]+[\delta r\cdot (H(x) \delta r)] + ...$$ where $H(x)$ is the Hessian with entries $H(x)_{i,j}=f_{x_ix_j}$ and $x_i$ for $1\le i\le n$ is the standard basis I don't understand what comes after the ""..."" and would appreciate it if someone could tell me(or point me to an existing explanation that I cannot find). I know already what taylor series are in multiple dimensions, but I haven't seen them written in this form before.","['multivariable-calculus', 'calculus', 'definition', 'taylor-expansion']"
4177840,Taking infinite-width limit on multilayer perceptrons can simplify Taylor expansion,"Settings When we apply Taylor expansion to a trained network function $f(x; \theta^*)$ around the initialized value of the parameters $\theta$ .
Suppose $\theta$ and the derivatives $f(x;\theta)$ are both scalar, we obtain $$
f(x;\theta^{*}) = f(x;\theta) + (\theta^{*}-\theta) \frac{df}{d\theta}+\frac{1}{2}(\theta^{*}-\theta)^2\frac{d^2f}{d\theta^2}
$$ Now consider a neural network architecture that has a width of $n$ , and a fixed depth of $L$ . According to The Principles of Deep Learning Theory (PDLT) (p.7) , if we take the limit, a.k.a the infinite-width limit, on an idealized network, $$
\lim_{n\rightarrow \infty} p(f^{*})
$$ we can simplify the distribution over trained networks $p(f^{*})$ . Without giving any proof, the book gives two claims: Claim 1 All the higher derivative terms $\frac{d^kf}{d\theta^k}$ for $k \geq 2$ will effectively vanish, meaning we only need to keep track of two terms, $$
f, \frac{df}{d\theta}
$$ Claim 2 The distributions of these random functions will be independent $$
\lim_{n\rightarrow \infty} p \left( f,\frac{df}{d\theta},\frac{d^2f}{d\theta^2}, \ldots \right) =p(f)p \left( \frac{df}{d\theta} \right)
$$ Would someone explain the above claims?","['machine-learning', 'taylor-expansion', 'probability', 'neural-networks']"
4177853,Recursive chain rule,"Given the following equations: $$
\begin{aligned}
o_t&=\sigma(x_t, h_{t-1};W_o) \\
\tilde{c}_t&=\tanh(x_t, h_{t-1};W_g) \\
f_t&=\sigma(x_t, h_{t-1};W_f) \\
i_t&=\sigma(x_t, h_{t-1};W_i) \\
c_t&=f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t} \\
&=\sigma(x_t, h_{t-1};W_f)\odot{c_{t-1}}+\sigma(x_t, h_{t-1};W_i)\odot{\tanh(x_t, h_{t-1};W_g)} \\
h_t&=o_t\odot\tanh(c_t) \\
\end{aligned}
$$ Lower case variables are vectors and $W$ 's are matrices. $x_t$ is a new input at each time. The subscript $t$ denotes time from $t=1,\dots N$ , where $h_0=c_0$ are zeros vectors. As can be seen these are recursive equations from which $h_t$ comes from a function of $c_t$ which comes from a function of $c_{t-1}$ . And I'm trying to derive: $$\frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial}{\partial c_{t-1}}(f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t}).$$ So my question is given that $f_t$ , $i_t$ and $g_t$ are functions of $h_{t-1}$ which is a function of $c_{t-1}$ does that matter? Is this derivative just equal to $f_t$ ? Or do the other terms need to be accounted for such that the derivation is something like: $$\frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial{f_t}}{\partial{c_{t-1}}}\odot{c_{t-1}}+f_t\odot\frac{\partial{c_{t-1}}}{\partial{c_{t-1}}}+\frac{\partial{i_t}}{\partial{c_{t-1}}}\odot{\tilde{c}_t}+i_t\odot\frac{\partial{\tilde{c}_t}}{\partial{c_{t-1}}}?$$ Any help would be greatly appreciated.","['hadamard-product', 'multivariable-calculus', 'partial-derivative', 'derivatives', 'chain-rule']"
4177858,What are projective spaces?,"I am trying to understand Projective Spaces, but I have to be honest is not so easy for me. I premise that I am not a mathematics student and I have not studied anything about topology and only a very little of differential geometry. Unfortunately, I have to learn about projective spaces even if I don't have any background, so after spending much time trying to understand it, I am here asking for help. I have found some video lectures and some explanations about this topic, but I have also to understand terms like rank 1 orthonormal projectors and chordal distance, which I really can't understand what they are. What I would like to ask is: Does it exist some reference which explains these arguments easily or can someone give me a simple explanation of these concepts?","['general-topology', 'algebraic-geometry', 'differential-topology', 'algebraic-topology']"
4177878,Evans' PDE Problem 6 Chapter 6 - Existence and uniqueness of weak solutions of Poisson's equation with mixed Dirichlet-Neumann boundary conditions,"Suppose $U \subset \mathbb R^n$ is an open, bounded and connected set, with smooth boundary $\partial U$ consisting of two disjoint, closed sets $\Gamma 1$ and $\Gamma 2$ . Define what it means for $u$ to be a weak solution of Poisson's equation with mixed Dirichlet-Neumann boundary conditions: \begin{cases}
-\Delta u = f \ \ \ \text{in $U$} \\
u = 0 \ \ \ \text{on $\Gamma_1$} \\
\frac{\partial u}{\partial \nu} = 0 \ \ \text{on $\Gamma_2$}.
\end{cases} Discuss the existence and uniqueness of weak solutions.
[Source: Evans' PDE 2nd edition, page 366, problem 6] My attempt: Let $u \in C^\infty(U)$ be a solution of the problem and let $H^1_{\Gamma_1}(U) = \{v \in H^1(U)\colon v = 0 \text{ on } \Gamma_1 \}$ , which is a Hilbert space with respect to the inner product in $H^1$ . To define a weak solution of the Poisson's equation multiply it by $v \in H^1_{\Gamma_1}(U)$ and integrate by parts \begin{align*}
\int_U fv \,dx
&= -\int_U \Delta u \cdot v \,dx \\
&= \int_U Du \cdot Dv \,dx - \int_{\partial U} \frac{\partial u}{\partial \nu} \cdot v \, dS \\
&= \int_U Du \cdot Dv \,dx - \int_{\Gamma_1} \frac{\partial u}{\partial \nu} \cdot v \, dS - \int_{\Gamma_2} \frac{\partial u}{\partial \nu} \cdot v \, dS \\
&= \int_U Du \cdot Dv \,dx
\end{align*} where the second integral on RHS is zero since $v \in H^1_{\Gamma_1}(U)$ and the third integral is zero since $\frac{\partial u}{\partial \nu} = 0$ on $\Gamma_2$ . Is it ok to take $v$ only in $H^1_{\Gamma_1}(U)$ or $v$ must be taken in $H^1(U)$ ? Define the bilinear map $B:H^1(U)\times H^1_{\Gamma_1}(U) \to \mathbb R$ by $B[u,v] = \int_U Du \cdot Dv \, dx$ . To prove the existence and uniqueness of weak solutions we have to prove that $B$ satisfies the hypotheses of Lax-Milgram, i.e. that $B$ is continuous (linear + bounded) and coercive. For the boundness we can use Cauchy-Schwarz and the definition of Sobolev norm $||u||_{H^1} = ||u||_{L^2}+||Du||_{L^2}$ , which implies $||u||_{H^1} \ge ||Du||_{L^2}$ . So $$
|B[u,v]|
\le \int_U |Du \cdot Dv| \, dx
\le ||Du||_{L^2(U)}||Dv||_{L^2(U)}
\le ||u||_{H^1(U)}||v||_{H^1(U)}.
$$ Is all good until here? What about the proof that $B$ is coercive? I read something about the trace operator $T$ , maybe is better to define $B$ through $T$ ?","['boundary-value-problem', 'elliptic-equations', 'functional-analysis', 'partial-differential-equations']"
4177893,What is the intuition behind this proof of Silver's Theorem?,"Context There is a well-known result on the Generalized Continuum Hypothesis for singular cardinals : Silver's Theorem: Let $\aleph _{\lambda }$ be a singular cardinal such that its cofinality is greater than $\omega$ . If for every $\alpha < \lambda$ , $2^{\aleph _{\alpha }}=\aleph _{\alpha +1}$ , then $2^{\aleph _{\lambda }}=\aleph _{\lambda +1}$ . Proof Sketch: For simplicity, let us assume that $\lambda = \omega _1$ , the first uncountable ordinal number. For each $\alpha < \omega _1$ , let $A_{\alpha }= \mathcal{P}(\omega _{\alpha })$ , so $|A_{\alpha }|=2^{\aleph _{\alpha }}=\aleph _{\alpha +1}$ . For every set $X \subseteq \omega _{\omega _1}$ , let $f_X \in \prod _{\alpha < \omega _1}A_ {\alpha }$ be the function defined by $$f_X (\alpha )= X \cap \omega _{\alpha }.$$ The set $F= \{ f_X \mid X \in \mathcal{P} (\omega _{\omega _1}) \}$ is a family of almost disjoint functions, meaning that for any $f_X$ and $f_Y$ in $F$ there is some $\alpha < \omega _1$ such that $f(\beta ) \neq g(\beta )$ for all $\beta \ge \alpha$ . Now, let $U$ be an ultrafilter on $\omega _1$ that extends the closed unbounded filter . Thus every set $S \in U$ is stationary . Let us define a relation $<$ on $F$ as follows: $$f < g \quad \text{if and only if} \quad \{ \alpha < \omega _1 \mid f(\alpha ) < g(\alpha ) \} \in U.$$ It can be shown that $<$ is a linear ordering on $F$ . Now, if $f,g \in F$ and $g <f$ , then $g \in F_f$ , where $$F_f=\{ g \in F \mid \text{for some stationary } T, g(\alpha ) < f( \alpha ) \text{ for all } \alpha \in T \}.$$ By a slight modification of the following lemma, $|F_f| \le \aleph _{\omega _1}$ . Thus for every $f \in F$ , $|\{ g \in F \mid g < f \}| \le \aleph _{\omega _1}$ . As $<$ is a linear ordering of the set $F$ , it follows that $|F| \le \aleph _{\omega _1 +1}$ . Therefore, $2^{\aleph _{\omega _1}}=\aleph _{\omega _1 +1}$ . Lemma: Let $\{ A_{\alpha } \mid \alpha < \omega _1 \}$ be a family of sets such that $|A_{\alpha }| \le \aleph _{\alpha }$ for every $\alpha < \omega _1$ , and let $F$ be a family of almost disjoint functions, $$F \subset \prod _{\alpha < \omega _1} A_{\alpha }.$$ Then $|F| \le \aleph _{\omega _1}$ . This lemma can be proved as follows. Let $S_0$ be the set of all limit ordinals $0 < \alpha < \omega _1$ . For $f\in F$ and and $\alpha \in S_0$ , let $f^{*}(\alpha )$ denote the least $\beta$ such that $f( \alpha ) < \omega _{\beta }$ . As $f^{*}(\alpha ) < \alpha$ for every $\alpha \in S_0$ , there exists, by Fodor's Lemma , a stationary set $S \subset S_0$ such that $f^{*}$ is constant on $S$ . Therefore $f$ restricted to $S$ is a function from $S$ into $\omega _{\beta }$ , for some $\beta < \omega _1$ , which is denoted by $\varphi (f)$ . It can be shown that $\varphi$ is a one-to-one mapping from $F$ into the set of functions defined on a subset $S$ of $\omega _1$ into some $\omega _{beta} < \omega _{\omega _1}$ . Thus we have $$|F| \le 2^{\aleph _1} \cdot \sum _{\beta < \omega _1} \aleph _{\beta } ^{\aleph _1} \le \aleph _{\omega _1}.$$ Question I have no question about details of the proof. However, I cannot understand how such a proof can come into one's mind from scratch; this proof does not seem intuitive and natural to me at all . What background and idea leads a one to such a proof? What is the intuition behind the proof? What were being thought when someone went to prove this? I have the fish; please teach me how to fish. P.S. Please note that my math knowledge is very low, so please answer this post in a way that I can understand.","['elementary-set-theory', 'cardinals', 'intuition']"
4177904,Is this line uniquely determined?,"Let $\alpha: x-y+2z-2=0$ and $\beta: x-2y-2z+3=0$ be two planes in $\mathbb{R}^3$ . I am asked to find a line $d_1\subset \alpha$ such that $d_1$ is perpendicular on the line $\alpha \cap \beta$ and that passes through the point $(1, 1, 1)$ . Here's what I did: I intersected the planes $\alpha$ and $\beta$ and I got that $\alpha\cap \beta: \frac{x-7}{-6}=\frac{y-5}{-4}=\frac{z}{1}$ . Let $u=(a, b, c)$ be a direction vector of $d_1$ . Since $d_1 \perp \alpha \cap \beta$ , we must have $-6a-4b+c=0$ . Because $d_1 \subset \alpha$ , it follows that $a-b+2c=0$ ( $u$ must be in $\alpha$ 's direction). If I solve these two equations together, I get that $b=-\frac{13}{7}a$ and $c=-\frac{10}{7}a$ . As a result, I can take the direction vector to be $(7, -13, -10)$ . I got that $d_1:\frac{x-1}{7}=\frac{y-1}{-13}=\frac{z-1}{-10}$ . This is indeed a line contained in $\alpha$ . I wonder, however, if the conditions I imposed guarantee that the result is going to be correct. My question comes from that fact that the condition that $u$ is in $\alpha$ 's direction only implies that my vector is parallel to the plane. Is that perpendicularity enough to make sure that what I will get out of the system of equations is what I want?","['linear-algebra', 'geometry', '3d']"
4177954,Claim: Let $f: \mathbb{R^+} \to \mathbb{R}$ defined by $f(x)=\sqrt{x}$. Then $f$ is an injection,"Show that the function is an injection by finding a left-inverse of the function, or explain why you can't find such a function given $f$ . Let $f: \mathbb{R^+} \to \mathbb{R}$ defined by $f(x)=\sqrt{x}$ . My Attempt Proof. To prove that $f$ is injective, we need only to show that there exists a function $g: \mathbb{R} \to \mathbb{R^+}$ such that for all $x\in \mathbb{R^+}$ we have $(g \circ f)(x)=x$ . Let $g: \mathbb{R} \to \mathbb{R^+}$ defined by $g(x)=x^2$ . Totality: Because every element of the domain, $\mathbb{R}$ , is assigned to something; that is, the square of any real number is defined, it follows that the totality condition is satisfied. Existence: Every assignment is not a well-defined existing element of $\mathbb{R^+}$ . Counterexample: Let $x=0 \in\mathbb{R}$ , but $g(0) = 0 \notin \mathbb{R^+}$ Uniqueness: As every element of the domain is assigned to only one element of the codomain $\Rightarrow$ existence is satisfied. As $g$ does not satisfy the condition $\forall x \in \mathbb{R}$ , $\exists! y \in \mathbb{R^+}$ such that $g(x)=y$ $\Rightarrow$ $g$ is not a well-defined left-inverse of $f$ . However, if $f: \mathbb{R^+} \to \mathbb{R^+}$ defined by $f(x)=\sqrt{x}$ , then $f$ is an injection whereby $g: \mathbb{R^+} \to \mathbb{R^+}$ defined by $g(x)=x^2$ is its left-inverse; that is, $\forall x \in \mathbb{R^+}$ , $g(f(x)) = x$ .","['functions', 'solution-verification']"
4177955,How many numbers are there such that its number of decimal digits equals to the number of its distinct prime factors?,"Problem A positive integer is said to be balanced if the number of its decimal digits equals the number of its distinct prime factors. For instance, $15$ is balanced, while $49$ is not. How many balanced numbers are there? My thoughts One digit balanced numbers are the prime ones. So, we have $4$ one digit balanced numbers. Number of 2 digit balanced numbers = number of 2 digit numbers - number of 2 digit prime numbers - number of 2 digit prime powers = $90-21-10=59$ . We can continue doing this until we find no balanced number. But this gets messier in each step. Edit: As in the comments, I missed many of the numbers. Is there some easier way to solve the problem?","['prime-factorization', 'divisibility', 'number-theory', 'elementary-number-theory', 'recreational-mathematics']"
4177999,Why do $ \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ and $ \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}$ generate a free semigroup?,"The title, basically (we do not include inverses, but I assume it would still hold if we did? But we would generate the free group and not the semigroup). We need to show that we cannot get the same matrix by two different sequences of multiplying these two matrices together. I tried finding the general form but couldn't really. Hints preferred - please don't spoil it unless necessary.
Thanks a lot in advance!","['matrices', 'abstract-algebra', 'semigroups']"
4178051,"In abstract algebra, how important is it to draw a distinction between polynomials and polynomial functions? [duplicate]","This question already has answers here : Do we really need polynomials (In contrast to polynomial functions)? (6 answers) Closed 3 years ago . In introductory analysis, it is common to define a single-variable polynomial as a function $p:\Bbb{R}\mapsto\Bbb{R}$ (or $\Bbb{C}\mapsto\Bbb{C})$ such that $$
p(x)=\sum_{i=0}^{n}a_ix^i \text{ for all $x$.}
$$ This means that $x^2$ is technically not a polynomial, but rather the value of the polynomial $t\mapsto t^2$ at the point $x$ . However, according to Wikipedia , in abstract algebra one often distinguishes between polynomials and polynomial functions . According to this definition, $x^2$ is literally a polynomial, whereas $t\mapsto t^2$ is the corresponding polynomial function. My question is: In abstract algebra, what is the purpose of defining polynomials as formal algebraic expressions, rather than as functions? How important is it to draw a distinction between polynomials and polynomial functions?","['definition', 'abstract-algebra', 'polynomials', 'real-analysis']"
4178091,A question on differentiability of a variant of Thomae function,"Let $$f(x) =
\begin{cases}
\dfrac{1}{q^2},  & \text{if $x=\dfrac{p}{q} $ is rational and in lowest terms;} \\[2ex]
0, & \text{if $x$ is irrational}
\end{cases}$$ Where is f continuous? Is f differentiable anywhere? My attempt: I can prove that $\lim_{x \to c}f(x)=0$ for any point real value c. for each positive number $\epsilon$ , the set $(c-1,c) \cup(c,c+1)$ contains only finitely many rational numbers p/q with $1/q^2 \geq \epsilon$ (namely , $q^2 \leq 1/\epsilon$ ). Then I let $\delta$ be the distance between $c$ and the closest such rational number. Then for all x satisfying $0<|x-c|<\delta$ , we have $0\leq f(x)=f(p/q)=1/q^2 \leq \epsilon$ . Thus, I prove that $\lim_{x \to c}f(x)=0$ for any point real value c. So this means f(x) is continuous at every irrational number, and discontinuous at every rational number. For differentiability, I only know f(x) is not differentiable at every rational number, because its discontinuous there. But how to prove f(x) also not differentiable at every irrational number? Hint: For any irrational number c, there are infinitely many fractions p/q in lowest terms such that $|c-p/q|<1/q^2$ . But I don't know this hint is is used in the continuous part ot differentiable part, since I already proved the continuous part.",['analysis']
4178104,Why does the triangle inequality seem to be false when rewritten based on $|x| = \sqrt{x^{2}}$?,"The triangle inequality is $$|x + y| \leq |x| + |y|.$$ Also, we know that $|x| = \sqrt{x^{2}}$ . Then, \begin{align*}
    \sqrt{x + y} &\leq \sqrt{x} + \sqrt{y} \\
    x + y &\leq x + y + 2\sqrt{xy} \\
    0 &\leq 2\sqrt{xy} \\\\
    \sqrt{xy} &\geq 0
\end{align*} Now, we can see that the inequality $|x + y| \le |x| + |y|$ holds for real $x$ and $y$ , but $\sqrt{xy} \geq 0$ does not for $x < 0, y > 0$ or $x > 0, y < 0$ . What seems to be the problem? Is it the statement $|x| = \sqrt{x^{2}}$ or is it much more than that?","['algebra-precalculus', 'absolute-value']"
