question_id,title,body,tags
4142770,Find all $x\in\mathbb{C^n}$ such that $||Ax||_2=1$ and $||x||_2=1$,"Given $A\in\mathbb{C}^{n\times n}$ , find all $x\in\mathbb{C^n}$ such that $||Ax||_2=1$ and $||x||_2=1$ . Lets do SVD: $A=U\Sigma V^*$ , where $\Sigma=\mathrm{diag}\{\sigma_1,\ldots,\sigma_n\}$ . We do change of a coordinates $x=Vy,$ then we need to find $y\in\mathbb{C^n}$ such that $||\Sigma y||_2=1$ and $||y||_2=1$ .","['normed-spaces', 'linear-algebra', 'svd', 'singular-values']"
4142774,The sum of five different positive integers is 320. The sum of greatest three integers in this set is 283.,"The sum of five different positive integers is $320$ . The sum of the greatest three integers in this set is $283$ . The sum of the greatest and least integers is $119$ . If $x$ is the greatest integer in the set, what is the positive difference between the greatest possible value and the least possible value of $x$ ? I obtained the equations $$\begin{align}x+b+c+d+e &= 320\\
x+b+c &=283\\
x+e &= 119\\
d+e &= 37\\
b+c+d &= 201\end{align}$$ How to proceed after this?","['algebra-precalculus', 'recreational-mathematics']"
4142837,The probability that more than 10500 passengers travel with buses in 72 hours,"Question : Buses go from a terminal to a destination city with a rate of $10$ bus per hour. The number of passengers on each bus is independent of the other buses and assumed to follow this distribution: $10$ passengers with a probability of $0.6$ , $20$ passengers with a probability of $0.2$ , and $30$ passengers with a probability of $0.2$ . What is the probability that in $72$ hours, more than $10500$ passengers reach the destination using the buses in this terminal? (Hint: you should use normal approximation ) Note : I've seen a lot of similar questions. However, this one asks for the probability of something related to number of passengers, and not a probability which is related to the waiting time of the buses. My problem is that I cannot even understand what random variable we are looking for. For instance, can I conclude that in $72$ hours, $720$ buses will pass? If yes, then what? How should I proceed? I mean, first of all I need somebody to rewrite the question in a  mathematical way.","['probability', 'random-variables']"
4142917,Prove $\cos(x)<\frac{\sin^2(x)}{x^2}$ [duplicate],"This question already has answers here : Prove $\sin(x)\tan(x) > x^2$ for $x \in ( \,0, \frac{\pi}{2}) \,$ (5 answers) Closed 3 years ago . I have to prove that $\cos(x)<\frac{\sin^2(x)}{x^2}$ for $0<x<\frac\pi2$ . By using the MVT I got that $\frac{\sin^2(x)}{x^2}=\cos^2(c)>\cos^2(x)$ for a $c\in[0, x]$ but this doesn't help since $\cos^2(x)<\cos(x)$ because $0<\cos x<1$ . The derivative didnt help either, since the derivatives of both $\cos(x)x^2-\sin^2(x)$ and $\cos(x)-\frac{\sin^2(x)}{x^2}$ don't have roots that I can easily find/I don't see a way to prove that they don't have roots. Any help is appreciated!","['trigonometry', 'inequality']"
4143038,Uniform stochastic process on $\Bbb R^n$,Are there known examples (and do they exist at all) of a stochastic process $X$ with a.s. continuous trajectories such that $X_t$ is uniformly distributed on an $\mathbb R^n$ ball around origin with a radius of $t$ ?,"['stochastic-processes', 'probability-theory']"
4143048,The time it takes for a candle having a lifetime that follows an exponential distribution to go off,"We have $5$ candles each having a lifetime which follows an exponential distribution with parameter $\lambda$ . We light up each candle at time $t=0$ . Assume that $Y$ is the time that it takes for the third candle to go off. What is the expectation and variance of $Y$ ? My try :
First of all, I believe having $5$ candles is irrelevant. We only need to consider one random variable following the exponential distribution, like $X\sim \exp(\lambda)$ . It means that on average, it takes $\frac{1}{\lambda}$ for the candle to go off. However, this does not seem like a random variable. It seems like it is a constant. Then, it won't be meaningful to calculate the expectation and variance. Am I right? Also, we know that at some point, the candle ""will"" go off. So, does this mean that we cannot predict at which time it will? I am totally confused thinking about these concepts. I appreciate if someone enlightens me. Note: There is a similar question here . However, the question has not been answered due to the lack of attemps provided by the OP.","['exponential-distribution', 'probability', 'random-variables']"
4143128,Calculate $\lim_{x\rightarrow0}\frac{(e^{\sin x}+ \sin x)^{\frac{1}{\sin x}}-(e^{\tan x}+ \tan x)^{\frac{1}{\tan x}}}{x^3}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to calculate the following limit? $$\lim_{x\rightarrow0}\frac{(e^{\sin x}+ \sin x)^{\frac{1}{\sin x}}-(e^{\tan x}+ \tan x)^{\frac{1}{\tan x}}}{x^3}$$ I thought of L'Hopital's rule, Taylor expansion, and limit the form of $e^x$ , but the presence of $\sin x$ and $\tan x$ make it hard to apply them. Could anyone give me a hint?","['limits', 'calculus']"
4143154,Solving $x^2+100=y^3$ in $\mathbb{Z}^2$,"Find all solutions to $x^2+100=y^3$ where $(x,y)\in\mathbb{Z}^2$ . Here's my progress so far: Let $K=\mathbb{Q}(i)$ , then $R:=\mathcal{O}_K=\mathbb{Z}[i]$ is a U.F.D. If $\alpha=x+10i$ , then $\alpha\bar{\alpha}=y^3$ . If $\pi\in R$ is a prime element dividing $\alpha$ and $\bar\alpha$ , then $\pi|(\alpha-\bar\alpha)=20i$ , so $\pi$ divides one of $2$ or $5$ ( $i\in R^{\times}$ ), which factor as follows in $R$ : $2=i\pi_2^2$ where $\pi_2=1+i$ , and $\bar\pi_2=-\pi_2$ $5=\pi_5\bar\pi_5$ where $\pi_5=2+i$ . So the prime factorisation of $\alpha$ and $\bar\alpha$ look like follows: $$\alpha=u\cdot\pi_2^a\pi_5^b\bar\pi_5^c\cdot\rho_1^{e_i}\cdots\rho_n^{e_n}\\
\bar\alpha=v\cdot\pi_2^a\bar\pi_5^b\pi_5^c\cdot\bar\rho_1^{e_i}\cdots\bar\rho_n^{e_n}\\
$$ where $a,b,c,e_i,n\in\mathbb{N}$ ; $u,v\in R^{\times}$ ; and $\rho_i$ primes in $R$ distinct from each other and the $\pi_p$ . Then: $y^3=uv\cdot\pi_2^{2a}\pi_5^{b+c}\bar\pi_5^{b+c}\cdot(\rho_1\bar\rho_1)^{e_1}\cdots(\rho_n\bar\rho_n)^{e_n}$ , and since we're working in a U.F.D, each prime exponent on the right must be divisible by $3$ , thus $3|a$ , $3|e_i$ and $3|(b+c)$ , i.e: $b+c\equiv 0\mod3$ . So we have three cases: $b\equiv 0 \mod3 \implies c\equiv 0 \mod3$ , and so $\alpha=\beta^3$ for some $\beta\in R$ . $b\equiv 1 \mod3 \implies c\equiv 2 \mod3$ , and so $\alpha\pi_5^2\bar\pi_5=\beta^3$ for some $\beta\in R$ . $b\equiv 2 \mod3 \implies c\equiv 1 \mod3$ , and so $\alpha\pi_5\bar\pi_5^2=\beta^3$ for some $\beta\in R$ . Letting $\beta=s+it$ , we can 'easily' solve the first case: $$\begin{align}x+10i&=(s+it)^3\\
&=s^3+3is^2t-3st^2-it^3\\
10&=t(3s^2-t^2)\end{align}$$ So $t|10$ and $s=\pm\sqrt{\frac13(\frac{10}{t}+t^2)}$ , the only integer solution is $(s,t)=(\pm3,5)$ , which yields $(x,y)=(\pm198,34)$ However, I'm struggling to solve the other two cases using the techniques I've familiar with... attempting to do the same thing in the second case got me this: $$\begin{align}10x-50&=a^3-3ab^2\\100+5x&=3a^3b-b^3\end{align}$$ From this we have $250 = 6a^2b-2b^3-a^3+3ab^2$ , which means $b-a\equiv 1 \mod3$ ... but I'm not sure how this is helpful. Wolfram $\alpha$ told me that there are more solutions to be found! Can I solve this system for $x$ or does the method not work in the last two cases? Should I be looking at ideals instead - though this seems unnecessary in a U.F.D? Are there other choices of $\alpha$ which unlock the other solutions?","['number-theory', 'gaussian-integers', 'diophantine-equations']"
4143183,How do I designate the infimum of the union of an indexed family of sets of real numbers using set theory?,"I have an arbitrary collection of sets $\{U_i\}$ from the topology $\Omega := {\varnothing, [0, \infty)} \cup \{(a, \infty): a \in \mathbb{R}_{0+}\}$ . In the case the arbitrary collection $\{U_i\}$ doesn't have either $\varnothing$ or $[0, \infty)$ , the union of all its members $\bigcup U_i$ is $(a^*, \infty)$ where $a^*$ is the least such $a^*$ . Now, for the life of me, I have been suffering trying to designate that set and its infimum with set theory. Is its infimum $a^* = \inf \{a_i : (a_i, \infty) \in U_i\}$ ? I want to say it is identical to $\inf \{x : (x, \infty) \in U_i \}$ , but I'm not sure, I'm a bit confused by the notation. The problem (I think?) is that there's no hint that this set should range over all the possible $U_i$ sets (that have been defined $(a, \infty)$ for $a \in \mathbb{R}_{0+}$ ) which are in the collection $\{U_i\}$ and grab only the value of the lower bound of the interval. I feel like this designation should mention the indexing set somehow. Maybe $\inf \{x : (x, \infty) \in U_i,  i \in I \}$ ? But in that case, don't I have to define the index set? And how would I define the index set of a collection that is arbitrary? Any hints, help, reading suggestions or keywords are highly appreciated.","['elementary-set-theory', 'notation']"
4143230,Showing $\tan70° = \tan20° + 2\tan50°$,"Q. Prove that $\tan 70° = \tan 20° + 2\tan 50°$ . My approach: $ LHS = \tan 70° = \dfrac1{\cot 70°} = \dfrac1{\tan 20°}$ $ \begin{align}
RHS 
&= \tan 20° + 2\tan 50° \\
&=  \tan 20° + 2\tan (20+30)° \\
&= \tan 20° + \dfrac{2(\tan 20° + 1/√3)}{1 - \tan 20°/√3} \\
&=\dfrac{2 + 3√3 \tan 20° - \tan^2 20°)}{√3 - \tan 20°} 
\end{align}
$ Why are the two sides not equal despite being expressed in the same terms? Can someone offer some help? Much to my surprise, my friend just expanded tan70° and cross-multiplied the terms to prove it.","['alternative-proof', 'trigonometry', 'solution-verification']"
4143267,Transfer and fusion in a centralizer,"Suppose $G$ is a finite group of order divisible by $8$ , with an element $\tau$ of order 2 whose centralizer $C_G(\tau)$ is elementary abelian of order 4. I suspect $G/[G,G]$ must have even order, but I'm not sure how to prove it. I thought about using transfer and fusion: $\tau$ cannot be conjugate to any involution in the center of a Sylow 2-subgroup (if so, then conjugate that Sylow back so that $\tau$ itself is in the center of a Sylow 2-subgroup, but then that entire Sylow 2-subgroup is contained in the centralizer, contradicting the hypotheses on orders). Since $C_G(\tau)$ contains the center of every Sylow 2-subgroup containing $\tau$ that means that the center of the Sylow 2-subgroups are cyclic of order 2. However, since I don't know much else about the Sylow 2-subgroup, I wasn't sure how to use the transfer. The goal is to see what sort of classification of groups I can get which have $C_G(\tau)$ of order 4, similar to the one of order 8 mentioned in another question .","['involutions', 'transfer-theory', 'finite-groups', 'sylow-theory', 'group-theory']"
4143285,Prove or disprove statement about convergence of random variables,"Prove or disprove the following statement: \begin{align*}
\text { If } X_{N} \stackrel{d}{\rightarrow} X \text { then } \frac{X_{N}}{\sqrt{N}} \stackrel{P}{\rightarrow} 0
\end{align*} where $\stackrel{d}{\rightarrow}$ is convergence in distribution and $\stackrel{P}{\rightarrow}$ is convergence in probability. Intuitively, if the sequence $X_1, \ldots, X_n$ is converging to a fixed distribution as $n \to \infty$ , then dividing that random variable by larger and larger values of $\sqrt{n}$ should converge to zero. So I think that statement is true. However, I feel like I don't have the mathematical language in probability theory to express this precisely. Could someone help me with this problem?","['probability-distributions', 'probability-theory']"
4143348,Question about a sequence $a_{n+1}=2^{a_n}-1$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $0<t<1$ and define a sequence $(a_n)_{n=1}^\infty$ by: $$a_{n+1}=2^{a_n}-1 ~~~ , ~~~ a_1=t$$ Prove that $(a_n)_{n=1}^\infty$ is decreasing. Prove that $a_{n+1}=2^{a_n}-1$ is convergent and compute its limit. I have tried to prove 1 in many ways, by induction, and by definition, but I miss something here that doesn't let me continue the proof but I don't know what it is. I will appreciate some hints and way of thinking for this kind of problem. Thanks a lot!","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4143370,Calculate $ \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n+1} H_n$ [duplicate],"This question already exists : On a series involving harmonic numbers Closed 3 years ago . Define $$
H_n = \sum_{k=1}^n \frac{1}{k}
$$ I need to calculate the sum $$
S = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n+1} H_n
$$ Using the following integral representation of $ H_n$ $$
H_n = -n \int_0^1 x^{n-1}\ln(1-x) dx
$$ and exchanging the order of summation, I obtained $$
S = -\int_0^1 \left(\frac{1}{1+x}+\frac{\ln(1+x)-x}{x^2}\right) \ln(1-x) dx
$$ Using Wolfram Alpha, I got $$
S \approx 0.240227
$$ so I guess $$
S = \frac{(\ln 2)^2}{2}
$$ But I don't know how to calculate the integral. Any idea?","['integration', 'limits', 'sequences-and-series']"
4143374,Can every increasing negative function be expressed as a product with these properties?,"Let $h:(0,1] \to (-\infty,0)$ be a $C^1$ function, with $h'>0$ . I am looking for sufficient conditions on $h$ that imply the existence of $C^1$ functions $\lambda:(0,1] \to (-\infty,0)$ , $g:(0,1] \to (0,1]$ such that $$
h=\lambda g,  \,\,\,\,\,\lambda'>0, \,\,\,\,\,g'<0 \tag{1}
$$ Note that $h'=\lambda' g+\lambda g'>0$ ,
since both summands are positive. Thus, $h'>0$ is a necessary condition for $h$ to be expressible as in $(1)$ . Is it sufficient? Can every $h$ with $h'>0$ be expressed in this way? The motivation for this convoluted question comes from trying to analyses when the solution to a certain minimization  problem is convex. (a bit too long to describe here). An equivalent reformulation: (thanks to Alex Jones). Take any $g: (0,1] \to (0,1]$ with $g' < 0$ . Then, we must have $\lambda = h/g < 0$ . Since $g > 0$ , the condition $\lambda' > 0$ is equivalent to $h'/h < g'/g$ . So, the question boils down to this: Does there exist a function $g:(0,1] \to (0,1]$ , with $g' < 0$ , such that $h'/h < g'/g$ ? Since the quantity $g'/g$ is invariant under positive scaling of $g$ ( $g \to c g$ for $c >0$ ), it suffices to search for bounded $g$ . Note that if $h$ is bounded, then by taking $g=-h$ we get $h'/h = g'/g$ instead of $h'/h < g'/g$ . So we are ""nearly there"".","['calculus', 'examples-counterexamples', 'real-analysis']"
4143385,Closed form expression for the series $\sum n!/n^n$,"It is quite well known that the series $\sum n!/n^n$ converges. For instance, the question of convergence was addressed in this thread. However, I was wondering if there is a closed form expression for this sum. By closed form, I mean an expression where the answer is in the form of a function of a well known kind, or an expression which could even be a definite integral of some function. I tried to use generating functions and find an expression for this sum but I did not go very far. If someone can derive an expression for this sum starting from some known power series, that would be great.","['power-series', 'generating-functions', 'convergence-divergence', 'sequences-and-series']"
4143394,Meaning of $\mathcal{O}_X$ and line bundle notation,"I have gotten a little confused about the notation regarding line bundles and their holomorphic sections on projective spaces, and subsequently projective varieties, as well as the meaning of $\mathcal{O}_X$ .
(My background is more in physics than algebraic geometry). Let me start with line bundles on $\mathbb{P}^n$ . These can be denoted by $\mathcal{O}_{\mathbb{P}^n}(k)$ , for $k\in \mathbb{Z}$ . There are various ways to arrive at this description, I think the standard method is to define the tautological line bundle -- where the fibre over $p\in\mathbb{P}^n$ is the line in $\mathbb{C}^n$ described by $p$ -- by $\mathcal{O}_{\mathbb{P}^n}(-1)$ , and its dual, the hyperplane bundle, as $\mathcal{O}_{\mathbb{P}^n}(1)$ , and then denote $$
\mathcal{O}_{\mathbb{P}^n}(k) = \mathcal{O}_{\mathbb{P}^n}(1)^{\otimes k}, \quad k\geq1
$$ $$
\mathcal{O}_{\mathbb{P}^n}(-k) = \mathcal{O}_{\mathbb{P}^n}(-1)^{\otimes k}, \quad k\geq1
$$ We can also denote by $\mathcal{O}_{\mathbb{P}^n} = \mathcal{O}_{\mathbb{P}^n}(0)$ the trivial bundle on $\mathbb{P}^n$ , with the property that $\mathcal{O}_{\mathbb{P}^n}(k)\otimes\mathcal{O}_{\mathbb{P}^n}(-k) = \mathcal{O}_{\mathbb{P}^n}$ . The tensor product then gives the space of line bundles the structure of an Abelian group, $Pic(\mathbb{P^n})=\mathbb{Z}$ . I believe that for a projective hypersurface $Y \subset \mathbb{P}^n$ , ignoring any special cases, this notation can be carried over to line bundles on $Y$ . Question 1 Is this statement more generally true, for higher codimension varieties $Y$ in $\mathbb{P}^n$ ? (via e.g. some Lefschetz theorem?) Now for a given complex manifold $X$ , we can associate to $X$ the structure sheaf $\mathcal{O}_X$ , which is the sheaf of local holomorphic functions on $X$ . I understand that there is a relationship between the space of sections of a vector bundle and a particular locally free sheaf. Question 2 Can we directly associate $\mathcal{O}_X$ with a line bundle on $X$ ? Is there some statement about the degrees of the sections of this line bundle? Question 3 I believe the sections of $\mathcal{O}_{\mathbb{P}^n}(k)$ , which are elements in $H^0(\mathbb{P}^n, \mathcal{O}_{\mathbb{P}^n}(k))$ , are homogeneous polynomials of degree $k$ . This should mean that sections of $\mathcal{O}_{\mathbb{P}^n}$ are locally constant polynomials. The previous should then be true for $Y\subset \mathbb{P}^n$ , so that $H^0(Y, \mathcal{O}_Y)$ is the space of locally constant polynomials on $Y$ . Is this the case, and in general can we classify line bundles on projective varieties based on the degrees of their sections? Or is the more fundamental object the 'cocycle' associated with the transition functions of the local trivialization (for $\mathcal{O}_{\mathbb{P}^n}(k)$ these are the $k$ -th power of the transition functions of $\mathcal{O}_{\mathbb{P}^n}(1)$ )? Question 4 Is it correct to call the sheaf of sections of $\mathcal{O}_{\mathbb{P}^n}$ the structure sheaf of $\mathbb{P}^n$ ? How can this be if $\mathcal{O}_X$ is supposed to include all holomorphic functions, not just locally constant ones? In particular my confusion arises in the description of the exponential sequence, where the locally constant sheaf is called $\mathbf{Z}$ , what is this objects relation to line bundles? (... of course this could just refer to the sheaf of integer valued functions?) Sorry for the length, I believe these are all essentially different versions of the same question: what is the relationship of $\mathcal{O}_X$ to line bundles and sections of line bundles on $X$ . My confusion arises because in the literature there are (with good reason) jumps between the sheaf description and the vector/line bundle description.","['algebraic-geometry', 'projective-geometry', 'sheaf-cohomology', 'sheaf-theory']"
4143404,Finite type scheme over a ring of $S$-integers is separated?,"I have a question in the following setting: Let $k$ be a number field with ring of integers $O_k$ . For a finite set of places $S$ of $k$ we can form the ring of $S$ -integers $O_{k,S}$ . Let $\mathbb{A}_k$ be the adéle ring of $k$ . For a $k$ -variety (or more generally any scheme over the spectrum of the adéles), I want to understand the set of adélic points $X(\mathbb{A}_k)$ . Everything I need is contained in Exercise  3.4. in Poonen's ""Rational points on varieties"", so I will present to you my struggles with it. The first part of the exercise says to find, for a $k$ -variety $X$ , a separated, finite-type scheme $\mathcal{X}$ over $O_{k,S}$ , for some $S$ , such that $\mathcal{X} \times_{O_{k,S}} k = X$ . I have constructed the scheme $\mathcal{X}$ by covering $X$ with spectra of finitely generated $k$ -algebras $k[x_1,\dots, x_{n_i}]/I_i$ and setting $S_i$ to be the set of places under which some coefficients of the polynomials in $I_i$ have negative valuation. (Note that this is finite because $I_i$ is finitely generated.) The ideal $I_i$ is then an ideal of the same polynomial ring considered with coefficients in the ring $O_{k,S_i}$ . We can now set $S= \bigcup S_i$ and glue these to get $\mathcal{X}$ over $O_{k,S}$ . One then sees that this becomes isomorphic to $X$ over $k$ ; by construction it is of finite type over $O_{k,S}$ . My problem now is to show that it is separated. My intuition tells me that this should be almost trivially true, because everything is so well behaved. I failed using the valuative criterion, but I have the following idea using Stacks 26.21.7 : Choose $p_i,p_j \in \mathcal{X}$ lying over $\mathfrak{p} \in \text{Spec }O_{k,S}$ . Let $A_i =\text{Spec }O_{k,S_i}[x_1,\dots, x_{n_i}]/I_i$ and $A_j = \text{Spec }O_{k,S_j}[x_1,\dots, x_{n_j}]/I_j$ be open affines around $p_i$ and $p_j$ , respectively. Then we can take the maximum $n_{ij}$ of $i,j$ and take $S_{ij}= S_i \cup S_j$ and $I_i, I_j$ to be ideals in a polynomial ring over $O_{k,S_{ij}}$ in $n_{ij}$ variables. The intersection $A_i \cap A_j$ should then be $\text{Spec }O_{k,S_{ij}}[x_1,\dots, x_{n_{ij}}]/(I_i + I_j)$ , hence affine. The natural map $$
O_{\mathcal{X}}(A_i) \otimes O_{\mathcal{X}}(A_j)\rightarrow O_{\mathcal{X}}(A_i\cap A_j) 
$$ is an isomorphism, because $$
O_{\mathcal{X}}(A_i) \otimes O_{\mathcal{X}}(A_j) = \Gamma(A_i, O_{A_i}) \otimes \Gamma(A_j, O_{A_j}) = O_{k,S_{i}}[x_1,\dots, x_{n_{i}}]/I_i \otimes O_{k,S_{j}}[x_1,\dots, x_{n_{j}}]/I_j
$$ and $$O_{\mathcal{X}}(A_i\cap A_j) = \Gamma(A_i\cap A_j, O_{A_i\cap A_j}) = O_{k,S_{ij}}[x_1,\dots, x_{n_{ij}}]/(I_i + I_j).
$$ If this is correct, it would follow that $\mathcal{X}$ is separated. Are there any mistakes here? If there is a better way to argue for this, I would love to hear it. Thanks already and if I should clarify more, just let me know!","['number-theory', 'adeles', 'algebraic-geometry', 'arithmetic-geometry', 'schemes']"
4143428,Proving ODE does not have bounded solutions,"Consider the equation \begin{equation}
        x'' + (a+b\cos t)x = 0
    \end{equation} and let $u, v$ be solutions such that \begin{equation}
    u(0) = 1, u'(0) = 0; v(0) = 0, v'(0) = 1
\end{equation} Set $F(a, b) = u(2\pi) + v'(2\pi)$ and show that if $|F(a, b)| > 2$ then no solutions remain bounded for all real $t$ . My attempt is below: Consider $F_{a, b}(t) = u(t) + v'(t)$ . We know that $|F_{a, b}(2\pi)|>2$ . We can now differentiate $F_{a, b}(t)$ with respect to $t$ to obtain: \begin{align}
    F'_{a, b}(t) &= u'(t) + v''(t)\\
    &= u'(t) - (a+b\cos(t))v(t)
\end{align} where in the last equality we used the fact that $v$ is a solution to the given ODE. We can differentiate $F'_{a, b}(t)$ again to obtain: \begin{align}
    F''_{a, b}(t) &= u''(t) - (a+b\cos(t))v'(t) + b\sin(t)v(t)\\
    &= -(a+b\cos(t))u(t) - (a+b\cos(t))v'(t) + b\sin(t)v(t)\\
    &= -(a+b\cos(t))(u(t)+v'(t)) + b\sin(t)v(t)\\
    &= -(a+b\cos(t))F_{a,b}(t) +  b\sin(t)v(t)
\end{align} This looks promising because now I have $F_{a, b}(t)$ and $v(t)$ in the same equation but I'm having trouble finishing this off myself. Some hints would be greatly appreciated.",['ordinary-differential-equations']
4143507,Second-order linear elliptic PDE on $S^2\times S^2$,"I'm facing the problem of proving existence/non-existence of a (unique and or weak) non-trivial solution to the following PDE on the Riemannian manifold ( $S^2\times S^2$ , $g$ ) with $g$ smooth (but not quite the round) metric tensor: $$Pu=0,\quad P=\Delta+A:C^\infty(S^2\times S^2)\rightarrow C^\infty(S^2\times S^2)$$ with $u$ real-valued, $\Delta=\ast\mathrm{d}\ast\mathrm{d}$ the Laplacian and $A$ containing first- and zeroth-order terms with smooth non-constant coefficients. Ellipticity is evident from the Laplacian, hence $P$ is Fredholm. Unfortunately, the index (which only depends on the principal symbol, i.e. $\sigma(\Delta)$ ) vanishes, so I can't use it to show existence of solutions. I also thought about proving things locally and then patching using partitions of unity but this doesn't seem to be working. Although there is an abundance of literature about existence/regularity theory on $\mathbb{R}^n$ for second-order linear elliptic operators, the only reference I found that contains similar considerations are the notes by Melrose (and to the best of my very limited understanding, he does not talk about existence at all, only about regularity). So I'd appreciate both, good references about the topic and concrete hints how to show that there does(n't) exist some $u$ . Cheers!","['elliptic-equations', 'linear-pde', 'partial-differential-equations', 'differential-geometry']"
4143523,"In van Kampen’s theorem, what happens to the loops not in $\pi_1(U_\alpha \cap U_\beta)$?","I’ve just read the set up and the statement for van Kampen’s theorem. Here’s the version that we use in our class. van Kampen's Theorem . Suppose we have an open cover $\left\{U_{\alpha}: \alpha \in A\right\}$ of $\left(X, x_{0}\right)$ by path-connected open sets $U_{\alpha} \ni x_{0}$ containing the basepoint. As above, let $F:= \ast_{\alpha} \pi_{1}\left(U_{\alpha}\right)$ be the free product and $\varphi: F \rightarrow \pi_{1}(X)$ be the map induced by the inclusions. Also let $K \triangleleft F$ be the normal subgroup generated by the words $i_{\alpha \beta}(\sigma) i_{\beta \alpha}(\sigma)^{-1}$ for $\sigma \in \pi_{1}\left(U_{\alpha} \cap U_{\beta}\right) .$ Then since $K<\operatorname{ker} \varphi$ as explained above, we get a map $\bar{\varphi}: F / K \rightarrow \pi_{1}(X)$ . If $U_{\alpha} \cap U_{\beta}$ is path-connected for all $\alpha, \beta \in A$ , then $\bar{\varphi}$ (or equivalently $\varphi$ ) is surjective. If furthermore $U_{\alpha} \cap U_{\beta} \cap U_{\gamma}$ is path-connected for all $\alpha, \beta, \gamma \in A$ , then $\bar{\varphi}$ is injective, thus an isomorphism $F / K \cong \pi_{1}(X).$ The accompanying diagram is as follows: What is not clear to me is what happens, under $\bar{\varphi}$ (or $\varphi$ ), to the loops in $X$ that is not in $U_\alpha \cap U_\beta$ . For example, let $X$ be covered by $U_\alpha$ , $U_\beta$ , and a “weird” (but still path-connected) $U_\gamma$ . The loop in red (let’s call it $\lambda$ ) is in $\pi_1(X)$ , and it has a corresponding element $a_\gamma^j$ in $F$ , where $a_\gamma$ is a generator of $\pi_1(U_\gamma)$ . Under the projection $p: F \to F/K$ , it is sent to the coset $a_\gamma^jK$ of $K$ in $F$ . I’m inclined to think that $a_\gamma^j$ “gets its own coset” under $p$ , as do other loops not in $\pi_1(U_\alpha \cap U_\beta)$ (if any), since they have nothing to do with $K$ . Is there anything else we know about $a_\gamma^jK$ in this setting? What would its elements look like? Also, the reason for this question is that when the theorem was introduced, I got the impression that the diagram would somehow give a complete picture of $\pi_1(X)$ . But it seems like that is not the case, or at least some elements of $\pi_1(X)$ would not be described as precisely as others with this diagram. Would a remedy be that, instead of considering $F$ modulo just $K$ , we take $F$ modulo all such normal subgroups over all pairs of covering sets, i.e. $F/\kappa$ , where $\kappa$ is the normal subgroup of $F$ generated by all the $K_1,K_2,\dots,K_t$ ?","['fundamental-groups', 'free-groups', 'free-product', 'group-theory', 'algebraic-topology']"
4143525,Partially inverting a function,"Say I had a function $f:\mathbb{R}_x\rightarrow\mathbb{R}_z$ with $z=f(x)=x+1$ (the subscripts on the $\mathbb{R}$ are just there to indicate which variable corresponds to which space). Since $f$ is bijective, I could define the inverse as $f^{-1}:\mathbb{R}_z\rightarrow\mathbb{R}_x$ with $x=f^{-1}(z)=z-1$ . The key property of an inverse is that $\forall x \in \mathbb{R}_x\,,\ f^{-1}(f(x))=x$ . Now suppose I had a function $f:\mathbb{R}_x\times\mathbb{R}_y\rightarrow\mathbb{R}_z$ with $z=f(x,y)=x+y$ . I can't really have an inverse because the map is not bijective, e.g. both $f(1,0)$ and $f(0,1)$ map to $1$ . However, for a project I'm working on, it would be useful to talk about a ""partial"" or ""weak"" inverse. For instance, with just $z$ I can't recover any inputs, but with both $z$ and $x$ I could recover $y$ . So I could think of a partial/weak inverse as a function $f^{-1}_{\mathbb{R}_x}:\mathbb{R}_x\times\mathbb{R}_z\rightarrow\mathbb{R}_y$ with $y=f^{-1}_{\mathbb{R}_x}(z,x) = z-x$ . This function is bijective, and $\forall x \in \mathbb{R}_x$ and $\forall y \in \mathbb{R}_y$ , $f^{-1}_{\mathbb{R}_x}(f(x,y),x)=(x+y)-x=y$ , which is not exactly the inverse property, but useful (for us at least) nonetheless. Is there an existing name and notation for this kind of partial/weak inverse? Or is it something we'll just have to make up and define?","['notation', 'functions', 'inverse', 'inverse-function']"
4143526,Sub-martingale and a.s convergence,"I was reading lecture on martingale where the following result is proved. Unfortunately it's not very detailed. I appreciate if you could explain why $Y_\infty^{(p)}$ are equal $\mathbb{P}$ -a.s, from the fact $Q_{p+1}\subset Q_p$ and how to conclude from the right continuity that $X_r \to Y_{\infty} \ \mathbb{P}$ -a.s. Update: perhaps the a.s equality is true because, $Y_{k}^{(p)}=Y_{2k}^{(p+1)},$ the limit is unique a.s so $Y_{\infty}^{(p)}=Y_{\infty}^{(p+1)},$ for all $p \geq 1$ , so $Q_p$ are not necessary.","['stochastic-analysis', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4143623,Sokhotski–Plemelj theorem for the real line,"The Sokhotski–Plemelj theorem for the real line is stated at https://en.wikipedia.org/wiki/Sokhotski–Plemelj_theorem : Sokhotski–Plemelj theorem. Let $f$ be a complex-valued function that is defined and continuous on the real line, and let $a$ , $b$ , and $x_0$ be real constants with ${\displaystyle a<x_0<b}$ . Then $${\displaystyle \lim _{\varepsilon \to 0^{+}}\int _{a}^{b}{\frac {f(x)}{x-x_0\pm i\varepsilon }}\,dx=\mp i\pi f(x_0)+{\mathcal {P}}\int _{a}^{b}{\frac {f(x)}{x-x_0}}\,dx.}$$ I also gather from Confusion concerning the Sokhotski–Plemelj theorem: two different values for the same real integral that there is a more general version  of the theorem that is valid over the whole real line, so that, under some mild hypotheses on $f(x)$ , one has $${\displaystyle \lim _{\varepsilon \to 0^{+}}\int _{-\infty}^{\infty}{\frac {f(x)}{x-x_0\pm i\varepsilon }}\,dx=\mp i\pi f(x_0)+{\mathcal {P}}\int _{-\infty}^{\infty}{\frac {f(x)}{x-x_0}}\,dx.}$$ My question is: what (mild) hypotheses on $f(x)$ are sufficient for the more general version stated above to hold?  I'm also looking for a proof or appropriate reference.",['complex-analysis']
4143668,Orthogonal transformation of a set of points to positive orthant,"Let $x_1,x_2\ldots,x_n$ are vectors from $\mathbb{R}^d$ . Also assume that $x_i^{\top}x_j \geq 0$ for all $i,j=1,2,\ldots,n$ . I am wondering if there is an orthogonal matrix $W$ such that the entries of $Wx_i$ 's are non-negative for $i=1,2,\ldots,n$ . I can prove the statement for $d=2$ . Any ideas or counterexample for the cases when $d\geq 3$ will be very helpful.","['vectors', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'linear-transformations']"
4143740,Existence of infinite subset $S$ of $B_{\ell_2}$ s.t. $\|x-y\| > \sqrt{2}$ for all $x\neq y$ in $S$.,"Prove that there exists an infinite subset $S$ of $B_{\ell_2}$ (unit ball of $\ell_2$ ) such that $\|x-y\| > \sqrt{2}$ for all $x\neq y$ in $S$ . Attempt . Subset $S=\{e_n:n\in \mathbb{N}\}$ , where $e_n(k)= 1$ for $n=k$ and $0$ otherwise does not work, since $\|e_n-e_m\|=\sqrt{2}$ for $n\neq m$ . Thanks in advance for the help.","['hilbert-spaces', 'functional-analysis', 'analysis']"
4143746,"The collection $\{x : \exists y(x\in y)\}$ is not a set. (""Set Theory: A First Course"" by Daniel W. Cunningham)","I am reading ""Set Theory: A First Course"" by Daniel W. Cunningham. There is the following exercise in this book: Exercise 2.1.33 The collection $\{x : \exists y(x\in y)\}$ is not a set. My answer is here: Asume that $\exists A\forall x(x\in A)$ holds. Let $\forall x(x\in A_0)$ holds. By Subset Axiom, $\exists S\forall x(x\in S\leftrightarrow(x\in A_0\land x\notin x))$ holds. Let $\forall x(x\in B_0\leftrightarrow(x\in A_0\land x\notin x))$ holds. $B_0\in B_0\lor B_0\notin B_0$ holds. If $B_0\in B_0$ holds, then $B_0\in A_0\land B_0\notin B_0$ holds. So, both $B_0\in B_0$ and $B_0\notin B_0$ hold. This is a contradiction. If $B_0\notin B_0$ holds, then $\neg (B_0\in A_0\land B_0\notin x)$ holds. So, $B_0\notin A_0\lor B_0\in B_0$ holds. Since $B_0\notin B_0$ , $B_0\notin A_0$ holds. This is a contradiction. So, $\neg(\exists A\forall x(x\in A))$ holds. Assume that $\exists C\forall x(x\in C\leftrightarrow\exists y(x\in y))$ holds. Let $\forall x(x\in C_0\leftrightarrow\exists y(x\in y))$ holds. By Pairing Axiom, $\forall u\forall v\exists D\forall x(x\in D\leftrightarrow x=u\lor x=v))$ holds. So, $\forall x\exists D\forall z(z\in D\leftrightarrow z=x)$ holds. So, $\forall x\exists D(x\in D\leftrightarrow x=x)$ holds. So, $\forall x\exists D(x\in D)$ holds. So, $\forall x\exists y(x\in y)$ holds. So, $\exists C\forall x(x\in C)$ holds. This is a contradiction. So, $\neg(\exists C\forall x(x\in C\leftrightarrow\exists y(x\in y)))$ holds. My question is here: In this book, there doesn't exist a definition of a collection. But the author wrote ""The collection $\{x : \exists y(x\in y)\}$ is not a set."". Is this OK? I think we can write $\{x : \phi(x)\}$ if and only if $\exists S\forall x(x\in S\leftrightarrow\phi(x))$ .","['elementary-set-theory', 'logic']"
4143819,"Let $X_1, ... , X_5$ be a random sample from $p(x;\theta)=(1-\theta)^{x-1}\theta$. Find the UMPT size $\alpha = 0.1$.","Let $X_1, \ldots , X_5$ be a random sample from $p(x;\theta)=(1-\theta)^{x-1} \theta$ , for $x=1,2,3,\ldots$ . Consider $H_0:\theta \geq 0.7\text{ vs. } H_1:\theta<0.7$ . Find the UMPT size $\alpha = 0.1$ . My try: let $$H_0^{'}:\theta_0 = 0.7\text{ vs. } H_1^{'}:\theta=\theta_1 \quad(\theta_1<0.7)$$ Now, using the NP Lemma $$\frac{L(\theta_1)}{L(\theta_0)} = \frac{(1-\theta_0)\theta_1^n(1-\theta_1)^{\sum ^5_{i=1}x_i}}{(1-\theta_1)\theta_0^n(1-\theta_0)^{\sum ^5_{i=1}x_i}}\geq k$$ Then I end up with $$\sum ^5_{i=1}x_i\geq c$$ Then $$\Psi^*(x)=
\begin{cases}
1&\sum ^5_{i=1}x_i\geq c\\
0&\sum ^5_{i=1}x_i< c.
\end{cases}$$ In order to find alpha, I need to know the probability distribution of $\sum ^5_{i=1}x_i$ but I'm not sure how to do this or if my method is correct. Any suggestions would be great!","['statistics', 'probability-distributions', 'probability']"
4143821,$f_n(x)= f(x+n)$ show that the limit function is uniformly continuous,"Let $f$ be a real-valued continuous function on $I=\{x\in \mathbb{R} | x \geq 0\}$ . For a positive integer $n$ the function on $I$ is defined by \begin{align*}f_n(x)= f(x+n)\end{align*} Answer the following questions when the sequence of functions $\{f_n(x)\}_{n=1}^\infty$ converges uniformly on $I$ The function $g$ on $I$ is defined by $g(x)=\lim _{n\xrightarrow{}{}\infty}f_n(x)$ , show that $g$ is uniformly continuous on $I$ show that $f$ is uniformly continuous on $I$ Here we need to show that $|g(x)-g(y)|<\epsilon$ whenever $|x-y|<\delta$ for all $x,y\in I$ . Since $\{f_n(x)\}_{n=1}^\infty$ coverge uniformly to $g$ we have $|f_n{(x)}-g(x)|<\epsilon \implies |f(x+n)-g(x)|<\epsilon$ whenever $n>N$ . How do we proceed from here. Any hints or a solution would be appreciated.","['uniform-continuity', 'sequence-of-function', 'continuity', 'uniform-convergence', 'sequences-and-series']"
4143824,Finding the region of attraction using a Lyapunov function,"I'm trying to find an estimate for the region of attraction of an equilibrium point. The notes from Nonlinear Control by Khalil suggest that defining $$
V(x) = x^TPx,
$$ where $P$ is the solution of $$
PA+A^TP=-I,
$$ will yield the best results for an estimate. It also assures that the estimate can be found from these types of Lyapunov functions for exponentially stable equilibrium points. The system I am studying is defined by $$
\begin{gathered}
\dot{x}_1 = x_1 - x_1^3 + x_2 \\
\dot{x}_2 = x_1 - 3x_2. 
\end{gathered}
$$ The linealization matrix around the equilibrium point $x^* = \{\frac{2}{\sqrt{3}},\frac{2}{3\sqrt{3}}\}$ , is $A = 
\begin{bmatrix} 
-3 & 1 \\
 1 & -3\\ 
\end{bmatrix}
$ ,
and so, $P = 
\begin{bmatrix} 
3/16 & 1/16 \\
 1/16 & 3/16\\ 
\end{bmatrix}
$ . Given the previous values, I took the derivative of V(x) w.r.t. time and substituted the system in the equation (did it in Mathematica to try and simplify it as much as possible), giving $$
\dot{V}(x) = \frac{1}{8}\left(4x_1^2-3x_1^4+4x_1x_2-x_1^3x_2-8x_2^2\right).
$$ Now I need to find a region where $\dot{V}(x)$ is negative definite, but when I try to use inequalities with the norm of $x$ I get only positive norms raised to a power, and so $\dot{V}(x)$ can never be negative definite. Using $$
|x_1|\leq||x||, \quad |x_1x_2|\leq\frac{1}{2}||x||^2,
$$ I arrived at $$
\begin{gathered}
\dot{V}(x) \leq \frac{1}{8}\left(4||x||^2+3||x||^4+2||x||^2+\frac{1}{2}||x||^4+8||x||^2\right) \\
\leq \frac{1}{8}\left(14||x||^2+\frac{7}{2}||x||^4\right).
\end{gathered}
$$ As you can see, it never is negative. Am I being overly aggresive with converting everything to norms? How can I find the region where $\dot{V}(x)$ is negative? Thanks in advance.","['ordinary-differential-equations', 'lyapunov-functions', 'basins-of-attraction', 'stability-theory', 'dynamical-systems']"
4143832,"Show that $T: L^2([0, \infty )) \to C_0([0,\infty))$ is compact operator.","Let $C_0 \left( [0,\infty) \right)$ be the set of continuous functions on $[0,\infty)$ vanishing at $\infty$ , which is Banach space with $\lVert f \rVert_\infty = \underset{t \in [0,\infty)}{\sup} |f(t)|$ . Define $T \in B\left(L^2([0,\infty)), C_0 ([0,\infty)) \right)$ by $$Tf(t) = \frac{1}{1+t} \int_0^t f(s) ds.$$ Show that $T$ is a compact operator. Hint: To show that $TB_{L^2 [0,\infty)}$ is totally bounded, take sufficiently large $R >0$ such that $\underset{t \ge R}{\sup} |Tf(t)|$ is uniformly small for $f \in B_{L^2[0,\infty)}$ , and then apply the Ascoli-Arzela theorem to the restriction of $TB_{L^2[0,\infty)}$ to $[0,R]$ . I do not know how to apply this hint. Edited: I have seen some similar questions in the web, but all of them was on closed intervals. In this case the interval is not bounded, so at first -as I know- I have to prove that $T$ is bounded, which is not realy clear when the interval is not bounded. And After that I need to prove the compactness of the operator, by definition or by theorems, Could you please show me steps (hints) that i can follow. My proof till now: First let's show the boundedness of $T$ . $|Tf(t)| \le \frac{1}{1+t} \int_0^t f(s)ds \le \|f\|_2 \frac{t}{1+t}$ . So, $\|T\| \le 1$ Is is OK?","['compact-operators', 'functional-analysis']"
4143844,Moment generating function of continuous random variable,"I got a problem that said ""find the Moment-generating function of $$f(t)=\frac{1}{4}e^{-t/4}\mathbb{I}_{(0,\infty)}(t)$$ And I solved it like this but I don't know if this' right $$\begin{align*}&M_{T}(y)=E\left [ e^{yt} \right ]\\&=\int_{0}^{\infty}e^{yt}\frac{1}{4}e^{-t/4}\,dt\\&=\frac{1}{4}\int_{0}^{\infty}e^{yt-t/4}\,dt \end{align*}$$ so $\lim_{t\rightarrow \infty}\frac{1}{4}\int_0^\infty e^{yt-t/4} \,dt$ Let $u=yt-\frac{t}{4}\Rightarrow du=(y-\frac{1}{4})dt \Rightarrow \frac{1}{y-\frac{1}{4}}\, du=dt$ $$\begin{align*}&\lim_{t\rightarrow \infty}\frac{1}{4}\int_0^{yt-\frac{t}{4}} e^u \frac{1}{y-\frac{1}{4}} \, du\\&=\lim_{t\rightarrow \infty}\frac{1}{4(y-\frac{1}{4})} \int_0^{yt-\frac{t}{4}} e^u \, du\\&=\lim_{t\rightarrow \infty}\frac{e^{yt-\frac{t}{4}}-1}{4y-1} \end{align*}$$ So $M_T(y)=\lim_{t\rightarrow \infty}\frac{e^{yt-\frac{t}{4}}-1}{4y-1}$","['moment-generating-functions', 'limits', 'probability', 'random-variables']"
4143847,Reference request on Ehresmann connections,"I've been asked read up on Ehresmann connections. I have experience in smooth manifold theory, vector bundles, and Riemannian geometry, but I feel in unfamiliar territory after briefly looking into the definition online. Therefore, I would like to find a good book so that I can learn the groundwork required to understand what an Ehresmann connection is. It seems there are a few ways to characterize an Ehresmann connection. My professor describes it as being defined in terms of a horizontal/vertical splitting for locally trivial fibrations, so I'd like to figure out what this means and how the Ehresmann connection is related to this ""splitting"".","['connections', 'reference-request', 'differential-geometry']"
4143866,When will $f(x)=c$ separate the space into two parts ($f(x)>c$ and $f(x)<c$)?,"When will $f(x)=c$ separating the space into two parts ( $f(x)>c$ and $f(x)<c$ )? Here $x$ is a vector. Let me clarify what I mean by ""separating"". I want $f(x)=c$ to be a continuous boundary such that $f(x)>c$ and $f(x)<c$ are on each side of it. I don't know the exact definition of ""side"" in mathematics, but I feel that the way $\mathbb{R}^2$ is separated as inside and outside by $x^2+y^2=1$ does not count. Is there any other condition in addition to $f$ being linear? My guess is that $f(x)$ being monotonic (and continuous) is a sufficient condition. For example, if $f(x,y)$ is strictly increasing in both $x$ and $y$ , then $f(x)>c$ will be to the northeast of $f(x)=c$ and $f(x)<c$ will be to the southwest of $f(x)=c$ . I really appreciate your help, thank you very much!","['general-topology', 'geometry']"
4143872,Bisecting two areas with one line,"I'm not a mathematician, so please feel free to improve my terminology. In 2D, I am wondering if it is always possible to bisect two non-intersecting, non-overlapping convex arbitrary shapes with a single line no matter the position or orientation of the shapes. For example, see the image below.  I want the line to cut the blue shape into two equal pieces.  Same for the red shape. It seems pretty obvious to me that this should always be possible, but how would one prove it? This question came up while I was cutting slices of garlic bread with a knife.  I cut two pieces at a time to speed up the process.  I have to cut the pieces almost exactly in half so my kids won't argue about who got the most.  :-)",['geometry']
4143957,Joint probability upper bound,"Let $X$ and $Y$ independent random variables distributed as $N(0,1)$ . For any $a\in [-1,1]$ , and any positive $t\in\mathbb{R}$ how to find an upper bound of the joint probability $P(aX+\sqrt{1-a^2}Y>t, X>t)$ ? Here $t$ can be thought of a large number so that we are interested in the upper tail and using standard normal tail bound $P(X>t)\leq e^{-t^2/2}$ . My primary interest is to bound to the quantity $P(aX+\sqrt{1-a^2}Y>t | X>t)$ , which can be written as $\frac{P(aX+\sqrt{1-a^2}Y>t, X>t)}{P(X>t)}$ . For the conditional probability, consider three extreme cases: i) $a=1$ , then $P(X>t | X>t)=1$ ii) $a=0$ , then $P(Y>t|X>t)=P(Y>t)\leq e^{-t^2/2}$ (since $X$ and $Y$ are independent and using tail bound of $Y$ ). iii) $a=-1$ , then $P(-X>t|X>t)=0$ . For any other values of $a$ , how to find an upper bound of the conditional probability as a function of $a$ ?","['statistics', 'probability-distributions', 'computational-mathematics', 'probability-theory', 'probability']"
4143987,"Lipschitz-continuity of the vector valued function $f(t,y_1,y_2) = (y_2, At - B\sin y_1)$","In order to show that the function $f(t,y_1,y_2) = (y_2, At - B\sin y_1)$ , is Lipschitz-continuous with respect to the variable $\underline{y}=(y_1,y_2)$ , it is sufficient to show there exists a number $K\geq 0$ , such that $\vert \partial_{j} f(t,y_1,y_2)\vert\leq K$ for all $j = (y_1,y_2)$ . So this one $K$ must bound each of the partials of $f$ . Because $$\vert \partial_{1} f(t,y_1,y_2)\vert=\vert (0,-B\cos y_1)\vert\leq \vert B\vert$$ and $$\vert \partial_{2} f(t,y_1,y_2)\vert=\vert (1,0)\vert\leq1$$ then choosing $K:=\vert B\vert+1$ implies that the function is lipschitz-continuous with respect to $\underline{y}=(y_1,y_2)$ . This $K$ is not necessarily the minimal choice, but it has no bearing on the conclusion, correct?","['multivariable-calculus', 'lipschitz-functions']"
4144001,General formula for $\int_0^1 (x-1)(x-2)\dots (x-n) dx$,"Is there a general formula for $$
I_n = \int_0^1 (x-1)(x-2)\dots (x-n) dx
$$ in terms of familiar sequences of number (Harmonic numbers, Bell numbers, Bernoulli numbers, etc) ? Define $$
P_n(x) = \int_0^x (u-1)(u-2)\dots (u-n) du
$$ Have the polynomials $P_n(x)$ appeared in the literature? Is there a simple recursive formula for $P_n(x)$ ? What are some interesting properties of $P_n(x)$ ? The explicit formula for $P_n(x)$ can be obtained by expanding $(u-1)(u-2)\dots(u-n)$ and integrating terms by terms $$
P_n(x) = \frac{x^{n+1}}{n+1} + \sum_{k=1}^n \frac{x^{n-k+1}}{n-k+1} \sum_{1 \leq j_1 < \dots < j_k \leq n} (-1)^k j_1 j_2 \dots j_k
$$ Also, it is easy to see that $$
P_{n+1}'(x) = (x-n-1)P'_n(x)
$$","['polynomials', 'sequences-and-series']"
4144027,Inverse of structured block matrix,"Let $V$ be a finite-dimensional vector space and consider the space $X=V\times V\times V\times V.$ Consider the block matrix $$A = \begin{pmatrix} A_1 & A_2 \\ A_2^* & -A_1\end{pmatrix}$$ where $A_1 = \operatorname{diag}(\lambda_1,\lambda_2)$ for $\lambda_i \in \mathbb C$ and $A_2: V^2 \to V^2.$ We then consider $$K=(A-\lambda)^{-1}.$$ Question: Can we express the resolvent in the form $$K = \begin{pmatrix} T_1(\lambda)(T_2-\lambda)^{-1} & * \\ * & T_3(\lambda) (T_4-\lambda)^{-1}\end{pmatrix}$$ where $T_1,..., T_4$ are some matrices and $T_1,T_3$ may depend smoothly on $\lambda$ , whereas $T_2,T_4$ are independent of $\lambda$ and $*$ are elements I do not really care about. Please let me know if you have any questions.","['operator-theory', 'analysis', 'real-analysis', 'matrices', 'linear-algebra']"
4144040,Is this a valid proof for the normalness of subgroups in galois theory?,"Let $F/\mathbb{Q}$ be a finite galois extension, and $Aut(F/\mathbb{Q})$ be the corresponding galois group. What I want to show is that for all intermediate fields between $F$ and $\mathbb{Q}$ , where $\mathbb{Q} \subset F_{1} \subset F_{2} \cdots \subset F_{m} = F$ , if the field extensions are normal, then the corresponding subgroups of $Aut(F/\mathbb{Q})$ are also normal. What I thought about was, because we can represent $F$ as a vector space over $\mathbb{Q}$ , we can map every element in $Aut(F/\mathbb{Q})$ to matrices of $|F|$ dimension. My question is, if all the field extensions are normal, can we describe elements of $Aut(F/\mathbb{Q})$ as diagonal matrices? Because since all diagonal matrices are commutable, for any intermediate field, we would have $g\cdot Aut(F/F_{k}) = Aut(F/F_{k}) \cdot g$ for $g \in Aut(F/F_{k})$ , hence $Aut(F/F_{k})$ is a normal subgroup of $Aut(F/F_{k})$ . What I'm currently not sure about is whether or not diagonal matrices can represent all possible automorphisms of $F$ that fixes $\mathbb{Q}$ . If this is a valid argument, is it because the normalcy of the field extensions implies that all automorphisms on the field can be written as a diagonal matrix? If not, is it possible to prove the normalcy of subgroups using this line of thought?","['galois-theory', 'galois-extensions', 'group-theory', 'abstract-algebra']"
4144069,Is this relation already discovered?,"$$
\sum_{d \mid (n,k_1,k_2, \dots,k_m)}\mu(d)\binom{n/d}{k_1/d, k_2/d, \dots, k_m/d} \equiv 0 \pmod n
$$ where $\mu$ is the Moebius mu function.
I've found above interesting divisibility properties. I've already proven this and it seems just exercise-level. I would like to know it is alreay proven in somewhere.","['number-theory', 'elementary-number-theory', 'reference-request', 'mobius-function', 'multinomial-coefficients']"
4144083,Finding the kernel of a morphism : Galois Theory and Decomposition Field,"I am currently doing an internship in a research laboratory ( I am in my third year of Bachelor ) and I'm really struggling with the things I have to do. For instance, here's something I'm having trouble with. Let $L$ be a finite Galois extension of $\mathbb{Q}$ , $O_L$ its ring of integers, and $I$ an ideal of $O_L$ . Let $K$ be the decomposition field of $I$ . Let $R=I\cap O_K$ . Suppose $n=[L : \mathbb{Q} ]$ and $g=[ K : \mathbb{Q} ]$ . Suppose also that we have a basis $(b_i)_{1 \leq i \leq n/g}$ of $O_L$ over $O_K$ . In order to find an isomorphism between $(O_K/R)^{n/g}$ and $O_L/I$ , I wanted to proceed like that : $(O_K)^{\frac{n}{g}} \overset{f_1}{\longrightarrow} O_L \overset{f_2}{\longrightarrow} O_L /I$ $f_1$ being : $ (x_1,\cdots , x_{n/g}) \longrightarrow \sum\limits_{i=1}^{n/g} x_i b_i$ and $f_2$ being the canonical surjection. In order to find my isomorphism, I need to prove that the kernel of the composition of $f_1$ and $f_2$ is $R^{\frac{n}{g}}$ . The Kernel of $f_2$ being $I$ , what's left to prove is that : $R^{\frac{n}{g}}=\{ x\in (O_K)^{\frac{n}{g}} ~:~ f_1(x)\in I \}$ The $\subset$ of the equality is easy, but I can't prove the $\supset$ . Please forgive my mistakes, I'm still learning, and English isn't my first language. Thank you for your help !","['galois-theory', 'number-theory', 'algebraic-number-theory']"
4144089,The possible orientations of a $2 \times 2 \times 2$ Rubik’s cube,"So a $2 \times 2 \times 2$ cube has $8$ distinct pieces. With each of them having 3 colours(one on each of their exposed edges). Thus, the as echo piece has $3$ different orientations and there are $8$ pieces, won’t there be a total of $3^8=6561$ combinations? I looked it up and found out there were around $3,674,160$ combinations. Where am I wrong ?","['permutations', 'combinatorics', 'rubiks-cube']"
4144091,Why is this map on the tensor product not surjective?,"Let $X,Y$ be sets and consider $\mathbb{C}(X)$ the set of all functions $X \to \mathbb{C}$ with the componentwise operations. I showed that there is an embedding of algebras $$\mathbb{C}(X) \otimes \mathbb{C}(Y) \to \mathbb{C}(X \times Y): f \otimes g \mapsto [(x,y)\mapsto f(x)g(y)]$$ If $X,Y$ are infinite, I believe this mapping is not surjective. My intuition is that there are functions $g(x,y)$ which can not be written as $\sum_i f_i(x) g_i(y)$ (i.e. we can not always detach the variables). However, I struggle to formalise this idea. Any help?","['abstract-algebra', 'tensor-products']"
4144141,Solving PDE with Laplace Transform - Non Homogenous ODE,"Given the following PDE: $$u_{tt}(x,t)-u_{xx}(x,t)=e^{-t}$$ $$-\infty <x<+ \infty , t>0$$ $$u(x,0)=0,u_t(x,0)=\frac{1}{1+x^2}$$ solve it by using the Laplace transform. Here's my attempt so far: Let $$\mathcal L \{u(x,t) \} \equiv U(x,s)\equiv\int_0^\infty u(x,t)e^{-st} \,dt .$$ and the result I've calculated is: $$U_{xx}(x,s)-s^2 U(x,s) = -\frac{1}{1+x^2}-\frac{1}{s+1}=f(x,s)$$ I've concluded that the relation above is a non-homogenous 2nd order ODE in respect to the variable x. I have tried to solve it but so far it hasn't worked. Which method do you think would be the best to solve it? Could you provide some guiding steps?","['laplace-transform', 'ordinary-differential-equations', 'partial-differential-equations']"
4144144,How to understand Gimbal Lock? [duplicate],"This question already has answers here : Euler angles and gimbal lock (5 answers) Closed 2 years ago . I have read so many articles on the web about Gimbal Lock and also many Q&A on Stackexchange/Stackoverflow, but nevertheless I cannot fully understand why Gimbal Lock should occur and why it should be a problem. Let our Euler Angles formalism the following: at the beginning the object reference frame and the world reference frame are aligned (right-hand rule) the first rotation occurs about the (world or object) z-axys (yaw) the second rotation occurs about the object y-axis which is different from the world's one (pitch) the third rotation occurs about the object x-axis which is different from the world's one (roll) Note that I am using what Wikipedia calls intrinsic formalism (ZYX or z,y',x'') to highlight that the pitch and roll rotations occur about new axis. From several animations with physical gimbals I understood that when the yaw-gimbal and the roll-gimbal are aligned (and it happens when the object pitches of $\pm\frac\pi2$ ) then the rotation about those gimbals are the same. Note that in this case we are facing with a different formalism because for example when the object pitches then the yaw gimbal doesn't follow the object orientation (see for example this top cited video at 0:47). Anyway mathematically speaking I cannot understand why gimbal lock should occur. I try to explain what i mean in the following. Denoting with $$R_X=\begin{bmatrix}1&0&0\\
0&\cos(\varphi)&-\sin(\varphi)\\
0&\sin(\varphi)&\cos(\varphi)\end{bmatrix}$$ $$R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\
0&1&0\\
-\sin(\theta)&0&\cos(\theta)\end{bmatrix}$$ $$R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\
\sin(\psi)&\cos(\psi)&0\\
0&0&1\end{bmatrix}$$ the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles $(yaw,pitch,roll)=(\psi,\theta,\varphi)$ is given by: $$R=R_Z\cdot R_Y\cdot R_X$$ (note the order of rotations ZYX because I am working in the object reference frame see this ). Suppose now that i perform the following elementary rotations: angle $\psi$ about z-axis (yaw) angle $\theta = -\frac\pi2$ about y-axis (pitch which should cause gimbal lock) angle $\varphi$ about x-axis (yaw) Composing these rotation using the formula above I obtain: $$R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\
0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\
1&0&0\end{bmatrix}$$ and in different articles they say that the fact that the only angle that appear in that expression is $(\varphi+\psi)$ means that yaw and roll are linked and so we lose a degree of freedom. But it sounds strange to me. The only thing I can deduce from this result is that there are infinite sequences yaw-pitch-roll (or better yaw- $\left(-\frac\pi2\right)$ -roll) which lead to the same orientation. Also I could note that given this orientation I cannot uniquely determine which sequence leads to it, but I cannot see any other problem. Some articles say that when I am in a gimbal lock situation, to reach some position near the object I need to make a non-intuitive path (and this could be a problem in rendering applications, I have seen some animations in which the object overturns after being in a gimbal lock situations and before to reach another orientation near the gimbal lock one) but I don't know why it should happen). Let be in this situation: $$R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\
0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\
1&0&0\end{bmatrix}$$ and suppose $\varphi+\psi = 0$ . I obtain $$R_0=\begin{bmatrix}0&0&-1\\
0&1&0\\
1&0&0\end{bmatrix}$$ Perturbing $\varphi$ or $\psi$ by a small $\varepsilon>0$ , we have $$R_{\varepsilon}=\begin{bmatrix}0&-\sin(\varepsilon)&-\cos(\varepsilon)\\
0&\cos(\varepsilon)&-\sin(\varepsilon)\\
1&0&0\end{bmatrix}$$ which could be approximated by $$R_{\varepsilon}=\begin{bmatrix}0&-\varepsilon&\varepsilon^2-1\\
0&1-\varepsilon^2&-\varepsilon\\
1&0&0\end{bmatrix}$$ that seems to me a trasformation ""near"" to $R_0$ . So please help me to understand what I am missing.","['matrices', 'angle', 'rotations']"
4144377,Cardinality of set of limit points,"Let $S$ be subset of real with usual Topology. $T$ be set of all isolated points of $S$ .
What can we say about the cardinality of $T'$ .
Where $T'$ denotes set of limit points of T.
I have found examples of $S$ such that $T'$ can be countably infinite set.
Is it possible that $T'$ can be uncountable set ?","['elementary-set-theory', 'real-numbers', 'general-topology', 'real-analysis']"
4144451,Is a cylindroid a manifold with corners?,"First of all to follows we remember some elementary definitions and results about manifolds. Definition A function $f$ defined in a subset $S$ of $\Bbb R^k$ is said of class $C^r$ if it can be extended to a function $\phi$ (said $C^r$ -extension) that is of class $C^r$ in a open neighborhood of $S$ . Lemma If $f$ is a function defined in a subset $S$ of $\Bbb R^n$ such that for any $x\in S$ there exist a function $f_x$ defined in a neighborhood of $x$ that is of class $C^r$ and compatible with $f$ on $U_x\cap S$ then $f$ is of class $C^r$ . Lemma If $U$ is an open set of $H^n_k:=\Bbb R^{n-k}\times[0,+\infty)^k$ for any $k\le n$ then the derivatives of two different extensions $\phi$ and $\varphi$ of a $C^r$ -function $f$ agree in $U$ . Definition A $k$ -manifold with boundary/corners in $\Bbb R^n$ of calss $C^r$ is a subspace $M$ of $\Bbb R^n$ whose points have a neighborhood $V$ in $M$ that is the immage of a homeomorphism $\phi$ of calss $C^r$ defined an open set $U$ of $\Bbb R^k$ or of $H^k_1/H^k_m$ and whose derivative has rank $k$ . So now let be $M$ a $(n-1)$ -manifold with boundary of class $C^r$ in $\Bbb R^n$ and thus let be $\gamma$ a injective curve defined in the unit interval $I:=[0,1]$ such that $\gamma(0)=0$ and such that the unit tangent vector not lies -for each $t\in I$ - to the tangent space at any point of $M$ . So we call cylindroid $C$ of trajectory $\gamma$ and of section $M$ the set $$
C:=\bigcup_{t\in I}\big(M+\gamma(t)\big)
$$ that is obtained moving along $\gamma$ the points of $M$ . Click here to see an example. So if $\xi\in C$ then there exist a coordinate patch $\alpha:U\rightarrow V$ and $t\in I$ such that $$
\xi=\alpha(x)+\gamma(t)
$$ for any $x\in U$ and thus let be $\phi$ the function from $U\times I$ to $\Bbb R^n$ defined through the equation $$
\phi(x,t):=\alpha(x)+\gamma(t)
$$ for any $(x,t)\in U\times I$ and then we prove that any restriction of this function is a coordinate patch about $\xi$ . So we observe that the immage of $\phi$ is open in $C$ : indeed the immage $V$ of $\alpha$ is an open set of $M$ so that there exist an open set $W$ of $\Bbb R^n$ whose intersection with $M$ is $V$ and so remembering that the translation is a bijection we observe that $$
\phi[U\times I]=\bigcup_{t\in I}\big(V+\gamma(t)\big)=\bigcup_{t\in I}\Big((W\cap M)+\gamma(t)\Big)=\bigcup_{t\in I}\Big(\big(W+\gamma(t)\big)\cap\big(M+\gamma(t)\big)\Big)=\\
\Biggl(\bigcup_{t\in I}\big(W+\gamma(t)\big)\Biggl)\cap\Biggl(\bigcup_{t\in I}\big(M\cap\gamma(t)\big)\Biggl)=\Biggl(\bigcup_{t\in I}\big(W+\gamma(t)\big)\Biggl)\cap C
$$ and thus we conclude that the immage of $\phi$ is open in $C$ because the translations are a homeomorphism and thus $W+\gamma(t)$ is open for each $t\in I$ . Now if $\overset{°}M$ and $\partial M$ are the sets of the interior and boundary points of $M$ then we observe that $C$ is union of the sets $$
\bigcup_{t\in\text{int}\, I}\big(\overset{°}M+\gamma(t)\big)\,\,\,\text{and}\,\,\,\bigcup_{t\in\text{bd}\, I}\big(\overset{°}M+\gamma(t)\big)\,\,\,\text{and}\,\,\,\bigcup_{t\in I}\big(\partial M+\gamma(t)\big)
$$ so that we analyse separately the case where $\xi$ is an element of the first set and the case where $\xi$ is an element of the second or either of the third: in particular this means to analyse separately the case where the set $ U $ is open in $ \Bbb R^{n-1} $ and $ t $ is an element of $ \text{int} \, I $ and the case where this is not and so just this is what we will do to follows -clearly this can be done independentely from the definition of the three mentioned sets   nevertheless we decided to define them because we thought it makes more clear the following argumentetions, that's all. In particular in the example posted above these sets are respectively the invisible part, the red part, the green and balck parts. So first of all we observe that if $\xi$ is an element of the first set then $\alpha$ is a coordinate patch of $M$ defined in an open set $U$ of $\Bbb R^{n-1}$ and so the map $\phi$ defined above is a diffeomorphism in a neighborhood at any point of $U\times\text{int}\,I$ where is contained $\phi^{-1}(\xi)$ , because if the unit tangent vector of $\gamma$ not lies to the tangent space at any point of $M$ then the derivative of $\phi$ is an isomorphism and so the statement follows directely from the inverse function theorem. So we conclude that the set $$
\bigcup_{t\,\in\,\text{int}\,I}\big(\overset{°}M+\gamma(t)\big)
$$ is a $n$ -manifold without boundary. In particular in this way we proved that the invisible part of the linked example is a manifold without boundary. Now if $\xi$ is a not an element of the first set then the previous argumentations hold only with some efforts that we show to follow. So the functions $\alpha$ and $\gamma$ can be extended to two $C^r$ -functions $\beta$ and $\psi$ defined in a open neighborhood of $U$ and $I$ respectively so that the function $\phi$ can be extended to a function $\varphi:=\beta+\psi$ defined in a open neighborhood of $U\times I$ and in particular at any point $(x,t)$ of $U\times I$ this function has not singular derivative so that by the inverse function theorem there exist a (rectangular) open neighborhood $W_x\times W_t$ where $\varphi$ is a diffeomorphism. Now the set $\varphi[W_x\times W_t]$ is open in $\Bbb R^n$ and it is not disjoint from $C$ -indeed $W_x\times W_t$ is not disjoint from $U\times I$ and $\varphi$ is compatible with $\phi$ in $U\times I$ and the immage of $\phi$ is contained in $C$ - so that by the continuity of $\phi$ the set $$ 
\phi^{-1}[\varphi[W_x\times W_t]]
$$ is (not empty and) open in $U\times I$ and contains $W_x\times W_t\cap U\times I$ where $\phi$ is a diffeomorphism. So if we prove that the immage of $W_x\times W_t\cap U\times I$ through $\phi$ is open in $C$ then we conclude that the restriction of $\phi$ to this set is a coordinate patch defined in a open set of $\Bbb R^{n-1}\times[0,+\infty)$ or in a open set of $\Bbb R^{n-2}\times[0,+\infty)^2$ if $U$ is open in $\Bbb R^{n-1}$ and $t$ is not an element of $\text{int}\, I$ or if $U$ is not open in $\Bbb R^{n-1}$ and $t$ is an element of $\text{int}\, I$ or either if $U$ is not open in $\Bbb R^{n-1}$ and $t$ is not an element of $\text{int}\, I$ respectively. So how do this? Could someone help me, please? To follows some observations that I tried to use: naturally you are not constrained to read it if you do not desidre. OBSERVATION If the function $\phi$ was injective in $U\times I$ the statement follows immediately because in particular the function $\varphi$ would be injective in $W_x\times W_t\cup U\times I$ so that $$
\phi[W_x\times W_t\cap U\times I]=\varphi[W_x\times W_t\cap U\times I]=\varphi[W_x\times W_t]\cap\varphi[U\times I]=\\
\varphi[W_x\times W_t]\cap\phi[U\times I]
$$ having remembered that $\varphi$ and $\phi$ are compatible in $U\times I$ . So in particular we observe that the injectivity of $\phi$ follows immediately if the set $M+\gamma(t_0)$ and $M+\gamma(t_1)$ was disjoint for any $t_0,t_1\in I$ such that $t_0\neq t_1$ and in particular I tried to prove this using the injectivity of $\gamma$ that above I did not use. Moreover since $\alpha$ is a homeomorphism and since $W_x$ is open in $\Bbb R^{n-1}$ then $\alpha[W_x\cap U]$ is open in $M$ and so exist a open set $W$ of $\Bbb R^n$ whose intersection with $M$ is $\alpha[W_x\cap U]$ and thus implementing the argumentations used above it follows that $$
\phi[W_x\times W_t\cap U\times I]=...=\Biggl[\bigcup_{t\in W_t\cap I}\big(W+\gamma(t)\big)\Biggl]\cap\Biggl[\bigcup_{t\in W_t\cap I}\big(M+\gamma(t)\big)\Biggl]
$$ so that if the set $\Biggl[\bigcup_{t\in W_t\cap I}\big(M+\gamma(t)\big)\Biggl]$ and $\Biggl[\bigcup_{t\in I\setminus W_t}\big(M+\gamma(t)\big)\Biggl]$ was disjoint then $\phi[W_x\times W_t\cap U\times I]$ was open in $C$ and this surely happens if the set $M+\gamma(t_0)$ and $M+\gamma(t_1)$ was disjoint for any $t_0,t_1\in I$ such that $t_0\neq t_1$ .","['manifolds-with-boundary', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
4144461,Q: Volume involving spherical and polar coordinates,"I'm trying to find the volume between the surfaces defined by the following equations: $$
x^2 + y^2 + z^2 = b^2
$$ $$
y^2 \tan^2 a = x^2 + z^2
$$ I need to find the volume using two different methods: the first one is by spherical coordinates and the second one by polar coordinates. The problem is I get two different results: Using spherical coordinates I found: $$
x = \rho \sin \phi \cos \theta \\
z = \rho \sin \phi \sin \theta \\
y = \rho \cos \phi
$$ $$
V= \int_0^{2\pi}\int_0^{a}\int_0^{b}\rho^2\sin\phi \:d\rho d\phi d\theta = \frac{2\pi}{3}b^3(-\cos(a)+1)
$$ and using polar: $$
x = r \cos \theta \\
z = r \sin \theta \\
y = y
$$ $$
V = \int_0^{2\pi}\int_0^{b\sin a}\int_0^{\sqrt{b^2-r^2}}r \:dy dr d\theta = -\frac{2\pi}{3}b^3(\cos^3(a)-1)
$$ So I know I have made a mistake at some point but I can't find it. If anyone can help I'd appreciate it. Thanks!","['volume', 'multivariable-calculus', 'calculus', 'change-of-variable', 'multiple-integral']"
4144528,"Show that if $f\in\mathcal L^1((a,b))$ has a weak derivative, then it has a continuous modification","Let $\lambda$ denote the Lebesgue measure on $\mathcal B(\mathbb R)$ , $a,b\in\mathbb R$ with $a<b$ , $I:=(a,b)$ , $X,Y$ be $\mathbb R$ -Banach spaces and $\iota$ be a continuous embedding of $X$ into $Y$ . If $p\in[1,\infty]$ , say $f\in\mathcal L^1(I,X)$ has a weak derivative in $L^p(I,Y)$ if there is a $g\in\mathcal L^p(I,Y)$ with $$\int_I\varphi'\iota f\:{\rm d}\lambda=-\int_I\varphi g\:{\rm d}\lambda\;\;\;\text{for all }\varphi\in C_c^\infty(I).$$ In that case, $f':=g$ . I would like to show that if $f\in\mathcal L^1(I,X)$ has a weak derivative in $L^1(I,Y)$ , then there is a $\tilde f\in C(I,Y)$ with $\iota f=\tilde f$ $\lambda$ -a.e. if $Y=X'$ and $X$ is continuously embedded into a Hilbert space $H$ via $\kappa:X\to H$ , then in the situation of (1.) there is a $\tilde f\in C(I,Y)$ with $\kappa f=\tilde f$ $\lambda$ -a.e. Regarding (1.): It's easy to see that if $g\in\mathcal L^1(I,Y)$ and $c\in Y$ , then $$f(t):=c+\int_{(a,\:t)}g\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I$$ is continuous and weakly differentiable with $f'=g$ . Now, in the situation of (1.), by the aforementioned fact, $$g(t):=\int_{(a,\:t)}f'\:{\rm d}\lambda\;\;\;\text{for }t\in\overline I$$ is continuous and weakly differentiable with $$g'=f'\tag2.$$ Now, in Mathematical Tools for the Study of the Incompressible Navier-Stokes Equations and Related Models (Corollary II.4.2), the special case $X=Y=\mathbb R$ is considered and we can find the following argumentation The mentioned lemma is the following: Maybe I'm missing something, but I don't get why the argumentation is that ""complicated"". By $(1)$ and $(2)$ , it holds $$\int_I\varphi'(\iota f-g)\:{\rm d}\lambda=0\;\;\;\text{for all }\varphi\in C_c^\infty(I)\tag3.$$ Now, if $\varphi\in C_c^\infty(I)$ , then $$\psi(t):=\int_a^t\varphi(s)\:{\rm d}s\;\;\;\text{for }t\in I$$ is obviously in $C_c^\infty(I)$ as well and we've got $\psi'=\varphi$ . So, we can replace "" $\varphi'$ "" in $(3)$ by "" $\varphi$ "". And now the du Bois-Reymond lemma should immediately yield $\iota f=g$ $\lambda$ -a.e.. Am I missing something? Regarding (2.): At the moment, I've got no idea why this follows and would need some help to tackle that problem.","['measure-theory', 'real-analysis', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4144542,complex inequality (solution verification) [duplicate],"This question already exists : Solution verification to complex analysis problem, prove $f$ is a polynomial. Closed 3 years ago . Let $f$ be entire, $|f(z)|\leq 3|z|^4+1$ prove $f$ is a polynomial with deggre $\leq 4$ Solution: $f^{(4)}(z_0)=\frac{4!}{2\pi i} \int_c\frac{f(z)}{(z-z_0)^5}dz$ , $\Rightarrow$ $|f^{(4)}(z_0)|\leq \frac{4!}{2\pi i} \int_c |\frac{f(z)}{(z-z_0)^5}|dz \leq \frac{4!}{2\pi i} \int_c \frac{|f(z)|}{|(z-z_0)^5|}dz \leq$ $$\leq \frac{4!}{2\pi i} \int_{0}^{2\pi}\frac{|f(z_0+re^{it})ire^{it}|}{|(re^{it})^5|}dt \leq \frac{4!}{2\pi i} \int_{0}^{2\pi}\frac{(3(|z_0|+r)^4+1)r}{r^5}dt \leq  \frac{4!}{2\pi i} 2\pi\frac{(3(|z_0|+r)^4+1)}{r^4}$$ that means $f^{(4)}(z)$ is bounded so from Liouville's theorem its constant and therefore $f$ is a polynomial with deggre $\leq 4$ Is my solution correct ?","['complex-analysis', 'solution-verification']"
4144595,Find $f\in C^{1}$ such that $x\partial_{x}f+y\partial_{y}f=(x^2+y^2)^{1/2}$,"In a problem I am looking to find a function $C^{1}$ , $f:\mathbb{R}^{2}\to\mathbb{R}$ such that \begin{equation*}
    x\frac{\partial f}{\partial x}(x,y)+y\frac{\partial f}{\partial y}(x,y)=(x^2+y^2)^{1/2}
\end{equation*} I tried doing the change of variables \begin{eqnarray*}
    x=u\text{ and }y=uv
\end{eqnarray*} getting \begin{eqnarray*}
    u\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+uv\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} & = & \sqrt{u^{2}+u^{2}v^{2}}\\
    \Rightarrow u\frac{\partial f}{\partial u} + u\frac{\partial f}{\partial u} & = & \sqrt{u^{2}+u^{2}v^{2}}\\
    \Rightarrow 2u\frac{\partial f}{\partial u} & = & |u|\sqrt{1+v^{2}}\\
\end{eqnarray*} But from this point I don't know how to move forward, I tried to integrate u which led me to a resolution by trigonometric substitution (giving something of the form $f(x,y)=\frac{1}{2}\sec(\theta(x,y))+C$ ) I would appreciate any ideas or indications on how to proceed.","['analysis', 'real-analysis', 'continuity', 'multivariable-calculus', 'partial-differential-equations']"
4144598,$\lim_{x\to 0} \left(\int_0^1 3y+2(1-y)^xdy\right)^{1/x}$,"$$\lim_{x\to 0} \left(\int_0^1 \left(3y+2(1-y)^x\right)  dy\right)^{1/x}$$ I was solving this problem, and when I solve I got the answer which the software says incorrect. My try: Firstly, the simplest approach I directly solved the integration to get $$\lim_{x\to 0} \left(\frac{3x+7}{2x+2}\right)^{\frac1x}=3.5^{\pm \infty}$$ not unique, so i thought it should be $\color{blue}{\text{Does not Exist}}$ Next approach was to use the shortcut formula of $1^\infty$ form involving exponent directly which gives me answer as $\frac1e$ which is also incorrect but later I feel it should be incorrect because when x tends to zero, the inner integral is not tends to 1. Furthermore, if put the limit at the very start, I wanna know out of those 7 indeterminate forms $\color{red}{\text{Which form is this limit ?}}$ Do you also think the question is wrong ?","['integration', 'definite-integrals', 'limits-without-lhopital', 'calculus', 'limits']"
4144655,Evaluate $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}$. [duplicate],"This question already has answers here : Evaluating the series $\sum\limits_{n=1}^{\infty} \frac{1}{n^{3} \binom{2n}{n}} $ (2 answers) Closed 2 years ago . Evaluate $$\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}.$$ My work so far and background to the problem. This question was inspired by the second page of this paper. The author of the paper doesn't mention how he managed to prove the $4$ identities (shown below) at the top of the second page of the paper, so I tried to find my own method of doing so. \begin{align}
\sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1} &= \frac{1}{3}+\frac{2\pi\sqrt3}{27} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1} &= \frac{\pi\sqrt3}{9} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1} &= \frac{\pi^2}{18} \\
\sum\limits_{n=1}^{\infty}\frac{1}{n^4}\binom{2n}{n}^{-1} &= \frac{17\pi^4}{3240}
\end{align} The method I used was similar to the method the author used in the rest of his paper: trying to find the generating functions for particular sequences. Using this method, like the author I was successful in proving the first $3$ identities, since I obtained the following: $$\begin{align}\sum\limits_{n=1}^{\infty}\binom{2n}{n}^{-1}x^n&=4\sqrt\frac{x}{(4-x)^3}\arcsin\frac{\sqrt{x}}{2}+\frac{x}{4-x}\\
\sum\limits_{n=1}^{\infty}\frac{1}{n}\binom{2n}{n}^{-1}x^n&=2\sqrt{\frac{x}{4-x}}\arcsin\frac{\sqrt x}{2}\\
\sum\limits_{n=1}^{\infty}\frac{1}{n^2}\binom{2n}{n}^{-1}x^n&=2\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\end{align}$$ I obtained the second and third power series by dividing the power series above each one by $x$ and then integrating. I then attempted to find a closed form for $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}x^n$ in order to evaluate $\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}$ . The way I tried to do this was finding $$\int\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x$$ but according to WA this has no solution in terms of elementary functions and it involves some polylogarithms which I am not familiar with at all really, especially when they have a complex argument. I then realized that $$\sum\limits_{n=1}^{\infty}\frac{1}{n^3}\binom{2n}{n}^{-1}=\int_0^1\frac{2}{x}\left(\arcsin\frac{\sqrt{x}}{2}\right)^2\mathrm{d}x=\int_0^{1/2}\frac{4}{u}\left(\arcsin u\right)^2\mathrm{d}u$$ (where the second equality is found by using the substitution $u=\frac{\sqrt{x}}{2}$ ) which could be more helpful since definite integrals can often be evaluated in terms of elementary functions even if the indefinite integral cannot. From there however I could not think of a way of continuing. Thank you for your help.","['integration', 'summation', 'calculus', 'closed-form', 'sequences-and-series']"
4144709,$\lim_{n\to\infty}(\int_0^1 \frac{1}{1+x^n} dx)^n$,"$$\lim_{n\to\infty}\left(\int_0^1 \frac{1}{1+x^n} dx\right)^n$$ I solved this problem as in the link: Click here I got the answer as $0$ but it says incorrect. I think the answer is wrong or there is some error with the question. My try: $$=\lim_{n\to\infty}\left(\int_0^1 \sum_{m=0}^{\infty} (-1)^mx^{mn}dx\right)^n$$ $$=\lim_{n\to\infty}\left( \sum_{m=0}^{\infty} \frac{(-1)^m}{1+mn}\right)^n$$ $$=\color{blue}{\lim_{n\to\infty}\left( \frac{\Phi(-1,1,\frac1n)}{n}\right)^n=0 ????}$$ Do you think the problem is incorrect or answer is incorrectly provided by the author ?","['integration', 'definite-integrals', 'limits-without-lhopital', 'calculus', 'limits']"
4144738,"Rubik's cube, elements of order $7$ and $11$","The order of the Rubik's cube group is $$43 252 003 274 489 856 000 = 2^{27} \times 3^{14} \times 5^3 \times 7^2 \times 11$$ Cauchy's theorem guarantees an element of order 7, as well as one of order 11. An element of order 2 is R2. An element of order 3 is a U permutation. An element of order 4 is R. An element of order 5 is R U R' U. But I am unable to find an element of order 7 or one of order 11. Does any one know of such elements?","['group-theory', 'rubiks-cube']"
4144748,"In $S_7$, describe the conjugates of $(1 2)(3 4 5)$","In $S_7$ , describe the conjugates of $(1 2)(3 4 5)$ What I have done I'm not quite sure what the textbook refers to by ""describe"", in any case, I calculated (?) how many of them there are. it is right? If not, what should I actually do? Let $\sigma=(1 2)(3 4 5)$ and $\tau$ any permutatión in $S_7$ , then $\tau \sigma \tau^{-1}$ has the same cycle structure of $\sigma$ . Since $\tau \sigma \tau^{-1}= (\tau(1) \tau(2))(\tau(3) \tau(4) \tau(5)) $ , for $\tau(1)$ we have $7$ possibilities and $6$ possibilities for $\tau(2)$ . Since $(\tau(1)\tau(2))=(\tau(2)\tau(1))$ , we have $(6\cdot7)/2=21$ $2$ -cycles. Similarly, we have $(5\cdot 4\cdot 3)/3=20$ $3$ -cycles. So in total, there are $21\cdot 20=420$ conjugates of $(1 2)(3 4 5)$ .","['permutation-cycles', 'abstract-algebra', 'solution-verification', 'symmetric-groups', 'group-theory']"
4144762,Topological continuity and open sets,"I have a question about the answer in this thread below: Proving restriction of function is continuous The author of the answer says "" $f^{-1}(U) \cap A$ is open in $A$ "". We have $f^{-1}_A(U) = \{a \in A: f_A(a) \in U\} \underbrace{=}_{f = f_A \text{ on }A} \{a \in A: f(a) \in U\} = f^{-1}(U)$ By definition of $f_A$ , we have $f^{-1}(U) \subseteq A.$ So, what's the point of saying (over-specifying?) $f^{-1}(U) \cap A$ is open instead of just saying $f^{-1}(U)$ is open?","['elementary-set-theory', 'general-topology']"
4144770,Bayesian statistics to find a distribution,"Suppose that a random sample $X_1, ..., X_n$ is taken from a distribution where $X$ is a random variable that follows a Poisson distribution with mean $w$ . Furthermore, $w$ is modeled with a random variable $W$ that follows a chi-square distribution with $r = 4$ degrees of freedom. (i) Determine the posterior distribution of $W$ given that $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$ (ii) Find the Bayesian estimator for $W$ that minimizes the mean-square error (iii) Before the sample was taken, Jack tells you that $w$ lies in $(0, 1)$ , however, no further information is provided about which values $w$ would most likely take in this interval. Do you pick a new prior pdf for $W$ ? Justify your answer, stating what new prior pdf you would take if so, or if not, why you would not pick a new prior pdf. Hint: Integration is unnecessary. The posterior distribution should be familiar if you use proportionality! You should use this posterior distribution to find the mean-square error. My attempt: (i) From chi-square with $r = 4$ , the prior pdf of $W$ is $$f_W(w) = \frac{we^{-w/2}}{4}$$ The pdf of a Poisson with mean $w$ is: $$f(x|w) = \frac{w^{x}e^{-w}}{x!}$$ The joint pdf of $X_1, X_2, ..., X_n$ is: $$F(x_1, ..., x_n|w) = \frac{w^{\sum{x_i}}e^{-nw}}{\prod{x_i!}}$$ The posterior probability density function (pdf) of $W$ given $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$ is: $$f(w|x) = \frac{F(x_1, ..., x_n|w) \cdot f_W(w)}{f_X(x)} = \frac{F(x_1, ..., x_n|w) \cdot f_W(w)}{\int_{-\infty}^{\infty} F(x_1, ..., x_n|w) \cdot f_W(w) \,dw}$$ We apply proportionality: $$\frac{F(x_1, ..., x_n|w) \cdot f_W(w)}{\int_{-\infty}^{\infty} F(x_1, ..., x_n|w) \cdot f_W(w) \,dw} \propto F(x_1, ..., x_n|w) \cdot f_W(w) = \frac{w^{1+\sum{x_i}}e^{-nw-\frac{w}{2}}}{4\prod{x_i!}}$$ From here, there are a few possible posteriors to choose from: Chi-square, Exponential or Gamma. By mere inspection: If Chi-square, we have parameter $r = 4 + 2\sum{x_i}$ . If Exponential, we have parameter $\theta = \frac{-2}{w(2n+1)}$ or $\theta = \frac{1}{w^{1+\sum{x_i}}}$ . If Gamma, we have parameters $\alpha = 2 + \sum{x_i}$ and $\theta = \frac{2}{2n+1}$ . I am unsure which one is the correct posterior distribution as they all seem likely. (ii) Depends on (i) but am I supposed to use the fact that $E(W|X)$ is the least mean square here? (iii) Not sure. Any assistance especially with (i) and (iii) is much appreciated.","['statistics', 'probability-distributions', 'parameter-estimation', 'bayesian', 'solution-verification']"
4144800,Poisson distribution with an integer $\lambda$ value,"I have noticed that when a Poisson distribution has an integer value of $\lambda$ , the following holds: $$
\mathbb{P}[X = \lambda] = \mathbb{P}[X = \lambda - 1]
$$ I have been able to prove this rather simply using an algebraic method. However, my question is, is there an intuitive reason for why this is the case ? For example, if an average of 10 phone calls are received an hour, why is it that 10 phone calls are equally likely as 9 phone calls?","['poisson-distribution', 'intuition', 'probability-distributions', 'probability']"
4144817,Confidence interval and moment,"Suppose a random sample of size $n = 9$ is taken, where $X$ is normally distributed with unknown mean $\mu$ and unknown variance $\sigma^2$ . Consider the following cases (a) Before the sample is taken, it is noted that the endpoints of a two-sided $90$ % confidence interval are random variables derived from the sample mean $\bar{X}$ and sample standard deviation $S$ . If $L$ is the length of the interval, find the second moment of $L$ in terms of the variance $\sigma^2$ . (b) The sample is taken and we find that $\bar{x} = 1.56$ and $s^2 = 0.68$ . Determine the approximate one-sided confidence interval for $\mu$ . My attempt: (a) The population variance is unknown. So we find the t value. Using the tables or an online tool with $\alpha = 0.1 \implies \frac{\alpha}{2} = 0.05$ and $8$ degrees of freedom gives $t_{0.05} = 1.8595480$ . (If I am not wrong then) Length = $2 \cdot t_{0.05} \cdot \frac{s}{\sqrt{n}} = 2 \cdot 1.8595480 \cdot \frac{s}{\sqrt{9}} \approx 1.2397s$ . The second moment is given by $E(L^2) = E((1.2397s)^2) = 1.5369E(s^2)$ From this point I am not sure how to write this in terms of $\sigma^2$ . I read of a similar problem that used unbiased estimators but I cannot see what conditions $E(ks^2)$ (where $k$ is a constant) needs to be considered an unbiased estimator of $\sigma^2$ . (b) We have an unknown mean and variance so we use $t$ rather than $z$ . Note that $\alpha = 0.1$ and there are $8$ degrees of freedom. The lower bound, $m$ , is given by $m = \bar{x} - t_{0.1} \cdot \frac{s}{\sqrt{n}}$ . With the tables or the use of an online tool, we get that $t_{0.1} \approx 1.3968153$ . Hence $m = 1.56 - 1.3968153 \cdot \sqrt{\frac{0.68}{9}} = 1.176052$ . Is this correct thus far? Any assistance you could provide with (a) is appreciated.","['statistics', 'solution-verification', 'confidence-interval', 'parameter-estimation']"
4144823,"Convergence of a sequence in $(C[0,1])^*$","I would like to determine if the following sequence of linear functionals on $C[0,1]$ converges weakly (i.e. pointwise) and strongly (with respect to the norm) $$ \phi_n(f)=\int\limits_{0}^{1}(nt^2-[nt^2])f(t)dt,$$ where $[x]$ means greatest integer that is less than or equal to $x$ . I do not know how to investigate this question. Thanks in advance for your help.","['definite-integrals', 'functional-analysis', 'sequences-and-series']"
4144836,Solve Differential Equation: $y' = \frac{\sqrt{x^2+y^2}-x}{y}$,"Solve Differential Equation: $y' = \frac{\sqrt{x^2+y^2}-x}{y}$ This looks like a problem that would do well with a conversion to polar coordinates ( $r^2 = x^2 + y^2$ and $x = r\cos(\theta)$ $y = r\sin(\theta)$ ). However, I am confused on how to change $y' = \frac{dy}{dx}$ to $\frac{dr}{d\theta}$ , in order to do separation of variables. Could someone demonstrate a solution using this approach?","['polar-coordinates', 'ordinary-differential-equations']"
4144842,Sample size from proportions,"Let $X$ be a random variable that follows a Bernoulli distribution with parameter $p$ . If the maximum error of the $90$ % confidence interval is $0.2$ , what sample size is required under the following scenarios (i) $p$ is unknown. (ii) $p \geq 0.8$ My attempt: (i) As $p$ is unknown the sample size needed is: $$n = \frac{z_{\alpha/2}^2 \cdot 0.25}{E^2} = \frac{1.6448536^2 \cdot 0.25}{0.2^2} \approx 16.9096$$ We take the upper integer $17$ . (ii) $p\geq 0.8 \implies 1-p \leq 0.2 \implies p(1-p) \leq 0.16$ The sample size needed here is: $$n = \frac{z_{\alpha/2}^2 \cdot 0.16}{E^2} = \frac{1.6448536^2 \cdot 0.16}{0.2^2} \approx 10.8222$$ We take the upper integer $11$ . Are these correct? Any assistance is appreciated.","['statistics', 'solution-verification', 'random-variables']"
4144884,Function which integrates to $0$ against test functions with mean $0$ is constant almost everywhere.,"Suppose $U$ is a bounded domain in $\mathbb{R}^n$ and $u \in L^1(U)$ has the property that $$\int_{U}u\phi=0 $$ for all $\phi \in C_{c}^{\infty}(U)$ which satisfy $\int \phi = 0$ . I'd like to show that $u = \bar{u}:= \frac{1}{|U|}\int_{U}u$ almost everywhere. Here was my initial approach. We have $$\int(u-\bar{u})^2 = \int u(u-\bar{u}) - \int \bar{u}(u-\bar{u}) = \int u(u-\bar{u}) .$$ Now I'm tempted to approximate $u-\bar{u}$ by smooth compactly supported mollifiers and apply something like the dominated convergence theorem to conclude that the last integral on the line above vanishes, but no straightforward argument is coming to mind. I'd appreciate if someone could help me out here.","['measure-theory', 'distribution-theory', 'real-analysis', 'calculus', 'functional-analysis']"
4144910,"On certain algebraic functions on the interval $[0, 1]$","Let $\mathcal{C}$ be the class of continuous and polynomially bounded functions that map the interval [0, 1] to [0, 1]. A function $f(x)$ is polynomially bounded if both $f$ and $1-f$ are bounded below by min( $x^n$ , $(1-x)^n$ ) for some integer $n$ (Keane and O'Brien 1994). This implies that $f$ admits no roots on (0, 1) and can't take on the value 0 or 1 except possibly at 0 and/or 1. A function $f(x)$ is algebraic over the rational numbers if— it can be a solution of a system of polynomial equations whose coefficients are rational numbers, or equivalently, there is a nonzero polynomial $P(x, y)$ in two variables and whose coefficients are rational numbers, such that $P(x, f(x)) = 0$ for every $x$ in the domain of $f$ . Then: Is a function in the class $\mathcal{C}$ algebraic over the rational numbers only if it's real analytic on the interval $(0, 1)$ ? Is a function in the class $\mathcal{C}$ algebraic over the rational numbers only if it's $\alpha$ -Hölder continuous for some $\alpha > 0$ ? (See note 2 below.) Are there functions in class $\mathcal{C}$ that— are algebraic over the rational numbers, but are not algebraic over the non-negative rational numbers? Motivation: According to Mossel and Peres (2005), a function in the class $\mathcal{C}$ can be simulated by a pushdown automaton only if the function is algebraic over the rational numbers, but the converse is not known to be true. Banderier and Drmota (2015) proved certain things involving algebraic functions over the non-negative real numbers, including their so-called ""critical exponents"" and the fact that these exponents impose a kind of lower bound on the complexity of any context-free grammar generating those functions. My question 3 may help answer whether those results apply more generally to algebraic functions over the rational numbers (not just over non-negative ones) in class $\mathcal{C}$ . Just like question 3, the other two questions may help answer whether certain things can be concluded about all algebraic functions in class $\mathcal{C}$ over rational numbers. Notes: A Google Scholar search didn't help me much in answering the first two questions. For example, Flajolet (1987) discussed how to determine if a power series is algebraic over rationals, but gave no example of a function that is algebraic over rationals but not analytic. Moreover, apparently not all algebraic functions over rationals might be expressible as power series, as Richman's results suggest. It is relatively easy to show that constants, the identity $x$ , and arbitrary additions and multiplications of these functions are Lipschitz continuous (1-Hölder continuous), and that those functions together with radicals are $\alpha$ -Hölder continuous.  However, it's not so easy to show whether those functions together with their reciprocals are $\alpha$ -Hölder continuous, or whether that remains true with arbitrary algebraic functions in the class $\mathcal{C}$ , including those that can't be expressed in terms of radicals.  Also, I believe that a function that maps (0, 1) to (0, 1) is algebraic over the rationals only if it's polynomially bounded. REFERENCES: Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Mossel, Elchanan, and Yuval Peres. New coins from old: computing with unknown bias. Combinatorica, 25(6), pp.707-724. Banderier, C. And Drmota, M., 2015. Formulae and Asymptotics for Coefficients of Algebraic Functions. Comb. Probab. Comput., 24(1), pp.1-53. Richman, Fred. ""Algebraic functions, calculus style."" Communications in Algebra 40, no. 7 (2012): 2671-2683. Flajolet, P., 1987. Analytic models and ambiguity of context-free languages. Theoretical Computer Science, 49(2-3), pp.283-309.","['analytic-functions', 'algebraic-geometry', 'functions', 'real-analysis']"
4144934,Why can we not artifically change the state space of this Markov chain?,"I am slightly confused about the solution to Exercise 11 in Chapter 4 of Introduction to Probability Models , by Sheldon M. Ross. Context: On any given day Gary is either cheerful (C), so-so (S), or glum
(G). If he is cheerful today, then he will be C, S, or G tomorrow with respective
probabilities 0.5, 0.4, 0.1. If he is feeling so-so today, then he will be C, S, or
G tomorrow with probabilities 0.3, 0.4, 0.3. If he is glum today, then he will be
C, S, or G tomorrow with probabilities 0.2, 0.3, 0.5.
Letting $X_n$ denote Gary’s mood on the nth day, then $\{X_n, n \geq 0$ } is a three-state
Markov chain (state 0 = C, state 1 = S, state 2 = G) with transition probability
matrix $$P = \begin{pmatrix} 0.5 & 0.4 & 0.1 \\ 0.3 & 0.4 & 0.3 \\ 0.2 & 0.3 & .5 \end{pmatrix}. $$ The question: In Example 4.3, Gary was in a glum mood four days ago. Given that he hasn’t felt
cheerful in a week, what is the probability he is feeling glum today? According to the solution manual, the answer is $\frac{P_{2,2}^4}{1-P_{2,0}^4}$ , with $$P = \begin{pmatrix} 1 & 0 & 0 \\ 0.3 & 0.4 & 0.3 \\ 0.2 & 0.3 & .5 \end{pmatrix}.$$ However, my initial approach had been to  form a new, reduced transition probability matrix $$P = \begin{pmatrix} \frac{4}{7} & \frac{3}{7} \\ \frac{3}{8} & \frac{5}{8} \end{pmatrix},$$ where I just completely neglect the possibility of going into state 0, and simply report $P_{1,1}^4$ as the answer. Why is this approach wrong? To be clear, I fully understand the logic behind the correct answer, and I know something is not right with the way I tried to solve the problem, but I would appreciate a clear explanation as to why it doesn't work.","['stochastic-processes', 'markov-chains', 'probability']"
4144947,Use Stokes Theorem to Prove Two Integrals on Differentiable Manifiolds are equivalent,"I have to answer the following problem: Let $\omega= ydx + xzdy + xdz$ . Let $S_1$ be the portion of the upper hemisphere given by $\phi_1(u,v)=(u,v,\sqrt{4-u^2-v^2})$ ; where $u^2 + v^2 \leq 2$ . Let $S_2$ be the disc in the plane $z=\sqrt{2}$ , given by $\phi_2(u,v)=(u,v,\sqrt{2})$ ; where $u^2 + v^2 \leq 2$ . Use Stokes theorem to show $\int\int_{s_1}d\omega=\int\int_{s_2}d\omega$ I know that I could technically calculate each integral to prove this, but since the directions are asking for the use of Stokes theorem, I feel like there's some trick that I'm missing. The two parameterizations look like they have equivalent boundaries, but I'm getting stuck on exactly how to show that. Any help would be greatly appreciated","['manifolds-with-boundary', 'multivariable-calculus', 'stokes-theorem', 'manifolds', 'differential-geometry']"
4144961,Constant Rank Theorem without using the Inverse Function Theorem,"Background Scanning Wikipedia's article on the Implicit Function Theorem I came across the following comment in the ""See Also"" section: Both the implicit function theorem and the inverse function theorem can be seen as special cases of the constant rank theorem. The article on the Inverse Function Theorem makes a similar claim (see here ): The inverse function theorem (and the implicit function theorem) can be seen as a special case of the constant rank theorem, (...) ${}^\text{[11]}$ . Question 1: Is this implying that the Constant Rank Theorem can be proved without using the Inverse Function Theorem? Main question If the answer to question 1 is ""yes"" then Question 2: How can you proof the Constant Rank Theorem without using either the Inverse- or the Implicit Function Theorem? For the sake of completeness, the Constant Rank Theorem roughly asserts that if the rank of a $C^r$ function $f:\mathbb{R}^m\to\mathbb{R}^n$ is constant $k$ on an open neighbourhood $U$ of $x_0$ , then there are $C^r$ diffeomorphisms $\alpha,\beta$ such that $$\alpha \circ f \circ\beta (x_1,...,x_m) = (x_1,...,x_k,0,...,0)$$ near $x_0$ . (See for instance John M. Lee, Introduction to Smooth Manifolds , p. 81.) Wikipedia's reference Wikipedia's reference for the second claim is the following: [11]: Boothby, William M. (1986). An Introduction to Differentiable Manifolds and Riemannian Geometry . pp. 46-50. As you can see from this screenshot , Boothby uses the Inverse Function Theorem in his proof of the Constant Rank Theorem.","['multivariable-calculus', 'differential-geometry', 'real-analysis']"
4144972,Relation Between Binomial Coefficients and Sine Function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I was looking at the graph for $\dbinom{n}{x}$ and I found using Wolfram Alpha that for $n\in\mathbb{Z}$ and $x\neq0,1,\dots,n$ , $$\dbinom{n}{x}=\dfrac{\left(-1\right)^nn!\sin(\pi x)}{\pi\prod_{m=0}^{n}\left(x-m\right)}.$$ I'm curious why the sine function would appear in something like a binomial coefficient. Edit: By $\dbinom{n}{x}$ , I mean $\dfrac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x+1)}$ .","['gamma-function', 'algebra-precalculus', 'binomial-coefficients']"
4145017,Asymptotic growth of summation of $\sum_{j\leq t}' 2^{\omega(j)}$ (restricting sum to a certain subset of $j$),"The following question came up in a thesis discussion I had with a student (undergraduate). I am a number theory researcher, but not within analytic number theory. One challenge I have faced is not having good intuition for what the answers should be (unless I see clearly a proof, which then creates a vicious circle!). Here is the question asked in its most basic form. Recall that $\omega(n)$ is used to denote the number of distinct prime divisors of $n$ . Let me call an integer $n$ good if for all primes $p \mid n$ , we have $p \equiv \pm 1 \pmod{8}$ . What is the asymptotic behavior of $\displaystyle D(t) = \sum_{\substack{n\leq t\\\text{$n$ good}}} 2^{\omega(n)}?$ What I seem to be able to see right now is that $D(t) = ct + o(t^{3/4})$ where $c = \frac{2\sqrt{2}\log(1+\sqrt{2})}{\pi^2}$ (see below). The issue is that the error term seems grossly overestimated. So, I am turning to you all to ask what better trained people can say about the sum $D(t)$ . Now I explain where this arises and what I know. The sum $D(t)$ arises from counting a certain set of ideals in the algebraic integer ring $\mathbf Z[\sqrt 2]$ . Namely, $D(t)$ counts the number of pairs of ideals $I,J \subseteq \mathbf Z[\sqrt 2]$ such that $N(I) = N(J)$ and $\gcd(I,J) = 1$ . Here $N(I)$ is the ideal norm (if $I$ is generated by $a+b\sqrt{2}$ then $N(I) = |a^2-2b^2|$ ). Second, I know just enough technique from analytic number theory to consider the Dirichlet series $$
F(s) = \sum_{\substack{n\geq 1\\ \text{$n$ is good}}} \frac{2^{\omega(n)}}{n^s}.
$$ This factors nicely into $$
F(s) = \zeta(s)G(s),
$$ where $$
G(s) = \prod_{p\equiv \pm 1 \pmod{8}} (1 + p^{-s}) \times \prod_{p\not\equiv \pm 1 \pmod{8}} (1-p^{-s}) = \frac{1-2^{-s}}{1-2^{-2s}}\frac{L(s,\chi_8)}{\zeta(2s)}.
$$ Here, $\chi_8$ is the Dirichet character modulo $8$ that sends $1,7 \mapsto 1$ and $3,5 \mapsto -1$ . I guess I should say $\zeta(s)$ is Riemann's zeta function. (Without the restriction to $n$ being good, the series $F(s)$ would have factored into $\zeta(s)^2/\zeta(2s)$ .) If I understand correctly, the Dirichlet series $G(s)$ converges on $\mathrm{Re}(s) \geq 1/2$ (the tricky part is that $1/\zeta(s)$ converges at $s = 1$ , which is related to the prime number theorem) and so $G(s) = \sum b_n/n^s$ has partial sums that satisfy $\sum_{n\leq t} b_n = o(\sqrt{t})$ . Then, using $F(s) = \zeta(s)G(s)$ , I used the hyperbola principle (my most advanced trick, to be honest) to see that $$
D(t) = G(1) t + o(t^{3/4}).
$$ (If I haven't made any mistake, which is a big iff...). The value of $G(1)$ can be calculated to be $\frac{2\sqrt{2}\log(1+\sqrt{2})}{\pi^2}$ using the famous formula for $\zeta(2)$ and the evaluation of $L(1,\chi_8)$ either by the class number formula or just doing it by hand. For what it's worth, I plotted $D(t) - ct$ for $t \leq 20000$ (first plot) and $t \leq 100,000$ (second plot) and, well, the difference looks small, way better than just $o(t^{3/4})$ but not as small as $O(1)$ ... First plot. Values of $D(t) - ct$ for $t \leq 20000$ Second plot. Values of $D(t) - ct$ for $t \leq 100000$ (apparently noobs cannot post images...)","['analytic-number-theory', 'number-theory', 'asymptotics', 'dirichlet-series']"
4145019,"Evaluating $\lim\limits_{n \to \infty}\frac{\int_0^{1/2} x^{nx}\,\mathrm dx}{\int_{1/2}^1 x^{nx}\,\mathrm dx}$","Let $f_n(x):=x^{nx}=(x^x)^n$ and $f_n(0)=1.$ It's easy to see, for any fixed $n$ , $f_n(x)$ is decreasing over $[0,1/e]$ and increasing over $[1/e,1]$ .
Therefore for $0\le x\le 1/2$ , it holds that $$e^{-n/e}=f_n(1/e)\le f_n(x)\le f_n(0)=1,$$ and
for $1/2 \le x\le 1$ , it holds that $$2^{-n/2}=f_n(1/2)\le f_n(x)\le f_n(1)=1.$$ Hence $$\frac{1}{2}e^{-n/e}\le \int_0^{\frac{1}{2}} f_n(x)\,\mathrm dx\le \frac{1}{2},\qquad\frac{1}{2}\cdot2^{-n/2}\le \int_{\frac{1}{2}}^1 f_n(x)\,\mathrm dx\le \frac{1}{2}.$$ But this seems to be helpless.","['limits', 'calculus', 'definite-integrals']"
4145049,Number of zeros outside the disk $\{ z : |z| \leq 2 \}$,"I need to count (including the multiplicities of the zeros) number of the zeros outside the disk $\{ z : |z| \leq  2 \}$ for the polynomial $f(z) = z^7 +9z^4 -7z +3$ . I know this should be direct application for Rouche's theorem, but I tried all choices for the two functions to get the required inequality $ |p(z)| < |q(z)|$ for $|z|=2 $ , but none of them works. Should I consider a different curve or what terms I should consider to get the required inequality? I think $z^7 +3$ should work but couldn't confirm that.","['complex-analysis', 'rouches-theorem', 'roots']"
4145178,Show that $f\left( \bigcap_{\alpha \in \mathcal{A}}A_\alpha\right) \subset \bigcap_{\alpha \in \mathcal{A}}fA_\alpha$,"Let $f: X \to Y$ where $X$ and $Y$ are non-empty and let $\{A_\alpha \mid \alpha \in \mathcal{A} \} \subset \mathcal{P}(X)$ . Show that $$f\left( \bigcap_{\alpha \in \mathcal{A}}A_\alpha\right) \subset \bigcap_{\alpha \in \mathcal{A}}fA_\alpha$$ I'm stuck on how to prove this. If I take some $y \in f\left( \bigcap_{\alpha \in \mathcal{A}}A_\alpha\right)$ , then there must exist $x \in \bigcap_{\alpha \in \mathcal{A}}A_\alpha$ , but this means that $x$ is in every $A_\alpha$ regardless of what $\alpha$ is right? I don't see how this is of help here?","['elementary-set-theory', 'functions']"
4145208,What is the relationship between these two definitions of generating functions?,"I'm doing my bachelor's thesis on Integrable Hamiltonian Systems, and one important part of the thesis will be proving the Liouville theorem. For this theorem I'm using the book by Arnold ""Mathematical Methods of Classical Mechanics"", but up to this point I've used the book by Ana Cannas ""Lectures on symplectic geometry"". The problem is: in the book by Ana Cannas, the equations that describe the symplectomorphism $\varphi: T^*X:=M=(x,p)\rightarrow T^*Y:=N=(y,q)$ that a generating function $f$ generates are the following (lecture 4). \begin{align*}
    p_i= &\frac{\partial f}{\partial x_i}(x,y) \\
    q_i=-&\frac{\partial f}{\partial y_i}(x,y)  
\end{align*} We have to solve this system of equations for the coordinates $y_i$ and $q_i$ . But according to Arnold's book (page 284, section 50 C), the equations that describe the symplectomorphism would be (using the aforementioned notation, not the one that Arnold uses) \begin{align*}
  x_i=&\frac{\partial f}{\partial p_i} \\
  q_i=&\frac{\partial f}{\partial y_i}
\end{align*} In the book by Ana Cannas, the generating function is defined as a function $f \in C^\infty(X\times Y)$ and that generates a closed form $df$ , whose image is a lagrangian submanifold of $T^*(X\times Y)$ then we make the 'twist' of the submanifold, which must be a graph of a symplectomorphism... . If anyone can help me to understand how this relates to the view that Arnold has on generating functions it would be of great help, since I need this to understand the construction of action-angle variables. Thanks in advance for the answers.","['integrable-systems', 'symplectic-geometry', 'differential-geometry', 'classical-mechanics', 'dynamical-systems']"
4145228,T-test and F-test in Multiple Linear Regression,"In simple linear regression, $$
y = \beta_0 + \beta_1X_1,
$$ the T-test for $\hat{\beta_1}$ is $$
H_0: \beta_1 = \beta_1^0 \quad \text{and} \quad H_A: \beta_1 \neq \beta_1^0,
$$ where $\beta_1^0 = 0$ , and the F-test is $$
H_0: \beta_1 = 0 \quad \text{and} \quad H_A: \beta_1 \neq 0.
$$ We know that the T-statistics is $$
T = \frac{\hat{\beta}_1}{se(\hat{\beta}_1)} \implies t_{n-2}
$$ and F-statistics is $$
F = \frac{SSreg}{\frac{RSS}{n-1}} \implies F_{1,n-2},
$$ where $RSS = \sum(y_i-\hat{y}_i)^2$ and $SSreg = \sum(\hat{y}_i - \bar{y})^2$ . We know that $$
T^2 = F
$$ Problem: I am wondering whether this property still holds in multiple linear regression where predictors are $x_1,x_2,...,x_p$ . Since now the case is $$
H_0: \beta_j = 0 \quad \text{and} \quad H_A: \beta_j \neq 0. 
$$ I don't see why it can work here since I find the degrees of freedom don't match in two tests. So I am wondering how can we show the property still holds in this case analytically?","['linear-regression', 'statistics', 'hypothesis-testing']"
4145243,Finding the vector field of ODE in cartesian coordinate,"The way I approached the problem is first by solving a system of equations: $$
r_x \frac{dx}{dt} + r_y\frac{dy}{dt} = \frac{dr}{dt} = -r
$$ $$
\theta_x \frac{dx}{dt} + \theta_y\frac{dy}{dt} = \frac{2}{\ln(x^2+y^2)}
$$ where $r = \sqrt{x^2+y^2}$ and $\theta = tan^{-1}\frac yx$ . The problem is after solving for $\frac {dx} {dt}$ and $\frac{dy}{dt}$ I got the following vector field: $$
\left(\frac{dx}{dt}, \frac{dy}{dt}\right) = \left(-x-\frac{2y}{ln(x^2+y^2)},-y+\frac{2x}{ln(x^2+y^2)}\right)
$$ And this mess doesn't seem to be differentiable at $(0,0)$ . Anyone got some alternative approach or can point out where I made a mistake? Thanks! EDIT: I found somewhere I made a mistake in getting to the vector field so here I edited the problem to show the new (hopefully correct) vector field obtained. However it is still not differentiable at the origin.",['ordinary-differential-equations']
4145244,Type of critical points in three dimensions,"I am facing an exercise about maxima and minima for the function $$f(x, y, z) = xye^x - xyz$$ So the gradient is $$\nabla f(x, y, z) = (ye^x + xye^x - yz, xe^x - xz, -xy)$$ The solutions I found are the points $$P = (0, 0, z)$$ $$Q = (0, y, 1)$$ $$K = (x, 0, e^x)$$ Now the Hessian matrix reads $$H = 
\begin{pmatrix}
2ye^x + xye^x & e^x + xe^x - z & -y \\
e^x + xe^x - z & 0 & -x \\
-y & -x & 0
\end{pmatrix}
$$ The problem is that when I evaluate the Hessian in the points I have found, in all the three cases I find one zero eigenvalue, which means I cannot say anything about the point. How is this possible? Is there a way to say anything about those points?","['maxima-minima', 'multivariable-calculus', 'hessian-matrix', 'eigenvalues-eigenvectors']"
4145277,Feynman's trick to evaluate the integral $\int\limits_{0}^{2\pi}\sin^{8}(x)dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I would like to evaluate the following integral using differentiation under the integral sign. $$\int\limits_{0}^{2\pi}\sin^{8}(x)dx$$ Unfortunately, I can't come up with a proper choice for a function with a parameter. $\sin^{8}(ax)$ won't work out, neither will $\sin^{a}(x)$ ... So maybe someone could give me a hint to make an appropriate choice of that function. Thanks!","['integration', 'calculus', 'definite-integrals', 'leibniz-integral-rule']"
4145280,"To evaluate the surface integral $\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy)$","How to evaluate the surface integral $$\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy)$$ where $S$ is the outer surface of the ellipsoid $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ that lie above the $xy-$ plane. My thought was to substitute $x=a \sin\theta \cos\psi, y=b \sin\theta \sin\psi, x=c \cos\theta$ , where $0\leq \theta\leq \frac{\pi}{2}$ and $0\leq \psi\leq 2\pi$ . But in this way i got the answer as $-abc\pi$ , which is suppose to be $2abc\pi$ . Any help is highly appreciated. Thank you in advance. My steps $$\frac{\delta(x,y)}{\delta(\theta,\psi)}=ab\sin\theta \cos\psi$$ $$\frac{\delta(y,z)}{\delta(\theta,\psi)}=bc\sin^2\theta \cos \psi$$ $$\frac{\delta(z,x)}{\delta(\theta,\psi)}=ac\sin^2\theta \sin \psi$$ The given integral $$=\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(x\frac{\delta(y,z)}{\delta(\theta,\psi)}+y\frac{\delta(z,x)}{\delta(\theta,\psi)}+z\frac{\delta(x,y)}{\delta(\theta,\psi)})\,d\theta\, d\psi$$ $$=\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(\sin \theta)\,d\theta\, d\psi$$ $$=-abc\pi$$","['surface-integrals', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
4145310,Finite groups whose Sylow 2-subroups are Klein groups,"I am reading part VIII of Burnside's ""Notes on the Theory of Groups of Finite Order"". It was published in 1895 (Proc. London Math. Soc., vol. 26, p. 325-338, 1895, Collected Papers, vol. I, p. 590-596). Burnside first proves something that I understand (in today's language) as follows : $\mathbf{Statement \ 1}.$ Let $G$ be a finite group whose Sylow 2-subgroups are Klein groups. Then one (and only one) of the two following cases holds : (i) for every Sylow 2-subgroup $V$ of $G$ , the three elements of order 2 of $V$ form a conjugacy class in $N_{G}(V)$ , $\vert N_{G}(V) \vert = 3 \vert C_{G}(V) \vert$ (and thus $\vert G \vert$ is divisible by 12) and all elements of order 2 in $G$ are conjugate in $G$ ; (ii) for every Sylow 2-subgroup $V$ of $G$ , the three elements of order 2 of $V$ are pairwise non-conjugate in $G$ and $N_{G}(V) = C_{G}(V)$ . I have no problem with that. Thereafter, Burnside's goal seems to be to prove that if $G$ is simple, it is the case (i) of Statement 1 that takes place. Today, it is an easy consequence of Burnside's transfer theorem : in case (ii), we have $N_{G}(V) = C_{G}(V)$ for every Sylow 2-subgroup $V$ of $G$ , thus, by Burnside's transfer theorem, $G$ cannot be simple. But Burnside doesn't use his transfer theorem here. (I presume that he had not yet stated this theorem in 1895. By the way, when and where did he publish it for the first time ? I don't find the answer on Internet.) I must confess that I don't understand this part of Burnside's paper. He considers a group, let us say $G$ , of order $N = 4m$ , with $m$ odd, he assumes that the Sylow 2-subgroups of $G$ are Klein groups and, if I understand it correctly, he notes $4m_{1}$ the order of the normalizer in $G$ of a Sylow 2-subgroup of $G$ . Then he says (p. 590-591 in the Collected Papers, vol. I) : ""Suppose now that this is not the case [I understand : Suppose that we are not in case (i) of Statement 1, in other words suppose that we are in case (ii) of Statement 1.], so that the group contains 3 different conjugate sets of operations of order 2 [OK]. Every operation [Burnside says ""operation"" where we say ""element""] of order 2 is certainly self-conjugate [For an element of a group, Burnside says ""self-conjugate"" where we say ""central""; for a subgroup, he says ""self-conjugate"" where we say ""normal"".] in a sub-group of order $2^{2}m_{1}$ [Indeed, every element $v$ of order 2 of $G$ is contained in a Sylow 2-subgroup $V$ of $G$ and, since we are in case (ii) of Statement 1, $N_{G}(V)$ centralizes $V$ and thus centralizes $v$ . Recall that $2^{2}m_{1}$ is the order of $N_{G}(V)$ for every Sylow 2-subgroup $V$ of $G$ .], and may be self-conjugate in a more extensive sub-group. [In other words : $N_{G}(V)$ is contained in $C_{G}(v)$ and it is possible that $C_{G}(v)$ is greater than $N_{G}(V)$ .] Let, then, $S$ , an operation of order 2 be self-conjugate within a group of order $2^{2}m_{1} \mu $ . [I understand : let $S$ be an element of order 2 of $G$ and let $2^{2}m_{1} \mu $ be the order of $C_{G}(S)$ .] An operation of this sub-group whose order is odd, and which is not contained in the sub-group of order $2^{2}m_{1}$ , is permutable with $S$ , and with no operation of order 2 which is not conjugate to $S$ ."" What is exactly the meaning of this last sentence ? Burnside says ""THE sub-group of order $2^{2}m_{1}$ "". But $S$ can belong to several Sylow 2-subgroups of $G$ . If $S$ is in a Sylow 2-subgroup $V$ of $G$ and also in another Sylow 2-subgroup $W$ of $G$ , what is ""THE sub-group of order $2^{2}m_{1}$ "" ? $N_{G}(V)$ or $N_{G}(W)$ ? In any case, I think that the following statement is wrong : $\mathbf{Statement \ 2}.$ (not from Burnside, and wrong). Let $G$ be a finite group whose Sylow 2-subgroups are Klein groups. Assume that case (ii) of Statement 1 holds, so that, for every Sylow 2-subgroup $V$ of $G$ , the three elements of order 2 of $V$ are pairwise non-conjugate in $G$ and $N_{G}(V) = C_{G}(V)$ . Let $v$ be an element of order 2 of $G$ , let $V$ be a Sylow 2-subgroup of $G$ containing $v$ , let $t$ be an element of odd order in $C_{G}(v)$ and assume that $t$ is not in $N_{G}(V) = C_{G}(V)$ . Then all elements of order 2 in $C_{G}(t)$ are conjugate in $G$ . Here is a counterexample. Let $H$ be a nonabelian group of order 21. Then $H$ is a semidirect product $\langle x_{7} \rangle \rtimes  \langle y_{3} \rangle $ , where $x_{7}$ is an element of order 7, $y_{3}$ an element of order 3 and $y_{3} x_{7} {y_{3}}^{-1} = {x_{7}}^{2}$ . By von Dyck's theorem, or whatever, there exists one and only one automorphism $\sigma$ of H such that $\sigma (x_{7}) = {x_{7}}^{-1}$ and $\sigma (y_{3}) = y_{3}$ . Clearly, $\sigma$ is of order 2 in $Aut(H)$ . Choose a Klein group $V = \{1, a, b, c\}$ . Let $\tau : v \mapsto \tau_{v}$ be the homomorphism from $V$ to $Aut(H)$ such that $\tau_{1} = \tau_{a} = id_{H}, \tau_{b} = \tau_{c} =\sigma$ . Put $G = H \rtimes _{\tau}V$ . Clearly the Sylow 2-subgroups of $G = H \rtimes _{\tau}V$ are Klein groups. It is easy to check that (1) $\qquad (1, a)$ is a central element of order 2 in $G = H \rtimes _{\tau}V$ ; (2) $\qquad (1, b)$ is an element of order 2 in $G = H \rtimes _{\tau}V$ and it commutes with $(y_{3}, 1)$ , which is an element of order 3 in $G = H \rtimes _{\tau}V$ ; (3) $\qquad (x_{7}, b)$ is an element of order 2 in $G = H \rtimes _{\tau}V$ and it doesn't commute with $(y_{3}, 1)$ . From (1), it results that $(1, a)$ is conjugate to no element of $G = H \rtimes _{\tau}V$ other than $(1, a)$ itself. Since (for example by (2) ), $G = H \rtimes _{\tau}V$ has at least one other element of order 2 than $(1, a)$ , it is not true that all the elements of order 2 in $G = H \rtimes _{\tau}V$ are conjugate in $G = H \rtimes _{\tau}V$ . Thus we are not in case (i) of Statement 1, thus (4) $\qquad$ it is case (ii) of Statement 1 which holds, Put $W = \langle (1, a), (x_{7}, b)\rangle$ . In view of (1) and (3), $W$ is a Sylow 2-subgroup $W$ of $G = H \rtimes _{\tau}V$ , $(y_{3}, 1)$ commutes with $(1, a)$ , $(y_{3}, 1)$ is not in $C_{G}(W)$ . Thus, if Statement 2 was true, all the elements of order 2 of $G = H \rtimes _{\tau}V$ commuting with $(y_{3}, 1)$ should be conjugate to eachother in $G$ . It is false, since, by (1) and (2), both $(1, a)$ and $(1, b)$ are elements of order 2 commuting with $(y_{3}, 1)$ , but they are not conjugate in $G$ (since $(1, a)$ is central in $G$ ). Thus Statement 2 is wrong. So my question is : do you think that Burnside's proof is correct and, if it is the case, could you make it clearer ? I will now copy the whole proof of Burnside. Maybe it can help you understand his ideas. ""Suppose now that this is not the case, so that the group contains 3 different conjugate sets of operations of order 2. Every operation of order 2 is certainly self-conjugate in a sub-group of order $2^{2}m_{1}$ , and may be self-conjugate in a more extensive sub-group. Let, then, $S$ , an operation of order 2 be self-conjugate within a group of order $2^{2}m_{1} \mu $ . An operation of this sub-group whose order is odd, and which is not contained in the sub-group of order $2^{2}m_{1}$ , is permutable with $S$ , and with no operation of order 2 which is not conjugate to $S$ . Hence it must form one of a set of $2r$ conjugate operations , where $r$ is odd. If now the group be represented as a permutation group, consisting of the permutations of these $2r$ operations among themselves which arise by transforming them by all the operations of the group, any operation of order 2 which is not conjugate to $S$ will give a substitution in the permutation-group, consisting in $r$ transpositions, i.e. an odd substitution, and, therefore, the group cannot be simple. Hence, the group is certainly not simple unless the maximum sub-group, which contains an operation of order 2 self-conjugately, is of order $2^{2}m_{1}$ ; and when this condition is satisfied every operation of order 2 is permutable with just 2 other operations of order 2, and with no more. But, now, if $A$ , $B$ are two operations of order 2 belonging to different conjugate sets, and if $(AB)^{n} = 1$ , $A$ , $B$ generate a dihedral group of order $2n$ . If $n$ were odd, $A$ and $B$ would be conjugate, which is not the case. Hence, $n$ must be even, and then $(AB)^{\frac{1}{2}n}$ is an operation of order 2 which is permutable with $n$ distinct pairs [?] of operations of order 2. But this is in direct contradiction to what has just been proved, so that this case cannot occur. It follows that, if a group whose order is $2^{2}m$ ( $m$ odd) contains 3 different conjugate sets of operations of order 2, it cannot be simple."" $\mathbf{Edit}$ (May 22, 2021). I still feel that Burnside's proof is not absolutely correct, but I think it can be fixed (without use of transfer theory), by use of Burnside's ideas. Let $G$ be a finite simple group whose Sylow 2-subgroups are Klein groups and let us prove that all the elements of order 2 in $G$ are conjugate. $\mathbf{Lemma \ 1.}$ Let $G$ a finite simple group, let $t$ be an element of $G$ such that the number of conjugates of $t$ in $G$ is of the form $2r$ , with $r$ odd. Then, for every element $s$ of order 2 in $G$ , there exists at least one G-conjugate of $t$ which commutes with $s$ . Proof. Let $Conj(t)$ denote the set of conjugates of $t$ in $G$ . From the hypotheses, $Conj(t)$ has more than an element, thus the simple group $G$ is nonabelian and acts nontrivially on $Conj(t)$ by conjugation, thus the permutation $u \mapsto sus^{-1}$ is even. If $s$ commuted with no conjugate of $t$ , this permutation should be the product of $r$ transpositions (with disjoint supports) and should thus be odd, contradiction. $\mathbf{Lemma \ 2.}$ Let $G$ be a group, let $a, b$ two elements of order 2 in $G$ , not conjugate with each other in $G$ . Assume that $ab$ has finite order $n$ . Then $\langle a, b \rangle$ is a dihedral group of order $2n$ and $n$ is even. Proof. The fact that $\langle a, b \rangle$ is a dihedral group of order $2n$ is classical (Rotman 1999, theor. 3.32, p. 68). Assume (absurdly) that $n$ is odd. Then the order of $\langle a, b \rangle$ is even and not divisible by 4, thus the Sylow 2-subgroups of $\langle a, b \rangle$ are its subgroups of order 2, thus (Sylow) all the subgroups of order 2 of $\langle a, b \rangle$ are conjugate in $\langle a, b \rangle$ , thus all the elements of order 2 of $\langle a, b \rangle$ are conjugate in $\langle a, b \rangle$ , thus $a$ and $b$ are conjugate in $\langle a, b \rangle$ and thus in $G$ , contradiction. Here is another proof that $n$ is even. If $n$ was odd, say $n = 2k + 1$ , we would have $(ab)^{2k+1} = 1$ , $(ab)^{k} a (ab)^{-k} = b$ thus $a$ and $b$ would be conjugate, contradiction. $\mathbf{Theorem.}$ Let $G$ a finite simple group whose Sylow 2-subgroups are Klein groups. Then all the elements of order 2 of $G$ are conjugate. Proof. Assume first that (hyp. 1) $\qquad $ there exists an element $t$ of $G$ such that the number of conjugates of $t$ is of the form $2r$ , with $r$ odd. Then, by Lemma 1, every element of order 2 of $G$ commutes with (at least) one conjugate of $t$ . In other words, (2) $\qquad$ every element of order 2 of $G$ is conjugate in $G$ with an element (of order 2) of $C_{G}(t)$ . On the other hand, since the order of $G$ is divisible by 4 and not by 8, it results from (1) that $\vert C_{G}(t) \vert$ is divisible by 2 and not by 4. Thus the Sylow 2-subgroups of $C_{G}(t)$ are its subgroups of order 2, thus (Sylow) the subgroups of order 2 of $C_{G}(t)$ are conjugate in $C_{G}(t)$ , which amounts to say that the elements of order 2 of $C_{G}(t)$ are conjugate in $C_{G}(t)$ . With (2), this proves that all the elements of order 2 of $G$ are conjugate in $G$ . Thus the theorem is true in our hypothesis (1), thus we can assume that this hypothesis is not satisfied, i.e. (3) $\qquad$ for every element $t$ of $G$ , the number of conjugates of $t$ in $G$ is odd or divisible by 4. Assume that (absurdly), (hyp.4) $\qquad$ there exist two elements $a$ and $b$ of order 2 of $G$ which are not conjugate in $G$ . Let $n$ denote the order of $ab$ . By Lemma 2, $n$ is even and $\langle a, b \rangle$ is a dihedral group of order $2n$ (and thus of order divisible by 4). Assume that (absurdly) (hyp. 5) $\qquad n$ is not equal to 2. Then, since $n$ is even and $G$ has no element of order 4, $n$ is not a power of 2, thus the dihedral group $\langle a, b \rangle$ has at least one element $t$ of odd prime order $p$ , In a dihedral group, every element of odd order is conjugate to its inverse, thus $t$ and $t^{-1}$ are conjugate in $\langle a, b \rangle$ . A fortiori, (6) $\qquad t$ and $t^{-1}$ are conjugate in $G$ . Let $t'$ be a conjugate of $t$ in $G$ . Then $t'$ is of order $p$ . Since $p$ is odd and $\neq 1$ , $t'$ and $t'^{-1}$ are distinct. Moreover, in view of (6), $t'$ and $t'^{-1}$ are conjugate in $G$ . Thus the conjugates of $t$ in $G$ are in even number, thus, in view of (3), the number of conjugates of $t$ in $G$ is divisible by 4. Since the order of $G$ is divisible by 4 and not by 8, this implies that (7) $\qquad \vert C_{G}(t) \vert$ is odd. But $t$ , being of order different from 2, is in the unique cyclic subgroup $C$ of index 2 of the dihedral group $\langle a, b \rangle$ . Since $C$ is abelian, $C$ centralizes $t$ , thus $C_{G}(t)$ contains $C$ . But the order of $C$ is $n$ and is thus even, thus $C_{G}(t) \vert$ is even, which contradicts (7). Thus our hypothesis (5) is false, thus (always in our hypothesis 4), $n = 2$ , in other words the order of $ab$ is 2, which implies that $a$ and $b$ commute. Thus we have proved that (8) $\qquad$ for all elements $a, b$ of order 2 of $G$ which are not conjugate in $G$ , $a$ and $b$ commute. If $a$ and $b$ are two such elements, then, every conjugate $b'$ of $b$ in $G$ is an element of order 2 which is not conjugate to $a$ , thus the hypotheses on $a$ and $b$ in (8) are satisfied by $a$ and $b'$ , thus every conjugate of $b$ commutes with $a$ , thus (9) $\qquad$ the subgroup of $G$ generated by the conjugates of $b$ is contained in $C_{G}(a)$ . But the subgroup of $G$ generated by the conjugates of $b$ is a nontrivial normal subgroup of $G$ , thus, since $G$ is assumed to be simple, the subgroup of $G$ generated by the conjugates of $b$ is the whole $G$ , thus (9) means that $C_{G}(a) = G$ , thus $a$ is central in $G$ , which is impossible, since $G$ is a nonabelian simple group. Thus our hypothesis (4) is absurd, which proves the theorem. We can conclude from this theorem that (if $G$ is a simple group whose Sylow 2-subgroups are Klein groups), then, for every Sylow 2-subgroup $V$ of $G$ , the three elements of order 2 of $V$ are conjugate in $N_{G}(V)$ . (Rotman 1999, Lemma 7.49, p. 196.) So, we have proved, without transfer theory, that if $G$ is a simple group whose Sylow 2-subgroups are Klein groups, then case (i) of the Statement 1 above holds. Do you think my formulation of the proof is better than Burnside's, or do I see difficulties wherd there is none?","['group-theory', 'sylow-theory', 'finite-groups']"
4145337,Need help understanding spline and basis function,"I am reading chapter 5 of The Elements of Statistical Learning by Hastie et al.  I would like to ask: Are the two knots positioned at the two red circles in the top left panel? What is the purpose of $h_1(X) = 1$ ? What is the purpose of $h_2(X) = X$ ? Why do we only have $K$ basis functions?  For instance with 2 knots, dont we need 3 equations (one for each interval)? Can I have an explanation for equation $(5.5)$ ? Thank you so much!","['matrices', 'statistics', 'spline', 'linear-algebra']"
4145345,Prove: $\cos^3 \frac x3 + \cos^3 \frac{x+2\pi}{3} + \cos^3 \frac{x+4\pi}{3} = \frac{3}{4} \cos x$,"I tried to solve the following identity: $$\cos^3 \frac x3 + \cos^3 \frac{x+2\pi}{3} + \cos^3 \frac{x+4\pi}{3} = \frac{3}{4} \cos x$$ I applied formulas, $\cos (a+b)$ and $\cos^3 x$ and I arrived at $$\cos 3x + 2 \cos x - 3 \cos \frac{x}{3}=0 $$ After this I am stuck and any formula I applied, got complicated. Can someone give me an idea as to how I could prove this identity?",['trigonometry']
4145386,General formula for addition of a sin x + b cos x,"I have a question about the addition formula for $\sin x$ and $\cos x$ . Generally, I
have seen the following formula referenced $$a\sin x + b \cos x = \sqrt{a^2+b^2} \cos ( x +  \arctan{-\frac{a}{b}})$$ however, this formula seems to only hold for $b>0$ . I have attached an
image for $b>0$ and for $b<0$ . First I thought it had something to do with the
principle value of $\arctan$ , however, after some experimentation I
found that the
following to hold for all $a$ and $b$ $$a\sin x + b \cos x =\frac{b}{|b|} \sqrt{a^2+b^2} \cos ( x +
\arctan{-\frac{a}{b}})
$$ except for $b=0$ . Does anybody know this seems to occur and moreover what is the principled way of dealing with simplifications for $a\sin x + b \cos x$ (when given values and when not)? Thanks in advance",['trigonometry']
4145387,Confusion of proof on nowhere dense sets,"I am reading the book `Measure and Category' by Oxtoby. In the first chapter, they focus solely on the real line. I am confused by the following proof: I understand that, since $ A_1, A_2 $ are nowhere dense, given an interval $ I $ , both sets are not dense on $ I $ .
Therefore, there are subintervals $ I_1 ,I_2 $ that lie on the complement of $ A_1, A_2$ . The proof then says `Hence $ I_2 \subset I -(A_1 \cup A_2) $ . I am struggling to see how that follows from the previous sentence. Surely, $  I -(A_1 \cup A_2) $ need not contain $  I -A_1 $ ?","['elementary-set-theory', 'proof-explanation', 'real-analysis']"
4145392,Evaluation of $\sum_{n=1}^{\infty}\frac{1}{n(2n+1)}=2-2\ln(2)$,"I came across the following statements $$\sum_{n=1}^{\infty} \frac{1}{n(2 n+1)}=2-2\ln 2 \qquad \tag{1}$$ $$\sum_{n=1}^{\infty} \frac{1}{n(3 n+1)}=3-\frac{3 \ln 3}{2}-\frac{\pi}{2 \sqrt{3}} \qquad \tag{2}$$ $$\sum_{n=1}^{\infty} \frac{1}{n(4 n+1)}=4-\frac{\pi}{2}-3 \ln 2 \qquad \tag{3}$$ $$\sum_{n=1}^{\infty} \frac{1}{n(6 n+1)}=6-\frac{\sqrt{3} \pi}{2}-\frac{3 \ln 3}{2}-2 \ln 2 \qquad \tag{4}$$ The (1) by partial fractions $$\sum_{n=1}^{\infty}\frac{1}{n(2n+1)}=\sum_{n=1}^{\infty}\frac{1}{n}-\frac{2}{2n+1}$$ $$=\sum_{n=1}^{\infty}\frac{1}{n}-\frac{1}{n+\frac{1}{2}}$$ Recall the Digamma function $$\psi(x+1)=\gamma+\sum_{n=1}^{\infty}\frac{1}{n}-\frac{1}{n+x}$$ Therefore $$\sum_{n=1}^{\infty}\frac{1}{n}-\frac{1}{n+\frac{1}{2}}=\psi(1+\frac{1}{2})-\gamma$$ $$\sum_{n=1}^{\infty}\frac{1}{n(2n+1)}=\psi\left(\frac{3}{2}\right)-\gamma$$ In the same token we can derive the relation for the other three ralations. My Question is: can we calculate the values of the digamma function for those values without resorting in the Gauss´s Digamma formula? $$\psi\left(\frac{r}{m}\right)=-\gamma-\ln (2 m)-\frac{\pi}{2} \cot \left(\frac{r \pi}{m}\right)+2 \sum_{n=1}^{\left\lfloor\frac{m-1}{2}\right\rfloor} \cos \left(\frac{2 \pi n r}{m}\right) \ln \sin \left(\frac{\pi n}{m}\right)$$ I tried this approach also, but I think the resulting integral is divergent $$\sum_{n=1}^{\infty} \frac{1}{n(2 n+1)}=\sum_{n=1}^{\infty} \frac{1}{n}\int_{0}^{1}x^{2n}dx=\int_{0}^{1}\sum_{n=1}^{\infty} \frac{x^{2n}}{n}=-\int_{0}^{1}\ln(1-x^2)dx $$","['integration', 'polygamma', 'digamma-function', 'sequences-and-series']"
4145394,Problem in understanding Graph Enumeration ? (From the book on Graphical Enumeration by Palmer and Harary),"I was reading the first answer to this question, where the answer refers to a book by F. Harary and E.M. Palmer. In the book on page 8, they provide the following result: Theorem: The exponential generating functions $G(x)$ and $C(x)$ for labeled
graphs and labeled connected graphs come to terms in the following relation $$1+G(x) = e^{C(x)}$$ What is intriguing to me is the following statement (the last paragraph of page 8) : Furthermore, it is evident that if the exponential generating function for a
class of graphs is known, then the exponential generating function for the
corresponding connected graphs will be the formal logarithm of the first
series, just as in (1 .2.6) for all graphs 1.2.6 is the equation above. So if I have a generating function, $G'(x)$ , for any arbitrary property, $P(a)$ , on labelled graphs, then I can find generating function for a labelled graph which obeys $P(a)$ and is connected as follows: $$1+G'(x) = e^{C'(x)}$$ Where $C'(x)$ is the generating function for the connected graph such that it obeys $P(a)$ . Is my understanding of the statement correct ?
So for example if $G'(x)$ is the generating function for 2-regular graphs then the generating function for $C'(x)$ i.e connected 2-regular graphs is given as follows : $$C'(x) = \log(1+G'(x))$$","['graph-theory', 'combinatorics']"
4145406,Induced Second Fundamental Form for a Graph,"If one has a Riemannian $3$ -manifold $(M,g)$ and a warped product space $(M \times\mathbb{R}, g + \phi^2 dt^2)$ , the induced metric of a hypersurface $\Sigma$ given by a graph $t = f(x)$ is $g + \phi^2 df^2$ , but in that case how does one define the induced second fundamental form of $\Sigma$ ?","['riemannian-geometry', 'differential-geometry']"
4145421,How do I approach functions that look like they're neither odd nor even but they actually are?,"I came across the following function and was asked to determine if its odd or even or neither: $f(x)=x[x^2]+\frac{1}{\sqrt{1-x^2}}$ , where [.] is the greatest integer function. I started with the general approach of finding $f(-x)$ which came out to be: $f(-x)=-x[x^2]+\frac{1}{\sqrt{1-x^2}}$ Looking at it that way, it simply looks like its neither odd nor even. But when I checked my book, it was given to be even. And plotting its graph on a graphing tool again revealed its symmetry about y-axis showing that its even.  How can I tackle these kind of problems? Its clear that its designed to lure someone into the trap of thinking its neither odd nor even. So I am thinking we should rearrange $f(x)$ and then find $f(-x)$ but I dont really know how.","['even-and-odd-functions', 'functions']"
4145487,OEIS entry - A316312 has a question: Is it true that if k is a term then 100 * k is a term? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Refer https://oeis.org/A316312 - the sequence in OEIS. The sequence says Numbers n such that sum of the digits of the numbers 1, 2, 3, ... up
to (n - 1) is divisible by n. A few terms from the sequence are as follows:
1, 3, 5, 7, 9, 12, 15, 20, 27, 40, 45, 60, 63, 80, 81, 100, 180, 181, 300, 360, 363, 500, 540, 545, 700, 720, 727, 900, 909, 912, 915, 1137, 1140, 1200, 1500, 1560, 1563, 2000, 2700, 2720, 2727, 4000, 4500, 4540, 4545, 6000, 6300, 6360, 6363, 8000, 8100, 8180, ... It is evident from the sequence that if k is a term then 100*k is also a term. We (Author of the sequence and another Math nerd from GanitCharcha) haven't been able to prove or disprove it and we also do not know whether this has been proved or disproved. If anyone can help us in this regard by providing pointers or any solution.","['number-theory', 'computational-mathematics', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4145524,"Differential ""Freshman's dream"" for Laplacian operator.","Today I encountered quite an interesting phenomenon. There is an exercise in multivariable calculus that asks students to prove the identity $$
\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = e^{-2\xi} \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right),
$$ where the coordinates transformation is given by $(x,y) = F(\xi,\theta) = (e^\xi \cos(\theta), e^\xi \sin(\theta))$ , assuming $f \in C^2$ . I have seen a person misunderstood the question and proved $$
\left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 = e^{-2\xi} \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right)
$$ instead. To my surprise, his proof contains no mistake and the misinterpreted equation is actually true! This got me into thinking about the generalization of the above ""Freshman's dream"" for Laplacian operator: Which coordinates transformation $(x,y) = F(\xi,\theta)$ (or, equivalently, $(\xi,\theta) = G(x,y)$ )has the property that for any $f\in C^2$ , we have $$\begin{align}
\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} &= h(\xi,\theta) \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right) \\
&\text{if and only if} \\
\left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 &= h(\xi,\theta) \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right)
\end{align}$$ on an open domain $D\subset\Bbb R^2$ for some (sufficiently smooth) function $h>0$ ? Here's my thought so far: Suppose that our coordinates transformation is given by $(\xi,\theta) = G(x,y) = (G_1(x,y),G_2(x,y))$ . By some calculation (that I shall skip), we can compute that the Laplacian $\Delta = \partial_x^2 + \partial_y^2$ in the coordinate $(\xi,\theta)$ can be written as $$
\partial_x^2 + \partial_y^2 = (\Delta G_1)\partial_\xi + (\Delta G_2)\partial_\theta + |\nabla G_1|^2 \partial_\xi^2 + |\nabla G_2|^2 \partial_\theta^2 + 2(\nabla G_1 \cdot \nabla G_2)\partial_\xi \partial_\theta,
$$ hence $\partial_x^2 + \partial_y^2 = h(\partial_\xi^2 + \partial_\theta^2)$ if and only if $$
|\nabla G_1|^2 = |\nabla G_2|^2 = h, \quad \Delta G_1 = \Delta G_2 = 0, \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0.
$$ On the other hand, we have $$
\begin{pmatrix}\partial_x f &\partial_y f \end{pmatrix} = \begin{pmatrix}\partial_\xi f &\partial_\theta f \end{pmatrix} \begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\
\partial_x G_2 &\partial_y G_2 \end{pmatrix},
$$ hence in order that $(\partial_x f)^2 + (\partial_y f)^2 = h ((\partial_\xi f)^2 + (\partial_\theta f)^2 )$ , we need the Jacobian matrix to be of the form $$
\begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\
\partial_x G_2 &\partial_y G_2 \end{pmatrix} = \sqrt{h}\ M,
$$ where $M$ is an orthogonal matrix at each point on $D$ . In particular, this is equivalent to $$
|\nabla G_1|^2 = |\nabla G_2|^2 = h \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0.
$$ It seems like the ""miracle"" we have seen earlier is a little bit less surprising than expected! From my calculation above (unless I made some mistakes), it seems like $$\begin{align}
\partial_x^2 f + \partial_y^2 f &= h(\partial_\xi^2 f + \partial_\theta^2 f) \\
&\text{if and only if} \\ 
(\partial_x f)^2 + (\partial_y f)^2 &= h ((\partial_\xi f)^2 + (\partial_\theta f)^2 ) \ \ \text{and} \ \ \Delta G_1 = \Delta G_2 = 0, 
\end{align}$$ i.e. $G$ is a harmonic function coordinate-wise. In particular, this is true for our original coordinate transformation function since we can rewrite $(x,y) = (e^\xi \cos(\theta), e^\xi \sin(\theta))$ as $(\xi,\theta) = G(x,y)$ , where $$
G(x,y) = (G_1(x,y) , G_2(x,y)) = \left( \frac12 \ln(x^2+y^2), \arctan\left( \frac{y}{x} \right) \right).
$$ Here $G_1$ and $G_2$ are indeed harmonic (on an appropriate domain). I want to know if we can find more interesting examples like this? I have a feeling that this should be related to holomorphic functions and harmonic conjugate but my knowledge of complex analysis is pretty limited. Is there a general theory that would allow us to construct a coordinate transformation function $G$ satisfying the above properties?","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'partial-differential-equations', 'differential-geometry']"
4145556,Does this Optimization paper contain a mistake?,"I am reading the popular paper Stochastic ADMM by Hua Ouyang et. al. I have redone one of their calculations many times, and I keep coming up with a different answer. It is much more likely that I am incorrect rather than the paper, but could somebody please take a look? Thank you very much. The problem is \begin{align*} &\text{minimize } \ \  \theta_1(x) + \theta_2(y)\\
& \text{subject to } \ \  Ax + By = b, x \in \mathcal{X}
\end{align*} The augmented Lagrangian, as given on page 3 (bottom left) is $$L_{\beta}(x, y, \lambda) = \theta_1(x) + \theta_2(y) + \lambda^T(Ax + By - b) + \frac \beta 2 ||Ax + By - b||^2.$$ Shortly after this, it claimed that the x-subproblem $$\arg \min_{x \in \mathcal{X}} L_{\beta}(x, y^k, \lambda^k)$$ (where $y^k, \lambda^k$ are the $k^{th}$ iterates) can be written as $$\arg \min_{x \in \mathcal{X}} \theta_1(x) + \frac \beta 2||Ax + By^k - b - \frac 1 \beta \lambda^k ||^2.$$ In my calculations, I keep getting $+ \frac 1 \beta \lambda^k$ instead of $- \frac 1 \beta \lambda^k$ . My Calculation Here, I took away the superscript $k$ and the subscript $x \in \mathcal{X}$ for convenience. I also add and neglect constant terms without explicitly saying so. \begin{align*}
\arg \min_x L_{\beta}(x, y, \lambda) &= \arg \min _x \theta_1(x) + \theta_2(y) + \lambda^T (Ax + By - b) + \frac \beta 2||Ax+By-b||^2\\
&= \arg \min_x \theta_1(x) + \lambda^TAx + \frac \beta 2(x^TA^TAx + 2y^TB^TAx - 2b^TAx )\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2(By - b + \frac 1 \beta \lambda)^TAx)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx + v^Tv)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + v||^2\\
&= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + By - b + \frac 1 \beta \lambda ||^2
\end{align*}","['optimization', 'numerical-methods', 'analysis']"
4145629,Calculating a contact hamiltonian vector field,"I'm currently trying to study contact geometry and stumbled upon the section talking about contact vector fields. Contact vector fields are constructed in a similar manner as the symplectic vector fields so i tried to do the derivation myself, but am stuck at the moment. We are given the relations $\mathcal{L}_{X_H}=f_H \eta \quad  -H= \eta (X_H) \quad \mathcal{L}_{X_H}\eta=d\eta(X_H, \cdot) + d[\eta(X_H)] \quad dH=d\eta(X_H,\cdot)-\mathcal{L}_{X_H}\eta \quad$ and $ f_H=-\xi(X)$ Where as $\xi$ is the reebs vector field (also called characteristic vector field) and $\eta=dw-p_idx^i$ is the contact form in darboux coordinates.
I wanted to proceed like you do in symplectic geometry, by figuring out the components of the vector field by comparing coefficients. In the following calculations i will denote the components of the contact hamiltonian vector field with an sub- or superscript H What I've done so far is $dH=-dp_i \wedge dx^i(X_H,\cdot)-\xi(H)p_idx^i+\xi(H)dw$ $dH=-p^H_idx^i+x^i_Hdp_i -\xi(H)p_idx^i+\xi(H)dw $ $dH= -(p_i\frac{\partial H}{\partial w}+\frac{\partial H}{\partial q^i})dq^i+\frac{\partial H}{\partial p_i}dp_i + \frac{\partial H}{\partial w}dw $ if $\frac{\partial H}{\partial w}=w_H$ and $\frac{\partial H}{\partial p_i}=x^i_H$ , it also holds that: $\eta(X_H)=dw(X_H)-p_idx^i(X_H)$ $\eta(X_H)=\frac{\partial H}{\partial w}-p_i\frac{\partial H}{\partial p_i}$ $\Rightarrow \frac{\partial H}{\partial w}= p\frac{\partial H}{\partial p} - H$ So if I manage to compare the terms with a version of $dH$ calculated in a different way, so that i can recognize the terms as the wanted $p_i,x^i$ and $w$ , i get the wanted contact hamiltonian vector field $X_H=(p_i\frac{\partial H}{\partial p_i} - H) \frac{\partial}{\partial w} -(p_i\frac{\partial H}{\partial w}+\frac{\partial H}{\partial q^i}) \frac{\partial}{\partial p_i } + \frac{\partial H}{\partial p_i}\frac{\partial}{\partial q^i}$ . My problem is i have no idea, what the different way of calculating $dH$ is and i also have no idea, why my variables should be conjugate to each other, when I'm considering a system that is not restricted to a Legendre submanifold. For reference I am reading the script of Alessandro Bravetti ""contact geometry and thermodynamics"" and ""contact hamiltonian mechanics"" a paper from Alessandro Bravetti Edit: After thinking about it a bit. If I calculate $dH$ normally i would get: $dH=\frac{\partial H}{\partial p_i}dp_i + \frac{\partial H}{\partial x^i}dx^i + \frac{\partial H}{\partial w}dw$ . If i could equate these partial derivatives to be the components of my contact Hamiltonian vector field $X_H$ , I would be done (i think). This could be true, if my $p_i$ and $x^i$ are truly conjugate to each other even when not restricted to a Legendre submanifold. Does anyone have potentially an idea, if my variables are also conjugate to each other, when not restricted to a Legendre submanifold? Edit 2: Imposing on this question the fact that we are talking about thermodynamic systems, my $x$ and $p$ variables should be indeed conjugate to each other. So the last question would be, why i can equate $\frac{\partial H}{\partial w}$ to be my $w$ component of $X_H$","['contact-geometry', 'vector-fields', 'symplectic-geometry', 'differential-geometry']"
4145638,$A$ is positive semidefinite $\iff \text{det} (B_K) \geq 0$,"Let $A \in \mathbb R^{n \times n}$ a symmetric matrix. Show that $A$ is positive semidefinite $\iff$ all its symmetric minors are $\geq 0$ , that means $\det(B_K) \geq 0$ for all $K \subseteq \{1,\cdots,n\}$ . (Let $K = \{ l_1, \cdots, l_k \} \subseteq \{1,\cdots,n\}$ where $1 \leq l_1 < l_2 < \cdots < l_k \leq n$ . The matrix $B_K \in \mathbb R^{k \times k}$ is the matrix with $(B_K)_{ij}=A_{l_il_j}$ , $1\leq i,j \leq k$ ). $\Longrightarrow$ $A$ is symmetric matrix so by the spectral theorem, we have that $$A=U\begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix}U^T$$ with $U$ an orthogonal matrix and $\lambda_i \geq 0$ $\forall i$ because A is positive semidefinite. We have that $(B_K)=\begin{bmatrix}
u_{l_1} \\
\cdots \\ u_{l_k} 
\end{bmatrix}$ $\begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix}$ $\begin{bmatrix}
u_{l_1}^T &
\cdots & u_{l_k}^T
\end{bmatrix}$ , where $u_{l_i}$ is the $l_i^\text{ th}$ line of $U$ . Since $\begin{bmatrix}
u_{l_1} \\
\cdots \\ u_{l_k} 
\end{bmatrix} \cdot \begin{bmatrix}
u_{l_1}^T &
\cdots & u_{l_k}^T
\end{bmatrix}=I_k$ then $\det(B_K)=\det\begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix} = \Pi_{i=1}^n \lambda_i \geq 0$ . Can someone help me for the $\Longleftarrow$ way ?","['orthogonal-matrices', 'linear-algebra', 'positive-semidefinite', 'symmetric-matrices']"
4145658,Real World CNC Machining Geometry Problem,"I need to re-find center on a circle. In our tool room a tool maker milled an annulus with a 4 hole pattern. The holes were milled in a square, each equidistant from the center of the annulus. The part was removed from the machine and pins were installed into the holes. Then returned to the machine for more work. In order to re-find the center the tool maker puts a dial indicator in the spindle and moves the spindle close to the center of the part and checks the pins with the indicator. The indicator will tell him the relative distance each pin is from the center of the spindle. in other words the indicator tip is some at some arbitrary radius from the center of the spindle and when it moves past a pin the dial shows how much closer the pin is to center than the neutral position of the tip. Known: The radius of the annulus, pin pattern and pins The origin of the coordinate system is within the annulus The origin of the coordinate system is defined by the axis of the spindle/indicator. The difference, radially, between a radius of constant, but arbitrary length and the distance from the pins and the origin of the coordinate system. Unknown The angle of the square hole pattern to the coordinate system Location of the center of the annulus relative to the coordinate system. Assumptions: The parts are machined to perfect tolerances Looking for: An equation that describes the center of the annulus Update: Please forgive the hand drawn diagram (if anyone has a recommendation for an easy way to draw pretty diagrams on the computer, please let me know). Here is a drawing of how I visualize the problem. R is the arbitrary, unknown radius of the indicator centered at the origin. $\delta_A$ (etc.) is the measurement from the indicator. Therefor, point A (the closest point of the pin to the center) is at a distance of R- $\delta_A$ from the center, with an unknown angle $\theta_A$ Square ABCD is of known size but not known position, so I cannot simply find it's center. AB=BC=CD=AD, AC=BD; known: length AC Looking for the position of center point c (sorry about any ambiguity around pin C and center point c)","['trigonometry', 'geometry']"
4145664,"$f(a)=b,f(b)=c,f(c)=a$, find $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}$","Let $f(x)=x^2-x-2$ . It is given that $a, b,c \in \mathbb{R}$ such that $$f(a)=b,f(b)=c,f(c)=a$$ Then find the value of $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}$ . My attempt:
We have: $$\begin{array}{l}
a^{2}-a-2=b \\
b^{2}-b-2=c \\
c^{2}-c-2=a
\end{array}$$ Subtracting pair wise we get: $$(a-b)(a+b)=a-c$$ $$(b-c)(b+c)=b-a$$ $$(c-a)(c+a)=c-b$$ Trivially $a=b=c$ satisfies. So we get $a=b=c=\sqrt{3}+1, 1-\sqrt{3}$ Thus we get $$\frac{1}{a}+\frac{1}{b}+\frac{1}{c}=\frac{3\sqrt{3}+3}{2}, \frac{-3-3\sqrt{3}}{2}$$ Now if $a\ne b \ne c$ Then we have: $$(a+b)(b+c)(c+a)=-1$$ Letting $p=a+b+c$ we have $$(p-a)(p-b)(p-c)=-1$$ $\implies$ $$p(ab+bc+ca)-abc+1=0$$ Any hint here?","['algebra-precalculus', 'symmetric-polynomials', 'polynomials']"
4145736,Find all $\alpha$ such that $\frac{\cos \alpha - a \sin \alpha}{a \cos \alpha + \sin \alpha}$ is rational (given $a$ rational),"Given $a$ rational, find all $\alpha$ such that the expression $$\frac{\cos \alpha - a \sin \alpha}{a \cos \alpha + \sin \alpha}$$ is rational","['trigonometry', 'rational-numbers']"
4145772,"Why is tan(x) denote a line perpendicular to the tangent, not the tangent itself?","I asked a question before, and was directed here instead. :^(  Hopefully, someone reading on this thread might still be able to help answer my question. I am trying to understand how the word ""tangent"" was decided in trigonometric nomenclature, why did mathematicians choose the name of the exact opposite of the tangent line?  In geometry, the definition of Tangent is ""a straight line or plane that touches a curve or curved surface at a point, but if extended does not cross it at that point"". Geometric definition of Tangent BTW - In Doug M's response, he has 2 pics of unit circles.  (It is nice to have a visual to work with, by the way)  On the 1st one, he shows a red line and says the vertical line that intersects with the line radiating from the unit circle is ""has a measure of  tanx"".  I get what he is trying to get across with this statement.  That is the definition of x/y in a unit circle (1/1=1), but not in every quadrant.  Tanx of 135° is negative, there is a sign associated with that measure.  TanX is the slope of the line that contains the radius perpendicular to the circle.  The way I read Doug's statement seems to only be a true (between 0-90° and at exactly 180°).  Of course, I will clearly accept corrections to any wayward logic.","['trigonometry', 'tangent-line']"
4145786,Why aren't differential forms just functions?,"Vectors on a manifold are defined as directional derivatives evaluated at a point and one-forms (in the associated cotangent space) are defined as maps from vectors to real numbers. However, why aren't functions on the manifold themselves one-forms? Clearly functions also take vectors to real numbers. I haven't seen any book talk about this. In addition since any linear map from a vectors to a real numbers on a tangent space can be expressed as a linear combination of one-forms, what does this mean for functions? It doesn't make sense for functions to be expressed as linear combinations of one-forms because they are 0-forms. Here's my understanding of how functions relate to one-forms. Given a coordinate chart $\Phi:M \rightarrow \mathbb{R}^n$ on a n-dimensional manifold $M$ , we can think of the coordinates as functions $x^\alpha:M \rightarrow \mathbb{R}$ . Now, vectors in  a tangent space at some point $p \in M$ can be expressed as $V=V^\beta \partial_\beta|_p$ . The coordinate one-form $dx^\alpha$ is defined by it's action on a vector: $$dx^\alpha (V^\beta \partial_\beta) = V^\beta dx^\alpha (\partial_\beta) \stackrel{?}{=} V^\beta \partial_\beta (x^\alpha)|_p = V^\beta \delta^\alpha _\beta = V^\alpha$$ Although this is standard, I think about the one-form $dx^\alpha$ as placing the coordinate function in the directional derivative $\partial_\beta$ . Clearly, the coordinate function only changes when the directional derivative is along the direction of $x^\beta$ . Hence, we get the kronecker delta. However, I do not know if this is the right way of think about one-forms (hence the question mark on the equal sign). But if this is right, the one-form is just the coordinate function $x^\alpha$ . So what's the difference between a one-form and a function? I seem to be misunderstanding something but I am not sure what. Any help is appreciated.","['manifolds', 'differential-forms', 'differential-geometry']"
4145790,Probability that the sum of the numbers shown is a multiple of $5$,"A regular die is rolled $n$ times such that $5|n$ . Probability such that the sum of the numbers shown is a multiple of $5$ is given by $$\frac{a^n+b}{c\cdot d^n}.$$ Find $a,b,c$ and $d$ . What I thought is that we need to find coefficient of all those exponents of $x$ which are multiples of $5$ in the expansion of $(x+x^2+....+ x^6)^n$ . Thereafter I am unable to solve it further.","['binomial-coefficients', 'probability']"
