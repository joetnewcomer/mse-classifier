question_id,title,body,tags
2217821,Sign of determinant when using $det A^\top A$,"We have been given matrix:
$$A = 
        \begin{pmatrix}
        a& b& c &d \\
        b &−a& d& −c\\
        c& −d &−a& b \\
        d &c& −b& −a\\
        \end{pmatrix}
    $$ ...and have been asked to calculate $\det(A)$ using $AA^T$. We see that: $$AA^T=\begin{pmatrix}
        a^2+ b^2+ c^2+ d^2& 0& 0&0\\
        0 &a^2+ b^2+ c^2+ d^2& 0& 0\\
        0& 0 &a^2+ b^2+ c^2+ d^2& b \\
       0 &0& 0& a^2+ b^2+ c^2+ d^2\\
        \end{pmatrix}
    $$ So, $\det(AA^T)= (a^2+ b^2+ c^2+ d^2)^4$ Now which should I choose: $(a^2+ b^2+ c^2+ d^2)^2 $ or $ -(a^2+ b^2+ c^2+ d^2)^2$ ? Please explain me which one and why.","['linear-algebra', 'determinant']"
2217865,Find Expected value,"Suppose, $X \sim \operatorname{Poisson}(\lambda)$, Then what is  $\operatorname{E}\left(\dfrac{1}{X+1}\right)$ ? My attempt: Let $Y = \dfrac{1}{X+1}.$ Then $$
F_Y(t) = \{ Y \leq t\} = \left\{ X \geq \frac{1-t} t \right\} = 1- F_Y \left( \frac{1-t} t \right).
$$ is this way okay? Any hint/alternative way to solve this ...","['descriptive-statistics', 'statistics', 'stationary-processes', 'statistical-inference']"
2217870,ODE's: study of the case $x'=f(x/t)$ with $f:\Bbb{R}\to \Bbb{R}$ a $C^1$ function with $f(r)=r$ for some $r\in \Bbb{R}$...,"(Exercise 6, Chap. 1, from Sotomayor's ODE Lessons): Let $f:\Bbb{R}\to \Bbb{R}$ be a $C^1$ function and $r\in \Bbb{R}$ such that $f(r)=r$ . Show that a) if $f'(r)<1$ , then no solution of the equation $$x'=f\left(\frac{x}{t}\right)\,\,\,\,(*)$$ is tangent at $0$ to the solution $\varphi(t)=rt$ . b) if $f'(r)>1$ , then there are infinitely many solutions of $(*)$ tangent to $\varphi(t)=rt$ at the origin. Two functions $\varphi$ and $\psi$ defined for $t>0$ are said to be tangent at $0$ if $\lim\limits_{t\to 0}\dfrac{\psi(t)-\varphi(t)}{t}=0$ . There is a duplicate for this question here: If $f$ continuous differentiable and $f'(r) < 1,$ then $x'=f(x/t)$ has no other solution tangent at zero to $\phi(t)=rt$ but only the answer to item a) is given there and, furthermore, I could not understand it...",['ordinary-differential-equations']
2217906,What are the three non-isomorphic 2-dimensional algebras over $\mathbb{R}$?,"What are the three non-isomorphic $2$ -dimensional algebras over $\mathbb{R}$ ? Am I right in thinking they are $\lbrace x+iy: x,y\in\mathbb{R},\ i^{2}=-1\rbrace$ , $\lbrace x+jy: x,y\in\mathbb{R},\ j^{2}=1\rbrace$ and $\lbrace x+\varepsilon y: x,y\in\mathbb{R},\ \varepsilon^{2}=0\rbrace$ ?","['abstract-algebra', 'representation-theory']"
2217921,Expressing this angle in terms of other angles,"I want to express $\theta$ in terms of $\alpha$ and $\beta$ (or their tangents or other trig functions), but have no idea how to do so. Can someone please clarify? I just don't see the needed relations.","['trigonometry', 'geometry']"
2217923,Reducing an SIR model,"Past Paper Question: Given a SIRS model: \begin{align}
\dfrac{dS}{dt} & = -\alpha IS+\gamma R \tag1 \\[8pt]
\dfrac{dI}{dt} & = \alpha IS-\beta I\\[8pt]
\dfrac{dR}{dt} & = \beta I-\gamma R,
\end{align} where $S$, $I$ and $R$ denote the number of susceptible, infected and recovered individuals respectively and $t$ is time. Show that the total population N is constant, and hence reduce the system to two equations for $S$ and $I$. My Attempt: Since we are only looking over a small time interval, natural births and deaths are negligible hence we have a closed population so:
  $$S\left( t\right) +I\left( t\right) +R\left( t\right) =N \tag4 $$ My Question: How do you use this to reduce the above to a system of two ODE's?","['biology', 'ordinary-differential-equations', 'mathematical-modeling']"
2217925,Expand binomially to prove trigonometric identity,"Prompt: By expanding $\left(z+\frac{1}{z}\right)^4$ show that $\cos^4\theta = \frac{1}{8}(\cos4\theta + 4\cos2\theta + 3).$ I did the expansion using binomial equation as follows $$\begin{align*}
\left(z+\frac{1}{z}\right)^4 &= z^4 + \binom{4}{1}z^3.\frac{1}{z} + \binom{4}{2}z^2.\frac{1}{z^2} + \binom{4}{3}z^3.\frac{1}{z}+\frac{1}{z^4}\\
&=z^4+4z^2+6+\frac{4}{z^2}+\frac{1}{z^4}\\
&=z^4+\frac{1}{z^4}+4\left(z^2+\frac{1}{z^2}\right) + 6.  -(eqn 1)
\end{align*}
$$ I'm not sure how to go on about rest of the problem. [update]
Reading comments, I tried assuming $z = e^{i\theta}$ $2\cos\theta = e^{i\theta} + e^{-i\theta}$ $(2\cos\theta)^4 = (e^{i\theta} + e^{-i\theta})^4$ $=e^{4i\theta} + e^{-4i\theta} + 8(e^{2i\theta}+e^{-2i\theta})+6$  (from eqn 1)","['binomial-theorem', 'trigonometry']"
2217950,Every Hamel basis has cardinality continuum $(c)$,"I found the demonstration of this assertion in a book: ""Let the cardinality of some Hamel basis be $\kappa$. We can easily calculate the cardinality of the generated vector space: it is $\aleph_0(\kappa+\kappa^2+\ldots)=\aleph_0\kappa = \kappa$ and since this must be equal to c , we obtain $\kappa$ = c ."" ( c is the cardinality of the continuum) How did they get $\aleph_0(\kappa+\kappa^2+\ldots)$ ?",['functional-analysis']
2217967,Integration -- can't figure out the substitution,"I need to evaluate the integral
$$ \int_0^1 \frac{6\pi}{\lvert{2 - 3e^{2\pi i t}}\rvert^2} \,\mathrm{d}t.$$
The problem I am having is that I can't find a nice way to re-write the denominator to invoke a substitution. Perhaps I can view this as a contour integral in the complex plane? Any ideas appreciated.","['complex-analysis', 'integration']"
2217983,How to find $g^2(x)$ from $f^{-1}g(x)$ and $g(f(x))$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given that $f^{-1}g(x)=3x-2$ and $gf(x)=12x-8$. Find $g^2(x)$. Can anyone give me some hints on this question?","['algebra-precalculus', 'functions']"
2218018,Global sections of a sheaf restricted to a closed subscheme,Let $X$ be non-singular projective variety. Consider a smooth closed subvariety say $Y$ of $X$. Let $F$ be a torsion-free coherent sheaf on $X$. Then is there any description of global sections of $F|_Y$ in terms of global sections of $F$ on $X$? There is a short exact sequence $$0\rightarrow I_Y\rightarrow O_X\rightarrow O_Y.$$ However if we tensor with $F$ we get only right exactness. Another issue is that I don't know if $O_Y\otimes F$ can be written as $F|_Y$ since I know the projection formula only for $F$ locally free. Amy insight would be helpful.,['algebraic-geometry']
2218019,Complex integral (Miscellaneous Problems),"Show that for $-1<p<1$, 
$$ \int_0^{\infty} \frac{\cos(px)}{\cosh x} \,dx = \frac{\pi}{2\cosh(p\pi/2)}.$$ I make L.S to $$\frac{e^{ipx}+e^{-ipx}}{e^{x}+e^{-x}}$$ and I cannot approach next step.","['complex-analysis', 'integration']"
2218060,Laurent expansion of $\exp(z+\frac 1z)$,"This question is similar to this one , but I want to explicitly calculate the Laurent series for the function
$$f(z)=\exp\left (z+\frac 1z\right)$$
around $z=0$, i.e. find a closed form for the coefficients $b_n$ of the expression
$$f(z)=\sum_{n=-\infty}^{+\infty}b_nz^n$$
Using the usual series for $\exp(\cdot)$, 
$$
\begin{align}
\exp\left(z+\frac 1z\right) &=\sum_{n=0}^\infty\frac{\left(z+\frac 1 z\right)^n}{n!}
\\&=\sum_{n=0}^{\infty}\sum_{k=0}^n \frac{\binom n k z^kz^{k-n}}{n!}
\\&=\sum_{n=0}^{\infty}\sum_{k=0}^n \frac{1}{k!(n-k)!}z^{2k-n}
\\&=\sum_{n=0}^{\infty}z^{-n}\sum_{k=0}^n \frac{1}{k!(n-k)!}z^{2k}
\end{align}
$$ Forgetting about coefficients, the $z$ terms in the sum indexed by $k$ are of the kind
$$1+z^2+z^4+\ldots +z^{2n}$$
and when multiplied by $z^{-n}$ are symmetric, becoming of the kind
$$z^{-n}+z^{-n+2}+z^{-n+4}+\ldots+z^n \tag 1$$
It seems like from this we can reconstruct each coefficient. Let's try to understand what the coefficient for $z^0$ should be; clearly only even $n$ contribute to this coefficient since $z^{2k-n}$ always has odd exponent for odd $n$. If $n$ is even, the only term which has $0$ exponent in $(1)$ is the central one, i.e. the coefficient of $z^0$ is 
$$\sum_{n=0: \text{ n is even}}^\infty \frac{1}{\left(\frac n2\right)!\left(\frac n2\right)!}=\sum_{k=0}^\infty\frac{1}{k!k!}$$ Playing a bit with the coefficients, I conjecture that a closed form for $(b_n)_{n\in \mathbb Z}$ is $$b_n=b_{-n}=\sum_{k=0}^\infty\frac{1}{k!(n+k)!}$$ I have two questions: Can my argument for the calculation of $b_0$ be generalized to yield a closed form for $b_n$? Is the conjectured closed form for $b_n$ correct? How can we find a closed form for $b_n$?","['laurent-series', 'complex-analysis', 'sequences-and-series']"
2218081,Deriving formula from geometric relations,"If we know that and we have this triangle Then how can we derive this relation? ($\alpha$ and $\beta$ are assumed to be small, so that $\tan(\alpha) \approx \alpha$ and $\tan(\beta) \approx \beta$). We also know that $\theta = \alpha + \beta$, so $$\alpha \approx \frac{4GM}{c^2b}-\tan(\beta)=\frac{4GM}{c^2b}-\frac{b}{D}$$ Also, $b=d\tan(\alpha)=D\tan(\beta)$. But what next? Where is the square root coming from?","['trigonometry', 'geometry']"
2218085,Proving $\cos(4\theta) - 4\cos(2\theta) \equiv 8\sin^4\theta - 3$.,"Prove the following identity:
  $$\cos(4\theta) - 4\cos(2\theta) \equiv 8\sin^4\theta - 3$$ How can I express $\cos(4\theta) $ in other terms?",['trigonometry']
2218092,Statistics of a racketball/volleyball/badminton game,"It's been a while since I've used my knowledge in statistics and I have no idea how to turn that problem into an equation. I wanted to challenge myself but I failed. I thought maybe you too would like this challenge. Starting with pre-defined score and who currently has the ball, I'd like to estimate the odds of winning of each player/team assuming they are precisely as likely to win any exchange. A player/team scores a point every time it wins an exchange when it has the ball. If it wins an exchange when the other player/team has the ball, the scores stays the same but it gets the ball and the chance to score some points. A racketball game, for example, ends when a player reaches 15 points but the game may continue as long as no one leads by 2 points (15-14 is not a valid final score, but 16-14 is). There is theoretically two possibilities that this game never ends : either you enter a loop where no-one scores any point or no-one can lead by two points, and this is what I find very tricky. Still I am pretty convinced that this can be solved, just not by a newbie like me. This problem has 4 variables : The current score of the player/team who has the ball The current score of the other player/team The score at which a game normally ends The minimum number of points by which the winning team/player has to lead This question looks like mine but starts with the final score. Maybe it could be useful anyway.",['statistics']
2218096,"Chapter 3, Theorem 15 on Marcus' Number Fields","I am reading Daniel Marcus' book on Number Fields and I got stuck in the proof of Theorem $15$ , Chapter $3$ . Theorem 15 : Let $I$ be a nonzero ideal in a Dedekind domain $R$ . Then there is a nonzero ideal $J$ such that $IJ$ is principal. Before developing the proof, Marcus proves two lemmas: Lemma 1 : In a Dedekind domain, every nonzero ideal contains a product of nonzero prime ideals. Lemma 2 : Let $A$ be a nonzero proper ideal in a Dedekind domain $R$ with field of fractions $K$ . Then there is an element $\gamma \in K\setminus R$ such that $\gamma A\subset R$ . Then the proof of Theorem 15 starts by Let $\alpha$ be a nonzero member of $I$ and let $J=\left \{\beta \in R:\beta I\subset \left (\alpha\right )\right \}$ . Then $J$ is easily seen to be an ideal (nonzero since $\alpha \in J$ ) and clearly $IJ\subset \left (\alpha\right )$ . We will show that equality holds. Consider the set $A=\frac{1}{\alpha}IJ$ . This is contained in $R$ (recall $IJ\subset \left (\alpha\right )$ ), and in fact $A$ is an ideal (verify this). If $A=R$ then $IJ=\left (\alpha\right )$ and we are finished; otherwise $A$ is a proper ideal and we can apply Lemma 2. Thus $\gamma A\subset R$ , $\gamma \in K\setminus R$ . We will obtain a contradiction from this. Since $R$ is integrally closed in $K$ , it is enough to show that $\gamma$ is a root of a monic polynomial over $R$ . Observe that $A=\frac{1}{\alpha}IJ$ contains $J$ since $\alpha \in I$ ; thus $\gamma J\subset \gamma A\subset R$ . It follows that $\gamma J\subset J$ ; to see why this is true go back to the definition of $J$ and use the fact that $\gamma J$ and $\gamma A$ are both contained in $R$ . At this point I got stuck. I cannot see why $\gamma J\subset J$ .","['number-theory', 'ring-theory', 'dedekind-domain', 'algebraic-number-theory']"
2218110,Multiplier for the space of functions which have a primitive,"Let us denote by $Pr([a,b])$ the set of functions $f:[a,b]\to\mathbb R$, which have a primitive, i.e. an everywhere differentiable function $F:[a,b]\to\mathbb R$ such that $F'(x)=f(x)$ for all $x\in[a,b]$ (Limits at the boundaries are considered to be one-sided). Now, suppose a function $g:[a,b]\to\mathbb R$ has the following property: For each $f\in Pr([a,b])$ we have $f\cdot g\in Pr([a,b])$. I'd like to show (or disprove), that $g\in L^\infty([a,b])$ (i.e. that $g$ is essentially bounded). I was able to show that $g$ has the following properties: $g\in L^p([a,b])$ for each $p\in[1,\infty)$. $p\circ g\in Pr([a,b])$ for each polynomial $p$, in particular, $g\in Pr([a,b])$. The property $g\in Pr([a,b])$ alone is, of course, too weak to guarantee $g\in L^\infty([a,b])$, since there are well-known examples of derivatives which are essentially unbounded. Any help is highly appreciated. Thanks in advance!","['derivatives', 'real-analysis', 'integration']"
2218114,Theoretical Advantages of Lebesgue Integration,"This wiki article opens with the two sentences In measure theory, Lebesgue's dominated convergence theorem provides sufficient conditions under which almost everywhere convergence of a sequence of functions implies convergence in the $L_1$ norm. Its power and utility are two of the primary theoretical advantages ( my boldface ) of Lebesgue integration over Riemann integration. I thought the term ""theoretical advantages"" was interesting. I only know enough about Lebesgue integration to be able to understand terms like ""Lebesgue measurable"" when they crop up, and always thought of Lebesgue integration as a way of making a wider class of functions integrable than would be for the Riemann integral alone. Is there a generally agreed upon set of ""theoretical advantages"" like this for Lebesgue integration?","['real-analysis', 'integration', 'lebesgue-integral', 'measure-theory', 'analysis']"
2218118,Are there non-square matrices that are both left and right invertible?,"I am aware that invertible square matrices are left invertible and right invertible, and that the left and right inverses are equal. However, I was wondering whether exists a non square $m\times n$ matrice $A$ , so that exist both: An $n\times m$ matrix $B$ so that $AB = I_m$ An $n\times m$ matrix $C$ so that $CA = I_n$ I just couldn't think of an example nor of a proof that these two conditions provide that A is necessarily square.","['matrices', 'linear-algebra']"
2218155,How to find equilibria of a system of ODEs,"Firstly, I am not asking anyone to solve my equations for me. I am just looking for guidance on how to find the equilibria of my system and point me in the correct direction with analysing stability. My system is as follows, \begin{equation}
\frac{dy_i}{dx} = a_i y_i + \sum ^N_{j=1}(b_{ij}-c_{ij})y_i\cdot y_j\qquad i=1,\dots ,N
\end{equation} Here $N$ can be quite large. $a_i$, $b_{ij}$ and $c_{ij}$ are just positive parameters. I understand that there are in general four types of equilibrium solution. $y_i = 0\qquad$    $\forall i\in N$ $y_i=y_i*\qquad$     $\forall i\in N$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots,y_N =0)$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots, y_{j-1} =0, y_j=y_j*, y_{j+1}=0,\dots,y_N =0)$ Where $y_i*$ is an equilibrium value. To find these we set each $dy_i/dx =0$ and where the nullclines intersect we have an equilibrium point. So in the case of simply re-arranging the above equation for $y_i$ we find that, 
\begin{equation}
y_i* = \frac{a_i - \sum ^N_{j\neq i} (c_{ij} - b_{ij})y_j}{(c_{ii} - b_{ii})}
\end{equation} However I am a little lost as how to proceed from here! i.e how do I find the other equilibria?","['ordinary-differential-equations', 'dynamical-systems']"
2218186,"Relationship between Spin(3), SU(2), unit quaternions, and SO(3)","There may be some short-hand / informal statements that are tripping me up, but I am getting confused trying to understand the relationship between Spin(3), SU(2), SO(3), and the unit quaternions. Trying to find information online, many discussions seem to say SO(3) and SU(2) are isomorphic (for example wikipedia ). Mathworld says SU(2) is isomorphic to $O^+_3(2)$ which I'm not quite sure how that relates to SO(3) (I have not seen that notation before). While others state SU(2) is isomorphic to the unit quaternions which are in turn a double cover of SO(3). Which seems to suggest there is a lot of ""short-hand"" discussion going on and sometimes people say isomorphic ignoring a double cover? Or maybe I just misunderstand, and a double cover doesn't really matter for some reason? My best effort of trying to figure out what is going on seems to suggest: $$Spin(3) \cong SU(2) \cong \{q \in \mathbb{H} | q\bar{q}=1 \} \not \cong SO(3)$$ Is that close, or are even more of those actually double covers? What is the correct relationship between these groups? (and what does $O^+_3(2)$ denote?)","['quaternions', 'group-theory', 'group-isomorphism']"
2218192,Fourier transform of Hermite polynomial times a Gaussian,What is the Fourier transform of an (n-th order Hermite polynomial multiplied by a Gaussian)?,"['gaussian-integral', 'integration', 'hermite-polynomials', 'fourier-analysis']"
2218200,Greedy Algorithm - Exchange Argument,"Consider the following problem : Input: $2n$ positive integers (repetitions allowed) $l_1,l_2, ..., l_{2n}$. Output: $n$ pairs $(l_{11}, l_{12}), (l_{21}, l_{22}), ..., (l_{n1}, l_{n2})$ such that $\sum_{j=1}^{n} l_{j1}\cdot l_{j2}$ is maximal. My solution is to pick the 2 largest integers from the input on each greedy iteration, and it will provide the maximal sum ($\sum_{j=1}^{n} l_{j1}\cdot l_{j2}$). I'm trying to proof the correctness of the algorithm using exchange argument by induction, but I'm not sure how to formally prove that after swapping an element between my solution and the optimal solution, I have a solution which is not worse than before. I'll appreciate any direction. Thanks.","['algorithms', 'computer-science', 'discrete-mathematics']"
2218203,Distributional derivative of $\frac{e^{i|x|}}{|x|}$,"For $x\in\mathbb{R}^3$ we have $\frac{1}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ and $\frac{1}{|x|} \to 0$ as $|x| \to \infty$. Hence we can conclude that
$$
f(x)=\frac{e^{i|x|}}{|x|}
$$
defines a (tempered) distribution
$$
\phi \mapsto \int_{\mathbb{R}^3}f\phi.
$$
For a distribution $T \in \mathcal{S}'(\mathbb{R}^3)$ its derivative is defined as $(\partial x_i T)\phi = -T(\partial x_i \phi)$. Now I want to calculate the partial distributional derivative of $f$. Starting with the definition yields $$
-\int_{\mathbb{R}^3}\frac{e^{i|x|}}{|x|}\partial x_i \phi(x) dx
$$ But how do I proceed? Since $f$ has a singularitie in $0$ there could somewhere appear a delta function.","['real-analysis', 'distribution-theory', 'functional-analysis', 'integration', 'analysis']"
2218214,Where can I find the Inscribed Rectangle Problem proof?,"I've been looking into the Toeplitz' Conjecture and became very interested, so I began to study it. Here is the conjecture: For any Jordan curve $\space \gamma \space$, there exist four distinct points on $ \space \gamma \space$ such that these four points are the vertices of a square. In order to study this, a mathematician H. Vaughan wrote a paper on the proof that: For any Jordan curve $\space \gamma \space$, there exist four distinct points on $ \space \gamma \space$ such that these four points are the vertices of a rectangle. But I can't seem to find the paper, or at least anyone else rigorously explaining the proof. The best I've found is this video: https://www.youtube.com/watch?v=AmgkSdhK4K8&t=313s Though I would love a read on the proof. Thank you.","['geometric-topology', 'general-topology', 'problem-solving']"
2218294,Implicit Differentiation Quesrion,$$x^y = y^x$$ is the equation My solution is :$$\frac { dy }{ dx } =\left( \ln { y-\frac { y }{ x }  }  \right) /\left( \ln { x-\frac { x }{ y }  }  \right) $$ Just wondering if this is correct!,"['derivatives', 'calculus']"
2218336,Parameterization of a triangle in $\mathbb R^3$,"I'm asked to find the area of a triangular region $T$ with vertices at $(1,1,0)$, $(2,1,2)$, and $(2,3,3)$. With the help of software, I was able to conjure up the following parameterization for $T$, $$\mathbf r(s,t)=(t+1,2s+1,s+2t)$$ where $t\in[0,1]$ and $s\in[0,t]$. Proceeding with this setup, I find the area to be $$\int_{t=0}^{t=1}\int_{s=0}^{s=t}\|\mathbf r_s\times\mathbf r_t\|\,\mathrm ds\,\mathrm dt=\sqrt{21}\int_0^1t\,\mathrm dt=\frac{\sqrt{21}}2$$ The problem is that I don't understand why this parameterization works. (My memory of the details of this part of vector calculus is a bit stale.) What I do understand is how to find the parametric equations for each edge of $T$. For the vertices $(1,1,0)$ and $(2,1,2)$, I have $\vec r_1=(1+t,1,2t)$; for $(1,1,0)$ and $(2,3,3)$, I have $\vec r_2=(1+t,1+2t,3t)$; and finally, for $(2,1,2)$ and $(2,3,3)$, I have $\vec r_3=(2,1+2t,2+t)$. In each case, $t\in[0,1]$. It seems my jury-rigged $\mathbf r$ relies heavily on $\vec r_2$, but I don't immediately see why that is the case.","['multivariable-calculus', 'surface-integrals', 'area']"
2218400,Questions about Hausdorff measure and general metric outer measures.,"The Hasudorff measure of a set $A\subset \mathbb R^n$ is defined as 
$$ \mathcal H^s(A) = \lim_{\delta\downarrow 0} \mathcal H^s_\delta(A)$$ 
where 
$$\mathcal H^s_\delta(A) = \inf\left\{\sum_{j=1}^\infty (\text{diam }S_j)^s : \{S_j\}_{j=1}^\infty \subset 2^{\mathbb R ^n} \text{ covers } A \text{ and } \text{diam } S_j < \delta  \right\} $$
It is easy to see that $\mathcal H^s_\delta(A)$ is non-increasing in $\delta$. However, I don't quite understand why $\mathcal H^s_\delta(A)$ is not actually constant in $\delta$ for each $A$; it would seem that the infimum should be achieved as the diameters of the sets $S_j$ get smaller and smaller, meaning that bounding the diameter of $S_j$ above by $\delta$ should do nothing at all to $\inf\sum_{j=1}^\infty (\text{diam }S_j)^s $. Moreover, suppose we approach Lebesgue measure analogously. Let $\mathscr{L^n}(A)$ be $n$-dimensional Lebesgue measure, and let 
$$\mathscr{L_\delta^n}(A) := \inf\left\{\sum_{j=1}^\infty|B_j| : \{B_j\}_{j=1}^\infty \subset 2^{\mathbb R ^n} \text{ is a box cover of } A \text{ and } \text{diam } B_j < \delta  \right\} $$
we find that $\mathscr{L^n_\delta}(A)$ is constant in $\delta$, so that for any $\delta>0$ 
$$\mathscr{L^n}(A) = \mathscr{L^n_\delta}(A)= \lim_{\epsilon\rightarrow 0}\mathscr{L^n_\epsilon}(A)$$ So what about switching from volumes of boxes to diameters of arbitrary sets makes $\mathcal H^s_\delta(A)$ not necessarily constant in $\delta$? This question hints at a much more general question about the construction of outer measures via the Carathéodory method. A brief recap on this method (as I have learned it in class): Suppose $\mathcal F$ is a collection of subsets of some metric space $X$, let $A\subset X$, and consider the following set
$$\mathcal C_{\delta, \mathcal F}(A) : = \left\{\{S_j\}^\infty\subset \mathcal F \mid \{S_j\}^\infty \text{ covers } A \text{ and } \text{diam }S_j< \delta\right\}$$
and consider any function $\zeta: \mathcal F \rightarrow [0,\infty]$.
Then, the following functions are outer measures: $$\mu_{\delta, \mathcal F}(A) := \inf_{\{S_j\}^\infty \in \mathcal C_{\delta, \mathcal F}(A)} \sum^\infty_{j=1} \zeta(S_j)$$
$$\mu_{\mathcal F}(A) := \lim_{\delta \downarrow 0}\mu_{\delta, \mathcal F}(A)$$
Notice that the  Hausdorff measure $\mathcal H ^ s(A)$ is the special case where $\mathcal F = 2^X$ and $\zeta(S) = (\text{diam } S)^s$. Under what conditions is $\mu_{\delta,\mathcal F}(A)$ not constant in $\delta$ for every $A$, as is the case for $\mathcal H^s_{\delta}$?","['hausdorff-measure', 'geometric-measure-theory', 'measure-theory']"
2218408,Cauchy's Theorem and Contour Integrals,"Using Cauchy's Theorem and the function $f(z)=ze^{-ikz}e^{-z^2/{2a^2}}$ to evaluate $$\int_{0}^{\infty} x\sin(x)e^{-x^2/{2a^2}} $$ Thoughts: I am having trouble understanding the relationship between the complex function given, and the integrand. I have tried writing $sin(x)$ as $(e^{iz}-e^{-iz})/2$, but the function I get doesn't match. Also why is there a $k$ in the function they gave me?","['complex-analysis', 'complex-integration']"
2218433,"How ""messy"" can a multivariable function be?","In calculus lectures, we are told that approaching a limit in a multivariable function through some paths does not guarantee the existence of the limit, for example: here . And one can see that, in the following example its easy to see that there are $2$ paths that seem to point, in one case, that the limit is $0$ and in another case, that the limit is $1$ and hence it does not exist. $$f(x,y)=\frac{2xy}{x^2+y^2} \quad f(0,0)=0  $$ But the objects we have in question are continuous functions with the exception of perhaps, the point $(0,0)$ or some other strategically placed point . It seems intuitively reasonable that one can approach $(0,0)$ though $2$ or some ""low"" number of different paths and have different possible limits but it doesn't seems reasonable that one could have a function such that the possible limit is different for $278$ paths, for example. I'm not sure if these lectures try to take into account the full generality of multivariable functions, but  it seems absurd that a  $2-$variable function can have such a messy structure that actually allows it to have an arbitrary number of paths that suggest (each one) different possible limits. But I don't know what to say/think when the number of variables is greater than $3$. EDIT: There are interesting related sub-questions I forgot to add: Is it possible that we test the existence of a limit with all the lines and polar coordinates, obtain a possible limit $L$ for these tests and yet, the function have another path such that the possible limit for this path is different of $L$? The question above glooms to this: Isn't there a minimum number of paths given by families of curves in which, the possible limit is $L$ for all of them and this actually guarantees that the limit is $L$? I make this question because it seems extremely counter-intuitive that given - for example - tests with all the lines, all the parabolas and polar coordinates all with possible limit $L$, there could still be one path that gives a possible limit different of $L$.","['multivariable-calculus', 'real-analysis', 'soft-question', 'limits']"
2218444,Combinatorial proofs of the equation $\sum_{k=0}^{n} k{n \choose k}^2 = n {2n-1 \choose n-1}$,"$$ \sum_{k=0}^{n} k{n \choose k}^2 = n {2n-1 \choose n-1}$$
How can we prove this? Basically, my method to choose a list of length $n$ from a list of length $2n$. We specify a specific element, then I got $${2n-1 \choose n-1}$$ as we select n elements, it becomes:$$n {2n-1 \choose n-1}$$
How can we use this to prove it? Another method is to make a middle pivot in $2n$ list, choose $i-1$ from the front $n$ elements, and choose $n-(i-1)$ elements from back list.(*2 as we can do this to back last) we get $$2{n-1 \choose i-1}{n \choose n-(i-1)}$$
How can we use this to prove it?","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
2218454,Prove $\sum\limits_{n = 1}^\infty \frac{( - 1)^n}{\ln n + \sin n} $ is convergent.,"Help prove the alternating series $\sum\limits_{n = 1}^\infty \frac{(-1)^n}{\ln n + \sin n}$ is convergent. $\frac 1 {\ln n + \sin n}$ is a decreasing sequence but it is not motonically decreasing. I am not sure how to deal with this situation. My failed attempt.. For even terms, $$\sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + 1} \le \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + \sin 2n} \leqslant \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n - 1} $$ where the two ""bound"" series do not converge For odd terms $$\sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln (2n + 1) - 1} \leqslant \sum_{n = 1}^\infty \frac{(-1)^{2n + 1}}{\ln (2n + 1) + \sin (2n + 1)}  \leqslant \sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln 2n + 1 + 1} $$ where the two ""bound"" series do not converge.","['real-analysis', 'sequences-and-series', 'calculus']"
2218462,Prove $A^{T}$ is diagonalizable,"Let $A$ be an $n \times n$ matrix. Prove that if $A$ is diagonalizable, then $A^T$ is also diagonalizable. By definition, since $A$ is diagonalizable, $A = P^{-1} B P$, where $B$ is a diagonal matrix and $P$ is an invertible matrix. $A^T = P^T B^T (P^{-1})^T$ $B^T$ is a diagonal matrix by definition, we have $(P^{-1})^T = (P^T)^{-1}$, let $D = P^{-1}$ then, $A^T = DB^T D^{-1}$ where $B^T$ is diagonal, thus $A^T$ is diagonalizable by definition Does this work?","['matrices', 'diagonalization', 'linear-algebra', 'vector-spaces']"
2218501,Metamathematics of the Banach space consequences of the Baire category theorem,"My question is about a comment in Tao's presentation of the Uniform Boundedness Principle, the Open Mapping theorem, and the Closed Graph theorem in his blog post 245B, Notes 9: The Baire category theorem and its Banach space consequences .  He says typically the ""easy"" directions of the theorems are used in practice while the ""hard"" directions provide metamathematical justification for the approach. Strictly speaking, these theorems are not used much directly in practice, because one usually works in the reverse direction (i.e. first proving quantitative bounds, and then deriving qualitative corollaries); but the above three theorems help explain why we usually approach qualitative problems in functional analysis via their quantitative counterparts. Despite being given three examples of this phenomenon, I still don't understand what he means at all.  The examples are below, and I added the boldface to highlight what I don't get.  Could someone please expand on this, maybe with a simpler example?  I'm aware this is not supposed to be a formal logical concept, but I just don't understand what he's saying. In the discussion following Example 1 (Fourier inversion formula by Uniform Boundedness), Tao writes This argument only used the “easy” implication of Corollary 1, namely the deduction of 2. from 3.  The “hard” implication using the Baire category theorem was not directly utilised.  However, from a metamathematical standpoint, that implication is important because it tells us that the above strategy to prove convergence in norm of the Fourier inversion formula on $L^2$ – i.e. to obtain uniform operator norms on the partial sums, and to establish convergence on a dense subclass of “nice” functions – is in some sense the only strategy available to prove such a result. In Remark 5 following the Open Mapping Principle, Tao writes The open mapping theorem provides metamathematical justification for the method of a priori estimates for solving linear equations such as $Lu = f$ for a given datum $f \in Y$ and for an unknown $u \in X$, which is of course a familiar problem in linear PDE.  The a priori method assumes that $f$ is in some dense class of nice functions (e.g. smooth functions) in which solvability of $Lu=f$ is presumably easy, and then proceeds to obtain the a priori estimate $\|u\|_X \leq C \|f\|_Y$ for some constant $C$.  Theorem 3 then assures that $Lu=f$ is solvable for all $f$ in $Y$ (with a similar bound).  As before, this implication does not directly use the Baire category theorem, but that theorem helps explain why this method is “not wasteful.” Finally, following the Closed Graph Theorem Tao writes In practice, one should think of $Z$ as some sort of “low regularity” space with a weak topology, and $Y$ as a “high regularity” subspace with a stronger topology.  Corollary 3 motivates the method of a priori estimates to establish the $Y$-regularity of some linear transform $Tx$ of an arbitrary element $x$ in a Banach space $X$, by first establishing the a priori estimate $\|Tx\|_Y \leq C \|x\|_X$ for a dense subclass of “nice” elements of $X$, and then using the above corollary (and some weak continuity of $T$ in a low regularity space) to conclude.  The closed graph theorem provides the metamathematical explanation as to why this approach is at least as powerful as any other approach to proving regularity.","['functional-analysis', 'meta-math', 'real-analysis', 'problem-solving']"
2218506,Probability of drawing 5 cards from a deck of 52 that will have the same suit?,"A standard deck of cards has 52 members consisting of 4 suits each with 13 members.  Five cards are dealt from the randomly mixed deck.  What is the probability that all cards are the same suit? EDIT: How I went about it before posting this question was doing (1/4) as the first card probability because my thought process was that we'll draw 1 suit out of the 4 for the first probability. Then I proceeded to account of the 2nd dealt card with the probability of (12/51) since 1 card has been dealt already out of the 13 cards for that suit, also subtracting 1 from the total amount of cards able to be dealt. So for the 3rd card: (11/50) 4th card: (10/49) 5th card: (9/48) Giving us the total overall probability for drawing 5 cards of the same suit: $$ (1/4) * (12/51) * (11/50) * (10/49) * (9/48) = 33/66640 $$ EDIT2: My practice quiz given by TA's is still saying I have the incorrect answer. Given how the answer should be: $ 33/16660 $ (explained in numerous ways in the thread), I contacted the TA's to see if maybe the have setup the question incorrectly. Will update when I get an answer back. EDIT3: Got an answer back from my TA's who tested the test. They did have the answer wrong on their end. Everyone who helped me was correct!","['probability', 'card-games', 'discrete-mathematics']"
2218513,The derivative of a moving L2 norm,"Consider the class of $C^0$ signals $x(t)$ such that
$$
\left|x\left(t+T\right)-x\left(t\right)\right|\leq\epsilon 
$$
I would like to find a uniform bound on the derivative of its moving rms, which is defined by
$$
J\left(t\right)=\left(\frac{1}{T}\int_{t-T}^{t}\left|x\left(\tau\right)\right|^{p}d\tau\right)^{\frac{1}{p}}
$$
with $p=2$. Apparently, when $\epsilon=0$, $J$ is always constant and the derivative is uniformly 0; so I wonder whether $
\left|\frac{dJ}{dt}\right|
$ is uniformly small when $\epsilon$ is small. It might be worth noting that a uniform bound could be found when $p=1$ (in the case of moving average), see https://math.stackexchange.com/q/57307 . It would also be interesting to find out whether it holds for other $p\in[1,\infty]$.","['functional-analysis', 'signal-processing', 'ordinary-differential-equations', 'calculus']"
2218514,Closed-Form Solution for Permutation Table,"For given $N,n\in\mathbb N$ I am looking for a formula that generates all $N\choose{n}$ combinations in the following way: E.g. take  $N=3,n=1$. Then there are ${3\choose1}=3$ combinations such that 1| - + +
2| + - +
3| + + - or $N=4,n=0$. Then there are ${4\choose0}=1$ combinations such that 1| + + + + or $N=4,n=1$. Then there are ${4\choose1}=4$ combinations such that 1| - + + +
2| + - + +
3| + + - +
4| + + + - or $N=4,n=2$. Then there are ${4\choose2}=6$ combinations such that 1| - - + +
2| - + - +
3| - + + -
4| + - - +
5| + - + -
6| + + - - So $n$ determines the number of - in each combination (each having $N$ elements). The final order of the combinations does not matter (i.e. whether - - + + or + - + - comes first is irrelevant). But is there a formula for $f_{i,k,n,N}$ (either taking value $-1$ or $+1$) such that we get the set of combinations: $$\left\{(f_{i,k,n,N}\ \text{with}\ i=1,2,\ldots N)\in\mathbb \{-1,1\}^N \mathrel{\bigg|} k=1,2,\ldots, {N\choose n}\right\}$$ Perhaps something like $f_{i,k,n,N}=(-1)^{i+k+\ldots}$ ?","['permutations', 'combinatorics', 'combinations']"
2218541,What is the point of a local base?,"What's all the fuss about local bases? I find that everywhere I look people are confused of the notion of a local base, and frankly I am as well, because it seems to me it's equivalent an incredibly simple formulation, but everyone else expounds endlessly on ""filters"" and ""filter bases"" and comes up with these elaborate logical statements. The definition of a ""local base"" that I've learned is that if $x\in X$ where $(X,\ \tau)$ is a topological space, and $N_x$ is the neighbourhood filter of $x$ (that is, the set of neighbourhoods of $x$), then a local base of $x$ is a subset $B_x\subset N_x$ such that $\forall A\in N_x$ there is some $B\in B_x$ such that $B\subseteq A$. Alright, so, why don't we take $B_x$ to be the set of open sets containing $x$? That makes perfect sense to me, since every neighbourhood must contain some open set, and so the above logical formulation of a local base is satisfied. There you go, boom. Nothing special. No filters or anything. Clean and simple. So why do topologists make such a fuss of local bases? There's all this stuff on ""first countable"" and ""second countable"" and it seems all the separation axioms of topological spaces are rooted in the idea of bases (some local, some global, from what I can tell). The definition of a locally convex TVS uses local bases. I see tons of people confused over what a local base is. What makes local bases important? Why are people so confused, myself included over them? Why is such a complicated definition necessary? Is it because the $B_x$ I described isn't the only local base?",['general-topology']
2218639,Computing a sum involving the floor function,"If $n$ is a natural number and $x$ is a real number, compute the sum $$\sum_{0 \leqslant i < j\leqslant n}\left\lfloor \frac{x + i}{j}\right\rfloor$$ I have tried substituting some numbers to try and see a pattern, but I do not seem to be getting anywhere. 
I think the solving the problem would involve the use of Hermite's identity (as they look pretty similar). Thanks for your help.","['number-theory', 'elementary-number-theory']"
2218689,Should one study number theory before studying abstract algebra?,"My question is the opposite of the question posted here . I have started studying abstract algebra independently. I'm wondering if it is necessary, or at least advised, to have studied elementary number theory prior to studying abstract algebra, say at the level of Dummit and Foote's Abstract Algebra or Knapp's Basic Algebra .","['abstract-algebra', 'soft-question', 'elementary-number-theory']"
2218695,$ L ^ 2 $ norm of a function and its derivative,"Assume that $ f $ is a real $ C ^ 1 $ function, and $ f ( a ) = f ( b ) = 0 $ . Show that $$ \| f \| _ { L ^ 2 ( a , b ) } \le \frac { b - a } 2 \| f ' \| _ { L ^ 2 ( a , b ) } \text . $$ My attempt: I have been mostly trying to do integration by parts and then apply Cauchy-Schwarz. I also tried with mean value theorem but got no luck. Also I found a result that might be useful: Suppose that $ f $ is continuously differentiable on $ [ a , b ] $ and $ f ( a ) = f ( b ) = 0 $ . Then $$ \sup _ { a \le t \le b } | f ( t ) | \le \frac { b - a } 2 \int _ a ^ b | f ' ( t ) | \ \mathrm d t \text . $$","['normed-spaces', 'real-analysis', 'calculus']"
2218704,Why do we find the determinant when finding extrema of multivariable functions?,"So I know that when we are looking for the relative extrema of a function, f(x,y), we must eventually get the 2nd partial derivatives of f(x,y) (i.e. $f_{xx}(x,y)$, $f_{xy}(x,y)$, $f_{yx}(x,y)$, and $f_{yy}(x,y)$). We then must get the determinant of the matrix $$
\begin{matrix}
f_{xx}(x,y) & f_{xy}(x,y) \\
f_{yx}(x,y) & f_{yy}(x,y) \\
\end{matrix}
$$ And then we use the determinant to help figure out what we need to know. However, why do we need to find the determinant? What is the reasoning behind this?","['multivariable-calculus', 'hessian-matrix', 'determinant']"
2218736,Calculting limit $\cos(\sqrt{x+1})-\cos(\sqrt{x})$ as $x\to \infty$,"I am not sure how to calculate the limit:
$$\lim_{x\to\infty}\cos(\sqrt{x+1})-\cos(\sqrt{x})$$ I applied trigonometric identity to get: $$L=-\lim_{x\to \infty} 2\sin\left(\dfrac{1}{2}\dfrac{1}{(\sqrt{1+x}-\sqrt{x})}\right) 
\sin\left(\dfrac{1}{2}\dfrac{1}{(\sqrt{1+x}+\sqrt{x})}\right)$$ Not sure how to proceed using trigonometry. Also, is there a method using L'Hospital for $\infty-\infty$ form (not for this question obviously!)?","['calculus', 'limits']"
2218779,Find the measure $\mu$ such that $f = \int g d\mu$,"I have the following equation: $$P(x)=\int_0^\infty f(x,y) g(y)dy $$ This is, I have an equation relating P to the weighted integral of f with respect to the weight $g(y)$. I want to somehow invert this equation to get an expression for $g$ in terms of $P$ and $f$ (which I know). I though of expressing this equation as: $$P = \int f d\mu$$ Where the measure $\mu$ is given by $g(y)dy$. Then the problem would be for a given $P$ and $f$ to find the measure $\mu$ verifying the above equality. Is this doable? If it is, how can I do that? I also tried applying some kind of transform to my original equation to see if I can solve it in an easier way. I was thinking of applying a Laplace transform (because of the limits of my original integral) but I have the problem that I have no delay when integrating over the $y$ variable so I can't get a simplified equation as I would get in the case of a convolution. How can I find $g$ in terms of $P$ and $f$? EDIT: I have been reading this last few days and I've found what is exactly what I want to solve. I have a Fredholm equation of the first kind with a continuous (kind of Gaussian) kernel. Is this kind of equations solvable analytically or the best thing that I can get is a numerical solution?","['functional-analysis', 'integration', 'measure-theory']"
2218787,How can we show that $\int_{0}^{\pi/2}x\cos(8x)\ln\left(1+\tan x\over 1-\tan x\right)\mathrm dx={\pi\over 12}?$,"Consider the integral $(1)$ $$\int_{0}^{\pi/2}x\cos(8x)\ln\left(1+\tan x\over 1-\tan x\right)\mathrm dx={\pi\over 12}\tag1$$ An attempt: Rewrite $(1)$ as $$\int_{0}^{\pi/2}x\cos(8x)\ln\tan\left(x+{\pi\over 4}\right)\mathrm dx\tag2$$ $$\int_{0}^{\pi/2}x\cdot{1-\tan^2(4x)\over 1+\tan^2(4x)}\ln\tan\left(x+{\pi\over 4}\right)\mathrm dx\tag3$$ Or we can rewrite $(1)$ as $$\color{red}{\int_{0}^{\pi/2}x\cos^2(4x)\ln\tan\left(x+{\pi\over 4}\right)\mathrm dx}-\int_{0}^{\pi/2}x\sin^2(4x)\ln\tan\left(x+{\pi\over 4}\right)\mathrm dx=I_1+I_2\tag4$$ Applying $\ln\tan x$ series to the red part $I_1$ becomes $$I_1=\int_{0}^{\pi/2}x\cos^2(4x)\ln\left(x+{\pi\over 4}\right)\mathrm dx+\sum_{n=1}^{\infty}{2^{2n}(2^{2n-1}-1)B_n\over n(2n)!}\int_{0}^{\pi/2}x(x+\pi/4)^{2n}\cos^2(4x)\mathrm dx\tag5$$ This looked too complicate, how else can we prove $(1)?$","['integration', 'definite-integrals', 'calculus']"
2218794,Is the number $\sum\limits_{n=1}^\infty2^{-n^2}$ rational?,Is the number $\sum\limits_{n=1}^\infty2^{-n^2}$ rational? I could prove that the series is convergent (as it is bounded above by the geometric series with common ratio $\frac{1}{2}$. But how do I prove that it is rational (or not)?,"['number-theory', 'real-analysis', 'rationality-testing', 'sequences-and-series']"
2218805,Show that $\sum\limits_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}=2n+\frac23+o(1)$,"When modeling the average queue length of an M/D/1/K queue, we encountered the expression $$A_n=\sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i}$$ Empirically, $$A_n\approx2n + \frac{2}{3}$$ for large $n$, and at least for $n > 5$. For instance, $$A_6=12.66666714138\qquad A_{10}=20.66666666648$$ These values are too neat to be a mere coincidence, but we are not able to explain them. Mathematically, the result to be proven is
$$\lim_{n\to\infty}\ (A_n-2n)=\frac23$$ that is, $$\lim_{n\to\infty} \left( \sum_{i=0}^n \frac{(-1)^i}{i!}(n-i)^ie^{n-i} - 2n\right) = \frac{2}{3}$$ Any pointers on how to prove this would be greatly appreciated.","['real-analysis', 'summation', 'limits']"
2218831,How to Solve this Particular Euler Differential Equation?,"I was asked to solve an Euler ODE: Here's my work so far: I'm asked to solve the following Euler DE using the method of variation of parameters:
  $$x^2y''-y=(x^2+1)\sin{x}$$
  I already determined the homogeneous solution which resulted in:
  $$y_h=c_1 x^{\frac{\sqrt{5}+1}{2}}+c_2 x^{\frac{-\sqrt{5}+1}{2}}$$
  I'm having trouble finding the particular solution though...
  $$y_p=u_1 y_1+u_2 y_2$$
  $$u_1=-\int \frac{R(x)\cdot y_2}{W}~dx \qquad \qquad  W=\text{wronskian}=-\sqrt{5}$$
  $$u_1=\frac{1}{\sqrt{5}}\int (x^2+1)\sin{x}\cdot x^{\frac{-\sqrt{5}+1}{2}}~dx$$
  But this integral is too complicated for me to solve. I computed it and found that it gives a gamma function and a really long answer. Is there anything I missed that could simplify this problem?",['ordinary-differential-equations']
2218867,what is the domain of $\sqrt[3]{x}$,Find the Domain of $\sqrt[3]{x}$ Can we take negative numbers in domain? Because $F(x)=x^3$ and $g(x)=\sqrt[3]{x}$ are inverse of each other so their graphs are symmetric about $y=x$. But my book gave graph of $\sqrt[3]{x}$ only in first quadrant? Any reasons,"['polynomials', 'functions']"
2218886,"If a f's restriction to D is measurable and E~D is measurable, then f is measurable on E","I am trying to prove the following proposition: Let f be an extended real-valued function on E. Let D be a measurable subset of E. (i) If f is measurable on E and f = g a.e. on E, then g is measurable on E. (ii) f is measurable on E if and only if it's restriction to D and E~D are measurable. And I can do it, until I reach 2(ii) going ""<-"". How should I go about it?
I found this answer: $f$ is measurable iff its restrictions are measurable but the thing is he is only going in one direction and not the other. I feel like the proof should be trivial because even Royden's skip over it, but I can't figure it out...","['measurable-functions', 'lebesgue-measure', 'measure-theory']"
2218917,Can the graph of $\tan^{-1}{\left(\frac{\sin x}{x}\right)}$ be expressed as $Ce^{-kx}\cos(\omega x + \phi)$?,"After graphing $\sin x$ , I thought of trying something interesting. I wanted to plot the angle $\theta$ that a point $(x,\sin x )$ makes with the origin on the $y$ -axis, against $x$ on the $x$ -axis. $$\tan\theta = \frac{\sin x}{x}\Rightarrow \theta=\tan^{-1}{\left(\frac{\sin x}{x}\right)}$$ Graphing $y = 20\times\theta$ (multiply by 20 for graphical purposes): Part of it reminded me of the graph for the damped oscillator (specifically, the $x>0$ part).
That made me wonder if it was possible to find constants $C,k,\omega,$ and $\phi$ such that $$\theta = Ce^{-kx}\cos(\omega x + \phi)$$ However, after toying with Grapher for a while, $y = \theta$ didn't seem to decrease exponentially. That led me to this question: is there any analytical way to find real constants $C,k,\omega,$ and $\phi$ such that $\theta = Ce^{-kx}\cos(\omega x + \phi)$ ? Furthermore, are there any complex constants $C,k,\omega, \text{and } \phi$ ?","['real-analysis', 'exponential-function', 'trigonometry', 'complex-analysis', 'graphing-functions']"
2218936,Closed form bijection between integers and pairs thereof,"I know that it's simple enough to map the integers, $\mathbb{Z}$, to pairs of integers, $\mathbb{Z}^2$, in a bijective way (i.e. a one-to-one mapping). You can wrap the integers around the origin of the 2D Cartesian grid like a spiral, or you can use some space filling curve, or you can map the integers individually to naturals by ""folding up"" the real line and then use a common bijection between naturals, like the Cantor pairing function (which is what the answer to this question does). But so far, I haven't been able to come up with a bijection that has a simple closed form. I know you can express that ""folding up"" operation using $\mathrm{sgn}$ and $\lfloor\cdot\rfloor$, but those seem inelegant, and I'm looking for something that feels more ""natural"" (no pun intended) to the domain of $\mathbb{Z}$ than mapping them to naturals first, applying a bijection there, and mapping the result back to an integer. For the positive naturals, there's this very elegant map: $$ f(a,b) = 2^{a-1}(2b-1) $$ Extending $b$ to the integers is not a problem, but then a) we can't cover $0$ with this, b) we've already covered all other integers, so there's no room left if we want to extend $a$ to the integers as well (which would require changes anyway, since we can't just work with negative powers of $2$ here). I also can't find anything like a list of common bijections between integers and pairs thereof, I suspect because most mathematicians stop caring after any one bijection has been found. So my question is, are there any bijections between $\mathbb{Z}$ and $\mathbb{Z}^2$ which can be expressed in a simple closed form, akin to $f$ above, without reducing the problem to the naturals? I'd take a bijection where only one direction of the mapping has a closed form, but if both directions can be expressed in closed form that would be preferable. Bijections that are formulated in terms of the Gaussian integers, $\mathbf{Z}[i]$, instead of $\mathbb{Z}^2$ would also be acceptable.","['gaussian-integers', 'functions', 'closed-form', 'integers', 'elementary-number-theory']"
2218967,"Any sets A and B, P(A ∪ B) = P(A) ∪ P(B)","Here P is the power set.
Can some show or explain to me how to do this? 
I have failed trying to prove it using laws for set equivalence, it seems my method is wrong.",['elementary-set-theory']
2218980,Iterated function derivative,"Suppose that we have a real valued function $f(x)$ and  define the function $g(x)$ such that $g(g(x))=f(x)$. What is the derivative of $g(x)$? And there is some rule that express this derivative in terms of the derivative of $f(x)$? It seems a simple question but, searching on the web, I found references to the functional square root of a function (which refer to the Shroeder's functional equation for generalizations), but noting about the derivation of such function. Someone knows some useful reference to this topic and, more in general, to fractional iteration of functions?","['reference-request', 'functions', 'functional-equations']"
2218985,ODE when the nonhomogeneity is complex,"How should I use the method of undetermined coefficients to solve the following differential equation? I know how to do it when the nonhomogeneity is real, such as polynomial(t) * Sin(t), but I am not sure what to do when the right-hand side is complex. $${\frac {d^{2}y}{dt^{2}}}+2{\frac {dy}{dt}}+4y= \sqrt{3} e^{-t+\sqrt{3}it}$$","['ordinary-differential-equations', 'complex-numbers']"
2218997,Does the Radon-Nikodym chain rule imply the usual chain rule?,"If I understand correctly, for signed measures $\nu,\mu$ and $\lambda$ on a common measurable space, if $\nu$ and $\mu$ have enough null sets, then $$\frac{d\nu}{d\lambda} = \frac{d \nu}{d\mu}\frac{d \mu}{d\lambda},$$ $\lambda$-almost everywhere. This looks suspiciously like the Leibniz chain rule, except that, well, the Leibniz chain rule doesn't make too much sense to me, whereas the above formula is perfectly clear. Anyway, I was wondering if its possible to recover the Langrange-notation version of the chain rule from single-variable calculus, namely $$(g \circ f)' = (g' \circ f) \cdot f',$$ from the above formula.","['chain-rule', 'measure-theory', 'calculus']"
2219001,"""Smooth"" tensor field","I am learning differential geometry and I have a question on a definition. We call a tensor field a function that to a point on the manifold ($p\in M$), we associate smoothly a tensor. But what does smoothly mean here? In the book we defined a smooth function on a manifold $M$ to another $N$ as a function $f : M \rightarrow N $ which verifies : $ \phi \circ f \circ \psi^{-1} $ is a $C^{\infty}$ function from $\mathbb{R}^m$ to $\mathbb{R}^n$ (where $m$ and $n$ are the dimensions of the manifold $M$ and $N$ and $\phi$ and $\psi$ maps functions on $M$ and $N$. But a tensor is not a function $f : M \rightarrow N $.",['differential-geometry']
2219028,"Point of dense orbit under the action of the tent map on $[0,1]$","Consider the tent map $f:[0,1] \to [0,1]$, defined by $f(x)=2x$ if $x<0.5$, or $x =0.5$ and $f(x)=2-2x$ if $x>0.5$. Describe a point in $[0,1]$ whose orbit under $f$ is dense in $[0,1]$. This is a question which is part of a course in fractal geometry and chaotic dynamics. I realise that the functions in this equation are the inverse of the equations in an iterated function system which maps $[0,1]$ onto itself (i.e., $[0,1]$ is the attractor). However, I'm not quite sure how to find the points with periodic orbits, and I'm not sure if it is best to examine the original equation or the iterated functions system to find the points.","['real-analysis', 'chaos-theory', 'dynamical-systems', 'analysis']"
2219032,Examples of cardinals such that $a>b$ and $a<a^b$,"In a test where the topic was cardinal arithmetic, one student claimed that $a^b=a$ for any infinite cardinals such that $a>b$ (and used this claim in some computations). It is clear that $a\le a^b$, but it is possible that the inequality is strict. Coincidentally, the student got the correct result, since all cardinals in this problem were beth numbers and for such numbers you cannot get a counterexample. (If $a=2^c$ where $c\ge b$, then we get $a^b=2^{c\cdot b} = 2^c = a$.) The simplest counterexample I was able to come up with was 
$$\aleph_\omega<\aleph_\omega^{\aleph_0}.$$
Indeed we have $\operatorname{cf}(\aleph_\omega)=\aleph_0$, so we get from König's theorem that
$$\aleph_\omega<\aleph_\omega^{\operatorname{cf}(\aleph_\omega)}=\aleph_\omega^{\aleph_0}.$$ Are there some other simple (interesting, insightful) counterexamples to this claim? I.e., some other examples of infinite cardinals such that $$a>b\text{ and }a<a^b.$$ Some comments on an example I suggested - for example, if it can be simplified or generalized - are welcome as well. (To make things easier, let us work in ZFC; i.e., we assume that Axiom of Choice holds.)","['inequality', 'examples-counterexamples', 'cardinals', 'elementary-set-theory']"
2219063,An $n\times n$ matrix $A$ such that $PA^T = AP$,"Suppose that I have an $n\times n$ real matrix $A$ such that $PA^T = AP$ for every invertible matrix $P$. Does this imply that $A$ is a multiple of the identity matrix (i.e., $A= c\, I_{n\times n}$ for some $c\in \mathbb{R}$.) ?","['matrices', 'linear-algebra']"
2219086,Understanding finite morphisms,"Let $f:Y\to X$ be a surjective finite morphism between Noetherian schemes. The definition of a finite morphisms requires $f^\#_U:\mathcal{O}_X(U)\to \mathcal{O}_Y(f^{-1}(U))$ to be a finite ring homomorphism, for every affine open $U\subset X$. Does that imply $f^\#_U:\mathcal{O}_X(U)\to \mathcal{O}_Y(f^{-1}(U))$ is a finite ring homomorphism for an arbitrary open $U\subset X$, and in particular for $U=X$? If not, is there some mild conditions under which these homomorphism become finite? References or counterexamples would be appreciated.","['schemes', 'algebraic-geometry']"
2219101,If $f$ is even and $\pi$-periodic then $\int_0^\infty f(x)\frac{\sin{x}}{x}\ dx=\int_0^{\pi/2} f(x)\ dx$,"In Gradshteyn & Ryzhik's Tables of integrals, series, and products (1996) it is stated without proof that If $f$ is an even, $\pi$-periodic function then $$\int_0^\infty f(x)\frac{\sin{x}}{x}\ dx=\int_0^{\pi/2} f(x)\ dx$$ assuming that the LHS integral exists. I tried to prove this by breaking up the integral over $[0,\infty[$ into intervals of length $\pi$ and summing to infinity but I could not get any further than finding: $$\int_0^\infty f(x)\frac{\sin{x}}{x}\ dx=\sum_{n=0}^\infty\int_0^\pi f(x)\frac{(-1)^n\sin{x}}{n\pi+x}\ dx$$ which doesn't seem too fruitful. My gut tells me that Fourier transforms should get involved at some point. Does anyone have a proof or a sketch of a proof?","['real-analysis', 'integration', 'fourier-analysis']"
2219121,"How to prove $AB = \{xy \in \mathbb R : x \in A, y \in B\}$ is an open set?","Let $A,B \subset \mathbb R$, $AB = \{xy \in \mathbb R : x \in A, y \in B\}$.
If A and B are open set, would AB be an open set? I understand this question graphically or intuitively, but I'm confused how to prove it in detailed way. Can anybody give me an idea?","['general-topology', 'analysis']"
2219125,Prove using combinatorics $\sum\limits_{r=0}^n2^{n-r}*\binom{n+r}{n}=2^{2n}$,Prove using combinatorics $\sum\limits_{r=0}^n2^{n-r}*\binom{n+r}{n}=2^{2n}$ The right side is choosing some persons from $2n$ people  but I can't find a way to explain the left side it has both combination and power of $2$.Any hints?,['combinatorics']
2219143,Name for affine functions which only shift and scale,"Fix a field $\mathbb K$. Is there a name for functions $f: \mathbb K^n \to \mathbb K^n$ given by
$$
f(x) = \alpha x + b,
$$
where $b \in \mathbb K^n$ and $\alpha \in \mathbb K$ (so $\alpha$ is a scalar, not a matrix)?","['terminology', 'linear-algebra', 'functions']"
2219163,sum of two determinants,"If \begin{vmatrix}a&c\\b&d\end{vmatrix} and \begin{vmatrix}a&e\\b&f\end{vmatrix}, then sum of these determinant can be written as in terms of another determinant given by
    \begin{vmatrix}a&c+e\\b&d+f\end{vmatrix} is it right?",['matrices']
2219177,"If $f$ is continuous, and $f(x) \rightarrow 2$ as $x \rightarrow \infty$ then $f$ is uniformly continuous","I recently asked for some direction with how to connect a convergent function to proving it is uniformly continuous and I was directed to an awesome post that explained a lot for me. So now I tried to prove this problem which pretty much asks a specific case of that and I was wondering if some people could look it over for errors. Suppose $f$ is continuous on $[0, \infty)$ and $f(x) \rightarrow 2$ as $x \rightarrow \infty$. Prove $f$ is uniformly continuous on $[0, \infty)$. Since $f(x) \rightarrow 2$ as $x \rightarrow \infty$, then for $\epsilon>0, \exists k $ s.t. $ \forall x \geq k$ we have $|f(x) - 2| < \frac{\epsilon}{2}$. Also, note that since $f$ is continuous on $[0, \infty)$ it is uniformly continuous on $[0, k+2]$ and so for $\epsilon >0$, $\exists \delta_1 > 0$ s.t. if $x,y \in [0, k+2]$ and $|x-y| < \delta_1$ then $|f(x) - f(y)| < \epsilon$. Now given $\epsilon >0$, take $\delta = \min(\delta_1, \frac{1}{2}) > 0$. Suppose $x,y \in [0, \infty)$ and $|x-y| < \delta$. Then if $x > k + 1$, since $|x-y| < \frac{1}{2}$ we have $k+1 - y < x - y < \frac{1}{2}$ and so $y > k$. Together, we have $x, y > k$ and $|x-y| < \delta$, so $|f(x) - f(y)| = |(f(x) - 2) + (-f(y) + 2)| \leq |f(x) -2| + |f(y) - 2| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$. If $x \leq k + 1$, since $|x - y| < \frac{1}{2}$, then we have $y < k + \frac{3}{2} < k+2$. Together then, we have $x, y \in [0,k+2]$ and $|x-y| < \delta \leq \delta_1$ and so $|f(x) - f(y)| < \epsilon$. Thus we have shown for $\epsilon >0$, there is a $\delta > 0$ s.t. if $x,y \in [0, \infty)$ and $|x-y| < 0$, then $|f(x) - f(y)| < \epsilon$.","['uniform-continuity', 'real-analysis', 'analysis', 'proof-verification']"
2219195,Integral over space of matrices,"Let $\Omega_n=\{Y\in GL_n(\mathbb{R}):Y^t=Y>0\}$ be the set of symmetric positive definite real square matrices of size $n$, and let $dY=\prod_{1\le i\le j\le n}dY_{ij}$. Let $F:\Omega_n\to\mathbb{R}$ be any function such that $F(BAB^t)=F(A)$ for any orthogonal $B\in O_n(\mathbb{R})$, of which I have to compute the integral
$$I:=\int_{\Omega_n}F(Y)dY$$
(assuming such integral makes sense) and I want to make a change of variables. Let $\Gamma_n=\{diag(\lambda_1,\dots,\lambda_n):0<\lambda_1<\dots<\lambda_n\}$. Most of the $Y\in\Omega_n$ (exactly those with distinct eigenvalues) can be diagonalized in a unique way to a $\gamma\in\Gamma$. Is it true that the subset of $\Omega_n$ consisting of matrices with at least two coincident eigenvalues has measure $0$ (with respect to $dY$)? I think so, since the condition forces such subset -informally speaking- to be of lower dimension than $\Omega_n$. If that's the case, what is the correct formal argument? Provided this is the case, let $\Omega_n^+$ be the subset of matrices with distinct eigenvalues, so that we can integrate over $\Omega_n^+$ rather than $\Omega_n$. Every $Y\in\Omega_n^+$ can be diagonalized as $Y=U\gamma U^t$ for a unique $\gamma\in\Gamma_n$ and some orthogonal $U\in O_n(\mathbb{R})$. Further, $U$ is unique up to right multiplication by a matrix of the form $diag(\pm 1,\dots,\pm 1)$ (of which we have exactly $2^n$ choices). By changing variables, I ideally want to express my original integral $I$ (using the hypothesis $F(BAB^t)=F(A)$)as
  $$I=\int_{\Omega_n^+}F(Y)dY=\int_{O_n(\mathbb{R})/diag(\pm 1,\dots,\pm 1)}\int_{\Gamma_n}F(\gamma)|J(\gamma,U)|dU d\gamma$$
  where $J(\gamma,U)$ is the Jacobian determinant. What are the correct measures $dU$ and $d\gamma$? I think that $d\gamma=d\lambda_1\cdots d\lambda_n$, while I have no idea about $dU$. I have read on a paper that $J(\gamma,U)$ turns out to depend only on $\gamma$ (no explanation why), in which case I can further simplify 
  $$I=\int_{O_n(\mathbb{R})/diag(\pm 1,\dots,\pm 1)}dU \cdot \int_{\Gamma_n}F(\gamma)|J(\gamma)|d\gamma$$
  Is this correct? In this case, is the first integral convergent?","['matrices', 'integration']"
2219205,"Prob. 19, Chap. 4 in Baby Rudin: Any real function on $\mathbb{R}$ whit the intermediate-value property for which ... is continuous","Here is Prob. 19, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function with domain $\mathbb{R}^1$ which has the intermediate value property: If $f(a) < c < f(b)$, then $f(x) = c$ for some $x$ between $a$ and $b$. Suppose also, for every rational $r$, that the set of all $x$ with $f(x) = r$ is closed. Prove that $f$ is continuous. Hint: If $x_n \to x_0$ but $f\left( x_n \right) > r > f(x_0)$ for some $r$ and all $n$, then $f \left( t_n \right) = r$ for some $t_n$ between $x_0$ and $x_n$; thus $t_n \to x_0$. Find a contradiction. (N. J. Fine, Amer. Math. Monthly, vol. 73, 1966, p. 782.) My effort: Suppose $f$ satisfies the hypotheses in Prob. 19, Chap. 4 in Baby Rudin, but $f$ fails to be continuous at a point $p \in \mathbb{R}$. Then there is a sequence $x_n$ of real numbers such that $$x_n \to p, \ \mbox{ but } \  f\left( x_n \right) \not\to f(p) \ \mbox{ as } \  n \to \infty.$$ Thus, there is a positive real number $\varepsilon_0$ such that, for every natural number $N$, there is a natural number $n_N > N$ such that $$f\left( x_{n_N} \right) \not\in \left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right).$$
  Therefore there is a subsequence $\left\{ y_n \right\}$ of $\left\{ x_n \right\}$ the images $f\left( y_n \right)$ of each of whose terms are outside the segment $\left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right)$. So there is a subsequence $\left\{ z_n \right\}$ of $\left\{ y_n \right\}$ such that
  $$f\left( z_n \right) \leq  \ f(p)-\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}$$ 
  or $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$ 
  Let's assume, without any loss of generality, that 
  $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$ 
  Let $r$ be a rational number such that $$ f(p) < r < f(p) + \varepsilon_0. \ \tag{1} $$ 
  Then we see that 
  $$f\left( z_n \right) > r > f(p) \ \mbox{ for all } n \in \mathbb{N}.$$
  Now as $\left\{ x_n \right\}$ converges to $p$, so $\left\{ z_n \right\}$ also converges to $p$. Now as $f$ satisfies the intermediate value property, so, for each $n \in \mathbb{N}$, there is a point $t_n$ between $z_n$ and $p$ such that $$ f\left( t_n \right) = r. $$ 
  Then the sequence $\left\{ t_n \right\}$ must also converge to $p$. But $\left\{ t_n \right\}$ is a sequence in the closed set $$ f^{-1} \left( \{ r \} \right) = \left\{ \ x \in \mathbb{R} \ \colon \ f(x) = r \ \right\}.$$
  So the point $p$ must also belong to this set, which implies that $f(p) = r$, which contradicts (1) above. Hence any function $f$ which satisfies the all of the hypotheses of Prob. 19, Chap. 4 in Baby Rudin, 3rd edition, must also be continuous on all of $\mathbb{R}^1$. Is my proof correct? If so, have I correctly used the hint given by Rudin? If not, then where have I gone wrong?","['continuity', 'real-analysis', 'analysis']"
2219227,How to calculate this determinant? [duplicate],"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . How to calculate this determinant? $$A=\begin{bmatrix}n-1&k&k&k&\ldots& k\\k&n-1&k&k&\ldots &k\\\ldots&\ldots&\ldots &&\ldots\\\\k&k&k&k&\ldots &n-1\\
\end{bmatrix}_{n\times n}$$ where $n,k\in \Bbb N$ are fixed. I tried for $n=3$ and got the characteristic polynomial as $(x-2-k)^2(x-2+2k).$ How to find it for general $n\in \Bbb N$?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'determinant']"
2219231,What is $\lim_{n\to\infty} \frac{x_n}{y_n}$ provided $x_n=2y_{n}-y_{n-1}$ and $y_n=3y_{n-2}-y_{n-2}$ with $x_0=y_0=1$?,"Suppose $x_n$ and $y_n$ satisfy:
$$
        \begin{pmatrix}
        x_n \\
        y_n \\
        \end{pmatrix}=\begin{pmatrix}
        2&1 \\
        1&1 \\
        \end{pmatrix}\begin{pmatrix}
        x_{n-1} \\
        y_{n-1} \\
        \end{pmatrix} $$ 
with $x_0=y_0=1$. What is $\displaystyle\lim_{n\to\infty} \frac{x_n}{y_n}$ assuming this limit exists? After doing some algebra, I've got $x_n=2y_{n}-y_{n-1}$ and $y_n=3y_{n-2}-y_{n-2}$, but cannot go further.","['sequences-and-series', 'linear-algebra', 'calculus', 'limits']"
2219267,Is the Haar measure on $\mathbb{Q}_p$ complete?,"The field of $p$-adic numbers $\mathbb{Q}_p$ is locally compact, and so there exists a Haar measure on $(\mathbb{Q}_p,+)$. My question is whether a Haar measure on $\mathbb{Q}_p$ will also be a complete measure? The reason this question came to my mind is that I am studying about adeles and I noticed that we take the Haar measure on the infinite place, $\mathbb{R}$, to be the Lebesgue measure and not the Borel measure restricted to the Borel sets. So I wondered whether the Haar measure on $\mathbb{Q}_p$ is also complete, but I couldn't find any way to answer the question because we simply assert the existence of Haar measures on $\mathbb{Q}_p$ and move on to computing integrals with respect to this measure.","['adeles', 'measure-theory', 'haar-measure', 'p-adic-number-theory']"
2219272,"If $0$ is the only eigenvalue of a linear operator, is the operator nilpotent","In a finite dimensional vector space, if $0$ is an eigenvalue and the only eigenvalue of a linear operator, is that operator nilpotent? There is this post which shows the other direction. Prove that the only eigenvalue of a nilpotent operator is 0? I would think the question would be posed as ""iff"" to the extent the answer to my question is affirmative. To the extent that is not the case, I would please appreciate an example to that effect. Thanks","['linear-algebra', 'linear-transformations', 'nilpotence']"
2219285,Do all parallelizable manifolds admit a flat metric?,A simply connected manifold $M$ admits a flat metric/connection if and only if $M$ is parallelizable. What happens/can happen if $M$ is notsimply-connected? Definitions: https://en.wikipedia.org/wiki/Parallelizable_manifold https://en.wikipedia.org/wiki/Simply_connected_space,"['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
2219296,"What's the intuition behind ""representable morphisms""?","A central notion in many algebro-geometrical stuff appears to be so-called ""representable morphisms"". A general (read: hand-wavy) definition could be the following, as far as I can tell: Let $\mathsf{C}$ be a site, and let $f : F \to G$ be a morphisms of sheaves over $\mathsf{C}$. Then $f$ is representable by an object of type $T$ if, for all representable sheaves $h_X$ and all morphisms $h_X \to G$, the fiber product $F \times_G h_X$ is of type $T$. (Here the type $T$ could be ""affine scheme"", ""scheme"", ""algebraic space"", ""stack"", ""manifold"", ""differentiable manifold"", ""open subset of $\mathbb{R}^n$""...) I'm more used to topology, so of course my first instinct was to try and see what this meant for topological spaces. As far as I can tell, a continuous map $f : X \to Y$ is representable by an open subset of $\mathbb{R}^n$ (resp. a manifold) iff for all open subsets of $Y$ that are homeomorphic to an open subset of $\mathbb{R}^n$, then $f^{-1}(U)$ is an open subset of $X$ homeomorphic to an open subset of $\mathbb{R}^n$ (resp. to a manifold). Honestly, this isn't very enlightening... I can follow the definition and understand how the ""representability"" condition is used, but I have no real intuition for it. What does it mean? Surely the name wasn't chosen randomly, so if a morphism is represented by (say) a scheme, then what scheme is that, and in what sense does it represent $f$?","['intuition', 'sheaf-theory', 'algebraic-geometry']"
2219299,How to know if equation adds more info to a set of equations?,"I'm looking for a formalized way to be able to know whether adding an equation to a set of linear equations adds new information or not, does anyone know of a way? For a really simple case, let's say I have these equations: $A = 3\\
B+C = 4\\
D = 8
$ Obviously, the equation below doesn't add any new information: $-A = -3$ But it gets less obvious when adding an equation like this: $B + D = 6$ Or: $C = 2$ When the list of equations is longer, it gets a lot less obvious to me as well: $
A=3\\
B+C=4\\
D=8\\
C=4\\
F=3\\
E=12\\
F+G=4\\
H=1
$ If I want to add a new equation: $D+E=7$ I can see that $D$ and $E$ are already fully specified, so this doesn't really add any new information, but a more complex expression would be harder for me to rule out. Is there a formalized way to see whether or not adding a new equation will add more information?  Possibly by looking at the matrix form of the equations? Here is the last group of equations: $
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
* 
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
F \\
G \\
H \\
\end{bmatrix}
=
\begin{bmatrix}
3 \\
4 \\
8 \\
4 \\
3 \\
12 \\
4 \\
1 \\
\end{bmatrix}
$ And here is the equation I wanted to add to the set: $
\begin{bmatrix}
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
\end{bmatrix}
* 
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
F \\
G \\
H \\
\end{bmatrix}
=
7
$ Thanks for any help you can provide!","['linear-algebra', 'systems-of-equations']"
2219302,Are $\pi$ and $\ln(2)$ linearly independent over rational numbers? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Are $\pi$ and $\ln(2)$ linearly independent over rational numbers? Are there any proofs either way, or partial results?","['number-theory', 'abstract-algebra']"
2219345,Complex modulus of $\left|\frac{-3z+2i}{2iz+1}\right|$ given that $\left|z\right|=\frac{1}{\sqrt3}$,"The following question was on my first year algebra exam way back in 1989. If $\left|z\right|=\frac{1}{\sqrt3}$, then find $\left|\frac{-3z+2i}{2iz+1}\right|$. I couldn't figure it out then, and 28 years on, I still can't. It was only worth 4 marks, so the solution must be simpler than all the things I have tried over the years to no avail.","['complex-analysis', 'complex-numbers', 'mobius-transformation']"
2219385,$\lim_{k \to \infty} \frac{x_k}{k^2}$,"Let $(x_k)$ be the sequence of real numbers defined as follows: $x_1=1$; $x_{k+1}=x_k+\sqrt {x_k}$ for $k>0$. Find $$\lim_{k \to \infty} \frac{x_k}{k^2}$$ My thought is to find some $y_k$ which is a function of $x_k$ and find the limit of this $y_k$, but I'm not sure if this will work, or what my $y_k$ should be.","['real-analysis', 'sequences-and-series', 'limits']"
2219398,The quotient of the euclidean division,"$\text{Let } n,\ a_1,\ a_2, \ldots, \ a_n \text{ be natural numbers, such that:} $ $$n = q_1a_1 + r_1, 0\le r_1<a_1$$
$$q_1 = q_2a_2 + r_2, 0\le r_2<a_2$$
$$\vdots$$
$$q_{k-1} = q_ka_k + r_k, 0\le r_k<a_k$$ I have to prove that $$\begin{cases} 
\hfill q_k = \left\lfloor\frac{n}{\prod_{i=1}^{k} a_i}\right\rfloor \\
\hfill \left\lfloor \frac{n}{\prod_{i=1}^{k} a_i} \right\rfloor = \left\lfloor\frac{\left\lfloor\frac{n}{\prod_{i=1}^{k-1} a_i}\right\rfloor}{a_k}\right\rfloor\\
\end{cases}$$ Could someone give me some hints how to prove that without using induction?",['number-theory']
2219411,Similarity Between Lie Algebra and Group Theory,"In this semester I'm taking a lecture about Lie algebra. I'm quite surprised because theory of Lie algebra has very similar structure as theory of groups. Here is some of my observations: Notion of nilpotent and solvable is almost same and their property is similar. Theory of representation is also similar. For example, Schur's Lemma holds for representation of groups and Lie algebras. Classification of Lie algebras and Finite groups has similar parts, like exceptional groups of Lie type and some matrix groups. They are just some observations so it can be meaningless set of phenomenons. But, do you know any systematic reason of this phenomenon, then please tell me something. I have just naive explanation: commutator of groups and Lie brakets has very similar properties. Maybe Lie algebra - Lie group correspondence has something to do with, but in my intuition finite dimensional Lie algebra is similar with finite groups and Lie groups are infinite, so it may not be ultimate answer. Is there any systematic reason?","['finite-groups', 'group-theory', 'lie-algebras']"
2219417,"When $x$ is divided by $y$, and $x$ is not divisible by $y$, what is the maximum number of decimal places?","Somewhere (I will provide reference once I remember where I got this statement from), I read that: ""When $x$ is divided by $y$ , and the resulting number is a decimal *, then the maximum number of recurring digits in the result is $(y-1)$ "" *Definition of decimal : A number in the form of $a$ , or in other words, a constant. Examples: 1.23 or 3.63 or 28... etc. I have found this true for many values of $\frac{x}{y}$ . Examples: $1/2 = 0.5$ The recurring digits is 5 $1/3 = 0.\overline3$ The recurring digit is 3 $1/7 = 0.\overline{142857}$ The recurring digits are 142857 $1/11 = 0.\overline{09}$ The recurring digits are 09. The statement is true at the moment because $2>1, 3>1, 7>6, 11>2$ Is there a good algebraic proof for this?",['number-theory']
2219432,Compute Flux (Divergence Theorem),"Given that $\textbf{F} =  \langle x\cos^2z,  y\sin^2z,  \sqrt{x^2+y^2}\:z \rangle $ and let E be the solid cone above the $xy$-plane and inside z = $1 -\sqrt{x^2+y^2}.$ I'm trying to use the divergence theorem to compute the flux.
$$\iint_{D} \textbf{F} \cdot \textbf{N} \: dS = \iiint_E \nabla \cdot \textbf{F}\:d\textbf{V}$$ Attempt: $$\text{div}\textbf{F} \:=\: \nabla \cdot \textbf{F} 
=\cos^2z + \sin^2z + \sqrt{x^2+y^2} \:
= z + r \:
$$ Since solid cone is above the $xy$-plane, $\text{z} \ge 0$ and $z = 1-\sqrt{x^2+y^2} = 1-r$. Hence $ 0 \le \text{z} \le 1-r$. Is this bound right for z? \begin{align*}
\iiint_E \nabla \cdot \textbf{F}\:d\textbf{V} &= \int_0^{2\pi} \int_0^{1} \int_0^{1-r} (z+r) r\: dz\: dr\: d\theta \\
&= 2\pi \int_0^{1} \left(\frac{z^2r}{2} + r^2z\right)\bigg\rvert_0^{1-r} dr \\
&= 2\pi \cdot \frac{1}{2} \int_0^1 r-r^3\, dr \\
&= \pi \left(\frac{r^2}{2}-\frac{r^4}{4}\right)\bigg\rvert_0^1 \\
&= \frac{\pi}{4}.
\end{align*} The answer given was $\frac{\pi}{2}.\:$ 
Perhaps I'm doing something wrong. One thing that I'm not sure of is picturing the surface that they're looking for. Also, not sure if my bounds are right. Where am I going wrong here? Would appreciate some help! Thank you.","['multivariable-calculus', 'surface-integrals', 'calculus']"
2219513,Rectifiable Jordan curve with bad polygonal approximations,"This question is inspired by this other question of mine. Is there an example of rectifiable Jordan curve $\gamma:[0,1]\to\mathbb R^2$ such that, for some $\delta>0$, no polygonal path $\overline{\gamma(t_0)\gamma(t_1)\cdots\gamma(t_n)}$ with $0=t_0<t_1<\cdots<t_n=1$ and $\max_{1\leq i\leq n}(t_i-t_{i-1})<\delta$, is a Jordan curve? According to this question (with no answer yet), such curve $\gamma$ cannot be piecewise regular.","['general-topology', 'plane-curves']"
2219524,"Prob. 23, Chap. 4, in Baby Rudin: Every convex function is continuous and every increasing convex function of a convex function is convex","Here is Prob. 23, Chap. 4, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: A real-valued function $f$ defined in $(a, b)$ is said to be convex if $$ f \big( \lambda x + (1- \lambda) y \big) \leq \lambda f(x) + (1-\lambda) f(y)$$ whenever $a < x < b$ , $a < y < b$ , $0 < \lambda < 1$ . Prove that every convex function is continuous. [Although this proof is available at Math SE, I would prefer a direct, $\varepsilon$ - $\delta$ proof.] Prove that every increasing convex function of a convex function is convex. [ How to?] (For example, if $f$ is convex, so is $e^f$ .) If $f$ is convex in $(a, b)$ and if $a < s < t < u < b$ , show that $$ \frac{ f(t)-f(s)}{t-s} \leq \frac{ f(u)-f(s)}{u-s} \leq \frac{ f(u)-f(t)}{u-t}.$$ [ How to? ] I would appreciate if the proofs are elementary (but rigorous enough for Rudin) and are only based on the machinary developed by Rudin up to this point in the book. Is every real convex function $f$ defined in $(a, b)$ also uniformly continuous? An afterthought: Here's my attempt: Let $x$ , $y$ , and $z$ be any three real numbers such that $$x < y < z.$$ Then we have $$0 < y-x < z-x,$$ which implies that $$0< \frac{ y-x}{z-x} < 1;$$ moreover if we put $$\lambda \colon= \frac{y-x}{z-x},$$ then we see that $$
\begin{align}
(1-\lambda)x+\lambda z &= \left( 1- \frac{y-x}{z-x} \right) x + \frac{y-x}{z-x} z \\
&= \frac{(z-y)x+(y-x)z}{z-x} \\
&= y.
\end{align}
$$ Thus we have shown that for any three real numbers $x$ , $y$ , and $z$ such that $x < y < z$ , we can write $y$ as $$y = (1-\lambda)x + \lambda z, \ \mbox{ where } \ \lambda = \frac{y-x}{z-x} \ \mbox{ and } \ 0 < \lambda < 1. \ \tag{0}$$ In what follows, we will be using this fact on several occasions. First, we show that $f$ is bounded on every closed interval $[ c, d] \subset (a, b)$ . Now let $t \in (c, d)$ , where $c$ and $d$ are any two real numbers such that $a < c < d < b$ . Then we can write $t$ as $$ t = (1- \lambda) c + \lambda d, \ \mbox{ where } \ \lambda = \frac{t-c}{d-c} \in (0, 1).$$ Then $$ 
\begin{align}
f(t) &= f \left( (1-\lambda)c+\lambda d \right) \\
&\leq (1-\lambda) f(c) + \lambda f(d) \\
&\leq (1-\lambda + \lambda ) \max \left\{ f(c), f(d) \right\} \\
&=  \max \left\{ f(c), f(d) \right\}.
\end{align}
$$ Thus $$f(t) \leq \max \left\{ f(c), f(d) \right\} \ \mbox{ for all } \ t \in [c, d] \ \tag{1}.$$ If $$c < t < \frac{c+d}{2},$$ then we can also conclude that $$t < \frac{c+d}{2} < d, $$ and so $$\frac{c+d}{2} = (1-\lambda) t + \lambda d, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2} - t}{d-t} = 
\frac{c+d-2t}{2(d-t)} \in (0, 1).$$ Then $$
\begin{align}
f\left( \frac{c+d}{2} \right) &= f \left( (1- \lambda) t + \lambda d \right) \\
&\leq (1-\lambda) f(t) + \lambda f(d) \\
&\leq f(t) + f(d),
\end{align}
$$ which implies that $$f(t) \geq f\left( \frac{c+d}{2} \right) - f(d) \ \mbox{ for all } \ t \in \left(c, \frac{c+d}{2} \right) 
\ \tag{2} $$ And, if $$\frac{c+d}{2} < t < d,$$ then we can also conclude that $$c < \frac{c+d}{2} < t,$$ and so $$\frac{c+d}{2} = (1-\lambda) c + \lambda t, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2}-c}{t-c} = \frac{d-c}{2(t-c)} \ 
\mbox{ so that } \ 0 < \lambda < 1.$$ Then $$
\begin{align}
f\left( \frac{c+d}{2} \right) &= f\left( (1-\lambda) c + \lambda t\right) \\
&\leq (1-\lambda) f(c) + \lambda f(t) \\
&\leq f(c) + f(t),
\end{align}
$$ which implies that $$f(t) \geq f\left( \frac{c+d}{2} \right) - f(c) \ \mbox{ for all } \ t \in \left( \frac{c+d}{2}, d \right) \ 
\tag{3}.$$ From (2) and (3) we can conclude that $$f(t) \geq \min \left\{ \ f(c), \ f(d), \ f\left( \frac{c+d}{2} \right) - f(c), \ f\left( \frac{c+d}{2} \right) - f(d), \ 
 f\left( \frac{c+d}{2} \right) \ \right\} \\
 \mbox{ for all } \ t \in [c, d] \ \tag{4} 
$$ From (1) and (4) we can comclude that, given any two real numbers $c$ and $d$ which satisfy $a < c < d < b$ , we can find a real number $M > 0$ such that $$ \left\vert f(t) \right\vert \leq M \ \mbox{ for all } \ t \in [c, d]. \ \tag{5} $$ Now let $\eta$ be a real number such that $$0 < \eta < \frac{d-c}{2}, \ \tag{6a} $$ and let $x$ and $y$ be any two real numbers such that $$ c +\eta <  x < y  < d-\eta. \ \tag{6b} $$ Thus, we have the following chain of inequalities: $$c < c+\eta < x < y < d-\eta < d, \ \mbox{ and } \ c+\eta < \frac{c+d}{2} < d-\eta. \ \tag{6} $$ Therefore we can conclude that $$ c < x < y < d, \ \tag{7}$$ and so $$
y = (1-\lambda) x + \lambda d, \ \mbox{ where } \ \lambda = \frac{y-x}{d-x} \ \mbox{ so that } \ 0 < \lambda < 1,
$$ and then $$
\begin{align}
f(y) - f(x) &= f\left( (1-\lambda) x + \lambda d \right) - f(x) \\
&\leq  (1-\lambda) f(x) + \lambda f(d) - f(x) \\
&= \lambda \left( f(d) - f(x) \right) \\
&\leq \lambda \left| f(d) - f(x) \right| \\
&\leq \lambda \left( \left| f(d) \right| + \left| f(x) \right| \right) \\
&\leq 2\lambda M \ \mbox{ [ by (5) above ] } \\
&= \frac{2M(y-x)}{d-x} \\
&< \frac{2M(y-x)}{\eta}. \ \mbox{ [ by (6b) above ] }  \ \tag{8a}
\end{align}
$$ And, from (7) we can also write $$x = (1-\lambda) c + \lambda y, \ \mbox{ where } \ \lambda = \frac{x-c}{y-c} \ 
\mbox{ so that } \ 0 < \lambda < 1, $$ and then $$
\begin{align}
f(x) - f(y) 
&= f\left( (1-\lambda) c + \lambda y \right) - f(y) \\
&\leq (1-\lambda) f(c) + \lambda f(y) - f(y) \\
&= (1-\lambda) \left( f(c) - f(y) \right) \\
&\leq (1-\lambda)  \left| f(c) - f(y) \right| \\
&\leq (1-\lambda) \left( \left| f(c) \right| + \left| f(y) \right|  \right) \\
&\leq 2(1-\lambda) M \ \mbox{ [ again by (5) above ]} \\
&= \frac{2M(y-x)}{y-c} \\
&< \frac{2M(y-x)}{\eta}. \ \mbox{ [ again by (6b) above ] } \ \tag{8b}
\end{align}
$$ From (8a) and (8b) we can conclude that $$\left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left(y-x \right) $$ whenever $a < c < d < b$ , $0 < \eta < \frac{d-c}{2}$ , and $c+\eta < x < y < d-\eta$ .
Therefore, interchanging the roles of $x$ and $y$ in the last result we can also conclude that,  whenever $a < c < d < b$ and $0 < \eta < \frac{d-c}{2}$ , we have $$\left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left\vert x-y \right\vert  \ \mbox{ for all } \ x, y \in (c+\eta, d-\eta). \ \tag{8} $$ Now let $p$ be any given point of $(a, b)$ , and let $\varepsilon$ be any positive real number.
We can choose some real numbers $c$ and $d$ such that $$a < c < p < d < b,$$ and then we can choose a real number $\eta$ such that $$0 < \eta < 
\min \left\{ \ \frac{d-c}{2}, \ p-c, \ d-p \ \right\}.$$ Then $$p \in (c+\eta, d-\eta);$$ that is, $$c+\eta < p < d-\eta.$$ Let us choose a real number $\delta$ such that $$ 0 < \delta < \min \left\{ \ \frac{\eta}{2M+1}\varepsilon, \ p-c-\eta, \ d-\eta-p \ \right\}.$$ Then any $x \in (a, b)$ which satisfies $\left\vert x-p \right\vert < \delta$ also belongs to $(c+\eta, d-\eta)$ and therefore by (8) above also satisfies $$
\begin{align}
\left\vert f(x) - f(p) \right\vert &\leq \frac{2M}{\eta} \vert x-y\vert \\
&\leq \frac{2M}{\eta} \frac{\eta}{2M+1}\varepsilon \\
&< \varepsilon.
\end{align}
$$ Hence $f$ is continuous at every point $p \in (a, b)$ . Is this proof correct? If so, then is my presentation good enough? If not, then where lie the flaws? Now let $f$ be a real convex function on $(a, b)$ , let $g$ be a real increasing convex function defined on a segment $(c, d)$ in $\mathbb{R}^1$ such that $$ f\left( (a, b) \right) \subset (c, d),$$ and let $h$ be the  function defined in $(a, b)$ as follows: $$h(x) = g\left(f(x) \right) \ \mbox{ for all } \ x \in (a, b).$$ We show that $h$ is convex. For this, let $x, y \in (a, b)$ and $\lambda \in (0, 1)$ . Then we note that $$
\begin{align}
h \left( (1-\lambda) x + \lambda y \right) &= g \left( f \left( (1-\lambda) x + \lambda y \right) \right) \\
&\leq g\left( \ (1-\lambda) f(x) + \lambda f(y) \ \right) \\
& \ \ \  \mbox{ [ because of the convexity of $f$, } \\
& \ \ \ \mbox{   we have $f \left( (1-\lambda) x + \lambda y \right) \leq (1-\lambda) f(x) + \lambda f(y)$ } \\ 
& \ \ \ \mbox{   and because $g$ is increasing ]} \\
&\leq (1-\lambda) g \left( f(x) \right) + \lambda g \left( f(y) \right) \\
& \ \ \ \mbox{ [ because of the convexity of $g$ ] } \\
&= (1-\lambda) h(x) + \lambda h(y).
\end{align}
$$ Hence $h$ is convex. Is the formulation of this result correct and general enough? If so, then is my proof (and the presentation thereof) good enough? If $a < s < t < u < b$ , then we can write $$t = (1-\lambda) s + \lambda u, \ \mbox{ where } \ \lambda = \frac{t-s}{u-s} \in (0, 1), $$ and then $$
\begin{align}
f(t) &= f\left( (1-\lambda) s + \lambda u \right) \\
&\leq (1-\lambda) f(s) + \lambda f(u) \\
&= \left( 1 - \frac{t-s}{u-s} \right) f(s) + \frac{t-s}{u-s} f(u) \\
&= \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u), \ \tag{9}
\end{align}
$$ and so $$ 
\begin{align}
\frac{f(t) - f(s)}{t-s}  &\leq \frac{1}{t-s} \left[ \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) - f(s) \right] \\
&= \frac{1}{t-s} \left[ \left( \frac{u-t}{u-s} - 1 \right) f(s) + \frac{t-s}{u-s} f(u) \right] \\
&= \frac{1}{t-s} \left[ \frac{s-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) \right]  \\ 
&= \frac{1}{t-s} \frac{t-s}{u-s} \left[ f(u) - f(s) \right]  \\ 
&= \frac{ f(u)-f(s)}{u-s}.
\end{align}
$$ Thus we have shown that if $a < s < t < u < b$ , then $$ \frac{f(t) - f(s)}{t-s}  \leq \frac{ f(u)-f(s)}{u-s}. \ \tag{10} $$ Now from (9) above, we obtain $$f(t) \leq \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u),$$ which upon dividing both sides by $u-t$ becomes $$
\begin{align}
\frac{ f(t)}{u-t} &\leq \frac{f(s)}{u-s} + \frac{t-s}{(u-s)(u-t)} f(u) \\
&= \frac{f(s)}{u-s} + \frac{(u-s) - (u-t) }{(u-s)(u-t)} f(u) \\
&= \frac{f(s)}{u-s} + \left( \frac{1}{u-t} - \frac{1}{u-s} \right) f(u) \\
&= \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t}.  
\end{align}
$$ Thus we have shown that $$\frac{ f(t)}{u-t} \leq \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t},$$ which implies that $$\frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t} \ \tag{11} $$ if $a < s < t < u < b$ . From (10) and (11), we conclude that if $a< s< t< u< b$ , then $$\frac{f(t) - f(s)}{t-s}  \leq  \frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t}, $$ as required. Is this proof correct? If so, then what is the presentation like? If there is (are) any problem(s) in this proof, then at what point?","['real-analysis', 'convex-analysis', 'continuity', 'uniform-continuity', 'analysis']"
2219533,Is there an example of an ordered ring that is not isomorphic to any subring of the real numbers?,"Somebody told me it's possible to show the existence of rings non-isomorphic to any subring of the real numbers using model theory, but they couldn't get an example.","['abstract-algebra', 'ring-theory', 'order-theory']"
2219539,Serre Twisting Sheaf Etymology,"Is there a reasonable and coherent explanation of why the Serre Twisting Sheaf has the word ""twisting"" in its name?",['algebraic-geometry']
2219544,What will be $((1 2 3 4)(1 5 7)^{-1}(2 4 3 6))^{111}$?,"Calculating the following permutation: $$((1 2 3 4)(1 5 7)^{-1}(2 4 3 6))^{111}$$ Well, I can write that $(1 5 7)^{-1}=(7 5 1)$ So :$$((1 2 3 4)(7 5 1)(2 4 3 6))$$ which is equal to: $$(1 2 3 4)^{111}(7 5 1)^{111}(2 4 3 6)^{111}$$ I can write the following: $$111 \equiv -1 (mod 4) \implies 111 \equiv -1 (mod 3) \implies (1 2 3 4)^{111}=(1 2 3 4)^{-1}=(4 3 2 1)$$
then, $$111 \equiv0( mod 3) \implies (1 2 3 4)^{0}=id $$ and $$ 111 \equiv -1( mod 4) \implies (2 4 3 6)^{111}=(2 4 3 6)^{-1}=(6 3 4 2)$$ so I get $$ (4 3 2 1)(6 3 4 2)$$ I'm not sure ,that my solution is correct, and how can I multiply $$ (4 3 2 1)(6 3 4 2)$$ in the very last step?","['permutations', 'discrete-mathematics']"
2219573,Statistical Manifold with Non-trivial Topology,"Let $(\Omega, E)$ be a measure space. An $n$-dimensional statistical model is then a tuple $(\Theta, \mathcal{M}, \Phi)$ where $\Theta \subseteq \mathbb{R}^n$ open, $\mathcal{M} = \{p_\theta := p(\cdot | \theta), \theta \in \Theta\}$ is the set of parameterized probability distributions on $\Omega$ and the mapping $\Phi: \Theta \to \mathcal{M}$ via $\theta \mapsto p(\cdot | \theta)$ is injective.$^1$ In this context, one refers to $\mathcal{M}$ as a statistical manifold, as the mapping $\Phi$ serves as a coordinate system for $\mathcal{M}$. Question : Can anyone give me an example of a statistical manifold with  non-trivial topology?  If this is not possible, can you give an explanation for why this is. Many thanks! $^1$ This description is taken from here .","['examples-counterexamples', 'differential-topology', 'probability-theory', 'probability-distributions', 'differential-geometry']"
2219592,Proving compactness of intersection and union of two compact sets in Hausdorff space.,"Let $(X,\tau)$ be a Hausdorff space. $A,B \subseteq (X,\tau)$ which are compact sets. Show that the intersection and the union of A and B are compact sets. Here is the what i did  so far but i think there are some points that is not fit, i can feel but can't figure out :) thanks advance  for guidince. Proof: Every compact subset of Hausdorff space is closed set. So, $A,B \subseteq X$ which are compact sets are closed sets. Thus $A \cup B$ and $A \cap B$ are  closed sets (By the previous topologicial theorem which is union and intersection of two closed sets are closed set.) As a result of this $X \setminus (A \cup B)$ and $X \setminus (A \cap B)$ are open sets (By the definition of closed set.) Let $\mathcal{U}$ is cover for $A \cup B$. Because of $X \setminus (A \cup B)$ is copen set, we can write a cover for the Hausdorff space which is given as
$X \subseteq \mathcal{U} \cup (X \setminus (A \cup B) ) $ .Thus, By being compact set, for every cover of X there exists a finite subcover of X. So difference of the finite subcover and  $X \setminus A \cup B$ gives us a finite subcover  of  $A \cup B$. Let $\mathcal{V}$ is cover for $A \cap B$. Because of $X \setminus (A \cap B)$ is copen set, we can write a cover for the Hausdorff space which is given as $ 
X \subseteq \mathcal{V} \cup (X \setminus (A \cap B) )
$.Thus, by being compact set, for every cover of X there exists a finite subcover of X. So difference of the finite subcover and  $X \setminus A \cap B$ gives us a finite subcover  of  $A \cap B$","['general-topology', 'compactness']"
2219639,Direct Proof Discrete Math,"Original Question: Show that if $n$ is an odd integer, then $n^2$ is odd. Possible Solution: Proof :
  Assume that $n$ is an odd integer. This implies that there is some integer $k$ such that $n = 2k + 1$. Then $n^{2} = (2k+1)^{2} = 4k^{2} + 4k + 1 = 2(2k^{2} + 2k) + 1$. Thus, $n^{2}$ is odd. Logical Questions: Why does the solution assume $n$ to be $2k + 1$ ? How do you know $n^{2}$ is odd based on $2(2k^{2}+2k)+1$ ? I don't see how $2k + 1$ for $n$ and $2(2k^{2}+2k)+1$ for $n^{2}$ means an odd integer. Some clarification would be helpful.",['discrete-mathematics']
2219683,"If $\int_{0}^{1}f=\int_{0}^{1}g=1$, then there exists an interval $I \subset [0,1]$ such that $\int_{I}f=\int_{I} g =\frac{1}{2}$","$f,g$ are integrable functions whose domain is $[0,1]$ and $\int_{0}^{1}f=\int_{0}^{1}g=1$. How do I show that there exists an interval $I$ such that $I \subset [0,1]$ and $\int_{I}f=\int_{I} g =\frac{1}{2}$? It seems as if I should apply Brouwer's fixpoint theorem but I don't see how I could do that. I tried using the $\int_{I}(f-g)$ function but that didn't lead anywhere. Could you help me?","['geometric-topology', 'general-topology']"
2219700,What is the position of the surviving mouse?,"I have this question that I think it may be very interesting to all maths' lovers. A cat caught $n$ (integer) mice and put them in line, numbered them from 1 to $n$, from left to right.
He starts eating every other mouse, starting with the mouse at the 1st position, i.e. 1, 3, 5 ... (all mice at the odd positions will be gone).
He then starts a new iteration, no matter if there is a surviving mouse at the end of the line, by going back to the left and eats every other mouse again, starting always with the first surviving mouse from the previous iteration. Until there is one mouse left. What is the position of the surviving mouse in the original sequence from 1 to n?","['recurrence-relations', 'discrete-mathematics']"
2219788,Exact Pole-Zero Cancellation in a SISO Transfer Function,"Pole-Zero cancellation is one of those phantoms in control which are often either sidestepped or only treated informally. Personally, I've never deeply understood the reasoning behind some of the rules--in fact, there isn't even really a consensus on what that reasoning is. For the purpose of this question, suppose $G$ is a SISO transfer function. Let $p$ be a pole which is exactly cancelled by a zero at the same location. We treat the system $G$ formally in the sense that we assume 100% fidelity to whatever we are trying to model. If you prefer to think of the system in terms of differential equations, this is equivalent to saying the differential equations perfectly model the system. The question is whether or not the system $\tilde{G}$ obtained from cancelling $p$ is distinguishable from $G$. My answer is no by the following reasoning. If $G$ has 100% fidelity then every system parameter is uniquely determined by $G$. It follows that if $H$ is another transfer function and $H(s) = G(s)$ for all $s$, then $H$ is mathematically indistinguishable from $G$ and by extension so too are their system parameters. As regard $G$ and $\tilde{G}$ we have
$$G(s) = \frac{s-p}{s-p}\tilde{G}(s)$$
by their definition. Clearly $G(s) = \tilde{G}(s)$ whenever $s\neq p$, so we need only check that $G(p) = \tilde{G}(p).$ To do so we may evaluate
$$
\lim_{s\rightarrow p}G(s) = \lim_{s\rightarrow p}\frac{(s-p)\tilde{G}(s)}{s-p},
$$
which by L'Hopital's rule is equivalent to
$$
\lim_{s\rightarrow p}\ (s-p)\tilde{G}'(s) + \lim_{s\rightarrow p}\ \tilde{G}(s) = \tilde{G}(p).
$$
This calculation finds $G = \tilde{G}$, showing the system before and after pole-zero cancellation are equivalent. I therefore conclude that exact SISO pole-zero cancellation under the assumption of 100% model fidelity has no affect on the system. What are the thoughts of others? Edit Stelios has made some thoughtful points regarding the nature of $p$ as a removable singularity. Unfortunately some of the conventions I've used also appear to have caused confusion, and I will clear them up presently. (1) When I write $G(p)$ I mean $\lim_{s\rightarrow p}G(s)$. I concede that it is unclear whether showing $\lim_{s\rightarrow p}G(s) = \lim_{s\rightarrow p}\tilde{G}(s)$ is equivalent to saying $G(s) = \tilde{G}(s)$ for all $s$, since, as Stelios has argued, it is unclear whether or not $G(p)$ can be assigned any meaning in this way. (2) Supposing that two transfer functions $G$ and $H$ can be assigned meaning for every $s$ in a domain which they share, it is no more than a basic set theoretic statement to define $G =H$ iff $G(s) = H(s)$ for all $s$. In fact, if $A$ and $B$ are sets and $f$ maps $A$ to $B$, then WLOG we may regard $f$ as a subset of $A \times B$ consisting of the points $(x,f(x))$ for $x \in D(f)$. If $g$ is another function and $D(g) = D(f)$, then if $g(x) = f(x)$ for each $x$ in their domain it follows that $f$ and $g$ describe the same subset of $A \times B$. This is surely enough to say $f = g$.","['signal-processing', 'complex-analysis', 'control-theory', 'linear-control']"
2219801,"$f: \mathbb{R^2} \to \mathbb{R}$ a $C^\infty$ function such that $f(x,0)=f(0,y)=0$ then exists $g$ such that $f(x,y)=xy\, g(x,y)$","I'm trying to solve the following exercise Let $f: \mathbb{R^2} \to \mathbb{R}$ a $C^\infty$ function such that $f(x,0)=f(0,y)=0$ $\forall x,y \in \mathbb{R}$ .
Then there exists a $C^\infty$ function $g:\mathbb{R^2} \to \mathbb{R}$ satisfying $f(x,y)=xy\, g(x,y)$ $\forall x,y \in \mathbb{R}$ I thought first about expanding $f$ using the taylor theorem, we would have $$f(x,y)=f(0,0)+f'(0,0)(x.y)+\frac{f''(0,0)(x,y)^2}{2!}+r(x,y),$$ but $f(0,0)=0$ and $f_x(0,0)=f_y(0,0)=0$ since $f$ is zero on the $x$ and $y$ axis. $f''(0,0)(x,y)^2=f_{xx}(0,0)x^2+f_{xy}(0,0)xy+f_{yy}(0,0)y^2$ , now we have a term that has a product of $x$ and $y$ . But I don't know how (and if it is possible) to proceed from here.","['multivariable-calculus', 'real-analysis', 'taylor-expansion', 'smooth-functions']"
2219802,Total number of possible combinations for a vector of variable length,"I am trying to understand the formula behind generalizing the computation of the number of unique combinations possible for a vector of length n. Say x=(a,b,c) with n  = 3, the number of possible combinations would be: [3 choose 1 (a,b,c) + 3 choose 2 (ab, ac, bc) + 3 choose 3 (abc) ]. While if n is 4, the number of possible combinations would be [4 choose 1 (a,b,c,d) + 4 choose 2 (ab, ac, ad, bc, bd, cd) + 4 choose 3 (abc, abd, bcd, acd) + 4 choose 1 (abcd)], and so on for increasing number of n. But what is the general formuation for these type of combinations? Thanks for any help! 
Fra","['combinations', 'statistics']"
2219809,non-linear ODE $tdy + \tan^2(t+y)dt = 0$,"I really have tried to solve this equation: That my shot: $\tan(y+t) = u\\
\sec^2(y+t).y = du/dt\\
1+\tan^2(y+t).y = du/dt\\
(1+u^2)y = du/dt$ This is a final equation: $t.dy/du + (1+u^2).y + u^2 = 0$ My problem is a independent variable $t$.",['ordinary-differential-equations']
2219831,Difference of vectors living in different tangent spaces,"I have a question about tangent vectors of manifolds. Imagine that I have a vector $V$ living in $T_pM$ and $W$ in $T_qM$. In my book it is written that the difference between those vectors is ill defined. I would like to really understand why. Indeed If my manifold has dimension $m$, $V$ and $W$ are vectors of same dimension so I could imagine to subtract them. I understood that it is because for example if I have the coordinates of $V$ in a given basis in $T_pM$ I would have no idea of the coordinates $V$ would have in $T_qM$ (because : how to associate a basis of $T_pM$ to a basis of $T_qM$). But if I take $M=\mathbb{R}^n$, we can compare vectors of different points. So what makes it work in $\mathbb{R}^n$ and not in any general manifold $M$ (we have no problem of associating basis here for example). I think that an answer to this last question would help me to visualise better things. PS : I'm a beginner in differential geometry so not too complex answers please :)",['differential-geometry']
2219832,Determining a matrix given the characteristic and minimal polynomial,"Let $p_a=(x-2)^2(x-7)^4x$ be the characteristic polynomial of the matrix $A$ and $(x-2)^2(x-7)x$ the minimal polynomial. Determine the matrix $A$. My work: I know the matrix has to be $7x7$ and in its diagonal it must have two $2$, four $7$ and one $0$, so: \begin{bmatrix}{}
    2&  &  &  & & & \\
    &  2&  &  & & &\\
    & & 7 &  & & &\\
   &  &  & 7 & & &\\
    &  &  &  & 7& & \\
    &  &  &  & & 7 &\\
    &  &  &  &  &  & 0\\  \end{bmatrix} I don't know how to follow, what information gives me the minimal polynomial?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
