question_id,title,body,tags
4897141,Does $ \frac{\sin(\theta-\alpha)}{\sin\alpha}=\frac{\cos(\theta+\gamma-\alpha)}{\cos(\gamma-\alpha)}$ have an analytical solution for $\alpha$?,"The equation is about $\alpha$ : $$ \frac{\sin(\theta-\alpha)}{\sin\alpha}=\frac{\cos(\theta+\gamma-\alpha)}{\cos(\gamma-\alpha)} \tag{1}\label{1}$$ where $\theta$ and $\gamma$ are given. I failed to transform it with identities and separate the $\alpha$ . This equation was derived when I was solving a geometrical problem for my project: $\mathrm{AB}$ is an object on the screen with its center labeled as $\mathrm{C}$ , and $\mathrm{O}$ represents for the observer, whose distance to the screen is $d$ . Given the visual angle $\theta$ that $\mathrm{AB}$ is set to subtend to the observer and its lateral offset angle $\gamma$ , I want to determine the size $l$ of $\mathrm{AB}$ . I will describe the process in case it helps. The $\alpha$ in $\eqref{1}$ , aka. $\angle{\mathrm{AOC}}$ , is an intermediate I used. In $\triangle\mathrm{OAC}$ , according to the Law of Sines, we have $$\frac{\mathrm{AC}}{\sin\alpha}=\frac{\mathrm{OA}}{\sin\angle\mathrm{OCA}} \tag{2}$$ Similarly in $\triangle\mathrm{OBC}$ , $$\frac{\mathrm{BC}}{\sin(\theta-\alpha)}=\frac{\mathrm{OB}}{\sin\angle\mathrm{OCB}} \tag{3}$$ Considering that $\mathrm{AC}=\mathrm{BC}$ and $\angle\mathrm{OCA}+\angle\mathrm{OCB}=\pi$ , we know that $$\frac{\mathrm{OA}}{\mathrm{OB}}=\frac{\sin(\theta-\alpha)}{\sin\alpha} \tag{4}\label{4}$$ On the other hand, $\triangle\mathrm{ODA}$ and $\triangle\mathrm{ODB}$ have a common right-angle side $\mathrm{OD}$ , so $$\frac{\mathrm{OA}}{\mathrm{OB}}=\frac{\mathrm{OD}/\cos(\gamma-\alpha)}{\mathrm{OD}/\cos(\theta+\gamma-\alpha)}=\frac{\cos(\theta+\gamma-\alpha)}{\cos(\gamma-\alpha)} \tag{5}\label{5}$$ Then bridging $\eqref{4}$ and $\eqref{5}$ will lead to $\eqref{1}$ . And it's obviously one step away from getting $l$ if $\alpha$ is determined.","['trigonometry', 'systems-of-equations', 'triangles']"
4897187,Proving that the function $f(x) := |x|^{\frac{\lambda - n}{p}} (1- \psi(x))$ satisfies two specific properties related with limits and supremums.,"Let $1 \leqslant p < \infty$ and $0 < \lambda < n$ , where $n \in \mathbb N$ is an arbitrary fixed integer that stands for the dimension of the euclidian space $\mathbb R^n$ . In everything that follows, I am dealing with the usual Lebesgue integral on $\mathbb R^n$ . Question. Let $\psi \in C_c^\infty(\mathbb R^n)$ be a function from the class $C^\infty(\mathbb R^n)$ with compact support such that $\chi_{B(0,1)} < \psi < \chi_{B(0,2)}$ . Moreover, in $\mathbb R^n$ define the function $$ f(x) := |x|^{\frac{\lambda - n}{p}}(1-\psi(x)), $$ for every $x \in \mathbb R^n \setminus \{0\}$ . Prove that $f$ satisfies the following properties: $$ \lim_{r \to \infty} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \neq 0 $$ and $$ \lim_{\xi \to 0} \, \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy = 0. $$ My attempt. I believe I was able to deal with $1.$ but I don't know how to proceed to prove $2.$ . For $1.$ , I've done the following: Take an arbitrary value $r > 2$ . We have that \begin{align*} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy &\geqslant r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} (1-\psi(y))^p \, dy  \\[.2cm]
                                   & > r^{-\lambda}\int_{B(0,r)}|y|^{\lambda - n} (1-\chi_{B(0,2)}(y))^p \, dy \\[.2cm] 
&= r^{-\lambda} \int_{B(0,r)} |y|^{\lambda -n} \, dy - r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy.
\end{align*} Now let us analyse each of the two last integrals separately. For the first one, we have that $$ \int_{B(0,r)} |y|^{\lambda - n} \, dy = c(n) \int_0^r t^{\lambda - 1} \, dt = c(n) \frac{r^\lambda}{\lambda}. $$ On the other hand, for the second one, noting that $r > 2$ we have that $B(0,r) \cap B(0,2) = B(0,2)$ , from which we obtain $$ \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy = \int_{B(0,2)} |y|^{\lambda - n} \, dy = c(n)\int_0^2 t^{\lambda - 1} = c(n)\frac{2^\lambda}{\lambda}. $$ Consequently, going back to the original calculations, it follows that $$ \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)}|f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} - c(n)r^{-\lambda} \frac{2^\lambda}{\lambda}.  $$ By taking the limit on both sides as $r \to \infty$ , we conclude that $$ \lim_{r \to \infty} \, \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} > 0,$$ which is enough to prove $1.$ Does anyone have any idea on how I can proceed to deal with property $2.$ ? Thanks for any help in advance.","['lebesgue-integral', 'solution-verification', 'functional-analysis', 'limits', 'supremum-and-infimum']"
4897196,Probability of a random cyclic quadrilateral enclosing a fixed point in its circle,"I finally found a single integral solving the natural generalisation of the problem discussed here : For $n\ge1$ pick $n+2$ points uniformly at random on the unit circle. What is the probability $P_n(x)$ of the convex hull of the points containing $X=(x,0)$ where $0\le x\le1$ ? If the convex hull does not contain $X$ exactly one of the following cases must hold: All $n+2$ points are on the same side of the $x$ -axis ( $OX$ ). This happens with probability $2^{-(n+1)}$ . There is a point $V$ with Weierstrass parameter $v>0$ and another point $T$ with parameter $t<k/v$ where $k=\frac{x-1}{x+1}$ , and none of the remaining points' parameters are between $t$ and $v$ . All points thus lie ""left"" of the segment $TV$ which lies ""left"" of $X$ . The probability of a point falling in the arc $TV$ is $\frac{\arctan v-\arctan t}\pi$ , while $t,v$ are Cauchy-distributed and can be selected from the pool of points in $(n+2)(n+1)$ ways. This case's contribution to the overall probability is thus $$\frac{(n+2)(n+1)}{\pi^2}\int_0^\infty\int_{-\infty}^{k/v}\frac1{1+v^2}\frac1{1+t^2}\left(1-\frac{\arctan v-\arctan t}\pi\right)^n\,dt\,dv$$ There is a point $V$ with Weierstrass parameter $v>0$ and another point $T$ with parameter $k/v<t<0$ and all of the remaining points' parameters are between $t$ and $v$ , i.e. they are ""right"" of $TV$ which in turn is ""right"" of $X$ . By similar reasoning to the previous case the present case's contribution is $$\frac{(n+2)(n+1)}{\pi^2}\int_0^\infty\int_{k/v}^0\frac1{1+v^2}\frac1{1+t^2}\left(\frac{\arctan v-\arctan t}\pi\right)^n\,dt\,dv$$ Putting everything together we have $$P_n(x)=1-2^{-(n+1)}-\frac{(n+2)(n+1)}{\pi^2}\left(\int_0^\infty\int_{-\infty}^{k/v}\frac1{1+v^2}\frac1{1+t^2}\left(1-\frac{\arctan v-\arctan t}\pi\right)^n\,dt\,dv + \int_0^\infty\int_{k/v}^0\frac1{1+v^2}\frac1{1+t^2}\left(\frac{\arctan v-\arctan t}\pi\right)^n\,dt\,dv\right)$$ This can be turned into a single integral by expanding and solving the $t$ integral. Define the polynomial $$Q(V,K)=\sum_{i=0}^n\sum_{j=0}^i\binom ni\binom ij\frac{(-V)^{i-j}}{\pi^i}\frac{K^{j+1}-(-\pi/2)^{j+1}}{j+1}+\sum_{i=0}^n\binom ni\frac{V^{n-i}}{\pi^n}\frac{(-K)^{i+1}}{i+1}$$ Then $$P_n(x)=1-2^{-(n+1)}-\frac{(n+2)(n+1)}{\pi^2}\int_0^\infty\frac{Q(\arctan v,\arctan k/v)}{1+v^2}\,dv$$ I already worked out that $$P_1(x)=\frac14-\frac3{2\pi^2}\operatorname{Li}_2(x^2)$$ The new surprising thing, though, is that numerical calculations strongly suggest $$\boxed{P_2(x)=2P_1(x)}$$ i.e. it is exactly twice as likely that four random points on the unit circle will enclose $X$ than will three random points, no matter where $X$ is. This relation does not extend to $P_3(x)$ and beyond. How can the boxed relation be proved?","['integration', 'definite-integrals', 'geometric-probability', 'probability']"
4897201,Projective plane over the octonions (Cayley plane),"You can use octonion algebra's $\mathbb{O}$ over a field to coordinatize projective planes. They are called Cayley planes as far as I know. You can't use the usual approach with homogeneous coordinates, but as for explicit constructions, I only found some very vague descriptions (for example here using phrases like ""turning $\mathbb{O}\oplus\mathbb{O}$ into an affine space in the usual way"" and ""adding points at infinity""). Can someone explain or give me the reference to somewhere where it is constructed in an understandable and rigorous way? I also know that you can use planar ternary rings to coordinatise planes, but are octonion algebra's always planar ternary rings? Edit: I don't think they are. Any help would be appreciated.","['projective-geometry', 'geometry', 'octonions', 'reference-request', 'algebraic-topology']"
4897203,Does $X \times \mathbb{R} \simeq X$ hold for infinite dimension inner product space $X$?,"The $X$ is infinite dimension real inner product space, not restricting it as a Hilbert space. This question has troubled my friend and me a lot of days. It is obviously true when $X$ is infinite dimension Hilbert space. However, I can't answer this in arbitrary space. I have thought to find all of the style of infinite dimension space. I thought any space may like $l^2(\alpha) \times \mathbb{R}^{⊕\beta}$ , but it's wrong, such as $l^2(\mathbb{N})^{⊕ω}$ . Any useful answer will be appreciated. Add: where isomorphic maintain inner product. It isn't only as a algebraic isomorphic. $X \times \mathbb{R}$ is a inner product space, which inner product is $(x,r) \cdot (y,s) = x \cdot y + rs$ .","['inner-products', 'dimension-theory-analysis', 'functional-analysis', 'analysis']"
4897294,Probability that Mercury is the nearest planet to Earth.,"Motivation : We tend to think of Venus as the nearest planet to Earth because at its nearest approach to Earth, Venus is the closest at 39 million Km away. This is followed by Mars at 56 million Km and Mercury at 83 million Km. Surprisingly, if we look at the entire year, Mercury ends up being the closest to Earth 46% of the time, followed by Venus 36% of the time, and Mars 18%. This is because orbits of Venus and Mars are huge compared to that of Mercury so while Mercury does not have the nearest approach to Earth, its is never too far away; but Venus and Mars spend most of their time far away from Earth. In fact on an average, Mercury is the nearest planet to every other planet in the solar system . Our simplistic system : Consider a circular co-planar planetary system with a sun in the center and $n$ planets $p_1, p_2,\ldots, p_n$ orbiting counter clockwise at radius $r_1 < r_2 <\ldots < r_n$ respectively. Assuming the mass of the planets are negligible compared to that of the sun, orbital velocity is $v_i = \displaystyle \frac{\beta}{\sqrt{r_i}}$ , where $\beta$ is some constant. For any given planet, the the remaining planets are equally likely to be anywhere in their respective orbits i.e. uniformly distributed relative to one another over a long period of time. Also, if $r_{i+1} - r_i > r_{i-1} + r_{i}$ then $p_{i+1}$ will never be the nearest planet to $p_{i}$ . So to make the system interesting, we must have $r_{i+1} < r_{i-1} + 2 r_{i} < 3r_i$ . Hence we assume that $r_{i+1} = \alpha r_{i} = r_1 \alpha^i$ where $1 < \alpha < 3$ and $i \ge 1$ . This makes sense because most planetary system are know to follow some kind of Titius Bode type rules with their own set of constants. Question : What is the probability that the inner most planet is the nearest planet to the $k$ -th planet? Is it possible to have a closed form in term of $r_i, \alpha$ and $\beta$ ? Update : I think the direction and orbital velocity matters because once a planet $p_i$ becomes the nearest planet of $p_k$ , it remains the nearest planet to $p_k$ for some time until another planet $p_j$ becomes the nearest i.e. even though the relative positions of the planets are uniformly distributed over a long period of time, there is a non-random order which determines which is the nearest at time $T+1$ given the state of the system at time $T$ . I have updated the questions accordingly.","['uniform-distribution', 'geometric-probability', 'circles', 'geometry', 'probability']"
4897331,"If a space is covered by two totally disconnected closed subsets, is the space totally disconnected?","Let $(X, \mathcal{T})$ be a topological space, $C_0, C_1 \subset X$ be closed totally disconnected subsets, and $C_0 \cup C_1 = X$ . A space is totally disconnected if each of its connected components are singletons. Is $\mathcal{T}$ totally disconnected?",['general-topology']
4897337,How to get the size of the angle bisector of a triangle using the law of cosines,"Question:Prove that in any arbitrary triangle like $\triangle ABC$ , the size of the bisector of angle A is obtained from the following equation. $$ AD = \frac{2AC\cdot AB\cos \frac{\hat A}{2}}{AC + AB}$$ My teacher said that it can be proved by using the theorem of cosines. Attempts: Since $d_a$ is the bisector of angle A, according to the property of the bisector in the triangle, it can be written $$\frac{BD}{CD} = \frac{AB}{AC}$$ $$BD = \frac{AB\cdot CD}{AC}$$ Then I wrote the cosine theorem for the two triangles $\triangle BAD$ and $\triangle CAD$ and tried that maybe I could replace something with the above equation in these two formulas, but I didn't achieve anything. $$BD^2 = AB^2 + AD^2 - 2AB\cdot AD\cos \frac{\hat A}{2}$$ $$CD^2 = AC^2 + AD^2 - 2AC\cdot AD\cos \frac{\hat A}{2}$$ $$(\frac{AB\cdot CD}{AC})^2 = AB^2 + AD^2 - 2AB\cdot AD\cos \frac{\hat A}{2}$$ $$CD^2 = (\frac{AC}{AB})^2\cdot (AB^2 + AD^2 - 2AB\cdot AD\cos \frac{\hat A}{2})$$ $$AC^2 + AD^2 - 2AC\cdot AD\cos \frac{\hat A}{2} = (\frac{AC}{AB})^2\cdot (AB^2 + AD^2 - 2AB\cdot AD\cos \frac{\hat A}{2})$$ $$AD = (\frac{AC}{AB})^2 - 2AB\cdot (\frac{AC}{AB})^2\cos \frac{\hat A}{2} + 2AC\cos \frac{\hat A}{2}$$","['trigonometry', 'geometry']"
4897347,$\int_0^{2\pi}\arccos\left(\sin\left(x\right)\right)dx$,"$$\int_0^{2\pi}\arccos\left(\sin\left(x\right)\right)dx$$ I started by splitting it into multiple intervals $$\int_0^{2\pi}\arccos\left(\sin\left(x\right)\right)dx=\int_0^{\frac{\pi}{2}}\arccos\left(\sin\left(x\right)\right)dx+\int_{\frac{\pi}{2}}^{2\pi}\arccos\left(\sin\left(x\right)\right)dx$$ which is $$\int_0^{\frac{\pi}{2}}\frac{\pi}{2}-xdx+\int_{\frac{\pi}{2}}^{2\pi}\arccos\left(\sin\left(x\right)\right)dx$$ $$\int_0^{2\pi}\arccos\left(\sin\left(x\right)\right)dx=\int_0^{\frac{\pi}{2}}\frac{\pi}{2}-xdx+\int_{\frac{\pi}{2}}^{2\pi}\arccos\left(\sin\left(x\right)\right)dx$$ $$\int_0^{2\pi}\arccos\left(\sin\left(x\right)\right)dx=\int_0^{\frac{\pi}{2}}\frac{\pi}{2}-xdx$\int_{\frac{\pi}{2}}^{\pi}\pi-\arccos\left(\sin\left(x\right)\right)dx+\int_{\pi}^{2\pi}\arccos\left(\sin\left(x\right)\right)$+$$ and so on , would that be right? Source :- MIT integration BEE",['integration']
4897348,"Which Leray acyclicity theorems are true, and when?","I've been doing some reading around the subject and I've encountered three (arguably five) variations of the Leray acyclicity theorem given by different sources. I know three of them are true; I'm unsure of the other two, but certainly don't know where they are proven. Let $\check{H^k}(X;\mathscr{F})$ denote the Cech cohomology of a (pre)sheaf $\mathscr{F}$ via the full colimit over all open covers ordered by refinement. Let $\mathcal{H}^k(X;\mathscr{F})$ denote the Grothendieck-derived functor sheaf cohomology. In various places I have seen the following: Definition: Let $X$ be a space, $\mathscr{F}$ a sheaf on $X$ . Suppose $\mathfrak{U}$ is an open cover of $X$ which contains all its finite intersections (and the emptyset) such that $\check{H^k}(W;\mathscr{F})=0$ for all $k\ge1$ and all $W\in\mathfrak{U}$ . In such a case say $\mathfrak{U}$ is a good cover for $\mathscr{F}$ . Leray/Cartan acyclicity theorems (some of them at least will be true ;) ): $(1i)$ If $\mathfrak{U}$ is good then $\mathcal{H}^\bullet(X;\mathscr{F})\cong\check{H^k}(\mathfrak{U},X;\mathscr{F})$ in all degrees $(1ii)$ If $\mathfrak{U}$ is good and is a basis for the topology then $\mathcal{H}^\bullet(X;\mathscr{F})\cong\check{H^k}(\mathfrak{U},X;\mathscr{F})$ in all degrees $(2i)$ If $\mathfrak{U}$ is good then $\mathcal{H}^\bullet(X;\mathscr{F})\cong\check{H^k}(X;\mathscr{F})$ in all degree, so that the canonical map $\check{H^k}(\mathfrak{U},X;\mathscr{F})\to\check{H^k}(X;\mathscr{F})$ is an isomorphism $(2ii)$ If $\mathfrak{U}$ is good and is a basis for the topology then $\mathcal{H}^\bullet(X;\mathscr{F})\cong\check{H^k}(X;\mathscr{F})$ so that the canonical map $\check{H^k}(\mathfrak{U},X;\mathscr{F})\to\check{H^k}(X;\mathscr{F})$ is an isomorphism $(3i)$ If $\mathfrak{U}$ is good then for any refinement $\mathfrak{V}$ of $\mathfrak{U}$ , the canonical map $\check{H^k}(\mathfrak{U},X;\mathscr{F})\to\check{H^k}(\mathfrak{V},X;\mathscr{F})$ is an isomorphism $(3ii)$ If $\mathfrak{U}$ is good and is a basis for the topology then for any refinement $\mathfrak{V}$ of $\mathfrak{U}$ , the canonical map $\check{H^k}(\mathfrak{U},X;\mathscr{F})\to\check{H^k}(\mathfrak{V},X;\mathscr{F})$ is an isomorphism $(1ii),(2ii)$ are true by spectral sequence arguments. I don't know about $(3ii)$ but it seems more plausible than $(3i)$ . Assertions $(2i)$ and $(3i)$ I have seen, $(2i)$ on our very own site , but never with proof. I would like to note that Godement proves $(2ii)$ using a basis hypothesis (I presume that's what "" $\mathfrak{U}$ contient des ouverts arbitrairement petites"" should mean) and does not claim $(2i)$ . A key obstruction to $(2i)$ is this; the spectral sequence argument that we would want to use would have to say that, if $\mathfrak{U}$ is good, then $\check{H^k}(X;\mathscr{H}^n(\mathscr{F}))$ vanishes for all $n\ge1,k\ge0$ where $\mathscr{H}^n(\mathscr{F})$ is the presheaf $V\mapsto\mathcal{H}^n(V;\mathscr{F})$ . But then I'd need to say something about arbitrary covers, which I can't. I've read through most of Godement, chapter $5$ . I can handle the French but it's still quite dense and I may have missed something. Does anyone know authoritative references for $(2i),(3i)$ or better still does anyone know how to adapt the ideas I've presented to give proofs for them here on MSE?","['sheaf-cohomology', 'spectral-sequences', 'algebraic-geometry', 'sheaf-theory', 'algebraic-topology']"
4897412,Existence of a postive measurable set such that $T^{-k}(E)\cap E=\emptyset$ for a particular $k\ge 1.$,"Let $(X,\mathcal B,\mu)$ be a atomless probability measure space and $T:X\to X$ be a non-singular transformation such that $\mu\left(\{x\in X: T^n(x)=x\}\right)=0$ for every $n\ge 1.$ Let $A\in \mathcal B$ such that $\mu(A)>0$ and $k\ge 1$ a positive integer. I want to show that there exists a set $E\in \mathcal B$ such that $\mu(E)>0$ with $E\subseteq A$ and $T^{-k}(E)\cap E=\emptyset.$ Let $E=A\setminus T^{-k}(A)$ and $y\in T^{-k}(E)\cap E$ . Then $y\in E\implies y\notin T^{-k}(A)$ . Also $y\in T^{-k}E\implies T^k(y)\in E\implies T^ky\in A\implies y\in T^{-k}A.$ So, $T^{-k}(E)\cap E =\emptyset$ . But I am unable to construct the set $E$ out of the aforesaid hypothesis such that $\mu(E)>0$ . Please help me to solve this. Thank you for your time and help.","['measure-theory', 'ergodic-theory', 'transformation', 'functional-analysis']"
4897518,Stokes theorem to calculate line integral,"Let $\gamma$ be the intersection between $z=x^2+y^2$ and the plane $z=1+2x$ . Calculate the work done by the field $F=(0,x,-y)$ when the curve $\gamma$ traverses on lap in positive direction seen from z-axis. I should solve this using stokes theorem. Setting both equations above equal we get $1+2x=x^2+y^2\iff 2=(x-1)^2+y^2$ . Then I set $x=s \cos t+1$ and $y=s\sin t$ with $0\leq s\leq \sqrt{2}$ and $0\leq t\leq2\pi$ . Then parametrization of the plane gives $r(s,t)=(s \cos t+1,s\sin t, 2+s\cos t)$ . Further, $r_s=(\cos t, \sin t,\cos t)$ and $r_t=(-s \sin t, s \cos t, -s \sin t)$ and $r_s \times r_t=(-s,0,s)$ , also $\text{curl }F=(-1,0,1) $ so $\int\int_\Gamma \text{curl } F\cdot ndS=\int^{2\pi}_0\int^{\sqrt{2}}_0 2s dsdt=4\pi $ . But the answer is $6\pi$ . So how do I solve this without calculating the area of the ellipse given of the intersecting line? Does not the parametrization I wrote give the surface which is enclosed by the line of the intersection?","['multivariable-calculus', 'vector-analysis']"
4897559,Does ODE solution implies existence for a proportional ODE?,"Suppose an ODE $$\frac{d}{dt} x(t) = f(x,t)$$ has a global solution for some initial condition $x(0) = x_0$ . I am interested in whether, for an arbitrary constant $k>0$ , the proportional ODE $$\frac{d}{dt} x(t) = k \cdot f(x,t)$$ has a global solution for the same initial condition $x(0) = x_0$ . Does the existence of the solution to the first ODE weaken at all the conditions needed to ensure existence of the solution to the second ODE? In all of the examples and counterexamples to existence that I know of, multiplying $f$ by a constant does not affect existence. But I don't see a path to a general proof.","['ordinary-differential-equations', 'real-analysis']"
4897574,"If $f(n)$ is the number of groups of order $n$, then is $f(a)\cdot f(b)\leq f(a\cdot b)$?","Let $f(n)$ be the number of groups of order $n$ up to isomorphism .
We want to prove that: $$f(a) \cdot f(b) \leq f(a \cdot b)$$ for all nonnegative integers $a$ and $b$ .
Our progress: If $a \cdot b \leq 8191$ , then our conjecture holds. If $a + b \leq 848$ , then our conjecture holds. If $\gcd(a, b) = 1$ , then our conjecture holds. If $a \cdot b$ has less than 5 primes in its integer factorization, then our conjecture holds. If $f(a \cdot b) \leq 3$ , then our conjecture holds. Let $c$ and $d$ nonnegative integers.
Let $p$ a prime number.
If $c + d \leq 9$ or $c=d$ or $c\cdot(5\cdot c + 24) + d\cdot(5\cdot d + 24) \leq 17 \cdot c \cdot d + 9$ then $$f(p^{c}) \cdot f(p^{d}) \leq f(p^{c + d})$$ Methods: A lower bound of $f(2048)$ was used. What are the tightest known bounds for the number of groups of order $2048$? For all positive integers $a$ and $b$ : $$f(a) \leq f(a \cdot b)$$ If $a \cdot b = 0$ , then our conjecture holds.
If $1\leq a \cdot b \leq 8191$ :
37592 tests in an algorithm. 180625 tests in an algorithm.
If $f(65274) = 36$ , then $36 \leq f(130548)$ . Thanks @testaccount. Let $p$ , $q$ , $r$ , $s$ distinct prime numbers.
Study of 11 cases for $a \cdot b$ : $p$ , $p^{2}$ , $p\cdot q$ , $p^{3}$ , $p^{2}\cdot q$ , $p \cdot q \cdot r$ , $p^{4}$ , $p^{3} \cdot q$ , $p^{2} \cdot q^{2}$ , $p^{2} \cdot q \cdot r$ , $p \cdot q \cdot r \cdot s$ . Our conjecture is valid for all squarefree $a \cdot b$ . https://www.math.auckland.ac.nz/~obrien/research/gnu.pdf Let $p$ a prime number and $m$ a positive integer. $$p^{\frac{2}{27}m^{2}(m-6)} \leq f(p^{m}) \leq p^{\frac{1}{6}(m^{3}-m)}$$ Source: https://groups.quendi.de/ Known bounds for the number of groups of a given order. https://arxiv.org/abs/1702.02616","['group-theory', 'finite-groups']"
4897579,Evaluate $\int_0^\infty\frac{dx}{1+x^2}\prod_i\arctan a_ix$ (product of arctangents and Lorentzian),"Define $$I(a_1,\dots,a_n)=\int_0^\infty\frac{dx}{1+x^2}\prod_{i=1}^n\arctan a_ix$$ with $a_i>0$ . By this answer $\newcommand{Li}{\operatorname{Li}_2}$ $$I(a,b)=
\frac\pi4\left(\frac{\pi^2}6
-\Li\left(\frac{1-a}{1+a}\right)
-\Li\left(\frac{1-b}{1+b}\right)
+\Li\left(\frac{1-a}{1+a}\frac{1-b}{1+b}\right)\right)$$ By taking the limit as $b\to\infty$ or by here we get $$I(a)=
\frac12\left(\frac{\pi^2}4
-\Li\left(\frac{1-a}{1+a}\right)
+\Li\left(-\frac{1-a}{1+a}\right)\right)$$ But I have not been able to reduce $I(a,b,c)$ to polylogarithms like the previous two equations. Can $I(a_1,\dots,a_n)$ be reduced to elementary functions, polylogarithms and possibly other special functions appearing in the literature for general $a_i$ and $n$ ? Contour integration might help for even $n$ . If not, then what about the special case where $a_i\in\{1,a\}$ for a given $a$ ? Such integrals occur in the probability discussed here . Here is my attempt at calculating $I(a,b,c)$ . Define the self-inverse transformation $f:v\mapsto\frac{1-v}{1+v}$ , $x=f(a),y=f(b),z=f(c)$ and $J(a,b,c)=I(f(a),f(b),f(c))$ . Then $I(a,b,c)=J(x,y,z)$ with $-1<x,y,z<1$ and we can get the Maclaurin series expansion of $J$ in $x,y,z$ by differentiating under the integral sign. Numerical results strongly suggest that the $x^iy^jz^k$ coefficient of this series is the sum of $\frac{2^{p-2}}{(i+j+k)(i-j-k)(j-k-i)(k-i-j)}$ if $i+j+k$ is odd, where $p$ of the exponents $i,j,k$ are positive $\frac{\pi^2}{16i^2}$ if $p=2$ and (without loss of generality) $i=j>0,k=0$ $-\frac{\pi^2}{8i^2}$ if $p=1$ and (without loss of generality) $i>0,j=k=0$ $\frac{\pi^4}{64}$ if $p=0$ (i.e. $i=j=k=0$ ) This would mean $\newcommand{Li}{\operatorname{Li}_2}$ $$I(a,b,c)=\frac{\pi^4}{64}-\frac{\pi^2}8(\Li(x)+\Li(y)+\Li(z))+\frac{\pi^2}{16}(\Li(xy)+\Li(zx)+\Li(yz))$$ $$+\sum_{i,j,k=0,2\nmid i+j+k}^\infty\frac{2^{p-2}}{(i+j+k)(i-j-k)(j-k-i)(k-i-j)}x^iy^jz^k$$ But an observed pattern is not a proof. How can this last equation be proved and can the last sum be reduced to special functions in the literature?","['integration', 'polylogarithm', 'definite-integrals', 'closed-form']"
4897623,Q. 18 from A first course in probability by Sheldon Ross,"Each of 20 families selected to take part in a treasure hunt consist of a mother, father, son, and daughter. Assuming that they look for the treasure in pairs that are randomly chosen from the 80 participating individuals and that each pair has the same probability of finding the treasure, calculate the probability that the pair that finds the treasure includes a mother but not her daughter. Solution I thought Sample space: $80\times79$ As there are 20 mothers so we select and each have one daughter so event will have $20\times78$ samples.
This gives answer $0.247$ . But answer in book is $0.3734$ .",['probability']
4897626,Number of ways in which 5 girls and 5 boys can be arranged in a line such that only 4 girls stand adjacent to one another,"I have already calculated n but i am confused to find the value of m. I have tried the following process:- Selected 4 girls from 5 girls in ${}_5C_1$ ways and treated it as a unit After selecting 4 girls, we are left with 6 people (5B and 1G), these 6 people create 7 gaps among one other but we can place the 4 girl unit in only one of the 5 gaps ( excluded the gap created by the one remaining girl). So out of 5 gaps i can select 1 gap in ${}_5C_1$ and place the 4 girls there. Additionally, these 4 girls can be permuted among themselves in 4! ways in that gap. Finally, the 5 boys can be permuted among themselves in 5! Ways
(We cannot include the remaining girl in the permutation of the 5 boys as then the permutation would be 6! which would also include the case when all the girls are together) Where did i go wrong in these process?","['permutations', 'combinatorics']"
4897658,On stopping time and filtration containments,"This is an attempt to clarify an issue from Section 6 in the proof of Theorem 2.2. Bass, R. F. , Uniqueness in law for pure jump Markov processes , Probab. Theory Relat. Fields 79, No. 2, 271-287 (1988). ZBL0664.60080 . Let $D([0,\infty))$ be the set of all $f : [0,\infty) \to \mathbb R^d$ which are right continuous and possess left sided limits at all points in $[0,\infty)$ . For each $t \geq 0$ let $X_t : D([0,\infty)) \to \mathbb R^d$ be given by $X_t(\omega) = \omega(t)$ (coordinate projections). The augmented natural filtration of $\{X_t\}_{t \geq 0}$ will be denoted by $\{\mathcal F_t\}_{t \geq 0}$ with $\mathcal F_{\infty} = \sigma(\cup_{n \geq 1} \mathcal F_n)$ . We can define $\mathcal F_{\tau}$ for $\tau$ an a.s. finite stopping time in the usual way. For $\omega \in D([0,\infty))$ and $\tau$ an a.s. finite stopping time, define $\theta_{\tau} : D([0,\infty)) \to D([0,\infty))$ by $(\theta_{\tau}(\omega))(t) = \omega(t + \tau(\omega))$ . (So this is a ""random shift"" by the stopping time $\tau$ ). Now, given $A \in \mathcal{F}_{\tau}$ and $B \in \mathcal F_{\infty}$ , let $E_{A,B}$ be the event given by $E_{A,B} = A \cap \{\omega : \theta_{\tau}(\omega) \in B\}$ . Question : Is $\mathcal F_{\infty} = \sigma(\{E_{A,B} : A \in \mathcal{F}_{\tau}, B \in \mathcal F_{\infty}\})$ ? According to Bass, this is true! Intuitively, it does seem to be true. What does it mean to belong to some $E_{A,B}$ ? The point is, $A \in \mathcal F_{\tau}$ means that membership in $A$ is determined by the values in the collection $\{X_t\}_{0 \leq t \leq \tau}$ . On the other hand, $\{\omega : \theta_{\tau}(\omega) \in B\}$ is a set in which membership is determined purely by the values in the collection $\{X_t\}_{t \geq \tau}$ . Thus, effectively, $E_{A,B}$ is an event which is determined by two separate conditions, one applying purely for $\{X_t\}_{0 \leq t \leq \tau}$ and the other purely applying for $\{X_t\}_{t \geq \tau}$ . This leads me to believe that I can use the following steps : A function $f : D([0,\infty)) \to \mathbb R$ is measurable with respect to $\sigma(\{E_{A,B} : A \in \mathcal{F}_{\tau}, B \in \mathcal F_{\infty}\})$ if and only if it's an a.e. limit of functions of the form $F(\omega) = f_1(\omega)f_2(\theta_{\tau} \circ \omega)$ where $f_1$ depends only on the first "" $\tau$ "" coordinates of $\omega$ and $f_2$ is arbitrary and measurable, and the set of finite sums/products of such functions. Every function measurable with respect to $\mathcal F_{\infty}$ (one can reduce this to just being measurable with respect to some $\mathcal F_{n}$ ) can be approximated arbitrarily by finite sums/products of functions of the form $F(\omega)$ , $\omega$ described as above. This way, the set of measurable functions becomes the same for both collections and we're done. However, neither step is particularly clear to me. -The first isn't because I'm not able to make ""first $\tau$ coordinates"" rigorous in my working. For the second, it's known that if $f : \Omega_1 \times \Omega_2 \to \mathbb R$ is a measurable function ( $\Omega_1,\Omega_2$ some good enough measurable spaces) then you can approximate it arbitrarily by sums and products of functions of the form $g(x)h(y)$ , $g : \Omega_1 \to \mathbb R$ and $h : \Omega_2 \to \mathbb R$ arbitrary measurable. However, in this case, we want $\Omega_1$ to be ""the first $\tau$ coordinates"" so $\Omega_1$ is varying with the sample, creating problems for me. I'd appreciate a proof that uses these steps, although I'm assuming there should be one using the $\pi$ - $\lambda$ or monotone class theorem or something that I'm also very happy to go with.","['measurable-functions', 'measure-theory', 'probability-theory', 'filtrations']"
4897869,"Prove that if $\forall n\in\Bbb N\quad a_{n+1}^2-a_{n+1}=a_n\in\Bbb Q$, then the sequence is constant","This question was posted, downvoted and closed today ( 2022 Thailand Olympiad problem ) and 8 days ago ( $f(x+1)^{2} - f(x+1) = f(x)$ . What values of $f(1)$ allow $f(x)$ to be always rational if $x$ is natural number? ). I want to re-ask it more precisely and with some more ""context"", propose an answer, and ask whether you have other solutions. Prove that the only sequences $(a_n)$ of rational numbers such that $$a_{n+1}^2-a_{n+1}=a_n$$ are the two constant sequences $0$ and $2$ . Note that we don't assume $a_n\ge0$ a priori. My first step (thanks to @cansomeonehelpmeout's comment on the previous post): Letting $b_n=2a_n-1$ , the problem is equivalent to: prove that the only sequences $(b_n)$ of rational numbers such that $$b_{n+1}^2=2b_n+3$$ are the two constant sequences $-1$ and $3$ . My next steps : I intend to prove that the $b_n$ s must be integers, obviously $\ge-\frac32$ , but also $\le3$ , whence the conclusion.","['contest-math', 'rational-numbers', 'recurrence-relations', 'sequences-and-series']"
4897887,Deriving the CAPM pricing kernel from the general SDF and consumption-based kernel,"I'm reading the paper "" Quality minus junk "" by Asness et al. (2019) and trying to understand the pricing kernel definition they provide on page 6. The authors present the following pricing kernel: $$
\frac{M_{t+1}}{M_t} = \frac{1}{1+r^f} \left(1 + e^M_{t+1}\right)
$$ where $r^f$ is the risk-free rate and $e^M_{t+1}$ is the zero-mean innovation to the pricing kernel. They then state that if the Capital Asset Pricing Model (CAPM) holds, the pricing kernel is: $$
e^M_{t+1} = -\lambda_t \left(\frac{r^{MKT}{t+1}-E_t(r^{MKT}{t+1})}{\sigma^2_t(r^{MKT}_{t+1})}\right)
$$ where $\lambda_t=E_t(r^{MKT}_{t+1})-r^f$ is the market risk premium. I'm familiar with two other common forms of the pricing kernel: The general stochastic discount factor (SDF) equation from asset pricing theory can be seen here: Stochastic discount factor The consumption-based pricing kernel from the consumption CAPM: $$
m_{t+1} = \beta \frac{u'(c_{t+1})}{u'(c_t)}
$$ My question is: How can I derive the two euqations used in the paper from either the general SDF equation or the consumption-based pricing kernel? I'd appreciate a step-by-step explanation of the derivation process. Additionally, I'm curious about the economic intuition behind the CAPM pricing kernel formula. How does it relate to the pricing kernel and to and to the basic CAPM model? Please let me know if you need further context from the paper or have any other questions.","['statistics', 'finance', 'probability']"
4897924,Deformation invariance of cohomology and Chern classes,"Let $f\colon X\to Y$ be a smooth projective morphism between smooth complex quasi-projective varieties. Then it is known that each fiber of $f$ has isomorphic cohomology ring. I'm wondering if the following cohomology class is also deformation invariant: (1) If $L$ is a line bundle on $X$ , then is the first Chern class $c_1(L_y)\in H^{2i}(X_y, \mathbb{Q})$ vanishing for a point $y\in Y$ implies it is vanishing for all $y\in Y$ ? (2) If $Z\subset X$ is a smooth subvariety such that $Z\to Y$ is also smooth, for any two points $y,y'\in Y$ , can we always choose an isomorphism $H^*(X_y, \mathbb{Q})\cong H^*(X_{y'}, \mathbb{Q})$ which maps $ch(\mathcal{O}_{Z_y})$ to $ch(\mathcal{O}_{Z_{y'}})$ ?",['algebraic-geometry']
4897960,Book recommendations for Combinatorics for Computer Science Students,"I am a computer science student with an interest in competitive programming. I am currently looking to deepen my understanding of combinatorics, as it is a crucial part of algorithm design and analysis. Despite searching, I have not found good resources that are tailored for computer science students or those focusing on competitive programming. Could anyone recommend comprehensive books or resources on combinatorics that are particularly suited for computer science students? Ideally, these resources would cover both fundamental concepts and advanced topics, with applications to algorithm challenges.","['programming', 'book-recommendation', 'reference-request', 'combinatorics', 'computer-science']"
4898014,"Find values of $a,b$. such that a matrix is diagonalizable","Find all values of $a,b\in\mathbb{R}$ such that $A$ is diagonalizable. $$A=\begin{pmatrix}
-1 & a & b\\
0 & 1 & 2\\
0 & 2 & 1\\
\end{pmatrix}.$$ So far, I have that: $$det(A-\lambda I)=\begin{vmatrix} 
-1-\lambda & a & b\\
0 & 1-\lambda & 2\\
0 & 2 & 1-\lambda\\
\end{vmatrix}=-(\lambda +1)^2(\lambda -3).$$ So $A$ has eigenvalues $\lambda=-1$ , $\lambda=3$ Now, let $v=(x,y,z)^T$ . Then $v\in ker(A--1I)\iff$ $$\begin{pmatrix} 
0&a&b\\
0&2&2\\
0&2&2
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z
\end{pmatrix}=\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}$$ $$\iff ay+bz=0, 2y+2z=0, 2y+2z=0.$$ $$\iff y=(-bz)/a, y=-z, z=-y$$ So, $v\in ker(A--1I) \iff v=\begin{pmatrix}
x\\
(-b•-y)/a\\
-y
\end{pmatrix}=x\begin{pmatrix} 
1\\
0\\
0
\end{pmatrix}+y\begin{pmatrix}
0\\
b/a\\
-1
\end{pmatrix}$ . Similarly for $\lambda=3$ , $v\in ker(A-3I)\iff$ $x=(ay)/4+(bz)4, y=z, z=y \iff$ $$ v=\begin{pmatrix} 
(ay)/4 + (bz)/4\\
y\\
z
\end{pmatrix}$$ $$=y\begin{pmatrix}
a/4\\
1\\
0
\end{pmatrix}+z\begin{pmatrix}
b/4\\
0\\
1
\end{pmatrix}.$$ But after this, I don't know how to find the values of $a,b$ such that $A$ is diagonalizable.","['matrices', 'diagonalization', 'linear-algebra', 'linear-transformations']"
4898033,"Example of CW-complex with $G$-action, which is not $G$-CW-complex","Let $G$ be a quasi-compact, Hausdorff topological group and let $G$ act on a CW-complex $X$ such that the $G$ -action sends cells to cells and boundaries of cells to boundaries of cells. Further, assume that if $e$ is an cell of $X$ such that $g\cdot e\cap e $ is nonempty, then $g\cdot x = x$ for all $x \in e$ . If $G$ is finite then it is known that $X$ is a $G$ -CW-complex with filtration coming from the underlying CW-complex filtration of $X$ . Can someone give me an example, where this is not the case?","['equivariant-topology', 'equivariant-maps', 'cw-complexes', 'general-topology', 'group-actions']"
4898048,Concatenation of $f$-related vector fields/tangent vectors,"This is probably a highly trivial question, but I just can't wrap my head around it. Let $M,N$ be two manifolds and $f: M \rightarrow N$ smooth. Let further $X^1, X^2$ be two smooth vector fields on $M$ and $Y^1, Y^2$ on $N$ . Further, $Y^i$ is $f$ -related to $X^i$ , that is
for all $p \in M$ : $$Y^i_{f(p)} = D_pf(X^i_p).$$ Let $g \in \mathcal{C}^{\infty} (N)$ . Now, what exactly does the following concatenation (which is for example encountered in Lie brackets) mean? $$Y^1_{f(p)}(Y^2_{f(p)}(g))$$ If I try to ""unwrap"" it in terms of $X$ I always end up with ill-defined expressions: $$Y^1_{f(p)}(Y^2_{f(p)}(g)) = D_pf(X^1_p)(D_pf(X^2_p)(g)) \\
= D_pf(X^1_p)(X^2_p(g \circ f)) \\
= X^1_p(X^2_p(g \circ f)\circ f).$$ The problem is that $D_pf(X^i_p)(\cdot)$ expects some $g \in \mathcal{C}^{\infty} (N)$ . I understand that $X^2_p(g \circ f)$ is to be understood as $g \mapsto X^2_p(g \circ f)$ , but how can this be concatenated again with $f$ ? In what sense is $X^2_p(g \circ f) \in \mathcal{C}^{\infty} (N)$ so that $D_pf(X^1_p)(X^2_p(g \circ f))$ is well-defined?","['vector-fields', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4898052,Dudley's inequality: Sending $\delta$ to $0$,"Let $\{X_t\}_{t \in T}$ denote a mean-zero separable sub-Gaussian stochastic process with variance proxy $\sigma^2 = 1$ , defined on some metric space $T$ with metric $d$ . Dudley's integral inequality, as stated in my textbook (Wainwright's high dimensional statistics) is given as: $$
\mathbb{E}\left[\sup_{t, s \in T} X_t - X_s\right] \leq 2 \mathbb{E}\left[\sup_{t, s \in T : d(t,s) < \delta} X_t - X_s\right] + 32 \int_{\delta/4}^\infty \sqrt{\log N(T,u,d)}du
$$ Here $N$ is the covering number of $T$ in the metric $d$ by balls of size $u$ . As a remark, he states that we may take $\delta \to 0$ and get the bound: $$
\mathbb{E}\left[\sup_{t, s \in T} X_t - X_s\right]  \leq 32 \int_{0}^\infty \sqrt{\log N(T,u,d)}du
$$ Question: As $\delta \to 0$ , why is it the case that the first term: $$
\mathbb{E}\left[\sup_{t, s \in T : d(t,s) < \delta} X_t - X_s\right] 
$$ tends to $0$ ? I am not able to prove it as the only bounds I know are for finite collections of random variables. Even if I try to use some kind of compactness argument, the fact that the index set is infinite still gives me problems. Is there some assumption that is needed on the metric space $(T,d)$ ?","['statistics', 'probability-theory']"
4898059,Reference request non existence of minimal resolution.,"In this page of Wikipedia( https://en.wikipedia.org/wiki/Resolution_of_singularities ), it writes, the hypersurface in $\mathbb{A}_\mathbb{C}^4$ defined by the equation $xy-zw$ has no minimal resolution. Do you know any reference that have a proof of this fact.
If possible, a standard reference of birational geometry, singularity theory , minimal model probelem, is appreciated.","['algebraic-geometry', 'singularity-theory', 'schemes', 'reference-request']"
4898092,Understanding Proof of Proposition 2.2.5 from Measure Theory by Donald Cohn,"Overview I am self-studying Donald Cohn's Measury Theory . I have some questions about his proof of Proposition 2.2.5, and I would like to confirm if my attempt to understand his proof is correct. Here is the statement of the proposition with the proof: Proposition 2.2.5 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $\mathscr{A}_{\mu}$ be the completion of $\mathscr{A}$ under $\mu$ . Then a function $f:X\to[-\infty,+\infty]$ is $\mathscr{A}_{\mu}$ -measurable if and only if there are $\mathscr{A}$ -measurable functions $f_0,f_1:X\to[-\infty,+\infty]$ such that \begin{align}
    f_0\leq f\leq f_1\ \textit{holds everywhere on $X$}\tag1
\end{align} and \begin{align}
    f_0=f_1\ \textit{holds $\mu$-almost everywhere on $X$.}\tag2
\end{align} Proof $\quad$ First suppose that there exist $\mathscr{A}$ -measurable functions $f_0$ and $f_1$ that satisfy (1) and (2). Then $f_0$ is $\mathscr{A}_{\mu}$ -measurable and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. Hence, Proposition 2.2.2 applied to the space $(X,\mathscr{A}_{\mu},\overline{\mu})$ , implies that $f$ is $\mathscr{A}_{\mu}$ -measurable. Now suppose that $f:X\to[-\infty,+\infty]$ is $\mathscr{A}_{\mu}$ -measurable. If $f$ is simple and $[0,+\infty)$ -valued, say attaining values $a_1,\dots,a_k$ on the sets $A_1,\dots,A_k$ , then there are sets $B_1,\dots,B_k$ and $C_1,\dots,C_k$ that belong to $\mathscr{A}$ and satisfy $C_i\subseteq A_i\subseteq B_i$ and $\mu(B_i-C_i)=0$ for each $i$ . The functions $f_0$ and $f_1$ defined by $f_0=\sum_ia_i\chi_{C_i}$ and $f_1=\sum_ia_i\chi_{B_i}$ then satisfy (1) and (2). We can deal with the case where $f$ is simple and real-valued by applying the preceding argument to the positive and negative parts of $f$ . Finally, let $f:X\to[-\infty,+\infty]$ be an arbitrary $\mathscr{A}_{\mu}$ -measurable function, and choose a sequence $\{g_n\}$ of simple $\mathscr{A}_{\mu}$ -measurable functions from $X$ to $\mathbb{R}$ such that $f(x) = \lim_{n\to\infty}g_n(x)$ holds at each $x$ in $X$ . If for each $n$ , we choose $\mathscr{A}$ -measurable functions $g_{0,n}$ and $g_{1,n}$ such that \begin{align*}
    g_{0,n}\leq g_n\leq g_{1,n}\ \text{holds everywhere on $X$}
\end{align*} and \begin{align*}
    g_{0,n}=g_{1,n}\ \text{holds $\mu$-almost everywhere on $X$,}
\end{align*} then the required functions $f_0$ and $f_1$ can be constructed by letting $f_0$ be $\limsup_{n\to\infty}g_{0,n}$ and $f_1$ be $\liminf_{n\to\infty}g_{1,n}$ . My Questions In the first paragraph of the proof, it asserts that $f_0$ is $\mathscr{A}_{\mu}$ -measurable and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. But why? In the second paragraph of the proof, it says that "" $f$ ... attaining values $a_1,\dots,a_k$ on the sets $A_1,\dots,A_k$ . My question is why can we assume that the set of possible values attained by $f$ is finite? Still in the second paragrph, why do $f_0=\sum_ia_i\chi_{C_i}$ and $f_1=\sum_ia_i\chi_{B_i}$ satisfy (1) and (2)? The existence of $g_{0,n}$ and $g_{1,n}$ for each $n$ ? In the last paragraph, why do $f_0=\limsup_{n\to\infty}g_{0,n}$ and $f_1=\liminf_{n\to\infty}g_{1,n}$ satisfy (1) and (2)? My Attempts Here are my attempts so far: First suppose that there exist $\mathscr{A}$ -measurable functions $f_0$ and $f_1$ that satisfy (1) and (2). The measurability of $f_0$ implies that $\{x\in X:f_0(x)\leq t\}\in\mathscr{A}\subseteq\mathscr{A}_{\mu}$ . It follows that $f_0$ is $\mathscr{A}_{\mu}$ -measurable. Since $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ , we have $\{x\in X:f_0(x)\neq f_1(x)\}$ is $\mu$ -negligible. So let $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:f_0(x)\neq f_1(x)\}\subseteq A$ , and $\mu(A)=0$ . Since $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ , it follows that $\{x\in X:f(x)\neq f_0(x)\} \subseteq \{x\in X:f_0(x)\neq f_1(x)\}$ . Therefore, the set $A$ satisfies $A\in\mathscr{A}\subseteq\mathscr{A}_{\mu}$ , $\{x\in X:f(x)\neq f_0(x)\}\subseteq A$ , and $\overline{\mu}(A)=\mu(A)=0$ , so that the set $\{x\in X:f(x)\neq f_0(x)\}$ is $\overline{\mu}$ -negligible and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. I still don't know why. I think the set of possible values may not even be countable. If $x\in A_i$ , then $f(x)=a_i$ . Also, $x\in A_i\subseteq B_i$ implies that $\chi_{B_i}(x)=1$ , so that $f_1(x)\geq a_i$ . Now, \begin{align*}
\chi_{C_i}(x)=
\begin{cases}
1\ \text{if $x \in C_i$}\\
0\ \text{if $x \in A_i-C_i$}
\end{cases}.
\end{align*} Moreover if $x \in A_i-C_i$ then $x \notin C_j$ for $j\neq i$ , because otherwise if $x\in C_j\subseteq A_j$ it would contradicts $A_i\bigcap A_j=\emptyset$ for $i\neq j$ . Therefore, $f_0(x)\leq a_i$ . Thus, $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ . We now show that $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ . Consider the set $\bigcup_i(B_i-C_i)$ . Since each $B_i$ and $C_i$ belongs to $\mathscr{A}$ , we have $\bigcup_i(B_i-C_i)\in\mathscr{A}$ . Since $\mu(B_i-C_i)=0$ for each $i$ , the countable additivity of $\mu$ implies that $\mu\left(\bigcup_i(B_i-C_i)\right)=0$ . Now suppose $f_0(x)\neq f_1(x)$ for some $x$ . If $x$ is in some $C_i$ , then $x\in A_i\subseteq B_i$ and $x\notin A_j$ for $j\neq i$ and so $x\notin B_j$ for $j\neq i$ . Then $f_0(x) = a_i = f_1(x)$ , a contradiction. Therefore, $x$ is not in any $C_i$ . But $x$ must be in some $B_i$ . Thus, $x\in\bigcup_i(B_i-C_i)$ , and so $\{x\in X:f_0(x)\neq f_1(x)\}\subseteq\bigcup_i(B_i-C_i)$ . Hence, $\{x\in X:f_0(x)\neq f_1(x)\}$ is $\mu$ -negligible and $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ . The third paragraph in the original proof guarantees the existence of such $g_{0,n}$ and $g_{1,n}$ for each $n$ . To show $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ , note that \begin{align*}
f_0(x) &= \limsup_{n\to\infty}g_{0,n}(x)\\
&=\inf_k\sup_{n\geq k}g_{0,n}(x)\\
&\leq\inf_k\sup_{n\geq k}g_{n}(x)\\
&=\limsup_{n\to\infty}g_n(x)\\
&=\lim_{n\to\infty}g_n(x)\\
&= f(x)\\
&= \lim_{n\to\infty}g_n(x)\\
&= \liminf_{n\to\infty}g_n(x)\\
&= \sup_k\inf_{n\geq K}g_n(x)\\
&\leq \sup_k\inf_{n\geq k}g_{1,n}(x)\\
&= \liminf_{n\to\infty}g_{1,n}(x)\\
&= f_1(x),
\end{align*} and we are done. We now prove $f_0=f_1$ holds $\mu$ -almost everywhere. Suppose that $\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)$ ; that is, suppose that \begin{align*}
\inf_k\sup_{n\geq k}g_{0,n}(x)\neq\sup_k\inf_{n\geq k}g_{1,n}(x).\tag3
\end{align*} Then $g_{0,n}\neq g_{1,n}$ for some $n$ . For otherwise, if $g_{0,n} = g_{1,n}$ , we would have equality in (3). Therefore, $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\}$ . Since $g_{0,n}=g_{1,n}$ holds $\mu$ -almost everywhere, it follows that $\{x\in X:g_{0,n}\neq g_{1,n}\}$ is $\mu$ -negligible; that is, there exists an $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A}$ , and $\mu(A)=0$ . Hence, there exists an $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A}$ , and $\mu(A)=0$ , which means $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}$ is $\mu$ -negligible, and so $f_0=f_1$ holds $\mu$ -almost everywhere. For This Post Could someone please help me with my question 2? Moreover, I would really appreciate it if someone could help me check if my attempt for the other four questions are correct!","['measure-theory', 'proof-explanation', 'analysis', 'real-analysis', 'almost-everywhere']"
4898104,Geometry of origami saddle surfaces made of five or six square paper sheets connected around a point,"I connected five and six square paper sheets (which are all initially flat and have the same dimensions) using tapes to create two smooth saddle surfaces (see below), but I couldn't figure out the analytical/numerical geometric shapes of these two surfaces?! I apologize if 'saddle surfaces' have more rigorous definitions in mathematics. In my opinion, these two 3D shapes are ruled saddle surfaces (because the line segments connecting the center and each point on the edges are all straight), and I can obtain the entire surface once I have all the vertcices and curved edges determined. Furthermore, all the curved edges can be determined by calculating the geodesics connecting those vertices. So the question boils down to the coordinates of all vertices in 3D space. Please correct me if I am wrong. I do not have a strong background in differential geometry. If you don't have an answer for analytical shapes but have some thoughts on solving it numerically, that would also be great!! Any help will be appreciated!! Thank you so much for your time!!","['geodesic', 'geometry', 'origami', 'manifolds', 'differential-geometry']"
4898138,Confusion regarding vector/matrix multiplication in index notation,"I came across this question and answer (sorry I don't have an electronic source for it, only a paper copy). After reading the answer it had me questioning the notation one uses to denote row/column vectors in tensor expressions: Consider the following contravariant vectors: $a^\mu=(1,1,0,0)$ , $b^\mu=(0,1,0,0)$ and $c^\mu=(0,0,0,1)$ . Derive the following quantities: $\phi=a_\mu b^\mu$ , $\psi=a_\mu a^\mu$ , $V_\mu=a_\nu b_\mu a^\nu$ , $W^{\nu\mu}=c^\nu a^\mu$ , $P_{\mu\nu}=a_\mu b_\nu$ and $Q_\mu^{\,\,\,\nu}=b_\mu c^\nu$ . Here are the solutions: First we can construct the covariant versions of these vectors, $a_\mu=(-1,1,0,0)$ , $b_\mu=(0,1,0,0)$ and $c_\mu=(0,0,0,1)$ , then we have $$\phi=a_\mu b^\mu=a_0b^0+a_1b^1+a_2b^2 +a_3b^3=1\tag{a}$$ $$\psi=a_\mu a^\mu =a_0a^0+a_1a^1+a_2a^2+a_3a^3=-1+1=0\tag{b}$$ $$V_\mu=a_\nu b_\mu a^\nu=\left(a_\nu a^\nu\right)b_\mu=0\qquad \text{(using the result (b))}\tag{c}$$ $$W^{\nu\mu}=c^\nu a^\mu = \begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1&1&0&0\end{pmatrix}\tag{d}$$ $$P_{\mu\nu}=a_\mu b_\nu = \begin{pmatrix}0&-1&0&0\\0&1&0&0\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{e}$$ $$Q_\mu^{\,\,\,\nu}=b_\mu c^\nu=\begin{pmatrix}0&0&0&0\\0&0&0&1\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{f}$$ In the above extract the metric signature used is $\eta_{\mu\nu}=\text{diag}(-1,1,1,1)$ and for consistency I will follow this convention in the rest of this post. That's the end of the solutions, my problem is that I just don't understand why both the $a^\mu,\,b^\mu,\,c^\mu$ and $a_\mu,\,b_\mu,\,c_\mu$ are written as row vectors. I think either the contravariant or covariant vectors must be written as column vectors for matrix multiplication to even make sense. For example, to compute $a_\mu$ I could write $$a_\mu=\eta_{\nu\mu}a^\nu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}\begin{pmatrix}1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}^T\tag{1}$$ which is in the right form for matrix multiplication to apply, a $(4\times 4)\times(4\times 1)$ matrix product. In the second equality I used the fact that the Minkowski metric is symmetric, $\eta_{\mu\nu}=\eta_{\nu\mu}$ Now this is the part that really confuses me , I'm to understand that tensor expressions are commutative - since all the information is captured in the repeated (contracted) indices, just like in expression $(\mathrm{c})$ the $b_\mu$ was commuted past the $a^\nu$ . So if what I'm saying is really true, I may also write $$a_\mu=a^\nu\eta_{\nu\mu}=\begin{pmatrix}1&1&0&0\end{pmatrix}\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}\tag{2}$$ which is also in the right form for matrix multiplication to apply, a $(1\times 4)\times(4\times 4)$ matrix product. This also turns out to be the same expression the author got for $a_\mu$ . But $(1)\ne(2)$ , so why don't I get the same result? Remark: At first sight this question may look so trivial and stupid that it should just be downvoted without mercy. But I ask you to please bear with me, this is not obvious to me at all. Update in response to comments by @JackozeeHakkiuz and @Kurt G. In what follows I'm going to write out every little tedious step in the calculation, this may be painful or incredibly annoying to read for those of you well-acquainted with tensor expression manipulations. But I am a beginner to this, and it is clear that I do not understand how to work with tensors, so I feel I must labour the point. I'm sorry about this. Firstly to address the commutativity of $a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$ . Now I mentioned in the comments that I do not wish to compute these four-vectors using matrix-multiplication (as that was what lead to the confusion to begin with). Instead I will use the Einstein summation convention and then try to explain what the problem is: So, starting with $a^\nu\eta_{\mu\nu}$ , $$a_\mu=a^\nu\eta_{\mu\nu}=a^\nu\eta_{\nu\mu}=\begin{pmatrix}a^0\eta_{00} + a^1 \eta_{10} + a^2\eta_{20}+ a^3\eta_{30}\\a^0\eta_{01} + a^1 \eta_{11} + a^2\eta_{21}+ a^3\eta_{31}\\a^0\eta_{02} + a^1 \eta_{12} + a^2\eta_{22}+ a^3\eta_{23}\\a^0\eta_{03} + a^1 \eta_{13} + a^2\eta_{23}+ a^3\eta_{33}\end{pmatrix}$$ $$=\begin{pmatrix}a^0\eta_{00} + 0 + 0 + 0\\0 + a^1 \eta_{11} + 0 + 0\\0 + 0 + a^2\eta_{22}+ 0\\0 + 0 + 0+ a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}a^0\eta_{00}\\a^1 \eta_{11} \\a^2\eta_{22}\\a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}1\times (-1)\\ 1\times (1) \\0 \times (1)\\ 0\times (1)\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix}$$ Where I have used the four-vector $a^\mu=(1,1,0,0)$ and the fact that $\eta$ is diagonal: $\eta_{00}=-1$ , $\eta_{11}=\eta_{22}=\eta_{33}=1$ which indeed matches the expression for $a_\mu$ in the author's solution above. Now for the reverse order, $\eta_{\mu\nu}a^\nu$ , $$a_\mu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}\eta_{00}a^0 + \eta_{01}a^1 + \eta_{02}a^2+ \eta_{03}a^3\\\eta_{10}a^0 + \eta_{11}a^1 + \eta_{12}a^2+ \eta_{13}a^3\\\eta_{20}a^0 + \eta_{21}a^1 + \eta_{22}a^2+ \eta_{23}a^3\\\eta_{30}a^0 + \eta_{31}a^1 + \eta_{32}a^2+ \eta_{33}a^3\end{pmatrix}$$ $$=\begin{pmatrix}\eta_{00}a^0 + 0 + 0 + 0\\0 +  \eta_{11}a^1 + 0 + 0\\0 + 0 + \eta_{22}a^2+ 0\\0 + 0 + 0+ \eta_{33}a^3\end{pmatrix}=\begin{pmatrix}\eta_{00}a^0\\ \eta_{11}a^1 \\\eta_{22}a^2\\\eta_{33}a^3\end{pmatrix}=\begin{pmatrix}(-1)\times 1\\ (1)\times 1 \\(1) \times 0\\ (1)\times 0\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix}$$ Which shows that $a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$ (tensor factors commute). But do factors in tensor expressions always commute? For instance, I am going to use eqn. $(\mathrm{d})$ in the author's solution as an example. I could compute $W^{\nu\mu}$ in two different ways: Firstly, I could write $$W^{\nu\mu}=c^\nu \color{red}{a^\mu} = \begin{pmatrix}c^0a^0&c^0a^1&c^0a^2&c^0a^3\\c^1a^0&c^1a^1&c^1a^2&c^1a^3\\c^2a^0&c^2a^1&c^2a^2&c^3a^3\\c^3a^0&c^3a^1&c^3a^2&c^3a^3\end{pmatrix}$$ $$=\begin{pmatrix}0\times (1)& 0\times(1)&0\times (0)&1\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\1\times (1)& 1\times(1)&1\times (0)&1\times (0)\end{pmatrix}=\begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1& 1&0&0\end{pmatrix}$$ Great, that matches the expression $(\mathrm{d})$ as written by the author, where I used $c^\mu=(0,0,0,1)$ and $a^\mu=(1,1,0,0)$ . But I'm to understand that factors in tensor expressions are commutative, so what is to stop me from writing $$W^{\nu\mu}\stackrel{\color{blue}{?}}{=}\color{red}{a^\mu} c^\nu = \begin{pmatrix}a^0c^0&a^0c^1&a^0c^2&a^0c^3\\a^1c^0&a^1c^1&a^1c^2&a^1c^3\\a^2c^0&a^2c^1&a^2c^2&a^2c^3\\a^3c^0&a^3c^1&a^3c^2&a^3c^3\end{pmatrix}$$ $$=\begin{pmatrix}(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\end{pmatrix}=\begin{pmatrix}0&0&0&1\\0&0&0&1\\0&0&0&0\\0& 0&0&0\end{pmatrix}?$$ So $$\bbox[5px,border:2px solid darkgreen]
{W^{\nu\mu}\stackrel{\color{blue}{?}}{=}c^\nu \color{red}{a^\mu}\ne \color{red}{a^\mu} c^\nu}$$ I think I'm misunderstanding the comments regarding the commutativity, but from what was written it was my impression that all information is captured in the tensor indices, hence I can commute tensor factors at will in any tensor expression. This was mentioned for the case of $$a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$$ I assumed this statement generalized to any tensor expression. Clearly this is not the case for $W^{\nu\mu}$ so there is something else going on here I don't understand. CORRECTIONS: Sorry for the late edit, I've just realised I made a typo for second expression involving $W^{\nu\mu}$ ; in the previous version I (mistakenly) just switched the order of the indices, $\nu$ and $\mu$ instead of the four-vectors $a^\nu$ and $c^\mu$ . I have marked the relevant areas in red and it is the equation in the green box that I am questioning. Apologies for the excessively long post.","['vectors', 'tensors', 'matrices', 'solution-verification', 'intuition']"
4898150,For which topologies do self-homeomorphisms preserve the collection of open sets?,"Inspired by Does a homeomorphism preserve the open sets? , I wanted to ask the corresponding question: while $X$ may have a topology $\mathcal T$ and a homeomorphism from $(X,\mathcal T)$ to $(X,\mathcal T')$ such that $\mathcal T\neq \mathcal T'$ , under what conditions must $\mathcal T= \mathcal T'$ ? A simple example is the discrete topology; can this be generalized?",['general-topology']
4898156,Does the kurtosis need to be finite for the sample variance to be consistent?,"It is known (see this answer ) that if $\mu_4$ is the fourth central moment of a distribution and $\sigma$ is the standard deviation, then we can write $$\operatorname{Var}(S^2_n)=\frac{1}{n}\left[\mu_4-\frac{n-3}{n-1}\sigma^4\right].$$ This goes to $0$ as $n\to\infty$ , hence if the kurtosis (and therefore the fourth moment) is finite, then the sample variance $S^2$ is consistent for the population variance (since $\mathbb{E}(S^2_n)=\sigma^2)$ . Does the converse hold? Does the kurtosis need to be finite for $S^2_n$ to converge in probability to $\sigma^2$ ? I have tried simulating the sampling distribution of $S^2$ for various distributions; I tried looking at the sampling distribution for a $t_4$ population (which only has up to the third moment) as well as a $t_5$ population (which has finite kurtosis), and this seems to support the idea that the kurtosis does in fact need to exist (but obviously there could be a counterexample with another distribution).","['statistics', 'variance', 'probability-distributions', 'parameter-estimation', 'probability']"
4898196,Find the set of values of $\alpha$ so that $f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2}$ is one one.,"Let $f$ be a function defined in its domain given by $f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2}$ . Find the set of values of $\alpha$ so that $f(x)$ is one-one . My attempt As $f(x)$ have to be one-one so any line $\parallel$ to $x$ -axis must cut the graph only once. So, necessary condition is that $f(x)$ must be monotonic. $$f'(x)={(\alpha+6x-8x^2)(2\alpha x+6)-(\alpha x^2+6x-8)(6-16x)\over(\alpha+6x-8x^2)^2}$$ $$\implies f'(x)={6(\alpha+8)x^2+2(\alpha+8)(\alpha-8)x+6(\alpha+8)\over(\alpha+6x-8x^2)^2}=\frac{Q(x)}{(\alpha+6x-8x^2)^2}$$ In order to have $f'(x)\le0$ or $\ge0$ $\forall$ $x\in R$ , Discriminant of $Q(x)$ $\le0$ . $$\implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le 0 \\
{\implies \alpha \in [ 2 \; , \; 14] \tag{1}}$$ But, for this range $f(x) $ is not one one (I have projected it in desmos for verification) and I found it later that $(1)$ is also the condition for which $f(x)$ is onto i.e. Range of $f \in \mathbb R$ . $\boxed{y=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2} \\ \implies (\alpha+8y)x^2+(6-6y)x+(-8-\alpha y)=0 \\ \text{as}\;  x\in \mathbb R  \;; D\ge0 \\ \implies 6-6y)^2+4(\alpha+8y)(8+\alpha y)\ge 0 \\ \implies (9+8\alpha)y^2+(\alpha^2+46)y+(8\alpha+9)\ge0\; \;  \text{now as} \;  y\in R\\ \implies (\alpha^2+46)^2-4(8\alpha+9)^2\le0 \implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le0 \; \text{....same as (1) above}}$ Desmos Graph shows that it is many one for this range as line line parallel to x axis cut it 2 times although function continuously increases for all $x\in \mathbb R$ ...  Please tell what is happening here and how to solve this question... Link for the diagram Question Source: India's JEE math practice book.","['calculus', 'graphing-functions', 'quadratics', 'algebra-precalculus']"
4898213,What is p capacity of set actually saying?,"Fix $1\leq p<n$ . Define, \begin{equation}
K^p\equiv\{f:\mathbb{R^n}\rightarrow\mathbb{R}\ \vert\ f\geq 0, f\in L^{p^{\ast}}(\mathbb{R}^n), Df\in L^{p}(\mathbb{R}^n;\mathbb{R}^n)\}.
\end{equation} If $A\subset\mathbb{R}^n$ we define the quantity \begin{equation}
\text{Cap}_p(A)    \equiv   \inf\left\{\int_{\mathbb{R}^n}\vert Df\vert^p\text{ d}x\ \middle|\   f\in K^p, A\subset\text{int}\{f\geq 1\}\right\}
\end{equation} as the $p$ -capacity of $A$ (denoted by Cap $_p(A)$ ).
Can someone break down what is this actually doing? I am not able to see what it is really calculating and implying. Can someone give me a much more detailed description on it? Since only the most abstract definitions are being given.",['measure-theory']
4898219,Probability that at least 1 student is taking a language class solution,"This is a problem from A First Course in Probability by Ross: An elementary school is offering 3 language
classes: one in Spanish, one in French, and one in
German. The classes are open to any of the 100
students in the school. There are 28 students in the
Spanish class, 26 in the French class, and 16 in the
German class. There are 12 students that are in
both Spanish and French, 4 that are in both Spanish and German, and 6 that are in both French and
German. In addition, there are 2 students taking
all 3 classes. The question I am trying to answer is: If 2 students are chosen randomly, what is the
probability that at least 1 is taking a language
class? From a previous question, the answer that a randomly chosen student is not in any of the language classes is 1/2. The answer to this question is 149/198, and it is calculated as $$1-\frac{50\choose2}{100\choose 2}=1-\frac{49}{198}=\frac{149}{198}.$$ My question is why can't this be calculated as $$1 - P(\text{Both students not taking language classes}) = 1 - \left( \frac{1}{2} \right)^2 = \frac{3}{4}$$ since the individual probability that a student is not taking a language class is 1/2?","['statistics', 'combinatorics', 'probability']"
4898253,Open sets on a surface with locally connected boundary,"Let $\Sigma$ be a surface and $\Omega$ be an open subset of $\Sigma$ . Suppose that $\Omega$ is homeomorphic to the open unit disk $\mathbb{D}$ and is relatively compact in $\Sigma$ . I'm interested in the 'boundary behaviour' of maps $\varphi:\mathbb{D}\to\Omega$ when $\partial\Omega$ is locally connected. When $\Sigma$ is the complex plane or the Riemann sphere, then the Caratheodory-Torhorst Theorem can be used to show that there exists a homeomorphism $\varphi:\mathbb{D}\to\Omega$ which can be continuously extended to $\overline{\varphi}:\overline{\mathbb{D}}\to\overline{\Omega}$ , where the bar denotes closure in its respective space. (Here, the map $\overline{\varphi}$ need not be a homeomorphism.) I want to know whether this is true on general surfaces. If $\partial\Omega$ , the boundary of $\Omega$ , is locally connected, does there exist a homeomorphism $\varphi:\mathbb{D}\to\Omega$ which can be continuously extended to $\overline{\varphi}:\overline{\mathbb{D}}\to\overline{\Omega}$ ? If this is not true in general, when can we guarantee the existence of such $\varphi$ and $\overline{\varphi}$ ?","['riemann-surfaces', 'surfaces', 'complex-analysis', 'geometric-topology', 'low-dimensional-topology']"
4898275,"Is 1/3 included in the sequence 0.3, 0.33, 0.333,...? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 months ago . Improve this question I assume that $\frac{1}{3}$ is equal to $0.3333...$ . Let's define a sequence as follows: $0.3$ , $0.33$ , $0.333$ , $0.3333$ ,... Question: is $\frac{1}{3}$ included in this sequence? Every item in the sequence clearly has finite number of decimals, and $\frac{1}{3}$ has infinite decimals so it is clearly not included. On the other hand the limit of this sequence is $\frac{1}{3}$ so it seems ok to say that it ""includes"" $\frac{1}{3}$ . What is the correct answer?","['limits', 'convergence-divergence', 'decimal-expansion']"
4898401,"PDF of $Y=g(X)$ when $X\sim N(0,1)$. $g(X)$ is a piecewise function where each part is constant.","Let $X$ be a random variable with pdf: \begin{equation} f_X(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \end{equation} Now define $Y$ to be $Y=g(X)$ where $g(x)=1$ if $x>0$ , $=\frac{1}{2}$ if $x=0$ and $=-1$ if $x<0$ . I want to find the pdf of $Y$ . This is my attempt to find its CFD: \begin{equation} P[Y\leq y] = 1,y\geq1\\ P[Y\leq y]=f_X(0), -1\leq y < 1\\ P[Y\leq y]= 0 , y<-1 \end{equation} The goal is then to differentiate the CFD of $Y$ to obtain its pdf, however I'm not sure if I've calculated it correctly. Can you help me please?","['probability-distributions', 'probability-theory', 'random-variables']"
4898435,Eigenvalues of a 8x8 Matrix,"This is the 6th problem of the TACA in August, 2023. One have 2 hours to solve 15 problems. I am wondering how to calculate the eigenvalues of the following 8 by 8 matrix by hand. Note that this is NOT a circulant matrix. $A=\begin{pmatrix}
10  &-9  &  &  &  &  &  &-9 \\
-9  &10  &-9  &  &  &  &  & \\
  &9  &10  &9  &  &  &  & \\
  &  &9  &10  &9  &  &  & \\
  &  &  &9  &10  &9  &  & \\
  &  &  &  &-9  &10  &-9  & \\
  &  &  &  &  &-9  &10  &-9 \\
-9  &  &  &  &  &  &-9  &10
\end{pmatrix}$ (Required in the contest: Let $\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_8$ be the eigenvalues of A, find the value of $\lambda_6$ .I have no idea if this makes it simpler.) Calculated with computers, $\lambda_1=\lambda_2=10-9\sqrt{2},\lambda_3=\lambda_4=\lambda_5=\lambda_6=10,\lambda_7=\lambda_8=10+9\sqrt{2}$ I have no idea how to solve $|\lambda I-A|=O$ in a normal way. Maybe I need new methods? When I participated, I mistook A for the following circulant matrix B. $B=\begin{pmatrix}
10  &-9  &  &  &  &  &  &-9 \\
-9  &10  &-9  &  &  &  &  & \\
  &-9  &10  &-9  &  &  &  & \\
  &  &-9  &10  &-9  &  &  & \\
  &  &  &-9  &10  &-9  &  & \\
  &  &  &  &-9  &10  &-9  & \\
  &  &  &  &  &-9  &10  &-9 \\
-9  &  &  &  &  &  &-9  &10
\end{pmatrix}$ I took the 8th unit roots to $-9x^7-9x+10$ and got $\lambda_1=10-18,\lambda_2=\lambda_3=10-9\sqrt{2},\lambda_4=\lambda_5=10,\lambda_6=\lambda_7=10+9\sqrt{2},\lambda_8=10+18$ . I don't know if this helps with the original problem either.","['matrices', 'linear-algebra', 'contest-math', 'eigenvalues-eigenvectors']"
4898480,Interpreting a notation in calculus of variations (differentiating with respect to a derivative),"Consider a functional $J[y]$ defined by: $$J[y] = \int_a^b F(x, y, y') dx \tag{1}$$ Here, $F$ is a function that depends on the independent variable $x$ , the function $y(x)$ , and its derivative $$y' = \frac{dy}{dx} \tag{2}$$ . In the calculus of variations, the operation of differentiating $F$ with respect to $y'$ is involved: $$\frac{\partial F}{\partial y'} \tag{3}$$ This represents the rate of change of the function $F$ with respect to the derivative of $y$ , $y'$ . Operationally, since $y'$ is $\frac{dy}{dx}$ , this differentiation is investigating how sensitive $F$ is to changes in the rate at which $y$ changes with respect to $x$ . I find the notation $\frac{\partial F}{\partial y'}$ a bit confusing in the sense that we are differentiating a function with respect to the derivative. If we just think $y'$ is just another variable symbol and proceed normally as most books do it does not cause much problems, but my question is : What is the mathematical meaning of $\frac{\partial F}{\partial y'}$ in terms of limits? In ordinary differential calculus, we don't encounter differentiation with respect to a derivative itself. Thanks.","['calculus-of-variations', 'multivariable-calculus', 'functional-analysis', 'intuition', 'derivatives']"
4898513,Differentiability of function $f:\mathbb{C} \to \mathbb{C}; f(z)=z\exp(\bar{z})$,"Let $f: \mathbb{C} \to \mathbb{C}$ be defined by $f(z)=z\exp(\bar{z})$ . Identify the points where $f$ has complex derivative. At $z=0$ : $$
\lim_{z \to 0} \frac{f(z)-f(0)}{z-0} 
= \lim_{z \to 0} \frac{z\exp(\bar{z})}{z} 
= \lim_{z \to 0} \exp(\bar{z}) = 1
$$ So, $f$ is differentiable at $0$ .
But for $\mathbb{C}^{\ast}$ with the prior knowledge that $\exp(\bar{z})$ is not differentiable anywhere, $f$ is not complex-differentiable at any non-zero complex number, because that'd imply $$
\exp(\bar{z}) = \frac{z\exp(\bar{z})}{z}
$$ would be complex-differentiable as it'd be the product of complex-differentiable functions on $\mathbb{C}^{\ast}$ .
So $f$ is only complex-differentiable at $0$ . Is this argument correct?",['complex-analysis']
4898590,How did Ramanujan find $\sum_{n=0}^\infty (-1)^n\frac{(1/2)_n(1/4)_n(3/4)_n}{n!^3}\frac{644n+41}{25920^n}=\frac{288\sqrt{5}}{5\pi}?$,"The formula $$\sum_{n=0}^\infty (-1)^n\frac{(1/2)_n(1/4)_n(3/4)_n}{n!^3}\frac{644n+41}{25920^n}=\frac{288\sqrt{5}}{5\pi}$$ (in older notation) appears as eq. 38 in Ramanujan's paper Modular equations and approximations to $\pi$ ; $(a)_n$ is the Pochhammer symbol. But – the formula is like an exercise for the reader. Allegedly, it can be deduced from the theory of modular equations, but how exactly? It should be somehow possible to prove the formula from Clausen's formulas and representations of $$25P(q^{50})-P(q^2)$$ where $$P(q)=1-24\sum_{k=1}^\infty \frac{kq^k}{1-q^k}$$ with $|q|\lt 1$ . The hypergeometric representation of Ramanujan's formula is known to be $$41\,_3F_2\left(\frac14,\frac12,\frac34,1,1,-\frac{1}{25920}\right)-\frac{161}{69120}\,  _3F_2\left(\frac54,\frac32,\frac74,2,2,-\frac{1}{25920}\right)=\frac{288\sqrt{5}}{5\pi}.$$ What I tried Let $$K(x)=\int_0^{\pi/2}\dfrac{dt}{\sqrt{1-x\sin^2 t}}$$ and $$E(x)=\int_0^{\pi/2}\sqrt{1-x\sin^2 t}\,dt$$ be the elliptic integrals.
Using the hypergeometric differential equation, the problem at hand shoud then be reducible to $$K\left(\dfrac{1}{2}-6\sqrt{-360+161\sqrt{5}}\right)\left(2E\left(\dfrac{1}{2}-6\sqrt{-360+161\sqrt{5}}\right)-\left(1+\sqrt{-840+376\sqrt{5}}\right)K\left(\dfrac{1}{2}-6\sqrt{-360+161\sqrt{5}}\right)\right)=\frac{\pi}{10}.$$ Now I recognized the argument of the elliptic integrals as a special value of the modular lambda function $\lambda$ ( https://en.wikipedia.org/wiki/Modular_lambda_function ): $$\lambda (5i)=\dfrac{1}{2}-6\sqrt{-360+161\sqrt{5}}.$$ So if someone can prove that $$K(\lambda (5i))=\dfrac{\sqrt{5}+2}{20}\dfrac{\Gamma (1/4)^2}{\sqrt{\pi}}$$ and $$E(\lambda (5i))=\dfrac{(-2+\sqrt{5})\pi^{3/2}}{\Gamma (1/4)^2}+\dfrac{\left(2+\sqrt{5}+2\sqrt{-10+6\sqrt{5}}\right)\Gamma (1/4)^2}{40\sqrt{\pi}},$$ (that's out of my reach), then Ramanujan's formula is proved. Edit I noticed that $K(\lambda (5i))$ can be easily proved to have the desired closed form by using division values of elliptic functions with parameter $-1$ . But evaluating $E(\lambda (5i))$ seems hard... Edit #2 The question has been answered on MathOverflow: https://mathoverflow.net/questions/469126/how-did-ramanujan-find-sum-n-0-infty-1n-frac1-2-n1-4-n3-4-nn/469135#469135","['definite-integrals', 'pi', 'sequences-and-series', 'elliptic-integrals', 'hypergeometric-function']"
4898597,Coefficient of $x^{21}$ in $(1+x+x^2+\dots+x^{10})^4$,"Find the coefficient of $x^{21}$ in $(1+x+x^2+\dots+x^{10})^4$ I tried splitting the terms inside the bracket into two parts $1+x+\dots+x^9$ and $x^{10}$ , and then tried binomial theorem, but that got unreasonably hard to deal with. Another idea that came is to write the expression inside bracket as $\smash{\dfrac{x^{11}-1}{x-1}}$ , but I don't know how to proceed. Help","['algebra-precalculus', 'binomial-coefficients', 'combinatorics', 'polynomials']"
4898628,Prove that a function doesn't have a horizontal asymptote,"Suppose that $$
f'(x) = \frac{x^2+8x}{x^2+8}
$$ with a horizontal asymptote $y=1$ . Prove that $f(x)$ doesn't have a horizontal asymptote. One of my classmates suggested that if the function $f(x)$ has a horizontal asymptote, then its slope must approach $0$ . But $f'(x) \to 1$ ; thus $f(x)$ doesn't have a horizontal asymptote. But the teacher said that this explanation wouldn't get full points if it were on the test. Any better explanations? (We haven't learned integrals and limits yet. Only the fact that limits are used with the horizontal asymptote).","['limits', 'calculus', 'derivatives', 'rational-functions']"
4898668,"Consider the following recurrence, $a_n = \frac{1}{2}a_{n-1}+1$ where $a_1 = 1$. Guess a pattern for $a_n$ and prove it by induction. [duplicate]","This question already has answers here : Solve the recurrence relation $a_n={1\over2}a_{n-1}+1$, $a_1=1$ (2 answers) Closed 2 months ago . This post was edited and submitted for review 2 months ago and failed to reopen the post: Original close reason(s) were not resolved Consider the following recurrence, $$a_n = \frac{1}{2}a_{n-1}+1$$ where $a_1 = 1$ . (a) Guess a pattern for $a_n$ and prove it by induction. (b) Convert the recurrence for $a_n$ into the form $a_n = Aa_{n−1} + Ba_{n−2}$ by eliminating the constant $1$ in the recurrence. Solve for $a_n$ using the characteristic equation. (c) Find $a_n$ using the generating function method. What I did I did find a pattern. $$a_n=\frac{2^n-1}{2^{n-1}}, n>1$$ I am stuck on how to convert the function into the form $a_n = Aa_{n−1} + Ba_{n−2}$ by eliminating the constant $1$ in the recurrence. Also how to find $a_n$ using the generating function method. NOTE: This question is more than just the recurrence. It is also asking for the induction proof, characteristic equation, and generating function method. It is entirely different from this question: Solve the recurrence relation $a_n={1\over2}a_{n-1}+1$, $a_1=1$ . Please re-open.","['recurrence-relations', 'discrete-mathematics']"
4898697,"If a sequence can be broken down into two subsequences that both converge to the same value, does the main sequence necessarily converge?","I have the sequence $\{ \frac{1}{1}, \frac{1}{3}, \frac{1}{2}, \frac{1}{4}, \frac{1}{3}, \frac{1}{5}, \frac{1}{4}, \frac{1}{6},...\}$ . I'm having trouble coming up with an explicit description of the sequence itself, but I noticed that the sequence consists of two subsequences: $\{\frac{1}{1}, \frac{1}{2}, \frac{1}{3}, \frac{1}{4},...\}$ and $\{\frac{1}{3},\frac{1}{4}, \frac{1}{5}, \frac{1}{6},...\}$ , where the first subsequence is all of the even-indexed terms, and the second subsequence is all of the odd-indexed terms. Since both of those subsequences converge to 0, can I say that the original sequence must also converge to 0?",['sequences-and-series']
4898739,Number of One-One Functions,"This question has been asked in my exam and I have stuck. The question says: Let $S=\{1,2,3,4,5,6\}$ . The number of one-one functions $f$ defined from $S$ to $P(S)$ , where $P(S)$ stands for power set of $S$ , such that $f(n)\subset f(m)$ wherever $n<m$ , is what? My approach to the problem: Since $1<2<3<4<5<6$ , it implies $f(1)\subset f(2)\subset f(3)\subset f(4)\subset f(5)\subset f(6)$ , i.e., function's value at lower element is proper subset of function's value at higher element. This constraint can be broken into cases: (i) $n(f(1))=0,n(f(2))=1, n(f(3))=2, n(f(4))=3,n(f(5))=4, n(f(6))=5$ (ii) $n(f(1))=0,n(f(2))=1, n(f(3))=2, n(f(4))=3,n(f(5))=4, n(f(6))=6$ (iii) $n(f(1))=0,n(f(2))=1, n(f(3))=2, n(f(4))=3,n(f(5))=5, n(f(6))=6$ (iv) $n(f(1))=0,n(f(2))=1, n(f(3))=2, n(f(4))=4,n(f(5))=5, n(f(6))=6$ (v) $n(f(1))=0,n(f(2))=1, n(f(3))=3, n(f(4))=4,n(f(5))=5, n(f(6))=6$ (vi) $n(f(1))=0,n(f(2))=2, n(f(3))=3, n(f(4))=4,n(f(5))=5, n(f(6))=6$ (vii) $n(f(1))=1,n(f(2))=2, n(f(3))=3, n(f(4))=4,n(f(5))=5, n(f(6))=6$ Since $f(n)≠f(m)\iff n≠m$ because all images have different sizes, so function is one-one already. Now, if we make a function, say, by case (iii), then one such way is: $f(1)=\phi, f(2)=\{3\}, f(3)=\{3,6\}, f(4)=\{1,3,6\}, f(5)=\{1,2,3,5,6\}, f(6)=\{1,2,3,4,5,6\}=S$ . There are $$^6C_0*^6C_1*^6C_2*^6C_3*^6C_5*^6C_6$$ ways possible alone in case (iii). Multiply and divide by $^6C_4$ , we get: $$(\prod_{r=0}^{6}{^6C_r})*(\frac{1}{^6C_4})$$ For final answer, we add all the cases: $$(\prod_{r=0}^{6}{^6C_r})(\sum_{r=0}^6\frac{1}{^6C_r})$$ But the answer provided is $3240$ , along with a very confusing solution, which is different from my answer. Please help.","['relations', 'functions', 'combinatorics', 'elementary-set-theory', 'set-theory']"
4898741,"Consider the following recurrence, $a_n=\frac{4a_{n-1}^3+2a_{n-1}-a_{n-2}}{1+4a_{n-1}a_{n-2}}$ where $a_0=0, a_1=1$. Show every $a_n$ is an integer.","Consider the following recurrence, $$a_n=\frac{4a_{n-1}^3+2a_{n-1}-a_{n-2}}{1+4a_{n-1}a_{n-2}}$$ where $a_0=0, a_1=1$ . (a) Show that every $a_n$ is an integer. (b) Find the general term of $a_n$ . What I've done: I've calculated some values: $$a_2=6, a_3=35, a_4=204, a_5=1189, a_6=6930, a_7=40391,\\ a_8=235416, a_9=1372105, \ldots $$ I also tried to find the condition to make the value of fraction integer, but I was stuck since the formula is about cubic polynomial and I don't know how to deal with it.","['number-theory', 'recurrence-relations']"
4898827,Two definitons of a pullback of a differential form,"Let $f: X \to Y$ be a morphism of spaces with admitting differential forms (e.g. real manifold, complex manifold, smooth algebraic variety, schemes). Let $\Omega^n_Y$ denote the sheaf of $n$ -forms on $Y$ . When one speaks of a ""pullback of a differential form,"" there are at least two possible interpretations. The first is the one from differential geometry: given a section on $\Omega^n_Y(U)$ , we get a section on $\Omega^n_X(f^{-1}(U))$ defined locally by $$g \; dy_1 \wedge \dots \wedge dy_n \mapsto (g \circ f) \; d(y_1 \circ f) \wedge \dots \wedge d(y_n \circ f).$$ This gives us a bona fide section of $\Omega^n_X$ ; using the chain rule, we can check this map respects the respective transition functions for $\Omega^{n}_Y$ and $\Omega^n_X$ , and this is what most non-algebraic geometers seem to mean when referring to pullback of differential forms. However, there is another common usage of the word ""pullback"": the canonical map on sections $H^0(Y, \Omega^n_Y) \to H^0(X, f^*\Omega^n_Y)$ , or more generally $H^0(U, \Omega^n_Y) \to H^0(f^{-1}(U), f^*\Omega^n_Y).$ I really only know how describe this map affine-locally in the algebraic category. For example, if $X = \text{Spec } A$ and $Y = \text{Spec } B$ are $k$ -varieties, then $\Omega_Y$ is the sheaf associated the module of Kähler differentials $\Omega_{B/k}$ , and $f^*\Omega_Y$ is the sheaf associated to the $A$ -module $A \otimes_B \Omega_{B/k}$ . Then the map on sections is given locally by sending $\omega \in \Omega_{B/k}$ to $1 \otimes \omega \in A \otimes_B \Omega_{B/k}$ . These two definitions of pullback cannot be literally the same, since in general $f^*\Omega^n_Y \not \simeq \Omega^n_X$ –the outputted sections live on entirely different sheaves. But I have deeply confused myself trying to think about how they are related and whether we can easily convert between the two versions. Is there something I am missing, or a better way of thinking about this? I worry because I cannot find any literature that seems to address this conflict, so I feel like there is some obvious thing that is being left unsaid.","['complex-geometry', 'algebraic-geometry', 'pullback', 'differential-forms', 'differential-geometry']"
4898828,How to evaluate$\lim\limits_{n\to\infty}n\left(n\left(n..\left(\int_0^1\left(\frac{\sqrt[n]{x}+1}{2}\right)^ndx-l_0\right)-l_1..\right)-l_{m}\right)$?,"$$\lim_{n \to \infty} \int_0^1 \left( \frac{\sqrt[n]{x}+1}{2} \right)^n dx= \frac{2}{3}$$ I became curious what happens If we do the following: $$\lim_{n \to \infty} n\left(\int_0^1 \left( \frac{\sqrt[n]{x}+1}{2} \right)^n dx- \frac{2}{3}\right)$$ The numerical methods suggest that this will be $\frac 2 {27}$ what If we took this farther ? $$= \lim_{n \to \infty} n \left(n\left(\int_0^1 \left( \frac{\sqrt[n]{x}+1}{2} \right)^n dx- \frac{2}{3}\right)-\frac 2 {27} \right) ?$$ lets define $\displaystyle l_0:= \frac{2}{3}$ , $\displaystyle x_{n,0}:=\int_0^1\left(\frac{\sqrt[n]{x}+1}{2}\right)^ndx$ , $ \displaystyle l_1:= \frac 2 {27} $ , $\displaystyle x_{n,1}:= n\left(\int_0^1 \left( \frac{\sqrt[n]{x}+1}{2} \right)^n dx- \frac{2}{3}\right)$ In general $\displaystyle x_{n , m+1}= n (x_{n,m}- l_{m} )$ and $\displaystyle  l_{m}= \lim\limits_{n \to \infty} x_{n,m}$ . Assuming of course that $l_m \in \mathbb{R}$ Is it possible to find a closed form for all $l_m $ ? and what is $\lim\limits_{m \to \infty} l_m$ ?","['integration', 'definite-integrals', 'recurrence-relations', 'real-analysis', 'limits']"
4898900,"The vertices of a hexagon are random points on a unit circle; $a,b,c$ are the lengths of three random sides. Conjecture: $P(ab<c)=\frac35$.","The vertices of a hexagon are uniformly random points on a unit circle; $a,b,c$ are the lengths of three distinct random sides. A simulation with $10^7$ such random hexagons yielded a proportion of $0.60008$ of them satisfying $ab<c$ . Is the following conjecture true: $P(ab<c)=\dfrac35$ . Probabilities involving the side lengths of random polygons inscribed in a circle are often simple rational numbers; for example here , here and here . Those examples involve triangles, which are determined by only two parameters (two random central angles, with the third angle determined by the others), so it was easy to set up an integral. But this question involves a hexagon, which involves five parameters (five random angles, with the sixth angle determined by the others), and I do not know how to set up an integral. Sometimes these kinds of questions can be answered without integration, using pure geometry, for example here . But I have been unable to find a purely geometric proof for this question. $P(ab<c)$ seems to be a simple rational number only for triangles and hexagons, among $n$ -gons for $3\le n\le 12$ , based on simulations. Distribution of $a$ Here I derive $f(x)$ , the density function of $a$ , which is the same as the density function of $b$ and $c$ . However, I do not know if this is helpful, because $a,b,c$ are not independent. Suppose the first chosen random point on the circle is $O$ , and $a$ is the length of the side immediately anticlockwise from $O$ . $a>x$ if all the other chosen points are on the major arc subtended by the red line segment, and the other chosen points are not all on the minor arc subtended by the green line segment. $\theta=\arcsin \frac{x}{2}$ $\begin{align}
P(a<x)&=1-P(a>x)\\
&=1-\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5-\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5\right)
\end{align}$ $\begin{align}
f(x)&=\frac{d}{dx}P(a<x)\\
&=\frac{5}{\pi\sqrt{4-x^2}}\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4+\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4\right)\\
\end{align}$ I checked that $\int_0^2f(x)\mathrm dx=1$ .","['integration', 'conjectures', 'geometric-probability', 'circles', 'probability']"
4898932,The category of abelian groups with quasi-homomorphisms,"Let $A$ and $B$ be abelian groups. Say that a map $f: A \to B$ is a quasi-homomorphism if there exists a finite $D \subseteq B$ such that $$\forall a_1, a_2 \in A: f(a_1 + a_2) - f(a_1) - f(a_2) \in D$$ We wish to show that if $A$ , $B$ and $C$ are abelian groups and $f: A \to B$ and $g: B \to C$ are quasi-homomorphisms, then $g \circ f: A \to C$ is a quasi-homomorphism. So take a finite $D \subseteq B$ such that $$\forall a_1, a_2 \in A: f(a_1 + a_2) - f(a_1) - f(a_2) \in D$$ and a finite $E \subseteq C$ such that $$\forall b_1, b_2 \in B: g(b_1 + b_2) - g(b_1) - g(b_2) \in E$$ Define $$F = \{g(d) + e_1 + e_2 \mid d \in D, e_1, e_2 \in E\} \subseteq C$$ As $D$ and $E$ are finite, $F$ is also finite. Now take any $a_1, a_2 \in A$ . We wish to show that $$g(f(a_1 + a_2)) - g(f(a_1)) - g(f(a_2)) \in F$$ Now, set $$d = f(a_1 + a_2) - f(a_1) - f(a_2) \in D$$ Set $$e_1 = g(d + f(a_1) + f(a_2)) - g(d) - g(f(a_1) + f(a_2)) \in E$$ Set $$e_2 = g(f(a_1) + f(a_2)) - g(f(a_1)) + g(f(a_2)) \in E$$ Then we have $$g(f(a_1 + a_2)) - g(f(a_1)) - g(f(a_2)) = g(d + f(a_1) + f(a_2)) - g(f(a_1)) - g(f(a_2)) = e_1 + g(d) + g(f(a_1) + f(a_2)) - g(f(a_1)) - g(f(a_2)) = e_1 + g(d) + e_2 \in F$$ Thus, $g \circ f$ is a quasi-homomorphism. In particular, we can form a category $\mathsf{QuasiAb}$ whose objects are abelian groups and whose morphisms are quasi-homomorphisms. Is this proof correct? Can you give any reference on $\mathsf{QuasiAb}$ ?","['category-theory', 'reference-request', 'solution-verification', 'group-theory', 'abelian-groups']"
4898954,Non-autonomous system of two nonlinear ordinary differential equations with conditions,"Consider the ODE system: $$
\frac{df}{dx}= -\sqrt{g},\tag{1}
$$ $$
\frac{dg}{dx}= -\sqrt{x}f,\tag{2}
$$ where $f=f\left(x\right)$ and $g=g\left(x\right)$ are the functions on the interval $x\in\left[0,1\right]$ . The following four conditions are also given: $$
f\left(0\right)=F>0,\quad g\left(0\right)=G>0,\tag{3}
$$ $$
f\left(1\right)=0,\quad g\left(1\right)=0,\tag{4}
$$ where $F$ and $G$ are some positive constants. I am interested in the following questions. Does there exist a continuous non-negative (i.e. $f\left(x\right)\geq0$ and $g\left(x\right)\geq0$ , $\forall x\in\left[0,1\right]$ ) non-trivial solution of the system (1) and (2) consistent with the additional conditions (3) and (4) for some positive constants $F$ and $G$ ? If so, is there any way to express the exact value of the constants $F$ and $G$ ? The second question is the most interesting for me. This issue originates from physics. Essentially, these equations are obtained from the one-dimensional diffusion problem. That is the reason why I am only interested in positive continuous solutions. An approximate numerical solution can be found. We can consider the equations (1) and (2) with initial conditions (4) as a Cauchy problem. Solving it allows us to determine
the constants $F$ and $G$ from the conditions (3). Starting from the point $x=1$ , one can obtain only a trivial solution. Thus, we should start from a point close to unity: $x=1-\epsilon$ , $1\gg\epsilon>0$ , and use the asymptotics as the initial conditions: $$
f\left(1-\epsilon\right)\sim\frac{\epsilon^{3}}{36},\quad g\left(1-\epsilon\right)\sim\frac{\epsilon^{4}}{144},\qquad\epsilon\rightarrow+0.
$$ The numerical solutions presented in the figures below correspond to the following constants: $$
F\simeq2.04\times10^{-2},\quad G\simeq2.35\times10^{-3}.
$$","['systems-of-equations', 'ordinary-differential-equations', 'cauchy-problem', 'nonlinear-system', 'mathematical-physics']"
4898969,Distance between two sets different from zero,"Consider these sets $$
A\equiv \bigcap_{\delta>0} \liminf_{n\rightarrow \infty} \{x \in X: d(p_n, [\ell(x), u(x)])\leq \delta\}
$$ $$
A_n\equiv \{x \in X: d(p_n, [\ell(x), u(x)])=0\}
$$ where: $A$ is non-empty. $(p_n)_n$ is a sequence of  reals taking values in $[0,1]$ . $\ell(\cdot)$ and $u(\cdot)$ are real  function taking values in $(0,1)$ with $\ell<u$ . $d\big(p_n,  [\ell(x), u(x) ] \big):= \inf \big\{|p_n - y| : y \in [\ell(x), u(x) ] \big\}$ . Let $$
d_H(A, B)\equiv \max\{\sup_{x\in B}d(x,A), \sup_{x\in A}d(x, B)\},
$$ denote the Hausdorff distance. Can you give me a simple example of why $d_H(A,A_n)$ may be different from zero?","['limits', 'sequences-and-series', 'hausdorff-distance', 'real-analysis']"
4898978,Exercise on Girsanov's theorem,"currently I am trying to solve the following exercise. Exercise: Let $b : \mathbb R → \mathbb R$ be Lipschitz, and let $t \mapsto X(t)$ be the unique strong solution of the 1-dimensional SDE given by $$
dX(t) = b(X(t)) dt + dB(t), \quad X(0) = x \in \mathbb R
$$ Use Girsanov's theorem to show that for any $M < \infty$ , $x \in \mathbb R$ and $T > 0$ , we have that $P(X(t) > M) > 0$ . Choose $b(x) = -r$ , where $r>0$ is a constant. Prove that, for all x, we have that $\lim_{t \to \infty} X(t) = -\infty$ almost surely. Compare this with the result in part (a). My Solution : My idea is that under the probability space $(\Omega, \mathcal{F}, \mathbb P)$ the solution to the SDE is given by a Brownian motion + (non-deterministic) drift. Thus using Girsanov we want to find another probability space $(\Omega, \mathcal{F},Q)$ together with the Radon Nikodym derivative $dQ^{T}/dP^{T}$ such that we can trivially bound the event. Calculation : Take arbitary $M>0$ and $T>0$ , we want to find the Radon Nikodym derivative of $(X_t + H_t : t \leq T)$ with respect to $(X_t : t \leq T)$ . Check the following. Take $N_T = \int^{T}_0 b(X_s) dB_s$ , then since $b \colon \mathbb R \to \mathbb R$ is Lipschitz, one can easily show that $\mathcal{E}(N)$ (exponential martingale) is u.i. Using the Girsanov theorem we know that the Radon Nikodym derivative $Z_T$ is given by $$
Z_T := \frac{dQ^{T}}{dP^{T}} = \exp\left(-\int^{1}_0 b(X_s) dB_s + \frac{1}{2} \int^{1}_0 b^2(X_s) ds \right)
$$ Applying this via Girsanov, while denoting $A = \{X(T) > M\}$ gives that $$
P(A) = \mathbf{E}_{P}\left(\mathbf{1}_{A} Z^T \right) = Q\left(\{B(T) > M\}\right) > 0,
$$ where $B$ is a $Q$ -Brownian motion and the last inequality follows easily using the definition of Brownian motion. For the second item, we can easily verify the limit via for example Borel Cantelli. We don't get a contradiction with respect to (a), since Girsanov theorem only gives a Radon Nikodym derivative up to some fixed time $t \leq T$ . Is this essentially correct? I am not 100% sure, since the drift of my underlying process also depends on the process itself and is non-deterministic. How does this compare to the usual case when one has deterministic drift?","['stochastic-processes', 'solution-verification', 'probability-theory', 'stochastic-calculus']"
4898994,"Prove via contraposition that if $(a_n)$ is decreasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim_{n\to\infty}na_n=0$","""Normal way"" Let $(a_n)_{n\in\mathbb{N}}$ be a monotonically decreasing sequence of non-negative real numbers.
Furthermore, let $\sum_{n=1}^\infty a_n$ be convergent. To show is $\lim_{n\to\infty}n\cdot a_n=0$ Cauchy's convergence test gives us $$\forall \varepsilon>0 \quad \exists N\in\mathbb{N} \quad \forall m > n \ge N \colon \quad \sum_{k=n+1}^m a_k < \varepsilon$$ Since $(a_n)_{n\in\mathbb{N}}$ is monotonically decreasing, we get $$(m-n)a_m\leq\sum_{k=n+1}^m a_k < \varepsilon$$ Now, let $m\geq 2N$ and $n:=\left\lfloor\frac{m}{2}\right\rfloor$ , which satisfies $m > n \ge N$ , so we get $$\frac{m}{2}a_m\leq(m-n)a_m< \varepsilon$$ So, $\lim_{n\to\infty}\frac{n}{2}\cdot a_n=0$ and thus $$\lim_{n\to\infty}n\cdot a_n=0$$ Contraposition This wasn't a big deal, however I'm wondering, whether the statement can also be proven directly via its contraposition, so: Let $(a_n)_{n\in\mathbb{N}}$ be a monotonically decreasing sequence of non-negative real numbers.
Furthermore, let $\lim_{n\to\infty}n\cdot a_n\neq0$ To show is that $\sum_{n=1}^\infty a_n$ is not convergent. By definition, we have $$\exists \varepsilon>0 \; \forall N\in\mathbb{N} \; \exists n \ge N\colon \; n\cdot a_n\geq\varepsilon$$ However, this only gives us $a_n\geq\frac{\varepsilon}{n}$ , and then due to $(a_n)_{n\in\mathbb{N}}$ being monotonically decreasing, we get $\sum_{k=1}^n a_k\geq\varepsilon$ , and that doesn't seem to be of much use. Question Do you see how the proof via contraposition could be done?","['alternative-proof', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4899004,How to find the approximate value of $g(z) = \sum_{n=1}^\infty z^n/n^{3/2}$ near $z = 1$?,"I am trying to calculate how the chemical potential of a non-interacting Bose gas above the condensation temperature, which boils down to obtaining the leading order expansion of the function $g(z) = \sum_{n=1}^\infty \frac{z^n}{n^{3/2}}$ , for $z = 1-\epsilon$ , as a function of $\epsilon >0$ . Here is a mathematica plot of the function. I tried naively expanding the binomial series of $(1-\epsilon)^n$ , but all the coefficient of the Taylor series are divergent. For example, $\sum_{n=1}^\infty \frac{(1-\epsilon)^n}{n^{3/2}} = \sum_{n=1}^\infty \frac{(1-n\epsilon+\cdots)}{n^{3/2}} = \zeta(3/2) - \epsilon \sum_{n=1}^\infty\frac{1}{n^{1/2}} + \cdots$ . Here $\sum_{n=1}^\infty\frac{1}{n^{1/2}}$ diverges. From numerics, $\left(g(1-\epsilon) - g(1)\right) \sim -\sqrt{\epsilon}$ upto leading order (which implies, the chemical potential decreases quadratically with temperature beyond the condensation temperature). How to demonstrate this square root dependence using analytic methods?","['sequences-and-series', 'real-analysis']"
4899005,"$\mu(H)=0$ for every half plane $H$, implies the measure $\mu=0$?","A set $H$ is called a half plane if $H$ is of the form $H=\{(x,y)\in \mathbb R^2: ax+by\leq c\}$ where $a,b,c \in \mathbb R$ . Let $\mu$ be a finite complex Borel measure on $\mathbb R^2$ . If $\mu(H)=0$ for every half plane $H$ . Can we conclude that the measure $\mu=0$ ? My simple observations: We can assume $\mu$ is real valued. Let $\chi_H$ be the indicator function on $H$ , we can deduce that for every half plane $H$ , the convolution $$\chi_H*\mu(z)=0.$$","['measure-theory', 'real-analysis']"
4899019,Entire functions which satisfy a coefficient property,"Let $f$ be an entire function. Then we can write $$f(z) = \displaystyle \sum_{n=0}^{\infty}c_n z^n$$ for some $c_0, c_1, \ldots$ . For a positive real $r$ , let $$M_r = \sup \{ |f(z)| : |z| = r \}$$ . The questions are: (i) Explain why we have $$|c_n| \leq \frac{M_r}{r^n}$$ for all $n$ (ii) Now let $n$ and $r$ be fixed. For which entire functions $f$ do we have $$|c_n| = \frac{M_r}{r^n}?$$ I wanted to check if my answers were correct (mostly for (ii)). (i) $$|c_n| = \frac{1}{2\pi}\left|\int_{|w| = r} \frac{f(w)}{w^{n+1}}dw\right| \leq \frac{1}{2\pi} \cdot 2\pi r \sup_{|w| = r}\left|\frac{f(w)}{w^{n+1}}\right| = \frac{M_r}{r^n}$$ (ii) $$ |c_n| = \frac{M_r}{r^n} \iff \frac{1}{2\pi}\left|\int_{|w| = r} \frac{f(w)}{w^{n+1}}dw\right| = \frac{1}{2\pi} \cdot 2\pi r \sup_{|w| = r}\left|\frac{f(w)}{w^{n+1}}\right| \iff \left|\int_{|w| = r} \frac{f(w)}{w^{n+1}}dw\right| = \cdot 2\pi r \sup_{|w| = r}\left|\frac{f(w)}{w^{n+1}}\right| \iff \frac{f(z)}{z^{n+1}} = c, c\in \mathbb{C} \iff f(z) = c z^{n+1}$$","['complex-analysis', 'functions', 'taylor-expansion']"
4899047,Permutations of 10 players within 2 Badminton courts: Covering $10$-vertex complete graph $K_{10} $ by two disjoint $K_4$,"I am facing this everyday problem and I wanted to actually see how to formalise and reason on. We have 10 players and two courts in our badminton matches. We define a shift to be an instance of badminton games happening where two teams of 4 players are playing against each other in the respective courts and the remaining two are resting. We assume that we always are able to have the two games happening side by side in the two courts and they both end at around the same time. That is one can imagine in a permutation of these ten players in each subsequent shifts . Now I want to solve for taking the least number of shifts to get all the players to encounter each other. I can combinatorially create permutations where following 6 shifts each player encounters another. I would like to know whether there can be no possibility of getting the solution with lesser number of shifts . I want to know more on how to reason for the above problem or what tools to use to solve such a combinatorial situation? How does one go about formalising and algorithmically strategising such combinatorial problems? Thank you. My playing around with the problem solution instance and coming up with 6 shifts. Let $\{\ a,b,c,\ldots,j\ \}$ be the set of players. $a^x$ at shift $k$ indicates that player $a$ has encountered $x$ many different players by and including shift $k$ , for eg. in shift $1$ , each player playing the game has encountered three other distinct players: Shift $1:\ a^3 b^3 c^3 d^3\ -\ e^3 f^3 g^3 h^3\ -\ i^0 j^0$ Shift $2:\ a^6 i^3 e^5 f^5\ -\ b^6 j^3 g^5 h^5\ -\ c^3 d^3$ Shift $3:\ i^6 g^7 c^6 h^7\ -\ e^7 f^7 j^6 d^6\ -\ a^6 b^6$ Shift $4:\ a^8 g^9 h^9 d^8\ -\ e^9 f^9 b^8 c^8\ -\ i^6 j^6$ Shift $5:\ a^9 c^9 j^8 e^9\ -\ b^9 i^8 d^9 f^9\ -\ g^9 h^9$ Shift $6:\ a^9 b^9 i^9 j^9\ -\ c^9 d^9 g^9 h^9\ -\ e^9 f^9$ I am quite sure taking out $ij$ in the 4th shift is where it took more shifts!","['combinatorial-designs', 'permutations', 'graph-theory', 'popular-math', 'combinatorics']"
4899054,"Set theory: how do I ""distribute"" overlapping sets?","Let me preface by saying that I'm a software engineer; I'm neither a mathematician nor a logician in the academic sense. So I apologize if terms / symbols are not accurate for representing sets. EDIT: I agree with commenters that the original text I had was worded way too confusingly. I've boiled it down to what I think is the simplest example I can think of. Say I have this sequence/set: abcd , with each letter representing a distinct item. In the domain language I'm using, any item or groups of items can be extended. So, say I extend b with e . That means that we need to represent 2 new sequences: abcd and aecd , but a proper way to structure this would be a(b|e)cd with the group of items that will be distributed across sequences. If I were to extend cd with f , then the list would be a(b|e)(cd|f) as the smallest possible representation of this list, which represents all list combinations. So, let's start again with abcd . Given abcd , here is a list of extensions to be made, with the bracketed search (the ""find"") and => listing what it is extended with: [abc]d => h abc[d] => e ab[c]d => g ab[cd] => f Meaning: [abc] will be [abc] OR [h] -- represented as (abc|h)d [d] will be [d] OR [e] -- taking prev. as input, represented as (abc|h)(d|e) [c] will be [c] OR [g] -- (ab(c|g)|h)(d|e) [cd] will be [cd] OR [f] -- ??? I'm stumped as to how to determine the simplest representation of combinations of items for the last one. If I were to substitute the last item first, it would be ab(cd|f) , but once lists are overlapping, I'm unsure how to mathematically / algorithmically determine the smallest possible set representation. ORIGINAL QUESTION: To try to boil this problem down, I'm working on a stylesheet language. Let's say I have this set:
{ .one, .two, .three.four } (For the purpose of simplification, consider these strings, or arrays of strings i.e. it's represented more like: { '.one', '.two', ['.three', '.four'] } I can ""extend"" this set and represent it as nested sets. So, for example: Given the above, I can extend .four with .five , this creates the nested set:
{ .one, .two, .three{ .four, .five } } (with the actual data structure in memory being like { '.one', '.two', ['.three', { '.four', '.five' }] } Continuing on, given the result of the #1, I extend .three.four with .six . This creates the set { .one, .two, { .three{ .four, .five }, .six } } Continuing, I extend .three with .seven , and I the resulting set should be: { .one, .two, { { .three, .seven }{ .four, .five }, .six } } Now, the following is where my brain breaks. I want to extend {.one, .two, .three} with .eight . In other words, had I started with { .one, .two, .three } and I extended with .eight , the end result would be: { { .one, .two, .three }, .eight }. With the result from #2, however, I need to redistribute such that I end up with a set like: { .one, .two, { { .three, .seven }{ .four, .five }, .six}, { .eight{ { .four, .five } } } or { { .one, .two, .three }, .eight }{ { .three, .four }, { .one, .two, { { .six{ .four, .five }, .six } } So, I know what the possible results should be for the smallest grouping of distributed sets. Another way to phrase the problem (maybe this is simpler to demonstrate): Say I have a set  { abxy }. I want to extend { ab } with { c }, which would result in { abxy, cxy }. However, I want to represent this as the simplest set: { (ab|c)xy } Now, I want to extend { bx } with { d }. Again, if this were distributed, it would be { abxy, cxy, dy }. However, how do I algorithmically calculate the simplest set, and duplicate items that need to be distributed? I can represent this as { ((ab|c)x|d)y } as the simplest possible form, but then how do I extend { cx } with { e } or { xy } with { f } from that simple form? I hope one or both of those explanations make sense. Any help figuring this out algorithmically would be appreciated!","['elementary-set-theory', 'set-theory']"
4899107,Bayesian Inference Intractability,"When looking at Bayesian posteriors $$
  p(z \mid x) = \frac{p(x \mid z)p(z)}{\int p(x \mid z')p(z')dz'}
$$ The denominator commonly intractable. I understand this is due to the possibility of high dimensionality, but I'm not sure why a Monte-Carlo estimate approach is not commonly used
e.g. $$
\int p(x \mid z')p(z')dz' = \mathbb{E}_z(p(x \mid z)) 
\approx \frac{1}{L}\sum_{i=1}^{L} p(x \mid z_i)
$$ Particularly when we usually choose a prior $p(z)$ which we can easily sample. Any help would be appreciated.","['statistical-inference', 'statistics', 'monte-carlo', 'bayesian', 'machine-learning']"
4899149,Differential 1-forms over an (hyper)elliptic curve,"Given am elliptic curve $E: y^2=x^3+ax+b$ , a nice basis for $\Omega^1$ (for the holomorphic 1-forms) might be $\{\frac{dx}{y}\}$ . Given a hyperelliptic curve $H: y^2=P(x)$ where $P(x)$ is of degree $2g$ or $2g+1$ , by this MSE post (user8268 answered) , a suitable basis for the holomorphic 1-forms is $\frac{x^kdx}{y}$ , $k=0,\dots,g-1$ . In the same fashion of the elliptic curve case, we know we can identify $H$ with a $g$ -dimensional complex torus whose generetors are the periods $\int_{\gamma_{1,k}} \frac{x^kdx}{y}$ and $\int_{\gamma_{2,k}} \frac{x^kdx}{y}$ where $\gamma_{1,k}$ and $\gamma_{2,k}$ (for all $k=0,\dots,g-1$ ) are basis elements of $H_1$ the first homology. A few days ago I read the book (the theorem I read is in the beggining of 4.3.6): Bifurcations of Planar Vector Fields and Hilbert's Sixteenth Problem (I think the specific theorem I'm reffering to changed a bit in some newer versions). Here is the theorem (yellow): So we got a relation between the integrals of $\frac{x^kdx}{y}$ and the integrals of $x^kydx$ .
But who are $x^kydx$ ? where did they come from? That is the first time I saw a use for these forms. Thanks in advance.","['algebraic-geometry', 'elliptic-integrals']"
4899186,Zariski tangent spaces over residue fields,"I came up with the following question while looking at the definition of Zariski tangent spaces. Let $R$ be a noetherian ring, and let $\mathfrak{m}\subset R$ be a maximal ideal. Write $k:=R/\mathfrak{m}$ as the residue field. Denote by $\tilde{\mathfrak{m}}$ the unique maximal ideal in the local ring $R_\mathfrak{m}$ . Then it is well known that $R_\mathfrak{m}/\tilde{\mathfrak{m}}$ is canonically isomorphic to $k$ . Hence, both $\mathfrak{m}/\mathfrak{m}^2$ and $\tilde{\mathfrak{m}}/\tilde{\mathfrak{m}}^2$ have canonical structures as $k$ -vector spaces. Question : it is true that $\dim_k(\mathfrak{m}/\mathfrak{m}^2)=\dim_k(\tilde{\mathfrak{m}}/\tilde{\mathfrak{m}}^2)$ ? If so, are they canonically isomorphic as $k$ -vector spaces? If not, is there a way to add some extra hypotheses to make this true? Say, assume $k$ is algebraically closed, or assume $R$ is a finitely generated $k$ -algebra, etc. The reason why I ask this is the following: in my algebraic geometry class we proved that if $X$ is an affine variety over $k$ , then the $k$ -vector space of derivations at a point $p$ is isomorphic to $\mathfrak{m}_p/\mathfrak{m}_p^2$ , where $\mathfrak{m}_p\subset \mathscr{O}(X)=k[X]$ is the maximal ideal of global regular functions vanishing at $p$ . It is not the maximal ideal of local germs vanishing at $p$ . This is different from the usual definition of Zariski tangent spaces, and I would like to reconcile these two definitions. Any help would be greatly appreciated. Thanks!","['algebraic-geometry', 'commutative-algebra']"
4899201,Rudin theorem $7.8$,"There is the definition of $(D\mu)(x)$ : Accordingly, let us fix a dimension $k$ , denote the open ball with center $x\in\mathbb{R}^k$ and radius $r>0$ by $$B(x,r)=\{y\in\mathbb{R}^k:\lvert y-x\rvert<r\}$$ (the absolute value indicates the euclidean metric, as in Sec. 2.19), associate to any complex Borel measure $\mu$ on $\mathbb{R}$ the quotients $$(Q_r\mu)(x)=\frac{\mu(B(x,r))}{m(B(x,r))},$$ where $m=m_k$ is Lebesgues measure on $\mathbb{R}^k$ , and define the symmetric derivative of $\mu$ at $x$ to be $$(D\mu)(x)=\lim_{r\to0}(Q_r\mu)(x)$$ at those points $x\in\mathbb{R}^k$ at which this limit exists. There is the definition of Lebesgue points: If $f \in L^{1}(\Bbb R^{k})$ , any $x \in \Bbb R^{k}$ for which it is true that $$ \lim_{r\to 0}  \frac{1}{m(B_r)} \int_{B(x,r)} |f(y) - f(x)|\  dm(y) = 0$$ is called a Lebesgue point of $f$ There is the theorem:
Suppose $\mu$ is a complex Borel measure on $R^k$ , and $\mu$ is absolutely continuous with respect to $m$ . Let $f$ be the Radom-Nikodym derivative of $\mu$ with respect to $m$ . Then $D\mu = f$ a.e. and $$\mu(E) = \int_E (D\mu)dm $$ ( mark this by $(1)$ )
for all Borel sets $E \subset R^{k}$ . The Radon-Nikodym theorem asserts that $(1)$ holds with $f$ in place of $D\mu$ . At any Lebesgue point $x$ of $f$ , it follows that $$f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm = \lim_{r\to 0} \frac{\mu(B(x,r))}{m(B(x,r))}. $$ Thus $(D\mu)(x)$ exists and equals $f(x)$ at every Lebesgue point of $f$ , hence a.e. I don't understand how do we get that $f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm$ at any Lebesgue point $x$ of $f$ . Any help would be appreciated.","['measure-theory', 'analysis', 'real-analysis', 'complex-analysis', 'functional-analysis']"
4899220,Sublinear function $f$ with $f(x)+f(-x)=0$ for some $x \neq 0$ is linear.,"A sublinear function $f$ has the following properties: $f(x+y) \leq f(x) + f(y)\;\; \forall x,y \in \mathbb R^d$ for any $t > 0, x \in\mathbb R^d, f(tx) = tf(x)$ Now let $f(x_0) + f(-x_0) = 0$ at some point $x_0 \neq 0$ , I want to prove that $f$ is actually linear. I have proved $f(0)=0$ by simply let $x = y = 0$ and $x = x_0, y= -x_0$ , but I don't know how to proceed. Could someone help me?","['convex-optimization', 'convex-analysis', 'linear-algebra', 'real-analysis']"
4899256,Does such a thing as $\lim_\limits{g(x)\to a} f(x)$ exists?,"Does such a thing as $\lim_\limits{g(x)\to a} f(x)$ exists? I thought about this out of thin air while messing with limits on W|A, of course it didn't output anything when I tried, but I thought it was an interesting concept, at first I thought about $\lim_\limits{x^2\to 4}x$ , and reached the the conclusion that $\lim_\limits{x^2\to 4}x = 2$ , since $x = 2 \rightarrow x^2 = 4$ , but I don't think my logic was correct there because what I did was basically $\lim_\limits{x\to2}x^2=4$ . Then I decided to ask some of my professors if $\lim_\limits{x^2\to a} f(x)$ is something that exists, two of them said it might be $\lim_\limits{x\to a} \sqrt{f(x)}$ , or it was $\lim_\limits{x\to a} f(\sqrt{x})$ , now I'm not 100% sure what they said but no one was sure about this.","['limits', 'calculus']"
4899263,"Proving the $f^{-1}$, $f_*$ adjunction of sheaves directly","This question asks about proving the adjunction between $f^{-1}$ and $f_*$ for sheaves by showing that the unit and counit have the necessary properties by looking at stalks. I understand the answer there as to why it suffices to look at stalks, but I get just as lost in the notation and the various direct limits when looking at stalks as I do when looking at sections. So, how do we see that the unit and counit identities hold by looking at stalks? If it is helpful, I've defined my $\eta_\mathscr{G} : \mathscr{G} \to f_* f^{-1} \mathscr{G}$ to be the morphism given by the canonical maps $\mathscr{G}(U) \to \varinjlim_{V \supset f(f^{-1}(U))} \mathscr{G}(V)$ , which exist since $U \supset f(f^{-1}(U))$ ; and my $\varepsilon_{\mathscr{F}} : f^{-1} f_* \mathscr{F} \to \mathscr{F}$ to be the morphism defined by the maps $\varinjlim_{V \supset f(U)} \mathscr{F} (f^{-1} (V)) \to \mathscr{F}(U)$ given by restriction $\mathscr{F}(f^{-1}(V)) \to \mathscr{F}(U)$ on every $V$ , and the universal property of colimits.","['algebraic-geometry', 'sheaf-theory']"
4899318,Asymptotic Behaviour of the Remainder of Certain Alternating Series,"Let $a,b >0$ be real constants. Empirical observation (as in: asking WolframAlpha) suggests $$   \lim_{n\to \infty} n \cdot \sum_{k=0}^\infty (\frac{1}{n+ak} - \frac{1}{n+b+ak}) = \frac{b}{a} \tag{$*$}$$ Note that the series in question can be interpreted as the ""remainder"" / ""tail end"" of a (conditionally!) convergent alternating series. To see what is going on, confirm that e.g. ( $a=3, b=1, n=1000$ ) $$1000 \cdot (\frac{1}{1000}-\frac{1}{1001}+\frac{1}{1003}-\frac{1}{1004}+\frac{1}{1006}-\frac{1}{1007} + \dots) \approx \frac{1}{3}$$ Questions: Is there a better proof for $(*)$ than my attempt at the bottom of this question? What references treat estimates for the asymptotic behaviour of ""remainders"" like the above? Would there be finer estimates, after subtracting the above error ""of order $1/n$ "", of order $1/n^2$ , $1/n^3$ , ...? Context : A student challenged me to (show existence of, and) find $$\lim_{n\to \infty} n \cdot \int_0^{\frac{\pi}{4}}\tan^n(x) dx$$ Now the easy and well-known recursive formula $$\int \tan^{n}(x)dx = \frac{1}{n-1}\tan^{n-1}(x) - \int \tan^{n-2}(x) dx$$ gives $$\int_0^{\frac{\pi}{4}}\tan^n(x) dx = \begin{cases} \frac{1}{n-1}-\frac{1}{n-3}+\frac{1}{n-5} - \dots \pm 1\mp \frac{\pi}{4} \text{ if } n \text{ even}\\ \frac{1}{n-1}-\frac{1}{n-3}+\frac{1}{n-5} - \dots \pm \frac{1}{2}\mp \frac{\ln(2)}{2} \text{ if } n \text{ odd}\end{cases}$$ This is a funny case distinction because of course $\frac{\pi}{4} = 1-\frac{1}{3} +\frac{1}{5} \dots $ (Leibniz-Madhava) while $\frac{\ln(2)}{2} = \frac{1}{2}-\frac{1}{4}+\frac{1}{6} -\dots$ , so in both cases , the question becomes to estimate the "" $1/n$ -order"" growth of the ""tail end"" of the series, $$\lim_{n\to \infty} n \cdot (\frac{1}{n+1}-\frac{1}{n+3}+\frac{1}{n+5} \dots)$$ which by a slight adjustment of $(*)$ (for $b=2, a=4$ ) is $\dfrac{1}{2}$ . My own proof idea for $(*)$ : By a standard estimate for convergent alternating series, for $K$ big enough (e.g. $n+aK > n^2$ ) we have $$\sum_{k=0}^\infty (\frac{1}{n+ak} - \frac{1}{n+b+ak}) = \sum_{k=0}^K(\frac{1}{n+ak} - \frac{1}{n+b+ak}) + O \left(\frac{1}{n^2}\right)$$ and now that finite sum can be split up into its positive and negative terms, and we get $$\sum_{k=0}^K \frac{1}{n+ak} - \sum_{k=0}^K \frac{1}{n+b+ak}\\ =\frac{1}{a} \left(\sum_{k=0}^K \frac{1}{\frac{n}{a}+k} - \sum_{k=0}^K \frac{1}{\frac{n+b}{a}+k} \right)\\ \stackrel{**}\approx \frac{1}{a} \left((\ln (\frac{n}{a}+K) -\ln(\frac{n}{a})) - (\ln(\frac{n+b}{a}+K) - \ln(\frac{n+b}{a})) \right) \\
= \frac{1}{a}\ln(\frac{n+b}{n}) + \frac{1}{a}\ln(\dfrac{\frac{n}{a}+K}{\frac{n+b}{a}+K})$$ Since we can choose $K$ as big as we want, the second term becomes irrelevant. So when we take the limit $n\to \infty$ , the whole things behaves like $\frac{n}{a}\ln(\frac{n+b}{n}) = \frac{1}{a}\ln((1+\frac{b}{n})^n)$ which of course goes to $\frac{b}{a}$ . To justify $**$ up to $O(\frac{1}{n^2})$ : Taylor expansion says that for big enough $c$ (namely, $\frac{n}{a}+k$ and $\frac{n+b}{a}+k$ ), $$\ln(\frac{c+1}{c}) = \frac{1}{c} - \frac{1}{2c^2} + \frac{1}{3c^3} - \dots$$ so that $$(\ln (\frac{n}{a}+K) -\ln(\frac{n}{a})) - (\ln(\frac{n+b}{a}+K) - \ln(\frac{n+b}{a})) - \left(\sum_{k=0}^K \frac{1}{\frac{n}{a}+k} - \sum_{k=0}^K \frac{1}{\frac{n+b}{a}+k}\right) \\ = -\frac{1}{2} \underbrace{\left( \sum_{k=0}^K \frac{1}{(\frac{n}{a}+k)^2} - \sum_{k=0}^K \frac{1}{(\frac{n+b}{a}+k)^2}\right)}_{<\frac{1}{(\frac{n}{a}+K)^2} \in O(1/n^2)} + \frac{1}{3}\underbrace{\left( \sum_{k=0}^K \frac{1}{(\frac{n}{a}+k)^3} - \sum_{k=0}^K \frac{1}{(\frac{n+b}{a}+k)^3}\right)}_{<\frac{1}{(\frac{n}{a}+K)^3} \in O(1/n^3)} +\dots $$ with the estimates in the bottom again by interpreting these as alternating series.","['estimation', 'reference-request', 'real-analysis', 'sequences-and-series', 'alternating-expression']"
4899344,Topology of sets defined by real-valued functions (again),"This is a follow-up to this previous question , but a bit more specific. Suppose I have a simple Euclidean space $X = \mathbb{R}$ , or $X = \mathbb{R}^2$ , or $X = \mathbb{R}^3$ , and a continuous real-valued function $f:X\to \mathbb R$ . I can define sets like: \begin{align}
A &= \{x \in X: f(x) = 0 \} \\
B &= \{x \in X: f(x) \le 0 \} \\
C &= \{x \in X: f(x) < 0 \} \\
\end{align} It would be nice if we could say that $A$ is the boundary of $B$ , or that $C$ is the interior of $B$ . Those statements are not true, in general. If we want them to be true, what extra conditions do we have to impose on $f$ . For example, what if $X = \mathbb{R}$ and $f$ is strictly monotone. Or how about if $f(x) = \|x\| - 1$ . Any norm? Or what if $f$ is (strictly?) convex, or Lipschitz, or a polynomial? Or $f(x) = $ signed distance from $x$ to some (convex?) set $S$ . I’m open to any suggestions regarding sufficient (or even required) properties of $f$ , especially ones that have some geometric flavor. The answer to my previous question says that $f$ being an open map is sufficient, but that's typically difficult for me to check, so I'm looking for other sufficient conditions. This paper looks relevant, for the case where $f$ is differentiable, but I haven’t managed to wade through all of it yet. This answer says that if $f$ is convex, then $\overline{C} = B$ . Context: these sorts of sets are used to model shapes in computer systems, so I’m interested in their topology.","['geometry', 'real-analysis', 'continuity', 'general-topology', 'constraints']"
4899435,Question in the proof of the Riesz Representation theorem of non-negative functionals in geometric measure theory written by Leon Simon,"The problem is from the proof of Theorem 1.5.12 in Leon Simon's book: Geometric Measure Theory Suppose $X$ is a locally compact Hausdorff space, $\mathcal{K}^{+}$ is the set of all non-negative continuous functions on $X$ with compact support, $\lambda:\mathcal{K}^{+}\to [0,\infty)$ is linear. $U$ open in $X$ , $K$ compact in $X$ , $A\subset X$ . we define $$\mu(U)=\sup\{\lambda(f):f\in\mathcal{K}^{+},f\leq 1,supp(f)\subset U\}$$ $$\mu(A)=\inf\{\mu(U):U\mbox{ open},U\supset A\}$$ Prove that $\mu(U) = \sup\{\mu(K):K\mbox{ compact}$ , $K\subset U\}$ . In Leon Simon's book, the proof of this statement is too short to understand, so I was looking for clearer and more rigorous proof. Obviously LHS $\geq$ RHS, however I cannot prove the equality. Any help will be appreciated.","['measure-theory', 'riesz-representation-theorem', 'geometric-measure-theory', 'real-analysis']"
4899453,Condition on derivatives,"I am working with a ""well-behaved"" optimization problem of the form: \begin{equation*}
\max_{x} f( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y})
\end{equation*} where $\displaystyle f:\mathbb{R}^{n+3}\rightarrow \mathbb{R}$ is continously differentiable, $\displaystyle x\in \mathbb{R}$ , $\displaystyle \mathbf{y} \in \mathbb{R}^{n}$ are parameters, and $\displaystyle g_{k} :\mathbb{R}\rightarrow \mathbb{R}$ for $\displaystyle k=1,2,3\ $ are continuosly differentiable. I am interested in studying the assumptions I could make on $\displaystyle f$ and $\displaystyle g_{k}$ to study the ""comparative statics"" of the solution around an optimum. Say that there is a function $\displaystyle x\left(\mathbf{y}^{*}\right) :\mathbb{R}^{n+1}\rightarrow \mathbb{R}$ such that: \begin{equation*}
\sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}\left( x\left(\mathbf{y}^{*}\right) ,\mathbf{y}^{*}\right) =0
\end{equation*} I am taking functional assumptions over the $\displaystyle g_{k}$ functions and study their implications, such as: \begin{gather*}
g_{1}( x) =cy_{1} x-cy_{2} ,\ c\in [ 0,1]\\
g_{2}( x) =( 1-y_{1}) x+y_{2}\\
g_{3}( x) =x
\end{gather*} The standard problem I am building on takes $\displaystyle c=0$ , so I am interested in studying what happens when I increase $\displaystyle c$ around the optimal solution. I am interested in elasticity notions, for instance: \begin{equation*}
{\textstyle \epsilon =\frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right)\frac{1-y_{1}}{x\left(\mathbf{y}^{*}\right)}}
\end{equation*} Using the Implicit function theorem (I include my derivations below), I find that $\displaystyle \epsilon $ is given by: \begin{equation*}
{\textstyle \epsilon =-\frac{\frac{1}{x} c\left(\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}}\right) +cy_{1}\left( c\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( c\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( c\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{cy_{1}\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}}
\end{equation*} I am looking for conditions that may guarantee an easy characterization of the expression above when we increase $c$ . Clearly, differentiating the expression above with respect to $\displaystyle c$ yields a messy expression, especially due to the emergence of third order derivatives. Here are ny thoughts: The first-order effect given by $\frac{1}{x}\frac{\partial f}{\partial g_{1}}$ should dominate in most solutions, meaning that the derivative should have the sign of - $\frac{1}{x}\frac{\partial f}{\partial g_{1}}$ Second-order effects may matter for high values of $\displaystyle c$ (this is clear for $\displaystyle c\rightarrow \infty $ ), but this should not be \ a concern since I assume $\displaystyle c\leq 1$ . This means that in order for the first-order effect to be dominant I need a conition on the derivatives of my $\displaystyle f$ function. My intuition is that I need something like $\displaystyle f^{( n)}$ to be decrasing in $\displaystyle n$ . Clearly, there are some ""nice"" functions that do the trick, especially those in which the second-order derivative of $\displaystyle g_{1}$ is zero, but I was wondering if there could be a more general alternative. Derivations I can approach this by applying the implicit function theorem, I know that I can study the function: \begin{equation*}
F( x,\mathbf{y}) =\sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y}) =cy_{1}\frac{\partial f}{\partial g_{1}} +( 1-y_{1})\frac{\partial f}{\partial g_{2}} +\frac{\partial f}{\partial g_{3}} =0
\end{equation*} And that if, for $\displaystyle \mathbf{y}^{*}$ such that $\displaystyle x\left(\mathbf{y}^{*}\right) =x^{*}$ , and if $\frac{\partial F}{\partial x}\left( x^{*} ,\mathbf{y}^{*}\right) \neq 0$ , then \begin{equation*}
\frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{\frac{\partial F}{\partial y_{1}} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]}{\frac{\partial F}{\partial x} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]} =-\frac{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial y_{1}}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x\partial y_{1}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)}{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial x}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x^{2}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)}
\end{equation*} In this case: \begin{gather*}
{\textstyle \frac{d\ F}{d\ y_{1}} =c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}\\
\end{gather*} \begin{gather}
{\textstyle \frac{d\ F}{d\ x} =cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right]}\\
{\textstyle +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}} \notag
\end{gather} And: \begin{equation*}
{\textstyle \frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{{\textstyle cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}}}
\end{equation*}","['implicit-function', 'derivatives', 'implicit-function-theorem', 'real-analysis']"
4899459,When is this closed set compact,"Apparently in the polish space $^\omega\omega$ a closed $K\subset\hspace{1mm}^\omega\omega$ is bounded and therefore compact if it is completely below some $f\in \hspace{1mm}^\omega\omega$ as in $K= \{ g \in K: \forall n\in \omega: g(n)\leq f(n) \}$ . I don't really understand this sort of compactness, since often the $f$ 's bounding those sets are unbounded themselves. I assume the bounded part stems from the fact that the $g(n), n\leq m$ for fixed $m$ get actually bounded. How is this proven?","['general-topology', 'polish-spaces']"
4899527,Why pullback of ideal sheaf should be the conormal sheaf?,"I'm sorry that this isn't really a math question, but this gap between my intuition and the truth bothers me. For closed subvariety (for simplicity) with ideal sheaf $\mathcal{I}$ , the pullback $i^*(\mathcal{I}/\mathcal{I}^2)$ is equal to $i^*(\mathcal{I})$ and this is a conormal sheaf. However, my interpretation of an ideal sheaf was 'functions vanishing at the subvariety', for example, it corresponds to just $I$ when the subvariety is $V(I)$ in the affine case. Therefore I thought it would be more reasonable for the pullback to be either zero (as $I$ vanishes on subvariety) or just $I$ itself (which formally doesn't make sense). In other words, it feels 'very lucky' to me that pullback suddenly sees only 'first-order terms'. I would be satisfied if some explanation could be given, at least for the simplest case, the cotangent space $m/m^2 = m\otimes_A A/m$ . This simple algebraic equality looks weird when I think about these things. Should I just ignore all this analogy as $I/I^2$ is no longer a sheaf of ideals?","['philosophy', 'algebraic-geometry']"
4899528,"What is the summation order when writing $\sum_{i,j=1}^\infty$?","Especially for a conditionally convergent series where summation order matters? If it is equivalent to $\sum_{i=1}^\infty\sum_{j=1}^\infty$ , does it mean that $\sum_{i,j=1}^\infty\ne\sum_{j,i=1}^\infty$ ? Example: I saw people claiming $$\sum_{m,n=1}^\infty\frac{(-1)^{m+n}\,mn}{(m+n)^2}=\frac{\ln 2}{6}-\frac{1}{24}$$ While it is probably correct in some way, if I ""transform"" it into $$
\sum_{m,n=1}^\infty\frac{(-1)^{m+n}\,mn}{(m+n)^2}\stackrel{?}{=}\sum_{s=2}^\infty\frac{(-1)^s}{s^2}\sum_{m=1}^{s-1}m(s-m)=\sum_{s=2}^\infty\frac{(-1)^s(s+1)(s-1)}{6s},
$$ we got a divergent series. So should I take $\sum_{i,j=1}^\infty$ as $\sum_{i=1}^\infty\sum_{j=1}^\infty$ by default? Or should we not write $\sum_{i,j=1}^\infty$ for a conditionally convergent series in the first place?",['sequences-and-series']
4899546,Derive the 0-1 law from a functional equation of a martingale limit,"I am trying to understand a proof given by Jabbour-Hattab in the paper ""Martingales and Large Deviations for Binary Search Trees"". In this paper, we consider the limit of a non-negative martingale, denoted by $M(z)$ , where $z$ can be taken as a real positive number, it is not really important at this part of the proof. The limit does exist almost surely, and in the proof we consider the case where it exists. For this limit, we want to prove the 0-1 law, i.e. $\mathbb{P}(M(z)=0)=0$ or $\mathbb{P}(M(z)=0)=1$ . In the last step of the proof, we get the following equation $\mathbb{E}(e^{-tM(z)})=\int_0^1\mathbb{E}(e^{-tzx^{2z-1}M(z)})\mathbb{E}(e^{-tz(1-x)^{2z-1}M(z)}) \ dx.$ From this, it follows for $t\to \infty$ that the equation becomes $\mathbb{P}(M(z)=0)=(\mathbb{P}(M(z)=0))^2$ , which implies the 0-1 law. I understand why the zero law follows from the last equation, but I don't see why the last equation follows from the one before. I thought about using the fact that if a non-negative random variable $Y$ satisfies $\mathbb{E}(Y)=0$ , then $Y=0$ almost surely, but I didn't come further.
Any help would be appreciated.","['limits', 'martingales', 'probability-theory', 'probability']"
4899661,Is it possible to prove that this matrix is invertible?,"I am trying to determine whether the following, $ n $ by $ n $ matrix: $$ A = \begin{pmatrix} \Delta_{11} - 1 & \Delta_{12} & ... & \Delta_{1n} \\
\Delta_{21} & \Delta_{22} - 1 & ... & \Delta_{2n} \\
... \\ \Delta_{n1} & \Delta_{n2} & ... & \Delta_{nn} - 1
 \end{pmatrix} $$ Subject to $ \Delta_{ij} \in [0,1] $ and: $$ 0 < \sum_{j=1}^{n}\Delta_{ij} < 1, \text{ } \forall i $$ Is invertible in general or not. I started out by noticing that this is equivalent to saying that the matrix $ \Delta = A + I $ does not have an eigenvalue equal to one. I was hoping that, since $ \Delta $ is ""almost a Markov matrix"" (i.e. the rows do not add up to one, but all the other properties are there), and we know that Markov matrices have an eigenvalue equal to one, then maybe I can squeeze out a proof that such an ""almost Markov matrix"" does not have an eigenvalue equal to one - but so far I've been unsuccessful.","['matrices', 'linear-algebra']"
4899682,Are differentials on their own in stochastic calculus just an abuse of notation?,"In stochastic calculus, it is often standard to write a DE in differential form, such as $\mathrm dY = H \, \mathrm dX$ for the stochastic integral $$\displaystyle\int\limits_0^t H \, \mathrm d X := \displaystyle\int\limits_0^t H_s \, \mathrm d X_s.$$ The most common sense-interpretation of an expression like $\mathrm d A = \displaystyle\sum_{k=0}^{n-1} \alpha_{k, t} \, \mathrm dX_{k, t}$ I can think of is the stochastic integral expression $$\displaystyle\sum_{k=0}^{n-1} \displaystyle\int\limits_0^t \alpha_{k, s} \, \mathrm d X_{k, s}.$$ For example: in common derivations of the Black–Scholes–Merton equation, one will see expressions like $$\mathrm d V=\left(\mu S_t \frac{\partial V}{\partial S_t}+\frac{\partial V}{\partial t}+\frac{ \sigma^2 S_t^2}{2} \frac{\partial^2 V}{\partial S_t^2}\right) \, \mathrm d t+\sigma S_t \frac{\partial V}{\partial S_t} \, \mathrm d W_t$$ appear. Thus, under this interpretation, $\mathrm d X_k$ is just the integrating function in the Itô integral. Do differentials make sense in the stochastic case or is this just an abuse of notation? In this answer , we can read that convenience is one major reason for the differential notation, but note that the respondent never assigns any meaning to stochastic differentials: ""[I am] NEVER going to assign meaning to an expression like $\mathrm dW_t$ on its own. I will ONLY use it in a context where a corresponding integral expression makes sense."" Some literature in finance will define a stochastic differential as $$\mathrm d z(t) := \lim _{\Delta t \to 0} \Delta z=\lim _{\Delta t \to 0} \sqrt{\Delta t} \tilde{\epsilon},$$ but clearly this is the same kind of informal kludgely hand-waving (the limit is obviously zero!) you hear about before learning about how differentials in analysis are constructed (via the exterior derivative, $k$ -forms, etc.). Some books even include expressions involving sqaures of differentials, $$(\mathrm dB_t)^2 = (B_{t+\mathrm dt}-B_t)^2 = \mathrm dt$$ which are also left vacantly undefined. One can first attempt to formalize random differentials in the following manner:
assume a smooth manifold $\mathcal M$ . Now consider a fiber bundle $\pi \colon E \to \mathcal M$ , in this case we want the cotangent space $E \cong T^*\mathcal M$ . We can then define the randomization of $\Gamma(E) \cong \operatorname{Hom}\left(\mathcal M, E\right)$ as $\Gamma(E) \mapsto \Omega \times \Gamma(E)$ , for some probability space $\Omega$ . Finally a random $1$ -form is just a map $$\begin{align*}\Omega \times M &\to \Gamma(T^* \mathcal M) \cong \operatorname{Hom}\left(\mathcal M, T^*\mathcal M\right) \\
 \gamma &\colon (\omega, x) \mapsto \alpha_\omega \end{align*},$$ giving us one form to integrate for every $\omega \in \Omega$ . The map $\gamma$ could then be integrated against, yielding a random variable: $$\displaystyle\int_{\mathcal M} \gamma(\omega, x) =  \mathbb X(\omega).$$ Now, this only yields a random variable $\mathbb X$ and not a stochastic process $\left\{X_t \right\}_t$ . As the theory the theory of forms is generally very well-understood, it seems like the generalization to stochastic forms should not be that hard. The machinery and tools differential forms generalize to the language of chain complexes, co-chain complexes, cohomology, the cup product $\smile$ and cap product $\frown$ , the $\operatorname{Tor}$ and $\operatorname{Ext}$ functor, and so on. It seems to be very useful to be able to be able to express your theory in these terms, hence my motivation for trying to formalize stochastic differentials. Can we formalize stochastic differentials, have this been done and what constructions have been attempted? How can we understand a symbol like $\mathrm dW_t$ ?","['stochastic-integrals', 'stochastic-calculus', 'stochastic-differential-equations', 'probability-theory', 'differential-forms']"
4899743,Writing a general solution to differential equation with Bessel functions,"Consider a mass $m$ is placed on a horizontal level surface and attached to a spring, whose other end is attached to a vertical wall. The mass moves in a viscous medium, where the resistance force acting on it is proportional to the velocity: $\vec{F}_p=-\gamma\vec{\nu}$ , where $\gamma$ is a known positive constant. Due to aging, the spring stiffness coefficient decreases exponentially over time: $k\left ( t\right ) = k_0\mathrm{e}^{-\alpha t}$ , where $k_0$ and $\alpha$ are other known positive constants. Write down the differential equation describing the motion of the equilibrium position derived mass (in the one-dimensional case) and find its general solution $x(t)$ , expressing it in terms of Bessel functions. Hint : Use the substitution $s=\mathrm{e}^{-\alpha t/2}.$ I tried applying Newton's second law and got $m\ddot{x}=-k_0e^{-\alpha t}x-\gamma\dot{x} \implies \ddot{x}=-\frac{k_0}me^{-\alpha t}x-\frac\gamma m\dot{x}$ , if correct. Then I denote $\begin{aligned}&\omega_0=\sqrt{k_0/m}. , \beta=\frac\gamma m\end{aligned}$ , and get $\ddot{x}=-\omega_0^2e^{-\alpha t}x-\beta\dot{x}$ . I don't have ideas on how I apply the substitution and where to use the Bessel functions? Any help?","['physics', 'classical-mechanics', 'ordinary-differential-equations', 'bessel-functions']"
4899766,"Convolution between $f$ and $g$, with $g$ being in the Schwartz class. Does it follow that $f \ast g \in C^\infty$?","Usually, the convolution between two functions $f,g$ defined on $\mathbb R^n$ is given by $$ (f \ast g)(x) = \int_{\mathbb R^n} f(x-y)g(y) \, dy.  $$ Right now I am wondering about a specific property of the convolution between two functions. More precisely, I am wondering about the validity of the following affirmation: Let $f,g$ be two functions defined on $\mathbb R^n$ such that the convolution $f \ast g$ is well-defined. Moreover, assume that $g \in \mathcal S(\mathbb R^n)$ , where $\mathcal S(\mathbb R^n)$ stands for the usual class of infinitely differentiable functions that decrease rapidly at infinity (Schwartz class).  Then, we have that $f \ast g \in C^\infty$ . Context. I am reading an article and trying to understand some of its results. In one of the proofs, the authors seem to use the property that I've highlighted above. The only extra assumption that these authors have is that $f$ is a locally $p-$ integrable function on $\mathbb R^n$ , that is, $f \in L^p_{\text{loc}}(\mathbb R^n)$ (I am not sure if this assumption is relevant to the highlighted property). I am not very interested in a detailed proof to my question, more like a YES/NO answer so that I can move on in the article. Ideally I am looking for a reference for this kind of results, if someone is aware of one. Thanks for any help in advance.","['convolution', 'schwartz-space', 'functional-analysis']"
4899775,"If $X$ is a continuous supermartingale, why is, for every $N$, $(X_s)_{s\leq N}$ uniformly integrable?","If $X$ is a continuous supermartingale, why is, for every $N$ , $(X_s)_{s\leq N}$ uniformly integrable? This statement is used, without proof, in another statement. I haven't been able to prove it. Is it an easy argument? Or does it need some work?","['martingales', 'uniform-integrability', 'probability-theory']"
4899797,"Finding a closed form for $ \int_0^1 \frac1x \ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x $","I want a closed form for the following integral $$
\int_0^1 \frac1x\;\ln\left(\frac {\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x
$$ An integration by parts attempt on my end led to the following integral $$
\int_0^1 \frac1x \;\ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right) \, \mathrm{d}x 
= \int_0^1 \ln(x)\left(\frac{1}{(1-x)\ln\left(\frac{1-x}{2}\right)} 
+ \frac{1}{(1+x)\ln\left(\frac{1+x}{2}\right)}\right) \, \mathrm{d}x
$$ I have seen variations of these integrals, and usually they are solved by Feynman's trick.  I think the first one admits to a nice result, but I am not sure on how to handle the second one. If anyone has any ideas please let me know, there might be a substitution I am not seeing? Apologies for the small integral sign for such a big integrand, I am unsure on how to make them bigger.","['definite-integrals', 'logarithms', 'real-analysis', 'complex-analysis', 'calculus']"
4899806,Feynman-Kac theorem of the weak solution of parabolic PDEs,Is there any reference on the Feynman-Kac theorem of the weak solution of parabolic PDEs? So far I can only find the one for classical solution.,"['parabolic-pde', 'reference-request', 'stochastic-differential-equations', 'partial-differential-equations', 'probability-theory']"
4899817,"If $f:\mathbb{Z}\times\mathbb{Z}\to\mathbb{Z}$ is defined by $f((a,b))=b$, then does $f \circ f$ exist?","Let $f: \mathbb{Z} \times \mathbb{Z} \rightarrow \mathbb{Z}$ be defined by $f((a,b))=b$ . I am confused as to whether $f \circ f$ exists here. I was taught that for this composite function to exist, the codomain of the inner function must be equal to the domain of the outer function. Now I understand $\mathbb{Z}$ is not equal to $\mathbb{Z} \times \mathbb{Z}$ , however since $f$ in this case doesn't even use $a$ , is it at all possible for $f \circ f$ to indeed exist?","['functions', 'discrete-mathematics', 'function-and-relation-composition']"
4899823,"Completeness of $X$ with normal distribution $N(\theta,\theta^2)$ with equal mean and standard deviation: Integral of scale of a function","Suppose that $f:\mathbb{R} \to \mathbb{R}$ is a Borel function such that, for every $a > 0$ , we always have: $$\begin{equation*}
\int_{\mathbb{R}}f(ax)\exp \left(-\frac{1}{2}(x-1)^2\right)dx =0
\end{equation*}$$ Does it follow that $f = 0$ almost surely ? Does the answer change if we limit the attention only to $f \in L^{\infty}$ ? The background of the question is that, suppose $X$ obeys the distribution $N(\theta, \theta^2)$ for some $\theta> 0$ and we wish to determine if $X$ is a (boundly) completes statistic for $\theta$ . Expanding the definition out with a change of variable will give the statement of the problem above. I am looking for an analysis solution . Intuitively, the exponential centers at $1$ and the scale is not centered at $1$ . A counterexample would be surprising but I cannot prove it.","['statistics', 'analysis', 'real-analysis', 'functional-analysis', 'probability']"
4899868,Solving the Secretary Problem using Simple Math,"In our computer coding class, we learned about the Secretary Problem ( https://en.wikipedia.org/wiki/Secretary_problem ). The goal of this problem is to find out the ""optimal cutoff"" for an interview process: What is the least number of candidates you need to interview that maximizes your chance of finding the best candidate? If there are $n$ candidates (assume their skill level is randomly distributed) and you rank the first $1/e$ % of candidates (i.e 37%) and then pick the candidate better than the best of the $1/e$ set, there is a $1/e$ % chance you will pick the best candidate in total. We learned how to write a simulation for this: library(ggplot2)
n <- 100
n_simulations <- 1000
first_set_size <- round(n / exp(1))
results <- data.frame(iteration = integer(), category = character(), candidate = integer())

for (i in 1:n_simulations) {
    candidates <- sample(1:n)
    first_set <- candidates[1:first_set_size]
    remaining_set <- candidates[(first_set_size + 1):n]
    
    best_in_first_set <- max(first_set)
    stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]
    
    results <- rbind(results, 
                     data.frame(iteration = i, category = ""Best Candidate"", candidate = n),
                     data.frame(iteration = i, category = ""Best in First Set"", candidate = best_in_first_set),
                     data.frame(iteration = i, category = ""Stopping Rule Candidate"", candidate = stopping_rule_candidate))
}


average_best_picked <- mean(results $candidate[results$ category == ""Stopping Rule Candidate""] == n)


ggplot(results, aes(x = iteration, y = candidate, color = category)) +
    geom_jitter(width = 0.3) +
    labs(x = ""Iteration"", y = ""Candidate Number"", 
         title = paste(""Average times best candidate picked:"", round(average_best_picked, 2)),
         colour = ""Legend"") +  # Change the legend title here
    scale_color_manual(values = c(""Best Candidate"" = ""red"", ""Best in First Set"" = ""blue"", ""Stopping Rule Candidate"" = ""green"")) +
    theme_minimal() +
    theme(legend.position = ""bottom"") I am now trying to understand the mathematical solution to this problem but I am getting confused how to do this. I came up with the following logic for $N$ candidates. Suppose we have an integer number line from $0$ to $N$ that looks like this: $$[(0,R) , (R+1, N)]$$ If we find the 2nd best candidate between $(0,R)$ , we are guaranteed to find the best candidate between $(R,N)$ . Condition 1: Relative to the entire set of candidates, the probability of finding the 2nd best candidate between $(0,R)$ is $\frac{R}{N}$ Condition 2: The probability of finding the overall best candidate between $(R+1, N)$ is $1 - \frac{R}{N}$ . My logic is that $\frac{R}{N}$ is the probability of finding the overall best candidate in $R$ trials. So the probability of finding the overall best candidate outside of this set is $1 - \frac{R}{N}$ To find the overall best candidate according to the rules of this problem, both of these conditions have to be simultaneously true (i.e. multiply them by each other, joint probability) Our goal is now to find out the optimal $R$ . Let's call this optimal value as $R^*$ . We can write a sum to express this as: $$ P(R = R^*) = \sum_{R} \left[ \frac{R}{N} \cdot (1- \frac{R}{N} \right)] $$ However from here I am stuck. I think a derivative needs to be taken to find out which value of $R$ optimizes this equation. But I am not sure how to then prove that there will be a $1/e$ chance of finding the best candidate after reject the first $1/e$ % of all candidates. Any ideas what I can do? PS: Our prof showed us a simulation which shows what happens when you try different ""Stopping Rules"" (we can see that they are all suboptimal compared to 1/e): n <- 100
n_simulations <- 10000
results <- data.frame(stopping_rule = integer(), success_rate = numeric())

for (rule in 1:100) {
    success_count <- 0
    for (i in 1:n_simulations) {
        candidates <- sample(1:n)
        first_set_size <- round(n * rule / 100)
        first_set <- candidates[1:first_set_size]
        remaining_set <- candidates[(first_set_size + 1):n]
        
        best_in_first_set <- max(first_set)
        stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]
        
        if (!is.na(stopping_rule_candidate) && stopping_rule_candidate == n) {
            success_count <- success_count + 1
        }
    }
    success_rate <- success_count / n_simulations
    results <- rbind(results, data.frame(stopping_rule = rule, success_rate = success_rate))
}

best_rule <- results $stopping_rule[which.max(results$ success_rate)]
best_rate <- max(results$success_rate)


results $color <- ifelse(results$ stopping_rule == best_rule, ""red"", ""black"")

ggplot(results, aes(x = stopping_rule, y = success_rate, color = color)) +
    geom_point() +
    geom_text(data = subset(results, stopping_rule == best_rule), 
              aes(label = paste(""Best Rule: "", best_rule, ""%"")), 
              vjust = -1) +
    scale_color_identity() +
    labs(x = ""Stopping Rule (%)"", y = ""Success Rate"",
         title = paste(""Success Rate of Different Stopping Rules\nBest Rule: "", best_rule, ""% with Success Rate: "", round(best_rate, 2))) +
    theme_minimal()","['optimization', 'calculus', 'derivatives', 'probability']"
4899896,Is there a way to calculate this probability cleanly without brute force?,"Let $L$ and $a$ be positive integer constants. I will flip a coin $L$ times. Let $n_1, \ldots, n_s$ be the count of consecutive runs of the same flip. For example, if $L = 5$ and I flipped $HHTHT$ , then $n_1 = 2, n_2 = 1, n_3 = 1, n_4 = 1$ . Note $L = \sum n_i$ . Let $X$ denote the random variable representing $(n_1 + 1) (n_2 + 1) \cdots (n_s+1) - 1$ . What is $Pr(X < a)$ in terms of a and L? I would prefer something of the form $Pr(X < a) = $ , but if it is a bound (please no Markov lol) that's also fine.","['combinatorics', 'probability', 'random-variables']"
4899940,Compute the integral: $\int_{1}^{\infty}\frac{2+\sqrt{x+1}}{(1+x)^2-\sqrt{x-1}}dx$,"The integral is from my MIT Integration Bee 2025 Mock Training Problems. But unfortunately, I forgot the solutions nor the trick to this problem. I tried letting $u=\sqrt{x-1}$ , then perform trig-substitution on $u=\sqrt{2}\sec\theta$ .
However, this resulted to a nasty rational trig-function.
I tried letting $u=\sqrt{x+1}$ , but then $u=\sqrt{2}\tan\theta$ results the same nasty rational trig-function.
I thought about maybe finding a sneaky way to conjugate the denominator, but that square is out of the way. Any sneaky way to solve this integral?","['integration', 'definite-integrals']"
4899950,Show that $ \mu(A_{i_1}\cap A_{i_2}\cap\dots \cap A_{i_k})>c^k-\epsilon $,"Let $((0,1], \mathcal{B},\mu)$ be a measure space. Let $\{A_i\}_{i\ge 1}$ be a sequence of Borel measurable sets so that $\mu(A_i)\ge c$ for all $i\ge 1$ and some universal constants $c\in (0,1)$ . Show that for all $k=1,2,3,\dots$ and $\epsilon>0$ , there exists $i_1<i_2<\dots<i_k$ so that $$
\mu(A_{i_1}\cap A_{i_2}\cap\dots \cap A_{i_k})>c^k-\epsilon
$$ I am stuck on this question. I try to upper bound $$
(\sum_{i=1}^n \mu(A_i))^k
$$ for some $n\ge 1$ ... I also try to specific case $k=2$ : to show that $$
\mu(A_{i_1}\cap A_{i_2})>c^2-\epsilon
$$ We have $$
\mu((A_{i_1}\cap A_{i_2})^c)=\mu(A_{i_1}^c\cup A_{i_2}^c)\le \mu(A_{i_1}^c)+\mu(A_{i_2}^c)\le 2(1-c)
$$ So $$
\mu(A_{i_1}\cap A_{i_2})\ge 1-2(1-c)=2c-1
$$","['measure-theory', 'probability']"
4899959,"Is there a sequence of non-trivial continuous functions $f_{n+1}(f_{n+1}(x))=f_{n}(x)$, $f_2(x)= \frac{1-x}{1+x}$?","Just for curiosity I was trying to find a sequence of non-trivial continuous functions at $\mathbb{R}$ except finite many points such that $f_{n+1}(f_{n+1}(x))=f_{n}(x)$ , $f_1(x)=x$ and by non trivial I mean that $f_n (x)\ne x , \ f_n(x) \ne \frac{1}{x}, \ f(x)\ne -x$ . Let $f_2(x)= \frac{1-x}{1+x}$ I tried to find $f_3(x)$ but bit was too hard for me to find so I am not sure if a closed form of $f_n$ exist and I don't think there is a unique solution if the closed form exits. Note that $f_{n+1}\ne -f_n$ , $f_{n+1}\ne -\frac{1}{f_n} , \ f_{n+1}\ne f_n$ these are the trivial cases. Is there a closed form for $f_n$ ? As I said before I think there might be infinite many non trivial solutions but if that the case I want any solution.","['functional-equations', 'recurrence-relations', 'real-analysis', 'functions', 'algebra-precalculus']"
4899960,Consistency of bootstrap estimator that is continuously $\rho_\infty$-Frechet differentiable,"Theorem. Suppose $T$ is continuously $\rho_r$ -Frechet differentiable at $F$ with the influence function satisfying $0<E[\phi_F(X_1)]^2<\infty$ and that $\int F(x)[1-F(x)]^{r/2}dx<\infty$ . Define $$\hat G_n(t)=P_{F_n}\Bigg{(}\sqrt{n}\Big{[}T(F_n^*)-T(F_n)\Big{]}\le t\Bigg{)}\\G_n(t)=P_{F_n}\Bigg{(}\sqrt{n}\Big{[}T(F_n)-T(F)\Big{]}\le t\Bigg{)}$$ where, $F_n$ is the emperical distribution corresponding to $X_1,X_2,...X_n\overset{\text{iid}}{\sim}F$ and $F_n^*$ is the emperical distrubtion corresponding to $X_1^*,...X_n^*\overset{\text{iid}}{\sim}F_n$ . Then, $$\rho_r(\hat G_n,G)\overset{as}{\rightarrow}0$$ If specifically, $T$ is continuously $\rho_\infty$ -Frechet differentiable at $F$ . Then, $$\rho_\infty(\hat G_n,G)\overset{P}{\rightarrow}0$$ I am only interested in the proof for the special case: $T$ is continuously $\rho_\infty$ -Frechet differentiable at $F$ . Then, (known result) $$T(F_n)-T(F)=\frac{1}{n}\sum_{i=1}^n\phi(X_i)+o_p(n^{-1/2})$$ Let $r_n^*=T(F_n^*)-T(F_n)-\left(\frac{1}{n}\sum_1^n\phi(X_i^*)-\frac{1}{n}\sum_i^n\phi(X_i)\right)$ . We first show that $\forall \eta>0$ , $$P_*\left\{\sqrt{n}|r_n^*|>\eta\right\}\overset{as}{\rightarrow}0$$ I do not understand how we arrive at the following claim. From the differentiability of $T$ , for any $\epsilon>0$ , there is a $\delta_\epsilon>0$ such that $$P_*\left\{\sqrt{n}|r_n^*|>\eta\right\}\le P_*\left\{\sqrt{n}\rho_\infty(F_n^*,F_n)>\eta/\epsilon\right\}+P_*\left\{\rho_\infty(F_n^*,F_n)>\delta_\epsilon\right\}.$$ Definition. $T$ is continuously $\rho$ -Frechet differentiable at $G$ if $$\frac{T(H_k)-T(G_k)-L_T(H_k-G_k;G)}{\rho(H_k,G_k)}\rightarrow0$$ for any $G_k,H_k$ satisfying $\rho(H_k,G_k)\rightarrow0,\rho(G_k,G)\rightarrow0$ . Reference. Theorem 3.6 The Jackknife & Bootstrap, Jun Shao","['statistics', 'weak-convergence', 'frechet-derivative', 'bootstrap-sampling', 'probability-theory']"
4899973,When does a sparse failing subsequence of $(a_n)_n\subset\mathbb{R}_{>0}$ exist when $\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c$ exists,"My question is a bit longer than the actual title, so here is the full question: Let $(a_n)_n\subset\mathbb{R}_{>0}$ be a sequence of positive real numbers such that the limit $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c$$ exist for some $c\in\mathbb{R}$ . Then, it isn't too difficult to show that there exist a dense subsequence $(a_{n_k})_k$ of $(a_n)$ such that $$\lim_{k\to\infty}a_{n_k} = c$$ where by a ""dense"" subsequence we mean that $$\lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 1$$ Consequently, by a ""sparse"" subsequence of indices $(n_k)_k$ we mean such a subsequence of indices that $$\lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 0$$ Knowing that such a dense subsequence always exist when the limit of the mean exist, what do we know about the existence of sparse sequences $(a_{n_k})_k$ such that $$\lim_{k\to\infty}a_{n_k}\neq c$$ I'm interested in learning about any facts and/or references that discuss this question!","['limits', 'sequences-and-series', 'reference-request', 'real-analysis']"
4900045,"Example of finitely additive probability with no atom for all subsets of $[0,1]$","I am looking for an example of a probability measure $\mu$ on $[0,1]$ such that $\mu(A)$ is defined for ALL subsets $A\subset[0,1]$ $\mu$ is finitely additive but not $\sigma$ -additive No atom: If $\mu(A)>0$ then for all $r\in(0,1)$ , there is $B\subset A$ such that $\mu(B)=r\mu(A)$ Where can I find an example of such measure? with an explicit formula for $\mu(A)$ for any $A\subset[0,1]$ ?","['measure-theory', 'probability']"
4900047,"Can you have a function that is differentiable, yet the jacobian matrix is not continuous?","Suppose we have a function $f:\mathbb{R}^n\to\mathbb{R}^m$ that is differentiable at $x\in \mathbb{R}^n$ . We all know that this means that there exists a linear map $D_{f(x)} \in L(\mathbb{R}^n,\mathbb{R}^m)$ such that $f(x+h)$ can be approximated by the affine linear map $f(x) + D_{f(x)}h$ for small values of h. We also know that $D_{f(x)}$ (if it exists) has matrix representation $\partial f(x)$ . I also know that if $\partial f(x)$ is continuous on some open ball around $x$ , then we know that $\partial f(x) = D_{f(x)}$ . My question is, is it possible to have a function $f:\mathbb{R}^n\to\mathbb{R}^m$ Such that $f$ is differentiable at $x \in \mathbb{R}^n$ , and $\partial f(x) = D_{f(x)}$ but $\partial f(x)$ is not continuous at $x$ ? If possible can someone give an example? If not can someone prove it? My guess is that it is possible, it would just be some function whose derivative is not continuous but excited to see what everyone has to say.","['calculus', 'derivatives', 'analysis']"
4900067,Uniform Convergence of Empirical Means: Universal Separability Implies Measurability,"On page 38 of Pollard's book ""Convergence of Stochastic Processes"" one finds the following exercise (Problem 3 in Chapter II): Call a class of functions $\mathcal{F}$ universally separable if there exists a countable subclass $\mathcal{F}_0$ such that each $f$ in $\mathcal{F}$ can be written as a pointwise limit of a sequence in $\mathcal{F}_0$ . If $\mathcal{F}$ has an envelope $F$ for which $PF<\infty$ , prove that universal separability implies measurability of $\lVert P_n-P\rVert$ . Notes: $P$ is a probability measure on a set $S$ resp. on a $\sigma$ -algebra $\Sigma\subseteq\mathcal{P}(S)$ . $\mathcal{F}$ is a set of (measurable) functions mapping from $S$ into $\mathbb{R}$ . $P_n$ is the empirical probability measure that puts equal mass at each of the $n$ observations $\xi_1,\dots,\xi_n$ . More precisely: $$P_nf=\frac{1}{n}\sum\limits_{i=1}^n f(\xi_i)$$ for $f\in\mathcal{F}$ . $Pf$ denotes the expected value of $P_nf$ (if $Pf<\infty$ ). $\lVert P_n-P\rVert:=\sup\limits_{f\in\mathcal{F}}|P_nf-Pf|$ . An envelope $F$ for $\mathcal{F}$ is a measurable map $F\colon S\to\mathbb{R}$ such that $|f|\leq F$ for any $f\in\mathcal{F}$ . I personally am mostly interested in the case when $\mathcal{F}$ is a set of characteristic functions, i.e. functions whose image is a subset of {0,1}. In this case, $\mathcal{F}$ is universally separable, as witnessed by the countable subclass $\mathcal{F}_0$ , if for any $f\in\mathcal{F}$ there exists a sequence $(f_n)_n$ in $\mathcal{F}_0$ such that for any $s\in S$ there exists $n_s\in\mathbb{N}$ such that $f(s)=f_n(s)$ for any $n\geq n_s$ . Moreover, in my setting, I usually assume that $S$ is a topological space and $\Sigma$ is the corresponding Borel $\sigma$ -algebra generated by the topology. My attempts so far: First note that $P_n$ depends on $\underline{\xi}=(\xi_1,\dots,\xi_n)$ . Therefore, I denote by $P_n(\underline{\xi})f$ the empirical mean $$\frac{1}{n}\sum\limits_{i=1}^n f(\xi_i).$$ Further, we have $$Pf=\mathbb{E}_{\underline{\xi}\sim P^n}[P_n(\underline{\xi})f]=\mathbb{E}_{\xi\sim P}[f(\xi)],$$ where $P^n$ denotes the product measure $P\otimes\dots\otimes P$ . The map $$\lVert P_n-P\rVert\colon S^n\to\mathbb{R},\ \underline{\xi}\mapsto\sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})f-Pf|$$ is measurable if for any $\varepsilon>0$ the set { $\underline{\xi}=(\xi_1,\dots,\xi_n)\in S^n\mid \lVert P_n(\underline{\xi})-P\rVert\leq\varepsilon$ } is a member of the product $\sigma$ -algebra $\Sigma^n=\Sigma\otimes\dots\otimes\Sigma$ . I suppose that the idea is to show that $$\sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})(f)-Pf|=\sup\limits_{f\in\mathcal{F}_0}|P_n(\underline{\xi})(f)-Pf|.$$ However, I don't see how one can show this. Given $f\in\mathcal{F}$ and $\underline{\xi}\in S^n$ , the universal separability of $\mathcal{F}$ implies that there exists $f_0\in\mathcal{F}_0$ such that $P_n(\underline{\xi})f=P_n(\underline{\xi})f_0$ . However, I don't see how the subclass $\mathcal{F}_0$ can be used with regard to $Pf$ . I'm looking forward to comments and answers! Please let me know if I shall clarify some point.","['measure-theory', 'statistics', 'empirical-processes', 'uniform-convergence', 'borel-sets']"
4900081,"Argue whether the generalized double integral $\int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy$ where $D$ is $x\geq1 , y\geq1$ converge or diverge. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question In the first part of the problem, I proved that $$ \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dy)dx $$ and $$ \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dx)dy $$ converge, $\frac{-\pi}{4}$ and $\frac{\pi}{4}$ . The second part of the problem is whether this result is enough to show that $$\int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy\ \text{where}\  D\  \text{is}\  x\geq1 , y\geq1$$ also converges. I am tempted to use the comparison criteria but the domain not being a circle confuses me as to what I should use as comparison.",['multivariable-calculus']
4900096,Cauchy product of zeta function,"$\zeta(s)= \sum_{n=1}^{\infty} \frac{1}{n^s}; s \in \mathbb{C}$ . For $Re(s) > 1 $ , we have that the above series converges absolutely. And in this case, I wrote the Cauchy product of $\zeta(s)$ by itself as: \begin{align*}
\sum_{n=1}^{\infty} \sum_{k=1}^{n}\frac{1}{k^s}\frac{1}{(n+1-k)^s}
\end{align*} But now I can't seem to find a way to arrive at the result $$ \sum_{n=1}^{\infty}\frac{d(n)}{n^s}$$ where $d(n)$ is the number of divisors of $n$ . Any suggestions?","['complex-analysis', 'riemann-zeta', 'cauchy-product', 'sequences-and-series']"
4900114,Shifted Borel set,"Let $A$ be bounded Borel subset of the real line with $\lambda(A)>0$ where $\lambda$ is the Lebesgue measure. How does one show that $f(x)=\lambda(A \cap (x+A)$ is continuous in $x$ ? My attempt: Let $(x_n)$ be any sequence converging to $x$ , then it suffices to show $f(x_n)$ converges to $f(x)$ . In particular, if one shows $\mathbb{1}_{A \cap (x_n+A)}$ converges to $\mathbb{1}_{A \cap (x+A)}$ then by dominating convergence we are done. Unfortunately, if $x$ is an isolated point this does not seem work. Is it possible to fix this argument? Much appreciated.",['measure-theory']
4900126,Expressing the area of an isosceles triangle as a function of one of its angles.,"We are given a circle with radius $1$ , its center point and an inscribed isosceles triangle with $AB=AC$ and its height (as shown in the picture below). Can we express the area $(ABC)$ as a function of $θ$ where $θ=B \hat{A} C$ ? How I tried: I put the diagram in a Cartesian coordinate system. And since the circle is of radius $1$ the points are $A(0,1),\, B(\cos x_1, \sin x_1),\, C(\cos x_2, \sin x_2)$ . But then since we don't know where exactly points $B,C$ are in the circle we can't say that they have the same $y$ value and make the substitution $\sin x_1 = - \sin x_2$ or $\cos x_1 = \cos x_2$ . How can I proceed? Any help would be appreciated.","['trigonometry', 'algebra-precalculus', 'geometry', 'analytic-geometry']"
4900132,Can we conclude that $f=g$ a.e. if $\int _Efd\mu =\int _Egd\mu $ for all measurable sets $E$?,"Let $(X,\Sigma,\mu )$ be a measure space and $f,g:X\to\color{red}{[0,\infty ]}$ measurable functions. If $\int _Efd\mu =\int _Egd\mu $ for all $E\in \Sigma$ , can we conclude that $f=g$ a.e.? I know that that proposition is true if $\mu$ if finite and the difference $f(x)-g(x)$ is well defined for all $x\in X$ . However I don't know if that proposition is still true in the general case. Thank you for your attention!","['measure-theory', 'lebesgue-integral']"
