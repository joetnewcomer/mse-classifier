question_id,title,body,tags
1093583,Sum of compact sets,"Let $A,B$ two non-empty compact subsets of a normed space X. How can we prove that the set $S=A+B=\{a+b : a \in A, b \in B\}$ is compact? Here's my reasoning: Let $\Omega = \{\Omega_1, \Omega_2,…\}$ be an open cover of $S$. $\Omega$ induces two open cover $X,Y$ respectively of $A,B$, where $X_i = \{a \in A : a+b \in \Omega_i~~for~some~b\}$ $Y_i = \{b \in B : a+b \in \Omega_i~~for~some~a\}$ in practice $\Omega_i = X_i + Y_i$. Now my idea is build a finite subcover this way: consider a finite subcover $X_F = \{X_j : j \in J\}$ where $J$ is a finite set of indices. If $Y_F = \{Y_j : j \in J\}$ is a finite subcover of $B$, then we are done as $\{\Omega_j : j \in J\}$ is a subcover of $\Omega$. Otherwise we can keep adding indexes to set $J$ until $Y_F$ becomes a finite subcover of $B$. This is not too formal but I don't want to make a simple question unreadable, I think the idea should be clear. Does this works? Is there a better/easier way to get the same result?",['general-topology']
1093607,Help needed to understand statements about torus,"I am having trouble understanding two statements: Let $A$ be an algebraic curve in $\mathbb{P}^2$ over $\mathbb{C}.$ Consider its normalization $$\pi: \hat{A} \to A.$$ If genus $g(\hat{A})=1,$ then $\hat{A}$ is a topologically a torus . I understand up to here, as it follows from Riemann-Roch theorem. What I don't understand are the following statements: It's (i.e. torus's) group of holomorphic automorphisms $Aut(\hat{A})$ is a complex Lie group whose connected component at the identity is a complex torus $T$ which acts freely and transitively on $\hat{A}$ . The quotient $\Gamma:= Aut(\hat{A})/T$ is a finite group of order at most six. I don't understand either of these points. Could someone clarify? Which theorems are being used here? Please let me know of references, if possible, for the proof.","['complex-geometry', 'algebraic-geometry', 'lie-groups', 'proof-explanation', 'reference-request']"
1093633,Tangent vectors: arrows vs. derivatives,"I have a very hard time accepting the differential-geometric definition of a vector as a derivative operator, $$v = v^{\mu} \partial_{\mu}.$$ I want to make sure that the following line of reasoning is correct - if it is, then I will finally be at peace with the aforementioned definition. (Geometrical) vectors are most naturally introduced as displacements in an affine space $\mathbb{A}$. E.g. a geometrical vector $\vec{v} \in \mathbb{V}$ (a vector space) can be thought of as a difference of two points (positions) $\mathcal{P, Q} \in \mathbb{A}$, so that $$\vec{v} = \mathcal{Q}-\mathcal{P}.$$ We can think of $\vec{v}$ as a good old fashioned arrow starting at $\mathcal{P}$ and ending at $\mathcal{Q}$. Of course, we cannot play this game outside of $\mathbb{A}$, but this is still valid locally in any manifold $\mathcal{M}$ since manifolds are locally Euclid (more specific, affine) spaces. If $\gamma:\mathbb{R} \to \mathcal{M}$ is a curve in $\mathcal{M}$ with $\gamma(0) = \mathcal{P}$, we can subtract $\mathcal{P}$ from its infinitesimally close neighbor, which lies on $\gamma$, to get a tangent vector$$\vec{v} = \gamma'(0).$$ First question: Am I correct to assume that $\gamma'(0)$ can be thought of as an arrow lying in the tangent space $T_{\mathcal{P}}\mathcal{M}$, not necessarily ending anywhere in $\mathcal{M}$? Furthermore, it should be obvious from the definition that this is a pure geometrical object and not a (differential) operator? Second question: Is the operator-vector $v = v^{\mu} \partial_\mu$ just an object isomorphic to $\vec{v}$ and is therefore also known as the tangent vector, but whose character is more algebraic and less geometric? In other words, although $\vec{v}$ (an arrow) and $v$ (a derivative) are isomorphic, they are clearly different objects? Third question: If the answer to the last question is yes, given some coordinates $\{x^\mu\}$, we should be able to write $$\vec{v} = v^{\mu} \vec{e}_\mu,$$ where now $\vec{e}_\mu$ is a geometric vector (arrow) basis and not a differential operator $\partial_\mu$. How is $\vec{e}_\mu$ related to $\gamma$?","['manifolds', 'differential-geometry']"
1093646,Help on proving a trigonometric identity involving cot and half angles,"Prove: $\cot\frac{x+y}{2}=-\left(\frac{\sin x-\sin y}{\cos x-\cos y}\right)$. My original idea was to do this:
$\cot\frac{x+y}{2}$ = $\frac{\cos\frac{x+y}{2}}{\sin\frac{x+y}{2}}$, then substitute in the formulas for $\cos\frac{x+y}{2}$ and $\sin\frac{x+y}{2}$, but that became messy very quickly. Did I have the correct original idea, but overthink it, or is there any easier way? Hints only, please.","['geometry', 'trigonometry']"
1093655,"Boundary, unions and intersections","Let $(X,\tau)$ be a topological space. If $A,B\subseteq X$, then $\partial(A\cap B)\subseteq(\partial A\cap B)\cup(A\cap\partial B)\cup(\partial A\cap\partial B)$, where $\partial$ denotes the boundary operation. Is it possible to generalize this result? I. e., if $\{A_i\}_{i\in I}$ is a family of subsets of $X$, then
$$\partial(\bigcap_{i\in I}A_i)\subseteq\bigcup_{J\subseteq I, J\neq\emptyset}\cap_{j\in J}\partial A_j\bigcap\cap_{i\in I\setminus J}A_i?$$",['general-topology']
1093656,Compute the limit $\lim_{n\to\infty}{(\sqrt[n]{e}-\frac{2}{n})^n}$ [duplicate],"This question already has answers here : Find this limit: $ \lim_{n \to \infty}{(e^{\frac{1}{n}} - \frac{2}{n})^n}$ (3 answers) Closed 9 years ago . How can I compute this limit of the sequence?
 $$\lim_{n\to\infty}{(\sqrt[n]{e}-\frac{2}{n})^n}$$",['limits']
1093681,Prove property of symmetric difference: if $A \triangle B \subseteq A$ then $B \subseteq A$,"This problem is from Velleman p143 5. Recall from Section 1.4 that the symmetric difference of two sets A and B is the set $ A \triangle B = (A \setminus B) \cup (B \setminus A) = ( A \cup B) \setminus (A \cap B) $. Prove that if $ A \triangle B \subseteq A $ then $ B \subseteq A $. My strategy: The crux of this proof is to reduce $ ( A \cup B) \setminus (A \cap B) $ down to the fundamental logical form. $ (A \setminus B) \cup (B \setminus A) = (x \in A \land x \notin B) \lor (x \in B \land x \notin A) $. This gives us a disjunction where if either case is true, then $  x \in A $. My proof: Let x be an arbitrary member of B. Suppose that $ A \triangle B \subseteq A $. All the members of $ A \triangle B $ are in either A or B, but not both. The case where $ x \in A \land x \notin B $ is not applicable since $ x \in B $. That leaves the case where $ (x \in B \land x \notin A) $. Since $ x \in B $, we know that $ x \in A \triangle B $ and we're given this means that $ x \in A $. Therefore $ x \in B \to x \in A $. Since x was arbitrary, we assert that $ B \subseteq A $. My issue: The proof seems OK, I think. The Velleman Proof Designer seems to jive with my argument. However, it's really bothering me that the logical decomposition involves $ (x \in B \land x \notin A ) \to x \in A $. How can x be a member of A and at the same time not be a member of A?","['elementary-set-theory', 'proof-verification']"
1093695,Ice cream issue in Lem's 'Extraordinary Hotel',Could you clarify the ice cream issue mentioned at the end of the story The Extraordinary Hotel ( pages 189-190 here )?,"['infinity', 'elementary-set-theory', 'soft-question', 'paradoxes']"
1093696,"Is arrow notation for vectors ""not mathematically mature""?","Assuming that we can't bold our variables (say, we're writing math as opposed to typing it), is it ""not mathematically mature"" to put an arrow over a vector? I ask this because in my linear algebra class, my professor never used arrow notation, so sometimes it wasn't obvious between distinguishing a scalar and a vector. (Granted, he did reserve $u$, $v$, and $w$ to mean vectors.) At the same time, my machine learning class used arrows to denote vectors, but I know some other machine learning literature chooses not to put arrows on top of their vectors. Ultimately, I just want a yes or no answer, so at least I do not seem like an immature writer when writing my own papers someday.","['notation', 'linear-algebra', 'vectors']"
1093717,Stopped-sum of Exponential random variables,"Let $\xi_1, \xi_2, \ldots \xi_n, \ldots$ - independent random variables having exponential distribution $p_{\xi_i} (x) = \lambda e^{- \lambda x}, \; x \ge 0$ and $p_{\xi_i} (x) = 0, \; x < 0$. Let $\nu = \min \{n \ge 1 : \xi_n > 1\}$. Need to find the distribution function of a random variable $g = \xi_1 + \xi_2 + \ldots \xi_{\nu}$ that is, find the probability $\mathbb{P}(g < x) = \mathbb{P} (\xi_1 + \xi_2 + \ldots \xi_{\nu} < x)$. I made the following calculations: $\mathbb{P} (\xi_1 + \xi_2 + \ldots \xi_{\nu} < x) = \sum_{k = 1}^{\infty} \mathbb{P} (\xi_1 + \xi_2 + \ldots \xi_k < x, \nu = k) = \sum_{k = 1}^{\infty} \mathbb{P} (\xi_1 + \xi_2 + \ldots \xi_k < x, \xi_1 \le 1, \ldots \xi_{k-1} \le 1, \xi_k > 1)$. The probability of the sum can be represented as integral: $\mathbb{P} (\xi_1 + \xi_2 + \ldots \xi_k < x, \xi_1 \le 1, \ldots \xi_{k-1} \le 1, \xi_k > 1) = \int\limits_D \lambda^k e^{- \lambda u_1} e^{- \lambda u_2} \ldots e^{- \lambda u_k} {d}u_1 \ldots {d}u_k$, where $D = \{ u_1 + \ldots u_k < x, u_1 \le 1, \ldots u_{k-1} \le 1, u_k > 1\}$. I'm afraid that this integral cannot be calculated. Is it somehow easier to find the distribution function $\mathbb{P} (g < x)$?","['probability-theory', 'probability-distributions', 'probability']"
1093723,Galois group of $X^4 + 2X^2+4$,"Find the Galois group of $f(X) = X^4 + 2X^2+4$ over $\mathbb{Q}$. Let $L$ be the splitting field of $f$ over $\mathbb{Q}$.  Finding the roots of this polynomial, I got $$X^2 = \frac{-2\pm \sqrt{4-16}}{2} = -1 \pm \sqrt{3}i$$ so the roots are $\alpha_1 = \sqrt{-1+\sqrt{3}i}, \alpha_2 =\sqrt{-1-\sqrt{3}}i, \alpha_3 =  -\sqrt{-1+\sqrt{3}i}$ and  $ \alpha_4 -\sqrt{-1-\sqrt{3}}i$.  Now $L = \mathbb{Q}(\alpha_1, \alpha_2)$, and since $$\alpha_1 \alpha_2  = \sqrt{(-1 + \sqrt{3}i)(-1-\sqrt{3}i)} = \sqrt{1+3} = 2$$ we see that $L = \mathbb{Q}(\alpha_1) = \mathbb{Q}(\alpha_2)$.  It's not difficult to see that $[\mathbb{Q}(\alpha_1) : \mathbb{Q}] = 4$, so the Galois group of $L/\mathbb{Q}$ over $\mathbb{Q}$ is is either $\mathbb{Z}/4\mathbb{Z}$ or $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$. My guess is that this Galois group is cyclic, since I think that $\mathbb{Q}(\sqrt{3}i)$ (a cyclotomic extension by a cubic root of unity) is the only intermediate subfield of $L/\mathbb{Q}$.  How can I know for sure?","['galois-theory', 'abstract-algebra']"
1093727,The fundamental group of the projective plane minus 2 points?,"I'm trying to compute the fundamental group of $\mathbb{RP}^2$ minus 2 points. I'm using the presentation $\langle a\mid a^2\rangle$. Meaning that I'm taking the disk and identifying the two sides. I'm conjecturing that it is $\mathbb{Z}\times\mathbb{Z}$. I'm trying to use Seifert -Van Kampen but I can't get my open sets to work nicely. The reason why I think it is $\mathbb{Z}\times\mathbb{Z}$ is because clearly a loop around each hole would give you two distinct loops, say $a$ and $b$. But then it seems that $ab$ is homotopic to $ba$. This relation leads me to think that it might be $\mathbb{Z}\times\mathbb{Z}$. Any hint would be appreciated.","['general-topology', 'geometry', 'algebraic-topology']"
1093782,Stuck on integrating $\int x/(1-x)dx$,"My attempt: Let $u = 1-x$ , $du = -dx$ , $x = 1-u$, so: 
\begin{align*}
\int \frac{x}{1-x}\, dx &= - \int \frac{1-u}u\, du \\
&= - \left( \int \frac 1 u\, du - \int 1 \, du \right) \\
&= - \ln(|u|) + u \\
 &= -\ln(|1-x|) + (1-x)
\end{align*} But the answer is supposed to be $-x - \ln(|1-x|)$. Why do I have an extra 1?","['calculus', 'integration']"
1093786,Approximation for elliptic integral of second kind,"My (physics) book gives the following approximation: $\int_{-\pi/2}^{\pi/2} \sqrt{1-(1-a^2) \sin(k)^2} dk \approx 2 + (a_1 - b_1 \ln a^2) a^2 + O(a^2 \ln a^2)$ where a1 and b1 are ""(unspecified) numerical constants.""  I've been looking for either a derivation of this, or the same approximation listed elsewhere and have gotten nowhere.  Can someone help me along?","['elliptic-integrals', 'special-functions', 'integration']"
1093805,"Classify all entire functions $f$ for which $\forall n\in \mathbb{N} \, f\big(\frac{1}{n}\big)=f\big(\frac{-1}{n}\big)=\frac{1}{n^2}$","Here is my solution to the problem in the title and below that are two questions regarding the given information. Suppose $f=u+iv$ is entire and $$f\bigg(\frac{1}{n}\bigg)=f\bigg(\frac{-1}{n}\bigg)=\frac{1}{n^2}\,\,\,\,\,(*)$$ Note that $z=0$ is a limit point of $\frac{1}{n}$ and $v(\frac{1}{n})=0$ for all $n$. So $v\equiv0$. Hence $f=u$. Since $f$ is holomorphic, it satisfies the Cauchy-Riemann equations: $$\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}=0,\frac{\partial u}{\partial y}=\frac{-\partial v}{\partial x}=0$$
So $u\equiv C$, where $C$ is a constant. By $(*)$, $C=0$.
 Hence the only entire function satisfying $(*)$ is $f\equiv0$. 1) Is the condition that $f(1/n)=f(-1/n)$ uncessesary? 2) What is the significance of '$2$' in $\frac{1}{n^2}$?",['complex-analysis']
1093841,Proving $\Gamma(x)$ is holomorphic,"My professor defined Gamma function in the following way:$$\Gamma(z)= \lim \limits_{n \rightarrow \infty} \frac{n!n^z}{z(z+1)....(z+n)}$$
Now we first observe that $f_n(z)= \frac{n!n^z}{z(z+1)....(z+n)}$ is holomorphic in ${\operatorname{Re} z>0}$. Then we have to show that the limit exists and also that $f_n$ converge uniformly. But I am unable to show that limit exists also I cannot prove that $f_n$ converge uniformly. If anyone can give a hint, it would be really great. Thank you.","['gamma-function', 'uniform-convergence', 'complex-analysis', 'limits']"
1094879,Number of squares in $(\mathbb{Z}/p\mathbb{Z})^\times$,$x$ is a square in $(\mathbb{Z} /p \mathbb{Z})^\times$ iff there is a $y \in (\mathbb{Z} /p \mathbb{Z})^\times$ such that $x \equiv y^2 \mod p$. I am asked to show that there are exactly $\frac{p-1}{2}$ squares in $(\mathbb{Z} /p \mathbb{Z})^\times$. How do I tackle this?,"['ring-theory', 'elementary-number-theory', 'group-theory', 'abstract-algebra']"
1094888,"Riemann Hypothesis, is this statement equivalent to Mertens function statement?","All: I saw one form of Riemann Hypothesis, it says:
$$
\lim ∑(μ(n))/n^σ 
$$
Converges for all σ > ½ Is this statement same as the order of Mertens function is less than square root of n ?","['riemann-hypothesis', 'analytic-number-theory', 'number-theory']"
1094920,A question about open sets containing a closed disc,"I'm reading a book on analysis and it says that in $\mathbb{R}^n$ with usual topology, any open set containing a closed ball also contains an open ball which contains the closed ball. But it doesn't seem clear to me. Could anyone tell me why it is so?","['general-topology', 'analysis']"
1094930,Is there any identity for $\sum_{k=0}^{n-1}\tan(x+ka) $??,"I found this series
$$
\sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n}\right)=−n\cot\left(\frac{n\pi}{2}+n\theta\right)
$$
but it's not what I need.","['sequences-and-series', 'trigonometry', 'calculus', 'trigonometric-series']"
1094935,Question about relatively closed set,"I know that $G$ is relatively open set in $U$ if and only if there exists open set $G_1$in $R$ such that $G=U \cap G_1$. By using this, I want to show that F is relatively closed set if and only if there exists closed set $F_1$ in $R$ such that $F = U \cap F_1$. I have tried if $F$ is relatively closed in $R$ then $F^c \equiv U\setminus F $ is relatively open in $U$ so that there exists open set $G_1$ in R such that $U\setminus F = U \cap G_1$. I don't know how to show this theorem in this way. Help me to prove it.","['general-topology', 'real-analysis']"
1094949,Expected length of minimum chord,"You are given a circle of radius $1$. Suppose you pick $n$ independent points randomly on the circle and join neighboring points with lines to create chords. What is the expected length of the shortest chord? I thought about doing this using angles from the center, i.e. minimize $E[\min 2\sin(\theta_i / 2)]$ where $\theta_i$ are random on $[0, 2\pi]$ and sum of $\theta_i = 2\pi$. But not really sure where to go about it from here.","['geometry', 'geometric-probability', 'probability', 'expectation']"
1094953,$E \to S$ surjective in degrees $\geq 1$ implies $\widetilde{E} \to \widetilde{S}$ surjective,"In the proof of Theorem II.8.13 in Hartshorne (which is the Euler sequence), the author writes: Let $S = A[x_0, \ldots, x_n]$. [...] The exact sequence
  $$0 \to M \to E \to S$$
  of graded $S$-modules gives rise to an exact sequence of sheaves of $\mathbb{P}^n_A$,
  $$0 \to \widetilde{M} \to \widetilde{E} \to \mathcal{O}_X \to 0.$$
  Note that $E \to S$ is not surjective, but is surjective in all degrees $\geq 1$, so the corresponding map of sheaves is surjective . Hartshorne doesn't give any justification for this last statement.  I've come up with a justification which I think holds, but I haven't seen it written down anywhere and I don't quite trust myself to do commutative algebra, so, two questions: Is the following result true, and its proof valid? Is this the intended justification? Proposition. Let $S$ be a graded ring, $M = \bigoplus\limits_{d=0}^\infty M_d$ a graded $S$-modules, and $$\bigoplus\limits_{d=n}^\infty M_d \subseteq N \subseteq M$$ a graded $S$-submodule.  For each $\mathfrak{p} \in \operatorname{Proj} S$, we have $N_{(\mathfrak{p})} = M_{(\mathfrak{p})}$. Proof. Write $T$ for the set of homogeneous elements in $S \setminus \mathfrak{p}$, so that $S_{(\mathfrak{p})} = (T^{-1}S)_0$, $M_{(\mathfrak{p})} = (T^{-1}M)_0$, and so forth.  Clearly $N_{(\mathfrak{p})} \subseteq M_{(\mathfrak{p})}$.  Consider an element $\frac{m}{t} \in M_{(\mathfrak{p})}$. Since $S_+ \not \subseteq \mathfrak{p}$ we can find some homogeneous $v \in S_+$ with $v \not \in \mathfrak{p}$.  Then $v$ is both an element of $S$ of positive degree and a unit of $T^{-1} S$.  It follows that $mv^n$ must have degree at least $n$, so $$mv^n \in \bigoplus\limits_{d=n}^\infty M_d \subseteq N.$$
Consequently,
$$\frac{m}{t} = \frac{m v^n}{t v^n} \in N_{(\mathfrak{p})}.$$ As a corollary, any map of $S$-modules that was surjective in sufficiently high degree would necessarily  induce a surjective map of sheaves on $\operatorname{Proj} S$.","['commutative-algebra', 'algebraic-geometry', 'proof-verification']"
1094963,"Spivak Calculus on Manifolds, Theorem 5-2","In the proof Theorem 5-2 of Spivak Calculus on Mannifolds how is
\begin{align*}
V_2\cap M=\{f(a):(a,0)\in V_1\}?
\end{align*}
(That $\{f(a):(a,0)\in V_1\}=\{g(a,0):(a,0)\in V_1\}$ is clear.) Edit :  Due to a comment, Theorem 5-2 proves the equivalence of the following two definitions of a $k$-dimensional regular submanifold in $\mathbb{R}^n$. Definition 1 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following condition is satisfied: (M)  There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $V\subset\mathbb{R}^n$, and a diffeomorphism $h:U\rightarrow V$ such that
\begin{align*}
h(U\cap M)&=V\cap(\mathbb{R}^k\times\{0\})\\
&=\{y\in V:y^{k+1}=\dots=y^n\}.
\end{align*} Definition 2 .  A subset $M\subset\mathbb{R}^n$ is a $k$-dimensional submanifold if for every point $x\in M$ the following ""coordinate condition"" is satisfied: (C) There is an open set $U\subset \mathbb R^n$ containing $x$, an open set $W\subset\mathbb{R}^k$, and an injective differentiable function $f:W\rightarrow\mathbb{R}^n$ satisfying: $f(W)=M\cap U$. $f'(y)$ has maximum rank $k$ for each $y\in W$. $f^{-1}:f(W)\rightarrow W$ is continuous. The part of the proof mentioned above is proving Definition 2 $\Rightarrow$ Definition 1.","['multivariable-calculus', 'calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
1094967,Prove that $\operatorname{Trace}(A^2) \le 0$,"Let $A \in M_n(\mathbb{R})$  is a antisymmetric matrix such as $A^T=-A$.
Prove that $\operatorname{Trace}(A^2) \le 0 $ I see that, for some matrix such as, their terms in diagonal are negative ?","['trace', 'matrices', 'linear-algebra']"
1094978,"At most $2n$ vectors, the angle between which $\geq\pi/2$.","In a previous question it is proved that in $\mathbb R^n$ there are at most $n+1$ vectors, the angle between which $>π/2$. How to prove that there are at most $2n$ vectors, the angle between which $\geq\pi/2$?","['linear-algebra', 'euclidean-geometry', 'combinatorics']"
1094985,Co-Area formula in Riemannian geometry,"I wonder if the following holds true: Let $z:[0,1]\times B^{n-1}_r(0)\to(M^n,g), (t,p)\mapsto z(t,p)$ be a diffeomorphism a.e. onto its image with respect to the Lebesgue measure on $A:=[0,1]\times B^{n-1}_r(0)$. I have two questions: (1) Is there a variant of the co-area formula that gives me the following identity which I expect to be true $$
\int_{z(A)}\mathrm d\operatorname{vol}_g = \int_{B^{n-1}(0)}\mathcal H^1_g(z([0,1]\times\{p\}))\mathrm d\mathcal L^{n-1}(p),
$$
where $\mathcal H^1_g$ denotes the 1-Hausdorff measure with respect to $g$ and $\mathcal L^{n-1}$ is the $(n-1)$-dimenaional Lebesgue measure? If yes, does someone have a reference or a proof? (2) How can one show that
$$\mathcal H^1_g(z([0,1]\times\{p\}))=\int_0^1\sqrt{g\left(\frac{\partial z}{\partial s}(s,p),\frac{\partial z}{\partial s}(s,p)\right)}~\mathrm ds,$$
which I expect also to be true.","['measure-theory', 'lebesgue-measure', 'geometric-measure-theory', 'differential-geometry']"
1094990,Understanding Integration in Complex analysis,"In real analysis, we can understand integration as areas under the curve. In complex analysis, I saw many theorems (Cauchy's theorem, Goursat's theorem etc.) about complex integration, but I couldn't see its relation with ""area"". Q. What should one basically understand by the complex integration, and what should we think when there is some discussion of complex integration in a book.",['complex-analysis']
1095008,Reference request: groups of order $p^4$.,"I am looking for a textbook or a paper which include the classification of groups of order $p^4$ ($p$ is prime) using generators and relations. In particular I like to understand which group $G$ ""exist"" for the odd $p$'s and do not exist for $p=2$.","['groups-enumeration', 'finite-groups', 'group-theory', 'p-groups']"
1095013,How many natural numbers less than 200 will have 12 factors/divisors?,"How many natural numbers less than 200 will have 12 factors (a.k.a. divisors)? I think the answer is $11$. Firstly there can be at most $3$ distinct prime factors. $12=1\cdot12
  =2\cdot6
  =3\cdot4
  =2\cdot2\cdot3$ $N=a^{11}
 =a\cdot b^5
 =a^2\cdot b^3
 =a\cdot b\cdot c^2$ Then, $1$ prime factor is not possible because the smallest $2^{11}> 200$. So, the options are: For $2$ prime factors: $72, 96, 160$ For $3$ prime factors: $60,90,150,84,140,126,132,156$. I think this list is exhaustive. But am I right? Is there any other approach?","['elementary-number-theory', 'combinatorics']"
1095024,Leray's theorem up to some degree,"I am interested in the proof of Leray's theorem that relates Čech cohomology and sheaf cohomology. The theorem states that if we have a space $X$, a sheaf $\mathcal{F}$ and a covering of $X$ such that our sheaf is acyclic on the covering, i.e. Čech cohomology  is $0$ on every finite intersection of elements in the covering, then these two cohomologies agree. Now, my question. All the proves that I've found are by induction and assume that cohomology vanishes in every degree, so $\check{\mathrm{H}}^p(\mathcal{U}, \mathcal{F})=0$ for every $p>0$. This looks like a rather strong assumption to check, especially if one is interested in computing cohomology just in low degrees (in particular I am interested in degree 0 and 1). Looking at the proves that I found, indeed, it seems to me that we just need that Čech cohomology vanishes up to degree $p$ to prove the isomorphism in this degree (for example Climbing Mount Bourbaki ). I may be very wrong on this point, so the question is Can someone provide a counter example in which we really need that cohomology vanishes in all the degrees and not just up to what we are interested in? or Can someone point out a reference which clearly states that we can just check up to degree $p$ (or $p+1$, depending on your indexing) to prove the isomorphism? Thank you in advance, Davide EDIT: Despite the 5 ""ups"" I received no answer, so I decided to ask the same question also on Math Overflow .","['homology-cohomology', 'sheaf-theory', 'algebraic-geometry', 'sheaf-cohomology']"
1095028,"Groups $G,H$ with surjective homomorphisms that are quotient of each other","Let $G$ and $H$ be groups and assume that there exist onto homomorphisms $$\phi :G\rightarrow H,\qquad\psi :H\rightarrow G.$$ If $H$ and $G$ are finite then it is clear that $H$ and $G$ are isomorphic.
It is also valid for finitly generated abelian groups.
However, in general this is not true even in the abelian case. My question is: if we know that $G,H$ are finitly generated and such $\phi ,\psi$ exist then are $G$ and $H$ isomorphic?",['group-theory']
1095032,The Zariski density for two given sets.,"Let $A$ and $B$ be two subsets of $\mathbb{C}^n$： $ A = \mathbb{Z}^n$, and $B=\{ (z_1,z_2, \dots , z_n) \in A \text{ such that } z_1>z_2>\cdots> z_n\}$. My questions：
Are these two subsets Zariski dense in $\mathbb{C}^n$? Thanks very much!","['algebraic-geometry', 'algebraic-topology', 'abstract-algebra']"
1095037,Finding an integer $n$ such that $\sin(n)$ is close to 1,"Given some $\epsilon>0$, is there an efficient way to find an integer $n$ such that 
$$1-\sin(n)<\epsilon$$ We all know there is always one (and many), and so I can test all $n$ from $0$ until I find a good candidate, but I ask for some efficient algorithm that given some $\epsilon$, computes quickly such an $n$.",['real-analysis']
1095039,"Completeness of $ L^{p} $ spaces and ""rapidly Cauchy"" sequences","http://math.harvard.edu/~ctm/home/text/books/royden-fitzpatrick/royden-fitzpatrick.pdf In the book of Royden, the completeness of $ L^{p}    $ spaces has been done using what he calls ""rapidly Cauchy"" sequences. A sequence $ f_{n} \in  X $, where $ X $ is a normed linear is said to be rapidly Cauchy if there is a sequence of positive reals $ \epsilon_{k} $ such that $ \sum_{  k  = 1 } ^ { \infty}  \epsilon_{k}         $ is convergent and $$    ||        f_{k+1} - f_{k} ||    < \epsilon_{k}   ^{2}         ,       $$ for all $ k \in \mathbb{N}                                      $. However, I don't understand why do we need to square the $ \epsilon_{k} $. The proofs all work fine even if I just put $    \epsilon_{k} $ as an upper bound on the $ ||  f_{k+1} - f_{k} || $ norm.  As far as I see, all propositions and theorems up to the Reisz-Fischer Theorem remain valid. Question. What purpose does squaring the $ \epsilon_{k} $ term serve in the chapter?","['measure-theory', 'lebesgue-measure', 'lp-spaces', 'complete-spaces']"
1095040,"Proving an integral $\int \sqrt{a^2 - u^2} \, du$",How can I prove this?? Any hint or help would be great! Thanks :) $$\int \sqrt{a^2-u^2} \mathrm{d}u = \frac{u}{2} \sqrt{a^2-u^2} +\frac{a^2}{2} \sin^{-1}\left(\frac{u}{a}\right) + C$$,"['calculus', 'integration']"
1095083,How can I show that $\lim_{n\to\infty} 2^n \left( \frac{n}{n+1} \right ) ^{n^2} = 0$?,"How to calculate limit of the following expression:
$$2^n \left( \frac{n}{n+1} \right ) ^{n^2} $$
I know that limit of this sequence is equal to zero, but how to show that?","['sequences-and-series', 'limits']"
1095158,The mathematics of anaglyph images,"Note: I'm not quite sure whether this question properly belongs to the Mathematica or to the mathematics Stack Exchange. But because my question mainly concerns general mathematical principles rather than specific Mathematica issues, I've opted for the latter.) For as long as I can remember, I've been fond of anaglyph pictures. An anaglyph is a stereoscopic image made by encoding each eye's image using filters of different (usually chromatically opposite) colors, typically red and cyan. (Wikipedia.) Anaglyphs are viewed using special eyeglasses with two differently colored lenses. Nowadays I especially like anaglyphs depicting mathematical objects. That's why I was quite excited when I found a Mathematica notebook with which one can easily turn an ordinary Mathematica 3d image into an anaglyph. Here it is: Mathematica anaglyphs . After tinkering with it a bit, the notebook produces very satisfactory results. The actual anaglyph-generating code is surprisingly simple. Basically, it takes a 3d scene and then creates the two color filtered images, using slightly different camera positions for each image. Both these positions lie on a circle of radius $5$ and are separated by a small angle $2\delta=1.4^\circ$. (As far as I know, the coordinate system that Mathematica uses is scaled with respect to the bounding box of the scene.) Unfortunately, the code itself does not contain any comments on its inner workings. In particular, it doesn't account for a few seemingly arbitrary choices of parameters. My main question is: where does this angle $2\delta=1.4^\circ$ come from? Is it a constant? Or does it depend on certain other variables involved in creating the image (such as the radius $R$ of the circle on which the camera is moving). Also: is there a special reason for choosing $R=5$? And finally: is the method I described really the most efficient and accurate way of producing anaglyphs? Any answers and/or additional explanations of the mathematics of anaglyphs would be greatly appreciated.","['geometry', 'computer-vision', 'mathematica', 'image-processing']"
1095202,No group of order $400$ is simple - clarification,"I was reading through a proof that no group of order $400$ is simple which can be found here: https://math.stackexchange.com/a/79644/169389 Here is an outline for a solution. First of all, $|G| = 400 = 2^4 \cdot 5^2\ $ . By Sylow's theorem we know that the number of Sylow 5-subgroups must be a divisor of $2^4$ and that it is $1$ modulo $5$ . Thus it is either $1$ or $2^4$ . If there is only one Sylow 5-subgroup, it must be normal. For the other case, suppose first that the intersections of different Sylow 5-subgroups are always trivial. By counting elements you can conclude that $G$ has exactly one Sylow 2-subgroup, which is then normal. If we have Sylow 5-subgroups $P$ and $Q$ such that $P \cap Q \neq \{1\}$ , then $|P \cap Q| = 5$ . Therefore $P \cap Q$ is normal in $P$ and $Q$ , and thus is normal in the subgroup $\langle P, Q \rangle$ generated by $P$ and $Q$ . Finally, show that either $\langle P, Q \rangle$ is normal in $G$ or equals $G$ . I am trying to get to grips with general problems like this and feel that this is one of the best explanations, but I still feel that this argument goes too quickly and I still need some clarification on a few things. First problem : In the second paragraph we consider $n_5$ (number of sylow 5-subgroups) $=2^4$ and suppose that the intersections of different Sylow $5$ -subgroups $=1$ . When he says ""by counting the elements"", I guess he means the intersections of different Sylow $5$ -subgroups which would be $1$ which we have assumed, how can we now conclude that $n_2=1$ ? Second problem : I am wondering why $|P \cap Q| = 5$ and not $5$ or $25$ , I think that it may be because they are different their intersection would have to be $5$ since if the intersection is only of order $25$ when the groups are the same and of order $25$ , but if this could be clarified, it would be greatly appreciated Third problem : To show $\langle P, Q \rangle$ is normal in $G$ or equals $G$ . How did a commenter discern that the possible orders for $\langle P, Q \rangle$ are $50,100$ and $200$ , why not $40$ or $25$ ?","['simple-groups', 'abstract-algebra', 'sylow-theory', 'finite-groups', 'group-theory']"
1095272,Comparing sample and population standard deviation,"I want to compute the standard deviation of some data points that I obtain during four series of experiments. For the first three experiments that I have conducted, the number of data points that I obtain is quite limited and I am able to compute the ""classic"" population standard deviation. For the fourth experiment however, I obtain a very large number of data points (more than 2^48) for which I can not compute the population standard deviation. I can nethertheless compute the sample standard deviation on say, the first 2^32 data points. I am then wondering, is it right to compare the population standard deviation of the first three experiments with the sample standard deviation of the fourth experiment ? Thanks","['statistics', 'standard-deviation']"
1095323,Eigenvalues of Kronecker Product,"Maybe it's simple but I can't see the solution of this problem (Russell Merris, Multilinear Algebra , CRC Press, 1997, chapter 6, p.202, exercise 4): Let $\lambda_1,\ldots,\lambda_p$ be the eigenvalues of $A\in\mathbb C_{p,p}$ (multiplicities included), and $\omega_1,\ldots,\omega_q$ be the eigenvalues of $B\in\mathbb C_{q,q}$ respectively. Find the eigenvalues of a. $A \otimes B - B \otimes A$ . b. $A \otimes B + B \otimes A$ . From chapter 5, I know the eigenvalues of $A\otimes B$ and $A\otimes I_q + I_p \otimes B$ : The eigenvalues of $A \otimes B$ are $\lambda_i \cdot \omega_j$ , $1 \leq i \leq p$ , $1 \leq j \leq q$ The eigenvalues of $A\otimes I_q + I_p \otimes B$ are $\lambda_i + \omega_j$ , $1 \leq i \leq p$ , $1 \leq j \leq q$ These facts may give us a decomposition of $A\otimes B \pm B \otimes A$ . This may be very simple but I need a hint. I also made some Matlab calculations with integer matrices, and I get non-integer/non-real eigenvalues... maybe square roots are involved... Thanks!","['eigenvalues-eigenvectors', 'matrices', 'tensor-products', 'kronecker-product', 'multilinear-algebra']"
1095324,How to solve: $\frac{x}{\log_2(x) }= y$,"For example, I can solve: $x \log_2(x) = y$ $x \log_2(x) = x \log_e(x) / \log_e(2) = e^{\log_e(x)} \log_e(x) / \log_e(2)$ $e^{\log_e(x)} \log_e(x) = y\log_e(2)$ $e^{W(z)} W(z) = z$, where W(z) is the Lambert W-function $log_e(x) = W(y\log_e(2))$ $x = e^{W(y\log_e(2))}$ But how to (find $x$) solve: $\frac{x}{\log_2(x)} = y$ Answer: $$\displaystyle\begin{array}$x&=& \frac{1}{e^{W(ln(\frac{1}{2})^{1/y})}} = \frac{1}{e^{W(−ln(2)/y)}}
\end{array}$$","['calculus', 'transcendental-equations', 'discrete-mathematics']"
1095348,Finding Derivatives $f(x)={1\over x+1}$,"I'm using the Limit Definition to find the derivative, $$f'(x)=\lim_{\Delta x \to 0} {f(x+\Delta x) - f(x) \over \Delta x}$$
$$$$
Now, I want to find the derivative for the function, $$f(x)={1 \over x+1}$$ So, here's what I did. $$\lim_{\Delta x \to 0} {{1 \over (x+\Delta x) +1} - {1 \over x+1}\over \Delta x}$$ Now, I think I can multiply the numerator and the denominator by the least common multiple to get rid of the denominator in the numerator?? I'm not sure what to do from here.
Thanks","['calculus', 'derivatives', 'limits']"
1095378,Showing that a function is in $L^1$,"I need to prove the following statement or find a counter-example:
Let $u\in L^1\cap C^2$ with $u''\in L^1$. Then $u'\in L^1$. Unfortunately, I have no idea how to prove or disprove it, since the $|\bullet|$ in the definition of $L^1$ is giving me huge problems. I found counter-examples if either $u\notin L^1$ or $u''\notin L^1$, but none of them could be generalized to a example that satisfies all the conditions.","['lebesgue-integral', 'functional-analysis', 'real-analysis']"
1095391,$\frac{(2n)!}{4^n n!^2} = \frac{(2n-1)!!}{(2n)!!}=\prod_{k=1}^{n}\bigl(1-\frac{1}{2k}\bigr)$,"i cant see why we have : $$\frac{(2n)!}{4^n n!^2} = \frac{(2n-1)!!}{(2n)!!}$$ $$\dfrac{(2n-1)!!}{(2n)!!} =\prod_{k=1}^{n}\left(1-\dfrac{1}{2k}\right),$$ Even i see the notion of Double factorial this question is related to that one : Behaviour of the sequence $u_n = \frac{\sqrt{n}}{4^n}\binom{2n}{n}$ For $\frac{(2n)!}{4^n n!^2} = \frac{(2n-1)!!}{(2n)!!}$ $\dfrac{(2n)!}{4^n n!^2}=\dfrac{(2n)!}{2^{2n} n!^2}=\dfrac{(2n)\times (2n-1)!}{2^{2n} (n\times (n-2)!)^2}$ For $\dfrac{(2n-1)!!}{(2n)!!} =\prod_{k=1}^{n}\left(1-\dfrac{1}{2k}\right),$ note that $n!! = \prod_{i=0}^k (n-2i) = n (n-2) (n-4) \cdots$ $\dfrac{(2n-1)!!}{(2n)!!}=\dfrac{\prod_{i=0}^k (2n-1-2i)}{\prod_{i=0}^k (2n-2i)}$","['calculus', 'algebra-precalculus', 'real-analysis', 'analysis']"
1095420,Comparing notions of degree of vector bundle,"In this question, $X$ will be a smooth complex projective variety.  This question will be about comparing two different ways of calculating the degree of a vector bundle on such an $X$. I understand that, unless $X$ is a curve or $\mbox{Pic}(X)=\mathbb Z$, there is no well-defined notion of ""degree"" for vector bundles. I would like to use $X=\mathbb{P}^2$ as an example.  This is a variety with $\mbox{Pic}(X)=\mathbb Z$.  A vector bundle $V$ on $\mathbb{P}^2$ has a well-defined degree, which we define to be the image of $\det V$ under the natural identification of $\mbox{Pic}(X)$ with $\mathbb Z$.  For instance, if $V$ is the tangent bundle of $\mathbb{P}^2$, then $\det V=\mathcal O(3)$, and so $\deg V=3$. I have also seen the following way of defining the degree, for bundles on projective varieties: since $X$ is projective, there is a map $\kappa$ from $X$ into a projective space of the same or higher dimension.  Then we define a line bundle $\mathcal O_X(1)$ on $X$ by pulling back the $\mathcal O(1)$ from the projective space via $\kappa$.  Then, the degree of a bundle $V$ on $X$ is a number which I have seen denoted either $\deg(V):=c_1(\mathcal O_X(1))^{d-1}.c_1(V)$ or $\deg(V):=c_1(\mathcal O_X(1))^{d-1}\cap c_1(V)$, where $d=\dim(X)$. Question: In the case of $X=\mathbb P^2$, the map $\kappa$ is the identity, and we have $\deg(V)=c_1(\mathcal O(1))\cap c_1(V)$, where $\mathcal O(1)$ is just the hyperplane bundle of $\mathbb P^2$ itself. When $V$ is the tangent bundle, how do I see that the number $c_1(\mathcal O(1))\cap c_1(V)$ is equal to $3$?  My ""instinct"" is to think of $\mathcal O(1)$ as the line bundle of a divisor $D$, which in turn is determined by a hypersurface, specifically a projective line in $\mathbb P^2$, and then ask how many times that line is intersected by a ""generic"" section of $V$ (a vector field on $\mathbb P^2$, in this case, but I don't want this to depend too much on the fact that these sections are vector fields). Is this the right way to think? If it is, then two more questions: (1) How can I see in a fairly intuitive way that this intersection number is $3$? (2) Why would the intersection product of two Chern classes (elements of the cohomology ring $H^*(X,\mathbb Z)$) be equal to the intersection number of a divisor and a vector field (or of two divisors, more generally)?  (I guess there is a cohomology $\leftrightarrow$ geometry philosophy that I need to understand better.)","['vector-bundles', 'algebraic-geometry', 'holomorphic-bundles', 'algebraic-topology']"
1095425,To check continuity of Multivariable functions,"To check continuity of function at origin given by 
$$F (x, y) = \begin{cases}\dfrac{xy^{2}}{x^{2} + y ^{4}}&;& \mbox{otherwise},\\
0&;&\mbox{ at origin}.
\end{cases}$$","['multivariable-calculus', 'continuity', 'limits']"
1095446,How to obtain $n$ maximally different binary vectors with equal number of zeros and ones?,"Imagine the set of all binary vectors of length $2m$ where each of the vectors has $m$ ones and $m$ zeros. I want to select some $n$ of these vectors such that the shortest distance among all pairs of vectors from this set of $n$ vectors is maximized. In other words I want a set of vectors which are maximally distant from each other. I define distance by the number of positions at which the corresponding digits are different. For example distance between vectors $[1,0,0,1,1,0]$ and $[1,1,0,0,0,1]$ is $4$. For example for $m=4$ and $n=2$ the solution can be $[1,1,1,1,0,0,0,0]$ and $[0,0,0,0,1,1,1,1]$. How can I find a solution for arbitrary $m$ and $n$?","['coding-theory', 'discrete-mathematics', 'vectors', 'extremal-combinatorics', 'combinatorics']"
1095468,Construction of bijection between set of functions,"$f : X \leftrightarrow Y$ and i want to construct bijection between $\lbrace g :     
X \rightarrow A \rbrace$ and $\lbrace g : Y \rightarrow A \rbrace$.
How to do this?",['elementary-set-theory']
1095477,"Connections between the solution of simple ordinary equation, normal distribution and heat equation","The solution to the following simple first-order linear ordinary differential equation: $$x'=-tx, x(0)=\frac{1}{\sqrt{2\pi}}$$ is the Standard normal distribution ! One solution to another famous (partial) differential equation, the heat equation, is - given the right conditions - also the normal distribution. This is far from being a coincidence (see e.g. here and many more). My question Does this mean anything? Is there an intuition why this is the case or is this just a coincidence? Or are there any deeper connections between the above ordinary differential equation, the heat equation and the normal distribution? Edit: I asked a question concerning the physical interpretation of the right hand side of the equation here: https://physics.stackexchange.com/questions/158425/which-physical-entities-equal-distance-times-time This is also the reason why I changed the original notation to $t$ and $x(t)$ because I think it could give a better physical intuition to think in terms of time, distance and velocity.","['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'connections', 'intuition']"
1095478,"If $X$ and $Y$ are independent random variables, is $\mathbb{E}(f(X)\mid X\le Y)=\mathbb{E}(f(X))$?","This could be a stupid question, but here goes. If $X$ and $Y$ are independent random variables, is $\mathbb{E}(f(X)\mid X\le Y)=\mathbb{E}(f(X))$? The answer depends on the sigma algebras generated by the event $X\le Y$ and I do not think they are independent of $X$.","['probability-theory', 'probability']"
1095490,"If $\{A, B, C\}$, $\{X, Y, Z\}$, and $\{A + B + C, X + Y + Z\}$ are independent random variables, are $\{A, B, C, X, Y, Z\}$ independent?","Let {A,B,C} be independent random variables and let {X,Y,Z} be independent random variables. Given that A+B+C and X+Y+Z are independent, is {A,B,C,X,Y,Z} independent? How do I show it?",['probability-theory']
1095493,Show that a particular sequence ($x_{n+1}= \frac{1}{4 - x_{n}}$) converges,"Would someone be able to verify that my proof that the following sequence converges is sound? We have a sequence $\left ( x_{n} \right )_{n\geq i}$ where $x_{1} = 3$ and $x_{n+1}= \frac{1}{4 - x_{n}}$. We wish to show that this sequence converges (and then determine its limit). So, I applied the monotone convergence theorem, showing it is bounded and monotone decreasing by induction and hence converges. So, the base case is fine, showing that $\frac{1}{4} \leq x_{2} = 1 < x_{1} = 3 \leq 3$. Then I made the assumption that $\frac{1}{4} \leq x_{k+1} < x_{k} \leq 3$. Now, we want to show: $\frac{1}{4} \leq x_{k+2} < x_{k+1} \leq 3$ (I think this is correct?) So, it is probably possible to do this as one big inequality if you are good at that, however, for simplicity, I split it and showed $x_{k+2} < x_{k+1}$ first and then that $\frac{1}{4} \leq x_{k+2}$ and $x_{k+1} \leq 3$. So: $x_{k+1} < x_{k}$ (by our assumption) $ \Rightarrow   4 - x_{k} < 4 - x_{k+1}  $ $ \Rightarrow   \frac{1}{4 - x_{k+1}} < \frac{1}{4 - x_{k}}  $ (both terms less than 3 by our assumption, so, we can divide without changing inequality sign) $ \Rightarrow  x_{k+2} < x_{k+1}   $ $ \Rightarrow$ monotone decreasing So, now that we have shown that the sequence is decreasing, we just need to show both bounds, like so: $\frac{1}{4} \leq x_{k+1}$ and $x_{k} \leq 3$ (by our assumption) $ \Rightarrow 4 - x_{k+1} \leq 4 - \frac{1}{4}$ and $ 4 - 3 \leq 4 - x_{k}$ $ \Rightarrow \frac{1}{\frac{15}{4}} \leq \frac{1}{4-x_{k+1}} $ and $ \frac{1}{4 - x_{k}} \leq 1$ $ \Rightarrow \frac{1}{4} < \frac{4}{15} \leq x_{k+2}$ and $ x_{k+1} \leq 1 < 3$ Thus, combining both parts of the proof together once again we form the inequality: $ \frac{1}{4} < x_{k+2} < x_{k+1} < 3$ Hence, by induction, we have shown that the sequence is bounded below by $\frac{1}{4}$, is strictly decreasing and is bounded above by 3, thus, the sequence converges by the monotone convergence theorem. Does anyone spot any errors in my proof? Thanks!","['real-analysis', 'analysis']"
1095541,How to simplify a fraction involving factorials,I have following term: $$\frac{\frac{3^{2k+2}}{(2k+2)!}}{\frac{2^{2k}}{(2k)!}}=\frac{3^{2k+2}\cdot(2k)!}{(2k+2)! \cdot 3^{2k}}=9\cdot\frac{(2k)!}{(2k+2)!}$$ I know that you can simplify even further to $$\frac{9}{(2k+2)(2k+1)}$$ but I do not get the intermediate steps. I know that $$(n+1)! = n!(n+1) = (n-1)! \cdot (n+1) \cdot n$$ but I still do not know how to apply this on terms different from $(n+1)$ (e.g. $(2n+2)$)?,"['factorial', 'algebra-precalculus']"
1095568,"A fair die is rolled $N > 6$ times. What is the probability the last 6 rolls were exactly $1,2,3,4,5,6$?","You rolls a fair die $N > 6$ times and you want to rolls the sequence $1,2,3,4,5,6$ in this order. What is the probability that the last 6 rolls were (in consecutive order) $1,2,3,4,5,6$, (So, on the Nth roll you get a 6, on the $(N-1)$th roll you get a 5, etc.), AND you did not roll this sequence any times before the last 6 rolls. If I state another way, what is the probability that you roll the sequence 1,2,3,4,5,6 starting from the $(N-5)$th roll and you did not roll this sequence starting from any roll before the $(N-5)$th roll? This is not a homework problems, just something I'm thinking about.","['dice', 'probability']"
1095569,What's the connection between exceptional divisor and projectivized tangent space?,"This is one homework problem and hence I want some hint but not a whole answer. Let $P$ be a projective space and $X\subset P$ be a non-singular variety.
Prove that the collection $L_p$ of lines contained in $X$ that pass through $p$ defines a closed subset of the projectivized tangent space $\mathbb{P}(T_p X)$. In other word, we want to show that such defined $L_p$ defines a closed subset of $\mathbb{P}(T_p X)$:$$L_p = \{l:p\in l\subset X, l\text{ is a 1-dimensional linear subspace in P}\}.$$ I cannot see the connection between $L_p$ and $\mathbb{P}(T_p X)$. Please try to explain the connection using elementary knowledge. Thanks.","['algebraic-geometry', 'algebraic-curves']"
1095621,"Integration question: $\int \sqrt{x^2-4}\, dx $",I am looking for a way to integrate $$\int \sqrt{x^2-4}\  dx $$ using trigonometric substitutions. All my attempts so far lead to complicated solutions that were uncomputable.,"['calculus', 'integration', 'indefinite-integrals']"
1095623,What is $\mathbb{P}^{\infty}$?,"Can we look at a complex projective space $\mathbb{P}^{\infty}$? I am curious to know what would it be. What is the right intuition to think about it? I know $\mathbb{P}^{n}$ is a space of $1-$dimensional subspaces of $\mathbb{C}^{n+1}$, but have never seen $\mathbb{P}^{\infty}$.","['algebraic-geometry', 'algebraic-topology', 'complex-geometry', 'projective-geometry']"
1095641,"If $f(AB) =f(A)f(B)$, then $A$ is inversible iff $f(A)\neq 0$","Let $f:\mathscr M_n(\mathbb K) \to \mathbb K$ be a non constant function such as $f(AB) = f(A)f(B)$ for all $A,B$ in $\mathscr M_n(\mathbb K)$. The question is to show that $M\in GL_n(\mathbb K)$ iff $f(M)\neq 0$. I've already shown that $f(Id) = 1$ and $M\in GL_n(\mathbb K) \Rightarrow f(M)\neq 0$. I'm stuck on the converse.","['matrices', 'inverse']"
1095659,An inequality concerning non-negative integer matrices with constant row and column sums,"I'd appreciate any suggestions for how to prove (or disprove) the inequality described below. Some notation first: for positive integers $k$ and $M$, let ${\mathcal D}_{k,M}$ denote the set of all $k \times k$ non-negative integer matrices with row and column sums all equal to $M$. For $k \times k$ matrices $A = [a_{i,j}]$ and $B = [b_{i,j}]$, we write $A \le B$ if $a_{i,j} \le b_{i,j}$ for all $i,j$. Now, let $M, N$ be fixed positive integers with $M < N$. I'd like to prove that for any $B \in \mathcal{D}_{k,N}$, we have $\displaystyle \sum_{A \in \mathcal{D}_{k,M}: A \le B} \prod_{i,j} \frac{\binom{b_{i,j}}{a_{i,j}}}{\binom{N-b_{i,j}}{M-a_{i,j}}} \ge {\binom{N}{M}}^{2k-k^2}$. It is easy to check that the inequality holds with equality when $B$ is $N$ times a permutation matrix. It is also straightforward to show that the inequality holds when $B$ has at most two non-zero entries in each row. In particular, the inequality holds when $B$ is a convex combination of $B_1$ and $B_2$, where each of $B_1$ and $B_2$ is $N$ times a permutation matrix. How to proceed when $B$ is a convex combination of three or more such matrices is not clear.","['matrices', 'inequality', 'binomial-coefficients']"
1095666,Lebesgue integral and absolute value,"I wonder why we say that $f$ is integrable iff $\int|f|\,d\mu$ is finite? Why we use absolute value? Won't it be enough to have that $\int f\, d\mu$ is finite to call $f$ integrable? Are there functions $f$ for which $\int f \ d\mu$ is finite but $\int |f| \ d\mu$ is not (like conditional convergence for series)?. But then why we would call such $f$ not Lebesgue integrable?","['lebesgue-integral', 'integration']"
1095671,Question about a proof in Rudin's book - annihilators,"I'm reading the proof of the following theorem in Rudin's ""Functional Analysis"": Let $M$ be a closed subspace of a Banach space $X$. 
The Hahn-Banach theorem extends each $m^* \in M^*$ to a functional $x^* \in X^*$. 
Define $\sigma (m^*) = x^* + M^{\perp}$. 
Then $\sigma$ is an isometric isomorphism of $M^*$ onto $X^*/M^{\perp}$. My problem is this part of the proof: Fix $m^* \in M^*$. If $x^* \in X^*$ extends $m^*$, it is obvious that $||m^*|| \le ||x^*||$ The greatest lower bound of the numbers $||x^*||$ so obtained is $||x^* + M^{\perp}||$, by the 
definition of the quotient norm. Hence 
$||m^*|| \le || \sigma (m^*) ||  \le ||x^*||$ 
But Theorem 3.3 furnishes an extension $x^*$ of $m^*$ with $||x^*|| = ||m^*||$. It follows 
that $|| \sigma( m^*)|| = ||m^*||$. 3.3 Theorem Suppose $M$ is a subspace of a vector space $X$, $p$ is a seminorm on $X$, 
and $f$ is a linear functional on $M$ such that 
$|f(x)| \le p(x) \ \  (x \in M)$. 
Then $f$ extends to a linear functional $\Lambda$ on $X$ that satisfies 
$| \Lambda x | \le p(x)$  $ \ \ (x \in X)$. What I don't see is how theorem 3.3 implies the equality of norms: $||x^*|| = ||m^*||$ and why is $||m^*|| \le ||x^*||$ (in the first line). It would be clear to me if $||x^*||$ was a regular norm of a linear mapping (we take the supremum over a bigger set) but I don't see it if it is a quotient norm. Could you explain that to me? Thank you","['normed-spaces', 'functional-analysis']"
1095690,How to evaluate $\cos(22^\circ)\cos(38^\circ) - \sin(22^\circ)\sin(38^\circ)$?,How does one evaluate this? Does this generalize to $\cos(x)\cos(y) - \sin(x)\sin(y)$?,['trigonometry']
1095711,Measurability of an a.e. pointwise limit of measurable functions.,Suppose that $(f_n)_n$ is a sequence of measurable functions on a set $E$ and that $f_n \to f$ a.e.on $E$. Does this imply that $f$ is  measurable? I know that pointwise limit of measurable function is measurable. But here we only have convergence a.e. So I got confused.,"['measure-theory', 'convergence-divergence', 'almost-everywhere']"
1095720,Prove inequality $\frac{a^3}{b+c}+\frac{b^3}{c+a}+\frac{c^3}{a+b}\geq(a-b)^2+(b-c)^2+(c-a)^2 $,"I cannot prove the following inequality. Let $a,b,c$ be positive numbers.Prove that:
  $\dfrac{a^3}{b+c}+\dfrac{b^3}{c+a}+\dfrac{c^3}{a+b}\geq(a-b)^2+(b-c)^2+(c-a)^2. $ I tried to use Cauchy inequality. I think the functions $f(a,b,c)=\dfrac{a^3}{b+c}+\dfrac{b^3}{c+a}+\dfrac{c^3}{a+b}$ and $g(a,b,c)=(a-b)^2+(b-c)^2+(c-a)^2$ are  symmetric with respect to $a,b,c.$","['inequality', 'calculus', 'algebra-precalculus']"
1095737,Is The Degree of the Minimal Monomial of a Group Equal to Its Exponent?,"Suppose $G$ is a group with identity $e$ such that $x^n=e$ for all $x\in G$ and such that no smaller $n$ satisfies this. If, for some constants $a_i\in G$ it holds for all $x$ that
$$a_1\cdot x\cdot a_2\cdot x\cdot a_3\cdot x\cdot \ldots\cdot a_k \cdot x = e$$
then does $n$ necessarily divide $k$? Put another way, if we consider the above to be a ""monomial"" term, defining its degree to be the number of times it multiplies by $x$, is it necessary that the exponent of the group divides the degree of any monomial sending all elements to the identity? This is clearly true of abelian groups, because then the above expression collapses to
$$x^k=\prod_{i=1}^ka_i^{-1}$$
for all $x$, implying $x^k=y^k$ for all $x,y\in G$, and setting $y=e$ yields that the above is equivalent to $x^k=e$ for all $x$, which does imply that the exponent of the group divides $k$. Is the statement true in the non-abelian case too?",['group-theory']
1095738,Why for dirac function $\int_{-\infty}^{\infty}\delta(x)\ dx=1$,"The dirac function is defined as $\delta(x)=\infty$ when $x=0$, $\delta(x)=0$ otherwise. I am wondering why we can derive $\int_{-\infty}^{\infty}\delta(x)\ dx=1$, or this is just a definition",['analysis']
1095739,"Question on Proof: when two measures agree on $\pi$-system, then they agree on generated $\sigma$-algbra","Let $\mu_1, \mu_2$ be measures on $\sigma(\mathcal P)$, where $\mathcal P$ is a $\pi$-system, and suppose they are $\sigma$-finite on $\mathcal P$. If $\mu_1$ and $\mu_2$ agree on $\mathcal P$, then they agree on $\sigma(\mathcal P)$. My proof: Define
$$
 \mathcal G := \{ A \in \sigma(\mathcal P) : \mu_1(A) = \mu_2(A) \}.
$$
Then $\mathcal P \subseteq \mathcal G$ because they agree on $\mathcal P$. Further $\mathcal G$ is a $\sigma$-algebra: i) $\Omega \in \mathcal G$. Let $\Omega = \bigcup_k P_k$ where the $P_k$ are disjoint, and $\mu_1(P_k) < \infty$ and $P_k \in \mathcal P$ (and so $\mu_2(P_k) = \mu_1(P_k)$), such a decomposition exists because of $\sigma$-finiteness. Then
$$
 \mu_1(\Omega) = \mu_1\left( \bigcup_k P_k \right) = \sum_k \mu_1(P_k) = \sum_k \mu_2(P_k) = \mu_2\left(\bigcup P_k\right) = \mu_2(\Omega)
$$ 
and so $\Omega \in \mathcal G$. ii) Closure under complement. Let $A \in \mathcal G$, then $A \in \sigma(\mathcal P)$ and $\mu_1(A) = \mu_2(A)$, by this
$$
 \mu_1(A^c) = 1 - \mu_1(A) = 1 - \mu_2(A) = \mu_2(A^2)
$$
and so $A^C \in \mathcal G$. iii) Closure under countable union. Let $\{ A_n \} \subseteq \mathcal G$. Then $\bigcup A_n \in \sigma(\mathcal P)$
because it is a $\sigma$-algebra and
$$
 \mu_1\left( \bigcup_n A_n \right) = \sum_n \mu_1( A_n ) = \sum_n \mu_2( A_n ) = \mu_2\left( \bigcup_n A_n \right)
$$
and so $\bigcup A_n \in \mathcal G$. With $\mathcal P \subseteq \mathcal G$ we have $\sigma(\mathcal P) \subseteq \mathcal G$, and more specific $\sigma(\mathcal P) = \mathcal G$, so that both measures agree on all sets of $\sigma(\mathcal P)$. $\square$ Is this proof okay? I came up with it myself, but when I look it up in a book, the one I found is much more complicated, which makes me suspicious, because my proof is quite simple and straight, but I do not see where it could fail?","['probability-theory', 'measure-theory']"
1095743,Innocent looking open problems in real analysis,"Are there any apparently easy problems or conjectures in basic real analysis (that is, calculus) that are still open? By apparently easy, I mean: so much so, that, if it was for the statement alone, they could be part of a calculus book for undergraduates?","['calculus', 'big-list', 'soft-question', 'real-analysis', 'open-problem']"
1095751,a way to integrate: $\int (\sqrt{x} +3)/(2+ x^ \frac{1}{3}) dx$,"Im looking for a way to integrate: 
$$
\int \frac{ \sqrt{x} +3}{2+ x^ \frac{1}{3}} dx
$$ 
that would make it efficient and not too difficult... Any suggestions?","['calculus', 'integration', 'indefinite-integrals']"
1095753,Find $\lim_{x \to 0}\frac{x-\sin(x)\cos(x)}{\sin(x)-\sin(x)\cos(x)}$,"Find $$\lim_{x \to 0}\frac{x-\sin(x)\cos(x)}{\sin(x)-\sin(x)\cos(x)}\;.$$ Applying L'Hopital's rule directly does not seem to get me anywhere.  I also tried dividing the numerator and denominator by $\sin(x)$, which did not seem to work. Is there a some sort of trick I am missing here?","['calculus', 'limits']"
1095756,Row-normalized and column-normalized matrix notation,"I'm searching for the mathematical, algebraic notations of a row-normalized and column-normalized matrix. For example, let us consider the following matrix A: $$
A = \begin{pmatrix}
2 & 7 \\
4 & 3
\end{pmatrix}
$$ What is the mathematical notation of its corresponding row-normalized matrix? $$
\begin{pmatrix}
2/9 & 7/9 \\
4/7 & 3/7
\end{pmatrix}
$$ What is the mathematical notation of its corresponding column-normalized matrix? $$
\begin{pmatrix}
2/6 & 7/10 \\
4/6 & 3/10
\end{pmatrix}
$$ Best regards.","['notation', 'matrices']"
1095770,2N digit number with exactly N ones and N zeros. Prove there exists a number divisible by N,"Given a decimal number 2N digits long, but containing exactly N ones (1s) and N zeros (0s). Prove (or disprove) there exists an arrangement of the N 1s such that N is a divisor of the 2N digit number. The problem requires that the MSD always contain a 1. (Otherwise the specification of a 2N digit number would be ambiguous). Note, this is not asking if ALL arrangements of the N digits produces a 2N digit number divisible by N. Rather, it is asking if there exists SOME arrangement of the N digits causing the 2N digit number to be divisible by N. I'm adding some thoughts I had on this. A 1 at the ith digit (Di) represents the ith power of 10. The 2N digit number will be divisible by N if SUM(Di)=0(mod N) Successive powers (in natural sequence) of 10 will generate all p-1 residues Mod N if 10 is a primitive root of N= p= prime number. A full set of p-1 residues has a sum=0 (mod N=p) In this case we need one more 1 for the full N 1 digits. It's effect can be cancelled because we have a complete set of residues. We still maintain Sum=0 mod p, by deleting one Di of them and adding 2 such that their sum = the deleted Di. So problem is soluble when N is prime and 10 is a primitive root of N. If 10 is not primitive (N still prime), the cycle length of successive powers is less than p-1, where cycle length is a divisor of p-1. In this case we cannot necessarily delete one and find 2 whose sum mod p is the same. That is because only a subset of residues is available. More complications are when considering non sequential arrangements and composite N",['number-theory']
1095778,Induced Subgraph in Gap program,"I'm stuck with a function in Gap. I want to construct an induced subgraph but when I remove a vertex, example 3 in V=(1,2,3,4,5), Gap numbers the remaining vertices from 1-4. Can I somehow keep the vertex number as they were before? 
I use the trivial group when constructing graph with EdgeOrbitsGraph. There is a third parameter in the function but I haven't learned about groups yet so I don't know how to use it.
Thank you Example gap> LoadPackage(""grape"");
─────────────────────────────────────────────────────────────────────────────
Loading  GRAPE 4.6.1 (GRaph Algorithms using PErmutation groups)
by Leonard H. Soicher (http://www.maths.qmul.ac.uk/~leonard/).
Homepage: http://www.maths.qmul.ac.uk/~leonard/grape/
─────────────────────────────────────────────────────────────────────────────
true
gap> T:=SymmetricGroup(0);
Group(())
gap> L:=[[1,2],[2,1],[1,3],[3,1],[2,3],[3,2]];
[ [ 1, 2 ], [ 2, 1 ], [ 1, 3 ], [ 3, 1 ], [ 2, 3 ], [ 3, 2 ] ]
gap> G:=EdgeOrbitsGraph(T, L, 3); 
rec( adjacencies := [ [ 2, 3 ], [ 1, 3 ], [ 1, 2 ] ], group := Group(()), 
  isGraph := true, order := 3, representatives := [ 1, 2, 3 ], 
  schreierVector := [ -1, -2, -3 ] )
gap> V1:=[1,3]; 
[ 1, 3 ]
gap> H:=InducedSubgraph(G, V1); 
rec( adjacencies := [ [ 2 ], [ 1 ] ], group := Group(()), isGraph := true, 
  names := [ 1, 3 ], order := 2, representatives := [ 1, 2 ], 
  schreierVector := [ -1, -2 ] )","['gap', 'discrete-mathematics']"
1095813,How to solve this rational equation?,"I'm stuck on this rational expression. I factored and simplified, by what do I do next? Should I divide x/2x and 8/4? I posted my work below. Thank you!","['factoring', 'rational-functions', 'algebra-precalculus']"
1095814,when can I say there is a relationship between events?,"I am by no means a math expert, but I am analyzing very large weather data data for a computer science course .I am taking away all the weather related issues out of the analyzing and just looking strictly at it in the eyes of statistics. Lets say there are 2 events, 1 occurs 250 times in the year, and the other event occurs 50 times in a year. The second event occurs every single day the first event occurred. Can I say that there is a relationship between the two events? I am not sure if it is safe to say there is a good chance there is a relationship between the data because this could just be a coincidence. For example if the event A occurred 25 times a year and event B occurred 15 times again event B occurring every time event A occurred I would be more confident to say there is a relationship between the two events because there is less of a chance it is a coincidence due the the number of times event A occurred. To rephrase, lets say event A occurred 365 times in the year and event B occurred 10 times in the year, event B would occur every single time event A occurred, but this is just because event A occurred every day so there is no relationship. Is there some sort of standard to say that when two events have a statistically high probability of having a relationship of some sort? Hope this makes sense.",['statistics']
1095827,Scaling and rotating a square so that it is inscribed in the original square,"I have a square with a side length of 100 cm.  I then want to rotate a square clockwise by ten degrees so that it is scaled and contained inside the existing square. The image below is what I'm attempting, but with a square, not a triangle. (Image created by Alain Matthes ). And here is the square I'm trying to create, so hopefully this helps: So my question, how do I determine the distance between the original point and the rotated point? Or, how do I determine the location of the rotated point on its own, independent of the original square?  I'm not sure what formula I would use in either instance. Any assistance in this endeavor would be greatly appreciated.","['geometry', 'polygons', 'rotations', 'art', 'euclidean-geometry']"
1095869,Zero Partials imply Constant Function Theorem or Proof,"Let $f(x,y)$ be a function of two variables. Given that 
$$\frac{\partial f}{\partial x}=0$$ for all $y$, and 
$$\frac{\partial f}{\partial y}=0$$ for all $x$, is there a theorem that states $f(x,y)=$ constant for all $x$, $y$? Or how would I prove this rigorously?","['multivariable-calculus', 'partial-derivative']"
1095900,Mollifiers: Integral Bound,"Desirable is an example such that:
$$\varphi_0\in\mathcal{C}^\infty_0:\quad\int_0^\infty\left|\varphi_0^{(n)}(s)\right|\mathrm{d}s\leq2^n$$ (It should not exist as one would obtain entire elements for generators of semigroups.) How to prove that they must always exceed the bound?","['functional-analysis', 'integration', 'real-analysis']"
1095952,Simulating simultaneous rotation of an object about a fixed origin given limited resources.,"Sorry if the title is a bit cryptic. It's the best I could come up with. First of all, this question is related to another question I posted here , but that question wasn't posed correctly and ended up generating responses that may be helpful for some people who stumble upon the question, but don't address my problem as it stands. Also, a disclaimer: I am going to be posting some code here. I will be annotating it to try to make it as clear as possible what I am doing. Okay, so I am using the Processing programming language to move a box around randomly(according to Perlin noise) As the box moves around, I would like it to rotate such that it looks like it is rolling. I am not looking to simulate exact physics here, I am only looking for a visually pleasing solution. The general rules for rotation are that, if the box is only moving to the right, then it should rotate around the Y axis such that it rotates to the right and if the box is only moving upwards, it should rotate about the  $X$-axis such that it looks like it is rolling directly upwards. In other words, if this is what the box normally looks like: Then, if I say: rotate2D(45,0) I should get: and if I say rotate2D(0,45) I should get: (for clarity, the function declaration for rotate2D looks like rotate2D(horizontal, vertical) ) Processing makes this easy to do with the rotation functions that it provides, which are: rotateX();
rotateY();
rotatez(); The problem arises when you want to rotate in 2 dimensions simultaneously. You see, in Processing, If I call rotateX() to rotate an object about the $X$-axis, it is actually the coordinate system itself that gets shifted and all of the other axis go along with it, so when I go to do rotateY() after doing a rotateX() I wont get the result I want because the $Y$-axis that I am rotating around has now been skewed. You can see this in the pictures above. So, If I do: rotateX(radians(45));
rotateY(radians(45)); I get: and if I do: rotateY(radians(45));
rotateX(radians(45)); I get: But what I want is something like: which I drew by calling my function rotate2D() The problem is that the results that my function produces are a bit counter intuitive. It is not apparent in the above example, but lets say I want to do: rotate2D(90,90); . I would think that would produce a box that looks like It hasn't been rotated at all right? However, it actually produces: This is my current implementation of rotate2D() The Function: void rotate2D(float horizontal, float vertical)
{  
  float[] rotations ={horizontal, vertical};
  if(rotations[0]>rotations[1])
  {
    float tmp = rotations[1];
    rotations[1]=rotations[0];
    rotations[0]=tmp;
  } 

  if(rotations[1]==vertical) rotateX(radians(rotations[1]-rotations[0]));
  else rotateY(radians(rotations[1]-rotations[0]));

  float hIncr= (horizontal<0) ? -0.1 : 0.1;
  float vIncr= (vertical<0) ? -0.1 : 0.1;
  for(float i=0; i<=abs(rotations[0]);i+=0.1)
  {    
    rotateY(radians(hIncr));
    rotateX(radians(vIncr));
  }   

} Function Explanation: What the function is doing is essentially as follows (and this is all you really need to address the question from a mathematical standpoint). The function slowly performs a bunch of $X$ and $Y$ axis rotations right after each other by a small amount each time(0.1 degrees) until it reaches an upper bound. That upper bound is the smaller of the values that the function was passed. The box will have already been rotated about the proper axis by the difference between the larger and the smaller value before this sequence takes place. To further clarify, I am looking for a mathematical explanation for why this process is producing counter intuitive results, which will hopefully shed light on a solution. The ideal end goal here is to find a formula that i can use to simulate rotation about a fixed axis in multiple dimensions when all I can do is rotate 'the world' one axis at a time as described above (which is essentially equivalent to rotating the objects relative axes) please do not hesitate to ask me to clarify further. Thanks in advance. EDIT After reading the comments, I now understand that rotations about 2 axis do not commute and I should not expect the results that I was previously expecting. What I am calculating and passing to my function is the net rotation in each direction. So, is it possible to edit my formula so that it produces this net rotation? For example, in the 90/90 example, If I continue the sequence for ~37 extra steps (127 total) I get the result that I intuitively want to get) This would need to work for all combinations of inputs though.","['geometry', 'rotations', 'matrices', 'trigonometry', 'matrix-equations']"
1095988,Convergene of a sequence if and only if limsup and liminf agree,"This is in terms of sets (within the context of probability measure theory) if that matters. The book says ""this is equivalent"" without proof, I want to prove the gap! Indicator function of a set $A$ is defined as: $\mathbb{I}_A(\omega)=1$ if $\omega\in A$  else $=0$ defined over all of $\Omega$, basically it's a function that is 1 over $A$ and 0 elsewhere. Let $\{A_n\}$ be a sequence of subsets of $\Omega$ inside the $\sigma$-algebra $\mathcal{A}$ (this just means it is closed under countable union and intersections) $A_n\rightarrow A$ ($A_n$ is said to converge) if $\lim_{n\rightarrow\infty}(\mathbb{I}_{A_n})=\mathbb{I}_A$ Limsup and liminf are defined as follows $\limsup_{n\rightarrow\infty}(A_n)=\cap^\infty_{n=1}(\cup_{i=n}^\infty A_i)$ $\liminf_{n\rightarrow\infty}(A_n)=\cup^\infty_{n=1}(\cap_{i=n}^\infty A_i)$ The claim is $A_n\rightarrow A\iff A=\limsup(A_n)=\liminf(A_n)$ I've tagged this ""Real analysis"" because that's where I learnt about limits, and consider the definition of them to be under its umbrella. What have I tried? I'm struggling to get started, I'm not sure if I should be looking at the limit of the indicator function pointwise or as a function, and if so how to consider it in those separate lights, and then use the information from limsup and liminf to show the result. So I need to more carefully define the limit of the indicator function (does it start: $\forall\omega\in\Omega\forall\epsilon>0$ or does it start $\forall\epsilon>0\forall\omega\in\Omega$) then get that in terms of the limsup and liminf. I can see glimpses of the proof but evidently cannot formulate it entirely. I'd love some clearing up. I've spotted similar questions just now and I shall read those, I would none the less like to know how I should be treating convergence of a function, point-wise or not.","['measure-theory', 'real-analysis']"
1096028,How to take the definite integral on both sides of a differential equation?,"For instance, $$a \cdot ds=dt$$ I know that one can take the indefinite integral on both sides to get $$\int a ds = \int 1 dt$$ But how do I take the definite integral of both sides, and exactly what do I need to know to do this? (Specifically, the bounds.  How do I know what bounds to use?)","['ordinary-differential-equations', 'calculus']"
1096041,Continuous function on a compact set with no fixed points,"I'm reviewing this problem for my analysis qual. Let $f:X\rightarrow X$ be a continuous mapping from a metric space to itself. Assume $f $ has no fixed points. Prove that, if $X $ is compact, there exists an $\epsilon $ such that $d (x, f (x)) \ge \epsilon $ for every $x\in X $. I'm having trouble figuring out what to do. I predict having to take neighborhoods of every $x\in X $ and find a finite subcover. But I'm not sure where that gets me in terms of the fixed point thing. I know that $f (x) \ne x $ means each center of the balls comprising the finite subcover will have to move, but I don't know what that gets me. Help?","['continuity', 'compactness', 'analysis']"
1096051,Calculate size and power of a given PMF,"Let $X$ be a random variable having probability mass function $f(x) = \begin{cases}
\dfrac{2+4a_1+a_2}{6},  & \text{if $x=1$} \\
\dfrac{2-2a_1+a_2}{6}, & \text{if $x=2$} \\
\dfrac{1-a_1-a_2}{3}, & \text{if $x=3$}
\end{cases}$ where $a_1>0$ and $a_2>0$ are unknown parameters such that $a_1+a_2\leq 1$. For testing the null hypothesis $H_0:a_1+a_2=1$, against the alternative hypothesis $H_1:a_1=a_2=0$, suppose that the critical region is $C=\{2,3\}$. Then calculate the size and power for the critical region. Power=$1-\beta$ where $\beta=P(\text{accept }H_0\mid H_1\text{ is true})=\dfrac{2+4a_1+a_2}{6}=\dfrac{1}{3}$ So Power = 2/3 Now, size $= P(\text{reject }H_0\mid H_0\text{ is true})=\text{critical region} = \dfrac{2-2a_1+a_2}{6}+\dfrac{1-a_1-a_2}{3}$. So, how do I calculate a numerical value here ? Please advise.","['statistics', 'hypothesis-testing', 'statistical-inference']"
1096053,Is $0.\overline{0}1$ possible? [duplicate],"This question already has answers here : Does $1.0000000000\cdots 1$ with an infinite number of $0$ in it exist? (8 answers) Closed 9 years ago . Is $0.\overline{0}1$ possible in mathematical terms? I know that if you have a repeating decimal, the number is infinite and doesn't end. Thus, the 1 at the end here would stop the repeating decimal, am I right? Is this possible?",['algebra-precalculus']
1096069,What is the coefficient of $x^{18}$ in the expansion of $(x+x^{2}+x^{3}+x^{4}+x^{5}+x^{6})^{4}$?,How to approach this type of question in general? How to use binomial theorem? How to use multinomial theorem? Are there any other combinatorial arguments available to solve this type of question?,"['discrete-mathematics', 'elementary-number-theory', 'combinatorics']"
1096089,Continuous function with non-negative second derivative in the weak sense is convex,"I am currently working through a section of Peter Petersen's Riemannian Geometry in which he talks about weak second derivatives of functions. I am trying to work through the details of why a function on a Riemannian manifold $(M,g)$ with non-negative Hessian in the weak sense is convex. I understand how one can reduce the problem to the case where $M$ is the real line with the Euclidean metric by precomposing with unit speed geodesics, so I have been able to reduce the problem to the following: Proposition: Let $f$ be a continuous function defined on an open interval in $\mathbf{R}$ . We say that $f''(p)\geq 0$ in the weak sense if for every $\varepsilon>0$ there exists a smooth function $f_\varepsilon$ defined in a neighborhood of $p$ such that (1) $f(p)=f_\varepsilon(p)$ (2) $f\geq f_\varepsilon$ (3) $f_\varepsilon''(p)\geq -\varepsilon$ . Then if $f''\geq 0$ everywhere in the weak sense, then $f$ is convex. I have tried for a while to come up with a direction to move in from here. My ideas keep running into the problem that the definition of $f_\varepsilon$ only necessarily has nice properties in a small neighborhood of $p$ . This makes the naive approach of trying to directly apply the definition of convexity more difficult (compared to when we assume our function is actually twice differentiable). Given $x_1$ , $x_2$ and $t\in(0,1)$ , for example, we cannot guarantee the function $f_\varepsilon$ defined around $p=tx_1+(1-t)x_2$ is even defined at $x_1$ or $x_2$ . I have a feeling I am over-thinking this problem. I would appreciate any input/hints anyone would be willing to provide as to how I might proceed from here.","['riemannian-geometry', 'real-analysis']"
1096128,Elementary Applications of Cayley's Theorem in Group Theory,"The Cayley's theorem says that every group $G$ is a subgroup of some symmetric group. More precisely, if $G$ is a group of order $n$, then $G$ is a subgroup of $S_n$. In the course on group theory, this theorem is taught without applications. I came across one interesting application: If $|G|=2n$ where $n$ is odd, then $G$ contains a normal subgroup of order $n$. Q. What are the other elementary applications of Cayley's theorem in group theory, which can be explained to the undergraduates?","['big-list', 'finite-groups', 'group-theory', 'soft-question']"
1096138,Measure of intersection of three sets,"Suppose, $S_{1}$, $S_{2}$ & $S_{3}$ are measurable subsets of $[0,1]$, each of measure $\dfrac{3}{4}$ such that the measure of $S_{1}\cup S_{2}\cup S_{3}$ is $1$. Then the measure of $S_{1}\cap S_{2}\cap S_{3}$ lies in : (a) $\left[0,\dfrac{1}{16}\right]$ (b) $\left[\dfrac{1}{16},\dfrac{1}{8}\right]$ (c) $\left[\dfrac{1}{8},\dfrac{1}{4}\right]$ (d) $\left[\dfrac{1}{4},1\right]$. We know that, $$m\left(S_{1}\cup S_{2}\cup S_{3}\right)=m\left(S_{1}\right)+m\left(S_{2}\right)+m\left(S_{3}\right)-m\left(S_{1}\cap S_{2}\right)-m\left(S_{2}\cap S_{3}\right)-m\left(S_{1}\cap S_{3}\right)+m\left(S_{1}\cap S_{2}\cap S_{3}\right).$$ From this relation how we can determine the required interval?","['measure-theory', 'real-analysis']"
1096141,Parametrised vs Regular Surfaces,"Two types of surfaces in $\mathbb{R}^3$ are usually studied in introductory books on differential geometry: Parametrised or immersed surface :  Is an immersion $F:U\rightarrow\mathbb{R}^3$ from an open set $U\subset\mathbb{R}^2$ (the differential $\text{d}F(u):\mathbb{R}^2\rightarrow\mathbb{R}^3$ is injective on $U$).  The trace or image $S=F(U)\subset\mathbb{R}^3$ of the immersion is often called the parametrised surface. Parametrised surfaces are studied in Kuhnel Differential Geometry: Curves-Surfaces-Manifolds and in higher dimensions (parametrised hypersurfaces) in the literature on mean curvature flow. Remark :  Occasionally a parametrised surface is defined to be an injective immersion, in which case the trace $S=F(U)\subset\mathbb{R}^3$ becomes an immersed submanifold (itself a $2$-dimensional manifold for which the inclusion $\iota:S\hookrightarrow\mathbb{R}^3$ is an immersion). Regular or embedded Surface :  Is a subset $S\subset\mathbb{R}^3$ for which every point $p\in S$ has a local parametrisation, namely a smooth homeomorphism $\mathbf{x}:U\rightarrow S\cap V$, where $U\subset\mathbb{R}^2$ is an open set and $V\subset\mathbb{R}^3$ is an open neighbourhood of $p$, whose differential $\text{d}\mathbf{x}(u):\mathbb{R}^2\rightarrow\mathbb{R}^3$ is injective on $U$.  Regular surfaces are studied in do Carmo Differential Geometry of Curves and Surfaces . Remark :  A regular surface is just a $2$-dimensional regular submanifold $S\subset\mathbb{R}^3$, namely a $2$-dimensional manifold with the subspace topology for which the inclusion $\iota:S\hookrightarrow\mathbb{R}^3$ is an embedding (an immersion which is also a homeomorphism onto its image). We can show that regular surfaces are particular instances of parametrised surfaces : Regular surfaces are the image of an embedding (the inclusion map), and conversely the image of an embedding (between $\mathbb{R}^2$ and $\mathbb{R}^3$) is an embedded surface. Question 1:  What is the geometric intuition behind the difference between parametrised and regular surfaces? Question 2:  What are some examples of parametrised surfaces that are not regular surfaces? (Please prove your example is not a regular surface.)","['differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds', 'surfaces']"
1096164,Proving that a Hölder space is a Banach space,"I am trying to show that the Hölder space $C^{k,\gamma}(\bar{U})$ is a Banach space. To do this, I successfully proved that the mapping $\| \quad \| : C^{k,\gamma}(\bar{U}) \to [0,\infty)$ is a norm, by proving its properties. But how do I show that the sequence $\{u_k\}_{k=1}^\infty \subset C^{k,\gamma}(\bar{U})$ converges to $u \in C^{k,\gamma}(\bar{U})$, that is, how do I show that $$\lim_{k \to \infty}\|u_k-u\|=0,$$ which would mean the normed linear space is complete , and hence a Banach space? Here are the following taken from PDE Evans, 2nd edition, page 255: Definition. The Hölder space $$C^{k,\gamma}(\bar{U})$$ consists of all functions $u \in C^k(\bar{U})$ for which the norm $$\|u\|_{C^{k,y}(\bar{U})}:= \sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})}$$ is finite. Also from page 254, Definitions. (i) If $u : U \to \mathbb{R}$ is bounded and continuous, we write $$\|u\|_{C(\bar{U})}:=\sup_{x\in U}|u(x)|.$$ (ii) The $\gamma$th-Hölder seminorm of $u : U \to \mathbb{R}$ is $$[u]_{C^{0,\gamma}(\bar{U})}:=\sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|u(x)-u(y)|}{|x-y|^\gamma} \right\},$$ and the $\gamma$th-Hölder norm is $$\|u\|_{C^{0,\gamma}(\bar{U})}:=\|u\|_{C(\bar{U})}+[u]_{C^{0,\gamma}(\bar{U})}.$$ This is all I got so far:
\begin{align}
\|u_k-u\|_{C^{k,\gamma}(\bar{U})}&=\sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})} \\
&= \sum_{|\alpha|\le k} \sup_{x\in U} |u_k(x)-u(x)|+ \sum_{|\alpha|=k} \sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|[u_k(x)-u(x)]-[u_k(y)-u(y)]|}{|x-y|^\gamma} \right\}.
\end{align}
Now, where can I go from here, to show that $\lim_{k \to \infty}\|u_k-u\|_{C^{k,\gamma}(\bar{U})}=0$? The sequence is Cauchy, and I have to use that fact somehow.","['holder-spaces', 'functional-analysis', 'real-analysis', 'banach-spaces']"
1096166,Why were proofs avoiding complex analysis preferred in number theory? Is this distinction still important?,"I read on Wikipedia that an elementary proof in number theory means a proof which does not use complex analysis. From what I recall reading, in Hardy's time, proofs avoiding complex analysis were preferred. I would like to ask these two questions, with the stress on the second one: What were the reasons that proofs without complex analysis were preferred? Is this distinction still important today? This question arose from a discussion on meta about this post . I have seen this related post, but I'd say it is not the same question: Elementary proof of the Prime Number Theorem - Need? . That question is asking more about whether finding an elementary proof had some important consequences for using similar method in other areas. (But Matt E's answer posted there also deals with the motivation for the search for an elementary proof. So it can also be considered an answer to the first bullet above.) This MathOverflow post is also interesting in this context: Complex and Elementary Proofs in Number Theory . It discusses where number-theoretic results based on complex analysis can also be shown without using complex-analytic methods.",['number-theory']
1096194,Confusion to use formula $l = r\theta$,"I have been teaching my brother some trignometry. There is a formula as arc length of circumference of a circle.  The basic formula is $$l = r\theta.$$
But sometimes for length they use $l = 2r$ and other times $l = 2\pi r$. I want to know when to use $l = 2r$ and when to use $l = 2\pi r$.",['trigonometry']
1096242,Need help solving this differential equation,"$$p^3 - 2xyp + 4y^2 = 0$$
where $p = \mathrm dy / \mathrm dx$. I don't know which type of equation it is or how to simplify it.
Though there is an observation that
$$(xy^2)′=y^2+2xyy′$$","['differential', 'ordinary-differential-equations']"
1096253,Problems with the proof that $\ell^p$ is complete,"By struggling with the proof that $\ell^p$ is complete, I looked up different proofs by different authors, and I ended up focusing on the one given by Kreyszig in his classic book on functional analysis, because I found it the most perspicuous for my level. Still, there are some point that are not completely clear, thus I will write down here the whole proof and I will add my remarks and doubts. [I] Theorem: The space $\ell^p$ is complete; here $p$ is fixed and $1 \leq p < \infty$ . Proof: Let $(x_n)$ be any Cauchy sequence in $\ell^p$ , where $x_m = ( \xi^{(m)}_1, \xi^{(m)}_2, \dots  )$ . Then, for every $\varepsilon > 0 $ there is an $N$ such that for all $m, n >N$ , \begin{equation} d ( x_m, x_n ) = \Bigg( \sum_{j=1}^\infty |\xi^{(m)}_j - \xi^{(n)}_j |^p
 \Bigg)^{\frac{1}{p}} < \varepsilon \hspace{2cm} \text{(1)} .
 \end{equation} It follows that for every $j = 1,2, \dots$ we have for $m,n >N$ $$ |\xi^{(m)}_j - \xi^{(n)}_j | < \varepsilon \hspace{2cm}
 \text{(2)}. $$ We choose a fixed $j$ . From (2) we see that $(
 \xi^{(1)}_j, \xi^{(2)}_j, \dots )$ is a Cauchy sequence of numbers. It
converges since $\Re$ is complete, say $\xi^{(m)}_j \to \xi_j$ as $m
 \to \infty$ . Using these limits, we define $x = (\xi_1, \xi_2, \dots)$ and show that $x \in \ell^p$ and $x_m \to x$ . This looks fine, [II] From (1) we have for all $m,n >N$ $$ \sum_{j=1}^k |\xi^{(m)}_j -
 \xi^{(n)}_j |^p  < \varepsilon^p \hspace{2cm} (k=1,2,\dots). $$ First problems! I see where the $\varepsilon^p$ comes from, but what puzzles me is the $k$ on the top of $\sum$ . I see it comes from (1) (indeed, if it works for $j \to \infty$ , then it has to work necessarily for a finite $k$ . Still, I don't see why we actually need to do it . Why don't we simply stick to (1)? [III] Letting $n \to \infty$ , we obtain for $m>N$ $$ \sum_{j=1}^k
 |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm}
 (k=1,2,\dots). $$ Same problem as before, plus the mysterious $\leq$ between LHS and RHS. Indeed, I think the idea should be that the sequence converge and it is Cauchy, thus the $\varepsilon > 0$ is the same that we use in the definition of Cauchy sequence, and in the standard limit definition. But then it should be still $<$ and not $\leq$ . [IV] We may now let $k \to \infty$ ; then for $m >N$ $$
\sum_{j=1}^\infty |\xi^{(m)}_j - \xi_j |^p  \leq \varepsilon^p \hspace{2cm} \text{(3)}.
$$ Bypassing the previous related problems, this is fine. [V] This shows that $x_m - x = ( \xi^{(m)}_j - \xi_j ) \in \ell^p$ . Not completely sure I see why it is actually the case. [VI] Since $x_m \in \ell^p$ , it follows by means of the Minkowski inequality, that $$
x = x_m + (x - x_m) \in \ell^p.
$$ The reference to the Minkowski inequality is really mysterious. [VII] Furthermore, the series in (3) represents $[d(x_m,x_)]^p$ , so that (3) implies that $x_m \to x$ . Since $(x_m)$ was an arbitrary Cauchy sequence in $\ell^p$ , this proves the completeness of $\ell^p$ , where $1 \leq p < \infty$ . QED Sorry for the vivisection of this rather straightforward proof, but I have the feeling that by properly catching each step here, I could improve dramatically my overall understanding of real analysis tools and procedures. Thank you for your time and patience.
As always, any feedback is most welcome! Edit for BOUNTY: I am editing this question because I am putting a bounty. True enough, I received very helpful comments, but still they were only comments, and I would really  love to have an answer, because I do feel a lot of the things that look here mysterious have to be fairly important. I also have the feeling that some of the problems I showed in these questions, e.g. the one that is below under (1), can give the feeling that it is impossible I can actually try to read something sort of advanced, without having a solid grasps of other things, and maybe this could put a potential answerer in the akward position of feeling “Come on, man, are you kidding me? Don’t let me waste my time. I cannot go back to teach you 2+2!”. However, that’s how things are, and this is mostly due to the fact that I am self-taught, which put me in the position to choose my own topics. But, actually, exactly those very naive questions are the ones that would set me on the right path to keep on studying properly. Hence, I renumbered the parts in which I divided the proof of the theorem in order to easily refer the questions to each part. 1) Is the change from $\infty$ to $k$ in [II] related to the fact that we are dealing with a series, and thus we standardly see how a series behave with its finite terms, before letting the limit goes to $\infty$ (which is what happen in [III] ? 2) Is in step [V] implicitly assumed that it is the case due to the fact that it is for all $j$ ? 3) How does step [VI] come from the Minkowski inequality? Of course, any other addition or explanation is most welcome. Thanks for your time!","['lp-spaces', 'self-learning', 'functional-analysis', 'real-analysis']"
1096255,"Example: operator injective, then the adjoint is NOT surjective","Let $T: V \rightarrow W$ be a bounded operator on normed spaces $V,W$. Now, there is a unique adjoint operator $T': W' \rightarrow V'$ defined by $T'(\alpha) = \alpha \circ T$. In finite dimensional spaces, we have that if $T$ is injective, then $T'$ is surjective. From basic Hilbert space theory, I suspect that the Banach space adjoint must not be surjective in general. Probably, $T'$ has only dense range. Since we don't know that the range of $T'$ is closed( which is trivial in the finite-dimensional case), $T'$ does not have to be onto. Does anybody know an example of this situation, where $T$ is injective, but $T'$ not surjective? If anything is unclear, please let me know.","['hilbert-spaces', 'functional-analysis', 'real-analysis', 'banach-spaces']"
1096266,Chances of someone being of a certain gender at websites,I have 2 of websites and I know the chances of a visitor being a female or male. Let's say I have 2 website where the chance of a new visitor being a female is 80%. If the visitor comes on website 1 I know the chance of that visitor being female is 80%. But what if that visitor comes on both websites. Is the chance still 80%? Or am I more certain that visitor is a female? If so what is the equation I should use? The website are not dependent of each other.,['statistics']
1096268,Show three ways that $f(z)=\frac{\overline{z}}{z-1}$ is not analytic,"I need to show the complex function $$f(z)=\frac{\overline{z}}{z-1}$$ is not analytic in three ways; using Cauchy's equations, geometrically, and by integrating over the circle of radius 2. I used Cauchy's equations using $u(x,y)=\frac{x^2-x-y^2}{(x-1)^2+y}$ and $v(x,y)=\frac{y-2xy}{(x-1)^2+y}$.The calculations were complicated, but I managed to show they were not equal. For the second part, I chose the lines $x=1$ and $y=1$, mapped them, and showed that the slopes of their tangents at their intersection points did not meet at a right angle. I believe this answers the question. My biggest question lies in the integral. I did it, but got 0. Here is what I did:
We want $$\oint_C \frac{\overline{z}}{z-1}$$, where $C$ is given by $|z|=2$. I chose to parameterize $z$, giving $z=2e^{it}$ for $0\leq t\leq2\pi$. This gives $\overline{z}=2e^{-it}$ and $dz=2ie^{it}dt$, and so I integrated the following: $$\int_0^{2\pi} \frac{2e^{-it}}{2e^{it}-1}2ie^{it}dt.$$ Unfortunately, upon integration, I got that the integral is equal to $0$. I didn't expect this, as the integral should vanish if it is analytic, but it clearly is not at $z=1$. Did I make a mistake? Thanks!","['parametric', 'integration', 'complex-analysis']"
1096298,Question about sum of chi-squared distribution,"I want to prove that the sum of two independent chi-squared random variables is a chi-squared random variable. I am supposed to only use the fact that if $Q$ has a chi-squared distribution with parameter k then Q = $Z_1^2$ + $Z_2^2$ + ... + $Z_k^2$ where each $Z_i$ is a standard normally distributed random variable and {$Z_1$,...,$Z_k$} is independent. My attempt at a proof: Let $Q_1$ and $Q_2$ be independent random variables with chi-squared distributions, with parameters a and b, respectively.
Let {$X_1$,...,$X_a$,$Y_1$,...,$Y_b$} be a set of independent random variables with standard normal distributions. Then we can write $Q_1$ = $X_1^2$ + $X_2^2$ + ... + $X_a^2$ $Q_2$ = $Y_1^2$ + $Y_2^2$ + ... + $Y_b^2$  , and $Q_1$ and $Q_2$ are independent because {$X_1$,...,$X_a$,$Y_1$,...,$Y_b$} is independent. so $Q_2$ + $Q_2$ = $X_1^2$ + $X_2^2$ + ... + $X_a^2$ + $Y_1^2$ + $Y_2^2$ + ... + $Y_b^2$. Since {$X_1$,...,$X_a$,$Y_1$,...,$Y_b$} is independent, $Q_2$ + $Q_2$ is a chi-squared random variable with parameter a+b. I don't think my proof is correct. I think the problem is that if we are given $Q_1$ and $Q_2$ that are independent, we can't just write them in terms of {$X_1$,...,$X_a$,$Y_1$,...,$Y_b$}. But I am not really sure. Please tell me why my proof is incorrect (or maybe it is correct). Any help is appreciated.","['statistics', 'probability-distributions']"
1096313,Is the trace set of orthogonal matrix compact?,Show that $$T = \{ \mbox{tr} (A) : A \in O_n (\mathbb{R}) \}$$ is compact. I tried to show this set is compact. I could not. Any hint would suffice. Thanks in advance.,"['trace', 'matrices', 'linear-algebra', 'orthogonal-matrices', 'compactness']"
