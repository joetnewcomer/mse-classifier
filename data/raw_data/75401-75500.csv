question_id,title,body,tags
939153,On definition of gamma function.,"We all know that gamma function's definition is
$$\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds}$$
and it is divergent for
$x<0$. Yesterday, I was studying about Bessel function and i came up with a dilemma. In Bessel function for negative numbers, i.e. $J_{-\alpha}$ they use a term like $\Gamma(m-\alpha+1)$ and when $m<\alpha-1$ series is defined although gamma function is undefined.
Also in class there were questions like finding $\Gamma\left(-\frac{1}{2}\right)$ where it is a definite value? $$\Gamma\left(-\frac{1}{2}\right)=-2\sqrt{\pi}$$
but how is it well-defined, isn't it diverging? And how is it a negative number? Summarizing, how exactly is gamma function defined?","['gamma-function', 'ordinary-differential-equations', 'special-functions', 'bessel-functions']"
939154,Acyclic but not flasque sheaf of abelian group?,"Is there a sheaf of abelian groups which is acyclic but not flasque? Maybe we can try $0\to \mathcal{F'}\to \mathcal{F}\to \mathcal{F''}\to 0$ where $\mathcal{F',F''}$ are flasque but $\mathcal{F}$ is not.","['sheaf-theory', 'algebraic-geometry']"
939171,"Find $B$ if $AB=BC$ and $A,C$ are invertible","Suppose $A$ and $C$ are known invertible complex matrices of possibly different orders. If $B$ is an unknown matrix of appropriate order such that $AB = BC$, then how could one solve for $B$?","['matrices', 'linear-algebra']"
939198,Conditional probability plane problem,"I was presented with this problem and am not sure where to take it. A plane is missing and is presume to have equal probability of going down in any of 3 regions. If a plane is actually down in region $i$, let $1-\alpha$ denote the probability that the plane will be found upon a search of the $ith$ region, $i=1,2,3.$ What is the conditional probability that the plane is in: A) region 1, given the search of region 1 is unsuccessful B) region 2, given the search of region 1 is unsuccessful C) region 3, given the search of region 1 is unsuccessful My idea: So we know $P(1)=P(2)=P(3) = \frac{1}{3}$. (Probability it is that region) We also know that $P(F_1\mid 1)=1-\alpha_1$, and similarly for the other regions. Then for A) want $P(1\mid F_1^c)$ $$P(1\mid F_1^c) = \frac{P(F_1^c\mid 1)P(1)}{P(F_1^c\mid 1)P(1)+P(F_2^c\mid 1)P(2)+P(F_3^c\mid 1)P(3)} = 
\frac{\alpha_1}{\alpha_1+\alpha_2+\alpha_3}$$ However I feel this is wrong but cannot find why. For B) and C) I am at a loss. I know we want $P(2|F_1^c)$ and $P(2\mid F_1^c)$ but I can't see how we could use the same formula to get that. Any help would be much appreciated.","['solution-verification', 'probability', 'conditional-probability']"
939227,Correctness of proof that an ordered field S that has the supremum property also has the infimum property,"First question I have is how would you describe the relationship between an ordered field and an ordered set and continue the proof by treating the field as a set?  I want to say that right in the beginning to make things clear but I don't know how to say it. Here's the rest of the proof: Let A be a nonempty subset of S s.t.  it is bounded above and D be the set of all lower bounds for A. Fix a to be elements of A.  Then for each d in subset D d is a lower bound for A so d $\leq$ a  for all a in A. Since d is bounded above by A it has the supremum $\gamma$ .  Since every a is an upper bound for d, $\gamma \leq a$ for each element a in subset A.  So $\gamma$ is a lower bound for A and is clearly an infimum since it is an upper bound for the set of lower bounds.","['proof-verification', 'real-analysis', 'analysis']"
939235,Proof that the set of irrational numbers is dense in the reals,"The hint I was given was to simply prove that $y=xz$ is irrational given that $x$ is nonzero, $x$ is rational and $z$ is irrational.  Here's how I did it: Claim: $y=xz$ is irrational. Proof: Assume $x\neq0$, $x$ is rational and $z$ is irrational. By contradiction  assume that $y=xz$ is rational.  This means $y$ can be expressed as $m/n$, $m$ and $n$ being integers; $y$ can be expressed similarly as $p/q$, $p$ and $q$ being integers.  By substitution, we have that $$ p/q=mz/n$$
and $$z=pn/qm, qm \neq 0.$$
Since $pn$ and $qm$ are integers $z$ has to be rational. In addition to this it seems like there's a part 2 as follows: Proof: Given an interval $(x,y)$ we will choose a positive irrational number, $z$, say.  By density of the rationals there is a rational $p$ in the interval $(x/z, y/z)$ s.t. $$ x/z <p< y/z.$$  From this we see that $pz$ is irrational since it is the product of a rational and irrational number. Is the $pz$ the $xz$ that we proved is irrational in the first proof?  So ideally when presenting a full proof like this, should we do part 2 then part 1?","['proof-verification', 'real-analysis', 'analysis']"
939264,Coincidence? : $d(ax^2+bx+c)/dx=\pm \sqrt{\Delta}$,"As the title says, is it just a coincidence that $d(ax^2+bx+c)/dx=\pm \sqrt{\Delta}$? (where $\Delta=b^2-4ac$, i.e. discriminant of the quadratic). We can get this easily from rearranging the quadratic formula: $$x=\frac{-b\pm \sqrt{b^2-4ac}}{2a}$$ $$\iff 2ax+b=\pm \sqrt{b^2-4ac}$$ $$\iff \frac{d(ax^2+bx+c)}{dx}=\pm \sqrt{\Delta}$$ but that doesn't explain why it's true, i.e. why the derivative of a quadratic equals $\pm$ square root of $\Delta$. Seems  a touch mysterious. EDIT: I found this in a book entitled ""Vedic Mathematics"" by Bharati.","['quadratics', 'roots', 'derivatives', 'polynomials']"
939275,A space curve is planar if and only if its torsion is everywhere 0,"Can someone please explain this proof to me. I know that a circle is planar and has nonzero constant curvature, so this must be an exception, but I am a little lost on the proof. 
Thanks!","['planar-graphs', 'differential-geometry']"
939279,Proof about Inclusion-exclusion formula?,The problem requires to use the indicate function to prove the Inclusion-exclusion formula. But I really don't know what to do. Anyone can help with that? Thanks!,['probability-theory']
939320,Probability of 1 billion monkeys typing a sentence if they type for 10 billion years,"Suppose a billion monkeys type on word processors at a rate of 10 symbols per second. Assume that the word processors produce 27 symbols, namely, 26 letters of the English alphabet and a space. These monkeys type for 10 billion years. What is the probability that they can type the first sentence of Lincoln’s “Gettysburg Address”? Four score and seven years ago our fathers brought forth on this continent a new nation conceived in liberty and dedicated to the proposition that all men are created equal. Hint: Look up Boole’s inequality to provide an upper bound for the probability! This is a homework question. I just want some pointers how to move forward from what I have done so far. Below I will explain my research so far. First I calculated the probability of the monkey 1 typing the sentence ( this question helped me do that); let's say that probability is $p$ : $$P(\text{Monkey 1 types our sentence}) = P(M_1)= p$$ Now let's say that the monkeys are labeled $M_1$ to $M_{10^9}$ , so given the hint in the question I calculated the upper bound for the probabilities of union of all $P(M_i)$ (the probability that i-th monkey types the sentence) using Boole's inequality. Since $P(M_i)=P(M_1)=p$ , $$ P\left(\bigcup_i M_i\right) \le \sum_{i=1}^{10^9}P(M_i)=\sum^{10^9} p=10^9 \,p$$ Am I correct till this point? If yes, what can I do more in this question? I tried to study Bonferroni inequality for lower bounds but was unsuccessful to obtain a logical step. If not, how to approach the problem? As I am new, edit suggestions will also be appreciated.","['probability-theory', 'probability']"
939324,Approximation of a rational number,"I have been asked the following problem:
Every real number $x$ can be written as a sum of the form 
$$
\sum_{i=1}^{\infty} \frac{a_n}{n^2},
$$
where $a_1\in \mathbb{Z}$, $a_2=0,1,2,3$, and $0\leq a_n<n$ are integers for $n\geq 3$. (*)If  $x$ is a rational number, then the expression can be of finitely many terms. I don't know how to prove it. I have two expressions for $1/3$: $$\frac{1}{3}=\frac{1}{2^2}+\frac{3}{6^2},$$
and 
$$\frac{1}{3}=\frac{1}{2^2}+\frac{1}{4^2}+\sum_{n=1}^{\infty}\frac{1}{49^n}.$$ I have no idea how to distinct them. Could anyone provide me a proof/disproof of (*)? Thank you every much!",['number-theory']
939405,A stimulating book about algebra,"I need to fill some gaps in my algebra knowledge. The problem is: While I do realise the importance and utility of the subject, I do not find it appealing.
  Is there any book around which shows you why the subject is beautiful and stimulate curiosity? To list some books which I think do that in other fields: Gravitation by Misner, Thorne and Wheeler (not just for the GR part but also for the differential geometry bit), the shape of space by Week for topology, probably visual complex analysis by Needham, differential topology by Guillemin and Pollack. For some more details:
I am trying to learn the amount of algebra usually known by a math graduate student who is not planning to work in abstract algebra. I have been trained as a theoretical physicist so I received fully rigorous courses in linear algebra and finite group theory (taught by mathematicians) but pretty much nothing else (of course I have had a nodding acquaintance with several other concepts). The need for some more serious grounding in algebra first arose when I was trying to apply some algebraic topology tools. Therefore I am thinking of learning first about modules and rings (leaving aside a more serious study of fields for the moment), then learn a bit of the categorical viewpoint and see the various structures in action by learning a bit of homological/commutative algebra. I was thinking to get the basic on Lang undergraduate algebra, and then move on to the first few chapters of Hilton-Stammbach possibly complemented by Lang grad book and Atiyah-MacDonald. I will be happy to hear feedback on my study plan as well, but the main point here is if you can find me a book which will make algebra fascinating!","['book-recommendation', 'reference-request', 'abstract-algebra']"
939423,"Rational Points, classical versus modern notion","In classical algebraic geometry, a $\mathbb Q$-rational point on a, say, complex affine variety $V\subseteq\mathbb C^n$ is a point $p=(p_1,\ldots,p_n)$ with $\forall i: p_i\in\mathbb Q$. Now, in modern language, a $\mathbb Q$-rational point is a morphism $\operatorname{Spec}(\mathbb Q)\to V$. Clearly, if $V$ is defined as $\operatorname{Spec}(\mathbb C[X_1,\ldots,X_n]/I)$, then $V$ has no $\mathbb Q$-rational points in this language. Of course, if $I$ is generated by polynomials with coefficients in $\mathbb Q$, we could look at  $V_{\mathbb Q}=\operatorname{Spec}(\mathbb Q[X_1,\ldots,X_n]/I)$ so $V=V_{\mathbb Q}\times_{\operatorname{Spec}(\mathbb Q)}\operatorname{Spec}(\mathbb C)$ and the classically $\mathbb Q$-rational points are the $\mathbb Q$-rational points of $V_{\mathbb Q}$. However, the object $V_{\mathbb Q}$ now fails to capture all classically $\mathbb C$-rational points. For instance, the scheme $V_{\mathbb Q}=\operatorname{Spec}(\mathbb Q[x]/\langle x(x^2+1)\rangle)$ contains two points, one of which is $\mathbb Q$-rational. However, base-changing it to $\mathbb C$, we get $3$ points, none of which is $\mathbb Q$-rational. This is somewhat unsatisfactory to me. The classical definition seems so straightforward and clean, and I have a single object $V$ containing all my $\mathbb C$-rational points while it is still possible to easily identify the $\mathbb Q$-rational points. Unfortunately, the usual textbooks don't really elaborate on the merits of modern language with regard to this particular aspect. I would be very grateful if someone could do so. I guess my question should be phrased as follows: Given a scheme $V$ defined over $\mathbb C$, what is the correct way, in modern language, to identify what I would classically consider to be the $\mathbb Q$-rational points in $V$?",['algebraic-geometry']
939437,How is $r(\theta) = \sin \frac\theta2$ symmetric about the x-axis?,I understand how it is symmetric about the $y$-axis. because $r(-\theta) = \sin \left(-\frac\theta2\right)=-\sin \left(\frac\theta2\right)=-r(\theta)$ But how is it symmetric about $x$-axis?,"['trigonometry', 'polar-coordinates', 'symmetry']"
939451,"Proof: For all integers $x$ and $y$, if $x^2+ y^2= 0$ then $x =0$ and $y =0$","I need help proving the following statement: For all integers $x$ and $y$, if $x^2+ y^2= 0$ then $x =0$ and $y =0$ The statement is true, I just need to know the thought process, or a lead in the right direction. I think I might have to use a contradiction, but I don't know where to begin. Any help would be much appreciated.",['discrete-mathematics']
939468,Determinant of the sum of an identity matrix and a rank-two-symmetric matrix,"Suppose $I$ is an $n \times n$ identity matrix, and $S$ is the $n \times n$ symmetric matrix with rank equals two.  I was reading something saying that:
$$\det(I-S)=(1-\lambda_1)(1-\lambda_2)$$ where $\lambda_1$ and $\lambda_2$ are the two largest (in absolute values) eigenvalues of $S$.  Can anyone provide some clues for proving this?  Thanks in advance!","['matrix-equations', 'matrices', 'block-matrices', 'determinant']"
939493,Isomorphism group between $\mathbb R$ and one of it's propre subgroups.,"Is there a  subgroup $H$ of $(\mathbb R,+)$ such that $H \neq  \mathbb R$ and $H \simeq \mathbb R$ as groups?",['group-theory']
939562,Is is true that if $E|X_n - X| \to 0$ then $E[X_n] \to E[X] $?,"My question is motivated by the following problem: Show that if $|X_n - X| \le Y_n$ and $E[Y_n] \to 0$ then $E[X_n] \to E[X]$. I started off by saying that since
$$|X_n - X|\ge 0 $$
then $$E[|X_n - X|]\ge 0 $$ At the same time 
$$E[|X_n - X|]\le E[Y_n] $$ and so $$0 \le E[|X_n - X|] \le E[Y_n]$$ By the squeeze theorem then $E[|X_n - X|] \to 0$. I don't know how to proceed from here. I know that if $E[|X_n - X|] = 0$ then I  can set up a contradiction, like so: Suppose that $|X_n - X| = c$, $c \ne 0$ and $E[|X_n - X|] = 0$. Then $$E[|X_n - X|] = E[c] = c \ne 0. $$ But I don't know how to show this would hold in the limit. I am planning to use this to show that $X_n \to X$ and therefore $E[X_n] \to E[X]$","['measure-theory', 'probability']"
939599,"Is the ""product rule"" for the boundary of a Cartesian product of closed sets an accident?","Given two closed sets $A$ and $B$ living in topological spaces $X$ and $Y$, the boundary of $A\times B$ in the product topology, denoted (suggestively) by $\partial(A\times B)$, is given by $$\partial(A\times B) = ((\partial A)\times B)\cup (A\times(\partial B)).$$ For three closed sets, we have $$\partial(A\times B\times C) = ((\partial A)\times B\times C)\cup (A\times(\partial B)\times C) \cup (A\times B\times(\partial C))$$ and so on. This is extremely similar to the product rule for derivatives, with $\times$ replaced by $\cdot$, $\partial$ replaced with the derivative and $\cup$ replaced with $+$. This seems way too similar to be a happy coincidence but my imagination can't seem to easily make a connection between derivatives and boundaries at an fundamental level. There are quite obvious connections between the two at the level of smooth manifolds but I'm not sure if that is a very satisfying connection since it seems to put the cart before the horse. I'm aware of such fields as differential algebra which study algebraic structures endowed with objects which obey something resembling the Leibniz law and this seems to be along the lines of this phenomenon. Is this truly a coincidence or should there be deep reasons for why the product rule underlies both derivatives and boundaries of (closed) sets?","['general-topology', 'soft-question']"
939613,Almost surely equality,"suppose that X = Y almost surely.i.e. P(X=Y)=1. Then how can one show that the events $X^{-1}(M)$and  $Y^{-1}(M)$ are equal almost
surely for each Borel set M ∈ B.","['probability-theory', 'measure-theory']"
939702,Limits without L'Hopitals Rule,Evaluate the limit without using L'hopital's rule a)$$\lim_{x \to 0} \frac {(1+2x)^{1/3}-1}{x} $$ I got the answer as $l=\frac 23$... but I used L'hopitals rule for that... How can I do it another way? b)$$\lim_{x \to 5^-} \frac {e^x}{(x-5)^3}$$ $l=-\infty$ c)$$\lim_{x \to \frac {\pi} 2} \frac{\sin x}{\cos^2x} - \tan^2 x$$ I don't know how to work with this at all So basically I was able to find most of the limits through L'Hopitals Rule... BUT how do I find the limits without using his rule?,"['trigonometry', 'calculus', 'limits-without-lhopital', 'limits']"
939710,A collection of pairwise disjoint open intervals must be countable,"Let $U$ be a collection of pairwise disjoint open intervals. That is, members of $U$ are open intervals in $\mathbb{R}$ and any two distinct members of $U$ are disjoint. Show that $U$ is countable. My attempted proof: Let $U \subseteq \mathbb{R}$ be a collection of pairwise disjoint opern intervals. Let $(a,b) \subseteq U$ be one such interval. Since $\mathbb{Q}$ is dense in $\mathbb{R}$, there exists a rational number $q_{a,b} \in (a,b)$. Define $f: U \to \mathbb{Q}$ by $f((a,b))=q_{a,b}$. As $f(a_1,b_1)=f(a_2,b_2)$, or $q_{a_1,b_1}=q_{a_2,b_2}$, implies $(a_1,b_1)=(a_2,b_2)$, we deduce that $f$ is injective. It was proved already in my lecture class that $\mathbb{Q}$ is countable, and that union of countable sets is countable. Thus, we deduce that $U$ is countable. (End of attempted proof) I feel confident about my proof except the injectivity part. I was looking for help there, assuming the rest of my proof is cogent. If not, please let me know how I can improve on it.","['proof-verification', 'elementary-set-theory', 'real-analysis']"
939807,Does There Exist an Explicit Formula Describing Every Possible Sequence of Numbers?,"This thread was previously titled ""Does a Set Require an Explicit Formula to Exist?"". I'm reading H. Enderton's Elements of Set Theory and working through understanding the Zermelo-Fraenkel axioms. While reading, the following question regarding the nature of sets occurred to me. Suppose I give you an infinite set of integers, which seems to have no definable pattern. For instance, suppose I start listing elements of the set:
\begin{equation*}
S=\{3,4,11,199,205,6090,11238,...\}
\end{equation*}
I have simply chosen numbers at complete random and ordered them. As far as I know, there is no formula that describes those numbers. Suppose this set continues forever. According to set theory, does this set exist? It seems hard to say it doesn't exist -- after all, I've begun to write it already, so how could it not exist? However, I believe set theory requires sets to have an explicit formula. Then the question becomes: given any arbitrary sequence of numbers like the one I've listed above (with no apparent ""pattern""), does there exist a formula describing the sequence? I realize this is a somewhat philosophical question, and it's coming from a new reader of set theory. However, this seems to be the kind of question that set theory was invented to answer. Thank you for your thoughts and explanations! Edit : Thanks to Yuval for his answer! His answer raised some thoughts which I wanted to add to the question. Given an arbitrary set of integers, although I can't find a formula describing it, since the set of ALL integers exists, then by the power set axiom any subset of it should exist as well. So, could it just be that I'm just not clever enough to formulate an explicit formula for my arbitrary set of integers, but such a formula (as complicated as it may be!) actually exists? Maybe the existence of a formula describing any conceivable sequence of numbers is taken as an axiom, or maybe the opposite is true and it can be proven there exists a sequence without a formula. I'm also wondering, although I know very little about it -- does this question have anything to do with the Axiom of Choice? Thanks again for your thoughts and explanations!","['set-theory', 'sequences-and-series']"
939827,Euler triangle inequality proof without words,"Today I was studying geometric inequalities and I saw this inequality: $$R \ge 2r$$ Unfortunately, the book did not provide any proof or further explanations. So I just did a little research about it. I found that the name of inequality is euler triangle inequality and there was a simple proof about it. If $O$ is circumcenter of $\triangle ABC$ and $I$ is incenter of it and $OI=d$ then $$d^2=R(R-2r)$$ $R$ is circumradius and $r$ is inradius. from this identity we can drive $R-2r\ge0 \Rightarrow R\ge2r$ but I did not like this proof. So I continued my research till I found this from MAA . First, the author proves 3rd lemma and using them simply proves the inequality. My problem was on understanding second lemma. Here is a description about it: The second, whose proof uses a rectangle composed of triangles similar to the right triangles in FIGURE 1(b), expresses the product $xyz$ in terms of the inradius $r$ and the sum $x + y + z$ . Figure 1(b) And the lemma proof LEMMA 2. $xyz = r^2(x + y + z)$ . Proof. Letting $w$ denote $\sqrt{r^2+x^2}$ , we have I can't understand what sides are given in figure 3 and I can't reach $r^2(x+y)$ (I tried using Pythagorean but I was not succesful).","['geometry', 'inequality', 'euclidean-geometry']"
939841,Prove sequence with fibonacci recurrence bounded by $(7/4)^n$,"Posting even though correct just for feedback, etc. $n_0,n_1$ are lower/upper bounds of true values for strong induction.  Guess I could have used different values, like 2 and 3, or 1 and 2, but it is ""arbitrary"" so long as the entire range in question is covered. Sequence $a_1,a_2,a_3,...,a_n$ defined by 
$a_1=1,a_2=2;a_n=a_{n-1}+a_{n-2},n\ge 3$.
Prove for all $n\ge 1, a_n<(\frac{7}{4})^n:S(n)=a_n<(\frac{7}{4})^n$
$n_0=3,n_1=4;S(1),S(2),S(3)/S(n_0)/S(n-1),S(4)/S(n_1)/S(n)$(are true);
$a_{n+1}=a_n+a_{n-1}<(\frac{7}{4})^n+a_{n-1}<(\frac{7}{4})^n+(\frac{7}{4})^{n-1}=(\frac{7}{4})((\frac{7}{4})^{n-1}+(\frac{7}{4})^{n-2})$
$=(\frac{7}{4})(\frac{7}{4})^n((\frac{7}{4})^{-1}+(\frac{7}{4})^{-2})=(\frac{7}{4})^{n+1}((\frac{7}{4})^{-1}+(\frac{7}{4})^{-2})<(\frac{7}{4})^{n+1}$","['induction', 'discrete-mathematics', 'proof-verification']"
939852,The limit of reciprocal is the reciprocal of the limit,"While I was reading the book Exploratory examples for real analysis , I came across this: to show that : $$\lim_{x\rightarrow x_o}\frac{1}{h(x)} = \frac{1}{\lim_{x\rightarrow x_o}{h(x)}} = \frac{1}{M}$$ Discussion:Let $\epsilon \gt 0$.we want to find $\delta \gt 0$ such that if  $0 \lt |x-x_0| \lt \delta $,then $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|\lt \epsilon$. In order to get expression involving $|h(x)-M|$,we first find the common denominator:
       $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|=\Big|\frac{M-h(x)}{M~h(x)}\Big|$
     $~=\Big|\frac{h(x)-M}{|M|~|h(x)|}\Big|$. As we have to deal with variable term $|h(x)|$ in denominator.We can construct bound .Let $\epsilon =\frac{|M|}{2}$ ,since lim$_{x\to x_o}h(x)=M$,there exists $\delta_1\gt 0~~$ s.t. if $0 \lt |x-x_0| \lt \delta_1 $,then $|h(x)-{M}|\lt \frac{|M|}{2}$. As you study steps below,identify that step where $h(x)\neq 0$ for all $0 \lt |x-x_0| \lt \delta_1 $ plays a critical role: $|M|=|M-h(x)+h(x)|$ $~~~~~~\leq |M-h(x)|+|h(x)|$ $~~~~~~= |h(x)-M|+|h(x)|$ $~~~~~~\lt \frac{M}{2}+|h(x)| \implies$ $\frac{|M|}{2} \lt |h(x)| \implies$ $\frac{1}{h(x)}\lt \frac{2}{|M|}$,where $0 \lt |x-x_0| \lt \delta_1 $. Now that we've bound for $1/h(x)$,we can apply the definition to term $|h(x)-M|$. Since 
   lim$_{x\rightarrow x_o}h(x)=M$,we can find $\delta_2 \gt 0$ such that, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~$if  $0 \lt |x-x_0| \lt \delta_2$, then $|h(x)-M|\lt \frac{|M|^2 \epsilon }{2}$ The thing I can't understand is for what purpose did the author work with such complicated expression involving $\epsilon $ in the end of the discussion?","['calculus', 'proof-explanation', 'limits']"
939869,generalization of midpoint-convex,"Let f : (a,b) → R is a midpoint-convex function (I didn't say continuity). Here I'd like to verify following inequality """"directly"""". f( (x1+x2+x3)/3 ) ≤ (f(x1)+f(x2)+f(x3))/3 .. I can easily demonstrate for n=2^k BUT not case for n=2^k, HOW can I demonstrate this inequallity? Thanks for your consideration.","['functions', 'inequality', 'real-analysis']"
939902,Alternate proof for $a^2+b^2+c^2\le 9R^2$,"As I studying geometric inequalities, one of those famous inequalities is $$a^2+b^2+c^2\le 9R^2$$ I did some research and I found that there is a proof (not exactly the this inequality but an useful identity) of this on geometry revisited book section 1.7. the identity is $$OH^2=9R^2-(a^2+b^2+c^2)$$ where $H$ is orthocenter and $O$ is circumcenter. the proof of this identity uses Stewart's theorem, Euler line and ... . I find the proof not very nice and a little bit brute force. I want to know is there any different proof for it? and what is the name of this inequality?","['geometry', 'inequality', 'geometric-inequalities', 'triangles', 'alternative-proof']"
939937,"Evaluating $\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx$","What tools would you recommend me for evaluating this integral? $$\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx$$ My first thought was to use the beta function, but it's hard to get such a form because of $\cos(2x)$. What other options do I have?","['improper-integrals', 'calculus', 'integration', 'definite-integrals', 'real-analysis']"
939953,Elements of $GL_{2}(\mathbb{Z})$ of finite order,"Prove that any element of $GL_{2}(\mathbb{Z})$ of finite order has order $1,2,3,4,6$ using Field Theory. My idea is to reduce such a finite order matrix say $A$ with order $n$ to modulo a prime $p$ . $\det A=\pm1$ so $A$ will land inside $G=\{M\in GL_{2}(\mathbb{F}_{p}):\det M=\pm 1\}$ . $|G|=2p(p^{2}-1)$ . So $n\mid 2p(p^{2}-1)$ for all prime $p$ . I don't know how to proceed after this.","['matrices', 'linear-groups', 'abstract-algebra', 'field-theory']"
939958,Do there exist two singular measures whose convolution is absolutely continuous?,"Let $\mu, \nu$ be finite complex measures with compact supports on the real line, and assume that they are singular with respect to the Lebesgue measure. Can their convolution $\mu\ast\nu$ have a nonzero absolutely continuous component?","['measure-theory', 'singular-measures', 'functional-analysis', 'real-analysis']"
940011,Finding continuity and differentiability of a multivariate function,"Determine whether the following functions are differentiable, continuous, and whether its partial derivatives exists at point $(0,0)$: (a) $$f(x, y) = \sin x \sin(x + y) \sin(x − y)$$ (b)$$f(x,y)=\sqrt{|xy|}$$ (c)$$f(x, y) = 1 − \sin\sqrt{x^2 + y^2}$$ (d) $$f(x,y) = \begin{cases} \dfrac{xy}{x^2+y^2} & \text{if $x^2+y^2>0 $} \\ 0 & \text{if $x=y=0$} \end{cases}$$ (e) $$f(x,y) = \begin{cases} 1 & \text{if $x y \ne 0$} \\ 0
 & \text{if $xy=0$} \end{cases}$$ (f)$$f(x,y) = \begin{cases} \dfrac{x^2-y^2}{x^2+y^2} & \text{if $x^2+y^2>0$} \\ 0 & \text{if $x=y=0$} \end{cases}$$ My try: For (a), using the definition of the derivative for a multivariate function, the limit tends to $0$, hence it's differentiable and its partial derivative exists and it's continuous. For (b) I mentioned that is not differentiable as using the definition of the derivative for a multivariate function, the limit does not tend to $0$. While it's continuous, as the limit of the function $f(x,y)$ tends to $0$. To determine whether its partial derivative exists, this part is tricky because of the modulus sign in the function hence I'm unsure whether a modulus is differentiable for this case. For part (c) it should be continuous but not differentiable at $(0,0)$ because its partial derivative does not exists at $(0,0)$. As to why its partial derivatives does not exists, lets say to find $$f_x$$, we let $y=0$, the expression $f(x,y)$ becomes $1-\sin(|x|)$ which is not differentable at $(0,0)$, hence partial derivatives cannot exists at $(0,0)$. For (d), this question is also tricky, as although initially I though its partial derivatives exists, now I think likewise. Because if I want to differentiate the function with respect to $x$ for example, I would sub in the value of $y=0$ making the numerator a zero and hence assume the derivative is $0$. However, on closer look, there is still the denominator of $x^2$ and if $x^2=0$ the denominator becomes $0$ and since $0/0$ is undefined, the partial derivatives do not exist. As for continuity, it is not continuous and hence not differentiable. For (e) Not differentiable, Discontinuous, Partial derivatives defined
(because ""not continuous"" will mean it's not differentiable, but I'm unsure of the Partial derivatives portion though because it appears the the partial derivatives are $0$ but I also have the feeling that the partial derivatives do not exist.) For (f) Not differentiable, Continuous, Partial derivatives defined. This question appears to be similar to question (d) I have already attempted these questions many times, but I keep answering these questions incorrectly. I know that I must be missing out on some parts especially when these are tricky questions which are not as simple it might seem. Could anyone help me please? Thanks!","['multivariable-calculus', 'calculus', 'continuity', 'partial-derivative', 'derivatives']"
940028,Sums of binomial coefficients,"Does anyone know something about the following sums?
$$
S_m(n)=\sum\limits_{k=o}^n(-1)^k{mn\choose mk}
$$
Notice that $S_m(n)=0$ for odd $n$, so we only consider $S_m(2n)$. It holds that $S_0(2n)=1$, $S_1(2n)=0$, $S_2(2n)=(-4)^n$, $S_3(2n)=\frac{2}{3}(-27)^n$, but $S_4(2n)$ is no longer a geometric progression.","['summation', 'discrete-mathematics', 'binomial-coefficients', 'combinatorics']"
940034,Converting Recursive Function into Closed/Explicit Form,"so I have this recursive function here: $\forall n>1,f(n) = 2(f(n-1)) + n-1$, (where it is $0$ when $n$ is less than $1$) So I have tried to use iteration for this but it just gets more complicated as I go on, so can anyone help me out here? Thanks so much! Sorry for the bad format as I am new here. Cheers!","['recursion', 'closed-form', 'functions']"
940037,Integrate $\int\sqrt\frac{\sin(x-a)}{\sin(x+a)}dx$,"Integrate   $$I=\int\sqrt\frac{\sin(x-a)}{\sin(x+a)}dx$$ Let $$\begin{align}u^2=\frac{\sin(x-a)}{\sin(x+a)}\implies 2udu&=\frac{\sin(x+a)\cos(x-a)-\sin(x-a)\cos(x+a)}{\sin^2(x+a)}dx\\2udu&=\frac{\sin((x+a)-(x-a))}{\sin^2(x+a)}dx\\
2udu&=\frac{\sin(2a)}{\sin^2(x+a)}dx\end{align}$$
Now: 
$$\begin{align}u^2&=\frac{\sin(x+a-2a)}{\sin(x+a)}
\\u^2&=\frac{\sin(x+a)\cos(2a)-\cos(x+a)\sin(2a)}{\sin(x+a)}
\\u^2&=\cos(2a)-\sin(2a)\cot(x+a)
\\\cot(x+a)&=(\cos(2a)-u^2)\csc(2a)
\\\csc^2(x+a)=\cot^2(x+a)+1&=(\cos(2a)-u^2)^2\csc^2(2a)+1
\\\csc^2(x+a)&=\frac{\cos^2(2a)+u^4-2u^2\cos2(2a)+\sin^2(2a)}{\csc^2(2a)}
\\\sin^2(x+a)&=\frac{\sin^2(2a)}{u^4-2u^2\cos(2a)+1}\end{align}$$
Now:
$$\begin{align}
I&=\int u.\frac{2udu\sin^2(x+a)}{\sin(2a)}\\
I&=\int\frac2{\sin(2a)}.u^2.\frac{\sin^2(2a)}{u^4-2u^2\cos(2a)+1}du
\\I&=2\sin(2a)\int\frac{u^2}{u^4-2ku^2+1}du\quad k:=\cos 2a
\\\frac If&=\int\frac{2+u^{-2}-u^{-2}}{u^2-2k+u^{-2}}du=\int\frac{1+u^{-2}}{u^2-2k+u^{-2}}du+\int\frac{1-u^{-2}}{u^2-2k+u^{-2}}du\quad \\f:=\sin(2a)
\\&=\int\frac{d(u-u^{-1})}{(u-u^{-1})^2+2-2k}+\int\frac{d(u+u^{-1})}{(u+u^{-1})^2-2-2k}\end{align}$$ Now: $2-2k=2(1-\cos 2a)=4\sin^24a,2+2k=2(1+\cos 2a)=4\cos^24a$ So:
$$I=\sin2a\left(\frac1{2\sin4a}\arctan\left(\frac{u-u^{-1}}{2\sin(4a)}\right)+\frac1{4\cos4a}\ln\left|\frac{u+u^{-1}-2\cos 4a}{u+u^{-1}+2\cos 4a}\right|\right)+C$$
Or:
$$I=\frac1{4\cos2a}\arctan\left(\frac{-\sin a\cos x}{\sin4a\sqrt{\sin(x+a)\sin(x-a)}}\right)+\frac{\sin2a}{4\cos 4a}\ln\left|\frac{\sin x\cos a-\cos4a\sqrt{\sin(x+a)\sin(x-a)}}{\sin x\cos a+\cos4a\sqrt{\sin(x+a)\sin(x-a)}}\right|+C$$
But the textbook answer is: $$\cos a\arccos\left(\frac{\cos x}{\cos a}\right)-\sin a\ln(\sin x+\sqrt{\sin^2x-\sin^2a})+c$$",['integration']
940058,Cardinality of the power set [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Show that the cardinality of the power set of a finite non-empty $N$ set is a multiple of $2$. 
Then, show that it is exactly expressed by $2^n$, where $n$ is the cardinality of $N$ and that this holds also when $N$ is empty.",['elementary-set-theory']
940087,Convergent or divergent $\sum\limits_{n=0}^{\infty }{\frac{1\cdot 3\cdot 5...(2n-1)}{2\cdot 4\cdot 6...(2n+2)}}$,"\begin{align}
  & \sum\limits_{n=0}^{\infty }{\frac{1\cdot 3\cdot 5...(2n-1)}{2\cdot 4\cdot 6...(2n+2)}} \\ 
 & \text{ordering} \\ 
 & a_{n}=\frac{1\cdot 3\cdot 5...(2n-1)}{2\cdot 4\cdot 6...(2n+2)}=\frac{1\cdot 3\cdot 5...(2n-3)(2n-1)\cdot 1}{2\cdot 4\cdot 6...(2n-2)(2n)(2n+2)}= \\ 
 & \underbrace{\left( 1-\frac{1}{2} \right)\left( 1-\frac{1}{4} \right)...\left( 1-\frac{1}{2n} \right)}_{n\text{ times}}\frac{1}{(2n+2)} \\ 
 & \text{clearly} \\ 
 & \left( 1-\frac{1}{2} \right)^{n}\frac{1}{(2n+2)}\le a_{n}\le \left( 1-\frac{1}{2n} \right)^{n}\frac{1}{(2n+2)} \\ 
 & \text{Root test} \\ 
 & \sqrt[n]{\left( 1-\frac{1}{2} \right)^{n}}\frac{1}{\sqrt[n]{(2n+2)}}\le \sqrt[n]{a_{n}}\le \sqrt[n]{\left( 1-\frac{1}{2n} \right)^{n}}\frac{1}{\sqrt[n]{(2n+2)}} \\ 
 & n\to \infty  \\ 
 & \frac{1}{2}\le \underset{n\to \infty }{\mathop{\lim }}\,\sqrt[n]{a_{n}}\le 1 \\ 
 & \text{nothing :(} \\ 
 & \text{Ratio test} \\ 
 & \frac{a_{n}}{a_{n-1}}=\frac{1\cdot 3\cdot 5...(2n-3)(2n-1)}{2\cdot 4\cdot 6...(2n)(2n+2)}\centerdot \frac{2\cdot 4\cdot 6...(2(n-1)+2)}{1\cdot 3\cdot 5...(2(n-1)-1)}=\frac{2n-1}{2n+2} \\ 
 & \underset{n\to \infty }{\mathop{\lim }}\,\frac{a_{n}}{a_{n-1}}=1 \\ 
 & \text{nothing again} \\ 
\end{align} Any suggestions?",['sequences-and-series']
940102,"Why is that for any trigonometric function $f, f(2\pi + \theta )=f(\theta )$ for any value of $\theta$ [proof reading]","Here was the question asked to me ::  Why is that for any trigonometric function $f, f(2\pi + \theta )=f(\theta )$ for any value of $\theta$ I spontaneously said that it was because of their very definition. Here is the complete description of answer i provided. Normally we define trigonometric functions for acute angles using right angled triangle as follows. $$\sin \theta = \frac {opp}{hyp} \qquad  \cos \theta = \frac {adj}{hyp}  \qquad \tan \theta = \frac {opp}{adj}$$ $$\csc \theta = \frac {hyp}{opp} \qquad \sec \theta = \frac {hyp}{adj} \qquad \cot \theta = \frac {adj}{opp}$$ But we can extend our definitions for even other angles using point $P(a,b)$ on cartesian plane as follows Definition of trigonometric functions as i remember Consider a point in cartesian plane in $1^{st}$ quadrant as shown $$\sin \theta = \frac {b}{r} \qquad \cos \theta = \frac {a}{r} \qquad \tan \theta = \frac {b}{a}$$ $$\csc \theta = \frac {r}{b} \qquad \sec \theta = \frac {r}{a} \qquad \cot \theta = \frac {a}{b}$$ Now consider another point in $2^{nd}$ quadrant $$\sin \theta = \frac {b}{r} \qquad\cos \theta = \frac {-a}{r} \qquad \tan \theta = \frac {b}{-a}$$ $$\csc \theta = \frac {r}{b} \qquad \sec \theta = \frac {r}{-a} \qquad \cot \theta = \frac {-a}{b}$$ As you can see we consider the signed values of x and y in the definition keeping in mind that r is always greater than zero. similarly we can extend this definition (in other quadrants) for other angles. so if we rotate the line passing through origin and $P(a,b)$ through an angle of $360^o$ we again reach the same point and hence $f(2\pi + \theta )=f(\theta )$ where f is any trigonometric function. Is my arguement for the question given to me correct ???","['trigonometry', 'algebra-precalculus', 'definition']"
940118,What can be said about a measure with given marginal measures,"Let $(X,\mathcal F_X,\mu_X)$, $(Y,\mathcal F_Y,\mu_Y)$ be two measure spaces. Let $\mu$ be a measure on $\bigl(X\times Y, \sigma(\mathcal F_X \times \mathcal F_Y)\bigr)$ such that for each $A \in \mathcal F_X$ and each $B \in \mathcal F_Y$ we have
$$
   \mu(A \times Y) = \int_A f_\mu(x) \, d\mu_X(x), \quad (1) \\
   \mu(X \times B) = \int_B g_\mu(y) \, d\mu_Y(y). \quad (2)
$$
In other words, we know that marginal measures of measure $\mu$ are absolutely continuous w.r.t. measures $\mu_X$ and $\mu_Y$ with known densities $f_\mu$ and $g_\mu$, respectively. Is it possible to say that measure $\mu$ is absolutely continuous w.r.t. measure $\mu_X \otimes \mu_Y$, i.e. that there exists such function $u(x,y)$ that for each $C \in \sigma(\mathcal F_X \times \mathcal F_Y)$
$$
  \begin{align}
    \mu(C) & = \int\limits_C u(x,y) \, d(\mu_X \otimes \mu_Y)(x,y), \quad (3) \\
    f_\mu(x) & = \int\limits_Y u(x,y) \, d\mu_Y(y), \quad \text{a.e. w.r.t. $\mu_X$}, \quad (4)\\
    g_\mu(y) & = \int\limits_X u(x,y) \, d\mu_X(x) \, \quad \text{a.e. w.r.t. $\mu_Y$?} \quad (5)
  \end{align}
$$
I think that the answer in negative. In the Monge-Kantorovich problem one solves the problem of minimization
$$
   \int\limits_{X \times Y} h(x,y) \, d\mu(x,y) \to \min_\mu, \quad \text{$\mu$ satisfies (1), (2)},
$$
with known $\mu_X$, $\mu_Y$, $f_\mu$, $g_\mu$, $h$. And often it is shown that there exists a solution $\mu$, concentrated on some surface $y = F(x)$. If my considerations are correct, is it possible to say something about this measure $\mu$ in analogy with formulas (4)-(6)?","['probability-theory', 'measure-theory', 'optimization']"
940147,Maximal inequality for a sequence of partial sums of independent random variables [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let $X_n$, $n=1,2,3,...$ be a sequence of independent (not necessarily identically distributed) random variables, let $S_n=\sum_{i=1}^nX_i$. Prove the following maximal inequality: For all $t>0$,$$\mathbb{P}\left(\max_{1\leq i\leq n}|S_i|\geq t \right)\leq 3\max_{1\leq i\leq n}\mathbb{P}\left(|S_i|\geq\frac{t}{3} \right)$$","['probability-theory', 'integral-inequality', 'independence']"
940164,"Proving, that closure of set is equal this set iff set is closed","I've started intorduction to topology course and I need help with one of the problems: Let $A \subset(X,T). $ Prove that $cl(A) = A\iff A$ is closed. It may looks trivial, but I had a little (too long...) break from math and I'm struugling now. Need some help and time to understand problems. Also, if anyone can recommend good book/website with introduction to topology and/or functional analysis, I'd be grateful :) Have a nice math day!","['general-topology', 'self-learning']"
940166,Minimum and Maximum of a set,I need help at the following: How can we find the minimum and the maximum of a set of $N$ numbers using $1.5N $ comparisons?,['discrete-mathematics']
940171,how to prove that $\sin(90+v) = \cos(v)$,I need to prove that $\sin(90+v) = \cos v$ and that $\cos(90+v) = -\sin v$ So I did the following steps to prove these statements $\sin(90+v) = \sin(90-(-v)) = \cos(-v) = \cos(v)$ $\cos(90+v) = \cos(90-(-v)) = \sin(-v) = -\sin(v)$ Is this correct? Thanks!!,['trigonometry']
940193,Convergence of $\sum{a_kb_k}$ if $\sum{a_k}$ converges and $\sum{b_k}$ absolutely converges.,"Convergence of $\sum{a_kb_k}$ if $\sum{a_k}$ converges and $\sum{b_k}$ absolutely converges. I tried to think that
Since $\sum |b_k|$ is bounded I thought that $\sum a_k b_k$ $<$ $S\sum a_k$. Is it a correct inequality? If both sequences are just conditionally converge,
then do $\sum{a_k b_k}$ converge?","['real-analysis', 'analysis']"
940205,Proving $\sum_i \binom{k}{k-i} \binom{n-k}{i} = \binom{n}{k}$,"I am in the middle of a probability question. The question is indeed simple. For the sake of clarity of the notation, I also include the question here, which is from Sheldon M. Ross's Introduction to Probability Models : A fair coin is independently flipped $n$ times, $k$ times by A and $n-k$ times by B. Show that the probability that A and B flip the same number of heads is equal to the probability that there are a total of $k$ heads. I have reduced the question to proving $$\sum_i \binom{k}{k-i} \binom{n-k}{i} = \binom{n}{k}$$ and I cannot move on. So far, I have tried to expand $\binom{k}{k-i}$ and $\binom{n-k}{i}$ by the definition of binomial coefficient and then observe for common terms. However, it comes out to be a mess and I am not sure if there is other way to do it. Thanks in advance.","['probability-theory', 'binomial-coefficients']"
940206,First Isomorphism Theorem to identify a quotient,"I understand the notion of quotient groups quite well (I think), but I'm struggling a little bit with the following problem: Let $G$ denote the group of 2x2 invertible real upper triangular matrices, and $H\vartriangleleft G$ the subgroup with $a_{11}=a_{22}$. Identify the quotient group $G/H$ up to isomorphism using the First Isomorphism Theorem. The theorem gives a way to find the quotient once a homomorphism $\varphi:G\to G'$ with $G'$ some unknown group and $\ker\varphi=H$ has been found. Is there any systematic way to find $\varphi$ and/or $G'$ without blindly groping around in the dark and trying test cases?",['abstract-algebra']
940240,"What is the intuition behind homeomorphism, especially behind the geometrical notion of ""gluing together""?","Intuitively, a homeomorphism is a way of mapping two spaces without any tearing or gluing together . Thus, I would expect the formal definition of homeomorphism in terms of continuous functions to be intuitive as well. Specifically, the book I am reading says (translated from Chinese): A homeomorphism is a mapping $f$ with the properties of $f$ is both surjective (don't create new points [in Chinese: 不产生新点]) and injective (without overlapping [in Chinese: 不出现重叠现象]); $f$ is continuous (without tearing [in Chinese: 不撕裂]); $f^{-1}$ is continuous (without gluing [in Chinese: 不粘连]). The first two properties are quite intuitive to me. However, I am confused about the third one. What is gluing geometrically? What is the difference of it from non-injection (thus, overlapping) of a mapping? How is it related to the continuity of $f^{-1}$ ? Consider the frequently-used example which demonstrates that the mapping $$f: [0,1) \to S^{1}: f(t) = e^{i 2 \pi t}$$ , where $S^{1}$ is the unit circle in the complex plane,
is not a homeomorphism. With $f^{-1}$ being not continuous, I would expect some gluing phenomenon in the deformation between $[0,1)$ and $S^{1}$ . What is the gluing phenomenon in the above example?","['definition', 'general-topology', 'functions', 'continuity', 'intuition']"
940264,Derivative of $f(x) = x^2 \sin(1/x)$ using the derivative definition,"derivative of $f(x) = x^2 \sin(1/x)$ using the derivative definition When not using the derivative definition I get $\cos (1/x) + 2x \sin(1/x)$,
which WolframAlpha agrees to. However when I try solving it using the derivative definition:
$$\lim_ {h\to 0} = \frac{f(x+h) - f(x)}{h} $$ I get:
$$2x \sin \left(\frac{1}{x+h} \right ) + h \sin \left(\frac{1}{x+h}\right)$$ which in return results in, as $h \to 0$: $$2x \sin (1/x)$$ So what am I doing wrong when using the def of derivatives?",['derivatives']
940266,Cardinality of sets regarding,"Consider the following sets of functions on $\mathbb{R}$. $W=$The set of all constant functions on $\mathbb{R}$ $X=$The set of polynomial functions on $\mathbb{R}$ $Y=$ The set of continuous functions on $\mathbb{R}$ $Z=$ The set of all functions on $\mathbb{R}$ Which of the sets has the same cardinality as that of $ \mathbb{R} $? Only $ W $ Only $ W $ and $X$ Only $ W, X$ and $Y$ All of $ W,X,Y $ and $Z$ I suppose $W$ is the correct answer as $X$ is equivalent to a subset of $\mathbb R\times \mathbb R$ and $X\subset  Y\subset Z$. Am I correct?","['elementary-set-theory', 'real-analysis']"
940293,"Group of order $|G|=pqr$, $p,q,r$ primes has a normal subgroup of order","Let $p,q,r$ be positive primes, $p<q<r$, and let $G$ be a group with $|G|=pqr$. Show that there exists a normal subgroup $H$ of $G$ of order $qr$. I've seen this post Groups of order $pqr$ and their normal subgroups with the same problem, but I couldn't follow the answer. I'll write what I've done up to now: We know that $$n_r \equiv 1 (r),\ n_r\mid pq,\ r>q,p \implies n_r \in \{1,pq\}.$$ By a similar argument, we arrive to $$n_q \in \{1,r,pr\},$$ $$n_p \in \{1,q,r,qr\}.$$ Here $n_i$ denotes the number of $i$-Sylow groups for $i=p,q,r$. If $n_r=1=n_q$, then there are only one $r$-Sylow subgroup, $H$ and one $q$-Sylow subgroup, $K$, then $H,K \lhd G$, so $KH$ is a normal subgroup and $|KH|=qr$. I don't know what to do for other cases, I would appreciate some help.","['sylow-theory', 'finite-groups', 'group-theory', 'abstract-algebra']"
940313,Can someone explain this proof by case example?,"I don't fully understand this example the book gives.  I understand the section where they examine when $x\gt0$ but when $x\lt0$ things get murky for me. Prove that for every real number $x$, $x\le\lvert x \rvert$.",['discrete-mathematics']
940338,The myth of no prime formula?,"Terence Tao claims : For instance, we have an exact formula for the $n^\text{th}$ square
  number – it is $n^2$ – but we do not have a (useful) exact
  formula for the $n^\text{th}$ prime number $p_n$! “God may not play dice with the universe, but something
  strange is going on with the prime numbers.” (Paul Erdős,
  1913–1996) However there exist an exact formula for the nth prime A double sum for the n th prime p n is $$p_n=1+\sum_{k=1}^{2(\lfloor n\ln n\rfloor+1)}\Biggl[1-\Biggl\lfloor\frac{\sum_{j=2}^k 1+\lfloor s(j)\rfloor}n\Biggr\rfloor\Biggr],\tag{13}$$ where $$s(j)\equiv-\frac{\sum_{s=1}^j \bigl(\bigl\lfloor\frac js\bigr\rfloor-\bigl\lfloor\frac{j-1}s\bigr\rfloor\bigr)-2}j\tag{14}$$ (See Prime formulas at MathWorld.)  There exist even an exact formula for prime counting function $\pi (x)$. So why are mathematicians trying to prove the Riemann hypothesis to find a better estimate for $\pi(x)$ and $p_n$ when they have those exact formulas?","['prime-numbers', 'riemann-hypothesis', 'number-theory']"
940351,Equality in set theory,"In Introduction to Axiomatic Set Theory by G. Takeuti and W. M. Zaring chapter 3 It is given: Definition of equality as: $a=b \Leftrightarrow (\forall x)[x \in a \Leftrightarrow x \in b]$. And it is written this is incomplete in sense variable $x$ does not occur in $a = b$ and hence we have not made clear which of infinitely many formulas, equivalent under alphabetic change of variables, we intend $a=b$ to be abbreviation for. But the problem is easily resolved by specifying that $x$ is the first variable on our list of variables(given in alphabetical order) that is distinct from $a$ and $b$. I get the point what problem is but didn't get point how it got resolved from last sentence","['logic', 'elementary-set-theory']"
940354,What is the limit of this specific function?,"Please evaluate the following limit for me: $$\lim_{x \to -1} \frac{\sqrt{x^2+8}-3}{x+1} $$ I'd tried my best to solve this but unfortunately, it's too difficult for me. I tried multiplying by its conjugate and factoring the $x^2$ out but I can't get rid of that $x+1$ in the denominator so it always stay in the indeterminate form.","['algebra-precalculus', 'limits']"
940405,Picard Iterates Converge Uniformly,"I have a homework question that asks to show that the Picard iterates
$$ \phi_{n+1}(t) = \int_0^t 1 + \phi_n^2(s) \, ds, \quad \quad \phi_0(t) = 0 $$
converge uniformly on any compact interval $[-r, r] \subseteq (- \pi/2, \pi/2)$. I was thinking about stating the fact that those Picard iterates represent the initial value problem
$$ x' = 1 + x^2 = f(x), \quad \quad x(0) = 0, $$
whose solution is of course $x(t) = \tan(t)$.  That is,
$$ \phi_{n+1}(t) = \int_0^t f(\phi_n(s)) \, ds, \quad \quad \phi_0(t) = 0. $$
Then use the fact that $f$ is Lipschitz on any compact interval to get a bound
$$ \| \phi_{n+1} - \phi_n \| \leq K \| \phi_n - \phi_{n-1} \| $$
for some constant $K$ (where $\| \cdot \|$ is the sup norm).  Then I could bound $ \| \phi_n - \phi_m \|$ and show that $\{ \phi_i \}$ is a Cauchy sequence and I'd be done.  However, I think $K$ has to be less than $1$ for that argument to work and I don't think I can show that.  Also, that argument doesn't take into account the restriction $[-r, r] \subseteq (- \pi/2, \pi/2)$, only the fact that $f$ is Lipschitz on any compact interval. I'd really appreciate a push in the right direction.","['lipschitz-functions', 'ordinary-differential-equations', 'convergence-divergence', 'real-analysis', 'analysis']"
940413,How to parallel transport a coframe field in a geodesic normal neighborhood?,"From Chern: Lectures on Differential Geometry, page 147 Chern claimed that a torsion-free connection is completely determined locally by the curvature tensor. To show that he considered a geodesic normal neighborhood at a fixed point $O$. He choose a natural frame at $O$, then parallel transport it along the radial lines to form a frame field $\{ e_{i}\}$ in the neighborhood. If we denote the dual frame field by $\{\theta^{i}\}$ such that $\theta^{i}(e_{j})=\delta_{ij}$, and let the direction vector of the radial line to be $\alpha^{i}_{0}$ at the origin, then Chern claims that 
$$
\theta^{i}\equiv a^{i}dt \pmod{d\alpha^{k}}, \theta^{j}_{i}\equiv 0 \pmod{d\alpha^{k}}, a^{k}=t\alpha^{k}_{0}
$$
because the frame field is parallel along the geodesic curve $a^{i}t$. Here $\theta^{j}_{i}$ is the connection matrix. However, it is not clear to me how to derive it. It is clear that we can write the frame field by
$$
X=x^{i}\frac{\partial}{\partial u_{i}}, x_{i}=ta^{i}_{0}
$$
Now substitute this into the equation $DX=0$ we get
$$
\frac{dx^{i}}{dt}+x^{j}\Gamma^{i}_{jk}\frac{du^{k}}{dt}=0\rightarrow a^{i}_{0}+ta^{j}_{0}\Gamma^{i}_{jk}a^{k}_{0}=0\rightarrow a^{i}_{0}=0?
$$
But I do not see how this help me to get $\theta^{i}\equiv a^{i}dt \pmod{d\alpha^{k}}$. I assume it should be something either self-evident or trivial. One way I thought about deriving it is to regard $\{\theta^{i}\}$ to be a covector field, thus we can use the equation $D\theta^{i}=-\sum^{q}_{\alpha=1}\theta^{i}_{j}\theta^{j}=0$, but it does not give me the result I wanted.","['riemannian-geometry', 'differential-geometry']"
940415,Is $f(x) = \infty$ a function?,"Recently, while solving a problem where a certain set of functions $f:\mathbb Z^+ \rightarrow \mathbb Z^+$ had to be found given a number of conditions, I noticed that $f(n)=\lim_{a\to+\infty} a$, where  $n\in \mathbb Z^+$, was a solution. My question is simply whether $f(n)=\lim_{a\to+\infty} a$ really  can be classified as a function. If not, why not, and if yes, does the condition $f:\mathbb Z^+ \rightarrow \mathbb Z^+$ still hold?",['functions']
940432,Diagonalizing $xyz$,"The quadratic form  $g(x,y) = xy$    can be diagonalized by the change of variables  $x = (u + v)$   and  $y = (u - v)$ . However, it seems unlikely that the cubic form  $f(x,y,z) = xyz$, can be diagonalized by a linear change of variables. Is there a short computational or theoretical proof of this? Thanks.","['linear-algebra', 'abstract-algebra']"
940442,Prove that $n_p(N)$ divides $n_p(G)$,"Let $N$ be a normal subgroup of $G$ where $G$ is finite group, then we have to prove $n_p(N)$ divides $n_p(G)$ ( here $n_p(G)$ means number of sylow $p$-subgroups of $G$) I was able to prove that $n_p(N)$ $\leq$ $n_p(G)$ as by proving first that for any $P \in Syl_p(G)$ there is a conjugate $gPg^{-1}$ such that $gPg^{-1}\cap H \in Syl_p(H)$ for any subgroup $H$ of $G$ whether normal or not.( by considering action of $H$ on $G/P$ and then considering $stab_gP$ And by replacing $H$ by $N$ i.e. a normal subgroup I could show that for any $P \in Syl_p(G), P\cap N \in Syl_p(N)$ and all $p$-sylow subgroups of $N$ arise in this way. It all follows from a paper of Conrad, but from here and this proof i can't seem to figure out how to show it will divide $n_p(G)$. Please help.","['sylow-theory', 'finite-groups', 'group-theory']"
940443,"Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. [duplicate]","This question already has answers here : $\langle 2,x \rangle$ is a non-principal ideal in $\mathbb Z [x];\, $ $D[x]$ PID $\iff D$ field, for a domain $D$ (8 answers) Closed 8 months ago . I've just read a proof of the statement : Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. In one part of the proof there is a step which I don't understand. I'll copy the proof : Let $u\in R$ be non-zero. Then using the principal ideal property, for some $f\in R[X]$ we have $\langle u,X\rangle = \langle f \rangle \subseteq R[X]$ . Therefore, for some $p,q∈R[X], u=fp$ and $X=fq$ . By properties of degree we conclude that $f=a$ and $q=b+cX$ for some $a,b,c\in R$ . Substituting into the equation $X=fq$ we obtain $X=ab+acX$ which implies that $ac=1$ , i.e. $a\in R^\times$ , the group of units of $R$ . Therefore, $\langle f\rangle =\langle 1\rangle=R[X]$ . Therefore, there exist $r,s\in R[X]$ such that $ru+sX=1$ and if $d$ is the constant term of $r$ , then we have $ud=1$ . Therefore $u\in R^\times$ . Our choice of $u$ was arbitrary, so this shows that $R^{\times}⊇R \setminus \{0\}$ , which says precisely that $R$ is a field. I don't understand this: "" $X=ab+acX$ which implies that $ac=1$ , i.e. $a\in R^\times$ , the group of units of $R$ . Therefore, $\langle f\rangle = \langle 1\rangle =R[X]$ ."" Why does the fact that $a=f$ is a unit implies that $\langle f\rangle = \langle 1\rangle$ ? I would appreciate if someone could explain this to me. Thanks in advance.","['principal-ideal-domains', 'ring-theory', 'abstract-algebra']"
940456,"Check that the parametrization x(u,v)is conformal if and only if E=G and F=0.","Check that the parametrization x(u,v)is conformal if and only if E=G and F=0. 
I am slightly confused with what this question is asking me. Could someone please walk me through this question. I believe that for --> we need to choose two convenient pairs of orthogonal directions. However, I am unsure of where to start and how to proceed with this question. 
Thank yoU!","['conformal-geometry', 'differential-geometry']"
940495,Integration by parts on all of $\mathbb{R}^n$ with $n>1$,"So this came up as I was thinking about the uniqueness of solutions to the wave equation. I have seen proofs for uniqueness on all of $\mathbb{R}$ or on bounded subsets of $\mathbb{R}^n$, but never $\textit{explicitly}$ on all of $\mathbb{R}^n$. I tried to use the energy approach method to see if I could show uniqueness on all of $\mathbb{R}^n$, however, I ran into trouble when I started doing the integration by parts. i.e., I realized that I don't know the validity of integration by parts on an unbounded domain in dimensions greater than 1. So is this valid? It seems like a proof would look something like below $\lim_{R \rightarrow \infty} \int_{B(0,R)} u\cdot \nabla v d\bar{x} = \lim_{R \rightarrow \infty} [\int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}- \int_{B(0,R)} v \nabla \cdot u  d\bar{x}]$ However, is $\lim_{R \rightarrow \infty} \int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}$ a well defined expression? Part of my worry is I looked up multidimensional integration by parts on wikipedia but it explicitly stated the expression for bounded domains and didn't mention unbounded. From experience past experience there isn't necessarily problems with taking integrals over infinite domains, but I haven't encountered many infinite integrals over a surface. Can anybody offer some illumination?","['multivariable-calculus', 'improper-integrals', 'partial-differential-equations', 'vector-analysis', 'wave-equation']"
940526,Two possible senses of a random variable being a function of another random variable,"Given two random variables X and Y (assumed measurable as usual), consider two conditions: There is a (not necessarily measurable) function $f: \mathbb R \to \mathbb R$ such that $Y = f(X)$ holds. There is a Borel-measurable function $f: \mathbb R \to \mathbb R$ such that $Y = f(X)$ almost surely. Does the first condition imply the second condition? In other words, if X determines Y in the set theory sense, does X determine Y in the probability theory sense too?","['probability-theory', 'measure-theory']"
940531,Applying the Law of Large Numbers?,"$X_k$, $k \geq 1$ are iid random variables such that
$$\limsup_{n\rightarrow\infty} \frac{X_n}{n} < \infty$$
with probability 1. We want to show that
$$\limsup_{n\rightarrow\infty} \frac{\sum_{i=1}^n X_i}{n} < \infty$$
with probability 1. The hint says to apply the law of large numbers to the sequence $\max(X_k,0), k \geq 1$. SLLN gives that
$$\frac{\sum_{i=1}^n \max(X_i,0)}{n} \rightarrow \mathbb{E}\max(X,0) = \mathbb{E}(X; X>0)$$
almost surely. I feel that the idea here is that $\limsup X_n/n < \infty$ a.s. implies that $\mathbb{E}(X;X>0)$, but I am not really sure how to approach this...","['probability-theory', 'probability']"
940600,Skew Symmetric Matrix Properties,"We have a theorem says that ""ODD-SIZED SKEW-SYMMETRIC MATRICES ARE
SINGULAR"" . Proof link is given here if needed. Now let us assume we have a $3\times 3$ skew symmetric matrices of the form $ \begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix}$ and an Identity matrix $I_{3\times3}$ Question Can we say determinant of $I_{3\times3}+\begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix} \tag 1$ is not zero always? if so how can we prove it mathematically ? NB:: $a_1,a_2,a_3$ cant be zero together at a time","['matrix-decomposition', 'matrices', 'linear-algebra', 'matrix-equations', 'block-matrices']"
940629,Understanding the proof that $c_0$ is a closed subspace of $\ell^\infty$,"The problem is given: source Let $c_0$ be a space of real sequences $x = \{x_n\}_{n = 1}^\infty=0$ converging to $0$. Let $\ell^\infty$ be a set of real sequences $w = \{w_k\}^\infty_{k=0}$ furnished with the norm $\|w \|_\infty = \sup_{1 \leq n \leq \infty} |w_n|$. Prove that $c_0$ is closed in $\ell^\infty$ The pdf says TL;DR : Please check whether my recap of the proof is correct? As detail as possible so I have a deep understanding of what is really going on. Thank you Start with a sequence in $c_0$, our goal is to show that its limit $\omega  \in \ell^\infty$ is also in $c_0$. This limit exists by completeness of $\ell^\infty$. Denote this sequence by $(x_n^k)_{k=1}^{\infty} = ( \{x^1_1,x_2^1,\dots \}, \{x_1^2, x_2^2,\dots \}, \dots)$. As the objects in $c_0$ are sequences, we have a sequence of sequences. A quick remark: My intuition tells me that since the elements of $c_0$ all converge to $0$, it would seem to make sense to me that this sequence (of sequences) will also converge to a sequence (of sequences) to $0.$ That is $\omega = 0.$ Instead of showing that the entire sequence (of sequences) $(x_n^k)_{k=1}^{\infty}$ converges to a limit (of sequences) $(\omega^{k}_n)_{k =1}^{\infty}$ (this should be a sequence of constant sequences), we show that each term of $(x_n^k)_{k=1}^{\infty}$ converges to each term of $(\omega_n^k)_{k = 1}^{\infty}$. Note that for a fixed $k$, $\omega_n^k = (\omega_1^k, \omega_2^k, \dots, )$, so the elements are numbers I am guessing $(\omega_n^k)_{k =1}^{\infty}$ is another sequence in $c_0$? I don't know why you want $(\omega_n^k)_{k =1}^{\infty}$to be in $\ell^\infty$ in the first place. I am also guessing that when they say Take $\epsilon >0$ and $N_0 \in \mathbb{N}$ such that $\sup_{1 \leq n \leq \infty} |x_{n}^{k} -  \omega_n| < \epsilon/2$ for all $k > N_0$. They are using the convergence of $(x_n^k)_{k=1}^{\infty}$ I am guessing that when they say For each $k $, choose $N_1 \in \Bbb N$ such that $|x_n^k| < \epsilon/2$ for all $n > N_1$ and $k > N_0$ They are using the convergence of the elements in $(x_n^k)_{k=1}^{\infty}$, which all go to $0$. Remark: As opposed to (5), they don't really know if the sequence (of sequence) $(\omega_n)$ really go to a sequence (of sequences) of $0$s. So they denote this ""mystery"" limit of sequence (of sequences) by $(\omega_n^k)_{k = 1}^{\infty}$ (7) I think the conclusion now is that $0 = \omega \in c_0$? (8) Would it be more firm to make $k,n > \max \{N_1,N_1\}$","['proof-explanation', 'functional-analysis', 'banach-spaces']"
940634,Solutions of homogeneous linear differential equation form a vector space,Show that the solutions of a homogeneous linear differential equation $y''+a(x)y'+b(x)y = 0$ form a vector space. What is its dimension? I understand that the dimension is 2 and that 0 is a solution to the differential equation ($0''+a(x)*0'+b(x)*0=0$). How does one go about proving the other two properties of a vector space: closed under addition and closed under multiplication?,"['vector-spaces', 'linear-algebra', 'ordinary-differential-equations']"
940640,Why do we use dummy variables in integrals?,I want to know why we use dummy variables in integrals?,"['calculus', 'integration']"
940653,Writing the roots of a polynomial with varying coefficients as continuous functions?,"Consider the monic polynomial $$p_{\zeta}(z) = z^n + a_{n-1}(\zeta)z^{n-1} + \dots + a_0(\zeta), $$ where the $a_{i}$'s are continuous functions defined over $\mathbb{C}$. As is well known, the roots of this polynomial depend 'continuously' on the $a_{i}$'s in some sense. However, I was wondering if a stronger statement holds: is it possible to pick continuous functions $f_1, \dots, f_n$, at least locally, such that the roots of $p_{\zeta}(z)$ are  exactly $f_1(\zeta), \dots, f_n(\zeta)$, counting multiplicities? Edit: Note this is easily answered in the affirmative around a $\zeta_0$ such that $p_{\zeta_0}(z)$ has $n$ distinct roots. The real difficulty is in dealing with the case where $p_{\zeta}(z)$ has repeated roots.","['roots', 'algebraic-geometry', 'polynomials', 'analysis', 'complex-analysis']"
940666,"Find all functions $f:\Bbb Q\rightarrow\Bbb Q$ satisfying $f(x+y)+f(x-y)=2f(x)+2f(y)$ for all $x,y\in\Bbb Q$","Find all functions $f:\Bbb Q\rightarrow\Bbb Q$ satisfying $f(x+y)+f(x-y)=2f(x)+2f(y)$ for all $x,y\in\Bbb Q$ I don't know how to proceed, any help would be really appreciated..","['contest-math', 'functions']"
940716,Prove $\sum_{i=0}^{i=x} {x \choose i} {y+i \choose x}+\sum_{i=0}^{i=x} {x \choose i} {y+1+i \choose x}$,"How to prove that 
$$\sum_{i=0}^{i=x} {x \choose i} {y+i \choose x}+\sum_{i=0}^{i=x} {x \choose i} {y+1+i \choose x}=\sum_{i=0}^{i=x+1} {x+1 \choose i} {y+i \choose x}$$ ? I tried to break the right side of equation down: 
$$\sum_{i=0}^{i=x+1} {x+1 \choose i} {y+i \choose x}=\sum_{i=0}^{i=x} {x+1 \choose i} {y+i \choose x}+{x+1 \choose x+1} {y+x+1 \choose x}$$ Then I tried Vandermonde's Identity:
$${y+x+1 \choose x} = \sum_{i=0}^{i=x} {y+1 \choose i}{x \choose x-i}$$ Now I am totally lost. Can someone please tell me how to prove this equation?","['summation', 'binomial-coefficients', 'combinatorics']"
940755,Are there Cantor sets of non-zero measure?,"A cantor set is generated by removing a centered open interval from $[0, 1]$ and repeating the process infinitely on the two leftover segments. What if the first interval we remove is of length $r$, then we remove an interval of length $r^2$ from the two leftovers, $r^3$ from the four leftovers after that, and so on? Obviously we have $r<1$. Then the total amount removed is something like $\sum (2r)^n$ (give or take a few off-by-one errors), which can be made less than $1$ if $r$ is small enough, implying that what's left over (the Cantor set) is of non-zero measure. (The only thing you have to worry about as far as I can see is if the process might involve trying to remove a segment of length $r^n$ from a segment of length $<r^n$. After a lot of calculation I think I've shown this is impossible if $r<\frac 1 3$)","['measure-theory', 'cantor-set']"
940773,Suppose A and B are sets. Prove that $A\subseteq B$ if and only if $A \cap B = A$.,"Suppose $A$ and $B$ are sets. Prove that $A \subseteq B$ if and only if $A \cap B = A$ . Here's how I see it being proved. If $A$ and $B$ are sets,and the intersection of $A$ and $B$ is equal to $A$ , then the elements in $A$ are in both the set $A$ and $B$ . Therefore, the set of $A$ is a subset of $B$ since all the elements are contained in the interesection of sets $A$ and $B$ are equal to $A$ . Can I prove it that way?",['elementary-set-theory']
940785,A proof involving nested integrals and induction [duplicate],"This question already has answers here : Evaluate a Nested Integral (2 answers) Closed 9 years ago . Prove that $$\int_0^x dx_1 \int_0^{x_1}dx_2 \cdots \int_0^{x_{n-1}}f(x_n) \, dx_n =\frac{1}{(n-1)!}\int_0^x (x-t)^{n-1}f(t) \, dt$$ I'm trying induction over $n$. The case $n=1$ is trivial. When $n=2$ \begin{align}\int_0^x dx_1 \int_0^{x_1}f(x_{2})\,dx_2 = & \int_0^x\int_0^{x_1}f(x_2) \, dx_2 \, dx_1 \\
= & \int_0^x \int_{x_2}^{x} f(x_2) \, dx_1 \, dx_2 \\
=& \int_0^x f(x_2)(x_1-x_2) \, dx_2 = \frac{1}{(2-1)!}\int_0^x (x-t)f(t) \, dt\end{align} I couldn't take the induction step though. Any thoughts? I appreciate the help.","['multivariable-calculus', 'integration']"
940790,2nd order nonlinear ODE question,"I am looking for help to solve the following $F(x,y(x),y'(x),y''(x))=0$ equation: $$
xy''(x)-y'(x)-(x^2)y(x)y'(x)=0
$$ Very much appreciated.",['ordinary-differential-equations']
940821,Proving the derived set $E'$ is closed.,"I was reading the proof in Rudin, but it uses the metric. Is this not true if $X$ is a general topological space and $E' \subset X$ (especially if it is not Hausdorff $T_1$)? I can't come up with a counterexample. To be specific, it's difficult for me to pass from one neighbourhood to the other without using a metric. Note that $E'$ is the set of all limit points.","['general-topology', 'examples-counterexamples']"
940841,Coupon Collector Problem Extension,"Humans have two copies of each of $23$ chromosome, for a total of $46$ chromosomes. If you want to sequence someone's DNA, you can just use a normal cell, since they have all the chromosomes. But if you have a sample of gamete cells, each one has only one copy of each chromosome. So, how would you find the expected number of gamete cells you'd have to sequence to get the person's entire genome? If humans only had $1$ chromosome, then this would be a simple coupon collector problem, which asks the expected number of times you'd choose randomly between two coupons until you get them both. In this case, there are $23$ pairs of coupons, and you randomly select one from each pair, and you want to know how many times you expect to do this until you get all $46$ coupons.",['probability']
940856,Find the Basis and Dimension of a Solution Space for homogeneous systems,"I have the following system of equations: $$\left\{\begin{array}{c}
x+2y-2z+2s-t=0\\
x+2y-z+3s-2t=0\\
2x+4y-7z+s+t=0
\end{array}\right.$$ Which forms the following matrix $$\left[\begin{array}{ccccc|c}
1 & 2 & -2 & 2 & -1 & 0\\
1 & 2 & -1 & 3 & -2 & 0\\
2 & 4 & -7 & 1 & 1 & 0
\end{array}\right]$$ Which I then row reduced to the following form: $$\left[\begin{array}{ccccc|c}
1 & 2 & 0 & 4 & -3 & 0\\
0 & 0 & 1 & 1 & -1 & 0\\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]$$ I am unsure from this point how to find the basis for the solution set.  Any help of direction would be appreciated.  I know that the dimension would be $n-r$ where $n$ is the number of unknowns and $r$ is the rank of the matrix but I do not know how to find the basis.","['matrices', 'linear-algebra', 'systems-of-equations']"
940883,Find Point of intersection of the tangent plane to surface,"Find the point of intersection of the tangent plane to the surface $z+1=xe^y\cos(z)$ at the point $(1,0,0)$ and the line $L$ given by: $x=2t, y=t+1, z=1-3t$.",['multivariable-calculus']
940938,proof regarding the commutativity of an arbitrary oddball binary operator?,"My niece has shown me a problem for her advanced high school algebra class that I am personally finding fascinating, regarding the proof (or lack thereof) of the commutativity of a particular arbitrarily defined binary operator (basically, a randomly defined operator just for the sake of an interesting homework problem). (This is not a homework-tag question; as a programmer I am personally interested in this.) Here is the operator: Define a # b ( a and b are positive integers) such that # is a
  binary operator that reverses the (decimal) digits of positive integer a , adds the
  reversed number so obtained to positive integer b , then adds up the (decimal) digits
  of the result of this addition. If the result has more than one (decimal) digit, add these digits together to obtain a new result, and repeat until only one digit remains. ( NOTE : I added the final sentence of the above definition after seeing @angryavian's answer, below - the final sentence was not in the original problem, but we noticed that commutativity applies when the final sentence is added to the definition of the operator.) Putting in a number of various examples has always worked, so that it seems from examples that # is commutative.  For example, a = 791 , b = 907 results in: a # b = 791 # 907 -> 197 + 907 = 1104 --> 6 b # a = 907 # 791 -> 709 + 791 = 1500 --> 6 ... Other random examples also work, such as a = 1348 , b = 26935 . As a programmer, I could write a simple program that creates an iterative loop (involving taking the modulus base 10 until no more digits remain) just to test any combination of numbers.  But, proving the commutativity of this operator is another story. It strikes me as unintuitive that such an arbitrary operator as this would turn out to be commutative, but every combination of numbers we've tried results in the same value when a and b are reversed. I do not yet have a sense of how to go about actually proving this (assuming it's true). How would one go about proving that this operator # is commutative?  (Assuming that it is, indeed, commutative and that the numbers we've tested aren't just coincidentally commutative.)",['algebra-precalculus']
940991,How are critical values derived for the Kolmogorov-Smirnov Test?,"One appealing feature of the K-S test is that it is distribution-free. So this leads to my question - how are the critical values for the K-S derived, then? Is there a way to express the critical values as an integral, like for percentiles of the standard normal distribution? Sources that have such information would be very helpful (i.e., a textbook). See, for example, the table below (from http://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf ).",['statistics']
941015,Three dice rolled,"Three indistinguishable (fair) dice are thrown simultaneously at random.
Find the probability that the sum of the three dice is less than six and that no two dice show the same face (the same number). I know that the probability that the sum three dice is less than six is 10/216 (or 5/108) and the probability that no two dice show the same face is 5/9. I don't know how to put them all together?!","['statistics', 'dice', 'probability']"
941029,Is every convex function on an open interval continuous? [duplicate],"This question already has answers here : Prove that every convex function is continuous (11 answers) Closed 3 years ago . Let $f:(a,b)\rightarrow \mathbb{R}$. $f$ satisfied the following property: If $\forall x_{1},x_{0},x_{2}\in(a,b)$ and $x_{1}<x_{0}<x_{2};$then$\frac{f(x_{0})-f(x_{1})}{x_{0}-x_{1}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}.$ My question : whether the function $f\in C((a,b))?$ We can get  $\displaystyle\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}. $Let $\displaystyle g(x)=\frac{f(x)-f(x_{0})}{x-x_{0}},$ if we can prove $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0});$ then we have $f^{'}_{-}(x_{0}) $ exists .In a similar way ,$f^{'}_{+}(x_{0}) $ exists. 
$f^{'}_{-}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}-}f(x)=f(x_{0});$ $f^{'}_{+}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}+}f(x)=f(x_{0}).$  Obviously, $f$ is coutinuous at $x=x_{0}.$Further, $f\in C((a,b))!$ But  I failed to prove  $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0}).$Sometimes  I doubt the conculsion that  $f\in C((a,b)).$
 Either make a counterexample to deny it ,or prove the conculsion is correct?","['convex-analysis', 'continuity', 'real-analysis']"
941036,"the zeros of $\sin(z)$, where $z$ is a complex number","How do I find the zeros of $\sin(z)$, where $z$ is a complex number? I know that along the real line we have zeros along $k\pi$, where $k$ is an integer. But what about the rest of the plane? The taylor series:
$$
\sum_{n=0}^{\infty}(-1)^n \dfrac{z^{2n+1}}{(2n+1)!},
$$
doesn't really tell me that much. How do I find the other zeros?",['complex-analysis']
941051,Explanation of formula for distance between triangles in a triangular grid,"Lets say you have a triangle similar to the one below, with each triangle numbered $(N, i) $ where $N$ is the row number and $i$ is the position within the row. From any triangle, you are allowed to move to any triangle which shares an edge with her current triangle. This can be seen in the diagram: As an example, it takes 3 moves to get from (3, 3) to (2, 2). How would you find a formula to find the number of moves it takes to get from a triangle specified by the coordinates $(a_i, a_j)$ to another triangle specified by coordinates $(b_i, b_j)$? When looking for a solution to this problem online I found this monstrous formula: $$\left| a_{i}-b_{i} \right|\; +\; \left| \left( a_{i}-\left\lfloor \frac{a_{j}}{2} \right\rfloor \right)-\left( b_{i}-\left\lfloor \frac{b_{j}}{2} \right\rfloor \right) \right|\; +\; \left| \left\lfloor \frac{a_{j}+1}{2} \right\rfloor-\left\lfloor \frac{b_{j}+1}{2} \right\rfloor \right|$$ but I have no idea how they came up with this. I believe this is some variant of taxicab/manhatan geometry except with triangle instead of rectangles. Could someone please try to explain/prove why this formula is valid? I believe that the first addend, $|a_i - b_i|$ comes from the observation that to get from any triangle $(a_i, a_j)$ to $(b_i, b_j)$ you must make $|a_i - b_i|$ moves downward (or upward depending on the order). The other two addends combined seem to give the number of moves left/right (depending on the position), but I cannot figure out the intuition/logic behind why it works.","['geometry', 'coordinate-systems', 'combinatorics']"
941055,Proof of the uncountability of reals using the diagonal argument—problem?,"Consider a common proof of the uncountability of $(0,1]$, as presented here for example: We assume that the reals can be arranged in a sequence $x_k$, represent every number in $x_k$ by its nonterminating decimal expansion, use the diagonal argument to create a sequence of decimal digits that differs from the nonterminating decimal expansion of each number in $x_k$ and then claim that this sequence of digits so generated must be the decimal expansion of some number not in $x_k$. My problem is with this last step. Could it not be the case that the new sequence of digits is just the terminating decimal expansion of some number already in $x_k$? Would it not be more correct to use the sequence $x_k$ to generate a sequence of all (both terminating and non-terminating) decimal expansion of all numbers in $[0,1)$ and then apply the diagonal argument?","['elementary-set-theory', 'real-analysis']"
941066,Lie Group Decomposition as Semidirect Product of Connected and Discrete Groups,"I've believed for a long time that every Lie group can be decomposed as the semidirect product of a connected Lie group and a discrete Lie group. However, in this Math Overflow thread , it is mentioned that this is not true, and that there exist obstructions to this decomposition. Unfortunately, the level of complexity in the answers goes a bit over my head. This type of decomposition is also addressed in this thread . With this loss of faith in my intuition, I have three questions: Is it true that Lie groups do not necessarily decompose into the semidirect product of a connected group and a discrete group? If this decomposition doesn't always work, then is there a particularly nice argument as to why it doesn't? If this decomposition doesn't always work, then is there a nice example I can use to convince myself that my intuition is wrong? Edit : As much as I love Qiaochu Yuan's answer, which is thorough and (as usual) a role model among Stack Exchange answers, this is not what I'm looking for. I'm aware of the general extension problem, and that not every short exact sequence splits, but my intuition has led me to think that each Lie group can be decomposed this way. I will certainly play around with his outline for a construction, but I would much prefer a more concrete answer. Once again, thank you! Edit : As of yet, playing with the construction suggested by Qiaochu hasn't yielded anything interesting. This question is still open to answers!","['lie-groups', 'group-extensions', 'group-theory', 'semidirect-product']"
941123,Why consider square-integrable functions?,"Why are $L^2$ functions important? From reading around I have three hypotheses: they show up in QM (but, why?) they form an inner product space (but, is that a ""tight bound"" or is the class easily extended to a bigger inner-product space?) they represent ""finite energy"", which everything in the world obeys All I really know is I keep seeing the term ""square-integrable"" tossed around as if it's obviously important, obviously the condition one would want to impose, etc. But why?","['functional-analysis', 'integration']"
941150,What is the difference between independent and mutually exclusive events?,"Two events are mutually exclusive if they can't both happen. Independent events are events where knowledge of the probability of one doesn't change the probability of the other. Are these definitions correct? If possible, please give more than one example and counterexample.",['probability']
941165,For all $\xi \in \mathbb{C}$ we have $e^{-\pi\xi^2}=\int_{-\infty}^\infty \! e^{-\pi x^2}e^{2\pi ix\xi}\ \mathrm{d}x.$,"This is Exercise 2.4 in Stein & Shakarchi's Complex Analysis . Prove that for all $\xi \in \mathbb{C}$
$$e^{-\pi\xi^2}=\int_{-\infty}^\infty \! e^{-\pi x^2}e^{2\pi ix\xi}\ \mathrm{d}x.$$
They prove it for the real case, so I assume that I'm supposed to use that. All I can think to do is write $\xi=a+ib$, which gives a term $e^{2\pi i xa}$ in the integral which becomes 1, since the power is a real multiple of $2\pi$ and $i$. That leaves
$$e^{-\pi\xi^2}=\int_{-\infty}^\infty \! e^{-\pi x^2}e^{-2\pi xb}\ \mathrm{d}x$$
but I don't have any idea where to go from there, or if this is even a fruitful direction. Any hints would be appreciated.",['complex-analysis']
941172,"Integral $\int_0^\pi \frac{x\,\operatorname dx}{a^2\cos^2x+b^2\sin^2x}$","Integrate:
$$
\int_0^\pi \frac{x\,\operatorname dx}{a^2\cos^2x+b^2\sin^2x}
$$","['definite-integrals', 'trigonometry', 'integration']"
941176,Calc 2: Integration by Parts w/ trig identities,"$$\int e^{3\theta}\sec^4(e^{3\theta})\tan^{11}(e^{3\theta})d\theta$$
I just want to make sure that I'm doing this correctly so that I can understand the material. I would also appreciate any tips or shortcuts for this type of problem! So I started off with substitution:
$$x=e^{3\theta},\;dx=3e^{3\theta}d\theta,\;d\theta=\frac13\frac1xdx$$
To get:
$$\frac13\int\frac xx \sec^4x\tan^{11}xdx \Rightarrow 
\frac13\int\sec^4x\tan^{11}xdx$$
Since $\tan^{odd}$:
$$u=\sec x,\;du=\sec x\tan x\;dx$$ $$\frac13\int\sec^3x\tan^{10}x(\sec x\tan x)dx$$
$$\frac13\int u^3(u^2-1)^5du$$
$$\frac13\int\sec^3x\tan^{10}x(\sec x\tan x)dx$$
$$\frac13\bigg(\frac{1}{14}u^{14}-\frac{5}{12}u^{12}+u^{10}-\frac54 u^8+\frac56 u^{6}-\frac14 u^{4}\bigg)+C$$
$$\frac{1}{42}\sec(e^{3\theta})^{14}-\frac{5}{36}\sec(e^{3\theta})^{12}+\frac13\sec(e^{3\theta})^{10}-\frac{5}{12} \sec(e^{3\theta})^8+\frac{5}{18} \sec(e^{3\theta})^{6}-\frac{1}{12} \sec(e^{3\theta})^{4}+C$$ Is this correct?","['trigonometry', 'calculus', 'integration']"
941209,"Probability that the second-best player finishes second in a single-elimination tournament, given that better players always defeat weaker players?","A chess tournament (single-elimination format) has 16 players. Suppose that no two players have the same strength, and that each player always defeats the players weaker than himself/herself (i.e. no draws). The loser of the final round becomes the runner-up. What is the chance that the second-best player turns out to be the runner-up? What if there are $2^n$ players? I'm not sure how to approach this. Would it be correct to think that the the probability is $\frac{14}{15} \times \frac{6}{7} \times \frac{2}{3}$, since at each round, there is only one person who can cause the player not to advance, and the number of players in each round is halved? How then, would I approach the follow-up question, where I am supposed to answer this in the general case?","['probability', 'combinatorics']"
941222,Evaluating sums and integrals using Taylor's Theorem,"Taylor's theorem states that $$f(x)-\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k = \int_a^x \frac{f^{(n+1)} (t)}{n!} (x - t)^n \, dt $$ We can use this to evaluate integrals. For example, consider $f(x)=\frac{b!x^{b+n+1}}{(b+n+1)!}$. This has $f^{(k)}(0)=0$ for $k\leq n$ and $f^{(n+1)}(x)=x^b$. Hence, by Taylor's theorem, $$f(1)-\sum_{k=0}^n\frac{f^{(k)}(0)}{k!} = \int_0^1 \frac{f^{(n+1)} (t)}{n!} (1 - t)^n \, dt $$
$$\implies \frac{b!}{(b+n+1)!}=\frac1{n!}\int_0^1t^b(1-t)^n\,dt$$
$$\implies\beta(b+1,n+1)=\int_0^1t^b(1-t)^n\,dt=\frac{b!\, n!}{(b+n+1)!}$$ So we have determined an explicit expression for the Beta function very efficiently using Taylor's theorem. Taylor's theorem could also be used to evaluate partial sums using knowledge of the infinite sum; for example, considering $f(x)=\sum\limits_{k=0}^\infty kx^k=\frac x{(1-x)^2}=\frac1{(1-x)^2}-\frac1{1-x}$ for $|x|<1$, and noting that $f^{(n+1)}(x)=(n+2)!(1-x)^{-(n+3)}-(n+1)!(1-x)^{-(n+2)}$, we can evaluate its partial sum as $$\sum_{k=0}^nkx^k=\frac x{(1-x)^2}-(n+2)(n+1)\int_0^x(1-t)^{-(n+3)} (x - t)^n \, dt$$ $$-(n+1)\int_0^x(1-t)^{-(n+2)} (x - t)^n \, dt ={\frac {x ( 1-{x}^{n} ) }{ \left( 1-x \right) ^{2}}}-{
\frac {n{x}^{n+1}}{1-x}}
$$
after evaluating the integrals and simplifying. In fact, this holds for all $x$ except $x=1$, and if a limit towards $1$ is taken, we correctly obtain $\frac{n(n+1)}{2}$.
What other sort of sums and integrals can be evaluated using Taylor's Theorem? Is this an effective method for summing or integrating? Are there certain limitations we should be aware of with this method?","['power-series', 'calculus', 'integration', 'big-list', 'taylor-expansion']"
941233,Sketch curve $r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k}$,"This is from ""Multivariable Calculus, Concepts and Contexts"" by Stewart. He says ""The parametric equations for this curve are: $x=\cos t$ , $y=\sin t$ , $z=t$ . This makes sense. However, he then goes on to state that ""Since $x^2+y^2=\cos^2t+\sin^2t=1$ , the curve must lie on the circular cylinder $x^2+y^2=1$ . I understand the trigonometric identity, I just don't understand how we get to even consider $x^2+y^2=1$ since the vector function is defined as $r(t)=\cos t\hat{\bf i}+\sin t\hat{\bf j}+t\hat{\bf k}$ . Where does $x^2+y^2=1$ come from? How do we get to use that fact when $x=\cos t$ and $y=\sin t$ ?","['multivariable-calculus', 'parametric']"
941253,Finding all solutions of an expression and expressing them in the form $a+bi$,$$6x^2+12x+7=0$$ Steps I took: $$\frac { -12\pm \sqrt { 12^{ 2 }-4\cdot6\cdot7 }  }{ 12 } $$ $$\frac { -12\pm \sqrt { -24 }  }{ 12 } $$ $$\frac { -12\pm i\sqrt { 24 }  }{ 12 } $$ $$\frac { -12\pm 2i\sqrt { 6 }  }{ 12 } $$ I don't know where to go from here to arrive at the correct answer...,['algebra-precalculus']
941275,Method for deriving an Exponential Moving Average?,"I have a formula for an exponentially weighted moving average function defined recursively as: $S_t = a*Y_t+(1-a)*S_{t-1}$ Where: $a\in (0,1)\cap \mathbb{Q}$ $t$ represents time $Y_t$ is the value at a time period $t$ $S_t$ is the value of the EMA at any time period t. I am trying to find a generally applicable solution for the derivative of $S_t$ in regards to $dt$. Any assistance would be most appreciated!","['recurrence-relations', 'calculus', 'statistics', 'time-series', 'derivatives']"
941291,Inverse of a diagonal matrix plus a constant,"I am looking for an efficient solution for inverting a matrix of the following form: $$D+aP$$ where $D$ is a (full-rank) diagonal matrix, $a$ is a constant, and $P$ is an all-ones matrix. Inverse of constant matrix plus diagonal matrix gives a solution to the special case where all diagonal entries of $D$ are the same. The Sherman-Morrison formula is also capable of providing a solution to this problem by setting appropriate $u$ and $v$, but it loses efficiency ($O(n^3)$ time complexity to compute) at high dimension. I am hoping to get a result in the same form so the space and time complexity are both $O(n)$.","['numerical-linear-algebra', 'linear-algebra', 'inverse']"
941355,Exchange of volume between spheres until they no longer intersect,"Let's say we have two spheres (named $1$ & $2$) whose sizes are stored as radii ($r_1$ and $r_2$) with centres being $d$ distance apart. Now when the two spheres intersect (id est, $d < r_1 + r_2$) I want to 'drain' the volume of one sphere into the other precisely such that there is no intersect, yet no gap either (thus $d = r'_1 + r'_2$). The larger sphere will siphon from the smaller sphere. Let $1$ be larger; $v_1 > v_2$. My thought coming into this was that it would be simple, I'm just making one smaller, and one larger, but the larger one would have grown 'slower' because of it already being larger (probably due to the relationship between volume and radius, $v = \frac{4\pi r^3}{3}$). But I'm having trouble deducing the volume, $\Delta v$, that would be needed to shift between the spheres (which won't move during the instantaneous exchange) to influence the radii correctly. Given that the volume is 'traded' between the two ($v'_1 = v_1 + \Delta v$, $v'_2 = v_2 - \Delta v$), the sum volume of the two spheres will be conserved ($v_1 + v_2 = v'_1 + v'_2$). I'm writing the program such that the mathematical expression won't need to be robust enough to accept a lot of corner cases, so we can assume some things (like the centre of the smaller sphere will not be inside the larger sphere's bounds) if it helps. One of the things we tried (when we couldn't solve the cube-root-containing simultaneous equations we came up with) was finding the volume of the intersection , and just using that as $\Delta v$ (the resultant spheres still had radii that overlapped).","['geometry', 'algebra-precalculus']"
941361,Why can the meromorphic function only have finitely many poles in the complex plane?,There is a proof in Complex Analysis by Stein-Shakarchi that I do not understand. I have highlighted it in red. Why can they say that $f$ only has a finite number of points? What makes them able to exclude the countable number in the definition of a meromorphic function?,"['proof-explanation', 'complex-analysis']"
