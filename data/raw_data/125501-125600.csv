question_id,title,body,tags
1902184,Which entire functions $h(z)$ can be written as $h(z)=f(z+1)-f(z)$ for some entire function $f$? [duplicate],"This question already has answers here : Existence of an holomorphic function (2 answers) Closed 7 years ago . Question : For which entire functions $h(z)$ does there exist an entire function $f(z)$ such that $h(z)=f(z+1)-f(z)$? What I have tried : Suppose that $f:\mathbb{C}\to\mathbb{C}$ is an entire function, and let $\displaystyle f(z)=\sum_{n=0}^\infty a_nz^n$ be its Taylor series expansion.  Then $$\displaystyle f(z+1)=\sum_{n=0}^\infty a_n\sum_{i=0}^n\binom{n}{i}z^i=\sum_{n=0}^\infty z^n\left[\sum_{j=n}^\infty a_j\binom{j}{n}\right].$$ Therefore $$f(z+1)-f(z)=\sum_{n=0}^\infty z^n\left[\sum_{j=n+1}^\infty a_j\binom{j}{n}\right].$$ For $\{a_n\}\in\mathbb{C}^\infty$, define $c_n=\displaystyle\sum_{j=n+1}^\infty a_j\binom{j}{n}$, if this sequence converges.  If $\{a_n\}$ is a sequence for which each $c_n$ converges, then define $\Pi(\{a_n\})=\{c_n\}$. Lets let $\mathscr{H}$ denote the collection of all sequences of complex numbers $\{a_n\}$ such that $\Pi(\{a_n\})$ is well-defined.  Let $\mathscr{H}_e$ denote the collection of sequences such that $\displaystyle\sum_{n=0}^\infty a_nz^n$ has infinite radius of convergence.  It is not hard to see from the above work that $\mathscr{H}_e\subset\mathscr{H}$.  Let $\mathscr{H}_0\subset\mathscr{H}_e$ be the collection of finite sequences (ie those corresponding to polynomials). I know that $\Pi:\mathscr{H}_0\to\mathscr{H}_0$ is surjective.  I want to know what $\Pi(\mathscr{H}_e)$ is (I now know that $\Pi:\mathscr{H}_e\to\mathscr{H}_e$ is not surjective as noted in my comment below). Bonus question : If we mod out $\mathscr{H}$ by the relation $\{a_n\}\sim\{b_n\}$ if and only if $a_k=b_k$ for each $k>0$, then $\Pi:\mathscr{H}_0\to\mathscr{H}_0$ is injective.  Is $\Pi:\mathscr{H}\to\mathscr{H}$ injective when modded out similarly?","['complex-analysis', 'recurrence-relations', 'entire-functions', 'sequences-and-series']"
1902188,"Inverse of the sum $\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j$","$k\in\mathbb{N}$ The inverse of the sum $$b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j$$ is obviously 
$$a_k=\sum\limits_{j=1}^k \binom{k-1}{j-1}\frac{b_j}{k^j}$$ . How can one proof it (in a clear manner)? Thanks in advance. Background of the question: It’s  $$\sum\limits_{k=1}^\infty \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\sum\limits_{k=1}^\infty \frac{a_k}{k}$$  with $\,\displaystyle b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k}a_j $. Note: A special case is $\displaystyle a_k:=\frac{1}{k^n}$ with $n\in\mathbb{N}$ and therefore $\,\displaystyle b_k=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k-n}$ (see Stirling numbers of the second kind) 
$$\sum\limits_{k=1}^n \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\zeta(n+1)$$ and the invers equation can be found in A formula for $\int\limits_0^\infty (\frac{x}{e^x-1})^n dx$ .","['analytic-combinatorics', 'summation', 'binomial-coefficients', 'analysis']"
1902249,Discontinuous solutions of a linear ode with singularities,"I've came across the following ode in my studies
$$a(t)\dot{x}(t) = A(t)x(t),$$
where $x\in \mathbb{R}^m$, $A(t),a(t)$ are analytic and $a(t) \geq 0$. I would like to understand what happens to a solution when it passes through a moment $t_0$ when $a(t_0) = 0$. In particular for other reasons it seems that there might be a weak solution with a jump discontinuity, but I am not able to verify this directly. So I was wondering, if maybe someone stumbled upon this type of ode's and could give me some references about known results for this type of equations. Or maybe some advise on how one could study it's solutions. Thank you.",['ordinary-differential-equations']
1902300,"With regards to measure theory, why do we use different differentials in the integrand?","I remember my lecturer telling me about the importance of measure theory once, where she asked why do we use differentials such as $dx, dF, dP$ and $d\mu$ in an integrand - all different depending on what we want. I was told to look up why, and now that I am self-teaching myself measure theory I am very much interested why. There is a very good answer to the case of using $dx$, but why is it that we use different differentials, and how do we know which to use when computing integrals? Note : I have so far only learnt about $dx$ and the basics of integrating with respect to a measure $\mu$. I think I am right in saying $dP$ and $dF$ are used for distributions. Are there other examples you may also know of that may be interesting as well?","['integration', 'measure-theory', 'calculus']"
1902318,Equivalence of two definitions of Closed Immersion,"I would like to prove the equivalence of the following two definitions (this question is mostly proof verification) Let $(f:X\to Y, f^\sharp: \mathscr{O}_Y\to f_*\mathscr{O}_X)$ be a
  morphism of schemes. Then the following are equivalent $f:X\to Y$ induces a homeomorphism between $X$ and a closed subset of $Y$ and furthermore $f^\sharp: \mathscr{O}_Y\to f_*\mathscr{O}_X$
  is surjective (epimorphism). $(f,f^\sharp)$ is an affine morphism, and given any open affine subset $\mathrm{Spec} B\subset Y$ with $f^{-1}(\mathrm{Spec
B}):=\mathrm{Spec A}\subset X$ affine open, the canonical ring
  homomorphism $f^\sharp_B:B\to A$ is surjective. A morphism satisfying one (and therefore both) properties is called a closed immersion . I want to make sure I got it right. Here is a sketch of proof: ($2\to 1$) $(f,f^\sharp)$ with def. 2 is a finite morphism, hence $f$ is closed. Given any surjection of ring $B\to A$, there is bijection $\mathrm{Spec}A\simeq \mathrm{Spec}B/I$ for some ideal. This means $f$ is also one-to-one. Hence the induced map $X\to f(X)$ is a continuous, closed, bijection and hence a homeomorphism. Moreover if $f_B^\sharp$ is surjective, then the induced map on stalks is the colimit of a bunch of surjections and hence is itself a surjection. ($1\to 2$) I think this is essentially a consequence of Exercise 3.11 (a,b) of Hartshorne. Part (a) says that closed immersions (def. 1) are stable under base change. Part (b) says a closed subscheme (def. 1) of an affine scheme is affine. Combining the two we find that $(f,f^\sharp)$ is an affine morphism. To show that $f^\sharp_B: B\to A$ is surjective, we use the fact that a homomorphism of $B$-modules $M\to N$ is a surjection iff for all prime ideals $\mathfrak{p}\subset B$, the induced maps $M_\mathfrak{p}\to N_{\mathfrak{p}}$ are surjective. This together with the fact that $f^\sharp_y$ are surjective for all $y\in \mathrm{Spec}B$ should finish the job.","['algebraic-geometry', 'proof-verification']"
1902326,Calculating Residues around given poles using limit fomula,"i am having an issue with a sample question I have been given and I cannot figure out the correct way of dealing with the question. I have to calculate the following residue of $\displaystyle\frac{\cos(z^3) - 4e^z}{z^8 - z}$ at $z = 0$ So, usually I would use the limit formula like this one: Linky but I do not know what the order of the function is, i guess that is my main problem.  I know 7 is incorrect but how do I go about finding the order? Thanks","['complex-analysis', 'residue-calculus']"
1902330,Solutions of $X^2+2=Y^3$ in $\mathbb{Z}[i]$,"The Mathematician James T. Cross, on his work ""Primitive Pythagorean Triples of Gaussian Integers"", displayed a method
for generating all Pythagorean triples in $\mathbb{Z}[i]$. Inspired by his work, I want to find all the gaussian integer solutions of the equation $X^2+2=Y^3$. In $\mathbb{Z}$, the only solution is $x=5,y=3$ or $x=-5,y=3$. A trivial solution in $\mathbb{Z}[i]$ is $x=i$, $y=1$. My question is: Any other mathematician solved this problem any time before?","['number-theory', 'reference-request', 'gaussian-integers']"
1902343,Short proof for the determinant of a $4$ by $4$ matrix,"Prove that $\det \begin{bmatrix}x&y&z&t\\-y&x&-t&z\\-z&t&x&-y\\-t&-z&y&x\end{bmatrix} = (x^2+y^2+z^2+t^2)^2$ I'm looking for an elegant proof that doesn't involve bruteforce. Since the answer is given, I'm thinking we can argue that the determinant here is a homogeneous polynomial $P(x,y,z,t)$ with degree $4$, that is invariant under $x\to -x$ and permutations of $x,y,z,t$. As a result, $P(x,y,z,t) = \lambda (x^4+y^4+z^4+t^4) + \delta (x^2y^2+x^2z^2 + x^2t^2+y^2z^2 + y^2t^2 + z^2t^2)$ $\lambda$ and $\delta$ can be found by computing $P(0,0,0,1)$ or some such. The problem is, it doesn't look easy to prove that $P$ doesn't change under permutation of $x,y,z,t$, neither that it's invariant when the variables are negated. Can you suggest another short proof, or prove the two claims above ?","['polynomials', 'alternative-proof', 'determinant', 'proof-writing', 'linear-algebra']"
1902358,"Why does a Gorenstein isolated three-fold singularity have a canonical (3, 0) form on the singularity?","I'm trying to read a physics paper, and when talking about rational, graded, Gorenstein, isolated three-fold singularities they say: ""Here graded means that the singularity should have a  $\mathbb{C}^*$ action, and Gorenstein means that there is a canonical well defined $(3, 0)$ form on the singularity, finally rational means that the weights of the $(3, 0)$ form under the $\mathbb{C}^∗$ action is positive."" I'm struggling to see how these conditions follow from the definitions of rational and Gorenstein that I know and love. I know that the canonical sheaf is invertible on a Gorenstein variety; but I don't see how that gives a $(3, 0)$ form.","['string-theory', 'sheaf-theory', 'mathematical-physics', 'algebraic-geometry']"
1902363,Test of being a rational number for $(1-\frac13+\frac15-\frac17+\cdots)/(1+\frac14+\frac19+\frac1{16}+\cdots)$,"Is the following expression a rational number? $$\frac{1-\dfrac13+\dfrac15-\dfrac17+\cdots}{1+\dfrac14+\dfrac19+\dfrac1{16}+\cdots}$$ My thoughts: The sum and product of two rational numbers is a rational number. So is the difference. As well as the quotient, when the denominator is not zero. However, the answer key says that this is not a rational number. Could anyone help me understand why this is not a rational number?","['algebra-precalculus', 'rationality-testing', 'rational-numbers', 'sequences-and-series']"
1902365,"Ist it possible to ""write out"" f(x) = sin(x) or other trigonomical functions [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Today I had a discussion with a friend about trigonometric functions. We were stuck at a function where we couldn't extract $x$ from a trigonometric function, like (oversimplifying) $$
(x-4)\cdot(x-2)=\sin\left(x\cdot\frac{\pi}{2}\right) 
$$
I know this question can't be solved by analytic methods, but we asked us why. If $\sin$ is just another function (like our professor told us) why can't we get the instructions of the function and apply them to $x$? To make it clearer: if we have the function $g(x) = x + 2$ we could write 
$52 + g(2) = 56$ or $52 + (2 + 2)= 56$, both equations are identical. But if we can do this why can we only write $52 + sin(90) = 52,89\dots$ and not $$ 
52 + \text{insert some instructions based on a pattern here} = 52,89\dots
$$ Thanks to everybody in advance.","['trigonometry', 'functions']"
1902376,On calculations from $\zeta(3)=\frac{2}{\pi}\sum_{n=1}^\infty\int_0^\infty \frac{\sin ((n+1)x)\sin (nx)}{(xn^2)^2}dx$,"I was inspired in a formula that I've found in Internet, page 5 of Jameson 's notes about Frullani integrals , to ask to Wolfram Alpha this integral $$\int_0^\infty\frac{\sin ax\sin bx}{x^2}dx=\frac{\pi}{4} \left(  \left| a+b \right|- \left| a+b \right|\right), $$
that I presume well known. Then I did the specialisation $a=n+1$, $b=n$ for a fixed integer $n\geq 1$ and after I've multiplied by $\frac{1}{n^4}$ one has if there are no mistakes $$\zeta(3)=\frac{2}{\pi}\sum_{n=1}^\infty\int_0^\infty \frac{\sin ((n+1)x)\sin (nx)}{(xn^2)^2}dx.$$ My goal is learn more mathematics to encourage myself to study more. Question. It's possible do more interesting calculations with nice mathematical content to deduce some identity from previous approach/identity? Thanks in advance. My attempt was that I know that it is possible to ask to a CAS by the series $\sum_{n=1}^\infty\frac{\sin ((n+1)x)\sin (nx)}{n^4}$, but I don't know cvery well what's means the result. Also I know that I can do the change $y=xn^2$ of variable in previous integral to get it as $$\int_0^\infty \frac{\sin (\frac{n+1}{n^2}y)\sin (\frac{y}{n})}{(yn)^2}dy.$$
But neither I don't know if such change of variables will be useful.","['special-functions', 'integration', 'sequences-and-series', 'trigonometric-series']"
1902421,Prove that the trace of the matrix product $U'AU$ is maximized by setting $U$'s columns to $A$'s eigenvectors,"I have an expression which I want to maximize:
$$\mbox{trace} (U^T\Sigma U)$$ $\Sigma \in R^{d \times d}$ is symmetric and positive definite $U \in R^{d \times r}$ its columns are orthonormal. I am following a proof which states that U is optimal when its columns are the eigenvectors of $\Sigma$: let $\Sigma = V\Delta V^T$, the eigen-decomposition; let $B=V^TU$ then:
$$U=VB$$
$$U^T\Sigma U= B^TV^TV\Delta V^TVB = B^T\Delta B$$
the last statement is mathematically correct. However, how does this proof to me that the columns of U are the eigenvectors of $\Sigma$? For some background: I am trying to understand a derivation of PCA. The point is that when this trace is maximized the objective function will be minimized.","['eigenvalues-eigenvectors', 'linear-algebra']"
1902438,"Solve the following integral: $\int_{x_0}^\infty \log(x) e^{-(x-a)^2/b}dx \quad x_0,b>0$","I want to compute this integral preferably in closed-form without expanding the $\log$ function; however, efficiently computable approximations might also solve my problem: $\int_{x_0}^\infty \log(x) e^{-\frac{(x-a)^2}{b}}dx \quad  x_0,b>0$ Background:
I am trying to compute the expectation of a logarithmic function of random variables from a truncated normal distribution: $\mathbb{E}_{x\sim \mathcal{N}(\mu,\sigma^2|x\geq x_0>0)}[\log(x)]$ Equivalently, I need to solve the follwing integral: $\int_{x_0}^\infty \log(x) \frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma^2\sqrt{2\pi}(1-\Phi(\frac{x_0-\mu}{\sigma}))}dx=\frac{1}{\sigma^2\sqrt{2\pi}(1-\Phi(\frac{x_0-\mu}{\sigma}))}\int_{x_0}^\infty \log(x) e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx$ I'm stuck here. Thanks for any help.","['conditional-expectation', 'expectation', 'integration', 'normal-distribution']"
1902453,Square root of a matrix.,"Determine all $A,B \in \mathbf{M}_{2}(\mathbf{R})$ such that $A^2+B^2=\begin{pmatrix} 22 & 44\\  
14 & 28 \end{pmatrix}$ and $AB+BA=\begin{pmatrix} 10 & 20\\  
2 & 4 \end{pmatrix}$. I have tried to assume that $A=\begin{pmatrix} a & b\\  
c & d \end{pmatrix}$ and $B=\begin{pmatrix} e & f\\  
g & h \end{pmatrix}$. And we know that $(A+B)^{2}=A^{2}+B^{2}+AB+BA$ and $(A-B)^{2}=A^{2}+B^{2}-(AB+BA)$. Then, I will end up with $8$ equations and $8$ which can be solve, but it is really complicated. I am just wondering if someone could show me how to take the square root of an matrix. Please if you show that please make sure that you will prove it. Would any one have an idea what is the input code for solving that in MATLAB",['matrices']
1902462,Using 8x8 Binary Matrices as a hash,"I had the idea of computing a 64 bit hash of a text string by assigning a unique binary 8x8 matrix to each character, and computing the hashes of larger strings by multiplying the matrices corresponding to the substrings.  In this system both addition and multiplication of matrix elements would be modulo 2.  If this works, it would have the benefit that the hash of two concatenated strings would be the product of their hashes. This method would completely fall apart if some matrices were much more likely to turn up than others, if a large subset of the space of matrices were unusable, or if there were a risk of generating a degenerate matrix from the product of two non-degenerate matrices, causing the whole product to collapse.  As a result, I have a few questions. 1) What percentage of the space of 64 bit values are usable as hashes, i.e. the matrix they represent does not have determinant zero? 2) Given two matrices chosen randomly from the set of usable hashes, is their product equally likely to generate any other matrix in the usable set, or are some products more likely than others? 3) Will the determinant of the product of two matrices with non-zero determinants ever be zero? EDIT: My initial idea was to turn each 8 bit character with bits $abcdefgh$ into the following matrix: $\begin{bmatrix}
a \oplus h & a \oplus g & a \oplus f & a \oplus e & a \oplus d & a \oplus c & a \oplus b & 1 \\
b \oplus h & b \oplus g & b \oplus f & b \oplus e & b \oplus d & b \oplus c & 1 & 0 \\
c \oplus h & c \oplus g & c \oplus f & c \oplus e & c \oplus d & 1 & 0 & 0 \\
d \oplus h & d \oplus g & d \oplus f & d \oplus e & 1 & 0 & 0 & 0 \\
e \oplus h & e \oplus g & e \oplus f & 1 & 0 & 0 & 0 & 0 \\
f \oplus h & f \oplus g & 1 & 0 & 0 & 0 & 0 & 0 \\
g \oplus h & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}$","['binary', 'probability', 'linear-algebra']"
1902522,Generate function from lookup table?,"I feel like this question must have been asked before, but I'm just not able to find it. Probably because I just don't know the terminology of my issue... My question is really simple. I have the following lookup table (is that what it's called?): Voltage (V) | Battery state of charge (%)
< 53.0      | 0
53.0        | 10
54.1        | 20
56.2        | 30
57.7        | 40
59.1        | 50
60.5        | 60
61.7        | 70
62.8        | 80
63.9        | 90
65          | 100 and I want to create a function that can return an approximation for the ""Battery state of charge"" for any Voltage value between 65 and 53. For example 64.45 could return 95%. Note: I'm not looking for a 100% perfect solution here, as I probably wouldn't understand it anyway.",['functions']
1902526,Find n-th derivative of $f(x)=\sqrt{1+\cosh{x}}$,"$f(x)=\sqrt{1+\cosh{x}}$ $f'(x)=\frac{\sinh{x}}{2\sqrt{\cosh{x}+1}}$ $f''(x)=\frac{(\cosh{\frac{x}{2}})^4}{(\cosh{x}+1)^\frac{3}{2}}$ I wanted to use Leibnitz rule, but i can't seem to find any similarities between derivatives. I don't know how to expand $\cosh{x}$ as Taylor series. These are the only two general methods i know for finding upper derivatives. I think there must be some trick. Anyone has any ideas?
This is what i got so far. $$f(x)=\frac{\sum_{n=0}^\infty ( \frac{(\frac{x}{2})^n}{n!}+\frac{(\frac{-x}{2})^n}{n!})}{\sqrt{2}}=\frac{\sum_{k=0}^\infty \frac{2(\frac{x}{2})^{2k}}{(2k)!}}{\sqrt{2}}$$ Dunno what to do now New approach. $f(x)=\sqrt{2}\cosh{\frac{x}{2}}$ $f'(x)=\frac{\sqrt{2}}{2}\sinh{\frac{x}{2}}$ $f''(x)=\frac{\sqrt{2}}{4}\cosh{\frac{x}{2}}=\frac{1}{4}f(x)$ For $n$ even $f^{(n)}=\frac{1}{2^n}f(x)=\frac{\sqrt{2}}{2^n}\cosh{\frac{x}{2}}$ For $n$ odd $f^{(n)}=\frac{\sqrt{2}}{2^n}\sinh{\frac{x}{2}}$","['derivatives', 'real-analysis']"
1902531,Why does the sequence of functions $f_n(x)=nx^n(1-x)$ not converge uniformly?,"Let $(f_n)_n$ be a sequence of functions defined by $$f_n : [0,1] \rightarrow \mathbb{R}: x \mapsto nx^n(1-x). $$ We have $\lim_{n \to \infty} f_n(x) = 0$ for all $x$. My textbook says the convergence isn't uniformly though, and I don't understand why. I computed $$\sup_{x \in [0,1]} |f_n(x) - 0 | = \sup_{x \in [0,1]} | nx^n (1-x) | = \ ? $$ 
How do I figure out this supremum? When I use the definition, I see that $$| nx^n(1-x) - 0 | = | nx^n - nx^{n+1}| \leq | nx^n | + | nx^{n+1}| \leq n + n = 2n. $$ And I can never get this smaller than $\epsilon$. But I couldn't find an explicit $\epsilon > 0$ and a $x \in [0,1]$ such that $ |f_n(x)| \geq \epsilon. $ Any help is appreciated.","['real-analysis', 'sequences-and-series', 'uniform-convergence']"
1902532,Is there a name for this technique?,"I have to assume this is fairly common, but I'd like to know if there is a term for this technique, and what branch of mathematics it is a part of. (For reference, I have no formal training, but fell into a set of problems I need to solve, and I'm trying to understand the landscape.) Forgive me if I'm expressing this poorly, and feel free to correct my notation and terminology: For a set of numbers $1,2,3,4,$ where $0<n<5$ $4 + n = n$ and $1 - n = 5 - n$ I understand this is analogous to a 2-bit register, and there's a problem involving reduction of symmetries in a 2^2 Latin square where this proved quite handy.","['terminology', 'discrete-mathematics', 'notation', 'elementary-number-theory']"
1902540,Calculate the limits of the sequence $\frac{2}{n+2}$ from first principles,i know that it can then be simplified by saying $\frac{2}{n+2}$ < $\frac{2}{n}$ but then would it just continue as normal saying that you would then choose an integer $N(\epsilon)$ within the set of natural numbers such that $$N(\epsilon) > \frac{2}{\epsilon} $$ possible by Archimedes principle. if someone could talk me through this it would be much appreciated.,"['epsilon-delta', 'sequences-and-series', 'calculus', 'limits']"
1902582,Is a quadratic differential literally a square of a differential?,"I have seen definitions of quadratic differentials as functions $\omega : TS \to \mathbb{C}$ that satisfy $\omega(\lambda v) = \lambda^2 \omega(v)$, and that we can write them as $\omega = f_i dz_i^2$ where $f_i = f_j \left(\frac{dz_i}{dz_j}\right)^2$. Does that mean that the $dz^2$ is literally the square of $dz$? I.e. that for a point $p \in S$ and $v \in T_pS$ that $(dz^2)|_p(v) = (dz|_p(v))^2$? As such, those quadratic differentials would satisfy the square of the scalar multiple thing, along with $$dz^2(v + w) = dz^2(v) + 2 dz(v) dz(w) + dz^2(w).$$","['riemann-surfaces', 'complex-analysis', 'differential-forms']"
1902603,Discrete uniform probability on a sample space of prime cardinality,"My question is to show that if I have a fair die with $p$ faces, where $p$ is prime, and the experiment consists of rolling it once, no two proper events can be independent. Here is my approach:
Suppose $A$ and $B$ are independent events. Then $$P(A \cap B) = P(A) * P(B) = \frac{|A|}{p} * \frac{|B|}{p} = \frac{|A||B|}{p^{2}} < 1$$
Not sure where a contradiction would happen to know how to proceed.
Any help?","['probability-theory', 'probability']"
1902640,Does absolutely continuity of probability measures imply absolute continuity of conditional probability measures almost everywhere?,"I have a Polish space $X$ equipped with a Borel probability measure $\mu$. I have another Polish space $Y$ and a continuous function $f : X \rightarrow Y$. I give $Y$ the pushforward probability measure $\nu(\cdot) = \mu\left(f^{-1}(\cdot)\right)$. Now suppose I have another Borel probability measure $\mu'$ on $X$ such that $\mu \ll \mu'$ and $\mu\left(f^{-1}(\cdot)\right) = \mu'\left(f^{-1}(\cdot)\right)$. Is it true that $\mu_y \ll \mu_y'$ $\nu$-almost everywhere? Where $\mu_y$ and $\mu_y'$ are the conditional probability measures of $\mu$ and $\mu'$. So far I have tried the following : We know $\mu(A) > 0  \Rightarrow \mu'(A) > 0$ for all measurable set $A \subset X$. Let $Y_A := \{ y \in Y \; | \; \mu_y'(A) > 0\} \cup \{ y \in Y \; | \; \mu_y(A) = 0 \}$. Pick $A \subset X$ such that $\mu(A) > 0$, then $\mu'(A) = \int_{Y} \mu'_y(A) d\nu > 0$ (by the disintegration theorem) so we know $\nu(Y_A) = 1$. I was then able to show
\begin{align*} \nu\left(\bigcap_{A \subset X\text{ with non-emptry interior }} Y_A \right) = 1.
\end{align*} but I am still short of showing \begin{align*} \nu\left(\bigcap_{A\subset X \text{ measurable}} Y_A \right) = 1.
\end{align*} I have solved it for $Y$ countable, but it would be great to have the general case.","['metric-spaces', 'probability', 'measure-theory']"
1902647,How do I find the original paper of each famous theorem?,"Lately, I writing some essays whose topics are mathematics-heavy. Even though they are not research papers and will never be published, I just want to give proper references to each famous theorem/ideas. However, finding the original source of each theorem proves to be a much more difficult task than I thought. This brings me to my question How, in general, do I find the original papers of each famous theorem? Specifically, how do I find Caratheodory's paper on the extension theory that now bare his name?","['reference-request', 'book-recommendation', 'measure-theory', 'soft-question']"
1902737,"Is there an alternative definition of uniform differentiability that implies differentiability, instead of requiring it?","Let $I \subseteq \mathbb{R}$ be an interval and let $f: I \to \mathbb{R}$. Let's start looking at the definitions of continuity, uniform continuity and differentiability. Yes, I really like sentences in the language of set theory . Definition 1: $f$ is continuous at $t \in I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 2: $f$ is continuous on $I$ if $$\forall \, t \in I \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 3: $f$ is uniformly continuous on $I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ or, put more nicely, $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \quad \bigg[ \quad |x-y|<\delta \implies |f(x)-f(t)| < \varepsilon \quad \bigg]$$ Definition 4: $f$ is differentiable at $t \in I$ if $$\exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Definition 5: $f$ is differentiable on $I$ if $$\forall \, t \in I \quad \exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Now, note that in the definition of uniform continuity, we did not require the function to be continuous; instead, our definition implies it (quite trivially!). Now let's proceed to the most common definition 1 of uniform differentiability: Definition 6: $f$ is uniformly differentiable on $I$ if $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - f'(t)\,\right| < \varepsilon$$ or, equivalently, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $\quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0$
$$\quad \forall \, t \in I \quad \forall \, h \quad \bigg[ \enspace \big(\,\, 0 < |h| < \delta \,\,\big) \land \big(\,\, t+h \in I \,\,\big) \implies \left|\,\dfrac{f(t+h)-f(t)}{h} - f'(t)\,\right| < \varepsilon \enspace \bigg]$$ or, as I prefer, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \enspace \bigg[ \enspace 0 < |x-y| < \delta \implies \left|\,\dfrac{f(x)-f(y)}{x-y} - f'(x)\,\right| < \varepsilon \enspace \bigg]$$ The given definition of uniform differentiability requires differentiability! (And as it stands it is indeed required because we refer to $f'(t)$ inside the definition) Is there a way to ""fix"" this? In other words: Is there an alternative definition of uniform differentiability, that does not require differentiability, and follows the same set-theory, $\varepsilon$-$\delta$ theme of the definitions 1 to 5? The problems I encountered in my attempt: I tried to alter the given Definition 6, to include the quantifier $\exists \, v \in \mathbb{R}$, and then use $v$ in place of $f'(t)$, but I couldn't make this work, because there seems to be a unbreakable loop : I need the ""$\forall \, t$"" somewhere $v$ clearly depends on $t$, so the ""$\exists \, v$"" would have to be placed to the right of ""$\forall \, t$"" $\delta$ cannot depend on $t$, so the ""$\exists \, \delta$"" would have to be placed to the left of ""$\forall \, t$"" But $v$ cannod depend on $\delta$ (and therefore on $\varepsilon$), since $v$ is some sort of limit, which means the same $v$ must work for all epsilons! Then ""$\exists \, v$"" would have to be placed to the left of ""$\exists \, \delta$"", which is impossible given the above considerations! After thinking about this, it seemed to me that I would need two different epsilons and two different deltas inside the definition, but that looks too ugly, and too close to simply replacing the sentence ""$f \,\,\text{is differentiable on} \,\,I$"" by its set-theory equivalent, which is clearly not what I want here. Then I got stuck. 1 the three forms presented in Definition 6 were manipulated and ""converted to the language of set theory"" by me; but are grounded on many other posts on Math.SE such as here , here , here and here .","['derivatives', 'real-analysis', 'epsilon-delta', 'logic', 'uniform-continuity']"
1902741,Does any homomorphism map generators to generators?,"I think similar questions may have been asked, so apologies if this is the case. I haven't been able to find one that helps me yet though. Anyway, for an assignment question I've figured out a solution but it relies on my assumption that an isomorphism between cyclic groups maps generators to generators. I've seen this stated before but I'm not quite sure how to prove it. Could anyone help me? I attempted using the idea that the identity gets mapped to the identity and trying to generalise to generators but got stuck. Thanks in advance!","['abstract-algebra', 'group-theory', 'group-isomorphism']"
1902794,A set with measure $0$ has a translate containing no rational number.,"Suppose $E$ is a set with measure $0$ . Show there exists $t\in \mathbb{R}$ such that $E+t$ contains no rational number. My idea is to find an interval in $E$ , then we can get a contradiction. I try to begin with a point in $E$ and then consider if there is an interval containing this point in $E$ . But I don't know how to start. Maybe, we can go by contradiction.  If $E+t$ contains a rational number $q_t$ for every $t\in \Bbb R$ , then we have a function $f:\mathbb{R}\to\mathbb{Q}$ , $t\mapsto q_t$ .  But this idea leads nowhere.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'rational-numbers']"
1902847,Poincaré conjecture for positively curved Thurston geometries,"The passages below are from Terence Tao's blog . I don't know how to get Corollary 1 from Exercises 2 and 3. I understand that the spherical space form (as the metric) is invariant under $SO(n)$, where $n$ is one plus the dimension of the manifold. Exercise 2 . Let $M$, $M'$ be connected manifolds of the same dimension. Show that $M\#M'$ is compact if and only if $M$ and $M'$ are both compact. Show that $M\#M'$ is orientable if and only if $M$ and $M'$ are both orientable. Show that $M\#M'$ is simply connected if and only if $M$ and $M'$ are both simply connected. The sphere also plays a special role, as the identity for the connected sum operation: Exercise 3 . Let $M$ be a connected manifold, and let $S$ be a sphere of the same dimension. Show that $M\#S$ (or $S\#M$) is homeomorphic to $M$. $\lozenge$ Recall that of the spherical space forms and $S^2$-bundles over $S^1$ mentioned above, the sphere $S^3$ was the only one which was simply connected. From Exercises 2 and 3 we thus have Corollary 1 . (Poincaré conjecture for positively curved Thurston geometries) Let $M$ be a simply connected 3-manifold which is the connected sum of finitely many spherical space forms and $S^2$-bundles over $S^1$. Then $M$ is homeomorphic to the sphere $S^3$. ( pic 1 , pic 2 )","['geometric-topology', 'riemannian-geometry', 'differential-geometry', 'curvature']"
1902850,Efficient algorithm to find all unique combinations of set given duplicates,"The number of combinations for a 4 choose 2 is 6. But what if I have a set in which 2 of the elements are the same? Of course I could do a unique check for each item but this is too computationally expensive with larger sets (as is calculating all the possible combinations in the first place). Knowing that my sets have many duplicates, I am trying to find an algorithm that can take advantage of this to reduce the amount of work it takes to generate all the combinations I care about. Normally, [1',1'',2,3] choose 2 yields [1',1''] [1',2] [1',3] [1'',2] [1'',3] [2,3] . I know though that [1',2] == [1'',2] and [1',3] == [1'',3] so the number of combinations I am interested in is only 4. To be clear, I am trying to generate the actual unique combinations, not just the number of them that exist, and without having to check each generated set against previous sets. Update Here is a working implementation in javascript using the lodash utility library for convenience: let _ = require('lodash');
let _items_1 = ['A','A','A','B','B','C'];
let items_1 = {'A':3, 'B':2, 'C':1};
function UniqueCombinations(set, n) {
    let combinations = [];
    let props = Object.getOwnPropertyNames(set);
    for (let p = 0; p < props.length; p++) {
        for (let i = Math.min(set[props[p]], n); i > 0 ; i--) {
            // if (_.sum(_.values(_.pick(set, props.slice(p+1)))) < n-i) continue;
            if (n-i > 0) {
                let rest = UniqueCombinations(_.pick(set, props.slice(p+1)), n-i)
                for (let c = 0; c < rest.length; c++) {
                    let combination = {};
                    if (i > 0) combination[props[p]] = i;
                    Object.assign(combination, rest[c]);
                    combinations.push(combination);
                }
            } else {
                let combination = {};
                combination[props[p]] = i;
                combinations.push(combination);
            }
        }
    }
    return combinations;
}
let combinations = UniqueCombinations(items_1, 3);
console.log(combinations);
combinations.forEach(combination => console.log(_.transform(combination, function(result, value, key) {
    result.push(...key.repeat(value).split(''));
}, []))); [ { A: 3 },
    { A: 2, B: 1 },
    { A: 2, C: 1 },
    { A: 1, B: 2 },
    { A: 1, B: 1, C: 1 },
    { B: 2, C: 1 } ] [ 'A', 'A', 'A' ]
  [ 'A', 'A', 'B' ]
  [ 'A', 'A', 'C' ]
  [ 'A', 'B', 'B' ]
  [ 'A', 'B', 'C' ]
  [ 'B', 'B', 'C' ] This is by no means the cleanest or most performant implementation but a fine starting place for anyone else looking to do this.","['combinatorics', 'algorithms']"
1902861,Inverse Gaussian maximum likelihood estimation lambda,"For a regular IG($\mu$,$\lambda$) with pdf:
   $f(x;\mu,\lambda) = \frac{\lambda}{2\pi x^3}^{1/2}$
$e^{\frac{-\lambda}{2\mu^2}\frac{(x-\mu)^2}{x}}$ Taking the log likelihood to produce $\frac{n}{2}Ln(\lambda)+ \frac{n}{2}Ln(1/2\pi) + K - \frac{\lambda}{2\mu} \sum_1^n{\frac{(x_i-\mu)^2}{x_i}}$,
k some term not in $\mu \,or \lambda$ Proceeding to differentiate the one relevant term at the end I get that $\sum_1^n{\frac{(x_i-mu)^2}{x_i}}$ = 0 If this is correct, how does this imply that $\mu$ = sample mean? What about the second parameter $\lambda$?
It should reduce to $\lambda hat = \frac{1}{n}\sum_1^n{(\frac{1}{x_i} - \frac{1}{\mu^2})}$ Where $\mu$ is the estimated $\mu$ just shown.","['maximum-likelihood', 'statistics', 'calculus']"
1902897,Birthday Problem: Why isn't the probability 253/365,"Consider a set of $23$ unrelated people. Because each 23  pair of people shares the same birthday with probability $1/365$, and there are $\binom{23}2 = 253$ pairs, why isn’t the probability that at least two people have the same birthday equal to $253/365$?","['birthday', 'probability-theory', 'probability', 'problem-solving']"
1903013,Quadratic and Greatest Integer Function,"If both roots of the quadratic equation $x^2 - 2ax + a^2 - 1$ lie in $(-2, 2)$ then which of the following can be $[a]$ ? $[a]$ denotes greatest Integer function of $a$ $$A. -1$$ $$B. 1$$ $$C. 2$$ $$D. 3$$ I have solve it using graphs: The graph will intersect the x-axis somewhere between $-2$ and $2$. Hence we can conclude that $f(2)$ and $f(-2)$ will be greater than zero. Now there will be two quadratic equation in $a$ and we will get $4$ values of $a$ I.e. $-3, -1 , 3 ,1$. From these values the answer should be opton $D.$ but the answer is given as option $A.$ Kindly help.","['functions', 'quadratics']"
1903041,Constrution of a particular topological space,"Let $n \in \mathbb N$ be a fixed natural number. Does there always exists a topological space $(X, \tau)$ such that $\vert \tau \vert=n$ ? I am interested in both cases when the cardinality of $X$ is finite and cardinality of $X$ in infinite? Its is clear that if $n=2^k$ then we can easily construct required topological spaces in which cardinality of $X$ is $k$ and cardinality of $\tau$ is $2^k$. It is also clear that the above fact is true for numbers other than $2^k$. For example, Sierpinski Space . But I am unable to see the other cases? P.S: The above question is motivated from this question of Measure Theory about cardinality of sigma algebra.",['general-topology']
1903057,Calculating a derivative of (2015)-th order of function $f(x)$,"So I have
$\displaystyle{\,\mathrm{f}\left(\, x\, \right) =
\frac{x - 1}{\,\sqrt{\, 3 - x\,}\,}}$ And I want to calculate $\,\mathrm{f}^{\mathrm{\left(\, 2015\, \right)}}
\,\left(\, 1\,\right)$ So I got the first and second derivative:
$$\mathrm{f}'(x)=\frac{-x+5}{2[(-x+3)^{\frac{3}{2}}]}$$
$$ \mathrm{f}''(x)= \frac{-2(-x+3)^{\frac{3}{2}} + (-x+5)\sqrt{-x+3}}{4(-x+3)^3}$$ Perhaps I should look at some of the next derivatives for pattern? or is that not going to help, there is also a formula I know for getting a high order derivative, but it's not going to help if I don't calculate all 2014 of them as well...
So any help with this would be appreciated.","['derivatives', 'calculus']"
1903071,"Permutations of $\{1,\ldots,2pq\}$ modulo $2pq$","I am proposing here a variant of this problem . Let $p$ and $q$ be distinct odd primes. Is it true that there exists a permutation $\sigma$ of $\{1,\ldots,2pq\}\times \{1,2\}$ such that
$$
\{\sigma(1,x),\ldots,(2pq)\sigma(2pq,x)\}=\{1,\ldots,2pq\}
$$
modulo $2pq$ for each $x \in \{1,2\}$? [I think the answer is affirmative, as in the other case] The question is related to this one . In particular, it follows that the answer is affirmative if $p=3$ and $q=5$.","['number-theory', 'reduced-residue-system', 'permutations', 'elementary-number-theory']"
1903086,"Prove $\forall n\ge 2, 2^{n}-1 \nmid \ 3^{n}-1$","How can I prove for $n\ge 2$ that
$$ 2^{n}-1 \nmid \ 3^{n}-1$$
I have not been able to make any progress on this whatsoever. I would appreciate if someone can give me a hint!","['number-theory', 'divisibility', 'elementary-number-theory']"
1903093,Topology in a proper class,"Topological spaces are usually modelled over sets. What happens if I try to topologize a proper class? Say, the class of all maps from naturals to cardinals or similar. Suppose I specify a topology on such a class by defining convergence for a net. What usual properties or facts may I lose? I would like to go on to take quotient spaces and continuous parametric families of elements etc. What could potentially go wrong? Thank you.",['general-topology']
1903095,Is the closure of a compact set compact?,While looking up information on compact operators I came across these two conflicting posts. If a set is compact then it is closed Topology: Example of a compact set but its closure not compact So the first link says that if a set $U$ is compact then it is closed. $U$ closed means $U = \overline{U}$ and hence $\overline{U}$ is compact. This seems to be in direct contradiction with the second post?,"['functional-analysis', 'general-topology', 'compactness']"
1903099,Why is $ (2+\sqrt{3})^n+(2-\sqrt{3})^n$ an integer?,"Answers to limit $\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$ start by saying that $ (2+\sqrt{3})^n+(2-\sqrt{3})^n $ is an integer, but how can one see that is true? Update: I was hoping there is something more than binomial formula for cases like $ (a+\sqrt[m]{b})^n+(a-\sqrt[m]{b})^n $ to be an integer","['algebra-precalculus', 'elementary-number-theory']"
1903107,How to do this two complex integrals?,"The first integral reads: $$
f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i(k_0-\alpha k_z)\eta}
$$ the second integral is very similar, except the infinitesimal parts:
$$
f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i\eta}
$$ where $\alpha>1$, $k_0>0$ is finite positive number, $\eta = 0^+$ is the infinitesimal positive number. $\vec{k}=(k_x,k_y,k_z), \vec{r}=(x,y,z), k=\sqrt{k_x^2+k_y^2+k_z^2}$. As a hint , the integral for $\alpha=0$ can be done as following: \begin{align}
&\int\mathrm{d}^3k \frac{e^{i\vec{k}\cdot\vec{r}}}{k_0^2-k^2+i\eta} \quad\text{--- making $\vec{r}$ as $z$ axis for $\vec{k}$ } \\
=& 2\pi\int_{-1}^1\mathrm{d}\xi\int_0^\infty \mathrm{d}k k^2\frac{e^{ikr\xi}}{k_0^2-k^2+i\eta}\quad\text{--- integral over $\phi_k$, and let $\xi=\cos\theta_k$}\\
=&-\frac{2\pi}{i r} \int_0^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}(e^{ikr}-e^{-ikr})\\
=&-\frac{2\pi}{i r}\int_{-\infty}^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}e^{ikr} \quad\text{--- use the upper semicircle as contour.} \\
=&-(2\pi)^2\frac{e^{ik_0r}}{2r}
\end{align} It seems that the first integral is zero as pointed by @Fabian. As for the second integral, @Felix Martin makes a variable substitution and integrate out one dimension, it seems that it's hard to proceed and get an analytical form. Therefore, is it possible to get the asymptotic form of the second integral when $r\to\infty$? (i.e. keep only $1/r$ part, ignore $1/r^2$ etc.)","['asymptotics', 'complex-analysis', 'integration', 'definite-integrals', 'contour-integration']"
1903186,Determining if two Sudoku boards are in the same equivalence class,"Consider the following $9 \times 9$ Sudoku board 963 174 258
178 325 649
254 689 731
           
821 437 596
496 852 317
735 961 824
         
589 713 462
317 246 985
642 598 173 and the following partially filled Sudoku board .6. 1.4 .5.
2.. ... ..1
..8 3.5 6..
         
8.. 4.7 ..6
..6 ... 3..
7.. 9.1 ..4
         
5.. ... ..2
.4. 5.8 .7.
..7 2.6 9.. The task is to find out if they are in the same equivalence class . Sudoku boards are closed under the operations below : Relabeling symbols (9!) Band permutations (3!) Row permutations within a band ( ${3!}^3$ ) Stack permutations (3!) Column permutations within a stack ( ${3!}^3$ ) Reflection, transposition and rotation (2). How can I find out (without testing for all possible combinations of the unsolved board) if they are in the same equivalence class? The approach should probably be to define a canonical form for the solved puzzle and then find a method to derive the canonical form from any Sudoku board. But what is my choice of canonical form and how to derive the canonical form from any Sudoku board  (given that we have a solved board)?","['combinatorics', 'recreational-mathematics', 'self-learning', 'sudoku']"
1903204,Is This Proof Valid? - $\sum_{k=1}^{n}\mu(k!)=1$ for $n \geq 3$,"I decided to prove this by induction and I'm not sure if this is a valid proof, or maybe there is something that I'm missing. Anyway, I'd appreciate some feedback nonetheless. Note: This proof assumes knowledge of the Möbius function. Proof that $\sum_{k=1}^{n}\mu(k!)=1$ for $n \geq 3$: Base Case Let $n=3$ and test: $\sum_{k=1}^{3}\mu(k!) = \mu(1) + \mu(2 \cdot 1) + \mu(3 \cdot 2 \cdot 1)$ $=(1)+(-1)+(1)=1$ True for base case ($n=3$). Induction Hypothesis Assume that it is true for $m$ for $m \geq 3$: $\sum_{k=1}^{m}\mu(k!)=1$ Inductive Step: Using the Inductive Hypothesis as a premise, we have: $\sum_{k=1}^{m+1}\mu(k!)=(\sum_{k=1}^{m}\mu(k!)=1)+\mu((m+1)!)$ $=1+\mu((m+1)!)$ Since $m\geq 3$, we have $m+1\geq 4$. But then $(m+1)!=(m+1)(m)...(4)(3)(2)(1)$
Notice the factor of $(4)$ in the expanded sum. Since $4=2^2$ is a square number, $(m+1)!$ has a squared factor, and according to the definition of the Möbius function, $\mu((m+1)!)=0$. Finally, we have $=1+\mu((m+1)!) = 1+0 = 1$ True for inductive step, therefore true for all cases.","['proof-verification', 'number-theory', 'induction', 'mobius-function', 'discrete-mathematics']"
1903215,Solving the geometric series for q,"Is there a general way to find the $q > 0$ solving the equation from the geometric series $$1+q+q^2+q^3+\ldots + q^n = a$$ or $$\frac{1-q^{n+1}}{1-q} = a\quad\text{with } q \neq 1$$ for $a > 1$ and $n\in\mathbb N$? My thoughts: Since polynomials aren't solvable in general for degree 5 or higher, I guess the above equation has no explicit solution for $n\ge 5$. In this case numerical approximations can be used. For $n=5$ also this method can be used.","['real-analysis', 'polynomials', 'roots', 'calculus']"
1903228,How to find the linear transformation associated with a given matrix?,"Good day, I have a little doubt: It is well known that given two bases (or even one if we consider the canonical basis) of a vector space, every linear transformation $T:V \rightarrow W$ can be represented as a matrix, but since this is an isomorphism between $L(V,W)$ and $\mathbb{M}_{m\times n}$ where the latter represents the space $m\times n$ matrices on the same field in which are defined respectively vector spaces. That's where my question comes up, I know find the matrix associated with the linear transformation, but not know how to move from the matrix transformation, ie given any matrix, find the linear transformation that defines it. I wish you could please explain the theoretical process and to see a practical example. Thank you very much, I know it's definitely something silly, but I'm still a student.",['linear-algebra']
1903264,Why is the fundamental group a sheaf in the etale topology?,"In this paper by Minhyong Kim on p5, there is a variety $X$ defined over $\mathbb{Q}$, $G = \pi_1(X(\mathbb{C}),b)$ the topological fundamental group of the associated complex algebraic variety, and $G$^ the profinite completion of $G$. Kim states that $G$^ is a sheaf of groups for the etale topology on Spec($\mathbb{Q}$). Why is this? A sheaf should be an assignment of groups to the etale covers of $\mathbb{Q}$ with morphisms in the opposite direction. How does this assignment work? The ways I've tried to think about it have the functoriality going in the wrong direction. He then says that such a sheaf is just a set with a continuous action of Gal($\bar{\mathbb{Q}}/\mathbb{Q}$). I suppose this is just the outer action coming from the homotopy exact sequence, but I'm more interested in understanding the concrete description of a sheaf. How do they relate?","['galois-theory', 'algebraic-geometry', 'arithmetic-geometry', 'fundamental-groups', 'grothendieck-topologies']"
1903302,"Why ""countability"" in definition of Lebesgue measures?","According to Wikipedia, the definition of the Lebesgue outer measure of a set $E$ is as follows: $$
\lambda^*(E) = \operatorname{inf} \left\{\sum_{k=1}^\infty l(I_k) : {(I_k)_{k \in \mathbb N}} \text{ is a sequence of open intervals with } E\subseteq \bigcup_{k=1}^\infty I_k\right\}
$$ Suppose we remove the countability requirement:
$$
\lambda^*(E) = \operatorname{inf} \left\{\sum_{I \in X} l(I) : X \text{ is a set of open intervals with } E\subseteq \bigcup_{I \in X} I\right\}
$$ (Where we could just define $\sum_{I \in X} l(I)$ to be $\operatorname{sup} \left\{\sum_{I \in Y} l(I) : Y \text{ is a finite subset of } X \right\}$.) Why would this not work? What would break if we remove the countability requirement?","['lebesgue-measure', 'measure-theory']"
1903308,Moore closure and realizing the Kuratowski monoid,"It is well-known result of Kuratowski that starting from a topological closure operation $C: P(X) \to P(X)$ (taking a subset $A \subseteq X$ to its closure $C(A)$ relative to a given topology on $X$) and the complementation operation $\neg: P(X) \to P(X)$, and taking the closure of $C, \neg$ under composition, we can get no more than 14 operations on $P(X)$. These are $$1, \qquad \neg, \qquad C, \qquad \neg C, \qquad C \neg, \qquad \neg C \neg, \qquad C \neg C, \qquad \neg C \neg C, \qquad C \neg C \neg, \qquad \neg C \neg C \neg, \qquad C \neg C \neg C, \qquad \neg C \neg C \neg C, \qquad C \neg C \neg C \neg, \qquad \neg C \neg C \neg C \neg $$ where $1$ denotes the identity operator $P(X) \to P(X)$. This result alone isn't particularly topological: if $C: P(X) \to P(X)$ is any Moore closure operator (meaning $A \subseteq C(A)$ and $CC(A) = C(A)$ and $C(A) \subseteq C(B)$ whenever $A \subseteq B$), then again there is a maximum of 14 possible such operations. In a nutshell, one may prove that $C \neg C \neg C \neg C = C \neg C$ just by playing with the Moore closure axioms, and observe that the free monoid in letters $C, \neg$, modulo the relations $\neg \neg = 1$, $C C = C$, and $C \neg C \neg C \neg C = C \neg C$, results in the 14-element monoid indicated above, called the Kuratowski monoid . Part of the lore here is an exercise in point-set topology: give a topology for which all 14 such operations are distinguished, i.e., exhibit a space $X$ and a subset $A \subseteq X$ where you get 14 distinct subsets by applying these operations. (One solution: take $\mathbb{R}$ with its usual topology, and $A = (0, 1) \cup (1, 2) \cup \{3\} \cup ([4, 5] \cap \mathbb{Q})$.) But there are many, many types of Moore closure operations (e.g., for any algebraic theory $T$ and $T$-algebra $X$, define $C(A)$ to be the smallest subalgebra containing $A$). Most of them don't satisfy the axioms that make a closure operator topological , viz.: $C(\emptyset) = \emptyset$ and $C(A \cup B) = C(A) \cup C(B)$. Question: is there a reasonably simple example of a non-topological Moore closure operator $C$ for which the 14 Kuratowski operations are all distinct? (I'm having trouble thinking of really good tags; please feel free to add more or re-tag.)","['combinatorics', 'general-topology', 'elementary-set-theory']"
1903312,Rotation by Householder matrices,"I have two vectors, let's say $u, v \in \mathbb{R}^n$ with the same norm $\|u\| = \|v\| = 1$. I want to map first vector to second using 2 Householder reflections $(I - 2pp^T), \|p\| = 1$. Is it always possible and what is the formula for such reflections? For me it seems that it is true, because we need the rotation, so every rotation can be represented as a composition of 2 reflections. But I can't find exactly formula. Thanks for the help!","['matrices', 'linear-algebra']"
1903343,Give an example of a non compact Hausdorff space such that $\Delta$ is closed but $Y$ is not Hausdorff,"Suppose $X$ is compact Hausdorff space and $f : X \to Y$ be a quotient map. Then it is well known that $Y$ is Hausdorff iff $\Delta =\{(x,y) \mid f(x)=f(y) \}$ is closed in $X \times X$. For example a proof of this can be found here . I am looking for a counterexample if $X$ is not compact i.e. Give an example (preferably non artificial) of a non compact Hausdorff space $X$ such that $\Delta$ is closed in $X \times X$ but $Y$ is not Hausdorff.","['general-topology', 'examples-counterexamples', 'compactness', 'separation-axioms']"
1903377,Linearization which is a Sturm-Liouville problem: Stability questions,"Consider the scalar phase equation
$$
\theta_t=\theta_{xx}+f(\theta),\qquad f(\theta+2\pi).\qquad (1)
$$
Traveling waves profiles $\theta(x-ct)$ can be found using phase-plane analysis for
$$
\theta_{\xi\xi}=-c\theta_{\xi}-f(\theta).\qquad (2)
$$
Writing (1) in moving coordinates $t$ and $\xi$, we have
$$
\theta_t=\theta_{\xi\xi}+c\theta_{\xi}+f(\theta).\qquad (3)
$$
Traveling waves solutions are then time-independent solutions of (3). Now, it is said: One can obtain stability information fairly easy in this situation: all monotone profiles (now with \theta considered as a function in $\mathbb{R}$) are stable and all-non-monotone profiles are unstable. This can be readily seen by inspecting the linearization, which is a Sturm-Liouville problem with eigenvalue zero and eigenfunction given by the derivative of the wave profile. Since the first eigenfunction (when it exists, that is, when the most unstable eigenvalue is not contained in the essential spectrum) of scalar eigenvalue problems has a sign. This shows that the monotone pulses and fronts, with derivative belonging to the point spectrum, are stable [...]"" I would like to understand this! As far as I see, what is meant is linearization of (3) in a traveling wave profile $\theta^*=\theta^*(\xi), \xi=x-ct$. This should be given by the ODE \begin{equation}
\theta_t=\theta_{\xi\xi}+c\theta_{\xi}+\partial_{\theta}f(\theta^*)\theta,\qquad (1)
\end{equation}
hence the linearizing operator should be given by
$$
L_{*}\theta:=\theta_{\xi\xi}+c\theta_{\xi}+\partial_{\theta}f(\theta^*)\theta=u_t
$$ Putting this into the Sturm-Liouville form, I get
$$
\frac{d}{d\xi}\left(e^{c\xi}\frac{d\theta}{d\xi}\right)+e^{c\xi}\partial_{\theta}f(\theta^*)\theta=e^{c\xi}u_t.
$$
As boundary conditions, I think that
$$
\theta(0)=\theta(2\pi), \theta'(0)=\theta'(2\pi)
$$ 
could be reasonable here. Next, one can show that
$$
\mathcal{L}P(\xi)=0,\text{ with }P(\xi)=\frac{d}{dk}\theta^*(\xi+k)_{|k=0}=\frac{d}{d\xi}\theta^*(\xi).
$$ which shows that $\theta^*_{\xi}$ is indeed an eigenfunction to eigenvalue $\lambda=0$. Now, I only can guess what is meant with the stability claims given above. Open questions for me are: (1) What is meant with the ""first eigenfunction""? I guess this means the eigenfunction belonging to the largest eigenvalue . (2) Why does the ""first eigenfunction"" not exist if the most unstable eigenvalue is contained in the essential spectrum? And why does it exist if it is not? (3) Why does the ""first eigenfunction"" have a sign and why does this imply that monotone profiles (monotone pulses and monotone fronts) with derivative belonging to the point spectrum are stable?","['sturm-liouville', 'ordinary-differential-equations', 'partial-differential-equations']"
1903391,Prove inequality $\arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}}$,"I was trying to figure out if the following function can serve as a mean (see mean value theorem): $$\arccos \left( \frac{\sin y-\sin x}{y-x} \right)$$ And turns out that for $x,y \leq \pi$ it does serve as a mean admirably. But then I've noticed that for $0<x<1$ the following two functions are very close (see the picture): Now how would you prove: $$\arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}}$$ It's probably easier to consider another equivalent inequality: $$\frac{\sin 1-\sin x}{1-x}  \geq \cos \sqrt{\frac{1+x+x^2}{3}}$$ Or even: $$ \text{sinc} \left(\frac{1-x}{2} \right) \cos \left(\frac{1+x}{2} \right)  \geq \cos \sqrt{\frac{1+x+x^2}{3}}$$ We could use Taylor series, but that's too cumbersome in my opinion. Another way would be Mean value theorem itself, but I encounter the same problem. Is there a simple way to prove this inequality? My calculus is not as sharp as it used to be (just kidding, it was never sharp). Edit Just to confirm (numerically) that the inequality holds, here is the plot of the difference between the two functions:","['means', 'inequality', 'trigonometry', 'calculus']"
1903401,Vanishing of a Vector Field,"Suppose I have some smooth vector field $F$ defined on $\mathbb{R}^n$. Further, suppose that there is some radius $R>0$ such that for all $||x||=R$,
$$
\langle x , F(x) \rangle < 0
$$ Ideally, I want to claim that $F$ vanishes somewhere inside the ball $||x||<R$. The motivation behind this question is to understand the dynamics of the flow under $F$, which never leaves the ball. Is this true in general? If not, Is this true if I assume that $\nabla \cdot F < 0$ everywhere? If not, Is this true if I assume that $F(x) = -\nabla\psi(x)+Jx$ where $\psi$ is a convex function and $J$ is a skew-symmetric matrix? I believe that the answer to 2 is affirmative, but have failed to prove the claim.","['stability-in-odes', 'ordinary-differential-equations', 'analysis']"
1903416,"Is there a bijective function mapping Natural numbers to Natural numbers, other than $f(n) = n$?","Is there a function that can be bijective, with the set of natural numbers as domain and range, other than $f(n) = n$?","['elementary-set-theory', 'functions']"
1903446,Diagonalizing the Laplace operator,"A basic fact about the Fourier transform regarding derivatives gives (having the Spectral Theorem of self-adjoint operators in mind) that
$$ -\Delta = \mathcal{F}^{-1} M_{\xi^2} \mathcal{F},$$
where $\Delta$ is the Laplacian on $L^2(\mathbb{R})$, $\mathcal{F}$ the $L^2$ Fourier transform and $M_{\xi^2}$ the operator of multiplication by $\xi^2$. I suppose similar diagonalization processes are possible for Laplacians on $L^2([0, \infty))$ and on $L^2([0, 1])$, where the role of the Fourier transform is replaced by the Laplace transform or the Fourier series. Unfortunately I cannot find any good informations on these things. In particular I would be interested in the role of the boundary conditions of self-adjoint realizations of the Laplacian when diagonalizing the operator. Any explanations or references would be appreciated.","['functional-analysis', 'spectral-theory', 'fourier-analysis', 'laplace-transform']"
1903459,Householder's transformations of space,"Let we have two vectors $u, v \in \mathbb{R}^n$ such that the angle between them is acute ($u^Tv > 0$). I want to prove that there are such unitary transformation, that maps this vector to the first octant. Moreover I want to prove that such map can be represented as a product of two Householder reflections. My idea is that if we choose a 2-dimentional plane containing vectors $(1, 0,0,...,0)^T$ and $(0, \frac{1}{\sqrt{n-1}}, \frac{1}{\sqrt{n-1}}, ..., \frac{1}{\sqrt{n-1}})^T$ and map vectors $u$ and $v$ such way that they lies in this plane and bisetor between $u$ and $v$ maps to vector $(\frac{1}{2}, \frac{1}{2\sqrt{n-1}}, \frac{1}{2\sqrt{n-1}}, ..., \frac{1}{2\sqrt{n-1}})^T$ then we get necessary transform. So because of length are reserved, this map is unitary. This is proof of existing such map. But I still can not find such two Householder matrices that represent it. Thanks for the help!","['matrices', 'linear-algebra', 'linear-transformations']"
1903465,Understanding extension of scalars,"Let $V$ be a finite dimensional complex vector space. I recently asked how to find a Natural isomorphism between $\mathbb{C}\otimes_{\mathbb{R}}V$ and $V\oplus V.$ and got some very nice answers. In particular, I was told that the map $$c\otimes v \mapsto (\Re(c)v,\Im(c)v)$$ is a complex linear isomorphism. Here is the problem I am having with this: As I understand it, the scalar multiplication defined on the extension of scalars is given by $$c'(c\otimes v) = (c'c)\otimes v$$ and with this multiplication the above map is not complex linear. However, if I use
$$c'(c\otimes v)=c\otimes (c'v)$$
as my scalar multiplication then the given map is indeed complex linear. So how do I reconcile this with the definition of scalar multiplication in an extension of scalars? Many thanks!","['abstract-algebra', 'tensor-products', 'vector-spaces']"
1903467,"Find the limit of $\int_0^1\cdots\int_0^1 \sin(\sqrt[n]{x_1\cdots x_n}) \, dx_1\cdots dx_n$ using a probabilistic approach","I am looking for the solution of this integral: $$
\lim_{n \rightarrow \infty} \int_0^1\cdots\int_0^1 \sin(\sqrt[n]{x_1\cdots x_n}) \, dx_1\cdots dx_n
$$ I know that $X_1,\ldots,X_n \sim \mathcal{U}[0,1]$ . I tried to use law of large numbers and compute the expected value but it failed because I can't apply this law to product of random variables. Does anyone have any idea?","['law-of-large-numbers', 'probability-theory', 'integration']"
1903470,Sum of the series $1\cdot3\cdot2^2+2\cdot4\cdot3^2+3\cdot5\cdot4^2+\cdots$,"Find the sum of $n$ terms of following series: $$1\cdot3\cdot2^2+2\cdot4\cdot3^2+3\cdot5\cdot4^2+\cdots$$ I was trying to use $S_n=\sum T_n$, but while writing $T_n$ I get a term having $n^4$ and I don't know $\sum n^4$. Is there any other way find the sum?",['sequences-and-series']
1903473,Permutation subsequences,"Given three permutations $p_1,p_2,p_3$ of $\{1,2,\ldots,n^3+1\}$, prove that two of them have a common subsequence of length $n+1$. I have tried to solve this using the pigenhole principle but I didnt progress too much, any help would be appreciated edit: when I say subsequence I mean that there are $1<=r<q<=3$ 
 and $1<=i(1)<i(2)<...<i(n+1)<=n^3+1$  and  $1<=j(1)<j(2)<...<j(n+1)<=n^3+1$ so that $p_r(i1)=p_q(j1)$, $p_r(i2)=p_q(j2)$ and so on, not necessarily consecutive","['permutations', 'combinatorics', 'pigeonhole-principle', 'sequences-and-series']"
1903495,"Prove $ dy_1 \, dy_2=|J|\,dx_1 \, dx_2$, where $|J|$ is the determinant of the Jacobian.","Suppose,$y_1=y_1(x_1,x_2)$ and $y_2=y_2(x_1,x_2)$,
such that, $$dy_1=\frac{\partial y_1}{\partial x_1}dx_1+\frac{\partial y_1}{\partial x_2} \, dx_2$$ $$dy_2=\frac{\partial y_2}{\partial x_1}dx_1+\frac{\partial y_2}{\partial x_1} \, dx_2$$ Then,I've taken the product of the above two,but unable to reach to the result.","['multivariable-calculus', 'differential-geometry', 'calculus']"
1903501,Morphism of curve associated to an extension of fields of dim $1$,"This is a basic question in algebraic geometry which probably is already answered somewhere in the web, but I am struggling too much on it and I couldn't find anything which could be of help.I thought that it is maybe worth to ask you if you can give me the right idea. I use a step inside the proof of Corollary II.6.10 of Hartshorne, Algebraic Geometry, as an example of the idea I am struggling to get. In the following $X$ is a curve (in the sense of Hartshorne, i.e. integral separated (complete nonsingular) scheme of finite type and dimension $1$ over a field $k$) and $K(X)$ is the field of rational functions on it. Given $f\in K(X)^*$ such that $f\notin k$, consider the extension $k(f)\subset K(X)$ of field. Claim : this extension of fields induces a morphisms $X\to\mathbb{P}^1$ of curves. What I thought: I know (Hartshorne, Corollary I.6.12) that there is an equivalence of categories between the category of nonsingular projective curves with dominant morphisms and the category of function fields of dimension $1$ over $k$ with $k$-homomorphisms. Moreover this equivalence reverse the arrows (Theorem I.4.4). The equivalence is described in terms of abstract nonsingular curves as defined by Hartshorne in Section I.6. In particular, to every function field $K$ of dimension $1$ we can associate a nonsingular projective curve $C_K$ (Theorem I.6.9). Coming back to the claim, I should get a morphism of curves $$X\to C_{k(f)}.$$ Now, why is $C_{k(f)}\cong\mathbb{P}^1$? I've tried to work out something explicitly, but it turns out that this situation is too generic. Moreover, later on inside the book (exercise IV.2.2.(b)), he seems to claim the same using the extension $k(x)\subset k(x,\beta)$ (here $\beta$ is the solution in $k(x,y)$ of a polynomial in $k(x)[y]$). I mean that he defines the morphism $X=C_{k(x,\beta)}\to\mathbb{P}^1$, from whom I deduce (maybe erroneously) that $C_{k(x)}\cong\mathbb{P}^1$ as well. I think I am lacking something theoretic here. Disclaimer: from a complex geometric point of view, I know that a meromorphic function $f$ on a curve $X$ gives a morphism $X\to\mathbb{P}^1$ sending $x$ to $f(x)$ and this is exactly what I expect the map in the claim does. This is also quite convincing that the answer to the question is affirmative for every $f\in K(X)$ (even if there still is a discrepancy between that function field and $K(\mathbb{P}^1)$). But what about $k(x)$? I have no evidence for it. Thank you very much for any kind of hint/suggestion/advice/answer you can give me and I hope I am not completely misunderstanding everything!",['algebraic-geometry']
1903515,Relating primal and dual characterization of an (interpolation) norm on $\ell_1+\ell_2$,"For any fixed $t> 0$, the $K$-functional defines a norm on the space $\ell_1+\ell_2$:
$$
\lVert a\rVert_{K(t)} = \inf\{\lVert a'\rVert_1+ t\lVert a''\rVert_2 : a'\in\ell_1,\ a''\in\ell_2,\ a'+a''=a\}. \tag{1}
$$ Now, one can show (Lemma 1, [1]) that, for $a\in\ell_2$ this is equal to the dual formulation
$$
\lVert a\rVert_{K(t)} = \sup\left\{\sum_{n=1}^\infty a_nb_n : (b_n)_{n\in\mathbb{N}}\in\ell_2,\ \lVert b\rVert_\infty \leq 1,\ \lVert b\rVert_2 \leq t \right\}. \tag{2}
$$ My question is: is there a way to relate (2) to (1)? More precisely, is there a correspondence between the $b\in\ell_2$ from (2) and the $(a',a'')\in\ell_1\times\ell_2$ from (1)? [1] Montgomery-Smith, S. J. (1990). The distribution of Rademacher sums . Proc. Amer. Math. Soc., 109(2), 517–517. doi:10.1090/s0002-9939-1990-1013975-0","['functional-analysis', 'lp-spaces', 'interpolation-theory']"
1903556,"How to work out eigenvalues, eigenfunctions, $M$ and $\delta$ for this problem","Let $$p(x)= \begin{cases}
M \in \mathbb{R}, & x \in \left(\frac{1}{2}-\delta,\frac{1}{2}+\delta\right) \\
1, & x\in \left(0,\frac{1}{2}-\delta)\cup(\frac{1}{2}+\delta,1\right) .
\end{cases}$$ An asymptotic expansion for the equation (known as the Elastic Differential Equation); $$y'' + \lambda^{2} p(x) y = 0$$ where $\lambda$ is a variable, and $(a,b)=(0,1)$ is; $$y(x,\lambda)=e^{i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)+\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+e^{-i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{-1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)-\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+\mathcal{O}(\frac{1}{\lambda^2})$$ Where $e_+$ is a constant, and; $$d(x)=\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{5(p'(x))^2}{16(p(x))^{\frac{5}{2}}}dx -\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{p''(x)}{4(p(x))^{\frac{3}{2}}}dx$$ Now $$y_1'' + \lambda^2 y_1 = 0, x\in\left(0,\frac{1}{2}-\delta\right)$$ $$y_2'' + \lambda^2 M y_2 = 0, x\in\left(\frac{1}{2}-\delta, \frac{1}{2}+\delta\right)$$ $$y_3'' + \lambda^2 y_3 = 0, x\in\left(\frac{1}{2}+\delta,1\right)$$ $y_1$, $y_2$, $y_3$ have solutions of the form; $$y_1=a_1 e^{i\lambda x} + b_1 e^{-i\lambda x}$$ $$y_2=a_2 e^{i\lambda\sqrt{M}x} + b_2 e^{-i\lambda\sqrt{M}x}$$ $$y_3=a_3 e^{i\lambda x} + b_3 e^{-i\lambda x}$$Initial value and boundary conditions: $$y_1\left(\frac{1}{2}-\delta\right)=y_2\left(\frac{1}{2}-\delta\right)$$
$$y_1'\left(\frac{1}{2}-\delta\right)=y_2'\left(\frac{1}{2}-\delta\right)$$
$$y_2\left(\frac{1}{2}+\delta\right)=y_3\left(\frac{1}{2}+\delta\right)$$
$$y_2'\left(\frac{1}{2}+\delta\right)=y_3'\left(\frac{1}{2}+\delta\right)$$
$$y_1(0)=0$$
$$y_3(1)=0$$ Then using these 6 equations I get the system; $$\begin{bmatrix} g & \frac{1}{g} & -f & -\frac{1}{f} & 0 & 0 \\ i\lambda g & \frac{-i\lambda}{g} & -i\lambda \sqrt{M}f & \frac{i\lambda \sqrt{M}}{f} & 0 & 0 \\ 0 & 0 & h & \frac{1}{h} & -k & \frac{-1}{k} \\ 0 & 0 & i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k} \\ 1 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 1 \end{bmatrix}.\begin{bmatrix} a_1 \\ b_1 \\ a_2 \\ b_2 \\ a_3 \\ b_3 \end{bmatrix}=0$$ Where; $$f= e^{i\lambda \sqrt{M}\left(\frac{1}{2}-\delta\right)}$$
$$g= e^{i\lambda \left(\frac{1}{2}-\delta\right)}$$
$$h= e^{i\lambda \sqrt{M}\left(\frac{1}{2}+\delta\right)}$$
$$k= e^{i\lambda \left(\frac{1}{2}+\delta\right)}$$ What I am trying to do; Deriving formulae for $M$ and $\delta$, then hence obtain eigenvalues and eigenfunctions for $y(x,\lambda)$. I have done work on this, and will provide what I have done in the future via updates to this question; Attempt 1); Simplifying the first matrix using Gaussian elimination, then have 6 equations; $a_1=0$, $b_1=0$,  $a_2=0$, $b_2=0$, $a_3=0$, $b_3=0$ Hence $y_1 = y_2 = y_3 =0$, but no apparent useful results to me for $M$ and $\delta$. Attempt 2); --To be updated-- Using the fact that the determinant of the 1st matrix is equal to 0 for the system to have solutions, I have then expanded this determinant to give the following equation; $$-8\sqrt{M}+(1-\sqrt{M})\frac{fg}{hk}+(1+2\sqrt{M}-M)\frac{hk}{fg}+(1+2\sqrt{M}+M)(\frac{fk}{gh}+\frac{gh}{fk})=0$$ This then simplifies to; --To be updated-- $$a^{2}s^{4a\delta}-a^{2}s^{2\delta(2a-2)}-a^{2}s^{2\delta(a+1)}-8(a+1)s^{2a\delta}+(2-a)=0$$ where $M \ge 0 \in \Bbb R$, $a=\sqrt{M}+1 \in \Bbb R$, $s=e^{i\lambda} \in \Bbb C$, $a \ge 1$, $\delta \in \Bbb R$. (I have another question where I ask for help on this equation.) Attempt 3) Using the fact that the determinant of the characteristic matrix is equal to 0 for eigenvalues; --To be updated-- $$\det\begin{bmatrix} f-X & \frac{1}{f} & -g & -\frac{1}{g} \\ i\lambda \sqrt{M}f & \frac{-i\lambda \sqrt{M}}{f}-X & -i\lambda g & \frac{i\lambda}{g} \\ h & \frac{1}{h} & -k-X & -\frac{1}{k} \\ i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k}-X \end{bmatrix}=0$$ where $X$ is an eigenvalue. I have then got a cubic equation in $X$ , from simplifying this; $$a'X^3 + a''X^2 + a^{(3)}X + a^{(4)} + a^{(5)} + a^{(6)}=0 $$ The coefficients of this cubic equation, $a^{(n)}$ though are complex functions of $\lambda, M, f, g, h$ and $k$. Questions / help requested; 1) Derivation of the eigenvalues and eigenvectors, what this means for $M$ and $\delta$, and an example illustrating this. 2) Derivation of formulae for $M$ and $\delta$. 3) What happens to $M$ & $\delta$ as $\lambda$ tends to infinity? 4) I am not sure which method - Attempt 1 / Attempt 2 / Attempt 3 is more fruitful. Am I approaching this in a sound way? What would be better method(s) if any?","['eigenvalues-eigenvectors', 'matrices', 'asymptotics', 'eigenfunctions', 'ordinary-differential-equations']"
1903577,Very general inner product determinant inequality,"Let $V$ be a vector space and $\langle \cdot, \cdot\rangle$ be an inner product on $V$. Prove that for any positive integer $n$ and any $x_1,\dots,x_n \in V$ \begin{equation}\det
\left[
	\begin{array}{cccc}
		\langle x_1,  x_1 \rangle & \langle x_2,  x_1 \rangle &\dots & \langle x_n,  x_1 \rangle\\
		\langle x_1,  x_2 \rangle & \langle x_2,  x_2 \rangle & \dots & \langle x_n,  x_2 \rangle\\
		\vdots & \vdots & \ddots & \vdots \\
		\langle  x_1,  x_n \rangle & \langle x_2,  x_n \rangle & \dots & \langle  x_n,  x_n \rangle
  \end{array}
\right]\geq 0\,.
\end{equation} The case of $n=1$ is trivial, it follows from the inner product's defining property. The case of $n=2$ is true due to the Cauchy-Schwarz inequality . A less general formula came up for $n=3$ during physics research, where there are physical reasons to expect that this inequality ought to hold. I cannot find a counterexample for any vector space dimension or matrix dimension. Is this fundamental inequality a well-known theorem?","['inner-products', 'linear-algebra', 'hilbert-spaces', 'determinant']"
1903613,A curious determinantal identity,"A real $n\times n$ matrix $A$ can be uniquely decomposed into the sum of a symmetric matrix $B$ and a skew-symmetric matrix $C$ by setting $B=\frac{A+A^T}{2}$ and $C=\frac{A-A^T}{2}$. When $n=2$ there is an interesting result for the determinants of these matrices: if $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, then $B=\begin{bmatrix}a & \frac{b+c}{2} \\ \frac{b+c}{2} & d\end{bmatrix}$ and $C=\begin{bmatrix}0&\frac{b-c}{2} \\ -\frac{b-c}{2} & 0\end{bmatrix}$, hence
$$ \det(B)+\det(C)=ad-\frac{(b+c)^2}{4}+\frac{(b-c)^2}{4}=ad-bc=\det(A) $$ By testing a few examples, I've come to the conclusion that this identity does not hold for $n=3$ or $n=4$. My question, therefore, is whether the equality $\det(A)=\det(B)+\det(C)$ for $2\times 2$ matrices is simply a coincidence, or whether there is something interesting going on.","['matrices', 'linear-algebra', 'determinant']"
1903625,Can I find the length of the side of a triangle WITHOUT a calculator?,"I was on Khan Academy watching a video where he was finding the length of a side of a right triangle where one angle measured 65° and the length of the side adjacent to the angle was 5. The side he was trying to find was opposite to the angle that measured 65°, so to find the length of the side, he used $tan(65°) = \frac{a}{b}$, and he got 10.7 when he multiplied 5 by $tan(65°)$. (This is the link to the video: https://www.khanacademy.org/math/trigonometry/trigonometry-right-triangles/trig-solve-for-a-side/v/example-trig-to-solve-the-sides-and-angles-of-a-right-triangle ) Since I don't have a graphing calculator, I used my scientific calculator to do the same thing, but I got 7.8, and I also used the calculator on my phone which gave me -7.4. Is there a way I can do the same thing WITHOUT a calculator?","['angle', 'calculator', 'trigonometry']"
1903637,Cover of the disk that behaves like a circle,"Is there a cover of the disk in the plane by three open connected sets $U_1, U_2, U_3$ such that any two of these sets have nonempty intersection, but the triple intersection $U_1 ∩ U_2 ∩ U_3$ is empty? Clearly this is possible for the disk's boundary, i.e. a circle, but it does not seem possible for the disk itself. I can tell that such a cover will not be a good cover , since this would imply the disk is homotopically equivalent to the covering's nerve, in this case a triangle.","['algebraic-topology', 'geometric-topology', 'general-topology']"
1903677,Second order derivative of the inverse matrix operator,"Let $f : Gl_{n}(\mathbb{R}) \to Gl_{n}(\mathbb{R})$ defined by $f(X)=X^{-1}$ . Compute $f''(X)(H,K)$ . I calculated $f'(X).H=-X^{-1}HX^{-1}$ so I tried to use some composition of linear functions but did not find the appropriate functions. Can anyone help me in this matter?","['derivatives', 'hessian-matrix', 'matrices', 'matrix-calculus', 'linear-algebra']"
1903742,Testing Zeros Of The Riemann Hypothesis [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I was on Mathworld some time ago when I read this from http://mathworld.wolfram.com/RiemannHypothesis.html : The Riemann hypothesis was computationally tested and found to be true for the first 200000001 zeros by Brent et al. (1982), covering zeros sigma+it in the region 0 < t < 81702130.19. My question is: How can you be sure that you haven't missed any zeros? It seems to me that it is impossible because for any fixed t one would have to check all real sigma values between 0 and 1. And even if there was some way to do that one would still need to test all real values of t between 0 and 81702130.19. Do they have a list of ""candidate zeros"" that they would just try out? Thanks in advance.","['computer-science', 'number-theory', 'riemann-hypothesis', 'riemann-zeta', 'experimental-mathematics']"
1903813,Measure of distance between two gaussian distributions,"let' say I have two different phenomena classes, and I extract two different kinds of values for each of them. For example, comparing two different leaves, I extract length and weight of several hundreds of instances. From this experimental observation values, I calculate the mean and standard derivation, and assume they follow a normal distribution, so: $L_{1}\sim\mathcal N(\mu_{11},\sigma_{11})$ Length distribution for first class of leaf $W_{1}\sim\mathcal N(\mu_{21},\sigma_{22})$ Weight distribution for first class of leaf $L_{2}\sim\mathcal N(\mu_{31},\sigma_{32})$ Length distribution for second class of leaf $W_{2}\sim\mathcal N(\mu_{41},\sigma_{42})$ Weight distribution for second class of leaf I want to take the characteristic that better distinguishes between both classes, so I need some kind of measurement of distance between $L_{1},L_{2}$ and $W_{1},W_{2}$, to take the one with longest distance.
Which mathematical notion helps me here?","['statistics', 'probability', 'pattern-recognition']"
1903827,Is the set of square integrable functions of a metric measure space again a metric measure space?,"Given a metric measure space $(X,d,m)$ (i.e. a metric space $(X,d)$ equipped with a $\sigma$-finite Borel measure $m$), we can consider the space $L^2(X,m)$ of ""square-integrable functions"": $$L^2(X, m) = \left\{f: X\to \mathbb R, f \text{ measureable and }\int_X |f|^2 dm <\infty\right\}.$$ Further, $L^2(X.m)$ admits a norm $|| f ||_{L^p}:=\left(\int_{X}|f|^2dm\right)^{1/2}$, which can be used to turn it into a metric space. My questions are the following: Is there any measure on $L^2(X,m)$ which turns it into a metric measure space? if there is, then, is there a ""nice relation"" between $(X,d,m)$ and $L^2(X,m)$? (Unfortunately I don't have yet decided what this should mean but any interactions would be welcome). EDIT: For the sake of obtaining something non-trivial, I would like to also require that the measures involved have non-empty support. I am aware that Dirac measures work, however, if possible I would like to know about a measure that has more to do with the original measure $m$ on $X$.","['functional-analysis', 'real-analysis', 'metric-spaces', 'measure-theory']"
1903860,Generalized Dirichlet Energy,"Question What is the corresponding Euler-Lagrange equation that minimizes the energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla^k u \|^2 \> dx$$ where $\Omega \subseteq \mathbb{R}^n$ and $u: \Omega  \mapsto \mathbb{R}$. Conjecture $$\Delta^{k} u = 0$$ Background For the standard Dirichlet Energy $$E[u] = \frac{1}{2}\int_{\Omega} \|\nabla u \|^2 \> dV$$ we can prove that harmonic functions minimize this energy. A standard way to do this is to assume we have a function $w$ such that $\Delta w = 0$ for $x \in \Omega$. This means $$\int_\Omega \Delta w(w-u) \> dx = 0$$
$$\int_\Omega \nabla u \cdot \nabla w \> dx - \int_\Omega \|\nabla w\|^2 \> dx = 0$$
$$\frac{1}{2} \int_\Omega \|\nabla u\|^2 + \|\nabla w\|^2 \> dx - \int_\Omega \|\nabla w\|^2 \> dx \ge 0$$
$$\frac{1}{2} \int_\Omega \|\nabla u\|^2  \ge \frac{1}{2} \int_\Omega \|\nabla w\|^2 \> dx$$ Since we didn't lose generality, this argument works for any choice of $u$. I suspect a similar technique should show the more general version, but I haven't been able to work out the identities. The reason I am interested is this more general energy can be used to enforce more continuity at the handles of a mesh deformation. I'm going to keep looking into this and maybe post my own answer if no one beats me to it.","['multivariable-calculus', 'calculus-of-variations', 'partial-differential-equations']"
1903886,"Show that $(1,1,1)$, $(a,b,c)$, $(a^2, b^2, c^2)$ are linearly indepdenent for distinct $a,b,c$","Show that $(1,1,1)$, $(a,b,c)$, $(a^2, b^2, c^2)$ are linearly indepdenent, where $a,b,$ and $c$ are distinct real numbers. I will show my attempt and then state where I get stuck. Suppose $c_1(1,1,1) + c_2(a,b,c) + c_3(a^2,b^2,c^2) = 0$ This leads to the three equations:
$c_1+c_2a+c_3a^2 = c_1+c_2b+c_3b^2 = c_1+c_2c+c_3c^2 = 0$. Now I am not sure how to sure from here that each $c_i$ must be 0. Hints appreciated.",['linear-algebra']
1903887,Show that $\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log(S(t))$,"Let $T$ a continuous non-negative random variable that represents a failure time.
  Taking $S(t)=P(T\geq t)$ and $f(t)$ the probability density function
  of $T$. Show that
  $$\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log S(t)$$ where
  $\lambda(t)$ represents a failure rate. I had some additional information
$$\lambda(t)=\frac{P(T\geq t)-P(T\geq t+\Delta t)}{\Delta t \, S(t)}$$
and $$\lambda(t)=\lim_{\Delta t\rightarrow 0}\frac{P(t\leq T<t+\Delta t\mid T\leq t)}{\Delta t}$$ I tried to do some manipulation with that, but I can't get nowhere.","['self-learning', 'statistics', 'proof-writing', 'probability']"
1903902,Derivative of $\rm a^T X^T X X^T X b$ with respect to $\rm X$,"I'm trying to take the derivative of a 4th order equation with respect to a matrix. It has the following form $$\frac{\displaystyle \partial \bf a^T X^T X X^T X b}{\displaystyle \partial \bf X} = \Large ?$$ $\bf a$ and $\bf b$ are vectors and $\bf X$ is a matrix so, in effect, it's the derivative of a scalar with respect to a matrix. I found the basic derivatives in Matrix Calculus on Wikipedia and I found the second order derivative in The Matrix Cookbook . This gives me the solution for the second order case $$\frac{\displaystyle \partial \bf a^T X^T X b}{\displaystyle \partial \bf X} = \bf X (ab^T+ba^T)$$ I wonder if there is a similar solution for the 4th order case?","['matrices', 'scalar-fields', 'matrix-calculus', 'derivatives']"
1903915,spectrum of the multiplication operator for general measure spaces,"Suppose $(X, \mu)$ is a measure space. If we additionally assume that $X$ is $\sigma$-finite, then for $f\in L^\infty(X, \mu)$, the spectrum of the multiplication operator $M_f$ on $L^2$ equals the essential range of $f$. Although $\sigma$-finiteness plays a role in the proof that I know for this statement, I'm not sure if the assumption can be dropped with a different proof. I wonder if anyone can provide a proof or a counterexample for this. (My feeling is that a counterexample should exist if $X$ is not $\sigma$-finite.) Thanks!","['functional-analysis', 'operator-theory']"
1903929,How is the automorphism group of a Lie group given a differential structure?,"This is what I understand. Let $G$ be a simply connected Lie group. Given an isomorphism $\phi : G \to G$, we get an isomorphism $d\phi : \frak{g} \to \frak{g}$, and this induces an abstract group homomorphism (by the chain rule) $\operatorname{Aut}(G) \to \operatorname{Aut}(\frak{g})$. Since $G$ is simply connected, for any homomorphism $\psi : \frak{g} \to \frak{g}$ there exists a unique homomorphism $\phi : G  \to G$ such that $\psi = d\phi$. This fact allows us to identify the abstract groups $Aut(G)$ and $Aut(\frak{g})$. Here is where my understanding is very shaky. Now, $\frak{g}$ is just a finite dimensional vector space (endowed with a Lie bracket, ofc. Fix a basis so that $\frak{g} = \mathbb{R}^n$. Now, $\operatorname{Aut}(\frak{g})$ can be identified with a subgroup of matrices in $\operatorname{GL}(n, \mathbb{R})$. (Namely, the matrices $A$ such that $A[e_i, e_j] = [Ae_i, Ae_j]$ for the chosen basis.) As such, $\operatorname{Aut}(\frak{g})$ inherits a differential structure, and we just slap that onto $\operatorname{Aut}(G)$ through the above identification. Functoriality has to be checked, ofc. Is my understanding correct?","['group-theory', 'manifolds', 'differential-geometry', 'lie-algebras', 'lie-groups']"
1903940,"A near ""geometric mean"" that handles zeros and negative numbers","By definition of a geomean, a geometric mean of a set of numbers containing zero is 0. (See Geometric mean of a dataset containing zeros )  However in financial data that is commonplace and sometimes finance data has negative numbers. The inverse hyperbolic sine mean doesn't suffer from these shortcomings. Let's define that as: sinh(mean(asinh(money))) It handles negatives, positives and zeros with ease. Why won't people use the inverse hyperbolic sine mean (IHS) mean? Why don't they teach the IHS mean? Is there a better term for this? Surely someone else has thought of it. This isn't a question about R as the code is only given to be informative. set.seed(12)
(money=sort(round(rlnorm(100,6,4)))+1)
sinh(mean(asinh(money))) #gives 393.7934
exp(mean(log(money))) #gives 391.2577
prod(money)^(1/length(money)) #gives 391.2577 but can overflow
median(money) #261.5 Notice with positive numbers, the IHS mean is nearly identical. set.seed(12)
(money=sort(round(rlnorm(100,6,4))))
sinh(mean(asinh(money))) #gives 369.4342
exp(mean(log(money))) #gives 0
prod(money)^(1/length(money)) #gives 0 
median(money) #260.5 Notice with positive numbers and zeros, the IHS mean is more useful. set.seed(12)
(money=sort(round(rlnorm(100,6,4)))-10000)
sinh(mean(asinh(money))) #gives -201.5435
exp(mean(log(money))) #gives NaN
prod(money)^(1/length(money)) #gives Inf so WRONG due to overflow
median(money) #-9739.5 Notice with mixed negative/positive numbers and zeros, the IHS mean is more useful. set.seed(12)
(money=sort(round(rlnorm(100,6,4)))*-1)
sinh(mean(asinh(money))) #gives -369.4342
exp(mean(log(money))) #gives NaN
prod(money)^(1/length(money)) #gives 0
median(money) #-260.5 Notice with purely negative, the IHS mean is still useful.","['hyperbolic-functions', 'means', 'economics', 'statistics', 'finance']"
1903976,When do $n$ ants in cyclic pursuit with constant velocity converge?,"I'm reading a paper ( Ants, Crickets and Frogs in Cyclic Pursuit ) and trying to understand one of the simpler results. The following is a paraphrasing of the parts I'm using, but check the paper if more context is needed: Consider $n$ moving ants $\mathrm{x}_1(t)$ through $\mathrm{x}_n(t)$ in a vector space $\mathcal{L}$ . Each ant $\mathrm{x}_i$ moves at all times in the direction of the ant $\mathrm{x}_{i-1}$ (and $\mathrm{x}_1 \to \mathrm{x}_n$ ) with constant velocity $v$ . For $i = 1,...n$ , define the initial conditions as $\mathrm{x}_i(0) = \xi_i$ and the following system of $n$ differential equations: $$\frac{d}{dt} \mathrm{x}_i(t) = v \ \frac{\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)}{||\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)||}$$ We can construct an equivalent model in barycentric coordinates by defining $$\mathrm{y}_i(t) = \mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)$$ Then for $i = 1,...,n$ , we have $\mathrm{y}_i(0) = \xi_{i-1} - \xi_i$ and the following system of equations: $$\frac{d}{dt} \mathrm{y}_i(t) = v \left(\frac{\mathrm{y}_{i-1}(t)}{||\mathrm{y}_{i-1}(t)||} - \frac{\mathrm{y}_i(t)}{||\mathrm{y}_i(t)||} \right)$$ For this system, we can prove the following: $\textbf{Lemma 1}$ The difference vectors $\mathrm{y}_i(t)$ have the following properties: $\sum \mathrm{y}_i = 0$ $\frac{d}{dt} ||\mathrm{y}_i|| = (\cos \alpha_i - 1) v$ Where $\alpha_i$ is the angle between $\mathrm{y}_i$ and $\mathrm{y}_{i-1}$ . $\textbf{Proof:}$ The first part follows by definition and the second comes from projecting $\mathrm{y}_{i-1}$ onto $\mathrm{y}_i$ . Call $T$ the termination time where all ants collide and define $Y(t) = \sum_i ||\mathrm{y}_i(t)||$ . Then comes the part I'm most curious about. They claim If the speeds are constant and equal, Lemma $1$ part 2 implies that $T \le Y(0)/v$ . I think this works by summing both sides of the second equality to get $$\sum_i \frac{d}{dt}||\mathrm{y}_i|| = v \sum_i (\cos \alpha_i - 1) \le -v$$ which would prove the claim, but the inequality is where I get stuck. I'm thinking we can bound the sum somehow by using the fact that $\sum_i \alpha_i \ge 2\pi$ , but I'm not quite sure how to do this. Edit: It appears this claim is false as pointed out by @stewbasic. I'd still like to get answers for the following: A general bound for the termination time $T$ . Under what conditions do the ants collide simultaneously? For simultaneity, a sufficient condition is fine provided it covers the specific case of cube. For example, if a cycle of $n$ ants all start a constant distance $\mathrm{y}(0) = ||\mathrm{y}_i(0)||$ from their pursuer and prey and all start the same distance $d$ from their centroid, is this enough to ensure simultaneity of collision? If it simplifies things, the time bound need only apply to cases that satisfy the sufficient condition for simultaneity. Specific case : Consider $8$ ants on the corners of a unit cube with pursuit cycle given by $$(0,0,0) \to (1,0,0) \to (1,1,0) \to (1,1,1) \to (1,0,1) \to (0,0,1) \to (0,1,1) \to (0,1,0)$$ I'd ideally like to get the bound $T \le 2/v$ , but even $T \le 8/v$ would acceptable. The actual value is about $1.96$ , which I can get from simulation where their convergence path looks like this: Here's an animated version I created: Edit: As @VictorZurkowski points out, the proof of Theorem $1$ includes a (not very strict) bound on $T$ which reduces in the constant velocity case to $$T \le \frac{Y(0)}{v\left(1 - \cos\left(\frac{2\pi}{m}\right)\right)}$$ where $m$ is the number of survivors. For the cube case with unit speed, this becomes $$T \le \frac{Y(0)}{1\left(1 - \cos\left(\frac{2\pi}{8}\right)\right)} = \frac{8}{\left(1 - \cos\left(\frac{\pi}{4}\right)\right)} = 8\left(2 + \sqrt{2}\right) \approx 27.3137$$ Using symmetry, I think I could get this down to $2 \left(2+\sqrt{2}\right) \approx 6.82843$ . All of the $\alpha_i$ follow one of these two curves where the $y$ -axis is in degrees and the $x$ -axis is $t$ in percent of of $T$ (at 100% they converge). We can see that they converge to the planar case where $\alpha_i = \frac{\pi}{4} = 45^{\circ}$ I believe the planar case is worst case in terms of convergence time and this website has some nice bounds for regular polygons with unit area. If my calculations are correct, an octagon with unit length sides would have path length $2\sqrt{4+3\sqrt{2}} \approx 5.742$ . [To be revised and continued...]","['dynamical-systems', 'ordinary-differential-equations', 'linear-algebra', 'proof-explanation']"
1903977,Continuity of limit of continuous functions implies uniform convergence?,"We know that uniform convergence of continuous functions implies the continuity of the limit. I'm wondering if the inverse is true: if a sequence of continuous functions $\{f_n\}$ converges to continuous function $f$, then is it true that $\{f_n\}$ converges uniformly to $f$. My attempt is that for any $x$ in the domain and $\epsilon$, there exists a $a$ such that $|f(x)-f_n(x)|\le|f(x)-f(a)|+|f(a)-f_n(a)|+|f_n(a)-f_n(x)|$. But I can't estimate the distance between $|f(a)-f_n(a)|$. Any tips ?","['real-analysis', 'uniform-convergence', 'analysis']"
1904039,Proof Verification : Prove -(-a)=a using only ordered field axioms [duplicate],"This question already has answers here : Prove $-(-a)=a$ using only ordered field axioms (6 answers) Closed 7 years ago . I need to prove for all real numbers $a$, $-(-a) = a$ using only the following axioms: Thanks to many members of the Mathematics Stackexchange Community, I have the following proof worked out: Theorem: The Additive Inverse Identity is Unique $
( \forall a,b,c \in \mathbb{R} )(a+b=0) \land (a+c=0) \\
( \forall a,b,c \in \mathbb{R} )(a+b=a+c) \\
( \forall a,b,c \in \mathbb{R} )(b=c) \\
\text{QED} \\
$ Theorem $\textbf{a} \cdot \textbf{0 = 0}$ $
\begin{align}
a \cdot 0 &= a \cdot 0 \\
a \cdot (0 + 0) &= a \cdot 0 \\
a \cdot 0 + a \cdot 0 &= a \cdot 0 \\
a \cdot 0 + a \cdot 0 + (-a) \cdot 0 &= a \cdot 0 + (-a) \cdot 0 \\
a \cdot 0 &= 0 \\
&\text{QED} \\
\end{align}
$ Theorem: $-\textbf{1} \cdot \textbf{(a) = (}-\textbf{a)}$ $
\begin{align}
a \cdot 0 &= 0 \\
a \cdot \left[ 1 + (-1) \right] &= a + (-a) \\
1 \cdot a + (-1) \cdot a &= a + (-a) \\
-1 \cdot (a) &= (-a) \\
&\text{QED} \\
\end{align}
$ Theorem: $-\textbf{(}-\textbf{a) = a}$ $
\begin{align}
0 &= 0 \\
-a \cdot 0 &= 0 \\
-a \cdot \left[ 1 + (-1) \right] &= 0 \\
-a \cdot 1 + -a \cdot (-1) &= 0 \\
-a + \left[ -(-a) \right] &= 0 \\
-(-a) &= a \\
\text{QED} \\
\end{align}
$ Does everything look alright? Have I missed anything? Also--why is it necessary to show that the additive inverse is unique? Many thanks in advance.","['field-theory', 'analysis', 'proof-verification']"
1904065,Show that there does not exist a strictly increasing function $f : \mathbb Q \to \mathbb R$ such that $f(\mathbb Q) = \mathbb R$.,"The following exercise in an analysis text and I am trying to solve it without concepts of general topology but fail. Show that there does not exist a strictly increasing function $f : \mathbb Q \to \mathbb R$ such that $f(\mathbb Q) = \mathbb R$. Attempt 1. Suppose that the function $f(D) = \mathbb R$ is monotone. If its image $f(D)$ is an interval, then the function $f$ is continuous. So, if we suppose by contradiction that a strictly increasing function $f : \mathbb Q \to \mathbb R$ exists such that $f(\mathbb Q) = \mathbb R$ it must be continuous. Attempt 2. Since intersection of the set of irrational numbers of the domain is empty so by a convergence of a sequence in the domain $\mathbb Q$ in either case of converging to a rational or irrational number there is nothing to reach a contradiction. Attempt 3. The function $f$ is injective and it's not surjective so the inverse function is not defined such that I can use theorems about an inverse of a function. Please help!",['real-analysis']
1904066,"Has $(\max(0, x))^2$ continuous first derivative?","At first glance $(\max(0, x))^2$ looks smooth. If I'm not mistaken: if $x \le 0$ the derivative equals $2 \times\max(0, x) \times0 = 0$ if $x \ge 0$ the derivative equals $2 \times \max(0, x) \times 1$ and it equal zero if $x = 0$ Looks like derivative of $(\max(0, x))^2$ is continuous. But when I tried to check with Wolfram Alpha it shows me that derivative of $(\max(0, x))^2$ is indeterminate at $x = 0$. Link So can anyone tell me where I made a mistake?","['derivatives', 'calculus']"
1904093,Finding the determinant of a skew-symmetric matrix $K$,"Find the determinant of the skew-symmetric matrix $K$ $$K = \begin{bmatrix}
0 & 1 & 3\\
-1 & 0 & 4 \\
-3 & -4 & 0 \\
\end{bmatrix}$$ My Attempted Solution: I performed the following row operations to reduce $K$ into upper-triangular form $U$ $R_2 \leftrightarrow R_1$ $R_3 - (l_{31} = 3)R_1$ $R_3 \leftrightarrow R_2$ $R_3 - (l_{32} = -4) R_2$ $$U = \begin{bmatrix}
-1 & 0 & 4 \\
0 & -4 & -12 \\
0 & 0 & -45
\end{bmatrix}$$ From this I got 
$$\begin{align}
\det(K) &= \pm \  \det(U) \\
&= + \det(U) & \text{(Even no. of row exchanges)} \\
& = (-1)(-4)(-45) \\
&= 180
\end{align}$$ However the correct answer is $\det(K) = 0$. What could I have done wrong, I wouldn't think it would've been the row operations as the row operations apart from the row exchanges don't affect the $\det(K)$? Any hints or suggestiong are greatly appreciated","['matrices', 'linear-algebra', 'determinant']"
1904113,Limit $c^n n!/n^n$ as $n$ goes to infinity,"Let $c>0$ be a real number. I would like to study the convergence of $a_n := c^n n!/n^n$, when $n \to \infty$, in function of $c$. I know (from this question) that $n!>(n/e)^n$, so that $c^n n!/n^n>1$ for $c ≥e$. But this doesn't imply that the sequence goes to infinity. And I'm not sure what to do for $c<e$. I tried usual tests (D'Alembert...), without any success. I would like to avoid using Stirling approximation. Thank you for your help!","['sequences-and-series', 'limits']"
1904135,Is it possible to have a $f(\vec{r})$ satisfy this relation?,"It is known that:
$$
(\nabla^2+k^2)(-\frac{e^{ikr}}{4\pi r})=\delta(\vec{r})
$$
where $k>0$ and $\delta(\vec{r})$ is the three dimensional Dirac delta function . My question is, is it possible to find a function $f(\vec{r})$ that satisfies the following relation:
$$
\left[\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}-(\frac{\partial}{\partial z}+i\alpha)^2-k^2\right] f(\vec{r})=\delta(\vec{r})
$$ Where $\alpha>0,k>0$.","['partial-fractions', 'partial-differential-equations', 'greens-function', 'dirac-delta', 'ordinary-differential-equations']"
1904162,"Faster way to get a rational integral $\int_2^3 \frac{x^{100}+1}{x^3+1} \, dx$","So my problem is calculating this integral:
$$
\int_{2}^{3} \frac{x^{100}+1}{x^3+1} \, dx
$$
I know it can be done by polynomial division but that is really tedious I would have to divide $\approx 30$ times. And i know that Mathematica would give me the answer in the blink of an eye. Is there a clever way to do it pen&paper?","['integration', 'definite-integrals', 'calculus', 'rational-functions']"
1904173,Sober Topological Space and its associated site,"I was reading recently for Grothendieck topologies on an arbitrary category $\mathcal{C}$ (i.e. sites) and although did understand a couple of things I read something on Wiki article https://en.wikipedia.org/wiki/Grothendieck_topology for Grothendieck topologies which didn't get. If $X$ is a sober topological space, says, that we can recover the topological space again from its associated site. What does this sentence mean? How do we recover $X$ and under which construction does this happen? Thank you!","['category-theory', 'grothendieck-topologies', 'algebraic-geometry']"
1904218,$m+n$ divides $a_m+a_n$,"Let $a_1,a_2,\dots$ be a sequence of positive integers such that $m+n$ divides $a_m+a_n$ for all $m<n$. Is it necessary that $n$ divides $a_n$ for all $n$? Examples of such a sequence is $a_n=kn$ for some positive integer $k$. From $m+n$ dividing $a_m+a_n$ we see that if we fix $a_1,\dots,a_{n-1}$, then $a_n$ is fixed modulo $n+1,n+2,\dots,2n-1$.","['divisibility', 'sequences-and-series', 'elementary-number-theory']"
1904220,Conflicting definitions of continuity (strict or non-strict inequality)?,"On page 97 of Kreyzig's functional analysis book he provides a proof that a linear operator $T$ is continuous if and only if it is bounded. When proving that $T$ is continuous implies that $T$ is bounded he says that if we assume $T$ is continuous at some $x_0$ then for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert \le \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert \le \delta$. But on the previous page where he gave the definition of a continuous operator $T$ he used strictly less inequalities for $\varepsilon$ and $\delta$. That is, $T$ is continuous at some $x_0$ means that for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert < \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert < \delta$. How can he extend the strict inequality to a less than or equal to inequality?","['functional-analysis', 'continuity', 'definition']"
1904265,Decomposition of a module over an integral domain,"If $M$ is an $R$-module over an integral domain $R$, then must it be true that $M\cong \mathrm{Tor}(M) \oplus M/\mathrm{Tor}(M)$? I am interested in the case where $M$ has finite rank $n$, if that simplifies things. $M$ having rank $n$ means that $M$ has a maximal $R$-linearly independent set of size $n$.","['abstract-algebra', 'ring-theory', 'modules']"
1904277,Proof that the event that the series $\sum\limits_nX_n$ converges is in the tail sigma-field,"This is probably very stupid question, but I'm stuck. Suppose that $(X_n)_n$ is a sequence of independent random variables. Let $\mathcal{F}_n = \sigma(X_n)$. I am trying to proof that $\left\lbrace\omega\colon\sum_n X_n(\omega) < \infty\right\rbrace$ is a tail event. I suppose, that it should easily follow from the fact, that $\left\lbrace\omega\colon\sum_{k=n}^{\infty} X_k(\omega) < \infty\right\rbrace\in\sigma(\mathcal{F}_n, \mathcal{F}_{n+1}, \ldots)$. My problem is, I can't proof the last one. It makes me feel so stupid :(","['probability-theory', 'measure-theory']"
1904287,The set of all finite sub-sets of $\mathbb{N}$ is countable and the set of **all** sub-sets of $\mathbb{N}$ is continuum. My Proof.,"$K_\mathbb{N}$ set of all finite sub-sets of $\mathbb{N}$. Prove that the cardinal number of  $K_\mathbb{N}$ is countable. Also prove that the cardinal number of the set of all sub-sets of $\mathbb{N}$ is continuum($A_\mathbb{N}$). Create a bijection (or injection) between $K_\mathbb{N}$ and $\mathbb{N}$. First things first, I proved in class that the set of all sequences which members are from $\mathbb{N}$ is continuum. So can I just represent these subsets as sequences? (but rising sequences, so the mapping could be bijective). Then all I would have to do is modify the proof from class that the set of all rising sequences of naturals is continuum cardinality. Proving that $K_\mathbb{N}=\{a_i,...,a_n\}$is countable, I could maybe do the mapping: $$f(\{a_1,...,a_n\})=0,a_1...a_n$$ The codomain is a subset of $\mathbb{Q}$ which is countable. This function is injective at the very least, therefore $K_\mathbb{N}$ is countable. I do know how to make this function (bijection or injection) between $K_\mathbb{N}$ and $\mathbb{N}$.",['elementary-set-theory']
1904288,Why we use $2\cos\theta=\sqrt3$ to find $x^{72}+x^{66}$ given that $x+\frac1x=\sqrt3$?,"$x + \frac{1}{x} = \sqrt 3$  then find the value of $x^{72}+x^{66}$. My teacher said that in this case, where $x + \frac{1}{x}$ is equal to any value which is less than $2$, like $1$ or $\sqrt{3}$, just put this value in equal of $2 \cos \theta$ and after you will get value of $\theta$ after that divide the value $180$ by that theta value and after getting the result put that result  in equation $x^n + 1=0$,  as ""$n$"". Please explain what is basic concept behind this.","['algebra-precalculus', 'roots', 'complex-numbers']"
1904292,Determine all homomorphic images of $D_4$ up to isomorphism.,Determine all homomorphic images of $D_4$ up to isomorphism. What exactly is a homomorphic image without mentioning a second group?  Isn't a homomorphism a map between two groups?,"['finite-groups', 'abstract-algebra', 'group-theory', 'definition']"
1904310,Find the maximum of a real function of $2016$ variables,"Find the maximum of the function $$f(\bar x)=\sqrt{x_1^2+2x_2^2+......2016x_{2016}^2}$$ $\bar x=(x_1,x_2,....x_{2016}) \in \mathbb{R}^{2016}$ where the domain of $f$ is $$\bar x=(x_1,x_2,....x_{2016}) \in \mathbb{R}^{2016} :\sum_{n=1}^{2016}x_n^2=1.$$ $f^2(\bar x)={x_1^2+2x_2^2+......2016x_{2016}^2}=\sum_{n=1}^{2016}nx_n^2$ I don't know how to use condition $||\bar x||=1$.","['maxima-minima', 'real-analysis', 'optimization', 'functions']"
1904380,Showing that the dimension of a CW complex is well defined.,"This is from page 204 of Rotman's An Introduction to Algebraic Topology . After some elementary definitions and facts about CW copmlexes, exercise 8.27 asks: Define the dimension of a CW complex $(X,E)$ to be $$\mbox{dim }X=\mbox{sup}\{\mbox{dim}(e):e\in E\}.$$ If $E'$ is another CW decomposition of $X$, show that $(X,E)$ and $(X,E')$ have the same dimension. My attempt: To distinguish, let us write $\mbox{dim }(X)$ for $(X,E)$ and $\mbox{dim' }(X)$ for $(X,E')$. First consider the case when both dimensions are finite, say $\mbox{dim }(X)=m$ and $\mbox{dim' }(X)=n$ and assume for contradiction that $m<n<\infty$ holds. Choose an $n$-cell $e'$ in $(X,E')$ which must be open in $X$ since it is of largest dimension in $E'$. Viewing $X$ as $(X,E)$, $e'$ meets some of the cells in $E$ nontrivially, so let $e$ be such a cell in $E$ with maximal dimension. We now have $k=\mbox{dim}(e)\leq m<n$. Now what I want to show is that the intersection $e'\cap e$ must be open in $X$, but I can't proceed any further. I think that after this fact is established, one can use the Invariance of Domain to conclude easily that $k<n$ gives a contradiction: $e'\cap e$ is homeomorphic to some open subset of $\mathbb{R}^{n}$ and to some open subset of $\mathbb{R}^{k}$ at the same time. Is it true that for my choice of $e',e$, the intersection $e'\cap e$ is open in $X$? (I strongly believe that this is the case!) And how should one take care of the case when one of the dimension is infinite, say $n=\infty$? (for in that case we can't choose open $e'$ so easily...). Any help would be appreciated. Please enlighten me. EDIT: OK, I figured out that the intersection must be open in $X$. So the only remaining question is about how to handle the case $n=\infty$.","['algebraic-topology', 'general-topology', 'cw-complexes']"
1904411,Almost sure convergence and lim sup,"I'm struggling to understand the beginning of the solution to the following exercise: Let $(X_n)_{n\geq 1}$ and $X$ be random variables. Prove that $X_n \to X$ almost surely if and only if for every $\epsilon>0$
$$P(\limsup\limits_{n\to \infty}\{|X_n-X|\geq \epsilon\})=0$$ Solution: Notice that $\omega \in\limsup\limits_{n\to \infty}\{|X_n-X|\geq \epsilon\}$ iff there exist a subsequence $(n_k)_{k\geq 1}$ such that $|X_{n_k}-X|\geq \epsilon$, so we have
$$\{\limsup\limits_{n\to \infty}|X_n-X|> \epsilon\}\subset \limsup\limits_{n\to \infty}\{|X_n-X|\geq \epsilon\} \subset \{\limsup\limits_{n\to \infty}|X_n-X|\geq \epsilon\}\quad (*)$$ and so on.. I don't understand what this means. First what subsequence are we referring to? A subsequence that does converge? Why do we need this exactly? What's the difference between those 3 sets in line $(*)$. Why does it matter if the $\limsup$ is within the brackets and why does it matter whether $>$ or $\geq$? I really don't understand what the difference are. I'm happy if someone could explain it to me.","['probability-theory', 'limsup-and-liminf', 'measure-theory', 'convergence-divergence', 'random-variables']"
1904416,How do row operations affect $\det(U)$?,"'We can do row operations without changing $\det(A)$' - A quote from Introduction to Linear Algebra by G. Strang But let's say I have an arbitrary upper triangular matrix $U$ $$U = \begin{bmatrix}
a & a & a \\
0 & b & b \\
0 & 0 & c \\
\end{bmatrix}$$ And I perform the following row operations on $U$ to bring it to $U'$ $\frac{1}{a}R_1 \rightarrow R_1$ $\frac{1}{b}R_2 \rightarrow R_2$ $\frac{1}{c}R_3 \rightarrow R_3$ Then $U'$ is: $$U' = \begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1 \\
\end{bmatrix}$$ But now $\det(U) = abc$ and $\det(U') = 1$, thus $$\det(U) \neq \det(U')$$ All I've done is perform row operations on $U$ to bring it to $U'$, but by performing those row operations, their determinants lose equality. How can that be possible? So how is this seeming contradiction is resolved. I'm assuming that I must have some misconception either on row operations or on determinants. Furthermore on a deeper level, what geometric interpretation/meaning does scaling the rows as I've done bringing $U$ to $U'$, have on the determinant? Since the determinants of $U$ and $U'$ are obviously no longer equal, geometrically what is this scaling doing to the determinant?","['matrices', 'linear-algebra', 'determinant']"
