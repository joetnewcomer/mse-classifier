question_id,title,body,tags
3387814,At what rate do these common definitions converge to the constant $e=2.718...$?,"$e=2.718\ldots$ is often defined to be $$\lim_{n\to\infty} (1+1/n)^n$$ (from continuously compounded interest) and $$\lim_{n\to\infty}\sum_{k=0}^n \frac{1}{k!}$$ (from Taylor series). My question is, what are the respective rates of convergence for each of these? For example, is the error from $e$ in $O(\frac{1}{n})$ ? Or is the number of digits of accuracy in $O(n)$ ? If it's not too involved, a basic overview of a proof would also be appreciated. And just for bonus points, since I'm curious, what is the state-of-the-art algorithm for calculating digits of $e$ ?","['sequences-and-series', 'numerical-methods', 'limits', 'convergence-divergence', 'exponential-function']"
3387823,Definitions which were chosen because we write functions on the left,"Writing function application as a left action has been baked into mathematics since the Bernoullis in the 1700s (cf. this MSE question ). Because of this tradition, a lot of mathematics notation has been chosen to complement this. One (perhaps shocking) example is the commutator $[a,b]$ in a group. Traditionally we use left group actions (to be in line with left-function application), and so the conjugation action is $g \cdot h = ghg^{-1}$ . This leads us to define $[a,b] = aba^{-1}b^{-1}$ (so that, among other things, $s \cdot [a,b] = [s \cdot a, s \cdot b]$ ). It has become accepted to instead use right group actions, denoted $h^g = g^{-1}hg$ , and with this convention we would instead write $[a,b] = a^{-1}b^{-1}ab$ . This gives the identity $[a,b]^s = [a^s, b^s]$ . There are lots of places in mathematics where we have an arbitrary choice to make. Left/Right Cosets in algebra, Left/Right Linearity of the complex inner product, etc. I am curious if any of these conventions have to do with functions acting on the left or the right, as in the above example. Thanks in advance!","['notation', 'functions', 'big-list']"
3387854,How to find the number of roots of the equation $4\cos(e^x)=2^x+2^{-x}$?,"How to find the number of roots of the equation $4\cos(e^x)=2^x+2^{-x}$ ? Generally, for these kinds of problems I used to draw the graphs of the functions on either sides of the equation. Then find the number of points where they intersect each other, and hence the number of roots. But here, I face difficulty in drawing both the graphs of the functions (to actual scale) to determine the number of roots. I was able to somehow draw the graphs, but I don't know to draw them accurate enough to determine the number of roots. So,I used graphing calculator to determine the graphs and hence the number of roots as follows: Clearly, the number of roots is $4$ . Is there any other method of finding the number of roots of the given equation? If you are using the graphical method, please give some guidance on how to plot the graphs of such complicated functions (eg. $4\cos (e^x)$ ) accurate enough to determine the number of intersection points.","['alternative-proof', 'functions', 'roots', 'graphing-functions']"
3387871,On the definition of uniform convergence in probability,"Let $(\Omega,\mathcal F,\mathbb P)$ be a probability space, and, for each $y\in Y$ , let $\{X_n^y\}_{n\in\mathbb N}$ be a sequence of random variables converging uniformly (in $y$ ) in probability to a random variable $X^y$ . That is, $$ \lim_{n\to\infty} \mathbb P\left(\sup_{y\in Y} \left\vert X_n^y - X^y \right\vert > \varepsilon\right) = 0 \quad\forall\varepsilon>0. \tag{1}\label{1}  $$ This appears to be the standard definition of uniform convergence in probability. However, it seems to me that there is an alternative ""reasonable"" definition(*) of uniform convergence in probability, which is $$ \lim_{n\to\infty} \sup_{y\in Y}\mathbb P\left( \left\vert X_n^y - X^y \right\vert > \varepsilon\right) = 0 \quad\forall\varepsilon>0. \tag{2}\label{2}  $$ It is easy to see that \eqref{1} implies \eqref{2}. This leads me to the following questions. Why is \eqref{1} taken to be the standard definition of uniform convergence in probability? Is \eqref{2} in fact weaker than \eqref{1}? That is, is there a sequence of random variables that converge according to \eqref{2} but to not according to \eqref{1}? Are there natural conditions where \eqref{2} is equivalent to \eqref{1}? By ""natural conditions"" I mean conditions we typically expect sequences to satisfy, though I'm open to alternative interpretations of ""natural"". I imagine the answer to my first question has to do with the fact that \eqref{1} conforms more closely to our usual definition of uniform convergence on, say, a metric space, while \eqref{2} doesn't, but I haven't managed to formalise this properly. On the other hand, for \eqref{1}, I believe we need to take extra care to guarantee the measurability of $\sup_y \vert X_n^y - X^y \vert$ , but no such issue is faced by \eqref{2}. For my third question, I am happy to see answers that specialise to contexts where uniform convergence in probability is used often. One example of such a context is in stochastic processes, where $Y\subseteq [0,\infty]$ , and each $X_n^y$ might be (a.s.) càdlàg in $y$ . (*) By this I mean that if you told me to write down the definition of uniform convergence in probability, and I didn't already know the standard definition, I would be unsure about whether you meant \eqref{1} or \eqref{2}.","['measure-theory', 'convergence-divergence', 'probability-theory', 'real-analysis']"
3387917,Understanding Chinese Remainder Theorem using Schemes,"I was looking at Ravi Vakil's Algebraic Geometry notes and in 4.4.11, he states that the Chinese Remainder Theorem is a geometric fact that can be understood through schemes. He uses the example of $\mathbb{Z}/(60)$ to illustrate the sketch of the idea. I am trying to understand how to prove the statement in general using his sketch and ran into some trouble. First I'll explain his general sketch and what I have understood in the proof so far : The Chinese Remainder Theorem says that knowing an integer modulo 60 is the same as knowing an integer modulo $2$ , $3$ , and $5$ . What is Spec $\mathbb{Z}/(60)$ ? It is those prime ideals containing $(60)$ , i.e., $(2)$ , $(3)$ , and $(5)$ . This part is fairly simple. In the general version, this translates as follows: Let $n = p_1^{k_1}\cdot p_2^{k_2}\cdot \ldots \cdot p_r^{k_r} \in \mathbb{Z}$ Then Spec $\mathbb{Z}/(n) = \{(p_1), (p_2), \ldots,(p_r)\}$ (With the standard affine structure sheaf, which we shall denote as $\mathcal{O}$ ) They (here, (2),(3), and (5)) are all closed points, as these are maximal ideals, so the topology is the discrete topology Likewise, in the general case this also holds, i.e, each $(p_i)$ is closed since it's maximal and so Spec $\mathbb{Z}/(n)$ has a discrete topology. What are the stalks? You can check that they are $\mathbb{Z}/4$ , $ \mathbb{Z}/3$ , and $\mathbb{Z}/5$ . Here's where I had to work some things out. This is what I understood to be the case in general. Claim: $\mathcal{O}_{(p_i)} = (\mathbb{Z}/(n))_{(p_i)} \simeq \mathbb{Z}/(p_i^{k_i})$ for each $1 \leq i \leq r$ Proof: Let $\pi:\mathbb{Z}/(n) \rightarrow \mathbb{Z}/(p_i^{k_i})$ be the canonical map given by $\bar{x} \mapsto \bar{x}$ . Let $i:\mathbb{Z}/(n) \rightarrow (\mathbb{Z}/(n))_{(p_i)}$ be the standard inclusion $\bar{x} \mapsto \frac{\bar{x}}{1}$ Since $\pi(\mathbb{Z}/(n)-(p_i))$ is the set of units in $\mathbb{Z}/(p_i^{k_i})$ , by the universla property of localization, we have a morphism $\psi:(\mathbb{Z}/(n))_{p_i} \rightarrow \mathbb{Z}/(p_i^{k_i})$ such that $\psi \circ i = \pi$ . Similarly, since $i(\bar{p_i}^{k_i}) = \frac{\bar{p_i}^{k_i}}{1} = \frac{\bar{n} }{p_1^{k_1}\cdot p_2^{k_2}\cdot \ldots \cdot p_{i-1}^{k_{i-1}} \cdot p_{i+1}^{k_{i+1}}\cdot \ldots \cdot p_r^{k_r}} = 0$ , by the universal property of quotients, we have a morphism $\gamma : \frac{\mathbb{Z}/(n)}{(p_i^{k_i})} \simeq \mathbb{Z}/(p_i^{k_i}) \rightarrow (\mathbb{Z}/(n))_{(p_i)}$ such that $\gamma \circ \pi = i$ It isn't too difficult to show that $\gamma$ and $\psi$ are inverses of each other. Hence the claim is proved. So far so good (I think). So what are the global sections on this scheme? (here, on $\mathbb{Z}/(60)$ ) They are the sections on this open set $(2)$ , this other open set $(3)$ , and this third open set $(5)$ . In other words, we have a natural isomorphism of rings $\mathbb{Z}/(60) \rightarrow \mathbb{Z}/(4) \times \mathbb{Z}/(3) \times \mathbb{Z}/(5)$ Here's where I'm a little lost in translation. There are three things I don't quite get. 1) If we're looking at global sections on the scheme Spec $\mathbb{Z}/(60)$ , then don't we need to look at $\mathcal{O}($ Spec $\mathbb{Z}/(60))$ and not any of $\mathcal{O} ((2))$ , $\mathcal{O} ((3))$ , and $\mathcal{O} ((5))$ ? I'm still trying to understand schemes and sheaves so I want to understand rigorously what Vakil means here. 2) I know that we can look at $(p_i^{k_i})$ as the principal open $\mathcal{D} $ := D $((p_1^{k_1}\cdot p_2^{k_2}\cdot \ldots \cdot p_{i-1}^{k_{i-1}} \cdot p_{i+1}^{k_{i+1}}\cdot \ldots \cdot p_r^{k_r}))$ and so $\mathcal{O}(\mathcal{D}) = (\mathbb{Z}/(n))_{(p_1^{k_1}\cdot p_2^{k_2}\cdot \ldots \cdot p_{i-1}^{k_{i-1}} \cdot p_{i+1}^{k_{i+1}}\cdot \ldots \cdot p_r^{k_r})}$ I don't see why this localization should give us $\mathbb{Z}/(p_i^{k_i})$ as (I think) he seems to be suggesting when he is looking at sections of $\mathcal{O}_{\text{Spec}\mathbb{Z}/(60)}$ at $(2)$ , $(3)$ , and $(5)$ . 3) How does looking at these global sections automatically give us the desired isomorphism? I don't see the connection between looking at (global) sections of the sheaf and immediately deducing the required isomorphism. I'm still trying to learn how schemes work and am trying to develop an intuition for them. Because of that, I'm not very fluent with translating Vakil's less-rigorous questions to very rigorous claims (Or at the very least, they don't seem to rigorous to me. Though I'm well aware that that's solely a lack of understanding on my part). I'd appreciate any help and thanks for taking the time to read this! Edit: Fixed the typo mentioned in the first answer","['affine-schemes', 'chinese-remainder-theorem', 'algebraic-geometry', 'schemes']"
3387930,Linear regression model with a known parameter,"Question: Consider the following linear regression with one parameter (intercept $\beta_{0}$ known). $ y_{i} = \beta_{0}^{*} + \beta_{1}x_{i} + \epsilon_{i} $ for i = 1,....n a) Compute the LSE , MLE , mean and varience for $\beta_{1}$ . I understand how to derive the following when both the slope and intercept is unknown. However I'm unsure what the effect of knowing what the intercept will have on the new computations. Attempt: We minimsise the LSE by setting $\partial_{\beta_{1}} S(\beta_{1}) = - \sum_{i=1}^{n} 2x_{i}(y_{i}-\beta_{0}^{*}-\beta_{1}x_{i}) = 0 $ After long computations we get $\beta_{1} = \frac{\sum_{i=1}^{n} x_{i} (y_{i} - \beta_{0}^{*})   }{\sum_{i=1}^{n}(x_{i}^2)}$ The computation of the least square estimator doesn't seem to care wether $b_{0}$ is known or unknown so would be get the same LSE,MLE,mean and variance as with the case $b_{0}$ is unknown","['linear-regression', 'statistics']"
3387964,Proof $P(X<Y)=1/2$ when $X$ and $Y$ i.i.d continuous (symmetry),I'd like to prove with the integral that $P(X < Y)=1/2$ when $X$ and $Y$ i.i.d continuous random variable (they are symmetry therefore). I tried with convolution calling $Z=X-Y$ and $$P(Z<0)=\int\int f_X(z+y)f_Y(y)dydx$$ but then I wasn't able to reach a result.,"['convolution', 'symmetry', 'probability-theory', 'random-variables']"
3388041,Show that the following expressions are divisible by the following numbers/values,"1) Suppose n is an arbitrary integer (i) Show that $n(n + 1)$ is divisible by $2$ (ii) Show that $n(n + 1)(n + 2)$ is divisible by $3!$ attempt: Not sure if it's correct. i) $n^2 + n = $ even number Two odd numbers added together must be an even number, so $n^2 + n$ will always be an even number, so it is divisible by $2$ . ii) note sure how to do.","['quantifiers', 'discrete-mathematics']"
3388065,"For each set, determine if it is compact, and if not, why not","For each set, determine if it is compact, and if not, why not(it's not closed or not bounded $?$ ) $1.\{(x,y):1<e^{x^2+y^4}(x^4+y^2+1)<2\}$ $2. \{(x,y,z):|x|+|y|+|z|≤3\}$ $3. ⋃_{n∈\mathbb{Z}^+}[0,2−\frac{1}{n}]⊂\mathbb{R}$ $4. \{(x,e^x):0≤x≤10\}$ $5. \{(x,y):1≤e^{x^2+y^4}(x^4+y^2+1)≤2\}$ $6. \{(x,y):y=\sin(x)\}$ I checked the definition of compect set Def. compect set A set $S⊆\mathbb{R}^n$ is said to be compact if every sequence in $S$ has a subsequence that converges to a limit in $S$ . Seems like if the set isn't compact, then it's either not bounded or not closed, Take the contrapositive have bounded and closed implies compact $\dots$ My attempts $1.$ seems bounded but not closed so not compact $2.$ closed and compact so compact $3.$ union of closed set is closed and closed set are bounded so compact $4.$ bounded so compact $5.$ bounded so compact $6.$ not bounded so not compact Which might not be correct $\dots$ I'm a little confused about those concepts, Any help would be appreciated.",['multivariable-calculus']
3388140,Show that $(1+x)^{(1+x)}>e^x$,"How can I prove that $(1+x)^{(1+x)}>e^x$ for all $x>0$ ? The problem arose as I tried to prove the well-known & intuitive econometric principle that the more often you compound your interest, the more interest you ultimately get (in maths, that $\frac{d}{dn}((1+\frac{1}{n})^n)>0$ for $n>0$ ). An interesting further problem is to prove that $(a+x)^{(a+x)}>e^x$ is true for all $x>0$ if and only if $a\geq1$ .","['inequality', 'analysis']"
3388171,"Solve the PDE $xu_y-yu_x=0$ with $u(x,0)=x^2$ using the Method of Characteristics","The following PDE is given: $xu_y-yu_x=0$ with $u(x,0)=x^2$ The following topics; $yU_x-xU_y=1, U(x,0)=0$ Solution of the PDE $yu_x+xu_y=0$ subject to the initial condition $u(x,0) = \exp \left(-\frac{x^2}{2}\right)$ Solving $-yu_x+xu_y = u$ using method of characteristics did not solve my troubles, because the PDE is not equal to 0, and the initial condition is different. I am stuck near the end of the problem. I used the method of characteristics for PDE: $x_t=-y, y_t=x, u_t=0, x(0,s)=s, y(0,s)=0, u(0,s)=s^2$ $x'(t)=-y(t)$ thus $ x''(t)=-y'(t)=-x(t)$ so $x(t)=c_{11}cos(t)+c_{12}sin(t)$ . Using $x(0,s)=s$ we get $c_{11}=s, c_{12}=0$ thus $x(t,s)=s\cos{t}$ Same procedure for $y'(t)=x(t), y''(t)=x'(t)=-y(t)$ thus $y(t,s)=s\cos{t}$ And for $u$ , we get $u(t,s)=s^2$ Now, how exactly do I solve for $t$ and $s$ to write $u$ as a function of $x$ and $y$ (and not $t$ and $s$ )? I cannot solve for $t$ or $s$ using $x$ and $y$ given that $x(t,s)=y(t,s)$ , i.e. they are identical to each other. That is my first question. My second question is: if the initial condition was $u(x,0)=x, x > 0$ instead of $u(x,0)=x^2$ (so no square, and without $x>0$ ), do we agree that it would only change the result for $u$ , i.e. for $x(t,s)$ and $y(t,s)$ , we would get the same result than here. Thank you for taking your time to help me. Edit: As @Mattos correctly points out, $y(t)=s\sin(t)$ . That changes a lot of things. Now, we can use $x^2+y^2=s^2(\cos{x}^2+\sin{x}^2)=s^2=u$ . Thus, $u(x,y)=x^2+y^2$ .","['transport-equation', 'partial-differential-equations', 'analysis', 'real-analysis']"
3388172,An inequality for polynomials with positives coefficients,"I have found in my old paper this theorem : Let $a_i>0$ be real numbers and $x,y>0$ then we have : $$(x+y)f\Big(\frac{x^2+y^2}{x+y}\Big)(f(x)+f(y))\geq 2(xf(x)+yf(y))f\Big(\frac{x+y}{2}\Big)$$ Where : $$f(x)=\sum_{i=0}^{n}a_ix^i$$ The problem is I can't find the proof I made before . Furthermore I don't know if it's true but I have checked this inequality a week with Pari\Gp and random polynomials defined as before . So first I just want a counter-examples if it exists . If it's true if think it's a little bit hard to prove . I have tried the power series but without success . 
Finally  it's a refinement of Jensen's inequality for polynomials with positives coefficients . Thanks a lot if you have a hint or a counter-example . Ps:I continue to check this  and the equality case is to $x=y$","['jensen-inequality', 'examples-counterexamples', 'multivariable-calculus', 'polynomials', 'inequality']"
3388260,Computing a Feynman integral,"Reading the lectures notes by A. Connes and M. Marcolli I have some difficulty undestanding how they compute the Feynman integrals using integration by parts. Consider the following integral of formulas (1.32), (1.34): $$
  I = \mathcal N_0∫ φ(x)φ(y) \, \exp(iS_0(φ)) \, \mathcal D[φ],
$$ where $\mathcal N_0$ is a normalization factor $$
  \mathcal N_0^{-1} = ∫ \exp(iS_0(φ)) \, \mathcal D[φ],
$$ and $$
  S_O(φ) = (2\pi)^{-D} ∫ \frac 1 2 (p^2 - m^2) \hat φ(p) \hat φ(-p) \, d^D p, \\
\hat φ(p) = ∫ \exp(-i p x) φ(x) \, d^D x.
$$ The authors say that using formal integration by parts (note that the the integrand is a product of two linear forms in the fields times the exponent of a quadratic form in the fields) one gets something like: $$
I = i (2\pi)^D ∫ \frac{\delta(p_1+p_2)}{p_1^2-m^2} e^{\pm ip(x-y)} d^D p_1 d^D p_2
$$ I do not understand this computation. Could you please explain it?","['quantum-mechanics', 'functional-analysis', 'quantum-field-theory']"
3388263,Why does $561$ divides $3^{561}-3$ if it doesn't divide $3$ nor $3^{560} -1$,"$561$ is a Carmicheal number.
I was asked in an exercise to prove that it is so by proving that $561 | a^{561}-a$ for any integer a. Now, if $561 | 3^{561}-3$ then $561|3*(3^{560}-1)$ But since $\gcd(561,3) > 1$ then it is false that $561 | 3^{560}-1$ (Isn't it?) Then $561 | 3$ which is false. Then by counterexample it would be false that $561 | a^{561}-a$ for any integer a. What I am missing in the argument? Thanks.","['number-theory', 'discrete-mathematics', 'prime-numbers']"
3388293,Help solving the non-linear ODE $\frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t}$,"The particular solution given in the question is $u(t) = t$ . I know that this is a Ricatti differential equation, therefore it's solution is $y=u(t)+\frac{1}{v(t)}$ . So first I've write the ODE in the Ricatti form: $$\frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t}$$ $$\frac{dy}{dt} = y^2(t^3)+y(-2t^4+\frac1t) +t^2$$ So I've used the substituition $y=t+\frac{1}{v(t)} \Rightarrow y' = 1-\frac{dv}{v^2dt}$ . $$t^3y^2 + y(-2t^4+\frac1t )+t^2 = 1 - \frac{dv}{v^2dt}$$ $$...$$ In the end, I've got: $$-t^5+\frac{1}{v^2}+\frac{1}{vt}=-\frac{dv}{v^2dt}$$ $$-t^5v^2+1+\frac{v}{t}=-\frac{dv}{dt}$$ But, the final equation would be a first order linear ODE, and this one isn't. Can you help me solving this? --
PS: Should I try to solve this new equation again using Ricatti? I will try to do this now. But, if you have any suggestions, please tell me",['ordinary-differential-equations']
3388302,Proper coloring of a cuboid.,"Question: The different faces of a cuboid, having the measurements $1 \times 2 \times 2$ cm, is to be painted. Colors at disposal are blue, yellow, and red. In how many ways can this be done, if two colorings are considered as being equivalent when one of them can be obtained from the other one by a rotation of the cuboid? I was thinking it may be something in this way? Because the rotations should be the same as the ones for a cube? right? one identity element which leaves all 3^6 elements of X unchanged six 90-degree face rotations, each of which leaves 3^3 of the elements of X unchanged three 180-degree face rotations, each of which leaves 3^4 of the elements of X unchanged eight 120-degree vertex rotations, each of which leaves 3^2 of the elements of X unchanged six 180-degree edge rotations, each of which leaves 3^3 of the elements of X unchanged Then after putting the numbers inte Burnsdies formula i get the answer 57, someone who can confirm that im doing this the right way?","['polya-counting-theory', 'abstract-algebra', 'combinatorics']"
3388310,"Show that in any set of $2n$ integers, there is a subset of $n$ integers whose sum is divisible by $n$.","There was a problem in a recent programming competition which my friend solved by assuming the following conjecture: Show that for any set of $2n$ integers, there is a subset of $n$ integers whose sum is divisible by $n$ . I have thought about this problem for a while but can't seem to prove it, but I couldn't come up with a counter-example either. A similar problem has a well-known solution: show that for any set of $n$ integers, there is a non-empty subset whose sum is divisible by $n$ . The proof goes as follows. Suppose the set is $\{x_1, x_2, \dots, x_n\}$ and hence define $s_i = \left(x_1 + x_2 + \dots + x_i\right)\bmod n$ , with $s_0 = 0$ . Then we have the set $\{s_0, s_1, \dots, s_n\}$ with $n+1$ elements, but each $s_i$ can take only $n$ distinct values, so there are two $i, j$ with $i\neq j$ such that $s_i = s_j$ . Then $s_j - s_i = x_{i+1} + x_{i+2} + \dots + x_j$ is divisible by $n$ . However, this approach can't directly be applied to this problem since now we need to ensure that we choose exactly $n$ integers.","['pigeonhole-principle', 'combinatorics', 'modular-arithmetic']"
3388334,"Clarke's tangent cone, Bouligand's tangent cone, and set regularity","For a set $C$ (which may not be convex) and a point $x\in C$ : Bouligand's Tangent cone is defined as $$
T(C,x) = \left\{v : \lim_{\theta\to 0_+} \inf \frac{d(x+\theta v, C)}{\theta} = 0\right\}
$$ and where $d(x,C) = \min_{y\in C} \|x-y\|$ the distance from a point to a set. Clarke's tangent cone is $$
T_C(C,x) = \left\{ v : \lim_{y\to x, y\in C, \theta\to 0_+} \frac{d(y+\theta v,C)}{\theta} = 0 \right\}
$$ A set is regular if $T(x,C) = T_C(x,C)$ for all $x\in C$ . My questions are a bit general, as I'm trying to build intuition. If a set is convex, then is it always regular? (Including possibly infinitely-dimensional sets? What if we restrict to finite-dimensional sets?) Would it be fair to say that here, both definitions boil down to the ""usual"" tangent cone definition, e.g. $$
T_0(C,x) = \lim_{r\to 0}\mathrm{cone}(\{y\in C: \|x-y\|\leq r\})
$$ Do funny things happen if $C$ is a low dimensional subspace (e.g. convex but unbounded and with empty interior?) Now assume that I have a set which is nonconvex, shaped like a cashew (e.g. no nonsmooth points.) Then it seems like the tangent cone at any point is just a halfspace, using either definition. Does this seem true? Now assume that I have a set which is ""pointy"" and nonconvex, like Pacman. In particular, take $x$ to be the point most inside Pacman's mouth. More precisely, consider $$
C = \{x : \|x\| \leq 1\} \cap \{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\}
$$ for some $\pi/2 > \alpha > 0$ , 
and take $x = 0$ . I suppose the tangent cone, using either definition, at this point, is the set $\{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\}$ , and the normal cone, defined as the polar of the tangent cone, is empty (in both definitions). Does this sound sensible? Finally, the main question is: what is an example of a set which is not regular? I presume such sets must be nonconvex; can they also be finite-dimensional? What about compact / closed / bounded? Thanks for any discussion!","['convex-analysis', 'non-convex-optimization', 'analysis']"
3388367,How to find the sum of digits of a number whose prime factorisation is given,"For example, if $n = 15^2 \times 5^{18}$ in base $10$ . Is there a way to find the upper and lower bounds for the sum of digits of this number? The answer is given to be $6\leq s \leq 140$ .
Also, is there a way to calculate the exact sum? Also, how do i find the number of digits of this number?","['number-theory', 'elementary-number-theory']"
3388370,Proof that a metric space $X$ isometrically embeds to the set of bounded continuous functions from $X$ to $\mathbb R$,"Given any metric space $(X,d)$ there is an embedding from $X$ to $C_b(X,\mathbb R)$ , which is the set of bounded continuous functions from X to $\mathbb R$ , with the $\ell_{\infty}$ norm. Given an $x_0\in X$ let a function $G$ be defined where $x\in X$ goes to the function $f_x$ defined as $f_x(y)= d(y,x)-d(y,x_0)$ . Showing that $G$ is a function and that for all $x$ , $G(x)=f_x$ is a continuous bounded function from $X$ to $\mathbb R$ is easy. What I don't understand is how $G$ is an isometric embeding. My approach was given two $f_x$ and $f_z$ their distance is clearly less or equal to $d(x,z)$ . To show they are greater or equal I note that $d(f_x, f_z)$ (defined by the $\ell_\infty$ norm) must be greater than $d(f_x(x),f_z(x))$ (this is in the metric $d$ of $X$ ) which in turn is equal to $d(x,z)$ , Is this correct? Am I missing something?","['isometry', 'functions', 'metric-spaces']"
3388380,"Prove that $\int \binom{n}{k}p^k(1−p)^{n−k}\,dp$ from $0$ to $1$ is $\frac{1}{n+1}$","Prove that $$
\int_0^1 \binom{n}{k} p^{k}( 1-p ) ^{n-k}dp=\frac{1}{n+1}.
$$ I met this question in ""The Art and Craft of Problem Solving"". The whole question is from Bay Area Math Meet 2000, and by using an alternative method to interpret that question, we can get the result. The book then asked us to use 2 different methods to solve it, repeat integration by part. This is easy, I solve it. Using manipulation of the binomial series. I can not solve the question using this method. How can I use the 
binomial series to solve it?","['integration', 'definite-integrals', 'alternative-proof', 'calculus', 'binomial-coefficients']"
3388452,FInitary functors-a definition,"Can someone complete and correct this definition of a finitary functor and perhaps give a link to the definition? $F:\text{Set}\to \text{Set}$ is finitary iff for every $X$ and $x\in X$ there is a finite $Y$ and $i:Y\to X$ with $Fx\in Fi[FY]$ . The situation is that I've seen this at the blackboard in a seminar on category theory but haven't caught the details of the defintion. The equivalent definition of a finitary functor should be that it preserves directed colimits, but I do not see the equivalence of these two definitions
(this and the one almost correct above) either.","['elementary-set-theory', 'functors', 'category-theory']"
3388500,Proof Lyapunov function for a given system,"I have the following exercise: Let $V : \Re^{L} \rightarrow \Re$ be twice continuously differentiable. Let $x^{*}$ be an isolated minimum of $V$ . Show that $V$ is a strict Lyapunov function for the system $$ \dot{x} = -\nabla V(x) = (-\frac{\partial{V}}{\partial{x_{1}}}, ...,-\frac{\partial{V}}{\partial{x_{L}}}) $$ and deduce that $x^{*}$ is asymptotically stable. I understand that I need to show that $V(x^{*})= 0$ , $V(x)>0 \; \text{if} \; x \in \Re^{L}  \text{ \ {$x^{*}$}}$ , and $\dot{V} < 0 $ in $\Re^{L}$ . I am able to get the following: $$ \dot{V} = <\nabla V, \dot{x}> = \frac{\partial{V}}{\partial{x_{1}}}(-\frac{\partial{V}}{\partial{x_{1}}}) + ... + \frac{\partial{V}}{\partial{x_{L}}}(-\frac{\partial{V}}{\partial{x_{L}}}) = \sum_{i = 1}^{L}-(\frac{\partial{V}}{\partial{x_{i}}})^{2}$$ $$ \Rightarrow \dot{V} < 0 $$ . I am unable to show that $V(x^{*}) = 0 $ and that $V(x)>0 \; \text{if} \; x \in \Re^{L}  \text{ \ {$x^{*}$}}$ . I understand the argument going from Lyapunov function to these facts but not the other way around or how to show this here. Any help/tips are welcome. Thanks in advance!","['stability-in-odes', 'lyapunov-functions', 'ordinary-differential-equations']"
3388508,Do the squares of an arithmetic progression ever sum to a power of three?,"Can it be shown that $$\sum_{q=0}^{u}(n+qd)^{2}\ne 3^t \ \ \ \ \forall n,d,u,t\in\mathbb{N}$$ Where we let $\mathbb{N}$ denote positive integers. I am not confident there is no counterexample. Edit, My attempted If $u=4k-1$ then $\sum_{q=0}^{u}(n+qd)^{2}\ne 3^t $ Formula $$ \sum_{q=0}^{u}(n+qd)^{2} =n^2(u+1)+d(2n+d)\frac{(u+1)u}{2} +d^2\frac{(u+1)u(u-1)}{3} \  \ \ \ \  \  ...eq(1)$$ Proof Let's suppose $\sum_{q=0}^{u}(n+qd)^{2}=3^t $ By $eq(1)$ we can write $2×3^{t+1}=6n^2(u+1)+3d(2n+d)(u+1)u+2d^2(u+1)u(u-1)$ Now consider $u=4k-1$ $\implies  3^{t+1}=12n^2k+6d(2n+d)k(4k-1)+8d^2k(4k-1)(2k-1) $ We know $12n^2k=even$ $6d(2n+d)k(4k-1)=even$ $8d^2k(4k-1)(2k-1)=even$ And $even +even +even =even\ne 3^{t+1}$ It's show complete proof for $u=4k-1$","['number-theory', 'elementary-number-theory']"
3388573,$ABC$ is a triangle such that $BC = 74 cm$ and point $D$ is on $BC$ so $BD=14cm$.If $\angle ADB=60^\circ$ then what is the area of triangle $ABC$?,"In the figure below, $ABC$ is a triangle such that $\angle BAC = 150^\circ$ , $BC = 74 cm$ and point $D$ is on $BC$ such that $BD=14cm$ .If $\angle ADB=60^\circ$ ,then what is the area, in $cm^2$ , of triangle $ABC$ ? I have no idea how to solve this, I think the height can somehow be calculated, but I dont know how, help aswell as solutions would be appreciated Taken from the 2019 IMC","['contest-math', 'euclidean-geometry', 'area', 'geometry']"
3388699,What is the ring defining the intersection of two open affines in a variety?,"Suppose we have two affine open subschemes $X=Spec(A)$ and $Y=Spec(B)$ of a separated scheme $Z$ (let's even assume that $Z$ is a $k$ -variety). Then we know that the intersection $X\cap Y$ is an affine subscheme. My question is: what is the coordinate ring of $X\cap Y$ ? The way to show that the intersection is affine is to notice that $X\cap Y\cong (X\times_k Y)\cap \Delta$ , where $\Delta$ is the diagonal. Since $Z$ is separated, $(X\times_k Y)\cap \Delta$ is a closed subscheme of the affine scheme $X\times_k Y=Spec(A\otimes_k B)$ and hence must be affine. So now the question becomes: what is the surjective ring map inducing this closed immersion? I want to say it's just $f\colon A\otimes_k B\rightarrow k[A,B], \, a\otimes b\mapsto ab$ , where by $k[A,B]$ I just mean the $k$ -algebra generated by $A$ and $B$ . Then we would just have $X\cap Y=Spec((A\otimes_k B)/I)$ , where $I=ker(f)$ . Ravi Vakil uses a similar map in Proposition 10.1.3 to show that the diagonal morphism is a locally closed embedding. But I don't understand what happens when we restrict to $X\times_k Y$ .","['affine-schemes', 'algebraic-geometry', 'affine-varieties']"
3388703,"Studying continuity of $f(x,y)=\frac{x+y}{x-y}$","Let $$f(x,y)=\left\{
\begin{array}{ll}
      \frac{x+y}{x-y} & , x \neq y \\
      1 & , x=y \\
\end{array} 
\right.$$ I want to see whether $f(x,y)$ is continuous at points $\{(x,y)\in \Bbb{R}^2:x=y\}$ . In understand I have to compute a limit, but I am not sure how to do it (not sure about notation either). I see that we need $x \to y$ somehow, but these are both variables here. How to deal with this? Is something like \begin{equation}
\lim_{(x,y) \to (x,x)} \frac{x+y}{x-y}
\end{equation} correct? Does this restrict how we approach the function in any way? What about defining $x=y+\varepsilon$ and then studying $\varepsilon \to 0$ ?","['multivariable-calculus', 'limits', 'calculus', 'continuity']"
3388707,How to prove that automorphisms of $\mathbb P^n$ arise from linear maps in $\mathbb A^{n+1}$?,"Let $U_n$ be the space obtained by removing the origin from $\mathbb A^{n+1}$ . Of course, $k^\star$ acts multiplicatively on $U_n$ , and the orbit space $U_n / k^\star$ is the usual projective space $\mathbb P^n$ . I would like to argue that every automorphism $\varphi : \mathbb P^n \to \mathbb P^n$ arises from an automorphism $\tilde \varphi : U_n \to U_n$ that respects the orbits. If I were working with nice Hausdorff manifolds, then I could say something like Note that $U_n$ is simply connected, because it retracts to a sphere. Thus, if we lift $\varphi$ to an automorphism of some covering space of $U_n$ , this covering space turns out to be $U_n$ itself. But in algebraic geometry land, our spaces are topologically horrible, so this doesn't work. Is there any covering space theory to begin with? What can I do?","['algebraic-geometry', 'algebraic-topology']"
3388737,question about the notation of conditional expectation,"Let $X$ be a standard normal random variable. We need to compute the integral $$E(X~|X>0).$$ In the book, they give the answer ${1\over \sqrt{2\pi}}$ . Then I am confused with this conditional expectation notation.
Since from my understanding, it should be $$\int_0^\infty 2\cdot {x\over \sqrt{2\pi}} e^{-{x^2\over 2}}\, dx$$ which gives the answer $\sqrt{{2\over\pi}}$ . I add a factor ""2"" here. Can anyone tell me this conditon expectation $E(X|X>0)$ notation meaning? Since it is not standard, we usually conditioning on a $\sigma$ -algebra, rather than a set.","['conditional-expectation', 'conditional-probability', 'probability-theory', 'probability']"
3388744,Antiderivative of a branch function,"Let $ f: \mathbb{R} \mapsto \mathbb{R} $ , $ f(x) = e^{x} $ for $ x \leq 0 $ and $ ax+b $ for $ x > 0 $ . I want to determine the real numbers $ a $ and $ b $ for which $ f $ admits antiderivatives. I took a plausible antiderivative $ F(x) = e^{x} + c $ for $ x < 0 $ , $ F(0) = \alpha $ and $ a \frac{x^{2}}{2} + bx + d $ for $ x > 0 $ and using the fact that $ F $ must be first continuous and then differentiable, I showed that $ b = 1 $ . This is easy to see as from continuity we must have that $ d = \alpha = c+1 $ . Then, since F is clearly differentiable on $ \mathbb{R} \setminus \{0\} $ if we impose the condition that the left and right derivatives at $ 0 $ of $ F $ to be equal so we must have: $$ \lim_{x\rightarrow 0, x <0 }\frac{F(x)-F(0)}{x-0} = \lim_{x\rightarrow 0, x > 0 }\frac{F(x)-F(0)}{x-0}$$ Thus: $$ \lim_{x\rightarrow 0, x <0 }\frac{e^{x}+c-c-1}{x} =  \lim_{x\rightarrow 0, x > 0 }\frac{a\frac{x^{2}}{2}+bx+c+1-c-1}{x} $$ Which gives us $ b = 1 $ . My question is then, does this suffice? Can $ a $ be chosen arbitrarily such that $ f $ has antiderivatives as long as $ b = 1 $ . It would seem that the function $ F(X) = e^{x}+c $ for $ x \leq 0 $ and $ F(x) = a\frac{x^{2}}{2}+x+c+1 $ for $ x > 0 $ for $ c \in \mathbb{R} $ does indeed satisfy the necessary conditions. Is it maybe that I am missing something here? I would appreciate any comments. Thank you!","['integration', 'derivatives', 'real-analysis']"
3388745,Kiefer bound on variance of unbiased estimator,"I am trying to work through the very short paper entitled ""On Minimium Variance Estimators"" by J. Kiefer (1952). In this work he gives two explicit examples on calculating the lower bound on the variance of the two random variables. In both cases, he calculates the bound two different ways, using equation (3) and (4) of his paper. Eq. 4 of his paper is the Chapman-Robbins bound, whereas Eq. 3 is a more general Barankin type bound. I think I understand how he got his Chapman-Robbins result, but I am confused as to how he calculates the more general bound. Ill now paraphrase his first example:
For n observations on a uniform distribution from $0$ to $\theta$ , $(\Omega =\{\theta|\theta>0\})$ , the random variable Y is the maximum of those observations with pdf $f(y;\theta)=ny^{n-1}/\theta^{n}$ for $0\leq y\leq \theta$ and is $0$ elsewhere. The Chapman Robbins bound on a pdf $f(y;\theta)$ is given by \begin{align}
E_{\theta}(t-\theta)^2\geq[\inf_h\frac{1}{h^2}(\int_{\chi}\frac{(f(y;\theta+h))^2}{f(y;\theta)}\text{d}\mu-1)]^{-1}
\end{align} , for any $t(y)$ such that $E_{\theta}t=\theta$ . Kiefer finds that for $n=1$ , $[(\int_{\chi}\frac{(f(y;\theta+h))^2}{f(y;\theta)}\text{d}\mu-1)]=-\frac{1}{h(\theta+h)}$ , which in turn yields a bound of $\theta^2/4$ . Lets start by deriving this result in more detail. We have that $\frac{(f(y;\theta+h))^2}{f(y;\theta)}=\frac{\theta}{(\theta+h)^2}$ , and $\chi$ has support from $0$ to $\theta+h$ (note that $-\theta<h<0$ ), so $\int_{\chi}\frac{(f(y;\theta+h))^2}{f(y;\theta)}\text{d}\mu=\int_{0}^{\theta+h}\frac{\theta}{(\theta+h)^2}\text{d}y=\frac{\theta}{(\theta+h)}$ , and so we have that $[\inf_h\frac{1}{h^2}(\frac{\theta}{(\theta+h)}-1)]=\inf_h(-\frac{1}{h(\theta+h)})$ as desired. This term obtains its minimum at $h=-\theta/2$ , and is $4/\theta^2$ . The Chapman-Robbins bound is the inverse of this term, $\theta^2/4$ . This coincides with what Kiefer found in his paper. Now for the more general Barankin type bound, which Kiefer gives as \begin{align}
E_{\theta}(t-\theta)^2\geq\sup_{\lambda_1}\left[(E_1(h))^2\left(\int_{\chi}\frac{[\int_{\Omega_{\theta}}f(y;\theta+h)\text{d}\lambda_{1}(h)]^{2}}{f(y;\theta)}\text{d}\mu-1\right)^{-1}\right]
\end{align} where $E_{1}(h)=\int_{\Omega_{\theta}}h\text{d}\lambda_{1}(h)$ , Kiefer claims the ideal measure $\text{d}\lambda_{1}(h)=\frac{n+1}{\theta}(h/\theta+1)^n\text{d}h$ . Proceeding now for general $n$ , we have that $E_{1}(h)=\int_{\Omega_{\theta}}h\frac{n+1}{\theta}(h/\theta+1)^n\text{d}h=\int_{\Omega_{\theta}}(h+\theta-\theta)\frac{n+1}{\theta^{n+1}}(h+\theta)^n\text{d}h=\int_{-\theta}^{0}\frac{n+1}{\theta^{n+1}}(h+\theta)^{n+1}\text{d}h-\int_{-\theta}^{0}\frac{n+1}{\theta^{n}}(h+\theta)^{n}\text{d}h=\frac{n+1}{n+2}\theta-\theta=-\frac{1}{n+2}\theta$ . We now calculate the integral \begin{align}
\int_{\Omega_{\theta}}f(y;\theta+h)\text{d}\lambda_{1}(h) = \int_{-\theta}^{0}\frac{n y^{n-1}}{(\theta+h)^n}\frac{n+1}{\theta^{n+1}}(\theta+h)^{n}\text{d}h = \frac{n(n+1)y^{n-1}}{\theta^{n}}
\end{align} This means that \begin{align}
\int_{\chi}\frac{[\int_{\Omega_{\theta}}f(y;\theta+h)\text{d}\lambda_{1}(h)]^{2}}{f(y;\theta)}\text{d}\mu = \int_{\chi}(n+1)^2 f(y;\theta) \text{d}\mu = (n+1)^2
\end{align} Since $(n+1)^2-1=n(n+2)$ we can combine our earlier results to find \begin{align}
\left[(E_1(h))^2\left(\int_{\chi}\frac{[\int_{\Omega_{\theta}}f(y;\theta+h)\text{d}\lambda_{1}(h)]^{2}}{f(y;\theta)}\text{d}\mu-1\right)^{-1}\right] = \frac{1}{n(n+2)^3}\theta^{2}.
\end{align} However, Kiefer finds the variance to be bounded by $\frac{1}{n(n+2)}\theta^{2}$ , which equals $\frac{n+1}{n}Y$ , so their is an unbiased estimator that reaches this bound (and clearly the answer I derived is wrong). My questions are: where did I go wrong in my derivation? Also, is there an easy way to see how Kiefer knew to choose $\text{d}\lambda_{1}(h)$ the way that he did, other than the obvious answer that this gives the best bound? Also, if anyone has general advice on calculation of tight bounds beyond Chapman-Robbins bound, I would be interested in hearing their strategies. Hopefully I have included enough notation to make the problem palatable, but if not I can copy more from the paper. edit: fixed incorrect mathematical symbol.","['statistics', 'parameter-estimation', 'probability', 'estimation']"
3388753,"$h(x)=\langle f(x),g(x)\rangle$ is differentiable in $c$ and $Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle$ with $u\in \mathbb{R}^m$.","Let $G\subset\mathbb{R}^m$ where $G$ is open and $c\in G$ . If $f,g:G\to\mathbb{R}^n$ differentiable in $c$ then $h:G\to\mathbb{R}$ defined by $h(x)=\langle f(x),g(x)\rangle$ is differentiable in $c$ and $$Dh(c)u=\langle Df(c)u,g(c)\rangle+\langle f(c),Dg(c)u\rangle$$ with $u\in \mathbb{R}^m$ . I'm somewhat  lost in how to proof this. I know that since $f,g$ are differentiable in $c$ then we have existence of $Df(c)$ and $Dg(c)$ . As well since $\langle\cdot ,\cdot\rangle$ is bilinear , then $D\langle u,v\rangle(x,y)= \langle x,v\rangle+\langle u,y\rangle$ . I have tried connecting these to things but haven't been able to get anywhere.","['multivariable-calculus', 'derivatives', 'analysis', 'real-analysis']"
3388767,Prove $\tan(\frac{x}{2}) = \frac{\sin x}{1 + \cos x} $ using the quadratic formula,"I am trying to prove the fact that $\tan \frac{x}{2} = \frac{1-\cos x}{\sin x}$ or alternatively $\tan \frac{x}{2} = \frac{1- \cos x}{\sin x}$ .  (I understand that it can be proved using the half-angle identities of $\sin$ and $\cos$ but I want to understand how to get to the solution from this specific method of derivation.) \begin{align*}
    \tan(2x) &= \frac{2\tan(x)}{1-\tan(x)^2} \\
    \tan(x) &= \frac{2\tan(\frac{x}{2})}{1-\tan(\frac{x}{2})^2} \\
\end{align*} I now let $A=\tan x$ and $B=\tan \frac{x}{2}$ \begin{align*}
    A\cdot(1-B^2) &= 2B\\
    AB^2+2B-A &= 0 \\
\end{align*} Now I solve for B using the quadratic formula. \begin{align*}
    B &= \frac{-2\pm \sqrt{4+4A^2}}{2A} \\
    B &= \frac{-1\pm \sqrt{1+A^2}}{A} \\
    \tan(\frac{x}{2}) &= \frac{-1\pm \sqrt{1+\tan(x)^2}}{\tan(x)}\\
    \tan(\frac{x}{2}) &= \frac{-1\pm \sqrt{(\sec x)^2}}{\tan(x)}\\
    \tan(\frac{x}{2}) &= \frac{-1\pm |\sec x|}{\tan(x)}
\end{align*} I am confused as to how to continue at this point (firstly, not sure how to deal with the absolute value, and secondly not sure how to deal with the plus-minus).
Any help is greatly appreciated, as I feel like I do not fully understand how to manipulate absolute values and the meaning of the plus-minus.",['trigonometry']
3388779,Proving outer measure property,"I am self-studying analysis by Sheldon Axler. This is the one of exercise problem in his book. He uses $|\cdot|$ to indicate the outer measure. Prove that if $A\subset \mathbb{R}$ and $t>0$ , then $|A|=|A\cap(-t, t)|+|A\cap(\mathbb{R}\setminus(-t, t))|$ . $|A|\leq|A\cap(-t, t)|+|A\cap(\mathbb{R}\setminus(-t, t))|$ is obvious. But how do I prove inequality from the opposite side? And in his next exercise, he somehow extends the property: Prove that $|A|=\lim_{t\rightarrow\infty}|A\cap(-t, t)|$ for all $A\subset\mathbb{R}$ . Does this problem related to the previous problem? Any hints would be appreciated. Thanks","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3388787,Correcting the set in my proof.,"The question was: Find $\int_{[0, \pi/2]} f$ if $$f(x) = \begin{cases} 
      \sin{x}, & if  \cos(x) \in \mathbb{Q}, \\
      \sin^2{x}, &  if \cos(x) \not\in \mathbb{Q}.
   \end{cases}$$ My answer was: Assume that $0 \leq x \leq \pi/2$ , then taking the cosine of this, we get $\cos 0 \geq \cos x \geq \cos (\pi/2),$ so, $1 \geq \cos (x) \geq 0$ (because $\cos (x)$ is a decreasing function in this interval.)\ Now, by monotonicity of measure $$m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}.$$ But, $m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0.$ \ This is because $\mathbb{Q}$ is countable and hence its measure is 0 and $\{[0, \pi/2] \cap  \mathbb{Q}\} \subset \mathbb{Q}$ , then by monotonicity of measure $m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0.$ \ And since the integral of  each integrable function $f$ on a set of measure equal to $0$ is $0$ , we have:\ $\int_{[0, \pi/2]} f = \int_{[0, \pi/2] \cap  \mathbb{Q}} f + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = 0 + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x}  = \int_{[0, \pi/2] \cap  \mathbb{Q}}  \sin^2{x} + \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x} dx = \int_{[0, \pi/2]}  \sin^2{x} dx,$ \ Where in the last equality we have changed the Lebesgue integral over $[0, \pi/2]$ into Riemann integral over $[0, \pi/2]$ because our function $\sin^2{x}$ is Riemann integrable and bounded by $[0,1]$ and the domain of integration is closed and bounded interval then by \textbf{ Theorem 3, on page 73} the Lebesgue integral is the Riemann integral.\ Now we can compute this integral:\ $$\int_0^{\pi/2}f(x)\,\mathrm d x=\int_0^{\pi/2}\sin^2(x)\,\mathrm d x = \int_0^{\pi/2} \{ \frac{1 - \cos{2x}}{2} \} d x = \frac{\pi}{4} - ( \frac{1}{4} \times 0) = \frac{\pi}{4}. $$ But it turns out that: My justification in this step: $$m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}.$$ was wrong, could anyone help me correct this step please?","['measure-theory', 'proof-writing', 'analysis', 'real-analysis']"
3388811,Trying to understand antisymmetry,"I'm trying to understand antisymmetry in relations and I'm really confused. I know that the defintion of antisymmetry is as follows: if $xRy$ and $yRx$ then $  x = y$ . I'm aware that the contrapositive exists: $x\neq y \Rightarrow \left( x,y\right) \notin R$ or $(y,x) \notin R $ Now let's take an example: the relation $R = \{ (a , a), (b , c), (c , b), (d , d) \}$ on $X = \{ a, b, c, d \}$ is not antisymmetric because both $(b,c)$ and $(c,b)$ are in $R$ . I am not sure to understand the justification for it being not antisymmetric. If we take the first definition of antisymmetry we see that we have $xRy$ and $yRx$ therefore we should have $x = y$ . However that's not the case because if we set $b = c$ we wouldn't need to have two elements. Therefore it's not antisymmetric. Is my understanding correct? It doesn't seem that I can use the contrapositive here.","['relations', 'discrete-mathematics']"
3388836,Proving following holds for almost everywhere,"I am studying real analysis and encountered this problem. Prove that for almost everywhere $x\in\mathbb{R}$ , $\lim_{n\rightarrow\infty}|\cos{nx}|^{\frac{1}{n}}=1$ . What theorem can I use to solve this problem? I don't know how to start. Thanks.",['real-analysis']
3388841,"Show $\int_ {\mathbb{D}}f(\frac{\alpha(x_1+ix_2)+\beta}{\bar{\beta}(x_1+ix_2)+\bar{\alpha}})d\mu(x_1,x_2)=\int_ {\mathbb{D}}f(x_1+ix_2)d\mu(x_1,x_2)$","Definitions needed in the problem: $\text{SU}(1,1)=\left\{\left( \begin{array}{ccc}
\alpha & \beta \\\overline\beta & \overline\alpha \end{array} \right)\mid \alpha,\beta\in \mathbb C,|\alpha|^2-|\beta|^2=1\right\}$ $\mathbb{D}={\{(x_1+ix_2)\in\mathbb{C}:(x_1^2+x_2^2)<1}\}$ Problem: Let $d\mu(x_1, x_2) =\frac{4}{(1- (x_1 ^ 2 + x_2 ^ 2)) ^ 2}d\lambda(x_1, x_2)$ where $\lambda$ denotes the measure of Lebesgue on the unit disk $\mathbb{D} $ . Show that for all $\left( \begin{array}{ccc}
\alpha & \beta \\\overline\beta & \overline\alpha \end{array} \right)\in\text{SU}(1,1)$ , $f\in\text{L}_1(\mathbb{D},\mu)$ : $\int_ {\mathbb{D}}f(\frac{\alpha(x_1+ix_2)+\beta}{\bar{\beta}(x_1+ix_2)+\bar{\alpha}})d\mu(x_1,x_2)=\int_ {\mathbb{D}}f(x_1+ix_2)d\mu(x_1,x_2)$ Idea (proof): Let $\left( \begin{array}{ccc}
\alpha & \beta \\\overline\beta & \overline\alpha \end{array} \right)\in\text{SU}(1,1)$ , $f\in\text{L}_1(\mathbb{D},\mu)$ then $|\alpha|^2-|\beta|^2=1$ and $\int_ {\mathbb{D}}f(\frac{\alpha(x_1+ix_2)+\beta}{\bar{\beta}(x_1+ix_2)+\bar{\alpha}})d\mu(x_1,x_2)=\int_ {\mathbb{D}}f(\frac{\beta\bar{\beta}-\alpha\bar{\alpha}}{\bar{\beta}^2}((x_1+ix_2)+\frac{\bar{\alpha}}{\bar{\beta}})^{-1}+\frac{\alpha}{\bar{\beta}})d\mu(x_1,x_2)=\int_ {\mathbb{D}}f(\frac{-1}{\bar{\beta}^2}((x_1+ix_2)+\frac{\bar{\alpha}}{\bar{\beta}})^{-1}+\frac{\alpha}{\bar{\beta}})d\mu(x_1,x_2)$ How can I continue?
Can anybody help me please? Thanks...","['riemannian-geometry', 'real-analysis', 'complex-analysis', 'hyperbolic-geometry', 'differential-geometry']"
3388850,"Theorem: Let $n$ be an integer. If $n^2$ is even, then $n$ is even","I have a proof by contrapositive (must be contrapositive) for the statement above, but I have been told that there is an error in my proof. Can someone verify this proof for me, and perhaps shed some light as to where this error is? Proof, by contrapositive. 1. Instead of the original statement, we prove the contrapositive.
2. If $n^2$ is odd, then $n$ is odd.
3. Assume that $n^2$ is odd.
4. Claim $n$ is odd.
5. By assumption, $n^2 = n \cdot n$ is odd.
6. If a product is odd, then each factor is odd.
7. Therefore, $n$ is odd.","['elementary-number-theory', 'proof-verification', 'discrete-mathematics']"
3388911,Prove that $\sum_{n=0}^\infty\frac{x^{2n+1}}{1+x^{4n+2}}\neq0$ for complex $x$,"For $x\in\mathbb C$ with $0<|x|<1$ , does this function have any roots? $$f(x)=\sum_{n=0}^\infty\frac{x^{2n+1}}{1+x^{4n+2}}$$ $$=\sum_{n=0}^\infty x^{2n+1}\sum_{m=0}^\infty(-x^{4n+2})^m$$ $$=\sum_{n=0}^\infty\sum_{m=0}^\infty(-1)^mx^{(2n+1)(2m+1)}$$ Finding an infinite product representation would help. I don't remember where I found this, but here's another form of the function: $$f(x)\overset?=\sum_{n=0}^\infty\sum_{m=0}^\infty x^{((4n+1)^2+(4m+1)^2)/2}$$","['number-theory', 'roots', 'complex-analysis', 'sequences-and-series', 'infinite-product']"
3389029,How should we interpret $\frac {d}{dt}$?,"I've been using derivatives and integrals mechanically for years without really questioning the symbols.  I recently watched some YouTube videos and came to understand that: $$\frac {dx}{dt}$$ basically means, for some function, $f(t)=x$ , an infinitesimal change in $t$ , or $dt$ , results in an infinitesimal change in $x$ , or $dx$ .  The ratio of those two numbers is the derivative, or the instantaneous tangent line of $f(t)$ at $t$ .  So far, so good. So could someone explain how to interpret this: $$\frac {d}{dt}$$ I get that the bottom part is an infinitesimal change in $t$ , but what is the top part?  And how should I read an expression like $$\frac {d^2x}{dt^2}$$ My main confusion is the $d$ part seems to have an existence on it's own without the dimension.",['derivatives']
3389129,Is a set with a single element like {0} dense in itself?,"Self explanatory really. Couldn't find an answer. I know that there can be sets with a highest and lowest element that are dense in themselves for example $\mathbb{Q}\cap[0,1]$ but I'm not sure about sets with just a single element.",['general-topology']
3389240,Maximum size of minimal generating set for $S_n$,"Consider all minimal generating sets for $S_n$ . The smallest of them have size 2, that is, $S_n$ can be generated by two permutations. What are the largest ones? As I understand, there should exist minimal generating sets of size $n - 1$ . E.g. the Coxeter generating set $(1\;2),\, (2\;3),\, \dots,\, (n - 1\;n)$ appears to be minimal: if we remove any single permutation $(i\;i + 1)$ , then $S_i \times S_{n-i}$ will be generated. And also $(1\;2),\, (1\;3),\, \dots,\, (1\;n)$ appears to be minimal: without any single permutation, $S_{n-1}$ will be generated. Is it true that a minimal generating set for $S_n$ cannot have more than $n - 1$ elements? Is it somehow related to Jerrum's filter that is guaranteed to return a generating set of size $\le n - 1$ ?","['permutations', 'group-theory', 'symmetric-groups', 'finite-groups']"
3389245,homomorphisms of B(H),"Let $\varphi\colon B(H)\to B(H)$ is an injective $\ast-$ homomorphism ( $\varphi$ -linear, $\varphi(xy)=\varphi(x)\varphi(y)$ , $\varphi(x^{\ast})=\varphi(x)^{\ast}$ ), where $H-$ separable Hilbert space. Is it true that $\varphi$ is actually $\ast-$ isomorphism? In other words $\varphi$ is also a surjection.","['c-star-algebras', 'operator-algebras', 'operator-theory', 'hilbert-spaces', 'functional-analysis']"
3389303,Find the digits of a number,"Consider the following statement! $\overline{ABCD}+\overline{EFG}=8768$ , and $\overline{ABC}+\overline{DEFG}=6005$ . If $A,\,B,\,C,\,D,\,E,\,F,$ and $G$ are different numbers, Then, what is $\overline{ABCD}$ ? My idea is : $$\begin{aligned}
&\overline{ABCD}+\overline{EFG}=8768\\
&1000(A)+100(B+E)+10(C+F)+(D+G)=8768\\
&\begin{cases}
&A=8\\
&B+E=7\\
&C+F=6\\
&D+G=8
\end{cases}
\end{aligned}
$$ But i don't think that this is the best idea. Cz when i tried to solve this, i got the wrong numbers that doesn't satisfied the equation. Please help.",['algebra-precalculus']
3389321,Is there a polynomial which detects when the two smallest roots of a given real polynomial are equal?,"The discriminant of a polynomial over a field is a ""universal""* polynomial function of its coefficients , which is zero if and only if the polynomial has a multiple root in some field extension. Now, let's limit the discussion to polynomials $p(x) \in \mathbb{R}[X]$ with real coefficients, with all their roots real and non-negative. Is there a ""universal""* polynomial in the coefficients of such $p(x)$ , which is zero if and only if the two smallest roots of $p(x)$ are equal? (equivalently, the smallest root of $p$ has multiplicity greater than $1$ ). If not, is there such a universal real-analytic function of the coefficients? *By ""universal"", I mean that the coefficients of the discriminant are independent of $p$ .","['real-analysis', 'field-theory', 'algebraic-geometry', 'polynomials', 'real-algebraic-geometry']"
3389324,$f$ is continuous in $x$ and $y$ and maps compact set into compact set. Show that $f$ is a continuous function on $\mathbb R^2$.,"Suppose $f$ is defined on $\mathbb{R}^2$ , $f$ is continuous in $x$ and $y$ respectively and $f$ maps compact set into compact set. Show that $f$ is a continuous function on $\mathbb R^2$ . Suppose $f$ is not continuous at $(x_0,\,y_0)$ , then there are $\varepsilon_0>0$ and a sequence $\{(x_n,\,y_n)\}$ converging to $(x_0,\,y_0)$ such that $$
|f(x_n,\,y_n)-f(x_0,\,y_0)|\geq\varepsilon_0.
$$ Let $K=\{(x_n,\,y_n)\}_{n=0}^\infty$ . $K$ is compact. Thus, $f(K)$ is also compact. Then I don't know what to do and am unable to use the condition that $f$ is continuous in $x$ and $y$ respectively.",['analysis']
3389352,Intersection of Two $L^p$ Spaces,"I would like to know what a common method is for proving the following: Let $\Omega \subseteq \mathbb{R}^{N}$ , some natural number $N$ . Suppose $f \in L^p(\Omega) \cap L^q(\Omega)$ for some $1 \leq p < q \leq \infty$ . Then $f \in L^r(\Omega)$ for all $p < r < q$ . I feel like you could show this with just the generalised Holder inequality, but not sure. Any help appreciated.","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3389361,When will $\frac{\sqrt{n^2+1}} {\sqrt{2}}$ be rational?,"For what n is this rational, $$\frac{\sqrt{n^2+1}} {\sqrt{2}}$$ So far I have found the integers 1,7,41 and I have found some rational solutions to this as well but I'm looking to get a more general sense. So when is this a rational number? Are there any restrictive properties that can be said about when this is rational? What can be said about n? Edit: Thanks for the help on the integer solutions but is there anything that can be said about when it is rational ?",['number-theory']
3389399,Exponential map on $S^2$,"Suppose we are on the two dimensional sphere $S^2$ the Riemannian metric $$(g_{ij})_{i,j}=\begin{bmatrix}1 & 0 \\ 0 & \sin^2(\theta)\end{bmatrix}$$ in spherical local coordinates ( $\theta \in (0,\pi)$ , $\varphi\in (0,2\pi)$ ). Is it possible to have an explicit local expression in spherical coordinates of the exponential map? This is my attempt: using this formula given in $\mathbb{R}^3$ For a point $p\in S^2\subset \mathbb{R}^3$ and a direction $v\in \mathbb{R}^3$ , the exponential map is given by \begin{equation}\exp_p(v)=\cos(|v|)p+\sin(|v|)\frac{v}{|v|},\end{equation} where $|\cdot|$ is the usual norm in $\mathbb{R}^3$ . I can convert $p=(p^1,p^2)$ and $v=v^1\partial_\theta+v^2\partial_\phi$ in local spherical coordinates, obtaining \begin{equation}\exp_p(v)=\Bigl(p^1\cos(|v|)+\frac{v^1}{|v|},p^2\sin(|v|)+\frac{v^2}{|v|}\Bigr),\end{equation} where $|\cdot|$ is the norm induced by the Riemannian metric. Is this correct?","['spheres', 'geometry', 'riemannian-geometry', 'differential-geometry']"
3389403,"How might I define a parabola in vertex form, such that...","Given the formula for a parabola in vertex form: $y = a(x-h)^2+k$ , as $h$ and $k$ are changed, the $a$ value will adjust in order to keep the left hand $x$ -intercept anchored to the origin. I'm really only interested in quadrant $I$ , and between $0 ≤ x,y ≤ 1$ . What I'm really looking to do is graph a curve (the type is negotiable) that rises from $(0, 0)$ , peaks at some target/adjustable number near $(0.5, 1)$ , and then falls, depending on where the peak is, to $(1, 0)$ . I considered using a spline for this, but this curve will drive a function in an inefficient node based system, where I need to keep the math complexity is low as possible. A parabola seems like the best fit. Thanks! :)","['conic-sections', 'algebra-precalculus', 'interpolation', 'quadratics']"
3389407,A Diophantine Equation related to arithmetic progression: $T_n=a^n+b^n+c^n$.,"I'm working on a problem that for complex $a,b,c$ , when will these four numbers $a+b+c, a^2+b^2+c^2, a^3+b^3+c^3, a^4+b^4+c^4$ becomes an arithmetic progression with integer values. If we let this arithmetic progression as $u,u+d,u+2d,u+3d$ it can be verify that if the common difference $d$ is nonzero, the possible values are $$(u,d)=(0,6),(3,2),(6,4),(6,30),(7,6)$$ but I don't know is there any other solution yet. By letting $t=u-2$ , this problem eventually leading me to find all $t\in\mathbb Z$ such that $$6t^4-2t^2+4t+1$$ is a $\textbf{Perfect Square}$ . I've done computing $|t|\leq 1000$ and only find out that the possible integer solutions are $t=-2,-1,0,1,4,5$ , so it is reasonable to think that this expression is a perfect square for only these $6$ integer values of $t$ . It takes me to solve this Diophantine equation $N^2=6t^4-2t^2+4t+1$ but I couldn't make any progress, is there any way to solve this problem?","['number-theory', 'square-numbers', 'arithmetic-progressions']"
3389419,What is the least positive integer $n$ for which $|\sin(n)-\sin(n^{\circ})|<0.005$,"What is the least positive integer $n$ for which $|\sin(n)-\sin(n^{\circ})|<0.005$ ? (That is the difference between sine of $n$ radians and sine of $n$ degrees is less than $0.005$ ) My attempt: $\sin(n^{\circ})=\sin(\frac{n\pi}{180})$ $\sin(A)-\sin(B)=2\sin(\frac{A-B}{2})\cos(\frac{A+B}{2})$ Putting $A=n$ , and $B=\frac{n\pi}{180}$ , the given inequality becomes; $$|2\sin(\frac{n-\frac{n\pi}{180}}{2})\cos(\frac{n+\frac{n\pi}{180}}{2})|<0.005$$ Simplifying, we get; $$|\sin(\frac{n(180-\pi)}{360})\cos(\frac{n(180-\pi)}{360})|<0.0025$$ Substituting $u=\frac{n(180-\pi)}{360}$ , we get; $\sin(u)\cos(u)<0.0025$ Expanding both sine and cosine, we get: $(u-\frac{u^3}{6}+\dots)(1-\frac{u^2}{2}+\dots)<0.0025$ or $u-\frac{u^3}{2}-\frac{u^3}{6}+\frac{u^5}{12}-\dots<0.0025$ I stuck here to find $u$ . Otherwise, by finding $u$ , I can solve for $n$ since $n=\frac{360u}{180-\pi}$ and I will round the result. Using Microsoft Excel, I found that $n=176$ I do not know if my approach is right. If right, how to solve for $u$ . If wrong, how to solve this problem: What is the least positive integer $n$ for which $|\sin(n)-\sin(n^{\circ})|<0.005$ ?","['trigonometry', 'integers', 'absolute-value', 'inequality']"
3389478,When does the underlying map of a polynomial induce a permutation on $\mathbb{Z}/p\mathbb{Z}$?,"For example, the underlying function of the polynomial $$f(x)=4x^2-3x^7$$ induces a permutation on $\mathbb{Z}/11\mathbb{Z}$ , though I only know the proof by ""brutal force"" (is there a cleverer proof?). In general, is there a criterion determining if a polynomial $f(x)$ induces a permutation on $\mathbb{Z}/p\mathbb{Z}$ ? Edit : I am not even sure if it matters whether $p$ is prime or not but I will tag this question ""finite fields"" for now. Does it matter?","['permutations', 'finite-fields', 'abstract-algebra', 'polynomials', 'group-theory']"
3389529,Does this proof that every open set in a metric space is a union of open balls use the axiom of choice?,"I am reading ""A Course in Analysis vol.3"" by Kazuo Matsuzaka. There is the following proposition in this book. Does the author use the axiom of choice in the proof? Proposition 4: A subset $A$ of a metric space $X$ is open in $X$ if and only if $A$ is a union of open balls in $X$ . Proof: An open ball is open, so a union of open balls is open. Conversely, if $A$ is open and $a \in A$ , then there exists a positive real number $r(a)$ such that $B(a ; r(a)) \subset A$ . Obviously $$A = \bigcup_{a \in A} B(a ; r(a))$$ holds. I think the author's proof uses the axiom of choice as follows: Let $S_a := \{x \in \mathbb{R} | x > 0, B(a ; x) \subset A\}$ . Since $A$ is open, for any $a \in A$ , $S_a \ne \emptyset$ . So, by the axiom of choice, there exists a mapping $r$ from $A$ to $\bigcup_{a \in A} S_a$ such that $r(a) \in S_a$ .","['axiom-of-choice', 'general-topology', 'metric-spaces']"
3389535,Elements in a finite group of order prime,"I'm trying to prove the following statement: Let $p=101$ . 
 Let $x\in (\mathbb{Z}/p\mathbb{Z})^*$ be arbitrary. Show that there exists $y\in (\mathbb{Z}/p\mathbb{Z})^*$ such that $y^3=x$ . Here is my work.
 Since $p$ is a prime. We know that the group of units $(\mathbb{Z}/p\mathbb{Z})^*$ is  a field and cyclic.
  Let $\alpha\in(\mathbb{Z}/p\mathbb{Z})^*$ be a primitive root, then $x=\alpha^j$ for some $j\in \mathbb{Z}$ . But I cannot go further. 
  I also tried Fermat's little theorem as follows; \begin{equation*}
  x^{100} \equiv 1 \quad mod(101)
  \end{equation*} and \begin{equation*}
  (x^{3})^{33}x \equiv 1 \quad mod(101)
  \end{equation*} But I cannot see the exact solution. Can you help me please?","['number-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3389558,Classification of surjective group homomorphisms,"Let $G$ be an arbitrary fixed group. First, we know that if $f: G \rightarrow G'$ is a group homomorphism, then $\ker f$ is a normal subgruop of $G$ . Second, we also know that if $H$ is a normal subgroup of $G$ , then the canonical map from $G$ into $G/H$ is a homomorphism whose kernel is $H$ . Questions If $f_1: G \rightarrow G_1$ and $f_2: G \rightarrow G_2$ are two surjective group homomorphisms whose kernel are same, then is there an isomorphism $h:G_1 \rightarrow G_2$ such that $h \circ f_1 = f_2$ ? If the answer for the above question is true, then can we say that ""the cardiality of surjective group homomorphism whose domain is $G$ (up to isomorphism) is equal to the cardinality of the set of normal subgroups of $G$ ""? Is there more relationships between the set of surjective homomorphisms (from $G$ ) and the set of normal subgroups of $G$ ? Thank you for concerning about this question.","['group-homomorphism', 'group-theory', 'normal-subgroups']"
3389582,Representation formula for the $n$th power of a self-adjoint operator in terms of the spectral measure,"Let $H$ be a $\mathbb R$ -Hilbertspace, $A$ be a self-adjoint linear contraction on $H$ and $E:\mathcal B(\mathbb R)\to\mathfrak L(H)$ denote the spectral measure associated with $A$ . By contractivity, the spectrum $\sigma(A)$ of $A$ is contained in the closed unit ball $\overline B_1(0)$ around $0$ and hence $$A^n=\int_{B_1(0)}\lambda^n\:E({\rm d}\lambda)+\int_{\partial B_1(0)}\lambda^n\:E({\rm d}\lambda)\;\;\;\text{for all }n\in\mathbb N\tag1.$$ In the complex setting, we can show that (see Lemma 1 in this paper ) $$A^n=\int_0^{2\pi}e^{{\rm i}\lambda n}\left(\left.E\right|_{\partial B_1(0)}+f(\lambda){\rm d}\lambda\right),\tag2$$ where $$f(\lambda):=\frac1{2\pi}\int_{B_1(0)}\frac{1-|\mu|^2}{|1-\mu e^{{\rm i}\lambda}|^2}\:E({\rm d}\mu)\;\;\;\text{for }\lambda\in\mathbb R,$$ for all $n\in\mathbb N$ . Is there a similar formula on the real case?","['hilbert-spaces', 'operator-theory', 'spectral-theory', 'functional-analysis']"
3389612,The $L^1$ distance of two CDF is the $L^1$ distance of the quantile function coupling,"In a book I found the following exercise: Let $F,G$ be two cumulative distribution functions. Then $$\int_0^1 \vert F^{-1} (t) - G^{-1} (t)\vert \text d t = \int_\Bbb R \vert F(x) - G(x) \vert \text d x$$ where $H^{-1} (t)  := \inf \{ x \in \Bbb R : H(x) > t\}$ , for a CDF $H$ . I tried to write $\vert F(x) - G(x) \vert = \int_0^1 1_{(-\infty , \vert F(x) - G(x) \vert]} (t) \text d t$ and use Fubini afterwards, in order to rewrite the set $\{x : t\leq \vert F(x) - G(x) \vert\}$ for fixed $t$ and obtaining a set with mass $\vert F^{-1}(t) - G^{-1}(t) \vert$ . But I fail with the last part. Maybe this is even the wrong approach.","['quantile-function', 'cumulative-distribution-functions', 'probability-distributions', 'probability-theory', 'probability']"
3389614,Non trivial valuation over a finite extension $F/\mathbb{Q}_p$ is equivalent to the one induced by $v_p$,"I'm stuck on a question in an exercise on p-adic numbers. Actually, let $p$ a prime number, $F/\mathbb{Q}_p$ a finite extension of fields, $\mathcal{O}$ the integral closure of $\mathbb{Z}_p$ on $F$ , $\mathfrak{p}$ the maximal ideal of $\mathcal{O}$ . We have already shown that : $\forall l \in \mathbb{N}_{\geq 2}$ , $\gcd(l,p)=1 \;\forall x \in \mathfrak{p} \; \exists y \in \mathcal{O} \quad y^l=1+x$ , using Hensel's lemma. Now, the next question is : Deduce from this that for any non trivial valuation $w$ on $F$ , if we are
  noting $\mathcal{O}_w$ its valuation ring, we have  : $\mathcal{O} \subset O_w$ . (We know as well that if we are noting : $v(x) = v_p(N_{F/\mathbb{Q}_p}(x))$ , then $\mathcal{O} = \{ x \in F \; | \; v(x) \geq 0\}$ ) To do this, I thought to show that $\mathbb{Z}_p \subset O_w$ , as $O_w$ is integrally closed on $F$ , we would have $\mathcal{O} \subset O_w$ . 
Now, if $x \in \mathbb{Z}_p$ , $x = \frac{a}{b}$ , where $\gcd(p,b)=1, \gcd(a,b)=1$ . We also know that the restriction to $\mathbb{Q}$ of $|.|_w$ (which is the n.a absolute value corresponding to the valuation) is either trivial or $|.|_q$ for a prime $q$ , by Ostrowski's theorem. So : $w(x) = w(\frac{a}{b}) = w(a)-w(b)$ . If $w$ is trivial over $\mathbb{Q}$ , $w(x) = 0$ and we have the result. Otherwise, $w(x) = v_q(a) - v_q(b)$ . But then I'm stuck, cause actually the result I have to prove indicates that $q=p$ , but there is no way (I found) from here to conclude this (and it's actually quite normal, cause this is what implies the next question). So, anyone could help me, please ? Thank you !","['number-theory', 'p-adic-number-theory', 'abstract-algebra']"
3389615,The composition space of $L^p$ spaces is complete.,"Here is a homework problem from real analysis class. We take as our underlying space as the product space $\{(x,t)\}=\mathbb R^d\times \mathbb R$ , with the product measure $dxdt$ , where $dx$ and $dt$ are Lebesgue measures on $\mathbb R^d$ and $\mathbb R$ , repectively. We define $L_t^r(L_x^p)=L^{p,r}$ with $1\leq p\leq\infty$ , $1\leq r\leq\infty$ , to be the space of equivalence class of jointly measurable functions $f(x,t)$ for which the norm $$\|f\|_{L^{p,r}}=\left(\int_\mathbb R\left(\int_{\mathbb R^d} |f(x,t)|^p\,dx\right)^{\frac rp}\right)^{\frac1r}$$ is finite when $p<\infty$ and $r<\infty$ and an obvious variant when $p=\infty$ and $r=\infty$ . What I need to do is to verify that $L^{p,r}$ with this norm is complete and hence is a Banach space. My attempt: Suppose $p<\infty$ and $r<\infty$ . Suppose $\{f_n(x,t)\}_1^\infty$ is a Cauchy sequence in $L^{p,r}$ . Let $g_n(t)=\|f_n(\cdot,t)\|_{L_x^p}$ for all $n\geq 1$ then $\{g_n\}_1^\infty\subset L_t^r$ and by Minkowski's inequality \begin{align*}
\|g_n-g_m\|_{L_t^r}&=\left(\int_\mathbb R\left|\|f_n(\cdot,t)\|_{L_x^p}-\|f_m(\cdot,t)\|_{L_x^p}\right|^r\right)^{\frac1r}\\
&\leq \left(\int_\mathbb R\left|\|f_n(\cdot,t)-f_m(\cdot,t)\|_{L_x^p}\right|^r\right)^{\frac1r}\\
&=\|f_n-f_m\|_{L^{p,r}},
\end{align*} $\{g_n(t)\}_1^\infty$ is a Cauchy sequence in $L_t^r$ and thus there is $g(t)\in L_t^r$ such that $g_n\to g$ in $L_t^r$ . But how can we find the $L^{p,r}$ limit of $f_n$ from this? Another thought: I want to establish a quasi-Chebyshev's inequality under this norm and then we can deduce that $\{f_n\}$ is Cauchy in measure from the assumption that it is Cauchy in $L^{p,r}$ , and then find an a.s.-convergent subsequence of $\{f_n\}$ and go on from here. But I failed to establish the desired inequality. Any help will be appreciated.","['lp-spaces', 'functional-analysis', 'real-analysis']"
3389634,Relationship between the derivative of $f(x)/x$ and the second derivative of $f(x)$,"In economics, a ""progressive tax"" is a tax in which the average tax rate (taxes paid / personal income) increases as the taxable amount increases. This can be formally described as having a tax function $T$ of income $z$ such as $\frac{\partial T(z)/z}{\partial z} > 0$ (definition 1). An alternative definition of a progressive tax is that marginal rates are increasing, that is $\frac{\partial^2 T}{\partial z^2} > 0$ (definition 2). The second definition implies the first definition. It is not clear (to me, at least) under what assumptions on $T$ does the first definition implies the second. It is not true for any $T$ . For instance if $T$ is defined as $T(z) = a \times z - k$ , with $a > 0$ and $k>0$ , then the marginal rate is $a$ (constant, and not increasing) and the average rate is $a - k/z$ (increasing). The question is then: when does the first definition implies the second one?","['economics', 'calculus', 'functions']"
3389649,Strong convergence and convergence of integral,"A sequence of probability measures $\{\mu_n\}$ converges strongly if for each measurable set $A$ $$\mu_{n}(A) \rightarrow \mu(A)$$ (notice that this convergence is different than the total variation) when does this convergence imply $$\int f \,d\mu_n\to \int f \,d\mu $$ for every bounded measurable function? I'm interested in the case in which the space is not Polish.",['measure-theory']
3389655,"$y''-y=e^x$ initial conditions $y(0)=0$, $y'(0) = \frac{3}{2}$","This is the solution to the reference: How did the blue line come out, or set it out?
This is really hard for me to think about, what is the general method?","['initial-value-problems', 'proof-explanation', 'ordinary-differential-equations']"
3389676,How to prove if an only if for a equivalence relation?,"Let $H$ and $K$ be subgroups of the group $G$ . Let $x,y \in G$ and define a relation on G by $x \equiv y$ if and only if there exists $h \in H$ and $k \in K$ such that $x = hyk$ . Prove that the relation is an equivalence relation. I proved the equivalence relation but how do I prove the ""if and only if"" part?","['proof-writing', 'abstract-algebra', 'discrete-mathematics']"
3389682,Inequality : $\frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big)$,"It's a charming problem : Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big)$$ I know the identity : Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a}=1.5$$ But I think it's not relevant here . I try also majorization with the inequality : Let $a\geq b\geq c>0$ such that $a+b+c=1$ then we have : $$\exp\Big(\frac{-2}{3}\Big)a\geq \frac{a}{\exp(a+b)}$$ Second line of the majorization : $$\exp\Big(\frac{-2}{3}\Big)^2ab\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)}$$ Third line of the majorization : $$\exp\Big(\frac{-2}{3}\Big)^3abc\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)}\frac{c}{\exp(c+a)}$$ The lines are easy to check with the condition remains to apply Karamata's inequality and we are done . Unfortunately the second line fails . My question :
Have you a proof ? Thanks a lot for sharing your time and knowledge .","['rearrangement-inequality', 'multivariable-calculus', 'sum-of-squares-method', 'inequality', 'exponential-function']"
3389703,subtlety around minimisation of expectation of an r.v,"I'm attempting the following questions The first question is fine. However i feel like things get interesting in the second question. Now i'm not too familiar with things like functionals but this kind of approach made sense to me the most because i feel as simplified solutions i have found overlook a subtlety and just say c) follows from b) by essentially noticing $\mathbb{E}(\mathbb{E}(Y|X)) = \mathbb{E}(Y)$ . As far as i'm aware $\mathbb{E}((Y-f(X))^2|X)$ returns a random variable and not a value in $\mathbb{R}$ . Where as $\mathbb{E}(Y-f(X))^2$ returns a value in $\mathbb{R}$ . If this is correct then what does it mean to minimise the random variable in b. ? Well we can deduce that $\mathbb{E}((Y-f(x))^2|X) = (f(x) - \mathbb{E}(Y|X))^2 - (\mathbb{E}(Y|X))^2 + E(Y^2|X)$ . Now if we let $\mathcal{F}$ denote the space of functions we want $f' \in \mathcal{F}$ that satisfies $(f'(x) - \mathbb{E}(Y|X))^2 - (\mathbb{E}(Y|X))^2 + E(Y^2|X) \leq (f(x) - \mathbb{E}(Y|X))^2 - (\mathbb{E}(Y|X))^2 + E(Y^2|X)$ for all $f \in \mathcal{F}$ . Now clearly $f'(x) = \mathbb{E}(Y|X)$ satisfies this as the square term is then $0$ . $\textbf{Key Result}$ : So let the random variable $F' =(f'(x) - \mathbb{E}(Y|X))^2 - (\mathbb{E}(Y|X))^2 + E(Y^2|X)$ where $f'(x) = \mathbb{E}(Y|X)$ and let the random variable $F = (f(x) - \mathbb{E}(Y|X))^2 - (\mathbb{E}(Y|X))^2 + E(Y^2|X)$ where $f$ is any function in $\mathcal{F}$ then i know it's always the case that $F' \leq F$ Now i can use the key result for part c. I know that $\mathbb{E}(Y-f(x))^2 = \mathbb{E}(\mathbb{E}((Y-f(x))|X)$ . For two random variables $X, Y$ its trivial that if $X \leq Y \Rightarrow \mathbb{E}(X) \leq \mathbb{E}(Y)$ . So again considering the space of functions $\mathcal{F}$ . Let $f' = \mathbb{E}(Y|X)$ and obtain $F'$ from $f'$ and $F$ from any other $f$ like we did before. Then i know that $F' \leq F$ and thus $\mathbb{E}(F') \leq \mathbb{E}(F)$ . So i can conclude that to minimize $(Y- f(X))^2$ we require $f(x) = \mathbb{E}(Y|X)$ . Firstly is my reasoning correct? If it is correct have i over complicated it and is there a simpler solution? Thanks.","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
3389787,Are products of analytic continuations also analytic?,"The question of the value, if any depending on which answer you choose, of $\sum_{n=1}^\infty n$ has been addressed a few times.  At least here Does $\zeta(-1)=-1/12$ or $\zeta(-1) \to -1/12$? and here Why does $1+2+3+\cdots = -\frac{1}{12}$? .  I do not want to re-open that question in general, but I have a question about a specific step of one of the approaches (or purported approaches as you may like) to computing the result. Under the zeta function regularization technique, one ultimately observes that $$ \left( 1 - 2^{1-s} \right) \zeta(s) = \eta(s) $$ for the Riemann zeta function $\zeta$ and the Dirichlet eta function $\eta$ .  One usually arrives at this result by using the series representations of these two functions and performing manipulations on them that are valid for complex values of $s$ where the series representations of $\zeta$ and $\eta$ converge . That seems fine as far as it goes, under the assumption that each function is evaluated at a value of $s$ where the series converges.  The method then continues to assert that the relationship holds for the analytic continuations of $\zeta$ and $\eta$ .  That's the step that motivates my question. Is it generally true that if $f(s) g(s) = h(s)$ on an open set $U$ that this relationship will continue to hold for their analytic continuations to larger sets?  If not generally true, what is the special property of $\zeta$ and $\eta$ that makes it true for the case outlined above? My sense is that it's not generally true because of differences in which potential supersets of $U$ each individual function has an analytic continuation, but I'm operating well on the fringe of my understanding of this topic.","['complex-analysis', 'riemann-zeta', 'sequences-and-series']"
3389811,"What's the quotient space of torus $S^1\times S^1$ under equivalence relation $(z,w)\sim (w,z)$ [duplicate]","This question already has an answer here : Orbit space of torus homeomorphic to mobius strip (1 answer) Closed 4 years ago . Consider the quotient space of torus $S^1\times S^1$ under the equivalence relation $(z,w)\sim (w,z)$ .
I'm trying to visulaize it but find it pretty hard to do. Any hint?","['general-topology', 'quotient-spaces']"
3389827,Composition of uniformly continuous maps,"Fix $p>1$ . Let $\mathscr C[0,2]$ be the space of continuous functions on $[0,2]$ with metric given by $$d_{p}(f,g)=||f-g||_p:=\left(\int_0^2 |f(x)-g(x)|^p 
\ dx\right)^{\frac{1}{p}}.$$ Consider the map $\Psi:\mathscr C[0,2]\rightarrow \mathbb R$ given by $$\Psi(f)=\left(\int_0^2 f(x)\ dx \right)^4.$$ Writing $\Psi = T\circ\Phi$ where $T(x)=x^4$ and $\Phi:\mathscr C[0,2]\rightarrow \mathbb R$ is given by $$\Phi(f)=\int_0^2 f(x)\ dx,$$ one can show that $\Phi$ is uniformly continuous, and so $\Psi$ is continuous. However, one can also show that $\Psi$ is $\textbf{not}$ uniformly continuous: let $f_n(x) = n+\frac{1}{n^3}$ , $g_n(x)= n$ . Then $$||f_n-g_n||_p=\frac{2^{1/p}}{n^3}\rightarrow 0 \quad \text{as}\quad n\rightarrow \infty.$$ On the other hand, $$|\Psi(f_n)-\Psi(g_n)|=16\left(\left(n+\frac{1}{n^3}\right)^4-n^4\right)\not\rightarrow 0\quad \text{as}\quad n\rightarrow \infty.$$ This seems to contradict the fact that the composition $f\circ g$ of two uniformly functions $f,g:\mathbb R\rightarrow \mathbb R$ is uniformly continuous (see this for example). Is this phenomenon due to the different metrics used?",['real-analysis']
3389832,Simplest proof/greatest generality in which $|R/(\det A) R| = |R^n/A R^n|$,"$\def\ZZ{\mathbb{Z}}$ Let $A$ be an $n \times n$ integer matrix with nonzero determinant. Then the abelian group $\ZZ^n / A \ZZ^n$ has order $|\det A|$ . I've known this for ages, but I just realized I don't know a really clean proof. For example, it can be deduced by putting $A$ into Smith Normal Form, but that's surely overkill. Question 1 What is a quick elementary proof that $|\ZZ^n/A \ZZ^n| = |\det A|$ ? We can rewrite this as $|\ZZ^n/A \ZZ^n| = |\ZZ/(\det A) \ZZ|$ and, in this formulation, it is a meaningful equation for any commutative ring $T$ . Using Smith Normal Form, it is true for PID's and, using localization followed by Smith Normal Form, it is true for Dedekind domains. It is probably not reasonable to ask this question unless $R$ is a finite quotient domain, meaning an integral domain where $R/a R$ is finite for any nonzero $a \in R$ . Question 2 What is the most general class of commutative rings in which $|R^n/A R^n| = |R/(\det A) R|$ ? For example, does $\ZZ[\sqrt{8}]$ have this property?","['abstract-algebra', 'determinant', 'commutative-algebra']"
3389846,Integral of null function,"I am a math grad student, and I understand that from an undergrad calc students perspective that it is fine notation/convention to write $$\int dx=\int 1 dx$$ But I don't have any idea why we should be able to say that this thing is undefined. 
If we view $\int$ as an operator, what is $\int dx$ the integral of? Where is the function? we can just throw a 1 in there with no problems? it seems like we are taking the integral of a null function, but we should be looking at functions from $\mathbb R \rightarrow \mathbb R$ so I don't know why this would be allowed.
Ay comments are welcome.
Thanks","['integration', 'functions', 'riemann-integration']"
3389852,How to solve $\sin(3x)+\cos(2x)={1\over2}$,"I have a trouble to continuing this problem.
My work so far : $$\sin(3x)+\cos(2x)={1\over2}$$ I try using this : $$\sin(3x)+\sin(90^\circ-2x)={1\over2}$$ $$2\sin\left({3x+90^\circ-2x\over2}\right)\cos\left({3x-90^\circ+2x\over2}\right)={1\over2}$$ $$4\sin\left({x+90^\circ\over2}\right)\cos\left({5x-90^\circ\over2}\right)=1$$ How to solving this equation ?
Thanks for your help.","['algebra-precalculus', 'trigonometry']"
3389859,Weak Law of Large Number and Truncated Mean,"I am working on this question: Let $X_{1},\cdots, X_{n}$ be i.i.d with $P(X_{i}=(-1)^{k}k)=1/(c_{0}k^{2}\log k)$ for $k\geq 2$ , where $c_{0}$ is a normalizer. Prove that $E|X_{1}|=\infty$ and yet there exists a constant $\mu<\infty$ such that $$\dfrac{1}{n}\sum_{i=1}^{n}X_{i}\longrightarrow\mu,\ \text{in distribution}.$$ I got a solution but I cannot understand it. The solution argued as follows: Firstly we have $$P(|X_{i}|>n)<\sum_{k=n+1}^{\infty}\dfrac{1}{c_{0}k^{2}\log k}\leq \dfrac{1}{c_{0}n\log n},$$ so we have $nP(|X_{i}|>n)\longrightarrow 0$ , and thus the weak law can be applied. $$E|X_{i}|=\sum_{k=2}^{\infty}\dfrac{1}{c_{0}k\log k}=\infty,$$ but the truncated mean $\mu_{n}=EX_{i}\mathbb{1}_{(|X_{i}|\leq n)}=\sum_{k=2}^{n}(-1)^{k}\dfrac{1}{c_{0}k\log k}\longrightarrow \sum_{k=2}^{\infty}(-1)^{k}\dfrac{1}{c_{0}k\log k},$ since the latter is an alternating series with decreasing terms for $k\geq 3.$ I have following confusions: (1) I only have $P(X_{i}=(-1)^{k}k)$ , how does it calculate the $P(|X_{i}|)>n$ and $E|X_{i}|$ ? and why $$\sum_{k=n+1}^{\infty}\dfrac{1}{c_{0}k^{2}\log k}\leq \dfrac{1}{c_{0}n\log n}?$$ (2) Does the weak law have anything to do $E|X_{i}|=\infty$ ? or $E|X_{i}|=\infty$ can be concluded directly from that infinity sum? if so, why? (3) I understand that the weak law implies that $$\dfrac{1}{n}\sum_{i=1}^{n}X_{i}-\mu_{n}\longrightarrow 0\ \text{in distribution,}$$ and I understand that the solution implies the desired $\mu$ is the infinite sum that $\mu_{n}$ converges to, but how could I connect $\mu$ with $\mu_{n}$ in the weak law? Can I do the following: $$\dfrac{1}{n}\sum_{i=1}^{n}X_{i}-\mu_{n}\longrightarrow_{p} 0,$$ but $\mu_{n}\longrightarrow \mu<\infty$ , so $$\dfrac{1}{n}\sum_{i=1}^{n}X_{i}\longrightarrow_{p} \mu?$$ By the way, here is the weak law related to the truncated mean: Show that if $X_{1},\cdots, X_{n}$ are i.i.d and $\lim_{k\rightarrow\infty}kP(|X|>k)=0$ , then if we let $S_{n}=X_{1}+\cdots+X_{n}$ and let $\mu_{n}=E[X_{1}\mathbb{1}_{(|X_{1}|\leq n)}]$ , then we have $$S_{n}/n-\mu_{n}\longrightarrow_{p} 0.$$ Thank you! Edit 1: Okay I think I figure it all out, and the proof I am going to present basically follows the answer of kasa , I just add more technical but non-trivial details: Proof: For brevity, set $S_{n}:=X_{1}+\cdots+X_{n}$ . For me to type Latex easier, let us set $C:=\dfrac{1}{c_{0}}.$ Let us firstly deal with $E|X_{1}|$ . We have the following calculation: $$E|X_{1}|=\sum_{k=2}^{\infty}kP(|X_{1}|=k)=\sum_{k=2}^{\infty}\dfrac{Ck}{k^{2}\log k}=C\sum_{k=2}^{\infty}\dfrac{1}{k\log k},$$ and I claim that $\sum_{k=2}^{\infty}\dfrac{1}{k\log k}=\infty$ . Indeed, if we do a change of variable $u=\log (x)$ to the following integral, we have $$\int_{2}^{\infty}\dfrac{1}{x\log x}dx=[\log(\log x)]_{2}^{\infty}=\lim_{b\rightarrow\infty}\log(\log(b))-\log (2)=\infty.$$ Therefore, it follows from the integral test that $\sum_{k=2}^{\infty}\dfrac{1}{k\log k}=\infty$ . Thus, $E|X_{1}|=\infty$ . Now, for the second part of this question, denote the truncated mean by $$\mu_{n}:=E(X_{i}\mathbb{1}_{(|X_{i}\leq n)})=\sum_{k=2}^{n}(-1)^{k}\dfrac{C}{k\log k}.$$ Then, notice that $$\mu_{n}\longrightarrow \sum_{k=2}^{\infty}(-1)^{k}\dfrac{C}{k\log k}<\infty,$$ since the latter one is actually an alternating sum with decreasing terms for $k\geq 3$ . Denote the limit of $\mu_{n}$ to be $\mu$ , and we will show that $\mu$ is the desired constant we need. Firstly, we know that $\mu<\infty$ by above argument. Then, since $\mu_{n}\longrightarrow\mu$ , I claim that $\mu_{n}\longrightarrow_{p}\mu$ . Indeed, since $\mu_{n}\longrightarrow\mu$ , the set $$\mathcal{O}:=\{\omega:\lim_{n\rightarrow\infty}\mu_{n}\neq \mu\},$$ has $P(\mathcal{O})=0$ . Now, fix $\epsilon>0$ , then define the sets $$A_{n}:=\bigcup_{m\geq n}\{|\mu_{m}-\mu|>\epsilon\},\ \text{and}\ A:=\bigcap_{n\geq 1}A_{n},$$ which is clearly to see that $A_{n}\searrow A$ , and thus $P(A)=\lim_{n\rightarrow\infty}P(A_{n})$ by continuity. On the other hand, for $\omega_{0}\in\mathcal{O}^{c}$ , by definition it means $|\mu_{n}(\omega_{0})-\mu(\omega_{0})|<\epsilon$ for all $n>N$ for some $N$ . Therefore, for all $n\geq N$ , $\omega_{0}\notin A_{n}$ and thus $\omega_{0}\notin A$ . Therefore, $A\cap \mathcal{O}^{c}=\varnothing$ , which implies $A\subset\mathcal{O}$ . Thus, by monotonicity, we have $P(A)=0$ , and thus $\lim_{n\rightarrow\infty}P(A_{n})=0$ . Finally, we can see that $P(|\mu_{n}-\mu|>\epsilon)\leq P(A_{n})\longrightarrow 0$ , and thus $\mu_{n}\longrightarrow_{p}\mu$ . The point here is that pointwise convergence can certainly imply almost sure convergence. Then the proof is exactly the same as almost sure convergence implying convergence in probability. On the other hand, we have the following computation: \begin{align*}
P(|X_{i}|>n)&=\sum_{k=n+1}^{\infty}\dfrac{C}{k^{2}\log k}\leq C\sum_{k=n+1}^{\infty}\dfrac{1}{k^{2}\log n}\\
&=\dfrac{C}{\log n}\sum_{k=n+1}^{\infty}\dfrac{1}{k^{2}}\leq\dfrac{C}{\log n}\sum_{k=n+1}^{\infty}\dfrac{1}{k(k-1)}\\
&\leq\dfrac{C}{n\log n},
\end{align*} which implies that $$nP(|X_{i}|>n)\longrightarrow 0,\ \text{as}\ n\longrightarrow\infty.$$ Therefore, by the weak law of large number, we have $$S_{n}/n-\mu_{n}\longrightarrow_{p} 0.$$ To conclude the proof, we need the following lemma.  I will put the proof of lemma in the end. Lemma: If we have two random variable $X_{n}$ and $Y_{n}$ such that $X_{n}\longrightarrow_{p} a$ and $Y_{n}\longrightarrow_{p} b$ for some constant $a$ and $b$ , then $$X_{n}+Y_{n}\longrightarrow_{p} a+b.$$ Now, by this lemma, we can set $X_{n}:=S_{n}/n-\mu_{n}$ and $Y_{n}:=\mu_{n}$ , then we have $$S_{n}/n=X_{n}+Y_{n}\longrightarrow_{p}0+\mu=\mu,\ \text{as desired.}$$ Proof of lemma: Let $\epsilon>0$ , then consider $P(|X_{n}+Y_{n}-a-b|\geq\epsilon)$ . Then, using the hypothesis that $X_{n}\longrightarrow_{p}a$ and $Y_{n}\longrightarrow_{p}b$ , we have the following computation: \begin{align*}
P(|X_{n}+Y_{n}-a-b|\geq\epsilon)&=P(|(X_{n}-a)+(Y_{n}-b)|\geq\epsilon)\\
&\leq P(|X_{n}-a|\geq\epsilon/2\ \text{or}\ |Y_{n}-b|\geq\epsilon/2)\\
&\leq P(|X_{n}-a|\geq\epsilon/2)+P(|Y_{n}-b|\geq \epsilon/2)\longrightarrow 0+0=0\ \text{as}\ n\rightarrow\infty.
\end{align*} Therefore, $X_{n}+Y_{n}\longrightarrow_{p} a+b.$ The only part I am still doubting is that $\mu_{n}$ is not a random variable, it is just a sequence of number, so I don't know if the convergence in probability even makes sense to $\mu_{n}$ . It will be really appreciated if anyone can clarify my confusion here :) Let me express my appreciation to kasa for his/her patience and brilliant answer. Please let me know if you think anything can be improved or reined in my proof :)","['proof-explanation', 'law-of-large-numbers', 'probability-theory']"
3389941,Limit of sequence (by definiton),"I have this assignment: $$\lim_{n\to \infty }\frac{(-1)^n-3}{n^2}=0$$ I have to prove that limit by definition, but I am stuck with $$|a_n - 0|< \epsilon$$ Dont know how to deal with that absolute value. The solution is $$n_0=\left[\frac{2}{\sqrt\epsilon}\right]+1$$","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3389976,What is the motivation for using cross-entropy to compare two probability vectors?,"Define a ""probability vector"" to be a vector $p = (p_1,\ldots, p_K) \in \mathbb R^K$ whose components are nonnegative and which satisfies $\sum_{k=1}^K p_k = 1$ . We can think of a probability vector as specifying a probability mass function (PMF) for a random variable with $K$ distinct possible values. A straightforward and intuitive way to compare two vectors $p$ and $q$ in $\mathbb R^K$ is to compute the quantity $$
d(p,q) = \frac12 \| p - q \|_2^2,
$$ which is small when $p$ is close to $q$ .
However, if $p$ and $q$ are probability vectors, I think it is somehow more natural to compare them using the ""cross-entropy loss function"" $\ell$ defined by $$
\ell(p,q) = -\sum_{k=1}^K q_k \log(p_k).
$$ (This function is only defined when all components of $p$ are nonzero.) Question: What is the motivation for using the cross-entropy loss function when comparing probability vectors? Is there a viewpoint that makes it directly obvious that this is the ""correct"" thing to do? Some additional background information: This method of comparing probability vectors is fundamental in machine learning, because we have the following ""recipe"" for a classification algorithm which classifies objects into one of $K$ distinct classes. Suppose that we are given a list of training examples $x_i \in \mathbb R^n$ and corresponding one-hot encoded label vectors $y_i \in \mathbb R^K$ . (So if the $i$ th training example belongs to class $k$ , then the $k$ th component of the vector $y_i$ is $1$ and the other components are $0$ .) Let $S: \mathbb R^K \to \mathbb R^K$ be the softmax function defined by $$
S(u) = \begin{bmatrix} \frac{e^{u_1}}{\sum_k e^{u_k}} \\ \vdots \\ \frac{e^{u_K}}{\sum_k e^{u_k}} \end{bmatrix}.
$$ The softmax function is useful because it converts a vector in $\mathbb R^K$ into a probability vector.
To develop a classification algorithm, we attempt to find a function $f: \mathbb R^n \to \mathbb R^K$ such that for each training example $x_i$ the probability vector $p_i = S(f(x_i))$ is close to $y_i$ in the sense that $\ell(p_i, y_i)$ is small. For example, $f$ might be a neural network with a particular architecture, and the parameter vector $\theta$ which contains the weights of the neural network is chosen to minimize $$
\sum_{i = 1}^N \ell(p_i, y_i),
$$ where $N$ is the number of training examples. (Multiclass logistic regression is the especially simple case where $f$ is assumed to be affine: $f(x_i) = A x_i + b$ .) One way to discover the cross-entropy loss function is to go through the steps of using maximum likelihood estimation to estimate the parameter vector $\theta$ which specifies $f$ (assuming that $f$ is restricted to be a member of a certain parameterized family of functions, such as affine functions or neural networks with a particular architecture). The cross-entropy loss function just pops out of the MLE procedure. This is the approach that currently seems the most clear to me. There is also an information theory viewpoint. Is there any simple way to recognize that the cross-entropy loss function is a ""natural"" way to compare probability vectors?","['machine-learning', 'statistical-inference', 'probability']"
3389978,Boundedness of the Hilbert-Hankel operator on $L^p(\mathbb{R^+})$,"This is an exercise in Lax. The Hilbert-Hankel operator is defined to be $f\mapsto g(r)=\int^\infty_0 \frac{f(t)}{t+r}dt$ . The question is to show the operator is a bounded map of $L^p(\mathbb{R^+})\rightarrow L^p(\mathbb{R^+})$ . Here is what I have so far: I need to show that $\frac{||g||_p}{||f||_p}\le C$ for all $f\in L^p(\mathbb{R^+})$ . Assume $f\ge 0$ , by Minkowski's inequality for integrals, $||g||_p\le c\int^\infty_0 f(t)t^{1/p-1}dt=cI$ . Now I have no idea to show the bound of $I/||f||_p$ , or equivalently the bound of $I^p/||f||_p^p$ . Or do it need to divide the integral for different treatments?","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'laplace-transform']"
3389983,Explicit description of tangent spaces of $O(n)$,"I would like someone to verify the calcuations that I'm making about the tangent spaces of $O(n)$ , and help answer some general questions that I have from this special case. I don't know much about Lie theory at all, so I'm flying by the seat of my pants here :) So, we have the space $O(n) \equiv \{ X | X^TX = I \}$ . It's not hard to show that the tangent space should satisfy the constraint: tangent space at $P \in O(n) \equiv T_P O(n) \equiv \{ Z | Z^T P + P^T Z = 0\}$ . Now, by performing the usual Lie algebra trick of ""calculate tangent space at identity"", we see that: $$
T_I O(n) = \{ Z | Z^T + Z = 0 \}
$$ Next, to calculate the tangent space at some arbitrary point $P \in O(n)$ , we consider the map: $$
f: O(n) \rightarrow O(n) \qquad f(X) = PX
$$ Note that $f(I) = P$ , and hence the differential of this map, $df$ will have the type $df: T_I O(n) \rightarrow T_P O(n)$ . Now, all that's left to do is to calculate the differential. To perform this, we take a curve: $$
c: (-\epsilon, \epsilon) \rightarrow O(n) \qquad
c(t) = e^{Kt} \qquad
K^T = -K
$$ This has image in $O(n)$ , since $$
c(t)^T c(t) = e^{K^T t}e^{Kt} = e^{t(K^T + K)} = e^{t\cdot 0} = I
$$ Hence, this is a valid curve. Now, we compute $df$ : $$
df \equiv \frac{d}{dt} (f \circ c)(t) \vert_{t = 0} = \frac{d}{dt} P  e^{Kt} \vert_{t=0} = PKe^{K0} = PK
$$ Hence, the tangent space at the point $P$ is $T_PO(n) \equiv \{ PK | K^T + K = 0 \}$ I am uncomfortable with many things that ""automatically work"" in this proof: Is this proof correct? If not, where is it wrong? Why is the choice of the integral curve $c(t) = e^{Kt}$ the ""correct"" choice? How do I prove that the mapping of tangent spaces is indeed a bijection, and not some artefact of the curve parametrization I chose? What is the ""general form"" of this proof, for an arbitrary matrix Lie group $M$ ? Can I state that the tangent space at a point $P$ will be of the form $T_P M \equiv \{ PZ \mid Z \in T_I M \}$ ? If not, why is this not correct? Is there a slick proof of this fact?","['lie-algebras', 'smooth-manifolds', 'manifolds', 'lie-groups', 'differential-geometry']"
3390010,Proving exact sequence of abelian groups splits,"I ran across this question on an old University of Maryland Algebra qual (1993, 4a): Consider an exact sequence of abelian groups $0\rightarrow \mathbb{Q} \rightarrow B \rightarrow \mathbb{Z}_6\rightarrow 0$ . Show that $B\cong \mathbb{Q}\oplus \mathbb{Z}_6$ . I know that there are examples of exact sequences of abelian groups which do not split, such as $0\rightarrow \mathbb{Z}_2 \rightarrow \mathbb{Z}_4 \rightarrow \mathbb{Z}_2 \rightarrow 0$ , so my guess is that the presence of $\mathbb{Q}$ plays a role in the existence of a back-map from $B$ , although I've had no luck thus far in constructing it.","['group-theory', 'abstract-algebra', 'abelian-groups', 'exact-sequence']"
3390037,"How to solve the PDE $u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x)$, $u(t,0)=Ae^{-t^2}\sin{\omega t}$","The following PDE is given: $u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x)$ , $u(t,0)=Ae^{-t^2}\sin{\omega t}$ May I ask you for hints about how to solve that PDE ? I don't think that I can use the method of characteristics, because there is a second derivative ( $u_{tt}$ ). Separation of variables doesn't seem to be an option either. And also, don't we need a second initial condition if we have a second partial derivative ? I am truly lost, and would appreciate any hint/help. Edit: separation of variables seems to work, but the initial condition makes things complicated. If we do so, $A$ would depend on $t$ , which cannot be.","['partial-differential-equations', 'analysis', 'real-analysis']"
3390080,Property of Normal Subgroups under Special Conditions,"Let $M$ and $N$ be normal subgroups of $G$ such that $G = MN$ . Show that for any $m \in M$ and $n \in N$ , we can find elements $m' \in M, n' \in N$ such that $mn' = nm'$ . I've tried using properties of normal subgroups (i.e. $gNg^{-1} \subseteq N$ ), but I can't seem to relate m directly with n in the desired form. Any hints?","['normal-subgroups', 'group-theory', 'abstract-algebra']"
3390092,Sum of Banach spaces norm,"Let $ \left ( X_{1},\left \|  \right \|_{1} \right ) $ , $ \left ( X_{2},\left \|  \right \|_{2} \right ) $ two Banach spaces in the vector space X. How to prove that $\left \| x \right \|= \inf \left \{\left \| x_{1} \right \|_{1}+\left \| x_{2} \right \|_{2}:x=x_{1}+x_{2} \ , \ x_{1}\in X_{1} \ \ x_{2}\in X_{2} \right \} $ defines a norm in $ X_{1}+X_{2} $ My attempt : $ \left \| x \right \|=0 \Leftrightarrow \exists \left ( x_{n} \right )_{n\in \mathbb{N}}\subset X_{1} \ ,  \exists \left ( y_{n} \right )_{n\in \mathbb{N}}\subset X_{2}: x=x_{n}+y_{n} \ and \ \lim_{n\rightarrow \infty}\left \| x_{n} \right \|_{1}=0 \ $ and $ \lim_{n\rightarrow \infty}\left \| y_{n} \right \|_{2}=0 $ I can't prove that $x=0$","['banach-spaces', 'functional-analysis']"
3390098,Proof that the ratio between the logs of the product and LCM of the Fibonacci numbers converges to $\frac{\pi^2}{6}$,"I came across this amazing fact on Twitter . $$\lim_{n\to\infty} \frac{\log\left(F_1 \cdots F_n\right)}{\log \text{LCM}\left(F_1, \ldots,  F_n\right)} = \frac{\pi^2}{6}$$ where $F_i$ is the $i$ th Fibonacci number and LCM = Least Common Multiple. This is such an interesting link between the Fibonacci numbers (which are closely linked to the Golden Ratio $\varphi$ ) and $\pi$ . I'm trying to prove why this is the case. I know that, for the numerator, it'll become something like \begin{align}
\log\left(F_1 \cdots F_n\right) &\sim \log\left(\varphi^1 \cdots \varphi^n\right) \\
&\sim \frac{\log \varphi}{2} n^2
\end{align} Reverse engineering, that tells me that the denominator will turn out to be the same, but with an extra factor of $\frac{6}{\pi^2}$ . What I'm wondering is, how to prove that this will be the case? Some relevant tools might be that $$\prod_i \left(1 - \frac{1}{p_i^2}\right) = \frac{6}{\pi^2}$$ , where $p_i$ is the $i$ th prime. Also, it's not hard to show that the LCM of the first $n$ natural numbers is roughly $e^n$ . Finally, the probability that the $n$ th Fibonacci is prime is roughly $\sim\frac{\log\varphi}{n}$ I think (though I'm not actually sure if this has been proven). As an aside, even though it's cool that this is done with the Fibonacci sequence, judging by how the $\varphi$ factor just cleanly cancels on top and bottom, I have a feeling that this fact might be true for other linear integer recurrence relations that have an exploding dominant eigenvalue.","['fibonacci-numbers', 'number-theory', 'gcd-and-lcm', 'pi', 'prime-numbers']"
3390101,Variation of matrix Bernstein inequality,"Let $[X_1, X_2, ..., X_r]$ be a set of independent $d_1 \times d_2$ dimensional random matrices with $\mathbb{E}(X_i) = 0$ and $\|X_i\| \leq B$ (bounded operator norm). Introduce the sum of random matrices, $Z = \sum_{i=1}^r X_i$ Define matrix variance proxy: $$\sigma^2 = \max \left( \| \mathbb{E}(ZZ^T)\|, \|\mathbb{E}(Z^TZ) \|\right) .$$ Then Bernstein inequality for rectangular matrices states that: $$P(\|Z\| \geq t) \leq (d_1+d_2)\exp\left(\frac{-t^2/2}{\sigma^2 + Bt/3}\right).$$ However, I have also seen people use the following version: $$P(\|Z\| \geq t) \leq 2(d_1+d_2)\exp\left(\frac{-t^2/2}{\sigma^2 + Bt}\right).$$ My question is how these two inequalities are related to each other? Does one imply another or they are equivalent?","['inequality', 'concentration-of-measure', 'probability-theory']"
3390118,How to compute derivative with Hadamard product?,"Let $\mathbf{x}$ , $\mathbf{y}$ and $\mathbf{z}$ are $n$ -dimensional column vector, and $$f =  \mathbf{x}\circ \mathbf{y} \circ\mathbf{z}$$ Here $\circ$ is the element-wise Hadamard product. Then how to compute the gradient $\frac{\partial f }{\partial\mathbf{x}}$ , $\frac{\partial f }{\partial\mathbf{y}}$ , and $\frac{\partial f }{\partial\mathbf{z}}$ ? My solution is $$\frac{\partial f }{\partial\mathbf{x}} = \mathbf{y} \circ\mathbf{z}, \frac{\partial f }{\partial\mathbf{y}} = \mathbf{x} \circ\mathbf{z},\frac{\partial f }{\partial\mathbf{z}} = \mathbf{x} \circ\mathbf{y}$$ Is it correct?
thanks.","['derivatives', 'hadamard-product', 'linear-algebra', 'optimization']"
3390192,"What is the probability of winning a best of 5 game, if I already won the first game?","For example, if I am playing a game with a friend and we both are equally likely to win each game, then what is the probability of me winning the best of 5 match, given that I already won the 1st game ? I did some work on this question, but I don't think this method seems so sound. $P$ (I win in 3 games) = $(\frac{1}{2})^2$ $P$ (I win in 4 games) = $(\frac{1}{2})^3 \cdot \frac{2!}{1! \cdot 1!}$ $P$ (I win in 5 games) = $(\frac{1}{2})^4 \cdot \frac{3!}{1! \cdot 2!}$ $P$ (me winning) = sum of the above probabilities = 0.6875","['statistics', 'probability']"
3390213,Showing that successive arches of $y=e^{-\alpha x}\sin\beta x$ bound areas in geometric progression with ratio $q=e^{-\alpha\pi/\beta}$,"Prove that the areas $S_0$ , $S_1$ , $S_2$ , $S_3$ , $\ldots$ , limited by the $x$ -axis and the curve $y=e^{-\alpha x}\sin\beta x$ , for $x\geq 0$ , form a geometric progression of the ratio $q=e^{-\alpha\pi/\beta}$ (see image below) Then I tried to do it by calculation: I don't think it works very well ...","['contest-math', 'calculus', 'trigonometric-integrals', 'trigonometry', 'geometric-progressions']"
3390254,Show $e^{f(x)} = 4x$ has a unique solution using Banach fixed point theorem,"I am stuck trying to prove the following: Let $f: \mathbb{R} \to [0,1]$ be a contractive map. Use the Banach contraction principle to show that the equation $$e^{f(x)} = 4x$$ has a unique solution. In order to use the principle, I need to restrict the domain to $[0,1]$ . Then there is some unique $\overline{x} \in [0,1]$ such that $f(\overline{x}) = \overline{x}$ . The problem I am having is trying to derive the right hand side, since $$e^{f(\overline{x})} = e^\overline{x}$$ does not seem to help much.","['general-topology', 'fixed-point-theorems']"
3390274,Let $ABCD $ a square and $P $ inside the square s.t. $\angle PCD=\angle PDC=15^{°} $.,Let $ABCD $ a square  and $P $ inside the square s.t. $\angle PCD=\angle PDC=15^{°} $ . Show that $PAB $ is equilateral. It's easy to prove that $PA=PB $ . But I need a construction to show that the others sides are congruent. It's possible to solve it without trigonometry? I need a construction.,"['euclidean-geometry', 'geometry']"
3390297,Probability of satisfying an event if I only have 3 attempts to do so,"For example, if I want to attempt to win a challenge, and I only have three tries to win the challenge, then what would $P$ (I win the challenge) be if the probability of me winning the challenge in 1 attempt is $\frac{1}{3}$ ? The attempts are independent, and if I win the challenge on an attempt or fail all 3 tries, then I will not attempt the challenge anymore. To try and solve this question, I did the following: $P$ (Lose an attempt) = 1 - $\frac{1}{3}$ = $\frac{2}{3}$ $P$ (Win all 3 attempts) = 1 - $(\frac{2}{3})^3$ But I don't know if this is correct, since I would stop making further attempts if I won on the first try, or on the second try, etc.","['statistics', 'probability']"
3390331,"If $S$ is a non-empty set, then we can take $r$ such that $r \in S$. Why do we need the axiom of choice? [duplicate]","This question already has answers here : Axiom of Choice: Where does my argument for proving the axiom of choice fail? Help me understand why this is an axiom, and not a theorem. (4 answers) Closed 4 years ago . If $S$ is a non-empty set, then we can take $r$ such that $r \in S$ . Suppose that for any $a \in A$ , $S_a$ is a non-empty set. Then, we can take $r(a)$ such that $r(a) \in S_a$ for any $a \in A$ . Why do we need the axiom of choice in 2.? What is the difference between 1. and 2.? Do I need to study basics of axiomatic set theory to appreciate the axiom of choice?","['elementary-set-theory', 'axiom-of-choice']"
3390380,Problem with Gambler's ruin,"Consider a gambler who has $k$ coins when he enters a casino. The gambler plays a game in which he wins $1$ coin if he wins a round and loses $1$ coin if he loses a round. He wins a round with probability $\displaystyle \frac{1}{2}$ and loses a round with probability $\displaystyle \frac{1}{2}$ . The gambler is considered to win the game if he ends with $n$ coins ( $n \gt k$ ) at some point of time and is considered to lose a game if he ends with $0$ coins. What is the probability that the gambler wins the game on the $m^{th}$ round(where $m\gt n-k$ and $m=n-k+2r $ for some $r\in\Bbb{N}$ ) such that he does not end with $0$ coins or $n$ coins in any of the earlier $m-1$ rounds. $\color{green}{\text{My try:}}$ Due to a lot of restrictions on the parameters and the event, I tried to work out the problems for some small values of $n,m,k$ to get an idea on how the probability might be. On obtaining some sequences of numbers I tried searching the sequence on OEIS to get an idea over the explicit form for the probability. But even after trying a lot of values for $n,m,k$ I couldn't conjecture an explicit form for the probability. If we denote the probability that the gambler wins in the $m^{th} $ round by $p_m$ then I could only conjecture that $$p_m=\displaystyle f_{n,k,m} \left(\frac{1}{2}\right)^{m}$$ For some natural numbers $f_{n,k,m}$ which depend on the values of $n,k,m$ . It is quite easily noticeable that $$f_{n,k,n-k}=1$$ but other than this I couldn't find a general pattern for the $f_{n,k,m}$ 's. Any help would be greatly appreciated. Also if it would be possible to create a generating function for $f_{n,k,m}$ then that generating function would also suffice to solve the problem ( I tried to form a generating function for the $f_{n,k,m}$ 's but failed miserably). * Edit * Some values I tried are (""assuming I have counted them correctly""): $$f_{6,2,4}=f_{6,3,3}=f_{5,2,3}=f_{6,4,2}=f_{5,1,4}=1$$ $$f_{6,2,6}=4$$ $$f_{6,2,8}=13$$ $$f_{6,3,5}=3$$ $$f_{6,3,7}=9$$ $$f_{6,3,9}=27$$ $$f_{5,2,5}=3$$ $$f_{5,2,7}=8$$ $$f_{5,2,9}=21$$ $$f_{5,2,11}=55$$ $$f_{6,4,4}=2$$ $$f_{6,4,6}=5$$ $$f_{6,4,8}=14$$ $$f_{5,1,6}=3$$ $$f_{5,1,8}=8$$ $$f_{5,1,10}=21$$ $$f_{5,1,12}=55$$","['random-walk', 'markov-chains', 'combinatorics', 'probability-theory', 'probability']"
3390404,Convergence of series in probability to 1 implies convergence of pointwise max over $n$ in probability to 0,"If we have non-negative random variables $X_k \geq 0$ such that $\frac{1}{n}\sum_{i = 1}^n X_i \to 1$ in probability. How do we show that $\frac{\max_{1 \leq k \leq n} X_k}{n} \to 0$ in probability? So far I've tried toying with Borel Cantelli and strengthening to strong law, but haven't gotten much.","['borel-cantelli-lemmas', 'law-of-large-numbers', 'probability-limit-theorems', 'probability-theory']"
3390407,A Generalization of Beatty's Theorem,"If $t>0,t^2, t+\frac{1}{t},t+t^2,\frac{1}{t}+\frac{1}{t^2}$ are all irrational number, $$a_n=n+\left \lfloor \frac{n}{t} \right \rfloor+\left \lfloor \frac{n}{t^2} \right \rfloor,\\
b_n=n+\left \lfloor \frac{n}{t} \right \rfloor +\left \lfloor nt \right \rfloor,\\
c_n=n+\left \lfloor nt \right \rfloor+\left \lfloor nt^2 \right \rfloor, $$ then every positive integer appear exactly once. In other words, the sequences $a_1,b_1,c_1,a_2,b_2,c_2,\cdots$ together contain all the positive integers without repetition.
I have checked every integer from $1$ to $10^6$ for $t=2^\frac{1}{4}$ : $$a_n=1, 4, 7, 9, 12, 15, 16, 19, 22, 25, 27, 30, 32, 34, 37, 40, 43, 45, 47, 50,\dots \\
b_n=2, 5, 8, 11, 14, 18, 20, 23, 26, 29, 33, 36, 38, 41, 44, 48, 51, 54, 56, 59,\dots \\
c_n=3, 6, 10, 13, 17, 21, 24, 28, 31, 35, 39, 42, 46, 49, 53, 57, 61, 64, 67, 71,\dots $$ PS: This is a special case of following statement： If $t_1,t_2,\cdots,t_k>0$ ,and $\forall i \not =j,\frac{t_j}{t_i}$ is irrational, $$a_i(n)=\sum_{j=1}^k{\left \lfloor \frac{t_j}{t_i}n \right \rfloor},i=1,2,\cdots,k,$$ then every positive integer appear exactly once in $a_1(n),\cdots,a_k(n)$ .","['number-theory', 'sequences-and-series']"
3390443,Probability of a number being rational,"If $x \in [0, 1]$ , what is $\text{P}(x\in \mathbb Q)$ ? 
  In other words, what is the probability that $x$ is rational? This is what I tried: $$\begin{array}{rcl}\text{P}(x \in \mathbb Q) &=& \displaystyle \int^1_0 f(x)\,dx
\end{array}$$ where $$f(x) = \begin{cases}1, & x \in \mathbb Q\\0, & x\notin\mathbb Q\end{cases}$$ However, the function is not Riemann-integrable. I want to try comparing the cardinalities of the rational set and the irrational set by using a one-to-one mapping between the two sets. But I don't have idea how I can do it. Can anyone give a hint?","['integration', 'elementary-set-theory', 'probability', 'rational-numbers']"
3390449,Show that $x^4-20200y^2=1$ has no solution in postive integers,"Show that $x^4-20200y^2=1$ has no solution in postive integers. This topic is a question for the Chinese middle school students' mathematics competition today, so I think this problem has a simple solution.","['number-theory', 'diophantine-equations']"
3390546,Evaluate $\sum _{k=0}^{\infty } \frac{L_{2 k+1}}{(2 k+1)^2 \binom{2 k}{k}}$,How to prove $$\sum _{k=0}^{\infty } \frac{L_{2 k+1}}{(2 k+1)^2 \binom{2 k}{k}}=\frac{8}{5} \left(C-\frac{1}{8} \pi  \log \left(\frac{\sqrt{50-22 \sqrt{5}}+10}{10-\sqrt{50-22 \sqrt{5}}}\right)\right)$$ Where $L_k$ denotes Lucas number and $C$ Catalan? Any help will be appreciated.,"['lucas-numbers', 'closed-form', 'sequences-and-series']"
3390564,Continued fraction of $\sqrt{D}$,"Let $\sqrt{D}=[a_0;\overline{a_1,\cdots,a_k}],$ where $k$ is the length of circulating part, we know $a_k=2a_0.$ Denote $r_j=[\overline{a_j,a_{j+1},\cdots,a_{j+k-1}}]$ , where $a_{j+k}=a_j,j>0.$ Prove that $r_1 r_2\cdots r_k=x+y\sqrt{D},$ where $x,y$ is the least positive integer solution to $x^2-Dy^2=(-1)^k.$ For example, $D=14,\sqrt{14}=[3,\overline{1,2,1,6}],k=4.$ $$r_1=[\overline{1,2,1,6}]=\frac{3+\sqrt{14}}{5},\\
r_2=[\overline{2,1,6,1}]=\frac{2+\sqrt{14}}{2},\\
r_3=[\overline{1,6,1,2}]=\frac{2+\sqrt{14}}{5},\\
r_4=[\overline{6,1,2,1}]=3+\sqrt{14},\\
r_1r_2r_3r_4=15+4\sqrt{14},$$ and $15^2-14 \times 4^2=1.$","['number-theory', 'continued-fractions', 'pell-type-equations']"
3390600,Is the composition of a monotonic/strictly increasing/decreasing function $f$ with another function $g$ also monotonic/strictly increasing/decreasing?,"I believe, I read a theorem that stated that if $f(x)$ is a monotonic increasing/decreasing or strictly increasing/decreasing then $g(f(x))$ is also monotonic increasing/decreasing or strictly increasing/decreasing. Is this true?","['calculus', 'functions']"
3390641,Where can I learn about binary quartic forms?,"While studying a larger problem, I occured upon the question of which integers can be represented by the binary quartic form $$f(x,y)=xy(x^2+y^2)$$ with $x,y\in\mathbb Z$ . The standard text An Introduction to the Theory of Numbers by Niven, Zuckerman and Montgomery has an excellent chapter on binary quadratic forms, but unfortunately does not go into cubic or quartic forms. A simple Google search on ""binary quartic forms"" returns multiple papers on the subject, but nothing containing a systematic discussion about it. Several other Number Theory books I've seen do not appear to go into the theory of binary quartic forms either. Question. Where can I learn about binary quartic forms? Note that although the form $f(x,y)$ mentioned above is one of the reasons I want to learn about this, the purpose of this question is not to ask for an answer to the specific question of which integers are represented by $f(x,y)$ . A reference/link to any resource (e.g. a textbook) which contains a systematic discussion on the subject from which I can start learning would be most appreciated. Thanks in advance! Please note that I am mainly looking for a reference for binary quartic forms (not quadratic!), over $\mathbb Z$ specifically--although I would be happy to look at references which also discuss forms more generally, and where this is only, say, one of the subsections.","['number-theory', 'abstract-algebra', 'book-recommendation', 'reference-request']"
