question_id,title,body,tags
3012418,Probability that Carl has to wait at least 10 minutes for one of the others and for both of the others to show up?,"This question is for the other subquestions for the same problem here . For those not willing to click the link, I will post the exercise problem here as well. Alice, Bob, and Carl arrange to meet for lunch on a certain day. They arrive independently at uniformly distributed times between 1 p.m. and 1:30 p.m. For the rest of the problem, assume Carl arrives first at 1:10 p.m. and condition on this fact. (b) What is the probability that Carl will have to wait more than 10 minutes for one of the others to show up? (c) What is the probability that Carl will have to wait more than 10 minutes for both of the others to show up? Apparently, my approach to the solution has given me the answers in a swapped manner, which I can't understand why. Allow me to elaborate. My approach Let's say that the events for Alice and Bob arriving are each $A$ and $B$ i.i.d. on Unif( $10,\ 30$ ). (b) The probability that Carl will wait more than 10 minutes for at least one of the others to arrive is: (Probability that Carl waits less than 10 minutes for Alice and more than 10 for Bob) + (Probability that Carl waits less than 10 minutes for Bob and more than 10 for Alice) + (Probability that Carl waits more than 10 minutes for both) which gives me the result of $\left(\frac{1}{2} * \frac{1}{2}\right) + \left(\frac{1}{2} * \frac{1}{2}\right) + \left(\frac{1}{2} * \frac{1}{2}\right) = \frac{3}{4}$ . (c) Probability that Carl waits more than 10 minutes for both is the probability that $A$ and $B$ both fall in the interval ( $20$ , $30$ ) in the total interval ( $10$ , $30$ ). Therefore we get the probability $\frac{1}{2} * \frac{1}{2} = \frac{1}{4}$ . However, apparently the correct answer is $\frac{1}{4}$ for (b) and $\frac{3}{4}$ for (c), and I'm not entirely sure why. The rationale the textbook gives is that for (b) we need to compute the time that both of them arrive after 1:20 p.m., and for (c) we simple take the complement that both of them arrive between 1:10 and 1:20 p.m. $1 - \frac{1}{4}$ . I'm not entirely sure how these solutions make sense, though, and was hoping someone would be kind enough to provide me with some insight. Thank you.","['uniform-distribution', 'probability-theory', 'probability']"
3012492,A guide to Algebraic Geometry,"I have completed one semester course on Commutative Algebra and Riemann Surfaces, and currently I am trying to read Algebraic Geometry. While reading from different books I feel that I must need a direction such that I can cover much topics in a semester. That is to say, what are the fundamental results or topics in Algebraic Geometry that I should read in a one semester course. Besides this I feel that I should follow some text books or some well written lecture notes as a prepratory metarial. It will be helpful to me if someone guide me by saying that which topics I have to read , from which books or notes. Thanks in advance.","['riemann-surfaces', 'book-recommendation', 'algebraic-geometry', 'soft-question', 'commutative-algebra']"
3012506,Deriving the probability on the Probability of an Event - Chebyshev's Theorem,"Question: For the random variable X, where E(X) = 0 and Var(X) = 1, derive the upper bound on the probability of the event $\{|X-.6| > .2\}$ . My attempt: Using Chebyshev's Inequality- Prob( $|X-E(X)| > \epsilon) \leq \frac{Var(X)}{\epsilon^2} = \frac{1}{(.2)^2} = 25.$ A) With no distributional assumption is this correct? I am thinking that because the probability is greater than 1 then this is a trivial inequality. I have seen alternate definitions of Chebyshev's inequality where $\epsilon$ is $k \sigma$ and if k is greater than or equal to 1 then this happens. However, 25 seems like an unusual number to get. Am I missing something? B) As a follow up and for further understanding, how would the probability change if X was normally distributed with mean = 0 and variance = 1? Using the probability normal approximation integral I get the probability to be 3.96953... Although 25 and 3.96 are very different, is accuracy even a question since both probabilities are greater than 1?","['statistics', 'probability']"
3012511,"If $Q_kR_k$ converges to $QR$, where this represents their respective $QR$ decompositions, then $Q_k \rightarrow Q$ and $R_k \rightarrow R$?","Suppose $Q_kR_k \rightarrow QR$ as $k \rightarrow \infty$ , where $Q_k, Q$ are orthogonal matrices and $R_k, R$ are upper triangular with positive diagonal entries, then would the uniqueness of the $QR$ decomposition imply that $Q_k \rightarrow Q$ and $R_k \rightarrow R?$ I need this detail for a proof, but I wasn't able to prove it. It would suffice to show that if $Q_kR_k \rightarrow I$ , then $Q_k \rightarrow I$ and $R_k \rightarrow I$ . Edit: If the limit of $Q_k$ and $R_k$ exist, then they must be $I$ , but how would one show that these limits exist, if they do?","['matrices', 'matrix-decomposition']"
3012513,Is there a systematic way to construct functions with prescribed local extrema?,"I'm teaching multivariable calculus and having a hard time coming up with optimization problems. Suppose I have three lists of points $\{a_1, \dotsc, a_r\}$ , $\{b_1, \dotsc, b_s\}$ , and $\{c_1, \dotsc, c_t\}$ in $\Bbb R^n$ . Is it possible to construct a function $f:\Bbb R^n\to\Bbb R$ that has a local max at every $a_i$ , a local min at every $b_i$ , a saddle point at every $c_i$ , and no other local extrema? Furthermore, is it possible to accomplish this with $f\in\Bbb R[x_1, \dotsc, x_n]$ ? A weaker version of this question is: given points $\{p_1,\dotsc,p_k\}$ in $\Bbb R^n$ , is it possible to construct a function $f:\Bbb R^n\to \Bbb R$ whose set of critical points is exactly $\{p_1,\dotsc,p_k\}$ ? So, for instance, is there a systematic way to construct $f\in\Bbb R[x, y, z]$ with a local maximum at $(1, -3, 2)$ and a saddle point at $(2, -8, 1)$ ?","['optimization', 'multivariable-calculus', 'hessian-matrix']"
3012524,Are these answers regarding focus and directrix correct?,"1. Find the vertex, focus and directrix for the parabola given by $y = 2x^2$ $(y-0) = 2(x-0)^2$ $(x-0)^2 = \frac{1}{2}(y-0)$ $4p = \frac{1}{2} \implies p = \frac{1}{8}$ V: $(0,0)$ F: $(0,\frac{1}{8})$ D: $y = -\frac{1}{8}$ 2. Find the vertex, focus and directrix for the parabola given by $$y = -2018x^2$$ $(x-0)^2 = -\frac{1}{2018}(y-0)$ $4p = -\frac{1}{2018} \implies p = -\frac{1}{8072}$ V: $(0,0)$ F: $(0,-\frac{1}{8072})$ D: $y = \frac{1}{8072}$ 3. Find the vertex, focus and directrix for the parabola given by $(y-2)^2 = 8(x+5)$ $4p = 8 \implies p = 2$ V: $(-5,2)$ F: $(-3,2)$ D: $x= -7$ 4. Find the vertex, focus and directrix for the parabola given by $(y+6)^2 = \frac{1}{2}(x-1)$ $4p = \frac{1}{2} \implies p = \frac{1}{8}$ V: $(1,-6)$ F: $(\frac{9}{8},-6)$ D: $x = \frac{7}{8}$ 5. Find the vertex, focus and directrix for the parabola given by $y = 2x^2+5x-7$ $x^2+\frac{5}{2}x-\frac{7}{2} = \frac{y}{2}$ $x^2+\frac{5}{2}x = \frac{y}{2} +\frac{7}{2}$ $x^2+\frac{5}{2}x +\frac{25}{16}= \frac{y}{2} +\frac{7}{2}+\frac{25}{16}$ $(x+\frac{5}{4})^2 = \frac{81}{16}+\frac{y}{2}$ $(x+\frac{5}{4})^2 = \frac{1}{2}(\frac{81}{8}+y)$ $4p = \frac{1}{2} \implies p = \frac{1}{8}$ V: $(-\frac{5}{2}, -\frac{81}{8})$ F: $(-\frac{5}{2}, -\frac{80}{8})$ D: $y = -\frac{82}{8}$ 6.  Find the vertex, focus and directrix for the parabola given by $y = -\frac{x^2}{4} - 2x +8$ $-4y = x^2 +8x -32$ $x^2 +8x +16 = 16+32+(-4y)$ $(x+4)^2 = 48 - 4y$ $(x+4)^2 = 4(12 - y)$ $(x+4)^2 = -4(y-12)$ $4p = -4 \implies p= -1$ V: $(-4,12)$ F: $(-4,11)$ D: $y = 13$ 7. Find the equation of the parabola which passes through the point $(8,12)$ with a vertex of $(4,-2)$ This one I'm a little confused about, aren't there multiple parabolas which could could through this point and have this vertex? I'm just going to assume it's squared in the x term, so: $(x-4)^2 = 4p(y+2)$ $(8-4)^2 = 4p(12+2)$ $16 = 4p(14)$ $\frac{16}{14} = 4p \implies p = \frac{4}{14}$ The equation is $(x-4)^2 = \frac{16}{14} (y+2)$ 8. Explain, with words, how the distance between the vertex and focus of a
parabola affects the steepness of the parabola As the distance between the focus and directrix increases, $|p|$ decreases which means the parabola widens.","['conic-sections', 'geometry']"
3012569,Substitutions in Modular Arithmetic,"I've just learned modular arithmetic today, and am really struggling to understand a certain theorem. The theorem given to us states the following: Let m $\in \mathbb{N}$ . For any integers a, b, c, and d, if $a \equiv b \pmod m$ and $c \equiv d \pmod m$ then, $a+c \equiv b+d \pmod m$ $a-c \equiv b-d \pmod m$ $ac \equiv bd \pmod m$ In the next section, the notes state the following: ""We can use properties of congruence to prove the (familiar) rule that an
integer is divisible by 3 if and only if the sum of its decimal digits is divisible
by 3. The key is to observe that $10 \equiv 1 \pmod 3$ and so by Theorem 5.10.3 [theorem stated above]
you can change 10 to 1 wherever it occurs. Remember that $3|n$ if and only
if $n \equiv 0 \pmod 3$ ."" Next, it goes through the proof it was talking about at the beginning of the first quote: Suppose $n=d_k \cdot b_k + d_{k-1}\cdot b_{k-1} + \dots + d_1\cdot b + d_0$ where $d_k, d_{k-1},\dots, d_0$ are the digits of $n$ . Also assume that $3|n$ . We now have the following: \begin{align}
n \equiv 0 \pmod 3 &\iff d_k\cdot 10^k + d_{k-1}\cdot 10^{k-1} + \dots + d_1\cdot 10 + d_0 \equiv 0 \pmod 3\\
&\iff d_k \cdot 1^k + d_{k-1}\cdot 1^{k-1} + \dots + d_1 \cdot 1 + d_0 \equiv 0 \pmod 3
\end{align} since $10 \equiv 1 \pmod 3$ . I don't quite understand how any parts of the theorem stated above allows for substitution. Thanks for any help.","['number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3012643,Multidimensionally Lipschitz continuous iff Lipschitz continuous in every coordinate,"Definition . A function $f$ defined on a set $S \subseteq \mathbb R$ is said to be Lipschitz continuous on $S$ if there exists an $L \geq 0$ such that $$\|f(x_1) - f(x_2)\| \le L\|x_1 - x_2\|$$ for all $x_1$ and $x_2$ in $S$ such that $x_1 \ne x_2$ . Consider some function $f: \mathbb R^n \to \mathbb R^n$ taking several variables $x^{(k)}$ with $1 < k \leq n$ as argument. Let $f_i: \mathbb R^n \to \mathbb R$ be the function of $i$ -th coordinate of the function value of $f$ . Are the following statements true? If $f$ is Lipschitz continuous, then $f_i$ is Lipschitz continuous $\forall i \in \{1, \dots, n\}$ . If $f_i$ is Lipschitz continuous $\forall i \in \{1, \dots, n\}$ , then $f$ is Lipschitz continuous.","['continuity', 'multivariable-calculus', 'lipschitz-functions']"
3012664,Why is this integral (that looks complicated) so easy to evaluate?,I encounter the following integral during Statistics lecture and the professor quickly solved it as if he assumed we know that this integral is easy so solve: $$ \int_{0}^{\infty} xe^{-(1-t)x} dx = \frac{1}{(1-t)^2}$$ I suspect that this involves a probability distribution but I do not know which. Why is this integral so easy to solve? I know I must have been missing something.,"['integration', 'probability-distributions', 'improper-integrals', 'probability-theory']"
3012711,"Showing that $\|x \| = \sup_{y \neq 0} \frac{|\langle x,y \rangle|}{\|y\|}$","Exercise : Let $H$ be an inner product space and $x \in H$ . Show that : $$\|x \| = \sup_{y \neq 0} \frac{|\langle x,y \rangle|}{\|y\|}$$ Attempt : If $x=0$ then the equality follows imidiatelly as an equality with respect to $0$ . Let it now be that $x \neq 0$ . For $y \in H$ with $y \neq 0$ , by the Cauchy-Schwarz inequality, it is : $$\langle x,y \rangle^2 \leq \langle x,x \rangle \cdot \langle y,y \rangle \Leftrightarrow |\langle x,x \rangle|\leq \langle x,x \rangle^{1/2} \cdot \langle y,y \rangle^{1/2} = \|x\|\cdot\|y\|$$ Thus, it also holds that : $$\|x\| \geq \sup_{y \neq 0} \frac{|\langle x,y \rangle|}{\|y\|}$$ How would I show the other direction of the inequality, though, to prove that it must be equal to it ? I thought about expressing $\|x\|$ as $$\|x\| = \bigg\langle x,\frac{x}{\|x\|}\bigg\rangle$$ but can't see anything obvious. Any tips will be greatly appreciated.","['inner-products', 'normed-spaces', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
3012768,Showing that the equation $x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i$ has a unique solution.,"Exercise : Consider the infinite-dimensional system of equations : $$x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i, \quad i=1,2,3,\dots$$ We suppose that $b=(b_1,b_2,\dots) \in \ell^\infty$ and that it exists $0<\theta<1$ such that : $$\sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta$$ Show that the system of equations given has a unique solution $x=(x_1,x_2,\dots) \in \ell^\infty$ . Attempt : In previous exercises and lessons, I have proved the following lemma : Let $X$ be a Banach space and $T \in B(X)$ with $\|T\| \leq \theta < 1$ . Then, if $y \in X$ , the equation $x = y + Tx$ has a unique solution $x \in X$ . My initial idea is to work over manipulating the exercise to reach the statements of the lemma above. The equation for the infinite-dimensional system of equations given, can be rewritten as : $$x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i \Rightarrow x = b + Tx$$ where $x = (x_1,x_2,\dots)$ and $T$ be an operator, such that : $$Tx = \sum_{j=1}^\infty a_{ij}x_j$$ Now, I observe that $\ell^\infty$ is a Banach space and since $b \in \ell^\infty$ then $x - Tx \in \ell^\infty$ which means that $x \in \ell^\infty$ and $T$ is an operator defined over $\ell^\infty$ . Now, I need to prove that $T \in B(\ell^\infty)$ , which means that $T$ is a bounded linear operator $T : \ell^\infty \to \ell^\infty$ and that $\|T\| < 1$ . For the case of linearity, let $x,y \in \ell^\infty$ and $\lambda \in \mathbb R$ . Then, it is $$T(\lambda x + y) = \sum_{j=1}^\infty a_{ij}(\lambda x_j + y_j) = \sum_{j=1}^\infty \lambda a_{ij} x_j + \sum_{j=1}^\infty a_{ij}y_j$$ $$=$$ $$\lambda \sum_{j=1}^\infty a_{ij}x_j + \sum_{j=1}^\infty a_{ij}y_j = \lambda T x + Ty$$ which proves that $T$ is a linear operator. Now, for the case of $T$ being bounded : $$\|Tx\|_\infty = \bigg\|\sum_{j=1}^\infty a_{ij}x_j \bigg\|_\infty \leq \sum_{j=1}^\infty \|a_{ij}x_j\|_\infty \leq \sum_{j=1}^\infty |a_{ij}| \|x\|_\infty$$ But, it is $$\sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta < 1$$ thus, by combining the two last results above, we get : $$\|Tx\|_\infty \leq \theta \|x\|_\infty$$ which proves that $T$ is a bounded linear operator $T \in B(\ell^\infty)$ and thus the equation we transformed has a unique solution $x \in \ell^\infty$ which means that the initial infinite-dimensional system of equations has a unique solution $x=(x_1,x2,\dots) \in \ell^\infty$ . Question : Is my approach correct and rigorous enough ? Any recommendations on what I can improve or any mistakes ? I would appreciate any input or approvement of my approach.","['banach-spaces', 'operator-theory', 'proof-verification', 'real-analysis', 'functional-analysis']"
3012786,Use $\cos 5\theta$ to find the roots of $x(16x^4 - 20x^2 + 5) = 0$,"I used $\cos(3\theta + 2\theta)$ to prove the first part, but I don't know how to the $2$ nd part. Show that $\cos 5\theta=16\cos^5\theta-20\cos^3\theta+5\cos\theta,$ and hence show that $$\text{the roots of }x(16x^4 - 20x^2 + 5) \text{ are: } 0,\cos\frac{\pi}{10}, \cos\frac{3\pi}{10},\cos\frac{7\pi}{10}, \cos\frac{9\pi}{10}$$","['trigonometry', 'complex-numbers']"
3012811,Showing $\sum\limits_{j=0}^M \frac{M \choose j}{N+M \choose j} = \frac{N+M+1}{N+1}$,"In an answer to another question , I stated $$\sum\limits_{j=0}^M \frac{M \choose j}{N+M \choose j} = \frac{N+M+1}{N+1}.$$ It is clearly true when $N=0$ since you add up $M+1$ copies of $1$ , and when $M=0$ since you add up one copy of $1$ .  And, for example, with $M=4$ and $N=9$ you get $\frac{1}{1}+\frac{4}{13}+\frac{6}{78}+\frac{4}{286}+\frac{1}{715} = \frac{14}{10}$ as expected. But how might you approach a general proof?","['binomial-coefficients', 'combinatorics']"
3012819,A function whose averages lie in a set does not need to take values in this set almost everywhere,"Let $ (X,\Sigma,\mu)$ be a finite measure space. ( $\mu(X)<\infty$ ). Let $f \in L^1(\mu)$ be a complex-valued function, and let $S \subseteq C$ be a non-closed subset. Suppose that for every $E \in \Sigma$ , $\frac{1}{\mu(E)}\int_E fd\mu \in S  $ . Is it true that $f(x) \in S$ almost everywhere? I know this is true if $S$ is closed (this is theorem 1.40 in ""Real and complex analysis"" by Rudin). I am looking for counter-examples when $S$ is not closed.","['measure-theory', 'examples-counterexamples']"
3013001,Is the conductor of an L-function F the absolute value of the discriminant ofsome number field related to F?,"In the theory of automorphic forms, ramified primes of an L-function divide the so-called conductor thereof. On the other hand, one can define for a number field $ K $ an integral invariant $ \Delta_{K} $ equal to the square of the determinant of some square matrix defined through a basis of the ring of integers $ O_{K} $ and complex embeddings.
It appears that the conductor of the Dedekind zeta function of a number field $ K $ is $ \vert \Delta_{K}\vert $ . Hence my question  : is it possible for any L-function $ F $ to define a related number field whose absolute value of the discriminant is the conductor of $ F $ ? If yes, how is it defined ?","['number-theory', 'l-functions', 'algebraic-number-theory', 'prime-numbers']"
3013007,Notation: limit in two variables,"I want to show that the function $$
f: \mathbb{R}^2 \to \mathbb{R}, \
(x,y) \mapsto \begin{cases}
x^2(x-1)(y-1)\sin(xy), & (x,y) \in [0,1]^2 \\ 0, & \text{elsewhere.}
\end{cases}
$$ is continuous. Obviously, both pieces are continuous because they are composed of elementary continuous functions.
Now I only need to show that the transition between both pieces is continuous and don't know how to notate it properly. My Idea was \begin{align*}
            \lim_{\|x,y\|_{\infty} \nearrow 1} f|_{[0,1]}
            & = \lim_{\max(x,y) \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\
            & = \begin{cases}
            \lim_{x \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\
            \lim_{y \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy)
            \end{cases}
            = \begin{cases} 1^2(1 - 1)(y - 1)\sin(y) \\ x^2(x - 1)(1 - 1)\sin(x) \end{cases}
            = 0
        \end{align*} I've never seen it done that way but didn't have a better idea. Second Attempt For all $a \in [0,1]$ we have \begin{equation*}   
    \begin{cases}
        \lim\limits_{(x,y)\to(0,a)} f(x,y)
        = 0^2(0 - 1)(a - 1)\sin(0)
        = 0
        = f(0,a), \\
        \lim\limits_{(x,y)\to(1,a)} f(x,y)
        = 1^2(1 - 1)(a - 1)\sin(y)
        = 0
        = f(1,a). \\
        \lim\limits_{(x,y)\to(a,0)} f(x,y)
        = a^2(a - 1)(0 - 1)\sin(0)
        = 0
        = f(a,0), \\
        \lim\limits_{(x,y)\to(a,1)} f(x,1)
        = a^2(a - 1)(1 - 1)\sin(x)
        = 0
        = f(a,1) 
    \end{cases}
\end{equation*}","['proof-verification', 'real-analysis', 'continuity', 'multivariable-calculus', 'limits']"
3013010,How to find out the global minimum of the following expression,"What is the global minimum of the expression \begin{align} |x-1| &+ |x-2|+|x-5|+|x-6|+|x-8|+|x-9|+|x- 10| \\&+
 |x-11|+|x-12|+|x-17|+|x-24|+|x-31|+ |x-32|? \end{align} I've solved questions of this sort before but there were only 3 terms. I solved those by expanding all the terms in the modulus and drawing a graph. This question came in a paper which requires the student to solve it within 5 minutes. What's a better method?",['calculus']
3013058,Probability of three sequential events,"A smuggler wants to transfer his smuggled goods from city A to city B. There are three police check posts between these two cities. Assume that there is no communication among the check posts. The probabilities of him being caught at these three stops are $0.7, 0.5$ and $0.3$ respectively. What is the probability that he successfully transfers his goods? A] $0.105$ B] $0.5$ C] $0.245$ D] $0.045$ Is it as simple as calculating success in all three scenarios: $0.3 * 0.5 * 0.7 = 0.105$ . Is A correct answer ?","['proof-verification', 'probability']"
3013072,Exact sequence with finite groups: $0 \to A \xrightarrow[]{\alpha} \mathbb{Z}^d \xrightarrow[]{\beta} \mathbb{Z} \xrightarrow[]{\gamma} B \to0$.,"I have this exact sequence of abelian groups: $$0 \to A \xrightarrow[]{\alpha} \mathbb{Z}^d \xrightarrow[]{\beta} \mathbb{Z} \xrightarrow[]{\gamma} B \to0$$ with $A$ and $B$ finite abelian groups and $d \in \mathbb{N}$ . I would like to conclude $d=1$ , but I have no idea how could use the information $A,B$ are finite. The only thing I noticed that $\gamma$ cannot be injective, but I don't know how to proceed. I'm a bit rusty in Algebra... Could someone give some help? Thanks!","['group-theory', 'abstract-algebra', 'modules']"
3013115,Choosing the best or second best secretary,"We have a hall with $N$ secretaries.  We choose randomly one secretary at a time and interview them.  After the interview either we hire the secretary (and all others go home) or move to the next one (we can't go back and hire them later). The interviewer wants to hire either the best or second best secretary with the highest probability My approach: I know we should use this strategy: after we interview the first $aN$ candidates (which we don't hire), then we choose the first one that's better than the previous $aN$ candidates. I am just stuck knowing what is the value of $a$ .  What is the probability of this value? and how we get to it? I have seen the ""Secretary Problem"" where we want to choose the best one, the difference here is that we are satisfied with the best or second best one equally",['probability']
3013148,Characterization of sets in $\mathcal F^X_t =\sigma(\{X_s: s\leq t\})$ where $(X_t)_t$ is a stochastic process,"I'm selfstudying stochastic processes. There is a question on the characterization of sets in $\mathcal F_t^X:=\sigma(\{X_s: s\leq t \})$ . Let $T \subset \mathbb R^+$ be the time index set. We consider $\Omega \subset E^T$ where $E$ is some (nice) set, e.g. $\mathbb R$ with the property that for eacht $t\in T$ and $\omega\in \Omega$ there is $\bar \omega  \in \Omega$ such that $\bar \omega_s = \omega_{s\wedge t}$ for all $s\in T$ (I personally don't know why we need this). Let $\mathcal F=\mathcal E^T\cap \Omega$ where $\mathcal E^T$ is the $\sigma$ -algebra on $E^T$ . Let $X=(X_t)_{t\in T}$ be the canonical process on $(\Omega,\mathcal F)$ . The problem I have is to prove the following assertion: Assertion. $A\in \mathcal F^X_t$ implies the following $A\in\mathcal F_\infty^X$ . $\omega \in A$ and $X_s(\omega)=X_s(\omega')$ for all $s\in T$ with $s\leq t$ imply $\omega'\in A$ . Attempt. Number (1) follows from $\mathcal F_t^X\subset\mathcal F_\infty^X$ . But I'm having troubles with the second implication. I started like this, let $\mathcal C$ be a $\pi$ -system that generates $\mathcal E$ . Then in one of the notes they say $\mathcal F_t^X$ is generated by the following $\pi$ -system $\mathcal C_t^X$ , defined as \begin{align}
\mathcal C_t^X = \{X_{t_1}^{-1}(C_1)\cap ...\cap X_{t_n}^{-1}(C_n) : t_1<t_2 <...<t_n\leq t, C_1,...,C_n\in\mathcal C, n=1,2,....\}
\end{align} Of course, if $A\in \mathcal C_t^X$ , then $\omega \in A$ if and only if \begin{align*}
X_{t_1}(\omega) \in C_1,....,X_{t_n}(\omega)\in C_n
\end{align*} for some set $C_1, C_2,..$ etc. But then it is immediately clear that $\omega'\in A$ . I can only do this in the case $A$ is in the $\pi$ -system. How can I conclude that the assertion also holds if $A$ is not in the $\pi$ -system?","['stochastic-processes', 'measure-theory', 'probability-theory']"
3013166,Compute $\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p}$ where $p\geq 1$,"I was recently told to compute some integral, and the result turned out to be a scalar multiple of the series $$\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p},$$ where $p\geq 1$ . I know it converges by comparison for $$\dfrac{1}{(n(n+1))^p}\leq\dfrac{1}{n(n+1)}<\dfrac{1}{n^2},$$ and we know thanks to Euler that $$\sum\limits_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6.$$ I managed to work out the cases where $p=1$ and $p=2$ . With $p=1$ being a telescoping sum, and my solution for $p=2$ being $$\frac13\pi^2-3,$$ which I obtained based on Euler's solution to the Basel Problem. I see no way to generalize the results to values to arbitrary values of $p$ however. Any advice on where to start would be much appreciated. Also, in absence of another formula, is the series itself a valid answer? Given that it converges of course.","['sequences-and-series', 'real-analysis']"
3013168,Solve the integro-differential equation $x^2 \frac{dy}{dx}+\int_{0}^{x} t^2y^5(t)dt=0$,If $y$ is a function of $x$ then solve The integro-differential equation $$x^2 \frac{dy}{dx}+\int_{0}^{x} t^2y^5(t)dt=0$$ I tried to differentiate both sides w.r.t $x$ we get $$x^2 \frac{d^2y}{dx^2}+2x\frac{dy}{dx}+x^2y^5=0$$ $\implies$ $$x\frac{d^2y}{dx^2}+2\frac{dy}{dx}+xy^5=0$$ Letting $p=\frac{dy}{dx}$ we have a Linear differential equation as: $$\frac{dp}{dx}+\frac{2p}{x}=-y^5$$ using Integrating factor method we get $$px^2=-\int x^2y^5dx+C$$ How to proceed from here?,"['algebra-precalculus', 'integral-equations', 'ordinary-differential-equations']"
3013195,Can the empty set be the image of a function on $\mathbb{N}$? [duplicate],"This question already has answers here : Why is there no function with a nonempty domain and an empty range? (6 answers) Closed 5 years ago . I cannot find any example of function $f:\mathbb{N}\rightarrow\mathbb{N}$ of which we can say that $$
f(\mathbb{N})=\emptyset
$$ Does there exists any?","['elementary-set-theory', 'functions']"
3013206,Birthday problem: why not: combinations without/with replacement??,My first intuition on the birthday problem was: The number of ways $k$ people have different birthdays is the combinations $\binom{365}{k}$ The number of ways $k$ people can have birthdays is the combinations with replacement . So the probability that all $k$ people have birthdays on different days is $$ P = \frac{\binom{365}{k}}{\binom{365+k-1}{k}}$$ What is wrong with this logic?,"['statistics', 'combinatorics']"
3013209,Permutations vs. Combinatorial vs. Factorials vs. Exponents,"I'm currently working on a probability course, and I am constantly having trouble figuring out when to use permutations vs. combinations vs. factorials vs. exponents in order to calculate sample size, or in order to complete calculations. I was wondering if there is some kind of set of rules to go by when deciding when to use each? Thank you very much.","['permutations', 'combinations', 'factorial', 'probability']"
3013214,Functions satisfying $f(a+b) =\frac{f(a)+f(b)}{f(a)*f(b)}$,"i was looking for a function that satisfies $f(a+b) =\frac{f(a)+f(b)}{f(a)*f(b)}$ for all $a,b \in \Bbb{N}$ . I have never seen such a problem before and i would like some kind of help to get me started.","['functional-equations', 'functions']"
3013257,Closed form of $\int{\lfloor{x}\rfloor}dx$,"I calculated $\int{\lfloor{x}\rfloor}dx$ and i got this result: $$\int{\lfloor{x}\rfloor}dx = \frac{x^2-x}{2}+\sum_{k=1}^{\infty}\left(\frac{\sin(k\pi x)}{k\pi}\right)^2+c$$ Do you know if this series have a closed form? We found a nice identity! $$\int{\lfloor{x}\rfloor}dx = \frac{\pi^2\left(x^2-x\right)+\Re\left[ \mathrm{Li_2}\left(e^{i2\pi x}\right)\right]}{2\pi^2}+c$$ So $$\int_0^x{\lfloor{t}\rfloor}dt = \frac{\pi^2\left(x^2-x\right)+\Re\left[ \mathrm{Li_2}\left(e^{i2\pi x}\right)\right]}{2\pi^2}+\frac{1}{12}$$ Also, using $$\Re\left[ \mathrm{Li_2}\left(e^{ix}\right)\right] =\sum_{k\ge1}\frac{\cos(kx)}{k}=\frac{x^2}{4}+\frac{\pi x}{2}+\frac{\pi^2}{6}, \forall x\in\left[0,2\pi\right]$$ We get $$\forall x\in\left[0,1\right] \forall \alpha\in\mathbb{Z},\ \ \ \ \ \Re\left[ \mathrm{Li_2}\left(e^{i2\pi x}\right)\right] =\pi^2\left((x-\alpha)^2-(x-\alpha)+\frac{1}{6}\right)$$","['integration', 'ceiling-and-floor-functions', 'calculus', 'polylogarithm', 'sequences-and-series']"
3013259,The $\sigma$-algebra generated by $F = \{A \subseteq \mathbb{R} : 0 \in A^{\circ} \text{or } 0 \in (A^c)^\circ\}$ contains all singletons.,"I am trying to show that the $\sigma$ -algebra generated by $F = \{A \subseteq \mathbb{R} : 0 \in A^{\circ} \text{or } 0 \in (A^c)^\circ\}$ contains all singleton sets $\{x\} \in \mathbb{R}$ . I know that $F$ is an algebra. I believe $F$ itself is not a $\sigma$ -algebra, since, if we take an infinite sequence of $A_n \in F$ such that $0 \in ((A_n)^c)^\circ$ for each $n \in \mathbb{N}$ then we do not necessarily have that $0 \in \cup_{n=1}^{\infty}A_n$ (since $0 \in \cap_{n=1}^{\infty}(A_n^c)^\circ \supseteq (\cap_{n=1}^{\infty}((A_n)^c)^\circ = (\cup_{n=1}^{\infty}A_n)^\circ$ ). So, I am not exactly sure how to explicitly construct and/or describe the $\sigma$ -algebra generated by $F$ . (From there, I am pretty sure I should be able to show that the singleton sets are in $\sigma(F)$ . I just don't know how to get to that point in the first place.) This link gave an explanation about constructing a $\sigma$ -algebra from a collection of sets, but it ended up being more confusing than helpful.","['elementary-set-theory', 'real-analysis']"
3013273,"$G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot \text{sign}(g(x)) \mathrm{d}x$","Today during an exam I got the following exercise : Let $h, g \in C^0([0,1], \| \cdot \|_1)$ such that the set : $Z(g) = \{x \in [0,1] \mid g(x) = 0\}$ is a finite union of intervals. Then let's defined : $$ G : t \mapsto \| g + th \|_1$$ If the function $G$ has a derivative at $0$ prove that : $$G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot sign(g(x)) \mathrm{d}x$$ First of all I don't understand how $G$ is defined because do we consider that : $$G(t) = \int_{[0,1]} \mid g(x) + th(x) \mid \mathrm{d}x$$ Or we consider that : $$G(t) = \int_{[0,1]} \mid g(t) + th(t) \mid \mathrm{d}t$$ ? Then I tried considering : $$\frac{G(h) - G(0)}{h}$$ In order to find the derivative. Yet the $\mid \cdot \mid$ make the task not so easy. So I tried the to split the integrand and study the part : $\int_{Z(g)}$ I get (using the second interpretation of $G$ ) : $$\frac{\int_{Z(g)} \mid th(t) \mid \mathrm{d}t}{t} $$ But it doesn't seem to help...","['integration', 'real-analysis', 'calculus', 'sequences-and-series', 'derivatives']"
3013334,Calculate $\sum_{k=0}^{n} \frac{(-1)^kk}{4k^2-1}$,"Calculate $\sum_{k=0}^{n} \frac{(-1)^k k}{4k^2-1}$ $\sum_{k=0}^{n} \frac{(-1)^k k}{4k^2-1}=\sum_{k=0}^{n} \frac{(-1)^k k}{(2k-1)(2k+1)}=\sum_{k=0}^{n} (-1)^k k\frac{1}{2}(\frac{1}{2k-1}-\frac{1}{2k+1})$ after that I get this $\sum_{k=0}^{n} (-1)^k \frac{k}{2k-1}+(-1)^{n+1}\frac{n+1}{2n+1}+\sum_{k=0}^{n}\frac{1}{2} \frac{(-1)^k}{2k+1} $ . But that does not help me so much I do not know how to continue, i try everything after I still get the same, can you help me? $\sum_{}^{}$ $\sum_{}^{1}$","['summation', 'discrete-mathematics']"
3013354,"Intuitively, what does being a UFD have to do with line bundles/first homology","In this question I asked for geometric clarification of the fact $\mathbb R[x,y,z]/ \left\langle x^2+y^2+z^2 -1 \right\rangle$ is a UFD in contrast to $\mathbb R[x,y]/ \left\langle x^2+y^2 -1 \right\rangle$ . The answer pointed to connections to the class group, and then the Picard group and therefore line bundles. That's all great, but I would like naive geometric intuition as to why non-unique factorizations hint at non-trivial line bundles/first homology. First instance the circle has two distinct factorizations $$y\cdot y=y^2\equiv_{\mathbb S^1}1-x^2=(1+x)(1-x)$$ but I don't understand what this non-uniqueness is saying geometrically. What's the picture here?","['algebraic-geometry', 'unique-factorization-domains', 'commutative-algebra']"
3013385,Sections of a line bundle can be extended if the complement is of codimension 2?,"Let $X$ be a smooth projective variery over $\mathbb C$ , and $U$ be an open subset of $X$ such that the complement of $U$ has (complex) codimension $2$ . Let $L$ be a line bundle on $X$ . Is the following statement true? The natural map $$H^0(X,L)\to H^0(U,L|_U)$$ is an isomorphsim. It is easy to see the injectivity, as varieties are irreducible, hence $U$ is dominant. For surjectivity I am not sure if it's true. But if it is, I think it should be something like Hartogs' theorem. My attampt 1: Any section $s\in H^0(U,L|_U)$ can be viewed as a rational function on $U$ , hence is automatically a rational function $\tilde s$ on $X$ . So remains to check the conditions on valuations are the same, that is, for any irreducible divisor $Z$ on $X$ , $ord_Z(\tilde s)$ in $X$ should be consistent with $ord_{Z\cap U}(s)$ in $U$ . But I have no idea how to do it then. My attampt 2: We can find a set of open balls $B_i$ which covers $X$ . Then for every section $s\in  H^0(U,L|_U)$ , the restriction $s_i=s|_{B_i\cap U}$ can be viewed as a holomorphic function, by Hartogs' extension theorem we can extend it to $\tilde s_i$ on $B_i$ , and then glue to a section on $X$ . It seems this is done?","['complex-geometry', 'algebraic-geometry', 'line-bundles']"
3013407,"How to show that the natural logarithm is Lipschitz on $[\beta, \infty)$","I want to show the following result: Let $\ln(x)$ have domain $D = [\beta, \infty)$ then $|\ln(x) - \ln(y)|
 \leq \dfrac{1}{\beta} |x-y|, \forall x,y \in D$ I am confused as to how to prove this seemingly simple looking claim. Can someone please help?","['calculus', 'logarithms', 'lipschitz-functions', 'real-analysis']"
3013438,Link between polynomial and derivative of polynomial,"I can't seem to solve this problem, can anyone help me please? The problem is: Let real numbers $a$ , $b$ and $c$ , with $a ≤ b ≤ c$ be the 3 roots of the polynomial $p(x)=x^3 + qx^2 + rx + s$ . Show that if we divide the interval $[b, c]$ into six equal parts, then one of the root of $p'(x)$ (the derivative of the polynomial $p(x)$ ) will be in the 4th part. What I did was: Because we know the roots of $p(x)$ are $a$ , $b$ and $c$ , we can write the polynomial $p(x)$ like this: $p(x) = (x-a)(x-b)(x-c)$ So we have $p(x) = x^3 + qx^2 + rx + s = (x-a)(x-b)(x-c)$ We find the value of $q$ , $r$ and $s$ : $q = -(a+b+c)$ $r = (ab + ac + bc)$ $s = -abc$ We have that $p'(x) = 3x^2 + 2qx + r$ We want to find the roots of $p'(x)$ , so if we apply the quadratic formula, we get: $(-2q ± 2*\sqrt{q^2 - 3r})/6$ Because we know the value of q,r and s, we can rewrite this expression like this: $(2(a+b+c) ± 2\sqrt{a^2 + b^2 + c^2 - ab - bc -ca})/6$ But I am stuck here, any help would be great. Thank you in advance.","['real-numbers', 'roots', 'functions', 'polynomials', 'derivatives']"
3013477,"Can there be a non-isolated ""pole"" or ""removable singularity""?",A pole or removable or even essential singularity must be isolated a priori. But still we can try to talk about the limit of the function at the point even on a disk removing some (countable amount of) points. A well-know example of non-isolated singularity will be $z=0$ for $$f(z)=\frac{1}{\sin(\frac{1}{z})}.$$ But $\lim_{z\to0}f(z)$ does not exist. My question is can there be a function with non-isolated singularity at $0$ with $\lim_{z\to0}|f(z)|=\infty$ or even $\lim_{z\to0}f(z)=c$ ?,['complex-analysis']
3013500,Working with the case $\Omega$ is countable and $\Omega$ is uncountable for a singleton set generator $\mathcal{E}$,"Let $\Omega \neq \varnothing$ . Let $\mathcal{E}:=\{ \{\omega\} : \omega \in \Omega\}$ Show: i) If $\Omega$ is countable, then $\sigma(\mathcal{E})=2^{\Omega}$ . ii) If $\Omega$ is uncountable, then: if $A \in \sigma(\mathcal{E}) \iff  A$ or $A^{C}$ is countable My ideas: i) I think (i) is rather trivial. For $\sigma(\mathcal{E})\subseteq 2^{\Omega}$ . Let $A \in \sigma(\mathcal{E})$ . Then $A$ can be written as a countable union of singletons which are by definition $\subseteq \Omega$ and therefore $\in 2^{\Omega}$ . $2^{\Omega} \subseteq \sigma(\mathcal{E})$ is trivial. ii) I do not really know where to begin with an uncountable $\Omega$ . Any tips?","['measure-theory', 'probability', 'real-analysis']"
3013501,"A continuous function on $[0,1]$ that is not of a bounded veriation","Suppose $f$ is continuous on [0, 1]. Must there be a nondegenerate closed subinterval $[a, b]$ of [0, 1] for which the restriction of $f$ to $[a, b]$ is of bounded variation? $\mathbf{My\ attempt}:$ Suppose for all $[a,b]\subseteq [0,1]$ with $a\neq b$ , we have $f$ restricted to $[a,b]$ is not of a bounded variation, $i.e$ for all $n \in \mathbb{Z^+}$ there exists a partition $\mathcal{P}_{{n}}$ such that $V(f,\mathcal{P_n})\geq n$ . I am not sure if there would be some examples shows that is not true, I am using a contradiction here to conclude that there must be a nondegenerate interval of $[0,1$ such that $f$ restricted to this interval is of a bounded variation. I will appreciate it any help with that. Thank you.","['measure-theory', 'lebesgue-measure']"
3013525,Gauss-bonnet just for geodesic triangles,"I have a question about some ways for proving the Gauss-Bonnet theorem just for small geodesic triangles. The general formula for the Gauss-Bonnet theorem is $$\iint_R KdS+\sum_{i=0}^k\int_{s_i}^{s_{i+1}} k_gds+\sum_{i=0}^k\theta_i=2\pi.$$ The ingredients here are a small portion $R$ of a surface $S$ , its boundary constituted by $k$ arcs (not necessarily geodesic arcs) and the ''exterior'' angles $\theta_i$ measured counterclockwise at the corner of the mentioned boundary, $K$ is the gaussian curvature of the surface and $k_g$ is the geodesic curvature of the portions of the boundary of $R$ . Of course, if we consider the region $R$ to be a small geodesic triangle $T$ , that is, it is contained in some small normal neighborhood of the surface and its boundary is the union of three small geodesic segments, we have the following nice version of Gauss-Bonnet formula $$\iint_T KdS=2\pi-\theta_1-\theta_2-\theta_3,$$ where the $\theta_i$ are just the external angles at the corners of the geodesic boundary triangle of $T$ . My question is, then: is there an elementary way to obtaining the ""geodesic triangle version"" above without using the ""complete version"" (for example, using Stoke's theorem)? Any reference will be of great help! Thanks in advance for all the community!","['geodesic', 'curvature', 'reference-request', 'stokes-theorem', 'differential-geometry']"
3013529,Proving that $f(x)=0$ in all points of continuity if $f$ is orthogonal to all polynomials,"Suppose that the function $f$ is: 1) Riemann integrable (not necessarily continuous) function on $\big[a,b \big]$ ; 2) $\forall n \geq 0$ $\int_{a}^{b}{f(x) x^n} = 0$ (in particular, it means that the function is orthogonal to all polynomials). Prove that $f(x) = 0$ in all points of continuity $f$ .","['riemann-integration', 'functional-analysis', 'real-analysis']"
3013543,isomorphism between subset of SU(2) and SO(3),I know that there is a  surjective map $\Phi : SU(2)\to SO(3)  $ . My question is if there is a subgroup $A \subset SU(2)$ such that $\Phi_{|A}:A\to SO(3)$ can be a (group) isomorphism. What would $A$ be? Thank you,"['group-theory', 'quaternions', 'lie-groups']"
3013566,Buffon's needle: expected number of intersections & pmf when $l > d$,"Earlier results have shown that when $l < d$ , the expected number of crossings of a needle of length $l$ with vertical lines spaced $d$ apart is $\frac{2l}{\pi d}$ , which is also the expression for the probability that a needle intersects a line. I'm looking for an intuitive explanation for why that is the case (is that even the case...?) when the needle is longer ie. $l > d$ (consider $l = 3, d = 1$ for example). This does not match the expression for the probability that a needle intersects a line when $l > d$ ; rather, it matches the expression for the probability that a needle intersects a line when $l < d$ . Is this just because the possible numbers of crossings are no longer restricted to $0$ and $1$ (ie. the $0$ term cancels out when computing the expected value)? And, how would one find the PMF of the number of crossings when $l > d$ (for a simpler case such as $l = 3, d = 1$ )? The possible values for the numbers of crossings are $0, 1, 2, 3, 4$ if I'm not mistaken. But I don't know where to go from there. edit: still looking for the PMF!","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
3013575,Equivalence of limit supremum of sequence of sets and sequence of functions,"Let $X_1,X_2,\dots$ be a sequence of real-valued random variables where $X_n:\Omega \to \mathbb{R}$ . For any sequence of events $A_n \subset \Omega$ define $\limsup_nA_n := \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_m$ . Moreover, define the (extended-real) random variable $Y \equiv (\limsup_nX_n)$ by $Y(\omega) := \limsup_n\{X_n(\omega) \mid n=1,2,\dots\}$ . How would you establish that $$
\limsup_{n}\{\omega \mid X_n(\omega) \in B\} = \{\omega \mid (\limsup_nX_n)(\omega)\ \in B \}, \qquad (*)
$$ for any Borel set $B$ ? Is the proposition $(*)$ even true?","['limsup-and-liminf', 'measure-theory', 'probability-theory', 'probability']"
3013609,Proving Limit Rigorously,"Find the limit $$\large \lim_{x\to \infty}(\ln x)^{\frac{20}x}$$ I understood that as x approached infinity, $20/x$ approached 0. This would mean that the limit would tend toward $1$ . However, $\ln x$ also approaches infinity as $x$ approaches infinity. Thus, I suspected the answer to be $1$ (and it indeed is the answer), however I feel like this answer is not sufficiently rigorous. How could I rigorously prove $1$ as the answer? Any ideas/hints would be appreciated. Note: I am a highschooler and my teachers often tell me to take these answers in faith. Thus, I may not understand any fancy notations that may usually be used in solving limit. Thanks!","['limits', 'calculus']"
3013628,Making sure if it is Cauchy,"In my real analysis exam I had a problem in which I proved that given any positive number $a\lt 1$ if $|x_{n+1} - x_n|\lt {a^n}$ for all natural numbers $n$ then $(x_n)$ is a Cauchy sequence. This was solved successfully but the question is if $|x_{n+1} - x_n|\lt \frac 1n$ does that mean $(x_n)$ is Cauchy? Well my answer was yes because I could write this in the form of the first one, but now I am somehow confused with what I have answered since $1/n$ is a sequence of $n$ so maybe the answer is not necessarily true... Can you please provide me with the correct answer for this question?","['cauchy-sequences', 'real-analysis']"
3013664,Another Limit Conundrum,"For what values of $a$ and $b$ is the following limit true? $$\lim_{x\to0}\left(\frac{\tan2x}{x^3}+\frac{a}{x^2}+\frac{\sin bx}{x}\right)=0$$ This question is really confusing me. I know that $\tan(2x)/x^3$ approaches infinity as $x$ goes to $0$ (L'Hôpital's). I also understand that $\sin(bx)/x$ goes to $b$ as $x$ approaches $0$ . However, I am not sure how to get rid of this infinity with the middle term. Any ideas? Thanks!","['limits', 'calculus']"
3013670,Is the sum of all rational numbers between two integers infinity,"If there are infinite numbers between two rational numbers then would that entail that the sum of all numbers, say between 1 and 2, be infinity? I believe that this cannot be true and has to do something with area under a curve?","['integration', 'calculus']"
3013672,What is log-utility?,"I came across this problem today: Calculate the log-utility optimal fraction of your capital to bet on a fair coin flip where you win $x$ on heads and lose $y$ on tails. What is the meaning of log-utility in log-utility optimal fraction? Is this an portfolio management term or a statistics term? Apologies if this is in the wrong SE, please close this if it’s irrelevant!","['statistics', 'utility', 'probability']"
3013709,Prove that there exists a triangle which can be cut into 2005 congruent triangles.,"I thought maybe we can start with congruent triangle and try to cut it similar to how we create a Sierpinski's Triangle?  However, the number of smaller triangles we get is a power of $4$ so it does not work. Any ideas?","['contest-math', 'geometry']"
3013710,Cogeodesic flow on compact manifold has compact leaves and manifold,"Let $g_{ij}$ be positive definite metric on a compact manifold $M$ . Consider hamiltonian $H=\sum_{ij}\frac{1}{2}g_{ij}(x) p_ip_j$ with $p_i\in\Gamma(T^\star M)$ . Then define $E_\lambda=\{(x,p)\in T^\star M\vert H(x,p)=\lambda\}$ $\textbf{Q:}$ $E_\lambda$ is compact. How do I see this obviously? I did the following. Take any open covering of $E_\lambda$ and refine it if necessary s.t. one has trivialization of $T^\star M$ . Then project down to $M$ and then lift up to $T^\star M$ and this selects some of the open covering. Since $M$ is compact, $g_{ij}(x)$ has minimal value $g$ . Then use $\sum_{ij}g p_ip_j\leq\lambda$ to bound $p_i$ in closed set. Hence, $E_\lambda$ is compact by selecting the finite open covering. Ref: Riemannian Geometry and Geometric Analysis Jost. Chpt 2 Lie Groups and Vector Bunbldes Pg 70","['riemannian-geometry', 'geometry', 'ordinary-differential-equations', 'differential-geometry']"
3013712,How to calculate the line integral with respect to the circle in counterclockwise direction,"Consider the vector field $F=<y,-x>$ . Compute the line integral $$\int_CF\cdot dr$$ where $C$ is the circle of radius $3$ centered at the origin counterclockwise. My Try: The circle is $x^2+y^2=9$ $$\cases{x=3\cos t \\ y=3\sin t} \text{ for } 0\le t\le2\pi$$ Now how do I calculate $\int_CF\cdot dr$ ? Can anyone explain how to solve this?","['integration', 'multivariable-calculus', 'calculus', 'line-integrals']"
3013833,characteristic function and distribution completely determined by moments,"Let $X$ be a real valued random variable. My textbook states that if the moment generating function $E[e^{sX}]$ is finite in a neighborhood of zero, the distribution of $X$ is determined completely by the moments. However, I cannot find a similar statement with the characteristic function $E[e^{i t X}]$ . So I tried to deduce one. Let $X_1$ and $X_2$ have the same moments of all orders. Then their characteristic functions $\phi_{X_1}$ and $\phi_{X_2}$ are infinitely differentiable at $0$ and the derivatives have the same value at $0$ . $\phi_{X_1}$ and $\phi_{X_2}$ have the same Taylor series expansion at $0$ . If the radius of convergence of the series is infinite, $\phi_{X_1}=\phi_{X_2}$ . A characteristic function uniquely determines the distribution. So we conclude that $X_1$ and $X_2$ have the same distribution. If the radius of convergence is zero, we cannot say that $\phi_{X_1}=\phi_{X_2}$ nor the same distribution. If the radius of convergence is positive but finite, we cannot say that $\phi_{X_1}(s)=\phi_{X_2}(s)$ for $s$ beyond the radius of convergence. If we consider analytic extensions of $\phi_{X_1}$ and $\phi_{X_2}$ on the complex domain, they have singularities somewhere but it may be possible that $\phi_{X_1} = \phi_{X_2}$ for whole real line and the singularities could give helpful information on this. Q1: From the above argument (2), if a probability distribution has all moments and its characteristic function is analytic in the whole real line, the distribution is completely determined by the moments. Is this correct? Q2: Are there some useful theorems considering the analytic extension of a characteristic function on complex plane, related to the determination by moments?","['complex-analysis', 'characteristic-functions', 'probability-theory']"
3013834,"Let $f$ be integrable on $[a,b]$ and suppose for each integrable function $g$ defined on $[a,b]$, $\int^{b}_afg=0$, then $f(x)=0,\forall x\in[a,b]$","I do not think this is true, but at the same time I am not sure. I know that if we assume that f is continuous instead of integrable then this statement is true. I just do not know how to provide a counterexample if it is false to show that this is wrong. Integrable does not imply continuity I know that much.","['integration', 'continuity', 'real-analysis']"
3013837,Surface area of $x^2+z^2=a^2$ inside of $x^2+y^2 = 2ay$ and in first octant,"The questions is What is the surface area of $x^2+z^2=a^2$ inside of $x^2+y^2 = 2ay$ and in first octant? My attempt The second equation can be rewritten as $x^2 + (y-a)^2=a^2$ to make it easier to work with. After this I tried parametrizing the first cylinder with $x= \operatorname{acos}\theta$ , $z=\operatorname{asin}\theta$ and $y=y$ . This is where I get stuck. The area integral should be $A= \iint||T_{\theta} \times T_y||dS$ = $\iint a dyd\theta$ . I'm not sure how to place the bounds on $\theta$ and $y$ . My first guess was to let $0 \leq\theta \leq \pi/2$ , but I'm still not sure what to do with $y$ . Any help would be greatly appreciated.","['multivariable-calculus', 'surface-integrals']"
3014004,how to prove chi-square statistics conforms to chi-square distribution with contingency table?,"chi-square test(principle used in C4.5's CVP Pruning), also called chi-square statistics, also called chi-square goodness-of fit How to prove $\sum_{i=1}^{i=r}\sum_{j=1}^{j=c}\frac{(x_{ij}-E_{ij} )^2}{E_{ij}}   = \chi^2_{(r-1)(c-1)}$ where $E_{ij}=\frac{N_i·N_j}{N}$ , $N$ is the total counts of the whole datasets. $N_i$ are the counts of the sub-datasets of the same-value of feature $N_j$ are the counts of the sub-datasets of the same-class please help,thanks～！ here is contingency table /------------------------------------------------ here are some references which are not clear: https://arxiv.org/pdf/1808.09171.pdf (not mention why $k-1$ is used in formula(5)) https://www.math.utah.edu/~davar/ps-pdf-files/Chisquared.pdf (Not mention why $\Theta<1$ from (9)->(10)) https://arxiv.org/pdf/1808.09171 (page 4th not mention what is X*with a line on it) http://personal.psu.edu/drh20/asymp/fall2006/lectures/ANGELchpt07.pdf (Page 109th,Not mention why $Cov(X_{ij},X_{il}=-p_ip_l)$ )","['statistics', 'normal-distribution', 'chi-squared', 'probability-theory', 'probability']"
3014022,A faster solution to this equation: $e^{-ix}(1+e^{-ix}) = -1$,"I want to find $x$ such that $$e^{-ix}(1+e^{-ix}) = -1$$ I was able to see that it was the same as $\cos x = -0.5$ with a bit of work, but I think there must be a faster way since usually questions from GATE exam have elegant solutions. Let me give some context, I needed to solve this equation in order to find the frequencies rejected by a digital system. Question 10.2 (GATE IN 2003 digital systems rejection of frequencies cause gain is zero) The following is my attempt So I wanted an alternative way to solve it, it would be nice if it was faster by hand and also any fast way to find the general form for x once we it's a set of angles?","['alternative-proof', 'trigonometry', 'polynomials', 'algebra-precalculus', 'complex-numbers']"
3014033,Notation for the derivative of a function: $f'$ or $f'(x)\;$?,"The derivative of a function is often defined as $f'$ and $f'(x)$ . So which one is it? $f'(x)$ is the output of the function $f'$ , so why do I see people using $f'$ and $f'(x)$ interchangeably to refer to the derivative of a function?","['notation', 'calculus', 'derivatives']"
3014034,"Determine all functions $f : \mathbb{N} \rightarrow \mathbb{N}$ such that, for every positive integer $n$, we have: $2n+2001≤f(f(n))+f(n)≤2n+2002$.","Determine all functions $f : \mathbb{N} \rightarrow \mathbb{N}$ such that, for every positive integer $n$ , we have: $$2n+2001≤f(f(n))+f(n)≤2n+2002\,.$$ I don't know where to start as in is there a function that I can get to the solution by slightly modifying it? Any ideas","['contest-math', 'functional-equations', 'functional-inequalities', 'discrete-mathematics', 'problem-solving']"
3014039,Finding the probability of two random variables being equal to 1,"Question: A die is thrown $n+2$ times. After each throw a ' $+$ ' is recorded for $4$ , $5$ , or $6$ and ' $-$ ' for $1$ , $2$ , or $3$ , the signs forming an ordered sequence. To each, except the first and the last sign, is attached a characteristic random variable which takes the value $1$ if both the neighboring signs differ from the one between them and $0$ otherwise. If $X_1, X_2, \ldots , X_n$ are the characteristic random variables, find the mean and variance of $X = \sum_{i=1}^n X_i$ Problematic part: $$ V(X) = V(X_1+X_2+\cdots+X_n) = \sum_{i=1}^n V(X_i) +2\sum_{i=1}^n\sum_{j=1, j>i}^n Cov(X_i,X_j) $$ Calculating the variance of $X$ requires calculating the covariance of two arbitrarily chosen random variables $X_i$ and $X_j$ . The formula then used is $Cov(X_i,X_j) = E(X_iX_j) - E(X_i)E(X_j)$ Which brings us to the essence of my problem -- How to find $E(X_iX_j)$ ? It is certain that $X_iX_j$ can take only two values, namely, $0$ and $1$ . Therefore $E(X_iX_j) = 1.P(X_iX_j=1) + 0.P(X_iX_j=0)= P(X_iX_j=1)$ But at this point I'm not sure how to compute the probability. The book I'm using says the probability is $1/8$ , but I can't seem to wrap my head around the reasoning. An intuitive explanation would be highly appreciated!","['statistics', 'covariance', 'expected-value', 'probability', 'random-variables']"
3014089,Equivalent Definition of Strong Convexity,"Let $f \in \mathcal{C}^2(\mathbb{R}^n).$ Recall that we defined $f$ to be strongly convex if there exists $\beta > 0$ such that $\langle D^{2}f|_{x}y,y\rangle\ge\beta$ for every $x, y \in \mathbb{R}^{n}$ or equivalently if the function $g(x)=f(x)-\frac{\beta}{2}\|x\|^{2}$ is convex. Show that if there exists $\gamma>0$ such that $$f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)-\gamma t(1-t)\|x-y\|^{2}\tag{*}$$ for all $x,y \in \mathbb{R}^{n}, t \in [0,1]$ then the function $g(x)=f(x)-\gamma\|x\|^2$ is convex. I tried showing that the function $g(x)=f(x)-\gamma \|x\|^2$ is convex using the above assumption but did not strike any luck. Any hints on how to proceed for this problem are appreciated. Than you for you help.","['convex-optimization', 'multivariable-calculus', 'convex-analysis', 'real-analysis']"
3014113,Prove or disprove that if $\lim\limits_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq c$ then $\lim\limits_{x\to0^+}xf'(x)=0$,"A function $f$ defined on interval $(0,1)$ with a continuous twice derivation $(f\in{C^2(0,1)})$ satisfies $\lim_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq{C}$ where $C$ is a fixed positive real number. Prove $\lim_{x\to0^+}xf'(x)=0$ (or disprove it!) I've tried several ways like calculating $yf'(y)-xf'(x)=f(y)-f(x)+\int_{x}^{y}tf''(t)\,dt$ to prove the limitation exists (and failed). I also think it is similer to L'Hospital rule $\lim_{x\to0^+}\frac{f'(x)}{\frac{1}{x}}=\lim_{x\to0^+}\frac{f''(x)}{-\frac{1}{x^2}}$ ,but it's only a sufficient condition and probably wrong.","['limits', 'derivatives', 'real-analysis']"
3014227,prove existence of the limit of a sequence,"So I have the following problem: $ x_0 = 1 , x_1 = 2 , $ and $x_{n+1} = 2x_n + x_{n-1} $ for $ n \geq 1.$ Show that: $\hspace{2mm} \lim_{n\to \infty} \frac{x_n}{x_{n+1}}   $ exists. Then show that the Limit is equal to $\sqrt{2}-1$ . For this I thought i could use the fact that $x_n$ is bounded and I thought that it was monotonically falling, but that is not the case, so I ran out of ideas. And I don't know how to calculate the limit... Thank you very much for your help!","['limits', 'sequences-and-series']"
3014284,How to show $2^{ℵ_0} \leq \mathfrak c$ [duplicate],This question already has answers here : Easiest way to prove that $2^{\aleph_0} = c$ (3 answers) Closed 5 years ago . I want to show $2^{ℵ_0}=\mathfrak c$ . I already showed $\mathfrak c \leq 2^{ℵ_0}$ as follows: Each real number is constructed from an integer part and a decimal fraction. The decimal fraction is countable and has $ℵ_0$ digits. So we have $\mathfrak c \leq ℵ_0 * 10^{ℵ_0} \leq 2^{ℵ_0} * (2^4)^{ℵ_0} = 2^{ℵ_0}$ since $ℵ_0 + 4ℵ_0=ℵ_0$ But how can I prove the other way $2^{ℵ_0} \leq \mathfrak c$ ?,"['elementary-set-theory', 'cardinals']"
3014296,Double integral with Hankel transform,"Let's say we have a double integral in the following form: $$I=\int_0^\infty \int_0^\infty f(x) g(y) J_0(xy) x y dx dy $$ Using the definition of the Hankel transform, we can write: $$I=\int_0^\infty  F(y) g(y)  y dy=\int_0^\infty  G(x) f(x)  x dx$$ Or, using the self-inverse property of the transform: $$I=\int_0^\infty \int_0^\infty F(x) G(y) J_0(xy) x y dx dy $$ I wasn't able to find my functions of interest in the tables of direct or inverse Hankel transforms. Which is why I decided to ask a general question: Is there a way to simplify such an integral if we don't know the exact Hankel transform for either function? I know this is a long shot, but maybe there are some identities I don't know which could help. Just in case it's important, my functions are: $$f(x)=\frac{e^{-ax}}{x^2+b^2} \\ g(y)=e^{-y} L_l \left( \frac{2L+1}{l+L+1} y \right) L_L \left( \frac{2l+1}{l+L+1} y \right)$$ Where $L_l$ are Laguerre polynomials. So this could also be related to Laplace transform. Though I have more hopes for Hankel, as one of the integrals is already a result of Laplace transform and going back would be counter-intuitive. But I'm interested in the general case too. Probably wouldn't help, but there's an interesting connection between Laguerre polynomials and Bessel functions, in particular: $$L_n(z)= \frac{2}{n!} e^z z^{n+1} \int_0^\infty e^{-z t^2} t^{2n+1} J_0(2zt) dt$$ More useful integrals I found in G-R: $$\int_0^\infty \frac{x \sin (ax)}{x^2+b^2} J_0(y x) dx=\frac{\pi}{2} e^{-ab} I_0 (b y), \qquad y \leq a$$ $$\int_0^\infty \frac{x \cos (ax)}{x^2+b^2} J_0(y x) dx= \cosh (a b) K_0(b y), \qquad y \geq a $$ Heck if I know how to convert these results to the exponential form, as they work for different ranges of $y$ .","['integration', 'integral-transforms', 'bessel-functions']"
3014317,Functional equation for distribution function,"I have next functional equation for some distribution: $$\overline F_\xi^2(T) = \overline F_\xi(2T) \forall T>0$$ If suggest, that it differentiable, we can do something like this: $$2\overline F_\xi(T) \cdot \overline F'_\xi(T) = 2\overline F'_\xi(2T)$$ $$\overline F_\xi(T) \cdot \overline F'_\xi(T) = \overline F'_\xi(2T)$$ But how to solve this differential functional equation?
where $$\overline F_\xi = 1 - F_\xi = P(\xi > T)$$ Also, I have $\mathbb E\xi=1.$","['probability-distributions', 'probability-theory', 'ordinary-differential-equations']"
3014349,$\operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB)$ whenever $AB=BA$?,"Let $A,B$ be $n\times n$ matrices. If $AB=BA$ , then $\operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB)$ . Is this rank inequality correct? No counterexample seems to exist. Here's what I've done. When $A$ is a Jordan block this is easily proved. I tried to bring $A$ into Jordan form for $n=2,3,4$ and found no counterexample. So currently I think it is true. By Fitting's lemma it suffices to consider the case in which $A$ is nilpotent. We can bring $A$ into Jordan form. Then the blocks in $B$ are upper triangular. However, even in this case $\operatorname{rank}(B^2)$ is not tractable. Any hints will be appreciated!","['matrices', 'inequality', 'linear-algebra', 'matrix-rank']"
3014356,Continuous spectrum,"I understand that the definition of spectrum of an operator(where operator means to be in functional analysis).
The point spectrum(eigenvlue) is used for solving some partial equations(e.g. heat equation, wave equation). But I don't know what the continuous spectrum means.
Why do we need to consider the continuous spectrum? I'd appreciate it if you could answer this question.","['spectral-theory', 'functional-analysis']"
3014357,Reversal of an Autoregressive Cauchy Markov Chain,"Let $\mu_0 (dx)$ be the standard one-dimensional Cauchy distribution, i.e. \begin{align}
\mu_0 (dx) = \frac{1}{\pi} \frac{1}{1+x^2} dx.
\end{align} Suppose I fix $h \in [0, 1]$ , and form a Markov chain $\{X_n\}_{n \geqslant 0}$ as follows: At step $n$ , I sample $Y_n \sim \mu_0$ . I then set $X_{n+1} = (1 - h) X_n + h Y_n$ It is not so hard to show that this chain admits $\mu_0$ as a stationary measure, as this essentially comes from the fact that Cauchy distribution is a stable distribution. What I'm interested in is the reversal of this Markov chain. More precisely, if the chain I describe above uses the Markov kernel $q (x \to dy)$ , I want to understand the Markov kernel $r$ such that \begin{align}
\mu_0 (dx) q (x \to dy) = \mu_0 (dy) r (y \to dx).
\end{align} Fortunately, all of the quantities involved have densities with respect to Lebesgue measure, and as such, I can write down what $r (y \to dx)$ is: \begin{align}
r (y \to dx) = \frac{1 + y^2}{\pi} \frac{1}{1 + x^2} \frac{h}{ h^2 + (y - (1 - h) x)^2} dx.
\end{align} My question is then: is there a simple, elegant way to draw exact samples from $r$ ? I would highlight that this is not a purely algorithmic question; I'd really like to understand what this reversal kernel $r$ is doing. A nice byproduct of that would then be that I could simulate from it easily. For completeness, some of the `purely algorithmic' solutions I had considered were the following. I could try rejection sampling, and in principle this would work, but it wouldn't really give me insight into the nature of the Markov chain. I could try something like the inverse CDF method, but it seems to me that the CDF of $r$ is not particularly nice to work with. As such, I'd have to use e.g. Newton iterations to use this method, and I'd prefer to not have to do this.","['markov-chains', 'probability']"
3014381,Origin of Taylor Series,"Historically, the Taylor series representations or truncated Taylor series approximations of a function at a point $x_0$ was first done by taking Newton's form of an interpolation polynomial for points of the form $x_0 + n \Delta$ , where $\Delta$ is a positive real number and $n$ is a natural number, and then taking the limit as $\Delta$ goes to $0$ . Could someone explain in detail how this was done? Let $f$ be a function on an interval of real numbers. Let $\Delta$ be a real number and let $x_0$ be in the domain of $f$ such that $x_0$ , $x_0 + \Delta$ , $\dots$ , $x_0 + n\Delta$ is in the domain of $f$ , where $n$ is a positive integer. Newton's form of the interpolation polynomial of the data $\{ (x_0 + k \Delta, f(x_0 + k\Delta) \}$ is $$f(x_0) + \frac{f(x_0 + \Delta) - g_0(x_0+\Delta)}{\Delta}(x-x_0) + \cdots + \frac{f(x_0 + n\Delta ) - g_{n-1}(x_0 + n\Delta)}{n!\Delta}(x-x_0)\cdots (x-(n-1)\Delta),
$$ where $g_k$ is the interpolation polynomial for the first $k+1$ points. Taking the limit as $\Delta$ tends towards $0$ gives $$ 
f(x_0) + f'(x_0)\cdot x + \cdots + \frac{1}{n!}\cdot (\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n})\cdot x^n,
$$ as long as $f$ is sufficently smooth and the limit $\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n}$ exits. But why does the limit $\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n}$ equal $f^{(n)}(x_0)$ ?","['analysis', 'real-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series']"
3014406,What does the Jacobian matrix of the projection mapping for Normal bundle look like? (2.3.14 G&P),I want to solve this question: I feel like the previous question is similar to the one given in this link: Natural projection of tangent bundle is submersion Am I correct? but what does the Jacobian matrix look like in our situation here? Thanks.,"['transversality', 'general-topology', 'differential-topology', 'differential-geometry']"
3014423,Factorial Proof - ${n \choose r-1}+{n \choose r}={n+1 \choose r}$,${n \choose r-1}+{n \choose r}={n+1 \choose r}.$ So what I tried to do was expand the first and second term. $\frac{n!}{(r-1)!(n+1-r)!}+\frac{n!}{(r)!(n-r)!}.$ Then what I did was try to get common denominators. $\frac{n!}{(r-1)!(n+1-r)(n-r)!}+\frac{n!}{(r)(r-1)!(n-r)!}.$ Then I attempted to combine and get the common denominator. $\frac{(n!)(r)+(n!)(n+1-r)}{(r-1)!(n+1-r)(n-r)!(r)}.$ From here I simplified a bit more but didn't get a nice answer. Some insight would be helpful.,"['algebra-precalculus', 'binomial-coefficients', 'factorial']"
3014424,Proving that $\|T\|=\max\{\sqrt{\lambda};\;\lambda\in \sigma(T^*T)\}$.,"Let $\mathcal{B}(F)$ the algebra of all bounded linear operators on an infinite-dimensional complex Hilbert space $F$ . It is well known that if $T\in \mathcal{B}(F)$ , then $$\|T\|=\displaystyle\sup_{\|x\|=1}\|Tx\|.$$ I want to prove that for $T\in \mathcal{B}(F)$ , we have $$\|T\|=\max\{\sqrt{\lambda};\;\lambda\in \sigma(T^*T)=\sigma(TT^*)\},$$ where $\sigma(A)$ denotes the spectrum of an operator $A$ .","['operator-theory', 'functional-analysis', 'reference-request']"
3014429,All nilpotent $2 \times 2$ matrices satisfy $A^{2}=0$,I have problems to show that if $A$ is a $2 \times 2$ matrix and if there exists some positive integer such that $A^{n}=0$ then $A^{2}=0$ . I only showed that $A$ is a singular matrix but nothing else. Thanks any help will be appreciated.,"['matrices', 'nilpotence', 'linear-algebra']"
3014448,Correct notation for writing array reshape,"Suppose I am coding in python. Then one can do e.g. something like this: import numpy as np
A = np.random.rand((6,6))
# Lets reshape it
A_new = A.reshape(-1,3) So it went from 2D array (matrix) to a 2D array with six 12 rows and three columns. How would one write the re-shape operation formally in mathematical notation? Transpose is obviously easy, but is there a parallel for reshapes? $$\mathbf{A} \in \mathbb{R}^{6\times6} \rightarrow \mathbf{A} \in \mathbb{R}^{12 \times 3}$$ Thx","['matrices', 'matrix-equations', 'notation', 'vectors']"
3014450,If $F(x)=\frac{x^4-3}{x^4+1}$ is a primitive of $f(x)$ find $\int_{0}^{1} xf(x) dx$,Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function. If $F(x)=\frac{x^4-3}{x^4+1}$ is a primitive of $f(x)$ find $\int_{0}^{1} xf(x) dx$ I literally have no idea how to integrate this. I tried integrating by parts (and finding the derivative of $F(x)$ ) but I end up getting a even worse integral... The correct answer apparently is $-3$ .,"['integration', 'definite-integrals', 'real-analysis', 'calculus', 'derivatives']"
3014490,Asymptotic behaviour of recurrence,"In DNA chain, there are four types of bases: A, C, G, T. Let $g(n)$ be the number of configurations of a DNA chain of length $n$ , in which the sequences TT and TG never appear. Write a recurrence for $g(n)$ . Determine the asymptotic behavior of the recurrence. Also, prove whether this rate of growth is $o(n^{ n^{1/2}}), \Theta(n^{ n^{1/2}}),$ or $\Omega(n^{ n^{1/2}})$ . My answer: $g(n)=A(n)+C(n)+G(n)+T(n)$ $g(n)=g(n-1)+g(n-1)+g(n-1)+A(n-1)+C(n-1)$ $g(n)=3g(n-1)+2g(n-2)$ After solving this recurrence, we get $$\begin{align}
g(n) &= (\frac{(4)(17)^{1/2}+16}{(3)(17)^{1/2}+17})(\frac{3+(17)^{1/2})}{2})^n \\
&+(\frac{(17)^{1/2}-5)}{2(17)^{1/2}})(\frac{3-(17)^{1/2}}{2})^n
\end{align}$$ Therefore when $n$ is large, $(\frac{3-(17)^{1/2})}{2})^n$ become smaller and smaller
So we can write $$g(n) = \Theta((\frac{3+(17)^{1/2})}{2})^n).$$ (Do I determine the asymptotic behavior of the recurrence correctly?) But I do not know what is the rate of growth, if we take the limit of $\frac{g(n)}{n^{ n^{1/2}}}$ ,where n tend to infinity, it seems impossible to calculate it. How could I determine the rate of growth?","['biology', 'asymptotics', 'discrete-mathematics']"
3014506,Drawing random subspaces from Grassmannian with uniform probability,"Consider the Grassmannian manifold $G(M, N)$ of $M$ -dimensional subspaces in $R^N$ . I want to approximate (stochastically) an integral of the form $$
\int_{G(M, N)} f(v) \, dv,
$$ where $f : G(M, N) \to R$ is some function and $dv$ is the Haar measure on the Grassmannian. I want to approximate the integral with sampling, and therefore I need a method to uniformly draw samples with respect to the measure dv. I'm happy about hints / references on how to do that.","['measure-theory', 'geometric-measure-theory', 'grassmannian', 'haar-measure', 'probability-theory']"
3014696,Confusion about implicit differentiation $\frac{dy}{dx}$,"Today I learnt about implicit differentiation using this: $\frac{d}{dx}(f)$ = $\frac{df}{dy} \times \frac{dy}{dx}$ I don't understand when doing implicit differentiation how the d/dy part works for y terms: $\frac{d(y^2)}{dx} = \frac{d(y^2)}{dy}\times\frac{dy}{dx}=2y\frac{dy}{dx}$ Why is the derivative of $y^2$ with respect to $y$ , $2y$ ? Do you assume it's a function of something else?
And similarly if you apply this same formula to an $x$ term (even though its not needed) you get: $\frac{d(x^2)}{dx} =\frac{d(x^2)}{dy}\times\frac{dy}{dx}$ How does that simplify to the $2x$ I know it is? Thanks","['implicit-differentiation', 'ordinary-differential-equations']"
3014713,Finding derivative of $f(x)$ where $f(xy) = f(x) + f(y)$ - without change of variable,"Let $f(x)$ be a function $(0,\infty) \to R$ and for every $x,y$ in the domain we have: $$f(xy) = f(x) + f(y)$$ It is like logarithm but we don't know the exact form of the function. we know it is differentiable at x=1. Now we want to show it is differentiable at its domain and its derivative is $f'(x) = \frac{1}{x} f'(1)$ . Solution: I can find that $f(1) = 0$ and $f(x/y) = f(x) - f(y)$ So we have: $f'(1) = \lim_{h\to 0} \frac{f(1+h) - f(1)}{h} = \lim_{h\to 0}\frac{f(1+h)}{h}  $ so $f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h\to 0} \frac{f(\frac{x+h}{x})}{h}  = \lim_{h\to 0} \frac{f(1 + h/x)}{h}$ . I know I can solve it with a simple change of variables $h \to 0 ~~~\rightarrow~~~~ h/x \to 0$ , But I want to know is there any way to solve this without changing variable? Maybe using definition of limit?","['limits', 'change-of-variable', 'derivatives', 'logarithms']"
3014729,"How many different pairs of integers $(x,y)$ modulo $p$ such that $ax^2+by^2+c \equiv 0\pmod{p}$?","Let $p$ be an odd prime number and $a,b,c$ be integers coprime with $p$ . How many different pairs of integers $(x,y)$ modulo $p$ such that $ax^2+by^2+c \equiv 0\pmod{p}$ ? Until now I haven't had any specific way to approach this problem. How can I find the number of solutions of the equation $ax^2+by^2+c \equiv 0\pmod{p}$ ? Or can it only be counted with some given conditions of $a,b,c$ ? (Please let me know if I should add some details to this problem)","['number-theory', 'elementary-number-theory', 'calculus', 'combinatorics', 'discrete-mathematics']"
3014736,Is there any known closed-form solutions to $y''yx+y'y-(y')^2x=x^3$?,The title says it all... The equation $y''yx+y'y+(y')^2x=x^3$ belongs to a known DE class (listed on EqWorld). The equation $y''yx+y'y-(y')^2x=x^3$ appears to be way more difficult to solve... Is the analytic solution to this equation known at all?,"['closed-form', 'ordinary-differential-equations']"
3014759,Constructing an infinite chain of subfields of 'hyper' algebraic numbers?,"This has now been cross posted to MO. Let $F$ be a subset of $\mathbb{R}$ and let $S_F$ denote the set of values which satisfy some generalized polynomial whose exponents and coefficients are drawn from $F$ . 
That is, we let $S_F$ denote $$\bigg \{x \in \mathbb{R}: 0=\sum_{i=1}^n{a_i x^{e_i}}: e_i \in F \text{ distinct}, a_i\in F \text{ non-zero}, n\in \mathbb{N}  \bigg  \}$$ Then $S_{\mathbb{\mathbb{Q}}}$ is the set of algebraic real numbers and we start to see the beginnings of a chain: $
\mathbb{Q}
\subsetneq S_\mathbb{Q} \subsetneq
S_{S_\mathbb{Q}} $ Main Question Does this chain continue forever? That is, we let $A_0= \mathbb{Q}$ and let $A_{n+1}=S_{A_{n}}$ . Is it the case that $A_n \subsetneq A_{n+1}$ for all $n\in\mathbb{N}$ ? Other curiosities: Is $A_i$ always a field? Perhaps, the argument is analogous to this . Or maybe this is just the case in a more general setting: Is it the case that $F \subset \mathbb{R}$ , a field implies that $S_F$ is a field? Is it possible to see that $e\notin \cup A_i$ ? Perhaps this is just a tweaking of LW Theorem.","['field-theory', 'transcendental-numbers', 'real-analysis']"
3014772,An error in a proof due to variable creep?,"I'm working through some exam practice questions and I came across this one: Identify the error in the following “proof.” Let u, m, n be three integers. If u|mn and gcd(u, m) = 1, then m = ±1.
If gcd(u, m) = 1, then 1 = us + mt for some integers s, t. If u|mn, then
us = mn for some integer s. Hence, 1 = mn + mt = m(n + t), which
implies that m|1, and therefore m = ±1. Now I think the problem here is that we cannot go from the linear transformation statement 1 = us + mt and then state that u|mn => mn = us for some integer s, because we already have a value of s in the previous statement. so we would need to define u|mn => mn = uj for some integer j.
And now the substitution doesn't work and we cannot proceed. Does this make sense to you guys? It makes sense to me that we cannot use s in two different places here. But I'm having a hard time trying to explain why in a clear manner.",['discrete-mathematics']
3014880,"$L^1([0,1])$ closed unit ball is not weakly compact","So I would like to prove this result by constructing a sequence of functions $u_n$ in $L^1([0,1])$ , such that $\|u_n\|_{L^1}\leq 1$ for all $n$ , but this subsequence does not have a convergent subsequence. My idea was to take an approximation to the identity, say, $u_n = n\mathbf{1}_{[0,1/n]}$ . Taking any continuous function on $\phi$ on $[0,1]$ , ( $\phi$ is also $L^{\infty}$ as a result), defines a linear functional on $L^1$ : $$
f(u_n) = \int_0^1 u_n\phi \mathrm{d}\mu
$$ And taking the limit to infinity, we see that $f(u_n) = \phi(0)$ . Here is where my argument needs work: ""This means that $u_n$ converges weakly to the Delta function, but this is not an element of $L^1$ "". How can I clarify this? Do I have to use the sequence: $$
\int_0^1 u_n\phi \mathrm{d}\mu
$$ as a sequence in the dual of $C[0,1]$ and show then that $f_n(\phi) \xrightarrow{w*} \phi(0)$ ? Thanks for the help.","['measure-theory', 'real-analysis', 'lp-spaces', 'weak-topology', 'functional-analysis']"
3014929,"Prove that $f(x)=\log\sqrt{\frac{1+x}{1-x}}$ is surjective from $(-1,1)$ to $\mathbb{R}$.","I have to prove that the function $\;f:(-1,1)\to \mathbb{R}\;$ defined by $f(x)=\log\sqrt{\frac{1+x}{1-x}}\;$ is bijective. I have already proved that it is injective: $$f(x)=f(y)$$ $$\log\sqrt{\frac{1+x}{1-x}}=\log\sqrt{\frac{1+y}{1-y}}$$ $$\log\sqrt{\frac{(1+x)(1-y)}{(1-x)(1+y)}}=0$$ $$\frac{1+x-y-xy}{1-x+y-xy}=1$$ $$x=y$$ But now, how can I prove that the function is surjective?","['calculus', 'functions', 'functional-analysis']"
3014937,Find function $f$ given $f(x+1) - f(x) = \frac{1}{x^2}$,"I need to find the expression of function $f$ , all we know about $f$ is: $\begin{cases} \forall x>0, f(x+1)-f(x) = \frac{1}{x^2}
\\ f \text{ is continuous on } ]0, +\infty[ \text{ and } \lim\limits_{x \to +\infty} f(x) = 0 \end{cases}$ Any help would be appreciated.","['integration', 'limits', 'calculus', 'functional-equations']"
3014943,How to calculate the index number for a curve around a linear system's fixed point without integrals?,"We know $$ \phi = \tan^{-1} \frac{\dot{y}}{\dot{x}},$$ yet so far I've only been able to calculate the index of curves by using the integral $$
\frac{1}{2\pi} \oint_C \frac{\dot{y}\ddot{x} - \dot{x}\ddot{y}}{\dot{x}^2 + \dot{y}^2} dt.
$$ I'm only working with simple $2\times 2$ linear system fixed points, i.e. centers, saddles, stable nodes, etc...
While my method works, with $x = \cos t$ , $y = \sin t$ , and $t \in [0,2\pi]$ , I was told very quickly by my prof. that we can also directly calculate it with only the difference of $\phi$ at $t = 0, 2\pi$ . Yet, every parameterization has cancelled out to zero when I calculate arctan: when the index is suppose to 1.
It seems like my parameterization will always cancel out since $0 \equiv 2\pi \pmod{2\pi}$ . What am I missing?","['winding-number', 'ordinary-differential-equations']"
3014945,"Using the Law of Cosines results in an ""invalid"" answer, why?","I have the following isosceles triangle: I want to find the $\alpha$ angle, and I know that it is obtuse. My first instinct was to get the length of $BM$ using the Law of Cosines, which results in two answers: a negative one and a positive one; I immediately descredited the negative one because all lengths are assumed to be positive in geometry, or so have I assumed thus far... $$BM^2 = x^2 + 0.25x^2 - x^2\cos(50)$$ $$BM \approx \pm 0.78x$$ From here, I thought I could easily extrapolate $\alpha$ by plugging it into the Law of Sines formula, but to my surprise I did not get the correct result, $\alpha \approx 100.53^\circ$ , but $\alpha - 180 \approx 79.47^\circ$ . $$\frac{BM}{\sin(50)} = \frac{x}{\sin(\alpha)}$$ $$\downarrow$$ $$\frac{0.78x}{\sin(50)} = \frac{x}{\sin(\alpha)}$$ $$\downarrow$$ $$\sin(\alpha) = \frac{x\cdot \sin(50)}{0.78x} \rightarrow \alpha \approx 79.16^\circ$$ I assume this is because I discredited what is a valid trigonometrical answer, but why is it? Until now I have been under the impression that all lengths of geometrical shapes must be positive. I am aware there are other methods to solve this, but I am only particularly interested in why my specific one does not behave the way I want it to. Thanks in advance.","['trigonometry', 'geometry']"
3014948,Finding expectation of joint uniform continuous distribution without integrating,"From SOA sample 138: A machine consists of two components, whose lifetimes have the joint density function $$f(x,y)=
\begin{cases}
{1\over50}, & \text{for }x>0,y>0,x+y<10 \\
0, & \text{otherwise}
\end{cases}$$ The machine operates until both components fail.
  Calculate the expected operational time of the machine. I was able to get the solution by deciding on case $Y>X$ , drawing a triangle with vertices at $(0,0)$ , $(0,10)$ , and $(5,5)$ and then integrating $\int_0^5 \int_{x}^{10-x}{y\over50}\ dy \ dx$ to get $2.5$ , and then doubling by symmetry for the case of $X>Y$ to get $5$ . However the SOA solution does it a different way that I am trying to understand: Suppose the component represented by the random variable $X$ fails last. This is represented by
  the triangle with vertices at $(0, 0)$ , $(10, 0)$ and $(5, 5)$ . Because the density is uniform over this
  region, the mean value of $X$ and thus the expected operational time of the machine is 5. By
  symmetry, if the component represented by the random variable $Y$ fails last, the expected
  operational time of the machine is also $5$ . Thus, the unconditional expected operational time of
  the machine must be $5$ as well. I bolded the part that I do not understand. Where do they get $5$ just by looking at the triangle, which is $25$ in area?","['statistics', 'probability']"
3014962,"Strength of ""every affine scheme is compact""","Among the first results one usually sees, right after defining schemes, is that affine schemes are compact, however this statement is stronger than $\mathsf{ZF}$ : In $\mathsf{ZFC}$ we have that $$\operatorname{Spec}\left(\prod_{i=1}^\infty\Bbb F_2\right)\cong\beta\Bbb N,$$ the Stone-Čech compactification on $\Bbb N$ , but in $\mathsf{ZF}$ the only ultrafilters we can prove to exist on $\Bbb N$ are the principal ones, corresponding to $\Bbb N\subseteq\beta\Bbb N$ , which is a discrete subspace, hence not compact. So, if $\beta\Bbb N\setminus\Bbb N=\varnothing$ we have an affine scheme which is not compact. Is the statement ""every affine scheme is compact"" equivalent to some more well-known statement over $\mathsf{ZF}$ ? Or does it imply/is it implied by some more well known statements weaker than $\mathsf{AC}$ ?","['axiom-of-choice', 'algebraic-geometry', 'set-theory']"
3014970,Probability of getting out of a circular area,"An airplane is moving (straight) within a circle of radius $R$ with constant speed $V$ for $t$ seconds. It can start at any place within the circle and move in each direction (uniform distributions).  What is the probability that it gets out of the circle (as a function of $V$ , $t$ and $R$ )?","['circles', 'geometry', 'probability']"
3014981,Can a function have two derivatives?,"I am a senior in high school so I know I am simply misunderstanding something but I don't know what, please have patience. I was tasked to find the derivative for the following function: $$ y = \frac{ (4x)^{1/5} }{5} + { \left( \frac{1}{x^3} \right) } ^ {1/4} $$ Simplifying: $$ y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{1 ^ {1/4}}{x ^ {3/4}} } $$ $$ y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{\pm 1}{x ^ {3/4}} } $$ Because $ 1 ^ {1/n} = \pm 1 $ , given $n$ is even $$ y = \frac{ 4^{1/5} }{5} x^{1/5} \pm { x ^ {-3/4} } $$ Taking the derivative using power rule: $$ \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{-3}{4} { x ^ {-7/4} } $$ which is the same as $$ \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{3}{4} { x ^ {-7/4} } $$ And that is the part that I find difficult to understand. I know that I should be adding the second term(I graphed it multiple times to make sure), but I cannot catch my error and my teacher did't want to discuss it. So I know I am doing something wrong because one function cannot have more than one derivative.","['calculus', 'derivatives']"
3014999,How to show $a$ and $b$ are different??,"We have a continuously differenciable function $f:[0,1]\rightarrow[0,1]$ such that $f(0)=0, f(1)=1$ . We need to show that there exists different real numbers $a,b\in(0,1)$ such that $f'(a)f'(b)=1.$ I used the mean value theorem as follows: There exists $\alpha\in(0,1)$ such that $(f\circ f)'(\alpha)=1$ , that is to say, $f'(f(\alpha))f'(\alpha)=1$ . Let $a=f(\alpha)$ and $b=\alpha$ . Then, $f'(a)f'(b)=1$ . But I didn't use the continuity of the derivative, and a don't know how to show that $a\neq b$ .
Thanks, any help will be appreciated.","['calculus', 'analysis']"
3015031,Matrix performing local differintegral analysis being its own inverse. Coincidence?,"I found a curious matrix $$T = \begin{bmatrix}1&2&1\\1&0&-1\\1&-2&1\end{bmatrix}$$ This matrix (or actually $\frac 1 2 T$ ) performs Local mean value (integral) estimation. Local derivative estimation by central midpoint distance. Local second order derivative by midpoint (but half step length compared to 2). Yet if we calculate its inverse, we find: $$T^{-1} = \frac 1 4 T$$ or maybe more interesting: $$\left(\frac 1 2 T\right)^{-1}=\frac 1 2 T$$ Matrix and inverse are the same! Is this a coincidence? Or what properties or choices of estimators will give us this curious behavior? Is this a property of differential and integral operators in general or just because of some specific choice of how to estimate them?","['computational-mathematics', 'calculus', 'linear-algebra', 'signal-processing', 'soft-question']"
3015046,Showing that $a^2+b^2+c^2+d^2+e^2+65=abcde$ has integer solutions greater than $2018$?,"This question comes from a Chinese high school olympiad training program. It seems remarkably more difficult (and indeed, interesting!) than all other problems arising in the same program, especially since an elementary (high-school level) solution is probably available. Show that there exists integers $a,b,c,d,e>2018$ so that the equation $$a^2+b^2+c^2+d^2+e^2+65=abcde$$ is satisfied. For what it's worth, here's what I have tried. Rewriting the equation as a quadratic polynomial in $a$ , $$a^2-(bcde)a+b^2+c^2+d^2+e^2+65=0.$$ For there to be integer solutions, the discriminant must be a perfect square. Hence $$ b^2c^2d^2e^2-4b^2-4c^2-4d^2-4e^2-4\cdot5\cdot13=n^2.$$ I don't however see how I can solve this equation, especially due to the large number of unknowns. Any ideas? Edit: Ivan Neretin presents an excellent answer by Vieta Jumping, which I'm sure will yield results. However, the training program I mentioned has not discussed such advanced tactics as Vieta Jumping yet, and only covered $\gcd$ , $\operatorname{lcm}$ , factorisation of polynomials, discriminants, modular arithmetic, divisibility and quadratic residues. Hence despite Ivan's excellent solution, I would still be extremely appreciative of a more elementary solution.","['contest-math', 'number-theory', 'elementary-number-theory']"
3015077,Can we refine this asymptotic for Laguerre polynomials?,"I just found an interesting and useful limit for Laguerre polynomials: $$\lim_{n \to \infty} L_n \left( \frac{2r}{n+1/2} \right)=J_0(2 \sqrt{2r})$$ I'm using specifically this form of the argument because it's the one I'm working with in the application. Of course, we can set any fixed number instead of $1/2$ in the denominator. I found this limit in a paper , which references G. Szego. Orthogonal Polynomials. Amer. Math. Soc. Colloq. Publ. 23, Amer. Math. Soc. Providence,
RI, 1975. Fourth Edition. , , Theorem 8.1.3. While the limit is useful for very large orders and smallish $r$ , I would really like to know if there's a refinement that could be applied to derive an asymptotic expansion, which would depend on $n$ . Here's an illustration which shows that the limit is not that good for larger $r$ (though it does approximate the roots better than the magnitude): Not sure how we could refine this asymptotic or how the original limit was derived (as I don't have the linked book). One way is considering the differential equations for both functions. There's also an interesting result from Gradshteyn-Ryzhik: $$L_n(z)= \frac{2}{n!} e^z \int_0^\infty e^{-t^2} t^{2n+1} J_0(2t \sqrt{z}) dt$$ Which may or may not be related to the limit above.","['approximation', 'asymptotics', 'limits', 'orthogonal-polynomials', 'bessel-functions']"
3015103,"A bounded, continuous function on $\mathbb{R}^d$ is the pointwise limit of a bounded sequence of linear combinations of Borel indicators","Where can I find a proof of the following fact? Any bounded, continuous real-valued function on $\mathbb{R}^d$ is the pointwise limit of a bounded sequence of linear combinations of indicators of Borel sets","['measure-theory', 'reference-request', 'real-analysis', 'calculus', 'borel-sets']"
3015118,Path integral solution to heat equation,"Let $(M,g)$ be a compact Riemannian manifold. Then the solution $u:M\times [0,\infty)\to \mathbb{R}$ of the heat equation $\partial_t u=\Delta_gu$ starting at $u_0\in C^{\infty}(M)$ is given by the path integral $$
u(x,t)=\int_{\gamma \in H_x} e^{-E(\gamma)/4t}u_0(\gamma(1))d\gamma,
$$ where the integral is taken over the classical Wiener space $H_x\subset L^{2,1}([0,1],M)$ of finite energy paths $\gamma:[0,1]\to M$ starting at $x$ (i.e. $\gamma(0)=x$ ). Also, here $$
E(\gamma)=\int_0^1\Big|\frac{d\gamma}{ds}\Big|^2ds
$$ is the Dirichlet energy of a curve in $(M,g)$ and the measure of integration is the Wiener measure. See this article for a survey of the above. Question: Does the above path integral formula for $u$ hold for more general parabolic PDEs? More precisely, let $L:C^{\infty}(M)\to C^{\infty}(M)$ be a second order elliptic operator (e.g. above we took $L=\Delta_g$ ). Then can we write the solution $u:M\times [0,T)\to \mathbb{R}$ of the parabolic PDE $$
\partial_tu=Lu
$$ with initial condition $u_0\in C^{\infty}(M)$ as the path integral $$
u(x,t)=\int_{\gamma \in H_x} e^{-S(\gamma)/4t}u_0(\gamma(1))D\gamma,
$$ for some functional $S:H_x\to \mathbb{R}$ on the path space? What is $S$ in this case? Note that $S=E$ when $L=\Delta$ .","['ordinary-differential-equations', 'functional-analysis', 'partial-differential-equations', 'mathematical-physics', 'differential-geometry']"
3015121,Sturm-Liouville Completeness Proof,"I am looking for a basic but rigorous introduction to Sturm-Liouville theory. In particular, I would like to see a proof that the eigenfunction solutions of Sturm-Liouville problems are complete. Some books seem to suggest that this follows from the completeness of eigenvectors of finite-dimensional Hermitian operators in linear algebra, but that can't be right. All the linear algebra proofs I can find use the fact that the (finite) number of eigenvectors of a Hermitian matrix equals the (finite) dimension of the matrix, and this result cannot carry over into infinite-dimensional function space.","['sturm-liouville', 'ordinary-differential-equations']"
3015139,Is the metric tensor relative to a reference coordinate system?,"I am fairly new to this topic, and I don't know anything about tensors, but I have to know what a metric tensor is and this is how I have been thinking about it: 1)A metric tensor is a mathematical entity that expresses distances between points in generalized coordinates; This part I got quite well I think, but now here's my question: I am a being that lives in a flat 3D space, that is, everything I see and experience is according to cartesian coordinates and even if I represent something with other coordinate system (say, with spherical coordinates) that new system arises from my experience on a 3D flat space. Another way of saying this is that the ""instinctive"" metric tensor I live by is: $$g_1=\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{bmatrix}$$ But if I then, somehow, want to be in a spherical space, that is, a space where the coordinates are $(r,\theta ,\phi )$ , then I say (well the books do) that my new metric space is: $$g_2=\begin{bmatrix}
    1 & 0 & 0 \\
    0 & r^2 & 0 \\
    0 & 0 & r^2sin^2(\theta)
\end{bmatrix}$$ But that results seems to me that is biased,that is, $g_2$ exists as it is because I want that a given distance between 2 points in 3D space to have the same value in the spherical space between those 2 same points when they are parametrized from one space to the other. But what if it all went backwards? Suppose now that I live in a 3D spherical space, I could represent all of space with a 3 orthogonal axis space (like the figure below) and simply replace x,y and z by $(r,\theta ,\phi)$ and I would and my ""natural"" metric tensor would still be $g_1$ but when I went to the 3D flat space, then $g_2$ would not be the same. My question/argument is that all metric tensors are relative to our own experience of day to day life, and all other metric spaces exist as they do to agree with our experience, to be in conformity with our reference ( $g_1$ ), thus we cannot talk of metric tensors as absolute entities, we have to mention the reference as well.","['tensors', 'metric-spaces', 'differential-geometry']"
