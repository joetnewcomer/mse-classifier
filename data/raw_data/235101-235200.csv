question_id,title,body,tags
4917304,Close form of $\int_a^b x^n \sqrt{(b-x)(x-a)}dx$,"I am interested in the integrals of form $$I_n=\int_a^b x^n \sqrt{(b-x)(x-a)}dx$$ with $b>a>0$ .
I have already derived the close form solutions for the first few integrals below \begin{align}
& I_0=\int_a^b \sqrt{(b-x)(x-a)}dx=\frac\pi8(b-a)^2 \\
& I_1=\int_a^b x\sqrt{(b-x)(x-a)}dx=\frac\pi{16}(b-a)^2 (a+b)\\
& I_2=\int_a^b x^2\sqrt{(b-x)(x-a)}dx=\frac\pi{128}(b-a)^2 (5a^2+6ab+5b^2)\\
\end{align} I am wondering if there is a systematic derivation that leads to a generalized close form solution for $I_n$ .","['integration', 'definite-integrals']"
4917320,"Show that if $f$ is a measurable complex-valued function on $(X,\mathscr{A})$, then $|f|$ is also measurable.","I need to show If $f$ is a measurable complex-valued function on $(X,\mathscr{A})$ , then $|f|$ is also measurable. I tried it myself, but don't know if my work is correct or not? Could someone please help me check? Thank you very much! My attempt: Let $f:(X,\mathscr{A})\to(\mathbb{C},\mathscr{B}(\mathbb{C}))$ be measurable. Write $f(x)=u(x)+iv(x)$ , where $u,v:X\to\mathbb{R}$ . Then $|f(x)|=\sqrt{u^2(x)+v^2(x)}\in\mathbb{R}$ . We want to show that $|f|:(X,\mathscr{A})\to(\mathbb{R},\mathscr{B}(\mathbb{R}))$ is measurable. Let $\mathscr{B}_0$ be the collection of all subintervals of $\mathbb{R}$ of the form $(-\infty,b]$ . Then the Borel sigma-algebra $\mathscr{B}(\mathbb{R})$ is equal to $\sigma(\mathscr{B}_0)$ . Let $B\in\mathscr{B}_0$ , then $B=\{z\in\mathbb{R}:a\leq b\}$ for some $b\in\mathbb{R}$ . Then \begin{align}
|f|^{-1}(B) = \{x\in X:|f(x)|\in B\} = \{x\in X:|f(x)|\leq b\}.
\end{align} If $b<0$ , then $|f|^{-1}(b)=\emptyset\in\mathscr{A}$ . So suppose $b\geq0$ . Since $f$ is measurable with respect to $\mathscr{A}$ and $\mathscr{B}(\mathbb{C})$ , its real and imaginary part $u$ and $v$ are $\mathscr{A}$ -measurable, so are $u^2$ , $v^2$ , and $u^2+v^2$ . Thus, \begin{align}
|f|^{-1}(B) = \{x\in X:|f(x)|\leq b\} = \{x\in X:u^2(x)+v^2(x)\leq b^2\} \in\mathscr{A}.
\end{align} Since $B$ is arbitrary, we have proved that $|f|$ is measurable. If there is any mistake, please point it out! I really appreciate it!","['measure-theory', 'proof-writing', 'analysis', 'real-analysis', 'solution-verification']"
4917323,Asymptotic expansion of a differential equation,"I am a physicist studying a nonhomogenous differential equation of the general form \begin{equation}
a(x)f^{(n)}(x) +b(x)f^{(n-1)}(x) +\dots + c(x)f(x) = \mathcal{F}(f(x),x),
\tag{1}
\end{equation} where $a(x)$ , $b(x)$ , $\dots$ , $c(x)$ are rational functions and $\mathcal{F}(f(x),x)$ is a complicated expression. The equation would be linear without this term.  Crucially, the explicit form of $\mathcal{F}(f(x),x)$ is not known unless one does an aymptotic expansion for $x \gg1$ , in which case one has \begin{equation}
\mathcal{F}(f(x),x) = \frac{\log(x)}{x^4} + \mathcal{O}\left( \frac{1}{x^5} \right).
\end{equation} Inserting this approximation into the equation makes it linear.
The solutions to the differential equation are expected to go to zero at large $x$ , and so do the coefficient functions $a(x)$ , $b(x)$ , $\dots$ , $c(x)$ . However, whereas the left-hand side of $(1)$ is exact, substituting $\mathcal{F}(f(x),x) \to \log(x)/x^4$ on the right-hand side assumes an asymptotic expansion. This brings me to the question: is it possible to do the corresponding asymptotic expansion on the rest of the equation, from the known forms of $a(x)$ , $b(x)$ , $c(x)$ ? In other words, Is it possible to, from a given differential equation whose solution is regular at $x\to \infty$ , write another (hopefully simpler) equation for the asymptotic behavior of the solution? There is no dimensionless number to use as expansion parameter for perturbation theory, but only some terms of the asymptotic expansion of $f(x)$ at infinity are needed (no matching with the ""internal"" solution is required). I expect $f(x)$ to be of the form \begin{equation}
f(x) = f_0+\frac{f_1(x)}{x}+\frac{f_2(x)}{x^2} + \dots \, , \hspace{1cm} \text{for $x\gg1$},
\end{equation} where the dominant dependence on $x$ of each term is given the power of $1/x$ , but the $f_i(x)$ may still have some milder dependence on $x$ . It is possible to expand each term of $(1)$ in powers of $1/x$ and look at each power separately, but I do know if that is a valid approach.","['asymptotics', 'ordinary-differential-equations']"
4917327,Does anyone know a non-trivial surjective multiplicative homomorphism from the $4\times 4$ matrices to the $2 \times 2 $ matrices?,It's well known the determinant provides a surjective homomorphism between $n \times n$ real matrices and $\mathbb{R}$ with the operation of multiplication. I was curious of inter-matrix multiplicative homorphisms. The trivial idea of viewing a $4\times 4$ matrix as a $2\times 2$ matrix with entries in the $2\times 2$ matrices suggests a determinant formula (of the usual type) but unfortunately that doesn't actually seem to work at all as a homomorphism after investigating it with sympy. Doing some reading I stumbled upon: Quasideterminants for non-commutative rings. But i'm a bit dissatisfied. I would like to think such a homomorphism should exist but I just don't know how to find it.,"['matrices', 'determinant', 'abstract-algebra', 'linear-algebra']"
4917403,Sequence $\{a_n\}$ where $a_n$ is the smallest proper multiple of $a_{n-1}$ containing $a_{n-2}$ in its digits,"Recently, I've been interested in sequences $\{a_n\}$ where $a_n$ is the smallest proper multiple (multiple of a number larger than itself) of $a_{n-1}$ containing $a_{n-2}$ in its digits. A number $Y$ is said to be containing $X$ in its digits if one can obtain $X$ by removing zero or more digits from $Y$ . What I'm really interested in is the behavior of the sequence under different starting values. With $a_1 = 1$ and $a_2 = 1$ , the sequence is just the power of tens. With $a_1 = 1$ and $a_2 = 2$ , the sequence goes $$1, 2, 10, 20, 100, 200, 1000, 2000, ...$$ With $a_1 = 2$ and $a_2 = 2$ , the sequence goes $$2, 2, 12, 24, 120, 240, 1200, ...$$ So far, $\frac{a_{n+1}}{a_n}$ remains bounded as $n \rightarrow \infty$ . With $a_1=1$ and $a_2 = 3$ (see OEIS sequence A372880 .), something different happens, the sequence goes: $$1, 3, 12, 36, 612, 1836, 168912, 10810368, 16366897152, 51703028103168, 1563447866811697152,...$$ And the ratio between the consecutive sequence members goes: $$3, 4, 3, 17, 3, 92, 64, 1514, 3159, 30239, 15043784, 49130515,...$$ It seems to me that $\frac{a_{n+1}}{a_n} \rightarrow \infty$ as $n\rightarrow \infty$ . Main Question : Can anyone prove/disprove that the ratio does diverge for the starting values $(1, 3)$ ? Another conjecture : For any valid starting value, the ratio is either: eventually becomes $10$ (example: $(1, 1)$ as starting values.) bouncing between $2$ and $5$ (example $(1, 2)$ or $(2, 2)$ as starting values.) diverge to $\infty$ . (the starting values $(1, 3)$ is expected to fall in this category)","['number-theory', 'elementary-number-theory', 'recurrence-relations', 'oeis', 'recreational-mathematics']"
4917462,Permutations of Triangle Centers: Investigating Relationships Between Circumcircle Intersections.,"I was solving this problem : Reconstruct the triangle from the points at which the extended
bisector, median and altitude drawn from a common vertex intersect the circumscribed circle. I found the solution to this problem but I wonder since we can always construct the triangle back from these three points  what are the relations between  the different triangles that can be constructed by swapping the points. i.e given the three points $E,F,G$ In the triangle $ABC$ let the $E$ be the point that intersects the extended bisector and the circumscribed circle, let $F$ be the point that intersects the extended altitude and the circumscribed circle, let $G$ be the point that intersects the extended median and the circumscribed circle,In the triangle $XYZ$ let the $E$ be the point that intersects the extended bisector and the circumscribed circle, let $G$ be the point that intersects the extended altitude and the circumscribed circle, let $F$ be the point that intersects the extended median and the circumscribed circle. For some reason the point that  intersects the extended bisector and the circumscribed circle is always between the point that intersects the extended altitude and the circumscribed circle  and  the point that intersects the extended median and the circumscribed circle (I couldn't rigorously prove why ), so there are only $2!$ triangles which could swap the points $F,G$ . Before I drew these triangle I thought that they will be similar triangles but I was wrong, But I still think that these two triangle must have some interesting relation between them as for any triangle there is a unique triangle that can be obtained with this method. I will define two triangle that are constructed this way by permutated triangles . Another question If given the two triangles $ABC$ and $XYZ$ how to determine if they are permutated triangles?","['euclidean-geometry', 'geometry', 'geometric-construction']"
4917467,Why $\nabla f$ do not exactly coincide with $D f$ (it's its transpose),"Is there any reason (historical, or of any other kind) to why $$\nabla f= \begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \\ \end{bmatrix}$$ for a three variables, scalar-valued function $f$ is the transpose of the total derivative of $f$ (and is not exactly equal to the total derivative of $f$ ) ? Also, as an auxiliary: is my following reasoning right, in finding the Hessian of $f$ ? I perform the matrix product between the $3\times 1$ nabla operator matrix: $$\nabla = \begin{bmatrix}\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix}$$ and the $1\times 3$ total derivative matrix. I came up with this ""out of luck"", but I have to say that I never know when to use $\nabla$ or instead its transpose. I am also further confused by the fact that $H$ is informally described as $\nabla ^2 $ in its Wikipedia article. Is it supposed to mean $\nabla$ applied to $\nabla f$ ? PS: $f$ is assumed to be twice differentiable.","['hessian-matrix', 'transpose', 'derivatives']"
4917470,probability convergence as compactness for a.s. convergence,"I stumbled upon the sequential characterization of probability convergence. $ X_n \xrightarrow{\mathbb{P}} X$ if and only if for each subsequence $X_{n_k}$ , there exists a subsequence $X_{n_{k_j}}$ such that $X_{n_{k_j}} \xrightarrow{a.s.} X$ . Now, normally, for some idea of convergence $\to$ in a space $\Omega$ , one defines (sequential) compactness as follows: For all sequences $X_n$ in $\Omega$ there exist a convergent subsequence $X_{n_k} \to X$ for $X\in\Omega$ . This does very much sound like the characterization above, if not for the fact that X is fixed a priori . Thus, here comes the question: is there some way to interpret the notion of $\mathbb{P}$ convergence as the compactness of the space $X_n$ with respect to a.s. convergence? Thank you.","['convergence-probability', 'probability-theory', 'probability', 'strong-convergence']"
4917482,Let $G$ be a non-abelian finite group whose all non-linear irreducible characters are faithful. Is there a classification of these groups?,"Let $G$ be a non-abelian finite group whose all non-linear irreducible characters are faithful. Is there some kind of a classification of these groups? If $G$ is a finite non-abelian simple group, the above is certainly true. The symmetric groups $S_n$ also has the above property. I can see that for a group $G$ as above, if $N\neq \{1\}$ is a normal subgroup, then $[G,G]\leq N$ . In other words, $G/N$ is abelian. The converse is also true, that is, if $G/N$ is abelian for all normal subgroups $\{1\} \neq N\leq G$ , then all non-linear irreducible characters are faithful. So in other words, I am looking for a classification of non-abelian finite groups whose non-trivial quotients are all abelian. Thanks in advance for any kind of reference or help.","['characters', 'representation-theory', 'finite-groups', 'abstract-algebra', 'group-theory']"
4917499,The limit of $\frac{a^n}{(1+a)(1+a^2) \ldots (1+a^n)}$ $a>0$,"I was given the following question in a homework assignment: Given $a>0$ either both statements are true, both false, or one is true, and the other false (any combination): 1) $\lim_{n \to \infty} \frac{a^n}{(1+a)(1+a^2) \ldots (1+a^n)} = 1$ 2) $\lim_{n\to \infty } \frac{a^n}{(1+a)(1+a^2) \ldots (1+a^n)} = a$ My solution to disprove (1): I constructed a sequence: $b_n = \frac{a^n}{(1+a)(1+a^2) \ldots (1+a^n)}$ . Since $b_n >0$ for all $n$ I was able to use the test: $\lim_{n\to \infty} \frac{b_{n+1}}{b_n} = a \cdot \bigg( \frac{1}{1+a^{n+1}}\bigg)$ . If $0<a<1$ , then the limit of the ratio is less than 1, and $\lim_{n\to \infty}b_n =0$ . If $a \ge 1$ then the limit of the ration goes to 0 (zero). In any case, for $0<a<1$ $\lim_{n \to \infty}b_n \ne 1$ . As for (2) I suppose the same logic would apply, but I am not very sure... I would greatly appreciate either a reassurance for my solution, or a tip in the right direction, if my solution, to any of the sections, is incorrect. Thank you very much in advance!","['limits', 'calculus', 'sequences-and-series']"
4917546,Embedding Riemannian manifold into Euclidean space with matching distances,"Let $(\mathcal{M},g)$ be a Riemannian manifold and $d$ the induced shortest path distance. I am interested in deciding whether there exists an embedding $\iota:\mathcal{M}\to \mathbb{R}^M$ , with $M$ not necessarily related to the dimension of the manifold $\mathcal{M}$ , such that $$d(x,y)=||\iota(X)-\iota(y)||_2, \quad \forall x,y\in \mathcal{M}.$$ In the literature, I could not really find an answer to my question: On the one hand, the Nash embedding theorem guarantees that each Riemannian manifold can be isometrically embedded into some $\mathbb{R}^M.$ However, in my understanding this means that $$d(x,y)=\text{dist}(\iota(x),\iota(y)),$$ where $\text{dist}$ denotes the path distance induced on the embedded submanifold $\iota(\mathcal{M})$ by using the Euclidean scalar product as Riemannian metric on $\iota(\mathcal{M}).$ This is not what I am looking for. On the other hand, I have seen in Lee's Riemannian manifolds book that a Riemannian manifold is locally isometric to Euclidean space, iff the curvature tensor vanishes. However, this is also not exactly what I am looking for, because in my understanding this deals with the situation, where $M=\text{dim}(\mathcal{M}).$ In this situation, it answers my question (the necessary and sufficient condition is then just that the curvature tensor of $(M,g)$ is $0$ ), however, I can not see why such an embedding should not exist for a higher dimensional $\mathbb{R}^M$ . Is my understanding of the two results correct? An if so, are there any results about my original question?","['riemannian-geometry', 'differential-geometry']"
4917554,Is every normalization a blowup?,"Is the normalization of a variety always a blowup along some coherent ideal sheaf? If not, I would like to see a concrete counter-example. Let $Y \to X$ be the normalization. The answer is positive in the following situations: $X$ is quasi-projective. This follows since normalization is a projective birational morphism. The singularities are mild, namely $X$ is Gorenstein and $Y$ is both Gorenstein and Cohen–Macaulay. In this case, by arXiv:1608.04525 , normalization is the blowup along the conductor ideal. By variety , I mean an integral separated scheme of finite type over the complex numbers.","['blowup', 'singularity-theory', 'algebraic-geometry', 'birational-geometry', 'schemes']"
4917597,Best uniform approximation of $x^{n+2}$ in $\mathbb{P_n}$,"Let $n\geq 1$ be an integer and $f(x)=x^{n+2}$ for all $ x \in [−1, 1]$ . Find the best uniform
approximation of $f$ in $\mathbb{P}_n$ . Attempt: Let's solve this first for $f(x)=x^{n+1}$ instead. Suppose $p \in \mathbb{P}_n$ is the best uniform approximation. Then $g=f-p \in \mathbb{P}_{n+1}$ with leading coefficient $1$ . As $g$ has the smallest norm among all polynomials in $\mathbb{P}_{n+1}$ with leading coefficient $1$ , $g$ must be the $(n+1)$ st Chebyshev Polynomial. Now, for $f(x)=x^{n+2}$ the same argument does not work. But by Chebyshev Alternation theorem there must exist $n+2$ distinct points $-1 \leq x_1 < x_2 < \cdots < x_{n+2} \leq 1$ such that $g=f-p$ attains its maximum magnitude at those points with alternating signs. As $g' \in \mathbb{P}_{n+1}$ , either $x_1=-1$ or $x_{n+2}=1$ . However, I can't make any progress from here. Can I get any hints/insights?","['numerical-methods', 'chebyshev-polynomials', 'analysis']"
4917604,Discriminant formula - do the coefficients include their signs?,"Beginner here. Can I ask someone to explain to me the following? Given the following general form of quadratic equation: $$
ax^2+bx+c=0
$$ and the following formula for discriminant: $$
D=b^2−4ac,
$$ what is the $b$ coefficient in the following equation? $$
2x^2 - 2x = 0
$$ Is it $-2$ or $2$ ? Does the coefficient count with the positive/negative sign? I am confused because for example $-2^2= -4$ , and so here the sign is not taken into consideration, unless it's $(-2)^2$ .","['algebra-precalculus', 'quadratics', 'linear-algebra']"
4917670,"Show $\lim\limits_{n \to \infty}\frac{f\left(nx\right)}{n^2}=0$ a.e. $x\in\mathbb{R}$ if $f \in L^1\left([0,T]\right)$ and periodic.","Show $\lim\limits_{n \to \infty}\frac{f\left(nx\right)}{n^2}=0$ a.e. $x\in\mathbb{R}$ if $f \in L^1\left([0,T]\right)$ and periodic in $\mathbb{R}$ , where $T>0$ is the period. My idea is as follows: it is easy to demonstrate $f$ is measurable on $\mathbb{R}$ and then we have $$
\int_{[0,T]}\frac{\left|f(nx)\right|}{n^2}\mathrm{d}x=\frac{1}{n^3}\int_{[0,nT]}\left|f(x)\right|\mathrm{d}x=\frac{c}{n^2}
$$ where $c=\int_{[0,T]}\left|f(x)\right|\mathrm{d}x<\infty$ . Therefore we have $$
\lim\limits_{n \to \infty}\int_{[0,T]}\frac{\left|f(nx)\right|}{n^2}\mathrm{d}x=0
$$ and so $\frac{\left|f(nx)\right|}{n^2}$ is $L^1$ convergent to 0. Therefore we can show it converges to 0 in measure and then there is a subsequence which converges to 0 a.e. $x\in\mathbb{R}$ . However, I encounter problems when showing the whole sequence converges to 0 a.e. $x\in\mathbb{R}$ , can anyone give me some hints?","['integration', 'pointwise-convergence', 'real-analysis']"
4917689,Definition of smooth map between smooth manifolds,"Let $(M,\mathcal{A})$ an m-dimensional smooth manifold and $(N, \mathcal{B})$ a n-dimensional smooth manifold, with $\mathcal{A}:=\{\phi_i:U_i\to V_i \ | \ i \in I\}$ an atlas for $M$ (that is: $U_i \subseteq M$ and $V_i \subseteq \mathbb{R}^m$ are open sets, $\phi$ is an omeomorphism and the maps $\phi_\alpha\phi_\beta^{-1}$ are $C^{\infty}$ )  and $\mathcal{B}:=\{\psi_i:W_j\to Z_j \ | \ j \in J\}$ an atlas for $N$ . A function $f:(M,\mathcal{A}) \to (N,\mathcal{B})$ is smooth if $\forall p \in M, \ \exists U_i \in \mathcal{A} \  \exists W_j \in \mathcal{B}$ such that $p\in U_i, \ f(p) \in W_j$ and $f(U_i) \subseteq W_j$ The composition $\psi_j \circ f \circ \phi_i^{-1}$ is $C^{\infty}$ The problem is that a manifold is usually defined as $(M, [\mathcal{A}])$ where $[\mathcal{A}]$ is the equivalence class of all smooth atlas equivalent to $\mathcal{A}$ (two atlas $\mathcal{A}$ and $\mathcal{A'}$ are equivalent if $\mathcal{A} \cup \mathcal{A'}$ is an atlas). Is the definition of smooth function independent of the choice of equivalent atlas? Let $\mathcal{A'}$ equivalent to $\mathcal{A}$ and $\mathcal{B'}$ equivalent to $\mathcal{B}$ , I can't find $U'_i \in \mathcal{A'}$ and $W'_j \in \mathcal{B'}$ such that $p\in U'_i, \ f(p) \in W'_j$ and $f(U'_i) \subseteq W'_j$ . Can you give me suggestions of what should i choose as $U'_i$ and $W'_j$ ?","['smooth-manifolds', 'differential-geometry']"
4917703,Kalman Filter -- handling large covariance matrices with principal-component-like structures,"I understand that the estimation of the covariance matrices are the important part of the Kalman filter. However in my use case my covariance matrices are really big but with a pretty neat factor model (PCA-like) structure. Borrowing from Wikipedia convention , what I mean is that our $Q$ and $R$ matrices can be written as some sort of reduced form $$Q =B\Sigma_{Q}B^{T}+\Sigma_{qq}$$ where $\Sigma_{qq}$ is a diagonal matrix and the dimension of $\Sigma_{Q}$ is significantly smaller than the dimension of $Q$ and similarly $$R =B\Sigma_{R} B^{T}+\Sigma_{rr}$$ For simplicity we can even assume $\Sigma_{Q} = c \Sigma_{R}$ , $\Sigma_{qq} = c \Sigma_{rr}$ . i.e they are scaled by a constant. Different from PCA or SVD, $\Sigma_Q$ or $\Sigma_R$ are not diagonal matrices, as in different ""factors/components"" have correlations, but it's enough to reduce the complexity of $R$ and $Q$ by quite a bit. Now, can the ""reduced form"" of the covariance matrices be carried into the rest of the update steps (again borrowed from wikipedia ), such as $P$ ? If I still have to operate on a full $P$ , it'd defeat the purposes of the reduced forms I developed for $R$ and $Q$ . We can further assume $F=I$ in my use case, if that helps, but not much we can assume about $H$ . If necessarily we can also assume I have no observation noise (i.e. $R$ is 0). I'd like to have a more general case without making assumption on $F$ or $R$ if possible for my own intellectual curiosity. \begin{aligned}
\mathbf {P} _{k\mid k-1}&=\mathbf {F} _{k}\mathbf {P} _{k-1\mid k-1}\mathbf {F} _{k}^{\textsf {T}}+\mathbf {Q} _{k},\\
\mathbf {S} _{k}&=\mathbf {H} _{k}\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}+\mathbf {R} _{k},\\
\mathbf {K} _{k}&=\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}\mathbf {S} _{k}^{-1},\\
\mathbf {P} _{k|k}&=\left(\mathbf {I} -\mathbf {K} _{k}\mathbf {H} _{k}\right)\mathbf {P} _{k|k-1}.
\end{aligned}","['kalman-filter', 'control-theory', 'linear-algebra', 'numerical-linear-algebra', 'numerical-methods']"
4917707,Find number of ways to arrange 1 to n in a line so that no two consecutive numbers are adjacent,"Find number of ways to arrange $1$ to $n$ in a line so that no two consecutive numbers are  adjacent. I know this is https://oeis.org/A002464 !! I tried it with inclusion exclusion and got the number of ways as $n!-\sum_{k=1}^{n-1}$$\left((-1)^k(n-k)!\sum_{r=1}^{k}2^r\binom{n-k}{r}\binom{k-1}{r-1}\right)$ I also tried it with generating function my approach is almost same as this Flajolet and Sedgewick generating function for Hertzsprung Problem I want to know how to establish following recurrence: Let $a_n$ be the required number of ways. Then $a_n=(n+1)a_{n-1}-(n-2)a_{n-2}-(n-5)a_{n-3}+(n-3)a_{n-4}\forall n\ge 4;a_0=1,a_1=1,a_2=0,a_3=0.$ Same problem here: Special permutations of $\{1,2,3,\ldots,n\}$ Similar problems but simpler than the current one: How many permutations of $\{1,2,3,...,n\}$ there are with no 2 consecutive numbers? Number of permutations $\langle a_1,\ldots,a_n\rangle$ of $\{ 1,\ldots ,n \}$ with $a_{i+1} - a_i \ne 1$ Show number of permutations on $[n]$ where $i$ is not followed by $i+1$ is $D_n + D_{n-1}$","['permutations', 'recurrence-relations', 'inclusion-exclusion', 'combinatorics', 'generating-functions']"
4917760,Sum of dependent Bernoulli random variables,"I tried solving this problem. Let $X_1,X_2, \dots$ be independent and identically distributed variables with $X_i∼\text{Ber}(\frac{1}{2})$ for all $i\in \mathbb{N}$ . Define $Y_i=\max\{X_i,X_{i+1}\}$ for all $i\in \mathbb{N}.$ Let $Z_n=\sum_{i=1}^nY_i.$ Calculate the mean and variance of $Z_n$ . Prove that $\lim_{n\to \infty}P(Z_n=n/2)=0$ . Prove that the sequence of independent variables ( $Z_n/n$ )
converges in probability to $3/4$ as $n\to\infty$ . I did solve 1 and 3, the mean and the variance, the mean is $3n/4$ and the variance is $5n/16-1/8$ . I have a problem with the second question.
I do know that $Y_i$ is $\text{Ber}(3/4)$ , but I can't deduce that $Z_n$ distribution is $\text{bin}(n,p)$ because the $Y_i$ are dependent.
I tried using the law of total probability on $X_1$ , and so on the next $X_i$ , but I just couldn't find any pattern.
I also tried Chebyshev's inequality, but it doesn't work as well. Any help with this question?","['statistics', 'probability-theory', 'probability']"
4917822,What is the Euclidean norm of the vector containing all $k$-order partial derivatives of $|x|$?,"Denote $|x|$ the Euclidean norm of a vector $x\in\mathbb R^N$ . Also denote $D^kf$ as the vector in $\mathbb R^{N^k}$ containing all $k$ -order partial derivatives of the function $f\colon\mathbb R^N\to\mathbb R$ . Consider the following norm for $D^kf(x)\in\mathbb R^{N^k}$ : $$\|D^kf(x)\|=\left[\sum_{i_1,\ldots,i_k=1}^N\left(\dfrac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)\right)^2\right]^{\frac12}. $$ Now let $f(x)=|x|$ . My question is if the following equation holds for some $C_{N,k}\in\mathbb R$ : $$\|D^kf(x)\|=\dfrac{C_{N,k}}{|x|^{k-1}},\quad\forall x\in\mathbb R^N\backslash\{0\}\mbox{ and }k\in\mathbb N=\{1,2,\ldots\}. $$ The case $k=1,2,3$ : I was able to prove for those cases because $$\dfrac{\partial f}{\partial x_i}(x)=\dfrac{x_i}{|x|},\ \dfrac{\partial^2f}{\partial x_i\partial x_j}(x)=\dfrac{\delta_{ij}}{|x|}-\dfrac{x_ix_j}{|x|^3},$$ and $$\dfrac{\partial^3f}{\partial x_i\partial x_j\partial x_k}(x)=\dfrac{3x_ix_jx_k}{|x|^5}-\dfrac{\delta_{jk}x_i}{|x|^3}-\dfrac{\delta_{ij}x_k}{|x|^3}-\dfrac{\delta_{ik}x_j}{|x|^3}.$$ After doing the calculations, we have $$\|D^1f(x)\|=1,\ \|D^2f(x)\|=\dfrac{\sqrt{N-1}}{|x|},\mbox{ and }\|D^3f(x)\|=\dfrac{\sqrt{3N-3}}{|x|^2}.$$ Therefore, the question is true for $k=1,2,3$ with $C_{N,1}=1$ , $C_{N,2}=\sqrt{N-1}$ , and $C_{N,3}=\sqrt{3N-3}$ . My try: I only have two types of ideas that seem promising. First idea: proving by induction on $k$ . However, I was not able to use the induction hypothesis properly. Second idea: for $k\geq3$ , we have $$\dfrac{\partial^k}{\partial x_{i_1}\cdots\partial x_{i_k}}\left(|x|^2\right)=0.$$ Unfortunately, I was not able to develop the left-term (using chain rule) to isolate the term $\frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)$ to calculate $\|D^kf(x)\|$ . A last attempt (less promising) is to develop by ""brute force"" the term $\frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)$ to obtain an expression for it which should be formally proved by induction on $k$ .","['normed-spaces', 'real-analysis', 'partial-derivative', 'induction', 'derivatives']"
4917956,Measure of projection of a set is zero,"Suppose $S$ is measurable in $[0,1]^2$ and the orthogonal projection of $S$ onto the $x$ and $y$ axes has measure zero. Can the measure of the orthogonal projection of $S$ onto the line $y=x$ be positive? Each of these measures is the 1 dimensional Lebesgue measure. If we require just the $x$ axis projection to be zero, the answer is obviously ""no"" -- we can take the entire $y$ axis. The projection has measure $1/2$ .",['measure-theory']
4917993,Proving that a group is infinite and nonabelian,"As an exercise I am trying to prove that the group $$G = \langle a,b,c \mid ac = ba, ab=ca,  bc=ab\rangle$$ is infinite and non-abelian. Moreover, the author claims that its center has finite index. I have tried to find a group which is (a subgroup of) this abstract group, but to no advance. I have seen similar posts on the forum, but their answers were rather specific to the considered group, and did not provide a general approach.","['group-presentation', 'infinite-groups', 'combinatorial-group-theory', 'abstract-algebra', 'group-theory']"
4918054,Limit with a geometric interpretation,"Let $f:ℝ \to ℝ$ be a $C^∞$ curve. Determine the following limit; $$\lim_{x_1 \to x_2} \dfrac{ \int_{x_1}^{x_2} \sqrt{1+f'(x)^2} dx}{\sqrt{(x_2-x_1)^2+(f(x_2)-f(x_1))^2}}$$ My attempt: I recognized that the numerator represents the arc length of the curve from $P(x_1,f(x_1)$ to $Q(x_2,f(x_2))$ whereas the denominator is the length of the chord $PQ$ . Intuitively, it seems as if when $Q$ tends to $P$ , the numerator and denominator will become nearly equal so I think the limit should tend to $1$ . I quickly verified this in case of a circle and it seems to work. However, I do not know how to prove this for any general function; applying L'hopital's rule once (after letting $x_2=x_1+h$ ) just complicates the limit further although it gets rid of the integral. And, I was looking forward to using this observation about arc length and chord length in my answer but I couldn't.
Is there perhaps a better way of doing this using the squeeze theorem? I would appreciate any method which does not use L'hopital's rule and is not complicated computationally. Is my conjecture even correct?","['limits-without-lhopital', 'limits', 'arc-length', 'definite-integrals']"
4918075,Arranging Letters in a string,"Given AACBBBDD. Find the number of ways to arrange these letters in a row such that all B are separated from each other and all D are separated from each other. Note: B and D can come together. But B should be separated from other B and D should be separated from other D. The problem I'm facing: If we solve it by GAP METHOD. First of all arrange the letters on which there is no constraints. So arranging AAC = 3!/(2!*1!) Now there are 4 gaps created. In these 4 gaps we need to fill 3 B and 2 D. Now there can be three ways to fill B,D. If we first fill 3 B then after filling there will be total 7 gaps. 4(old)+(3 new gaps created by placing B). now in these 7 gaps we need to fill 2 D. But if we fill 2 D first then after filling there will be total 6 gaps. 4(old)+(2 new gaps created by placing D). now in these 6 gaps we need to fill 3 B. And if we consider filling all 3 B AND 2 D together then there are only 4 gaps so cant do this.","['permutations', 'combinations', 'combinatorics']"
4918118,Convergence of Step Functions Generated by Uniformly Distributed Random Points,"I have encountered a surprisingly complicated problem to solve and I'm looking for some help. It could be difficult because I don't have a background in probability and so don't know the appropriate terms to search, or it is a genuinely difficult thing to prove. Preamble: Let $f:[0,1] \to \mathbb{R}$ be a continuous function. Then, for $n\geq 1$ we consider evenly spaced points $0 \leq x_1 < x_2 < \dots < x_n \leq 1$ and a partition $\mathcal{P}_n$ of the interval $[0,1]$ into exactly $n$ disjoint intervals so that each $x_i$ belongs to exactly one of these partition intervals. Using these points and the partition we can construct a step function approximation of $f$ , denoted $f_n$ , by assigning $f_n(x) = f(x_k)$ for all $x$ in the $k$ th partition interval. Then, a typical exercise one might encounter in real analysis is to prove that $\lim_{n\to \infty} \|f_n - f\|_\infty \to 0$ , which follows form the uniform continuity of $f$ . My Problem: Now, suppose that for $n\geq 1$ we have $u_1,\dots,u_n$ drawn independently form the uniform distribution on $[0,1]$ and we set the $x_1,\dots,x_n$ to be the corresponding order statistics. My question is: if we construct the step functions $f_n$ as above, can we still get uniform convergence to the original function $f$ with high probability? What I have so Far: A previous question on here has provided that if the $x_1,\dots,x_n$ are the order statistics of the i.i.d. points $u_1,\dots,u_n$ we have that $$\mathbb{P}\bigg(\max_{1 \leq k \leq n} |x_{k+1} - x_{k}| \leq \delta \bigg) = \sum_{k = 0}^{\lfloor 1/\delta\rfloor} (-1)^k{n+1\choose k}(1 - k\delta)^n $$ where $x_0 = 0$ , $x_{n+1} = 1$ , and for all $\delta \in [1/(n+1),1]$ . Of course, once one can argue that with high probability the points become close together, one can again use uniform continuity of $f$ on $[0,1]$ to get the desired result. However, I'm unable to get anything asymptotic out of the above formula. I've run simulations with $\delta = 1/\sqrt{n}$ , for example, and the above probability quickly converges to 1 as $n \to \infty$ , but again, I can't prove it. Question: It seemed to me that my problem would have been answered at some point in the mathematical literature, but I can't find any reference. Therefore, could someone either point me to a reference or help me out with a proof here? Thank you!","['statistics', 'real-analysis', 'order-statistics', 'uniform-convergence', 'probability']"
4918129,Examples of continuous functions that are monotone along all lines,"I am looking for different examples (or even a complete characterization if this is possible) of continuous functions that are monotone along all lines. By that I mean functions $f\colon X\to\mathbb{R}$ such that for all $x,y\in X$ the function $g_{x,y}:[0,1]\to\mathbb{R}$ , $\alpha\mapsto f(\alpha x + (1-\alpha)y)$ is monotone. $X\subset\mathbb{R}^n$ should be convex so that everything is well defined. If $n=1$ and $X$ is an interval then obviously these are just the the usual monotone functions. Also for general $n$ , every affine function is monoton allong all lines since $f(\alpha x + (1-\alpha)y)=\alpha f(x) + (1-\alpha) f(y)$ . What other examples of functions are there satisfying this property? Note that there are easy examples of functions $\big(\text{ like }f(x)=\Vert x\Vert^2\big)$ such that if you fix a certain $x$ (here $x=0$ ), then $g_{x,y}$ is monotone for all $y$ . However $f$ is still not monotone allong all lines. Also this choice of $f$ shows that there seems to be no easy criterion, like looking at partial derivatives for example.","['examples-counterexamples', 'analysis', 'real-analysis', 'functions', 'convex-analysis']"
4918140,"If $E \in \mathcal{A}$ satisfies $\mu(E)>0$, then there exists $F \subset E, F \in \mathcal{A}$ with $0 < \mu(F) < \infty$","Suppose $(X, \mathcal{A}, \mu)$ is a $\sigma$ -finite measure space. If $E \in \mathcal{A}$ satisfies $\mu(E)>0$ , then there exists $F \subset E, F \in \mathcal{A}$ with $0 < \mu(F) < \infty$ We are given very minimal information to work with so I suspect the result must be pretty straightforward. As $E \in \mathcal{A}$ , then $E$ a countable union, intersection, and/or complement of other sets in $\mathcal{A}$ . Given that the measure $\mu$ preserves measures of unions of disjoint sets, perhaps it would be most intuitive to partition $E$ in the following way: $$E= \bigcup_{i=1}^\infty E_i \hspace{1.3cm} E_j \cap E_k = \varnothing, \hspace{0.2cm} j \neq k$$ where each $E_i \in \mathcal{A}$ . The cheap argument would be to pick some $E_i$ out of this collection which has positive measure (but what if $E_i$ has measure zero?). Then instead it may be better to pick a subcollection $\{E_{i_k}\}_{k=1}^\infty$ from $\{E_i\}_{i=1}^\infty$ so that $\cup_k E_{i_k}$ has positive measure $-$ but we do not know which sets to select. In particular, if $\mu(E) = \infty$ , then picking the wrong $E_i$ in the collection would result in another subset with infinite measure. Is partitioning $E$ in the manner as above the best way to  approach this or is there a better way of looking at it? NOTICE After reading the comments and asnwers, I'm seeing that my notation is mildly confusion. My use of the symbol "" $\subset$ "" is a mere subsethood; it makes no distinction between proper and non-proper inclusion. Hence, $A \subset B \iff \forall \, x \in A, x \in B$",['measure-theory']
4918184,"Alternative proof $g(u)=6 + 5 \sin u + \sin(2 u)- \cos u - \cos(2 u) \ge 0$ for $u\in\left[-\frac \pi 2, \frac \pi 2\right]$","The given inequality $$g(u)=6 + 5 \sin u + \sin(2 u)- \cos u - \cos(2 u)  \ge 0$$ for $u\in\left[-\frac \pi 2, \frac \pi 2\right]$ , comes out from an answer given to this other recent question . The solution given there makes use of elementary analysis tools, mainly derivatives, and some numerical check. Can we find an alternative and more effective or elegant proof?","['inequality', 'real-analysis', 'alternative-proof', 'calculus', 'trigonometry']"
4918254,"Analysing a Lebesgue integral inequality for $|t^{-n} \phi(x/t)|$, where $\phi \in C_c^\infty \cap L^1$ with $\| \phi \|_1 = 1$.","Context. Let $C^k(\mathbb R^n)$ denote the space of functions defined on $\mathbb R^n$ that are $k$ times continuously differentiable, where $k \geqslant 1$ is an integer. As usual, define $C^\infty(\mathbb R^n)$ to be the intersection of all $C^k(\mathbb R^n)$ and let $C_c(\mathbb R^n)$ denote the space of continuous functions on $\mathbb R^n$ with compact support. Furthermore, let $C_c^\infty(\mathbb R^n) = C_c(\mathbb R^n) \cap C^\infty(\mathbb R^n).$ Consider the usual Euclidean space $\mathbb R^n$ equipped with the Lebesgue measure. Furthermore, let $\phi \in C_c^\infty(\mathbb R^n)$ be a kernel with unitary norm, that is, $\| \phi \|_1 = 1$ (here, $\| \phi \|_1$ stands for the $L^1$ -norm of $\phi$ ) and define the dillations $$ \phi_t(x) = t^{-n} \phi\left( \frac{x}{t} \right), $$ for every $x \in \mathbb R^n$ and $t > 0$ . Finally, let $\delta > 0$ be an arbitrary fixed constant. In an article I am reading, the authors claim the following. Now, we use the very well-known inequality $$ \int_{|z| \geqslant \delta}|\phi_t(z)| \, dz \leqslant c \, \int_{|z| \geqslant \delta} \frac{t}{|z|^{n+1}} \, dz \leqslant c \, t, $$ where the constant $c$ is independet of $t$ . Basically, I am wondering where this inequality comes from and if anyone has a reference that approaches this result. For what it's worth, under these conditions I am already aware that $\phi_t \in C_c^\infty(\mathbb R^n)$ with $\| \phi_t \|_1 = 1$ (for details about this, see this post ). Thanks for any help in advance.","['lebesgue-integral', 'reference-request', 'real-analysis', 'functions', 'functional-analysis']"
4918260,Bounds on the Kolmogorov distance,"Let $X, Y, Z$ be random variables with distribution functions $F_X, F_Y, F_Z$ .
The Kolmogorov distance is defined as $$d_K(X, Z) = \sup_{t\in\mathbb{R}} |F_X(t)-F_Z(t)|.$$ Assume that $Z$ has a bounded p.d.f. $f_Z$ and that $Y$ is a centered r.v. with finite second moment. I am looking for bounds of the form $$ d_K(X+Y, Z) \leq d_K(X,Z) + r(Y,Z), $$ where $r(Y,Z)$ depends on $Y$ and $Z$ in some way. What I have so far So far, I have the following: For any $\varepsilon>0$ it holds that $$ d_K(X+Y, Z) \leq d_K(X,Z) + \varepsilon \lVert f_Z\rVert_\infty + P(|Y|>\varepsilon). \qquad(1)$$ (Proof sketch below)
Using the Markov inequality to bound $P(|Y|>\varepsilon) \leq E[Y^2]/\varepsilon^2$ and taking $\varepsilon=E[Y^2]^\frac{1}{3}$ then gives $$ d_K(X+Y, Z) \leq d_K(X,Z) + (\lVert f_Z\rVert_\infty + 1) E[Y^2]^\frac{1}{3}. $$ When $X$ and $Y$ are independent, there is a better bound (Lemma 3 in this paper ): $$ d_K(X+Y, Z) \leq d_K(X,Z) + \lVert f_Z\rVert_\infty \cdot E|Y|. $$ By Jensen's inequality $E|Y| \leq E[Y^2]^\frac{1}{2}$ , so we get the exponent 1/2, which is better than the one of 1/3 above, at least when $E[Y^2]<1$ . Questions My main question is the following:
Can I do any better than above (exponent 1/3) when $X$ and $Y$ are not independent? Even if you don't have an answer to this exact problem, I appreciate any ideas/references on how to bound $d_K(X+Y, Z)$ for 'small $Y$ '. If you have any (preferably citeable) reference for (1), I would much appreciate it. I will include a sketch of my proof of (1), please tell me if my reasoning is unsound. Proof of (1) For $\varepsilon>0, t\in\mathbb{R}$ , it holds that $$\begin{aligned}
        P(X+Y \leq t)
            &= P(X+Y \leq t, |Y|\leq \varepsilon) + P(X+Y\leq t, |Y|>\varepsilon)\\
            &\leq P(X - \varepsilon \leq t) + P(|Y|>\varepsilon)\\
            &= F_X(t+\varepsilon) + P(|Y| > \varepsilon),
\end{aligned}$$ hence $$\begin{aligned}
        P(X+Y\leq t) - P(Z\leq t)
            &\leq F_X(t+\varepsilon) - F_Z(t) + P(|Y| > \varepsilon)\\
            &\leq |F_X(t+\varepsilon) - F_Z(t+\varepsilon)| + |F_Z(t+\varepsilon) - F_Z(t)| + P(|Y| > \varepsilon)\\
            &\leq d_K(X, Z) + \varepsilon \lVert f_Z\rVert_\infty + P(|Y| > \varepsilon).
\end{aligned}$$ The lower bound follows similarly.","['probability-distributions', 'probability-theory', 'probability']"
4918287,Reference for unknown notion related to continuity,"I was studying general topology when a question came to my mind. Let's consider a continuous function f between two topological spaces (for sake of conveniency, I'll indicate just their topologies) T (domain) and S (codomain). It is known, for example, that refining the topology on the domain or coarsening the one on the codomain preserves continuity. Let's call f precise between the topologies T and S if, for every topologies T' coarser than T and S' finer than S and such that f is continuous from T' to S' , it is true that T is coarser than T' and S is finer than S' (i.e. T = T' and S = S' ). I have a sense that this property can be useful to characterise final and initial topologies induced by a map and a topology, including identifications. Anyone has ever seen anything like that in a treatise on topology, or some analogous, or has any observation in merit? EDIT What is the appropriate setting to study the topologies that make a given function continuous? Is it a poset, or a lattice, or something more specific?","['general-topology', 'geometry', 'algebraic-topology']"
4918321,Can there be a multiplicative linear operator for real square matrices?,"Let me be more precise: Is there an $f: \mathbb R^{n \times n}\to \mathbb R$ such that for any square matrices $A$ and $B$ of the same order $n \times n$ we have: $f(A+B) = f(A) + f(B)$ $f(kA) = kf(A), k \in \mathbb R$ $f(AB) = f(A)f(B)$ Can there be such an $f$ ? It is clear that $f(0) = 0, f(-A) = -f(A), f(I) =1 $ (or $0$ ). Do we get some contradiction from this? All nilpotent matrices have $f=0$ EDIT:
A few interesting corollaries that one of your answers allowed me to arrive to: There is also no operator that satisfies only $1$ and $3$ to all square matrices but the trivial one $f \equiv 0$ . If an operator satisfies $1$ and $2$ , then it must be a random (but fixed) linear combination of the elements of the matrix, like the trace or just any random element of fixed coordinates.","['matrices', 'linear-algebra', 'linear-transformations']"
4918342,Proving that restriction of $\sigma$-algebra is a $\sigma$-algebra,"Let $X$ be a set and $\Sigma$ be a $\sigma$ -algebra of $X$ . Show that $\Sigma \cap A = \{E \cap A: E \in \Sigma \}$ is a $\sigma$ -algebra of $X$ My attempt actually showed that this is impossible: Suppose $F \in \Sigma \cap A$ and that $\Sigma \cap A$ is a $\sigma$ -algebra of $X$ , then $F = E \cap A$ for some $E \in \Sigma$ . Since $\Sigma \cap A$ is a $\sigma$ -algebra, we have that $F^c \in \Sigma \cap A$ , but that means $F^c=E^c \cup A^c=G \cap A$ , for some $G \in \Sigma$ . Thus: $A^c \subset E^c \cup A^c \subset G \cap A \subset A$ , which is impossible. Could you guys help me find my mistake? Thank you",['measure-theory']
4918375,"Proof: If $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$, then $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is locally $\mu$-null.","I am self-studying Measure Theory by Donald Cohn. When proving this statement: If $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ , then $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is locally $\mu$ -null. the book wrote: If $\{M_n\}$ is a nonincreasing sequence of real numbers such that $\|f\|_{\infty}=\lim_{n\to\infty}M_n$ and such that for each $n$ the set $\{x\in X:|f(x)|>M_n\}$ is locally $\mu$ -null, then the set $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is the union of the sets $\{x\in X:|f(x)|>M_n\}$ and so is locally $\mu$ -null. I can see that the union of a sequence of locally $\mu$ -null sets is locally $\mu$ -null. But my question is: How can we make sure that such a sequence $\mathbf{\{M_n\}}$ exists? Could someone please explain this for me? Thank you very much! Some related definitions: Definition $\quad$ A subset $B$ of $X$ is $\mu$ -negligible (or $\mu$ -null ) if there is a subset $A$ of $X$ such that $A\in\mathscr{A}$ , $B \subseteq A$ , and $\mu(A)=0$ . Definition $\quad$ Let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ be the set of all bounded real-valued $\mathscr{A}$ -measurable functions on $X$ , and let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{C})$ be the set of all bounded complex-valued $\mathscr{A}$ -measurable functions on $X$ . In discussions that are valid for both real- and complex-valued functions we will often use $\mathscr{L}^p(X,\mathscr{A},\mu)$ to represent either $\mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{R})$ or $\mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{C})$ . Definition $\quad$ A subset $N$ of $X$ is locally $\mu$ -null if for each set $A$ that belongs to $\mathscr{A}$ and satisfies $\mu(A)<+\infty$ the set $A\bigcap N$ is $\mu$ -null. A property of points of $X$ is said to hold \textit{locally almost everywhere} if the set of points at which it fails to hold is locally null. Definition $\quad$ We can define $\|\cdot\|_p$ in the case where $p=+\infty$ by letting $\|f\|_{\infty}$ be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)|>M\}$ is locally $\mu$ -null. An Update Thanks for @kobe's answer! I would like to add one more thing for my own record: that the set $A$ is nonempty, where $A=\Big\{M\in\mathbb{R}_+: \{x\in X:|f(x)|>M\}\ \text{is locally $\mu$-null}\Big\}$ . This is because of the fact that $f\in\mathscr{L}^{\infty}$ , which means $f$ is bounded, which implies there is a real number $M$ such that $|f(x)|\leq M$ for all $x$ , which means the set $\{x\in X:|f(x)|>M\}=\emptyset$ and thus is $\mu$ -null and so is locally $\mu$ -null.","['proof-explanation', 'measure-theory', 'analysis', 'real-analysis']"
4918383,"Point $D$ inside $\triangle{ABC}$ such that $\angle{ABD}=\angle{CBD}=6^{\circ}$,$\angle{BCD}=12^{\circ}$,$\angle{ACD}=18^{\circ}$, $\angle{BAD}=?$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question Point $D$ is in $\triangle{ABC}$ such that $\angle{ABD}=\angle{CBD}=6^{\circ}$ , $\angle{BCD}=12^{\circ}$ , $\angle{ACD}=18^{\circ}$ , $\color{red}{\angle{BAD}=?}$ Can we find a pure elementary geometric approach to answer this question?","['euclidean-geometry', 'puzzle', 'geometry']"
4918436,Why thinking about generating function algebraically doesn't care about convergence.,"In mcs.pdf 16.1.1, it says: $G(x)-xG(x)=1$ Solving for $G(x)$ gives $$
\frac{1}{1-x}=G(x)::=\sum_{n=0}^{\infty}x^n\tag{16.3}
$$ ... But in the context of generat-
ing functions, we regard infinite series as formal algebraic objects . Equations such
as (16.3) and (16.5) define symbolic identities that hold for purely algebraic rea-
sons. It is done by canceling terms. Since that is one infinite series , we can leave only 1 after canceling forever . But why do we not need to care about convergence and say that the equation holds? Is there one better  explanation for the reasons behind? The book says later in 16.1.1 (Sorry for not continuing reading and asking one question instead here): We’ll explain this further in
Section 16.5 at the end of this chapter, but for now, take it on faith that you don’t
need to worry about convergence. After reading chapter 16.5, I thought I understood this question. The key part is It simply means that $G(x)$ really refers to its infinite sequence of coefficients $(g_0, g_1, \ldots)$ in the ring of formal power series. In other words,
the powers of the variable x just serve as a place holders —and as reminders of the
definition of convolution. The above means same as what PrincessEev says ""we associate with sequences in a more intuitive way"", the 1st paragraph of Theo Bendit's answer and wikipedia (See this lecture point 1 of ""Conventions and Notation"" for the similarity with ""power series"") power series can be viewed as a generalization of polynomials , where the number of terms is allowed to be infinite, with no requirements of convergence . Thus, the series may no longer represent a function of its variable , merely a formal sequence of coefficients The section 16.5 almost says same as Theo Bendit's answer. Although after reading these, why the definition is helpful seems to be not very obvious. Maybe it is better to rethink after learning abstract algebra and knowing the whole knowledge realm of that topic. Thanks for all. (This question is edited after the comments and the answer)","['discrete-mathematics', 'generating-functions']"
4918461,Classifying finite groups where order is multiplicative on elements with coprime orders,"It's well known that $|g_1 g_2| = |g_1||g_2|$ whenever $g_1$ and $g_2$ are commuting elements of a group with $\gcd(|g_1|, |g_2|) = 1$ . So, for example, $\gcd(|g_1|, |g_2|) = 1$ always implies that $|g_1 g_2| = |g_1||g_2|$ in an abelian group. I am interested in classifying finite groups where the above condition holds for every pair of elements when we drop the hypothesis that they must commute. More precisely, my question is the following. Question: Can we classify all finite groups $G$ in which $|g_1 g_2| = |g_1| |g_2|$ for every $g_1, g_2$ in $G$ with $\gcd(|g_1|, |g_2|) = 1$ ? If so, how? Of course, by ""classify"" I mean up to isomorphism. I have managed to prove that every finite nilpotent group works by using the fact that every finite nilpotent group is a product of $p$ -groups and by proving that the condition holds for all such products. I've included my proof of this at the end of the post. I can't come up with any other examples of groups that work. Since nilpotent groups are ""close to abelian,"" I wouldn't be surprised if these are the only ones. We could try to use the fact that a finite group is nilpotent if and only if two elements with coprime orders commute . This seems more plausible since this condition seems similar to the one here, but I don't really see how to do it. Am I on the right track here? I imagine we might be able to proceed a bit more directly using Sylow's theorems, but I'm not sure how. If we can prove that every Sylow subgroup of $G$ is normal, then it will follow that $G$ is nilpotent . (To be honest, I really don't know how to proceed. Any feedback or nudges would be very appreciated.) Here's my proof that finite products of finite $p$ -groups work. Let $G = G_1 \times \cdots \times G_n$ , where each $G_i$ is a finite group of order $p_i^{n_i}$ for some prime $p_i$ and some positive integer $n_i$ . Assume that the primes $p_1, \dots, p_n$ are distinct. Let's take two elements $g = (g_1, \dots, g_n)$ and $h = (h_1, \dots, h_n)$ of $G$ and assume that their orders are coprime. Each $g_i$ and $h_i$ is an element of a group of order $p_i^{n_i}$ , so they have orders $p_i^{a_i}$ and $p_i^{b_i}$ , respectively, for some non-negative integers $a_i,b_i$ . The order of $g$ is $$
|g| = \operatorname{lcm}(|g_1|, \dots, |g_n|) = \operatorname{lcm}(p_1^{a_1},\dots,p_n^{a_n}) = p_1^{a_1} \cdots p_n^{a_n},
$$ where the last equality uses the fact that the primes are distinct. It follows that $|g|$ and $|h|$ are coprime if and only if, for each $i$ , one of $a_i$ or $b_i$ is zero. Let's reorder the indices so that $a_1 = \cdots = a_j = b_{j+1} = \cdots = b_n = 0$ . Then $g_1, \dots, g_j, h_{j+1}, \dots, h_n$ are the identity elements of their respective groups, and it follows that $$
gh = (h_1,\dots,h_j,g_{j+1},\dots,g_n).
$$ This implies that $$
|gh| = \operatorname{lcm}(p_1^{b_1}, \dots, p_j^{b_j}, p_{j+1}^{a_{j+1}}, \dots, p_n^{a_n}) = p_1^{b_1} \cdots p_j^{b_j} \cdot p_{j+1}^{a_{j+1}} \cdots p_n^{a_n} = |g| |h|,
$$ and we are done.","['nilpotent-groups', 'group-theory', 'sylow-theory', 'finite-groups']"
4918485,The set of all ellipsoids $\mathcal{E}(A)$ contained in a bounded open set $A$ is compact,"We call $A\subset\Bbb R^d$ a convex body if $A$ is a convex, non-empty, open, and bounded. The open unit ball in $\Bbb R^d$ is denoted by $B_d$ . In Tao-Vu's book, they say: Define an ellipsoid to be any set $E$ of the form $E = L(B_d) + x_0$ , where $B_d$ is the unit ball, $x_0 ∈ \Bbb R^d$ , and $L$ is a (possibly degenerate) linear transformation in $\Bbb R^d$ ; we allow the ellipsoid to be degenerate for compactness reasons. Since $A$ is open and bounded, it is easy to see that the set of all ellipsoids $E$ contained in $A$ is a compact set (with respect to the usual topology on $L$ and $x_0$ .) It seems the convexity assumption on $A$ is not required to show the compactness of the set of ellipsoids. The set can be written explicitly as $$\mathcal E(A) = \{E = L(B_d) + x_0: L:\Bbb R^d \to \Bbb R^d \text{ linear, }x_0 \in \Bbb R^d, E \subset A\}.$$ In particular, we can identify every ellipsoid $L(B_d) + x_0$ with a tuple $(L,x_0)$ , so $$\mathcal E(A) \subset M_d(\Bbb R) \times \Bbb R^d,$$ and we want to show that $\mathcal E(A)$ is compact with respect to the topology on $M_d(\Bbb R) \times \Bbb R^d$ . How should this argument go? I tried a sequential argument, with ellipsoids $\{(L_n,x_n)\}_{n=1}^\infty$ contained in $A$ , but I'm not sure how to use the compactness of $\overline{A}$ to extract a convergent subsequence. Thank you!","['ellipsoids', 'analysis', 'real-analysis', 'general-topology', 'compactness']"
4918489,Shapes with simple distance functions.,"Given a set $A$ in $\mathbb{R}^2$ , the distance function (DF) of $A$ is defined as $$
\delta_A(\mathbf{x}) = \inf\{\|\mathbf{x}-\mathbf{y}\|: \mathbf{y} \in A \}
$$ Some sets $A$ have a nice tidy closed-form DF. Here “ tidy ” means that the DF can be written as a formula involving a combination of elementary functions (like polynomials, roots, logs, trig functions and their inverses, etc.), or it can be computed in a program without resorting to iterative numerical methods. For reasons of my own, I want to exclude “min” and “max” from the list of allowed elementary functions. Examples: if $A$ is a circular disk or an infinite straight line, it obviously has a tidy DF. I was amazed to find out the other day that the same is true of the involute of a circle. Conversely, the DF of a (cubic) Bézier curve is not at all tidy. My question: what other shapes have tidy DFs? For bonus points: same question, but in $\mathbb{R}^3$ . Obviously spheres, cylinders, cones and tori have tidy DFs. What else? Why is this interesting? Because DFs are sometimes used to represent shapes in engineering & manufacturing, and computations are obviously easier if the DFs are tidy.","['analytic-geometry', 'geometry', 'metric-spaces']"
4918494,Evaluate $\lim\limits_{x\to\infty}x\!\left[2x\!-\!\left(x^3\!+\!x^2\!+\!x\right)^{\!\frac13}\!\!-\!\left(x^3\!-\!x^2\!+\!x\right)^{\!\frac13}\right]$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question Evaluate $\lim\limits_{x\to \infty}x\left[2x-\left(x^3+x^2+x\right)^{\frac{1}{3}}-\left(x^3-x^2+x\right)^{\frac{1}{3}}\right]$ My Approach: Formula I used $(1+x)^{n}=1+nx+\frac{n(n-1)}{2!}x^2+.....\infty$ where $x\in(-1,1)$ $\lim\limits_{x\to \infty}x\left[2x-x\left(1+\left(\frac{1}{x}+\frac{1}{x^2}\right)\right)^{\frac{1}{3}}-x\left(1+\left(\frac{-1}{x}+\frac{1}{x^2}\right)\right)^{\frac{1}{3}}\right]$ $\implies\lim\limits_{x\to \infty}x\left[2x-x\left(1+\left(\frac{1}{3x}+\frac{1}{3x^2}\right)\right)-x\left(1+\left(\frac{-1}{3x}+\frac{1}{3x^2}\right)\right)\right]$ $\implies\lim\limits_{x\to\infty}x\left(-\frac{2}{3x}\right)=-\frac{2}{3}$ But given answer is $\frac{2}{9}$ . I am attaching given solution below. Also I am attaching my two more solutions in Image formart.","['indeterminate-forms', 'limits', 'calculus']"
4918505,When is $ \sum_{k \in \mathbb{Z}}\left(\frac{\sin(k)}{k}\right)^{n}=2 \int_{0}^{\infty}\left(\frac{\sin(x)}{x}\right)^{n}dx$?,"Define sequences $$a_n = \sum_{k \in \mathbb{Z}}\left(\dfrac{\sin(k)}{k}\right)^{n},  b_n = \int_{0}^{\infty}\left(\dfrac{\sin(x)}{x}\right)^{n}dx, \quad n \in \mathbb{N}.$$ I am trying to see if there is a relation between these two sequences by studying sequence $c_n = a_n-2b_n$ . From [1 , 2 , 3 , 4 , 5] , sequence $b_n$ is completely understood, as $$ b_n=\frac{\pi}{2^n (n-1)!}  
    \sum_{k=0}^{\lfloor n/2\rfloor} (-1)^k {n \choose k} (n-2k)^{n-1}$$ and the first few values are $$ b_1 = b_2 = \frac{\pi}{2}, b_3=\frac{3\pi}{8}, b_4 = \frac{\pi}{3}, b_5 = \frac{115\pi}{384}, b_6 = \frac{11\pi}{40}.$$ Now coming to the other sequence, [6 , 7 ], $a_n$ is also completely known, $$ a_n= \frac{(-1)^{n}\pi}{2^{n}(n-1)!}\sum_{\ell = -\lfloor n/(2\pi)\rfloor}^{\lfloor n/(2\pi)\rfloor}\left(\sum_{k = 0}^n(-1)^k{n\choose k} (2\pi \ell - n+2k)^{n-1}\operatorname{sign}(2\pi \ell-n+2k)\right).$$ First few values of the sequence are $$ a_1 = a_2 = \pi, a_3=\frac{3\pi}{4}, a_4 = \frac{2\pi}{3}, a_5 = \frac{115\pi}{192}, a_6 = \frac{11\pi}{20}.$$ This implies $c_{n}=0$ for $n=1,2,3,4,5,6$ . However $c_7 \approx 0.0000185$ , which seemed out of place from the pattern we observed before. why is the case $n=7$ special, as in why is this the first natural number where the pattern differed? Does the sequence $c_n$ converge? (If it does then behavior of $a_n$ is completely understood by $b_n$ ) Addendum : Thanks to @Dave, this paper (pg 8, example 3) clearly explains why $n=7$ is the first integer when $c_n \ne 0$ . Moreover, the last line of the example states that these two quantities are quite different (possibly suggesting $c_n \ne 0$ for $n \ge 7$ ). The authors however don't delve into analysis of sequence $c_n$ .","['integration', 'sequences-and-series', 'combinatorics', 'real-analysis']"
4918506,A remarkable fact about the unit circle; looking for a shape with an even more remarkable fact.,"You may have heard of the following remarkable fact about the unit circle: If $n$ equally spaced points are drawn on a unit circle, and line segments are drawn from one of the points to each of the other points, then the product of the lengths of these $n-1$ line segments, equals $n$ ( proof ). This made me wonder, does there exist a shape such that the same kind of product is not equal to $n$ , but rather a constant? That is: Does there exist a two dimensional shape (not necessarily convex) and a fixed point on its perimeter, such that, if $n$ equally spaced (in terms of distance along the perimeter) points, one of which is the fixed point, are drawn on the perimeter, and line segments are drawn from the fixed point to each of the other points, then the product $P(n)$ of the lengths of these $n-1$ line segments, is constant for all $n$ ? Ellipse? Suppose that the shape in question could be the ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ , where $a$ and $b$ are positive, and the fixed point is on an axis of the ellipse, say $Q(0,-b)$ . We have $P(2)=P(4)\implies 2b=2b(a^2+b^2)\implies a^2=1-b^2$ . So the equation of the ellipse is $\frac{x^2}{1-b^2}+\frac{y^2}{b^2}=1$ , or $y=\pm b\sqrt{1-\frac{x^2}{1-b^2}}$ . For $n=8$ , suppose the point $A$ on the ellipse in the first quadrant, and the point $B$ on the ellipse in the fourth quadrant, have $x$ -coordinate $s$ . We have $P(8)=P(4)\implies AQ^2BQ^2=1$ . That is, $$\left(s^2+b^2\left(\sqrt{1-\frac{s^2}{1-b^2}}+1\right)^2\right)\left(s^2+b^2\left(\sqrt{1-\frac{s^2}{1-b^2}}-1\right)^2\right)=1$$ Here is the graph of $b$ against $s$ . According to the graph, $b\ge 1$ , so $a^2=1-b^2\le 0$ , so there is no ellipse. Sine curves? I also considered the graph $y=\pm a\sin\left(\frac{x}{a}\right), 0\le x\le a\pi$ , with fixed point $(0,0)$ . Numerical investigation suggests that there exists a constant $C\approx 0.7571$ such that, as $n\to\infty$ , If $a<C$ then $P(n)\to 0$ . If $a=C$ then $\dfrac{P(n)}{n}\to L\approx1.07$ . If $a>C$ then $P(n)\to\infty$ . So this kind of sine graph doesn't work. Possibly related question I asked the following question on MO: ""Does there exist a continuous function $f(x)$ such that $f(0)=0$ and $0<\lim\limits_{n\to\infty}\prod\limits_{k=1}^n f\left(\frac{k}{n}\right)<\infty$ ?"" The answer was yes. However, my question now is not about limits. Also, the answer in the MO question involved spikes in the graph, which I don't think is possible in a graph of line segment length against distance along perimeter from the fixed point.","['arc-length', 'circles', 'geometry', 'products']"
4918559,Smallest eigenvalue of matrix with random elements (non-central Wishart),"Suppose that $X \in \mathbb R^{d \times n}$ is a random matrix with independent entries, each of which follows the standard normal law $\mathcal N(0, 1)$ , and that $M \in \mathbb R^{d \times n}$ is a given non-random matrix. Consider the random matrices $$
A = XX^\top \in \mathbb R^{d\times d}, \qquad B = (X + M)(X + M)^\top \in \mathbb R^{d \times d}.
$$ Intuitively, it seems to me that the following statement should hold true irrespectively of $d$ , $n$ and $M$ : $$
\tag{1}
\forall r \geq 0, \qquad
\mathbb P\Big[\lambda_{\min}(A) \geq r \Bigl] ≤ \mathbb P\Big[\lambda_{\min}(B) \geq r \Bigl].
$$ This appears to be a simple statement, but I have not found it in the literature on non-central Wishart distributions. Does (1) follow from known results, or is there a simple approach to prove or disprove it? Case $d = 1$ . The statement is true in this case. Indeed, for each $i \in \{1, \dotsc, n\}$ let $(X_i, Y_i)$ denote a coupling  (with couplings for different values of $i$ independent of each other) such that $$
X_i \sim \mathcal N(0, 1), \qquad 
Y_i \sim \mathcal N(m_i, 1), \qquad
$$ and additionally $$
X_i^2 \leq Y_i^2 \qquad \text{almost surely}.
$$ Showing that such a coupling exists is not difficult.
Then $Y$ is equal in law to $X + M$ and $$
XX^\top = \sum_{i=1}^{n} X_i^2 \leq \sum_{i=1}^{n} Y_i^2 = YY^\top \qquad \text{almost surely.}
$$ The conclusion then follows easily.","['random-matrices', 'statistics', 'probability']"
4918564,Is it possible to integrate $\int \frac{x^2 - 2x + 3}{x^4 - x^3 + x^2 - x + 1} \ dx$ using methods typical of undergraduate calculus?,"I was doing a calculus problem and got to this final integral: $$\int\limits_{0}^{1} \frac{x^2 - 2x + 3}{x^4 - x^3 + x^2 - x + 1} \ dx.$$ Calculators are supposed to be used for these problems, so I just input the final integral into my calculator and got the result. However, I wondered if there is a method to integrate it manually. For simplicity, let's remove the limits and focus on the indefinite form. The nominator suggests a $(x-1)$ dummy variable, so I tried rewriting the integrand in terms of $u = (x-1)$ and got: $$\int \frac{u^2 + 2}{u^4 + 3u^3 + 4u^2 + 2u + 1} \ du$$ This still doesn't seem to lead to any well-known forms that I'm aware of. My question is, is it possible to integrate this using knowledge of undergraduate calculus, and if yes, I'd like to hear some directions to tackle it. Thank you!","['integration', 'indefinite-integrals', 'calculus', 'rational-functions']"
4918639,Fubini-Study metric/form,"I'm looking to find a way to derive the Fubini-Study metric on $\Bbb CP^n$ and the corresponding Kähler form, but I cannot find a proper derivation for this. In most references I found they just state that the Kähler form is defined either as $$\omega_{FS} = i\partial\bar{\partial}\log \|s\|^2$$ for a section $s$ or something like $$
\omega_{FS} ={\frac {i}{2}}\partial {\bar {\partial }}\log |\mathbf {Z} |^{2}
$$ as wikipedia does. How are these things derived they seem very mysterious?","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
4918644,A coin game - is it a group?,"This is an exercise in the book ""A book of abstract algebra"" by Pinter, 3E. Let's devise a coin game. Imagine there are two coins, each can be placed in either location $A$ or $B$ . Moreover, each coin can be flipped. Define 8 moves: $M_1$ - flip over the coin at $A$ $M_2$ - flip over the coin at $B$ $M_3$ - flip over both coins $M_4$ - switch the coins $M_5$ - flip the coin at A, then switch $M_6$ - flip the coin at B, then switch $M_7$ - flip both coins, then switch $I$ - do not change anything Example: $M_4 * M_1 = M_2 * M_4 = M_6$ ""switch"" means ""change places"" I will spare you from what the exercise is asking to do. But I have noticed something playing with the problem. One of the aspects of the definitions of an operation implies that if $*$ is an operation, then $a*b$ is unambiguously and uniquely defined. An operation on the above set $G = \{ M_1, M_2, M_3, M_4, M_5, M_6, M_7, I \}$ has been defined as ""performing any two moves in succession"". Imagine now that we have done: $M_1 * M_2$ , well then the result of this operation can be expressed with two elements in the set, namely: $M_1 * M_2 = M_3 = M_7$ and so this creates ambiguity as to what this specific operation (""performing any two moves in succession"") assigns two members of the set ( $M_1, M_2$ ) to. As such, can we even consider $\langle G, *\rangle$ a group (I obviously must be missing / misunderstanding something)?","['group-theory', 'abstract-algebra']"
4918704,Can a function $f$ exist such that $f(x)f(y) = xy + 1$ for positive real $x$ and $y$,"Problem. Can a function $f$ exist such that $f(x)f(y) = xy + 1$ for positive real $x$ and $y$ If not, is there something like a next best thing, perhaps only for integers, finite field, or maybe only for most values rather than all. If something like this can exist it would be huge to get an explicit formula, as it would make computing something much simpler. I sincerely apologize if this is actually trivial or if it's an ill formed question. But I honestly can't think of anything.","['functional-equations', 'functions']"
4918734,"Finding $\int_\Gamma \frac{z f'(z)}{f(z)} \, dz$ over a given contour [duplicate]","This question already has answers here : Using argument principle to compute an integral (1 answer) Calculate $\int_\Gamma \frac{f'(z)z}{f(z)}\, \operatorname dz$ (2 answers) Closed last month . Let $f(z)=z^4-2z^3+2z^2-3z+60$ and let $\Gamma$ be the circle $|z|=5$ . I want to find $$\int_\Gamma \frac{z f'(z)}{f(z)} \, dz$$ Supposing we had $f'(z)$ in the numerator instead of $z f'(z)$ , this would very clearly be solved with the Argument Principle. Here, we cannot use it. So, we investigate using Rouche's theorem. Note that if we let $g(z) = z^4$ , then for $z$ on $\Gamma$ , we have $|f(z)-g(z)| < 2(5)^3+2(5)^2+3(5)+60 < 5^4 = |g(z)|$ and so since $g$ has four zeros interior to $\Gamma$ , then so does $f$ . Going back to the integral, we have \begin{align}
\underbrace{\int_\Gamma \frac{z f'(z)}{f(z)}}_{=I_5} \, dz = \int_\Gamma \frac{z (4z^3-6z^2+4z-3)}{z^4-2z^3+2z^2-3z+60} = \int_\Gamma \frac{4z^4-6z^3+4z^2-3z}{z^4-2z^3+2z^2-3z+60} \tag{1}
\end{align} Denote the above integral by $I_5$ $($ because $\Gamma$ is the curve $|z|=5)$ . Now, because $\Gamma$ already contains all the zeros of $f(z)$ , then if $I_R$ is the same integral as in $(1)$ except with $\Gamma$ being the curve $|z|=R$ , we must have $$|I_5| = |I_R| \hspace{1cm} \forall \, R \geq 5 \tag{2}$$ So, we get \begin{align}
|I_5| = |I_R| &= \left| \int_{|z|=R}\frac{z f'(z)}{f(z)} \, dz \right| \\
&\leq \int_{|z|=5} \left| \frac{z f'(z)}{f(z)} \, dz \right| \\
&= \int_{|z|=5} \frac{|z f'(z)|}{|f(z)|} \, |dz| \\ 
&= \frac{4R^4-6R^3+4R^2-3R}{R^4-2R^4+2R^2-3R+60} \cdot 2\pi R
\end{align} But now we have a problem because $|I_R| \to \infty$ as $R \to \infty$ . In fact, I suspect that the method I've contrived is problematic because it requires the degree of the numerator to be at least two less than the denominator if you want the integral to vanish if to be at least one less if you want the integral to converge to a nonzero magnitude. In our case, $\deg(z \cdot f'(z)) = \deg(f(z))$ and so the extra $|dz|$ is the final culprit in all this which makes the magnitude of the integral diverge. Perhaps there is a better way to approach this?","['complex-analysis', 'rouches-theorem']"
4918789,Where is the error in evaluating this series? $\sum_{n=1}^{\infty}\frac{1}{n(4n^2-1)^2}$,"Where is the error in evaluating this series? $$\sum_{n=1}^{\infty}\frac{1}{n(4n^2-1)^2}$$ We have $$
\frac{1}{n(4n^2-1)^2} 
= \frac{1}{n} - \frac{1}{2n-1} - \frac{1}{2n+1} 
+ \frac{1}{2(2n-1)^2} - \frac{1}{2(2n+1)^2}, 
$$ so \begin{align}
\sum_{n=1}^{\infty} \frac{1}{n(4n^2-1)^2} 
&= \sum_{n=1}^{\infty} \left({\frac{1}{2(2n-1)^2}-\frac{1}{2(2n+1)^2}}\right) 
+ \sum_{n=1}^{\infty} \left({\frac{1}{n} - \frac{1}{2n-1} - \frac{1}{2n+1}}\right) \\ 
&= \frac{1}{2} + \sum_{n=1}^{\infty} \int^1_0 (x^{n-1} - x^{2n-2} - x^{2n})\,dx \\ 
&= \frac{1}{2} + \int^1_0 \sum^{\infty}_{n=1} (x^{n-1} - x^{2n-2} - x^{2n})\,dx \\
&= \frac{1}{2} + \int^1_0 \left(\frac{1}{1-x} - \frac{1}{1-x^2} - \frac{x^2}{1-x^2}\right)\,dx \\ 
&= \frac{1}{2} + \int^1_0 \frac{x-x^2}{1-x^2}\,dx \\
&= \frac{1}{2} + \int^1_0 \frac{x}{1+x}\,dx \\
&= \frac{3}{2} - \ln(2).
\end{align} Therfore, $$
\sum_{n=1}^{\infty} \frac{1}{n(4n^2-1)^2} = \frac{3}{2} - \ln(2).
$$ But using wolfram alpha : $$
\sum_{n=1}^{\infty} \frac{1}{n(4n^2-1)^2} = \frac{3}{2} - 2\ln(2).
$$ Where is the error?","['power-series', 'calculus', 'sequences-and-series']"
4918838,Generalizing Putnam 2014/B5,"Consider the following natural generalization of the game in Putnam 2014/B5 . Some of these remarks have also been made by Kent Merryfield in an AoPS thread . The main question is in a blockquote below. Rules : Let $G$ be a fixed finite group. Alice and Bob take turns choosing an element of $G$ with Alice going first. At each turn, a player must choose an unchosen element of $G$ which commutes with all previously chosen elements. A player loses the game when they have no legal moves. For which finite groups $G$ does Alice have a winning strategy? We make the following observations about the game: Lemma : The elements chosen in a game must form a maximal abelian subgroup of $G$ . Proof : Consider the subgroup $H$ generated by the chosen elements. Since the generators mutually commute, the subgroup $H$ is abelian. Furthermore, every element of $H$ must have been chosen (otherwise, the game could have been continued with the unchosen elements of $H$ ). Finally, if $K$ is an abelian subgroup of $G$ with $H \leqslant K$ , then $H = K$ (otherwise, the game could have been continued with elements in $K - H$ ). Thus, $H$ is a maximal abelian subgroup of $G$ . Corollary : If $G$ has odd order, then Alice always wins. If $Z(G)$ has even order, then Bob always wins. In particular, if $G$ is nilpotent, then Alice/Bob win if $G$ has odd/even order. Proof : If $G$ has odd order, then by Lagrange's theorem, every maximal abelian subgroup has odd order. Similarly, if $Z(G)$ has even order, then every maximal abelian subgroup must contain $Z(G)$ and hence have even order. In the nilpotent case, $G$ has even order iff $Z(G)$ has even order. Proposition : Alice has a winning strategy iff there exists $g \in G$ such that the centralizer $C_G(g)$ has odd order. Proof : First, if such an element $g$ exists, then Alice can choose $g$ in the first move. The maximal abelian subgroup $H$ chosen at the end of the game must be a subgroup of $C_G(g)$ and must hence have odd order, meaning that Alice must always win after choosing $g$ . However, if no such $g$ exists, then for any first choice $h \in G$ , Bob can respond to Alice by choosing an element of even order in the group $C_G(h)$ by Cauchy's theorem. Consequently, the maximal abelian subgroup $H$ chosen at the end of the game must have even order by Lagrange's theorem, giving Bob the win. Question : Consider the following statements about a finite group $G$ : $1$ . The center $Z(G)$ has even order $2$ . Every maximal abelian subgroup of $G$ has even order $3$ . For all $g \in G$ , the centralizer $C_G(g)$ has even order We have $(1) \Rightarrow (2) \Rightarrow (3)$ . Do either/both of these
implications hold the other way? Examples : Here are a few examples for various finite groups $G$ : As mentioned before, if $G$ is nilpotent, then Alice wins iff $G$ has odd order. In particular, if $G$ is a $p$ -group for some prime $p$ , then Alice wins iff $p$ is an odd prime. If $G = D_{2n}$ , then Alice wins iff $n$ is odd. If $n$ is even, then $Z(G)$ has even order, whilst if $n$ is odd, then any non-trivial rotation has a centralizer of odd order. If $G = \mathrm{GL}_n(\mathbb{F}_q)$ (this is the original problem from the Putnam), then Alice has a winning strategy iff $q$ is even. Indeed, if $q$ is odd, then $Z(G)$ contains the involution $-I$ , while if $q$ is even, then we can find a transformation $T$ with centralizer of odd order (Give $\mathbb{F}_q^n$ the additional structure of $\mathbb{F}_{q^n}$ and let $T$ be multiplication by a generator of $\mathbb{F}^{\times}_{q^n}$ . Then, $C_G(T) \cong \mathbb{F}^{\times}_{q^n}$ has odd order). If $G = S_n$ for $n > 2$ , then Alice has a winning strategy. Let $m$ be the largest odd integer which is at most $n$ , then the conjugacy class of an $m$ -cycle has order $(m-1)!$ , so the centralizer has order $m$ , which is odd. Consequently, Alice can win for $G = A_n$ .","['group-theory', 'finite-groups']"
4918845,How to show $\lfloor\frac{n}{2}\rfloor\lfloor\frac{n-1}{2}\rfloor\lfloor\frac{m}{2}\rfloor\lfloor\frac{m-1}{2}\rfloor$ is equal to this expression?,"I am trying to show equality between the two expressions $$\frac {\lfloor{\frac n 2}\rfloor(\lfloor{\frac n 2}\rfloor - 1)\lfloor{\frac m 2}\rfloor(\lfloor{\frac m 2}\rfloor - 1)} 4 + \frac {\lfloor{\frac n 2}\rfloor(\lfloor{\frac n 2}\rfloor - 1)\lceil{\frac m 2}\rceil(\lceil{\frac m 2}\rceil - 1)} 4 + \frac {\lceil{\frac n 2}\rceil(\lceil{\frac n 2}\rceil - 1)\lfloor{\frac m 2}\rfloor(\lfloor{\frac m 2}\rfloor - 1)} 4 + \frac {\lceil{\frac n 2}\rceil(\lceil{\frac n 2}\rceil - 1)\lceil{\frac m 2}\rceil(\lceil{\frac m 2}\rceil - 1)} 4$$ and $$Z(m, n)=\lfloor\frac{n}{2}\rfloor\lfloor\frac{n-1}{2}\rfloor\lfloor\frac{m}{2}\rfloor\lfloor\frac{m-1}{2}\rfloor$$ for all $n,m \in \mathbb{N}$ .
The reason why is that I am studying Zarankiewicz's Conjecture , and the first expression I gave was my attempt to come up with an expression equal to the number of crossings of the complete bipartite graph $K_{n,m}$ when drawn according to Zarankiewicz's construction, and the second expression is the expression said to be the actual amount of crossings in such a drawing. I suspect my expression is correct as there is equality between these expressions for the values of $n,m$ which I tested. So how can I show equality? I see of course that the first expression is equal to $\frac {(\lfloor{\frac n 2}\rfloor(\lfloor{\frac n 2}\rfloor - 1) + \lceil{\frac n 2}\rceil(\lceil{\frac n 2}\rceil - 1))(\lfloor{\frac m 2}\rfloor(\lfloor{\frac m 2}\rfloor - 1) + \lceil{\frac m 2}\rceil(\lceil{\frac m 2}\rceil - 1))} 4$ , but got stuck trying to find further meaningful simplifications.","['algebra-precalculus', 'discrete-mathematics', 'ceiling-and-floor-functions']"
4918897,Why do we care about convergence of the Laplace transform?,"When I took elementary differential equations, with the textbook of Boyce & DiPrima, I learned about using the Laplace transform to solve some initial value problems. I also took a course in combinatorial analysis, where I learned to use generating functions to solve some recurrences. The two methods seem somewhat analogous, and if we write $x=e^{-s}$ we see the Laplace transform $$\mathcal L\{a(t)\}=\int_0^\infty e^{-st}a(t)dt=\int_0^\infty a(t)x^tdt$$ as the continuous analogue of the ordinary generating function $$\sum_{n=0}^\infty a_nx^n.$$ Now, in combinatorics class, I was told that convergence of generating functions was unimportant, because we could think of them as formal power series. I would have thought that convergence of Laplace transforms was unimportant for analogous reasons. However, Boyce & DiPrima make a bit of a fuss about convergence, and the short table of Laplace transforms in their book takes the trouble to specify the domain of convergence for each transform. My question. Why do we care about convergence of Laplace transforms? Or don't we? Or is it that convergence is unimportant for the very elementary applications I learned about, but becomes important in more advanced work? In other words: Is there some reason we can't use ""formal Laplace transforms"" to solve differential equations the same way we can use formal power series to solve recurrences?","['recurrence-relations', 'laplace-transform', 'ordinary-differential-equations', 'generating-functions']"
4918930,"how to evaluate $\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy$","How to evaluate: \begin{align*}
&\int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
\end{align*} My attempt(almost a complete solution) $$
I := \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 4\log_{\pi}{\sqrt{x}} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
$$ $$
= \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{(\sqrt{xy}+\sqrt{x^{3} y^{3}}) \left[ \log_{\pi}^{2}{x} + 2\log_{\pi}{x} \log_{\pi}{y} + \log_{\pi}^{2}{y} \right]} \, dx \, dy
$$ $$
= \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy
$$ $$
= \ln^{2}{\pi} \int_{0}^{1}\int_{0}^{1} \frac{2-x-y}{\sqrt{xy}(1+xy) \log_{\pi}^{2}{xy}} \, dx \, dy
$$ Change of variables: $$
\begin{cases}
u = xy \\
v = y
\end{cases}
\quad , \quad \frac{\partial (u, v)}{\partial (x, y)} = \begin{vmatrix}
y & x \\
0 & 1 
\end{vmatrix} = y = v
$$ Thus: $$
\begin{cases}
x = \frac{u}{v} \\
y = v
\end{cases}
\quad \text{and} \quad \left| \frac{\partial (x, y)}{\partial (u, v)} \right| = \frac{1}{v}
$$ So: \begin{align*}
I &= \ln^{2}{\pi} \iint_{D} \frac{2 - \frac{u}{v} - v}{\sqrt{u}(1+u) \ln^{2}{u}} \cdot \frac{1}{v} \, du \, dv
\end{align*} Where: $D = \left\{ (u, v) \, | \, 0 \leq u \leq v \leq 1 \right\}$ Therefore: $$
I = \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \int_{u}^{1} \left( \frac{2}{v} - \frac{u}{v^{2}} - 1 \right) \, dv \right] \, du
$$ $$
= \ln^{2}{\pi} \int_{0}^{1} \frac{1}{\sqrt{u}(1+u) \ln^{2}{u}} \left[ \left( 2 \ln{v} + \frac{u}{v} - v \right) \bigg|_{u}^{1} \right] \, du
$$ $$
= 2 \ln^{2}{\pi} \int_{0}^{1} \frac{u-1-\ln{u}}{\sqrt{u}(1+u) \ln^{2}{u}} \, du \quad \left( \text{let } u = e^{-t} \right)
$$ $$
= 2 \ln^{2}{\pi} \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-t} - 1 + t}{t^{2}} \, dt
$$ Let: $$
J(\lambda) := \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{e^{-\lambda t} - 1 + \lambda t}{t^{2}} \, dt \quad (\lambda \geq 0)
$$ Then: \begin{align*}
J'(\lambda) &= \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot \frac{1 - e^{-\lambda t}}{t} \, dt
\end{align*} And: $$
J''(\lambda) = \int_{0}^{+\infty} \frac{e^{-\frac{t}{2}}}{1+e^{-t}} \cdot e^{-\lambda t} \, dt
$$ $$
= \int_{0}^{+\infty} \sum_{n=0}^{+\infty} (-1)^{n} \cdot e^{-(\lambda + n + \frac{1}{2}) t} \, dt
$$ $$
= \sum_{n=0}^{+\infty} (-1)^{n} \int_{0}^{+\infty} e^{-(\lambda + n + \frac{1}{2}) t} \, dt
$$ $$
= \sum_{n=0}^{+\infty} \frac{(-1)^{n}}{\lambda + n + \frac{1}{2}}
$$ Given: $$
J'(0) = 0 \Rightarrow J'(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \ln{\left(1 + \frac{2\lambda}{2n+1}\right)}
$$ And: $$
J(0) = 0 \Rightarrow J(\lambda) = \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \left( \lambda + \frac{2n+1}{2} \right) \ln{\left(1 + \frac{2\lambda}{2n+1}\right)} - \lambda \right]
$$ So: \begin{align*}
I &= 2 \ln^{2}{\pi} \cdot J(1) \\
&= 2 \ln^{2}{\pi} \cdot \sum_{n=0}^{+\infty} (-1)^{n} \cdot \left[ \frac{2n+3}{2} \ln{\left(1 + \frac{2}{2n+1}\right)} - 1 \right]
\end{align*} I don't know how to evaluate the last sum, and can someone check if my proof is correct?","['integration', 'definite-integrals', 'multivariable-calculus', 'closed-form', 'sequences-and-series']"
4919001,Bounding $\|A^r - B^r\|$ for $r \geq 1$.,"Let $A$ , $B$ be positive semi-definite self-adjoint matrices. Is it true that, for $r \geq 1$ , $r \in \mathbb{R}$ , $$
\|A^r - B^r\| \leq rc^{r-1}\|A - B\|
$$ where $\|\cdot\|$ denotes the operator norm and $c = \max\{\|A\|, \|B\|\}$ . When $r \in [0,1]$ , using that $f:x \to x^r$ is operator monotone we get the classical inequality $$
\|A^r - B^r \| \leq \|A - B \|^r.
$$ And if $A \geq \alpha I$ and $B \geq \alpha I$ with $\alpha >0$ , we have (see Matrix Analysis, Rajendra Bhatia) $$
\|A^r - B^r\| \leq r\alpha^{r-1}\|A - B\|.
$$ However when $r \geq 1$ we loose the operator monotonicity and I then struggle to prove the first inequality.","['operator-theory', 'linear-algebra', 'monotone-operator-theory']"
4919035,$C^1$ function $f>0$ on $\Bbb R$ satisfies $f ^ { \prime } ( x ) = f ( x - 1 )$.,"$C^1$ function $f>0$ on $\Bbb R$ satisfies $f ^ { \prime } ( x ) = f ( x - 1 )$ . How to find $f$ ? It is easy to deduce that $f(-\infty)=0, f(+\infty)=+\infty$ . Then... If $f'(x)=f(1-x)$ , then $f''(x)=-f'(1-x)=f(x)$ . It is easy. But here $f'(x)=f(x-1)$ ...","['calculus', 'functions']"
4919126,How to Evaluate the Integral $\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy?$,"Question: How to Evaluate the Integral $$\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy?$$ My attempt I'm looking for a method to evaluate it. I've attempted a substitution to simplify the expression, but I'm not sure how to proceed further. Here’s what I've done so far: I used the substitution $\sqrt{1+\sin(y)} \rightarrow x$ .
This transforms the integral into: $$-2\int_{1}^{\sqrt{2}}\frac{\ln(x^{2}-1)}{x^{2}-2}dx.$$ I further decomposed the logarithm: $$-2\int_{1}^{\sqrt{2}}\frac{\ln(x+1)+\ln(x-1)}{(x-\sqrt{2})(x+\sqrt{2})}dx.$$ I'm stuck at this point. Edit ;The integral was given by our professor, who mentioned it has a very beautiful closed form. I have already emailed our professor, who gave the integral to my friends and me. If I receive any updates, I'll let you know.","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4919133,Want an example of uncorrelated but dependent joint Bernoulli example [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question Can anyone give an example (joint probability table) that two Bernoulli variables are uncorrelated but not independent?","['bernoulli-numbers', 'probability']"
4919161,Operator with basis-independent trace but which is not trace class,"Let $A$ be a trace-class operator on a separable Hilbert space $H$ . By definition, this means that the quantity $$\sum_i\langle|A|e_i,e_i\rangle$$ is finite, where $\{e_i\}_i$ is an orthonormal basis of $H$ . This has the consequence that the trace of $A$ , $$\sum_i\langle Ae_i,e_i\rangle,$$ is well-defined in the sense that it is independent of the choice of orthonormal basis, as well as the ordering of elements within a basis. I'm wondering if the converse is true, i.e. whether having a well-defined trace in the above sense implies that $A$ is trace class. Equivalently: Question: Is there a bounded operator $T$ on $H$ that is not trace-class, yet $\sum_i\langle Te_i,e_i\rangle$ is well-defined independent of the choice of orthonormal basis $\{e_i\}_i$ and of reordering of basis elements?","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4919182,Strange substitution made in a paper to find asymptotics,"In the quoted section from this paper , why is the author able to ""substitute this result into Eq. (2.1)""? This should hold for $z$ large. But not everything on the contour is large. Why can the author make this substitution? Let $T=\frac{4}{27} t^3$ and $K_{1 / 6}(T)$ be the modified Bessel functions of order $\frac{1}{6}$ . Now previously the paper showed $$\operatorname{Ai}^3(x)=C_1 \int_{\mathcal{L}_1} t^{1 / 2} K_{1 / 6}(T) \exp \left(\frac{5}{27} t^3-x t\right) d t \qquad \qquad (2.1)
$$ (If you see a rectangle, the symbol is mathcal{L}_1). Next we need to show that the constant $C_1$ can be chosen so that both sides of this equation have the same asymptotic behavior as $x \rightarrow \infty$ . For that purpose we first recall that in the complete sense of Watson [3] $$
K_{1 / 6}(T) \sim\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} t^{-3 / 2} e^{-T} \quad\left(|\operatorname{ph} t|<\frac{1}{3} \pi\right)
$$ Substituting this result into Eq. (2.1) Why? then gives $$
\operatorname{Ai}^3(x) \sim C_1\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} \int_{\mathcal{L}_1} t^{-1} \exp \left(\frac{1}{27} t^3-x t\right) d t
$$ and an application of the saddle-point method to this integral gives $$
\operatorname{Ai}^3(x) \sim i C_1 2^{3 / 2} 3 \pi x^{-3 / 4} e^{-3 \xi}
$$","['geometry', 'asymptotics', 'analysis', 'complex-analysis', 'real-analysis']"
4919187,Dynamics of a sliding cube on the $XY$ and $YZ$ planes,"A cube with side length $a$ , is initially placed with one vertex at the origin, and its faces parallel to the coordinate planes ( $XY, XZ, YZ$ ) and totally lying in the first octant. Then its rotated and shifted such that its base which was initially coincident with the $XY$ plane, now makes an angle of $45^\circ$ with the $XY$ plane and the $YZ$ plane. The cube is then given an infinitesimal push in order to cause to start sliding without friction on both of these planes. Question :  Find the location of the center of the cube and the angle its base makes with the $XY$ plane as a function of time, from $t = 0$ till the final time the base is coincident with the $XY$ plane again. My Attempt: Assuming the cube has a uniform density, and mass $m$ , and that the gravitational acceleration is $g$ pointing the negative $Z$ direction. Also, let $(x,y,z)$ be the location of the center of mass of the cube.
And let $\theta$ be the angle the base of the cube makes with the $XY$ plane. There will a reaction $F_z$ from the $XY$ plane, therefore, $  m \ddot{z} = F_z - m g $ and also, there will be a reaction $F_x$ from the $YZ$ wall pointing the positive $X$ direction, therefore, $ m \ddot{x} = F_x $ Now, taking the moments about the lower edge (the edge touching the $XY$ plane) , we get $ - m g (a/2) \cos \theta + F_x (a/2) \sin \theta = \tau_Y = I_Y \ddot{\theta} $ We need to relate $(x,y,z)$ with $\theta$ , and this given by $ (x,y,z) = ( (a/2) ( \cos \theta + \sin \theta ) , a/2 , (a/2) (\cos \theta + \sin \theta ) ) $ So that $ (\dot{x}, \dot{y}, \dot{z} ) = (a/2) \dot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta ) $ And $ (\ddot{x}, \ddot{y}, \ddot{z} ) = (a/2) \ddot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta ) + (a/2) ( \dot{\theta} )^2 ( - \cos \theta - \sin \theta, 0, - \cos \theta - \sin \theta ) $ The moment of inertia of a cube about an axis through its center is $\dfrac{m a^2}{6}$ .  Therefore, the moment of inertia about its edge is (by the parallel axis theorem) equal to $\dfrac{2 m a^2}{3} $ Plugging all this into moment equation, we get $- 6 g \cos \theta +  3 a  \bigg( \ddot{\theta}( \cos \theta - \sin \theta ) - (\dot{\theta})^2 (\cos \theta + \sin \theta) \bigg) = 4 a \ddot{\theta} $ And this is a second-order non-linear ordinary differential equation in $\theta(t)$ with initial conditions $\theta(0) = \dfrac{\pi}{4} $ and $ \dot{\theta}(0) = 0 $ .  It can be solved numerically with the Runge-Kutta 4th-Order method, with very precise results. That I all I could come up with.  Your comments on the correctness of this analysis, as well as alternative solutions are much appreciated.","['physics', 'nonlinear-dynamics', 'ordinary-differential-equations']"
4919239,Second order ODE solution approach,"Find a solution for the following second order ODE $$x''(t)+(x'(t))^2+\sin(x(t))=0, \; x(0)=\pi,\;x'(0)=0.$$ I am familiar with the characteristics equations method or assuming $y=e^{rx}$ , however both approaches seem unavailable for this equation. I tried simplifying the equation by assuming that we only seek a solution around the point $x(0)=\pi$ and therefore $\sin(x)\approx-x$ , which reduces the problem to $$x''+(x')^2-x=0.$$ From here I still don't see how to work around $(x')^2$","['calculus', 'ordinary-differential-equations']"
4919251,"Evaluating $ \lim_{n\to \infty} \frac{1}{n}\int_0^n \max(\{x\},\{\sqrt{2}x\},\{\sqrt{3}x\})dx $, where $\{x\}$ is the fractional part of $x$","I found an interesting problem on the internet: $\lim_{n\to \infty} \frac{1}{n}\int_0^n \max(\{x\},\{\sqrt{2}x\},\{\sqrt{3}x\})dx$ , where $\{x\}$ is the fractional part of $x$ . I have known that this problem can be regarded as calculating the expectation of the maximum of three independent uniform random variables $X_i \overset{i.i.d}{\sim} \mathrm{Unif}(0,1)$ . However, I do not understand the reasoning behind why this is the case.","['limits', 'calculus', 'probability']"
4919270,"Maximise $xyz$ such that $x+xy+xyz=1$, $y+yz+xyz=2$, $z+zx+xyz=4$","$x, y, z$ are real numbers which satisfy the following: $$x + xy + xyz = 1$$ $$y + yz + xyz = 2$$ $$z + zx + xyz = 4$$ Then find the maximum value of $xyz$ . I tried adding and subtracting a few equations and substituting iteratively into the others which didn't work. Then I tried letting $xyz = k$ and trying to get stuff in terms of $k$ but I couldn't proceed with that either.","['contest-math', 'optimization', 'algebra-precalculus', 'systems-of-equations']"
4919281,Cohomology of $\mathbb{Z}^2$ with coefficients in $\mathbb{Z}$,"Let $G = \mathbb{Z}^2$ and let $\mathbb{Z}$ be the trivial module. Then, $H^{1}(\mathbb{Z}^2,\mathbb{Z}) \cong \text{Hom}(\mathbb{Z}^2, \mathbb{Z}) \cong \mathbb{Z}^2$ , which is generated by projection maps onto $\mathbb{Z}$ i.e. $\pi_i : \mathbb{Z}^2 \to \mathbb{Z}, \mathbf{v} =  (v_1,v_2) \mapsto v_i$ . I would like to prove that $\pi_i^2 = 0$ with the cup product, but I'm not sure how to get the answer. Here is my attempted computation: $$\pi^2_i(\mathbf{v}, \mathbf{w}) = (-1)^{1 \cdot 1} \pi_i(\mathbf{v}) \otimes(\mathbf{v}\pi_i(\mathbf{w}))= -v_1 \otimes w_1$$ since $\mathbb{Z}^2$ has trivial action on the trivial module. How do I see this is zero in $H^2$ ? Edit : While looking at some more examples, I think I may understand why this is, so I would appreciate confirmation of whether or not my solution is correct; I have added the solution verification tag. I think the actual value of $\pi^2_i(\mathbf{v}, \mathbf{w}) $ is not too relevant. The point is that, under the standard isomorphism $\mathbb{Z} \otimes \mathbb{Z} \cong \mathbb{Z}$ for the tensor product in the RHS, we have: $$\pi^2_i(\mathbf{v}, \mathbf{w}) =  -v_1w_1$$ In particular, this isomorphism is invariant under the map $\rtimes$ which exchanges the factors $M \otimes N \cong N \otimes M$ , where we have $M = N = \mathbb{Z}$ here. Therefore, $$\pi_i^2(\mathbf{v}, \mathbf{w}) = \rtimes(\pi_i^2(\mathbf{v},\mathbf{w}))$$ However, $\pi_i$ commutes with itself, so by the graded commutative structure on $H^{\bullet}$ , we have that: $$\pi^2_i(\mathbf{v}, \mathbf{w}) = - \rtimes(\pi_i^2(\mathbf{v},\mathbf{w})) $$ This equality is in $\mathbb{Z}$ , which has no elements of order $2$ (or torsion at all), so $\pi^2_i(\mathbf{v}, \mathbf{w}) = 0$ . Please let me know if I have made any mistakes.","['group-theory', 'solution-verification', 'homology-cohomology', 'group-cohomology']"
4919289,MacCluer Exercise 3.2,"In MacCluer's Elementary Functional Analysis, exercise 3.2 states: We introduce some terminology for the purpose of this problem: If $X$ is either a
real or complex vector space (meaning that the scalars used in scalar multiplication
are real, or, respectively, complex), we say that a real-valued $\phi$ is a real-linear functional if $\phi(x + y) = \phi(x)+ \phi(y)$ and $\phi(\alpha x) = \alpha\phi(x)$ holds for all $x,y \in X$ and $\alpha$ real. For $X$ a complex vector space, we say that (a complex-valued) $\phi$ is a complex linear functional if these relationships hold for all $x,y\in X$ and $\alpha$ complex. (c) Suppose $u$ is a real-linear functional on $X$ . Define $\phi : X \rightarrow \mathbb{C}$ by $\phi(x) =
u(x)−iu(ix)$ . Show that $\phi$ is a complex-linear functional on $X$ . (d) Now suppose $X$ is a normed linear space. For $\phi$ and $u$ related as above, show
that $\vert\vert \phi \vert\vert $ = $\vert\vert u\vert\vert$ . I am confused by part d): why $\vert\vert \phi \vert\vert $ = $\vert\vert u\vert\vert$ . From my understanding, since $u$ is a real valued functional, we ought to have $$\vert\vert \phi(x)\vert\vert = \vert\vert u(x) - i u(ix)\vert \vert = \sqrt{\vert u(x)\vert^2 + \vert u(ix)\vert^2} \geq \vert u(x) \vert,$$ which means the only possible way for $\vert\vert \phi \vert\vert $ = $\vert\vert u\vert\vert$ is when $\vert\vert\phi(x)\vert\vert$ approaches $\vert\vert \phi\vert\vert$ implies $u(ix)$ approaches $0$ . However, I don't see why this should be true - is there any mistake in my reasoning or any way to prove this?",['functional-analysis']
4919325,"Show that $\frac{x^3+y^3}{x^2+y^2}$ is not differentiable at $(0,0)$","Let be $f:\mathbb{R}^2\to\mathbb{R}$ where $$
f(x,y):=\begin{cases} \frac{x^3+y^3}{x^2+y^2},&(x,y)\neq (0,0)\\0,&(x,y)=(0,0).\end{cases}
$$ Show that $f$ is not differentiable at $(0,0)$ . My approach: If $f$ was differentiable at $(0,0)$ , by definition there must exist a unique matrix $A=(a,b)$ such that \begin{align*}
&\lim\limits_{{x\choose y}\to{0\choose 0}}\frac{\left|\frac{x^3+y^3}{x^2+y^2}-0-A{x\choose y}\right|}{\Vert {x\choose y}\Vert_2}=0.
\end{align*} We consider the sequence $\left(\frac{1}{n},0\right)$ which implies that \begin{align*}
&\lim\limits_{n\to\infty}\frac{\left|\frac{\frac{1}{n^3}+0^3}{\frac{1}{n^2}+0^2}-0-A{\frac{1}{n}\choose 0}\right|}{\frac{1}{n}}=1-a\implies a=1.
\end{align*} Similarly, $\left(0,\frac{1}{n}\right)$ implies \begin{align*}
&\lim\limits_{n\to\infty}\frac{\left|\frac{\frac{1}{n^3}+0^3}{\frac{1}{n^2}+0^2}-0-A{0\choose \frac{1}{n}}\right|}{\frac{1}{n}}=1-b\implies b=1.
\end{align*} Finally, we look at the sequence $\left(\frac{1}{n},\frac{1}{n}\right)$ and see that \begin{align*}
&\lim\limits_{n\to\infty}\frac{\left|\frac{\frac{1}{n^3}+\frac{1}{n^3}}{\frac{1}{n^2}+\frac{1}{n^2}}-0-A{\frac{1}{n}\choose \frac{1}{n}}\right|}{\sqrt{2}\frac{1}{n}}=\frac{1}{\sqrt{2}}(1-a-b).
\end{align*} As we have already shown that $a=b=1$ it is not possible that the last limit attains $0$ . So there doesn't exists a unique matrix $A$ . This apporach feels a bit lengthy but I was not able to produce a contradiction only by using one sequence. Maybe someone else has a swifter approach or can give me some feedback on this.","['alternative-proof', 'limits', 'derivatives', 'real-analysis']"
4919334,"If $\sum_{n=1}^\infty a_n^2$ converges, then $\lim_{n\to \infty} \frac 1 {\sqrt n} \sum_{k=1}^n a_k =0$","Problem. Prove that if $\sum_{n=1}^\infty a_n^2$ converges, then $\lim_{n\to \infty} \frac 1 {\sqrt n} \sum_{k=1}^n a_k =0$ . The problem arises from the following question: Let $(e_i)_{i=1}^\infty $ be an orthonormal sequence in a Hilbert spcae $H$ . Prove that $u_n= \frac 1 {\sqrt n} \sum_{k=1}^n e_k$ weakly converges to $0$ . I am aware of a proof using Cauchy-Schwarz inequality, using the fact that $\sum_{k=m+1}^n \frac{a_k}{\sqrt{n-(m+1)}} \le \sqrt{\sum_{k=m+1}^n a_k^2}$ . However, I am looking for different approaches for this problem, perhaps more ""elementary"".","['orthonormal', 'weak-convergence', 'hilbert-spaces', 'cauchy-schwarz-inequality', 'sequences-and-series']"
4919343,Why is $2\sum_{k=a}^{b}ki^k=(1+i)i^a-i^a+i^b+(1-i)i^bb$,"I am studying to skip into 10th grade, and on the curriculum, it has a section titled ""Investigation of large sums and products of consecutive powers if $i$ "" (I kid you not) immediately after ""Introduction to the complex number $i$ "" and ""Introduction of the sigma and pi notation."" It gives problems in the form $\sum\limits_{k=a}^{b}ki^k$ . I put this equation in WolframAlpha, and here is the answer to it: $$2\sum\limits_{k=a}^{b}ki^k=(1+i)i^a-i^a+i^b+(1-i)i^bb$$ 😭How did they get this result?
😭 ADDITIONAL INFO: Obviously, $i^n=i^{n+4}$ because $i^{n+4}=i^ni^4=i^n(1)=i^n$ , so there is definitely a pattern here. Please explain it like I'm dumb so I can understand. NO INDUCTION plz, I want derivation.",['complex-analysis']
4919397,"$\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu)$: $1\leq p_1<p_2<+\infty$, $\mu$ counting measure","I need to prove the following: Suppose that $1\leq p_1<p_2<+\infty$ . Let $\mu$ is the counting measure on the $\sigma$ -algebra $\mathscr{A}$ of all subsets of $\mathbb{N}$ . Then $\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu)$ . My Question: I tried it myself, but I got stuck on a step of my attempt (see below). I am not sure if my existing steps are correct or not either. I would really appreciate it if someone could check my existing work and help me out with where I got stuck. Moreover, I would like to see if there is any easier solution. Thanks a lot for any help! My Attempt: Let $f\in\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)$ . Then $f$ is an $\mathscr{A}$ -measurable function on $\mathbb{N}$ such that $|f|^{p_1}$ is integrable. So $\int\left(|f|^{p_1}\right)^+d\mu = \int|f|^{p_1}d\mu < +\infty$ . Write $f=\sum_{i=1}^{\infty}a_i\chi_{\{i\}}$ . Then $|f|^{p_1} = \sum_{i=1}^{\infty}|a_i|^{p_1}\chi_{\{i\}}$ .  I want to prove that \begin{align*}
\int|f|^{p_1}d\mu &= \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\}\\
&= \sum_{i=1}^{\infty}|a_i|^{p_1}\quad (< +\infty).
\end{align*} I couldn't figure out how to prove that $\sum_{i=1}^{\infty}|a_i|^{p_1}$ is the least upper bound of the set $\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\}$ . What remains is to apply the following result (see also this post ): Suppose that $1\leq p_1<p_2<=\infty$ . Then each sequence $\{a_n\}$ that satisfies $\sum|a_n|^{p_1}<+\infty$ also satisfies $\sum|a_n|^{p_2}<+\infty$ If the part where I got stuck is true, then this result would imply that $\int|f|^{p_2}d\mu<+\infty$ , and thus $f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu)$ . Note: $\mathscr{S}_+$ is the set of simple nonnegative real-valued $\mathscr{A}$ -measurable functoin. Note: I aware that related questions have been asked here and here . But this question is asking about different stuff. Thank you very much in advance! Update: Thanks to @ThànhNguyễn's comment. I wrote an answer for this post. I would really appreciate if someone could help me check if it is correct or not! Thank you very much! Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","['integration', 'measure-theory', 'proof-writing', 'analysis', 'real-analysis']"
4919405,Solving the integral $\int_{\frac{\pi}{6}}^{\frac{\pi}{3}} \frac{\sin x + \cos x}{\sqrt{\sin 2x}} \ dx$,My textbook has the following problem: $$\int_{\frac{\pi}{6}}^{\frac{\pi}{3}} \frac{\sin x + \cos x}{\sqrt{\sin 2x}} \ dx$$ Using the trigonometric identity: $$\sin 2x = 2 \sin x \cos x$$ the integration simplifies to: $$\frac{1}{\sqrt{2}} \int_{\frac{\pi}{6}}^{\frac{\pi}{3}} \left( \sqrt{\tan x} + \sqrt{\cot x} \right) \ dx$$ I'm stuck here.,"['integration', 'trigonometry', 'definite-integrals']"
4919439,Calculating $\sum\limits_{k=0}^n\binom{n}{k}/\left(2^k+2^{n-k}\right)$,"I am trying to find a closed form for $\sum\limits_{k=0}^n\frac{\binom{n}{k}}{2^k+2^{n-k}}$ . I saw on quora that integration can be used to rewrite portions of such equations, and so I attempted this. So far, this is what I got: $$\begin{align}
\sum\limits_{k=0}^n\frac{\binom{n}{k}}{2^k+2^{n-k}} &=\sum\limits_{k=0}^n\binom{n}{k}\int\limits_0^1x^{2^k+2^{n-k}-1}dx\\\\
&=\sum\limits_{k=0}^n\binom{n}{k}\int\limits_0^1\sum\limits_{j=0}^\infty\binom{2^k+2^{n-k}-1}{j}(x-1)^jdx\\\\
&=\sum\limits_{k=0}^n\binom{n}{k}\sum\limits_{j=0}^\infty\binom{2^k+2^{n-k}-1}{j}\left[\frac{(x-1)^j}{j+1}\right]I_{0}^{1}&&\quad\text{where $I_0^1$ means evaluate between 0 and 1}\\\\
&=\sum\limits_{k=0}^n\sum\limits_{j=0}^\infty\binom{n}{k}\binom{2^k+2^{n-k}-1}{j}\frac{(-1)^{j+1}}{j+1}
\end{align}$$ I am not sure if I have made much progress in this way, but even if I have then I am now a bit stuck... ** EDIT 1 **
I am guessing there is an error in my derivation somewhere because If I try to evaluate this using Mathematica the infinite sum does not converge. To see this, take a look at this online notebook . I am also interested in what people have to say w.r.t. the Egorychev method , which seems promising but which I have not been able to use in this case so far...","['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4919451,"Prove that if $f: U \rightarrow \mathbb{R}^n$, then $f=(f_1,...,f_n)$","I gave myself a claim to prove: If $f: U \rightarrow \mathbb{R}^n$ , then we can write $f$ as $f=(f_1,...,f_n)$ . I tried to prove it this way: Assume $f: U \rightarrow \mathbb{R}^n$ . Then we know that $f(p) \in \mathbb{R}^n$ . So we can write $f(p) = (x_1,...,x_n)$ . We can proceed in two ways: Since $f(p) = (x_1,...,x_n)$ , each $x_i$ depends on $p$ , and so it is a function of $p$ , and thus each $x_i$ is a function $x_i: U \rightarrow \mathbb{R}$ . This line of ""proof"" seems not formal enough and doesn't show rigorously that $x_i$ is a function from $U$ to $\mathbb{R}$ . Since $f(p) \in \mathbb{R}^n$ , we can define $f_i \:= f \cdot \mathbf{e}_i$ (which is a function from $U$ to $\mathbb{R}$ ), and then $f = (f,\mathbf{e}_i)\mathbf{e}_i = (f_1,...,f_n)$ (I used Einstein summation convention), where $\mathbf{e}_i$ is the standard basis of $\mathbb{R}^n$ . I know it is a very basic proof, but I am not confident in my proof-writing skills yet, so I would be grateful if you could provide your input and advice on how to make it more rigorous and precise.","['elementary-set-theory', 'proof-writing', 'functions', 'solution-verification']"
4919572,"$\int\frac{\sin 2x\sin 3x}{\sin 2x+\sin 3x}\,\mathrm dx$","$$\int\frac{\sin 2x\sin 3x}{\sin 2x+\sin 3x}\,\mathrm dx$$ My attempt:
Rewriting the numerator: We can use the double angle identity for sine: $$\sin(A)\sin(B) = \frac12\left(\cos(A - B) - \cos(A + B)\right)$$ Here, let $A = 2x$ and $B = 3x$ . Substitute these values into the identity: $$\sin(2x)\sin(3x) = \frac12(\cos(2x - 3x) - \cos(2x + 3x))$$ Apply this identity to rewrite the numerator: $$\sin^2(x)\sin(3x) = \frac12(\cos(x) - \cos(5x))$$ Now I am stuck here","['integration', 'trigonometry']"
4919590,A constructive proof of this innocent set theoretic proposition?,"I was reading Freiwald's An Introduction to Set Theory and Topology , and I came across the following exercise from Chapter 1: E8. Suppose $A$ , $B$ , $C$ , and $D$ are sets with $A\ne\emptyset$ and $B\ne\emptyset$ . Show that if $$(A\times B)\cup (B\times A) = (C\times D)\cup(D\times C)\tag{1},$$ then either ( $A = C$ and $B = D$ ) or ( $A = D$ and $B = C$ ). I prove this by contradiction , analyzing the following cases: $\underline{a_0\in A\setminus C \text{ and } a_1\in A\setminus D}$ : Then due to nonemptiness of $B$ and Eq. (1), we get $a_0\in D$ and $a_1\in C$ . Thus, $(a_0, a_1)\in D\times C$ and again by using (1), we get that one of $a_0$ or $a_1$ , say $a_i$ , is in $B$ . Since $a_i\in A$ already, we have that $(a_i, a_i)\in A\times B$ so that by (1) again, we must have that $a_i\in C\cap D$ , a contradiction. $\underline{a_0\in A\setminus C \text{ and } d_0\in D\setminus A}$ : Then due to similar reasons as above, $d_0\in B$ so that $(a_0, d_0)\in A\times B$ . Since $a_0\notin C$ , we have $(a_0, d_0)\in D\times C$ so that $d_0\in C$ . But then $(d_0, d_0)\in C\times D$ so that by (1), $d_0\in A$ – false. $\underline{a_0\in A\setminus C \text{ and } b_0\in B\setminus C}$ : Then $(a_0, b_0)\in A\times B$ so that by (1), one of $a_0$ or $b_0$ must be in $C$ , again false. The rest of the cases are similar. Question: Now, this above uses Double Negation Elimination (DNE) to prove what is a very innocent-looking proposition, which, it feels like, must be provable constructively. Any insights on this?","['proof-writing', 'alternative-proof', 'intuition', 'elementary-set-theory', 'constructive-mathematics']"
4919603,Assistance with an exercise on field endomorphisms,"Working through the problems in a book on field theory (Field Extensions and Galois Theory by Bastida). I came across one which I thought looked like a ""routine"" exercise, but has been particularly stubborn. Suppose $K$ is a field with $char(K) \neq 2$ and $u:K \rightarrow K$ a map so that $u(x+y) = u(x)+u(y)$ for all $x,y \in K$ , $u(1) = 1$ and $u(x)u(1/x) = 1$ for all $x \in K^*$ . Show that $u$ is an endomorphism. Seems straightforward, I just need to show $u(xy) = u(x)u(y)$ . I figured out that I can reduce the problem to just verifying it for ""squares."" That is, if I can just show $u(x^2) = u(x)^2$ for all $x$ . In that case, we can say: $u((x+y)^2) = u(x^2+ xy + xy + y^2) = u(x^2) + 2u(xy) + u(y^2)$ . But also $u((x+y)^2) = u(x+y)u(x+y) = u(x)^2 + 2u(x)u(y) + u(y)^2$ . Then we have $2u(xy) = 2u(x)u(y)$ and since $K$ is a field not of characteristic 2, we can cancel the 2's and get the result. Alas, proving the special case $u(x^2) = u(x)^2$ has resisted my efforts. Of course $x = 0$ is not a problem. Otherwise, I was trying to play around with $u(x)u(1/x) = 1$ and $u(1) = 1$ to get it. The closest I've managed to get by tinkering with these is $u(x^2\cdot 1/x) = u(x)^2u(1/x)$ . A hint in the right direction would be appreciated, or, if this approach won't work, a nudge in another direction.","['field-theory', 'galois-theory', 'abstract-algebra']"
4919608,Extracting a subsequence common to infinitely many sets from an uncountable collection with uniform positive upper density,"Let $\{a_n\},\{b_n\}$ be strictly increasing sequence of positive integers satisfying $a_1<b_1<a_2<b_2<a_3<b_3<\ldots$ and $(b_n-a_n) \to \infty$ . Define $I_n:= [a_n,b_n]$ , meaning the set of all consecutive integers from $a_n$ to $b_n$ .. Let $A_{\alpha},\alpha \in \mathcal{A}$ be an uncountable family of infinite subsets of $\mathbb{N}$ that satisfies the following: There exists some $c \in (0,1)$ so that for all $\alpha \in \mathcal{A}$ there exists some $n(\alpha) \in \mathbb{N}$ so that $|A_{\alpha} \cap I_n|\geq c(b_n-a_n)$ for all $n\geq n(\alpha)$ . (Here $|B|$ is the number of elements of a finite subset $B \subset \mathbb{N}$ ) Then my question is the following: Does there exist a strictly increasing sequence of positive integers $\{l_n\} \to \infty$ for which there exists an infinite subset $\mathcal{B}\subset \mathcal{A}$ so that $l_n \in A_{\alpha}$ for all $n \in \mathbb{N}$ , $\alpha \in \mathcal{B}$ ?","['extremal-combinatorics', 'real-analysis', 'sequences-and-series', 'descriptive-set-theory', 'set-theory']"
4919617,"Function satisfying $y(0)=1$ and $y(1)=4$ such that $(y')^2/y<4$ for all $x\in (0,1)$","I am trying to find a function satisfying $$
y\left(0\right) = 1\quad\mbox{and}\quad y\left(1\right) = 4\quad\mbox{such that}\quad {\left(y'\right)^{2} \over y} < 4\,\, \forall x \in \left(0,1\right)
$$ Does anyone have an idea how to go about this ?. I tried with building simple functions satisfying the first two conditions such as $4^{x}$ or $3\sin\left(\pi x/2\right) +1 $ but they all fail to satisfy the last condition.","['calculus', 'ordinary-differential-equations', 'calculus-of-variations']"
4919655,"Solving $c_{n} - 4c_{n-1} + 4c_{n-2} = 3$, where $c_{0} = c_{1} = 11$","Consider the following non-homogeneous recurrence relation \begin{equation} c_{n} - 4c_{n-1} + 4c_{n-2} = 3 \end{equation} where $c_{0} = c_{1} = 11$ . I am trying to derive the generating function $C_{1}(x) = \sum_{n=0}^{\infty} c_{n}x^{n}$ and separately solve the recurrence relation by homogenizing it first. To find the generating function I multiply both sides by $x^{n}$ and sum over $n\geq 2$ . Simplifying leads to the following expression \begin{equation} C(x) - a_{0} - a_{1}x -4x(C(x) - a_{0}) + 4x^{2}(x) = \frac{3x^{2}}{(1-x)}.\end{equation} By factorizing and inserting the known values for $a_{0}$ and $a_{1}$ , I arrived at $$C(x) = \frac{11 - 33x}{(1-4x+4x^{2})} + \frac{3x^{2}}{(1-x)(1-4x+4x^{2})} \tag{1}$$ To homogenize the recurrence relation, note that $c_{n-1} = 4c_{n-2}-4c_{n-3} + 3$ . If we subtract this relation from $c_{n} = 4c_{n-1} - 4c_{n-2} + 3$ we get: \begin{equation} c_{n} - c_{n-1} = 4c_{n-1} - 8c_{n-2} + 4c_{n-3} \Rightarrow c_{n} = 5c_{n-1} - 8c_{n-2} + 4c_{n-3},\end{equation} where $c_{0} = c_{1} = 11$ and $c_{2} = 3$ . This recurrence relation has characteristic polynomial $x^{3} - 5x^{2} + 8x - 4 = (x-1)(x-2)^{2}$ . Thus \begin{equation} c_{n} = (A + Bn)2^{n} + C. \end{equation} With the initial conditions we get a system of equations, which can be solved after which we get $A = 8$ , $B = -4$ and $C = 3$ . My first question is whether these solutions are correct, but one thing is troubling me in particular. Note that using our values for $A$ , $B$ and $C$ we can also write: \begin{equation}
C(x) = \frac{8}{(1-2x)} - \frac{4}{(1-2x)^{2}} + \frac{3}{(1-x)} \tag{2}.
\end{equation} But after checking, the partial fraction expansion of (1) is not equal to (2)? Should they not be equal? The last two terms are the same, perhaps I made a computational mistake that I am unable to see. Any help is appreciated! Edit The expression in (2) is obtained by substituting the suitable values in the following general formula for $C(x)$ \begin{equation}
C(x) = \frac{A_{1, 1}}{1- \alpha_{1}x} + \dots + \frac{A_{1,m_{1}}}{(1-\alpha_{1}x)^{m_{1}}} + \dots + \frac{A_{j,1}}{1-\alpha_{j}x} + \dots + \frac{A_{j,m_{j}}}{(1-\alpha_{j}x)^{m_{j}}}
\end{equation} where $\alpha_{j}$ are roots of the characteristic polynomial with respective multiplicity $m_{j}$ and $A_{j, m_{j}}$ are constants (in place for $A$ , $B$ , $C$ etc.) Hopefully this clarifies where I got (2) from.","['combinatorics', 'recurrence-relations', 'generating-functions']"
4919658,"$f:\mathbb R\to [0,\infty)$ is a 3 times differentiable and $\max_{x\in\mathbb R}|f'''(x)|\le 1$. Prove that: $f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0 $","I have a problem in Analysis: Let $f:(-\infty,\infty)\to [0,\infty)$ be a three times differentiable and satisfy: $$\max_{x\in\mathbb R}|f'''(x)|\le 1$$ Prove that: $$f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0,\quad \forall x\in\mathbb R$$ Here is my solution: The Taylor series of $f(y)$ at $y=x$ is $f(y)=f(x) + f'(x)(y-x)+\dfrac{f''(x)(y-x)^2}{2}+\dfrac{f'''(\xi)(y-x)^3}{6}$ with $\xi\in \mathbb R$ . Because $f(x)\ge 0,\forall x\in\mathbb R$ and $\displaystyle\max_{x\in\mathbb R}|f'''(x)|\le 1$ , so $$0\le f(x)+f'(x)(y-x)+\frac{f''(x)(y-x)^2}{2}+\frac{(y-x)^3}{6}$$ At this point, I don't know how to continue solving the problem. Can you help me with it?","['calculus', 'derivatives', 'taylor-expansion', 'real-analysis']"
4919666,Derivation of the characteristic polynomial for homogenous difference equations,"I'm struggling to find a resource that proves every solution to the homogenous difference equation $$
a_n = c_1a_{n-1} + c_2a_{n-2} + \dots + c_da_{n-d}. 
$$ is of the form $$
a_n = k_1n^{p_1}r^n + k_2n^{p_2}r^n + \dots + k_nn^{p_n}r^n. 
$$ where $r$ is a root of the characteristic polynomial $$
q(\lambda) = \lambda^d + a_1\lambda^{d-1} + a_2\lambda^{d-2} + \dots + a_d
$$ and and $p_i$ is an integer of at least zero, but less than the multiplicity of $r$ .
From this Wikipedia article it seems one can do this using matrices / eigenvalues or generating functions, but I can't find how. I understand that from the first-order $a_n = ra_{n-1}$ we can easily see $a_n = r^na_0$ and we can ""ansatz"" from there, but I would really like to see a proof that shows how you arrive here or at least how to start. Thanks.","['discrete-mathematics', 'linear-algebra', 'recurrence-relations', 'combinatorics']"
4919670,$\int\limits_{0}^{\pi}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot\cos(x)\right)}}{\sin(x)}\;\mathrm{d}x}$,"I am trying to solve this integral: $$\int_{0}^{\pi}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot\cos(x)\right)}}{\sin(x)}\;\mathrm{d}x}$$ I know that it must have something to do with the Cin function: $$\mathrm{Cin}(x)=\int_{0}^{x}{\frac{1-\cos(t)}{t}\;\mathrm{d}t}$$ but I can't get it to take this form. The full solution (special thanks to @stoic-santiago): $$\begin{align}\Gamma&=\int_{0}^{\pi}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot\cos(\vartheta)\right)}}{\sin(\vartheta)}\;\mathrm{d}\vartheta}=\int_{0}^{\pi}{\sin(\vartheta)\cdot\frac{\cos^2{\left(\frac{\pi}{2}\cdot\cos(\vartheta)\right)}}{1-\cos^2(\vartheta)}\;\mathrm{d}\vartheta} \\
	&=\begin{bmatrix}
		u=\cos(\vartheta) & \\
		\frac{\mathrm{d}u}{\mathrm{d}\vartheta}=-\sin(\vartheta) & \mathrm{d}\vartheta=-\frac{\mathrm{d}u}{\sin(\vartheta)}
	\end{bmatrix}=-\int_{1}^{-1}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot u\right)}}{1-u^2}\;\mathrm{d}u} \\
	&=\left[\frac{1}{1-u^2}=\frac{1}{(1+u)(1-u)}=\frac{1}{2}\cdot\left(\frac{1}{1+u}+\frac{1}{1-u}\right)\right] \\
	&=-\frac{1}{2}\cdot\int_{1}^{-1}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot u\right)}}{1+u}\;\mathrm{d}u}-\frac{1}{2}\cdot\int_{1}^{-1}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot u\right)}}{1-u}\;\mathrm{d}u} \\
	&=\begin{bmatrix}
		t=1+u & \\
		\frac{\mathrm{d}t}{\mathrm{d}u}=1 & \mathrm{d}t=\mathrm{d}u
	\end{bmatrix}\;\text{and}\;\begin{bmatrix}
		v=1-u & \\
		\frac{\mathrm{d}v}{\mathrm{d}u}=-1 & \mathrm{d}v=-\mathrm{d}u
	\end{bmatrix} \\
	&=-\frac{1}{2}\cdot\int_{2}^{0}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot (t-1)\right)}}{t}\;\mathrm{d}t}+\frac{1}{2}\cdot\int_{0}^{2}{\frac{\cos^2{\left(\frac{\pi}{2}\cdot (1-v)\right)}}{v}\;\mathrm{d}v} \\
	&=-\frac{1}{4}\cdot\left[\int_{2}^{0}{\frac{2\cos^2{\left(\frac{\pi}{2}\cdot (t-1)\right)}-1+1}{t}\;\mathrm{d}t}-\int_{0}^{2}{\frac{2\cos^2{\left(\frac{\pi}{2}\cdot (1-v)\right)}-1+1}{v}\;\mathrm{d}v}\right] \\
	&=-\frac{1}{4}\cdot\left[\int_{2}^{0}{\frac{\cos{\left(\pi\cdot (t-1)\right)}+1}{t}\;\mathrm{d}t}-\int_{0}^{2}{\frac{\cos{\left(\pi\cdot (1-v)\right)}+1}{v}\;\mathrm{d}v}\right] \\
	&=-\frac{1}{4}\cdot\left[\int_{2}^{0}{\frac{1-\cos{(\pi t)}}{t}\;\mathrm{d}t}-\int_{0}^{2}{\frac{1-\cos{(\pi v)}}{v}\;\mathrm{d}v}\right] \\
	&=\begin{bmatrix}
		x=\pi t & \\
		\frac{\mathrm{d}x}{\mathrm{d}t}=\pi & \mathrm{d}t=\frac{\mathrm{d}x}{\pi}
	\end{bmatrix}\;\text{and}\;\begin{bmatrix}
		y=\pi v & \\
		\frac{\mathrm{d}y}{\mathrm{d}v}=\pi & \mathrm{d}v=\frac{\mathrm{d}y}{\pi}
	\end{bmatrix} \\
	&=-\frac{1}{4}\cdot\left[\int_{2\pi}^{0}{\frac{1-\cos{(x)}}{\frac{x}{\pi}}\;\frac{\mathrm{d}x}{\pi}}-\int_{0}^{2\pi}{\frac{1-\cos{(y)}}{\frac{y}{\pi}}\;\frac{\mathrm{d}y}{\pi}}\right] \\
	&=-\frac{1}{4}\cdot\left[\int_{2\pi}^{0}{\frac{1-\cos{(x)}}{x}\;\mathrm{d}x}-\int_{0}^{2\pi}{\frac{1-\cos{(y)}}{y}\;\mathrm{d}y}\right] \\
	&=\frac{1}{4}\cdot\left[\mathrm{Cin}(2\pi)+\mathrm{Cin}(2\pi)\right]=\frac{\mathrm{Cin}(2\pi)}{2}\end{align}$$","['integration', 'trigonometric-integrals']"
4919696,"FOB Poker: Probability of a sequence of numbers (with the existence of ""wild-card"" number)","At work we have 2 factor authentication using a fob that generates a sequence of 6 numbers 0-9. We started playing ""poker"" by having everyone generate a number at the same time and see who has poker type hands with the number sequence. There are some differences: Poker has a fixed deck so ""numbers"" do not repeat, but the fob repeats numbers obviously. We make the zeros ""wild."" There are 6 instead of 5 values. Therefore, we do not know how to rank some of the ""hands."" 2 questions: (1) Do these differences effect the probability of the standard (5 value) ""poker"" hands? (2) How do I calculate the probability of ""hands"" utilizing the 6th digit ( 3 pair, two triples, straight of 6 numbers, 6 of a kind,... etc.)? I would like to generate a ""hand"" ranking table to quell some arguments. Any help is greatly appreciated!","['statistics', 'probability']"
4919710,"Identify the sequence of $-3, -1, 3, 11, 27, ...$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last month . Improve this question Original question is - In the case below an initial term and a recursive formula are given. Find u( n ) $u( 1 ) = - 3, u( n ) = 2 u( n - 1 ) + 5$ I have tried applying my knowledge of Arithmetic Progression(AP), Quadratic Progression(QP), Geometric Progression(GP). And the most I could deduce from the sequence is that the difference bettween each term is multiplying by 2 hence it is a Geometric Sequence. $-3, -1, 3, 11, 27, ...$ $-1 - (-3) = 2$ $3 - (-1) = 4$ $11 - 3 = 8$ $27 - 11 = 16$ The goal is to identify the general term of the sequence. I would love to see the steps too.","['functions', 'sequences-and-series']"
4919747,"Confusion on using ""unless"" more than once in proposition","I'm having trouble interpreting this highlighted sentence (from Discrete Math Rosen Textbook) properly due to using unless more than once in this sentence. I understand that q unless (not p) is the same as saying p implies q, but I'm not sure how to exactly apply that rule here for this sentence. Is it equivalent to saying (by rearranging the sentence a bit): Unless every input value is tested (call it proposition ""p""), unless the correctness of the program is established (call it proposition ""q""), no amount of testing can show it produces the desired output for all input values (call it proposition ""r""). or If every input value is not tested, then if the correctness of the program is not established, no amount of testing can show it produces the desired output for all input values. or (not p) --> ( (not q) --> r) or ( (not p) and (not q) ) --> r Kindly please help me here. It might just be the commas throwing me off (my bad english lol)","['first-order-logic', 'propositional-calculus', 'logic', 'discrete-mathematics']"
4919771,"Evaluating $\int_0^{\infty}\frac{e^{-tx}\sin{t}}{t}dt, x>0$","The task is to compute $$F(x) = \int_0^{\infty}\frac{e^{-tx}\sin{t}}{t}dt, x>0 $$ and from simple integration by parts one can see that $$\int_0^{\infty}e^{-xt}\sin{t}dt = \frac{1}{1+x^2}. $$ What I tried to do was differentiate F(x) with respect to x (under the integral sign) which gave $$ \frac{d}{dx}\int_0^{\infty}\frac{e^{-xt}\sin{t}}{t}dt = \int_0^{\infty}\frac{-te^{-xt}\sin{t}}{t}dt = -\frac{1}{1+x^2}  $$ I think that the answer is then found by integrating with respect to x on both sides ( $x\in(0,\infty)$ ): $$F(x)=\int_0^{\infty}\frac{e^{-xt}\sin{t}}{t}dt = -\int_0^{\infty}\frac{1}{1+x^2}dx=[-arctan(x)]_0^{\infty} = -\frac{\pi}{2}+ 0 = -\frac{\pi}{2}  $$ But the answer sheet says that the answer is $ F(x) = -arctan(x) + \frac{\pi}{2}$ . Where did I go wrong?",['multivariable-calculus']
4919774,Differentiating Dirac delta with product rule,"I have here an equation. $$
h'(t_2) \delta(t_1 - t_2) = [h(t_2) - h(t_1)] \delta'(t_1 - t_2)
$$ I checked the equality by integrating both sides with a test function. $$
\int d t_1 \phi(t_1) \ldots \to h'(t_2) \phi(t_2) \\
\int d t_2 \phi(t_2) \ldots \to h'(t_1) \phi(t_1) 
$$ Is this equation mathematically correct? I see a very similar derivation in this answer . If correct, can this kind of equation be derived using some sort of product rule? Checking using test functions may not be very practical for more complicated expressions. E.g. $$
h'(t_1) \delta(t_1 - t_3) = \int d t_2 h(t_2) [\delta'(t_1 - t_2) \delta(t_2 - t_3) - \delta(t_1 - t_2) \delta'(t_2 - t_3)]
$$","['derivatives', 'dirac-delta', 'distribution-theory']"
4919783,Integral of $x^2+y^2$ on the domain $(x^2 + y^2)^2 \le x^2-y^2$,"I need to calculate the double integral $\iint_D (x^2 + y^2)$ $D = \{ (x, y) \in \mathbb{R}^2 \mid (x^2 + y^2)^2 \leq x^2 - y^2 \}$ The answer is $\frac{\pi}{8}$ I've tried using the substitution rule for double integral with polar transformation, and with $u = (x^2 + y^2)$ , $v = (x^2 - y^2)$ but I couldn't figure out the domain after the substitution.","['multivariable-calculus', 'calculus', 'definite-integrals']"
4919800,Shortest vector problem as hidden subgroup problem,"I posted this question on the cryptography stack exchange with a bounty, but I haven’t gotten much attention. I think part of the reason might be that I’m really interested in the use of group theory in cryptography — not cryptography and computational efficiency per se. For full transparency though, here is a link to the cryptography stack exchange post. Review of SVP and HSP SVP : Given a lattice $L$ in a normed vector space $V$ , the shortest vector problem asks you to find the point in the lattice with the smallest norm. This is a hard problem, in a certain sense of the word “hard,” and is related to cracking certain lattice-based cryptosystems like NTRU. HSP : Let $G$ be a group, $H$ a subgroup of $G$ , and $f: G \rightarrow X$ a set-map. We say that $f$ hides the subgroup $H$ if for all $a,b \in G$ , we have that $f(a) = f(g)$ if and only if $a$ and $b$ are in the same coset of $H$ . Suppose you’re given an oracle for some $f:G \rightarrow X$ , and you know that $f$ is hiding some subgroup $H$ of $G$ . The hidden subgroup problem asks if you can efficiently find a generating set for $H$ . Here efficiently is barring strategies like “compute $f(g)$ for every $g \in G$ .” Several problems that underpin cryptosystems such as factoring and computing discrete logarithms can be viewed as instances of the hidden subgroup problem. Question I am interested in whether or not there is a way to directly view the Shortest Vector Problem (SVP) in a finite dimensional lattice as an instance of the Hidden Subgroup Problem (HSP). Looking around online, I can see that SVP is intimately related to the dihedral hidden subgroup problem. Indeed, Oded Regev has given a quantum reduction from a certain unique shortest vector problem to dihedral coset sampling. The thing is, I’m not particularly interested in the quantum computation aspect — I merely want to understand if SVP can be viewed as a case of HSP, in the same way that other “hard problems” that underpin cryptosystems like factoring and discrete logarithms can. If I am understanding Regev’s result correctly, such a solution might not be possible. Regev’s reduction from Unique SVP to HSP cannot be carried out efficiently on a classical computer. But it seems that Regev is primarily interested in the quantum aspect and fleshing out a connection between lattice problems and quantum computation. So it could be that there is some other way. Any resources would be greatly appreciated. Thanks","['cryptography', 'group-theory', 'computational-complexity', 'integer-lattices']"
4919811,How can I show that $\int_0^{\frac{\pi}{2}}\sin\left(\frac{x}{2}\right) \text{arctanh}\left(\sin(2x)\right) \ dx$.,"Question : How can I show that \begin{align}
& \int_0^{\frac{\pi}{2}} \sin\left(\frac{x}{2}\right) \text{arctanh}\left(\sin(2x)\right)\,dx
\\[2mm] = & \
{\small\log\left(\left(2\sqrt{2-\sqrt{2}}+2\sqrt{2}-1\right)^{\sqrt{2+\sqrt{2}}} \left(1+2\sqrt{2}-2\sqrt{2+\sqrt{2}}\right)^{\sqrt{2-\sqrt{2}}}\right)}
\end{align} My attempt I will rewrite $\text{arctanh}(\sin(2x))$ in terms of logarithms. We know that $$
   \text{arctanh}(y) = \frac{1}{2} \ln\left(\frac{1+y}{1-y}\right)
   $$ Here $y=\sin(2x)$ $$
   \text{arctanh}(\sin(2x)) = \frac{1}{2} \ln\left(\frac{1+\sin(2x)}{1-\sin(2x)}\right)
   $$ Using trigonometric identities, we can write: $$
   1 + \sin(2x) = 1 + 2 \sin(x) \cos(x)
   $$ $$
   1 - \sin(2x) = 1 - 2 \sin(x) \cos(x)
   $$ or $\sin(2x) = 2 \sin(x) \cos(x)$ Substitute $\text{arctanh}(\sin(2x))$ : $$
   \int_0^{\frac{\pi}{2}} \sin\left(\frac{x}{2}\right) \text{arctanh}(\sin(2x)) \, dx \int_0^{\frac{\pi}{2}} \sin\left(\frac{x}{2}\right) \cdot \frac{1}{2} \ln\left(\frac{1+\sin(2x)}{1-\sin(2x)}\right) \ dx
   $$ Simplifying the logarithm expression: Using the given trigonometric identities: $$
   1 + \sin(2x) = \left(\sin\left(\frac{x}{2}\right) + \cos\left(\frac{x}{2}\right)\right)^2
   $$ $$
   1 - \sin(2x) = \left(\sin\left(\frac{x}{2}\right) - \cos\left(\frac{x}{2}\right)\right)^2
   $$ Thus, $$
   \ln\left(\frac{1+\sin(2x)}{1-\sin(2x)}\right) = \ln\left(\frac{\left(\sin\left(\frac{x}{2}\right) + \cos\left(\frac{x}{2}\right)\right)^2}{\left(\sin\left(\frac{x}{2}\right) - \cos\left(\frac{x}{2}\right)\right)^2}\right)
   $$ so our integral becomes $$
\int_0^{\frac{\pi}{2}} \sin\left(\frac{x}{2}\right) \ln\left(\frac{\left(\sin\left(\frac{x}{2}\right) + \cos\left(\frac{x}{2}\right)\right)^2}{\left(\sin\left(\frac{x}{2}\right) - \cos\left(\frac{x}{2}\right)\right)^2}\right)\, dx
$$","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4919824,Limiting growth ratio of $[x^n]f(x)^n$,"Based on a few examples (mainly data from OEIS as well as a bit of theory) I've arrived at the following conjecture: Conjecture: Let $f(x)$ be analytic at $0$ and nonlinear, and let $[x^n]f(x)$ represent the coefficient of $x^n$ in its power series. Let $$\alpha:=\lim_{n\to\infty}\frac{[x^{n+1}]\left(f(x)^{n+1}\right)}{[x^n]\left(f(x)^n\right)}$$ assuming it exists. Then $f(x)-\alpha x$ has a double root. Here's one application of this conjecture. Let $P_n$ be the probability that if you roll an $n$ -sided die $n$ times, none of the outcomes appear more than three times. Then using the above conjecture with $f(x)=1+x+\frac{x^2}2+\frac{x^3}6$ , together with some generating function know-how, reveals that $$\lim_{n\to\infty}\frac{P_{n+1}}{P_n}=0.97868\ldots=\frac{2.66032\dots}e$$ where $c=2.66032\dots$ is the unique real such that $(1+x+\frac{x^2}2+\frac{x^3}6)-cx$ has a double root. A discriminant calculation reveals that $c$ is algebraic: $8c^3-21c^2-2=0$ . (Note to anyone trying to verify this numerically: $P_{n+1}/P_n\cdot(1+\frac1n)^n$ converges faster, and then you can divide the result by $e$ .) Unfortunately, my analytic combinatorics-fu isn't strong enough for me to actually prove this conjecture, so I turn to Stack Exchange. Is this conjecture true, and if so, how can I prove it? PS. I leave it to the reader to verify that the location of the double root in question is also a root of $xf'(x)-f(x)$ . EDIT: In fact, $\alpha$ can also be described as a local extreme value of $f(x)/x$ . That might be a simpler way to describe it. PPS. This is a variation of the fact that if $\beta=\lim_{n\to\infty}[x^{n+1}]f(x)/[x^n]f(x)$ exists then its reciprocal is a pole of $f$ , that is, $f(1/\beta)=\infty$ . The simplest example of this is $f(x)=1/(1-bx)$ and $[x^n]f(x)=b^n$ . This observation is essentially the start of analytic combinatorics, if my understanding is right. EDIT: Actually, $1/\beta$ could be some type of singularity other than a pole; an example would be $f(x)=\sqrt{1-x}$ , which has a branch cut at $1/\beta=1$ .","['analytic-combinatorics', 'combinatorics', 'generating-functions']"
4919833,Integral's computation does not match WolframAlpha result,"So I'm computing the following integral: \begin{align}
\int_0^1 \frac{\tan^{-1}(x)\ln(x)}{x}dx
\end{align} I started out with a simple integration by parts which yielded: \begin{align}
-\int_0^1 \frac{\ln^2(x)}{x^2 + 1}dx
\end{align} A nifty substitution $x \to \frac{1}{x}$ and adding the new integral to the original gives: \begin{align}
-\frac 12 \int_0^{\infty} \frac{\ln^2(x)}{x^2 + 1}dx
\end{align} The integral now warrants a fairly obvious substitution $x \to \tan x$ : \begin{align}
-\frac 12 \int_0^{\frac{\pi}{2}}\ln^2(\tan x) dx
\end{align} Which may be written as: \begin{align}
-\frac 12 \frac{d^2}{ds^2} \left ( \int_0^{\frac{\pi}{2}} \tan^s (x)dx \right )_{s = 0}
\end{align} Using the beta function yields: \begin{align}
-\frac{\pi}{4} \frac{d^2}{ds^2} \left [ \sec \left ( \frac{\pi s}{2} \right ) \right ]_{s=0} = -\frac{\pi^3}{16}
\end{align} However, Wolfram Alpha notes the answer to be $-\frac{\pi^3}{32}$ , if the descrepancy has been noticed, I'd be grateful!","['integration', 'calculus']"
4919920,Evaluating $\lim_{n\to\infty}\sum_{i=1}^n \exp\left(\frac{-|x-X_i|^2}{2(\sigma/n)^2}\right)$,"Let $X_1,...,X_n\stackrel{iid}{\sim} \mu$ where has a density with respect to the Lebesgue measure on $\mathbb{R}$ : $\mu(dx)=\rho(x)dx$ . For every $x$ show $$
\lim_{n\to\infty}\sum_{i=1}^n \exp\left(\frac{-|x-X_i|^2}{2(\sigma/n)^2}\right)=\rho(x)\sigma\sqrt{2\pi}.
$$ I am very confident the result is true as it has been empirically verified. I arrived at this conjecture because of the following formal argument: \begin{align}
\sum_{i=1}^n \exp\left(\frac{-|x-X_i|^2}{2(\sigma/n)^2}\right)&\approx n \int\exp\left(\frac{-|x-y|^2}{2(\sigma/n)^2}\right)\rho(y)dy\\
&=\sigma\sqrt{2\pi}\left[\int\frac{n}{\sigma\sqrt{2\pi}}\exp\left(\frac{-|y|^2}{2(\sigma/n)^2}\right)\rho(x+y)dy\right]\\
&\xrightarrow{n\to\infty}\sigma\sqrt{2\pi}\int\delta(y)\rho(x+y)dy=\sigma\sqrt{2\pi}\rho(x)
\end{align} where I have used the law of large numbers (this is really the part where I am not being precise), the weak convergence of the gaussian to the Dirac delta, and finally the definition of the Dirac delta.","['normal-distribution', 'real-analysis', 'sequences-and-series', 'limits', 'probability']"
4919936,"The ""seashell constant"": closed form for $\frac12\exp\int_0^1-\log(\sin(\frac{\pi}{6}+\frac{2\pi}{3}x))\mathrm dx$?","I am looking for a closed form for $$
R = \frac{1}{2}\,\exp\left(-\int_{0}^{1}
\log\left(\sin\left(\frac{\pi}{6} +
\frac{2\pi}{3}\,x\right)\right){\rm d}x\right)\approx 0.6159
$$ Wolfram does not give a closed form for $R$ . Wolfram expresses the integral in terms of dilogarithms with complex arguments; is that the simplest way to express $R$ without an integral? Where $R$ comes from On a circular arc with central angle $\dfrac{4\pi}{3}$ and radius $r$ , draw $n+1$ equally spaced points, including two at the ends of the arc. Draw line segments from each of these points to the midpoint of the complementary arc. Here is an example with $n=7$ . Let $P(r,n)=\text{product of lengths of the $n+1$ line segments}$ . INTERESTING FACT: $\lim\limits_{n\to\infty}P(R,n)=R$ . I call this constant ""the seashell constant"". Proof We have $$P(r,n)=\prod\limits_{k=0}^{n}2r\sin\left(\frac{\pi}{6}+\frac{2\pi}{3}\left(\frac{k}{n}\right)\right)=\exp\left[n\cdot\color{red}{\frac{1}{n}\sum\limits_{k=0}^{n}\log\left(2r\sin\left(\frac{\pi}{6}+\frac{2\pi}{3}\left(\frac{k}{n}\right)\right)\right)}\right]$$ If $P(r,n)$ converges then the red part must converge to $0$ . The red part converges to $\int_0^1\log\left(2r\sin\left(\frac{\pi}{6}+\frac{2\pi}{3}x\right)\right)\mathrm dx$ . Setting this integral equal to $0$ gives $r=\frac12\exp\int_0^1-\log\left(\sin\left(\frac{\pi}{6}+\frac{2\pi}{3}x\right)\right)\mathrm dx$ . Call this value $R$ . Now we will show that $\lim\limits_{n\to\infty}P(R,n)=R$ . For ease of notation, let $f(x)=\log\left(\sin\left(\frac{\pi}{6}+\frac{2\pi}{3}x\right)\right)$ . $\begin{align}
\lim\limits_{n\to\infty}P(R,n)&=\lim\limits_{n\to\infty}\exp\sum\limits_{k=0}^{n}\left(\log(2R)+f\left(\frac{k}{n}\right)\right)\\
&=\lim\limits_{n\to\infty}\exp\left[\log(2R)+n\log(2R)+\sum\limits_{k=1}^{n}f\left(\frac{k}{n}\right)+f(0)\right]\\
&=\lim\limits_{n\to\infty}\exp\left[\log(2R)\color{blue}{-n\int_0^1f(x)\mathrm dx+\sum\limits_{k=1}^{n}f\left(\frac{k}{n}\right)}-\log 2\right]\\
\end{align}$ By considering riemann sums, and noting that $f(0)=f(1)$ , we can see that: $$\lim\limits_{n\to\infty}\left(\color{blue}{-n\int_0^1f(x)\mathrm dx+\sum\limits_{k=1}^{n}f\left(\frac{k}{n}\right)}\right)=0$$ $\therefore \lim\limits_{n\to\infty}P(R,n)=R$ I have asked other questions about seashells, for example here and here .","['integration', 'geometry', 'closed-form', 'infinite-product', 'constants']"
4919952,Example and intuition for Hartshorne exercise II.2.14(c),"Hartshorne's exercise II.2.14(c) has the reader prove that if $\varphi : S \to T$ is a graded homomorphism of graded rings, and $\varphi_d : S_d \to T_d$ is an isomorphism for all $d \geq d_0$ , then the induced morphism $f : \operatorname{Proj T} \to \operatorname{Proj S}$ is an isomorphism. I was able to prove the result, but I feel like I'm lacking intuition about what this should tell me about projective spaces/the proj construction. I think that an example might help me find this geometric intuition. Can anyone give an example of a graded ring homomorphism that is an isomorphism only in sufficiently large degrees? Even better would be an explanation of the geometric intuition behind the example.","['algebraic-geometry', 'projective-schemes']"
4919965,Proving that $A \subseteq B$ iff $\mathcal{P}(A) \subseteq \mathcal{P}(B)$,"I'm proving the following statement in Daniel Velleman's How to Prove It (p. 133): Prove that $A \subseteq B$ iff $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ . My proof Assume $\mathcal{P}(A) \nsubseteq \mathcal{P}(B)$ , i.e. assume $\exists C \in \mathcal{P}(A): C \notin \mathcal{P}(B)$ . This means $\exists C \subseteq A : C \nsubseteq B$ , i.e. $\exists x \in A : x \notin B$ . But this contradicts the assumption that $A \subseteq B$ . Hence $\forall C(C \in \mathcal{P}(A) \implies C \in \mathcal{P}(B))$ , i.e. $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ .
Conversely, we have that $A \in \mathcal{P}(A)$ , which implies that $A \in \mathcal{P}(B)$ since $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ . Hence $A \subseteq B$ .
Since $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ if $A \subseteq B$ and $A \subseteq B$ if $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ , $A \subseteq B$ iff $\mathcal{P}(A) \subseteq \mathcal{P}(B)$ . $\square$ Is there a way to prove the first direction of the equivalence directly instead of resorting to contradiction?","['elementary-set-theory', 'proof-writing']"
4919970,Dual space isomorphism non-canonical choice example,"In a lot of resources that I have read it is mentioned that the isomorphism between $V$ and $V^*$ is non-canonical, but I was never sure that I properly understood precisely what this means. I haven't studied category theory so I couldn't really understand some other answers on math.stack based on category theory arguments. I will try to explain how I understand things and maybe someone could comment on that. Setting Let $V$ be a finite-dimensional vector space. Its dual $V^*$ is the space of linear maps from $V$ to $\mathbb{F}$ : $V^* = L(V,\mathbb{F})$ . Now let $v$ be an arbitrary vector from $V$ . If I fix a basis $F = (f_1,\ldots,f_n)$ for $V$ then I can write $v = \sum_{i=1}^n [v]_F^i f_i$ , for some unique coefficient vector $[v]_F\in\mathbb{F}^n$ . Then as far as I understand the ""non-canonical isomorphism"" that depends on this basis is $v\mapsto\alpha = \sum_{i=1}^n [\alpha]^{F^*}_if^i = \sum_{i=1}^n [v]^i_F f^i$ where $F^* = (f^1,\ldots,f^n)$ is the dual basis w.r.t. $F$ such that $f^i(f_j) = \delta^i_j$ . Now let me choose a different basis $G = (g_1,\ldots,g_n)$ for $V$ , then the dual basis w.r.t. it is $G^*=(g^1,\ldots,g^n)$ such that $g^i(g_j) = \delta^i_j$ . Now I can write the vector $v$ in terms of the basis $G$ : $v = \sum_{i=1}^n [v]_G^i g_i$ , and I can form a functional $\beta = \sum_{i=1}^n [\beta]^{G^*}_ig^i = \sum_{i=1}^n [v]^i_G g^i$ . For $\alpha$ and $\beta$ to be well-defined (i.e. so that the values $\alpha(u)$ and $\beta(u)$ are fixed for every $u\in V$ irrespective of the coordinate representation), their coordinates must transform in an appropriate manner under a change of basis. Let $P$ be the change of basis matrix such that $g_i = \sum_{j=1}^n f_j P^j_i$ then in order to have $f^i(f_j) =\delta^i_k \implies g^i(g_j) = \delta^i_j$ I need that $g^i = \sum_{j=1}^n (P^{-1})^i_j f^j$ since: $$g^i(g_j) = \sum_{k=1}^n (P^{-1})^i_k f^k \left(\sum_{l=1}^n P^l_j f_l\right) = \sum_{k=1}^{n}\sum_{l=1}^n(P^{-1})^i_kP^l_jf^k(f_l) = \sum_{k=1}^n(P^{-1})^i_kP^k_j = \delta^i_j.$$ If I make it so that I transform the coordinates as follows: $[\alpha]^{G^*} = [\alpha]^{F^*}P$ and $[v]_G = P^{-1}[v]_F$ , then I should get the same result for $\alpha(v)$ regardless of the chosen basis. Of course now $[\alpha]^{G^*}\ne[v]_G^T=[\beta]^{G^*}$ in general showing that $\alpha\ne \beta$ . The Question Is the ""non-canonical isomorphism"" referring to the fact that $\alpha\ne \beta$ in the general case? That is, the construction of $\alpha$ was a function of my choice of $f_1,\ldots,f_n$ , while the construction of $\beta$ was a function of my choice of $g_1,\ldots,g_n$ . I assume that $\alpha$ could miraculously be equal to $\beta$ in some cases (e.g. if $f_1=g_1$ and $[v]^i_F=0=[v]^i_G$ for $i>1$ ), but in general it is not, so the non-canonicity is referring to the dependence on the choice of basis? If I had a canonical basis, for example $(e_1,\ldots,e_n)$ in $\mathbb{R}^n$ , would this then be considered a canonical choice instead? On the other hand, if I have a non-degenerate bilinear form $B:V\times V\to\mathbb{F}$ , I can construct an isomorphism $\gamma_v = B(v,\_)$ , that is independent of the basis. So is the word canonic referring to independence of the choice of basis in the initial definition of the functional? Example If I take a basis for the space of polynomials of at most degree $1$ then I could choose the monomial basis $(f_1,f_2) = (1,x)$ or the Lagrange basis $(g_1,g_2) = \left(\frac{x-x_0}{x_1-x_0}, \frac{x-x_1}{x_0-x_1}\right)$ . The dual bases are given as $(f^1,f^2) = \left(\delta_0, \delta_0 \circ \frac{d}{dx}\right)$ and $(g^1,g^2) = \left(\delta_{x_1},\delta_{x_0}\right)$ ,
where $\delta_p(f) = f(p)$ . Now if I have the vector $v = a\frac{x-x_0}{x_1-x_0} + b\frac{x-x_1}{x_0-x_1}$ then $v = \frac{bx_1-ax_0}{x_1-x_0}+\frac{a-b}{x_1-x_0}x = p\cdot 1 + qx $ . This induces the two functionals $\alpha = p\delta_0 + q\delta_0 \circ \frac{d}{dx}$ and $\beta = a\delta_{x_1}+b\delta_{x_0}$ . Now $\beta(v) = a^2+b^2$ while $\alpha(v) = p^2+q^2$ , and in general those are unequal, thus $\alpha\ne\beta$ . However if I were to take for example the inner product $\langle v, w\rangle = \int_{0}^{1} v(t) w(t)\,dt$ then I could define the ""canonical isomorphism"" $\alpha_v = \langle v, \_\rangle$ . Now a subsequent question is - didn't I just swap the choice of the initial basis for a choice of an inner product? Because I could of course take a number of other inner products that would result in different functionals. In what sense is the choice of inner product more canonical than the choice of basis? Or did I misunderstand this, and a choice of inner product doesn't result in a canonical isomorphism either. But if I go with that and I presuppose that the choice is canonical only when a canonical inner product exists, wouldn't this be the same argument as presupposing that a canonical basis exists? Basically I have trouble making sense of what ""canonical"" is referring to in the first place. Canonical Isomorphism to the Double Dual $V^{**}$ The isomorphism $v\mapsto v^{**}(\phi) = \phi(v)$ is considered ""canonical"" , I assume because there is no choice whatsoever in the above definition - I neither choose a basis nor an inner product. So is the above the only thing that ""canonical"" can refer to? Or is a vector space with a bilinear form isomorphism to $V^*$ also considered ""canonical"" if I assume that there is some distinguished canonical bilinear form? And similarly a choice $v\mapsto \alpha = \sum_{i=1}[\alpha]^{E^*}_ie^i = \sum_{i=1}^n[v]^i_E e_i$ would be considered ""canonical"" if there is some distinguished ""canonical"" basis $E$ ? Essentially, what do I need for something to be considered canonic? Edit: Attempt at a solution of the exercise from coiso For $(V,\mathbb{F}_2)$ with $\dim = 1$ , with $V=\{0,x_1\}$ and $V^*=\{0^*,y^1\}$ I can construct the table: \begin{equation}
\begin{array}{c|c}
& 0^* & y^1 \\
\hline
0 & 0 & 0 \\
\hline
x_1 & 0 & 1 
\end{array}
\end{equation} If I assign $0\ne x_1\mapsto 0^*$ and $0\mapsto y^1$ then $0^* = (x_1)^* = (x_1+0)^* = 0^* + y^1 = y^1$ , and the assignment is not injective hence not an isomorphism. So there is only one valid choice and then $V\cong V^*$ . For $(V,\mathbb{F}_2)$ with $\dim = 2$ , with $V=\{0,x_1,x_2,x_{12}=x_1+x_2\}$ and $V^* = \{0^*, y^1, y^2, y^{12}\}$ I can construct the table: \begin{equation}
\begin{array}{c|c|c|c|c}
& 0^* & y^1 & y^2 & y^{12} \\
\hline
0 & 0 & 0 & 0 & 0 \\
\hline
x_1 & 0 & 1 & 0 & 1 \\
\hline
x_2 & 0 & 0 & 1 & 1 \\
\hline
x_{12} & 0 & 1 & 1 & 0
\end{array}
\end{equation} I cannot have any other columns in the table as otherwise the functionals won't be linear.
One possible assignment, when taking $x_1,x_2$ as basis vectors for $V$ is $0\mapsto 0^*$ and $x_J\mapsto y^J$ . I could of course take $x_1,x_{12}$ instead as a basis, then the dual basis must be $x_1\mapsto y^{12}$ and $x_{12}\mapsto y^2$ . Then the only option left is $x_2\mapsto y^1$ . I don't get why any of the two assignments should be more canonical, unless I am missing something and the second assignment breaks linearity, which doesn't seem obvious from the table. Edit 2: Other Isomorphisms to the Double Dual Thinking about this I realized that I could define the map $I_{\lambda} : V\to V^{**}$ , $v\mapsto I_{\lambda}(V) = \lambda v^{**}$ such that $I_{\lambda}(v)(\phi) = \lambda v^{**}(\phi) = \lambda \phi(v)$ for $\lambda \ne 0$ . This map looks like it is bijective, and it is linear since $I_{\lambda}(au + bv)(\phi) = \lambda \phi(au+bv) = a\lambda\phi(u)+b\lambda\phi(v) = (aI(u)+bI(v))(\phi)$ , so it must be an isomorphism. Yet I suppose this would not be termed canonical, or will it? To my understanding it is the map $I_{1}(v)(\phi) = \phi(v)$ that is the canonical isomorphism, i.e., where $\lambda=1$ . Clearly the above family of isomorphisms $\{I_{\lambda}\,:\,\lambda \ne 0\}$ do not depend on a choice of basis or inner product, and yet I do not think that they would be termed canonical. So I am even more confused now, as clearly ""canonical"" is not referring to a choice of basis or inner product in this case. Or is it that all $I_{\lambda}$ are considered canonical isomorphisms? I also found this video , I don't know how relevant it is to the above setting though. From the video it becomes clear that canonical isomorphism is a kind of equality that is true for a certain subset of statements, but not all. So the question is what is canonical referring to in the above cases. Are all $I_{\lambda}$ canonical isomorphisms or is $I_1$ the only canonical isomorphism? Is $v\mapsto B(v, \_)$ canonical if there is only a single non-degenerate bilinear form $B$ given, i.e. $(V,B)\stackrel{canon}{\cong} (V^*, B^*) = (V^*, B(B^{-1}, B^{-1})$ but $V\stackrel{non-canon}{\cong} V^*$ , and also $(V,(B_1, B_2)) \stackrel{non-canon}{\cong} (V^*, (B^*_1, B^*_2))$ . Is the isomorphism canonical if a single basis is specified but no more, i.e. $(V, E) \stackrel{canon}{\cong} (V^*, E^*)$ but $(V, (E_1, E_2)) \stackrel{non-canon}{\cong} (V, (E_1^*, E_2^*))$ ? Also in the answers Mikhail Katz mentioned that in differential geometry the map may be considered non-canonical if it is not continuous, but I assume this is slightly different, as I don't need a notion of continuity to talk about the algebraic dual and double dual as far as I know. Edit 4: What I Gathered so Far There are many isomorphisms $V\to V^{**}$ but the canonical one is given by the evaluation functional $\Lambda:V\to V^{**}$ : $$\Lambda(v)(\phi) := \phi(v), \quad \forall \phi\in V^*.$$ According to coiso also the isomorphisms $I_{\lambda}(v)(\phi) = \lambda\phi(v)$ for $\lambda\ne 0$ can be termed canonical, but they are not the canonical isomorphism, which is instead given by the evaluation functional $\Lambda(v)$ . As far as I understood the motivation for calling those canonical is because they do not require making a choice of basis, and we care greatly about results being coordinate invariant in linear algebra, differential geometry, physics, etc. The isomorphism $J : (V,B) \to (V^*,B^*)$ where $J(v) = B(v,\_)$ is also canonical if $B$ is a non-degenerate bilinear form (conjugate-linear in one of the slots in the complex-valued case). If $B$ is degenerate then the latter is not bijective so not an isomorphism, but I guess that restricted to the subspace where it is non-degenerate it will be a bijection and thus a canonical isomorphism on that subspace. Arturo Magidin made it clear that the isomorphism $J$ is canonical because it is between the vector space along with its inner product/bilinear form $(V,B)$ and its counterpart $(V^*,B^*)$ . As far as I understood a map $J_B : V\to V^*$ where $J_B(v) = B(v,\_)$ would not provide a canonical isomorphism between $V$ and $V^*$ by themselves, since one could choose a different bilinear form, which would result in a different identification. The isomorphism $K_F:V\to V^*$ : $$K_F(v) = \sum_{i=1}^n [v]^i_F f^i \in V^*,$$ where $F = (f_1,\ldots,f_n)$ is a basis for $V$ and $F^*=(f^1,\ldots,f^n)$ is the dual basis for $V^*$ such that $f^i(f_j) = \delta^i_j$ , is not a canonical isomorphism between $V$ and $V^*$ in general. This is because it depend on a choice of basis $F$ , and as illustrated by my above examples a different basis can result in a different result. A special case when there is a canonical isomorphism is when $\dim V\leq 2$ and the underlying field is $\mathbb{F}_2$ as given in the solution by coiso. I believe that also a one-dimensional space is canonically isomorphic to its dual (even if it is not over $\mathbb{F}_2$ ) since the map $K_F$ and $K_G$ actually produce the same functionals irrespective of the choice of $F$ and $G$ (the scalar difference gets canceled out by the change of basis). Finally, supposedly also $K : (V,F) \to (V^*,F^*)$ where $K(v) = K_F(v)$ should also be a canonical isomorphism since I am now identifying a vector space along with a basis. The latter is more restrictive than providing a non-degenerate bilinear form however.","['change-of-basis', 'linear-algebra', 'vector-space-isomorphism', 'dual-spaces']"
4919993,Find a partial area of a trapezoid,"The formula for the area of a trapezoid is $$A = \frac{(a+b)}{2}h$$ where a and b are the length of each base and h is the trapezoid's height. So I want to figure out the area of a portion of the trapezoid. Instead of a height of h, I want to figure it out for a height of g. But this portion of the trapezoid still contains the complete smaller base of the original trapezoid. Let a be the smaller base. Because the height is now g (g < h), the second base, b, will be smaller, let's call that new base, f.  So, $$f = a + \frac{g}{h}b$$ So then the area of this portion of the original trapezoid is $$A_p = \frac{(a+f)}{2}g = \frac{2a+\frac{g}{h}b}{2}g$$ Is this correct, particularly the formula for calculating $f$ ? P.S.  I've added a drawing.",['geometry']
4920012,Integrating $\int{\frac{dx}{\sqrt{(x-a)(b-x)}}}$ two ways gives very different-looking answers. How to show algebraically they differ by a constant?,"From Apostol's Calculus Volume 1 2nd ed. , 6.22 #46, the task is to integrate $$\int{\frac{dx}{\sqrt{(x-a)(b-x)}}}$$ Method 1 : The provided hint is to use the substitution $x-a=(b-a)\sin^2(u)$ . Thus $b-x=(b-a)\cos^2(u)$ and $dx=2(b-a)\sin u \cos u$ . The integral simplifies to $$\int{\frac{2(b-a)}{|b-a|}du}$$ The result is $$\frac{2(b-a)}{|b-a|}\;\arcsin{\sqrt{\frac{x-a}{b-a}}} \;+ C \tag1$$ Method 2 : I expand the denominator and complete the square, yielding: $$\int{\frac{dx}{\sqrt{\left(\frac{b-a}{2}\right)^2-\left(x-\frac{b+a}{2}\right)^2}}}$$ which results in: $$\arcsin{\frac{2x-b-a}{|b-a|}} \;+ C \tag2$$ I differentiated both results using Wolfram Alpha and got back the original function. Their graphs are the same, just offset by a constant ( 1 and 2 ). These functions differ by only a constant, and yet they are so distinct in appearance! One has a square root inside the arcsin, and one does not. I am curious to know whether we can algebraically convert one form to the other. I asked Wolfram Alpha to do that it and it failed. Might a reader here have any ideas?","['integration', 'calculus', 'inverse-trigonometric-functions', 'trigonometry']"
4920019,Number of unit cubes meeting the boundary of a convex set,"Suppose $C \subseteq [0,n)^k$ is a convex set, and $\partial C$ is its topological boundary: its closure minus its interior. Is it true that $\partial C$ meets at most $2kn^{k-1}$ unit cubes in [0,n)^k? By a unit cube, I mean a set of the form $[n_1,n_1 + 1) \times [n_2, n_2 + 1) \times \ldots \times [n_k, n_k + 1)$ ? where $n_1, \ldots, n_k$ are integers. I'm motivated by the fact that if $C_1 \subseteq C_2 \subseteq \mathbb{R}^k$ are convex, then their surface areas are related by $\text{SA}(C_1) \leq \text{SA}(C_2)$ . So the surface area of $C \subseteq [0,n)^k$ is maximized when $C = [0,n)^k$ which has surface area $2k n^{k-1}$ . The number of unit cubes that meet $\partial C$ should be roughly its surface area.","['convex-geometry', 'discrete-geometry', 'geometry']"
4920046,How many turns will it take for this coin flipping game to end?,"Here is a problem (coin flipping game) I recently thought about. Suppose there are 2 Discrete Time Markov Chains: Markov Chain A (3-state chain): $$
X_t  = 
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & 1/3 \\
0 & 0 & 1 \\
\end{bmatrix}
$$ Markov Chain B (4-state chain): $$
Y_t  = 
\begin{bmatrix}
1/4 & 1/4 & 1/4 & 1/4 \\
1/4 & 1/4 & 1/4 & 1/4 \\
0 & 0 & 1 & 0 \\
1/4 & 1/4 & 1/4 & 1/4 \\
\end{bmatrix}
$$ In this problem, we start in Chain A in state 1. At each new time point, we flip a 2-sided fair coin : If heads, we stay in the same state we are currently in but move to Chain B. If tails, we stay in Chain A. The first time we move from Chain A to Chain B, we permanently stay in Chain B Once we reach State 3 (whether in Chain A or in Chain B), we stop forever. I tried to represent all of this mathematically: $$
B_t = 
\begin{cases} 
1 & \text{with probability } p \\
0 & \text{with probability } 1-p 
\end{cases}
$$ $$
S_t = \sum_{i=0}^{t} B_i
$$ $$
Z_t = 
\begin{cases} 
X_t & \text{if } S_t = 0 \\
Y_t & \text{if } S_t \geq 1 
\end{cases}
$$ $$
T = \min \{ t \geq 0 : Z_t = 3 \}
$$ Based on this, I am interested in answering the following types of questions: On average, what percent of the time will $Z_t$ stop in $X_t$ vs $Y_t$ ? Is it possible to find out the joint probability distribution of (number of steps to switch from $X_t$ to $Y_t$ , number of steps for process to stop)? I am not sure how to answer these questions, so I wrote R simulations to try and answer them numerically. For example: library(tidyverse)
library(dplyr)
library(ggplot2)
library(RColorBrewer)


simulate_markov_chain <- function(simulation_num) {
    # Transition matrices
    transition_matrix_A <- matrix(c(1/3, 1/3, 1/3,  # probabilities from state 1
                                    1/3, 1/3, 1/3,  # probabilities from state 2
                                    0,   0,   1),   # probabilities from state 3
                                  nrow = 3, byrow = TRUE)
    
    transition_matrix_B <- matrix(c(1/4, 1/4, 1/4, 1/4,  # probabilities from state 1
                                    1/4, 1/4, 1/4, 1/4,  # probabilities from state 2
                                    0,   0,   1,   0,    # probabilities from state 3
                                    1/4, 1/4, 1/4, 1/4), # probabilities from state 4
                                  nrow = 4, byrow = TRUE)
    
    # Initial state and chain
    state <- 1
    chain <- ""A""
    
   
    path_df <- data.frame(iteration = 1, chain = chain, state = state)
    
    # Simulate 
    iteration <- 1
    while (state != 3) {
        # Flip a coin
        coin_flip <- sample(c(""heads"", ""tails""), size = 1, prob = c(0.5, 0.5))
        
        # Check if switch to chain B
        if (coin_flip == ""heads"" || chain == ""B"") {
            chain <- ""B""
            state <- sample(1:4, size = 1, prob = transition_matrix_B[state, ])
        } else {
            state <- sample(1:3, size = 1, prob = transition_matrix_A[state, ])
        }
        
       
        iteration <- iteration + 1
        path_df <- rbind(path_df, data.frame(iteration = iteration, chain = chain, state = state))
    }
    
   
    path_df$simulation_num <- simulation_num
    
    return(path_df)
}



# simulation 1000 times 
results <- map_dfr(1:1000, simulate_markov_chain)




############################################################


results <- results %>%
    group_by(simulation_num) %>%
    mutate(transition_iteration = ifelse(chain == ""B"" & lag(chain) == ""A"", iteration, NA),
           max_iteration = max(iteration)) %>%
    ungroup()



transition_iterations <- results %>% 
    group_by(simulation_num) %>% 
    summarise(transition_iteration = min(transition_iteration, na.rm = TRUE),
              max_iteration = max(iteration))


# Filter out rows with Inf values
transition_iterations <- transition_iterations %>% filter(!is.infinite(transition_iteration), !is.infinite(max_iteration))

density_estimate <- MASS::kde2d(x = transition_iterations $transition_iteration, 
                                y = transition_iterations$ max_iteration, 
                                n = 100)

filled.contour(x = density_estimate $x, 
               y = density_estimate$ y, 
               z = density_estimate$z, 
               color.palette = colorRampPalette(brewer.pal(9, ""Greens"")),
               plot.title = title(main = ""Contour Plot of Transition Iteration vs Max Iteration"",
                                  xlab = ""Transition Iteration"", 
                                  ylab = ""Max Iteration""),
               plot.axes = {axis(1); axis(2); }) As we can see, the majority of the games finish relatively early on (i.e. bottom left corner of the plot). Note that sometimes $Z_t$ never transitions to $Y_t$ therefore the transition time is Infinite and I had to remove these rows for the simulation. I am not sure if a probability distribution can be defined with such infinite values. But analytically (i.e. without simulations), is it possible to answer the following questions? On average, what percent of the time will $Z_t$ stop in $X_t$ vs $Y_t$ ? (i.e. stopping in Chain A vs stopping in Chain B) Is it possible to find out the joint probability distribution of (number of steps to switch from $X_t$ to $Y_t$ , number of steps for process to stop)? Within a given simulation, after $n$ turns of playing the game, what is the probability we are in Chain A? Normally I would have used the Fundamental Matrix approach (e.g. https://en.wikipedia.org/wiki/Absorbing_Markov_chain ), but I am not sure if it can be applied here. Thanks! Note: I made this visualization for the percent of games in Chain A vs Chain B as the number of turns in a simulation increase: library(ggplot2)
chain_counts <- results %>%
    group_by(iteration, chain) %>%
    summarise(n = n(), .groups = ""drop"")

total_counts <- results %>%
    group_by(iteration) %>%
    summarise(total = n(), .groups = ""drop"")

percentages <- inner_join(chain_counts, total_counts, by = ""iteration"") %>%
    mutate(percentage = n / total * 100)


ggplot(percentages, aes(x = iteration, y = percentage, fill = chain)) +
    geom_bar(stat = ""identity"", position = ""dodge"") +
    labs(x = ""Simulation Number"", y = ""Percentage"", fill = ""Chain"") +
    theme_minimal() +
    scale_fill_brewer(palette = ""Set1"") +
    scale_x_continuous(breaks = seq(min(percentages $iteration), max(percentages$ iteration), by = 1)) +ggtitle(""Length of Game vs Percentage of Games in Chain A/ Chain B"")",['probability']
