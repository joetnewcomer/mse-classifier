question_id,title,body,tags
1409095,Uniform convergence of characteristic functions implies uniform convergence of distribution,"Let $F(x)$ and $(F_{n})_{n\geq 1}$ be some distribution functions and let
$\varphi(t)$ and $(\varphi_{n})_{n\geq 1}$ be their respective characteristic functions. I am trying to show that if: $\sup_t |\varphi_n(t) -\varphi(t)| \rightarrow 0$, then $\sup_x |F_n(x) -F(x)| \rightarrow 0$. By Levy's theorem, I know that $F_n$ converges weakly to $F$. And I also know that, if $F$ is continuous, I have the result I want. So I believe I should use the uniform convergence of the characteristic functions to show that $F$ must be continuous. I am trying to show that I must have: $\int_{-\infty}^{\infty} |\varphi(t)|dt < \infty$ which would imply that $F$ is continuous, but I am stuck at that. Anyone can shed some light? Thanks!","['probability-theory', 'characteristic-functions', 'weak-convergence']"
1409123,Every commutative ring is a quotient of a normal ring?,"In the book Étale cohomology by Milne I found on p. 37 (in the context of constructing the henselization of a local ring) the following claim: 
""Every ring is a quotient of a normal ring"". The same is used in Local rings by Nagata on p. 180. Why is this true? Both Milne and Nagata don't explain this so it seems I'm just too blind. Is this only true for $R$ a local ring or is it also true for any commutative ring? Is there a global version of this statement? Is it true that for any scheme $X$ there is a locally closed embedding $X \hookrightarrow Y$ into a normal scheme $Y$? Is there an official term for such an embedding? EDIT . Perfect answer by Pete below led me to the following additional question I should have asked above already: Is any noetherian ring a quotient of a normal noetherian ring?","['algebraic-geometry', 'schemes', 'commutative-algebra']"
1409145,Littlewood Richardson rules for the orthogonal group SO(d),"I have a question related to tensor products of Young diagrams. More precisely, I know the Littlewood Richardson rules for the general linear group GL(d) and would like to know the restriction of these rules for the orthogonal group SO(d). Explicitly, the Littlewood Richardson reads (as extracted from http://arxiv.org/abs/hep-th/0611263 ): The decomposition of an outer product $\mu\cdot\nu$ of irreducible
representations $\mu$ and $\nu$ of $\mathfrak{S}_{n_1}$ and
$\mathfrak{S}_{n_2}$, respectively, into irreducible
representations of $\mathfrak{S}_{n_1+n_2}$ can be determined by
means of the following algorithm involving Young diagrams. The
product is commutative, so it does not matter which factor is
regarded as the ``right-hand'' one. [In practice, on should choose
the simpler Young diagram for that role.] (I) Label each box in the top row of the right-hand diagram, $\nu$,
    by $a$, each box in the second row by $b$, etc. (II) Add the labeled boxes of $\nu$ to the left-hand diagram $\mu$,
    one at a time, first the $a$s, then the $b$s, ..., subject to these constraints: (A) No two boxes in the same column are labeled with the same letter; (B) At all stages the result is a legitimate Young diagram; (C) At each stage, if the letters are read right-to-left along the rows, from top to bottom, one never encounters more $b$s than $a$s, more $c$s than $b$s, etc. (III) Each of the distinct diagrams constructed in this way specifies an
  irreducible subrepresentation $\lambda$, appearing in the decomposition of
  the outer product. The same labeled Young diagram may arise in more than one way;the multiplicity of that representation must be counted accordingly. Thanks already for any references or comments !","['group-theory', 'tensor-products', 'young-tableaux']"
1409167,Gradient flow of Dirichlet energy,"I have heard that the gradient flow of the Dirichlet energy gives a solution of the heat equation, i.e. if $u(t,x) \in C^1( [0,\infty) \times \mathbb R^d)$ solves 
$$ u_t(t,x) = - dE(u(t,x)), $$
where $$  E(u) := \dfrac{1}{2} \int \|\nabla_x u\|^2, $$
then
$$ \partial_t u = \Delta_x u := \sum_i \partial_{x_i,x_i}^2 u. $$ As I am trying to prove this fact, I have some problems in calculating the Frechet derivative of the Dirichlet energy $dE$.
If I understand correctly, fixed $f$, $dE$ is a linear operator from $C^1$ into $\mathbb R$ which should satisfy
$$ \lim_{\|h\|_{C^1} \to 0} \dfrac{E(f+h) - E(f) - dE(h) }{\|h\|_{C^1}} = 
\lim_{\|h\|_{C^1} \to 0} \dfrac{(\int 2 \nabla f \cdot \nabla h + \nabla h \cdot \nabla h) - dE(h)}{\|h\|_{C^1}} = 0 ,$$
but how to find it?
Or should I not compute the differential of $E$ at all?","['gradient-flows', 'reference-request', 'real-analysis', 'partial-differential-equations']"
1409180,Zero Gaussian curvature and restriction estimates,"Let 
  $$
  \left(\int_M|\hat{f}|^qd\mu(\xi)\right)^{1/q}\leq c||f||_{L^p(\mathbb{R}^n)}
  $$
  be a restriction estimate for a hypersurface $M\subset\mathbb{R}^n,~1<p<\infty$ and $\mu$ a surface measure. If we want nontrivial results when analysing this inequality, then the hypersurface $M$ must have some Gaussian curvature. Why is this? What happens if $M$ has zero Gaussian curvature? Specifically, what is the connection between the Gaussian curvature of the hypersurface, and establishing the restriction estimate? The above inequality can be read as ""we can apply the surface Fourier transform to the hypersurface under $L^q$ norm and extend it to $f$ on $\mathbb{R}^n$"". So why does the Gaussian curvature of $M$ matter? Thank you in advance for your help.","['fourier-restriction', 'fourier-analysis', 'real-analysis', 'harmonic-analysis', 'differential-geometry']"
1409195,How to find $ab+cd$ given that $a^2+b^2=c^2+d^2=1$ and $ac+bd=0$?,It is given that $a^2+b^2=c^2+d^2=1 $ And it is also given that $ac+bd=0$ What then is the value of $ab+cd$ ?,"['contest-math', 'algebra-precalculus']"
1409197,Induced group homomorphism $\text{SL}_n(\mathbb{Z}) \twoheadrightarrow \text{SL}_n(\mathbb{Z}/m\mathbb{Z})$ surjective?,"Let $n, m > 1$. The map $\mathbb{Z} \twoheadrightarrow \mathbb{Z}/m\mathbb{Z}$, of reduction mod $m$, induces a group homomorphism $F: \text{SL}_n(\mathbb{Z}) \to \text{SL}_n(\mathbb{Z}/m\mathbb{Z})$. My question is, is $F$ surjective or not?","['abstract-algebra', 'determinant', 'number-theory', 'matrices']"
1409202,"Why do ""the Dynkin components of a weight play the role of eigenvalues with respect to the generators $H^i$ of the Cartan subalgebra""?","In the book ""Symmetries, Lie Algebras and Representations: A Graduate Course for Physicists"" by Jürgen Fuchs,Christoph Schweigert the authors write ""In the description of representations, the Dynkin components of a weight play the role of eigenvalues with respect to the generators $H^i$ of the Cartan subalgebra."" They don't explain this any further, thus does anyone understand why this is the case? In my understanding there are three commonly used bases for the weight space The simple root basis $$ r =(a_1,a_2,a_3,\ldots)=a_1 \alpha_1 + a_2 \alpha_2 + a_3 \alpha_3 +\ldots,$$ where $α_i$ denotes the simple roots. The fundamental weight basis (=Dynkin basis),
$$ w =(b_1,b_2,b_3,\ldots)=b_1 \omega_1 + b_2  \omega_2 + b_3  \omega_3 +\ldots,$$ where $\omega_i$ denotes the fundamental weights. We can change between these two bases using the Cartan matrix and its inverse. The H-basis , where we write each weight or root in terms of the eigenvalue of the Cartan generators $H_i$, when they act on them: $$ H_i r = \lambda_i r \rightarrow r=( \lambda_1, \lambda_2, \lambda_3, \ldots ) $$ What the authors say in the quote above is basically that the second and third basis are the same. Is it because we have some freedom in choosing a basis for the Cartan subalgebra ? In other words, are the second and third basis in my list simply different choices of a basis for the Cartan subalgebra? If yes, why?","['representation-theory', 'lie-groups', 'abstract-algebra', 'group-theory', 'lie-algebras']"
1409234,"Each number in a subset $S\subseteq \{1,\ldots,2n\}$ does not divide another one. Then $\max |S|$?","This problem comes from a seemingly innocuous question from a professor during a lesson for a Math Olympiad course. [A part of this question is really a classic of number theory/combinatorics] Problem: Let $S$ be a a non-empty subset of $\{1,2,\ldots,2n\}$. It is clear that if $|S|$ is ""enough large"" then, independently from the choice of $S$, there are for sure some distinct $a,b \in S$ such that $a$ divides $b$. Then, what is the maximal $k$ such that, for some $S$ with cardinality $k$, if $a,b$ are distinct members of $S$ then $a$ does not divide $b$? [Ps. I know the solution]","['elementary-number-theory', 'divisibility', 'combinatorics']"
1409252,Solving this 2nd Order non-homogeneous PDE,"I am trying to solve the following equation: $$3u_{xx} - 10u_{xt} - 3u_{tt} = \sin(x + t)$$ I know that the left hand side is a quadratic equation which I have to factorise. Then I let one of the factors equal to $v$ and solve the first order non-homogeneous PDE. But I don't know how to do this. I know the solution to this equation is: $u(x, t) = f(3x - t) + g(x - 3t) + \frac{1}{16}\sin(x + t)$ Thanks for your help!","['differential', 'wave-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
1409316,An upper bound for a function,"I am trying to find an upper bound $b\ge f(x)~\forall x\ge0$ for the following function:
$$f(x)=\frac{x}{(w+ux^2)^2},$$
where $w,u>0$ are parameter values. I am interested in the positive domain $0\le x<\infty$. As $x\to0$, $f(x)\to 0$, and as $x\to\infty$, $f(x)\to0$. Since $f(x)>0$ for all $x>0$ and since it is continuous, a maximum exists. If the function were simpler, I would solve for the arg max $x^*$ and substitute in the function to get the exact maximum (smallest upper bound) $f(x^*)$. However, setting $f'(x)=0$ yields a fourth degree polynomial, so substituting the exact $x^*$ that maximizes $f(x)$ is not practical (though technically possible). How can I determine an upper bound, even if it is larger than the actual maximum? I am looking for an analytical technique, i.e., an approach that gives me a condition as a function of the parameter values $w,u$. Ideally, the bound would be close to the maximum. Thanks!","['optimization', 'inequality', 'algebra-precalculus', 'functions']"
1409350,What is an Eigenbasis and how do I calculate it with the information below.,"I have the matrix $$A = \begin{bmatrix}
4 & 2 & 2\\
2 & 4 & 2\\
2 & 2 & 4
\end{bmatrix}$$ I've calculated the Eigenvalues and Eigenvectors as follows with help in a previous question: $\lambda = 2$ or $8$ $$E_{\lambda = 2} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} + b \begin{bmatrix} -1\\ 0\\ 1 \end{bmatrix} \mid a, b \in \mathbb R \right\}$$ $$E_{\lambda = 8} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a \begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix} \mid \mathbb R\right\}$$ Is the Eigenbasis simply: $$E_{\lambda = 2} = \operatorname{span}\left(\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} \begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix} \right) $$ $$E_{\lambda = 8} = \operatorname{span}\left(\begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix}\right)$$","['proof-explanation', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'linear-algebra']"
1409364,Continuous functions with domain in the Natural Numbers,"Can functions with domain in the Natural Numbers be continuous?
In the high school, it is teached an intuitive notion of continuous functions: functions which will always appear as an ""unbroken curve"", no matter how close one zooms in the graph. This would be the opposite of discrete functions, which would appear as ""dots"" if one zooms close enough. In Analysis, this intuitive notion is abandoned to make way for formalism. Rudin, in ""Principles of Mathematical Analysis"" defines continuous functions as it follows: Which, for me, can be written in formal language as: $\forall\varepsilon>0\quad\exists\delta>0 \quad\forall x \in E\quad  ((d_{x}(x,p)<\delta)\implies(d_{y}(f(x),f(y))<\epsilon))$ Functions with domain in the natural numbers will appear as dots in a graph, instead of an unbroken curve. I am trying to formally prove that no function with domain on the Naturals is continuous, can somebody help me out? PS: What is the domain of $\varepsilon$  and $\delta $?","['analysis', 'elementary-number-theory']"
1409368,Sum of $\sum_{n=1}^{\infty}(-1)^{n+1}\frac{x^{2n-1}}{2n-1}$,"I've been working with the series: $$\sum_{n=1}^{\infty}(-1)^{n+1}\frac{x^{2n-1}}{2n-1}$$ From the ratio test it is clear that the series converges for $|x| < 1$, but I'm unable to obtain the sum of the series. I'm looking for any hint of how to obtain the sum. Thanks in advance!","['sequences-and-series', 'real-analysis']"
1409372,"What is the expected length of the largest run of heads if we make 1,000 flips?","Is there a way to calculate on average, the maximum amount of times we can expect a coin to land heads during 1,000 flips? So the answer (and formula if one exists) I am looking for would be something like: during 1,000 flips we can expect a maximum run of 12 heads in a row.",['probability']
1409392,Various forms of the Confluent Heun Equation,"The Confluent Heun equation is expressed in various forms. It's non-symmetrical canonical form is:
\begin{equation}
\frac{d^{2}y}{dx^{2}}+(\beta+\frac{\gamma}{x}+\frac{\delta}{x-1})\frac{dy}{dx}+\frac{\alpha\beta x-q}{x(x-1)} y =0
\end{equation} with a particular transformation it is possible to change it to a Schrodinger-like equation:
\begin{equation}
\frac{d^{2}y}{dx^{2}}+(A+\frac{B}{x}+\frac{C}{x-1}+\frac{D}{x^{2}}+\frac{E}{(x-1)^{2}})y=0
\end{equation} What is the transformation in order to go from one form to another? and how are $A, B, C, D$ and $E$ related to $\alpha, \beta, \gamma, \delta$ and $q$?","['ordinary-differential-equations', 'special-functions']"
1409399,Showing a bound exists,"I was able to derive the following differential equations I have to work with for a function $V$: $$ \begin{align*}
  dV(x_1,x_2,x_3,x_4) &= \left(x_1^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{21}x_2^2+e_{31}x_3^2+e_{41}x_4^2 \right]\\
    &+  \left(x_2^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)+e_{12}x_1^2-c_{32}x_3^2-c_{42}x_4^2 \right]\\
    &+  \left(x_3^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{13}x_1^2+e_{23}x_2^2+e_{43}x_4^2 \right]\\
    &+  \left(x_4^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{14}x_1^2+e_{24}x_2^2-c_{34}x_3^2 \right] dt\\
\end{align*}$$ What I would like to do is find constants $c_0, c_1, c_2, c_3, c_4$ so that: $$dV(x_1,x_2,x_3,x_4) \leq c_0+c_1x_1^2+c_2x_2^2+c_3x_3^2+c_4x_4^2 dt$$ All the parameters $e_{ij}$ and $c_{ij}$ are positive, and each $x_i >0$ as well. Clearly this bound may not exist if the $e_{ij}$ are too large since the equations may explode, so the suitable condition  to prevent this is suppose to be $\Pi e_{ij} < \Pi c_{ij}$. What that should do is somehow create enough push from the negative terms to prevent explosion. However, I have been unable to use that condition and show the bound I'm looking for exists. Please help!","['ordinary-differential-equations', 'inequality']"
1409491,Solving logarithmic equations,"The equation that I'm trying to solve is:
$$\log _{5x+9}(x^2+6x+9)+\log _{x+3}(5x^2+24x+27)=4$$ Using algebra and principles of logarithms I managed to get the equation down to $$\frac{2\left(\log \left(x+3\right)\right)^2+\left(\log \left(5x+9\right)\right)^2}{\log \left(x+3\right)\log \left(5x+9\right)}=3$$
 assuming that everything I've done so far is correct. I'm not sure what to do from here. I think there might be some kind of perfect square to factor but I'm not sure how I would even factor it from this situation (or if it would help). Any suggestions?","['logarithms', 'functions']"
1409509,Show that the following mapping is a contraction.,"I have the following problem from a past paper: ""Show that the mapping, $$T(x_1,x_2)=\left(\frac{x_1+2x_2}5-1,\frac{x_1-2x_2}7+1\right)$$ is a contraction on $(\mathbb R^2,d_\infty)$."" I understand that the way to approach this problem is to consider two points in $\mathbb R^2$, say $x=(x_1,x_2)$ and $y=(y_1,y_2)$, and then to consider $d_\infty(T(x),T(y))$, however, I am tripping myself up with the fact that we have both components $x_1$ and $x_2$ included in both arguments of the mapping, and so, I am not too sure how to continue. On the other hand, if I had the mapping, $T(x_1, x_2) = (\frac{x_1}2,\frac{x_2}3)$, it is easy to see that in this case, $$d_\infty(T(x),T(y))=d_\infty\left(\frac12|x_1-x_2|,\frac13|y_1-y_2|\right)$$ because each argument of the mapping includes only one of the components of the input vectors. How should I proceed with the former example?","['metric-spaces', 'real-analysis', 'functional-analysis', 'functions', 'general-topology']"
1409513,A discussion on fourier and laplace transforms and differential equations ...?,"i have read many of the answers and explanations about the similarities and differences between laplace and fourier transform. Laplace can be used to analyze unstable systems. Fourier is a subset of laplace. Some signals have fourier but laplace is not defined , for instance cosine or sine from -infinity to +infinity. i have studied signals and systems and basic control theory in my undergrad. My question is related to the solution of differntial equations using these transforms. Wherever i have seen it is written that fourier is used for steady state analysis (example : in solution of circuits) whereas for transient response we resort to laplace. What exactly is the thing that enables laplace to incorporate initial conditions (hence unilateral laplace) and solve ODEs ? Similarly what prevents fourier from taking these into account? Let me explain at least what i understand. when solving for the laplace of the derivative of a function (using integration by parts) we input the initial conditions there and they usually end up appearing as decaying exponentials (at the natural modes/poles of the system) in the final response. The constants are chosen so as to satisfy the initial conditions. On a sidenote, this is also related to linearity and upon initial conditions that are not 0 (rest) the system ends up becoming non-linear because of not satisfying the zeros input zero output property. Now when the fourier is found for the derivative of a function the limits of the integral go from -infinity to + infinity and consequently the initial conditions can't be absorbed. Why cant we define the integral to be from 0 to infinity and incorporate initial conditions just as in laplace? i know that fourier is closely linked to convolution and changing these limits would wrong that but still can this formulation be used for solving an initial value ODE? if yes , kindly provide an example. here is one that i found . i don't understand one step (second step of finding the general solution'u') in it but other than that i can't find a mistake. Solving an initial value ODE problem using fourier transform i know this might be too basic for some or too narrowed down but it has bugged me my whole undergrad and now i really need an answer from  an expert.
Thank u and regards
khurram","['fourier-analysis', 'laplace-transform', 'ordinary-differential-equations']"
1409545,3D projection coordinates onto 2D plane to determine transformation matrix?,"I'm not sure if there is an actual solution to this problem or not, but thought I would give it a shot here to see if anyone has any ideas. So here goes: I basically have three vertices of a rigid triangle with known 3D coordinates. The vertices are projected onto a 2D plane (by projection, I mean that each vertex would basically have a fixed line drawn from it to the 2D plane, and that ""line"" would also stay rigid to the triangle so that the lines would move along with the triangle if it is transformed), in which I also know the 2D coordinates. A transformation matrix is applied to the original three points (can be a combination of rotation and translation) and I now know the new 2D projection coordinates. Is it possible to obtain either the unknown transformation matrix or the new coordinates? Any ideas are much appreciated. Thanks!","['projective-geometry', 'geometry', 'linear-algebra']"
1409596,parallel resistors,"Consider the set $E_b = \left\{1, 1.2, 1.5, 1.8, 2.2, 2.7, 3.3, 3.9, 4.7, 5.6, 6.8, 8.2\right\}$. This is our base set. Let's define the set $E$ as follows:
$$ E = \left\{ 10^k e \mid k=0,1,2,\ldots, \text{for every} e \in E_b \right\}$$
and $\Omega$ as the class of all subsets of $E$. We are intereted in defining a mapping $f$ from $\mathbb{N} \setminus \left\{1 \right\}$ to $\Omega$ such that $f(n)$ is mapped to a set $E_k \in \Omega$ according to the following rule:
$$ \frac{1}{n} = \sum_{e_k \in E_k}{\frac{1}{e_k}}$$
Here are some examples: $f(4) = \left\{10, 12, 15\right\}$ as $\frac{1}{4} = \frac{1}{10} + \frac{1}{12} + \frac{1}{15}$ $f(12) = \left\{12\right\}$ $f(19) = $ I haven't found a set that could be mapped to $19$, it could be an empty set $f(20) = \left\{22, 220 \right\}$ as $\frac{1}{20} = \frac{1}{22} + \frac{1}{220}$ This question/puzzle was brought to my attention by a colleague (an electrical engineer), the idea is to synthesize a desired resistor (an integer-valued in this case) with a given set of standard resistors (set $E$) in parallel (two parallel resistors result in $\frac1{\frac{1}{R_1}+\frac{1}{R_2}}$). Now I'm interested to know if we can synthesize a resistor with a minimum numbers of standard resistors, in other words a lower bound on the cardinality of $f(n)$. More interestingly, I'd like to know if someone can come up with an algorithm to construct such a mapping for every $n \in \mathbb{N} \setminus \left\{1 \right\}$, not necessarily with minimum cardinality. Also, for what numbers $f(n)$ is an empty set? I hope I've articulated the question in mathematical terms clear enough.","['number-theory', 'puzzle']"
1409619,Are strongly equivalent metrics mutually complete?,"Maybe I'm missing something, but I can't seem to find any references to my exact question. If two metrics, $d_1(x,y)$ and $d_2(x,y)$ are strongly equivalent , then there exists two positive constants, $\alpha, \beta$, such that $$\alpha d_1(x,y) \leq d_2(x,y) \leq \beta d_1(x,y).$$ My question is: if one metric is complete, must the other be complete as well? I suspect no, but I haven't seen any explicit counterexamples. Conversely, if you have one metric that is complete, and another metric that is not, does this imply that the two cannot be strongly equivalent?","['analysis', 'metric-spaces', 'complete-spaces', 'general-topology']"
1409624,How is the Directional Derivative a linear transform?,"So I know basically what a directional derivative is and how to calculate it using the gradient vector, but I'm a bit lost on the more advanced approach of looking at it as a linear transform. I've read that multivariable calculus is about approximating nonlinear maps with linear ones, and I know that the Jacobian is the matrix associated with the directional derivative. However, I still don't really understand the directional derivative as a linear transform. For example, what is the input of the transform? Also, what is meant by approximating nonlinear maps by linear maps? I only know a little linear algebra so that might be why I'm so confused.",['multivariable-calculus']
1409634,Complex-valued differential forms.,"Let $X$ be smooth (real) manifold and let $T^{*}(X)_{\mathbb{C}}$ denote the complexification of the cotangent bundle. We define the complex valued differential r-forms on $X$ to be the smooth sections of the bundle $\wedge^r_{\mathbb{C}} T^{*} (X)_{\mathbb{C}} \to X$ (for instance see page 31 of Differntial Analysis on Complex Manifolds by Raymond Wells). Denote the vector space of these complex-valued r-forms by $\Omega^r(X)_{\mathbb{C}}$. Apparently, we can define a version of the exterior derivative on complex-valued r-forms by taking the real exterior derivative and ""extending by complex linearity"" but I cannot understand why this is true. One approach would be to prove that $\Omega^r(X)_{\mathbb{C}}$ is isomorphic as a vector space to $\Omega^r(X) \otimes_{\mathbb{R}} \mathbb{C}$ (where $\Omega^r(X)$ denotes the usual real differential r-forms) but I cannot see how to prove this.","['complex-geometry', 'differential-geometry', 'manifolds']"
1409687,"What is the ""topology induced by a metric""?","My book gives the following definition: Let $(M,d)$ be a metric space, and let $\mathcal{T}$ be the collection of all subsets of $M$ that are open in the metric space sense...  $\mathcal{T}$ is called the topology generated by d Then it later says to prove things about two different metrics generating the same topology. However I'm very confused about what all this means. When I read ""all subsets,"" I think of the theorem that says a set with $n$ elements will have $2^n$ subsets. (ex. all the subsets of $\{1,2,3\}$ are $\{\emptyset, \{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\} \}$). So I think ""all subsets that are open"" would be like, If M is the set (1,5), then an open subset would be any set $(a,b)$ where $1<a<b<5$, and then all subsets would be all possible union combinations of these ""$(a,b)$"" sets. Its seems to me that metrics have nothing to do with subsets. Clearly there is something i'm not understanding. Please Help me. It would be very enlightening, if someone could give me some simple examples of 1 set and 2 metrics generating two different typologies.","['metric-spaces', 'general-topology']"
1409692,Manifold that is not a Euclidean space,"I just started reading a textbook, and it keeps saying that an $n$-dimensional manifold is a topological space with the same local properties as Euclidean $n$-space. I don't really understand what is meant by ""local properties."" What what would be an example for, say, a $2$-dimensional manifold that is not a Euclidean $2$-space like $\mathbb{R}^2$ or something, but still has the ""local properties"" of the Euclidean space?","['euclidean-geometry', 'manifolds', 'general-topology']"
1409725,"Each number in $S\subseteq \{1,\ldots,2n\}$ does not divide another one, with $|S|= n$. In how many ways?","Let $f(n)$ be the number of subsets $S\subseteq \{1,2,\ldots,2n\}$ such that $|S|=n$ and $a$ does not divide $b$ whenever $a,b \in S$ are distinct. Can we evaluate $f(n)$, at least asimptotically? The question is related to this other one , where I had a more complicated (and unuseful) solution. In particular, is it true that $f(n)=o(n)$? Moreover, is it true that $f(n)$ is definitively bigger than $(\ln n)^k$ for any constant $k$? Edit: the first conjecture has been proved to be false (see the answer of Robert below). Then, can we say that $\ln f(n)=O(n)$?","['asymptotics', 'number-theory', 'elementary-number-theory', 'combinatorics']"
1409736,Solving ODE rigorously,"I am given the ODE $$(f''(r)+\frac{f'(r)}{r})(1+f'(r)^2)-f'(r)^2f''(r)=0$$ and want to solve it rigorously for $r>0.$ So especially, I don't want to loose any solutions. $\textbf{Derivation of the solutions}$
First I multiply by $r$ $$(rf''(r)+f'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ which is nothing but $$\frac{d}{dr}(rf'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ now assuming $f'(r) \neq 0$ (Problem 1) I can rewrite this as $$\frac{1}{r f'(r)} \frac{d}{dr}(rf'(r)) = \frac{f'(r)f''(r)}{1+f'(r)^2}$$ Assuming $f'(r)>0$ (Problem 2) we get $$\frac{d}{dr}\ln(rf'(r)) = \frac{1}{2} \frac{d}{dr} \ln(1+f'(r)^2)$$ which gives after integrating $$\ln(r^2f'(r)^2) = ln(1+f'(r)^2)+C$$ for some constant $C.$ Exponentiating this gives $$r^2 f'(r)^2 = \underbrace{e^C}_{=:C_1^2>0} (1+f'(r)^2).$$ or alternatively $$f'(r)^2(r^2-C_1^2) = C_1^2$$ Problem (3) $r \neq C_1$ and Problem (4) (taking $\pm$) $$f'(r) = \frac{ \pm C_1 }{\sqrt{r^2-C_1^2}} = \frac{ \pm 1 }{\sqrt{\frac{r^2}{C_1^2}-1}}$$ Integrating gives then $f(r) = \pm C_1 \cosh^{-1} (\frac{r}{C_1})+d$ where $d$ is arbitrary and $C_1 >0.$ $\textbf{Regarding the problems:}$ Now, I want to understand whether I can get rid of the 4 problems (small inaccuracies in the way I am solving this ODE.) Regarding problem 1: The question is: Do I really have to assume $f'(r) \neq 0$ or can I avoid this somehow? Regarding problem 2: If I assume $f'(r)<0$ somwhere, it is quite obvious that we can rewrite the left hand side as $$\frac{1}{-rf'(r)} \frac{d}{dr}(-rf'(r)) = \ln(-rf'(r))$$ which would be well-defined now. Now, since we eventually square the argument of the $\ln $anyway (two equations below the statement of the problem in the derivation), we see that even for $f'(r)<0$ we would end up with the same equation as for $f'(r)>0$. Thus, problem 2 is apparently not a problem. Problem 3: Is it really necessary to assume $ r \neq C_1$? I guess yes, as otherwise we would get $0=f'(r)^2(r^2-C_1^2) = C_1^2=r^2$ which is a contradiction to $r \neq 0.$ Am I right? Problem 4: I don't like this $\pm $ there, as it could be that a solution has both positive and negative $f'(r)$, so the way I write the solution is not that general.","['calculus', 'real-analysis', 'dynamical-systems', 'analysis', 'ordinary-differential-equations']"
1409769,Why is n-th Fréchet derivative symmetric?,"Let $V,W$ be nonzero normed spaces over $\mathbb{K}$. Let $E$ be open in $V$ and $f:E\rightarrow W$ be a twice Fréchet-differentiable function. Then, $D^2 f: E\rightarrow \mathscr{L}_2(V^2,W)$ is symmetric. That is, at any point $p\in E$, $((D^2 f)(p) )(x,y)= ((D^2 f) (p))(y,x)$. It is not that I don't understand the proof, but I don't understand why it must hold. What's the geometric meaning of higher order Fréchet derivatives? First order Fréchet derivative $(D f)(p)$ is a function that approximates $f(x)$ where $x$ near $p$, by means of a linear transformation. From the definition $(D^2 f)(p)$ is a linear approximation of $(Df)(x)$ where $x$ is near $p$. However, I'm not sure what does $D^2 f$ say about $f$. What does $D^n f$ represent of $f$ exactly? Thank you in advance.","['differential-operators', 'analysis', 'multivariable-calculus']"
1409774,Solve $\frac{1}{2^\theta}\sum_{k=0}^{\theta} {\theta\choose k} \delta(k)=\theta$ for $\delta$,"The following arises in unbiased estimation of a parameter for the binomial distribution, but that information is not needed for solving the question. I tried solving this by manipulating the sum to get a recursion, but failed to do so. Find an explicit function $\delta:\{0,1,\dots\}\to\{1,2,\dots\}$ such that the following holds for all $\theta\in\{1,2,\dots\}$: $$
 \frac{1}{2^\theta}\sum_{k=0}^{\theta} {\theta\choose k} \delta(k)=\theta
$$ EDIT Note that the usual unbiased estimator for this binomial parameter, i.e. the function $\delta(x)=2x$ is not a solution since $\delta$ may not map to $0$. Considering $\theta = 1$ leads to $\delta(0)=\delta(1)=1$, whence considering $\theta=2$ gives that $$
\delta(0) + 2 \delta(1) + \delta(2) = 8 \iff \delta(2)=5,
$$ and so on one may continue to find $\delta(k)$ for every $k$, but I cannot find the pattern and solve to get an explicit function.","['summation', 'statistics', 'binomial-theorem', 'binomial-coefficients', 'binomial-distribution']"
1409775,Proving uniqueness of solutions to $\sin^2A + \sin^2B = \sin (A+B)$ without using multivariable calculus,"In the course of solving a trigonometric problem (see $a^2+b^2=2Rc$,where $R$ is the circumradius of the triangle.Then prove that $ABC$ is a right triangle ), in one approach the following equation needed to be solved: $$\boxed{\sin^2A + \sin^2B = \sin (A+B)}$$ subject to $A\in(0,\frac{\pi}{2}),B\in(0,\frac{\pi}{2}),\pi-(A+B) > \max(A,B)$, i.e. $A$ and $B$ are the two angles of a triangle not opposite the longest side. Clearly, any $A,B\:|\:A+B=\frac{\pi}{2}$ is a family of solutions. Since multivariable calculus is presumably beyond the level of the original problem: How to prove that there are no other solutions $\underline{\text{without}}$ using multivariable calculus? [I don't think a trig identity will suffice as there are other solutions if the restrictions on $A,B$ are relaxed.] (For completeness - using multivariable calculus) Part 1 Proof that $\sin^2A + \sin^2B < \sin (A+B)$ over region $R_1=\{0<A,B\land A+B<\frac{\pi}{2}\}$. Consider $$f(x,y)=\sin^2x + \sin^2y - \sin (x+y)$$ 
Then
$$\begin{align}
f_x &= \sin 2x - \cos(x+y) \\
f_y &= \sin 2y - \cos(x+y) \\
f_{xx} &= 2\cos 2x + \sin(x+y) \\
f_{yy} &= 2\cos 2y + \sin(x+y) \\
f_{xy} &= \sin(x+y) &= f_{yx} \\
\end{align}$$ For local extrema we require $f_x=f_y=0$. But $$f_x=f_y=0 \implies \sin2x=\sin2y \implies y=x \lor y=\frac{\pi}{2}-x$$ Exclude $y=\frac{\pi}{2}-x$ as it violates the restriction that $x+y<\frac{\pi}{2}$. If $y=x$, $f_x=0 \implies \sin2x=\sin2y \implies x=y=\frac{\pi}{8}$. The determinant of the Hessian is 
$$D(x,y) = \begin{vmatrix} f_{xx} & f_{xy}\\ f_{yx} & f_{yy} \end{vmatrix} = f_{xx}f_{yy} - f_{xy}f_{yx} = f_{xx}f_{yy} - (f_{xy})^2$$ At $P(\frac{\pi}{8},\frac{\pi}{8})$, we have $$\begin{align}
f_{xx}&=2\cos\frac{\pi}{4}+\sin\frac{\pi}{4}=\frac{3}{\sqrt2} \\
f_{yy}&=2\cos\frac{\pi}{4}+\sin\frac{\pi}{4}=\frac{3}{\sqrt2} \\
f_{xy}&=\sin\frac{\pi}{4}=\frac{1}{\sqrt2} \\
D(x,y)&=\frac{9}{2}-\frac{1}{2}=4
\end{align}$$ Since both $f_{xx}$ and $D$ are positive at $P$, this is a local minimum (with $f(x,y)|_P=1-\sqrt2$). On the boundaries: $f(0,y)=\sin^2y-\sin y<0$ on $x=0,y\in(0,\frac{\pi}{2})$ $f(x,0)=\sin^2x-\sin x<0$ on $x\in(0,\frac{\pi}{2}),y=0$ $f(0,0)=0$ $f(x,y)=0$ for $x,y\geq0,x+y=\frac{\pi}{2}$ Since $(0,0)$ is not part of the domain, and there are no other local extrema, we have $f(x,y)<0$ over $R_1$. Part 2 It will be sufficient to prove that $\sin^2A + \sin^2B > \sin (A+B)$ over region $R_2=\{0<A,B<\frac{\pi}{2} \land A+B>\frac{\pi}{2}   \}$.","['alternative-proof', 'trigonometry']"
1409802,"Lie bracket of two vectors $X,Y$ perpendicular to $Z$ is perpendicular to $Z$","Where $Z$ is a Killing vector field (is this even necessary?)
In case more assumptions are necessary, I have:
$[Z,X] = [Z,Y] = g(Z,X) = g(Z,Y) = 0$
I want to prove $g([X,Y],Z) = 0$ I am trying to prove this without using coordinates. I kind of have a rough argument using the definition of the Lie derivative (I'm moving a vector that's always perpendicular to $Z$ through a curve in the orthogonal space to $Z$ so necessarily the difference must be in the orthogonal space) but I am not sure how to formalize it and whether I can simply put the definition inside the dot product. Are the assumptions that $Z$ is a Killing vector field necessary, and what about $[Z,X] = [Z,Y] = 0$? edit: I forgot to mention, the metric is of the form $ds^2 = \Theta dz^2 + \hat{g}_{ab}dx^adx^b$ where nothing depends on $z$. I think that the precise name for this is polarized axial symmetry, or equivalently Ernst potential is $0$.","['differential-geometry', 'lie-derivative']"
1409803,Derivative exists by first principles but undefined when using chain rule,"Consider the function $h$ defined by \begin{align}
h(z,y)=(z^3+y^3)^{\frac{1}{3}}
\end{align} Then \begin{align*}
h_z(0,0)&=\lim_{t\rightarrow 0}\frac{(t^3)^{\frac{1}{3}}}{t}\\
&=1
\end{align*} When differentiating via the chain rule we have \begin{align}
h_z(z,y)&=\frac{z^2}{(z^3+y^3)^{\frac{2}{3}}}
\end{align} This function is not defined at (0,0), but when calculating from first principles we get a well defined answer. What is going on here?","['calculus', 'real-analysis', 'multivariable-calculus']"
1409811,"A conjectured result for $\sum_{n=1}^\infty\frac{(-1)^n\,H_{n/5}}n$","Let $H_q$ denote harmonic numbers (generalized to a non-integer index $q$):
$$H_q=\sum_{k=1}^\infty\left(\frac1k-\frac1{k+q}\right)=\int_0^1\frac{1-x^q}{1-x}dx=\gamma+\psi(q+1),\tag1$$
where $\psi(z)=\Gamma'(z)/\Gamma(z)$ is the digamma function . My goal is to evaluate the following series:
$$\mathcal S_m=\sum_{n=1}^\infty\frac{(-1)^n\,H_{n/m}}n.\tag2$$
Using the integral representation from $(1)$ we can get equivalent integral forms:
$$\mathcal S_m=\int_0^1\frac{\ln(1+\sqrt[m]x)-\ln2}{1-x}\,dx=m\int_0^1\frac{\ln(1+z)-\ln2}{1-z^m}\,z^{m-1}dz.\tag3$$
Here are some simple cases:
$$\begin{align}&\mathcal S_1=\frac{\ln^22}2-\frac{\pi^2}{12}\hspace{7.7em}\color{maroon}{\mathcal S_2=\ln^22-\frac{\pi^2}{12}}\\\\&\color{blue}{\mathcal S_3=\frac{3\ln^22}2-\frac{\pi^2}9+\frac12\,\operatorname{Li}_2\!\left(\tfrac14\right)}\hspace{2em}\color{green}{\mathcal S_4=\frac{7\ln^22}4-\frac{5\pi^2}{48}}\end{align}\tag4$$
For $\mathcal S_5$ the integral can be found using Mathematica (there is even a closed-form antiderivative, so it should be possible in principle to prove it by differentiation), but the result takes tens of thousands characters to write down (you can see it here ), and Mathematica cannot do much simplification on it ( here is a simplified result). But I was able to conjecture a much simpler closed form that fits numerically with a high precision: $$\mathcal S_5\stackrel{\color{gray}?}=\frac{\ln^22}2-\frac{\ln^25}4+\ln2\cdot\ln5-\frac12\,\operatorname{Li}_2\!\left(\tfrac15\right)-\operatorname{Li}_2\!\left(\frac{\sqrt5-1}2\right)\tag{$\diamond$}$$ I hope there is a way to prove this result manually without going through huge intermediate expressions, but so far I have not found it.","['harmonic-numbers', 'closed-form', 'definite-integrals', 'sequences-and-series', 'polylogarithm']"
1409817,What mathematics topics pertain more towards applied mathematics?,"I'm entering my second year of undergrad (majoring in mathematics), and I've found that I am really bad at Linear Algebra, but very good at Calculus and Differential Equations. I'm hoping to venture onto Sci. Computing/Applied Maths, but I'm worried my inadequacy (as quite personally, unfortunate lack of interest) for Lin. Alg. will prevent me from being successful in topics such as Numerical Analysis, Algebra, as well as Scientific Computing. Does anyone in the applied maths field/experience with applied maths have any advice on what I should do? That is, what else is there like Calculus/DEs that will help me in this field? Or do i just need to buck up and get on my Lin. Alg. horse in order to get remotely close to where I want to go? I appreciate any and all input.","['numerical-linear-algebra', 'computational-mathematics', 'applications', 'linear-algebra', 'ordinary-differential-equations']"
1409823,"Every year, there is a contest...","Every year, there is a contest to see who has the heaviest pumpkins for that year. Last year, a farmer brought 5 pumpkins to the contest. Instead of weighing them one at a time, he informed the judges, ""When I weighed two at a time, I got the following weights: 108, 112, 113, 114, 115, 116, 117, 118, 120, and 122."" How much did each pumpkin weigh? I tried solving this problem by summing all the weights (1,155) and dividing by 10 to get the average weight of two pumpkins (115.5). I then divided by 2 to get the average weight of each pumpkin (57.75). To determine the middle pumpkin, I multiplied by 5 to find the total weight of the five pumpkins (288.75) and then subtracted the lightest and heaviest weight ('C' = 58.75). Fast forward, I determined 'A' = 53.25, 'B' = 54.75, 'C' = 58.75, 'D' = 60.75, and 'E' = 61.25. However upon review, I determined that these values could not produce the 10 weights. In addition, I concluded the values must end in .5 to increase the combinations. The question is: is there a correct answer and what is it, or is this unsolvable? Thanks.",['algebra-precalculus']
1409837,Motivation for the Definition of Compact Space,"A compact topological space is defined as a space, $C$, such that for any set $\mathcal{A}$ of open sets such that $C \subseteq \bigcup_{U\in \mathcal{A}} U$, there is finite set $\mathcal{A'} \subseteq \mathcal{A}$ such that $C \subseteq \bigcup_{U'\in \mathcal{A'}} U'$. Now, this definition leads to many interesting results, but if I were teaching someone about compact sets, how would I motivate this? Concepts like sequential compactness, open and closedness, and even connectedness are reasonably easy to motivate. I can not see how to motivate this definition. Compact spaces are often seen as generalizations of finite spaces. They are also seen as a generalization of boundedness and closedness. I can't see how to connect the definition with these concepts. Alternatively, is there a definition of a compact set which is easier to motivate?","['motivation', 'education', 'definition', 'compactness', 'general-topology']"
1409862,Average length of a cycle in a n-permutation,"What is the average length of a cycle in a permutation of $\{1,2,3,\dots ,n\}$?","['combinatorics', 'permutations']"
1409872,"If $\int_0^{x/3} f(t)dt =\int_0^xf(t)dt$, prove $f$ is identically $0$","$f:[0,1] \to \mathbf R$ is continuous. If $$\int_0^{x/3} f(t)dt =\int_0^xf(t)dt$$ for all $x$ in $[0,1]$, prove that $f$ is identically $0$. My thought is to prove that the maximum and minimum of $f$ are equal then $f$ is constant and this constant can only be zero. But I can't think of a way to do that. Can somebody help and give me some hints. Thanks.",['real-analysis']
1409911,Is the space of $G$-maps $G/H \to X$ naturally homeomorphic to $X^H$?,"Let $X$ be a $G$-space, where $G$ is a (discrete) group. For a subgroup $H$ of $G$, define$$X^H = \{x : hx = x \text{ for all }h \in H\} \subset X;$$$X^H$ is the $H$-fixed point subspace of $X$. Topologize the set of functions $G/H \to X$ as the product of copies of $X$ indexed on the elements of $G/H$, and give the set of $G$-maps $G/H \to X$ the subspace topology. Is the space of $G$-maps $G/H \to X$ naturally homeomorphic to $X^H$?","['group-theory', 'algebraic-topology']"
1409946,"Is this GRE math problem about the $x$,$y$-intercepts of two perpendicular lines wrong?","I'm working out of the Manhattan GRE test prep book and I've come across a question that I can't figure out why they chose the answer they did. ""Perpendicular lines m and n intersect at point (a,b), where a>b>0. The slope of m is between 0 and 1. Which of the following statements must be true? Indicate all that apply"" a: The x-intercept of line m is positive b: The y-intercept of line m is negative c: The x-intercept of line n is positive d: The y-intercept of line n is positive e: The product of the x and y intercepts of line m is negative f: The sum of the x intercepts of lines m and n is positive The book answer is C and E I understand C, but I can't comprehend how E MUST be true. The slope of line m can go through the origin. 0*0=0 is not negative. The book solution says the intersections of m will always have opposite signs.","['plane-geometry', 'geometry', 'analytic-geometry']"
1409948,About closedness and boundedness of $H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\}$,"Let $H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\}$. To check which one is true: (a) $H$ is bounded (b) $H$ is closed (c) $H$ is a subspace (d) $H$  has interior points My try: (c) is not true as $x_n\in H$ does not imply that $cx_n\in H$ for $c\in \mathbb R$.
(d) is also not true as $x_n\in H$ does not imply $x_n+t\in H$ for $t$ however small it is. I am not sure about (a),(b). How to proceed?",['functional-analysis']
1409963,A Banach Space cannot have a denumerable basis:Why is it true?,I came across the following theorem: A Banach Space cannot have a denumerable basis which has been proven in my book. I can't understand why is it true since $\mathbb R$ is a banach space over $\mathbb R$ and it has a countable basis i.e $\{1\}$ Where am I missing the link?,['functional-analysis']
1409965,Limits of cosine and sine [duplicate],This question already has answers here : Approximation of the Sine function near $0$ (3 answers) How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? (30 answers) Closed 8 years ago . When $\theta$ is very small why $\sin \theta$ is similar to $\theta$ and $\cos\theta$ similar to $1$? Is it related to limits or we can prove it simply by using diagrams?,"['limits', 'trigonometry']"
1409998,Expected value of trials to obtain a red ball in a box of white balls.,"I have a problem that involves a box containg N balls, one of which is red and the rest of which (N-1) are white. The question involves finding the expected value and variance for the number of trials to obtain the red ball in the case that (a) the balls are replaced and the case (b) the balls are not replaced. For (a) I said that
$$P(X=1) = \frac{1}{n}$$
$$P(X=2) = \frac{n-1}{n}\cdot\frac{1}{n}=\frac{n-1}{n^2}$$
$$P(X=3) = \frac{n-1}{n}\cdot\frac{n-1}{n}\cdot\frac{1}{n}=\frac{(n-1)^2}{n^3}$$
etc. So that:
$$E(X) = \sum xP(X=x) = \sum_{k=1}^{n} k\frac{(n-1)^{k-1}}{n^{k}}$$ For (b) I said that:
$$P(X=1) = \frac{1}{n}$$
$$P(X=2) = \frac{n-1}{n}\cdot\frac{1}{n-1} = \frac{1}{n}$$
$$P(X=3) = \frac{n-1}{n}\cdot\frac{n-2}{n-1}\cdot\frac{1}{n-2} = \frac{1}{n}$$
etc. So that:
$$E(X) = \sum xP(X=x) = \sum_{k=1}^n k\frac{1}{n}$$ So originally when I was writing this question I had obtained the same result for both but then found an error in my ways. Instead I'll ask is this working correct? Any assistance is appreciated, thank you.","['probability', 'statistics', 'balls-in-bins', 'expectation']"
1410008,"$P$ is a monic polynomial of degree $n$ , then which are correct?","Suppose that $P$ is a monic polynomial of degree $n$ in one variable with real coefficients and $K$ is a real number. Then which of the following statements are necessarily correct ? If $n$ is even and $K>0$ then there exists $x_0\in \mathbb R$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K<0$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For any natural number $n$, and $0<K<1$ then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K \in \mathbb R$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For (3), consider, $P(x)=x^2+5$. Then , there does not exist $x_0\in \mathbb R$ such that $0<e^{-x_0}P(x_0)<1$. So it is FALSE. But what about the others ?","['analysis', 'real-analysis']"
1410020,"If $b \equiv 0 \pmod a$ and $c \equiv 0 \pmod b$, then $c \equiv 0 \pmod a$","The question is If $b \equiv 0 \pmod a$ and $c \equiv 0 \pmod b$, then $c \equiv 0 \pmod a$. My attempt is that $b \equiv 0 \pmod a$ can be written $a\mid b-0 = a\mid b$
and the same with $c \equiv 0 \pmod b$ can be written $b\mid c-0 = b\mid c$
then the $c \equiv 0 \pmod a$ can also be written $a\mid c-0 = a\mid c$. Now all I have to show is that $a\mid b$ and $b\mid c$ then $a\mid c$ and to show $a\mid c$ I can write
$c= ak$ for some integer $k$. Now I start my proof let $a\mid b$ be $b=a\cdot r$ for some integer $r$ (1) let $b\mid c$ be $c=b\cdot s$ for some integer $s$ (2) Sub (1) into (2) and get $c=(ar)\cdot s$ $c=a\cdot(r\cdot s)$ so $c=a\cdot k$ therefore the statement $b \equiv 0 \pmod a$ and $c \equiv 0 \pmod b$, then $c \equiv 0 \pmod a$ is true. I'm wondering if this is a good/correct proof?
any input would be appreciated.","['proof-verification', 'modular-arithmetic', 'discrete-mathematics']"
1410024,What do we call the result of wedging together the columns of a matrix?,"We can wedge together the columns of a square matrix to compute its determinant. More generally, the exterior product of the columns of a $b \times a$ matrix tells us the determinant of each $a \times a$ submatrix. This can also be used to test linear dependence among the columns, and to compute the rank. Question. The result of wedging together the columns of a matrix $A$ in the order they appear is called the [what] of $A$? I'm also interested in answers of the form: ""Really, this should be thought of as something you can do to a linear transform, and its called the [whatever] of a linear transform.""","['matrices', 'determinant', 'terminology', 'exterior-algebra', 'multilinear-algebra']"
1410035,What does $\|u\|$ mean?,"What does $\left\Vert \mathbf{u}\right\Vert$ mean in this equation? How would this equation be performed? I'm extremely terrible in discrete mathematics and a simplistic answer would be ideal. (Don't answer it directly, I want to practice)","['vector-spaces', 'linear-algebra', 'vectors', 'normed-spaces']"
1410049,Maximum of $\cos \alpha_{1}\cdot \cos \alpha_{2}\cdot \cos \alpha_{3}....\cos \alpha_{n}.$,"Maximum value of $\cos \alpha_{1}\cdot \cos \alpha_{2}\cdot \cos \alpha_{3}\cdot \cos \alpha_{4}....\cos \alpha_{n}.$ If it is given that $\cot \alpha_{1}\cdot \cot \alpha_{2}\cdot \cot \alpha_{3}.......\cot \alpha_{n}  = 1$ and $\displaystyle 0 \leq \alpha_{1},\alpha_{2},\alpha_{3},.......,\alpha_{n}\leq \frac{\pi}{2}.$ $\bf{My\; Try::}$ Using $\bf{A.M\geq G.M}$ $$\displaystyle \frac{\sin^2 \alpha_{1}+\sin^2 \alpha_{2}+\sin^2 \alpha_{3}+........+\sin^2 \alpha_{n}}{n}\geq \sqrt[n]{\sin^2 \alpha_{1}\cdot \sin^2 \alpha_{2}...\sin^2 \alpha_{n}}$$ So we get $$\displaystyle \sin^2 \alpha_{1}+\sin^2 \alpha_{2}+\sin^2 \alpha_{3}+........+\sin^2 \alpha_{n}\geq n\cdot \sqrt[n]{\sin^2 \alpha_{1}\cdot \sin^2 \alpha_{2}...\sin^2 \alpha_{n}}.......(1)$$ Similarly $$\displaystyle \frac{\cos^2 \alpha_{1}+\cos^2 \alpha_{2}+\cos^2 \alpha_{3}+........+\cos^2 \alpha_{n}}{n}\geq \sqrt[n]{\cos^2 \alpha_{1}\cdot \cos^2 \alpha_{2}...\cos^2 \alpha_{n}}$$ So we get $$\displaystyle \cos^2 \alpha_{1}+\cos^2 \alpha_{2}+\cos^2 \alpha_{3}+........+\cos^2 \alpha_{n}\geq n\cdot \sqrt[n]{\cos^2 \alpha_{1}\cdot \cos^2 \alpha_{2}...\cos^2 \alpha_{n}}.......(2)$$ Now Adding $(1)$ and $(2)\;,$ We get $$n\geq 2n\cdot \sqrt[n]{\cos^2 \alpha_{1}\cdot \cos^2 \alpha_{2}...\cos^2 \alpha_{n}}$$ Because above it is given that $$\sin \alpha_{1}\cdot \sin\alpha_{2}\cdot\sin \alpha_{3}.........\sin\alpha_{n}=\cos\alpha_{1}
\cdot\cos\alpha_{2}\cdot \sin\alpha_{3}......\cos\alpha_{n}$$ So we get $$\displaystyle \cos \alpha_{1}\cdot \cos \alpha_{2}\cdot \cos \alpha_{3}\cdot \cos \alpha_{4}....\cos \alpha_{n}\leq \frac{1}{2^{\frac{n}{2}}}$$ My question is that can we solve it using any other short method? If yes then please explain it here. Thanks.",['trigonometry']
1410071,Diophantine equation: $13^x+3=y^2$,"$x,y$ are positive integers. $$13^x+3=y^2\iff \left(4+\sqrt{3}\right)^x\left(4-\sqrt{3}\right)^x=\left(y+\sqrt3\right)\left(y-\sqrt3\right)$$ $\gcd\left(y+\sqrt3, y-\sqrt3\right)=1$, therefore $y+\sqrt3=\left(4+\sqrt{3}\right)^x$ (implies $x=1$), since $\Bbb Z\left[\sqrt{3}\right]$ is a UFD and $4+\sqrt{3}$ is prime (since $4^2-3=13$ is prime) and $y-\sqrt{3}$ has negative $\sqrt3$ part. My question: why can't RHS have units, e.g. $y+\sqrt3=\left(2+\sqrt3\right)\left(4+\sqrt{3}\right)^x$. (then $y-\sqrt3=\left(2-\sqrt3\right)\left(4-\sqrt{3}\right)^x$) Solution found in Introduction to Diophatine Equations by Andreescu Titu (page $315$).","['abstract-algebra', 'unique-factorization-domains', 'number-theory', 'diophantine-equations']"
1410088,How to determine a kind of distance between two permutations?,"Let's define a distance between two permutation of length $N$: it is the minimum steps to change one to be another. ""A step of change"" means that exchanging any two elements' location. For example, series $\{1,2,3\}$ can be changed into $\{2,1,3\}$ in one step, by exchanging the location of $1$ and $2$. So it means $$\text{d}(\{1,2,3\},\{2,1,3\}) = 1$$ $\text{d}$ means distance. So, we can easily get that $\text{d}(\{1,2,3\},\{3,1,2\})$ is $2$ and so on. But while $N$ is big enough, how can I get the distance between two arbitrary series?","['discrete-mathematics', 'permutations']"
1410108,Does $\lim\limits_{n \to +∞} \sum_{k=1}^n \frac{n\cdot \ln (k)}{n^2+k^2}$ diverge?,Does the limit of this summation diverge? $$\lim\limits_{n \to +∞} \sum_{k=1}^n \frac{n\cdot \ln (k)}{n^2+k^2}$$ Thanks!,"['summation', 'limits']"
1410144,A proof for the Mobius Strip parametrization,"According to Elementary Differential Geometry by A N Pressley, a parameterization for Mobius strip is : $\textit{Example 4.9}$ The Möbius band is the surface obtained by rotating a straight line segment $\cal L$ around its midpoint $P$ at the same time as $P$ moves around a circle $\cal C$, in such a way that as $P$ moves once around $\cal C$, $\cal L$ makes a half-turn about $P$. If we take $\cal C$ to be the circle $x^2+y^2=1$ in the $xy$-plane, and $\cal L$ to be a segment of length $1$ that is initially parallel to the $z$-axis with its midpoint $P$ at $(1,0,0)$, then after $P$ has rotated by an angle $\theta$ around the $z$-axis, $\cal L$ should have rotated by $\theta/2$ around $P$ in the plane containing $P$ and the $z$-axis. The point of $\cal L$ initially at $(1,0,t)$ is then at the point $$\boldsymbol\sigma(t,\theta)=\left(\left(1-t\sin\dfrac\theta2\right)\cos\theta,\left(1-t\sin\dfrac\theta2\right)\sin\theta,t\cos\dfrac\theta2\right).$$ We take the domain of definition of $\boldsymbol\sigma$ to be $$U=\{(t,\theta)\in\mathbf R^2\mid-1/2<t<1/2,\ 0<\theta<2\pi\}.$$ And according to Wiki another parameterization is $x(u,v)= \left(1+\frac{v}{2} \cos \frac{u}{2}\right)\cos u\\ y(u,v)= \left(1+\frac{v}{2} \cos\frac{u}{2}\right)\sin u\\ z(u,v)= \frac{v}{2}\sin \frac{u}{2}.$ My questions are: 1- how these two 'different' parameterizations can be transformed to each other? Supposing the domains remain same ($0 ≤ u < 2π$ and $−1 ≤ v ≤ 1$) is it not possible by changing variables to do the reparameterizations (esp. $x$ and $y$ in $(x,y,z)$). 2- How (at least one of) the mentioned parameterizations be derived? Wiki and the book mentioning with no proof. Thank you.",['differential-geometry']
1410157,Integers which are the sum of non-zero squares,"Lagrange's four-square theorem states that every natural number can be written as the sum of four squares, allowing for zeros in the sum (e.g. $6=2^2+1^2+1^2+0^2$). Is there a similar result in which zeros are not allowed in the sum? For example, does there exist $n\in\mathbb{N}$ such that every natural number greater than $n$ can be written as the sum of five non-zero squares, or six non-zero squares, for example?","['number-theory', 'elementary-number-theory']"
1410164,"Prove that $\int_0^1 \frac{1}{1+\ln^2 x}\,dx = \int_1^\infty \frac{\sin(x-1)}{x}\,dx $","I've found the following identity. $$\int_0^1 \frac{1}{1+\ln^2 x}\,dx = \int_1^\infty \frac{\sin(x-1)}{x}\,dx $$ I could verify it by using CAS, and calculate the integrals in term of exponential and trigonometric integrals , then using identities between them. However, I think there is a more elegent way to prove it. How could we prove this identity? Also would be nice to see some references.","['calculus', 'definite-integrals', 'logarithms', 'trigonometry', 'integration']"
1410185,Is $\sqrt[3]{-1}=-1$?,"I observe that if we claim that $\sqrt[3]{-1}=-1$, we reach a contradiction. Let's, indeed, suppose that $\sqrt[3]{-1}=-1$. Then, since the properties of powers are preserved, we have: $$\sqrt[3]{-1}=(-1)^{\frac{1}{3}}=(-1)^{\frac{2}{6}}=\sqrt[6]{(-1)^2}=\sqrt[6]{1}=1$$ which is a clear contradiction to what we assumed...","['abstract-algebra', 'roots', 'roots-of-unity']"
1410242,Closeness of measures on a cardinal,"Given an uncountable $\kappa$ and a $\kappa$-complete nontrivial non-normal ultrafilter on $\kappa$, and some $g:\kappa\to\kappa$ with $<_{U}$-rank $\kappa$ (where $f_0<_Uf_1$ iff $\{i<\kappa\, |\, f_0(i)<f_1(i)\}\in U$), we can define a normal ultrafilter $D = \{ X\subseteq \kappa\, |\, g^{-1}(X)\in U\}$ and an embedding $k:Ult(V,D)\to Ult(V,U)$ by $k([f]_D)= [f\circ g]_U$. I see that $\kappa < crit(k) \leq [id_{\kappa}]_U<(2^{\kappa})^+$ and I was wondering if anything more can be said (in ZFC) about $crit(k)$, and otherwise what values of $crit(k)$ are consistent, and for example if it's consistent that $\kappa$ is measurable, there exist non-normal measures on $\kappa$ and always this $crit(k)=[id_{\kappa}]_U$. Basically, I would be glad to be pointed at any result that could make this more clear for me.","['large-cardinals', 'set-theory', 'measure-theory']"
1410259,System with arbitrary function of an unknown,"How can I solve the following system
$$ (u_x)^2 - (u_t)^2 = 1 \\
 u_{xx} - u_{tt} = f(u) $$ where $f$ is an arbitrary function of $u$, $u$ and $f$ to be determined. I don't know any approach, therefore I am very interested about you ideas and grateful for your help!","['polynomials', 'multivariable-calculus', 'numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
1410276,Closed-form of $\operatorname{Li}_2\left(1 \pm i\sqrt{3}\right)$,"I've found the following identity while I was going through a quite difficult path. $$
\Re\operatorname{Li}_2\left(1 \pm i\sqrt{3}\right) = \frac{\pi^2}{24} -\frac{1}{2}\ln^2 2 - \frac{1}{4}\operatorname{Li}_2\left(\tfrac{1}{4}\right),$$ where $\operatorname{Li}_2$ is the dilogarithm function . I think we could prove it directly from dilogarithm identites. How could we prove the identity above? Furthermore Could we specify $\Im\operatorname{Li}_2\left(1 \pm i\sqrt{3}\right)$? There is a similar question here .","['calculus', 'special-functions', 'closed-form', 'integration', 'polylogarithm']"
1410285,"A discontinuous function $f: X \rightarrow Y$ satisfying: for each closed ball $B$ of $Y, f^{-1}(B)$ is closed in $X$","Find a function $f: X \rightarrow Y$ between metric spaces $X$ and $Y$ that is not continuous but has the property that for each closed ball $B$ of $Y, f^{-1}(B)$ is closed in $X$ Solution Attempt: A continuous function $f : X \rightarrow Y$ is defined as : For every open set $V$ in $Y$, there's an open set $U \in X$ such that $f(U) \subseteq V$. Strategy: The issue with closed sets while defining continuity can be that a closed set $U_o \in X $ can also contain limit points which may not get mapped to the closed set $V_o$. EDIT: Let $f: X \rightarrow Y$ be a an identity function where $X$ is a non-discrete metric space and $Y$ is a metric space with the discrete metric. (As far as it's set elements were concerned, $X$=$Y$..)So, $f(x) = x ~\forall x \in X$. $X$ is non discrete metric space $\implies$ there is atleast one subset of $X$ which is closed but not open in $X$. Could you please explain how these arguments tell us that $f^{-1}(x_{|Y}) = x_{|X}$  is closed in $X$? . Thank you!","['metric-spaces', 'continuity', 'real-analysis', 'advice']"
1410288,What rotation rules can be applied to stacked cubes to make a 3D spirograph?,"If you arrange building blocks for example toy cubes so that every next cube is tilted over its base by 30 degrees and rotated to it's right by 12 degrees, it would wind through space in a helical fashion. What rules can be applied to pivot the blocks so that they create assymetrical spirograph structures? Examples of rules for n building blocks can be if(n=17) then rot(y)= n/17*10... any kind of rotation of the blocks relative to n. The structure is built by rotating blocks so it isn't a parametric structure which can place the blocks and then line them up. ADDENDUM- Another view of the problem... All polygons from 3 to 100 sides, must have angles that add up to 360. if i can rotate some lines of the polygon outwards in 3D, to make polygons with n sides, like 800 sides, I want to know how to add together the angles so that they form a closed mobeous polygon similar to spirographs and so on. the cubes can be viewed as lines also. Perhaps i have to simplify all the subsequent angles into 3 point triangles.","['sequences-and-series', 'geometry', 'rotations']"
1410293,"Calculating $\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx$","Do you see any fast way of calculating this one? $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx$$ Numerically, it's about $$\approx 111.024457130115028409990464833072173251135063166330638343951498907293$$ or in a predicted closed form $$\frac{4 }{3}\pi ^3+32 \pi  \log (2).$$ Ideas, suggestions, opinions are welcome, and the solutions are optionals . Supplementary question for the integrals lovers: calculate in closed form $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^3 \, dx.$$ As a note, it would be remarkable to be able to find a solution for the generalization below $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^n \, dx.$$","['calculus', 'special-functions', 'definite-integrals', 'integration', 'polylogarithm']"
1410297,A surjective endomorphism (of a Noetherian ring) is injective.,"The problem is stated as follows: ""Let $R$ be a Noetherian ring and $\theta$ be a ring homomorphism from $R$ to $R$. Show that if $\theta$ is surjective then it is also injective."" Regardless of the right solution, I don't understand why is the following wrong: We have $\theta: R\to R$. By the isomorphism theorem $R/\ker\theta\cong\operatorname{Im}\theta$. Since $\operatorname{Im}\theta = R$, it follows $\ker\theta =0$, so it's injective. Harsh criticism will be appreciated. Thanks.","['abstract-algebra', 'proof-verification']"
1410298,Chance of Drawing All of a Subset,"I have a simple question but I can't seem to find the answer anywhere. Say that I have a set $\mathbb Z$ and a subset of that $\mathbb X$. I want to draw elements from $\mathbb Z$ until there is at least a 50% chance that I have drawn all the elements of $\mathbb X$. Clearly if $|\mathbb X| = 1$ then I just need to draw $\frac{|\mathbb Z|}{2}$. But how many will I need to draw if $|\mathbb X| > 1$? EDIT: GBQT made a good comment , I'm talking about drawing with replacement. Drawing element $e$ from $\mathbb Z$ does not preclude $e$ from being drawn again on subsequent draws. But $\mathbb X$ is finite, as in it has each element of $\mathbb X$ is unique.","['elementary-set-theory', 'percentages', 'algebra-precalculus']"
1410308,Is the total differential the same as the directional derivative?,"The way I understand it, the total differential and the directional derivative are both linear approximations of the change in a function at a certain point. So if I know the change in $x$ and $y$ from the initial point, then I plug those into the total differential to find the approximate change in $z$. But isn't this the same as finding the directional derivative in the direction of $$ v = (\text{change in } x, \text{change in } y)? $$",['multivariable-calculus']
1410331,Characteristic function with modulus 1 implies degenerate distribution,"Let $X$ be a random variable with characteristic function $\phi(\ )$ satisfying $|\phi(t)|=1$ for all $|t|\leq 1/T$ with some $T>0$. Show that $X$ is degenerate, i.e., there is $c$ such that $P(X=c)=1$. My try : $|\phi(t)|^2=1 \implies (\mathbb{E}(\cos tX))^2+(\mathbb{E}(\sin tX))^2=1=\mathbb{E}(\cos^2 tX+\sin^2 tX)=\mathbb{E}(\cos^2 tX)+\mathbb{E}(\sin^2 tX)$ so we can say that $\sin tX=\mathbb{E}(\sin tX), \cos tX=\mathbb{E}(\cos tX)$, that is $\phi(t)=\rm{e}^{\rm{i}tX}$ for $|t|\leq 1/T$. But I cannot go anywhere from here, can someone help me? Thanks. Edit : 
I found out this fact. Let $\psi(t)=|\phi(t)|^2$ which is a characteristic function and its real, and $\psi(t)=1, |t|\leq 1/T$. Now employ the inequality $\Re(1-\psi(t))\leq 4\Re(1-\psi(t/2))$ now apply this $n$ times we get $(1-\psi(t))\leq 4^n\left(1-\psi\left(\dfrac{t}{2^n}\right)\right)$ now for any $t$ we get rhs goes to $0$. But since $\psi$ is real it is also $\le 1$ so $\psi(t)=1$ for all $t$. Now we have $|\phi(t)|=1$ for all $t$. I know this is pretty pointless, but I don't understand any of the proofs given below, if someone would clearly explain why the sets mentioned have only one element in common, I would be grateful.","['characteristic-functions', 'probability']"
1410343,Definition of homeomorphic?,"I am looking up the definition of ""homeomorphic"" and the source I am looking at says there are two different definitions: Possessing similarity of form, Continuous, one-to-one, in surjection, and having a continuous inverse. seems to be speaking of a particular function/mapping, so I'm okay with that. But ""possessing similarity of form"" is not rigorous so I don't understand what is meant by that. Does it just mean there exists a function that is continuous, one-to-one, in surjection, and has a continuous inverse from one set to another? Like when it is said that e.g. ""the $2$-sphere is not homeomorphic to $\mathbb{R}^2$,"" does that mean there exists no function $f: S^2 \to \mathbb{R}^2$ such that $f$ is continuous, one-to-one, in surjection, and has a continuous inverse?","['definition', 'functions']"
1410372,Two disjoint real projective planes in real projective space?,"Let $\mathbb{R}\mathbb{P}^3$ be the real projective three-space. It is clear that any two hyperplanes in $\mathbb{R}\mathbb{P}^3$ intersect. But I wonder whether one could embed two copies of the real projective plane somehow to $\mathbb{R}\mathbb{P}^3$ so that the images are disjoint: Can I find two subsets $P_1, P_2 \subseteq \mathbb{R}\mathbb{P}^3$ which are disjoint and both homeomorphic to the real projective plane $\mathbb{R}\mathbb{P}^2$?","['projective-space', 'general-topology', 'geometric-topology']"
1410373,"""Well known properties"" of Poisson distribution","I'm working with Bradley Efron (2010): Large Scale Inference and my question concerns the proof of Lemma 2.3. Here we have
$z_i \sim F_0$ with probability $\pi_0$, 
$z_i \sim F_1$ with probability $\pi_1$ and $N \sim Poi(\eta)$.
The $z_i$ are independent of each other and $i$ runs from $1$ to $N$. $N_0(\mathcal{Z}) := \vert \{ i: z_i \sim F_0, z_i \in \mathcal{Z} \} \vert $ $N_1(\mathcal{Z}) := \vert \{ i: z_i \sim F_1, z_i \in \mathcal{Z} \} \vert $ $N_+(\mathcal{Z}) := N_0(\mathcal{Z})+ N_1(\mathcal{Z}) $ Due to ""well known properties of the Poisson distribution"" it follows that $N_0(\mathcal{Z}) \sim Poi(\eta \pi_0 F_0(\mathcal{Z}))$ independently of
$N_1(\mathcal{Z}) \sim Poi(\eta \pi_1 F_1(\mathcal{Z}))$ as well as $N_0(\mathcal{Z}) \vert N_+(\mathcal{Z})  \sim Bin(N_+(\mathcal{Z}), \pi_0 F_0(\mathcal{Z}) / F(\mathcal{Z})  )$ With $F = \pi_0 F_0 + \pi_1 F_1$ being the ""mixed distribution"". Can someone help me out with those well known properties needed to conclude 1. and 2. ?","['probability', 'statistics']"
1410396,Motivations for Hyperbolic Geometry,"Why would one study hyperbolic geometry? I am only aware of the motivation where you give axioms for elementary euclidean geometry and then start to wonder wether the parallel axiom is necessary. You then see that if you negate the axiom you get the hyperbolic space instead of the euclidean space. But if this were the only motivation then one might very well learn the construction of the space and then stop. Instead it is taught in elementary and differential geometry and this can't be only because the theorems are exotic if compared to the euclidean case. I am mostly looking for mathematical motivations here, so what are relations to other fields, what are the advanced topics and such. Why is hyperbolic geometry of interest for a mathematician?","['geometry', 'hyperbolic-geometry', 'riemannian-geometry', 'soft-question']"
1410406,Conditions for convergence of derivatives from pointwise convergence,"Let $\{f_n\}_n$ be a sequence of functions $\mathbb R \rightarrow \mathbb R$ which converges pointwise to $f$, ie: $$f_n(x) \rightarrow f(x) \hspace{10pt}\hbox{for all $x$}.$$ What additional conditions are needed so that the derivatives at $0$ of $\{f_n\}_n$ converge to the derivatives at $0$ of $f$ ? One condition that would make sense is ""uniform convergence on all compact sets"" though I can't seem to find references","['reference-request', 'convergence-divergence', 'uniform-convergence', 'derivatives']"
1410439,Evaluating $\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx$,"Evaluating $$\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx$$ I had read the similar problem , but it doesn't work.","['calculus', 'integration']"
1410440,How to compute fraction sums?,"For example, $$\sum\limits_{k=1}^{n}\frac{1}{(2k-1)(2k+1)}=\frac{n}{2n+1}$$
Is there an easier way to evaluate fraction sums (without using partial sums)?","['sequences-and-series', 'calculus']"
1410450,Pentagonal Numbers,"I recently was passing some time on Project Euler, when I came across this question . It deals with finding Pentagonal Numbers $P_j$ and $P_k$ such that $P_j+P_k$ and $P_j-P_k$ are also pentagonal numbers and $P_j-P_k$ is minimized. Without loss of generality, I am simply going to say that $j>k$ for the sake of solving the problem. If you are not familiar with Project Euler, the idea of the site is to provide mathematical problems that can be solved with simple programs. It provides very good exercises in computer science and algorithms. I noticed that some problems on the site can easily be solved with pen and paper (no ""brute force"" calculations needed). I feel as though this problem (though perhaps not as easily as some) can be solved by hand, although I keep hitting dead ends. I have started with what I know:
There are multiple (perhaps even infinitely many) solutions $j,k,n_1,n_2$ to the following: \begin{align}
P_j + P_k &= P_{n_1}\\
P_j - P_k &= P_{n_2}
\end{align} I want to relate these variables in a way such that I can find a class of solutions to this problem. Then, from there it should be easy to find the solution such that $P_j-P_k$ is minimized. Using the relations described above and using the formula for a pentagonal number, I arrive at the following: \begin{align}
3j^2 - j + 3k^2 - k &= 3n_1^2 - n_1\\
3j^2 - j - 3k^2 + k &= 3n_2^2 - n_2
\end{align} From here, I am unable to progress in any beneficial way. It seems that I hit a dead end no matter where I go from here. Any help is appreciated, thanks! EDIT: I am looking for a solution. I simply do not have the proper background in number theory to elaborate on what these hints may be telling me to do. Nevertheless, I am really intrigued by how I could solve this. Please provide or help me to come up with a specific way of solving this problem. Thanks!","['systems-of-equations', 'number-theory', 'diophantine-equations']"
1410478,Is it necessary for the Imaginary-axis to be perpendicular to the Real-axis?,"The Real number line is in one dimension. If you want to map a complex number, you would have to add a second dimension to that number line-   the Imaginary-axis. The Imaginary-axis is always perpendicular to the Real-axis. Here is my question: Would you still be able to use the complex plane if the imaginary-axis wasn't perpendicular to the Real-axis? In other words, is it still possible to prove theorems involving complex numbers (in a geometrical way) if the Imaginary-axis wasn't perpendicular to the Real-axis? For example: Could you prove Euler's formula if the Imaginary-axis was tilted to a 45 degree angle?","['analytic-geometry', 'complex-analysis', 'complex-numbers']"
1410499,Primitive of $dz/z$ is a branch of log,"Let $D$ a connected open set of $\mathbb{C}$. A continuous function $f:D\to \mathbb{C}$ is a branch of log if $e^{f(t)}=t$ on $D$. In my book (Cartan) it is written that if $F$ is a primitive of the 1-form $dz/z$ on $D$ then $F$ is a branch of log on $D$. It is easy to show that if $f$ is a branch of log on $D$, then $f'(t)=1/t$. So I'd like to prove the converse statement. Any suggestions?","['complex-analysis', 'logarithms']"
1410510,"$f:[a,b]\to \mathbb R$ is continuous , has a finite number of local maxima and minima ; then how to prove that $f$ is bounded variation on $[a,b]$ ?","If $f:[a,b]\to \mathbb R$ is a continuous function having  finite number of local maxima and minima ; then how to prove that $f$ is bounded variation on $[a,b]$ ?","['analysis', 'continuity', 'real-analysis', 'bounded-variation']"
1410556,Riemann-Roch analysis of point divisor ring on smooth genus 3 Riemann surface,"Let $C=C_4\subset\mathbb{P}^2$ be the smooth genus 3 Riemann surface given by a quartic curve.
Let $P\in C$ be a point, and $D=P$ the divisor given by the point $P$.
Let $R(D)=\bigoplus_{n\geqslant0}\mathcal{L}(nD)$ be the graded ring associated with the divisor $D$. We know that a canonical divisor $k$ on $C$ has degree $2g-2=4$ and that it is thus linearly equivalent to a hyperplane divisor $H=H_L$ for any line $L\subset\mathbb{P}^2$. So by Riemann-Roch, we know that $\ell(nD)=n-2$ for $n\geqslant5$, since then $\deg(k-nD)<0$ thus $\ell(k-nD)=0$. My question then is how do we calculate $\ell(nD)$ for $n=2,3,4$ ? The cases $n=0,1$ give us $\ell(nD)=1$, and we know that $\ell(nD)$ is non-decreasing.
If we can further show that $\ell(nD),\ell(k-nD)>0$ then we can use Clifford's theorem to obtain some bounds, but this still gives us a few possible options.","['algebraic-geometry', 'riemann-surfaces', 'algebraic-curves']"
1410582,Normalizer of a Sylow 2-subgroups of dihedral groups,"I can't solve the following exercise which is the last exercise in page 146 of Dummi & Foote's Abstract Algebra : Let $2n=2^ak$ where $k$ is odd. Prove that the number of Sylow 2-subgroups of $D_{2n}$ is $k$. [Prove that if $P\in Syl_2(D_{2n})$ then $N_{D_{2n}}(P)=P.$] Here $D_{2n}=<r,s\mid r^n=s^2=1,\,rs=sr^{-1}>$ is the dihedral group of order $2n$ and $Syl_2(D_{2n})$ is the set of all Sylow 2-subgroups of $D_{2n}$. It is easy to solve the exercise if we prove that $N_{D_{2n}}(P)=P$ and that's what I can't prove. Could you give me some hints? My attempt I got the following results. They might be useless, but who knows? 1) If $a=1$ then Sylow $2$-subgroups of $D_{2n}$ have order $2$ and so their number is the number of elements of $D_{2n}$ of order $2$ which is $n$ because the set of elements of order $2$ of $D_{2n}$ is $\{sr^i:i\in\{0,\,\dots,\,n-1\}\}$. So we can assume that $a>1$. 2) Let $P\in Syl_2(D_{2n})$ and suppose $a>1$ (from 1). Then $P$ must have an element of order $2$ in $\{sr^i:i\in\{0,\,\dots,\,n-1\}\}$ . In fact, we know that all cyclic subgroups of order larger than $2$ of a dihedral group are generated by a power of $r$, so if $P$ is cyclic then $\exists m\in\mathbb{Z}^+,\,P=<r^m>$ and $m$ is a divisor of $n$. Then we have:$|r^m|=\frac{2^{a-1}k}{m}=2^a$ and so $k=2m$ which is false because $k$ is odd. Thus $P$ isn't cyclic and so $P$ is generated by at least two elements ($P$ is finite so it has a finite number of generators and let $S$ be a set of generators of $P$ so that $P=<S>$). If all elements of $S$ have order larger than $2$ they must be powers of $r$ and so $S\subset <R>$ thus $P\le <r>$ which is impossible, otherwise $P$ would be cyclic. Thus at least one element in $S$ has order $2$. But we know that there exists a unique element of $<r>$ which order is $2$. Thus we have two possible cases: Case 1: There is an element of order larger than $2$ in $S$. This element must be a power of $r$. If all other elements of $S$ are powers of $r$, then $P\le <r>$ and we get the same contradiction as before. Case 2: All elements of $S$ have order $2$. There are at least two distinct ones otherwise $P$ would be cyclic. They can't both be powers of $r$ since only one power of $r$ has order $2$ and so one isn't a power of $2$. Conclusion: In both cases, there's at least one element of $S$ which order is $2$ and isn't a power of $r$, so $\{sr^i:i\in\{0,\,\dots,\,n-1\}\}\cap P\neq\emptyset$ So here's all I the informations I could get (that might be useless and the exercise might not need any of them ^^). Could you please give me some hints?","['sylow-theory', 'dihedral-groups', 'abstract-algebra', 'group-theory', 'finite-groups']"
1410595,"Sum $x_1^{2015}+x_2^{2015}+x_3^{2015}+x_4^{2015}$ where $x_1, \dots, x_4$ are the roots of $f=X^4+4X^3+6X^2+aX+b$","Consider the polynomial: $$f=X^4+4X^3+6X^2+aX+b$$
We know that $f$ has four real roots. Let $x_1,x_2,x_3,x_4$ be the roots of this polynomial. How can one compute $$x_1^{2015}+x_2^{2015}+x_3^{2015}+x_4^{2015}?$$
If $a=4$ and $b=1$, we obtain a self-reciprocal (palindromic) polynomial. We can write $f=(X+1)^4$, thus $x_1=x_2=x_3=x_4=1$. Hence the sum computes to $-4$.
Are there any other cases to consider ($a,b$)? I thought using the formula for the quartic equation and paying attention to the cases where we have only real roots.
Any ideas? Thank you!","['polynomials', 'algebra-precalculus', 'roots']"
1410596,Find: $\lim_{n \to \infty} \int_0^{\infty} \arctan(nx) e^{- x^n}dx$,"Find: $$\lim_{n \to \infty} \int_0^{\infty} \arctan(nx) e^{- x^n}dx$$ Probably, no recursive form could be found, and elementary tools (integration by parts, change of variable, etc.) are not useful here. How can I find such a limit? Thank you.","['calculus', 'real-analysis', 'limits', 'integration']"
1410604,Prove $((a+b)/2)^n\leq (a^n+b^n)/2$,"Struggling with this proof. Prove that $$\left(\frac{a+b}{2}\right)^n≤\frac{a^n+b^n}{2},$$ where $a$ and $b$ are real numbers such that $a+b≥0$ and $n$ is a positive integer. What technique would you use to prove this (e.g. induction, direct, counter example). How would you go about proving it? Thanks in advance.","['algebra-precalculus', 'inequality']"
1410623,Calculate $\lim_{n\to\infty} (\frac{1}{{1\cdot2}} + \frac{1}{{2\cdot3}} + \frac{1}{{3\cdot4}} + \cdots + \frac{1}{{n(n + 1)}})$,"Calculate $$\lim_{n\to\infty} \left(\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} +  \cdots + \frac{1}{n(n + 1)}\right).
$$ If reduce to a common denominator we get $$\lim _{n\to\infty}\left(\frac{X}{{n!(n + 1)}}\right).$$ How can I find $X$ and calculate limit?","['telescopic-series', 'sequences-and-series', 'limits', 'algebra-precalculus']"
1410635,"If $f: \Bbb{N} \to \Bbb{N}$ is strictly increasing and $f(f(n))=3n$, find $f(2001)$. [duplicate]",This question already has an answer here : Let $f: \mathbb N \rightarrow \mathbb N$ are increasing function such that $f\left(f(n)\right)=3n$. Find $f(2017)$ [duplicate] (1 answer) Closed 4 years ago . I have this question which seems a little harder than I thought. It has been about an hour for me hitting aimless thoughts on this one. I can really use a hint here if some one knows how to tackle it. Let $f: \Bbb{N} \to \Bbb{N}$ such that $f$ is strictly increasing and $f(f(n))=3n$ for all $n \in \Bbb{N}$. Find $f(2001)$.,"['functional-equations', 'functions']"
1410642,Proving that a set is open using epsilons.,"I'm trying to prove that the set $$A=\{x=(x_{1},x_{2})\in\mathbb{R}^2:x_{1}^{2}+x_{2}^{2}>1\}$$ is open in $\mathbb{R}^2$ with the usual norm is open with the definition of ""epsilons"". My attempt is based in taking $\epsilon=x_{1}^{2}+x_{2}^{2}-1,$ but simply don't achieve to prove that $B(x,\epsilon)\subset A.$ I tried too using another norm because of the equivalence among norms, but it's useless. Another form to attack the problem is taking complement over A, so $A^c$ is closed that's why is an closed ball. However I'd know the path using epsilon definition. Anyone could help me? Thanks in advance.","['metric-spaces', 'multivariable-calculus', 'general-topology']"
1410646,A function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous.,"I'm having a confusion over the veracity of the statement that a function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous. I've seen from a problem in Spivak's Calculus that this statement is true. Proof: For the case where $f$ takes on each value only once: If $f$ is not continuous, there exists $\epsilon gt 0$ such that there are $x$'s arbitrarily close to $a$ with $f(x)\gt f(a)+\epsilon$, or $f(x)\lt f(a)-\epsilon$. Say the first, then there are $x$'s arbitrarily close to $a$ with $x\gt a$ or $x\lt a$, with $f(x)\gt f(a)+\epsilon$. Say the first. Then by IVT, there is $x'\in (a,x)$ with $f(x')\lt f(a)+\epsilon.$ Also by assumption, there is a $y\in (a,x')$ with $f(y)\gt f(a)+\epsilon$. So by IVT, we can find a $x_1\in (y,x'),$ and $x_2\in (x',x)$ with $f(x_1)=f(x_2)=f(a)+\epsilon$, which is a contradiction. The proof for the case where $f$ takes on each value only finitely many times proceeds by inductive reasoning to the above proof. However, I just came across a problem that states that if $h:[0,1]\to R$ takes each value exactly twice, then $h$ cannot be continuous on $[0,1]$. The proof to this problem contradicts the above statement. Proof: Suppose that $h$ is continuous. (1) By the Min-Max Theorem, $\exists c_1\in [0,1]$, which attains the maximum value of the function. Since each value is taken exactly twice, there is another $c_2\in [0,1]$, say $c_1\lt c_2$ such that $h(c_1)=h(c_2)$. Now if $0\lt c_1$, then we can choose $a_1,a_2\in (0,1)$ and a real number $k$ such that $0\lt a_1\lt c_1\lt a_2\lt c_2$ and $h(a_1)\lt k \lt h(c_1), h(a_2)\lt k \lt h(c_2)$. Then since $h(c_1)=h(c_2)$ are maximum and $h$ is continuous, by IVT there are $b_i\in (0,1)$ such that $h(b_i)=k$ and $a_1\lt b_1\lt c_1\lt b_2\lt a_2\lt b_3\lt c_2$. This is a contradiction. Hence $c_1=0.$ Likewise $c_2=1$. (2) Now by the same reasoning as above, we can show that there are $d_1, d_2$ for which $h$ attains the minimum and $d_1=0, d_2=1$. Hence by (1) and (2) h is a constant function, which is a contradiction to the assumption. Now I don't see any errors in the reasoning of either proof, but the conclusions seem contradictory. How can I reconcile this situation?","['analysis', 'continuity', 'calculus', 'real-analysis']"
1410654,Almost sure convergence through subsequences,"$\{X_i\}$'s are independent Poisson random variables with parameters $\lambda_i$ respectively satisfying $\sum_{n=1}^{\infty}\lambda_n=\infty$. Define $S_n=X_1+X_2+\cdots +X_n$ then show that $$\frac{S_n}{\mathbb{E}(S_n)}\to 1 \quad \text{a.e}$$ My try : This looks like a problem with the method of subsequences. Let us first prove convergence in probability.
$$P\left(|S_n-\mathbb{E}(S_n)|>\delta \mathbb{E}(S_n)\right)\le \frac{\text{var}(S_n)}{\delta^2(\mathbb{E}(S_n))^2}=\frac{1}{\delta^2\mathbb{E}(S_n)}\tag{$\ast$}$$
Since variance and mean of poisson are the same. Also the given series implies $\mathbb{E}(S_n)\to \infty$. So we have the probability convergence. Now define $n_k=\min \{n \mid \mathbb{E}(S_n)\geq k^2\}$ Now putting $T_k=S_{n_k}$ in $(\ast)$ we get that $$P\left(\left|\frac{T_k}{\mathbb{E}(T_k)}-1\right|>\delta\right)\leq \frac{1}{\delta^2k^2}$$ so summing over $k$, we get the lhs converges hence an easy application of Borel Cantelli Lemma 1 gives $\dfrac{T_k}{\mathbb{E}(T_k)}\to 1 \quad \text{a.e}$. Now I want to prove $$\frac{T_k}{\mathbb{E}(T_{k+1})}\le \frac{S_n}{\mathbb{E}(S_n)}\le \frac{T_{k+1}}{\mathbb{E}(T_{k})}$$ for $n_k\le n<n_{k+1}$ which would give me my solution. Which is obvious. But to finish the proof I need $\dfrac{\mathbb{E}(T_k)}{\mathbb{E}(T_{k+1})}\to 1$. But I can't show this fact. How to do this? I tried to do it with $k^2$ replaced by $2^k$, but it doesn't help. Can someone help? EDIT : I found this pdf which solves this problem in page 3. There they claim, that if we show $I_n/\mathbb{E}(I_n)\to 1 \; \text{a.e}$ then they also have $S_n/\mathbb{E}(S_n)\to 1\; \text{a.e}$. Why is that? I can see the convergence in probability for $S_n$ but nothing further. Can someone help me? Thanks a lot.","['probability-theory', 'independence', 'borel-cantelli-lemmas']"
1410668,Intuition: why distinct eigenvalues $\implies$ linearly independent eigen vectors?,"Suppose you have an $~n\times n~$ matrix with $~n~$ distinct (not repeated) eigenvalues. There is a theorem telling us that the eigen vectors corresponding to these eigenvalues must be linearly independent . I can basically follow the proof, but I am looking for an intuitive explanation of why this is the case. Can anyone offer some insight?","['eigenvalues-eigenvectors', 'linear-algebra']"
1410672,Is it true that $A \in A$?,"I defined the set $A$ as follow: \begin{align}
A_0 & =\varnothing \\
A_1 & =\{A_0\}=\{\varnothing\} \\
A_2 & =\{A_1\}=\{\{\varnothing\}\} \\
A_3 & =\{A_2\}=\{\{\{\varnothing\}\}\} \\
& {}\ \ \vdots \\
A & =A_\infty= \{\{\{\cdots\{\varnothing\}\cdots\}\}\}
\end{align} And now this is clear that $A\in A$. Is there any mistake in my conclusion?","['elementary-set-theory', 'logic']"
1410687,Books on Prime numbers,I am a graduate student and have just finished Burton's book on number theory. Now I want to read further on prime numbers. Does anyone have any suggestion?,"['book-recommendation', 'prime-numbers', 'number-theory']"
1410737,Limit of a sequence of holomorphic functions,"Let $f_n$ be a sequence of holomorphic functions on a domain $D \subset \mathbb{C}$ converging to a function $f$, and also converging uniformly on compact subsets. Suppose each function has at most $m$ zeros (counted with multiplicity) for some fixed $m \in \{0, 1, 2, ... \}$. Show that either $f$ is exactly zero on $D$ or has at most $m$ zeros on $D$. I'm not sure how to approach this problem, but a hint to get my started would be much appreciated. Context: I'm studying for a qual.",['complex-analysis']
1410743,Evaluate $\int \theta\sec\theta \tan\theta \ d\theta$,"integral of $\int \theta\sec\theta \tan\theta \  d\theta$ my work $\frac{d}{d\theta}\sec(θ) = \sec(\theta)\tan(\theta)$ So if we let $u = \theta$ and $v' = \sec(\theta)\tan(\theta)$, then we get: $u = \theta, du = d\theta$ and $v = \sec(\theta), dv = \sec(\theta)\tan(\theta)d\theta$ Hence $$\int \theta \sec(\theta)\tan(\theta) d\theta  = \theta\sec(\theta) - \int\sec(\theta) d\theta $$ Now, the integral of $\sec(\theta)$ is a particularly tricky integral, but it comes to: $$\int \sec(\theta) d \theta = \ln|\sec(\theta) + \tan(\theta)| + C$$ integral comes to: $$\int \theta \sec(\theta)\tan(\theta) d\theta  = \theta \sec(\theta) - \ln|\sec(\theta) + \tan(\theta)| + C $$ but my answer is not like this picture please help me","['calculus', 'integration']"
1410750,Why is the solution of an ordinary differential equation required to be defined on an interval? [duplicate],"This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I am reading A First Course in Differential Equations with Modeling Applications (10th Edition) and here is a definition: Any function $\phi$, defined on an interval $I$ and possessing at least $n$ derivatives that are continuous on $I$, which when substituted into an $n$th-order ordinary differential equation reduces the equation to the identity, is said to be a solution of the equation on the interval. I am wondering why the condition ""on an interval $I$"" is important (this has been emphasized in other places of the book). For example, why it is not okay to say that $$y=x^{-1}, x\neq 0$$ is a solution to the differential equation $y'=-x^{-2}$?","['definition', 'ordinary-differential-equations']"
1410774,What allows us divide/multiply dx in calculus?,"I've read nearly all of the threads on this topic but none seem to answer my question or lead me in the best direction. When performing U-substituion or even in it's most basic form: $y = 2x$ , $dy=2\,dx$ . What allows us to do this and what/where does in come in a calculus textbook? None of my teachers have been able to explain it to me. It makes it difficult for me to proceed when even the basics don't seem intuitive. I've read that the idea of differentials varies but is there a single understanding regarding them to which I can pertain? I feel I'm missing something by just doing it in my calculations and not understanding why. I've genuinely read all posts regarding this, it's been something on my mind for a long time so please don't close this. Cheers",['calculus']
1410793,Matrix representations of tensors,"I've been trying to teach myself general relativity, and I always get stuck at the same point: I don't really understand what the metric tensor is. Unless I'm incorrect, and please correct me if I'm wrong, I realize that it defines a geodesic distance in some coordinate system over some manifold. However, what ultimately confuses me is the calculation. For example, the metric tensor in Euclidean space is defined to be $g_{ij}= \frac{\partial x^k}{ \partial x^i}\frac{\partial x^l}{ \partial x^j}$, according to Wikipedia anyway ( https://en.wikipedia.org/wiki/Metric_tensor ). But doesn't this have a matrix representation? If so, I have absolutely no idea what it would look like. Can someone give an example. Furthermore, I know that the metric tensor can be used to raise or lower the index of an arbitrary tensor. For example, considering the Riemannian curvature tensor, $R_{ijkl} = g_{kp}R^p_{ijl}$. Is there a matrix equation (using matrix representation) that explains why this happened. I have read ( Using metric to raise and lower indices ) and understand that it has to do with a change from covariant to covariant basis. But if $g_{kp}$ is a matrix with a certain number of rows and columns, mustn't $R^p_{ijl}$ also be a matrix of rows and columns? Did any of them disappear upon taking the transformation? This does not seem possible. So what is the difference between a matrix of covariant elements versus contravariant elements? Please use matrices to explain and not Einstein summation notation. Secondly, what does the inverse of the metric tensor look like and how does it cancel out indeces of another matrix? For example, the Ricci tensor is given by $R_{ik} = g^{jl}R_{ijkl}$. What happened to those elements in the matrix $R_{ijkl}$ that look like $a_{ijkl}$? Lastly, can anyone give a matrix representation of a Christoffel symbol $\Gamma^k_{ij} = \frac{1}{2} g^{kl} \frac{ \partial g_{jl}}{ \partial x^i}+ \frac{ \partial g_{il}}{ \partial x^j}− \frac{ \partial g_{ij}}{ \partial x^l}$? What does the partial derivative of the matrix $g_{jl}$ in terms of $x^i$ look like? How does the matrix $g_{jl}$ differ from the matrix $g_{il}$ and from the matrix $g_{ij}$? Are they somehow the same matrix? What I am ultimately trying to figure out is how to do actual calculations using tensors. Ultimately, there must be a list of numbers representing coordinates in a given reference frame. But how do these numbers interact with one another when indeces are changed and canceled out? If anyone can shed light on this, using examples with numbers if possible, I would greatly appreciate it.","['general-relativity', 'tensors', 'matrices']"
1410799,Area of a sphere bounded by hyperplanes,"Say we have a sphere in d-dimensional space, and k hyperplanes (d-1 dimensional) all passing through the origin. Is there a way to calculate (or approximate) the area of the surface of the sphere enclosed by the half-spaces \begin{align*}w_1 \cdot x &\leq 0 \\
w_2 \cdot x &\leq 0\\ \vdots& \\w_k \cdot x &\leq 0\end{align*}","['spherical-geometry', 'geometry']"
1410803,evaluating some limits with $\ln(x)$,I don't understand how to prove these results. $\lim\limits_{x \to +\infty}\dfrac{\ln{x}}{x} = 0$ $\lim\limits_{x \to 0^{+}}x\ln{x} = 0$,"['calculus', 'limits', 'logarithms']"
1410854,Find the image of $|z+1|=2$ under $f(z) = \frac{1}{z}$ where $z \in \mathbb C$,Find the image of $|z+1|=2$ under $f(z) = \frac{1}{z}$ where $z \in \mathbb C$ My attempt: Let $z = x + iy$ $\displaystyle |z+1|=2 \iff | (x + iy)+1|=2 \iff |(x+1) +iy|=2 \iff (x+1)^2 + y^2 = 4$ Let $w = u + iv$ Now let $\displaystyle w = \frac{1}{z}$ hence we have that \begin{align}z &= \frac{1}{w} \\ &= \frac{1}{u + iv} \\ &= \frac{u-iv}{u^2 + v^2} \\ &= \frac{u}{u^2 + v^2} + i \big( - \frac{v}{u^2 + v^2} \big)\end{align} From which we can deduce that $\displaystyle x = \frac{u}{u^2 + v^2}$ and $\displaystyle y = - \frac{v}{u^2 + v^2}$ and thus $$\displaystyle \bigg(\frac{u}{u^2 + v^2} +1\bigg)^2 + \bigg(- \frac{v}{u^2 + v^2}\bigg)^2 = 4$$ This is where I am stuck. I keep on messing up the simplification. Can someone please show me how to simplify this?,['complex-analysis']
