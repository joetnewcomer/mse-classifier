question_id,title,body,tags
4638349,When can we interchange partial and total derivates?,"I have a question regarding a math proof, which can be found on 17 (Lemma 4) on this document . A screenshot is provided below. I am a bit unsure how equation 29 is derived. They seem to be using some sort of
interchange of partial derivatives, but I don't understand the justification of switching partial derivative for the total derivative.","['partial-derivative', 'calculus', 'derivatives']"
4638373,Prove $\frac{Ax}{ \Vert Ax \Vert}$ is a contraction mapping,"I am going through a proof of the Perron-Frobenius theorem which uses the Banach fixed point theorem. The author first asks the reader to consider the space $$
X = \left\{ x \in  \mathbb{R}^{d} : \Vert x\Vert^{2} = 1 \text{ and } x_{i} \geq 0 \right\}
$$ Then consider the matrix $A$ with strictly positive entries. We define the map: $$
T(x) = \frac{Ax}{\Vert Ax \Vert}
$$ The authors then argue that, since T is a contraction mapping with respect to the geodesic sphere distance, the Banach fixed point Theorem can be applied. This results in a unique fixed point satisfying the conditions of the Theorem. That is, there exists only one eigenvector with strictly positive entries (Proving that the corresponding eigenvalue is maximal is done in the latter part of the proof). I understand the overall idea of the proof but am struggling to work out why $T$ is a contraction mapping. The spherical distance is given by: $$
d(x, y) = \cos^{-1}(x^{\top} y)
$$ Thus we need to prove that $$
\cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq k\cos^{-1}(x^\top y)
$$ for some $0 < k < 1$ . One observation I tried using is that $\cos^{-1}$ monotonically decreasing on the domain $[-1, 1]$ . Thus being able to show $$
\cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq \cos^{-1}(x^\top y)
$$ is equivalent to showing that $$
\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert} \geq x^\top y
$$ My plan was then to use basic facts about operator norms to prove this inequality but I couldn't make any progress. EDIT: This question is based on the following lecture notes .","['linear-algebra', 'functional-analysis']"
4638382,More elegant way to prove $\cos(\phi)\leq 1-\frac{2\phi^2}{\pi^2} $?,"Today I came across the inequality $$
\cos(\phi)\leq 1-\frac{2\phi^2}{\pi^2}\tag{1}
$$ which holds for (actually, iff) $\phi\in[-\pi,\pi]$ ;
this can be used, e.g., to show that for all $z\in\mathbb C\setminus\{0\}$ $$
\operatorname{Arg}(z)\leq \frac\pi2\left|\frac{z}{|z|}-1\right|\,.
$$ I believe most calculus students would prove (1) by showing that the function \begin{align*}
f:[0,\pi]&\to\mathbb R\\
\phi&\mapsto 1-\frac{2\phi^2}{\pi^2}-\cos(\phi)
\end{align*} is non-negative.
As $f$ is differentiable it attains its minimum either at the boundary or at a critical point.
As $f(0)=0=f(\pi)$ it suffices to show that $f$ restricted to $(0,\pi)$ has no local minima -- this can be done via some basic, yet somewhat tedious calculations (e.g., "" $\sin(\phi)=\frac{4}{\pi^2}\phi$ has exactly one solution in $(0,\infty)$ , and this solution is in $(\frac{\pi}2,\pi)"")$ . This seems overly complicated and it feels like there should exist a more ""elegant"" approach to this problem, but so far I could not come up with anything of sorts.
For example similar inequalities can be cast into inequalities of certain integrals (e.g., here ), but I was not yet able to cast the above problem into such a framework. Any comments and ideas are welcome!","['alternative-proof', 'calculus', 'trigonometry', 'inequality']"
4638480,Calculating a definite integral when $1/0$ arises twice and seems to cancel out.,"We have \begin{align*} I=&\int_{0}^{\frac{\pi}{2}}\frac{1}{1+\sin x}dx \\
=&\int_{0}^{\frac{\pi}{2}}\frac{1-\sin x}{\cos^{2}x}dx \\
=&[\tan x]^{\frac{\pi}{2}}_{0} -\int_{0}^{\frac{\pi}{2}}\frac{\sin x}{\cos^2x}dx
\end{align*} And by letting $u=\cos x \Rightarrow du =-\sin x dx$ We get \begin{align*} I=&[\tan x]^{\frac{\pi}{2}}_{0} +\left[ \frac{1}{u}\right]^{1}_{0}  \\
=&\frac{1}{0}+1-\frac{1}{0} \\ =&1\end{align*} Is the last step right? Can you just cancel these out like that or should you do it another way? Because the answer here is in fact $1$ . This might be more of an algebra question and that's why I tagged it as such. I've never come across this before.","['calculus', 'algebra-precalculus']"
4638500,A dodgy contour integration method giving the correct result,"Consider the following integral $$I = \int_{-\infty}^\infty \frac{x}{\sinh x}dx$$ Using contour integration with some rectangular contour, it is not too hard to show that the integral evaluates to $\pi^2/2$ . However, as shown in this reply , it is apparently possible to get the correct answer using a semi-circular contour in the upper half-plane, despite the fact that the integral over the contour technically doesn't vanish. As the radius of the semi-circular contour tends to infinity, there are an infinite number of poles contained within the contour with residues $i\pi n$ and $-i\pi n$ for $n$ is even and odd respectively. By assuming that the integral on the semi-circular contour vanishes, the residue theorem yields $$I = 2\pi i \sum_{n=0}^\infty (-1)^n (i\pi n) = -2\pi^2\sum_{n=0}^\infty (-1)^n n = \frac{\pi^2}{2}$$ if we consider that the last series goes to $-1/4$ . Now, despite the fact that the final answer is correct, the method used seems heretical for three reasons: Assuming the integral on the semi-circular contour vanishes when it clearly doesn't on the imaginary axis. Considering an infinite number of poles within the contour (although it's not too hard to accept that the residue theorem can also be used if the number of poles within a contour is countably infinite). Assuming that the last traditionally divergent series actually converges in this context. Why does this nevertheless yield the correct result? It's a safe assumption that it is not coincidental, since the entire family of integrals $$I(n) = \int_{-\infty}^\infty \left(\frac{x}{\sinh x}\right)^n dx$$ can seemingly be evaluated correctly by using this method, with the only difference being the type of divergent series at the end.","['complex-analysis', 'divergent-series', 'contour-integration']"
4638503,"Integral from MIT Integration Bee 2023 Quarterfinals - $\lim_{n\to\infty} \frac{1}{n} \int_{0}^{n} \cos^2(\pi x^2/\sqrt{2}) \, \mathrm{d}x$","This question is from the MIT Integration Bee 2023 Quarterfinal #2. I'm doing these integrals to improve my quick math skills, and was wondering if there's a way to solve this within two minutes.
The goal is to show that $$\lim_{n\to\infty} \frac{1}{n} \int_{0}^{n} \cos^2\left(\frac{\pi x^2}{\sqrt{2}}\right) \,\textrm{d}x = \frac{1}{2} $$ I've tried L'Hospital's Rule then FTC1 but I don't think the first part would even apply here. I'm not sure how to approach such a problem, especially if it's to be solved within two minutes.","['integration', 'definite-integrals']"
4638524,Projective Space in Differential Geometry,"I'm currently reading M. do Carmo's, ""Riemannian Geometry.""  I've noticed that projective $n$ -space $\mathbb{RP}^n$ or $\mathbb{CP}^n$ is often used as an example in this differential geometry text and several others.  As far as I can tell, projective space is mostly applicable to algebraic geometry.  Why is projective space given as an example in differential geometry texts, when it doesn't seem to be much used in this area?","['algebraic-geometry', 'projective-space', 'smooth-manifolds', 'differential-geometry']"
4638580,Are neighbourhood deformation retracts transitive?,"I have the following definition of a neighbourhood deformation retract (or NDR, for short): A pair $(X, A)$ is called an NDR if $A\subseteq X$ is closed and there exists a neighbourhood $A\subseteq V\subseteq X$ and a retraction $r: V\rightarrow A$ , i.e. $r|_A=\mathrm{id}_A$ and there is a homotopy $H: V\times [0, 1]\rightarrow V$ such that $H(-, 0)=r, H(-, 1)=\mathrm{id}_V$ and $H|_{A\times [0, 1]}=\mathrm{pr}_A$ . Now I was wondering about the following: If $A\subseteq B$ is an NDR and $B\subseteq X$ is an NDR, does it follow that $A\subseteq X$ is an NDR? I suspect this is true, but somehow I cannot produce a proof. I was trying to argue as follows: There is a neighbourhood $A\subseteq V\subseteq B$ and a retraction $r: V\rightarrow A$ ; moreover, there is a neighbourhood $B\subseteq W\subseteq X$ and a retraction $s: W\rightarrow B$ . Now I want to choose $V'=s^{-1}(V)\subseteq X$ as a neighbourhood of $A$ in $X$ and use $r\circ s: V'\rightarrow A$ as a retraction. However, the problem is that I do not know whether the homotopy $H_s: W\times [0, 1]\rightarrow W$ between $s$ and $\mathrm{id}_W$ restricts to a homotopy $V'\times [0, 1]\rightarrow V'$ , i.e. if it keeps points of $V'$ inside $V'$ . Am I missing something here?","['general-topology', 'homotopy-theory', 'algebraic-topology']"
4638583,Why is this almost affine code not equivalent to a linear code?,"I am a bachelor student who just started studying coding theory and I came across the following example in ""Generalized Hamming weights for almost affine codes"" by Johnsen and Verdure, page 4 ( https://arxiv.org/abs/1601.01504 ): We will use a running example throughout this paper. It is the almost affine code C
′
in [14, Example 5]. It is a code of length 3 and dimension 2 on the alphabet F = {0, 1, 2, 3}. Its set
of codewords is $$
\begin{matrix}
000 & 011 & 022 & 033 \\
101 & 112 & 123 & 130 \\
202 & 213 & 220 & 231 \\
303 & 310 & 321 & 332 \\
\end{matrix}
$$ [...] This is an example of an almost affine code which is not
equivalent to a linear code, and not even to a multilinear code. I understand why the code is almost affine. However, to me, it seems like this is a linear code, since all the codewords can be expressed as linear combinations of 101 and 011. What mistake am I making? Also, how would I check that it is not equivalent to a multilinear code? Thank you in advance.","['matroids', 'coding-theory', 'combinatorics', 'discrete-mathematics']"
4638640,"An attempt at generalizing a family of integrals $\int_0^{\alpha}{\frac{\sin \left( \pi x \right)}{x^x\left( \alpha -x \right) ^{\alpha -x}}}\, dx$","Main question $$
\int_0^1{\frac{\sin \left( \pi x \right)}{x^x\left( 1-x \right) ^{1-x}}}\mathrm{d}x
$$ This is a famous integral. It can be evaluated by contour integration, and it equals to $\pi/e$ .
I tried to generalized it, and I wrote $$
I_{\alpha}=\int_0^{\alpha}{\frac{\sin \left( \pi x \right)}{x^x\left( \alpha -x \right) ^{\alpha -x}}}\mathrm{d}x
$$ here $\alpha\in\left(0,\infty\right)$ . By substituting $x=\frac{\alpha}{e^t+1}$ , $I_{\alpha}$ becomes $$
\alpha ^{1-\alpha}\int_{-\infty}^{\infty}{\frac{e^{t\left( 1-\alpha \right)}}{\left( 1+e^t \right) ^{2-\alpha}}\sin \left( \frac{\alpha \pi}{e^t+1} \right) \exp \left( \frac{\alpha t}{e^t+1} \right)}\mathrm{d}x
$$ Or equivalently $$
\alpha ^{1-\alpha}\int_{\infty}^{-\infty}{\frac{e^{t\left( 1-\alpha \right)}}{\left( 1+e^t \right) ^{2-\alpha}}\sin \left( \frac{-\alpha \pi}{e^t+1} \right) \exp \left( \frac{\alpha t}{e^t+1} \right)}\mathrm{d}x
$$ This implies \begin{align*}
I_{\alpha}&=\alpha ^{1-\alpha}\Im \left\{ \int_{-\infty}^{\infty}{\frac{e^{t\left( 1-\alpha \right)}}{\left( 1+e^t \right) ^{2-\alpha}}\exp \left( \frac{\alpha \left( t+i\pi \right)}{e^t+1} \right)}\mathrm{d}x \right\} 
\\&
=\alpha ^{1-\alpha}\Im \left\{ \int_{\infty}^{-\infty}{\frac{e^{t\left( 1-\alpha \right)}}{\left( 1+e^t \right) ^{2-\alpha}}\exp \left( \frac{\alpha \left( t-i\pi \right)}{e^t+1} \right)}\mathrm{d}x \right\} 
\end{align*} Then I tried to apply contour integration here. $$
f\left( z \right) =\frac{e^{z\left( 1-\alpha \right)}}{\left( 1-e^z \right) ^{2-\alpha}}\exp \left( \frac{\alpha z}{1-e^z} \right) 
$$ The contour I used I integrate this contour clockwise. By residue theorem $$
\oint_C{f\left( z \right) \mathrm{d}z}=\int_{\psi _-}{+\int_{\psi _+}{+}\int_{\phi _-}{+}\int_{\phi _+}{f\left( z \right) \mathrm{d}z}}=2\pi i\mathrm{Res}\left[ f\left( z \right) ,0 \right] 
$$ The vertical parts actually vanish when $R$ approaches infinity.
As for the horizontal part, since $$
f\left( x+i\pi \right) =\frac{e^{x\left( 1-\alpha \right)}e^{i\pi \left( 1-\alpha \right)}}{\left( 1+e^x \right) ^{2-\alpha}}\exp \left( \frac{x+i\pi}{1+e^x} \right) 
\\
f\left( x-i\pi \right) =\frac{e^{x\left( 1-\alpha \right)}e^{i\pi \left( \alpha -1 \right)}}{\left( 1+e^x \right) ^{2-\alpha}}\exp \left( \frac{x-i\pi}{1+e^x} \right) 
$$ therefore, by adding the horizontal part together, there is $$
\Im \left\{\int_{\psi _-}{+\int_{\psi _+}{f\left( z \right) \mathrm{d}z}}\right\} = \frac{2\cos \left( \pi \left( \alpha -1 \right) \right)}{\alpha ^{1-\alpha}} I_{\alpha} 
$$ So I have $$
I_{\alpha}=\alpha ^{1-\alpha}\Im \left\{ \frac{\pi i\mathrm{Res}\left[ f\left( z \right) ,0 \right]}{\cos \left( \pi \left( \alpha -1 \right) \right)} \right\} 
$$ Then I was stuck. How do I calculate this residue? It's somehow very difficult for me... Have I done anything worng? Addendum 1 There are few questions stemmed form this question. Such as $$
J_{\alpha}=\int_0^{\alpha}{\frac{\sin \left( \pi x/\alpha \right)}{x^x\left( \alpha -x \right) ^{\alpha -x}}}\mathrm{d}x
\\
K_{\alpha ,\beta}=\int_0^{\alpha}{\frac{\sin \left( \pi x \right)}{x^{x+\beta}\left( \alpha -x \right) ^{\alpha -x}}}\mathrm{d}x
\\
L_{\alpha ,\beta}=\int_0^{\alpha}{\frac{\sin \left( \pi x/\alpha \right)}{x^{x+\beta}\left( \alpha -x \right) ^{\alpha -x}}}\mathrm{d}x
$$ which I doubt a closed form even exist. Another interesting thing, it seems that $$
K_{1,1}=\int_0^1{\frac{\sin \left( \pi x \right)}{x^{1+x}\left( 1-x \right) ^{1-x}}}\mathrm{d}x=\pi 
$$ Addendum 2 For $\alpha=n\in\mathbb{N}$ , $e^{i\pi\left(1-\alpha\right)}$ and $e^{i\pi\left(\alpha-1\right)}$ actually evaluates in the same sign so the expression simplifies to $$I_n=n ^{1-n}\Im \left\{ \pi i\mathrm{Res}\left[ f\left( z \right) ,0 \right]\right\} $$ and this actually evaluates to $$I_n=\frac{\pi}{e}\delta_{n,1}$$ Given by what desmos graphed this seems to be truth Plus, I was noted by the comment that my previous method has a branch cut problem. So I might want to modify contour $C$ a little bit to exclude the branch cut. I took a look at $f(z)$ and it seems that there're branch points in $i2\mathbb{Z}\pi$ . My thought currently is to aim the brach of the ones bigger than $0$ upward and the rest of it downward and make a little ""key hole branch"" from below. Don't know if that works tho.","['integration', 'complex-analysis', 'contour-integration', 'definite-integrals']"
4638720,"There is a fox that hides in an infinite row of bushes (indexed by $\mathbb{N}$), give a strategy to eventually catch the fox.","Recently, one of my professors gave a riddle to my class. He got it while studying abroad some years ago. It goes like this, Before you is an infinite row of bushes (indexed by $\mathbb{N}$ ). A fox is in one of the bushes, but you do not know which. Each time you look at a bush, if the fox is not there, it jumps ahead a fixed but unknown number of bushes. Give a strategy to eventually capture the fox (in a finite, but potentially large number of attempts). Let $a \in \mathbb{N}$ be the initial position of the fox and let $b \in \mathbb{N}$ be the unknown jump size. If $(s_n)$ is our sequence of guesses (i.e. $s_4 = 8$ means our fourth guess is the $8$ th bush) and $(t_n) = a + b \cdot (n - 1)$ is the sequence of the fox's location, then we are looking to find $(s_n)$ so that there exists an $n \in \mathbb{N}$ where $s_n = t_n$ . Here are my thoughts so far: (1) Because the number of bushes the fox jumps is constant and unknown, our strategy must involve guesses that grow farther apart over time. Say we start with the first bush and cycle through every bush after it, then if $b > 1$ or $a > 1$ , we will never catch the fox. (2) Because our strategy will involve guesses that are increasingly farther apart if we pass the fox at any point, we will not catch it. Thus, it also makes sense to start our guessing with 1. (3) I considered $(s_n)$ = the $n$ th prime. If there is some property that relates $(t_n)$ to prime numbers, maybe this could be useful. I do not know much about primes, so I did not get very far with this strategy. (4) I made some progress using Fibonacci numbers. If $(s_n) =$ { $1,1,2,3,5,8,13,\dots$ }, then it would suffice to show that on the $n$ th guess, the fox cannot be in the interval $(s_{n-1},s_n)$ . The first number that does not appear is 4. Thus, the fox would not be caught if $t_5 = 4$ . This is not possible because the only $a,b$ that satisfy this are $a = 1$ and $b = 1$ , but if $a=1$ we catch the fox on the first guess. The next possible set of bushes in which the fox would not be caught is {6,7}. This would be $t_6 = 6$ or $t_6 = 7$ . Once again, the only $a,b$ that satisfy this are $a = 1$ and $b = 1$ . This appears to work for the first couple of cases, but this strategy breaks down pretty soon. Notice, if $t_7 = 10$ , then with $a = 4$ and $b = 1$ , the fox is not caught. Even if this strategy did work, I am not sure how to generalize to an arbitrary guess. (5) I have previously asked a question related to this here in which I asked the question strictly in terms of sequences and recieved an answer. I am not sure how the answer applies to the question of the fox and the bushes or if that answer even works.","['recreational-mathematics', 'puzzle', 'sequences-and-series']"
4638750,What in a rigorous context (no notation abuse) is happening that allows you to solve this ODE problem?,"In know that in a rigorous setting $dy \over dx$ isn't a fraction but is instead defined as the function: $\frac{d}{dx}y$ . Treating it as a fraction is technically considered an abuse of notation that is sometimes done as a shortcut for solving problems or explaining concepts. However, I have been trying to help another person with their intro to differential equations class and am confused on what the solution to the following problem is in a rigorous setting. Here is the problem (and solution): $$ dy - (y-1)^2 \space dx = 0 $$ Such that (given in their notes): $$ p(y) \space dy = g(x) \space dx $$ Then (according to their solution): $$ dy = (y-1)^2 \space dx \rightarrow \frac{1}{(y-1)^2} \space dy = dx$$ Which in turn implies that: $$\int (y-1)^{-2} \space dy = \int dx  = \frac{-1}{y-1} = x + c  $$ $$\rightarrow y = 1 - \frac{1}{x+c}$$ I'
ve heard before that the $dy$ and $dx$ in integrals can be thought of a ""signaling"" which variable to integrate, but here you can see them being abused  as symbolic variables that are manipulated as if they were algebraic symbols. Not only that, but they are being manipulated as if there was an additive inverse $-dx$ for the $dx$ , otherwise the first step of the solution wouldn't be possible. My friend is confused on what the professor is doing, and I can't in good faith right now explain to him why this is allowable, because this solution feels like a heavy abuse of notation, and I don't know what is happening conceptually that allows the solution. What allows you to treat $dx$ and $dy$ as real-valued elements of a fraction in this solution? In a rigorous setting, what is really going on that allows you to solve this problem for $y$ ?","['integration', 'ordinary-differential-equations', 'real-analysis', 'calculus', 'derivatives']"
4638754,What numbers are sums of 2 squares in $\mathbb Z/n\mathbb Z$ (modulo $n$)?,"The negative case of the Fermat Christmas Theorem (i.e. that all numbers, in particular primes, that are $3 \pmod 4$ can't be expressed as the sum of two squares) is very quickly proven by seeing that $0,1$ are the only squares in $\mathbb Z/4\mathbb Z$ , and hence $0,1,2$ are the only possible residues of sums of 2 squares in $\mathbb Z /4\mathbb Z$ . This teaches us one very easy method of ruling out solutions to infinitely many Diophantine equations is by showing that they don't have solutions in some (finite!) quotient ring $\mathbb Z/n\mathbb Z$ . More generally, this is one way that solutions (to Diophantine equations) in $\mathbb Z$ vs. $\mathbb Z/n\mathbb Z$ ""talk to each other"". I am interested in all possible residue classes $r \pmod n$ that this method rules out for the Diophantine equations $x^2 + y^2 = r \pmod n$ . This boils down to wanting to know (what's not in) the image (some subset of $\mathbb Z/n\mathbb Z$ closed under multiplication) of the (multiplicative) map $N_n: \mathbb Z/n\mathbb Z [i] \to \mathbb Z/n\mathbb Z$ where $N$ refers to the norm $N(x+iy):=x ^2 + y^2$ . For $n$ some prime $p$ , a classic pigeonhole argument ( Sum of two squares modulo p ) tells us that because there are many squares, $N_p$ is surjective.
The Chinese remainder theorem tells us that it suffices to look at $n$ some prime power, say $p^n$ (abusing notation a little).  With a little work we can figure out all the squares in $\mathbb Z/p^n\mathbb Z$ : see Efficient way to find squares mod a prime power? , or the formula for the exact number of squares in the quotient ring formula for the number of perfect squares mod $N$ . Unfortunately this count tells us that the pigeonhole argument doesn't work for $n$ composite. Running some experiments https://onlinegdb.com/8raZ0MIzt , I conjecture the following: If $p \equiv 1 \pmod 4$ (I tested $p=5, n=1,2,3,4$ ; and $p=13, n=1,2,3$ ), then $N_{p^n}$ is surjective. If $p \equiv 3 \pmod 4$ (tested with $p=3,7, 11$ , all powers $<1000$ ), then the only numbers $N_{p^n}$ misses are $r$ who are multiples of $p$ , and whose power of $p$ in the factorization of $r$ is odd (i.e. $r$ with $v_p(r) \in \{1,3,5, \ldots\}$ where $v_p(n)$ denotes the highest power of $p$ dividing $n$ ). If $p=2$ , then the only numbers $N_{2^n}$ misses are all the $3 \pmod 4$ numbers, and their multiples by powers of $2$ . E.g. for $\mathbb Z/512\mathbb Z$ , we have all the $3 \pmod 4$ numbers $3=4(1)-1, 7, 11, 15, 19, 23, 27, 31,\ldots, 503, 507, 511=4(2^{9-2})-1$ ; and multiplying by 2: $6, 14, 22, 30, \ldots, 502, 510 = 2\cdot [4(2^{9-3})-1]$ ; and multiplying by 4: $12, 28, \ldots, 508 = 4\cdot [4(2^{9-4})-1]$ ; and so on for a total of $255 = 2^8-1$ bad/impossible numbers. From the Fermat Christmas Theorem, it is easy to show that the ""bad/impossible"" numbers I listed out above are in fact impossible. We have the following ""iff"" statement there exist $x,y\in \mathbb Z$ s.t. $r = x^2 + y^2 \pmod p^n \iff$ there is $k$ and $x,y\in \mathbb Z$ s.t. $p^nk + r = x^2+y^2$ ; or its negation: $r \in \mathbb Z/p^n\mathbb Z$ is not reachable by $N_{p^n} \iff$ for all $k$ , $kp^n+r$ is not a sum of squares (which by the Fermat Christmas Theorem is equivalent to ""every $3 \pmod 4$ primes in the factorization of $kp^n+r$ are raised to an odd power"") Analyzing in the $p=3\pmod 4$ case or $p=2$ case, we see Indeed if $m$ is odd $<n$ ( $<n$ so that $p^m\in \{0,\ldots, p^n-1\}$ ), then $kp^n+p^m = p^m(kp^{n-m}+1)$ , which for any $k$ indeed has highest power of $p$ equal to $m$ (which is odd). If $r=2^m(4l+3)$ , then $k2^n + r = 2^m(k2^{n-m}+(4l+3))$ is (for any $k$ ) a power of 2 times a $3 \pmod 4$ number provided that $m<n$ is not equal to $n-1$ , which we know to be impossible. If $m=n-1$ , then the only possible $r \in \{0,\ldots, 2^n-1\}$ with a factor of $2^m$ is just $2^m$ , which we know to be the sum of squares $2^m= (2^{(m-1)/2})^2 + (2^{(m-1)/2})^2$ if $m$ odd; or $2^m = (2^{m/2})^2+0^2$ if $m$ even. So indeed the conditions I wrote are sufficient to be impossible, but I don't know how to show that they are necessary . Similar questions are: Expressing elements of $\mathbb{Z}/n\mathbb{Z}$ as sums of squares which leads to https://www.jstor.org/stable/2318299?seq=5 , which is is about the uniform problem of what $n$ are s.t. EVERY number in $\mathbb Z / n\mathbb Z$ are the sum of two squares; and https://mathoverflow.net/questions/306392/number-of-ways-to-write-an-integer-as-a-sum-of-squares-modulo-k which is a bit too general (more than 2 squares, and counts number of solutions instead of just saying which are/aren't possible -- I suppose in principle it is possible to get an answer to my question from this resource, but it requires being able to tell what Ramanujan sums are zero/non-zero. I would prefer a more low-tech approach.)","['number-theory', 'square-numbers', 'sums-of-squares', 'elementary-number-theory']"
4638769,The orthogonal standard form of antisymmetric matrix,"Matrix $A=\left( a_{ij} \right) \in M_n\left( \mathbb{R} \right) 
$ ,if $A$ is an antisymmetric matrix,then $a_{ij}=-a_{ji}$ . If $B$ is a real symmetric matrix,then exist $P\in O_n\left( \mathbb{R} \right) $ , $P^TBP=\mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\} 
$ , $\mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\} $ is called the orthogonal standard form of $B$ I want to prove that if $A$ is a antisymmetric matrix, then exist $P\in O_n\left( \mathbb{R} \right) 
$ , $P^TAP=\mathrm{diag}\left\{ \left( \begin{matrix}
	0&		a_1\\
	-a_1&		0\\
\end{matrix} \right) ,...,\left( \begin{matrix}
	0&		a_m\\
	-a_m&		0\\
\end{matrix} \right) ,0,..,.0 \right\} 
$ .the block diagonal matrix called the orthogonal standard form of $A$ . There is my method: I want to prove it by induction, $A$ is an antisymmetric matrix,suppose $A=\left( \begin{matrix}
	A_1&		\alpha _1\\
	-{\alpha _1}^T&		0\\
\end{matrix} \right)$ ,where $A_1\in \,\,M_{n-1}\left( \mathbb{R} \right) 
$ , $A_1$ is an antisymmetric matrix,by induction,there exist $P=\left( \begin{matrix}
	P_1&		\\
	&		1\\
\end{matrix} \right) \in O_n\left( \mathbb{R} \right) 
$ $s.t.$ $P^TAP=\left( \begin{matrix}
	{P_1}^TA_1P_1&		{P_1}^T\alpha _1\\
	-{\alpha _1}^TP_1&		0\\
\end{matrix} \right) 
$ . where ${P_1}^TA_1P_1
$ become the standard form, but I don't know how to deal with ${P_1}^T\alpha _1
$ and $-{\alpha _1}^TP_1
$ . Thank you for sharing your mind.","['matrices', 'linear-algebra', 'orthogonality']"
4638836,Trivial tangent bundle not diffeomorphic to 2n-dimensional open set,"I was trying to find an explicit example of a trivial tangent bundle, i.e. $TM = M\times \mathbb{R}^n$ , with $M$ a smooth manifold without boundary of dimension $n\in\mathbb{N}$ , which is not diffeomorphic to any open subset $U\subseteq\mathbb{R}^{2n}$ . It seems, but maybe I'm wrong, that all the easy classical example of trivial tangent bundle are actually diffeomorphic to some open $2n$ -dimensional set. For instance, take the circle $S^1$ , its tangent bundle is a cylinder $S^1\times\mathbb{R}$ . We can parametrize it as $$
\\\{
(x,y,z)\in\mathbb{R}^3
,
x^2+y^2=1
\},
$$ and by dilating/schrinking it a bit and projecting it on the $z=0$ plane we obtain that it is diffeomorphic to an open annulus: take $$
C
=
\{
(x,y,z),
\;
x^2+y^2
=
\frac{2}{\pi}\arctan z + 2
\}
$$ The projection of $C$ on the plane $z=0$ is one-to-one and its image is the open annulus $A=\{ 1 < x^2+y^2 < 3 \}$ . The 2D-torus doesn't work either, because it's diffeomorphic to $S^1\times S^1$ and adapting the previous argument one obtain again that the tangent bundle is diffeomorphic to an open set of $\mathbb{R}^4$ . I have also considered some examples of Lie groups, for instance $SL(2,\mathbb{R})$ ,
but also this fails since it can be seen as $S^1\times\mathbb{R}^2$ and again one can apply the same argument. Does anyone know a nice example? EDIT: I'd rather appreciate a solution that takes into account the case of $M$ a non-compact manifold (even though I don't know if it changes anything).","['diffeomorphism', 'smooth-manifolds', 'tangent-bundle', 'lie-groups', 'differential-geometry']"
4639011,Finding asymptotic of $ \frac{e^{-n}}{(n-1)!}\int_0^\infty\prod_{k=1}^{n-1}(x+k)e^{-x}dx$,"I'm interested in finding the asymptotic at $n\to\infty$ of $$b_n:= \frac{e^{-n}}{(n-1)!}\int_0^\infty\prod_{k=1}^{n-1}(x+k)\,e^{-x}dx=e^{-n}\int_0^\infty\frac{e^{-x}}{x\,B(n;x)}dx$$ Using a consecutive application of Laplace' method, I managed to get (here) $$b_n\sim(e-1)^{-n}$$ but this approach is not rigorous, and I cannot find even next asymptotic term, let alone a full asymptotic series. So, my questions are: how we can handle beta-function in this (and similar) expressions at $n\to\infty$ whether we can get asymptotic in a rigorous way ?","['integration', 'asymptotics']"
4639027,"Calculating $\mathbb{E}[2\sin (\pi Z)|\cos (\pi Z)]$ when $Z$ is uniform on $[0,2]$","I am trying to calculate the following conditional expectation. Let Z be a uniformly distributed random variable on the closed interval $[0, 2]$ . Define $X = \cos(\pi Z)$ and $Y = 2\sin(\pi Z)$ . Calculate $\mathbb{E}[Y|X] = \mathbb{E}[Y|\sigma(X)]$ . I have tried multiple approaches, but don't know how to proceed since I am only familiar with the standard techniques for calculating the conditional expectation, i.e. when Y is independent of X, Y is measurable with respect to $\sigma(X)$ or if a joint density $f(x,y)$ exists. The first two parts don't apply here and since X and Y aren't independent I don't know how to calculate the joint density. Any help is greatly appreciated. Thank you very much!","['conditional-probability', 'conditional-expectation', 'probability-theory', 'density-function']"
4639117,How to find $\int_0^\infty\frac{x!}{x^x}dx$,"I wonder how I could evaluate $$\lim_{a\rightarrow0}\int_a^\infty\frac{x!}{x^x}dx$$ Where $x!$ is defined for all complex numbers with a positive real part. This is inspired by a limit I found on BlackPenRedPen. I tried using the Residue theorem using the quarter circle contour on the complex plane with a singularity at the origin, but I don't know how to use it. This is the contour I am using. Consider the outer circle as a contour $\Gamma$ . Let the smaller semicircle be $\gamma$ . Let the radius of the big semicircle be $R$ and the radius of the smaller semicircle be $r$ . Then $$\int_0^\infty f(z)dz=\lim_{(r,R)\rightarrow(0,\infty)}\left(\int_\Gamma+\int_\gamma+\int_r^Rf(z)dz+\int_{ri}^{Ri}f(z)dz\right)=2\pi i\sum_k\text{Res}(f, z_k)$$ Where $f(z)=\dfrac{z!}{z^z}$ , $0^0=1$ , and $z_k$ are the poles of $f(z)$ . After that I am stuck as I don't think $f(z)$ has any poles. Any ideas?","['integration', 'calculus', 'definite-integrals', 'residue-calculus']"
4639166,Find the $n$th power of a 3-by-3 circulant matrix,"Consider the matrix given as $$A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix}$$ Write a formula for $A^n$ for $n\in\mathbb{N}$ . $$$$ My attempt: The first that comes to mind is to diagonalize it and hence find the formula for $A^n$ , but that is very messy, so I tried to do something else it goes as: bserve that $$A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix}=a_0\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}+a_1\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix}+a_2\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix}$$ Let $U=\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix}$ then we will have that $$\begin{matrix}U^2=\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix} & \text{ and } & U^3=\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}\end{matrix}$$ Hence we have $A=a_0I+a_1U+a_2U^2 = a_0U^3+a_1U+a_2U^2=(a_0U^2+a_1I+a_2U)U$ This got me thinking that there might be an easy way to solve the above problem, but I could not make any further progress. Please Help and thanks in advance.","['matrices', 'linear-algebra', 'diagonalization', 'circulant-matrices', 'matrix-decomposition']"
4639177,Find the limit of this large expression,Find $$\lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{(n+1)\left(n+\frac{1}{2}\right)\left(n+\frac{1}{2^2}\right)\cdots\left(n+\frac{1}{2^{n-1}}\right)\right\}^n$$ where $()$ and $\{\}$ are nothing except brackets. I wrote the above expression as $$\lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{\prod_{i=0}^{n-1}\left(n+\frac{1}{2^i}\right)\right\}^n$$ I can't apply L $'$ Hopital's rule here. I'm stuck. Any help is greatly appreciated. I honestly don't know what $(a)$ means and imho I guess $[a]$ means greatest integer less than or equal to $a$ .,"['limits', 'calculus']"
4639180,"Finding exponential generating function of sequence $\left\{0,1,2a,3a^2,4a^3,\ldots\right\}$","I need to find exponential generating function for the sequence $\left\{0,1,2a,3a^2,4a^3,\ldots\right\}$ . My attempt: I know that $e^x$ generates $\{1,1,1,1,\ldots\}$ , so $\,e^{ax}$ must generate $\{1,a,a^2,a^3,a^4,\ldots\}$ .
Then I tried taking derivative of $\,e^{ax}$ , which is $\,ae^{ax}$ so that I would have the sequence $\left\{1,2a,3a^2,4a^3,\ldots\right\}$ .
If I multiply the sequence by $\,x\,,\,$ that should give me the sequence $\left\{0,1,2a,3a^2,4a^3,\ldots\right\}$ . Is my approach correct?","['discrete-mathematics', 'generating-functions']"
4639186,"Examples of Fano varieties having $h^i(X, O_X) \neq 0$ for some $i >0$ in positive characteristic","A Fano variety $X$ is a (smooth) projective algebraic variety whose anticanonical bundle $-K_X$ is ample. In characteristic $0$ , by Kodaira vanishing, $h^i(X, O_X) = 0$ for all $i>0$ . In his paper ""Fano varieties in positive characteristic"", Shepherd-Barron proves that it's also true for Fano threefolds in positive characteristic. In ""Regular Del Pezzo surfaces with irregularity"", Maddock constructs a regular (non smooth) Del Pezzo surface over an imperfect field of characteristic 2 having $h^1(X, O_X) \neq 0$ . Are there (other) examples of Fano varieties such that $h^i(X, O_X)  \neq 0$ for some $i>0$ ? Ideally smooth varieties in dimension $d \geq 3$ over a perfect field of characteristic $p \geq 3$ , but I'd be also happy if some of those conditions are not satisfied !","['algebraic-geometry', 'examples-counterexamples', 'positive-characteristic']"
4639255,"A function f is even. Ithere are 4 values $x_1 ,x_2 , x_3, x_4$ satisfying $f(\frac{x+1}{3-x}) = f(x)$, then the value of $x_1 + x_2 + x_3 + x_4$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question A function f is even. It is known that there are four values $x_1 ,x_2 , x_3, x_4$ satisfying $f(\frac{x+1}{3-x}) = f(x)$ , then the value of $x_1 + x_2 + x_3 + x_4$ is A)0 B)2 C)4 D)6 My attempt: I just tried doing $\frac{x+1}{3-x} = \pm \ x$ and I got $ x = 1,\, 2 + \sqrt{5}\,  2 - \sqrt{5}$ . That leaves one more value of x right so does it mean the f is a many-one function if it is then I should get 4 more solutions, what am I missing and what's the final answer?","['algebra-precalculus', 'functions']"
4639262,"Let $A = \{1, 2, 3, 4, 5, 6, 7\}$, and let $N$ be the number of functions $f$ from set $A$ to set $A$ such that $f(f(x))$ is a constant function.","Let $A = \{1, 2, 3, 4, 5, 6, 7\}$ , and let $N$ be the number of functions $f$ from set $A$ to set $A$ such that $f(f(x))$ is a constant function. Find the remainder when $N$ is divided by $1000$ . Help Needed: I am trying to understand this solution. Define the three layers as domain $x$ , codomain $f(x)$ , and codomain $f(f(x))$ . Each one of them is contained in the set $A$ . We know that $f(f(x))$ is a constant function, or in other words, can only take on one value. So, we can start off by choosing that value $c$ in $7$ ways. So now, we choose the values that can be $f(x)$ for all those values should satisfy $f(f(x))=c$ . Let's $S$ be that set of values. First things first, we must have $5$ to be part of $S$ , for the $S$ is part of the domain of $x$ . Since the values in $i\in S$ all satisfy $f(i) = 5$ , we have $5$ to be a value that $f(x)$ can be. Now, for the elements other than $5$ : If we have $k$ elements other than $5$ that can be part of $S$ , we will have $\binom{6}{k}$ ways to choose those values. There will also be $k$ ways for each of the elements in $A$ other than $5$ and those in set $S$ (for when function $f$ is applied on those values, we already know it would be $5$ ). There are $6-k$ elements in $A$ other than $5$ and those in set $S$ . So, there should be $6^{6-k}$ ways to match the domain $x$ to the values of $f(x)$ . So, summing up all possible values of $k$ ( $[1,6]$ ), we have $\sum_{k=1}^6 k^{6-k} = 6\cdot 1 + 15\cdot 16 + 20\cdot 27 + 15\cdot 16 + 6\cdot 5 + 1) = 1057$ Multiplying that by the original $7$ for the choice of $c$ , we have $7 \cdot 1057 = 7\boxed{399}.$ I've given two days but still not able to  understand. I tried using smaller set. Any other way to solve this problem are also welcomed. This question has been asked here Number of functions $f$ on $\{1,\cdots,7\}$ s.t. $f(f(x))$ is constant too. New Update (15-05-2023) Let a small set $A=\{1,2,3,4\}$ Let constant Value is $4,$ or $3,$ or $2,$ or $1$ i.e. $f(f(x))=4,\text{or} \;3, \text{or} \;2,\text{or}\; 1 \;\forall x\in A$ and middle layer contain only one element. For example $f(f(1))=f(4)=4, f(f(2))=f(4)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ $f(f(1))=f(3)=3, f(f(2))=f(3)=3, f(f(3))=f(3)=3, f(f(4))=f(3)=3$ $f(f(1))=f(2)=2, f(f(2))=f(2)=2, f(f(3))=f(2)=2, f(f(4))=f(2)=2$ $f(f(1))=f(1)=1, f(f(2))=f(1)=1, f(f(3))=f(1)=1, f(f(4))=f(1)=1$ There are four such function obtained above. Now Let Constant value of function $f(f(x)) $ is $4$ and middle layer contains two elements. let two elements in middle layer to be ${3,4}$ $f(f(1))=f(4)=4, f(f(2))=f(3)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ $f(f(1))=f(3)=4, f(f(2))=f(4)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ $f(f(1))=f(3)=4, f(f(2))=f(3)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ Now Constant value is $4$ and middle layer contains two elements let them to be ${2,4}$ $f(f(1))=f(2)=4, f(f(2))=f(4)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ $f(f(1))=f(4)=4, f(f(2))=f(4)=4, f(f(3))=f(2)=4, f(f(4))=f(4)=4$ $f(f(1))=f(2)=4, f(f(2))=f(4)=4, f(f(3))=f(2)=4, f(f(4))=f(4)=4$ Now Constant value is $4$ and middle layer contains two elements let them to be ${1,4}$ $f(f(1))=f(4)=4, f(f(2))=f(1)=4, f(f(3))=f(1)=4, f(f(4))=f(4)=4$ $f(f(1))=f(4)=4, f(f(2))=f(4)=4, f(f(3))=f(1)=4, f(f(4))=f(4)=4$ $f(f(1))=f(4)=4, f(f(2))=f(4)=4, f(f(3))=f(4)=4, f(f(4))=f(4)=4$ Similarly if Constant element is $1,$ or $2,$ or $(3)$ total of such function are Thirty-Six Now Overall function will $40$","['permutations', 'functions', 'combinatorics', 'graphing-functions']"
4639264,2023 MIT Integration Bee Regular Season Problem 6,"How do we solve $$\int\left(\frac{x^6+x^4-x^2-1}{x^4}\right)e^{x+\frac{1}{x}}dx$$ ? This is an integral in the 2023 MIT integration bee . I thought that maybe this is a consequence of the chain rule, but that is not the case. Then I thought there is a product rule hiding in the integrand, but it doesn't seem like it.","['integration', 'indefinite-integrals', 'calculus', 'contest-math']"
4639297,Local Truncation Error of Adam Bashforth 3-step,"I want to get the local truncation error of this Adam Bashforth 3-step method $$y_{n+1} = y_n + \frac{h}{12} (23 f_{n+2} − 16 f_{n+1} + 5 f_n)$$ I started by remembering the following Taylor development $$ y(x_{n+1}) = y(x_n) +hy'(x_n) +\frac{h^2}{2}y''(x_n)+\frac{h^3}{6}y'''(x_n)+\frac{h^4}{24 }y^{(4)}(x_n)$$ After, I do the subtraction of these two expressions and I get the following $$y(x_{n+1})- y_{n+1} = y(x_n)-y_n + hy'(x_n) - \frac{h}{12} (23 f_{n+2}) +\frac{h^2}{2}y''(x_n)+ 16\frac{h}{12} (f_{n+1}) +\frac{h^3}{6}y'''(x_n)- 5\frac{h}{12} (f_n) +\frac{h^4}{24 }y^{(4)}(x_n)$$ But from here I don't know how to continue since I try to simplify some terms to finally arrive at $$LTE=\frac{h^4}
{8} y^{(4)}(\eta).$$ I'm struggling to catch the error in the calculation but I'm finally stuck","['numerical-methods', 'ordinary-differential-equations']"
4639301,Derivation for expression for second covariant derivative,"This is a direct follow up to my earlier question: Reconciling different expressions for Riemann curvature tensor . I think it deserves a separate question, plus I don't want to disturb the answerer of the earlier post by repeated queries in comments. The answer in that highlights this identity: $$\nabla_{\partial_a}(\nabla_{\partial_b}Z)=\nabla_{\nabla_{\partial_a}\partial_b}Z+\nabla^2_{\partial_a,\partial_b}Z\tag{1}$$ The derivation of the above identity is through a calculation of $\nabla_X(\nabla_Y(Z))$ . I will avoid using any notational shortcuts that people get used to after experience. Here is my version of the calculation (the only version of chain rule for covariant derivatives I know is $\nabla_X(fY)=X(f)Y+f\nabla_X(Y)$ , which is what I'm using): $$\nabla_X(\nabla_Y(Z))=X^a\nabla_{\partial_a}(Y^b\nabla_{\partial_b}(Z))=X^a(\partial_aY^b)(\nabla_{\partial_b}(Z))+X^aY^b\nabla_{\partial_a}(\nabla_{\partial_b}(Z))$$ $$=X(Y^b)(\nabla_{\partial_b}(Z))+X^aY^b\nabla_{\partial_a}(\nabla_{\partial_b}(Z))$$ $$=\nabla_X(Y^b)(\nabla_{\partial_b}(Z))+X^aY^b\nabla_{\partial_a}(\nabla_{\partial_b}(Z))\tag{2}$$ Now on the RHS, I have a problem with both terms! First term : It seems that the 1st RHS term is supposed to equal $\nabla_{\nabla_X(Y)}(Z)$ . But then $$\nabla_{\nabla_X(Y)}(Z)=\nabla_{(\nabla_X(Y))^b\partial_b}(Z)=(\nabla_X(Y))^b\nabla_{\partial_b}(Z)\tag{3}$$ Let me evaluate $(\nabla_X(Y))^b$ : $$(\nabla_X(Y))^b=(\nabla_X(Y^c\partial_c))^b=\big(X(Y^c)\partial_c+Y^c\nabla_X(\partial_c)\big)^b=X(Y^b)+(Y^c\nabla_X(\partial_c))^b$$ $$=\nabla_X(Y^b)+Y^c(X^d\nabla_{\partial_d}(\partial_c))^b=\nabla_X(Y^b)+Y^c(X^d\Gamma^e_{dc}\partial_e)^b$$ $$=\nabla_X(Y^b)+Y^cX^d\Gamma^b_{dc}\tag{4}$$ This means $$\nabla_{\nabla_X(Y)}(Z)=(\nabla_X(Y))^b\nabla_{\partial_b}(Z)\neq \nabla_X(Y^b)(\nabla_{\partial_b}(Z))\tag{5}$$ How is this inconsistency resolved? Second term : The 2nd RHS term is supposed to be $\nabla_{X,Y}^2(Z)$ . This means that $$\big(\nabla_{X,Y}^2(Z)\big)^c=\big(\ \ X^aY^b\nabla_{\partial_a}(\nabla_{\partial_b}(Z))\ \ \big)^c=X^aY^b\big(\nabla_{\partial_a}(\nabla_{\partial_b}(Z))(\text{d}x^c)\big)$$ where the last term involves the $(1,0)$ -tensor $\nabla_{\partial_a}(\nabla_{\partial_b}(Z))$ acting on $\text{d}x^c$ . But on wikipedia , the following equation is given: $(\nabla^2_{u,v}w)^a=u^cv^b\nabla_c\nabla_bw^a$ (I don't know if this equation is a result , or a definition , or what), which can be re-written as $$\big(\nabla_{X,Y}^2(Z)\big)^c=X^aY^b\nabla_a\nabla_bZ^c=X^aY^b\big(\nabla\nabla Z(\text{d}x^c,\partial_a,\partial_b)\big)$$ It boils down to showing that $\nabla\nabla Z(\text{d}x^c,\partial_a,\partial_b)=\nabla_{\partial_a}(\nabla_{\partial_b}(Z))(\text{d}x^c)$ . Now by definition of total covariant derivative, $$\nabla(\mathcal{A})(\omega^1,\ldots,\omega^r,X_1,\ldots,X_s,X)=\nabla_X(\mathcal{A})(\omega^1,\ldots,\omega^r,X_1,\ldots,X_s)$$ So $$\nabla_{\partial_a}(\nabla_{\partial_b}(Z))(\text{d}x^c)=\nabla(\nabla_{\partial_b}(Z))(\text{d}x^c,\partial_a)\tag{6}$$ and $$\nabla\nabla Z(\text{d}x^c,\partial_a,\partial_b)\equiv\nabla(\nabla(Z))(\text{d}x^c,\partial_a,\partial_b)=\nabla_{\partial_b}(\nabla(Z))(\text{d}x^c,\partial_a)\tag{7}$$ How do I even equate these two (RHS of eq.s 6 and 7) ? Is there some property of total covariant derivative that I'm missing out on?","['connections', 'differential-geometry']"
4639330,Proof that Corresponding Angle Postulate $\iff$ Playfair's Postulate,"I will state both postulates here: Corresponding Angle Postulate: If two lines are cut by a transversal, then the lines are parallel if and only if the corresponding angles are congruent. And Playfair's Postulate: Given a line and a point not on that line, there is exactly one line passing through the point that is parallel to the given line. I have looked online for a proof that they are equivalent but I am struggling to find one. We would have to prove and if an only if statement. Corr $\angle$ implies Playfair and Playfair implies Corr $\angle$ . Any guidance on what a proof of these two would look like?","['euclidean-geometry', 'geometry']"
4639381,Set theoretic issues in the definition of k-space or final topology wrt a proper class of functions,"In Ronald Brown's Topology and Groupoids ( pdf here ), section 5.9, p. 182, a topological space $X$ is called a k-space if the topology on $X$ coincides with the final topology with respect to all continuous maps $f: C\to X$ for all compact Hausdorff spaces $C$ . The collection of all such spaces $C$ and maps $f$ is not a set, but a proper class.  So it may seem difficult at first to define k-spaces in this way.  But then Proposition 5.9.1 shows that one can in fact choose a set of such spaces $C$ and functions $f$ to achieve the same thing.  I had some issues with the proof, so I'll ask about it in the slightly more general setting of final topologies with respect to a proper class of functions. Suppose $X$ is a set and we have a (well-defined in ZFC) proper class $\mathcal{F}$ of functions $f:Y\to X$ where each $Y$ is a topological space.  (ZFC does not formally have the notion of class, but a well defined class just corresponds to some first order formula in ZFC.  That is the case for all compact Hausdorff spaces for example.)
We want to define the final topology $\tau_{\mathcal F}$ on $X$ with respect to $\mathcal{F}$ .  It should be the finest topology that makes all these maps continuous.  Equivalently, a set $U$ in $X$ will be open iff $f^{-1}(U)$ is open in $Y$ for all $f:Y\to X$ in $\mathcal{F}$ . Result: There is a set $\mathcal{G}$ of elements of $\mathcal{F}$ such that the final topology on $X$ with respect to $\mathcal{G}$ is the same as $\tau_{\mathcal F}$ . The proof (paraphrased from Brown's 5.9.1) goes like this.  For each set $U\subseteq X$ that is not open in $\tau_{\mathcal F}$ choose some $f_U:Y_U\to X$ in $\mathcal{F}$ such that $f_U^{-1}(U)$ is not open in $Y_U$ .  There is a set of such $U$ and the collection $\mathcal{G}$ of the $f_U$ is also a set and has the required property. But does this really work in ZFC?  AC only works when starting with a set of sets, and there is no such thing as the axiom of global choice in ZFC.  I thought we could salvage the argument of choosing the $Y_U$ and $f_U$ like this.  We only care about spaces up to homeomorphism, and each set can be well ordered.  So we can assume each $Y_U$ will be an ordinal.  And by well ordering of ordinals, we can assume $Y_U$ will be the least ordinal that works for $U$ .  All this is expressible by a first order formula in ZFC.  So now we have a set of ordinals and a set of all possible topologies for each of these ordinals and a set of continuous functions between these topological spaces and $X$ .  So finally we can apply regular AC. Does that work, or is there an alternative approach?","['general-topology', 'set-theory']"
4639550,The Ñ function. Where is this chaotic tweak of the geometric series continuous?,"Let us consider the function $\nu:(-1,1)\to[-1,1]$ defined as follows $$\nu(x):=\begin{cases}0 & \text{ if } x=0\\ \frac{1}{\left\lfloor \frac{1}{x}+\frac{1}{2}\right\rfloor} & \text{ if } x\neq0\end{cases}$$ noticing that, since $\lfloor t+\frac{1}{2}\rfloor$ is the rounding function, it is the case that $\nu(x)\approx \frac{1}{x^{-1}}=x=\text{Id}(x)$ . So we might see this function as a tweak function that takes every number of $(-1,1)$ to its closest one in $S:=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\}$ . But this function alone is not that much of a deal. For instance, its continuity is quite easy to study. It is continuous everywhere except at the points $(-1,1)\cap\{\frac{2}{n}: n\text{ odd}\}$ since the rounding function jumps precisely when $t=n+\frac{1}{2}$ . Now, I wanted to examine what would happen if I tweaked known well behaved series with this function. For this reason, I considered the series $\tilde N:(-1,1)\to\mathbb{R}$ defined as $\tilde N(x)=\sum_{n=0}^\infty \nu(x^n)$ which looks like this . $ \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \ $ My main questions are on how to study its continuity. More precisely: Is this function continuous at $S=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\}$ ? Is this function discontinuous at $(-1,1)\cap\{\frac{2}{n}:n \text{ odd}\}$ ? Precisely when is it continuous/discontinuous? What is happening where the domain is negative? (Reminds me of the logistic map ) Though the $3$ rd one is the one that I really care for. Now I will prove the things that I know although they aren't that much. $\color{green}{\bf{Proposition\ 1.}}$ $\tilde N(x)$ converges absolutely on $(-1,1)$ . Hence, it is well defined. Proof. Notice that it can be easily checked that $|\nu(x)|\leq\frac{3}{2}|x|$ . Because of this, we know that $\sum_{n=0}^\infty |\nu(x^n)|\leq \frac{3}{2}\frac{1}{1-|x|}$ . $\quad\Box$ $\color{green}{\bf{Proposition\ 2.}}$ If we consider $N(x)=\frac{1}{1-x}$ , then $\forall x\in S:\tilde N(x)=N(x)$ . Proof. If $x=0$ then $\tilde N(0)=1=N(0)$ . Now, if $x=\frac{1}{m}$ for some $m\in\mathbb{Z}^*$ then the series becomes $$\tilde N\left(\frac{1}{m}\right)=\sum_{n=0}^\infty\frac{1}{\lfloor m^n+1/2\rfloor}=\sum_{n=0}^\infty\frac{1}{m^n}=N\left(\frac{1}{m}\right)\quad\Box$$ Update: It is quite easy to show that $\tilde N$ is continuous at $0$ by bounding the $\nu$ function. $\color{seagreen}{\bf{Lemma\ 1.}}$ If $x\in(-1,1)$ is such that $|x|<\frac{2}{k}$ for some odd $k\geq1$ then $\frac{k}{k+1}|x|\leq|\nu(x)|\leq\frac{k+2}{k+1}|x|$ . Proof. This can be easily checked by making use of known inequalities of the floor function. $$|x|<\frac{2}{k}\Rightarrow \color{blue}{\frac{k}{k+1}|x|}\leq\color{purple}{|\nu(x)|} \leq \color{red}{\frac{k+2}{k+1}|x|}$$ $\color{green}{\bf{Proposition\ 3.}}$ The $\tilde N(x)$ function is continuous at $x=0$ . Proof. There are several ways of formalizing it but the idea is to make use of the above lemma since it is saying that $\nu(x)\sim x$ as $x\to0$ . We will examine $\lim_{x\to0^+}\tilde N(x)$ and $\lim_{x\to0^-}\tilde N(x)$ to see that they are both equal to $1=\tilde N(0)$ and, consequently, to see that $\tilde N$ is indeed continuous at $0$ . Suppose that $0<x<\frac{2}{k}$ for odd $k\geq3$ . Then we know that $\frac{k}{k+1}x\leq\nu(x)\leq\frac{k+2}{k+1}x$ which implies that, since $\frac{k}{k+1}\leq\nu(1)\leq\frac{k+2}{k+1}$ is trivially true (as $\nu(1)=1$ ) and $\forall n\geq1:x^n\leq x<2/k$ (as $0<x<1$ ), the following inequalities hold $$\underline{\frac{k}{k+1}}\leq\frac{k}{k+1}\frac{1}{1-x}\leq\underline{\tilde N(x)}\leq\frac{k+2}{k+1}\frac{1}{1-x}\leq\underline{\frac{k+2}{k+1}\frac{1}{1-2/k}}$$ which implies $\lim_{x\to0^+}\tilde N(x)=1$ by the squeeze theorem since both bounds tend to $1$ as $k\to+\infty$ . The limit from the left, however, is somewhat trickier. But not that much, to be fair. The idea is to notice that $\text{sign}(\nu(x))=\text{sign}(x)$ . Thus, if $x<0$ , we have that $\nu(x^n)=(-1)^n|\nu(x^n)|$ . With this in mind, let's study the other limit. Suppose now that $-\frac{2}{k}<x<0$ for odd $k\geq3$ . Let's try to bound $\tilde N(x)$ with bounds that only depend on $k$ that tend to $1$ as $k\to+\infty$ . As an upper bound, we have \begin{align*}
\tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\leq\sum_{n=0}^\infty|\nu(x^{2n})|\leq\frac{k+2}{k+1}\frac{1}{1-x^2}\leq\frac{k+2}{k+1}\frac{1}{1-4/k^2}
\end{align*} and, as a lower bound, we have \begin{align*}
\tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\geq\frac{k}{k+1}\frac{1}{1-x^2}-\frac{k+2}{k+1}\frac{|x|}{1-x^2}\\
&=\frac{k-(k+2)|x|}{(k+1)(1-x^2)}\geq\frac{k-(k+2)\frac{2}{k}}{(k+1)(1-x^2)}\geq\frac{k-2-\frac{4}{k}}{k+1}\end{align*} which, finally, implies that $\lim_{x\to0^-}\tilde N(x)=1$ as well. $\quad\Box$","['continuity', 'sequences-and-series', 'real-analysis']"
4639602,Finding logic for a different form of an answer.,"$20$ persons are sitting on a clean round table. Find the number of ways of selecting $4$ persons. Such that no two of them are consecutive. My approach I selected one out of $20$ , and the remaining $3$ should be selected out of $17$ persons (removing the selected one and his adjacents). With at least gap of one person between them. So this is now equivalent to selecting $3$ person in a linear arrangement not adjacent to each other from $17$ persons for which the ways is $^{15}\text{C}_3$ . Multiplying both selection gives, $^{20}\text{C}_1\cdot^{15}\text{C}_3$ But this has repetition four times for each of the first selected person. So, total cases is $\frac{^{20}\text{C}_1\cdot^{15}\text{C}_3}{4}$ But the text book in which the question was gives answer as $\frac{^{20}\text{C}_1\cdot^{15}\text{C}_3}{4}$ and $^{17}\text{C}_4- ^{15}\text{C}_2$ And has a note that both answers have separate logic. Obviously these are inter convertible and has same value, but I am unable to find a different logic for this one.","['permutations', 'combinatorics', 'contest-math']"
4639626,Numbers of subsets with restrictions,"We have a class of 30 students. In how many ways can we choose a group of 5 students (order does not matter)? In how many ways can we split the class in 7 unordered groups? We want to split the class into five teams (a, b, c, d, e). every team has the same number of students? First question I solved this with combination formula $\binom{30}{5}$ The second I think this can be solved with stirlink 2nd kind number $S(30,7)$ , am I right?? And the last one, it's hard for me to say that this is the correct method. I chose 6 students first in $\binom{30}{6}$ ways, then we have 24 students left. I chose again 6 students in $\binom{24}{6}$ ways and so on. So my final answer would be $\binom{30}{6}\ast \binom{24}{6}\ast \binom{18}{6}\ast\binom{12}{6}\ast \binom{6}{6}$ . Is that a good solve?","['combinatorics', 'discrete-mathematics']"
4639673,Applicability of Glivenko-Cantelli theorem to functions other than cdf,"Glivenko-Cantelli theorem states that approximation function $F_n(t):=\frac{1}{n} \sum_{i=1}^{n} [X_i < t]$ uniformly converges to actual cumulative distribution function $F$ . That is, $\mathrm{sup}_{t} ||F_n(t) - F(t)|| \to 0$ , a.s. Reading through the proof of the theorem, I found that the proof uses only the three properties of $F_n, F$ pair: Property 1: $\forall t, F_n(t) \to F(t)$ , a.s. Property 2: $F$ is bounded function Property 3: $F$ and $F_n$ is monotonically increasing function Thus, I think any function-approx-function-pair $(f_n, f)$ satisfies the all three properties, then Glivenko-Cantelli theorem can also be applied to that pair, and conclude that $\mathrm{sup}_t||f_n(t) - f(t)|| \to 0$ , a.s. Is this correct? Or, is there other important properties that $(f, f_n)$ must have? Proof of Glivenko-Cantelli For your information, here is that proof of Glivenko-Cantelli fetched from Thm. 19.1 of [1] *By the strong law of large numbers, both $F_{n} \to F$ , a.s., and $F_n(t-) \to F(t-)$ , a.s for given $t$ . ( Property 1 used ).
Given a fixed $\epsilon > 0$ , there exists a partition $-\infty = t_0 < t_1 < \cdots < t_k = \infty$ such that $F(t_i-)-F(t_{i-1}) < \epsilon$ for every $i$ . ( Property 2 and 3 used )
Now for $t_{i-1}\leq t < t_i$ , $ F_n(t) - F(t) \leq F_n(t_i -) - F(t_i -) + \epsilon$ ( Property 3 used ) $ F_n(t) - F(t) \geq F_n(t_{i-1}) - F(t_{i-1}) - \epsilon$ ( Property 3 used ) The onvergene of $F_n(t)$ and $F_n(t-)$ for every fixed $t$ is certainly uniform for $t$ in the finite set $\{t_1,\ldots, t_n\}$ . Conclude that $\mathrm{limsup} (\mathrm{sup}_{t}||F_n(t) - F(t)|) \leq \epsilon$ almost surely. This is true for every $\epsilon$ and hence the limit superior is zero.* [1] Van der Vaart, Aad W. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.","['statistics', 'probability-limit-theorems', 'probability-theory']"
4639681,Convergence of the Expectation-Maximization algorithm,"Studying the Expectation-Maximization algorithm, I noticed that I couldn't find any proof that the parameters actually converge, nor that the limit is a local extremum of the likelihood (or even just a point where the gradient vanishes). The proofs that I found use the argument that at each step the likelihood increases, and thus as it is a monotonically increasing bounded sequence it must converge. But my 2 questions remain unanswered: Why does the convergence of the values of the likelihood function imply the convergence of the parameters? Assuming the parameters converge, why do they converge to a local extremum of the likelihood function? I would love to see a proof, or even a good reference will do. Thanks","['machine-learning', 'expectation-maximization', 'statistics']"
4639686,Why is a smooth scheme over a field reduced?,"I think I have some fundamental misunderstanding of smoothness of scheme now. It is mentioned in many places that ""a scheme smooth over a field is reduced"" (which, of course, also enjoys other nice properties.). But now consider the situation that $X=\text{Spec }k[t]/(t^n) $ and $Y=\text{Spec }k$ , where $k$ is a field. (Let's say it has characteristic 0.) Let $f:X\to Y$ be the natural map. Then, following the definition in Hartshorne's book, $f$ is flat. $\dim X=\dim Y+0$ . $\Omega_{X/Y}=0$ Thus, $f$ is smooth of relative dimension 0. But I think it should not be smooth. What is my misunderstanding?","['algebraic-geometry', 'commutative-algebra']"
4639705,"Does it always happen $\|ax+by\|=1 \implies |a+b|\leq 1?$ for $\|x\|=\|y\|=1$ (where $a, b\in \mathbb{R}$)","I am new in functional analysis. I was reading a theory related to unit ball in Banach space. While studying the norm attainment set of points I met with a confusion: For a finite dimensional real Banach space $\mathbb{X}$ and for $\|x\|=\|y\|=1$ , do always $\|ax+by\|=1 \implies |a+b|\leq 1? $ (where $a, b\in \mathbb{R}$ ) I think the answer is yes. Also I have a doubt on whether the statement is true in any normed linear space. I have tried with my limited knowledge to prove it by contradiction i.e., by considering $|a+b|>1$ trying to find two distinct elements $x, y$ with $\|x\|=\|y\|=1$ such that $\|ax+by\|\ne 1$ but I could not able to proceed. Please help me. Thank you in advance. Thank you all for the quick suggestions but all the examples are in Hilbert space. What will happen if $\mathbb{X}$ is not a Hilbert space? Is the statement still wrong?","['banach-spaces', 'normed-spaces', 'linear-algebra', 'functional-analysis']"
4639727,Find the roots of the equation $(1+\tan^2x)\sin x-\tan^2x+1=0$ which satisfy the inequality $\tan x<0$,"Find the roots of the equation $$(1+\tan^2x)\sin x-\tan^2x+1=0$$ which satisfy the inequality $$\tan x<0$$ Shold I solve the equation first and then try to find which of the roots satisfy the inequality? Should I use $\tan x$ in the solution itself and obtain only the needed roots? I wasn't able to see how the inequality can be used beforehand, so I solved the equation. For $x\ne \dfrac{\pi}{2}(2k+1),k\in\mathbb{Z}$ , the equation is $$\dfrac{\sin^2x+\cos^2x}{\cos^2x}\sin x-\left(\dfrac{\sin^2x}{\cos^2x}-\dfrac{\cos^2x}{\cos^2x}\right)=0$$ which is equivalent to $$\sin x+\cos2x=0\\\sin x+1-2\sin^2x=0\\2\sin^2x-\sin x-1=0$$ which gives for the sine $-\dfrac12$ or $1$ . Then the solutions are $$\begin{cases}x=-\dfrac{\pi}{6}+2k\pi\\x=\dfrac{7\pi}{6}+2k\pi\end{cases}\cup x=\dfrac{\pi}{2}+2k\pi$$ How do I use $\tan x<0$ ?",['trigonometry']
4639785,Let $T \subseteq \mathbb{R}$ and $f: T \to \mathbb{R}$ be a continous function. Does $f$ have an antiderivative?,"The Fundamental Theorem of (differential and integral) Calculus states, that for any continuous function $f: (a, b) \to \mathbb{R}$ the Riemann integral $$F(\beta) = \int_{\alpha}^{\beta} f(t) dt \quad \alpha, \beta \in (a,b)$$ is an antiderivative of $f$ . My question is: what happens when the domain of the continuous function $f$ is not an interval? It is known that any open set $O \subseteq \mathbb{R}$ is the union of countable disjoint open interval. Using this fact, I think one can show, that any continuous function $f: O \to \mathbb{R}$ has an antiderivative. But what about a closed domain?","['integration', 'general-topology', 'derivatives', 'real-analysis']"
4639793,Find a basis for set of matrices,"Let $M_{2n}$ denote the set of all square matrices of size $(2n)\times (2n)$ . I managed to show that the following set is a subvector space of $M_{2n}(K)$ : $$
N_{2n}:=\left\{A\in M_{2n}(K): QA+A^\top Q=0\right\}
$$ with $Q\in M_{2n}(K)$ being the block matrix given by $$
Q:=\begin{pmatrix}0 & E_n\\ -E_n & 0\end{pmatrix}
$$ where $E_n$ is the $n$ -dimensional identity matrix. I am asked to find a basis for $N_{2n}$ and I do not really know how to do that. I started with the special case $n=1$ . In this case, if I let $A=\begin{pmatrix}a & b\\ c& d\end{pmatrix}$ be in $N_2$ , then I get the condition $$
a = -d
$$ while $c$ and $b$ have no restrictions. So I think that a basis is given by $$
\left\{\begin{pmatrix}1 & 0\\ 0 & -1\end{pmatrix},\begin{pmatrix}0 & 1\\0 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\1 & 0\end{pmatrix}\right\}
$$ In the general case, if I write $A\in N_{2n}$ in block form $A=\begin{pmatrix}B & C\\D & F\end{pmatrix}$ , I get the condition $$
B=-F
$$ but I don't know if this is helpful to find a basis...","['matrices', 'linear-algebra']"
4639810,Calculating the derivative of the flow of a time-dependent family of vector fields,"Suppose on a manifold $M$ I have a family of complete vector fields, $V(t) \in \Gamma(TM)$ for each $t \in \mathbb R$ with $V(0) = 0$ . Also assume that $[V(t_0), V(t_1)]=0$ for any $t_0, t_1 \in \mathbb R$ . Let $s \rightarrow \phi_s^X(p)$ be the integral curve of the vector field $X$ with initial condition $p \in M$ , parametrized by $s$ . My question is, is there any way to compute $$ \frac{d}{dt} \phi_s^{V(t)}(p) $$ for given fixed $s \neq 0$ and $p \in M$ ? For example, I am trying to compute $$ \frac{d}{dt}\rvert_{t=0} \phi_1^{V(t)}(p) $$ for such a family. I tried looking at the theorem for flows of time-dependent vector fields to obtain some chance of computation. There, an integral curve of $V$ with initial condition $(t_0, p) \in \mathbb R \times M$ is a curve $\psi^{(t_0,p)}: \mathbb R \rightarrow M$ such that $$ \frac{d}{dt} \psi^{(t_0,p)}(t) = V(t)_{\psi^{(t_0,p)}(t)} $$ According to Theorem 9.48 of Lee's ""Introduction to smooth manifolds"", such integral curves always exist and have good composition properties. However, I failed to see any relationship between these integral curves and the flows $\phi_t^{V(t)}(p)$ . Initially I had thought it reasonable that $$ \psi^{(t_0,p)}(t) = \phi_{t-t_0}^{V(t)}(p) $$ but I don't actually know how to prove that this is the case, and I feel like what I'm asking is equivalent to proving that this is the case; both computations seem to reduce to the question of how to differentiate the flow respect to $t$ when $V(t)$ changes.","['vector-fields', 'differential-geometry']"
4639815,"Classification of topologically constrained foliations on $X$ in low dimensions i.e. $2,3$","Take $X=(0,1)^n.$ Fix points $p,q$ s.t. $\text{dist}_n(p,q)=\sqrt{n}.$ Problem: Classify analytic regular foliations of $X$ with $(n-1)-$ dim. leaves which are topologically $(0,\sqrt{n})\times S^{n-2} $ accumulating to $p,q.$ The problem is trivial until dim. $2,$ and I have found one (real analytic foliation) solution in $n=2$ dim. but have not completely classified for $n=2.$ I conjecture that there are no solutions for $n\gt2$ and I would be shocked if there was a solution. My solution for $n=2$ : Foliation $F_s=\big\{\log x \log y=s: s\in (0,\infty) \big\}$ which satisfies $\cup_s F_s=X,$ is clearly analytic and regular, and satisfies the topological constraint as well as the accumulation criterion. In this case, note that $p=(0,1)$ and $q=(1,0).$ I feel like there should be a more elegant approach here. My solution does not let one see the global classification picture and is only an isolated example that happens to work. Perhaps the problem is easier in the complex setting. Edit 3/19/2023: I will relax the conditions. Let $n=3.$ Take $X=(0,1)^n.$ Fix points $p,q$ s.t. $\text{dist}_n(p,q)=\sqrt{n}.$ Construct a smooth regular foliation of $X$ with $(n-1)-$ dim. leaves which are topologically $(0,\sqrt{n})\times S^{n-2} $ accumulating to $p,q.$ I think this is a much more tractable problem. I am only looking for a construction in dimension $3$ now, where I've replaced ""analytic"" with ""smooth."" I am including the following related question I posted $10$ months ago (link below), because it is an attempt at a solution (in the smooth case) for any dimension $n.$ This method leverages distribution theory and imagines the leaves as arising from integrating some continuous bivariate probability distribution. In this way a single leaf is precisely the distribution when all the parameters are fixed. What's interesting about this method is that you get the topological condition for sufficient constraints on the probability distribution. Since the definition in the linked post below includes $p,q$ in the leafs, then deleting $p,q$ would match up with my definition in this post. Understanding a subspace of $S^n$ for a given distribution .","['riemannian-geometry', 'geometric-topology', 'manifolds', 'differential-topology', 'differential-geometry']"
4639882,How to simplify this trigonometric equation,"I am not sure how to further simplify this expression: $$\sec^2(\arcsin(y/r)) \times \frac{\frac{1}{r}}{\sqrt{1-(\frac{y}{r})^2}} \times \frac{r}{1000}$$ How should I further simplify the $\sec^2(\arcsin(y/r))$ ?
The simplified version looks like this: $$\left(\frac r{10\sqrt{r^2-y^2}}\right)^3$$","['algebra-precalculus', 'trigonometry']"
4639928,Is $(\Bbb Q \times \Bbb I)\cup(\Bbb I \times \Bbb Q)$ connected?,"I've been stuck with this question for a couple days so I was hoping to get some answers here. My question is about subsets in $\Bbb R^2$ with the usual topology. I know that $\Bbb R^2 \setminus \Bbb Q^2$ is connected because it is path connected, which is easy enough to see. Unless I'm mistaken, $\Bbb R^2 \setminus \Bbb I^2$ (where $\Bbb I = \Bbb R \setminus \Bbb Q$ is the set of irrational numbers) is also path connected. My question is if $(\Bbb Q \times \Bbb I)\cup(\Bbb I \times \Bbb Q) = (\Bbb R^2 \setminus \Bbb Q^2) \cap (\Bbb R^2 \setminus \Bbb I^2)$ is also connected. If it is path connected, it is tougher to see than in the case of the two previously mentioned sets, but I'm only looking to see if it is connected, not path connected, I mention path connectedness because in the previous sets that was a quick way to show they are indeed connected. I've been trying to find clopen sets but it's also proving difficult, be it because there aren't any or because I can't find them. I would greatly appreciate some pointers, thank you!","['general-topology', 'connectedness']"
4639929,Does convergence in probability imply convergence in expected value? [duplicate],"This question already has answers here : Does convergence in distribution implies convergence of expectation? (2 answers) Closed last year . Suppose that $Y_N \overset{p}{\to} Y$ as $N \rightarrow \infty$ , i.e., $Y_N$ converges to $Y$ in probability. How do I prove that $$ E(Y_N) \rightarrow E(Y) $$ as $N \rightarrow \infty$ ? This is easy when $Y_N \overset{a.s.}{\to} Y$ as one can apply dominated convergence theorem (since integration over a set of measure zero yields zero). My definition for the expected value is $$ E(X)=\int_{\Omega} X dP, $$ where $\Omega$ is the space and $P$ probability measure.","['expected-value', 'probability-limit-theorems', 'probability-theory', 'probability']"
4639930,Even more mysterious sum equaling to $\frac{7(p^2-1)}{24} - h(-p)$ where $p \equiv 3 \pmod{4}$,"This is a direct follow-up to this question, which I think is interesting enough to be a separate problem. Please check that post out too in case there are ideas there that might transfer here. In the post linked, they found numerically that for odd primes $p \equiv 1 \pmod{4}$ , the sum of all $n$ such that $(n^2+2n+1 \  \mathrm{mod} p) < (n^2 \ \mathrm{mod} p)$ is exactly $\frac{7}{24}(p^2-1)$ . It is natural to generalise this for $p \equiv 3 \pmod{4}$ , and you won't believe what I found! Here's some Sage code: x = var('x')
for p in prime_range(6, 1001):
    if p % 4 != 3: continue
    ns = [n for n in range(p) if (n^2 + 2*n + 1) % p < (n^2) % p] # your condition
    R.<t> = NumberField(x^2 + p) # setup Q[sqrt(-p)]
    assert sum(ns) == (p^2 - 1) * 7 / 24 - R.class_number() # observed = ??? As annotated above, I conjecture that for odd primes $p > 3$ satisfying $p \equiv 3 \pmod{4}$ , we have $$
\sum_{0 \leq n < p} n \cdot [(n+1)^2\,\mathrm{mod}\, p \leq n^2\,\mathrm{mod}\, p] = \frac{7}{24}(p^2-1)-h(\sqrt{-p}).
$$ Feel free to play with the Sage script above. I suspect that there might be some ""symmetry"" or relation with quadratic residues, since it is well known that the class number is (-1 / p) times sum of n * (n | p). I will set a bounty when I can.","['class-field-theory', 'number-theory']"
4639933,Let $X$ be a topological space such that $X \times \mathbb{R}$ is homeomorphic to $\mathbb{R}^2$. Must $X$ be homeomorphic to $\mathbb{R}$?,"This question was posted on twitter here as a quiz but the author never gave an answer, so I thought I'd try here. I don't have much experience with topology so I'm stumped. From searching online it seems that it's not the case in general (e.g. $X \times \mathbb{R} \cong \mathbb{R}^n$ does NOT imply $X \cong \mathbb{R}^{n-1}$ ) so I figure that the answer will have to use the fact that the product space is two dimensional, or that the quotient (either the subspace of $\mathbb{R}^2$ homeomoprhic to $X$ or the fibers of that map)  are one dimensional. Some people on twitter mentioned trying to prove that $X$ (or the image of $X$ embedded in $\mathbb{R}^2$ has all the properties that distinguish the real numbers line topologically (i.e. connected, locally connected separable metric space, such that every point is a strong cut point). Perhaps once can also impose an ordering on $X$ that matches the ordering of $\mathbb{R}$ ? I'm pretty sure that the map $ \mathbb{R}^2 \rightarrow X $ induced by the product is a quotient map, perhaps this preserves useful properties such as being connected, locally connected, etc.? Also $X$ seems to be a sub-metric space of $\mathbb{R}^2$ by embedding every point $x \in X$ into $\phi(x, 0) \in \mathbb{R}^2$ where $\phi$ is the isomorphism from $X \times \mathbb{R}$ into $\mathbb{R}^2$ . Any help would be appreciated, thanks!","['product-space', 'general-topology', 'metric-spaces', 'low-dimensional-topology']"
4639959,Modeling a probabilistic game with changing probabilities,"I am trying to model a simple game (a more complex problem I am working on reduces to it). You roll an $n$ -sided dice labeled 1 to $n$ . (This can be a normal 6-sided dice if you'd like.) If the dice shows 1 , you draw a card from a deck of $m$ cards labeled 1 to $m$ . (This can be a normal 52-card deck if you'd like.) You record which card you drew, replace it in the deck, and shuffle. This process repeats until you draw a card you have previously seen. The question: what is the distribution of the expected length of this game, in terms of the number of dice rolls? I am able to model the individual sub-games without an issue: Game 1 , the dice game, is geometrically distributed with $p=1/n$ . On average, we expect to see a 1 every $n$ throws. Game 2 , the card game, is a bit more complex, but we can model it like this: On our $(i+1)$ -th draw, we have already seen $i$ cards, all of which are unique (if not, the game would already be over). Therefore, the probability the given card $i+1$ matches one of the previous cards is $\frac{i}{m}$ for a deck of size $m$ . Factoring in the probability of not having already won, the distribution of winning the game on draw $i$ is $$
P(i) = \frac{m!/(m-i)!}{m^i} \cdot \frac{i}{m}
$$ (The numerator of the left fraction is just the number of permutations of $i$ items from $m$ items.) I can't find a closed form for the expected value of this PDF, but it matches simulations given a value of $m$ . For example, consider the standard case of $n=6$ and $m=52$ . We expect a 1 on the dice every 6 rolls. We expect to draw a duplicate card after 9.7 draws, on average. (Solved numerically in Mathematica and confirmed in simulation.) So, we expect the game to last $\approx 58$ rolls on average, since we can multiply expectations of independent events. But what is the actual distribution of game lengths? Here's some simulation data. I play the game 100,000 times: The simulation code looks like this: def play_game():
    cards_seen = set()
    for i in it.count(1):
        dice = random.randint(1, 6)
        if dice != 1:
            continue

        card = random.randint(1, 52)
        if card in cards_seen:
            return i, 1 + len(cards_seen)

        cards_seen.add(card) Statistics from the simulation: Mean number of turns: 58.19 ...median: 54 Mean number of cards drawn: 9.69 ...median: 9 Implied number of turns/per card (i.e., dice rolls 1): 6.005 This looks very familiar (i.e., vaguely negative-binomial), but the changing probability of pulling a matching card makes such an analysis tricky. I know the combination of distributions can be computed via convolution , but I'm not clear how I would apply that here, since these two distributions are not really independent — whether we play the card subgame is directly dependent on the results of the dice subgame. I believe this game can be modeled as a Markov chain, but I'm not sure if that helps with the analysis. We have dice states $D_k$ and card states $C_k$ . We start at state $D_1$ and move to state $C_1$ with probability $1/6$ , or remain at state $D_1$ with probability $5/6$ . At some state $C_k$ , we end the game with probability $P(k)$ from above, and move to state $D_{k+1}$ with probability $1-P(k)$ . Now the question reduces to finding the distribution of the number of steps in this chain.","['probability-distributions', 'markov-chains', 'card-games', 'probability']"
4639985,"Integral from MIT Integration Bee 2023 Semifinals - $\int_{0}^{\pi} \frac{2\cos(x)-\cos(2021x)-2\cos(2022x)-\cos(2023x)+2}{1-\cos(2x)}\,\textrm{d}x$","This question is from the MIT Integration Bee 2023 Semifinal #1. This integral should be solved within three minutes, and the goal is to show $$\int_{0}^{\pi} \frac{2\cos(x)-\cos(2021x)-2\cos(2022x)-\cos(2023x)+2}{1-\cos(2x)}\,\textrm{d}x = 2022\pi$$ One of the things I tried was to use the difference of cosines identity, but that didn't help me. I also looked at dividing each term individually after applying the double-angle identity to the denominator, but $\cos(ax)/\cos^2(x)$ becomes very hard to integrate for sufficiently large $a$ . Finally, I tried to apply the transformation $x \mapsto \pi/2-x$ in an attempt to see if there was symmetry I could take advantage of, but it simply resulted in the same problem as before. I'm not sure how to approach this question from here.","['integration', 'definite-integrals']"
4640002,How do I calculate the vertical travel of an oscillating arm with a fulcrum?,"I'm trying to calculate the vertical travel of the longer arm in a motion like this: I've had a go at defining the variables involved: Where the measurables are: S = Short arm length L = Long arm length F = Distance from origin of L to the fulcrum And the unknowns are: V = Total vertical travel distance of L I'd like to express V in terms of the measurable variables (perhaps more than defined above, if I've missed any, which is likely). I don't have much knowledge in this area and no idea how to approach working this out formally, short of brute forcing the measurable variables and working backwards.","['classical-mechanics', 'trigonometry', 'geometry']"
4640030,MIT Integration bee 2023 Regular Season $\int_0^\pi x\sin^4(x)dx$,"How do I find $$\int_0^\pi x\sin^4xdx$$ ? This is the 8th question in the MIT integration bee regular season. You could find integrals here. Integration by parts directly is out of question. I thought of substituting $1-\cos^2x=\sin^2x$ , but that is just more complicated. Any ideas?","['integration', 'calculus', 'definite-integrals', 'contest-math']"
4640179,Computing irreducible components of a simple algebraic set.,"Let $X = V(x^2+y^2-z) \subseteq \mathbb{A}^3$ , I'd like to compute the irreducible components of the algebraic set, and the dimension of the components. I know this is a simple example, but I'm really struggling with the concepts. I know this corresponds to $x^2+y^2-z=0$ so it's just a paraboloid, so intuitively I assume that it's already irreducible. I know that $X$ is irreducible $\iff I(X)$ is prime, but I also know that if the ideal of an algebraic set is principal, $X$ is irreducible if and only if the generator is irreducible. Either way I need to know $I(X)$ so I just guessed that $I(X) = (x^2+y^2-z)$ , the fact that $(x^2+y^2-z) \subseteq I(X)$ is obvious. But how can I show the reverse containment? If $f \in I(X)$ then all I know is that $f \in k[x,y,z]$ but that it vanishes on $X$ , I don't see what that gives me. If there's some instant justification, something along the lines of $I(X) = (f)$ whenever $X = V(f)$ let me know. Also let me know how I might justify the guess for $I(X)$ , I have absolutely zero intuition for what's going on. I know the question is super basic, but I'm really struggling with the algebraic geometry material.","['algebraic-geometry', 'abstract-algebra']"
4640195,The Sub Gradient and the Proximal Operator of ${L}_1$ Norm with Metric,"I want to solve the optimization: $$\arg\underset{x}{\min}f(x) = \arg\underset{x}{\min}\lambda\lVert Mx\rVert_1 + \frac{1}{2}\lVert x-y\rVert_2^2$$ Where $x,y\in\mathbb{R}^{n}$ and $M\in\mathbb{R}^{n\times n}$ (a positive semi-definite matrix representing a metric in my case). Taking n=2 as example, after trying to develop the sub gradient of this target function I got (for component $x_1$ ) : $$\frac{\partial{f}}{\partial{x_1}}=\sum_{i=1}^2 m_{i1}\ \text{sign}(m_{i1}x_1 + m_{i2}x_2)+(x_1-y_1)$$ By following the derivation of $L_1$ regularized least squares (ref. https://angms.science/doc/CVX/ISTA0.pdf ), we can draw polylines in $x_1x_2$ plane for a $y$ value, and finally find the intersection point where $\vec{0}\in\frac{\partial{f}}{\partial{x}}$ . But this method is hard to be generalized to 3D and higher dimension. I'm looking forward to some general solutions. Thanks a lot!","['convex-optimization', 'multivariable-calculus', 'linear-algebra', 'proximal-operators']"
4640215,Linear projections and topological direct sum in normed/Banach spaces,"When learning about the concept of complemented subspaces of a Banach space, I'm curious with the following question: Let $X$ be a vector space over $\mathbb{C}$ or $\mathbb{R}$ and let $E$ and $F$ be algebraic complemented subspaces with trivial intersection, such that $X=\ E\oplus F$ . If defining a norm $\|\cdot\|$ on $X$ , and equipping $E$ and $F$ with their norm-induced subspace topologies, respectively, is it always true that, as topological spaces, $X\simeq E\times F$ ? A closely related proposition in terms of linear projections can be stated as follows. Let $(X, \|\cdot\|)$ be a normed space over $\mathbb{C}$ or $\mathbb{R}$ , and $P: X\to X$ a linear projection such that $P^2=P$ . Then, do we always have a topological direct sum result $X\simeq \text{ran}(P) \oplus \text{Ker}(P)$ , where the range and kernel are equipped with their subspace topologies, respectively? Here I'm not assuming continuity of $P$ or whether the range or kernel are closed subspaces. Background: I'm trying to understand, on Banach spaces, why there can be discontinuous projections with a closed range. In a Banach space $X$ , if a subspace $E\subset X$ is closed, then its quotient space $X/E$ is a Banach space (complete) under the quotient norm. Purely algebraically, we would then have $X \simeq E \oplus (X/E)$ . But, for a subspace $F\subset X$ algebraically complemented to $E$ , we would also have algebraic direct sum result $X=E\oplus F$ . This seems to suggest, the natural algebraic isomorphism from $F$ to $(X/E)$ should also be a homeomorphism. But, since not every closed subspace $E$ is (alg + top) complemented, that shouldn't be true. My question is to help me understand this confusion. If the propositions in my question are true, then I can understand, even when $\text{ran}(P)$ is closed, the algebraic isomorphism from $X/\text{ran}(P)$ to $\text{Ker}(P)$ can fail to be a homeomorphism. So the kernel can still be non-closed. I already know the following two facts/propositions about linear projections on Banach spaces. If a projection $P$ is continuous, then $\text{ran}(P)$ is closed, since it is the kernel of $I-P$ . If a projeciton $P$ has closed range and closed kernel, it is continuous. This can be proved since everything is Banach (complete) and so we can freely apply the Open Mapping Thm/Inverse Mapping Thm wherever possible. See here . Thanks in advance. Update: The conclusive answer should be the following. Let $X$ be a normed vector space and $E\subset X$ a closed subspace, with its purely algebraic complemented subspace $F\subset X$ such that $X=E\oplus F$ . Then, the original norm topology $\tau_X$ of the whole space, the product topology of the subspaces $\tau|_E \times \tau|_F$ , and the product topology with the quotient space $\tau|_E \times \tau_{X/E}$ , are in general all different.","['banach-spaces', 'functional-analysis', 'product-space', 'quotient-spaces']"
4640217,What did I get wrong in solving a simple permutation problem?,"Among 4 boys and 3 girls, let's pick 2 boys and 2 girls, and let them sit around a circle. How many ways are possible? There are $_4C_2=6$ ways to pick 2 boys out of 4, and $_3C_2=3$ ways to pick 2 girls out of 3. We now have 4 people chosen and there are $3!=6$ ways to arrange them in a circle. Thus I wrote as an answer $_4C_2\times_3C_2\times3!=6\times3\times6=108$ , but the correct answer appears to be 54. Am I missing something, or is the given answer incorrect?","['permutations', 'combinations', 'combinatorics']"
4640290,How to prove that these subspaces of $\Bbb R^3$ are pairwise non-homeomorphic?,"Let $\mathbb{R}^3$ be usual topological space and $\mathbb{Q}$ the set of rational numbers. Define $X,Y,Z,$ and $W$ as follows $$\begin{align}X&=\{(x,y,z)\in\Bbb R^3\mid |x|+|y|+|z|\in\mathbb Q\}\\
Y&=\{(x,y,z)\in\Bbb R^3\mid xyz=1\}\\
Z&=\{(x,y,z)\in\Bbb R^3\mid x^2+y^2+z^2=1\}\\
W&=\{(x,y,z)\in\Bbb R^3\mid xyz=0\}
\end{align}$$ Which of the following statements is correct? $a.$ $X$ is homeomorphic to $Y.$ $b.$ $Z$ is homeomorphic to $W.$ $c.$ $Y$ is homeomorphic to $W.$ $d.$ $X$ is not homeomorphic to $W.$ According to me $X$ is NOT connected but $W$ is connected so answer is option $d?$ Am I right? Thank you .","['connectedness', 'general-topology', 'compactness']"
4640309,When is a cdf absolutely continuous?,"Let $\mu$ be a probability distribution on $(\mathbb R, \mathcal B(\mathbb R))$ and $X$ be a random variable with distribution $\mu$ . According to the theory developed in Section 3.5 of Folland's Real Analysis , $\mu$ is absolutely continuous w.r.t. Lebesgue measure iff the cdf $F_X$ is absolutely continuous. Assume that $F_X$ is known. What are tractable conditions on $F_X$ which guarantee that $F_X$ is absolutely continuous ? For example, I know that the following condition is sufficient (since it implies that $F_X$ is Lipschitz): $F_X$ is continuous over $\mathbb R$ , $F_X$ is differentiable except perhaps at finitely many points, and $(F_X)'$ is bounded. Are the following conditions also sufficient ? $F_X$ is continuous over $\mathbb R$ and $F_X$ is differentiable except perhaps at finitely many points. $F_X$ is continuous over $\mathbb R$ and $F_X$ is differentiable except perhaps at countably many points. $F_X$ is continuous over $\mathbb R$ and $\int_{\mathbb R} F_X'(x) dx = 1$ . Note that $F_X$ already has some regularity (non-decreasing, cadlag, limits at $\pm \infty$ , differentiable a.e.).","['measure-theory', 'absolute-continuity', 'probability-theory', 'real-analysis']"
4640324,Find the average value of the average value of the image of a random finite set.,"Context: This is part of an absurd thought experiment in which an immortal, telekinetic rat tries to push a blinking button from a variable distance while piloting a spacecraft blindfolded at relativistic speeds. I need to calculate what the rat most likely believes is the probability that the button is lit. This is my attempt to do that. Fix $\omega\in\Bbb R$ , and let $L:\Bbb R\to\Bbb R$ be given by $$L(t)=\lfloor\sin(\omega t)+1\rfloor$$ $\color{lightgray}{\text{[This is the signal controlling the light]}}$ For each $n\in\Bbb N$ , let $[\Bbb R]^n=\{S\subseteq\Bbb R:|S|=n\}$ . Let $[\Bbb R]^{<\Bbb N}=\bigcup_{n\in\Bbb N}[\Bbb R]^n$ , and define $A:[\Bbb R]^{<\Bbb N}\to\Bbb R$ according to $$\begin{align}
A(T)&=\frac1{|T|}\sum_{t\in T}\lfloor\sin(\omega t)+1\rfloor\\
&=\frac1{|T|}\sum_{t\in T}L(t)
\end{align}$$ That is, $A(T)$ is the average value of the image of $T$ under $L$ . $\color{lightgray}{\text{[Each finite subset of $\Bbb R$ corresponds to a possible set of times (as measured from the button)}\\
{\text{when information about the state of the button reaches the rat. Due to time dilation and}\\
\text{unpredictable movement (a result of the blindfold), we can assume nothing about the likelihood}\\
\text{of a given set.]}}}$ Questions: How is the average value of $A$ defined? What is the average value of $A$ ? It seems extremely obvious that the answer should be $1/2$ . Realistically, a large enough uniform random sampling of a sufficiently large interval $[a,b]$ will almost always land you with a value of $1/2$ ; but I don't know how to show that the average average is actually $1/2$ , and that the apparent tendency towards this value isn't a miraculous coincidence. Intuitively, I'd think that we'd want to take some kind limit-integral like $$\overline A=\lim_{R\to[\Bbb R]^{<\Bbb N}}\mu(R)^{-1}\int_R A(T)\ dT$$ Given $|[\Bbb R]^{<\Bbb N}|=|\Bbb R|$ (obviously true), there must be some bijection $C:\Bbb R\to[\Bbb R]^{<\Bbb N}$ . Assuming that we can define $C$ in a nice way, we could reasonably assert $$\lim_{R\to[\Bbb R]^{<\Bbb N}}\mu(R)^{-1}\int_R A(T)\ dT=\lim_{a\to-\infty}\lim_{b\to\infty}\frac1{b-a}\int_a^b A(C(t))\ dt$$ but 1) this requires $A\circ C:\Bbb R\to\Bbb R$ to be continuous, which I'm not sure is even possible (and I don't know how to prove it either way), 2) there may not be a nice way to define $C$ even if such a continuous function does exist.","['average', 'statistics', 'real-analysis']"
4640326,Proof of this series converging to zero does not convince me,"I have a function $F$ that is bounded and such that $F(x) \to 0$ whenever $x\to 0$ . Then, I have the following series $$
\sum_{k=0}^\infty \frac{F(2^{k+1}\delta)}{2^k}
$$ for $\delta > 0$ . I want to show that this series converges to $0$ as $\delta\to 0$ . Proof from the book which I don't understand Given $\epsilon > 0$ we choose $N$ so large that $$
\sum_{k \geq N} \frac{1}{2^k} < \epsilon.
$$ Then, by making $\delta$ sufficiently small, we have (since $F(x)\to 0$ as $x\to 0$ ) $$
F(2^k \delta) < \frac{\epsilon}{N}
$$ whenever $k=0, 1, \ldots, N-1$ . Since $F$ is bounded then the result follows. What I don't understand I am very lost by this proof. What is it doing in the first step? How does it know I can find such an $N$ ? And how does it know that by choosing $\delta$ sufficiently small, I can bound $F(2^k \delta)$ ?","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4640420,Does independence of random variables imply independence of conditional expectations?,"Does $X\perp\!\!\!\perp Y\implies\mathbb{E}[Z|X]\perp\!\!\!\perp\mathbb{E}[Z|Y]$ for any r.v. Z? I would think so, because $X\perp\!\!\!\perp Y$ implies $\sigma(X)\perp\!\!\!\perp\sigma(Y)$ and $\sigma(\mathbb{E}[Z|X])\subset\sigma(X)$ as well as $\sigma(\mathbb{E}[Z|Y])\subset\sigma(Y)$ for any r.v. $Z$ per definition, q.e.d. However my intuitions fail half the time in probability theory, so I would be happy if someone could confirm this or point out an error.","['conditional-expectation', 'independence', 'probability-theory']"
4640479,Prove $x^3 = x^2$ in an abelian semigroup given two elements with a specific property,"Let $(A, \cdot)$ be an abelian semigroup with $|A| \geq 2$ such that there exists $a, b \in A$ for which $a^2x^3b^2 = x^2, \forall x \in A$ . Prove that $x^3 = x^2, \forall x \in A$ . I first tried playing along with the given equality by substituting $x \rightarrow a$ and $x \rightarrow b$ , with no real results. However, I found it particularly useful to consider $x \rightarrow ab$ , and then I obtained $a^2(ab)^3b^2 = (ab)^2$ , but since $A$ is abelian, we can write that $(ab)^5 = (ab)^2$ , for the fixed $a, b \in A$ . Then I set $x \rightarrow abx$ , and I obtained that $a^2(abx)^3b^2 = (abx)^2$ , which after rearranging with respect to the commutativity of the semigroup, I obtained $(ab)^5x^3 = (ab)^2x^2$ . However, we know from the previous result that $(ab)^5 = (ab)^2$ , then the new equality becomes $(ab)^2x^3 = (ab)^2x^2, \forall x \in A$ . Edit. Setting $x \rightarrow a, x \rightarrow b$ and multiplying the two resulting equalities, we can also obtain $(ab)^7 = (ab)^2$ . Do not know if that can be utilized in any useful way. This was as far as I could get, and I am completely lost because I cannot find any meaningful way of reducing the $(ab)^2$ terms from the left. Any hints or answers would really help me out a lot figuring this out. Thanks a lot!","['abstract-algebra', 'semigroups', 'commutative-algebra']"
4640487,"Evaluation of $\int_0^1\frac{\log x\,dx}{\sqrt{x(1-x)(1-cx)}}$","Assume $c$ is a small real number. QUESTION. What is the value of this integral in terms of the complete elliptic function $K(k)$ ? $$\int_0^1\frac{\log x}{\sqrt{x(1-x)(1-cx)}}\,dx.$$ I got as far as (give or take some silly errors) expressing the integral as $$\int_{-\omega_1}^{\omega_1}\log\left(\wp(z)-\wp(\omega_2)\right)dz$$ where $\wp(\omega_1)=e_1, \wp(\omega_2)=e_3$ and $\wp(\omega_3)=e_2$ while the $e_j$ 's are the (real) roots of the cubic equation associated to the Weierstrass elliptic function of the current problem. UPDATE. I have found a solution to this problem using the Weierstrass functions. However, I welcome any sort of alternative approach.","['integration', 'logarithms', 'calculus', 'closed-form', 'elliptic-integrals']"
4640502,Prove $\lim \limits_{h \to 0} \frac{a^h-1}{h} = \ln (a)$ without using L'hospital's.,"I'm a Calc 2 student and was curious as to why $\frac{d}{dx}a^x = a^x\ln a$ . Using the limit definition you can arrive at $\frac{d}{dx}a^x = \lim \limits_{h \to 0}  \frac{a^x(a^h-1)}{h}$ so the part of the limit involving $h$ must be $\ln (a)$ . I was thinking it'd be cool to use the definition of $e^x = \sum_{n=1}^\infty \frac{x^n}{n!}$ by setting $y=\sum_{n=1}^\infty \frac{x^n}{n!}$ and solving for $x$ which I'm guessing yields some sort of sum representation for $\ln(x)$ . Given the above I have a couple questions. How can I turn a limit expression such as $\lim \limits_{h \to 0}  \frac{(a^h-1)}{h}$ into a Riemann sum and vice versa? How would someone go about solving for $x$ in $y =\sum_{n=1}^\infty \frac{x^n}{n!}$ I know there are prob way easier ways to figure that out, but I'm curious as to whether this way of solving it works and how it pans out. Thanks.","['logarithms', 'riemann-sum', 'limits', 'derivatives', 'exponential-function']"
4640543,"Proving commutativity in a subset $H \subset G$ of a finite group given $x^{-2}y^5 \in H, \forall x, y \in H$ and $x^{-2}y^5 = y^5x^{-2}$","Let $n \geq 1, n \in \mathbb{N}$ and $(G, \cdot)$ a group with $|G| = 10n + 1$ and $H \subset G$ such that $x^{-2}y^5 \in H, \forall x, y \in H$ . Prove that if $x^{-2}y^5 = y^5x^{-2}, \forall x, y \in H$ , then $xy = yx, \forall x, y \in H$ . I feel like this problem is a dead end for me. I first started by choosing $x \in H$ , and by setting $y \rightarrow x$ in the hypothesis, we obtain that $x^3 \in H$ (1). Next, knowing that both $x, x^3 \in H$ , I set $x \rightarrow x^3$ and $y \rightarrow x$ in the hypothesis, obtaining $(x^3)^{-2}x^5 = x^{-6}x^5 \in H$ , thus $x^{-1} \in H, \forall x \in H$ (2). After proving that the inverse of any element in $H$ resides in $H$ , I had the idea of maybe showing that $H$ is a subgroup of $G$ , then we have to somehow prove that $xy \in H$ by writing it in terms of the hypothesis, and by using the fact that $x^{-2}y^5 = y^5x^{-2}$ we might be able to prove $xy = yx$ . I then began asking myself if $e \in H$ . I thought about showing $x^{10k + 1} \in H, \forall x \in H, k \in \mathbb{Z}$ , so that we can use a consequence of Lagrange's theorem ( $x^{ord(G)} = e$ ), and therefore prove that $e \in H, \forall n \in \mathbb{N}$ . Using (1) and (2), we get that $x^{-3} \in H$ , then by the hypothesis, we get that $(x^{-3})^{-2}x^5 \in H \Leftrightarrow x^{11} \in H \Leftrightarrow x^{10 * 1 + 1} \in H$ . Likewise, I can prove that $x^{21} = (x^{-3})^{-2}(x^3)^5 \in H \Leftrightarrow x^{21} \in H \Leftrightarrow x^{20 * 1 + 1} \in H$ . I went on to produce similar results for $x^{31}$ and $x^{41}$ , but I cannot seem to prove the general induction case. Maybe I am wrong. Lastly, I tried taking $x, y \in H$ and setting $x \rightarrow xy$ , therefore obtaining that $(xy)^{-2}y^5 \in H$ and trying to use the commutativity property, but to no avail. Any hints would be very much appreciated on this problem. If anybody has any solution and is willing to share, I would also highly appreciate that. Thanks a lot for taking the time to read through this!","['group-theory', 'abstract-algebra', 'finite-groups']"
4640547,Can every Riemannian manifold be written as a statistical manifold?,"Given a sufficiently nice PDF $f:M\times \Theta \to \mathbb{R}$ where $M\subset \mathbb{R}^D$ and $\Theta\subset \mathbb{R}^p$ if the Fisher Information matrix $$I_{ij}(\vec{\theta})=\mathbb{E}\left[\left(\partial_{\theta_i} \log f(\vec{X}, \vec{\theta})\right) \cdot \left(\partial_{\theta_j} \log f(\vec{X}, \vec{\theta})\right)\bigg|\theta\right]$$ is positive definite (PD) then it forms a metric tensor in the variable $\theta$ and we can say that $(\Theta, I)$ is a manifold, called a statistical manifold. Thus, for nice enough PDFs, we can always find an associated a statistical manifold. Question: Given a Riemannian manifold $(\Theta,g)$ with metric tensor $g$ , can we find a PDF $f:M\times \Theta\to\mathbb{R}$ (for some $M$ ) such that $I(\vec{\theta})=g(\vec{\theta})$ ? If so, is such a PDF unique? In other words: is every metric tensor the Fisher information metric for some RV $X$ ? Another way to ask is whether every metric tensor $g$ on $\Theta$ can be written as $$g(\theta) = - \mathbb{E}\left[\nabla_\theta^2 \log f(\vec{X}, \vec{\theta})\bigg| \theta\right],$$ for some sufficiently nice $f$ ? Some thoughts: I would wager that the class of manifolds is larger than the class of statistical manifolds (those whose metric tensors are the FIM of some RV). But this is just a rough guess and I am not sure what tools to use to approach this question. If anybody has suggestions or hints I would gladly expend more effort on this and add some attempts.","['information-geometry', 'differential-geometry']"
4640572,Energy bound for a closed curve,Let $\gamma : S^1 \rightarrow M$ be a smooth map from a circle of length 1 to a closed manifold $M$ with nonpositive curvature. Could we find a constant $C > 0$ depending only on $M$ such that $$\int_{S^1}||\nabla_{\dot{\gamma}}\nabla_{\dot{\gamma}}\dot{\gamma}||^2 \ge C\int_{S^1}||\nabla_{\dot{\gamma}}\dot{\gamma}||^2$$ for arbitrary $\gamma$ ? I have constructed counterexamples on the 2-sphere and a noncompact manifold with negative curvature; that's the reason for the assumption on $M$ .,"['curves', 'integral-inequality', 'differential-geometry']"
4640582,How is $\frac{\cot10^\circ-\cot20^\circ}{\tan40^\circ(\cot10^\circ+\cot20^\circ)-2}=\frac{\sqrt3}{3}$,"Can someone show me how $$\dfrac{\cot10^\circ-\cot20^\circ}{\tan40^\circ(\cot10^\circ+\cot20^\circ)-2}=\dfrac{\sqrt3}{3}$$ Is this an obvious simplification, as I really cannot see it. The LHS is equivalent to $$\dfrac{\frac{\cos10^\circ}{\sin10^\circ}-\frac{\cos20^\circ}{\sin20^\circ}}{\frac{\sin40^\circ}{\cos40^\circ}\left(\frac{\cos10^\circ}{\sin10^\circ}+\frac{\cos20^\circ}{\sin20^\circ}\right)-2}$$ which is $$\dfrac{\frac{1}{\sin20^\circ}}{\frac{\cos20^\circ}{\sin10^\circ\cos40^\circ}-2}$$ Is there a straight-forward simplification, or is this the approach?","['algebra-precalculus', 'trigonometry']"
4640587,"Given $a_1$, find an increasing sequence so that $a_1+\dots+a_k $ divides $ a_1^2+\dots+a_k^2$ for all $k$","Prove that for every natural number $a_1>1$ there's an infinite series $a_1<a_2<a_3<...$ such that for every natural number $k,$ $a_1+a_2+...+a_k \vert a_1^2+a_2^2+...+a_k^2$ . At first glance, I thought this problem is an induction problem and could be solved by forming the sequence inductively. I tried that and didn't get anywhere even though I'm pretty sure there must be an inductive solution. I went in a different direction, experimenting with series and trying the series $$a_2=3\cdot a_1$$ $$a_3=3\cdot a_2=9\cdot a_1$$ $$a_4=3\cdot a_3=9\cdot a_2=27\cdot a_1$$ and so on... (esentially every number is three times the number before it in the series) This way, if we assume the condition stands for $k-2$ and try to prove it for $k-1$ we get $$a_1\cdot (\frac{3^k-1}{2}) \vert a_1^2\cdot (\frac{9^k-1}{8})$$ From here, we know $a_1 \vert a_1^2$ so we need $\frac{3^k-1}{2} \vert \frac{9^k-1}{8}$ , but since $\frac{9^k-1}{8}=\frac{3^{2k}-1}{8}=\frac{(3^k-1)(3^k+1)}{8}=\frac{(3^k-1)}{2}\cdot \frac{(3^k+1)}{4}$ , so actually we get that $\frac{3^k-1}{2} \vert \frac{9^k-1}{8}$ if $\frac{(3^k+1)}{4}$ is a whole number. This only works for $k\equiv 1$ (mod 2). I'm not sure where to go from here, I've tried proving this, but for even numbers and haven't gotten anywhere. I'm worried I might be going deep down the series solution rabbithole even though it might not lead anywhere. Any help is appreciated, thanks!","['elementary-number-theory', 'induction', 'sequences-and-series']"
4640657,"Show that $\sigma(a+\lambda b) = \sigma(a) + \lambda \sigma(b)$, for all $\lambda \in \mathbb{C}$.","Let $a,b$ be two elements of a Banach algebra $A$ . Suppose that $\sigma(a+\lambda b)$ contains only one element for all $\lambda \in \mathbb{C}$ .
Prove that both $\sigma(a)$ and $\sigma(b)$ contain only one point in their respective spectra, and that $\sigma(a+\lambda b) = \sigma(a) + \lambda \sigma(b)$ , for all $\lambda \in \mathbb{C}$ . Consider $f: \mathbb{C} \to A$ given by $f(\lambda)=a+\lambda b$ , then clearly $f$ is analytic. By assumption, $\sigma(a+\lambda b)=\sigma(f(\lambda)) = \{\alpha(\lambda)\}$ for all $\lambda \in \mathbb{C}$ , where $\alpha$ is a mapping from $\mathbb{C}$ into $\mathbb{C}$ . By Corollary 3.4.18 in Aupetit's A Primer on Spectral Theorey , $\alpha$ is holomorphic on $\mathbb{C}$ , and therefore, for every $\lambda \in \mathbb{C}$ , \begin{align*}
\alpha(\lambda) &= \alpha_0 + \alpha_1\lambda + \alpha_2\lambda^2 + \dots
\end{align*} Taking $\lambda = 0$ shows that $\sigma(a)=\sigma(f(0))= \{\alpha(0)\}=\{\alpha_0\}$ , which shows that $\sigma(a)$ contains only one element. How can I use this to continue with the problem, or am I on the wrong track completely?","['complex-analysis', 'banach-algebras', 'spectral-theory', 'functional-analysis']"
4640664,Limit of $n\cdot(1-\frac{1}{n})^{n\cdot \ln(n)}$ as $n \to \infty$,"I am aware of the well-known limit that $$\lim_{n\to \infty}\left(1-\frac{1}{n}\right)^n = \frac{1}{e}$$ However, I am having trouble evaluating it within a more complex function that also depends on $n$ . For example, $$\lim_{n\to \infty}n\cdot\left(1-\frac{1}{n}\right)^{n\cdot \ln(n)}$$ Can we nonetheless still evaluate the expression as $$n\cdot \left(\frac{1}{e}\right)^{\ln(n)} = n \cdot n^{-1} = 1$$","['limits', 'sequences-and-series', 'real-analysis']"
4640727,the bracket partition function ? $3 = 1 + 1 + 1 = 1 + 2 = 1 + (1 + 1) $,"I want to know in how many ways we can write a positive integer by using strict positive integers, addition and brackets. The order of addition does not matter. For instance $$3 = 1 + 1 + 1 = 1 + 2 = 1 + (1 + 1) $$ invalid expression are brackets over the whole such as $(1+1+1),(3)$ and nonsensical bracket use such as $1+1+(1)$ or $1+() + 2$ not closing the brackets. So we can write 3 in 4 ways. Let $M(n)$ be the number of ways we can write $n$ like that. This function $M(n)$ is clearly an analogue to the partition function, and it grows faster. But how fast does this function grow ? What is known about it ? I tried to find useful recurrence equations and generating functions but failed. Similar question for $T(n)$ which counts the number of ways but where the order matters for distinct parts.
So here $1+1+1$ counts as one way , but $1 + 2$ and $2+1$ count as 2 ways. edit See also : https://mathworld.wolfram.com/Bracketing.html https://mathworld.wolfram.com/SuperCatalanNumber.html https://en.wikipedia.org/wiki/Partition_function_(mathematics)","['integer-partitions', 'combinatorics', 'discrete-mathematics', 'asymptotics']"
4640740,Prove: $\dim(W_1 \cap W_2) \ge \dim V - 2\dim W $,"Let $ V $ be a linear space over a field $ F $ and let $ W, W_1, W_2 $ be subspaces of $ V$ . Assume that $ V = W \oplus W_1 = W \oplus W_2. $ Prove that $$\dim(W_1 \cap W_2) \ge \dim V - 2 \dim W.$$ Could you, please, verify the following proof? $$\dim W_1 + \dim W = \dim V \\
\dim W_2 + \dim W = \dim V$$ \begin{align}
\dim(W_1 + W_2) 
&= \dim W_1 + \dim W_2 - \dim (W_1 \cap W_2) \\
&= \dim V - \dim W + \dim V - \dim W - \dim(W_1 \cap W_2) \\
&= 2 \dim V - 2 \dim W - \dim(W_1 \cap W_2)
\end{align} $$\dim(W_1 + W_2) + \dim(W_1 \cap W_2) = 2 \dim V - 2 \dim W$$ Since $W_1 + W_2$ is a subspace of $V$ , we have $\dim(W_1 + W_2) \le \dim V$ and, therefore, we get: $$\dim(W_1 \cap W_2) \ge \dim V - 2 \dim W.$$","['solution-verification', 'linear-algebra', 'vector-spaces']"
4640775,Condition implying $N(H)/H$ a Coxeter group?,"I'm interested in which finite groups can arise as $$
N(H)/H
$$ for $ H $ a connected subgroup of a compact connected simple Lie group $ G $ . One obvious family of examples is take $ H $ to be the maximal torus then $ N(H)/H $ is the Weyl group of $ G $ . For some other examples I looked at $ N(H)/H $ is just cyclic 2. Also all the $ N(H)/H $ given in the second column of tables 5,6,7,8 of [https://arxiv.org/abs/math/0605784] seem to be Coxeter groups or at least complex reflection groups. Is it possible that $ N(H)/H $ is always a Coxeter group? (again I'm assuming $ H $ a connected subgroup of a connected Lie group $ G $ ) Seems like a bit of a crazy conjecture, but mostly I'm just interested in understanding the structure of the finite group $ N(H)/H $ .","['representation-theory', 'finite-groups', 'coxeter-groups', 'group-theory', 'lie-groups']"
4640788,What is the right definition for the limits from the left and the right?,"While I am reading some books about analysis, i found that the definitions used for the notion of the limit from the right and the limit from the left are not always the same in all the references.
Some references give the definitions as follows: Note that the definition above takes in accounting that $x$ may take the value of $a$ which guarantee the equivalence given in the following theorem: Whereas some other references use the following definitions: but in this case the previous theorem will not hold, we can for example define a function $f$ to be $1$ at $x_{0}=0$ and to be $x$ at $x\neq0$ , the limit from the right and the limit from the left at $0$ for this function will $0$ but $f$ doesn't have a limit!
and if we choose the first definition then I think that many examples have been solved in the wrong way, for instance let's take the following function: usually we say that the limit from the right of $0$ is $3$ but in fact if we apply the first definition we will find that the limit from the right of $0$ doesn't exist!
Do you have any suggestions to deal with this situation?","['limits', 'functions', 'real-analysis']"
4640794,Is $\frac{1}{x^2+1/n}$ uniformly integrable?,"For a set of functions $f_n\in F$ my definition of uniform integrability over the range $[-1,1]$ is $$\forall \epsilon>0, \exists M_{\epsilon}>0 : \sup_{f_n\in F}\int_{\{|f_n|\geq M_{\epsilon}\}}|f_n|\,d\mu<\epsilon$$ where $n=1,2,3...$ .
My function of interest is confusing though, $$f_n(x)=\frac{1}{x^2+1/n}$$ because it is integrable over $x\in [-1,1]$ for any finite $n$ , but its limit in $n\rightarrow\infty$ is not integrable. Using the definition of uniform integrability, for any $\epsilon$ and any arbitrarily large $n$ , I can find an $M_{\epsilon}$ that makes the statement true. But on the contrary, for any $\epsilon$ and any $M_{\epsilon}$ , I can find an $n$ that makes it false.  This seems like a problem of which infinity grows faster.  How can I resolve whether my test function is uniformly integrable or not?","['uniform-integrability', 'measure-theory']"
4640797,When is the product of two matrices diagonalizable?,"Say, I have two square matrices, $A$ and $B$ , not necessarily Hermitian, whose eigenvalues and eigenvectors are known, and I also know if they are diagonalizable. Is there a way to figure out if their product $AB$ is diagonalizable without explicitly calculating the eigenvalues and eigenvectors of $AB$ ?","['matrices', 'diagonalization', 'linear-algebra', 'eigenvalues-eigenvectors']"
4640809,Using Maclaurin series to solve $y'=xe^x$.,"I am trying to solve a differential equation using the Maclaurin series. The differential equation is $$y'=xe^x$$ My solution 1: $xe^x$ expressed in sigma notation is: $\sum_{k=0}^\infty\frac{x^{k+1}}{k!}$ and $y'=\sum_{k=0}^\infty(k+1)a_{k+1}x^k$ . Equating them will lead to $$\sum_{k=0}^\infty(k+1)a_{k+1}x^k=\sum_{k=0}^\infty\frac{x^{k+1}}{k!}$$ I was trying to comparing the coefficients, so as to obtain a recurrence relation. But I couldn't do it because the $x$ on both sides of the equation has different exponents ( $k$ and $k+1$ ) respectively. My solution 2: I integrate the differential equation to obtain $y=xe^x-e^x+c$ which in sigma notation corresponds to $$y=c+\sum_{k=0}^\infty\frac{x^{k+1}}{k!}-\sum_{k=0}^\infty\frac{x^k}{k!}$$ which can be samplified as $$y=c+\sum_{k=0}^\infty\frac{x^k(x-1)}{k!}$$ Question: Am I going in the right direction? In both solutions, I am stuck and am unable to arrive at the textbook solution of $$y=c+\sum_{k=0}^\infty\frac{x^{k+2}}{(k+2)k!}$$ Thank you in advance.","['power-series', 'ordinary-differential-equations']"
4640828,Why haven't I heard of the Hausdorff quotient?,"I just today discovered that, similar to the Kolmogorov quotient, there is a universal quotient from any topological space into a Hausdorff space, which any continuous map into a Hausdorff space factors through. This seems to be all but invisible on the internet: the only  things I could find talking about it are a single MathOverflow post and a Bachelors dissertation citing it. This hasn't been mentioned in any topology material I have seen, and I only thought to look because I stumbled across it myself. The Kolmogorov quotient is mentioned everywhere, and seen as very important, so why is the Hausdorff quotient so neglected? Similarly, there is (I am near certain, as the same proof works for each of them) a universal $T_1$ quotient, which I also haven't seen mentioned, and doesn't seem to have a single page discussing it. The only reason I can think that this might be is that the quotient may be quite trivial for most spaces that crop up, as it seems more 'aggressive' than the Kolmogorov quotient in combining points. I'm not aware of any non-Hausdorff connected spaces for which the quotient is not the topological space of size $1$ , for example. On the other hand, I only discovered this today. And it seems unlikely that all quotients are that trivial. I would also be interested to know any interesting examples of the $T_1$ and Hausdorff quotient that you're aware of.","['general-topology', 'soft-question', 'quotient-spaces']"
4640937,Solve over natural numbers: $m^3=2n^3+6n^2$. Functional equation gives rise to a diophantine equation!,"My question is basically to find all natural numbers $(m,n)$ such that $m^3=2n^3+6n^2$ First for some background (this is not really that relevant but anyways):
I was trying to solve an olympiad functional equation $f:\mathbb{N} \to \mathbb{N}$ such that $f(x+y)=f(x)+f(y)+3(x+y)\sqrt[3]{f(x)f(y)}$ Then we will get that $f(x)$ will always be a perfect cube so setting $f(x)=g(x)^3$ and then $P(1,1)$ gives $g(2)^3=2g(1)^3+6g(1)^2$ . And then I noticed that if $g(1)=1$ we could just induct up to get $g(x)=x$ always, so this is where the question comes from, because if i prove the only solution in natural numbers to the diophantine in $(1,2)$ I will be done. Now back to the diophantine, obvioulsy $(m,n)=(2,1)$ is a solution and wolfram alhpha says it is the only solution. For the past one hour or so I have been trying to prove this but have done basically nothing... I have just been doing random subsitutions and making cases and stuff, like $m=2y$ so then we get $y \equiv x \mod 3$ and then making cases like if $x$ is divisible by $3$ or not but I have not even been able to rule out one case... There are infact more integer solutions so I believe we could use some inequalities somehow but I am not sure how whatsoever... Any help on this problem will be appreciated... Thanks!","['contest-math', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'inequality']"
4640948,Existence of a mapping that induces a certain push-forward measure,"Let $X \subset \mathbb{R}^n$ be compact and consider the measurable space $(X, \mathcal{B}(X))$ where $\mathcal{B}(X)$ is the Borel $\sigma-$ algebra over $X$ . Denote by $\mu$ the Lebesgue measure on this space. Let $\nu$ be another measure on this space such that, $\nu \ll \mu$ , i.e., it's absolutely continuous w.r.t $\mu$ . Can we always find a measurable mapping $T: X \to X$ such that $T_\# \mu = \nu$ , i.e., such the push-forward measure induced by $T$ is equivalent to $\nu$ ? If not, what conditions one needs to impose on $\nu$ for this to hold? I am not sure how to approach this, but my intuition is that $T$ , if it exists, should be non-singular.","['pushforward', 'measure-theory', 'lebesgue-measure']"
4640956,When will $a \pm b\sqrt{c}$ will have a nth root of the same form?,"So recently I had a homework problem for my abstract algebra class which I solved where I had to prove $\sqrt[3]{8+\sqrt{325}} +\sqrt[3]{8-\sqrt{325}}=3$ .  It was simple enough, all one has to do is cube it and after a bit of algebra note the cubic polynomial is relatively easy to factor. I did not do this though, instead my approach was a bit more complicated (I missed the simple approach).  I made an educated guess that the form of the cube root of $8 \pm \sqrt{325}$ would be $m \pm n\sqrt{13}$ .  This was because of at least a couple of things I had noted.  Similar already solved problems in the book had solutions essentially in this form.  Moreover, I knew I needed it to simplify to a rational number so I really wanted the squares to cancel out, and intuitively the form just made sense to me. Yet while my intuition got me the right answer, I really feel like something deeper is lurking here.  Part of me suspects that its possible to prove with some conditions imposed on a,b,c that the nth root of $a \pm b\sqrt{c}$ must also be of that form, though I suspect some weirdness potentially (and I am unsure how to exactly state this).  Though, a large part of me is very unsure.  Why should I even suspect roots to exist in that field, $\mathbb Q[\sqrt{13}]$ is obviously not algebraically closed since -1 doesn't have a root. So, with $a,b,c \in \mathbb Q$ when will $a \pm b\sqrt{c}$ have an nth root of the same form?","['complex-analysis', 'number-theory', 'abstract-algebra']"
4640962,Prove that $\frac a{a+2b}+\frac bc+\frac{c^2}{\left(c+2a\right)^2 }\ge1$,"(sqing) Let $a$ , $b$ , $c>0$ . Prove that $\dfrac a{a+2b}+\dfrac bc+\dfrac{c^2}{\left(c+2a\right)^2 }\ge1$ . Source (unsolved) In this inequality, $b$ appears the least frequent, so I want to deal with it. I took $a$ , $c$ as parameters and derived $b$ , we have
\[\frac{\text d}{\text db}~\left(\dfrac a{a+2b}+\dfrac bc\right)=\frac1c-\frac{2a}{(a+2b)^2}.\tag1\label1\]
When $2c>a$ , $\eqref1=0$ gives $b=\dfrac{\sqrt{2ac}-a}2$ . When $2c\le a$ , $\eqref1>0$ is true for all $b$ , so $b\to0$ shall minimize the left side. But plugging back makes the expressions complicated. What should I do?","['algebra-precalculus', 'inequality']"
4641049,"What purpose does the adjective ""monotonically"" serve in the context of a ""monotonically increasing/decreasing function""?","Why employ the adjective ""monotonically"" when referring to an ""increasing"" or ""decreasing"" function? Forsooth, if the function is indeed exhibiting an upward or downward trend, it is inherently ""monotonic"". Thus, it would seem that the term ""monotonically"" serves no purpose but to belabor the point, leading one to question its necessity.","['functions', 'monotone-functions', 'terminology']"
4641083,Find limit of sequence $(x_n)$,"Find limit of sequence $(x_n)$ : $$x_1 = a >0$$ $$x_{n+1} = \frac{n}{2n-1}\frac{x_n^2+2}{x_n}, n \in Z^+$$ I think I can prove $(x_n)$ is low bounded (which is obvious that $x_n>0$ ) and decreasing sequence. Then I can calculate the limit of sequence is $\sqrt{2}$ All my attempts to prove it's a decreasing sequence have been unsuccessful. My attemps: Try to prove $x_{n+1}-x_{n} <0$ from a number $N_0$ large enough. It lead to I have to prove $x_n \ge \sqrt{\frac{2n}{n-1}}$ and I stuck. Does anyone have an idea?","['sequences-and-series', 'analysis', 'real-analysis']"
4641122,Conceptual reason why Coxeter groups are never simple,"Is there a conceptual reason why (non-abelian) Coxeter groups are never simple? For example is there some obvious normal subgroup that can be defined? Or perhaps it is for some reason clear that the commutator subgroup of a Coxeter group is always proper? I was looking through the list of Coxeter groups and noticed that a lot of them are pretty close to simple, but not quite. For example the Coxeter groups of type $ A_n $ are $ S_{n+1} $ which has the simple index $ 2 $ alternating subgroup $ Alt_{n+1} $ for $ n \geq 4 $ . Coxeter groups of type $ E_6, E_7, E_8, H_3 $ also have index 2 simple subgroups: $ PSp_4(3), Sp_6(2), GO_8^+(2),Alt_5 $ respectively. The Coxeter groups of type $ B_n=C_n $ and type $ D_n $ have a single non-abelian simple composition factor: $ Alt_n $ , but these groups are not simple. The Coxeter groups of type $ F_4 $ and $ I_2(n) $ are solvable. Finally, the Coxeter group of type $ H_4 $ has an index $ 4 $ subgroup $ Alt_5 \times Alt_5 $ . So by exhaustion no Coxeter group is simple. I was wondering if there is a nice conceptual reason for that, especially since some of them get so close to being simple (like $ A_n, E_6,E_7,E_8, H_3 $ ), but aren't quite.","['group-theory', 'finite-groups', 'coxeter-groups']"
4641184,"I have 16 balls (they are the same, order does not matter) to put in 4 drawers. The condition: every drawer must have an odd number of balls","What I did was I calculated the options of putting 16 balls in 4 drawers with the condition of every drawer must have an even number of balls.
In order to ensure I have an even number of balls in every drawer I inserted the balls in pairs.
So I have 8 pairs to insert to 4 drawers. the result is 165 options ( according to the formula ).
In order to solve my question can I do 16 choose 4 ( that are all the options ) minus 165 (even number of balls in every drawer) to get the odd number of balls in each drawer that i'm asked?","['combinatorics', 'discrete-mathematics']"
4641187,Folland: Convergence in weak topology iff convergence in dual,"I am self-learning real analysis from Folland and got stuck on weak topology convergence. He defines weak topology as follows: If $X$ is a normed vector space, the weak topology on $X$ is the weak topology generated by $\{f\,|\, f\in X^{*}\}$ . Then he asserts the following which I am trying to prove: If $\langle x_\alpha \rangle$ is a net in $X$ , then in the weak topology $\langle x_\alpha \rangle \to x \iff f(x_\alpha) \to f(x) \,\, \forall \, f \in X^*$ . Proof: $\implies$ Let $U$ be any ngh of $f(x)$ . Q1. Does this mean $V:=f^{-1}(U)$ is a ngh of $x$ ( in the weak topology? ). Assuming Q1 , we have $\exists \, \alpha_0$ such that $x_\alpha \in V \, \, \forall \, \alpha \geq \alpha_0$ . Hence $f(x_\alpha) \in U \, \, \forall \, \alpha \geq \alpha_0$ . Thus, $f(x_\alpha) \to f(x)$ . Q2. How can I go about the reverse implication?","['topological-vector-spaces', 'functional-analysis', 'weak-convergence', 'weak-topology']"
4641198,Find analytic function definition matching algorithmic one,"I am a Software Engineer who, within ~the last decade has had very little contact with more-than-primary-school-levels of math. To my chagrin it seems that I forgot not just how to do many things, I also forgot how they are called, so looking them up on my own has become annoyingly difficult. Apologies in advance for the likely gutted idiom. Here is a value table of a function $f(x, y) \to z$ with $x \in ([0, 10] \subset \mathbb{N}), z \in ([0, x] \subset \mathbb{N})$ , $y \in ([0, 10] \subset \mathbb{R})$ : $$
\begin{matrix}
\left.\mathrm{x}\middle\\\mathrm{y}\right. & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\
2 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 2 & 2 & 2 \\
3 & 0 & 0 & 0 & 1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 \\
4 & 0 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 \\
5 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 \\
6 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 5 & 6 \\
7 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 4 & 5 & 6 & 7 \\
8 & 0 & 0 & 1 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
9 & 0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
10 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10
\end{matrix}
$$ The table is filled by this algorithm: fill in $z$ from right to left starting with $x$ and decrementing by 1 until 0 expand $z$ from left to right such that no sequence of same $z$ is longer than the previous one, until the row is completely filled (I do not care about the table, other than as a tool for verification, and within this question, visualisation.) Now, ""gut feeling"" tells me there should be a ""traditionally"" defined function perfectly fitting this table, but I find that I can recall neither how to proceed with constructing one, nor the necessary vocabulary to properly describe the task. I am pretty sure with the right words it should be easy to find step-by-step tutorials by search machine, but trying queries like ""linear fitting"" do not produce results that... well, to be fair, that I can identify as being useful. E.g. here is the first Google hit I got for that query. ... Well, somehow I have here no random variables and no errors, nor am I trying to ""predict"" or ""forecast"" or ""explain"" anything? I also cannot quite imagine, what ""overfitting"" would even mean in this context. Long story short: That does not seem to be the right place to look in. ... Through ""intuition"" I have approximated this function as $f'(x, y) = \lfloor x \times \frac{y}{10} \rceil_{half \ down}$ , but that one behaves notably different: For lack of better words, the given value table ""prefers"" strictly lower $z$ whereas the approximation alternates between ""preferring"" and ""avoiding"" intervals within each row of $z$ . $f'$ value table for the same input values (differences in bold): $$
\begin{matrix}
\left.\mathrm{x}\middle\\\mathrm{y}\right. & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\
2 & 0 & 0 & 0 & \boldsymbol 1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 \\
3 & 0 & 0 & \boldsymbol 1 & 1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 \\
4 & 0 & 0 & \boldsymbol 1 & 1 & \boldsymbol 2 & 2 & 2 & 3 & 3 & 4 & 4 \\
5 & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 \\
6 & 0 & \boldsymbol 1 & 1 & \boldsymbol 2 & 2 & \boldsymbol 3 & \boldsymbol 4 & \boldsymbol 4 & \boldsymbol 5 & 5 & 6 \\
7 & 0 & \boldsymbol 1 & 1 & \boldsymbol 2 & \boldsymbol 3 & \boldsymbol 3 & \boldsymbol 4 & \boldsymbol 5 & \boldsymbol 6 & 6 & 7 \\
8 & 0 & \boldsymbol 1 & \boldsymbol 2 & \boldsymbol 2 & \boldsymbol 3 & \boldsymbol 4 & \boldsymbol 5 & \boldsymbol 6 & 6 & 7 & 8 \\
9 & 0 & \boldsymbol 1 & \boldsymbol 2 & \boldsymbol 3 & \boldsymbol 4 & 4 & 5 & 6 & 7 & 8 & 9 \\
10 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10
\end{matrix}
$$ I tried to play a bit with the rounding mode, or flooring instead of rounding, but did not find any further improvement through guesswork alone. All other attempts diverged harder than $f'$ . Questions: How can I create a better fit, besides ""wild guessing""? How can I know whether an inaccurate fit is ""the best I can get""? (E.g. if it just does not get any better than $f'$ , how would I know that I should stop searching?) - What vocabulary is used to describe what I am trying to do here? (Specifically if we regard the value tables as merely a visualisation tool and concentrate on the algorithmic vs. the analytic form of the function.)",['functions']
4641204,"Finding values $a \in M = (0, \infty)$ so that $(M\setminus\{a\}, \circ)$ is a group given $x \circ y = f(x)^{g(y)}$","Let $M = (0, \infty)$ . Find all real numbers $a > 0$ , such that there exist two functions $f : M \rightarrow M$ and $g : M \rightarrow \mathbb{R}$ such that $(M \setminus \{a\}, \circ)$ is a group, given that $x \circ y = f(x)^{g(y)}, \forall x, y \in M$ . Given that $(M \setminus \{a\}, \circ)$ is a group, I first attempted to find out what exactly happens once we apply the transformations $x \rightarrow 1_M$ and $y \rightarrow 1_M$ , where $1_M \in M$ is the identity element of the group. Therefore, we know that $x \circ 1_M = x, \forall x \in M$ and respectively $1_M \circ x = x, \forall x \in M$ . Additionally, we can use the fact that $x \circ 1_M = 1_M \circ x, \forall x \in M$ . Thus, we obtain: $x \circ 1_M = f(x)^{g(1_M)} \Leftrightarrow f(x)^{g(1_M)} = x, \forall x \in M$ (1) $1_M \circ x = f(1_M)^{g(x)} \Leftrightarrow f(1_M)^{g(x)} = x, \forall x \in M$ (2) (2) is particularly interesting because it resembles $e^{\ln x} = x$ . However, this is where I am stuck. I thought about taking the natural log of both sides but $g(M) \subset \mathbb{R}$ , so we might get $g(x) \leq 0$ for some $x \in M$ . Additionally, I saw that $f(x) = x$ and $g(x) = \ln(x)$ verify conditions (1) and (2), but I could not do more than that. I suspect $1_M \in \{1, e\}$ , but I cannot prove that either. Any hints would help me a lot, and I want to thank anyone who could shed some light on this problem for me!","['group-theory', 'abstract-algebra']"
4641244,"Limit of $(|x| + |y|)\ln(x^2 + y^4)$ at $(0,0)$","I want to show that $$\lim\limits_{(x,y) \to (0,0)} (\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4) = 0$$ First I let $\lVert (x,y) \rVert = \lvert x \rvert + \lvert y \rvert\ < \delta$ , and assume that $x,y < 1$ so $x^2 + y^4 < \lvert x \rvert + \lvert y \rvert$ . Then $\ln(x^2 + y^4) < \ln(\lvert x \rvert + \lvert y \rvert)$ . However, $\lvert \ln(x^2 + y^4)\rvert > \lvert \ln(\lvert x \rvert + \lvert y \rvert)\rvert$ , which is where I am stuck because I wanted to show that $\lvert(\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4)\rvert < \lvert x \rvert + \lvert y \rvert < \delta $ . It does not seem like this approach will work & I am not sure what else I can try.","['limits', 'multivariable-calculus', 'real-analysis']"
4641289,Does a weak solution to the geodesic equations coincide with a strong solution?,"Suppose $u^\alpha$ ( $\alpha = 0, 1, 2, 3$ ) is a weak solution to the geodesic equations $u^\alpha(t, x) \partial_\alpha u^\beta(t, x) = -\Gamma^\beta_{\mu\nu}u^\mu u^\nu$ . By this I mean that $u^\alpha \in C([0, T]; H^3(\mathbb{R}^3))$ and is continuous and solves the equation almost everywhere. Does $u^\alpha$ necessarily coincide with the geodesic vector field $\dot\gamma$ of the metric (which is, say, $C^1$ if the metric is $C^2$ )? Some more details: Here $u^\alpha(t, x)$ is a vector field on spacetime, which we take to be diffeomorphic to $\mathbb{R} \times \mathbb{R}^3$ . It is a weak solution in the sense that $$
\int_0^T \int_{\mathbb{R}^3} \Big( u^\alpha(t, x) \partial_\alpha u^\beta(t, x) + \Gamma^\beta_{\mu\nu}u^\mu u^\nu\Big)\psi(t, x)\, dx \, dt = 0
$$ for any $\psi \in C_c^\infty([0, T] \times \mathbb{R}^3; \mathbb{R})$ . The initial conditions are a vector field $X$ on $\{0\} \times \mathbb{R}^3$ . On the other hand, we can let $\gamma_x(\tau)$ be the geodesic which at affine parameter $\tau = 0$ is $x \in \mathbb{R}^3$ and has $\dot\gamma_x(0) = X_x$ . Assuming that the geodesics foliate the space nicely (which can be shown via an inverse function theorem), this gives a $C^1$ vector field $v(t, x)$ on spacetime which at $(t, x)$ is the tangent vector to the curve $\gamma$ passing through $(t, x)$ . Are these two vector fields equal?","['partial-differential-equations', 'differential-geometry']"
4641363,How can I solve the expected number of frog jumps problem?,"A frog sits on the real number line at 0. It makes repeated jumps of random
distance forward. For every jump, the frog advances by a random amount,
drawn (independently) from the uniform distribution over $U([0, 1])$ . The frog
stops once it has reached or surpassed 1. How many jumps does the frog make on average? What’s the standard
deviation? Here is my answer: Let $N$ be a random variable with possible values 2, 3, 4, ... which represents the number of jumps the frog makes immediately after it has reached or surpassed 1. We will neglect the possibility of only one jump being required. Let $X_1$ , $X_2$ , ... $X_n$ be a set of random variates taken from $U([0,1])$ . Let $$S_n = \sum_{i=1}^n X_i.$$ For $n\ge2$ , the probability that $N=n$ is given by $$
\begin{aligned}
P(N=n) &= P[(S_n \ge 1) \cap (S_{n-1}<1)] \\
&= P(S_n \ge 1)P(S_{n-1}<1).
\end{aligned}
$$ From the CDF of the Irwin-Hall distribution we know that $$P(S_n\le x)=\sum_{k=0}^n\frac{(-1)^k}{n!(n-k)!}(x-k)^n_+.$$ Hence, $$P(S_n\le 1)=\frac{1}{n!}.$$ Similarly, $$P(S_{n-1}\le 1)=\frac{1}{(n-1)!},$$ $$P(S_n > 1)=1 - \frac{1}{n!},$$ $$\implies P(N=n)=\frac{1}{(n-1)!} - \frac{1}{n!(n-1)!}.$$ Hence the expected value of $N$ (i.e. the average number of jumps) is given by (see WolframAlpha ), $$\begin{aligned}
E(N)&=\sum_{n=2}^\infty nP(N=n) \\
&=\sum_{n=2}^\infty \frac{n}{(n-1)!}\left(1-\frac{1}{n!}\right) \\
&= 2e - I_0(2) \\
&\approx 3.1570.
\end{aligned}$$ Let $\mu = E(N)$ . Now we need to calculate, $$E[(N-\mu)^2] = E(N^2) - \mu^2.$$ The first term is given by ( see WolframAlpha ): $$\begin{aligned}
E(N^2) &= \sum_{n=2}^\infty n^2P(N^2=n^2) \\
&= \sum_{n=2}^\infty n^2P(N=n) \\\
&=\sum_{n=2}^\infty \frac{n^2}{(n-1)!}\left(1-\frac{1}{n!}\right) \\
&=5e-I_0(2) - I_1(2)\\
&\approx 9.7212.
\end{aligned}$$ Hence the standard deviation, $\sigma$ is approximately (see WolframAlpha ), $$
\begin{aligned}
\sigma \approx 0.2185.
\end{aligned}
$$ However, when I check these results using the code below, it seems that $$\mu\approx 2.7222 \approx e?,$$ $$\sigma \approx 0.8752.$$ Can you see where I went wrong? import numpy as np

num_trials = int(1e5)
N = np.zeros(num_trials)
for n in range(num_trials):
    X = 0
    while X < 1:
        N[n] += 1
        X += np.random.uniform()

print(np.mean(N))
print(np.std(N))","['expected-value', 'statistics', 'standard-deviation', 'probability']"
4641422,If $a_n$ must be all $0$?,"Show that if a sequence ${a_n}$ satisfies $\sum_{n=1}^{\infty}{a_nn^k}=0$ , for all $k=1,2,...$ , then $a_n=0$ for all $n$ . From the series I can only get $\underset{n\rightarrow \infty}{\lim}a_nn^k=0$ , $\forall k
$ . I am confused that if $a_n=e^{-n}$ , then $\underset{n\rightarrow \infty}{\lim}a_nn^k=0$ , $\forall k
$ , so maybe there really exists a sequence $a_n$ not all $a_n=0$ , s.t. $\sum_{n=1}^{\infty}{a_nn^k}=0$ , $k=1,2,...$ . The $\sum_{n=1}^{\infty}{a_nn^k}=0$ can be regarded as an inner product of $(a_1,...,a_n,...)$ and $(1^k,2^k,...,n^k,...)$ . I want to use the Hilbert space $l_2$ to solve the question. As we have $(a_1,...,a_n,...)\in l^2
$ , but $(1^k,2^k,...,n^k,...)\notin l^2$ , so it's useless. Thank you for sharing your mind.","['analysis', 'sequences-and-series']"
4641441,Path integral of the connection $1$-form of the Hopf bundle,"Let $\mathcal H$ be a complex Hilbert space with scalar product $\langle \cdot,\cdot\rangle$ . Let us consider on $\mathcal H$ the $1$ -form defined for any $\psi\in \mathcal H$ by $$
\widetilde \omega_\psi\colon [\gamma]_\psi \in T_\psi \mathcal H\mapsto \Im\langle \psi,\gamma'(0)\rangle\in \mathbb R\,.
$$ Let's $\omega=\iota^\ast \widetilde \omega$ , where $\iota\colon \mathcal S\mathcal H\hookrightarrow \mathcal H$ is the canonical inclusion of the Hilbert sphere $\mathcal S\mathcal H=\{\psi\in \mathcal H\mid \langle \psi,\psi\rangle=1\}$ . Thus, $\omega$ is a $1$ -form on $\mathcal S\mathcal H$ . Let $P\colon [0,1]\to \mathbb P\mathcal H$ be a closed curve on the complex Hilbert space $\mathbb P\mathcal H$ . Let us assume that $P([0,1])\subseteq U_{P(0)}$ , where for any $\psi\in \mathcal S\mathcal H$ , $U_\psi=\Pi(\mathcal S\mathcal H\setminus \psi^\perp)$ is the domain of the coordinate chart $$
u_\psi \colon \chi\in U_\psi\mapsto \frac{\chi}{\langle \psi,\chi\rangle}-\psi\in \psi^\perp\,.
$$ Let $s$ be a local section of the projection $\Pi\colon \mathcal S\mathcal H\to \mathbb P\mathcal H$ defined on $U_\psi$ . Let us define $A=s^\ast \omega$ , so that $A$ is a $1$ -form on $U_\psi$ . Now I have trouble in calculating $$
\int_P A\,.
$$ My attempt First, $$ 
\int_P A=\int_P s^\ast\iota^\ast \widetilde \omega=\int_0^1 P^\ast s^\ast \iota^\ast \widetilde \omega=\int_0^1 (\iota\circ s\circ P)^\ast \widetilde \omega\,.
$$ Since $\widetilde \omega$ is not written in terms of differentials ( $\mathcal H$ can be infinite-dimensional), I don't know how to calculate the pullback, and so how to continue...","['principal-bundles', 'hopf-fibration', 'differential-forms', 'differential-geometry']"
4641473,Evaluate $\int \sqrt{\frac{1+x^2}{x^2-x^4}}dx$,"Evaluate $$\large{\int} \small{\sqrt{\frac{1+x^2}{x^2-x^4}} \space {\large{dx}}}$$ Note that this is a Q&A post and if you have another way of solving this problem, please do present your solution.","['integration', 'solution-verification', 'definite-integrals']"
4641476,Lebesgue Measure on $1$-dimensional Euclidean Space,"Let $E$ be a Lebesgue measurable set in $\mathbb{R}$ ,
with $m(E) < \infty$ ,
then there exists a compact subset $K$ of $E$ , s.t. $m(K)=\frac{1}{2}m(E)$ . My attempt: $\forall x \geq 0$ , define a function $$f(x)=m(E \cap [-x,x]) \; .$$ For any $0 \leq x < y$ , $$|f(x)-f(y)| \leq 2|y-x| \; ,$$ hence $f$ is continuous on $[0,\infty)$ , since $$
f(0)=0 \;, f(\infty)=m(E) \; ,
$$ then by Intermidiate Value Thm., there
exists $x_0 \in (0,\infty)$ , s.t. $$m(E \cap [-x_0,x_0])=f(x_0)=\frac{1}{2}m(E) \; .$$ Since $E \cap [-x_0,x_0]$ is measurable,
then for each $n \in \mathbb{N}$ , there exists compact subset $K_n$ of $E \cap [-x_0,x_0]$ , s.t. $$m((E \cap [-x_0,x_0])\setminus K_n) < \frac{1}{n} \; .$$ But we could not say $\lim_{n\rightarrow \infty}K_n$ is compact.
Then I got stuck here.","['measure-theory', 'lebesgue-measure']"
4641515,Find out limit of the following question,"let, $f(x) =x^\frac{1}{3}$ be a diffrentiable function on $ (0, \infty).$ Given that $$\frac{f(3+h) -f(3)}{h}=f'(3+\theta(h)h)$$ Then find out $\lim_{h\to 0+} \theta(h) =? $ Since, $f$ is diffrentiable at $3$ , I think limit must be $0$ as it tends to $f'(3)$ Please help me",['limits']
4641541,Does the dominant prime factor contribute about $62\%$ of the value of the logarithm of numbers?,"Let $n = p_1^{a_1}p_2^{a_2}\cdots p_k^{a_k}$ be the prime factorization of $n$ for some primes such that $p_i < p_{i+1}$ . We define the minor prime factor and the dominant prime factor of $n$ as the primes which has the smallest the largest contributions respectively to the value of $n$ . Clearly, the dominant and the minor prime factors of $n$ is not necessarily its largest and smallest prime factors. E.g.  if $n = 2^8 3^6 5^2 7^2$ then the largest and the smallest prime factors of $n$ are $7$ and $2$ respectively where as its dominant and the minor prime factors are $3$ and $5$ respectively since $3^6 = 729$ is greater than $2^8 = 256, 5^2=25$ and $7^2=49$ . Definition 1 : The minor prime factor of $n$ is defined as the prime factor $p_m$ such that $p_m^{a_m} \le p_i^{a_i}$ . Definition 2 : If a number as two or more distinct prime factors, then the dominant prime factor of $n$ is defined as the prime factor $p_d$ such that $p_d^{a_d} \ge p_i^{a_i}$ for any $i \ne d$ . Note : If a number has only one distinct prime factor then its minor and its dominant prime factors are the same i.e. $p_m = p_d$ . Since $\frac{a_1 \ln p_1}{\ln n} + \frac{a_2 \ln p_2}{\ln n} + \cdots + \frac{a_k \ln p_k}{\ln n} = 1$ we can say that each term in this sum is the contribution of the respective prime to the value of $\ln n$ and we can ask on and average what is the contribution of the minor/dominant prime to logarithm of a number. Let $p_d$ be the dominant prime factor of $n$ . Experimental data for all $n \le 4 \times 10^{8}$ and the average over several smaller intervals upto $10^{100}$ ; suggests that such that $$
\lim_{x \to \infty}\frac{1}{x}\sum_{n = 1}^x \frac{a_d\ln p_d}{\ln n} \sim 0.62
$$ Conjecture : The dominant prime factor contributes approximately $62\%$ of the value of the logarithm of natural numbers. Can this be proved or disproved? Update 24 Feb 2023 : Digging into literature, I read about the Golomb-Dickman Constant whose value $0.6243299$ is suspiciously close to observed mean value of the dominant prime factor. This constant is related to the largest prime factor (not the dominant prime factor) through the Dickman Function . SageMath Code: n      = 2
step   = target = 10^6
r_max1 = r_max2 = 0

while True:
    x = prime_factors(n)
    max1 = 1
    max2 = 1
    
    for factor in x:
        p = 1
        check = 1
        while True:
            check = check*factor
            if n%check == 0:
                p = p + 1
            else:
                p = p - 1
                check = check/factor
                if check > max1:
                    max1 = check
                    max2 = check
                break
    
    if len(x) == 1:
        max2 = 1

    # print(n,max)
    l     = ln(n).n()
    lmax1  = ln(max1).n()
    lmax2  = ln(max2).n()
    r_max1 = r_max1 + lmax1/l
    r_max2 = r_max2 + lmax2/l

    if n == target:
        print(n, 'lower_bound:', r_max2/n, 'upper_bound:', r_max1/n, 'mean:', (r_max2/n + r_max1/n)/2)
        target = target + step
    n = n + 1","['divisibility', 'elementary-number-theory', 'number-theory', 'limits', 'prime-numbers']"
4641543,Show that $\sqrt{2} + 2 \sqrt{3} + 2 \sqrt{5} + \sqrt{7} < 12$,"I've found this inequality, it seems cute: $$\sqrt{2} + 2 \sqrt{3} + 2 \sqrt{5} + \sqrt{7} < 12$$ Note: It checks with a calculator .  I am also interested in some methods that work for more general sum of radicals. Thank you for your interest! $\bf{Added:}$ I found this inequality by some random search. Note that all of the coefficients on LHS are positive, this can be an advantage. In the method of @heropup, one wants to check that a certain integer $N$ ( here $N=12$ ) is larger than a totally real algebraic number $\alpha$ that is  larger than all the other conjugates $\alpha_j$ .  Say its minimal polynomial is $f(x)$ .   Then we need to show that $N>$ all of the roots of $f(x)$ . Note that the roots of all derivatives of $f$ are also between the largest and the smallest of $\alpha_j$ 's. The above inequality is equivalent to $f(N)$ , $f'(N)$ , $f''(N)$ , $\ldots > 0$ .  With Taylor's formula, this is equivalent to:   the polynomial $f(N+t)$ has all of the coefficients positive in $t$ . This is an automated check that can be done instantly after we found $f(x)$ . This method does not work if $\alpha$ is not the largest ( or smallest) of all its conjugates. But there is a way to ""make it the larges"" using a transformation $x \mapsto 1/(a-x)$ . I will leave out the details. There was a previous question on this site about finding a general method for comparing arbitrary real algebraic numbers.  That seems like a quite involved problem.  It is easy to compare with $0$ a real algebraic number $\alpha$ with all its other conjugates non-real.  The test is simple $\alpha> 0$ if and only if $N(\alpha) > 0$ , where $N(\cdot)$ is the norm. In particular this applies to cubic fields with discriminant $<0$ .   The general problem of comparing with $0$ an algebraic number $\alpha$ is simple if $\alpha$ is the $k$ -th smallest ( or largest) root of an irreducible polynomial.  Indeed, one needs to see how many positive/negative roots does the polynomial have , something that can be decided ( in principle).  This is harder if one is given an expression involving several real algebraics. Even if we can find an equation whose root is the given expression, it could be harder to decide the order rank of the roots.  So this seems like a difficult problem. But perhaps it is solved somewhere in the literature. One other approach of @Parcly Taxel: simply gets rid of all the roots, little by little, something that I thought not possible just by counting.  That is interesting!  I am not fully convinced that it is so for an arbitrary number of roots as I was assured by Parcly... tempting to through another challenge to see how that works, what is the logic there... Another approach, of @Luke Collins, and @Claude Leibovici uses the approximation of square roots with their convergents. It turns out that the convergents are easy to form, since the continued fractions are periodic ( and in this case, quite simple). Summing up: some methods are ingenious and algebraic, other use some the minimal polynomial, other use continued fractions and convergents.  Perhaps I should mention that there are other way to approximate roots. Parcly seems to prefer power series.  I like Newton's method. Claude Leibovici might like Pade approximants too. I've learned a lot from All the answers! Thank you very much to All the contributors!","['algebra-precalculus', 'radicals', 'inequality']"
4641588,Expected “peripheral” points in the unit square,"Background Grimmett & Stirzaker’s Probability and Random Processes (4th ed. 2020), exercise 4.2.5, reads: Peripheral points. Let $P_i = (X_i, Y_i)$ , $1\le  i \le n$ , be independent, uniformly distributed points in the unit square $[0,1]^2$ . A point $P_i$ is called peripheral if, for all $r=1, 2, \dots, n$ , either $X_r\le X_i$ or $Y_r\le Y_i$ , or both. Show that the mean number of peripheral points is $n\left(\frac{3}{4}\right)^{n-1}$ . The proof they give is easy: Define an indicator function $I_i$ that is 1 if the point $P_i$ is peripheral. Then $\mathbb{E}(I_i) = \mathbb{P}(I_i=1) = \left(\frac{3}{4}\right)^{n-1}$ , and setting the number of peripheral points $X:=\sum I_i$ , the result follows from the linearity of expectation. Here is a plot of $\mathbb{E}(X)= n\left(\frac{3}{4}\right)^{n-1}$ : Question Notably, for $n\ge 9$ , we have $\mathbb{E}(X) = n\left(\frac{3}{4}\right)^{n-1} < 1$ . This is a contradiction to the following argument: Assume $n$ points placed in the unit square (never mind how they are distributed). Since the set of points is finite, there exist at least one point with a maximal X coordinate, and at least one point with a maximal Y coordinate. (These points might be the same.) Therefore the number of peripheral points is $X \ge 1$ always, and thus $\mathbb{E}(X)\ge 1$ . Put differently, if for $n\ge 9$ we have $\mathbb{E}(X) < 1$ then there should exist a configuration of points such that no point is peripheral. I can’t see how that is true? Any help clearing up my confusion would be greatly appreciated, thanks!","['expected-value', 'uniform-distribution', 'probability']"
4641666,Minimal presentation of non-abelian group of order $p^3$ and exponent $p^2$,"Let $p$ be an odd prime and let $H$ be the non-abelian $p$ -group of order $p^3$ and exponent $p^2$ . A presentation of $H$ is given by $$H=\langle a,b \mid a^{p^2}=1,b^p=1,[a,b]=a^p \rangle. $$ How do I find a presentation with $2$ generators and $2$ relations (if it exists)? I played around with the relations and couldn't see how any two relations imply the third and I tried using different genrators. Motivation: Let $G$ be a finite $p$ -group and let $d(G)$ be the minimal number of generators of $G$ . Let $R(G)$ and $r(G)$ be the minimal number of relations needed to present $G$ as an abstract and pro- $p$ group respectively. It is always true that $R(G)\geq r(G)$ and I think there is no known example for $G$ such that $R(G)>r(G)$ . Since in this case I know that $r(H)=2$ , I expect such a presentation to exist.","['group-presentation', 'finite-groups', 'combinatorial-group-theory', 'p-groups', 'group-theory']"
