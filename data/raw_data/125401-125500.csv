question_id,title,body,tags
1900161,"Piece-wise Function Differentable, and Continuous","I have this piece-wise function: $$f(x)=\begin{cases}
2x+3 & \text{x$\le4$} \\
x^2-5 &\text{x$\gt 4$}\end{cases}$$ I know the function is continuous at $x=4$ except how would I go finding out if it were differentable at that point? I tried by taking the derivative of the top portion and the bottom portion and got $$2$$
  $$2x$$ Then I plugged in $4$ for both and got $2$, and $8$.  Since $2 \ne 8$ I deduced that at $x=4$ the function is not differentable.  Is my ideology correct of am I wrong?","['derivatives', 'piecewise-continuity', 'calculus', 'limits']"
1900179,Why is there no natural isomorphism between $V$ and its dual?,"Question While looking over the exercise $3.F-34$ in Linear Algebra Done Right , I encountered the following paragraph Suppose $V$ is finite dimensional. Then $V$ and $V'$ are isomorphic, but finding an isomorphism from $V$ onto $V'$ , generally, requires choosing a basis of $V$ . In contrast, the isomorphism from $V$ to $V''$ does not require a choice of basis and thus is considered more natural . and these questions showed up in my mind: $1$ . Does the word natural just means that we don't need to choose a basis? I have seen the word canonical is used in the similar manner too. Is there a more precise definition for natural or canonical ? $2$ . Assuming the answer to question $1$ is Yes , then why there is no natural isomorphism from $V$ onto $V'$ ? $3$ . I think that there is a relation between the answer to question $2$ and the proof of Riesz representation theorem. So, if we cannot find a natural isomorphism between $V$ and $V'$ then it means that we cannot prove Riesz representation theorem without choosing a basis of $V$ . Is this true? Complementary Information The Isomorphism from $V$ onto $V^{''}$ . Suppose $V$ is a finite dimensional vector space. Consider the following map $$
\Lambda(v)(\phi)=\phi(v),  \qquad \forall v \in V, \,\, \forall \phi \in V^{'}
$$ then $\Lambda$ is an isomorphism from $V$ onto $V''$ . Riesz Representation Theorem . Suppose $V$ is a finite dimensional linear space equipped with an inner product and $\phi$ is a linear functional on $V$ . Then there is a unique vector $v_0 \in V$ such that $$\phi(v) = {\langle v,v_0 \rangle}_{V}, \qquad \forall v \in V$$ Other Related Posts I found the following posts related to this question on MSE and MO. Post $1$ , Post $2$ , Post $3$ , Post $4$ .","['dual-spaces', 'linear-algebra', 'duality-theorems']"
1900209,What is $\sum_{n=1}^{\infty}\left(\frac i6\right)^n$ where $i=\sqrt{-1}$?,"What is $\sum_{n=1}^{\infty}\left(\frac i6\right)^n$? where $i=\sqrt{-1}$ I want to evaluate the GP
$$\frac i6 + \left(\frac i6\right)^2+\cdots \infty$$ I am thinking about using the formula for an infinite GP in reals: $\frac{a}{1-r}$ This is true if $r\lt 1$. But the comparison $\frac i6 \lt 1$ is invalid. So, I went to the initial formula, sum $= \frac{a(1-r^n)}{1-r}$. Our above formula will be valid if $\lim_{n\to\infty}\left(\frac i6\right)^n$ is $0$. That's where I'm stuck. I am not sure how to evaluate this limit.","['sequences-and-series', 'limits']"
1900213,Dirichlet characters and quadratic fields,"Let $\chi$ be a Dirichlet character. This is, $$\chi: (\mathbb{Z}/m\mathbb{Z})^* \to S^1$$ I'm trying to understand the relation between those characters and fiel extensions. For example, the principal (trivial) character $\chi_0$ mod $1$ is naturally associated to $\mathbb{Q}$, in the sense that $L(s,\chi_0)=\zeta(s)$. In particular I'm interested in the following. Let $K$ be a quadratic extensions. We know that: $$\zeta_K(s)=\zeta(s)L(s,\chi_K)$$ Let's call quadratic characters to those who appear that way. There should be a bijection between quadratic characters and quadratic extensions (right?). Are all non-principal characters quadratic? But perhaps the most natural question is, Given a Dirichlet character $\chi$, how do we get a field extension
  $K$ from it? Is $K$ quadratic always? Is this process unique? Thanks for any information. I'd appreciate references also.","['number-theory', 'extension-field', 'algebraic-number-theory', 'characters']"
1900245,What should I study first to understand Lojasiewicz gradient inequality?,"I have read the paper ""Some application of the Lojasiewicz gradient inequality"" of Alain Haraux in the last days, and I found it really difficult for me from some first lines. For example, what is the definition for Lyapunov function that matches things referred in the article, since I've found several definitions for it on the internet. Here is the first page of the paper: I want to ask for some foundation knowledge to study first in order to prepare for reading this article. Thank you very much.","['ordinary-differential-equations', 'dynamical-systems', 'vector-analysis']"
1900260,"Is R an equivalence relation ? if yes, how? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $a$ is related to $b$ ($a\mathrel{R} b$) if and only $a\cdot b > 0$ over the set of non-zero rational numbers.  Is $R$ an equivalence relation? If yes how?",['elementary-set-theory']
1900290,Alternative characterization of functions satisfying this convexity-related condition?,"I'm wondering if there is any other characterization of the set of multivariate functions $f(x_1, ..., x_n) > 0$ such that, for any $i,$ $f$ is convex as a function of $x_i$ when $\{x_j\}_{j\neq i}$ are fixed at any values. Convex functions satisfy this condition, but it's a strictly weaker condition than convexity (see: Proof that a coordinate-wise convex function is convex? )? Products $\prod_{i=1}^n f_i(x_i)$ where each $f_i$ is convex and positive are in this set. Is this all? If there's some characterization without positivity constraints, I'd also be interested.","['convex-analysis', 'functions']"
1900297,"Diffeomorphism preserves interior, exterior and boundary","Let $g : A \to B$ be a diffeomorphism of open sets $A, B \subseteq \mathbb{R}^n$. That is, $g$ is a bijection and both $g,g^{-1}$ are of class $C^r$ for some $r \geq 1$. Let $S$ be a subset of $A$, and let $g(S) = T$. Is it true that $g(\text{Int }S) = \text{Int }T$, $g(\text{Ext }S \cap A) = \text{Ext }T \cap B$ and $g(\text{Bd }S) = \text{Bd }T$? [Here we define $\text{Int }S$ as the union of all open sets contained in $S$, $\text{Ext }S$ as the union of all open sets disjoint from $S$, $\text{Bd }S$ as the set of points whose every neighborhood contain points from $S$ and $\mathbb{R}^n-S$. These 3 sets are disjoint and their union is $\mathbb{R}^n$.] I think it's true, and believe I have a proof: Indeed $g^{-1}$ is continuous, and since $\text{Int }S$ is open,
  $g(\text{Int }S)$ is open, so $g(\text{Int }S) \subseteq \text{Int }T$.
  Similarly $g$ is continuous, so $g^{-1}(\text{Int }T) \subseteq \text{Int }S$. It follows that $g(\text{Int }S) = \text{Int }T$. Similarly $g(\text{Ext }S \cap A)$ is open, and $\text{Ext }S \cap A
\subseteq A - S$, so that $g(\text{Ext }S\cap A) \subseteq \text{Ext
}T \cap B$. Then $g(\text{Ext }S\cap A) = \text{Ext }T \cap B$. Finally $g(A) = B$ so that $g(\text{Int }S \cup \text{Bd }S \cup (\text{Ext }S\cap
A)) = \text{Int }T \cup \text{Bd }T \cup (\text{Ext }T\cap B)$. Since $g$ is a bijection, it follows that $g(\text{Bd }S) = \text{Bd }T$. But I'm having doubts because Munkres' Analysis on Manifolds requires $S$ to be compact (Theorem 18.2, page 154). Is the statement true, or is there a flaw with the above proof?","['real-analysis', 'manifolds', 'general-topology', 'differential-geometry', 'analysis']"
1900326,find integers a and b such $x^2-x-1$ divides $ax^{17}+bx^{16}+1 = 0$,"find integers a and b  such $x^2-x-1$ divides  $ax^{17}+bx^{16}+1  = 0$ By really long division i got :- $$Q=ax^{15} + (a+b)x^{14} + \dots +(610a + 377b) $$
$$R = x(987a+610b)+1+610a+377b$$ since remainder is $0 $,
$$987a+610b = 0$$
$$1+610a+377b = 0$$ from which i got $a = -610, b = 987$ but from wolfram alpha the remainder of $-610x^{17}+987x^{16}+1  \over x^2-x-1$ is $x-1$ Somebody please show me where i went wrong ? Thanks.","['algebra-precalculus', 'fibonacci-numbers']"
1900328,"Intuition for Integrating ""Against a Test Function"" in Distribution Theory","I know that we can consider (locally integrable) functions and measures to be distributions via the relationships
$$
\langle T_{f},\varphi\rangle=\int_{\mathbb{R}} f(x)\varphi(x) \, d\mu(x)
$$
and
$$
\langle T_{\mu},\varphi\rangle=\int_{\mathbb{R}} \varphi(x) \, d\mu(x)
$$
and the fact that
$$
\langle T_{f},\varphi \rangle = \langle T_{g} , \varphi\rangle \implies f=g \, a.e.
$$
but I'm not completely sure how by integrating against a test function that we can recover the pointwise values of $f$ or $\mu$ (at least a.e.). My understanding is that the value of the distribution $T_{f}$ depend on the test function that it is being evaluated at and the values of the distribution determine the pointwise values of $f$ almost everywhere since evaluating the function at a point would involve a integrating against the limit of a sequence of test functions whose limit is not a test function (i.e. the Dirac distribution). My intuition tells me that the test function just serves as some sort of ""smooth approximation to an indicator function"" and then you can dilate the test function in the same manner you might change the support of an indicator function. Am I on the right track or am I missing something?","['functional-analysis', 'distribution-theory']"
1900341,"What is an example of a field, that is not a sigma-field?","What is an example of a field, that is not a $\sigma$-field? I read a $\sigma$-field requires closure of it's elements (which are sets) under countable unions, countable intersection, and complement; and a field only requires closure under finite unions, finite intersections, and complement.",['measure-theory']
1900349,Generalising Parseval's Identity using the Convolution Theorem,"Suppose that we have a $2\pi$-periodic, integrable function $f: \mathbb{R} \rightarrow \mathbb{R}$, whose Fourier coefficients are known. Parseval's identity tells us that: $$\displaystyle \frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^{2}dx =  \sum_{n = -\infty}^{\infty}|\widehat{f(n)}|^2,$$ where $\widehat{f(n)}$ are the Fourier coefficients of $f$. Suppose we instead want to replace $f(x)$ with $f(x)^{q}$, say: then it would suffice to determine the Fourier coefficients of the $q$-th power of $f$. Is repeated application of the convolution theorem the usual (or, most efficient) way of finding powers of the Fourier coefficients of functions, where the Fourier coefficients of the original function are already known? Moreover, can Parseval's identity be extended in this way, by replacing $f$ with a power of $f$ instead? For example, suppose that we are interested in the following integral: $$\displaystyle \int_{-\pi}^{\pi}|f(x)|^{4} dx.$$ I would like to know if it is valid to say the following: $$\displaystyle \frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^{4}dx = \frac{1}{2\pi}\int_{-\pi}^{\pi}|(f(x))^{2}|^{2}dx = \sum_{n = -\infty}^{\infty} |\widehat{f(n)^{2}}|^{2} = \sum_{n = -\infty}^{\infty} | (\hat{f} \ast \hat{f})(n)|^{2},$$ where $f \ast g$ denotes the convolution of $f$ and $g$, given by $(f \ast g)(t) := \int_{-\infty}^{\infty} f(\tau)g(t - \tau)d\tau,$ and $\widehat{f \ast g} = \hat{f} \cdot \hat{g}$ is the convolution theorem for the Fourier transforms of $f$ and $g$. Is this manipulation valid?","['functional-analysis', 'real-analysis', 'fourier-analysis']"
1900375,Is it possible that two independent variables become dependent conditioning on a third random variable,"Is that possible that random variables $X$ and $Y$ are independent but they are no longer independent if condition on another random variable $Z$? Is there a mathematical example and an approximate real life example for it? What does it mean intuitively that two random variables are independent but condition on another random variable, these two old random variables suddenly have relation? I was thinking about the following real life example. 
Say a doctor wants to know if a new medicine is effective of reduce the blood pressure of the population (for all men and women). Let $Y_{1i}$ be the random variable represents the distribution of the blood pressure of the population if they all take the medicine and $Y_{0i}$ be the random variable represents the distribution of the blood pressure of the population if they all did not take the medicine. Let $D_i$ be the random outcome of the $i^{th}$ coin flip. So if it is head, then subject $i$ takes the medicine and if it is tail, then subject $i$ drinks pure water. So in this case, $D_i$ is independent of $Y_{1i}$, i.e., learning the value of $D_i$ does not help you to know better about the distribution of $Y_{1i}$. But now, what if I tell you subject $i$ is male, then condition on this information, will $D_i$ and $Y_{1i}$ be independent?","['independence', 'probability-theory', 'probability']"
1900414,25th derivation of $\cos{x^3}$,"I have to calculate 25th derivative of function $f(x)=\cos{x^3}$ in $0$, $f^{(25)} (0)$. In my college, we usually use Newton-Leibnitz rule. We usually derivate it couple of times and then get something like $f^{(4)} = f^{(2)}x^2 + f^{(0)}$. This is not from this task, i am just giving you example.","['derivatives', 'real-analysis']"
1900419,Chance versus Skill,"Question. How does one mathematically analyze situations that involve chance and skill? Let's take the coin flip as a simple example. Assume that it possible to skillfully flip a coin to get the landing you want. Also assume zero cheating. FIRST SCENARIO The world's 5 most talented coin flippers gather to compete. The results: Person 1 Coin Flips : 100 Success Rate : 100% Person 2 Coin Flips : 10,000 Success Rate : 90% Person 3 Coin Flips : 1,000,000 Success Rate : 80% Person 4 Coin Flips : 100,000,000 Success Rate : 70% Person 5 Coin Flips : 10,000,000,000 Success Rate : 60% Each person claims he is the best coin flipper. How would you analyze the results? SECOND SCENARIO A man claims he is so skilled at coin flipping, he can always land heads. He flips one coin. Sure enough, heads. He flips again. Heads again. He flips 100 times. All heads. 1000 times. Still all heads. After 100,000,000,000,000 flips, every single one heads, he stops and
says ""I told you so."" When do we go from thinking ""He's lucky !"" to ""He's good !""?","['mathematical-modeling', 'statistics', 'probability', 'soft-question']"
1900420,"Is a complex determinant still a ""volume""? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I like the interpretation of the determinant of an $m\times n$ matrix as the volume of the $n$-dimensional parallelepiped whose sides are the columns of the matrix. Does this interpretation hold also for a complex matrix? If so what's the meaning of a complex volume?","['matrices', 'complex-numbers', 'determinant']"
1900432,Constant coefficient PDEs have infinite dimensional kernels?,"I'm looking for exactly what's in the title: a proof of the fact that constant coefficient linear PDEs have infinite dimensional solution spaces (as a subspace of the smooth functions). This should be simple but for some reason I'm not seeing it. Obviously this follows from the $d=2$ case, and I wanted to somehow change variables in such a way that I end up with only one variable (because $d^n/dx^n$ clearly has an infinite dimensional kernel - at least all functions of only $y$) but I'm not really sure what that means. Edit: I think there should be some sort of correspondence between points in the zero set of the characteristic polynomial and solutions to the equation, but I have no idea how to formalize this.","['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1900437,Proof that the dimension of a matrix row space is equal to the dimension of its column space,"I have the following theorem: Theorem 3.12. Let A be an m  n matrix. Then the dimension of its row
space is equal to the dimension of its column space. And the following proof is given: Proof. Suppose that $\lbrace v_1,v_2,\dots,v_k\rbrace$ is a basis for the column space of $A$.  Then each column of $A$ can be expressed as a linear combination of these vectors; suppose that the $i$-th column $c_i$ is given by $$c_i = \gamma_{1i}v_1+\gamma_{2i}v_2+\dots+\gamma_{ki}v_k$$ Now form two matrices as follows: $B$ is an $m\times k$ matrix whose columns are the basis vectors $v_i$, while $C=(\gamma_{ij})$ is a $k\times n$ matrix whose $i$-th column contains the coefficients $\gamma_{1i},\gamma_{2i,}\dots,\gamma_{ki}$.  It then follows$^7$ that $A=BC$. However, we can also view the product $A= BC$ as expressing the rows of $A$ as a linear combination of the rows of $C$ with the $i$-th row of $B$ giving the coefficients for the linear combination that determines the $i$-th row of $A$.  Therefore, the rows of $C$ are a spanning set for the row space of $A$, and so the dimension of the row space of $A$ is at most $k$.  We conclude that: $$\dim(\operatorname{rowsp}(A))\leq\dim(\operatorname{colsp}(A))$$
  Applying the same argument to $A^t$, we conclude that:$$\dim(\operatorname{colsp}(A))\leq\dim(\operatorname{rowsp}(A))$$and hence these values are equal However, I am finding this proof impossible to follow and understand. Can someone please offer an alternative proof or explain what this proof is saying? Thank you.",['linear-algebra']
1900454,"Meaning of bounds in $ \iint_{0\leq x^2+y^2\leq 4} \, dx\, dy $","If the bounds are written as
$$
\iint_{0\leq x^2+y^2\leq 4} \, dx\, dy
$$
What does it mean? I want the bounds written as
$$
\int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \, dx\,dy
$$
Same for a triple integral, if
$$
\iiint_{2\leq x^2+y^2+z^2 \leq 6} \, dx\,dy\,dz
$$
What is $
\int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \int_{z_\text{lower}}^{z_\text{upper}} \, dx\,dy \,dz
$? Thanks!","['multivariable-calculus', 'calculus']"
1900460,Why do complex functions have derivatives?,"I'm sorry if I sound too ignorant, I don't have a high level of knowledge in math. The function $f(z)=z^2$ (where $z$ is a complex number) has a derivative equal to $2z$. I'm really confused about this. If we define the derivative of $f(z)$ as the limit as $h$ approaches $0$ (being $h$ a complex number) of $(f(z+h)-f(z))/h$, then clearly the derivative is $2z$, but what does this derivative represent?? Also, shouldn't we be able to represent a complex function in 4-dimensional space, since our input and output have 2 variables each ($z=x+iy$) and then we could take directional derivatives...right? But if we define the derivative as above, it would be the same if we approach it from all directions. That's what's bothering me so much. I would really appreciate any explanation. Thanks!","['derivatives', 'intuition', 'complex-numbers', 'partial-derivative', 'complex-analysis']"
1900495,"If a surface, M, has $\pi_1(M)=\mathbb{Z}\ast…\ast \mathbb{Z}$, is M a finitely punctured closed surface?","Let $M$ be a connected topological 2-manifold.  I.e. a Hausdorff space locally homeomorphic to $\mathbb{R}^2$.  Suppose $\pi_1(M)$ is a finitely generated free group.  Must $M$ be homeomorphic to a closed surface with finitely many punctures? I don't think it matters, but I actually only care about orientable $M$. This is inspired by the answer to this question: Can two different topological spaces cover each other?","['manifolds', 'geometric-topology', 'general-topology', 'low-dimensional-topology']"
1900536,How to use Roberto Frucht Formula to find permutations with limited repitition,"I asked a question at this Number of arrangements of red, blue, and green balls in which a maximum of three balls of the same color are placed in the five slots . Now the scenario is the same I have a bag of three balls (look at the link) red blue green and I have 5 slots.. Every time I pull a ball from the bag I placed it in a slot. I keep randomly pulling balls from the bag and fill five slots..when all five slots are filled, repeat the process I still want to number of arrangements of red, blue, and green balls in which a maximum of three balls of the same color are placed in the five slots I found an interesting article by Roberto Frucht (1966), Permutations with limited repetitions that derives a formula to do this. I want to find the results using this breakthrough formula: for ""the number of $r$-permutations (called variations, r at a time in the older literature) with limited repetition, where each one of the $n$ different things to be permuted may appear at most $s$ times."" Note: In the professor's research he gives an example that shows how many permutations can be computed if 4 items are chosen from 7 items where only a maximum of two items and repeat itself in the arrangement. Note the formula shows that Bell polynomials and Stirling Strings of the second kind play a significant role in deriving the formula. I am requesting some practical help as to how to use this formula.Specifically, plugging values in and getting the expected results.","['combinatorics', 'statistics']"
1900566,Universal Family of a Fine Moduli Space,"I'm reading up on fine moduli spaces and I'm having difficulty seeing how every family over a scheme $B$ is the pullback of the universal family along a unique morphism. In fact, I'm not sure what this means. To make my question more precise, I'll use the notation of Harris and Morrison, If $F$ is a moduli functor representable by a scheme $M$, let $\Psi: Mor(-,M) \to F$ be the corresponding natural isomorphism. Pulling back the identity on $M$, $1_M$, we get a family in $F(M)$, $\mathbf{1}: U\to M$. Let $\phi: D\to B$ be a family in $F(B)$. How does one realize this family as the pullback of $U$ via $\Psi(\phi)$? Furthermore, I've seen the claim that $D\cong B\times_M U$. Harris and Morrison claim that there is a fibre product diagram $\begin{array}{ccc} D &\rightarrow& U \\ \phi \downarrow && \downarrow \mathbf{1} \\ B&\xrightarrow{\Psi(\phi)}& M\end{array}$ but what is the top morphism and why is this a fibre product?","['moduli-space', 'algebraic-geometry']"
1900571,using measure theory in a proof of invariance of domain,"https://terrytao.wordpress.com/2011/06/13 Terence Tao, in the blog cited above, derives invariance of domain
from the fixed point theorem
without reliance on algebraic-topology. He does, however, appeal to measure theory
for a result that I am unable to replicate. If I understand him he asserts: If $P$ is a polynomial in $
\mathbb{R}^n$ then there exists an $y\in \mathbb{R}^n$ and a $\delta > 0$
such that $P(x+\epsilon y)\neq 0$ for $x\in S^{n-1}$ and $0 < \epsilon < \delta$. He says this follows from the fact that $S^{n-1}$ and $P(S^{n-1})$ both
have measure $0$ in $\mathbb{R}^n$.
Do I interpret him correctly? His assertions sound plausible but I need a hint or two to
make a convincing argument of my own. Is it true that an infinitesimal displacement is
sufficient to make 2 sets of measure 0 disjoint?",['measure-theory']
1900578,Combinatorial proofs of the following identities,"I've been trying to find combinatorial proofs of the following two identities: 1: $\displaystyle\sum_{i=0}^{k} \binom{n}{i} = \sum_{i=0}^{k} \binom{n-1-i}{k-i} 2^i$ with $0 \le k \le n-1$ 2: $\displaystyle\binom{2m}{2n} = \sum_{k=0}^{n} \binom{2n+1}{2k+1} \binom{m+k}{2n}$ For 1: The LHS is counting the number of subsets of size at most k from a set of size n.  The $2^i$ in the RHS makes me think of partitioning based on what elements can be considered from the full set then either including them or not, but I can't think of a way of doing this partition without overcounting and trying to interpret the binomial term hasn't helped. For 2: Again, the LHS is simple enough but I'm lost on how to interpret the RHS.  Just from looking at it I feel like I should be considering some parity argument but haven't come up with anything else. Any suggestions on how to proceed? Should I be looking for a more formal bijection?","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
1900588,Two questions on homotopy sequence for étale fundamental groups,"I have two questions regarding the homotopy exact sequence associated to étale fundamental groups. Firstly, let me set things up (I'm following Szamuely's book , namely Proposition 5.6.1 and Remark 5.6.3.2). Let $X$ be a quasi-compact, geometrically integral scheme over a field $k$ [we might as well assume, for relevance to my questions here, that $k$ is finitely generated over $\mathbb{Q}$]. Fix an algebraic closure $\bar{k}$ of $k$ [hence, under our additional assumption, $\bar{k}$ is also a separable closure $k_s$ of $k$] and let $\bar{X} = X_{\bar{k}}$ denote the base change of $X$ by $\text{Spec}(\bar{k})\to\text{Spec}(k)$. Let $\bar{x}:\text{Spec}(\bar{k})\to\bar{X}$ denote a geometric point of $\bar{X}$ and let 
$$x : \text{Spec}(\bar{k})\xrightarrow{\bar{x}}\bar{X}\xrightarrow{p_X} X$$
be the geometric point of $X$ obtained from $\bar{x}$ via the natural morphism. The homotopy exact sequence of étale fundamental groups (with respect to these basepoints) is an exact sequence of étale fundamental groups
$$1\to\pi_1 (\bar{X},\bar{x})\to\pi_1(X,x)\to \text{Gal}(\bar{k}/k)\to 1$$
where the homomorphisms between fundamental groups are induced by the natural maps $\bar{X}\to X\to\text{Spec}(k)$. 1) My first question concerns the proof of exactness of this sequence (specifically, exactness in the middle), although it is really just a geometrical question and not directly related to fundamental groups. In the proof of Proposition 5.6.1 Szamuely obtains two finite étale Galois covers $Y\to X$ and $X_L\to X$ (where $L/k$ is a finite Galois extension) having the same function field. Thus $X_L$ and $Y$ are isomorphic over the generic point of $X$, so there is a dense open $U\subseteq X$ so that $X_L \times_X U\cong Y\times_X U$. Szamuely then says that local freeness of the covers $Y\to X$, $X_L\to X$ (being étale) implies that $Y\cong X_L$ i.e. that this isomorphism extends over the whole of $X$. I don't understand how local freeness is used to obtain this isomorphism. Could somebody explain this? 2) My second question concerns the role of the basepoint in Remark 5.6.3.2, on the Section Conjecture. Recall any $k$-point of $X$ can be described as a section $$y: \text{Spec}(k)\to X$$
of the structural morphism $t:X\to\text{Spec}(k)$. A choice of algebraic closure then composes to give a geometric point $$\bar{y}:\text{Spec}(\bar{k})\to\text{Spec}(k)\to X.$$
Let $\sigma_y : \text{Gal}(\bar{k}/k) \to \pi_1 (X,\bar{y})$ denote the induced homomorphism on fundamental groups. Then $\sigma_y$ isn't quite a splitting of the homotopy sequence because $\pi_1 (X,\bar{y})$ and $\pi_1 (X,x)$ are, technically, different groups. However, they are isomorphic via
$$\lambda : \pi_1 (X,\bar{y})\to\pi_1 (X,x)$$
which is unique up to conjugation by an element of $\pi_1 (X,x)$. Szamuely then claims that $\lambda \circ\sigma_y$ is a section of the homomorphism $t_*:\pi_1 (X,x)\to \text{Gal}(\bar{k}/k)$. I don't understand why this is the case and why $t_* \circ\lambda\circ\sigma_y$ is not an arbitrary automorphism of $\text{Gal}(\bar{k}/k)$. The question boils down to whether the isomorphism $\lambda$ induced by an étale path (an isomorphism of the fibre functors $F_{\bar{y}}\xrightarrow{\sim}F_x$) is compatible with the structural morphism $t:X\to\text{Spec}(k)$. i.e. let $t'_*:\pi_1 (X,\bar{y})\to \text{Gal}(\bar{k}/k)$ denote the induced homomorphism on fundamental groups with basepoint $\bar{y}$. Then is $t'_* = t_* \circ\lambda$?","['galois-theory', 'algebraic-geometry', 'etale-cohomology', 'schemes', 'fundamental-groups']"
1900594,"Find coefficients of polynomial, knowing its roots are consecutive integers","In the function $f(x)= x^3-15x^2+ax+b$ the graph has $3$ consecutive points where it crosses the x-axis. 
These $3$ points are consecutive integers. Find $a$ and $b$ for this is you know that $a$ and $b$ are real numbers. How do I start to find the answer?","['algebra-precalculus', 'roots', 'polynomials']"
1900599,Motivation of countable additivity in probability theory,"I looked at the lecture note saying that when $\Omega$, a sample space, is uncountably infinite, then we would have problem without countable additivity axiom. Can you please give me an example? Also, I looked at the extension theorem (Thr 3.1 of Billingsley, P. Probability and Measure). It states that for a probability measure, $P$, a field $F$ of subsets of $\Omega$, then there exists a probability measure $Q$ on $\sigma({F})$, generated $\sigma$-fields by $F$ such that $Q(A) = P(A), \forall A\in F$. Suppose $\Omega$ is an uncoutable infinite sets, then how one can have probability measure, $P$ in the first place?",['probability-theory']
1900620,Gram-Schmidt process in Minkowski space $\Bbb L^n$.,"I'm trying to prove a version of Gram-Schmidt orthogonalization process in Minkowski space $\Bbb L^n$ (for concreteness, I'll put the sign last). I am not interested in the existence of orthonormal bases, but instead in the algorithm. Namely, suppose that $\{v_1,\cdots,v_k\}\subseteq\Bbb L^n$ is a linearly independent set which does not contain any lightlike vectors, and whose span is non-degenerate. I'd try to mimic the usual proof by induction. If $k=1$, take $u_1 = v_1$, done. And if I assume $\{u_1,\cdots,u_k\}$ constructed, I'd define $$u_{k+1} = v_{k+1} - \sum_{j=1}^n\frac{\langle v_{k+1},u_j\rangle}{\langle u_j,u_j\rangle}u_j = v_{k+1} - \sum_{j=1}^n\epsilon_j\frac{\langle v_{k+1},u_j\rangle}{\|u_j\|^2}u_j,$$which is orthogonal to the previous $u_i$'s. But: One of the $u_i$'s could be lightlike and the construction would stop there. I'm not using (as far as I can see) non-degenerability of the span of the initial vectors. Also, I tried applying the GS process to the plane $y=z$ in $\Bbb L^3$, starting with a basis with no lightlike vectors... it produced a freaking lightlike vector (and gave me an orthogonal basis, hooray!). I mean... it's no surprise a lightlike vector came up, assuming the GS process works here... but why should it? I'm terribly lost. Can someone help me state the result correctly and maybe give me a little push on the proof? Thanks.","['semi-riemannian-geometry', 'bilinear-form', 'gram-schmidt', 'orthogonality', 'linear-algebra']"
1900625,The probability that a randomly selected integer is squarefree,What does the probability that a randomly selected integer is squarefree mean (in this context )?,"['number-theory', 'riemann-zeta']"
1900643,Jordan Normal Form - Number of Ones on Superdiagonal,"I was reading notes on Jordan Normal Form and it says that for a given matrix $A$, the number of ones on the super-diagonal of its associated Jordan matrix is equal to $n-d$, however they seem to assume an implied meaning of $n$ and $d$ which I can't seem to figure out. If you scroll to the bottom, where they mention it, they use $d$ once saying that the Jordan Matrix is of the form
$$\left(\begin{array}{cccc}
J_1 & 0 & \cdots & 0 \\
0 & J_2 & \cdots & 0 \\
\vdots & \vdots &  \ddots&\vdots \\
0 & 0 & \cdots & J_d \\
\end{array}\right)$$
appearing to refer to the number of Jordan blocks. They also use $n$ on the bullet above referring to the number of distinct eigenvalues of $A$. Is this interpretation of $n$ and $d$ most likely what they meant (i.e. does $n-d$ give the number of ones on the superdiagonal)? If so can anyone explain why?","['matrices', 'jordan-normal-form', 'linear-algebra']"
1900704,"What is $\int_0^1 \frac{\log \left(1-x^2\right) \sin ^{-1}(x)^2}{x^2} \, dx$?","I encountered the integral $$\int_0^1 \frac{\log \left(1-x^2\right) \sin ^{-1}(x)^2}{x^2} \, dx = -0.9393323982...$$ while researching the evaluation of harmonic sums. Mathematica 11 is not able to evaluate this integral, and is not able to evaluate the underlying indefinite integral. I considered using the Maclaurin series for the expression $\sin ^{-1}(x)^2$ to evaluate this integral. Using this Maclaurin series, it is easily seen that the problem of evaluating the above integral is equivalent to the problem of evaluating the series $$\sum _{n=1}^{\infty } \frac{2^{2 n-1} H_{n-\frac{1}{2}}}{(1-2 n) n^2 \binom{2 n}{n}}=-0.9393323982...$$ which Mathematica 11 is not able to evaluate directly. I also considered using the Maclaurin series for the expression $\log \left(1-x^2\right)$ to evaluate the above integral. Using this Macluarin series, it is easily seen that this integral is equal to $$\frac{1}{4} \left(16 \pi  C-\pi ^3-\pi ^2 \log (4)\right)+\sum _{n=1}^{\infty } \frac{\, _3F_2\left(\frac{1}{2},\frac{1}{2},1;\frac{3}{2},n+1;1\right)}{n^2-2 n^3}=-0.939332...,$$ but Mathematica is unable to evaluate the above infinite series. What is a closed-form evaluation of $\int_0^1 \frac{\log \left(1-x^2\right) \sin ^{-1}(x)^2}{x^2} \, dx$? What techniques can be applied to evaluate this definite integral?","['logarithms', 'integration', 'definite-integrals', 'closed-form']"
1900714,Integral of $\int_0^\infty \frac{(\ln{x})^2}{1+x^2}dx$,"I am trying to teach myself the residue theorem, and one of the problems I am looking at is $$\int_0^\infty \frac{(\ln{x})^2}{1+x^2}dx$$ With a branch point at $0$, and a branch cut extending down the negative imaginary axis, this leads to a contour integral (containing a poles at $\pm i$), looking like an upside down U. In turn, this contour integral can be broken up into four smaller integrals. The bottom two integrals (running along the real axis) are used to ""solve"" the problem by setting them equal to the residue (times appropriate prefactor). However, the integrals on the inside and outside curved portions of the ""U"" can be eliminated, as they are equal to $0$. I do not know how to calculate the integral of these two curves, and as such cannot see why they are $0$. The solutions I am looking at simply state ""via the l'Hospital's rule"" without any calculations shown. Disregarding the backstory, below is the integral I am trying to solve, which is the inside of the U shape. $\lim \limits_{r \to 0}\int_\pi^0 \frac{(\ln{z})^2}{1+z^2} d\theta$, where $z$ is the complex number represented by $z = re^{i\theta}$","['limits', 'residue-calculus', 'complex-analysis', 'integration', 'contour-integration']"
1900725,the residue of the function $e^{-e^{1/z}}$?,"the residue of the function $e^{-e^{-1/z}}$ since we can write $\displaystyle e^{- \{1+\frac{1}{z}+\frac{1}{2!Z^2}+\frac{1}{3!z^3}+......\}}$
this means that 0 is the pole
how we going to processed for further step",['complex-analysis']
1900728,Degree of map $f \colon M \to N$ from compact manifold to noncompact manifold is $0$,"The following is adapted from my professor's summary notes on Differential Geometry: Let $M$ and $N$ be two oriented connected manifolds of the same dimension $m$ , and let $f \colon M \to N$ be a proper map.  Let $I \colon H_{\mathrm{c}}^m(N) \to \mathbf{R}$ be the isomorphism given by integration from the $m^{\mathrm{th}}$ -degree compactly supported de Rahm cohomology space of $N$ to the real numbers, and similarly consider $I \colon H_{\mathrm{c}}^m(M) \to \mathbf{R}$ , as well as the map on cohomology $H_\mathrm{c}^m(f) \colon H_\mathrm{c}^m(N) \to H_\mathrm{c}^m(M)$ induced by $f$ . Then the linear map $I \circ H_\mathrm{c}^m(f) \circ I^{-1} \colon \mathbf{R} \to \mathbf{R}$ is multiplication by a real number $\mathrm{deg}(f)$ which we call the cohomological degree of the proper map $f \colon M \to N$ . .... He then continues to define the geometric degree $\mathrm{deg}_y(f)$ of $f$ at a regular value $y \in N$ in the usual way as the sum of local intersection numbers.  He then states: Theorem. The geometric degree $\mathrm{deg}_y(f)$ of $f \colon M \to N$ at each regular value $y \in N$ is equal to the cohomological degree $\mathrm{deg}(f)$ . ....[some corollaries] Corollary. If $M$ is compact and $N$ is non-compact, the degree $\mathrm{deg}(f)$ is $0$ . Again, these notes have statements only, no proofs. Could someone help me understand why the corollary is true?  I really have no idea.  Thanks.","['integration', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
1900769,Prove that G is an abelian group given $a^2b=b=ba^2$,"Let G be a semigroup such that $\forall a,b \in$ G , $a^2b=b=ba^2$.  Prove that G is an abelian group. My attempt at the problem. Fix $a_0 \in G$, then for all $b \in G$, $a_0^2b=ba_0^2=b$.  We let $e =a_0^2$  (that is $\forall b \in G$, $eb=be=b$) Now let $a \in G$, then $a^2e=e$, but we also have from above that $a^2e=a^2$.   Therefore $a^2=e$ for all $a \in G$.  Thus $e$ is the identity element of G.  Furthermore for any $a \in G$, we have $a^{-1}=a$ since $a^2=e$.  Hence G is a group.  To see that G is abelian, let $a,b \in G$.  Consider the following: $ab =(ba)^2(ab)=(ba)(ba)(ab)=(ba)ba^2b=(ba)beb=(ba)b^2=(ba)e=ba$ Hence G is an abelian group. Is this okay?  Thank you in advance.","['abstract-algebra', 'group-theory']"
1900803,Convergence of a power series and interchanging order of summation and integration.,"If we have the expression $$f(x) = \frac{1}{(2\pi i)^n} \int_{\gamma_1} \int_{\gamma_2} \cdots \int_{\gamma_n} f(z) \prod_{j=1}^n \left( \sum_{i=0}^{\infty} \frac{x_j^i}{z_j^{i+1}} \right)dz_j.$$ How would I show that the series $\prod_{j=1}^{n} \sum_{i=0}^{\infty} \frac{x_j^i}{z_j^{i+1}}dz_j$ converges absolutely, such that I can write $f(x)$ as $$f(x) = \frac{1}{(2\pi i)^n} \sum_{i=0}^{\infty} \prod_{j=1}^n \int_{\gamma_j} \frac{f(z)}{z_j^{i+1}}dz_j?$$ Note that this is a small argument that makes up a larger proof that I'm working on.","['complex-analysis', 'sequences-and-series']"
1900807,Geodesics and covering maps,"I know of the following proposition: Let $p: (N, h) \rightarrow (M,g)$ be a Riemannian covering map. The geodesics of $(M,g)$ are the projections of the geodesics of $(N,h)$, and the geodesics of $(N,h)$ are the liftings of those of $(M,g)$. If I would be asked to prove this, I would use the local length minimizing property of geodesics and the fact that the covering map doesn't change the length of a smooth curve since it is a local isometry. However, in the book ""Riemannian Geometry"" by Gallot, Hulin and Lafontaine, this fact is proven way before the local length minimizing property is discussed (Proposition 2.81). See also this question for a proof: geodesics, covering map and its lift I'm not sure I fully understand (the final step of) these proofs. Would the following argument be correct? : If one considers local coordinates, a geodesic is a smooth curve that fulfills a certain differential equation which essentially only depends on the Christoffel symbols, which in turn only depend on the Riemannian metric. Since the Riemannian metric behaves nicely under a Riemannian covering map (i.e. $h = p^*(g)$) the above statement follows. EDIT: Just to clarify, I understand how the proposition can be proven using the locally length minimizing property of geodesics. I want to understand how it can be proven without using this property of geodesics.","['covering-spaces', 'riemannian-geometry', 'differential-geometry', 'geodesic']"
1900810,Derived functors of abelianization,"The abelianization functor from the category of groups to that of abelian groups is right exact in the sense that it takes a short exact sequence
$$1 \to K \to G \to H \to 1$$
to a shorter exact sequence
$$K_{\mathrm{ab}} \to G_{\mathrm{ab}} \to H_{\mathrm{ab}} \to 0.$$
(See Show that the abelianization functor is right exact .) If the category of groups were an abelian category, then we would be able to define derived functors of abelianization. The category of groups isn't actually abelian, or even additive, but at least exactness still makes sense, so one might hope something would go through anyway. 1. What breaks down? 2. What can be salvaged? 3. Is there some morally allied modification to this naive hope that is actually studied?","['homological-algebra', 'group-theory']"
1900821,"Conditions under which the special orthogonal group $\text{SO}_{p,q}$ is quasi-split","A reductive group $G$ is quasi-split if it has a Borel subgroup defined over the underlying field. Let $\text{SO}_{p,q}$ be the special orthogonal group of type $(p,q)$.
I vaguely remember that there is a statement in the spirit of: ""If $\vert p-q\vert$ is smaller than $K\in\mathbb{Z}$, then $\text{SO}_{p,q}$ is quasi-split."" Do I remember correctly? And if so, what is the correct value of $K?$ Furthermore, a reference for a proof of the fact will be much appreciated. EDIT It seems my memory was correct and it is $K=3$: In D. Prasad's lecture notes the statement
$$\text{""The group $\text{SO}(p, q)$ is quasi-split if and only if $|p − q| ≤ 2$.""}$$
is given without proof or further explanation. I'm still interested in a good (non-lecture notes) reference (i.e. a book or paper) or a proof for this fact.","['reference-request', 'abstract-algebra', 'algebraic-groups', 'group-theory']"
1900838,Convex hull that ignores sets of measure zero,"In my research I use a definition of convex hull that is compatible with measure theory, in the sense that it ignores negligible sets. Are there any references to the following concept, or similar concepts, in the literature? (The name, obviously, need not be the same.) Suppose $A \subset \mathbb{R}^d$, $d \in \mathbb{Z}_+$. Define the closed essential convex hull of $A$ as the intersection of all half-spaces $H \subset \mathbb{R}^d$ that satisfy
\begin{equation}
m(A \setminus H) = 0,
\end{equation}
where $m$ is the $d$-dimensional Lebesgue measure.","['reference-request', 'measure-theory', 'convex-hulls', 'convex-analysis']"
1900908,equality between two sigma algebras,"Take two sets $E_1$ and $E_2$, and assume $f$ is a function $E_1 \to E_2$.
Take now a family of subsets of $E_2$ and call it $(O_i)_{i\in I}$, and consider the family $\left(f^{-1}(O_i)\right)_{i\in I}$ of subsets of $E_1$. 
Call $\mathcal B_1$ the $\sigma$-algebra on $E_1$ generated by $\left(f^{-1}(O_i)\right)_{i\in I}$ and $\mathcal B_2$ the $\sigma$-algebra on $E_2$ generated by $(O_i)_{i\in I}$ Is it true that $\mathcal B_1 = f^{-1}(\mathcal B_2)$ ?
What I see is that at least $\mathcal B_1 \subset f^{-1}(\mathcal B_2)$. If this is not true how would one prove that for a measurable function $f: E \to \overline{\mathbb{R}}$ the preimage of any borel set of $\overline{\mathbb{R}}$ is a measurable set of $E$, when measurability has been defined by: $f^{-1}(]c;+ \infty[)$ is a measurable set for any $c\in\mathbb{R}$",['measure-theory']
1900928,Number of Dyck Paths that touch the diagonal exactly $k$ times?,"What is the number of Dyck Ppaths from $(0,0)$ to $(n,n)$ which touch the diagonal exactly $k$ times? The only permissible moves are  from $(x,y)$ to $(x+1,y)$ or $(x,y+1)$ . I know that when there is no restriction on the number of times we are allowed to touch the diagonal, the result is the $n^{th}$ Catalan Number. If we touch the diagonal after $2*i_1, 2*i_2, \dots, 2*i_k$ moves respectively such that $2 \sum_{j=1}^{k} i_j = 2n$ then the result seems to be the Catalan $k$ -fold convolution formula as given here . This link claims that we can derive this formula in a simpler way by showing a bijection to paths related to the Catalan Triangle but the explanation seems to be rather unclear. Any help or pointers in literature or hints?","['combinatorics', 'catalan-numbers']"
1900994,The square of any odd integer is odd.,"Suppose that $n$ is an odd integer. Then $n = 2k + 1$ for some integer $k$. Hence $n^2=(2k+1)(2k+1)=4k^2+4k+1=2(2k^2+2k)+1$. Since $k$ is an integer, $2k^2 + 2k$ is an integer. Thus $n^2 = 2k' + 1$ for some integer $k'$. Therefore $n^2$ is odd. Is this correct?","['discrete-mathematics', 'proof-verification', 'elementary-number-theory']"
1901033,Exercise EG.1.1 from David Williams' “Probability with Martingales” page 224,"I have a hard time with the following exercise: Planet X is a ball with center O. Three spaceships A, B and C land at random on its surface, their positions being independent and each uniformly distributed on the surface. Spaceships A and B can communicate directly by radio if $\measuredangle AOB < 90°$. Show that the probability that they can keep in touch (with, for example, A communicating with B via C if necessary) is $$\frac{\pi + 2}{4 \pi}$$. I can reach a result if I consider a circle instead of a sphere, but I have a hard time to define the coordinates of point placed randomly on a sphere.","['probability-theory', 'probability']"
1901059,Intuition for geometric definition of Henselian (local) scheme?,"An excerpt from section 2.3 of Néron Models : Let $R$ be a local ring with maximal ideal $\mathfrak m$ and residue field $k$ . Let $S$ be the affine (local) scheme of $R$ , and let $s$ be the closed point of $S$ . From a geometric point of view, Henselian and strictly Henselian rings can be introduced via schemes which satisfy certain aspects of the inverse function theorem. Definition 1. The local scheme $S$ is called Henselian if each étale map $X\to S$ is a local isomorphism at all points of $X$ over $s$ with trivial residue field extension $k(x)=k(s)$ . If, in addition, the residue field $k(s)$ is separably closed, $S$ is called strictly Henselian . What's the geometric intuition here? In other words, why do we only ask the inverse function theorem to hold at points of the special fiber with trivial residue field extension?","['intuition', 'algebraic-geometry', 'ring-theory', 'hensels-lemma', 'commutative-algebra']"
1901062,"If $\frac{a}{b+c}+\frac{b}{c+a}+\frac{c}{a+b}=1$, what can we say about $\frac{a^2}{b+c}+\frac{b^2}{c+a}+\frac{c^2}{a+b}$?","Suppose that $a,b,c$ are three real numbers such that $\dfrac{a}{b+c}+\dfrac{b}{c+a}+\dfrac{c}{a+b}=1$ .  What are the possible values for $\dfrac{a^2}{b+c}+\dfrac{b^2}{c+a}+\dfrac{c^2}{a+b}$ ? After clearing the denominators, we have $$a(c+a)(a+b)+b(b+c)(a+b)+c(b+c)(c+a)=(a+b)(b+c)(a+c)\,.$$ That is, $$a^3+b^3+c^3+abc=0\,.$$ But then I'm stuck. This question is related, but a bit different. Thank you for your help!","['algebra-precalculus', 'contest-math', 'symmetric-functions', 'rational-functions']"
1901080,When is this set a group?,"Suppose that for every $i\in\mathbb{N}$ we have a sequence of integers
$$s_i=(n_{i1},n_{i2},n_{i3},\ldots).$$
Consider the following two operations:
$$s_i^{-1}:=(n_{1i},n_{2i},n_{3i},\ldots)$$
and
$$s_is_j:=(n_{i1}+n_{j1},n_{i2}+n_{j2},n_{i3}+n_{j3},\ldots).$$ Question: For what choices of $n_{ij}$ is the set $S=\{s_i:i\in\mathbb{N}\}$ a group with the above operations? One trivial example is to take $s_i=(0,0,0,\ldots)$ for all $i$. Then, $S$ is the group with one element. Are there other examples? Or can we prove that this is the only example? Edit 1: I think I have another example, but it is fairly difficult to explain. Subedit: The example is wrong since the repeated sum of a row doesn't appear anywhere. It appears to have to do with some modification of the Fibonacci sequence. The first few elements of the matrix $(n_{ij})_{i,j\in\mathbb{N}}$ are
$$
\begin{matrix}
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 1 & 1 & 2 & 1 & 2 & 2 & 3 & 2 & 3 & 3 \\
 0 &-1 & 0 &-1 &-2 &-1 &-3 &-2 &-2 &-1 &-3 &-2 &-3 \\
 0 &-1 & 1 & 0 &-1 & 1 &-2 & 0 & 0 & 2 &-1 & 1 & 0 \\
 0 &-1 & 2 & 1 & 0 & 3 &-1 & 2 & 2 & 5 & 1 & 4 & 3 \\
 0 &-2 & 1 &-1 &-3 & 0 &-5 &-2 &-2 & 1 &-4 &-1 &-3 \\
 0 &-1 & 3 & 2 & 1 & 5 & 0 & 4 & 4 & 8 & 3 & 7 & 6 \\
 0 &-2 & 2 & 0 &-2 & 2 &-4 & 0 & 0 & 4 &-2 & 2 & 0 \\
 0 &-2 & 2 & 0 &-2 & 2 &-4 & 0 & 0 & 4 &-2 & 2 & 0 \\
 0 &-3 & 1 &-2 &-5 &-1 &-8 &-4 &-4 & 0 &-7 &-3 &-6 \\
 0 &-2 & 3 & 1 &-1 & 4 &-3 & 2 & 2 & 7 & 0 & 5 & 3 \\
 0 &-3 & 2 &-1 &-4 & 1 &-7 &-2 &-2 & 3 &-5 & 0 &-3 \\
 0 &-3 & 3 & 0 &-3 & 3 &-6 & 0 & 0 & 6 &-3 & 3 & 0
\end{matrix}
$$
Basically, I started with $\begin{smallmatrix}0 & 0 & 0 \\ 0 & 0 & 1\end{smallmatrix}$ and then the rest is uniquely determined. The identity element is $0$. (Yes, there is a repeated row, and it has to be like that.) Edit 2: If $S$ is a group under those operations, then we can show that the identity element is $0$ and $n_{ij}+n_{ji}=0$ for all $i,j$. Indeed, let $e$ be the identity. Then,
$$e=s_is_i^{-1}=(n_{i1}+n_{1i},n_{i2}+n_{2i},\ldots).$$
Also, $s_i=s_ie$ so
$$(n_{i1},n_{i2},\ldots)=(n_{i1}+n_{i1}+n_{1i},n_{i2}+n_{i2}+n_{2i},\ldots)$$
and hence $n_{ij}+n_{ji}=0$ for all $i,j$. In particular, $e=0$. Edit 3: By Edit 2, the question is equivalent to finding an infinite skew-symmetric matrix $(n_{ij})_{i,j\in\mathbb{N}}$ such that the sum of every two rows is again some row of the matrix. Edit 1 is an attempt at such a matrix.",['group-theory']
1901094,Limits: How is it possible to change the way to write an equation and have different results?,"I still have a very brief experience with calculus and limits, so this might actually be a silly question with a very simple answer. Imagine you are trying to find the limit of $f(x)$ when $x$ approaches 1, being 
$f(x) = \frac{\sqrt x-1}{x-1}$ Replacing $x$ by 1 right away makes this $\frac{0}{0}$. If you say that the denominator $x-1$ is equal to $(\sqrt x+1)(\sqrt x-1)$, and continue the equation, $\lim \limits_{x \to 1} = \frac{\sqrt x-1}{(\sqrt x+1)(\sqrt x-1)} \to \lim \limits_{x \to 1} = \frac{1}{\sqrt x+1} \to \lim \limits_{x \to 1} = \frac{1}{2}$ you get $\frac{1}{2}$, which is, according to my answer sheet, indeed the limit of $f(x)$ as $x$ approaches 1. What I don't understand though, is how did I get different results($\frac{0}{0}$ and $\frac{1}{2}$) just by changing the way to write the equation, not just in this case, but in almost any basic limit example question i've encountered. It is clear that changing the denominator to another expression that means exactly the same drastically change the final result, but how is this possible?","['calculus', 'limits']"
1901113,Is there something special about the normal distribution in the context of the random vector being equally likely to point in any direction?,"In the context of the error vector $$\epsilon: \Omega \to \mathbb R^n$$ of a multivariable regression, I have heard roughly the following: The distribution of $\epsilon$ is uniform over the angle, so the random vector is equally likely to point in any direction. Distributions other than the Gaussian one don't give a spherically symmetrical distribution. As far as I can tell, it says that the multivariable normal distribution can be characterized in this way. It made me very curios. I was looking for a proof and saw this : Among spherically symmetric distributions are not only multivariate normal distributions with covariance matrices of form $σ^2I$ but also, for example, certain cases of standard multivariate t and logistic distributions. So the first quote is wrong and there is nothing special about normal distribution in this regard?","['regression', 'normal-distribution', 'probability-theory', 'multivariable-calculus', 'statistics']"
1901126,Visualizing $\cap_{i = 1}^\infty A_i = (\cup_{i = 1}^\infty A_i^c)^c$,"As the question title suggests, how do I visualize$$\cap_{i = 1}^\infty A_i = (\cup_{i = 1}^\infty A_i^c)^c?$$Let's start with the right-hand side. So I have a bunch of circles representing the $A_i$'s, right? Then I take the complements of each one, so for each $A_i$, the corresponding $A_i^c$ is the entire ambient space with the $A_i$ removed. I have trouble visualizing that taking the union of all those $A_i^c$'s then taking the complement of that union is our desired intersection $\cap_{i = 1}^\infty A_i$. Could anybody help me visualize this?","['real-analysis', 'visualization', 'elementary-set-theory']"
1901133,Euclid's Elements missing axiom of M. Pasch examples,"In The Mathematical Experience, Study Edition by Philip J. Davis, Reuben Hersh, and Elena Anne Marchisotto it states pp.175-176: Just what constitutes the ""straightness"" of the straight line? There is undoubtedly more in this notion than we know and more than we can state in words or formulas. Here is an instance of this ""more."" Suppose $a$, $b$, $c$, $d$ are four points on a line. Suppose $b$ is between $a$ and $c$, and $c$ is between $b$ and $d$. Then what can we conclude about $a$, $b$, and $d$? It will not take you long to conclude that $b$ must lie between $a$ and $d$. This fact, surprisingly, cannot be proved from Euclid's axioms; it has to be added as an additional axiom in geometry. This omission of Euclid was first noticed 2000 years after Euclid, by M. Pasch in 1882! Moreover, there are important theorems in Euclid whose complete proof requires Pasch's axiom; without it, the proofs are not valid. See pp.21-22 for a description of Pasch's axiom and a picture of Pasch from the linked seminar slides: StanfordLogicSeminarApril2014.pdf From wikipedia: Pasch's axiom A more informal version of the axiom is often seen: If a line, not passing through any vertex of a triangle, meets one side of the triangle then it meets another side. I wanted to know which theorems in the Elements are considered in worst shape as a consequence of Pasch's missing axiom? Also have there been any more axioms found to be missing like Paschs?","['math-history', 'euclidean-geometry', 'geometry']"
1901177,"Find the value of $\tan A + \tan B$, given values of $\frac{\sin (A)}{\sin (B)}$ and $\frac{\cos (A)}{\cos (B)}$","Given $$\frac{\sin (A)}{\sin (B)} = \frac{\sqrt{3}}{2}$$ $$\frac{\cos (A)}{\cos (B)} = \frac{\sqrt{5}}{3}$$ Find $\tan A + \tan B$. Approach Dividing the equations, we get the relation between $\tan A$ and $\tan B$ but that doesn't help in getting the value of $\tan A + \tan B$. The value comes in terms of $\tan A$ or $\tan B$ but the expected answer is independent of any variable . Also $$\frac{\sin(A)\cdot\cos(B) + \sin(B)\cdot\cos(A)}{\cos(A)\cdot\cos(B)} = \tan(A) + \tan(B)$$ We could get a value only if instead of $\cos A$ there was $\sin B$ in the relation(which we get on adding the ratios)","['plane-geometry', 'trigonometry']"
1901190,"Liars, adjunctions, and functions $f : S \rightarrow UFS$. Does this lead anywhere interesting?","A student of mine was recently given the following question: ""At least one of us is lying,"" said Andrew. ""Only one of us is lying,"" said Bertas. ""Squeak, two of us are lying,"" said the Chipmunk. ""Either three or four of us are lying,"" said Daisy. ""Elmo thinks that everybody is lying,"" said Elmo. How many liars are there altogether? We can rehash this into language that is more easy to analyse as follows: Andrew: the number of truthspeakers is an element of $\{0,1,2,3,4\}.$ Bertas: the number of truthspeakers is an element of $\{4\}.$ Chipmunk: the number of truthspeakers is an element of $\{3\}.$ Daisy: the number of truthspeakers is an element of $\{1,2\}.$ Elmo: the number of truthspeakers is an element of $\{0\}.$ With a bit of thought, we see that the answer is ""$2$ truthspeakers."" It can't be $0$, because then Andrew and Elmo would be truthspeakers. It can't be $1$, because then both Andrew and Daisy would be truthspeakers. It can be $2$, with Andrew and Daisy being the only truthspeakers. It can't be $3$ or more, because the intersection of any three distinct sets listed above is empty. Anyway, I thought this question was pretty cool, enough so that I tried throwing some abstract nonsense at it. Here's what I came up with. Definition 0. Consider a category concrete over $\mathbf{Set}.$ Lets refer to its objects as algebras , and lets assume there's a free algebra on every set. Write $U$ for the underlying set functor, $F$ for the free functor, and $\Phi$ for the adjunction $F \dashv U$. Given a set $S$ equipped with a function $f : S \rightarrow UFS$ an algebra $X$, a realization of $f$ in $X$ is a function $g : S \rightarrow UX$ such that $g=U(\Phi_{S,X}(g)) \circ f$ For instance: Work over the concrete category of Boolean algebras; so in particular, $F$ is the free Boolean algebra functor. Define $S = \{A,B,C,D,E\}$. Define $f : S \rightarrow UFS$ so that it expresses whom the truthspeakers are conjectured to be: For example: $$f(A) = \neg(\neg A \wedge \neg B \wedge \neg C \wedge \neg D \wedge \neg E), \qquad f(E) = \bot$$ Then the unique realization of $f$ in the boolean algebra $\{0,1\}$ is $$g : A,D \mapsto 1, \qquad g : B,C,E \mapsto 0.$$ Here's a more simple example: Liar's paradox. The function $f : \{x\} \rightarrow F(\{x\})$ given by $x \mapsto \neg x$ has no realizations in $\{0,1\}$. What I'd like to know is: Question. Does this train of thought lead to any interesting mathematics?","['abstract-algebra', 'universal-algebra', 'logic', 'category-theory', 'monads']"
1901221,Lorenz attractor depending on the numerical solution method,"I have a problem. I am using two different numerical methods to try to “solve” the Lorentz attractor. Those are the Euler method (RK(s=1)) and the trapezoidal method with fixed-point iterations. The problem is that providing both methods the same initial parameters and the same iteration parameters (step-size, time-interval) I get two different “solutions”: Euler method Trapezoid method: I have revised my script several times and I cannot find any error. So I am starting to think that the solution will depend on the method. If so, please tell me, so I can stop looking for the mistake in the script.","['numerical-methods', 'chaos-theory', 'ordinary-differential-equations', 'dynamical-systems']"
1901222,Borel $\sigma$-algebras are never complete?,"Let $X$ be a smooth oriented manifold of positive dimension . Let $\Lambda$ be a positive linear functional on $C_c(X)$. By Riesz Representation theorem there exists a unique measure $\mu$ on a $\sigma$-algebra $\mathfrak B$, which sarisfies  $I(f)=\int_X f d \mu$ for all $f \in C_c(X)$. In addition, $\mathfrak B,\mu$ satisfy: (a) $\mathfrak{B}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{B}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{B}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{B}$. Is it true that $\mathfrak B(X)$ is not complete w.r.t $\mu$? Note: It turns out that $\mathfrak B$ is unique . Thus, an equivalent question is whether $\mathfrak B= \mathfrak B(X)$. (Since $\mathfrak B(X)$ satisfies all the conditions except maybe $(f)$). I guess that if $\Lambda$ is such that the induced measure $\mu$  is absolutely continuous w.r.t some Riemannian measure* (restricted to $\mathfrak B(X)$), then $\mathfrak B(X)$ won't be complete. *By a Riemannian measure, I mean any measure $\tilde \mu$ which we get from Riesz theorem, by taking $\Lambda(f)=\int_X f Vol_{\mathfrak g}$, where $Vol_{\mathfrak g}$ is the Riemannian volume form of some Riemannian metric $\mathfrak g$ on $X$.","['real-analysis', 'measure-theory']"
1901238,Does every abelian C* algebra have a single self-adjoint generator?,Does an abelian von Neumann algebra have this property? Is there some interesting class of C* algebras that does?,"['functional-analysis', 'functional-calculus', 'c-star-algebras', 'operator-algebras']"
1901245,How to prove each element of the following sequence is a perfect square?,"Sequence $\{a_n\}$ satisfies the following formula: $a_{n+2}=14a_{n+1}-a_n+12$, and $a_1=1, a_2=1$. It is easy to check that $a_3=25$ and $a_4=361$. The question is how to prove each element of the sequence $\{a_n\}$ is a perfect square?","['number-theory', 'sequences-and-series']"
1901250,"If a manifold has a transitive action by $G$, why is it equal to $G/\mathrm{stabiliser}$?","I saw the below content in Tao's blogs . I don't know why the manifold equal to the quotient of the group by the stabiliser? Besides, the stabiliser depend on $x$, according to Wiki. For example, $G$ is a group, $x\in G$, then 
$$G_x=\{g\in G : gx=x\}$$ is a stabiliser of $x$. Although it is not different in $S^3$. But it is different in other manifold.","['group-actions', 'differential-geometry', 'group-theory', 'lie-groups']"
1901273,How to induce a metric on $S^3$ from the transitive group action of $SO(4)$?,"Picture below is from the Tao's blogs , I understand the Killing form as 
$$
B(x,y)=\operatorname{trace}(\operatorname{ad}(x)\operatorname{ad}(y))
$$
$SO(4)$ is the group acting on $S^3$. I don't understand the content above red line.","['riemannian-geometry', 'differential-geometry', 'lie-algebras', 'lie-groups']"
1901277,Largest factored composite Cunningham number?,"A number of the form $b^{n}\pm 1$ is a Cunningham number , 
 with non-square integer $b>1$ and integer $n>2$. The Fermat and Mersenne numbers are Cunningham numbers. The Cunningham Project records various results. For $n=2$, the largest twin primes likely provides the answer, $(3756801695685 \times 2^{666669})^2 - 1$ in this case. Is $F_{11} = 2^{2048} +1$ (617 digits) the largest completely factored Cunningham number? Or are there some special numbers of this type that are completely factored? EDIT: I'm going to also rule out Wagstaff primes .  $2^{13372531} +1$ is ( probably ) completely factored.","['number-theory', 'prime-factorization']"
1901282,Why not always make a linear system's matrix symmetric?,"This may be quite a naive question, but as I'm reading about different methods to solve linear systems of the type Ax=b with A a n x n matrix, I wonder why we should not always solve an equivalent system with a symmetric matrix obtained by multiplying the equation with the transpose of A, i.e. $$A^T A x = A^T b$$ (especially in the context of large, sparse matrices and matrix free methods where symmetric matrices have quite desirable properties)? Edit: Lets consider that the problem is well posed so that the square matrix A is invertible and the solution x is unique.","['matrices', 'numerical-linear-algebra', 'linear-algebra']"
1901305,"integral $\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x$","Evaluate , if possible in a closed form, the integral: $$\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x$$ Basically, I have not done that much. I broke the integral \begin{align*}
\int_{0}^{\pi} \left ( \frac{\pi}{2}-x \right ) \frac{\tan x}{x} \, {\rm d}x  &= \int_{0}^{\pi/2} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x  + \int_{\pi/2}^{\pi} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x\\ 
 &\!\!\!\!\!\!\overset{u=\pi/2-x}{=\! =\! =\! =\! =\! =\!} \int_{0}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u + \int_{-\pi/2}^{0} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\ 
 &= \int_{-\pi/2}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\ 
 &\approx 2.13897
\end{align*} I have no idea how to evaluate this. I was thinking of IBP and then some kind of Fourier , but I cannot get it to work. Any ideas?","['real-analysis', 'improper-integrals']"
1901314,How can I express this shape as a parameterized function?,"I want a (hopefully continuous) function that can describe the following shape. Although the image is not exact, I want the expression to include an increase in crest amplitudes. For reference, this is an image of a self replicating nano-scale crack in which the delamination and fracture occurs simultaneously to create these amazing shapes. I am trying to backwards engineer an interaction between these two forces to create specific shapes. In order to do that, I need the position function of the crack front. (this crack front position function is what I am asking about)","['parametric', 'analysis', 'geometry']"
1901327,Use of Gauss-Green identity,"In a solution of the problem 'what is the $C^1$ curve in $\mathbb R^2$, parametrized by arc length, whose inside area is maximum' my professor said that the area could be calculated as $\int_0^{2\pi} x y' dt$ where $x(t),y(t)$ is the parametrisation of the curve. This, he said, by Gauss-Green identity. The statement I know is $\int_{\Omega} \Delta u\ v+\int_\Omega \nabla u\cdot\nabla v =\int_{\partial \Omega}v\frac{\partial u}{\partial\nu}$. How can one relate this statement to the above result? (I think one should take $u=x$, $v=y$, but I don't see how this leads to the end, although it is the intuitive way, and, also, the problem seems in fact very easy). Thank you in advance.","['integration', 'differential-geometry']"
1901377,Why two possibles Jordan Canonical forms of a matrix cannot be similar?,"Consider an matrix $A_{5x5}$ with only one eigenvalue $\lambda$. If the dimension of the eigenspace $\lambda$ is two, then we can have two possibilities of the Jordan blocs here: I didnt understand the statement marked by yellow so I have two questions about it: 1) If $(J^{(2)}-\lambda I)^3$ and $(J^{(3)}-\lambda I)^3$ were equal zero, then $J^{(2)}$ and $J^{(3)}$ would be similar? Why ? 2) Like in the reference, why the fact that $(J^{(2)}-\lambda I)^3=0$ and $(J^{(3)}-\lambda I)^3 \ne 0$ guarantees that $J^{(2)}$ and $J^{(3)}$ are not similar? I dont need a proof, just an understandment. One can find the cited reference here: http://math.postech.ac.kr/~sungpyo/LinearAlge-2007/Chap8.pdf , and the statement marked by yellow in the bottom of the second page of the reference. obs: I dont know if the tag ""jordan normal form"" applies here, since I am studying jordan canonical form (dont really know if there's any difference).","['jordan-normal-form', 'linear-algebra']"
1901455,Calculate the non-server's advantage in a win-by-two game,"In volleyball, a point is awarded for every ""rally"" and a game is won by the first team to get to $N$ points, but you must win a game (sometimes called a ""set"") by two or more points. (If one team gets to $N$ points but the other team has $N-1$ points, play continues until one team is ahead by two.) The team that has won the last point always serves to the next point. It is well known that a high levels of play, the serving side is at a disadvantage in the sense of having a lower probability of winning that point (since the receiving side has the first opportunity to ""attack""). Let us say two teams are evenly matched, and the probability of the serving side winning a given point is $p$ with $0<p<\frac12$. It might be thought that since you need to win a game by two points, there will be an even number of points played in any sufficiently close game, and the disadvantage of having to serve first will even out.  This is not the case.  For example, if $N=2$ (that is, you start out even and the first team to be ahead by two points will win), the probability of the initial server winning is
$$
S_{22} = \frac{1}{3-2p}
$$
Here I have introduced a notation:  $S_{ab}$ is the probability of the server winning if the server needs at least $a$ more points, and the non-server needs at least $b$ more points.  So for example, in a game to $N=21$, if the score is $20$ serving to $19$, that position would be represented by $S_{13}$. My question is, for $k>2$, what is $S_{kk}$? That is, in a game to $N=k$, what is the probability that the starting server will win (and thus how much of a disadvantage is the first serve)?  I would guess that this probability can be obtained in closed form, but if not, I'd like to see the asymptotic behavior of $\frac12-S_{kk}$. Calculation of $S_{22}$: $$S_{22} = p S_{13} + (1-p) ( (1-S_{13}) \\
S_{13} = p + (1-p) (1-S_{22} )
$$
Take the value of $S_{13}$ from the second equation and plug it into the first.  Group terms to get
$$
S_{22}(3p-2p^2) = p
$$","['asymptotics', 'probability']"
1901470,Manifold Galerkin method,"Standard Galerkin method reduces the problem Find $u\in V$ such that $a(u,v) = f(v)$ for all $v \in V$,
    where $V$ is Hilbert space, $a$ is bilinear form and $f\in V^*$. to a finite dimensional problem by introducing a $n$-dimensional subspace $V_n\subset V$, then we look for an approximation $u_n$ of the solution $u$ such that Find $u_n \in V_n$ such that $a(u_n,v) = f(v)$ for all $v \in V_n$. I would like to replace $V_n$ by a $n$-dimensional manifold $\mathcal{M}_n$ in $V$. So the reduced problem would be Find $u_n\in \mathcal{M}_n$ such that $a(u_n,v)=f(v)$ for all $v\in T_{u_n}\mathcal{M}_n$, where $T_{u_n}\mathcal{M}_n$ is tangent space to the manifold $\mathcal{M}_n$ at the point $u_n$. What do we know about this problem? What are the conditions on $\mathcal{M}_n$ for existence of $u_n$? Does $u_n$ converge to $u$ as we make $\mathcal{M}_n$ bigger and bigger ($n\rightarrow \infty$)? What is this method called?(I called it Manifold Galerkin method ) Can you please point me to the literature where they discuss this problem?","['partial-differential-equations', 'reference-request', 'functional-analysis', 'galerkin-methods', 'numerical-methods']"
1901485,Convergence of $\sum_n \frac{|\sin(n^2)|}{n}$,"A problem in Makarov's Selected problems in real analysis asks to investigate the convergence of $\displaystyle \sum_n \frac{|\sin(n^2)|}{n}$ I'm clueless at the moment. I can't find any good property of the sequence $|\sin(n^2)|$. $|\sin(n^2)|$ is small whenever $n\sim \sqrt{p\pi}$, and, as $p\to \infty$, the $\sqrt{p\pi}$ get closer to each other since $\sqrt{(p+1)\pi}-\sqrt{p\pi}\sim \frac 12 \sqrt{\frac{\pi}{p}}$. Any hint is appreciated.","['real-analysis', 'sequences-and-series']"
1901519,Evaluate an improper integral involving log [duplicate],"This question already has answers here : Evaluating $\int_0^{\infty}\frac{\ln(x^2+1)}{x^2+1}dx$ (7 answers) Closed 7 years ago . Studying for complex analysis, I stumbled upon this problem. Evaluate
$$\int_{0}^{\infty} \frac{\log(1+x^2)}{1+x^2}~dx.$$
So the integrand suggests that I need to use the function $f(z)=\frac{\log 1+z^2}{1+z^2},$ which has two branch points and two simple poles at $z=i$ and $z=-i,$ which lead me to use a semi-circle contour in the right-halfplane having branch cuts at $i$ and $-i.$ But then the process become too lengthy. Is there any other method to go by doing the problem. I was thinking of making a substitution and reducing this to a form to use Gamma function, but wasn't successful. Any help is appreciated.","['complex-analysis', 'improper-integrals', 'contour-integration']"
1901545,limit $\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$,"$\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$ I tried to write it as $\sin (n\pi - \theta)$ to get the form $∞-∞$ form within $\sin$ function. But could not proceed after that. How should I do it? Edit:I am sorry, I forgot to mention $n\in \mathbb{N}$",['limits']
1901546,Show that a Lissajous curve has incommesurate frequencies iff it isdense in a rectangle,"If we have a Lissajous curve given by the parametric equations
$$x(t)=A\sin(\omega_x t)$$
$$y(t)=B\sin(\omega_y t + \delta),$$ how can we show that the curve is dense in the rectangle of sides $A,B$ if and only if $\omega_x$ and $\omega_y$ are incommensurate (i.e. their ratio is irrational)? What I tried:
Suppose it is not dense, so there exists a neighborhood of radius $r$ around some $x_0$, $y_0$ through which the curve doesn't pass. Since the parametric functions are continuous in time, a neighborhood of size $r$ in distance translates to some neighborhood of size $\tau$ in time. The curve is at $x=x_0$ at the times
$$t_x=\{\frac{1}{\omega_x}\left[\sin^{-1}(x_0/A)+2n\pi\right]|n\in \mathbb{N}\}$$
and at $y=y_0$ at the times $$t_x=\{\frac{1}{\omega_y}\left[\sin{-1}(y_0/B)-\delta+2m\pi\right]|m\in \mathbb N\}.$$ If any two points in in the sets $t_x, t_y$ are less than $\tau$ away, then the trajectory goes through the $r$-neighborhood around $x_0,y_0$, giving a contradiction. For this we need to find integers $m,n$ such that
$$\frac{\sin^{-1}(x_0/A)}{2\pi\omega_x} - \frac{\sin^{-1}(y_0/B)-\delta}{2\pi\omega_y}+\frac{n}{\omega_x}-\frac{m}{\omega_y}<\tau$$
At this point it seems that I need something tat states that the difference $n/\omega_x - m/\omega_y$ can be arbitrarily close to any real if $\omega_x$ and $\omega_y$ are incommensurate, but I have not been able to prove anything like that.","['real-analysis', 'functional-analysis', 'complex-analysis', 'ordinary-differential-equations', 'analysis']"
1901551,The multiplicative groups $\mathbb{Q}^\ast$ and $\mathbb{R}^\ast$ are not isomorphic [duplicate],"This question already has answers here : Does there exist any surjective group homomorphism from $(\mathbb R^* , .)$ onto $(\mathbb Q^* , .)$? (3 answers) Closed 3 years ago . Of course, since the cardinality of $\mathbb{R}$ exceeds the cardinality of $\mathbb{Q}$, there does not exist a bijection between $\mathbb{Q}^\ast$ and $\mathbb{R}^\ast$, let alone a group isomorphism. My question is whether it is also possible to prove that these multiplicative groups are not isomorphic without using a cardinality argument.","['alternative-proof', 'group-theory', 'group-isomorphism']"
1901563,Checking that a torsion-free abelian group has finite rank,"Suppose $G$ is a torsion-free abelian group and that $G \otimes_\mathbb{Z}\mathbb{Z}_l$ is free of finite rank as a $\mathbb{Z}_l$ -module, where $\mathbb{Z}_l$ denotes the $l$ -adic integers and $l $ is a fixed prime number. Can we conclude that $G$ is free of finite rank? Additionally, what if the statement is true for all primes $l$ ?","['abelian-groups', 'abstract-algebra', 'group-theory', 'tensor-products']"
1901598,Why does A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C)?,"If I have the elements from sets A and B, and I want to find the set A ∪ (B ∩ C), I end up with just the elements of A. On the other hand, if I have the elements from A and B and want to find (A ∪ B) ∩ (A ∪ C), it seems like I end up with (A ∩ B) ∩ A, which is just (A ∩ B). This set was not a possible outcome with A ∪ (B ∩ C). Where has my reasoning going wrong?","['logic', 'elementary-set-theory']"
1901603,If $a$ and $b$ are prime to each other and $n$ is prime then$\frac{a^n+b^n}{a+b}$ and $a+b$ have no common factor. [duplicate],"This question already has answers here : Show that $\gcd\left(\frac{a^n-b^n}{a-b},a-b\right)=\gcd(n d^{n-1},a-b)$ (5 answers) Closed 3 years ago . Question is: If $a$ and $b$ are prime to each other and $n$ is prime then prove that $\frac{a^n+b^n}{a+b}$ and $a+b$ have no common factor unless $a+b$ is a multiple of $n$ . This is what I thought so far: $a^n$ is also prime to $b^n$ because of the theorem (If $a$ is prime to $b$ then $a^n$ is also prime to $b^n$ ) , Also $a^n$ and $b^n$ is a divisor of $(a+b)$ therefore by theorem (If $a$ is prime to $b$ , and each of these numbers is a divisors of $N$ , then $ab$ is a divisor of $N$ ) $a^nb^n$ is a divisor of $(a+b)$ . Thereafter no idea, How should I proceed further if I'm thinking in a right way and If not, How should I prove this? Source: Higher Algebra by Barnard and Child","['number-theory', 'elementary-number-theory']"
1901632,Why does the error term in Taylor's Theorem converge to Zero (in two senses)?,"Let us consider Taylor's Theorem in the case of one real variable. For simplicity let us assume that $f$ is infinitely differentiable about $a$. Then we have that $$f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(k)}(a)}{k!}(x-a)^k + h_k(x)(x-a)^k$$ My question is why does the remainder term converge to $0$? In particular, why do we have that $$ \lim_{x \to a} h_k(x) = 0$$ as well as that $$ \lim_{k \to\infty} h_k(x) = 0?$$ If I understood this last statement, in particular, then I would understand why $$f(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!} (x-a)^k$$ which has also been a source of confusion for me.","['taylor-expansion', 'power-series', 'calculus']"
1901666,How many irreducibles in $\mathbb Z[\sqrt - 5]$?,"Finding and counting irreducibles in non-UFD's seems harder to me than finding and counting primes in UFD's. The number of primes in the UFD $\mathbb Z[\sqrt -1]$ (the gaussian integers) is about $\pi(n)$ where $\pi$ is the usual prime counting function and $n$ is the norm.
In other words, the number of gaussian primes with norm at most $n$ is almost equal to $\pi(n)$. To see this notice primes that are the Sum of 2 squares are $ 1 \mod 4$. Let the counting function of such primes be $\pi_2(n)$. It is Well known that $\pi_2(n) $ ~ $ \pi(n)/2$. 
The number of gaussian primes is ( ignore unit multiples ) thus $2  \pi_2(n) $ ~ $\pi(n)$. A similar thing happens for Eisenstein integers. How many irreducibles are there in the integral domain $\mathbb Z[\sqrt - 5]$ ? Is it asymptotic to $$ T_5 \pi(n) $$ Where $n$ is again the norm and $T_5$ is rational ? Does a non-UFD imply more irreducibles than a UFD or less ... Or neither ? Are the number of irreducibles in $\mathbb Z[\sqrt - p]$ for $p$ a prime always asymptotic of the form $$ T_p \pi(n) $$ Where again $T_p$ is rational. Is $T_p$ the class number ??? If so why ??","['abstract-algebra', 'unique-factorization-domains', 'asymptotics', 'algebraic-number-theory']"
1901668,Linear Algebra involving matrix equations and determinants,"I'm studying for a linear algebra exam and one of the questions on a practice exam is as follows: For which positive integers n does there exist a matrix $A\in\mathbb{R}^{n\times n}$ such that $A^2+A+I=0$? Note: Here I is the identity matrix. I know how to solve it for $A^2 + I=0$ by rearranging it as $A^2=-I$ then taking the determinant of both sides to get $\det(A)^2=(-1)^n$ and because $\det(A)^2$ must be positive there is no solution for odd values of n . When I apply this technique to the above question I get:
$A^2+A=-I\implies A(A+I)=-I\implies \det(A)\det(A+I)=(-1)^n$ but I'm stuck there because if $\det(A)<0$ then $\det(A+I)$ can be positive or negative so I can't use the same argument as above.  I've tried manipulating the equation in other ways and applying the determinant and haven't been able to come up with anything. Any help is appreciated!","['matrices', 'matrix-equations', 'linear-algebra', 'determinant']"
1901678,Intuition for proof of monotone class theorem?,"Here is the monotone class theorem from my real analysis textbook. Suppose $\mathcal{A}_0$ is an algebra, $\mathcal{A}$ is the smallest $\sigma$ -algebra containing $\mathcal{A}_0$ , and $\mathcal{M}$ is the smallest monotone class containing $\mathcal{A}_0$ . Then $\mathcal{M} = \mathcal{A}$ . Here is the proof in the book. A $\sigma$ -algebra is clearly a monotone class, so $\mathcal{M} \subset \mathcal{A}$ . We must show $\mathcal{A} \subset \mathcal{M}$ . Let $\mathcal{N}_1 = \{A \in \mathcal{M} : A^c \in \mathcal{M}\}$ . Note $\mathcal{N}_1$ is contained in $\mathcal{M}$ and contains $\mathcal{A}_0$ . If $A_i \uparrow A$ and each $A_i \in \mathcal{N}_1$ , then each $A_i^c \in \mathcal{M}$ and $A_i^c \downarrow A^c$ . Since $\mathcal{M}$ is a monotone class, $A^c \in \mathcal{M}$ , and so $A \in \mathcal{N}_1$ . Similarly, if $A_i \downarrow A$ and each $A_i \in \mathcal{N}_1$ , then $A \in \mathcal{N}_1$ . Therefore $\mathcal{N}_1$ is a monotone class. Hence $\mathcal{N}_1 = \mathcal{M}$ , and we conclude $\mathcal{M}$ is closed under the operation of taking complements. Let $\mathcal{N}_2 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{A}_0\}$ . Note the following: $\mathcal{N}_2$ is contained in $\mathcal{M}$ and $\mathcal{N}_2$ contains $\mathcal{A}_0$ because $\mathcal{A}_0$ is an algebra. If $A_i \uparrow A$ , each $A_i \in \mathcal{N}_2$ , and $B \in \mathcal{A}_0$ , then $A \cap B = \cup_{i = 1}^\infty (A_i \cap B)$ . Because $\mathcal{M}$ is a monotone class, $A \cap B \in \mathcal{M}$ , which implies $A \in \mathcal{N}_2$ . We use a similar argument when $A_i \downarrow A$ . Therefore $\mathcal{N}_2$ is a monotone class, and we conclude $\mathcal{N}_2 = \mathcal{M}$ . In other words, if $B \in \mathcal{A}_0$ and $A \in \mathcal{M}$ , then $A \cap B \in \mathcal{M}$ . Let $\mathcal{N}_3 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{M}\}$ . As in the preceding paragraph, $\mathcal{N}_3$ is a monotone class contained in $\mathcal{M}$ . By the last sentence of the preceding paragraph, $\mathcal{N}_3$ contains $\mathcal{A}_0$ . Hence $\mathcal{N}_3 = \mathcal{M}$ . We thus have that $\mathcal{M}$ is a monotone class closed under the operations of taking complements and taking finite intersections. If $A_1, A_2, \ldots$ are elements of $\mathcal{M}$ , then $B_n = A_1 \cap \ldots \cap A_n \in \mathcal{M}$ for each $n$ and $B_n \downarrow \cap_{i = 1}^\infty A_i$ . Since $\mathcal{M}$ is a monotone class, we have that $\cap_{i = 1}^\infty A_i \in \mathcal{M}$ . If $A_1, A_2, \ldots $ are in $\mathcal{M}$ , then $A_1^c, A_2^c, \ldots$ are in $\mathcal{M}$ , hence $\cap_{i = 1}^\infty A_i^c \in \mathcal{M}$ , and then $$\cup_{i = 1}^\infty A_i = (\cap_{i = 1}^\infty A_i^c)^c \in \mathcal{M}.$$ This shows that $\mathcal{M}$ is a $\sigma$ -algebra, and so $A \subset \mathcal{M}$ . The proof of this theorem is rather technical. I have a few questions about it. What is the underlying intuition behind the proof? What are the one to three key ideas this proof boils down to? What is the geometric significance of this result/how can I visualize it? Thanks in advance!","['general-topology', 'real-analysis', 'measure-theory', 'probability-theory']"
1901681,"The set $2^{\Bbb{N}}$ of all functions $f: \Bbb{N} \rightarrow \{0,1\}$ is not countable","This question has been asked several times: The set of all functions from $\mathbb{N} \to \{0, 1\}$ is uncountable? The set of all functions from $\mathbb{N} \to \{0, 1\}$ is uncountable? I just had some follow-up questions that I didn't understand. 
For the first one it says to supposed that $g: \Bbb{N} \rightarrow \{0,1\}$ is a bijection, define $b(k) = 1 - g(k)(k)$, then $b(k)$ is not in the image of $g(k)$ by the ways its constructed but I don't understand why $b(k)$ is an element of $2^{\Bbb{N}}$, I understand that $g(k)$ will evaluate to $0$ or $1$ but $g(k)(k)$ is not necessarily one or zero, it could be $g(20)(20) = 1*20 = 20$ for example. For the second question, it says to show that $F(f) = \{ n \in \Bbb{N} : f(n) = 1\}$ is a bijection $F(f): 2^{\Bbb{N}} \rightarrow P(\Bbb{N})$. I can see how the function is surjective, since by construction for every $n \in \Bbb{N} \ \exists f(n) \in 2^{\Bbb{N}}$ s.t. $f(n) = 1$, but I can not see how $F(f(n))$ is injective, if $f(n) = 0$, then that $n$ doesn't get mapped into $\Bbb{N}$, in this case wouldn't $F(f(n))$ just be the empty set, suppose for a certain $n \in \Bbb{N}$ there exists not function $f$, s.t. $f(n) = 1$, then wouldn't that n never be in $P(\Bbb{N})$. Thanks in advance","['elementary-set-theory', 'functions']"
1901689,What are some interesting un-intuitive problems in probability aside from Monty Hall?,Does anyone know of some interesting and almost strange problems in probability? I know that probability is sometimes notorious for being mind-bending and un-intuitive! (Monty Hall is already an obvious one),"['intuition', 'independence', 'probability-theory', 'probability', 'soft-question']"
1901690,Why headphones get tangled if headphones are in a pocket?,I wonder if there is some mathematical reason that explains why always headphones cords are tangled if they are in a pocket. Thanks in advance.,"['knot-theory', 'general-topology']"
1901799,Proof verification: the product of two continuous functions is continuous,"Prove that if $f, g$ : $X$ → $\mathbb R$ are continuous at $a$ ∈ $\mathbb R$, then $f · g$ is continuous in $a$. If $f$ is continuous at $a$, then $∀ε_f > 0, ∃δ_f > 0$ such that $|x-a| < δ_f$ iff $|f(x) - f(a)| < ε_f$ and if $g$ is continuous at $a$, then $∀ε_g > 0, ∃δ_g > 0$ such that $|x-a| < δ_g$ iff $|g(x) - g(a)| < ε_g$. Now make $|f(x)g(x) - f(a)g(a)|$ = 
$|f(x)g(x) - f(x)g(a) + f(x)g(a) - f(a)g(a)|$ $≤$ 
$|f(x)||g(x)-g(a)| + |g(a)||f(x)-f(a)|$ < $ε_f|g(a)| + ε_g|f(x)|$. (1) Notice that if $|f(x) - f(a)| < ε_f$ then $|f(x)| - |f(a)| < ε_f$, so $|f(x)| < ε_f + |f(a)|$. That implies, from (1) : $|f(x)g(x) - f(a)g(a)| < ε_f|g(a)| + ε_g(ε_f + |f(a)|) = ε_f(|g(a)| + ε_g|f(a)|)$. Now I'm a little lost. From the proofs I've seen, it seems to me that I could simply take $δ = min(δ_g, δ_f)$. Also, since $ε_f$ and $ε_g$ can be made as small as one wants, there will be some δ that satisfies the conditions for continuity. But from what I read, I should explicitly show a δ in terms of ε. Anyway, I reasoned that since $f$ and $g$ are continuous, I can make bounds on $x$ symmetrically* as done to $f(x)g(x)$ and got the following: δ = $δ_f(|a| + δ_g|a|)$ = $δ_f|a|(1 + δ_g)$ so: $|x-a| < δ_f|a|(1 + δ_g)$ iff $|f(x)g(x) - f(a)g(a)| <  ε_f(|g(a)| + ε_g|f(a)|)$ I need to clarify/better define this, but it is basically the notion that making the same operations I made on the bounds of $f(x)g(x)$ in relation to the bounds of $f(x)$ and $g(x)$ individually over the bounds of $x$ for each function, should leave me with the adequate bounds for $x$ on the composite function.","['continuity', 'epsilon-delta', 'real-analysis', 'solution-verification']"
1901807,Showing that $T:V\to V$ has a cyclic vector if its eigenspaces all have dimension one.,"Let $V$ be an $n$-dimensional complex vector space and $T:V\to V$. Suppose that $$\{v\in V: Tv = \lambda v\}$$ has dimension $1$ or $0$ for all $\lambda\in \mathbb{C}$. Show that there exists some $w\in V$ such that $\{w,Tw,\dots, T^{n-1}w\}$ is linearly independent. I tried using the canonical forms to answer this question but I don't think that's the right way to go.","['abstract-algebra', 'linear-algebra']"
1901812,Characterization of ray class fields in terms of ramification alone,"Some background:  when I first heard about ray class fields a year or so ago, I was told that the Hilbert class field of a global field $K$ is the maximal abelian extension of $K$ that is unramified at all (finite and infinite) primes of $K$, and that ray class fields are similar except allowing some ramification at finitely many primes.  This eventually crystallized in my head as a quasi-definition of ray class fields:  ""the ray class field of $K$ with modulus $\mathfrak m$ is the maximal abelian extension of $K$ that is unramified away from $\mathfrak m$, and is allowed to have some restricted ramification at primes dividing $\mathfrak m$.""  Now that I'm learning about ray class fields properly, I'm struggling to formulate precisely what this ""restricted ramification"" is, and I haven't found a reference that does so explicitly. The definition of ray class fields that I'm working with is summarized in section 2.9 of these wonderfully concise notes by Bjorn Poonen:  given a global field $K$ and a modulus $\mathfrak m$, we construct a particular open subgroup $U_{\mathfrak m}$ in the idele group $\mathbb A_K^{\times}$, and let $U_{\mathfrak m}'$ be its image in the quotient $\mathbb A_K^{\times}/K^{\times} = C_K$.  Then this is a finite-index open subgroup, so it corresponds to a finite-index open subgroup of $\widehat{C_K}$, isomorphic to $\mathrm{Gal}(K^{ab}/K)$ via the global Artin homomorphism, which fixes a finite extension $K_{\mathfrak m}/K$ that we call the ray class field. I would like the following to be true: if $\mathfrak{m} = \prod_{\mathfrak p} \mathfrak p^{a_p}$, the ray class field $K_\mathfrak{m}$ is the maximal abelian extension of $K$ that is unramified away from $\mathfrak m$, and that has trivial higher ramification group $G^{a_{\mathfrak p}}$ at finite primes $\mathfrak p$ dividing $\mathfrak m$. (Note the upper numbering on the ramification groups; I previously thought I had a counterexample to this, but I was using lower numbering.)  A friend and I have more or less worked out why this should be true:  from section 1.3 in Poonen's notes, the local Artin map for $K_{\mathfrak p}$ maps the filtration $\mathcal O_{K_{\mathfrak p}}^{\times} \supset 1 + \mathfrak p \supset 1 + \mathfrak p^2 \supset \cdots$ isomorphically onto the higher ramification groups $G^0 \supset G^1 \supset G^2 \supset \cdots$ in $\mathrm{Gal}(K_{\mathfrak p}^{ab}/K_{\mathfrak p})$, so (after using local-global compatibility) the fact that $U_{\mathfrak m}$ contains $1 + \mathfrak p^{a_{\mathfrak p}}$ should be exactly what we need to force the triviality of $G^{a_{\mathfrak p}}$ in $\mathrm{Gal}(K_{\mathfrak m}/K)$. Can someone confirm that this statement is correct, or fix it if not?","['number-theory', 'class-field-theory']"
1901822,Functional Taylor series,"The functional Taylor series of the functinal $f$ about the function $g$ is defined as $f[g+\epsilon\lambda(x)]=f[g_0]+\int dx \frac{\delta f[g_0]}{\delta g(x)}\lambda(x)+\frac{1}{2!}\int dx dx^\prime \frac{\delta^2f[g_0]}{\delta g(x^\prime)\delta g(x)}\lambda(x)\lambda(x') + ... $ (it is assumed that $\lambda(x)=0$ outside the interval in question) but in what sense does this converge? Usually this means that the derivatives of both sides are the same. Following the text here . How does the author derive $D_{\lambda_1}D_{\lambda_2}(f(g))=\int dx dx^\prime \frac{\delta^2f[g_0]}{\delta g(x^\prime)\delta g(x)}\lambda(x)\lambda(x') $ I know how to compute derivatives by definition as described here , or by a way similar to the derivation of Euler-Lagrange equations. Where does the double integral come from in the equation above? I have my own way (maybe it's wrong) to construct such integrals as limiting differentials $$\Delta g=g+\epsilon\lambda-g=\epsilon\lambda$$
$$df=\sum_{i=0}^n\frac{\partial f}{\partial g_i}\Delta g=\sum_{i=0}^n\frac{\partial f}{\partial g(x_0+\epsilon i)}\epsilon\lambda(x_0+\epsilon i)$$
Taking the limit, treating each point as a separate variable, we get
$$df=\int \frac{\partial f}{\partial g(x)}\lambda(x) dx$$
Similarly for second order. But I fail to deduce, how this implies convergence of the Taylor series (first equation). How, or which derivatives match comparing left side with the right side of the equality? I would greatly appreciate any clarification. Please forgive any lapses, I am an amateur exploring his interest.","['functional-analysis', 'functional-calculus', 'calculus-of-variations']"
1901857,How to evaluate the integral $\int \sqrt{1+\sin(x)} dx$,"To find: $$\int \sqrt{1+\sin(x)} dx$$ What I  tried: I put $\tan(\frac{x}{2}) = t$, using which I got it to: $$I = 2\int \dfrac{1+t}{(1+t^2)^{\frac{3}{2}}}dt$$ Now I am badly stuck. There seems no way to approach this one. Please give a hint. Also, can we initially to some manipulations on the original integral to make it easy? Thank you.","['integration', 'trigonometry', 'trigonometric-integrals', 'calculus']"
1901870,very basic question about coordinate geometry,"I am taking a beginner course in machine learning and have confused myself horribly about something. The idea is that there is a line which splits a 2D plane into two distinct regions as shown in the figure: Now the instructor says that anything below the line is greater than $0$ and anything above is less than $0$. Now, it is not clear to me why that should be? Is there an intuition behind why anything below this line should be greater than zero (according to the line equation) and anything above should be less than 0.","['coordinate-systems', 'machine-learning', 'linear-algebra', 'geometry']"
1901873,Showing the norm' is non-negative,"Question:
  Prove that a norm is always non-negative. Assume that the norm is negative. Define:
$f: \mathbb{R}^{n}\rightarrow \mathbb{R}^{-}_{0}$ $v \mapsto \left ( \vec{v} \right )f=\left \| \vec{v} \right \|$ $\left ( \vec{u}+\vec{v} \right )f=\left \| \vec{u}+\vec{v} \right \| \in \mathbb{R}^{-}_{0}$
and 
$\left ( \vec{u} \right )f=\left \| \vec{u} \right \|,\left ( \vec{v} \right )f=\left \| \vec{v} \right \|=\left \| \vec{v} \right \| \in \mathbb{R}^{-}_{0}$ At this point I am unable to progress further. Any help is appreciated.","['normed-spaces', 'functions', 'functional-analysis', 'lp-spaces', 'vector-spaces']"
1901919,Is this function necessarily a polynomial?,Given a function with two characteristics: $$f'(x_0)=f''(x_0)= \cdots =f^{(k-1)}(x_0)=0$$ and $$f^{(k)}(x_0)\ne0$$ for some $k\ge2$ Does this function necessarily have to be a polynomial? Why / why not?,"['polynomials', 'calculus']"
1901975,Symmetric Banded-Toeplitz Matrix Norm and Projector,"Let the following Symmetric Banded-Toeplitz Matrix of size $N$ be defined as
\begin{equation}
\pmb{T}(\pmb{c})
=
\begin{bmatrix}
c_0 & c_1 & c_2 & \cdots  & c_{m-1} & 0 & \cdots  &   0\\
c_1 & c_0 & c_1 & \cdots  & c_{m-2} & c_{m-1} & \cdots  &   0\\
\vdots    &  &     & \ddots & \ddots & & & \vdots \\
0 & \cdots & c_{m-1} & c_{m-2} & \cdots & c_1 & c_0 & c_1\\  
0 & \cdots  & 0 & c_{m-1} & \cdots & c_2 & c_1 & c_0\\  
\end{bmatrix}
\end{equation} Can we bound or say anything about the following: 1) $\Vert \pmb{T}(\pmb{c}) \pmb{x} \Vert^2 \leq $ ?? 2) If $\pmb{P_A} = \pmb{A}(\pmb{A}^{\text{H}}\pmb{A})^{-1}\pmb{A}^{\text{H}}$, can we say anything about $\pmb{P_{\pmb{T}(\pmb{c})A}}$ ??","['matrices', 'linear-algebra', 'discrete-mathematics']"
1901977,How to show the real part of a holomorphic function is harmonic,"So I know that for a real valued function $u$, it's harmonic if it's continuously differentiable and it satisfies $$u_{xx}+u_{yy}=0$$ How do I show that for a generic holomorphic function $f$, that the real part $Re(f)(z)$ is harmonic?","['harmonic-functions', 'analysis', 'functions']"
1901986,(Rudin's) Definition of a harmonic function,"In Chapter 11 of Rudin's RCA, a harmonic function is defined to be a complex continuous function $u$ on a plane open set such that the Laplacian of $u$, i.e. the sum of its pure second-order partial derivatives
$$u_{xx}+u_{yy}$$
is $0$. I was wondering if this was missing a condition, namely that $u_{xy}=u_{yx}$, because I was unable to show the following without this assumption. For every harmonic function $u$ whose domain includes the image of a holomorphic function $f$ in a plane open set $\Omega$, the composite $u\circ f$ is harmonic in $\Omega$. I attempted to show this by just brute calculation, and what remained was  $u_{xy}-u_{yx}$ with some partials of $f$ multiplied on the outside. Of course, Rudin later shows that harmonic functions have continuous partial derivatives of all orders because the real-valued ones are locally real parts of holomorphic functions, but I think his proof relies on a certain composite of functions being harmonic...","['complex-analysis', 'harmonic-functions']"
1901992,Is there an algebraic way to prove this relationship between the roots of a real polynomial and the roots of its derivative?,"Let $$\sum_{0 \leq i\leq n} a_ix^i$$ be  a polynomial (real coefficients) with at least two real roots. Is there an algebraic way to show that for any two roots $k_1, k_2$ of this polynomial, the polynomial $$\sum_{1 \leq i\leq n} i \cdot a_ix^{i-1} $$ admits at least one root $c$ satisfying $k_1 <c < k_2$? Analytically, this is of course a consequence of Rolle's theorem. Edit: ""Algebra"" is as broad as you want it to be. Elementary or abstract. The completeness of $\mathbb{R}$ is essential, so it won't be purely algebraic. I was mainly hoping for something without derivatives.","['abstract-algebra', 'roots', 'polynomials']"
1902009,Proof of $(K_1+K_2)^* = K_1^*\cap K_2^*$: the dual of sum of convex cones is same to the intersection of duals of convex cones,"Let $K\subset R^n$ be a cone: $\lambda x \in K$ if $x \in K$ and $\lambda>0$. Let $K^*$ be the conjugate cone of $K$, defined as $K^* = \{x^* | \langle x, x^*\rangle \ge 0, \forall x \in K\}$. I am trying to prove the next theorem and do not understand why convexity is needed for $K_1$ and $K_2$. Let $K_1$ and $K_2$ be convex cones in $R^n$. Then
  \begin{equation}
(K_1+K_2)^* = K_1^*\cap K_2^*.
\end{equation} My trial is as follows: If $x^* \in K_1^*\cap K_2^*$, then $\langle x_1+x_2, x^*\rangle = \langle x_1,x^*\rangle + \langle x_2,x^*\rangle \ge 0$ for every $x_1 \in K_1$ and $x_2 \in K_2$, so $x^* \in (K_1+K_2)^*$ and $K_1^* \cap K_2^* \subset (K_1 + K_2)^*$. Suppose $x^* \in (K_1+K_2)^*$ but $x^* \notin K_1^*$. Then, there exists $x_1 \in K_1$ such that $\langle x_1, x^*\rangle < 0$. Since $K_2$ is a cone, $\lambda x_2 \in K_2$ for every $x_2 \in K_2$ and $\lambda>0$. Thus, $\langle x_2,x^*\rangle$ can be arbitrarily small and there exists $x_2 \in K_2$ such that $\langle x_1,x^*\rangle + \langle x_2,x^*\rangle < 0$. It contradicts the assumption that $x^* \in (K_1+K_2)^*$. Therefore, if $x^* \in (K_1+K_2)^*$, then $x^*\in K_1^*\cap K_2^*$ and $(K_1+K_2)^* \subset K_1^*\cap K_2^*$. It completes the proof. Where should I use convexity of $K_1$ and $K_2$ or can I remove convexity from the Theorem?","['real-analysis', 'convex-optimization', 'convex-analysis', 'analysis']"
1902030,"Prove that $\sum\limits_{j=k}^n\,(-1)^{j-k}\,\binom{j}{k}\,\binom{2n-j}{j}\,2^{2(n-j)}=\binom{2n+1}{2k+1}$.","In an attempt to answer this thread , I discovered an identity involving binomial coefficients.  However, I am not able to find a proof.  All tricks are welcome. Let $n$ and $k$ be nonnegative integers with $k\leq n$.  Prove that $$\sum\limits_{j=k}^n\,(-1)^{j-k}\,\binom{j}{k}\,\binom{2n-j}{j}\,2^{2(n-j)}=\binom{2n+1}{2k+1}\,.$$","['binomial-coefficients', 'complex-analysis', 'combinatorics', 'summation', 'analytic-combinatorics']"
1902124,Does this recurrence have a closed form limit $x_{n+1}=x_n-\frac{a}{3^{2n+1}x_n}$?,"I have a first order nonlinear recurrence relation: $$x_{n+1}=x_n-\frac{a}{3^{2n+1}x_n}$$ Here $a,x_0$ are positive constants and $a<x_0$. (Also $x_0=A+B$ and $a=(A−B)^2$, for some $A,B>0$). For these conditions the recurrence quickly converges to a certain limit, which depends only on $a,x_0$: $$\lim_{n \to \infty}x_n=X(a,x_0)$$ I don't know if this limit has closed form or not, and there is no general method for dealing with nonlinear recurrence relations. Can $X(a,x_0)$ have a closed form and how to obtain it? I don't need the explicit expression for $x_n$, only the limit. I tried to turn it into a differential equation, but I don't know if I've done it correctly, and how the solution to the ODE relates to the original problem: $$x_{n+1}-x_n=-\frac{a}{3^{2n+1}x_n}$$ $$\frac{df(t)}{dt}=-\frac{b}{3^{2t} f(t)}$$ $$\frac{1}{2} f^2=\frac{b}{2\ln 3} 3^{-2t}+C$$ $$f(t)=\sqrt{\frac{a}{3\ln 3} 3^{-2t}+C}$$ If I set: $$x_n=\sqrt{\frac{a}{3\ln 3} 3^{-2n}+C}$$ I get: $$C=x_0^2-\frac{a}{3\ln 3}$$ $$\lim_{n \to \infty}x_n=\sqrt{C}=\sqrt{x_0^2-\frac{a}{3\ln 3}}$$ But that's not correct. Does this work only with linear recurrences? I have also inverted the recurrence: $$x_n=\frac{1}{2} \left(x_{n+1}+\sqrt{x_{n+1}^2+\frac{4a}{3^{2n+1}}} \right)$$ This works correctly, but I'm not sure how it may help.","['recurrence-relations', 'asymptotics', 'sequences-and-series']"
1902132,"""Vanishing inner product implies orthogonality"" Is it a definition or theorem?","The definition of inner product is $\langle u,v\rangle=u_1\bar{v_1}+\cdots+u_n\bar{v_n}$ . Two vectors $u,v \in V$ are said to be orthogonal if $\langle u,v\rangle=0$ , where $V$ is complex vector space. But why is it true? A lot of books just present this fact without giving any proof. Why such an expression $u_1\bar{v_1}+\cdots+u_n\bar{v_n}=0$ will lead to the fact that $u$ and $v$ are orthogonal? Is it an intuition behind the expression $u_1\bar{v_1}+\cdots+u_n\bar{v_n}$ that can explain orthogonality? I read some proof about it but they don't seem correct. For example, some said that $\langle u,v\rangle=|u||v|\cos\theta$ and thus $\theta=90^\circ$ . But why are they equivalent? Some use the Pythagoras theorem, $$\begin{array}{rl}
|u+v|^2 &=\langle u+v,u+v\rangle\\
&=|u|^2+2\langle u,v\rangle+|v|^2
\end{array}$$ So dot product $= 0$ implies orthogonality, but this only works for real vector space. So why exactly inner product $= 0$ implies orthogonality?","['linear-algebra', 'inner-products']"
1902173,How do I solve this ODE: $\frac{d^N y}{dx^N}=x^L y^M$,"I have found a way to solve some unspecified equations of this type using Lie Theory:
$$\frac{d^N y}{dx^N}=x^L y^M$$ The answer is
$y=\bigg[\beta (\beta -1)(\beta-2)...(\beta-N+1)\bigg]^{\frac{1}{M-1}}x^{\beta}$ with $\beta = \frac{L+N}{1-M}$ I'll post the step-by-step solution in a few days, but I was wondering: aside from group theory, does anybody know another way to solve this problem? I promised I would post the solution, so here it is. In 1886 Sophus Lie stated that if a differential equation is invariant under the transformation of an infinite continuous group (Lie group), it can be rewritten as a function of the stabilizers of that group.  Lie also discovered that the relationships between the stabilizers could be found by taking their derivatives, and that these relationships, expressed in canonical form, could be used to solve the DEQ.  Dresner, Olver and others have noted that when the general solution cannot be derived, a special solution may often be found algebraically.  It was this that prompted me to propose and solve the above equation, and having solved it I began to wonder whether or not anybody else had heard of the technique. (I've used it before, but never on anything so generalized as this DEQ.) Allow primed values to be the transformed variables, such that $x'=\lambda x$ and $y'=\lambda^\beta y$.  The unit conversion for this group occurs when $\lambda =1$. (Values of x' form a subgroup of y', and proving that all values of y' satisfy group properties is a trivial exercise.) Then $dx'=\lambda dx$, $dy'=\lambda^\beta dy$.  The transformed DEQ is 
$$\frac{d^N (\lambda^\beta y)}{(\lambda dx)^N}=(\lambda x)^L (\lambda^\beta y)^M$$
For invariance, $\lambda^{\beta -N}=\lambda^{L+\beta M}$ or $\beta = \frac{L+N}{1-M}$.  Once $\beta$ is found, invariance is assured. To find the stabilizers, first find the coefficients of the infinitesimal transformations and then use the method of characteristics.  For convenience, use Newton's notation such that $\dot{y}=\frac{dy}{dx}$, $\ddot{y}=\frac{d^2 y}{dx^2}$, etc.  Then $\dot{y}'=\lambda^{\beta -1}\dot{y}$, $\ddot{y}'=\lambda^{\beta -2} \ddot{y}$, $\dddot{y}'=\lambda^{\beta -3}\dddot{y}$ .... $\frac{\partial x'}{\partial \lambda}\big|_{\lambda_o
=1}=x$, $\frac{\partial y'}{\partial \lambda}\big|_{\lambda_o
=1}=\beta y$, $\frac{\partial \dot{y}'}{\partial \lambda}\big|_{\lambda_o
=1}=(\beta-1) \dot{y}$, $\frac{\partial \ddot{y}'}{\partial \lambda}\big|_{\lambda_o
=1}=(\beta-2) \ddot{y}$, $\frac{\partial \dddot{y}'}{\partial \lambda}\big|_{\lambda_o
=1}=(\beta-3) \dddot{y}$, ..... 
$$d\lambda=\frac{dx}{x}=\frac{dy}{\beta y}=\frac{d\dot{y}}{(\beta -1) \dot{y}}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}}=\frac{d\dddot{y}}{(\beta -3) \dddot{y}}=....$$ Integrating $\frac{dx}{x}=\frac{dy}{\beta y}$ produces $S_o =\frac{y}{x^\beta}$ where $S_o$ is a constant of integration, and constants are stabilizers for the group of polynomials . This is easily shown to be true by applying the group transformation to the stabilizer: it does not change. $$S_o'=\frac{y'}{(x')^\beta}=\frac{\lambda^\beta y}{\lambda^\beta x^\beta}=S_o$$
Likewise, $\frac{dx}{x}=\frac{d\dot{y}}{(\beta -1) \dot{y}} \rightarrow S_1 =\frac{\dot{y}}{x^{\beta -1}}$, another stabilizer.  The next one is $\frac{dx}{x}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}} \rightarrow S_2 =\frac{\ddot{y}}{x^{\beta -2}}$.  According to Lie there are an infinite number of stabilizers following the same pattern, so we can assume that $$S_{N-1}=\frac{\overset{(N-1)\bullet}{y}}{x^{\beta -N+1}}$$ $$S_N=\frac{\overset{N \bullet}{y}}{x^{\beta -N}}$$ Unless you are in error, at this point the DEQ may be rewritten in stabilizer form.  This is accomplished by dividing the left side by $x^{\beta -N}$ and dividing the right side by $x^{L+\beta M}$ (remembering that $\beta -N = L+ \beta M)$.  $$\frac{\overset{N \bullet}{y}}{x^{\beta -N}}=\frac{x^L y^M}{x^L y^{\beta M}}=\big(\frac{y}{x^\beta}\big)^M$$ $$S_N=S_o^M$$ Lie asserts that by taking the derivatives of these stabilizers we can find the canonical relationships between them.  With a little thought it is easy to see that $$x\frac{dS_o}{dx}=S_1-\beta S_o$$ $$x\frac{dS_1}{dx}=S_2 -(\beta -1)S_1$$ $$....$$  $$x\frac{dS_{N-1}}{dx}=S_N - (\beta -N+1)S_{N-1}$$ Within the direction field of the DEQ, singularities, saddle points and the separatrices that connect them are found in places where the above derivatives are all set equal to zero.  It is here that the special solutions are found.  Thus, $$S_1=\beta S_o$$ $$S_2=(\beta -1)S_1=\beta (\beta -1)S_o$$ $$....$$ $$S_N=\beta (\beta -1)(\beta -2)...(\beta -N+1)S_o$$ Substitute this last equation into the transformed DEQ and with a little algebraic effort you end up at the given solution. I really enjoy using this method because it exploits a group property of polynomials themselves, of which DEQ's are a subgroup.  It also turns the traditional numerical ""shooting method"" into a back-of-the-envelope calculation. Sophus Lie was a genius.  I highly recommend his book ""Differential Invariants.""  Lie's style is a bit arcane and some modern rigor and nomenclature is missing, but the struggle is well worth it.  Many of his ideas have not been touched since his death.","['ordinary-differential-equations', 'lie-algebras']"
