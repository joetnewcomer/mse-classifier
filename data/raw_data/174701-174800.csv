question_id,title,body,tags
3121684,Intuition of a Smooth Curve,"I'm currently taking an intro to Differential Geometry course, and am having trouble with the definition of a smooth curve. If you consider the curve $\lambda(t) = \left (\cos^3(t),\sin^3(t)\right )$ then clearly each of $x(t)$ and $y(t)$ are infinitely differentiable, and so by definition $\lambda(t)$ should be smooth. However plotting the curve shows that its graph has 4 cusps, and doesn't seem to be 'smooth' in any way that makes sense to me. Could somebody clarify for me the intuition behind what makes a curve smooth?","['smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3121694,Sets that are both Sum-free and Product-free,"Let $P_o$ be the primes excluding $2$ . $P_o \subset \mathbb{N}$ has the following property $Q$ : For any $a,b \in P_o$ , $a + b \not\in P_o$ . For any $a,b \in P_o$ , $ab \not\in P_o$ . So both addition and multiplication necessarily leave the set $P_o$ . $P_o$ has natural density $0$ . Q1 . Is there a set $S \subset \mathbb{N}$ with positive density
  that satisfies property $Q$ ? Answered quickly by @JoséCarlosSantos: Yes .
Permit me then to add a new question: Q2 . What is largest density $S \subset \mathbb{N}$ that satisfies property $Q$ ? Santos's example has density $\frac{1}{3}$ .","['number-theory', 'examples-counterexamples', 'prime-numbers']"
3121715,Strangely but closely related parametrized curves,"Compare the following two parametrized curves for $k \in \mathbb{N}^+$ : $$x_r(t) = \cos(t)(1 + r\sin(kt))$$ $$y_r(t) = \sin(t)(1 + r\sin(kt))$$ with $0 \leq t < 2\pi$ and $0 \leq r \leq 1$ (being the plot of the sine function with amplitude $r$ over the circle instead of the real axis ) and $$X_R(t) = R\cos(t/2)\sin(kt/2)$$ $$Y_R(t) = R\sin(t/2)\sin(kt/2)$$ (which give Grandi roses ) with $R = 2\sqrt{r}$ . Find depicted the outer (""sine"") curve $(x_r,y_r)$ thinner, the inner (""rose"") curve $(X_R,Y_R)$ thicker, and the argument circle (for $t$ ) only looming. The curves are coloured according to the argument $t$ that gives rise to them, the color ranging from black (for $t=0$ ) over red (for $t=\pi/2$ ), white (for $t=\pi$ ) and blue (for $t=3\pi/2$ ) back to black (for $t = 2\pi$ ). Here for $k=1,3,5,7$ : My questions are: Why doesn't this work for even $k$ ? What happens when $r = 1$ , that is when also $(x_r,y_r)$ exhibits an $k$ -fold intersection point – like $(X_R,Y_R)$ always does? Especially: 
  How are the curves $(x_1,y_1)$ and $(X_2,Y_2)$ related (topologically)? Which other pairs of curves $(x,y)$ , $(X,Y)$ behave in a similar way? For $r < 1$ the curves $(x_r,y_r)$ and $(X_R,Y_R)$ are obviously not homeomorphic. On the other hand $(x_1,y_1)$ and $(X_2,Y_2)$ are homeomorphic as point sets – but not as parametrized curves, because there is no continuous bijection $f: [0,2\pi] \rightarrow [0,2\pi]$ such that $x_1(t) = g(X_2(f(t)))$ , $y_1(t) = g(Y_2(f(t)))$ with $g$ the homeomorphism that maps the two curves as points sets. Is this the right way to say it – ""homeomorphic as point sets, but not as parametrized curves"" – and is that all there is to say? To see what goes wrong for even $k$ , find here the cases $k=2,4,6$ . I didn't try to ""fix"" them:","['number-theory', 'curves', 'visualization', 'geometry', 'general-topology']"
3121724,Computing the index $\left(\mathbb Z\left[\frac{1+\sqrt{5}}{2}\right]:\mathbb Z \left[\sqrt{5}\right]\right)$?,"If $\mathcal O=\mathbb Z\left[\frac{1+\sqrt{5}}{2}\right]$ , I'd like to show that $\left(\mathcal O:\mathbb Z \left[\sqrt{5}\right]\right)=2$ . It's to show that Dedekind's factorisation theorem doesn't apply to the prime 2. I feel like this should be some basic computation and I'm just over thinking it/forgetting some of my algebra, but what is the best way to do this? I tried to define a map from $\mathcal O$ to $\mathbb F_2$ with kernel $\mathbb Z\left[\sqrt{2}\right]$ but things weren't working out. I was also thinking of looking at these things as rank 2 $\mathbb Z$ -modules, but this seemed like overkill maybe.  Thanks.","['quotient-group', 'algebraic-number-theory', 'abstract-algebra']"
3121738,Is the set of points where two measurables functions are equal measurable?,"Let $(\Omega,\mathscr{F},P)$ be a measure space and $T$ be a metric space and $f,g:\Omega\rightarrow T$ be measurable functions. Then, is the set $\{w\in \Omega:f(w)\neq g(w)\}$ measurable? I could prove this for the case $T$ is a separable topological vector space, but I am not sure for the case when $T$ is a metric space. EDIT Let $d$ be a metric on $X$ . Since $d$ is continuous, $d$ is ( $\mathscr{B}_{X\times X}, \mathscr{B}_\mathbb{R}$ )-measurable. Since $f,g$ are measurable, the map $F(w):=(f(w),g(w))$ is measurable with respect to $(\mathscr{F},\mathscr{B}_X\otimes \mathscr{B}_X)$ . My point is that $d\circ F$ need not be measurable. To see this, pick $A\in \mathscr{B}_\mathbb{R}$ . Then, $d^{-1}(A)\in \mathscr{B}_{X\times X}$ . If $d^{-1}(A)\in \mathscr{B}_X \otimes \mathscr{B}_X$ , then $F^{-1}(d^{-1}(A))\in \mathscr{F}$ so we are done. However , $d^{-1}(A)$ need not be in $\mathscr{B}_X \otimes \mathscr{B}_X$ without the separability condition , because $\mathscr{B}_X \otimes \mathscr{B}_X \neq \mathscr{B}_{X\times X}$ in general.","['measure-theory', 'real-analysis']"
3121772,Constructing the flat vector bundle associated to a given linear representation of the fundamental group,"I'm reading this notes , and I have some questions about the contruction on page 18. Let $M$ be a connected manifold and $E\rightarrow M$ a flat vector bundle over $M$ . Consider $\{(U_\alpha, \phi_{U_\alpha})\}$ a flat atlas of $E\rightarrow M$ , i.e the transition functions $g_{U_\alpha, U_\beta}$ associated to the trivializations are locally constants.
There is a canonical way to associate a representation $\rho:\pi_{1}(M, m_0)\rightarrow \mathrm{GL}(d, \mathbb{K})$ to this flat vector bundle as follows: Fix an open set $U$ containing $m_0$ , such that $U$ is in the atlas. For any loop $\gamma: [0, 1] → M$ , $γ(0) = γ(1) = m_0$ there are a subdivision $0 = t_0 ≤ t_1 ≤ · · · ≤ t_N ≤ t_{N+1} = 1$ of $[0, 1]$ and open sets $U_0, \dots, U_N$ in the atlas, such that $U_0 = U_N = U$ and, for all $i=0, \dots, N$ , $\gamma([t_i, t_{i+1}])\subset U_i$ . then, define: $$
\rho(\gamma)=g_{U_N ,U_{N−1}}(γ(t_N))\dots g_{U_2,U_1}(γ(t_2))g_{U_1,U_0}(γ(t_1))
$$ The question is about the proof that the following map is sujective: $$
\frac{\{\mbox{flat bundles of rank $d$ over $M$}\}}{\mbox{isomorphism}}\rightarrow \frac{\mathrm{Hom}(\pi_{1}(M, m_0), \mathrm{GL}(d, \mathbb{K}))}{\mbox{conjugacy}}
$$ i.e, given a linear representation $\eta$ of $\pi_{1}(M, m_0)$ , I need to find a flat vector bundle $E_\eta$ , such that the associeted representation $\rho$ described later is equal to $\eta$ . Here is the construction: Let $\eta : π_1(M, m_0)→\mathrm{GL}(d,\mathbb{K})$ be a morphism, then the trivial bundle $\widetilde{M} × \mathbb{K}^d$ over the universal cover $\widetilde{M}$ of $M$ has an obvious flat structure together with an action of $π_1(M, m_0)$ preserving the flat structure: $$
γ · (\tilde m, v) = (γ\tilde m, \eta(γ)v)\quad ∀γ ∈ π_1(M, m_0), \tilde m \in M, v \in \mathbb{K}^{d}.
$$ Here $ \tilde m→ γ\tilde m$ is the natural action of $π_1(M, m_0)$ on the universal cover $\widetilde{M}$ . The quotient of the trivial bundle $\widetilde{M} × \mathbb{K}^d$ by this action is denoted $E_\eta$ and
  it is naturally a flat bundle over the base $M$ . I'm trying to find the trivizalizations of $E_\eta$ . Does any one knows how to find it? Any hints?","['connections', 'vector-bundles', 'fundamental-groups', 'differential-geometry']"
3121775,Heesch numbers in 3D,"At the Tiling Database there are 3, 20, 198, 1390 ( A054361 ) non-tiling polyominoes of order 7 to 10. In 3D, solids with a particular Heesch number don't seem to be well known. Glenn Rhoads found the minimal Heesch 2 polyomino in his 2003 thesis ""Planar Tilings and the Search for an Aperiodic Prototile"". If this polyform is modified to be a polycube, can it fill space? If not, the 3D Heesch number is at least 2 and possibly 3. That basically uses layering of an existing 2D Heesch object. What is the Heesch number for this polycube? If thickened 2D Heesch objects are excluded, what are the simplest polyhedra, polyforms, and shapes which have 3D Heesch numbers 1, 2, 3? For a 3x3x3 cube, add an outie 1x1x1 cube on one face center and remove innie 1x1x1 cubes from centers of other faces.  Stack 27 of these with the outie up.  Rotate four of them to fill the center innies.  The center shape is completely surrounded, showing a purely 3D Heesch-1 object. Here is a Heesch-2 object that likely gets weird in 3D.","['3d', 'tiling', 'geometry', 'recreational-mathematics', 'open-problem']"
3121779,What distribution has the probability generating function $\mathbb E[s^X]=\left(\frac{1-\delta P}{1-sP}\right)^{\frac{1-s}{\delta-s}}$?,"Is it possible to obtain the probability mass function for the discrete random variable $X$ associated with the following probability generating function? $$
\mathbb E[s^X]=\left(\frac{1-\delta P}{1-sP}\right)^{\frac{1-s}{\delta-s}}
$$ In the limit $\delta\to1$ , this is the generating function of the Geometric( $P$ ) distribution, which models the number of successes before the first failure given a success probability $P$ . It would be great to obtain the distribution of $X$ for any $0<\delta<1$ , or at least some sort of asymptotic expansion about $\delta\approx 1$ . Any ideas would be very much appreciated!","['statistics', 'probability-distributions', 'probability', 'generating-functions']"
3121781,Dot product with a constant,"What is $\vec{a}\cdot (\vec{b}+c)$ , where $c$ is a constant? Am I able to just take out the constant, or do I multiply the constant by the vector as a scalar? EDIT (for context): I was finding the work done on a particle moving on the curve $C$ : $x=cos(t)$ , $y=sin(t)$ , from $2\pi \leq t \leq 0$ , through the vector field $\vec{F}(x,y)= -y\hat{i} +x\hat{j}$ . I found that the work done is $-2\pi$ . I wanted to figure out how that work changes when that same circle is moved horizontally by $n$ units, so that $x=cos(t)+n$ , where $n$ is a constant. Here’s my work which led me to this issue: $$\oint_{C} \vec{F} \cdot d\vec{r}$$ $$\oint_{2\pi}^{0} (-sin(t)\hat{i} +(cos(t) +n)\hat{j}) \cdot (-sin(t)\hat{i} +cos(t)\hat{j}) dt$$ But I didn’t know how to cross $(cos(t)+n)$ with $(cos(t)$","['abstract-algebra', 'vectors']"
3121812,non isomorphism between $A^1$ and zero set of $y^2-x^3=0$'s coordinate ring by cohomology argument?,"Consider $A_k^1=\mathrm{Spec}(k[s])$ and $V=\mathrm{Spec}(k[x,y]/(y^2-x^3))$ . Suppose $\mathrm{char}(k)\neq 2,3$ . There are various ways (dimension of tangent space, integral closure and constructing ring homomorphism...) to show $A^1$ not isomorphic to $V$ as schemes. Q: Is there a cohomological argument to show $A^1$ not isomorphic to $V$ ? I know $A^1$ is normalization of $V$ for sure. My guess is that ring $K(-)$ functor will identify the difference between the two schemes. $K(k[s])$ should be isomorphic to $Z$ as every f.g. projective is free. What is $K(k[x,y]/(y^2-x^3))$ then? What are other alternatives to see two schemes different?","['algebraic-geometry', 'abstract-algebra']"
3121819,Convex function attains maximum at extreme point,"So I am working on the proof that a convex continuous function $f$ on a convex and compact set $X$ attains its maximum at an extreme point of $X\subset\Omega$ where $\Omega$ is a locally convex space and there is a small part that is tripping me up. So far I have: Suppose, for contradiction, that $f$ attains its max at a point that is not extreme call it $x_0$ . So there exists $x_1,x_2\in X$ distinct and $t\in(0,1)$ s.t. $x_0=tx_1+(1-t)x_2$ . This means by the convexity of $f$ that $$f(x_0)\leq tf(x_1)+(1-t)f(x_2)\leq\max\{f(x_1),f(x_2)\},$$ which yields a contradiction when the strict inequality holds. My problem is getting the result when equality holds. I still haven't used continuity of $f$ or compactness of $A$ and so I am thinking those must come into play here, but I can't see how.","['locally-convex-spaces', 'functional-analysis']"
3121836,"$f: \mathbb R \to \mathbb R, f\in C^1$ then $f$ perserves measurability","There's some ambiguity around these terms so I'll be as clear as I can. A set $A \subseteq R$ is said to be negligible if for all $\epsilon > 0$ there are boxes $Q_1, Q_2, \dots$ such that $A \subset \bigcup Q_i$ and $\sum V(Q_i) < \epsilon$ A set $E \subseteq \mathbb R$ is said to be Jordan measurable if $\partial E$ is negligible. The proposition I wish to prove is that if $f: \mathbb R \to \mathbb R$ is $C^1$ and $E \subset \mathbb R$ is Jordan measurable then $f(E)$ is also Jordan measurable. What I tried: Not sure if this is the correct direction. Let $\epsilon > 0$ . $E$ is Jordan measurable so there are closed intervals $I_1, I_2, \dots$ such that $\partial E \subset \bigcup I_k$ and $\sum V(I_k) < \epsilon$ . $f$ is $C^1$ on $I_k$ and so it's Lipschitz there. We know that Lipschitz functions map negligible sets to negligible sets, and so $f(I_k)$ is negligible for all $k$ . It is clear to see now that $f(\partial E) \subset\bigcup f(I_k)$ is negligible, since it's a countable union of negligible sets. But we weren't asked about $f(\partial E)$ . We were asked about $\partial f(E)$ . Is there any relation between the two? If $f$ is a diffeomorphism, then I know that they are equal. But can we say anything when $f$ is not know to be invertible, but rather just $C^1$ ?","['calculus', 'lipschitz-functions', 'measure-theory']"
3121841,Derivative of $a^x$ where $a \in \mathbb{R}^+$,"I work in a University Math Tutor Lab, and recently had a student ask for help taking the derivative of $a^x$ where $a \in \mathbb{R}$ . I know one can manipulate it into a pretty straightforward Chain Rule, \begin{align*}
(a^x)'= \left(e^{\ln(a^x)}\right)'=\left(e^{x\ln(a)}\right)'= \ln(a)\,e^{\ln(a^x)}=\ln(a)\,a^x.
\end{align*} However, I was hoping someone could provide an intuitive explanation to give students why they should expect $\ln(a)$ to be in their derivative rather than just memorizing the process. (I know one could say that in a similar fashion the derivative of $e^x$ is $\ln(e)\, e^x=e^x$ , but I don't see this as giving students a helpful way to remember that the derivative of $a^x$ has $\ln(a)$ in it.)","['calculus', 'derivatives']"
3121858,"Why is an angle defined as the ratio between the arc length and the radius, and why is arc length defined as the product of the radius and an angle?","I'm having a hard time understanding why $\theta = \frac{s}{r}$ and $s = \theta r$ where $s$ is the arc length, $r$ is the radius, and $\theta$ is the angle measure. I understand how one is derived from the other through algebraic manipulation. I'm trying to understand intuitively : 1) Why dividing some amount of the circle (arc length) by the length of its radius would dictate how much an angle $\theta$ ""opens up."" Since it's a ratio of two lengths, then why would the result be an angle measure? 2) Why multiplying the radius by an angle measure would give the arc length. I'm having a hard time grasping this one because the angle measure and the radius seem like (for lack of a better term) two different ""units"" to me. An angle $\theta$ is a measure of angular rotation whereas the radius is a length, so how does multiplying the two then give back another length i.e. the arc length?","['algebra-precalculus', 'trigonometry']"
3121869,Integral $\int_0^{2π} e^{e^{ix}} dx$,"Work out  the integral $$\int_0^{2π}  e^{\large e^{ix}} \, dx.$$ I am now stuck with this for $2$ days, so please help! Here is my try: $$I=\int_0^{2π}  e^{\large e^{ix}} dx=\int_0^{2\pi}  e^{\large{\cos x+i\sin x}}dx$$ $$=\int_0^{2\pi}e^{cos x}\left(\cos(\sin x) +i\sin(\sin x)\right) dx$$ $$\overset{\large2\pi -x \rightarrow x}=\int_0^{2\pi} e^{\cos x} \left(\cos (\sin  x)+i\sin(\sin(2\pi- x)\right)dx$$ $$\Rightarrow 2I=\int_0^{2\pi} e^{{\cos x}} \cdot 2\cos (\sin x)dx$$ $$\text{as}\ \sin(2\pi -x )=-\sin x$$ $$\Rightarrow I=\int_0^{2\pi} e^{\cos x}\cos (\sin x)dx$$ $$=\int_0^{2\pi} \left(e^{\cos x}\cos (\sin x)+e^{\cos (\pi-x)}\cos (\sin x)\right)dx$$ $$=\int_0^{\pi} 2 \left(\frac{e^{\cos x}+e^{-\cos x}}{2}\right)\cos(\sin x)dx$$ $$=2\int_0^\pi \operatorname{cosh}(\cos x)\cos(\sin x)dx $$","['integration', 'calculus', 'definite-integrals']"
3121896,"what property of a coin system makes it ""canonical""","A canonical coin system is defined as one where the greedy algorithm is the optimum algorithm for the change-making problem ( https://en.wikipedia.org/wiki/Change-making_problem ). What can I do to build a ""canonical"" coin system without just checking all possible outputs? Is it that each coin is a multiple of some other number? Or that no coin is larger than a certain ratio of a smaller coin?","['combinations', 'combinatorics']"
3121905,Numerical solution of ODE with Delta function,"I want to model a dynamical system of the form $\frac{\text{d}x}{\text{d}t} = f(x)+nx\delta(\pi(t-0.2)).
$ The problem is that I have a point source which is reoccurring at fixed time steps (say at 0.2,1.2,2.2...). How can I handle this numerically? 
I have tried to figure out solutions and it seems that there are two ""easy"" approaches: 1) Consider a grid with a fixed step size and define your delta function as 0 on all grid points except the relevant points. However, the influence of the delta function depends strongly on the step size solving this numerically which is why I think this is not correct. 2) Use a Gaussian to represent the Delta-function. This could also work for adaptive step sizes. However, in this case the results depend on the variance of the Gaussian which should be small. Is this a better approach?
If this approach works, I could generate an array consisting of the Gaussians at different time steps which is zero elsewhere and use this with normal ODE-solvers, right? The third approach is a little bit nasty in the sense that it cannot be used with ""normal"" ODE solvers. It would be to evaluate f(x) until we approach the point source and take $y(+\epsilon)=e^ny(-\epsilon)$ ( Numerical way to deal with Dirac delta. ). This does not depend on the variance or step size. Should I go with this one? I could use normal ODE-solvers until the fixed time step and then just take the exponential. And do you have any references on that? Thank you very much for your help!","['numerical-methods', 'dirac-delta', 'ordinary-differential-equations']"
3121938,Maximum Likelihood Estimate with different parameters,"Suppose that X and Y are independent Poisson distributed values with means $\theta$ and $2\theta$ , respectively. Consider the combined estimator of $\theta$ $$
\tilde{\theta} = k_1 X + k_2 Y
$$ where $k_1$ and $k_2$ are arbitrary constants. Find the condition on $k_1$ and $k_2$ such that $\tilde{\theta}$ is an unbiased estimator of $\theta$ . For $\tilde{\theta}$ unbiased, show that the variance of the estimator is minimized by taking $k_1 = 1/3$ and $k_2 = 1/3$ . Given observations $x$ and $y$ find the maximum likelihood estimate of $\theta$ and hence show that $\tilde{\theta}$ is also the maximum likelihood estimator. I have gotten (1) and (2) okay, but it's (3) I am having trouble with, I'd be okay if $X$ and $Y$ had the same parameter but I'm having trouble with $X$ and $Y$ having different parameters, any help would be appreciated. NOTE For (1) I got $k_1 = 1 - 2k_2$ . For (2) I found the variance of $\tilde{\theta}$ , then differentiated and let equal to zero to minimize - therefore we get (after subbing in $k_2 = 1 - k_1/2 $ ) $$3k_1-1=0,$$ which when subbing in $1/3$ , we see it is minimised. Thank you.","['statistical-inference', 'statistics', 'poisson-distribution', 'estimation', 'maximum-likelihood']"
3121957,Show that convolution of two $L^1(\mathbb{R})$ functions is continuous,"Suppose $f, g \in L^1(\mathbb{R})$ . I want to show that their convolution is continuous. I can show continuity if one of the functions were in $L^\infty(\mathbb{R})$ . I have tried to approximate one of the functions, say $f$ with $f_M = f\cdot \mathbb{I}\{ |f(x)| \leq M\}$ . But I have problem bounding the residual, which would be of form: $$ \int_\mathbb{R} |f(x-z) - f_M(x-z)|\cdot |g(z)| dz$$ Any help/hint would be greatly appreciated!","['convolution', 'functional-analysis']"
3121963,"Disprove or prove using delta-epsilon definition of limit that $\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0$ [duplicate]","This question already has answers here : What is $\lim_{(x,y)\rightarrow(0,0)} \frac{x^3-y^3}{x^2-y^2}$? (2 answers) Find $\lim \limits_{(x,y) \rightarrow (0,0)}\frac{x^3 y^3 }{x^2+y^2}$ (epsilon delta proof) (1 answer) Closed 5 years ago . I want to prove if the following limit exists, using epsilon-delta definition, or prove it doesn't exist: $$\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0$$ My attempt: 
First I proved some directional limits, like for $y=mx$ , and $y=ax^n$ , and for all of them I got 0. So I conjectured that this limit exists and it's 0. Then I have to prove: $$\forall \delta \gt 0 : \exists \epsilon \gt 0 : \|(x,y)\| \lt \epsilon \rightarrow \left| \frac{x^3-y^3}{x^2-y^2}\right| \lt \delta$$ First I noted that $\frac{x^3-y^3}{x^2-y^2} = \frac{(x^2+xy+y^2)(x-y)}{(x+y)(x-y)} = \frac{x^2+xy+y^2}{x+y}$ . Then I did $\left|\frac{x^2+xy+y^2}{x+y}\right| \leq \left|\frac{x(x+y)}{x+y}\right|+\frac{y^2}{\vert x+y\vert} = \vert x \vert + \frac{y^2}{\vert x+y\vert}$ Using $\|(x,y)\| = \vert x\vert + \vert y\vert$ and assuming $\|(x,y)\| \lt \epsilon$ $\vert x\vert + \vert y\vert  = \vert x \vert + \frac{ y^2}{\vert y \vert} \geq \vert x \vert+\frac{y^2}{\vert y\vert+\vert x\vert}$ but I can't continue from that since $\vert x + y \vert \leq \vert x \vert + \vert y \vert$ . I don't know what else to try.","['limits', 'multivariable-calculus', 'real-analysis']"
3121971,Is there a name for a square matrix with constant diagonal and off-diagonal elements?,"I am interested in real symmetric matrices of the form: $$\mathbf{M} = \begin{bmatrix}
a & t & t & \cdots & t \\
t & a & t & \cdots & t \\
t & t & a & \cdots & t \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
t & t & t & \cdots & a \text{ } \text{ } \\
\end{bmatrix}.$$ This is a simple matrix form where the diagonal elements have a constant value $a \in \mathbb{R}$ and the off-diagonal elements have a constant value $t \in \mathbb{R}$ .  Some useful special cases of this matrix form are the centering matrix and the equicorrelation matrix .  (This matrix form is also a particular case of the Toeplitz matrix , but it is much simpler than that general form.) Question: Does this matrix form have a name?","['matrices', 'symmetric-matrices', 'terminology']"
3121973,Does there exist an infinite geometric progression whose terms are all squares,"I am aware of the fact that the squares don't contain an infinite arithmetic subsequence, but I was wondering if the squares contain an infinite geometric sequence. In other words, does there exist an infinite geometric sequence whose terms are all squares?","['elementary-number-theory', 'square-numbers', 'sequences-and-series']"
3121983,"Let $\alpha,\beta \in \mathbb{R}$ then find conditions for when $\lim\limits_{(x,y)\rightarrow(0,0)}\frac{x^{\alpha}y^\beta}{x^2+xy+y^2}$ exists.","I don't know how to approach this exercise, I tried looking at directional paths $y=mx$ and found out that if $\alpha+\beta-2=0$ then the limit doesn't exist. From the same directional paths I think that the condition $\alpha+\beta-2<0$ means the limit doesn't exist since if I approach $0$ from the right then the limit must be $+\infty$ and from the left is $-\infty$ . Though I really don't know if I used the definition correctly, is this part right? I'm lost at $\alpha+\beta-2>0$ , I know the limit must be $0$ but I don't know how to get extra conditions so that I get a for sure convergent limit.","['multivariable-calculus', 'limits', 'calculus']"
3122008,"Direct proof that the irrationals on $[0,1]$ have measure $1$","I am seeking a “direct” proof that the Lebesgue measure of the irrationals on the unit interval is $1$ .  The standard proof I see is that the measure of the unit interval is 1, and the rationals have $0$ measure, so the irrationals must have measure $1$ . That is a perfectly good proof, but if we didn’t know the rationals have $0$ measure, is there a “direct” proof that the irrationals have measure $1$ ?","['measure-theory', 'lebesgue-measure']"
3122036,Showing that $\frac{\mathbb{R}[x]}{\langle x \rangle}$ and $\frac{\mathbb{R}[x]}{\langle x-1 \rangle}$ are not isomorphic as $\mathbb{R}[x]$ modules.,"I'm trying to solve an exercise which asks me to prove that $\frac{\mathbb{R}[x]}{\langle x \rangle}$ and $\frac{\mathbb{R}[x]}{\langle x-1 \rangle}$ are isomorphic as rings, but not as $\mathbb{R}[x]$ modules. It's easy to show they're isomorphic as rings - you can either do it directly or use the first isomorphism theorem for rings to show they're both isomorphic to $\mathbb{R}$ . I'm not sure how to approach showing that they're not isomorphic as modules though - it seems natural to try to prove it by contradiction, but if I assume there is an isomorphism I'm struggling to see where the contradiction comes from. Your help would be much appreciated.","['module-isomorphism', 'ring-isomorphism', 'modules', 'ring-theory', 'abstract-algebra']"
3122047,Double integral of $xe^y$ over the area inside $x^2 + y^2 = 1$ but outside $x^2 + y^2 = 2y$,"My question goes like this: Let R be the area inside $x^2 + y^2 = 1$ and outside $x^2 + y^2 = 2y$ . Calculate $\int\int_R xe^y dA$ . How sould I approach this question? I tried to use integration with polar cordinates, but then I end up with the following very complicated integral when inserting $x = r \cos\theta$ and $y = r \sin \theta$ : $$\int_0^{\frac{\pi}{4}} \int_1^{2\sin\theta} r^2 \cos\theta e^{r \sin\theta} dr d\theta$$ Is this a wrong approach, or have I simply done something wrong? Should i use substitution of variables instead? If yes, any suggestions as to which substitution? All help is very much apprectiated!","['integration', 'multivariable-calculus', 'calculus', 'indefinite-integrals']"
3122098,"What does ""c"" represent in this equation? $\frac{(x-h)^2}{a^2}+\frac{(y-k)^2}{b^2}=c$","In the following equation of an ellipse what does ""c"" stand for? I first thought it is just some scale factor but I am not sure. I know that normally the value used is 1 but my problem comes from me believing that ""c"" is a scale factor. I believe ""c"" is a scale factor yet I do not understand how it works, let me illustrate: In this diagram I made using Desmos both ellipses are exactly the same yet the red one was made using draw.io (I made it have the same length and width as the Desmos-made ellipse yet when I screenshot it and moved it onto Desmos it was resized) and the blue one using Desmos (as evidenced on the left side of the image). Now clearly this draw.io ellipse underwent a maginification of 0.5x since its real dimension are 12 units by 8 units, yet it crosses the x-axis at (-6,0) and (6,0) and the y-axis at (0,-4) and (0,4). Nonetheless when I take this into consideration in the Desmos-made equation the ellipse does not resize accordingly. Can someone explain to me why this happens? ""c"" in this equation should equal .25. Edit: I made a mistake when constructing my question. What I really wanted to ask was how do I find ""c"" given the ellipse made in draw.io?","['conic-sections', 'geometry']"
3122120,What does it mean geometrically for a graded ring to be generated in degree $1$?,"This is a rather basic question that I have never really thought about until now, when I was forced to think about a sheafy version. This condition is needed for example to define the relative Proj construction, and I don't understand what exactly about it is making something 'projective.' Here's an example result who's proof I understand algebraically, but not geometrically. I would like to explain geometrically what this proof is about. Consider the well-known result that if $f: Z \to X$ is a projective, birational map between quasiprojective varieties, there is a coherent sheaf of ideals $\mathscr{I}$ on $X$ so that $Z \simeq \tilde{X}$ , the blow up with respect to $I$ . This is proven in Hartshorne and the proof sketch is as follows. Since $f$ is projective we have a map $i: Z \hookrightarrow \mathbb{P}^n_X$ and an invertible sheaf $\mathscr{L} = i^\ast \mathcal{O}_X(1)$ on $Z$ . We define $\mathscr{S} = \bigoplus_{d \geq 0} \mathscr{L}^d$ . Since $\mathscr{L}$ is coherent, each power is and the sum is quasicoherent. The condition to make $Z$ the Proj of something is that $\mathscr{S}$ needs to be generated in degree $1$ . This isn't the case, so we work affinely and find a related algebra corresponding to a degree $D$ Veronese embedding so that after composing $i$ above with the Veronese embedding, the condition holds. Some more technicalities follow in this argument concerning whether or not we have a coherent sheaf of ideals, and arranging this to be the case, but the primary issue has already arisen. What is this Veronese embedding doing, geometrically? I understand how it takes degree $D$ things to hyperplanes - this isn't quite what I'm looking for and is a little bit on-the-nose. Stated another way, what is composing with this embedding fixing about this object in projective space that is detected about being generated in degree $1$ , that isn't there in general? Can we see visually/geometrically what fails in the Proj construction when this condition isn't met? I apologize if this question is still not clear, and I worry that more rambling is only making it worse, so I'll stop here and try to clarify if anyone doesn't understand what I'm asking.",['algebraic-geometry']
3122136,Proving $\frac{\sin 2x - \cos x}{4\sin^2x -1} = \frac{\sin^2x+\cos x+\cos^2x}{2\sin x +1} $,"I need to prove that this is an identity: $$\frac{\sin 2x - \cos x}{4\sin^2x -1} = \frac{\sin^2x+\cos x+\cos^2x}{2\sin x +1} $$ Here's what I'm confused on: I noticed that the expression in the denominator for the first expression ( $4sin^2(x) -1)$ looks like a double angle identity , but it also looks like a difference of squares . I know I can solve this as a difference of squares. Am I correct that this is a double angle identity, and if so, how can I solve it? I could rearrange the numerator for the second expression ( $\sin^2x+\cos x+\cos^2x$ ) to sub in $1$ for $\sin^2\theta+\cos^2\theta$ ( Pythagorean identity , where $\sin^2\theta+\cos^2\theta=1$ )
By doing so, I'd get $\cos x + 1$ . But, this must be incorrect, for the final answer is $\dfrac{\cos x}{2\sin x-1} = \dfrac{\cos x}{2\sin x-1}$ . That means the cosine must've been multiplied by $1$ . How is that so? (Here's the picture of the problem)","['algebra-precalculus', 'trigonometry']"
3122152,"Multiplying two numbers using only the ""left shift"" operator","At Geeks for Geeks, I came across a question ""Add two numbers without using arithmetic operators"" , which basically states that you can multiply two numbers using the left shift operator. I am confused how that can be accomplished. I am trying to understand the logic here. It states that We can solve this problem with the shift operator. The idea is based
  on the fact that every number can be represented in binary form. And
  multiplication with a number is equivalent to multiplication with
  powers of 2. Powers of 2 can be obtained using left shift operator. I dont understand what the above means. I know that left shifting by 1 is like multiplying by 2, and left shifting by 2 is is like multiplying that number by 4. However, I am confused at how I can use left shift to evaluate something like 7 * 3 ?","['number-theory', 'computer-science']"
3122160,Proving this Hall algebra is commutative without Matlis duality,"For a finite abelian $p$ -group $G$ we have that $$
  G \simeq \mathbf{Z}/(p)^{\lambda_1} \oplus \dotsb \oplus  \mathbf{Z}/(p)^{\lambda_r}
$$ for some positive integers $\lambda_1 \geq \dotsb \geq \lambda_r$ . Note that $G$ is uniquely determined by $p$ and this partition $\lambda = (\lambda_1, \dotsc, \lambda_r)$ , so let's call $\lambda$ the type of $G$ . For types $\lambda$ , $\mu$ , and $\nu$ , define the Hall number $g_{\mu,\nu}^\lambda(p)$ to be the number of normal subgroups $N \mathrel{\triangleleft} G$ of type $\nu$ such that $G/N$ has type $\mu$ . These Hall numbers serve as the structure constants of an associative algebra called the Hall algebra . It turns out that this algebra is commutative, i.e. $g_{\mu,\nu}^\lambda(p) = g_{\nu,\mu}^\lambda(p)$ . The proof of this that I'm looking at, following the more general theory in MacDonald's Symmetric Functions and Hall Polynomials , goes like this: You realize that we're looking at the category of finite-length modules over $\mathbf{Z}_p$ , the $p$ -adic integers . The Prüfer $p$ -group $\mathbf{Z}(p^\infty)$ is the injective hull of $\boldsymbol{k} = \mathbf{Z}/(p)$ in this category, and the functor $\mathrm{Hom}({-},\mathbf{Z}(p^\infty))$ , via Matlis duality , gives you a bijection of the short exact sequences in question, so $g_{\mu,\nu}^\lambda(p) = g_{\nu,\mu}^\lambda(p)$ . Proving this can also be approached by developing the theory of characters of finite abelian groups , section 3 in particular. But this is really the same approach in a different language: $\mathbf{Z}(p^\infty)$ plays the role of $S_1$ in this context. But in either approach, we're introducing some heavy stuff just to prove a fact about $p$ -groups and partitions. Is there a elementary way to prove that $g_{\mu,\nu}^\lambda(p) = g_{\nu,\mu}^\lambda(p)$ in the case of finite abelian $p$ -groups?","['characters', 'representation-theory', 'abstract-algebra', 'p-groups', 'group-theory']"
3122173,Solution to $ xy \ \frac {d^2y} {dx^2} + (x\ \frac {dy}{dx} - 2\ y) \frac {dy}{dx} = 0$,"How can I find the general solution to this: $ xy \ \frac {d^2y} {dx^2} + (x\  \frac {dy}{dx} - 2\ y) \frac {dy}{dx} = 0$ I have learned various methods including: 1. integrating factor method 2. undetermined coefficient 3. variations of parameters 4. bernoulli equation 5. cauchy euler 6. second order ODE with constant coefficient 7. reduction of order 8. separation of variable But none seems to be applicable to this one.
I have tried various ways arranging the equation to this: $xy \ y'' = - (xy'-2y)y'$ or $y'' + y^{-1} y'' - 2x^{-1} \ y' = 0$ But then I'm stuck. Any help would be apreciated! :)",['ordinary-differential-equations']
3122196,Integral problem. Unsure of the approach.,I have this integral: $$\int_0^1 \frac{1 + 12t}{1 + 3t}dt$$ I can split this up into: $$\int_0^1 \frac{1}{1+3t} dt + \int_0^1\frac{12t}{1+3t}dt$$ The left side: $u = 1+3t$ and $du = 3dt$ and $\frac{du}{3} = dt$ so $$\frac{1}{3}  \int_0^1 \frac{1}{u} du = \frac{1}{3} \ln |u| + C$$ But what about the right?,"['integration', 'calculus']"
3122225,Every line bundle on a complex algebraic curve has a meromorphic section,"Every line bundle $L$ on a complex algebraic curve $X$ is of the form $\mathcal{O}(D)$ , where $D$ is some divisor on $X$ . This means $L$ has at least one nonzero meromorphic global section, i.e. $$H^0(L \otimes_{\mathcal{O}_X} \mathcal{M}_X) \neq 0$$ where $\mathcal{M}_X$ is the sheaf of meromorphic functions on $X$ . Is there any way to see this by directly calculating global sections of the sheaf $L \otimes_{\mathcal{O}_X} \mathcal{M}_X$ and without assuming $L \cong \mathcal{O}(D)$ for some divisor $D$ ? I've seen other arguments so I'm mostly just interested in a calculation of the above cohomology group. More generally, how does one compute $H^0(V \otimes_{\mathcal{O}_X} \mathcal{M}_X)$ , where $V$ is an arbitrary vector bundle on $X$ ?","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'riemann-surfaces']"
3122237,"Two known lengths, two known vertex positions, find coupled exterior angles in triangle","labeled sketch of triangle I know the positions of two vertices of a triangle $A$ and $B$ , two side lengths (one by virtue of the two positions) and $l$ , one exterior angle $\theta_2$ , and another angle $\theta_1$ between the known side $l$ and a horizontal axis. The two angles are coupled: $\theta_2=K\theta_1$ I wish to solve for a single angle (either one of the two) in terms of $A,B,K$ and $l$ , but not in terms of the other angle. I have tried solving this in many ways but I can never untangle the trigonometry. This problem is related to the inverse-kinematics of an underactuated 2-link finger. Here is one of my simpler solution attempts: Let $L$ be the distance between $A$ and $B$ . Let $\alpha$ be the angle between the horizontal axis and $L$ . Let $\phi$ be the angle opposite $l$ . We can find both $\phi$ and $\alpha$ using our knowledge of coordinates $A$ and $B$ and trig. $\alpha = \arctan(B_y/B_x), \phi = \theta_1 + \theta_2 - \alpha$ Then using the law of sines $\frac{L}{\sin{\alpha}} = \frac{l}{\sin{\phi}}$ Rearranging $L\sin(\theta_1+\theta_2-\alpha)=l\sin(\pi-\theta_2)$ Using the relationship between the angles, and the fact that $\sin(\pi-x)=\sin(x)$ $l\sin(\theta_2)=L\sin(\frac{\theta_2}{K}+\theta_2-\alpha)$ However, trying to solve for $\theta_2$ yields no solution in MATLAB, Mathematica, and Maple. No matter how I approach this problem, there doesn't seem to be a solution. It feels simple enough that one should exist, but maybe I'm not seeing the full picture?","['triangles', 'euclidean-geometry', 'trigonometry', 'geometry']"
3122316,What's the precise statement of the continuous-time optional stopping theorem?,"I searched high and low in a number of probability / financial mathematics textbooks and surprisingly cannot find any precise statement of the continuous time optional stopping theorem. In particular, none of the sources I find tells me what the conditions exactly are which allow us to apply the optional stopping theorem. Let's start with a much-talked-about example. Let $B_t$ be the standard Brownian motion and $\mathcal F_t$ be the filtration it generates. What's the expected time it takes for $B_t$ to hit either $-\alpha<0$ or $\beta>0$ ? Usually, we do as follows: let $\tau$ be a stopping time w.r.t. $\mathcal F_t$ defined as $$\tau = \inf_{t>0}\{t\mid B_t=-\alpha \vee B_t=\beta\}$$ Now, noting that $B_t$ and $B_t^2-t$ are both martingales, and then apply the optional stopping theorem on them with the stopping time $\tau$ . The problem is, why can we apply the optional stopping theorem in this case? I don't think we can apply this theorem without a set of conditions that the martingale and the stopping time must satisfy. So what are this set of conditions? Wikipedia gives the conditions of this theorem in discrete time. What I'm looking for are their continuous time counterparts.","['stochastic-processes', 'martingales', 'stopping-times', 'probability-theory', 'probability']"
3122320,Prove that transfinite hierarchy of Borel sets will eventually stabilize at some $\lambda < \omega_1$,"We define the transfinite hierarchy of Borel sets $\langle {\bf \Sigma}^0_\alpha, {\bf \Pi}^0_\alpha \rangle_{\alpha \in \rm{Ord}}$ as follows: $$\begin{aligned} &\begin{cases}
{\bf \Sigma}^0_1 &= \{B \subseteq \mathbb R \mid B\text{ is open}\}\\
{\bf \Pi}^0_1    &= \{B \subseteq \mathbb R \mid B\text{ is closed}\}\end{cases}\\
&\begin{cases}
{\bf \Sigma}^0_{\alpha + 1} &= \{\bigcup_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in {\bf \Pi}^0_\alpha\}\\
{\bf \Pi}^0_{\alpha + 1}    &= \{\bigcap_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in {\bf \Sigma}^0_\alpha\}\end{cases} \text{ for all ordinal } \alpha\\
&\begin{cases}
{\bf \Sigma}^0_\alpha &= \{\bigcup_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in \bigcup^0_{\xi < \alpha}{\bf \Pi}^0_\xi\}\\
{\bf \Pi}^0_\alpha    &= \{\bigcap_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in \bigcup^0_{\xi < \alpha}{\bf \Sigma}^0_\xi\}
\end{cases} \text{ for all limit ordinal } \alpha \end{aligned}$$ Prove that there exists $\lambda < \omega_1$ such that ${\bf \Sigma}_\alpha^0 = {\bf \Sigma}_{\alpha + 1}^0 = {\bf \Pi}_\alpha^0 = {\bf \Pi}_{\alpha + 1}$ for all $\alpha \ge \lambda$ . In below attempt, I successfully proved that there exist ordinals $\alpha,\beta,\gamma$ such that $$\begin{cases}
{\bf \Sigma}_\alpha^0 = {\bf \Sigma}_{\alpha + 1}^0\\
{\bf \Pi}_\beta^0 = {\bf \Pi}_{\beta + 1}^0\\
{\bf \Pi}_\gamma^0 = {\bf \Sigma}_{\gamma + 1}^0
\end{cases}$$ Next, I try to prove that there exists ordinal $\delta$ such that ${\bf \Sigma}_\delta^0 = {\bf \Sigma}_{\delta+ 1}^0 = {\bf \Pi}_\delta^0 = {\bf \Pi}_{\delta+ 1}$ by showing that $\forall \xi \ge \alpha:{\bf \Sigma}_\xi^0 = {\bf \Sigma}_{\alpha}^0$ and $\forall \xi \ge \beta:{\bf \Pi}_\xi^0 = {\bf \Pi}_{\beta}^0$ and $\forall \xi > \gamma:{\bf \Sigma}_\xi^0 = {\bf \Pi}_{\gamma}^0$ . But I am stuck at this step. Please shed me some light! My attempt: Lemma: ${\bf \Sigma}_\alpha^0 \subseteq {\bf \Sigma}_\beta^0 \quad {\bf \Sigma}_\alpha^0 \subseteq {\bf \Pi}_\beta^0 \quad {\bf \Pi}_\alpha^0 \subseteq {\bf \Pi}_\beta^0 \quad {\bf \Pi}_\alpha^0 \subseteq {\bf \Sigma}_\beta^0$ for all ordinals $\alpha < \beta$ It follows directly from our Lemma that $\begin{cases}
{\bf \Sigma}^0_\alpha &= \{\bigcup_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in \bigcup^0_{\xi < \alpha}{\bf \Pi}^0_\xi\}\\
{\bf \Pi}^0_\alpha    &= \{\bigcap_{n \in \mathbb N}B_n \mid \forall n \in \mathbb N: B_n \in \bigcup^0_{\xi < \alpha}{\bf \Sigma}^0_\xi\}
\end{cases}$ for all ordinal $\alpha > 1$ . Assume the contrary that there is no ordinal $\alpha$ such that ${\bf \Sigma}_\alpha^0 = {\bf \Sigma}_{\alpha + 1}^0$ . Then ${\bf \Sigma}_\alpha^0 \subsetneq {\bf \Sigma}_{\alpha + 1}^0$ for all ordinal $\alpha$ . Let $\Theta$ be the Hartogs number of $\mathcal P(\mathcal P(\Bbb R))$ . We define a function $\phi:\Theta \to \mathcal P(\mathcal P(\Bbb R))$ by $\phi(\alpha)={\bf \Sigma}_{\alpha + 1}^0 - {\bf \Sigma}_{\alpha}^0$ . Clearly, $\phi$ is injective, leading to a contradiction. Hence there exists an ordinal $\alpha$ such that ${\bf \Sigma}_\alpha^0 = {\bf \Sigma}_{\alpha + 1}^0$ . Similarly, there exists an ordinal $\beta$ such that ${\bf \Pi}_\beta^0 = {\bf \Pi}_{\beta + 1}^0$ and ${\bf \Sigma}_{\gamma + 1}^0 = {\bf \Pi}_\gamma^0$ .","['elementary-set-theory', 'real-analysis']"
3122327,How can you construct the bijection between real numbers and functions over naturals? [duplicate],"This question already has answers here : Question: Compare the cardinality between the sets $\Bbb R$ and $\Bbb N^{\Bbb N}$ (4 answers) Closed 5 years ago . I was reading ""Classical Recursion Theory"" by Odifreddi and it starts with this phrase: Recall that Classical Recursion Theory is the study of real numbers or, equivalently, functions over the natural numbers. I come from Computer Science so this was puzzling to me at first. I understand he's referring to the fact that there is a bijection between $\mathbb{N}$ $\rightarrow$ $\mathbb{N}$ and $\mathbb{R}$ . I know that | $\mathbb{N}$ $\rightarrow$ $\mathbb{N}$ | is greater than | $\mathbb{N}$ | (Cantor's diagonal), but how can you find the real $x$ corresponding to a certain function? I know that given a real $x$ we can construct a function like this: $f(n) =$ the $n$ -th digit of $x$ . But what is the other way around? We can't use digits in this case, I think, because we have numbers with more than one digit; I don't think changing the digit system would help (I think every digit system has a finite alphabet?) and that way we would link more functions to the same real number. I know that you can't compute reals in a strict sense (you would need $\infty$ digits) but I was wondering if there was at least some sketch-procedure of how to represent a function with a real number, to have at least logically a glimpse of what is the number $x$ corresponding to a generic function $f$ : $\mathbb{N}$ $\rightarrow$ $\mathbb{N}$ .","['elementary-set-theory', 'computability']"
3122346,Chain rule where intermediate variable is a matrix,"How does one calculate the derivative of a scalar with respect to a matrix using the chain rule where the intermediate variable is a matrix ? For example: $$\frac{\partial L}{\partial \mathbf W} = \frac{\partial L}{\partial \mathbf Y} \frac{\partial \mathbf Y}{\partial \mathbf W}$$ If $\mathbf Y$ were a vector ( $\mathbf y$ ), the chain rule would suggest that we need to sum across all the individual elements of $\mathbf y$ , i.e. $$\frac{\partial L}{\partial \mathbf W} = \sum_i \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \mathbf W} \text{, where $y_i$ is an element of vector $\mathbf y$}$$ Is it OK to assume that the extension of that rule to the case where $\mathbf Y$ is a matrix is as follows? $$\frac{\partial L}{\partial \mathbf W} = \sum_{i,j} \frac{\partial L}{\partial \mathbf Y_{i,j}} \frac{\partial \mathbf Y_{i,j}}{\partial \mathbf W}\text{, where $\mathbf Y_{i,j}$ is an element of matrix $\mathbf Y$}$$","['vectors', 'matrices', 'multivariable-calculus', 'matrix-calculus', 'derivatives']"
3122353,Does the specific condition on a normal subgroup of a finite group imply that it is a direct factor?,"Suppose $G$ is a finite group, $H \triangleleft G$ , such that $H$ is simple and $Var(H) = Var(G) = Var(\frac{G}{H})$ (Here $Var(G)$ stands for minimal group variety containing $G$ ). Does that imply that $G \cong H \times \frac{G}{H}$ ? If $H \cong C_p$ for some prime $p$ , then $G$ is an abelian group of exponent $p$ for some prime $p$ , which results $G \cong C_p^n$ for some natural $n$ . So by classification of abelian finite groups $H$ is a direct factor of $G$ . So $G \cong H \times \frac{G}{H}$ . However I do not know what to do here in non-abelian case.","['universal-algebra', 'direct-product', 'finite-groups', 'abstract-algebra', 'group-theory']"
3122373,A rigorous yet intuitive summary of inflection and critical points for beginning calculus?,"I haven't done these in awhile. While my analysis covered continuity but not differentiability, I have so far not revisited these in learning geometry or algebra. I am trying to help a calculus student, so any notion of 'interior' is intuitive. First, please verify if these are correct. I think I may be missing phrases like 'open neighbourhood' or 'open interval' and might have confused my ifs and only ifs. Some collections like Wikipedia, are mixed for both higher maths and beginning calculus. Let $A, B \subseteq \mathbb R$ . Definition of critical point : A function $f: A \to B$ has a critical point at $x \in A$ (I represent the point $(x,f(x))$ by the real number $x$ ) if (a) $x$ is in the interior of $A$ and (b) $f'(x)$ is undefined or $f'(x)=0$ . 1.1. Example to illustrate the 'interior' part of Definition (1) : The function $f: A \to \mathbb R, f(t)=t^2$ does not have a critical point at $x=0$ if $A \in \{(-\infty,0], [-7,-3) \cup [-2,0], [0,4], [-8,-6) \cup \{0\} \cup (2,10),\{0\},\{0,1\} \}$ ( rigorously, all these $A$ 's are given the subspace topology...or the topology of $\mathbb R$ ...not sure...which is the right one? ). 1.2. An example for the 'undefined' part of Definition (1) $f:\mathbb R \to \mathbb R, f(t)=|t|$ Definition of inflection point : A function $f: A \to B$ has an inflection point at $(x,f(x))$ , where $x \in A$ may be at the boundary of $A$ or the interior of $A$ but must be in $A$ (I believe this is both intuitive and rigorous: A point in a set is either in the set's interior or the set's boundary), if $f$ has a tangent line at $(x,f(x))$ and the concavity of $f$ changes at $(x,f(x))$ . 2.1. Equivalent definition of inflection point : (This is my attempt to try to rigorously explain 'has a tangent line' while incorporating tangent lines with infinite slope like in this example with $f(t)=t^{1/3}$ ) A function $f: A \to B$ has an inflection point at $(x,f(x))$ if $f'$ exists in $C$ , the intersection of the whole of $A$ and not just $A$ 's interior with some open interval $(a,b)$ ( $x \in C = A \cap (a,b)$ ) where (a) $f'$ is increasing on $(\inf C,x)$ and not increasing (constant, decreasing or does not exist) on $(x,\sup C)$ or (b) $f'$ is decreasing on $(\inf C,x)$ and not decreasing (constant, increasing or does not exist) on $(x,\sup C)$ . Equivalent definition of an inflection point when $x$ is an interior point (for the previous example of $f(t)=t^{1/3}$ , I assume the concern is that $x=0$ is not an interior point): A function $f: A \to B$ has an inflection point at $x$ , where $x$ is an interior point of $A$ , if $f'$ exists in some open interval $(a,b)$ that both contains $x$ and is contained in $A$ ( $x \in (a,b) \subseteq A$ , and $(a,b)$ is contained in $A$ by the definition of $x$ as an interior point of $A$ ), where (a) $f'$ is increasing on $(a,x)$ and decreasing on $(x,b)$ or (b) $f'$ is decreasing on $(a,x)$ and increasing on $(x,b)$ . Consequence of the definition of inflection : A function $f: A \to B$ has an inflection point at $x \in A$ only if $f''(x)$ is undefined or $f''(x)=0$ . 4.1. An example of Consequence (4) for the 'undefined' part is based precisely on Example (1.2) $f: \mathbb R \to \mathbb R,
  f(t) = \begin{cases}
    -\frac{t^2}{2} &\text{if $t < 0$} \\
    \frac{t^2}{2} &\text{if $t \geq 0$.}
  \end{cases}, f'(t)=|t|
$ . Observation : The 'undefined or zero part' of Consequence (4) is just like in Definition (1) except (4) is not a definition for inflection. 5.1. A counterexample to the converse of Consequence (4) for $f''(x)=0$ is $f: \mathbb R \to \mathbb R, f(t)=\frac{t^4}{12},f''(t)=t^2$ , where $x=0$ is an undulation point rather than an inflection point. 5.2. A counterexample to the converse of Consequence (4): When is $f''(x)$ undefined but $x$ not an inflection point of $f$? Proposition : A differentiable function $f: A \to B$ has an inflection point at an interior point $(x,f(x))$ only if $f'$ has a critical point at $(x,f(x))$ . Second , I use the above to rigorously (as rigorously as possible for calculus students) answer as follows. Please verify. If an answer or argument (such as if something in the first part above is wrong) is incorrect, then please give the corresponding correct answer or argument. : Find critical points and inflection points given the derivative (also check for consistency with Darboux's theorem)? I am splitting this up to not cover a lot in one post.","['real-analysis', 'calculus', 'limits', 'general-topology', 'derivatives']"
3122402,tiles covering a $7\times 7$ square,"A $7 \times 7$ board is divided into $49$ unit squares. Tiles, like the one shown below, are placed onto this board. The tiles can be rotated and each tile neatly covers two squares. Note that each single tile consists of two unit squares joined at a corner.
What is the minimum number of tiles that can be placed onto the board so that every uncovered square will be adjacent to at least one covered square? Note: Two squares are adjacent if they share a common side. My attempt: These tiles cover a shape like $$\begin{matrix}0&0&1&0\\0&1&1&1\\1&1&1&0\\0&1&0&0\\ \end{matrix}$$ where $'1'$ s are the covered squares/next to covered squares. Using $3$ -coloring (to the $7\times7$ board) we need at least $7$ tiles. I found a way to do it with 8 tiles. But is there a way prove $7$ isn't enough (or is it actually enough)?","['chessboard', 'combinatorics', 'tiling', 'coloring']"
3122411,Cokernel in Categories of Module (Exercise 4.4 in Blyth's book),"i have been working on ""Module Theory:An Approach to Linear Algebra"" by T. S. Blyth and i am stuck on exercise 4.4 which is ""Let $f: M \to N $ be an $R$ -morphism. By a cokernel of $f$ we mean a pair $(P,\pi)$ consisting of an $R$ -module $P$ together with an $R$ -epimorphism $\pi: N\to P$ such that (1) $\pi \circ f = 0$ (2) for every $R$ -module X and every $R$ -morphism $g:N\to X$ such that $g\circ f = 0$ there is a unique $R$ -morphism $\alpha: X\to P$ such that $\pi = g \circ \alpha$ . Prove that $(N/Imf, \phi)$ ( $\phi$ is the canonical mapping) is a cokernel of $f$ .
"" (1) is not that hard, but for (2) i can only prove that there exist a unique $R$ -morphism $h:N/Imf \to X$ not the other way around which is required. Am i missing something? Is it possible that the definition of the cokernel is wrong? Thanks","['abstract-algebra', 'category-theory', 'modules']"
3122471,"Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Prove $|S| < \infty$","Question: Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$ . Prove $|S| < \infty$ . Notice this is not true in $\mathbb R[X]$ , as $|x-a|\leq2^x $ , $a\in[0,1]$ shows. After experimenting a few rounds on Desmos, I have found no $p(x)\in \mathbb Z[X]$ with degree $\geq3$ satisfy this property. It looks like a piece of cake, but it turns out that the behavior of $|p(x)|$ is quite chaotic (If we drop the absolute value, the problem will become uninteresting). Of course, $2^x$ is going to eventually dominate all polynomials. But how can I prove all but finitely many polynomials dominate $2^x$ at the beginning? I think I've underestimated the difficulty of the problem. Any hint is appreciated. Follow-up question: Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Find $|S|$.","['polynomials', 'analysis', 'real-analysis']"
3122517,Counterexample of weak convergence,"Let $E$ be a vector space over a field $K$ , $x\in E$ and a sequence $\{x_n \}_{n \in \mathbb{N}} \subset E$ . Question: I need to find a counter example of two different linear functionals $$\psi,\varphi\in E' $$ such that $$\lim_{n \to \infty} \psi(x_n) = \psi(x)$$ but $$\lim_{n \to \infty} \varphi(x_n) \neq \varphi(x)$$ Thanks!","['weak-convergence', 'convergence-divergence', 'functional-analysis', 'analysis']"
3122521,Computing a Contour Integral in the Complex Plane,"I am trying to compute $\int \frac{z^5 + z^3 + 19}{z^3 - 3z^2 +3z - 1} dz$ where $ \gamma$ is the circle of radius 7 centered at the origin.
I have that $\gamma(t)= 7e^{it}$ so that $\gamma'(t)= 7ie^{it}$ for $t$ between 0 and $2 \Pi$ .
I have also factored the denominator to get $(z-1)^3$ . I have been trying to use $\int f(z) dz = \int f(\gamma(t))(\gamma '(t)) dt,$ but I keep getting strange answers that are taking a while to compute, so I'm not sure where I am going wrong.   Should I be using another method?","['complex-analysis', 'contour-integration', 'rational-functions']"
3122556,Hyperplanes and convex sets in Hilbert space,"The simplest Hahn-Banach extension theorem in Hilbert space $X$ avoids the use of the axiom of choice by virtue of the Riesz representation theorem. But what about the version of the theorem where the sought-for linear functional is required to remain below a given sub-linear or convex function? Also, to which extent can we separate 2 disjoint convex sets by a hyperplane without Zorn? Can we assert that any hyperplane $H\subset X$ has a translate that is tangent to a given bounded closed convex subset $C\subset X$ ?","['hahn-banach-theorem', 'convex-analysis', 'functional-analysis']"
3122600,VC dimension of half planes,"Let $\mathcal H$ be the set of all half spaces in the two-dimensional plane ( $\mathbf{R}^2$ ). Two questions. 1) How can we formally show that the VC dimension of our half spaces is 3? That is, how can we formally show that it is impossible to shatter four points (i.e. perfectly classify some set of four points using half spaces). 2) If the separating line passes through the origin , what is the VC dimension? I want to think that it is 2 in this case, but what if one of the points is on the origin? Many thanks","['machine-learning', 'geometry']"
3122623,"What is the number of pairs of $(a,b)$ of positive real numbers satisfying $a^4+b^4<1$ and $a^2+b^2>1$?","The number of pairs of $(a,b)$ of positive real numbers satisfying $a^4+b^4<1$ and $a^2+b^2>1$ is - $(i)$ 0 $(ii)$ 1 $(iii)$ 2 $(iv)$ more than 2 Solution:We have $a,b>0$ , According to the given situation, $0<a^4+b^4<1<a^2+b^2\implies a^4+b^4<a^2+b^2\implies 0<a^2(a^2-1)<b^2(1-b^2)\implies b^2(1-b)(1+b)<0 ;a^2(a-1)(a+1)>0\implies a\in(-\infty,-1)\cup (0,1);b\in(-1,0)\cup(1,\infty)$ But, in particular, if we choose $a=-1/2,b=1/2$ ,then $a^4+b^4=1/16+1/16=1/8<1$ and $a^2+b^2=1/4+1/4=1/2<1$ .Which contradicts the given condition $a^2+b^2>1$ Where is the mistake in my approach? How should I approach this problem(means how to think ?). Can Triangle inequality be used here? Please provide some hint...","['real-numbers', 'inequality', 'combinatorics', 'triangles']"
3122679,Proving the Borel-Cantelli Lemma,"Let $\{{E_k}\}^{\infty}_{k=1}$ be a countable family of measurable subsets of $\mathbb{R}^d$ and that \begin{equation} % Equation (1)
\sum^{\infty}_{k=1}m(E_k)<\infty
\end{equation} Let \begin{align*}
E&=\{x\in \mathbb{R}^d:x\in E_k, \text{ for infinitely many $k$ }\} \\
&= \underset{k\rightarrow \infty}{\lim \sup}(E_k).\\
\end{align*} (a) Show that $E$ is measurable (b) Prove $m(E)=0.$ My Proof Attempt: Proof . Let the assumptions be as above. We will prove part (a) by showing that \begin{equation*}
E=\cap^{\infty}_{n=1}\cup_{k\geq n}E_k. 
\end{equation*} Hence, E would be measurable, since for every
fixed $n$ , $\cup_{k\geq n}E_k$ is measurable since it is a countable union of measurable sets.
Then $\cap^{\infty}_{n=1}\cup_{k\geq n}E_k$ is the countable intersection of measurable sets. From here, we shall denote $\cup_{k\geq n}E_k$ as $S_n$ .
 Let $x\in \cap^{\infty}_{n=1}S_n$ .  Then $x\in S_n$ for every $n\in \mathbb{N}$ . Hence, $x$ must be in $E_k$ for infinitely many $k$ , otherwise there would exist
an $N\in \mathbb{N}$ such that $x\notin S_N$ . Leaving $x$ out of the intersection.
Thus, $\cap^{\infty}_{n=1}S_n\subset E$ . Conversely, let $x\in E.$ Then $x\in E_k$ for infinitely many $k$ . Therefore, $\forall n\in \mathbb{N}$ , $x\in S_n$ . Otherwise, $\exists N\in \mathbb{N}$ such that $x\notin S_N$ .
Which would imply that $x\in E_k$ for only $k$ up to $N$ , i.e. finitely many. A contradiction.
Therefore, $x\in \cap^{\infty}_{n=1}S_n$ . Hence, they contain one
another and equality holds. This proves part (1). Now for part (b). Fix $\epsilon>0$ . We need to show that there exists $N\in \mathbb{N}$ such that \begin{equation*}
m(S_N)\leq \epsilon
\end{equation*} Then since $E\subset S_N$ , monotonicity of the measure would imply that $m(E)\leq \epsilon$ .
Hence, proving our desired conclusion as we let $\epsilon \rightarrow 0$ . Since $\sum^{\infty}_{k=1}m(E_k)<\infty$ , there exists $N\in \mathbb{N}$ such that \begin{equation*}
\left| \sum^{\infty}_{k=N}m(E_k)\right |\leq \epsilon
\end{equation*} By definition, \begin{equation*}
m(S_N)=m(\cup_{k\geq N}E_k)=\sum^{\infty}_{k=N}m(E_k)
\end{equation*} Thus, $m(S_N)\leq \epsilon$ . This completes our proof. Any corrections of the proof, or comments on style of the proof are welcome and appreciated. Thank you all for your time.","['measure-theory', 'proof-verification', 'real-analysis']"
3122728,Evaluate $\int_2^6 \frac{\ln(x-1)}{x^2+2x+2}dx$,Evaluate $$I=\int_2^6 \frac{\ln(x-1)}{x^2+2x+2}dx$$ I tried the substitution $t=x-1$ which gave $$I=\int_1^5 \frac{\ln(t)}{t^2+4t+5}dt$$ Then I let $u=\ln t$ which gave $$I=\int_0^{\ln 5} \frac{ue^u}{e^{2u}+4e^u+5}du$$ I don't see other useful substitutions and I don't know how to finish,"['integration', 'calculus', 'definite-integrals']"
3122757,Variables change in derivation. What is wrong?,"I have two set of variables, which are related: $\left\{ \alpha, \beta, \gamma \right\}$ and $\left\{ v_0, v_1, v_2 \right\} = \left\{ \alpha, \beta, \beta \gamma \right\}$ Now, I want to compute the partial derivative $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)}$ . My first approach: as $\alpha$ and $\gamma$ do not depend on $\beta$ , $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} = \frac{1}{\alpha  \gamma} \frac{\partial \beta}{\partial \beta} = \frac{1}{\alpha  \gamma} = \frac{v_1}{v_0 v_2}$ . But also, one could thinks as follows: $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} =  \frac{\partial v_1}{\partial \left( v_0 v_2 \right)} = 0$ Something should be meshed up related with the dependencies, but I cannot see it. Can anyone give me a hand?","['derivatives', 'chain-rule']"
3122763,Strong and weak limits in a dense set for a sequence of bounded operators,"I am working through Lax's ""Functional Analysis"", and I'm trying to prove Theorem 6 of section 15.2, dealing with weak and strong convergence in the operator space. We denote by $s-\lim$ the fact that a sequence converges strongly (in norm), and by $w-\lim$ the weak convergence. Specifically, the theorem states: Let X, U be Banach spaces, $M_n$ a sequence of linear maps in $\textit{L}(X,U)$ uniformly bounded in norm: $\mid M_n\mid\leq c$ for all $n$ . Suppose further that $s-\lim M_nx$ exists for a dense set of $x$ in $X$ . Then, $M_n$ converges strongly, i.e., the $s-\lim$ exists for all $x \in X$ . I need to prove this result, and also an analogous one for weak convergence. My attempt at the strong convergence goes as follows:
Let $D\subset X$ be the dense set for which $s-\lim M_nx$ exists. If $x \in X-D$ , since D is dense, there exists a sequence $(x_j)_{j\in \mathbb{N}} \subset D$ such that $x_j \to x$ as $j\to\infty$ . For each $j\in\mathbb{N}$ , let $y_j = s-\lim_{n\to\infty} M_n(x_j)$ . Then, we claim that the sequence $(y_j)_{j\in\mathbb{N}}$ is Cauchy in U.
This follows from the fact that $(x_j)_{j\in\mathbb{N}}$ is Cauchy and $(M_n)$ is uniformly bounded in norm: $\mid y_j - y_k \mid = \mid \lim M_n(x_j) - \lim M_n(x_k) \mid = \mid \lim M_n(x_j-x_k) \mid = \lim \mid M_n(x_j-x_k)\mid \leq \liminf \mid M_n \mid \mid x_j - x_k\mid \leq c\mid x_j-x_k\mid$ Hence, since U is Banach, $y_j \to y$ as $j\to\infty$ for some $y\in U$ . Now, we claim that $M_nx \to y$ as $n\to\infty$ . This follows since: $\mid M_nx-y \mid \leq \mid M_nx-M_nx_j\mid + \mid M_nx_j-y_j\mid + \mid y_j-y \mid$ . Since the above is true $\forall j \in \mathbb{N}$ and we have that $x_j \to x$ and $y_j \to y$ , given $\varepsilon>0$ we find an $j_0$ such that $\mid x_j - x \mid < \frac{\varepsilon}{3c}$ and $\mid y_j -y \mid < \frac{\varepsilon}{3}$ for all $j\geq j_0$ . Then, we choose $n$ large enough such that $\mid M_nx_{j_0} - y_{j_0} \mid < \frac{\varepsilon}{3}$ , and the result follows. Firstly, I don't see where we use the completeness of X. Futhermore, I'm not sure this proof generalizes for weak convergence. For weak convergence, the weak limit would exist for a dense set $D\subset X$ . Then, we proceed in the same manner: if $x\in X-D$ , there exists a sequence such that $x_j\to x$ as $j\to\infty$ . Let $l \in U'$ be non-zero. Since the weak limit exists in D, we have that $M_nx_j$ converges weakly to $y_j \in U$ . Hence, $l(M_nx_j)\to l(y_j)$ for some $y_j \in U$ . We claim that $(l(y_j))_j$ is Cauchy in $\mathbb{K}$ . Again, this follows since: $\mid l(y_j) - l(y_k) \mid = \mid \lim_n l(M_nx_j) - \lim_n l(M_nx_k) \mid \leq \liminf \mid l \mid \mid M_n \mid \mid x_j - x_k \mid $ Hence, $l(y_j)\to l(y)$ for a certain $y\in U$ , again, this is because $l$ is surjective. Finally, we claim that $l(M_nx) \to l(y)$ , and the proof is analogous to the case above.
However, I don't think that this is enough to conclude that the weak limit $w-\lim M_nx$ exists. This is because our choice of $y$ depends very much on the functional $l$ we chose to start with. Assuming that my proof on the strong limit is right, is there a better way to generalize it to prove the statement about weak limits? If not, how can I approach this problem?","['proof-verification', 'functional-analysis', 'weak-convergence']"
3122792,Conditional running maximum of Geometric Brownian Motion (maximum of Brownian Bridge),"I would appreciate help proving a formula that I came across on page 774 of these lecture note slides on pricing barrier options : https://www.csie.ntu.edu.tw/~lyuu/finance1/2015/20150520.pdf Assume the stock price follows a Geometric Brownian Motion, i.e. $dS_t = \mu S_t dt + \sigma S_t dW_t$ for $t \in [0,T]$ , and $W_t$ is a Brownian Motion (seeded at 0) under the measure $\mathbb{P}$ . Let $H$ be a barrier satisfying $H > S(0)$ and $H > S(T)$ . Then the formula i'm trying to prove is : $$\mathbb{P} \Big[ \max_{t \in [0,T]} S(t) < H \  | \ S(0) = S_0, S(T) = S_T \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big).$$ Using the solution to the SDE, namely $S(t) = S_0 \exp((\mu - \sigma^2/2)t + \sigma W_t)$ , we can rewrite this probability as $$\mathbb{P} \Big[ \max_{t \in [0,T]} W_t + \theta t < b \  | \ W_0 = 0, W_T + \theta T = a \Big] $$ $$ = \mathbb{E}^{\mathbb{P}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} W_t + \theta t < b \}}  | \ W_0 = 0, W_T + \theta T = a \Big],$$ where $\theta := \mu/\sigma -\sigma/2$ , $b:= \ln(H/S_0)/\sigma$ and $a:= \ln(S_T/S_0)/\sigma$ . My understanding is that under the measure $\mathbb{P}$ , $W_t$ is a Brownian Motion, but $W_t + \theta t$ isn't. So what I would have done is to apply Girsanov's theorem. Setting $\tilde{W}_t := W_t + \theta t$ , then we know that $\tilde{W}_t$ is a Brownian Motion (also seeded at 0) under $\tilde{\mathbb{P}}$ , satisfying $$\frac{d\tilde{\mathbb{P}}}{d\mathbb{P}} = \exp(-\theta W_T -\theta^2 T /2) = \exp(-\theta \tilde{W}_T + \theta^2 T /2).$$ Then our calculation becomes equal to $$\mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \frac{d\mathbb{P}}{d\tilde{\mathbb{P}}}  \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big]$$ $$=\exp(\theta a - \theta^2 T /2) \mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big]$$ $$=\exp(\theta a - \theta^2 T /2) \mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big].$$ The latter probability is ""well-known"" as the probability of the running maximum of a Brownian Bridge. An online derivation is given in : https://eventuallyalmosteverywhere.wordpress.com/tag/brownian-bridge/ This probability is equal to $$\mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big( \frac{a^2 - (2b-a)^2}{2T} \Big).$$ Plugging in the values of $a$ and $b$ we find that $$\mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big) \ !!$$ But this is supposed to be the final answer to my problem, so it appears that the factor $\exp(\theta a - \theta^2 T /2)$ is not supposed to be there....
Any help will be greatly appreciated !","['conditional-probability', 'probability-distributions', 'brownian-motion', 'probability']"
3122795,Solve $\log_2(3^x-1)=\log_3(2^x+1)$,Solve the following equation over the real number(preferably without calculus): $$\log_2(3^x-1)=\log_3(2^x+1).$$ This problem is from a math contest held where I learn; I was unable to do much at all tinkering with it; I have observed the solution $x=1$ but haven't been able to prove there are no others or determine them if there are.,"['algebra-precalculus', 'exponential-function', 'logarithms']"
3122800,Why does evaluation of a two-variable limit fail when using polar coordinates?,"The definition of the limit of a two-variable function: $\lim\limits_{(x,y)\to (a,b)}f(x,y)=L\,$ if and only if for all $\epsilon>0$ there exists a $\delta >0$ such that $$0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Consider the following proposition (I do realize that it is not true): Let $f^*(r,\theta) := f(a+r\cos\theta,b+r\sin\theta)$ . Then $$\lim\limits_{(x,y)\to(a,b)} f(x,y) = L \iff \lim\limits_{r\to0^+} f^*(r,\theta) = L$$ Proof. Suppose that $\lim\limits_{(x,y)\to(a,b)} f(x,y) = L$ . This means that for all $\epsilon>0$ there exists $\delta>0$ such that $$0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ If we let $x=a+r\cos\theta$ , $y=b+r\sin\theta$ , then we get that $$0<\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=r<\delta \implies |f^*(r,\theta)-L|<\epsilon$$ Thus, by definition, $\lim\limits_{r\to0^+}f^*(r,\theta) = L$ . Now suppose that $\lim\limits_{r\to0^+}f^*(r,\theta) = L$ . This means that for all $\epsilon>0$ there exists $\delta>0$ such that $$0<r<\delta \implies |f^*(r,\theta)-L|<\epsilon$$ Again, letting $x=a+r\cos\theta$ , $y=b+r\sin\theta$ , we get $$0<r=\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Again, by definition, $\lim\limits_{(x,y)\to(a,b)} f(x,y) = L$ . Q.E.D. I am aware that the above proof can be done by directly proving the equivalence, but I didn't want to risk making it less clear that way. The problem The proposition is incorrect, or so I am inclined to believe. Consider the following function: $$f(x,y) = \frac{x^2y}{x^4+y^2}$$ and the following limit: $$\lim\limits_{(x,y) \to (0,0)} f(x,y)$$ The function is taken from, and my question heavily relies on, this post. Changing to polar coordinates, and after some arrangements, the limit of $f(x,y)$ as $(x,y) \to (0,0)$ is $$\lim_{r \to 0^+} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$ Some answers to that question say that you have to be careful with that limit. My understanding is that this refers to ""substituting $r$ with 0"". I know that this is the wrong approach. But, I get that this limit is always 0, regardless of $\theta$ . And I didn't find it very difficult to arrive at this conclusion. I break it down into two cases. First, I assume that $\sin\theta\neq0$ . The numerator tends to 0 and the denominator tends to $\sin^2\theta$ . Therefore, the limit is just $0/\sin^2\theta$ , i.e. zero. The second case is $\sin\theta = 0$ . But, now the under-limit function is identically zero for all $r\neq0$ , yielding a limit that is zero. This proves that the limit is zero . Have I done something wrong? One answer of the above mentioned post says that, when looking for this limit, you have to analyze the case where $\theta$ is a function of $r$ . Why? With the limit being zero regardless of $\theta$ , my proposition would imply that the limit of $f(x,y)$ is 0. Yet, I know that this is not the case , because if I let $y=x^2$ , the ""limit"" evaluates to $\frac{1}{2}$ .
I have always been told that for a limit to exist, it needs to be the same for every path you approach the limit point on. I've always taken that for granted, and it made intuitive sense to me. But now, thinking more deeply about it, I don't really know why that is. This also has to do with the fact that this comes up nowhere in the proof of my proposition. My thoughts My proof relies on (or so I think) the fact that every point $(x,y) \in \mathbb R$ is representable in polar form as the pair $(r,\theta)$ and that this representation is unique if we restrict $\theta \in [0, 2\pi)$ . Is this correct? As far as I can see $r$ and $\theta$ can be independent variables. I cannot figure out why one would need to allow for $\theta$ to be a function of $r$ . I also rely on the fact that $\sqrt{(x-a)^2+(x-b)^2}$ and $r$ are always equal. Am I missing something? I will sum up my questions: How does the definition of the limit imply that all paths of approach yield the same result? I am looking for an intuitive explanation. What is wrong with my proposition/proof? How does the fact that it fails for the function $f(x,y)$ and the path $(x,x^2)$ , relate to the proof of my proposition. In other words, can you pinpoint exactly where the proof fails? Where does the notion to let $\theta = \theta(r)$ (or even $r=r(\theta)$ ?) come from? I suppose that this is closely related to question 1. When can/should I use polar coordinates to prove that a limit exists ? Thank you for your patience.","['limits', 'multivariable-calculus', 'polar-coordinates', 'real-analysis']"
3122852,Transient random walk on 3-color 3-regular tree,"Suppose that $T=(V,E)$ is a 3-regular tree with root $0$ . Suppose that $0$ is colored green. All other vertices are colored blue, red or green, such that each vertex has exactly one neighbour of each color. Multiple colorings are possible. Just pick one. Let $R=(R(n))_{n \geq 0}$ be a random walk on $V$ that starts at the root, so $R(0)=0$ . At each step, it steps to the (nearest) neighbour colored $x\in\{\text{blue},\text{red},\text{green}\}$ with probability $p_x$ . Assume that $p_{\text{blue}},p_{\text{red}},p_{\text{green}}>0$ and $p_{\text{blue}}+p_{\text{red}}+p_{\text{green}}=1$ . It is known that $R$ is transient. For each subtree induced by vertices L, M, R (see figure) I wish to compute the probability that $R$ ""escapes to infinity"" in that specific subtree. This problem is trivial if $p_{\text{blue}},p_{\text{red}},p_{\text{green}}=\frac{1}{3}$ : for each subtree induced by L, M, R the walk $R$ escapes in it with probability $\frac{1}{3}$ by symmetry. I have tried to use similar symmetry arguments in the general case without succes. Can anyone help me with the general case?","['trees', 'coloring', 'random-walk', 'graph-theory', 'combinatorics']"
3122873,concurrency of $n$ lines,"there are $n>2$ lines of the kind $l_i : a_ix+b_iy+c_i=0, i= 1,2,3,\cdots n$ what is the condition for these n lines to be concurrent? I know that for three lines concurrency the condition is given by the determinant $\begin{vmatrix}
a_1 & b_1 & c_1  \\
a_2 & b_2 & c_2 \\
a_3 & b_3 & c_3 
\end{vmatrix} = 0$ but how is it best generalized for $n$ lines? do i have to write the determinant for every three line combination or is there some compact single expression for this?","['matrices', 'geometry']"
3122908,Existence of the derivative at a point is implied by a version of the symmetric derivative plus continuity,"This is problem 1.1.1.(ii) on p.10 from Flett's book ""Differential Analysis"" . Variants of the problem have appeared in this forum under the subject of symmetric derivative (e.g., here and here ). Flett phrases the problem in terms of functions having a normed space as codomain, and limits, but I am having a really hard time figuring out the simplest case of a function $f:\mathbb R\to\mathbb R$ such that for all sequences $(x_{n})$ and $(y_{n})$ satisfying $x_{n}>c>y_{n}$ and converging to $c$ the limit $\lim_{n\to\infty}\frac{f(x_{n})-f(y_{n})}{x_{n}-y_{n}}$ exists and equals $L$ ; the function $f$ is continuous at $c$ . Then, the derivative of $f$ at $c$ exists, and moreover $f^{\prime}(c) = L$ . A (sort of) converse to this result states that if $f$ is differentiable at $c$ , then its symmetric derivative, meaning the limit $\lim_{h\to 0}\frac{f(c+h)-f(c-h)}{2h}$ , exists and equals $L$ . This is not difficult to prove using a middle-man trick. What I cannot figure out is how the additional condition that $f$ is continuous at $c$ on top of (a version of) the symmetric derivative at $c$ guarantees that $f$ is differentiable at $c$ and $f^{\prime}(c)$ equals the symmetric derivative at that point. Any suggestions?","['limits', 'calculus', 'derivatives', 'real-analysis']"
3122920,"Fundamental group of $Q:=\{[x,y,z,t,u,v] \in \mathbb{R}P^5 |x^2+y^2+z^2-t^2-u^2-v^2=0\}$ and a covering of a quadric","I was doing some exercises in order to prepare myself for the writing exam of General Topology. We didn't give enough attention to quadrics, so this exercise is giving me hard times, any tips about it? I can almost see what could be happen with $C$ , given that, heuristically speaking, between an elliptic hyperboloid and a torus there's a clear homomorphism. But $Q$ is completely exotic to me. Consider $\,\,Q:=\{[x,y,z,t,u,v] \in \mathbb{R}P^5 | \,\,x^2+y^2+z^2-t^2-u^2-v^2=0\}$ . Using the map $\pi: S^5 \to \mathbb{R}P^5\,\,$ show that $Q$ is arc-connected and find its fundamental group $\pi_1(Q)$ . Consider $C :=\{[x,y,z,t] \in \mathbb{R}P^3 | \,\,x^2+y^2-z^2-t^2=0\}$ . Using the map $\pi:S^3 \to \mathbb{R}P^3$ , show that the torus $T^2$ is a covering of $C$ ; and using the map $\phi:S^1 \times S^1 \to S^1 \times S^1$ where $\phi(z,w)=(zw,z \bar{w})$ , show that the quadric $C$ is homeomorphic to $T^2$ .","['general-topology', 'projective-geometry', 'algebraic-topology']"
3122950,Proving after n seconds all of intersection points of an infinite by infinite grid become the same color [duplicate],"This question already has answers here : $n$ black squares on a grid become disappear in $n$ moves. (1 answer) black squares on infinite cross-lined paper [closed] (4 answers) Closed last year . Consider an infinite by infinite grid.
We color the intersection points of the gird in the following method: $n$ points are colored black the remaining points are colored white Now every second the colors change in this way:
every point changes its color to the color of the point among these there points:itself,the point above it,the point to the right; which its color is repeated at least two times!
in better words:each time the color of a point is changed base on which color is repeated more among itself and two other neighbors (right hand-side and above).the color that is repeated more(two to three times) is the color that this point changes to. Prove that after $n$ seconds all of the points are going to be white. for the solution I only could get so far: if in the $k$ th second a point is black then at least $k$ black points should existed above and right of that point. I don't know it is useful or not are how can it be used exactly","['induction', 'discrete-mathematics']"
3122967,Partition the set of naturals from $1$ to $16$ into two subsets satisfying certain conditions,"I want to partition the set $S=(1,2,3,...,16)$ into two subsets $A=(a_1,a_2,...,a_8) $ and $B=(b_1,b_2,...,b_8)$ such that: $$[1]\;\;\;\sum_{i=1}^{8}a_i=\sum_{i=1}^{8}b_i$$ And: $$[2]\;\;\;\sum_{i=1}^{8}a_i^2=\sum_{i=1}^{8}b_i^2$$ And: $$[3]\;\;\;\sum_{i=1}^{8}a_i^3=\sum_{i=1}^{8}b_i^3$$ I tried some configurations, I found that $A=(1,4,6,7,9,12,14,15)$ and $B=(2,3,5,8,10,11,13,16)$ satisfy $[1]$ and $[2]$ but not $[3]$ , I figured out that the sum in $[3]$ should be $9248$ but I'm kinda stuck, I don't know what else I could do other than painstakingly compute other configurations...","['elementary-set-theory', 'elementary-number-theory']"
3123000,If $\lim\limits_{x \to \infty} f(x) = \infty$ then a special interval exists?,"Sorry for the bad title, I didn't know any word to describe what I need. Suppose $f:\mathbb R \to \mathbb R$ is $C^1$ and that $\lim\limits_{x\to \infty}f(x) = \infty$ Can we say for sure that there is some interval $[0, b]$ such that $f(x) >\max\limits_{y \in [0,b]}f(y)$ for all $x >b$ ? More generally, suppose $f:\mathbb R^n \to \mathbb R^n$ a $C^1$ function such that $\lim\limits_{|x| \to \infty}|f(x)| = \infty$ Can we say for certain that there is closed ball $B(0, r)$ such that $|f(x)| > \max\limits_{y\in B(0, r)} |f(y)|$ ? this seems natural and trivial but I have yet to find a formal proof.","['limits', 'calculus', 'normed-spaces']"
3123014,"Finding a smooth atlas of $M=\{ (x,y,z) \in \mathbb{R}^3 \; | \; x^2+y^2-z^2=1 \}$","We know that $M=\{ (x,y,z) \in \mathbb{R}^3 \; | \; x^2+y^2-z^2=1 \}$ is a submanifold of $\mathbb{R}^3$ , with dimension $2$ . The point is that I have to find an atlas which define the structure of $C^{\infty}$ manifold for $M$ . I just discovered these objects, and I have to say that I don't know how to do. As $M$ is a submanifold of dimension $2$ , I think I should find some open sets $(U_i)_i$ of $M$ , and some open sets $(V_i)_i$ of $\mathbb{R}^2$ , and some homeomorphism $\psi : U_i \rightarrow V_i$ , such that the transition map are $C^\infty$ . My first idea was to use the theorem of the implicit functions, to have $z$ function of $x,y$ and then I would be able to solve the problem I think. But all I know is that : $(x,y,z) \in M \iff z = +- \sqrt{x^2+y^2-1}$ , and from that, I tried several things but it doesn't lead where I wanted. Someone could help me, and explain me how to do, and what's the idea between thoses objects ? Thank you !","['geometry', 'differential-geometry']"
3123017,Strategy for solving $\sqrt{3}\cos(2x) - \sin(x)\cos(x) = 1$,"I believe the way to solve the following equation is to use the ""R-formula"": $$ \sqrt{3}\cos(2x) - \sin(x)\cos(x) = 1 $$ If so, it should be rewritten as: $ R\cos(x-\mathcal{L})$ or $R(\cos x\cos\mathcal{L}+\sin x\sin\mathcal{L})$ . With coefficients equated as: $ \sqrt{3} = R\cos\mathcal{L} $ $ ? = R\sin x $ In the top equation, it looks like $ \cos x $ is the coefficient of $ \sin x $ . But that doesn't work with R-formula (does it?). I have spent some time trying to use double-angle identities for $\cos(2x)$ to remove something from the $\sin(x)\cos(x)$ expression, but am uncertain whether that's possible due to the leading $\sqrt{3}$ . I have also taken several suggestions from threads like this one , but repeatedly end up uncertain due to the same double-angle issue and/or coefficient issue. Is R-formula the right approach to solving this equation? Should a double-angle identity be used first? Or is an entirely different approach correct?",['trigonometry']
3123022,A bizarre matrix product -- does it have a name?,"Let $\mathbb{F}$ be a field, let $A\in\mathbb{F}^{n\times \ell m}$ , and let $B \in\mathbb{F}^{m\times p}$ . Let $A_j$ be the submatrix of $A$ consisting of columns $(j-1)m+1$ through $jm$ and define $$
A*B = \begin{bmatrix}A_1B \ | \ \dots \ | \ A_\ell B\end{bmatrix} \in \mathbb{F}^{n\times \ell p}
$$ For example, $$
\begin{bmatrix}
1 & 2 & 3 & 4\\ 5 & 6 & 7 & 8
\end{bmatrix}*\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}\begin{bmatrix}
1 & 2 \\ 5 & 6
\end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\ | \ \begin{bmatrix}
3 & 4\\ 7 & 8
\end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} \end{bmatrix}=\begin{bmatrix}2 & 1 & 4 & 3\\ 6 & 5 & 8 & 7 \end{bmatrix}
$$ Does this bizarre matrix product have a name? I would like to use it to write something like $XY = C*\text{vec}(X)$ where $C = [I\otimes \text{col}_1(Y)^T\dots I\otimes \text{col}_p(Y)^T]$","['matrices', 'linear-algebra']"
3123037,Bounded sequence in dual with any weak*convergent subsequence,"Let $(E,\|\cdot\|_E)$ be a separable normed vector space over a field $\mathbb{K}$ and $E'$ its dual. Theorem: Each bounded sequence in $E'$ has a weak-* convergent subsequence. Question: With $(E,\|\cdot\|_E)$ not being separable, provide an example of a bounded sequence in $E'$ without any weak-* convergent subsequence. I thought about an example on the space of continuous functions over a non compact set $(C((0,1)),\|\cdot\|_\infty)$ , where $\|\cdot\|_\infty$ is the supremum's norm. But any other would be highly appreciated. Thanks.","['weak-convergence', 'convergence-divergence', 'functional-analysis', 'analysis']"
3123098,Show that the free group on three generators is a subgroup of the free group on two generators,"I have been asked to show that the free group on three generators is a subgroup of the free group on two generators. The following definition has been taken from the appendix to Armstrong's $\textit{Basic Topology}$ : The free subgroup $F^X$ on $r$ generators $X=\{x_1,...,x_r\}$ has been defined as the infinite set of words obtained by concatenating the generators $x_i$ and their inverses $x_i^{-1}$ into words, where the inverse relation is $x_ix_i^{-1}=e$ , the empty word, which is the identity element of the group, and naturally satisfies the relation that $e$ concatenated with any word $w\in F^X$ produces the same word $w$ . So the free group on three generators would be $F^X$ , where $X=\{a,b,c\}$ , and the free group on two generators would be $F^Y$ , where $Y=\{a,b\}$ . We want to show that $F^X$ is a subgroup of $F^Y$ . Now a requisite for a group being a subgroup of another, is that it is a subset of the group. But I cannot seem to see how $F^X$ can be a subset of $F^Y$ seeing as $c\not\in F^Y$ . Even when one considers relabelling, one cannot discount the fact that the longest word we can create using distinct letters in $F^X$ is $ac^{-1}b^{-1}a^{-1}bc$ , or some valid rearrangement of those letters. This word is of length 6. On the other hand, the longest word one can create using distinct letters in $F^Y$ is $a^{-1}bab^{-1}$ , which has length 4. So $F^X$ must contain elements that are not in $F^Y$ . The notion that a group generated by a greater number of free elements should be a subgroup of one generated by a lesser number seems absurd to me, and I have almost convinced myself that the statement must be false. All help and input would be highly appreciated. Follow-up: The comments below have clarified the matter by indicating that the free group on three generators is $\textit{isomorphic}$ to a subgroup of the free group on two generators, which is what the two answers given below have proven. In response to this, I ask the following: As the free group on two generators clearly is a subgroup of the free group on three generators, does this imply that they are isomorphic groups?","['group-theory', 'free-groups', 'algebraic-topology']"
3123104,Derivative of sigmoid function that contains vectors,Could someone show me how to take the derivative of this function with respect to $w_i$ ? $f(w) = \frac{1}{1+e^{-w^Tx}}$ $w$ and $x$ are both vectors $\in \mathbb{R}^D$ How would this be different from taking the derivative with respect to $w$ itself?,"['calculus', 'matrix-calculus', 'derivatives']"
3123145,Algebraic proof that two statements of the fundamental theorem of algebra are equivalent,"Students studying the fundamental theorem of algebra in high school are probably familiar with the statement that goes something like the following. Every non-zero, single-variable, degree $n$ polynomial with complex coefficients has, counted with multiplicity, exactly $n$ complex roots. However, another statement I often see (and comes first in the Wikipedia article) is the following. The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. These statements are equivalent and Wikipedia says that this can be proven by successive polynomial long division. However, Wikipedia does not give this proof. It is absent from Wolfram MathWorld as well. My question is, what is the proof that these statements are equivalent? I know that the fundamental theorem of algebra cannot be proven algebraically, but can this equivalence be proven with only algebra, i.e., polynomial division?","['proof-explanation', 'real-analysis', 'complex-analysis', 'polynomials', 'algebra-precalculus']"
3123150,A trig integral with a tangent substitution,"From Complex Function Theory by Palka, an example shows for positive integers $n$ that $$\int_0^\pi \sin^{2n}t \; dt=\frac{\pi}{4}\binom{2n}{n}$$ using the Residue Thm, and I understood his derivation. But he went on to say that this can be found using familiar techniques, he mentions the substitution $u= \tan (t/2)$ and to do partial fractions. I like attempting to evaluate integrals b/c they often involve clever tricks (that I usually can't think of, but appreciate once I see), so I wanted to give this one a try. Using the suggested substitution $$\cos t=2 \cos^2 \frac{t}{2}-1= \frac{2}{u^2+1}-1$$ and $2du= \sec^2(t/2)$ $\leftrightarrow$ $dt=2du/(u^2+1)$ (converting from tan to cos I drew a right triangle w/ legs 1 and $u$ ), so $$ \begin{split}
\int_0^\pi \sin^{2n}t \; dt =& \int_0^\pi ( \sin^2t)^n \; dt= \int_0^\pi \big( \frac{1- \cos 2t}{2}\big)^n \; dt \\
=&  \frac{1}{2^{n+1}}\int_0^{2 \pi}(1- \cos t)^n \; dt = \frac{1}{2^{n+1}}\int_{- \pi}^\pi (1- \cos t)^n \; dt \\
=& \frac{1}{2^{n+1}}\int_{- \infty}^\infty \big( 1- \frac{2}{u^2+1}+1 \big)^n \frac{2}{u^2+1}\; du \\
=& \int_{- \infty}^\infty \frac{u^{2n}}{(u^2+1)^{n+1}}\; du
\end{split}$$ but the integrand doesn't look any easier to deal with. How should I proceed from here? Partial fractions should result in a linear combination of terms $1/(u^2+1)^k$ , so even assuming I can do the decomp for general $n$ , I realize I don't know how to integrate it for $k \neq 1$ . Thanks a lot in advance for any help.","['integration', 'calculus', 'definite-integrals']"
3123162,"Maximum entropy distribution given constrained minimum, maximum, and mean","What kind of distribution do we get if we constrain the range to be the unit interval and also constrain the mean to be $\alpha$ ? If we read this table , we see the following two examples of maximum entropy distributions. The standard normal distribution is the maximum entropy distribution with mean 0 and variance 1. Similarly, if we restrict out attention to distributions with bounded support, then the uniform distribution is the maximum entropy distribution. I'm writing a small library to estimate quantiles in a stream of data and am currently keeping track of the sample minimum, maximum, and mean, but am not using the sample mean to estimate quantiles directly. So, what kind of distribution do we get if we constrain the range to be the unit interval and also constrain the mean to be $\alpha$ ? In general, is there a good strategy for figuring out a closed form for the pdf of a maximum entropy distribution satisfying an arbitrary collection of properties?","['entropy', 'probability']"
3123169,Let $A$ be such a set that its power set $P(A)$ is a chain with inclusion as the partial ordering. What can be said about $P(A)$?,"I don't have a solution manual to this specific exercise and they don't give a hint what kind of answer they are looking for. The problem is picked from Kaplansky "" Set Theory and Metric Spaces "". Does anyone here have any ideas?","['elementary-set-theory', 'combinatorics']"
3123182,Abstract algebra subgroup proof verification,"This is my first attempt at a formally written proof so I would appreciate any pointers as far as proof-writing technique or the validity of the actual proof itself. Note: I have not formally taken abstract algebra or a proof-writing course so I am sure that I am lacking in many proof-writing aspects, so I would really love a lot of constructive criticism both on the actual proof itself, and the way I wrote the proof. I also go into greater detail than might be appropriate for this type of proof because I am shaky on a lot of the math foundations so I figure that any imperfections in my knowledge will be more easily seen with a more explicit construction of this proof. Thank you all in advance. Let $G$ be a finite group, and let $S$ be a nonempty subset of $G$ . Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$ . (HINT: It remains to prove that $S$ contains $e$ and is closed with respect to inverses. Let $S$ = { $a_1$ ... $a_n$ }. If $a_i$ $∈$ $S$ , consider the distinct elements $a_ia_1$ , $a_ia_2$ , $...$ $a_ia_n$ Proof: First we will define a function $A_1 : S \rightarrow S$ that maps $s \mapsto a_1s$ . This function is injective because $$a_1y = a_1x$$ $$a^{-1}_1a_1y = a^{-1}_1a_1x$$ $$y = x$$ The function is then surjective because $A_1(S) \subseteq S$ and since $A_1$ is injective, it contains $|S|$ elements. Therefore $A_1$ maps onto every element in $S$ and is therefore surjective as well. This means that $a_1$ is in the image of $A_1$ . Therefore $$A_1(a_1) = a_1$$ $$a_1s = a_1$$ $$s = e$$ Since $S$ is closed under multiplication, $e \in S$ . Next, we will define a function $A_2 : S \rightarrow S$ that maps $s \mapsto a^2_1s$ . This function is also injective $$a^2_1x = a^2_1y$$ $$x = y$$ It follows that this function is also surjective since it too is injective and contains |S| elements. This means that $a_1$ is in the image of $A_2$ as well. Therefore $$A_2(z) = a_1$$ $$a^2_1z = a_1$$ $$z = a^{-1}_1$$ Since $S$ is closed under multiplication $a^{-1}_1 \in S$ . Therefore $e, a^{-1}_1 \in S$ so $S$ is a subgroup of $G$ . Please tear this apart! Thanks in advance.","['proof-verification', 'finite-groups', 'proof-writing', 'abstract-algebra', 'group-theory']"
3123187,"prove Bézout's Theorem: few axioms, restricted language","I take Bézout's theorem to be the statement that given two different irreducible algebraic curves in two-dimensional projective space over an arbitrary field $K$ , of degrees $m$ and $n$ , the number of points of intersection does not exceed $mn$ . The only proof I know assumes that $K$ is infinite, so that via a projective transformation you can prevent $[1,0,0]$ from sharing the same line with any pair of distinct points of intersection. Could I prove the same theorem in a more restricted language: perhaps one in which the infinite size of $K$ or the possibility of field extensions cannot be expressed?","['model-theory', 'algebraic-geometry']"
3123210,Which positive integers can NOT be written as a sum of consecutive positive integers,"I have been wondering about this question for a while (I am almost sure that I read it in some contest mathematics book a while ago) Determine which positive integers cannot be written as a sum of consecutive positive integers. All numbers will be considered positive integers in the following unless stated otherwise. I was thinking as follows: odd numbers will always work since if we take an odd $m$ , we have $$
m=2\cdot k+1=k+(k+1).
$$ Then I realised that all numbers having at least one odd prime factor will also work. Take $n$ as a number having an odd prime factor and let this odd prime factor be $m$ and let $n=m\cdot k$ . Then $$
n=\underbrace{k+k+\ldots+k}_{m \ \text{times}}
$$ and since $m$ is odd, say $m=2\cdot l+1$ we have $$
n=\underbrace{k+k+\ldots+k}_{l \ \text{times}}+k+\underbrace{k+k+\ldots+k}_{l \ \text{times}}
$$ and we can start moving $1$ s one by one from the left side of the middle "" $k$ "" to the right side of it $$
n=\underbrace{k+k+\ldots+(k-1)}_{l \ \text{times}}+k+\underbrace{(k+1)+k+\ldots+k}_{l \ \text{times}}
$$ $$
n=\underbrace{k+k+\ldots(k-2)+(k-1)}_{l \ \text{times}}+k+\underbrace{(k+1)+(k+2)+\ldots+k}_{l \ \text{times}}
$$ this way we end up in a sum of consecutive integers. It may happen that some integers on the left will be negative which is undesired but since these numbers are consecutive integers we always be able to cancel out the negative ones without disturbing the structure of the uncancelled ones. To see this in action let us take $22=11\cdot 2$ . This would give $$
2+2+2+2+2+2+2+2+2+2+2
$$ so $$
2+2+2+2+1+2+3+2+2+2+2
$$ the next step gives $$
2+2+2+0+1+2+3+4+2+2+2
$$ it seems that now we are out of numbers but we just continue moving an increasing number of $1$ s to the right $$
2+2+(-1)+0+1+2+3+4+5+2+2
$$ $$
2+(-2)+(-1)+0+1+2+3+4+5+6+2
$$ finally $$
\underbrace{(-3)+(-2)+(-1)+0+1+2+3}_{=0}+4+5+6+7
$$ so we get $$
22=4+5+6+7.
$$ Now we have a method to generate the desired expression for all odd numbers and all the ones having at least one odd prime factor. Which numbers are left? The ones having ONLY even primes in their prime factorisation. These are of the form $2^n$ . I was thinking what do these have in common and I thought that they are separated by a factor of $2$ . The first of these are $1$ for $n=0$ and $1$ cannot be expressed in the desired for so if I could show that $$
m \ \text{cannot be expressed}\Rightarrow 2m \ \text{cannot be expressed}
$$ I am done. So I was trying to show the contrapositive $$
2m \ \text{can be expressed}\Rightarrow m \ \text{can be expressed}.
$$ So assume that $2m$ can be expressed. Now, eihter $m$ is odd in which case we are done since odds were easy to express or it is even, that is $m=2l$ . From here the argument repeats. Eihter $l$ is odd in which case we are done or it is even, so $l=2k$ and so on. After a while we factored out all of the $2$ 's and we end up with and odd number which we can express and we are done. So integers of the form $2^n$ cannot be expressed in this form. Is my reasoning correct? If not please point out where I made a mistake. If it turns out to be correct then we are all happy I can sleep well tonight but it would be nice to see a more rigorous proof of this so if someone can provide one I am more than happy to read it. Since this is a contest question I expect it to have a really elegant solution and I do not consider mine an elegant one.","['contest-math', 'number-theory', 'elementary-number-theory', 'proof-verification', 'prime-numbers']"
3123226,Why does $G$ need to be finite in this proof of a normal subgroup $N$ containing all elements of order coprime to $[G:N]$?,"Here is a proof from Keith Condrad's document with cosets instead of classes: Theorem 2. If $G$ is a finite group and $N \triangleleft G$ then any element of $G$ with order relatively prime to $[G:N]$ lies in $N$ .  In
  particular, if $N$ has index $2$ then all elements of $ G$ with odd
  order lie in $N$ . Proof :   Let $g$ be an element of $G$ with order $m$ , which is relatively prime to $[G:N]$ . The equation $g^m=e$ gives $(gN)^m=N \in G/N$ .   Also $(gN)^{[G:N]}=N$ , as $[G:N]$ is the order
  of $G/N$ . So the order of $gN \in G/N$ divides $m$ and $[G:N]$ . These numbers are relatively prime, so $gN=N$ , which means $g \in N$ . Why is it required that $G$ is finite? Does the theorem also hold for groups in general? I suspected it because of $[G:N]$ , but we set it out to be coprime to $m$ , is $[G:N]=\infty$ allowed?","['group-theory', 'abstract-algebra']"
3123240,Application of Kolmogorov 0-1 Law for $S_x := \{\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}1_{A_{i}}\leq x\}$,"I have come across the following exercise during a course on probability, and I'm nearly certain it has to be proved using the Kolmogorov 0-1 Law, but the theorem is only stated in the lecture notes, and no examples on how to apply it were given. Let $A_1, A_2, \dots$ be any independent sequence of events and let $S_x := \{\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}1_{A_{i}}\leq x\}$ . Prove that for each $x\in\mathbb{R}$ we have $\mathbb{P}(S_x)=\{0,1\}$ . I've found that the answer is trivial for any $x\in[0,1)^C$ , but that's obviously not the point of the exercise. So far it seems like it is not possible to construct a sequence $B_i$ of independent events such that $S_x$ is in the tail-field, since there is seemingly no way to prove independence of the $B_i$ without more knowledge about $\mathbb{P}:\Omega\rightarrow [0,1]$ . A friend suggested maybe it is possible that you can make a probability triple with a tail-field as its sigma-algebra. Then if we can show that $f_n(\omega)=\frac{1}{n}\sum_{i=1}^{n}1_{A_{i}}$ is a measurable function, then $\lim_{n\rightarrow\infty}f_n^{-1}([-\infty,x])$ would have to be inside the tail-field, hence also have probability $0$ or $1$ . It seems like if the last approach were to work, we must first show that $1_{A_{i}}$ is measurable in this new probability triple. Meaning we have to show that both $A_i$ and $A_i^C$ have to be in the tail-field. But since I have no prior experience with tail-fields it is not obvious at all how one could construct such a thing (if it's even possible). Any help would be appreciated. One last note: $\{\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}1_{A_{i}}\leq x\} \iff \{\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=m}^{n}1_{A_{i}}\leq x\}$ . I found a similar question here , but there seems to be a big jump  in the logic of the first answer.","['limits', 'probability-limit-theorems', 'independence', 'probability-theory']"
3123246,Closed form of $\int_0^\pi \ln\left(1+\sin^2(t)\right) dt$?,I attempted to evaluate this integral but seem to be getting nowhere $$I=\int_0^\pi \ln\left(1+\sin^2(t)\right) dt$$ Wolfram returns the value $I\approx 1.18266$ but was not able to provide a closed form for me. I suspect that one could exist but I'm not sure how to proceed. Any help will be appreciated.,"['integration', 'definite-integrals', 'closed-form', 'logarithms']"
3123270,Computing curvature of the quotient of the tautological connection,"I am trying to understand a certain passage in the book ""Geometry of Four-Manifolds"" by Donaldson and Kronheimer (specifically, a computation in section 5.2.3). I am confused on the proof of Proposition 5.2.17, which states: Proposition (5.2.17) Let $\hat{\nabla}$ be the tautological connection on the $SO(3)$ -bundle $\mathfrak g_{\pi_2^*E}$ , and $\nabla$ the quotient of this connection on the quotient bundle $\mathfrak g_{\mathbb{P}}\to \mathscr{B}^*\times X$ . The three components of the curvature of $\nabla$ at a point $([A], x)$ are given by $F(\nabla)(u,v) = F(A)(u,v)$ $F(\nabla)(a,v) = \langle a, v\rangle$ $F(\nabla)(a,b) = - 2 G_A\{a,b\}|_x$ . Here, $u,v\in T_x X$ , $a,b\in \Omega^1(\mathfrak g_E)$ satisfying $d_A^*a=d_A^*b=0$ ; $G_A$ is the Green's operator for the Laplacian $d_A^*d_A$ on $\Omega^0(\mathfrak g_E)$ ; and $\{,\}$ is the natural pairing formed from a metric on $X$ and the Lie bracket on $\operatorname{Lie}(G)$ , $G$ being the gauge group. Here, $\mathscr{B}^*$ is the space of irreducible connections on a bundle $E\to X$ modulo the action of the group of gauge transformations. I am specifically confused about how they apply equation (5.2.16) (which is marked as $(*)$ below) to deduce this curvature, mainly because I do not understand how they figure out what $\Phi$ ""does"" in the case of the qoutient of the tautological connection. So, my question is, How do they determine what $\Phi$ is in order to apply equation $(*)$ below to deduce this Proposition? Here are the relevant details from this passage in the book.
Suppose a Lie group $\Gamma$ acts freely and properly on a manifold $\hat{Y}$ . Also assume we have a bundle $\hat{E}\to \hat{Y}$ and an action of $\Gamma$ on $\hat{E}$ that is linear on fibers and that covers the group action on $\hat{Y}$ . Let $Y := \hat{Y}/\Gamma$ and $E:= \hat{E}/\Gamma$ . We now suppose we are given two things: A connection $\hat{\nabla}$ in $\hat{E}$ invariant under $\Gamma$ . A connection $H$ in the $\Gamma$ -bundle $p:\hat{Y}\to Y$ . One then gets a quotient connection $\nabla$ in $E$ from this. Then, in order to compute its curvature, introduce the 1-form $B \in \Omega_{ \hat Y }^1 \otimes \operatorname{End}(\hat{E})$ given by $$ B:= \hat{\nabla} - p^* \nabla.$$ Then, because $B$ vanishes on $H$ -horizontal vectors, we can write $B$ as $\Phi \circ \theta$ , where $\theta$ is the connection 1-form for $H$ and $\Phi: \operatorname{Lie}(\Gamma) \to \operatorname{End}(\hat{E})$ is a linear map. One can then compute that $$(*)\quad F(\nabla)(U,V) = F(\hat{\nabla})(\hat{U},\hat{V}) - \Phi\circ \Theta(U,V)$$ where $U,V\in T_y Y$ and $\hat{U},\hat{V}$ are horizontal lifts to $T\hat{Y}$ . They apply this to the $SO(3)$ -bundle of Lie algebras $g_{\pi_2^*E}$ obtained from the pullback of $E$ along $\pi_2:\mathscr{A}^*\times X \to X$ and $\mathscr A^*$ is the space of irreducible connections on $E$ ; where $\Gamma$ the group of gauge transformations modulo $\pm 1$ ; with $\hat{Y} = \mathscr{A}^*\times X$ ; with $H$ being the connection on $\mathscr{A}^*$ obtained from slice neighborhoods for the action of the gauge transformations; and $\hat{\nabla}$ is the tautological connection on $\pi_2^*(E)$ (or, rather, the induced one on $\mathfrak g_{\pi_2^* E}$ ). They use the results that for $H$ , the connection form $\theta$ and curvature form $\Theta$ are $\theta_A(a) = -G_A d_A^* a$ and $\Theta_A(a,b) = -2G_A{a,b}$ , which I am fine with. What is really bothering me is how they figure what $\Phi$ (or $B$ for that matter) is, since the pullback of the quotient of the tautological connection $p^* \nabla$ gives me a serious headache. Really, any advice or other resources for this calculation would be appreciated. Edit: I have attempted to try and calculate it from basic principles. Let's call the Let's say we are at a point $(A, x)$ of $\mathscr{A}^*\times X$ . The mapping $\gamma:\operatorname{Lie}(G)\to T_\nabla \mathscr{A}^* \times T_x X$ is given by taking a section $\xi \in \Omega^0(\mathfrak{g}_E)$ and sending it to $$\gamma(\xi) = (d_A\xi, 0)\in \Omega^1(\mathfrak{g}_E)\times T_x X = T_\nabla \mathscr{A}^*\times T_xX.$$ The value of $\Phi(\xi)$ at $x$ is the endomorphism on $\mathfrak{g}_{\pi_2^*E}$ determined by $B(\gamma(\xi))$ . Then we proceed to determine how $\hat{\nabla}_{\gamma(\xi)}$ and $(p^*\nabla)_{\gamma(\xi)}$ perform on sections of $\mathfrak{g}_{\pi_2^*E}$ . I think $p^*\nabla$ is easiest to calculate: since a pullback connection is uniquely determined by the general formula $$
(p^*\nabla)_v(p^*s) = p^*(\nabla_{p_*v} s), \quad v\in T\hat{Y}, s:Y\to E,
$$ we see that $p^*(\nabla)_{\gamma(\xi)}$ is identically $0$ on all sections because $p_*(\gamma (\xi))=0$ , which is true because $\gamma$ maps into the vertical subspace.
However, because $\hat\nabla$ is tautological, it is trivial in the $\mathscr{A}^*$ directions, so I also think $\hat\nabla_{\gamma (\xi)}=0$ . This leads me to think $\Phi$ is identically $0$ , which seems suspect.","['gauge-theory', 'differential-geometry']"
3123275,Why doesn't ds appear in the statement of Green's Theorem?,"I am trying to compare the line integral stated in Green's Theorem with the definition of a line integral. According to Wikipedia: $$
\oint_C(L dx+Mdy)=\int^b_af(\textbf{r}(t))|\textbf{r}'(t)|dt.
$$ My intuition tells me that $\textbf{r}(t)=(x(t),y(t))$ . Let $ds=\sqrt{dx^2+dy^2}$ . Thus, $|\textbf{r}'(t)|dt=ds$ , right? So if $C$ is a simple closed curve, is the expression below equivalent to the expression of the line integral in Green's Theorem? $$
\oint_C(L+M)ds
$$ My question really is: what are $f$ and $\textbf{r}$ in the statement of Green's theorem? I was only able to recreate the statement by definition by letting $L$ and $M$ be independent of $y$ and $x$ respectively, as seen below: $$
\oint_CL(x(t))|\frac{dx}{dt}|dt+M(y(t))|\frac{dy}{dt}|dt.
$$ The problem that I run into here is that $\partial L/\partial y$ and $\partial M/\partial x$ are then both 0.",['differential-geometry']
3123301,"Show that all normals to $\gamma(t)=(\cos(t)+t\sin(t),\sin(t)-t\cos(t))$ are the same distance from the origin.","Show that all normals to $\gamma(t)=(\cos(t)+t\sin(t),\sin(t)-t\cos(t))$ are the same distance from the origin. My attempt: Let $\vec{p}=(\cos(t_0)+t_0\sin(t_0),\sin(t_0)-t_0\cos(t_0))$ be any arbitrary point for $t_0\in\mathbb{R}$ . Then the tangent vector at $\vec{p}$ is given by $\dot\gamma(t_0)=(t_0\cos(t_0),t_0\sin(t_0))\implies$ the slope of the tangent vector at any point is given by $m=\tan(t_0)\implies$ the slope of any normal line is given by $m_{\perp}=-\cot(t_0)$ . Now we calculate the normal line at any point $\vec{p}:$ $$y-(\sin(t_0)-t_0\cos(t_0))=-\cot(t_0)(x-\cos(t_0)-t_o\sin(t_0))\implies$$ $$\cot(t_0)x+y+(2t_0\cos(t_0)+\cot(t_0)\cos(t_0)-\sin(t_0))=0$$ Recall that the distance from $Ax+By+C=0$ and $Q(x_0,y_0)$ is: $$|l,Q|=\frac{|Ax_0+By_0+C|}{\sqrt{A^2+B^2}}$$ Hence $$|l,Q|=\frac{\sqrt{4t_0^2\cos^2(t_0)+\cot^2(t_0)\cos^2(t_0)+\sin^2(t_0)}}{\sqrt{\cot^2(t_0)+1}}$$ How can I proceed from here? Thanks in advance! $$$$ $$$$ Further progress: Following the advice of user429040, the parametric form of any normal line is: $$\mathscr l=(x(t_0)-tt_0\sin(t_0), y(t_0)+tt_o\cos(t_o))$$ The goal is to now minimize the norm of this parametric line over $t$ , and show that this minimum does not depend on $t_0$ : $$|\mathscr l|=|(x(t_0)-tt_0\sin(t_0), y(t_0)+tt_o\cos(t_o))|=((t-1)^2(t_0^2+1))^\frac{1}{2}\implies\min|\mathscr l|=0.$$ However, we can graphically confirm and confirm by Prof. Blatter's answer above that this conclusion is incorrect. Where do I go from here?","['analytic-geometry', 'differential-geometry']"
3123313,Two people take turns coloring a convex polyhedron,"Rachel and Beatrice take turns coloring the faces of a convex polyhedron red and blue, respectively. A player wins if she gets her color on three faces that share a common vertex. If Rachel goes first and both players use their optimal strategies, who wins the game? I don't really understand where to start. I tried playing the game but that didn't help much for me because I couldn't think of any ideas. I know that for a tetrahedron, the game results in a tie. I know that for a cube the first person wins if they pick something adjacent to their first move. After this, I am not sure with a pentagon because if it is a square-based-pyramid, the first person wins if the pick the square but if it is a triangular prism, it results in a tie.","['game-theory', 'polyhedra', 'combinatorics']"
3123320,How do you determine if the series $\sum\limits_{k=1}^\infty \left(1-\frac1k\right)^{k^2}$ converges? [duplicate],"This question already has answers here : Convergence of $\sum _{k=1}^\infty (1-\frac{1}{k})^{k^2}$ [closed] (3 answers) Closed 5 years ago . $$\sum_{k=1}^\infty \mathrm{(1-\frac{1}{k})}^{\mathrm{k}^{2}}$$ I tried using the limit comparison test with $$\sum_{k=1}^\infty \mathrm{(1-\frac{1}{k})}^{\mathrm{k}^{}}$$ but this leads to a limit of 0, which doesn't help. I think this may involve some use of $\mathrm{e}^x$ , but I don't know where else to start. Any suggestions?","['convergence-divergence', 'sequences-and-series']"
3123328,How to tell whether function $x^a \mod{b}$ is bijective or not?,"What conditions must integers $a$ , $b$ suffice to make $x^a \mod{b}$ a bijective function? (I brute force tested out two findings, but not sure whether they are correct or not: If $b$ is prime and $a$ is coprime to $b-1$ then $x^a \mod{b}$ is bijective. If $b$ is not prime, $x^a \mod{b}$ is not bijective unless $a = 1$ . Not sure how to prove these two findings mathematically) Any help is much appreciated!","['elementary-number-theory', 'functions', 'modular-arithmetic']"
3123390,Intrinsic vs. extrinsic surface curvature,"This may be just a terminology question (or not). A 2-cylinder is intrinsically flat. Its curvature cannot be detected from inside (although its topology can be studied by making various round trips). When we view the cylinder as isometrically embedded in $\Bbb{R}^3$ , we see its cylindrical shape and its extrinsic curvature. In this case the intrinsic and extrinsic curvatures are separate and independent, so no confusion. Now consider a 2-sphere. It's curvature can be precisely studied from inside by measuring angles and distances, so it's curvature is intrinsic. When we view the sphere as isometrically embedded in $\Bbb{R}^3$ , we see its spherical shape and its [...] curvature. What term fills the blank? On one hand, the embedded ""shape"" implies extrinsic curvature (at least in the cylinder example). On the other hand, extrinsic curvature is inconsequential inside, but the curvature of the sphere has clear consequences inside. What is the accepted terminology here? Neither extrinsic nor intrinsic seems to fit. What term should be used to describe the curvature of an embedded sphere?","['metric-spaces', 'differential-geometry']"
3123407,"Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Find $|S|$.","Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$ . Find $|S|$ . This is a follow-up question to this . (Thanks to @EricWofsey for brilliant answer.) Please see the previous post to see his proof since I don't want to reproduce it here. I'm phrasing this as a separate question because I want to accept his answer. Still, it's not obvious to me that what's the exact cardinality of $S$ . I haven't found any polynomials with degree $> 3$ satisfy this property. The one who gave the initial problem to me didn't know the answer, either. There is enough evidence to suspect this is going to be much harder. Can someone provide some insight? Many thanks.","['polynomials', 'analysis', 'real-analysis']"
3123409,Can this puzzle be solved mathematically?,"There are $6$ people in a ship. As the journey is long, they decide to take shifts. In each shift, some wake up and control the ship while others sleep. How many such shifts are required so that for any two persons $A$ and $B$ there exists a shift such that $A$ wakes and $B$ sleeps (minimum)? My try: I don't know why but I felt that the minimum number of sittings will be when $3$ persons sleep and the other $3$ wake. So I got the following solution : SLEEP 123 356 146 245 So the required number of sittings is 4. But I think my solution to be a matter of luck. So my question is "" Is there a mathematical solution to this question?""","['puzzle', 'logic', 'combinatorics', 'discrete-mathematics']"
3123545,"If $\|A(x)\|_Y \geq c \|x\|_X$ and $\dim X = + \infty$, can $A \in \mathcal{L}(X,Y)$ be a compact operator?","Exercise : Let $X,Y$ be Banach spaces with $\dim X = + \infty$ and $A \in \mathcal{L}(X,Y)$ such that : $$\|A(x)\|_Y \geq c\|x\|_X \; \; \forall x \in X \; \text{and} \; c>0$$ Can the operator $A$ be compact ? Attempt : First of all, we know that an operator is compact if it transfers bounded sets to relatively compact sets (which means that they have a compact closure). Let's assume now that $A$ is compact. Since $X$ is infinite dimensional, the unit ball $B_1^X$ is bounded (but not totally bounded). That would mean that $\overline{A(B_1^X)}$ should be compact. But, from the inequality relation gives, it would be : $$\|A(B_1^X)\|_Y \geq c\|B_1^X\|_X \implies \|A(B_1^X)\|_Y \geq c' >0$$ But $c'$ could be as large as we'd like and thus the quantity $\|A(B_1^X)\|_Y$ is not bounded, which also means that the $\|\overline{A(B_1^X)}\|_Y$ is also not bounded, thus \overline{A(B_1^X)} is not compact (???). Question : Is my intuition and especially my final argument correct ? If not, what other way could I approach that specific problem ?","['banach-spaces', 'compact-operators', 'operator-theory', 'functional-analysis', 'compactness']"
3123577,"Automorphism group of ($\mathbb{R}^{\times}, \cdot$)","Let ( $\mathbb{R}^{\times}, \cdot$ ) be the multiplicative group of non-zero real numbers. I want to find the automorphism group of ( $\mathbb{R}^{\times}, \cdot$ ). My guess is that it is the group of all rational numbers $p/q$ such that $p, q$ are odd integers which are co-prime. We know that any automorphism of the additive group of real numbers ( $\mathbb{R}, +$ ) is of the form $x \mapsto \lambda x$ . Also, ( $\mathbb{R}_{>0},\cdot $ ) is isomorphic to ( $\mathbb{R}, +$ ), where ( $\mathbb{R}_{>0},\cdot $ ) denotes the multiplicative group of positive reals. $\require{AMScd}$ \begin{CD}
(\mathbb{R}_{>0}, \cdot) @>{\phi}>> (\mathbb{R}_{>0}, \cdot)\\
@V{\log(x)}VV  @VV{\log(x)}V\\
(\mathbb{R},+) @>{\lambda x}>> (\mathbb{R}, +)
\end{CD} By the diagram above, any automorphism $\phi$ of $(\mathbb{R}_{>0}, \cdot)$ is of the form $$ \phi(x) = \exp(\lambda \log(x)) = x^\lambda$$ where $\lambda \in \mathbb{R}^{\times}$ . As any automorphism $\psi$ of ( $\mathbb{R}^{\times}, \cdot$ ) should take positive numbers to positive numbers, $\psi$ must restrict to an automorphism on ( $\mathbb{R}_{>0},\cdot $ ). The only automorphisms of ( $\mathbb{R}_{>0},\cdot $ ) which extend to the automorphisms of ( $\mathbb{R}^{\times}, \cdot$ ) are $x \mapsto x^\lambda$ such that $\lambda = \frac{p}{q}$ such that $p,q$ are odd integers which are co-prime. This is because: For irrational $\lambda$ 's, the map is not well defined for negative reals. For $\lambda = \frac{p}{q}$ , and $q$ even, the map is not well defined for negative reals. For $\lambda = \frac{p}{q}$ , and $p$ even, the inverse of this map does not exist. From here on, I don't know how to proceed in proving my guess, if it is right in the first place.","['automorphism-group', 'group-theory']"
3123582,Set or list compression,"Apologies for poor use of terms, I do not understand enough of the problem to even ask the right questions. My main question is, what domain of mathematics is this problem and is it solved problem which has provably perfect solution or is it fundamentally hard problem. Consider I have list of items describing pair of source and destination, let's imagine for example bus route from a source city to a destination city. And we want to express the list with fewest possible lines without losing information. S1 => D1 S1 => D2 S2 => D1 S2 => D2 D1 => S1 D1 => S2 D2 => S1 D2 => S2 Naive compression algorithm could be: step1, group by Bside S1 => [D1, D2] S2 => [D1, D2] D1 => [S1, S2] D2 => [S1, S2] step2, group by Aside [S1, S2] => [D1, D2] [D1, D2] => [S1, D2] step3, collapse direction [S1, D2] <=> [D1, D2] However this algorithm doesn't guarantee shortest possible list for all possible lists. Because depending on list we have, the order of operations might need to be different for best result or we may need to not compress maximally in earlier step to have better overall compression in the end. adding another dimension to the list Now what if the list items have additional dimension, such as colour: S1 => D1 (red) S1 => D1 (blue) S2 => D1 (red) S2 => D1 (blue) S1 => D1 (green) Would compress to: [S1, S2] => D1 [red, blue] S1 => D1 green Now if we have perfect solution to the 1st problem, the naive approach for the 2nd problem is to group the list by colour, yielding colourless lists. Apply perfect solution to each colour separately then collapse colours if same lines appear in multiple colours. But this approach too suffers from the problem that we might not want to compress earlier perfectly to have best overall compression. Only solution I have is to either not try to do perfect solution or brute-force perfect solution by trying everything.","['well-orders', 'graph-theory', 'information-theory', 'order-theory', 'elementary-set-theory']"
3123605,Why 0.33... is the only expression of 1/3? [duplicate],"This question already has answers here : Rigorous proof that $\frac{1}{3} = 0.333\ldots$ (3 answers) Closed 5 years ago . I am an undergraduate math student who loves mathematics very much, and I am confused by a math problem. Given $1/3$ , we know that $0.33...$ (there are infinite $3$ s) is the decimal expression. But how can we prove that it is the only decimal expression of $1/3$ ? Sorry for my mistakes for my wrong math typing and not good English. Thank you very much for your answers!","['algebra-precalculus', 'decimal-expansion', 'recreational-mathematics']"
3123606,Principal angles between subspaces,"Let $A$ , $B$ be $k$ -dimensional subspaces of an Euclidean space $V$ of dimension $\geq 2k$ . How to find an orthonormal system $x_1,...,x_{2k}$ in $V$ and numbers $\phi_1,...,\phi_k \in [0,\frac{\pi}{2}]$ such that $\{x_1,...,x_k \} $ is a basis (obviously orthonormal) in $A$ and $\{ x_1\cos \phi_1 +x_{k+1} \sin\phi_1,...,x_k\cos \phi_k +x_{2k}\sin \phi_k \} $ is a basis (orthonormal) of $B$ ? I wish a proof or references. I'm not interested in the original Jordan proof from 1875. Thanks.",['linear-algebra']
3123620,An identity involving binomial coefficients and Bernoulli numbers.,"While solving a problem I came across the following identity, which holds by numerical evidence: $$
\sum_{k=1}^i\frac1k\binom{i}{k-1}\binom kj{B_{k-j}}=\delta_{ij}.
$$ where $B$ are the Bernoulli numbers . I have no experience with Bernoulli numbers, so any hint for proving the equality will be appreciated.","['binomial-coefficients', 'combinatorics', 'bernoulli-numbers']"
3123662,A Maximal Order is a Total Order,"A set $X$ together with a binary relation $\leq$ such that for all $x,y,z\in X$ , O $1$ . $x\leq x$ $O2$ . $x\leq y$ and $y\leq x$ then $x=y$ $O3$ . $x\leq y$ and $y\leq z$ then $x\leq z$ is called an ordered or sometiomes a partially ordered . An ordered $(X,\leq)$ is called a total order if for all $x,y\in X$ , either $x\leq y$ or $y\leq x$ . Define $S:=\left\{R\subseteq X\times X : \text{R is a order on X}\right\}$ $(M,\leq)$ is maximal order in $S$ that for any $M\in S$ if $M\subseteq R$ then $M=R$ . Question: Maximal orders are total order. My proof of the question. Define $S:=\left\{R\subseteq X\times X : \text{R is a order on X}\right\}$ . Let $(M,\leq)$ be maximal order in $S$ . We will show $(M,\leq)$ is also a total order. Assume not, that is there is an element of $M$ such that it is not comparable, so define $$M'=M\cup\left\{(a,b)\right\}$$ We need to show $M'$ is a partially ordered, so we need to check: $i)$ $(x,x)\not\in M'$ for any $x\in M'$ . Proof of i). Since $(x,x)\not\in \left\{(a,b)\right\}$ and $(x,x)\not\in M$ because $M$ is a partially order, hence $(x,x)\not\in M'$ for any $x\in M'$ . $ii)$ if $(x,y)\in M'$ and $(y,x)\in M'$ for all $x,y\in M'$ , then $x=y$ . Proof of ii). Assume $(x,y)\in M'$ and $(y,x)\in M'$ . If $(x,y)\in\left\{(a,b)\right\}$ , then $(x,y)=(a,b)$ , so $(y,x)\in M$ , hence $x=y$ . If $(x,y),(y,x)\in M$ , then we would have a contradiction because our assumption $M$ is not a total order. Then, wlog $(x,y)\in\left\{(a,b)\right\}$ . $iii)$ . If $(x,y)\in M'$ and $(y,z)\in M'$ , then $(x,z)\in M'$ . Proof of iii). Assume $(x,y)\in M'$ and $(y,z)\in M'$ . Wlog, $(x,y)\in\left\{(a,b)\right\}$ , then $(x,y)=(a,b)$ , so $(y,z)\in M'$ , hence $(y,z)\in M'$ , so if $(x,y),(y,z)\in M$ , then $M'$ is not transitive because $(a,b)\in M'$ but $(b,a)\not\in M'$ . So, I couldn't coninue, can you help me?","['elementary-set-theory', 'order-theory', 'proof-writing']"
3123688,Question on a step of the proof of Theorem 1.25 of Introduction to Fourier Analysis on Euclidean Spaces,"Theorem 1.25: Suppose $ \phi \in L^1(\mathbb{R^n}) $ and $ \int_{\mathbb{R^n}} \phi =1 $ .
  Also, let $\phi_{\epsilon}(x)=\frac{\phi\left(\frac{x}{\epsilon}\right)}{\epsilon^n}$ .Moreover , suppose that : $ \psi(x)= \operatorname{esssup}_{|y|\geq |x|} |\phi(y)|$ is in $ L^1(\mathbb{R^n}) $ and $ f \in L^p(\mathbb{R^n}) $ for some $ p \in [1,\infty] $ Then $ \lim_{\epsilon \to 0} (f \ast \phi_{\epsilon})(x)=f(x)$ for every $ x \in L_f $ where $L_f$ is the Lebesgue set of $ f $ . In the course of the proof the author obtains for $ x \in L_f$ and for an arbitrary $\delta > 0 $ an $h>0 $ such that : $$
\frac{1}{r^n} \int_{|t|<r} |f(x-t)-f(x)|\mathrm{d}t < \delta , \quad \forall 0<r \leq h
$$ Then the author considers the function $$
g(r)= \int_{S^{n-1}} |f(x-ry)-f(x)| \mathrm{d} \sigma_y
$$ where $ \sigma $ is the surface measure on $ S^{n-1} $ . Then he considers the function $$G(r)=\int_{0}^{r} g(s)s^{n-1} \mathrm{d}s 
$$ Now the step that I do not quite get is why the following calculation is fully justified : $$
\begin{split}
 \int_{ |t|<h} |f(x-t)-f(x)| &\frac{1}{\epsilon^n} \psi\left(\frac{t}{ \epsilon}\right) \mathrm{d}t = \int_{0}^{h} r^{n-1}g(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \mathrm{d}r \\
&= \left. G(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \right|_0^h-\int_0^h G(r)\,\mathrm{d}\left( \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right)\right)
\end{split}$$ While it clear for me why $ G $ and $ \psi $ is differentiable ( $ G $ because of the Lebesgue's differentiation theorem and $ \psi $ because it is decreasing [ in $ r $ ] ) I am not sure why the  integration by parts formula here is valid. For example I know that if R and Q are both absolutely continuous then we do have that $ \int_{a}^{b} RQ^{\prime}+QR^{\prime}=R(b)Q(b)-R(a)Q(a) $ but I am not sure that  this applies here. Thank you in advance for your help.","['fourier-analysis', 'harmonic-analysis', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3123695,ODE Interval of Validity,"The problem is stated as such; Solve the I.V.P $$y'=\frac{1+3x^2}{3y^2-6y}; y(0)=1
$$ and determine the interval in which the solution is valid. The hint given is:  To find the interval of definition, look for points where the integral curve has a vertical tangent line. I was able to solve the IVP with the following solution; $$y^3-3y^2-x-x^3+2=0$$ But with the hint, I am under the assumption that the curve has vertical tangent lines when the denominator of the derivative is equal to 0, which would be when $y=0$ and $y=2$ .  But the solution is give as $|x|<1$ .  What am I doing wrong?","['calculus', 'derivatives', 'ordinary-differential-equations']"
3123715,Minimum value of expression having $2$ variables,"Minimum value of $$\bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2$$ for real $x\geq 0,y\in[-2,2]$ Try: Using Partial derivative $$f(x,y) = \bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2$$ $$\frac{df(x,y)}{dx}=2\bigg(x-4-\sqrt{4-y^2}\bigg)+\frac{4}{\sqrt{x}}\bigg(4\sqrt{x}-y\bigg)$$ $$\frac{df(x,y)}{dy}=-2y\frac{\bigg(x-4-\sqrt{4-y^2}\bigg)}{\sqrt{4-y^2}}-2\bigg(4\sqrt{x}-y\bigg)$$ Put $\displaystyle \frac{df}{dx} =0$ and $\displaystyle \frac{df}{dy}=0$ But these $2$ equation is tedious work Could some help me how to solve it , Thanks in advance",['multivariable-calculus']
3123716,Number of ways the room can be tiled with $I$ shaped and $L$ shaped tiles,"There is a room of dimension $2\times 12$ units. You have to tile it. There are two types of tiles: I-Shaped - it is of a dimension $1\times 2$ units, L-Shaped -  it is in the shape of $L$ which has an area of $3$ sq. units. In how many ways can you tile it? I have studied some basics of permutation and combination but I can't use it in the this question. Note : The $I$ shaped tiles are identical from both sides. You can rotate the tiles in any direction if it will fit. You have an unlimited supply of tiles.","['logic', 'combinatorics', 'puzzle']"
3123735,The set at which a function's oscillation is at least $\epsilon$ is closed,"Let $f$ be a bounded function on a compact interval $J$ , and let $I(c,r)$ denote
  the open interval centered at $c$ of radius $r>0$ . Let $osc(f,c,r)=\sup|f(x)-f(y)|$ , where
  the supremum is taken over all $x,y\in J\cap I(c,r)$ , and define the oscillation of $f$ at $c$ by $osc(f,c)=\underset{r\rightarrow 0}{\lim}osc(f,c,r)$ . Clearly, $f$ is continuous
  at $c\in J$ if and only if $osc(f,c)=0$ . Prove that for every $\epsilon>0$ , the set of points $c$ in $J$ such that $osc(f,c)\geq \epsilon$ is compact. My Proof Attempt: Proof. Let the assumptions be as above. Let $\epsilon > 0$ . Define \begin{equation*}
D_\epsilon=\{c\in J: osc(f,c)\geq \epsilon\}
\end{equation*} It suffices to show that $D_\epsilon$ is closed as any closed subset of a compact set,
 in this case namely $J$ , is also a compact set. Let $(x_n)_n\subset D_\epsilon$ such that $x_n\rightarrow x\in J$ . We will prove that $x\in D_\epsilon$ and, thus, $D_\epsilon$ would be
closed. Claim : Since $x_n\in D_\epsilon$ , that implies that for sufficiently small $\varrho$ , \begin{equation*}
osc(f,x_n,\varrho)\geq \epsilon
\end{equation*} We will prove this claim by the following: Let $r>r'>0$ and $x\in J$ , then $J\cap I(x,r')\subset J\cap I(x,r)$ . This means that $osc(f,x,r)\geq$ $osc(f,x,r')$ since $osc(f,x,r)$ is defined to be the supremum over the set $\{d(f(x),f(y)): x,y\in J\cap I(x,r)\}$ . But any $x,y\in J\cap I(x,r')$ must also be in $J\cap I(x,r)$ . Thus, $osc(f,x,r)$ cannot increase as $r$ is made smaller. If $osc(f,x)\geq \epsilon$ , It cannot be that for ""really small"" $r>0$ that $osc(f,x,r)<\epsilon$ . Because then the limit cannot be $\geq \epsilon$ . This proves the Claim . Now, given any $r>0$ , either $(x-\frac{r}{2},x)\subset J\cap I(x,r)$ or $(x,x+\frac{r}{2})\subset J\cap I(x,r)$ , or
both. WLOG, suppose $(x-\frac{r}{2},x)\subset J\cap I(x,r)$ and denote it as $I$ . 
Then $\exists N\in \mathbb{N}$ such that for all $n\geq N,$ it follows that $x_n\in I$ . 
As $I$ is open, we can
choose $\varrho'$ sufficiently small such that $B_{\varrho'}(x_n)\subset I$ . Set $\rho=max\{\varrho, \varrho'\}$ . Then \begin{equation*}
osc(f,x,r)\geq osc(f,x_n,\rho)\geq \epsilon
\end{equation*} Since we choose $N$ and $\rho$ for any given $r>0$ , 
it follows that $osc(f,x)\geq \epsilon$ and $x\in D_\epsilon$ . This completes our proof. Any corrections of the proof or comments on style
are very much appreciated. Thank you guys for your time.","['measure-theory', 'proof-verification', 'real-analysis']"
