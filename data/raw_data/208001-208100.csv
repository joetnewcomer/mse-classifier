question_id,title,body,tags
4167561,"How to know that we have found the correct solution set of an inequality? AND why is ""my method"" not working? [duplicate]","This question already has answers here : Algebra: What allows us to do the same thing to both sides of an equation? (12 answers) Closed 3 years ago . Background: I was solving this inequality: $(1-4x)^{-1}\geq 7$ . I did it as follows: $$
\begin{array}{l}
\quad(1-4 x)^{-1} \geq 7 \\
\Rightarrow \frac{1}{1-4 x} \geq 7 \\
\Rightarrow \frac{1}{7} \geq 1-4 x \\ 
\quad\left[\begin{array}{l}
1-4 x \neq 0 \text { and is  positive as if }\\
 \frac{a}{b}>0 
\text { and } a>0 \Rightarrow b>0.\\
\text {Hence I can multiply} \text{the sides with 1-4x,}\\
\text{without reversing sign. }
\end{array}\right] \\
\Rightarrow \frac{-6}{7} \geq-4 x \\
\Rightarrow x \geq\frac{6}{28} \\
\Rightarrow x \geq\frac{3}{14} \\
\therefore \text { Soln set }=\left[\frac{3}{14}, \infty\right)
\end{array}
$$ But this is not the answer; the answer is: $
\left\{x \mid \frac{3}{14} \leq x<\frac{1}{4}\right\} \text { or }\left[\frac{3}{14}, \frac{1}{4}\right)
$ After thinking some time I realised that the other end point can be found if we solve the question as follows: $$
\begin{aligned}
& \frac{1}{1-4 x} \geq 7 \\
\Rightarrow & 1-4x>0 \quad[\text { The reason is same }] \\
\Rightarrow & x<\frac{1}{4}
\end{aligned}
$$ Question: This post has two main questions: First question: I know that the maximum number of roots an equation, in one variable of any degree, can have, will be same as its degree. So for the inequalties, how could I be sure that I have found all the end point(s)? In some question the solution set is union of other two disjoint sets, So in this cases how could I be sure that I have found all the disjoint sets?
For this specific question, how could I be sure that this is finally the solution set? Can't it be that the author(of the book the problem is from) missed considering other inequalities, like me, which could have given us another set(s), so finally the answer would be union of those sets? So I can sum up the above question in this question: How to know that we have found the correct solution set of an inequality? Second question: Adding, subtracting and multiply(by non-zero number/polynomial) both side by same number/polynomial always led us to equivalent equation and inequality. In this case I multiplied by $1-4x$ , which I know for sure is not zero. So according to the logic I must get an equivalent inequality i.e. the solution set of the new inequality must be same as that of the original inequality, but apparently this is not the case with this question i.e. solution set of $(1-4x)^{-1}\geq 7$ is not same as that of $\frac{1}{1-4x}\geq 7$ . Why? I can get the correct answer by solving as others have mentioned in the answer, but, the second question is, why is ""my method"" not working?","['algebra-precalculus', 'inequality']"
4167567,Can we express the theory of a single topology as a multi-sorted theory?,"I've heard the result before that the theory of topologies cannot be expressed as a first-order theory , but I can come up with a simple multisorted theory that seems to capture the open set axiomatization of a topology. I'm familiar with the statement that the theory of topologies isn't first order mostly as a quick way to demonstrate that first-order logic doesn't capture every theory we might be interested in (similar to referencing torsion groups). I'm curious whether this attempt at a first-order theory for a single topology succeeds or fails, and what extra subtlety there is in the statement ""topologies are not a first-order theory"". This axiomatization uses variations of the axioms and axiom schemas in the simple theory of types and the set theory NFU . Multiple sorts can be paraphrased away into predicates giving us a single-sorted theory at the cost of some verbosity. First, we have three sorts. $X$ is the sort of the underlying topological space, $S$ for ""set"" is the sort corresponding to $2^X$ informally and $F$ for ""family"" is the sort corresponding to $2^{2^X}$ informally. I will now attempt to construct a theory of topology. The sorts are as above and there are three predicate symbols. $\in$ is a predicate symbol corresponding to the elementhood relation. $O$ is a unary predicate on $S$ that identifies open sets and $O^F$ is a unary predicte on $F$ that identifies families of open sets. First, we have two axiom schemas of comprehension. Comprehension from $X$ to $S$ . $$ \exists s : S \mathop. \forall x : X \mathop. x \in s \leftrightarrow \varphi(x) $$ Comprehension from $S$ to $F$ . $$ \exists f : F \mathop. \forall s : S \mathop. s \in f \leftrightarrow \varphi(x) $$ And we have two axioms of extensionality, one for each level. $$ \forall a : S \mathop. \forall b : S \mathop. (\forall x : X \mathop. x \in a \leftrightarrow x \in b) \to a = b $$ $$ \forall a : F \mathop. \forall b : F \mathop. (\forall s : F \mathop. s \in a \leftrightarrow s \in b) \to a = b $$ For the remaining cases I'll omit sort annotations for the sake of brevity. These axioms should be understood to apply at both the $S$ and $F$ levels. Definition of the complement $$ \forall x \mathop. \forall a \mathop. a \in x^c \leftrightarrow a \not\in x $$ Definition of binary intersection $$ \forall a \mathop. \forall b \mathop. \forall x \mathop. (x \in a \cap b) \leftrightarrow (x \in a \land x \in b) $$ Definition of binary union $$ \forall a \mathop. \forall b \mathop. \forall x \mathop. (x \in a \cup b) \leftrightarrow (x \in a \lor x \in b) $$ The definition of arbitrary union applies only to $F$ . $$ \forall f \in F \mathop. \forall x \in X \mathop. (x \in \cup f) \leftrightarrow (\exists b : S \mathop. x \in b \land b \in f) $$ $O^F$ is defined as follows. A family of open sets is a family of sets where every set is open. $$ \forall f : F \mathop. O^F(f) \leftrightarrow (\forall s : S \mathop. s \in f \to O(s)) $$ And, finally the axioms of topology. The empty set is open. $\varnothing$ is a constant in sort $S$ . $$ O(\varnothing) $$ The whole space is open. $X$ is a constant in sort $S$ . $$ O(X) $$ Open sets are closed under binary intersection. $$ \forall a : S \mathop. \forall b : S\mathop. O(a) \land O(b) \to O(a \cap b) $$ Open sets are closed under arbitrary union. $$ \forall f : F \mathop. O^F(f) \to O(\cup f) $$ This completes the multisorted axiomatization of topological space.","['general-topology', 'first-order-logic']"
4167568,Finding a confidence interval for parameter of normal distribution,"We observe a simple random sample $(X_1,\ldots , X_n$ ), where the distribution of observed property $X$ is given by its density: $f(x)=\frac{1}{x\sigma \sqrt{2\pi}}e^{-\frac{(\ln x-m)^2}{2\sigma^2}}, x>0$ . Find a 80% confidence interval for $\sigma$ , if we know: Mean value of logarithms of the values from the dataset that we get deviates from its expected value no more than $\frac{\sigma}{2}$ , with probability 0.98758; If $(x_1, \ldots ,x_n)$ is our dataset (a realisation of our sample), $\sum_{i=1}^n \ln x_i=3.139797$ and $\sum_{i=1}^n (\ln x_i)^2=35.63178$ . My attempt: It's not hard to see that, if $Y=\ln X$ , then $Y$ has $\mathscr{N}(m,\sigma^2)$ distribution. Now we work with $Y$ . Let $$\overline{Y}_n=\frac{1}{n}\sum_{i=1}^n Y_i$$ $$\overline{S}_n^2=\frac{1}{n}\sum_{i=1}^n (Y_i-\overline{Y}_n)^2$$ .
We know that $\frac{n\overline{S}_n^2}{\sigma^2}$ has $\chi_{n-1}^2$ distribution. If we look for confidence interval in the form: $$p\{U_n\leq \sigma \leq V_n\}=0.8$$ , we can actually demand $$p\{\frac{n\overline{S}_n^2}{V_n^2}\leq  \frac{n\overline{S}_n^2}{\sigma^2} \leq \frac{n\overline{S}_n^2}{U_n^2} \}=0.8$$ . We can demand: $$p\{\frac{n\overline{S}_n^2}{V_n^2}>\frac{n\overline{S}_n^2}{\sigma^2}\}=p\{\frac{n\overline{S}_n^2}{U_n^2}<\frac{n\overline{S}_n^2}{\sigma^2}\}=\frac{1-0.8}{2}=0.1$$ .
This gives us (from what we know about $\chi_{n-1}^2$ distribution): $$V_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.9}^2}$$ $$U_n^2=\frac{n\overline{S}_n^2}{\chi_{n-1,0.1}^2}$$ . I assume that the info given in this problem means that one confidence interval for $\frac{1}{n}\sum_{i=1}^n \ln x_i-m$ , with confidence level $0.98758$ is $[-\frac{\sigma}{2}, \frac{\sigma}{2}]$ . But I don't know how to move on from here. We also know that for: $$\overline{S}_*^2=\frac{1}{n}\sum_{i=1}^n (Y_i-m)^2$$ $\frac{n\overline{S}_*^2}{\sigma^2}$ has $\chi_n^2$ distribution. I thought maybe we should use this because of this CI that has been given to us, but I don't know how to do that.","['statistical-inference', 'statistics', 'confidence-interval', 'probability-distributions', 'probability']"
4167586,"Exercise 5, Chapter 8. Complex analysis, Stein","Suppose that $F:\mathbb{H}\rightarrow\mathbb{C}$ is holomorphic and bounded where $\mathbb{H}$ is the upper-half plane. Also suppose that $F(z)$ vanishes when $z=ir_n$ , $n=1,2,\ldots,$ where $\{r_n\}$ is a bounded sequence of positive numbers. Prove that if $\sum_{n=1}^\infty r_n=\infty$ then $F=0$ . Progress: Since $\{r_n\}$ is bounded has a subsequence convergent $\{r_{n_k}\}$ to $r_0\in [0,\infty)$ . If $r_0\in (0,\infty)\subset \mathbb{H}$ then $F=0$ . So I think the problem is when $r_0=0$ . In that case, for simplicity, I considered that $\{r_n\}$ converges to $0$ . I tried to use the Schwarz lemma , but I get an inequality that doesn't seem to help: $$F(z)\leq M\left| \frac{r_{n_0}-r_n}{r_{n_0}+r_n}\right|,\forall |z|\leq r_n,z\in \mathbb{H}$$ where $M$ is a bound for $F$ and $r_{n_0}> r_n$ are sufficiently small.",['complex-analysis']
4167686,I need help with some formal proof writing,"Okay so I have started to teach myself ""Abstract algebra"", but before that, I have to clear the preliminaries such as set theory and Linear algebra. So I was trying to prove the idempotent law $A \cup A = A$ I need help if someone could check if the proof looks formal or not. The Proof: Lemma: $A \cup A = A$ Proof: $\text{Let x be an arbitrary element of} A \cup A, \\\Rightarrow x\in A \cup A\\\Rightarrow x \in A \lor x\in A \\ \Rightarrow x \in A \\ \Rightarrow A \subset A \cup A \hspace{20mm} ... (i) \\.\\\text{Let y be an arbitrary element of A,}\\ \Rightarrow y \in A \\ \Rightarrow y \in A \lor y\in A \\ \Rightarrow y \in A \cup A \\ \Rightarrow A \cup A \subset A \hspace{20mm}...(ii)\\.\\\text{From statement (i) & (ii) we conclude that,}\\ A \cup A = A\\\hspace{40mm}Q.E.D$ Okay so this pretty much, how I did it, please check it and tell me at what places I had to write some extra steps or which steps can be removed and is the proof formal or not? Thank you for reading! (PS: I am pretty bad at latex so maybe parts of the proof look bad, also I am a high school student and haven't taken any Proof based or discrete mathematics class.)","['proof-writing', 'abstract-algebra', 'solution-verification', 'discrete-mathematics']"
4167688,Can we always draw a circle that is internally tangent to three circles?,"Here is the picture of the problem: I'm looking for an intuitive explanation and also an outline of how a rigorous proof would look like.
Our professor directly started to speak about finding the radius of the small circle, but it got me thinking. How can we ensure that such a circle always exists? I researched and came across Appolonius Problem , but I couldn't find an explanation for my problem. I know that three points define a circle, but how does that help me? Thank you in advance.","['euclidean-geometry', 'trigonometry', 'geometry', 'geometric-construction']"
4167713,"Suppose $\lim_n e^{itb_n}$ exists for all $|t|\le \delta$, show that $\limsup |b_n| < \infty$","I would like some help with the following: Let $(b_n)_{n \ge 1}$ be a sequence of numbers such that $\lim_n e^{itb_n}$ exists for all $|t|\le \delta$ , $\delta >0$ . Show that $\limsup |b_n| < \infty$ . Edit: as per Mindlack 's hint, below is an attempt to a solution. For each $n$ define $$
f_n(t) = e^{itb_n}, \quad |t| < \delta.
$$ Then, by assumption, $(f_n(t))_n$ converges for every $|t|\le \delta$ . Obviously each $f_n$ is square integrable since $$
\int_{-\delta}^{\delta} \left | f_n \right |^2 \, d \mu = \int_{-\delta}^{\delta} 1 \, d \mu = 2\delta.
$$ As pointwise convergence implies convergence in $L^2$ , and $L^2([-t,t], \mu)$ is a complete metric space, the limit is also square integrable. To calculate the Fourier transform of this limit, one may use the Dominated convergence theorem. \begin{align*}
\int_{-\delta}^{\delta} e^{-2 \pi it \xi}\lim_n f_n(t) \, d \mu(t) &= \lim_n\int_{-\delta}^{\delta} e^{-2 \pi it \xi}f_n(t) \, d \mu(t) \\
&= \lim_n\int_{-\delta}^{\delta} e^{-2 \pi it \xi}e^{itb_n} \, d \mu(t) \\
&= \lim_n\int_{-\delta}^{\delta} e^{it(b_n -2 \pi \xi)}\, d \mu(t)\\
&= \lim_n \left (\frac{e^{i\delta(b_n -2 \pi \xi)}}{i(b_n -2 \pi \xi)} - \frac{e^{-i\delta(b_n -2 \pi \xi)}}{i(b_n -2 \pi \xi)} \right ) \\
&= \lim_n \frac{2i \sin(\delta(b_n -2 \pi \xi))}{i(b_n -2 \pi \xi)} \\
&= \lim_n \frac{2 \sin(\delta(b_n -2 \pi \xi))}{b_n -2 \pi \xi}.
\end{align*} I'm not sure what conclusion I could draw from this? Thanks in advance!","['complex-analysis', 'limsup-and-liminf', 'complex-numbers', 'sequences-and-series']"
4167732,Symmetric Numerical Semigroups for a fixed Embedding Dimension,"I was trying to find some symmetric Numerical Semigroups with embedding dimensions $5,6,7,...$ so on, like $S:=\langle 6,17,27,28,38\rangle$ (Frobenius Number = $49$ ). Now, I want to know if there is any general formula to find explicitly the list of the all possible symmetric Numerical Semigroups for a fixed embedding dimension or any general form. I got a general form of symmetric Numerical Semigroups in the book "" Numerical Semigroups by J.C. Rosales & P.A. Garcia Sanchez "". The form is like this, $S:=\langle m,m+1,qm+2q+2, . . . ,qm+(m−1)\rangle$ ; Multiplicity ' $m$ ', embedding dimension ' $(m-2q)$ ', Frobenius Number ' $(2qm+2q+1)$ '. But, I want all the symmetric Numerical Semigroups or any general form for a fixed Embedding Dimension otherthan this. Can anyone help me ? Thanks in advance.","['algebraic-geometry', 'semigroups']"
4167778,"Asymptotic expansion of $\int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1}$ as $k\to0$","I have the integral $$I = \int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1}$$ and I would need the asymptotic expansion of the integral for small values of $k$ . For $k=0$ we get quite easily $I=4\sqrt{2}$ , while for $k\gg 1$ , we have $I\sim 4|k|$ . Now, trying to perform the integration using special functions (at first glance, one would think that it could be done in terms of elliptic functions) yields no result. Gradštejn and Ryžik are of no help, nor Mathematica or other softwares. We can Taylor expand the integrand and integrate each term of the expansion, but the $2n$ -th term, for $n>1$ , reads $$ \frac{k^{2n}}{(2n)!}\frac{ (-1)^{n-1} (2n-3)!! }{2^n} \frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}}$$ having set $k=0$ in the derivative. Of course, integrating each term would mean performing $$\int_0^{2\pi}\mathrm{d} \theta\,\frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}}$$ which is divergent for $n>1$ . Any ideas about how to proceed? Are there some kind of 'regularization' techniques that could help?","['integration', 'calculus', 'definite-integrals', 'taylor-expansion']"
4167787,Minimize $x^*(A+A^*)x$ such that $x^*A^*Ax=1$ and $x^*x=1$,"Given $A\in\mathbb{C}^{n\times n}$ , such that it has singular values larger than $1$ and smaller than $1$ , \begin{array}{ll} \underset{x\in\mathbb{C^n}}{\text{minimize}} & x^*(A+A^*)x.\\ \text{subject to} & x^*A^*Ax=1,\\&x^*x=1\end{array} My attempt: I couldn't get anything done for general $A$ . Assume $A$ is hermitian, then $A=U\Sigma U^*$ , where $U$ is unitary and $\Sigma$ is real diagonal. Let $y=U^*x$ , then \begin{array}{ll} \underset{y\in\mathbb{C^n}}{\text{minimize}} & 2y^*\Sigma y.\\ \text{subject to} & y^*\Sigma^2y=1,\\&y^*y=1\end{array} Using Lagrange multiplier, we can find: $L(y,\lambda_1,\lambda_2)=2y^*\Sigma y-\lambda_1(y^*\Sigma^2y-1)-\lambda_2(y^*y-1)$ . Then \begin{align}
&\frac{\partial L(y,\lambda_1,\lambda_2)}{\partial y}=4\Sigma y-2\lambda_1\Sigma^2y-2\lambda_2y=0
\end{align} gives us $y_i=0$ or $\lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0$ for all $i=1,2,\ldots,n.$ From $\lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0$ , we can get $\sigma_i=\frac{1\pm\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , so here I think that if there are many distinct $\sigma_i$ 's, then we will get $\sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , $\sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ (here $j_i$ is any number from $1$ to $n$ ), and $y_k=0$ for all $k\neq j_2$ and $k\neq j_1$ . From $\sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , $\sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , we can find that $\lambda_1=\frac{2}{\sigma_{j_1}+\sigma_{j_2}}$ and $\lambda_2=\frac{2\sigma_{j_1}\sigma_{j_2}}{\sigma_{j_1}+\sigma_{j_2}}$ . From this observations, I got impression that for Hermitian $A$ , original problem is equivalent to \begin{array}{ll} \underset{y\in\mathbb{C^2}}{\text{minimize}} & 2y^*\begin{bmatrix}
\sigma_{j_1} & \\
 & \sigma_{j_2}
\end{bmatrix} y\quad=\quad\underset{\sigma_{j_1},\sigma_{j_2}}{\text{minimize}} &\sigma_{j_1}\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}}+\sigma_{j_2}\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}}\\ \text{subject to} & y^*\begin{bmatrix}
\sigma_{j_1} & \\
 & \sigma_{j_2}
\end{bmatrix}^2y=1,&\sigma_{j_1}>1\\&y^*y=1&\sigma_{j_2}<1\end{array} when $y_1=\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}}$ and $y_2=\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}}$ . Can you please tell me if the above calculations make sense? Any help on solving (analytically or numerically) the original problem for general $A$ would be appreciated.","['qcqp', 'normed-spaces', 'linear-algebra', 'optimization', 'singular-values']"
4167867,"If $\dfrac{1-\sin x}{1+\sin x}=4$, what is the value of $\tan \frac {x}2$?","If $\dfrac{1-\sin x}{1+\sin x}=4$ , what is the value of $\tan \frac
{x}2$ ? $1)-3\qquad\qquad2)2\qquad\qquad3)-2\qquad\qquad4)3$ Here is my method: $$1-\sin x=4+4\sin x\quad\Rightarrow\sin x=-\frac{3}5$$ We have $\quad\sin x=\dfrac{2\tan(\frac x2)}{1+\tan^2(\frac{x}2)}=-\frac35\quad$ . by testing the options we can find out $\tan(\frac x2)=-3$ works (although by solving the quadratic I get $\tan(\frac x2)=-\frac13$ too. $-3$ isn't the only possible value.) I wonder is it possible to solve the question with other (quick) approaches?","['algebra-precalculus', 'trigonometry']"
4167906,"If a matrix $A$ is orthogonal, show that $\Delta(f \circ A) = (\Delta f)\circ A$","Let $O(d)$ be the space of orthogonal $d \times d$ matrices over $\mathbb{R}$ . These matrices act on $C_c^2(\mathbb{R}^d)$ , the set of twice continuously differentiable functions $f : \mathbb{R}^d \to \mathbb{R}$ with compact support, via $(f \circ A)(x) := f(Ax)$ , where $A \in O(d)$ . I am trying to show that for all $f \in C_c^2(\mathbb{R}^d) $ and for all $A \in O(d)$ , we have $\Delta(f \circ A) = (\Delta f)\circ A$ . Here, $\Delta$ denotes the Laplace operator. I am aware of the hint in the answer given to this related question here , however I am unable to complete the proof. Further hints on how exactly to apply the chain rule, as well as a full proof would be very much appreciated. Thanks! Edit : Until now, I have the following: Let $A := (a_{ij})$ , $x := (x_1,...,x_d)$ . Since $AA^\top = I$ , we have $$ \sum_{j=1}^d a_{ij}a_{kj} = \delta_{ik}, $$ where $\delta_{ik}$ is the Kronecker Delta. We thus have $$(f \circ A)(x) = f(Ax) = f \left( \sum_{i=1}^da_{1i}x_i,..., \sum_{i=1}^da_{di}x_i\right).$$ We denote $z_i = g_i(x_1,...,x_d) = \sum_{k=1}^da_{ik}x_k$ . With the chain rule $$ \frac{\partial f}{\partial x_j} = \sum_{i=1}^d  \frac{\partial f}{\partial z_i} \frac{\partial z_i}{\partial x_j},$$ we get that \begin{align} 
\frac{\partial}{\partial x_j}(f \circ A)(x) &= \frac{\partial}{\partial x_j}f(z_1,...,z_d)\\
&= \sum_{i=1}^d \frac{\partial f(z_1,...,z_d)}{\partial z_i} \frac{\partial g_i(x_1,...,x_d)}{\partial x_j}\\
&= \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i}.
\end{align} In the next step we get \begin{align}
\frac{\partial^2}{\partial x_j^2}(f \circ A)(x) = \frac{\partial}{\partial x_j} \left( \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i} \right) = \sum_{i=1}^d a_{ij} \frac{\partial^2 f(z_1,...,z_d)}{\partial x_j \partial z_i},
\end{align} but I'm not sure if this last step is valid.
Now, if we compute $(\Delta f) \circ A$ , we get \begin{align}
((\Delta f)\circ A)(x) &= (\Delta f)(Ax)\\ 
&= \left( \sum_{j=1}^d \frac{\partial^2f}{\partial x_j^2} \right) (z_1,....z_d)\\
&= \sum_{j=1}^d \frac{\partial^2f(z_1,...,z_d)}{\partial x_j^2},
\end{align} and since we already computed the first and second partial derivative of $f$ , we get that $\Delta(f \circ A) = (\Delta f) \circ A$ . I have two problems: I haven't used the fact that $A$ is orthogonal. I'm unsure whether or not I'm using the definition of the chain rule correctly. Any help is appreciated!","['orthogonality', 'real-analysis', 'laplacian', 'multivariable-calculus', 'linear-algebra']"
4167917,Density of Kernel Operating on a measure,"Suppose $(S, \mathcal{S})$ is a measurable space, $K:S\times \mathcal{S}\to [0, 1]$ is a Markov Kernel and $\mu:\mathcal{S}\to [0, 1]$ is a probability measure. The kernel operates on measures so the following is a measure on $(S, \mathcal{S})$ $$
\mu K(A) = \int \mu(dx) K(x, A) \qquad \qquad A\in\mathcal{S}
$$ Assuming all the regularity conditions needed, what is the density of this new measure? For instance, suppose that $\mu$ and $K$ have densities $$
\frac{d \mu}{d \lambda} = p_\mu \qquad \text{and} \qquad \frac{d K(x, \cdot)}{d \lambda} 
 = p_K 
$$ with respect to some dominating (Lebesgue) measure $\mu \ll \lambda$ and $K\ll \lambda$ . What is the density of $\mu K$ ? $$
\frac{d \mu K}{d \lambda} = \frac{d}{d\lambda} \int K(x, A) d\mu(x) = ?
$$","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'probability-theory', 'probability']"
4167925,Is there a closed-form for $\sum _{k=1}^{\infty }\frac{\operatorname{Si}\left(k\right)}{k^2}$?,"So far I've got this: $$\sum _{k=1}^{\infty }\frac{\operatorname{Si}\left(k\right)}{k^2}=\int _0^1\left(\sum _{k=1}^{\infty }\frac{\sin \left(kx\right)}{k^2}\right)\frac{1}{x}\:dx$$ $$=\int _0^1\frac{\operatorname{Cl}_2\left(x\right)}{x}\:dx=\operatorname{\mathfrak{R}} \left\{\int _0^1\ln \left(1-e^{ix}\right)\ln \left(x\right)\:dx\right\}$$ $$=\frac{1}{2}\ln \left(2\right)\int _0^1\ln \left(x\right)\:dx+\frac{1}{2}\int _0^1\ln \left(1-\cos \left(x\right)\right)\ln \left(x\right)\:dx$$ $$=-\ln \left(2\right)+2\ln \left(2\right)\int _0^{\frac{1}{2}}\ln \left(\sin \left(x\right)\right)\:dx+2\int _0^{\frac{1}{2}}\ln \left(\sin \left(x\right)\right)\ln \left(x\right)\:dx$$ Where $\operatorname{Si}\left(k\right)=\int _0^k\frac{\sin \left(x\right)}{x}\:dx$ is the sine integral and $\operatorname{Cl}_2\left(x\right)=\sum _{k=1}^{\infty }\frac{\sin \left(kx\right)}{k^2}$ is the Clausen function. Note that: $$\int _0^{\frac{1}{2}}\ln \left(\sin \left(x\right)\right)\:dx=\frac{i}{8}-\frac{i}{2}\zeta \left(2\right)-\frac{1}{2}\ln \left(1-e^i\right)+\frac{i}{2}\operatorname{Li}_2\left(e^i\right)+\frac{1}{2}\ln \left(\sin \left(\frac{1}{2}\right)\right)$$ But I've no idea how to even start with that other integral, how to proceed with it or the sum?","['integration', 'polylogarithm', 'definite-integrals', 'sequences-and-series']"
4167981,Understanding a Detail in Proof of Chain Rule,"I am currently studying from Courant's Differential and Integral Calculus, and I have been trying to understand his proof of the chain rule, which uses the idea that the derivative is the best linear approximation of the function at a point. To give context for Courant's notation, I will write out his definition of a compound function before giving his proof of the chain rule: Definition of compound function: Let $\phi (x)$ be a function which is differentiable in an interval $x \in [a, b]$ and assumes all values in the interval $\phi \in [\alpha, \beta]$ . Consider a second differentiable function $g(\phi)$ of the independent variable $\phi$ , in which the variable $\phi$ ranges over the interval from $\alpha$ to $\beta$ . We can now regard the function $g(\phi) = g \{ \phi (x) \} = f(x)$ as a function of x in the interval $x \in [a, b]$ . Proof of Chain Rule: For any arbitrary $\Delta x \neq 0$ and corresponding values $\Delta \phi$ and $\Delta g$ there exist two quantities $\epsilon$ and $\eta$ , tending to 0 with $\Delta x$ , such that $$\Delta g = g^{'}(\phi)\Delta \phi + \epsilon\Delta\phi, \\ \Delta \phi = \phi^{'}(x)\Delta x + \eta\Delta x;$$ we have only to calculate $\eta$ from the second equation and, where $\Delta \phi \neq 0$ , $\epsilon$ from the first equation, while if $\Delta \phi$ = 0, we put $\epsilon = 0$ . If in the first of these equations we now substitute the value of $\Delta \phi$ from the second equation, we obtain $$\Delta g = g^{'}(\phi)\phi^{'}(x)\Delta x + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}\Delta x$$ which can be rewritten as $$\frac {\Delta g}{\Delta x} = g^{'}(\phi)\phi^{'}(x) + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}.$$ In this equation, we can let $\Delta x$ tend to $0$ , and the bracket on the right tends to zero with $\Delta x$ . The left hand side of our equation has a limit $f^{'}(x)$ , and this limit is equal to the first term on the right hand side: $$f^{'}(x)=g^{'}(\phi)\phi^{'}(x)$$ The part I am confused about in Courant's proof is why he goes out of his way to state: we have only to calculate $\eta$ from the second equation and, where $\Delta \phi \neq 0$ , $\epsilon$ from the first equation, while if $\Delta \phi$ = 0, we put $\epsilon = 0$ Why do we have to set $\epsilon$ to equal $0$ when $\Delta \phi$ is $0$ ? Why can't we set $\epsilon$ to equal to 1? From my naive perspective, I can't see an issue of setting $\epsilon$ to 1 when $\Delta \phi$ is $0$ . From what I understand, the entire proof is only based on the condition that $g(\phi)$ and $\phi(x)$ are differentiable, which is what allows us to write the expressions for $\Delta g$ and $\Delta \phi$ and let us claim that there exist two quantities $\epsilon$ and $\eta$ that tend to $0$ with $\Delta x$ . Is there something I am misunderstanding about Courant's proof or is my interpretation of his proof correct?","['calculus', 'derivatives', 'real-analysis']"
4168020,"Proving $\frac{a}{b}+\left(\frac{3b+c}{3c+b}\right)^2+\left(\frac{2c+a}{2a+c}\right)^3\ge 3$ for positive $a$, $b$, $c$","Let $a,b,c$ be positive real number. Prove that $$\dfrac{a}{b}+\left(\dfrac{3b+c}{3c+b}\right)^2+\left(\dfrac{2c+a}{2a+c}\right)^3\ge 3$$ It is not a symmetric inequality. I know that the equality occur when $a=b=c$ but It's hard to know  how to approach it. First, let $x=\dfrac{a}{b},y=\dfrac{b}{c},z=\dfrac{c}{a}$ , then $xyz=1$ and then inequality becomes $$x+\left(\dfrac{3y+1}{y+3}\right)^2+\left(\dfrac{2z+1}{z+2}\right)^3\ge 3 \tag1$$ My direction is use UCT method but $$\left(\dfrac{3y+1}{y+3}\right)^2\ge y\quad\Leftrightarrow\quad y\le 1 \tag2$$ and similar to $$\left(\dfrac{2z+1}{z+2}\right)^3\ge y \tag3$$ and I don't have any information about $y,z$ . I wonder if it was the right way to solve it! Please give me some hint! Thanks!","['contest-math', 'algebra-precalculus', 'cauchy-schwarz-inequality', 'inequality']"
4168045,Showing the data processing inequality,"Let $X \sim P_\theta$ for some distribution $P_\theta$ parametrized by $\theta \in \Theta \subset \mathbb R$ and $Y \sim Q(\cdot | X)$ for some distribution $Q$ . Assume that $P_\theta$ has a density $p_\theta$ , with respect to some ground measure $\mu$ and $Q(\cdot|X)$ has a density $q(\cdot | X)$ with respect to the same $\mu$ for every $X$ . Show that the Fisher information of $X$ is at least as large as that of $Y$ : $$\mathbb E\left [ \frac{\int q(Y | X=x) \dot p_\theta(x)dx}{\int q(Y | X=x) p_\theta(x)dx} \right ]^2 \leq \mathbb E\left[\frac{\dot p_\theta(X)}{p_\theta(X)}\right]^2$$ where $\dot p_\theta(x)$ is the first derivative of the density $p_\theta$ with respect to $\theta$ . I'm looking for hints for solving this problem (preferably vague hints that will allow me to still solve the problem myself). What I've tried The inequality makes some sense to me intuitively but I'm having trouble understanding why it's true. I would usually first try to show a pointwise inequality, but that won't work here since the expectations are over different variables. I also tried expressing the expectations as integrals and switching the order of integration, but that didn't give me any new insights. My instinct is to apply Cauchy-Schwarz or Jensen's inequality, but I haven't figured out the right way to apply them here.","['statistics', 'jensen-inequality', 'cauchy-schwarz-inequality', 'probability']"
4168060,"What does ""parity pattern"" mean?","Five lattice points are chosen in the plane lattice. Prove that you can always choose two of these points such that the segment joining these points pass through another lattice point. (The lattice consists of all points of the plane with integral coordinates). The solution reads as follows; Let us consider the parity pattern of the coordinates of these lattice points. There are only four possible patterns: $(e,e) , (e,o) , (o,e) , (o,o)$ . Among the five lattice points, there will be two points, say $a= (a, b)$ and $B = (c,d)$ with the same parity pattern. Consider the midpoint $L$ of $AB$ . $L= ( \frac{a+c}{2} , \frac{b+d}{2} )$ . $a$ and $c$ as well as $b$ and $d$ have the same parity , and so $L$ is a lattice point. I do not fully understand the solution provided.
How did the midpoint idea come from?
How does it work?
Would appreciate any insight provided by anyone to this solution.","['pigeonhole-principle', 'problem-solving', 'geometry']"
4168078,Compact manifold and eigenvalues,"Let $X$ be a compact manifold and $f : X \to X$ a smooth map. Show that $\{x \in X | f(x) =x,\ 1 \text{ is not an eigenvalue of }df_x : T_xX \to T_xX\}$ is a finite set. If $1$ isn't an eigenvalue then $\det(df_x - I)\neq 0$ which is an open set condition. So I guess we can do a ""open cover has finite subcover"" argument.
But I didn't really use $f(x)=x$ condition.","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
4168096,Second fundamental form of an immersion,"I cannot quite replicate the following calculation from Aubin's Some Nonlinear Problems in Riemannian Geometry (page 349). The setting is as follows. Let $(M^n,g)$ and $(\tilde{M}^m,\tilde{g})$ be two $C^\infty$ Riemannian manifolds and let $f:M\rightarrow\tilde{M}$ be a smooth immersion. Let $\nabla$ be the Levi-Civita connection of $(M,g)$ and let $\tilde{\nabla}$ be the Levi-Civita connection of $(\tilde{M},\tilde{g})$ . Now let $\{x^i\}_{i=1}^n$ be local coordinates in a neighbourhood of $P\in M$ and let $\{y^\alpha\}_{\alpha=1}^m$ be local coordinates in a neighbourhood of $f(P)$ in $\tilde{M}$ . Say $f$ is injective on a neighbourhood $\Omega$ of $P$ in $M$ . Let $Y$ be a vector field on $\Omega$ and extend $\tilde{Y}=f_*Y$ to a neighbourhood of $f(P)$ .
For $X$ in $T_x\Omega$ , let $\tilde{X}=f_*X$ . Finally, verify that $\tilde{\nabla}_{\tilde{X}}\tilde{Y}$ is well defined and define the second fundamental form $\alpha_x$ of $f$ at $x\in \Omega$ by $$\alpha_x(X,Y)=\tilde{\nabla}_{\tilde{X}}\tilde{Y}-f_*(\nabla_X Y)$$ The book now claims that in coordinates we have $$\alpha_x(X,Y)=\left[\partial^2_{ij}f^\gamma(x)-\Gamma_{ij}^k\partial_k f^\gamma+\tilde{\Gamma}_{\alpha\beta}^\gamma(f(x))\partial_if^\alpha(x)\partial_jf^\beta(x)\right]X^iY^j\frac{\partial}{\partial y^\gamma}$$ where $\nabla_{\frac{\partial}{\partial x^i}}\frac{\partial}{\partial x^j}=\Gamma_{ij}^k\frac{\partial}{\partial x^k}$ and $\tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta}=\tilde{\Gamma}_{\alpha\beta}^\gamma\frac{\partial}{\partial y^\gamma}$ . I can easily calculate the coordinate expression of $f_*(\nabla_X Y)$ . Indeed I get $$f_*(\nabla_X Y)=(X^i\partial_i Y^j\partial_jf^\gamma+X^iY^j\Gamma_{ij}^k\partial_kf^\gamma)\frac{\partial}{\partial y^\gamma}$$ However, when I try to calculate $\tilde{\nabla}_{\tilde{X}}\tilde{Y}$ I run into trouble, since at some point I have to calculate $$\tilde{\nabla}_{\partial_if^\alpha\frac{\partial}{\partial y^\alpha}}\left(Y^j\partial_j f^\beta\frac{\partial}{\partial y^\beta}\right)=\color{red}{\left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)\frac{\partial}{\partial y^\beta}}+Y^j\partial_if^\alpha\partial_j f^\beta\tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta}$$ and the red summand seems syntactically wrong. Formally, $$\left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)=\frac{\partial}{\partial x^i}\left(Y^j\partial_j f^\beta\right)=\partial_iY^j\partial_j f^\beta+Y^j\partial_{ij}^2f^\beta$$ which gives the correct answer, but I do not quite understand what is going on in this formal calculation. What am I missing? Moreover, what does the second partial $\partial_{ij}^2f^\beta$ of a map between manifolds mean exactly?","['connections', 'submanifold', 'riemannian-geometry', 'differential-geometry']"
4168131,Number of cards to be pulled,"There is a game with $10$ cards(cards are number $1$ to $10$ ). They are distributed to $2$ players randomly. Player who has the highest sum of card values wins the game. Now, before the cards are dealt, player A will say a number. This number is the number of cards he will RANDOMLY pull from player B, and then return the cards of his choice to player B.(The cards returned cannot be the cards he has taken from player B). So, what is the number of cards player A should pull from player B to maximize his chances of winning the game? If Player A says $0$ , his winning probability remains at $0.5$ If Player A says $5$ , he takes all of B's card, and give all of his cards to B. His winning probability remains at $0.5$ again I assume that there is a unimodal winning probability with the peak at the centre. However, I am facing difficulty in which one of these $2$ will be larger and by how much? Part $2$ - Can this be generalised to ' $n$ ' cards?","['permutations', 'combinatorics', 'card-games', 'probability']"
4168160,"Probability that the sum of two integers in $\{1,\dots,n\}$ equals a perfect square","I found the following question in MIT OCW's Mathematical Problem Solving and I'd like to know if my solution is ok: ""Let $p_n$ be the probability that $c+d$ is a perfect square when the integers $c$ and $d$ are selected independently at random from the set $\{1,\dots,n\}$ . Show that $\lim_{n\to \infty} p_n\sqrt{n}$ exists, and express this limit in the form $r(\sqrt{s}-t)$ where $s$ and $t$ are integers and $r$ is a rational number."" To me, it looks like that $$p_n= \sum_{k=2}^{\lfloor\sqrt{n}\rfloor} \sum_{i=1}^{k^2-1} P\left(c=i,d=k^2-i\right)=\sum_{k=2}^{\lfloor\sqrt{n}\rfloor}\sum_{i=1}^{k^2-1} \frac{1}{n^2}=\frac{\lfloor\sqrt{n}\rfloor(2\lfloor\sqrt{n}\rfloor^2+3\lfloor\sqrt{n}\rfloor-5)}{6n^2}$$ for $n\ge 4$ and hence $\lim p_n\sqrt{n}=\frac{1}{3}$ using $\lfloor\sqrt{n}\rfloor\le \sqrt{n}<\lfloor\sqrt{n}\rfloor+1$ and sandwich theorem. However, the form the problem tells to write the answer makes me think I may have made some mistake. Can someone say whether my solution is correct or not and in negative case, where I missed? Thanks in advance! EDIT: I realized that the question does not say that the perfect square must be in $\{1,\dots,n\}$ . So I must add to the above a sum with $\lfloor \sqrt{n}\rfloor<k\le \lfloor\sqrt{2n}\rfloor$ , being careful to the restrictions that $1\le c,d\le n$ . Hence: $$p_n=\sum_{k=2}^{\lfloor\sqrt{n}\rfloor}\sum_{i=1}^{k^2-1} \frac{1}{n^2}+\sum_{k=\lfloor\sqrt{n}\rfloor+1}^{\lfloor\sqrt{2n-1}\rfloor} \sum_{i=k^2-n}^{n-1} \frac{1}{n^2}=$$ $$=\frac{m_n(2m_n^2+3m_n-2)}{3n^2}-\frac{q_n(q_n+1)(2q_n+1)}{6n^2}+\frac{2q_n}{n}-\frac{2m_n}{n}$$ where $m_n=\lfloor \sqrt{n}\rfloor$ and $q_n=\lfloor \sqrt{2n-1}\rfloor$ . We can compute $$\lim_n \frac{m_n(2m_n^2+3m_n-2)}{3n^{3/2}}=\frac{2}{3}$$ $$\lim_n \frac{q_n(q_n+1)(2q_n+1)}{6n^{3/2}}=\frac{2\sqrt{2}}{3}$$ $$\lim_n \frac{2q_n}{\sqrt{n}}=2\sqrt{2}$$ $$\lim_n \frac{2m_n}{\sqrt{n}}=2$$ Therefore $$\lim_{n\to \infty}\ p_n\sqrt{n}=\frac{4}{3}(\sqrt{2}-1)\approx 0.552284749831$$","['recreational-mathematics', 'combinatorics', 'problem-solving', 'probability']"
4168165,Radon-Nikodym for product measure,"Let $(E, \mathcal{E})$ be a measurable space , $\eta:\mathcal{E}\to [0,1]$ be a probability measure defined on it and $K:E\times\mathcal{E}\to [0, 1]$ a Markov Kernel from $(E, \mathcal{E})$ onto itself. Let $\mathcal{E}\otimes \mathcal{E}$ be the product sigma algebra of the cartesian product $E\times E$ . Then define the product measure as $$
(\eta \times K)(A\times B) = \eta(A) K(x, B) \qquad \qquad \forall\, A, B\in \mathcal{E} \quad \text{and} \quad x\in E
$$ This is a measure $\eta \times K: \mathcal{E}\otimes\mathcal{E} \to [0, 1]$ defined on $\mathcal{E}\otimes\mathcal{E}$ so now ( assuming any regularity condition I need to assume ) I would like to find the Radon-Nikodym derivative of $\eta \times K$ . Suppose $\eta \ll \lambda$ and $K(x, \cdot) \ll \lambda$ have densities $$
\frac{d \eta}{d\lambda}(x) = p_\eta(x) \qquad \frac{d K(x, \cdot)}{d \lambda}(y) = k(y\mid x)
$$ Can I say that the density of the product measure is now this? $$
\frac{d \eta \times K}{d \lambda \times \lambda}(x, y) = \frac{d \eta}{d \lambda}(x) \cdot \frac{d K(x, \cdot)}{d \lambda}(y) = p_\eta(x) k(y \mid x)
$$","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'probability-theory', 'radon-nikodym']"
4168195,Nth order central difference for odd $n$,"I am interested in the $n^{\mathrm{th}}$ -order central difference of an expression $f$ . The general form of the $n^{\mathrm{th}}$ -order central difference is given by $$\delta_h^n[f](x)=\sum_{i=0}^n(-1)^i {n \choose i}f(x+(\frac{n}{2}-i)h).$$ For odd $n$ , the central difference will have $h$ multiplied by non-integers, which can often be problematic. According to this Wiki article , the problem can be circumvented by taking the average of $\delta^n[f](x-\frac{h}{2})$ and $\delta^n[f](x+\frac{h}{2})$ . What does that actually mean? Does this mean that I adjust the generalized formula from $f(x+(\frac{n}{2}-i)h)$ (only this part?) to e.g., $\delta^n[f](x-\frac{h}{2})$ ? And if so, $i$ is just a bookkeeping device. For example, for the third order difference, does it mean that I use the generalized formula for $i = \{0,2\}$ and the adjusted formula for $i = \{1,3\}$ ? I know, of course, that it is also possible to use the explicit formula for, e.g., the third derivative. However, I am interested in the application of the generalized expression.","['numerical-methods', 'finite-differences', 'derivatives', 'discrete-mathematics']"
4168198,Complement of codim 2 manifold is connected?,"If M is a connected n-manifold and N is a codim 2 submanifold
then M-N is connected. Is this true? I want to show $H_0(M-N)=\mathbb Z$ . I think it's better to do in $\mathbb Z_2$ coefficient because we get orientability. But because no compactness is mentioned I can't use Poincare duality. How can I resolve this?","['manifolds', 'general-topology', 'homology-cohomology']"
4168282,Calculating the Derivative of a Norm,"Let $\alpha$ be a positive real number (i.e. $\alpha > 0)$ , and consider $r\left(\alpha\right) := \| Ax_{\alpha}^{\delta} - y^{\delta} \|$ , where $A: X\rightarrow Y$ is a linear operator between finite-dimensional vector spaces. Now, also define $\Psi(\alpha) := \vert\vert x_{\alpha}^{\delta} \vert\vert$ . In a lecture, we said that $$\frac{\mathrm dr}{\mathrm d\Psi}$$ would be negative for all $\alpha$ , but I don't see this yet. I think the norm which we considered was the usual Euclidean norm, so we have $$\frac{\mathrm dr}{\mathrm d\Psi} = \frac{\mathrm d \sqrt{\|Ax_{\alpha}^{\delta}\|^2 - 2A x_{\alpha}^{\delta}y^{\delta} + \| y^{\delta} \|^2}}{\mathrm d\| x_{\alpha}^{\delta}\|}.$$ I am not quite sure how to differentiate $\|Ax_{\alpha}^{\delta}\|^2$ and $- 2A x_{\alpha}^{\delta}y^{\delta}$ with respect to $\| x_{\alpha}^{\delta} \|$ . In case is helps: We know that $\Psi(\alpha)$ is monotone decreasing in $\alpha$ . (I know I didn't write what $x_{\alpha}^{\delta}$ and $y^{\delta}$ are, but I believe that this is rather irrelevant for my question.)","['derivatives', 'analysis']"
4168291,perpendicular projections in negatively curved manifolds,"Consider the hyperbolic plane $\mathbb{H}$ of dimension $2$ (think of it as the half-plane model). Let $c$ be a geodesic on $\mathbb{H}$ and let $z_1, z_2$ points such that $r=d(z_1,c)=d(z_2,c)$ and let $c(s_1), c(s_2)$ ( $s_1< s_2$ ) be the corresponding foot-point projection on $c$ . Also assume that $z_1,z_2$ lies on the same side of the geodesic. Using the formula for the distance between two points and some hyperbolic trigonometry, it is possible to show \begin{equation}
\cosh(d(z_1,z_2)/2)= \sinh(r)\cosh((s_2-s_1)/2).
\end{equation} Let now $M$ be a complete, simply connected manifold of dimension $2$ with sectional curvature $K\leq -1$ . Consider $\gamma$ a geodesic in $M$ , $p_1,p_2\in M$ such that $d(p_i,\gamma)=r$ for $i=1,2$ and assume they lie on the same side of the $\gamma$ . Let $\gamma(s_1), \gamma(s_2)$ be the corresponding foot-point projection. I think that the following inequality is true: \begin{equation}
\cosh(d_M(p_1,p_2)/2)\geq \sinh(r)\cosh((s_2-s_1)/2).
\end{equation} However, I cannot find any reference for it and I am not so sure about how to prove it. My idea would be to consider appropriate points $p\in M$ , $z\in\mathbb{H}$ and an isometry $i:T_p M\rightarrow T_z \mathbb{H}$ together with the map $\varphi = \exp_z \circ i \circ \exp_p^{-1}$ ( $\exp_*$ is the exponential map) and claim that we can choose $i$ so that $d_M(\gamma(s_1), \gamma(s_2))= d_{\mathbb{H}}(\varphi(\gamma(s_1)), \varphi(\gamma(s_2)))$ , $d_M(\gamma(s_i), p_i)=d_{\mathbb{H}}(\varphi(\gamma(s_i)), \varphi(p_i))$ for $i=1,2$ and apply the corollary of Rauch's comparison theorem about lengths of curves (see for example Proposition 2.5 in Do Carmo's ""Riemannian geometry"") to claim $d_M(p_2,p_1)\geq d_{\mathbb{H}}(\varphi(p_2), \varphi(p_1))$ and then use the result for the hyperbolic space. However, I think that this claim for $i$ may not be true. Any suggestions? Thanks for the help!","['projection', 'hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
4168318,Does this morphism extend to projective varieties?,"We work over the complex numbers. Let $\mathbb{A}^1\times \mathbb{A}^1\to \mathbb{A}^1$ be the morphism given by $(x,y)\mapsto x-y$ . Does this extend to a morphism $\mathbb{P}^1\times \mathbb{P}^1\to \mathbb{P}^1$ ? I have the feeling it does not because it contracts the diagonal (which is ample on $\mathbb{P}^1\times \mathbb{P}^1$ ), but I can't think of an ""easier"" argument.",['algebraic-geometry']
4168370,Finding a distribution with Bayesian statistics,"Suppose $X$ is a random variable that follows a binomial distribution with parameters $n = 5$ and $p$ where $0 \leq p \leq 1$ is unknown. Through a Bayesian approach, $p$ is modeled with a random variable $P$ . Some researchers have established that it is very likely that $p$ is close to $1$ and not likely that $p$ is close to $0$ . (i) Which of the following densities is the best choice of prior for $P$ : $f(p) = 1, 0 \leq p \leq 1$ , $g(p) = 2p, 0 \leq p \leq 1$ , or $h(p) = e^{-p}, p \geq 0$ . Justify. (ii) Let $f_P(p)$ be the prior you chose in (i) and you observe that $X = 5$ . Given this observation, find the posterior density $P$ . (iii) Between the prior and posterior densities of $P$ in (ii), which has more density near $1$ ? Explain why the density you identify should have more density near $1$ ? My attempt: (i) I am not quite sure which prior is the best. I eliminated the third one $h(p)$ due to the support being too large. Then, the first one $f(p)$ seems to assume that it is always $1$ , which doesn't seem consistent. The third one also seems most likely as it only multiplies the values by $2$ which corresponds to the fact that the researchers already established that $p$ is close to $1$ . I also recall that usually, the prior and posterior of a binomial tend to be beta. The general beta distribution support is $0$ to $1$ which eliminates $h(p)$ . (ii) The chosen prior is $f_P(p) = 2p, 0 \leq p \leq 1$ The pdf of $X$ is: $$f(x\mid p) = \frac{5!}{x!(5-x)!} p^x (1-p)^{5-x},$$ $x = 0, 1, 2, 3, 4, 5$ . The posterior is then: $$f(p\mid x) = \frac{f(x\mid p)\cdot f_P(p)}{f_X(x)} = \frac{f(x\mid p)\cdot f_P(p)}{f_X(x)} = \frac{f(x\mid p)\cdot f_P(p)}{\int_{-\infty}^\infty f(x\mid p) \cdot f_P(p) \,dp}$$ We apply proportionality: $$\frac{f(x\mid p)\cdot f_P(p)}{\int_{-\infty}^\infty f(x\mid p) \cdot f_P(p) \,dp} \propto f(x\mid p)\cdot f_P(p) = f(x\mid p) = \frac{5!}{x!(5-x)!} p^x (1-p)^{5-x} \cdot 2p = \frac{2\cdot 5!}{x!(5-x)!} p^{x + 1} (1-p)^{5-x}$$ As I stated before, the posterior of a binomial is often beta, and both the appearance of the pdf and support seem to agree with this. So I conclude that the posterior distribution is Beta with parameters $\alpha$ as $x + 2$ and $\beta$ as $6 - x$ . The posterior pdf: $f(p\mid x) = \frac{\Gamma (8)}{\Gamma (x + 2) \Gamma (6 - x)} p^{x + 1} (1 - p)^{5 - x}$ , $0 < p < 1$ . Letting $X = 5$ , $f(p\mid 5) = \frac{\Gamma (8)}{\Gamma (7) \Gamma (1)} p^{6} (1 - p)^{0} = \frac{\Gamma (8)}{\Gamma (7) \Gamma (1)} p^{6}$ (iii) Not sure about this one. Any assistance is much appreciated. Also, if anyone can provide a better explanation for (i), that would be great.","['statistics', 'probability-distributions', 'parameter-estimation', 'bayesian', 'solution-verification']"
4168430,A real valued function defined on a Borel subset of $\mathbb{R}$ with a countable number of discontinuities is Borel measurable,"I have proved the following statement and I would like to know if my proof is correct and/or/if/how it can be improved, thanks. ""Suppose $X$ is a Borel subset of $\mathbb{R}$ and $f:X\to\mathbb{R}$ is a function such that $\{x\in X: f\text{ is not continuous at }x\}$ is a countable set. Prove $f$ is a Borel measurable function."" My proof: ( EDIT: my proof is wrong in the case $\{x\in X: f\text{ is not continuous at }x\}$ is a countable set (a counterexample, as pointed out by Ramiro is the Thomae function) but it should work in the case $\{x\in X: f\text{ is not continuous at }x\}$ is finite ) Let $d_i, i\geq 1$ denote the points at which $f$ is discontinuous and fix $a\in\mathbb{R}$ ; if $x\in X$ and $f(x)>a$ then either $x=d_i$ for some $i\geq 1$ or $f$ is continuous at $x$ so if we set $\delta_x:=\frac{1}{2}\cdot\inf\{|x-d_i|:i\geq 1\}$ we have $f((x-\delta_x,x+\delta_x)\cap X)\subseteq (a,+\infty)$ (equivalently $(x-\delta_x,x+\delta_x)\cap X\subseteq f^{-1}((a,+\infty))$ being $f$ continuous in this whole set so $f^{-1}((a,+\infty))=\bigcup_{x\in f^{1}((a,+\infty)), x\neq d_i\forall i\geq 1} (x-\delta_x,x+\delta_x)\cap X\cup \{d_i:f(d_i)>a\}$ which being the union of two Borel sets ( $(x-\delta_x,x+\delta_x)$ is an open subset of $\mathbb{R}$ so it is Borel, $X$ is a Borel set by hypothesis so their intersection is Borel too and so is also their union and $\{d_i:f(d_i)>a\}$ is countable hence Borel too) is Borel. So, by LEMMA, we can conclude that $f$ is Borel-measurable, as desired. LEMMA. Suppose $(X,\mathcal{S})$ is a measurable space and $f:X\to\mathbb{R}$ is a function such that $f^{-1}((a,+\infty))\in\mathcal{S}$ for all $\in\mathbb{R}$ then $f$ is an $\mathcal{S}$ -measurable function. DEF. (measurable function) Suppose $(X,\mathcal{S})$ is a measurable space. A function $f:X\to\mathbb{R}$ is called $\mathcal{S}$ -measurable if $f^{-1}(B)\in\mathcal{S}$ for every Borel set $B\subset\mathbb{R}$ . DEF. (Borel-measurable function) Suppose $X\subset\mathbb{R}$ . A function $f:X\to\mathbb{R}$ is called Borel measurable if $f^{-1}(B)$ is a Borel set for every Borel set $B\subset\mathbb{R}$ .","['measure-theory', 'real-analysis', 'solution-verification', 'measurable-functions', 'borel-sets']"
4168439,MLE of two variables,"Suppose $X$ and $Y$ are random variables and let $x_1, ..., x_n$ be observed values from a random sample of $X$ . Assume that $Y_i = 2\alpha x_i + \alpha + \beta_i$ where $\alpha$ is unknown and $\beta_1, ..., \beta_n$ are iid. $N(0, \sigma^2)$ with $\sigma^2$ being unknown (Equivalently assume that the conditional expectation of $Y$ depends linearly on $X$ and that the slope of the line is double the y-intercept). (i) Determine the maximum likelihood estimators for $\alpha$ and $\sigma^2$ . (ii) You take $3$ samples and observe $(x_1, y_1) = (1, 4)$ , $(x_2, y_2) = (0, 1)$ , and $(x_3, y_3) = (3, 6)$ . Find the point estimate for $\alpha$ using the MLE you found in (i). (iii) What are the residuals of this model and what do they measure? Hint: The MLE of $\sigma^2$ is the average of the squares of the residuals. My attempt: (i) I used this likelihood function $$L(\alpha, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} exp(\sum_{i=1}^{n}\frac{-(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2})$$ The negative of the natural log of this is $$-ln(L(\alpha, \sigma^2)) = \frac{n}{2}\ln(2\pi \sigma^2) + \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2}$$ We let the latter half of the RHS be another function $G$ so that: $$G(\alpha) = \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2}$$ Minimizing this w.r.t. $\alpha$ gives $$G' = \sum_{i=1}^{n} 2(y_i - 2 \alpha x_i - \alpha)(-2x_i - 1) = 0$$ And so the MLE for $\alpha$ is $$\hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2}$$ We check the second partial derivative: $$G'' = \sum_{i=1}^{n} 8x_i^2 + 8x_i + 2$$ This is greater than $0$ so it is a minimum. Now, minimizing $-\ln(L)$ w.r.t. $\sigma^2$ gives $$-ln(L(\alpha, \sigma^2))' = \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 = 0$$ And so the MLE for $\sigma^2$ is $$\hat{\sigma}^2 =  \frac{1}{n}\sum_{i = 1}^{n} (y_i - 2\alpha x_i - \alpha)^2$$ We check the second partial: $$-ln(L(\alpha, \sigma^2))'' = \frac{-n}{2\sigma^4} + \frac{1}{\sigma^6}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2$$ This is greater than $0$ $\iff$ $\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 > n \sigma^2$ However, I am not sure how to show that this is true. (ii) The MLE from (i) was $$\hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2}$$ So $$\alpha = \frac{(4 \cdot 4 \cdot 1 + 2\cdot 4) + (4\cdot 1\cdot 0 + 2\cdot 1) + (4\cdot 6\cdot 3 + 2\cdot 6)}{(8 \cdot 1^2 + 8\cdot 1 + 2) + (8 \cdot 0^2 + 8 \cdot 0 + 2) + (8 \cdot 3^2 + 8\cdot 3 + 2) } = \frac{110}{118}$$ (iii) Going by the hint, the residual is $Y_i - 2\alpha x_i - \alpha$ . It is the difference between the actual and observed data, and is used to determine whether a line or curve is appropriate for the data. Any assistance especially with the last part of (i) is appreciated. Was my explanation about residuals correct?","['statistics', 'regression', 'parameter-estimation', 'solution-verification', 'maximum-likelihood']"
4168446,Removable singularities for a formal Stieltjes transform,"Hypothesis: Let $p$ be a non-constant $\mathbb{C}$ -polynomial, and let $$s_{p}(z):=\sum_{\zeta : p(\zeta)=z}{\frac{f(\zeta)}{p'(\zeta)}}\qquad(z\in U)$$ be its formal Stieltjes transform, where $U\subset\mathbb{C}$ is a non-empty open subset such that for any $z\in U$ , $f$ is analytic around $\zeta\in\mathbb{C}$ satisfying $p(\zeta)=z$ . To prove: If the set $\Omega:=\{z\in U:\exists\zeta\in\mathbb{C},p(\zeta)-z=0=p'(\zeta)\}$ is finite, then the elements of $\Omega$ are removable singularities of $s_{p}$ . My attempt: Let $z_{o}\in\Omega$ be arbitrary and let $\zeta_{o}\in\mathbb{C}$ be such that $p(\zeta_{o})-z_{o}=0=p'(\zeta_{o})$ . Since $\Omega$ is finite, then we can find a path $\gamma\subset\mathbb{C}$ which encloses $\zeta_{o}$ , and so that $f/p'$ is analytic on it and on its interior (except at $\zeta_{o}$ , of course). Therefore, for $\zeta$ close enough to $\zeta_{o}$ , $$(\zeta-\zeta_{o})\frac{f(\zeta)}{p'(\zeta)}=\frac{(\zeta-\zeta_{o})^{2-n_{o}}}{2\pi\text{i}q(\zeta)}\oint_{\gamma}{\frac{f(x)}{x-\zeta}\mathrm{d}x}$$ by the analytic nature of $f$ , where $n_{o}$ is the multiplicity of $\zeta_{o}$ (w.r.t $p$ ) and $q$ is a non-vanishing polynomial on a small enough neighborhood of $\zeta_{o}$ enclosed by $\gamma$ . My idea is to use Riemann's theorem to conclude, but it fails for $n_{o}\ge 2$ . Edit: The author just suggested me: ""Try using contour integrals of $\frac{f(\zeta)}{p'(\zeta)}$ instead."" But once again, I failed to conclude. I obtained, for instance, that the coefficients $b_{n}$ ( $n\ge 1$ ) of the principal part of the Laurent expansion of $\frac{f(\zeta)}{p'(\zeta)}$ are given by $$\oint_{\gamma}{\frac{f(x)}{q(x)}(x-\zeta_{o})^{n-n_{o}}\frac{\mathrm{d}x}{2\pi\text{i}}}.$$ Thus, by the analytic nature of $\frac{f(x)}{q(x)}$ at $\zeta_{o}$ , for all $n\ge n_{o}$ , $b_{n}=0$ . By the way, this is the final part of the proof of Prop. 2.5.7 on Topics in Random Matrix Theory by Terence Tao ( https://terrytao.files.wordpress.com/2011/08/matrix-book.pdf ). New attempt: I would like to know if the following is correct: $$\frac{f(\zeta_{o})}{p'(\zeta_{o})}:=\oint_{\gamma}{\frac{f(x)/p'(x)}{x-\zeta_{o}}\frac{\mathrm{d}x}{2\pi\text{i}}}=\oint_{\gamma}{\frac{f(x)/q(x)}{(x-\zeta_{o})^{n_{o}}}\frac{\mathrm{d}x}{2\pi\text{i}}}=\frac{1}{(n_{o}-1)!}\frac{\mathrm{d}^{(n_{o}-1)}}{\mathrm{d}x^{(n_{o}-1)}}\Bigg[\frac{f(x)}{q(x)}\Bigg]_{x=\zeta_{o}}$$ is well-defined since $q$ does not vanish on a small enough neighborhood of $\zeta_{o}$ . Is this enough to claim that $\zeta_{o}$ is a removable singularity?. Sorry in advance if this is a silly question.","['laurent-series', 'stochastic-analysis', 'complex-analysis', 'taylor-expansion', 'probability-theory']"
4168449,Colossally abundant numbers and the Riemann hypothesis,"[This question has lead me to ask a follow up on MathOverflow .] A recent tweet by John Baez has reminded me of the astonishing fact $^1$ that the Riemann hypothesis (RH) can be disproved by finding a number $n > 5040$ such that $$\frac{\sigma(n)}{n \ln\ln n} > e^\gamma \;,$$ where $\gamma$ is Euler's constant.
We can take the logarithms and introduce a function $$s(n) = \ln \frac{\sigma(n)}{n \ln\ln n}$$ such that the bound becomes $s(n) > \gamma$ .
This is convenient, because if we know the prime factorisation $n = \prod_{q} q^{k_q}$ of a candidate number, we can rather easily compute $$s(n) = \sum_{q} \ln \frac{q^{k_q+1} - 1}{q - 1} - \sum_{q} k_q \ln q - \ln\ln \sum_{q} k_q \ln q \;,$$ without too much numerical trouble. $^2$ Now, it has been shown that if the RH is false, the inequality will fail to hold for a colossally abundant number .
These were introduced by Alaoglu & Erdős (1944) and are defined such that $n$ is colossally abundant if and only if there is $\epsilon > 0$ such that for all $k > 1$ $$\frac{\sigma(n)}{n^{1+\epsilon}} \ge \frac{\sigma(k)}{k^{1+\epsilon}} \;.$$ Importantly, their Theorem 10 gives a simple enumeration of the prime exponents of the colossally abundant number associated with a particular $\epsilon > 0$ , $$k_q(\epsilon) = \left\lfloor \frac{\ln \frac{q^{1+\epsilon}-1}{q^{\epsilon}-1}}{\ln q}\right\rfloor - 1 \;.$$ Since all we need to compute the function $s(n)$ are a large list of primes $q$ and their exponents $k_q$ , we can pick any $\epsilon > 0$ , for example at random, compute the complete list of nonzero $k_q(\epsilon)$ , and obtain $s(n)$ rather straightforwardly.
The result of a random draw as function of $\epsilon$ is shown here: It seems to me as if the values $s(n)$ approach the bound in an almost perfectly exponential manner. The curve is noisy, but as far as I can tell only due to the floor function, since everything else appears perfectly smooth. In what way could specific values of $\epsilon > 0$ make the noise cross the bound to potentially disprove the RH? More to the point, I don't understand how the structure of the primes comes into play here.
The expression in the floor function in $k_q(\epsilon)$ smoothly approaches unity for fixed $\epsilon$ and increasing $q$ , so for any $\epsilon$ there is a smallest prime $r$ such that that $k_q = 0$ for all $q \ge r$ .
In which way can the primes conspire such that a small change $\delta\epsilon$ , not big enough to change $r$ , changes enough $k_q$ for $q < r$ to make a significant change to $s(n)$ ? Even more interestingly, we can ostensibly make the following observation: For $\epsilon < 0.01$ , the function $s(n)$ is monotonically increasing as $\epsilon$ approaches zero. If this were true, it would mean that the RH can be disproved with the limit $\epsilon \to 0$ of $s(n)$ alone (which is of course a very hard problem). This seems a very analytical problem however, and makes me wonder again how and where the structure of the primes matters in this. And just to make it perfectly clear: The question is not ""does this disprove the RH"". The question is ""how could this, in principle, disprove the RH"". $^1$ Astonishing, because I can understand the problem even though I know nothing about the RH. $^2$ If someone is interested in trying, note there is trouble in double precision once $\epsilon$ get to $10^{-9}$ . You can rewrite the function $s(n)$ to treat the many, many, many $k_q = 1$ for the largest primes separately, $$s(n) = \sum_{q \colon k_q>1} \ln \frac{q^{k_q+1} - 1}{q^{k_q} \, (q - 1)} + \sum_{q \colon k_q=1} \ln (1 + 1/q) - \ln\ln \sum_{q} k_q \ln q \;.$$ The often available log1p function further helps with $\log(1 + 1/q) = \operatorname{log1p}(1/q)$ .","['number-theory', 'prime-numbers', 'riemann-hypothesis']"
4168451,"calculating the coefficient of skewness for $f_Y(y) = e^{-y} ,y>0$","I'm trying to find the coefficient of skewness for the pdf: $f_Y(y) = e^{-y}; y>0$ After having calculated for $\mu$ and $\sigma^2$ I get the values $(1, 1)$ respectively. Then placing this into the equation for skewness: $$\gamma = \frac{E[(X-\mu)^3]}{1} = \sum_{j=0}^{3} \binom3jE(Y^j)(-1^{3-j}).$$ I'm not sure how to proceed from here, and having a look at the solution to this exercise I find that $E(Y^j) = j!$ why is this the case?","['expected-value', 'statistics', 'probability']"
4168489,"Find a family of random variables $X$, with density $f$, such that $X$ and $Y=f(X)$ have the same distribution.","I have to demonstrate the following Probability/Statistics result: ""Find a family of random variables $X$ , having pdf $f$ , such that $X$ and $Y=f(X)$ have the same distribution."" I have tried to find a proof to this result and I found out that, in the case of pdf $f$ strictly increasing, the result is true if $f$ is the identity function. I have to find a characterization even in the following 2 cases: $f$ strictly decreasing; $f$ stictly increseasing for $x\leq m$ and stricly decreasing for $x \geq m$ (where $m$ is the mode of the distribution). Any ideas and/or suggestions?","['statistics', 'probability-distributions', 'probability', 'random-variables']"
4168493,Terminology; Basis for a Topology,"In section 13 of Munkres's topology, he defines a basis $\mathcal{B}$ containing subsets $B$ of a set $X$ . One of the first things he proves is that the resulting collection, $\mathcal{T}$ , is in fact a topology. Here is my confusion. He uses ""is an element of $\tau$ "" and ""is open"" interchangeably. But after we have a topology, we can call its elements open sets, so this seems circular to me. Is the idea that elements generated by a basis are open, and then we check this is actually a topology? Otherwise,  it seems to me that we should be talking about membership in $\mathcal{T}$ and only use the notion of openness after checking that it satisfies the axioms for a topology.","['general-topology', 'terminology']"
4168512,About a nice way to solve $\int e^x \Big(\frac{1-\sin x}{1-\cos x}\Big)dx$,"Conisder the following problems $$\int e^x \Big(\frac{1-\sin x}{1-\cos x}\Big)dx.$$ or $$ \int e^x\Big(\frac{1+x\ln x}{x}\Big) dx$$ I have seen these problems (or simpler to them) in many various places but I do not have a specific reference for them. However, I did not see them in the standard calculus books like Stweart unless I missed it. The idea to solve these problems by using the following $$\int e^x (f(x)+f'(x)) dx=\int \frac{d}{dx} \Big(e^x f(x)\Big)dx.$$ Generally speaking, it depends about simplifying the function that are multiplying by $e^x$ and rewrites it as $f(x)+f'(x).$ For the seek of completeness,  I will answer the first one by using the rule that I mention. Also, I was curious to see if there is another way to solve them by using one of the integral techniques.","['integration', 'indefinite-integrals', 'calculus']"
4168532,$H: \mathbb{R^2 } \rightarrow \mathbb{R^2}$ be a homeomorphism such that $H(\partial B_1)= \partial B_2$. Show that $H(B_2)=B_1$.,"Let $B_1,\ B_2 \subset  \mathbb{R^2}$ be disks, and let $H:  \mathbb{R^2 } \rightarrow  \mathbb{R^2}$ be a homeomorhism such that $H(\partial B_1)= \partial B_2$ . Show that $H(B_2)=B_1$ . My attempt: Since $\partial B_1$ , and $\partial B_2$ are 1-spheres, by Schönflies Theorem there exist homeomorphisms $H_1,H_2: \mathbb{R^2} \rightarrow \mathbb{R^2}$ such that $H_1(S^1)= \partial B_1$ , and $H_2(S^1)= \partial B_2$ . So, $S^1 = H_i^{-1}(\partial B_i)$ for $i=1,2$ . $$H_1^{-1}(\partial B_1)=H_2^{-1}(\partial B_2)$$ $$H_2 \circ H_1^{-1}(\partial B_1) = \partial B_2$$ It is clear that $H_2 \circ H_1^{-1}$ is a homemorphism. Now, I wish to prove the following claim. Claim: $H_2 \circ H_1^{-1}(B_1)=B_2$ . This is all I have done so far. Am I on the right track? I am open to any help. Additionally, even though it seems a bit irrelevant I have another question. If we have a homeomorphism $H: \mathbb{R^n} \rightarrow \mathbb{R^n}$ , and we have two homeomorphic subsets of $\mathbb{R^n}$ , say $B$ and $D^2$ ,(unit disk), can we say directly that $H: B \rightarrow D^2$ is also a homeomorphism?","['geometric-topology', 'general-topology', 'differential-geometry']"
4168548,"If $\displaystyle \bigoplus_{i=1}^{n} \mathbb{Z} \cong \bigoplus_{i=1}^{m} \mathbb{Z}$ as groups, then $n=m.$","If $\displaystyle \bigoplus_{i=1}^{n} \mathbb{Z} \cong \bigoplus_{i=1}^{m} \mathbb{Z}$ as groups, then $n=m.$ Here is my proof: Let $G$ be a group such that $\varphi:G \rightarrow \displaystyle \bigoplus_{i=1}^{n} \mathbb{Z}$ is a isomorphism, consider the subgroup of $2G:=G+G$ then, define $2\varphi:2G \rightarrow \displaystyle \bigoplus_{i=1}^{n} \mathbb{2Z}$ , such that $2\varphi(x):=\varphi (x)+\varphi (x), $ is a isomorphism, then $G/2G \cong \displaystyle \bigoplus_{i=1}^{n} \mathbb{Z_2}$ , thus, $|G/2G|=2^n$ . Finally, if $\displaystyle \bigoplus_{i=1}^{n} \mathbb{Z} \cong G\cong \displaystyle \bigoplus_{i=1}^{m} \mathbb{Z}$ . $2^m=|G/2G|=2^n$ , we conclude that $n=m$ . Is my proof ok?","['direct-product', 'group-theory', 'solution-verification', 'group-isomorphism']"
4168550,Derivative of a $1$-parameter family of Riemannian metrics,"Let $S^n$ be a closed manifold and let $(M^{n+1},g)$ be a complete Riemannian manifold. Consider $\varphi: S \to M$ a fixed immersion and let $\varphi_t : S \to M$ , $t\in(-\varepsilon, \varepsilon)$ , be the following $1$ -parameter family of immersions (for sufficiently small $\varepsilon$ ): $$ \varphi_t(p) = \operatorname{exp}_{\varphi(p)}(t f(p) N(p)), $$ where $f \in C^{\infty}(S)$ and $N$ is a unit normal for $S$ along $\varphi$ . How do I compute $$\left. \frac{\mathrm{d}}{\mathrm{d}t} \right\vert_{t=0} \varphi_t^{\ast}g \quad ?$$ Notice that $\varphi_t^{\ast}g$ is a $1$ -parameter family of symmetric $(0,2)$ -tensors on $S$ . The derivative above resembles the definition of the Lie derivative, but there are no flows of vector fields involved.","['riemannian-geometry', 'tensors', 'lie-derivative', 'smooth-manifolds', 'differential-geometry']"
4168553,A faster way to find the complex locus of $ | z^* + 2i | + | z | = a $,"I need to find the equation of the locus of $ z = x + iy $ which satisfies $$ | z^* + 2i | + | z | = a $$ for some real constant $ a $ , where $ z^* $ is the conjugate, $ x - iy $ . I already solved this problem by simply substituting these forms in and going through the extremely lengthy algebra (which involves expanding to an equation with 14 terms up to quartic degree) and there must be a quicker way. Intuitively I can see that the locus equation is saying that the sum of two distances from $ 0 $ and $ 2i $ is constant, suggesting an ellipse with foci at these points. Is there a way to get to the equation from here? What about only in the special cases $ a = 4 $ and $ a = 2 $ ? For reference, the equation is - verified with this plotter $$ a^2 x^2 + (a^2 - 4)(y - 1)^2 = \frac{1}{4} a^2 (a^2 - 4). $$ Thanks for any help!","['locus', 'algebra-precalculus', 'conic-sections', 'complex-numbers']"
4168567,Infinite series of a product,"Let $a_n = \prod_{i=0}^{n-1} (r+i)$ , with $0<r<1$ . I  want to find the infinite series $$ 
\sum_{n=0}^{\infty} n\,\frac{a_n}{n!}.
$$ I tried to look at the bounds $ r^n < a_n < (r+n-1)^n$ , but the upper bound is not a good one, because $\sum_{0}^{\infty} n\,\frac{(r+n-1)^n}{n!}$ diverges. The ratio test also doesn't work because $$\lim_{n\rightarrow \infty} \frac{a_{n+1}/(n+1)!}{a_n/n!}=1.$$ Is the above series convergent? If so, what is the value of that?","['infinite-product', 'convergence-divergence', 'sequences-and-series']"
4168572,Integral over domain of infinite tetration of x over extended domain from 0 to $\sqrt[e]e$. Possible $\int_{e^{-e}}^{e^\frac1e} x^{x^{…}}dx$ solution.,"I have been trying to find an interesting constant over the domain of the infinite tetration of x and have just almost figured out the area with a non integral infinite sum representation. Just one constant is in my way. D denotes the domain . This question is different from this one as it has the full domain such that the imaginary part is $0$ and not for a single point. Here is a demo of my expansion. Here is my source for the product logarithm/W-Lambert function series. My inspiration for the series is here . Finally here is a graph of the constant. My work is as follows. I used a bit of software to help with the evaluation at the end. Here is data about the generalized incomplete gamma function used here. $$
\mathrm{G=\int_D x^{x^{x^…}}dx=\int_D {^\infty x}\, dx=\int_D-\frac{W(-ln(x))}{ln(x)}dx= 1.265188689361227081430914184615901039501069191363542653701819999950085943915822836313002058708863484…\implies G+\int_0^{\sqrt[-e]e }\frac {W(-ln(x))}{ln(x)}dx= \int_{\sqrt[-e]e}^{\sqrt[e]e}\frac {W(-ln(x))}{ln(x)}dx= \sum_{n=1}^\infty\frac{n^{n-1}}{n!} \int_{\sqrt[-e]e}^{\sqrt[e]e} ln^{n-1}(x)dx= \sum_{n=1}^\infty\frac{(-n)^{n-1}}{n!}Γ\left(n,-\frac 1e,\frac 1e\right)=\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)= 0.886369135921835965080748…=G-0.378819553439391116350165…}
$$ I have also found the amazing result of being able to integrate the $\mathrm{x^{th}}$ root of x using a theorem on the integral of an inverse function . Here is my work. $$\mathrm{\int_{eW\left(\frac 1e\right)}^e \left(x^\frac 1x=\sqrt[x]x\right)dx+\int_{e^{-\frac 1e}}^{e^\frac 1e} {^\infty x}\,dx=e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)=e^{1-\frac1e}\left(e^\frac2e-W\left(\frac 1e\right)\right)=e^{1+\frac1e}\left(1-e^{-\frac2e}W\left(\frac 1e\right)\right)\implies \int_{eW\left(\frac 1e\right)}^e x^\frac 1xdx= e^{1+\frac1e}-e^{1-\frac1e} W\left(\frac 1e\right)-\sum_{n=1}^\infty\frac{(-n)^{n-1}}{n}Q\left(n,-\frac 1e, \frac 1e\right)}$$ In order to find an exact form of G, I need to find the following. The other form uses the following identity here : $$\mathrm{I= \int_0^ {e^{-\frac 1e}}  x^{x^{x^…}}dx=\int_0^{e^{-\frac 1e}} {^\infty x}\, dx=\int_0^ {e^{-\frac 1e}} -\frac{W(-ln(x))}{ln(x)}dx=e^{1-\frac1e}W\left(\frac1e\right)-\int_0^ {eW\left(\frac1e\right)} \sqrt[x]x dx=0.378819553439391116350165…}$$ The previous result is proof of the following. I guess this link here is not accurate anymore. I will give an example if wanted of this result. Using another Wikipedia theorem proves that: $$\mathrm{\int x^{\frac1x}dx=x^{\frac1x+1}+\sum_{n=1}^\infty (-1)^nn^{n-2} Q\left(n,-\frac{ln(x)}{x}\right)+C,eW\left(\frac1e\right)\le x\le e}$$ more info on this result I found this series based on this answer from @mathphile which does not give the right result as the n=0 term diverges and even trying to use the lower integration bound as $e^{-e}$ still gives the wrong result. The user’s answer would have cracked the question. How do I evaluate this integral? A closed form is wanted, but optional. Please give me any hints as the already used series expansion for this other integral is not in the interval of convergence. This is the main constant that I need to find. Also, please correct me and give me feedback! Note: These Taylor series expansions were found: Taylor Series Unfortunately, it seems like there may be uneven radiuses of convergence which may make us need to have multiple series. Also see nth derivative of xth root of x at x=1 OEIS which has an actual formula. We are so close to the answer.","['definite-integrals', 'real-analysis', 'power-towers', 'constants', 'tetration']"
4168612,"In the diagram, given that $ADGB$ is an isosceles trapezoid, $CE = CD, BC = BD,$ $\angle AOD = 30^{\circ} $, prove that $BD = DF$.","In the diagram, given that $ADGB$ is an isosceles trapezoid, $CE = CD, BC = BD,$ $\angle AOD = 30^{\circ} $ , please prove that $BD = DF$ . My idea: Or to prove $BD = DF$ , only proving that $\angle DBO = 45^{\circ}$ would suffice.","['euclidean-geometry', 'angle', 'geometry', 'plane-geometry']"
4168617,How do I find such a limit: $\lim_{x \to 0}\frac{\int_0^{\sin x} \sqrt{\tan t} \ dt}{\int_0^{\tan x} \sqrt{\sin t} \ dt}$?,"How do I find the limit $$
\lim_{x \to 0}\frac{\int_0^{\sin x} \sqrt{\tan t} \,\mathrm dt}{\int_0^{\tan x} \sqrt{\sin t} \,\mathrm dt}?
$$ The functions in numerator and denominator are a mirror image to each other. That's why I believe it is possible to merge them together under the limit. However, I know that I can go with the limit ""under the integral"" and I'm not sure how to merge those 2 integrals. Any help would be much appreciated.","['integration', 'analysis', 'real-analysis', 'limits', 'trigonometry']"
4168624,Adding together numerators and denominators --- does this protocol generate all positive rational numbers?,"I was playing around with integer coordinates on a square grid and came up with the following protocol: Start with the sequence $\{\frac{0}{1},\frac{1}{0}\} = \{0,\infty\}$ .  Add the numerators and denominators together, and insert the result: $$\left\{\frac{0}{1},\frac{1}{0}\right\} \rightarrow \left\{\frac{0}{1},\frac{0+1}{1+0},\frac{1}{0}\right\}= \{0,1,\infty\}$$ Repeat this process: $$\{0,...,\frac{a}{b},\frac{c}{d},...,\infty\} \rightarrow \{0,...,\frac{a}{b},\frac{a+c}{b+d},\frac{c}{d},...,\infty\}$$ The first few iterations of this process yield: $$\left\{\frac{0}{1}, \frac{1}{1}, \frac{1}{0}\right\}$$ $$\left\{\frac{0}{1}, \frac{1}{2}, \frac{1}{1}, \frac{2}{1}, \frac{1}{0}\right\}$$ $$\left\{\frac{0}{1}, \frac{1}{3}, \frac{1}{2}, \frac{2}{3}, \frac{1}{1}, \frac{3}{2}, \frac{2}{1}, \frac{3}{1}, \frac{1}{0}\right\}$$ $$\left\{\frac{0}{1}, \frac{1}{4}, \frac{1}{3}, \frac{2}{5}, \frac{1}{2}, \frac{3}{5}, \frac{2}{3}, \frac{3}{4}, \frac{1}{1}, \frac{4}{3}, \frac{3}{2}, \frac{5}{3}, \frac{2}{1}, \frac{5}{2}, \frac{3}{1}, \frac{4}{1}, \frac{1}{0}\right\}$$ $$\left\{\frac{0}{1}, \frac{1}{5}, \frac{1}{4}, \frac{2}{7}, \frac{1}{3}, \frac{3}{8}, \frac{2}{5}, \frac{3}{7}, \frac{1}{2}, \frac{4}{7}, \frac{3}{5}, \frac{5}{8}, \frac{2}{3}, \frac{5}{7}, \frac{3}{4}, \frac{4}{5}, \frac{1}{1}, \frac{5}{4}, \frac{4}{3}, \frac{7}{5}, \frac{3}{2}, \frac{8}{5}, \frac{5}{3}, \frac{7}{4}, \frac{2}{1}, \frac{7}{3}, \frac{5}{2}, \frac{8}{3}, \frac{3}{1}, \frac{7}{2}, \frac{4}{1}, \frac{5}{1}, \frac{1}{0}\right\}$$ $$\left\{\frac{0}{1}, \frac{1}{6}, \frac{1}{5}, \frac{2}{9}, \frac{1}{4}, \frac{3}{11}, \frac{2}{7}, \frac{3}{10}, \frac{1}{3}, \frac{4}{11}, \frac{3}{8}, \frac{5}{13}, \frac{2}{5}, \frac{5}{12}, \frac{3}{7}, \frac{4}{9}, \frac{1}{2}, \frac{5}{9}, \frac{4}{7}, \frac{7}{12}, \frac{3}{5}, \frac{8}{13}, \frac{5}{8}, \frac{7}{11}, \frac{2}{3}, \frac{7}{10}, \frac{5}{7}, \frac{8}{11}, \frac{3}{4}, \frac{7}{9}, \frac{4}{5}, \frac{5}{6}, \frac{1}{1}, \frac{6}{5}, \frac{5}{4}, \frac{9}{7}, \frac{4}{3}, \frac{11}{8}, \frac{7}{5}, \frac{10}{7}, \frac{3}{2}, \frac{11}{7}, \frac{8}{5}, \frac{13}{8}, \frac{5}{3}, \frac{12}{7}, \frac{7}{4}, \frac{9}{5}, \frac{2}{1}, \frac{9}{4}, \frac{7}{3}, \frac{12}{5}, \frac{5}{2}, \frac{13}{5}, \frac{8}{3}, \frac{11}{4}, \frac{3}{1}, \frac{10}{3}, \frac{7}{2}, \frac{11}{3}, \frac{4}{1}, \frac{9}{2}, \frac{5}{1}, \frac{6}{1}, \frac{1}{0}\right\}$$ My question is: Does this process eventually create all positive rational numbers?  (Is this process well-known?) My intuition is : There might be some formal definition for ""simple"" (something like ""small numerator and denominator"") such that the following statement holds: If $\frac{a}{b}$ and $\frac{c}{d}$ are positive rational numbers in reduced form with $\frac{a}{b}<\frac{c}{d}$ , s.t. for any $x$ , $\frac{a}{b}<x<\frac{c}{d}$ $\implies$ $\frac{a}{b}$ and $\frac{c}{d}$ are simpler than $x$ , then $\frac{a+c}{b+d}$ is the simplest number that lies between $\frac{a}{b}$ and $\frac{c}{d}$ . Then it would make sense that this process creates all the rational numbers in order of ""simplicity."" My attempts :
It's pretty straightforward to show that this process creates all positive integers, as well as all rational numbers of the form $\frac{a+1}{a}$ (and their reciprocals). In general, you can look at a number, and show that all the numbers directly to the left or right have a specific form (e.g. all numbers directly to the left of $\frac 3 2$ have form $\frac{1+3n}{1 + 2n}$ ). By hand, I've confirmed that all positive rational numbers with numerator and denominator up to $10$ appear. I thought maybe I could disprove my conjecture by comparing the process to starting with $\{0,1\}$ and splitting the interval repeatedly in half (which creates all positive rational numbers $\leq 1$ with denominator a power of $2$ ), but I made no headway. I wrote a program to show that, at least for the first $20$ iterations, the $\gcd$ of the numerator and the denominator is always $1$ , but I haven't been able to prove that this should in general be the case.","['recreational-mathematics', 'rational-numbers', 'recurrence-relations', 'sequences-and-series']"
4168702,Is Pythagoras the only relation to hold between $\cos$ and $\sin$?,"Pythagoras says that $\cos^2 \theta + \mathrm{sin}^2\theta = 1$ for all real $\theta$. (Vague) Question. Is this the only relationship between the functions $\cos$ and $\sin$? More precisely: Let $\langle \cos,\sin\rangle$ denote the intersection of all subalgebras of the $\mathbb{R}$-algebra of all functions $\mathbb{R} \rightarrow \mathbb{R}$ containing $\{\cos,\sin\}$. (By default, all my algebras are unital, associative and commutative.) Let $A$ denote the $\mathbb{R}$-algebra presented by the generators $\{c,s\}$ and the relation $c^2+s^2=1$. There is a unique $\mathbb{R}$-algebra homomorphism $\varphi : A \rightarrow \langle \cos,\sin\rangle$ given as follows. $$\varphi(c) = \cos, \,\,\varphi(s) = \sin$$ We know that $\varphi$ is surjective. Question. Is $\varphi$ injective? So consider $f \in A$. Then $f = \sum_{i,j : \mathbb{N}}a_{ij}s^ic^i$ for certain choices of $a_{ij} : \mathbb{R}$. Now suppose $\varphi(f)=0$. We want to show that $f=0$. Ideas, anyone?","['trigonometry', 'abstract-algebra']"
4168716,Matrix power of $e$ with complex e-values,"Calculate $e^A$ where $$A = \begin{bmatrix}1&0&3\\-1&2&0\\0&1&-1\end{bmatrix}$$ I knew how to do it if it was diagonalizable and real eigenvalues. How can I calculate when the matrix has complex eigenvalues. This matrix $A$ seems to have two conjugate complex evalues. I know how to calculate $\det \left(e^A\right)$ without knowing $e^A$ as: $$\det \left(e^A\right) = \det \left(e^J\right) = \det \left(e^{\text{tr}J}\right) = \det \left(e^{\text{tr}A}\right) = e^2$$ where $A = CJC^{-1}$ a Jordan form of $A$ , but I want to calculate $e^A$ . When I tried to find its eigenvalues to diagonalize it I came to $$\lambda^3-2\lambda^2-\lambda+5 = 0$$ which I couldn't solve. Also, I would really appreciate some beginner-friendly source references to learn more about problems of this type.","['jordan-normal-form', 'ordinary-differential-equations', 'reference-request', 'calculus', 'linear-algebra']"
4168722,Orthogonal basis of $L^2(\mathbb{R})$ and $L^2(\mathbb{R^+})$,"Let $\mathbb{R}^+ = [0, \infty)$ , i.e. the positive real numbers.
Let $L^2(\mathbb{R})$ and $L^2(\mathbb{R}^+)$ be the sets of real-valued functions with domains in $\mathbb{R}$ and $\mathbb{R}^+$ respectively that are square integrate with respect to the Lebesgue measure. These sets are endowed with the inner product $$
\left( f, g \right) = \int_{-\infty}^{\infty} f g \ \mathsf{d} x,
$$ and $$
\left( f, g \right) = \int_{0}^{\infty} f g \ \mathsf{d} x,
$$ respectively. I have the following questions Are the sets $L^2(\mathbb{R})$ and $L^2(\mathbb{R}^+)$ separable Hilbert Spaces?. In such a case, what Schauder basis exist for these sets?. Is there an known orthogonal family of functions on these sets? Motivation: For the case $L^2(\Omega)$ where $\Omega$ is compact, this is known to be true. The families of orthogonal functions on this set are used in a wide range of engineering applications, e.g. solving PDES in compact domains. However, I have not found similar results when $\Omega$ is unbounded. The existence of an orthogonal set of functions in $L^2(\mathbb{R})$ and $L^2(\mathbb{R}^+)$ would be useful to solve problems in unbounded domains.","['linear-algebra', 'functional-analysis']"
4168756,Projectivities of line $\mathbb{P_1}$ over $\mathbb{Z/2Z}$ field,"Let the projective line $\mathbb{P_1}$ over the field of $\mathbb{Z/2Z}$ . I'm asked to prove there are 6 projectivities between $\mathbb{P_1} \longrightarrow \mathbb{P_1}$ and giving the equations. This is what I thought about it. As long as there is 2 classes in $\mathbb{Z/2Z}$ and four points to determine the projectivity, we can combine them in 7 ways, acording to the condition $ad - bc \neq 0$ : $(0,1,1,1)$ , $(1,0,1,1)$ , $(1,1,0,1)$ , $(1,1,1,0)$ , $(0,1,1,0)$ , $(1,0,0,1)$ . Because if we take $(1,1,1,1)$ : $$z \rightarrow \frac{z+1}{z+1} = 1$$ which is no line. Actually, all equations goes by: $$z \rightarrow \frac{az+b}{dz+d}$$ I'm not sure to need to prove there are projectivities, that can be done through the double reason.
Is it correct? Edit: Let $T: E \longrightarrow E'$ an isomorphism between $\mathbb{K}$ - vectorial spaces. The aplication $\hat{T}: \mathbb{P}(E) \longrightarrow \mathbb{P}(E')$ and $\pi: E \longrightarrow \mathbb{P}(E) $ , then $\hat{T}$ is projectivity iff $\hat{T}(\pi (e)) = \pi (T(e))$ .","['solution-verification', 'projection', 'geometry', 'projective-space']"
4168778,Polynomial Graph Question Whose Solution Does Not Make Sense,"The question is: How many roots of the equation $3x^4 +6x^3 + x^2 +6x +3$ are real? The textbook's solution is: Let $f(x) = 3x^4 +6x^3 + x^2 +6x +3$ . Now $$f'(x) =12x^3 +18x^2 +2x +6\\
    \implies f''(x) = 36x^2 +36x +2 ≠ 0 \; \forall x \in \mathbb{R} $$ So, graph of $f'(x)$ intersects $x$ -axis only once. Hence, $f(x)$ has only one turning point. So $f(x)=0$ has maximum two real roots. Now $f(0)=3>0$ and $f(-1)=-5<0$ So graph cuts $x$ -axis between $-1$ and $1$ . Hence $f(x)=0$ has two real roots. My Question: How on earth does $f''(x)$ have no real solutions? Using the quadratic formula I get $2$ real negative roots. Am I missing something here? Also how does the fact that the graph cuts the $x$ -axis between $1$ and $-1$ imply that there are $2$ real roots? I just started reading the portion in the textbook on Graphs Of Polynomial Functions and I am in $12$ th grade so if you could, please explain in a little simple way thank you. Also sorry if the formatting is weird this is the first time I have asked a question here :) .","['algebra-precalculus', 'graphing-functions', 'derivatives', 'polynomials']"
4168801,Verification of solution for $\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)}$,Can you verify the solution to the following (very simple) problem? $$\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)}.$$ $$\text{limit} = \lim_{x\to 0^+} \frac{\ln(\frac{\sin(x/2)}{x/2})+\ln(x/2)}{\ln(\sin x/x)+\ln(x)} = \lim_{x\to 0^+} \frac{\frac{\ln(\frac{\sin(x/2)}{x/2})}{\ln(x)}+\frac{\ln(x/2)}{\ln(x)}}  {\frac{\ln(\sin x/x)}{\ln(x)}+1} \\=  \lim_{x\to 0^+} \frac{\frac{\ln 1}{-\infty}+\frac{\ln(x/2)}{\ln (x)}}{\frac{\ln 1}{-\infty}+1} = \lim_{x\to 0^+} \frac{\ln(x/2)}{\ln(x)} = \lim_{x\to 0^+} \frac{\ln(x)-\ln 2}{\ln(x)}  =1. $$ What did I do wrong? The given answer is $2$ .,"['limits', 'solution-verification', 'limits-without-lhopital']"
4168870,When is the centralizer of a subgroup equal to the subgroup itself?,"I am going through a proof of the proposition Assume $G$ is a group of order $pq$ , where $p$ and $q$ are primes with $p\le q$ and $p$ does not divide $q-1$ . Then $G$ is abelian. in Abstract Algbra by Dummit and Foote, and I am stuck at one step. The relevant assumptions are: $Z(G)=1$ (the center of $G$ is the trivial subgroup), $G$ has an element $x$ of order $q$ , and $H=\langle x\rangle$ . At this point the author concludes that $C_G(H)=H$ , ( $C_G(H)$ represents the centralizer of $H$ in $G$ ) but I cannot see why. I can see $H\subset C_G(H)$ . Also $C_G(H)=C_G(x)$ . But how to proceed? In particular, I do not know how the condition $Z(G)=1$ is relevant here. Any help is appreciated. If you believe more information is needed in the context, I can provide it.","['group-theory', 'abstract-algebra']"
4168910,"Proof that singleton sets are Borel sets in $\Omega = (0,1]$.","I’m reading the following definition and lemma, and I’d like to ask some questions. Definition 7.3 (a) Consider $\Omega=(0,1] .$ Let $\mathcal{C}_{0}$ be the collection of all open intervals in $(0,1] .$ Then $\sigma\left(\mathcal{C}_{0}\right)$ , the $\sigma-$ algebra generated by $\mathcal{C}_{0}$ , is called the Borel $\sigma$ - algebra. It is denoted by $\mathcal{B}((0,1])$ . (b) An element of $\mathcal{B}((0,1])$ is called a Borel-measurable set, or simply a Borel set. Thus, every open interval in $(0,1]$ is a Borel set. We next prove that every singleton set in $(0,1]$ is a Borel set. Lemma 7.4 Every singleton set $\{b\}, 0<b \leq 1$ , is a Borel set, i.e., $\{b\} \in \mathcal{B}((0,1])$ . Proof: Consider the collection of sets set $\left\{\left(b-\frac{1}{n}, b+\frac{1}{n}\right), n \geq 1\right\} .$ By the definition of Borel sets. $$
\left(b-\frac{1}{n}, b+\frac{1}{n}\right) \in \mathcal{B}((0,1]) .
$$ Using the properties of $\sigma$ -algebra, $$
\begin{aligned}
&\left(b-\frac{1}{n}, b+\frac{1}{n}\right)^{c} \in \mathcal{B}((0,1]) \\
\Longrightarrow & \bigcup_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right)^{c} \in \mathcal{B}((0,1]) \\
\Longrightarrow &\left(\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right)\right)^{c} \in \mathcal{B}((0,1]) \\
\Longrightarrow & \bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right) \in \mathcal{B}((0,1]) .
\end{aligned}
$$ Next, we claim that $$
\{b\}=\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right) .
$$ i.e., $b$ is the only element in $\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right)$ . We prove this by contradiction. Let $h$ be an element in $\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right)$ other than $b$ . For every such $h$ , there exists a large enough $n_{0}$ such that $h \notin$ $\left(b-\frac{1}{n_{0}}, b+\frac{1}{n_{0}}\right) .$ This implies $h \notin \bigcap_{n=1}^{\infty}\left(b-\frac{1}{n}, b+\frac{1}{n}\right) .$ Using $(7.1)$ and $(7.2)$ , thus, proves that $\{b\} \in$ $\mathcal{B}((0,1])$ I’m having a bit of trouble understanding what a typical element of $\sigma\left(\mathcal{C}_{0}\right)$ would be (or in general, what an element of a $\sigma$ -algebra generated by a set is). Would it be correct to say that, if $F \in \sigma\left(\mathcal{C}_{0}\right)$ , then either $F = \Omega \setminus S$ , where $S$ is a countable union of open intervals in $\Omega$ , or $F$ itself is a countable union of open intervals in $\Omega$ ? In the proof, when checking that $\{b\} \in \mathcal{B}((0,1])$ , are we only checking that the complement of $\{b\}$ is in $\mathcal{B}((0,1])$ ? In the first sentence of the proof, for a fixed $b$ , isn’t it true that not all $n$ ’s work?","['borel-sets', 'proof-explanation', 'measure-theory']"
4168939,"If $f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12}$, then $\int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx =\,$?","This is a question from a practice workbook for a college entrance exam. Let $$f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12}.$$ Find $$\int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx.$$ While I know that computing $f(f(x))$ is an option, it is very time consuming and wouldn't be practical considering the time limit of the exam. I believe there must be a more elegant solution. Looking at the limits, I tried to find useful things about $f(\frac{1}{7}+\frac{6}{7}-x)$ The relation I obtained was that $f(x) + f(1-x) = 12/12 = 1$ . I don't know how to use this for the direct integral of $f(f(x)).$","['integration', 'calculus', 'functions', 'definite-integrals']"
4168964,Property for Indefinite Integral,"Let $f$ be a continuous function on $[a, b]$ . Assume that there occurs a positive constant $T$ for which $$
|f(y)| \leq T \int_{a}^{y}|f(t)| d t
$$ for every value $y$ in $[a, b]$ . Prove that the equality $f(y)=0$ holds for any $y$ in $[a, b]$ . What could be the major strategies and original intuition behind finding a contradiction if one assumes the existence of some non-zero value? I tried to incorporate the continuity of function and indefinite integral, but it seems no success in mathematical rigor.","['integration', 'indefinite-integrals', 'derivatives']"
4168965,"Show that $\mathcal C : Y^2=X(X^2-pX+p^2)$ has rank $0$, where $p$ is a prime $\equiv$ 2 (mod 3) and 5 (mod 8)","The question is from a masters' level elliptic curves exam (Oxford, 2017): Let $p$ be a prime congruent to 2 (mod 3) and 5 (mod 8). Show that the elliptic curve $\mathcal C : Y^2=X\left(X^2-pX+p^2\right)$ has Mordell-Weil rank 0. Because of the layout of the course, I am 99% certain that this question can be solved by a fairly standard 2-descent argument. The problem is, I get stuck at excluding a potential generator. I'll set up the 2-descent and then explain where I get stuck. Let $\mathcal D: V^2=U(U^2+2pU-3p^2)$ . Then $\mathcal D$ is an elliptic curve, and there are 2-isogenies $\varphi\colon\mathcal C\to\mathcal D$ and $\hat{\varphi}\colon\mathcal D\to\mathcal C$ such that $\varphi\circ\hat{\varphi}$ and $\hat{\varphi}\circ\varphi$ are the multiplication-by-2 maps. We calculate $\mathcal C[2](\mathbb Q)$ (the rational 2-torsion of $\mathcal C$ ), $G:=\mathcal C(\mathbb Q)/\hat{\varphi}(\mathcal D(\mathbb Q))$ and $\hat{\varphi}(H):=\hat{\varphi}(\mathcal D(\mathbb Q)/\varphi(\mathcal C(\mathbb Q)))$ . From these last two sets we can calculate $\mathcal C(\mathbb Q)/2\mathcal C(\mathbb Q)$ and this, combined with the 2-torsion, allows us to determine the rank of $\mathcal C(\mathbb Q)$ . Obviously $\mathcal C[2](\mathbb Q)=\{\mathcal O,(0,0)\}\cong C_2$ , so that's fine. To calculate $G$ , we define a map $\hat{q}\colon G\to\mathbb Q^\times/\left(\mathbb Q^\times\right)^2$ by $\mathcal O\mapsto1,(0,y)\mapsto p^2=1$ and $(x,y)\mapsto x$ when $x\neq0$ . One can show that $\hat{q}$ is a well-defined injective group homomorphism, whose image consists of all squarefree integers $r\mid p^2$ for which there exist integers $l,m$ and $n$ , not all 0, with $\gcd(l,m)=1$ , such that $$\hat{W}_r : rl^4-pl^2m^2+p^2m^4/r = n^2.$$ Thus $\mathrm{im}~\hat{q}\leq\{\pm1,\pm p\}$ . We have $\hat{q}(\mathcal O)=\hat{q}(0,0)=1$ . The equations $\hat{W}_{-1}$ and $\hat{W}_{-p}$ have the form ""nonpositive = nonnegative"", so have no nontrivial solutions. On dividing $\hat{W}_p$ through by $p$ and completing the square, one can see that this has a solution mod $p$ only if 3 is a square mod $p$ , or $p\mid l$ and $m$ . The former is excluded by the given congruence info, the latter by $\gcd(l,m)=1$ . Thus $\mathrm{im}~\hat{q}=\{1\}$ and $G=\{\mathcal O\}$ . To calculate $H$ , in a similar vein we define a map $q$ by $\mathcal O\mapsto1,(0,v)\mapsto -3p^2=-3$ and $(u,v)\mapsto u$ when $u\neq0$ . Now we're looking for squarefree integers $r\mid-3p^2$ satisfying $$W_r : rl^4+2pl^2m^2-3p^2m^4/r=n^2.$$ Thus $\mathrm{im}~q\leq\{\pm1,\pm3,\pm p,\pm3p\}$ . Note that $\mathcal D$ factorises as $V^2=U(U-p)(U+3p)$ , so we have $q(\mathcal O)=1$ , $q(0,0)=-3$ , $q(p,0)=p$ and $q(-3p,0)=-3p$ . There's only one nontrivial coset left, so it suffices to exclude any one of $-1, 3$ , $-p$ or $3p$ from $\mathrm{im}~q$ to demonstrate that $\mathrm{im}~q=\{1,-3,p,-3p\}$ . The relevant equations are: $$W_{-1}:-l^4+2pl^2m^2+3p^2m^4=n^2,$$ $$W_{3}:3l^4+2pl^2m^2-p^2m^4=n^2,$$ $$W_{-p}:-pl^4+2pl^2m^2+3pm^4=n^2,$$ $$W_{3p}:3pl^4+2pl^2m^2-m^4=n^2.$$ The problem — It's in showing that one of the above equations has no solution that I get stuck. I've looked at them mod $p$ , mod 3, mod 9, even mod 16, and I've tried completing the square, but I can't seem to arrive at a contradiction. I believe that we need to exclude the above values of $r$ to get rank 0, which I will now justify. Let's suppose on the contrary that $\mathrm{im}~q=\{\pm1,\pm3,\pm p,\pm3p\}$ . Then $H=\langle (0,0),(0,p),R \rangle$ , where $R$ is some other point on $\mathcal D(\mathbb Q)$ (say $q(R)=-1$ ). Now $\hat{\varphi}(\mathcal O)=\hat{\varphi}(0,0)=\mathcal O$ and $\hat{\varphi}(0,p)=\hat{\varphi}(0,-3p)=(0,0)$ , so since $\hat{\varphi}$ is 2-to-1, $\hat{\varphi}(R)$ is independent of $(0,0)$ in $\mathcal C(\mathbb Q)/2\mathcal C(\mathbb Q)$ . Thus $\mathcal C(\mathbb Q)/2\mathcal C(\mathbb Q)=\langle (0,0),R \rangle\cong C_2\times C_2$ . So on removing the 2-torsion we get that the rank of $\mathcal C(\mathbb Q)$ is 1. Which is not 0. So we need to exclude this point $R$ . Edit: Simplicity of Solution — I'm not entirely sure whether this contravenes the rules of this site, but since this question comes from an exam, I require a solution that could reasonably be believed to be arrived at by a late undergraduate/early graduate who has only been acquainted with elliptic curves for one term, has approximately 30 minutes, and no access to a computer.","['number-theory', 'modular-arithmetic', 'elliptic-curves', 'arithmetic-geometry']"
4168994,Show that the given function has one zero inside the unit disk,"Let $f$ be an analytic function on $\{z:|z|\leq 1\}$ such that $\operatorname{Re}(\overline{z}f(z))>0$ for $|z| =1$ . Then $f$ has one simple zero on the unit disk. My attempt: I tried to show this using argument principle i.e. Claim : $\frac{1}{2\pi i}\int_{|z|=1}\frac{f'(z)}{f(z)}=1$ . To see this, $\frac{1}{2\pi i}\int_{0}^{2\pi}\frac{f'(e^{i\theta})}{f(e^{i\theta})}ie^{i\theta}d\theta = \frac{1}{2\pi}\int_0^{2\pi}\frac{f'(e^{i\theta})}{f(e^{i\theta})e^{-i\theta}}d\theta$ . I got $f(z)\overline{z}$ on the denominator but I don't know how to apply the given condition since even if I take the real part of the integral, I don't have any information of $f'$ . Could you give any hints?",['complex-analysis']
4169011,On $\int_0^{\pi} \operatorname{Si}^n(x) \ \mathrm{d}x $,"The Sine Integral, $\operatorname{Si}(x)$ , is defined as $$\operatorname{Si}(x)=\int_0^x \frac{\sin t}{t} \ \mathrm{d}t $$ It follows that $$\int_0^{\pi} \operatorname{Si}(x) \ \mathrm{d}x = \bigg[x \operatorname{Si}(x) \bigg]_0^{\pi}-\int_0^{\pi} \sin x \ \mathrm{d}x = \pi \operatorname{Si} (\pi)-2$$ and further $$\int_0^{\pi} \operatorname{Si}^2 (x) \ \mathrm{d}x = \bigg[ x \operatorname{Si}^2(x) \bigg]_0^{\pi} -2\int_0^{\pi} \operatorname{Si} (x) \sin x \ \mathrm{d}x \\ = \pi \operatorname{Si}^2(\pi) -2 \left( \big[ -\cos x \operatorname{Si} (x) \big]_0^{\pi}+\int_0^{\pi} \frac{\sin x \cos x}{x} \ \mathrm{d}x \right)\\ = \pi \operatorname{Si}^2(\pi)-2 \operatorname{Si} (\pi)-\int_{2x=0}^{2\pi}\frac{\sin 2x}{2x} \ \mathrm{d}(2x)\\ = \pi\operatorname{Si}^2(\pi)-2 \operatorname{Si} (\pi)-\operatorname{Si} (2\pi)$$ Following a similar procedure on $\int_0^{\pi} \operatorname{Si}^3(x) \ \mathrm{d}x$ doesn’t quite work, because it requires evaluating the integral $\int_0^{\pi} \operatorname{Si}^2(x) \sin x \ \mathrm{d}x$ , or equivalently, $\int_0^{\pi} \frac{\operatorname{Si}(x) \sin 2x}{x} \ \mathrm{d}x$ . This leads me to ask, can (and how can) $\int_0^{\pi} \operatorname{Si}^3(x) \ \mathrm{d}x$ be expressed in terms of elementary or special functions? How about the general case, i.e. $$\color{purple}{\int_0^{\pi} \operatorname{Si}^n(x) \ \mathrm{d}x} \; ? $$","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4169034,Proving the existence of $ξ$ and $η$ such that $f'(\xi)(\xi-a)+f'(\eta)(\eta-b)+f(a)+f(b)=0$,"Let $f$ be continuous on $[a,b]$ , and differentiable on $(a,b)$ , $\int_a^b f(x)dx=0$ . Show that there exists two distinct $\xi,\eta\in(a,b)$ , such that $$f'(\xi)(\xi-a)+f'(\eta)(\eta-b)+f(a)+f(b)=0$$ . My attempt: Let $F(x)=(x-a)[f(x)+f(a)]-\int_a^x f(t)dt$ , $G(x)=(x-b)[f(x)+f(b)]-\int_b^x f(t)dt$ . Then $F(a)=0, F(b)=-G(a), G(b)=0.$ And what we need is to show some $\xi\neq \eta$ , such that $F'(\xi)+G'(\eta)=0$ . Consider $0<c<1$ , and by the Lagrange intermediate value, it suffices to show for some $c$ , $\frac{F(c)}{c-a}+\frac{-G(c)}{b-c}=0$ . But such a $c$ is not obvious by continuity.",['calculus']
4169076,Why is the integral of the tangent function a natural log function?,"I understand the mechanics of using $u$ -substitution to solve $$\int\tan(x)\,dx = -\ln(\cos(x))$$ The textbook/online $u$ -sub solution examples smack of ""because that's what makes it work.""  What I'm looking for is an explanation of why it works.  What are the properties of these trig and log functions that enable them to be related via integrals and derivatives?","['integration', 'calculus', 'logarithms']"
4169110,Orbits of $\mathbb{Z}/n\mathbb{Z}$ under the action of its multiplicative group $\left(\mathbb{Z}/n\mathbb{Z}\right)^{\times}$,"Let $\mathbb{Z}/n\mathbb{Z}$ , $\left(\mathbb{Z}/n\mathbb{Z}\right)^{\times}$ , and $\mathcal{D}_n$ be the ring of integers modulo $n$ , its multiplicative group of order $\varphi(n)$ , and the set of divisors of $n$ , respectively. The map $\phi: \left(\mathbb{Z}/n\mathbb{Z}\right)^{\times}\times\mathbb{Z}/n\mathbb{Z}\rightarrow\mathbb{Z}/n\mathbb{Z}$ , given by $\phi(g,x):=gx$ is a left group action on $\mathbb{Z}/n\mathbb{Z}$ . I am having a hard time in showing that $\phi$ decomposes $\mathbb{Z}/n\mathbb{Z}$ into $|\mathcal{D}_n|$ -many distinct orbits given by $\left(\mathbb{Z}/n\mathbb{Z}\right)^{\times}\cdot\bar{\delta}$ , where $\delta$ is a divisor of $n$ . Let me give an example. If $n=12$ , then $\mathbb{Z}/12\mathbb{Z}=\{\bar{0},\dots,\bar{11}\}$ , $\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}=\{\bar{1}, \bar{5},\bar{7}, \bar{11}\}$ , and $\mathcal{D}_{12}=\{1,2,3,4,6,12\}$ and we have: $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{1}=\{\bar{1}, \bar{5},\bar{7}, \bar{11}\},$$ $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{2}=\{\bar{2}, \bar{10}\},$$ $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{3}=\{\bar{3}, \bar{9}\},$$ $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{4}=\{\bar{4}, \bar{8}\},$$ $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{6}=\{\bar{6}\},$$ $$\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{0}=\{\bar{0}\},$$ and $$\bigcup_{\delta|12}\left(\mathbb{Z}/12\mathbb{Z}\right)^{\times}\cdot\bar{\delta}=\mathbb{Z}/12\mathbb{Z}.$$ I would be very grateful for any help or insights.","['elementary-number-theory', 'group-theory', 'group-actions', 'modular-arithmetic']"
4169121,Inverse element of a quotient group (proof),"When we prove that the quotient group is indeed a group we have for the inverse element, if $G$ is a group and $H$ is a normal subgroup of $G$ $$(xH)(x^{-1}H)=xx^{-1}HH=xx^{-1}H=1H$$ We were told that the fact in the first equality that we can switch the places is because $H$ is a normal subgroup. I don't see the connection unfortunately. For the normal subgroup we have that $xHx^{-1} \subseteq H$ but how does that relate to commutativity, if it is not given that $G$ is abelian?","['proof-explanation', 'quotient-group', 'abstract-algebra', 'normal-subgroups', 'group-theory']"
4169170,Correct notation for (partial) derivative evaluated in a given point,"Consider a function $f : \mathbb{R}^N \to \mathbb{R}.$ In general, we write the function in the from $f({\bf x})$ , where ${\bf x} = [x_1, x_2, \ldots, x_N]^\top \in \mathbb{R}^N$ . Consider the partial derivative with respect to the $i$ -th component of ${\bf x}.$ I would write: $$\frac{\partial f}{\partial x_i} ~\text{or} ~ \frac{\partial f({\bf x})}{\partial x_i}.$$ What is the correct way to represent this derivative when evaluated in the point ${\bf y} \in \mathbb{R}^N$ ? I don't feel so comfortable writing: $$\frac{\partial f({\bf y})}{\partial x_i},$$ since the reader may miss the relationships between $x_i$ and the argument of the function. Is $$\left.\frac{\partial f({\bf x})}{\partial x_i}\right|_{{\bf x} = {\bf y}},$$ a more correct way to write the partial derivative evaluated in ${\bf y}$ ?","['notation', 'soft-question', 'derivatives', 'partial-derivative']"
4169174,Lagrange quartic resolvent $x_1+ix_2-x_3-ix_4$,"Suppose we want to solve the ""reduced"" quartic equation $x^4+px^2+qx+r=0$ by means of Lagrange resolvent. I denote the roots by $x_1, x_2, x_3, x_4$ ; we have $x_1+x_2+x_3+x_4=0$ . In many texts one typically reads: the ""generic"" Lagrange resolvent would be $R=x_1+ix_2-x_3-ix_4$ ; however, Lagrange found easier resolvents, for example $(x_1+x_2)(x_3+x_4)$ . That's fine; but I still want to see how $R$ works. When one permutes the roots in all possible ways, $R^4$ has six (rather than three) distinct values. So we get an equation $\prod (x-\ldots)$ of degree 6 (its coefficients are some polynomials in $p,q,r$ ). At this point some texts say ""with some tricks, it can be reduced to degree 3"". I computed this equation: its coefficients are not pleasant, and have no clear pattern. So I guess that the tricks should not be applied on the explicit form. Hence the question: What are the tricks? How one reduces this equation to degree 3?","['galois-theory', 'abstract-algebra', 'quartics']"
4169222,Douglas' Lemma for Bounded operators on Hilbert Space,"Here is the original paper. There are a few steps (practically all of them) I don't understand: If $A=BC$ , then $AA^* = BCC^*B^* = \|C\|^2BB^* - B(\|C\|^2\mathbb I - CC^*)B^* \le  \|C\|^2BB^* $ Why the $\le$ sign? In general, the notation $X \le Y$ is meaningful if the operator $Y-X$ is positive, namely $Y-X \ge 0$ . In this case, $B(\|C\|^2\mathbb I - CC^*)B^* \ge 0$ ? If we suppose ${\rm ran}A \subset {\rm ran}B$ , then we can define an operator $C_1$ on $\scr H$ as follows: for $f \in \scr H \dots $ there exists $h \in  \{\ker B \}^\perp$ for which $Bh = Af$ . Set $C_1 f = h.$ Why should such an $h$ exists? I mean what does $\{\ker B \}^\perp \equiv 
\overline {{\rm ran } B^*}$ have to do with $ { \rm ran}B$ ? $\dots$ because $\ker B$ is closed, it follows that $h \in  \{\ker B \}^\perp$ so that $Bh = Af$ . Hence $C_1$ has been shown to be bounded. The purpose was to show the graph of $C_1$ is closed. No idea how he proved it this way. Define a mapping $D : {\rm ran}B^* \to {\rm ran}A^*$ so that $D(B^*f)=A^*f$ . Then $D$ is well defined since $\|D(B^*f)\|^2 \le \dots \le \lambda^2\|B^*f\|^2$ . Is an operator well-defined just because it is bounded? Remark : this more recent article starts exactly with Douglas lemma, except it uses $A^*A$ and $B^*B$ instead of $AA^*$ and $BB^*$ of the original paper. These are clearly not the same operators, since $A$ and $B$ are not supposed to be normal in the first place, just bounded. So? Which one is true?","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4169230,Finding recursive relation by using Goulden- Jackson method,"I saw this question yesterday . Moreover , it has two beautiful answers such that Number of $n$ length word that can be formed using the alphabets $a$, $b$, $c$, $d$ such that $a$ and $b$ never come together. The question says that Number of $n$ length word that can be formed using the alphabets $a$ , $b$ , $c$ , $d$ such that $a$ and $b$ never come together. I tried to solve this question , but i reached to wrong answer because of overcounting. When i tried Goulden-Cluster method , i made very big mistake because i should have calculated the number of $n$ lenght word that does not contain $ab$ or $ba$ . You can see my wrong answer for this question such that ""Lets use recursion. Let say that $n$ length string end up with $a$ and do not have $a$ and $b $ together , so there are $a_{n-1}$ such strings Let say that $n$ length string end up with $b$ and do not have $a$ and $b $ together , so there are $a_{n-1}$ such strings Let say that $n$ length string end up with $c$ and do not have $a$ and $b $ together , so there are $a_{n-1}$ such strings Let say that $n$ length string end up with $d$ and do not have $a$ and $b $ together , so there are $a_{n-1}$ such strings $\color{blue}{However}$ , when it end up with $a$ or $b$ ,the preceding term before the last term may be $b$ for $a$ and $a$ for $b$ , such that $.....ab$ or $......ba$ . Hence ,we must $\color{red}{subtract}$ those strings. These strings can be indicated by $a_{n-2}$ . Then $a_n=4a_{n-1}-2a_{n-2}$ where $a_3=48,a_2=14 , a_1=4 , a_0=1$ MORE EXPLANATION= Lets assume that a string ends up with $\color{red}{a}$ , and it does not have any substring containing $a$ and $b$ together.Because of it ends up with $\color{red}{a}$ , the substring that does not contain any $a$ and $b$ together has lenght $a_{n-1}$ .However , we can see that this substring that does not contain any $a$ and $b$ together might start with $\color{blue}{b}$ such that $(....\color{blue}{b}-\color{red}{a})$ . Then , we obtain strings such that they contain $a$ and $b$ are together in the beginning but not in the rest of string. So , there are $a_{n-2}$ such strings. Thus ,we must subtract them from $4a_{n-1}$ strings. Moreover, this situation is valid for those string that end up with $\color{blue}{b}$ . Hence there are $2a_{n-2}$ such strings. Moreover, when i check the result by goulden -cluster method , i reached the same answer."" Now , i am looking for the true solution by using $\color{red}{Goulden - Jackson Cluster}$ method for this question. Firstly , i thought that i should find the generating function for never $ab$ .Then , find it for never $ba$ . At last , add them each other and subtract the generating function of never $ab$ and $ba$ at the same time. However, i did not work . Thanks in advance...","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4169245,"How do you simplify $\cup\{A, \cup A\}$?","I want to simplify $\cup\{A, \cup A\},$ and also $\cup\cup\{A, \cup A\},$ so forth. I thought $\cup\{A, \cup A\}$ would not be simplified more. To say this in plain english, this union is a set that contains 1) all elements in $A$ and 2) all elements in all elements in $A,$ which I am not sure how to simplify. Then $\cup\cup\{A, \cup A\}$ is a set that contains all elements in the first union that I just described. Is it possible to simplify this union more? Thanks for your help.","['elementary-set-theory', 'set-theory']"
4169278,How to solve a limit with a given condition $ f(0)=\frac{1}{3}$?,"I need to solve the limit, $$\lim_{x\rightarrow 0} \frac{1-\cos(x)}{f(x)\sin^2(x)}$$ Limit with a given condition such that $ f(0)=\frac{1}{3}$ . I think that to solve the limit is quite simple, but how to substitute for $x$ with the given condition, $ f(0) = \frac{1}{3}$ ? I just need some hints to make it clear.","['limits', 'calculus']"
4169295,Can a self-inverse function $y=f(x)$ always be expressed as an equation that is symmetric in $x$ and $y$?,"For example, the simple self-inverse function $y = 6 - x$ can be written as $x+y=6$ , which is symmetric in $x$ and $y$ .  Less apparent (to me at least), $y = (x+1)/(1-x)$ can be expanded and written as $x + y = xy - 1$ , which is also symmetric in $x$ and $y$ , and must therefore be self-inverse. Can all self-inverse functions be expressed this way?",['functions']
4169308,Understanding a basic permutation problem,"I am failing to wrap my head around a solution to a rather basic problem: At a party, n men and m women put their drinks on a table and go out on the floor
to dance. When they return, none of them recognizes his or her drink, so everyone
takes a drink at random. What is the probability that each man selects his own
drink? The solution is: $$\frac{m!}{(n+m)!}$$ From what I intuit, $n!$ is the total permutations of the drinks among the men, and the above solution is the probability that all men get another man's drink, not necessarily that each one gets his own . My solution was, there is a $\frac{1}{n+m}$ probability of the first man getting his drink, then $\frac{1}{n+m-1}$ for the second, $\frac{1}{n+m-2}$ for the third, ... , $\frac{1}{m+1}$ for the $n^{th}$ . Resulting in a probability of $\prod\limits_{i=0}^{n-1} \frac{1}{n+m-i}$ . Can someone explain their thinking when approaching this problem? EDIT Issue arose from confusing variables and failing to identify that $$
\begin{align}
\frac{m!}{(n+m)!} &= \frac{m!}{(n+m)(n+m-1)...(n+m-(n-1))(m!)}\\
&= \prod\limits_{i=0}^{n-1} \frac{1}{n+m-i}\\
\end{align}
$$","['permutations', 'probability']"
4169322,Estimate nearly-singular Gaussian covariance matrix,"Suppose I have $m$ samples drawn from a Gaussian in $\mathbb{R}^n$ , and need sample covariance $\Sigma_m$ to be $\epsilon$ -close to true covariance $\Sigma$ : $$E\|\Sigma_m-\Sigma\| \le \epsilon \|\Sigma\|$$ How many samples do I need? My distribution is nearly singular, in sense that intrinsic dimension $r$ is much smaller than embedding dimension $n$ where $$r=\frac{\text{tr}(\Sigma)}{\|\Sigma\|}$$ The term ""intrinsic dimensions"" comes from Tropp's book , Chapter 7. I found the following sample-size requirement in Vershynin, High-Dimensional Probability Remark 5.6.3, for an arbitrary distribution: $$m \approx \epsilon^{-2} r \log n$$ Can this be tightened for a Gaussian distribution? In particular, I'm wondering if the $\log n$ factor can be dropped. Here's what error looks like for various dimensions with intrinsic dimension fixed. notebook","['statistics', 'probability-theory', 'normal-distribution']"
4169332,Is there a pattern to the coefficients in the piecewise equations of the Irwin–Hall distributions?,"Intro and Problem Statement The Irwin–Hall distribution is a probability distribution of the sum of $n$ independent, uniformly-distributed, continuous random variables in the interval $[0, 1]$ . The distribution is a piecewise polynomial function composed of $n$ sections of degree $(n − 1)$ which, combined, cover the interval $[0, n]$ . The zeroth piecewise segment is always of the form $\frac{x^{n − 1}}{(n − 1)!}$ . The last segment is always of the form $\frac{(−x + n)^{n − 1}}{(n − 1)!}$ . The segments in between are more complicated, and are where I'm running into trouble. I want to find a pattern that allows me to generate the piecewise equation for an arbitrary segment in the distribution of an arbitrary number of variables. The Basics The probability distribution of a single variable looks like a horizontal line (degree zero). The equation of the probability distribution is: $$f_X(x; 1) = \begin{cases}
1 & : 0 ≤ x ≤ 1 \\
0 & : \text{otherwise}
\end{cases}$$ If we add a second identical variable to the first, their combined probability distribution runs from 0 to 2, composed of two diagonal lines (degree one). The equation of the combined probability distribution of two variables is: $$f_X(x; 2) = \begin{cases}
x & : 0 ≤ x ≤ 1 \\
−x + 2 & : 1 < x ≤ 2 \\
0 & : \text{otherwise}
\end{cases}$$ If we add a third variable, the combined probability distribution runs from 0 to 3, composed of three parabolic arcs (degree two). The equation of the combined probability distribution of three variables is: $$f_X(x; 3) = \begin{cases}
\frac{x²}{2} & : 0 ≤ x ≤ 1 \\
\frac{−2 x² + 6x − 3}{2} & : 1 < x ≤ 2 \\
\frac{(−x + 3)²}{2} & :  2 < x ≤ 3 \\
0 & : \text{otherwise}
\end{cases}$$ As more and more variables are added (as $n$ gets large), their combined Irwin-Hall distribution approaches a normal distribution with mean $μ = \frac{n}{2}$ and variance $σ² = \frac{n}{12}$ . How I'm Building Up the Equations The $(n − 1)$ th derivative of the $k$ th segment in the distribution, skipping the zeroth segment whose $(n − 1)$ th derivative is always 1, is of the form $\frac{(−1)^k}{k!} (n − 1) (n − 2) (n − 3) \, ... (n − k)$ , or equivalently $\frac{(−1)^k (n − 1)!}{k! (n − 1 − k)!}$ . For example, in the case of four variables, the 3rd derivatives of the four segments are: $\frac{ 1}{0!} = 1$ , $\frac{−1}{1!} (4 − 1) = −3$ , $\frac{ 1}{2!} (4 − 1) (4 − 2) = 3$ , and $\frac{−1}{3!} (4 − 1) (4 − 2) (4 − 3) = −1$ . In the case of five variables, the 4th derivatives of the five segments are: $\frac{ 1}{0!} = 1$ , $\frac{−1}{1!} (5 − 1) = −4$ , $\frac{ 1}{2!} (5 − 1) (5 − 2) = 6$ , $\frac{−1}{3!} (5 − 1) (5 − 2) (5 − 3) = −4$ , and $\frac{ 1}{4!} (5 − 1) (5 − 2) (5 − 3) (5 − 4) = 1$ . These numbers correspond to the $(n − 1)$ th row of Pascal's triangle, with every other number negative. That is, for $n = 4$ variables, the 3rd derivatives of the four segments are the four terms in the 3rd row of the triangle, with odd positions negated. To build up the full $n$ -segment distribution curve, we take the integrals of these base numbers $(n − 1)$ times, adding a new constant term each time. The constant term for a given integration step depends on the number $m$ of integrations, including the current step (starting at one). The constant we add to the zeroth segment is always zero. The constant we add to the first segment is $\frac{(−1)^{m − 1}}{0! × m!} (n)$ . The constant we add to the second segment is $\frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n)$ . The constant we add to the third segment is $\frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + 2! (1 + 2^{m − 1} + 3^{m − 1}) n)$ . The constant we add to the fourth segment is $\frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n)$ These are the expressions I want to figure out how to create more of. As an example, the equation for the third segment ( $k = 3$ ) after three integrations is: $$\int \left( \int \left( \int \left( \frac{−1}{3!} (n − 1) (n − 2) (n − 3) \right)  + \frac{n³ − 5n² + 6n}{2! × 1!} \right) − \frac{3n³ − 13n² + 12n}{2! × 2!} \right) + \frac{9n³ − 35n² + 28n}{2! × 3!}$$ In the case of four variables, that simplifies to: $$\int \left( \int \left( \int \left( −1 \right)  + 4 \right) − 8 \right) + \frac{32}{3}$$ Which expands into: $$\frac{−x³}{6} + 2x² − 8x + \frac{32}{3}$$ Which can be re-written as: $$\frac{−x³ + 12x² − 48x + 64}{3!}$$ Which factors into: $$\frac{(−x + 4)³}{3!}$$ The whole process is complicated, but not difficult. The Stumbling Block The problem is, I can't figure out the patterns for creating constants beyond the fourth segment. For the first four segments, I have the patterns for their constant terms and can generate equations for the probability distributions of any number of variables. Using WolframAlpha, I've worked out the equations for generating the constants out to eight segments for values of $n$ up to 6, but by the seventh or eighth segment and variable it starts running out of computation time and won't give an answer. And using only the eight equations I am able to get, I have been unable to find the patterns for all the coefficients of those equations. To recap, here are the equations for the terms that I have figured out: $0$ $\frac{(−1)^{m − 1}}{0! × m!} (n)$ $\frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n)$ $\frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + (1 + 2^{m − 1} + 3^{m − 1}) n)$ $\frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n)$ Beyond four, though, I'm stuck. I feel like there should be a pattern to these equations that I can use to generate the equations for the constants for an arbitrary segment, but if there is one, I can't find it. Some partial patterns do stand out to me: The expressions are polynomials in terms of $n$ with coefficients composed of sums of numbers raised to the power of $(m − 1)$ The $k$ th expression is $k$ terms long, with highest degree $k$ and containing terms of all lower degrees down to and including 1, with zero constant term The first term in the $k$ th expression is $k^{m − 1} × n^k$ The last term in the $k$ th expression is of the form $(−1)^{k − 1} × (k − 1)! × (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + \, ... + k^{m − 1}) × n$ The terms alternate positive and negative — the first is always positive, the second is always negative, and so on Here are the equations for the fifth segment's constants (numbered by $m$ value): $\frac{ 1}{4! × 1!} (     n⁵ −     14n⁴ +      71n³ −     154n² +     120n)$ $\frac{−1}{4! × 2!} (    5n⁵ −     66n⁴ +     307n³ −     582n² +     360n)$ $\frac{ 1}{4! × 3!} (   25n⁵ −    314n⁴ +    1367n³ −    2374n² +    1320n)$ $\frac{−1}{4! × 4!} (  125n⁵ −   1506n⁴ +    6235n³ −   10230n² +    5400n)$ $\frac{ 1}{4! × 5!} (  625n⁵ −   7274n⁴ +   28991n³ −   45814n² +   23496n)$ $\frac{−1}{4! × 6!} ( 3125n⁵ −  35346n⁴ +  136867n³ −  210822n² +  106200n)$ $\frac{ 1}{4! × 7!} (15625n⁵ − 172634n⁴ +  653927n³ −  989254n² +  492360n)$ $\frac{−1}{4! × 8!} (78125n⁵ − 846786n⁴ + 3153835n³ − 4708950n² + 2323800n)$ Here are the equations for the sixth segment's constants (numbered by $m$ value): $\frac{−1}{5! × 1!} (      n⁶ −      20n⁵ +      155n⁴ −      580n³ +      1044n² −      720n)$ $\frac{ 1}{5! × 2!} (     6n⁶ −     115n⁵ +      840n⁴ −     2885n³ +      4554n² −     2520n)$ $\frac{−1}{5! × 3!} (    36n⁶ −     665n⁵ +     4630n⁴ −    14935n³ +     21734n² −    10920n)$ $\frac{ 1}{5! × 4!} (   216n⁶ −    3865n⁵ +    25890n⁴ −    79775n³ +    110334n² −    52920n)$ $\frac{−1}{5! × 5!} (  1296n⁶ −   22565n⁵ +   146530n⁴ −   436555n³ +    584174n² −   273000n)$ $\frac{ 1}{5! × 6!} (  7776n⁶ −  132265n⁵ +   837690n⁴ −  2433935n³ +   3184734n² −  1464120n)$ $\frac{−1}{5! × 7!} ( 46656n⁶ −  777965n⁵ +  4828930n⁴ − 13767235n³ +  17730014n² −  8060520n)$ $\frac{ 1}{5! × 8!} (279936n⁶ − 4589665n⁵ + 28028490n⁴ − 78754775n³ + 100247214n² − 45211320n)$ And here are the only equations I've been able to figure out for the seventh segment's constants (numbered by $m$ value): $\frac{ 1}{6! × 1!} ( n⁷ −  27n⁶ +  295n⁵ −  1665n⁴ +  5104n³ −  8028n² +  5040n)$ $\frac{−1}{6! × 2!} (7n⁷ − 183n⁶ + 1915n⁵ − 10185n⁴ + 28678n³ − 39672n² + 20160n)$ I've tried different ways of factoring the equations, but haven't been able to find a pattern there either. Is there a pattern for figuring out the rest of the coefficients for these equations to generate the constant terms for the integrals that lead to the probability distribution?","['statistics', 'probability-distributions', 'pattern-recognition', 'probability']"
4169343,Where is it possible to have an isometric immersion of $\mathbb{H}^2$?,"I'm researching the Hilbert's Theorem .- There is no isometric immersion of the hyperbolic plane $\mathbb{H}^2$ in $\mathbb{R}^3$ . But on the way I have come across several results, such as: Efimov's theorem .- There is no isometric immersion of a complete surface with Gaussian curvature bounded superiorly by a negative constant, at $\mathbb{R}^3$ . Then the question arose, where is it possible to have an isometric immersion of $\mathbb{H}^2$ ? I read that there is an isometric immersion in $\mathbb{R}^5$ , but I only got this result, Blanusa's theorem .- There is an isometric embedding proper to the hyperbolic plane $\mathbb{H}^2$ in $\mathbb{R}^6$ . From Efimov's theorem I have managed to find references to read, but from Blanusa's Theorem or other similar ones I have not obtained much information. If someone knows the topics and could give me some references or perhaps other theorems that can enrich my research, I would appreciate it very much. Some documents i have found: D. Brander - Isometric Embeddings between Space Forms T. Milnor - Efimov's theorem about complete immersed surface of negative curvature Do Carmo - Differential geometry And others Here","['isometry', 'hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
4169371,Solving ODE-systems with Matrix exponential is wrong?,"Originally I've learned that the solution of a systems of coupled ODE: $$\underbrace{\left[\begin{array}{cc}{y_1}'(x)\\ \vdots \\{y_n}'(x)\end{array}\right]}_{y'(x)}= 
\underbrace{\left[\begin{array}{cccc}&a_{1\,1} &\cdots &a_{1\,n}
\\ &\vdots \quad &&\vdots \\
&a_{n\,1}&\cdots&a_{n\,n}\end{array}\right]}_{A}\,
\underbrace{\left[\begin{array}{cc}{y_1}(x)\\ \vdots \\{y_n}(x)\end{array}\right]}_{y(x)}$$ is determined by: $$y(x) = \exp(A\,x)\,C$$ where $C$ is a vector with constants $\left[\begin{array}{cc}C_1\\ \vdots \\C_n\end{array}\right]$ and $\exp(A\,x)$ the matrix exponential, that can be at best calculated by: $$\exp(A\,x) = V^{-1}\,\exp(\Lambda\,x)\,V$$ where $V$ is a vector full of Eigenvectors: $\left[\begin{array}{cc}v_1&\cdots&v_n\end{array}\right]$ and $\Lambda$ a matrix full of Eigenvalues on its main diagonal: $\left[\begin{array}{ccc}\lambda_1&\\ &\ddots\\&&v_n\end{array}\right]$ Now apparently this leads to another solution compared to: $$y(x) = c_1\,v_1\,\exp(\lambda_1\,x)+\cdots+c_n\,v_n\,\exp(\lambda_n\,x)$$ Even if one told me both solutions were to solve a system of ODE For example consider the system: $$\left(\begin{array}{cc}{y_1}'(x) \\ {y_2}'(x)\end{array}\right) = \left(\begin{array}{cc} 4 & 10\\ 8 & 2 \end{array}\right)\,\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right)$$ with Eigenvalues $\lambda_1 = 12, \lambda_2 = -6$ and Eigenvectors: $v_1 = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right), v_2 = \left(\begin{array}{cc}1 \\ -1\end{array}\right)$ According to the second plain solution process I'd obtain: $$\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = \left(\begin{array}{cc}1 \\ 8/10\end{array}\right)\,\exp(12\,x)+\left(\begin{array}{cc}1 \\ -1\end{array}\right)\,\exp(-6\,x)$$ However the matrix exponential spits: $$\left(\begin{array}{cc}{y_1}(x) \\ {y_2}(x)\end{array}\right) = C\,\left(\begin{array}{cc} \frac{4\,{\mathrm{e}}^{-6\,x}}{9}+\frac{5\,{\mathrm{e}}^{12\,x}}{9} & \frac{5\,{\mathrm{e}}^{12\,x}}{9}-\frac{5\,{\mathrm{e}}^{-6\,x}}{9}\\ \frac{4\,{\mathrm{e}}^{12\,x}}{9}-\frac{4\,{\mathrm{e}}^{-6\,x}}{9} & \frac{5\,{\mathrm{e}}^{-6\,x}}{9}+\frac{4\,{\mathrm{e}}^{12\,x}}{9} \end{array}\right)\,$$ Probably those two are inconvenient, because the constants are set differently. In fact the second approach is independent of constants somehow. So how's that all in relation with each other?","['systems-of-equations', 'ordinary-differential-equations']"
4169389,"Over a general field, does $h^1(X,\mathcal{O}_X)=0$ imply $\text{Pic}(X)\simeq \text{NS}(X)$?","Let $X$ be a smooth algebraic surface over $\Bbb{C}$ . From the exponential sheaf sequence $1\to 2\pi i\,\Bbb{Z}\to \mathcal{O}_X\to\mathcal{O}_X^*\to 1$ we get a long exact sequence on cohomology, from which we obtain the maps $$H^1(X,\mathcal{O}_X)\to\text{Pic(X)}\xrightarrow{\delta} H^2(X,\Bbb{Z})$$ The Néron-Severi group is given by $\text{NS}(X):=\text{im }g$ and, by definition, divisors $D_1,D_2\in\text{Pic}(X)$ are algebraically equivalent if and only if $\delta(D_1)=\delta(D_2)$ . Consequently, if $h^1(X,\mathcal{O}_X)=0$ then by exactness $\delta$ is injective and induces an isomorphism $\text{Pic}(X)\simeq \text{NS}(X)$ , i.e. linear equivalence and algebraic equivalence coincide in $X$ . (if there is something wrong in the above, please let me know) Now my question is: in a more general setting, do linear and algebraic equivalencies coincide when $X$ is over an algebraically closed field $k$ and $h^1(X,\mathcal{O}_X)=0$ holds? I hope the answer is yes, but I don't know a general version of the exponential sheaf sequence, so I don't know how to get the sequence on cohomology. P.S.: if additional hypothesis on $X$ are necessary, I'd happy to know which.","['algebraic-geometry', 'surfaces', 'line-bundles']"
4169422,Homework Question -Vector Calculus Area,"I want to calculate the area of a semi-circle. I can use this $\iint x^2+y^2\,\mathrm dx\,\mathrm dy$ or I can use this $\iint r^2 r^2 \sin(\phi) \,\mathrm \,d\theta\,\mathrm dr$ . I can see where the $r\,\mathrm d\theta \, dr$ comes from but why is it wrong to say $x=r\cos\theta$ and $y=r\sin \theta$ , then differentiate and multiply and get $$(dr)^2\cos\theta \sin\theta +r\,dr\,d\theta \cos^2 \theta −r\,dr\,d \theta \sin^2\theta −r^2(d\theta)^2\sin\theta \cos\theta?$$ Obviously I can't use the above in the integral but I cant see where this is wrong.","['multivariable-calculus', 'multiple-integral', 'area', 'vector-analysis']"
4169437,largest $n$ such that the $n$th derivative of $f(x) = x^\alpha \sin \left(\frac{1}{x} \right)$ at $x=0$ exists.,"Could you verify the following solution? Let, $$
f(x) =
\begin{cases}
x^\alpha \sin \left(\frac{1}{x} \right), &x\neq 0\\
0, & x=0,
\end{cases}
$$ where $\alpha \in \Bbb R$ is a constant. Find largest $n$ such that the $n$ th derivative of $f$ at $x=0$ exists. Since, each time we differentiate $f,$ we will get terms of the form $x^i \cdot \sin(1/x)$ or $x^i \cdot \cos(1/x)$ differentiating 1st time, the lowest value of $i$ obtained is $\alpha-2$ differentiating 2nd time, the lowest value of $i$ obtained is $\alpha-4$ differentiating $n$ times,  the lowest value of $i$ obtained is $\alpha-2n$ for $f^n(0)$ to exist we need, we need $f^{n-1}$ to be differentiable.  hence we need: $$\alpha-2(n-1)>1 \implies \alpha-2n+2>1 \implies \frac{\alpha+1}{2}>n \implies n<\frac{\alpha+1}{2}$$ hence the largest $n$ such that $f^n(0)$ exists is $n=\left\lceil\frac{\alpha-1}{2}\right\rceil$ so, if $\alpha=100$ , then $n=49$ if $\alpha=1$ , then $n=0$ if $\alpha=2$ , then $n=1$ if $\alpha=3$ , then $n=1$ if $\alpha=4$ , then $n=2$ if $\alpha=5$ , then $n=2$ if $\alpha=6$ , then $n=3$ and so on... is this correct?? edit: what i am doing is this. first calculate, $f'(x)$ as follows $$
f'(x) =
\begin{cases}
\alpha \cdot x^{\alpha-1}\sin \left(\frac{1}{x}\right) - x^{\alpha-2}\cos(\frac{1}{x}), &x\neq 0\\
0, & x=0,
\end{cases}
$$ where, $f'(0)$ is calculated using 1st principles. this way, we keep differentiating, and keep finding $f^i(x)$ and we keep evaluating $\lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0}$ going this way,
say for some $i$ , $\lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0}$ does not exist, then our choice of $n$ is that $i$ , how is this wrong??","['limits', 'solution-verification', 'derivatives']"
4169445,A step in the proof of the Hille-Yosida theorem from Rudin,"I'm getting stuck on perhaps a simple step in the Hille-Yosida theorem from 13.37 in Rudin's functional analysis.
I wonder if someone has had this same difficulty before or knows how to get around it - Setup: $A$ is a densely defined operator with domain $\mathcal{D}(A)$ in a Banach space $X$ and there are constants $C, \gamma >0$ such that for every $\lambda > \gamma$ and $m\in \mathbb{N}$ , $$
\| (\lambda I - A)^{-m}\| \leq C(\lambda - \gamma)^{-m}.
$$ The claim is then that $A$ is the infinitesimal generator of a semi-group of operators.
For small $\varepsilon$ , the bounded operator $S(\varepsilon)$ is defined to be $(I-\varepsilon A)^{-1}:X \to \mathcal{D}(A)$ .
It follows from the definitions that $AS(\varepsilon) = \varepsilon^{-1}(S(\varepsilon)-I)$ from which one can show that $e^{tAS(\varepsilon)}$ converges weakly to a bounded $Q(t)$ . Moreover, $\{Q(t) \}$ gives a semi-group.
Thus it has an infinitesimal generator $\tilde{A}$ .
Using the resolvent formulas for $AS(\varepsilon)$ and $\tilde{A}$ , we have for all $x$ and $\lambda$ sufficiently large, $$
(\lambda I - \tilde{A})^{-1}x = \int_0^\infty e^{-\lambda t} Q(t)x dt
$$ and $$
(\lambda I - AS(\varepsilon))^{-1}x = \int_0^\infty e^{-\lambda t} e^{tAS(\varepsilon)}x dt.
$$ One can easily justify the limit $$
\lim_{\varepsilon \to 0} \int_0^\infty e^{-\lambda t} e^{tAS(\varepsilon)}x dt = \int_0^\infty e^{-\lambda t} Q(t)x dt.
$$ Question: In order to compare $\tilde{A}$ and $A$ , how does one see that $$
\lim_{\varepsilon \to 0} (\lambda I - AS(\varepsilon))^{-1}x = (\lambda I - A)^{-1}x?
$$ THANK YOU!","['semigroup-of-operators', 'operator-theory', 'functional-analysis']"
4169502,Let $G$ be a group and $H$ solvable subgroup of index smaller or equal to $4.$ Show $G$ is a solvable group. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $G$ be a group and $H$ solvable subgroup of index smaller or equal to $4$ . Show $G$ is a solvable group. I was thinking about letting $G$ act on $G/H$ and then use a lemma that if $\ker\phi $ and ${\rm im}\, \phi$ are solvable then $G$ is solvable. But I got stuck","['group-theory', 'group-actions', 'solvable-groups']"
4169584,Systematic procedure for calculating derivatives involving Dirac delta terms,"As we know , $\nabla \cdot (\hat{r}/r^2) = 4\pi \delta^3(\vec{r})$ . This particular instance is easily justified by appealing to the divergence theorem, but there are also more complicated examples such as this one . My question is actually inspired by a post over at physics SE ( link ) where it seems that a delta function term has been left unaccounted for somehow, but I'm not sure where. In general, derivatives are easy to calculate systematically using the chain rule, product rule, etc. but it seems that when the result contains Dirac delta terms, some ad hoc method is used to derive them. Is there a systematic way to do it?","['derivatives', 'dirac-delta']"
4169663,What mapping does Escher use?,"In Escher's hyperbolic tesselations, he takes (effectively) a tesselation of the plane and maps it to a tesselation of the unit disk, by a mapping that takes straight lines to circles meeting the disk boundary at right angles.  What precisely is this mapping (with a formula)?  I am aware that this is effectively a mapping from the hyperbolic plane to the Poincare disk, taking a mapping from a tesselation of the hyperbolic plane to the disk.  However, what is it as a map from $R^2$ to $D$ , with formula.  I believe that it is a conformal map. What is the correct formula in higher dimensions, also?","['complex-analysis', 'tessellations', 'geometry']"
4169664,Find $\int _0^1\frac{12\arctan ^2 x\ln (\frac{(1-x)^2}{1+x^2})-\ln ^3(\frac{(1-x)^2}{1+x^2})}{x}\:dx$,"I want to find and prove that: $$\int _0^1\frac{12\arctan ^2\left(x\right)\ln \left(\frac{\left(1-x\right)^2}{1+x^2}\right)-\ln ^3\left(\frac{\left(1-x\right)^2}{1+x^2}\right)}{x}\:dx=\frac{9 \pi ^4}{16}$$ I tried to split the integral: $$=12\int _0^1\frac{\arctan ^2\left(x\right)\ln \left(\frac{\left(1-x\right)^2}{1+x^2}\right)}{x}\:dx+12\int _0^1\frac{\ln ^2\left(1-x\right)\ln \left(1+x^2\right)}{x}\:dx$$ $$-6\int _0^1\frac{\ln \left(1-x\right)\ln ^2\left(1+x^2\right)}{x}\:dx+\int _0^1\frac{\ln ^3\left(1+x^2\right)}{x}\:dx-8\int _0^1\frac{\ln ^3\left(1-x\right)}{x}\:dx$$ But the first $3$ integrals are very tough, expanding some terms yield very complicated series.
I also attempted to integrate by parts but it was rather messy and I'd prefer not to put it here. I have faith that there must be a trick for finding this one because of its closed forms simplicity. Note $2$ : I found this integral in a certain mathematics group, I've ask for hints but received none.","['integration', 'definite-integrals', 'logarithms', 'calculus', 'trigonometric-integrals']"
4169674,Prove by induction that $x^n$ can be calculated with at most $\log_2(n+1)$ multiplications,"Basis. When $n=0$ , no multiplication is done to compute $x^{0}=1$ . Since $\log_{2}(1)=0$ , the base case holds. Step. Suppose the claim holds for some $k \geq 0$ . To show that $x^{k+1}$ can be computed with at most $\log_{2}(k+2)$ multiplications, notice that $x^{k+1} = x^{k}x$ . We perform no multiplications to compute $x$ , and the induction hypothesis guarantees we need no more than $\log_{2}(k+1)$ multiplications to compute $x^{k}$ . Finally, we multiply $x^{k}$ with $x$ . Thus, $x^{k+1}$ can be computed with at most $$
\log_{2}(k+1) + 1 = \log_{2}(k+1) + \log_{2}(2) = \log_{2}(2(k+1)) = \log_{2}(k+k+2)
$$ multiplications. How do I finish this proof? Am I not seeing some logarithm property?","['induction', 'solution-verification', 'discrete-mathematics']"
4169682,Why is the average magnitude of the imaginary parts of the roots of these polynomials greater than that of the real part?,"Let $f_n(x)$ be a polynomial obtained by replacing $10$ with $x$ in the base $10$ expansion of $n, n \ge 10$ . Eg. $f_{98}(x) = 9x+8$ and $f_{2021}(x) = 2x^3 + 2x + 1$ . $f_n(x)$ has exactly $[k = \log_{10}n]$ roots each comprising of its real and imaginary parts, either of which can be zero. We call $f_n(x)$ as the characteristic polynomial of $n$ since $f_n(10) = n$ . I was comparing the average the magnitude or the absolute value of the real part of the roots of $f_n(x)$ to that of the imaginary parts. My naïve expectation was that they should be equal. However data suggests otherwise. For $n \le 10^4$ , I computed all the roots for the roots of $f_n(x)$ ; there are $28683$ roots in total since for $n \ge 100, f_n(x)$ has multiple roots. I plotted the graph of the average magnitude of the real parts and the imaginary parts. After around the $23700$ -th root, average of the real part falls below that of the real part and the seems to indicate no hope of a recovery. Question : Is there any reason why beyond a certain point, the average magnitude of the imaginary parts of the roots of the characteristic polynomial should be greater than that of the real part?","['roots', 'asymptotics', 'polynomials', 'numerical-methods', 'algebra-precalculus']"
4169719,Calculating gradient of scalar with vector jacobian products,"I am trying to calculate the $\phi$ gradient of the scalar \begin{align*} 
F = \sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\lambda_i \tag{1}
\end{align*} $f, \theta \in \mathcal{R}^{m}$ , $h_i \in \mathcal{R}^k$ , $h_i$ is a shorthand for $h_i(x_i, \theta)$ where $x_i \in \mathcal{R}^k$ . $\lambda_i \in \mathcal{R}^k$ and are defined by \begin{align*} 
&\lambda_{n} = 0 \\
&\lambda_{i-1} = \dfrac{\partial h_i}{\partial x_i}^T\lambda_i + \dfrac{\partial p_i}{\partial x_i}^T\beta_i(\phi)
\end{align*} where $\beta_i$ and $p_i = p_i(x_i)$ are scalars. Supposedly, using vector-jacobian products (i.e. reverse mode differentiation) the gradient $\nabla_{\phi}F$ can be calculated in some constant times complexity as $F$ . However, after much trying I am unable to express the gradient using only vector-jacobian products. I can only get it down to either matrix-jacobian products, or jacobian-vector products. Either way, this means calculating the gradient would scale with the dimension of $x$ , meaning it would not be constant, but scale with $k$ . This leaves me confused as to whether I am doing something wrong, or there is some sort of caveat to reverse differentiation that I'm not aware of. I highly suspect the former but I'm not really sure what to do differently. Gradient calculation Method A Using \begin{align*} 
\dfrac{\partial \lambda_{i-1}}{\partial \phi} = \dfrac{\partial h_i}{\partial x_i}^T\dfrac{\partial \lambda_i}{\partial \phi} + \dfrac{\partial p_i}{\partial x_i}^T\dfrac{\partial \beta_i}{\partial \phi}
\end{align*} so that the gradient is \begin{align*} 
\sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\dfrac{\partial \lambda_i }{\partial \phi} .
\end{align*} You can massage this further, but you always end up having to form some sort of matrix in $\mathcal{R}^{k\times k}$ . Method B The gradient is \begin{align*} 
\sum_{i=0}^{n-1}\xi_i^T\dfrac{\partial p_{i+1}}{\partial x_{i+1}}^T\dfrac{\partial \beta_{1+i}}{\partial \phi}
\end{align*} where $\xi$ are given by \begin{align*} 
&\xi_0^T = f^T\dfrac{\partial h_0}{\partial \theta}^T \\
& \xi_{i+1}^T = f^T\dfrac{\partial h_{i+1}}{\theta}^T + \xi_i^T\dfrac{\partial h_{i+1}}{\partial x_{i+1}}^T
\end{align*} In this case you have to calculate $\partial h_{i+1}/\partial x_{i+1}$ .","['jacobian', 'matrix-calculus', 'derivatives']"
4169720,"Prove that $m(U(n,\epsilon)) \xrightarrow{n\to\infty} 0$ where $U(n,\epsilon) = \{x: f_n(x) > \epsilon\}$","Suppose $(f_n)_{n\in\mathbb N}$ is a sequence of continuous functions on $[0,1]$ such that $f_n(x) \xrightarrow{n\to\infty} 0$ for every $0\le x\le 1$ , $0\le f_n\le 1$ for every $n\in\mathbb N$ . Define $U(n,\epsilon) = \{x: f_n(x) > \epsilon\}$ for $\epsilon > 0$ . Prove that $$m(U(n,\epsilon)) \xrightarrow{n\to\infty} 0$$ where $m$ is the restriction of the Lebesgue measure to $[0,1]$ . Try not to use anything fancy such as the Dominated Convergence Theorem (DCT). Try to work with the assumptions directly, such as continuity of $f_n$ and compactness of $[0,1]$ . I didn't make substantial progress, but let me collect all my thoughts here. I saw that $$\bigcap_{n=1}^\infty U(n,\epsilon) = \varnothing$$ If we could use the DCT, I would do (hope it's correct) $$\lim_{n\to\infty} m(U(n,\epsilon)) = \lim_{n\to\infty}\int_0^1 \mathbf{1}_{U(n,\epsilon)} \mathrm{d}m = \int_0^1 \lim_{n\to\infty}\mathbf{1}_{U(n,\epsilon)} \mathrm{d}m = 0$$ where $\mathbf{1}_A$ is the indicator function of set $A$ . However, I'm looking for ways to prove this without DCT. Another thing I noticed is that $(U(n,\epsilon))^c = [0,1]\setminus U(n,\epsilon)$ is compact, since it is closed and bounded. It is obviously bounded since it is contained in $[0,1]$ and it is closed since it is the continuous pull-back of a closed set, namely $[0,\epsilon]$ . Clearly, $U(n,\epsilon)$ is open. Any thoughts on what to do next? Thank you.","['measure-theory', 'real-analysis']"
4169724,How to find a point estimate for a given random sample of exponential distribution given the sample variance and four out of five sample values?,"Let $\left(x_1,x_2,x_3,x_4,x_5\right)$ be the observed values of a random sample of size $5$ from an exponential distribution with parameter $\beta$ . Out of five observed values four are given as $x_1=2$ , $x_2=4$ , $x_3=5$ , $x_4=5$ and if the sample variance is $s^2=1.5$ , then a point estimate of $\beta$ is? In this question a valid method of solving would be to assume a value $x$ for $x_5$ and finding sample variance with one equation and one variable. However, this neglects the first statement about the distribution that is being used and fails to take advantage of the properties that come along with the distribution. What is the significance of that first statement and how does it play a role in the solution?","['parameter-estimation', 'statistics', 'variance', 'exponential-distribution']"
4169748,Constructing a connection $1$-form from local forms.,"I am following Section $10.1.3$ of Geometry, Topology and Physics by Nakahara, and have ran in to an issue regarding local connection forms. Consider a principal $G$ -bundle, $P(M,G)$ , and an open cover $\{U_{i}\}$ for $M$ . Let $\mathcal{A}_{i}\in\mathfrak{g}\otimes\Omega^{1}(U_{i})$ be a Lie algebra valued $1$ -form on $U_{i}$ , and let $\sigma_{i}:U_{i}\to\pi^{-1}(U_{i})$ be a local section of $P$ . I wish to show that there exists a connection $1$ -form $\omega$ such that $\mathcal{A}_{i}=\sigma^{*}_{i}\omega$ . The candidate form is given locally by: $$
\omega_{i}=g_{i}^{-1}\pi^{*}\mathcal{A}_{i}g_{i}+g_{i}^{-1}d_{P}g_{i}
$$ where $g_{i}$ is the canonical local trivialisation, define by $\phi_{i}^{-1}(u)=(p,g_{i})$ for $u=\sigma_{i}(p)g_{i}$ . I have been able to show that indeed $\mathcal{A}_{i}=\sigma_{i}^{*}\omega_{i}$ , but for this to be a connection $1$ -form, I must additionally show that: $$
\omega_{i}(A^{\#})=A
$$ where $A\in\mathfrak{g}$ , and $A^{\#}$ is the fundamental vector field, defined by: $$
A^{\#}f(u)
=
\frac{d}{dt}f(ue^{tA})|_{t=0}
$$ for $f:P\to\mathbb{R}$ and $u\in P$ . I was able to show that indeed $(g_{i}^{-1}\pi^{*}\mathcal{A}_{i}g_{i})(A^{\#})=0$ , but I am struggling with the second term. The calculation in Nakahara goes like: $$
(g_{i}^{-1}d_{P}g_{i})(A^{\#})
=
g_{i}^{-1}(u)
\frac{dg(ue^{tA})}{dt}\biggr|_{t=0}
=
g_{i}^{-1}(u)g_{i}(u)\left(\frac{d}{dt}e^{tA}\right)\biggr|_{t=0}
=
A
$$ but I am unable to follow. I am very new to this topic, and any help would be much appreciated.","['principal-bundles', 'connections', 'fiber-bundles', 'mathematical-physics', 'differential-geometry']"
4169750,Is there an intuitive way to understand why $\det(AB)=\det(A)\det(B)\;$?,"Let $A, B \in L(V)$ (or equivalently $n \times n$ matrix), where $V$ is a $n$ -dimensional vector space. There are a multiple of proofs why $\det(A)\det(B) = \det(AB)$ , but I couldn't find a satisfactory that is intuition-appealing. First, the most imminent motivation for determinant is volume: $\det(A)$ is oriented volume of parallelepiped consisting $n$ -column vectors of $A$ . Given three matrices $A, B, AB$ , we have three distinct parallelepiped each of which has oriented volume $\det(A)$ , $\det(B)$ , and $\det(AB)$ , respectively. Second, the given $A, B \in L(V)$ as in linear transformation, $AB$ is simply a composition of linear transformation. I am trying to relate these two concepts, but it feels that the gap between two concepts are too broad. Is there a nice interpretation that fills the gap between these two concepts ?","['matrices', 'linear-algebra']"
4169797,"Prob. 25, Chap. 2, in Royden's REAL ANALYSIS: The assumption that $m\left(B_1\right)<\infty$ is necessary in ...","Here is Prob. 25, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Show that the assumption that $m\left( B_1 \right) < \infty$ is necessary in part (ii) of the theorem regarding continuity of measure. Here is the theorem regarding continuity of measure (i.e. Theorem 15) in Royden: Lebesgue measure possesses the following continuity properties: (i) If $\left\{ A_k \right\}_{k=1}^\infty$ is an ascending collection of measurable sets, then $$
m \left( \bigcup_{k=1}^\infty A_k \right) = \lim_{k \to \infty} m \left( A_k \right).
$$ (ii) If $\left\{ B_k \right\}_{k=1}^\infty$ is a descending collection of measurable sets and $m \left( B_1 \right) < \infty$ , then $$
m \left( \bigcap_{k=1}^\infty B_k \right) = \lim_{k \to \infty} m \left( B_k \right).
$$ My Attempt: For each positive integer $k$ , let us put $$ 
B_k := \big( k, \infty \big) = \big\{ x \in \mathbb{R} \mid k < x < \infty \big\}. 
$$ Then, for each $k$ , we have $$
B_k = \big( k, \infty \big) \supset \big( k+1, \infty \big) = B_{k+1}.
$$ And, by Proposition 8 every set $B_k$ , being an interval, is measurable, and by Proposition 1 we have $$
m \left( B_k \right) = m^* \left( B_k \right) = l \left( B_k \right) = \infty.
$$ Therefore $$
\lim_{k \to \infty} m \left( B_k \right) = \infty. \tag{1}
$$ On the other hand, we note that $$
\bigcap_{k=1}^\infty B_k = \emptyset, 
$$ because for every real number $a$ we can find a positive integer $k > a$ , and thus $a \not\in B_k$ for any such $k$ ; therefore we obtain $$
m \left( \bigcap_{k=1}^\infty B_k \right) = 0. \tag{2}
$$ From (1) and (2) we conclude that part (ii) of Theorem 15 does not hold for this particular countable collection of measurable sets for which $m \left( B_1 \right) \not< \infty$ . Is my proof correct and clear enough? If not, then where are the issues?","['measure-theory', 'analysis', 'real-analysis', 'measurable-sets', 'solution-verification']"
4169825,Complex integration to solve real integral $\int_{0}^{2}\frac{\sqrt{x\left(2-x\right)}}{\left(x+1\right)^{2}\left(3-x\right)}dx.$,"I have to solve this integral $$\int_{0}^{2}\frac{\sqrt{x\left(2-x\right)}}{\left(x+1\right)^{2}\left(3-x\right)}dx.$$ Thus, I formed the following contour: By Chauchy theorem I have $$I=\int_{C_{R}}f\left(z\right)dz+\int_{C_{r1}}f\left(z\right)dz+\int_{k_{\epsilon}}f\left(z\right)dz+\int_{z_{1}}f\left(z\right)dz+\int_{k_{\rho}}f\left(z\right)dz+\int_{C_{r2}}f\left(z\right)dz+\int_{z_{2}}f\left(z\right)dz=0$$ Now I use Jordan's lemma to calculate the values over small circles. I have $$\lim_{z\rightarrow0}z\cdot\frac{\sqrt{z\left(2-z\right)}}{\left(z+1\right)^{2}\left(3-x\right)}=0\,\,\Rightarrow\,\,\lim_{r\rightarrow0}\int_{k_{\rho}}f\left(z\right)dz=0$$ $$\lim_{z\rightarrow3}\left(z-3\right)\cdot\frac{\sqrt{z\left(2-x\right)}}{\left(z+1\right)^{2}\left(3-z\right)}=0 \Rightarrow\,\,\lim_{r\rightarrow0}\int_{C_{r1}}f\left(z\right)dz=0$$ Now I have the problem with term $(x+1)^2$ because I cannot apply Jordan's lemma because I cannot find the limit. $$\lim_{z\rightarrow-1}\left(z+1\right)\cdot\frac{\sqrt{z\left(2-z\right)}}{\left(z+1\right)^{2}\left(3-z\right)}=?$$ Does anyone have idea how to solve this? I think lemma cannot be applied here but I don't know the other way. I also found the function is regular in infinity thus integral over $C_{R}$ is zero.","['complex-analysis', 'contour-integration', 'complex-integration']"
4169831,"Does the ( so called) "" CAB"" factoring method work for every quadratic trinomial?","Note : my qustion assumes one is dealing with a trinomial that admits of a factorization. By ""CAB "" method I mean he following process: when confronted with an expression of the form $Ax^2+Bx+C$ (1) compute the product $AC$ (2) find 2 numbers $N_1$ and $N_2$ such that : $N_1\times N_2 = AC $ and $ N_1+N_2 = B$ (3) rewrite the original expression as $Ax^2+(N_1+N_2)x +C$ (4) develop this last expression and factor it by grouping I roughly see why it works when the factorization to be reached has the form $(ax+b)(x+c)$ but it's much less cleear to me when it has the form $ ( ax+b)(cx+d)$ . Hence my question : does the method work when the factorization to be reached contains two binomials in which the ""x-term"" has a coefficient different from $1$ ( in each one of these two binomials) ?","['algebra-precalculus', 'soft-question', 'polynomials']"
4169895,Don't know how to solve limits that have tan(x) as a power.,"I have these two problems. The first one is: $$\lim_{x \to \pi/2} \left(\frac{2x} \pi\right)^{\tan(x)}$$ and the second one is: $$\lim_{x \to \pi/4} (\tan(x))^{\tan(2x)}$$ The way I'm trying to solve them is by taking the natural logarithm of both sides and then trying to apply l'Hôpital's rule on the limit's side. However, in both cases, I found myself stuck trying to figure out how to find the limit of the denominator, which happens to be $$\frac{1} {\tan x}$$ And $$\frac{1}{\tan(2x)}$$ .","['limits', 'trigonometry']"
4169903,How to understand transformations on operators,"This question is related, or maybe is better to say inspired , to/by this other one about quantum mechanics . From what I currently understand the action of a transformation $T$ on another matrix, or to be more general on another operator $A$ , is obtained by applying the following formula: $$TAT^{-1}$$ Just to put some reference this formula can be found on this wikipedia article about change of basis . The result of this calculation should be another operator $A'$ $$A'=TAT^{-1} \tag{1}$$ that does exactly what $A$ was doing but on the transformed space (right?). Through the course of my studies I was always struggling to remember this formula; I was always making confusion between $TAT^{-1}$ and $T^{-1}AT$ , and I was always forced to double check. But then I found some nice physical/mathematical interpretation of this formula that made remembering it a lot easier: of course the formula must be $TAT^{-1}$ , because we want the action of $A'$ to be the same of $A$ but in the transformed space, so what we do to accomplish this is Using $T^{-1}$ to turn $\vec{v}'$ into $v$ Now that we are dealing with a vector in the non-transformed space we can apply $A$ to get the desired action Then we put back the result in the transformed space by applying $T$ This interpretation allows to clearly understand and remember why $T^{-1}$ must come before $T$ . I was really convinced by my own reasoning, but then I realized that there is a huge problem with my line of though: lets take $T$ to be a rotation , and lets also take $A$ to be some operator involving rotations . Problem now is that rotations do not commute , rotating in one way and then in another way is not the same as doing the opposite, and so my interpretation on why (1) works breaks apart badly! So is my interpretation of (1) wrong? Or does (1) not work with rotations? Seems strange.. In the case of my explanation of (1) being wrong: is there some other useful interpretation of (1)? This topic has also huge physical implications: for example in quantum mechanics we apply the parity operator $P$ to other hermitian operators all the time, and it would be nice to understand why the formula should be $PAP^{-1}$ and not $P^{-1}AP$ . (Maybe using parity as an example is not a great idea since $P=P^{-1}$ , but come on you get what I am saying here!)","['operator-algebras', 'matrices', 'change-of-basis', 'quantum-mechanics', 'transformation']"
4169972,Find the angle or prove it is constant,"We are given a regular pentagon ABCDE (with the letters in clockwise sequence).
A point Q moves on side BC and we draw a circle with center Q which passes through the vertex A and intersects the diagonal BE in point R. Prove that angle AQR is constant. If $s$ is the side of the regular pentagon, then the length of its diagonal is $d = s*\frac {\sqrt5+1}{2}$ .
Then, length of segment RE is $d - s = s*\frac {\sqrt5-1}{2}$ . Also if Q coincides with B, then the triangle ABR is a ""golden"" triangle and its base side $t$ is $\frac {s}{φ}$ where $φ$ is the golden ratio. So $t = RE$ and the angle $ARE = 180^\circ-72^\circ = 108^\circ$ . So AR and RE are 2 sides of a regular pentagon. So, angle $AQR = 36^\circ$ . Edit: I added the diagram, as you asked.
The pink polygon is not part of the problem - I just sketched it in my attempt to prove the question. I also tried to prove that quadrilateral $ABQR$ is cyclic, in which case, angle $α$ would be equal to $ABE$ , which we know is $36^\circ$ . Is this a sufficient proof?","['solution-verification', 'geometry']"
4170038,Measurability of the Moore-Penrose Inverse,"Let $A^+$ denote the Moore-Penrose inverse as a function on the space of real $n\times m$ matrices $A\in\mathbb{R}^{n\times m}$ . Is this function $\mathcal B(\mathbb{R}^{n\times m})$ measurable? From here we have the following limit relation: $A^+=\lim_{\delta\downarrow 0} (A'A+\delta I_m)^{-1}A'$ , so in particular we have $A^+=\lim_{k\to \infty} (A'A+\frac{1}{k} I_m)^{-1}A'$ . Matrix transpose and matrix multiplication are continuous on $\mathbb{R}^{n\times m}$ , and matrix inversion on the set of invertible matrices $A\in\mathbb R^{m\times m}$ is also continuous (see here ). Since continuous functions are measurable, we get that $A^+$ is the pointwise limit of a sequence of measurable functions, hence measurable. Is this reasoning ok? Thank you for your help.","['matrices', 'measure-theory', 'pseudoinverse', 'linear-algebra']"
4170045,Approximation of $\operatorname{Poisson}(\lambda)$ by Binomial,"I don't really understand the approximation of $\operatorname{Poisson}(\lambda)$ distributions by binomial distributions (law of rare events?). For example if I consider $X\sim \operatorname{Poissson}(1)$ , its distribution is not symmetric as drawed in wikipedia (orange curve). However, what I understood is that if we take say $n=1000$ (""big"") and $p=0.001$ (such that $np=1$ ) and we consider the binomial distribution $Y\sim B(n,p)$ we will have that the distribution of $X$ and the distribution of $Y$ are almost the same. But $X$ has a distribution which is not symmetric and $Y$ has a distribution that is symmetric which is confusing me.","['intuition', 'probability-theory', 'probability']"
4170077,Solution Verification: Find the mass of $V$. (Triple integral),"We define $V=\{(x,y,z)\in\mathbb{R}^3: x^2+y^2+z^2\le z$ } Mass Density is given by $g(x,y,z)=\sqrt{x^2+y^2+z^2}$ , Find the Mass of $V$ . My Work: So I need to find $I=\iiint_{V}\sqrt{x^2+y^2+z^2}dxdydz$ , and in order to do that, I will move to spherical coordinations: $x=rsin\phi cos\theta$ $y=rsin\phi sin\theta$ $z=rcos\phi$ . And so $x^2+y^2+z^2=r^2\le z = rcos\theta \Longrightarrow r\le cos\theta \Longrightarrow -\frac{\pi}{2} \le\theta \le \frac{\pi}{2}$ ( $cos\theta \ge 0)$ . and so there's no restrictions on $0 \le \phi \le \pi$ . and $0 \le r\le cos\theta$ . So: $$I=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\int_{0}^{\pi}\int^{cos\theta}_{0}r.r^2sin\phi drd\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int_0^{\pi}\frac{cos^4\theta}{4}sin\phi ]d\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{1+cos^2(2\theta)}{2}sin\phi]d\phi d\theta = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{3+cos(4\theta)}{4}sin\phi]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0(3sin\phi+cos(4\theta)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi+sin(4\pi)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi)]d\phi=\frac{3}{4}\pi (-cos(-\pi) - cos(\pi))=\frac{6\pi}{4}$$ I would really appreciate any feedback about my solution, even the smallest mistakes, and would love to get an approval of my answer, Thanks in advance!","['integration', 'multivariable-calculus', 'solution-verification']"
4170129,Outer automorphisms of extraspecial groups in GAP,"Let $G_n$ be the extraspecial group of order $2^{1+2n}$ . Its outer automorphism group is known to be isomorphic to the general orthogonal group $GO(2n)$ . I'd like to get an explicit map of this isomorphism in gap. Here's what I tried so far: n:=4;
grp:=ExtraspecialGroup(2^(2*n+1),""+"");
aut:=AutomorphismGroup(grp);
inn:=InnerAutomorphismsAutomorphismGroup(aut);
ort:=GeneralOrthogonalGroup(+1,2*n,2);
Print(""|out| = "",Size(aut)/Size(inn),""\n"");
Print(""|ort| = "",Size(ort),""\n"");
gen_aut:=GeneratorsOfGroup(aut);
gen_ort:=GeneratorsOfGroup(ort);
Print(""generators of aut = "",Length(gen_aut),""\n"");
Print(""generators of ort = "",Length(gen_ort),""\n"");

|out| = 348364800
|ort| = 348364800
generators of aut = 10
generators of ort = 3 The size of the two groups (out and ort) matches as expected, but I'm not sure how to proceed from here. How would you define the outer automorphism group in GAP and how would you align its generators to those of the orthogonal group and find the isomorphism.","['gap', 'group-theory', 'group-isomorphism', 'finite-groups']"
