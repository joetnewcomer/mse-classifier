question_id,title,body,tags
693943,Where does the power $2$ come from in the Pythagorean theorem?,"So $$a^2+ b ^2 =c^2$$ in a right triangle, but where does the power $2$ come from? I know we can use different metrics in the Euclidean space. If we use the $p$-metrics, where $p$ is in place of $2$, the case $p=2$ is the only one that makes $\mathbb{R}^n$ an inner product space (right?). So it's the only case where we can even talk about right triangles. But I guess my question is then that is there some sort of underlying (physical) reason for the exponent $2$?","['geometry', 'metric-spaces']"
693969,Distributive nearring,"A nearring is a ring-like structure $R$ such that $(R, +, 0)$ is a group (possibly non-abelian) $(R, \cdot)$ is a semigroup $(x+y)\,z=xz+yz$ (right-distributivity) In other words, three conditions that may fail differ it from an actual ring: Addition may be non-commutative There may be no multiplicative identity Left-distributivity may fail It's easy enough to give examples of nearrings for which arbitrary subsets of these 3 conditions nevertheless hold, except for (2)+(3) and pure (3). It is well-known that existence of identity and two-sided distributivity implies (1), so (2)+(3) without (1) is impossible. What about pure (3)? Question : are there fully distributive nearrings with non-abelian additive subgroups? Some thoughts: Such nearrings are necessarily pretty weird. Adapting the usual proof that (2)+(3) implies (1), we can argue that
$$
a\,((x+y)-(y+x))=0
$$
for all $x,y,z\in R$. This seems a pretty strong condition, even though lack of multiplicative identity does not allow us to conclude immediately $x+y=y+x$. Definition Let left-distributing element be $a\in R$ such that $a(x+y)=ax+ay$ for all $x, y\in R$ All nearrings are isomprhic to subsets of $M(G)$ - nearring of functions from group $G$ to itself with pointwise addition and composition as multiplication. Left-distributing elements are precisely endomorphisms. Still, set of endomorphisms does contain identity (likely minor obstacle), and for nonabelian $G$ is not a sub-nearring of $R$ (major problem). Product of left-distributing elements is left-distributing, but sum of left-distributing elements may fail to be, so the sub-nearring generated by endomorphisms in general most likely does not satisfy (3). Sub-nearring generated by endomorphisms mapping $G$ to $Z(G)$ would do the trick, but addition would be abelian.","['ring-theory', 'abstract-algebra']"
693971,"find a 4th order linear, non-homo ODE whose general solution:","How to find a fourth order, linear, not homogenous ODE with general solution: $y=c_1+c_2 x+c_3 e^{2x}\cos x+c_4e^{2x}\sin x-x e^{-x}$? Is there a specific method? I feel like it is guesswork to a certain degree. I can tell some parts such as the $c_1$ term will originally have had to be some sort of degree $4$ polynomial, and the $\sin,\cos$ terms will be some linear combination  $a\cos +b\sin$ (I think)  as well. but the other ones aren't as obvious to me. Any help would be appreicated. thank you!",['ordinary-differential-equations']
693975,Find generating function For sequences,"Can anyone out here help? The exercise says: ""Find the generating function for each of the sequences below (the general term is given)""
Now, the question is how do you find one for those:
a) $U_n = 3$ if $n$ is a multiple of $2$, otherwise $U_n = 0$
b) $U_n = 5$ if $n$ is a multiple of $3$, otherwise $U_n = 1$ If someone could help out in at least how to get the summation formula here (for line b), it would be really helpful. For a), if I'm not wrong, the summation goes like  $\sum_{k = 0}^n 3 (z^2)^k$ and $S(z) = 3/(1-z^2)$. But how do you get that ""1"" to show when $k$ is not a multiple of $3$ on line b)? It should go something like $\sum 5 (z^3)^k$ , and then be $1z^k$, whenever $k$ is not a multiple of $3$. Thanks in advance!","['generating-functions', 'summation', 'discrete-mathematics', 'functions']"
693981,Prove or disprove the continuity of the Identity function on the topological space?,"Let $\tau_1$be the usual topology on $\mathbb R$ . Define another topology $\tau_2$ on $\mathbb R$ by $$\tau_2 = \{U \subseteq \mathbb R \ \ | \ \ U^c \ \ is \ \ either \ \ finite \ \ or \ \ empty \ \ or \ \mathbb R\ \ \}$$ If $I : (\mathbb R , \tau_1) \rightarrow (\mathbb R , \tau_2)$ is the identity map , then $I$ is continous but not $I^{-1}$ $I^{-1}$ is continous but not $I$. both $I$ and $I^{-1}$ are continous . neither $I$ nor $I^{-1}$ are continous . My attempt is : since $(0,\infty)$ is open in $\tau_1$ but not in $\tau_2$ . so $\tau_2$ is not finer than $\tau_1$ . so $I^{-1}$ is not continous because we have a result $I$ is continous iff $\tau_1$ is finer than $\tau_2$ , similarly this result hold for $I^{-1}$ I think $I$ is continous because take any closed set in $\tau_2$ which is a finite set is closed in $\tau_1$ because R is Hausdorff. Iwould be thankfull who someone give me your valuable time  to check my solution.",['general-topology']
693998,Does $f(x)\in L^1$ imply that $f'(x) \in L^1$?,"Let $f(x)$ be defined for all real numbers differentiable function of one variable.We know that:
$$\int_{-\infty }^{+\infty } |f(x)| \, dx\neq +\infty$$
Problem is to resolve if it is possible or not that:
$$\int_{-\infty }^{+\infty } |f'(x)| \, dx= +\infty$$","['measure-theory', 'functional-analysis', 'real-analysis']"
693999,Is the determinant of a matrix Lipschitz continuous?,"I want to know if the determinant of a matrix is Lipschitz continuous or not. To be precise, does there exist a constant $K$ such that $|\det(A)-\det(B)|\leq K||A-B||_F$, for all matrices $A,B\in \mathcal{C}^{n\times n}$? If the answer is no, then what about being HÃ¶lder continuous? Does $|\det(A)-\det(B)|\leq K||A-B||_F^\alpha$ hold for some constant $K$ and $\alpha$? Can anyone help me on this? Thank you in advance!","['matrices', 'complex-analysis', 'real-analysis', 'analysis']"
694017,Transition from parametric to nonparametric statistics: what is $\Theta$?,"During my first statistics course I learned that a statistical model is a collection of probability measures $\mathcal{P}$, where we can index each measure by a 'parameter' $\theta$ such that $\mathcal{P} = \{P_\theta\,\,|\,\,\theta\in\Theta\}$. My first question is: What exactly is $\Theta$? I am now working on a project concerning nonparametric statistics where $\Theta$ is always an (infinite dimensional) vector space. However, when we look at the parametric normal family $\{N(\mu,\sigma^2)\,\,|(\mu,\sigma)\in\Theta = \mathbb{R}\times(0,\infty)\}$, then clearly $\Theta$ is no vector space. A possible answer that I thought of was that $\Theta$ is in general a metric space (although maybe just a set is enough?), but then how do we mark the transistion between a parametric model and non parametric model. To only separate when $\Theta$ is an infinite dimensional vector space produces strange cases. For example: when we consider an infinite dimensional vector space, but interpret it as just a metric space, do we suddenly deal with a parametric model? That seems odd... My second question: What exactly separates parametric and nonparametric models when we look at $\Theta$. Thank you!",['statistics']
694032,Origin in vector space?,"In the wikipedia article about vector space I do not understand this sentence Roughly, affine spaces are vector spaces whose origin is not specified. A vector space does not need an origin. When one writes: $\vec{v} = \pmatrix{v_x\\v_y\\v_z} = \pmatrix{1\\-4\\7}$ it only needs a basis, not an origin. Am I wrong, or what am I misunderstanding ?","['vector-spaces', 'coordinate-systems', 'linear-algebra', 'affine-geometry']"
694088,Scalar Autonomous Differential Equation?,"What precisely is a scalar autonomous differential equation? I'm confused about what this precisely means, more so because we did not discuss this in any lectures nor is it, as far as I can tell, defined in the notes. I've tried looking it up as well, but I couldn't find anything, only specific examples.
It occurs in the following question: Show that every scalar autonomous differential equation with $f\in\mathcal{C}^1$ is of gradient-type. Furthermore, show that the matrix of a linear gradient equation is symmetric. Here gradient-type means that for a (system of) autonomous differential equation(s) $\dot{y}=f(y)$, there exists $V:\mathbb{R}^n\supset\!\to\mathbb{R}$ such that $f=\operatorname{grad}V$ with $V\in\mathcal{C}^2$. Furthermore, what is a (linear) gradient equation? Do they simply mean a (linear) differential equation of gradient-type? The notes are not clear about it at all. Any help would be appreciated. Thank you!",['ordinary-differential-equations']
694147,Modular arithmetic - Suggestions to begin,"I've always wanted to start studying modular arithmetic to try to solve problems like: $$\text{find } n \in \mathbb{N} : 4n^2 \equiv 1 ~(\text{mod }{10^4})$$ I have a good basis in mathematical analysis and logic. Where can I start to study modular arithmetic? I prefer some internet tutorial, but also a book is good.
Thank you very much!","['modular-arithmetic', 'discrete-mathematics', 'reference-request', 'soft-question']"
694183,Prove $\nabla f$ is orthogonal to the surface $f$,"I'm trying to prove that $\nabla f$ is orthogonal to the surface $f$. I think I have a valid proof but I'm not sure that it is rigorous. To prove $\nabla f$ is normal  I am proving that $\nabla f\cdot u=0$ for any vector $u$ tangent to the surface $f$. Say $z=f(x,y)$ then $\nabla f$ $= \left(\begin{array}{c}
\delta_x\\
\delta_y\end{array}\right)$ and choosing any vector $v=(v_1,v_2)$ we need to construct the tangent vector to the surface. The tangent vector is $(v_1,v_2,f'_u$( x )). We achieve $f'_u$( x ) from $\nabla f\cdot u$ which gives us $f'_u$( x )$= v_1\delta_x + v_2\delta_y$. Therefore the tangent vector is $u=(v_1,v_2,v_1\delta_x+v_2\delta_y)$. Now we can dot $\nabla f$ with $u$. However now I came across the problem that we had one vector with 3 rows and one with 2 rows. Therefore I redefined $\nabla f$ and called a new fucntion $g(x,y,z)=f(x,y)-z$ at the level set $g(x,y,z)=0$. Now $\nabla g$ has three components and $$\nabla g=(\delta_x,\delta_y,-1)$$ I then took the dot product of $\nabla g$ and $u$ and this gave $$v_1\delta_x+v_2\delta_y-(v_1\delta_x+v_2\delta_y)=0$$ Therefore $\nabla g$ is orthogonal to the level set $g(x,y,z)=0$ and by equivalence $\nabla f$ is orthogonal to the surface $f(x,y)$. Now that is I'm guessing that's a long way round but would you class it as rigorous? Any suggestions to improve it? Is there any way to not have to use $g(x,y,z)$? Thanks","['multivariable-calculus', 'proof-verification']"
694205,About limits and successions,"This is a problem which I am not sure I solved correctly, mainly because there are some passages which are not very rigourous. Let $\{x_n\} $ be an increasing succession such that $x_n > 0$ and the
  tangent at the point $x_n$ of the function $y = \cos x$ passes through
  the origin. Let $c_n$ be the coefficient of the tangent line at the
  point $x_n$, calculate $$\lim_{n\to \infty} x_n - n\pi$$ and $$\lim _{n \to \infty} nc_{2n}$$ My attempt Well the first thing I did was finding that it must be $\tan x_n = -\frac{1}{x_n}$ So it seems easy to see (from the graphics) from this that $x_n \to \infty$ as $n \to \infty$ This implies than $\lim_{n \to \infty} \tan x_n = 0 \Rightarrow x_n \sim n\pi$ But this is not enough to prove the first limit as it is an insufficient approximation. So I'm going to say that since $\tan {x_n} \sim x_n - n\pi$, then $$x_n = -\frac{1}{\tan x_n} \sim -\frac{1}{x_n - n\pi}$$ Basically I'm identifying $x_n$ with its second order approximation. from the last equation one easily find $$x_n = \frac{n\pi + \sqrt{n^2\pi^2 - 4}}{2}$$ So $$\lim_{n \to \infty} x_n - n\pi = -\frac{2}{n \pi} = 0$$ For the second one, $$c_{2n} = -\sin(x_{2n}) \sim \sin(-\frac{2}{n \pi}) \sim -\frac{2}{n \pi}$$
So $$\lim _{x \to \infty} nc_{2n} = n \cdot -\frac{2}{n \pi} = -\frac{2}{\pi}$$","['sequences-and-series', 'calculus', 'geometry', 'limits']"
694216,"If H is a subgroup of G, then H has no more Sylow subgroups than G","If $H$ is a subgroup of the finite group $G$, then how do I show that $n_p(H) \leq n_p(G)$? Here $n_p(X)$ is the number of Sylow $p$-subgroups in the finite group $X$. Here is my attempt: Suppose the order of $G$ is $n$, the order of $H$ is $m$, then $m$ divides $n$ by Lagrange's theorem. By Sylow's counting theorem $n_p(H) = 1 \pmod p$ and $n_p(H)$ divides $m$, $n_p(G) =1 \pmod p$, $n_p(G)$ divides $n$. How do I continue after this?","['sylow-theory', 'finite-groups', 'group-theory', 'abstract-algebra']"
694220,Vector valued function in $\mathbb{R}^2$ with non-finite arc length,"Find a curve $\mathcal{C}$ : x = g (t) , $a \leq t \leq b$ in $\mathbb{R}^2$, where g $\in \mathcal{C}[a,b]$ such that $\mathcal{C}$ does NOT have finite arc length","['multivariable-calculus', 'vector-analysis']"
694223,Weakly convergent distributions on $\mathbb{R}$ with densities relating to Lebesgue-measure that do not converge,"Show that there exist weakly convergent distributions on $\mathbb{R}$ that have a density relating to the Lebesgue measure $\lambda$ but the densities do not converge. Hint: $f_n(x):=1+\cos(2\pi n x)$ To be honest I do not understand this task completely.
Nevertheless I tried to use the given hint. Maybe
$$
\nu_n(x):=f_n(x)\lambda(x)
$$
are those distributions on $\mathbb{R}$? At least $\nu_n\ll\lambda$ and both $\lambda$ and $\nu_n$ are $\sigma$-finite so that from Radon-NikodÃ½m it follows that there is $\lambda$-a.s. a density of $\nu_n$ relating to $\lambda$. But I do not know if this is meant and especially if the $\nu_n$ do weakly converge... Could you please help me? Sincerely yours math12","['weak-convergence', 'measure-theory']"
694227,What is the importance of the infinitesimal generator of Brownian motion?,"I have read that the infinitesimal generator of Brownian motion is $\frac{1}{2}\small\triangle$. Unfortunately, I have no background in semigroup theory, and the expositions of semigroup theory I have found lack any motivation or intuition. What is the infinitesimal generator of a process intuitively, and why is it interesting or useful to know that the generator of Brownian motion is $\frac{1}{2}\small\triangle$?","['probability-theory', 'stochastic-processes', 'markov-process', 'brownian-motion']"
694228,Inequality for norm of linear combination of linearly independent vectors,"I'm trying to find a proof for the following: Let {$u_{1},...,u_{n}$} be a linearly independent set of a normed space $X$. Then, there is a constant $c>0$ such that for every set of scalars $\{\alpha_{1},...,\alpha_{n}\}:$ $$\| \alpha_{1}u_{1}+...+\alpha_{n}u_{n} \| \ge c(|\alpha_{1}|+...|\alpha_{n}|)$$ I don't even know how to start.","['normed-spaces', 'linear-algebra', 'inequality']"
694235,Proof that the predictable sigma algebra is also generated by continuous and adapted processes,"I'm reading George Lowther's blog and have a question about the proof of lemma 2. We want to verify that the predictable sigma algebra is also generated by the continuous and adapted processes. One direction is clear, for the other the idea is to write a left continuous process $X$ as the limit of a ccontinuous process. For this purpose he defines $$Y_t^n:=n\int_{t-\frac{1}{n}}^t\mathbf1_{|X_{s\vee 0}|\le n}X_{s\vee 0} ds$$ I've proved the following statement (see this question ). For $f:\mathbb{R}\to\mathbb{R}$ Borel, bounded the map $t\mapsto \int_{t-h}^tf(s)ds$ is lipschitz continuous. Assuming additionally left continuity of $f$ we have $$\lim_{h\downarrow 0}\frac{1}{h}\int_{t-h}^tf(s)ds=f(t)\tag{1}$$ $(1)$ implies the continuity of $Y^n_t$ and the convergence of $Y_t^n\to X_t$. This leads to the following two question: George Lowther writes $Y^n\to X$, what does this mean? I wonder why left-continuity is important comparing with right continuity. We can write $$\lim_{h\downarrow 0}\frac{1}{h}\int_{t}^{t+h}f(s)ds=f(t)\tag{1'}$$ if we assume $f$ to be right continuous. Hence we would conclude that the optional sigma algebra, which is generated by the right continuous and adapted processes is also generated by the continuous and adapted processes implying that the optional and predictable sigma algebra coincide. Therefore, I made a mistake in the right continuous case, but I can't see where. It would be appreciated if someone could explain what goes wrong in the right continuous case.","['stochastic-processes', 'measure-theory']"
694249,is this proof that if $ B $ is infinite correct,"Given A is an infinite set and $A \subseteq B $ prove that B is infinite, Ok so there exists  $ f:A\to A\ $ that is one to one but not onto by definition of an infinite set. to prove that $B$ is infinite i have to prove that there exists a function $ g:B\to B\ $ that is one to one and not onto and define it somehow, so here is how i started, I noticed that since $A \subseteq B $ then $ f:A\to A\ $ can be expressed as $ f:A\to B\ $ then i defined $ h:B\to A\ $ as a one to one function and defined $ g:B\to B\ $ to be the the composition of $ f $ with $ h $ , then since $ h $ is one to one and $ f $ is one to one then $ g $ is one to one and since $ f $ is not onto then $ g $ is not onto, now since i have defined a function $ g:B\to B\ $ that is one to one and not onto then $ B $ is infinite.
Sorry if i have a bad proof writing style but im only 16 years old and im not that good at proofs",['elementary-set-theory']
694251,Artin exercise on free modules homomorphism,"2.3 Let $A$ be the matrix of a homomorphism $\varphi:\mathbb Z^n\to\mathbb Z^m$ of free $\mathbb Z$ -modules. (a) Prove that $\varphi$ is injective if and only if the rank of $A$ , as a real matrix, is $n$ . (b) Prove that $\varphi$ is surjective if and only if the greatest common divisor of the
determinants of the $m\times m$ minors of $A$ is $1$ . This is from Chapter 12 (1st Edition) of Artin. I believe it is Ch 14 in the 2nd Edition. I saw this question here earlier, but it appears to have been deleted. After coming across it in Artin, I tried it myself and am stuck, particularly on (b). I thought to extend $\varphi$ to a $\mathbb Q$ -homomorphism $\mathbb Q^n\to\mathbb Q^m$ and use Smith normal form, but I am having trouble making this work and not sure if it will. Does anyone have a direction that may be helpful? Also, is there a generalization of this exercise? For instance, a homomorphism $\varphi:R^n\to R^m$ with $R$ a Euclidean domain?","['modules', 'linear-algebra', 'abstract-algebra']"
694257,complement of a finite subset of a path-connected space is path-connected,"Given a smooth connected manifold $X$ of dim $\geq 2$, I need to show that $X\setminus Y$ is connected, for some $Y\subseteq X$ finite. The claim is intuitively obvious to me, but is not finding the right argument to prove. For smooth manifolds, I know that path-connectedness and connectedness are equivalent. To prove that $X\setminus Y$ is path-connected, I take $x,y\in X\setminus Y$. I now need to show that there exists a cont's path $\gamma:[0,1]\longrightarrow X\setminus Y$ such that $\gamma(0)=x, \gamma(1)=y$. Along the way, I need to use the finiteness of $Y$, and also the fact that $X$ is also path-connected. I don't know how? I haven't done fundamental group yet, so hint that does not involve it would be helpful.","['general-topology', 'manifolds']"
694269,Natural matrix norm of an inverse matrix,"Let $\left\|\cdot\right\| : \text{GL}(n,\mathbb{R})\to\mathbb{R}_{\ge 0}$ denote the natural matrix norm, i.e. $$\left\|A\right\|:=\max_{x\ne 0}\frac{\left\|Ax\right\|}{\left\|x\right\|}=\max_{\left\|x\right\|=1}\left\|Ax\right\|$$
is induced by a vector norm $\left\|\cdot\right\| : \mathbb{R}^n\to\mathbb{R}_{\ge 0}$. I want to show, that it holds $$\left\|A^{-1}\right\|=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1}$$ Proof : 
\begin{equation}
\begin{split}
\left\|A^{-1}\right\|&=\max_{\left\|x\right\|=1}\left\|A^{-1}x\right\|\\
&=\max_{\left\|Ay\right\|=1}\left\|y\right\|\\
&=\left(\min_{\left\|Ay\right\|=1}\left\|y\right\|^{-1}\right)^{-1}\\
&=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1}
\end{split}
\end{equation}
How does the last step work?","['matrices', 'normed-spaces', 'inverse', 'matrix-norms']"
694324,Find the range of values of $x$ which satisfies the inequality.,"Find the range of values of $x$ which satisfies the inequality $(2x+1)(3x-1)<14$. I have done more similar sums and I know how to solve it. I tried this one too but my answer doesn't matches the book's answer. I did in this way: Solving, i.e, After rearranging the equation and factorizing it, I get $(3x+5)(2x-3)<0$ Is this right? Anyways, then finding the range of values, I get: $x<-\frac{5}{3}$ or $ x>\frac{3}{2}$ But my book says the answer should be $-\frac{5}{3}<x<\frac{3}{2}$. Did I do any mistake?","['inequality', 'algebra-precalculus', 'functional-inequalities']"
694335,Is it possible to solve this equation by hand?,"I am working on a physics task, and reduced it to the following equation for $y$:
$$\frac{1}{4y^3}-\frac{2}{(y^2+b^2)^{\frac{3}{2}}}=0$$
I handed it to Mathematica, and it gave me two real solutions, $$y_{1,2} = \pm\frac{b}{\sqrt{3}},$$
along with some complex ones.
My question is, how can I see this? I mean, how can I solve such an equation by hand? I tried moving stuff around a bit, but all I found was a sextic equation, which I really didn't feel like approaching. Is there any easier way? Does it lower the hassle, if only real solutions are to be found?",['algebra-precalculus']
694353,Solve the following differential equation: $xy' - y = x^2$,"I'm preparing to exam in Linear Algebra $2$ and I have problems with differential equations.. For example, the following exercise: Solve the following differential equation: $xy' - y = x^2$ . I started to solve: $$xy' - y = x^2$$ $$ \implies y' - \frac{y}{x} = x$$ I need to find some $u$ and multiply both sides by it: $$uy' - \frac{u}{x}y = ux$$ I need somehow to satisfy the product rule of derivative, by finding $u$ such that $u' = -\frac{u}{x}$ and by this get: $(uy)' = uy' + u'y$ . I need the help to find $u$ . I need to find $u$ such that $u' = -\frac{u}{x}$ . How would you find $u$ ? thanks in advance.","['linear-algebra', 'calculus', 'ordinary-differential-equations', 'derivatives']"
694375,Find basis of the annihilator set,"$V$ $= \text{span}\{(1,2,3),(1,1,1)\}$ $\subseteq \mathbb{R}^3$. Find the vectors spanning $V^0$ in terms of the usual basis for $(\mathbb{R}^3)^*$. So we want linear functionals $f \in V^*$ such that $f(v)=0$ for every $f \in V$. We know that $v=\alpha_1 v_1 + \alpha_2 v_2$, so $0=f(v)=\alpha_1 f(v_1)+\alpha_2 f(v_2)$. But I cannot see where to go from here.#","['vector-spaces', 'linear-algebra', 'functions']"
694394,Volume with triple integrals,"Calculate integral $$\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV$$ Where $V\subset\mathbb{R}^3$ is the exterior of a origocentered sphere with radius of 2 \begin{align*}
V=&\iiint_V \frac{e^{-x^2-y^2-z^2}}{\sqrt{x^2+y^2+z^2}} dV \\
=& \int_0^{2\pi}\int_0^{\pi}\int_0^2 \frac{e^{-r^2}}{r}\rho^2\sin\phi dr d\phi d \theta \\
=& 2\pi \Biggl[-\cos\phi\Biggr]_0^{\pi} \int_0^2 re^{-r^2} dr \\
=& 2\pi (\underbrace{-\cos\pi}_{=1} +\underbrace{\cos0}_{=1}) \int_0^2 re^{-r^2} dr \\
=& 4\pi  \Biggl[\frac{-e^{-r^2}}{2}\Biggr]_0^2 \\
=& 2\pi  \left( -e^{-2^2}+1\right) \\
=& 2\pi -2\pi e^{-4}
\end{align*}","['multivariable-calculus', 'calculus']"
694411,How can I solve this definite integral ?,How can I solve this? I need help with the steps,"['definite-integrals', 'calculus', 'integration', 'derivatives']"
694416,Understanding group presentation as a quotient,"I'm just starting to learn a little group theory, so please forgive any ignorance I demonstrate in the following. I'm trying to understand the concept of a group being defined based on its presentation $G = \langle S \mid R \rangle$ as the quotient group of the free group $F_S$ generated by $S$, and the normal subgroup $R^{F_S}$ generated by $R$ in $F_S$. I understand the basic idea that the normal subgroup $R^{F_S}$, is gathering up all the elements of $F_S$ that are to be defined as equivalent to $1_G$ (the identity in group $G$). I also understand that dividing $F_S$ by $R^{F_S}$ (is ""dividing"" the correct term?) partitions $F_S$ and that each of these partitions will essentially end up as a different element of $G$, and further, since $R^{F_S}$ is a normal subgroup, the partitions can be turned into a group by inheriting the group operation of $F_S$. But I'm having trouble wrapping my head around exactly how the relators set $R$ partitions $F_S$. Not mechanically, that part I understand in terms of finding the cosets $gR^{F_S}$, but in a more intuitive sense, how do we tell how these relators define the structure of the group $G$? Taking the dihedral group $D_4$, for instance, presenting it as $\langle r, v | r^4 = v^2 = rfrf = 1 \rangle$, how do we understand from this that $fR^{F_S}$, and $r^3R^{F_S}$, and $rfrR^{F_S}$, will all produce their ""own"" cosets, instead of being equal to $R^{F_S}$, or equal to one another? My gut feel is that it has something to do with being ""less"" than the relators, or in some sense ""comprime"" to the relators, but I don't think I have the vocabulary to describe it correctly, and I may, for that matter, be completely off base.","['group-theory', 'group-presentation']"
694449,Evaluating $\int \frac{\sqrt{1-x^2}}{x^2} \operatorname d \! x$,"I am trying to find $$\int \dfrac{\sqrt{1-x^2}}{x^2} \operatorname d \! x$$ I can get to $${\int \dfrac{\sin x}{\cos^2x}\operatorname d \! x}$$ I tried u substitution by making $u=\cos x$ and $du= â\sin x dx$ but this leads to $$\int \dfrac{1}{u^2}\operatorname d \! u$$
and this is where I get stuck $$ \dfrac{1}{\cos x}.$$ The actual answer is $\int \frac{\sqrt{1-x^2}}{x^2}dx=-\frac{\sqrt{1-x^2}}{x}-\arcsin \left(x\right)+C$ From where does the arcsin(x) come from? Also the first term doesn't make sense if I'm dealing with 1/cosx ... :(","['trigonometry', 'integration', 'indefinite-integrals']"
694486,Show that the directional derivative is linear by definition,"If $f$ is differentiable at $x$, the map $h\mapsto f(x+h)-f(x)$ should be approximately linear. The scalar multiplicativity can be seen by noting that $$\lim_{h\to 0}\frac{f(x+ch)-f(x)}{h} = \lim_{h\to 0}c\frac{f(x+ch)-f(x)}{ch}= c\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\quad \text{and}\\ \lim_{t\to 0}\frac{f(x+ctv)-f(x)}{t}=\lim_{t\to 0}c\frac{f(x+ctv)-f(x)}{ct}=c\lim_{t\to 0}\frac{f(x+tv)-f(x)}{t},$$ where on the second line, $x$ and $v$ are vectors. The additivity of this map is less easy to see for me. How can we show it? That is, how to show that $$\lim_{t\to 0}\frac{f(x+t(v+w))-f(x)}{t} = \lim_{h\to 0}\frac{f(x+tv)-f(x)}{t}+\lim_{h\to 0}\frac{f(x+tw)-f(x)}{t}?$$","['calculus', 'derivatives']"
694487,expected value of this markov chain,"Question: A bag contains $3$ white chips and $3$ red chips. You repeatedly draw a chip at random from the bag. If it's white, you set it aside; if it's red, you put it back in the bag. After removing all $3$ white chips, you stop. What is the expected number of times you will draw from the bag? I believe this gives me the markov chain: [ $0.5$ , $0.5$ , $0$ , $0$ ] [ $0$ , $0.6$ , $0.4$ , $0$ ] [ $0$ , $0$ , $0.75$ , $0.25$ ] [ $0$ , $0$ , $0$ , $1$ ] But how am I supposed to find the expected number of times until getting to state $4$ ?","['markov-chains', 'expected-value', 'probability']"
694496,"Motivations for and connections between the topologies of Vietoris, Fell and Chabauty","My main interest is in the Chabauty topology on the space of closed subgroups of a locally compact topological group, merely out of curiosity. Wikipedia states ""it is an adaptation of the Fell topology construction, which itself derives from the Vietoris topology concept."" So, to understand the one, one could be served by understanding the other two. Unfortunately topology and functional analysis are not among my strong suits, so I am looking to grasp the moral purpose or design behind these topologies as well as see how they are connected to each other. Perhaps some motivation is necessary in order to see that the Chabauty topology is interesting in the first place. For $({\Bbb R},+)$ , the proper closed subgroups are the cyclic ones $\alpha\Bbb Z$ , $\alpha\in\Bbb R^{+}$ . The space of closed subgroups ${\rm Cha}(\Bbb R)$ is then a kind of moduli space of discrete lattices. (It seems people use the notation ${\cal C}(G)$ for this topological space, but I want to distinguish it from the various spaces of continuous maps or the group ${\rm C}^*$ -algebra.) The space ${\rm Cha}(\Bbb C)$ is homeomorphic to the $4$ -sphere, and the subspace comprised of lattices of $\Bbb C$ is homeomorphic to the product of an open interval with the complement of the trefoil knot in the $3$ -sphere. Questions . What is the main idea behind these three topologies respectively; what kind of information are they intended to capture? What do they say about the original space or topological group? In what way does the Fell topology derive from the Vietoris topology, and in what way does the Chabauty topology derive from the Fell topology? Some definitions, more for my sake than anyone else's. Let $X$ be a topological space. Denote by $K(X)$ the collection of compact subsets of $X$ . For open subsets $U\subseteq X$ define $U^+=\{ K\in K(X):K\cap U\ne\varnothing\}$ , $U^-=\{K\in K(X):K\subseteq U\}$ . Then the Vietoris topology on ${\cal P}(X)\setminus\{\varnothing\}$ has as subbase all the $U^+$ and $U^-$ as $U$ ranges over the open subsets of $U\subseteq X$ . A complex inner product space $H$ is a Hilbert space if it is closed with respect to the metric induced by the inner product. The norm of a linear operator between normed vector spaces is defined by $\|A\|:={\rm sup}\|Av\|/\|v\|$ over $v\ne0$ . A ${\rm C}^*$ -algebra is an algebra of continuous linear endomorphisms of a Hilbert space which is closed in the norm topology and closed under taking adjoints. A ${}^*$ -homomorphism $A\to B$ of ${\rm C}^*$ -algebras is an algebra homomorphism which is bounded as a linear operator and commutes with taking adjoints. The group ${\rm C}^*$ -algebra of a topological group $G$ is the completion of $C_c(G)$ [the algebra of $\Bbb C$ -valued continuous functions from $G$ with compact support] with respect to the norm $\|f\|_{{\rm C}^*}:=\sup_\pi\|\pi(f)\}$ where $\pi$ is taken over ${}^*$ -representations of $C_c(G)$ on Hilbert spaces. The Fell topology is the topology on the dual space of ${\rm C}^{*}(G)$ . Let ${\rm Cha}(G)$ denote the collection of closed subgroups of a locally compact topological group $G$ ; the Chabauty topology it is endowed with admits as neighborhood basis the sets $${\cal V}_{K,U}(C)=\{D\in{\rm Cha}(G):D\cap K\subset CU,~C\cap K\subset DU\} $$ as $C$ ranges over ${\rm Cha}(G)$ , $K$ ranges over compact subsets of $G$ and $U$ ranges over open neighborhoods of $G$ 's identity element.","['operator-algebras', 'topological-groups', 'functional-analysis']"
694508,Norm of a matrix and lower bound for its determinant,"Assume that $M$ is a positive constant, $A=[a_{ij}]$ is a matrix, and $\vert a_{ij}\vert \geq M $ for all $1\leq i,j \leq n$. Also, assume that $\det(A) \neq 0$ .Can we conclude that there exists a constant $C > 0$ such that $\det(A) \geq CM$?","['inequality', 'upper-lower-bounds', 'matrices', 'linear-algebra', 'determinant']"
694523,"How to prove $\int_1^\infty\frac{K(x)^2}x dx=\frac{i\,\pi^3}8$?","How can I prove the following identity?
$$\int_1^\infty\frac{K(x)^2}x dx\stackrel{\color{#B0B0B0}?}=\frac{i\,\pi^3}8,\tag1$$
where $K(x)$ is the complete elliptic integral of the 1Ë¢áµ kind :
$$K(x)={_2F_1}\left(\frac12,\frac12;\ 1;\ x^2\right)\cdot\frac\pi2.\tag2$$","['closed-form', 'special-functions', 'definite-integrals', 'elliptic-integrals', 'complex-analysis']"
694532,Proving Trig Identity,I'm trying to prove the following identity: $$\frac{\tan^2(x)+1}{\csc^2(x)} = \tan^2(x)$$ I tried this page but I couldn't make any sense out of their steps listed.,['trigonometry']
694548,Formal definition of a random variable,"The concept of random variable is not new to me and I know the measure theory. Anyway, I started reading the book ""Stochastic Differential Equation"" by B. Oksendal, and I'm having some problems in understanding. Given a probability space $(\Omega, \mathcal{F}, P)$ , $X : \Omega
 \rightarrow \mathbb{R}^n$ is a random variable if $X$ is a $\mathcal{F}$ -measurable function, or equivalently, if $$ X^{-1}(U) = \{\omega \in \Omega; X(w) \in U\} \in \mathcal{F} $$ for all the open set $U \subset \mathbb{R}^n$ . My main problem is that I always thought that $$X \in \Omega$$ while the book by B. Oksendal asserts that $X : \Omega \rightarrow \mathbb{R}^n$ and hence $X(\omega) \in \mathbb{R}^n$ for $\omega \in \Omega$ . For example, consider a fair dice. Then, $\Omega = \{1, 2, 3, 4, 5, 6\}$ .
One can be concerned to evaluate the probability that the dice result is even. In this case, we write: $$P(X \in \{2, 4, 6\}) = ...$$ In general, the probability of an event $U \subset \Omega$ ( $U \in \mathcal{F}$ ) is normally written as follows: $$P(X \in U)$$ This notation suggest me that $X \in \Omega$ . Again, I don't understand what means that $X : \Omega \rightarrow \mathbb{R}^n$ .","['probability-theory', 'measure-theory', 'random-variables']"
694549,Differentiability in $R^n$,"I have the definition of the derivative for $f:\mathbb R^n \rightarrow\mathbb  R^m$ at a point $a$ as: $f$ is differentiable at a then there exists a linear map $L:\mathbb  R^n \rightarrow\mathbb  R^m$ such that $\frac{f(a+h)-f(a)-L(h)}{||h||} \rightarrow 0$ as $h \rightarrow 0$ . (0, a, h are vectors in $\mathbb R^n$). I am trying to derive from this definition that the following holds: If we denote by $D(f(a))$ = total derivative of $f$ at point $a$ then it is true that if $f$ is differentiable at $a$: $||f(x) - f(a) -D(f(a))(x-a)|| < \epsilon ||x-a||$ for $x$ close to $a$. I know this seems (or it is) very easy but since the total derivatives is now a matrix of partial derviative I am not sure how to continue a proof. Extending it from $\mathbb R$ seems dangerous because I cannot say $D(f(a))(x-a) = L(x-a)$ or is this true?","['multivariable-calculus', 'partial-derivative', 'derivatives']"
694555,Why is the infinite union of cofinite sets necessarily cofinite with regard to the cofinite topology of the reals?,"The open sets of the cofinite topology on $\mathbb{R}$ are defined as all sets in $\mathbb{R}$ whose complement is finite.  Can someone point out the error in my logic with regards to the union of any open sets also being open? Let:
$$U = \bigcup_{i=1}^{\infty}\{U_i \mid \mathbb{R} - i\}$$
So for example, $U_1 = \mathbb{R} - 1$, $U_2 = \mathbb{R} - 2$, etc. Thus each $U_i \in U$ is cofinite and in the set of open sets. However, isn't the set $U$ not cofinite since $U = \mathbb{R} - \mathbb{N}$ meaning the complement of $U$ is $\mathbb{N}$ which is not finite?","['general-topology', 'elementary-set-theory']"
694592,How to handle rounding in uneven number bases?,"Is there any formal rule for handling rounding operations for the middle element in an uneven base? For example, in ternary {0,1,2}, what would I round a number ending in 1 to?",['number-theory']
694607,Automorphism and Inner automorphism,"What is an example of an automorphism of a group G that does not belong to Inn(G), the group of all inner automorphisms?",['group-theory']
694615,Liouville's Criterion and Integration of $\sin(z)/z$ using elementary functions,Liouville's Criterion: $\int fe^g$ is elementary iff there is rational function $q$ in $\mathbb{C}(z)$ such that $$f=q'+qg'$$ where $f\neq0$ and $g$ is a non constant function in $\mathbb{C}(z)$. Now using this fact: How should I proceed to prove that $\int \dfrac{\sin z}{z}dz$ cannot be written using elementary functions?,"['complex-integration', 'integration', 'elementary-functions']"
694619,Why has $\int \sin (\sin x) dx$ not been solved yet?,"I have Calculus 2 background, so please try to keep your answers around that level. I inly want a brief explanation. What is it about $\sin (\sin x)$ that makes it difficult to integrate? Also, what makes us think that we express $\int \sin (\sin x) dx$ in terms of already known functions? I imagine if we didn't think we can express it in known functions, then we would've already given it a name and defined it as a special function with its own name.","['special-functions', 'indefinite-integrals', 'calculus', 'soft-question']"
694636,$S_6$ contains a subgroup $H$ that is isomorphic to $S_5$ but not conjugate to $S_5$,"$$H = \langle (1 2 3 4), (3 4 5 6)\rangle $$ I'm kind of lost. I see that $\\S_5$ is embedded in $\\S_6$ in the form of a pentad.
Then the action of $H$ on the pentad induces a group homomorphism from $H$ into the permutations of the synthemes, which are isomorphic to $\\S_5$.
But I'm not sure how to show that the homomorphism is an isomorphism, and that is not conjugate in $\\S_5$ Let me know if I need to clarify anything.","['group-theory', 'abstract-algebra']"
694637,How can I prove the trigonometric Problem?,"How can I show  the following trigonometric problem : $$\frac{1}{3}\leq \frac{\sec^2\theta-\tan^2\theta}{\sec^2\theta+\tan^2    
    \theta}\leq 3$$ I have tried in the following way : $$
\begin{align}
& \phantom{\Rightarrow}
\frac{\sec^2\theta-\tan^2\theta}{\sec^2\theta+\tan^2
    \theta}-\frac{1}{3} \\[8pt]
& \Rightarrow  \frac{1}{\sec^2\theta+\tan^2
    \theta}-\frac{1}{3} \\[8pt]
& \Rightarrow  \frac{\cos^2\theta}{1+\sin^2
    \theta}-\frac{1}{3} \\[8pt]
& \Rightarrow  \frac{1-\sin^2\theta}{1+\sin^2
    \theta}-\frac{1}{3} \\[8pt]
& \Rightarrow  \frac{2-4\sin^2\theta}{3(1+\sin^2
    \theta)} \\[8pt]
& \Rightarrow  \frac{2\cos 2\theta}{3(1+\sin^2
    \theta)}
\end{align}
$$ How can I show $ \dfrac{2\cos 2\theta}{3(1+\sin^2    
    \theta)}\geq 0$ ?","['trigonometry', 'inequality']"
694640,Surface all of whose normals intersect at a point,"I am new to differential geometry and encountered difficulty when trying to solve the following problem from Dubrovin's Modern Geometry It's the first problem in exercise 8.4: Find the surface all of whose normals intersect at a point. Intuitively I believe that the surface should be a sphere, but I am not sure how to show this formally. Can some one please help me out? Thanks a lot.",['differential-geometry']
694697,Infinitely differentiable function with divergent Taylor series?,"I'd greatly appreciate it if someone could provide examples of the following: 1) A infinitely differentiable function whose Taylor series does not converge to the function. 2) An infinitely differentiable function whose Taylor series diverges. My differential equations text says that ""most"" smooth functions are of one of these types. What can be done if we weaken the infinitely differentiable condition? (Please do not use $f(x) = e^{-1/x^2}$ as an example!)","['convergence-divergence', 'real-analysis', 'taylor-expansion']"
694699,Infinite series for $ \sqrt 2 $,"What is infinite series for $ \sqrt 2 $? I don't mean continued fraction. That kind of series such as like for $e, \pi, $etc.",['sequences-and-series']
694701,"Normal variables, uniform integrability and limit of a martingale","I'm stuck in the following problem! Would be great if you can help. Suppose you have $(Y_n)_{n\in \mathbb N}$, a sequence of independent and identically distributed random variables, $Y_1 \sim \mathcal N (0,\sigma^2)$. Define $(\mathcal F_n)_{n\in \mathbb N}$ as the natural filtration, $\mathcal F_n = \sigma(Y_1,...,Y_n)$. We define now two processes: $X_n = Y_1 + ... + Y_n$ and $Z_n^u = \exp\left(\displaystyle uX_n - \frac{nu^2\sigma^2}2 \right)$. Since $(Z_n^u)$ is a martingale bounded in $\mathcal L^1$ (i.e. $\sup_n E(|Z_n^u|) < \infty$), converges almost surely to a random variable $Z_\infty^u \in \mathcal L^1$ $-$for every $u\in \mathbb R$$-$ in virtue of the Doob's supermartingale convergence theorem. My main question is: how one can find the limit? For which values of $u$ is true that $Z_n^u = \mathbb E (Z_\infty^u | \mathcal F_n)$? A known theorem says that $Z_n^u = \mathbb E (Z_\infty^u | \mathcal F_n)$ is true when $Z_n^u \to Z_\infty^u$ in $L^1$, or the sequence $(Z_n)$ is uniformly integrable, but I don't know how to show neither of these conditions.","['probability-theory', 'martingales', 'convergence-divergence']"
694713,How prove for any $k$ then have $a^2_{1}+a^2_{2}+a^2_{3}+\cdots+a^2_{k}=m^3$,"for any positive integer $k$,there exsit $m\in N$ and $a_{i}\in N,i=1,2,\cdots,k$,such (1): $a_{i}\neq a_{j},i\forall i\neq j$, (2): 
$$a^2_{1}+a^2_{2}+a^2_{3}+\cdots+a^2_{k}=m^3$$ My idea: if $k=1$,then we let $a_{1}=1$,then $$a^2_{1}=1^3=m^3$$
if
$k=2$, then we let $a_{1}=5,a_{2}=10$,then we have
$$a^2_{1}+a^2_{2}=5^2+10^2=125=5^3=m^3$$
if $k=3$, note
$$3^2+4^2+10^2=125=5^3$$
then let $a_{1}=3,a_{2}=4,a_{3}=10,m=5$ But follow I can't find it and How prove this problem ,Thank you",['number-theory']
694736,Weak convergence in a Hilbert Space,What does it mean for a sequence $\{f_n\}_{n=1}^\infty\subseteq H$ to converge weakly? I know it means that it converges in the weak topology and I've read a few definitions of weak topology which all seemed quite confusing and it seems like I'm missing something important here.,"['weak-convergence', 'hilbert-spaces', 'functional-analysis']"
694741,what the central limit theorem says,"Asked what the central limit theorem says, a student replies, ""as you take larger
and larger samples from a population, the histogram of the sample values looks more and
more Normal"". Is the student right? Explain your answer. My answer is no, the student is wrong. My explanation is the histogram of the sample values will look like the population distribution, whatever it might happen to be. The central limit theorem says that the histogram of sample means (from many large samples) will look more and more Normal. Am I right about it? It is that simple? Is there anything more I can say about this?","['statistics', 'probability-distributions', 'probability', 'probability-theory']"
694745,Proving or disproving if sets are equal,"Does this mean we can ignore A because its a subset of B, so now were only dealing with B or C? So you substitute B or C with A. Am I right because that gives me the right answer. 
(C-C) and empty set = empty set
or 
(B-b) and empty set = empty set.
Please help, I made a reasonable attempt on this problem",['discrete-mathematics']
694756,Serre Fibration long exact sequence,"I want to know if there exists something like the long exact sequence 
on the following case: Let $p : E\rightarrow B$ a continuous surjective map such that 
there exists an open dense subset $U$ of $B$ such that $p$ its a trivial bundle over $p^{-1}(U)$ (i mean that $p^{-1}(U)\cong U \times T$ for some topological space $T$ and $p$ acts as the projection on the first coordinate) . For some $b\in U$ there exists something like the Serre long exact sequence
associated to the maps $p^{-1} ( b ) \rightarrow E \rightarrow B$. In particullary interested on the case when $E,B$ and $T$ are algebraic varieties and $P$ is a quotient for the action of a reductive algebraic group on $E$.","['algebraic-geometry', 'algebraic-topology']"
694771,how should I show that it is wedge of infinite circles?,"I know that the shape that we see it below is homotopy equivalent of wedge of infinite circles,so the fundamental group of it is $\prod _{1}(\vee _{\alpha \in A}S^{1})=\ast _{\alpha \in A }\mathbb{Z}$,but I don't know how should I show that it is  wedge of infinite circles,also I can't imagine what is happening for this shape.please help me with your knowledge,thanks.","['general-topology', 'homotopy-theory', 'algebraic-topology', 'fundamental-groups']"
694783,Orthogonal idempotents from disjoint union in $\text{Spec}(A)$,"Let $A$ be a commutative ring with unity.  Suppose that $X=\text{Spec}(A)$ is a disjoint union $X_1\cup X_2$ of topological spaces.  Show that $A$ has a pair of orthogonal idempotents $e_1,e_2$ (i.e. $e_1+e_2=1$ and $e_1e_2=0$) such that $X_1=\operatorname{Spec}(e_1A)$ and $X_2=\operatorname{Spec}(e_2A)$. I think it should be easy but I don't know how to proceed with unspecified topology on $\text{Spec}(A)$.  Do I use a mapping somehow from $A$ to $X$? I see this post If SpecA is not connected then there is a nontrivial idempotent but don't think is an ""exact duplicate."" At least, the wording is different enough so I had found it before and did not see it was a duplicate. (And I still don't think so.)","['commutative-algebra', 'ring-theory', 'algebraic-geometry']"
694792,How to simplify $\cot(\sec^{-1}(e^x))$,"I've been trying to simplify $\cot(\sec^{-1}(e^x))$. I thought substitution might the way to go about it so I said: let $u = \sec^{-1}({e^x})$ I'm therefore trying to find $\cot(u)$ From $u = \sec^{-1}({e^x})$ I can say: $\sec(u) = e^x$ $\therefore \cos(u) = \dfrac{1}{e^x} $ $\therefore \cot(u) = \dfrac{1}{e^x\sin(x)}$ However, I want my answer just in terms on exponentials and I'm not too sure how to go about it after this point? Thank you :)","['trigonometry', 'exponential-function']"
694828,"How to show whether a $3\times 4$ matrix has no solution, a unique solution or infinitely many solutions?","The system is : $$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
-2 & 5 & -4 & -1 & | & b \\
1 & -10 & 22 & 8 & | & c
\end{matrix}
$$ After Gaussian elimination, I found that $$
\begin{array}{cccc|cc}
1 & -4 & 6 & a &  & 0 \\
0 & 1 & -\tfrac{8}{3} & - \left( 2a- \tfrac{1}{3} \right) & & - \tfrac{1}{3}b \\
0 & 0 & 0 & 10-5a & & c-2b
\end{array}
$$ Is it correct and I can continue to determine whether there is no solution, a unique solution or infinitely many solutions? Here are the operations: $$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
-2 & 5 & -4 & -1 & | & b \\
1 & -10 & 22 & 8 & | & c
\end{matrix}
$$ $R_2+2R_1\rightarrow R_2$
$$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
0 & -3 & 8 & 2a-1 & | & b \\
1 & -10 & 22 & 8 & | & c
\end{matrix}
$$ $R_3-R_1\rightarrow R_3$
$$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
0 & -3 & 8 & 2a-1 & | & b \\
0 & -6 & 16 & 8-a & | & c
\end{matrix}
$$ $R_3-2R_2\rightarrow R_3$
$$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
0 & -3 & 8 & 2a-1 & | & b \\
0 & 0 & 0 & 10-5a & | & c-2b
\end{matrix}
$$ $-\frac 13(R_2)\rightarrow R_2$
$$
\begin{matrix}
1 & -4 & 6 & a & | & 0 \\
0 & 1 & -\frac 83 & -\tfrac{2a-1}{3} & | & -\frac 13b \\
0 & 0 & 0 & 10-5a & | & c-2b
\end{matrix}
$$","['matrices', 'linear-algebra', 'gaussian-elimination']"
694836,Finite groups with all maximal subgroups of prime power index are solvable?,"It is well known that in finite solvable groups, any maximal subgroup has prime power index. Question: If $G$ is a finite group in which any maximal subgroup has prime power index, is $G$ solvable? Up to groups of order $<180$, the answer to the question seems to be yes.","['solvable-groups', 'finite-groups', 'group-theory', 'abstract-algebra']"
694837,Shrinking Lemma for Arbitrary Open Covers of Normal Spaces,"I was reading Munkres' Topology book and I came across this Shrinking Lemma:  If {$U_1, ..., U_n$} is an open cover of a normal space $X$, then there is an open cover {$V_1, ..., V_n$} such that the closure $\overline{V_i} \subset U_i$ for each $i = 1,...,n$.  The proof goes like this: $A = X - (U_2\cup ...\cup U_n)$ is closed and since {$U_i$} covers $X$, $A \subset U_1$.  Since $X$ is normal, there is an open set $V_1$ such that $A \subset V_1$ and $\overline{V_1} \subset U_1$.  The collection {$V_1, U_2, ..., U_n$} covers X.  In general, given open sets $V_1, ..., V_{k-1}$ such that {$V_1, ...,V_{k-1}, U_k, U_{k+1},...,U_n$} covers $X$, let $A = X - (V_1\cup ...\cup V_{k-1}) - (U_{k+1}\cup ...\cup U_n)$.  $A$ is closed so there exists an open set $V_k$ such that $A \subset V_k$ and $\overline{V_k} \subset U_k$.  {$V_1, ...,V_k, U_{k+1},...,U_n$} is an open cover of $X$.  At the $n$th step of the induction, our result is proved. This obviously generalizes to a countably infinite open cover.  What about an uncountable open cover (of nonempty sets)?  Does the proof below work? Suppose $X$ is a normal space and {$U_\alpha$}$_{\alpha \in J}$ is an open cover of $X$ where $J$ is an uncountable index set and each $U_\alpha$ is nonempty.  There is an order relation on $J$ that is a well-ordering by the well-ordering theorem.  Let $\alpha_0$ be the smallest element of $J$.  The set $A = X - \bigcup\limits_{\alpha \in J - \{\alpha_0\}} U_\alpha$ is closed and is a subset of $U_{\alpha_0}$ so there is an open set $V_{\alpha_0}$ such that $A \subset V_{\alpha_0} \subset \overline{V_{\alpha_0}} \subset U_{\alpha_0}$.  Then {$U_\alpha\space\vert\space\alpha \in J - \{\alpha_0\}$}$\cup${$V_{\alpha_0}$} is an open cover of $X$.  Let $\mathcal{C}$ = {open sets $V\space\vert\space\overline{V} \subset U_\alpha$ for some $\alpha \in J$} and $\mathcal{F}$ be the set of functions mapping a section of $J$ into $\mathcal{C}$. For all $\alpha \in J$, let $B_\alpha = \{V \in \mathcal{C}\space\vert\space \overline{V} \subset U_\alpha$ and $X - \bigcup\limits_{\lambda < \alpha}f(\lambda) - \bigcup\limits_{\lambda > \alpha}U_\lambda \subset V$ for some $f \in \mathcal{F}$ with domain $S_\alpha$}.  If it's unclear, a section of $J$ is just $S_\alpha =$ {$\lambda \in J\space \vert\space \lambda < \alpha$} for some $\alpha \in J$.  Let $\mathcal{B}$ = {$B_\alpha\space\vert\space\alpha \in J$}.  By the axiom of choice, there is a choice function $c : \mathcal{B} \longrightarrow \bigcup\limits_{B \in \mathcal{B}}B = \mathcal{C}$ such that $c(B) \in B\space\forall\space B \in \mathcal{B}$.  Define $\rho : \mathcal{F} \longrightarrow \mathcal{C}$ by $\rho(f) = c(B_\alpha)$ where $S_\alpha$ is the domain of $f$.  Then by the general principle of recursive definition, there is a unique function $h : J \longrightarrow\mathcal{C}$ such that $h(\alpha) = \rho(h\vert S_\alpha)$.  So for each $\alpha \in J$, we have an open set $V_\alpha = h(\alpha) = \rho(h\vert S_\alpha) = c(B_\alpha)$ such that $\overline{V_\alpha} \subset U_\alpha$ and $X - \bigcup\limits_{\lambda < \alpha}h(\lambda) - \bigcup\limits_{\lambda > \alpha}U_\lambda \subset V_\alpha$.  The set {$V_\alpha$} is an open cover of $X$.",['general-topology']
694842,Constructing a triangle given 2 sides and the inscribed circle's radius [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question A Triangle has 2 sides of length 4 and 9. The largest circle than can be drawn in the triangle so as to touch all 3 sides has radius 1 cm. Find the length of the 3rd side.","['geometry', 'trigonometry']"
694853,Different methods of evaluating $\int\sqrt{a^2-x^2}dx$:,"Is there a simple and nice way to solve $\int\sqrt{a^2-x^2}dx$: PS:I am not looking for a substitution like $x=a\sin p$,",['integration']
694868,Concentration of measure vs large deviation,"When reading some probability publications I am always not sure why they call this or that inequality a 'concentration inequality' or 'large deviation inequality'. For me these (concentration of measure and large deviation theory) just describe the same phenomenon. So I ask, is there a formal difference between concentration and large deviation inequalities? Or are these really different concepts?","['probability-theory', 'measure-theory', 'large-deviation-theory', 'concentration-of-measure']"
694872,Why is not the answer to all probability questions 1/2.,"Ok, I know this is wrong, but I want someone to tell me why. Let's take a normal heads tails example of a fair coin. The probability of getting head = 1/2. And I write this is because, either it will be heads, or not . Hence two cases that makes it 1/2. Now, I know I can't do this to all the cases. For example, the probability of getting a 2 when I roll a dice. I know the answer is 1/6 but why can't I do, either the outcome will be 2 or not . And in that case, my probability is 1/2.",['probability']
694912,Two non isomorphic algebraic varieties,"Consider the two subsets: $X_1=\mathbb P_1(\mathbb C)\setminus\{0,1,\infty, e \}$ and $X_2=\mathbb P_1(\mathbb C)\setminus\{0,1,\infty, \pi \}$. They are two varieties in the sense of the first chapter of Hartshorne (I'd like to avoid the scheme theory here), but I don't understand why they are not isomorphic. I need the simplest reason that explain the absence of an isomorphism, so a motivation that involves less theory possible. Thanks in advance.",['algebraic-geometry']
694914,"How to obtain a diffeomorphism between $\mathrm{SL}(2,\mathbb{R})$ and $ (\mathbb{C}\!\smallsetminus\!\{0\})\times\mathbb R$?","Could someone please give me a tip on how to show that the map $\mathrm{SL}(2,\mathbb{R}) \to (\mathbb{C}\setminus\{0\})\times\mathbb{R}$ \begin{equation}\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\to(a+i c,ab+cd)\end{equation} 
is a diffeomorphism? Thanks.","['multivariable-calculus', 'differential-geometry', 'manifolds', 'lie-groups', 'real-analysis']"
694928,"Logarithms, prove this limit.","Mathematica knows that: $$\log (n)=\lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$$ Kind of tautological starting with logarithms, but I would like to know better why this limit works: $${\Large \log (n)=\lim_{s\to 1+\frac{2 i \pi  k}{\log (n)}} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s-i \Im(s))}$$ for $k$ an integer. Solving it symbolically for some integer $k$ while leaving $n$ as a variable Mathematica says it is equal to zero. But setting $n$ to any value I get $\log (n)$. Mathematica: Table[Limit[Zeta[s - I*Im[s]]* Total[{1 - 1/(2)^(s - 1)}], 
  s -> (1 + (2*I \[Pi]*k/Log[2]))], {k, 1, 12}]
N[%, 12] and: Table[Table[
  Limit[Zeta[s - I*Im[s]]*Total[{1 - 1/(n)^(s - 1)}], 
   s -> (1 + (2*I \[Pi]*k/Log[n]))], {k, 1, 6}], {n, 1, 12}]
 N[%, 12]","['logarithms', 'riemann-zeta', 'limits']"
694949,Proving the Obvious,"Is it normal that I have the hardest time when I'm trying to prove statements that are blatantly obvious on a visual and/or intuitive level? For instance, how does one go about formally proving the following statement? Given a set $P$ of points on the real plane that are not all collinear, prove that there is a subset of $P$ that corresponds to the convex hull of $P$. Furthermore, that this polygon is unique (up to collinear points). An intuitive 'proof' would be ""Stretch a rubber band such that it contains all the points, and release it."" This, at least to me, makes it obvious that the above statement is true, but of course it's not very rigorous.","['geometry', 'convex-analysis', 'self-learning', 'proof-writing']"
694953,Do you need to find the domain of a function to prove injectivity?,"I teach math and have had this debate with a fellow teacher. I believe the domain of a function is really not important to determine if a function is or is not injective if there is no ""real life"" context. What do you think? would you find the domain of a function like $f(x)=1-\ln(2-e^{2x})$ to show if the function is injective? If so, why?","['proof-writing', 'functions']"
694962,What is (fundamentally) a coordinate system ?,"Consider the following construction of vectors and points. Let's start with a vector space , or more specifically a coordinate space $F^N$ over a field $F$ and of $N$ dimensions. The elements of this vector space are vectors $\vec{v}$ and we can express their coordinates using a basis $(\vec{e_1}, \vec{e_2}, ... \vec{e_N})$ of the vector space (and any coordinate space comes with a standard basis ). Using this vector space, one can construct the related affine space $A$, whose elements are points. As there is no origin in an affine space, to locate a point one need a coordinate system which can be specified by an origin and a basis. (First question: does this construction make sense ?) My problem, is that I am not sure to understand what is fundamentally a coordinate system... My problem is the following: in 3D cartesian coordinates , it is pretty simple: the vector space is $\mathbb{R}^{3}$, so we can construct the related affine space with points, and by specifying an origin $O$ plus the standard basis, we can identify the position of the point $P$ by the coordinates of the vector going from $O$ to $P$. But with spherical coordinates I am lost and I am not sure to really understand what mathematical objects is/are $(r, \theta, \phi)$ and $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$: in the contrary of cartesian coordinates, the basis $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ will change from one point to another. Why is that ? Is the vector space underlying $(r, \theta, \phi)$ and linked with $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ the same as the one we use in the case of cartesian coordinates. I think I am missing a basic thing...","['vector-spaces', 'geometry', 'coordinate-systems', '3d', 'linear-algebra']"
694982,"If both the union and the power set of a set are transitive, does the set have to transitive?","If $x$ is a set, and $\mathcal{P}x$ and $\cup x$ are both transitive sets, does $x$ necessarily have to be transitive?",['elementary-set-theory']
694992,Is this Frechet derivative correct?,"Problem statement: Let $u \in L^2[0, 1]$ and $$J(u) = \int_0^1 u(t) u(1-t)dt$$ Find $J'(u)$ and $J''(u)$. Attempted solution: First derivative There is a hint that the derivative looks like this: $$J(u + h) - J(u) = \langle J'(u), h\rangle + o(\|h\|)$$ I do not really understand why, since this is not the definition of the Frechet derivative. However: $$
\begin{split}
J(u + h) - J(u) &= \int_0^1\left(u\left(t\right) + h\left(t\right)\right)\left(u\left(1 - t\right) + h\left(1 - t\right)\right)dt - \int_0^1u\left(t\right)u\left(1 - t\right)dt \\
&=\int_0^1 h(t)(u(1 - t) + h(1 - t))dt + \int_0^1u(t) h(1 - t)dt \\
&= \int_0^1h(t) h(1 - t)dt + \int_0^1h(t)u(1 - t)dt + \int_0^1u(t)h(1 - t)dt 
\end{split}
$$ 
Where it can be shown that the last two integrals are actually equal. Which results in: $$J(u + h) - J(u) = \int_0^1 h(t) h(1 - t)dt + 2 \int_0^1 u(1 - t) h(t) dt$$ If we now recall the definition of scalar product in $L^2[0, 1]$: $\langle f, g \rangle = \int_0^1 f(t) g(t) dt$ then we can rewrite our result as: $$J(u + h) - J(u) = \langle 2 u(1 - t), h(t)\rangle + \langle h(t), h(1 - t)\rangle$$ Let's now show $\langle h(t), h(1 - t)\rangle = o(\|h\|)$:
$$\lim_{h \to 0} \frac{|\langle h(t), h(1 - t)\rangle|}{\|h(t)\|} \le \lim_{h \to 0} \frac{\|h(t)\| \|h(1 - t)\|}{\|h(t)\|} = \lim_{h \to 0} \|h(1 - t)\| = 0$$ Where the inequality is the Cauchy-Bunyakovsky-Schwarz inequality. Second derivative There are no hints this time but here is the definition from class (B is a matrix): $$ J'(u + p) - J'(u) = Bp + o(\|p\|)$$ As far as I understand, $J''(u) = B$ at point p. Which leaves me with: $$J'(u + p) - J'(u) = 2(u(1 - t) + p(1 - t)) - 2 u(1 - t) = 2p(1 - t) = o(\|p\|)$$ Which means $J''(u) = B = 0$. My current answer to the problem: $J'(u) = 2 u(1 - t)$ $J''(u) = 0$ My questions: Is my answer correct, are there any mistakes? Why does the first hint look like this? There are no scalar products in the definition of a Frechet derivative...","['optimization', 'derivatives']"
695004,"Prove, that two equations are equivalent","EDIT: Missed something very important! Sorry! We have $x^4+1=2(2x-1)^{1/4}$ not $x^4+1=2\sqrt{2x-1}$. One friend of mine told me that the equation $x^4+1=2(2x-1)^{1/4}$, where $x\geq \frac{1}{2}$
is equivalent to $$x^4+1=2x$$. How did he obtain this? The equation $x^4+1=2(2x-1)^{1/4}$ is equivalent to $\frac{x^4+1}{2}=\sqrt{2x-1}$ and we can consider two functions: One is        $f:[\frac{1}{2},\infty)\rightarrow[0,\infty)$, $f(x)=(2x-1)^{1/4}$ and                       $g:[0,\infty)\rightarrow [\frac{1}{2},\infty), g(x)=\frac{x^4+1}{2}$. $ g $is the inverse of$ f$ so they can only intersect on the line $y=x$. Then we can consider the equation $x^4+1=2x$
Is that enough?","['functions', 'polynomials']"
695017,Finding MLE of $f(x;\theta) =1$ if $\theta-1/2<x< \theta+1/2$,"Let $X_1,...,X_n$ have density: $$f(x;\theta) = \begin{cases} 1 & \text{if } \theta-1/2<x< \theta+1/2 \\ 
0 & \text{otherwise} \end{cases}$$ Let $Y_1=\min \lbrace X_1,\ldots,X_n \rbrace$ and $Y_n=\max \lbrace X_1,\ldots,X_n\rbrace$ Show that any statistic $u(X_1,\ldots,X_n)$ that satisfies $Y_n-1/2<u<Y_1+1/2$ is a maximum likelihood estimate of $\theta$. My attempt: First rewrite the density: $$f(x;\theta) = \begin{cases} 1 &\text{if } -1/2<x- \theta<1/2 \\ 
0 & \text{otherwise} \end{cases}$$ Okay, so we obviously need to use the fact that $Y_n-1/2<u<Y_1+1/2$, I started by using the regular MLE finding method: $$L(\theta)=\prod_i(x-\theta)$$ $$\log L(\theta)=\log\prod_i(x_i-\theta)=\sum_i \log(x_i- \theta)$$ $$(\log L(\theta))'=\sum_i \frac 1 {x_i- \theta} =0$$ And I'm stuck here..I think that I need to use the given here but I'm not sure how to proceed. Any help would really be appreciated:) Thanks!","['statistics', 'maximum-likelihood', 'statistical-inference']"
695020,Expected value of the distance between 2 uniformly distributed points on circle,"I have the following problem (related to Bertrand ): Given a circle of radius $a=1$. Choose 2 points randomly on the circle circumference.
Then connect these points using a line with length $b$.   What is the expected length of this line? ($\mathbb{E}[b]$=..?) I have tried this: $x_i=\cos(\theta_i), y_i=\sin(\theta_i), \quad i=1,2$, where $\theta_i$ is uniformly distributed on $[0,2\pi]$ Then I tried to compute the squared distance. The squared distance between two points in the Eucledian space is: $$d^2=(\cos(\theta_1)-\cos(\theta_2))^2+(\sin(\theta_1)-\sin(\theta_2))^2 $$ Now taking expectations I got: $$E(d^2)=2-2 \ ( \ E(\cos(\theta_1)\cos(\theta_2) + E(\sin(\theta_1)\sin(\theta_2) \ )$$ (as $E(\cos^2(\theta_i))=E(\sin^2(\theta_j))$ Then $$E(\cos(\theta_1)\cos(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi}\theta_1 \theta_2\cos^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \cos^2(\frac{1}{2\pi})$$ and $$E(\sin(\theta_1)\sin(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi} \theta_1 \theta_2\sin^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \sin^2(\frac{1}{2\pi})$$ so that $$d^2=2-4 \pi^2 \left(\cos^2(\frac{1}{2 \pi}) + \sin^2(\frac{1}{2\pi})\right)=2-4 \pi^2$$ But that doesn't make sense since it is negative. Any help would be appreciated","['paradoxes', 'probability-theory', 'geometric-probability', 'euclidean-geometry', 'probability']"
695033,"Understanding the setup for the probability that $Ax^2+Bx+C$ has real roots if A, B, and C are random variables uniformly distributed over (0,1).","Suppose that $A, B,$ and $C$ are independent random variables, each being uniformly distributed over $(0,1)$ . What is the probability that $Ax^2 + Bx + C$ has real roots? First, I set $P(B^2 - 4AC \ge 0)$ Then I am told that $$\begin{align}
\int_0^1 \int_0^1 \int_{\min\{1, \sqrt{4ac}\}}^1 1 \;\text{d}b\,\text{d}c\,\text{d}
&a= \int_0^1 \int_0^{\min(1, \frac{1}{4a})}\int_{\sqrt{4ac}}^1  1\;\text{d}b\,\text{d}c\,\text{d}a\\
&= \int_0^{\frac{1}{4}} \int_0^1 \int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a + \int_{\frac{1}{4}}^1 \int_0^{\frac{1}{4a}}\int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a
\end{align}$$ why the middle integrate from $0$ to $\min(1, \frac{1}{4a})$ from the second integral...where does $\frac{1}{4a}$ come from? why the min{...} does not go to the front integral? why they break up into last step like this (I refer to one integral + another integral) ? Thanks a lot","['probability-theory', 'multivariable-calculus', 'probability-distributions', 'probability']"
695041,Upper and Lower one sided confidence level,"Iam trying to calculate upper and lower confidence levels for a parameter, but i can't get it straight (in this case $\sigma^2$): the reference variable: $R_{\sigma^2} := \frac{n-1s^2}{\sigma^2} \sim \chi^2(n-1)$ where $s^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n} (x_i-\bar{x})^2 $ now for the confidence interval we get something like this $1-\alpha = P\big(\chi^2_{1-\alpha/2}(n-1) < R_{\sigma^2} < \chi^2_{\alpha/2}(n-1) \big)= P\big( \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)} \big) < \sigma^2 <  \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}\big)$ ok that was the context. now i want to find the one-sided upper and lower confidence level and Iam thinking something like this: (for the one sided upper confidence level) $ \alpha=  P\big(R_{\sigma^2}  > \chi^2_{\alpha}(n-1)\big) = P\big(\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)}\  > \sigma^2 \big)  $ so the upper confidence level is given by $\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)} = \bar{\sigma^2}$ but in my book it says $\frac{(n-1)s^2}{ \chi^2_{\textbf{1-$\alpha$} }(n-1)} = \bar{\sigma^2}$ 
Iam a big confused about this can someone help me to understand (observe  $\chi^2_{1-\alpha}(n-1)$)","['statistics', 'statistical-inference']"
695081,"Grassmannian, Plucker coordinates",In which books can I find something about the grassmannian and the plucker coordinates ?,"['grassmannian', 'differential-geometry', 'schubert-calculus', 'projective-geometry', 'reference-request']"
695093,Using QR algorithm to compute the SVD of a matrix,How to use the QR algorithm to compute the SVD of a matrix $X\in R^{m\times n}$? Is there any algorithm for doing that?,"['matrices', 'eigenvalues-eigenvectors']"
695132,What is meant with unique smallest/largest topology?,"I'm doing this exercise: Let $\{T_\alpha\}$ be a family of topologies on $X$. Show that there
  is a unique smallest topology on $X$ containing all the collections
  $T_\alpha$, and a unique largest topology contained in all $T_\alpha$. I have proved everything except the unique part.. I just can't get my head around what is meant with unique here. Which may sounds silly. I have proved that the intersection is a topology. And if you are a topology that is also contained in every $T_\alpha$, than you surely are contained in the intersection, so you are not larger. But I don't see from what it follows that this intersection is the unique largest topology contained in all $T_\alpha$. One part of my head say it is trivial, the other part gets confused. Like it is redundant to talk about unique in this context. The same for proving the uniqueness of the smallest topology. Edit Should I read topology $A$ larger than topology $B$ as, $A$ has more elements than $B$ ? I thought that, because the author uses the word finer for $A \supset B$.",['general-topology']
695154,Differentiability of a multivariable function,"I want to study differentiability of $f$ at the origin: $$ f(x,y) = ( x^3 + y^3)^{1/3} $$ MY attempt: I claim $f$ is not differentiable at the origin because its partial derivatives are not defined at the origin. Is this correct?","['multivariable-calculus', 'calculus']"
695191,About two isomorphic schemes,"My question is related to an answer I read on MO: https://mathoverflow.net/questions/157973/classical-algebraic-varieties-vs-k-schemes-vs-schemes In the accepted answer, the user Julian Rosen claims that if $\sigma\in\operatorname{Aut}\mathbb C$ exchanges $e$ and $\pi$, then the two schemes associated to the varieties $V_1$ and $V_2$ are isomorphic. I don't undestand why this is true. Thanks in advance. Edit: As I say in some comments, the problem is that when one sends $V_1$ and $V_2$ in the category of schemes with the functor $t(\cdot)$ (see Hartshorne Proposition 2.6), I don't understand how the element $\sigma$ works.","['algebraic-geometry', 'schemes']"
695192,Elementary proof of geometric / negative binomial distribution in birth-death processes,"The birth-death process concerns a population of $n_0$ individuals, each of which reproduces and dies at a constant rate as time $t$ increases from $t=0$ . Each individual splits into two individuals (birth) with rate $\lambda$ , and each individual dies at rate $\mu$ . One can conceptualize this process as a continuous-time Markov chain with states $0, 1, 2,\ldots$ representing the current size of the population. We are interested in the distribution $P_t(n) = P(N(t) = n)$ of the population size $N(t)$ at time $t$ . It can be shown that for the pure birth process ( $\mu = 0$ ), at fixed time $t$ the probability $P_t(n)$ has the form of a negative binomial distribution: $$
P_t(n) = {n-1 \choose n-n_0} (e^{- \lambda  t})^{n_0} (1 - e^{-\lambda t})^{n - n_0}.
$$ A similar negative binomial formula can be derived for a nonzero death rate. This is typically proven by solving a PDE involving the generating function governing the process; see for example these slides or (Grimmett and Stirzaker, section 6.11). It would be interesting to prove the formula in a more elementary fashion, by identifying how exactly the birth-death process corresponds with the process that defines the binomial distribution. This is similar to how combinatorialists want a bijective proof of an enumeration theorem after they find a generating function proof. I have not been able to find the desired elementary proof, even for the special case of a pure birth process starting from 1 individual ( $n_0 = 1$ and $\mu = 0$ ), in which case the negative binomial simplifies to the geometric distribution. Does anyone know where to find it, or how to derive it from first principles? Here's what I've got so far: recall that the negative binomial distribution is the distribution of ""the number of successes in a sequence of Bernoulli trials before a specified number of failures occurs."" In the case of the formula above, we can compare with the formula on wikipedia to see that the number of successes is $n - n_0$ , the number of failures is $n_0$ , the success probability on each Bernoulli trial is $1 - e^{\lambda t}$ , and the failure probability is $e^{\lambda t}$ . The failure probability is recognizable as the probability that an exponentially distributed waiting time (with rate $\lambda$ ) exceeds $t$ . Thus, the formula seems to be telling us that the birth process can be simulated by throwing exponential waiting times onto the real line until $n_0$ of them have exceeded the threshold $t$ . But it's unclear what realizations of the process are generated by this simulation method, and how it is able to automagically place the proper probabilistic weight on these realizations.","['stochastic-processes', 'markov-process', 'markov-chains', 'probability']"
695217,Why is the preimage of a Spec $A$ the Spec of its integral closure in a morphism of curves?,"Let $\phi:X\to Y$ be a morphism of nonsigular complete curves. Let Spec $A:=U\subset Y$ be an open set. Why is $\phi^{-1} (U) =$ Spec $B$, the spectrum of the integral closure of $A$ in $K(X)$? Can we weaken the hypothesis of nonsingularity and completeness? This statement is, for example, in Hartshorne II.6.8. I see the containment $\phi^{-1} U \subset$ Spec $B$, but how do I prove the other one? How do I finish the following argument (is this the way to think about it?)? Every point $p\in \phi^{-1}(U)$ is such that the local ring $\mathcal O_{Y,q}$ of some $q\in U$ dominates $\mathcal O_{X,p}$...","['algebraic-geometry', 'schemes']"
695245,Showing that the n first derivatives of (xÂ²-1)^n have at least r roots (for the r-th derivative)?,"I have f(x) = (xÂ²-1)^n. I want to show that, for r = 0,1,2,...,n, the r-th derivative is a polynomial (that's easy to show) that has no fewer than r distinct roots in (-1,1).
I guess I need to use induction here. I've showed that these derivatives have actually no more than r distinct roots but I can't show the actual result...
My only idea was to use the Rolle's Theorem but it does not ensure enough roots.. Can you help me? Thank you","['polynomials', 'roots', 'derivatives', 'analysis']"
695254,Generalized Bayes Estimator,"Consider a decision problem in which the model parameter, $\theta$, is any
integer, the distribution for the integer observation, y, given $\theta$ is $P(y|\theta) = 1/3$ if $y \in [\theta - 1, \theta + 1]$ and 0 otherwise. The action space is the integers, and the loss function is 0-1 loss function. The decision rule is $\delta(y) = y$. Find decision rules $\delta'$ and $\delta''$ such that $\delta'$ dominates $\delta$ and $\delta''$ dominates $\delta'$ and is admissible. I think that here we use the improper prior $f(\theta) = 1$ and so the posterior is the same as the prior (i.e. discrete uniform on $[\theta - 1, \theta + 1]$). Then with the 0-1 loss the associated Bayes estimator is the posterior mode, which in this case could be one of three values. Neither, however, seem to dominate $\delta(y) = y$. Then I tried randomized rules (for example with 1/2 prob. take $y+1$ and with 1/2 prob take $y-1$), but that also doesn't seem to work....","['statistics', 'bayesian']"
695268,Determine best possible Lipschitz constant,"I'm slightly confused by a homework problem here...I've been given the function: $ f(u) = log(u) $ With the bounds: $ 2 \leq u \lt \infty $ Now I thought I understood what the Lipschitz Condition required, but that was a function of two variables, but here I only have $u$. Believe it or not, I'm not a math major and I'm confused as to how to proceed using the definition of the Lipschitz constant.","['numerical-methods', 'continuity', 'analysis']"
695290,Proof that the limit of the square root is the square root of the limit,"I'm trying to prove that:
$$\lim_{n\rightarrow\infty} \sqrt{a_n} = \sqrt{\lim_{n\rightarrow\infty} a_n}$$ Given $a_n > 0$ for all $n$. My initial idea was to start with the definition of limit (assuming $\lim_{n\rightarrow\infty} a_n = l$): $$|\sqrt{a_n} - \sqrt{l}| = |\frac{(\sqrt{a_n} - \sqrt{l}) (\sqrt{a_n} + \sqrt{l})}{(\sqrt{a_n} + \sqrt{l})}| = |\frac{a_n - l}{(\sqrt{a_n} + \sqrt{l})}|$$ The problem is that $\sqrt{a_n} + \sqrt{l}$ could be less than $1$. And therefore I can't continue the proof using this approach. Edit: forgot to mention that $a_n$ converges is another hypothesis.","['sequences-and-series', 'calculus', 'analysis']"
695412,Derivative of a Vector with respect to its norm (special relativity),"I came across an equation (related to special relativity) that requires me to to take a derivative of a vector with respect to to it's own norm. In a bit more detail, what I mean is, let: $$\vec T(\tau) = \{T_1(\tau), T_2(\tau), \dotsb, T_t(\tau)\}$$
$$\vec X(\tau) = \{X_1(\tau), X_2(\tau), \dotsb, X_x(\tau)\}$$
With $\vec T$ and $\vec X$ having dimensions $t$ and $x$ respectively and both being vector functions of $\tau$. Also let: $$\tau^2 = w^2 - s^2 = \vec T \cdot \vec T - \vec X \cdot \vec X$$ Where: $$w = \lVert \vec T \rVert \qquad s = \lVert \vec X \rVert$$ Knowing that:
$$d\tau^2 = dw^2 - ds^2= d\vec T \cdot d\vec T - d\vec X \cdot d\vec X$$ How then, can take the derivative $\frac {d\vec T}{dw}$? It may be useful to know that $d\vec X = [\beta]\cdot d\vec T$, where $[\beta]$ is a constant matrix of dimension $x\times t$. Thanks in advance.","['multivariable-calculus', 'normed-spaces', 'calculus', 'vectors']"
695421,"How to show that the monomials are not a Schauder basis for $C[0,1]$","why the monomials are not a Schauder basis for $C[0,1]$? $p_n(x)=x^n$ such that $(p_n)$ does not form a Schauder basis for $C[0,1]$ span$\lbrace p_n : n\ge 0\rbrace$ is dense in $C[0,1]$ by Weierstrass approxmation theorem, but I cannot figure out why they are not a Schauder basis. Could you please explain","['functional-analysis', 'real-analysis', 'analysis']"
695432,Does an infinite collection of circles accumulates at a circle?,"There is an infinite collection of closed circles in the plane, all within a finite bounding square. Does it contain an infinite sequence of circles that converge to a circle? Assume that a point is a circle of radius 0, and that convergence is defined by symmetric difference (i.e. the area of the symmetric difference between the sequence and the limit goes to 0). I thought of the following proof: Represent each circle as a triple $(x,y,r)$. Since the collection is bounded, the $x$, $y$ and $r$ values of all circles are bounded. Therefore, there is an infinite sequence of triples $(x_i,y_i,r_i)$, that converges to a triple $(x_L,y_L,r_L)$. The limit triple represents a circle, and the symmetric difference between the circles in the sequence and the limit circle goes to 0. My questions are: Is this proof correct? Is there another proof, that uses purely geometrical considerations (i.e. does not transform the circles to triples of numbers)?","['general-topology', 'geometry', 'sequences-and-series', 'circles']"
695448,When is the global section functor exact?,"Given sheaves $\mathcal{F}_1,\mathcal{F}_2, \mathcal{F}_3$ on some scheme $X$, and an exact sequence $$ 0\rightarrow \mathcal{F}_1\rightarrow \mathcal{F}_2\rightarrow  \mathcal{F}_3\rightarrow  0 ,$$ when do we know that the global section functor $\Gamma(X,*)$ is exact when applied to the above exact sequence? The global section functor is left exact, so I suppose I am asking when does $\Gamma(X,*)$ preserve a surjective morphism of sheaves. Of course we know this when $X=\operatorname{Spec}(A)$ is an affine scheme and the $\mathcal{F}_i$ are coherent $\mathcal{O}_X$-modules. Are there more notable cases, and more importantly, do we know of an if and only if condition on the scheme and sheaves to ensure that the global section functor is exact?","['sheaf-theory', 'algebraic-geometry', 'schemes']"
695451,Commutator subgroup of $GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ is $SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$,"How would I go about showing this, where $p$ is an odd prime? The inclusion $[GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z}),GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})] \subseteq SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ is relatively clear. I'm wondering if the proof for the other inclusion follows from the result that $[GL_{2}(\mathbb{Z}/p\mathbb{Z}),GL_{2}(\mathbb{Z}/p\mathbb{Z})]=SL_{2}(\mathbb{Z}/p\mathbb{Z})$. I've tried to use the result above with the natural homomorphism from $GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ to $GL_{2}(\mathbb{Z}/p\mathbb{Z})$ to prove this but to no avail.","['matrices', 'group-theory', 'abstract-algebra', 'linear-groups']"
695463,Demonstrate that the limit of a function of two variables does not exist,"From my multivariable textbook: $$\lim_{|x,y|\to|0,0|}\frac{y^2\sin^2 x}{x^4+y^4}$$ (original screenshot) Wolfram indicates that the limit DNE, but does not list the steps used to solve. Is there a particular substitution that I'm overlooking?","['multivariable-calculus', 'functions', 'limits']"
695466,Incorrect calculation? (Statistics homework),"I'm currently working on some statistics homework, and as you might guess from the title, I got the wrong result, and the reason why I'm writing it here is because I don't have a clue why it's wrong. The question is as follows: A student is driving from city A to B. On route to B there's 2 intersections where she will randomly choose which one to pass through. 1.
At the first intersection she can choose between Bridge A or B in the proportion 3:1 (Meaning bridge A is chosen with probability 3/4). At bridge A there's a 0.5 probability of being delayed by 0 mins. and a 0.5 probability of being delayed by 10mins. At bridge B there's a 0.4 probability of being delayed by 5mins. and a 0.6 probability of being delayed by 7mins. 2.
At the second intersection she can choose between road A or B in the proportion 2:1. At road A there's a 0.5 probability of being delayed by 1min. and a 0.5 probability of being delayed by 2 mins. At road B there's a 0.1 probability of being delayed by 3mins. and a 0.9 probability of being delayed by 9mins. Out from this answer the following: Let X be a stochastic variable which shows the student's total delay in minutes between city A to B. Calculate the following: $S_x$, $E(X)$, $Var(X)$ and $\sigma_X$ Let's just take the calculation for $S_X$ since I find that one to be the hardest. Since X is stochastic I can find that it has the following values:
1, 2, 3, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 16, 19 In order to calculate $S_x$ I have to calculate the expected value which I did as: E(X)=$\frac{(1+2+3+6+7+8+8+9+10+11+12+13+14+16+19)}{15} = 9.2667$ Then I believe I calculate $S_X$ the following way: $S_X$ = $\sqrt(\frac{E(X)}{N})$ = 4.8634 Unless I'm using the wrong method (which I most likely am), then there's something missing in my book because what my teacher calls $S_X$ ""Support"". Which I can find nothing about, I also tried to look up standard deviation, which also turned out to be wrong.",['statistics']
695471,Transforming semimartingale to local martingale by change of measure,"Consider a continuous $\mathbb{P}$ - semimartingale X which can be decomposed as M+A (M is local martingale and A is bounded variation process). Is it possible to change measure to $\mathbb{Q}$ s.t.  $\mathbb{Q} << \mathbb{P}$ and X will be $\mathbb{Q}$ local martingale? I was trying to use Girsanov change of measure. However, I was not able to show existence of a local martingale $N$ whose cross variation with $M$ as $A$. Any help/reference is highly appreciated.","['probability-theory', 'stochastic-processes', 'stochastic-analysis', 'martingales']"
695489,boolean function,"Let f = âshe is out of workâ and s = âshe is spending more.â Write the following
statements in symbolic form:
1. Neither is she out of work nor is she spending more. ANS : not f and not s is this answer corret if not can i please get help",['discrete-mathematics']
695492,Isometry vs. measure preserving?,"Consider functions between two measured metric spaces. What is the relation between an isometry and a function which preserves the measure of subsets? This question arose in my head as I thought about the surjection of the Cantor set onto $[0,1]$. This question had something, but I don't think it completely settles it.","['general-topology', 'measure-theory', 'differential-geometry']"
695496,issues with simple algebraic equations,"$ab + a + b = 250$ $bc + b + c = 300$ $ac + a + c = 216$ then find $a + b + c = ?$ MY APPROACH: (i) * c , (ii) * a , (iii) * b then we get $abc + ac + bc = 250c$ $abc + ab + ac = 300a$ $abc + ab + bc = 216b$ (iv)+(v)+(vi) $3abc + 2ac +2ab + 2bc = 300a + 216b +250c$ now i cant solve it ?? how to solve??","['algebra-precalculus', 'systems-of-equations']"
