question_id,title,body,tags
3692046,"The function $R(x)=rank(Df(x))$ is locally constant on $\Omega$, i.e. it is constant in a neighbourhood of every point $x \in \Omega$.","Let $f:\Bbb R^n \to \Bbb R^n$ be a mapping of class $C^1$ . Prove that there is an open and dense set $\Omega \subseteq \Bbb R^n$ such that the function $R(x)=rank(Df(x))$ is locally constant on $\Omega$ , i.e. it is constant in a neighbourhood of every point $x \in \Omega$ . I was first thinking to use rank theorem which states that if $Df(x_0)=m$ then $\exists $ a diffeomorphism $\Phi$ in the neighbourhood of $x_0$ and $\Psi$ in the neighbourhood of $f(x_0)$ such that $\Psi \circ f \circ \Phi^{-1}(x_1, \cdots , x_n)=(x_1, , \cdots , x_m)$ . Then I was thinking to take differentiation and then chain rule but it won't work as we will get $D(f( \Phi^{-1}))$ . The second way I was thinking was to use the dense property e.g: Can we say anything about $\{x \in \Bbb R^n| D(f(x))\geq k\}$ and then work with it? I deleted my earlier post as that was incorrect. Please help..","['real-analysis', 'multivariable-calculus', 'manifolds', 'differential-topology', 'differential-geometry']"
3692052,Are computer integers a finite group (under addition with overflow)?,"The integers and the integers modulo a prime are both groups under addition. What about the computer representation of integers (e.g. int64)? It's closed under addition, since a sum that is too large wraps around to the negatives. It also inherits the other group properties from the integers (associativity, identity, inverse). So int64 seems like a finite group, but am I missing anything?","['group-theory', 'abstract-algebra', 'finite-groups', 'computer-science']"
3692054,"What happens to differentials like $dA$, $dV$, $ds$, and $dS$ under a change of variables?","Motivating example I have noticed that the derivation of the moment of inertia of a solid ellipsoid uses a change of variables to transform it into a unit sphere; then the unit sphere's MoI gets scaled by the Jacobian determinant. I wondered if I could use a similar technique to get the perimeter of an ellipse. (My ultimate goal is to compute the moment of inertia of a hollow ellipsoid, and I reason that if I can do an arc integral along an ellipse, then I can introduce another parameter to do a surface integral over the ellipsoid.) The ellipse is defined by $$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$ and its arc length is $$\int_C ds$$ where $C$ is the ellipse itself. Now, the ellipse can be transformed into the unit circle if we assert $$x = au \\ y = vb \\ \implies\\ u^2 + v^2 = 1$$ This transformation has $$\left\vert J\right\vert =
\left\vert
\begin{matrix}
a & 0 \\ 0 & b \\
\end{matrix}\right\vert
= ab$$ And under this transformation, $$\int_C ds = \int_{C'} \left\vert J\right\vert \,ds' = ab \int_{C'} ds'$$ But $C'$ is just the transformed ellipse, which is the unit circle! So this is just $ab$ times the circumference of the unit circle, and the perimeter is simply $$2\pi a b$$ I know this is wrong , so I thought that I need to transform $ds$ as well. I tried $$ds = \sqrt{dx^2 + dy^2} = \sqrt{(a\,du)^2 +(b\,dv)^2}$$ which doesn't really get me anywhere, as I expected, given that there isn't a simple formula for the perimeter of an ellipse. Nonetheless, is this kind of manipulation correct in principle? If so, why isn't it possible to derive a closed-form expression for the perimeter of an ellipse from this? How come transforming $ds$ doesn't work here, but transforming $dV$ using the Jacobian in the above ellipsoid example works just fine? Is it because the moment of inertia problem considers a hollow ellipsoid, which would be analogous to integrating over the area of an ellipse (not its perimeter)? The 3D analogue of the perimeter would be the surface area, and a surface integral over the ellipsoid is required to find the moment of inertia of a hollow ellipsoid—I assume this latter quantity has no closed-form expression, either? My central question In general, what happens to ""geometric"" differentials like $dA$ , $dV$ , $ds$ , and $dS$ under a change of variables?",['multivariable-calculus']
3692082,Find a polynomial of degree at most 7 which leaves remainders –1 and 1 upon division by $(x-1)^4$ and $(x+1)^4$ respectively,"A polynomial of degree at most 7 is such that leaves remainders –1 and 1 upon division by $(x-1)^4$ and $(x+1)^4$ respectively. Find the sum of roots of this polynomial. Now as we have to find sum , I thinks it's pointing towards using viete. From remainder theorem , we get $f(x) = g_1(x)(x-1)^4-1$ and $f(x) = g_2(x)(x+1)^4+1$ where $g_{1,2}(x)$ is a polynomial of degree atmost 3 . But from this point , I get no more ideas. Like assuming a cubic for g(x) and then using binomial on $(x-1)^4$ is to too long and doesn't get me anywhere. Please help The above approach is followed Here but there it's manageable because the powers are small, so is there no alternate elegant way ??","['algebra-precalculus', 'polynomials']"
3692153,"Lebesgue integral, Is the solution right?","I'am trying to understand Lebesgue integration Compute $\int_{0}^{\pi}$ f(x)dx Where $f(x) = \begin{cases} 
sin x & \text{ if  }  x \in \mathbb{I}  \\ 
 cosx & \text{ if  }  x \in \mathbb{Q} 
\end{cases}$ I tried this (L) $\int_{[0,\pi]} f(x) dx = (L)\int_{[0,\pi] \cap \mathbb{Q}} f(x)dx + (L)\int_{[0,\pi] \cap \mathbb{I}} f(x)dx = (L)\int_{[0,\pi] \cap \mathbb{Q}} cos x dx + (L)\int_{[0,\pi] \cap \mathbb{I}} sinxdx = (R)\int_{[0,\pi]} cos x dx + (R)\int_{[0,\pi]} sinxdx = \int_{0}^{\pi} (sinx+cosx)dx = sin(\pi) - sin(0) + - (cos(\pi) - cos(0)) = 0 - (-2) = 2$ Is my sollution correct? 
Can you recommend any good materials with emamples of lebesgue integration? Thank you!","['integration', 'trigonometry', 'lebesgue-integral', 'functional-analysis']"
3692160,Identifying binomial distribution for finding variance,"If a variable $x$ takes values $0,1,2,....n$ with frequencies equal to the binomial coefficients $\binom {12}0,\binom {12}1,\binom {12}2,.....\binom {12}{12}$ , then variance of distribution is \begin{array}{|c|c|c|c|}
\hline
x& 0 & 1 & 2&......&12 \\ \hline
f &\binom {12}0 &\binom {12}1 &\binom {12}2&......&\binom {12}{12}\\ \hline
\end{array} It is solved in my reference as $$
\sigma^2=n/4=12/4=3
$$ as if it is a binomial distribution with $p=q=1/2$ . I understand it must be a binomial distribution but where do we have the clue that the probability of success in each trial is $1/2$ ? My thinking says irrespective of the probability of success the frequency of each case is the above binomial coefficients. So where am I thinking wrong about it ?","['statistics', 'probability']"
3692192,Optics problem: Simplifying the expression $ {r_v ^2 + r^2-2rr_v\cos\delta} \over {1+(rr_v)^2-2rr_v \cos \delta} $,"I was doing an optics problem and I ended up with the following expression for a reflexion factor: $$ {r_v ^2 + r^2-2rr_v\cos\delta} \over {1+(rr_v)^2-2rr_v \cos \delta} $$ Now, I could just plug my numbers here, but I couldn't help noticing that the numerator is the opposite side of a triangle with sides $r_v$ and $r$ , and an angle $\delta$ between them. Similarly, the denominator is the opposite side of a triangle with sides $1$ and $ { r r_v} $ , and an equal angle $\delta$ between them. Taking into account that $1>r>r_v$ , I made the following drawing: This is a really nice geometrical interpretation of the above formula, which makes me suspect that there is a way to simplify it. However I haven't been able to. Completing squares and using $1-\cos\delta=2\sin^2\delta/2$ yields the following: $$ {(r_v-r)^2+4rr_v\sin^2\delta/2} \over {(rr_v-1)^2+4rr_v\sin^2\delta/2} $$ ...which isn't really that simplified. Is there something else I can do?","['physics', 'trigonometry']"
3692205,Orthonormal columns implies orthonormal rows,"I find it non-intuitive if I impose that all of a square matrix's columns are normalized and mutually orthogonal, then all its rows are also normalized and mutually orthogonal. Any intuitive explanation for this? Also if I relax the conditions to be only mutually orthogonal without being normalized, is this still true? And why so?",['linear-algebra']
3692246,Can this binary operation of vectors on a simplex be mapped to addition in a vector space in general?,"I'm working with a problem where I have $N$ -dimensional vectors in the first orthant confined to the unit simplex, i.e. their components satisfy \begin{align}
     v_i & > 0 \ \forall\, i\text{ and} \\
     \sum_{i=1}^N v_i & = 1.
\end{align} Call the space these vectors are in $\Delta^{N-1}$ for the simplex it is (excluding the boundary). Define the binary operation for $v$ , $w\in \Delta^{N-1}$ \begin{align}
    v\odot w & = \frac{v_i w_i}{\sum_j v_j w_j} \\
     &\equiv u.
\end{align} This operation defines an abelian group. Clearly $u\in \Delta^{N-1}\ \forall \ v,\ w$ , so it is closed. It is obviously commutative. It is also associative \begin{align}
   u\odot(v\odot w) & = \frac{u_i \frac{v_i w_i}{\sum_j v_j w_j}}{\sum_k u_k \frac{v_k w_k}{\sum_j v_j w_j}} \\
   & = \frac{u_i v_i w_i}{\sum_k u_k v_k w_k} \\
   & = (u\odot v)\odot w.
\end{align} The identity element is obvious $e_i = \frac{1}{N}\ \forall\ i$ . The inverse element is likewise obvious $[v^{-1}]_i = \frac{v_i^{-1}}{\sum_{j=1}^N v_j^{-1}}$ . (The inverse element requirement is the reason for excluding the boundary vectors). Is there a mapping from $\Delta^{N-1}$ to $\mathbb{R}^{N-1}$ that maps the $\odot$ operation to vector addition? The case for $N=2$ is actually pretty straightforward. If we map vectors to \begin{align}
    \phi_v &= \ln\left(\frac{v_1}{v_2}\right)
\end{align} then $\phi_v + \phi_w$ will have the same value as $\ln(u_1/u_2)$ . How can this be generalized?","['group-theory', 'statistics', 'probability']"
3692275,recursion number of possible paths,"There is a terraced structure with N floors (here, for example 4 floors). Each floor has 4 cells. Write the recursive function that describes the number of all the possible paths from the left bottom cell to the top right cell.
We can go up, down, right but not left and we can't visit in the same cell again.
Use only one variable.","['discrete-mathematics', 'recursion']"
3692286,Pseudo-isomorphism in Iwasawa Theory,"Let $\Lambda$ denote the Iwasawa algebra $\mathbb{Z}_p[[\Gamma]]$ , where $\Gamma$ is a group isomorphic to $\mathbb{Z}_p$ . We know that $\Lambda$ is homeomorphic to $\mathbb{Z}_p[[T]]$ , ring of power series in the variable $T$ . Two modules $\Lambda$ -modules $A$ and $B$ are said to be pseudo-isomorphic if there is a $\Lambda$ -homomorphism $f\colon A\to B$ with finite kernel and cokernel, i.e. they sit in an exact sequence like $$0\to F_1\to A\to B\to F_2\to 0,$$ with $F_1$ and $F_2$ finite $\Lambda$ -modules. We write $A\sim B$ . If $A$ is finitely generated, then $$A\sim \Lambda^{n}\oplus \bigoplus_i \Lambda/f_i\Lambda$$ for some $f_i\in \Lambda$ . The characteristic ideal is $\prod_i f_i\Lambda$ . I have a few questions: 1) Some authors define the characteristic ideal to be 0 in the non-torsion case, others does not specify. Is there a reason for which one is preferable? Is true in every case that the characteristic ideal well-behaves under exact sequences? 2) The pseudo-isomorphism relation is not symmetric in general, but it is in the case of torsion modules. Is there a simple way to see this? 3)Is this relation transitive in general, or maybe just in the case of torsion? No book says something like this, but if it is not symmetric, how can we say that if $A$ and $B$ are pseudo-isomorphic, then they have the same characteristic ideal? For (3), it seems to me natural that if $f\colon A\to B$ and $g\colon B\to C$ have finite kernel and cokernel, also the composition has finite kernel and cokernel. For example, the kernel of the composition is the set of elements going to zero under $f$ , plus the ones going to the kernel of $g$ , and also this second part is finite since both kernels are finite. Am I missing something?","['number-theory', 'algebraic-number-theory', 'cyclotomic-fields']"
3692345,Any reference(a book) that defines the $n$-dimensional rotation matrix?,"I want to refer to a mathematics book that explains the n-dimensional rotation matrix or rotation transformation. Wikipedia concentrates most on 2D or 3D.
There are things that one can say definition here and there , but I think it is not a good idea to use the definition there. Actually they don't seem to be definitions. Strang's ""Linear Algebra"", Barret O'neill's ""Elementary Differential Geometry"" deal only with 2D or 3D cases. I think physicist are more interested in the general case, due to the theory of relativity. I found one explanation in ""Geometrical Methods of Mathematical Physics"" by Bernard Schutz. But I think it doesn't define the rotation matrix. Artin's ""Geometric Alegebra"" defines the rotation group as an isometry $\sigma:V\to V$ such that $\det\sigma=1$ .
But the language there is so abstract that I can't catch any of them. Can anyone give a reference that defines rotation transformation on $\mathbb R^n$ and state as a property that $A$ is a rotation matrix if and only if $A\in SO(n)$ ? This is the end of the question and the below is what I wanted to do.
I wanted to prove that if $A\in SO(n)$ , then $A$ is a rotation about a line through the origin in $\mathbb R^n$ .
So I need to define the rotation transformation(or matrix) in $n$ dimensional Euclidean space.","['matrices', 'geometry', 'rotations', 'reference-request']"
3692365,Difference between a function and a section of a fibre bundle,"Suppose $E \rightarrow B$ with projection map $\pi$ and fibre F is a fibre bundle, with a section $\sigma$ . How is $\sigma$ different from a function $f:B \rightarrow F$ ? The standard answer I find everywhere is that $\sigma$ is only a function if E is the trivial bundle $B \times F$ , or that in general no such function exists with 'global structure', but I do not see why the former is true, and I do not understand what the latter means. A section seems like it is associating an element of the fibre F to every point in B. How is this not a function, even if E has some sort of non trivial topology? Why should 'twists' or other aberrations from a product in the space E matter, since the fibre over any point in B is the same?","['fiber-bundles', 'differential-geometry']"
3692407,On Borel $\sigma$-algebra of subset as a subspace,"Let $S$ be a subset of the set of real numbers $\mathbb{R}$ , let $\mathcal{B}$ be the Borel $\sigma$ -algebra generated by all open subsets of $\mathbb{R}$ . Consider $S$ as a topological space endowed with the subspace topology (i.e., the topology inherited from $\mathbb{R}$ ), and let ${\mathcal{B}}_S$ denote the Borel $\sigma$ -algebra generated by all open subsets of S. Is it true that ${\mathcal{B}}_S\subset \mathcal{B}$ ? (Here, the set $S$ is not necessarily a Borel set in $\mathbb{R}$ .) I think, in general, this is not true. However, I could not find a counter-example.","['borel-sets', 'measure-theory']"
3692412,Proving that $|f(x+h)-f(x)|\geq c|h|$ for every $x\in K$ compact set,"Let $f: U \rightarrow \mathbb R^n$ be a $C^1$ function defined over an open set $U\subset \mathbb R^m$ . If $K\subset U$ is a compact set such that for every $x\in K$ the associated linear operator $f'(x): \mathbb R^m \rightarrow \mathbb R^n$ is injective, then exists $c>0, \delta>0$ such that $$|f(x+h)-f(x)|\geq c|h|$$ for every $x\in K$ and $|h|<\delta$ . What I did so far: Since for every $x \in K$ the operator $f'(x)$ is injective, then it is a bijection between $\mathbb R^m$ and its image. Thus, there must exist a constant $c$ (very likely to be dependent on x) satisfying $|f'(x)h|\geq 2c|h|$ . If I could, somehow, prove that this $c$ holds for every $x\in K$ or its bounded from above, I would be done; because in this case, since $f$ is $C^1$ , it is uniformly differentiable and we obtain $\delta>0 $ such that $|f(x+h)-f(x)-f'(x)h| < c|h|$ for every $|h|<\delta$ , $x\in K$ . Therefore, $|f(x+h)-f(x)|\geq |f'(x)h| - |f(x+h)-f(x)-f'(x)h|\geq 2c|h| - c|h| = c|h|$ and it's over. So the problem here is pretty much finding an argument to control, from above, the set of constants $c$ satisfying $|f'(x)h|\geq 2c|h|$ . How can I do this? Should I use that $x\mapsto |f'(x)| $ attains a minimum value over $K$ ? I tried this one, but I wasn't successful. Any help, hint?","['analysis', 'real-analysis']"
3692414,Circle permutation without consecutive integers placed together,"How many ways can $n$ numbers from 1 to $n$ be arranged in a circular order without consecutive integers being placed together? (Note: 1 and $n$ are also considered consecutive integers) For example, if $n=5$ , there are 2 ways: 53142 and 24135 (in a circle); and if $n=6$ , I get 6 ways: 531462 and 264135, 241536 and 635142, 631524 and 425136 (in a circle). I have tried inclusion-exclusion principle but I have found it's difficult to get the expression; and have also failed to get its recursive equation. Any help in this matter will be appreciated!","['permutations', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
3692435,Prove: $\sum\limits_{k=0}^{n}\frac{1}{k+1}\binom{2k}{k}\binom{2n-2k}{n-k} = \binom{2n+1}{n}$ [duplicate],This question already has answers here : Combinatorics Identity about Catalan numbers: $\sum_{k=0}^n \frac{1}{k+1}\binom{2k}k \binom{2n-2k}{n-k}=\binom{2n+1}n$ (3 answers) Closed 3 years ago . prove the following identity: $\displaystyle\sum_{k=0}^{n}\frac{1}{k+1}\binom{2k}{k}\binom{2n-2k}{n-k} = \binom{2n+1}{n}$ what I tried: I figured that: $\displaystyle\binom{2n+1}{n} = (2n+1) C_n$ and $\displaystyle\sum_{k=0}^{n}\frac{1}{k+1}\binom{2k}{k}\binom{2n-2k}{n-k}= \sum_{k=0}^{n}C_k\binom{2n-2k}{n-k}$ from here i tried simplifying: $\displaystyle\binom{2n-2k}{n-k}$ to something i could work with but did not succeed I also know that $\displaystyle C_n = \sum_{k=0}^{n-1}C_k C_{n-k-1}$ so I tried to prove : $\displaystyle\sum_{k=0}^{n}C_k\binom{2n-2k}{n-k}= C_n + \sum_{k=0}^{n-1}C_k\binom{2n-2k}{n-k} = C_n + 2n\sum_{k=0}^{n-1}C_kC_{n-k-1}$ but that approach also failed (couldn't prove the last equality) any suggestions?,"['catalan-numbers', 'summation', 'binomial-coefficients', 'combinatorics']"
3692497,Can we define differentiation without using a norm?,"Since all norms on $\mathbb R^n$ are equivalent, the following question makes sense: Can we define the notion of ""differentiability"" of a map $\mathbb R^n \to \mathbb R$ without refering to a norm at all? Can we define the derivative itself? (That is, without mentioning any kind of norm, or a distance induced by it). In fact, I guess that one could ask that even for $n=1$ .","['multivariable-calculus', 'calculus', 'differential-topology', 'derivatives', 'differential-geometry']"
3692649,A question on Fontaine's periods rings,"Let $K$ be a complete discrete valued field of characteristic zero with perfect residue field $k$ of characteristic $p>0$ , $\mathcal{O}_K$ its ring of integers, $C$ the completion of an algebraic closure of $K$ and $\mathcal{O}_C$ its ring of integers. Fontaine defines in "" Le corps des périodes p-adiques "" (see loc.cit, remark 1.2.4.(c)) $$\rm{A}_{inf}(\mathcal{O}_C|\mathcal{O}_K)$$ as the completion of $\mathcal{O}_K\otimes_{W(k)}W(\mathcal{O}_C^{\flat})$ for the topology defined by the ideal ( $p$ )+ker( $\theta$ ), for a certain map $$\theta :\mathcal{O}_K\otimes_{W(k)}W(\mathcal{O}_C^{\flat}) \to \mathcal{O}_C$$ where $W$ denotes Witt vectors, and $\mathcal{O}_C^{\flat}$ is the tilt of $\mathcal{O}_C$ , that is, the projective limit of $...\to \mathcal{O}_C/p\mathcal{O}_C\to \mathcal{O}_C/p\mathcal{O}_C$ where the maps are Frobenius. Later, in 1.3.3, it is claimed that $$\rm{A}_{inf}(\mathcal{O}_C|\mathcal{O}_K)=\mathcal{O}_K\otimes_{W(k)}W(\mathcal{O}_C^{\flat})$$ (that is, there is no need of completing). Why is this last claim true?","['algebraic-number-theory', 'p-adic-number-theory', 'algebraic-geometry']"
3692719,Prove that $\sum{\frac{1}{n_k}}$ is convergent .,"The sequence $(n_k)$ is a strictly increasing positive sequence of integers which satisfies the condition $\lim_{k\to\infty}\frac{n_k}{n_1...n_{k-1}}=\infty$ . Now this has been my attempt, $\frac{n_k}{n_1...n_{k-1}}>1$ for all $k \ge N_1$ .
So $n_k > n_1...n_{k-1}$ for all $k \ge N_1$ . Since the sequence is strictly increasing so $(n_k) >M_{N_1}$ for all $k \ge N_1$ ,then $\frac{1}{n_k}<\frac{1}{M_{N_1}}$ for all $k \ge N_1$ .Now let us chose $\min m=(n_1,...,n_{N_1-1},{M_{N_1}})$ . Then $n_k> m$ for all $k \in N$ Now $\sum{\frac{1}{n_k}}<\sum{\frac{1}{n_1...n_{k-1}}}<\sum{\frac{1}{m^{k-1}}}$ for $ k \ge N_1$ which leads to a geometric sequence. I think this method should work.It would be very helpful if someone goes through my attempt and point out my mistake. Edit1:The question also has a subpart which ask to prove that the sum is irrational.How do I proceed?","['sequences-and-series', 'real-analysis']"
3692728,From derivatives to gradients in backpropagation,"Consider the following explanation of backpropagation from Wikipedia: Given an input–output pair {\displaystyle (x,y)}(x,y), the loss is: $ C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots )))$ The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input $x$ : ${\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdots {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}}$ These terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights: $\displaystyle {\frac {dC}{da^{L}}}\cdot (f^{L})'\cdot W^{L}\cdot (f^{L-1})'\cdot W^{L-1}\cdots 
(f^{1})'\cdot W^{1}.$ So far so good. Then they say: The gradient $\nabla$ is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed , but the entries are the same: $ \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\cdots \cdot (W^{L-1})^{T}\cdot (f^{L-1})'\cdot (W^{L})^{T}\cdot (f^{L})'\cdot \nabla _{a^{L}}C.$ Why ? i.e. why does converting this to ""gradient form"" (not sure if that's what they actually did above) requires ""transposing matrices and reversing the ordering of multiplication""? In case it helps, I thought this article here (layout conventions in matrix calculus) may help, but I'm not able to cross-reference hat convention was used above, and why vectors / matrices are transposed, and why they chose to re-arrange the ordering of the terms","['matrix-calculus', 'derivatives', 'neural-networks', 'chain-rule']"
3692758,Is there a polynomial $p$ such that $x^n$ divides $1+x-p^2$??,"I'm studying Ring Theory and I have a list of exercises to do, actually I'm trying to prove that for each nilpotent matrix $A \in F^{n^2}$ , there is a matrix $N$ s.t $I_n+A = N^2$ . So, if we have for $n \geq 2$ a polynomial $p(x)$ s.t $(1+x - p^2) = x^n q(x)$ , then applying $A$ , we have $N= p(A)$ . May you help me with a tip about $p$ ??","['matrices', 'ring-theory', 'polynomials']"
3692794,Evaluate the integral $\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz$,"I need to calculate the integral $$\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz$$ being $\Gamma=S_1\cap S_2$ , given: $S_1=\{(x,y,z)\in\mathbb{R}:2x+2y+z=3\}$ $S_2=\{(x,y,z)\in\mathbb{R}:z=9-x^2-y^2\}$ In my problem i'm asked to solve it using both direct integration and the Stokes Theorem . I've started trying the Stokes part, calculating the rotational of $F$ : $$F=((y^2-z^2),(z^2-x^2),(x^2-y^2))\Longrightarrow \text{rot}(F)=-2(y+z,x+z,x+y)$$ Now, I know that the normal vector $N$ is $(2,2,1)$ (because $S_1\subset\Gamma$ and that's $S_1$ 's normal vector at any point). So now I using Stokes I have that (after simplifying) $$\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz=\frac{-2}{3}\iint_S(3x+3y+4z)d\sigma$$ I'm stucked here. I don't get what $S$ am I supposed to use in the double integral (I guess  I must do some variable change). For the direct integration part, i don't know where to start. I will thank any help. Edit: I need two different solutions, one using direct integration, and another using Stokes Theorem.","['integration', 'surface-integrals', 'multivariable-calculus', 'lebesgue-integral']"
3692817,why $x_m$ converges weakly to $x_\infty$?,"Let $(X,\|.\|)$ be reflexive Banach space and $Y$ be a closed separable subspace of $X$ $\big((Y ,\|.\|)$ is clearly a separable reflexive Banach space $\big)$ , then the dual space $Y^*$ of $Y$ is separable. Let $\{y_n^*\}$ be a countable dense subset of $Y^*$ . Let $\{x_m\}$ be a bounded sequence in $X$ , such that $$
\langle y_n^*, x_m\rangle\underset{m}{\to }z_n\qquad \forall n
$$ With $z_n\in\mathbb{R}$ . We suppose that the sequence $\{x_m\}$ has a subsequece $\{x_{m_i}\}$ weakly convergente in $Y$ to an element $x_\infty$ . Then $$
\langle y_n^*, x_\infty\rangle=z_n\qquad \forall n\qquad (*)
$$ Since $\{y_n^*\}$ separates the points of $Y$ , it follows from $(*)$ that every limit point of $\{x_m\}$ must equal $x_\infty$ . My problem I don't understand why : we can conclude that $x_m$ converges weakly to $x_\infty$ This result was used in the article Infinite-Dimentional Extension of a Theorem of Komlos of Erik J.Balder , on pages 186-187. In the context of the article, the auther says that: "" $\{s_n(t)\}$ converges weakly to a point $y_t$ in $Y$ ."" But i don't understand why. An idea please.","['banach-spaces', 'functional-analysis', 'weak-convergence']"
3692833,Collection of subset generating every pairs of elements,"I'm looking forward to a construction with the following property: Given a set S of n elements, find a small/the smallest collection of subsets of S of size k such that for every pair of elements a, b in S, there exist a subset containing both a and b.
I'm especially wondering about k = log(n), or k=sqrt(n). Said otherwise, is there a nice way to get a complete graph on n vertices from the union of cliques on some subsets of vertices of size k ?
If yes, do you know of an algorithm able to find subsets that ""cover"" an arbitrary graph rather than the complete graph ?","['graph-theory', 'combinatorial-designs', 'combinatorics', 'algorithms']"
3692898,Can we find a function that finds the number of points inside a square centred on the origin?,"This question was inspired from this problem . For this problem, the radius of a square will be the distance from its centre to
  any of its four vertices. A lattice point is a point $(a, b)$ in the plane where $a$ and $b$ are both integers. Find a function $f$ on the positive real numbers so that $f(r)$ is the largest
  possible number of lattice points inside any square of radius $r$ centred at the
  origin. A point on the perimeter is considered to be inside the square. I misread the problem linked above and was trying to solve this question. I tried writing down the values of $f(r)$ depending on the value of $r$ . For example, when $1 \le r <\sqrt 2, f(r)=5$ and when $\sqrt 2 \le r < 2, f(r)=9$ . Next, I tried to focus only on the lower bounds, where the square's corners fall on boundary points. I could see that $r$ was of the form $\sqrt {a^2+b^2}$ so maybe I could try a function involving a lower bound. When the square's vertices fall on lattice points, we can use Pick's theorem to calculate the area. Here, $i$ means the number of interior points and $b$ means the number of boundary points. According to Pick's theorem, $A=\frac 12b + i-1$ However, as it is a square with diagonal $2r$ , $A=2r^2$ $2r^2=\frac 12b + i-1$ We know that $f(r)=b+i$ when $r$ is such that the square has lattice points as vertices. So: $f(r)=4r^2+1-i$ or $f(r)=4r^2+b+1$ I could not progress any further. I thought I could form a relationship between the current square and the next largest square because $f\left(r_{\text{Current Square}}\right)=4r^2+1-f\left(r_{\text{Next Largest Square}}\right)$ but I am finding it difficult to define how we can get from a square to the next largest/smallest one. How should I progress? Is it even possible to find $f(r)$ ?","['contest-math', 'coordinate-systems', 'geometry']"
3692904,How to evaluate limit of a sequence $ \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}} $,"I need a help with evaluating a limit of a sequence $$ \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}}. $$ The problem is that $\lim_{n \to \infty} \left( -2 \right)^n$ does not exist. What can we even tell from this when there's a part oscilating between $+\infty$ and $- \infty$ . Wolfram says it should equal to $ - \infty$ , but how to get there? The only thing I know might help is to factor out the fastest growing terms. $$
\lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}}
=
\frac{1}{3}\lim_{n \to \infty} \frac {2 \cdot 3^{2n} - \left( -2 \right)^n}{2 \cdot 3^n - 6 \cdot 2^{2n}}
=
\frac{1}{3} \lim_{n \to \infty} \left( \frac {3}{2} \right)^{2n} \cdot \frac {2 -  \left( - \frac {2} {9} \right)^n}{2 \cdot \left( \frac{3}{4} \right)^n - 6}
$$ Now we can see that $\left( \frac {3} {4} \right)^n $ goes to zero. What about $\left(- \frac{2}{9} \right)^n $ ? I guess that despite the fact that the values oscilate between $+$ and $-$ , the overall fraction has to go to zero, hence giving $$\left| \frac{1}{3} \cdot \infty \cdot \frac{2}{-6} \right| = \left| - \frac{\infty}{9} \right| = -\infty$$ Does that make sense?","['limits', 'calculus']"
3692972,Lp Space: an intuitive understanding.,"Would someone be willing to give me an intuitive understanding of the Lp space? I have several analysis books which (seemingly) approach the topic differently, which confuses me more, and a simple Google search hasn't led me to any greater enlightenment either. I would be grateful for any feedback from the community. Thanks in advance.","['measure-theory', 'lp-spaces', 'analysis']"
3692982,"If $\sum_{n=1}^{\infty} a_{n}$ converges, then$\sum_{n=1}^{\infty} \sin(a_{n})$ also converges","If series $\sum_{n=1}^{\infty} a_{n}$ converges, prove series $\sum_{n=1}^{\infty} \sin(a_{n})$ converges  too. Is this a series function problem or something related to? I tried this: If $a_{n} \ge 0 $ for all $n$ , then $| \sin (a_{n})| \le a_{n}$ , on the other hand $\sin(x)$ is continuous function in $[0,x]$ and differentiable in $(0,x)$ then exist $c \in (0,x)$ and $$(x-0)\cos (c)= \sin(x)-\sin(0)$$ then $$x\cos(c)= \sin(x)$$ but, $|\sin(x)|=|x\cos(c)|= |x| |\cos(c)| \le |x|$ , thus $|\sin (x)| \le |x|$ . We know, $|\sin (a_{n})| \le |a_{n}| $ and $\sum_{n=1}^{\infty} a_{n}$ converges, then $\sum_{n=1}^{\infty} |\sin(a_{n})|$ converges. Therefore $\sum_{n=1}^{\infty} \sin(a_{n})$ converges. But this proof use ${a_{n}}$ positive and in the original problem I don't have this hypotesis.","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
3692998,Modified Newton method and contraction principle,"I am studying Newton's method modified by the book Zorich, Mathematical analysis II, page 39,40: It seems to me, if I make no mistakes, that there is a problem in the derivative of $ A (x) $ . The author says that $ | A '(x) | = | [f' (x_0)] ^ {- 1} \cdot f '(x) | $ , while I would say that: $$ | A '(x) | = | 1- [f' (x_0)] ^ {- 1} \cdot f '(x) | $$ Am I wrong?","['newton-raphson', 'numerical-methods', 'derivatives', 'analysis']"
3693000,"If A positive definite then $\det(A) \leq \prod_{i=1}^n a_{i,i}$","If $A=(a_{ij})_{ij=1,...n}\in \mathbb{C}^{n \times n}$ positive definite $ \Rightarrow \det(A) \leq \prod_{i=1}^n a_{i,i}$ I've argumented with the Cholesky-decomposition (which exists because A is pos. def.) that $A = CC^*$ with $C$ being an lower triangular matrix and $C^*$ being an upper triangular matrix. Then $C$ and $C^*$ have the form... $C =\left( \begin{array}{rrrr}
x_{1,1} & 0 & 0 & 0 \\
x_{1,2} & x_{2,2} & 0 & 0 \\
... & ... & ... & 0 \\
x_{1,n} & x_{2,n} & ... & x_{n,n} \\
\end{array}\right) $ and $C^*=\left( \begin{array}{rrrr}
x_{1,1} & x_{1,2} & ... & x_{1,n} \\
0 & x_{2,2} & ... & x_{2,n} \\
0 & 0 & ... & ... \\
0 & 0 & 0 & x_{n,n} \\
\end{array}\right) $ then $CC^* = \left( \begin{array}{rrrr}
x_{1,1}^2 &  &  & * \\
 & x_{2,2}^2 +x_{1,2}^2 &  &  \\
 &  & ... &  \\
* &  &  & x_{n,n}^2 + x_{1,n}^2+x_{2,n}^2+...+x_{n-1,n}^2\\
\end{array}\right) $ Obviously, $\prod_{i=1}^n a_{i,i} = x_{1,1}^2*(x_{2,2}^2+x_{1,2}^2)*...*(x_{n,n}^2+x_{1,n}^2+...+x_{n-1,n}^2)$ And $\det(A) = \det(CC^*) = \det(C) * \det(C^*) = \det(C^*)*\det(C^*) = x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2$ If what I've proven so far is correct, I'm very happy. But something bothers me. In $\mathbb{R}$ it is clear that $\det(A) \leq \prod_{i=1}^n a_{i,i}$ because the other sums are positive real numbers. Since I'm in the complex realm this formular doesn't hold true, does it? I cannot define $""\leq""$ with complex numbers since it's not an order relation. There is no guarantee that the other sums besides $x_{1,1}^2*x_{2,2}^2 *...*x_{n,n}^2$ are greater than 0 or even a real number, because $(a+bi)^2 = (a^2-b^2)+i(2ab)$ is still a complex number in most cases. Can someone check my findings and elaborate? Thanks in advance.","['matrices', 'positive-definite', 'linear-algebra', 'complex-numbers']"
3693016,Hilbert space version of the notion of conditionally weakly-mixing functions.,"$\newcommand{\norm}[1]{\|#1\|}$ $\newcommand{\ab}[1]{\langle #1\rangle}$ $\newcommand{\mr}{\mathscr}$ $\newcommand{\mc}{\mathcal}$ $\newcommand{\E}{\mathbb E}$ $\newcommand{\C}{\mathbf C}$ Background/Definitions Let $(X, \mc X, \mu, T)$ be an invertible measure preserving system.
A function $f\in L^2$ is said to be weakly-mixing if $$
\lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{f, T^n f}|^2 = 0
$$ This definition can be given for a more abstract setting.
Let $H$ be a Hilbert space equipped with a unitary operator $U:H\to H$ .
A point $x\in H$ is said to be weakly-mixing if $$
\lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{x, T^nx}|^2 = 0
$$ Now suppose $\mc D$ and $\mc A$ are $T$ -invariant sub- $\sigma$ -algebras of $\mc X$ with $\mc A\subseteq \mc D$ .
Then we have a factor map $(X, \mc D, \mu, T)\to (X, \mc A, \mu, T)$ mapping $x$ to $x$ .
Thus $(X, \mc D)$ is an extension of $(X, \mc A)$ .
We define a conditional inner product on $L^2(\mc D)$ as $$
\ab{f, g}_{(\mc D|\mc A)} = \E[f\bar g| \mc A]
$$ for all $f, g\in L^2(\mc D)$ .
Note that the ""inner product"" is valued in $L^1(\mc A)$ and not in $\C$ .
Let $L^2(\mc D|\mc A)$ be the subspace of $L^2(\mc D)$ consisting of all $f\in L^2(\mc D)$ such that $$
\norm{f}_{(\mc D|\mc A)} := \sqrt{\E[|f|^2|\mc A]}
$$ is in $L^\infty(\mc A)$ . Definition. A function $f\in L^2(\mc D| \mc A)$ is said to be conditionally weakly-mixing if for each $g\in L^2(\mc D|\mc A)$ we have $$
\lim_{N\to \infty} \frac{1}{N}\sum_{n=0}^{N-1} \|\ab{T^nf, g}_{(\mc D|\mc A)}\|_{L^2(\mc A)}^2 = 0
$$ This is a natural conditional version of the non-conditional definition given above. Question Question. Is there a Hilbert space version of definition of conditionally weakly-mixing. The first thing that comes to mind is to consider a Hilbert space $K$ equipped with a unitary operator $U$ , and let $H$ be a $U$ -invariant closed linear subspace of $K$ .
Thus $K$ mimics $L^2(\mc D)$ and $H$ mimics $L^2(\mc A)$ . Since the adjoint of the inclusion $L^2(\mc A)\to L^2(\mc D)$ is the conditional expectation map $\E[\cdot|\mc A]:L^2(\mc D)\to L^2(\mc A)$ , we may define the conditional inner product of $x$ and $y$ in $K$ as $\ab{x, y}_{(K|H)} = \ab{i^*x, i^*y}_H$ , where $i:H\to K$ is the inclusion map and $i^*$ is the adjoint of $i$ .
However, $\ab{x, y}_{(K|H)}$ is a complex number, unlike in the case of the conditional inner product.
So perhaps we need more structure on the Hilbert space to come up with a proper definition.","['measure-theory', 'ergodic-theory', 'functional-analysis']"
3693074,Showing that $\left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v|$ implies $|f(x)|\le\frac{1}{2}|x|^2$,"I got this question from my analysis book: Let $f:U\to\mathbb{R}^n$ be of class $\mathcal{C}^1$ in an open convex $U\subseteq\mathbb{R}^m$ , with $0\in U$ and $f(0)=0$ . (a) If $|f'(x)|\le|x|$ for all $x\in U$ then $|f(x)|\le\frac{1}{2}|x|^2$ for all $x\in U$ . (b) Conclude that if $f(0)=f'(0)=0$ with $f\in\mathcal{C}^2$ then $\left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v|$ implies $|f(x)|\le\frac{1}{2}|x|^2$ . I already done the (a) part and there should be only a few steps to conclude the (b) part but I'm struggling with it. I know that $\frac{\partial^2 f }{\partial u\partial v}(x)=(f''(x))(u,v)$ if we see $f''(x)$ as a bilinear function $f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n$ but from here on I don't know what to do. Can someone please shed some light on this? For the (a) part see this question . I also wonder how my question relates to this because it would imply that $|f(x)|\le\frac{1}{2}|x|^2$ for any $f$ satisfying the conditions in (b). I think that the author of that question misinterpreted the question statement given his comments on the answers, and the answers also not proving exactly what the author stated (and also he got the question from the same book that I'm reading).","['multivariable-calculus', 'real-analysis']"
3693082,"Finding the general formula for the sequence with $d_0=1$, $d_1=-1$, and $d_k=4 d_{k-2}$","Suppose that we want to find a general formula for the terms of the sequence $$d_k=4 d_{k-2}, \text{ where } d_0=1 \text{ and } d_1=-1$$ I have done the following: \begin{align*}d_k=4d_{k-2}&=2^2d_{k-2} \\ &=2^2\left (2^2d_{(k-2)-2}\right )=2^4d_{k-4} \\ &  =2^4\left (2^2d_{(k-4)-2}\right )=2^6d_{k-6} \\ & = 2^6\left (2^2d_{(k-6)-2}\right )=2^8d_{k-8} \\ & = \ldots \\ & = 2^id_{k-i}\ , \ \ i \text{ even}\end{align*} If $k$ even, then at the last step we have for $i=k$ (since $k$ is the maximum even number $\leq k$ ) : $d_k=2^kd_{k-k}=2^kd_0=2^k$ . If $k$ odd, then at the last step we have for $i=k-1$ (since $k-1$ is the maximum even number $\leq k$ ) : $d_k=2^{k-1}d_{k-(k-1)}=2^{k-1}d_1=-2^{k-1}$ . How can we find the general form for the terms of the recurrence relation? Or do we distinguish cases when $k$ is even and odd? I am interested to find the general formula without using the characteristic equation.","['recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
3693093,Probabilistic Proof of a Hausdorff-Young Type Inequality,"Let $1 \leq p <2$ and let $q$ be the Holder conjugate of $p$ so that $\frac{1}{p} + \frac{1}{q} = 1$ . Show that for any $\epsilon >0$ , there exists a Schwartz function $f \in S(\mathbb{R}^d)$ , such that: $$
\|\hat{f}\|_{L^{q}(\mathbb{R}^d)} \leq \epsilon \|f\|_{L^p(\mathbb{R}^d)}
$$ The exercise suggests that as a hint, one should use Khintchine's inequality: If $\epsilon_{n}$ is a IID sequence of $\mathrm{Unif}(\{-1,1\})$ random variables
(random choice of signs) and $x_n$ is (finite) sequence of complex numbers we have a constant $C(p) >0$ with: $$
\frac{1}{C(p)}\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2} \leq \left(\mathbb{E}\left[\left(\sum_{n = 1}^N\epsilon_nx_n\right)^p\right]\right)^{1/p} \leq C(p)\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2}
$$ Does anyone have any ideas as to how one should properly apply this inequality?","['harmonic-analysis', 'probability-theory', 'real-analysis']"
3693131,Smallest possible dimensions of a piece of paper after one fold.,"So I've been thinking, can someone explain mathematically if I started with a square of paper dimensions 1x1, what the smallest single side dimension could be reached after one fold. My gut instinct is that after only one fold there's no way to make the paper less than half the height/width. When I say single side dimension, imagine drawing a bounding box around the folded paper and taking the smaller of the two side dimensions.","['proof-writing', 'geometry', 'puzzle']"
3693239,Jacobson radical of upper triangular matrix ring,"I do not follow this solution for the Jacobson radical of the upper triangular matrix ring $U_2(\mathbb{Z}_{63})$ . NB: in the solution, the result that “ $1-ra$ is a unit for all $r\in R$ iff $a$ belongs to every maximal left ideal in a ring $R$ “ is being used In the last paragraph it says it is sufficient for $(1-ra),(1-tc)$ to belong to $\mathbb{Z}_{63} \setminus \{0\}$ . How can this be the case when $\mathbb{Z}_{63}$ is a domain? For example $7,9 \in \mathbb{Z}_{63}$ but their product is $0$ .","['matrices', 'ring-theory', 'radicals', 'abstract-algebra']"
3693268,Dynamic Systems: Adjustment time to perturbations,"I am new to the field of dynamic systems and have what I feel is a pretty basic question. If I have a simple dynamic system $\dot{x}=-kx$ with one stable equilbrium point, and I move my solution away from the equilibrium point by a value $\Delta x$ , how long will it take for my solution to converge back to the equilibrium point? Let's call this adjustmnet time ( $\tau$ ). What happens if this is a nonlinear dynamic system, for instance $\dot{x}=-kx^3$ ? Then how long does it take? Can someone: Point me to another questions if it has already been asked Point me to a reference where I can figure this out on my own Walk me through a solution","['perturbation-theory', 'ordinary-differential-equations', 'dynamical-systems']"
3693280,Generate a Poisson random variable from a standard uniform random variable.,"I can't solve the following exercise: A random number generator generates random values $U \sim \text{U}(0,1)$ from the standard uniform distribution.  Use $U$ to generate a random variable $P \sim \text{Pois}(\lambda = 5)$ from a Poisson distribution with rate parameter equal to five. Comment: In previous tasks I was asked to use $U$ to generate an exponential random variable $E \sim \text{Exp}(\lambda)$ .  The solution was to take $E \equiv -\tfrac{1}{\lambda} \ln(1-U)$ . I think that this can be helpful because of the relation between Poisson distributions and exponential, but I'm not sure.","['statistics', 'probability']"
3693358,Wikipedia's definition of an Algebraic Space,"Wikipedia defines an algebraic space $\mathfrak{X}$ to be a sheaf on the big étale site $(\text{Sch}/S)_{et}$ , such that: There is a surjective étale morphism $h_X\to \mathfrak{X}$ The diagonal morphism $\Delta_{\mathfrak{X}/S}:\mathfrak{X}\to\mathfrak{X}\times\mathfrak{X}$ is representable. What does the first condition actually mean? Possibly it makes sense by means of the second condition, but I wasn't sure. What does it mean for a morphism onto an étale sheaf to be étale?","['definition', 'algebraic-geometry', 'sheaf-theory']"
3693397,Identify a pullback line bundle on $\mathbb{P}^1$,"Consider a degree $d$ map $f:\mathbb{P}^1 \to \mathbb{P}^m$ for $d \geq 1$ and $m \geq 2$ , together with a line bundle $\mathcal{O}_{\mathbb{P}^m}(l)$ over $\mathbb{P}^m$ for $l \geq 1$ . Then we have a pullback line bundle $f^*\mathcal{O}_{\mathbb{P}^m}(l)$ over $\mathbb{P}^1$ . Is $f^*\mathcal{O}_{\mathbb{P}^m}(l) \cong \mathcal{O}_{\mathbb{P}^1}(dl)$ always true? Let $[x_0,x_1]$ and $[y_0, \dots, y_m]$ be the homogeneous coordinates of $\mathbb{P}^1$ and $\mathbb{P}^m$ , respectively. Then one can argue that a degree $l$ homogeneous polynomial in $y_0, \dots, y_m$ pullbacks to a degree $dl$ homogeneous polynomial in $x_0$ and $x_1$ . But these may not span the global sections $H^0 (\mathbb{P}^1, \mathcal{O}_{\mathbb{P}^1}(dl))$ . For example, consider $m = 2$ and $l=3$ , and a degree $2$ map $f:\mathbb{P}^1 \to \mathbb{P}^2$ defined by $$[x_0,x_1] \mapsto [x_0^2,x_1^2,0].$$ Then the global section $x_0^5x_1$ in $H^0 (\mathbb{P}^1, \mathcal{O}_{\mathbb{P}^1}(6))$ is missing from the pullbacks of degree $3$ polynomials in $y_0,y_1,y_2$ . What's wrong with the above argument?","['pullback', 'algebraic-geometry', 'line-bundles']"
3693526,What's the relation between $\mathbf E(X)$ and $\mathbf E(e^X)$?,"Given $\mathbf E(X)=0$ and $-1\le X\le 1$ , show that $\mathbf E(\text{exp}(\sqrt{2}X))\le e^{\sqrt{2}}-\sqrt{2}$ . It seems that Jensen's inequality will help, but I have no idea. Thanks in advance.","['expected-value', 'probability-theory', 'probability', 'real-analysis']"
3693547,"Ahmed integral revisited $\int_0^1 \frac{\tan ^{-1}\left(\sqrt{x^2+4}\right)}{\left(x^2+2\right) \sqrt{x^2+4}} \, dx$","How to prove $$\small \int_0^1 \frac{\tan ^{-1}\left(\sqrt{x^2+4}\right)}{\left(x^2+2\right) \sqrt{x^2+4}} dx=-\frac{\pi \:\arctan \left(\frac{1}{\sqrt{2}}\right)}{8}+\frac{\arctan \left(\frac{1}{\sqrt{2}}\right)\arctan \left(\sqrt{2}\right)}{4}+\frac{\pi }{4}\arctan \left(\frac{1}{\sqrt{5}}\right)\;?$$ I came across this Ahmed integral on the site ""Art of problem solving"", and have found no proof so far. (These two problems seems to be related though). Any help will be appreciated!","['integration', 'definite-integrals', 'closed-form']"
3693726,A group of order $pq$,"I want to prove that if a group of order $pq$ (where $p,q$ are primes) with $p>q$ is not abelian, then $p \equiv1$ mod $q$ . I don't know if this is correct but I think I have a proof using characters: $G$ is abelian if and only if $G'=[G,G]$ is trivial. Otherwise, it should be $|G'|=p$ or $|G'|=q$ or $|G'|=pq$ . Let's see what happens.
If $|G'|=p$ , then $|G/G'|=q$ and therefore there are $q$ linear characters of $G$ . Hence $\sum_{\chi\in Irr(G)}\chi(1)^2=|G|$ implies that $q+\sum_{\chi\in Irr(G),\chi(1)\neq 1}\chi(1)^2=pq$ . We know that $\chi(1)|pq$ and so if $\chi(1)\neq 1$ it should be $\chi(1)=p$ or $\chi(1)=q$ or $\chi(1)=pq$ . Since $q<p$ , it is easily seen that for all $\chi$ (that are not of degree 1) we necessarily have $\chi(1)=q$ . Therefore if $N$ is the number of characters with $\chi(1)=q$ , we get $q+Nq^2=pq$ and therefore $p=Nq+1$ , i.e $p \equiv 1$ mod $q$ . With very similar reasoning we see that it cannot happen that $|G'|=q$ of $|G'|=pq$ and so if $G$ is not abelian, then $p \equiv1$ mod $q$ . Is the above correct? And if so, can you find a proof without using characters (maybe that's easy, but I really can't see it..)?","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
3693772,Using definition of derivative at an inequality,"The question is really simple but I'm not sure how can I prove it. Let $f : \mathbb{R} \rightarrow \mathbb{R} \phantom{2}$ a function that  verifies : $\exists\, K \in \mathbb{R^+}, \phantom{1}\forall\, x,y \in \mathbb{R}: \lvert f(y)-f(x) \rvert \le K\lvert \cos y - \cos x \rvert$ Then f is differentiable at $0$ I proved that f is a Lipschitz function since by mean value theorem $$
\forall x,y \in \mathbb{R} \phantom{3}:\frac{|\cos y - \cos x|}{|y-x|}\le 1 \implies |\cos y - \cos x|\le|x-y| 
$$ Then $$\exists K \in \mathbb{R^+} \forall x,y \in \mathbb{R}: |f(y)-f(x)| \le K|\cos y - \cos x|\le K|x-y|$$ So for $x = 0$ we have that $|f(0)-f(x)| \le K|0-x|$ But I'm stilled confused on how can I apply this inequality on $$
\lim_{x\to c} \frac{f(x) - f(c)}{x-c}
$$ And if it's differentiable what's the value of $f′(0)$ ?","['analysis', 'real-analysis', 'calculus', 'functions', 'derivatives']"
3693807,Evaluate the limit for $p<1$,"I found this limit calculation problem in a book.
For a real number $p\geq 0$ we have $$\lim_{n\rightarrow \infty}\frac{\left (1^{1^p}2^{2^p}\dots n^{n^p}\right )^{\frac{1}{n^{p+1}}}}{n^{\frac{1}{p+1}}}=e^{-\frac{1}{(p+1)^2}}$$ After taking logarithm this is equivalent to showing $$\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log k-\frac{1}{p+1}\log n\rightarrow -\frac{1}{(p+1)^2}$$ as $n\rightarrow \infty$ Now we know $$\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p \log \left (\frac{k}{n} \right )\rightarrow \int_0^1x^p\log x  \ dx=-\frac{1}{(p+1)^2}$$ To balance we have to evaluate the limit of $$\left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n$$ Now observe the sum in brackets is the error of the Riemann sum associated to the Riemann Integral $\displaystyle{\int_0^1x^p dx}$ In the interval $\left [\frac{k}{n},\frac{k+1}{n} \right ]$ if we apply MVT to the function $x^p$ we get $$\left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \left |\left (\frac{k+1}{n} \right )^p -\left (\frac{k}{n} \right )^p\right |=\frac{|p z_k^{p-1} |}{n}$$ for some $z_{k}\in \left [\frac{k}{n},\frac{k+1}{n} \right ] $ So we get $$\sup_{x\in \left [\frac{k}{n},\frac{k+1}{n} \right ]}\left |x^p-\left (\frac{k}{n} \right )^p \right |\leq \frac{p}{n}$$ if $p\geq 1$ Then we have $$\left | \sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right |=\left |\sum_{k=0}^{n-1} \int_{\frac{k}{n}}^{\frac{k+1}{n}}\left(x^p - \left ( \frac{k}{n}\right )^p \right )dx \right |\leq \frac{p}{n}$$ $$\implies \left (\sum_{k=1}^n \frac{1}{n}{\left ( \frac{k}{n}\right )}^p -\frac{1}{p+1}\right ) \log n\rightarrow 0$$ as $n\rightarrow \infty$ and we are done. I am really having trouble with the $p<1$ case. Some help will be very appreciated.","['limits', 'analysis', 'real-analysis']"
3693817,Wrong proof of $TM$ is diffeomorphic to $M\times \mathbb{R^m}$,"I want to see what's wrong in here: Let $M$ be a smooth manifold with dimension $m$ . I will show $TM$ is diffeomorphic to $M\times \mathbb{R^m}$ . proof ) Define $F:TM\rightarrow M\times \mathbb{R^m}$ by $F(p,v)=(p,v^1,...,v^m)$ where $v=v^i\frac{\partial}{\partial x^i}\in T_pM$ .
Let $(U,\phi)$ be a chart containing $p$ . Then, $(\pi^{-1}(U),\widetilde{\phi})$ is a chart containing $(p,v)$ where $\pi:TM\rightarrow M$ given by $\pi(p,v)=p$ and $\widetilde{\phi}(p,v)=(\phi(p),v^1,...,v^m)$ . And $(U\times \mathbb{R^m},\phi \times Id)$ is a chart containing $F(p,v)$ .
Using above, $(\phi\times Id)\circ F\circ \widetilde{\phi}^{-1}:\widetilde{\phi} (\pi^{-1}(U))\rightarrow \phi(U)\times \mathbb{R^m}$ is an identity map (Note that $\widetilde{\phi} (\pi^{-1}(U))$ is $\phi(U)\times \mathbb{R^m}$ by calculation.). Thus $F$ is smooth. $F^{-1}:M\times \mathbb{R^m}\rightarrow TM$ is given by $F^{-1}(p,v^1,...,v^m)=(p,v)$ where $v=v^i\frac{\partial}{\partial x^i}\in T_pM$ . With above charts, we have $\widetilde{\phi}\circ F^{-1}\circ (\phi\times Id)^{-1}:\phi(U)\times \mathbb{R^m}\rightarrow \widetilde{\phi}(\pi^{-1}(U))$ is also identity map. Thus $F^{-1}$ is smooth. $\blacksquare$ But I know $TM$ may not diffeomorphic to $M\times \mathbb{R^m}$ . What's wrong in my proof?","['smooth-manifolds', 'differential-geometry']"
3693821,How to solve this integral with transformation to polar coordinates?,"How do I determine new limits when transforming to polar coordinates. I have this example, and I don't know how to solve it correctly. $$
\iint_D \frac{\ln\left(x^2+y^2\right)}{x^2+y^2}\,dx\,dy
$$ where $D: 1\leq x^2+y^2\leq e^2.$ So I transformed $x$ and $y$ to polar coordinates: $x=r\cos\phi$ and $y=r\sin\phi$ ; $x^2+y^2=r^2\cos \phi +r^2\sin \phi=r^2.$ I got $$
\iint \frac{\ln\left(x^2+y^2\right)}{x^2+y^2}\,dx\,dy = \int d\phi \int \frac{\ln r^2}{r^2}r\,dr.
$$ My question is how to determine the new limits of integration after transforming them to polar coordinates.","['integration', 'polar-coordinates', 'definite-integrals']"
3693867,"How can i rewrite my specific $F_{n,2d}^a$ polynomial to be a sum of $(3n-4)$ squares?","So, i've been messing around with the following $n$ -variate polynomials of degree $2d$ : $$F_{n,2d}^a = \sum_{i=1}^n a_ix_i^{2d} + 2d\prod_{i=1}^n x_i^{a_i} $$ Where $\sum_{i=1}^n a_i=2d$ Now, i want to show that this $F_{n,2d}^a$ can be written at most as the sum of $3n-4$ squares. I thought of doing it per induction, but there were two problems: $F_{2,2d}^a$ can be written as 2 sums of squares (which works) , but i couldn't prove it even if i could, it doesnt work because if you presume $\sum_{i=1}^n a_i=2d$ , you cant say this about $\sum_{i=1}^{n+1} a_i$ anymore. What can I do? (I would like to show the first one anyway, just to understand whats going on. so if you have any ideas on that, let me know) thanks!!","['semidefinite-programming', 'sums-of-squares', 'polynomials', 'discrete-mathematics']"
3693872,"I don't understand $\limsup_n\:A_n$ in Probability Theory, why should it be equal to ""infinitely many $A_n$ occur""?","The definition for limsup is: $$\bigcap _{n=1}^{\infty }\bigcup _{k=n}^{\infty }\:A_k$$ So this means that $$\left(A_1\cup \:A_2\cup \:A_3\cup \:...\right)\cap \left(A_2\cup \:\:A_3\cup \:\:...\right)\cap \:\left(A_3\cup \:\:\:...\right)\cap \:...$$ which is equal to the last element of this infinite sequence (let's call it $A_{\infty }$ ) For example let $A_n\:=\:\left[0,\frac{1}{n}\right]$ , then $$\left(A_1\cup \:A_2\cup \:A_3\cup \:...\right)\cap \left(A_2\cup \:\:A_3\cup \:\:...\right)\cap \:\left(A_3\cup \:\:\:...\right)\cap \:... = \left\{0\right\}$$ as $$\left[0,\frac{1}{\infty }\right]\:=\:\left[0,\:0\right]\:=\:\left\{0\right\}$$ The problem here with Probability Theory and its understanding of limsup is that we can create such kind of scenarios that "" $A_{\infty }$ "" won't be equal to ""ifinitely many $A_n$ occur"". For example let $A_{n}=\left\{\text{person number n wins the lottery}\right\}$ , "" $A_{\infty }$ "" (and thus $\limsup_n\:A_n$ ) would mean that the ""last person of this infinite sequence will win the lottery"", and not that ""infinitely many $A_n$ occur"" which is equal to ""everyone wins"" (as this is clearly not the case here, because only the $\infty$ th person wins) I really need help to understand this concept","['limsup-and-liminf', 'probability-theory']"
3693993,How does Grinberg's theorem work?,"Grinberg's theorem is a condition used to prove the existence of an Hamilton cycle on a planar graph. It is formulated in this way: Let $G$ be a finite planar graph with a Hamiltonian cycle $C$ , with a fixed planar embedding. Denote by $ƒ_k$ and $g_k$ the number of $k$ -gonal faces of the embedding that are inside and outside of $C$ , respectively. Then $$
 \sum_{k \geq 3} (k-2)(f_k - g_k) = 0
$$ While i think i understood the definition i do not know how to apply it on a real problem. For instance, in a graph like that: how can i identify the internal/external faces of an hypothetical Hamilton cycle $C$ if what i want to do is actually find one of it(an Hamilton cycle)? I mean, the theorem should be used(as far as i understood) to prove(or disprove) the existence of an Hamilton cycle, yet the definition implies that i have to find one to use the whole theorem. Anybody can help me to understand? I'd like to see an example, even a different one from what i brought should be fine.","['graph-theory', 'hamiltonian-path', 'discrete-mathematics', 'planar-graphs']"
3694013,"If $z+\frac{1}{z}=2\cos\theta,$ where $z\in\Bbb C$, show that $\left|\frac{z^{2 n}-1}{z^{2n}+1}\right|=|\tan n\theta|$","If $z+\frac{1}{z}=2 \cos \theta,$ where $z$ is a complex number, show that $$
\left|\frac{z^{2 n}-1}{z^{2 n}+1}\right|=|\tan n \theta|
$$ My Approach: $$
\begin{array}{l}|\sin \theta|=\left|\sqrt{1-\cos ^{2} \theta}\right| \\ =\left|\sqrt{1-\left(\frac{z^{2}+1}{2z}\right)^{2}}\right| \\ =\left|\sqrt{\frac{4 z^{2}-z^{4}-2 z^{2}-1}{4 z^{2}}} \right|\\ =\left|\sqrt{\frac{-\left(z^{4}-2 z^{2}+1\right)}{4 z^{2}}}\right|=\left|\sqrt{\frac{\left(z^{2}-1\right)^{2}}{4 z^{2}}}\right| \\ =|\frac{z^{2}-1}{2 z}|\end{array}
$$ So $|\tan \theta|=\left|\frac{z^{2}-1}{z^{2}+1}\right|$ ( proven when $n = 1$ ) Is there any way to prove directly by taking $n$ ?","['trigonometry', 'solution-verification', 'complex-numbers']"
3694054,Radon-Nikodym Derivative of a Mixed Distribution,"When we have a continuous distribution $F_X(x)$ , we can take the Radon-Nikodym derivative (RND) of the probability measure with respect to Lebesgue measure to get the density $f_X(x)$ . When we have a discrete distribution, we can take the RND with respect to the counting measure to get the mass function (so ""density"" in measure theoretic probability). When we have a mixed distribution, say 50% standard normal and 50% 0s (so a jump discontinuity in the CDF at 0, see the drawing), what would be the measure to use that is some kind of hybrid of Lebesgue and counting measure? (Starts out looking normal, jumps up at 0, then gets back to looking normal.)","['lebesgue-measure', 'probability-distributions', 'density-function', 'probability-theory', 'radon-nikodym']"
3694093,Why are you less likely to roll at least 1/6 of the dice as 6 when the number of dice increases?,"So, I recently watched a V-Sauce video discussing a collaboration between Sir Isaac Newton and Samuel Pepys on a probability problem regarding the probability of rolling at least one six on six six-sided dice, compared to the probability of rolling at least 2 sixes on 12 dice or 3 sixes on 18 dice. The answer they arrived at was that the probability of rolling (N/6) 6s on N dice decreased as X increases - the odds are .6651 for 6 dice, .6187 for 12 dice, and .5973 for 18 dice. However, this seems counterintuitive to me - as the sample size of a sample grows, the more likely it is to conform to the true probability, right? That's the basis of a lot of the statistical analysis that underlies things like p-values. Why don't the probabilities increase to reflect this, instead of decreasing the way they actually do?","['statistical-inference', 'statistics', 'probability-theory', 'probability']"
3694121,Solve Polya urn with generating function?,"The (simple) Polya urn contains $a \in \mathbf N$ black and $b \in \mathbf N $ white balls at the initial time $t=0$ , and, at each time $t \in \mathbf Z_{+}$ , a ball is picked uniformly at random in the urn and put back in the urn together with a ball of the same color to give the composition of the urn at time $t+1$ . The Polya urn is very well understood : there is a.s. convergence of the proportion of black balls (as a bounded martingale) and the limit is $\beta(a,b)$ distributed. An alternative description of the process is : if $(U_t)_{t \in \mathbf Z_{+}}$ is a sequence of independent uniform random variables on $[0,1]$ , the number of blacks balls at time $t$ satisfies $$X_0=a , \text{ and } X_{t+1}=X_{t}+ 1_\left\{ U_t < \frac{X_t}{t+a+b}\right\}$$ I know two methods to get the aforementioned limiting distribution of the proportion $X_t/(t+a+b)$ : computing explicitly the marginal of $X_t$ (see Durrett, Probability, Theory and Examples, section 4.3.2), using the combinatorics of the problem, or use an alternative construction of the process (the first edition of the book on Markov chains by Levin Peres Wilmer used this derivation) also based on exchangeability. These proofs are not very robust though. My question : is it possible to give a proof based on the recursive equation in distribution displayed above (e.g. using convergence of the generating functions)?","['polya-urn-model', 'combinatorics', 'probability', 'generating-functions']"
3694124,Why do the Galois and Hecke action commute on the Picard group of the modular curve $X_1(N)$?,"In chapter 9 of Diamond-Shurman's book A First Course in Modular Forms , when they construct the Tate module associated to the modular curve $X_1(N)$ , they state that the Galois action and Hecke action on the Picard group $\mathrm{Pic}^0(X_1(N))$ commute (on page 387). While I understand why Hecke operators commute with Hecke operators, why would a Hecke operator commute with an element of the Galois group? In other words, for $T\in\mathbb{T}_{\mathbb{Z}}$ and $\sigma\in\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ , why does \begin{equation}
\sigma(T(x))=T(\sigma(x))
\end{equation} for all $x\in\mathrm{Pic}^0(X_1(N))$ ? A priori, the Galois group is noncommutative, so $\sigma\sigma'\ne\sigma'\sigma$ for all $\sigma,\sigma'\in\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ . I feel like I'm missing something basic about group actions or (Galois) modules, but I can't see what.","['modular-forms', 'algebraic-geometry', 'galois-representations']"
3694204,Verification of Integrals,"I am very new to integrals. If someone would kindly take a look at them and confirm they are set up correctly that would be great! D is the triangle with vertices $(0,0) (4,-2) (4,8)$ Evaluate $\int \int_D e^{x^2 +1}dA$ My attempt : $\int_0^4 \int_0^{2x}  e^{x^2 +1}dydx$ + $\int_0^4 \int_0^{\frac{1}{2}x}  e^{x^2 +1}dydx$ $\approx 18116212.53$ D is the region where $x \geq 0$ bounded by $z = 4-x^2-y^2$ and xy plane. Evaluate $\int \int \int_D z+2dA$ My attempt : $\int_\frac{-\pi}{2}^{\frac{\pi}{2}} \int_0^2 \int_0^{4-r^2} (z+2)r dzdrd\theta$ $= \frac{40\pi}{3}$","['integration', 'multivariable-calculus', 'calculus', 'solution-verification']"
3694239,"I can say that : according to the previous lemma we have: $ \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.$","Let $(E,\mathcal{A},\mu)$ be a finite measure space. Let $ X $ be a Banach space and $ H $ a Hilbert space. For $ t \in E $ , we set $ F_a (f)(t) = f (t) 1_{\| f \| \leq a} (t)$ Lemma :   Let $ \{x_n \} $ be a sequence of $ X $ weakly converging to $ x_\infty \in X $ . Then there is an integer $ N \geq 1 $ such that: $$\|x_\infty\|\leq 2\inf_{n\geq N}\|x_n\|$$ Let $\{f_n\}$ be a bounded sequenece in $\mathcal{L}^1_H(:=\{f:E\to H: f \text{ Bochner-integrable function}\})$ My problem is to show that: there exists a subsequence of $\{f_n^{'}\}$ of $\{f_n\}$ for all $ k\geq 1 $ it exists $ u_k \in \mathcal {L}_H^2 $ such that: $$
F_k(f_n^{'}) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text{and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n^{'}) \|_2, \qquad \forall n \geq k.
$$ My effort Let $ k \geq 1 $ , the sequence $ \{F_k (f_n) \} $ is bounded in $ \mathcal {L}_H ^2 $ , and therefore it is relatively weakly compact. There then exists (for each $ k $ ) a subsequence $ \{f_n^k\} $ of $ \{f_n \} $ and $ u_k \in \mathcal{L}_H^2 $ so that $ f_n^{k + 1} $ is a subsequence of $ \{f_n ^ k \} $ and $$
F_k (f_n ^ k) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H^2)} {\underset{n}{\longrightarrow}} u_k,
$$ and according to the previous lemma  we have: $$ \| u_k \|_2 \leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1, $$ because $ \{F_k (f_n ^ k) \} $ is a weakly convergent sequence in the Banach space $ (L^2_H, \|. \| _2) $ . We set $ f_n^{'} = f_n^n $ $ (n \geq 1) $ . So $$
F_k (f_n^{'}) \overset {\sigma (\mathcal {L}_H^2, \mathcal {L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text {and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n ^ {'}) \|_2, \qquad \forall n \geq k.
$$ Questions : I can say that : because $ \{F_k (f_n ^ k) \} $ is a weakly convergent sequence in the Banach space $ (L^2_H, \|. \| _2) $ . I can say that : according to the previous lemma  we have: $$ \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.$$ An idea please.","['hilbert-spaces', 'probability-theory', 'functional-analysis', 'measure-theory']"
3694244,"Evaluate a real integral, e.g. $\int_{0}^{\infty}\frac{x^2}{(x^2+1)(x^2+4)}\:\mathrm dx$ with complex analysis","My question is more theoretical, i.e, i cannot quiet understand the ""method"" itself. For example $$\int_{0}^{\infty}\frac{x^2}{(x^2+1)(x^2+4)}\:dx$$ I know that the fact of the denominator having no real roots and that $deg((x^2+1)(x^2+4))-deg(x^2)\geq2$ are ""important informations"" but i dont how to apply them. Can someone help me? I know that we are supposed to apply the residue Theorem after, but i cannot understand the steps in order to apply it. Thank you!","['complex-analysis', 'residue-calculus', 'improper-integrals']"
3694394,Imaginary numbers calculation for DFT,"I am trying to understand the Fourier transformation and the math behind it, so I was trying to use this formula: $$ x_k = \sum_{n=0}^{N-1} x_n e^{-\frac{j2{\pi}kn}{N}} $$ to calculate all $ x_{0\ldots3} $ for $N = 4$ and $$
x = 
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3
\end{pmatrix} = 
\begin{pmatrix} 
1 \\
2 - {j} \\
-{j} \\
-1 + 2{j}
\end{pmatrix}
$$ If I want to calculate $ x_1 $ : \begin{align}
x_1 & = x_0\cdot e^{-\frac{{j}2{\pi}1 \cdot 0}{4}} + x_1\cdot e^{-\frac{{j}2{\pi}1{\cdot}1}{4}} + x_2\cdot e^{-\frac{j2\pi1\cdot2}{4}} + x_3\cdot e^{-\frac{j2\pi1\cdot 3}{4}} \\[8pt]
x_1 & = x_0\cdot e^0 + 
x_1\cdot e^{-\frac{j\pi}{2}} + x_2\cdot e^{-j\pi} + x_3\cdot e^{-\frac{3j\pi}{2}} \\[8pt]
x_1 & = 1 + (2-j)\cdot e^{-\frac{j\pi}{2}} - j\cdot e^{-j\pi} + (-1 + 2j)\cdot e^{-\frac{3j\pi}{2}}
\end{align} so according to Wikipedia's articles for DFT this should be equal to $ x_1 = -2 - 2j $ but I am missing the last step. I got a hint that I should use the Euler's formula that $ e^{j\pi} + 1 = 0 $ but I am still unable to simplify it to $ x_1 = -2 - 2j $ [EDIT] I wanted to share my solution here, which might help some people in the future: 
According to Euler's formula: $ e^{jx} = \cos(x) + j\sin(x) $ , $ j^2 + 1 = 0 $ .
We can then simplify the following expressions to: \begin{align}
e^{-\frac{j\pi}{2}} & = \cos\Big(-\frac{\pi}{2}\Big) + j\sin\Big(-\frac{\pi}{2}\Big) = 0 - j = -j \\[6pt]
e^{-j\pi} & = \cos(-\pi) + j\sin(-\pi) = -1 + j\cdot0 = -1 \\[6pt]
e^{-\frac{3j\pi}{2}} & = \cos\Big(-\frac{3\pi}{2}\Big) + j\sin\Big(-\frac{3\pi}{2} \Big) = 0 + j\cdot1 = j \\[6pt]
e^{-2j\pi} & = \cos(-2\pi) + j\sin(-2\pi) = 1 
\end{align} Knowing that the cosinusoidal and sinusoidal functions have the same value every $ 2\pi $ and opposite sign every $ \pi $ intervals we can easily calculate the other functions if needed. $$
x_1 = 1 + (-j)(2 - j) - j(-1) + j(-1 + 2j) = 1 + j(-2 + j + 1 - 1 + 2j)
$$ $$
x_1 = 1 + j(-2 + 3j)  = 1 - 2j + 3j^2 = 1 - 2j - 3(-1) = -2 - 2j
$$","['euler-mascheroni-constant', 'fourier-analysis', 'fourier-transform', 'discrete-mathematics']"
3694401,Intuition behind Willmore energy,"I read about Willmore Energy is a quantitative measure of how much a given surface deviates from a round sphere. Also, I heard that it says that things in nature tends to change their shape in such a way that they use the least energy to survive. And it ends up to be a sphere by using Willmore energy. But how does this link to this formula? $$W=\int_SH^2dA-\int_SKdA$$ I mean, what does this has to do with all of the real-life interpretations?",['differential-geometry']
3694403,Compute the integral $\int\limits_{0}^{\pi/2} \frac{dx}{\sqrt{1 + \sin x}}$,"The original task was to find the arc length of $y = \ln(1 + \sin(x))$ where $x \in [0, \pi/2]$ . Using the general formula for arc length of $y = f(x)$ I've got $\sqrt2 \int\limits_0^{\pi/2}\frac{dx}{\sqrt{1+ \sin x}}$ . I've tried to make a substitution $\sqrt{1 + \sin x} = t$ and $dx = \frac{2t\,dt}{\sqrt{1 - (t^2 - 1)^2}}$ which gives an integral $2\sqrt{2} \int\limits_1^{\sqrt{2}}\frac{dt}{t\sqrt{2 - t^2}}$ . This integral might be not so difficult to compute, but the strange thing is that this task gives only three points (max is $38$ ) for correct solution. I think it became too hard and other (and much easier) solution should exist. So, the question is the following: is there any (easier) method to compute this integral (or may be some easier way to find the arc length of the curve)?","['integration', 'definite-integrals']"
3694447,"there exists a sequence of simple functions $\{g_n\}$ bounded in $L^1$, such that $f_n-g_n\to 0 \text{ a.e and in } L^1.$","Let $(E,\mathcal{A},\mu)$ be a finite measure space, and $\{f_n\}$ be a sequence bounded in $L^1$ . Why does there exist a sequence of simple functions $\{g_n\}$ bounded in $L^1$ , such that: $$
f_n-g_n\to 0 \text{ a.e and in } L^1.
$$","['measure-theory', 'probability-theory']"
3694450,$\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!}$=$\frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}}$,"I am told to prove that : $$\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!} = \frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}}$$ where $H_n(x)$ is Hermite polynomial.I am wondering how to prove it.please help me how to prove this.
Thanks in advance!","['special-functions', 'ordinary-differential-equations', 'calculus', 'functions', 'hermite-polynomials']"
3694475,How does $H$ act on $G^t$ in the wreath product $G^t \wr H$?,"I'm reading this expository paper about group theory in the Rubik's cube. I'm a little confused by the definition of the wreath product in this paper. Example 3.12 on page 12 states that the elements of the wreath product $(\mathbb{Z}/2\mathbb{Z})^3 \wr S_3$ for the set X={0, 1, 2} are { $(0,0,0) \rho, (1,0,0) \rho, (0, 1, 0) \rho, (0, 0, 1) \rho, (1, 1, 0) \rho, (0, 1, 1) \rho, (1, 0, 1) \rho, (1, 1, 1) \rho$ } where $\rho$ is an element of $S_3$ . My question is: is $\rho$ just an arbitrary element in $S_3$ , or is $\rho$ somehow specially chosen? I don't understand how $S_3$ acts on $(\mathbb{Z}/2\mathbb{Z})^3$ in the same way as $S_3$ acts on the set X. More generally, I don't understand how, for a direct product $G^t \wr H$ , where $H$ acts on a set of size $t$ , how $H$ acts on $G^t$ . Specifically, which elements of $H$ are acting on $G^t$ ? P.s I'd really appreciate it if answers were geared towards a high school student with little formal abstract algebra background. Thank you. :)","['wreath-product', 'group-theory', 'abstract-algebra', 'rubiks-cube']"
3694500,What book would you recommend to significantly improve my problem solving skills? [duplicate],"This question already has answers here : How can I improve my problem solving/critical thinking skills and learn higher math? (12 answers) Closed 9 months ago . I am a straight-A student (going to the ninth grade) and do nearly perfectly in math, the problem is that my school (like many other schools I suppose) makes you memorize the steps, the formulas, etc. and just apply that to similar questions, and since I’ll probably be opting for IGCSE and eventually A levels, I find it necessary to significantly improve my critical thinking skills because I never had adequate practice for that. I’m sorry for all that background, but I would like your recommendation of books/websites that would help me. Do you think these books are appropriate: How to Solve It by Polya The Art of Problem Solving Vol. 1: The Basics & How to Solve It Vol. 2: And Beyond Problem Solving Strategies by Engel Problem-Solving through Problems Mathematical Discovery: On Understanding, Learning and Teaching Problem Solving Do you think that Solving Mathematical Problems by Tao is enough for me to significantly improve?","['algebra-precalculus', 'problem-solving', 'book-recommendation', 'reference-request']"
3694541,Cartesian Product of an indexed family,This is an exercise I'm trying to do but I don't know if $A=\emptyset$ makes sense: $$ A=\emptyset\ and\ \{X_\alpha:\alpha\in A\} $$ Calculate $ \prod \limits_{\alpha \in A}X\alpha$ Does it makes sense? Any help would be great!,['elementary-set-theory']
3694572,Limit of sum of bounded iid random variables,"Let $(V^i_N)_{i\ge 1}$ be a sequence of i.i.d. real random variables such that $|V^i_N|\le c$ for some $c$ independent of $N$ . Suppose that each $V^i_N$ converges in law to some random variable $V^i$ with $E[V^i]=m$ . Then $
\lim\limits_{N\to \infty} \frac 1N \sum\limits_{i=1}^N V^i_N=m
$ in law. It seems like it should be solved by the law of large numbers, but they don't have the same mean, so I don't know how to use it here. Thanks for your answers!","['convergence-divergence', 'law-of-large-numbers', 'probability-theory']"
3694581,Probability of winning high-low with full deck,"I play a game on a site I use that's a version of the casino game of ""High Low."" A standard 52-card deck is shuffled, and the top card is revealed. The player guesses whether the next card on the top of the deck is higher or lower than the revealed card. This continues iteratively until either (1) the player guesses wrong or (2) there are no more cards left to reveal. If the next card is the same as the revealed card, it is a ""freebie"" for the player and the game continues. Suit is not relevant, just value, with 10 < J < Q < K < A. There is no card replacement. What I am wondering is what is the probability of ""winning the game"" (guessing correctly 51 times in a row) under different strategies. I am just not sure how to abstract the problem. I figure that once you settle on a strategy, it really becomes a question of how the deck is shuffled, i.e. the probability of a specific type of permutation of the cards. These are the two strategies I'm interested in: A ""naive"" strategy where you always guess higher for 2 through 7, always guess lower for 9 through A, and flip a coin for 8. I presume this is an easier question to answer. A ""card counter"" strategy where you keep track of all the cards you have seen, and then choose the more likely option. For example, if the first card is a 2, and the second card is an 8, you would guess the third card is higher since it has a slightly higher probability (24/50 vs. 23/50). For example, imagine the deck was simply sequential (2, 2, 2, 2, 3, 3, ..., K, A, A, A, A). The naive strategy would guess correctly until you got to the first 9, while the card counter strategy would win this game. I was able to make a simulation of the game and the strategies in Python very easily, which provided interesting results for probabilities of lower scores, but because the probability of getting all 51 is correct is so low it doesn't converge meaningfully for higher scores even after millions of trials. How could I go about abstracting the game to determine this probability? The number of permutations of the deck is large but finite, so it seems like a straightforward combinatorics question to me, but I'm getting tripped up on how each element needs to relate to all of those before it and also incorporating the coin flips. I thought about a Markov model but the probability of the next card depends on more than just the last card flipped over. Is this even a tractable question?","['stochastic-processes', 'combinatorics', 'card-games', 'probability']"
3694593,"Prove that a torus triangulation cannot have degrees of vertices $5, 7, 6, 6, 6, 6, \ldots$","I found one rather interesting but intractable topology problem. Prove that a torus triangulation cannot have degrees of vertices $5, 7, 6, 6, 6, 6, \ldots$ Despite various attempts to contract the graph or reduce it to irreducible triangulation, nothing came of it. What can you recommend?","['graph-theory', 'geometric-topology', 'geometry']"
3694671,"if a scatter plot is above a given equation, is it a bad fit","So, a friend of mine asked for help with a question concerning scatter plots. She graphed it out and looked like To me, it looked like a good fit. However, other friends said that it was wasn't a good fit because all the data is above the equation and said ""you want the data to be evenly spread above and below the line"".
It has been a while since I've done scatter plots, but I want to know the answer to my dumb question: If all the given data is above a given model but relatively close to the given model, is it still a bad fit because the data isn't spread evenly above and below the data? Thank you so much! It's just a dumb question I have and I'm just curious as I haven't done scatter plots in a very long time :)",['statistics']
3694730,Sigma algebra generated by product of random variable,"This question relates to this .
Let $X,Y$ be two (real) random variables on $(\Omega,F)$ . It was stated that $$\sigma(XY)\subseteq \sigma(X,Y).$$ The argument used is as follows: let $f(x,y)=xy$ be a function and $B$ any Borel-measurable set. Then $$\{XY\in B\}=\{(X,Y)\in f^{-1}(B)\}$$ and this latter is in $\sigma(X,Y)$ (or, is $\sigma(X,Y)$ -measurable). Can someone clarify me this equality? Is this argument correct? My attempt $$XY(w)\in B \iff [f(X,Y)](w)\in B$$ By definition, $f^{-1}(B)=\{x:f(x)\in B \}$ . So $$[f(X,Y)](w)\in B\iff (X,Y)(w)\in f^{-1}(B)$$ The direction ( $\implies$ ) of the last equivalence is not clear to me","['measure-theory', 'probability-theory']"
3694734,Hamiltonian Cycle Problem,"At the moment I'm trying to prove the statement: $K_n$ is an edge disjoint union of Hamiltonian cycles when $n$ is odd. ($K_n$ is the complete graph with $n$ vertices) So far, I think I've come up with a proof. We know the total number of edges in $K_n$ is $n(n-1)/2$ (or $n \choose 2$) and we can split our graph into individual Hamiltonian cycles of degree 2. We also know that for n vertices all having degree 2, there must consequently be $n$ edges. Thus we write $n(n-1)/2 = n + n + ... n$ (here I'm just splitting $K_n$'s edges into some number of distinct Hamiltonian paths) and the deduction that $n$ must be odd follows easily. However, the assumption I made - that we can always split $K_n$ into Hamiltonian paths of degree 2 if $K_n$ can be written as a disjoint union described above - I'm having trouble proving. I've only relied on trying different values for $n$ trials and it hasn't faltered yet. So, I'm asking: If it is true, how do you prove that if $K_n$ can be split into distinct Hamiltonian cycles, it can be split in such a way that each Hamiltonian cycle is of degree 2?","['graph-theory', 'discrete-mathematics']"
3694813,"Exercise 5.22 in Brezis, ""Functional Analysis Sobolev Spaces and Partial Differential Equations"".","Let $H$ be a Hilbert space, $C\subseteq H$ a nonempty closed convex set and $T:C\to C$ a nonlinear contraction, that is $$
(*)\qquad|Tu - Tv| \leq |u-v|.
$$ Let $(u_n)$ be a sequence in $C$ such that $u_n\rightharpoonup u$ weakly in $H$ and $u_n - Tu_n\to f$ strongly in $H$ . I want to prove that $f = u - Tu$ . My attempt : From inequality $(*)$ it is easy to prove that $$
((w-Tw)-(v-Tv),w-v)\geq 0, \qquad \forall w,v\in C.
$$ Put $w = u_n$ , then as $u_n - v\rightharpoonup u-v$ weakly and $u_n - Tu_n - (v - T)\to f-(v-T)$ strongly, we obtain that $$
(**)\qquad (f-(v-Tv),u-v)\geq 0, \qquad \forall v\in C.
$$ Because $C$ is convex we have that $u\in C$ . I believe that from inequality $(**)$ we can prove that $(u,f)$ belongs to the graph of $I-T$ , but I'm stuck here. Maybe my attempt is not the right way to prove the result, so any help willbe appreciated. The book then asks to use this result to prove that if $C$ is bounded, then $T$ has a fixed point (this is the reason I'm adding the ""fixed-point-theorems"" tag), but this is a simple consequence of the Banach fixed point theorem and the above result.","['hilbert-spaces', 'fixed-point-theorems', 'functional-analysis', 'weak-convergence']"
3694814,How to show that a certain number is not divisible by another in a proof,"I am given the prompt: ""Let $x$ and $y$ represent two integers such that their product $xy$ is divisible by 3. Then at least one of the two integers is divisible by 3."" I am to prove this using contraposition. So far, I have, ""If neither x nor y are divisible by 3, then their product xy is not divisible by 3."" My problem comes in when trying to prove this. I am not sure how I am supposed to say that a number is not divisible by 3 in a useful way that I can use in the proof. How can you show that x or y is not divisible by 3 so that I can show that their product is also not divisible by 3?","['elementary-number-theory', 'discrete-mathematics']"
3694859,Boundary of union of open subsets,"Let $X$ be a topological space and each $V_i \subset X$ be an open subset of $X$ , where $i \in I$ . Denote $V_I = \{V_i : i \in I\}$ . Below I'll show that $$(*) \quad \quad \partial \left(\bigcup V_I\right) = \overline{\bigcup \partial V_I} \setminus \bigcup V_I $$ provided $X$ is locally connected, or $V_I$ is locally finite. The problem Does this equation hold in arbitrary topological spaces without restrictions? Also of interest are other conditions under which this equation holds. Note For an arbitrary collection $V_I$ in an arbitrary topological space, $$\overline{\bigcup \overline{V_I}} = \overline{\bigcup V_I}$$ Hence, we always have $$
\begin{aligned}
\partial \left(\bigcup V_I\right) & = \overline{\bigcup V_I} \setminus \bigcup V_I \\
{} & = \overline{\bigcup \overline{V_I}} \setminus \bigcup V_I \\
{} & \supset \overline{\bigcup \partial V_I} \setminus \bigcup V_I 
\end{aligned}
$$ Theorem A Let $X$ be a topological space, $U, V \subset X$ both be open, and $U$ be connected. Then $U \cap V \neq \emptyset$ and $U \setminus V \neq \emptyset$ if and only if $U \cap \partial V \neq \emptyset$ . Proof A Suppose $U \cap \partial V = \emptyset$ . Then $U = (U \cap V) \cup (U \setminus \overline{V})$ , and these subsets are disjoint. Since $U$ is connected, either $U \cap V = \emptyset$ , or $U \setminus \overline{V} = \emptyset$ . Because of the assumption, the latter is equivalent to $U \setminus V = \emptyset$ . Suppose $U \cap \partial V \neq \emptyset$ . Then $U \cap \overline{V} \cap \overline{X \setminus V} \neq \emptyset$ , which implies $U \cap \overline{V} \neq \emptyset$ and $U \cap \overline{X \setminus V} \neq \emptyset$ . Since $U$ is open, $U \cap \overline{V} = \emptyset \iff U \cap V \neq \emptyset$ . Since $V$ is open, $U \setminus V = U \cap (X \setminus V) = U \cap \overline{X \setminus V} \neq \emptyset$ . Theorem B Let $(X, \mathcal{T})$ be a locally connected topological space, and $V_I$ be as in the problem description. Then $(*)$ holds. Proof B Let $U = \bigcup V_I$ , and denote by $\mathcal{T}^*(x)$ the connected open neighborhoods of $x$ . By Theorem A, $$
\begin{aligned}
{} & x \in \partial U \\
\iff & x \in \overline{U} \setminus U \\
\iff & x \in \overline{\bigcup V_I} \setminus U \\
\iff & \forall W \in \mathcal{T}^*(x) : W \cap \bigcup V_I \neq \emptyset \land x \in X \setminus U \\
\iff & \forall W \in \mathcal{T}^*(x) : \exists i \in I: W \cap V_i \neq \emptyset \land x \in X \setminus U \\
\iff & \forall W \in \mathcal{T}^*(x) : \exists i \in I: W \cap V_i \neq \emptyset \land W \setminus V_i \neq \emptyset \land x \in X \setminus U \\
\iff & \forall W \in \mathcal{T}^*(x) : \exists i \in I: W \cap \partial V_i \neq \emptyset \land x \in X \setminus U \\
\iff & x \in \overline{\bigcup \partial V_I} \setminus U.
\end{aligned}
$$ Theorem C Let $(X, \mathcal{T})$ be a topological space, and $V_I$ be as in the problem description, and also locally finite. Then $(*)$ holds. Proof C Let $U = \bigcup V_I$ . For a locally finite collection (open subset or not), it holds that $$\overline{\bigcup V_I} = \bigcup \overline{V_I}.$$ Therefore $$
\begin{aligned}
\partial U & = \overline{U} \setminus U \\
{} & = \overline{\bigcup V_I} \setminus U \\
{} & = \bigcup \overline{V_I} \setminus U \\
{} & = \bigcup \{\overline{V_i} : i \in I\} \setminus U \\
{} & = \bigcup \{\overline{V_i} \setminus V_i : i \in I\} \setminus U \\
{} & = \bigcup \partial V_I \setminus U \\
{} & = \overline{\bigcup \partial V_I} \setminus U.
\end{aligned}
$$",['general-topology']
3694866,Fields involving NaN,"Motivation NaN as a value exists in IEEE floating-point numbers. Since every operation involving NaN has NaN as the outcome, IEEE floating-point numbers are not fields. I want to define a new algebraic structure so NaN could be encapsulated. Definitions A field $(F,+,\times)$ is NaNable if there exists a binary operation $\sim$ such that $(F \cup \{\text{NaN}\}, \sim, +)$ is a field, where $\text{NaN}$ is the identity element of $\sim$ . $(F \cup \{\text{NaN}\}, \sim, +,\times)$ is a NaNification of $(F,+,\times)$ , and is a NaN-field . Examples Examples of NaNable finite fields include $\mathbb{Z}_2$ , $\mathbb{Z}_3$ , $\mathbb{Z}_7$ , and $\mathbb{Z}_{31}$ . The operation tables of NaNification of $\mathbb{Z}_3$ is: $$
\begin{array}{c|cccc}
\sim & \text{NaN} & 0 & 1 & 2 \\ \hline
\text{NaN} & \text{NaN} & 0 & 1 & 2 \\
0 & 0 & \text{NaN} & 2 & 1 \\
1 & 1 & 2 & \text{NaN} & 0 \\
2 & 2 & 1 & 0 & \text{NaN} \\
\end{array}
$$ $$
\begin{array}{c|cccc}
+ & \text{NaN} & 0 & 1 & 2 \\ \hline
\text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} \\
0 & \text{NaN} & 0 & 1 & 2 \\
1 & \text{NaN} & 1 & 2 & 0 \\
2 & \text{NaN} & 2 & 0 & 1 \\
\end{array}
$$ $$
\begin{array}{c|ccc}
\times & 0 & 1 & 2 \\ \hline
0 & 0 & 0 & 0 \\
1 & 0 & 1 & 2 \\
2 & 0 & 2 & 1 \\
\end{array}
$$ Note that multiplication involving $\text{NaN}$ is not defined yet. Questions Is there a notable example of NaNable infinite field? Is there a dedicated name for $\sim$ ? Not as a symbol, but as an operation? It turns out that if every multiplication involving $\text{NaN}$ is defined as $\text{NaN}$ , distributivity is satisfied in the NaN-field. Does this mean I should define it accordingly?",['abstract-algebra']
3694921,The number of unitary circulant matrices over a finite field $\mathbb{F}_{q^2}$,"Suppose $\mathbb{F}=\mathbb{F}_{q^2}$ , where $q$ is a prime power. The conjugate of elements in $\mathbb{F}$ is defined by $\overline{x}=x^q$ . I need to find the number of $n\times n$ unitary circulant matrices over $\mathbb{F}$ . The number of invertible circulant matrices over a finite field can be seen elsewhere, such as when $n,q$ coprime and my question when $n=\operatorname{char} q$ . Is there any better method to calculate this number other than considering each entry? Added on 30 May 2020: Let $C$ be the subgroup of $\operatorname{GL}_n(q^2)$ of all circulant matrices. Is $C\operatorname{GU}_n(q)$ a subgroup of $\operatorname{GL}_n(q^2)$ ? That is, is $C\operatorname{GU}_n(q)=\operatorname{GU}_n(q)C$ ? If that is correct then $C\operatorname{GU}_n(q)=\operatorname{GL}_n(q^2)$ and so $|C\cap\operatorname{GU}_n(q)|$ follows. Here we denote by $\operatorname{GU}_n(q)$ the general unitary group over $\mathbb{F}_{q^2}$ .","['unitary-matrices', 'finite-fields', 'matrices', 'combinatorics', 'circulant-matrices']"
3695129,Does there exist an infinite set S that is closed under infinite unions but not finite unions?,"Does there exist an infinite set $S$ of sets, such that for every infinite subset $I$ of $S$ , $\bigcup I \in S$ , but $S$ is not closed under finite unions?",['elementary-set-theory']
3695130,Find all possible positive integers $x$ and $y$ such that the equation: $(x+y)(x-y)=\frac{(y+1)(y-1)}{24}$ is satisfied.,"My approach so far: The given equation can be rewritten as: $x^2 -y^2=\frac{y^2 -1}{24}.$ This gives $24x^2 +1=25y^2=(5y)^2.$ So $(24x^2+1)$ must also be a perfect square. This implies $x=0, 1$ is two such possible values for $x$ . As $x>0$ , so $x=1$ is a possible solution. Corresponding to this, we get $y=1$ . But how to check does there exist any other solutions or not? Please suggest.. Thanks in advance.","['number-theory', 'diophantine-equations']"
3695139,"Why are picewise continuous functions on $[a,b]$ bounded?","Consider a picewise continuous function $f:[a,b] \to \mathbb{R}$ , i.e there are $a=t_0<\dots <t_n=b$ such that $f$ is continuous on each open interval $(t_i,t_{i+1})$ and the limits $\lim\limits_{x \uparrow t_i}f(x)$ and $\lim\limits_{x \downarrow t_i}f(x)$ exists for all $i=1,\dots,n$ . Why does there exist a constant $M>0$ such that $|f(x)| \leq M$ for all $x \in [a,b]$ ?","['analysis', 'calculus-of-variations']"
3695151,"Is $(X, Y)$ always absolutely continuous with respect to $P_X \otimes P_Y$?","Definitions: Let $X: (\Omega, \mathcal A) \to (\mathbb R, \mathcal B)$ be a random variable on the probability space $(\Omega, \mathcal A, P)$ and define its distribution as the probability measure $P_X(B) = P(X \in B)$ on $\mathcal B$ . ( $\mathcal B$ is the Borel sigma algebra). A random variable is absolutely continuous with respect to a measure $\mu$ if its distribution is, i.e. $P_X(B)=0$ for all $B \in \mathcal B$ with $\mu(B) = 0.$ $X$ may not be absolutely continuous with respect to the Lebesgue measure $\lambda$ but it is always absolutely continuous with respect to $P_X$ . Now let $(X,Y): (\Omega, \mathcal A) \to (\mathbb R^2, \mathcal B^2)$ be a random vector with distribution $P_{X, Y}((X, Y) \in B)$ , $B \in \mathcal B^2$ . By the same reasoning as before, $(X,Y)$ is abolutely continous with respect to $P_{X, Y}$ even if it is not absolutely continuous with respect to $\lambda^2$ . Question: Will $(X, Y)$ always be absolutely continuous with respect to the product measure $P_X \otimes P_Y$ ? What I did: We need to verify that $P_{X, Y}(B) = 0$ whenever $(P_X \otimes P_Y)(B)=0$ . If $B = B_1 \times B_2$ is a rectangular set then this is clearly true because $(P_X \otimes P_Y)(B)=0$ implies $P_X(B_1) = 0$ or $P_Y(B_2)=0$ ( $P_X(B_1) = 0$ , say) and then $$P_{X, Y}(B) = P((X, Y) \in B_1 \times B_2) = P(X \in B_1, Y \in B_2) \le P(X \in B_1) = 0.$$ But for non-rectangular sets I'm not sure how to proceed.","['measure-theory', 'lebesgue-integral', 'real-analysis', 'probability-theory', 'probability']"
3695175,"Why does Stolz- Cesaro fail to evaluate the limit of $\dfrac{n + n^2 + n^3 + n^4 + \ldots + n^n}{1^n + 2^n + 3^n + 4^n + \ldots +n^n}$, [duplicate]","This question already has answers here : Evaluate $\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.$ (3 answers) Closed 4 years ago . I need to find the limit of the sequence $\dfrac{n + n^2 + n^3 + n^4  + \ldots + n^n}{1^n + 2^n + 3^n + 4^n + \ldots +n^n}$ , My strategy is to use Stolz's Cesaro theorem for this sequence. Now, the numerator is given by : $x_r = n^1+ n^2 +n^3 + \ldots +n^r$ , so $x_{n+1} - x_{n} = n^{n+1}$ Similarly for denominator $y_r = 1^n + 2^n + 3^n +\ldots +r^n$ , so $y_{n+1}- y_{n} = (n+1)^n$ Using Stolz Cesaro, this limit is equivalent to $\displaystyle \lim \dfrac{n^{n+1}}{(n + 1)^n}$ , which diverges to $ +\infty$ , However ans given to me is $\dfrac{e-1}{e}$ , Can anyone tell where is the error in my solution ? Thanks.","['limits', 'sequences-and-series', 'real-analysis']"
3695236,Combinatorial identity: $\sum_{i=0}^{k}\binom{n}{i}p^{i}q^{n-i}+ \sum_{i=k}^{n-1}\binom{i}{k}p^{k+1}q^{i-k}=1$.,"While answering a recent question I came across an unexpected identity: $$
\sum_{i=0}^{k}\binom{n}{i}p^{i}q^{n-i}+
\sum_{i=k}^{n-1}\binom{i}{k}p^{k+1}q^{i-k}=1.\tag1
$$ valid provided that $p+q=1$ . The identity (1) can be also written as: $$
\sum_{i=k}^{n}\binom{n}{i}p^{i}q^{n-i-1}=\sum_{i=k}^{n}\binom{i-1}{k-1} p^{k}q^{i-k}.\tag2
$$ Is there a simple combinatorial (probabilistic) explanation of the result for $k<n$ ?","['binomial-coefficients', 'combinatorics', 'probability']"
3695247,General question about derivatives.,"Consider a differentiable function $f:\mathbb{R} \rightarrow \mathbb{R}$ . The derivative of the function at any point can be written as: \begin{align*}
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{align*} Suppose we have a constant $c > 0$ , is it true that: \begin{align*}
f'(x) = \lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} \hspace{3ex}?
\end{align*} Since when dividing $h$ by some constant in the numerator, it will still become arbitrarily small. Or does it follow that: \begin{align*}
\lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} = \frac{1}{c}\lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{\frac{h}{c}} = \frac{f'(x)}{c} \hspace{3ex} ? 
\end{align*} I think that the second case is correct, but I still wanted to be 100% sure.","['limits', 'derivatives']"
3695332,Question from PRMO 2019.,"An ant leaves the anthill for its morning exercise. It walks 4 feet east and then makes a 160° turn 
to the right and walks 4 more feet. It then makes another 160° turn to the right and walks 4 more 
feet. If the ant continues this pattern until it reaches the anthill again, what is the distance in feet it would have travelled? Well I have the solution but I am not able to understand it.
Let $A_0 (0,0)$ , $A_1 (4\cos0^\circ, 4\sin0^\circ)$ , $A_2 (4\cos0^\circ + 4\cos160^\circ, 4\sin 0^\circ + 4\sin160^\circ)$ ,......, $A_n = (0,0)$ which gives $$4(\cos 0^\circ + \cos160^\circ +......+\cos(160(n-1))^\circ) = 0$$ and $$4(\sin 0^\circ +\sin 160^\circ +..... +\sin(160(n-1))^\circ) = 0$$ which in turn, gives $$\sin((80n)^\circ) = 0$$ $$n = 9$$ Then distance covered = $4×9$ = $36\space \text{feet}$ . Please explain how do we get that $\cos 0$ and $\cos 60$ etc. in it.
This question has been asked in PRMO 2019 in India","['coordinate-systems', 'trigonometry', 'geometry']"
3695405,Examples in number theory where a heuristic argument fails,"Many conjectures in number theory are motivated by heuristic arguments, and many results that are known to be true can be predicted by heuristic arguments. To give an example, consider the Euler totient function $\phi(n) = \lvert \{ x \leq n \mid (x,n) = 1  \} \rvert$ . If we try to estimate the size of $\sum_{n \leq X} \phi(n)$ as $X \rightarrow \infty$ we might loosely argue as follows: By the definition of $\phi$ we expect $\sum_{n \leq X} \phi(n)$ to be of order $X^2$ with density given by the probability of two random numbers $m,n\leq X$ being coprime. Assuming that the prime factors of a given number are random we might estimate this probability to be $\frac{1}{2} \prod_{p \leq X} \left(1-\frac{1}{p^2}\right)$ , where the factor of $1/2$ appears because we need to attribute for counting both the pair $(m,n)$ and $(n,m)$ In fact, a fairly basic argument shows that $$
\sum_{n \leq X} \phi(n) \sim \frac{3}{\pi^2}X^2, 
$$ where we note that $$
\frac{1}{2} \prod_{p}\left(1-\frac{1}{p^2} \right) = \frac{1}{2\cdot\zeta(2)} = \frac{3}{\pi^2},
$$ so the heuristic argument predicted the correct asymptotic. My question is, are there examples where a similar kind of argument does not predict the right answer? I would expect this to happen, especially for more subtle examples since results such as Chebyshev's bias show that the distribution of primes is not as uniform as one might expect, however I am not aware of any explicit example.","['analytic-number-theory', 'number-theory', 'combinatorial-number-theory', 'big-list']"
3695443,When and why do formulae involving sums over $x_i$ change to formulae involving $X$ in statistics? Specifically when dealing with likelihoods.,"I've been reading up on stats recently and a question I'm working through involves calculating the log-likelihood of a distribution w.r.t a parameter $\beta$ . From my understanding, for some probability density function $f(x)$ that depends on a parameter $\beta$ , the likelihood is defined as $$[1] \qquad L(\beta) = f(x_1|\beta)\times f(x_2|\beta)\times ... f(x_n|\beta) = \prod_i^n f(x_i|\beta) $$ and the log likelihood as $$[2] \qquad l(\beta) = \sum_i^n \log[f(x_i|\beta)] $$ The answer to the question then goes on to declare $$[3] \qquad l(\beta) = n \log[f(X|\beta)] $$ My question is, why can you change from a sum over $x_i$ in [2] to just a $X$ in [3]? Is [3] just a short hand for [2] or is there an important statistical concept or convention that I've not encountered? From reading books and online searches it seems to be something to do with considering the whole distribution of $X$ , but I've not encountered a proper explanation of this or an intuitive explanation. My intuition is that [3] is wrong and that you can only sum over the $x_i$ 's if $x_1=x_2=...=x_n$ , but then I'm still confused as to why the $x$ 's would change to an $X$ . Thanks in advance. --- Edit with more context ---
Thanks for people's help so far. I think i need to explain my question a bit better so I'm going to add some context of the problem I'm trying to solve. The problem that lead me to ask this question was about deriving the Cramer-Rao lower bound using a formula involving the second derivative of $\log[f(x|\beta)]$ . From the book I'm using, I have the CRLB as $$[4] \qquad V(\hat{\beta}) \geq \frac{1}{I(\beta)} $$ and the information as $$[5] \qquad I(\beta) = n i(\beta) = E[-l''(\beta)] = E[U(\beta)^2] $$ I also have some extra information from the question, $$[5] \qquad \frac{d}{d\beta}\log[f(x|\beta)] = \frac{1}{\beta} + log[x] $$ from this I can get the second derivative $$[6] \qquad \frac{d^2}{d\beta^2}\log[f(x|\beta)] = \frac{-1}{\beta^2} $$ This is where I got stuck. From looking at the information given to me, I'm pretty sure I have to use the $I(\beta) = E[-l''(\beta)]$ version of [5] to find the CRLB. And they've given [6] which heavily implies I have to use [2] to find the answer. My logic for the next step was $$[7] \qquad I(\beta) = E\left[-\frac{d^2}{d\beta^2}\left(\sum_i^n \log[f(x_i|\beta)]\right)\right] $$ and you can put the derivative inside the sum to get $$[8] \qquad I(\beta) = E\left[-\sum_i^n\left(\frac{d^2}{d\beta^2}\log[f(x_i|\beta)]\right)\right] $$ Here is where I got stuck, I'm don't know if I can use [6] to solve [8] as [6] involves an $x$ , whereas [8] involves an $x_i$ . I have the answer for this question provided to me, so I looked there for guidance but it was pretty unhelpful. I've copied it below in case it's useful to you guys book answer The CRLB is $\frac{1}{I(\beta)}$ and $I(\beta)=E(-l''(\beta)) $ . so $$ CRLB = \frac{-1}{n E\left(\frac{d^2}{d\beta^2}\log[f(X|\beta)]\right)} = \frac{-1}{n\frac{-1}{\beta^2}} = \frac{\beta^2}{n} $$ I'll call the last equation above [BA] for ""book answer"". I have a few questions about [BA] I've been dealing with $f(x|\beta)$ throughout the question, why does it change to $f(X|\beta)$ now? Also, if [8] is correct, why does $x_i$ change to $X$ ? Where does the $n$ in [BA] come from? I tried working backwards from [BA] towards my equation [7], that's where I got [3] from originally. I think that I'm not understanding some part of the notation regarding $x_i$ $x$ and $X$ . My current thinking is that $X$ is a random variable that has some associated p.d.f, $x_i$ is the $i^{th}$ ""draw"" from $X$ and $x$ are all of the ""draws"" from $X$ collected in a vector. But I'm pretty sure this must be wrong. Thanks again for your help :-)","['statistical-inference', 'statistics', 'log-likelihood', 'density-function']"
3695494,"Question on paper of Mazur, Tate, Teitelbaum and $p$-adic L functions of modular forms","I'm trying to fill in the details in proposition 14 of this paper by Mazur, Tate, and Teitelbaum. In particular, I'd like to understand the following. Let $f$ be a cuspidal eigenform of weight $k$ and level $\Gamma_1(N)$ , with nebentypus $\epsilon$ . Fix a prime $p\not\mid N$ and suppose that the polynomial $x^2-a_px+\epsilon(p)p^{k-1}$ has an ""allowable root"" $\alpha$ (i.e., $ord_p(\alpha)<k-1$ ), where $a_p$ is the $T_p$ -eigenvalue of $f$ . Let $\chi$ be a $p$ -adic character of conductor $p^n$ . I am trying to fill in the details on their claim that $$\tag{1}
L_p(f,\alpha,\chi)=\frac{1}{\alpha^n}\frac{p^n}{\tau(\overline{\chi})}L(f_{\overline{\chi}},1)
$$ where the $L$ function on the right is the classical $L$ -function of the twisted form $f_{\overline{\chi}}(z)=\sum \overline{\chi}(n)a_nq^n$ , and $\tau$ is a Gauss sum. In this case, they state in the article that (1) is a ""straightforward computation"" and cite a few earlier formulas, but I'm clearly having some trouble doing this computation. In the article, they define the $p$ -adic $L$ -function by $$
L_p(f,\alpha,\chi):= \int_{\mathbb{Z}_p^\times}\chi d\mu_{f,\alpha},
$$ where $\mu_{f,\alpha}$ is the measure on $\mathbb{Z}_p^\times$ defined in terms of modular symbols $\lambda_{f,P}$ ( $P$ is a one variable complex polynomial): $$
 \mu_{f,\alpha}(P;a,p^n)=\frac{1}{\alpha^n}\lambda_{f,P}(a,p^n)-\frac{\epsilon(p)p^{k-2}}{\alpha^{n+1}}\lambda_{f,P}(a,p^{n-1}).
$$ Essentially, this is the measure of the function $P$ on the open set $a+p^n\mathbb{Z}_p$ . In other words, the integral satisfies: $$
\int_{a+p^n\mathbb{Z}_p}x^j d\mu_{f,\alpha}=\mu_{f,\alpha}(z^j;a,p^n).
$$ So we can compute the $p$ -adic $L$ -function by computing modular symbols. My attempt at (1) is as follows. By 8.2., 8.5, and 8.6 in the article (I've put these below), it's easy to see that $$
L(f_{\overline{\chi}},1)=\lambda_{f_{\overline{\chi}},1}(0,1),
$$ so we want to relate $L_p(f,\alpha,\chi)$ to $\lambda_{f_{\overline{\chi}},1}(0,1)$ . Since $\chi$ is a $p$ -adic character of conductor $p^n$ , it is constant on the open sets $a+p^n\mathbb{Z}_p$ . So by definition we have \begin{align*}
L_p(f,\alpha,\chi) &=\sum_{a\in (\mathbb{Z}/p^n\mathbb{Z})^\times}\chi(a)\int_{a+p^n\mathbb{Z}_p}d\mu_{f,\alpha}\\
&=\sum_{a\in (\mathbb{Z}/p^n\mathbb{Z})^\times}\chi(a)\mu_{f,\alpha}(1;a,p^n)\\
&=\frac{1}{\alpha^n}\sum_{a\in (\mathbb{Z}/p^n\mathbb{Z})^\times}\chi(a)\big(\lambda_{f,1}(a,p^n)-\frac{\epsilon(p)p^{k-2}}{\alpha}\lambda_{f,1}(a,p^{n-1})\big),
\end{align*} and this is where I'm stuck. Clearly, it suffices to show that $$
\sum_{a\in (\mathbb{Z}/p^n\mathbb{Z})^\times}\chi(a)\big(\lambda_{f,1}(a,p^n)-\frac{\epsilon(p)p^{k-2}}{\alpha}\lambda_{f,1}(a,p^{n-1})\big)=\frac{p^n}{\tau(\overline{\chi})}\lambda_{f_{\overline{\chi}},1}(0,1),
$$ but this just doesn't seem to be true, since, for example, it would appear that formula 8.5 in the article shows that $$
\sum_{a\in (\mathbb{Z}/p^n\mathbb{Z})^\times}\chi(a)\lambda_{f,1}(a,p^n)=\frac{p^n}{\tau(\overline{\chi})}\lambda_{f_{\overline{\chi}},1}(0,1).
$$ Where am I going wrong here? Formulae: Let $\chi$ be a character of conductor $m$ . (8.2) $\chi(-1)\tau(\chi)\tau(\overline{\chi})=m$ (8.5) $\lambda_{f_{\overline \chi}, P(mz)}(b,n)=\frac{1}{\tau(\chi)}\sum_{a\in (\mathbb{Z}/m\mathbb{Z})^\times}\chi(a)\lambda_{f,P}(mb-na,mn)$ (8.6) $L(f_{\overline \chi},n+1)=\frac{1}{n+1}\frac{(-2\pi i)^n}{m^{n+1}}\tau(\bar\chi)\sum_{a\in (\mathbb{Z}/m\mathbb{Z})^\times} \chi(a)\lambda_{f,z^n}(a,m)$","['number-theory', 'p-adic-number-theory', 'modular-forms', 'l-functions']"
3695545,Show $A$ is Hermitian and find the orthonormal basis for $V$ in which $A$ is diagonalizable.,"Let $\{e_1,e_2,e_4\}$ be an orthonormal basis for a complex unitary space $V$ . Let's define the vectors: $f_j=e_j-\frac14\sum\limits_{i=1}^4e_i, j\in\{1,2,3,4\}$ . Let $A\in\mathcal L(V), Ax:=\sum\limits_{j=1}^4\langle x,f_j\rangle f_j$ . Show $A$ is Hermitian and find the orthonormal basis for $V$ in which $A$ is diagonalizable. Note: typo  corrected. My attempt: Let's compute $f_1,f_2,f_3,f_4$ first. $\begin{aligned}f_j=e_j-\frac14\sum\limits_{i=1}^4, e_i\implies&f_1=\frac34e_1-\frac14(e_2+e_3+e_4)\\&f_2=\frac34e_2-\frac14(e_1+e_3+e_4)\\&f_3=\frac34e_3-\frac14(e_1+e_2+e_4)\\&f_4=\frac34e_4-\frac14(e_1+e_2+e_3)\end{aligned}$ $\begin{aligned}Ae_i&=\sum\limits_{j=1}^4\langle e_i,f_j\rangle f_j\implies Ae_1=\left\langle e_1,\frac34e_1-\frac14(e_2+e_3+e_4)\right\rangle f_1+\left\langle e_1,\frac34e_2-\frac14(e_1+e_3+e_4)\right\rangle f_2+\left\langle e_1,\frac34e_3-\frac14(e_1+e_2+e_4)\right\rangle f_3+\left\langle e_1,\frac34e_4-\frac14(e_1+e_2+e_3)\right\rangle f_4=\frac34f_1-\frac14(f_2+f_3+f_4)\end{aligned}$ $\ Ae_2=\frac34f_2-\frac14(f_1+f_3+f_4)\\Ae_3=\frac34f_3-\frac14(f_1+f_2+f_4)\\Ae_4=\frac34f_4-\frac14(f_1+f_2+f_3)$ Then, $$[A]_e^f=\begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix}$$ About the matrix representation of $A\in\mathcal L(V)$ : $A\in M_n(\Bbb R)\ \&\ A=A^\tau\ \implies A=A^*\iff A\ \text{is normal}\implies A\text{ is diagonalizable in some orthonormal basis}$ $\{a_1,a_2,a_3,a_4\}$ Let's find the eigenvalues and the corresponding eigenspaces using the formula derived here . According to the notation I used in the thread, $a_j=\frac34-\lambda\ \forall j\in\{1,2,3,4\}$ and $x=-\frac14$ . $$\det(A-\lambda I)=\begin{vmatrix}\frac34-\lambda&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34-\lambda&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34-\lambda&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34-\lambda\end{vmatrix}=\left(\frac34-\lambda+\frac14\right)^4\left(1-\frac14\cdot 4\cdot\frac1{\frac34-\lambda+\frac14}\right)=-\lambda(1-\lambda)^3=\lambda(\lambda-1)(1-\lambda)^2\implies\sigma(A)=\{0,1\}$$ Let's use the fact $\Omega$ is the orthonormal complement of the row-space (explanation for the indeces orther I used) since $\boxed{E_A(0)\oplus E_A(1)=V}$ : Now, $E_A(0)=\ker(A)$ : $$\begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\1&1&-3&1\\1&-3&1&1\\-3&1&1&1\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-4&4\\0&-4&0&4\\0&4&4&-8\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-1&1\\0&-1&0&1\\0&0&0&0\end{bmatrix}\sim\begin{bmatrix}1&0&0&-1\\0&-1&0&1\\0&0&-1&1\\0&0&0&0\end{bmatrix}$$ $$\implies E_A(0)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\1\\1\\1\end{bmatrix}}_{v_4}\right\}$$ $E_A(1)=\ker(A-I)$ : $$\begin{bmatrix}-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\end{bmatrix}\sim\begin{bmatrix}1&1&1&1\\0&0&0&0\\0&0&0&0\\0&0&0&0\end{bmatrix}$$ $$\implies E_A(1)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}}_{v_1},\underbrace{\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}}_{v_2},\underbrace{\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}}_{v_3}\right\}$$ Let's apply Gramm-Schmidt to the obtained basis for $V$ : $$a_1=\frac1{\|v_1\|}v_1=\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}=b_1$$ $$\begin{aligned}b_2&=v_2-\langle v_2,a_1\rangle a_1\\&=\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\frac32\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\end{aligned}$$ $$a_2=\frac1{\|b_2\|}b_2=\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}$$ $$\begin{aligned}b_3&=v_3-\langle v_3,a_1\rangle a_1-\langle v_3,a_2\rangle a_2\\&=\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\begin{bmatrix}0+\frac12\\0+\frac12\\-1\\1-\frac12-\frac12\end{bmatrix}=\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\end{aligned}$$ $$a_3=\frac1{\|b_3\|}b_3=\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}$$ $$\begin{aligned}b_4&=v_4-\langle v_4,a_1\rangle a_1-\langle v_4,a_2\rangle a_2-\langle v_4,a_3\rangle a_3\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac{\sqrt{6}}3\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\right\rangle\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\ \underline{\text{we can skip this step}}\end{aligned}$$ $$a_4=\frac1{\|b_4\|}b_4=\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix}$$ Therefore, a Hermitian operator $A$ is diagonalizable in the orthonormal basis: $$\{a_1,a_2,a_3,a_4\}=\left\{\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix},\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix},\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\right\}$$ May I ask if this is correct? If so, how could I improve my approach? Thank you in advance!","['operator-theory', 'solution-verification', 'linear-algebra', 'hermitian-matrices']"
3695559,Does polynomial generated by repeated application of matrix divide characteristic polynomial?,"Given a square matrix $A\in \mathbb C^{n,n}$ and a vector $v\ne0$ , the vectors $$
v, Av, A^2 v, \dots, A^n v
$$ are linearly dependent. Let now $m\le n$ be the smallest number such that $$
v, Av, A^2 v, \dots, A^m v
$$ are linearly dependent. 
Then there are coefficients, not all of them zero, such that $$
\sum_{i=0}^m a_i A^iv=0,
$$ or equivalently, $$
p(A)v=0
$$ for $p$ given by $p= \sum_{i=0}^m a_i t^i$ . My question is: does this polynomial divide the characteristic polynomial $p_A$ of $A$ ? Of course, $p$ and $p_A$ share a non-trivial factor. I think a proof of the claim above can be achieved using Jordan decomposition, but it looks like it would be complicated. Is there a more elementary proof? Does such a proof also work for other fields different from $\mathbb R,\mathbb C$ ? (Using this polynomial is the way how Axler proves existence of eigenvectors for the complex case and existence of small invariant subspaces for the real case. )","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3695574,Let $X$ be connected and $f:X\to\mathbb{R}$ continuous s.t. each point $x\in X$ has a nbh $U$ with $f(x)=\min_{y\in U} f(y)$. Show $f$ is constant.,"Let $X$ be a connected topological space and $f:X\to\mathbb{R}$ a
  continuous map such that each point $x\in X$ has a neighborhood $U$ with $f(x)=\min_{y\in U} f(y)$ . Show that $f$ is constant. My attempt: Consider $x\in X$ and $V:=\{ y\in X: f(y) \ge f(x)\}$ . There exists a neighborhood $U$ of $x$ such that $U\subseteq V$ . Now $V$ is closed, since $X\backslash V = f^{-1}(]-\infty, f(x)[)$ is open. I know that $f(X)$ is an interval in $\mathbb{R}$ . I want to use the connectedness of $X$ , but $V$ is not open, so $X=V\cup X\backslash V$ will give no additional information. I have no idea how to proceed.","['general-topology', 'connectedness']"
3695675,"Prove that L has four elements , the product of which is equal to the fourth power of an integer","The set $L$ consists of 2003 integers , none of which has a prime divisor larger than $24$ . Prove that $L$ has four elements , the product of which is equal to the fourth power of an integer. Above question statement , can someone please explain in simple intuitive manner how pigeonhole principle is applied in the question and how can we prove that?","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3695718,"Randomizing a regular polygon along its ""spokes"" while maintaining area","I have a 2d graphics question that seems like it'd fit better here than at stackoverflow.  Please forgive any breach of ettiquette as I am new to mathematics exchange.  I did search for a solution before posting this. I am taking a regular polygon (of arbitrary n sides) and adding a random value (between 1 and -1) to the length of each ""spoke"" (ie. each circumradius line segment).  I'd like to shift the random values so that when they are added to the polygon's spokes, the polygon's area is the same as the original polygon's area. I naively assumed that subtracting the total average of the values from each value (thus making the average of the values be 0) would do this, but this clearly does not work. Here is an example of what I'd like to achieve: I have an n=4 regular polygon (a square) with a circumradius of 1.  The area of this polygon is 2.  I also have a random value for each point of the polygon: (0.85, -0.75, 0.6, -0.4).  If I add these values to the ""spokes"" of the polygon then the resulting polygon has spokes of length (1.85, 0.25, 1.6, 0.6) and an area of: 1.85 * 0.25 * 0.5  +  0.25 * 1.6 * 0.5  +  1.6 * 0.6 * 0.5  +  0.6 * 1.85 * 0.5 1.46625 I'd like to shift the random values so that the resulting polygon has an area of 2 (ie. the original area).  If I reduce the values by their average of 0.075 then I get (0.775, -0.825, 0.525, -0.475). Adding these shifted values to the polygon results in a polygon with spokes of length (1.775, 0.175, 1.525, 0.525), which average out to 1.  The polygon's area is then: 1.775 * 0.175 * 0.5  +  0.175 * 1.525 * 0.5  +  1.525 * 0.525 * 0.5  +  0.525 * 1.775 * 0.5 1.155 So shifting to get an average of 0 does not work.  My question: What would I shift by instead so that the resulting polygon has the same area as the original polygon? EDIT:
joriki provided a solid answer to this question that was akin to normalizing a vector.  Unfortunately, this technique involves calculating the area of the resulting polygon, which becomes a bottleneck in software implementation (I am solving this problem each frame for a number of regular polygons of n=64 each).  I can try to optimize the solution if this is the only option, but a less computationally expensive solution would be preferred.","['area', 'geometry']"
3695763,What is it for a function with two argument places to be continuous in its first argument?,"I'm reading a paper where the authors describe a function $f(x, y)$ that is 'continuous in its first argument'. Specifically, $x \in [0, 1]$ while $y \in \{0, 1\}$ . I can't find the definition for a function that is continuous in its first argument, though I expect it is obvious. Can anyone help?","['continuity', 'functions', 'real-analysis']"
3695782,"Prove if $B$ has a smallest element, then this element is unique.","Working on the book: Daniel J. Velleman. ""HOW TO PROVE IT: A Structured Approach, Second Edition"" (p. 206) Theorem 4.4.6. Suppose $R$ is a partial order on a set $A$ , and $B \subseteq A$ . If $B$ has a smallest element, then this smallest element is unique. Thus, we
  can speak of the smallest element of $B$ rather than a smallest element. I symbolized "" $B$ has a smallest element"", as: $$\exists y\forall x(x \in B \to (y,x) \in R)$$ "" $B$ smallest element is unique"", as: $$\exists y(\forall x(x \in B \to (y,x) \in R) \land \forall z(\forall x(x \in B \to (z,x) \in R) \to y = z))$$ My proof skeleton using Fitch-style natural deduction: $
\def\fitch#1#2{\quad\begin{array}{|l}#1\\\hline#2\end{array}}
\def\Ae#1{\qquad\mathbf{\forall E} \: #1 \\}
\def\Ai#1{\qquad\mathbf{\forall I} \: #1 \\}
\def\Ee#1{\qquad\mathbf{\exists E} \: #1 \\}
\def\Ei#1{\qquad\mathbf{\exists I} \: #1 \\}
\def\R#1{\qquad\mathbf{R} \: #1 \\}
\def\ci#1{\qquad\mathbf{\land I} \: #1 \\}
\def\ce#1{\qquad\mathbf{\land E} \: #1 \\}
\def\oi#1{\qquad\mathbf{\lor I} \: #1 \\}
\def\oe#1{\qquad\mathbf{\lor E} \: #1 \\}
\def\ii#1{\qquad\mathbf{\to I} \: #1 \\}
\def\ie#1{\qquad\mathbf{\to E} \: #1 \\}
\def\be#1{\qquad\mathbf{\leftrightarrow E} \: #1 \\}
\def\bi#1{\qquad\mathbf{\leftrightarrow I} \: #1 \\}
\def\qi#1{\qquad\mathbf{=I}\\}
\def\qe#1{\qquad\mathbf{=E} \: #1 \\}
\def\ne#1{\qquad\mathbf{\neg E} \: #1 \\}
\def\ni#1{\qquad\mathbf{\neg I} \: #1 \\}
\def\IP#1{\qquad\mathbf{IP} \: #1 \\}
\def\x#1{\qquad\mathbf{X} \: #1 \\}
\def\DNE#1{\qquad\mathbf{DNE} \: #1 \\}
$ $
\fitch{1.\, \exists y\forall x(x \in B \to (y,x) \in R)}{
	\fitch{2.\, \forall x(x \in B \to (b',x) \in R)}{
	\fitch{3.\, \forall x(x \in B \to (b,x) \in R)}{
	  4.\,b \in B \to (b',b) \in R \Ae{2}
	  5.\,b \in B \to (b,b') \in R \Ae{3}
	  \vdots\\
}\\
b=b'\\
}\\
\forall x(x \in B \to (z,x) \in R) \to y = z)
}
$ As $R$ is a partial order, I would need to use antisymmetry property. But I do not know how to use it in lines 4,5 to infer that $(b',b) \in R \land (b,b') \in R$ . How can I fill the dots ? Am I missing some premise or step?","['formal-proofs', 'logic', 'relations', 'discrete-mathematics', 'natural-deduction']"
3695852,"Combinations with and without repetition: How many $6$ digit words you can assemble using each of the $0,1,2$ numbers twice?","I am very confused with combinations. Here is a question: How many 6 digit words you can assemble using each of the 0,1,2 numbers twice? My attempt was to use the ""with repetitions"" formula (because the numbers appear twice, huh?): $$D(n,k)={{n-1+k \choose k}}.$$ And the combination of all numbers should be: $D(6,2)⋅D(4,2)⋅D(2,2)=630$ . So far, so good. But I was shocked to hear that I am WRONG . Now, the right way to do that was using the distinct combinations: $${{6 \choose 2}}⋅{{4 \choose 2}}⋅{{2 \choose 2}}=90.$$ May somebody explain or give a hint why we need to use the distinct combinations formula?",['combinatorics']
3695860,prerequisites needed to Several complex variables,"What are the necessary knowledge in order to learn Several complex variables? In the beginning I thought that complex analysis and multivariable calculus were the only thing, but then I realized that in this branch of mathematics, functions are defined over the $\mathbb{C}^n$ space, so I think it would be a good idea to firstunderstand the $\mathbb{C}^n$ space and maps between vector spaces over $\mathbb{C}$ . So what are the list of prerequisites needed to understand  Several complex variables in an medium/advanced level?","['complex-analysis', 'several-complex-variables', 'soft-question']"
3695896,Functional equation with the property $P(x+1)=P(x)+2x+1$,"Find all polynomials with real-valued coefficients for which the following property $$P(x+1)=P(x)+2x+1$$ holds for all $x \in \mathbb{R}.$ This seems to be a functional equation, so the initial approach would be to try some values. For $x=0$ we would have that $$P(1)=P(0)+1.$$ For $x=1$ $$P(2)=P(1)+3.$$ For $x=2$ $$P(3) = P(2)+5.$$ Now if I would know something about $P(0)$ that would seem to be helpful. If I assume that $P(0)=0$ I would get that $$P(1)=1$$ $$P(2)=4$$ $$P(3)=9.$$ From here it would seem that $P(n)=n^2,$ however I’m not sure if I can make the assumption that $P(0)=0$ or can I?","['contest-math', 'functional-equations', 'algebra-precalculus', 'polynomials']"
3695944,Show that $M_p^p\equiv 1 \mod p^2$,"Can it be shown that $M_p^p\equiv 1 \mod p^2$ where $M_p=2^p-1$ is a Mersenne prime. I tried to develop the left part into into $2^{p^2}-1-pk2^p$ and use $2^{p^2}\equiv 2^p \mod p^2$ , but I get nowhere Thanks","['number-theory', 'modular-arithmetic', 'mersenne-numbers']"
