question_id,title,body,tags
2801088,Generating function for sequence $a_i={n+im-1 \choose im}$,"It is well known that $\sum_{i=0}^\infty {n+i-1 \choose i}x^i=\frac{1}{(1-x)^n},$ i.e. $\frac{1}{(1-x)^n}$ is  generating function for sequence $a_i={n+i-1 \choose i.}$ But I want to find generating function for sequence $a_i={n+im-1 \choose im.}$
Using The On-Line Encyclopedia of Integer Sequences, I understood that generating function for sequence $a_i={n+2i-1 \choose 2i.}$ is $\frac{1+(n-1)x}{(1-x)^2}.$ But I cann't recieve generating function if $m>2.$  I found two formulas   for multiset formula. But they didn't help me. Thanks a lot in advance for any help!","['generating-functions', 'combinatorics', 'multisets']"
2801107,Does equality of (weak) gradients imply equality of functions up to a constant?,"This seems like a pretty basic question, but I can't figure it out. Suppose we have two measurable functions, $u$ and $v$, whose (weak) partial derivatives are all well defined, and we have $\partial_i u = \partial_i v$ for each $i$. Can we then conclude that $u=v+C$ for some constant $C$? Note: I'm mainly interested in the weak case but I couldn't think of a proof in the classical setting either.","['derivatives', 'weak-derivatives', 'partial-derivative']"
2801115,"For which values of $\alpha$ is the disk $B = \{(x, y) \mid x^2+y^2 \leq 1\}$ positively invariant?","Given the following dynamical system $$\begin{aligned} \dot x &= f(x,y) = -x + \alpha y \\ \dot y &= g(x,y) = -y\end{aligned}$$ for which values of $\alpha$ is the disk $B = \{(x, y)\mid x^2+y^2 \leq 1\}$ positively invariant? Now what I have done is that I have taken the orbital derivative of $x^2 + y^2$ . This gives $$\frac{dV}{dt} = 2x\dot x + 2y \dot y = 2x(-x+\alpha y) - 2y^2$$ Now I say at the edge of this disk we have $x^2 + y^2 = 1$ so I now get $$\frac{dV}{dt}= -2 +2x\alpha y$$ and I want $\frac{dV}{dt} < 0$ so from this I conclude that $\alpha < \frac{1}{xy}$ . Now here I am just looking for some clarification. If I have done this wrong can someone please point me in the correct direction. I am not very confident in my answer due to the fact I have not got a actual numerate answer.","['derivatives', 'set-invariance', 'ordinary-differential-equations', 'dynamical-systems']"
2801153,Definition of non-homogeneous Birth–death process,"I know that the usual definition of a birth-death processes found in books uses a homogeneous Markov processes, defines a transition function and uses the derived q-matrix to define the birth and death rates. Clearly a thing as a non-homogeneous birth-death processes does exist which means, that the rates are time-dependant but I wonder how it is defined as we can't find a transition function in the first place?
Most books that I tried to find the answer in only defined the homogeneous case and suddenly started talking about non-homogeneous ones while I can't see a rigorous transition between the cases. Am I missing something obvious?
Thank you very much!","['stochastic-processes', 'probability-theory', 'probability', 'birth-death-process']"
2801162,"Limit of $\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)$","I need help to understand how we compute this kind of limit: $\lim_{(x,y)\rightarrow (0,0)}\ xy\log(\lvert x\rvert+\lvert y\rvert)$ I think we can use the squeeze theorem but I don't know how to bound the function, so I can use the theorem. If I suppose $0 \lt \sqrt{x^2+y^2} \lt 1$ then, but I'm struggle.. $ 0 \le \lvert f(x,y)\rvert = \lvert xy\log(\lvert x\rvert+\lvert y\rvert)\rvert$ Thanks in advance for the help.","['multivariable-calculus', 'calculus', 'limits']"
2801163,Equivalent characterisation of hypothesis & conclusion in Grothendieck's inequality,"The basic version of Grothendieck's inequality states that for a $m\times n$ matrix $[a_{i, j}]$, if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le 1,$$ for all $x_i, y_i\in \{-1, 1\}$, then $$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K,$$ for any unit vectors $u_i, v_j$ in a Hilbert space $H$ (i.e. $\|u_i\| = \|v_j\| =1$), with $K$ an absolute constant. The following conditions are equivalent to the hypothesis and conclusion of the prior statement respectively: hypothesis: if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le \max_i |x_i| \max_j|y_j|,$$ for all $x_i, y_i\in \mathbb{R}$. conclusion: then
$$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K\max_i\|u_i\|\max_j\|v_j\|,$$ for any vectors $u_i, v_j\in H$, a Hilbert space, with $K$ an absolute constant. This equivalence seems like it ought to be very simple to prove, however I seem to be having a mental block which is preventing me from proceeding & presume that I'm just not seeing something pretty obvious here. I'd be grateful for any hints or suggestions on how to approach the proof.","['matrices', 'inequality', 'hilbert-spaces']"
2801175,Passwords permutations under specific rules,"I am trying to practice some combinatorics problems before my discrete mathematics test, and I came up with the problem that I tried solving and would like to know if I am working in the correct path. How many passwords can we make following these rules: The length of the password must be exactly $8$ characters and the
  password must contain at least one digit and no more than $3$ digits (Only normal letters and digits are allowed). Here is what I tried: We have $26$ letters {$a,...,z$} and $10$ digits {$0,...,9$}.. with no restrictions, there are ${36}\choose{8}$ ways to choose the characters and arrange them in 8! ways. Now to follow the question, we have 3 different cases that we need to calculate: $7$ letters and $1$ digits ${26}\choose{7}$$*$${10}\choose{1}$ ways to choose them and then arrange them in $7!$ $6$ letters and $2$ digits ${26}\choose{6}$$*$${10}\choose{2}$ ways to choose them and then arrange them in $6!*2!$ $5$ letters and $3$ digits ${26}\choose{5}$$*$${10}\choose{3}$ ways to choose them and then arrange them in $5!*3!$ Final answer: We add the 3 cases together Am I correct or is this answer $26^7*10^1+26^6*10^2+26^5*10^3$ correct, and why/what's the difference and when is each used?","['permutations', 'combinatorics', 'combinations', 'discrete-mathematics']"
2801189,Solving explicitly or by induction: $0 \leq 2^{(n/2+10)}-2\pi n^{(3/2)}$,"Is it possible to solve the following equation explicitly? $$0 \leq 2^{(n/2+10)}-2\pi n^{(3/2)}$$ I know this is correct and the answer is $n \geq 0$. I've also tried induction on $2\pi n^{(3/2)} \leq 2^{(n/2+10)}$, but with no success. There is another method which involves taking successive derivatives and showing it's increasing but those equations themselves seem even harder to solve.","['algebra-precalculus', 'induction', 'calculus']"
2801190,Probability of k random integers being coprimes,"In this section of the Wikipedia article on coprime integers, it is stated that: More generally, the probability of $k$ randomly chosen integers being coprime is $1/\zeta(k)$. where $\zeta$ is the Riemann zeta function. Although there is no proof of this, the statement follows a fleshed out explanation about the probability of two random integers being coprime. Briefly, it proceeds as follows: The probability of any integer being divisible by some other integer $p$ is $1/p$ (because every $p^\mathrm{th}$ integer is). The probability that two random integers (chosen independently) are both divisible by $p$ is $1/p^2$. For a given integer, being divisible by primes $p$ or $q$ are two independent events, and hence the probability of both being true is $1/pq$. Therefore the probability that two random integers are coprimes (i.e. that they are not both divisible by any prime) is: $$
\prod_{\text{prime } p} 1 - \frac{1}{p^2} 
= \left( \prod_{\text{prime } p} \frac{1}{1 - p^{-2}} \right)^{-1}
= \frac{1}{\zeta(2)} \approx 0.61
$$ And it seems straightforward to apply the same reasoning for $k$ integers: The probability of $k$ random integers chosen independently being divisible by $p$ is $1/p^k$. Hence the probability that they are all coprimes is obtained when they are not all divisible by any prime: $$
\prod_{\text{prime } p} 1 - \frac{1}{p^k} 
= \frac{1}{\zeta(k)}
$$ My problem with this is one of intuition ; intuitively, considering several integers should increase the chances of two of them having a common prime factor, and therefore the probability of $k$ random integers being coprime should asymptotically decrease with $k$. However I think $1/\zeta$ is increasing on $[1, \infty)$, and quickly converges to 1. Where am I going wrong? And what is the right way to think about this?","['prime-numbers', 'coprime', 'probability', 'elementary-number-theory']"
2801203,"Find the following limit: $\lim \limits_{x,y \to 0,0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}$","Recently I came upon a limit which confused me. The reason is that when I try to solve the following limit using polar coordinates I get a constant which I do not know if it gives me information. Let : $$\lim_{x,y \to 0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}$$
Using polar coordnates I get this: 
$$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{\sin\left(r\sin\left(\theta\right)\right)+\log\left(1+r\cos\left(\theta\right)\right)}$$ Which is equal to:
$$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{r\sin\left(\theta\right)+r\cos\left(\theta\right)}=1$$
I already know this limit does not exist. Actually it was quite difficult to find a path for which I get a different limit... My question is: If I use polar coordinates and the result is not something that depends on $r,\theta$ then what I get is basically useless information? (I know that if that limit goes to infinity the limit of the function does not exist)","['multivariable-calculus', 'limits']"
2801218,Question on how to define the principle branch of the complex root function,"Can i just  check this which leads to my question:
a square root of a number a is a number y such that $y^2$= a. all a a=/=0 have 2 square roots so we choose a branch of this function so that it is single valued, called the principle branch. in the case when a is non-negative real that is the positive square root, denoted by 
$\sqrt{x}$ 
so the principle square root function is f(x)= $\sqrt{x}$ x>=0. if what i have said makes some sense then my question is how do we define the principle branch of the complex root function and is radical notation used to represent it. so does f(x) = $\sqrt{x}$ x any complex number represent this function and how is it defined? 
From wikipedia, if x is in exponential form then $\sqrt{x}$ = $\sqrt{r}$e^(itheta/2) with theta between - pi and pi. Thanks for the help Please don't attack me because to me this is a valid question.",['functions']
2801221,Why does the sheaf cohomology of the constant sheaf on $\mathbb{R}$ vanish?,"Sorry if this is a very basic fact, but for some reason I am not able to solve it. I am trying to prove for $X =\mathbb{R}$ that the sheaf cohomology groups $(i>0)$ of the constant sheaf $\mathbb{Z}_{X}$ vanish. From what I gather online it has something to do with $\mathbb{R}$ being simply connected. I know it is even contractible and that these cohomology groups are isomorphic for homotopic spaces, but I am looking for an argument not using this homotopy argument. I think I understand this sheaf, namely that $\mathbb{Z}_{X}(U) = \mathbb{Z}^{I}$ where I has cardinality equal to the number of connected components of $U$ and the restriction maps are just projections and diagonal maps. But somehow I can't seem to find a flasque/injective resolution or a sort of dimension shift argument. I can use excision and mayer-vietoris but I don't know if this helps. Thanks in advance for any help!","['sheaf-theory', 'sheaf-cohomology', 'algebraic-geometry']"
2801247,Is algebraic geometry necessary for Silverman's Rational points on Elliptic Curves Text,"I'm looking at soon studying some Elliptic Curves from Tate and Silverman's ""Rational Points on Elliptic Curves"" but I was wondering if knowing some algebraic geometry from Cox's ""Ideals, Varieties and Algorithms"" before reading Silverman's text would be a better approach or if I can just start learning elliptic curves right away. Will understanding Algebraic Geometry at the level of ""Ideals, Varieties and Algorithms"" be sufficient for intuitive understanding in Silverman's Text ""Rational Points on Elliptic Curves""? In any case what would be the best approach? As far as my background I know Abstract Algebra, Topology, slight amount of Number Theory, Linear Algebra, Real Analysis.","['elliptic-curves', 'reference-request', 'book-recommendation', 'algebraic-geometry']"
2801255,Substitution in complex integration,"Let $f$ and $\phi$ be continuously differentiable functions on the closed unit disc $D=\{\lambda\in \Bbb C; |\lambda|\leq1\}$ and suppose that there exists $u:[0,2\pi]\to[u_0, u_0+2\pi]$ continuously differentiable such that $$e^{iu(t)}=\phi(e^{it}), t\in[0,2\pi].$$ Let $$A=\int_0^{2\pi}|f(\phi(e^{it}))|^2dt.$$
The author of the book I am reading say that, taking te substitution $u=u(t)$ we conclude that $$A=\int_{u_0}^{u_0+2\pi}|f(e^{iu})|^2\frac{1}{|\phi'(e^{it(u)})|}du.$$ I want to conclude this equality, but I didn't undestand why we should put a modulus on that factor... If we differenciate $e^{iu(t)}=\phi(e^{it})$ in $t$ we have $$ie^{iu(t)}\frac{du}{dt}=ie^{it}\phi'(e^{it}), t\in[0,2\pi].$$ If we could take modulus in this equality I could conclude what I want... but why could I do this? The formula for integration by substitution is $$\int_{\varphi(a)}^{\varphi(b)}f(x)dx=\int_a^bf(\varphi(t))\varphi'(t)dt.$$","['substitution', 'calculus', 'complex-analysis', 'integration', 'definite-integrals']"
2801285,"For any positive integer $k$, are there always primes $p$ and $q$ such that $q-p=2^k$?","Experimental evidence suggests to me that there are always primes $p$ and $q$ such that $q-p=2^k$. Some examples include: $5-3=2$, $11-7=4$, $19-11=8$, $29-13=16$, $43-11=32$, etc. I am now sure how to go about proving this. It seems like it should be accessible enough, perhaps using something like Dirichlet's theorem for primes ($a+bk$ is prime for infinitely many $k$ if $\gcd(a,b)=1$). Can someone help me prove or disprove this?","['number-theory', 'experimental-mathematics', 'elementary-number-theory']"
2801327,Show that $f$ is a constant function.,"Problem Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that for all $x\in\mathbb{R}$ and for all $t\geqslant 0$, $$f(x)=f(e^tx)$$Show that $f$ is a constant function. My proof Take $t=1$. Then for any fixed $r \in \mathbb{R}$, it's clear that $$f(r)=f \left(\frac{r}{e}\right)=f \left(\frac{r}{e^{2}}\right)=f \left(\frac{r}{e^{3}}\right)=\cdots=f \left(\frac{r}{e^{n}}\right).$$Take the limits of both sides as $n \to \infty$. Notice that $\dfrac{r}{e^n} \to 0$ as $n \to \infty$. By the continuity of $f(x)$, we have $$f(r)=\lim_{n \to \infty}f(r)=\lim_{n \to \infty}f \left(\frac{r}{e^{n}}\right)=f\left(\lim_{n \to \infty}\frac{r}{e^{n}}\right)=f(0).$$This shows that $f(x) \equiv f(0)$. Please Correct me if I'm wrong.","['functions', 'proof-verification']"
2801339,$\int(x+1)dx$ yielding different results with $u$-substitution and termwise integration,"Considering two methods of integrating the very easy: $\int(x+1)dx$ First just going term by term:
$\int(x+1)dx = x^2/2 + x + C$ Or by making a u-subtitution. Let $u = x+1$, then $du = dx$ and the integral becomes $\int u du = u^2/2$ = $\frac {(x+1)^2}{2} + C$, which is not the same. Where have I gone wrong?",['calculus']
2801349,"Estimation of an average, and speed of convergence","I asked myself those two questions the other day, and I have a very limited backgroud in stats, so help would be appreciated! Sometimes in the middle of my grading, I look at the average of the students I have graded. Of course, it's not the exact average of the whole group but it gives an idea of what the group has done . This is the part that I tried to quantify. If you grade the exams of a classroom made of $N$ students, and suppose the average of the whole group is $\mu$, and a standard deviation $\sigma$. Suppose also a regular bell curve (to make it easier). I have two questions (the first I think I could end up finding it by going over my old books, but the second seems more complex to me): What is the ratio $n/N$ of corrected exams over the total amount of exams that would give you a correct estimate of the average, with let say a 5% accuracy and 95% of being correct? How fast does the intermediate average (the one you can compute after each exam graded) converges towards the actual average of the whole group. In other words, if $$\mu_n=\frac{1}{n}\cdot \Sigma_{k=1}^n G_k$$
and $$f(n)=\frac{|\mu_n-\mu|}{\mu}$$ How fast would $f(n)$ approach $0$? In a exponential, log, or exponent way? I tried to do some simulations, not enough though to get something. (it's a bit late, and I did it very fast on excel, but I'll try with a quick script later). Any partial, or complete answer (or even a reference to some article) would be appreciated! Thanks!","['statistics', 'central-limit-theorem', 'convergence-divergence', 'normal-distribution']"
2801439,Does this condition on the curvature implies existence of a parallel section?,"Let $E$ be a smooth vector bundle over a manifold $M$ ($\dim M>1$), equipped with a metric. Let $\nabla$ be a metric connection on $E$. Suppose there exist locally a non-zero section $\sigma \in \Gamma(E)$ which lies in $\ker R(X,Y)$ for all $X,Y \in \Gamma(TM)$. Does $\nabla$ admit a parallel section (locally)? Note that even if $\| \sigma\|=1$, it is not necessarily true that $\sigma$ parallel. (e.g. if $\nabla$ is flat). (We have to normalize: the point is that if $\sigma \in \ker R(X,Y)$ so is $f\sigma$ for any function $f$. A parallel section has a constant norm though.) Clearly, this is a necessary condition: If $\sigma$ is parallel, then
$R(X,Y)\sigma=d_{\nabla}^2\sigma(X,Y)=0$","['riemannian-geometry', 'curvature', 'differential-geometry', 'vector-bundles', 'connections']"
2801498,When does $\lim\limits_{n\to\infty}|a_{n+1}-a_n|=0$ imply $a_n$ convergent?,"Is it true: Let $\{a_n\}$ be real sequence. If $|a_{n+1}-a_n|<\frac1{3^n}$ for all $n$, then $\{a_n\}$ convergent. I am asking this, because I was pointed out that there is a real sequence $\{a_n\}$ such that $|a_{n+1}-a_n|<\frac1{n}$, yet $\{a_n\}$ not convergent.","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2801548,Is there an axiomatic characterization of the Lebesgue integral?,"Is there an axiomatic characterization of the Lebesgue integral w.r.t. some finite measure $\mu:\mathcal{F}\rightarrow[0,\infty)$, for instance as the function $I$ over the set of real-valued, $\mathcal{F}$-measurable functions that satisfies the following axioms: Linearity ($I(\alpha f + \beta g) = \alpha I(f) + \beta I(g)$) Positivity (for every $f \geq 0$, $I(f)\geq 0$) Continuity from below (for every $f, f_1, f_2, \dots$ such that $f_n\uparrow f$, $I(f) = \lim_{n\rightarrow\infty}I(f_n)$) Consistency with the measure (for every $A \in \mathcal{F}$, $I(\mathbb{1}_A) = \mu(A)$) I'm not asking whether the above axioms characterize the Lebesgue integral; I'm asking whether there is any set of axioms - be it the four above or another set - that characterizes the Lebesgue integral. For example, I've read here that there is an axiomatic characterization of the Lebesgue integral through linear functionals; I'd be interested in this characterization as well as in any other characterization, ideally 'simple' as in the four axioms above (which may or may not characterize the Lebesgue integral).","['functional-analysis', 'axioms', 'lebesgue-integral', 'measure-theory']"
2801592,"Finding $\lim\limits_{x→+∞}\int_{-∞}^{+∞}|f(t-x)-f(t)|\,\mathrm dt$","Let $f$ be integrable and continuous function on $\mathbb{R}$. Then I would like to find the value of:
$$\lim_{x \to +\infty} \int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t.$$ I have a lot of problem with these kind of ""theorical"" integration problems. Moreover, it is hard for me to even guess what the result is, maybe it is $0$, but I am not sure. So my little try, is the following:
$$\int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t \leq \int_{-\infty}^{+\infty}\sup_{\mathbb{R}} | f(t-x)-f(t) | \,\mathrm{d}t.$$ Here my goal is to prove that:
$$\sup_{\mathbb{R}} | f(t-x)-f(t) | \rightarrow 0,$$
yet the problem is that I do not know how to proceed, and it is possible that $0$ is not the right answer.","['real-analysis', 'limits']"
2801601,How does the ring $\mathbb R[x] / (x^2 - 1)$ work?,"I am new to ring theory and so some of its concepts are still not clear to me. One of them being the quotient rings. Let's say that I needed to construct this ring: $$\mathbb R[x] / (x^2 - 1)$$ 
The ring of polynomials with real coefficients ""quotiented"" by the ideal generated by the polynomial $(x^2 - 1)$. Let $C$ be the set of  cosets of the ideal. Then:
$$C = \{p + (x^2-1) : p \in \mathbb R[x] \}$$
Now, consider this coset:
$$D = x^3 + (x^2-1) = x^3 + kx^2 - k $$
And this
$$E = x^5 + (x^2-1) = x^5+kx^2-k$$ $$k \in \mathbb R$$
Therefore $$D+E = x^5 + x^4 +kx^2-k $$
Now, I know that the original ring only contains linear polynomials, and so my expression should somehow simplify. but I don't know how to do it. 
I can imagine why the multiples of $(x^2-1)$ will be sent to zero - they form the coset containing the natural element. But in my case the expression I got cannot be factored so that I can cancel anything but $(kx^2 - k)$ out. How do I simplify this? Although it may be inappropriate, please, could you give an answer in simple terms? I am not taking any course in abstract algebra, just some leisurely reading.","['abstract-algebra', 'ring-theory']"
2801614,Pairwise negative dot product implies linear independence,"Let $v_1, ..., v_{m+1} \in \mathbb{R}^n$ such that $v_i \cdot v_j < 0$ if $i \ne j$. Show that $v_1, ..., v_m$ are linearly independent. My attempt: Assume $v_1, ..., v_m \in \mathbb{R}^n$ are linearly dependent with pairwise negative dot product. Let $$S = \text{span}({v_1, ..., v_m})$$ I want to show that for any $v \in \mathbb{R}^n$ there exists some $v_i$ such that $v \cdot v_i \ge 0$. Since $v = u + w$ where $u \in S$ and $w \in S^\perp$, it suffices to show that there exists some $v_i$ such that $u\cdot v_i \ge 0$. But I am stuck trying to prove this.",['linear-algebra']
2801616,A curious coincidence in the series representation of $\zeta(7)$,"Let $\zeta(n)$ denote the Riemann Zeta function defined for positive integers $n$ as usual by: $$
\zeta(n)=\sum_{m=1}^{\infty} \frac{1}{m^n}.
$$ It is currently unknown whether there exists a series representation of $\zeta$ for odd integers $n>3$ of the form: $$
\zeta (n)=\xi _{{n}}\sum _{{m=1}}^{{\infty }}{\frac  {(-1)^{{m-1}}}{m^{{n}}{\binom  {2m}{m}}}},
$$ where $\xi_n$ is an algebraic number. For instance, for the case $n=3$, we have $\xi_3=\frac{5}{2}$. If such a $\xi_n$ is found, for any odd $n>3
$, one could use Apéry's proof of the irrationality of $\zeta(3)$ to prove the irrationality of $\zeta(n)$. Despite extensive computer searching, no such $\xi_n$ was found for $n>3$ and it can be shown (1) that if one exists, then the coefficients in its minimal polynomial must be enormous. You can imagine my surprise when I found that $$
\xi_7 \approx \frac{\pi^7}{500^2 \sinh^{-1}(\frac{1}{2})^7},
$$ which, despite the fact that it contains a power of $\pi$, is even more surprising given its striking similarity to the well-known value for $\xi_2$: $$
\xi_2 = \frac{\pi^2}{12 \sinh^{-1}(\frac{1}{2})^2}.
$$ I know that seemingly remarkable mathematical
coincidences are easy to generate , however this one is far too remarkable to immediately dismiss as pure coincidence. Don't get me wrong, it is most probably a coincidence, but a beatiful one indeed. How may one attempt to explain this coincidence? Is there a ""good"" reason for this, other than chance? References (1)  D. H. Bailey, J. Borwein, N. Calkin, R. Girgensohn, R. Luke, and V. Moll, Experimental Mathematics in Action, 2007.","['riemann-zeta', 'sequences-and-series']"
2801621,Evaluating $\int_0^1\frac{\ln^2(1+x^2)}{x^4}dx$,"I want to evaluate $$\int_0^1\frac{\ln^2(1+x^2)}{x^4}dx$$
My attempt: Letting
$$I(\alpha,\beta)=\int_0^1\frac{\ln(1+\alpha^2x^2)\ln(1+\beta^2x^2)}{x^4}dx$$
$$
\begin{aligned}
I_{12}''(\alpha,\beta)&=\int_0^1\frac{4\alpha\beta}{(1+\alpha^2x^2)(1+\beta^2x^2)}dx\\
&=\frac{4\alpha\beta}{\alpha^2-\beta^2}\int_0^1\frac{\alpha^2}{1+\alpha^2x^2}-\frac{\beta^2}{1+\beta^2x^2}dx\\
&=\frac{4\alpha\beta}{\alpha^2-\beta^2}(\alpha\arctan\alpha-\beta\arctan\beta)
\end{aligned}
$$
$$
I=\int_0^1\int_0^1I_{12}''(\alpha,\beta)d\beta d\alpha
$$
But I can't go further.","['integration', 'definite-integrals', 'calculus']"
2801623,Using Galmarino's test,"I am wondering if somebody has an example of use of Galmarino's test. The Galmarino test says that for $X=(X_t)_{t\in T}$ a continuous stochastic process with $\mathcal{F}$ the natural filtration, a random time $\tau$ is a stopping time if and only if for all $t\in T$, $\omega, \omega'\in\Omega$
$$\tau(\omega)\leq t, X_s(\omega)=X_s(\omega') \text{ for all }s\leq t \Rightarrow \tau(\omega')\leq t.$$ I can't find any examples. I did find a proof ( Proving Galmarino's Test ) and the answer to this question ( Hitting time of an open set is not a stopping time for Brownian Motion ) did help me to understand it a bit more. I tried to work out the case for canonical Brownian motion with $\tau=\inf\{t>0 \mid B_t\in (a,b)\}$ which is not a stopping time, as described in the answer. I understand why this $\tau$ is not a stopping time, but I can't wrap my head around why the condition is not satisfied. I want to understand this test so I can use it to verify whether a random time is a stopping time. I hope somebody can help me.","['stochastic-processes', 'probability-theory', 'brownian-motion', 'stopping-times']"
2801646,Liouville's theorem non-entire function,"I have the following problem which I don't understand. Find all entire functions $f$ such that
$$|f(z)|\ge \frac{1}{1+|z|^{2017}}=g(z), \; \forall z \in \Bbb{C}$$ The answer says that $f$ has to be constant. I feel like I should use Liouville's theorem. I have tried introducing a function $h(z) = \frac{g(z)}{f(z)}$. But I feel like this is wrong since $g(z)$ is not holomorphic so I know that $h$ is bounded by 1 but nothing else. How should I continue?","['complex-analysis', 'entire-functions']"
2801651,How to calculate probability distribution for combination of functions,"please excuse (or change, if possible) the title if it doesn't make sense. I have a problem which is something like this: You have a variable $i$ which starts at $i=1$. You also have a target number of $j$, and you have an n-sided die (in my case $j=6$ and $n=6$ but ideally i'd like to be able to solve this generally.) Now you repeatedly roll the n-sided die, each time the die the lands on a number greater than $i$ you increment $i$ by 1. This stops once $i=j$. I've already calculated the mean number of rolls until $i=j$ as: $$\sum_{x=n-(j-1)}^{n-1} \frac{n}{x}$$ Now i'd like to be able to produce a probability distribution of how many rolls until $i=j$. I'm currently only looking at $j=6$ and $n=6$ and have come up with a solution, however it is very manual. I can model the probability of raising $i$ from 1 to 2 (assuming 6-sided die) in $x$ rolls as: $$p_1(x)=\frac 16^{x-1}\times \frac 56 $$ Similarly the probability of raising $i$ from 2 to 3 in $x$ rolls $$p_2(x)=\frac{2}{6}^{x-1}\times \frac 46 $$ (I'll skip the definition of $p_3(x)$ to $p_5(x)$) Now for the probability of raising $i$ from 1 to 6 in just 5 rolls would be $$p_1(1) \times p_2(1) \times ... \times p_5(1)$$ But what I need to do next is the probability of raising $i$ from 1 to 6 in 6 rolls, I realise I need to do something like: $$p_1(2) \times p_2(1) \times ... \times p_5(1) + p_1(1) \times p_2(2) \times ... \times p_5(1) + ... \times p_4(1) \times p_5(2)$$ (Not sure if I removed too much from the above equation but essentially the sum of all the different products of $p_{1-5}$ where the arguments to the functions add up to 6, does that make sense?) And then the above but for all arguments summing to 7 and on to Infinity. So I'm wondering how to generalise this last step? It seems to be that it could/should be able to be represented as a function. Maybe there is something similar that already exists? Thank you in advance. Let me know if i need to edit the question to clarify anything :)",['statistics']
2801717,The unit group of a finite dimensional associative algebra is a Lie group?,"I am reading Serre's ""Lie algebras and Lie groups"" p.103.
Let $k$ be a complete valued field(for example $\mathbb{R}$, $\mathbb{C}$, or $\mathbb{Q}_p$) and $R$ be a finite dimensional associative $k$-algebra.
Surely $R$ is an additive Lie group.
The book asserts that the unit group $G_m(R)$ is a multiplicative Lie group
and also contains the proof, but I cannot understand it.
I copy the text here. ""We contend that $G_m(R)$ is an analytic group which is open as a subset of $R$. To show that $G_m(R)$ is open in $R$ it suffices to show that there is a neighborhood of $1$ contained in $G_m(R)$. Now, there exists an open neighborhood $U$ of $0$ in $R$ such that for $x \in U$ the series $\sum x^n$ converges. It follows $V=\{1-x:x \in U\} \subset G_m(R)$ and $V$ is a neighborhood of $1$. To show that $G_m(R)$ is an analytic group it remains to show that multiplication is a morphism. This follows since multiplication in $R$ is bilinear."" I cannot understand the first step and the final step: Why does there exist an open set $U$ which satisfies $\sum x^n$ converges? Why is multiplication a manifold morphism? (Also, It seems that we need $x\mapsto x^{-1}$ is a morphism.) From googling, I've found (ex1) of http://www.math.cornell.edu/~sjamaar/classes/6520/problems/2016-10-26.pdf , but still I cannot solve it.","['abstract-algebra', 'lie-algebras', 'lie-groups']"
2801740,"Explicit complete metric on $\mathbb{C}\backslash \{ 0,1\} $ with Gaussian curvature $K \leqslant - 1$.","I wonder how to give a complete metric on $\mathbb{C}\backslash \{ 0,1\} $ with Gaussian curvature $K \leqslant  - 1$ explicitly. I tried to modify the Poincaré disk model since it has constant curvature $-1$ under the hyperbolic metric. However, I still could not see how to make it right. Is it possible to use cutoff functions to give an explicit expression? Any help will be appreciated.","['riemann-surfaces', 'riemannian-geometry', 'curvature', 'hyperbolic-geometry', 'differential-geometry']"
2801744,Finding eigenvalues for a rotational transformation,"The question verbatim goes as following: Let the matrix $A$ be the standard matrix for the matrix transformation
  $T_{A} : R^{2} -> R^{2}$ which is given with the rotation $\pi/6$
  radians. Calculate all real eigenvalues for the matrix $A$ (i.e all eigenvalues which are real, $\lambda \in R$. The answer to this is: The matrix $A$ lacks real eigenvalues. This can be seen with out performing any calculations, since $Ax$ corresponds to the rotation
  $\pi/6$ radians, $Ax$ = $\lambda x$ can only be satisfied by the
  zero-vector. Is this because the rotation operation only rotates the coordinates and not scales them, this is my intuition behind the reasoning of the answer. However i'm not entirely sure why this transformation does't have any eigenvalues? I would be thankful if somebody could expand this a little for me.","['eigenvalues-eigenvectors', 'rotations', 'linear-algebra', 'linear-transformations']"
2801770,Permutation of partitions,"Let $N \subset \mathbb N$ and $\Pi$ the set of partitions of $N$.
I want to define a permutation such that the partition $\pi \in \Pi$ is identical with $\pi' \in \Pi$ up to a permutation $p : N \to N$ of the index $i \in N$. Exmaple Consider $N = \{1,2,3\}$ and $\pi = \{\{1\},\{2,3\}\}$. The permutation $p(N) = \{2,3,1\}$ shall result in $\pi' = \{\{2\},\{3,1\}\}$. Does there exist a common definition or how would you define $\pi'$ from $p$? Edit With respect to the comment one may defines $\rho : \Pi \to \Pi$ by $\rho(\pi) := \{\{p(i)\}_{i \in S} \mid S \in \pi\}$. Reconsdering the example $\pi = \{\{1\},\{2,3\}\}$ and $p(\{1,2,3\}) = \{2,3,1\}$ yields 
\begin{align}
\rho(\pi)& = \{\{p(i)\}_{i \in \{1\}}, \{p(i)\}_{i \in \{2,3\}}\}\\
& = \{\{p(1)\}, \{p(2),p(3)\}\\
& = \{\{2\},\{3,1\}\}\\
& = \pi'.
\end{align} Is the definition sound?","['elementary-set-theory', 'definition']"
2801793,Weak continuity of functions with values in a Banach space,"I have problems understanding the proof of the following lemma: Let $X$, $Y$ be Banach spaces, $X$ reflexive, and assume that $X$ is continuously, densely embeded into $Y$. Let $I \subset \mathbb{R}$ be open, bounded interval. Consider a function $\varphi \in L^{\infty}(I;X)$ such that $\varphi$ is weakly continuous from $\bar{I}$ to $Y$. $\space$ Then $\varphi$ is also weakly continuous from $\bar{I}$ to $X$. In the proof, $I$ stands for not only the interval, but also for the mapping that represents the embedding from the assumptions of the lemma. Moreover, $I^{*}$ stands for the associated continuous, dense embedding of $Y^{*}$ to $X^{*}$. I understand the proof until the point when they claim that $I \widetilde{\varphi}(t)=I\varphi(t)$ and $\widetilde{\varphi}(t)=\varphi(t)$. I don't see what it follows from. I'll be thankful for any help.","['functional-analysis', 'banach-spaces', 'weak-convergence', 'bochner-spaces']"
2801816,Is there any formula to find $\sin^{-1}a +\sin^{-1}b$,"We known that$$\tan^{-1} a  +\ tan^{-1}b=\tan^{-1}\left(\frac{a+b}{1-ab}\right).$$ Now we derive the above formula. Let$$
\tan^{-1}a=\theta _1 \implies \tan\theta_1=a,\\
\tan^{-1}b=\theta _2 \implies \tan\theta_2=b,\\
\theta _1+\theta _2 = \tan^{-1}a+ \tan^{-1}b,\\
\tan(\theta _1+\theta _2) = \tan(\tan^{-1}a+\tan^{-1}b)·
\tan(\theta _1+\theta _2)\\
=\frac{ \tan(\tan^{-1}a)+\tan( \tan^{-1}b)}{1- \tan(\tan^{-1}a)· \tan(\tan^{-1}b)},\\
(\theta _1+\theta _2) =\tan^{-1}\left(\frac {a+b}{1-ab}\right).
$$
Therefore, $\tan^{-1} a  +\ tan^{-1}b=\tan^{-1}\left(\dfrac{a+b}{1-ab}\right)$. For all  $\theta _1$ and $\theta _2$ such that $0 \le \theta _1,\theta _2 \lt 90$, in the similar way I try to find  $\sin^{-1}a +\sin^{-1}b$. Let$$
\sin^{-1}a=\theta_1, \quad \sin^{-1}b=\theta_2,\\
\sin(\theta_1+\theta_2)=\sin(\sin^{-1}a +\sin^{-1}b),\\
\sin(\theta_1+\theta_2)=\sin(\sin^{-1}a)·\cos(sin^{-1}b) +\sin(\sin^{-1}b)·\cos(sin^{-1}a),\\
\theta_1+\theta_2=\sin(\sin^{-1}a)·\cos(sin^{-1}b) +\sin(\sin^{-1}b)·\cos(sin^{-1}a),\\
\sin^{-1}a+\sin^{-1}b=\sin(\sin^{-1}a).cos(sin^{-1}b) +\sin(\sin^{-1}b).cos(sin^{-1}a),\\
\sin^{-1}a+\sin^{-1}b=\sin(a)·\cos(\sin^{-1}b) +\sin(b)·\cos(\sin^{-1}a).$$
It cannot be write in the form only using $a$ and $b$.
$$\tan(A+B)=\frac{\tan A+\tan B}{1-\tan A·\tan B}.$$ It contains only $\tan$ terms. But $\sin(A+B)=\sin A·\cos B+\cos A·\sin B$, which contains both $\sin$ and $\cos$ terms. Is there any formula for $\sin^{-1}a+\sin^{-1}b$?","['algebra-precalculus', 'trigonometry']"
2801822,$\frac{1}{1+z^2}$ as a power series in $z - a$,"I am trying to refresh my memory about some complex analysis and was intrigued by an exercise in Ahlfors Complex Analys (Ex. 1 in section 5.1.2) which asks you to develop $\frac{1}{1+z^2}$ in powers of $z - a$, $a$ being a real number. Ahlfors has just introduced what amounts to the method of solving problems like this using formal power series and then deriving results about complex power series by checking convergence (he doesn't use formal power series terminology but instead talks about polynomial approximations to the formal power series). I derive the following linear recurrence for the coefficients:
$$
\alpha_0 = \frac{1}{1+a^2} \quad \alpha_1 = \frac{-2a}{(1+a^2)^2} \quad \alpha_i = \frac{-2a\alpha_{i-1}-\alpha_{i-2}}{1+a^2}.
$$
which you can solve using the usual method for solving linear recurrences based on the characteristic polynomial:
$$
f(x) = x^2 + \frac{2a}{1+a^2}x + \frac{1}{1+a^2}.
$$
although it looks pretty messy. He then asks you to simplify this for the special case $a = 1$, which is more easily done just by calculating the first few $\alpha_i$ and observing that up to a power of $2$ the solution is periodic with period $8$ (or see Write $\frac {1}{1+z^2}$ as a power series centered at $z_0=1$ ). Now my question: why does Ahlfors say $a$ is real? As the discriminant of the characteristic polynomial is $\frac{-4}{1+a^2}$, it has two distinct roots for any complex $a \neq \pm i$. Does the assumption that $a$ is real lead to some  neater solution to the linear recurrence? Or an alternative approach to the problem? Or what?","['complex-analysis', 'recurrence-relations', 'power-series']"
2801881,Does there exist an infinite nilpotent group with finite center?,"Does there exist an infinite nilpotent group with finite center? I failed to prove that it does not, but any examples of such groups do not come to my mind either. Any help will be appreciated.","['abstract-algebra', 'group-theory']"
2801922,Relation between line integral of scalar function and surface integral,"I've seen the following identity on a book: $$\int_{\partial S} f \, d\vec{\ell} = \iint_S d\vec{S} \times \nabla f$$ where $f$ is a scalar function and $\partial S$ is a closed curve. I've been trying to prove it but I don't know where to start.",['multivariable-calculus']
2801926,Mobius function of product of primes,"We have the arithmetic function $$f(n)=\sum_{d\mid n}\mu (d)\cdot d$$ I want to show that $f\left (p_1^{e_1}\cdots p_k^{e_k}\right )=(-1)^k\cdot (p_1-1)\cdots (p_k-1)$. We have that $d$ is of the form $p_1^{a_1}\cdots p_k^{a_k}$ with $0\leq a_i\leq e_i$ right? So $$f\left (p_1^{e_1}\cdots p_k^{e_k}\right )=\sum_{d\mid n}\mu (d)\cdot d=\sum_{0\leq a_i\leq e_i, \forall i}\mu (p_1^{a_1}\cdots p_k^{a_k})\cdot p_1^{a_1}\cdots p_k^{a_k}$$ or not? How could we continue?","['number-theory', 'mobius-function']"
2801965,"Given a circle of radius r, and two points ('X' and 'Z') on that circle, can some circumcircular arc ""XYZ"" be constructed of length r?","I am strictly an amateur, not a professional mathematician or some such. This question occurred to me while considering the fact that an angle of 1 radian centered on the center of a circle will produce a circumcircular arc on the circle of the same length as the circle's radius, whereas two segments (AB and BC) of equal length intersecting at 60 degrees will, of course, define a third segment of equal length (between A and C). To elaborate: Define a circle C with centerpoint O and radius 'r'. Define two points, X and Z, on circle C. Define the lines OX and OZ. Define angle XOZ. Define the line OA bisecting XOZ. Define some point Y on OA such that the circumcircular arc XYZ is of
length r. Point 6, of course, is the one which I do not know how to do.  It has occurred to me that this problem is, of course, restricted to cases in which angle XOZ is of fewer than 60 degrees (as at 60 degrees, XYZ becomes a line segment, and above 60 degrees, no arc XYZ of length <= r can exist. It has also occurred to me that this problem could also be solved by defining some point P on OA such that a circle D with centerpoint P and which runs through X (and Z) exists, where the length of the arc XZ on circle D is r, but I also have no idea how to do that. Diagram EDIT: A related question of interest would be to define the function which describes the length of the line OY relative to the length of OX or OZ (ie: 'r'), and the angle of XOZ.  There would, naturally, be two valid values of OY, as indicated by @RossMillikan, one for a Y inside the circle, and one for a Y outside the circle.","['circles', 'arc-length', 'angle', 'euclidean-geometry', 'geometry']"
2801982,Finding the period of a nonlinear ODE,"I am studying a periodic physical system with a nonlinear ODE
$$x''=f(x)+g(x)x'^2$$ I think the periodicity comes from the $x'^2$ term because this provides two possible numbers to give a same right hand side value. The following shows three numerical curves of this equation
with $f(x)=x-x^3$ and $g(x)=2/x-x$. We can see that the curve is oscillating about the fixed point (by making $x''=0$ and $x'=0$, here the fixed point is $x^*=1$) I can solve the period for the almost fixed-point solution $x(t)=x^*+\epsilon \cdot \cos{\omega t}$ and this perturbation gives me a frequency $\omega=\sqrt{-f'(x^*)}$. For the particular example I gave, $\omega=\sqrt{-f'(1)}=\sqrt{-(1-3\cdot 1^2)}=\sqrt{2}$ so $T=2\pi/\omega=\sqrt{2}\pi \approx 4.44$ and it matches the red curve pretty well. My question is how can I analytically solve the period of the curves far away from the fixed point solution? Thank you for you attention!","['periodic-functions', 'dynamical-systems', 'fourier-analysis', 'nonlinear-system', 'ordinary-differential-equations']"
2801987,Residue of high order pole,"I'm trying to compute the residue $\displaystyle\operatorname{Res}\left(\frac{1}{(z^2+1)^7},i\right)$. I know that there is the formula: $$\operatorname{Res}(f,z_0)=\frac{1}{(m-1)!}\lim_{z\rightarrow z_0 }[(z-z_0)^mf(z)]^{(m-1)}$$ for a pole with order $m$. But I'm pretty sure that I should not try to compute the 6th derivative of $\dfrac{1}{(z+i)^7}$. Is there another way to compute the residue beside this formula?",['complex-analysis']
2801990,Calculating two parameters for a two variable equation,"I have the following equation: $$y=1-\frac{k}{x^b}, k>0$$ When $(x=4,y=0)$ and $(x=200, y=0.9)$. How do I solve this equation elegantly? When I try to do it, it always ends up being $b=0$ and $k=1$. My end goal is, to create a function which has lim y=0 and lim x=1 (that I can control two points on the graph) which looks like this formula.","['algebra-precalculus', 'calculus']"
2802017,about Spivak calculus on manifold theorem 1-4 proof.,"Here is the theorem 1-4: If B is compact and $\mathcal{O}$ is an open cover of $\{x\}\times B$,
  then there is an open set U $\in R^n$ containing $x$ such that $U\times
B$ is covered by a finite number of sets in $\mathcal{O}$. Spivak's proof goes like this: Since $\{x\} \times B$ is compact, we can assume at the outset that
  $\mathcal{O}$ is finite, and we need only find the open set $U$ such
  that $U\times B$ is covered by $\mathcal{O}$. For each $y \in B$ the
  point $(x,y)$ is in some open set $W$ in $\mathcal{O}$. Since $W$ is
  open, we have $(x,y) \in U_y \times V_y \subset W$ for some open
  rectangle $U_y \times V_y$. The sets $V_y$ cover the compact set $B$,
  so a finite number $\{V_{y_1}, ... , V_{y_k}\}$ also cover B. Let $U = U_{y_1}
 \cap ...\cap  U_{y_k}$ Then if $(x',y') \in U \times B$, we have  $y'
 \in V_{y_i}$ for some $i$, and certainly $x' \in U_{y_i}$ Hence
  $(x',y') \in U_y \times V_y$,, which is contained in some $W$in
  $\mathcal{O}$ I am happy with the first paragraph of the proof. But it seems to me that the use of the compactness of $B$ in the second paragraph is not necessary. My alternative second paragraph goes like this: Take $U = \cup_y U_y$. Then $U$ is open because each of $U_y$ is open.
  Each of $U_y$ is a subset of one of the elements of
  $\mathcal{O}$ ($W$). So $U$ is covered by the finite cover $\mathcal{O}$. $ B$
  is also covered by the finite cover $\mathcal{O}$. Thus $U\times B$ is
  covered by a finite subcover $\mathcal{O}$ I think this is likely to be wrong and i am missing something. Any clues? Edit: found the problem : it actually make little sense saying O covers B or U. They don’t live in the same dimensions.","['general-topology', 'calculus']"
2802062,Fermat primality test,"Is the following a correct statement for the Fermat primality test? For all $b$ if $n$ is prime and $(b,n)=1$ then $b^{n-1} \equiv 1 \bmod{n}$. The contrapositive is if $b^{n-1} \not \equiv 1 \bmod{n}$ then condition (1) and/or (2) is false (they are equivalent --both attest to $n$ not being prime).","['number-theory', 'prime-numbers', 'primality-test']"
2802084,Continuous function in compact sets - Real analysis,"I have the following proposition. $\textbf{Proposition:}$ If $f:X\times K\longrightarrow\mathbb{R}^n$ is a continuous function, $K$ is a compact set in $\mathbb{R}^n$ and $X\subseteq\mathbb{R}^n$ . Fixed $x_0\in X$ , prove that for every $\varepsilon>0$ exist $\delta>0$ such that if $\|x-x_0\|<\delta$ then $\|f(x,\alpha)-f(x_0,\alpha)\|<\varepsilon$ for every $\alpha\in K$ . $\textbf{Proof Sketch:}$ Fix $x_0\in X\subseteq\mathbb{R}^n$ . Consider the set $A=\{x_0\}\times K\subseteq\mathbb{R}^{2n}$ ; then $A$ is compact in $X\times K\subseteq\mathbb{R}^{2n}$ with the usual metric since $\{x_0\}$ is a finite set and therefore compact and $K$ is also compact in $\mathbb{R}^n$ by hypothesis and Cartesian product of compact sets is compact. Consider the open balls in $\mathbb{R}^{2n}$ centred in $(x_0,\alpha)\in A$ and radius 1, lets say for simplicity $$A(\alpha)=B_{\mathbb{R}^{2n}}((x_0,\alpha),1)=\{y\in\mathbb{R}^{2n}\ :\ \|y-(x_0,\alpha)\|<1\}.$$ Then, we have an open cover for $A:$ $$A\subseteq\bigcup_{\alpha\in K}A(\alpha).$$ Since $A$ is compact there exist $\alpha_1,\alpha_2,\cdots,\alpha_m\in K$ such that $$A\subseteq\bigcup_{i=1}^{m}A(\alpha_i).$$ Applying $f$ to both sides, by property of the direct image then $$f(A)\subseteq f\left(\bigcup_{i=1}^{m}A(\alpha_i)\right)=\bigcup_{i=1}^{m}f(A(\alpha_i)).$$ Since $f$ is continuous then $f(A)$ is compact in $\mathbb{R}^n$ , and by definition $$f(A)=\{f(x_0,\alpha)\in\mathbb{R}^n\ :\ \alpha\in K\}$$ $$f(A(\alpha_i))=\{f(x,\alpha)\in\mathbb{R}^n\ :\ \|(x,\alpha)-(x_0,\alpha_i)\|<1\}$$ for each $i=1,2,\cdots,m$ . Let $\varepsilon>0$ be arbitrary, consider the open balls in $\mathbb{R}^{n}$ centred in $f(x_0,k)$ and radius $\varepsilon$ for every $k\in K$ and define for simplicity $$B(k,\varepsilon)=B_{\mathbb{R}^n}(f(x_0,k),\varepsilon)=\{z\in\mathbb{R}^n\ :\ \|z-f(x_0,k)\|<\varepsilon\}.$$ Then, for every $\varepsilon>0$ we have an open cover for $f(A)$ $$f(A)\subseteq\bigcup_{k\in K}B(k,\varepsilon).$$ Since $f(A)$ is compact there exist $k_1,k_2,\cdots,k_{m'}\in K$ such that $$f(A)\subseteq\bigcup_{j=1}^{m'}B(k_j,\varepsilon).$$ Recall that $f$ is continuous and so there is $\delta_j>0$ such that $f(B'_j)\subseteq B(k_j,\varepsilon)$ where $B'_j$ is the open ball in $\mathbb{R}^{2n}$ centred in $(x_0,k_j)$ with radius $\delta_j$ for each $j=1,2,\cdots,m'$ . That is to say, $$B'_j=B_{\mathbb{R}^{2n}}((x_0,k_j),\delta_j)=\{y\in\mathbb{R}^{2n}\ :\ \|y-(x_0,k_j)\|<\delta_j\}$$ $$f(B'_j)=\{f(x,\alpha)\in\mathbb{R}^{n}\ :\ \|(x,\alpha)-(x_0,k_j)\|<\delta_j\}$$ Then, doing the joint of sets $$\bigcup_{j=1}^{m'}f(B'_j)\subseteq\bigcup_{j=1}^{m'}B(k_j,\varepsilon)$$ $\textbf{¡Stack!:}$ This where I get stuck; I've been thinking for a long time without resolution how to relate each $\alpha_i$ with each $k_j$ so that considering something like $\delta=\min\{1,\delta_1,\delta_2,\cdots,\delta_{m'}\}$ and assuming $\|x-x_0\|<\delta$ then I can get the desired result. I also think that this ""proof"" can be simpler without having to consider so many sets, however it's the path that comes to my mind. I have an introductory knowledge in real analysis so I look for any help you can give me. Edit Inspired by the contribution made by Mikhail Katz: $\textbf{Proof Sketch 2:}$ Fix $x_0\in X\subseteq\mathbb{R}^n$ . Consider the closed ball in $\mathbb{R}^n$ centred in $x_0$ with radius 1: $$B=B[x_0,1]=\{x\in\mathbb{R}^n:\|x-x_0\|\leq1\}$$ Every closed ball in $\mathbb{R}^n$ is compact so $B$ is compact and so closed and bounded in $\mathbb{R}^n$ . Consider the adherence (closure) of $X$ , this is $\overline{X}$ . Define $L=\overline{X}\cap B=\{x\in \overline{X}:\|x-x_0\|\leq1\}$ . Since $L\subseteq B$ then $L$ is bounded.
Since $\overline{X}$ and $B$ are closed and intersection of closed sets is always closed then $L$ is closed. So, by Heine-Borel's Theorem $L$ is compact in $\mathbb{R}^n$ . Define $S=X\cap B=\{x\in X:\|x-x_0\|\leq1\}\subseteq X$ . Therefore $S\subseteq L$ and $\overline{S}=L$ ; so $S$ is dense in $L$ . By hypothesis $f$ is continuous in $X\times K$ ; particularly $f$ is continuous in $S\times K$ . Since $S$ is dense in $L$ then $S\times K$ is dense in $L\times K$ . Therefore exist a continuous extension of $f$ from $S\times K$ to $L\times K$ , say $g$ . The set $L\times K$ is a Cartesian product of compact sets in $\mathbb{R}^n$ so it's compact in $\mathbb{R}^{2n}$ . Since $g$ is continuous in $L\times K$ then $g$ is uniformly continuous. For every $\varepsilon>0$ there is $\delta>0$ such that if $\|(x_1,\alpha_1)-(x_2,\alpha_2)\|<\delta$ then $\|g(x_1,\alpha_1)-g(x_2,\alpha_2)\|<\varepsilon$ for all $(x_1,\alpha_1),(x_2,\alpha_2)\in L\times K$ . In particular, for $x_1=x\in S$ , $x_2=x_0\in S$ and $\alpha_1=\alpha_2=\alpha\in K$ . Suppose: $$\|(x,\alpha)-(x_0,\alpha)\|=\|(x-x_0,\bar{0})\|=\|x-x_0\|<\delta$$ Since $g$ is an extension of $f$ and $x,x_0\in S$ then: $$\|g(x,\alpha)-g(x_0,\alpha)\|=\|f(x,\alpha)-f(x_0,\alpha)\|<\varepsilon$$ for all $\alpha\in K$ . $\textbf{Observation:}$ To guarantee the existence of $g$ it's sufficient that the set $S\times X$ be closed. If not, the statement may not be true. I don't know if this condition is satisfied or not in this context because $X$ is given as a nonempty set of $\mathbb{R}^n$ and nothing more about itself. For example, if we had in the hypotheses that $X\subseteq\mathbb{R}^n$ is closed then $X=\overline{X}$ and so $L=S$ ; the proof is complete and would be even simpler.","['continuity', 'real-analysis', 'compactness', 'functions']"
2802133,Characterization of the Hasse invariant using the dual Frobenius map,"This question comes from Hartshorne's Algebraic Geometry. I'm trying to show that given an elliptic curve $E \to \operatorname{Spec} k$, $\operatorname{char} k = p>0$, that the Hasse invariant of $E$ is one if and only if the dual Frobenius morphism (I think sometimes called the relative Frobenius) $\hat F' :X \to X_{(p)}$ is separable. The hint is to use that the tangent space to an elliptic curve is the tangent space to the Jacobian (isomorphic to $E$), which is just $H^1(X,\mathcal{O}_X)$. I can use Lemma 50.12.1 from the Stacks Project to conclude that $\hat F' :X \to X_{(p)}$ is separable if and only if $\hat F'^* \Omega_{X_{(p)}} \to \Omega_X$ is nonzero, which now by Hartshorne Proposition IV.2.1 this happens if and only if the sequence  $$0 \to \hat F'^* \Omega_{X_{(p)}} \to \Omega_X \to \Omega_{X/X_{(p)}} \to 0$$ is exact. If I can use this to get a nonzero map on cohomology $H^0(X,\Omega_{X}) \to H^0(X,\Omega_X)$, I can get the desired map using Serre Duality, which is the definition of nonzero Hasse invariant. However I'm not sure how to get the map on cohomology above.","['arithmetic-geometry', 'elliptic-curves', 'algebraic-geometry']"
2802170,Using Snake Oil Method to Evaluate Sum,"$$\sum_k \binom{n+k}{2k} \binom{2k}{k}\frac{(-1)^k}{k+1+m}$$
This is Problem 8 in ""Basic Practice"" Section of Concrete Maths by Knuth. In the book, the answer comes out to be: 
$$ (-1)^n \frac{m!n!}{(m+n+1)!}\binom{m}{n}$$ I want to know how to use generating functions to solve this problem since the original method is quite complicated and uses several clever tricks which i want to avoid. I have used the Snake Oil method and transformed the sum to: $$\frac{1}{1-x}\sum_k \frac{ \dbinom{2k}{k} \left(\dfrac{-x}{(1-x)^2}\right)^k}{k+1+m} $$ ... by using the steps given in Example 2 at pg 122 of Herbert Wilf's book on generating functions. Now, i am stuck, since i don't know what to do with the $k+1+m$ in the denominator. I also can't deduce whether this problem can even be solved by generating functions or not.","['generating-functions', 'combinatorics', 'summation']"
2802195,"For any two functions $f_1 : [0,1] →\mathbb R$ and $f_2 : [0,1] →\mathbb R$, check which among the statements are true.","For any two functions $f_1 : [0,1] →\mathbb R$ and $f_2 : [0,1]
 →\mathbb R$, deﬁne the function $g : [0,1] →\mathbb R$ as $g(x) =
 \max(f_1(x),f_2(x))$ for all $x ∈ [0,1]$. A. If $f_1$ and $f_2$ are linear, then $g$ is linear B. If $f_1$ and $f_2$ are diﬀerentiable, then g is diﬀerentiable C. If $f_1$ and $f_2$ are convex, then g is convex D. None of the above My attempt:- $g(x) =
 \max(f_1(x),f_2(x))=\frac{f_1(x)+f_2(x)}{2}+\frac{|f_1(x)-f_2(x)|}{2}$ A. $g(0)=\max\{f_1(0),f_2(0)\}=0$ $g(cx+y)=\max\{f_1(cx+y),f_2(cx+y)\}=\frac{f_1(cx+y)+f_2(cx+y)}{2}+\frac{|f_1(cx+y)-f_2(cx+y)|}{2}=\frac{cf_1(x)+f_1(y)+cf_2(x)+f_2(y)}{2}+\frac{|cf_1(x)+f_1(y)-(cf_2(x)+f_2(y))|}{2}=c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))+(f_1(y)-f_2(y))|}{2}$. $c(f_1(x)-f_2(x))$ and $(f_1(x)-f_2(x))$ lie in the same line. so, Equality holds in the triangular ineqality. $c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}\implies$
$c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+|c|\frac{|(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}$
It need not be linear. Since,If $c<0$, $|c|=-c$. B. $\lim_{h\to 0}\frac{g(h)-g(0)}{h}=\lim_{h\to 0}\frac{f_1(h)+f_2(h)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-(\frac{f_1(0)+f_2(0)}{2h}+\frac{|f_1(0)-f_2(0)|}{2h})=\lim_{h\to 0}\frac{f_1(h)-f_1(0)+f_2(h)-f_2(0)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-\frac{|f_1(0)-f_2(0)|}{2h}$ How do I proceed further? C. Let $x,y \in[0,1],g(tx+(1-t)y)=\max(f_1(tx+(1-t)y),f_2(tx+(1-t)y))=\frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)-f_2(tx+(1-t)y)|}{2}\leq \frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)|}{2}+\frac{f_2(tx+(1-t)y)|}{2}$ How do I proceed further? I couldn't find any counterexample for $B$ and $C$. So, I tried to prove it. I am not able to complete the proof. Plese help me.","['derivatives', 'real-analysis', 'convex-analysis']"
2802213,Sangaku - Find diameter of congruent circles in a $9$-$12$-$15$ right triangle,"My attention was brought to a sangaku problem in this book by Ubukata Tou. It shows this figure: The question asks us to find the diameter of the circles (both circles are congruent) in a right triangle ($∠ABC = 90$), where $AB = 9$ and $BC = 12$. It also says that the diameter of the two circles is $30/7$. How would you solve this problem. In the book, it also states that this was a problem from the early Edo period suggesting that trigonometry may not have been around in Japan then . It would be very interesting to see a solution without the use of trigonometry then.","['circles', 'sangaku', 'euclidean-geometry', 'triangles', 'geometry']"
2802290,Solution of $\nabla^2u=u_{xx}+u_{yy}=0$?,"Calculate the Fourier expansion of $u$ for $$\nabla^2u=u_{xx}+u_{yy}=0,\\ y\ge0,0\le x\le L\\ u(0,y)=0=u_x(L,y), u(x,0)=g(x)=x/L$$. Solution: By separation of variables, we propose the solution $u(x,y)=X(x)Y(y)$. After calculations and after applying the conditions I get that  the general solution is $u(x,y)=\sum_{n=0}^\infty A_ne^{\sqrt{(\lambda_n)}y}D_n\sin(\sqrt \lambda_nx),$ where $A_n=\frac{(g(x),X_n(x))}{\Vert X_n\Vert^2}, X_n(x)=\sin(\frac{(n+1/2)\pi}{L}x),\lambda_n=(\frac{n\pi}{L})^2$ I found $A_n$ when I applied the boundary condition $u(x,0)$. Is there a way to find $D_n$ so the solution would be complete? Or the solution it's fine as it is? Thanks in advance for your time and help.","['fourier-series', 'alternative-proof', 'fourier-analysis', 'partial-differential-equations', 'ordinary-differential-equations']"
2802316,General position is preserved under isomorphisms of algebraic varieties,"I was reading the General position article on Wikiepdia, and came across the following sentence: General position is preserved under biregular maps – if image points satisfy a relation, then under a biregular map this relation may be pulled back to the original points. Can someone point me to a reference where this is stated and proved? I cannot find any. I tried looking in Harris's Algebraic Geometry - A First Course and in Shafarevich's Basic Algebraic Geometry . And just to make sure I understand, is this what the sentence from Wikipedia is stating? Let $X \subset \mathbb R^m$ and $Y \subset \mathbb R^n$ be affine algebraic varieties and let $\phi : X \to Y$ be an isomorphism. Suppose $\{x_1, \ldots, x_k\} \subset X$ is such that for every $d \leq m+1$, no $d$ points of $\{x_i\}$ are not contained in an affine $(d-2)$-plane in $\mathbb R^m$. Then for every $d \leq n+1$, no $d$ points of $\{\phi(x_i)\}$ are not contained in an affine $(d-2)$-plane in $\mathbb R^n$. (It seems like there's also a statement for projective varieties, but I think they are equivalent.) EDIT: Thanks to Kenny Wong for pointing out that the statement above is not was is meant by the sentence from Wikipedia. However, in this case, I am confused the argument that 5 points in general linear position determine a conic in $\mathbb R^2$. I thought this was the argument: Start with 5 points $\{x_i\} \subset \mathbb R^2$ in genereal linear position. Let $y_i \in \mathbb R^5$ be the image of $x_i$ under the Veronese embedding. Since the Veronese embedding is an isomorphism onto its image, the points $\{y_i\}$ are in general linear position . Hence, there is a unique affine hyperplane passing through these 5 points, which gives us the unique conic. In the sentence in bold, don't we use the fact that the Veronese embedding preserves general linear position? Is this something special for the Veronese embedding (i.e., is not a property of all biregular maps)?",['algebraic-geometry']
2802325,"$N/F$ is finite galois. $E/F,L/F$ galois and $E,F\subset N$. Then $E\otimes_F L$ is a product of fields","$N/F$ is finite galois. $E/F,L/F$ galois and $E,F\subset N$. Then $E\otimes_F L\cong\prod_i EL$ where $i\in I$ with $|I|=\frac{G(N/F)}{G(N/E)G(N/F)}$. Since $E/F,L/F$ are galois, $G(N/E),G(N/F)$ are normal subgroups of $G(N/F)$. I can finish the proof easily by directly using simple extension argument. $E=\frac{F[x]}{(f)}=F(a)$ and $L=\frac{F[y]}{g}$. So $E\otimes_F L\cong F(a)\otimes_F\frac{F[x]}{g}\cong\frac{F(a)[y]}{(g)}$. Now from galois correspondence, $g$'s factor will split into product of degree $[L:E\cap L]$ polynomials. Now $G(N/E)G(N/L)$ fixes $E\cap L$. So $|\frac{G(N/F)}{G(N/E)G(N/F)}|$ counts exactly how many irreducible factors $g$ will have in $F(a)[y]$. $\textbf{Q:}$ I am trying a different proof by group ring method. Let $G_1=G(E/F), G_2=G(L/F)$. $E\cong F[G_1]$ as $F[G_1]$ module but this is also $F$ module isomorphism by $F[G_1]$ linearity. Similarly $L\cong F[G_2]$. So $E\otimes_FL\cong F[G_1]\otimes_FF[G_2]\cong F[G_1\times G_2]$. $F[G(EL/F)]\cong EL$ as $EL/F$ is galois. Now $G(EL/F)\to G_1\times G_2$ is injection. I want to write $G(EL/F)$'s coset in $G_1\times G_2$. However, I cannot directly see correspondence with $\prod_{gG(N/E)G(N/L)}EL$. I want to follow this path. Hint will be sufficient. Ref: Taylor Frolich Algebraic Number Theory Exercise Chpt 1.2","['number-theory', 'abstract-algebra', 'galois-theory']"
2802347,Explaining the trigonometric addition formulas via composition of rotations,"I am generally dissatisfied with the way trigonometric addition formulas like
$$
\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
$$
are derived in high school textbooks. There are numerous proofs, some of which are short but unintuitive, some of which introduce unnecessary calculations, many with restrictions on $\alpha$, $\beta$ and $\alpha + \beta$, like having to lie between $0$ and $\frac{\pi}{2}$ radians. To me, the addition formulas are simply a coordinatization of the observation that composing a rotation by $\alpha$ with a rotation by $\beta$ yields a rotation by $\alpha + \beta$. Hence in my opinion the proper way to prove such a formula would be to observe that
$$
\begin{bmatrix} \cos(\alpha + \beta) & -\sin(\alpha + \beta) \\ \sin(\alpha + \beta) & \cos(\alpha + \beta) \end{bmatrix} = \begin{bmatrix} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{bmatrix} \cdot \begin{bmatrix} \cos(\alpha) & -\sin(\alpha) \\ \sin(\alpha) & \cos(\alpha) \end{bmatrix}
$$ then work out the product on the right to obtain the formulas by equating entries. However, such a proof is beyond the scope of a high-school textbook, as high schoolers - if they know about matrices at all - are rarely taught the link between composition of linear transformations and matrix multiplication. Is there a way to salvage the essence of this proof - that the formulas are merely a way of expressing that a rotation by $\alpha+ \beta$ can be obtained by composing rotations by $\alpha$ and $\beta$ - without using linear algebra? Of course complex numbers are also out of the question.","['education', 'trigonometry', 'linear-algebra']"
2802352,Classyfing the simple submodules of $\mathbb{C[Q_8]}$,"I'm taking a course in abstract algebra, at the moment we are working on the Wedderburn theorem and I'm still unable to understand how to use it properly to solve exercises. I tried to solve a problem and explain my reasoning to see where are the holes in my understanding and to see if there's some other ways of thinking about it or suggestions for these type of problems. The problem I tried to solve is the following: Find the simple submodules of $\mathbb{C[Q_8]}$ (up to isomorphism). First by Maschke's theorem I know that the group algebra is semisimple because char( $\mathbb{C}$ ) doesn't divide char( $\mathbb{Q_8}$ ). This means I can now use Wedderburn's theorem as it's a finite dimensional algebra. By this theorem I have the following decomposition: $$ \mathbb{C[Q_8]} \simeq \prod_{i \in I} M_{n_i}(\mathbb{C})$$ As it's an isomorphism, they both must have the same dimensions over $\mathbb{C}$ . The dimension of $\mathbb{C[Q_8]}$ over $\mathbb{C}$ is 8. This implies that the only possible dimensions of the matrices algebras are $n_1=2,n_2=1,n_3=1,n_4=1,n_5=1$ because otherwise if $n_i = 1$ for $i \in \{ 1,...,8\}$ this would imply $\mathbb{C[Q_8]}$ is commutative which would imply $\mathbb{Q_8}$ is abelian as a group. The other cases $n_1 = 2, n_2 = 2$ cannot happen as there is always a submodule of a group algebra of dimension 1 (the one generated by $\sum_{h \in \mathbb{Q_8}} h$ , is this correct?). To find these subspaces isomorphic to $\mathbb{C}$ I considered the $x \in \mathbb{C[Q_8]}$ such that $i*x,  j*x,  k*x$ is a complex multiple of $x$ . From these I found the four subspaces, one of them being the one that's guaranteed to exist.
My problem is I can't seem to find the subspace isomorphic to $M_2(\mathbb{C})$ . Any tips and help would be greatly apreciated. I tried to solve the problem in another way. I read on the internet but couldn't prove it that $dim_{\mathbb{C}} Z(\mathbb{C[Q_8]}) = \# \text {conjugacy classes of  $\mathbb{Q_8}$}$ . I haven't worked that much with conjugacy classes but I gave it a try in this problem as I have seen that also, $dim_{\mathbb{C}} Z(\mathbb{C[Q_8]}) = \# \text {simples in the Wedderburn decomposition }$ . By this I understood the number of matrices algebras in the product that is isomorphic to the algebra $\mathbb{C[Q_8]}$ . In this particular case, the conjugacy classes I found are 5: the conjugacy class of $\{1\}$ and $\{-1\}$ as they both are in the center of the group. For each $i,j,k$ I found they are on their own conjugacy class. So indeed, the number of conjugacy classes is the same as the number of matrices algebras in the product, i.e 5. One question I have is if from these conjugacy classes I can get more information about these matrices algebras or they just determine the number of them in the product? I would very happy if someone could point me to notes or books that explain these theorem and if possible with exercises similar to the one I asked. Thanks for your help.","['abstract-algebra', 'modules', 'proof-verification']"
2802377,"Compute the number of n-tuples that contains odd numbers of 0's and 1's, using generating function","I think the approach should be to break the n-tuples into 3 parts, but I'm trying to solve a simpler problem, by changing the set of choices from $\{0,1,2\}$ to $\{0,1\}$ . My approach in this new problem is to specify two sequences (instead of three as in the original one): $a_n :=$ number of way to fill a set of n elements with an odd number of 0's. So $$a_n = \{0,1,0,1,0,1,0,1\ldots\}$$ Similarly, $b_n :=$ number of way to fill a set of n elements with an odd number of 1's. So $$b_n = \{0,1,0,1,0,1,0,1,\ldots\}$$ I found that the exponential generating function of $a_n$ and $b_n$ is: $$\hat{A}(x) = \hat{B}(x)= \sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)!} = \sinh(x)$$ I believe that $\hat{C}(x) := \hat{A}(x) \times \hat{B}(x) = \sinh^2(x)$ is the generating function of the modified problem, but I don't know how to simplify it to obtain a closed formula for $c_n$ . Also, am I right to assume that the difference between my problem and the original one is that the solution to the latter is ""larger"" than the former's? For example, if $n = 3$ , then my problem's $c_n$ should have a count of 0 (since there is no way to have an odd number of 0's and an odd number of 1's that sum into 3), but the original problem's should have a count of 1 (since we can add a 2 into (0,1,_) and obtain a satisfying tuple). Finally, going from my problem to the original one, how should I think about the remaining sequence?","['generating-functions', 'combinatorics', 'discrete-mathematics']"
2802382,"For $f$ continuous in $[0,1]$, differentiable in $(0,1)$ s.t. $f(0)=0$, prove $|f'(x_0)|\geq |f(x)|$ for some $x_0$ and all $x$ in $[0,1]$","Let $f:[0,1] \rightarrow \mathbb{R}$ be continuous in $[0,1]$ and differentiable in $(0,1)$, such that $f(0)=0.$ Prove that there is $x_0\in [0,1]$ such that $|f'(x_0)|\geq |f(x)|$ for all $x\in [0,1].$ Attempt. If we assume, by contradition, that for all $x\in [0,1]$ there is $z_x\in [0,1]$ such that: $$|f'(x)|<|f(z_x)|$$ then for all $n$ there is $z_n\in [0,1]$ such that $|f'(1/n)|<|f(z_n)|$. Even if we pass to a convergent subsequence, we do not get a contradiction, since $f'$ is not differentiable at $0$. I assume that this is not the right path to a solution. Thanks in advance for the help.","['derivatives', 'real-analysis', 'continuity', 'calculus']"
2802390,Examples of when the Riesz representation theorem doesn't hold,"I was wondering if anyone could give me some interesting ""counter examples"" to the Riesz representation theorem about functionals over Hilbert spaces.
When I say counter examples, I'm obviously talking about examples where some of the basic assumptions of the theorem aren't met, so the theorem doesn't hold.
In other words - could you show me some non-trivial examples of functionals over inner-product spaces that cannot be expressed as an inner-product with some vector in the vector space?
I already have an example from $C [0, 1]$ based on the standard $L^2$ integral inner-product, but I was wondering if anyone could enlighten me with a more interesting example.
I don't have much background, but I'm very interested to hear about this topic, and I'd appreciate it if you could give full explanations so I could understand.
Thanks in advance","['riesz-representation-theorem', 'examples-counterexamples', 'hilbert-spaces', 'functional-analysis', 'linear-algebra']"
2802403,"If $g$ is 2 times differentiable in $[a,b]$ and $g''+g'\,g=g$ and $g(a)=g(b)=0$, prove that $g=0$.","Let $g:[a,b]\rightarrow \mathbb{R}$ two times differentiable such that $$g''(x)+g'(x)\,g(x)=g(x),~x\in [a,b]$$
  and $g(a)=g(b)=0$. Prove that $g(x)=0$ for all $x\in [a,b].$ Attempt . It seemed like one of those exercises where multiplying be a suitable factor  we get a derivative. I started by multiplying with $g$, after with $e^g$ but I dind't get what I expected. Am I on the wrong path? Thanks in advance for the help!","['derivatives', 'real-analysis', 'calculus']"
2802404,"Let $X$ be an infinite set and $τ$ a topology on $X$. If every infinite subset of $X$ is in $τ$, prove that $τ$ is the discrete topology.","I have been thinking about this one for awhile, but I cannot crack it. I think the proof to this is similar to the proof that the topology containing all singleton sets is the discrete topology, in that that proof used the infinite union of singletons to build every subset. In this one I imagine that some how the use of finite intersections can create every finite subset of $X$, but that seems weird and I'm not sure how I would show that. Is this correct? Can someone show me where to start?",['general-topology']
2802415,Jensen's inequality for random functions in a Banach space,"Suppose I have a Banach space of functions over $\mathbb{R}$ with norm $||\cdot||$. Suppose that $f$ is a random function that takes values in this space such that $||f||\leq 1$. Suppose that for all $x\in\mathbb{R}$ $E[f(x)]$ exists and is finite (where the expectation is over the random function $f$). Can I then apply Jensen's inequality to get $||E[f]||\leq E[||f||] \leq 1$? Where $E[f]$ is the function of $\mathbb{R}$ defined by $E[f](x)=E[f(x)]$. Moreover, suppose that I do not know that $E[f(x)]$ is finite for all $x$. Can I then replace $f$ with some related function $\hat{f}$ so that $E[\hat{f}(x)]$ exists and is finite for all $x$ and $||\hat{f}-f||=0$. In the case where the norm is an $L_p(\mu)$ norm for some measure $\mu$ over $\mathbb{R}$, then $||\hat{f}-f||=0$ means that the two functions differ only on a subset of $\mathbb{R}$ of $\mu$ measure 0. I think I can prove the results above directly for $L_p$ norms (and easily for the supremum norm) but I wondered if they might be known to hold more generally either for all norms or perhaps all monotone norms. Any help greatly appreciated!","['functional-analysis', 'random-functions', 'measure-theory', 'jensen-inequality']"
2802424,Are Lp norms equivalent on bounded functions with compact support?,"Are Lp norms equivalent on certain space of functions? Here, functions are of course infinite dimensional.  I know that if functions are allowed to have singularity and be non-zero everywhere, then Lp norms are certainly not equivlent. If functions have support and no singularity, will things change? If I have to go further, giving a strick upperbound to the functions would be enough right?","['functional-analysis', 'lp-spaces']"
2802435,The expected number of random variables that should be drawn without replacement whose sum exceeds some threshold.,"Say we have a finite population of $N$ points $\{x_1,\dots,x_N\}$, and we draw samples at random without replacement until their sum exceeds some threshold $t$. We may assume that:  $\forall i\ $ $0<x_i<1$, $\sum_{i=1}^{N} x_i=k$  for a known $k$, and that $0<t<k$ is known. What is the expected number of draws? Is there a generalization of Wald’s Equation that applies to this case? Non-trivial bounds would be helpful too.","['probability-theory', 'probability', 'statistics']"
2802444,Equidifferentiable iff derivative is equicontinuous?,"Let $\{f_n\}$ a sequence of function differentiable at $x_0$ We have equidifferentiability at $x_0$, if $\lim_{h \to 0} \max_n \left|   \frac{f_n(x_0+h) - f(x_0)}{h}- f'_n(x_0)\right| = 0$ Are the two following statement true? (1?) The family is equidifferentiable at $x_0$ iff the derivatives $f'_n(x_0)$ are equicontinuous at $x_0$. (2?) If the derivatives $f'_n(x_0)$ are uniformly bounded, then $f_n(x_0)$ is equicontinuous at $x_0$ The result would be motivated by similar results when studying a single function $f$ over multiple $x$, (1') A function $f$ is uniformly differentiable iff its derivative is uniformly continuous. (2') If the derivative of $f$ is bounded, then $f$ is uniformly continuous.","['derivatives', 'sequence-of-function', 'equicontinuity', 'uniform-continuity']"
2802451,Constant sectional curvature and unit normal vector to a totally geodesic hypersurface,"I was reading about totally geodesic hypersurfaces when I found the next proposition: Proposition: The sectional curvature $K$ of $M$ is constant at $p$ if and only if every unit vector in $T_{p}M$ is normal to a totally geodesic hypersurface at $p$ . The proof is following by Codazzi equation. I'm stuck proving this. If $K$ is constant, we get from Codazzi equation that $R_{xy}x=K(\langle x,x\rangle y-\langle x,y\rangle x).$ Then,for nonnull $x\perp y$ such equation becomes $R_{xy}x=\langle x,x\rangle K(x,y) y.$ But I don't get how this works to prove that each unit vector on $T_{p}M$ is normal to a totally geodesic hypersurface.
I know that a semi-Riemann submanifold is totally geodesic if the shape tensor vanishes: $\mathrm{II}=0$ but I can't see how this works with the above to get the desire result. For the other direction I'm not sure how to proceed to get that $K$ is constant. Any kind of help is thanked in advanced.","['semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
2802452,Proof verification of finding the maximum order of a group generated by two elements satisfying some relations,"Suppose that $G$ is the group generated by $a,b$ satisfying the relations $a^8=b^2a^4=ab^{-1}ab=e$. I wish to show that $G$ has order at most 16.
There is already a solution to this problem here order of group generated by two element with some relation. but I did not find it natural to follow, for example since that was rather wordy with unnatural objects, so I am writing my own solution which is very brief and natural: $ab=ba^{-1}=ba^7$ so $ba=aba^{-6}=a^2ba^{-5}=...=a^7b$ so that every element of $G$ is of the form $a^ib^j$. In addition, $b^2=a^4$ so that in the representation $a^ib^j$, $j=0,1$. Also, of course $i=0,1,...,7$. So the maximum order is $2*8=16$. Is my solution right?","['abstract-algebra', 'group-theory']"
2802469,"Consider $X(s,v)=c(s)+r(N(s)\cos(v)+B(s)\sin(v)) $. Show: X regular if and only if $r< \frac{1}{k(s)}$","Let $c : (a,b) \rightarrow \mathbb{R}^3 $ be a space curve ( parameterized by arc length ) with curvature $k(s) \neq 0 \ \forall s \in (a,b) $. 
  Consider the tube $X : (a,b) \times \mathbb{R} \rightarrow  \mathbb{R}^3  $ with: 
  $$X(s,v)=c(s)+r(N(s)\cos(v)+B(s)\sin(v)) $$
  Remark: $N(s)$, $B(s)$  the normal, the binormal vector of $c(s)$. Moreover $r>0$. Show that : $ X $ is regular if and only if $r< \frac{1}{k(s)} \forall s \in (a,b)  $. So first of all we know that $c$ is parameterized by arc length. So we can use frenet equations: 
$c'(s) = T(s) = k(s)N(s)$ , $N'(s) = -k(s)T(s) + \tau(s)B(s)$ and $B'(s) = -\tau(s)N(s)$.  Remark: $\tau$ is torsion. We want that X is regular ( if and only if $k(s) \neq 0 \ \forall s \in (a,b) $). So we have to show that $D_X(s,v)$ has full rank. If I'm right the first column of $D_X(s,v)$ is $c'(s) +
r(N'(s)\cos(v)+B'(s)\sin(v))$ and the second one is $r(-N(s)\sin(v)+B(s)\cos(v))$. My idea is to show that these two vectors are linearly independent ( maybe with help of frenet?),  which means that $D_X(s,v)$ has full rank. I hope that you can help me with this.","['curves', 'differential-geometry', 'curvature']"
2802484,Combinatorial (and Algebraic Proof) of an Identity Involving Lah Numbers,"Question The question is to prove the following identity $$
L_{n+1, k+1}=\sum_{i=0}^nL_{i,k}(n+k+1)_{n-i}\tag{1}
$$ where $(n)_k$ denotes the falling factorial of length $k$ and $L_{n, k}$ denotes the Lah Numbers . This question is from Aigner's A Course in Enumeration . Context I am free to use the characterizations of the Lah Numbers $L_{n,k}$ as the connecting coefficients between the rising factorials and falling factorials or that it counts the number of ways to partition the set $[n]$ into $k$ nonempty linearly ordered subsets and have also proven the identity
$$
(x+1)^{(n)}=\sum_{k=0}^n L_{n+1, k+1}(x-1)_k\tag{2}
$$
where $x^{(n)}$ denotes the rising factorial. I am also aware of a two term recurrence for the Lah Numbers but would like to prove (1) without it. Attempt Combinatorial Proof Rewriting (1) as
$$
L_{n+1, k+1}=\sum_{i=0}^nL_{n-i,k}(n+k+1)_{i}\tag{3}
$$ 
my idea is to choose and arrange $i$ elements to be the same block as $1$ and then to partition the remaining $n-i$ elements into $k$ linearly ordered blocks. The problem is I am having trouble interpreting what $(n+k+1)_{i}$ means in this context. Algebraic Proof I have two ideas here and was unable to get far with both of them. First we can use the fact that $L_{i,k}=\frac{i!}{k!}\binom{i-1}{k-1}$ and substitute into the RHS of (1) and to manipulate the resulting binomial coefficients but it is a mess. My second idea was to show
$$
\sum_{k=0}^n\left(\sum_{i=0}^nL_{i,k}(n+k+1)_{n-i}\right)(x-1)_k=(x+1)^{(n)}\tag{4}
$$
and conclude using (2) but was unable to get beyond interchanging summation. Any help on a combinatorial or algebraic approach is welcome.","['combinatorics', 'binomial-coefficients', 'discrete-mathematics']"
2802487,Integral $\int_0^1 \frac{\sqrt x \ln x} {x^2 - x+1}dx$,"I am trying to evaluate $$I=\int_0^1 \frac{\sqrt x \ln x} {x^2 - x+1}dx=\int_0^1 \frac{\sqrt x (1+x)\ln x} {1+x^3}dx$$ Now if we expand into  geometric series: $$I=\sum_{n=0}^{\infty} (-1)^n \int_0^1 (x^{3/2}+x^{1 /2})x^{3n}\ln x dx$$ Also since $$I(k) =\int_0^1 x^kdx=\frac{1} {k+1}$$ Giving: $$I'(k) =\int_0^1 x^k\ln x dx=-\frac{1} {(k+1)^2 }$$ so using this we get $$I=\sum_{n=0}^{\infty} (-1)^{n+1}\left(\frac{1} {(6n+3)^2 }+\frac{1} {(6n+1) ^2 }\right)$$ Now when I plug this into wolfram-alpha the result differs from the value of the integral, also if I multiply by a  half it is really close to it. Where did I go wrong? Edit: Looks like I forgot a 2 in the denominator and to add $+1$ from $I'(k) $ and the correct series should be: $$I=\frac{4}{36}\sum_{n=0}^{\infty} (-1)^{n+1}\left(\frac{1} {(n+5/6)^2 }+\frac{1} {(n+1/2) ^2 }\right)$$ The second one is just $-4G$ where $G$ is the Catalan constant and can you show me how to transform the sum into a closed form? Trigamma or hurwitz zeta function as wolfram alpha gives as a solution. Many thanks in advance!",['integration']
2802614,How to find the derivative of this function?,"$$f(x) = \int_0^{\cos x}t^2 \, dt$$ I computed and got $f'(x) = - \cos^2(x) \sin(x)$. I checked the answer in the book and I got this wrong. But I think I did this correctly. I used part 1 of FTC to do this. Any ideas?","['derivatives', 'calculus']"
2802665,Possible orders of an element,"Suppose that $H$ is a subgroup of a group $G$ and the order of $H$ is $10$. Let also $a \in G$ and $a^6 \in H$. What are the possible orders of $a$? Since $a^6 \in H$, then $a^{6n} \in H$, for any integer $n$. Since $|H|= 10$, then $a^{60} = e$. Therefore $|a|$ will divide $60$. Can anyone please tell me how to proceed further?","['abstract-algebra', 'group-theory']"
2802676,Why do we have to deal with constructible sets?,"I'm a beginner in algebraic geometry. Recently, I learned about constructible sets and Chevalley's theorem. In a Noetherian space, constructible sets are some kind of finite union of open and closed sets, and Chevalley's theorem states that if $f : X \to Y$ is a morphism of finite type between Noetherian schemes, then the image of any constructible set $C \subset X$ is also constructible. Is there any good application of the theorem? I can't understand why we have to consider constructible sets. It resembles Borel sets in measure theory (but the situation is completely different since topology is far from Hausdorff), so maybe we can think of constructible sets as some sort of topologically simple sets. But such intuition doesn't give any result.",['algebraic-geometry']
2802700,Definition of the multiplicity of an analytic coherent sheaf,"In algebraic geometry, for a coherent sheaf $\mathcal{F}$ on a variety $X$ and an irreducible component $Z$ of $\mathrm{supp}(\mathcal{F})$, the multiplicity of $\mathcal{F}$ in $Z$ is 
$$
\mathrm{Length}_{\mathcal{O}_{X,\xi}}\mathcal{F}_\xi,
$$
where $\mathcal{O}_X$ is the structure sheaf of $X$ and $\xi$ is the generic point of $Z$. My question is: what is the analogous notion in the context of complex analytic geometry? More precisely, for a coherent analytic sheaf $\mathcal{F}$ on a complex analytic space $X$ and an analytic irreducible component $Z$ of $\mathrm{supp}(\mathcal{F})$, what is the multiplicity of $\mathcal{F}$ in $Z$?","['complex-geometry', 'algebraic-geometry']"
2802770,Does $\int_0^\infty \frac{\sin(\sin(x))}{x}dx$ converge or diverge?,"I'm trying to stud the convergence of $$\int_0^\infty  \frac{\sin(\sin(x))}{x}dx.$$ I really have no idea on how solve this integral. Unfortunately, I can't majorate absolutely $\frac{\sin(\sin(x))}{x}$ by a $L^1(0,\infty )$ function, so I'm thinking that it's doesn't converge, but I'm not able to prove it. Moreover, graphically it looks to converge slower than $x\longmapsto \frac{1}{x}$, so I imagine that it doesn't converge, but since $\frac{\sin(\sin(x))}{x}$ often change of sign, I could be surprised. For the context, it's a question of an exam of 1st year bachelor in mathematic, so we can't use theorem as dominated or monotone convergence theorem. And the argument should also be more or less elementary (I hope).","['improper-integrals', 'real-analysis', 'integration']"
2802799,For which Banach space tensor products is the multiplication in a C*-algebra a linear map?,"It is well-known that there is a variety of tensor products on the category of Banach spaces, ranging from the injective tensor product to the projective tensor product. What I would like to know is: to which tensor product(s) $\otimes_\alpha$ does the multiplication map $A\times A \to A$ for a $C^*$-algebra $A$ — a bilinear map of Banach spaces satisfying $||xy|| \leq ||x||\,||y||$ — descend to give a linear map $A\otimes_\alpha A \to A$?","['functional-analysis', 'c-star-algebras', 'banach-spaces']"
2802826,How is this a linear matrix inequality?,"In example 3.4 of Stephen Boyd & Lieven Vandenberghe's Convex Optimization , it is mentioned that the last condition of $$\text{epi} = \left\{ (x,Y,t) \mid Y \succ 0, x^T Y^{-1} x \leq t \right\}$$ is a linear matrix inequality (LMI) in $(x,Y,t)$ . However the linear matrix inequality is written as (in Eq. 2.11 of same book) $$A(x) = x_1 A_1 + x_2 A_2 + \cdots + x_n A_n \preceq B$$ where $A_i$ and $B$ are symmetric matrices. How to show that $x^TY^{-1}x\leq t$ is a linear inequality in $(x,Y,t)$ ? Any help in this regard will be much appreciated.","['matrices', 'symmetric-matrices', 'linear-matrix-inequality']"
2802857,"If $I = \int_{-\infty}^\infty(xu - 3tu^2)\mathrm{d}x$, show that $\frac{\mathrm{d}I}{\mathrm{d}t} = 0$.","Exercise : Show that the integral $\int (xu-3tu^2) \mathrm{d}x$ remains unchanged (is a constant of motion) for the equation KdV. Attempt : Let $u$ be a solution of the KdV equation, thus satisfying : $$u_t + u_{xxx} + 6uu_x = 0$$ Let also $I = \int_{-\infty}^\infty (xu-3tu^2) \mathrm{d}x$ and then the derivative with respect to time, will be : $$\frac{\mathrm{d}I}{\mathrm{d}t}=\int_{-\infty}^\infty\frac{\partial}{\partial t}(xu-3tu^2)\mathrm{d}x=\int_{-\infty}^\infty(xu_t-3u^2-6tuu_t)\mathrm{d}x$$
$$=$$
$$\int_{-\infty}^\infty (-xu_{xxx} - 6xuu_x - 3u^2 + 6tuu_{xxx}+36tu^2u_x)\mathrm{d}x$$ How would one proceed now to show that $\frac{\mathrm{d}I}{\mathrm{d}x} = 0$, thus the expressions above are equal to $0$ ?","['multivariable-calculus', 'improper-integrals', 'integration', 'partial-differential-equations']"
2802885,Chess tournament with 3 winners,"$20$ chess players participate in a tournament where every player plays with each of the others only once. The first $3$ players will get a prize. There are $2$ points awarded to the winner, no points awarded to the losing player and if the game is drawn, each player receives $1$ point. What is the least number of points to guarantee that some player will be among the first three? My approach is a bit simplistic: All possible pairs are $20C2 = 190$. All possible points are $2 \times 190 = 380$ because for each pair, the total number of points is $2$ (either $2+0$ or $1+1$). So the average number per player is $380/20 = 19$. Therefore if someone gets $20$ points, he will be in the top $3$, right?",['combinatorics']
2802893,expansion of exterior derivative of interior product,"Let $\omega$ be an 1-form and $X$ be a vectorfield. As usual $i_X \omega$ denotes the interior product and $\mathrm d$. the exterior derivative. Is there an expansion of the term
$$
\mathrm d (i_X \omega) \quad ?
$$ I would suspect that this can be expressed in terms of Lie-Derivatives, wedge-products and $i$ and $\mathrm d$ but I could not find anything in the literature.","['lie-derivative', 'exterior-algebra', 'reference-request', 'differential-forms', 'differential-geometry']"
2802918,Showing Holder condition implies uniform convergence of the Fourier seires,"Suppose $f:\mathbb T\rightarrow \mathbb C$ satisfies $$|f(s)-f(t)|<M|s-t|^a$$ for every $s,t\in \mathbb T$ with $M>0$ and $ a\in (0,1]$ fixed. I know that by the Riemann-Lebesgue Lemma, $$f(x)=\lim_{n \to \infty} s_n(f;x)$$ where $$s_n(f;x)=\int_{\mathbb T}f(x-t)\frac{\sin(n+\frac{1}{2})t}{\sin\frac{1}{2}t}dt$$ Furthermore, it is known that the convergence is uniform. And I am having trouble dealing with this uniform convergence part. I guess I should apply the Riemann-Lebesgue Lemma to some function $h(t)$ that uniformly bounds $$\frac{f(x-t)-f(x)}{\sin t/2}$$ in some appropriate way. But I cannot see it clearly. Can you help me with this? Thanks and regards.","['real-analysis', 'fourier-analysis', 'trigonometry', 'measure-theory', 'convergence-divergence']"
2802928,Minimal polynomial of $\sqrt{\sqrt[3]{7}-5}$,"I'm trying to find the minimal polynomial of $\alpha = \sqrt{\sqrt[3]{7}-5}$. Rather, I know that it will be $p(x) = x^6 +15x^4+75x^2+118$ by just squaring and then cubing appropriately, but I would like to verify that this is the minimal polynomial. The usual method I use (if for example, we have a square root inside the square root) is to verify that $[\mathbb{Q}(\alpha):\mathbb{Q}]=6$ by noting that 
$$ \mathbb{Q} \subset \mathbb{Q}(\sqrt[3]{7}) \subset \mathbb{Q}(\alpha)  $$
and then showing that $\sqrt[3]{7} - 5$ is not a square inside $\mathbb{Q}(\sqrt[3]{7})$. But I can't get the usual method of supposing it is a square and then arriving at a contradiction to work. I.e. writing $\sqrt[3]{7} - 5 = (a + b \sqrt[3]{7})^2$ and conjugating with $-\sqrt[3]{7} - 5 = (a - b \sqrt[3]{7})^2$ and then multiplying the two doesn't really help us (because we have a cube root instead of a square root). So my question is whether there is a simple way to do this kind of question when we have a cube root inside the main root? Also, as a side question, I'd like to ask if we really have that, say $p(x)$, is a minimal polynomial for $\theta$ over $K$ a field is equivalent to $p(\theta)=0$ and $p(x)$ is irreducible? Or is this not enough for minimality? Do we also have to show that the degree is minimal? So if we could show that $p(x)$ in the question above is irreducible, would that be enough to show it's the minimal polynomial or would we still have to do some stuff for the degree? Many thanks for any help.","['minimal-polynomials', 'abstract-algebra', 'extension-field', 'field-theory']"
2802933,"Prove or disprove that, for any $n \in \mathbb{N_+}$, there exist $a,b \in \mathbb{N_+} $ such that $\frac{a^2+b}{a+b^2}=n.$","Problem Prove or disprove that, for any $n \in \mathbb{N_+}$,
there exist $a,b \in \mathbb{N_+} $ such that $$\frac{a^2+b}{a+b^2}=n.$$ My Thought Assume that the statement is ture. Then, the equality is equivalent to that $$a^2-na+b-nb^2=0.$$ Regard it as a quadratic equation with respect of $a$.Then $$a=\dfrac{n \pm \sqrt{n^2+4nb^2-4b}}{2}.$$ Thus, $n^2+4nb^2-4b$ must be a square number. Let
$$n^2+4nb^2-4b=k^2,k \in \mathbb{N_+}.$$
How to go on with this? May it work? P.S. The statement seems to be true. Here are parts of verification examples:
\begin{array}{r|r|r}
               n&a&b \\ \hline
           1&1&1\\
           2&5&3\\
           3&5&2\\
           4&10&4\\
           5&27&11\\
           6&69&27\\
           \vdots&\vdots&\vdots
\end{array} Besides, the equation could be rewritten as $$n(2a-n)^2-(2nb-1)^2=n^3-1,$$ which is a $\textbf{ Pell-like equation}$. This will help?",['number-theory']
2802948,Explicit example of Hahn-Banach theorem on the finite dimensional space $\mathbb{R}^2$?,"The Hahn-Banach theorem allows us to extend linear functionals defined on a subspace of some vector space $V$ to the entire space. Is it possible to construct an explicit example of this in the finite dimensional case? For example, suppose $V = \mathbb{R}^2$ and $U=\mathbb{R} \subset V$. What would be a simple explicit example of the Hahn-Banach theorem, i.e. what are explicit expressions for $p:V \to \mathbb{R}$ is a sublinear function $\varphi: U \to \mathbb{R}$ is a linear functional on the linear subspace $U \subset V$ which is dominated by $p$ on $U$. The linear extension $\psi:V \to \mathbb{R}$ of $\varphi$ to the whole space such that
\begin{align}
\psi(x) = \varphi(x) \quad \forall x \in U, \\
\psi(x) = p(x) \quad \forall x \in V.
\end{align}","['functional-analysis', 'vector-spaces']"
2802959,"Difference between $x\in[0,1]$ and $x\in \{0,1\}$?","If I write 
$$
x\in [0,1] \tag 1
$$
does it mean $x$ could be ANY number between $0$ and $1$? Is it correct to call $[0,1]$ a set? Or should I instead write $\{[0,1]\}$? Q2: If I instead have
$$
x\in \{0,1\} \tag 2
$$
does it mean $x$ could be only $0$ OR $1$?","['elementary-set-theory', 'real-analysis', 'notation', 'calculus']"
2802967,ODE system - finding the general solution given 2 solutions,"Given the following ode system $\begin{cases}tx_1'=2x_2+2x_3+t^3e^t\\tx_2'=-x_1+3x_2+x_3+t^4\\tx_3'=-x_1+x_2+3x_3+t^3e^t\end{cases}$ and $2$ solutions of the system $$u_{(1)}=\begin{pmatrix}(a+b)t^2\\at^2\\bt^2\end{pmatrix},\quad u_{(2)}=\begin{pmatrix}2\ln t+1\\ \ln t+1\\\ln t+1\end{pmatrix}$$ What is the system's general solution ? Attempt I think it is quite clear that some sort of parameters variation is required due to given solutions. The only problem is that I am not sure how to approach this solution. I tried to develop some sort of a matrix  $$\psi(t)=\begin{pmatrix}(a+b)t^2&2\ln t+1\\at^2&\ln t+1\\bt^2&\ln t+1\end{pmatrix}$$ such that for some $v=\begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}$ fulfills $\psi(t)v'(t)=\begin{pmatrix}t^2e^t\\t^3\\t^2e^t\end{pmatrix}$; the same method we solve linear ode systems, but clearly this matrix multipication is undefined. How should I approach this? Thank you very much!",['ordinary-differential-equations']
2802986,"Finding all differentiable $f: [0,+\infty) \rightarrow [0,+\infty)$ such that $f(x) = f'(x^2)$ and $f(0)=0$","After some investigation it seems fairly obvious to me that the only such function is the zero function, however I haven't been able to prove it. By considering $$\alpha  =\sup\{x\in[0,+\infty) :f(x) = 0\},$$ I was able to show that $\alpha$ can only be $1$ or $0$ but I could not weed out those two possibilities. Any hints/solutions welcome. EDIT 1 Because of the continuity of $f$, we must have $f(\alpha) = 0$. Note that because of the relation given we have $$\int_0^{\sqrt \alpha}2xf'(x^2)\,\mathrm dx = f(\alpha),$$ but because of the relationship given this implies
$$\int_0^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha).$$
If $\alpha$ is strictly between $0$ and $1$, then $\sqrt \alpha > \alpha$, but then splitting the integral we get 
$$\int_{\alpha}^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha) = 0.$$ But by our choice of $α$, this integral should be non-zero since our function is positive. Hence $\alpha$ cannot be between $0$ and $1$. Now suppose it is greater than $1$, then we have $$f(\alpha^2) =\int_0^{\alpha}2xf(x)\,\mathrm dx = 0.$$
Since our function is $0$ on $[0,\alpha]$ (Note that it is increasing), this is again a contradiction because $\alpha^2 > \alpha$. Therefore $\alpha$ is $0$ or $1$. EDIT 2 I forgot to mention the important condition that $f(0)=0$.","['real-analysis', 'functional-equations']"
2802995,"In a random permutation, does uniform contiguous pair probability guarantee uniform permutation probability?","Let's say I have a set of $N$ random $k$-permutations $P_1,\ldots,P_N$ selected according to some probability distribution. I consider the probability distribution of the frequencies of contiguous pairs in a permuted sequence. I'm not sure how to express this properly so I'll give an example: with $k=4$ and the permutation $P_i=(24)$ we would reorder the sequence $[1,2,3,4]$ to $[1,4,3,2]$; what I call ""contiguous pairs"" for $P_i$ would be $(1, 4)$, $(4, 3)$, $(3, 2)$ and $(2, 1)$. So I can count the frequencies of the $k(k - 1)$ possible pairs for my $N$ permutations. Now, my intuition is that, if my permutations are sampled according to a uniform distribution, then each of these frequencies should be about the same as $N \rightarrow \infty$, and they would follow a binomial distribution with $N$ trials and probability $1 / (k - 1)$, because for each permutation any element $a$ may be followed by any of the other $k - 1$ elements with equal probability. I'm not completely sure this is really correct, though, because there are relationships between the frequency values (i.e. $\sum_{i \in \{1,\ldots,k\}\setminus{\{a\}}}\text{frequency}((a, i)) = N\, \forall\, a \in \{1,\ldots,k\}$). My question is, assuming the previous is actually correct, or otherwise that we know that uniformly sampled permutations have a probability distribution of frequencies of contiguous pairs $D$, can we invert the implication? That is, would observing $D$ in another set of random permutations mean they follow a uniform probability distribution? EDIT: I noticed that the above condition is definitely not enough for a uniform permutation distribution. You could imagine a random permutation such that the first element is always the same but the remaining ones are uniformly shuffled, then it would fulfill the condition but it wouldn't be what I am looking for. So I guess the conditions that I'd need to check are, following a uniform distribution of contiguous pairs and uniform probability of each element falling into each position. Background: I answered this question in Stack Overflow: How to verify that a shuffling algorithm is uniform? (I invite anyone willing to it to comment on my answer or post their own if they think it's wrong). I am a software engineer, so my background in statistics is not the strongest, but (after a misguided attempt) I proposed a small code snippet to measure the frequency with which each possible permutation was generated. While this is (I think) a correct approach, in the sense that it is measuring ""the right thing"", it takes a large number of trials to have significant results, so I was wondering if it may be possible to analyse a smaller statistic, such as the distribution of frequencies of contiguous pairs.","['permutations', 'statistics']"
2803031,"If $f(x) = e^{-|x|}$, show that $f''(x) - f(x) = -2\delta(x)$ (in the sense of distributions)","Exercise : If $f(x) = e^{-|x|}$, show that for its derivatives, it is : $f''(x)-f(x) = -2\delta(x)$, in the sense of distributions. Attempt : I am completely at a loss on how to handle such an exercise as we haven't studied distributions for more than 1-2 lessons, but I know that : The function $f(x)$ is continuous and differentiable in $\mathbb R \setminus \{0\} = \mathbb R^*$, with derivative : $$f'(x) = \begin{cases} -e^{-x}, & x>0 \\ e^x, & x<0\end{cases}$$ How would I proceed with modeling the derivatives now in the sense of distributions to prove the equation asked? To be mentioned, the function $\delta(x)$ is the famous Dirac-delta function, such that: $$\delta(x) = \begin{cases} +\infty & x=0, \\ 0 & x \neq 0 \end{cases}$$","['dirac-delta', 'distribution-theory', 'ordinary-differential-equations', 'partial-differential-equations']"
2803068,Finite simple group $G$ containing no elements of order $5$,"Assume that $G$ is a finite simple group and that $n_7(G)=8$ (where $n_7(G)$ is the number of Sylow $7$-subgroups). Prove that $G$ does not contain elements of order 5. I state that since $G$ is finite, then: $|G|=7\cdot p_1^{n_1}\cdot\dots\cdot p_k^{n_k}$, where $p_1,\dots ,p_k$ are primes. For the sylow theorems I know that $7|n_7(G)-1$ and $n_7(G)|p_1^{n_1}\cdot\dots\cdot p_k^{n_k}$. Now suppose by contraddiction that $p_i=5$ for a certain $i\in\{1,\dots, k\}$. So since $G$ is simple, then $n_5(G)\neq 1$. 
At this point I don't know how to conclude the proof... Any ideas or hints?","['finite-groups', 'sylow-theory', 'group-theory']"
2803090,Partial derivative of tensor with respect to tensor,"My question is related to continuum mechanics, taking partial derivative of tensor with respect to tensor. $$\mathbf{\sigma} = \lambda \hspace{1pt} tr(\mathbf{\epsilon})+ 2\mu\mathbf{\epsilon}$$ Where, $\mathbf{\sigma,\epsilon}$ are second order tensors, $tr(\mathbf{\epsilon})$ is trace of the tensor. I want to find $$\frac{\partial \mathbf{\sigma}}{\partial\mathbf{\epsilon}}$$ I start like this: $$ \mathbb{C_{ijmn}}=\frac{\partial {\sigma_{ij}}}{\partial{\epsilon_{mn}}} = \frac{\partial}{\partial \epsilon_{mn}} \big(\lambda\delta_{ij}\epsilon_{kk} + 2\mu\epsilon_{ij} \big)$$ $$=\lambda\delta_{ij}\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{kk} + 2\mu\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{ij}$$ $$=\lambda\delta_{ij}\delta_{km}\delta_{kn} + 2\mu\delta_{im}\delta_{jn}$$ $$=\lambda\delta_{ij}\delta_{mn}+2\mu\delta_{im}\delta_{jn}$$ Is this correct?","['derivatives', 'tensor-products', 'tensors']"
2803095,What is the frobenius map really?,"I know what the frobenius map is the naive sense, just the $p$th power map on an $\mathbb{F}_p$ algebra, and that this can be upgraded somewhat to the generator of the monoid of natural transformations on the identity functor on $\mathbb{F}_p$ algebras. To my knowledge, this is the reason it pops up anywhere one has anything of/over characteristic $p$. For instance, the frobenius morphism I have seen in algebraic geometry seem to just be unwinding the definitions down until we get elements of an $\mathbb{F}_p$ algebra, then applying this down to earth frobenius map. However, this doesn't seem fully satisfactory, in that before doing any of this, we fix a prime $p$, and use the frobenius map in this fixed characteristic. So my question is this: In what sense are the frobenius maps for different $p$ the same thing, can they be viewed as manifestations of the same underlying morphism of something?","['number-theory', 'category-theory', 'finite-fields']"
2803096,Basic functional analysis question: equivalence of norms.,"Suppose we have two norms on a vector space such that a linear functional is continuous with respect to one if and only if it is continuous with respect to the other. Show that the two norms are equivalent.
Two norms $||.||_1 $ and $||.||_2$ are equivalent if there are some constants $C_1,C_2$ such that $C_1||.||_1≤||.||_2≤C_2||.||_1$ Hints, please.",['functional-analysis']
2803134,How to evaluate this Riemann integral using this definition?,"First of all, here are Definitions 6.1, 6.2, and 6.3 in Baby Rudin, 3rd edition: Definition 6.1: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where 
  $$ a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.$$
  We write 
  $$ \Delta x_i = x_i - x_{i-1} \qquad (i = 1, \ldots, n). $$ 
  Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put
  $$
\begin{align}
 M_i &= \sup f(x) \qquad (x_{i-1} \leq x \leq x_i), \\
m_i &= \inf f(x) \qquad (x_{i-1} \leq x \leq x_i), \\
U(P, f) &= \sum_{i=1}^n M_i \Delta x_i, \\
L(P, f) &= \sum_{i=1}^n m_i \Delta x_i,
\end{align}
 $$
  and finally 
  $$ 
\begin{align}
\tag{1} \overline{\int}_a^b f dx &= \inf U(P, f), \\
\tag{2} \underline{\int}_a^b f dx &= \sup L(P, f),
\end{align}
$$
  where the $\inf$ and the $\sup$ are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively. If the upper and lower integrals are equal, we say that $f$ is Riemann-integrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemann-integrable functions), and we denote the common value of (1) and (2) by 
  $$ \tag{3} \int_a^b f dx, $$
  or by 
  $$ \tag{4} \int_a^b f(x) dx. $$
  This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that 
  $$ m \leq f(x) \leq M \qquad (a \leq x \leq b). $$
  Hence, for every $P$, 
  $$ m(b-a) \leq L(P, f) \leq U(P, f) \leq M (b-a), $$
  so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. . . . Definition 6.2: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write 
  $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$
  It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put 
  $$ 
\begin{align}
U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\
L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i, 
\end{align}
$$
  where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define 
  $$
\begin{align}
\tag{5} \overline{\int}_a^b f d \alpha = \inf U(P, f, \alpha), \\
\tag{6} \underline{\int}_a^b f d \alpha = \sup L(P, f, \alpha), 
\end{align}
$$
  the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by 
  $$ \tag{7} \int_a^b f d \alpha $$
  or sometimes by 
  $$ \tag{8} \int_a^b f(x) d \alpha(x). $$
  This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. By taking $\alpha(x) = x$, the Riemann integral is seen to be a special case of the Riemann-Stieltjes integral. . . . Definition 6.3: We say that the partition $P^*$ is a refinement of [a partition] $P$ if $P^* \supset P$ (that is, if every point of $P$ is also a point of $P^*$). Given two partitions $P_1$ and $P_2$, we say that $P^*$ is their common refinement if $P^* = P_1 \cup P_2$. Now using this machinery how can we evaluate the integral 
$$ \int_0^1 x^2 \ \mathrm{d} x? $$ Next, here are Theorems 6.4, 6.8, and 6.9: Theorem 6.4: If $P^*$ is a refinement of $P$, then 
  $$ \tag{9} L(P, f, \alpha) \leq L \left( P^*, f, \alpha \right) $$
  and 
  $$ \tag{10} U \left( P^*, f, \alpha \right) \leq U( P, f, \alpha). $$ And, so we have 
$$ L(P, f, \alpha) \leq L \left( P^*, f, \alpha \right) \leq U \left( P^*, f, \alpha \right) \leq U( P, f, \alpha). $$ Theorem 6.6: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that 
  $$ U(P, f, \alpha ) - L( P, f, \alpha ) < \varepsilon. $$ Theorem 6.8: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Theorem 6.9: If $f$ is monotonic on $[a, b]$ and $\alpha$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$. (We still assume, of course, that $\alpha$ is monotonic.) The function $f(x) = x^2$ is of course continuous as well as monotonic on the interval $[0, 1]$. Thus by either Theorem 6.8 or Theorem 6.9, our integral exists of course. My Attempt: Let $$P = \left\{ x_0, x_1, \ldots, x_{n-1}, x_n \right\}, $$
  where 
  $$ 0 = x_0 < x_1 < \ldots < x_{n-1} < x_n, $$
  be a partition of $[0, 1]$. Then as our function $f$ is sttictly increasing on $[0, 1]$, so we find that, for each $i = 1, \ldots, n$, we have 
  $$ m_i = f \left( x_{i-1} \right) = x_{i-1}^2 \qquad \mbox{ and } \qquad M_i = f \left( x_i \right) = x_i^2. $$
  [Refer to Definition 6.1 above for notation.]  Therefore
  $$ L(P, f) = \sum_{i=1}^n x_{i-1}^2 \left( x_i - x_{i-1} \right) \qquad \mbox{ and } \qquad U(P, f) = \sum_{i=1}^n x_{i}^2 \left( x_i - x_{i-1} \right).  $$ Now from these two formulas, can we compute the quantities in (1) and (2) in Definition 6.1 above? I have no idea of how we can. However, we can do the following trick: Let us put 
  $$ h \colon= \min \left\{ \ \Delta x_1, \ldots, \Delta x_n \ \right\}. $$
  Then of course this $h$ satisfies 
  $$ 0 < h \leq 1, $$
  from which we obtain 
  $$ \frac{1}{h} \geq 1. $$
  Now let us put
  $$ k = \left\lfloor \frac{1}{h} \right\rfloor + 1. $$
  This $k$ is of course a natural number, and  we also have the inequality
  $$ k-1 \leq \frac{1}{h} < k.$$
  Now let $P^\prime$ be the partition of $[0, 1]$ given by
  $$ P^\prime \colon= \left\{ \ 0, \frac{1}{k}, \ldots, \frac{k-1}{k}, 1 \ \right\}, $$
  and let 
  $$ P^* \colon= P \cup P^\prime. $$
  Then by Theorem 6.4  in Baby Rudin, we have the following two sets of inequalities:
  $$ L(P, f, \alpha) \leq L \left( P^*, f, \alpha \right) \leq U \left( P^*, f, \alpha \right) \leq U( P, f, \alpha). $$
  And, 
  $$ L \left( P^\prime, f, \alpha \right) \leq L \left( P^*, f, \alpha \right) \leq U \left( P^*, f, \alpha \right) \leq U\left( P^\prime, f, \alpha \right). $$ Now for the partition $P^\prime$, we compute 
  $$ L \left( P^\prime, f, \alpha \right) =  \frac{1}{k} \sum_{i=0 }^{n-1} \left( \frac{i}{k} \right)^2 = \frac{1}{k^3} \sum_{i=1}^{n-1} i^2 = \frac{ (k-1) (2k-1 ) }{6k^2} = \frac{1}{6} \left( 1 - \frac{1}{k} \right) \left( 2 - \frac{1}{k} \right), $$
  and 
  $$ U \left( P^\prime, f, \alpha \right) =  \frac{1}{k} \sum_{i=1 }^n \left( \frac{i}{k} \right)^2 = \frac{1}{k^3} \sum_{i=1}^{n} i^2 = \frac{ (k+1) (2k + 1 ) }{6k^2} = \frac{1}{6} \left( 1 + \frac{1}{k} \right) \left( 2 + \frac{1}{k} \right). $$
  And, the supremum of all the lower sums $L \left( P^\prime, f, \alpha \right)$ and the infimum of all the upper sums $U \left( P^\prime, f, \alpha \right)$ obtained in this manner each equals $1/3$. How to prove from here (or using some other device) that 
$$ \int_0^1 x^2 \ \mathrm{d} x = \frac{1}{3}?$$","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'analysis']"
2803164,Degrees of Freedom in a Wishart distribution,"What does the degrees of freedom parameter $n$ mean intuitively in a Wishart distribution $\mathcal{W}_p(\mathbf{V},n)$? Does it have any relation to the covariance of different dimensions of the resulting covariance matrix? Why is $n==p$ called a non-informative prior?","['probability-theory', 'probability', 'probability-distributions']"
2803199,How many $n$ that satisfies this condition,"Let, $$S_n = \sum_{r=1}^{n}\sin^{-1}\left(\frac{1}{\sqrt{r^2+1}}\right)$$ If $n=1$ then, $S_1=\sin^{-1}\left(\frac{1}{\sqrt{2}}\right) = 45^o$ If $n=3$ then, $S_3=\sin^{-1}\left(\frac{1}{\sqrt{2}}\right)+\sin^{-1}\left(\frac{1}{\sqrt{5}}\right)+ \sin^{-1}\left(\frac{1}{\sqrt{10}}\right)+ = 90^o$ From two equations we get, $S_3-S_1=45^o$ We introduce the new  variable  $A_n=S_{\frac{n(n+1)}{2}} - S_{\frac{n(n-1)}{2}}$ when, $A=45^o, n=1 $ and $n=2$ satisfies the condition. If $n=1 \implies  S_{\frac{1(1+1)}{2}} - S_{\frac{1(1-1)}{2}} =S_1-S_0=45^o$ If $n=2 \implies  S_{\frac{2(2+1)}{2}} - S_{\frac{2(2-1)}{2}} =S_3-S_1=45^o$ But $n=3$ does not satisfies the condition. I want to find the sequence of natural numbers satisfies the condition, if $n \gt 2$ satisfies this condition.","['algebra-precalculus', 'trigonometry', 'sequences-and-series', 'geometry']"
2803201,Covariance between an exponential random variable and the maximum of several exponential random variables,"Suppose $X_1, \ldots, X_n$ are i.i.d. exponential RV with parameter $\lambda$. Let $Z = \max\{X_1, \ldots, X_6\}$. My goal is to find $\mathrm{cov}(X_1, Z)$. I already know the c.d.f., p.d.f., and expectations of each $X_i$ and $Z$, I think I even found the joint p.d.f. of $Z$ and $X_1$ as: $$f_{Z,\ X_1}(z,x_1) =  \begin{cases}
5\lambda^2 e^{-\lambda x_1}(1-e^{-\lambda z})^4 & z \geq x_1 \geq 0 \\
0 & \mathrm{otherwise}
\end{cases}.
$$
(This joint distribution is not correct, see below). I found this by using $f_{Z,\ X_1} = f_{X_1} \ f_{Z|X_1}$ . But now I am sort of stuck because the integral 
$$E[X_1 Z] = \int\int_{R^2} x_1 z \ f_{Z,\ X_1} \ \mathrm{d}A$$ needed for $\mathrm{cov}(X_1,Z)$ gets cumbersome (and on my first attempt did not converge). Is there some slick thing to notice or do that I am missing? Note I am not yet confident with generating functions and moments.","['exponential-distribution', 'probability']"
2803222,Is the set of all sequences of complex number a subset of $\mathbb{C}^\infty$?,"I'm working through exercises in Axler's ""Linear Algebra Done Right"" and trying to verify Example 1.35 (e) on page 19, which is stated as follows: (e) The set of all sequence of complex numbers with limit 0 is a subspace of $\mathbb{C}^{\infty}$. My main question is: Wouldn't the set of all sequences of complex numbers for example include the sequence $(1,2,3)$ which is to my understanding not in $\mathbb{C}^{\infty}$ thereby making this not a subset and therefore not a subspace? My understanding is that $\mathbb{C}^\infty$ contains all lists of length $\infty$ of complex numbers just as $\mathbb{R}^3$ contains all lists of length 3 of real numbers. Also I now understand a list, tuple, sequence and vector to all be names for ordered collections of objects (generally numbers) of a certain length, and I'm wondering whether this is correct? Before this example I hadn't really made a connection between vectors and sequences, but it seems there is no difference in definition. There is a set of solutions available at http://linearalgebras.com/1c.html (question 2e in this case) which doesn't address my question.","['sequences-and-series', 'vector-spaces']"
2803272,How to take derivative with respect to logarithmic function,"I'm having a little bit of trouble trying to get my head around the following problem. I have this main function $$S_n(\alpha, \beta) = \dfrac{dV}{V} \dfrac{\alpha}{d\alpha} = \dfrac{4 \alpha  \beta^n}{(1 + \alpha)^2 - (1-\alpha)^2\beta^{2 n}}$$ The author explains that ""For all values of $\alpha$, the logarithm of $S$ is, asymptotically for small $\beta$, linearly related to the logarithm of $\beta$. Thus, the slope can be determined by: $$\dfrac{d \ln S}{d ln \beta}$$ From my understanding, I approached this problem by substituting $ln(x)$ using $$\dfrac{d \ln(x)}{dx} = 1/ x \Rightarrow d \ln (x) = \dfrac{dx}{x} $$ Therefore, the slope would be obtained using: $$ \dfrac{dS}{d \beta} \dfrac{\beta}{S}$$ But I'm not sure if this approach is correct, as so far it hasn't got me close to the results of the author. Any suggestion is highly appreciated.","['derivatives', 'linear-algebra']"
2803346,"Riesz representation theorem - yet another ""counter example""","this is a follow up on a previous question I asked. I was looking for examples of when the Riesz representation theorem doesn't hold because not all conditions are met. Meaning, I was looking for examples of functionals over inner-product spaces that cannot be represented as an inner-product with some specific vector. I came up with an examples of my own, but I'm not entirely sure it is correct, and I'd like to hear your opinion on if it's true. If it is true, I'd appreciate it if you'd help me prove it formally, because I'm having some trouble in this area: I'm looking at the space of all real sequences who's sum absolutely converges, with the inner product $\langle a, b\rangle=\Sigma_{k=1}^{\infty} a_{k}b_{k}$ . On this space, I define the linear functional that maps any sequence to its sum. It's pretty easy to see that the functional could hypothetically by represented as an inner product with the sequence: $(b_{n})=(1, 1, 1,...)$. But because this sequence is not a part of the vector space, the inner product is not defined on it. Do you think this example is correct? If it is, how would you prove that there can't be some other vector that can represent this functional? I'm completely new to this world of content, so any feedback would be appreciated (namely, if I made some mistake, please explain where it is and if there's a way to get around it somehow by changing parts of the example). Thanks in advance!","['riesz-representation-theorem', 'examples-counterexamples', 'hilbert-spaces', 'proof-verification', 'functional-analysis']"
2803433,Continued fraction of a sequence is unique,"Let $X\in\mathcal{P}(\mathbb{N})$ and define a continued fraction 
$$f(X)=\cfrac{1}{x_1+\cfrac{1}{x_2+\cfrac{1}{x_3+\cfrac{1}{x_4+\cdots}}}}$$
for each element $x_i\in X$. I'm wondering if $f:\mathcal{P}(\mathbb{N})\rightarrow [0,1]$ is injective. 
If not, would something like $$g(X)=\cfrac{|X|}{x_1+\cfrac{1}{x_2+\cfrac{1}{x_3+\cfrac{1}{x_4+\cdots}}}}$$ for a finite $X$ or 
$$h(X)=\cfrac{\bar{d}(X)}{x_1+\cfrac{1}{x_2+\cfrac{1}{x_3+\cfrac{1}{x_4+\cdots}}}}$$
for an infinite $X$, where $\bar{d}(A)$ is the upper asymptotic density of $X$, work as an injective function?","['continued-fractions', 'functions']"
