question_id,title,body,tags
1307255,Show that $p_n(a)\neq 0$ if $|a|=n$,"I am working the next problem: Consider the polynomials 
  $$
p_n(z)=\sum_{j=0}^{n}\frac{z^j}{j!}
$$
  For $n \geq 2$, show that if $a \in \mathbb{C}$ is such that $|a|=1$ or $|a|=n$, then $p_n(a)\neq 0$ The case $|a|=1$ has already been answer here on the last comment of this question , however I still have no clue for the case $|a|=n$. Any help? I have tried a lot of things, as you can see on the other question but nothing good.","['roots', 'complex-analysis', 'exponential-function']"
1307269,Wrong intuitive understanding of a limit?,"In my textbook, before introducing the epsilon delta definition, they gave a working definition of what a limit is. The definition sounded something like this ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$,  $f(x)$ gets closer to $L$"" But is that always the case with limits? What if $f(x) = 4,$ then we have $\lim \limits_{x \to 2}f(x) = 4$, but it is never true that when x gets closer to 2, f(x) gets closer to 4. Maybe instead we should say: ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$, $ f(x)$ gets closer to or equals $L$"". Please correct me if I'm wrong. I'm pretty new to this stuff. Btw, i understand that the epsilon delta definition has the constant function limit case covered, but I'm more interested in the working definition.",['limits']
1307282,Derivative of $(\ln x)^e$ [duplicate],"This question already has answers here : ""What if"" math joke: the derivative of $\ln(x)^e$ (5 answers) Closed 9 years ago . In Randall Munroe's What If , he says that ""if you want to be mean to first-year calculus students, you can ask them to take the derivative of $(lnx)^e$"" He says, as I would expect, that the result ""looks like it should be $1$ or something, but it's not."" Why is this? And what's the actual answer?","['logarithms', 'derivatives']"
1307287,Random Uniformly Distributed Points in a Circle,"I know that by just using a random angle and a random radius within the bounds of your circle, you will end up with points near the center of a circle.  Whereas if you do $\sqrt{Random(0,1)}*MaxRadius$ for your radius, you will end up with what appears to be a uniformly random point.  I am happy this works but I would like to understand where the square root comes from.  The Square Root function in this calculation seems magical to me and I would like to know what it means in this context.","['probability', 'random', 'polar-coordinates']"
1307290,"""Least trivial"" function preserving rationality","Is there a ""non-trivial"" function $f(x,y)$ such that $$f(x,y) \in \mathbb{Q} \iff x,y\in \mathbb{Q}?$$ An example of a ""trivial"" function would be $$f(x,y) = \begin{cases} 0 & x,y\in \mathbb{Q}\\ \pi & \text{otherwise} \end{cases}$$ 
or any other $f$ which effectively uses a cases function. The motivation is just my curiosity. Obviously, operations which preserve one direction of the $\iff$ are plentiful and well-studied. I was wondering how onerous the condition of the additional direction is on the choice of $f$. This question on mathoverflow seems related.","['rational-numbers', 'soft-question', 'functions']"
1307305,Linear Transformation Matrix with polynomials,"A linear transformation $T : P_2 \to P_2$ has matrix with respect to $S$ given by: $$[T]\,( S) = \begin{bmatrix}
1/2&-3&1/2\\
-1&4&-1\\
1/2&2&1/2\\
\end{bmatrix}
$$ How do you find $T(a+bx+cx^2)$? Thank you!!","['linear-transformations', 'matrix-equations', 'linear-algebra', 'matrices']"
1307337,Dimension of set of all homogeneous polynomial of degree $d$ in $n$-variables over a field $F$,"Let, $V$ be a set of all homogeneous polynomial of degree $d$ in $n$ -variables over a field $F$ . Then dimension of $V_F$ is (A) $\left(\begin{matrix}n\\d\end{matrix}\right)$ (B) $\left(\begin{matrix}d\\n\end{matrix}\right)$ (C) $\left(\begin{matrix}n+d-1\\d-1\end{matrix}\right)$ (D) $\left(\begin{matrix}n+d\\d\end{matrix}\right)$ . I tried through example for $n=2,3,4$ and $d=2,3,4$ and try to generalize the formula. For $n=3$ and $d=3$ we get the basis of $V$ is $\{x^3,y^3,z^3,xy^2,yx^2,yz^2,zy^2,xz^2,zx^2,xyz\}$ . So $dim(V)=10$ . Similarly for $n=3$ and $d=2$ we get $dim(V)=6$ . Also for $n=2$ and $d=2$ $dim(V)=3$ . But I could not generalize these for arbitrary $n$ and $d$ . Suggest to find the general formula or any other way to determine it directly..","['polynomials', 'vector-spaces', 'linear-algebra']"
1307364,Order and index of a normal subgroup $N$ are relatively prime,"Let $N$ be a normal subgroup of a finite group $G$. Assume that the order of $N$ and the index of $N$ in $G$ are relatively prime.
  Prove that if $g\in G$ satisfies $o(g)\mid o(N)$, then $g\in N$. I tried using some properties from number theory, but didn't get anywhere.
Any idea?","['abstract-algebra', 'group-theory', 'finite-groups', 'normal-subgroups']"
1307376,Objects that send only monomorphisms,"I just re-learned that fields can have only 1-to-1 homomorphisms from them. Is this a common trait in other categories? Can we extend, for instance, many topological spaces to spaces that have only 1-to-1 homomorphisms from them? In general, is there a name for a kind of object that only has injective arrows from such objects?","['general-topology', 'category-theory']"
1307398,find the minimum value of $\sqrt{x^2+4} + \sqrt{y^2+9}$,"Question: Given $x + y = 12$, find the minimum value of $\sqrt{x^2+4} + \sqrt{y^2+9}$? Key: I use $y = 12 - x$ and substitute into the equation, and derivative it. which I got this $$\frac{x}{\sqrt{x^2+4}} + \frac{x-12}{\sqrt{x^2-24x+153}} = f'(x).$$ However, after that. I don't know how to do next in order to find the minimum value. Please help!","['calculus', 'algebra-precalculus']"
1307399,"Roots of a $5$th degree polynomial are $\tan A, \tan B, \ldots, \tan E$","If $\tan A$, $\tan B$, $\tan C$, $\tan D$, and $\tan E$ are the roots of the equation $x^5-3x^4-4x^2+3x-5=0$, find the sum of all principal values of $A+B+C+D+E$. What do I do with that equation? Thanks.",['trigonometry']
1307420,How to find the poles of a green function?,"I am trying to construct a green function for $y''+\alpha^2u=f(x), u(0)=u(1), u'(0)=u'(1)$. For that I am trying to follow the procedure described here:( Construct the Green s function for the equation ) I was not able to know how to find ""$a$"".","['greens-function', 'functional-analysis', 'mathematical-physics', 'operator-theory', 'ordinary-differential-equations']"
1307429,Proof of Strong principle of Induction (T. Tao Analysis I),"I have no idea how to prove it by using only what the book has talked about so far. Can anyone help? The proof shouldn't be using set theory as set theory is only mentioned in the following chapter. The proof should only make use of the addition of natural numbers, order properties of natural numbers, the trichotomy of order for natural numbers and principle of induction. Proposition 2.2.14 (Strong principle of induction). Let $m_0$ be a natural number, and let $P(m)$ be a property pertaining to an arbitrary natural number $m$ . Suppose that for each $m ≥ m_0$ , we have the following implication: if $P(m')$ is true for all natural numbers $m_0 ≤ m' < m$ , then $P(m)$ is also true. (In particular, this means that $P(m_0)$ is true, since in this case the hypothesis is vacuous.) Then we can conclude that $P(m)$ is true for all natural numbers $m ≥ m_0$ . Exercise 2.2.5. Prove Proposition 2.2.14. (Hint: define $Q(n)$ to be the property that $P(m)$ is true for all $m_0 ≤ m < n$ ; note that $Q(n)$ is vacuously true when $n < m_0$ .) Thanks!",['analysis']
1307447,Help finding the critical values of α where the qualitative nature of the phase portrait for the system changes?,"I was asked to solved for the eigenvalues in terms of α for 2X2 matrix and so i did and my answer was marked as correct. Then I was asked to solve for this: The roots are complex when? There is a saddle point for? The equilibrium point is a stable node where? I tried solving the problem by finding values which would give me 1 and zero for the eigenvalues and got the values of -24/11, -25/11 and -26/11 but the were all wrong. How would i solve for these three questions? 
can someone walk me through the steps? The eigenvalues in terms of α are: $r = -1 + \dfrac{\sqrt{100 + 44 \alpha}}{2}$ $r_2 = -1 - \dfrac{\sqrt{100 + 44 \alpha}}{2}$","['matrices', 'eigenvalues-eigenvectors', 'eigenfunctions', 'matrix-equations', 'ordinary-differential-equations']"
1307451,"Prob. 8, Sec. 2.10 in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: The dual space of $c_0$ is $\ell^1$?","Let $c_0$ be the subspace of $\ell^\infty$ consisting of all sequences of (real or complex ) numbers converging to $0$ . How to prove that the dual space of $c_0$ is (isomorphic to) $\ell^1$ ? My effort: Let $f \in c_0^\prime$ . Then $f$ is a bounded linear functional with domain $c_0$ . Then, for any $x \colon= (\xi_j)_{j\in \mathbb{N}} \in c_0$ , we have $$x = \sum_{j=1}^\infty \xi_j e_j,$$ where $e_j = (\delta_{ij})$ for each $j \in \mathbb{N}$ . Each $e_j \in c_0$ .
Then using the boundedness (and the consequent continuity) of $f$ , we can conclude that $$f(x) = \sum_{j=1}^\infty \xi_j f(e_j).$$ So $$\vert f(x) \vert \leq \sum_{j=1}^\infty \vert \xi_j \vert \ \vert f(e_j) \vert \leq \Vert x \Vert_\infty \sum_{j=1}^\infty \vert f(e_j)\vert, $$ showing that $$\Vert f \Vert \leq \sum_{j=1}^\infty \vert f(e_j)\vert. $$ Is what I've stated so far correct? How to proceed?","['duality-theorems', 'real-analysis', 'functional-analysis', 'normed-spaces']"
1307456,Curves With Known Arc Length [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I would appreciate if you could list as many (planar) curves with known closed-form analytical expressions for the arc length as possible. Please include formulas for both  the curve and the arc length.  The implicit curves  are of particular interest to me. I might as well start the list : Circle $S^1$ implicit equation: $\quad\left(\frac{x}{r}\right)^2 + \left(\frac{y}{r}\right)^2 = 1, \quad$ parametrization: $\ \begin{cases} x = r\cos t, \\ y = r\sin t, \end{cases} \  t \in [0, 2\pi)$. arc length $ s(t) =  r\cdot t, \ t \in [0, 2\pi)$, and $s(x,y) = r \cdot \arctan\left(\frac{y}{x}\right), \ 0\le x,y\leq r $. Parabola with focal length $f$, perpendicular distance to the axis of symmetry  $p$. implicit equation: $\left( x - h\right)^2 = 4 p \, (y-k)$. arc length from the vertex of parabola $s = \frac{hq}{f} + f \ln \left( \frac{h+q}{f}\right)$, $h = p/2$, $q = \sqrt{f^2 + h^2}$. $ y =  x^2 - \frac{1}{8}\ln x $. arc length  from the point $(1,1)$: $\ s(x) =  x^2 + \frac{1}{8}\ln x - 1$. PS: Please do not hesitate to post curves in higher dimensions.","['calculus', 'curves', 'plane-curves', 'geometry', 'differential-geometry']"
1307475,Proving uniform continuity of trigonometric function arctan,"In my assignment I have to prove that the following function is uniformly continuous: $$f(x) = \arctan x \cdot \sin\left(\frac{1} {x}\right) $$ in the open interval $(0,\infty) $. I thought I'd prove the one side limits $ x\to 0^+$ and $ x\to \infty^-$. $$\lim_{x\to\infty^-}=\frac{\pi} {2}\cdot0=0.$$ For the second limit I thought using the squeeze theorem. Please let me know if I made a mistake: $$0\le \arctan x \cdot \sin \left(\frac{1} {x} \right)\le  \arctan x .$$ Therefore, from the squeeze theorem: $$\lim_{x\to 0^+} \arctan x \cdot\sin\left(\frac{1} {x}\right) =0$$ Since $\arctan$ and $\sin$ are continuous always when it is defined, we can say that the function is uniformly continuous, since the one side limits exists. I have only one concern here: zero is not defined here, in the open interval from the beginning. Is it a problem to use the squeeze theorem here? If it is, thought I'd use $\frac {x} {x+1} $ instead of $0$. Thanks, Alan","['calculus', 'real-analysis', 'limits', 'trigonometry']"
1307487,Evaluate $\lim_{n\to\infty}nI_n$ with $I_n=\int_0^1\frac{x^n}{x^2+3x+2}dx$,"We have to evaluate: $$\lim_{n\to\infty}nI_n$$ with $$I_n=\int_0^1\frac{x^n}{x^2+3x+2}\:dx.$$ There is an elegant way to solve this problem? Here is all my steps: My first ideea was to find a recurrence relation such that: $$I_{n+2}+3I_{n+1}+2I_n=\frac{1}{n+1},\forall x\in\mathbb{N}$$ Next step I show that $\forall x\in[0,1]\Rightarrow I_{n}\ge I_{n+1}\ge I_{n+2}$ Therefore it involving that: $$6I_n\ge 4I_{n+1}+2I_n\ge\frac{1}{n+1},\forall x\in\mathbb{N}$$ As I said above $$6I_{n+2}\leq 4I_{n+2}+2I_n\leq\frac{1}{n+1}$$ 
$\Rightarrow \frac{n}{6(n+1)}\leq nI_n\leq\frac{n}{6(n-1)},\forall x\in\mathbb{N}$ Therefore by squeeze thereom: $$nI_n\to\frac{1}{6}\:as\:n\to\infty$$","['calculus', 'real-analysis', 'definite-integrals', 'sequences-and-series', 'integration']"
1307497,Maximum area of a fenced playpen on the side of a house.,"Here's an interesting problem: you just got a really cute puppy, and you want it to have a large rectangular playpen to run around in. What's more, your neighbor just happened to have 100 feet of extra fencing, and decided to give it to you. You want one side of the playpen to be your house, and the other three sides must be surrounded by the fence. With only the 100 feet of fencing you got from your neighbor, what are the dimensions of the playpen? This problem can be solved using simple algebra. Pretend that the width of the playpen is perpendicular to the house, and the length is parallel. Give variables to each: $x$ for the width, and $y$ for the length. You have the two equations $2x+y=100$ and $x\cdot y=A$ where $A$ is the area of the pen. Solving for $y$ in the first equation, you get $y=100-2x$. Substituting $y$ into the second equation gives you $(100-2x)\cdot x=A$ To find the maximum value, we can first find the two x-intercepts: $(0,0)$ and $(50,0)$. The average of the x-values gives you the x-value of the vertex, which is $25$. Plugging this into the first equation, you get $y=50$. So there you have it. The width of the playpen is 25, and the length is 50 for a maximum area of $25\cdot 50=1250$. But this wasn't my real question. The situation above was of a rectangular playpen, but I'm wondering whether it's possible to find the maximum area of a playpen of any shape, but still with 100 feet of fencing. The side of the playpen that is formed by the wall must be at least 5 feet wide, in order to allow for movement between the house and the playpen (for both the owner and the puppy). If you have a suggestion or partial answer, please feel free to post it as an answer. If you have a full answer, that's even better, but I'm just looking for pointers.","['calculus-of-variations', 'geometry', 'area', 'algebra-precalculus']"
1307512,Cauchy's Integral Question Complex Number,"I have a question and I'm kind of stuck, I was wondering if you were able to help me move forward. The question is, 
Use Cauchy's integral formula to evaluate, $$
\int_{|z| = 1}\frac{e^{2z}}{z^2}dz
$$ where $|z| = 1$ is oriented counter-clockwise.  Using this result, evaluate the real integral $$
\int_0^{2\pi}e^{2\cos(t)}\cos(2\sin(t) - t)dt
$$ I have gotten to this step,  and I'm stuck. I have uploaded an image . Please do let me know if its unclear, ill rewrite it and upload another picture because Im not familiar with LaTex.
Here's my work: $$
|z| = 1 \text{ as } z(t) = e^{it}, t\in [0, 2\pi]
$$ \begin{align}
\int_{|z|} =&\ \int_0^{2\pi}\frac{e^{2e^{it}}}{\left(e^{it}\right)^2}ie^{it}dt \\
=&\ i\int_0^{2\pi}e^{-it}e^{2\cos(t) + 2i\sin(t)}dt \\
=&\ i\int_0^{2\pi} e^{2\cos(t) + i(2\sin(t) - t)}dt
\end{align}","['complex-integration', 'calculus', 'complex-analysis', 'complex-numbers']"
1307517,Why spherical coordinates is not a covering?,"Maybe this is an idiot question and I'm committing a trivial mistake. Let $\phi (\theta, \varphi) = (\cos \theta \sin \varphi, \sin \theta\sin \varphi, \cos \varphi)$ be the usual covering of the circle $\mathbb{S}^2 (1)$ by spherical coordinates I always thought that polar coordinates could be seen as the picture, for instance, in here http://en.wikipedia.org/wiki/Spherical_coordinate_system (second picture). In this case it would be a covering space map. However when $\theta = 0$, the coordinates satisfies $(\sin \varphi, 0, \cos \varphi)$, which is a circle in the plane $y = 0$, but the angle $\theta$ is $0$ and $(-1, 0, 0)$ belongs to circle! However according to the image in the link $\theta$ should be $\pi$. So $\phi (0 \times (0, 2\pi)) \cap \phi (\pi \times (0, 2\pi)) \neq \emptyset$. Therefore I'm now not sure if $\phi$ is a covering space. So, is $\phi$ a covering space map? And what's wrong with that $\theta$? Rephrasing my question into one: in squares of what size $\phi$ is injective? Thanks in advance. EDIT I've just realized that the map $\phi$ cannot be a covering. Since $\mathbb{S}^2(1)$ is already simply connected, $\mathbb{R}^2$ cannot be a covering space, otherwise it would be the universal covering. Let $U_{a, b} = (a, 2\pi + a)\times(b, \pi + b)$. The function $\phi|_{U_{a, b}}$ is not an injection unless $a, b \in \mathbb{Z}$ (as noted by user86418). As discussed in the comments, I (and I think the other commenters) thought that $\mathbb{R}^2$ could be tessellated by the rectangles $U_{i, j}$ where $i, j \in \mathbb{Z}$. However this is not true, since this tessellation would not be induced by a group $G$ (otherwise $G \cong \pi_1 (\mathbb{S}^2) = 0$) By making a more detailed analysis, it's possible to see that in the rectangle $R_{0,0} = \overline{U_{0, 0}}$, a vertical arrow pointing down is equivalent to an arrow pointing up (with the source) translated by $(\pi, 2\pi)$. More precisely, a point $(\theta, \varphi)$ is identified with a point $(\theta + \pi, 2\pi - \varphi)$. Together with this ""action"", there is the usual identification beetween  $(\theta, \varphi)$ and $(\theta + k2\pi, \varphi + l2\pi)$. In this case, the second action is given by $\mathbb{Z}^2$ (by translating by $2\pi$) and, in the quotient $X \cong \mathbb{R}^2/ \mathbb{Z}^2$, the first ""action"" turns in to a real action given by $\mathbb{Z}_2$. Therefore $\mathbb{S}^2 \cong X /\mathbb{Z}^2$. But, since $X \cong \mathbb{T}^2$ is a torus, this would imply that the sphere is a quotient of a torus by $\mathbb{Z}^2$. Is this correct? So summarizing, the action on the torus by  the additive group $\mathbb{Z}^2$ is given by $1. (\theta, \varphi) = (\theta + \pi, 2\pi - \varphi)$. So the questions are: Is $\mathbb{S}^2 \cong \mathbb{T}^2/ \mathbb{Z}^2$ as descripted above? Furthermore, for what open set $\phi$ fails to be a covering (i.e, when $\phi^{-1} (U)$ fails to be a disjoint union of isomorphic open sets)?","['differential-geometry', 'multivariable-calculus', 'covering-spaces']"
1307527,Relationship between twisting sheaves and divisor sheaves,"I'm not really entirely sure how to think about Serre's twisting sheaves $\mathscr{O}(i)$ - on any $\text{Proj}$ construction, really, but let's stick to something like $\mathbb{P}_2$ for now for simplicity. I know how to construct their sections and transitions functions on the usual open cover, and their global sections as homogenous polynomials, I have a hazy sense of the geometric intuition behind them via the Picard group structure and the very ample $\mathscr{O}(1)$-pullback condition, but I feel like I don't really understand them at all, to be honest. For example, $\mathscr{O}(1)$ is isomorphic as a line bundle hyperplane divisors (which I'm assuming means $\mathscr{O}(H)$ for some hyperplane $H$), according to Wikipedia, but is there any relationship between $\mathscr{O}(i)$ and other divisors? Does a divisor of degree $i$ have line bundle isomorphic to $\mathscr{O}(i)$?","['algebraic-geometry', 'algebraic-curves']"
1307530,how to prove a combinatorial identity,"I've encountered with following identity:
$$\sum\limits_{n=0}^\infty\binom{a+bn}{n} \left(\frac{z}{(1+z)^b}\right)^n=\frac{(1+z)^{1+a}}{1+(1-b)z}$$
Is it correct? how to prove it?","['sequences-and-series', 'generating-functions', 'combinatorics']"
1307537,Notation for integral of a vector function over an ellipsoid,"For a short proof, I need to write a point $\pmb y\in\mathbb{R}^p$ as 
 the integral of the surface of the ellipse $\pmb x^{\top}\pmb Q\pmb x=c$ where $\pmb Q$ is a $p$ by $p$ PSD matrix (for now $\pmb y$ is defined in words). What is the formal way to write this? Can we do better than: $$\pmb y=\int_{\pmb x\in\mathbb{R}^p:\pmb x^{\top}\pmb Q\pmb x=c} \pmb x d(\pmb x)$$ Also, I am not a professional mathematician so I am not too sure about the $d(\pmb x)$ part.","['surface-integrals', 'notation', 'multivariable-calculus']"
1307579,"If $n=\dim(V)$ and $n$ vectors are linearly independent, then they form a basis","If $V$ is a vector space and $v_1, v_2, . . . , v_n \in V$
span $V$, and $u_1, u_2, . . . , u_m ∈ V$ are linearly independent, then $m\le n$. Use this to prove that if $V$ has dimension $n$ and $u_1, u_2, . . . , u_n \in V$ are linearly independent
then $u_1, u_2,\le, u_n$ form a basis of $V$. Do I prove that $V$ has a basis with n elements? Not really sure how to approach this proof.","['vector-spaces', 'linear-algebra', 'span']"
1307630,High school algebra textbooks for gifted students,"Cross-posted to Math Educators Stack Exchange. ( link ) I am looking for high school algebra/mathematics textbooks targeted at talented students, as preparation for fully rigorous calculus à la Spivak. I am interested in the best materials available in English, French, German or Hebrew. Ideally, the book(s) should provide a comprehensive introduction to algebra at this level, starting from the most basic operations on polynomials. It should include necessary theory (e.g., Bezout's remainder theorem on polynomials, proof of the fundamental theorem of arithmetic, Euclid's algorithm, a more honest discussion of real numbers than usual, proofs of the properties of rational exponents, etc., and a general attitude that all statements are to be proved, with few exceptions). It should also have problems that range from exercises acquainting students with the basic algebraic manipulations on polynomials to much more difficult ones. Specifically, I am looking for something similar in spirit to a series of excellent Russian books by Vilenkin for students in so-called ""mathematical schools"" from grades 8 to 11, although I am only looking for the equivalent of the grade 8 and 9 books, which are at precalculus level. To give you an idea, here are a sample of typical problems from the grade-8 book. Perform the indicated operations. $\frac{3p^2mq}{2a^2 b^2} \cdot \frac{3abc}{8x^2 y^2} : \frac{9a^2 b^2 c^3}{28pxy}$ Prove that when $a \ne 0$, the polynomial $x^{2n} + a^{2n}$ is divisible neither by $x + a$ nor by $x - a$. Prove that if $a + b + c = 0$, then $a^3 + b^3 + c^3 + 3(a + b)(a + c) (b + c) = 0$. Prove that if $a > 1$, then $a^4 + 4$ is a composite number. Prove that if $n$ is relatively prime to $6$, then $n^2 - 1$ is divisible by 24. Simplify $\sqrt{36x^2}$. Simplify $\sqrt{12 + \sqrt{63}}$. Prove that the difference of the roots of the equation $5x^2 -2(5a + 3)x + 5a^2 + 6a + 1 = 0$ does not depend on $a$. Solve the inequality $|x - 6| \leq |x^2 - 5x + 2|$. And here are the chapter titles for the grade 8 and 9 books. Grade 8: Fractions. Polynomials. Divisibility; prime and composite numbers. Real numbers. Quadratic equations; systems of nonlinear equations; resolution of inequalities. Grade 9: Elements of set theory. Functions. Powers and roots. Equations and inequalities, and systems thereof. Sequences. Elements of trigonometry. Elements of combinatorics and probability theory. Broadly similar questions have been asked elsewhere, however the suggestions made there are not satisfactory for my purposes. The English translations of Gelfand's books are good; however they are not a sufficiently broad introduction to high school algebra, and do not have enough material on computational technique. They are more in the nature of supplements to an ordinary textbook. Some 19th century books like Hall and Knight have been suggested. On conceptual material, these tend to be too old in language and outlook. Basic Mathematics by Serge Lang seems more to dabble in various topics than to provide a thorough introduction to algebra. I am not inclined towards books with a very strong ""New Math"" orientation (1971-1983 France, for example). I don't think a student should need to understand the group of affine transformations of $\mathbb{R}$ to know what a line is. Also, previous questions have perhaps focused implicitly on material in English. I have in mind a student who can also easily read French, German or Hebrew if something better can be found in those languages. Edit. I'd like to clarify that I'm not asking for something identical to these books, just something as close as possible to their spirit. Fundamentally, this means: 1. It is a substitute for, rather than just a complement to, a regular school algebra textbook. 2. It is directed at the most able students. 3. It conveys the message that proofs and creative problem-solving are central to mathematics.","['education', 'reference-request', 'book-recommendation', 'algebra-precalculus']"
1307671,Determinant of block tridiagonal Toeplitz matrices,"Is there a formula to compute the determinant of block tridiagonal matrices, when the determinants of the involved matrices are known? In particular, I am interested in the case $$A = \begin{pmatrix} J_n & I_n & 0 & \cdots & \cdots & 0 \\ I_n & J_n & I_n & 0 & \cdots & 0 \\ 0 & I_n & J_n  & I_n & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots &  \ddots & 0 \\ 0 & \cdots & \cdots & I_n & J_n & I_n \\ 0 & \cdots & \cdots & \cdots & I_n & J_n \end{pmatrix}$$ where $J_n$ is the $n \times n$ tridiagonal matrix whose entries on the sub-, super- and main diagonals are all equal to $1$ and $I_n$ is identity matrix of size $n$ .","['toeplitz-matrices', 'matrices', 'determinant', 'block-matrices', 'linear-algebra']"
1307683,Find the field by the its multiplicative group,Suppose we have a group G. Is this a multiplicative (or additive) group of some field? I think that аn arbitrary group is not suitable (e.g. in the case of finite fields multiplicative group should be cyclic). What properties must have this group G (it's very interesting in the case of infinite groups)?,"['field-theory', 'group-theory']"
1307695,Fundamental theorem of calculus and complex integration,"I am teaching myself complex integration, and unfortunately my text book has left me confused as to when I can apply  the Fundamental theorem of calculus for complex integration. Consider the following on the unit circle (centered at $0$ with radius $1$) $$\oint cosec^{2}\left ( z \right )$$ i believe this integral to be $0$ because the  antederivative is $cot(z)$ and is defined on the unit circle, and $cosec(z)$ is continuous on the unit circle. The fact that the unit circle contains $0$, on which neither $cosec(z)$ nor $cot(z)$ is defined is irrelevant. The only thing that matters is that the statements above hold true on the path itself. consider now $$\oint \frac{1}{z}$$ also on the unit circle is the sole reason the FTOC fails to apply , that the antiderivative $ln(z)$ is not well defined at $z=1$ , assuming a branch cut $[0,+infinity]$ ? and therefore if we took the same integral on the unit circle minus ${1}$, would the integral exist and be $= 0$ ? Thank you","['complex-analysis', 'complex-integration']"
1307733,"If a mapping is bijective and regular, then the mapping is a diffeomorphism?","Let me ask a question which appears in the book 'Elementary Differential Geometry' written by O'Neil. The questions is: prove that if a one-to-one and onto mapping $f:\Bbb R ^n \to \Bbb R ^n$ is regular, then it is diffeomorphism. In the book, ""$f$ is regular"" means that the tangent map of $f$ is one to one. I think certainly I need to use the inverse function thorem. $f$ is a mapping so $f$ is in the class $\mathcal C ^1$. Since $f$ is regular and in the class $\mathcal C^1$, its Jacobian is invertible, so the derivative of $f$ is invertible. Therefore, I might apply the inverse function theorem to $f$: there exists an open set in $\Bbb R ^n$ in which the inverse of $f$ exists and the inverse belongs to the class $\mathcal C ^1$ so $f$ is a diffeomorphism. But to prove $f$ is a diffeomorphism, shouldn't I show the inverse of $f$ is in the class $\mathcal C ^1$ for every point in $\Bbb R ^n$? I would really appreciate any help. Thank you for reading.",['differential-geometry']
1307738,Application of Green's Theorem when undefined at origin,"Problem: Let $P={-y \over x^2+y^2}$ and $Q={x \over x^2+y^2}$ for $(x,y)\ne(0,0)$. Show that $\oint_{\partial \Omega}(Pdx + Qdy)=2\pi$ if $\Omega$ is any open set containing $(0,0)$ and with a sufficiently regular boundary. Working: Clearly, we cannot immediately apply Green's Theorem, because $P$ and $Q$ are not continuous at $(0,0)$. So, we can create a new region $\Omega_\epsilon$ which is $\Omega$ with a disc of radius $\epsilon$ centered at the origin excised from it. We then note ${\partial Q \over \partial x} - {\partial P \over \partial y} = 0$ and apply Green's Theorem over $\Omega_\epsilon$. Furthermore, $\oint_C(Pdx + Qdy)=2\pi$ if $C$ is any positively oriented circle centered at the origin. I get the general scheme of how to approach this problem, however I am unsure of how to argue it in a rigorous manner.","['multivariable-calculus', 'real-analysis']"
1307739,Linear Maps as Tensors,"Let $V$ and $W$ be finite dimensional vector spaces and let $V^{\ast}$ denote the dual $V$. I read that the space $V^{\ast}\otimes W$ may be thought of in four different ways: as the space of linear maps $V\to W$, as the space of linear maps $W^{\ast}\to V$, as the dual space to $V\otimes W^{\ast}$, and as the space of linear maps $V\times W^{\ast}\to \mathbb{C}$. I am having trouble seeing why this statement is true because I am not comfortable with dual spaces yet. Can someone help me out? Is this all a simple consequence of the universal property of tensor products? Thanks.","['multilinear-algebra', 'abstract-algebra', 'tensor-products']"
1307747,"Prove that if $\langle Tx,x\rangle =0$ for all $x \in X$, then $T = 0$","This is exercise 10 in section 3.2 of Kreyszig's Introductory Functional Analysis with Applications: (Zero Operator): Let $T:X \to X$ be a bounded linear operator on complex inner product space $X$. If $\langle Tx,x\rangle =0$ for all $x \in X$, show that $T=0$. I had solved little kindly guide me further Solution: Let $\langle Tx,x\rangle =0$ for all $x \in X$. Let $x=u+av$ where $v,u$ belong to $X$ and $a$ be the scalar then 
$$
\langle Tx,x\rangle =\langle T(u+av), u+av\rangle
$$
since $T$ is linear then 
\begin{align}
\langle Tx,x\rangle &=\langle Tu+aTv, u+av\rangle \\
    & =\langle Tu,u\rangle +\overline{a}\langle Tu,v\rangle +a\langle Tv,u\rangle +a\overline{a}\langle Tv,v\rangle \\
    & =\overline{a} \langle Tu,v\rangle +a\langle Tv,u\rangle 
\end{align}","['analysis', 'functional-analysis']"
1307785,"statistics , two different results related in their standard deviation","I have a question I don't know how to think about it. We have a sample of $25$ people, $16$ of them smoke and $9$ don't. The average capacity of their lungs in smokers is $103$ and in non-smokers is $95$ and the standard deviation of all people in the sample is $10$. Can we be sure with $95\%$ confidence that smokers have statistically more lung capacity? My problem is that I can't understand basically what does that deviation mean and how can it help? The problem seems easy but I am so confused about it. Thank you.",['statistics']
1307821,Why is limit superior called to be the largest accumulation point?,"In general, when there are multiple objects around which a sequence, function, or set accumulates, the inferior and superior limits extract the smallest and largest of them. This is quoted from wikipedia. I know limsup to be limit of the upper bound of a sequence. But I am unable to associate the definition of limsup to that of accumulation point. So, can anyone tell me how are they related to each other? And also why is limsup to be the largest accumulation point?? Can anyone also tell what the wiki statement wants to tell?","['elementary-set-theory', 'real-analysis']"
1307822,Properties of distribution of zeros of polynomial,"Polynomial $p_n(z) = (1 + \frac{z}{n})^n - 1$ has a property that all its zeros lie on the circle of radius $n$. It is easy to see because $$\frac{z}{n} = e^{\frac{i2\pi k}{n}} - 1$$ So we can ""fit"" zeros of polynomials of degree $n$ on the curve of degree $2$ (circle). Is it possible to do similar things i.e. find the curve of smaller than $n$ order where all zeros are placed, with the polynomial $$q_n(z) = \sum^n_{k = o}\frac{z^k}{k!} - 1$$ As both $p_n(z)$ and $q_n(z)$ $\to$ $e^z - 1$ as $n \to \infty$","['polynomials', 'complex-analysis', 'roots']"
1307854,How do we get the last relation?,"I am looking at the conservation of momentum. The force at $W$ from the tensions at the boundary $\partial{W}$ is $$\overrightarrow{S}_{\partial{W}}=-\int_{\partial{W}}p \cdot \overrightarrow{n}dA=-\int_{W}\nabla p dV$$ where $p(\overrightarrow{x}, t)$ the pressure and $\overrightarrow{n}$ the unit perpendicular vector. The massive forces is $$\overrightarrow{B}_{W}=\int_{W}\rho \overrightarrow{b}dV$$ where $\overrightarrow{b}$ the density of massive forces. So, the total force on the fluids in the volum $W$ is $$\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}( \rho \overrightarrow{b}-\nabla p)dV$$ From the second Newton's law we have that $\overrightarrow{F}=m\cdot \overrightarrow{a}$ and since $m=\int \rho dV$ and $\overrightarrow{a}=\frac{D\overrightarrow{u}}{Dt}$, where $\frac{D}{Dt}$ the material derivative, we have the following: $$\int_{W}\rho \frac{D\overrightarrow{u}}{Dt}dV=\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}(\rho \overrightarrow{b}-\nabla p)dV$$ The differential form of the conservation of momentum is $$\rho \frac{D\overrightarrow{u}}{Dt}=-\nabla p+\rho\overrightarrow{b}$$ We are looking for the integral form of the conservation of momentum. We have $$\rho \frac{\partial{\overrightarrow{u}}}{\partial{t}}=-\rho (\overrightarrow{u}\cdot \nabla )\overrightarrow{u}-\nabla p+\rho \overrightarrow{b}$$ From the differential form of the conservation of mass ($\frac{\partial{\rho}}{\partial{t}}+\nabla \cdot (\rho \overrightarrow{u})=0$) we get the following: $$\frac{\partial}{\partial{t}}(\rho \overrightarrow{u})=-div(\rho \overrightarrow{u})\overrightarrow{u}-\rho(\overrightarrow{u}\cdot \nabla)\overrightarrow{u}-\nabla p+\rho\overrightarrow{b}$$ Could you explain to me how we get the last relation??","['physics', 'classical-mechanics', 'derivatives']"
1307878,Expectation of hitting time of a markov chain,"Let $\{X_n\}$ be a homogenous Markov chain, taking values in N. $T_i:=\inf\{k\ge0:X_k=i\}$ is the first time when the chain arrives at i. I know that if X is irreducible positive recurrent, then $E_iT_i$ is finite. Now I want to know some results about $E_j T_i$, the expected time of first arrival at state $i$ starting from state $j$. I think that it should be finite almost surely. Is there any explicit form of $E_j T_i$, or inequality? And what about an arbitrary initial distribution?   Any help would be appreciated.","['probability', 'markov-chains']"
1307884,Evalute big determinant,"Today in exam I tried to evaluate this determinant but failed, only somehow ""guessed"" the answer I got here. Now in home I've managed to find something intuitive, just want to know whether the approach is correct, and is there more faster way exist. Given determinant
$$\det\begin{vmatrix}
1 & 2 & 3 & ... & n-2 & n-1 & n\\ 
2 & 3 & 4 & ... & n-1 & n & n\\ 
3 & 4 & 5 & ... & n & n & n \\ 
. & . & . & . & . & . &. \\
n & n & n & ... & n & n & n
\end{vmatrix}$$ First thing I did, was rearranging rows. I remember from another problem, where I used to evaluate determinant of matrix of this kind $\det\begin{vmatrix}
0 & 0 ... & 0 & 1\\ 
0 & 0 ... & 1 & 0\\ 
0 & 0 ... & 0 & 0\\ 
. & . & . & . \\
1 & 0 ... & 0 & 0
\end{vmatrix}$ is $(-1)^{\frac{n(n-1)}{2}}*\det\begin{vmatrix}
1 & 0 ... & 0 & 0\\ 
0 & 1 ... & 0 & 0\\ 
0 & 0 ... & 0 & 0\\ 
. & . & . & . \\
0 & 0 ... & 0 & 1
\end{vmatrix}=(-1)^{\frac{n(n-1)}{2}}$. So it becomes
$$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix}
n & n & n & ... & n & n & n \\
. & . & . & . & . & . &. \\
3 & 4 & 5 & ... & n & n & n \\ 
2 & 3 & 4 & ... & n-1 & n & n\\ 
1 & 2 & 3 & ... & n-2 & n-1 & n\\ 
\end{vmatrix}$$
And then I transposed it
$$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix}
n & n-1 & n-2 & ... & 3 & 2 & 1 \\
. & . & . & . & . & . &. \\
n & n & n & ... & n & n-1 & n-2 \\ 
n & n & n & ... & n & n & n-1\\ 
n & n & n & ... & n & n & n\\ 
\end{vmatrix}$$
and tried to subtract first row from all.
$$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix}
n & n-1 & n-2 & ... & 3 & 2 & 1 \\
0 & 1 & 1 & ... & 1 & 1 & 1 \\ 
0 & 1 & 2 & ... & 2 & 2 & 2\\ 
. & . & . & . & . & . &. \\
0 & 1 & 2 & ... & n-3 & n-2 & n-1\\ 
\end{vmatrix}$$
next step is subtracting second row from others below.
$$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix}
n & n-1 & n-2 & ... & 3 & 2 & 1 \\
0 & 1 & 1 & ... & 1 & 1 & 1 \\ 
0 & 0 & 1 & ... & 1 & 1 & 1\\ 
. & . & . & . & . & . &. \\
0 & 0 & 1 & ... & n-4 & n-3 & n-2\\ 
\end{vmatrix}$$
doing this for finite n we'll get
$$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix}
n & n-1 & n-2 & ... & 3 & 2 & 1 \\
0 & 1 & 1 & ... & 1 & 1 & 1 \\ 
0 & 0 & 1 & ... & 1 & 1 & 1\\ 
. & . & . & . & . & . &. \\
0 & 0 & 0 & ... & 0 & 0 & 1\\ 
\end{vmatrix}$$
So my final solution is $n*(-1)^{\frac{n(n-1)}{2}}$. Do you see any mistakes? And maybe there's more easier approach? thanks.","['determinant', 'matrices']"
1307888,Converge of a sequence in $L^p(\mathbb{R}^3)$,"Let $f(x)\in L^p(\mathbb{R}^3)$ for every $p\in [1, \infty]$. Let $B(n)\subset \mathbb{R}^3$ be the ball of radius $n$ centered at the origin. I want to show that the sequence
$$I_n(x)=\int_{\mathbb{R}^3}e^{-\xi^2/n}\int_{B(n)}f(y)e^{i(x-y)\xi}dyd\xi$$
converges in $L^p(\mathbb{R}^3)$ for every $p\in [1, \infty)$. I know that $L^p$ is complete, so I've tried to show that $I_n(x)$ is a Cauchy sequence. I've written 
$$I_n(x)=\int\int \chi_{B(n)}(y)f(y)e^{i(x-y)\xi}e^{-\xi^2/n}dyd\xi$$
and tried to use some inequalities to estimate $I_n-I_m$, but nothing obtained. Any suggestions?","['lp-spaces', 'analysis', 'convergence-divergence']"
1307892,"If p(x) is a non-negative quadratic polynomial, p(0)=8 and p(8)=0, what is p(-4)?","A quadratic polynomial $p(x)$ is such that $p(x)$ never takes any negative values. Also, $p(0)=8$ and $p(8)=0$. What would $p(-4)$ be? I tried doing it by taking the minimum value as zero that is the vertex of the polynomial at $8$. How do we go about after finding out values of $a$ and $b$ in the standard form of the equation $ax^2 + bx + c$ ? Could this be done in a shorter way graphically?","['quadratics', 'algebra-precalculus']"
1307905,How to prove that $U_{2^n}$ is isomorphic as group to $\mathbb Z_2 \times \mathbb Z_{2^{n-2}}$ for $n \ge 3$?,How to prove that $U_{2^n}$ is isomorphic as group to $\mathbb Z_2 \times \mathbb Z_{2^{n-2}}$ for $n \ge 3$ ?,"['abelian-groups', 'group-theory', 'group-isomorphism']"
1307906,"If $F:M\to N$ is a smooth embedding, then so is $dF:TM\to TN$.","Question: I am trying to show that if $M$ and $N$ are smooth manifolds (without boundary), and $$F:M\to N$$ is a smooth embedding, then the differential $$dF:TM\to TN,\quad dF(p,v)=(F(p),dF_p(v))$$ is also a smooth embedding. In particular, this shows that an embedded submanifold of a smooth manifold gives rise to an embedded submanifold of the tangent bundle in a natural way. It is not difficult to show that $dF$ is a smooth immersion. Indeed, it has coordinate representations of the form $$dF(x,v)=(F(x),DF(x)v),\quad(x,v)\in \hat{U}\times\mathbb{R}^m\subseteq\mathbb{R}^m\times\mathbb{R}^m$$ so $$D(dF)(x,v)=\begin{pmatrix}DF(x) & 0 \\ \ast & DF(x) \end{pmatrix},$$ which has full rank since $DF(x)$ has full rank. Hence, we at least have that $dF(TM)$ is a immersed submanifold of $TN$ . But now I am stuck in showing that $dF$ is a topological embedding. It is clearly injective, so the inverse $$(dF)^{-1}:dF(TM)\to TM$$ exists. But how do you show it is continuous? Definitions: Here ""smooth"" means $C^\infty$ . The assumption that $F$ is a smooth embedding means that $F$ is a smooth immersion (i.e. $dF_p:T_pM\to T_{F(p)}N$ is injective at each $p\in M$ ) and that $F$ is a topological embedding (i.e. $F:M\to F(M)$ is a homeomorphism when $F(M)$ is given the subspace topology inherited from $TN$ ).","['smooth-manifolds', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
1307923,$|G|=p^nm$ and number of subgroups of order $p^s$,"We suppose that $G$ is a finite group such that $|G|=p^nm$ and $(p,m)=1$ ($p$ is prime). If $s\le n$ and $r_s$ is the number of subgroups of $G$ with order $p^s$, I want to prove that $r_s$ is congruent to 1 $mod \ p$. I have already proved, using the class equation, that if $|G|=p^n$ then $r_s$ is congruent to 1 $mod \ p$. I think we may use this and a Sylow theorem to get what we want, but I haven't found a proper way. Could you give my a hint? Thank you.","['abstract-algebra', 'group-theory']"
1307936,What exactly is a conifold for mathematicians?,"For physicists a conifold is a generalization of a manifold that has a singular point. For example the resolved conifold is the space given by the solutions of 
$$xy-zt = 0$$
where $(x,y,z,t)\in \mathbb{C}$.
As we move far away from that point towards infinity the geometry looks like $S^2 \times S^3$. At the same time there is a proceedure known as smoothing the conifold for which we replace the zero by some complex number $\mu$,
$$xy-zt = \mu$$
and this is known as the deformed conifold. This one also looks like $S^2 \times S^3$ towards infinity. The details can be found in reference I would like to ask though what do mathematicians refer to those constructions as and give me some reference maybe.","['complex-geometry', 'algebraic-geometry', 'differential-geometry']"
1307949,Evaluating a definite integral using Bessel functions,"I've been examining an integral I remember encountering in complex analysis: $$
I = \int_0^\pi e^{\cos\theta} \cos\left(n\theta - \sin\theta\right) d\theta
$$ where $n$ is a nonnegative integer. It's not too hard to show that $I = \frac{\pi}{n!}$ by changing $\cos\left(n\theta - \sin\theta\right)$ into $\operatorname{Re}\left[e^{i(n\theta - \sin\theta)}\right]$ and making the substitution $z = e^{i\theta}$ to turn the integral into a contour integral on the complex unit circle. However, I came across the following representation of the Bessel functions of the first kind on Wikipedia: $$
J_n(x) = \frac{1}{\pi} \int_0^\pi \cos\left(n\theta - x\sin\theta\right) d\theta
$$ Given the similarity between this formula when $x = 1$ and the initial integral, I'm sure there's some way to evaluate the integral using properties of the Bessel functions. But so far I'm stuck as to figuring it out. I tried expanding $e^{\cos\theta}$ as a series but that got ugly pretty quickly. Any suggestions?","['bessel-functions', 'complex-analysis', 'definite-integrals']"
1307981,Calculating intersection with diagonal in $\mathbb{P}^2 \times \mathbb{P}^2$,"The following is example 6.1.2 from Fulton, Intersection Theory. Denote projective coordinates on $\mathbb{P}^2$ by $[x,y,z]$, and on $Y=\mathbb{P}^2 \times \mathbb{P}^2$ by $([x,y,z],[u,v,w])$. Consider the lines $A=V(x) \subset \mathbb{P}^2$ and $B=V(z) \subset \mathbb{P}^2$. They meet in a point $P$. Define divisors $D_1=2A+B=V(x^2 z)$ and $D_2=A+2B=V(x z^2)$ in $\mathbb{P}^2$. Let $X$ be the closed subscheme $D_1 \times D_2 \subset \mathbb{P}^2 \times \mathbb{P}^2$, so $X=V(x^2 z,uw^2)$. Furthermore, let $f:V=\mathbb{P}^2 \to \mathbb{P}^2 \times \mathbb{P}^2$ be the diagonal. Fulton claims that the intersection product $X \cdot V= 3\alpha + 3\beta + 3[P]$, where $\alpha,\beta$ are zero cycles of degree $1$ on $A$ resp. $B$. I am trying to understand how this is calculated. I am aware that one could probably reverse the roles of $V$ and $X$ here to simplify things; I do not want to do that at the moment.
First, one would calculate the intersection $W=V\times_Y X$. One sees that $W=V(x^2 z, xz^2) \subset \mathbb{P}^2$. If we denote $g:W \to X$, we need to calculate the cycle of $C=C_W V \subset g^* (N_X Y)$, and transfer it to $W$ via Gysin. Because nothing interesting happens at infinity (or does it?), we can only look at the affine picture on $\mathbb{A}^2 =\{y\neq 0\}\subset \mathbb{P}^2$. The normal cone would then be 
$$\mathrm{Spec} \bigoplus_n (x^2z,xz^2)^n/(x^2z,xz^2)^{n+1}\simeq \mathrm{Spec}\ k[x,z,U,T]/(x^2z,xz^2,zT-xU),$$
whereas the pullback of the normal bundle would be $\mathrm{Spec} \bigoplus_n k[x,z]/(xz^2,x^2z)\otimes ((x^2z,uw^2)^n/(x^2z,uw^2)^{n+1})$. How would one now calculate $[C]$, and in particular, the intersection class $s^*[C]$, where $s:W\to g^* N_X Y$ is the zero section?","['algebraic-geometry', 'intersection-theory']"
1307996,"Hartshorne generically finite morphisms (II, 3.7)","I have a question concerning one of the exercises of Hartshorne, Ch. II. Namely: Exercise 3.7 about gerneically finite morphisms. A morphism $f: X \rightarrow Y$ with Y irreducible and $\eta$ generic point of Y, is called generically finite, if $f^{-1}(\eta)$ is a finite set. Now let $f: X \rightarrow Y$ be a dominant, generically finite morphism of finite type of integral schemes. Show that there is an open dense subset $U \subseteq Y$ sth. $f^{-1}(U) \rightarrow U$ is finite. As a hint, one should prove first that the function field of $X$ is a finite field extension of the function field of $Y$. I have started with the affine case: $X = \operatorname{Spec}\ A$, $Y = \operatorname{Spec}\ B$ with function fields $K$ resp. $L$. The assertion that $K$ now is a finite field extension of $L$ follows from Noether's Normalization theorem and Zariski's Lemma. But how can I proceed from there? I would be grateful for an idea to help me go on. Thank you!","['algebraic-geometry', 'schemes']"
1307998,How to find the eigenvalues of a block-diagonal matrix?,The matrix $A$ below is a block diagonal matrix where each block $A_i$ is a  $4 \times 4$ matrix with known eigenvalues. $$A= \begin{pmatrix}A_1 & 0 & \cdots & 0 \\ 0 & A_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & A_n  \end{pmatrix}$$ How do I find the eigenvalues of the block diagonal matrix $A$? Does this mean that I will have $4 n$ eigenvalues? Am I correct in thinking that the eigenvalues of the block diagonal matrix $A$ above are just a list of the individual eigenvalues of each $A_i$ and not the product of everything?,"['eigenvalues-eigenvectors', 'block-matrices', 'linear-algebra', 'matrices']"
1308010,Stuck in expressing factor into a sum of three perfect squares.,Two part question. (i) Consider the function $f(x)=x^3-6kx+k^3+8$. Show that we can write $f(x)$ as $(x+k+2)P(x)$ where $P(x)$ is a quadratic function. (ii) Show that $2P(x)$ can be written as the sum of three perfect squares and hence solve $f(x)=0$ for all values of $k$. My attempt. (i) By long division I have $P(x)=x^2-(k+2)x+k^2-2k+4$. I believe this part to be correct by trying a few cases of $k$ with Wolfram Alpha. (ii) $2P(x)=2x^2-2kx-4x+2k^2-4k+8$. No idea how to continue.,"['factoring', 'functions']"
1308053,Need isomorphism theorem intuition,"I am currently trying to understand the isomorphism theorems. The issue I am having is that I am struggling to find a way to think about them. In Stillwell's Elements of Algebra, I found a way to understand the first theorem ($\frac{G}{\ker \phi} \simeq \operatorname{Im} \phi$ for any homomorphism $\phi:G\rightarrow G'$). It was proven in terms of set functions (since there is a one-to-one correspondence between the elements $e \in \operatorname{Im} \phi$ and $\phi^{-1}(e)$). However, the second and third theorems are not even shown. Also, my curriculum is taken from Fraleigh's A First Course in Abstract Algebra. There, the first isomorphism theorem is longer than just the isomorphism between $\frac{G}{\ker \phi}$ and $\operatorname{Im}\phi$; He adds that there is a unique isomorphism $\mu: G/\ker\phi \rightarrow \phi[G]$ such that $\phi(x) = \mu(\gamma(x))$ for each $x \in G$. Here, $\gamma$ is the canonical homomorphism from G to $\frac{G}{\ker\phi}$. I still do not understand what the canonical homomorphism is since it is not in the index of the book. More importantly, I am really having a hard time just thinking about the definitions involved. I can see a proof, and understand why each step is accurate. Yet, it is too abstract for me to process. This has happened once before when I was learning measure- and integration theory. I couldn't understand anything for months until one day I read a definition of the Lebesgue integral in Euclidean space, and suddenly everything just ""clicked""; Within a week I could understand, rather than just know, the entire curriculum. I am hoping for another one of those moments before exam. I have the same problem now with certain aspects of abstract algebra. They generally involve factor groups and/or mappings. So, I have a suspicion that if I ""get"" the isomorphism theorems, the rest will connect.","['abstract-algebra', 'group-isomorphism']"
1308054,Oberwolfach Problem with restrictions on how seatings can be changed,"There are $30$ people at an alumni dinner, seated at $3$ round tables of $10$ seats each. After every time interval $\Delta t$, a position change event is required where everyone changes position simultaneously, in order to have the opportunity to sit next to someone different on his/her left and right. This results in a different seating configuration. What is the minimum number of seating configurations (i.e. number of position change events $+1$) required for everyone to have sat next to every other person just once? NB - If this is not possible then the last condition can be modified from “just once” to “at least once”, but please specify accordingly. I tried to work this out for a smaller number $n$ of up to $10$ people, seated only on $1$ table, and found that the minimum number of configurations is $\displaystyle\bigg\lfloor\frac n2\bigg\rfloor$. However, it gets complicated when there are different tables. This is an example of the Oberwolfach problem, and various papers and articles on this are available on the web, most of them dealing with generalized cases and require a fairly good understanding of graph theory. It would be appreciated if anyone could derive a user-friendly solution to the question in this particular case.","['graph-theory', 'combinatorics', 'permutations']"
1308085,Probability of two zero inner products,"Consider a random $n+1$-dimensional vector $v$. Each $v_1,\dots,v_n$ equals $1$ with probability $1/2$ and $-1$ with probability $1/2$ independently.  We also set $v_{n+1} = v_1$. Now consider a random $n$-dimensional vector $w$. Each $w_i$ equals $1$ with probability $1/4$ and $-1$ with probability $1/4$ and equals $0$ with probability $1/2$, all sampled independently. We know $$P\left(\sum_{i=1}^n v_i w_i = 0\right) = \sum_{k=0}^{\lfloor \frac n2 \rfloor}\frac{n!}{k!(n-2k)!k!}\left(\frac14\right)^k\left(\frac12\right)^{n-2k}\left(\frac14\right)^k \sim \frac{1}{\sqrt{\pi n}}.$$ I wrote computer code to compute  probabilities exactly for smallish $n$. It seems numerically that 
$$P\left(\sum_{i=1}^n v_{i+1} w_i = 0 \; \land \sum_{i=1}^n v_{i} w_i = 0\right) = \frac{\sum _{i=0}^{\lfloor n/2 \rfloor} {2(n-2i) \choose n-2i} {n \choose 2i} {4i \choose 2i}}{   2^{3n - 1}}.$$ Is this equality true and how could one prove it?","['probability', 'combinatorics']"
1308088,Norm of the quotient map for a normed space [duplicate],"This question already has an answer here : Showing that the norm of the canonical projection $X\to X/M$ is $1$ (1 answer) Closed 9 years ago . Let $X$ be a normed space and $F$ a closed subspace. On $X/F$ let us take the quotient norm $||[x]|| = \inf_{y \in F} ||x - y||$. Consider the quotient $q : X \rightarrow X/F$. I can see that, if $||x|| = 1$, then $||q(x)|| = \inf_{y \in F} ||x - y|| \leq ||x - 0|| = 1$ since $0 \in F$. This proves that $q$ is bounded and $||q|| \leq 1$. How may I show that $||q|| = 1$?","['operator-theory', 'functional-analysis']"
1308141,Prove that there is no solution in terms of elementary functions of $y'=x^2+y^2$,"I have seen before that $$y'=x^2+y^2$$ Has no solution in terms of elementary functions. I typed it in on Wolfram Alpha ( here ) and got a giant solution with a bunch of Bessel functions that I did not understand. This equation is almost like a Ricatti equation, but not exactly, so there may be some insight to be gained there. Again, my question is: How do you prove that this differential equation has no solution in terms of elementary functions?",['ordinary-differential-equations']
1308158,How to interpret complex eigenvectors of the Jacobian matrix of a (linear) dynamical system?,"Consider a linear ODE system of the following form: $$ \frac {dx} {dt} = Ax $$
In case $A$ has real eigenvectors, I can interpret them as the directions in which the system will move, if the initial value is already a point on the eigenvector. The eigenvalues corresponding to each eigenvector would tell me whether the system approaches the fixed point (negative eigenvalue) or moves away from it (pos. eigenvalue). The plot below shows an example vectorfield for the case
$ A = \left( \begin{array}{cc} 2 & 3\\ 1 & 4 \end{array} \right) $, in which the Jacobian $A$ has eigenvalues $1$ and $5$ and eigenvectors $\left( \begin{array}{c} -3 \\ 1 \end{array} \right)$ (blue) and  $\left( \begin{array}{c} 1 \\ 1 \end{array} \right)$ (green). Plot of real eigenvectors: My question is how to interpret the eigenvectors in case they are complex? If the Jacobian has complex eigenvectors, this means the eigenvalues are also complex, so the system will oscillate. Do the complex eigenvectors tell me something about the rotational axis of these oscillations? I have plotted the vectorfield of several linear oscillatory systems and to me it seems like the real part of eigenvector points in the direction of the ellipse in which the system oscillates. Here is the vectorfield plot for the system
$$A = \left( \begin{array}{cc} 0 & -3\\ 1 & -2 \end{array} \right)$$ 
( eigenvalues are $-1 \pm \sqrt{-2}$). The real part of the eigenvector is shown in blue. Plot of complex eigenvectors: If this is correct, how can I interpret the imaginary parts of the eigenvectors? Could you maybe point me out an article or a book in which this is explained?","['stability-in-odes', 'dynamical-systems', 'ordinary-differential-equations']"
1308173,Two definitions of coherent sheaf,"There are at least two ways to define a (quasi)coherent sheaf on the scheme $(X,\mathcal{O}_X)$, namely the one given in Hartshorne's ""Algebraic geometry"" (I guess you know it: it is given in terms of ""abstract"" modules) and the second close to Wikipedia's one: $\mathcal{O}_X$-module $\mathcal{F}$ is quasicoherent if and only if each point has a neighbourhood $U$ such that the sequence 
$$\bigoplus_{i\in I} \mathcal{O}_U\to \bigoplus_{j\in J} \mathcal{O}_U\to \mathcal{F}|_{U}\to 0$$
is exact.
Moreover, $\mathcal{F}$ is coherent on noetherian $X$ if $J$ can be chosen finite. How can I prove these definitions are equivalent? I'm sorry if the question is childish: I am a beginner in schemes and such kind of geometry.","['algebraic-geometry', 'schemes']"
1308195,Circle Rolling on Ellipse,"I've gotten interested in describing a circle rolling on an ellipse; specifically, the curve traced out by a point on the circumference of the circle. I want a symbolic solution to the general case, radius $r$, axes $a$ and $b$. I've written nine polynomial equations in terms of various angles and lengths. Exactly what ""solution"" means is subject to debate.  Let $(u,v)$ be the point on the circle. Similar to the cycloid, I would like an equation for $u$ in terms of a ""natural"" angle in the problem.  Similarly, an equation for $v$. Perhaps it is necessary to have a differential equation, so maybe $du/dt$, $u$, and $t$, where $t$ is an angle in the problem. I would have thought this was known, but I can't find it anywhere.","['plane-curves', 'geometry']"
1308202,"Prove that $G = \langle x,y\ |\ x^2=y^2 \rangle $ is torsion-free.","Prove that $G = \langle x,y\ |\ x^2=y^2 \rangle $ is torsion-free. Here $x^2$ is central as $x^2y=yx^2$ similarly $y^2$ is central. Apart from this  I do not know how to proceed. Taking any arbitrary word in $G$ does not help.","['abstract-algebra', 'group-theory']"
1308206,When does this equation have a solutions in integers: $ z ^ x + \bar z ^ y = 1 $?,"Let $ z $ be a complex number such that $ z = \alpha + i \beta $ , where $ \alpha$ and $\beta$ are integers. Let $ \bar z $ be the complex conjugate of $ z $ , and let $ x $ and $ y $ be integers. When does the following equation have a solutions in integers? $$ z ^ x + \bar z ^ y = 1 $$","['number-theory', 'diophantine-equations', 'complex-numbers']"
1308222,Writing some basic sentences in the language of set theory.,"I am having some trouble proving that some basic sentences in the language $L_\in$ of first order set theory are $\Sigma_1$ or $\Pi_1$, the reason being that I do not know how exactly to write them. Unfortunately in most books dealing with ranks $\Sigma_n$ and $\Pi_n$ this knowledge is assumed which makes my life complicated. Now an introduction. Every quantifier free formula is $\Delta_0$. If $\phi$ and $\varphi$ are $\Delta_0$ then so is $\phi\wedge\varphi$ and $\neg\phi$ and finally if $\varphi$ is $\Delta_0$ then $(\exists x)(x\in y\wedge \phi)$ is also $\Delta_0$. A formula is $\Sigma_1$ if of the form $(\exists x)\phi$ for $\phi\in\Delta_0$ and in $\Pi_1$ if can we writen as $(\forall x)\phi$ for $\phi\in\Delta_0$. I would like to see that the following sentences are $\Sigma_1$ formulas: 1) $\alpha$ is not a cardinal (initial ordinal) 2) $cf(\alpha)\leq \beta$ 3) $\alpha$ is singular. 4) $|x|\leq |y|$ And the following are $\Pi_1$: 1) $\alpha$ is a cardinal. 2) $cf(\alpha)$ is regular. 3) $\alpha$ is weakly inaccessible 4) $y=P(x)$ The first one for example asks to write ""$\alpha$ is an ordinal and there is $\beta <\alpha$ and a surjection from $\beta$ to $\alpha$"". So it could be written as as ""exists $f$ function such that for some $\beta<\alpha$, $f$ is a surjection form $\beta$ to $\alpha$"". Now this I can see as $\Sigma_1$ formula, however I still doubt if the solution is correct. Other question arise like, given $x$ and $y$ fixed, if we range a variable among functions with domain in $x$ and range in $y$, is that not a bounded quantifier? That is, is the sentence ""$(\exists f)(f \text{ is a function } \wedge dom(f)\subset x \wedge rg(f)\subset y$)"" in $\Delta_0$?",['elementary-set-theory']
1308286,What is the cardinality of $P(\mathbb{R})\setminus P(\mathbb{Q})$?,"I'm a beginner in elementary set theory and I'm looking for a simple way 
(I can use facts from cardinal arithmetic) to show that: $|P(\mathbb{R})\setminus P(\mathbb{Q})|=|P(\mathbb{R})|$ I would like to see several approaches for this. thank you.","['elementary-set-theory', 'cardinals']"
1308289,Continuously extending a set of independent vectors to a basis.,"Question: Let $I=(a,b)$ be an interval and let
  $$v_i:I\to\mathbb{R}^n,\quad i=1,\ldots,k$$
  be continuous curves such that $v_1(t),\ldots,v_k(t)$ are linearly independent in $\mathbb{R}^n$ for every $t\in I$. Can we always find continuous curves
  $$v_{k+1},\ldots,v_n:I\to\mathbb{R}^n$$
  such that $v_1(t),\ldots,v_n(t)$ forms a basis for $\mathbb{R}^n$ for each $t\in I$? I know that if $v_1(t),\ldots,v_k(t)$ is a set of linearly independent vectors, then we can extend it to a basis $v_1(t),\ldots,v_n(t)$ for $\mathbb{R}^n$. But how do we ensure that $v_{k+1},\ldots,v_n$ will be continuous? It is intuitively very clear that there must exist such continuous curves, because there are a lot of possible choices for extending the set to a basis. Surely, there must be no problem of doing it continuously, but I cannot find a way to prove it rigorously. It is easy to do this in $\mathbb{R}^3$ when we have two curves $v_1,v_2$. In that case, we can just take the cross product
$$v_3(t)=v_1(t)\times v_2(t)\in\mathbb{R}^3.$$
But in general, I don't know any explicit expression for producing linearly independent vectors $v_{k+1},\ldots,v_n$ in terms of the previous ones. Is there any?","['continuity', 'linear-algebra', 'general-topology']"
1308303,Limit of $n\left(e-\left(1+\frac{1}{n}\right)^n\right)$,"I want to find the value of $$\lim\limits_{n \to \infty}n\left(e-\left(1+\frac{1}{n}\right)^n\right)$$ I have already tried using L'Hôpital's rule, only to find a seemingly more daunting limit.","['calculus', 'limits']"
1308315,Legendre Differential equation,"Consider 
$$
P_n(x)=\frac{1}{2^n\,n!}\cdot \frac{\mathrm{d}^n}{\mathrm{d}x^n} \left[ (x^2 -1)^n \right]
$$ 
and the Legendre Differential equation 
$$
\frac{d}{dx}\left( (1-x^2) \frac{du}{dx}\right) + n(n+1) u = 0.
$$ Why is $P_n$ a solution of this equation? First I used the product rule to calculate 
$$
\frac{d}{dx}\left( (1-x^2) \frac{du}{dx}\right),
$$ 
then I used the Binomial theorem  but the calculation is awkward. I have no idea how the calculate effective.. Could you help me? I'm stuck. I appreciate your help.","['analysis', 'ordinary-differential-equations']"
1308317,Finding variance,"Given a sapmle $(X_1, X_2 , \ldots , X_n)$ from  normal distribution with parameters $(a , \sigma ^ 2)$, find $$ \operatorname{Var}\left( \frac{1}{n} \sum\limits_{n=1}^n(X_i - \overline X)^2\right)$$ where $\overline X$ is the sample mean. I can calculate it when $\overline X$  is replaced with expectation using independence , but in this case I cannot use independence. I want to reduce it to fourth moment of normal distribution but I fail to.","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
1308323,l'Hospital's rule with trigonometric functions,$$\lim_{x\to0^+}\frac{1-\cos(x)}{x^2\sin(x)}$$ I keep running in circles using the L'Hospital rule. After the third time applying it I got 0 but this isnt true from the graph. I can see it goes to +ve infinity. Please let me know if anyone has an elegant solution to this lengthy problem.,['derivatives']
1308348,Sum of independent random variables is also independent,"Given that $X, Y$ and $Z$ are discrete independent random variables,
how can one show that $X+Y$ and $Z$ are independent as well? So far, I tried using the definition of independent variables and simplifying (X+Y)'s probability function using discrete convolution. I'm not sure that's the best way though.","['probability', 'random-variables']"
1308349,Velociraptor escape,"This is a mock test problem from xkcd . I think that he should run at the angle bisecting two of the dinosaurs, but then again, one is wounded.",['geometry']
1308365,Wronskian of $x^3$ and $x^2 |x|$,"The question is fairly straight forward, given $2$ functions $y_1=x^3$ and $y_2=x^2|x|$ determine whether the functions are linearly dependent or independent. I am conflicted because in my differential equations textbook it states that two functions are linearly independent if their quotient is a non-constant function for all $x$ . Here it is evident that for $x<0$ the quotient $\frac{y1}{y2} = 1$ while for $x<0$ , $\frac{y1}{y2}=-1$ . This would lead to a conclusion that the functions are linearly independent. However, in the textbook it is also stated that two linearly dependent functions will have a Wronskian of zero on the real number line. So the Wronskian $W(y_1(x), y_2(x)) = \det \begin{vmatrix}y_1(x)& y_2(x)\\ y_1'(x)& y_2'(x) \end{vmatrix}$ and $y_1'(x) = 3x^2$ the derivative of $y_2'(x)$ is slightly more tricky but checking with wolfram alpha $y_2'(x)=\frac{3x^3}{|x|}$ so the Wronskian is given by: $$W=y_1(x)y_2'(x)-y_2(x)y_1'(x)=x^3\times\frac{3x^3}{|x|}-x^2|x|\times3x^2$$ Now multiplying the second term by $\frac{|x|}{|x|}$ in order to add the terms yields $$W=\frac{3x^6}{|x|}-3x^4\times \frac{|x|^2}{|x|}$$ At this point I assume $|x|^2=x^2$ which yields: $$W=\frac{3x^6}{|x|}-\frac{3x^6}{|x|}=0 $$ This would imply the functions are linearly dependent.... The answer provided by the textbook states that the functions are linearly independent. Does anyone have some words of wisdom for me :) Cheers",['ordinary-differential-equations']
1308397,What is a strictly positive probability distribution?,"I'm reading about Markov Random Fields . In the wiki page it's written that When the joint probability distribution of the random variables is
  strictly positive,... I'm so confused! Because a probability distribution gives the probability of the random variable having a specific value and so a probability (per definition) can never be negative! So what do they mean by strictly positive?","['probability-theory', 'definition']"
1308398,Proof that the set of intersection points is finite,"I am trying to understand the proof of the following theorem: Let $f,g\in K[x,y]$ without a common factor. Then $\#V(f,g)<\infty$. (Here $K$ is a field and $V(f,g):=\left\{(a,b):f(a,b)=g(a,b)=0\right\}$.) One step in this proof is not clear to me: We consider $f$ and $g$ as elements of $K(y)[x]$, where $K(y)$ is the field of all rational functions over $K$ in $y$. Then $\gcd(f,g)=1$ in $K(y)[x]$. My question: Why is this the case? It is clear to me that $K[x,y]\subset K(y)[x]$, but why can't $f$ and $g$ have a common factor in this larger set? I tried to assume the opposite that there is some common factor $h\in K(y)[x]$ such that $f=h\cdot h_1$ and $g=h\cdot h_2$ where $h_1,h_2\in K(y)[x]$. If I then take the least common multiple of the denominators of $h_1,h_2$ and $h$, i.e. some $b\in K[y]$, I obtain the equations $b\cdot f=\bar h_1\cdot\bar h$ and $b\cdot g=\bar h_2\cdot\bar h$ for some $\bar h,\bar h_1,\bar h_2\in K[x,y]\subset K(y)[x]$. Know I tried to use that $K(y)[x]$ admits a unique decomposition of each element in a product of irreducible elements...and got stuck. As I am not an algebraist, could someone please explain to me in simples words, how to prove this result without using too much theory from algebraic geometry (which I am not familiar with)?
Thank you very much in advance!","['abstract-algebra', 'algebraic-geometry']"
1308407,If an abelian group has more than 3 elements of order 2 then it must have at least 7 elements with order 2.,"This is an exercise in Gallian, Contemporary Abstract Algebra.  Intuitively, I cannot believe the statement is true.  It is essentially saying there is no abelian group with 4, 5 or 6 elements of order 2 regardless of how large the group is.  I have tried to suppose a group had exactly 4 (or 5 or 6) elements of order 2 in hopes of reaching a contradiction.  I know that if a group is cyclic then it has exactly 1 element of order 2.  I know that an element of order 2 is its own inverse and it is not the identity.  I have observed that indeed in $C_2 \times C_2$ there are 3 elements of order 2 and in $C_2 \times C_2 \times C_2$ there are 7 elements of order 2.  However, none of this is convincing me that the statement is true.",['group-theory']
1308413,VC-Dimension of Balls intersected with half-spaces,"In $d$ dimensional Euclidean space, the VC-dimension of both the set of balls and the set of half-spaces is $d+1$. It follows that the VC-dimension of balls intersected with half-spaces is $O(d \log d)$. Can we get the better result that the VC dimension of balls intersected with half-spaces is $O(d)$? My reason for wondering is that I have seen in the appendix of several learning theory papers the claim that the VC-dimension of half-balls is at most $2d + 2$ (without proof). It seems that half-balls are not so much less powerful than arbitrary intersections of balls and half-spaces, so maybe a similar result holds here. Thanks!","['geometry', 'machine-learning']"
1308428,"Serret-Frenet for Non-unit ""speed"" space curves","The Serret-Frenet equations form a system of linear, often non-autonomous, ordinary differential equations that recover the local tangent, normal and binormal vectors of a unit ""speed"" space curve from the curve's curvature and torsion. The equations are scaled for space curves of non unit ""speed."" ( see sec. 5 of the previous Wikipedia article ) This means that in order to find the local frame  one must also supply data about the curve parameterization. Often, one wants to find the curve parameterization for known curvature/torsion variables. Question 1: So, if one were to solve the SF equations $
\begin{align}
\dfrac{d\mathbf{T}}{ds} &= & \kappa \mathbf{N}, \\
\dfrac{d\mathbf{N}}{ds} &= - \kappa \mathbf{T} & & + \tau \mathbf{B},\\
\dfrac{d\mathbf{B}}{ds} &= & -\tau \mathbf{N},
\end{align}
$ for a given curvature and torsion, then would the associated space curve be identical, up to its ""speed,"" to the one found by solving $
\frac{d}{ds} \begin{bmatrix}
\mathbf{T}\\
\mathbf{N}\\
\mathbf{B}
\end{bmatrix}
= \|\mathbf{r}'(s)\|
\begin{bmatrix}
0&\kappa&0\\
-\kappa&0&\tau\\
0&-\tau&0
\end{bmatrix}
\begin{bmatrix}
\mathbf{T}\\
\mathbf{N}\\
\mathbf{B}
\end{bmatrix}?$ It seems that this should not be the case since the $||\textbf{r}'(s)||$ prefactor could greatly complicate the system of equations. Question 2: Assuming that the answer to question 1 is no, then what is the utility of the SF equations? Knowledge of $||\textbf{r}'(s)||$ likely comes from knowledge of $\textbf{r}'(s)$, which should allow one to find the tangent, normal and binormal vectors bypassing SF.","['differential-geometry', 'ordinary-differential-equations']"
1308463,Exponential integration problem,"$$ \int 2^{3x} \times 5^x \times 3^{2x} dx $$ I think we're supposed to convert all the terms into log form, but I'm not sure, and other than that I have no idea how to tackle this problem.",['integration']
1308504,Calculate all of the second order partial derivatives of a function?,My attempt: $g_x$ = 3$x^2$-6x $g_y$ = 3$y^2$-12y $g_{xx}$ = 6x-6 $g_{yy}$ = 6y-12 $g_{xy}$ = 0 $g_{yx}$ = 0 Are these correct? Also would I be correct in saying that $g_{xy}$ = $g_{yx}$ for all functions?,"['partial-derivative', 'multivariable-calculus']"
1308506,Double integral over trapezoid,"Compute $$\iint_D \frac{1}{(1+(x+2y)^2)^2} \,dx\,dy$$ where $D$ is given by $x \geq 0 , \, y \geq 0, \, 1 \leq x+2y \leq 2 \\$. I am supposed to solve it with the help of contour lines . By drawing $D$ we get a trapezoid with the corners $(1,0), \, (2,0), \, (0, 1/2), \, (0,1)$. A contour line $\gamma$ to $f(x,y) = x+2y$ would be a line segment with end points $(\lambda, \frac{\lambda}{2})$. Consider the area of a trapezoid bounded by the line segment and the line segment with corners $(0, \frac{1}{2}), \, (1,0)$: 
$$ A(\lambda) = \frac{1}{2}(\sqrt{(1/2)^2+1}+\sqrt{\lambda + (\lambda /2)^2)})\sqrt{(\lambda-1)^2+(\lambda/2-1/2)^2} = \frac{1}{2} \sqrt{5/4} (\lambda +1)(\lambda -1)\sqrt{5/4}=\frac{5}{8} (\lambda^2-1) $$ 
where, on the right side of the first equality, we are taking the mean of the sides of the trapezoid times the height. 
The derivative is $A'(\lambda) = \frac{5}{4}\lambda$. The double integral can be computed as a single integral: $$\int_{\lambda=1}^{\lambda=2} \frac{1}{(1+\lambda^2)^2} \frac{5}{4} \lambda \, d \lambda =\cdots=3/16$$
This answer is wrong. The answer sheet says it is $3/40$. What have I done wrong?",['multivariable-calculus']
1308530,Prove factorial problem,"$\forall n\in\mathbb N$ $C_n=\frac{1}{n+1}\left(\begin{matrix}2n\\n\end{matrix}\right)$. Prove that $C_{n+1}=\left(\begin{matrix}2n\\n\end{matrix}\right)-\left(\begin{matrix}2n\\n-1\end{matrix}\right)$ I tried many ways but got nothing. Appreciate any tips.
My attempt: By defining, $C_{n+1}= \dfrac{1}{n+2}\left(\begin{matrix}2n+2\\n+1\end{matrix}\right)=\dfrac{1}{n+2}\dfrac{(2n+2)!}{(n+1)!(n+1)!}=\dfrac{1}{n+2}*\dfrac{(2n+2)(2n+1)n!}{(n+1)^2n!n!}$ $=\dfrac{1}{n+2}\dfrac{2(2n+1)n!}{(n+1)n!}$",['discrete-mathematics']
1308598,Compactness of a group with a bounded left-invariant metric,"Let $G$ be a group equipped with a left-invariant metric $d$: that is, $(G,d)$ is a metric space and $d(xy,xz) = d(y,z)$ for all $x,y,z \in G$.  Suppose further that $(G,d)$ is connected, locally compact, and bounded.  Must $(G,d)$ be compact? I can show from the local compactness that $(G,d)$ is complete.  I don't see how to get it to be totally bounded, but I also can't think of a counterexample. If it helps, you can assume that $(G,d)$ is a topological group (i.e. right translation and inversion are homeomorphisms; we already know that left translation is an isometry).  I would even be interested in the case where $G$ is a finite-dimensional Lie group and $d$ induces the manifold topology, but I do not want to assume that $d$ is induced by a left-invariant Riemannian metric.","['metric-spaces', 'lie-groups', 'abstract-algebra', 'group-theory', 'general-topology']"
1308606,Matrix which represents the product of ideal classes of 2 matrices.,"Let $f(x)$ be an irreducible monic degree $n$ polynomial with $\mathbb{Z}$-coefficients and $\Theta$ be a root of $f(x)$. There is an old theorem of Latimer and MacDuffee that there is a 1-1 correspondence between the similarity classes of integral matrices with characteristic polynomial $f(x)$ and $C(\mathbb{Z}[\Theta])$, the ideal class group of $\mathbb{Z}[\Theta]$. If $f(x)=x_n+a_{n-1}x_{n-1}+\cdots +a_0$, then a matrix which corresponds to the identity of $\mathbb{Z}[\Theta]$ is the companion matrix of $f(x)$, \begin{equation*}\begin{bmatrix}0&1&0&\cdots&0&0\\0&0&1&\cdots&0&0\\
\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&\cdots&0&1\\
-a_0&-a_1&-a_2&\cdots &-a_{n-2}&-a_{n-1}
\end{bmatrix}.
\end{equation*} Also,  if $[I_X]\in C(\mathbb{Z}[\Theta])$ corresponds to a matrix $X$, then $[I_X]^{-1}=[I_{X^T}]$. That is, the ideal class corresponds to the transpose of $X$ is the inverse of the ideal class corresponds to $X$. This is due to Taussky. Question: Suppose that $A$ and $B$ are two $n\times n$ matrices with characteristic polynomial $f(x)$. How can we find a matrix $X_{A,B}$ such that $[I_{X_{A,B}}]=[I_A][I_B]\in C(\mathbb{Z}[\Theta])$? Are there any known results about this?","['algebraic-number-theory', 'matrices']"
1308636,Given 5 integers show that you can find two whose sum or difference is divisible by 6.,"Given any $5$ integers, prove that you can pick two among them whose sum or difference is divisible by $6$ . (You are not allowed to pick the same number twice.) I'm trying to solve this problem using the pigeon hole principle.
When dividing an integer by 6 there are 6 different remainders, {0, 1, 2, 3, 4, 5}. Seeing as there are the same number of ""holes"" (remainders of those 5 integers after being divided by 6) as there are ""pigeons"" I'm not sure how to go about solving this.","['pigeonhole-principle', 'elementary-number-theory', 'combinatorics']"
1308656,Statistics: Why doesn't the probability of an accurate medical test equal the probability of you having disease?,"Suppose there is a test for Disease A that is correct 90% of the time. You had this test done, and it came out positive. I understand that the chance that this test is right is 90%, but I thought this would mean the chance that you have a disease should be 90% too. However, according to Bayes' rule, your chance of disease depends on the percentage of the population that has this disease. It sounds absurd: If the test is correct then you have it, and if it's not then you don't, 90% of the time- so there should be 90% chance that the results are right for you... But on other hand, say 100% of population has it. Then regardless of the chance the test says you have it, let it be 90% or 30%, your chance is still 100%... now all of a sudden it doesn't sound absurd. Please avoid using weird symbols as I'm not statistics expert. It just deludes things for me and buries the insight.",['statistics']
1308674,The group of automorphisms of the unit disk [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Does the group of (conformal) automorphisms of the complex unit disk have a normal subgroup? (different from the identity and the group itself)","['abstract-algebra', 'complex-analysis']"
1308676,Maps between combinatorial necklaces,"I am aware that the number of necklaces with $m$ red beads and $n$ white beads, where $\gcd(m,n)=1$ , is equal to $\frac{1}{m+n}\binom{m+n}{n}$ . For example, if $m = 3$ and $n = 4$ , there are 5 possible necklaces. I am curious as to whether there is a “natural” cycle structure on the set of red/white necklaces. In the previous example, such a cycle might begin RRRWWWW $\to$ RRWRWWW $\to$ RRWWRWW $\to \dots,$ where the intent is to switch the positions of some of the beads until we’ve seen every possible necklaces exactly once. Ideally this cycle procedure would be describable for different numbers of beads.  Thank you in advance for your help.","['combinatorics', 'necklace-and-bracelets']"
1308771,Struggling with an Application of Rouche's Theorem,"Prove that the zeros of the polynomial $p(z)=z^n+c_{n-1}z^{n-1}\cdots + c_1z+c_0$ all lie in the open disc centered at $0$ with radius $$R=\sqrt{1+\vert c_0\vert^2+\vert c_1\vert^2+\cdots + \vert c_{n-1}\vert ^2}$$ Attempt: I assume Rouche's Theorem is to be used. Let $g(z)=z^n$. I would like to show $\vert p-g\vert<\vert g\vert$ on the boundary of the disc, where $\vert g \vert =R^n$ (on the boundary). $$\vert p-g\vert \le\vert c_{n-1}z^{n-1}\vert+\cdots+\vert c_1z\vert + \vert c_0\vert\\\le nR^{n-1}\max \vert c_i \vert$$ the latter is the best useful upper bound that I can find, but it is too large since I cannot show that it is less than $R^n$. If it were less than $R^n$, then we'd have $n\max \vert c_i \vert<R$. As a counterexample, let $n=2,c_0=2,c_1=3$, so that $n\max \vert c_i\vert = 2\cdot 3,$ while $R=\sqrt {1+2^2+3^2}=\sqrt{14}.$ How can I find a sharper upper bound for $\vert p-g\vert$ that will allow me to use Rouche's Theorem?","['roots', 'complex-analysis']"
1308788,Must an irreducible element in $\mathbb{Z}[\sqrt{D}]$ have a prime norm?,"Let $D\in \mathbb{Z}$ where $D$ is not a perfect square. Prove that if $\alpha\in \mathbb{Z}[\sqrt{D}]$, and $\alpha= a+b\sqrt{D}$ with:                       $|a^2-Db^2| = p$, a rational prime, then $\alpha$ is irreducible in $\mathbb{Z}[\sqrt{D}]$. Is the converse true? So, for the if statement I figure I should assume $\alpha$ isn't irreducible, say $\alpha=\beta\gamma$ where they are not units, then $N(\beta)N(\gamma)=p$, but then I feel it is incorrect to assume a norm exists, because this would make it a Euclidean domain? is that the purpose of the absolute value?","['number-theory', 'notation', 'algebraic-number-theory']"
1308789,"If $f$ is defined on the Upper-Half Plane and $f(in)=e^{-n},$ find $f(1+i)$","Let $f$ be a bounded analytic function in the upper half plane. Suppose that $f(in)=e^{-n}$ for all $n\in \mathbb{N}$. Find $f(1+i)$ and explain why the value you found is the only one possible. I don't know how to proceed. I have two ideas, but I don't think they have any hope.
Let $g(z)=f(z)-e^{-z}$. Then $g(in)=0$ for $n\in \mathbb{N}$. I cannot apply the identity theorem to $g$ since the limit point of its zero set is $\infty$ which doesn't belong to the upper half-plane. The other idea was to apply Cayley's transformation $\frac{z-i}{z+i}$ to $g$, but in this case the limit point becomes $1$, which doesn't belong to the boundary of the unit disc. In either case, the identity theorem cannot be applied. Any input would be greatly appreciated.",['complex-analysis']
1308805,Limit of a function involving trigonometric exponents.,"Question: If $\large f(x)=[1^{\csc^2(x)}+2^{\csc^2(x)}+3^{\csc^2(x)}+\cdots+100^{\csc^2(x)}]^{\sin^2(x)}$ Find $\large \lim\limits_{x \to 0} f(x)$. My attempt: Let $\space y=f(x)$, Taking log on both sides I tried to simplify the expression but it messed it up a lot. So is there a general way to solve such problems or do I just need to simplify the expression and use the limit?","['calculus', 'limits', 'algebra-precalculus', 'trigonometry']"
1308847,Strategy for selecting the optimal time to check a cooldown timer,"This is a hard problem for me to word in the title, so I'll try to do better now. Consider the following ""game"": You are sitting in a room beside a table. In the middle of the table there exists a box containing a very large sum of money. The box will only open if you've waited long enough (the time to wait is a random value between $0$ and $MAX$ seconds inclusive). So, there is a chance that the box will be unlocked immediately, or it very well might not unlock until MAX seconds have elapsed. Here is the tricky part. The act of CHECKING the box to see if its open resets the timer back to zero. So say we have $MAX = 100$ seconds, $MIN = 0$ seconds, and the ACTUAL timer on the box is $25$ seconds. If we wait at least $25$ seconds, the box will be open. However, if we open at any time before 25 seconds, we have to wait at least $25$ seconds from THEN before the box will be open. So you might ask ""Who cares if I have to wait MAX seconds? Im guaranteed to be able to open it after MAX seconds"". Well what if we add the twist that each elapsed second since you sat down will decrease the value inside the box by $\frac{3}{4*MAX}$ of the original value. If we use that, then that means if you wait MAX seconds, you will get a quarter the money $\frac{money - 3*MAX}{4*MAX} = 0.25*money$. If you open it at $25$ seconds (max seconds is $100$ remember), then you get $\frac{money - (3*25)}{(4*100)*money} = \frac{325}{400}$ of the money, or 81.25%. This is the BEST you can do, you just don't know that. So, what is the optimal strategy to get the most money out of the box? If the actual timer was $0$ seconds, you could immediately take 100%. However if it's $10$ seconds and you first check $1s$, then $2s$, then $3s$... you're quickly losing value since the penalty is based on TOTAL ELAPSED TIME. So checking at 1, 2, 3 would be 6 elapsed seconds. Another key piece of information is that the actual timer value is EQUALLY LIKELY to be any time between 0 and MAX seconds. I think that this fact and the penalty equation ($\frac{3}{4*MAX}$ for example) are the main keys to solving this puzzle. Obviously you can guarantee yourself $25%$ money by just waiting MAX seconds, but we are greedy and want the most we can possibly get (On AVERAGE). What is our strategy? As for my thoughts... I think the answer is that you check every k seconds... maybe $\sqrt(MAX)$ or something. It would be something that I could easily play with and simulate in a programming sandbox, trying all sorts of different stepping schemes for a bunch of random actual values between 0 and some max. Eventually I may be able to pull out some sort of general solution. It would be more interesting to me though if anyone recognizes this problem and knows an analytical optimal solution based on the penalty amount and the fact that the chances are uniform. Any ideas are welcome, this is just for fun! PS. I thought of this because I was recently playing a game and got banned for joining and leaving too many games too quickly. You are banned for a time between MIN and MAX seconds (based on offense though, not equally likely) and every time you try to login your penalty timer resets. So you don't know what the penalty timer is, you just know the time you waited since your last try wasn't long enough. Goal of course is to play again in the quickest amount of time. But this should be pretty much identical to what I've described above. EDIT: SOLUTION
Thanks to the derivation of the optimization function by Bey, I was able to solve this problem. I noticed that with a few tests, 
$\sum_{i=1}^{N-2}t_i > \sum_{i=1}^{N-2}t_i*t_{i+1}$ Consider N = 3, which gives us:
$t_1 > \dfrac {1}{M} * (t_1*t_2)$ Which is the same as: $t_1(1 - \dfrac{t_2}{M}) > 0$ This is always true since  $t_2 \leq {M}, t_1 \geq 0$ Or, in general... $t_1 + t_2 +...+t_{N-2} > \dfrac {t_1t_2 + t_2t_3 +...+ t_{N-2}t_{N-1}} {M}$ because: $t_1(1 - \dfrac{t_2}{M}) + t_2(1 - \dfrac{t_3}{M}) +...+t_{N-2}(1 - \dfrac{t_{N-1}}{M}) > 0$ Thus, our best solution is to simple use N = 1 guess, where $t_1 = t_n = M$, because the cost function is zero there. So unfortunately not an exciting result, but it seems in such circumstances as this, you cannot do better on average than just making one choice which is the max time. If you don't know the maximum time? That's a different problem!","['optimization', 'algorithmic-game-theory', 'statistics', 'algorithms']"
1308872,"Prob. 9, Sec. 3.9 in Erwin Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: Finite-dimensional range and the form of images","Let $H$ be a Hilbert space, and let $T \colon H \to H$ be a bounded linear operator. Then how to show the following? The range of $T$ is finite-dimensional if and only if $T$ can be represented in the form 
$$Tx = \sum_{j=1}^n \langle x, v_j \rangle w_j \ \ \ [ v_j, w_j \in H].$$ My effort: Supppose that $\dim \mathscr{R}(T) = n$, and let $\{ e_1, \ldots, e_n \}$ be an orthonormal basis for $\mathscr{R}(T)$. Then, for every $x \in H$, we have 
$$Tx = \sum_{j=1}^n \langle Tx, e_j \rangle e_j = \sum_{j=1}^n \langle x, T^* e_j \rangle e_j,$$
where $T^*$ denotes the Hilbert adjoint operator of $T$. So we can take $w_j \colon= e_j$ and $v_j \colon= T^* e_j$. Is this reasoning correct? If so, then how to show the converse? Doest the above representation mean that the same $v_j$s and $w_j$s will be used for every $x \in H$? If so, then range of $T$ is spanned by the finitely many elements $w_1, \ldots, w_n$ and is clearly finite-dimensional.","['inner-products', 'real-analysis', 'functional-analysis', 'analysis', 'linear-algebra']"
1308876,Sum of Distances between Points on a Regular $n$-gon,"I received a question asking to determine a formula to sum the distances between all points of a regular $n$-gon inscribed in a circle of radius $1$. To solve this, I instead worked with the equivalent question of finding the sum of the distances between all distinct roots of $z^n-1$ in the complex plane. I showed that the distance between points $p$ vertices apart is $$\left|\exp\left(\frac{2\pi i p}{n}\right)-1 \right|=2\sin\left(\frac{\pi p}{n}\right)$$ I then determined how many lines connect points $p$ vertices apart and summed up these distances (this required separate formula for even and odd $n$). I then simplified the sum using $\sin(z)=\Im(e^{iz})$ and the formula for a finite geometric series. Several trig identities later, I arrived at the formula for the sum of the distances: 
$$n\cot\left(\frac\pi{2n}\right)$$ While the formula is surprisingly simple and elegant, my proof of it is very mechanical, requiring many trig identities, and not really giving any intuition into the final solution. Can anyone come up with other proofs of the formula?","['geometry', 'polygons', 'complex-numbers', 'trigonometry']"
1308895,Two problems related to continuity of a metric from Munkres' topology book,"Let $X$ be a metric space with metric $d$. Show that $d:X\times X\to \mathbb{R}$ is continuous. Let $X^\prime$ denote a space with the same underlying set as $X$. Show that if $d:X^\prime\times X^\prime \to \mathbb{R}$ is continuous then the topology of $X^\prime$ is finer than the topology of $X$. My attempt :
for the first one, we take some interval $(a,b)\subset \mathbb{R}$. We have to show $A=d^{-1}\left((a,b)\right)$ is open. So, choose any $(x,y)\in A$. We need to show that $(x,y)\in U\times V\subseteq d^{-1}((a,b))$ where $U,V$ are open in $X$. Here is where I am stuck. I also tried to use the usual $\epsilon-\delta$ idea but for that we need to have $X\times X$ as well as $\mathbb R$ as a metric space. Even if the former is possible using the uniform metric, we cannot do that on $\Bbb R$. For the second one, we want to find a basis of $X^\prime$'s topology, say $\mathcal T$ with elements $\mathcal{B}_\alpha$ for $\alpha\in J$. Then we take any basis element $B_d(x,\epsilon)$ of $X$. Let $y\in B_d(x,\epsilon)$. We want $y\in \mathcal{B}_\alpha\subseteq B_d(x,\epsilon)$. But how to proceed after that? Any help will be appreciated. Thanks a lot.","['metric-spaces', 'continuity', 'general-topology']"
1308928,What are some interesting blogs about general topology?,"We have several question asking about book recommendations for general topology - for example the posts linked to Best book for topology? or the posts mentioned in the relevant section  of http://meta.math.stackexchange.com/questions/1868/list-of-generalizations-of-common-questions . These posts range from beginner level books to a comprehensive reference. Are there also some blogs that would be useful for somebody interested in general topology? I am interested in blogs intended for audience of any level, from beginners to research level.","['online-resources', 'big-list', 'general-topology']"
1308944,Filtration of Markov Chains in general state space,"I am reading the book Markov Chains and Stochastic Stability from Meyn and Tweedie. They define Markov chains on a measurable state space $(E,\Sigma)$ (Chapter 3.4) and they define it on the space $\Omega = \prod_{i \in \mathbb{N}}E, $ with an $\sigma$-algebra $\mathcal{A}$ which is the smallest $\sigma$-algebra that contains all cylinder sets with only finitly many sets different from $E$ $$A_1 \times A_2 \times \dots A_n \times E \times E \times \dots$$ Then they define the Markov chain as a family of random variables $(X_n)_{n \in \mathbb{N}}$
where for $\omega=(x_n)_{n \in \mathbb{N}}\in \Omega$ they set
$$X_n(\omega)=x_n .$$ Thus, all Markov chains are defined on the same set $\Omega$ and the random variables $(X_n)$ are also always the same. Now if they talk about a certain initial distribution $\mu$ and transition kernel $p(x,A)$; then they assoicate a Markov chain to it by constructing a specific measure $\mathbb{P}_\mu$. Thus, by this definition, two Markov chains only differ on the probability measure of the probability space. My problem is that in the book they define the term
$$ \mathcal{F}_n = \sigma(X_0,\dots,X_n) \subseteq \mathcal{B}(X^{n+1})$$
and they say which is the smallest $\sigma$-field for which the random variable
  $\{X_0,\dots,X_n\}$ is measurable. In many cases $\mathcal{F}_n$ will
  coincide with $\mathcal{B}(X^{n+1})$, although this depend in
  particular on the initial measure $\mu$ choosen for a particular
  chain. How can $\mathcal{F}_n$ depend on the initial measure? The random variable is already defined as $X_n(\omega)=x_n$, and thus the measurability of $\{X_0,\dots,X_n\}$ depends only on $\Sigma$ and $\mathcal{A}$ where does the intial measure $\mu$ comes into play? Update: After seeing the answers, I think it is a good thing to provide my question with an example. Lets consider the case where $E=\{1,2\}$ and $\Omega = E \times E$, then the random variables $X_0$ and $X_1$ are already defined as above, in particular $X_0$ is defined as
$$ X_0 ((1,1))=X_0((1,2))=1$$ and $$X_0((2,1))=X_0((2,2))=2.$$  Now if $\mathbb{P}_\mu$ is the probability that $X_0 = 1 $, then we must have
$$  \mathbb{P}_\mu[\{(1,1),(1,2)\}]=1.$$
But this is completely independent from  defining $\mathcal{F}_0$ (or $\mathcal{F}_n$). In this case we always have
$$\mathcal{F}_0 = \{\{(1,1),(1,2)\},\{(2,1),(2,2)\},E,\emptyset \} $$
which does not depend on $\mu$. It seems to me that in the answers one believes that $\mathbb{P}_\mu[\{(2,1),(2,2)\}]=0$ implies somehow that this set should not belong to $\mathcal{F}_0$, but I think this is not correct.","['probability', 'markov-chains', 'measure-theory']"
1308972,Is this function a metric?,"Let $X$,$d$ be a metric space.
Define $d'$ as the minimum of $1$ and $d$: $$ d':\ X^2 \rightarrow \mathbb{R}:\ d'(x,y) = \min\{1,d(x,y)\} $$ The question is whether $d'$ is a metric. I've managed to prove that the first two properties of a metric hold for this function, but I can't seem to figure out how to prove the triangle inequality. Any hints?","['analysis', 'metric-spaces']"
1308983,Find the flux across a part of the surface $\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1$,"Consider the vector field $$F(x, y, z)= \frac{(x{\rm i} + y{\rm j} + z{\rm k})} {(x ^2+ y ^2 + z ^2)^\frac{3}{2}},$$
and let $S$ be the part of the surface $$\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1$$
in the first octant bounded by the planes $y = 0, y =x\sqrt{3}$
and $z = 0$, oriented upwards. Find the flux of $F$ across the surface $S$.","['surface-integrals', 'vector-analysis', 'calculus', 'multivariable-calculus']"
1308992,Why doesn't a Taylor series converge always?,The Taylor expansion itself can be derived from mean value theorems which themselves are valid over the entire domain of the function. Then why doesn't the Taylor series converge over the entire domain? I understand the part about the convergence of infinite series and the various tests. But I seem to be missing something very fundamental here..,"['taylor-expansion', 'real-analysis', 'soft-question']"
1309018,Logs rules and Solving,I've got the equation : $$-1=\frac{-8e^{-t} + 3e^t}{2e^t}$$ I've moved some stuff around to get : $5e^t = 8e^{-t} $ But not sure where to go from here. Thanks for any help,"['algebra-precalculus', 'logarithms']"
1309043,Is there a proof show that : $\cos(z)$ and $\sin(z)$ are images of unbounded functions?,"if we knew that :cos and sin are bounded function $\mathbb{R}$ for any real number $x$ . let $z $ be a complex variable , Is there a proof show that : $\cos(z)$ and $\sin(z)$ are images of  unbounded functions ? Any kind of help is appreciated.",['complex-analysis']
