question_id,title,body,tags
4275439,"If $f(\frac{x}{y})=\frac{f(x)}{f(y)}$ for all $x,y \in \Bbb R$ , $y \neq 0$ and $f'(x)$ exist for all all $x$ , $f(2)=4$, Then $f(5)$ is?","If $f\left(\frac{x}{y}\right)=\frac{f(x)}{f(y)}$ for all $x,y \in \Bbb R$ , $y \neq 0$ and $f'(x)$ exist for all $x$ , $f(2)=4$ ,  Then $f(5)$ is? I am solving this question using my teacher's method which is as follows My Approach: Differentiate with respect to $x$ assuming $y$ to be constant $f'\left(\frac{x}{y}\right) \cdot \frac{1}{y}=\frac{f'(x)}{f(y)}$ But i am not sure how to processed further using this method to get the result because i am not given any derivative. I know definition method to solve this question Here is the question which has been solved using direct differentiation Let $f:\mathbb{R \rightarrow R}$ satisfies $f(x)+f(y)= f \biggl(\frac{x+y}{1-xy}\biggl)$ and $f'(0)=5$. Then find $f(x)$ P.S.:- I've solved many question using direct differentiation method but I am not getting above question","['limits', 'derivatives']"
4275494,Prove $\sum_{k=0}^{n}\binom{n}{k}\frac{(-1)^{k}}{(2k+1)^{2}}=\frac{4^{n}}{2n+1}\binom{2n}{n}^{-1}\left(1+\frac{1}{3}+\cdots+\frac{1}{2n+1}\right)$,"show that $$\sum_{k=0}^{n}\binom{n}{k}\frac{(-1)^{k}}{(2k+1)^{2}}=\frac{4^{n}}{2n+1}\binom{2n}{n}^{-1}\bigg(1+\frac{1}{3}+\cdots+\frac{1}{2n+1}\bigg)$$ I try to prove this problem, but I don’t have idea. Just I discover that the left hand side is equal to $$\int_{0}^{1}\int_{0}^{1}(1-x^{2}y^{2})^{n}dxdy$$ Can I get hint of proving this problem. The second one is related to first one. Show that $$\sum_{n=1}^{\infty}\frac{1}{n}\sum_{k=0}^{n}\binom{n}{k}(-1)^{k}\frac{1}{(2k+1)^{2}}$$ is algebraic integer.","['power-series', 'sequences-and-series']"
4275651,Some questions about the concept of underdetermined systems,"I'm reading a linear algebra textbook and I have some confusion about the concept of underdetermined systems $A\mathbf{x}=\mathbf{y}$ : First, we know that for each vector $\mathbf{y}$ in $\mathbb{R}^m$ the underdetermined linear system is either inconsistent or has infinitely many solutions. So are there some theorems that tell us when the system is inconsistent and when it has infinitely many solutions? Second, if an underdetermined linear system has infinitely many solutions, is it guaranteed that a positive solution (all elements in $\mathbf{x}$ are positive) exists? It would be very appreciated if anyone could give some explanation on them.","['matrices', 'determinant', 'linear-algebra']"
4275727,Moment Generating Function (MGF) of Hypergeometric Distribution is No Greater Than MGF of Binomial Distribution with the Same Mean,"The Setup Consider a hypergeometric distribution $X$ with parameters $N, n, m,$ i.e. $$\mathbb{P}[X = k] = \frac{{m \choose k} {N-m \choose n-k}}{{N \choose n}},$$ for $k$ running from $0$ to $\min\{n,m\}.$ The mean $\mathbb{E}[X]$ is $\frac{mn}{N}.$ Let $Y$ be a $\mathsf{Bi}(m,n/N)$ distribution, i.e. $$\mathbb{P}[Y = k] = {m \choose k}\left(\frac{n}{N}\right)^k \left(1-\frac{n}{N} \right)^{m-k}.$$ $Y$ also has mean $\frac{mn}{N}.$ (In $X,$ the roles of $m$ and $n$ are interchangeable, so the following should hold for $\mathsf{Bi}(n,m/N)$ also.) In Janson, Łuczak, and Ruciński's Random Graphs (JLR), the proof of Theorem 2.10 (page 29 in my edition) is largely left as an exercise. It depends on the following fact: The Question $$(1) \ \ \forall u \in \mathbb{R}, \mathbb{E}[e^{uX}] \leq \mathbb{E}[e^{uY}].$$ That is, the MGF of $X$ is less than or equal to the MGF of $Y.$ Intuitively, this makes sense, since $Y$ seems more likely than $X$ to be far from its mean. How can we prove this? What I've Tried JLR references Hoeffding 1963 for this result. I went through the parts of that paper that seemed most relevant to this problem--""Sampling from a Finite Population"" and some of ""Sums of Dependent Random Variables"", but I was unable to find anything that proves $$\forall u \in \mathbb{R}, \mathbb{E}[e^{uX}] \leq \mathbb{E}[e^{uY}].$$ (I skimmed the rest of the paper as well.) One more remark about Hoeffding: when this paper talks about $\mathbb{E}[e^{cZ}]$ it seems to usually require $c>0$ , whereas our desired result holds for all $u \in \mathbb{R}.$ I have tried writing out explicit expressions for these MGFs: $$\mathbb{E}[e^{uX}] = \sum_{k} \frac{{m \choose k} {N-m \choose n-k}}{{N \choose n}} e^{uk}$$ and $$\mathbb{E}[e^{uY}] = \sum_{k} {m \choose k} \left(\frac{n}{N} \right)^k \left(1-\frac{n}{N}\right)^{m-k} e^{uk}.$$ These summands share factors of ${m \choose k}$ , and $e^{uk} > 0,$ so one might hope to prove $$(2) \ \ \forall k \leq m,n \leq N, \ \ \frac{{N - m \choose n-k}}{{N \choose n}}  \leq \left(\frac{n}{N} \right)^k \left(1 - \frac{n}{N} \right)^{m-k}.$$ $(2)$ , if true, would prove $(1), \mathbb{E}[e^{uX}] \leq \mathbb{E}[e^{uY}].$ I currently do not believe this inequality is true, based on some numbers I put into Maple (but I may have made an error there). In any case, although this inequality would be sufficient, it is not necessary for proving (1). I also tried expanding $e^{uX} = 1 + uX + \frac{u^2 X^2}{2} + \cdots,$ and similarly for $e^{uY}.$ For $u \geq 0,$ (probably using Fubini-Tonelli somewhere), it would suffice to prove $$(3) \ \ \forall K \in \mathbb{N}, \ \ \mathbb{E}[X^K] \leq \mathbb{E}[Y^K].$$ I do not know how to do this (although it seems true), and in any case it wouldn't directly prove (1) when $u < 0.$","['probabilistic-method', 'probability-distributions', 'combinatorics', 'probability', 'hypergeometric-function']"
4275737,"Isomorphism between global sections of normal sheaf and $\mathrm{Ext}^1_X(i_*\mathcal{O}_Z, i_*\mathcal{O}_Z)$","Let $X$ be an n–dimensional compact algebraic manifold, $i \colon Z \to X$ a closed submanifold, and let $\mathcal{N}_{Z|X}$ denote the normal sheaf. I have trouble understanding the isomorphism $$
H^0(Z,\mathcal{N}_{Z|X}) \xrightarrow{\cong} \mathrm{Ext}^1_X(i_*\mathcal{O}_Z, i_*\mathcal{O}_Z)
$$ stated in the paper https://arxiv.org/pdf/math/9912245.pdf in the statement of Proposition 8.7.
I think I understand how the map is defined, i.e., by applying the functor $\operatorname{Hom}_X(-, i_*\mathcal{O}_Z)$ to the short exact sequence $$ 0 \to \mathcal{I} \to \mathcal{O}_X \to i_* \mathcal{O}_Z \to 0$$ and considering the associated long exact sequence. I believe the map so obtained is injective,
however I don't understand how to prove it is surjective.",['algebraic-geometry']
4275873,Proving $\sum_{k=2}^n \frac{(k-2){n-k+2\choose k-1}+k{n-k+1\choose k-1}}{k{n\choose k}}=1$ for $n\geq 2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question My friend showed me a difficult combinatorial identity I cannot solve. Prove that: $$\sum_{k=2}^n \frac{(k-2){n-k+2\choose k-1}+k{n-k+1\choose k-1}}{k{n\choose k}}=1$$ for all $n\ge 2$ . How do I prove this? Edit: I would like to bump this again, due to lack of attention.","['summation', 'binomial-coefficients', 'combinatorics']"
4275961,"Given $2^{n-1}$ subsets of a set with $n$ elements with the property that any three have nonempty intersection, prove that ....","Given a family of $2^{n-1}$ subsets of a set $S=\{1,2,3,...,n-1,n\}$ with the property that any three have nonempty intersection. Prove that the intersection of all the sets in this family is nonempty. I'm interested if the finishing of the solution here can be done like this: As there was constructed $W$ is $n-1$ dimensional set in $\mathbb{F}_2^n$ . Thus we have $W^{\bot}\oplus W =\mathbb{F}_2^n$ and $\dim W^{\bot} =1$ , so $W^{\bot} = \{0,v\}$ for some vector $v\ne 0$ . We can assume WLOG $$v =
(\underbrace{1,1,1,...,1}_k,0,0,...,0)$$ Now each vector can be uniquely expressed with $v$ and some $w\in W$ , in particulary we have $$e_1=(1,0,0,...0) =av+w$$ for some $w\in W$ and $a \in
\mathbb{F}_2$ . So if $a=0$ then $e_1\in W$ so $0=v\cdot e_1 =1$ which is impossible. Thus $a=1$ then $e_1-v \in W$ so $0=v\cdot
(e_1-v) =1-k$ which means $k$ is odd. So each vector not in $W$ is generated with odd number of vectors
in $\{e_1,e_2,...e_k\}$ and an arbitrary number of vectors in $\{e_{k+1},e_{k+2},...e_n\}$ where $e_1,e_2,...,e_n$ represents the standard basis of $\mathbb{F}_2^n$ . Since the number of vectors not in $W$ is $2^{k-1} \cdot 2^{n-k} =
2^{n-1}$ , we see if $k\geq 3$ then we have in $e_1$ and $e_2$ in $W^C$ so sets $\{1\}$ and $\{2\}$ are in $\mathcal{F}$ which is
impossible. So $k=1$ and all sets in $\mathcal{F}$ have $1$ in
common. Non linear algebra solution: Prove that the intersection of all the sets is nonempty.","['contest-math', 'extremal-combinatorics', 'solution-verification', 'linear-algebra', 'combinatorics']"
4275976,Chain Rule for Partial Derivatives and Multi-variable Functions,"I have the following relation: $ c = 2c_1-1 $ I also know that $c_1 = c_1(z,t)$ . I want a way to simplify: $$ \frac{\partial^2}{\partial z^2}\left(\frac{\partial g(c_1)}{\partial c}\right) $$ which I know is going to involve the chain rule and then probably a product rule for the second one. My attempt is as follows:
First, I notice that $c_1 = \frac{c-1}{2} $ so I have $\frac{\partial g}{\partial c}=2\frac{\partial g}{\partial c_1} $ . Then we get $$ 2\frac{\partial}{\partial z}\frac{\partial}{\partial z}\left(\frac{\partial g}{\partial c_1}\right)=2\frac{\partial}{\partial z}\left[\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial c_1}{\partial z}\right] $$ Applying the second partial with respect to $z$ , we then have to use the product rule: $$ = 2\left[\frac{\partial^3g}{\partial c_1^3}\cdot\left(\frac{\partial c_1}{\partial z}\right)^2+\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial^2 c_1}{\partial z^2}\right] $$ Is this correct? Is there a simpler way to do this?","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
4275982,Counterexample on mixed partials,"I was thinking about Young's and Schwarz's theorem on when do partial derivatives be equal and I was wondering about how smooth can a function whose mixed partial are not equal be. I was wondering there is a function $f:\mathbb{R}^2 \to \mathbb{R}$ , such that the following hold. a) $f$ is continuous in $\mathbb{R}^2  $ (We can assume $f(0,0) = 0$ ) b) $f_x$ and $f_y$ and their partial derivativess exist everywhere (We can assume $f_x(0,0) = f_y(0,0) = 0$ ) c) $f_x$ and $f_y$ are continuous in $\mathbb{R}^2$ (so $f$ is differentiable at $(0,0)$ ) and the directional derivatives at $(0, 0)$ can be expressed as $L(v)$ , where $L$ is linear. d) $f_{xx}$ , $f_{yx}$ , $f_{xy}$ , $f_{yy}$ are continuous in $\mathbb{R}^2\smallsetminus\{(0, 0)\}$ . e) $f_{xx}$ is continuous at $(0,0)$ (so $f_x$ is differentiable at $(0, 0)$ ). (We can assume $f_{xx}(0,0) = f_{yy}(0,0) = 0$ . If $f_y$ was also differentiable at $(0, 0)$ , by Young's theorem mixed partial would be equal. If $f_{yy}$ or $f_{xy}$ we continuous at $(0,0)$ , $f_y$ would be differentiable at ( $0, 0$ ). Assumptions can be made because if such and $f$ exist, then $f - f(0,0)- xf_x(0,0) - yf_y(0,0) -  \frac{x^2}{2}f_{xx}(0,0) - \frac{y^2}{2}f_{yy}(0,0)$ will also satisfy the desired property.",['real-analysis']
4275986,What is the inverse of $f'(x)-f(x)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Consider the following function $P$ defined as $P(f(x))=f'(x)-f(x)$ What is the inverse of $P$ ? I can't figure this one out.
A hint would be very helpful.","['analysis', 'ordinary-differential-equations']"
4275987,Identifying null and alternative hypothesis,"I need help with an exercise. It says: The president of a university claims that the mean time spent partying by all students at this university is not more than 7 hours per week. A random sample of 40 students taken from this university showed that they spent an average of 9.50 hours partying the previous week with a standard deviation of 2.3 hours. Test at the 2.5% significance level whether the president’s claim is true. I understand that the null hypothesis is basically what the researcher is claiming. So here I'm stating the null and the alternative hypothesis as: $H_0: \mu=7$ $H_1:\mu\leq7$ I checked the solution for this exercise online and I'm getting: I have a Statistics book that always states the null hypothesis as $\mu$ equals a value and the alternative hypothesis, depending on the claim, as $\mu$ less, greater or different that value. So I'm now confused because in the solution I found they put the in the null hypothesis the claim rather than in the alternative and then in the alternative that $\mu>7$ when that's is not the claim. I know what I have to do after that, but I'm stuck with that because if it is a lower tail then I do not reject the null hypothesis but if it is a right tail I do reject the null hypothesis, so the conclusion would be different.","['statistical-inference', 'statistics', 'hypothesis-testing']"
4276035,Reformulate $x/(x+y)$ using a single $x$ and $y$,Is it possible to reformulate this expression to only list $x$ and $y$ once using common math functions?  The ranges of $x$ and $y$ are both $0$ to $1$ inclusive if that helps. Common being those typically found on a scientific calculator.,['algebra-precalculus']
4276040,Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$,"Problem : If $f(x)$ is continous at $x=0$, and $\lim\limits_{x\to 0} \dfrac{f(ax)-f(x)}{x}=b$, $a, b$ are constants and $|a|>1$, prove that $f'(0)$ exists and $f'(0)=\dfrac{b}{a-1}$. This approach is definitely wrong: \begin{align}
b&=\lim_{x\to 0} \frac{f(ax)-f(x)}{x}\\
&=\lim_{x\to 0} \frac{f(ax)-f(0)-(f(x)-f(0))}{x}\\
&=af'(0)-f'(0)\\
&=(a-1)f'(0)
\end{align} I will show you a case why this approach is wrong: \[f(x)= \begin{cases}
1,&x\neq0\\
0,&x=0
\end{cases}\]
  $\lim_{x\to0}\dfrac{f(3x)-f(x)}{x}=\lim_{x\to0} \dfrac{1-1}{x}=0$ but $\lim_{x\to0}\dfrac{f(3x)}{x}=\infty$,$\lim_{x\to0}\dfrac{f(x)}{x}=\infty$ Does anyone know how to prove it? Thanks in advance!","['calculus', 'derivatives']"
4276056,Intuition behind the method of characteristics,"I have learnt the method of characteristics in a past PDE course, but it has always been taught as a sort of sequence of steps which lead to a solution. This has always bothered me and I would like to understand the geometric picture behind the method and some of the consequences that follow (i.e. how and what it means by a solution propagating along characteristics). I have tried going through a few resources including books by Strauss and Evans, lecture notes such as this one by Stanford , and other posts on this website ( Explaining the method of characteristics ). Despite all this something is still not clicking intuitively. Here is what I have understood so far, let's take the following PDE as an example: $$a(x,y) u_x + b(x,y)u_y = c(x,y)$$ We first observe that the PDE can be written as $$\bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle \cdot \bigl \langle u_x, u_y, -1 \bigr\rangle = 0$$ which tells us that the vector $V = \bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle$ must lie in the tangent plane to the graph/surface $S = \{(x,y,u(x,y))\}$ . Hence we would like to determine $S$ such that for each point $(x,y,z) \in S$ , its tangent plane contains the vector $V$ . This amounts to to finding a curve $\mathcal{C}$ which lies in $S$ . We parametrize $\mathcal{C}$ by a variable $s$ , so that the tangent vector $V$ is now $$V_s = \bigl \langle a\bigl(x(s),y(s)\bigr), b\bigl(x(s),y(s)\bigr), c\bigl(x(s),y(s)\bigr)\bigr\rangle$$ Thus the curve $\mathcal{C} = \{(x(s), y(s), z(s))\}$ results in the following system of ODEs: $$\frac{dx}{ds} = a\bigl(x(s), y(s)\bigr)\\
\frac{dy}{ds} = b\bigl(x(s), y(s)\bigr)\\
\frac{dz}{ds} = c\bigl(x(s), y(s)\bigr)$$ We finally obtain our characteristic curves by solving the above system of equations. Taking the union of all the resulting characteristic curves results in a solution surface for our original PDE. Somehow, after solving a first-order PDE using this method, for example taking the transport equation, it becomes apparent that $z(x,t)$ is constant along the lines $x-at = x_0$ but how? Furthermore, how does this imply that $u(x,t) = z(x,t) = f(x-at)$ ? My questions: Why do we parametrize the curve $\mathcal{C}$ to begin with? What does it mean that characteristic curves propagate discontinuities or solutions? How does one infer this from this picture? I am still unclear what these curves are ultimately telling us. Does it tell us the ""path"" that some Cauchy data travels in the $xyz$ plane? Are there any mistakes in my understanding? The definition of $S$ seems to come out of nowhere, what is the motivation behind using such a surface?","['calculus', 'partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
4276066,"If $A$ and $B$ are orthogonal projection matrices, how can I show that trace$(AB) \le $ rank$(AB)$?","If $A$ and $B$ are orthogonal projection matrices, how can I show that trace $(AB) \le $ rank $(AB)$ ? I was using C-S inequality to get tr $(AB) \le \sqrt{tr(A^2)tr(B^2)}$ and I know that $tr(A^2)=$ rank $(A)$ . But I can't get the rank of $AB$ .","['matrix-rank', 'trace', 'matrices', 'linear-algebra', 'inequality']"
4276090,Matrix congruences,"If A, B are two integer square matrices of the same size such that $A\equiv B\pmod n$ , is $A^p\equiv B^p \pmod{pn} $ for a prime p dividing n?","['matrix-congruences', 'p-adic-number-theory', 'linear-algebra']"
4276120,A n-variable fractional inequality,"Problem: For positive numbers $a_1,a_2,\dots,a_n,$ note that $A=\sum\limits_{i=1}^{n}a_i, \,b_i=A-a_i,\,B=\sum\limits_{i=1}^{n}b_i.$ Prove $$
\frac{\prod\limits_{i=1}^{n}a_i}{\prod\limits_{i=1}^{n}(A-a_i)} \leqslant \frac{\prod\limits_{i=1}^{n}b_i}{\prod\limits_{i=1}^{n}(B-b_i)}
$$ I know this inequality from a friend of mine who claimed the problem came from the Internet. My friend and I tried it for days. It's clear that for $a_1=a_2=\dots=a_n=1$ the equality holds. The first thought is to use Jesen inequality after taking the logarithm. However for $f(x)=\ln\frac{x}{A-x}(A>x)$ we have $f''(x)=-\frac{A(A-2x)}{x^2(A-x)^2}$ which implies zero or one inflection point, so we can't use Jesen inequality directly. Rewriting the inequality as $$
\prod\limits_{i=1}^{n}(B-b_i)  \leqslant \frac{\prod\limits_{i=1}^{n}(A-a_i)^2}{\prod\limits_{i=1}^{n}a_i}
$$ makes the fact that $\prod\limits_{i=1}^{n}(B-b_i)$ is hard to deal clear. I wonder if there would be a nice solution.","['products', 'multivariable-calculus', 'roots', 'inequality']"
4276175,Biconditionals and Conjunctions in Truth Tables,"Given that a biconditional $p\iff q$ is True what can be concluded from the statement $\lnot p\land \lnot q$ ? In a worded example: I wear my running shoes if and only if I exercise. (True) I am not exercising AND I am not wearing my running shoes. (?) If we set up a truth table, the biconditional is True in two of the four occurrences, but we see that $\lnot p\land \lnot q$ is both True and False, which would mean there is no conclusion, correct?","['logic', 'geometry', 'discrete-mathematics']"
4276215,Solve for $x$ in $\sqrt{x+2\sqrt{x+2\sqrt{x+2\sqrt{3x}}}} = x$,"Solve for $x$ : $$\sqrt{x+2\sqrt{x+2\sqrt{x+2\sqrt{3x}}}} = x$$ I tried to substitute $y=x+2$ and then I try to solve the equation by again and again squaring. Then I got equation, $$(y-2)(3y^{14}-(y-2)^{15})=0$$ One solution is $y = 2$ and another is $y = 5.$ (I found $5$ as a solution of the equation by hit and trial method). Therefore, $x = 0$ or $3.$ I'm wondering if there's any  another method to solve it as the repeated squaring step seems to be somewhat absurd.","['nested-radicals', 'algebra-precalculus', 'radical-equations']"
4276288,Proper divisors of $P(x)$ congruent to 1 modulo $x$,"Let $P(x) $ be a polynomial of degree $n\ge 4$ with integer coefficients and constant term equal to $1$ . I am interested in Polynomials $P(x) $ such that for a fixed positive integer $b$ , there are finitely many positive integers $x$ such that $P(x) $ contains a proper divisor $d \equiv1\pmod x$ and $P(x+b) $ contains a proper divisor $d' \equiv1\pmod {x+b} $ . I know addition and multiplication generally do not see each other but out of curiosity, are there any such polynomials $P(x) $ with this property? Take $P(x) = x^4+x^3 +x^2 +x+1$ and $b=2$ . After a long search through all $x < 10 ^5$ , I find that only $x = 102$ satisfies this condition  ( $ P(102) =(15\cdot 102 +1)(700 \cdot 102+1)$ and $ P(104) =(5\cdot 104+1)(2180 \cdot 104+1)$ ).","['prime-factorization', 'modular-arithmetic', 'number-theory', 'elementary-number-theory', 'polynomials']"
4276341,Conditions for the SDE be transitive.,"Let $f:\mathbb R^3 \to \mathbb R^3$ be a smooth Lipschitz function (bounded if needed), and $W_t$ a $3$ -dimentional Brownian motion. Consider the SDE on $\mathbb R^3 \times \mathbb S^2$ (where $\mathbb S^2$ is the sphere). \begin{cases}
\mathrm{d}X_t = f(X_t) \mathrm{d}t + \mathrm{d} W_t\\
\mathrm{d}s_t = \mathrm{d}f(X_t) s_t - \langle s_t, \mathrm{d}f(X_t) s_t \rangle s_t \mathrm{d}t,
\end{cases} where $\mathrm{d}f(x)$ is the derivative of $f$ at $x$ . I would like to know is there any condition on $f,$ which makes the above SDE, transitive on $B(0,1) \times \mathbb{S}^2$ (where $B(0,1)$ is the ball of radius $1$ centered in $0$ ), in the following sense: For every $(x,\theta)\in B(0,1)\times \mathbb S^2$ and open set $U\subset B(0,1)\times \mathbb S^2,$ there exists $t= t(x,\theta,U)$ such that $$\mathbb E\left[\mathbb 1_U (X_t,s_t) \mid (X_0,s_0)= (x,\theta)\right] >0$$ So far, I was not able to figure what condition would imply this property. Can anyone help me?","['stochastic-processes', 'markov-process', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4276413,Confusion in understanding of Steinitz Exchange Lemma by an example,"I am trying to understand Steinitz Exchange Lemma from Kurosh's Group Theory: Suppose in a group $G$ , two finite systems of elements are given: $$
u', u'', \ldots, u^{(k)} \hskip5mm (I) \hskip1cm \mbox{ and } \hskip5mm v', v'', \ldots, v^{(l)} \hskip5mm (II)
$$ the first of which is linearly independent system, each of whose elements is linearly dependent on the second system. Then $k\le l$ and from $(II)$ , $k$ elements can be omitted such that the remaining elements with the elements of $(I)$ form a system equivalent to $(II)$ . My question is simple, but I am not getting my fault in understanding. Suppose we have group $G=\mathbb{Z}\oplus \mathbb{Z}$ , with following systems of elements. $$
(2,0), (0,2) \hskip5mm (I) \hskip1cm \mbox{ and } \hskip1cm (1,0), (0,1)\hskip5mm (II).
$$ It is clear that $(I)$ is linearly independent (their $\mathbb{Z}$ -linear combination is zero only when coefficients are zero). Members of $(I)$ are linearly dependent on $(II)$ (obvious). $k\le l$ (they are equal to two both). As per assertion, we can omit two elements from $(II)$ and replacing them two elements from $(I)$ , we get a system equivalent to $(II)$ ; it means that $(I)$ and $(II)$ should be equivalent. But this is not the case - members of $(II)$ are not $\mathbb{Z}$ -combinations of members of $(I)$ . Can anybody point out my misunderstanding?","['group-theory', 'abstract-algebra', 'abelian-groups']"
4276454,multiplicative extension of the almost complex structure $I$,"I was reading Huybrechts complex geometry book,in page 28-29 there is a linear operator defined as follows $\mathbf{I}: \bigwedge^{*} V_{\mathbb{C}} \rightarrow \bigwedge^{*} V_{\mathbb{C}}$ such that: $$\mathbf{I}=\sum_{p, q} i^{p-q} \cdot \Pi^{p, q}$$ with $\Pi^{p,q}$ is the natural projection on to the form of bidgree $(p,q)$ . Then in Lemma1.2.4,we try to show fundamental form $\omega(x,y)  = <I(x),y>$ over Eucliean vector space $V$ with compatible almost complex structure $I$ is type (1,1).Using the arguement as below: Since $$
(\mathbf{I} \omega)(v, w)=\omega(\mathbf{I}(v), \mathbf{I}(w))=\langle I(I(v)), I(w)\rangle=\omega(v, w)
$$ one finds $\mathbf{I}(\omega)=\omega$ , i.e. $\omega \in \Lambda^{1,1} V_{\mathbb{C}}^{*}$ . I don't really understant the $\mathbf{I}$ ,can someone explain this operator a little bit,why it can used to check the type of $\omega$ ? I know since $\omega = \omega_{0,2}+\omega_{1,1} + \omega_{2,0}$ after taking $\mathbf{I}\omega = -\omega_{0,1} + \omega_{1,1} - \omega_{2,0}$ which means it only has $(1,1)$ component.","['complex-analysis', 'almost-complex', 'linear-algebra', 'differential-geometry']"
4276457,Prove that $\frac{a+b}{c}+\frac{b+c}{a}+\frac{c+a}{b}\ge a+b+c+\dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}$ if $a^2+b^2+c^2+abc=4$.,"Let $a,b,c$ be positive real numbers such that $a^2+b^2+c^2+abc=4$ . How do you prove that $\dfrac{a+b}{c}+\dfrac{b+c}{a}+\dfrac{c+a}{b}\ge a+b+c+\dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}$ ? My Approach: I tried basic algebraic techniques, but they didn't work. So, I moved on to a trigonometric approach. Note that $a^2+b^2+c^2+abc=4$ implies that $a<2$ , and similarly for $b$ and $c$ . So, I can let $\qquad a=2\cos\alpha \qquad b=2\cos\beta \qquad c=2\cos\gamma \qquad 0<\alpha,\beta,\gamma<90^{\circ}$ where $a,b,c$ are side lengths of an acute triangle. These are the lengths of acute triangle because if any one of the angle was to be greater than or equal to $90^{\circ}$ , one of the side would be zero or negative due to our assumption. (By the way, if we put supposed values of $a,b,c$ in L.H.S of the given equation, the result is equal to $4$ .) Now, we need to prove that $\dfrac{\cos\alpha+\cos\beta}{\cos\gamma}+\dfrac{\cos\beta+\cos\gamma}{\cos\alpha}+\dfrac{\cos\gamma+\cos\alpha}{\cos\beta}\ge 2(\cos\alpha+\cos\beta+\cos\gamma)+\dfrac{1}{2}\left(\dfrac{1}{\cos\alpha}+\dfrac{1}{\cos\beta}+\dfrac{1}{\cos\gamma}\right)$ which can be further simplified into $\left(\cos\alpha+\cos\beta+\cos\gamma-\dfrac{1}{2}\right)\left(\dfrac{1}{\cos\alpha}+\dfrac{1}{\cos\beta}+\dfrac{1}{\cos\gamma}-2\right)\ge4$ . We need to prove this inequality given that $\alpha+\beta+\gamma=180^{\circ}$ and $0<\alpha,\beta,\gamma<90^{\circ}$ . I tried to look for triangle inequalities which would help me out but was too overwhelmed and frustrated after going through Wikipedia . After struggling a lot with this question, I'm not even sure if trigonometric approach is the best option. Please help me out!","['contest-math', 'inequality', 'a.m.-g.m.-inequality', 'geometric-inequalities', 'trigonometry']"
4276474,Calculating the volume of a region using double integration,"Question: Calculate the volume of the region in $y > 0$ enclosed by the planes $y = 0, z = 0, z = d − x + y$ and the parabolic cylinder $y=d-{x^2}/d$ , where $d$ is a positive valued constant. I know I'll be using a double integral to find the volume but I'm having a hard time visualising it and I'm not sure what to use for the limits. I think I integrate with respect to $y$ first, but I'm not sure what the limits would be? would they be $y=0$ and $y=d-{x^2}/d$ ? $$ \displaystyle \int \left[\int_{y=0}^{y=d-{x^2}/d} (d-x+y) \,dy \right] ~ dx\, ?$$ Not sure what my $x$ limits would be I was thinking either $-d,d$ or maybe $\sqrt{d},d$ Can anyone confirm whether I'm integrating the right thing and help me with my limits? thank you","['multivariable-calculus', 'multiple-integral', 'volume']"
4276489,"Generalization of $\int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}$","I came across a problem that required proving a specific case and then going on to generalize it. While I have no problem with the first part, I need some confidence from someone about the second part. Here we go. The problem statement : Show that for $\displaystyle 0<\alpha\leq \frac{\pi}{2}$ $$
\int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}\tag{1}
$$ Generalize the result in part (1). My Answer Attempt : Clearly, LHS of $(1)$ represents the length of the curve $f(\theta)=\sin\theta$ from $\theta=0$ to $\theta=\alpha$ and the RHS represents the distance from the origin to the point $(\alpha,\sin\alpha)$ , that is the length of the line from origin to the said point. Curve in red and Line in blue. Since both the curve and the line pass through the origin, and the shortest distance between two points is a straight line, we have $(1)$ proven. The generalization that I've come up with: For any continuous (not necessarily smooth) curve $f(x)$ in $[0,a]$ such that $f(0)=0$ and $a\in\mathbb{R}$ , we have $$\left |\int_0^a \sqrt{1+\big[f'(x)\big]^2}dx\right|\geq \sqrt{a^2+\big[f(a)\big]^2} $$ Equality holding if the curve is a
straight line or $a=0$ . This is basically saying that the shortest distance between two points is a straight line. The points in our case being the origin and a point on the curve $f(x)$ . The curve has to pass through the origin because otherwise the relation may not hold true. Example: The curve $f(x)=3$ in $[0,\infty)$ . Is this good?","['calculus', 'solution-verification', 'definite-integrals']"
4276495,Toss a coin until you get $n$ heads in a row. Expected number of tails tossed?,"You toss a fair coin until you get $n$ heads in a row. What is the expected number of tails you tossed? It can be well shown that the expected number of tosses required to get $n$ heads in a row is $2^{n+1}-2$ So naturally half of this is the expected number of tails? More formally: Let $H_n$ and $T_n$ respectively denote the number of heads and tails you have tossed in total up to time $n$ . $X_n : = H_n - T_n$ is a martinagle. As $X_{n+1} = X_n + C_{n+1}$ where $C_n = \{+1 \text{ if you tossed a head and } -1 \text{ if you tossed a tail on coin } n+1 \}$ And $\mathbb{E}[C_n] = 0 $ for all $n >=1$ Notice $X_0$ is $0$ and the stopping time $\tau$ to yield $n$ consecutive heads is a.s finite (by borell cantelli ) and in $\mathcal{L}_1$ Ergo by OST: $\mathbb{E}[H_{\tau} - T_{\tau}] = \mathbb{E}[X_{\tau}] = \mathbb{E}[X_0] = 0 $ And so $ \star \mathbb{E}[H_{\tau} ] = \mathbb{E}[T_{\tau} ]  $ We know from before that $\mathbb{E}[H_{\tau} + T_{\tau}] = 2^{n+1} -2 $ And so using $\star$ $\mathbb{E}[T_{\tau}] = \frac{1}{2} (2^{n+1} -2 ) = 2^n -1$ Having gone through all this I feel like I used an AK-47 to swat a fly. And that all the martingale theory was not needed. Is there a simply more concise way to prove that once you know the expected number of tosses for something, the expected number of tails will simply be half that?","['expected-value', 'stopping-times', 'probability']"
4276575,Bounds of specific function involving primes,"So i was playing around with factorials and Primes lately and i came up with a (to me) new function, which is: $$\prod_{p \leq x \text{, p is prime}} p^{\lfloor{log_p \lfloor x \rfloor \rfloor}}$$ Now this function is always less than the factorial and it's in some way or another exponential but i am having trouble finding an approximation or any kind of bounds to this function. So I would greatly appreciate some help.","['functions', 'prime-numbers']"
4276602,Show that there exists a strictly increasing function $g$ such that $\sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty$,"Let $f\colon \mathbb{R}^n\to [0,\infty)$ be continuous and radially unbounded, that is, for all $c>0$ , there exists $r>0$ such that if $\|x\|>r$ , then $f(x)>c.$ Assume that for all $x\in\mathbb{R}^n\backslash\{0\}$ , $f(x)>0$ . Prove or disprove that there exists a strictly increasing function $g\colon [0,\infty) \to [0,\infty)$ such that \begin{equation}
\sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty.
\end{equation} Here is my attempt: As $\|x\|\to\infty$ , if $f(x)$ goes to $\infty$ faster than $\|x\|$ then we can prove the claim easily. So I think the difficulty is the case where $f(x)$ goes to $\infty$ slower than $\|x\|$ . For example if $f(x)=\ln(1+\|x\|)$ , then $g(y)=e^y$ satisfy the claim. So I think the claim is true because it seems that we can always increase the growth rate of the denominator using an increasing $g$ but not sure how to prove the claim using these ideas. Any help is appreciated.","['real-analysis', 'multivariable-calculus', 'functions', 'limits', 'supremum-and-infimum']"
4276637,Do the Moufang identities *themselves* imply diassociativity / Moufang's theorem / Artin's theorem?,"A Moufang loop is a loop satisfying the Moufang identities .  Famously, these are diassociative -- the subloop generated by any two elements is associative (is a group) -- and more generally, they satisfy Moufang's theorem, that if any three elements associate, then so do anything they generate (i.e. they generate a group). Separately, if you have an alternative algebra or ring , then multiplication in it also satisfies the Moufang identities, is also diassociative, and also satisfies the analogue of Moufang's theorem. Now, in the case where every nonzero element has an inverse (as in the octonions), you could prove the latter by just appealing to Moufang's theorem for loops.  But in general you can't do that. So, something is going on here -- the Moufang identities, together with inverses, imply Moufang's theorem; and the Moufang identities, together with the existence of an addition operation that our multiplication distributes over, implies Moufang's theorem. It seems really funny to me that in both these cases, these identities imply the same result, but in each case, we need a different auxiliary assumption to make it work. So: Do the Moufang identities themselves imply Moufang's theorem?  That is to say, if we have a magma (and let's say it has an identity because we may as well), and it satisfies the Moufang identities, does it automatically satisfy the analogue of Moufang's theorem, including being diassocative ?  Or is there some counterexample to this? (And if the theorem doesn't hold in this setting, is there some simple additional assumption we could make, that would make it true, while also covering both the cases above?) I'm really wondering about this because this seems like an obvious question to ask, whether we can unify these two settings, but I haven't seen an answer stated anywhere. Thank you all!","['universal-algebra', 'magma', 'quasigroups', 'abstract-algebra', 'nonassociative-algebras']"
4276695,"Let $f(x)\triangleq\ln (1+x)$, and for all integer $n\ge 2$, let $f^n(x)\triangleq f(f^{n-1}(x))$. Calculate $\lim_{x\to\infty\\n\to\infty} f^n(x)=?$","Let $f\colon [0,\infty)\to [0,\infty)$ be defined by $f(x)\triangleq\ln (1+x)$ . For all integer $n\ge 2$ , let $f^n(x)\triangleq f(f^{n-1}(x))$ , where $f^{1}(x)\triangleq f(x)$ . How can we calculate the following limit? \begin{equation}
\lim_{x\to\infty\\n\to\infty} f^n(x)=?
\end{equation} Here is my attempt: Since $\ln x$ is much slower than $x$ , I think this limit is 0 but I do not know how to calculate it.","['limits', 'multivariable-calculus', 'real-analysis']"
4276710,Calculate the measure of the missing angle without trigonometry,"This was from a question that was closed recently. It says to find the measure of angle ' $a$ '. (answer: $60^\circ$ ) The below is my attempt to solve the question. As you'll see I drew $\triangle DAC$ that is congruent to $\triangle CFD$ . And I drew $\triangle CGD$ congruent to $\triangle DBC$ . Also, I drew parallelogram $FGDI$ . But any of these weren't useful to solve find the measure of angle $a$ . I can find the value using trigonometry. But sometimes trigonometry becomes boring so anyone in this community could help me to solve this question without using trigonometry? Thank you.","['euclidean-geometry', 'angle', 'geometry', 'plane-geometry']"
4276717,"If $\lvert X_{ij}\rvert \leq 1$ for all $i, j$, how do we show that the diagonal elements of $(X^T X)^{-1}$ are at least $\frac{1}{n}$?","I am reformulating the question in what I hope will be a simpler, clearer way. However, I am also leaving the original formulation below. Given an $n \times p$ matrix $X$ , if $\lvert X_{ij}\rvert \leq 1$ for all $i, j$ , how do we show that $(X^T X)^{-1}_{j,j} \geq \frac{1}{n}$ for all $j$ ? Original formulation: I have a linear model $Y = X\beta + \epsilon$ where $\epsilon \sim (0_n, \sigma^2 I_n)$ . The matrix $X$ is $n \times p$ . If $\hat \beta$ is the least squares estimator of $\beta$ and $\lvert x_{ij} \rvert \leq 1$ for all $i, j$ , then I want to show that $\text{Var}(\hat \beta_j) \geq \sigma^2 / n$ for $j = 1, \dots, p$ . (I guess the point here is that matrices $X$ with small values lead to large variability in the least squares estimators for $\beta$ .) I know that $\hat \beta = (X^T X)^{-1} X^T Y$ , and $\text{Var}(\hat \beta) = \sigma^2 (X^T X)^{-1}$ . I was thinking that if I had a formula for the diagonal entry of $(X^T X)^{-1}$ that might be handy, but I'm not sure where I am going with that. I was also thinking about the triangle inequality, but I'm not sure how to use it. One consequence of the assumption $\lvert x_{ij} \rvert \leq 1$ is that all entries in $X^T X$ will have absolute value less than $n$ , right? I was also playing with a toy $3 \times 2$ example for X, but that didn't seem to lead anywhere. I appreciate any help. Edit: Response to Hyperplane's answer (NOTE: the answer was deleted) I think it is clear that proving that $(X^T X)^{-1}_{j,j} \geq \frac{1}{n}$ for all $j$ will suffice, because multiplying both sides by $\sigma^2$ will give us the desired inequality. I see that you have argued that all the diagonal elements of $(X^T X)^{-1}$ are trapped between the smallest and largest eigenvalues of $(X^T X)^{-1}$ . In particular, \begin{align*}
(X^T X)^{-1}_{j,j} \geq \frac{1}{\lambda_{\max}(X^T X)}
\end{align*} because eigenvalues are inverted for an inverse matrix. Therefore, if we can show that \begin{align*}
\frac{1}{\lambda_{\max}(X^T X)} &\geq \frac{1}{n}, \text{ or equivalently}\\
n &\geq \lambda_{\max}(X^T X)
\end{align*} we'll essentially be done. (There is an assumption here that $\lambda_{\max}(X^T X) \neq 0$ ; I am not entirely sure how to argue this away.) My main concern is in your second last line. Matrix norms are new to me, but according to Wikipedia , we should have $\lambda_{\max}(X^T X) = \Vert X \Vert_2^2$ , which (unless I'm mistaken) means you've shown that $\lambda_{\max}(X^T X) \leq n^2$ , which is not good enough. I am hoping this is a simple misunderstanding on my part.","['statistics', 'matrices', 'least-squares', 'linear-algebra', 'inequality']"
4276728,Cubic polynomials with distinct integer roots,"Consider the polynomials $x^3+ax^2+c$ and $x^3+cx^2+a$ where $a,c\in\mathbb Z$ . Is it possible that both of the polynomials have three distinct integer roots? If yes, find such $a$ and $c$ such that $\lvert a\rvert+\lvert c\rvert$ is minimum. I'm intuitively thinking that such $a$ and $c$ exists. But I don't have a good reason to convince this to someone. I've done the following: Let the roots of the first polynomial $x^3+ax^2+c$ be $R_1,R_2,R_3$ . By Vieta's formula, we have $$\begin{align} & R_1+R_2+R_3=-a\\ & R_1R_2+R_2R_3+R_1R_3=0\\ & R_1R_2R_3=-c\end{align}$$ Let the roots of the second polynomial $x^3+cx^2+a$ be $r_1,r_2,r_3$ . By Vieta's formula, we have $$\begin{align} & r_1+r_2+r_3=-c\\ & r_1r_2+r_2r_3+r_1r_3=0\\ & r_1r_2r_3=-a\end{align}$$ So we have $$\begin{align} & R_1+R_2+R_3=r_1r_2r_3\\ & R_1R_2+R_2R_3+R_1R_3=r_1r_2+r_2r_3+r_1r_3\\ & R_1R_2R_3=r_1+r_2+r_3\end{align}$$ I'm unable to proceed from here.","['cubics', 'systems-of-equations', 'roots', 'polynomials', 'algebra-precalculus']"
4276730,Algebraic Peter-Weyl in positive characteristic,"To my understanding there is an algebraic version of Peter-Weyl that holds in characteristic $0$ that says for any reductive group $G$ one has that: $$k[G]=\bigoplus V\otimes V^*$$ as a $G\times G$ -representation, where the sum runs over all irreducible representations of $G$ , $V$ . I've heard that such a theorem may hold in positive characteristic, though maybe one needs to adjust to $V$ to a different class of $G$ -modules like Weyl modules. My concern is here that in positive characteristic many results need modified as reductive groups aren't linearly reductive (reps don't decompose into irreps). The proofs I've seen of algebraic Peter-Weyl use linear reductiveness. Does anyone have a reference for this? Is this true/ is there any way to decompose the action of $G$ on it's coordinate ring $k[G]$ when the base field $k$ is positive characteristic? (I'm more than happy to assume $k$ is algebraically closed). Edit: I will mention that the general definition of reductive here is that $G$ is a linear algebraic group with trivial unipotent radical.","['algebraic-groups', 'positive-characteristic', 'reductive-groups', 'representation-theory', 'algebraic-geometry']"
4276786,hard olympiad level matrix problem,"Let a matrix $A\in\Bbb M_n(R) $ , $n>2$ for which exists a number $a\in\Bbb [-2,2 ]$ so that : $A^2 -aA + I_n = O_n$ Prove that for any natural number $m\in\Bbb N$ there exists a unique $a_m\in\Bbb [-2 , 2 ]$ for which $A^{2m}-a_mA^m + I_n = O_n$ . How I tried solving it : I've written $A^2 = aA - I_n$ from which I generalized $A^n$ as the following series : $A^n = x_nA - x_{n-1}I_n$ where $x_1 = 1$ and $x_2 = a$ and where $x_{n+1} = ax_n - x_{n-1}$ after which i applied the characteristic equation and got : $r^2-ar=1=0$ so naturally $r_{1,2}=\frac{a\pm i\sqrt{4-a^2}}{2}$ We can write $r_{1,2} = \cos{t} \pm i\sin{t}$ where : $\sin{t} = \frac{\sqrt{4-a^2}}{2}$ , $\cos{t} = \frac{a}{2}$ and $t\in [0,\pi]$ so $x_n = C_1\cos{nt} + C_2\sin{nt}$ where if we replace $n$ with $1$ and $2$ and solve the system we get $C_1= 0$ and $C_2= \frac{2}{\sqrt{4-a_2} }$ so we easily get $x_n = \frac{2}{\sqrt{4-a_2} }\sin{nt}$ so $A^n = \frac{2}{\sqrt{4-a_2} }$ $[\sin{nt} A - \sin{(n-1)t} I_n]$ Furthermore we have that $A^2 -aA = I_n$ so $A(aI_n-A) = I_n$ so $\det{A}$ can't be equal to $0$ so we pretty much know the value of $A^{-n}$ , can someone help me proceed ?
I've tried to calculate $A^m + A^{-m}$ and got $a_mI_n$ as an answer, if this enough to prove the statement correct ? And if it isn't i could really use a hand here.","['matrices', 'linear-algebra']"
4276826,Integration by parts including delta function,"Often in physics we integrate by parts $$\int_{x_0}^{x_1} f(x) \frac{d}{dx}( \delta(x-y))dx$$ by: $$=[f(x) \delta(x-y)]_{x_0}^{x_1} - \int_{x_0}^{x_1} \delta(x-y) \frac{df}{dx} dx$$ . I have a really simple question, how can we assume that $[f(x) \delta(x-y)]_{x_0}^{x_1}=0$ ? Intuitively the delta function is zero except for at $x=y$ , but what if either $x_0$ or $x_1$ was equal to y? Is the answer simply 'we must assume separately that $x_0,x_1 \neq y$ , or is there something obvious that I'm missing, or is there some measure theory reason why we can say it is zero?","['integration', 'measure-theory', 'definite-integrals', 'dirac-delta', 'calculus']"
4276827,Does $\sin (y) + y = x^3 + x$ determine $y$ as a function of $x$?,"I was teaching my calculus class, and we were learning implicit differentiation. I learned it quite differently when I was at school, but regardless we found that $\sin (y) + y = x^3 + x$ has the derivative $\frac{dy}{dx} = \frac{3x^2 + 1}{\cos (y)+1}$ , and is therefore vertical when $y = (2k+1)\pi$ for all integers $k$ . However, this only occurs locally at a single point, and while we're unable to find a function $y = f(x)$ (or vice versa) that would allow us to find a value of $x$ for the aforementioned value of $y$ , I'm inclined to believe that it's a function. My students were very curious about the idea (even though it was slightly off topic, we were tryin to find the tangent line at $(0,0)$ ) and so I presented as evidence to my guess that it would be a function that the derivative never goes negative, and is only undefined at a point. How might I prove this more rigorously.","['calculus', 'functions', 'implicit-differentiation']"
4276834,$G$ is a compact group. Are 'periodic' functions dense in $C(G)$?,"Assume $G$ is a group, $f$ is a continous real number valued function on $G$ . The $G$ -action on $f$ is given by $(gf)(a)=f(g^{-1}a)$ . Say $f$ is periodic, when $Gf$ is contained in some finite dimensional (real) linear space. For example, let $G=\mathbb T$ , the circle group, then functions like $\sin(n\theta)$ and $\cos(n\theta)$ are periodic, since $(\zeta\sin)(n\theta)=\sin(n(\theta-\zeta))=\sin(n\theta)\cos(n\zeta)-\cos(n\theta)\sin(n\zeta)$ , which is always contained in the linear space generated by $\sin(n\theta)$ and $\cos(n\theta)$ . Therefore, by Fourier series, periodic functions in $C(\mathbb T)$ are dense. The question is, assume $G$ is compact, is the set of periodic functions always dense in $C(G)$ in general?","['group-theory', 'linear-algebra', 'functional-analysis']"
4276838,What sequences do we need to prove continuity,"The continuity of a real function $f$ at the point $x_0$ can be characterized with sequences as $$x_n \to x_0 \implies f(x_n) \to f(x_0) \space \space \forall (x_n)$$ But can we restrict the set of considered sequences to be the ones satisfying $$ \frac{c}{2^{n+1}} \leq |x_{n+1} - x_n| \leq \frac{C}{2^{n+1}} $$ for some positive constants $c$ and $C$ ? If any sequence converging to $x_0$ can be sparsened and/or filled to satisfy this condition, then it works. But is this true?","['continuity', 'sequences-and-series', 'real-analysis']"
4276876,Show that $\sim$ is an equivalence relation.,"Suppose $A,B$ sets and nonempty $X\subseteq A$ . For $f\in B^A$ define $\hat f = f_{|X} \in B^X$ . Define a relation $\sim $ on $B^A$ by $f\sim g \iff \hat f = \hat g$ . Show that $\sim$ is an equivalence relation. If $A,B$ are finite sets, how many equivalence classes are there? How many elements in each class? Denote: $B^A = \{f \text{ a function}\mid f:A\to B\}$ $\hat f = f_{|X} \in B^X$ as function $\hat f(x) = f(x), \forall x\in X$ . To show equivalence relation: $f\sim f \implies \hat f = \hat f \implies \sim$ is reflexive $f\sim g \implies \hat f=\hat g \implies \hat g = \hat f \implies g \sim f \implies \sim$ is symmetric $f\sim g \wedge g \sim h \implies \hat f = \hat g \wedge \hat g = \hat h \implies \hat f = \hat g = \hat h \implies f \sim h \implies \sim $ is transitive. Thus, $\sim$ is an equivalence relation. If $A,B$ are finite sets, then the number of equivalence classes are defined by the number of functions we have $\hat f : X \to B$ ? But how does this make sense? Can someone explain? If this above is the case then we have $|B|^{|X|}$ number of functions $\hat f : X \to B$ and, thus, the number of equivalence classes. (EDIT) We have $A-X$ elements by which two functions $f,g$ differ. Thus the number of functions $f:A-X \to B$ is $|B|^{|A-X|}$ . I am not sure where to proceed from here to characterize the number of elements in each equivalence class.","['equivalence-relations', 'relations', 'functions', 'combinatorics', 'solution-verification']"
4276903,Probability of a flush given that the first two cards are of the same suit.,"there. I'm looking to write probability of a flush (suppose it contains royal flush, straight flush, etc.) given that the first two cards are of the same suit. Here is the solution, let $F$ be the event that a flush appears, $T$ be the event that the first two are of the same suit. $$
\begin{aligned}
\mathbb{P}(F|T) &=\frac{\mathbb{P}(T|F)\mathbb{P}(F)}{\mathbb{P}(T)} \\
&= \frac{\mathbb{P}(F)}{\mathbb{P}(T)} \\
&= \frac{{4\choose 1} {13\choose 5}/{52 \choose 5}}{{4 \choose1} {13 \choose 2}{50 \choose 3}/{52 \choose 5}}
\end{aligned}
$$ I agree with all the above calculation except the last probability $\mathbb{P} (T)$ . I think the correct $\mathbb{P}(T)$ should be $$\frac{{4 \choose 1}{13\choose 2}}{{52 \choose 2}}$$ because the probability that there are two cards of the same suit in a hand is not equal to the probability that the first two cards are of the same suit. Am I right or wrong? Any help is greatly appreciated.","['poker', 'combinatorics', 'probability-theory', 'probability']"
4276935,Infinite and non-abelian fundamental group,"$\require{AMScd}$ I ran into trouble while trying to answer this question. I am trying to prove the following: Suppose $U_1,U_2$ and $U_3 := U_1 \cap U_2$ are open, path-connected subsets of $X = U_1 \cup U_2$ . Suppose that $\pi_1(U_1) \cong \mathbb{Z}/3$ , $\pi_1(U_2) \cong \mathbb{Z}/4$ and $\pi_1(U_3) \cong \mathbb{Z}/2$ . Show that $\pi_1(X)$ is infinite and non-abelian. Using van Kampen, we get the following commutative diagram: \begin{CD}
\mathbb{Z}/2 @>\varphi_1>> \mathbb{Z}/3\\
@V\varphi_2VV  @VV\psi_1V  \\
\mathbb{Z}/4 @>\psi_2>> \pi_1(X) 
\end{CD} Here, $\varphi_1,\varphi_2,\psi_1,\psi_2$ are the homomorphisms induced by the canonical inclusions $U_3 \hookrightarrow U_1$ , $U_3 \hookrightarrow U_2$ , $U_1 \hookrightarrow X$ and $U_2 \hookrightarrow X$ respectively. Now, since $$
\# \text{Hom}_\mathbb{Z}(\mathbb{Z}/2,\mathbb{Z}/3) = \gcd(2,3) = 1,
$$ we must have that $\varphi_1 = 0$ is the zero map. Since the diagram commutes, we also get $\psi_2 \circ \varphi_2 = 0$ . Using group presentation, we can write $\pi_1(U_1) \cong \langle \alpha \mid \alpha^3\rangle$ , $\pi_1(U_2) \cong \langle \beta \mid \beta^4\rangle$ and $\pi_1(U_3) \cong \langle \gamma \mid \gamma^2\rangle$ . Again by van Kampen, we get \begin{align}
\pi_1(X) \cong (\mathbb{Z}/3) *_{\mathbb{Z}/2} (\mathbb{Z}/4) \cong \left\langle \alpha, \beta \mid \alpha^3,\; \beta^4,\; 0=\varphi_2(\gamma)\right\rangle.
\end{align} My problem is that I'm not sure how to continue. Can we say anything about the map $\varphi_2$ ? Since $\# \text{Hom}_\mathbb{Z}(\mathbb{Z}/2,\mathbb{Z}/4) = \gcd(2,4)=2$ there are two possibilities for $\varphi_2$ . One is the zero map again, and the other is the map $0 \mapsto 0$ and $1\mapsto 2$ . In the case that $\varphi_2$ is indeed the zero map, the claim follows immediately. Is there something that we can infer from $\psi_2 \circ \varphi_2=0$ ?","['group-presentation', 'group-theory', 'fundamental-groups', 'general-topology', 'algebraic-topology']"
4276944,What does it mean to divide imaginary numbers exactly?.,"So division implies quantity, and i doesn't have a defined quantitative value (not that i know of at least). So how can we go about dividing complex numbers? Can we say that (a+bi)/(a+bi)=1? If so, can we justify this by referring to some set of complex number properties?",['complex-analysis']
4276997,Does almost sure convergence of random variables depend on their function representation?,"I am reasonably comfortable with probability but the course I am currently taking is approaching it much more formally and I am struggling with the definition of almost sure convergence. I have been given the definition: $X_n \rightarrow X$ almost surely if $P(X_n \rightarrow X \text{ as } n \rightarrow \infty) = 1$ where $\{ X_n \rightarrow X \text{ as } n \rightarrow \infty \}$ is the event $\{\omega \in \Omega: X_n(\omega) \rightarrow X(\omega) \text{ as } n \rightarrow \infty \}$ in the probability space $(\Omega, F, P)$ . If we let $\Omega = [0,1]$ and define $P([a,b]) = b-a$ , then the random variables $X_n(\omega) = 1$ if $\omega \in [0,1/n]$ , $0$ otherwise are a sequence of random variables with $P(X_n = 1) = 1/n, P(X_n = 0) = (n-1)/n$ . $\forall \omega \in (0,1], X_n(\omega) \rightarrow 0.$ $P((0,1]) = 1$ , and so $X_n \rightarrow 0$ almost surely. But if we define $X_n = 1$ if $\omega \in [1 + 1/2 + \cdots + 1/(n-1), 1+1/2+\cdots+1/n]$ where the interval is considered modulo $1$ (so $[3/4,5/4] = [0,1/2] \cup [3/4,1]$ ), then $$P(X_n = 1) = 1/n, P(X_n = 0) = (n-1)/n$$ but $\forall \omega \in \Omega, X_n(\omega) \not \rightarrow 0$ and so $X_n \not \rightarrow 0$ almost surely. This is clearly a contradiction?
It has been shown in my lecture notes that a sequence of random variables $X_n$ with $P(X_n = 1) = 1/n, P(X_n = 0) = (n-1)/n$ does NOT almost surely converge to $0$ , but then I don't understand why my first example is wrong.","['convergence-divergence', 'probability-distributions', 'probability-theory']"
4277028,"Proving ""If there exists a line containing exactly $n$ points, then any line contains exactly $n$ points"" from basic axioms of incidence","I have to show that the following theorem can be proven using the axioms cited below: If there exists a line that contains exactly $n$ points, then any line contains exactly n points, and any point has exactly $n$ lines that contain it. The axioms are the following: There exist at least one line. If L is a line, then there exist at least 3 points on L If L is a line, then there exists one point that is not on L Given any two points P and Q, there exists one and only one line that contains both P and Q If L and M are two lines, then there exists at least one point P that is contained in both L and M. My effort. I have thought about using induction to prove the statement, but I don't know how to start. Can you please provide me an overview of a proof of this theorem? Thank you so much for your time! UPDATE I managed to show that every point is contained in at least $n$ lines, but I have no idea on how to show that it is contained in exactly $n$ lines, which is what the theorem states.","['logic', 'geometry', 'axioms']"
4277030,Different ways to prove $\cos A+\cos B + \cos C=1+4\sin \frac A2 \sin \frac B2\sin \frac C2$,"I would like to find other ways to prove this identity $$\cos A+\cos B + \cos C=1+4\sin \frac A2 \sin \frac B2\sin \frac C2$$ Where $A,B,C$ are angles of a triangle.
My way to prove it is by $A+B+C=\pi$ . By using product to sum formula, $$2\cos \frac{A+B}{2}\cos\frac{A-B}{2}+\cos C=2\sin\frac C2\cos\frac{A-B}{2}+1-2\sin^2\frac C2$$ This can be trivially proved by factoring $2\sin\frac C2$ and a sum to product formula. I would like to see if there's another approach such as Assuming $B=?, C=?$ By cyclic polynomial? Since this is a cyclic identity Or even Vieta's formula?","['alternative-proof', 'trigonometry']"
4277073,Integration by parts involving the delta function,"Often in physics we integrate by parts $$\int_{x_0}^{x_1} f(x) \frac{d}{dx}( \delta(x-y))dx$$ by: $$=[f(x) \delta(x-y)]_{x_0}^{x_1} - \int_{x_0}^{x_1} \delta(x-y) \frac{df}{dx} dx$$ . I have a really simple question, how can we assume that $[f(x) \delta(x-y)]_{x_0}^{x_1}=0$ ? Intuitively the delta function is zero except for at $x=y$ , but what if either $x_0$ or $x_1$ was equal to y? Is the answer simply 'we must assume separately that $x_0,x_1 \neq y$ , or is there something obvious that I'm missing, or is there some measure theory reason why we can say it is zero?",['integration']
4277120,$f(x+y)+f(x-y)= 2x^2-2y^2$. How to find all the solutions to the problem?,Currently I'm learning about functions in my AoPS book. One of the problems is to find all possible solutions to the equation $$f(x+y)+f(x-y)=2x^2-2y^2$$ The book currently explains two methods of solving functional equations. One method is isolation but I don't think that'll work for this equation and another method is to substitute values in for $y$ which might work. My approach: I substituted $y=0$ and got $2f(x)=2x^2$ so i get $f(x)=x^2$ but once I plug my results back into the equation I get $2x^2+2y^2$ so this solution is wrong. Now I'm stuck because this is the only approach I have for this problem. What method should I use to approach this problem and is there even a possible solution?,"['functional-equations', 'algebra-precalculus', 'functions']"
4277213,Doubts in equations involving inverse tangent and inverse sine,"My textbook solves the equation $\arctan x + \arcsin \frac{x}{\sqrt{x^2+9/4}}=\frac{\pi}{4}$ by taking the tangent both sides and using the identity $\tan(\alpha+\beta)=\frac{\tan \alpha + \tan \beta}{1-\tan \alpha \tan \beta}$ ; that approach leads to the solutions $x=-3 \vee x=1/2$ , but $x=-3$ is not a solution since $\arctan(-3)+\arcsin \frac{-3}{\sqrt{(-3)^2+9/4}}=-3\displaystyle\frac\pi4 \ne \displaystyle\frac\pi4$ . My questions are: i) I believe that the reason why there is a mistake (the extra wrong solution $x=-3$ ) is when the author takes the tangent both sides of the equation: the range of the function $f:\mathbb{R} \to \mathbb{R}$ defined by $f(x)=\arctan x +\arcsin \frac{x}{\sqrt{x^2+9/4}}$ is $(-\pi, \pi)$ , so there are values of $f$ where $\tan f$ is not injective, that is for $x\in (-\pi,-\displaystyle\frac\pi2]\cup[\displaystyle\frac\pi2,\pi)$ , and thus, if I'm not wrong, the equation is not equivalent to the one that is obtained by taking tangent both sides because the implication $\tan(r)=\tan(s) \implies r=s$ is valid only where the tangent is injective. Maybe this can be solved by considering the two intervals $(-\pi,-\displaystyle\frac\pi2]\cup[\displaystyle\frac\pi2,\pi)$ , use the periodicity of the tangent to shift $x$ in an interval where the tangent is injective (for example, using $\tan(x+\pi)=\tan x$ for $x\in(-\pi,-\displaystyle\frac\pi2)$ , so that $\tan(x+\pi)$ is now injective for $x\in(-\pi,-\displaystyle\frac\pi2)$ ) and now use again the injectivity to apply tangent both sides. Is this the correct way to solve the equation? Using the injectivity in $(-\displaystyle\frac\pi2,\displaystyle\frac\pi2)$ and then consider other two cases for $(-\pi,-\displaystyle\frac\pi2]$ and $[\displaystyle\frac\pi2,\pi)$ to use again injectivity and consider the union of all the solutions set? ii) The identity $\tan(\alpha+\beta)=\frac{\tan \alpha + \tan \beta}{1-\tan \alpha \tan \beta}$ holds for $\alpha, \beta, \alpha+\beta \ne \displaystyle\frac\pi2+k\pi$ with $k\in\mathbb{Z}$ ; shouldn't I check that $\arctan x=\displaystyle\frac\pi2+k\pi$ , $\arcsin \frac{x}{\sqrt{x^2+9/4}}=\displaystyle\frac\pi2+k\pi$ and $f(x)=\displaystyle\frac\pi2+k\pi$ aren't solutions as well to not lose solutions after using the identity? Of course is obvious that $\arctan x \ne \displaystyle\frac\pi2+k\pi$ and $\arcsin \frac{x}{\sqrt{x^2+9/4}} \ne \displaystyle\frac\pi2+k\pi$ because they are bounded for values in $(-\displaystyle\frac\pi2,\displaystyle\frac\pi2)$ and $[-1,1]$ respectively, to me is not obvious to show that $f(x) \ne \displaystyle\frac\pi2+k\pi$ . Actually, I believe it is not always true: since $f$ is continuous and has image $(-\pi,\pi)$ , it follows that there exist $x_0,x_1$ such that $f(x_0)=\displaystyle\frac\pi2$ and $f(x_1)=-\displaystyle\frac\pi2$ (which are the only two values to check, because for $k \notin \{-1,0\}$ it is $\displaystyle\frac\pi2+k\pi \notin (-\pi,\pi)$ ); however, these are not solutions of $f(x)=\displaystyle\frac\pi4$ and so the equation is equivalent to the one obtained using the trigonometric identity for the angles sum of the tangent. Is this correct?","['algebra-precalculus', 'trigonometry']"
4277233,A characterization of almost sure convergence,"Suppose we have a sequence of positive random variables $X_1,X_2,...,X$ . I am trying to prove a characterization of almost sure convergence. It states that $X_n \rightarrow X$ almost surely iff for every $\epsilon >0$ , $\lim_{n \rightarrow \infty} P[\sup_{k \ge n} \frac{X_k}{X} > 1+ \epsilon]=0$ and $\lim_{n \rightarrow \infty} P[\sup_{k \ge n} \frac{X}{X_k} > 1+ \epsilon]=0$ . If I assume almost sure convergence, then the implication is easy but I am not being able to prove the other way round.","['measure-theory', 'sequences-and-series', 'convergence-divergence', 'probability-theory', 'probability']"
4277288,"all pairs $(p,q)$ of prime numbers which $p>q$ and $\frac{(p+q)^{p+q}(p-q)^{p-q}-1}{(p+q)^{p-q}(p-q)^{p+q}-1}$is an integer.","Find all pairs $(p,q)$ of prime numbers which $p>q$ and $$\frac{(p+q)^{p+q}(p-q)^{p-q}-1}{(p+q)^{p-q}(p-q)^{p+q}-1}$$ is an integer. Does anyone have idea on how to solve this? I did: $(p,q)=(3,2) $ satisfies.
By bounding, we get $p<3q$ $ (p + q)^{p - q} - 1 \le (p + q)^{p - q}(p - q)^{p + q} - 1 \le (p + q)^{2q} - (p - q)^{2q} \le (p + q)^{2q} - 1 .$ Any solution?","['contest-math', 'number-theory', 'elementary-number-theory']"
4277302,Conditional probability and probability space,"I have two exercises where I have got troubles with define probability/measurable space $(\Omega,S)$ . ( $1$ ) We have two dices. One dice is fair, it means that each number from $1,2,3,4,5,6$ falls with probability $\frac{1}{6}$ . The second dice is not fair. The probability of falling 6 is $\frac{1}{2} $ and the probability of falling number from $ 1,2,3,4,5 $ is $\frac{1}{10}$ . Randomly we choose one dice and we roll it $4$ -times. The number $6$ falls twice. What is the probability that chosen dice is not fair. I can compute probablity but I have been trying to build $(\Omega,S)$ . I have been trying somethink like that $$\Omega=\{(D_F; 1,1,1,1),(D_F; 2,1,1,1),(D_F; 1,2,1,1),\dots,(D_F; 6,6,6,5),(D_{F}; 6,6,6,6),(D_{notF}; 1,1,1,1),(D_{notF}; 2,1,1,1),(D_{notF}; 1,2,1,1),\dots,(D_{notF}; 6,6,6,5),(D_{notF}; 6,6,6,6)\},$$ where $D_F$ -fair dice, $D_{notF}$ -not fair dice. I would like to now if I am correct. ( $2$ ) Two shooters, independently of each other, shoot at a common target, one shot each. The probability that the fisrts shooters shoot the target is $\frac{8}{10}$ . The probability that the second shooters shoot the target is $\frac{4}{10}$ . We know that only one hit the target. What is the probability, that the target was hit by first shooter. I think that $\Omega$ can look likes $$\Omega=\{(A_H,B_H),(A_H,B_{notH}),(A_{notH},B_H),(A_{notH},B_{notH})\},$$ where $A_{H}$ - the first shooter hits the target, $A_{notH}$ - the first shooter does not hit the target, $B_{H}$ - the second shooter hits the target, $B_{notH}$ - the second shooter does not hit the target. Probability that the first ( $A$ ) shooter hits target is $\frac{8}{10}$ , it means that $$P\left(\{(A_H,B_H),(A_H,B_{notH})\}\right)=P\left(X\right)=\frac{8}{10},$$ and $$P\left(\{(A_{notH},B_{H}),(A_{notH},B_{notH})\}\right)=P\left(X^c\right)=\frac{2}{10},$$ Probability that the second ( $B$ ) shooter hits target is $\frac{4}{10}$ , it means that $$P\left(\{(A_H,B_H),(A_{notH},B_H)\}\right)=P\left(Y\right)=\frac{4}{10},$$ and $$P\left(\{(A_{notH},B_{notH}),(A_{H},B_{notH})\}\right)=P\left(Y^c\right)=\frac{6}{10},$$ The probability, that the target was hit by first shooter if we know that only one shooter hit formally is $$P(\{(A_H,B_H),(A_H,B_{notH})\}|\{(A_H,B_{notH}),(A_{notH},B_H)\})=P\left(X|Z\right).$$ I would like to compute this probability by using Bayes theorem, but I am little bit confused. I have been trying somthink like that $$P(X|Z)=\frac{P(Z|X)P(X)}{P(Z|X)P(X)+P(Z|X^c)P(X^c)},$$ but I am stuck.","['conditional-probability', 'probability-theory', 'probability']"
4277310,functional equation $f(xy)=\frac{f(x)f(y)}{f(1)}$,"I am looking for functions $f$ (in my case $\mathbb{N}^{*} \rightarrow \mathbb{R}^{+*}$ ) for which the ratio of the images only depends on the ratio of their antecedents : $$\frac{f(n_1.n_0)}{f(n_2.n_0)}=\frac{f(n_1)}{f(n_2)} \forall (n_0,n_1,n_2) \in \mathbb{N}^{*3}$$ Which is equivalent to $$f(n_1.n_2)=\frac{f(n_1).f(n_2)}{f(1)} \forall (n_1,n_2) \in \mathbb{N}^{*2}$$ It is obvious that $f(n)=c.n^m$ ( $c \in \mathbb{R}^{+*}$ and $m \in \mathbb{R}$ ) validate that equation. Can we prove that they are the only solutions ?","['functional-equations', 'functions']"
4277384,Prove that $\theta$ is an irrational multiple of $2\pi$ given $\cos(\theta/2)\equiv \cos^2(\pi/8)$,"How do we prove that $\theta$ is an irrational multiple of $2\pi$ given $\cos(\theta/2)\equiv \cos^2(\pi/8)$ ? With $\operatorname{SU}(2)$ rotations, \begin{align}R_z(\pi/4)R_x(\pi/4)&=[\cos(\pi/8)I-i\sin(\pi/8)Z][\cos(\pi/8)I-i\sin(\pi/8)X]\\
&=\cos^2(\pi/8)I-i[\cos(\pi/8)X+\sin(\pi/8)Y+\cos(\pi/8)Z]\sin(\pi/8)\\
&=\cos(\theta/2)I-i(\hat{n}.\vec{\sigma})\sin(\theta/2)=R_\hat{n}(\theta)\end{align} where $\vec{n}=(\cos(\pi/8),\sin(\pi/8),\cos(\pi/8))$ and $\hat{n}=\frac{\vec{n}}{||\vec{n}||}$ , and $\vec{\sigma}=(X,Y,Z)$ where $X,Y,Z$ are Pauli matrices. Thus $\cos(\theta/2)\equiv\cos^2(\pi/8)$ and $\sin(\theta/2)\equiv\sin(\pi/8)\sqrt{1+\cos^2(\pi/8)}$ . Original Context in my Reference Ref. to Page 196, 214 of QC and QI by Nelsen and Chuang Any hint on the possible ways to approach this could be appreciated. Note : Publication which possibly contains the proof","['field-theory', 'number-theory', 'irrational-numbers']"
4277393,Prove that $\frac{\partial f}{\partial z}$ is a holomorphic function,Let $\Omega \subset \mathbb{C}$ be an open connected subset of $\mathbb{C}$ .Assume that $f: \Omega \rightarrow \mathbb{C}$ is a $C^{2}$ map such that at any point $z \in \Omega$ either $\frac{\partial f}{\partial z}(z)=0$ or $\frac{\partial f}{\partial \bar{z}}(z)=0$ holds. Prove that $\frac{\partial f}{\partial z}: \Omega \rightarrow \mathbb{C}$ is a holomorphic function.(for definition of $\frac{\partial f}{\partial \bar{z}}(z)$ you can see definition of a Holomorphic function ) I try to solve this problem but I'm not sure that my way is right. I first show that if we define $$U=\{ z| z \in \Omega :\frac{\partial f}{\partial z}(z)=0 \} \ \ \ \ \ V=\{ z| z \in \Omega \frac{\partial f}{\partial \bar{z}}(z)=0\}$$ then U and V are open sets from that f is $C^{2}$ .(Is it true?) for points in U we have $\frac{\partial f}{\partial z}(z)=0$ is constant and so holomorphic.for points In $V$ we have $\frac{\partial f}{\partial \bar{z}}(z)=0$ and we have directly from $\frac{\partial f}{\partial \bar{z}}$ that f is holomorphic. then from that f is holomorphic and $C^{2}$ it is easy to see that we have Cauchy–Riemann equations for $\frac{\partial f}{\partial z}$ and so it is holomorphic in $V$ and so it's holomorphic in whole of $\Omega$ . But I think maybe my way is not true because I don't use the connectedness of $\Omega$ and I think maybe we have some problem in $U\cap V$ .,"['complex-analysis', 'analysis']"
4277410,Ordinary Derivative of a Function of a Polynomial,"I want to find higher order derivatives of: $$ f(x)=\frac{1}{4}(x^2-1)^2+K; \quad\quad K=\text{constant} $$ However, I want to find the derivatives with respect to the variable $y$ where we are given the relationship that $x=2y-1$ . My confusion is that following two approaches I am getting slightly different answers. Approach $1$ was to substitute from the start and then calculate the derivatives which gave (after expanding): $$ f(2y-1) = 4y^4-8y^3+4y^2+K  $$ Differentiating and applying the chain rule to the RHS, gives: $$ \frac{df}{dy}(2)=16y^3-24y^2+8y \implies \frac{df}{dy}=8y^3-12y^2+4y $$ My other approach was to calculate the derivatives with respect to $x$ and then use substitution afterwards: $$ f(x)=\frac{1}{4}x^4-\frac{1}{2}x^2+\frac{1}{4} +K $$ $$ \frac{df}{dx} = x^3-x $$ Using the chain rule, we have $\frac{df}{dx}=\frac{df}{dy}\frac{dy}{dx}=\frac{df}{dy}\cdot\frac{1}{2}$ . But this would imply: $$ \frac{df}{dy} = 2\frac{df}{dx}=2(x^3-x)=2[(2y-1)^3-(2y-1)]=16y^3-24y^2+8y $$ I believe I should get the same thing with either approach but I can't tell why they disagree by a factor of $2$ . For reference, what I'm actually interested in is the second and third derivatives of this function. Help would be much appreciated in resolving my mistake.","['calculus', 'derivatives', 'chain-rule', 'substitution']"
4277439,Winning strategy for a game with cubic equation,"The following problem is from USSR $1990$ : The following equation with erased coefficients is written on a blackboard: $$x^3+\dots x^2+\dots x+\dots =0$$ Two players are playing a game. In one move the first player chooses a number and the second player puts it instead of dots into one of the vacant places. After three moves the game is over. Is it possible for the first player to choose three numbers that will secure three distinct integer roots for the equation, no matter how the second player plays? The solution to this problem which I found from a book is as follows: Solution from book: Yes, it is possible. One strategy is as follows. At the beginning the first player chooses $0$ . $\color\green{\text{Case}\ 1}$ : If the second player makes it the constant term of the equation, the equation will be $$x^3+\dots x^2+\dots x=0$$ and the first player chooses successively $2$ and $-3$ to obtain $$x(x-1)(x+3)=0\ \ \text{or}\ \ x(x-1)(x-2)=0$$ $\color\red{\text{Case}\ 2}$ : If the second player puts $0$ as the coefficient of $x^2$ , then the equation becomes $x^3+bx+c=0$ with $b$ and $c$ not fixed yet. The first player chooses the number $\color\red{-(3\cdot4\cdot5)^2}$ and then depending on the move of the second player, either $c=0$ or $b=3^2\cdot4^2-4^2\cdot5^2-3^2\cdot5^2$ . This will result in the equations $$x(x-3\cdot4\cdot5)(x+3\cdot4\cdot5)=0$$ or $$(x+3^2)(x+4^2)(x-5^2)=0$$ $\color\red{\text{Case}\ 3}$ : If after the move of the second player the equation is $x^3+ax^2+c=0$ , the first player chooses $\color\red{6^2\cdot7^3}$ and then either $c=-49$ or $c=-6^8\cdot7^6$ to get the equations $$(x+2\cdot7)(x-3\cdot7)(x-6\cdot7)=0$$ or $$(x-2\cdot6^2\cdot7^2)(x+3\cdot6^2\cdot7^2)(x+6\cdot6^2\cdot7^2)=0$$ In all cases, we got equations with three distinct integer roots and we are done. $\ \ \blacksquare$ I understand the case 1 of the solution. But I don't understand the case 2 and case 3 (which is in red) of the solution, especially how the numbers in the second move (numbers in red) are chosen in these two cases. I know I have to use Vieta's formula. I have seen that choosing the numbers in the second move arbitrarily doesn't work.","['contest-math', 'cubics', 'proof-explanation', 'roots', 'algebra-precalculus']"
4277442,"Can I make a PL-embedding of a 2-disc into a smooth embedding, and also the other way around?","I have a piece-wise linear (PL) embedding of a disc $D^2$ into $\Bbb R^4$ . Now, I want to perform some operation on this disc, but this operation only works with smooth embeddings. So my hope is that I can transition to a smooth embedding, perform the operation, and transition back to a PL-disc. My questions are therefore: if $\phi:D^2\to\Bbb R^4$ is the PL-embedding, is there another embedding of $D^2$ in an $\epsilon$ -neighborhood of $\phi(D^2)$ , that agrees with $\phi$ on $\partial D^2$ and is smooth on the interior of $D^2$ (ideally also with curvature bounded linearly in the distance from $\partial D^2$ )? if $\psi:D^2\to\Bbb R^4$ is an embedding that is smooth on the interior of $D^2$ and PL on $\partial D^2$ , is there a PL-embedding of $D^2$ in an $\epsilon$ -neighborhood of $\psi(D^2)$ that agrees with $\psi$ on $\partial D^2$ ?","['geometric-topology', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4277445,How to get roots in the form of a quadratic from the quartic $x^4 - 4x^2 + 16$?,"So I need to factor this function into quadratics: $$x^4 - 4x^2 + 16$$ I know that there are only complex solutions to this question, however, it is still possible to obtain quadratic factors without requiring the imaginary unit to be present. I tried by simplifying it into a quadratic by replacing $x^2 =a$ . But I just end up getting (after completing the square) $$(x^2-2)^2+12$$ which is not in factored format. So could someone help me?","['roots', 'functions', 'factoring', 'algebra-precalculus', 'quartics']"
4277451,"If $\frac{\cos x+\cos y+\cos z}{\cos(x+y+z)}=\frac{\sin x+\sin y+\sin z}{\sin(x+y+z)}=T$, then $T=\cos(x+y)+\cos(x+z)+\cos(z+x)$",I am having a hard time solving the following problem. Question : Show that if: $$\frac{\cos x + \cos y + \cos z}{\cos (x+y+z)}=\frac{\sin x + \sin y + \sin z}{\sin (x+y+z)} = T$$ then $T=\cos (x+y) + \cos (x+z) + \cos (z+x)$ The question says to use the identity if: $$\frac{P}Q = \frac{R}S$$ then: $$\frac{P}Q = \frac{P+R}{Q+S}$$ Other info : It is worth three marks. I am in my last year of high school. We are expected to know products to sums identities and how to expand compound trig expressions. I've tried decomposing the compound angles and even squaring both fractions before merging them but even that hasn't really helped. (The reason why I considered $T^2$ was because then you would be able to use the Pythagorean identity a few times.) Any assistance would be greatly appreciated.,['trigonometry']
4277452,Limiting conditional probability measure,"$(\Omega, \mathcal{F}, P)$ is a probability triple. Suppose $B_{i} \in \mathcal{F}$ with $P\left(B_{i}\right)>0$ . Let $P_1(A) = P(A|B_1)$ ; define $P_{n}(A)=P_{n-1}\left(A \mid B_{n}\right)$ . What is the probability measure $Q(A)=\lim _{n \rightarrow \infty} P_{n}(A)$ for $A \in \mathcal{F}$ ? (What is the condition if the measure $Q$ exists?) Based on my understanding, $\displaystyle P_n(A) = \frac{P(A \cap(\cap_{i=1}^n B_i))}{P(\cap_{i=1}^n B_i)}$ . If $\{B_i\}_{i=1}^n$ is a decreasing sequence, $B_n \downarrow B$ , $B > 0$ according to the assumption, $P_n(A)$ is either 0 or 1. What about the more general situation? Does the limit exist when both numerator and denominator approach zero? Do we have something similar to L'Hospital's rule to calculate the limit? Edit Not sure whether the following analysis is reasonable or not. Let $C_k = \cap_{i=1}^k B_k$ , $C_k$ is a decreasing sequence; suppose $C_n \downarrow C$ , $C$ is nontrivial. if $A \cap C \neq \emptyset$ , $Q(A) = \frac{P(A \cap C)}{P(C)}$ ; if $A \cap C = \emptyset $ , $Q(A) = 0$ ;","['conditional-probability', 'probability-theory', 'probability']"
4277505,Find a weak solution of the ODE,"Find a weak solution to the following ODE: $u' + u = H_0(x)$ where $H_0(x) = \begin{cases}
0 & x < 0 \\
1 & x \geq 0 \end{cases}$ My professor advised that we try to guess the solution and then verify it. My first guess was naive because I did not know the ""derivative"" (I put quotes here because this isn't really a derivative) of $H_0(x)$ was $\delta_0(x)$ . I thought it was $0$ . I still state this because something consistent is happening. If you do guess $u = H_0$ , we can go ahead and attempt to find the weak derivative. Consider $\phi \in C_{c}^{\infty}$ (continuous functions with compact support) Then, $\displaystyle-\int_{-\infty}^{\infty} \phi'(x)u(x)dx = -\int_{-\infty}^{0} \phi'(x) * 0 dx - \int_{0}^{\infty} \phi'(x) * 1 dx$ Using integration by parts, $\displaystyle-[\phi(x) * 0 |_{-\infty}^{0} + \int_{-\infty}^{0} \phi(x) * 0 dx - [\phi(x) * 1 |_{0}^{\infty} + \int_{0}^{\infty} \phi(x) * 0 dx$ The first term is $0$ due to the multiplication. The third term only leaves the lower limit because $\phi$ is compactly supported. Therefore, I am left with $\displaystyle\boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) * 0 dx$ . This is very close to what I wanted, but I have an extra $\phi(0)$ . After our next lecture, I found out that the weak derivative of $H_0(x)$ does not exist and we need the distribution derivative to make it $\delta_0(x)$ . Therefore, I knew my initial guess was wrong. My next guess was to solve the ODE for both ""components."" What I mean is solve $u'+u = 0$ and $u' + u = 1$ . Just to see if this worked, I first plugged these into wolfram alpha and got $c e^{-x}$ and $c e^{-x} + 1$ respectively. Therefore, my guess was $u = \begin{cases}
ce^{-x} & x < 0 \\
ce^{-x} + 1 & x \geq 1 \end{cases}$ . Now, I attempted to find $u'$ \begin{align}&-\int_{-\infty}^{\infty} \phi'(x) u(x) dx = -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) (ce^{-x}+1) dx \\&= -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) dx\\ 
&= -\int_{-\infty}^{\infty} \phi'(x) ce^{-x} dx - [\phi(x) |_{0}^{\infty} \\ &= -[\phi(x) ce^{-x} |_{-\infty}^{\infty} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x} dx) + \phi(0)\\ 
&= \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x})dx\\ \end{align} . Again. $\phi(0)$ is there, At this point, I asked my professor if he could give me a hint. He told me my initial guess should be the solution to the ODE involving each component, which is exactly what I did. Since I did get the solutions through wolfram alpha,I went ahead and solved both ODEs by hand just to make sure something weird didn't happen. I ended up with $ce^{-x}$ and $1 - ce^{-x}$ respectively. While the second one is slightly different, it shouldn't make a difference because $c$ is a constant, so it could ""absorb"" the $-$ sign. I won't go through the details again, but one will end up with $\displaystyle\boxed{\phi(0)} + \int_{-\infty}^{0} \phi(x) (-ce^{-x}) dx + \int_{0}^{\infty} \phi(x) (ce^{-x}) = \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) u'(x) dx$ where $u'(x) = \begin{cases} 
-ce^{-x} & x < 0 \\
ce^{-x} & x \geq 1 \end{cases}$ . Again, the $\phi(0)$ is still there and I'm not sure how to get rid of it! Does anyone see what I'm doing wrong? Is my initial guess still wrong? Thanks!","['weak-derivatives', 'ordinary-differential-equations']"
4277544,Brownian local time: can we estimate $\mathbb{E}^B\left[\int_{\mathbb{R}}\left|L_{a}(t)-L_{a}(s)\right|^2da\right]$?,"Let $L_a(t),\; (a,t)\in \mathbb R\times [0,T]$ denote the local time of a Brownian motion $B$ . I am interested in the quantity $$\mathbb{E}^B\left[\int_{\mathbb{R}}\left|L_{a}(t)-L_{a}(s)\right|^2da\right]$$ where without loss of generality we can assume that $t\geq s$ . I've found some estimation of the $L^2$ -modulus of continuity of a Brownian local time that states that $$\mathbb{E}^B\left[\int_{\mathbb{R}}\left|L_{a+h}(t)-L_{a}(t)\right|^2da\right]=4th+O(h^2),$$ but can something be said about the expression above? Thanks in advance!","['local-time', 'stochastic-analysis', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4277596,Prove that concave functions are quasi-integrable,"Exercise: Let $\mu$ be a probability measure on $(\mathbb{R},\mathcal B (\mathbb R))$ with $ \int |x| \,d\mu<\infty$ . Let $f:\mathbb R \to \mathbb R$ be concave. Then $\int f \,d\mu$ exists in $[-\infty,\infty)$ . If $f$ is bounded below, then $f$ is $\mu$ -integrable. My attempt: Concave functions are continuous, so $f$ is measurable. From concavity of $f$ we have the inequalities $$f(y)\leq f(x)+f'_+(x)(y-x)$$ $$f(y)\leq f(x)+f'_-(x)(y-x)$$ holding for all $x,y\in\mathbb R$ , where $f'_+,f'_-$ denote the right and left derivatives of $f$ respectively. In particular there exists $a,b\in\mathbb R$ such that $$f(x)\leq g(x):=a+bx$$ for all $x\in\mathbb R$ . Since $x$ is $\mu$ -integrable, so is $g$ . From $f^+\leq g^+$ we get $$\int f^+ \, d\mu \leq \int g^+ \, d\mu<\infty$$ so $\int f \, d\mu=\int f^+ \, d\mu-\int f^- \, d\mu$ is defined in $[-\infty,\infty)$ . If $f$ is bounded below, say $c\leq f$ , then $f^-\leq -\min \{c,0\}$ so $\int f^- \, d\mu<\infty$ as well. Am I missing something? Thanks a lot for your help.","['measure-theory', 'lebesgue-integral', 'convex-analysis', 'probability-theory', 'probability']"
4277603,Eliminating $r$ from $6\tan(r+x)=3\tan(r+y)=2\tan(r+z)$,"Here is the question: If $$\boldsymbol{6\tan (r+x)=3\tan(r+y)=2\tan(r+z)}$$ show that $$\boldsymbol{3\sin^2(x-y)+5\sin^2(y-z)-2\sin^2(z-x)=0}$$ It seems easy but the natural approach goes off the rails.
What I have so far,
Take the two equalities and cross multiply, $$2\sin(r+x)\cos(r+y)=\sin(r+y)\cos(r+x)$$ $$3\sin(r+y)\cos(r+z)=2\sin (r+z)\cos(r+y)$$ then use trigonometric addition formula, $$2\sin(2r+x+y)+2\sin(x-y)=\sin (2r+x+y)+\sin(y-x)$$ $$3\sin(2r+y+z)+3\sin(y-z)=2\sin(2r+y+z)+2\sin(z-y)$$ And get $$\sin(2r+x+y)=3\sin(y-x)$$ $$\sin(2r+y+z)=5\sin(z-y)$$ Now use the sum formula for sine and solve the linear system giving, $$\sin 2r \sin(x-z)=3\sin(y-x)\sin(y+z)-5\sin(z-y)\sin(x+y)$$ $$\cos 2r \sin (x-z) =3\sin(y-x)\cos(y+z)-5\sin(z-y)\cos(x+y)$$ If we square them and add we eliminate $r$ . and get $$\sin^2 (x-z)=9\sin^2(y-x)+25\sin^2(z-y)$$ $$-30\sin(y-x)\sin(z-y)\cos(x-z)$$ But how to go from here ? This doesnt look anything like the desired formula.
Can there be an error in the original problem ? Am I making an error ? Or missing something ?","['algebra-precalculus', 'trigonometry']"
4277616,Arranging $120$ students into $6$ different groups so that the largest and smallest group differ by $2$ members,"In how many different ways can we arrange $120$ students into $6$ groups for $6$ different classes so that the largest group has at most $2$ members more than the smallest group? My initial plan was to use a generating function, but I stumbled across a problem. Let's mark the groups with numbers $1$ to $6$ and let $n_i,i\in\{1,\ldots,6\}$ denote the number of members of the $i-$ th group in some arrangment. To see where this would lead me, for a moment, I assumed $n_1\le n_2\le\cdots\le n_6\le n_1+2$ in hope to find some range $\{m,\ldots M\}$ for $n_i$ 's and use a generating function $f(x)=(x^m+\cdots+x^M)^6$ and find $\langle x^{120}\rangle-$ the coefficient in front of $x^{120}$ , however students are distinct entities and $m$ and $M$ still remained misterious. I then tried figuring out if I was on the somewhat right track by, again  taking $m=\min\{n_1,\ldots, n_6\}$ and write $n_i=m+j_i, j_i\in\{0,1,2\}.$ I believe, an arrangement with $2$ groups of $19, 2$ groups of $20$ and $2$ groups of $21$ people suggests there should be at least $19$ people in each group. I also had a look at this problem: The number of the partition of the set $A$ into $k$ bounded blocks. but, again, I don't have any bounds on the blocks. How should I proceed?","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4277619,Upper Bound on Probability of Maximum Statistic from Exponential Distribution,"Let $X_1,\cdots,X_n$ be I.I.D. Exponential $(\lambda)$ . Let $X_{(n)}$ denote the maximum ordered statistic. Prove that $$P\left(X_{(n)}\geq \frac{2\log(n)}{\lambda}\right)\leq\frac{1}{n}.$$ Work so far: $$
\begin{align} 
P\left(X_{(n)}\geq\frac{2\log(n)}{\lambda}\right)&=1-F_{X_{(n)}}\left(\frac{2\log(n)}{\lambda}\right)\\&=1-\left[F_X\left(\frac{2\log(n)}{\lambda}\right)\right]^n\\&=1-\left[1-\text{exp}\{-2\log(n)\}\right]^n\\&=1-\left[1-\frac{1}{n^2}\right]^n
\end{align} 
$$ From here, I can't think of a clever way to change the RHS to be less than or equal to $\frac{1}{n}$ . Any hints or tips would be much appreciated!","['statistics', 'order-statistics', 'probability']"
4277682,Show that a triangular block matrix (having all block matrices identical) is non diagonalizable,"Let $A$ be a $n$ -square matrix. Consider the upper triangular matrix : \begin{equation} M = \begin{pmatrix} A & A \\ 0 & A \end{pmatrix} \end{equation} I need to show that $M$ is diagonalizable if and only if $A$ is the zero matrix. I have thought of considering the characteristic polynomial of $M$ that equals $\chi_A^2$ . I'm thinking of using the result that a matrix is diagonalizable if and only if its minimal polynomial is of the form $\prod_i (X-\lambda_i)$ where the $\lambda_i$ are distinct, and using the fact that : \begin{equation} P(M)= \begin{pmatrix} P(A) & (XP')(A) \\ 0 & P(A) \end{pmatrix} \end{equation} but I don't really see how.","['matrices', 'diagonalization', 'linear-algebra', 'block-matrices']"
4277731,On the representation of $\sqrt{\pm p}$ in the integral basis of $\mathbb Q(\zeta_p)$,"I took another look at my previous question on proving a certain trigonometric identity related to the braced heptagon : $$\sin\frac\pi7-\sin\frac{2\pi}7-\sin\frac{4\pi}7=-\frac{\sqrt7}2$$ Around the time I posted the first question I was trying to find similar identities for higher primes $p$ in the hope of bracing larger polygons, but without success. When I took the second look it became clear that such identities exist for all odd primes because of the quadratic subfield embedded in every cyclotomic field – the roots of unity naturally correspond with the edges of the polygon to be braced. For example, with $p=11$ $$\frac{\sqrt{-11}-1}2=z+z^3+z^4+z^5+z^9\qquad z=e^{2i\pi/11}$$ and based on this I managed to find a cheap way to remove two degrees of freedom in a braced regular hendecagon using a Moser spindle: This leads in turn to an explicit braced regular hendecagon with 41 vertices and 79 edges , much better than Khodulyov's 155 . ( The proof of rigidity is explained in the docstring of the function generating this graph in Shibuya.) My question here concerns a pattern I noticed when finding the representations of $\sqrt{\pm p}$ in the integral basis of $\mathbb Q(\zeta_p)$ , which is also a $\mathbb Q$ -basis for the field (the sign is of course chosen to ensure containment). #!/usr/bin/env python3
from sympy import *
x, zt = symbols('x zeta')

for p in (3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53):
    quadratic = x*x-p if p%4 == 1 else x*x+p
    zval = exp(2*pi*I/p)
    expr = x - factor(quadratic, extension=zval).args[1].subs(zval, zt)
    print(f""p = {p}: \sqrt{{{x*x-quadratic}}} = {latex(expr)}\\\\"") No matter the $p$ two things seem to always hold: The coefficients in front of each $\zeta_p$ are always $2$ , except the $\zeta^0$ coefficient which is $1$ . If $p\equiv1\bmod4$ (so that we have $\sqrt{+p}$ ) the $\zeta$ -powers occur in pairs $\zeta^i,\zeta^{-i}$ . If $p\equiv3\bmod4$ (so that we have $\sqrt{-p}$ ) exactly one of $\zeta^i,\zeta^{-i}$ occurs for $1\le i\le\frac{p-1}2$ . To illustrate: $$p = 47: \sqrt{-47} = 2 \zeta^{21} + 2 \zeta^{18} + 2 \zeta^{17} + 2 \zeta^{16} + 2 \zeta^{14} + 2 \zeta^{12} + 2 \zeta^{9} + 2 \zeta^{8} + 2 \zeta^{7} + 2 \zeta^{6} + 2 \zeta^{4} + 2 \zeta^{3} + 2 \zeta^{2} + 2 \zeta + 1 + \frac{2}{\zeta^{5}} + \frac{2}{\zeta^{10}} + \frac{2}{\zeta^{11}} + \frac{2}{\zeta^{13}} + \frac{2}{\zeta^{15}} + \frac{2}{\zeta^{19}} + \frac{2}{\zeta^{20}} + \frac{2}{\zeta^{22}} + \frac{2}{\zeta^{23}}\\
p = 53: \sqrt{53} = 2 \zeta^{26} + 2 \zeta^{23} + 2 \zeta^{22} + 2 \zeta^{21} + 2 \zeta^{20} + 2 \zeta^{19} + 2 \zeta^{18} + 2 \zeta^{14} + 2 \zeta^{12} + 2 \zeta^{8} + 2 \zeta^{5} + 2 \zeta^{3} + 2 \zeta^{2} + 1 + \frac{2}{\zeta^{2}} + \frac{2}{\zeta^{3}} + \frac{2}{\zeta^{5}} + \frac{2}{\zeta^{8}} + \frac{2}{\zeta^{12}} + \frac{2}{\zeta^{14}} + \frac{2}{\zeta^{18}} + \frac{2}{\zeta^{19}} + \frac{2}{\zeta^{20}} + \frac{2}{\zeta^{21}} + \frac{2}{\zeta^{22}} + \frac{2}{\zeta^{23}} + \frac{2}{\zeta^{26}}$$ Are the two observations above true for all odd primes $p$ ? If so, how can they be proved?","['algebraic-number-theory', 'cyclotomic-fields', 'geometry', 'integral-basis']"
4277764,Repeated convolution of a pdf of the product of two (independent?) random variables,"Let $p\in \mathbb R$ and $k,n\in \mathbb N$ be fixed constants with $ k << n$ . Let $\bf A$ be a random vector of positive real numbers summing to $p$ , generated as follows: choose $n$ real numbers independently and uniformly between $0$ and $1$ to be the coordinates of $\bf A$ , then normalize so their sum is $p$ . Let $\bf x$ be a random ""mask"" vector of length $n$ , generated as follows. We choose a random subset of $k$ entries of $\bf x$ to be nonzero. Each of the nonzero entries is chosen uniformly in the range $(1,100)$ , independently of each other (and of $\bf A$ ). Question: How can we determine the cumulative distribution function of $\sum_{i=1}^{n} {\bf A}_i{\bf x}_i$ ? Edit: I did a bit of research and found out how to do steps 2 and 3 in the roadmap below ( I plan to do step 3 using Chernoff Bounds to approximate ). Thus, all that is left to do is find the PDF of $A_1$ , but this is really difficult, because I can’t find the point of intersection in order to integrate. Also, even I found the limits of integration, I’m not quite sure how I would take the derivative of such a complex function. I am currently trying to find a solution, but I welcome any input!","['statistics', 'combinatorics', 'convolution', 'probability']"
4277779,Asymptotic integration of $\frac{ x^{a-\frac{1}{2}} \cos \left(\frac{\pi a}{2}-\alpha x\right)}{(e^x-1)\sqrt{\alpha x}}$,"I need to solve this integration for $\alpha\gg 1$ $$\int_0^\infty \frac{ x^{a-\frac{1}{2}} \cos
   \left(\frac{\pi  a}{2}-\alpha  x\right)}{\left(e^x-1\right)
   \sqrt{\alpha  x}}{\rm d}x$$ Using the ""AsymptoticIntegrate"" function in Mathematica I obtained the result $$\int_0^\infty \frac{ x^{a-\frac{1}{2}} \cos
   \left(\frac{\pi  a}{2}-\alpha  x\right)}{\left(e^x-1\right)
   \sqrt{\alpha  x}}{\rm d}x\simeq a \sqrt{\alpha } \zeta (a+1) \left|\sin
   \left(\frac{\pi  a}{2}\right)\right| \Gamma (a)$$ How do I get this result by direct computation using standard identities?","['integration', 'definite-integrals', 'asymptotics']"
4277796,How does differential equations in the space of operator valued functions work? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question In my Quantum Mechanics class, we talked about Schrödinger Equation for the time evolution operator $\hat U(t)$ , $$i\hbar\frac{d\, \hat U(t)}{dt} = \hat H(t)\hat{U}(t),\qquad \hat U(0)=\mathrm{id},$$ with $\hat H(t)$ being some self adjoint operator on a Hilbert space $\mathcal H$ , with the solution $\hat U(t)$ being some unitary operators. And if $\hat H$ is independent of time, then we are given the solution $$\hat U(t) = e^{-i\hat H t/\hbar}.$$ We were told to just plug in and check that this is a solution, and were given no further explanation. I'm just basically not sure how most of this works. More specifically, I have the following questions. How is the derivative defined? My guess is that, we are using the operator norm for the operator, and define the derivative the normal way? If there's an integral, how is it defined? I've only seen lebesgue integral for real and complex valued function, and I couldn't think of a way it could work for operators. How do we know that the solution $\hat U (t)$ will remain unitary, provided the only thing we know is $\hat H(t)$ is self adjoint and $\hat U(0) = \mathrm{id}$ . If $\hat H(t_1)$ commutes with $\hat H(t_2)$ , is it true that the solution is $\hat U(t) = e^{-i\int \hat H(t) \,dt /\hbar}$ ? My intuition says this should be true, as $\hat U(t)$ should always commute with $\hat H(t)$ in this case. How much does the theory of normal differential equation carries over? Do we know that the solution exist? Is it unique? etc. The most general differential equation I've seen is for smooth functions on manifolds, and the one here is obviously very different (the functions aren't even commutative). What are some good references for these (if there are any)? Note: I'm an undergraduate student double majoring in math and physics, and I have taken several graduate courses. So don't expect me to know any of the advanced results, but I'd love to learn about them :)","['ordinary-differential-equations', 'functional-analysis', 'quantum-mechanics', 'physics', 'mathematical-physics']"
4277869,Paths in a square grid with diagonal moves allowed,"How many ways are there for a piece in the bottom-left corner of a 5x5 chessboard to move to the square marked $B$ in the figure below if the piece may only move up, right, and diagonally to the upper-right one square at a time? Attempt: I imagined the board as closed rooms and in each room I placed a door connecting each two rooms. I counted the doors, eliminated the repetitions and gave 75. As I could repeat the path in a perpendicular direction from the first room I added another 75. I Got $76+76=152$ I would like to know the flaw in my reasoning... the answer to the question is 321",['combinatorics']
4277878,centralizer of permutations in Symmetric group,"I would greatly appreciate it if you kindly give me some advice. Let $a$ and $b$ be two distinct permutations in $\text{Sym}(n)$ . When $C_{S_n}(a)=C_{S_n}(b)$ ? I guess either $a$ and $b$ be in the same conjugacy class with the cyclic type $(x)(y)(z\;w)a_0$ where $(z\;w)$ is a transposition and $a_0$ is a product of cycles of size at least three,
or $a=(x)(y)a_1$ and $b=(x\;y)b_1$ where $a_1$ and $b_1$ are conjugate. Moreover, $a_1$ and $b_1$ are the product of cycles of size at least three. Thanks in advance for your consideration.","['permutations', 'permutation-cycles', 'finite-groups', 'symmetric-groups', 'group-theory']"
4277924,Find Consistency Of System Specifications,"1. p ∧ ¬q = T 2. (q ∧ p) → r = T 3.¬p → ¬r = T 4.(¬q ∧ p) → r = T From Eq 1 , we got p = T and q = F Now Apply value of P in Eq 3 , we get: $$\begin{array}{cc}
p&¬p&r&¬r&¬p\to r\\ \hline
\color{red}{\text{T}}&\color{red}{\text{F}}&\color{blue}{\text{T}}&\color{red}{\text{F}}&\color{red}{\text{T}}\\
\color{red}{\text{T}}&\color{red}{\text{F}}&\color{blue}{\text{F}}&\color{red}{\text{T}}&\color{red}{\text{T}}\\
\text{F}&\text{T}&\text{T}&\text{F}&\text{F}\\
\text{F}&\text{T}&\text{F}&\text{T}&\text{T}
\end{array}$$ Now there are two possibilities when  ¬p→r is T, and ¬p is F but the r has two separate values. Is this System consistent or inconsistent?","['logic', 'discrete-mathematics']"
4277973,Problem Uniqueness $-\Delta u + u^3 = 0$,"Let $\Omega \subset \mathbb{R}^n$ open, bounded, connected, and with regular boundary. Assuming that $a(x) \geq 0$ for every $x \in \partial \Omega$ , show that there is at most a regular solution to the problem $$\left\{\begin{array}{rcl}
-\Delta u + u^3 & = & 0, \ \ \mbox{in} \ \Omega\\
\dfrac{du}{d\nu}(x) + a(x)u(x) & = & h(x), \ \ \mbox{on} \ \partial \Omega.
\end{array}\right.$$ Solution Proposal: Suppose there are $u$ and $v$ solutions to the above problem. Now consider $$w = u-v.$$ Since $u$ and $v$ are continuous functions in the compact $\overline{\Omega}$ , then they admit maximum and, consequently, there is $x_0 \in \overline{\Omega}$ such that $$w(x_0) = \max_{\overline{\Omega}} w.$$ Now, let's look at two cases: 1° case: Suppose that $x_0 \in \Omega$ . Since this point is the maximum, we have $\Delta w(x_0) \leq 0$ . Soon, $$u(x_0)^3 - v(x_0)^3 \leq 0 \ \ \Rightarrow \ \ [u(x_0) - v(x_0)][u^2(x_0) + u(x_0)v(x_0 ) + v^2(x_0)] \leq 0.$$ Assertion: $u^2(x_0) + u(x_0)v(x_0) + v^2(x_0) \geq 0$ ; Assume otherwise. Soon, $$0 \leq [u(x_0) + v(x_0)]^2 = u^2(x_0) + 2u(x_0)v(x_0) + v^2(x_0) < u(x_0)v(x_0). $$ As $u^2(x_0), v^2(x_0) \geq 0$ , we have an absurdity. So follows the statement. From the \textit{Assertion}, we have $u(x_0) \leq v(x_0)$ and, consequently, it follows that $w(x) \leq 0$ in $\overline{\Omega}$ since $w(x_0 ) \leq 0$ . Now note that \begin{eqnarray}\label{pv1}
\nonumber
\int_{\partial \Omega} \dfrac{\partial w}{\partial \nu} dS_x + \int_{\partial \Omega} a(x)w(x)dS_x = 0 & \Rightarrow & \int_{\Omega} \Delta w(x) dx = -\int_{\partial \Omega} a(x)w(x)dS_x\\
& \Rightarrow & \int_{\Omega} \Delta w(x) dx \geq 0.
\end{eqnarray} Finally, as it counts \begin{equation}\label{pv2}
\Delta w = [u(x)]^3 - [v(x)]^3 = \underbrace{[u(x) - v(x)]}_{\leq 0}\underbrace{[u^2 (x) + u(x)v(x) + v^2(x)]}_{\geq 0} \leq 0
\end{equation} in $\overline{\Omega}$ , we conclude that \begin{equation}\label{pv3}
\int_{\Omega} \Delta w(x) dx \leq 0.
\end{equation} So we have $\Delta w = 0$ in $\Omega$ . 2° case: Suppose that $x_0 \in \partial \Omega$ . [I couldn't show how in case 1!!!] Finally, as for cases 1 and 2 we have $\Delta w = 0$ in $\Omega$ , it follows that $$u^3 - v^3 = (u-v)(u^2 + uv + v^2) = 0 \ \ \mbox{em} \ \ \Omega.$$ So we have $\bullet$ If $u - v = 0$ in $\Omega$ , then $u = v$ in $\Omega$ ; $\bullet$ If $u^2 + uv + v^2 = 0$ in $\Omega$ , then $$0 \leq (u + v)^2 = u^2 + 2uv + v^2 = uv.$$ Hence, we conclude that $u = v = 0$ in $\Omega$ . Doubts: In this perspective, how to do case 2? Also, as I show $$u = v \ \ \mbox{in} \ \ \partial \Omega?$$","['laplacian', 'analysis', 'partial-differential-equations']"
4277997,Equation of motion through the Lagrangian with Lagrange multipliers,"I ask for advice, cause I'm a little confused. We have such a Lagrangian: $L=\frac{1}{2}m(\dot{x}^2+\dot{y}^2)-\lambda(x+xy+y-1)$ Here $\lambda(x+xy+y-1)$ is the constraint on the phase variables. I need to derive the equation of motion given the constraints and solve them numerically with the help of NDSolve . We do this in accordance with the classic formula: $\frac{d}{dt}(\frac{dL}{d\dot{q}})-\frac{dL}{dq}=0$ Where $q=[x,y]$ are generalized coordinates. I'm not sure about the Lagrange multiplier as a generalized coordinate. Clear[""Derivative""]

ClearAll[""Global`*""]

T = 1/2 m (x'[t]^2 + y'[t]^2);(*Kinetic Energy*)

f = \[Lambda] (x[t] + x[t] y[t] +y[t] - 1);(*Constraint*)

L = T - f;(*Lagrangian*)

D[D[L, x'[t]], t] - D[L, x[t]];

D[D[L, y'[t]], t] - D[L, y[t]];

D[D[L, \[Lambda]'[t]], t] - D[L, \[Lambda][t]]; Question: how are the Lagrange multipliers included in this system when compiling the ODE system and numerically solving it? Maybe this help? https://farside.ph.utexas.edu/teaching/336k/lectures/node90.html https://www.sciencedirect.com/science/article/abs/pii/0045782588900850","['ordinary-differential-equations', 'lagrange-multiplier', 'euler-lagrange-equation', 'numerical-calculus', 'nonlinear-dynamics']"
4278010,An example that Čech cohomology is not equal to derived cohomology with on an affine scheme with Zariski topology.,"There is an example in 3.1.10 $\mathbb{A}^1$ -homotopy theory of schemes demonstrating that the Čech cohomology on an affine scheme with Zariski topology can be different from the derived cohomology. Let $x_1,x_2 \in \mathbb{A}^2$ be two distinct closed points and let $S$ be the semilocal ring at $x_1$ and $x_2$ . Let $C_1,C_2 \subset \mathbb{A}^2$ be two irreducible curves intersecting transversely at $x_1,x_2$ . Let $U= \mathrm{Spec}\, S\backslash(C_1\cup C_2), V =  \mathrm{Spec}\, S \backslash\{x_1,x_2\}$ .  Let $j$ be the inclusion $U \rightarrow \mathrm{Spec}\, S$ . We have the decomposition $$V= (V\backslash(V\cap C_1))\cup (V\backslash(V\cap C_2))$$ whose Mayer-Vietoris sequence give $$0=H^0(V\backslash(V\cap C_1),j_!\mathbb{Z})\oplus H^0(V\backslash(V\cap C_2),j_!\mathbb{Z})\rightarrow H^0(U,j_!\mathbb{Z}) = H^0(U,\mathbb{Z})=\mathbb{Z}\rightarrow H^1(V,j_!\mathbb{Z}).$$ Therefore, $H^1(V,j_!\mathbb{Z})\neq 0$ . The decomposition $$\mathrm{Spec}\, S = (\mathrm{Spec}\, S\backslash x_1 )\cup (\mathrm{Spec}\, S\backslash x_2 )$$ gives a Mayer-Vietoris sequence $$H^1(\mathrm{Spec}\, S\backslash x_1,j_!\mathbb{Z})\oplus H^1(\mathrm{Spec}\, S\backslash x_2,j_!\mathbb{Z})\rightarrow H^1(V,j_!\mathbb{Z}) \rightarrow H^2(S,j_!\mathbb{Z}).$$ Then he conclude that $H^2(S,j_!\mathbb{Z})\neq 0$ . But how can he says that the image of the nonzero element in $H^1(V,j_!\mathbb{Z})\neq 0$ is not zero in $H^2(S,j_!\mathbb{Z})$ ? (He says that this is because $C_1$ and $C_2$ are irreducible. But I don't know how this can leads to the conclusion.)","['algebraic-geometry', 'sheaf-cohomology', 'commutative-algebra']"
4278042,What is the distribution of $T=\sum_{i=1}^{n} e^{-X_{i}}$?,"Let $ \ x_{1},x_{2},...,x_{n}$ a random sample with density $$f(x;\theta) = e^{-(x-\theta)}  e^{-e^{-(x-\theta)}}$$ where $\theta \in \mathbb R$ What is the distribution of $T=\sum_{i=1}^{n} e^{-x_{i}
}$ I try this way: $m_{T}(t)=E(e^{tT})=E(e^{t\sum_{i=1}^{n} e^{-x_{i}
}})=E(\prod_{i=1}^{n} e^{te^{-x_{i}}})= \prod_{i=1}^{n} E(e^{te^{-x_{i}}})= \prod _{i=1}^{n} m_{e^{-x_{i}}}(t)$ And then $m_{e^{-x}}(t)=E(e^{te^{-x}})= \int e^{te^{-x}} \left( e^{-(x-\theta)}  e^{-e^{-(x-\theta)}} \right) dx$ I don´t sure what is the support.","['statistics', 'probability-distributions']"
4278088,Prove certain ring is noetherian.,"Let $R$ be a commutative ring with the property that for each nonzero ideal $I$ and
element $a ∈ I$ there exists a unique ideal $J$ such that $IJ = (a)$ . Show that $R$ is
Noetherian. I am looking for some elementary proof to use really basic ring and generators property to solve the problem. My attempt is to show $J$ is finitely generated and use $IJ=JI$ to show $I$ is finitely generated. I suppose for $a\in I$ , there exists $b_1,b_2,...,b_n$ , such that $\sum_{i=1}^na_ib_i=a$ for some $a_i\in I$ , where $n$ is the minimal number of such kind of summation holds. I decided to show $J=(b_1,b_2,...,b_n)$ such that $IJ=(a)$ . But I couldn't go further, any hint or suggestions are appreciated. Thanks. Edit: The assumption that for all $a\in I$ , there exists a unique $J$ , such that $IJ=(a)$ is used in this proof. If it is not the case, we can't conclude that for the same $a\in I\cap J$ , $IJ=JI=(a)$ .","['ring-theory', 'abstract-algebra', 'commutative-algebra', 'noetherian']"
4278109,"Evaluate two integrals involving $\operatorname{Li}_3,\operatorname{Li}_4$","I need to evaluate $$\int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right )
\ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x.$$ $\operatorname{Li}_n(.)$ denotes the "" Polylogarithms "". Numerical tests derived $$\int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right )
\ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x
=\operatorname{Im}\left[\operatorname{Li}_3\left ( \frac{1+i}{2}  \right )
\right]^2+\frac{1911}{4096}\zeta(3)^2 +\frac{413}{1536}\zeta(3)\ln^32
-\frac{721}{6144}\pi^2\zeta(3)\ln2-\frac{41}{1536}\pi^2\ln^42
+\frac{77}{12288}\pi^4\ln^22+\frac{7}{256}\ln^62.$$ I checked this $$\int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_4\left ( \frac{1+x}{2}  \right )  \right )
\ln^3\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x
=3\operatorname{Im}
\left [ \operatorname{Li}_4\left ( \frac{1+i}{2}  \right ) \right ]^2
-\frac{343\pi^4\operatorname{Li}_4\left ( \frac{1}{2}  \right ) }{49152}
+\frac{693\operatorname{Li}_4\left ( \frac{1}{2}  \right )^2}{256}
-\frac{5\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{256}\ln^42
+\frac{25\pi^2\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{2048}\ln^22
-\frac{117649\pi^8}{2831155200}-\frac{\ln^82}{3072}
+\frac{5\pi^2\ln^62}{12288}-\frac{1061\pi^4\ln^42}{2949120} 
+\frac{343\pi^6\ln^22}{2359296}.$$","['integration', 'improper-integrals', 'calculus', 'contour-integration', 'polylogarithm']"
4278123,Sleepwalking students on a 9x9 checkerboard [duplicate],"This question already has an answer here : Moving counters on a chessboard (1 answer) Closed 2 years ago . Each square of a 9x9 checkerboard is initially slept on by one of 81 students. At noon, each student will wake up and randomly sleepwalk to a valid adjacent square horizontally or vertically (but not diagonally). What is the probability that two or more students end up on the same square?","['statistics', 'combinatorics', 'probability']"
4278147,Where is my error in finding the limit $\lim_{n \to \infty} \cos \frac x 2 \cos \frac x 4 \cdots \cos \frac{x}{2^n}$?,I was trying to solve $$\lim_{n\to\infty} \cos{x\over2}\cos{x\over4}\cos{x\over8}\cdots\cos{x\over2^n}$$ First we can use sine half angle formula from behind and get telescoping series and I got limit $$\lim_{n\to\infty} \cos{x\over2}\cos{x\over4}\cos{x\over8}\cdots\cos{x\over2^n}=\lim_{n\to\infty} \frac{\sin x}{2^n \sin{x\over2^n}}={\sin x\over x}$$ Then I thought using complex number given limit is $$\Re\left[\exp\left(ix\left({1\over2}+{1\over4}+\cdots\right)\right)\right] = \Re[e^{ix}]=\cos x$$ Which answer and solution is correct? What did I do wrong?,"['limits', 'trigonometry', 'complex-numbers']"
4278150,Minimum area ellipse passing through the vertices of an isosceles trapezoid,"An isosceles trapezoid has its four vertices as follows: $A(0, 0), B(10, 0), C(7, 5), D(3, 5)$ . I want to find the ellipse passing through the four vertices and having the minimum possible area.
​
What is the equation of this ellipse ? What I have tried: From symmetry, and orientation of the trapezoid, the center of the ellipse is at $(5, y_0)$ and its equation is $ \dfrac{(x - 5)^2}{a^2} + \dfrac{(y - y_0)^2 }{b^2 } = 1 $ Since $(0, 0)$ is on the ellipse, then $ \dfrac{25}{a^2} + \dfrac{ y_0^2}{b^2 } = 1 $ Since $(3, 5)$ is on the ellipse, then $\dfrac{4}{a^2} + \dfrac{ (5-y_0)^2}{b^2} = 1 $ which are two equation in three unknowns, and they are linear in $\dfrac{1}{a^2} $ and $\dfrac{1}{b^2} $ , hence, it can be solved readily to obtain: $\dfrac{1}{a^2} = \dfrac{ (5- y_0)^2 - y_0^2 }{ 25(5 - y_0)^2 - 4 y_0^2 }$ $\dfrac{1}{b^2} = \dfrac{21}{ 25(5 - y_0)^2 - 4 y_0^2 } $ Therefore, the area of the ellipse is $A = \pi a b = \pi \dfrac{ 25(5- y_0)^2 - 4 y_0^2 }{\sqrt{21} \sqrt{25 - 10 y_0} }$ And now to find the minimum area, I differentiate $A$ with respect to $y_0$ $\dfrac{d A}{d y_0} = 0 $ implies that $  (-50(5- y_0) - 8 y_0)( \sqrt{25 - 10 y_0} ) + \dfrac{5}{\sqrt{25 - 10 y_0} } (625 - 250 y_0 + 21 y_0^2) = 0 $ Multiplying through by $\sqrt{25 -10 y_0} $ , I get, $ (-250 + 42 y_0) (25 - 10 y_0) + 5 ( 625 - 250 y_0 + 21 y_0^2 ) = 0$ Which simplifies to the following quadratic equation, $ 315 y0^2 -2300 y_0  + 3125= 0$ The solutions of which are: $1.804809$ and $5.496778$ The second one is extraneous.  Therefore, $a^2 = \dfrac{ 25(5 - y_0)^2 - 4 y_0^2 }{ (5- y_0)^2 - y_0^2 }$ $b^2 = \dfrac{ 25(5 - y_0)^2 - 4 y_0^2 }{21} $ And these give: $a = 5.902508, b = 3.396089 $ So that the equation of the ellipse is now fully specified. I wonder whether there is a shorter and more direct way to solving this problem.","['calculus', 'conic-sections', 'geometry', 'recreational-mathematics']"
4278155,Is one point compactification functorial?,"In a algebraic topology course, I recently saw the proof of the theorem that the $n-$ and $m-$ dimensional vector spaces, $n\neq m$ , are not isomorphic (in $\mathbf{Top}$ ). The proof used the compactification of both of these spaces (which are $n-$ and $m-$ spheres respectively) and then the reduced homology functor which shows that these are not isomorphic. For this to work it is necessary that the compactification preserves isomorphisms. It was shortly (in elementary terms) explained why that is the case. I wonder now wether the compactification is functorial in general, or wether it just preserves isomorphisms in this specific case. tl;dr: Question is the title.","['general-topology', 'category-theory', 'algebraic-topology']"
4278158,Upper bound of probability of not getting all values in independent draws,"We draw $m$ uniform independent random values among $n$ , with $m\ge n$ . We consider the probability $p(m,n)$ that not all $n$ values have been drawn. We want an upper bound within a constant factor. The probability that a given value was not drawn is $q(m,n)=(1-1/n)^m$ . If these probabilities were independent, we could use $1-(1-q(m,n))^n$ for $p(m,n)$ . However the probabilities are dependent and that expression yields dead wrong values. How can we rigorously derive an upper bound? I needs not be tight, I'd be happy with something within any constant factor. The question's motivation is to prove that $\displaystyle\lim_{n\to+\infty}p(n\left\lceil\log_2(n)\right\rceil,n)=0$ , if possible with an explicit upper bound (towards answering an often-asked question about cryptographic hashes reaching their whole stated destination set, under a random oracle model). This itself is plausible because (as studied in the coupon collector's problem ) the expected number of draws to get all $n$ values is $n(\ln(n)+\gamma)+\frac12+\mathcal O(\frac1n)$ , and $n\left\lceil\log_2(n)\right\rceil$ grows faster than that, by a factor $\frac1{\ln(2)}>1$ .","['coupon-collector', 'probability']"
4278192,What is the motivation and intuition behind the symplectic form?,"That's the definition we got for the symplectic form: Let $$\omega : \: \mathbb{C}^n \times \mathbb{C}^n \rightarrow \mathbb{C}$$ be a bilinear, anti-symmetric and non-degenerate ( $\forall_{y \in \mathbb{C}^n} \: \omega(x,y)=0 \: \Rightarrow \: x=0$ ) map. Let $e_1, ..., e_n$ be the base of $\mathbb{C}^n$ which means, that for $x, y \in \mathbb{C}^n$ we got $x = \sum x_i e_i$ , $y = \sum y_i e_i$ Then, our symplectic form is equal to $$\omega(x,y) = x^T \Omega y$$ My question here is: What does this symplectic form intuitively tell us? How can I understand it in an intuitive way (and not abstract like here)?","['symplectic-geometry', 'geometry', 'matrices', 'bilinear-form', 'differential-geometry']"
4278217,Show that $\left(\frac{x_1^{x_2}}{x_2}\right)^p+\left(\frac{x_2^{x_3}}{x_3}\right)^p+\cdots+\left(\frac{x_n^{x_1}}{x_1}\right)^p\ge n$ for any $p\ge1$,"The inequality $\sqrt{\frac{a^b}{b}}+\sqrt{\frac{b^a}{a}}\ge 2$ for all $a,b>0$ was shown here using first-order Padé approximants on each exponent, where the minimum is attained at $a=b=1$ . By empirical evidence, it appears that inequalities of this type hold for an arbitrary number of variables. We can phrase the generalised problem as follows. Let $(x_i)_{1\le i\le n}$ be a sequence of positive real numbers. Define $\boldsymbol a=\begin{pmatrix}a_1&\cdots&a_n\end{pmatrix}$ such that $a_k=x_k^{x_{k+1}}/x_{k+1}$ for each $1\le k<n$ and $a_n=x_n^{x_1}/x_1$ . How do we show that $$\|\boldsymbol a\|_p^p\ge n$$ for any $p\ge1$ ? As before, AM-GM is far too weak since the inequality $\displaystyle\|\boldsymbol a\|_p^p\ge 2\left(\prod_{\text{cyc}}\frac{x_1^{x_2}}{x_2}\right)^{1/{2p}}$ does not guarantee the result when at least one $x_i$ is smaller than $1$ . We can eliminate the exponent on the denominator by taking $x_i=X_i^{1/p}$ so that $\displaystyle\|\boldsymbol a\|_p^p=\sum_{\text{cyc}}\frac{X_1^{X_2^{1/p}}}{X_2}$ but the approximant approach no longer becomes feasible; even in the case where $p$ is an integer the problem reduces to a posynomial inequality of rational degrees. Perhaps there are some obscure $L^p$ -norm/Hölder-type identities of use but I'm at a loss in terms of finding references. Empirical results: In the interval $p\in[1,\infty)$ , Wolfram suggests that the minimum is $n$ ( Notebook result ) which is obtained when $\boldsymbol a$ is the vector of ones. However, we note that in the interval $p\in(0,1)$ , the empirical minimum no longer displays this consistent behaviour as can be seen in this Notebook result .
The sequence $\approx(1.00,2.00,2.01,3.36,3.00,4.00)$ appears to increase almost linearly every two values, but I cannot verify it for a larger number of variables due to instability in the working precision.","['inequality', 'real-analysis']"
4278230,Kenneth Rosen - Discrete Math (product rule and truth table) - Section 6.1 n. 70,"I was trying to solve an exercise from Kenneth Rosen - Discrete Math's book. The exercise is the following, and it involves using the product rule: Use the product rule to show that there are $2^{2^{n}}$ different truth tables for propositions in $n$ variables. The solution should be something like this: We know that since there are two possible ways of assigning a truth value to a propositional variable, given $n$ propositional variables there are $2^{n}$ possible different assignment of truth values. Then we can ask ourselves how many different truth table can be built starting from those different assignment. A  truth table can be thought of as a function that takes as argument an assignment of truth values and gives as output another truth value. For instance, one truth table for $p,q,r$ can be considered as the function $t1$ that goes from the set of possible assignment for $p,q,r$ of cardinality 8 to the set ${T,F}$ and it is that function that always assign $T$ and never $F$ . A different truth table of the same variables could be the funciton $t2$ that for every possible assignment of truth value to $p,q,r$ it always assign $T$ unless $p$ is false, and so on. So each truth table corresponds to a function from the set of possible different assignment to the set of boolean values ${T,F}$ . Since we know that the set of possible assignment for $n$ propositional variables has cardinality $2^{n}$ and the codomain of the truth table functions is of cardinality $2$ ,then there are $2^{2^{n}}$ possibile ways of building truth tables for $n$ variables. Is it correct to consider truth table for $n$ variables as functions that go from the set of possible assignment to the set ${T,F}$ )","['propositional-calculus', 'combinatorics', 'discrete-mathematics']"
4278244,Why intersection of chords form a cardioid?,"Image from Wikipedia Put equally spaced points in a circle and label them 1,2,3,4,.. and so on.
Connect 1 to 2, 2 to 4, 3 to 6 and generally $n$ to $2n$ . The intersection of these chords will form a cardioid as shown in the above picture. Cardioids can also be made from rolling a circle over other and tracing this point. Image from Wikipedia Why doing these seemingly different things give the same result?
How is these two operations related? Mathologer's YT video on Cardioids and multiplication table Wikipedia article on Cardioid",['geometry']
4278249,Arnold's Topological Proof for the Insolvability of the Quintic: image of $f^\alpha(p)$ as $p$ traverses a commutator loop,"It seems similar questions have been asked about these notes by Leo Goldmakher (linked below) which give a topological proof by Arnold of the insolvability of the quintic by radicals. I think I understand everything except for how to prove exercise 1 on p. 4. For a bit of context, $\mathcal{F}_n$ is used to denote the space of complex polynomials (with leading coefficient equal to 1) without repeated roots. So a polynomial is of the form $z^n+a_{n-1}z^{n-1}+...+a_1z + a_0$ ; it is determined by the coefficients and so such polynomials may be regarded as a subset of $\mathbb{C}^n$ . The roots may be regarded as tuples in $\mathbb{C}^n$ . For example, if we take $n=2$ , we have some quadratics and there is a quadratic formula which gives us roots. However, if you move the coefficients in a large loop (so take a loop in $\mathcal{F}_2$ ) and keep track of the roots, it's possible to permute the roots. This means that the quadratic formula does not always map a loop of $\mathcal{F}_2$ to a loop in $\mathbb{C}^2$ . This is, of course, because square roots aren't really continuous functions on $\mathbb{C}$ . We need branch cuts and the theory of Riemann surfaces originates at this point. Now, here is the exercise. Exercise 1: Suppose $\gamma_1$ and $\gamma_2$ are two loops based at the same point in $\mathcal{F}_n$ , and pick any continuous function $f: \mathcal{F}_n \to \mathbb{C}$ . Then for any $\alpha \in \mathbb{Q}$ the image of $f(p)^\alpha$ as $p$ traverses the commutator loop $[\gamma_1, \gamma_2]$ is also a loop in $\mathbb{C}$ . This exercise seems to be saying there is something special about commutators; I don't think the statement is true for arbitrary loops and it seems to me that the $\alpha$ exponent should give rise to the same issues as we have with square roots not being continuous. So I'm somewhat at a lost on how to prove the statement though I imagine that it should be rather elementary. Any help is appreciated. https://web.williams.edu/Mathematics/lg5/394/ArnoldQuintic.pdf","['galois-theory', 'general-topology', 'polynomials', 'quintics']"
4278252,"How to find the exact value of the integral $ \int_{0}^{\infty} \frac{\sin ^{2n+1} x}{x^{2}} d x$, where $n$ is a natural number?","Let $I(m,n):=\displaystyle \int_{0}^{\infty} \frac{\sin ^{m} x}{x^{n}} d x$ , where $0<n\leq m.$ A month ago, I had found,in my essay , that when $m$ and $n$ are of the same parity and $2\leq n\leq m,$ $$\boxed{\int_0^{\infty} \frac{\sin^{m}x}{x^{n}}dx=\frac{(-1)^{\frac{m-n}{2}} \pi}{2^{m}(n-1) !} \sum_{k=0}^{\left\lfloor\frac{m-1}{2}\right\rfloor}(-1)^{k}\left(\begin{array}{l}
m \\
k
\end{array}\right)(m-2 k)^{n-1}},
$$ In other words, $I(m,n) =\dfrac{a\pi}{b}$ for some natural numbers $a$ and $b$ . However, when $m$ and $n$ are of different parity, $$I(m,n) =\dfrac{a}{b}ln(\dfrac{c}{d})$$ for some natural numbers $a, b, c $ and $d$ . Then I started to investigate whether there is a formula for $I(m,n)$ of different parity. Started from easy, I am going to find, by Frullani’s Integral Theorem ), a formula for, $$I(2n+1,2)=\int_{0}^{\infty} \frac{\sin ^{2n+1} x}{x^{2}} d x
=\frac{2 n+1}{2^{2 n}(-1)^{n-1}} \sum_{k=0}^{n-1}(-1)^{k}\left(\begin{array}{c}
2 n-1 \\
k
\end{array}\right) \ln \left|\frac{2 n+1-2 k}{2 n-3-2 k}\right|.
$$ Proof: $$
\begin{aligned}
I(2 n+1,2) &=\int_{0}^{\infty} \sin ^{2 n+1} x d\left(-\frac{1}{x}\right) \\
&\stackrel{IBP}{=} \int_{0}^{\infty} \frac{(2 n+1) \sin ^{2 n} x \cos x}{x} d x \\
&=(2 n+1) \int_{0}^{\infty} \frac{\sin 2 x}{2 x} \cdot \sin ^{2 n-1} x d x
\end{aligned}
$$ Expressing $\sin^{2n-1}x$ as a linear combination of $\sin(2n-1-2k)$ yields $$\begin{array}{l} \\ \displaystyle \quad I(2n+1,2)\\ \displaystyle =\frac{2 n+1}{2} \int_{0}^{\infty} \frac{\sin 2 x}{x} \left[\frac{1}{2^{2 n-2}(-1)^{n-1}} \sum_{k=0}^{n-1}(-1)^{k} \left(\begin{array}{c}
2 n-1 \\
k
\end{array}\right)\sin (2 n-1-2 k) x \right]dx\\
\displaystyle =\frac{2 n+1}{2^{2 n-1}(-1)^{n-1}} \sum_{k=0}^{n-1}(-1)^{k}\left(\begin{array}{c}
2 n-1 \\
k
\end{array}\right)\int_{0}^{\infty} \frac{\sin 2 x \sin (2 n-1-2 k)x}{x} d x \\
=\displaystyle \frac{2 n+1}{2^{2 n}(-1)^{n-1}} \frac{n-1}{k=0}^{n-1}(-1)^{k}\left(\begin{array}{c}
2 n-1 \\
k
\end{array}\right) \int_{0}^{\infty} \frac{\cos (2 n-3-2 k) x-\cos (2 n+1-2k)x}{x} dx\\
\displaystyle =\frac{2 n+1}{2^{2 n}(-1)^{n-1}} \sum_{k=0}^{n-1}(-1)^{k}\left(\begin{array}{c}
2 n-1 \\
k
\end{array}\right) \ln \left|\frac{2 n+1-2 k}{2 n-3-2 k}\right|\quad \blacksquare
\end{array}$$ (The last step using Frullani’s Integral Theorem ) For examples: $$
\begin{aligned}
\int_{0}^{\infty} \frac{\sin ^{5} x}{x^{2}} d x &=\frac{-5}{16}\left[\left(\begin{array}{c}
3 \\
0
\end{array}\right) \ln 5-\left(\begin{array}{l}
3 \\
1
\end{array}\right) \ln 3\right]=\frac{5}{16} \ln \frac{27}{5}
\end{aligned}
$$ $$
\int_{0}^{\infty} \frac{\sin ^{7} x}{x^{2}} d x=\frac{7}{64}\left[\left(\begin{array}{l}
5 \\
0
\end{array}\right) \ln \left(\frac{7}{3}\right)-\left(\begin{array}{c}
5 \\
1
\end{array}\right) \ln \left(\frac{5}{1}\right)+\left(\begin{array}{l}
5 \\
2
\end{array}\right) \ln \left(\frac{3}{1}\right)\right]= \frac{7}{64} \ln \left(\frac{137781}{3125}\right)
$$ Eureka! I succeeded to find a formula for $I(2n+1,2)$ .  I believe that we can find a reduction formula and use Induction to prove that when $m$ and $n$ are of different parity, $$I(m,n) =\dfrac{a}{b}\ln(\dfrac{c}{d})$$ However, it is difficult to find a general formula for $I(m,n)$ of different parity . Please give me opinions to go further.  Thank you very much!","['integration', 'trigonometry']"
4278256,What functions satisfy $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0?$,"Consider the equation, $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0$ for $n\geq 1$ . By trial and error, I see that $f(x_1,\cdots, x_n)=\sum_{i=1}^{n-1}\frac{x_i}{x_{i+1}}$ works because, $$\frac{\partial f}{\partial x_i}=-\frac{x_{i-1}}{x_i^2}+\frac{1}{x_{i+1}}, \text{ where }x_0=0,x_{n+1}=+\infty.$$ Then $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i}= \sum_{i=1}^{n}-\frac{x_{i-1}}{x_i}+\frac{x_i}{x_{i+1}}=0.$ But is there a general form of solutions perhaps in higher dimensions say $n\geq 3.$ I guess that when $n=2$ , we have $f(x_1, x_2) = f\left(\frac{x_1}{x_2}\right),$ but not sure how to express solutions in higher dimensions. Thus any suggestions/remarks will be much appreciated. Edit : In general, I want to find functions $f$ such that $x\cdot \nabla f=0$ and $v\cdot \nabla f =0$ where $x=(x_1,\cdots, x_n)$ and $v = (v_1,\cdots, v_n)$ is some constant vector in $\mathbb{R}^n.$","['ordinary-differential-equations', 'analysis', 'real-analysis', 'calculus', 'partial-differential-equations']"
4278354,Support of a Weil divisor equals to support of Cartier divisor?,"Sorry for my bad English. Let $X$ be locally fractional integral Noetherian scheme on which we can identify Weil divisor and Cartier divisor (cf. Hartshorne II.Prop.6.11). If $D$ is a Cartier divisor, we define support of $D$ as Supp $D:=\{x\in X| D_x\neq 1\}$ where $D_x$ is stalk at $x$ of $D\in {\scr K}^*/{\scr O}^*_X$ . On the other hand, we can  think $D$ as a Weil divisor, so $D=\sum_{i} n_i Y_i$ where $Y_i$ are prime divisors and $n_i\in \mathbb{Z}\backslash \{0\}$ . Now Supp $D=\bigcup_i Y_i$ ? Please help me, thanks.","['divisors-algebraic-geometry', 'algebraic-geometry']"
4278365,Expectation of Brownian motion at a stopping time depending on a brownian motion,"I have difficulties with a very concret example in stochastic calculus. Let $B$ and $W$ be two independent Brownian motions on a filtration $(F_t)_{t\geq0}$ and let be $\lambda = 1 + \exp(-B^2_1)$ a stopping time. Compute $\mathbb{E}[B_\lambda]$ , $\mathbb{E}[B_\lambda^2]$ , $\mathbb{E}[W_\lambda]$ and $\mathbb{E}[W_\lambda^2]$ . I firsted started to write that $\lambda$ is a stopping time so $\mathbb{E}[W_\lambda]=\mathbb{E}[W_0]=0$ and $\mathbb{E}[W_\lambda^2] = \lambda$ by definition of the brownian motion but I have really no intuition when computing the expectation of the brownian motion B at time $\lambda$ because it depends on $B_1$ so it can't be the same result than for $W$ .","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4278368,Solving an equation involving a matrix exponential,"Suppose we have unknown scalars $x_1, x_2, ...,x_m \in \mathbb{R}$ ,  known matrices $A_1, A_2, ...,A_m \in \mathbb{R}^{n\times n}$ , and two known vectors $s_0, s_1\in\mathbb{R}^n$ . I want to find $x_1, x_2, ...,x_m$ that satisfy the following equation: $$s_1 = \exp\!\left(\sum_{i=1}^{m}x_iA_i\right)\!s_0$$ I know that there can be no solutions, a finite number of solutions, or an infinite number of solutions. I would like to find all of the solutions that exist. The matrices $A_i$ don't have a special form or property. If you have recommended books on the topic that would be extremely helpful. I am also interested in ways to numerically find all of the solutions.","['vectors', 'ordinary-differential-equations', 'matrix-exponential', 'matrices', 'lambert-w']"
4278372,"$f: \Omega \to \Omega_1 \times \Omega_2$ is measurable if and only if $f_1,f_2$ are measurable","Following this question , I was suggested to give a try on this theorem. Could you check if my proof is fine? Let $(\Omega, \mathcal F)$ , $(\Omega_1, \mathcal F_1)$ , and $(\Omega_2, \mathcal F_2)$ be measurable spaces. $\mathcal F_1 \otimes \mathcal F_2$ the product $\sigma$ -algebra of $\mathcal F_1$ and $\mathcal F_2$ . $\rho_1$ and $\rho_2$ the projection maps from $\Omega_1 \times \Omega_2$ to $\Omega_1$ and $\Omega_2$ respectively. $f_1 = \rho_1 \circ f,f_2 = \rho_2 \circ f$ the first and second coordinates of $f$ . Then $f: \Omega \to \Omega_1 \times \Omega_2$ is measurable if and only if $f_1,f_2$ are measurable. My attempt: By construction of $\mathcal F_1 \otimes \mathcal F_2$ , $\rho_1$ and $\rho_2$ are measurable. If $f$ is measurable, then $f_1$ is the composition of $2$ measurable functions and thus is measurable. With similar argument, $f_2$ is measurable. Now we prove the converse direction. Assume $f_1$ and $f_2$ are measurable. Because $\mathcal F_1 \otimes \mathcal F_2 = \sigma (\mathcal F_1 \times \mathcal F_2)$ , we just have to verify that $f^{-1}(A \times B) \in \mathcal F$ for all $A \in \mathcal F_1$ and $B \in \mathcal F_2$ . In fact, $$\begin{aligned}f^{-1}(A \times B) &= \{ \omega \mid f(\omega) \in A \times B\} \\ &= \{ \omega \mid f_1(\omega) \in A \text{ and } f_2 (\omega) \in  B\} \\ &= f_1^{-1} (A) \cap f_2^{-1} (B) \in \mathcal F.  \end{aligned}$$ This completes the proof. Update: I added a lemma to justify that it's enough to prove $f^{-1}(\mathcal{F}_1 \times \mathcal{F}_2) \subseteq \mathcal{F}$ . Lemma: Let $\left(\Omega_{1}, \mathcal{F}\right)$ and $\left(\Omega_{2}, \sigma(\mathcal{C})\right)$ with $C \subseteq \Omega_2$ be two measurable spaces. A function $f: \Omega_{1} \rightarrow \Omega_{2}$ is measurable if $$A \in \mathcal{C} \quad \text {implies} \quad f^{-1}(A) \in \mathcal{F}.$$ Proof: Consider $\mathcal X = \{A \subseteq \Omega_2 \mid f^{-1}(A) \in \mathcal{F}\}$ . We have $f^{-1}(A^c) = (f^{-1}(A))^c$ and $f^{-1}(\cup_n A_n)=\cup_n f^{-1}(A_n)$ . This means $\mathcal X$ is a $\sigma$ -algebra over $\Omega_2$ . Moreover, $\mathcal C \subseteq \mathcal X$ . By definition of $\sigma(\mathcal{C})$ , we obtain $\sigma(\mathcal{C}) \subseteq \mathcal X$ . Hence $f$ is measurable.","['measure-theory', 'solution-verification']"
4278393,How to prove $\frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4}$ if $a+b+c=3$,"$a,b,c\ge 0,a+b+c=3.$ Prove: $$\frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4}$$ This  problem was found in this post . As you can see, no one in that post gave a correct proof but someone pointed out that this inequality might be a problem from Mathematical Reflections . However, the archive of the journal can't be downloaded. Therefore, we must find our own solution. I tried it myself surely. And because all variables show up in a single fraction, we are hard to use technique like tangent line. I tried to homogenise it, and it turned out to be (in case you didn't understand, this triangle denotes the coefficients of ever term of the polynomial. From the left-top is coefficient of $a^7$ , and right-top is that of $b^7$ , and the bottom is that of $c^7$ . And for instance, the "" $762$ "" on the second term of the second line denotes the coefficient of $a^5bc$ )
which is almost impossilbe to create a proof directly from it by hand.(Sure, since expanding is not always a perfect way) Can you come up with a solution with wit?","['summation', 'buffalo-way', 'alternative-proof', 'multivariable-calculus', 'inequality']"
4278422,Differentiation under expectation $f(x)=E[|x-Y|].$,"Let $Y$ be a random variable with zero points mass which is integrable. Let x be in $[-M,M]$ for some $m\in\mathbb{r}$ . Consider $$f(x)=E[|x-Y|].$$ I want to take the derivative of $f(x)$ in a point $x_0\in(-M,M)$ . Here is my attempt of a solution. Using dominated convergence with majorant $|Y|+|M|$ , and that $Y$ has zero point mass in $x_0$ I obtain that $$f'(x_0)=\lim_{h\rightarrow 0}\frac{f(x_0)-f(x_0+h)}{h}=\lim_{h\rightarrow 0}E[\frac{|x_0-Y|-|x_0+h-Y|}{h}]=\lim_{h\rightarrow 0}E[\frac{|x_0-Y|-|x_0+h-Y|}{h}1_{Y\not=x_0}]=E[\lim_{h\rightarrow 0}\frac{|x_0-Y|-|x_0+h-Y|}{h}]=E[-1_{x_0-Y<0}+1_{x_0-Y>0}]=2P(Y\leq x_0)-1.$$ Is the above argument correct hence in the above setting it is correct that $f'(x_0)=2P(Y\leq x_0)-1$ ? And if it is correct doesn't it also hold in general that $f(x)$ is differentiable on $(-M,M)$ I have tried for $U$ uniform([0,1]) and $x_0\in(0,1)$ where i get $E[|x-U|]=x^2-x+1/2$ which differentiated is $2x-1$ which is equal to $2P(U\leq x)-1$ .","['integration', 'statistics']"
