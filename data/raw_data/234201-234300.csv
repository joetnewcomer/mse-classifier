question_id,title,body,tags
4891178,Discontinuous solution of $y''(x)-2(1-x)(y'(x))^2=0$ with $y(0)=1$ and $y(2)=-1$,"I tried to solve the equation $$y''(x)-2(1-x)(y'(x))^2=0$$ with the conditions $$y(0)=1, y(2)=-1.$$ It's easy to verify that the function $$y(x)=\frac{1}{1-x}$$ satisfies both the equation and the conditions. The problem is the function is discontinuous at $x=1$ while what I know -May be wrong- is the solution of differential equation is always continuous!. So I tried to solve it using Mathematica and here is the result which is a perfect continuous solution. Does the problem have more than one solution? Is the discontinuous solution wrong? Is Mathematica wrong? Did I miss something? Edit: I worked the problem manually by the method of separation and obtained two distinct solutions: $$y(x)=\frac{1}{1-x}$$ and $$y(x)=\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{x-1}{\sqrt{\frac{1}{c}-1}})+\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{1}{\sqrt{\frac{1}{c}-1}})+1$$ where $$c=-2.2767175312280725973119838$$ The question now is:
Why should I consider the discontinuous function to be NOT solution of the differential equation??","['ordinary-differential-equations', 'initial-value-problems', 'real-analysis', 'continuity', 'boundary-value-problem']"
4891211,Relationship between the squares of first n natural numbers and first n natural odd numbers.,"Here's a question from high school mathematics. If $ 1^2 + 2^2 + 3^2 + 4^2 + 5^2 + \dots + 100^2 = x $ , then
( $1^2 + 3^2 + 5^2 + \dots + 99^2$ ) is equal to ? Options were: (a) $\frac{x}{2}-2525$ (b) $\frac{x}{2}+5050$ (c) $\frac{x}{2}-5050$ (d) $\frac{x}{2}+2525$ Is there a way to solve this question without using the formula for the summation of the squares of the first $n$ natural numbers.
I am definitely sure there's an easy way to do this without using the formula - $\frac{n(n+1)(2n+1)}{6}$ or those other summation formula for summing up the first $n$ odd/even natural numbers. The correct answer is (a).
I was able to solve(wrong) it by using the formula for the sum of square of first n natural numbers that I found online.
Here's my solution using those formulas: $$x = \frac{n(n+1)(2n+1)}{6} $$ $$x = {100 * 101 * 201 \over 6} = 338350$$ $${x\over 2} = {338350\over2}$$ $${x\over 2} = 169175$$ $$ e = 2^2 + 4^2 + 6^2 + 8^2 + 10^2 + \dots + 100^2$$ $$ e = {2n(n+1)(2n+1)\over3} $$ $$ e = {2*100(101)(201)\over3}$$ $$ e = 166650 $$ $$ {x\over2} - e = {x\over2} - e $$ $$ {x\over2} - e = 169175-166650 $$ $$ {x\over2} - e = 2525$$ $$ {x\over2} - 2525 = e$$ We know that sum of square of n natural numbers - sum of square of n natural even numbers = sum of square of n natural odd numbers. $$ x - ({x\over2} - 2525) = 1^2 + 3^2 + 5^2 + 7^2 + 9^2 + \dots + 99^2 $$ $$ {x\over2} + 2525 = 1^2 + 3^2 + 5^2 + 7^2 + 9^2 + \dots + 99^2 $$ But the correct answer should be ${x\over2}-2525$","['sums-of-squares', 'arithmetic-progressions', 'sequences-and-series']"
4891228,A Hölder bound on an integral involving the complex exponential.,"I want to show that if $\gamma \in (0,1)$ we have: $$\int_{\mathbb R^n}\frac{|e^{i x \cdot \xi}-e^{iy\cdot \xi}|^2}{|\xi|^{n+2\gamma}}\,\mathrm d \xi \le C_{n , \gamma} |x-y|^{2 \gamma}$$ for some $C_{n,\gamma}>0$ only dependent on $n, \gamma$ . Some ideas: We can use the fact $f(z)=e^{iz}$ has complex derivative of modulus $\le 1$ and Cauchy-Schwarz to bound the numerator. Hölder/Cauchy Schwarz inequalities in the relevant $L^p$ spaces come to mind, as does splitting up the integral into a near $0$ and tail term and bounding contributions. None of these seem to give bounds of the right form from what I have tried however.","['functional-inequalities', 'functional-analysis', 'analysis']"
4891248,ODE theory - conditions for differentiating an ODE,"I’m trying to find out more about solving ODEs, spurred on by my previous question about solving $(x’(t))^2+a(x(t))^2+b=0$ with say $x(0)=1,x(2)=3$ . In this case differentiating reveals the equation can be reduced to $x’’(t)+2ax(t)=0$ . So my question is can you always apply the derivative to any $x’’(t)=f(t)$ ? Can it introduce solutions that don’t solve the original equation? Is there a way to generalise this for other differential equations? I tried to think about it with an example like $u(x)=\sin(u(x))\implies u’(x)=u’(x)\cos(u(x))\implies \cos(u(x))=0$ so in this case it seems to work. Is there a chapter in a textbook where I can learn more about this?",['ordinary-differential-equations']
4891303,Showing that certain polynomials defined recursively have simple roots.,"Let $f_0(z)=z$ , and define recursively $f_{i}(z)=zf_{i-1}(z)-f_{i-1}(z)+1$ , for all $i \geq 1$ . I would like to show that $f_i(z)$ has at least a simple root (this should be true), for any $i$ . I can see this if the degree of $f_i$ is odd, and in that case the simple root would be $z=0$ . Maybe this is a silly question, but I'd need it to find a reduced point in a certain scheme. For this reason, I will also insert ""Algebraic Geometry"" in the tags.","['algebraic-geometry', 'abstract-algebra', 'roots', 'polynomials']"
4891340,Prove that $\mu_*\leq\mu^*$ where the definition of an inner and outer measure is induced by a measure,"I know that similar question has been asked and answered here , here , and here . But I am looking for a different proof based on different definitions. We have the following definition of an outer and inner measure indeuced by a measure: Definition $\quad$ Let $(X,\mathcal{A})$ be a measurable space, let $\mu$ be a measure on $\mathcal{A}$ , and let $A$ be an arbitrary subset of $X$ . Then $\mu^*(A)$ , the outer measure of $A$ , is defined by \begin{align*}
    \mu^*(A) = \inf\{\mu(B):A \subseteq B\ \text{and}\ B \in \mathcal{A}\},
\end{align*} and $\mu_*(A)$ , the inner measure of $A$ , is defined by \begin{align*}
    \mu_*(A) = \sup\{\mu(B):B \subseteq A\ \text{and}\ B \in \mathcal{A}\}.
\end{align*} I need to prove that $\mu_*(A) \leq \mu^*(A)$ holds for each subset $A$ of $X$ . This is how I would like to proceed and where I got stuck: Assume to the contrary that $\mu_*(A) > \mu^*(A)$ for some $A \in X$ . Then there is a $B \in \mathcal{A}$ such that $A \subseteq B$ and $\mu(B) < \mu_*(A)$ . From here, I wanted to say that there is a set $C \in \mathcal{A}$ such that $C \subseteq A$ and $\mu(C) > \mu(B)$ , which leads to a contradiction because $C \subseteq A \subseteq B$ implies that $\mu(C) \leq \mu(B)$ . However, I do not know how to state rigorously that such a set $C$ exists. Could anyone please help me out? Reference: Measure Theory by Donald Cohn Page 33.","['measure-theory', 'outer-measure', 'analysis', 'real-analysis', 'supremum-and-infimum']"
4891381,Geometric approach to minimize inner product,"Problem : Let $S$ be a surface $$S : z = \frac{x^2}{4} + \frac{y^2}{2} + 1$$ For two point $Q(x,y,z)$ on $S$ and $P(2,0,1)$ , evaluate $$\min \frac{\mathrm{OP}\circ\mathrm{OQ}}{|\mathrm{OQ}|}$$ My Attempt : Expand $\mathrm{OP}\circ\mathrm{OQ} = |\mathrm{OP}||\mathrm{OQ}|\cos\theta$ then we get $$\frac{\mathrm{OP}\circ\mathrm{OQ}}{|\mathrm{OQ}|} = |\mathrm{OP}|\cos\theta$$ And we know this is related to projection $\mathrm{OP}$ to $\mathrm{OQ}$ . But this made me harder because in this problem, $\mathrm{OQ}$ varies, not $\mathrm{OP}$ . If $\mathrm{OQ}$ fixed, find normal line of $\overline{\mathrm{OQ}}$ which is tangent of $S$ and do projection to it then I can get minimum (or maximum) easily, But this case is different. Is there any nice approach to avoid lagrange multiplier with three variables?","['optimization', 'multivariable-calculus', 'calculus', 'geometry']"
4891387,How to show IVP has unique solution,"Given $$
y' = \frac{ty+y}{ty+t}, \ 2\leq t\leq 4, \ y(2) = 4
$$ How do we show the solution and that it is unique. I have been able to get closer to a solution but I have not been able to isolate the $y$ . Also how can I show this has a unique solution, all the theorems I know of cannot be applied to this which has made me stuck for quite a while now.","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
4891396,Are cumulants the only additive functions of independent random variables?,"For a random variable $X$ , the cumulant generating function $CGF_X$ is defined as $CGF_X(t)=\log Ee^{tX}$ , and the nth cumulant $k_n(X)$ is defined as the coefficient of $t^n/n!$ in the corresponding power series. The cumulant $k_n$ has the following properties: $k_n(X+Y)=k_n(X)+k_n(Y)$ if X and Y are independent (additivity) $k_n(cX)=c^nk_n(X)$ for any scalar $c$ (homogeneity) $k_n(X)=p_n(EX, EX^2,\dots, EX^n)$ where $p_n$ is a universal polynomial (i.e. does not depend on X) Now suppose I have some other function $k'$ that satisfies properties 1-3. Is k' necessarily a scalar multiple of $k_n$ ? Motivation: The higher order cumulants can be somewhat mysterious-having a characterization like the above would make them seem much more natural. Alternatively, it would be interesting if there are other invariant polynomials besides the cumulants.","['statistics', 'moment-generating-functions', 'cumulants', 'invariant-theory', 'probability']"
4891415,"Determine a tangent plane on the surface $\psi(u,v)=(3u^2-2v^2,u-v,u+v)$ at the point $\psi(1,2)$.","So in this question basically I'm getting stuck after calculating the Jacobian of the function $\psi$ , somehow I'm trying to find the values of a vector that is on this plane, but I'm not so sure of what is going on. I've already tried to find the normal vector $\psi_u \times \psi_v$ , but still its hard to move with this. Here is what I've been trying: $\psi_u = \frac{\partial\psi}{\partial u} = (6u,1,1)$ $\psi_v = \frac{\partial\psi}{\partial v} = (-4v,-1,1)$ In the point $\psi(1,2)$ we got: $\psi_u=(6,1,1)$ and $\psi_v=(-8,-1,1)$ Finding the normal vector: $\psi_u \times \psi_v=(2,-14,2)$ Then finally in the equation of the tangent plane (in summary): $z+7y-x+3=0$ . If someone could give me a hint what step did I miss , I would appreciate. The solution here says: $x-7y+z-5=0$","['multivariable-calculus', 'calculus', 'surfaces']"
4891432,The minimum possible value of $\sqrt{58-42x}+\sqrt{149-140\sqrt{1-x^{2}}}$ WITHOUT the use of calculus [duplicate],"This question already has answers here : Find Minimum value of $\sqrt{58-42x}+\sqrt{149-140\sqrt{1-x^2}}$ (5 answers) Closed 3 months ago . My friend recently gave me another problem that, humiliatingly enough, I cannot seem to solve. Find the minimum of the function ( without the use of calculus ) $$\sqrt{58-42x}+\sqrt{149-140\sqrt{1-x^{2}}}$$ When $x\in[-1,1]$ . My thought process goes as follows:
The first thing that I tried was a trig substitution. Since there is a $\sqrt{1-x^{2}}$ , I decided to do $x=\cos(\theta)$ , simplifying it to $$\sqrt{58-42\cos\left(\theta\right)}+\sqrt{149-140\sin\left(\theta\right)}$$ However, I do not know how to continue with this problem, one person hinted towards the use of LOC(Law Of Cosines), and using geometry. But I do not know where to apply it. If you are willing to help, I would be grateful.","['maxima-minima', 'trigonometry', 'geometry']"
4891457,Verify a solution for a recursion based probability problem and propose an alternative,"Problem Statement: A trial succeeds at probability p, where $0 < p < 1$ . However, whenever the trial fails, p has a $\frac{2}{3}$ chance to be multiplied by $\frac{1}{2}$ and $\frac{1}{3}$ chance to be multiplied by $2$ (to a maximum of $1$ ). Is it guaranteed the trial will eventually succeed given infinite tries? My solution (I realized it is wrong after some simulations ): The decision tree (see the picture) showing all the possible events that can occur given the current probability p: Event 1: There's p chance that the process will end Event 2: There's $\frac{1}{3}(1-p)$ chance that the process will continue and p will double Event 3: There's $\frac{2}{3}(1-p)$ chance that the process will continue and p will be halved To calculate the chance that the process will stop we need to sum all the likelihoods of all of the paths that end in the tree. We do this with the following formula derived from the tree: $$f(p) = p + \frac{1}{3}(1-p) f(2p) + \frac{2}{3}(1-p) f(\frac{p}{2})$$ $$f(p) = p + (1-p)(\frac{1}{3} f(2p) + \frac{2}{3}f(\frac{p}{2}))$$ I'm not sure on the correctness of the following steps: It is intuitive and seems reasonable to say that $f(x \cdot p) = x \cdot f(p) $ . $f(p) = l$ , $f(2p) = 2l$ , $f(\frac{p}{2}) = \frac{l}{2}$ or alternatively, like with limits $f(p)=f(2p)=f(\frac{p}{2})$ ( both givin the same answer in the end, but we will use the first substitution ). $$l = p + (1-p)(\frac{1}{3} l + \frac{2}{3}l)$$ $$l = p + (1-p)l$$ $$l(1-1+p) = p$$ $$l \cdot p = p$$ $$l = 1 $$ And with this we conclude that it is guaranteed that the process will stop with infinite amount of steps. Edit: Perhaps we can upper bound $f(p)$ by assuming $f(2p) = 1$ and resolving the p<1 constraint. $$l = p + (1-p)(\frac{1}{3} + \frac{1}{3}l)$$ $$l = p + \frac{(1-p)}{3} + \frac{(1-p)}{3}l$$ $$l(1-\frac{(1-p)}{3}) = \frac{(2p+1)}{3}$$ $$l(\frac{(2+p)}{3}) = \frac{(2p+1)}{3}$$ $$l = \frac{2p+1}{p+2} $$ The only problems is that my experimental values for p = 0.125 are around 0.61 and this gives approximatively 0.588 Another upper bound can be $f(2p) = 1$ and $f(\frac{p}{2}) = f(p)$ which results in $x = 1$ meaning $f(p) < 1$","['limits', 'probability', 'recursion']"
4891478,What radius of circle has a circumference equally divided into 10 sections by a pentagram?,"Given a regular pentagram whose outer vertices lie on a circle of radius 1, a circle interior to and sharing a center with the larger circle will intersect the pentagram in ten places, save for two radii where it is 5, when it intersects the vertices of the inner pentagon and when it inscribes the inner pentagon, and 0 places when it is smaller than the inner pentagon. There are therefore two radii, one in each region of ten intersection points, where the circle is exactly divided into ten equal arcs. What are those two radii? Beyond knowing that the length of said arc is $\pi r/5$ , and that we can find the chord length of an arc of that length to get the radii (using a method that doesn't itself need the radii), I am unsure how to approach the the problem.","['arc-length', 'geometry']"
4891605,Can a ratio $\lim_{x \to c} f'(x)/f(x)$ of a derivative and a function be defined at $c$ when both are zero?,"Given a differentiable function $f$ where $f(c) = 0$ for some $c$ , can $\lim_{x \to c} \frac{f'(x)}{f(x)}$ have a defined value? I guess $f'(c)$ should also be zero for this to possibly have a value, but I'm stuck here.
(This is not a homework problem; one of my colleagues studying NN asked me)","['limits', 'calculus', 'analysis']"
4891615,How to estimate $10^{\frac{1}{2}} + 10^{\frac{1}{3}} + \ldots + 10^{\frac{1}{10}}$,"I heard an interesting interview question recently, which was as follows: Estimate the value of $X = 10^{\frac{1}{2}} + 10^{\frac{1}{3}} + \ldots + 10^{\frac{1}{10}}$ . You have 30 seconds to Compute it . My approach was as follows: $10^\frac{1}{2} = \sqrt{10} \approx 3$ $10^\frac{1}{3} = \sqrt[3]{10} \approx 2$ $10^\frac{1}{10} \approx 1$ And then I guessed that $10^\frac{1}{4 \ldots 9}$ would average to around $1.2$ (because I guessed that they would be more biased towards $1$ than $2$ , as the series would decrease quickly). This gave me my estimate as $X = 3 + 2 + 1 + 6 * 1.2 = 13.2$ , which is somewhat close to the answer. However, this is in no way rigorous or really theoretically motivated. Can someone provide a strategy that is? For reference, the actual answer is $X \approx 15.421$ $X \approx 15.421176223667882$ (with Calculators)","['exponentiation', 'summation', 'approximation', 'estimation', 'sequences-and-series']"
4891649,"If $f$ is continuous at $a$, is $f^{-1}$ continuous at $f(a)$?","Let $I\subseteq\mathbb{R}$ be an open interval, and $f:I\to\mathbb{R}$ an injective function. Let $a\in I$ , and suppose that $f$ is continuous at $a$ . Does it follow that $f^{-1}$ is continuous at $f(a)$ ? I know that if $f$ is assumed to be continuous in the entire $I$ , then $f^{-1}$ is continuous. (Here I use the fact that $f$ is strictly monotonic.) With local continuity, however, I cannot use monotonic properties.","['continuity', 'calculus', 'inverse-function', 'monotone-functions']"
4891698,Finding Value of the Infinite Product $ \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}}$,How I can find the value of the Infinite Product $$ \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}}$$ I tried like this : $p_n =\prod_{k=1}^{n}\frac{ e^\frac{1}{k}}{1+\frac{1}{k}}= \prod_{k=1}^{n}\frac{k}{k+1}.e^\frac{1}{n}=\frac{1}{2}.\frac{2}{3}.\frac{3}{4}.....\frac{n}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}}= \frac{1}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}} $ but I can't find its limit,"['analysis', 'products']"
4891714,Inverse Z Transform of $\frac{1}{(z-a)^2}$,"I'm Trying to find the Z-transform of $$\frac{1}{(z-a)^2}$$ in discrete, i.e using $u[n]$ . Using the known transformation of: $$n \cdot \alpha^n \cdot u[n]  \Longleftrightarrow \frac{\alpha \cdot z^{-1}}{(1-\alpha \cdot z^{-1})^2} = \frac{\alpha \cdot z}{(z-\alpha)^2}$$ I'm getting $$z^{-1}\left\{z^{-1} \cdot \frac{z}{(z-a)^2}\right\} \Rightarrow \text{Time shift by } -1 \Rightarrow (n-1) \cdot a^{(n-1)} \cdot u[n-1]$$ which according to Wolfram Alpha is wrong , apparently the answer is $$(n-1) \cdot a^{(n-2)} \cdot u[n-1]$$ from which I have not been able to understand why $a^{(n-2)}$ rather than $a^{(n-1)}$ . would appreciate some help.","['signal-processing', 'z-transform', 'discrete-mathematics']"
4891750,"Prove that if $f=\frac{1}{(x-a_1)*(x-a_2)...}$, then $f=\frac{k_1}{x-a_1}+\frac{k_2}{x-a_2}+...$ where $k_1=\frac{1}{(a_1-a_2) ... }$ distinct $a_i$","Restatement of Question for Easier Reference Prove that if $f=\frac{1}{(x-a_1)*(x-a_2)...}$ ,
for distinct $a_i$ then $f=\frac{k_1}{x-a_1}+\frac{k_2}{x-a_2}+...$ where $k_1=\frac{1}{a_1-a_2} ... \tag{Eq. 1}$ Motivation: Steps to Solve $k_1$ and $k_2$ if $f=\frac{k_1}{x-a_1}+\frac{k_2}{x-a_2}+...$ Exists If $f=\frac{1}{(x-a_1)*(x-a_2)...}=\frac{k_1}{x-a_1}+\frac{k_2}{x-a_2}+...$ , distinct $a_i$ , then start as follows: Multiply left and right sides by $x-a_1$ yielding: $$f=\frac{x-a_1}{(x-a_1)*(x-a_2)...}=\frac{k_1*(x-a_1)}{x-a_1}
+\frac{k_2*(x-a_1)}{x-a_2}+... \tag{Eq. 2}$$ Then take the limit $\underset{x \to {a_1}} \lim  f$ : $$\underset{x \to {a_1}} \lim  f=
\frac{1}{(a_1-a_2)...}=\underset{x \to {a_1}} \lim \frac{k_1*(x-a_1)}{x-a_1}
+\frac{k_2*(x-a_1)}{x-a_2}+... =k_1$$ $$\tag{Eq. 3}$$ Then, from Equation 3, arrive at Equation 4: $$\underset{x \to {a_1}} \lim  k_1 = \frac{1}{(a_1-a_2)...} \tag{Eq. 4}$$ But how is it possible to show that the Equality Represented by Equation 1 always exists given $f=\frac{1}{(x-a_1)(x-a_2)...}$ , distinct $a_i$ ? I found this one Swarthmore University Reference on ""Partial Fraction Expansion (or Decomposition)"" , but I still do not understand the proof of the concept. Also, I found the Wolfram Partial-Fraction-Calculator , but I still do not understand how it goes from the multiplied roots in the donominator to the individual roots $\frac{k_1}{x-a_1}...$ , the circumstances where this calculation is a proven identity, and the circumstances not provable. Also I find the Wikipedia reference not so easy for me to understand. Is there a contained, clear, and concise proof of the equality? I think that I am understanding the concept of a Polynomial GCD from this reference at Wolfram ""PolynomialGCD"" . For instance, if one wants to compute the Polynomal GCD of the following polynomials: $$\text{PolynomialGCD( }(x+3)^2*(x-2)*x,(x-2)*(x+3)*(x-1) 
\text{ ) }\\=(x-2)*(x+3)$$ $$\tag{Eq. 5}$$ I am starting to understand the concepts of the ""Fundamental Theorem of Algebra"" from ""Further linear algebra. Chapter II. Polynomials."" by Andrei Yafaev. This ties together the concepts of prime numbers and an irreducible polynomials. Maybe one way to start towards the answer is to take a simple case. Say $$f=\frac{k_1}{x-a_1}+\frac{k_2}{x-a_2} \tag{Eq. 6}$$ where $a_1$ and $a_2$ are distinct. So multiply both sides by $(x-a_1)(x-a_2)$ yielding: $$f*(x-a_1)(x-a_2)=(k_1)*(x-a_2)+(k_2)*(x-a_1) \tag{Eq. 7}$$ So, to get rid of the $x$ contribution in the numerator, set $k_1=-k_2$ . Also, the numerator needs to be $1$ ; so in this case $-k_1*a_2-k_2*a_1=1$ so $k_1*a_1-k_1*a_2=1$ so then $k_1=\frac{1}{a_1-a_2}$ and $k_2=\frac{1}{a_2-a_1}$ . Dividing Equation 7 again by $(x-a_1)(x-a_2)$ yields (as was to be proven for two variables $a_1$ and $a_2$ ): $$f(x)=\frac{1}{(x-a_1)(x-a_2)}$$ So I am looking for an answer similar to this one that extends the concept using simple math.  Already the math listed here is very helpful for a separate question that I am in the process of answering, namely: ""Help with the indefinite integral $\int \frac{dx}{2x^4 + 3x^2 + 5}$ "" Thank you for your comments and help that assisted me with the thought process to get the result for two distinct variables without advanced math concepts. If someone could also write-up a self-contained similar answer, I would greatly appreciate that!","['limits', 'calculus', 'polynomials', 'algebra-precalculus']"
4891755,Difficulty Grasping Weak and Weak-* Topologies and Their Intuition,"I've been wrestling with the concepts of weak and weak-* topologies and how they naturally arise in functional analysis. Despite my efforts, they seem to come out of nowhere for me, and I'm hoping to gain some clarity with your insights. Let’s talk about weak topology first. According to Folland( Real Analysis: Modern Techniques and Their Applications Page 168) One often wishes to study the operator $\frac{d}{dx} $ , or more complicated operators constructed from it, acting on various spaces of functions. Unfortunately, it is virtually impossible to define norms on most infinite-dimensional functions spaces so that $\frac{d}{dx}$ becomes a bounded operator."" This statement makes sense to me, but I'm struggling to piece everything together. For instance, the concept of a weak derivative kind of feels like we're saying, ""Since we're in a framework where we can't distinguish two functions when they coincide almost everywhere (a.e.), let's tweak the notion of derivative to respect this."" However, it's not clear to me how a weak derivative becomes bounded when the classical derivative doesn't. When it comes to different modes of convergence, I can somewhat visualize what's happening. For example, if $f_n \to f$ in $L^1([-1,1])$ , it's like saying the average distance between these functions is getting smaller, even though they may differ on a set of measure zero. This kind of intuition works for me for other types of convergence (like in measure, pointwise, almost everywhere pointwise, uniform, almost everywhere uniform, etc.). But when someone says $f_n \to f$ in the weak topology, I'm lost. I can't intuitively grasp what's actually happening. Moving on to weak-* convergence, I understand it as a way to naturally embed a Banach space $X$ (or make it isomorphic to a subset) into $X^{**}$ , its double dual. Sure, it's neat that $X$ lies within $X^{**}$ , aligning with our intuitions from finite-dimensional spaces, but I'm questioning if there's more to it. Is there a deeper insight I'm missing? I've been stuck on this for a while and would greatly appreciate any explanations or insights you could share. Thank you very much! P.S:I highly recommend you to use $d/dx$ as an example, especially in solving differential equations since it seems to me a key example.","['functional-analysis', 'real-analysis']"
4891765,Draw tangents at 3 random points on a circle to form a triangle. Show that the probability that a random side is shorter than the diameter is $1/2$.,"Choose three uniformly random points on a circle, and draw tangents to the circle at those points to form a triangle. (The triangle may or may not contain the circle.) For example: What is the probability, $P$ , that a randomly chosen side of the triangle is shorter than the diameter of the circle? I have found that $P=\frac12$ by using integration, and I will post my answer below. But since the probability is so simple, I am looking for an intuitive explanation . (Having said that, a probability's simplicity is no guarantee that there is an intuitive explanation; for example here and here are probability questions that have answers of $1/2$ but have resisted intuitive explanations.) Edit : If two or more of the tangent lines are parallel or coincident, re-choose the three points.","['geometric-probability', 'geometry', 'triangles', 'intuition', 'probability']"
4891797,Line integral over two squares,"I want to calculate the following integral $$\oint\limits_{C_1} \oint\limits_{C_2} \frac{\vec {dl_1} \cdot \vec {dl_2}} r,$$ where The square paths $C_1$ and $C_2$ over which the integration should be carried are shown in this picture : (the length of both sides of the squares is a given parameter $a$ , and the orientations are the ones shown). $r$ represents the distance from each $\vec {dl_1}$ to $\vec {dl_2}$ . The approach I took in order to solve this problem is considering the total integral as the (finite) sum of integrals restricted to a single side from each square and performing the calculation of each of these integrals: obviously, the chosen sides being perpendicular to each other imply the vanishing of the dot product and thus zero contribution from that term to the total value of the integral. My problem arises from the behaviour of $r$ , due to its dependence on both $\vec {dl_1}$ and $\vec {dl_2}$ . For example, the $r$ i get for the top of the left square and the bottom of the right square is: $$ 
r = \sqrt{a^2 + (3a - x - y)^2}
$$ where $x$ is the coordinate of the point $\vec {dl_1}$ on the left square and $y$ is the coordinate of $\vec {dl_2}$ on the right square (I would integrate from 0 to $a$ both times). Can this issue be solved and consequently is my approach fruitful? And if it is not, is there something easier that I am missing?","['integration', 'multivariable-calculus', 'calculus', 'physics', 'line-integrals']"
4891865,Question regarding nature of logarithmic equations,"While reading my textbook's chapter about logarithms and seeing the solved examples I noticed in various places that the author was able to make the $\log$ just disappear in a equation or inequality by making some changes on either side of the inequality or the equation for example: $$-1 \leq \log_9\left(\frac{x^2}{4}\right) \leq 1$$ changes to $$9^{-1} \leq \frac{x^2}{4} \leq 9^1.$$ This confuses me how does this happen and another curious thing when doing this ""logarithm disappearing"" on logs with bases less than $1$ the sign of the inequality changes. For example in one question the following was done $$\log_{1/2}(x^2 - 7x + 13) > 0$$ changes to $$x^2 - 7x + 13 < 1$$ I do not understand how does $1$ just appear on the other side and why does the inequality reverse, could someone explain this to me and my this ""log disappearing"" is different for bases less than $1$ and greater than $1$ .","['algebra-precalculus', 'logarithms', 'inequality']"
4891867,A maximal vectorial subspace has a 1-dimensional complement.,"I have seen this excercise on a book of Functional analysis and I am trying to prove it. Let $X$ be a vectorial space over a field $\Bbb{K} \, ( \Bbb{R} $ or $ \Bbb{C} )$ and $H \subset X$ a proper subspace. Then, the following are equivalent: $H$ is maximal (that is, for every subspace $H \subseteq M \subseteq X \rightarrow H=M \lor X=M$ ) It exists a subspace $L \subseteq X$ such that $\dim L = 1, H \cap L = \{ 0 \}$ and $X= H + L $ . $\dim  X/H  = 1$ It exists a linear functional $\varphi : X \rightarrow \Bbb{K}$ such that $\ker \varphi = H$ It is important to say that the book does not say anything about the dimension of $X$ , in fact there is problem before this about infinite dimensional spaces. This is why I find this problem difficult, the finite case would be very simple. I saw first that $4 \Rightarrow 3$ is easy to prove using the First Isomorhpy Theorem with $\varphi$ . In order to continue, I have tried to prove $3 \Rightarrow 2$ and $1 \Rightarrow 2$ by considering the linear function $$T: X \times H \rightarrow X, T(x,h) = x-h$$ and taking $L := T( X \times H )$ . This satisfies that $H + L = X$ , but I do not know if it works because I am not seeing how to prove $H \cap L = \{ 0 \}$ and $\dim L = 1$ since the possibly infinite dimension of $X$ . Also, I do not have very idea on how to ""reach"" $4$ from one of the others. I thought first on consider $\varphi ( x ) = ||| x + H  ||| $ where $||| \cdot ||| $ is the norm on $X/H$ induced by a norm on $X$ , but I am not sure about it, so any possible help would be appreciated.","['linear-algebra', 'functional-analysis']"
4891913,Proving that the point is fixed,"Circle with the center O, and also points A and B on it are fixed on a plane, M is the midpoint of AB. Points P and Q are chosen arbitrarily on the circle, the tangent line through P intersects AB at point C, CQ intersects circle the second time at point D. The circle passing through points D, M, Q intersects AB the second time at point E. Prove that the intersection of PE and OM remains constant for any choice of points P and Q. Here is the diagram (black points are fixed, blue ones are arbitrarily chosen, green depend on blue ones, and red point is the one in question) This was the last question on my final exam, and the teacher herself didn't know how to solve it. I have only noticed that CPOM is cyclic, but this didn't lead to anything. I also tried to solve it in coordinates, and this probably would lead us to a solution, but there were so many variables and equations that I decided to stop because of the lack of time.","['euclidean-geometry', 'geometry']"
4891950,Why 'If $rad(G)≥3$ then $rad(\bar G)\leq 2$?,"I was reading a proof about radius of self-complementory graph and I stumbled across statement that ""if $rad(G)≥3$ then $rad(\bar G)\leq 2$ and that if $diam(G)≥3$ then $diam(\bar G)\leq 3$ . I couldn't find similar statements anywhere else. Is that a common-sense kind of thing? Or is there a proof? Is it a theorem? I tried to draw a couple of graphs, verified that it's in fact true but I'd appreciate any extra info. Source of question: Prove that a self-complementary graph has radius $2$ and diameter $2$ or $3$ Thank you for clarification.","['graph-theory', 'discrete-mathematics']"
4891962,Definition of faithful family of representations,"I am not sure if this is the right site for this question, but it seems ""too simple"" a question to post on MO, so I chose to put it here. In 1.1.13 of Milne's translation of Deligne's paper titled Variétés de Shimura: interprétation modulaire, et techniques de construction de modèles canoniques , which can be found here , he states (here $G$ is an algebraic group, say over $\mathbb{R}$ ): ...for some faithful family $V_i$ of representations of $G$ ... My question is: what is a faithful family of representations? Does it mean that each $V_i$ is a faithful representation of $G$ , or does is mean that $G\rightarrow\prod\operatorname{GL}_{V_i}$ is injective. I believe it is the latter because the former would be a family of faithful representations and not a faithful family of representations. Or am I just being too pedantic? Thanks for any help.","['number-theory', 'algebraic-geometry', 'representation-theory', 'reductive-groups']"
4891982,Integrability of a vector field and its topology,"Today, in the lecture, we covered an example of a vector field which suffices the necessary condition for integrability, yet is not integrable. The following field also known as the angular form is an example of such a case. $$\omega = -\frac{y}{r^2}dx + \frac{x}{r^2}dy$$ Where $r = \sqrt{x^2+y^2}$ and the domain is $\Omega = \mathbb{R}^2/$ {(0,0)}
Clearly, we can check that the mixed partials are equal and that the necessary condition holds. But if we compute the circulation about the origin say a unit circle, we notice that it is not zero. I am aware that the necessary condition is a sufficient condition once we have a simply connected domain $\Omega$ . By slicing the plane of the above example we obtain a simply connected domain without the origin and have an integrable vector field. My question is the following: When we have holes in the domain of the vector field, do we have to check the circulation around each hole to make sure it is zero and then we can find its potential and state that it is integrable? If it is non-zero do we slice the domain and state that it is integrable on the restricted domain? I hope my question makes sense, thanks for any help, ideas or clarification! Edit: Furthermore after the slicing (removing any smooth curve that connects 0 to $\infty$ ) I claim that $\phi(x,y) = \arctan(\frac{y}{x})$ But it is discontinuous along the y-axis, so then it cannot be the potential function because it is not differentiable there. Am I making a mistake in this logic? Thanks!","['integration', 'vector-fields', 'multivariable-calculus', 'calculus', 'vector-analysis']"
4891983,Finding an analytical solution to a particular class of second order ODEs,"Let $$\ddot{y}+f(x)y=0$$ Be a differential equation such that $\{y,f\}\subseteq\mathcal{C}^\infty$ . We make the substitution $y=e^{u(x)}$ to get: $$\frac{d^2}{dx^2}e^{u(x)}+f(x)e^{u(x)}=0$$ Or $$(\ddot{u}(x)+\dot{u}(x)^2)e^{u(x)}+f(x)e^{u(x)}=0$$ Or, dividing by $e^{u(x)}$ $$\ddot{u}(x)+\dot{u}(x)^2=-f(x)$$ Which can be turned into a first order, non-linear ODE by order reduction $p(x)=\dot{u}(x)$ $$\dot{p}(x)+p(x)^2=-f(x)$$ Can this be used to find an analytical solution to the ODE?","['nonlinear-system', 'ordinary-differential-equations']"
4891989,"How can I show that the first exit time by a planar Brownian motion is a.s. finite, i.e. $\mathbb{P}_z(\tau_D<\infty)=1$?","I have found the following interesting proposition on planar Brownian motions, but I don't really understand the proof. Let $D$ be a proper simply connected domain and $z\in D$ . Let $B$ be a complex valued Brownian motion starting from $z$ . Then $\mathbb{P}_z(\tau_D<\infty)=1$ where $\tau_D:=\inf\{t\geq 0: B_t\notin D\}$ They have given the following proof: By the Riemann mapping theorem there exists a conformal isomorphism $f:D\rightarrow \mathbb{D}$ . By the conformal invariance theorem there exists a time change $\sigma(t)$ such that $W_t:=f(B_{\sigma(t)})$ is a complex Brownian motion for $t<\tau_{f(D)}=\inf\{t\geq 0: W_t\notin f(D)\}=\inf\{t\geq 0: B_{\sigma(t)}\notin D\}$ . Hence if $t\rightarrow \tau_{f(D)}$ then eventually $|W_t|\geq 1/2$ . But $B$ is neighbourhood recurrent by a previous corollary, so it visits the open set $\{z\in D: |z|<1/2\}=f^{-1}(\{|z|<1/2\})$ at an unbounded set of times a.s.
Hence $\tau_D<\infty$ a.s. My first question is, where do i need that $W_t$ is a complex Brownian motion, I mean why can't I only work with $B_t$ instead of $W$ ? Futhermore how does they conclude that $\tau_D<\infty$ ? I thouth about if I can show that $\tau_{f(D)}<\infty$ then $\tau_D$ needs to be finite a.s. since there is only  a time change in the game. Can someone explain me this proof or help me how to proof it similarly. I know the conformal invariance theorem and that a complex Brownian motion is neighbourhood recurrent.","['complex-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4892009,"Find the closed form of $_3F_2(\frac{1}{4},\frac{3}{4},\frac{5}{4};\frac{3}{2},\frac{7}{4};1)$","Context Some investigation suggests that the following identity is true: \begin{align}
_3F_2(\frac{1}{4},\frac{3}{4},\frac{5}{4};\frac{3}{2},\frac{7}{4};1)=\frac{3\sqrt{2}\sqrt{\pi}\left(2\log({1+\sqrt{2}})+\pi-2\sqrt{2} \right)}{\Gamma(\frac{1}{4})^2}\tag{1}
\end{align} In this question ( Prove $\,_3F_2\left ( \frac12,\frac12,1;\frac34,\frac34;-\frac18 \right ) =\frac23+\frac{\Gamma\left ( \frac34 \right )^2}{3\sqrt{\pi} }$ ) there is an answer proving a more difficult identity with the WZ method. Question Do you think that we can prove $(1)$ with the mentioned method? Thanks for your cooperation.","['integration', 'closed-form', 'hypergeometric-function', 'sequences-and-series']"
4892011,Mysterious Coordinates on $S^4$ involving Quaternions,"Let $U$ and $U'$ be $S^4 - x_N$ and $S^4 - x_S$ , respectively, where $x_N$ is the North pole and $x_S$ is the South pole. The usual stereographic projection maps $U$ into $R^4$ and $U'$ into $R^4$ . If we identify $R^4$ with the quaternions, then we can write the coordinates on $U$ as a quaternion $z$ and the coordinates on $U'$ as another quaternion $z'$ . Then, the change of coordinates between these two charts take the remarkably simple form $z'=1/z$ .
This is well known, and can be found in many references. My question regards a different choice of coordinates on $S^4$ , which I found on an old paper, that unfortunately skips the details on how its constructed.
The two patches $U$ and $U'$ are defined in the same way, but they are mapped to the disk $\{x \in R^4 | |x|^2 <1 \}$ , rather than the full $R^4$ . Such a region is again identified with quaternions via the usual isomorphism; let's say that the quaternion $z$ corresponds to the coordinates on $U$ and $z'$ corresponds to those on $U'$ , as before. The transition function reads: $z'= \frac{1 - ||z||}{||z||}z$ , where $||z||$ is the norm of $z$ . For reference, this question originated when reading the paper ""Homeomorphy classification of total spaces of sphere bundles over sphere"" by Tamura, page 31 (just before section 2). I tried a combination of stereographic coordinates with some conformal mapping of $R^4$ into the unit disk, but I could not obtain the transition function above. I have a feeling that it should be something simple, but I can't think of something that matches the properties that I described. Any suggestions on how these coordinates might be defined? Thanks in advance!","['spheres', 'spherical-coordinates', 'differential-topology', 'quaternions', 'differential-geometry']"
4892015,Is it true that $\sin^2((2n+1) \pi x) \geq c$ occurs often (in precise sense)?,"Fix $x \in [0, 1/2]$ and consider the function $f(n, x) = \sin^2((2n +1) \pi x)$ . Based on plotting, it seems like the following is true For every $x \in (0, 1/2)$ , there exists $c \in (0, 1]$ and a positive integer $N$ such that for every $k \geq 0$ , there exists an integer $m \in [kN, kN + N)$ such that $f(m, x) \geq c$ . In other words, we can partition the integers $\mathbb{N} = \cup_{k \geq 0} I_k$ where $I_k = [k N, kN + N) \cap \mathbb{N}$ and on each interval there is some $m \in I_k$ for which the function $f$ is bounded away from $0$ uniformly. I tried to prove or disprove this claim using the fact that $\sin^2((2n+1) \pi x) = \sin^2(\pi \{(2n+1) x\})$ where $\{t\}$ denotes the fractional part of $t$ , but I failed to make much progress. Is this claim true?","['periodic-functions', 'inequality', 'trigonometry']"
4892039,Minimum variance unbiased estimator for $\mu$ in Normal location model with known but random variance,"Consider observing $X \mid \sigma \sim N(\mu, \sigma^2)$ and $\sigma \sim F$ for some known distribution $F$ supported on the positive reals. We observe a single draw $(X, \sigma)$ . An estimator $T(X, \sigma)$ is unbiased for $\mu$ if $E[T(X,\sigma)] = \mu$ . $T(X, \sigma) = X$ is one such choice. Another choice is $T(X,\sigma) = w(\sigma) X$ for any $w(\sigma)$ where $E[w(\sigma)] = 1$ . Similarly, one could construct some unbiased estimator that uses $X^2, \ldots$ . There may be other more exotic choices as well. Among these estimators, what is the minimum variance estimator? The Cramer-Rao bound for $\mu$ with likelihood $f(x,\sigma) = f(\sigma) \frac{1}{\sigma} \varphi((x-\mu)/ \sigma)$ is $1/E[1/\sigma^2]$ . This is a lower bound on the minimum variance. Is there an estimator that achieves it?","['statistics', 'sufficient-statistics', 'parameter-estimation', 'normal-distribution', 'probability']"
4892052,Why does Terence Tao use a subset of a subset of $R$ in the definition of limit of a function?,"I'm studying real analysis using the book Analysis 1 of Terence Tao.
He defines the limit of a function at a point as follow (Definition 9.3.6) Definition 9.3.6 (Convergence of functions at a point). Let $X$ be a subset of $\mathbf{R}$ , let $f : X \to \mathbf{R}$ be a function, let $E$ be a subset of $X$ , $x_0$ be an adherent point of $E$ , and let $L$ be a real number. We say that $f$ converges to $L$ at $x_0$ in $E$ , and write $\lim_{x \to x_0; x \in E} f(x) = L$ , iff $f$ , after restricting to $E$ , is $\varepsilon$ -close to $L$ near $x_0$ for every $\varepsilon > 0$ . If $f$ does not converge to any number $L$ at $x_0$ , we say that $f$ diverges at $x_0$ , and leave $\lim_{x \to x_0; x \in E} f(x)$ undefined. It seems to me that this definition is too complicated, he has to go through a subset $E$ of a set $X$ which is a subset of $\mathbb{R}$ . I can't understand why didn't he directly define the limit of a function by just using the set $X$ (which is more than enough I think). Could you please explain me the motivation to use the set $E$ in this definition ? Thank you for your help!","['limits', 'functions', 'real-analysis']"
4892100,evaluate the volume of solid,"Consider the paraboloid $(\mathcal{P}): z=x^2+y^2$ and the plane $(\mathcal{Q}): 2x+2y+z=2$ .
Let $\mathcal{S}$ be the solid region bounded above by $(\mathcal{Q})$ and below by $(\mathcal{P})$ . Find the volume of the solid region $\mathcal{S}$ . After sketching the plane will cut the paraboloid and obtain the shadow in the $xy$ -plane as a circle of equation $$
R: (x+1)^2+(y+1)^2=4.
$$ Now to find the volume, I travel first in the $z$ -direction and get $$
V(\mathcal{S})=\iint_{R}\int_{x^2+y^2}^{2-2x-2y}dz dA=\iint_{R}\left(2-2x-2y-x^2-y^2\right)dA.
$$ Now to evaluate the double integral over the shadow $R$ , we may use cartesian coordinates and try to evaluate it, but the calculation are very hard. Is it possible to find the volume for example using cylindrical or spherical coordinates? How we can do it? Based on the comment below, $r^2=(x+1)^2+(y+1)^2$ , the volume could be $$
V(\mathcal{S})=\int_{0}^{2\pi} \int_{0}^2\left[-r^2+4\right]r dr d\theta.
$$ Is it correct in this way to solve it?","['integration', 'cylindrical-coordinates', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
4892107,Pushforward of vector field and its divergence,"Let $X:U_1 \rightarrow \mathbb{R}^2$ be a smooth vector field defined in an open subset of $\Bbb{R}^2$ and $\phi:U_1\rightarrow U_2$ a diffeomorphism between open subsets of $\Bbb{R}^2$ . Let $Y = \phi_*X$ , ie, $$Y(p) = d\phi_{\phi^{-1}(p)}(X(\phi^{-1}(p))),\qquad  p \in U_2.$$ Here, we are considering that if $X(x,y) = (P(x,y), Q(x,y))$ , where $P:U_1 \to \Bbb{R}$ and $Q:U_1\to \Bbb{R}$ are smooth functions, then $$
\operatorname{div}(X) := \frac{\partial P}{\partial x} 
+ \frac{\partial Q}{\partial y}.
$$ Is there any relation between $\operatorname{div}(X)$ and $\operatorname{div}(Y)$ ? Can I get $\operatorname{div}(Y)$ in terms of $\phi$ and $\operatorname{div}$ ?","['divergence-operator', 'ordinary-differential-equations', 'vector-fields', 'pushforward', 'differential-geometry']"
4892117,Can I suppose a measure preserving action is by homeos?,"Let $G$ be a countable group with a measure preserving action on a probability measure space $(X,\mathcal{B},\mu)$ . I am looking for sufficient conditions (as general as possible) so that there is a Cantor space $(C,\mathcal{C},\nu)$ (where $\nu$ is the completion of a Borel measure in $\mathcal{C}$ ) with a measure preserving action of $G$ on $(C,\mathcal{C},\nu)$ by homeomorphisms, and a measure preserving map $F:C\to X$ which commutes with the actions of $G$ . I get the impression that this should be true whenever $X$ is a Lebesgue-Rohlin space (the idea would be using a countable separating subset of $\mathcal{B}$ and its images by the action to define a subbasis of clopens for the topology of $\mathcal{C}$ ), although I have not tried to write it in detail. In any case, if there is a reference containing results like this one then it would be better to just cite it. Motivation: I am trying to prove something for general (or as general as possible) measure preserving systems but my current techniques use weak convergence of measures so I need the action to be by homeomorphisms in a compact metric space.","['measure-theory', 'lebesgue-measure', 'borel-measures', 'reference-request']"
4892130,Solving Xa=b for an unknown matrix X,"I'm interested in studying the solutions of $Xa=b$ for an unknown square matrix $X$ , and given (known) column vectors $a$ and $b$ in $\mathbb{R}^n$ . For any numerical $a, b$ , one can directly attempt solving the aforementioned system. But I'm interested in understanding the general setting to answer questions similar to the ones below. (1) Under what conditions a solution $X$ exists? (2) When does a symmetric solution exist? Under what conditions, no symmetric solution exists? (3) When does a unique, invertible, symmetric solution exist? The answer to (1) is easy: a solution exists whenever $a\ne \vec{0}$ , or both $a, b$ are zero vectors. $Xa=b$ is, of course, a system of $n$ linear equations in $n^2$ variables if there are no additional constraints on $X$ , in which case the equations are ""decoupled"" because of having disjoint set of variables. But, for instance, the number of variables is cut down to $n(n-1)/2$ if we require X to be symmetric. So, if $n(n-1)/2=n$ , that is, $n=3$ I expect (3) to be likely than when $n>3$ . What are some good ways to think about problems like this involving $Xa=b$ ? Any suggestions, or references are appreciated.","['matrices', 'systems-of-equations', 'linear-algebra', 'symmetric-matrices']"
4892153,Is multiplication of two complex numbers that are inside a complex regular polygon still in this polygon?,"For any two points inside a regular polygon on the complex plane, where the vertices of the polygon are on a unit circle, one of the vertices is at (1, 0), and the centre of the polygon is at (0,0).  Can we prove that the multiplication of these two complex number is still inside this polygon (a rotated and shortened version of one of the two original complex numbers)? I guess it's right but I haven't figured out how to prove it. Or maybe it's just not right. Take the square as an example, with four vetices $v_1, \ldots v_4$ , I am just wondering whether the multiplication of any $a_1a_2$ is still in this regular polygon.","['geometry', 'complex-numbers']"
4892178,Lines on cubic threefold in characteristic 3,"Altman and Kleiman in Foundations of the theory of Fano schemes prove that if $F = F_1(X) \subset \mathbb G(1, 4) = G$ is a Fano scheme of lines on cubic threefold in $\mathbb P^4$ with at most finitely many double points, then the natural map induced by restriction $$
H^0(G, \mathcal O(2)) \rightarrow H^0(F, \mathcal O(2))
$$ is injective whenever $\mathbb k$ is algebraically closed field of characteristic not $3$ , and thus every quadric hypersurface containing $F$ in the Plucker embedding must also contain $G$ at least in characteristic not $3$ . Sadly authors do not mention what happens in the excluded case of characteristic $3$ . I'm curious if such quadric can be constructed explicitely in any elegant/compact form (provided it even exists, what also to me is not immediately clear, as the proof is not constructive)",['algebraic-geometry']
4892195,Existence of smooth functions decaying fast near the boundary,"This question concerns the following claim: Let $M$ be a manifold and $U\subset M$ be open. For every smooth function $f:U\to\mathbb{R}$ , there exists a smooth function $g:M\to[0,1]$ such that $g$ is strictly positive on $U$ , vanishes outisde $U$ , and the function $$
fg(x)=\begin{cases}
f(x)g(x)&~x\in U,
\\
0&~x\notin U,
\end{cases}
$$ is smooth on $M$ . This cannot be solved directly using bump functions, since we are not assuming that $\operatorname{supp}g\subset U$ . I have been thinking about this for a while. An idea is to take a compact exhaustion of $U$ , take a sequence of bump functions $h_i$ on those compact sets, choose bounds on the derivatives of $h_i$ and $h_if$ , and finally consider a power series of the form similar to $\sum_i2^{-i}\frac{1}{1+C_i+D_i}$ , where $C_i,D_i$ are the bounds. This idea comes from the bottom of page 28 on the book $C^\infty$ -Differentiable Spaces by Juan A. Navarro González. However, I don't know how to prove this rigorously. The proof given in the book is unclear because it does not specify computing the derivative in which coordinate charts, and it does not provide full details. Any help would be greatly appreciated. If you have a different proof, please feel free to include it!","['smooth-manifolds', 'differential-geometry']"
4892204,How do you actually solve a linear homogeneous differential equation rigorously without splitting the differential,"I want to solve the differential equation $\frac{dy}{dx}=y$ , but I only know the informal way of doing it by splitting the differential. $$\frac{dy}{dx}=y$$ $$\frac{1}{y}dy=dx$$ $$\int\frac{1}{y}dy=\int dx$$ $$\ln|y|=x+C$$ $$y=Ce^x$$ I know that this is the correct answer because exponential functions are their own derivatives. However, this is not the correct way to do it, because $\frac{dy}{dx}$ is not a fraction, but an operator on $y$ instead. When I try to instead try to take the integral with respect to $x$ , I get $$\frac{dy}{dx}=y$$ $$\frac{dy}{dx}dx=ydx$$ $$\int\frac{d}{dx}(y)dx=\int ydx$$ $$y=yx+C$$ I know that this is not the correct answer, because taking the derivative does not give back the original differential equation. Which step is incorrect? Is it the way that I am setting up the integral, my integration of the derivative of $y$ , or is it integrating $y$ with respect to $x$ ? If the last option of my integration of $y$ on the right side being incorrect, why is that? Shouldn't this integral with respect to $x$ ""consider"" $y$ to be a constant? How is this done the formal way?",['calculus']
4892220,Understanding complexified Levi-Civita connection in complex geometry,"Proposition $3.18$ From this note we have, Let $(X,J,g)$ be a Hermitian manifold. If we denote the complexified Levi-Civita connection by $\nabla$ . $\nabla$ is characterized as the only connection on $T^{\mathbb R}X$ that is both torsion free and compatible with $g$ . Then by definition we can assume, $$\nabla_{\partial_i}\partial_j=\Gamma^k_{ij}\partial_k+\Gamma^{\bar k}_{ij}\partial_{\bar k}\tag1$$ Since $\nabla$ is a real operator, we also have, $$\nabla_{\partial_{\bar i}}\partial_{\bar j}=\overline{\Gamma^k_{ij}}\partial_{\bar k}+\overline{\Gamma^{\bar k}_{ij}}\partial_{k},\nabla_{\partial_{i}}\partial_{\bar j}=\overline{\Gamma^k_{\bar ij}}\partial_{\bar k}+\overline{\Gamma^{\bar k}_{\bar ij}}\partial_{k}\tag2$$ Then using the torsion free and metric compatibility condition we have $\Gamma^k_{ij}=\Gamma^k_{ji}=\Gamma^{\bar k}_{ij}=\Gamma^{\bar k}_{ji}$ and $\Gamma^{\bar k}_{ij}=\Gamma^{\bar k}_{\bar ij}=\Gamma^{k}_{\bar ij}=0$ resp. $(1)$ make sense, as we decompose everything into holomorphic and anti-holomorphic coordinates. But $(2)$ doesn't make sense to me. Why we need the complex conjugate of Christoffel symbols here? what's the complex conjugates of the Christoffel symbols? And how to determine $\nabla_{\partial_{\bar i}}\partial_j=$ ? As it wasn't explicitly mentioned but I guess it should be $\nabla_{\partial_{\bar i}}\partial_j\stackrel{?}{=}\overline{\Gamma^k_{i\bar j}}\partial_{\bar k}+\overline{\Gamma^{\bar k}_{i\bar j}}\partial_{k}$ (without knowing why it should look like this! As I have a doubt about the \bar used in the formulas like I guess it should be $\nabla_{\partial_{\bar i}}\partial_{j}$ in $(2)$ ). On section $8.4$ , Geometry, Topology and Physics by Nakahara define covariant derivative differently, Let (M, g) be a Hermitian manifold. We define a connection which is compatible with the complex structure. $$\nabla_\mu \frac{\partial}{\partial z^\nu}=\Gamma^\lambda_{\mu\nu}(z)\frac{\partial}{\partial z^\lambda}\equiv\Gamma^\lambda_{\mu\nu}\partial_\lambda, \quad\nabla_{\bar \mu} \frac{\partial}{\partial \bar z^\nu}=\Gamma^{\bar \lambda}_{\bar\mu\bar\nu}(z)\frac{\partial}{\partial \bar z^\lambda}\equiv\Gamma^{\bar \lambda}_{\bar\mu\bar\nu}\partial_{\bar\lambda}\tag3$$ Why isn't decomposition considered here? As @TedShifrin mention in the comment, $\frac{\partial}{\partial z^\nu}$ are holomorphic sections of the tangent bundle, their covariant derivatives are of type $(1,0)$ only. That's why $\nabla_\mu \frac{\partial}{\partial z^\nu}=\Gamma^\lambda_{\mu\nu}\partial_\lambda$ make sense. The only thing which still confuse me is $(2)$ . Actually, I want to know, how to locally determine this connection intuitively . It will be a great help if anyone give me any reference/resource where I can have the motivation/derivation. TIA","['complex-geometry', 'connections', 'differential-geometry']"
4892224,Probability questions that have answer $\frac{1}{2}$ but resist intuitive explanation.,"My question is: What are some examples of probability questions that have answer $\frac{1}{2}$ but resist intuitive explanation? Context Some probability questions have answer $\frac{1}{2}$ , and - as you might expect - have an intuitive explanation, i.e. an explanation that requires little calculation, instead utilizing a clever framing of the question that makes the answer obvious. For example: ""Flip seven unbiased coins; what is the probability of getting at least four heads?"" Some people will calculate $P=\frac{\binom{7}{4}+\binom{7}{5}+\binom{7}{6}+\binom{7}{7}}{2^7}=\frac{35+21+7+1}{128}=\frac{64}{128}=\frac{1}{2}$ , but there is an intuitive explanation: getting at least four heads means getting more heads than tails, which, by symmetry, has probability $\frac{1}{2}$ . Another example: Taking Seats on a Plane and its intuitive explanation . Another example, this one about geometric probability: ""Break a stick at two random points. The probability that the longest piece is at least twice as long as each of the other pieces is $\frac{1}{2}$ . Why?"" Here is a (somewhat) intuitive explanation. On the other hand, I have found that some probability questions have answer $\frac{1}{2}$ but do not seem to have an intuitive explanation . For example: The vertices of a triangle are three random points on a unit circle. The side lengths are $a,b,c$ . Show that $P(ab>c)=\frac{1}{2}$ . ( Update : Several explanations, of varying degrees of intuitiveness, have been given here . Draw tangents at 3 random points on a circle to form a triangle. Show that the probability that a random side is shorter than the diameter is $\frac{1}{2}$ . ( Update : An intuitive explanation has been given here .) Intuition is silent: Find the probability that the smallest circle enclosing $n$ random points on a disk lies completely on the disk, as $n\to\infty$ . Break a stick at $n$ random points. What is the probability that the three shortest pieces can form a triangle, as $n\to\infty$ ? I find these later kinds of questions interesting, because the answer, $\frac{1}{2}$ , is the simplest probability of all (unless you count $0$ or $1$ , but those are sort of degenerate), but there does not seem to be a simple explanation. It's as if the question is making fun of us: ""You can't find an elegant solution!"" Or maybe there is no elegant solution, and the answer is $\frac12$ by coincidence. I would be interested in more of these kinds of questions (they don't have to be related to geometry). Remarks I require that the answer is exactly $\frac{1}{2}$ , not any other number, to avoid a slippery slope of what numbers are acceptable. Let's avoid ad hoc questions, for example: ""Find the probability that two uniformly random points inside a unit square are within $d$ distance of each other, where $d$ is the smallest positive root of $\frac12x^4-\frac83x^3+\pi x^2-\frac12=0$ ( $d\approx0.5120$ )."" The answer is $\frac12$ because $d$ is the median distance. Obviously anyone asking this question already knows that the answer is $\frac12$ . I am using the ""big-list"" tag, whose description reads ""Questions asking for a ""big list"" of examples, illustrations, etc. Ask only when the topic is compelling..."" I think my question might provide some insight on probability and intuition.","['intuition', 'big-list', 'geometric-probability', 'probability']"
4892228,Self duality of a connection is invariant under a gauge transformation,"Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$ , and let $P\to M$ be a (smooth) principal $G$ -bundle over an oriented Riemannian smooth 4-manifold $(M,g)$ . Let $E=P\times_{\text{Ad}}\mathfrak{g}$ be the adjoint bundle of $P$ . The Hodge star operator $*_g:\Omega^2(M)\to \Omega^2(M)$ extends to $*_g:\Omega^2(M,E)\to \Omega^2(M,E)$ in a natural way. Suppose $\omega\in \Omega^1(P,\mathfrak{g})$ is a connection 1-form on $P$ , and let $\Omega\in \Omega^2(P,\mathfrak{g})$ be the curvature 2-form. The curvature 2-form is horizontal and $\textrm{Ad}$ -equivariant, and via the canonical isomorphism $\Omega^2_{\text{Ad,hor}}(P,\mathfrak{g})\cong \Omega^2(M,E)$ , $\Omega$ corresponds to an $E$ -valued 2-form $\bar{\Omega}$ on $M$ . Suppose that ${\Omega}$ is self-dual, i.e. $*_g\bar{\Omega}=\bar{\Omega}$ . Now let $\Theta:P\to P$ be a bundle automorphism; it is given by $\Theta(p)=p\cdot u(p)$ for some $G$ -equivariant function $u:P\to G$ . It is well-known that $\Theta^*\omega$ is also a connection 1-form and that its curvature 2-form is $\Theta^*\Omega$ . I want to show that $\Theta^*\Omega$ is also self-dual, i.e. $*_g\overline{\Theta^*\Omega}=\overline{\Theta^*\Omega}$ . Let $\theta:E\to E$ be the vector bundle automorphism $[p,v]\mapsto [\Theta(p),v]$ induced by $\Theta$ . Using the fact that $(\Theta^*\Omega)_p=\text{Ad}(u(p)^{-1})\circ \Omega_p$ , I've shown that $\overline{\Theta^*\Omega}=\theta^{-1}\circ \bar{\Omega}$ . So I am left to show that $*_g(\theta^{-1}\circ \bar{\Omega})=\theta^{-1}\circ \bar{\Omega}$ . This would be done if $*_g$ commutes with $\theta$ , but I'm not sure about this. Is it true? P.S. Maybe there should be an additional assumption that the bundle $E$ has a fiberwise metric and that $\theta:E\to E $ is compatible with it.","['principal-bundles', 'connections', 'curvature', 'gauge-theory', 'differential-geometry']"
4892267,"If $F:\mathbb R^m\to \mathbb R^m$ is continuous with $|F(x)-F(y)|\geq \lambda|x-y|$, then $F$ is surjective.","I know that this question has been asked for several times (such as this post and the one-dimensional case ). However, I still couldn't find an answer without using invariance of domain. The original question is stated as: If $F:\mathbb R^m\to \mathbb R^m$ is continuous with $|F(x)-F(y)|\geq \lambda |x-y|$ for some $\lambda>0$ , then $F$ is a homeomorphism. Here is my attempt:
It is clear that $F$ is injective and $F^{-1}:F(\mathbb R^m)\to \mathbb R^m$ is continuous. Hence,  it remains to show that $F$ is surjective. As $\mathbb R^m$ is connected, we can infer that $F(\mathbb R^m)=\mathbb R^m$ by proving that $F(\mathbb R^m)$ is a clopen subset. The closedness of $F(\mathbb R^m)$ is easy: Suppose $q_n$ is a sequence in $F(R^m)$ with $q_n\to q$ . Then there exists $p_n\in \mathbb R^m$ such that $F(p_n)=q_n$ . Since $|q_n-q_m|\geq \lambda |p_n-p_m|$ and $\{q_n\}$ is Cauchy, $p_n$ is Cauchy, which implies that $p_n\to p$ for some $p\in \mathbb R^m$ . Them by the continuity of $F$ , $q=F(p)\in F(\mathbb R^m)$ . However, I still need show that $F(\mathbb R^m)$ is open and that is where I get stumped. Since it  is a question given in our introductory real analysis courses, invariance of domain is completely out of scope of this curriculum. Thus, I seek for an elementary solution.","['general-topology', 'open-map', 'real-analysis']"
4892280,Express Integer as Product Complex Number,"Given positive integer $n$ . Find all integers can be expressed as $$S=\left (\sum_{k=1}^nz_k\right )\left (\sum_{k=1}^n \frac{1}{z_k}\right )$$ for some complex number $z_1,\cdots,z_n$ and $|z_1|=\cdots|z_n|=1$ . Here is my approach. For $n=1$ it's trivial. For $n\ge 2$ , let $z_k=e^{it_k}$ where $0<t_k\le 2\pi$ .
I have prove that $0\le S\le n^2$ and $$S = \left (\sum_{k=1}^n \cos(t_k)\right )^2 + \left (\sum_{k=1}^n \sin(t_k)\right )^2.$$ I have found that for perfect square numbers. For example, if $n=2k$ for $k$ positive integer: $t_1=t_2=\cdots=t_n=0\implies S=4k^2.$ $t_1=t_2=\cdots=t_x=0$ and $t_{x+1}=\cdots=t_{2k}=\pi$ $\implies S=0$ . $t_1=t_2=\cdots=t_x=0$ and $t_{x+1}=\cdots=t_{2k}=\pi$ $\implies S=(2k-2x)^2$ . $t_1=t_2=\cdots=t_{2x}=0$ and $$\left (t_{2x+1},t_{2x+2},t_{2x+3},t_{2x+4},\cdots,t_{2k-1},t_{2k}\right )=\left (\frac{\pi}{2},-\frac{\pi}{2},\frac{\pi}{2},-\frac{\pi}{2},\cdots, \frac{\pi}{2},-\frac{\pi}{2}\right ),$$ we have $S=(2x-1)^2$ . For the rest number I don't have any clue to make construction.","['complex-analysis', 'complex-numbers']"
4892313,Find a maximal area of a convex figure whose all $\mathbb{Z}^{2}$ shifts can make a full circle turn (current record is 0.8064846),"Problem. Consider the plane $\mathbb{R}^{2}$ . Given a convex solid figure $\mathcal{P}$ such that $(0,0)\in\mathcal{P}$ . For every pair $(n,m)$ of integers, let $\mathcal{P}_{n,m}$ be a shift of $\mathcal{P}$ with respect to a vector $(n,m)$ . For a time parameter varying from $0$ to $1$ , every figure $\mathcal{P}_{n,m}$ rotates around the point $(n,m)$ in the positive direction, each with its own non-negative speed, so that the following two conditions are satisfied: Two distinct figures can intersect (touch) only at the boundary; At the time moment $1$ , every figure has made a full $360^{\circ}$ turn. Find the maximal possible area of such $\mathcal{P}$ . The first construction. This problem occurred to me while working on the paper about defects and grain boundaries in tilings (the motivation comes from a study of $2D$ crystals), but it has an independent interest. The circle centered at $(0,0)$ with radius $\frac{1}{2}$ is, naturally, such a figure. Surprisingly, the area of $\mathcal{P}$ can be larger than $\frac{\pi}{4}$ . Indeed, let $ r=\big{(}1+\cos(\frac{\pi}{8})\big{)}^{-1}$ . Consider a circle (call it $\mathcal{C}$ ) of radius $r$ and centered at $(0,0)$ . Consider an octagon inscribed in $\mathcal{C}$ , as shown on the right: Let our $\mathcal{P}$ be made of this octagon with four circular segments of $\mathcal{C}$ added, as shown.
To the left of $\mathcal{P}$ we have $\mathcal{P}_{-1,0}$ . The latter can make a full circle rotation, in the process just touching $\mathcal{P}$ . This is due to the choice of $r$ , since this yields $\mathrm{OH}+\mathrm{OA}=1$ . Suppose now that we have a copy of $\mathcal{P}$ centered at every point $(m,n)$ , $m,n\in\mathbb{Z}$ : Those with $m+n$ even can make a full circle rotation. Afterwards, those with $m+n$ being odd can make a full circle rotation as well. The area of such $\mathcal{P}$ is equal to $$\frac{2\pi+4\sqrt{2}}{6+\sqrt{2}+4\sqrt{2+\sqrt{2}}}=0.8064733591_{+}>\frac{\pi}{4}=0.785398163_{+}.$$ Improvement. One can do even better. Let $X\in[0,\frac{1}{4}]$ , and $r=\frac{1}{1+\cos(\pi X)}$ . The following figure (shown here for $X=0.15$ ) also solves the problem: Its area is equal to $$\frac{2\sin(2\pi X)+\pi(1-4X)}{\big{(}1+\cos(\pi X)\big{)}^2}.$$ Surprisingly, the maximal value is achieved not for $X=\frac{1}{8}=0.125$ , but $X_{0}=0.1266579725_{+}$ , which is the solution to $$\sin(\pi X)=\pi\Big{(}X-\frac{1}{4}\Big{)}.$$ Skipping all the calculations, we formulate the following Proposition. Let $\vartheta$ be the unique solution to $\sin(\frac{\pi}{4}-x)=x$ . Then the area of $\mathcal{P}$ can be as large as $$\Theta=\frac{4\vartheta}{1+\sqrt{1-\vartheta^2}}=0.8064846301_{+}>0.8064733591_{+}.$$ Comments. We may assume that the boundary of a figure is piecewise smooth. The given solution employs a very simple  (""chess-like"") rotation pattern. The problem itself can be altered in at least four ways: The figure is non-convex; The rotation is allowed also in the negative direction, so that the total rotation is still $+360^{\circ}$ for each figure; Each $\mathcal{P}_{m,n}$ has an initial phase $\phi_{n,m}\in[0,2\pi]$ ; There are only $N^{2}$ figures $\mathcal{P}_{m,n}$ , $0\leq m,n\leq N-1$ , for some $N\in\mathbb{N}$ . For example, if we consider non-convex case and $N=2$ , then the constructed octagon-like $\mathcal{P}$ can be augmented with two more circular segments and non-convex parts. To find a rotation pattern for every piece so that all four eventually make a full turn is a simple puzzle: One can further ask whether $N=3$ or $N=4$ already yield the same answer as $N=\infty$ . I do not know the full solution for the initial problem even in the simplest ""chess-like"" rotation pattern case. If somebody can find an improvement to $\Theta$ in any direction (more sophisticated convex figure and/or rotation scheme), that would be great! Since the center of rotation for each figure is fixed, and due to convexity, intuition suggests that this problem must be easier than, say, the Moving Sofa Problem .","['optimization', 'combinatorial-geometry', 'combinatorics', 'geometry']"
4892365,Invariant Semi-Bounded Distributions under Hierarchical Transformation,"I am interested in determining the set of semi-bounded distributions that remain invariant when they are part of a hierarchical model. For example, let \begin{align}
 X\overset{iid}{\sim} p(x; \alpha)\\
 \alpha\overset{iid}{\sim} p(\alpha; \beta)\\
\end{align} then \begin{equation}
 X\overset{iid}{\sim} p(x; \beta)\\
\end{equation} The formulation of this is: \begin{equation}
   p(x, \beta) = \int_0^\infty p(x, \alpha) p(\alpha, \beta) d\alpha
\end{equation} I have not found a general way of solving this, but I have found a specific solution: a variation of the log-normal distribution with median $m$ on the natural scale and standard deviation $\sigma$ , specifically when the median has a hierarchical distribution. \begin{equation}
   p(x; m, \sigma) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\left(-\frac{\ln^2\left(\frac{x}{m}\right)}{2\sigma^2}\right)
\end{equation} So, this solution provides evidence that indeed such distributions do exist but it irks me that I cannot solve this problem more generally. If the community can assist me in understanding how to solve these types of functional problems, I would really appreciate it. UPDATE 1: Certain RV transformations also exhibit this behavior, namely those \begin{align}
 Y &= \alpha X ^ \beta \\
 X &\sim LN(m,\sigma)
\end{align} which provides the following pdf: \begin{equation}
q(y; m, \sigma, \alpha, \beta) = \frac{1}{\sigma |\beta|y\sqrt{2\pi}}\exp\left(-\frac{\left(\ln \alpha + \beta \ln m - \ln y\right)^2}{2\beta^2\sigma^2}\right)
\end{equation} which is still lognormal. Finally, the mixture distribution of lognormal PDFs also meet the criteria, so \begin{equation}
 r = \sum_{j=1}^J w_jq_j(y; m_j, \sigma_j, \alpha_j, \beta_j)
\end{equation} where $0\le w_j \le 1$ and \begin{equation}
\sum_{j=1}^J w_j=1
\end{equation} However, these are only expansions from the original insight that a special case of the lognormal distribution is invariant under hierarchical transformation. If it does not show that the lognormal is the only solution. That remains an open question for me. UPDATE 2: (Under Construction) If we assume the following relationship, we can begin to construct a subset of solutions: \begin{equation}
 p(x;a) = q(x) r(a)
\end{equation} This implies that a parameter $a$ can be factored out of the PDF. Note: this is not an ideal solution as it makes a pretty significant assumption about the structure of a solution, but it is arguably incremental progress. From this, we obtain: \begin{align}
 p(x; \beta) &= \int_0^\infty p(x;\alpha)p(\alpha; \beta) d\alpha\\
 &= \int_0^\infty q(x) r(\alpha) q(\alpha) r(\beta) d\alpha\\
 &= q(x) r(\beta) \int_0^\infty q(\alpha) r(\alpha) d\alpha\\
1 &= \int_0^\infty q(\alpha) r(\alpha) d\alpha\\
1 &= \int_0^\infty p(\alpha; \alpha) d\alpha\\
\end{align} The way I interpret this is that the parameter within the PDF must be interchangeable with the independent variable of the PDF. One example of this is when \begin{align}
  q(x) &= 2 x e^{-x^2}\\
  r(\lambda) &= 2 \lambda e^{-\lambda^2}\\
  &\therefore\\
  p(x; \lambda) &= 4 x \lambda e^{-x^2-\lambda^2}
\end{align} Update 3 (Under Construction): If we build a hierarchical distribution of Pareto distributions where the location parameter is hierarchically distributed, we begin to see a similar pattern to the last hierarchical distribution. Generalizing this, we obtain: \begin{equation}
  p(x; \alpha, \beta) = \frac{1}{N}\sum_{n=1}^N \frac{1}{n!}\alpha^{1+n} \beta^\alpha x^{-1-\alpha} \ln^n \left(\frac{x}{\beta}\right) \text{I}_{(\beta, \infty)}(x)
\end{equation}","['statistics', 'probability-distributions']"
4892383,Verification of Evan's representation formula $e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz$ for $t$ independent $B(y)$,"At the end of chapter 7.3 of Evan's PDE book, Evans states that if $B(y) = \sum_{j=1}^n y_jB_j$ for constant matrices $B_j$ and $\Gamma = \partial B(0,r)$ where $B(0,r)$ is a ball centered at zero in the complex plane with radius $r$ so large that $B(0,r)$ contains all eigenvalues of $B(y)$ , then we have the representation formula $$e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz$$ This formula comes out of the blue, but luckily Evans provides a proof for it. Unluckily, Evans does nothing to motivate the intermediary steps he takes. For example, at first, Evans wants to verify that if $A(t,y)$ is the RHS of the identity, i.e. the integral expression, then $$B(y)A(t,y)x = -\frac{1}{i}\frac{d}{dt}A(t,y)x$$ for all $x\in \mathbb{R}^m$ . He does this by writing: $$B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz\Longleftrightarrow$$ $$B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz\Longleftrightarrow$$ as $\int_{\Gamma}e^{-itz}dz = 0$ . Sure, we can add/subtract zeroes all we like since $\Gamma$ is a closed loop and $e^{-itz}$ is holomorphic. But 1.) Why can we pass $B(y)$ inside the integral? This might be just a definition thing; it has been a some time since I've had to work with multivariate integrals. 2.) Why are $$\frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz$$ and $$\frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz$$ equal to each other? Similar trickery happens a bit later when Evans writes that $$A(0, y)x = \frac{1}{2\pi i}\int_{\Gamma}(zI - B(y))^{-1}xdz = \frac{1}{2\pi i}\int_{\Gamma}\frac{x + B(y)(zI - B(y))^{-1}}{z}dz$$ I have no clue why this equality holds.","['complex-analysis', 'multivariable-calculus', 'functional-analysis', 'real-analysis']"
4892397,Direct proof that $\sum_{x=1}^n \sum_{y=0}^{x-1} \frac{1}{x(n-y)} = \sum_{x=1}^n \frac{1}{x^2}$,"I'm looking for a direct proof that for all $n\in \mathbb N$ , $$\sum_{x=1}^n \sum_{y=0}^{x-1} \frac{1}{x(n-y)} = \sum_{x=1}^n \frac{1}{x^2}.$$ It is easy to prove this relation by induction on $n$ , but I'm struggling to find a direct proof. The relation is so simple that it feels like there should be a short non-inductive proof, potentially using other well-known identities. Note that subtracting $\left(\sum_{x=1}^n \frac 1 x\right)^2$ from both side, the relation is equivalent to $$\sum_{x=1}^n \sum_{y=1}^{n-x} \frac{1}{xy}=\sum_{1\leq x\ne y \leq n} \frac{1}{xy}.$$ So a direct proof of this latter equality would also be sufficient.","['harmonic-numbers', 'summation', 'sequences-and-series']"
4892446,proof of differential is the scalar product with gradient: $D_g(x)(v) = \nabla(g)(x)\cdot v$,"I'm looking for a proof of this classical result for a differentiable multivariate function $g$ : let $D_g(x)$ be the differentiate (I also find total derivative ) of $g$ at point $x$ : $D_g(x)(v) = \nabla(g)(x)\cdot v$ My starting points are the following: A function $g$ is differentiable in $x$ means that there is a linear function (often called also a linear map ) $D_g(x)(v)$ such that $g(x+v) = g(x)+D_g(x)(v)+o(\lVert v\rVert)$ Gradient is defined as the vector of partial derivatives defined as below: $\frac{dg}{dx_i}=\lim_{h\to 0}\frac{g(x+he_i)-g(x)}{h}$ where $e_i$ is the $i^{th}$ vector of the canonical base. The actual property can be splitted in two: if the $g$ is differentiable, then it admits a gradient $D_g(x)(v) = \nabla(g)(x)\cdot v$ I found many ressources for definitions and list of properties, such as this one or this other one but I'm missing a proof. This video seems to takes the property as the definition of differentiability, and then proves the formula for directional derivative. Yet the definition of $D_g(x)$ seems more general (this is, for instance, the one that is used in Rademacher's theorem), and its expression as a scalar product, a consequence. I know it's a very fundamental result of multivariate calculus and even a mere link to a more complete lecture would be appreciated.","['multivariable-calculus', 'derivatives']"
4892449,On the definite integral $\displaystyle\int_{-1}^{1}e^{-\ln^2\left(\frac{1+x}{1-x}\right)}dx$,"On a problem I'm working on, I've come across this integral: $$I=\int_{-1}^{1}e^{-\ln^2\left(\frac{1+x}{1-x}\right)}dx$$ and I'm wondering how to evaluate it analytically. I don't think the context would be much help. A simple substitution $u=\frac{1+x}{1-x}$ yields $$I=2\int_0^{\infty}\frac{e^{-\ln^2(u)}}{(u+1)^2}du$$ But I don't know if this form is any easier to tackle. Any idea?","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4892450,"Formalising the problem and create a proof for the game ""Waffle""","Waffle is an online game at https://wafflegame.net/daily . It consists in moving letters (swapping them) to recreate the original words. While you have 15 moves, it can be done in 10. I usually try to guess the words first and then swap the letters using this logic (assuming either condition exists -- see comment below): If I am swapping a letter to get it in the right place, and the letter is the only one on the board (for example is an ""E"" and there is only one ""E"" that is not already green, but swapping it makes the ""E"" become green, which is in the right position), I swap it (I have no choice as it is the only letter) If I am swapping a letter and there are more of the same letter, I swap it only if both letters being swapped will go to the correct place (if I have two ""E"" that can be swapped, one, for example, with a ""G"" and another with an ""N"", but I also know that if I swap the ""E"" and the ""G"" both become green, then I swap the ""E"" and the ""G""). I believe this game could be formalised in mathematical terms to prove whether that approach will always produce a result in the minimum amount of moves. After all, we are just dealing with an initial set that has had a number of permutations and I want to find the minimum number of ""swaps"" to go back to the original position. Will rules 1) and 2) always produce a final result in the minimum number of moves? (I am asking because I get it in 10 moves, the minimum, about 99% of the times, not 100%, so I wonder if I am missing something). How can we formalise it in mathematical terms and write a proof?","['permutations', 'recreational-mathematics', 'puzzle', 'combinatorics']"
4892451,Convergence of square root of n series,"I'm having trouble determining the convergence/divergence of the following series: $$\sum_{n=1}^{\infty}\sqrt[n]{\frac{1}{n^{n+1}}}$$ I tried to solve it with the comparison test, yet I wasn't able to find the correct substitute: $$\sqrt[n]{\frac{1}{n^{n+1}}}=\frac{1}{n^{\frac{n+1}{n}}}$$ $$\frac{1}{n^2}<\frac{1}{n^{\frac{n+1}{n}}}< \frac{1}{n}$$ Should I use different criteria, or are there better elements to substitute in the inequality?","['convergence-divergence', 'analysis', 'sequences-and-series']"
4892452,Class of functions whose composition with any moment-generating/characteristic function remain MGF/CF,"Are there any known results on the class of functions $f$ for which $f(M_X)$ remains an mgf/cf for any MGF/CF $M_X$ . Elementary examples of such functions are $$f_{c}(x)=c,$$ $$f_{n}(x)=x^n,$$ and nice examples are $$f_{\lambda}(x)=x e^{ \lambda \left(x -1\right)}$$ $$f_{\alpha, \lambda}(x)=\alpha e^{-\lambda}+e^{ \lambda \left(x -1\right)}$$ appeared in this answer ( $\alpha, \lambda,  c>0$ , $n \in \mathbb N$ ). Also, inspired from $f_{\lambda}$ , we have the following function: $$f_{Y}(x)=\mathbb E \left (x^Y \right)=M_Y(\log x) $$ for any non-negative integer-valued random variable $Y$ with finite mgf $M_Y$ over $\mathbb R _{\, \ge 0}$ (e.g., $Y$ with geometric distribution cannot be used here); $f_{\lambda}$ is obtained from $Y$ with Poisson distribution. Indeed, the set of such functions is a convex set. Moreover, the set is closed under multiplication . It seems that every member of this set is obtained from multiplication/convex-combination of a number of the above functions (there may be a set of basic functions that generate all the set). 1) Can we determine the whole set, 2) show some interesting properties of the set, or 3) provide some other nice members, basically different from the above instances? Update: An interesting result provided in @Snoop's answer that characterizes the class of all functions whose composition with any (multivariate) CF remain CF. I think the result is correct (while I am not an expert in complex analysis). Using the results given earlier in the OP, we can see that any infinitely differentiable function $f: [1,\infty) \to [1,\infty)$ with the following three sufficient conditions: The  Maclaurin series of $f$ converges for any $x$ The coefficients of the Maclaurin series are non-negative The coefficients of the Maclaurin series sum to 1 is a function whose composition with any MGF remains an MGF. I think using the result developed for CFs and noting $M_X(t)=\varphi_X(-ti)$ , we can prove the above conditions are also necessary .","['statistics', 'real-analysis', 'complex-analysis', 'functional-analysis', 'probability']"
4892453,Equivalent of logarithms for products of matrices,"Let $a, b \in \mathbb{R}$ . We know that $\log(ab) = \log(a) + \log(b)$ . My question is very straightforward: Is there an equivalent function for one of the two most used matrix products? I.e., does there exist a function $f: M_n(\mathbb{R}) \to M_k(\mathbb{R})$ for some $k$ such that either: $f(MN) = f(M) + f(N)$ ; or $f(M \otimes N) = f(M) + f(N)$ , where $\otimes$ denotes the Kronecker product; I've never heard of any such function, though I believe the matrix exponential does the job when going from Kronecker sums to products, and the same for commuting matrices in the regular product. Perhaps this means at least one of the sums above has to be a Kronecker sum, but it would be even better to do it for the regular sum. Note also that I accept any $k$ , including $k = 1$ , in the definition of $f$ . Does anyone know if a function like that exists? Thanks in advance!","['matrices', 'functional-equations', 'kronecker-product']"
4892466,Solve $\int_{-\infty}^{+\infty}\frac{1}{\cosh x}\ dx$ using residue theory [ANSWERED],"I was trying to solve this exercises which asked to first solve $$I=\lim_{R\to +\infty}\oint_{\Gamma_R}\frac{1}{\cosh z}\ dz $$ where $\Gamma_R=\partial\{z=x+iy\in\mathbb{C}:-R\le x\le R, \ 0\le y\le \pi\}\equiv \partial D$ . From this integral, obtain the following one: $$J= \int_{-\infty}^{+\infty}\frac{1}{\cosh x}\ dx$$ My solution. Consider $f(z) = \frac{1}{\cosh z}$ ; this function, limited to the domain $D$ , is analytical everywhere, besides in the point $z_0=i \frac{\pi}{2}$ , where $\cosh z_0 = 0$ . Hence: $$\lim_{R\to +\infty}\oint_{\Gamma_R}\frac{1}{\cosh z}\ dz = 2\pi i \operatorname{Res} \left(f(z), z_0\right)$$ To calculate those residue, I used the fact that the numerator is non-zero, while the denominator is zero calculated in $z_0$ , so the residue should be given by: $$\operatorname{Res} \left(f(z), z_0\right)=\lim_{z\to z_0}\frac{1}{\frac{d}{dz}\cosh z}\equiv \frac{1}{\sinh\left(i\frac{\pi}{2}\right)}=-i$$ This implies that: $$\lim_{R\to +\infty}\oint_{\Gamma_R}\frac{1}{\cosh z}\ dz =2\pi$$ To obtain the integral $J$ from this result, I noted that: $$\oint_{\Gamma_R}\frac{1}{\cosh z}\ dz = \left(\int_{-R}^{+R} + \int_{C_R}\right)\frac{1}{\cosh z}\ dz$$ where $C_R = \{z=re^{i\theta}\in \mathbb{C} : |z| = R, \ 0\le \theta \le \pi\}$ . Since $$\left\lvert\int_{C_R}\frac{1}{\cosh z}\ dz\right\rvert\le \int_{C_R} \frac{2}{|e^z + e^{-z}|} \ dz \le 2\int_0^{2\pi}\frac{1}{||e^{R \cos\theta}|-|e^{-R\cos\theta}||}\ Rd\theta\le2\frac{R}{|e^{R}-e^{-R}|}\int_0^{2\pi} d\theta$$ converges to $0$ as $R\to +\infty$ because of the fact that $e^{-R} \to 0$ and $\frac{R}{e^R}\to 0$ , it means that: $$\lim_{R\to+\infty}\int_{-R}^{+R}\frac{1}{\cosh z}\ dz\equiv \lim_{R\to+\infty}\int_{-R}^{+R}\frac{1}{\cosh x}\ dx=\mathcal{P}\int_{-\infty}^{+\infty} \frac{1}{\cosh x} \ dx = \lim_{R\to +\infty}\oint_{\Gamma_R}\frac{1}{\cosh z}\ dz =2\pi$$ where $\mathcal{P}$ is the Cauchy principal part of the integral. The problem with this is that checking the result for $\int_{-\infty}^{+\infty} \frac{1}{\cosh x} \ dx$ , this should be equal to $\pi$ and not to $2\pi$ . I tried looking for mistakes, but I can't seem to find any. Can you please help me? Thanks in advance!","['complex-analysis', 'residue-calculus', 'improper-integrals', 'hyperbolic-functions']"
4892481,Limit Piecewise Function Rational and Irrational [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question I have \begin{align} f(x) =
\begin{cases} 
x^2,  & \text{if $x$ is rational} \\
-x^2, & \text{if $x$ is irrational}
\end{cases}
\end{align} I must compute the limit of $f(x)$ as $x$ tends to $0$ . I did $|f(x)| \leq |x|$ for $|x|<1$ , so by the sandwich theorem the limit of $f(x)$ as $x$ tends to $0$ is $0$ . Is this correct?","['analysis', 'real-analysis', 'calculus', 'solution-verification', 'limits']"
4892522,Simplify in closed-form $\sum_n P_n(0)^2 r^n P_n(\cos \theta)$,"Simplify in a closed form the sum $$S(r,\theta)=\sum_{n=0}^{\infty} P_n(0)^2 r^n P_n(\cos \theta)$$ where $P_n(x)$ is the Legendre polynomial and $0<r<1$ . Note that $P_n(0)= 0$ for odd $n$ and $P_n(0)= (-1)^n(2n-1)!!/(2n)!!$ for even $n$ . Further, note that for $\cos\theta = 1$ it simplifies as $$S(r,0)=\sum_{n=0}^{\infty} P_{2n}(0)^2 r^{2n} =\sum_{n=0}^{\infty} \left(\frac{(2n-1)!!}{(2n)!!}\right)^2 r^{2n} = \frac{2}{\pi} K(r)$$ where $K(k)$ is the elliptic integral of the second kind, which is the generating function of $\frac{\pi}{2}P_{2n}(0)^2$ . Is there a way to evaluate the infinite sum for arbitrary $\cos\theta$ ? If not, is there an approximate solution or at least a nice way to express this as a real integral? What is the asymptotics close to $r=1$ ?","['legendre-polynomials', 'closed-form', 'generating-functions', 'elliptic-integrals', 'sequences-and-series']"
4892545,Quotient of Graded Rings $S \to S/f$ inducing Homeomorphism on Proj,"Let $A$ be a Noetherian ring, and let $X$ be a closed subscheme of of $\mathbb{P}^r_A$ . We define the homogeneous
coordinate ring $S:=S(X)$ of $X$ for the given embedding to be $A[x_0, ..., x_r]/I$ (with induced grading),
where $I=I(X)$ is some homogenous ideal defining $X$ . (note that there are many homogeneous ideals $I(X)$ defining same closed $X \subset \mathbb{P}^r_k$ , but there exist a unique saturated $I^{\text{sat}}$ one; the ""biggest"" one with resp. having $X$ as resulting scheme; cp with question). Let $f \in S_d \subset S$ (i.e. $f$ of degree $d$ ) be a non zero homogeneous element such that the canonical quotient map $S \to S/(f)$ of graded rings induces a homeomorphism $\text{Proj} \ S/(f) \to \text{Proj} \ S $ of underlying proj schemes. Question: What can we say about this $f \in S$ ? More concretely, can we decide when is it a zero divisor or even nilpotent? (Note, that in affine situation it's always nilpotent - see also below how this fact interplays with ""projective"" situation  - so here I want to understand the analogous situation on level of Proj construction & homogeneous coordinate ring instead of Spec). I'm primary interested in following three cases: for arbitrary homogenous ideal $I \subset A[x_0, ..., x_r]$ for ideal $ I$ saturated : ie if for $f \in A[x_0, ..., x_r]$ there exist for each $i=0,1,...,r$ a positive integer $n_i$ such that $ f \cdot x_i^{n_i} \in I$ , then $f \in I$ with $I$ is saturated as in 2. and additionally $X$ is integral (=reduced and irreducible) scheme (The last question closely related with this question and I strongly conjecture in case $A=k$ field under assumptions in 3. one can show that $S$ is a domain; what would be equivalent that every such $f$ must be zero Let compare it firstly with affine situation with a fing $R$ , then if $f \in R$ such that $R \to R/(f)$ induces homeomorphism on specs, then $f$ is nilpotent. From this we can conclude that for every $s \in S_1$ the element $f/s^d \in S_{(s)}$ (inside the degree- $0$ part of the localization) is nilpotent. And if I'm not missing something, then this would already suffice to show that initial $f$ is nilpotent as element in homogenoeus ring $S$ , as we have natural inclusion $S_{(s)} \subset S_s $ and inside the latter $f= (f/s^d) \cdot s^d$ is by above nilpotent, and so we can conclude that $f$ is nilpotent in $S$ . Firstly, is the argumentation correct? If yes, then we can answer the 3 questions as follows: For (1) we can only state that it's nilpotent, nothing more. Can we say something more in case (2) if the ideal $I$ is saturated? The case (3) seems to be more restructive: As $S_{(s)}$ is domain, this implies that $f/s^d$ is zero or equivtly that $s^d \cdot f=0$ (by univ prop of localization) for every $s \in S_1$ . But this already implies (since $S$ Noetherian) that $f \in I$ as $I$ is assumed to be saturated, so $f=0$ in $ S$ . Let's come back to my concern in If I'm haven't overseen some mistake in my reasonings, then it seems that the case (3) answers the question if for integral $X$ the homogeneous ring $S$ in construction is a domain: If not, there exist two (wlog homogeneous; otherwise take summands of highest degrees) nonzero $f,g \in S$ with $f \cdot g=0$ . Then, on level of topology (as a scheme carries an underlying top space) we have $X= \text{Proj} \ S= \text{Proj} \ S/(f \cdot g)= \text{Proj} \ S/(f) \cup \text{Proj} \ S/(g)$ . As $X$ irreducible, wlog $X=\text{Proj} \ S/(f)$ on level of topological spaces. Then applying case (3) from above we deduce $f=0$ . Does the argument work? (I have bad feeling with ""topological"" part above - especially does the last equation hold in general setting - and because Hartshorne in the Exercise II.5.14(a) from question linked above explicitly assumes $A=k$ algebraically closed (...presumably he intended to use Nullstellensatz) while the approach I presented above not exploits it. So I'm sure, probably my approach is flawed. So finally conflating my original concern about properties of $f \in S$ with it's connection to Hartshorne's exercise in linked question in the way I tried to explain so far, are my considerations exposed above on posed questions correct or are there some (serious/ irreparable?) error(s) hidden? Especially, do we really need $k$ to to assume to be algebraically closed in the quoted Exercise as Hartshorne requires (that which is particularly gnawing in the story)","['graded-rings', 'commutative-algebra', 'projective-schemes', 'algebraic-geometry', 'schemes']"
4892558,A line bundle $L$ can be assumed to be ample (after eventually passing to its dual),"I'm reading Huybrechts' Lectures on K3 Surfaces and I got stuck reading example 2.3.9, which shows that any K3 surface $X$ with $\operatorname{Pic}(X)=\mathbb{Z}\cdot L$ and such that $(L)^2=4$ can be realized and a quartic $X \subset \mathbb{P}^3$ . His argument starts as follows: We may assume that $L$ is ample (after passing to its dual if necessary). Why is this true? There's yet another fact that I dont' understand: All curves in |L| are automatically irreducible, as $L$ generates $\operatorname{Pic}(X)$ . Hints are appreciated.","['complex-geometry', 'picard-group', 'k3-surfaces', 'algebraic-geometry', 'line-bundles']"
4892590,What is the image of a smooth map?,"Let $f: S^2 \to \mathbb{R}^n$ be a smooth map from the two-dimensional sphere to euclidean space. Let $X = \mathrm{Im}(f) \subset \mathbb{R}^n$ be the image topological space (note: the quotient topology induced from $S^2$ and the subspace topology induced from $\mathbb{R}^n$ coincide here). I am interested in knowing what properties this topological space has. Ideally, I would like $X$ to admit the structure of a $2$ -dimensional CW-complex. From the comments in this question , this seems highly unlikely. But maybe it is at least homotopic to a CW-complex. To this end, I came across the notion of Euclidean neighbourhood retracts. The characterisation at the end of these slides imply that it suffices to prove that $X$ has sufficient local connectivity properties. But I have no idea how to prove this. I'm not quite sure where to look for answers to these questions and so any references to relevant literature would be welcome.","['metric-spaces', 'geometric-topology', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4892592,Why does a product $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i})$ lead to infinity?,"I asked Wolfram Alpha to compute the following limit : $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i})$ and the answer was $\infty$ . I do not understand how this can be, as all elements in the product are smaller than 1. For comparison, when I asked about the following limit : $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+i})=1/2$ . What is the real value of these limits?","['limits', 'wolfram-alpha', 'products']"
4892671,Randomly choose $n$ numbers between $0$ and $1$. Add the smallest number from each repetition until the total is greater than 1. How many repetitions?,I saw this question which inspired my question. Choose $n$ uniformly random numbers between $0$ and $1$ and record the value of the smallest number. Do this again and add the second number to the first number. Keep doing this until the sum of the numbers exceeds $1$ . How many numbers are expected to be needed? I have not studied probability so I don't really know how to proceed with this. I looked at the answers to this question where the answer for $n=1$ is shown to be $e$ but I couldn't get a good enough grasp of the concepts to try them out in my problem. I also looked here but didn't understand it well enough either.,"['expected-value', 'probability-theory', 'probability']"
4892683,Range of the function $ f(x) = \frac{1}{1 - 2 \cos(x)}$,"My textbook has this question: Range of $ f(x) = \frac{1}{1 - 2 \cos(x)}$ is: (A) $ [ \frac{1}{3}, 1]$ (B) $ [ -1, \frac{1}{3}]$ (C) $ (-∞, -1] \cup [\frac{1}{3}, ∞] $ (D) $ [\frac{-1}{3}, 1] $ To solve this, I first did: $$ -1 \leq \cos(x) \leq 1 $$ Ultimately, I wrote: $$ -1 \leq 1 - 2 \cos(x) \leq 3 $$ After that shouldn't it become the following? $$ \frac{1}{3} \leq \frac{1}{1 - 2 \cos(x)} $$ and $$ \frac{1}{1 - 2 \cos(x)} \leq -1 $$ This would make option (C) correct, but my textbook says option (B). Why?","['inequality', 'functions', 'trigonometry']"
4892685,"How do we measure the ""holes"" (zeros) of the set of $\Bbb{Z}$-linear combinations of $f_i : G \to G$ where $G$ is an abelian group?","Question: Let $G$ be a normed abelian group .  Namely the triangle inequality holds $|g + h|\leq |g| + |h|$ .  An example would be $G = \Bbb{Z}$ together with $|\cdot| =$ absolute value. Now suppose that you have a finite set of functions $f_i : G\to G, i = 1..n$ , which are typically not group homomorphisms in applications.  Now take the space of $\Bbb{Z}$ -linear combinations of the $f_i$ , $X = \{ \sum_{j=1}^m \lambda_j f_j : \lambda_j \in \Bbb{Z}\}$ .  This forms an abelian subgroup of simply the set of all functions $\{f: G \to G\}$ under pointwise $+$ . Can we use group theory and / or topology to compute $$h = \min\{ |x| : 0 \neq x \in G; \forall f \in X,f(x) = 0\}$$ ? If not, then would the Number-theoretic Transform tell us anything (especially because we can assume periodicity; see below)? And if not to that, then
does this seem like a problem that Group Cohomology could address? Close upper bounds will work as well for my application, which is to prime numbers. Random thought: is there some sort of Zariski topology for abelian groups that has to do with general functions and not only polynomials? More Information: Take $G = \Bbb{Z}$ and let the $f_i : G \to G$ be $f_i(x) = \delta_0(x \mod i), i=1..n$ , so that the $f_i$ are essentially Dirac combs .  Then $h = p_{\pi(n) + 1} = \text{ a prime number}$ .  With certain choice of $f_i$ the above formulation will also apply to $2k$ -separated prime numbers ( $k=1$ is twin primes), in which case $h$ will be the least $2k$ -separated prime average that is greater than $n$ , for example.  Because of this formulation's generality, of course it easily applies to other open questions in NT. Thus, for these prime gap problems, the $f_i$ are all periodic with readily determined periods.  So, you can add that on as a hypothesis in the above question.","['prime-numbers', 'periodic-functions', 'prime-gaps', 'group-theory', 'algebraic-topology']"
4892686,Find all holomorphic functions $f=u+iv$ such that $g=u^2+iv^2$ is holomorphic,"Let $\Omega\subset \mathbb{C}$ be a complex region. Find all holomorphic functions $f=u+iv$ in $\Omega$ such that $g=u^2+iv^2$ also be holomorphic in $\Omega$ . My attempt: We know that Cauchy-Riemann equations for $f$ and $g$ must be satisfied in $\Omega$ , i.e. \begin{equation}\label{eq:CR-1}
		\Biggl\{
		\begin{aligned}
			u_x&=v_y,\\
			u_y&=-v_x,
		\end{aligned}
	\end{equation} and, \begin{equation}\label{eq:CR-2}
		\Biggl\{
		\begin{aligned}
			2uu_x&=2vv_y\\
			2uu_y&=-2vv_x
		\end{aligned}
		\, \Longleftrightarrow \, \Biggl\{
		\begin{aligned}
			uu_x&=vv_y\\
			uu_y&=-vv_x
		\end{aligned}
	\end{equation} From which we get \begin{equation}\label{eq:CR-3}
			\Biggl\{
		\begin{aligned}
			uu_x&=vu_x\\
			uu_y&=vu_y.
		\end{aligned}
			\, \Longleftrightarrow \,
		\Biggl\{
	\begin{aligned}
		(u-v)u_x&=0\\
		(u-v)u_y&=0
	\end{aligned}
\end{equation} I'm stuck in the last part because if I assume $u\neq v$ , for all $z\in \Omega$ then I get that $f$ must be constant. Same case if I assume that $u=v$ , for all $z\in \Omega$ . But, what happens for example if $u\neq v$ for some $z\in \Omega$ and at the same time $u=v$ for other values in $\Omega$ , or maybe this case is not possible?",['complex-analysis']
4892688,"If $f'' + f = 0$ and $f(0) = f'(0) = 0$, then $f = 0$.","Let $f \in D^{2}(\mathbb{R})$ verifying $f''+f = 0$ . Prove that if $f(0) = f'(0) = 0$ , then $f(x) = 0$ for all $x \in \mathbb{R}$ . Also, show that in any case $f(x) = f(0)\cos x + f'(0)\sin x$ . My attempt: Let $g:\mathbb{R} \to \mathbb{R}$ be defined by $g(x) = (f(x))^{2}+(f'(x))^{2}$ . Then $g \in D(\mathbb{R})$ and $$
g'(x) = 2f(x)f'(x) + 2f'(x)f''(x)\\
      = 2f'(x)(f(x)+f''(x))\\
      = 0,
$$ for all $x \in \mathbb{R}$ . Therefore $g$ is constant, so there exists some $c\in\mathbb{R}$ such that $g(x) = c$ for all $x \in \mathbb{R}$ . In particular, $$
c = g(0) = (f(0))^{2}+(f'(0))^{2} = 0.
$$ Thus, $$
(f(x))^{2}+(f'(x))^{2} = 0,\ \text{for all}\ x \in \mathbb{R}.
$$ And this implies that $$
f(x) = 0
$$ for every $x \in \mathbb{R}$ . Now suppose that $f(0) = b$ and $f'(0) = a$ for some $a,\ b \in \mathbb{R}$ and $f'' + f = 0$ . Consider $h:\mathbb{R} \to \mathbb{R}$ defined by $h(x) = f(x) - a \sin x - b \cos x$ . Then $$
h'(x) = f'(x) - a \cos x + b \sin x,\\
h''(x) = f''(x) + a \sin x + b \cos x.
$$ Hence, $$
h(x) + h''(x) = 0
$$ for every $x$ . Moreover, $$
h(0) = f(0) - a \sin 0 - b \cos 0 = b-b = 0,\ h'(0) = f'(0) - a \cos 0 + b \sin 0 = a-a = 0.
$$ So, by the previous result $h = 0$ , which says that $$
f(x) = a \sin x + b \cos x = f'(0) \sin x + f(0) \cos x
$$ for all $x \in \mathbb{R}.$ I´m not 100% sure if I can make the assumption that $f(0) =b,\ f'(0) = a$","['calculus', 'derivatives', 'real-analysis']"
4892697,How can I move Expectation into Taylor Series for $e^{\lambda X}$,"I see the following all the time in statistics: $$E[e^{\lambda X}] = E[\sum_{k = 1}^\infty \lambda^kX^k/k!] = \sum_{k = 1}^\infty \frac{\lambda^k}{k!}E[X^k]$$ where $X: \Omega \to R$ is a random variable on a sample space $\Omega$ . I would like to know how to justify this in detail. Things I've tried Monotone Convergence Theorem and Dominated Convergence Theorem don't seem to apply, because the partial sums are alternating and not dominated. The closest I have gotten is by noting that the radius of convergence of the power series for $e^x$ is $R = \infty$ ,so we have uniform convergence of the series on any bounded set. This yields something like (letting $\lambda =1$ ): $$
E[e^X] = \int_\Omega e^{X(w)}dP(w) = \lim_{L \to \infty}\int_{\Omega_L}\sum_k \frac{X(w)^k}{k!}dP(w) = \lim_{L \to \infty}\sum_k \int_{\Omega_L}\frac{X(w)^k}{k!}dP(w)
$$ where $\Omega_L = \{w : X(w) < L\}$ If we could move the limit into the series on the righthand side, we'd be done. The final expression can be written as an iterated integral wrt counting measure $$\lim_{L \to \infty}\int_Ng_L(k)d\mu(k)$$ with $g_L(k) = \int_{\Omega_L}\frac{X(w)^k}{k!}dP(w)$ . If all moments of $X$ are finite then I think Dominated Convergence lets us interchange the limit and integral as follows. $|g_L| \leq \int_\Omega \frac{|X(w)|^k}{k!}dP(w) =: f(k)$ , and by Tonelli's theorem: $$
\int_Nf(k)d\mu(k) = E[e^{|X|}] < \infty
$$ This kind of argument seems highly overcomplicated, and, in any case, I don't think it should be necessary to assume all moments are finite. If someone could set me straight here, I'd really appreciate it.","['measure-theory', 'probability']"
4892745,Differential equation with Fourier transform,"It is given the following differential equation: $$-\frac{d^2 x}{dt^2} + \frac{dx}{dt} = \theta(t) e^{-\varepsilon t}$$ where $\theta(t)$ is the Heaviside function (which equals $0$ for $t<0$ and equals $1$ for $t\ge 0$ ) and $\varepsilon$ is a real parameter which is always positive. Write the equation that the $\tilde{x}=\mathscr{F}[x]$ satisfies and find the most general solution (in the distribution sense). By anti-transforming, find a solution $x(t)$ to the differential equation, which must satisfy the contour condition of $\lim_{t\to -\infty} x(t) = 0$ . Find the most general solution $x(t)$ which satisfies the previous contour condition. Note: $\mathscr{F}$ represents the Fourier transform; $\tilde{x}(\omega)$ is the Fourier transform of $x(t)$ . Moreover, I will denote distribution associated to a certain function with the function itself. My solution: I started by Fourier-transforming the given differential equation to find the equation satisfied by $\tilde{x}(\omega)$ . Using the properties of the Fourier transform, I wrote that: $$- (-i\omega)^2 \tilde{x} (\omega) + (-i\omega)\tilde{x}(\omega) = \mathscr{F}[\theta(t) e^{-\varepsilon t}]$$ To evaluate the F-transform of the right-hand term, I calculated the integral directly: $$\mathscr{F}[\theta(t) e^{-\varepsilon t}] = \int_{-\infty}^{+\infty} \theta(t) e^{i\omega t} e^{-\varepsilon t} \ dt = \int_{0}^{+\infty} e^{t(i\omega - \varepsilon)}\ dt=\left[\frac{e^{t(i\omega-\varepsilon)}}{i\omega - \varepsilon}\right]_{0}^{+\infty}=-\frac{1}{i\omega - \varepsilon}$$ Hence: $$\tilde{x}(\omega) = -\frac{1}{(i\omega-1)(\omega^2-i\omega)}$$ This shouldn't be the most general solution; to obtain that, I either need to anti-transform this, which I don't know how to, or I need to solve the homogeneous differential equation $-x''(t) + x'(t) = 0$ and F-transform to add it to the $\tilde{x}(\omega)$ found above. By doing this, I should get $x(t) = A e^t + B$ , with $A$ and $B$ two real constants, but it isn't Fourier-transformable. What am I missing? If you could help me solving my doubts, I'll try to finish it by myself, but if you could provide the entire solution will be much more appreciated; I feel like I need to get more experience with this kind of exercise and, unfortunately, I don't have solutions to this. I'm sorry for having to ask you to solve this problem without even giving you a full solution to this, but I really don't know how to proceed. Thanks in advance for your help, I really appreciate it! EDIT 1: By using Jordan's lemma to calculate the anti-transform and obtain the particular solution of the differential equation, I get: $$x_P(t) = \left(\frac{e^{-\varepsilon t} }{\varepsilon (\varepsilon +1)} + \frac{1}{2 \varepsilon } \right) \theta (t) + \left(\frac{e^{t} }{1-i\varepsilon }+\frac{1}{2\varepsilon }\right) \theta (-t)$$ The problem with this is that the contour condition of the second point is never satisfied, which leads me to think that I did something wrong.","['fourier-analysis', 'fourier-transform', 'ordinary-differential-equations', 'distribution-theory']"
4892826,Prove that a point must be inside circumcircle,"Consider circumcircle $S$ of $\triangle abc$ and a point $v$ such that (i) $|\bar{va}| < |\bar{ab}|,|\bar{ac}|$ and (ii) $|\bar{vb}| < |\bar{ab}|,|\bar{bc}|$ and (iii) $|\bar{vc}| < |\bar{ac}|,|\bar{bc}|$ .  Prove that $v$ must be in the circumcircle $S$ . This's related to Delaunay triangulation and relative neighborhood graph. Attempt. Intuitively $\angle avb > \angle acb$ becuase $\bar{ab}$ is common to $\triangle avb$ and $\triangle acb$ and the other 2 sides of $\triangle avb$ are shorter that those of $\triangle acb$ . If indeed $\angle avb > \angle acb$ and both are subtended by same arc, then $v$ must be inside $S$ by the following fact If they are not subtended by same arc, then applying above procedure again on $\angle avc$ should work. But I cannot prove $\angle avb > \angle acb$ just by inspecting cosine formula.",['geometry']
4892898,1000 balls and 100 boxes,"There are $1000$ balls labelled $000,001,...,999$ and $100$ boxes labelled $00,01,...,99$ . A ball is allowed to put in a box if the number of the box could be obtained by removing one digit from the number of the ball. For example, ball 134 could be put into box $13$ , $14$ or $34$ . Show that we need at least $50$ boxes for all balls to fit in. I have constructed a way to put $1000$ balls into $50$ boxes: Choose the boxes with number $xy$ where $|x-y|$ equals $0,2$ or $4 \pmod{10}$ . That is, 00, 11, 22, 33, 44, 55, 66, 77, 88, 99
02, 13, 24, 35, 46, 57, 68, 79, 80, 91
04, 15, 26, ...                   , 93
06, 17, 28, ...                   , 95
08, 19, 20, ...                   , 97 Now I struggle on proving $49$ boxes is not enough. I attempt to find some balls that need to go into different boxes such as $123,147,789$ but it is nowhere near $50$ . Seems like it requires some case work but I don't see a delicate way to do it. Thanks in advance.","['pigeonhole-principle', 'combinatorics']"
4892919,Reference for Surface area measure,"Can someone please help me with some good references on Surface area measures for open, connected sets? What I meant by this is the following:
Suppose $\Omega$ is a bounded open, connected set in $\mathbb R^n.$ Then Surface area measure is a radon measure on the boundary of the set. I don't know what the exact definition is but I read somewhere it coincides with the $n-1$ - Hausdroff measure when the boundary is a $C^1$ -smooth manifold. I want to know what the most general setting is for them to be well defined.","['measure-theory', 'geometric-measure-theory', 'differential-geometry']"
4892923,Finding determinant of a $n\times n$ matrix.,"Let $A_{n\times n}$ = $((a_{ij}))$ $n\geq {3}$ , where $a_{ij}=(b_i^2-b_j^2)$ , $i,j=1,2,\ldots ,n$ for some distinct real numbers $b_1,b_2,\ldots ,b_n$ . Then what is $\det(A)$ ? Clearly the matrix $A$ is skew-symmetric and if $n$ is odd then the determinant is zero. I want to prove that for any $n\ge4$ , $\det(A)$ is $0$ .
For the case when $n=4$ , I have calculated the determinant by ordinary method. I want to generalize it, please someone help.
Thank you.","['matrices', 'determinant', 'linear-algebra']"
4892936,Maximum of Gaussians error check: Expectation of maximum of folded-normally distributed random variables,"I am trying to show that if $Z_1, Z_2, \ldots, Z_n\sim\mathcal{N}(0, \sigma^2)$ are i.i.d, then $\mathbb{E}[\max(|Z_1|, |Z_2|, \ldots, |Z_n|)]\leq \sigma\sqrt{2\log (2n)}$ . I tried to use the tail bound: $$\mathbb{E}[\max(|Z_1|, |Z_2|, \ldots, |Z_n|)]=\int_0^{\infty}\mathbb{P}(\max(|Z_1|, |Z_2|, \ldots, |Z_n|)>t)dt$$ However, we see that $$\mathbb{P}(\max(|Z_1|, |Z_2|, \ldots, |Z_n|)>t) = \mathbb{P}\left(\bigcup_{i=1}^n\{|Z_i|>t \}\right)\leq \sum_{i=1}^n\mathbb{P}(|Z_i|>t)\leq 2ne^{-t^2/2\sigma^2}$$ so $$\mathbb{E}[\max(|Z_1|, |Z_2|, \ldots, |Z_n|)]\leq 2n\int_0^{\infty} e^{-t^2/2\sigma^2}dt = 2n\sigma\sqrt{2/\pi}$$ which is of course, not the bound I want. Is there any way I can change some of these steps differently to get the bound I want? (Edit: I already looked at this question Expectation of the maximum of gaussian random variables , but the top answer suggested you could also approach this by a union bound and tail bound approach rather than the very nonintuitive exponential approach, so that was what I was trying to emulate.)","['statistics', 'normal-distribution', 'probability']"
4892966,"Confidence interval for parameter of normal distribution $X_i\sim N(\theta,\theta^2)$ with equal mean and standard deviation","A sample $X_1,\dots,X_n$ is drawn from the normal distribution $N(\theta,\theta^2)$ . I am asked to find a $90\%$ confidence interval for the population mean $\theta$ . Let $X_i\sim N(\theta,\theta^2)$ with $$\mathbb{E}(X_i)=\theta \text{ and } \mathbb{V}(X_i)=\theta^2$$ then the random variable $\bar{X}$ has $\mathbb{E}(\bar{X})=\theta \text{ and }\mathbb{V}(\bar{X})=\frac{\theta^2}{n}$ so, by virtue of the CLT we have that $$\bar{X}\sim N\Big(\theta,\frac{\theta^2}{n}\Big)$$ Now, standardizing we get $$\frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}\sim N(0,1)$$ If we are asked to give a $90\%$ confidence interval for $\theta$ and we know that the $90\%$ confidence interval for a $N(0,1)$ is $$(-1.64,1.64)$$ we can see that $$-1.64<\frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}<1.64$$ $$-\frac{1.64}{\sqrt{n}}<\frac{\bar{X}-\theta}{\theta}<\frac{1.64}{\sqrt{n}}$$ $$1-\frac{1.64}{\sqrt{n}}<\frac{\bar{X}}{\theta}<1+\frac{1.64}{\sqrt{n}}$$ $$\frac{1-\frac{1.64}{\sqrt{n}}}{\bar{X}}<\frac{1}{\theta}<\frac{1+\frac{1.64}{\sqrt{n}}}{\bar{X}}$$ $$\frac{\bar{X}}{1+\frac{1.64}{\sqrt{n}}}<\theta<\frac{\bar{X}}{1-\frac{1.64}{\sqrt{n}}}$$ is my confidence interval, right? I wanted to be sure that my result was correct and so I wanted to know your oppinion. Have I done everything correctly? Thank you.","['statistics', 'central-limit-theorem', 'confidence-interval', 'normal-distribution', 'probability']"
4893005,Question on gap in proof of Fermat's Last Theorem,"I just stumbled upon slides by Kevin Buzzard from 2020. (For context, he is a renowned mathematician who has famously recently begun formalizing the proof of Fermat's last theorem in Lean.) Unfortunately, I did not find a recording of his talk. Slide 14 caught my attention: $\hspace{2em}$ This greatly surprised me, since I previously thought that Wiles' fix of the famous gap in his proof was beyond reproach. It is the first time that I am hearing about doubt being cast upon the final 1995 version of his proof. This leads me to the following questions: how high is our credence in the correctness of the currently published proof of Fermat's last theorem? Is it still based on work by Gross on Hecke operators that is supposedly incomplete, or has this second gap been fixed? If so, how? I would greatly appreciate being pointed in the right direction, as an outsider to this field.","['number-theory', 'reference-request']"
4893052,Conjecture: Two different random triangles (both based on random points on a circle) have the same distribution of side length ratios.,"On a circle, choose three uniformly random points $A,B,C$ . Triangle $T_1$ has vertices $A,B,C$ . The side lengths of $T_1$ are, in random order, $a,b,c$ . Triangle $T_2$ is formed by drawing tangents to the circle at $A,B,C$ . ( $T_2$ may or may not lie inside the circle.) The side lengths of $T_2$ are, in random order, $d,e,f$ . Is the following conjecture true: The distribution of ratios of $a,b,c$ is the same as the distribution of ratios of $d,e,f$ . So for example, if my conjecture is true, then we would have the following probability equations: $P(a^2+b^2<c^2)=P(d^2+e^2<f^2)$ $P(a^3+b^3<c^3)=P(d^3+e^3<f^3)$ $P(ab<c^2)=P(de<f^2)$ $P\left(\frac1a+\frac1b<\frac1c\right)=P\left(\frac1d+\frac1e<\frac1f\right)$ (My conjecture, if true, would not imply that $P(ab<c)=P(de<f)$ , because these inequalities are not dimensionally homogeneous.) Basis for my conjecture I found that $P(ab<c^2)=\frac35$ . Then I wondered what $P(de<f^2)$ equals, and a simulation suggests that it also equals $\frac35$ . I found that $P\left(\frac1a+\frac1b<\frac1c\right)=\frac15$ . Then I wondered what $P\left(\frac1d+\frac1e<\frac1f\right)$ equals, and a simulation suggests that it also equals $\frac15$ . I tried some other probabilities, and simulations suggest that each probability has the same value for the two triangles. So I suspect that the two triangles have the same distribution of ratios of side lengths. But I don't know why.","['conjectures', 'geometric-probability', 'circles', 'geometry', 'triangles']"
4893060,Find the limit $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$,"I'm working through Advanced Calculus: Theory and Practice by John S. Petrovic and is currently working on problem 3.5.9, which is as follows: Find the limit and give a strict “ε − δ” proof that the result is
correct: $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ There are other threads which discuss the topic around finding the limit of the function ( $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ ), but do not supply a epsilon-delta proof that it is correct. Also, I have a particular question regarding the solution to this limit (I'll get to that). The most straight forward solution to $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ that I can find (and understand) goes like this: Note, $\sin(a) - \sin(b) = 2\cos(\frac{a+b}{2})\sin(\frac{a-b}{2})$ , therefore: $\sin(\sqrt{x+1}) - \sin(\sqrt{x}) = 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2})=$ $2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2}\frac{\sqrt{x+1}+\sqrt{x}}{\sqrt{x+1}+\sqrt{x}})=$ $2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ . At this stage the argument I find is this (which I'm doubtful of), because: $\lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0 \implies$ $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0$ , however does that really make sense? Using $\lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0$ imply that $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=$ $\lim_{{x \to \infty}} 2 \cdot\lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2}) \cdot \lim_{{x \to \infty}} \sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ . But $\lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})$ does not exists since $\lim_{{n \to \infty}} \cos(2n\pi)=1 \neq \lim_{{n \to \infty}} \cos(\frac{\pi}{2}+2n\pi)=0$ . So my reasoning is that according to the field axioms $0$ multiplied with any number equals $0$ , but $0$ multiplied with something undefined is not equal to $0$ . Therefore we must rely on the Squeeze theorem. $-2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ , where: $\lim_{{x \to \infty}} -2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0$ $\lim_{{x \to \infty}} 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0$ And therefore we have that $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0$ Does my argument make sense or is it unnecessary to use the Squeeze theorem here? Now, could someone please help explain to me how I define $M$ as an expression of $\epsilon$ so that I can make an ""epsilon-delta"" proof of the kind: For each $\epsilon>0$ , there exists a $M=...>0$ such that $|f(x)-L|<\epsilon$ whenever $x>M$ . Thank you.","['limits', 'limits-without-lhopital', 'epsilon-delta']"
4893061,Determine the length of the rod that can be inscribed in a cuboid,"Question You have a cuboid of dimensions $2a \times 2b \times 2c $ .  I want the find the (maximum) length of the right circular cylindrical rod of radius $r$ , that can inscribed in the cuboid. Use whatever means, analytic or numerical, to find this length. Note:  A similar question concerning inscribing a square bar in a cuboid is addressed here . My initial thoughts: We have to find the direction of the axis of the cylinder (the rod), and with an assumed length, we have the base (which is of known radius) touching the three adjacent intersecting faces of the cuboid near the end the of the rod.  The unknowns here are spherical coordinates angles $\theta$ and $\phi$ specifying the direction of the axis, and in addition to that, the length of the rod.  That's it, only three unknowns. To that end, let the cuboid be centered at the origin, its faces that intersect in the first octant at $x = a , y = b , z = c $ .  Let the length of the cylinder be $L = 2 \ell$ .  The unit vector direction of the axis of cylinder which passes through the center of the cuboid (the origin) can be parameterized as follows $ A = ( \sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta ) $ Two vectors that are orthogonal to $A$ are $ u_1 = (\cos \theta \cos \phi, \cos \theta \sin \phi, - \sin \theta)$ $u_2 = (- \sin \phi, \cos \phi, 0 ) $ The top base of the cylinder is described by $ P(t) = \ell A + r u_1 \cos t + r u_2 \sin t $ And we want this circle to be tangent to the plane $x = a$ , therefore, we want $P_x(t) = \ell A_x + r u_{1x} \cos t + r u_{2x} \sin t $ to have a single solution to $P_x(t) = a $ This implies that $ a - \ell \sin \theta \cos \phi = r \sqrt{ \cos^2 \theta \cos^2 \phi + \sin^2 \phi } $ Similarly for the other two faces, we have $ b - \ell \sin \theta \sin \phi = r \sqrt{ \cos^2 \theta \sin^2 \phi + \cos^2 \phi} $ and $ c - \ell \cos \theta = r \sin \theta $ This is a system of $3$ (nonlinear) equations in the three unknowns $ \theta, \phi, \ell $ .  They can be solved numerically quite easily using the well-known Newton-Raphson multi-variate method, provided that you have a good initial guess of the unknowns vector $X = [\theta, \phi, \ell ] $ . This is what I have as a answer to this question.  I appreciate other answers, or comments on the question and the attempted answer.","['analytic-geometry', 'linear-algebra', 'geometry', 'parametric']"
4893068,How to generalize curvature to n dimensions parameterized by time instead of arc length?,"I am a novice in mathematics in general and even more so in differential geometry. Currently, I am looking to generalize the Frenet-Serret formulas to $n$ dimensions. At the moment, I am interested in the generalization of curvature to n dimensions. On this Wikipedia page , a formula for generalized curvature is proposed, which is as follows: $$
X_i(s)=\frac{\langle e'_i(s),e_{i+1}(s)\rangle}{\|r'(s)\|}
$$ On the same page, $$e_j(s) = \frac{\bar{e}_j(s)}{\|\bar{e}_j(s)\|}$$ with $$\bar{e}_j(s) = r^{(j)}(s) - \sum_{i=1}^{j-1} \langle r^{(j)}(s),e_i(s)\rangle e_i(s)$$ . The problem, for me, is that both $X_i(s)$ and $e_j(s)$ are parameterized by the arc length $s$ . What I am looking for is rather than these expressions being expressed as a function of time $t$ . My question is, how do I transition from $X_i(s)$ and $e_j(s)$ to $X_i(t)$ and $e_j(t)$ ?","['curves', 'multivariable-calculus', 'applications', 'differential-geometry']"
4893090,Prove $\int_0^\pi\arcsin(\frac14\sqrt{8-2\sqrt{10-2\sqrt{17-8\cos x}}})dx=\frac{\pi^2}{5}$.,"There is numerical evidence that $$\int_0^\pi\arcsin\left(\frac14\sqrt{8-2\sqrt{10-2\sqrt{17-8\cos x}}}\right)dx=\frac{\pi^2}{5}.$$ How can this be proved? Context In another question , three random points on a circle are chosen, and tangents to the circle at those points form a triangle, with side lengths $d,e,f$ . The relevant question here is to prove that $P(de<f^2)=\frac35$ . Assuming the radius is $1$ , we have $d=\tan X-\tan(X+Y)$ $e=\tan Y-\tan(X+Y)$ $f=\tan X+\tan Y$ $P(de<f^2)=P((\tan X-\tan(X+Y))(\tan Y-\tan(X+Y))<(\tan X+\tan Y)^2)$ I rotated the graph $45^\circ$ by letting $X=y-x$ and $Y=y+x$ . Then Wolfram gave me $y$ as a function of $x$ . After some minor simplifications, I ended up with the integral in this question. My attempt Approachzero does not turn up anything similar. Wolfram does not evaluate the anti-derivative. Here is the graph of $y=\arcsin\left(\frac14\sqrt{8-2\sqrt{10-2\sqrt{17-8\cos x}}}\right)$ . I tried applying some general advice about related integrals, but I suspect this one may be out of my league.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'closed-form']"
4893178,Proof if X is a Hausdorff-separable topological space,"I would like to prove that if X is a separable topological space and Hausdorff, then $|X|\leq |\mathcal{P}(\mathcal{P}(\mathbb{N}))|$ . My idea: Since $X$ is separable, there is a dense countable $D\subset X$ . For each $x \in X$ , let $B_x$ be a fundamental system of neighborhoods of $x$ .
Let $$G\colon X \to \mathcal{P}(\mathcal{P}(D))$$ $$\hspace{3cm}x\mapsto A_x:=\{U\cap D: U\in B_x\}$$ This function is well defined. I would like to know how to use the hypothesis that X is Hausdorff to prove that G is injective? Thanks!",['general-topology']
4893248,Compare $3^4 \times 6^5 \times 7^8 \bigcirc 4^3 \times 5^6 \times 8^7$,"Compare $$3^4 \times 6^5 \times 7^8 \bigcirc 4^3 \times 5^6 \times 8^7$$ Options: $\text{(A)} > \space\space\space\space\space \text{(B)} <\space\space\space\space\space \text{(C)} =$ Notes: $1.$ Calculators are prohibited. $2.$ Students have no knowledge about logarithms, except a useful fact which is $$\text{For }e<a<b, b^a<a^b, \text{where }e\approx 2.72$$ $3.$ Students should know rules of exponents, such as $a^m \times a^n = a^{m+n}, (a^m)^n=a^{mn}$ , etc. $4.$ It is not required to show the work. Therefore, any feasible approximation can be applied. $5.$ The question should be answered in less than $2$ minutes. This appeared in a national exam preparation book for my sister. I claim that there is no typo. The reason for that is: Previous question is: Compare $$3^4 \times 5^6 \times 7^8 \bigcirc 4^3 \times 6^5 \times 8^7$$ Which is marked as $\color{green}{\text{EASY}}$ . My solution to it is by using note $\text{#}2$ mentioned above. The answer is $\text{(A)} >$ . However, the current question is marked as $\color{red}{\text{HARD}}$ My attempt (did not give any conclusion): Rearranging the numbers $$\frac{3^4 \times 7^8}{4^3 \times 8^7} \bigcirc \frac{5^6}{6^5}$$ Then factor out one of the $5$ 's $$\frac{3^4 \times 7^8}{4^3 \times 8^7} \bigcirc \frac{5^5}{6^5}\cdot 5$$ Now using the rule which says $\frac{a^n}{b^n}=\big(\frac{a}{b}\big)^n$ $$\frac{3^4 \times 7^8}{4^3 \times 8^7} \bigcirc \bigg(\frac{5}{6}\bigg)^5 \cdot 5$$ Clearly, $\text{LHS}>1$ (using note $\text{#}2$ mentioned above). Also $\big(\frac{5}{6}\big)<1 \implies \big(\frac{5}{6}\big)^5<1$ Now there are two possibilities: I. If $\big(\frac{5}{6}\big)^5 \cdot 5<1$ , then we can conclude that the correct option is $\text{(A)} >$ II. If $\big(\frac{5}{6}\big)^5 \cdot 5>1$ , then we cannot conclude from this. Then I realised that this approach is not feasible. Your help would be appreciated. THANKS!","['exponentiation', 'inequality', 'number-comparison', 'fractions', 'algebra-precalculus']"
4893278,Converse of Brouwer fixed point theorem,"Brouwer fixed point theorem is usually stated in the following way:
Let $B^n$ some closed ball of a Euclidean space, and let $f \colon B^{n} \rightarrow B^{n}$ be a continuous map. Then $f$ has a fixed point, that is, there exists $x \in B^n$ such that $f(x)=x$ . The result follows easily for any topological space $X$ homeomorphic to $B^{n}$ as well. Indeed, let $\phi \colon B^n \rightarrow X$ be an homeomorphism between both spaces, and let $g\colon X \rightarrow X$ be any continuous map. Then $\phi^{-1} \circ g \circ \phi\colon B^{n} \rightarrow B^{n}$ is a continuous map, so it has a fixed point $x$ , which means that $\phi^{-1} \circ g \circ \phi(x)=x$ . But this is equivalent to $g(\phi(x))=\phi(x)$ , so $\phi(x) \in X$ is a fixed point of $g$ . One may wonder if this theorem holds for other topological spaces, and the answer is negative in general, consider for example a traslation by a non-zero vector on $\mathbb{R}^{n}$ . Even in the compact case it does not hold in general. For example, if we take $\mathbb{S}^{n}$ , just consider the antipodal map and the theorem fails. The assertion is not either true for other compact topological spaces, such as tori. My question is, hence, the following: Does Brouwer fixed point theorem characterize Euclidean balls, i.e., if we have a topological space $X$ such that any continuous map $f \colon X \rightarrow X$ has a fixed point, is it homeomorphic to an Euclidean ball? Note that the case when $X$ is just a point, then it is homeomorphic to an Euclidean ball of $\mathbb{R}^0=\{0\}$ .","['fixed-point-theorems', 'general-topology', 'algebraic-topology']"
4893279,Area Vector of a flat region,"I don't understand the answers provided in the following problem (by the professor) Let $\vec{v}=\vec{i}+8\vec{j}-7\vec{k}$ and S be the Rectangular region with the orientation shown below. a) Find a normal vector to the plane in the Upward Direction. In order to do this, I first found from the given points the equation of the plane which was $2y+z-4=0$ . Then using the formula $\vec{n} = \pm (A,B,C)/ \sqrt {A^2+B^2+C^2}$ I found the possible unit normal vectors to be $\vec{n}=\pm (0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}})$ . However, according to the given answers, the correct answer is considered to be $\vec{n}= (0,\frac{-2}{\sqrt{5}},\frac{-1}{\sqrt{5}})$ . Why is this the case? Why is $\vec{n}=(0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}})$ incorrect? How can the selection of the positive or negative normal vector be explained by the upward direction asked in this case? b) Find the area vector. According to the theory, the area vector is $\vec{A}=\vec{n}A$ where $A$ is the area ofthe region. In this case, we have a rectangle whose width is $2$ and length is $2\sqrt{5}$ , as seen in the figure, which means its Area is $4\sqrt{5}$ . So, considering the answer provided in the previous question, the area vector would be $\vec{A}= (0,-8,-4)$ . However, the accepted answer is given to be $\vec{A}=(0,8,4)$ . What am I missing?","['multivariable-calculus', 'vector-analysis']"
4893307,"How to show that, the weak topology is the coarsest topology such that all $f:E \rightarrow \mathbb{K}$ are continuous?","Let E be a normed space, $E':=\{f:E \rightarrow \mathbb{K}| f \text{ is continuous and linear}\}$ . Define $p_f(x):=|f(x)| where f \in E'$ and $x \in E$ . Consider the family of seminorms $\mathcal{P}=\{p_f |f \in E'\}$ .
The topology induced by $\mathcal{P}$ is called the weak topology.
The weak topology is the coarsest topology such that every $f \in E'$ is continuous. I would like to show the topology induced by $\mathcal{P}$ is the ""coarsest topology such that every $f \in E'$ is continuous"".
I will call the topology induced by $\mathcal{P}$ $\tau_{\mathcal{P}}$ . I have thought about two ways to approach this: Approach 1: My first idea was to just go by the definition of coarsest topology, i.e.
Suppose $\tau$ is a topolgy such that  every $f \in E'$ is continuous.
I need to show that $\tau_{\mathcal{P}} \subseteq \tau$ . Approach 2: The idea hear is to use topology to ""generate"" the coarsest topology and then show that it is euqal to $\tau_{\mathcal{P}}$ . Let $X$ be a set and let Pow denote the power set.
I know that if $\mathcal{S}\subseteq Pow(X)$ . Then $\mathcal{S}$ is a subbasis for the coarsest topology $\tau$ on $X$ such all Elements of $\mathcal{S}$ are open. I thought about defining $\mathcal{S}:=\{f^{-1}(U)| f \in E' \text{ and } U \text{ open in } \mathbb{K}\}$ .
By defining $\mathcal{B}:=\{\cap_{S \in \mathcal{F}} |\mathcal{F} \subseteq S, \mathcal{F} \text{is finite}\}$ I can obtain a basis for the topology. If I now let $\tau:=\{\cup_{B \in \mathcal{C}} B|\mathcal{C} \subset \mathcal{B} \}$ I obtain the coarsest topology such that each $f \in E'$ is continuous, i.e. $\tau \subseteq \tau_{\mathcal{P}}$ . The only thing left to show is: $\tau_{\mathcal{P}} \subseteq \tau$ . So, let $U \in \tau_{\mathcal{P}}$ . Being open w.r.t $\tau_{\mathcal{P}}$ means that for every $x_0 \in U$ there exist $p_1,...,p_n$ and $r_1,...,r_n$ such that $\cap_{i=1}^n B_{p_i}(x_0,r_i) \subseteq U$ , where $B_{p}(x_0,r):=\{x \in E |p(x-x_0)<r\}$ . For $U$ to be open w.r.t $\tau$ means that there exist $f_1,...,f_n$ and $U_1,..., U_n$ open in $\mathbb{K}$ such that $\cap_{i=1}^n f^{-1}_i(U_i) \subseteq U$ . But I do not know how to argue that final step. Help or solution would be very appreciated.","['analysis', 'functional-analysis', 'weak-topology', 'locally-convex-spaces', 'general-topology']"
4893326,"Find a $(X,\mathscr{A})$ and finite measures $\mu$ and $\nu$ such that $\mu(X)=\nu(X)$ but $\{A\in\mathscr{A}:\mu(A)=\nu(A)\}$ not a sigma algebra","Problem The Problem is to find a measurable space $(X,\mathscr{A})$ together with finite measures $\mu$ and $\nu$ that agree on the whole set $\mu(X)=\nu(X)$ but are so that the collection \begin{align*}
\{A\in\mathscr{A}:\mu(A)=\nu(A)\}
\end{align*} is not a $\sigma$ -algebra. My Attempt Let \begin{align*}
X=\{1,2,3,4\}
\end{align*} and \begin{align*}
\mathscr{A}=\left\{\emptyset,X,\{1\},\{2,3,4\},\{2\},\{1,3,4\},\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,3\},\{2,4\},\{1,4\},\{2,3\}\right\}. 
\end{align*} Then, $\mathscr{A}$ is a $\sigma$ -algebra on $X$ , because it contains $X$ , it is closed under complementation, and it is closed under the formation of countable union. Define $\mu:\mathscr{A}\to[0,+\infty]$ by \begin{align*}
\begin{alignedat}{16}
&\mu(\emptyset)=0,      &&\mu(X)=4,              &&                  &&                 \\
&\mu(\{1\})=1,          &&\mu(\{2\})=3,          &&\mu(\{3\})=0,     &&\mu(\{4\})=0,    \\
&\mu(\{2,3,4\})=3,\quad &&\mu(\{1,3,4\})=1,\quad &&\mu(\{1,2,4\})=4,\quad &&\mu(\{1,2,3\})=4,\\
&\mu(\{1,2\})=4,        &&\mu(\{1,3\})=1,   &&\mu(\{1,4\})=1,   &&                 \\
&\mu(\{3,4\})=0,   &&\mu(\{2,4\})=3,   &&\mu(\{2,3\})=3.   &&                 
\end{alignedat}
\end{align*} Define $\nu:\mathscr{A}\to[0,+\infty]$ by \begin{align*}
\begin{alignedat}{16}
&\nu(\emptyset)=0, &&\nu(X)=4,         &&                  &&                 \\
&\nu(\{1\})=2,     &&\nu(\{2\})=2,     &&\nu(\{3\})=0,     &&\nu(\{4\})=0,    \\
&\nu(\{2,3,4\})=2,\quad &&\nu(\{1,3,4\})=2,\quad &&\nu(\{1,2,4\})=4,\quad &&\nu(\{1,2,3\})=4,\\
&\nu(\{1,2\})=4,   &&\nu(\{1,3\})=2,   &&\nu(\{1,4\})=1,   &&                 \\
&\nu(\{3,4\})=0,   &&\nu(\{2,4\})=2,   &&\nu(\{2,3\})=3.   &&                 
\end{alignedat}
\end{align*} Then, $\mu$ and $\nu$ are both measure on $(X,\mathscr{A})$ , because they are both countably additive for sequences of disjoint sets in $\mathscr{A}$ and $\mu(\emptyset)=\nu(\emptyset)=0$ . We have that \begin{align*}
\{A\in\mathscr{A}:\mu(A)=\nu(A)\}=\{\emptyset,X,\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,4\},\{2,3\}\}.
\end{align*} This collection is not a $\sigma$ -algebra on $X$ , because $\{2,3,4\}\notin\mathscr{A}$ but it is a countable union of elements in this collection. Question Could someone please help me check if my example is valid? In addition, is there a simpler solution to this problem? Thanks a lot in advance! Reference Measure Theory by Donald Cohn Section 1.6 Dynkin Classes Example 1.6.1. Update Thanks for @Zerox's checking. My example was wrong.","['measure-theory', 'analysis', 'real-analysis', 'alternative-proof', 'solution-verification']"
4893379,Integral of $\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy$,"I'm looking for anything that might help me solve the integral below: $$\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy$$ With $\\{r,d,\beta,y\\}\in\mathbb{R}$ , $\beta<0$ , $\\{r,d\\}>0$ The function is essentially the angle $\phi=\arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right)$ multiplied by an exponential decay, $e^{\beta x}$ . In the figure below, you can see that $\phi=\arctan\left(\frac{a}{d}\right)$ and $a=\sqrt{r^2-y^2}$ . The only indication I have that the integral has an analytic solution is that the region is closed and well defined. The function is not analytic at $y=\pm r$ but is is analytical around it. I've attempted a number of substitutions, integration by parts, and residue analysis. I've also tried to find it in some integral tables but I'm not sure what form to search it in. The obvious substitution is $y=r\sin({\theta})$ . This leads to $$\int_{-\pi/2}^{\pi/2}e^{\beta r \sin({\theta})}\arctan\left({\frac{r \cos\left(\theta\right)}{d}}\right)r\cos\left(\theta\right)d\theta$$ Which doesn't seem any easier to solve.","['integration', 'calculus', 'definite-integrals', 'exponential-function']"
4893383,Geometric argument for why $(\mathbf{x}-\mathbf{p}_1)\cdot (\mathbf{x}-\mathbf{p}_2) = c$ is a circle,"Let $\mathbf{p}_1, \mathbf{p}_2 \in \mathbb{R}^2, c \in \mathbb{R}$ . Then the equation $(\mathbf{x}-\mathbf{p}_1)\cdot (\mathbf{x}-\mathbf{p}_2) = c$ defines a circle with centre $\frac{\mathbf{p}_1 + \mathbf{p}_2}{2}$ . We can see this algebraically by expanding the equation to $\mathbf{x}\cdot\mathbf{x} - (\mathbf{p}_1 + \mathbf{p}_2)\cdot\mathbf{x} + \mathbf{p}_1\cdot\mathbf{p}_2 = c$ and completing the square, giving $\left\lvert\left\lvert\mathbf{x}-\left(\frac{\mathbf{p}_1 + \mathbf{p}_2}{2}\right)\right\rvert\right\rvert^2=R^2$ , where $R^2=\left\lvert\left\lvert\frac{\mathbf{p}_1 - \mathbf{p}_2}{2}\right\rvert\right\rvert^2+c$ My question is whether there's a way to use the geometric meaning of the dot product to interpret the original equation in a way that makes it clear why it gives a circle (or sphere in higher dimensions). In the case where $c=0$ , I can understand geometrically what's going on using Thales' Theorem and its converse, but I don't know how to think about this geometric property of ""two vectors having constant dot product"" for other values of $c$ . Edit: I accidentally asked a slightly different question to the one I intended to ask -- I'm looking for ""enlightening"" explanations for why this equation would correspond to a circle, since I didn't feel especially enlightened by crunching out the algebra. I assumed such an explanation would likely be geometric, but any explanation which provides insight is definitely welcomed, even if it happens to not involve much geometry.","['vectors', 'geometry']"
4893391,Is a scalar presented as a matrix or not here?,"In the linear algebra course I am taking, the inner product of 2 vectors $\langle u, v \rangle$ is defined as being a scalar; however, it is also viewed as being a product of 2 matrices as $u^Tv$ , as vectors are said to be a specific type of matrix, and the product of 2 matrices is defined as being a matrix. This seems to suggest a scalar is a matrix; yet matrix-scalar multiplication doesn't make sense if the scalar is a matrix. So my question is how to reconcile how in one case a scalar seems like it is a matrix, and on the other hand seems like it isn't, based on the way these concepts are expounded in the specific course I am taking. Is it the case that a scalar is indeed a matrix, but matrix-matrix multiplication is just defined in a unique way in the case of one of the factors being a scalar, which differs from the standard view?","['matrices', 'inner-products', 'linear-algebra', 'vectors']"
4893484,Diagonalization/Eigenvalues of block matrices,"Suppose I have two square $2^n \times 2^n$ matrices $A$ and $B$ . Suppose furthermore that $A$ and $B$ are both symmetric matrices with real coefficients. Therefore their eigenvectors can be chosen to be orthonormal, and the matrices can be diagonalized with orthonormal matrices $R_a, R_b$ : $$A=R_A^{T} \Lambda_A R_A, $$ $$B=R_B^{T} \Lambda_B R_B.$$ Now I form the block-matrix $M$ of size $2^{n+1} \times 2^{n+1}$ : $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix} $$ Clearly, $M$ will also be symmetric with real coefficients, and so its eigenvectors can be chosen to be orthonormal, and $M$ can be diagonalized with another orthonormal matrix: $$M=R_M^{T} \Lambda_M R_M.$$ Question: Is there any formula for the eigenvalues of $M$ in terms of the eigenvalues of $A$ and $B$ ? Is there any formula for $R_M$ in terms of $R_A$ and $R_B?$ Attempt:
Taking inspiration from the $2 \times 2$ case we have $$\begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{\sqrt{2}}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix} \frac{1}{\sqrt{2}} \begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix} $$ $$\begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix}\begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix} $$ We can observe that this also works in block form: $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} A-B & 0 \\ 0 & A+B \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix} $$ Now, this is not diagonalized yet, but the upper left and lower right block behave independently, and so: $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} R^T_{A-B} & 0 \\ 0 & R^T_{A+B} \end{pmatrix} \begin{pmatrix} \Lambda_{A-B} & 0 \\ 0 & \Lambda_{A+B} \end{pmatrix} \begin{pmatrix} R_{A-B} & 0 \\ 0 & R_{A+B} \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix} $$ Therefore, we could answer our original question if instead we answered: Question: Is there any formula for the eigenvalues of $A+B$ and $A-B$ in terms of the eigenvalues of $A$ and $B$ ? Is there any formula for $R_{A+B}$ and $R_{A-B}$ in terms of $R_A$ and $R_B?$ Context: Knowing how to do this would be useful for this problem: Sums of Matrices; 2-Color an $m \times k$ grid so that no $2 \times 2$ squares of the same color occur .","['matrices', 'linear-algebra']"
4893547,Does the strict inequality in the Layer Cake Representation Matter?,"I have seen both version of layer cake representation theorem stated:
Let $(\Omega, \Sigma, \mu)$ be a $\sigma$ -finite measure space $f: \Omega \to [0, \infty]$ -measurable function. Then the function $x \in [0, \infty] \to \mu(\{ \omega: f(\omega) > x \})$ is Borel measurable and (Version 1) $$
\int_\Omega f \,d\mu = \int_0 ^\infty \mu(\{ \omega: f(\omega) > x \}) \,dx.
$$ or (Version 2) $$
\int_\Omega f \,d\mu = \int_0 ^\infty \mu(\{ \omega: f(\omega) \geq x \}) \,dx.
$$ Now going through the proof, which is just a simply application of Fubini's, I think we can conclude both of the versions to be correct (?) as Lebesgue measures have point mass zero. I wonder if I could get some confirmation on if this is true: I will write the proof for the weak inequality here, one can imagine substituting strict inequality everywhere and it seems like everything should go through? $$
\int_0^\infty f(x) d \mu(x) 
= \int_0^\infty \int_0^\infty \mathbb{1}_{[0, f(x)]}(t) dt d \mu(x)
= \int_0^\infty \int_0^\infty \mathbb{1}_{f(x) \geq t}(x) d \mu (x) dt
= \int_0^\infty \mu \left ( \{ x : f(x) \geq t \} \right ) dt.
$$ Now if this was the case , then it seems to suggest if $f$ is integrable, we must have $$
\mu(\{ \omega: f(\omega) \geq x \}) = \mu(\{ \omega: f(\omega) > x \})
$$ for Lebesgue almost every $x$ , which seems to be a nice little corollary, but I wonder if there is a better way to show this instead of invoking the Layer cake representation? Update: This is my attempt of a proof for the second question and I wonder if I could get some suggestions on if the proof is correct:
Note that for Lebesgue almost every $x \geq 0$ , we have $\mu(\{ \omega: f(\omega) > x \}) = \mu(\{ \omega: f(\omega) \geq x \})$ . To see this, it suffices to note $\mu(\{ \omega: f(\omega) = x \}) > 0$ for at most countably many $x \geq 0$ . The function $x \mapsto \mu(\{ \omega: f(\omega) > x \})$ is a monotone function as $x_1 \geq x_2 \implies \mu(\{ \omega: f(\omega) > x_1 \}) \leq \mu(\{ \omega: f(\omega) > x_2 \})$ . Therefore, the function $x \mapsto \mu(\{ \omega: f(\omega) > x \})$ has at most countably many jump discontinuities. In particular, note if $\mu(\{ \omega: f(\omega) = x' \}) > 0$ for some $x'$ , then $x \mapsto \mu(\{ \omega: f(\omega) > x \})$ has a jump discontinuity at $x'$ . Indeed, let $x_n \to x'$ from the left hand side increasing. Let $\{ A_k \}_{k = 1} ^\infty$ be increasing sequence of $\mu$ -finite set from $\sigma$ -finiteness. Choose $k$ large enough so that $A_k \cap \{ \omega: f(\omega) > x' \} \not= \emptyset$ . Then $$
\lim_{n \to \infty} \mu(\{ \omega: f(\omega) > x_n \} \cap A_k) = \mu(\{ \omega: f(\omega) \geq x' \} \cap A_k).
$$ Moreover, let $x_n' \to x'$ from the right hand side decreasing, then $$
\lim_{n \to \infty} \mu(\{ \omega: f(\omega) > x_n' \}) = \mu(\{ \omega: f(\omega) > x' \}).
$$ Therefore, we have that \begin{align*}
    \lim_{n \to \infty} \mu(\{ \omega: f(\omega) > x_n \}) &= \lim_{k \to \infty}\lim_{n \to \infty} \mu(\{ \omega: f(\omega) > x_n \} \cap A_k) \\
    &= \mu(\{ \omega: f(\omega) \geq x' \}) \\
    &> \mu(\{ \omega: f(\omega) > x' \}) \\
    &= \lim_{n \to \infty} \mu(\{ \omega: f(\omega) > x_n' \}).
\end{align*} Note this shows that $x'$ is a point of jump discontinuity. In particular, we then must have $\mu(\{ \omega: f(\omega) > x \}) = \mu(\{ \omega: f(\omega) \geq x \})$ for Lebesgue almost every $x$ as it must be except from at most countably many points from the above discussion.","['measure-theory', 'solution-verification', 'real-analysis']"
4893575,Is the following connected space path connected?,"Let us consider the topological space $(\mathbb R,\mathcal O)$ , with topology $\mathcal O:=\{A\subseteq\mathbb R:A\subseteq\mathbb Q\mbox{ or } \mathbb Q\subseteq A \}$ , where $\mathbb Q$ is the set of rational numbers. It is easy to show that this space is a connected space, since for every pair of nonempty open subsets $U$ and $V$ satisfying $\mathbb R=U\cup V$ , we must have $U\cap V\ne\emptyset$ . On the other hand, i have no idea how to check if this space is a path connected space (i.e. checking that there is a path connecting every pair of points $x,y\in\mathbb R$ ). The first idea was to check the mapping $t\mapsto t(x-y)+x$ , but it is not a suitable candidate, since  it is not continuous. I feel that the class of continuous mappings from $[0,1]$ with the usual topology to $\mathbb R$ with the topology $\mathcal O$ is very confined, and that this is the key to prove (or disprove) the claim.","['general-topology', 'path-connected']"
4893598,Proof of a martingale condition regarding martingale transform,"I tried to work on my previous question regarding a Converse of a martingale transform theorem , and was told by a stochastics PhD student at my university that indeed if an integrable, adapted process $X$ in $\mathbb{R},$ and a predictable and bounded $C$ yield $$E[(C\bullet X)_n]=0$$ then $X$ must be a martingale. Where the transform is defined as $(C\bullet X)_n\stackrel{\text{def.}}{=}\sum_{k=1}^{n} C_k(X_k-X_{k-1})$ Yet she couldn't offer me a proof of it at that time. I myself see no way to prove it. I would appreciate any help or a reference to where I could find a proof.","['integration', 'reference-request', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4893623,On the solutions of $y''=yP(x)$ or $y'+y^2=P(x)$ [duplicate],"This question already has answers here : Finding an analytical solution to a particular class of second order ODEs (2 answers) Closed 3 months ago . Are there any general solutions to $y''=P(x)y$ ? A change of variables $y=e^u$ implies $y''=(u''+(u')^2)e^u $ and $u''+(u')^2=P(x)$ . Therefore, $y''=P(x)y$ reduces to $w'+w^2=P(x)$ , but I was not able to solve it (neither did WolframAlpha ). If it helps, $P(x)=\frac{Ax^2+B}{(x^2+a^2)^2}$ , but I am also interested in general solutions. Motivation These differential equation arise during finding a closed-form for the following series: $$
\sum_{n=1}^\infty \frac{(3n)!}{(2n)!n!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(3n)!}{n!n!n!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(6n)!}{n!(2n)!(3n)!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(4n)!}{n!n!(2n)!}x^n.
$$ Alternatively, it would be equally interesting if any of the above summations have closed-forms.","['calculus', 'ordinary-differential-equations']"
4893662,Positive integer sequence with sum $m$ can have at most $\left\lfloor\dfrac{m^2}{8}\right\rfloor$ increasing pairs,"Let $a_1,\cdots,a_n$ be a sequence of positive integers such that $\bullet$ $a_1+\cdots+a_n=m$ ; $\bullet$ The number of pairs $(i,j)$ such that $i<j$ and $a_i<a_j$ is $k$ . Must we have $$
k\le \dfrac{m^2}{8}?
$$ The relation $k\le \dfrac{m^2}{8}$ is based on the following two observations: Observation 1. If $a_1,\cdots,a_n$ consists of only $1$ 's and $2$ 's, then we have $k\le \dfrac{m^2}{8}$ , and the result is optimal. Proof . WLOG we can suppose that the last term in the sequence is $2$ . Suppose that there are $t$ $2$ 's, and the number of $1$ 's to the left of each $2$ is $b_1,\cdots,b_t$ , then $$
b_1\le \cdots\le b_t,\quad m=b_t+2t,\quad k=b_1+\cdots+b_t.
$$ We then have $k\le tb_t\le\dfrac{(b_t+2t)^2}{8}=\dfrac{m^2}{8}$ . Moreover, it is possible that $k=\left\lfloor\dfrac{m^2}{8}\right\rfloor$ : Let $t$ be the integer closest to $\dfrac{m}{4}$ (choose either one in case of a tie), and $$
b_1=\cdots=b_t = m-2t,
$$ which means that we pick the sequence consisting of $m-2t$ $1$ 's followed by $t$ $2$ 's. Then we have $k=t(m-2t)$ , and we see that \begin{align*}
m\equiv 0\,(\operatorname{mod}\,4)&\Rightarrow t=\dfrac{m}{4}\Rightarrow k=\dfrac{m^2}{8};\\
m\equiv 1\,(\operatorname{mod}\,4)&\Rightarrow t=\dfrac{m-1}{4}\Rightarrow k=\dfrac{m^2-1}{8};\\
m\equiv 2\,(\operatorname{mod}\,4)&\Rightarrow t=\dfrac{m-2}{4}\text{ or }\dfrac{m+2}{4}\Rightarrow k=\dfrac{m^2-4}{8};\\
m\equiv 3\,(\operatorname{mod}\,4)&\Rightarrow t=\dfrac{m+1}{4}\Rightarrow k=\dfrac{m^2-1}{8},
\end{align*} so $k=\left\lfloor\dfrac{m^2}{8}\right\rfloor$ in this case. Note 1.1. We can see that $k=0,\cdots,\left\lfloor\dfrac{m^2}{8}\right\rfloor$ are all possible because a nonincreasing sequence corresponds to $k=0$ , and swapping two adjacent entries in a sequence would leave $m$ unchanged, but $k$ changed by $0$ or $\pm 1$ . Note 1.2. The construction above gives the only optimal solutions. Indeed, if $k=\left\lfloor\dfrac{m^2}{8}\right\rfloor$ , then $$
t(m-2t)=tb_t\ge k=\left\lfloor\dfrac{m^2}{8}\right\rfloor\Rightarrow \left|t-\dfrac{m}{4}\right|\le\dfrac{1}{2}, k=tb_t\text{ (hence }b_1=\cdots=b_t).
$$ Observation 2. If $a_1,\cdots,a_n$ is nondecreasing, then $k\le\dfrac{m^2}{8}$ , and the result is also optimal. Proof . Suppose that each of $1,2,\cdots$ occurs $c_1,c_2,\cdots$ times in the sequence, where $c_i\neq 0$ for finitely many $i$ , then $$
m=c_1+2c_2+\cdots,\quad k=\sum_{i<j}c_ic_j=\dfrac{1}{2}\sum_{i\neq j}c_ic_j,
$$ so we have $$
m^2 - 8k = (c_1 - 2c_2 - c_3)^2 + 8c_3^2 + \sum^{\infty}_{i=4}i^2c_i^2+\sum_{\max\{i,j\}\ge 4,i\neq j}(ij-4)c_ic_j\ge 0.
$$ The result $k\le\dfrac{m^2}{8}$ for nondecreasing sequences is also optimal as shown by the $1-2$ sequences above. Note 2.1. If the equality holds, then we must have $c_3=c_4=\cdots=0$ , otherwise we would have $m^2-8k\ge 8c_i^2\ge 8$ . I don't have any idea for the general case. I guess that if $k$ attains its maximum with $m$ fixed, then $a_1,\cdots,a_n$ must consist of only $1$ 's and $2$ 's , but I don't know how to prove it. Any help appreciated. Edit. This question turns out to be trivial. For a general sequence, we apply the process of bubble sort. At each step either we do nothing, either we create a new increasing pair, so $k$ reaches its maximum when and only when the sequence is nondecreasing, and the result $k\le\dfrac{m^2}{8}$ follows from Observation 2 above.","['inequality', 'combinatorics']"
4893688,Why do we use the Jacobian determinant to change the variables of a double integral?,"I've been trying to learn (the basics of) Multivariable Calculus on Khan Academy, but I've begun facing difficulties with changing the variables of a double (& triple) integral. The problem is the following: Suppose we wanted to evaluate the double integral $$S=\iint_{D}(24x^2+12y^2)\ dx\ dy$$ by first applying a change of variables from $D$ to $R$ : $$x = X_{1}(u,v)=\frac{u}{4}$$ $$y = X_{2}(u,v)=\frac{v}{3}$$ What is $S$ under the change of variables? Previously, when dealing with questions of the change of variables in functions, I was taught to just substitute $X_{1}(u,v)$ and $X_{2}(u,v)$ into the original function $f(x,y)$ , and get my answer. ( If I've done a bad job explaining this please tell me so that I can clarify with an example ). However, I had never done a problem like this and I was stumped, so I looked at the solution, which was the following: For a transformation $\mathbf{X}:R\rightarrow D$ , the integral can be rewritten as $$\iint_{D}f(x,y)dA = \iint_{R}f(\mathbf{X}(u,v))|J(\mathbf{X})|\ du\ dv$$ It then goes on to calculate the determinant of the Jacobian, and solves the problem. Although I can understand how they got to the solution, I cannot comprehend the why . So far I have only seen the Jacobian as a means of describing a local linearization , and I've been told that its determinant represents the scale factor that some arbitrary area is multiplied by as it undergoes the transformation. ( Again, please tell me if I've explained this badly. ) Whilst typically with each question there are also videos/articles to go along with it, this question has provided me with no extra material and I haven't been able to understand why the Jacobian comes into play here. Could someone please explain to me why we use the Jacobian here, and point me to any extra material which could help me understand this problem better?
Additionally, could someone explain to me the purpose of changing the variables? (P.S., Unless I am mistaken, the author has made a mistake when they say $\mathbf{X}:R\rightarrow D$ , and should it be $\mathbf{X}:D\rightarrow R$ ) (Link to the quiz: https://www.khanacademy.org/math/multivariable-calculus/integrating-multivariable-functions/x786f2022:change-variables/e/change-of-variables--bound ) Thank you!",['multivariable-calculus']
