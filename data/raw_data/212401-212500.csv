question_id,title,body,tags
4284236,Special linear group over a quotient of a polynomial ring,"I'm interested in the structure of groups of the form $SL_2(A)$ where $A$ is a ring of the form $A=\mathbb{F}_p[x]/(x^2)$ , or more generally $A=\mathbb{F}_p[x]/(x^n)$ . It is clear that for $n=1$ we just recover $SL_2(\mathbb{F}_p)$ which is well-understood. Also if $p(x) \in \mathbb{F}_p[x]$ is irreducible then $SL_2(\mathbb{F}_p[x]/(p(x))) = SL_2(\mathbb{F}_q)$ for some finite field $\mathbb{F}_q$ , and again there exists a vast literature on these groups. My question is whether special linear groups over these slightly different (local) rings $\mathbb{F}_p[x]/(x^n)$ have been studied at all, and in particular their subgroups structure. Any references would be greatly appreciated.","['matrices', 'group-theory', 'abstract-algebra']"
4284300,limsup of a sequence of subadditive random variables with subgaussian tail.,"Suppose that $X_n$ is a sequence of random variables satisfying $\bullet P(X_n > t\sqrt{n}) < e^{-t^2}$ , and $\bullet X_{m+n} < X_m+X_n$ for all $n,m$ . We want to try and show that $$\limsup_{n\rightarrow \infty} \frac{X_n}{\sqrt{n\log\log(n)}} \leq 1$$ holds almost surely. My initial attempt was to look at the subsequence $X_{n_k}$ where $n_k = (1+\delta)^k$ for a small $\delta$ . Since $$
P(X_{n_k} > (1+\epsilon)\sqrt{n_k\log\log(n_k)}) < e^{-(1+\epsilon)\log\log(n_k)} = \frac{1}{\log(n_k)^{1+\epsilon}} = \left(\frac{1}{k\log(1+\delta)}\right)^{1+\epsilon}
$$ Borel-Cantelli implies that $$
\limsup_{k\rightarrow \infty} \frac{X_{n_k}}{\sqrt{n_k\log\log(n_k)}}\leq 1
$$ holds almost surely. I now want to use the subadditivity of the sequence to help me bound the terms $X_n$ for $n_k\leq n < n_{k+1}$ . In particular, we have that $$
\frac{X_n}{\sqrt{n\log\log n}} \leq \frac{X_{n_k}}{\sqrt{n_k\log\log n_k}} + \frac{X_{n-n_k}}{\sqrt{n\log\log n}}
$$ so that I need to show that $$
\frac{X_{n-n_k}}{\sqrt{n\log\log n}} \rightarrow 0
$$ almost surely. Heuristically, this should be true since $n-n_k \approx \delta (1+\delta)^k$ will be much smaller than $n$ , but I'm stuck here. Does anyone have any ideas on how I can bound this $X_{n-n_k}$ term? Is this even a good approach to this problem?","['borel-cantelli-lemmas', 'probability-theory', 'probability']"
4284310,Interchanging the variable while integrating - Allowed?,"Suppose we have this equation: $$\frac{dy}{dx} = \frac{y}{2x}$$ The next step usually is: $$\frac{dy}{y} = \frac{dx}{2x}$$ And then you integrate : $$\int\frac{dy}{y} = \int\frac{dx}{2x}$$ $$\ln(y) = \ln(\sqrt{x}) + c$$ But can we integrate like this? (i feel you can't, but can't find the reasoning): $$\int2x \ dy = \int y \ dx$$ $$2xy = xy + c$$ Can you tell if this is also plausible? If so, why? and If not, why not? Thanks!","['calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
4284318,Be $M \in M_2(\mathbb{R})$ with $det(M) = 1$,"Be $M \in M_2(\mathbb{R})$ with $\det(M) = 1$ If $|\text{tr}(M)| < 2$ , Prove that exist $P \in M_2(\mathbb{R})$ with $\det(P)=1$ and exist $\alpha \in \mathbb{R}$ such that: $ M = P\cdot \begin{bmatrix} \cos(\alpha) & -\sin(\alpha)  \\ \sin(\alpha) & \cos(\alpha) \end{bmatrix} \cdot P^{-1}$ $\\$ My attempt to solution: I could not solve the problem. And I do not know how to continue. $1)$ I started analising the caracteristic polynomial of M: $P(x)_M = x^2-\text{tr}(M)\cdot x + \det(M)$ And we know that $\det(M) = 1$ and $|\text{tr}(M)| < 2$ , implies that $\Delta = (\text{tr}(M))^2-4 \det(M)<0$ So, both eigenvalues of $M$ are complexes and distinct, and one is conjugate of another. $2)$ This matrix remember me the rotation matrix, but i don't know how to use that fact. I do not know what to do. I tried for a long time, but I really could not resolve.",['matrices']
4284321,$\cos(\cos(\cos(\cos(\cos(\cos(\cos(....(\theta)))))))))$ approaches a constant as the number of cosines increases [duplicate],"This question already has answers here : Let $f_{k+1}(x)=f_{k}(\cos x)$ and $f_{1}(x)=\cos x$ then $\lim_{k\to\infty}f_{k}(x)=0.73905\cdots$ [duplicate] (2 answers) Closed 2 years ago . I was experimenting with the software geogebra , and playing with a couple of unusual trigonometric functions and I encountered a quite strange phenomena when I entered this input - $$f(\theta)=\cos(\cos(\cos(\cos(\cos(\cos(\cos(....(\theta)))))))))$$ Let's assume the number of cosines approach infinity, though my input was 10-20 cosines. I asked the program to graph this function and surprisingly the graph was straight-lined, as in for any value of $\theta$ , the function is a constant value. I then created a program in GBD Online to recheck if this was actually correct or just a software error. The program iterated $f(\theta)$ for all values of $\theta$ from $1$ to $999$ . And again, surprisingly, the values approached the constant of $0.739085$ The specifics of the program is here . The graph in question is shown in this image . My questions for this post are - Why does the function approach a constant? Do all trigonometric functions approach a constant with infinite iterations? If not, why only cosine?","['trigonometric-series', 'trigonometry', 'graphing-functions', 'sequences-and-series']"
4284329,Exponential Growth of Functions,"Let $f,g: \mathbb{N} \longrightarrow [0,+\infty[$ be nondecreasing functions. Then we say that the function $f$ dominates the function $g$ if there are positive integers $A,B$ and $C$ such that $$Af(Br)\geqslant g(r)$$ for every $r\geqslant C$ . Let $h: \mathbb{N} \longrightarrow [0,+\infty[$ be a nondecreasing function. Then $h$ dominates the exponential function if and only if $$\limsup_{x \longrightarrow +\infty} \frac{\log h(r)}{r}>0$$ The questions are: If a function $T$ dominates the exponential function, does not the function $T$ have exponential growth (since the exponential function dominates $T$ ) so they are equivalent under the mutual dominance? Is there any function that strictly dominates the exponential function (the function dominates the exponential function but the exponential function does not dominate it)?","['analysis', 'real-analysis', 'calculus', 'functions', 'exponential-function']"
4284338,Markov Inequality results seems too high,"The Markov Inequality states that $$P(X>\alpha) \le \frac{E[X]}{\alpha}$$ If I flip a fair coin 100 times, I was trying to calculate the probability that there were at least 90 tails in order to determine whether this series of events was an outlier or not. I was expecting that this would be an outlier according to the Markov Inequality. Since heads and tails are equally likely, I stated that $E[X] = 50$ (and that $\alpha = 90$ ), which yields: $$P(X > 90) \le \frac{50}{90} = \frac{5}{9} \approx 0.56$$ This seems absurdly high to me because it is saying that this extreme event could be more likely than not (which would imply that this is not an outlier). I do realize that this is an upper bound, but am I missing something here?","['statistics', 'solution-verification']"
4284389,Sard's theorem proof - Using Implicit Function Theorem to construct a new coordinate representation,"The following is part of the proof of the Sard's theorem from John Lee's Introduction to Smooth Manifolds. I am struggling to understand the second paragraph from Step 1. So assuming that $\partial F^1 / \partial x^1(a)\neq 0 $ , how do we define new smooth coordinates $(u,v)=(u,v^2,\dots , v^m)$ in some neighborhood $V_a$ of $a$ in $U$ by $u=F^1, v^2 = x^2 ,\dots ,v^m=x^m?$ I think this is some form of the implicit function theorem, but from the theorem stated in the text, I cannot figure out how we can construct such a coordinate chart. Also, in terms of these coordinates how does F have the coordinate representation $$F(u,v^2, \dots, v^m) = (u,F^2(u,v),\dots ,F^n(u,v))?$$ These are probably simple applications of the theorem but I am really struggling to understand this formally. I would greatly appreciate some help.","['smooth-manifolds', 'multivariable-calculus', 'calculus', 'manifolds', 'differential-geometry']"
4284403,"n black balls k white balls in M bins, what is the probability of selecting a black ball from any bin.","There are $n$ black balls and $k$ white, with $M$ bins. The process for filling the bins is as follows: Consider each of the $N=n+k$ balls, one at a time. For each ball, select 1 of the $M$ bins uniformly at random. Place the ball in the bin. The $N$ balls are allocated into the $M$ bins. Now, from a bin select a single ball, if a bin contains multiple balls a single ball is chosen uniformly at random. I want to work out the probability that a black ball is selected from a particular bin? Here is my approach: Let, $X_r$ be the number of balls of type $r\in \{b,w\}$ in a bin let $y_r=n$ for black balls and $y_r=k$ for white balls. Then the probability it has $s$ balls of type $r$ is given by the binomial distribution $$P(X_r=s)=\binom{y_r}{s}\left(\frac{1}{M}\right)^s\left(1-\frac{1}{M} \right)^{N-s}.$$ By independence of the events the probability that a bin has $i$ white and $j$ black balls in it $(t=i+j)$ , with $X=X_w+X_b$ , is $$P(X=t)=\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}.$$ Let $P(b_t)$ be the probability of selecting a black ball given $t$ balls in the bin. Since, the selection is uniformly at random it should just be the product of the fraction of black balls and the likelihood of t balls being in the bin $$P(b_{t=i+j})=\frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t}.$$ Now, the total probability of choosing a black ball is given by $$ \sum_{i=0}^k \sum_{j=1}^n \frac{j}{i+j}\binom{n}{i}\binom{k}{j}\left(\frac{1}{M}\right)^t\left(1-\frac{1}{M} \right)^{N-t} \tag{*}\label{*}$$ where index $j$ starts at 1, since if no black balls are in the bin the probability of selecting one is zero and it ensures we don't get the undefined $\frac{0}{0}$ when $i=0$ . I think that $\eqref{*}$ is correct. My question is what ways can $\eqref{*}$ be simplified? I dont think I can use Vandermonde's Identity as the term $\left(\frac{1}{M}\right)^i$ is not able to be taken out of the sum over $i$ for example. I have put it into mathematica and get the following Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t), 
    {s, 1, nh}, {t, 0, nl}]


ProbHireH[M, nh, nl]=(1 - 1/M)^(m + n) (1/((1 - 1/M) M))^n 
DifferenceRoot[Function[{\[FormalY], \[FormalN]}, {(-1 + M) (-1 - \[FormalN] + 
          n) (-\[FormalN] + m + n) \[FormalY][\[FormalN]] + (2 + 
          5 \[FormalN] + 3 \[FormalN]^2 - 2 m - 2 \[FormalN] m - M - 
          3 \[FormalN] M - 2 \[FormalN]^2 M + m M + \[FormalN] m M - 
          3 n - 4 \[FormalN] n + m n + 2 M n + 3 \[FormalN] M n - 
          m M n + n^2 - M n^2) \[FormalY][
         1 + \[FormalN]] - (1 + \[FormalN]) (4 + 3 \[FormalN] - m - 
          M - \[FormalN] M - 2 n + M n) \[FormalY][
         2 + \[FormalN]] + (1 + \[FormalN]) (2 + \[FormalN]) \
\[FormalY][3 + \[FormalN]] == 0, \[FormalY][0] == 0, \[FormalY][1] == 
      Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))], \[FormalY][
       2] == (1 - 1/M) M n Hypergeometric2F1[-m, -1 + n, 
         n, -(1/(-1 + M))] + 
       Hypergeometric2F1[-m, n, 1 + n, -(1/(-1 + M))]}]][n] Now, while this is a simplification of sorts, I am unsure that this is the simplest respresentation. This is because with the independence of everything I think that the probability of choosing a black ball from a bin after the allocation, could be the product of the probability that a bin has at least one ball and the fraction of black balls overall $$ \left(1- \left(1-\frac{1}{M}\right)^{n+k}\right)\frac{n}{n+k}. \tag{**}\label{**}$$ I can veryify (although not exhaustively) that $\eqref{*}$ and $\eqref{**}$ yield the same probabilities, for example SimpleProbHireH[M_, nh_, nl_] := (1 - (1 - 1/M)^(nh + nl))*nh/(nh + nl)

SimpleProbHireH[10, 5, 5]=6513215599/20000000000 

ProbHireH[M_, nh_, nl_] := 
 Sum[(s/(s + t))*Binomial[nh, s]*Binomial[nl, t]*(1/M)^(s + t)*(1 - 1/M)^(nh + nl - s - t), {s, 1, nh}, {t, 0, nl}]


ProbHireH[10, 5, 5]=6513215599/20000000000 Any help with this would be much appreciated.","['polya-urn-model', 'combinatorics', 'balls-in-bins', 'probability']"
4284419,Binomial Expansion with Derivatives,"Question: We have a function $f(t)$ that is defined for all $t \in [0,T]$ . From our data, we can estimate two important parameters $\theta \in \mathbb{Z} \backslash 0$ and $n \in \mathbb{N}$ . Given $n$ , we can also estimate a function $g(n)$ . Our estimations are fairly accurate. Now it appears that there is a pattern. For example, for $\theta > 0$ , we have \begin{align}
&n=1,\quad g(1)= \frac{d f(t)}{dt}+\theta f(t), \\
&n=2,\quad g(2)= \frac{d^2 f(t)}{dt^2}+\theta^2 f(t)+2\theta \frac{d f(t)}{dt},
\end{align} and so on; therefore, I think we can write it as an ordinary differential equation $g(n)=(d_t+θ)^n f(t)$ where $d_t$ is a time-derivative operator. Now is there a way to explicitly determine $f(t)$ ? My Attempt 1) I tried to use Kovacic algorithm but it seems very complicated and I couldn't get anywhere with it. My Attempt 2) Although we have a nonhomogeneous equation due to $g(n)$ , if for every $n$ , we differentiate both sides, we have: \begin{align}
& n=1,\quad f'' + \theta f'= 0, \\
& n=2,\quad f'''+2 \theta f''+ \theta^2 f'=0,
\end{align} and so on. If we look closely, the characteristics equation for every $n$ emits two roots, $0$ and $-\theta$ so let us define $C_{1n}$ and $C_{2n}$ two constants and $f_n(t)$ , given the value of $n$ , then we can write $$f_n(t) = C_{1n} + C_{2n}e^{- \theta t}.$$ But the problem is that the only condition we have is $f_n(T)=0.$ Any suggestions or comment would be very appreciated. I'm more interested in knowing if there is a solution for $f$ and the method to get there rather than the final solution. Edit: I think I have made a mistake: since the root $-\theta$ is a repeated root in the second set of ODEs, I think the solution to $f$ is: $$f_n(t) = C_{0n} + e^{-\theta t}\sum_{k = 1}^{n} C_{kn} t^{k - 1}.$$","['calculus', 'binomial-coefficients', 'binomial-theorem', 'ordinary-differential-equations']"
4284426,Puzzle: construct a permutation with the smallest order,"I wonder if there is a quick logical way to solve the following, or simplify the following problem: Say I have three (or more) identity matrices of different sizes ( $I_m = m\times m$ , $I_n= n\times n$ , $I_q=q\times q$ ).
With these, I can construct a $(m+n+q) \times (m+n+q)$ block permutation matrix. For example: \begin{bmatrix}
0 & I_n & 0\\
I_m & 0 & 0\\
0 & 0 &I_q
\end{bmatrix} A trivial solution is: \begin{bmatrix}
I_m & 0 & 0\\
0 & I_n & 0\\
0 & 0 &I_q
\end{bmatrix} which is an identity matrix. How do we construct a permutation matrix with the next smallest order, just by looking at the values of $m$ , $n$ , and $q$ ? More advanced question: can we rank all possible block permutation matrices based on their orders? (again, just by inspecting the sizes of the identity matrices).","['permutations', 'group-theory', 'linear-algebra']"
4284481,How is the combinatorist's Lagrange Inversion equivalent to the complex analytic expression of the same theorem?,"I will present three equivalent combinatorical (I assume so, due to the contexts of the papers I found them in) formulations of the Lagrange Inversion Theorem - I have been baffled over the course of many hours as to how they agree with the Wikipedia statement of the theorem! In particular, it has been hinted but never really stated that Wikipedia's theorem is the same as everyone else's - so I don't even know if I'm wasting my time trying to make them match. If anyone knows where the link lies (or whether there is definitely no link) I'd greatly appreciate that - as usual, one has a hard time corroborating Wikipedia's mathematical statements. I did try to parse their ""formal residues"" explanation, but formal power series are not something I know much about, but their complex-analytic statement is of great interest to me. In these formulations, $[x^n]\{f(x)\}$ denotes the $n$ th coefficient of the curly-braced power series in $x$ . Let $R(t)$ be a power series not involving $x$ . Then there is a unique power series $f=f(x)$ such that $f(x)=x\cdot R(f(x))$ , and for any Laurent series $\varphi(t)$ not involving $x$ , and any integer $n\neq0$ , we have: $$\tag{1}[x^n]\{\varphi(f)\}=\frac{1}{n}[t^{n-1}]\{\varphi'(t)\cdot R(t)^n\}$$ From another source: Suppose $u=u(x)$ is a power series in $x$ satisfying $x=\frac{u}{\varphi(u)}$ where $\varphi(u)$ is a power series in $u$ with a nonzero constant term. Then for any power series $F(u)$ of $u$ , and $n(\in\Bbb Z)\neq0$ we have: $$\tag{2}[x^n]\{F(u(x))\}=\frac{1}{n}[u^{n-1}]\{F'(u)\cdot\varphi(u)^n\}$$ I write both presentations in case one is more revealing than the other. Another strangely put but more promising analytic formulation from the same sources (with proof!): Let $G(t)=\sum_{n=0}^\infty g_nt^n$ , where the $g_i$ are indeterminates. Then there is a unique power series $f$ satisfying: $$f=x+G(f)$$ -Note how this formulation uses an addition, not a multiplication- Then for any power series $\varphi(t)$ we have: $$\tag{3}\varphi(f)=\varphi(x)+\sum_{m=0}^\infty\frac{1}{m!}\frac{d^{m-1}}{dx^{m-1}}\left(\varphi'(x)\cdot G(x)^m\right)$$ But try as I might, I can think of no sensible choice of $f,G,\varphi,u,R$ , whatever, such that this boils down to: If $f$ is an analytic function in some neighbourhood of $a\in\Bbb C$ , such that $f'(a)\ne0$ , then (by the inverse function theorem), it has an analytic inverse $g$ in some neighbourhood of $a$ , such that if $z=f(w),\,g(z)=w$ for all $w$ in this neighbourhood. The Lagrange Inversion Theorem gives its power series as follows: $$\tag{4}g(z)=a+\sum_{n=0}^\infty\frac{(z-f(a))^n}{n!}\cdot\lim_{w\to a}\frac{d^{n-1}}{dw^{n-1}}\left[\left(\frac{w-a}{f(w)-f(a)}\right)^n\right]$$ At first, I attempted to show (by uniqueness of power series) that the coefficients in $4$ are in fact the same as $D_z^n g(z)|_{z=f(a)}$ , but that went unsuccessfully. I have spent quite some time with $1$ and $2$ , attempting to make them play nice (I saw in examples that it was common to let $F$ (as in $2$ ) equal the identity, so you just get $[x^n]\{u(x)\}=\frac{1}{n}[u^{n-1}]\{\varphi(u)^n\}$ ), but as the inverse in these combinatorical formulations is weird ( $u=x\cdot\varphi(u)$ does not yield the straightforward coefficient in $4$ to my eyes!) I had no luck. I can recognise Wikipedia's coefficient for $g$ as: $$D_z^{n-1}[(g'(z))^n]\big|_{z=f(a)}$$ Which suggests that the $\varphi$ function in $2$ should perhaps be $g'(z)$ , but as $\varphi$ has a bizarre relationship I couldn't proceed from here. The only successful observation I have made is that the $\frac{1}{n}$ does indeed make the $n!$ divisors balance - no further progress has been made. It has been shown to me - here - that $1,2,3$ are equivalent - but how are they equivalent to $4$ ?","['complex-analysis', 'proof-explanation', 'lagrange-inversion', 'combinatorics']"
4284511,Eventually $\implies$ Frequently,"Sequence $<x_n>$ is called eventually in $X$ if $\exists n_0\in \mathbb{N}$ such that $x_n\in X,\forall n\geq n_0$ Sequence $<x_n>$ is called frequently in $X$ if $\forall n_1\in \mathbb{N},\exists n\in \mathbb{N}$ such that $x_n\in X \,\text{and}\, n\geq n_1$ My lecture notes say eventually $\implies$ frequently because $\forall n_1\in \mathbb{N}\ni n_1\geq n_0,\exists n\in \mathbb{N}$ such that $x_n\in X \,\text{and}\, n\geq n_1$ . But this prove only works for $\forall n_1\geq n_0$ not $\forall n_1\in \mathbb{N}$ . So how could this proof be enough? Shouldn't the proof be eventually $\implies$ frequently because $\forall n_1\in \mathbb{N},\exists n\in \mathbb{N}\ni n\geq n_0$ such that $x_n\in X \,\text{and}\, n\geq n_1$ .","['sequences-and-series', 'real-analysis']"
4284575,Proving an operator is invertible and find inverse,"I been having trouble with the following question. This is a homework problem, so I just ned a bit of a push in the right direction. Suppose we have $E_1=\{f\in C^1([0,1])\,|\,f(0)=0\}$ with the norm $$
\|f\|=\sup_{x\in[0,1]}f(x)+\sup_{x\in[0,1]}f'(x)
$$ And $E_2=C([0,1])$ with the usual norm. Then, prove the operator $A$ given by $$
(Af)(x)=f'(x)-f(x)
$$ is invertible. Firstly, the space isn't Banach, so that rules out using a lot of the powerful theorems that I've learnt. To prove invertibility, the only way forward I can see is demonstrating the inequality $$||Af||\geq m||f||$$ For some $m>0$ . However, I'm having trouble working around the supremums to get this to work.
Then, to find the inverse, I've been told I should use the integrating factor method to solve the ODE. I haven't taken any applied math courses and know nothing about ODEs, so I'm struggling to see how to apply this to solve for the inverse operator. Some help here would be fantastic.
Thanks!","['operator-theory', 'analysis', 'functional-analysis', 'ordinary-differential-equations']"
4284602,Distance of scaled point on the unit sphere to an integer,"Let $S:=\{ x\in \mathbb{R}^d:||x||_2=1\}$ be the d-dimensional unit sphere, where $||x||_2$ is the euclidean norm. Let $\epsilon>0$ and $s\in S$ be an arbitrary point on the sphere. Is it correct that there exists an $\alpha>0$ and a $k\in \mathbb{Z}^d\setminus\{0\}$ such that the
distance between $\alpha s-k$ is less than $\epsilon$ ? In other words can i scale every point on the unit sphere such that the distance to an non-zero integer is arbitrarily small",['real-analysis']
4284667,Question about John Lee's Proof of Whitney's Embedding Theorem on the noncompact case,"I am reading the proof of the Whitney Embedding Theorem from John Lee's Introduction to Smooth Manifolds. However, I cannot figure out some statements from the proof. The proof relies on Lemma 6.14 that states if $M$ is a smooth $n$ -manifold with or without boundary and $M$ admits a smooth embedding into $\mathbb{R}^N$ for some $N$ , then it admits a proper smooth embedding into $\mathbb{R}^{2n+1}$ . The first part of the proof proves the theorem in the case $M$ is compact. Then according to the errata of the book, this argument applies to the case when $M$ is an arbitrary compact subset of a larger manifold $\tilde{M}$ with or without boundary, by covering $M$ with finitely many coordinate balls or half-balls for $\tilde{M}$ . The result is a smooth injective map $F:M\to \mathbb{R}^{nm+m}$ whose differential is injective at each point. [This is needed in the ensuing argument for the noncompact case, because the sets $E_i$ might not be regular domains when $\partial M \neq 0$ .] However, in the second part of the proof, I cannot see how lemma 6.14 applies to the compact sets $E_i$ . So the first part shows that for each $i$ there is a smooth injective map of $E_i$ into some Euclidean space whose differential is injective eat each point. But $E_i$ here, in the case $\partial M \neq 0$ , is only a compact subset of $M$ , so without a reference as to it being a codimension-0 submanifold of $M$ , the hypothesis of a smooth $n$ -manifold with or without boundary is not satisfied for $E_i$ . Indeed, the definition of smooth embedding requires the domain to be a smooth manifold but here we only have that $E_i$ is a compact subset of a smooth manifold. So how do we ensure an embedding $\varphi: E_i \to \mathbb{R}^{2n+1}$ ? Finally, in the $F$ constructed, how do we know that $F$ is proper because $f$ is?
I would greatly appreciate some help here.","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4284669,How do you deduce that matrices are equal,"i wanted to ask for a clarification. I was looking around in my linear algebra text when i reached this justification: Considered two coloumn vectors $X$ and $Y$ , and assume $$X ^tC Y = X^t C^t Y$$ for every $X,Y \in V$ , with $V$ an $n$ -dimensional vectorial space, with $X^t, Y^t$ being the transposed of $ X, Y$ . My book says that because of this is valid for every $X,Y$ , we can deduce: $$ C^t = C$$ Now, it is intuitively true, but i was wondering if ,maybe the general sum (it's a bilinear form) could equals without needing $C^t = C$ .
I've seen this type of justification also in other theorems, but i want to know if there is a way to prove it formally, beacuse i'm not satisfied. Thanks in advance.","['matrices', 'linear-algebra']"
4284674,Adding an element to a group,"I have a abelian group $(G,+)$ , and element $a \in G$ and some positive integer $n$ . I would like to add an element $b$ to $G$ such that $nb=a$ . To be more precise, I want to construct an Abelian group $G'$ such that $G$ is a subgroup of $G'$ and $G'$ has an element $b$ such that $nb=a$ . What I am thinking Consider $$
G':= G \times \{0,1,2, \ldots, n-1 \}
$$ with the operation $$
(g,k) \oplus (h, m)= \left\{
\begin{array}{cc}
(g+h, k+m) & \mbox{ if } k+m <n \\
(g+h+a, k+m-n) & \mbox{ if } k+m \geq n \\
\end{array}
\right.
$$ This looks like adding the 2-digit ""numbers $gk+hm$ with carrying. My intuition tells me that $(G', \oplus)$ is indeed a group, and then $b=(0,1)$ has the desired property. But it looks painfull to show that $G'$ is a group, and I would be surprised if this operation is not known ( maybe in the context of semi-direct products?). Question Is this construction known? If yes, what is a good reference for it? P.S. I think I've seen something like this in a group theory course I took more than 20 years ago, but not sure :)","['group-extensions', 'group-theory', 'abelian-groups']"
4284733,Combining probability density funciton and probability mass function,"I was working on this problem: Let N be a geometric random variable with parameter p. Suppose that the conditional distribution of X given that N = n is the gamma distribution with parameters n and λ. Find the conditional probability mass function of N given that X = x. And in the solution, they started with $$P(N=n|X=x)=\frac{f_{X|N}(x|n)P(N=n)}{f_{X}(x)}$$ However why is that they can simply combine the p.d.f. of X conditioned on N with the p.m.f. of N? Pmf expresses probability while pdf is a probability density.","['conditional-probability', 'statistics', 'geometric-probability']"
4284738,Children's Fruit Division,"How many ways can $11$ apples and $9$ pears be divided between 4 children so that each child receives five fruits? (Apples are identical. just like pears). Solution: $f\left(x,y\right)=\left(x^5+x^4y+x^3y^2+x^2y^3+xy^4+y^5\right)^4$ $f\left(x,y\right)=\left(\frac{x^6-y^6}{x-y}\right)^4$ $f\left(x,y\right)=\left(x^6-y^6\right)^4{\cdot\left(x-y\right)}^{-4}$ $f\left( x,y \right)={{\left( {{x}^{6}}-{{y}^{6}} \right)}^{4}}\cdot {{x}^{-4}}{{\left( 1-\frac{y}{x} \right)}^{-4}}$ $f\left(x,y\right)=\left(x^{24}-4x^{18}y^6+{6x}^{12}y^{12}-{4x}^6y^{18}+y^{24}\right)\cdot\sum_{k=0}^{\infty}\binom{3+k}{k}x^{-4-k}y^k$ The coefficient of $x^{11}y^9$ in $f\left(x,y\right)=\binom{3+9}{9}-4\binom{3+3}{3}=140.$ I'm right?","['solution-verification', 'combinatorics']"
4284778,Probability of passing the 3rd exam,"Let $A_i$ be the event that the student passes the ith exam. The probability of passing the 1st exam is 0.80. If he passes the 1st exam then the chance of passing the second is 0.81 but if fails the first then the chance he passes the second drops to 0.40. If he passes both then his chance of passing the third exam is 0.82. If he only passes one of the first two then the chance of passing the third is 0.60. If he fails both then the chance of passing the 3rd is 0.10. Probability of passing the 1st and 2nd exam: $P(A_1A_2)=P(A_2|A_1)P(A_1)=(0.81)(0.80)=0.648$ Probability of failing the 1st and 2nd exam: $P(A_1^CA_2^C)=P(A_2^C|A_1^C)P(A_1^C)=(1-0.80)(1-0.40)=0.12$ Probability of failing the 1st but passing the 2nd: $P(A_1^CA_2)=P(A_2|A_1^C)P(A_1^C)=(0.40)(0.20)=0.08$ Probability of passing the 3rd exam: $P(A_3)=P(A_3|A_1A_2)P(A_1A_2)+P(A_3|A_1^C \cup A_2^C)P(A_1^C \cup A_2^C)$ I'm struggling with #4, I'm not sure if I have the right idea in my attempt since the $A_i$ 's are not mutually exclusive and if my attempt is correct, I still don't know how to go about finding $P(A_3|A_1^C \cup A_2^C)$ . Please let me know how I could do #4 and if anything needs to be corrected.","['conditional-probability', 'probability']"
4284791,Show that the function it is a cumulative distribution function of some random vector,"Show that the function $$F(x,y)=\left\{\begin{array}{l}
(1-e^{-x})(1-e^{-y}) & \text {if} x \geqslant 0, y \geqslant 0\\
0 & \text {in another case}
\end{array}\right.$$ it is a cumulative distribution function of some random vector Normally it is asked to calculate the distribution function, here how to show that it actually comes from a random vector","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4284803,How to prove $\sin \frac{1}{k} > \frac{1}{k} - \frac{1}{k^2}$,"I am solving a question and I can't get over this step: proving $$\sin \frac{1}{k} > \frac{1}{k} - \frac{1}{k^2}$$ where $k$ is a positive integer. I tried using induction, but I failed. One of my friends managed to prove it using derivatives but I am searching for a solution which does not involve calculus or series. Proving this would help me solve a convergence problem.","['contest-math', 'trigonometry', 'inequality']"
4284841,Can a real matrix have arbitrary complex eigenvalues?,"Given a set $S$ of complex numbers such that $z\in S \implies \bar{z}\in S$ can I find a real matrix $M$ (in a space of dimension $|S|$ ) whose eigenvalues are precisely those in $S$ ? More generally is there a way to know when a real matrix M does exist? I'm struggling to come up with any counter examples. I thought about proving the stronger statement ""Every complex matrix is similar to a real matrix"" but I don't think thats easier to show. Something like jordan normal form could be useful but we don't really care about the jordan structure, just the eigenvalues. Another stronger (but less so than the last) that could work is ""Every diagonalisable complex matrix is similar to a real matrix"". There are also some nice ideas like knowing that the determinant and trace of a real matrix are real but the conjugate property probably makes this useless.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4284881,A question about a coincidence by multiple solutions,"I am solving a geometry problem as below I would like to find the value of $A'CA$ . I came up with three constructions, but they seem to be in short of one condition, namely: Construct $CD$ such that $CD\parallel AA'$ But this doesn't show that $A'B\parallel AC$ Reflect $\triangle A'AB$ , but that doesn't show that $B'C$ is a straight line Construct $BD$ such that $BD\parallel A'A$ , but that doesn't show that $A'B\parallel AC$ as well. Can these constructions find the answer or there exists some restrictions and is this a coincidence?",['geometry']
4284887,If $L \in GL_n(\mathbb{Q})$ such that $L^{-1} = L + L^2$ then $3\mid n$,"Suppose $L \in GL_n(\mathbb{Q})$ , i.e, a linear invertible map on $\mathbb{Q}^n$ . Then  I want to see that if $L^{-1} = L + L^2$ then $3\mid n$ . I have no idea how to start this.","['matrices', 'linear-algebra', 'linear-transformations']"
4284903,"What makes a regular measure ""regular""?","I get that definition, that $m$ is a regular measure if: $$m(A) = \inf \left\{ m(U) \: | \: A \subset U, \text{ U - open} \right\} = \sup \left\{ m(K) \: | \: K \subset A, \text{ K - compact} \right\}$$ But what makes this definition a definition of a regular measure? What makes a regular measure ""regular""? Why are regular measures interesting for us?",['measure-theory']
4284962,Probability of a egg surviving with a Poisson distribution,"The probability that a bird deposits $r$ eggs in its nest is given by the Poisson distribution with parameter $\lambda$ . Assume that the probability of a egg to survive in nature is $p$ and that the survival of each egg is independent. Knowing that a bird deposited at most $3$ eggs, what is the probability that there is exactly one survival egg? My attempt to solve this was as follows: P(exactly one survives| at most $3$ deposited) = P(exactly one survives and at most $3$ deposited)/P(at most $3$ deposited) P(at most $3$ deposited) = $\sum_{r=0}^{3}\frac{e^{-\lambda}{\lambda}^r}{r!}$ P(exactly one survives and at most $3$ deposited) = $\sum_{r=1}^{3}\frac{e^{-\lambda}{\lambda}^r \cdot {r \choose 1} \cdot p \cdot (1-p)^{r-1}}{r!}$ where the $r=0$ term vanishes because we can't have a egg surviving if there is none in the nest. Finally the result follows dividing 3./2. is that correct, could someone help me?","['poisson-distribution', 'statistics', 'probability-distributions', 'probability']"
4284964,Inequality involving Lipschitz derivative and Taylor's theorem,"I have come across an inequality that is supposed to follow from Taylor's theorem and I thought it was obvious until I realized I had the incorrect statement for the multi-variable version written down. How does the statement below follow from Taylors theorem? Let $f\in C^1(\mathbb{R}^n)$ and $|\nabla f(x) - \nabla f(y)| \leq L |x-y|$ , then by Taylor's theorem $$
f(x+h) \leq f(x) + \nabla f(x)\cdot h + \frac{L}{2}\Vert h\Vert^2
$$ Though we know the second derivative exists almost everywhere it doesn't seem like we can use it in Taylors theorem. Maybe a bound for the Peano remainder would work? Any thoughts?","['multivariable-calculus', 'real-analysis']"
4284968,Polynomials whose fractional part behaves like a logarithm,"For a given integer $m$ , I'm looking for a classification of all polynomials $P$ with rational coefficients satisfying the logarithm-like condition $$P(ab)=P(a)+P(b) \pmod 1$$ for any integers $a, b$ coprime to $m$ . I'm interested in these polynomials because they can be used to define Dirichlet characters of the form $$\chi_P(n) = \begin{cases} 0, & \gcd(n,m)>1 \\ e^{2\pi i P(n)}, & \gcd(n,m)=1\end{cases},$$ using the fact that the sequence $n \mapsto P(n) \pmod 1$ is periodic for any rational polynomial $P$ . I consider two rational polynomials equivalent if they induce the same polynomial function modulo $1$ on integers coprime to $m$ , i.e., if the associated characters are the same. Note that the logarithm-like condition implies that $P(1) = 0 \pmod 1$ , so up to equivalence it can be assumed that $P(1) = 0$ , that is, $(x-1)|P$ . The only examples of logarithm-like polynomials I've been able to find so far are equivalent to integral linear combinations of the degree $N$ polynomials $$P_{N,m}(x)=\frac{x^N - 1}{\Delta_{N,m}^2},$$ where $\Delta_{N,m} = \gcd\limits_{(a,m) \text{ coprime}}(a^N-1)$ . It is easy to prove that these polynomials satisfy the logarithm-like condition by an argument similar to the one in this answer . Not all of these polynomials will be inequivalent in general, e.g. all the polynomials with $N$ odd are equivalent to each other. For instance, if we fix $m=k!$ , in the limit of big $k$ the resulting sequence of polynomials $P_{N}(x) = \lim_{k\to \infty} P_{N,k!}(x)$ becomes $$P_{1}(x) = \frac{x-1}{2^2}, \: P_{2}(x) = \frac{x^2-1}{24^2}, \: P_{3}(x) = \frac{x^3-1}{2^2}, \: P_{4}(x) = \frac{x^4-1}{240^2}, \: P_{5}(x) = \frac{x^5-1}{2^2}, \: P_{6}(x) = \frac{x^6-1}{504^2}, \cdots$$ (as a curiosity, $\Delta_{N}$ for $N>1$ equals twice the denominator of the rational number $\zeta(1-N)$ , where $\zeta$ is the Riemann zeta function). Note that $P_{N,m}$ is an integral multiple of $P_N$ for any $m$ , so all the examples I know can be expressed up to equivalence as $\sum_{N=1}^{N_{\text{max}}} a_N P_N(x)$ with $a_N \in \mathbb{Z}$ . Are there any other examples, up to equivalence? EDIT: Here is a possible strategy to show that the examples above are the only ones. Consider a polynomial $P(x) = \sum_{N=0}^{N_{\text{max}}} p_N x^N$ with $p_N \in \mathbb{Q}$ that satisfies the logarithm-like condition for some $m$ . This condition can be reexpressed as $$\mathbb{Z} \ni P(ab)-P(a)-P(b) = \sum_{N=0}^{N_{\text{max}}} p_N ((ab)^N-a^N-b^N) =$$ $$= \sum_{N=0}^{N_{\text{max}}} p_N (a^N-1)(b^N-1) - \sum_{N=0}^{N_{\text{max}}} p_N.$$ But the last sum is equal to $P(1)$ , which by the remarks above may be taken to vanish. Hence we must have $$\sum_{N=0}^{N_{\text{max}}} p_N (a^N-1)(b^N-1) = \sum_{N=0}^{N_{\text{max}}} q_N \frac{a^N-1}{\Delta_{N,m}}\frac{b^N-1}{\Delta_{N,m}} \in \mathbb{Z},$$ where $q_N = p_N \Delta_{N,m}^2$ . This must hold true as $a,b$ range over all integers coprime to $m$ . If we can show that $q_N \in \mathbb{Z}$ for all $N$ , we will have proved that $P(x) = P(x)-P(1) = \sum_{N=0}^{N_{\text{max}}} q_N (x^N-1)/\Delta_{N,m}^2$ is an integral linear combination of $P_{N,m}(x)$ , and thus that the answer to my question is negative. Showing that the $q_N$ are integers for all $m$ amounts to showing that the set of vectors $$\Sigma_m = \left\{ \left(\frac{a-1}{\Delta_{1,m}}\cdot\frac{b-1}{\Delta_{1,m}}, \frac{a^2-1}{\Delta_{2,m}}\cdot\frac{b^2-1}{\Delta_{2,m}}, \ldots, \frac{a^{N_{\text{max}}}-1}{\Delta_{N_{\text{max}},m}}\cdot\frac{b^{N_{\text{max}}}-1}{\Delta_{N_{\text{max}},m}} \right) \middle\vert a,b \text{ coprime to } m \right\},$$ whose entries are integral by definition of $\Delta_{N,m}$ , is a spanning set for $\mathbb{Z}^{N_{\text{max}}}$ , since in that case the covector $(q_1,q_2,\ldots, q_{N_{\text{max}}})$ must belong to the dual lattice of $\mathbb{Z}^{N_{\text{max}}}$ , which is again $\mathbb{Z}^{N_{\text{max}}}$ . This seems intuitively plausible after some numerical experimentation (it says that the numbers $\frac{a^N-1}{\Delta_{N,m}}$ for $N=1, 2, \ldots$ are essentially ""uncorrelated""), and it can be easily checked for specific values of $m$ , but I'm not sure how to prove that it holds in general for arbitrary $m$ and $N_{\text{max}}$ . EDIT 2: The problem is more interesting than I thought: the proof strategy above fails because, as I just found, there do exist logarithm-like polynomials for which $q_N$ are noninteger. Two of these are $\frac12 P_{2,3} + \frac12 P_{1,3}$ and $\frac1{12} P_{3,2} - \frac1{12} P_{1,2}$ . I had missed them before because they turn out to be equivalent to polynomials with integer $q_N$ . E.g. the former polynomial is equivalent to $-P_{2,3}$ , and the latter is $4P_{2,2}$ . I still suspect that any logarithm-like polynomial is equivalent to one of that form, but the existence of these equivalences somewhat complicates things. As suggested by Merosity in the comments, another possible idea is to work one prime at a time by taking the $p$ -adic fractional part. We have $\{P_N\}_p = k_p P_{N,p}$ , where $k_p$ is some integer coprime to $p$ , so any logarithmic-like polynomial can be decomposed into (finitely many) $p$ -parts $P \cong \sum_p \{P\}_p$ , where each $\{P\}_p$ would be then conjecturally equivalent to an integral linear combination of $P_{N,p}$ . The problem would thus reduce to trying to prove the cases with $m=p$ first, and then finding a way to combine each individual proof into one that works for composite $m$ . A possible advantage of this approach is that $\Delta_{N,p}$ has a very simple expression (for odd primes it is $\Delta_{N,p}=p^{r+1}$ where $r$ is the maximum integer such that $p^r(p-1)|N$ , and for $p=2$ it's something similar but slightly different). However, even in this simplified setting one runs into ""exotic"" equivalences like the ones above, so I haven't been able to make much progress so far. EDIT 3: I apologize for so many edits, this is the last one for a while. I still haven't made progress on a proof, but I can't help but mention that the same phenomenon seems to hold for many other number rings $R$ , such as the Gaussian and Eisenstein integers $\mathbb{Z}[i]$ and $\mathbb{Z}[\omega]$ , or even orders in function fields like $\mathbb{F}_q[t]$ , if we replace $P(x)$ by $P(|x|)$ , where $|x|$ is the canonical (integer-valued, multiplicative) norm, and let $m$ be any element of $R$ . We then have that $\Delta^{(R)}_{N,m}$ (under the same definition as above) is related to the denominator of the Dedekind or Hasse-Weil zeta function associated to $R$ , and the corresponding $P^{(R)}_N(|x|)$ again seem to form an integral spanning set of the logarithm-like ""norm polynomials"" valued in $R$ , up to equivalence. I wonder if my conjecture would be easier to prove or disprove over some of these rings instead of $\mathbb{Z}$ ; I don't have much experience working with function fields myself, but I do know that proofs are sometimes easier to find in that setting. For now I'm adding a bounty since I ran out of ideas. Any answer that makes a significant step towards the classification (over some ring) would be appreciated, even if it's not a full proof.","['modular-arithmetic', 'dirichlet-character', 'number-theory', 'arithmetic-functions', 'polynomials']"
4284970,Proof Explanation: Cauchy's Theorem (Homotopy Version) - Why take discs of radii $3\epsilon$?,"I have two questions about the proof of Cauchy's Theorem (Homotopy Version) in Stein & Shakarchi's Complex Analysis . This is Theorem $5.1$ , in Chapter $3$ . If you have a copy, see Pg. $93-95$ . By uniform continuity of $F$ , how do we get $\delta$ such that $$|s_1 - s_2| < \delta \implies \sup_{t\in [a,b]} |\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon$$ My thoughts: By uniform continuity of $F$ , we have for every $\epsilon > 0$ , some $\delta > 0$ such that $\sqrt{(s_1-s_2)^2 + (t_1-t_2)^2}  < \delta$ implies $|F(s_1,t_1) - F(s_2,t_2)| = |\gamma_{s_1}(t_1) - \gamma_{s_2}(t_2)| < \epsilon$ . To prove the required implication above, we only need to show the continuity of the map $\varphi: [0,1] \to \mathcal C([a,b], \Bbb C)$ given by $\varphi(s) = F_s$ . $\mathcal C([a,b], \Bbb C)$ is endowed with the $\sup$ metric. What is the significance behind taking $3\epsilon$ and $2\epsilon$ in the proof? Why can't we take discs $\{D_0, \ldots, D_n\}$ of radii $\epsilon$ ? Pretty sure this has something to do with the claim from uniform continuity above, but I'm not able to figure it out. For $|s_1-s_2| < \delta$ , we have $|\gamma_{s_1}(t) - \gamma_{s_2}(t)| < \epsilon$ for all $t\in [a,b]$ , the two curves $\gamma_{s_1}$ and $\gamma_{s_2}$ could easily be contained in a union of discs of radii $\epsilon$ as well. Reference:","['complex-analysis', 'proof-explanation', 'analysis']"
4284992,Equivalence Relation on $\mathbb{N}\setminus\{1\}$,"Relation on $\mathbb{N}\setminus\{1\}$ : $$A_{1}=\{(x,y): x\text{ and }y \text{ are relatively prime}\}.$$ Determine which one of the three properties are satisfied: $i)$ $(2,2) \notin A_{1}$ . So it is not reflexive. $ii)$ $(2,3) \rightarrow (3,2) \in A_{1}$ . A_{1} is symmetric. $iii)$ $(2,3)$ and $(3,4)$ $\rightarrow$ $(2,4) \notin A_{1}$ . Is not transitive. That means, $A_{1}$ is not an equivalence relation. Is it okay? Thanks in advance!","['equivalence-relations', 'abstract-algebra', 'discrete-mathematics']"
4285023,Are there any explicit solutions known for this ODE?,"I am considering the following ODE, $$u''+\frac{5}{r}u'+K\left(\frac{2}{1+4r^2}\right)^2u=0$$ where $K>0$ and $u=u(r):\mathbb{R}_{+}\to \mathbb{R}.$ Can we deduce that any solution $u$ to the above ODE decays as follows $$u(r)\leq \frac{C}{r^{\alpha}}?$$","['derivatives', 'sturm-liouville', 'ordinary-differential-equations', 'real-analysis']"
4285032,"Could someone explain why $\sum_{\substack{a_1,\ldots,a_n\in\mathbb{N}_0\\a_1+\cdots+a_n=n}}\frac{n!}{a_1!\cdots a_n!}=n^n$?","I want to use this equality but I have no idea why it holds. Sure I can probably prove it via induction but it looks rather fiddly. (Let $n$ be a positive integer.) $$\sum_{\substack{a_1,\ldots,a_n\in\mathbb{N}_0\\a_1+\cdots+a_n=n}}\frac{n!}{a_1!\cdots a_n!}=n^n$$ Is there a simpler way/intuition to explain this equality? It looks like something from combinatorics/probability but I cannot exactly recall what. Thank you so much in advance! Edit: This arise from a homework problem where I am asked to look at the probability of arranging $n$ objects where there are indistinguishable items, $a_1$ number of first item etc and hence the expression in the sum. I wanted to sum up every possibility and thus I was curious why it would sum up to $n^n$ . (It sums up to $n^n$ , which was given in the original question but I wanted to know why.)","['power-towers', 'tetration', 'combinatorics', 'probability']"
4285047,What is the orthogonal projection with expectation?,"I am reading an advanced econometrics textbook. When it talks about least squares, it says that the orthogonal projection of A onto Z is $P_Z(A)=Z^\prime E[ZZ^\prime]^{-1}E[ZA_k]$ and when A is a vector $A=(A_1,...,A_k)^\prime$ , $P_Z(A)$ is defined as $$
P_Z(A)=(Z^\prime E[ZZ^\prime]^{-1}E[ZA_1],...,Z^\prime  E[ZZ^\prime]^{-1}E[ZA_k])^\prime.
$$ Although I know the basic projection matrix is $P=A(A^\prime A)^{-1}A^\prime$ , I can't interpret the expectation in this formula. Could you help me? And I think this is some basic statistic and matrix knowledge, so to pass the econometric course, could you give me some resources or advice to learn it?","['matrices', 'statistics', 'projection-matrices', 'economics']"
4285116,Characteristic function of non-central $\chi^2$,"Consider $X_1,\ldots, X_n$ iid random variables with $X_1\sim\mathcal{N}(0,1)$ , some real numbers $\alpha_1,\ldots,\alpha_n$ . Define $Z:=\sum_{i=1}^n (X_i+\alpha_i)^2$ and $\kappa:=\sum_{i=1}^n\alpha_i^2$ I want to show by direct integration, that $$E[e^{uZ}]=\frac{\exp({\frac{\kappa u}{1-2u}})}{(1-2u)^\frac{\kappa}{2}}$$ for any $u\in\mathbb{C}$ with $\Re(u)\le 0$ . What I have done so far is the following: $$
\begin{align}
E[e^{uZ}]&=E[e^{u\sum_{i=1}^n (X_i+\alpha_i)^2}]\\
&=\prod_{i=1}^nE[e^{u (X_i+\alpha_i)^2}]\text{, since }X_i\text{ are independent}\\
&=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{u (x+\alpha_i)^2}e^{-\frac{x^2}{2}}dx\text{, since }X_i\sim\mathcal{N}(0,1)\\
&=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{(2u-1) (\frac{x+\alpha_i}{\sqrt{2}})^2}e^{x\alpha_i+\frac{1}{2}\alpha_i^2}dx\\
\end{align}
$$ The right hand side can be written as $$
\begin{align}
\frac{\exp({\frac{\kappa u}{1-2u}})}{(1-2u)^\frac{\kappa}{2}}&=\exp\Big({\frac{\kappa u}{1-2u}}-\frac{\kappa}{2}\ln(1-2u)\Big)\\
\end{align}
$$ How can I go on from here? Maybe I can apply some Fourier Inversion formula? Thanks in advance!","['statistics', 'fourier-analysis', 'characteristic-functions', 'fourier-transform', 'probability-theory']"
4285136,Eliminating $\theta$ from $x=\cos\theta(2-\cos 2\theta)$ and $y=\sin\theta(2-\sin 2\theta)$,"If $$x=2\cos\theta-\cos\theta\cos 2\theta$$ $$y=2\sin\theta-\sin\theta\sin 2\theta$$ find a relation between $x$ and $y$ (not involving $\theta$ ). Another trig elimination that has me stumped. One approach is to express as homogeneous functions, $$x=3\sin^2\theta \cos\theta+\cos^3\theta$$ $$y=2\sin^3\theta+2\sin\theta\cos^2\theta-2\sin^2\theta\cos \theta$$ and by forming linear combinations of $x,y$ find some third power trig polynomials eg $(\cos \theta+\sin\theta)^3$ . But I am not finding any success with this problem. Note that this problem is from Hobson, Treatise on Plane Trigonometry 2 ed pg.97 #47. Slightly altered in the form I ask it. And may contain a typo, see discussion.","['algebra-precalculus', 'trigonometry']"
4285143,How to determine a derivative if the derivative is dependent itself?,"Let's suppose I've got a function $f(x)$ where I'd like to differentiate with respect to $t$ , but $t$ depends on $x$ : $t(x)$ . Thus the whole linked derivative thing: $\dfrac{\mathrm{d}f(x)}{\mathrm{d}t(x)}$ . Is this possible at all? Alternatively I had to find $t^{-1}(x)$ : $x(t)$ and then calculate the derivative fairly easy: $\mathrm{d} f(x(t))/\mathrm{d}t$ . But finding the inverse is not always possible. Is there another way? At all?",['derivatives']
4285216,Dividing numerator and denominator of integrand in definite integral,"I'm trying to solve the following definite integral $$\int_0^{\frac{\pi}{4}} \sqrt{\tan  x}\,dx$$ First, I made the substitution $u = \sqrt{\tan x}$ And arrived with: $$\int_0^1 \frac{2u^2}{u^4+1}\,du$$ When divided both numerator and denominator of the integrand by $u^2$ and with some manipulation I got $$\int_0^1 \frac{1+\frac{1}{u^2}}{(u-\frac{1}{u})^2+2}+\frac{1-\frac{1}{u^2}}{(u+\frac{1}{u})^2-2}\, du$$ This integral could be solved easily with u sub, giving the result: $$\left.\frac{1}{2\sqrt{2}} \ln\left|\frac{u^2-\sqrt{2}u+1}{u^2+\sqrt{2}u+1}\right|+\frac{1}{\sqrt{2}}\arctan\left(\frac{1}{\sqrt{2}}\left(u-\frac{1}{u}\right)\right)\ \right|_0^1$$ The integral however is not defined at $u=0$ , my question is: Are we allowed to do this kind of manipulation to definite integrals with rational function, and will proceeding with improper integral yield the correct result?","['integration', 'definite-integrals']"
4285217,"Measurability of maximum likelihood estimator. Is there a mistake in Lehmann's ""Theory of point estimation""?","I'm trying to prove that MLE from the proof of one theorem in Lehmann's ""Theory of point estimation"" (the theorem is below) is a measurable function. I know that under some regularity conditions (e.g. stats.stackexchange.com/questions/430954/example-of-a-non-measurable-maximum-likelihood-estimator
or when is the maximum likelihood estimator measurable )  MLE is measurable, but it didn't help me to prove that the closest root from the proof is a measurable function. The problem is as follows. By definition, an estimate is a measurable function. Even the existence of a measurable version of MLE is not so obvious, but here an extremum arises over the set of MLE. It is unlikely that a theorem from a classical book can be wrong, but how to prove it? The theorem is here: I think that in $(3.14)$ the set $|\hat{\theta}_n - \theta_0|$ may be not measurable (but I don't know counterexample).","['statistical-inference', 'statistics', 'measurable-functions', 'probability-theory', 'probability']"
4285283,Does Verdier Duality Fix Simple Perverse Sheaves?,"Suppose you have a connected reductive algebraic group $G$ acting on a ""nice"" variety $X$ , so that the orbits decompose $X$ into a Whitney stratification. Consider the category of Perverse Sheaves $\textbf{Per}(X)$ resulting from this construction. Is it the case that Verdier duality ""fixes"" the simple objects in this category? That is, denoting the Verdier dualtiy functor by $\mathbb{V}$ , is it the case that $\mathbb{V}IC(\mathcal{L}) \cong IC(\mathcal{L})$ for any irreducible local system $L$ on one of the orbits? If not, what if we restricted our attention to the simple equivariant perverse sheaves? Thanks in advance!","['algebraic-geometry', 'representation-theory', 'algebraic-groups', 'd-modules']"
4285297,A detailed and self-contained proof of Tonelli's theorem,"Motivation: I have seen the interchange of limit/derivative and integral many times, but don't know how such operation makes sense. I've always desired to remove this uncertainty by giving a detailed proof in which any argument is justified. In doing so, I've searched on the Internet and found that most proofs are either too long and need so many between lemmas (in this case, I feel that the main ideas are hidden), or omit important details (at least details not obvious for me). Then I come across these $2$ lectures ( here and here ). I basically copy the whole argument in the former for the measurability part. For the part of repeated integral, I'm proud that I do not copy but adapt the idea in the latter lecture (which is for complete measure) to my case (which is $\sigma$ -finite measure). It takes me $3$ days to complete the proof. I'm very happy because I now understand the machinery that makes the theorem right. This effort also solidifies my understanding of monotone and dominated convergence theorems. My question: My proof is very clear and detailed, so it's easy to follow. I hope somebody will take time help me verify it and point out anything unclear or incorrect. Thank you so much for your help! Related definitions of Bochner integrals can be found here . Let $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ be $\sigma$ -finite measure spaces. $\Sigma := \mathcal A \times \mathcal B$ . $\mathcal C := \mathcal A \otimes \mathcal B$ the product $\sigma$ -algebra of $\mathcal A$ and $\mathcal B$ . $\lambda := \mu \otimes \nu$ the product measure of $\mu$ and $\nu$ . $\mathcal S (X \times Y, \lambda, {\mathbb R}^+)$ the space of $\lambda$ -simple functions from $X \times Y$ to ${\mathbb R}^+$ . $\mathcal  L_0 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ the space of $\nu$ -measurable functions $X \times Y$ to $\overline{\mathbb R}{}^+$ . $\mathcal  L_1 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ the space of $\nu$ -integrable functions $X \times Y$ to $\overline{\mathbb R}{}^+$ . Tonelli's theorem: Let $f: X \times Y \to \overline{\mathbb R}{}^+$ measurable. $f_x: Y \to \mathbb R, \, y \mapsto f(x, y)$ for all $x \in X$ . $f_y: X \to \mathbb R, \, x \mapsto f(x, y)$ for all $y \in Y$ . Then (i) The maps $$\phi: X \ni x \mapsto \int_Y f_x  \, \mathrm d \nu  \quad \text{and} \quad \psi: Y \ni y \mapsto \int_X f_y  \, \mathrm d \mu $$ are measurable. (ii) $$\int_X \phi  \, \mathrm d \mu = \int_{X \times Y} f \, \mathrm d \lambda = \int_Y \psi \, \mathrm d \nu.$$ Proof: Lemma: Let $x \in X$ and $G \in \mathcal C$ . The $x$ -section $G_x$ defined by $G_x := \{y \in Y \mid (x,y) \in G\}$ is measurable. Proof: Let $\mathcal D_x := \{G \in \mathcal C \mid G_x \text{ is measurable}\}$ . Notice that $(G^c)_x =(G_x)^c$ and $(\bigcap_n G_n)_x = \bigcap_n (G_n)_x$ . So $\mathcal D_x$ is a $\sigma$ -algebra over $X \times Y$ . For $A \times B \in \Sigma$ , $(A \times B)_x = B$ if $x \in A$ and $\emptyset$ otherwise. So $\Sigma \subseteq \mathcal D_x$ and thus $\mathcal D_x = \mathcal C$ . We fix a non-decreasing sequence $(Y_n)$ of sets in $\mathcal B$ such that $\bigcup Y_n = Y$ and $\nu(Y_n) < \infty$ . By symmetry, it's sufficient to prove for $\phi$ . We proceed to prove (i) for $f =1_G$ with $G \in \mathcal C$ . We write $\phi_G$ instead of $\phi$ . Then $\phi_G(x) = \int_Y 1_{G} (x,y) \mathrm d \nu(y) = \int_Y 1_{G_x} \mathrm d \nu = \nu(G_x)$ , which is well-defined by our Lemma .  Let $\mathcal D := \{G \in \mathcal C \mid \phi_G \text{ is measurable}\}$ . First, we assume $\nu$ is finite. If $G \in \mathcal D$ , then $\phi_{G^\mathrm c}  = \nu(Y) -\phi_G$ is measurable because $\nu(Y)<\infty$ and $\phi_G$ is measurable. If $(G_n)$ is a sequence of pairwise disjoint (p.w.d.) sets in $\mathcal D$ , then $\phi_{\bigcup G_n}  = \sum_n \phi_{G_n}$ is measurable. As a result, $\mathcal D$ is indeed a $\sigma$ -algebra. If $G= A\times B \in \Sigma$ , then $\phi_G = \nu(B) 1_A$ which is simple and thus measurable. Hence $\Sigma \subseteq \mathcal D$ and thus $\mathcal D = \mathcal C$ . Second, we assume $\nu$ is $\sigma$ -finite. We define a sequence $(\nu_n)$ of finite measures on $Y$ by $\nu_n(B) := \nu(B \cap Y_n)$ for $B \in \mathcal B$ . Let $\phi_{G,n}(x) := \nu_n(G_x)$ and $\mathcal D_n := \{G \in \mathcal C \mid \phi_{G, n} \text{ is measurable}\}$ . As in the case $\nu$ is finite, we can prove that $\mathcal D_n = \mathcal C$ . We have $G_x \cap Y_n \nearrow G_x \cap Y = G_x$ , so $\nu_n (G_x) =\nu(G_x \cap Y_n) \nearrow \nu(G_x)$ by continuity from below of measure. This means $\phi_{G,n} \nearrow \phi_G$ and thus $\phi_{G}$ is measurable for $G \in \mathcal C$ . Hence $\mathcal D = \mathcal C$ . By linearity of integrals, (i) also holds for $\mathcal S (X \times Y, \lambda, \mathbb R^+)$ . If $f \in \mathcal L_0 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ , then there is a non-decreasing sequence of $(f_n)$ in $\mathcal S (X \times Y, \lambda, \mathbb R^+)$ such that $f_n \to f$ . It follows that (i) also holds for $f \in \mathcal L_0 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ . We proceed to prove (ii) for $f =1_G$ with $G=A \times B \in \Sigma$ . As shown above, $\phi_G = \nu(B) 1_A$ , so $\int_X \phi_{G} \, \mathrm d \mu = \mu(A)\nu(B)$ . On the other hand, $\int_{X \times Y} 1_{G} \, \mathrm d \lambda = \lambda(A\times B) = \mu(A) \nu(B)$ by construction of product measure $\lambda$ . Hence (ii) holds for this kind of $f$ . We proceed to prove (ii) for $f =1_G$ with $G = \bigcup_k A_k \times B_k$ with $A_k \times B_k \in \Sigma$ . By this result, we can assume that $(A_k \times B_k)$ are p.w.d., which implies $\phi_{G}  = \sum_{k} \phi_{A_k \times B_k}$ and $1_{G}  = \sum_{k} 1_{A_k \times B_k}$ . Clearly, $\sum_{k=1}^n \phi_{A_k \times B_k} \nearrow \phi_G$ and $\sum_{k=1}^n 1_{A_k \times B_k} \nearrow 1_{G}$ as $n \to \infty$ .  As such, \begin{align}
\int_X \phi_{G} \mathrm d \mu
&= \int_X \lim_n \sum_{k=1}^n \phi_{A_k \times B_k} \mathrm d \mu &&\overset{(1)}{=} \lim_n \int_X \sum_{k=1}^n \phi_{A_k \times B_k} \mathrm d \mu \\
&\overset{(2)}{=} \lim_n \sum_{k=1}^n \int_{X} \phi_{A_k \times B_k} \mathrm d \mu &&\overset{}{=} \lim_n \sum_{k=1}^n \int_{X \times Y}  1_{A_k \times B_k} \mathrm d \lambda \\
&\overset{(3)}{=} \lim_n \int_{X \times Y} \sum_{k=1}^n 1_{A_k \times B_k} \mathrm d \lambda &&\overset{(4)}{=} \int_{X \times Y} \lim_n \sum_{k=1}^n 1_{A_k \times B_k} \mathrm d \lambda\\
&= \int_{X \times Y} 1_G \mathrm d \lambda.
\end{align} Here we apply monotone convergence theorem (m.c.t) to obtain $(1)$ and $(4)$ . The interchange of finite sum and integral in $(2)$ and $(3)$ is valid [even if all individual integrals are $\infty$ ] because all individual functions are non-negative. Hence (ii) also holds for this type of $G$ . We proceed to prove (ii) for $f =1_G$ with $G \in \mathcal C$ and $\lambda (G) < \infty$ . By construction of product outer measure and the fact that $\lambda(G) < \infty$ , there exists a sequence $(G_n)$ such that $G_n = \bigcup_m A^{(n)}_m \times B^{(n)}_m \supseteq G$ , $A^{(n)}_m \times B^{(n)}_m \in \Sigma$ , and $\lambda (G_n) \searrow \lambda(G)$ . Without loss of generality, we assume $\lambda(G_1) < \infty$ . Let $E_k := \bigcap_{n=1}^k G_n$ and $E := \bigcap_k E_k$ . Then $G \subseteq E$ , $\lambda(G) = \lambda(E)$ , $E_1 = G_1$ , and $E_k \searrow E$ . By results (a) and (b) , $E_k$ can be written as countable union of p.w.d. sets in $\Sigma$ . As shown above, $\int_X \phi_{E_k} \mathrm d \mu = \int_{X \times Y} 1_{E_k} \mathrm d \lambda$ for all $k$ . Notice that $E_x = (\bigcap_{k} E_k)_x = \bigcap_{k} (E_k)_x$ implies $(E_k)_x \searrow E_x$ . By continuity from above of measure, $\phi_{E_k} = \nu((E_k)_x) \searrow \nu(E_x) =\phi_{E}$ and $1_{E_k} \searrow 1_E$ . We get \begin{align}
\int_X \phi_{E} \mathrm d \mu &= \int_X \lim_k \phi_{E_k} \mathrm d \mu &&\overset{(5)}{=} \lim_k \int_X \phi_{E_k} \mathrm d \mu \\
&= \lim_k \int_{X \times Y} 1_{E_k} \mathrm d \lambda &&\overset{(6)}{=}  \int_{X \times Y} \lim_k 1_{E_k} \mathrm d \lambda \\
&= \int_{X \times Y} 1_{E} \mathrm d \lambda &&= \lambda(E).
\end{align} It follows from $E_k \subseteq E_1$ that $0 \le \phi_{E_k} \le \phi_{E_1}$ and $0 \le 1_{E_k} \le 1_{E_1}$ for all $k$ . Also, $\int_X \phi_{E_1} \mathrm d \mu = \int_{X \times Y} 1_{E_1} \mathrm d \lambda = \lambda(E_1)= \lambda(G_1) < \infty$ . Here we apply dominated convergence theorem (d.c.t.) to obtain $(5)$ and $(6)$ . Let $F := E \setminus G$ . We have $\lambda(E) = \lambda (G) + \lambda(F)$ and $\lambda(E) = \lambda(G) < \infty$ , so $\lambda (F) =0$ . With the same reasoning, there is a sequence $H_1 \supseteq H_{2} \supseteq \cdots \supseteq H_k \supseteq \cdots$ such that $H_k$ is a countable union of p.w.d sets in $\Sigma$ and that $H := \bigcap_k H_k \supseteq F$ , $\lambda(F) = \lambda (H)=0$ , $\phi_{H_k} \searrow \phi_H$ , and $1_{H_k} \searrow 1_H$ . By twice applications of d.c.t again, we get $\int_X \phi_{H} \mathrm d \mu = \int_{X \times Y} 1_{H} \mathrm d \lambda = \lambda(H) = 0$ . This means $\nu(H_x) =\phi_H (x) = 0$ for $\mu$ -a.e $x \in X$ . On the other hand, $F_x \subseteq H_x$ is measurable by our Lemma, so $\phi_F (x) =\nu (F_x)=0$ for $\mu$ -a.e $x \in X$ . Hence $\int_X \phi_{F} \mathrm d \mu =0$ . It follows from $G \cup F = E$ and $G \cap F =\emptyset$ that $\phi_G +\phi_F = \phi_E$ and thus $\int_X ( \phi_{G} + \phi_F) \mathrm d \mu  = \int_X \phi_{E} \mathrm d \mu$ . Notice that $\phi_G, \phi_F \ge0$ , so $\int_X \phi_{G} \mathrm d \mu + \int_X \phi_{F} \mathrm d \mu = \int_X \phi_{E} \mathrm d \mu$ [even if both individual integrals are $\infty$ ] and thus $\int_X \phi_{G} \mathrm d \mu = \int_X \phi_{E} \mathrm d \mu$ . It follows from $\int_X \phi_{E} \mathrm d \mu = \lambda (G)$ that (ii) holds for $G \in \mathcal C$ with $\lambda (G) < \infty$ . By linearity of integrals, (ii) also holds for $\mathcal S (X \times Y, \lambda, \mathbb R^+)$ . If $f \in \mathcal L_0 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ , then there is a non-decreasing sequence of $(f_n)$ in $\mathcal S (X \times Y, \lambda, \mathbb R^+)$ such that $f_n \to f$ . By m.c.t., (ii) also holds for $f \in \mathcal L_0 (X \times Y, \lambda, \overline{\mathbb R}{}^+)$ .","['measure-theory', 'solution-verification', 'fubini-tonelli-theorems']"
4285327,Why do we need hypothesis of complete measure in this version of Fubini's theorem?,"I'm reading below Fubini's theorem in page 3 of this lecture note. Let $(X, \mathcal{A}, \mu)$ and $(Y, \mathcal{B}, \nu)$ be complete measure spaces, let $\gamma$ be the product outer measure on $X \times Y$ constructed above, and suppose that $f: X \times Y \rightarrow \mathbb{R}$ is $\gamma$ -integrable. Then (i) $f(x, y)$ is a $\mu$ -integrable function of $x$ for $\nu$ -a.e. $y \in Y$ ; (ii) $\int_{X} f(x, y) d \mu(x)$ is a $\nu$ -integrable function of y; (iii) $\int_{Y}\left(\int_{X} f(x, y) d \mu(x)\right) d \nu(y)=\int_{X \times Y} f(x, y) d \gamma$ . In the proof, $C$ is a $\gamma$ -measurable set of finite measure. For all $j$ , $\left\{A_{i}^{j} \times B_{i}^{j}\mid i=1,2, \ldots\right\}$ is pairwise disjoint family of $\mathcal{A}, \mathcal{B}$ rectangles. $E=\cap_{j}\left(\cup_{i} A_{i}^{j} \times B_{i}^{j}\right) \setminus C$ . In my understanding, $E = \left [\cap_{j}\left(\cup_{i} A_{i}^{j} \times B_{i}^{j}\right) \right] \cap C^c$ is $\gamma$ -measurable. Hence the $y$ -slice $E_y$ of $E$ defined by $$E_y \triangleq \{x \in X \mid (x, y) \in E\}$$ is also measurable by the lemma in this question. I mean by this lemma that we don't need the hypothesis of measure completeness to obtain the measurability of $E_y$ . However, the author said that But $\left.E \subset \cap\left(\cup_{i} E_{i}^{j} \times F_{i}^{j}\right)\right)$ and $\nu$ is a complete measure , so the slice $\{x:(x, y) \in E\}$ is also in $\mathcal{A}$ and also has $\mu$ -measure zero for $\nu$ -a.e. $y \in E$ . So they mean the measure completeness is necessary for the slice $E_y$ to be measurable. Could you please elaborate on my confusion?","['proof-explanation', 'measure-theory', 'fubini-tonelli-theorems', 'outer-measure']"
4285350,Is there a natural number $n$ for which $\sqrt[n]{22-10\sqrt7}=1-\sqrt7$,"Is there a natural number $n$ for which $$\sqrt[n]{22-10\sqrt7}=1-\sqrt7$$ My idea was to try to express $22-10\sqrt7$ as something to the power of $2$ , but it didn't work $$22-10\sqrt7=22-2\times5\times\sqrt7$$ Since $5^2=25, \sqrt7^2=7$ and $25+7\ne22$ . What else can we try?","['algebra-precalculus', 'radicals']"
4285391,What is the meaning of entire solution in LaSalle's invariance principle and how to apply this principle?,"This is from Hirsch/Smale/Devaney's textbook ""Differential Equations and Introduction to Chaos"" Page 199 Theorem. (Lasalle’s Invariance Principle) Let $X^∗$ be an equilibrium point for $X^\prime = F(X)$ and let $L : U
 → R$ be a Liapunov function for $X^∗$ , where $U$ is an open set
containing $X^∗$ . Let $P ⊂ U$ be a neighborhood of $X^∗$ that is
closed. Suppose that $P$ is positively invariant and that there is no
entire solution in $P − X^∗$ on which $L$ is constant. Then $X^∗$ is
asymptotically stable, and $P$ is contained in the basin of attraction
of $X^∗$ I am struggling with the meaning of ""no entire solution on which $L$ is constant"" part. It seems to be a terminology that is very specific to this theorem and found nowhere else (even in other books). I don't intuitively understand why this is required. For example, consider the famous Pendulum example , $$\dot x_1 = x_2$$ $$\dot x_2 = -a\sin(x_1) - bx_2$$ We wish to look at the stability of the equilibrium at $0$ .
They showed that you can have a Liapunov function, $L(x) = a(1-\cos(x_1)) + x_2^2/2$ and proved $\dot L(x) = 0 \implies x_2 = 0, x_1 \in [0, 2\pi]$ (which is a segment of the $x$ -axis) Then if we are to apply this theorem, we can pick $U$ to be the entire real line, and $P$ be the line-segment $[0, 2\pi]$ . Since $\dot L = 0$ on $P$ and $P$ is a line-segment, therefore $P$ is closed and positively invariant.  How do we proceed to prove the tricky statement there are no ""entire solution in $P \backslash \{0\}$ that on which $L$ is constant? To do this it seems we need to prove that since any $\{x_2 = 0, x_1 \in (0, 2\pi]\}$ could be a (constant) solution in $P \backslash \{0\}$ on which $L$ is for sure constant, therefore we need to eliminate all $\{x_2 = 0, x_1 \in (0, 2\pi]\}$ as being the solution of this system. Is this right?","['lyapunov-functions', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4285413,Linear Diophantine equation with three variables and a condition,"Let me start by saying that I'm new to Diophantine equations and my method certainly will not be the best possible one. I'm aware that a faster method of solving this particular case exists, but I want to know if what I did was correct and how I can finish the exercise. $$39x + 55y + 70z = 3274$$ $$ x+y+z=69$$ $$x \ge 0, y \ge 0, z \ge 0$$ What I tried: $$39x+55y=3274 - 70z$$ The GCD for 39 and 55 is one, so we can set aside $z=t$ to be a parameter, where $t$ is an integer. We proceed to solve this as a Diophantine equation with two variables. I now need to use the extended Euclidean algorithm to express $1$ as a linear combination of $39$ and $55$ . I got that $1 = 24 \cdot 39 - 17 \cdot 55$ Now, the solution would be $$x=78576 - 1680t + 55s$$ $$y=-55658 + 1190t - 39s$$ $$z=t$$ where $t$ and $s$ are integers.
I got this from the formula that $x = \lambda_1 \cdot b + s \cdot a_2$ and $y = \lambda_2 \cdot b - s \cdot a_1$ Where $\lambda_1$ and $\lambda_2$ are the coefficients in the Euclidean algorithm, $b$ is $3274 - 70t$ and $a_1$ and $a_2$ are $39$ and $55$ , respectively. It's obvious that $t \ge 0$ , and if I put that $x$ and $y$ are greater than zero, I get that $$s \ge \frac{1680t-78576}{55}$$ and $$ s \le \frac{-55658+1190t}{39}$$ which makes $$ \frac{1680t-78576}{55} \le \frac{-55658+1190t}{39}$$ which is $$t \le 46.77$$ I now have the whole range of integers $[0, 46]$ which of course isn't feasible to do (as I'd have to plug them into the inequalities for $s$ and get even more cases. How do I proceed from here? What do I do? I still have the condition $x+y+z=69$ but I feel like I'm missing something. Can it be really done this way, except that it's an unimaginably hefty job of testing each case?","['elementary-number-theory', 'discrete-mathematics', 'diophantine-equations']"
4285470,Find $\lim_{x\to 0}\frac{\ln\sin^2(ax)}{\ln\sin^2(bx)}$ without using the L'Hopital's rule or Taylor's series,"This limit is proposed to be solved without using the L'Hopital's rule or Taylor series: $$
\lim_{x\to0}\frac{\ln\sin^2(ax)}{\ln\sin^2(bx)},
$$ where $a,b=const$ . I know how to calculate this limit using the L'Hopital's rule: $$
\lim_{x\to0}\frac{\ln\sin^2(ax)}{\ln\sin^2(bx)}=
\lim_{x\to0}\frac{\frac{2a\sin (ax)\cos (ax)}{\sin^2(ax)}}{\frac{2b\sin (bx)\cos (bx)}{\sin^2(bx)}}=
\lim_{x\to0}\frac{a}{b}\cdot\frac{\sin (ax)\cos (ax)}{\sin (bx)\cos (bx)}\cdot\frac{\sin^2(bx)}{\sin^2(ax)}
$$ (using the asymptotic equivalence $\sin x\sim x$ ) $$
=\lim_{x\to0}\frac{a}{b}\cdot\frac{ax}{bx}\cdot\frac{(bx)^2}{(ax)^2}=1,
$$ but I don't know to calculate this limit without derivatives.","['limits', 'calculus', 'limits-without-lhopital', 'logarithms']"
4285504,Prove $a^2+b^2+c^2=x^2+y^2+z^2$ given that $a^2+x^2=b^2+y^2=c^2+z^2=(a+b)^2+(x+y)^2=(b+c)^2+(y+z)^2=(c+a)^2+(z+x)^2$,"Prove $$a^2+b^2+c^2=x^2+y^2+z^2$$ given that $$a^2+x^2=b^2+y^2=c^2+z^2=¥¥(a+b)^2+(x+y)^2=(b+c)^2+(y+z)^2=(c+a)^2+(z+x)^2$$ where $a,b,c,x,y,z ¥in¥mathbb R$ A friend forwarded me this problem which recently appeared in a math contest at Costa Rica. I've been working on it but I can't find a definite proof, though I have discovered several facts: Let $u$ be equal to the value of the equation, then $ab+xy=¥frac{-u}{2}$ and $a^2+b^2+c^2+x^2+y^2+z^2=3u$ which implies that $(a+b+c)^2+(x+y+z)^2 = 0 ¥implies a+b+c=x+y+z=0$ . I also noticed that, because of the equalities, working with complex numbers or vectors would be really tempting, as if $r=a+ix$ , $s=b+iy$ , $t=c+iz$ , then we have $|r|=|s|=|t|=|r+s|=|s+t|=|r+t|$ , but I can't find a way to make it useful. Any ideas/hints/solutions?","['contest-math', 'algebra-precalculus', 'complex-numbers']"
4285546,"If limit exists, is that function continuous?","If I have a function $f$ , and the limit exists at $a$ , and $\lim\limits_{x \to a} f(x) = \infty $ or $\lim\limits_{x \to a} f(x) = k $ ,  where $k$ is a constant, does that mean that the function is continuous on some interval in the domain of $f$ ? I think this is true, but I am not sure. Because for example, if $\lim\limits_{x \to a} f(x) = \infty $ , if I graph a function that fulfills this, there is at least some interval in its domain where it is continuous.","['limits', 'continuity']"
4285595,Weighted projective plane as a quotient of $\Bbb CP^2$,"For positive integers $a_0,\dots,a_n$ , consider the weighted projective space $\Bbb C\Bbb P(a_0,a_1,\dots,a_n)$ , which is the quotient of $\Bbb C^{n+1}-\{0\}$ by the action of $\Bbb C^*=\Bbb C-\{0\}$ defined by $t\cdot (z_0,\dots,z_n)=(t^{a_0}z_0,\dots,t^{a_n}z_n)$ . According to https://en.wikipedia.org/wiki/Weighted_projective_space , the weighted projective space $\Bbb C\Bbb P(a_0,a_1,\dots,a_n)$ is isomorphic to the quotient of projective space $\Bbb {CP}^n$ by the group that is the product of the groups of roots of unity of orders $a_0,a_1,\dots,a_n$ acting diagonally. I am trying to verify this in the simple case $n=2$ . For $m\geq1$ let $\mu_m$ be the group of roots of unity of order $m$ , and for $a,b,c\geq 1$ let $G=\mu_a\times \mu_b\times \mu_c$ . There is a well-defined diagonal action of $G$ on $\Bbb {CP}^2$ . I have to show that $\Bbb {CP}^2/G=\Bbb {CP}(a,b,c)$ . Maybe one way to do this is to define a map $\Bbb {CP}^2\to \Bbb {CP}(a,b,c)$ and descend to the quotient. But the map $\Bbb {CP}^2\to \Bbb {CP}(a,b,c)$ , $[x,y,z]\to [x,y,z]$ is not  well-defined and I got stuck. Any hints?","['general-topology', 'projective-space', 'algebraic-topology', 'quotient-spaces']"
4285611,Let $H$ be a set of non-positive Horn clauses. Show that $H$ it is satisfying. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $H$ be a set of non-positive Horn clauses. Show that $H$ it is satisfying. Is this true if there are positive clauses in $H$ ? I'm finding hard to answer this questio, I have think about it for a long time, but can't find a good example/aproach Can somebody help me? I'm just starting to see Horn clauses","['logic', 'discrete-mathematics']"
4285632,"Why is $\mathcal R=\{B\cup(C|A):B\in \sigma_{\mathcal R}(A\cap \mathcal G),C\in \sigma_{\mathcal R}(\mathcal G)\}$ a $\sigma$-Ring?","One of the remarks in our textbook says it is very trivial to show that $\mathcal R=\{B\cup(C|A):B\in \sigma_{\mathcal R}(A\cap \mathcal G),C\in \sigma_{\mathcal R}(\mathcal  G)\}$ is a $\sigma$ -ring, where $\mathcal  G$ is a family of subsets of a set $\Omega$ and $A\subset \Omega$ . I know that I need to show that $(B_1\cup(C_1|A))|(B_2\cup(C_2|A))\in \mathcal R$ and $\cup_{n=1}^{\infty}B_n\cup(C_n|A)\in \mathcal R$ but I can't figure how. It there a way to show this?","['elementary-set-theory', 'measure-theory', 'real-analysis']"
4285644,general solution to fractional differential equation,"I'd like to know the existence of the general solution to the following fractional differential equation $$D_{0+}^{\alpha} y(t)=0  \text{,}\label{1} \tag{1}$$ where $\alpha \in (1,2)$ and $$  D_{0+}^{\alpha} y(t)=\frac{1}{\Gamma(2-\alpha)} \left( \frac{d}{dt} \right)^2 \int_0^t \frac{y(s)}{(t-s)^{\alpha-1}} \, ds  \text{.}  $$ I know that $t^{\alpha-1}$ and $t^{\alpha-2}$ are solutions to problem \eqref{1}. My question is whether $C_1t^{\alpha-1} + C_2 t^{\alpha-2}$ is the general solution to problem \eqref{1}. Note that $D_{0+}^{\alpha}$ has second order derivative. My attempt : Let $Y(t)=\int_0^t \frac{y(s)}{(t-s)^{\alpha-1}}$ for fixed $y$ . Then $y$ is a solution to problem \eqref{1} if and only if $Y$ is a solution to problem $Y''(t)=0$ . We also know that $D_1+D_2 t$ : a general solution to problem $Y''(t)=0$ . $y(t)=t^{\alpha-1} \implies Y(t)=Ct$ and $y(t)=t^{\alpha-2} \implies Y(t)=C$ for a constant $C=\int_0^1 (1-v)^{1-\alpha}dv. $ I think I can prove it using them, but I'm not sure. I would be grateful if you could give any comment for my question.","['fractional-calculus', 'ordinary-differential-equations']"
4285670,Should I ignore ± sign when integrating square roots?,"I was solving the following integral: $$
\int \:\frac{x^2}{\sqrt{x^2+4}}dx
$$ $$
u=\sqrt{x^2+4}
$$ $$
\:du=\frac{2x}{2\sqrt{x^2+4}}dx=\frac{x}{u}dx
$$ $$
\int \:\frac{x^2}{\sqrt{x^2+4}}dx=\int \:\frac{x^2}{u}dx=\int \:xdu
$$ Now I only need to find what x means in terms of u: $$
u^2=x^2+4,\:u^2-4=x^2
$$ $$
x=\pm \sqrt{u^2-4}
$$ But now I have a problem, which is the plus minus sign, so my integral would be: $$
\int \pm \sqrt{u^2-4}du
$$ To avoid this problem, I decided to use integration by parts instead: $$
\int xdu\:=\:xu-\int \:udx\:=
$$ $$
x\sqrt{x^2+4}-\int \:\sqrt{x^2+4}dx
$$ But it looks like both equations yielded the same result and the plus minus sign was unnecessary. $$\int \pm \sqrt{u^2-4}du$$ $$x\sqrt{x^2+4}-\int \:\sqrt{x^2+4}dx$$ $$u=2sect,\:t=arcsec\left(\frac{u}{2}\right),\:du=2sec\left(t\right)tan\left(t\right)dt$$ $$x=2tan\left(t\right),\:t=arctan\left(\frac{x}{2}\right),\:dx=2sec^2tdt$$ $$\int \:\sqrt{u^2-4}du=\int \:2tan\left(t\right)\cdot 2sec\left(t\right)tan\left(t\right)dt=$$ $$x\sqrt{x^2+4}-\int \:\sqrt{x^2+4}dx=\:x\sqrt{x^2+4}-\int \:2sec\left(t\right)\cdot 2sec^2tdt=$$ $$4\int \:sec\left(t\right)tan^2\left(t\right)dt=4\int \:\:sec\left(t\right)\left(sec^2\left(t\right)-1\right)dt=$$ $$\:x\sqrt{x^2+4}-4\int \:sec^3tdt$$ $$4\int \:\:sec^3tdt-4\int \:sec\left(t\right)dt$$ $$\int \:sec^3tdt=\frac{1}{2}\sec \:\left(t\right)\tan \:\left(t\right)+\frac{1}{2}\ln \:\left|\tan \:\left(t\right)+\sec \:\left(t\right)\right|+C$$ $$=4\left(\frac{1}{2}\sec \:\:\left(t\right)\tan \:\:\left(t\right)+\frac{1}{2}\ln \:\:\left|\tan \:\:\left(t\right)+\sec \:\:\left(t\right)\right|-ln\left|\tan \:\:\:\left(t\right)+\sec \:\:\:\left(t\right)\right|\right)$$ $$=x\sqrt{x^2+4}-4\left[\frac{1}{2}\sec \left(t\right)\tan \left(t\right)+\frac{1}{2}\ln \left|\tan \left(t\right)+\sec \left(t\right)\right|\right]$$ $$=2\sec \left(t\right)\tan \left(t\right)-2\ln \left|\tan \:\:\left(t\right)+\sec \:\:\left(t\right)\right|$$ $$=x\sqrt{x^2+4}-2\sec \left(t\right)\tan \left(t\right)-2\ln \left|\tan \left(t\right)+\sec \left(t\right)\right|$$ $$=2\sec \left(sec^{-1}\left(\frac{u}{2}\right)\right)\tan \left(sec^{-1}\left(\frac{u}{2}\right)\right)-2\ln \left|\tan \:\:\left(sec^{-1}\left(\frac{u}{2}\right)\right)+\sec \:\:\left(sec^{-1}\left(\frac{u}{2}\right)\right)\right|$$ $$=x\sqrt{x^2+4}-2\sec \left(tan^{-1}\left(\frac{x}{2}\right)\right)\tan \left(tan^{-1}\left(\frac{x}{2}\right)\right)-2\ln \left|\tan \left(tan^{-1}\left(\frac{x}{2}\right)\right)+\sec \left(tan^{-1}\left(\frac{x}{2}\right)\right)\right|$$ $$sec=\frac{h}{a}=\frac{u}{2},\:o=\sqrt{u^2-2^2},\:tan=\frac{o}{a}=\frac{\sqrt{u^2-4}}{2}$$ $$tan=\frac{o}{a}=\frac{x}{2},\:h=\sqrt{x^2+2^2},\:sec=\frac{h}{a}=\frac{\sqrt{x^2+4}}{2}$$ $$=2\left(\frac{u}{2}\right)\frac{\sqrt{u^2-4}}{2}-2\ln \left(\left|\frac{\sqrt{u^2-4}}{2}+\frac{u}{2}\right|\right)$$ $$=x\sqrt{x^2+4}-2\frac{\sqrt{x^2+4}}{2}\left(\frac{x}{2}\right)-2\ln \:\left|\frac{x}{2}+\frac{\sqrt{x^2+4}}{2}\right|$$ $$=\frac{\sqrt{x^{2}+4}\sqrt{\left(\sqrt{x^{2}+4}\right)^{2}-4}}{2}-2\ln\left(\left|\frac{\sqrt{\left(\sqrt{x^{2}+4}\right)^{2}-4}}{2}+\frac{\sqrt{x^{2}+4}}{2}\right|\right)$$ $$=\frac{2x\sqrt{x^2+4}}{2}-\frac{x\sqrt{x^2+4}}{2}-2\ln \:\left|\frac{x}{2}+\frac{\sqrt{x^2+4}}{2}\right|$$ $$=\frac{x\sqrt{x^{2}+4}}{2}-2\ln\left|\frac{x+\sqrt{x^{2}+4}}{2}\right|$$ $$=\frac{x\sqrt{x^2+4}}{2}-2\ln \:\left|\frac{x+\sqrt{x^2+4}}{2}\right|$$ So, since both of them yield the exact same answer after simplification, I wonder if we can always assume that square roots are positive and omit the plus minus sign, or was my logic actually right that I should always try to avoid substitutions with plus minus square roots? As you can see by the graph it seems to work for both positive and negative x . My only suspicion is that in cases where it is not possible to simplify the formations such as fractional angles. Then maybe we could be getting it wrong... for example when answer is like this... $$sin\left(\frac{1}{8}cos^{-1}x\right)$$","['integration', 'trigonometric-integrals']"
4285680,Is any countable set in a bounded domain of $\mathbb{C}$ an analytic set?,"Let $X$ be a complex manifold. A closed subset $Y$ of $X$ is called an analytic set if for each $y \in Y$ , there is an open neighborhood $U$ of $y$ so that $Y \cap U$ is the zero set of finitely many holomorphic functions $f_{1}, \cdots, f_{k} \in \mathscr{O}(U)$ . Let $\Omega$ be a bounded domain in the complex plane $\mathbb{C}$ , and $A$ a countable set of $\Omega$ admitting no accumulation point. Ques: Is $A$ necessarily an analytic set in $\Omega$ ? As well known,   the Weierstrass factorization theorem tell us that any countable
set of $\mathbb{C}$ tending to infinity is an analytic set.","['complex-analysis', 'complex-geometry', 'several-complex-variables']"
4285693,Can we use the Lambert W solution $y=-3W(K_2x^{-4/3})$ instead of $y =-3W(\frac 1 3\sqrt[3]{-\frac{K_1}{x^4}})$ if we choose an appropriate constant?,"During the process of solving the separable differential equation $4y - x(y-3)y' = 0$ , our solution acquires a constant when we go from $\frac 4 x = \frac{y-3}{y}y'$ to $\ln x + C_1 = y - 3 \ln y$ . We then do: $$ e^{-\frac{4}{3}(\ln x) -\frac{C_1}{3}} = e^{\ln y - \frac y 3}$$ $$ x^{-\frac 4 3}e^{-\frac{C_1}3}= ye^{-\frac y 3}$$ $$-\frac 1 3 x^{-\frac 4 3}e^{-\frac{C_1}3}= -\frac y 3 e^{-\frac y 3}$$ I substitute $- \frac 1 3 e^{\frac {C_1}3}$ by the constant $K_1$ to get $$ K_1x^{-\frac 4 3}=-\frac y 3 e^{-\frac y 3}$$ applying the Lambert W function I get: $$-3W(K_1x^{-\frac 4 3}) = y$$ Instead, wolframalpha suggests me the solution $y = -3W\left( \frac 1 3\sqrt[3]{-\frac{K_2}{x^4}}\right)$ . My question is: is the process through which I substitute in the constant $K_1$ legitimate? Since otherwise, I'm unable to determine why my answer doesn't correspond to the wolframalpha one. The reason why I thought I was allowed to substitute $K_1$ was because, in the step where we go from $\frac 4 x = \frac{y-3}{y}y'$ to $\ln x + C_1 = y - 3 \ln y$ , we obviously see that taking the derivative of any constant makes our equation true, and therefore we would be free to choose whichever $K_1$ makes our equation look nice. But is this reasoning correct, given that wolframalpha provides a more complicated solution?","['integration', 'lambert-w', 'ordinary-differential-equations']"
4285700,Brownian motion started at infinity,"It is known (Theorem 3.46 in Peres-Mörters) that the harmonic measure of a set $A$ from infinity is well-defined by taking the limit as $x\to \infty$ of a Brownian motion started at $x$ , or by averaging the starting point on a sphere. This works in arbitrary dimension. Can the path itself be defined, rather than just the hitting distribution? Concretely, is it the same as running Brownian motion on the sphere (by stereographic projection) from the north pole until it hits $A$ ? Does it work then in all dimensions or just in dimension 2?","['brownian-motion', 'probability', 'random-variables']"
4285724,Decomposition of matrix occuring in problem of finding $n+1$ vectors in $\mathbb{R}^n$ with pairwise equal inner product,"I was toying with my intuition that there are always $n+1$ unit vectors in $\mathbb{R}^n$ such that every pair $v_i$ and $v_j$ ( $i\neq j$ ) has the same angle between them. As those vectors are normalized this is equivalent to $$v_i\cdot v_j=\begin{cases} 1,& i=j \\m, & i\neq j \end{cases}$$ with $|m|<1$ . So if $V$ is the $n\times (n+1)$ matrix whose columns are formed by the $v_i$ we have $$M:=V^TV=\begin{pmatrix}1 & m & \cdots &m \\m&1&\cdots&m\\ \vdots &&\ddots&\vdots \\ m&m&\cdots&1 \end{pmatrix}$$ $M$ is a $(n+1)\times (n+1)$ matrix and must be singular (as it is the product of two rank $n$ matrices). The sum of its rows is $(nm+1,nm+1,\ldots,nm+1)$ . For $m=-\frac{1}{n}$ this is the null vector giving the required dot product $m$ as a function of $n$ . E.g., for $n=2$ , $m=-\frac{1}{2}$ corresponding to the expected angle of $120°=\arccos{-\frac{1}{2}}$ . Now I'm asking is there a matrix decomposition so that I can get $V$ back from the product $M$ ? (I know that $V$ is not unique since I can freely rotate the vectors $v_i$ about an arbitrarily chosen fixed axis without changing their pairwise dot product.) I tried spectral decomposition of $M$ but that gives $n+1$ dimensional vectors and the $n$ nonzero eigenvalues of $M$ are all equal to $\frac{n+1}{n}$ which means that any linear combination of eigenvectors corresponding to that eigenvalue is also an eigenvector making the dot product between those eigenvectors rather arbitrary.","['matrix-decomposition', 'linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
4285780,Find this somewhat unpleasant limit,"I am solving a problem and after quite some computations and almost 1 day spent on it, I decided to ask here. It all boils down to finding this limit $$\lim_{x \to 0+}C(x) = ?$$ where $$C(x) = \frac{x-\sqrt{x^2+1}\cdot \ln \big(\sqrt{x^2+1} + x \big)}{2x^2\sqrt{x^2+1}\cdot \ln\big(\sqrt{x^2+1} + x \big)}$$ I applied L'Hôpital's rule a few times to get to here. Now WA says this limit is $$-1/6$$ and this is correct. So I am trying to compute this and derive that $$\lim_{x \to 0+}C(x) = -1/6$$ by hand. And after studying the sub-expressions, I can see this limit is of the kind $0/0$ but if I try to apply L'Hôpital's rule again to the expression $C(x)$ , it doesn't get simpler, it gets more complicated. So there must be some trick here. Maybe I need to divide the numerator and denominator by some expression. I tried that too a few times but I don't succeed at making it simpler. Or... is this problem not solvable at all just by using L'Hôpital's rule? But I don't see what other theory to apply here. Any help or hint as to how to proceed? Side note: Here is the original problem which led me to this expression $C(x)$ . It asks us to find this limit. $$\lim_{x \to 0+} \left(\frac{\ln(x+\sqrt{1+x^2})}{x}\right)^\frac{1}{x^2} = ?$$","['limits', 'calculus', 'real-analysis']"
4285821,Is $x = 2^6$ a statement,"I'm reading Susanna Epp's book on discrete mathematics. The exercise 2.1.5 ask which sentences are statements, a statement being something that is either true or false but not both. Question: is $x=2^6$ a statement?
Can equations with variables be statements?",['discrete-mathematics']
4285954,Location-privacy-preserving protocol for finding relative direction?,"Sorry if this is a silly question, but: imagine there are two agents on a finite 2d plane. Each agent knows her location but not the other's, and they want to find the relative directions between them (i.e. agent A wants to find unit vector in direction of agent B and vice versa), without compromising their locations. The agents communicate directly (so can't use a 3rd party that will have both their locations and provide the directions). Is there a protocol that the agents can use? I feel like there's a very simple solution I'm missing. My first approach was to try to converge on a distant point to which they share direction, but couldn't think of a way to compare directions to the distance point without exposing their locations after two iterations (maybe something with locality-sensitive hashing ?) Another approach was using homomorphic encryption , but again I couldn't figure how to have both locations encrypted by the same key to perform the calculation. Thanks! Edits in response to comments: There aren't sensors capable of physically inferring the direction, the agents only have a digital communication channel (e.g. two mobile devices communicating over the internet). As for leaking information, some information about the location (the direction) has to be compromised as @obareey mentioned, and it's also fine if some information about the distance leaks, as long as there's a way to limit this leakage while still calculating an accurate direction. I'm more interested in the information-communication aspect than the physical aspects. So the situation is similar to having two computer programs wanting to find relative directions in some shared conceptual plane without exposing individual positions.","['cryptography', 'geometry']"
4285963,Proving $\cos^{-1}(-x)=\pi-\cos^{-1}x$ without geometry.,"Let $$
\cos^{-1}x=a
\implies x=\cos a
$$ and $$
\cos^{-1}(-x)=b
\implies -x=\cos b
$$ Hence we have $$
\cos a+\cos b=0
$$ Using $$
\cos(A+B)+\cos(A-B)=2\cos A\cos B
$$ with $$
A=\frac{a+b}{2} \\
\text{ and } \\
B=\frac{a-b}{2}
$$ we get $$
\cos a+\cos b=2\cos\left(\frac{a+b}{2}\right) \cos\left(\frac{a-b}{2}\right)=0
$$ with which $$
\cos\left(\frac{a+b}{2}\right)=0 \text{ or } \cos\left(\frac{a-b}{2}\right)=0\\
\implies a=\pi-b \text{ or } a=\pi+b
$$ Now, to choose between the two, I'm making the below argument: To find the inverse of a function, the function has to be one-to-one. Hence in our case, both $\cos a$ and $\cos b$ have to be one-to-one, which is possible only when $$
n\pi\leq a,b \leq (n+1)\pi \text{, }n\in\mathbb{Z}
$$ From the above, we get, $$
-n\pi \leq \pi-b \leq (-n+1)\pi
$$ and $$
(n+1)\pi \leq \pi+b \leq (n+2)\pi
$$ The ranges can be reconciled for $a$ and $\pi-b$ by taking $n=0$ and we get $$
0 \leq a \leq \pi \\
0 \leq \pi-b \leq \pi
$$ But no value of $n\in\mathbb{Z}$ can simultaneously reconcile the ranges of $a$ and $\pi+b$ . Therefore, we conclude that $$
\cos^{-1}(-x)=\pi-\cos^{-1}(x)
$$ From this I also learnt that the inverse function is meaningful when the angle is discussed in the range $[0,\pi]$ .
Is this line of argument mathematically fool-proof?","['trigonometry', 'inverse-function']"
4286055,Volume inside an ellipsoid and offset cylinder,"I want to find the volume of the region inside the ellipsoid $$\frac{x^2}{4}+\frac{y^2}{4}+z^2=1$$ and the cylinder $$x^2+(y-1)^2=1$$ I tried shifting the axes so that the cylinder was centered at the origin, then evaluating an integral in cylindrical/polar coordinates. $$\int^{2\pi}_{0}\int^{1}_{0}\int^{\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}_{-\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}r dzdrd\theta$$ $$\int^{2\pi}_{0}\int^{1}_{0}r\sqrt{3-r^2-2rsin(\theta)} drd\theta$$ However, this integral gets messy. Is there an easier method of finding the volume?","['integration', 'multivariable-calculus', 'cylindrical-coordinates', 'multiple-integral']"
4286092,How to find or approximate probability distribution from known values of the characteristic function?,"If I have a known discrete values for the characteristic function (I know $a_n$ values for specific values of $\omega_n = \frac{2\pi i n}{d}$ ): $$a_n = \phi_x\left(\omega = \frac{2\pi in}{d}\right),$$ How do I find the probability distribution for this characteristic function? I've seen some stuff online about possibly needing an inverse discrete-time Fourier transform, but I haven't had much luck unfortunately. (Edit: I believe its actually related to the discrete-frequency Fourier Transform (DFFT), but this is much less well-documented on the internet.    A paper discussing the topic is here: The discrete frequency Fourier transform .  The DFFT differs from the DTFT in that it's defined for discrete values of frequency, giving a continuous function of time, whereas the DTFT is defined for discrete values of time, giving a continuous function of frequency) Alternatively, I'm just trying to find the variance in this probability distribution, so if there's a way to get the variance directly from discrete values of the characteristic function, I'd be happy to hear that :).","['statistics', 'characteristic-functions', 'fourier-transform', 'discrete-time', 'probability']"
4286136,Find a general solution to $xy' = y^2+y$,"I'm trying to find the general solution to $xy' = y^2+y$ , although I'm unsure as to whether I'm approaching this correctly. What I have tried: dividing both sides by x and substituting $u = y/x$ I get: $$y' = u^2x^2+u$$ Then substituting $y' = u'x + u$ I get the following: $$u'x+u = u^2x^2+u \implies u' = u^2x \implies \int\frac{du}{u^2}=\int x dx$$ Proceeding on with simplification after integration: $$\frac{1}{u}=\frac{x^2}{2}+c\implies y = \frac{2x}{x^2+c}$$ However, the answer shows $y=\frac{x}{(c-x)}$",['ordinary-differential-equations']
4286360,"Number of solutions to $x_1 +x_2 +x_3 +x_4 = 1097$ with multiple ""at least"" restrictions","How many solutions to $x_1 +x_2 +x_3 +x_4 = 1097$ , in nonnegative integers $x_1$ , $x_2$ , $x_3$ , and $x_4$ which satisfy at least one of the inequalities $x_1 \geq 100$ , $x_2 \leq 499$ ? I understand how to calculate the number of solutions to the unrestricted problem: $1097 + 4 - 1 \choose 1097$ solutions $= \binom{1100}{1097} = \binom{1100}{3}$ I also know how to handle a single restriction being added. For example if we just take $x_1 \geq 100$ : We have $x_1 = x_1' + 100$ . Then our problem is equivalent to $x_1' + x_2 + x_3 + x_4 = 997$ . Then we have $\binom{4 + 997 -1}{997} = \binom{1000}{3}$ solutions Finally, I know if we had to meet both restrictions, we could do the unrestricted number of solutions - the number of solutions that violate each requirement. Where I'm running into trouble is determining how to calculate the number of solutions that meet at least one restriction. How can I find the number of solutions that violate both restrictions?","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4286449,"Is there a closed form for this sequence $a_n = m, \ \binom{m}{2}\le n < \binom{m+1}{2}$?","I was playing around with the sequence: $$
2,2, 3,3,3,4,4,4,4,\dots
$$ If I denote the first element of said sequence by $a_1$ I realized the sequence can be written as $$
a_n = m, \quad  \binom{m}{2}\le n < \binom{m+1}{2}
$$ for $m \ge 2$ . Although the above does work, I would like to find a closed for of a sequence that gives $a_n$ in terms of common functions like the floor function. Does anyone know how I could do this? I managed to re-arrange the condition $ \binom{m}{2}\le n < \binom{m+1}{2}$ into $$
0\le \frac{n}{m} + \frac{1-m}{2} < 1 
$$ with hopes of using something like the floor function on it, but I couldn't seem to make it work.","['calculus', 'closed-form', 'sequences-and-series']"
4286487,Solution of elliptic problem is a minimizer of certain functional,"Let $f$ be continuously differentiable convex function on $\mathbb{R}$ and $U$ be Lipschitz domain. Suppose $u \in C^2(U) \cap C(\overline{U})$ is a solution to the following problem \begin{cases}
 -\Delta u + f'(u)=0  \ \ \ \ \ \ \ \text{ in }  \ \  U, \\ u(x)= 0  \ \ \ \ \ \ \ \text{ in } \ \ \ \partial U,
\end{cases} Show that $u$ is a minimizer for the following functional $$ \int_{U} \frac{|\nabla u|^2}{2} + f(u) \ \ dx$$ over all functions that vanish at the boundary. That is, if $v = 0$ on $\partial U$ , then the following hold $$ \int_{U} \frac{|\nabla u|^2}{2}  + f(u) \ \  dx \leq \int_{U} \frac{|\nabla v|^2}{2} + f(v) \ \  dx$$ My attempt: Let $J(u) =\int_{U} \frac{|\nabla u|^2}{2}  + f(u) \ \  dx$ . Suppose $v$ solves the problem and define $w = v-u$ . Then I want to show $J(v) \geq J(u)$ \begin{align} J(v) = J(u + w) &=  \int_{U} \frac{|\nabla u|^2}{2}  + f(u) \ \  dx + \int_{U} \frac{|\nabla w|^2}{2}  + f(w) \ \  dx + \int_U \nabla u \cdot \nabla w \ \ dx\\
&= J(u) + J(w) + a(u,w)
\end{align} Now it is clear that $J(w) \geq 0$ . How to deal with the form $a(u,w)$ to conclude the desired inequality? How to relate that to the latter part of the question including the inequality over the defined space of functions?","['calculus-of-variations', 'convex-analysis', 'functional-analysis', 'partial-differential-equations']"
4286570,Seeking help to find the exact value of $ \int_{0}^{\frac{\pi}{2}} \frac{x}{\sin ^{2n} x+\cos ^{2n} x} d x $ using substitutions?,"Latest Edit The closed form for $$I_n=
\frac{\pi}{4} \int_{0}^{\frac{\pi}{2}} \frac{d x}{\sin ^{2n} x+\cos ^{2n} x}
$$ is $$
\boxed{I_{n}=\frac{\pi^2}{8 n} \sum_{k=0}^{n-1}\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) \csc \frac{(2 k+1) \pi}{2 n}}
$$ Proof: Letting $t\mapsto \tan x$ yields $$
\begin{aligned}
\int_{0}^{\infty} \frac{\left(1+t^{2}\right)^{n-1}}{t^{2 n}+1}dt &=\sum_{k=0}^{n-1}\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) \int_{0}^{\infty} \frac{t^{2 k}}{t^{2 n}+1} d t .
\end{aligned}
$$ By my post , $$\int_{0}^{\infty} \frac{x^{r}}{x^{m}+1} d x=\frac{\pi}{m} \csc \frac{(r+1) \pi}{m},$$ We can now get its closed form: $$
\boxed{I_{n}=\frac{\pi^2}{8 n} \sum_{k=0}^{n-1}\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) \csc \frac{(2 k+1) \pi}{2 n}}
$$ Original version As the integral is not so difficult for $n=1,2,3$ ,  I just show how to find the exact value of the integral when $n=4.$ $$
I:=\int_{0}^{\frac{\pi}{2}} \frac{x}{\sin ^{8} x+\cos ^{8} x} d x
,$$ I changed the integral, as usual, by letting $x\mapsto \frac{\pi}{2} -x$ , $$I=
\frac{\pi}{4} \int_{0}^{\frac{\pi}{2}} \frac{d x}{\sin ^{8} x+\cos ^{8} x}
$$ Then multiplying both numerator and denominator by $\sec^8x$ and letting $t=\tan x $ yields $$
I=\frac{\pi}{4} \int_{0}^{\infty} \frac{\left(1+t^{2}\right)^{3}}{t^{8}+1} d t
$$ I was then stuck by the powers and start to think how to reduce them. Thinking for couple of days, I found a way to solve it with partial fractions only . Now I am going to share it with you. Observing that $$
\int_{0}^{\infty} \frac{d t}{t^{8}+1}\stackrel{t\mapsto\frac{1}{t}}{=} 
\int_{0}^{\infty} \frac{t^{6}}{t^{8}+1} d t$$ and $$
\int_{0}^{\infty} \frac{t^2d t}{t^{8}+1}\stackrel{t\mapsto\frac{1}{t}}{=} 
\int_{0}^{\infty} \frac{t^{4}}{t^{8}+1} d t,$$ we can reduce the power of the numerator to 2 that $$
I=\frac{\pi}{2} \underbrace{\int_{0}^{\infty} \frac{1+3 t^{2}}{t^{8}+1} d t}_{J} 
$$ To handle the power 8 in the denominator, we resolve the integrand into partial fractions. $$
J=\frac{1}{2 \sqrt{2}} \left[\underbrace{\int_{0}^{\infty} \frac{t^{2}-(3-\sqrt{2})}{t^{4}+\sqrt{2} t^{2}+1} d t}_{K}-\underbrace{\int_{0}^{\infty} \frac{t^{2}-(3+\sqrt{2})}{t^{4}-\sqrt{2} t^{2}+1} d t}_{L} \right]
$$ To deal with $K$ and $L$ , we play a little trick. $$
\begin{aligned}
K &=\int_{0}^{\infty} \frac{1-\frac{3-\sqrt{2}}{t^{2}}}{t^{2}+\frac{1}{t^{2}}+\sqrt{2}} d t \\
&=\int_{0}^{\infty} \frac{\frac{\sqrt{2}-2}{2}\left(1+\frac{1}{t^{2}}\right)+\frac{4-\sqrt{2}}{2}\left(1-\frac{1}{t^{2}}\right)}{t^{2}+\frac{1}{t^{2}}+\sqrt{2}} d t \\
&=\frac{\sqrt{2}-2}{2} \int_{0}^{\infty} \frac{d\left(t-\frac{1}{t}\right)}{\left(t-\frac{1}{t}\right)^{2}+(2+\sqrt{2})}+\frac{4-\sqrt{2}}{2} \int_{0}^{\infty} \frac{d\left(t+\frac{1}{t}\right)}{\left(t+\frac{1}{t}\right)^{2}-(2 -\sqrt{2})}\\
&=\frac{\sqrt{2}-2}{2 \sqrt{\sqrt{2}+2}}\left[\tan ^{-1}\left(\frac{t-\frac{1}{t}}{\sqrt{\sqrt{2}+2}}\right)\right]_{0}^{\infty}+0 \\
&=\frac{(\sqrt{2}-2) \pi}{2 \sqrt{\sqrt{2}+2}}
\end{aligned}
$$ Similarly, $$
\begin{aligned}
L &=-\frac{2+\sqrt{2}}{2} \int_{0}^{\infty} \frac{d\left(t-\frac{1}{t}\right)}{\left(t-\frac{1}{t}\right)^{2}+(2-\sqrt{2})} =-\frac{(2+\sqrt{2}) \pi}{2 \sqrt{2-\sqrt{2}}}
\end{aligned}
$$ $$
\therefore J=\frac{1}{\sqrt{2}}\left[\frac{(\sqrt{2}-2) \pi}{2 \sqrt{\sqrt{2}+2}}+\frac{(2+\sqrt{2}) \pi}{2 \sqrt{2-\sqrt{2}}}\right] =\frac{\pi}{4} \sqrt{10-\sqrt{2}}$$ Hence we can conclude that $$\boxed{I=\frac{\pi^{2}}{8} \sqrt{10-\sqrt{2}}}$$ How about when $n\geq 5$ , $$ \int_{0}^{\frac{\pi}{2}} \frac{x}{\sin ^{2n} x+\cos ^{2n} x} d x ?$$ Would you please help me?","['integration', 'trigonometry']"
4286604,"""rotating"" a matrix instead of transposing it","This is a silly question asked just out of curiosity. The question has nothing to do with rotation matrices as far as I know, but I didn't know how else to refer to the following operation. In linear algebra, suppose I have an $m\times n$ matrix $$
A = \begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \dots & a_{mn}
\end{pmatrix}
$$ Then suppose I ""rotate"" the elements of this matrix by 90 degrees to get an $n\times m$ matrix $$
A^\circlearrowright = \begin{pmatrix}
a_{m1} & \dots & a_{11} \\
\vdots & \ddots & \vdots \\
a_{mn} & \dots & a_{1n}
\end{pmatrix}.
$$ This is similar to the transpose, in that the transpose $A^T$ reflects the matrix $A$ along its diagonal, while $A^\circlearrowright$ rotates it by 90 degrees instead. The question is just whether this ""rotation"" operation has any sensible meaning when $A$ is interpreted as a linear transformation. I am guessing the answer is no, but it seemed worth asking anyway, just in case it's something that's been considered for some reason. A related operation is what we could call the ""anti-transpose"", which reflects the matrix along the 'other' diagonal and also results in an $n\times m$ matrix: $$
A^\bot \begin{pmatrix}
a_{mn} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \dots & a_{11}
\end{pmatrix}
$$ An application of this anti-transpose operation is mentioned in this answer on mathoverflow. (Thanks to Torsten Schoeneberg for pointing that out.)","['matrices', 'linear-algebra']"
4286648,"If $\frac{x^2-yz}{a^2-bc}=\frac{y^2-zx}{b^2-ca}=\frac{z^2-xy}{c^2-ab}$, prove that $\frac{x}{a}=\frac{y}{b}=\frac{z}{c}$.","Problem : Let $a, b, c, x, y, z$ be real numbers
such that $abc \ne 0$ , $ ~ a + b + c \ne 0$ , $~ (a^2 - bc)(b^2 - ca)(c^2 - ab)\ne 0$ , $xyz\ne0$ , $x+y+z\ne0$ , $(x-y)^2+(y-z)^2+(z-x)^2\ne0$ and $$\frac{x^2-yz}{a^2-bc}=\frac{y^2-zx}{b^2-ca}=\frac{z^2-xy}{c^2-ab} \ne 0.$$ Prove that $$\frac{x}{a}=\frac{y}{b}=\frac{z}{c}.$$ The converse can be proved easily. My Attempt: $
\frac{x^2-yz}{a^2-bc}=\frac{y^2-zx}{b^2-ca}=\frac{z^2-xy}{c^2-ab}\\
\Rightarrow\frac{x^2-yz-y^2+zx}{a^2-bc-b^2+ca}=\frac{y^2-zx-z^2+xy}{b^2-ca-c^2+ab}=\frac{z^2-xy-x^2+yz}{c^2-ab-a^2+bc}\ [By\ Addendo]\\\Rightarrow\frac{x-y}{a-b}\cdot\frac{x+y+z}{a+b+c}=\frac{y-z}{b-c}\cdot\frac{x+y+z}{a+b+c}=\frac{z-x}{c-a}\cdot\frac{x+y+z}{a+b+c}\\\Rightarrow\frac{x-y}{a-b}=\frac{y-z}{b-c}=\frac{z-x}{c-a}\ \left[Considering\ (x+y+z)\ and\ (a+b+c)\ \neq0\right]
$ Known Exceptional Counter-examples:- Macavity : $a=1, b=2, c=3, x=y=z$ River Li : $a = 1, b=2, c = -3, x = 7, y = 7, z = -14$ The conditions in the question have been edited to exclude exceptional counter-examples.","['algebra-precalculus', 'problem-solving', 'ratio']"
4286679,When does $\frac{x+\sqrt{x^2-1}}{x-\sqrt{x^2-1}}+\frac{x-\sqrt{x^2-1}}{x+\sqrt{x^2-1}}=2mx+4$ have $2$ real solutions?,"$$\frac{x+\sqrt{x^2-1}}{x-\sqrt{x^2-1}}+\frac{x-\sqrt{x^2-1}}{x+\sqrt{x^2-1}}=2mx+4$$ My solution: I consider only real numbers, so $|x|\geq1$ . After adding the fractions I get the following quadratic equation: $$2x^2-mx-3=0,$$ and it has 2 real solutions if $D>0$ , so $m^2+24>0$ . It comes down to every real number, but the answer in the book states that there is no real m value for which the main equation has 2 real solutions, and no detailed explanation has been included. I kindly ask for explanation.","['real-numbers', 'algebra-precalculus', 'quadratics']"
4286696,Is the set of strictly positive polynomials an open set? In case it is a smooth manifold.,"Let $I=[-1, 1]$ , $\mathcal{P}^n$ be the set of polynomials of degree $n$ with domain $I$ , $\mathcal{P}^n_+(I, \mathbb{R})$ be the set of real-valued polynomials with domain $I$ that are strictly positive $$
\forall p\in\mathcal{P}^n_+(I, \mathbb{R}) \ \ p(t) >0 \forall t\in I
$$ Is this set open with respect to the topology induced by the $C^0$ norm, i.e. $$
\| p \|_{C^0} = \sup_{t\in I} | p(t) |
$$ Attempt of proof: In order to prove that $\mathcal{P}_+(I, \mathbb{R})$ is open with respect to the mentioned topology I will try to demonstrate that all its elements has a neighborhood contained in it. First we define a ball as $$
N(p, r) = \left\{q\in\mathcal{P}^n\text{ s.t. } \| p - q \|_{C^0}<r\right\}
$$ Any ball is a neighborhood. We desire to prove that $$
p\in \mathcal{P}_+^n(I, \mathbb{R})  \implies \exists r>0\text{ s.t. } N(p, r) \subset \mathcal{P}^n_+(I, \mathbb{R}) 
$$ Let $p\in\mathcal{P}_+^n$ and the ball $$
N'(p, r) = \left\{p+e\in\mathcal{P}^n\text{ s.t. } \| e \|_{C^0}<r\right\}
$$ we can always find a sufficiently small $r>0$ such that $$
\inf_{t\in I} \left\{ p(t) + e(t)\right\} >0
$$ It this proof right? If it is, then as $\mathcal{P}^n$ is a vector space, then $\mathcal{P}_+^n$ is a smooth manifold.","['general-topology', 'linear-algebra', 'functional-analysis', 'polynomials']"
4286798,Why is the probability of getting a pair in a five-card poker hand so complicated?,"The working-out of this question has really confused me. I know the basics of probability but I don't get the calculations here. I think 13C2 * 4C2 determines the number of possible pairs, and the stuff in the brackets is the number of combinations possible with the cards remaining after a pair is obtained—but, in them, what does does each number specifically do? . Thanks in advance.","['poker', 'combinatorics', 'card-games', 'probability']"
4286817,Average expected number of throws,"Rene throws the ball into the river, then retrieves it, turns it randomly and throws it again. With each throw, exactly half of the ball gets wet (the lower hemisphere). How many times, on average, does Rene need to throw a ball to get it completely wet (each point of its surface at least once was in the lower hemisphere)? I found an article by Kevin Brown that talks about a simple formula for the probability that $n$ random points on a sphere lie in the same hemisphere. In our case, this formula $p_{n}=\frac{(n-2)(n-3)}{2^{n}}$ valid for all $n\geq 2$ gives the probability that we will need $n$ throws to completely wet the ball. From this probability, the average number of throws needed to completely wet the sphere is $7$ . I suppose it will also be possible to run a Monte Carlo simulation to verify this. Am I on the right track?","['statistics', 'probability']"
4286833,Finding all functions $f:\mathbb R\to\mathbb R$ which satisfy $f(xy)=yf(x)+x+f\bigl(f(y)-f(x)\bigr)$,"-Closed: It has a solution in AOPS. - Find all functions $f:\mathbb R\to\mathbb R$ which satisfy $$f(xy)=yf(x)+x+f\bigl(f(y)-f(x)\bigr)$$ for all $x,y\in\mathbb R$ . My attempt: \begin{align}
&P(x, x): f\left(x^2\right)=xf(x)+x+f(0). \\
&x=1; \ f(0)=-1. \\
\\
&P(x, 0): -1=x+f\bigl(-1-f(x)\bigr). \implies f\bigl(-f(x)-1\bigr)=-x-1. \\
&x=-1; f\bigl(-f(-1)-1\bigr)=0. \implies \exists t \ \text{ s.t. } f(t)=0. \\
\\
&P(t, 0): -1=t+f(-1). \\
&P(0, t): -1=-t+f(-1). \\
&\therefore 2t=f(1)-f(-1).
\ \\
&P(-1, 1): f(-1)=-f(1)+1+f(-2t). \\
&f(-1)+f(1)-2+f(2t)-f(-2t)=0.
\end{align}","['functional-equations', 'functions']"
4286870,2-generated finite non-Abelian simple groups and the existence of Hamiltonian cycles in their Cayley graph,"Given that $G = \langle a, b\rangle$ and that $a$ is an involution, when is it the case that there exists $c, d$ such that $G = 
\langle c, d\rangle$ and $cd$ is an involution? At present, I am reviewing the following paper by I. Pak and R. Radoičić (2009), to include their main result in my undergraduate dissertation (which is a survey on the Hamiltonicity of Cayley graphs and digraphs). This result is stated as follows: Theorem 1 Every finite group $G$ of size $|G| \geq 3$ has a generating set $S$ of size $|S| \leq \log_2 |G|$ , such that the corresponding Cayley graph $\Gamma(G, S)$ contains a Hamiltonian cycle. The proof relies on a further theorem given in the paper, which I shall also state. Firstly, note that $r(G)$ and $m(G)$ denote that number of Abelian and non-Abelian composition factors of $G$ . Theorem 2 Let $G$ be a finite group, and let $r(G)$ and $m(G)$ be as above. Then there exists a generating set $S$ , $\langle S \rangle = G$ , with $|S| \leq r(G) + 2m(G)$ , such that the corresponding Cayley graph $\Gamma(G, S)$ contains a Hamiltonian cycle. While I do understand the way with which Theorem 1 follows from Theorem 2 , the proof for Theorem 2 is not clear to me. Firstly, it is worth noting that the proof relies on consequences of the Classification of Finite Simple Groups; namely that "" every non-Abelian finite simple group can be generated by two elements, one of which is an involution "". No reference is given in the paper to a proof of the aforementioned remark. I have however found the following paper by C. S. H. King (2016), in which the author proves that, indeed, every non-Abelian finite simple group is generated by an involution and an element of prime order. Of course, given the survey nature of the dissertation (and that it is at undergraduate level), this result will be simply stated. However, the aim is to clarify for non-expert readers the proofs given by Pak and Radoičić in [ 1 ], and I think stating this reference [ 2 ] will help do so. The second result that comes into play in the proof of Theorem 2 is one given by R. A. Rankin (1966) , which I shall state in full below ('Lemma 3' refers to the numbering in [ 1 ] and not in [ 3 ]). Lemma 3 Let $G$ be a finite group, generated by two elements $\alpha$ and $\beta$ , such that $(\alpha\beta)^2 = 1$ . Then the Cayley graph $\Gamma = \Gamma(G, \{\alpha, \beta\})$ contains a Hamiltonian cycle. In other words, note that the product of the generators above is an involution. In the proof for Theorem 2 , the authors invoke the use of Lemma 3 as follows: It is a well known consequence from the classification of finite simple groups, that every non-Abelian finite simple group can be generated by two elements, one of which is an involution. Therefore Lemma 3 is applicable, and for every non-Abelian finite simple group produces a generating set $S$ , with $|S| = 2$ , such that the corresponding Cayley graph contains a Hamiltonian cycle. Finally, I can state my question in full: Lemma 3 requires that the product of the generators to be an involution, however we are only given that one of the generators is an involution; is the product necessarily an involution then? How exactly is Lemma 3 invoked? Many thanks for your time and for reading this far! :-)","['graph-theory', 'finitely-generated', 'simple-groups', 'group-theory', 'cayley-graphs']"
4286925,Angle eigenvector makes with $x$-axis,"Say I have a symmetric matrix that lives in $xy$ space. I will write it as $$A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}.$$ I am interested in the covariance matrix, so let me also say $a,c>0$ , and $A$ is positive-definite, which means $|b|\leq\sqrt{ac}$ . The eigenvalues of $A$ are therefore $\geq 0$ , and the eigenvectors are orthogonal. The eigenvalue problem $A\mathbf{x}=\lambda\mathbf{x}$ reads $$\begin{pmatrix} a & b \\ b & c \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} = \lambda \begin{pmatrix} x \\ y \end{pmatrix}.$$ The system of equations can be written as \begin{cases}
y = x(\lambda-a)/b \\ 
x = y(\lambda-c)/b,
\end{cases} and combined to yield $$ \frac{c-a}{b}=\frac{y}{x}-\frac{x}{y}.$$ In the figure above, the red and blue vectors are the eigenvectors with the larger and smaller eigenvalue respectively. Working with the largest one, I can write the equation above the figure as \begin{align}
    \frac{c-a}{b}  &= \tan{\theta}-\tan{\beta} \\
                                &= (1+\tan{\theta}\tan{\beta})\tan(\theta-\beta) \\
                                &= 2\tan(2\theta-\pi/2) \\
                                &=-2\cot(2\theta),
\end{align} where to write the second equality I have used the trigonometric identity for the tangent of a difference. The third line follows from writing $\tan \theta \tan \beta = 1$ and $\beta = \pi/2 - \theta$ . Lastly, I used the trigonometric relations between complementary angles to write the last equality, which can be recasted as \begin{equation}
    \tan(2\theta) = \frac{2b}{a -c}.
\end{equation} The following equation $$\theta = \frac{1}{2}\textrm{atan2}(2b,a-c),$$ where $\textrm{atan2}(y,x)=\textrm{Arg}(x+iy)$ , can be used to compute the angle the eigenvector with largest eigenvalue, $\mathbf{x}$ , makes with the $x$ -direction. Choosing the principal branch, the range of $\theta$ is $(-90,90]^\circ$ , i.e. the right half of the plane. PROBLEM As you may have noticed, I absorbed the eigenvalue $\lambda$ in the equation right before the figure. This means the rest of the derivation should hold for both eigenvectors of $A$ . However, I have tested the calculation with multiple cases and magically the angle which $\theta$ measures is always that between the $x$ -direction and the eigenvector with largest eigenvalue. As an example, we can look at the special case $$A = \begin{pmatrix} a & 0 \\ 0 & c \end{pmatrix},$$ where the eigenvectors are aligned with the coordinate axes, and $a$ and $c$ are the eigenvalues. If $a>c$ , we get $\theta=0^\circ$ . However, if $a < c$ , we get $\theta = 90^\circ$ . Can anybody provide me with an explanation of why the ambiguity in the eigenvector that $\theta$ is describing seems to fade away somehow? I'd really appreciate it! NEW OBSERVATION I have realised something interesting. If I multiply $$\tan(2\theta) = \frac{2b}{a -c}$$ by $-1$ twice, getting $$ \tan(2\theta) = \frac{-2b}{c-a},$$ and I proceed as before and write $$\theta = \frac{1}{2}\textrm{atan2}(-2b,c-a),$$ now the angle is calculated between the $x$ -axis and the eigenvector with the smaller eigenvalue! In the wikipedia article on $\textrm{atan2}$ , specifically the section called "" East-counterclockwise, north-clockwise and south-clockwise conventions, etc. "", it says: Apparently, changing the sign of the x- and/or y-arguments and swapping their positions can create 8 possible variations of the $\mathrm{atan2}$ function and they, interestingly, correspond to 8 possible definitions of the angle, namely, clockwise or counterclockwise starting from each of the 4 cardinal directions, north, east, south and west. I think this brings me closer to the answer to my question but I need some help putting everything together. VISUAL AID Let me call the eigenvectors with larger and smaller eigenvalue $\mathbf{L}$ and $\mathbf{S}$ respectively. Let's look at how a few eigenvectors might look like in the $(a-c,b)$ plane. Consider the sketch that follows. The eigenvectors $\mathbf{L}$ and $\mathbf{S}$ are drawn in red and blue respectively, and the angle they make with the $x$ axis is called $\theta_l$ and $\theta_s$ respectively in the top subplot. As you can see, traversing the $(a-c,b)$ plane clockwise leads to the direction of the eigenvectors in the $xy$ plane rotating clockwise too. The arrow heads indicate the side of the vectors which falls in the range $(-90,90]^\circ$ . I have used Mathematica to produce the following surface plots of the angles in the $(a-c,b)$ plane. This is how the angle between the $x$ axis and $\mathbf{L}$ , given by $$\theta_l=\frac{1}{2}\textrm{atan2}(2b,a-c),$$ looks like: And this is how the angle the $x$ axis makes with $\mathbf{S}$ , $$\theta_s=\frac{1}{2}\textrm{atan2}(-2b,c-a),$$ looks like: If you check the surface values you can see they match what my sketch described. Let's come back to what was described in the wikipedia link. We can see $\textrm{atan2}(y,x)$ follows the "" East-anticlockwise "" convention. We have $\theta_l=0^\circ$ when $\mathbf{L}$ is pointing East, and the angle grows as $\mathbf{L}$ rotates anticlockwise. Now, if we try to make sense of the $\theta_s$ values as if they described $\mathbf{L}$ too, we can see $\theta_s=0^\circ$ when $\mathbf{L}$ is pointing North, and grows as $\mathbf{L}$ rotates anticlockwise. Hence, the $\textrm{atan2}(-y,-x)$ convention might be "" North-anticlockwise "". But again, I don't know what is special about $\mathbf{L}$ . From the derivation of the angles, either equation could have corresponded to either eigenvector. There is still a missing piece of the puzzle which I believe must lie in the derivation of the equation for $\theta_l$ . Can anybody give me a hand? Any insights would be greatly appreciated. SUMMARY The equation $$\tan(2\theta) = \frac{2b}{a -c}$$ holds for the azimuths of both eigenvectors, L and S , of the matrix $$A = \begin{pmatrix} a & b \\ b & c \end{pmatrix}.$$ Let me define 2 new vectors: $\mathbf{p}=(a-c,2b)$ , with azimuth $\gamma_p=\textrm{atan2}(p_y,p_x)$ . $\mathbf{n}=-\mathbf{p}$ , with azimuth $\gamma_n=\textrm{atan2}(-p_y,-p_x)$ . We can write $$\tan(2\theta) = \tan(\gamma_p) = \tan(\gamma_n).$$ It turns out using the first equality we get $$2\theta_l=\gamma_p=\textrm{atan2}(2b,a-c),$$ where $\theta_l$ is the angle between the $x$ axis and the eigenvector with larger eigenvalue, $\mathbf{L}$ . The second equality $$2\theta_s = \gamma_n = \textrm{atan2}(-2b,c-a)$$ gives us $\theta_s$ , the angle between the $x$ axis and the eigenvector with smaller eigenvalue, $\mathbf{S}$ . Now the question remaining is why does that happen? How could I have predicted that $\mathbf{p}$ and $\mathbf{n}$ would always have an azimuth that is twice that of $\mathbf{L}$ and $\mathbf{S}$ respectively? DIAGONALISATION I have found this derivation from Howard E. Haber from his Physics 116A class of Winter 2011. He obtains the same equation for $\tan(2\theta)$ by diagonalising the matrix $A$ (note the difference in notation: he uses $b$ in $A_{22}$ , and $c$ in the off-diagonal terms). He then proceeds by setting constraints in the angle $\theta$ . When he plugs his eq. 1 in his eq. 8 he makes it explicit that $\theta$ is measuring the angle between the positive $x$ axis and the eigenvector with largest eigenvalue. The conclusions drawn are the same as mine, but I somehow bypassed all that when I decided to use the function atan2 (unjustifiably, but it works). The question remains: why does my approach of using atan2 work?","['angle', 'linear-algebra', 'eigenvalues-eigenvectors']"
4286936,Distribution of a conditional expectation,"I am reading a book on financial mathematics and a Theorem gives a price formula for a Call Option: $$
\begin{aligned}
\pi_{\text {call }}(t)
&=P(t, S) q(t, S, \mathcal{I})-K P(t, T) q(t, T, \mathcal{I}) \\
\end{aligned}
$$ where $\mathcal{I}=(A(S-T)+\log K, \infty)$ , and $q(t, S, d y)$ and $q(t, T, d y)$ denote the $\mathcal{F}_{t}$ conditional distributions of the real-valued random variable $Y=-B(S-T)^{\top} X(T)$ under the $S$ - and $T$ -forward measure, respectively. In the last step of the proof, he comes up with the formula $$
\pi(t)=P(t, S) \mathbb{Q}^{S}\left[E \mid \mathcal{F}_{t}\right]-K P(t, T) \mathbb{Q}^{T}\left[E \mid \mathcal{F}_{t}\right]
$$ for the exercise event $E=\left\{-B(S-T)^{\top} X(T)>A(S-T)+\log K\right\}$ . I am confused, because the first formula looks like a real value (the measure $q(t,T,\cdot)$ evaluated on the set $\mathcal{I}$ ) and the second formula looks like a random variable to me ( $\mathbb{Q}^{S}\left[E \mid \mathcal{F}_{t}\right]$ is a random variable). How do $q(t, T, \mathcal{I})$ and $\mathbb{Q}^{T}\left[E \mid \mathcal{F}_{t}\right]$ coincide? What exactly is a $\mathcal{F}_{t}$ conditional distribution of the real-valued random variable $Y=-B(S-T)^{\top} X(T)$ under $T$ -forward measure? We can rewrite $E$ $$E=\{Y> A(S-T)+\log K\}=\{Y\in (A(S-T)+\log K,\infty)\}=\{Y\in\mathcal{I}\}$$ and the conditional distribution $$\mathbb{Q}^{T}\left[E \mid \mathcal{F}_{t}\right]=\mathbb{Q}^{T}\left[Y\in\mathcal{I} \mid \mathcal{F}_{t}\right]=\mathbb{Q}^{T}\left[\cdot \mid \mathcal{F}_{t}\right](Y^{-1}(\mathcal{I}))$$ I guess $\mathbb{Q}^{T}\left[\cdot \mid \mathcal{F}_{t}\right]$ can be a probability measure, let's say $Q_1$ , in some circumstances? And then we would have the distribution of $Y$ under the measure $Q_1$ ?","['conditional-probability', 'probability-distributions', 'finance', 'conditional-expectation', 'probability-theory']"
4286948,"$ x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}$ Find $ c$ such that $ x_0$ in $ (0, c)$, $ (x_n)$ has a finite limit $ \lim x_n$ when $ n\to + \infty$","Given a real number $ c > 0$ , a sequence $ (x_n)$ of real numbers is defined by $ x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}$ for $ n \ge 0$ . Find all values of $ c$ such that for each initial value $ x_0$ in $ (0, c)$ , the sequence $ (x_n)$ is defined for all $ n$ and has a finite limit $ \lim x_n$ when $ n\to + \infty$ . For $f(1)$ to be defined, then $c - \sqrt {c + x_0} \geq 0 \Rightarrow c^2-c \geq x_0$ But $x_0 \in (0;c) \Rightarrow x_0<c$ Thus $c^2-c>c \Rightarrow c(c-2)>0$ . If $c<0 \Rightarrow c - \sqrt {c + x_n} < 0$ , which is a contradiction. Therefore $c \geq 2$ . I think we will induct with $x_{n+1}$ to have $ (x_n)$ is defined for all $ n$ by finding the range of $x_n$ . $\textbf{Definition.}$ The function $f : D \rightarrow D$ is called Lipschitz function on $D$ if there is exist $q \in \mathbb{R}, 0 < q < 1$ such that $|f(x) − f(y)| ≤ q|x − y| \Leftrightarrow |f'(x)| \leq q$ for all $x$ and $y$ in $D$ . $\textbf{Theorem.}$ If $f(x)$ is Lipschitz function on $D$ , then the sequence $\{x_n\}$ defined by $x_0 = a \in D, x_{n+1} = f(x_n)$ converges (has a finite limit $ \lim x_n$ when $ n\to + \infty$ ). Then we can prove $|{f'(x)}| \leq q < 1$ for all $x \in (0;?)$ to have $(x_n)$ has a finite limit $ \lim x_n$ when $ n\to + \infty$ .","['contest-math', 'limits', 'calculus', 'sequences-and-series']"
4286953,Would you recommend me a text book reference?,"I am searching for a text book reference or lecture notes, where I can find the following theorem:
I mean as similar as possible to the statement with these same words (if possible). Or including these 3 equivalences: THEOREM: Let $M\subset H$ , orthonormal, are equivalents a) $M$ is total basis (or orthonormal basis or Hilbert basis); b) For all $u \in H\quad  \sum_{i\in Ju}$ , $(u, u_i)u_i=u$ where $Ju=\{i\in J: u_i\in M \; \text{and} \; (u_i, u)\neq 0\}$ c) $\sum_{i\in J_u} |(u, u_i)|^2=|u|^2$ , for all $u \in H$ (Parseval identity) Remark: I wish this because I'm studying using Brezis' book, however there is appearing in different theorems, and it is not including the $\iff$ (equivalence).","['hilbert-spaces', 'functional-analysis', 'reference-request']"
4286995,An approximation by Stirling's formula,"Let $0<\alpha <1$ be a real number and $\alpha n$ be an integer. I want to prove the following fact $${n\choose \alpha n}=\frac{1+o(1)}{\sqrt{2\pi \alpha (1-\alpha)n}}2^{nH(\alpha)},$$ where $H(\alpha)=-\alpha \log_2 \alpha -(1-\alpha)\log_2 (1-\alpha)$ . To do this, I've used the Stirling's formula for the factorial: $$n!=(\frac{n}{e})^n \sqrt{2\pi n}e^{r(n)},$$ where $1/(12n+1)<r(n)<1/12n$ . What I've tried: $${n\choose \alpha n}=\frac{n!}{(\alpha n)! ((1-\alpha)n)!}=\frac{(\frac{n}{e})^n \sqrt{2\pi n}e^{r(n)}}{(\frac{\alpha n}{e})^{\alpha n} \sqrt{2\pi \alpha n}e^{r(\alpha n)}(\frac{(1-\alpha)n}{e})^{(1-\alpha)n} \sqrt{2\pi (1-\alpha)n}e^{r((1-\alpha)n)}}\\ \frac{e^{r(n)-r(\alpha n)-r((1-\alpha)n)}}{(\alpha^{\alpha}(1-\alpha)^{(1-\alpha)})^n\sqrt{2\pi \alpha(1-\alpha)n}}=\frac{e^{r(n)-r(\alpha n)-r((1-\alpha)n)}}{\sqrt{2\pi \alpha (1-\alpha)n}}2^{nH(\alpha)}.$$ Now how can I show that $e^{r(n)-r(\alpha n)-r((1-\alpha)n)}=1+o(1)$ ? Thanks in advance.","['factorial', 'combinatorics', 'discrete-mathematics', 'asymptotics']"
4287003,Can the conditional distribution $P(\cdot |\mathcal{G})$ be a probability measure?,"Consider a probability space $(\Omega,\mathcal{F},P)$ and some sub- $\sigma$ -algebra $\mathcal{G}\subseteq\mathcal{F}$ . Can the conditional distribution $$P(A|\mathcal{G})=E_P[1_{A}|\mathcal{G}],\quad\text{for }A\in\mathcal{F}$$ be an actual probability measure on $\mathcal{G}$ and be used as such? For example for a random variable $X$ , we would have $$P(X\in A|\mathcal{G})=P(\cdot|\mathcal{G})(X^{-1}(A))\quad$$ and so forth? Or let me ask in another way: Should I understand a conditional distribution as an hypothetical distribution?","['measure-theory', 'probability-theory']"
4287056,"Norm of the operator $A\colon L^2[0,1] \to L^2[0,1]$, $x(t) \mapsto (t-0.5)\cdot x(t)$","I'm trying to calculate a norm of the operator $$A\colon L^2[0,1] \to L^2[0,1],\qquad  x(t) \mapsto (t-0.5)\cdot x(t).$$ I started finding it as follows: $$\|Ax(t)\|^2 = \int_{0}^1 |(t-0.5)\cdot x(t)|^2\,dt$$ Then I've tried to apply Cauchy-Schwarz inequality, but I can't get $\int_{0}^1 |x(t)|^2\,dt$ (that would be equal to $\|x\|^2$ ) in my expression, so I can't proceed further. P.S. In addition to norm itself, I have to specify some value or sequence on which the norm is reached Please, tell me about the right way to find $\|A\|$ here","['operator-theory', 'normed-spaces', 'functional-analysis']"
4287072,Additive but not $\sigma$-additive measure $\mathbb{Q} \cap I$,"I read in an old book the following example of a measure: For the set $M=\mathbb{Q} \cap[0,1]$ denote with $S$ the set system of subsets of $M$ of the form $\mathbb{Q} \cap I$ , where $I$ is any interval in $[0,1]$ . Let us define the function $\mu: S \rightarrow \mathbb{R}$ as follows: for any set $A \in S$ of the form $A=\mathbb{Q} \cap I$ we set $\mu(A)=\ell(I)=b-a .$ Then it said without proof that $\mu$ is finitely additive, but not $\sigma$ -additive. As I did not get why I tried to prove it by myself and I tried to show that $S$ is a semi-ring, I guess that is important before I start with the other proof. We have $\emptyset \in \mathbb{Q}$ and furthermore $(\mathbb{Q} \cap I_1)\cap (\mathbb{Q} \cap I_2)=\mathbb{Q} \cap I_1 \cap I_2$ and the union of two closed intervals is either an interval or the disjoint union of two intervals.
Then $(\mathbb{Q} \cap I_1)\setminus (\mathbb{Q} \cap I_2)$ is also an interval or the disjoint union of two intervals. Now the proof. I do not quite understand how it cannot be $\sigma$ -additive. Does it have something in common with Cantor sets? I don't know how to start the proof here. Any help or explanation (maybe an idea for the beginning of a proof) is appreciated. If there is a proof...","['measure-theory', 'proof-writing']"
4287080,Multivariable Chain Rule for Implicit Multivariable Functions?,"I'd like to compute $\frac{\partial x}{\partial z}$ along $S$ at $(x,y,z)$ for $S: \frac{1}{x}+\arctan(y+2z)=1$ . My Approach: I can define $w(x,y,z)=\frac{1}{x}+\arctan(y+2z)$ and find the total differential and so on, i.e., $dw=w_x dx+w_y dy+w_z dz$ (we'd also need to use the fact that $y$ is held constant and $dw=0$ ). How can I use the multivariable chain rule here? I'd like to find $\frac{\partial x}{\partial z}$ using the chain rule, but I'm a little bummed out here because I am only used to using the chain rule for solving equations where, say, $y$ depends on $a,b$ and $a, b$ depend on $t$ (e.g., $\frac{dy}{dt}=\frac{\partial y}{\partial a}\frac{da}{dt}+\frac{\partial y}{\partial b}\frac{db}{dt}$ ).","['differential', 'multivariable-calculus', 'linear-algebra', 'chain-rule']"
4287082,Integral $\int_0^\infty \frac{\sin{t}}{e^t-1}dt$?,"Is it possible to compute $I=\int_0^\infty \frac{\sin{t}}{e^t-1}dt$ ? I encountered this problem while calculating sum $\sum_{n=1}^\infty \frac{1}{n^2+1}$ . This integral converges $I=\int_0^\varepsilon+\int_\varepsilon^\infty$ , first integral is finite since $\sin{t}/(e^t-1)\tilde{} 1$ and the second is finite since $\sin{t}/(e^t-1)<e^{-t}$ . Any help is welcome.
Thanks in advance.","['integration', 'definite-integrals', 'analysis']"
4287105,Show that the following group is not left orderable,"I was trying to solve an exercise from a book but i got stuck in my calculation. I'm trying to show that foundamental groups of a class of orientable 3-manifold is not left orderable where by this i mean that they admits a strict ordere invariant for left multiplication. The groups are Seifert manifold built over a surface where the surface is $\mathbb{R}P^{2}.$ They admit the following presentation: $$\pi_{1}(M)=\langle\gamma_{1},...,\gamma_{n},y,h\;|$$ $$yhy^{-1}=h^{-1},\gamma_{j}^{\alpha_{j}}=h^{-\beta_{j}},\gamma_{j}h\gamma_{j}^{-1}=h,y^{2}\gamma_{1}\cdots\gamma_{n}=1\rangle.$$ With some calculation I got to show that if $k>0,h>1$ the following holds: $h^{-k}<y^2<h^k$ and $h^{-k}<y^{-2}<h^{k}.$ In the book I was given the hint to show that $$yhy^{-1}>1\tag{$*$}$$ and this in facts concludes as it is a contradiction with the assumption that $h>1$ given the first relation but I could not find a way to prove the key fact $(*)$ . Any suggestion in what kind of relations I should look for? Thanks in advance.","['group-presentation', 'group-theory', 'abstract-algebra', 'manifolds', 'general-topology']"
4287109,Norm of the Riemannian curvature tensor,"Let $c: [a, b] \to M$ be a geodesic on a Riemannian manifold $(M, \left< \cdot, \cdot \right>)$ . Define $\mathcal{V}_c := \Gamma(c^* TM)$ , the space of vector fields along $c$ , and $\hat{\mathcal{V}}_c$ the vector fields $V \in \hat{\mathcal{V}}_c$ satisfying $V(a) = V(b) = 0$ . Consider the norm on $\hat{\mathcal{V}}_c$ given by $||X||^2 = \int_a^b \left[\left<D_t X, D_t X\right> + \left<X, X\right> \right]dt$ , and let $\hat{H}_c^1$ be the completion of $\hat{\mathcal{V}}_c$ w.r.t. $||\cdot||$ . This space is easily identified with the Sobolev space $H^1_0(I, \mathbb{R}^n)$ . We define the index form $I$ as a symmetric bilinear form on $\hat{H}_c^1$ given by $I(X, Y) = \int_a^b \left[\left<D_t X, D_t Y\right> - \left<R(\dot{c}, X)Y, \dot{c}\right>\right]dt$ . Define the index of $c$ , Ind $(c)$ , as the dimension of the largest subspace of $\hat{H}_c^1$ on which $I$ is negative definite. Lemma 4.3.2 of Riemannian Geometry and Global Analysis by Jost states that Ind $(c)$ is finite. To prove this statement, Jost supposes that it does not hold, so that we may assume the existence of a sequence $(X_n)\subset \hat{H}_c^1$ such that $I(X_n, X_n) \le 0$ and such that $(X_n)$ is orthonormal w.r.t. the $L^2_c$ product. From $I(X_n, X_n) \le 0$ , we immediately obtain $\int_a^b \left< D_t X_n, D_t X_n \right>dt \le \int_a^b \left<R(\dot{c}, X_n)X_n, \dot{c}\right>dt$ . Jost then concludes that $\int_a^b \left<R(\dot{c}, X_n)X_n, \dot{c}\right>dt \le E(c)\sup|R|$ , where $E$ is the energy of $c$ and $R$ is the curvature tensor on $M$ . Why does this hold? What does $\sup |R|$ actually mean here? Is it related to the extension of the Frobenius norm discussed here ? My first thought was to consider the vector field $Z(t) = \frac{\dot{c}(t)}{\sqrt{E(c)}}$ , so that $Z$ is smooth and $||Z||_{L_c^2} = 1$ . We then obtain $\int_a^b \left<R(\dot{c}, X)Y, \dot{c}\right>dt = E(c) \int_a^b \left<R(Z, X_n)X_n, Z\right>dt$ .","['curvature', 'normed-spaces', 'riemannian-geometry', 'differential-geometry']"
4287120,Is $f\in L^2$ the orthogonal sum of its conditional expectations?,"Let ( $\Omega, \mathscr{F}, P$ ) be a complete probability space and suppose $$\mathscr{F}_1, \mathscr{F}_2,..., \mathscr{F}_k\subset \mathscr{F}$$ are independent, complete sigma algebras which, together, generate $\mathscr F$ up to completion.  Let $f\in L^2(\Omega,\mathscr{F}, \mathbb{R})$ have zero mean. Is it true that $$f=\sum_{i=1}^k E[f ~|~\mathscr{F}_i]$$ and how to prove? I'm not sure if it matters but I'm particularly interested in the case where the $\mathscr F_i$ are the (completions of the) sigma algebras generated by independent Brownian motions $B_1,...,B_k$ . Maybe we can show $$\int_Ff=\int_F \sum_{i=1}^k E[f ~|~\mathscr{F}_i]$$ for every $F\in \mathscr{F}$ ?  If $F_j\in \mathscr{F}_j$ we have $$\int_{F_j} \sum_{i=1}^k E[f ~|~\mathscr{F}_i]= \sum_{i=1}^k \int_{F_j} E[f ~|~\mathscr{F}_i]=\int_{F_j} f~~~+\sum_{i\neq j} \int_{F_j} E[f ~|~\mathscr{F}_i]=\int_{F_j} f~~~+\sum_{i\neq j} P(F_j) E[f ]=\int_{F_j}f$$ So that checks out.  But then if I try to integrate over $F=F_1\cap F_2\cap...\cap F_k$ for $F_i\in \mathscr{F}_i$ (looking toward a monotone class argument) I get (writing $F_i' := F_1\cap...\cap \hat F_i\cap...\cap F_k$ ), $$ \sum_{i=1}^k \int_F E[f ~|~\mathscr{F}_i]= \sum_{i=1}^k \int_{F_i'} \mathbb{1}_{F_i}E[f ~|~\mathscr{F}_i] =\sum_{i=1}^k P({F_i'})\int \mathbb{1}_{F_i}E[f ~|~\mathscr{F}_i] = \sum_{i=1}^k P({F_i'})\int_{F_i}f  $$ which doesnt really look like $\int f$ to me.  But I guess I haven't used that the $\mathscr F_i$ generate $\mathscr F$ ...","['probability-theory', 'functional-analysis', 'analysis']"
4287160,Find the Maximum Likelihood Estimator,"Let $X_1,X_2 ,X_3 ,..., X_n$ be iid with cdf $𝐹(𝑥) = 1 − 𝑥^{−𝜃}, 𝑥 > 0, 𝜃 > 0$. What I have done so far, $L(\theta)=\prod\limits_{i=1}^n\theta x_i^{\theta -1}=\theta^n\prod\limits_{i=1}^n\ x_i^{\theta -1}$ $ln(L(\theta))=nln(\theta)+(\theta-1)^{n}\prod\limits_{i=1}^n ln(x_i)$ $\frac{d}{d\theta}ln(L(\theta))=\frac{n}{\theta}+n(\theta-1)^{n-1}\prod\limits_{i=1}^nln(x_i)=0$ $\theta(\theta -1)^{n-1}=\frac{-1}{\prod\limits_{i=1}^nln(x_i)}$ How do I proceed?",['statistics']
4287167,Integral Representation of a Double Sum,Let us assume we know the value of $x$ and $y$ . I'm trying to write the following double sum as an integral. I went through many pages and saw various methods but I'm completely lost with my problem. $$\sum_{n = 1}^{\infty} \frac{x^{n}}{n!} \sum_{m = 1}^{n} \frac{\left( m - 1 \right)!}{\left( m + y \right)!}$$ Could anyone please give me a hint so I can go about solving it myself?,"['integration', 'summation', 'calculus', 'sequences-and-series', 'power-series']"
4287210,Exponential bound on the tail of a gaussian,"Let $Z$ be a centered normal variable of variance $\sigma^2$ , I am trying to prove that, $$\sup_{t>0} \left( \mathbb{P}(Z \geq t) \exp\left( \frac{t^2}{2 \sigma^2} \right) \right) = \frac{1}{2} $$ I have proven the quantity is at least $\frac{1}{2}$ , by looking at the limit for $t \to 0$ , I'm trying to upper bound the quantity inside the $\sup$ by $\frac{1}{2}$ . I've obviously tried re-writing the probability as an integral, and with a change of variable I'm able to write: $$ \mathbb{P}(Z \geq t) \exp\left( \frac{t^2}{2 \sigma^2} \right) = \int_0^{\infty} \exp \left( \frac{-u^2 - 2tu}{2 \sigma^2} \right) du $$ If I'm not mistaken we don't know how to explicitly calculate these types of integrals, and I see no obvious upper bounds. I've tried re-writing the exponential as a series (both in this expression and the original integral). I've tried integrating by parts the product of exponentials in this last expression, but it simply leads to a difference between two terms making it even harder to upper bound. I've also tried a Cauchy-Schwarz upper bound on the product of exponentials but it yields something proportional to $\frac{1}{t}$ , thus insufficient (which makes sense because only one of the functions depends on $t$ ).","['concentration-of-measure', 'probability']"
4287213,Something wrong in question wording?,"Question:
The probability that a given positive integer lying between 1 and 100 (both inclusive) is NOT divisible by 2, 3 or 5 is ______ . My approach: Let n be the set for numbers divisible by 2. $n={2,4,6....100}$ let o be the set for numbers divisible by 3. $o={3,6,9..99}$ let set p be the set of numbers divisible by 5. $p={5,10,15...100}$ Let E be the event that the number is not divisible by 2,3 OR(this word is important) 5.
The objective of the question is to find the probability of this event E. Let X' denote the complement of any set X.
Let P(X) denote the probability of any event(set) X So according to me $E=n'\cup o'\cup p'$ $P(E)$ turns out to 0.97 My answer is wrong though, according to the given solution: $E=n'\cap o'\cap p'$ So I think the wording of the question was wrong,
the question should be: The probability that a given positive integer lying between 1 and 100 (both inclusive) is NOT divisible by 2, 3 AND 5 is ______. Instead of: The probability that a given positive integer lying between 1 and 100 (both inclusive) is NOT divisible by 2, 3 OR 5 is ______ .","['elementary-set-theory', 'probability']"
4287232,"Deduce that $Z_m$ is the Radon-Nikodym-density $Z_m=\frac{dQ_m}{dP}$ of the probability measure $Q_m$, which is equivalent to P","The information given: Consider an arbitrage-free one-period financial market model $(S^0;S)$ . We have a risk free asset $S^0$ with $S_0^0=1,S_1^0=1$ so the risk free rate $r=1$ . We have a risky asset $S_0=50$ and $S_1$ uniformly distributed on the interval $(0,100)$ under the probability measure $\mathbb{P}$ . It holds for every real valued continuous function $f$ that: $$\mathbb{E}_\mathbb{P}(f(S_1))=\int_{-\infty}^{\infty}\mathbb{1}_{(0,100)}(x)\frac{f(x)}{100}dx$$ Furthermore for $m\in\mathbb{N}$ we have $\phi(x)=\frac{2m+1}{50^{2m}}(x-50)^{2m}$ and $Z_m(\omega)=\phi_m(S_1(\omega))$ The question : Show that $Z_m>0$ $\mathbb{P}$ -a.s. and $\mathbb{E}_\mathbb{P}(Z_m)=1$ . This I have done by showing that $\phi(x)>0$ and calculating the integral respectively. Deduce that $Z_m$ is the Radon-Nikodym-density $Z_m=\frac{dQ_m}{dP}$ of the probability measure $Q_m$ , which is equivalent to $P$ . Here is it I'm struggling. I know the random variable $X$ satisfying: $$\mathbb{E}_{\mathbb{Q}}(\mathbb{1}_A)=\mathbb{Q}(A)=\int_AXd\mathbb{P}=E(1_AX)$$ is called the Radon-Nikodym-density. I don't know how to show that $Z_m$ satisfies this however.","['measure-theory', 'statistics', 'finance', 'probability-theory', 'radon-nikodym']"
4287253,Generalized expected Hamming distance,"Let $x,y\in\{0,1\}^n$ be uniformly drawn at random from the set of all length- $n$ bitstrings $\{0,1\}^n$ . Let $d(x,y)$ be the Hamming distance of two such bitstrings, i.e.  the number of positions at which the bitstrings differ. Further, let $f:\{0,1\}^n \to \{-1,1\}$ and let $r=\frac{|f^{-1}(-1)|}{2^n}$ be the ratio of bitstrings where $f$ is negative to all bitstrings. I am wondering how to compute $$\underset{x,y}{\mathbb{E}}[f(x)f(y)\;d(x,y)]$$ For symmetry reasons, I would think that this expression should only depend on the ratio $r$ . Is this intuition correct? EDIT: No, it's not as pointed out by the comments. For the constant function $f=+1$ with $|f^{-1}(-1)|=0$ , it is easy to calculate the expression. It's just the expected Hamming distance $$\underset{x,y}{\mathbb{E}}[d(x,y)]=n/2.$$ The calculation uses the fact that we know that to every $x$ , there are precisely $\binom{n}{k}$ many $y$ at Hamming distance $d(x,y)=k$ . So that $$\underset{x,y}{\mathbb{E}}[d(x,y)]=\frac{1}{2^{2n}}\sum_{x,y\in\{0,1\}^n}d(x,y)=\frac{2^n}{2^{2n}}\sum_{k=0}^n \binom{n}{k} k= n/2.$$ However, I am now trying to generalize this calculation to other $f$ . I would also be happy, for starters, with ideas on how to bound the expression.","['combinatorics', 'discrete-mathematics', 'bit-strings']"
4287256,Expected covered area as integral of the probability of being covered?,"If I have a disk $\mathcal{D}$ , where each point is ""covered"" randomly with probability given by $$P(x \text{ is covered}) = p(x)$$ so that each point of the disk has a position-dependent probability of being covered, I understand I can write that the expected value of the total covered area $$\mathbb{E}(\text{total covered area}) = \int_{\mathcal{D}}P(x \text{ is covered})  \mathrm{d}x$$ but I am not sure how this relates to the definition of expected value. It seems relatively clear but I'm not sure how to prove it. Does it follow straightforwardly?","['geometry', 'probability']"
4287259,Normal approximation to Poisson's distribution,"Historic data of a computer network suggest that connections with this network follow a Poisson's distribution with an average of 5 connections per minute. Find $t_0$ , which has probability equal to $0.9$ to happen at least once before $t_0$ I made an approximation using $\lambda=12$ , since $5/60 =1/12$ , but my answer doesn't match with the book (book says it's ~ $27.5$ seconds, and mine ~ $17.0$ seconds) Could someone help me?","['poisson-distribution', 'statistics', 'probability']"
4287262,Differentiating under the integral sign in Kelvin's theorem,"I know well that this has already been asked here: Differentiation under the integral sign - line integral? ..but the answer given there already assumes that the length of the contour does not depend on time.. which is the actual point of the question. So I'll rewrite here the problem. Suppose you have an ideal isoentropic fluid, and consider the circulation on a contour $C(t)$ evolving in time $$\Gamma(t) =  \oint_{C(t)}\textbf{v}(\textbf{x}(s,t),t)\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds.$$ Kelvin's Theorem proves $\Gamma$ to be constant in time. But its time derivative is $$ \frac{d\Gamma}{dt} = \oint_{C(t)}\frac{d\textbf{v}}{dt}\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds + \oint_{C(t)}\textbf{v}\cdot\frac{d}{dt}\frac{\partial}{\partial s}\textbf{x}ds + BTs,$$ where I denoted with $BTs$ the boundary terms. Now, the issue is that, according to my textbook, and to the aforementioned answer (and to the wikipedia page https://en.wikipedia.org/wiki/Kelvin%27s_circulation_theorem as well), these boundary terms are vanishing. So..why is that? Shouldn't the length of the line on which we take the circulation depend on time?","['derivatives', 'fluid-dynamics']"
4287264,"Show that on the interval $(0,\alpha)$, where $\alpha \in (0, \pi]$, $\cos(\theta) = \cos (\alpha - \theta)$ at precisely one point.","As part of a problem I am working on, I need to demonstrate that the following two functions intersect at precisely one point: For $\alpha \in (0, \pi]$ , consider the functions: \begin{align} \\
&\cos_1 : (0,\alpha) \to [-1,1], \text{ with the mapping rule }\cos_1(\theta)=\cos(\theta) \\  &\cos_2:  (0,\alpha) \to [-1,1], \text{ with the mapping rule }\cos_2(\theta)=\cos_1(\alpha - \theta)=\cos(\alpha-\theta) \end{align} i.e. they are just relative shifted trig-functions of the same frequency observed over the interval $(0, \alpha)$ . At the moment, the only approach I could think of uses calculus and works as follows: The derivative of $\cos_2$ can be written as: $\cos_2 '(\theta)=-\sin(\alpha - \theta)\cdot(-1)=\sin(\alpha - \theta)$ and the derivative of $\cos_1$ can be written as $-\sin(\theta)$ . Given these derivatives, we know that over the interval $(0,\alpha)$ , $\cos_2$ is strictly increasing and $\cos_1$ is strictly decreasing . Further, it is also apparent that $\cos_1(\theta) = \cos_2(\theta)$ when $\theta = \frac{\alpha}{2}$ . I next made the following (general) lemma that reads as: For a function $f$ and a function $g$ , if $f$ is strictly decreasing on some interval $I$ and $g$ is strictly increasing on the same interval $I$ , and it is known that there is an $x^*$ for which $f(x^*) = g(x^*)$ , then there exists no other $y \neq x^*$ such that $f(y)=g(y)$ . i.e. $x^*$ is the only point in $I$ where the functions are equal. Briefly, the proof works by contradiction. Suppose there is a $y \in I$ where $y \neq x^*$ such that $f(y)=g(y)$ . Either $y \lt x^*$ or $y \gt x^*$ . Suppose the former. By assumption: If $y \lt x^*$ , then $f(y) \gt f(x^*)$ and $g(y) \lt g(x^*)$ ...because $f$ is strictly decreasing and $g$ is strictly increasing. However, $g(y)=f(y) \gt f(x^*)=g(x^*)$ . This implies that $g(y) \gt g(x^*)$ , which is a contradiction. A similar argument works for $y \gt x^*$ . Therefore, we conclude that $x^*$ is the only such point in $I$ where $f$ and $g$ are equal. Applying this lemma to my particular trig case, I have proven that over the interval $(0, \alpha)$ , $\cos_1$ only equals $\cos_2$ at one point: $\theta = \frac{\alpha}{2}$ . Although I believe this proof is fine , it struck me as possibly overkill. Do I really need calculus to prove this statement? I feel as though there is a simpler argument that does not require me to invoke calculus. Any suggestions are appreciated.","['calculus', 'solution-verification', 'trigonometry']"
