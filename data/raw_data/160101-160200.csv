question_id,title,body,tags
2762432,Proving $\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) $ doesn't exist.,"How can I show that $$\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) $$ doesn't exist? I used the fact that $$\arctan{x}\ge x-\frac{x^3} {3}, $$ so the initial limit is less than $$\lim_{x\to\infty} \frac{x^5}{3} +O(x^4),$$ therefore the limit tends to infinity. Is this enough? If not, then how can I show this rigorously?",['limits']
2762447,Periodicity of the solution of a non-linear differential equation,"Consider the following non-linear differential equation,
$$
\dot{x}(t)=a-b\sin(x(t)), \ \ x(0)=x_0\in\mathbb{R},
$$
and assume that $a$ and $b$ are positive real numbers with $a>b$.
Note that the solution $x(t)$ exists and can be analytically computed ( see here ). My question: Is $\cos(x(t))$ a zero-mean periodic function of $t\ge 0$? Further on, in case the answer to my previous question is in the affirmative, it should be that
$$
\left|\int_{0}^t \cos(x(t))\, \mathrm{d}t\right|\le K(a,b), \ \ \forall t\ge 0,
$$
where $K(a,b)$ is a positive constant depending on $a$ and $b$ only. So an additional question is: Can we find an explicit expression for $K(a,b)$? N.B. Numerical simulations seem to confirm the above claims. However I couldn't quite prove them. So any help is really appreciated!","['periodic-functions', 'ordinary-differential-equations', 'analysis']"
2762449,Generating function to find the sum of digits,"How many integers between $30,000$ and $100,000$ have a sum of $15$ or less? I was approaching this problem with a generating function: $$g(x) = (x^3+x^4+x^5+x^6+x^7+x^8+x^9)(1+x+...+x^9)^4$$ First I'm going to pull out $x^3$ then I would need the coefficients of $x$ to the $15$th, $14$th, $13$th etc correct? If so, can someone help me through finding the coefficients? Also finding the coefficient of x to the 15th would be the same as finding the coefficient of x to the 12 is I pull out x$^3$","['generating-functions', 'combinatorics']"
2762457,Combinatorial and Algebraic Proof of an Identity involving Stirling Numbers of the second kind ${n+1\brace k+1}=\sum_i \binom{n}{i}{i\brace k}$,"The question is to prove the identity $$
{n+1\brace k+1}=\sum_i \binom{n}{i}{i\brace k}\tag{1}
$$ via a combinatorial proof and an algebraic proof. The question is from Aigner's A Course in Enumeration . The braces indicate Stirling numbers of the second kind. I have managed to prove the identity using the polynomial method (which I will show below), but have not made much progress on the combinatorial proof. My attempt : The left hand side represents the number of ways to partition an $n+1$ element set (say $[n+1]=\{1,\dotsc, n+1\}$) into $k+1$ sets. Each of the summands on the right hand side represents choosing $i$ elements where $k \leq i\leq n$ from $[n]$ and then partitioning them into $k$ sets. There is probably some sort of classification of the partition of an $n+1$ element set in to $k+1$ sets but I am not seeing it. Algebraic Proof: We expand $(x+1)^n$ in two different ways. First, note that
$$
(x+1)^n=\sum_{i=0}^n\binom{n}{i}x^i=\sum_{i=0}^n\binom{n}{i}\sum_{k=0}^i
{i\brace k} (x)_k=\sum_{k=0}^n(x)_k\left[\sum_{i=k}^n \binom{n}{i}{i\brace k}
\right]\tag{2}
$$ 
by the binomial theorem where $(x)_k$ is the falling factorial of length $k$. For the second way write $(x+1)^n$ as
$$
\sum_{k=0}^n{n\brace k}(x+1)_{k}
=\sum_{k=0}^n{n\brace k}[(x)_k+k(x)_{k-1}]=
\sum_{k=0}^n\left[{n\brace k}+(k+1){n\brace k+1}\right](x)_k\tag{3}
$$
and conclude using the recurrence relation for Stirling numbers.","['stirling-numbers', 'combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
2762475,Deriving a weak form of Stirling's formula,"I'm reading a book which gives a recipe of sorts to derive the approximations:
$$\ln(n!) = \sum_{m=1}^{n} \ln m= n\ln n -n + O(\ln n)$$
I get stuck towards the end of the derivation, here goes: for $m\ge1$, we have $$0 \le \int_m^{m+1}\ln x\, dx - \ln m = \int_m^{m+1}\ln \frac{x}{m}\, dx \le \int_m^{m+1}\frac{x}{m}-1\, dx  = \frac{1}{2m}$$ So far so good. Now we're told to sum for $m = 1, 2, ... n-1$, and use the fact that  primitive of $\ln x$ is $x\ln x - x$. Let's go ahead and do that; by summing we get: $$0\le\int_{1}^{n} \ln x\,dx - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ and simplifying the integral: $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ This is where the instructions stop. Now it feels like we are very close to the result, however I'm unable to conclude. I tried to use the fact that $$\sum_{m=1}^{n-1}\frac{1}{m} \le 1 + \ln(n-1)$$ but to no avail. The problem seems to be that our sums go to $n-1$ but we always have $n\ln n - n$ (Though we do seem to be getting the $n-1$). Any step in the right direction is welcome. EDIT It took a while but I did get it in the end.
$$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} (1+\ln (n-1)) $$
$$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le 1 + \ln (n-1) $$
$$-1 \le n\ln n - n - \sum_{m=1}^{n-1} \ln m \le \ln (n-1) \le \ln(n)$$
$$-1 - \ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$ and finally for $n\ge2$, $1 \le 2\ln 2$, so 
$$-3\ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$. We can happily conclude : $$\left|n\ln n - n - \sum_{m=1}^{n} \ln m \right| \le 3\ln n$$ Hence $$\ln(n!) = n\ln n -n + O(\ln n)$$ I'm unsure what to do about a question which I've managed to answer so I'll leave it up here.","['inequality', 'analysis']"
2762492,Find the probability of the type I error and type II error,"For a Poisson arrival process with parameter $\lambda$ per hour. Let $Y$ be the number of arrivals in a ten-hour period. Find (i) the probability of the type I error ($\alpha$); (ii) the probability of type II error ($\beta$) at $\lambda = 3.2$ for a test with rejection region 
$RR = \{Y : Y > 24 \}$, in testing $H_0$: $\lambda = 2$ vs $H_1$:  $\lambda > 2$.  Then (iii) adjust the $RR$ so the significance level is the closest to $0.05$. what I have already done: 1) I think the way that I to have solve for $\alpha$ is by calculating the $1-P(Y \le 24)$ with $\lambda =20$. which I get $\alpha = .15677$. 2) Then I solved for $\beta$ using $P(Y \le 24)$ with $\lambda = 32$. I get $\beta  = .0881$. 3) For the last part I used $1-P(Y \le c)$ guessing which value of $c$ would give me $\alpha =.05$. I got that $c =27$ because it gives me $\alpha = .05248$. Am I understanding this correctly?","['statistics', 'probability', 'probability-distributions']"
2762520,Proving the Kernel is a free group,"My goal is to show the kernel of the following is a free group with infinite generators, meaning there are no relations on the generators. If my group is the fundamental group with genus $g$ of surfaces $S_g=⟨a_1,b_1,…,a_g,b_g\mid[a_1,b_1]...[a_g,b_g]=1⟩ $ and I have a map $H$ from my group to the integers: $H: S_g\rightarrow\mathbb{Z}$ such that $a_1$ goes to $1$ and all other elements go to zero. For example, $a_1b_1a_1^{-1}$ would be $1+0-1=0$ so that would be in the kernel along with any combination of elements which has the sum of the $a_1$ exponents equal to zero.","['algebraic-topology', 'general-topology', 'group-theory']"
2762544,Uniform convergence of integrals over a parameter,"Let $\mu$ be a regular $\sigma$-finite Borel measure on $X$. Suppose we have a sequence of continuous functions
$$f_n: X \times [0,T] \rightarrow \mathbb{R}: (x,t) \mapsto f(x,t), $$
which converges pointwise to a continuous function $f: X \times [0,T] \rightarrow \mathbb{R}$.
 Suppose the following: There exists a $g: X \times [0,T] \rightarrow \mathbb{R}$ such that for each $t \in [0,T]$ the integral of $g(\cdot, t)$ exists and for all $n$: $f_n \leq g$, For each $t \in [0,T]$ we have that $$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t), $$ For each $x \in X$ the convergence $f_n(x,\cdot) \rightarrow f(x,\cdot)$ is uniform. Is this enough to conclude that the convergence 
$$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t) $$
is uniform in $t$? Maybe a useful theorem to prove it would be Egonov's Theorem. I found the article Emanual Parzen, Some conditions for uniform convergence of integrals where in the beginning of the article the author claims it is true and refers to a book. I looked into the book, but the notations and assumptions are too vague to me and the Theorem isn't stated in its proper form. I ask this question since I'm not sure if I interpreted the previous references correctly.","['real-analysis', 'sequences-and-series', 'measure-theory', 'convergence-divergence', 'analysis']"
2762554,Do you know a generalization of a formula about cyclotomic polynomial?,"Let n be a natural number. Then n-th cyclotomic polynomial is defined as follows:
$$\Phi_{n}(x)=\prod_{k\in\mathbb{N}_{<k},(n,k)=1}(x-\zeta^k)$$
where $\mathbb{N}_{<k}$ means the set of natural number less than $k$ and $(n,k)$ means the greatest common divisor of $n$ and $k$, and $\zeta=\exp\frac{2\pi i}{n}.$ The following is well known:
$$\Phi_{n}(x)=\prod_{d\mid n}(x^d-1)^{\mu(\frac{n}{d})}.$$ This can be written as $$\Phi_{n}(x)=\prod_{d\mid n}\Phi_1(x^d)^{\mu(\frac{n}{d})}.$$ Then I want to try to generalize above formula and make formula with form: $$\Phi_{n}(x)=\prod_{d}\Phi_{m}(x^d)^{\mu(\frac{n}{d})}.$$ Is this already given and known to some people or mathematicians? If you know anything, could you teach me?","['number-theory', 'abstract-algebra', 'mobius-function', 'cyclotomic-polynomials']"
2762559,Toral automorphism has positive entropy iff $\mu \left( (B+tv^-) \cap B \right) >0$ for infinitely many $t$,"Let $X = \mathbb T^2=\mathbb R^2/\mathbb Z^2$ be the torus, $\mu$ a $T$-invariant measure on $X$, $A \in M_2(\mathbb Z)$ a hyperbolic matrix, $T_A$ the associated automorphism. Let $v^-$ be an eigenvector of the smaller eigenvalue. (The contracting eigenvector.) We want to prove: $h_\mu(T_a)>0$ if and only if for every $B$ with $\mu(B)>0$, there are arbitrarily large $t$ for which $\mu \left( (B+tv^-) \cap B \right) >0$. This resembles the definitions of non-wandering set and conservative action, for $\mathbb R$-actions. Observations in the comments: The translations along $v^{-}$ may not be measure-preserving, so we cannot apply Poincaré recurrence. The claim is easy in case $\mu$ is the Haar measure $m_X$.","['dynamical-systems', 'entropy', 'ergodic-theory', 'measure-theory']"
2762564,Differential equation: $y'=y^2/x-1$,"Does anyone know how to find a general solution (or a particular solution, if a general solution is not feasible) to the following differential equation? $$y'=\frac{y^2}{x}-1$$ This isn't separable, and Laplace transforms don't work out nicely. I've also tried a whole bunch of substitutions, none of which have led to a more manageable differential equation. Any ideas?",['ordinary-differential-equations']
2762576,The length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.,"I am referring to Theorem 2.23 of the book is Linear Algebra Done Right by Axler. It mentions Theorem: In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof Suppose $u_1, u_2,.....,u_m$ is linearly independent in V. Suppose also that
  $w_1,w_2,...,w_n$ spans V. We need to prove that $m \leq n$. We do so through the
  multi-step process described below; note that in each step we add one of the $u$’s and remove one of the $w$’s. Step 1 Let B be the list $w_1,w_2,...,w_n$, which spans V. Thus adjoining any vector
  in V to this list produces a linearly dependent list (because the newly
  adjoined vector can be written as a linear combination of the other
  vectors). In particular, the list
  $u_1,w_1,...,w_n$ is linearly dependent. Thus by the Linear Dependence Lemma (2.21),
  we can remove one of the $w$’s so that the new list B (of length $n$)
  consisting of $u_1$ and the remaining $w$’s spans V. Step j The list B (of length $n$) from step $j-1$ spans V. Thus adjoining any
  vector to this list produces a linearly dependent list. In particular, the
  list of length $n+1$ obtained by adjoining $u_j$ to B, placing it just after
  $u_1,u_2,...,u_{j-1}$, is linearly dependent. By the Linear Dependence Lemma
  (2.21), one of the vectors in this list is in the span of the previous ones,
  and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of
  the $w$’s, not one of the $u$’s. We can remove that $w$ from B so that the
  new list B (of length $n$) consisting of $u_1,u_2,...,u_j$ and the remaining $w$’s
  spans V. I have problem with the part that states By the Linear Dependence Lemma (2.21), one of the vectors in this list is in the span of the previous ones, and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of the $w$’s, not one of the $u$’s. Why does linear independence of the list $u_1,u_2,...,u_j$ imply that one
of $u$'s cannot be written as a linear combination of the rest of $u$'s and $w$'s in the list? What I can understand is that if the author said it must be possible to select one of $w$'s as otherwise, the $u$'s will end up being linearly dependent, then he'd be right. If you cannot choose any of the $w$'s and the list is known to be linearly dependent, then one of the $u$'s will end up being in the span of the rest of the $u$'s. This is not what he states though. He states it has to be one of $w$'s. I think that statement is wrong. If I am making a mistake in the way I have understood the proof, please help me understand it correctly.",['linear-algebra']
2762596,Image of a map $f : k^3 \longrightarrow k^3$,"Let $k$ be an algebraically closed field and define $f : k^3 \longrightarrow k^3$ by $$f(x,y,z) = (x, xy, xyz).$$ I would like to verify that the image of this map is $$f(k^3) = \{ (0,0,0) \} \cup k^3 \backslash \{ x,y =0 \},$$ where $f^{-1}(k^3 \backslash \{ x,y =0 \}) = k^3 \backslash \{ x,y =0 \}$ and $f^{-1}(\{ (0,0,0) \}) = \{ (0,0,z) : z \in k \}$. Notice that (if my computation is correct) the image is neither open or closed in $k^3$. How does one determine if the image is dense? Please note that $k^3 = \mathbb{A}_k^3$ equipped with the Zariski topology. Context : qualifying exam preparation.","['affine-geometry', 'algebraic-geometry', 'proof-verification']"
2762677,"Calculate the probability of each individual event; that is, calculate 𝒑(𝑨), 𝒑(𝑩), and 𝒑(𝑪).","A red and a blue die are thrown. Both dice are fair (that is, all sides are equally likely).
The events 𝑨, 𝑩, and 𝑪 are defined as follows:
𝑨: The sum of the numbers on the two dice is at most 3.
𝑩: The sum of the numbers on the two dice is odd.
𝑪: The number on the red die is 3.
a. Calculate the probability of each individual event; that is, calculate 𝒑(𝑨),
𝒑(𝑩), and 𝒑(𝑪). b. What is 𝒑(𝑨|𝑩)? c. What is 𝒑(𝑩|𝑪)? d. What is 𝒑(𝑨|𝑪)? e. Consider all pairs of events: 𝑨 and 𝑩, 𝑩 and 𝑪, and 𝑨 and 𝑪. Which pairs of
events are independent? My attempt a) 
$p(A)=\frac { 3 }{ 36 } $=$\frac { 1 }{ 13 } $ $p(B)=\frac { 18 }{ 36 } $=$\frac { 1 }{ 1 } $ $p(C)=\frac { 1 }{ 6 } $ b) 𝒑(𝑨|𝑩) = $\frac{P(A\cap B)}{p(B)}$ = $\frac{\frac { 1 }{ 18 }}{\frac { 1 }{ 2 }}$ = $\frac { 1 }{ 9 } $ c) 𝒑(𝑩|𝑪) = $\frac{P(B\cap C)}{p(C)}$ = $\frac{\frac { 1 }{ 12 }}{\frac { 1 }{ 6 }}$ = $\frac { 1 }{ 2 } $ d) 𝒑(𝑨|𝑪) = $\frac{P(A\cap C)}{p(C)}$ = 0. because there is no common for A and C. can anyone verify this..","['probability', 'discrete-mathematics']"
2762709,Geometric Proof of Perron-Frobenius,"I am reading this paper ( A Geometric Proof of the Perron-Frobenius Theorem , A. Borobia, U. R. Trias, Revista Mathematica de la Universidad Conplutense de Madrid, Vol. 5, 1992) where a short geometric proof of the Perron-Frobenius theorem is given.
I am having trouble at one place, which I articulate below.
I sacrifice some generality in service of simplicity. Let $A$ be an $n\times n$ matrix with positive real entries and $T:\mathbf R^n\to \mathbf R^n$ be the linear map whose matrix representation with respect to the standard basis is same as $A$ .
Define $$C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i>0\}, \quad \bar C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i\geq 0\}$$ The following are true: Theorem 1. There is a positive eigenvalue of $T$ with corresponding eigenvector in $C$ . Theorem 2. If $\lambda$ is an eigenvalue as in Theorem 1, then the geometric multiplicity of $\lambda$ is $1$ . Theorem 3. If $\lambda$ is an eigenvalue as in Theorem 1, then the algebraic multiplicity of $\lambda$ is $1$ . (Clearly, Theorem $3$ subsumes Theorem 2.) Theorem 1 can be proved using Brouwer's fixed point theorem (BFPT).
We notice that if $R$ is the collection of all rays in $\mathbf R^n$ of the form $\{av:\ a\geq 0\}$ for some $v\in \mathbf R^n$ having all entries non-negative, then $R$ is fixed, as a set, by $T$ . But $R$ is homeomorphic to the $n-1$ disc, and thus by BFPT we see that there is some ray in $R$ that is fixed by $T$ . This immediately gives $1$ . (The fixed ray cannot lie in $\partial C$ because all entries of $A$ are positive.) For Theorem 2 we argue as follows. Let $\lambda$ be a positive eigenvalue of $T$ with $v$ as a corresponding eigenvector, all of whose entries are positive.
If the geometric multiplicity of $\lambda$ is not $1$ , then there is a vector $u\notin \text{span}(v)$ with $Tu=\lambda u$ .
Let $V$ be the plane spanned by $u$ and $v$ .
Each ray in $V$ is fixed by $T$ .
But there is a ray in $V$ spanned by a vector in $\partial C$ , which cannot remain fixed under $T$ , giving a contradiction. I am stuck with the proof of Theorem 3 . The proof in the above-cited paper proceeds as follows.
Let $\lambda$ be a positive eigenvalue with the corresponding eigenvector $v$ having all entries positive.
Assume that the algebraic multiplicity of $\lambda$ is more than $1$ .
Then we can find a $T$ -invariant plane $U$ containing $\text{span}(v)$ .
Let $S^1$ be identified with the set of rays in $U$ .
Let $r$ and $-r$ denote the rays spanned by $v$ and $-v$ respectively.
By Theorme 2, $S^1$ is not point-wise fixed under the action of $T$ . And here is what I don't follow: The set of points in $S^1$ which are fixed by the action of $T^2$ does not consist only of $r$ and $-r$ .
Other wise the dynamics of the action of $T^2$ over $S^1$ looks like this Here $L$ is the arc of $S^1$ formed by the intersection of $S^1$ with the set of rays in $\bar C$ . EDIT: Why $T^2$ needs to have a fixed point apart from $r$ and $-r$ can be argued as follows: Assume on the contrary. Note that $T^2$ is orientation preserving. So the arc ""above"" the points $-r$ and $r$ (including $-r$ and $r$ ) is mapped to itself under $T^2$ .
Since this is homeomorphic to the closed interval, either all points in the open arc converge to $r$ under iterates of $T^2$ , or all points of the open arc converge to $-r$ under iterates of $T^2$ . But the sets $C$ and $-C:=\{-x:\ x\in C\}$ are invariant under $T^2$ so we get a contradiction. Can somebody explain this last piece of reasoning.
And how is it helping us deduce that the algebraic multiplicity of $\lambda$ is $1$ .
This argument is at the bottom of the second page of the paper I mentioned.","['matrices', 'positive-matrices', 'markov-chains', 'algebraic-topology', 'linear-algebra']"
2762715,To find the smallest value of 'k' for the following equation,"Let $\mathrm a,b$ are positive real numbers such that for $\mathrm a - b = 10$, then the smallest value of the constant $\mathrm k$ for which $\mathrm {\sqrt {x^2 + ax}} - {\sqrt{x^2 + bx}} < k$ for all $\mathrm x>0$, is? I don't get how to approach this problem. Any help would be appreciated.","['linear-algebra', 'functions']"
2762752,Mistake in proof about vector spaces,"Hello this is my first post on here and I have a question about an error in a proof. My Linear Algebra professor was covering Linear Independence, Basis, and Dimension for Vector Spaces. We got to a proof from the textbook and he said the proof is wrong. He said to try and figure out what is wrong with it. I have been trying to figure out what is wrong and after thinking about it, talking with peers, and with my professor I have narrowed it down to a single sentence within the proof. I think that it is a wrong word choice. He said it is very subtle. I was hoping some of you can provide some insight. THM 6.11: Let $W$ be a subspace of a finite-dimensional vector space $V$. Then: (a) $W$ is finite-dimensional and $\dim W \le \dim V$. (b) $\dim W = \dim V$ if and only if $W = V$. He has a problem with the proof of part (a) which is as follows. Proof (a) Let $\dim V = n$. If $W = \{\mathbf{0}\}$, then $\dim(W) = 0 \le n = \dim V$. If $W$ is nonzero, then any basis $\mathcal{B}$ for $V$ (containing $n$ vectors) certainly spans $W$, since $W$ is contained in $V$. But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f). Hence, $W$ is finite-dimensional and $\dim W \le n = \dim V$. Theorem 6.10(f) states: Let $V$ be a vector space with $\dim V = n$. Then: (f) Any spanning set for $V$ can be reduced to a basis for $V$. My professor said that me and my peers were right about the error being in the sentence, ""But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f)."" I believe that the error has something to do with with the word reduced . I asked him and he said that I was on the right track but he didn't give me a clear answer. What is wrong with this proof? More specifically the sentence above? Any help would be much appreciated. (Textbook: Linear Algebra: A Modern Introduction by David Poole 4th edition. The proof is found in section 6.2: page 456)","['proof-explanation', 'linear-algebra', 'vector-spaces']"
2762770,Solve differential equation with three variables,"I have the following differential equation: $$z''+y'= \cos (x)$$ $$y''-z= \sin (x)$$ with $z(0)=1$, $z'(0)=-1$, $y(0)=1$ and $y'(0)=0$. These differential equations involves three variables $x, y, z$. Can someone drop a hint on how to start this.",['ordinary-differential-equations']
2762772,Vector fields on $S^3$,"Does there exist a smooth vector field from $S^3$ to its tangent bundle with exactly $3$ zeros? The only vector field I can think was $v(x_1,x_2,x_3,x_4)=(-x_2,x_1,-x_4,x_3)$ which has no zeros. How could I think about a one that has zeros?
Any help is appreciated.","['vector-bundles', 'differential-geometry']"
2762773,an intuitive example of convergence in distribution does not imply convergence in probability,"In statistical inference by casella, it provides a nice example that shows how convergence in probability does not imply almost surely convergence. However, there is no good counterexample showing how convergence in distribution does not imply convergence in probability. After spending some time, it seems like if I can find two CDFs that are identical but have different random variables, it should work out. But the most I spend time on it, this seems impossible. Convergence in distribution says that $$\lim_{n \to \infty} F_{X_n}(x) = F_X(x)$$ at all points x, and $F_X(x)$ be continuous. But if the cdfs are the same, then the pdfs are also the same. How can we have convergence in distribution and not convergence in prob?","['probability-limit-theorems', 'probability-theory', 'probability-distributions']"
2762812,an odd degree polynomial with cyclic Galois group has root all real,"I am thinking about following question: Let $f\in\mathbb{Q}[x]$ be an odd degree polynomial with cyclic Galois group. Prove that all the roots of $f$ are real. I tried to prove it by contraction. Suppose $f$ has some complex roots. Let $K$ be its splitting field. Then $[K:\mathbb{Q}]=odd$. I tried to derive something from the cyclic Galois group, but I failed. Or maybe I am reviewing for the prelim so I am really terrified... Any hints or explanations are really appreciated!!!","['abstract-algebra', 'galois-theory', 'field-theory']"
2762833,Extending a Cartier divisor on a curve to singular points?,"Let $k$ be an algebraically closed field with $\text{char } k \neq 2$. Suppose we have a projective plane curve with a singular point. The example I have in mind asking this question is the curve $y^{2}z = x^{3}$ in $\mathbb{P}_{k}^{2}$ of Example II.6.11.4 of Hartshorne. This has one singularity at the point $Z = (0, 0, 1)$. Call this curve $X$. This scheme is not even normal or regular in codimension $1$, so we can't talk about Weil divisors but we can talk about Cartier divisors. However the open subset $U = X \setminus \{ Z \}$ is locally factorial, so not only can we talk about Weil divisors, but we have a correspondence between Weil divisors and Cartier divisors. Is it possible to begin with a Cartier divisor on $U$, then extend it to all of $X$? The reason I want to do this is so that I can define a surjective degree map,
$$
\text{deg}: \text{CaCl } X \longrightarrow \mathbb{Z}.
$$
In order to do this, I want to be able to justify that every Weil divisor on $U$ can be realised as the restriction of some Cartier divisor on $X$. Since $U$ is locally factorial, this is equivalent to the claim that every Cartier divisor on $U$ can be realised as the restriction of a Cartier divisor on $X$. I'm fairly confident this is possible, but I haven't been able to figure out the details. Is anyone able to walk me through that process? In what generality does that hold? As an aside, is there some canonical or natural definition of degree for Cartier divisors in general?","['algebraic-curves', 'schemes', 'divisors-algebraic-geometry', 'algebraic-geometry']"
2762934,Krull-Schmidt for abelian varieties,"Let $k$ be a field.  If $A$ is an abelian variety over $k$, does $A$ have a unique decomposition as a direct product of directly indecomposable abelian varieties, up to rearrangement and isomorphism of the factors? I know that the corresponding statement in the isogeny category is true (as the isogeny category is semisimple), but that the actual category of abelian varieties is not semisimple (or even abelian). I believe the answer is no, based on an example my friend pointed out in Orlov's paper on derived equivalences of abelian varieties.  ( Link , Example 4.16.)  In that example, it is assumed that $A \times \hat{A} \cong B \times \hat{B}$, and the hypotheses on $A$ imply that each of the four summands is directly indecomposable.  I'm assuming from the discussion there that such varieties exist (other than $B = A$ and $B = \hat{A}$ of course), but it isn't clear to me how to construct them. Motivation:  a positive answer would simplify the last step of Zarhin's proof that there are only finitely many abelian varieties of a given dimension over a given finite field--a key step in the proof of the Tate conjecture for abelian varieties over finite fields.  (Edit:  Tate himself used a weaker version of the finiteness statement; Zarhin's trick simplifies his proof.)  A negative answer would explain the importance of passing to the isogeny category.","['abelian-varieties', 'algebraic-geometry']"
2762976,Evaluate line integral and determine if independent of path,"The exercise consists of calculating the integral $\int_{\gamma} \mathbf{F} 
 \cdot d \mathbf{r}$ where $\mathbf{F}(x,y)=\left( - \frac{y-1}{(x-1)^2 + (y-1)^2}, \frac{x-1}{(x-1)^2 + (y-1)^2} \right) + \left(\frac{y}{x^2 + y^2}, \frac{-x}{x^2 + y^2} \right)$ and $\gamma$ is the circle $x^2 + y^2 = 4$ oriented anticlockwise, and to determine if the integral is independent of its path on the set $D=\{(x,y): (x,y) \neq (0,0) \text{ and } (x,y) \neq (1,1) \}$ For the first part, I concluded that using Green's theorem is not an option, since the disc $x^2 + y^2 \leq 4$ contains singular points. Since it is appears to be a bit of a tricky function, I also chose not to parametrise it directly. Instead I opted to determine the potential function to the integral. I show that $\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$, and, solving it like a system of equations, found that the potential function is
$U(x,y) = \arctan{(\frac{y-1}{x-1})} - \arctan{(\frac{y}{x})}$. If I $U$ actually is a conservative vector field, I can then evaluate the integral simply by using the starting points and end points of $U$, by letting the starting point and end point to be, for instance, $(\sqrt{2}, \sqrt{2})$. Since $\gamma$ is a closed curve, the integral is zero. I then argue that, on the set $D$, the integral is independent of its path, since $\mathbf{F}$ is conservative, and the potential function $U$ does not have any singular points on $D$. I am a bit unsure whether this is the right approach, but I cannot see how else I would solve this exercise. Any input would be much appreciated.","['multivariable-calculus', 'vector-fields', 'integration']"
2762978,"Use characteristic curve method to solve the problem $u_t(x,t)+u_x(x,t)+u(x,t) = 0$","Use characteristic curve method to solve the problem:
   $$u_t(x,t)+u_x(x,t)+u(x,t) = 0 ~~\text{for } 0 < x < \infty, t > 0$$
   $$u(x,0) = \cos(2x)$$ $$u(0,t) = 1$$ Note that if I am allowed to use change of coordinates method (also a method in Strauss PDE book), I am able to solve this problem without much problem. However, the question specifically asked me to use the characteristic curve method: My attempt trying to mimic some examples (those examples are not very similar to this question) Let $u_x+u_t = -u$,  and we have $$\frac{dx}{ds} = 1, \frac{dt}{ds} = 1, \frac{du}{ds} = -u$$
Hence it follows that $\frac{dt}{dx} = 1$ and solving to get $c_1 =t-x$ where $c_1$ is a constant. Now $\frac{du}{dt} = -u$ solving to get $c_2 = e^t u$ where $c_1,c_2$ are both characteristic curves. Then I am quite lost as how should i continue using only the method of characteristics.","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
2763010,"Probability textbook introducing finite probability first, then Kolmogorov axioms","I'm looking for a probability textbook that would first build intuition using finite sample spaces and uniform probability mass over the sample space in a first part (coin toss, die, Bernoulli Urn, etc.) Then introduce Kolmogorov axioms and tackle infinite sample spaces in a second part. It's best if there are interesting applications in this second part.","['reference-request', 'probability-theory', 'probability']"
2763015,How to proceed in this proof?,"Let 
\begin{eqnarray}
\eta(x) = \mathbf{P}(Y=1 \mid X= x)
\end{eqnarray}
and $Y\in \{0,1\}$. Assume $\tilde{\eta}_1(x)$ and $\tilde{\eta}_0(x)$ are some approximations of $\eta(x)$ and $1-\eta(x)$ respectively and sum of them is not one. Also, define 
$$
g(x) = \begin{cases}
0; & \text{if } \tilde{\eta}_1(x) \leq \tilde{\eta}_0(x)\\
1; & \text{otherewise}
\end{cases},
\quad g^*(x) = \begin{cases}
0; & \text{if } \eta(x) \leq 1-\eta(x)\\
1  & \text{otherewise}
\end{cases}.
$$
How to prove the following theorem: 
$$
\mathbf{P}(Y\ne g(x) )-\mathbf{P}(Y\ne g^*(x) ) \leq \int_{\mathbb R^d} \vert 1-\eta(x)-\tilde{\eta}_0(x)\vert \mu(dx) + \int_{\mathbb R^d} \vert \eta(x)-\tilde{\eta}_1(x)\vert \mu(dx).
$$
This is Theorem 2.3 from the book by Devroye: A probabilistic Theory of Pattern Recognition . Edit: We know 
$$
\mathbf{P}(Y\ne g(x) )-\mathbf{P}(Y\ne g^*(x) )= \int_{\mathbb R^d} \Bigl\{\mathbf{P}(Y\ne g(X)\vert X= x )-\mathbf{P}(Y\ne g^*(X)\vert X= x )\Bigr\}\mu(dx),
$$
and
\begin{align*}
\mathbf{P}(Y\ne g(X) \mid X= x ) &= 1- \mathbf{P}(Y =  g(x) \mid X= x)\\
& = 1- (\mathbf{P}(Y = 0,\ g(x)=0 \mid X= x) + \mathbf{P}(Y = 1,\ g(x)=1 \mid X= x))\\
& = 1- (I_{[g(x)=0]}\mathbf{P}(Y = 0 \mid X= x) + I_{[g(x)=1]}\mathbf{P}(Y = 1\mid X= x)).
\end{align*}
Therefore, 
\begin{align*}
&\mathrel{\phantom{=}}{} \mathbf{P}(Y\ne g(X) \mid X= x )-\mathbf{P}(Y\ne g^*(X) \mid X= x )\\
& = (I_{[g^*(x)=0]}\mathbf{P}(Y = 0 \mid X= x) + I_{[g^*(x)=1]}\mathbf{P}(Y = 1 \mid X= x))\\
&\quad - (I_{[g(x)=0]}\mathbf{P}(Y = 0 \mid X= x) + I_{[g(x)=1]}\mathbf{P}(Y = 1 \mid X= x))\\
& = (I_{[g^*(x)=0]}(1-\eta(x)) + I_{[g^*(x)=1]}\eta(x))\\
&\quad - (I_{[g(x)=0]}\mathbf{P}(Y = 0 \mid X= x) + I_{[g(x)=1]}\mathbf{P}(Y = 1 \mid X= x)).
\end{align*}
But how to proceed?","['probability-theory', 'inequality', 'pattern-recognition']"
2763029,Limit Sup of mean square error of 2 Gaussian process,"I have been struggling to proceed to how to proof this theorem. I saw this theorem in statistics specially in spatial statistics. I have seen a proof to which I understand but I want to use a different approach ( I will post a link below).  So let me describe it.
Assume that $Z(x)$ has mean zero and is observe,then the best linear unbiased prediction (BLUP) at an (unobserved) location $x_∗$ is $\hat{Z}{(x_{∗})} = c^{*^T}C^{-1}Z(x)$ where $Z(x) =  (Z(x_1),...,Z(x_n))',  C_{ij} = K(x_i, x_j )$ , and $c^{*}_i = K(x_i, x_∗)$ . Suppose $G(0, V_{1}=C_{1} + \sigma^{2})$ and $G(0, V_{0} = C_{0} + \sigma^{2})$ are two gaussian measures on a random field $D$ , we need to show that, for an best linear unbiased predictor $\hat{Z}{(s_{o})} = c^{*^T}C^{-1}Z(x)$ As $ n - > \infty $ , $$ lim  \;sup \frac{MSE_1(\hat{Z}{(x_{∗})})}{{MSE_0(\hat{Z}{(x_{∗})})}} \rightarrow 1$$ . Consider the following to help you proof that: $C_1$ can be written as $EDE'$ a SVD. and EE' and orthogonal. $C_0$ can be written as $ULU'$ a SVD. and UU' are orthogonal. I want to write $ {MSE_1(\hat{Z}{(x_{∗})})} = E (Z(x*)- \hat{Z}{(x_{∗}}))^2$ and proceed by using $ \hat{Z}{(x_{∗})}$ and hint 1 and 2. Thank you https://arxiv.org/pdf/1108.1851.pdf","['probability-limit-theorems', 'stationary-processes', 'probability-theory', 'mean-square-error']"
2763051,"Find all positive integer $a,b$, such that $[a,b+2017]=[a+2017,b]$.","Find all positive integer $a,b$, such that
$$[a,b+2017]=[a+2017,b]$$
where $[x,y]$ is the least common multiple of the numbers $x,y$. My attempts : 1) If $a=b$ then pair $(a;a)$ is solution. 2) Let $a>b$ and $2017=p$. $$[a,b+p]=[a+p,b]$$
$$\frac{a(b+p)}{\gcd(a,b+p)}=\frac{(a+p)b}{\gcd(a+p,b)}$$ If $a>b$ then $a(b+p)>(a+p)b$ and thus $\gcd(a,b+p)>\gcd(a+p,b)\ge1$","['number-theory', 'divisibility', 'integers', 'elementary-number-theory']"
2763130,Find $2^A$ with $A$ is a matrix,"Let $$A=\begin{bmatrix} -1 &-2 &-2 \\1&2&1  \\-1&-1&0  \end{bmatrix}$$ How to find $2^A$ ? I find out that $A^2=I$ so it would be simple if they ask me how to find a power of $A$, but not. So could you help me?","['matrices', 'matrix-exponential', 'linear-algebra']"
2763157,What are the asymptotics of sums like $\frac16+\frac1{10}+\frac1{14}+\frac1{15}+\frac1{21}+\cdots+\frac1{p_{n-1}p_n}$?,"I've recently bumped into this old question of mine and I was wondering about the sum running over only numbers with distinct prime factors. More precisely: For $k,n\ge1$ let $S_{k,n}$ be the set of numbers whose factorization
  contains exactly $k$ distinct primes out of the first $n$ ones. So,
  for instance: $S_{1,n}=\{2,3,...,p_n\};$
  $S_{2,n}=\{6,10,...,2p_n,15,21,...,3p_n,...,p_{n-1}p_n\}.$ With both $k$ and $n$ allowed to vary, are the asymptotics of $$\sum_{m\in S_{k,n}} \frac1m$$ known? Up to what order?","['real-analysis', 'analytic-number-theory', 'number-theory', 'prime-numbers', 'sequences-and-series']"
2763177,"For matrix $A$, if $A^4 = 0$ does this also mean that $A^2 = 0$?","For an arbitrary matrix $A$,  if $A^4 = 0$ does this also mean that $A^2 = 0$? My thinking is that it does since I can reduce $A^4$ into $(A^2)^2$ but I'm not sure if this helps or not.","['matrices', 'linear-algebra', 'nilpotence']"
2763202,General equation/function to get angle from x and y coordinates?,"We can get the angle between $x$ and $y$ (or $\cos{\theta}$ and $\sin{\theta}$ respectively) from
$$
\theta =\tan^{-1}{\frac{y}{x}}
$$
but only if  $ -\frac{\pi}{2} < \theta < \frac{\pi}{2} $ since the cases with negative $x$ and/or $y$ exist where 
$$
\tan^{-1}{\frac{-y}{-x}} = \tan^{-1}{\frac{y}{x}}
$$ 
and 
$$
\tan^{-1}{\frac{-y}{x}}=\tan^{-1}{\frac{y}{-x}}.
$$
Let's also not ignore division by zero at $|\theta| = \pi/2$. When coding, I can often (in many coding languages) do something like theta = (y>=0)*arccos(x) + (y<0)*(2*pi - arccos(x)) where y>=0 and y<0 are boolean expressions that evaluate to $1$ or $0$ if the statements $y\geq 0$ and $y<0$ are true or false respectively, but this is not an expression I find easy to work with in pure math. What is a good mathematical way to express the angle for all four quadrants as an expression/function of $x$ and $y$? Sometimes I might want a function that is differentiable, and then I don't think I want to involve boolean statements in the mathematics since I am not very experienced with that kind of math, but I am open-minded. It must be differentiable at least.","['boolean-algebra', 'angle', 'trigonometry']"
2763219,Convergence in probability of sequence of independent random variables implies almost sure convergence of their sum?,"$\{X_n\}^{\infty}_{n=1}$ are independent random variables. Let $S_n = X_1 + \cdots + X_n$ be the random walk. Show that $\{S_n\}$ converges almost surely if and only if the random sequence converges in probability. My approach (Only if part): Suppose $X_n \geq 0$.
$|X_m - X_n| \leq |X_m|+|X_n| \leq |X_m + \cdots + X_n|$ for all $m,n \geq 1$. $\therefore \sup_{m,n \geq N} |X_m - X_n| \leq \sup_{m,n \geq N} |X_m + \cdots + X_n|$ for all $N$. $\therefore \mathbb{P}(\sup_{m,n \geq N} |X_m - X_n| > \epsilon) \leq \mathbb{P}(\sup_{m,n \geq N} |X_m + \cdots + X_n| > \epsilon)$ for all $N$. Limit of RHS (over $N$) is $0$ (Cauchy criterion of convergence of series). Hence so is the LHS. Now for general $\{X_n\}^{\infty}_{n=1}$, suppose $\sum X_n$ converges a.s. Therefore $\sum X^+_n$ and $\sum X^-_n$ also converge a.s. (Is this true?). By first part this means $X^+_n$ and $X^-_n$ converge in probability, to $X^+$ and $X^-$, say. Hence $(X^+_n - X^-_n)\xrightarrow[]{\mathbb{P}}(X^+ - X^-)$. (Algebraic property of probability convergence.) However I'm not able to get the if part. Also, suppose $X_n$'s are non-zero iid. They converge in probability but $\sum X_n$ does not converge. So I'm not even sure I understand the if part of the question correctly.","['probability-theory', 'measure-theory']"
2763229,How to solve a system of $4$ equations?,"I would like to solve the following system of equations in $ \mathbb{C} $ :
$$ \begin{cases} x+2y+3z+4t+3xy-2zt = 1 \\ 4x+y+2z+3t+2xy-4zt = 3 \\ 3x+4y+z+2t-xy+zt = -1 \\ 2x+3y+4z+t+4xy+2zt = 2 \end{cases} $$
I'm not able to solve it unfortunatly.
The idea is to transform this system in another system of the form :
$$ \begin{cases} a_{11}(x+y)+a_{12}(z+t)+a_{13} \ xy+a_{14} \ zt = b_{1} \\ a_{21}(x+y)+a_{22}(z+t)+a_{23} \ xy+a_{24}\ zt = b_{2} \\ 
a_{31}(x+y)+a_{32}(z+t)+a_{33} \ xy+a_{34} \ zt = b_{1} \\ a_{41}(x+y)+a_{42}(z+t)+a_{43} \ xy+a_{44} \ zt = b_{4} \end{cases} $$
such that : $ A = (a_{ij})_{1 \leq i,j \leq 4} $ is an invertible matrix, which is easy to solve in this case, right ? But, i don't know how to do it.
Could you help me please ? Thank you.","['algebra-precalculus', 'polynomials', 'systems-of-equations']"
2763244,Dense sets in Hilbert space of square summable sequences?,"Let $l^2 = \{x = (x_n)| x_n \in \mathbb{R}, \sum_{n=1}^{\infty} x_n^2 < \infty\}$  be the Hilbert space of square summable sequences and let $e_k$ denote the $k^{th}$ co-ordinate vector (with $1$ in $k^{th}$ place, $0$ elsewhere). Which of the following subspaces is NOT dense in $l^2$? span$\{e_1-e_2, e_2-e_3, \ldots\}$ span$\{2e_1-e_2, 2e_2-e_3, \ldots\}$ span$\{e_1-2e_2, e_2-2e_3, \ldots\}$ span$\{e_2, e_3, \ldots\}$ By definition, we have to prove that for any $x \in l^2$ and for any $\epsilon > 0$, there exists $y$ in the respective sets such that $\|x-y\| < \epsilon$. How to proceed further?","['orthonormal', 'lp-spaces', 'sequences-and-series', 'hilbert-spaces']"
2763249,Does a map which preserve harmonic forms preserve co-closedness of forms?,"$\newcommand{\M}{\mathcal{M}}$
$\newcommand{\N}{\mathcal{N}}$ Let $\M,\N$ be closed $d$-dimensional oriented Riemannian manifolds. Let $f:\M \to \N$ be smooth, and let $\delta=d^*$ be the adjoint of the exterior derivative. Let  $1 \le k \le d$ be fixed . Consider the following two properties $f$ can have: $\delta^{\N} \omega=0 \Rightarrow \delta^{\M}(f^*\omega)=0$ for every $k$-form $  \omega \in \Omega^k(\N)$. $\,\,\,\,$ 2. $\omega\in \Omega^k(\N)$ is harmonic $\Rightarrow f^*\omega$ is harmonic. Question: Does property 2 implies property 1? (Property 1 certainly implies property 2, since a form is harmonic if and only if it's closed and co-closed, and closedness of forms is preserevd automatically, by any smooth map). Comment: A map which satisfies  property $2$ for $k=1$, is called harmonic morphism . I suspect the answer is negative, since the space of harmonic forms is finite-dimensional. Thus property $2$ gives us a ""finite-dimensional"" information about $f$, while the requirement of property $1$ is on the much larger space $\text{ker} \delta$, which is infinite dimensional.","['harmonic-functions', 'riemannian-geometry', 'partial-differential-equations', 'differential-forms', 'differential-geometry']"
2763270,Quantile function for binomial distribution?,"A test will succeed with a certain percentage. Now this test is repeated X number of times. I want to be able to get an estimate of the total number of succeeded test. Given that I know both the probability of success and the X number of attempts, is it possible to symbolically compute this estimate? I want to use this in a piece of software. Where I generate a random value [0, 1] and then use the requested formula as a mapping function. From what I gathered I need a quantile function of binomial distribution? Can I compute or estimate that one symbolically?","['statistics', 'probability', 'quantile']"
2763290,Is there a fundamental problem with extending matrix concepts to tensors?,"We are familiar with the theory of matrices, more specifically their eigen-theorems and associated decompositions. Indeed singular value decomposition generalizes the spectral theorem for arbitrary matrices, not just square ones. Now it only seems natural to extend this idea of 2 dimensional array of numbers to higher dimensions, i.e. tensors. But as soon as we do this, everything breaks down. For example even the notion of rank of matrix (which we all agree to be minimum of either column rank or row rank for a matrix) seems to be conflated when it comes to tensors. The Wikipedia page seems to use degree , order and rank of a tensor synonymously (understandably due to different terminology used in different fields, but somewhat annoying nevertheless). ""The order (also degree or rank) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array."" Also for example, the very familiar concept of eigenvalues or vectors also flies out the window (though people have defined them for super-symmetric tensors).  So my question is this: What is the fundamental reason why the ""nice"" theorems we have in the matrix case do not extend to the case of tensors? I can think of a couple: Tensors exhibit the phenomenon of rank jumping as well as field dependence ; which would imply the usual rules of analysis need to be re-examined when dealing with them. A large class of matrices are groups, so tools from abstract algebra are available to deal with them. Matrices can be viewed as operators from one space to another unambiguously, whereas viewing a tensor as an operator between spaces can get confusing very quickly. I know there are extensions to SVD for tensors, for example Tucker decomposition , HOSVD, etc; so I am not claiming it can't be done. I also understand (somewhat) that mathematicians prefer to study tensors abstractly or using differential geometry and forms. I am just curious as to why the results generalizing the concepts form the matrix case are many ; what is the underlying cause for a lack of unifying framework. The above reasons seem valid roadblocks, but do they hint at something more fundamental?","['matrices', 'tensors', 'linear-algebra']"
2763306,Convergence of distribution functions implies weak convergence of associated probability measures,"I'm trying to prove the following: Let $P,P_n$ be probability measures on $\mathbb{R}^d$ with distribution functions $F$ and $F_n$ respectively. Show that $F_n(t)\to F(t)$ for all continuity points $t$ of $F$ implies that $P_n$ converges weakly to $P$. I'm using the following notion of weak convergence of measures. $P_n$ is said to converge weakly to $P$ on a metric space $S$, if $\displaystyle\lim_{n\to\infty}\int_{S}f\,dP_n=\int_{S}f\,dP$ for all bounded and continuous functions $f:S\to\mathbb{R}$. Related question are Equivalence of definition for weak convergence Convergence of Probability Measures and Respective Distribution Functions However, I'm still not getting any further. Any help would be appreciated.","['probability-theory', 'measure-theory']"
2763321,Finding all group homomorphisms of $S_n\to\mathbb{C}^*$ and $A_n\to\mathbb{C}^*$,"I need to find all group homomorphisms of $S_n\to\mathbb{C}^*$ (for $n\ge2$), and also all group homomorphisms of $A_n\to\mathbb{C}^*$ (for $n\ge2$). Someone on this site, two years ago, seems to have already asked the same question ( Finding homomorphisms of $S_n$ → C* and $A_n$ → C* ). I understand what the questioner says there, but I don't understand it when he says: Therefore there is exactly one $g:\mathbb{Z}/2\mathbb{Z}\to\mathbb{C}^*$, so all elements of $a\in\mathbb{C}^*$ give us $\operatorname{ord}(a)\mid g[\mathbb{Z}/2\mathbb{Z}]$, so $a$ can only be $-1$ or $1$. Now we know that $f:S_n\to\{-1,1\}$ … What is he saying there? And how to go on from there? (I do not understand the answer given either …) I hope that someone here can help me further. (And if I'm not allowed to ask this question because is has already been asked two years ago, please tell me what to do instead). Thank you in advance!","['group-homomorphism', 'group-theory']"
2763340,Are global extrema necessarily local extrema?,"Yesterday I asked a question on the existence of local extrema, and have received conflicting comments here . Now from this discussion I'm not sure whether global extrema are necessary local extrema, for a continuous function defined in a metric space. Is this a matter of style / author? or is there a definitive answer? I would appreciate any references too.","['real-analysis', 'calculus', 'functions']"
2763375,A question on density of double cosets in $\mathrm{SL}_2$,"Let $\Gamma$ be a lattice in $G = \text{SL}(2,\mathbb{R})$ and consider the subgroups
$$
N^- := \Bigl\{ \begin{pmatrix}1 & 0 \\ x & 1\end{pmatrix} : x \in \mathbb{R}\Bigr\}
$$
and
$$
A^+ := \Bigl\{ \begin{pmatrix}e^\tau & 0 \\ 0 & e^{-\tau}\end{pmatrix} : \tau  \geq 0\Bigr\}
$$
and
$$
P := \Bigl\{ \begin{pmatrix}* & * \\ 0 & *\end{pmatrix}  \in G\Bigr\}
$$
Then one knows that $U := N^- P$ is dense and open in $G$ and $\Gamma u (A^+)^{-1}$ is dense in $G$ for all $u \in U$. My question is: Given $g \in G$,  can we construct sequences $(a_n)_n$ in $A^+$, $(\gamma_n)_n$ in $\Gamma$ and $(u_n)_n$ in $U$ such that the following holds? $\gamma_nu_n^{-1}a_n^{-1} \to g$ $u_n \to e$ A statement like this is used in Mostow's book Strong Rigidity of Locally Symmetric Spaces in the proof of Lemma 8.5 on page 65 without explanation. However, starting with a sequence $(u_n)_n \to e$, I can not see how to get the sequences $(\gamma_n)_n$ and $(a_n)_n$ in a uniform way. Edit: I have now written down the question with specific groups.","['dynamical-systems', 'general-topology', 'group-theory', 'lie-groups']"
2763381,Infinite sum converging to 2,"How do I compute $$\sum_{r=1}^{\infty} \frac{8r}{4r^4 +1}$$ Calculating first few terms tells me that the sum converges to 2.
I have also tried squeezing the term.","['summation', 'sequences-and-series']"
2763434,Obtaining a positive definite covariance matrix of order statistics,"Suppose $X_1,\dots,X_n$ are independent samples from some distribution with known absolutely continuous CDF $F:\mathbb{R}\rightarrow[0,1]$. Let $X_{(1)},\dots,X_{(n)}$ denote the order statistics, i.e. the ordered sample. Defining the column vector $X_{(\cdot)}=[X_{(1)},\dots,X_{(n)}]'$, we want to numerically calculate $\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']$. The most obvious approach uses the fact that for $j,k\in\{1,\dots,n\}$, $j<k$: $$\mathbb{E}X_{(k)}=\int{x\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$
$$\mathbb{E}X_{(k)}^2=\int{x^2\frac{n! [F(x)]^{k-1}[1-F(x)]^{n-k}}{(k-1)!(n-k)!} dF(x)},$$
$$\mathbb{E}X_{(j)}X_{(k)}=\int{\int{xy\frac{n! [F(x)]^{j-1}[F(y)-F(x)]^{k-1-j}[1-F(y)]^{n-k}}{(j-1)!(k-j-1)!(n-k)!} 1[x\le y] dF(x) } dF(y)},$$ where $1[\cdot]$ is the indicator function. See e.g. Wikipedia here for an informal proof. Using these formulae with standard numerical integration methods works well for small $n$. However, for large $n$ the resulting covariance matrix often ends up non-positive definite unless implausibly many integration nodes are used. This is despite the individual elements of the covariance matrix usually being (loosely) close to a covariance matrix generated via a naïve Monte Carlo approach that guarantees positive definiteness (i.e. draw such a sample of length $n$, then sort it, repeat this lots of times, take the covariance). Is it possible to express these integrals in such a way that the resulting covariance matrix is guaranteed to be positive definite? E.g. is it the case that: $$\mathbb{E}[(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})(X_{(\cdot)}-\mathbb{E}X_{(\cdot)})']=\int{\int{g(u,v) dF(u)}dF(v)},$$ for some function $g:\mathbb{R}^2\rightarrow \mathbb{R}^{n\times n}$ where $g(u,v)$ is positive semi-definite for all $u,v\in\mathbb{R}$.","['positive-definite', 'integration', 'numerical-methods', 'order-statistics', 'random-variables']"
2763469,Limit involving ratios of inverse functions,"Let $H(t)$ be an strictly increasing function such that $\lim_{t\to \infty}H(t)=\infty$. Assuming that $$\lim_{t\to \infty}\frac{H(t)}{\log t}=1$$ I would like to prove or disprove that
$$\lim_{y\to \infty}\frac{H^{-1}(y)}{\exp(y)}=1$$
I have not been able to find a counterexample but also not been able to prove if it is true! Looks to me this has to be true!","['inverse-function', 'calculus', 'limits']"
2763474,Algebraic characterisation of quadrilaterals using angles and edge lengths,"In the following I use $A,B,C,D$ for the vertices and $\alpha,\beta,\gamma,\delta$ for the respective adjacent angles and $a,b,c,d$ for the 'adjacent' (in the sense that $a$ originates from $A$, and so on) edges in a general quadrilateral. I am searching for a simple algebraic characterisation in form of a set of non-redundant equations ($\mathbf{E}$) for the following specific types of quadrilaterals, namely the square ($S$), rectangle ($Re$), parallelogram ($P$), rhombus ($Rh$), trapez *$^1$ ($T$) and kite ($K$) in such a way that the defintion of all of these can be obtained by simply excluding some of the full set of equations which are given for the $S$. What I am seeking differs from 'the common way'. For example in using $\alpha=\beta=\gamma=\delta=\pi/2$ for the square there is no way to 'lower the symmetry' to say a parallelogram by just dropping these but at the same time without introducing new equations. I found a way to do this for all $S,Re,P,Rh,T$ except $K$. This works like the following: $\mathbf{E}=\{$I, II, III, IV, V$\}$ with $$\begin{align} \alpha+\beta=\pi  \tag{I} \\ \alpha+\gamma=\pi  \tag{II} \\ \alpha+\delta=\pi \tag{III}\\ \beta+\gamma=\pi \tag{IV} \\a=b \tag{V} \end{align}$$ With that $S$ is characterized by $\mathbf{E}$ (the whole set of equations I-V exactly define the square). $R$ is defined by $\mathbf{E}\setminus \{V\}$, $Rh$ is $\mathbf{E}\setminus \{II\}$, $P$ is $\mathbf{E}\setminus \{II,V\}$ and $T$ is $\mathbf{E}\setminus \{II,III,IV,V\}$. That looks all very nice, the problem here as mentioned above is $K$, for $K$ we need exactly the condition that one pair of opposite angles is equal ($\alpha=\gamma$) which cannot be expressed by simply removing some equations from $\mathbf{E}$. However I have found that the constraint for a kite could be encoded for example like $$\begin{align} \frac{\beta + \delta}{2} + \alpha = \pi  \tag{Ia} \\  
\frac{\beta + \delta}{2} + \gamma = \pi \tag{IIa} \end{align}$$ which looks somewhat similar to those in $\mathbf{E}$. So now it would be necessary to kind of 'project out' Ia and IIa from $\mathbf{E}$ and then include those into the new equations, but here I get somehow stuck. Can anyone help here? ($\mathbf{E}\setminus \{I,III,IV,V\}$ gives by the way a (general) cyclic quadrilateral ). $^1$) to avoid language confusion I call the quadrilateral with two sides parallel  a trapez * Additional comment I have found meanwhile another way to do the job similarly, but without restricting to conditions on purely angles and edge lengths, as pointed out in  the title . For example one can use the intersection of the diagonals very elegantly to characterise the quadrilaterals by an array of three ratios and an angle, but that would be a different story.","['linear-algebra', 'geometry']"
2763504,Convergence of $a_{n+2} = \sqrt{a_n} + \sqrt{a_{n+1}}$ [duplicate],"This question already has answers here : How to prove this limit exists: $a_{n+2}=\sqrt{a_{n+1}}+\sqrt{a_{n}}$ (5 answers) Convergence of $x_{n+2} = \sqrt{x_{n+1}} + \sqrt{x_{n}}$ (1 answer) Closed 6 years ago . Let  $a_1$ and $a_2$ be positive numbers and suppose that the sequence {$a_n$} is defined recursively by $a_{n+2} = √a_n + √a_{n+1}$. Show that this sequence is convergent. So, I have been able to show the convergence taking three different cases namely, Case 1:  Both $a_1$ , $a_2$ <4, then I proved that the sequence will be monotonically increasing as well as bounded and so converging Case 2:  Both $a_1$, $a_2$ >4, in this case the sequence is monotonically decreasing and bounded and hence convergent. Case 3: One of $a_1$ and $a_2$ is <4 & other >4, lets say, $a_1$<4<$a_2$ in this case sequence will alternatively increase and decrease i.e. $a_{2n-1}$ will be increasing and $a_{2n}$ will be decreasing and $a_{2n}-a_{2n-1}$ will converge to Zero. My question is that is there any general method through which we don't have to take all these cases and can prove the convergence of series in more generality?","['real-analysis', 'cauchy-sequences', 'limits', 'convergence-divergence', 'sequences-and-series']"
2763515,"For $2 \times 2$-matrix, $\|A^2\|=\|A\|^2$ implies that A is normal","I am reading book ""A Hilbert space problem book"", written by Halmos and getting some trouble. In problem 205, he claim that For two-by-two matrices an unpleasant computation proves a strong converse: if $\|A^2\|=\|A\|^2$, then $A$ is normal. I tried to use the determinant as the norm of $A$, but it seems to be useless. I also found a similar question on Mathstackexchange. Someone said that we should only consider matrix $\begin{pmatrix}
\lambda & 1\\ 
0 & \lambda
\end{pmatrix}$. I, however, do not understand why. Could you please give me some hint to solve this claim? Thank you for your time.","['matrices', 'linear-algebra', 'normal-operator']"
2763517,Derivitive of $e^{-x^2}$,"I am currently re-learning high school/university calculus for a project that I am working on. I know that the derivitive of the above function is as shown below:
$$ -2 \cdot x \cdot e^{-x^2} $$ I can get at this solution using the chain rule:
$$ g(x) = f(h(x) $$
$$ g^{'}(x) = f^{'}(h(x))  \cdot  h^{'}(x) $$ However, when I write $ e^{-x^2} $ as
$$ \frac{1}{e^{x^2}} $$ and then apply the quotient rule, I get the following: $$ g(x) = \frac{f(x)}{h(x)} $$
$$ g^{'}(x) = f^{'}(x) \cdot h(x) - f(x) \cdot h^{'}(x) $$ Apply this to my fraction I get:
$$ 0 \cdot e^{x^2} - e^{x^2}  \cdot  2 \cdot x $$
which is
$$ -2 \cdot x \cdot e^{x^2} $$ which is not the correct solution. Am I missing something here? Any help would be appreciated.","['derivatives', 'calculus']"
2763533,What is $A$ in this case?,"Suppose  $A$  is  any  $3×3$  non-singular matrix and $(A−3I)(A−5I)=O$, where $I=I_3$ and $O=O_3$.  If $αA+βA^{−1}=4I$, then $α+β$ is equal to ___ . This seems like a simple and straightforward question, I expanded out the given condition, $$(A−3I)(A−5I)=O \\\implies A^2 -8A +15I = O $$ and then pre-multipied $A^{-1}$ to both sides which  gives: $$A^{-1} \cdot (A^2 -8A +15I )= A^{-1} \cdot O \\\implies A - 8I +15A^{-1} = O \\\text{or}\ \frac{1}2A +\frac{15}{2}A^{-1} = 4I$$ Comparing with what was given in the question,$ \alpha = \frac{1}2$ and $ \beta = \frac{15}2$ which gives the sum to be 8. But then I thought, how is this possible? From the equation $(A−3I)(A−5I)=O$, it seems $A$ can be either $3I$ or $5I$ (definitely not both at the same time), when I substiuted $ A = 3I$ in $αA+βA^{−1}=4I$, I get both α and β in the same equation, if the other possibility is substituted, we get another equation which gives the same result as above when the two are solved simultaneously. But this can't be possible? What is actually the value of $A$ then, since I'm getting such a contradiction?",['matrices']
2763551,"Does $ \lim_\limits {(x,y)\to (0,0 )} \frac{\sin (x) -\sin (y)}{x + y}$ exist?","Does $$\lim_{(x,y)\to (0,0 )} \frac{\sin (x) - \sin (y)}{x + y}$$ exist? It's clear that the $\sin x$ will approach $0$ as $x$ or $y$ is approaching 0. Should I maybe use to find the answer polar coordinates? Where $x = r \cos \varphi$,$y = r \sin \varphi$ and so $r^2=x^2+y^2$.
I would appreciate any kind of help.","['multivariable-calculus', 'calculus']"
2763555,Uniform convergence and improper Riemann integral of the second kind,"It is well  known that the property: If $f_n:A\to \mathbb{R}$ is Riemann integrable for all $n$ and $(f_n)$ converges uniformly to $f:A\to\mathbb{R}$, then $f$ is Riemann integrable. is false in general  when $A$ is of the form $[a,\infty)$. For example : $$
f_n(x) = \left\{
    \begin{array}{ll}
        1/x & \mbox{if } 1\le x\le n\\\
        n/x^2 & \mbox{if }  x\ge  n
    \end{array}
\right.
$$ What about the case $A=(a,b)$?  Can we still  construct a counter-example?","['real-analysis', 'examples-counterexamples', 'uniform-convergence', 'riemann-integration', 'improper-integrals']"
2763578,"Find the cardinal $|P([0,1])\setminus P((0,1))|$","I'm a bit struggling in finding the cardinal size of the set $$|P([0,1])\setminus P((0,1))|$$
Where $P$ is the notation for ""power set"". I was thinking about starting with the fact that $(0,1)\cup\{0,1\}=[0,1]$. From there, I could point out that for each $S\in P((0,1))$, there are $S\cup\{0\}, S\cup\{1\},S\cup\{1,0\} \in P([0,1])$. Intuitively, this means that $B=P([0,1])$ is bigger than $A=P((0,1))$ times 4 (for each in A, there is that one in B, plus 3 more).
I have yet to study the arithmetic of set theory, so claiming that $|P([0,1])\setminus P((0,1))| = 2^\aleph \times 3 = 2^\aleph $ is out scope regarding my calculation. I don't quite know how to move on from here. Moreover, mathematically speaking, I'm a bit confused on how to properly write these claims. As I said, intuitively it seems logical to me that B is 4 times larger than A, but I want to be precise in me writings. Any suggestions are greatly appreciated.",['elementary-set-theory']
2763608,How to get the combinatory of the numbers?,"I have a statement that says: Let the digits be: $1, 2, 3, 4, 5, 6$ So, how many even numbers of
  three figures  different can be formed? So, the number should be like: $\overline{abc}$, where $a$ can be $1, 3 , 5$ that is $3$ options, $b$ can be $2$ options, because an odd number has already been used for $a$ and can not be repeated, and $c$ can be $3$ options, that are {$2, 4, 6$} because it must be a even number. So, they should be formed: $3 * 2 * 3 = 18$, according to the multiplicative principle. But my answer isn't correct, i would like to know why it is not correct, and how it should be done","['combinations', 'statistics', 'probability']"
2763657,Find the derivative of an integral function,"Find the derivative of the function: $$g(x)=\int _{ 1 }^{ \tan(x) }{ e^ {t^ 2}dt } $$ I have the answer as $\sec^2(x)\tan(x).$
Can anyone show the steps of how to solve this problem...","['derivatives', 'integration', 'chain-rule', 'calculus']"
2763670,How to compute the following integral in complex analysis?,"Let $C$ denote the unit circle centered at origin in $\mathbb{C}$. Then $$\frac{1}{2\pi i}\int_C|1+z+z^2|\,dz$$  where the integral is taken anti-clockwise in along $C$, equals what? Well I start with putting $z = e^{i \theta}$, $0 \leq \theta \leq 2\pi$. Then $$\frac{1}{2\pi i}\int_C|1+z+z^2|\,dz = \frac{1}{2\pi i} \int_{0}^{2\pi} \sqrt{3+2\cos(\theta)}\,  e^{i \theta} \,i\, d \theta$$ Am I going in the right manner? How do I proceed further?","['complex-analysis', 'contour-integration']"
2763730,$X_n\leq Y_n+o_p(1)$ or $X_n\geq Y_n+o_p(1)$ imply $X_n$ converges to $Y_n$,"Consider the sequences of real-valued random variables $\{X_n\}_{\forall n \in \mathbb{N}}, \{Y_n\}_{\forall n \in \mathbb{N}}$. A book that I'm reading makes the following claims whose interpretation is not clear to me. Case 1 : $X_n\leq Y_n+o_p(1)$, i.e. $X_n$ converges to $Y_n$. Case 2 : $X_n\geq Y_n+o_p(1)$, i.e. $X_n$ converges to $Y_n$. I guess that $o_p(1)$ is a sequence converging in probability to zero as $n\rightarrow \infty$. I would like your help to clarify firstly what does it mean $X_n\leq Y_n+o_p(1)$ (and, symmetrically, $X_n\geq Y_n+o_p(1)$), why they imply convergence, and which type of convergence.","['probability-theory', 'probability', 'convergence-divergence', 'asymptotics']"
2763732,Probability of getting exam in specific course with 8 courses and two exams,"A person takes 8 courses and is randomly chosen to take an exam in 2 of those courses. All courses have the same probability of being chosen. What is the probability that the person is chosen for a specific course? Me and my friend can't agree which is the correct answer: 1) $\frac{1}{8} + \frac{1}{7}=\frac{15}{56}\approx 26.79\%$ Reasoning : Imagine all the courses are in a bowl. When the exams are drawn, the principal first picks one course from the bowl with 8. Then he picks another course, but now there are only 7 courses to pick from. 2)  $\frac{1}{8}\times 2=\frac{1}{4}=25\%$ Reasoning : If there was just a single exam, the probability of drawing a specific course would be $\frac{1}{8}$. Now there are two exams, so one must simply multiply with the number of exams. 3) Something else entirely We have tried Googleing for a very long time, but couldn't find the answer. Edit: Replaced $\times$ with $+$ in first calculation (typo)","['combinatorics', 'probability-theory', 'probability']"
2763735,Quotient of subgroups,"Is it true that $$\mathbb{Z/4Z\subseteq Z/2Z}$$
Why precisely? Or the reverse $$\mathbb{Z/2Z \subseteq Z/4Z}$$ holds? I'm a beginner. How do I justify the true inclusion? 
How do I visualize $$\mathbb{Z/2Z \subseteq Z/4Z}$$
Thank you very much.","['group-theory', 'elementary-set-theory']"
2763749,$L^1$ convergence implies pointwise a.e. uniform bound of a subsequence,"Is the following statement true ? Let $f_n$ be a sequence of non-negative functions in $L^1(X,m)$, where $(X,\Sigma,\mu)$ is a probability space, such that $\|f_n - f\|_{L^1(X)}\to 0 $ for some $f \in L^1(X)$. Then there exists a subsequence $f_{n_k}$ and a function $g \in L^1(X,\mu)$ such that
$$f_{n_k}(x) \le g(x) \quad \text{for $\mu$-a.e  } x \in X  .$$ I know that there exists a subsequence that converges almost everywhere, but I don't really know how to use this information. Any suggestions?","['functional-analysis', 'real-analysis', 'measure-theory', 'analysis']"
2763804,"Calculating double limit $\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}$","Tried to evaluate $\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}$. I know it suppose to be 1. Used a few different methods (sandwich, playing with the terms, using the formal definition and so on), without success. Would appreciate any help, an hint would be better than full solution. (Note: part of a basic calculus course) Thanks","['multivariable-calculus', 'calculus', 'limits']"
2763828,What is this question about planar graphs describing?,"To preface: I'm not looking for the answer to this question. I just need help understanding what the question is asking. I know a fair bit about planar graphs, but not a ton. ""Every vertex of a finite planar graph is the intersection two
squares, one triangle, and one hexagon. Either show why this
can’t happen, or find how many vertices, edges, and each type
of face are in the graph."" So in this question, what exactly does it mean by ""the intersection of two squares, one triangle, and one hexagon."" I'm envisioning a planar graph that is 10 vertices, 13 edges, and 5 faces (which obeys Euler's formula). I started by drawing a hexagon, then on 2 of its outside edges, I drew squares that share one edge with the hexagon. Then I drew a single edge from the corner of one square to the other to make a triangle. Am I even close?","['graph-theory', 'discrete-mathematics']"
2763860,What is wrong in my answer? Subject: finding the integral of $\cot x$,"$$\int \cot x \,dx=\int \frac{\cos x\sin x}{\sin^2 x}\,dx. $$
Assume $t= \sin^2 x$. Then $dt= 2\sin x\cos x \,dx$.
Using the identity:
$$ \sin 2x =2\sin x\cos x$$ $$\int \cot x \,dx=\int \frac{1}{2t}\,dt=\frac{1}{2}\ln(\sin x)^2+c $$
Can you help me find out what I did wrong? 
Thanks a lot!","['integration', 'trigonometry', 'trigonometric-integrals']"
2763875,What is the derivative of this function: $\frac {d}{dx}x^{\lfloor{x}\rfloor}?$,"What is the derivative of the following function? $$\frac {d}{dx}x^{\lfloor{x}\rfloor}$$
  Here, $\lfloor x \rfloor$ is the floor function. I tried: $$\frac {d}{dx} x^x=\frac {d}{dx} e^{x \ln x}=x^x (\ln x +1)+C$$ But, here $\lfloor{x}\rfloor$ is problematic for me.","['derivatives', 'exponential-function', 'calculus', 'ceiling-and-floor-functions']"
2763900,Can integral domain be trivial? (error in Herstein's Topics in Algebra),"I am studying Ring Theory from Herstein's book ""Topics in algebra"" and I would like to discuss one moment. Definition 1. Let $R$ be a ring. The element $a\neq 0\in R$ is called zero-divisor  if there exists $b\neq 0\in R$ such that $ab=0_R$. Definition 2. An integral domain is the commutative ring which has not zero-divisors. Also it is easy prove the following fact: If $D$ is an integral domain of finite characteristics then characteristics is a prime number. Can trivial ring be an integral domain? If yes then it's characteristics is not prime which contradicts to the above fact! But Herstein does not say nothing about triviality of integral domain. Can anyone clarify this question, please?","['abstract-algebra', 'ring-theory']"
2763908,Making a topological space hausdorff,"Imagine you have a topological space $(M,T)$ which is not Hausdorff. Then, there is a set $S$ of pairs of points $x,y$ so that there are no disjoint neighborhoods of $x$ and $y$. It seems like the relation $x \sim y \leftrightarrow (x,y)\in S$ is an equivalence relation, which meens you can form the quotient space $M/\sim $, and it seems like this space is Hausdorff. Am I right with my conclusion and is there a name for this construction?","['general-topology', 'quotient-spaces']"
2763950,Result comparison of a trigonometric equation,"The Problem : Solve the equation: $$ \cos x=\cos3x+ 2\sin2x
\\$$ The Result : $$ x=k\frac{\pi}{2},k\in \mathbb{Z}$$ My solution: $$ \cos x=4\cos^3x-3\cos{x}+4\sin{x}\cos{x}\\ 4\cos^3x-4\cos{x}+4\sin{x}\cos{x}=0\\ 4\cos{x}(\cos^2{x}+\sin{x}-1)=0\\
\cos{x}(1-\sin^2{x}+\sin{x}-1)=0\\
\cos{x}(\sin{x}-\sin^2{x})=0\\
\cos{x}\sin{x}(1-\sin{x})=0\\
x=\frac{\pi}{2}+l\pi \lor x=m\pi \lor x=\frac{\pi}{2}+2n\pi; \hspace{0.4cm}l,m,n\in \mathbb{Z}$$ Question : Is my solution equal to the result?","['algebra-precalculus', 'trigonometry', 'proof-verification']"
2763995,How to prove the following measure theory result?,"Let $A$ and $B$ be Lebesgue measurable subsets of $(0,1)$ such that $m(A)>1/2$ and $m(B)>1/2$. Prove that there exist $a \in A$ and $b \in B$ such that $a+b=1$. I was doing this by assuming that it doesn't hold then $\forall a \in A$ and $b \in B$ we have either $a+b>1$ or $a+b<1$ then I wasn't able to get some contradiction.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2764018,Proof verification: Diffeomorphism is an isometry if and only if it preserves the arclength of a parametrised curve.,"I am trying to show that a diffeomorphism $\phi : S \rightarrow \overline{S}$ is an isometry if and only if the arc length of any parametrized curve in S is equal to the arc length of the image curve by $\phi$. $""\Rightarrow""$ 
We know that if the diffeomorphism $\phi$ is an isometry, then $$\left< \alpha '(t) , \alpha'(t) \right>_p = \left <d\phi_p(\alpha'(t)), d\phi_p(\alpha'(t)) \right>_{\phi(p)},$$ and since $\lvert \alpha'(t) \rvert = \left< \alpha'(t), \alpha'(t) \right >_p = \left <d\phi_p(\alpha'(t)), d\phi_p(\alpha'(t)) \right>_{\phi(p)}=\lvert d\phi_p (\alpha'(t)) \rvert = \lvert (\phi \circ \alpha)'(t) \rvert$, we conclude that arc length of $\alpha\left(t\right)$, $s(t) = \int_{0}^{t_0} \mid \alpha'(t) \mid dt = \int_{0}^{t_0} \lvert(\phi \circ \alpha)'\left(t\right)\rvert dt$ = length of image curve by $\phi$. $\therefore$ If $\phi$ is an isometry, then the arclength under $\phi$ is preserved. ""$\Leftarrow$""
NTS: If arclength under the map $\phi$ is preserved, then $\phi$ is an isometry. Since arc length of $\alpha$ is preserved under the diffeomorphism $\phi$, $$s(t) = \int_{0}^{t_0} \lvert \alpha'(t) \rvert dt = \int_{0}^{t_0} \lvert (\phi \circ \alpha)'(t) \rvert dt.$$ Differentiating both sides, we get:
$\frac{ds}{dt} = \lvert \alpha'(t) \rvert = \lvert (\phi \circ \alpha)'(t) \rvert$. Hence $$\left< \alpha'(t) , \alpha'(t) \right>_p = \left< d\phi (\alpha'(t), d\phi(\alpha'(t)) \right>_{\phi(p)}.$$ Since $\phi$ is a diffeomorphic map, and the differential $d\phi$ preserves the inner product, we conclude that $\phi : S \rightarrow \overline{S} $ is an isometry. $\therefore$ diffeomorphism $\phi$ is an isometry if and only if the arc length of any parametrised curve in $S$ is equal to the arc length of the image curve by $\phi$. Is this correct? Thanks","['differential-geometry', 'proof-verification']"
2764045,How many non-abelian groups of order $lpq$ are there?,"If $l,p,q$ are primes with $l<p<q$, such that $$p\nmid (q-1)\hspace{1cm} l\mid (p-1)\hspace{1cm} l\mid (q-1) $$ I want to show that there are at least $1$ and at most $(l+1)$
  non-abelian groups of order $lpq$. I already showed (using Sylow theorems) that in this case, if $G$ is a group with order $lpq$ then $$ G \cong C_{pq}\rtimes C_l .$$ The fact that there is at least one non-abelian group I showed the following way: we know that $|\text{Aut}(C_{pq})| = (p-1)(q-1)$, so $l$ divides its order, and as a consequence of the Sylow theorems we know that $C_l\hookrightarrow \text{Aut}(C_{pq})$, thus there is a non-trivial homomorphism which gives us a non-abelian group $C_{pq}\rtimes C_l$. The problem I'm having is showing that $\text{Aut}(C_{pq})$ can have at most $(l+1)$ copies of $C_l$ in it. Any hints?","['finite-groups', 'abstract-algebra', 'group-theory', 'sylow-theory', 'semidirect-product']"
2764071,How many strings of five decimal digits contain exactly three distinct digits?,"Here is my thought process so far: There are $10$ options for the first number, $9$ for the second, and $8$ for the third. According to this current structure, the fourth and fifth digits would have to be one or two of the three already selected digits in some order. If the first three digits were $1, 2, 3$ , then the possible permutations for the fourth and fifth spots are: $11, 22, 33, 12, 21, 13, 31, 23,$ and $32$ . So the answer so far would be $(10 \times 9 \times 8) \times (9) $ Therefore: Per three distinct digits, there are already $9$ options for a string. Additionally, the fourth and fifth spots which contain $1$ or more of the distinct digits for the first repeated occurrence don't have to be at the end. I thought that I should multiply the present answer by $(5$ choose $3) = 10$ to account for this additional variation in order, but I can tell that there must be some number of strings which are counted twice by this method, so $(10 \times 9 \times 8) \times (9) \times (10) = 64800$ is probably too great. Any suggestions? Thank you","['combinations', 'combinatorics', 'discrete-mathematics']"
2764089,Distributions and Probability Theory,Hi I am studying an advanced analysis course where we have been introduced to distributions. In our case these are linear functionals on the $L_{1}^{loc}$ space. I was wondering how these distributions are related to probability distributions or do they just share the same name?,"['functional-analysis', 'distribution-theory', 'probability-distributions']"
2764108,"Prime ideals in $\mathbb{C}[f(t),g(t)]$, where $f(t),g(t) \in \mathbb{C}[t]$.","Given two (monic) polynomials $f=f(t),g=g(t) \in \mathbb{C}[t]$, 
consider the ring $\mathbb{C}[f,g]$. Is it possible to describe all prime ideals and maximal ideals in 
  $\mathbb{C}[f,g]$? Of course, if $P$ is a prime ideal in $\mathbb{C}[f,g]$, then $\mathbb{C}[f,g]/P$ is an integral domain, and if $M$ is a maximal ideal in $\mathbb{C}[f,g]$, then $\mathbb{C}[f,g]/M$ is a field.
(Also, though I am not sure if this helps, $\mathbb{C}[f,g]$ may not be a principal ideal domain ). In particular, what is the answer for $f(t)=t^2$ and $g(t)=t^3$? This question is perhaps relevant. Edit: See also this question and this paper (maybe they are relevant). Thank you very much for any hints and comments!","['polynomials', 'algebraic-geometry', 'abstract-algebra', 'ring-theory', 'commutative-algebra']"
2764134,Random variables with rapidly varying tails,"I have been trying to prove the following: Consider an i.i.d. sample of random variables $\left\{ X_n \right\}$. Their distribution $F$ is said to satisfy the rapidly varying tail condition , if
  $\forall a > 1$ the following holds:
  $$
\lim_{x \to \infty} \frac{1 - F(ax)}{1 - F(x)} = 0
$$
  Show that if $\displaystyle \frac{\max_{i=1,\dots,n} X_i}{b_n} \overset{\mathbb{P}}{\to} 1$ (convergence in probability), for a sequence
  $b_n \to \infty$, then $F$ must satisfy the rapidly varying tail condition. I consider the definition of convergence in probability,
$$
   \lim_{n \to \infty} \left(
   \left|\frac{\max_{i=1, \dots, n}(X_1, \dots, X_n}{n} - 1 \right| > \varepsilon
\right) = 0
$$
and write (for the positive part): \begin{align*}
    \mathbb{P}\left(\max_{i=1,\dots,n} X_i > (1 + \varepsilon) b_n\right) &= 
    \mathbb{P}\left[ \left(
    \bigcap_1^n X_i \leq (1 + \varepsilon) b_n \right)^c \right] \\
&= \mathbb{P}\left(
    \bigcup_1^n X_i > (1 + \varepsilon) b_n \right) =
   n \mathbb{P}(X_1 > (1 + \varepsilon) b_n) \\
\end{align*}
For the negative part, I know that
\begin{align*}
    \mathbb{P}\left(\max_{i=1,\dots,n} X_i < (1 - \varepsilon) b_n\right) &= 
    \mathbb{P}\left[
    \bigcap_1^n X_i < (1 - \varepsilon) b_n \right] \\
&= 1 - n\mathbb{P}(X_i \geq (1 - \varepsilon)b_n)
\end{align*}
Taking limits in the positive and negative parts and equating them as they both have to be equal to 0 obtain
$$
\lim_{n \to \infty} n \mathbb{P}(X_1 > (1 + \varepsilon) b_n) = 0, \quad
\lim_{n \to \infty} n \mathbb{P}(X_1 \geq (1 - \varepsilon) b_n) = 1
$$
However, at this point I'm stuck. I can obviously deduce that
$$
\lim_{n \to \infty} \frac{1 - F(az_n)}{1 - F(z_n)} = 0, \quad
a := \frac{1}{1-\varepsilon} > 1, \; z_n = (1 - \varepsilon) b_n
$$
however this does not exactly imply the rapidly varying tail condition, which must hold for arbitrary $x \to \infty$.
Any ideas?","['probability-distributions', 'weak-convergence', 'statistics', 'probability', 'convergence-divergence']"
2764141,Let $f$ be entire. Assume that the function $g(z) = f(z)f(\frac{1}{z})$ is bounded on $\mathbb{C}-\{0\}$. Show that $f(z)=cz^m$,"This is what I have tried so far: Since $g(z)$ is bounded, then $\lim\limits_{z\rightarrow 0} zg(z)=0$ and hence $z=0$ is a removable singularity of $g(z)$. We can define $g(0) = \lim\limits_{z\rightarrow 0} f(z)f(\frac{1}{z})$ and make $g$ entire. Then $g(z)$ is a bounded entire function and hence $g$ is a constant function. In other words, $f(z)f(\frac{1}{z}) = c$ for some $c\in\mathbb{C}$ and for $z\neq 0$ I don't know how to continue from this step.
I tried to prove that $f(\frac{1}{z})$ has either a pole or a removable singularity at $z=0$ to show first that $f(z)$ is a polynomial or a constant but I failed.",['complex-analysis']
2764158,Determining the equivalence classes of a given relation,"I would like help with the specific problem: We take the set $C$ to be $C={1, 2, 3, 4}$ and the relation $\sim$ over $\mathcal{P}(\mathbb{N})$ defined as $A\sim B$ iff $A \cap C = B \cap C$ a) Prove that $\sim$ is an equivalence relation. b) Find the equivalence classes of $\emptyset$ , of $\{1\}$ and of $\{1, 8\}$ . c) How many equivalence classes are there? The first part of the problem I was able to resolve fairly easily. A~A is trivially true, so the conditions of reflexivity are met, if A~B then B~A is also easy to prove, and (I took D instead of C for transitivity because C is used in the exercise) if A~B and B~D, then A~D. My proofs for these statements were all quite simple because the relationship is an equality. Hopefully I didn't make any wrong assumptions there. to give an idea, for transitivity, I wrote: A~B: $A\cap C$ = $B\cap C$ B~D: $B\cap C$ = $D\cap C$ Then $A\cap C$ = $B\cap C$ = $D\cap C$ , thus $A\cap C$ = $D\cap C$ , thus A~D. Now, I'm rather stuck on the second and third parts of the problem. I don't know how to see the equivalence relations here.","['equivalence-relations', 'discrete-mathematics']"
2764172,A corollary of Hahn–Banach theorem and a generalized limit function of $\ell_\infty$,"A corollary of Hahn–Banach theorem states that Let $E$ be a normed vector space, $M$ a proper closed subspace and $x \in E$. If $d(x,M) = \delta > 0$, so exists $f \in E'$ such that $\|f\|=1$, $f(x)=\delta$ and $f(m)=0$  $\forall m \in M $. Consider $T: \ell_\infty \rightarrow \ell_\infty$ a bounded linear operator defined by $$T(x_1,x_2,x_3,\dots) = (x_2,x_3,\dots).$$ Let $M=\{ x-T(x) : x \in \ell_\infty\}$, so $M$ is a subspace of $\ell_\infty$. If $e=(1,1,1,\dots) \in \ell_\infty$, so $d(e,M)=1>0$. Then, applying the corollary above in $\overline{M} $, exists $f \in \ell_\infty'$ such that $\|f\|=1$, $f(e)=d(x, \overline{M}) = d(e, M) =1$ and $f(x)=0~\forall x \in M\subset \overline{M} $. I was able to show that $$f(x_1,x_2,x_3,\dots) = f(x_2,x_3,\dots) ~~\forall (x_n) \in \ell_\infty.$$ Besides that, we have that $\forall x = (x_n) \in c = \{ (x_n) \in \ell_\infty : x_n \text{ is convergent} \}$ $$f(x) = \lim_{n \rightarrow \infty} x_n$$ Now, let $x=(x_n), y=(y_n) \in \ell_\infty$ such that $x_n \geq y_n$ $\forall n \in \mathbb{N}$. How can I show that $f(x) \geq f(y)$?",['functional-analysis']
2764174,How to solve maximum likelihood estimates with inequality constraint?,"For example, let $ r_i \sim \operatorname{Binomial} (n, p_i)$, where $i \in [1,m]$. Assume that $ r_i $ are independent and also, put an inequality, $ p_1 < p_2 < \cdots < p_m $. How to find the maximum likelihood estimates of $ p_1, \ldots,p_m$ in such setting? I know the joint likelihood can be written as, $$ \sum_{i=1}^m \big(r_{i} \log p_{i} + (n-r_{i})\log(1-p_{i})\big)+ \text{constant}.$$ But how can I continue to derive the MLE with the constraint?","['statistical-inference', 'probability-theory', 'maximum-likelihood', 'statistics', 'probability']"
2764207,Points of affine scheme and points of topos,"Let $X =$Spec$(R)$ be an affine scheme and $\mathscr{E}_{X}$ denote the topos of sheaves of sets on $X$. Is it true that the (geometric) points of $\mathscr{E}_{X}$ are in bijection with the prime ideals of Spec$(R)$? That is, that the points of this topos are in bijection with the points of the scheme $X$? To each prime ideal $p \in $Spec$(R)$, one can form a morphism to $\varphi: R \rightarrow \overline{k}_{p}$ from $R$ to an algebraically closed field. This gives a geometric morphism from sets to $\mathscr{E}_{X}$ i.e. a point of the topos $\mathscr{E}_{X}$. However, I am unsure whether or not each such point of the topos must be of this form. If anyone can shed any light on this, or just point me to a reference where this is discussed, that would be great. Thanks. Edit: I have conflated two ideas simply because they are both called points --- This could just be a coincidence.","['topos-theory', 'algebraic-geometry']"
2764236,Perturbation evolution of a differential equation,"Let $a$, $b$ be two real positive parameters with $a>b$, and consider the following nonlinear differential equation:
\begin{align}
\dot{x}_{\varepsilon}(t) = a - b\sin(x_{\varepsilon}(t))+\varepsilon, \quad x_\varepsilon(0)\in\mathbb{R},
\end{align}
where $\varepsilon$ is a real constant.
Let us define 
$$
\Delta(t,\varepsilon):= |\cos({x}_{\varepsilon}(t))-\cos({x}_{0}(t))|
$$
Clearly, note that $\Delta(t,0)\equiv 0$. Is it true that $\Delta(t,\varepsilon)$ is linearly bounded in $t\ge 0$ and $|\varepsilon|$, that is
  $$
 \Delta(t,\varepsilon)\le K |\varepsilon| t,
$$
  where $K$ being a suitable constant? [In case the answer is no, is it possible to find upper bound such that $\Delta(t,\varepsilon)\le K |\varepsilon| p(t)$ where $p(t)$ is a polynomial function of $t$?] Numerical simulations seem to confirm this claim. Any help towards a theoretical confirmation (or rejection) of this claim is more than welcome! Many thanks! An initial (incomplete) attempt. We can rewrite $\Delta(t,\varepsilon)$ as
$$
\Delta(t,\varepsilon)= 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\sin\left(\frac{{x}_{\varepsilon}(t)+{x}_{0}(t)}{2}\right)\right|,
$$
so that 
$$
\Delta(t,\varepsilon)\le 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\right|\le |{x}_{\varepsilon}(t)-{x}_{0}(t)|.
$$
So now the problem is perhaps a bit simplified (?) and boils down to find an upper bound to $|{x}_{\varepsilon}(t)-{x}_{0}(t)|$. However, I don't know how to proceed from here.","['inequality', 'perturbation-theory', 'stability-in-odes', 'ordinary-differential-equations', 'analysis']"
2764319,Showing that $\sum\limits_{l=0}^\infty \frac{1}{(3l+2)!}=\frac{1}{3}(e-\frac{2}{\sqrt{e}} \cos(\frac{\pi}{3}-\frac{\sqrt{3}}{2}))$,"It is known that $\frac{1}{2\pi i} \int_C \frac{e^z}{z^3-1}dz=\sum\limits_{l=0}^\infty \frac{1}{(3l+2)!}$. Now I will like to evaluate $\frac{1}{2\pi i} \int_C \frac{e^z}{z^3-1}dz$ using Cauchy's Integral Formula, to show that $$\sum\limits_{l=0}^\infty \frac{1}{(3l+2)!}=\frac{1}{3}(e-\frac{2}{\sqrt{e}} \cos(\frac{\pi}{3}-\frac{\sqrt{3}}{2}))$$ There is a hint where we can assume the identity $$\prod\limits_{j=1,j\neq k}^m =(e^{2\pi ij/m}-e^{2\pi ik/m})=\frac{m}{e^{2\pi ik/m}}$$ I managed the following: $$\frac{1}{2\pi i} \int_C \frac{e^z}{z^3-1}dz=(\frac{e^z}{z+1})'|_{z=1}=\frac{e}{4}$$ by Cauchy's Integral Formula for higher derivatives, which is clearly not the desired solution. Where did I go wrong in my working? I feel like I may have a conceptual error of some sort..","['cauchy-integral-formula', 'complex-analysis', 'complex-integration']"
2764354,"Rings in which the ""mistake"" $(x + y) / d = x / d + y$ is justified","A common mistake by students in introductory algebra is (essentially) the following: $$\frac{x + y}{d} = \frac{x}{d} + y.$$ (See: ""Cancelling Everything in Sight."" ) I want to justify this mistake. The students are working in a field. Generalizing the situation to rings, we might write their mistake as $(x + y)d = xd + y$. This is equivalent to $yd = y$, hence the question: In what rings does $yd = y$ for all elements $y$ and $d$ with $d \neq 0$?","['algebra-precalculus', 'abstract-algebra', 'ring-theory']"
2764369,de Rham isomorphism,"Let $X$ be an open manifold, with one end $N$, Q How to show that $H^1_{c,dR}(X)\to H^1_{dR}(X)$ is an injective,?
here $H^1_{dR}$ denotes the de Rham cohomology and $H^1_{c,dR}$ denotes the compactly supported de Rham cohomology. Suppose that $H^1(N;\mathbb R)=0$. Q How to show that $H^1_{c,dR}(X)\cong H^1_{dR}(X)$?","['de-rham-cohomology', 'differential-geometry']"
2764398,In conditional probability why does conditioning on a specific one yield a very different answer to conditioning on at least one,"I was watching the Harvard stat110 course and there was this question pick a random 2 card hand from a standard deck. what is the probability that it's 2 ace given you have one ace what is the probability that it's 2 ace given you have ace of spade the result for Q1 is $\frac{1}{33}$ and the result for Q2 is $\frac{1}{17}$ in the lecture he mentioned a hint that it's because one is dealing with at least one while the other is a specific one, but I can't get my head around the intuition behind this idea.","['statistics', 'probability']"
2764424,Initial value problem for second order linear differential equation : why am I only getting zero as a solution?,"I am given the following ODE : $y'' - 2ay'+by = 0$ for some real constants $a,b$, along with the initial conditions $y(0) = y(1) = 0$. Furthermore I know that some function $y$ is a solution of this ODE. How do I show that $y(n) = 0$ for all natural numbers $n$? The idea would be to solve the equation : it is a second order linear differential equation. Hence, (skipping rigour) we solve $r^2 - 2ar + b = 0$ to get solutions $r_1,r_2$ which may or may not be equal (and may be complex). The solution is now given by : 1 : $y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}$ if $r_1 \neq r_2$ (note that if they are complex then by Euler's formula we have a linear combination of trigonometric functions coming in) 2 : $y(t) = c_1 e^{r_1t} + c_2te^{r_1}t$ if $r_1 = r_2$. In case 1 , substituting $y(0) = 0$ gives $c_1 + c_2 = 0$ and combining with $y(1)=0$ gives $e^{r_1} = e^{r_2}$ and $c_1 = -c_2$, so $\color{red}{e^{r_1 t} = e^{r_2 t} \mbox{ for all } t}$ hence the solution is identically zero. In case 2, substituting $y(0) = 0$ gives $c_1 = 0$  and then $y(1) = 0$ gives $c_2 = 0$ so the solution is identically zero. There is definitely something wrong here. I'd like people to point it out, since I think I've blindly used the formula given to me here. This function is supposed to be non-zero at least at the integers, and the fact that no more is provided hints that there are non-trivial solutions. EDIT : I found the error thanks to the great people below. You may find the error colored in red above.",['ordinary-differential-equations']
2764443,"For a random sample from the distribution $f(x)=e^{-(x-\theta)} , x>\theta$ , show that $2n[X_{(1)}-\theta]\sim\chi^2_{2}$","Show that for a random sample of size $n$ from the distribution $f(x)=e^{-(x-\theta)} , x>\theta$ , $2n[X_{(1)}-\theta] \sim \chi^2_{2}$ distribution and $2\sum_{i=2}^{n}[X_{(i)}-X_{(1)}]$ also has the $\chi^2_{2n-2}$ distribution and is independent of the first statistic.
  Here, $X_{(i)}$ is defined as the $i$ th order statistic. My approach: I did the following series of transformations:
$(X_1,X_2,..,X_n) \rightarrow (Y_1,Y_2,...,Y_n) \rightarrow (Y_{(1)},Y_{(2)},...,Y_{(n)}) \rightarrow (U_1,U_2,...U_n)$ where $Y_i=X_i-\theta$ , $U_1=2nY_{(1)}$ and $U_{i}=2(Y_{(i)}-Y_{(1)}) \ \text{for i =2,3,...n}$ SO, first the joint pdf of $X_1,X_2,...X_n$ is given by $f(x_1,x_2,...x_n)=e^{-\sum_{i=1}^{n}(x_i-\theta)} I_{x_i > \theta}$ Again, you can see $f(y_1,y_2,..,y_n)=e^{-\sum y_i} I_{y_i>0}$
Now, the joint pdf of order statistics $f_{1,2,...n}(y_1,..y_n)=n!e^{-\sum y_i} I_{y_1<y_2<...<y_n}$
Now transforming to $U$, the jacobian of transformation comes to be $\frac{1}{n2^n}$
Thus, $f(u_1,u_2,..u_n)=\frac{(n-1)!}{2^n}e^{\frac{-\sum u_i}{2}}$
From here I can deduce $u_1 \sim \chi^2_{2}$ But I cannot deduce anything from the remaining.
Help!","['probability-theory', 'probability-distributions', 'statistics', 'probability', 'order-statistics']"
2764451,$Ax=b$ has a solution over $\Bbb{F}_p$ for every prime $p~\implies$ existence of real solution??,"$A$ be an $m\times n$ matrix and $b$ a $m\times1$ vector, both with integer entries. If $Ax=b$ has a solution over $\Bbb{F}_p$ for every prime $p$, is a real solution guaranteed? I couldn't think of any easy way to begin this problem. Any hint please. Bonus question: A matrix with all diagonal elements odd integers and all non diagonal elements even integers, is the identity matrix in $\Bbb{F}_2$. Does that imply it is invertible as a real matrix?","['matrices', 'linear-algebra']"
2764458,Proving $f'(0)=0$ if $\lim_{x\to0}f(x)/x^2$ exists and is finite,"Edit: question says the function is differentiable Given $\lim_{x\to0}f(x)/x^2$ exists and is finite prove $f'(0)=0$ My attempt: $$f(x)=\sum_{k=0}^\infty {f^{n}(0)x^n\over n!}\\
\implies \lim_{x\to0}f(x)/x^2=\frac{f''(0)}2+\lim_{x\to0}\left({f(0)\over x^2}+{f'(0)\over x}\right)$$ How to proceed without l'hopital? This also assumes function is infnitely differntiable. How to avoid that? Is the argument that if $f'(0)\ne0$ then $\lim_{x\to0}\left({f'(0)\over x}\right)=\infty$ valid?","['limits-without-lhopital', 'limits']"
2764464,Disproving the existence of a specific infinite sequence of Fibonacci primes,"Consider the following sequence: $$ T_{1} = a,\: T_{i+1} = F_{T_{i}} $$ where $ a \in \mathbb{P} $ and $F_i$ is the $i$-th Fibonacci number. Is there a value of $a \neq 5$ such that this sequence generates only primes? I certainly don't expect an answer in the positive—as it would prove that there are infinitely many Fibonacci primes, which is an open problem—but I would like to know if there are any straightforward/obvious reasons for which such an $a$ wouldn't exist.","['fibonacci-numbers', 'prime-numbers', 'sequences-and-series', 'elementary-number-theory']"
2764486,Prove the maximum of two integrals of functions is less than or equal to the integral of the maximum of two functions,"Prove that on an interval $[a,b]$, if $f$ and $g$ are integrable, than $\max(f,g)$ is integrable and 
$$\max(\int_a^b f(x)dx, \int_a^b g(x)dx) \leq \int_a^b \max(f(x), g(x))dx. $$ I proved the first part by
$$\max(a,b)=\frac{a+b+|a-b|}{2}$$
so
$$\max(f(x),g(x))=\frac{f(x)+g(x)+|f(x)-g(x)|}{2}$$
The absolute value, the sum, and the difference, and the multiplication by a constant of Riemann integrable functions are all Riemann integrable, hence the max is also integrable. Can anyone help with the second part? Many thanks!","['real-analysis', 'integration', 'riemann-integration']"
2764488,How to find $\lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n}$?,"How to find $$
\lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n}?
$$ I was thinking about the Riemann sum, but I am not able to do it.","['real-analysis', 'sequences-and-series', 'calculus', 'limits']"
2764493,Compute $P(T< \infty)$,"Let $S_n = \xi_1 + \cdots + \xi_n$ be a random walk and suppose that $\phi(\theta_o) = \mathbb{E}\exp{\theta_o \xi_1} = 1$ for some $\theta_o <0$ and $\xi_i$ is not constant. Suppose $\xi_i$ are integer valued with $P(\xi_i < -1) = 0$, $P(\xi_i = -1) > 0 $ and $\mathbb{E}\xi_i > 0 $. Let $ T = \inf\{n: S_n = a\}$ with $a<0$. Use the martingale $X_n = \exp\{\theta_o S_n\}$ to conclude that $P(T< \infty) = \exp\{-\theta_o a\}$. My attempt until now : Since $X_n$ is a martingale, $X_{n \wedge T}$ is also a martingale. Thus, $\mathbb{E}[X_{n \wedge T}] = 1$. Next, $\mathbb{E}[\exp\{\theta_o (S_{n \wedge T} - a)\}]=\exp\{-\theta_0 a\}$ $\exp\{-\theta_0 a\} = \mathbb{E}[\mathbb{1}_{T<\infty}\exp\{\theta_o (S_{n \wedge T} - a)\}] + \mathbb{E}[\mathbb{1}_{T=\infty}\exp\{\theta_o (S_{n \wedge T} - a)\}] $ The first term in the right hand term goes to $\mathbb{P}(T<\infty)$ by the dominated convergence theorem since $\mathbb{1}_{T<\infty}\exp\{\theta_o (S_{n \wedge T} - a)\}$ goes to $\mathbb{1}_{T<\infty}\exp\{\theta_o (S_{T} - a)\} = \mathbb{1}_{T<\infty}$ almost surely and $\mathbb{1}_{T<\infty}\exp\{\theta_o (S_{n \wedge T} - a)\} < 1$ by def of $T$. To conclude I would like to show that $\mathbb{E}[\mathbb{1}_{T=\infty}\exp\{\theta_o (S_{n \wedge T} - a)\}] = \mathbb{E}[\mathbb{1}_{T=\infty}\exp\{\theta_o (S_{n} - a)\}]$ goes to zero as $n$ goes go infinity but I don't know how to proceede. Do you see any other approach to conclude the exercise ?","['random-walk', 'probability-theory', 'exponential-function', 'stopping-times', 'martingales']"
2764505,Outer and Inner expansions of odes,"I have been asked to determine the first two terms of the outer and inner expansions  of the ODE
 $$\epsilon \frac{d^2y}{dx^2}+\frac{dy}{dx}+\frac{1}{1+x}y=0$$ on $0<x<1$ with $0 < \epsilon \ll 1$ with boundary conditions $y(1)=1, y(0)=0$. I've worked out the boundary layer is at $x=0$ but I cant get the right answer even for the outer perturbation. I've been told to maybe Taylor expand it but I still can't seem to get it using the expansion $y=y_0+\epsilon y_1$. The solution is meant to be $y_0=\frac{2}{1 + x}$ which I can get and then I get lost getting $$y_1=\frac{-2}{1+x}+\frac{4}{(1+x)^2}$$ Can anyone see how you can get to this? Using the expansion $y=y_0+\epsilon y_1$ I have got the equation to be $\epsilon 
 y_0'' + y_0' + \epsilon y_1' + \frac{1}{1+x} y_0 + \frac{1}{1+x} \epsilon y_1=0$. The peeling off order one I have the equation $y_0'+ \frac{1}{1+x} y_0=0$, which I solved using the integrating factor to get $y_0=\frac{A}{1 + x}$ then used the BC at $x=1$ to get $A=2$. Next is when I got problems getting the answer as I got order $\epsilon$ to be $y_0''+y_1'+\frac{1}{1+x} y_1=0$. I got $y_0''=\frac{4}{(1+x)^3}$, so the equation becomes $y_1'+\frac{1}{1+x} y_1=\frac{-4}{(1+x)^3}$ but this is where I am struggling?","['boundary-value-problem', 'taylor-expansion', 'ordinary-differential-equations', 'mathematical-physics']"
2764507,Solving a constrained inhomogeneous first order ODE.,"Suppose that $A$ is a symmetric $n \times n$ matrix and $b \in \mathbf{R}^n$. I want to solve the following ODE, 
$$
\dot{x} + 2Ax +2b = 0,
$$
with $x: \mathbf{R}_+ \to \mathbf{S}^{n-1}$, so the domain is the nonnegative reals and codomain is the the unit sphere. Is this possible with the implicit constraints on $x$? Are there good references that explain how to solve this type of ODE numerically and analyze it mathematically (i.e., stable equilibrium, etc.)?","['reference-request', 'numerical-methods', 'ordinary-differential-equations']"
2764514,Probability of equally likely events,"I know that the probabilities of 2 equally-likely events is 0.5. Due to the fact that if i repeat the experiment for $N\rightarrow \infty$ the frequences of the 2 events are the same. But, going to a deeper level, why this happens ?","['statistics', 'probability']"
2764531,Show that $\sum\limits_{n=0}^{\infty} \binom{3n}{n} (\frac{4}{125})^n=\frac{1}{2\pi i}\int_{C(0;1)}\frac{-125}{(z-4)(4z^2+28z-1)}dz$,"Define $C(0;1)$ to be the circle with center $0$ and radius $1$ traversed in the counterclockwise direction. Show that 
  $$\sum\limits_{n=0}^{\infty} \binom{3n}{n} \left(\frac{4}{125}\right)^n=\frac{1}{2\pi i}\int_{C(0;1)}\frac{-125}{(z-4)(4z^2+28z-1)}dz \tag{1}$$ I know that $$4z^2+28z-1=4\left(z-\frac{-7+5\sqrt 2}{2}\right)\left(z-\frac{-7-5\sqrt 2}{2}\right)$$ and I managed to show that $$\frac{1}{2\pi i}\int_{C(0;1)}\frac{-125}{(z-4)(4z^2+28z-1)}dz=\frac{15\sqrt 2}{28} +\frac{5}{14}$$ by Cauchy's Integral Formula where $z=\frac{-7+5\sqrt 2}{2}$ is the only singularity in $C(0;1)$. Now my question is, is there a way to show $(1)$ by Laurent series expansion along with the binomial identity? I am clueless as to how I can proceed.","['cauchy-integral-formula', 'complex-analysis', 'binomial-theorem', 'complex-integration']"
2764540,Find a particular integral for $y''+y'+y = 2+x+\cos(x)$,"If it were merely $y''+y'+y = \cos(x)$, then I know that I would try $a\cos(kx) + b\sin(kx)$, but with the added $2 + x$, I am not entirely sure.",['ordinary-differential-equations']
2764593,Inequality involving supremum and integral,"Suppose that f is a real positive and integrable function, I'd like to know if the following holds $\sup_{t \in [0,1]} \int_\mathbb{R} f(x,t) \: dx \leq \int_\mathbb{R} \sup_{t \in [0,1]} f(x,t) \: dx$. Is there some reference?
Thanks","['real-analysis', 'integration', 'supremum-and-infimum', 'functions']"
2764652,Bounded sequence $\{a_n\}_n$ such that $a_n < \frac{a_{n−1} + a_{n+1}}{2}$. Is $\{a_n\}_n$ convergent?,"Let $\{a_n\}_n$ be a sequence of numbers in the interval $(0, 1)$ with the property that
  $$a_n < \frac{a_{n−1} + a_{n+1}}{2}$$
  for all $n = 2, 3, 4,\dots$. Show that this sequence is convergent. My attempt: We can write the inequality as $a_n - a_{n-1} < a_{n+1} - a_n$ So, sequence {$s_n$} = $a_{n+1} - a_n$ is monotonic and since -1<$s_n$<1 , it is also bounded and hence convergent. Sequence {$a_n$} is bounded and by Bolzano-Weierstrass property has a convergent subsequence {$a_{n_k}$}. Applying Cauchy sequence property on this convergent subsequence, we have for every $\epsilon$ >0, there is $N_0$, such that $|a_{n_l} - a_{n_k}|< \epsilon$ for all $l>k>N_0$ I feel like, from here I should have been able to prove this, but unfortunately I am stuck. Please help.","['real-analysis', 'cauchy-sequences', 'sequences-and-series', 'convergence-divergence']"
