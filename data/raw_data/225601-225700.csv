question_id,title,body,tags
4653373,"Measuring angle based on optic fibre extension, tricky geometry problem","For an experiment, I am trying to make use of an optic fibre to measure the angle of a mirror along the vertical axis. Simplifying to a two-dimensional slice, this mirror (line $S$ ) is connected to a fixed pivot on its top end, allowing free movement (between $-90^o$ and $+90^o$ degrees). The other bottom end is connected to optic fibre $l$ and another fixed pivot on the same vertical line as the first. Let the angle $\alpha$ be defined as the angle between $S$ and the vertical axis. For $\alpha = 0$ , let the length of the optic fibre $l$ be $L$ . For any other angle $\alpha$ , let the length be $L+\Delta L$ , with varying $\Delta L$ for varying $\alpha$ . I wish to express this angle $\alpha$ in terms of $L, S$ and $\Delta L$ . Please see the image below for a diagram I created in Geogebra. Here, I show circle segments for the movement of $S$ and $L$ with the extension of $\Delta L$ up until it connects to $S$ . Some guidelines with horizontal perpendicular lines are drawn. I have crunched a lot of trigonometry rules, Pythagoras, making use of areas and the composition of the triangles with guidelines, congruency and positioning it on a graph using functions, but I cannot seem to find an expression for $\alpha$ without any other variables than $S, L$ and $\Delta L$ . Would anyone be able to help? Thanks a lot!","['trigonometry', 'geometry']"
4653384,Establishing a simple upper bound (with one less parameter) to an unwieldy function with non-negative discrete parameters,"I have this function at hand with 3 non-negative integer parameters that I would like to put a bound to: $$f(n, b, d) = 1 - \left(1 - \left(\frac1n\right)^{b+d-1}\right)^{\left(n^{b-1}\right)}$$ My end goal is to show that for any positive $\varepsilon_0$ there is a $n_0$ and $d^*$ such that for all $n \ge n_0$ and $b > 0$ , $f(n, b, d^*) < \varepsilon_0$ . In simpler terms, I want to show that $f$ can be made arbitrarily small for any $n$ larger than a constant by affixing $d$ to be another large-enough constant. To do this, I am looking for a decreasing function $g(n, d)$ which satisfies $f(n, b, d) \le g(n, d)$ for all $b$ . Playing around with $f$ in Desmos's graphing calculator, it seems that $g(n, d) = \left(\frac{2}{d}\right)^d$ is such a function and seems very tight. link to graph I don't need it to be tight. I can make do with any other decreasing function that allows me to reach my goal. (I can even do away with the bounding function altogether so long as I can reach my goal, but I still am curious about how one can bound a function like $f$ .)","['multivariable-calculus', 'upper-lower-bounds', 'discrete-calculus']"
4653390,how to prove a torus in $\mathbb{R}^3$ is a smooth manifold,"I want to prove that $T^2=\{(x,y,z)\in\mathbb{R}^3:(\sqrt{x^2+y^2}-R)^2+z^2=r^2\}\ (r<R)$ is a smooth manifold. Since I am a student in physics and haven't learned much math, so I want to prove it by constructing an atlas with smooth transition functions. I tried taking something like $U_1=T^2\backslash\{(R\cos\theta,R\sin\theta,r):\theta\in[0,2\pi)\}\cup\{(R+r\cos\theta,0,R+r\sin\theta):\theta\in[0,2\pi)\}$ (namely a torus deleting the circle on the top and the circle in half plane $y=0,x>0$ ) and $\phi_1:U_1\to(0,2\pi)\times(0,2\pi):(R\cos\theta+r\cos\theta\sin\phi,R\sin\theta+r\sin\theta\sin\phi,r\cos\phi)\to(\theta,\phi)$ . Then I found that it seems I need to construct 2 more similar charts to get an atlas. Am I correct? Can we construct it more elegantly?","['submanifold', 'smooth-manifolds', 'differential-geometry']"
4653409,Does a connection exist between the roots of $\mathrm{Si}\left(\frac{z}{2}\right)$ and the roots of its derivative?,"From the answer to this question about the complex roots of $\mathrm{Si}\left(\frac{z}{2}\right)$ , we now know that the asymptotic of the $k$ -th complex root $z_k = x_k + i y_k $ is: \begin{align}
x_k &\approx 4\pi k - \frac{{\log k}}{{\pi k}} \\
\\
y_k &\approx 2\log x_k + 2\log (\pi /2)
\end{align} The Hadamard product for $\mathrm{Si}\left(\frac{z}{2}\right)$ is: $$\mathrm{Si}\left(\frac{z}{2}\right) = \frac{z}{2}\,\prod_{z_k} \left(1-\frac{z}{z_k} \right) \tag{1}$$ with $z_k$ taken as quadruples $z_k, -z_k, \overline{z_k}, -\overline{z_k}$ . The logarithmic derivative of the RHS of (1) then becomes: $$\frac{\mathrm{Si}'}{\mathrm{Si}}\left(\frac{z}{2}\right) = \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\tag{2}$$ hence, $$\mathrm{Si}'\left(\frac{z}{2}\right) = \frac{\sin\left(\frac{z}{2}\right)}{z} =\mathrm{Si}\left(\frac{z}{2}\right)\,\left( \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\right)\tag{3}$$ So, the regular real roots of $\displaystyle \frac{\sin\left(\frac{z}{2}\right)}{z}$ , i.e. $\mu_m = \pm 2\pi m$ with $m \in \mathbb{N}$ , will occur when: $$\sum_{z_k} \frac{1}{z-z_k} = -\frac{1}{z} \tag{4}$$ Question: Suppose we only know the asymptotic for the complex roots $z_k$ of $\mathrm{Si}\left(\frac{z}{2}\right)$ , could we then derive, e.g. using (4), any information/constraints about the location of the real roots $\mu_m$ of its derivative $\mathrm{Si}'\left(\frac{z}{2}\right)$ ? Potentially related questions here , here and here .","['integration', 'roots', 'asymptotics', 'complex-analysis', 'derivatives']"
4653420,Are there infinitely many primes of the form $y = 2p_1p_2 + 1$?,"I came up with this (I admit I'm probably not the first one to have this thought but I haven't been able to find anyone else with the same question) while reading about semiprimes. Clearly $y$ is always odd which suggests to me that it could also be prime a lot of the time. Additionally, There are infinitely many semiprimes so it seems somewhat likely (for purely intuitive reasons) that for semiprime $n$ it could be that $2n+1$ could also be prime.","['number-theory', 'prime-numbers', 'semiprimes']"
4653477,On the limit of a sum equalling $\pi$,"I've been looking at the following sum: $$S=\lim_{n\to\infty}\left(\frac{8}{n^2}\sum_{k=1}^n k\sqrt{\frac{n}{k}-1}\right)$$ Which I have proved converges to $\pi$ . My proof remains relatively simple. It goes as follows: We begin by developing the following expression: \begin{align}
S
&=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\frac{2k}{n}\sqrt{\frac{n}{k}-1}\right)\\
&=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\sqrt{\frac{4k}{n}-\frac{4k^2}{n^2}}\right)\\
&=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)
\end{align} One can notice that the expression within the limit is just the Riemann representation of the following integral: $$\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)=\int_{-1}^1\sqrt{1-x^2}\ dx$$ Which is simply the area of the unit circle in the top half of the plane, equalling $\frac{\pi}{2}$ . hence we have: \begin{align}
S&=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)\\&=2\int_{-1}^1\sqrt{1-x^2}\ dx=\pi\end{align} My question to you is; if you were asked to solve this limit problem, how would you go about it? This is a purely recreational question and i'm curious as to whether there exists even more elementary and elegant ways to prove that this sum is indeed equal to $\pi$ . I'm purely looking for beautiful proofs to this limit problem!","['integration', 'limits', 'calculus']"
4653479,How to find the limit to an undefined point (asymptotic),"I am wondering about: $$  \lim_{x \rightarrow -1} \frac{x+1}{ \sqrt{x+5}-2 } $$ which seems to be an asymptotic function with each side of its corresponding graph approaching negative or positive infinity at the value of -1 . Solved by WolframAlpha, the expected result was calculated: Click to see result . So, I was just learning for a math exam and using the website KhanAcademy to refresh my math skills, where above term was to be solved, and indeed a two-sided limit should be found, which apparently seems to be 4 . This was done by first rationalizing the term in this manner , which results in: $$  \lim_{x \rightarrow -1}( \sqrt{x+5}+2)\text{ for  }x \neq -1 $$ Substituting x for -1 in this simplified term doesn't result in getting $\frac{0}{0}$ but 4 , which apparently means that 4 is the both-sided limit (even though the root of 4 has 2 as well as -2 as the possible solution, making 0 another viable limit?). I can't proof which one of those two approaches contains an error and therefore wanted to ask here, I hope someone can clarify this matter :) Best regards",['limits']
4653518,Proving a property of the Fourier coefficients of a continuous function,"We have that the function f defined on [- $\pi$ , $\pi$ ] is continuous, and we want to prove the following statement about its Fourier coefficients $c_n$ $$c_n = c_{-n}\;\; \text{for all}\; n\; \Rightarrow \text{f even}. $$ I am using the convention $$c_n = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt .$$ I have come up with two ways to show it but I am not sure if there are any flaws with method 2. Method 1: $$c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt $$ The variable change $t\to -t$ in the second integral gives the equality $$ \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(-t)e^{-int}dt \\
\iff \frac{1}{2\pi} \int_{-\pi}^{\pi} (f(t)-f(-t))e^{-int}dx = 0 $$ Now, the integrand in the last integral is a continuous function and for such functions the integral is zero iff the integral is identically zero over that interval. Hence $$ (f(t)-f(-t))e^{-int} = 0 \iff f(t) = f(-t), t\in[-\pi,\pi] \iff \text{f even}. $$ Method 2: $$ c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt \\
\iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)(e^{int}-e^{-int})dx = 0 \iff \int_{-\pi}^{\pi} f(t)sin(nt) dt = 0 \\
\Rightarrow f(t)sin(nt)\;\; \text{odd}\; \Rightarrow f(t)\; \text{even}.$$ I am unsure of whether I can conclude that f(t)sin(nt) is an odd function. I guess there is the possibility that the integral over the symmetric interval vanishes for other reasons. I think the implication only goes in the direction of ""if g is odd then $\int_{-a}^a g(t)dt = 0$ "". I also wanted to ask if the statements $$ c_n = -c_{-n} \; \Rightarrow \; \text{f odd}. $$ and $$\overline{c_n} = c_{-n} \; \Rightarrow \; \text{f real-valued}$$ are proven in the same manner as the above.","['fourier-series', 'calculus', 'fourier-analysis', 'analysis']"
4653527,How to calculate the R square if not given the number of points,"This is an interview problem I encountered and I have no idea about how to deal with it. Hope to get any hint or help for the first step. Suppose there is a unit circle centered at origin, we randomly sample N points on it. There is also another point (K, K), where K >> 1. Now we do a linear regression to these N+1 points, please calculate the $R^2$","['linear-regression', 'statistics']"
4653604,Undertanding rank-deficiencies of the differential of matrix function $f(A)=A^3$ in the space of $3 \times 3$ matrices,"This a follow-on of this question ""Q"" asked some days ago on Maths SE to which I had given an answer that will be denoted ""A"". This question ""Q"" was dealing with the mapping : $$f_{A_0}(X)=A_0^2X+A_0XA_0+XA_0^2$$ In fact (it wasn't mentionned in ""Q""), this mapping is, in the 9-dimensional space $\frak{M}_{3 \times 3}$ of $3 \times 3$ matrices, the differential of function $f:A \to A^3$ in ""point"" $A_0$ . As such $f_{A_0}$ is a linear function having a certain rank $r(f_{A_0})$ which can be maximal, i.e., equal to $9$ (for most $A_0$ ) or have a rank deficiency (equivalent naming : ""rank drop""), like $8$ for some $A_0$ or even less in very rare cases (see statistical study below). It will be easier to work on the complementary value of the rank, i.e., the dimension of the kernel : $$\underbrace{\dim(\ker(f_{A_0}))}_k = 9 - \underbrace{\operatorname{rank}(f_{A_0})}_r$$ Taking a pragmatic approach, I have done a large scale simulation on matrices $A$ having integer entries between $-3$ and $3$ which has given these approximate proportion of values for the dimension $k$ of the kernel of $f_{A_0}$ : $$\begin{array}{c|ccccccc}k&0&1&2&3&4&5&6 \\\hline &97 \% &2.6 \%&0.2\%&0.1 \%&0.0 \%&0.0 \%&0.1 \%\end{array}$$ Side remark : In fact the relative percentages aren't important ; the important fact is the ""holes"" in the spectrum  : no matrices for $k=4,5,7,8$ . I know that this approach doesn't meet the approval of some readers : this question has been downvoted probably for this reason, but I don't bother, I think that all the tools we now have, in particular simulation methods, are good to use in order to establish conjectures . Question 1 : Set apart the fact that in a large majority of cases this differential is injective, how is it possible to explain the presence of these particular cases ( $k=1, 2,3,6$ ) and the absence of other cases ? Let us enter into the details. Case $k=1$ : for matrices $A$ having a one-dimensional kernel, I observe on my simulations that this kernel is always with rank-1 matrices $X$ . Why ? Moreover, one doesn't see the rationale existing between $A$ and $X$ (btw, it was the main question in question ""Q""). Two examples of a matrix $A$ and an element $X$ of the kernel of $f_A$ : $$A=\begin{pmatrix}0&-1&1\\1&-2&2\\-1&2&2\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&1&1\\0&1&1\end{pmatrix}$$ $$A=\begin{pmatrix}1&2&0\\2&1&0\\0&-2&0\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&0&0\\4&-2&3\end{pmatrix}$$ Let us turn now to other cases $k=2,3,6$ where $X$ has (necessarily) rank $1$ : Case k=2 : examples $$A=\begin{pmatrix}0&1&0\\0&0&1\\1&0&0\end{pmatrix} \text{ and } X_1=\begin{pmatrix}0&-1&0\\0&0&1\\0&0&0\end{pmatrix}, X_2=\begin{pmatrix}-2&2&0\\0&1&2\\-4&0&1\end{pmatrix}$$ Case k=3 : example $$A=\begin{pmatrix}0&1&1\\1&0&0\\-1&1&0\end{pmatrix}$$ We are here in a particular case of a matrix where rank $(A)=2$ , rank $(A^2)=1$ and $A^3=A^2$ . Case k=6 : example $$A=\begin{pmatrix}-1&1&-2\\2&0&1\\2&1&1\end{pmatrix}.$$ This matrix has a very specific property $A^3=3I$ . In fact, all matrices of this category ( $k=6$ ) are such that $A^3=\mu A$ for a certain real number $A$ . Why that ?
(Another example of this special case $k=6$ can be seen in my former answer ""A""). Question 2 (the main question in fact) How can the disparate observations seen above be understood and proved under a common globalizing explanation ?","['derivatives', 'matrix-rank', 'linear-algebra']"
4653610,Ash - Complex Variables - Proof of Maximum Principle,"I am self studying Ash & Novinger's Complex Variables. In subsection 2.4.12 of the book , the authors are trying to prove that if $f$ is analytic on an open connected set $\Omega$ and $\lvert f \rvert$ assumes a local maximum at some point in $\Omega$ then $f$ is constant in $\Omega$ . The outline of the proof is as follows: Suppose that $f$ assumes a local maximum at $z_0 \in \Omega$ . If $f(z_0)=0$ then $f(z)=0$ on some neighbourhood $B(z_0, \delta)$ of $z_0$ and identity theorem implies $f\equiv 0$ on $\Omega$ . So, the authors assume $f(z_0)\ne 0$ . By the Cauchy's Integral formula, we have that \begin{align*}
f(z_0)=\frac{1}{2\pi}\int_0^{2\pi} f(z_0 + re^{it})\,dt \leadsto 1=\frac{1}{2\pi}\int_0^{2\pi} \frac{f(z_0 + re^{it})}{f(z_0)}\,dt
\end{align*} for any $r<\delta$ . Using this, the authors then show that $\left\lvert f(z)/f(z_0) \right\rvert =1$ on $B(z_0, \delta)$ . They claim that $\Re \left( f(z)/f(z_0) \right) =1$ on $B(z_0, \delta)$ but do not prove it. I am stuck here. Here's my attempt: Let $r < \delta$ . Then we have that $1=\frac{1}{2\pi}\int_0^{2\pi} \frac{f(z_0 + re^{it})}{f(z_0)}\,dt$ . Taking real part both sides, we get $1 = \frac{1}{2\pi} \Re \left( \int_0^{2\pi} \frac{f(z_0 + re^{it})}{f(z_0)}\,dt \right) = \frac{1}{2\pi} \int_0^{2\pi} \Re\left(\frac{f(z_0 + re^{it})}{f(z_0)} \right) \, dt \le \frac{1}{2\pi} \int_0^{2\pi} \left\lvert \frac{f(z_0 +re^{it})}{f(z_0)} \right\rvert dt \le 1$ . Note that the last inequality is due to the fact that $\lvert f(z_0 + re^{it})\rvert \le \lvert f(z_0) \rvert$ for all $t\in [0,2\pi]$ . If it happened that $\Re\left(\frac{f(z_0 + re^{it})}{f(z_0)} \right) \ge 1$ for each $t\in [0,2\pi]$ then I could have concluded that $\Re\left(\frac{f(z)}{f(z_0)} \right) =1$ for each $z \in B(z_0, \delta)$ . But this may not hold! Any hints would be appreciated! EDIT: Following this up from this answer , we have that from my attempt that $\frac{1}{2\pi}\int_{0}^{2\pi}\underbrace{\left[ \left\lvert \frac{f(z_0 + re^{it})}{f(z_0)} \right\rvert - \Re \left( \frac{f(z_0 + re^{it})}{f(z_0)} \right) \right]}_{\ge 0} \, dt = 0$ . By using continuity, we can then complete the proof.","['complex-analysis', 'proof-explanation']"
4653632,Evaluating an Antiderivative to Two Different Functions,"I have recently been struggling with finding the antiderivative of the rational function $$\int \frac{1}{x \cdot \ln(x^3)}dx$$ I am only recently in calculus, so I expect to have made a basic mistake, which I would like to understand.
Initially, when I tried to evaluate this problem, I used the logarithmic power property to rewrite the integrand as $\frac{1}{3 \cdot x \cdot \ln(x)}$ When substituting $u = \ln(x)$ and $du = \frac{1}{x}dx$ , I rewrote the problem as $\int\frac{du}{3 \cdot u}$ . Factoring out the $\frac{1}{3}$ , I then got $\frac{1}{3} \cdot \int \frac{du}{u}$ , which I evaluated to $\frac{\ln|u|}{3} + C$ , or $$\frac{\ln|\ln(x)|}{3} + C$$ However, when I attempted to re-do the problem in a different way (without simplifying $\ln(x^3)$ ), I substituted $u = ln(x^3)$ and $du = \frac{3x^2dx}{x^3}$ , which I simplified to $\frac{3dx}{x}$ . When performing the substitution, I wound up with $\int\frac{du}{3 \cdot u}$ , and, when factoring out the constant, I am left with $\frac{1}{3} \cdot \int\frac{du}{u}$ , which I found the antiderivative to be $\frac{1}{3} \cdot \ln|u| + C$ , and, with a final substitution of $u$ , I am left with $$\frac{\ln|\ln(x^3)|}{3} + C$$ I am struggling to understand how these functions would have the same derivative (assuming I did found the antiderivative correctly both times), as one has $x^3$ and one simply has $x$ . All help is appreciated.","['integration', 'calculus', 'derivatives']"
4653643,Least eccentricity vertex can't have the most average distance,"The eccentricity of a vertex $\epsilon (v) $ is the maximum of distances $d(v, u) $ over all other vertices $u$ . The average distance of a vertex  avgd $(v) $ is the average of all $d(v, u)$ , more precisely $avgd(v) =\frac{\sum_{u\neq v} d(v, u)} {|V|-1}$ Now if the center of a graph is one single vertex $v$ , that is, there is a least eccentric vertex $v$ , so $\epsilon (v) <\epsilon (u), u \neq v$ , then avgd $(v) < \max(avgd(u)_{u \in V}) $ because this  vertex $v$ can't  simultaneously have  the greatest average distance. At least, that's what I assume. My intuition comes from a Graph where a small clique and a big clique are connected with one separate vertex. That vertex is then the least eccentric, but the small clique vertices have always a higher average distance. Anyone a proof or a counterexample? Some of my thoughts about this conjecture : I now realize that a graph consisting of one single vertex would be a trivial counterexample, so let's only consider simple connected  graphs with more than one vertex. I generated many graphs with a single vertex center and could indeed always find an $u$ with strictly greater average distance than the center. I wondered if we can also find such $u$ that is adjacent to the center, but that's not always the case. Although it might sound intuitive or  trivial that the most central point is not the point with the greatest average distance, it's not generally the case for any metric. It's  not generally true for the Euclidean distance for example. I thought it might be true because if there is singular center $ v$ , then some $v*$ has a shortest path through the center for more than half of the other vertices. But this turned out to be false, I found graphs with a singular center but without such a $v*$","['graph-theory', 'pigeonhole-principle', 'combinatorics']"
4653662,What is the domain of $\left(\frac{x^2-3}{x^2+4}\right)^{2x^3}$?,"I thought the domain of the function $$f(x)=\left(\frac{x^2-3}{x^2+4}\right)^{2x^3}$$ is $[\sqrt{3},\infty)$ , by using the formula $$x^y = \exp\bigl(y\ln(x)\bigr).$$ But then why I can compute $$
f(1) = \frac{4}{25} \text{ or } f(0) = 1?
$$ Is this valid? Any help would be appreciated. Thanks. :)","['algebra-precalculus', 'functions']"
4653710,Show $\nabla f=A\nabla g$ by chain rule,"Let $A$ be a $2\times 2$ an invertible matrix, $f:\mathbb R^2\to\mathbb R$ be smooth, and define $g:\mathbb R^2\to \mathbb R$ by $$g(Ax)=f(x),$$ i.e., if I set the variable of $g$ as $y$ , $g(y)=f(A^{-1}y).$ Then, show $\nabla f=A\nabla g$ using chain rule. Where do I mistake ? Let $D$ represent the derivative. \begin{align}
D_y g(y)
&=D_y(f(A^{-1}y))\\
&=(D_x f)(A^{-1}y)\cdot D_y(A^{-1}y)\\
&=(D_x f)(x)\cdot A^{-1} ,
\end{align} and since $D_y g(y)=\nabla g(y)$ , $(D_x f)(x)=\nabla f(x)$ , I get $$\nabla g=\nabla f\cdot A^{-1}$$ and then $$\nabla f=(\nabla g)A.$$","['scalar-fields', 'matrices', 'calculus', 'derivatives', 'chain-rule']"
4653846,Conditional expectation and the Monotone Class Theorem,"I'm trapped by the following problem 1.5 from Exercises in Probability
A Guided Tour from Measure Theory to Random Processes, via Conditioning In probability space $(\Omega,\mathcal{F},\mathbb{P})$ , let $\mathcal{G}_{}\subset \mathcal{F}$ and $Y\in \mathcal{G}$ ,
If $\mathop{{}\mathbb{E}}_{\mathcal{G}}g(X)=g(Y)$ for any bounded positive $g$ , show that $X = Y \text{ a.s. }$ . Where $\mathbb{E}_{\mathcal{G}}$ is expectation conditioning in $\mathcal{G}$ The solution is extend the identity to $$
\mathop{{}\mathbb{E}}_{\mathcal{G}} G(X,Y)=G(Y,Y)
$$ by monotone class theorem, then taking $G(x,y)=\mathbf{1}_{x \neq y}$ . But I'm confused how to get this from MCT. My attempt is to prove all $G$ forms a monotone class $\mathcal{H}$ , then I have to prove $\mathbf{1}_{A}(x,y)$ belong to this class for any borel $A\in \mathcal{B}(\mathbb{R}^2)$ . The section $\mathbf{1}_{A}(X,y)$ is bounded and positive thus belong to $\mathcal{H}$ , but how to proceed to say $\mathbf{1}_{A}$ also belong to $\mathcal{H}$ ?","['measure-theory', 'monotone-class-theorem', 'conditional-expectation', 'measurable-functions', 'probability']"
4653853,"Find var$(\overline{X_n}^2)$ given that $EX_1=\mu,E(X_1-\mu)^k=\alpha_k$","$X_i\sim F$ are $n$ iid observations. $\overline{X_n}$ is their mean. I want to find var $(\overline{X_n}^2)$ given that $EX_1=\mu,E(X_1-\mu)^k=\alpha_k$ . What I found out are: var $(\overline{X_n})=\frac{\alpha_2}{n}$ var $(\overline{X_n}^2)=E(\overline{X_n}^4)-[E(\overline{X_n}^2)]^2$ $E(\overline{X_n}^2)=\frac{\alpha_2}{n}+\mu^2$ I found a hint that $E[\overline{X}_n^4]={1\over n^4}\sum E[X_i X_j X_k X_\ell]$ but I can't figure how to get this or how to use this. I can also find $E(\overline{X_1}^4)$ in terms of $\alpha_k$ 's and $\mu$ but how do I use these to get $E(\overline{X_n}^4)$ ? Any help?","['expected-value', 'statistics', 'variance', 'means']"
4653882,Calculate the angle of Escape,"There is a 200 meter long and 10 meter wide road (200, 0, 10);
I am a pedestrian in the middle of the road at Vector3(100, 0, 5);
I can run 6 meters per second fast. A car with the width of 10 meters is driving on the road towards me. Means that I have to get off the road to avoid getting hit.
The car has a speed of 50 meters per second, means it hits me in 2 seconds. Now I want to calculate in which directions (Angle) I can run off the road to avoid the car in time before it hits me. Visualized: To calculate the maximum angle that I can run to escape the car, I tried the following already: Calculate the time it takes for the car to reach your current position, which is 2 seconds. Subtract the time it takes for you to reach the edge of the road when running straight towards the roadside (0.83 seconds) from the time the car takes to reach your current position (2 seconds). This gives you the maximum amount of time you have to reach the edge of the road when running diagonally towards the roadside: 1.17 seconds. For each angle θ, calculate the time it takes for you to reach the edge of the road using  ((0.5 * roadwidth) / sin(θ)) / speed If the time calculated in step 3 is less than or equal to the maximum time calculated in step 2, then you can escape the car by running at that angle. Otherwise, you cannot escape the car in time. Repeat step 4 for a range of angles to find the maximum angle at which you can escape the car. However, if I run diagonally against or away from the car towards the roadside, the car needs either more or less time than 2 seconds to reach me because I run towards or away from the car. So I would have to add that to the calculation somehow? Ultimately I want to calculate that in C++, maybe using a for loop via trial and error?
Or is there a better algorithm for that? I think the approach I'm doing is somewhat correct but there is probably some formular which can help me with that. Since I'm not the best in maths I'd appreciate it if someone could point me into the right direction :)","['angle', 'geometry']"
4653897,"If the Fourier coefficient $\hat{f}(k)$ of $f\in C^1(\mathbb T)$ is zero for all $|k|<N$, then $\|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty}$","Let $f\in C^1(\mathbb T)=C^1(\mathbb R/\mathbb Z)$ be a function such that $$\hat f(k):=\int_{\mathbb T}f(x)e^{-2\pi ikx}\,dx=0,\qquad \forall k\in\{-N+1,\cdots,-1,0,1,\cdots, N-1\}.$$ Show that $\|f\|_{L^\infty}\leq \frac CN \|f'\|_{L^\infty}$ for some $C>0$ indpendent of $f$ and $N$ . The best constant $C$ is $\frac14$ , as shown by this post , in which the proof uses a very ""hard"" theorem due to Vaaler . I'm trying to prove the decay rate $N^{-1}$ and I don't care about the best constant $C$ . I'm looking for an easier proof. First Try. By Cauchy inequality and the Plancherel identity, \begin{align*}
\|f\|_{L^\infty}&\leq\sum_{|k|\geq N}|\hat f(k)|=\sum_{|k|\geq N}\frac{\left|\widehat{f'}(k)\right|}{2\pi |k|}\\
&\leq \frac1{2\pi}\left(\sum_{|k|\geq N}\left|\widehat{f'}(k)\right|^2\right)^{1/2}\left(\sum_{|k|\geq N}\frac1{|k|^2}\right)^{1/2}\\
&\leq \frac1{2\pi}\left\|f'\right\|_{L^2}\left(\frac2N\right)^{1/2}=\frac C{\sqrt N}\left\|f'\right\|_{L^2}\\
&\leq \frac C{\sqrt N}\left\|f'\right\|_{L^\infty}\ \ .
\end{align*} The deacy rate on $N$ is not good enough to get the desired result. Second Try. (Also failed.) As Sarvesh Ravichandran Iyer suggested in the comments, using some ideas in the proof that I quoted above, I've made some progress. In this post , let $\psi(x)=x-1/2, x \in [0,1]$ , then $$\psi(x)=-\frac{1}{\pi}\sum_{k \ge 1}\frac{\sin 2\pi kx}{k}, 0<x<1$$ and $\psi*f'=-f$ . If we perturb $\psi$ by any trigonometric polynomial of degree at most $N-1$ and we get an $\eta=\psi-P_{N-1}$ we still have $\eta*f'=-f$ hence $||f||_{\infty}=||\eta*f'||_{\infty} \le ||\eta||_1||f'||_{\infty}$ and in particular if we find $||\eta||_1 \le \frac{C}{N}$ then we are done. A natural candidate is $$\eta(x)=\psi(x)+\frac{1}{\pi}\sum_{1\leq k\leq N-1}\frac{\sin 2\pi kx}{k}=-\frac{1}{\pi}\sum_{k \ge N}\frac{\sin 2\pi kx}{k}.$$ Now, the problem reduces to show that $||\eta||_1 \le \frac{C}{N}$ for this special function $\eta$ , where $C>0$ is independent of $N$ . To explore the oscillation in this summation, we can use the partial sum method. Let $S_M(x)=\sum_{k=N}^M \sin(2\pi kx)$ , then $$S_M(x)=-\frac{\cos((2M+1)\pi x)-\cos((2N-1)\pi x)}{2\sin \pi x}=\frac{\sin((M+N)\pi x)\sin((M-N+1)\pi x)}{\sin\pi x}.$$ The partial sum method applied on $\eta$ gives that $$\eta(x)=-\frac1\pi\sum_{k=N}^\infty \frac1{k(k+1)}S_k(x).$$ Note that near $x=0$ , the denominator is small, hence, naturally we split the integral into two parts: $\int_0^{1/2}|\eta|=\int_0^\delta+\int_\delta^{1/2}$ . By Plancherel we have $$\int_0^\delta |\eta(x)|\,dx\leq \delta^{1/2}\left(\int_0^\delta|\eta(x)|^2\,dx\right)^{1/2}\leq C\delta^{1/2}\left(\sum_{k=N}^\infty\frac1{k^2}\right)^{1/2}\leq C\sqrt{\frac\delta N}.$$ To get the correct decay rate $N^{-1}$ , it seems that $\delta=1/N$ is a good choice. It remains to estimate $\int_\delta^{1/2}|\eta|$ , we have $$\int_\delta^{1/2}|\eta(x)|\,dx\leq C\sum_{k=N}^\infty \frac1{k(k+1)} \int_\delta^{1/2}\frac1{|\sin(\pi x)|}\,dx\leq C\frac {|\log \delta|}{N}=C\frac{\log N}N.$$ Therefore, $\|\eta\|_1\le C\frac{\log N}N$ . This implies that $\|f\|_{L^\infty}\leq C\frac{\log N}N \|f'\|_{L^\infty}$ , which is better than my first try, but not satisfactory either. Any help would be appreciated!","['fourier-analysis', 'harmonic-analysis', 'real-analysis', 'alternative-proof', 'fourier-series']"
4653902,$\underset{n\rightarrow\infty }{\lim}\Vert \underset{i\geq 0}{\sum }\lambda _{i}^{n}x_{i}\Vert^{\frac{1}{n}}\not=0$ if $\lambda _{i}\not=0$?,"Let $H$ be a Hilbert space. Suppose we have a basis $\left( x_{i}\right)
_{i\geq 0}$ for $H$ (not necessarily orthonormal) and a sequence $\left(
\lambda _{i}\right) _{i\geq 0}$ in $\mathbb{C}$ , such that $\underset{i\geq 0}{\sum }x_{i}$ converges to a non-zero vector, $\underset{%
n\rightarrow \infty }{\lim }\lambda _{i}=0$ and $\lambda _{i}\not=0$ for all $i\geq 0$ . Do we have $\underset{n\rightarrow \infty }{\lim }\Vert 
\underset{i\geq 0}{\sum }\lambda _{i}^{n}x_{i}\Vert ^{\frac{1}{n}%
}\not=0$ ? If not, what additionally condition we should make on $\left(
\lambda _{i}\right) _{i\geq 0}$ to have this conclusion.
I am not familiar with this type of problem when the basis is not
orthogonal, and I think that the answer would be very complex. To be more
precise, I am facing this problem for a Riesz operator when $\left(
x_{i}\right) _{i\geq 0}$ represents a sequence of eigen vectors and $\left(
\lambda _{i}\right) _{i\geq 0}$ a sequence of corresponding non-zero eigen values.","['hilbert-spaces', 'functional-analysis', 'eigenvalues-eigenvectors', 'sequences-and-series']"
4653905,"Showing that $f \in \mathrm{BV}[a,b] \implies \int_a^b |f'| \le V[f;a,b]$","Problem Statement: If $f$ is of bounded variation on $[a,b]$ , show that $$\newcommand{\nc}{\newcommand}
\nc{\R}{\mathbb{R}}
\nc{\BV}{\mathrm{BV}}
\nc{\PP}{\mathcal{P}}
\nc{\abs}[1]{\left|#1\right|}
\nc{\set}[1]{\left\{ #1 \right\}}
\nc{\para}[1]{\left( #1 \right)}
\nc{\br}[1]{\left[ #1 \right]}
\nc{\ve}{\varepsilon}
\nc{\vp}{\varphi}
\int_a^b \abs{f'} \le V[a;b]
$$ This is the first part of Problem $7.9$ in Measure & Integral: An Introduction to Real Analysis by Richard Wheeden and Antoni Zygmund. Notes / Definitions: Here, the authors choose to denote total variation by $V[a;b]$ ; other common notations include $V[f;a,b]$ (specifying $f$ ), $\mathrm{TV}[f;a,b]$ , $V_a^b(f)$ , and so on. Integration is meant in the Lebesgue sense; we are dealing with functions $f : [a,b] \to \R$ . Given $f : [a,b] \to \R$ , we define its variation by $$
V[f;a,b] := \sup_{P \in \PP_{a,b}} \sum_i \abs{ f(x_i) - f(x_{i-1}) }
$$ We may associate a variation to a given partition by $$
V(f;P) := \sum_i \abs{ f(x_i) - f(x_{i-1}) }
$$ If $V[f;a,b] < \infty$ , then $f$ is of bounded variation and we say $f \in \BV[a,b]$ . Some Thoughts: It is known that $\BV[a,b]$ functions are differentiable a.e. with derivative in $L^1$ (Corollary $7.23$ of the source text), so the well-definedness of the integral is a non-issue. I've had a couple of ideas that went nowhere, however. Jordan Decomposition: We use that $\BV[a,b]$ functions may be written as the difference of monotone-increasing functions, and that monotone functions are differentiable a.e. Note that, since $f \in \BV[a,b]$ , we may write $f = \vp - \psi$ for $\vp,\psi$ monotone-increasing functions (and hence differentiable a.e.). Note that this means $\vp',\psi' \ge 0$ . Then $$\begin{align*}
\abs{f'} &= \abs{\vp' - \psi'} \\
&\le \abs{\vp'} + \abs{\psi'} \\
&= \vp' + \psi'
\end{align*}$$ giving us $$\begin{align*}
\int_a^b \abs{f'} &\le \int_a^b \vp' + \int_a^b \psi' \\
&\le \vp(b) - \vp(a) + \psi(b) - \psi(a) \\
&\le V(\vp;P) + V(\psi;P)
\end{align*}$$ for any partition $P = \set{x_i}_{i=0}^n$ . As $\vp,\psi$ are increasing, we may drop the absolute values in their variation: $$\begin{align*}
V(\vp;P) + V(\psi;P)
&= \sum_i \abs{ \vp(x_i) - \vp(x_{i-1}) } + \sum_i \abs{ \psi(x_i) - \psi(x_{i-1}) }\\
&= \sum_i  \vp(x_i) - \vp(x_{i-1})  + \sum_i   \psi(x_i) - \psi(x_{i-1})  \\
&= \sum_i  \vp(x_i) - \vp(x_{i-1})  +    \psi(x_i) - \psi(x_{i-1})  \\
&= V(\vp+\psi;P)
\end{align*}$$ This does not help us, as $f \ne \vp + \psi$ . This approach via decomposition is noted in this MSE post and related ones, but not to a sufficient level of detail to grasp what I need to finish the proof. Limit Definitions: Consider a sequence of partitions $P_n := \set{x_i^{(n)}}_{i=0}^{k_n}$ of $[a,b]$ , ordered in the usual sense. For simplicity, let $$\begin{align*}
\Delta_n x_i &:= x_i^{(n)} - x_{i-1}^{(n)} \\
\Delta_n f &:= f \para{ x_i^{(n)} } - f \para{ x_i^{(n)} }
\end{align*}$$ Then for each $i$ , by the mean value theorem, $\exists \xi_i^{(n)} \in \br{ x_{i-1}^{(n)} , x_{i}^{(n)} }$ such that $$
\frac{\Delta_n f}{\Delta_n x_i} = f' \para{ \xi_i^{(n)} }
$$ and hence $$
\abs{\Delta_n f} = \abs{f' \para{ \xi_i^{(n)} } \Delta_n x_i}= \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i
$$ and thus $$
\sum_i \abs{\Delta_n f}  = \sum_i \abs{f' \para{ \xi_i^{(n)} }} \Delta_n x_i
$$ Taking the limit $n \to \infty$ then gives $$
V[f;a,b] = \int_a^b \abs{f'}
$$ However, this is obviously stronger than necessary and does not clearly use that $f \in \BV[a,b]$ . I suspect the core issue lies with the existence of the $\xi_i^{(n)}$ ; perhaps there is a scenario in which they do not always exist? (I suppose issues on a zero-measure dense set might be enough to break this argument.) Still, there is an appeal in proving this for first principles, so it's hard for me to let go of this approach. Does anyone have ideas -- be it in terms of alternative approaches, or ways to salvage these?","['integration', 'lebesgue-integral', 'bounded-variation', 'real-analysis', 'derivatives']"
4653920,"Closed form of the recursion formula for $\int_{0}^{1} \ln^r(1-x)x^n \, dx$","It is easy to see that we have the following recursive formula for the indefinite integral of log powers: $$J_r := \int \ln^r(1-x) \, dx \stackrel{\text{sub.}+IBP}{=} -(1-x)\ln^r(1-x) - r\int \ln^{r-1}(1-x) \, dx = -(1-x)\ln^r(1-x) - rJ_{r-1}$$ with initial condition $J_0 = x$ . It is also fairly easy to obtain the explicit formula for this integral: $$J_r = (-1)^r (r!) x + (x-1)\sum_{j = 0}^{r-1} (-1)^j (r)_j \ln^{r-j}(1-x) \space\space\space(+C)$$ where $(r)_j = \prod_{k = 0}^{j-1} (r-k)$ is the falling factorial. We can use this to find a recursive relationship for the integral $$I_{r,n} := \int_{0}^{1} \ln^r(1-x)x^n \, dx$$ mentioned in the title. Skipping the finicky calculation part (applying IBP once using $J_r$ and moving $I_{r,n}$ to the LHS), we can calculate that $$I_{r,n} = \frac{(-1)^r r!}{(n+1)^2} - \frac{n}{n+1}\left(\sum\limits_{j = 1}^{r-1} (-1)^j (r)_j I_{r-j,n} - \sum\limits_{j = 0}^{r-1}(-1)^{j} (r)_j I_{r-j,n-1}\right)$$ with initial conditions $$I_{0,k} = \frac{1}{k+1} \text{ and } I_{r,0} = (-1)^r r!$$ The first initial condition is trivial, the second one can be calculated in the same way as the integral $J_r$ above, imposing the boundaries straight from the beginning. My question is: Does the relationship for $I_{r,n}$ entail a closed form (i.e. explicit) expression in terms of $r$ and $n$ ? If not, would it at least be possible to evaluate $I_{r,n}$ for some fixed $r \in \mathbb{N}$ ? Some background: This integral surprise-attacked me while trying to find the value of the integral $$-\int_{0}^{1} \frac{\ln^r(t)\ln(1-t)}{1-t} \, dt$$ by series expansion. I will add additional details if requested.","['integration', 'definite-integrals', 'recurrence-relations', 'reduction-formula', 'closed-form']"
4653935,On a generalization of Chudnovsky's $\pi$ formula,"Let $q=e^{2\pi i\tau}$ and $$E_2(\tau)=1-24\sum_{n=1}^\infty n\frac{q^n}{1-q^n},$$ $$E_4(\tau)=1+240\sum_{n=1}^\infty n^3\frac{q^n}{1-q^n},$$ $$E_6(\tau)=1-504\sum_{n=1}^\infty n^5\frac{q^n}{1-q^n},$$ $$J(\tau)=\frac{E_4(\tau)^3}{E_4(\tau)^3-E_6(\tau)^2},$$ $$s_2(\tau)=\frac{E_4(\tau)}{E_6(\tau)}\left(E_2(\tau)-\frac{3}{\pi \operatorname{Im}(\tau)}\right).$$ The paper https://arxiv.org/abs/1809.00533 proves that $$\frac{1}{2\pi\operatorname{Im}(\tau)}\sqrt{\frac{J(\tau)}{J(\tau)-1}}=\sum_{n=0}^\infty \left(\frac{1-s_2(\tau)}{6}+n\right)\frac{(6n)!}{(3n)!n!^3}\frac{1}{(1728J(\tau))^n}$$ for $\operatorname{Im}(\tau)\gt 1.25$ where $√$ is the principal branch of the square root , i.e. it is the square root function ""using"" the principal complex argument , the one ranging from $-\pi$ to $\pi$ . Consequently, this principal square root function has a branch cut at the negative real axis. Obviously one can choose another square root function, the branch cut can be elsewhere, for example it can be the ray emanating from $0$ and going in the direction of $i$ towards $i\infty$ . On p. 39 of the linked paper, the author argues that ""Thus we still have to prove for any $\tau$ with $\operatorname{Im}(\tau) \gt 1.25$ that our result doesn’t contain any hidden complex roots of unity:
We choose $\tau_8=\frac{i\sqrt{8}}{2}$ . Here we get that $q = e^{2\pi i\tau_8} = e^{-2π\sqrt{2}}$ is a real number.
Thus (from Thm. 4.5) both $J(\tau_8)$ and $s_2(\tau_8)$ are also real-valued. The approximations
together with the error estimates from Thm. 5.1 and 5.3 tell usthat both $J(\tau_8)$ and $\frac{1−s_2(\tau_8)}{6}$ are positive real numbers. This shows that all quantities in the equation of Thm. 9.7 are real-valued and positive at $\tau=\tau_8$ . The region $\operatorname{Im}(\tau) \gt 1.25$ is connected, both sides of the equation depend continuously on $\tau$ and are not zero. This proves that the equation is exact... "" What does he mean by ""depend continuously on $\tau$ ?"" What theorem does the text in bold even refer to? When I saw ""connected"", ""continuously"", it reminded me of the identity theorem, but reuns commented that $\frac{1}{2\pi\operatorname{Im}(\tau)}\sqrt{\frac{J(\tau)}{J(\tau)-1}}$ isn't holomorphic anywhere. It seems that the author proved the theorem only for some square root function, not necessarily the principal one. I can't see how choosing just one point suffices to determine the complex square root function. I took a look at the original paper of the Chudnovsky brothers. They didn't consider the equality for arbitrary $\tau$ with $\operatorname{Im}(\tau)\gt 1.25$ , but they considered $\tau$ at imaginary quadratic irrationals at which $J(\tau)$ is real , so they didn't have to take care about these subtleties.","['complex-analysis', 'modular-function', 'pi', 'sequences-and-series']"
4653956,Solve the following differential equation using power series :$(x-1)y''+xy'+\frac yx=0.$,"I was learning to solve differetial equations, using power series. There was a problem given as: Solve the following differential equation using power series : $$(x-1)y''+xy'+\frac yx=0.$$ I tried solving the problem in the following way: Assuming, $y=\Sigma_{n=0}^\infty c_nx^n$ to be a solution of $(x-1)y""+xy'+\frac yx=0.$ Now, $y'=\Sigma_{n=1}^\infty n c_nx^{n-1}$ and $y''=\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}.$ Substituting these in the given equation, we get, $$(x-1)\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies x\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies \Sigma_{n=2}^\infty n(n-1) c_nx^{n-1}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+\Sigma_{n=1}^\infty n c_nx^{n}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0.$$ Reindexing the summations we get, $$\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0.$$ Again, $$\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0\implies \Sigma_{n=1}^\infty (n+1)(n) c_{n+1}x^{n}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=1}^\infty (n) c_{n}x^{n}+ \Sigma_{n=-1}^\infty c_{n+1}x^{n}=0\implies \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0.$$ Now, the right-hand side of the equation is free of $x^{-1}$ term due to which, $c_0=0.$ So, $ \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0\implies  \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}]x^n+2c_2+c_1=0.$ Now, comparing LHS and RHS of the equation, we can say that $2c_2+c_1=0$ and $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0.$ Thus, $c_2=-\frac{c_1}{2}.$ For $n\geq 1,$ we have, $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0\implies (n^2+n+1) c_{n+1}- (n+2)(n+1) c_{n+2}+n c_{n}=0\implies -(n^2+n+1) c_{n+1}+(n+2)(n+1) c_{n+2}-n c_{n}=0\implies (n+2)(n+1) c_{n+2}-(n^2+n+1) c_{n+1}-n c_{n}=0.$ I don't know how to proceed further since, the equation $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0,$ is a recurrence relation though, but how to find the coefficient $c_n$ here ? I am not getting, how to solve this recurrence relation in order to find $c_n$ ? (I am just a beginner and I need an elementary level explanation of this.) I am skeptical , whether at all a power series solution exists or not. (I found this problem in a handout, so I don't know about the source. If anyone does know about it, please do let me know.) There might be several posts, concerning the same topic, but I can't seem to find it either. My attempt is modified after Elliot Yu's beautiful suggestion","['power-series', 'problem-solving', 'ordinary-differential-equations']"
4653987,Absolute sine inequality [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question How do I prove that $$|\sin(10x)|\leq 10|\sin(x)|$$ for all real numbers $x$ ?","['trigonometry', 'inequality']"
4654014,Area of a Lune general formula,I have been searching all over the internet looking for the general formula for the area of a lune and have only found this one on the Wolfram MathWorld page: I emailed the author asking for a derivation and received no response.  Does anyone know how this is derived geometrically?  Does anyone know a simpler formula that is derived geometrically?,"['trigonometry', 'geometry', 'derivation-of-formulae']"
4654032,Conjecture $ \sum _{i=1}^{n-1}\lfloor \frac{i^2}{n}\rfloor \ge \frac{\left(n-1\right)\left(n-2\right)}3$ where $n\in\mathbb N^*$,"Let $n \in \mathbb {N^*}$ and $$S_n = \sum_{k=1}^{n-1} (k^2 \bmod n)$$ with the first values 0, 1, 2, 2, 10, 13, 14, 12, 24, 45, 44, 38, 78, 77, 70, 56, 136, 129, 152, 130, 182, 209, 184, 148, 250, 325, 288, 294, 406, 365, 372, 304, 484, 561, 490, 402, 666, 665, 572, 540, 820, 805, 860, 726, 840, 897, 846, 680, 980, 1125 ​​and it appears that $$S_n\leq \frac {n(n-1)}2$$ We can show that $$S_n\leq \frac {n(n-1)}2\iff  \sum _{i=1}^{n-1}\left\lfloor \frac{i^2}{n}\right\rfloor  \ge \frac{\left(n-1\right)\left(n-2\right)}3$$ we have equality if $n$ is prime and $n \equiv 1 \mod 4$ Do you think this inequality is true? Addition : We fix a prime number $p$ that satisfies $p \equiv 1 \pmod{4}$ . Let $\forall n \in \mathbb{N}^*$ , $S_n$ be defined as the sum from $k = 0$ to $p^n - 1$ of $(k^2)_{p^n}$ . Numerically, it appears that $2S_n = p^n(p^n - p^{\lfloor n/2 \rfloor})$ . If we can prove this equality, we can deduce inequality  (the object of the thread) in many cases","['summation', 'ceiling-and-floor-functions', 'modular-arithmetic', 'number-theory', 'inequality']"
4654063,Generating random variables with sum zero,"I was trying to generate a random vector $(X_1,X_2,\ldots,X_n)$ such that $\sum_{i=1}^n X_i =0$ . There is also an underlying assumption that every proper subvector of this vector is independent (like $X_1,\ldots,X_{n-1}$ are independent, or $X_2,\ldots, X_n$ are independent).   But I am unable to generate this; in fact, I think it is not possible to do so. Suppose we take $n=3$ . We generate $X_1, X_2$ (so they are independent) and take $X_3 = -(X_1+X_2)$ . But this contradicts the underlying assumption that $X_1,X_3$ are independent. Because we should have $X_1, X_3$ independent as well, and this is not possible from Are $X$ and $X+Y$ independent, if $X$ and $Y$ are independent? . Is my reasoning correct or is the way I am generating such variables wrong? Any help or reference would be grateful.","['probability-theory', 'probability', 'random-variables']"
4654071,Is $\mathbb Z$ of finite presentation over $\mathbb F_1$?,"I know that there are several proposals for theories of the ""field with one element"", such as Deitmar's, based on ""commutative monoids with zero"" and monoid schemes , or Lorscheid's, based on blueprints . This is not my field at all (I am a theoretical computer scientist), so my understanding of this is extremely superficial and I was not able to figure out whether, within any of these theories, the integers are an algebra of finite presentation over the field with one element. Is this the case? If the current theories do not provide an answer (for example because it is not clear what ""finitely presented"" even means), is there a best guess?  I mean, in a fully working theory of the field with one element, is this expected to be true or not?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4654078,"Area of region enclosed by $y=0$, $x=1$ and $y=\sum\limits_{k=1}^{2n}k(-x)^k$ as $n\to\infty$","Let $A_n$ be the area of the region enclosed by $y=0$ , $x=1$ and $y=\sum\limits_{k=1}^{2n}k(-x)^k$ . Evaluate $\lim\limits_{n\to\infty}A_n$ . Numerical investigation suggests that the answer is $\frac12$ . Context I just made up the polynomial $y=-x+2x^2-3x^3+4x^4- \dots+2nx^{2n}$ , and discovered that there is a thin region bounded by this curve, the $x$ -axis and $x=1$ , whose area doesn't seem to vanish as $n\to\infty$ . My attempt To find a closed form of the polynomial, let $S=\sum\limits_{k=1}^{2n}k(-x)^k$ . $S+Sx$ is mostly a geometric series, and we get $S=\dfrac{2nx^{2n+2}+(2n+1)x^{2n+1}-x}{(1+x)^2}$ . Then we can try to evaluate $\int_{\alpha}^1 S dx$ , where $\alpha$ is the positive root of $S$ . There is a way to evaluate $\int_0^1 S dx$ , which might be adapted to $\int_\alpha^1 S dx$ , but since I don't know how to  find $\alpha$ , this approach doesn't seem to work.","['integration', 'area', 'polynomials', 'sequences-and-series', 'limits']"
4654099,Proof for $X^3=I$ $\implies$ $X$ is diagonalizable.,"Let $X$ be a $3\times 3$ complex matrix, and suppose $X^3=I.$ Then, show that $X$ is diagonalizable. I searched the solution, and this seems to work, but there is a part I don't understand. Proof Let me use this proposition : Let $A$ be $3\times 3$ matrix. If a polynomial $P(x)$ satisfies $P(A)=O$ , then $P(x)$ is devided by minimal polynomial of $A$ ; $\varphi_A(x)$ . Now, for $P(x):=x^3-1=(x-1)(x-\omega)(x-\omega^2)$ , I have $P(X)=X^3-I=O,$ so by the proposition, $P(x)$ is devided by $\varphi_X(x).$ Thus, $\varphi_X(x)$ doesn't have repeated roots, and therefore $X$ is diagonalizable. I don't understand the part "" Thus, $\varphi_X(x)$ doesn't have repeated roots."" What I know is that $P(x)$ can be devided by $\varphi_X(x)$ , so I have $P(x)=\varphi_X(x)\cdot B(x)$ for some polynomial $B(x)$ . Why this leads the fact that $\varphi_X(x)$ doesn't have repeated roots ? Postscript I reffered to Ben's answer and Stephen's answer in $A^3 = I$. Find the possible Jordan Forms???","['matrices', 'diagonalization', 'linear-algebra', 'minimal-polynomials']"
4654158,Evaluating $\lim_{x\to 0^+} \frac1{x} \cos^{-1} (\frac{\sin x}{x})$,"Today I came across the following limit problem: $$\lim_{x\to 0^+} \frac1{x} \cos^{-1} \Big(\frac{\sin x}{x}\Big)$$ Here's my work: $$\begin{align}L &= \lim_{x\to 0^+} \frac1{x} \cos^{-1}\left (\frac{\sin x}{x}\right) \tag{1}
\\& = \lim_{h\to 0}\frac1h \cos^{-1}\left(\frac{\sin h}{h}\right)\tag{2}
\\& = \lim_{h\to 0}\frac1h \sin^{-1}\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)
\\ & =  \lim_{h\to 0}\frac1h \frac{\sin^{-1}\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)}{\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)}\cdot \left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)
\\& =  \lim_{h\to 0}\frac{\sin^{-1}\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)}{\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)}\cdot \lim_{h\to 0}\frac1h\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)\tag{3}
\\& =  \lim_{h\to 0}\frac1h\left(\sqrt{1 - \frac{\sin^2 h}{h^2}}\right)
\\& = \lim_{h\to 0} \sqrt\frac{h + \sin h}{h} \cdot \sqrt{\frac{h -\sin h}{h^3}}
\\& = \lim_{h\to 0}\sqrt{1 + \frac{\sin h}{h} } \cdot \lim_{h\to 0}\sqrt{\frac{h -\sin h}{h^3}}\tag{4}
\\& = \sqrt{2} \cdot \frac1{\sqrt{6}}\\& = \frac1{\sqrt{3}}\end{align}$$ Details: $(1)$ Putting $x = 0 + h$ as $x \to 0^+$ , $h\to 0$ . $(2)$ Used $\cos^{-1}(x) = \sin^{-1}\sqrt{1-x^2}$ . $(3)$ Used $\lim_{x\to 0} \frac{\sin^{-1} x}{x} = 1$ . $(4)$ Used $\lim_{x\to 0}\frac{\sin x}{x}= 1$ for the first limit. And the second limit is evaluated using Maclaurin Series of $\sin h$ . Although I'm able to solve it but my method is too long. I'm looking for a shorter or alternative way to do it.","['limits', 'calculus', 'limits-without-lhopital']"
4654193,"Faulty generalization of $(-1)^{z}$, is there a hole in my argument","I am an Vietnamese grade 12 student. We were studying about exponential functions until I read the part that writes: For the function $f(x) = a^{x}$ , we always assume a is an positive real number that is not 1. In other words, $0< a \neq 1$ . What would happen if we set an $a$ that violates those properties? $a = 0$ and $a = 1$ yields pretty boring results (we would have $$f(x) = \begin{cases} 0 & x\subset \mathbb{R} \setminus \left \{ 0 \right \}\\undefined & x = 0\end{cases}$$ and $f(x) = 1$ ). AFter all, for complex numbers we defined the factorial and Fibonacci numbers , which are definitely much more difficult to define. What if we let $a$ be some negative number instead? Due to the rule $a^{x} b^{x} = (ab)^{x}$ (if we assume it holds anyway), we only need to define exponentiation for $-1$ , since if we somehow made a function that is $(-1)^{x}$ , we can define $a^{b}$ for all $a$ and $b$ in $\mathbb{R}$ (except $a = 0$ and $b = 0$ ) by writing $$a^{b} =\left | a \right |^{b} sgn(a)^{b} $$ Now onto the actual topic. Using GeoGebra, I plotted a few points in space. Due to the nature of this function, it is impossible to plot a consistent line. Points on GeoGebra . I noticed that the function $f(x) = (-1)^{x}$ also has the following properties: It is periodic with period 2 since : $(-1)^{x + 2} = (-1)^{x} 1^{2} = (-1)^{x}$ It takes values at $-1$ for odd integer inputs and $1$ for even integer inputs. These are also its maximum and minimum values over $\mathbb{N}$ The generalization I suggest will have these properties: Continuous over $\mathbb{R}$ Has maximum value $1$ at even integer inputs and minimum value at $-1$ at odd integer inputs. Therefore I suggest one such generalization is $f(x) = (-1)^{x} =\cos (\pi x)$ . This would also allow negative exponentiation to expand indefinitely to the complex numbers, since $\cos (\pi x)$ is well-defined over $\mathbb{C}$ . Is there a hole in my argument? Please point it out to me, thanks for reading!","['functions', 'exponential-function']"
4654202,Let $f(x)$ be a polynomial such that $f(x)*f(1/x) +3f(x)+3f(1/x)=0$ and $f(3) =24$. What is $f(2) +f(-2)$?,Let $f(x)$ be a polynomial such that $f(x)*f(1/x) +3f(x)+3f(1/x)=0$ and $f(3) =24$ . What is $f(2) +f(-2)$ ?,"['functional-analysis', 'analysis']"
4654208,"Ash & Novinger - Complex Variables - Exercise 2.4.1, possibly dealing with identity theorem","I am self-studying Ash & Novinger's Complex Variables. In Exercise 2.4.1 (Page 26) the authors ask the following exercise: Give an example of nonconstant, analytic function $f$ on a region $\Omega$ such that $f$ has a limit point of zeroes at a point outside $\Omega$ . It is immediate to see that the function $f: \mathbb C \setminus \{ 0 \} \to \mathbb C$ given by $f(z)=\sin (\frac{\pi}{z})$ has zeros at $1/k$ for each $k \in \mathbb N$ . However, the limit of the sequence whose terms are $1/k$ is $0$ does not belong to the domain. But, I think the authors are trying to illustrate something here. Does it have something to do with the identity theorem? In this case the zero set of $f$ consists elements of the form $1/k$ where $k \in \mathbb Z \setminus \{ 0 \}$ . So the zero set of $f$ does not contain any limit points of $\mathbb C \setminus \{ 0 \}$ . But then the identity theorem has nothing to do here. What exactly are the authors trying to illustrate here?",['complex-analysis']
4654229,Show that $\omega \mapsto \int_A \lambda^{i\omega}d\mu(\lambda)$ is analytic,"Let $\alpha\in \mathbb{R}$ and $D_\alpha$ be the open horizontal strip in the complex plane $\mathbb{C}$ bounded by $\mathbb{R}$ and $\mathbb{R}-i\alpha$ . Let $\mu$ be a complex measure on $[0, \infty[$ and consider the function $$D_\alpha \ni \omega \mapsto \int_{1/k}^k \lambda^{i\omega}d\mu(\lambda)$$ where $k\in \mathbb{N}$ is fixed.
I want to show that this function is analytic. Since every complex measure is a linear combination of finite positive measures, we may assume that $\mu$ is a finite positive measure. Consider a sequence $\{\omega_n\}\subseteq D_\alpha$ and $\omega\in D_\alpha$ with $\omega_n \ne \omega$ for all $n$ and $\omega_n\to \omega$ . It suffices to show that $$\lim_{n \to \infty} \int_{1/k}^k \frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}d\mu(\lambda) = \int_{1/k}^k i \operatorname{Log}(\lambda) \lambda^{i\omega}d \mu(\lambda)$$ so basically we need to justify why we can interchange limit and integral. For this, I wanted to apply the dominated convergence theorem. However, then I have to argue that there exists an integrable function $g$ on $[1/k,k]$ such that $$\left|\frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}\right|\le g(\lambda).$$ How do I find this function? Thanks in advance for your help/comments!","['measure-theory', 'complex-analysis', 'functional-analysis', 'derivatives', 'analytic-functions']"
4654287,"A (classical) bound on the jumps of a cadlag function: $\left|\Delta f(t)\right|\leq 2\,\sup_{0\leq s\leq t}\left|f(s)\right|$.","For a càdlàg function $f:[0,\infty)\rightarrow\mathbb{R}$ I need to prove the following intuitive bound $$
\left|\Delta f(t)\right|=\left|f(t)-f(t-)\right|\leq 2\,\sup_{0\leq s\leq t}\left|f(s)\right|
$$ The idea is that $$
\left|\Delta f(t)\right|\leq \left|f(t)\right|+\left|f(t-)\right|\leq \sup_{0\leq s\leq t}\left|f(s)\right|+\sup_{0\leq s\leq t}\left|f(s-)\right|
$$ But I miss to prove that $$\sup_{0\leq s\leq t}\left|f(s-)\right|=\sup_{0\leq s\leq t}\left|f(s)\right|$$ Any suggestion is welcome.","['functions', 'supremum-and-infimum', 'absolute-value', 'real-analysis']"
4654291,"Evaluate $\int_{0}^{\infty}{e^{-x^2}\cos(2x^2)\,dx}$","$$\begin{align}I&=\int_{0}^{\infty}{e^{-x^2}\cos(2x^2)\,dx}\\ I&\overset{u=2x^2}{=}\frac{\sqrt{2}}{4}\int_{0}^{\infty}e^{-\frac{u}{2}}\cos(u)\,\frac{du}{\sqrt{u}}\\I&=\Re\left(\frac{\sqrt{2}}{4}\int_{0}^{\infty}{e^{-u(\frac{1}{2}-i)}\,u^{\frac{1}{2}-1}\, du}\right)\end{align}$$ How can I continue here? It looks like the gamma function but I'm unsure how to eliminate the $(\frac{1}{2}-i)$ in this case. Usually it involves drawing a rectangular contour to justify a substitution but I don't think it can be applied at this stage (?)","['integration', 'complex-analysis']"
4654355,Extending holomorphic function on punctured plane to Riemann sphere,"I have a holomorphic function $f: \mathbb{C} \setminus \{0\} \rightarrow \mathbb{C}$ , and I wish to extend $f$ to a holomorphic function $\tilde{f}: \widehat{\mathbb{C} \setminus \{0\} } \rightarrow \hat{\mathbb{C}}$ where $\widehat{\mathbb{C} \setminus \{0\} }$ is homeomorphic to the Riemann sphere by adding two points at infinity. I believe that it suffices to determine where these two points at infinity go, so it seems that I should map both of them to $\infty$ in $\hat{\mathbb{C}}$ . (In the more specific context of the problem I'm working on, I also have to show that $\tilde{f}$ is a double branched cover.) However, I'm struggling to show that $\tilde{f}$ is holomorphic that at these two points at infinity. I've tried computing the derivative directly by definition, but I'm running to issues involving arithmetic with $\infty$ . Please let me know in the comments if you need the complete context of the problem in case this is not enough information.",['complex-analysis']
4654369,Fastest way to show that $D_6 \to S_5$ is an injective homomorphism,"I want to show that there is an injective homomorphism from $D_6 \to S_5$ where $D_6$ denotes the dihidral group of order 12 and $S_5$ the symmetric group. But I'm not sure how I can do this efficiently. I define $f: D_6 \to S_5$ by $f(\sigma) = (12)$ and $f(\rho) = (123)(45)$, with $\sigma$ being a reflection and $\rho$ being a rotation. I know that $D_6$ is generated by $\rho$ and $\sigma$ and that $S_5$ is generated $(12), (23), (34), (45)$. So what is the fastest way, for someone who is just starting with algebra, to show that this is a homomorphism? Do I have to show it explicitly for all 12 elements? What confuses me is that you have to show for all $x,y \in D_6$ we have $f(xy)=f(x)(y)$, while $x$ and $y$ can be any combination of $\rho$ and $\sigma$. Lastly, what is the fastest way to show it's kernel is trivial without going over all elements?","['group-homomorphism', 'dihedral-groups', 'abstract-algebra', 'group-theory', 'symmetry']"
4654397,How to prove this theorem for the number of components of a filled julia sets?,"If one finite critical point of $f(z)$ escapes to infinity by iterating, then the filled-in Julia set of $f(z)$ consists of infinitely many components. How to prove this ? I must admit I heard this in the context of polynomials so maybe there are restrictions on $f(z)$ ? Does $f(z)$ need to be a polynomial ? Or is entire sufficient ?
Or does $f(z)$ need to be an entire and fast converging taylor series ?
Maybe $f(z)$ needs to be bounded in some way like $|f(z)| < C \exp(|z|) $ or such. So , my question is two-fold : What is the exact most general definition of the theorem ? How to prove this theorem or maybe theorems if we consider the stronger versions ?","['complex-analysis', 'complex-dynamics', 'nonlinear-dynamics', 'fractals']"
4654420,Orbits of f.g. abelian group acting by affine transformations on a free abelian group,"Let $A$ be a finitely generated abelian subgroup of $\operatorname{Aff}(\mathbb{Z^n}) = \mathbb{Z}^n \rtimes \operatorname{GL}_n(\mathbb{Z})$ . Then $A$ acts naturally on $\mathbb{Z}^n$ : $$A \times \mathbb{Z}^n \to \mathbb{Z}^n \colon ((x,M),y) \mapsto x+My.$$ Given a generating set $ \{ a_1, \ldots, a_k \} $ of $A$ , is there a way to determine whether this action has finitely many orbits, and if so, determine representatives of these orbits? Some thoughts: If $n = 1$ , this is relatively straightforward. Each $a_i$ is of the form $(x_i,\pm 1)$ with $x_i \in \mathbb{Z}$ . Using that $A$ is abelian, we (eventually) end up in one of three cases: $A = \langle (x,-1) \rangle$ for some $x \in \mathbb{Z}$ . Then $A \cong \mathbb{Z}_2$ and there are infinitely many orbits. $A = \langle (x,1) \rangle$ for some $x \in \mathbb{Z}$ with $x \neq 0$ . Then $A \cong \mathbb{Z}$ and there are finitely many orbits, with representatives $0, 1, \ldots, x-1$ . $A = \langle (0,1) \rangle$ . Then $A$ is trivial and there are infinitely many orbits. However, the situation for $n \geq 2$ does not seem to be so straightforward. Intuitively, I expect there to be finitely many orbits if and only if $A \cong \mathbb{Z}^n$ . If $A \subseteq \mathbb{Z}^n$ , then this is certainly the case and we can easily find representatives of the orbits. However, we can have finitely many orbits when $a_i \notin \mathbb{Z}^n$ : the group $A$ given by $$ A := \left\langle \left( \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \right), \left( \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \right)\right\rangle$$ is isomorphic to $\mathbb{Z}^2$ and it acts transitively.","['computational-mathematics', 'group-theory', 'group-actions', 'abelian-groups']"
4654434,Limits of recursions like $f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)}$ !?,"Consider the sequences $$f(0)=1,f(1)=2$$ $$f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)}$$ $$g(0)=1,g(1)=2,g(2)=3,g(3)=4$$ $$g(n+4)=\frac{3}{g(n)} - \frac{3}{g(n+1)} + \frac{3}{g(n+2)} - \frac{3}{g(n+3)}$$ Then as $n$ goes to infinity : $$\lim f(n)^2 = 2$$ $$\lim g(n)^2 = 12$$ Notice that one can not simply plug in the limits $L_1$ or $L_2$ , because $$L_1=\frac{1}{L_1} - \frac{1}{L_1} = 0 ??$$ does not make sense and $$L_2=\frac{3}{L_2} - \frac{3}{L_2} + \frac{3}{L_2} - \frac{3}{L_2} = 0 ??$$ does not make sense either. So we must keep in mind that it behaves different; double limits or multiple limits or limits converging at different rates. But also noteworthy is this : The initial values $f(0),f(1),g(0),g(1),g(2),g(3)$ do not matter much as long as most of them are distinct like $f(0) \neq f(1)$ for instance.
We get the same limits anyways. Also keep in mind that replacing $-$ with $+$ in the formulas will also work BUT changing the positions of $+$ or $-$ will NOT. However although most initial conditions will work for both $f$ and $g$ , in particular if no division by zero occurs , some exceptional ones might not work. And what if we start with say gaussian integers or eisenstein integers or complex numbers ? Despite it almost always converges to these limits , what are the exceptional set of exceptions ? Are the exceptions related to continued fractions, julia sets or irrationality measures ? I considered iterating the equations a few times to get expressions for $f(n+5),g(n+5)$ and such and although it has its benefits and seems to make more sense in a way ( dealing with the + and - ) it also becomes more complicated fast. ( somewhat remind me of somos or modular equations ) Also want to note these equations are part of a large family, I could easily describe a similar $h(n)$ depending on 6 or 10 previous terms converging to algebraic numbers. I also considered reducing the dependancy on previous values, by special case starting positions or by trying to relate it to pure iteration ( of one previous value ) of other functions ( like fibonacci is related to exp ). But it seems non-trivial. It is not clearly related to typical numerical methods such as newton's method or continued fractions or the babylonian method. How do we even prove it attracts ? Notice limits like $\frac{f(2n)^2 - 2}{f(2n+2)^2 - 2}$ and the alike also do not seem to converge in an easy pattern, so this sets it apart from most numerical methods.
This is probably due to the divisions. I considered comparing to median methods and other averaging methods but it just seems different fundamentally. Any insight in this would be appreciated. edit I found this question as a ""+ analogue"" for the case $f(n)$ . For that case there are starting values that get different outcomes. So that suggests maybe here also ? This was asked in the comments and mentioned in the OP, so I add it now : Why does the process defined with $a_{n+2} = \frac{1}{a_n} + \frac{1}{a_{n+1}}$ converge to $\pm\sqrt{2}$ for most choices of the starting values? and the case for $g(n)$ is probably more complicated.","['complex-dynamics', 'recurrence-relations', 'limits', 'numerical-methods', 'dynamical-systems']"
4654438,Distribution of distances between random points on spheres,"This is in relation to the following blog post by John Baez: Random Points on a Sphere (Part 1) It is claimed that, for two unit quaternions $x,y\in S^3$ , uniformly randomly selected, the probability distribution of $|xy-yx|$ (how ""far apart"" $x$ and $y$ are from commuting) is given by: $$ P(d) = d-\frac{2d\arcsin(d/2)}{\pi},\quad d = |xy-yx|,\quad x,y\in S^3. $$ More generally, the probability distribution for the distance between two uniformly randomly chosen points on the $n$ -sphere is given by: $$P_n(d) = \frac{d^{n-2}(4-d^2)^{\frac{n-3}{2}}\Gamma(n/2)}{2^{n-3}\sqrt{\pi}\ \Gamma(\frac{n-1}{2})},\quad d=|x-y|,\quad x,y\in S^n.$$ My knowledge of probability theory is in all honesty, severely lacking. So my question is: Q: How does one derive these probability distributions?","['geometric-probability', 'probability', 'probability-distributions', 'geometry', 'quaternions']"
4654534,Is this exercise from Tao's Analysis 1 erroneous?,"On page 68 of the fourth edition of Tao's Analysis 1, is Exercise $3.5.12$ , the first part of which I believe is erroneous. The exercise is stated as follows: ( Note: $n++$ refers to the successor of $n$ ) Let $X$ be a set, let $f:\mathbf{N}\times X\to X$ be a function, and let $c$ be an element of $X$ . Show that there exists a function $a:X\to X$ such that $$a(0) = c$$ and $$a(n++) = f(n,a(n)) \text{ for all }n\in\mathbf{N},$$ and furthermore that this function is unique. The required properties of the function $a$ imply that the natural numbers lie in its domain. However, the domain specified here is any arbitrary set $X$ which may not contain the natural numbers. If the domain of $a$ was restricted to the set of natural numbers, the statement of the exercise itself serves as a complete definition of the function, from which it's existence and uniqueness follow directly (I may be wrong here. Do correct me if that's the case). However, this exercise is followed by a fairly elaborate hint, which indicates that the solution may not be this trivial. Alternatively, if $X$ was defined to be some set that contains the natural numbers but isn't limited to them, one could define $a$ as in the exercise statement for the natural numbers and as some constant for all elements of $X$ that aren't natural numbers, but then there would be no guarantee of uniqueness, as there would be more than one choice of this constant. Thus, it seems to me that this exercise is somewhat incorrect, but I would like to get some more opinions on this. Any views you may have on this would be much appreciated. Update: This error has now been reported, acknowledged, and listed as an erratum on the book's official site which can be found here.","['elementary-set-theory', 'functions', 'natural-numbers', 'set-theory']"
4654610,Does the series converge,"We know that all series of the following form diverge: \begin{equation}
 S_k = \sum_{n=\left\lceil \mathrm{e}^k \right\rceil}^\infty \frac{1}{n (\ln n) (\ln \ln n)\dots(\ln^k n)}
\end{equation}
where the notation $\ln^k$ is function composition. Additionally, this series is interesting because for large $k$, it diverges very slowly. Then does the following series converge? \begin{equation}
 S = \sum_{n=1}^\infty \frac{1}{f(n)}
\end{equation}
where $f(n)$ is defined recursively by the monotonic function \begin{equation}
f(n) = \begin{cases}
 n & \text{if }n \le \mathrm{e} \\
 n \cdot f{\left(\ln n\right)} & \text{otherwise}
\end{cases}
\end{equation} The first few terms of this series are \begin{equation}
1 + \frac{1}{2} + \frac{1}{3 \ln 3} + \frac{1}{4 \ln 4} + \dots + \frac{1}{16 (\ln 16) (\ln \ln 16)} + \dots
\end{equation} and the maximum power of $\ln$ in the denominator gradually increases. This series should eventually grow slower than all $S_k$ (it is in some kind of sense a limit) because the terms will eventually drop below those of $S_k$ for any particular $k$. But at the same time it seems to grow faster than all series of the form \begin{equation}
 T_k = \sum_{n=\left\lceil \mathrm{e}^k \right\rceil}^\infty \frac{1}{n (\ln n) (\ln \ln n)\dots(\ln^{k-1} n){(\ln^k n)}^2}
\end{equation} which are known to converge, albeit also very slowly.",['divergent-series']
4654679,Variance of sample fourth moment,"Let $X_1,X_2,\dots$ be i.i.d. standard normal variables. We assume we know that they are standardized and i.i.d., but we don't know the distribution. We want to estimate $M_4:=E[X^4_i]=3$ . One possibility is to compute the sample fourth moment: $$\hat{M}_4:=\frac{1}{n} \sum_{i=1}^n X_i^4$$ By the law of large numbers $\hat{M}_4$ will converge in probability to $M_4$ . Another possible estimator for $M_4$ is $$\hat{M}_{4}^{'}:=\frac{\hat{M}_4}{(\frac{1}{n}\sum_{i=1}^n X_i^2)^2} $$ Simulations seem to indicate that $\hat{M}_{4}^{'}$ is more precise than $\hat{M}_4$ , with a mean squared error smaller by a around $1/2$ . Are there theoretical reasons for this? Am looking for a formal calculation. Thanks a lot for your help.","['statistical-inference', 'statistics', 'probability-limit-theorems', 'probability-theory', 'probability']"
4654693,Asking for clarification of a Wikipedia article on uniform continuity,"A portion of Wikipedia article on uniform continuity Functions that have slopes that become unbounded on an infinite domain cannot be uniformly continuous. The exponential function $ x\mapsto e^{x}$ is continuous everywhere on the real line but is not uniformly continuous on the line, since its derivative tends to infinity as $x\to \infty$ We know that  a differentiable function $f:\Bbb{R}\to\Bbb{R}$ is Lipschitz iff $\|f'\|<\infty$ . $f$ Lipschitz implies $f$ is uniformly continuous. $f$ uniformly continuous but $f$ may not be differentiable $( x\mapsto{|x|})$ . Even when $f'$ exists, $f'$ need not be bounded $(x\mapsto{\sqrt{x}} $ on $(0, \infty) ) $ Please elaborate the quoted portion.","['uniform-continuity', 'soft-question', 'derivatives', 'real-analysis']"
4654695,"In a deck of cards, do a queen and a non-queen have equal chances of being the card that follows the first queen?","I encountered the following problem: From a shuffled deck the cards are flipped one by one until a queen shows up. Then again a card is flipped. Is it more likely that this card is the Jack of Spades or the Queen of Hearts? I managed to solve this by first finding the probability that the card will be the Jack of Spades under the extra condition that the rank of the Jack of Spades is $n$ . If $J_s$ denotes the event that the Jack of Spades is the card and $N$ denotes the rank of the Jack of Spades then: $$P(J_s\mid N=n)=\binom{52-n}3\times\binom{51}4^{-1}$$ This because under condition $N=n$ there are $\binom{51}4$ possibilities for the $4$ queens of which $\binom{52-n}3$ are favorable. Applying the hockey stick identity and the law of total probability we then find: $$P(J_s)=\frac1{52}$$ From this it can be deduced easily that also $P(Q_h)=\frac1{52}$ so apparantly the chances are equal. This amazed me because I could not find an explaining symmetry for that. My question arises from the fact that I am simply not satisfied with this solution and is: Could someone give me a ""nicer"" solution that avoids calculations and rests on something like a smart perspective of the case? I just feel that a nicer and more direct solution can be found. Thank you for taking notice of my question, which is purely an effort to enrich my intuitions on probability. Edit Thank you for comments already, but if you have a real answer then please do not hesitate to provide one. And preferably a complete one. Also: more perspectives will  imply a larger enrichment of my intuition. Thank you in advance.","['alternative-proof', 'combinatorics', 'card-games', 'intuition', 'probability']"
4654704,Functional equation and elliptic functions,"I'm sorry if this question is similar to a one already asked, I'm not aware of many basic facts about elliptic curves. Let $g$ be a meromorphic function on $\mathbb{C}$ , and $\Lambda$ be a lattice in $\mathbb{C}$ . Suppose that  for any $\lambda\in \Lambda$ , there are constants $c_{\lambda}$ such that for $z\in \mathbb{C}$ (out of poles) the following holds: $$ g(z+k\lambda)=g(z)+kc_{\lambda}$$ Is it true that $g(z)=\alpha z+R(\mathcal{S})(z)$ for some elliptic function $\mathcal{S}$ , polynomial $R$ and constant $\alpha$ ? My ideas are as follow: The field of meromorphic functions on $\mathbb{C}/\Lambda$ is $\mathbb{C}(\mathcal{S},\mathcal{S}')$ for some elliptic function $\mathcal{S}$ (for example the Weirestrass function) and $(\mathcal{S}')^2 = Q(\mathcal{S})$ for some polynomial $Q$ The assumption on $g$ implies that $h(z):=g'(z)$ is elliptic, that is $h(z)=P(\mathcal{S},\mathcal{S}')(z)$ by 1. ""Locally"" (that is for $z$ near enough from a zero $z_0$ of $\mathcal{S}$ , in order to find a determination of $\sqrt{Q(\mathcal{S})}$ ) : $$g(z)-g(z_0)=\int_{[z_0,z]} h(u) du = \int_{[z_0,z]} \mathcal{S}'(u) \frac{P(\mathcal{S},\mathcal{S}')(u)}{\sqrt{Q(\mathcal{S}(u))}} du$$ We may decompose the last integral into an integral with residu, giving $\alpha z$ , and the remainder would be a function of $\mathcal{S}$ by an apporpriate change of variable. I'm not sure because I have difficulties to write formally the statement 4. Is there a counterexample or do you know a classical proof otherwise ? Thank you very much, best regards","['complex-analysis', 'complex-geometry', 'elliptic-functions', 'elliptic-curves']"
4654793,What is the limiting mean value of the product of the exponents in the prime factorization of numbers?,"Let $n = p_1^{a_1}p_2^{a_2}\cdots p_m^{a_m}$ be the prime factorization of $n$ and let $f(n) = a_1 a_2 \cdots a_m$ . Using a heuristic argument I am able to show that the mean value of $\lim_{n \to \infty} \frac{1}{n}\sum_{k = 1}^n f(k)$ is close to $$
\bigg(2 - \frac{1}{\zeta(2)}\bigg)\bigg(N_c + \frac{1}{\zeta(2)} - \frac{2}{\zeta(3)}\bigg) - \frac{1}{\zeta(2)} + \frac{2}{\zeta(3)} \approx 1.95979
$$ where $N_c \approx 1.7052$ is the Niven's constant . Experimental data seems to be consistent with this heuristics. For $n = 10^6$ , the mean is $1.9406$ while for $n = 1.42 \times 10^{10}$ , the mean increased slightly to $1.94357$ . Question : What is the limiting value of the mean? Is there a closed form for the mean of $f(n)$ ?","['divisibility', 'number-theory', 'analytic-number-theory', 'limits', 'prime-numbers']"
4654795,Function that's nowhere equal to its derivative,"I am interested in finding complex, differentiable, non-constant functions that are nowhere equal to their derivatives. That is, I want to find functions $f:\mathbb{C}\to\mathbb{C},$ such that there exists no $z_0\in\mathbb{C}$ such that $f(z_0)=f'(z_0).$ Initially, I was looking for real functions with this property and found one fairly quickly. $f(x)=x^2+2$ is never equal to its derivative for $x\in\mathbb{R}.$ However, I haven't been able to find such a complex function. Any help in finding such a function would be appreciated.","['complex-analysis', 'calculus', 'functions', 'derivatives', 'soft-question']"
4654820,What does it mean that a process of random variables is measurable?,"Let $X:=(X_t)_{t\geq 0}$ be a process on a filtered probability space $(\Omega, \mathcal{F}, \Bbb{F}, \Bbb{P})$ where $\Bbb{F} :=(\mathcal{F}_t)_{t\geq 0}$ is a filtration. Now what does it mean for $X$ to be measurable? I know that measurability tells me that the preimage of sets in the sigma algebra are again in the sigma algebra.
But the problem is that I don't see from where to where $X$ goes and which sigma algebras the domain and codomain has. Can someone help me further? In a remark on the internet I have seen that $X$ should be $\mathcal{F}\otimes \mathcal{B}([0,\infty))$ measurable. But I don't see why.","['probability', 'stochastic-processes', 'probability-theory', 'stochastic-calculus', 'random-variables']"
4654825,Solve the differential equation : $(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0.$,"Solve the differential equation : $(x^2y-2xy^2)dx-(x^3-3x^2y)dy=0.$ My solution goes like this: This is a first order and first degree differential equation of the form $Mdx+Ndy=0,$ where $M=x^2y-2xy^2,N=-(x^3-3x^2y)=3x^2y-x^3.$ Now, since, $Mx+Ny=x^2y^2\neq 0,$ and homogeneous as well, so the integrating factor will be $\frac{1}{Mx+Ny}=\frac{1}{x^2y^2}.$ Multiplying $\frac{1}{x^2y^2}$ on both sides of the given equation we get, an exact differential equation i.e $$\frac{1}{x^2y^2}[(x^2y-2xy^2)dx-(x^3-3x^2y)dy]=0.$$ Now, the solution of this exact differential equation is $$\int [(\frac{x^2y-2xy^2}{x^2y^2})]dx+\int  \frac 3y dy=c\implies \frac xy - 2\log x+3\log y=c,$$ where $c$ is an arbitary constant. Is the above solution correct? If not, where is it going wrong ?","['calculus', 'solution-verification', 'derivatives', 'ordinary-differential-equations']"
4654838,Why does an additional -1/x term in the integrand of the Mellin transform representation of the Riemann zeta function not change its value?,"Using Mellin transforms, the product of the Riemann zeta function and the gamma function can be expressed as the following integral: $$\zeta\left(s\right)\mathrm{\Gamma}\left(s\right)=\int_{0}^{\infty}{\left(\frac{1}{e^x-1}\right)x^{s-1}dx}$$ However, I have also read sources stating that the following representation (apparently from Whittaker-Watson) is also valid for the critical strip 0 < Re(s) < 1 : $$\zeta\left(s\right)\mathrm{\Gamma}\left(s\right)=\int_{0}^{\infty}{\left(\frac{1}{e^x-1}-\frac{1}{x}\right)x^{s-1}dx}$$ It is amazing to me that the addition of the term $(-\frac{1}{x})$ in the integrand of the second representation does not change the value of the integral at all for any value of s in the critical strip! Can anyone provide a mathematical explanation for why and how this is the case?","['complex-analysis', 'riemann-zeta', 'mellin-transform', 'gamma-function']"
4654859,Weak and a.e. convergences for positive measures,"Let $1<p<\infty$ . Let $f_n$ be a sequence of (a.e.) positive functions, and $f$ a positive function, both in $\mathcal{L}^p$ . Is it true that:
If $f_n$ converges weakly to $f$ then there exists a sub-sequence $f_{n_k}$ such that $f_{n_k}$ converges a.e. to $f$ . I know that this is false without the positive constraint. Thanks for the help :).","['measure-theory', 'lebesgue-measure', 'functional-analysis']"
4654876,Use algebra to decide whether a continuous solution exists,"My problem , put in elementary terms, is: (let $\mathbb{k}$ denote the field of complex numbers, $\mathbb{k}^n$ is endowed with the usual topology) Let $p_1,p_2,...,p_r \in \mathbb{k}[x_1,...,x_n]$ and denote by $J$ the ideal  they generate. Also denote by $m_0$ the ideal of all polynomials that vanish at $0$ . I need to know that: $$f_1(0)=c_1,...,f_r(0)=c_r$$ $$p_1(x)f_1(x)+...+p_r(x)f_r(x)=0$$ Admits  a solution $f_1,...,f_r$ of continuous functions iff it admits a solution in polynomials (or put in other words: iff $c_1p_1(x)+...+c_rp_r(x)\in m_0J$ ) Less restrictive is the following question:
Consider the following quantity for a set of generators of $J$ : $|J|(x)=|p_1(x)|+...+|p_r(x)|$ (for any other set of generators the obtained norm quantities are equivalent for $x\to 0$ ). Is it true that the subideal of $J$ of $q(x)$ such that $\lim_{x\to 0} \frac{|q(x)|}{|J|(x)}=0$ is the same as the subideal of $q\in J$ such that $q^N \in m_0 J^N$ for some $N$ . More generally let $X$ be an affine variety that contains zero. $$f_1(0)=c_1,...,f_r(0)=c_r$$ $$p_1(x)f_1(x)+...+p_r(x)f_r(x)=0 \, x\in X$$ Admits a solution of continuous functions on X in the subspace topology ( which can be seen as the restriction of continuous functions due to tietze ) iff it admits a solution $f_1,...,f_r \in \mathbb{k}[X] $ ( or put differently $c_1p_1(x)+...+c_rp_r(x)\in m_0J$ in $\mathbb{k}[X]$ ) (For example $(x_1+x_2+x_3)f_1+(x_1x_2+x_2x_3+x_3x_1)f_2+x_1x_2x_3f_3=0$ admits no continuous solution unless c_1,c_2,c_3=0, also note that if one of $p_j(0)\neq 0$ then it’s a trivial exercise). I will be content with any other algebraic description given, also is there any reference that addresses these questions.","['general-topology', 'algebraic-geometry']"
4654899,"Prove that for $0< k \leq 1$ and $x \geq k$, $\ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x}$","I've verified that this inequality holds when graphed, but I have no idea how to prove the actual statement. I tried to use the Mean Value Theorem but that only gives me $$\ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x+\frac{1}{e-1}-k},$$ which is too weak of an inequality since $\frac{1}{e-1}-k$ can be less than $0$ . Any help with this would be appreciated.","['analysis', 'real-analysis', 'calculus', 'inequality', 'derivatives']"
4654925,"Ash & Novinger - Complex Variables - Defining a continuous argument, walking around the unit circle","I am self studying Ash & Novinger's Complex Variables. Before I present my question, here's a definition: Let $S \subset \mathbb C$ and $f : S \to \mathbb C \setminus \{ 0 \}$ be continuous. A function $\theta : S \to \mathbb R$ is said to be a continuous argument of $f$ if $\theta$ is continuous on $S$ and $f(s)=\lvert f(s) \rvert e^{i\theta (s)}$ for each $s \in S$ . In Example 3.1.5 (c) , the authors claim that $f : \{ z : \lvert z \rvert =1 \} \to \mathbb C$ does not have a continuous argument. They initially present a very handwavy argument and postpone its proof after they develop the necessary tools to prove it. Here's how the ""handwavy proof"" goes: The intuition underlying the above example is that if we walk entirely around the unit circle, a continuous
argument of $z$ must change by $2\pi$ . Thus the argument of $z$ must abruptly jump by $2\pi$ at the end of the trip, which contradicts continuity. Since this is handwavy, I accept it. Later, they go on to prove that: If $\gamma : [a,b] \to \mathbb C \setminus \{ 0 \}$ is continuous, then $\gamma$ has a continuous argument. I read the proof of this, it seems perfectly fine. In particular if I consider $\gamma : [0, 2\pi] \to \mathbb C$ given by $\gamma(t)=e^{it}$ is a ""zerofree"" continuous function and by the above theorem has a continuous argument. This seems a little counterintuitive to me. The images of $f$ and $\gamma$ are the same. How does the ""handwavy"" walking around the unit circle fail in this case?",['complex-analysis']
4654950,Proving unif. conv. of $\lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right)$,"I am currently working on trying to prove that $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \sum_{k=0}^\infty \frac{1}{k!}.$$ Here is my progress so far. $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n \frac{n!}{k! (n-k)!} \left(\frac{1}{n} \right)^n$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \cdot \frac{n}{n} \cdot \frac{n-1}{n} \cdot \frac{n-2}{n} \cdots \frac{n-(k-1)}{n}$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ Since for all $k > n$ , we have $\left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) = 0$ , since one of the factors of the form $\left( 1 - \frac{i}{n} \right )$ is guaranteed to be zero when $k > n$ , we can make the sum go to infinity without affecting the result. $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^\infty \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ At this point, I'd like to be able to swap the limits , which would allow the summation to reduce to $\sum \frac{1}{k!}$ as desired. To my knowledge, this is only possible when the conditions of the Moore-Osgood Theorem are satisfied, which say that if $f(m,n) = \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right)$ then we must have either $\lim_{m\rightarrow \infty} f(m,n)$ or $\lim_{n\rightarrow \infty} f(m,n)$ be uniformly convergent in order to be able to swap the $m \rightarrow \infty$ and $n \rightarrow \infty$ limits above. If we define $g(n) = \sum_{k=0}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$ , it is fairly evident that $\lim_{m \rightarrow \infty} f(m,n) = g(n)$ pointwise, but only uniform convergence would be sufficient for the Moore-Osgood Theorem. To show uniform convergence here, one would need to prove that $\forall \varepsilon > 0 : \exists M : \forall m \geq M : \forall n$ we have that $|f(m,n) - g(n)| < \varepsilon$ . I managed to deduce that $$|f(m,n) - g(n)|= \sum_{k=m+1}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \text{if } m< n \text{ and } 0 \text{ otherwise.}$$ At this point I feel like I must be really close, but my brain is a bit fried trying to figure out how to show that this is less than $\varepsilon$ for any arbitrary choice of $\varepsilon > 0$ under all possible values of $m \geq M$ and $n$ for a chosen $M$ . Is this even possible and could anyone provide a possible next step or hint of where to go from here?","['limits', 'uniform-convergence', 'eulers-number-e']"
4654974,How to get 5 points of an ellipse which is internaly tangent to two congruent intersecting circles.,"Let two circles $C$ and $C’$ intersecting at points $A$ , $B$ . I would like to construct an ellipse passing through $A$ and $B$ using the $5$ points construction of GeoGebra (foci unknown). The problem is that the ellipse must have two points of tangency ( $E$ with $C$ and $F$ with $C’$ ).
Do you know a method that uses inversion ? Is there a way to impose this tangency ?
Many thanks to any hint/help.","['conic-programming', 'geometry']"
4654987,Best strategy for a 'linearized' coupon collector problem,"The following problem is motivated by a certain aggravating side-quest in an old video game. The side-quest is itself a game which follows a sequence of rounds; each round can be a success or a failure, and the game ends after $N$ successes. In its simplest form, we take the probability of failure after $k$ successes to be $k/N$ , in which case we recover the coupon collector's problem . (In the original setting the failure of probability is rounded up to the nearest whole percentage less than 100%, e.g., 84.5% rounds to 85% but 99.5% rounds to 99%; I chose not to include this detail for simplicity.) Thus far there is nothing for the player to do besides keep drawing until success. To add a strategic element, we include a base cost for each round (one ""token""). Furthermore, we allow the player to bet multiple tokens: each token past the first reduces the probability of failure by 1%, to a minimum of 0%. Here the analogy with the coupon collector's problem breaks down: drawing multiple coupons can lead to more than one new coupon being drawn, and the probability to get any new coupons would moreover not be a linear function of the number already found. So this is a coupon collector's problem where the probabilities have been 'linearized'. The question is then what strategy to pursue, and this will very much depend on what goal we pursue. If what we want to minimize is the number of rounds, then we should always bet enough tokens to eliminate the chance of failure and thus finish in $N$ rounds. But these bets grow linearly and thus the total number of tokens required scales as $N^2$ . On the other hand, we can simply bet the one-token minimum each round. In that case the number of tokens is the same as the number of rounds, which scales as $N\log N$ per the coupon collector's problem. This
strategy therefore minimizes the token count at the cost of more rounds, especially near the end when the probability of failure is quite high. This last point suggests that a reasonable tradeoff between rounds and tokens can be achieved by a strategy which interpolates between the two outlined above: Bet the minimum initially, but transition towards higher bets as the success probability shrinks. But it's not at all clear to me what an 'optimal' strategy is, or even how to properly formulate the tradeoff, and I would appreciate some perspective on how this might best be framed.","['statistics', 'coupon-collector', 'probability']"
4655015,Find functions $f\left(x_{1}x_{2}\right)+f\left(x_{2}x_{3}\right)+\ldots+f\left(x_{n}x_{1}\right) \ge n \cdot f\left(x_{1}+x_{2}+\ldots+x_{n}\right)$,"Find all functions for which it exists $n$ a natural number greater or equal than $4$ for which it is true: $$f\left(x_{1}x_{2}\right)+f\left(x_{2}x_{3}\right)+\ldots+f\left(x_{n}x_{1}\right) \ge n \cdot f\left(x_{1}+x_{2}+\ldots+x_{n}\right)$$ regardless of the choice of real numbers $x_i$ . I think that this condition is simillar to a condition of convexity, therefore I should find a way to reduce the discussion to Jensen's inequality by making a renotation, for instance $y_1 = x_1x_2$ , $y_2 = x_2x_3$ etc. However I didn't manage to find a way to reach a result.","['functional-equations', 'inequality', 'functions', 'convex-analysis']"
4655084,Finding an example of a degree 5 rational curve in $\mathbb{P}^3$ which is not in a quadric.,"This is exercise IV.6.2 in Hartshorne, which asks us to find an example of a nonsingular rational degree $5$ curve in $\mathbb{P}^3$ which is not contained in a quadric. I'm somewhat at a loss for how to do this. Similar problems in this section relate these kind of questions to intrinsic properties of the curve (like having certain linear systems, etc.), but this certainly cannot work here, since all rational nonsingular curves are isomorphic. Can anyone offer me any hints as to how I can find such a curve? Is it just a matter of trying out different parametrizations? I also thought there was maybe some kind of moduli dimension argument here, but that also doesn't seem to work, since the moduli of rational nonsingular curves is just a point. Thanks! PS: In a solutions document I saw someone claim that $(s: t) \mapsto (s^5: s^4t: s^4t + as^3t^2: t^3)$ which could very well be right, but I don't really understand how one can find such curves.","['algebraic-curves', 'algebraic-geometry']"
4655088,"Waiting time pattern in Levin , Peres","In Levin and Peres, Markov chains and Mixing Times , under the section 17.3.2 Waiting time patterns, we are looking for the expected waiting time before the patter $101$ appears upon tossing fair coins. To define a Martingale, the author defines $A_{t}^{s}=\begin{cases}1\,\,,t=s \,\\ -2\,,t<\tau_{101},t=s+1\\4\,,t<\tau_{101},\,t=s+2\\0\,,\text{otherwise}\end{cases}$ Then defines $N_{s}^{t}=\sum_{r=1}^{t}A_{r}^{s}B_{r}$ where $(B_{r})_{r}$ is an iid sequence of symmetric bernoulli random variables taking values $-1$ and $1$ with equal probability. Then I don't understand that how $N_{s}^{\tau_{101}}$ is the profit of the $s-$ th gambler after $\tau_{101}$ games for $s<\tau_{101}$ say for $s=1$ I can clearly see by the gambling game that the profit of the $1-$ st gambler is $-1$ . But say $\tau_{101}(\omega)=6$ . Then $\sum_{r=1}^{6}A_{r}^{1}(\omega)B_{r}(\omega)=B_{1}(\omega)-2B_{2}(\omega)+4B_{3}(\omega)$ . Then say the sequence is $0,0,1,1,0,1$ for this particular $\omega$ . Then $N_{1}^{\tau_{101}(\omega)}=-1+2+4$ and not $-1$ as is claimed. Can anyone tell me where I am going wrong? EDIT: I think we can fix it by modifying $A_{t}^{s}$ as follows:- $A_{t}^{s}=\begin{cases}1\,\,,t=s \,\\ -2\,,t<\tau_{101},t=s+1,B_{s}=1\\4\,,t<\tau_{101},\,t=s+2,B_{s+1}=0\\0\,,\text{otherwise}\end{cases}$ Then as the author defines $A_{r}=\sum_{s=r-2}^{r}A_{r}^{s}$ is still predictable as $A_{r}^{r-2}$ , $A_{r}^{r-1}$ are measurable wrt $\sigma(B_{1},...,B_{r-1})$ and $A_{r}^{r}$ is just the constant $1$ which is always measurable.","['martingales', 'markov-process', 'probability-theory', 'probability']"
4655130,When can we interchange x and y in a double integral?,"I understand the principle of substituting variables in a double integral and using the Jacobian to multiply by the correct factor, but if I have a double integral in x and y, when is it justified to be able to interchange the x and y terms? For example in the following integral: $$\int \int \frac{1+2x^2}{1+x^4+6x^2y^2+y^4} -\frac{1+y^2}{2+x^4+y^4}dxdy$$ integrated over $x^2+y^2\leq R^2$ for some positive real $R$ . Can I interchange the x and y terms without changing the value of the integral? I.e. is this integral equal to the following integral: $$\int \int \frac{1+2y^2}{1+y^4+6x^2y^2+x^4} -\frac{1+x^2}{2+x^4+y^4}dxdy$$ (x and y have been interchanged). If yes is it because the region where it is integrated in is circular thus independent of x and y? I am having a tough time understanding when we can do this and when we can't.","['integration', 'improper-integrals', 'analysis']"
4655186,"For which values of the parameter, $f(x, y)$ is differentiable at $(0, 0)$?","For which values of $\alpha$ , the following function is differentiable at the origin? $$f(x, y) = \begin{cases} \frac{x^2y}{(x^4+y^2)^{\alpha}} & (x, y) \neq (0, 0) \\\\ 0 & (x, y) = (0, 0)\end{cases}$$ attempts I know that continuity is not a sufficient condition for a function to be differentiable at a point, yet it's necessary because if $f$ is not continuous then it's not differentiable (at a point). Then I at least need the continuity, when restricting to the lines though the origin $y = mx$ I get $$\lim_{x\to 0} f(x, mx) = \lim_{x\to 0} \frac{mx^3}{(x^{2\alpha}(x^2+m^2)} = 0 \quad \iff \alpha < \frac{3}{2}$$ Now, I know that for some $f$ , if $\exists$ the partial derivatives at a point of $f$ and they are continuous too, then $f$ is differentiable at that point. Is there a rapider way to check for the differentiability when $f$ depends on a parameter, or should I check for existence and continuity of $\partial f$ ? Thank you.","['multivariable-calculus', 'derivatives', 'real-analysis']"
4655198,Baby Rudin Theorem 1.19 Step 5,"I have no background in pure mathematics, and I'm trying to learn how to be more rigorous in general. To help with this, I am trying to make everything more explicit as I progress through Rudin's Principles of Mathematical Analysis. I am currently going through theorem 1.19's proof where Rudin constructs $\mathbb{R}$ from $\mathbb{Q}$ . In particular, in step 5, Rudin states that it is obvious that if $\alpha , \beta , \gamma \in \mathbb{R}$ and $\alpha < \beta$ , then $\alpha + \gamma < \beta + \gamma$ . So far, the book has defined $\alpha , \beta , \gamma$ as subsets of $\mathbb{Q}$ called cuts, and has shown that cuts respect the field axioms of addition. I am intuitively convinced that the set $\beta + \gamma$ contains elements that $\alpha + \gamma$ does not, but am struggling with how to formalize it. The following is my thought process on my (failed) attempt. I know that: $\beta > \alpha \implies \exists x \in \beta : x \notin \alpha$ , and I think the next step is the take some value $y \in \gamma$ and show that $y + x \in \beta + \gamma$ while $y + x \notin \alpha + \gamma$ . I originally thought this would work by taking $y = \sup \gamma$ , but $\sup \gamma$ is a set, not a rational number. When I try to think about taking $y < \sup \gamma$ (edit: sorry, this should say $y \in \gamma$ not $y < \sup \gamma$ ), it's clear to me that $y + x \in \beta + \gamma$ , but it's no longer clear to me that $y + x \notin \alpha + \gamma$ .
Any help would be greatly appreciated!","['real-numbers', 'proof-writing', 'rational-numbers', 'real-analysis']"
4655200,Can we use Burnside's lemma to count partitions of an integer?,"I am currently learning group theory and find that Burnside Lemma can be used to calculate partition number, although not that efficiently. Suppose we want to distribute $n$ different balls into $n$ different boxes. Each way of doing this corresponds to a function from $\{1,2,...,n\}$ to $\{1,2,...,n\}$ . Let $X$ consist of all these functions. But the partition number is the number of ways when the balls and the boxes are all identical. To achieve this, we use Burnside’s lemma. Let $S_n \times S_n$ act on $X$ by $\displaystyle{ \left( a , b \right) f = a \circ f \circ b }$ . Then each orbit is a partition. For example, in the case when $n=2$ , $\displaystyle{ \mid X |= 4 }$ and $\displaystyle{ \mid S |= 4 }$ . The identity in $\displaystyle{ S }$ has $4$ fixed points. $\displaystyle{ \left( \left( 1 \  2 \right) , \left( 1 \  2 \right) \right) }$ has 2 fixed points. $\displaystyle{ \left( 1 , \left( 1 \  2 \right) \right) }$ has 2 but $\displaystyle{ \left( \left( 1 \  2 \right) , 1 \right) }$ has $0$ . Thus the number of partitions is $\displaystyle{ \frac{ 4 + 2 + 2 }{ 4 } = 2 }$ . My question is whether there exists a more efficient and general method than enumerating all elements in $S_n \times S_n$ to find their fixed points. For example, can we reduce the enumeration to only elements in $S_n$ ? Are there any related results?","['integer-partitions', 'group-theory', 'polya-counting-theory', 'combinatorics']"
4655221,Reference request: infinitary Ramsey theory,"I was reading about the (various) infinite versions of Ramsey's theorem, and stumbled across a text containing a proof of the Bolzano-Weierstrass Theorem using it: Unfortunately, I forgot where I found this article. It explained the whole topic very well and stated Ramsey's (infinite) theorem in a very clear way. Does anyone recognize this text by chance? I searched this for a very long time. I'd be grateful for similar reading recommendations; that is, ""set-theoretic"" approach to the infinite Ramsey theorem and infinite graphs in general.","['infinitary-combinatorics', 'reference-request', 'ramsey-theory', 'discrete-mathematics', 'set-theory']"
4655233,"Polynomial that grows faster than any other polynomial outside $[−1,1]^n$","Consider this statement: ""Chebyshev polynomials increase in magnitude more quickly outside the range $[−1,1]$ than any other polynomial that is restricted to have magnitude no greater than one inside the range $[−1,1]$ ."" In the multivariate case, is there a degree $d$ polynomial that lies in $[-1,1]$ in the interval $[-1,1]^n$ and grows faster than any other polynomial of degree $d$ outside the $[-1,1]^n$ ? If so, what is this polynomial? It is natural to conjecture that $q(x_1,..,,x_n) = T_{d/n}(x_1)T_{d/n}(x_2)..T_{d/n}(x_n)$ (assuming $n|d$ ) is this polynomial ( $T_i(x)$ is the degree $i$ Chebyshev polynomial). For the univariate case, a proof can be found here . I have tried to generalize this proof to the bivariate case, but the fact that bivariate polynomials can have infinitely many roots (while univariate polynomials can have only at most degree-many roots) seems to be a barrier in adapting the proof.","['chebyshev-polynomials', 'approximation-theory', 'real-analysis', 'polynomials', 'numerical-methods']"
4655240,Value of expression $(\alpha^{110}+\beta^{110})-(\alpha^{98}+\beta^{98})$ in given quadratic equation,"If $\alpha,\beta$ are the roots of the equation $x^2+3^{\frac{1}{4}}x+3^{\frac{1}{2}}=0$ .
Then value of $\displaystyle \alpha^{98}(\alpha^{12}-1)+\beta^{98}(\beta^{12}-1)$ is From $\displaystyle x^2+3^{\frac{1}{4}}x+3^{\frac{1}{2}}=0$ ,we have $\displaystyle \alpha+\beta=-3^{\frac{1}{4}}$ and $\displaystyle \alpha\beta=\sqrt{3}$ Now we have to find value of $\displaystyle (\alpha^{110}+\beta^{110})-(\alpha^{98}+\beta^{98})$ I did not understand how can I write above expression into sum and product of roots Please have a look",['algebra-precalculus']
4655283,Proving that $\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6$,"Yes, you read that correctly. It can be proven that $$\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6$$ Where $H_x$ is the $x$ th harmonic number and is analytically continued to all positive reals. To prove this, we use a formula in the OP of this question where it is stated that $$H_x=\sum_{k=1}^\infty\frac{x}{k(x+k)}$$ And the rest is obvious. But this proof is very boring and not intuitive. So I want to see other methods of proving this limit.","['analysis', 'real-analysis', 'harmonic-numbers', 'calculus', 'limits']"
4655291,How should I integrate this function?,"Thank you for your precious time! Here is the integration I want to calculate: $$\int_{0}^{2 \pi} \int_{0}^{2 \pi} e^{j 2 \pi\left[ \cos \theta- \cos \varphi+ \cos \left(\theta-\varphi\right)\right]} e^{j \theta} e^{-j \varphi} d \theta \, d \varphi$$ I think this integration is related to Bessel functions of the first kind, when we remove the term $\cos(\theta-\varphi)$ above : $$\int_{0}^{2 \pi} \int_{0}^{2 \pi} e^{j 2 \pi\left[ \cos \theta- \cos \varphi\right]} e^{j \theta} e^{-j \varphi}\, d \theta d \varphi=\int_{0}^{2 \pi}e^{j (2 \pi \cos \theta+\theta)} d\theta \int_{0}^{2 \pi}e^{-j (2 \pi \cos \varphi+\varphi)}\, d\varphi$$ With the bessel integration: $$
J_{n}(x)=\dfrac{1}{2 \pi} \int_{-\pi}^{\pi} e^{j(n \tau-x \sin \tau)}\, d \tau
$$ The integration would be easy to get. But due to the $\cos(\theta-\varphi)$ term in the desired integration, I still can't find a good method. Could you give me any advice? Any help would be appreciated! -----------------------------------------2023/3/11------------------------------------------ According to @Semiclasical's suggestion, I tried: $$e^{j c \cos \left(\theta-\varphi\right)}=\sum_{n=-\infty}^{\infty} j^{n} J_{n}\left(c\right) e^{i n\left(\theta-\varphi\right)}$$ So the integration would be: $$\sum_{i=-\infty}^{\infty}J_n(c)j^n\int_{0}^{2 \pi} \int_{0}^{2 \pi} e^{j\left(a \cos \theta-b \cos \varphi\right)} e^{j\left(1+n\right) \theta} e^{-j\left(1+n\right) \varphi} d \theta d \varphi$$ And then it becomes: $$\sum_{i=-\infty}^{\infty} \dfrac{4 \pi^{2}}{j^{-n}} J_{n}\left(c\right) J_{1+n}\left(a\right) J_{-\left(1+n\right)}\left(b\right)$$ I think this is useful. But the integration becomes the summation of series. To be more specific, the integration becomes the summation of Bessel functions. It's still hard to get an analytical formula. I tried to truncate this series, but the accuracy severely degraded. So for now, I think I should try something else.
And currently, I'm planning to give Meijer-G function a try. Could you give me more suggestions? Any help is appreciated! -----------------------------------------2023/3/12------------------------------------------ I can't find useful Meijer-G functions for me. But I find that the equation below is actually very accurate(even after truncation): $$\sum_{i=-\infty}^{\infty} \dfrac{4 \pi^{2}}{j^{-n}} J_{n}\left(c\right) J_{1+n}\left(a\right) J_{-\left(1+n\right)}\left(b\right)$$ So I decide to stick on the sum of bessel functions. Since it's hard to get the bessel function summation directly. I used the asymptotic form of bessel functions: $$
J_{\alpha}(z) \sim \frac{1}{\Gamma(\alpha+1)}\left(\frac{z}{2}\right)^{\alpha}
$$ Then we can transform bessel series into power series. The summation might be easier. This should work only when $z$ is very small since this asymptotic form of bessel function only works when $z$ is very small. Could you give me any advice? Any help would be appreciated!","['integration', 'special-functions', 'complex-analysis', 'complex-numbers', 'bessel-functions']"
4655389,Prove side $c$ of a Triangle equals $\frac{0}{2}$,So in $\triangle ABC$ with angle $\theta$ we can calculate $$c^2 = a^2  + b^2 - 2ab\cos \theta $$ $$\cos\theta = \frac{a^2 + b^2 - c^2}{2ab} $$ $$\Longrightarrow$$ $$c^2 = a^2  + b^2 - 2ab\frac{a^2 + b^2 - c^2}{2ab}$$ $$2ab\frac{a^2 + b^2 - c^2}{2ab} = a^2 + b^2 - c^2$$ $$\Longrightarrow$$ $$c^2 = a^2  + b^2 - a^2 + b^2 - c^2$$ $$c^2+c^2 = a^2  + b^2 - a^2 + b^2$$ $$2c^2 = a^2  + b^2 - a^2 + b^2$$ $$c = \sqrt{\frac{0}{2}}$$ How did this happen? I'm sure I did something wrong. EDIT: I just forgot the brackets $$c^2 = a^2  + b^2 - (a^2 + b^2 - c^2)$$,"['triangles', 'geometry']"
4655455,Are there nonconstant smooth families of hypersurfaces over the affine line?,"Fix $d\in\mathbb{N}$ . Let $S=\mathbb{A}^1_{\mathbb{C}}$ be the affine line over the complex numbers and let $X\subset\mathbb{P}^n_S$ a family of smooth hypersurfaces of degree $d$ over $S$ . By this I mean that $X$ is a closed subscheme of $\mathbb{P}^n_S$ such that the fiber $X_s$ of $X\to S$ over any closed point $s$ of $S$ is a smooth hypersurface of degree $d$ in $\mathbb{P}^n_{\mathbb{C}}$ . I say that the family $X$ is essentially constant if for any two  closed points $s,s'$ of $S$ there is a linear change of coordinates of $\mathbb{P}^n_{\mathbb{C}}$ that sends $X_s$ to $X_{s'}$ . My question is: Is there a family of smooth hypersurfaces of degree $d$ over $S$ which is not essentially constant? If such a family would exist, then there would be a non-constant polynomial map $\mathbb{C}\to\mathbb{C}[x_0,\ldots,x_n]_d$ whose image does not intersect the discriminant which is not just given by applying an element of $\textrm{GL}_{n+1}(\mathbb{C}[t])$ to a certain polynomial. I was not able to construct such a map even for $n=1$ and $d=4$ which is probably the first non-trivial case.","['algebraic-geometry', 'deformation-theory']"
4655463,"Composition between mixed functions (scalar, vector...)","Consider $$f(x, y) = x^2+y^2 \qquad \qquad g(t) = (2t, t^2)$$ So $f: \mathbb{R}^2 \to \mathbb{R}$ and $g: \mathbb{R}\to \mathbb{R}^2$ . I know that then $g\circ f: \mathbb{R}\to \mathbb{R}$ . If instead I want $f\circ g$ , in this case I can do it because it's like to write $g: \mathbb{R}\to \mathbb{R}^2$ and $f: \mathbb{R}^2 \to \mathbb{R}$ , so then $$f\circ g: \mathbb{R}^2\to \mathbb{R}^2$$ But in the end, I don't really know how to compose. I don't get it at all. perpahs is it $$f\circ g = f(g(t)) = (2t)^2 + (t^2)^2$$ ?? And is it $$g\circ f = g(f(x,y)) = 2t^2 + t^4$$ ??
Help me thank you!","['multivariable-calculus', 'calculus', 'functions', 'real-analysis']"
4655478,L'Hopital's Rule for nested exponential function,"I am trying to look for the following limit: $$\lim_{x\to\infty} (xe^{1/x} - x)^x$$ I re-wrote the limit in the following way $$\exp\left( \lim_{x\to\infty} x\ln \left[ x(e^{1/x} -1) \right]  \right)$$ and I look for the limit of the exponent using L'Hopital's rule (beforehand, I changed the $x$ term adjacent to the $\ln$ term so that it becomes $1/x$ ). However, I only end up with more complicated expressions as I repeatedly use L'Hopital's rule (I checked first if the resulting function after applying L'Hopital's is of the form $0/0$ or $\infty/\infty$ ). Is there more efficient way to solve this?","['limits', 'infinity', 'exponential-function', 'analysis']"
4655491,Confusion about having limit value an given point,"Today , me and my friend discussed on a question which is about whether there is limit in a given point.To answer this , we assumed that let $f$ be a function such that $f:{1} \rightarrow {4} ,f(1)=4$ ,i.e, its domain is only $1$ and image is only $4$ , but these values are isolated. Thanks to this function , we write the point $(1,4)$ in cartesian coordinate system. I said that because of our $x$ value is an isolated number ,i.e $1$ , we cannot close to $1$ because we do not have $f(x)$ values for those values. However , my friend said that we have a limit here and the limit value is $1$ by using $\epsilon - \delta$ definition. After this discussion ,we went to our professors , one of them firstly said that there is no limit of a point , but after that she hesitated about it and wanted some time for thinking. The other professor said that we can find limit value for the given point using epsilon-delta. I am confused here , because i learned that limit is the behavior of a function in given x value when we get close to (but not equal) this point. Here , our function has domain $1$ , so we cannot talk about $1.000000...1 $ or $0.9999..$ . Briefly , what do you think about whether a given point has a limit value there ? If it has limit value can you prove it using epsilon delta and for point $(1,4) $ ? Addentum: By formal definition ; For the function f(x) defined on an interval that contains $x =a$ . Then we say that, $\lim_{x \to a}f(x) = L$ If for every number epsilon( $∈$ ) which greater than zero, there is some positive number delta $\delta$ such that, $| f(x) – L | < ∈$ where $0 < |x – a| < \delta$ Source : https://www.geeksforgeeks.org/formal-definition-of-limits/ So , we see that to be able to use epsilon-delta , we must have a defined interval , but we do not have any defined interval except for $x=1$ isolated point. Then how dare can you use epsilon delta ?","['real-analysis', 'calculus', 'definition', 'limits', 'general-topology']"
4655503,Does the set of diffeomorphisms which are induced by flows form a group?,"Let $M$ be a smooth manifold. 
Consider the set of diffeomorphisms which are induced by flows of vector fields. (which are not time-dependent) Is this set a subgroup of $\text{Diff}(M)$? (Note that not every diffeomorphism which is isotopic to the identity is induced by a flow of a vector field, see here for details). ""A naive attempt"": Maybe it's possible to construct a counter-example when taking $M=\mathbb{S}^2$. Every vector field on $\mathbb{S}^2$ vanishes at some point, hence every flow-diffeomorphism has a fixed point. Maybe we can find two vector fields, such that the composition of their flows is a diffeomorphism without fixed points.","['differential-topology', 'smooth-manifolds']"
4655513,Finding the matrix of $f$ in the base $V$,"Problem . Let $V = {\vec{v_1} = (5, 3, 1), \vec{v_2}= (1, −3, −2), \vec{v_3} = (1, 2, 1)}$ base of $\mathbb{R^3}$ and let $f$ be the linear operator on $\mathbb{R^3}$ defined
by $f(\vec{v_1}) = (−2, 1, 0)$ , $f(\vec{v_2}) = (−1, 3, 0)$ , $f(\vec{v_3}) = (−2, −3, 0)$ . Calculate the matrix of $f$ in the base $V$ ; How I was taught the lesson . Now the issue is that in my book and all of my notes they always define the matrices of change of base in this way: $$y'=F_v x'$$ for the matrix $F_v$ of $f$ in a base $V$ and the matrix $P$ to change basis from the canonical to the base $V$ : $$x'=Px$$ (I'm using ' for the base $V$ and no ' for the canonical). How I approached the problem . So I continued doing the problem: $$x'=Px\Rightarrow x=P^{-1} x'$$ since we were given the $y'$ in terms of $x$ (the canonical base) because $f(\vec{v_1})=-2\vec{e_1}+\vec{e_2}$ , $f(\vec{v_2})=-\vec{e_1}+2\vec{e_2}$ and $f(\vec{v_3})=-2\vec{e_1}-3\vec{e_2}$ , we have found that matrix $M$ such that $y'=Mx$ , so then: $$y'=Mx=M(P^{-1}x')=MP^{-1}x'=F_v x'$$ which I found the solution to be $F_v=MP^{-1}$ but my book now says it is $P^{-1}M$ , how is that possible? The way they approached the problem . The same was as I did, but instead of $y'=Mx$ they did $y'=xM$ , and likewise with $x'=xP$ changing the order of the matrix and putting it in the right? And they arrive to $F_v=P^{-1}M$ . My doubt . What happened? Have I done something wrong or is it just that both answers are valid? And why did they put the matrix on the right and not on the left, does it not matter as long as you are consistent with it?","['matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
4655567,Rudin 3.29 the series $\sum_{n=3}^{\infty} \frac{1}{n \ln n(\ln(\ln n))^2}$ converges,"In Rudin's Principles of Mathematical Analysis, we claim (with no proof of it) that $$\sum_{n=3}^{\infty} \frac{1}{n \ln n(\ln(\ln n))^2}$$ is convergent. After applying the Cauchy compression test (the term of the series is a decreasing, positive sequence) I get: $$\sum_{n=3}^{\infty} \frac{1}{n \ln n(\ln(\ln n))^2} = \frac{1}{\ln 2} \sum _{n=2}^{\infty} \frac{1}{n} \frac{1}{(\ln n + \ln (\ln 2))^2}$$ Do we have to use a certain majoration of the term of the series and conclude by the comparison test ? (since $\ln (\ln 2)< 0$ it seems quite impossible) Do we have to use these considerations ? (they seem a bit too difficult to understand for me) Or is there a $3^{rd}$ way to prove the convergence ?","['convergence-divergence', 'sequences-and-series']"
4655577,A polynomial with unique root in every $ \mathbb Z _ n $,"Let $ p ( x ) \in \mathbb N [ x ] $ be a polynomial with nonnegative integer coefficients, and $ a \in \mathbb Z $ be a given integer constant. If for all positive integers $ n $ , $ p ( x ) + a $ has a unique root in the finite ring $ \mathbb Z _ n $ , can we conclude that $ p ( x ) $ must be linear? This problem comes from another one that I'm working on, and can be considered as an special case weaker than the original problem. While working, I realized that I have no method for solving this special case either, and I need to become sure that it holds, first. The intuition behind why I think $ p ( x ) $ must be linear comes from Schur's conjecture (it is now a theorem in fact, yet still widely referred to by this name). If instead of having a unique root, $ p ( x ) + a $ were supposed to be a permutation polynomial , then by the conjecture it should have been the composition of linear functions and Dickson polynomials (of the first kind). But since all the coefficients of $ p ( x ) $ are nonnegative, and Dickson polynomials have a certain pattern of alternating signs in coefficients, it seems that $ p ( x ) $ must be linear (I haven't checked this in full detail, but it seems to be true). That made me more confident in believing that the nonnegativity of the coefficients of $ p ( x ) $ might be a strong enough condition to add to the weaker assumption of $ p ( x ) + a $ having a unique root, to get the same result. Also note that while the assumption of Schur's conjecture is that the polynomial is a permutation polynomial in the finite field $ \mathbb Z _ p $ for infinitely many prime numbers $ p $ , here we have the unique root property in $ \mathbb Z _ n $ for all positive integers $ n $ , which seems to be a very strong restriction. The above mentioned theory of permutation polynomials was the closest theory to my problem in the literature that I could find searching online. I couldn't find much information on polynomials having unique roots when considered over several finite fields or rings. This may come from my unfamiliarity with the subject, of course. I find it very helpful if you could give references to parts of the literature more directly related to the problem at hand.","['number-theory', 'finite-rings', 'roots', 'polynomials']"
4655578,Why doesn't the sample variance become the population variance when the sample has the whole population,"We know that for a sample of size $n$ the sample variance is $\displaystyle S^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$ Suppose I used the whole population of size $N$ as my sample then in this case the variance I compute is $\displaystyle S^{2} = \frac{1}{N-1}\sum_{i=1}^{N}(X_{i}-\mu)^{2}$ where $\mu=\frac{\sum_{i=1}^{N}X_i}{N}$ However, the population variance is $\displaystyle \sigma^{2} = \frac{1}{N}\sum_{i=1}^{N}(X_{i}-\mu)^{2}$ where $\mu=\frac{\sum_{i=1}^{N}X_i}{N}$ and this is counterintuitive. One would think that by using the unbiased estimator of the sample variance, convergence when the whole population is considered is guaranteed as the variance of an estimator approaches $0$ as $N \rightarrow \infty$","['statistics', 'variance']"
4655584,Limit inferior of bounded sequence [duplicate],"This question already has an answer here : Proof $\limsup_{n\to\infty}\max\{a_n,b_n\} =\max\{\limsup_{n\to\infty} a_{n}, \limsup_{n\to\infty} b_{n}\}$ (1 answer) Closed last year . I found an interesting problem that I can't tackle as I am studying real analysis on my own.
Let there be bounded sequences $(a_n)$ and $(b_n)$ . Proove that $\varliminf_{n\to\infty} (\min \{ a_n , b_n \} ) = \min \{ \varliminf_{n\to\infty} a_n , \varliminf_{n\to\infty} b_n \}$ .
So far I have figured out that I should use lower limit monotonicity and then use definition of subsequential sequence. Am I on the right track? I have been stuck on this problem for quite some time.","['limsup-and-liminf', 'calculus', 'analysis', 'real-analysis']"
4655598,Calculus concept,"Let's get straight to the point. In my course we have section of calculus. So if we have a function which have vertical tangent(s). Can we evaluate it's antiderivative. As we were told derivatives DNE for discontinuity, LHD not equal RHD and vertical tangent. So will it be true?","['integration', 'calculus', 'derivatives']"
4655637,What is the right measure for the upper half-plane?,"I think my question depends on the context of the problem so let me give you some background. I want to measure subsets of $\{(x,y)|~y>0, x\in \mathbb{R}\}$ . I'm not involved with differential geometry. So what is the right measure for the upper halfplane. If one identifies it with the hyperbolic plane I saw that one possibility is $\frac{1}{y^2}dxdy$ but on the other hand this is a subset of $\mathbb{R}^2$ so with the Lebesgue measure we can also measure subsets. The motivation behind it is as follows. We want to integrate a function on some subset of $\mathbb{R}^3$ and use cylindrical coordinates, assume there is no dependence on the angle. Then the integral becomes and integral on some subset of the upper halfplane. For me, it seems natural to measure things with the Lebesgue measure. Is there a ""natural"" way to decide which measure to use?","['measure-theory', 'lebesgue-measure', 'hyperbolic-geometry']"
4655642,How do I compare $\ln(x)$ to $\ln(x - 1)$?,"When I was playing around with the formula for $\sin(\theta)$ , I found out that $\ln(2i\sin(\theta)) = \ln(e^{2i\theta}-1) - i\theta$ . Using common sense, I could derive that this would equal a little under $i\theta$ . But I would like to know the exact value, so I was wondering if there was any way to compare $\ln(x)$ to $\ln(x - 1)$ . I have tried asking Wolfram Alpha and Microsoft Math Solver , but their output did not help. Raw Wolfram Alpha input: ln(e^(2iθ)-1)-iθ Raw Microsoft Maths Solver input (translated into raw input very weirdly): \log_{ e  }({  { e  }^{ 2i \theta    }  -1  })  -i \theta (They're both the same equation.)","['trigonometry', 'logarithms']"
4655644,Finding the eigenvalues of a 9x9 matrix,"Is there any way that I can find the eigenvalues of this matrix!? I've used SymPy and tried the M.eigenvals() function but it always runs out of time. My main goal is to diagonalise it so if anyone can help that's be great. Thanks! $$
\begin{bmatrix}
    -e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & 0 & 2\sqrt{3}ie^{ik_{1}} & 0 & 0 & 0 \\
    e^{ik_{1}} & -e^{ik_{1}} & e^{ik_{1}} & e^{ik_{1}} & 0 & 0 & 2\sqrt{3}ie^{-ik_{1}} & 0 & 0 \\
    e^{-ik_{2}} & e^{-ik_{2}} & -e^{-ik_{2}} & e^{-ik_{2}} & 0 & 0 & 0 & 2\sqrt{3}ie^{-ik_{2}}  & 0 \\
    e^{ik_{2}} & e^{ik_{2}} & e^{ik_{2}} & -e^{ik_{2}} & 0 & 0 & 0 & 0 & 2\sqrt{3}ie^{ik_{2}} \\
    0 & 0 & 0 & 0 & 4 & 0 & 0 & 0 & 0\\
    2\sqrt{3}ie^{i(k_{2}+k_{1})} & 0 & 0 & 0 & 0 & -e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})}\\
    0 & 2\sqrt{3}ie^{i(k_{1}-k_{2})} & 0 & 0 & 0 & e^{i(k_{1}-k_{2})} & -e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})}\\
    0 & 0 & 2\sqrt{3}ie^{i(k_{2}-k_{1})} & 0 & 0 & e^{i(k_{2}-k_{1})} & e^{i(k_{2}-k_{1})} & -e^{i(k_{2}-k_{1})} & e^{i(k_{2}+k_{1})}\\
    0 & 0 & 0 & 2\sqrt{3}ie^{i(k_{1}+k_{2})} & 0 & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & -e^{i(k_{1}+k_{2})}\\
\end{bmatrix}$$ Edit: For context, I'm trying to do a quantum walk on a D2Q9 lattice. I have done this sucessfully. Quantum walks use coin opertors and I want to transform my coin into fourier space. The above matrix is the coin transformed into fourier space, however I need the eigenvalues and eigenvectors of this coin to get a useable operator. An approximation may not scale well when using it as a coin operator in a quantum walk which is why I'm trying to find an exact solution and not an approximation.","['matrices', 'diagonalization', 'eigenvalues-eigenvectors']"
4655673,When does the derivative of the logarithm equal the logarithm of the derivative?,"Not sure how to show proof of work, but I have several pages of notes on my bathroom floor right now. All I get is horribly complicated nonlinear DEs. Is there a magic substitution that makes differentiation and logarithm commute? What I'm asking for is solutions to: $$\dfrac d {dx} \ln( f(x) ) = \ln\left( \dfrac d {dx} f(x) \right )$$",['ordinary-differential-equations']
4655698,Probability of Trees Being Planted Correctly,"OK, so here is a real-world problem that I am actually facing, right now. I ordered ten almond trees from a nursery: 6 Texas Mission (TM), and 4 Hall's Hardy (HH) which are pollinators for each other. I have two plots that the trees are to go in, one with 6 holes and one with 4. So in the 6 hole plot I need 4 TM and 2 HH in between the TM. In the 4 hole plot I need 2 TM and 2 HH. I absolutely must have pollinators near each other (in the same plot) or they will not bear fruit and all is for naught. The problem is ... THE VENDOR DID NOT LABEL THE TREES!!! And it's looking like there's no way to tell them apart until they bloom which is too late. So ... if I just plant the trees at random in the holes, what is the probability of the following: I really do appreciate any help I can get here.",['probability']
4655699,Is there a metric that makes $\mathbb{N} \cup \{\infty\}$ a complete metric space?,"Denote the set of extended natural numbers $\mathbb{N} \cup \{\infty \}$ by $X$ . Is there any metric $D$ that makes $X$ a complete metric space? In other words a distance function $D: \mathbb{N}\times \mathbb{N} \to \mathbb{R}$ such that $D(n,m)\geq 0$ $D(n,m)= 0$ if, only if, $m=n$ $D(m,n)=D(n,m)$ $D(m,n)\leq D(m,p)+D(p,n)$ My attempt. I tried using the metric $d(m,m)= \frac{|n-m|}{1+|m-n|}$ of $\mathbb{N}$ to produce a metric on $X=\mathbb{N} \cup \{\infty\}$ . Necessarily I made some additional definitions like $|n\pm\infty|=\infty$ for all $n\in\mathbb{N}$ $|\infty\pm n|=\infty$ for all $n\in\mathbb{N}$ $1+\infty=\infty$ $|\infty-\infty|=0$ $ \frac{\infty}{\infty}=1$ Define $$D(m,n)=\sum_{k=\min(m,n)}^{\max(m,n)}\frac{1}{2^k}d(\min(m,n),k).$$ Proof of 1.( $D(m,n) \geq 0$ for all $m,n \in X$ ): Since $d(\min(m,n),k) \geq 0$ for all $k \in \mathbb{N}$ , then $D(m,n) \geq 0$ . Proof of 2. ( $D(m,n) = 0$ if and only if $m = n$ ): If $m = n$ , then we have $\min(m,n) = \max(m,n) = m$ , and $d(m,k) = 0$ for all $k \in \mathbb{N}$ . So $D(m,n) = 0$ . If $D(m,n) = 0$ , then each term in the sum is zero, and since each term is non-negative, this implies that $d(\min(m,n),k) = 0$ for all $k \in \mathbb{N}$ such that $\min(m,n) \leq k \leq \max(m,n)$ . In particular, this implies that $\min(m,n) = \max(m,n)$ , i.e., $m = n$ . Proof of 3. ( $D(m,n) = D(n,m)$ for all $m,n \in X$ ): Just note that $\min(m,n) = \min(n,m)$ and $\max(m,n) = \max(n,m)$ , so $D(m,n) = D(n,m)$ . Now the triangular inequality. But I was not able to prove the triangular inequality. In fact, I think that $D$ may not satisfy the triangle inequality.","['metric-spaces', 'analysis', 'hausdorff-distance', 'natural-numbers', 'general-topology']"
4655731,Proving that the intersection of two regular surfaces is a regular curve,"I'm having some trouble with the following problem: Let $S_1$ and $S_2$ be two regular surfaces that intersect at a point $p$ , such that $T_pS_1\neq T_pS_2$ . Prove that there is an open neighborhood $U\subseteq\mathbb R^3$ of $p$ such that $S_1 \cap S_2\cap U$ is the trace of a regular curve. My first idea was to use the fact that, because $S_1$ and $S_2$ are regular surfaces, there are open neighborhoods $W,V$ in $\mathbb R^3$ of $p$ and $f:W\to\mathbb R$ , $g:V\to\mathbb R$ differentiable, such that: $$W\cap S_1=f^{-1}(\{0\})$$ $$V\cap S_2=g^{-1}(\{0\})$$ So with this, we can conclude, if we set $U=W\cap V$ , that $S_1\cap S_2\cap U$ is defined by the following system of equations: $$f(x,y,z)=0$$ $$g(x,y,z)=0$$ This is a system with 2 equations and 3 variables, so my intuition tells me that the solution will have 1 free variable, and thus be a curve, but I have no idea how to formalize this. My second idea was the following:
If $T_pS_1$ and $T_pS_2$ intersect and are not the same, then $T_pS_1\cap T_pS_2$ is a one-dimensional linear subspace of $\mathbb R^3$ , let it be the space generated by the vector $v$ . Now we know a vector that will be tangent to the curve we want to find. Maybe with this, we can construct the curve? Does any of these ideas work? If that's now the case, how can this be done?","['curves', 'solution-verification', 'surfaces', 'differential-geometry']"
4655761,Derivative of the Dirac delta function,"So, I was reading about the Dirac delta function and how its differentiation works. So, pretty much all texts and online sources I saw, define it using the integral: $$ \int_{-\infty}^{+\infty}x\dfrac{d\delta(x)}{dx} dx= \left. x\delta(x)\right|_{-\infty}^{\infty}-\int_{-\infty}^{+\infty}\delta(x)dx$$ and here all of them say the first term is zero, as $δ(x)$ is zero at infinities. But, won't $x$ be equal to infinity at that point? Won't this mean that the first term is not zero? My point stands, even if there was a function $f(x)$ instead of $x$ in the L.H.S. of the above equation. I originally had this doubt while solving problem 1.46 in Introduction to electrodynamics by D. J. Griffiths . Any help on this matter would be much appreciated.",['derivatives']
4655762,"If $G_1$, $G_2$ and $G_3$ are subgroup of $G$ then does the equality $\big\langle⟨ G_1\cup G_2⟩\cup G_3\big\rangle=⟨ G_1\cup G_2\cup G_3⟩$ holds?","So it is a well knew result that any intersection of subgroup is a subgroup and even it is a well knew result that the union of subgroup is not generally a subgroup. However, if $\mathcal S(G)$ is the collection of subgroups of any group $G$ then for any subset $X$ of $G$ then the collection $$
\mathcal G(X):=\{Y\in\mathcal S(G):X\subseteq Y\}
$$ is not empty so that its intersection $\langle X\rangle$ is a subgroup containing $X$ which is moreover the less subgroup containing it: thus if $\mathcal H$ is a collection of subgroups then we can say that $$
\langle\mathcal H\rangle:=\Big\langle\bigcup\mathcal H\Big\rangle
$$ is the subgroup generated by $\mathcal H$ . Now I know that the equality $$
\tag{1}\label{1}\begin{equation}\langle\mathcal H\rangle=\Big\{x_1\cdots x_n:x_i\in\bigcup\mathcal H\text{ for all }i=1,\dots, n\text{ with }n\in\omega\Big\}\end{equation}
$$ holds ( right? ) so that by this I asked if even the equality $$
\tag{2}\label{2}\begin{equation}\langle G_1\cup G_2\cup G_3\rangle=\big\langle\langle G_1\cup G_2\rangle \cup G_3\big\rangle\end{equation}
$$ holds for any $G_1,G_2,G_3\in\mathcal S(G)$ . So first of all I observed that $$
G_1\cup G_2\cup G_3\subseteq\langle G_1\cup G_2\rangle\cup G_3\subseteq\big\langle\langle G_1\cup G_2\rangle\cup G_3\big\rangle
$$ holds so that I conclude that $$
\langle G_1\cup G_2\cup G_3\rangle\subseteq\big\langle\langle G_1\cup G_2\rangle\cup G_3\big\rangle
$$ However, by \eqref{1} any element of $\big\langle\langle G_1\cup G_2\rangle\cup G_3\big\rangle$ is a finite product of elements of $\langle G_1\cup G_2\rangle$ and $G_3$ so that if by \eqref{1} any element of $\langle G_1\cup G_2\rangle$ is a product of elements of $G_1$ and $G_2$ then by associativity and by \eqref{1} we conclude that $$
\big\langle\langle G_1\cup G_2\rangle\cup G_3\big\rangle\subseteq\langle G_1\cup G_2\cup G_3\rangle
$$ Finally we conclude that the equality \eqref{2} holds. So I ask if actually \eqref{2} holds because I do not know if really \eqref{1} holds since my textbook do not write it explicitly although, at least it seems, it proves it. Moreover if \eqref{2} holds then is it possibile to prove it using associativity of intersection? For sake of completeness here a reference from my textbook - in Italian, sorry. Could someone help me, please?","['group-theory', 'abstract-algebra', 'solution-verification', 'examples-counterexamples']"
4655789,"Combinatorially proving $F_{n} = \sum_{i=0}^{\left\lfloor n/2 \right\rfloor}\binom{n-i}{i}$, where $F_n$ is the $n$-th Fibonacci number [duplicate]","This question already has answers here : How to show that this binomial sum satisfies the Fibonacci relation? (6 answers) Closed last year . Prove the following combinatorially: $$F_{n} = \sum_{i=0}^{\left\lfloor n/2 \right\rfloor}\binom{n-i}{i}$$ So, I know that the Fibonacci number counts the number of ways to cover a $1 \times n$ rectangle with checkers (covering a single square) and dominoes (covering two squares). I believe that will come up somewhere in the explanation, but I am having difficulty seeing how to show that the RHS counts the same as the Fibonacci number. EDIT: I think I have got it figured out. If we say that the LHS is the way to cover a nx1 rectangle with checkers and dominoes, then the floor of n/2 is the max amount of dominoes you could have. In this case, i represents the number of dominoes in that particular solution, hence why you have n-i positions. Then you figure out how many unique solutions there are when you have i dominoes, adding up all the solutions with the different number of dominoes.","['summation', 'fibonacci-numbers', 'combinatorial-proofs', 'binomial-coefficients', 'discrete-mathematics']"
4655791,How to make sense of partial derivatives w.r.t to polar coordinates?,"Ok, so I'm trying to learn multivariable calculus. As I come from a math background, I want to understand it as rigurously as possible. I've seen that differential geometry does the job, but I don't want to go that far yet. So my question is how to make sense of partial derivatives w.r.t to other coordinate systems besides the linear ones? For example, polar coordinates do not behave nicely under linear transformations. Even worse, the vector basis is local, like in physics, so it seems a nightmare to keep track of how to write things like gradients in a vector basis that is always changing with the point. I want a mathematical explanation to keep me sane, physical analogies will not do.","['partial-derivative', 'multivariable-calculus', 'differential', 'polar-coordinates']"
4655805,Encoding primes via ranks of sign matrices,"Crossposted at MathOverflow Recently I came across a very simply defined family of matrices: for $n \in \mathbb{N}$ , set $A_n := (a_{ij})_{1 \le i, j \le n}$ , where $$\displaystyle a_{ij} := (-1)^{\big\lfloor \dfrac{2(i-1)(j-1)}{n} \big\rfloor}$$ These are normalized $\pm 1$ symmetric $n \times n$ matrices. The first few are: $$
A_2 = \begin{bmatrix}
     1&1\\
     1&-1
     \end{bmatrix},
A_3 = \begin{bmatrix}
     1&1&1\\
     1&1&-1\\
     1&-1&1
     \end{bmatrix},
A_4 = \begin{bmatrix}
     1&1&1&1\\
     1&1&-1&-1\\
     1&-1&1&-1\\
     1&-1&-1&1
     \end{bmatrix}, \ldots
$$ Computing $\operatorname{rank}(A_n)$ for small $n$ quickly suggests a pattern: $$\operatorname{rank}(A_n) = \sigma_0(n) + \Big\lfloor \frac{n-1}{2} \Big\rfloor$$ where $\sigma_0(n)$ is the number (= sum of $0^\text{th}$ powers) of divisors of $n$ . My question is: Is this formula for $\operatorname{rank}(A_n)$ true for all $n$ ? If so, then since the minimal value of $\sigma_0$ is $2$ , which occurs exactly for prime $n$ , one would have $\operatorname{rank}(A_n) = \big\lfloor \frac{n+3}{2} \big\rfloor$ is minimal $\iff n$ is prime. (This would, in my opinion, be an interesting encoding of the primes in a purely linear-algebraic fashion.) I have tested this up to $n = 30$ . To save some trouble, this is A361003 in OEIS (coincidentally just added last week!). A combinatorial proof e.g. via A361001 would be fine. If anyone knows more about this family of matrices I would be happy to read more.","['number-theory', 'elementary-number-theory', 'matrices', 'linear-algebra', 'combinatorics']"
4655833,Nonstandard Complex Analysis?,I recently discovered Nonstandard Analysis and am slowly working my way through Kelsier's textbook and Foundations companion.  However while I have found plenty of stuff about real nonstandard analysis I have not been able to discover anything about complex nonstandard analysis.  Would anyone be able to recommend any such resources to add to my reading list once I am done with Kelsier's book?,"['nonstandard-analysis', 'book-recommendation', 'reference-request', 'complex-analysis', 'infinitesimals']"
4655839,Differential form $\sqrt{z}dz$ does not transform uniquely?,"On page 107 of his Algebraic Curves and Riemann surfaces Miranda writes The definition of a meromorphic or holomorphic $1$ -form $\omega$ suggests that in order to define $\omega$ on a Riemann surface $X$ , one must give local expression for $\omega$ (of the form $f(z)dz$ ) in each chart of an atlas for $X$ . In fact, one can define $\omega$ by giving a single formula in a single chart. […] A [second] problem may arise, namely that the local expression does not transform uniquely to the other points of $X$ . For example, consider the meromorphic $1$ -form $\sqrt{z}dz$ defined on the complex plane with the negative axis removed, where the branch of the square root is chosen so that $\sqrt{1}=1$ . This can be extended to the negative real axis but not uniquely. Hence we do not obtain a meromorphic $1$ -form on the whole of $\mathbb{C}^*$ . I am not sure that I understand this paragraph. Miranda seems to consider the chart $\phi\colon \mathbb{C}\setminus \mathbb{R}_{\leq 0}\rightarrow \mathbb{C}\setminus \mathbb{R}_{\leq0};z\mapsto z$ . Denote the corresponding local coordinate by $z$ . Then we considers the $1$ -form $\sqrt{z}dz$ , where $\sqrt{-}$ denotes the principal branch of the square root. Now, what is meant by the above sentence marked in bold? The principal branch of the square root function cannot be extended to a global holomorphic function. Does Miranda refer to the fact that it can locally be analytically continued so that one obtains a holomorphic function $f\colon U\rightarrow V$ such that $\mathbb{R}_{\leq 0}\subset U$ and $f(z)=\sqrt{z}$ for $z\in U\cap(\mathbb{C}\setminus \mathbb{R}_{\leq0})$ ? And this extension is not unique?","['complex-analysis', 'riemann-surfaces', 'branch-cuts', 'differential-forms']"
