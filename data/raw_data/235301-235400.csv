question_id,title,body,tags
4923072,Is the completion of a (not necessarily Noetherian) local ring flat?,"Let $R$ be a commutative local ring with unity, $\mathfrak{m}$ its maximal ideal, and $\widehat{R}$ the $\mathfrak{m}$ -adic completion of $R$ . Is it true in general that the canonical morphism $R\rightarrow \widehat{R}$ is flat? If $R$ is Noetherian, it is a well-known result that $R\rightarrow \widehat{R}$ is faithfully flat. I am interested in the case where $R$ is $\textbf{not}$ Noetherian. If this is not true in full generality, are there any other conditions that ensure that this completion is flat?","['noetherian', 'algebraic-geometry', 'abstract-algebra', 'flatness', 'commutative-algebra']"
4923090,Is $x^x = (x-1)^{x+1}$?,"Background: I was trying to estimate the size of $21^{21}$ for some problem and decided to use $20^{22}$ as hopefully a rough approximate ( $20^{22} = 2^{22} \cdot 10^{22} \approx 10^{28}$ ). But then I wanted to see if that was over or underestimate and got into this rabbithole of when they are equal. Anyways, I was doing some thinking and wanted to get an analytical solution to when $x^x = (x-1)^{x+1}$ . My original question was which was bigger for $x=21$ , but transitioned when I noticed that the $x^x > (x-1)^{x+1}$ for small $x$ , and then it flips when I go from $x=4$ to $x=5$ . I threw it in Desmos and got an approximate value of 4.141 (and I went woahhh that's $\pi +1$ but unfortunately that's only true to a couple decimal places :( ). Can anyone help me find an analytical or algebraic (as opposed to numerical/approximate) way of finding the value for x when these two expressions are equal? I was trying to approach it in a similar way to when $a^b$ grows faster than $b^a$ using logarithms but was struggling and would love some help if anyone has any suggestions. There is also this post which is similar but only talks about the inequality whereas I am more concerned about the equality. EDIT I found this paper which connects  the $x$ value of the solution to the original equation to be equal to $\alpha_0$ where $1+\frac{1}{(1+\frac{1}{(1+\frac{1}{...})^{\alpha}})^{\alpha}}$ converges for all $0 < \alpha < \alpha_0$ and diverges for all $\alpha > \alpha_0$ . Very interesting stuff. PS: I have an interesting follow up for $x^x$ vs $(x-n)^{x+n}$ that I would like to ask about but maybe some insight on this question will allow me to explore the following on my own :) Thanks!","['calculus', 'tetration', 'algebra-precalculus', 'transcendental-equations']"
4923094,"3.17 Brezis, $\ell^p$ convergence","I have been trying to try this exercise but either I have not understood it, or I have not been able to solve it. I would love to receive help on how to solve this exercise. 3.17 Let $\left(x^n\right)$ be a sequence in $\ell^p$ with $1 \leq p \leq \infty$ . Assuming $x^n \rightharpoonup x$ in $\sigma\left(\ell^p, \ell^{p^{\prime}}\right)$ prove that:
(a) $\left(x^n\right)$ is bounded in $\ell^p$ ,
(b) $x_i^n \underset{n \rightarrow \infty}{\longrightarrow} x_i$ for every $i$ , where $x^n=\left(x_1^n, x_2^n, \ldots, x_i^n, \ldots\right)$ and $x=$ $\left(x_1, x_2, \ldots, x_i, \ldots\right)$ Conversely, suppose $\left(x^n\right)$ is a sequence in $\ell^p$ with $1<p \leq \infty$ . Assume that (a) and (b) hold (for some limit denoted by $x_i$ ). Prove that $x \in \ell^p$ and that $x^n \rightharpoonup x$ in $\sigma\left(\ell^p, \ell^{p^{\prime}}\right)$ . I don't know if part a) should come out directly because $x^n \in \ell^p$ and consequently is bounded in the norm? For b), I really have no idea how I could solve it, only I found this link about a previous question: Pointwise convergence and boundedness imply weak convergence in $\ell^\infty$ [EDIT:] I think b) it is possible prove with using that $x_i^n = \langle e_i, x^n \rangle \to \langle e_i, x \rangle = x_i$ .  Then $x_i^n \to x_i$ weak.
[EDIT 2:] I think that this way of solving is true. To solve 2, we must show that the projections are dense on $E'$ ?",['functional-analysis']
4923128,Finding area of the shaded region,"My attempts: Attempt 1: Radius of the circle = $5$ Angle $AOB = 45$ degrees hence area of shaded part between A and B = ${1\over 8} \times \pi \times 25 - {1\over 2} \times 5^{2}{1\over \sqrt{2}}$ Then, by multiplying this area by 8, we get the answer as 7.9, which doesn't match any answer. Attempt 2: Now, if I consider the quadrilateral OHAB, the area will be ${1\over 4} \pi 25 - {1\over 2}5^{2} \sin 90$ Then, by multiplying the result with 4, we get $25 \pi - 50 = 28.5$ , which is way beyond the answer choices. Official Explanation: Can someone please explain the flaws in my attempts and how exactly the solution's author calculated the area of triangles? I can't seem to draw the diagram they used to calculate the areas of triangles.",['geometry']
4923163,Doubts regarding absolute value,"My fiancÃ©e is a teacher in a secondary school. She asked me a question connected to absolute value that I can't answer. Let's consider the following problem $$
\lvert x - 2 \rvert < \lvert x + 4 \rvert.
$$ The answer here is quite obvious, i.e. $x > -1, x \in \mathbb{R}$ . You can get the answer either by using a geometrical interpretation or dividing the real line in two points (i.e. 2 and -4) and consider some cases. Her students solve this problem using the following method. They consider two inequalities $x - 2 < x + 4$ and $x - 2 > -(x + 4)$ . They solve the inequalities, from the first one they get $x \in \mathbb{R}$ and from the second one $x > -1$ . That's the point that I think is wrong, but I don't have any proper argument against. When the students have two intervals (i.e. two answers from the inequalities) they either take the union or the sum of them . They use the following rule: use the logical operator (between the solutions to the inequalities) which would lead neither to the real line nor to the empty set. In the described example sum would lead to $\mathbb{R}$ so they choose intersection instead. The third point seems to be very wrong, nevertheless I can't find any decent argument which would prove that's it's not correct. Maybe it is? I would appreciate any counterexample or some argument for that. Edit.
Cases like $\lvert 2x \rvert < \lvert x \rvert$ are not the ones I'm looking for, because it's equivalent to $\lvert x \rvert < 0$ and now the third point does not appear.","['algebra-precalculus', 'absolute-value']"
4923165,Maximum coins with one Counterfeit coin among them that can be determined in 3 weighings given that the coin can be heavier or lighter,"What is the largest number of coins from which one can detect a counterfeit in three weighings with a pan balance, if it is known in advance only that the counterfeit coin differs in weight from the other coins? ----Zorich Mathematical Analysis I 2.2 Exercise 28 b) I have seen one answer online saying that: To find the largest number of coins we can detect a counterfeit from, in 3 weighings, we can perform the following steps: Consider the fact that we have three possible outcomes for each weighing: Left side is heavier, right side is heavier, or both sides are balanced. As there are 3 weighings, the total number of possible outcomes is
which is 27 possible outcomes. Keep in mind that one of these outcomes is ""all weighings are balanced."" This outcome corresponds to all the genuine coins being weighed. So, we have 26 distinct outcomes remaining for the coins with different weights. Divide the remaining outcomes into two groups, one for the heavier counterfeit coin and one for the lighter counterfeit coin. There will be
outcomes in each group. In each group, there are 13 outcomes. We can detect the counterfeit coin among 13 coins in each group. 6. In total, we can detect a counterfeit coin among
coins. Hence, the largest number of coins from which we can detect a counterfeit coin in three weighings with a pan balance is 26 coins. But I think the proof is unrigorous, so I want to ask for a rigorous solution to it. Also the subquestion prior to this subquestion states that there is a one to one correspondence between unbalanced ternary numbers to ternary numbers.That is, every $(\alpha_n\alpha_{n-1}\cdots \alpha_0)_3,\alpha_i\in\{0,1,2\}$ can be converted to $(\beta_m\beta_{m-1}\cdots\beta_0)_3,\beta\in\{-1,0,1\}$ Edit: And here is my solution to it: There are $3^3$ possibilities for weighings and they can't be all same, then $3^3-1$ , and then we have the possibility being divided over to heavier counterfeit coins and lighter counterfeit coins, and thus we have $\frac{3^3-1}{2}=13$ But I find it unrigorous and doubt if it is right.","['combinatorics', 'discrete-mathematics']"
4923189,How to calculate $\lim_{x\to\infty}\left(\sqrt[n]{\operatorname{\mathit{P_{n}}}(x)}-\sqrt[n]{\operatorname{\mathit{Q_{n}}}(x)}\right)$?,"Let $\operatorname{\mathit{P_{n}}}(x)=x^{n}+a_{n-1}x^{n-1}+a_{n-2}x^{n-2}+\cdots+a_{1}x+a_{0}, \operatorname{\mathit{Q_{n}}}(x)=x^{n}+b_{n-1}x^{n-1}+b_{n-2}x^{n-2}+\cdots+b_{1}x+b_{0}$ where $n\in\mathbb{Z}_+$ , how to calculate $\lim_{x\to\infty}\left(\sqrt[n]{\operatorname{\mathit{P_{n}}}(x)}-\sqrt[n]{\operatorname{\mathit{Q_{n}}}(x)}\right)$ ? I can calculate the limit for some small $n$ . For $n=1$ : $$\begin{aligned}
\lim_{x\to\infty}\left(\left(x+a_{0}\right)-\left(x+b_{0}\right)\right)&=\lim_{x\to\infty}\left(a_{0}-b_{0}\right)\\
&=a_{0}-b_{0}
\end{aligned}$$ For $n=2$ : $$\begin{aligned}
\lim_{x\to\infty}\left(\sqrt{x^{2}+a_{1}x+a_{0}}-\sqrt{x^{2}+b_{1}x+b_{0}}\right)&=\lim_{x\to\infty}\frac{\left(x^{2}+a_{1}x+a_{0}\right)-\left(x^{2}+b_{1}x+b_{0}\right)}{\sqrt{x^{2}+a_{1}x+a_{0}}+\sqrt{x^{2}+b_{1}x+b_{0}}}\\
&=\lim_{x\to\infty}\frac{\left(a_{1}-b_{1}\right)+\frac{a_{0}-b_{0}}{x}}{\sqrt{1+\frac{a_{1}}{x}+\frac{a_{0}}{x^{2}}}+\sqrt{1+\frac{b_{1}}{x}+\frac{b_{0}}{x^{2}}}}\\
&=\frac{a_{1}-b_{1}}{2}
\end{aligned}$$ For $n=3$ : $$\begin{aligned}
\lim_{x\to\infty}\left(\sqrt[3]{x^{3}+a_{2}x^{2}+a_{1}x+a_{0}}-\sqrt[3]{x^{3}+b_{2}x^{2}+b_{1}x+b_{0}}\right)&=\lim_{x\to\infty}\frac{\left(x^{3}+a_{2}x^{2}+a_{1}x+a_{0}\right)-\left(x^{3}+b_{2}x^{2}+b_{1}x+b_{0}\right)}{\sqrt[3]{\left(x^{3}+a_{2}x^2+a_{1}x+a_{0}\right)^{2}}+\sqrt[3]{\left(x^{3}+a_{2}x^{2}+a_{1}x+a_{0}\right)\left(x^3+b_{2}x^{2}+b_{1}x+b_{0}\right)}+\sqrt[3]{\left(x^{3}+b_{2}x^{2}+b_{1}x+b_{0}\right)^{2}}}\\
&=\lim_{x\to\infty}\frac{\left(a_{2}-b_{2}\right)+\frac{a_{1}-b_{1}}{x}+\frac{a_{0}-b_{0}}{x^{2}}}{\sqrt[3]{\left(1+\frac{a_{2}}{x}+\frac{a_{1}}{x^{2}}+\frac{a_{0}}{x^{3}}\right)^{2}}+\sqrt[3]{\left(1+\frac{a_{2}}{x}+\frac{a_{1}}{x^{2}}+\frac{a_{0}}{x^{3}}\right)\left(1+\frac{b_{2}}{x}+\frac{b_{1}}{x^{2}}+\frac{b_{0}}{x^{3}}\right)}+\sqrt[3]{\left(1+\frac{b_{2}}{x}+\frac{b_{1}}{x^{2}}+\frac{b_{0}}{x^{3}}\right)^{2}}}\\
&=\frac{a_{2}-b_{2}}{3}
\end{aligned}$$ However, it is hard to find the limit if $n$ is large. How to calculate the limit for arbitrary $n$ ?","['limits', 'calculus']"
4923234,Why Learn Measure Theory and Lebesgue Integration?,"As someone who has taken two semesters of real analysis, having been exposed to the rigorous definition of the Riemann-Stieltjes integral - why should I learn Lebesgue integration? The Riemann integral already furnishes us with a means of integrating continuous, bounded functions and functions with finitely-many jump discontinuities. What compelling advantage does the Lebesgue formulation offer to Riemann's? My interest is in an answer possessing utility in the sciences or engineering fields.","['integration', 'measure-theory', 'analysis']"
4923267,Why a vector function to $\mathbb{R}^n$ can be regarded as $n$ different functions?,"I am taking advanced calculus, and the professor said the following: we can regard $f: U \rightarrow \mathbb{R}^n$ as $n$ different functions, and write $f = (f_1,...,f_n)$ ."" I am not sure what the rigorous justification for representing the vector-valued function as $f = (f_1,...,f_n)$ (i.e. why we can write $f$ as $f = (f_1,...,f_n)$ ). How do we know that each component of the vector is a function of $p \in U$ ? I thought of the following: Since $f: U \rightarrow \mathbb{R}^n$ , then we know that $f(p)\in\mathbb{R}^n$ , and thus we can write $f(p)$ as $f = (f_1,...,f_n)$ , where $f_i$ are the components of $f(p)$ . we can say that $f$ is comprised of $n$ different functions because when writing $f_i = \langle f, \mathbf{e}_i \rangle$ , we have $n$ different functions from $U$ to $\mathbb{R}^n$ , so (I hope so) this answers why we can think of $f$ as being made of $n$ functions. but why $f_i$ is function? how do we know that each component of $f$ is a function on its own? I think that we have already established this on the previous point - since $f_i = (f,\mathbf{e}_i)$ , it is a composition of two functions ( $(*,\mathbf{e}_i): U \rightarrow \mathbb{R}$ composed over $f$ ), and thus is a function (or alternatively, say that $f_i = \pi_i \circ f$ which is again a function since it is the composition of the projection function over $f$ ). But I feel like I lack the ""intuitive"" understanding of why this is true... the only intuitive (and contorted) explanation I came close to is the following: Since $f$ is a vector-valued function, we know that $f(p) \in \mathbb{R}^n$ , and since it is a vector, we know that the vector and its components are equivalent. Since we know that $p$ determines $f(p)$ and that it is equivalent to its components because it is a vector, we can say that each of its components is also a function of $p$ . I would be grateful if you could make these points clearer (and maybe provide a formal definition, as I think that the cause for my misunderstanding is the lack of a formal definition. I looked for formal definitions in the literature (Ruding for example, but since it is very basic I wasn't able to find any rigid and rigorous definitions there).","['elementary-set-theory', 'multivariable-calculus', 'calculus', 'intuition']"
4923301,A very complete proof on the separability of $L^p$.,"I have to prove the following important result. Theorem . Let $(X,\mathcal{A},\mu)$ be a measure space such that: $(1)\;$ the measurable space $(X,\mathcal{A})$ is separable. $(2)\;$ $\mu$ is sigma finite. Then $L^p(X,\mathcal{A},\mu)$ is separable for $p\in [1,\infty)$ . We remember that a measurable space $(X,\mathcal{A})$ is said separable if exists a countable family $\mathcal{C}\subseteq \mathcal{P}(X)$ sich that $\mathcal{\sigma}_0(\mathcal{C})=\mathcal{A}$ , where $\sigma_0$ denotes the generated sigma algebra. We denote with $S(X,\mathcal{A},\mu)$ the set of all simple measurable function on $X$ a complex values.
We state two results that we will use. Theorem 1. Let $p\in [1,\infty)$ , then le simple function $S(X,\mathcal{A}, \mu)\cap L^p(X,\mathcal{A},\mu)$ are dense in $L^p$ . Lemma 2. Let $(X,\mathcal{A},\mu)$ be a finite measure space. Let $\mathcal{C}\subseteq\mathcal{P}(X)$ a family such that $\sigma_0(\mathcal{C})=\mathcal{A}$ ; let $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ the generated algebra of $\mathcal{C}$ . Then for all $F\in \mathcal{A}$ e for all $\varepsilon >0$ exists $G\in\mathcal{A}_0$ such that $$\mu(F\setminus G)+\mu(G\setminus F)<\varepsilon.$$ First case $\Large \mu(X)<\infty$ We introduce the collection $$S_{\mathbb{Q}}(X,\mathcal{A},\mu)=\{s\in S(X,\mathcal{A},\mu)\cap L^p\;:\; s(X)\subseteq\mathbb{Q}+i\mathbb{Q}\}$$ First step $S_{\mathbb{Q}}(X)$ is dense in $S(X)\cap L^p$ Let $\varepsilon > 0 $ and $s\in S\cap L^p$ fixed. Let $$s=\sum_{k=1}^n c_k \chi_{E_k}$$ the standard representation of $s$ . For all $k=1,\dots, n$ let $q_k$ be the complex number such that the real and imaginary parts are such that $$\max_{k=1,\dots, n}{|c_k-q_k|}<\frac{\varepsilon}{[n\mu(X)]^{\frac{1}{p}}}.$$ Then the simple function $$s_{\mathbb{Q}}=\sum_{k=1}^nq_k\chi_{E_k}\in S_{\mathbb{Q}}(X)$$ and results that \begin{eqnarray*}
\lVert s-s_{\mathbb{Q}} \rVert_p^p &=& \int_X \left | \sum_{k=1}^n(c_k-q_k)\chi_{E_k}\right |^p\; d\mu \\
&\color{red}{=}& \int_X\sum_{k=1}^n\lvert c_k-q_k \rvert^p\chi_{E_k}\;d\mu \\
&=&\sum_{k=1}^n\lvert c_k-q_k\rvert^p\mu(E_k) \\
&\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}\frac{\mu(E_k)}{\mu(X)}\\
&\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}=\varepsilon^p
\end{eqnarray*} The red inequality arises from the fact that the $E_k$ are disjoint and the last inequality follows from the fact that $\frac{\mu(E_k)}{\mu(X)}\le 1$ Now, since the space $(X,\mathcal{A})$ is separable, exists a countable family $\mathcal{C}\subseteq \mathcal{P}(X)$ such that $\sigma_0(\mathcal{C})=\mathcal{A}$ . Evidently the generate algebra $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ is countable. Now, we introduce the collection $$S_{\mathbb{Q},\mathcal{A}_0}(X,\mathcal{A},\mu)=\left\{s=\sum_{k=1}^n d_k\chi_{G_k}\in S_{\mathbb{Q}}\;|\; G_k\in\mathcal{A}_0, n\in\mathbb{N}\right\}.$$ We observe that this collection is also countable. Second step $S_{\mathbb{Q}, \mathcal{A}_0}(X)$ is dense in $S_{\mathbb{Q}}(X)$ Let $t\in S_{\mathbb{Q}}(X)$ , then $$t=\sum_{k=1}^nc_k\chi_{F_k},$$ where $F_k\in\mathcal{A}$ and $c_k\in \mathbb{Q}+i\mathbb{Q}$ ( $k=1,\dots, n$ ). From the above lemma 2 for all $k=1,\dots, n$ exists $G_k\in\mathcal{A}_0$ such that $$\mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)<\left(\frac{\varepsilon}{nM} \right)^p,$$ where $M:=\max _{k=1,\dots, n}|c_k|$ . Define $$s=\sum_{k=1}^nc_k\chi_{G_k},$$ then $s\in S_{\mathbb{Q},\mathcal{A}_0}$ and results that \begin{eqnarray*}
\lVert t-s \rVert_p &=& \left\lVert \sum_{k=1}^n c_k(\chi_{F_k}-\chi_{G_k})\right\rVert_p \\
& \stackrel{Minkowski}{\leq}& \sum_{k=1}^{n} \left\lVert c_k (\chi_{F_k}-\chi_{G_k})\right \rVert_p \\
&=& \sum_{k=1}^{n} \lvert c_k \rvert  \left\lVert \chi_{F_k}-\chi_{G_k}\right\rVert_p \\
&=& \sum_{k=1}^{n} \lvert c_k \rvert \{\mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)\}^{1/p}< nM\left(\frac{\varepsilon}{nM} \right)=\varepsilon.
\end{eqnarray*} Now, let $f\in L^p$ , then for Theorem 1. exists $s\in S(X)\cap L^p(X)$ such that $\lVert f-s \rVert_p<\varepsilon$ . For the frist step exists $s_{\mathbb{Q}}\in S_{\mathbb{Q}}(X)$ such that $\lVert s-s_{\mathbb{Q}}\rVert_p<\varepsilon$ , for the step 2 exists $t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0}$ such that $$\lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<\varepsilon$$ , then $$\lVert f-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<3\varepsilon$$ This completely proves the theorem in the finite case. Second case $\mu(X)=\infty$ Since $\mu$ is sigma finite exists an increasing sequence $\{E_n\}\subseteq\mathcal{A}$ such that $$X=\bigcup_{n=1}^\infty E_n\quad\text{and}\quad \mu(E_n)<\infty\;\forall n\in\mathbb{N}.$$ Let $s\in S(X)\cap L^p$ , then $$\infty > \int_X \lvert s\rvert^p\;d\mu=\lim_{n\to\infty}\int_{E_n}\lvert s \rvert^p\; d\mu,$$ then $$(\forall \varepsilon>0)\quad (\exists n_0\in\mathbb{N})\quad(\forall n>n_0)\quad \int_{X\setminus E_n} \lvert s \rvert^p\;d\mu < \varepsilon$$ We define $t_n:=s\chi_{E_n}$ , then $\{t_n\}\subseteq S(X)\cap L^p(X)$ . $$\lVert t_n-s \rVert_p=\int_{X\setminus E_n} \lvert s \rvert\; d\mu<\varepsilon$$ for all $n>n_0$ . We choose $$t:=t_{n_0+1},$$ then $$\lVert t-s\rVert_p<\varepsilon.$$ The simple function $t$ is zero outside $Y:=E_{n_0+1}$ , then can be seen defined only on $Y$ , that is $t\in S(Y)\cap L^p$ , observe that $\mu(Y)<\infty$ . Thus we have that:
Let $f\in L^p$ be a function, then fro theorem 1 exists $s\in S(X)\cap L^p$ such that $\lVert f - s \rVert_p<\varepsilon$ . For above passage, exists $t\in S(Y)\cap L^p$ where $Y\in\mathcal{A}$ and $\mu(Y)<\infty$ such that $\lvert t - s\rVert_p<\varepsilon$ . For the first step applied to $Y$ exists $s_{\mathbb{Q}}\in S_{\mathbb{Q}}(Y)$ such that $\lVert t -s_{\mathbb{Q}}\rVert_{L^p(Y)}<\varepsilon$ and for the second step exists $t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0}(Y)$ such that $\lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0}\rVert_{L^p(Y)}<\varepsilon$ . Defining $s_{\mathbb{Q}}$ and $t_{\mathbb{Q},\mathcal{A}_0}$ ugual to zero in $X\setminus Y$ we have that $$\lVert f - t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<4\varepsilon$$ Question Is this a correct proof?","['proof-explanation', 'proof-writing', 'solution-verification', 'real-analysis']"
4923306,Is every bounded complex valued harmonic function on the open unit disc a sum of bounded holomorphic function and bounded antiholomorphic function?,"Let $\varphi$ be a bounded complex valued harmonic function on the open unit disc. Then $\varphi = \psi + \bar \chi$ for some holomorphic functions $\psi, \chi$ on the open unit disc (see last line of the question for the decomposition). Is it necessarily the case that $\psi$ and $\chi$ are bounded? I am almost sure that this is false, however, I have not been able to find a counterexample yet. Is there any immediate counterexample for this? Any bounded harmonic function can be written as $\varphi (z) = \sum_{n \ge 0} c_n z^n + \sum_{n>0} c_{-n} \bar {z} ^n$ for each $z \in \mathbb D$ . Take $\psi (z) = \sum_{n \ge 0} c_n z^n $ and $\chi (z) =\sum_{n>0} \bar c_{-n} {z} ^n$ for $z\in \mathbb D$ . Are $\psi$ and $\chi$ bounded?","['complex-analysis', 'harmonic-functions']"
4923339,Is the number 3 in the Collatz conjecture arbitrary?,"One of the most famous conjectures in mathematics is the Collatz conjecture also known  as $3n+1$ but my question is why we multiply the odd number by 3? I get that the conjecture probably wants to multiply the odd number with other odd number and then add 1 to make it even and I can see why the conjecture avoided choosing the number 1 but is the number 3 arbitrary? What if we change that number to 5 or 7 or any odd number will the conjecture  still hold or there will be some loops that don't got to 1 and divergent sequence?
So is that $3$ arbitrary? Or there is a good reason for choosing it besides it is the smallest odd number bigger than 1? Also Is that $+1$ arbitrary? What happen if we change it to be any other odd number? Will that affect the conjecture? It seems like modifying the 3 to 5 will lead to a very slow sequence or divergence (the numbers that I tried didn't go to $1$ at all and I just assumed that the sequence  is very slow ).","['collatz-conjecture', 'number-theory', 'conjectures']"
4923374,Does there exist an entire function $f$ such that $f(z)+f(z^2)=z^3$ for all $z \in \mathbb{C}$?,"Does there exist an entire function $f$ such that $f(z)+f(z^2)=z^3$ for all $z \in \mathbb{C}$ ? This problem showed up on a qualifying exam; these exams enjoy including problems that utilize Liouville's theorem. Method 1: Show $f$ is bounded and use Liouville Here, I would suppose $f$ is entire and non-constant. If I can show that $f$ is bounded, I will have a contradiction which means that $f$ is constant. Method 2: Use Taylor Expansion Since $f$ is entire, $f$ has a Taylor series expansion about $z=0$ . Thus, $$f(z) = f(0) + f'(0)z + \frac{f''(0)}{2}z^2 + \dots \tag{1}$$ By hypothesis, $$f(z) = z^3-f(z^2) \tag{2}$$ But now, we can write $$f(z^2) = f(0) + f'(0)z^2 + \frac{f''(0)}{2}z^4 + \dots \tag{3}$$ Then, rewriting $(2)$ , we get $$f(0) + f'(0)z + \frac{f''(0)}{2}z^2 + \dots = z^3 - (f(0) + f'(0)z^2 + \frac{f''(0)}{2}z^4 + \dots) \tag{4}$$ I seek to derive a contradiction here but I experience no such luck. I'm suspecting that the first method of showing $f$ is bounded might be a better bet. Any ideas?","['complex-analysis', 'entire-functions']"
4923538,Show that there are infinitely many composite integers $n$ such that $n \mid (3^{n - 1} - 2^{n - 1})$,"Problem : Show that there are infinitely many composite integers $n$ such that $n \;|\; (3^{n - 1} - 2^{n - 1})$ . My Solution : Suppose that $n$ is the product of two distinct primes $n := pq$ . Then, by Fermat's Little Theorem, $n \mid 3^{n - 1} - 2^{n - 1}$ is equivalent to $$q \mid 3^{p - 1} - 2^{p - 1} \quad \text{and} \quad p \mid 3^{q - 1} - 2^{q - 1}.$$ First, we claim that if $p \equiv 1, 5, 19, 23 \pmod{24}$ , then $p$ is not a primitive divisor of $3^{p - 1} - 2^{p - 1}$ (i.e. $p$ divides $3^x - 2^x$ for some $x < p - 1$ as well as $x = p - 1$ ). Indeed, if $p \equiv 1, 5, 19, 23 \pmod{24}$ , then $\bigl(\tfrac{6}{p}\bigr) = 1$ , using the Legendre symbol. By Euler's Criterion, $$6^{\tfrac{p - 1}{2}} \equiv \left(\frac{6}{p}\right) \equiv 1 \equiv 4^{\tfrac{p - 1}{2}} \pmod{p}.$$ By dividing both sides by $2^{\tfrac{p - 1}{2}}$ , we get that $p \mid 3^{\tfrac{p - 1}{2}} - 2^{\tfrac{p - 1}{2}}$ . The infinitude of primes of that form is given by Dirichlet's Theorem. Returning to the original problem, fix $p \ge 7$ to be some prime congruent to $1$ , $5$ , $19$ , or $23$ modulo $24$ . By Zsigmondy's Theorem, there exists a primitive divisor of $3^{p - 1} - 2^{p - 1}$ , which is distinct from $p$ . Let it be $q$ . Then, by minimality $\operatorname{ord}_q \bigl(\tfrac{3}{2}\bigr) = p - 1$ . Since we know that $q \mid 3^{q - 1} - 2^{q - 1}$ by Fermat's Little Theorem, $p - 1 \mid q - 1$ . This implies that $p \mid 3^{q - 1} - 2^{q - 1}$ . So, we are done. Questions: I believe that this proof is correct. However there are some things that I donât like about it. Is there no easier way to prove that: There are infinitely many primes $p$ such that $p$ is not the only primitive divisor of $3^{p - 1} - 2^{p - 1}$ ? My proof of this fact is rather long, relies on the reader being able to calculate $\bigl(\frac{6}{p}\bigr)$ using Gaussâs Lemma and the Quadratic Reciprocity, and I think it distracts from the overall motivation of the proof which is just Zsigmondyâs Theorem; everything else is just patching up the small holes. Also, I donât particularly like how this proof relies on Dirichletâs Theorem, as it is quite difficult to prove, although the special case of $p \equiv 1 \pmod{n}$ is easier, using only cyclotomic polyonomials. Is there a completely different approach to the entire problem? Setting $n = pq$ was the only thing that came to me, however it also restricted the number of possibilities by quite a bit.","['contest-math', 'number-theory', 'elementary-number-theory']"
4923548,"Let $f: A \rightarrow B$ be a function, and X be a subset of A. Prove that $f(f^{-1}(f(X)))= f(X).$","The Problem: Let $f: A \rightarrow B$ be a function, and $X$ be a subset of $A$ . Prove that $f(f^{-1}(f(X))) = f(X)$ . My professor took off all the points because I didn't use ""equality of sets through double inclusion."" It's not like I didn't consider it, I just thought it was not needed. What I did was I let an element $x$ belonging to $X$ (the subset of $A$ ) and showed that $f(x) = f(f^{-1}(f(x)))$ for all $x$ , thus proving that $f(f^{-1}(f(X))) = f(X)$ where big $X$ is the subset of $A$ . How should I argue my point if I am not wrong?","['elementary-set-theory', 'proof-writing', 'solution-verification', 'set-theory']"
4923549,Is it weird that my probability theory lecturer thinks that $P(E)=0$ implies that $E=\emptyset$?,"I'm an undergraduate student in pure mathematics, and I'm taking a probability theory course based on the book A First Course in Probability by Sheldon Ross. My lecturer defined the conditional probability $P(A|B)$ of two events $A,B$ by $P(A|B)=\frac{P(A\cap B)}{P(B)}$ , provided that $B\neq\emptyset$ . I knew that $P(\emptyset)=0$ , and after some research, I realized that the converse is not true, namely, from $P(E)=0$ we can't conclude that $E=\emptyset$ . I wrote to my lecturer, and among other things, I gave him a simple counterexample. (First I asked him to prove his claim, but instead he ""proved"" to me that $P(\emptyset)=0$ , but he relied on $P(E^c)+P(E)=1$ which was proved in the book by ""the finite version"" of Axiom 3, which was proved using Axiom 3 and the fact that $P(\emptyset)=0$ .) The simple counterexample was the sample space $S=\{1,2\}$ together with the function $P$ defined by $P(\{1\})=0,P(\{2\})=1,P(\{1,2\})=1,P(\emptyset)=0$ , it's a simple matter to verify that $P$ satisfies the three axioms of probability, yet $P(\{1\})=0$ and $\{1\}\neq\emptyset$ . Yet he went in circles claiming that every sample point in the sample space (i.e., a singleton subset of the sample space) needs to have positive probabilityâwhich is of course nonsense. I was getting the feeling that he doesn't like rigour, and on further researching, I found out that his background is in engineering and applied statistics. Is it normal that he has this false belief (I mean normal for people who are more interested in applied probability rather than rigorous mathematics)?","['axioms', 'soft-question', 'probability']"
4923557,"Understanding the proof of $L^p(X,\mathscr{A},\mu)$ is complete ($1\leq p<+\infty$)","Background I have some questions when reading the proof of $L^p(X,\mathscr{A},\mu)$ is complete for $1\leq p<+\infty$ . The proof is proceeded by showing that each absolutely convergent series in $L^p(X,\mathscr{A},\mu)$ is convergent: Suppose that $1\leq p<+\infty$ and that $\{f_k\}$ is a sequence of functions that belong to $\mathscr{L}^p(X,\mathscr{A},\mu)$ and satisfy $\sum_k\|f_k\|_p<+\infty$ . Define $g:X\to[0,+\infty]$ by $$
g(x)=\left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p
$$ (of course $(+\infty)^p=+\infty$ ). Minkowski's inequality, applied to the function $|f_k|$ , implies $$
\left(\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\right)^{\frac{1}{p}} = \left\|\sum_{k=1}^n|f_k|\right\|_p \leq \sum_{k=1}^n\|f_k\|_p
$$ holds for each $n$ , and so it follows from the monotone convergence theorem that $$
\int gd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu \leq \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p;
$$ thus $g$ is integrable. Consequently $g(x)$ is finite for almost every $x$ (Corollary 2.3.14), and the series $\sum_kf_k(x)$ is absolutely convergent, and hence convergent, for almost every $x$ . Define a function $f$ on $X$ by \begin{align*}
f(x) = 
\begin{cases}
\sum_{k=1}^{\infty}f_k(x)\quad &\text{if $g(x)<+\infty$},\\
\\
0\quad &\text{otherwise}.
\end{cases}
\end{align*} Then $f$ is measurable and satisfies $|f|^p\leq g$ , and so it belongs to $\mathscr{L}^p(X,\mathscr{A},\mu)$ . Since $\lim_{n\to\infty}\left|\sum_{k=1}^nf_k(x)-f(x)\right|=0$ and $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ hold for almost every $x$ , the dominated convergence theorem implies that $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0$ . The completeness of $L^p(X,\mathscr{A},\mu)$ follows. My Questions Here are my questions: In order to apply the monotone convergence theorem, we need $g$ and $\left(\sum_{k=1}^n|f_k|\right)^p$ for each $n\in\mathbb{N}$ to be $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions. But why are they? Why is the integrability of $g$ follows from that inequality? Why is $f$ $\mathscr{A}$ -measurable? Why is $|f|^p\leq g$ ? Why is $|f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu)$ ? Why does $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ hold for almost every $x$ ? How does $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0$ follow in the end? My Attempt I tried to answer these questions myself one-by-one, and here is my attempt: Note that for each positive integer $k$ the function $f_k$ is complex-valued and $\mathscr{A}$ -measurable, thus $|f_k|$ is nonnegative real-valued $\mathscr{A}$ -measurable, and so $\sum_{k=1}^n|f_k|$ is nonnegative real-valued $\mathscr{A}$ -measurable.  For each positive integer $n$ let $F_n = \left(\sum_{k=1}^n|f_k|\right)^p$ . Clearly $F_n$ is nonnegative real-valued. We now prove that $F_n$ is $\mathscr{A}$ -measurable. For any $t\leq0$ , we have $$
\{x\in X:F_n(x)<t\}=\emptyset\in\mathscr{A}.
$$ For any $t>0$ , we have \begin{align*}
\{x\in X:F_n(x)<t\} &= \left\{x\in X:\left(\sum_{k=1}^n|f_k(x)|\right)^p<t\right\}\\
&= \left\{x\in X:\sum_{k=1}^n|f_k(x)|<t^{\frac{1}{p}}\right\}\\
&\in \mathscr{A}.
\end{align*} Thus $F_n$ is a nonnegative real-valued $\mathscr{A}$ -measurable function for each $n\in\mathbb{N}$ . Moreover, for each $x\in X$ and for each positive integer $n$ the sequence $\{F_n(x)\}$ satisfies $F_n(x)\geq0$ and $F_n(x)\leq F_{n+1}$ , thus $\lim_{n\to\infty}F_n(x)$ is either a positive number or equal to $+\infty$ . So $g=\lim_{n\to\infty}F_n = \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p$ is well defined and it is a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function. The Minkowski's inequality implies $$
\left(\int F_nd\mu\right)^{\frac{1}{p}}\leq\sum_{k=1}^n\|f_k\|_p
$$ holds for each $n\in\mathbb{N}$ . Then this inequality, together with monotone convergence theorem and the convergence of $\sum_{k=1}^{\infty}\|f_k\|_p$ , imply \begin{align*}
\int gd\mu &= \lim_{n\to\infty}\int F_nd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\\
&\leq \lim_{n\to\infty}\left(\sum_{k=1}^n\|f_k\|_p\right)^p = \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p < +\infty
\end{align*} (note that the existence of the limit $\lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu$ is implied by the monotone convergence theorem). Since $g$ is $[0,+\infty]$ -valued, it follows that $g^+=g$ and $g^-=0$ , and so $\int g^+d\mu$ and $\int g^-d\mu$ are both finite, which implies $g$ is integrable. Let $A=\{x\in X:g(x)<+\infty\}$ . Write $f$ as \begin{align*}
f &= \sum_{k=1}^{\infty}f_k\chi_A\\
&= \lim_{n\to\infty}\sum_{k=1}^nf_k\chi_A\\
&= \lim_{n\to\infty}h_n,
\end{align*} where $h_n = \sum_{k=1}^nf_k\chi_A$ . We prove that $A\in\mathscr{A}$ . We know that $g$ is $\mathscr{A}$ -measurable. Define $p:X\to[0,+\infty]$ by $p(x)=+\infty$ for all $x\in X$ . Then $\{x\in X: p(x)<t\}=\emptyset\in\mathscr{A}$ for all $t\in\mathbb{R}$ , and thus $p$ is $\mathscr{A}$ -measurable. Therefore, $A = \{x\in X:g(x)<p(x)\}\in\mathscr{A}$ , and so $A^c\in\mathscr{A}$ . Next, note that for all $k\in\mathbb{N}$ , $f_k$ is $\mathscr{A}$ -measurable. Moreover, for $t\leq0$ , $\{x\in X:\chi_A(x)<t\}=\emptyset\in\mathscr{A}$ ; for $t>1$ , $\{x\in X:\chi_A(x)<t\} = X\in\mathscr{A}$ ; for $1<t\leq1$ , $\{x\in X:\chi_A(x)<t\} = A^c\in\mathscr{A}$ . Thus, $h_n$ is $\mathscr{A}$ -measurable, and so $f=\lim_{n\to\infty}h_n$ is $\mathscr{A}$ -measurable. If $x\in A$ , then $|f(x)|^p = \left|\sum_{k=1}^{\infty}\right|^p \leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p = g(x)$ . If $x\notin A$ , then $|f(x)|^p=0\leq g(x)$ . We first prove that $|f|^p$ is $\mathscr{A}$ -measurable. If $t\leq0$ , then $\{x\in X:|f(x)|^p<t\}=\emptyset\in\mathscr{A}$ . If $t>0$ , then $\{x\in X:|f(x)|^p<t\} = \{x\in X:|f(x)|<t^{\frac{1}{p}}\}\in\mathscr{A}$ because $f$ and thus $|f|$ is $\mathscr{A}$ -measurable. Therefore, $|f|^p$ is a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function. We know that $g$ is also a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions. The integrability of $g$ implies that $\int gd\mu = \int g^+d\mu < +\infty$ . Then $|f|^p\leq g$ implies $\int|f|^pd\mu\leq\int gd\mu <+\infty$ . Since $\left(|f|^p\right)^+=|f|^p$ and $\left(|f|^p\right)^- = 0$ , we have $|f|^p$ is integrable, and so $|f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu)$ . If $x\in A$ , then \begin{align*}
\left|\sum_{k=1}^nf_k(x) - f(x)\right|^p &= \left|\sum_{k=n+1}^{\infty}f_k(x)\right|^p\\
&= \lim_{m\to\infty}\left|\sum_{k=n+1}^mf_k(x)\right|^p\\
&\leq \lim_{m\to\infty}\left(\sum_{k=n+1}^m|f_k(x)|\right)^p\\
&= \left(\sum_{k=n+1}^{\infty}|f_k(x)|\right)^p\\
&\leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p\\
&= g(x).
\end{align*} Thus, $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ holds for almost every $x$ . We know that $g$ is a $[0,+\infty]$ -valued integrable function on $X$ . Define functions $q$ and $q_1,q_2,\dots$ by $q=0$ and $q_n = \left|\sum_{k=1}^nf_k-f\right|^p$ . Then for all $x\in A$ , $q=\lim_{n\to\infty}q_n$ , and so $q=\lim_{n\to\infty}q_n$ holds $\mu$ -almost everywhere. Also, $|q_n|<g$ holds $\mu$ -almost everywhere. So the dominated convergence theorem implies $0=\int qd\mu = \lim_{n\to\infty}\int q_nd\mu$ and so $\lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0$ . Hence $\lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k-f\right|^pd\mu\right)^{\frac{1}{p}} = \lim_{n\to\infty}\|\sum_{k=1}^nf_k-f\|_p = \|\lim_{n\to\infty}\sum_{k=1}^nf_k - f\|_p = \|\sum_{k=1}^{\infty}f_k - f\|_p = 0$ . Therefore, $\sum_{k=1}^{\infty}f_k = f \in\mathscr{L}^p(X,\mathscr{A},\mu)$ . Where I Have Trouble My main concern is my attempt to Question 7. There are mainly two of them: First, I claimed that $\lim_{n\to\infty}\int q_nd\mu=0$ implies $\lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0$ . But I am not sure whether this is legit, because I have never seen a ""power law of limit of sequence"" from any analysis textbook. Basically, if $p\in\mathbb{R}$ and $\lim_{n\to\infty}a_n = a\in[-\infty,+\infty]$ , does this imply $\lim_{n\to\infty}a_n^p = a^p$ ? I don't think this is correct, because $a^p$ may not even be defined (consider for example square root of a negative number or the denominator is zero). But what if $a_n\in[0,+\infty]$ for all $n\in\mathbb{N}$ and $p\in\mathbb{R}_+$ ? How can we generalize such a result (in its most general possible case) and prove it? Secone, I claimed that the limit of norm is the norm of limit. But is this true? Why? Basically, $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p = \lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k - f\right|^pd\mu\right)^{\frac{1}{p}}$ , but is this equal to $\left(\int\left|\sum_{k=1}^{\infty}f_k-f\right|^pd\mu\right)^{\frac{1}{p}}$ ? Why? Last but not least, I know this is a lot to ask, but just in case you are interested in the proof, I would really appreciate it if you could help me check whether my answer to the other questions are correct. Thank you very much in advance! Reference: $\quad$ Theorem 3.4.1 from Measure Theory by Donald Cohn","['measure-theory', 'complete-spaces', 'analysis', 'real-analysis', 'lp-spaces']"
4923606,Efficient and unbiased estimation of the location ($\mu$) of truncated normal distribution with known scale ($\sigma^2$) and truncation points,"I have one observation $x$ which I know comes from the following truncated normal distribution : $$x \sim TN(\mu, \sigma^2, -\delta, \delta) \;\textrm{ where }\; \delta > 0$$ In my problem, the scale parameter $\sigma^2$ and the truncation points $-\delta$ and $\delta$ are known, but $\mu$ is not. I know that we can obtain $\hat{\mu}$ through maximum likelihood estimation: $$\hat{\mu} = argmax_{\mu \in \mathbb{R}} \frac{\phi\left(\frac{x - \mu}{\sigma}\right)}{\sigma\left(\Phi\left(\frac{\delta - \mu}{\sigma}\right) - \Phi\left(\frac{-\delta - \mu}{\sigma}\right)\right)}$$ But because I only have one observation, I am finding in simulations that the consistency guarantees of MLE are not sufficient and I actually have considerable bias in my estimates ( here they find a similar bias issue with MLE in the comments). So my questions are: For my estimation problem, are there any alternative estimators that deliver lower variance than MLE? Does a bias correction for the MLE estimator $\hat{\mu}$ in my case exist and/or how can I find it? Because my case (known $\sigma$ and truncation $-\delta, \delta$ ) is not typically analysed in the literature, I have been unsuccessful in finding anything specific to my problem in textbooks or research articles. Everything I have consulted (e.g. Clifford (1991)) discusses the more general case of many samples and unknown $\mu$ and $\sigma$ with unrestricted $a$ and $b$ truncation points. Any help is much appreciated. Answers that rely on numerical methods are perfectly acceptable. Reference: Cohen, A. Clifford. Truncated and censored samples: theory and applications. CRC press, 1991.","['statistics', 'variance', 'normal-distribution', 'maximum-likelihood']"
4923611,How to find examples of $L^p$ converging random variables where a specified non-Lipschitz continuous function does not converge in $L^p$?,"Background: I am preparing for a probability theory exam, and am struggling with a particular type of problem. The questions involve showing that if $X_n \xrightarrow{L^p} X$ , then it does not necessarily hold that for a given non-Lipschitz, unbounded continuous function $g$ , that $g(X_n) \xrightarrow{L^p} g(X)$ . The specific function $g$ changes every year, so I am looking for motivation on how to construct these counterexamples. For completeness, $X_n$ converges to $X$ in $L^p$ (denoted: $X_n \xrightarrow{L^p} X$ ) if: $$\mathbb E [ | X_n - X \mid ^p] \rightarrow 0 $$ I know that this does not imply that $g(X_n)$ converges to $g(X)$ in $L^p$ in general unless $g$ is bounded or Lipschitz continuous (interestingly, Nate Eldredge informed me that there exist non-bounded, non-Lipschitz functions that still converge in this manner, although for the purposes of this question, those are not relevant). How can I find examples of $L^p$ converging random variables, $(X_n)_{n \in \mathbb N}$ , where a specified non-Lipschitz continuous function of these random variables, $g(X_n)$ , does not converge in $L^p$ ?
(i.e. where we know that $X_n \xrightarrow{L^p} X$ but $g(X_n) \xrightarrow{L^p} g(X)$ is not true) Problem: The non-Lipchitz, unbounded functions that are given (to find counterexamples for) are continuous, so I don't need a strategy in complete generality. Some examples include taking $g(X_n)$ as $e^{X_n}$ and $X_n^2$ and showing that they do not converge in $L^p$ for all $X_n$ converging to $X$ in $L^p$ . At the moment, my strategy involves guessing sequences of random variables $X_n$ that I suspect converge in $L^p$ and then checking to see if these are suitable counterexamples. I would appreciate it if there is a more rigorous strategy I can employ. Note: I am not interested in knowing answers to those particular examples above, I am more interested in understanding a more general approach (although it may be helpful to refer to those to explain the general approach). Note $_2$ : I do not necessarily expect there to be a completely generalizable approach to this type of question. Therefore, if the best that can be done is to give some strong intuition as to how to think about constructing these sequences of random variables, then that also constitutes an answer to my question. Thank you in advance.","['lipschitz-functions', 'expected-value', 'convergence-divergence', 'probability-theory', 'random-variables']"
4923675,Can the gradient of a scalar field be expressed as a surface integral per unit volume?,"This question was migrated from Physics Stack Exchange because it can be answered on Mathematics Stack Exchange. Migrated last month . I am familiar with the usual equation for the gradient of a scalar field $\varphi$ $$
\nabla \phi = \frac{\partial \phi}{\partial x} \mathbf{i} + \frac{\partial \phi}{\partial y} \mathbf{j} + \frac{\partial \phi}{\partial z} \mathbf{k}.
$$ In Butkov, 'Mathematical Physics', page 33, he has an equation for grad $\varphi$ in terms of the limit of a surface integral of $\varphi$ per unit volume. $$
\nabla \phi = \lim_{\Delta V \to 0} \frac{1}{\Delta V} \iint \phi d\mathbf{S}.
$$ I do not understand this version. Is it even correct? If so, can someone explain how it agrees with the usual equation.
Edited: changed it to read 'surface integral'","['vector-fields', 'derivatives', 'differential-geometry']"
4923715,Is it possible to invoke the empty function?,"The empty function $f: \emptyset \rightarrow X$ is a function from the empty set to an arbitrary set $X$ . Since $\emptyset$ is the only subset of $\emptyset \times X = \emptyset$ , $f$ must be equal to $\emptyset$ . Is it possible to invoke the empty function? To clarify what I mean, consider a function $g: \{ 0 \} \rightarrow \{ 100 \}$ which is defined as $g(0) = 100$ . Note that I am allowed to write $g(0)$ to obtain an image of $0$ , which is $100$ . What is not clear for me is whether it is possible to use the same notation for the empty function; even if you were to invoke one as $f()$ , how do you know which element of the co-domain $X$ you obtain?","['elementary-set-theory', 'functions']"
4923724,"does dx equals to (x+h)-x? If it is, why isn't it explained like this?? [duplicate]","This question already has answers here : What actually is a differential? (3 answers) Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved While I was looking at a question regarding derivatives, I suddenly got enlightened when I realized, $dx=\lim_{h\rightarrow 0} (x+h)-(x)$ I noticed this while considering on the equation $\frac{df(x)}{dx}=\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-(x)}$ Up until now, i am surprised that I've never realized this before because now every differential equation and integrals make sense now. For example, dx at the end of the integral is actually meaning the width of the rectangle over the x axis. And also, I think it is much easier to grasp the differential equations or integral when it is shown like this. I wonder, is there anything wrong about this equation, if not, why isn't it taught in this way, Instead differentials and integrals are treated entirely different things, and we just try to deal with ""dx"" by not having a single idea about what that actually means.","['integration', 'differential', 'notation', 'education', 'derivatives']"
4923740,Derivatives of the determinant of a singular matrix w.r.t the matrix,"I have a $3 \times 3$ matrix $\bf{A}$ , and want to find the second order derivative of its determinant w.r.t the matrix itself $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ . Everything I can find online has assumed ${\bf{A}}$ to be invertible. In my case, $\bf{A}$ can be singular. I started from the Jacobian formula \begin{equation}
{\bf{A}} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} = \det(A) {\bf{I}}
\end{equation} Differentiate both sides w.r.t ${\bf{A}}$ , I got \begin{equation}
{\bf 1} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} + {\bf A} \cdot \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2} = \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} \cdot {\bf I}
\end{equation} where I use $\bf 1$ to denote the rank 4 identity tensor. However, when $\bf A$ is singular, this equation is also not helpful since I cannot solve $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ from it. So, for a general $3 \times 3$ matrix $\bf A$ which is not necessarily invertible, do we have a closed form solution for $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ ?","['tensors', 'multivariable-calculus', 'calculus', 'matrix-calculus', 'linear-algebra']"
4923746,How to apply integration by parts to simplify an integral of a cross product?,"I'm reading a physics paper and am trying to figure out how a certain expression is derived (If interested, see Appendix of the paper, Eq. (A7), (A8)). The authors skip a lot of derivation steps and at a certain point an expression like the following is encountered. Suppose we have the following: $$\iint_{S} \left[\nabla^{t}A \times \boldsymbol{B}^{t}\right]^{z} dxdy,$$ where $A$ is a scalar field, $\boldsymbol{B}$ is a vector field, $S$ lies completely in the xy-plane. The superscripts $t$ and $z$ denote the transverse (x, y) and longitudinal (z) components, i.e.: $$\nabla^{t}M = \left(\frac{\partial M}{\partial x}, \frac{\partial M}{\partial y},0\right)$$ is the transverse part of the gradient, and: $$\boldsymbol{N}^{t} = \left(N_{x}, N_{y}, 0\right);$$ $$\boldsymbol{N}^{z} = N_{z}.$$ At this point they state that they use ""an integration by parts"" and they somehow manage to move the nabla so that it is cross-product multiplied with $\boldsymbol{B}$ , i.e. the final expression should contain something like this: $$A(\nabla \times \boldsymbol{B})$$ together with other terms and the correct superscripts, which I have omitted here because I don't know them. Does anyone know some integration by parts formula that can help me get from the original expression to something looking like this? Any help and advice is much appreciated. Also, let me know if you would need more details. I've tried to keep things as abstract as I can.","['integration', 'multivariable-calculus', 'electromagnetism', 'vector-analysis']"
4923748,Difference between compensator of point process under real parameter an its MLE estimator,"Suppose we have some point process $N=N_{\theta_0}$ on the real line, driven by a conditional intensity $\lambda_{\theta_0}$ dependent on some finite-dimensional parameter $\theta_0\in\Theta\subset\mathbb R^d$ . Given an observation of this point process $N$ on $[0,T]$ , we intend to estimate $\theta_0$ by its maximum likelihood estimator (MLE) $\hat\theta_T$ , which is the solution to $$0=\int_0^T\frac{\dot\lambda_\theta(t)}{\lambda_\theta(t)}[\mathrm dN(t)-\lambda_\theta(t)\mathrm dt].$$ Details about such procedures can be found in The asymptotic behaviour of maximum likelihood estimators for stationary point processes by Ogata, 1978. I'm interested in the compensator of point processes, i.e. $\Lambda_\theta(t)=\int_0^t\lambda_\theta(s)\ \mathrm ds$ . In particular, I was wondering whether something can be said about the difference $\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)$ . Does this satisfy some limiting equation? Is anything known about this in the literature? E.g. is $T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T))$ Gaussian, or does it satisfy some (functional) CLT? It is known that $\hat\theta_T\to\theta_0$ in probability as $T\to\infty$ (see e.g. Theorem 2 from the paper by Ogata; which, however, does not say anything about the speed of convergence). Under regularity conditions from the same paper, $\lambda_\theta$ is locally Lipschitz in $\theta$ , so $\lambda_{\hat\theta_T}\to\lambda_{\theta_0}$ . Then it should not be too difficult to say something about $T^{-1}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T))$ , but I'm particularly interested in some nondegenerate limit for (something like) $$T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)).$$ Any help or reference is much appreciated.","['statistics', 'central-limit-theorem', 'point-processes', 'stochastic-processes', 'maximum-likelihood']"
4923780,Does $\sqrt{n}$-consistency imply consistency?,"It seems that $\sqrt{n}$ -consistency and consistency are quite different concepts. The former roughly says that most of the values attained by the scaled difference $\sqrt{n} (\hat{\theta}_n - \theta)$ are finite (regardless of $n$ ). The latter says that eventually most values of $\hat{\theta}_n$ are arbitrary close to $\theta$ (i.e. for large $n$ ). What is the formal relation between these two concepts? The accepted answer to this question states that $\sqrt{n} (\hat{\theta}_n - \theta) = O_p(1) \Longrightarrow \hat{\theta}_n - \theta = o_p(1)$ . Why is this implication true? How can I prove it? Here is my attempt at proving the above implication: Say $\forall \epsilon > 0$ $\exists M > 0$ such that $\sup_{n \in \mathbb{N}} P(\sqrt{n} \lvert \hat{\theta}_n - \theta \rvert > M) < \epsilon$ . That is, $\forall n \in \mathbb{N}$ $P(\sqrt{n} \lvert \hat{\theta}_n - \theta \rvert > M) < \epsilon$ . In particular, as $n \rightarrow \infty$ we have $P(\lvert \hat{\theta}_n - \theta \rvert > \delta_n) < \epsilon$ , where $\delta_n := \frac{M}{\sqrt{n}} \rightarrow 0$ . From here, I'm not sure how to proceed to conclude that $\hat{\theta}_n - \theta = o_p(1)$ . That is, I want to conclude that $\forall \delta > 0$ $P(\lvert \hat{\theta}_n - \theta \rvert > \delta) \rightarrow 0$ . I guess the reason I'm confused is that I've shown the convergence $P(\lvert \hat{\theta}_n - \theta \rvert > \delta_n) \rightarrow 0$ when $\delta_n \rightarrow 0$ . This seems unsatisfactory, because what I really ought to show is that $P(\lvert \hat{\theta}_n - \theta \rvert > \delta) \rightarrow 0$ for any $\delta > 0$ where $\delta$ is not required to change with $n$ . Is this perhaps not important?","['statistics', 'asymptotics', 'convergence-divergence', 'probability-theory', 'probability']"
4923823,"Prove that $\int _Xf_ngd\mu \overset{n\to\infty}{\to}\int _Xfgd\mu$ ,$\forall g\in \mathcal{L}^\infty (\mu )$ if it's true $\forall g\in C_b(X)$","Let $X$ be a Polish space and $\mu :\mathfrak{B}_X\to\overline{\mathbb{R}}$ a finite measure on the Borel subsets of $X$ . Suppose $(f_n)_{n\in\mathbb{N}}$ is a sequence of $\mathcal{L}^1(\mu )$ and $f\in \mathcal{L}^1(\mu )$ . If $\lim_{n\to\infty}\int _Xf_ngd\mu =\int _Xfgd\mu $ for all $g\in C_b(X)$ (continuous and bounded), can we conclude that $\lim_{n\to\infty}\int _Xf_ngd\mu =\int _Xfgd\mu $ for all $g\in \mathcal{L}^\infty (\mu )$ ? Using the 4.4.6 Theorem of the book ""Measure Theory"" (by V.I. Bogachev) and the Theorem 6.3.2 of the book ""Integration and Modern Analysis"" (by Benedetto and Czaja), we can conclude that $\lim_{n\to\infty}\int _Xf_ngd\mu =\int _Xfgd\mu $ , $\forall g\in \mathcal{L}^\infty (\mu )$ if and only if $\lim_{n\to\infty}\int _Bf_nd\mu =\int _Bfd\mu $ , $\forall B\in\mathfrak{B}_X$ . But I don't know how to prove previous limit. Also I wasn't able to find any counterexample.","['integration', 'measure-theory', 'analysis', 'lp-spaces', 'functional-analysis']"
4923848,"A question about the diagonal of the powers of real, symmetric matrices","Let $A \in M_{n\times n}(\mathbb{R})$ . Define $$\mathrm{diag}(A): M_{n\times n} \to M_{n \times 1} \text{ where } \mathrm{diag}(A) = \{a_{ii}\}_{i=1}^{n}$$ I have the following $\textbf{conjecture}$ : Let $A,B$ be real symmetric. There exists a unique permutation matrix $\pi$ for all $k\geq 0$ such that $$\mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k)$$ if and only if $$\mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k) ~~~(\star)$$ for $0\leq k \leq n$ . $\textbf{My approach:}$ ( $\implies$ ) follows by defintion. To show ( $\impliedby$ ), let $\ell > n$ . We can write the matrix $A^\ell $ as a linear combination of powers of $A$ , using Cayley-Hamilton theorem: $$A^\ell = c_{n-1}A^{n-1} + \cdots + c_1 A + c_0 I_n $$ where $c_i$ are real coefficients. We need a small lemma before we go: $\textbf{Lemma:}$ $$\mathrm{diag}: M_{n\times n} \to M_{n \times 1}$$ is a linear map.
Then it follows that $$\mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i)$$ Similarly, we get that $$\mathrm{diag}(B^\ell) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i)$$ Using $(\star)$ we get that $$\mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i) = \sum_{i=0}^{n-1} c_i \pi(\mathrm{diag}(B^i)) = \pi(\sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i)) $$ So it only remains to show that $$ \sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i)$$ I don't immidiately see why\if this is true. Would appreciate some general advice or reference.","['matrices', 'linear-algebra']"
4923854,How many numbers have a units digit that equals the digit sum of previous digits?,"How many numbers have a units digit that equals the digit sum of previous digits? No negative number solutions. No more than 3 digits Digit sum of all digits except the units digit must equal the units digit Feel free to solve without these specific restrictions Examples of a solution : 11, 22, 33... 101, 202, 303... 112, 224, 336... 123, 246 0 (347 is a solution because 3+4=7) How many total numbers exist? And what are some requirements for solutions other than this rule? I'm in high school and I thought of this problem and wasn't sure what field of math it would fall into but I think its an interesting challenge. Edits: @JMoravitz came up with a number of 55 solutions with 3 digits, but I was wondering if there could be a general summation for a variable number of digits. @aqualubix found general expression for the number of solutions with n digits.",['combinatorics']
4923879,"When solving a third-order ODE using reduction of order, what happens when the vâ term drops out","Given question Given $y=x$ is a solution of $6x^3yâââ - 24x^2yââ + 48xyâ-48y = 0$ . Find the general solution if $x>0$ . My work Since we are given that $y = x$ is a solution, my first approach would be to use reduction of order method. So, I would say that \begin{align}
y_2 &= v*y_1 \newline
y_2â &= v + vây_1 \newline
y_2ââ &= vâ + vâ + y_1vââ = 2vâ + y_1vââ \newline
y_2âââ&= 3vââ + y_1vâââ
\end{align} When I plug this into the original equation I get $$
6x^4vâââ - 6x^3vââ = 0
$$ Normally, when I use this method the v term will drop making my homogenous equation a function of $\frac{dv}{dx}$ âs but here my v term and vâ term dropped out. From some googling I found the following âIf youâve done all of your work correctly this should always happen. Sometimes, as in the repeated roots case, the first derivative term will also drop outâ but Iâm not quite sure what this means for my workâ¦. Iâm used to making the substitution $w=vâ$ . Could someone point me in the right direction?","['reduction-of-order-ode', 'calculus', 'ordinary-differential-equations']"
4923920,Expectation value in Markov process using backward generator and semigroup,"For the past few days, I've been studying continuous time Markov processes. I was making some exercises and ran into trouble with the following: Consider a continuous Markov process. The state space is $K = \{-1, +1\}$ with transition rates specified as by the parameters $v, H > 0$ in the following manner $$k(-1,+1) = v \quad \text{and} \quad k(+1,-1) = H/v.$$ We are then aksed to compute the following expectation value $$<e^{x_t}|x_0 = 1>,$$ for some time $t$ . Now I found that $$<e^{x_t}|x_0 = 1> = e \cdot S(t) = e \cdot e^{tL}$$ where S(t) is the semigroup (equivalent of transition matrix) and $L$ is the backward generator. I also found that $L$ is given by $$\begin{pmatrix}
-v & v\\
H/v & -H/v
\end{pmatrix}$$ However I do not see how I should now interpret $S(t)$ , since the question asks me to derive an expression for this expectation value in function of the parameters $v$ and $H$ . Ok, so I found out the expression i already have for the expression value is wrong and I could solve the exercise with the use of the Master equation, however I would like to know how i could sove it using the semigroup and generator formalism. I need to determine the probability at time $t$ to find the state in +1 or -1. I know that $$\mu_t = \mu_0S(t)$$ where mu_t is the probability row matrix at time $t$ and $\mu_0$ the same at time $0$ . So I actually just need to know how i can represent the matrix $S(t) = e^{tL}$ .","['markov-chains', 'markov-process', 'linear-algebra', 'probability-theory', 'probability']"
4923969,A kind of generalized eigenvalue problem,"Let $A$ , $B$ and $C$ denote three complex $2 \times 2$ matrices. Find necessary and sufficient conditions on $A$ , $B$ and $C$ for the existence of a nonzero $v \in \mathbb{C}^2$ such that $Av$ , $Bv$ and $Cv$ are all nonzero, and $Av$ , $Bv$ and $Cv$ are all multiples of each other. Motivation: I was playing around with a problem and was led to formulate the problem above. Well, actually, I am trying to prove a statement by contradiction. I assumed the statement was false, and then was able to show that it would imply the existence of a $v$ such as above for some specific $A$ , $B$ and $C$ . This led me to ask the question above, which is a kind of generalized eigenvalue problem, and can be formulated independently. Edit: I am thinking that the problem can be reformulated as to whether or not some rational cubic curve in $\mathbb{P}^3_\mathbb{C}$ intersects non-trivially the ""standard"" twisted cubic. Indeed, consider the map $\gamma$ from $\mathbb{C}^2$ to $\operatorname{Sym}^3(\mathbb{C}^2)$ which is defined by $$ \gamma(v) = Av \odot Bv \odot Cv. $$ This induces a cubic rational map $\tilde{\gamma}$ from $\mathbb{P}^1_\mathbb{C}$ to $\mathbb{P}^3_\mathbb{C}$ . I think that the image of this map can be written as an intersection of quadrics in $\mathbb{P}^3_\mathbb{C}$ , but I am not entirely sure. On the other hand, the twisted cubic is (up to some binomial coefficients) induced by the map $$ v \mapsto v \odot v \odot v.$$ And the image of the twisted cubic in $\mathbb{P}^3_\mathbb{C}$ is known to be the vanishing locus of an ideal which is generated by $4$ quadrics (it is not a complete intersection). So the problem is equivalent to whether or not the image of $\tilde{\gamma}$ intersects non-trivially the image of the twisted cubic. So I think it amounts to whether or not the common vanishing locus of finitely many quadrics in $\mathbb{P}^3_\mathbb{C}$ is non-empty. Also note that a similar question was asked before here, see @Theo Bendit's comment below. There is an answer in the linked post but I am hoping to get an alternative solution which is more algebro-geometric.","['linear-algebra', 'eigenvalues-eigenvectors']"
4924008,Taking derivative to determine differentiability,"The ""correct"" way to determine differentiability is to use the definition of the derivative as a limit. However, sometimes we have a function with a piecewise definition. Say for example (just an example, the question itself is more general) $f(x)=\begin{cases} 
e^x & x\leq 0\\
{\sin x} & x>0 \end{cases}$ A ""shortcut"" is to take the derivative of both pieces of the function at the ""breaking point"" $x=c$ and take the right hand and left hand limits of the derivatives as x approaches c. However, their equality does not always imply that the function is differentiable. In the above case this would not work: the limits of the derivatives of both parts are 1, but the function itself is not differentiable at $x=0$ because it is not continuous at the point. So it is only differentiable in the neighbourhood of x=0 but not at 0 itself. However, if the function above were continuous, say we had $(\sin x)+1$ instead, it would be differentiable at x=0.
It seems that continuity of f at x=0 was the ""missing piece"" to making it differentiable. So my question is, do the equality of the limits of the derivatives of $f$ from both sides of $x=c$ , AND the continuity of $f$ at $c$ , together imply that the function is differentiable at x=c? If yes can we prove it?","['calculus', 'derivatives']"
4924029,Nit picking at whether these two functions are the same,"I have a simple and dumb question: are these two functions the same? $$
f(x)=x 
$$ and $$
g(x)=\frac{x^2}{x}
$$ Obviously f and g are the same since you can simplify by x but are they really? $f$ is defined for all reals, but $g$ is not defined at $0$ . I am entirely self taught so I do not know if there is a formal difference between these two, hence why I am here.","['elementary-set-theory', 'algebra-precalculus', 'functions']"
4924068,Bounding the integral $\int_{a}^b t^{-1 + \delta } e^{2 \pi i t} dt$,"Let $1 < b$ and $0 < \delta \leq 1/2$ . I would like to show that the integral $$
\left| \int_{1}^b t^{-1 + \delta } e^{2 \pi i t}\; dt\right| 
$$ is bounded uniformly for any $1 < b$ . I would appreciate any suggestions on how to get started!
Thank you!","['integration', 'complex-analysis', 'definite-integrals']"
4924085,Can $f\circ f$ be defined when $f:\emptyset \to \mathbb{R}$?,"Along the lines of this post Let $f:\emptyset \to \mathbb{R}$ How can we define the composition of $f$ with itself? In any other case $f\circ f$ would be defined in a set $B=\{ x\in A  \land  f(x)\in A\}$ where A is the domain of $f$ . But since here $f$ is defined in the empty set , so the statements $x\in A$ and $f(x)\in A$ make no sense since there is no $x$ . What I have thought is $$(f\circ f)(x)=f(f(x)),\forall x\in\emptyset$$ If we are considering functions as sets of ordered pairs, the empty function is represented as the empty set of pairs: $$f=\emptyset$$ and so the composition $f\circ f $ would also be the empty set because there are no pairs to compose. $$f\circ f=\emptyset$$ From the post linked above and my experience. For a function $f:\emptyset \to Y$ the composition $f\circ f$ is again an empty function $\emptyset\to Y$ . This aligns with the formal properties of function composition where the domain remains empty and thus the result is trivially an empty function.","['elementary-set-theory', 'functions', 'function-and-relation-composition']"
4924148,Identity regarding the sum of products of binomial coefficients.,"Consider the following toy problem Person A and Person B have $n$ and $n+1$ fair coins respectively. If they both flip all their coins at the same time, what is the probability person B has more heads than person A? The answer, regardless of $n$ , is 50% -- which is pretty surprising.  To convince myself, I derived the distribution for the difference in heads between B and A.  Let $D=B-A$ and so $\Pr(D=k)$ is $$ \Pr(D=k \mid k>0) = \sum_{i=1}^{n+1} {n+1 \choose i}{n \choose i-k}2^{-2n-1} $$ and $$ \Pr(D=k \mid k\leq0) = \sum_{i=0}^{n} {n \choose i}{n+1 \choose i-k}2^{-2n-1} $$ If the answer to the question is 50%, this might mean that the distribution is symmetric about $k=0$ .  Naturally, I assumed that $\Pr(k=1) = \Pr(k=0)$ and tried to prove myself wrong or right. Using Maple (computer algebra) I tried to evaluate the sums for each, and in each case I am told that the sums (excluding the factor of $2^{-2n-1}$ ) are equal and equivalent to $$ {2n+1 \choose n}$$ See below Question Are the sums which involve the product of binomial coefficients some sort of identity? If so, what is the name of said identity and how are the two equivalent (I presume the answer lies in some index manipulation).","['summation', 'binomial-coefficients', 'combinatorics', 'probability']"
4924176,Choosing between the 'and' and 'implies' connectives,"$pol(x): x$ is a politician $liar(x): x $ is a liar All politicians are liars : $\forall x(pol(x) â liar(x))$ Some politicians are liars : $\exists x(pol(x) \land liar(x))$ No politicians are liars : $\forall x(pol(x) â Â¬liar(x))$ Some politicians are not liars : $\exists x(pol(x) \land Â¬liar(x))$ So, do we just use 'and' for $\exists$ statements and 'implies' for $\forall$ statements?","['quantifiers', 'logic', 'intuition', 'discrete-mathematics', 'logic-translation']"
4924256,What is the (fully rigorous) definition of a confidence interval?,"In a nutshell: what is the (fully rigorous) definition of a confidence interval? In page $92$ of Wasserman's All of Statistics , it is written that A $1 â Î±$ confidence interval for a parameter $Î¸$ is an interval $C_n = (a, b)$ where $a = a(X_1,...,X_n)$ and $b = b(X_1,...,X_n)$ are functions of the data such that $$P_Î¸(Î¸ â C_n) â¥ 1 â Î±, \ \ \ \ \text{ for all } Î¸ â Î.$$ In words, $(a, b)$ traps $Î¸$ with probability $1 â Î±$ . We call $1 â Î±$ the coverage of
the confidence interval. Warning! $C_n$ is random and $Î¸$ is fixed. I cannot understand the expression $P_\theta(\theta\in C_n)$ . In general, if we have a random variable $X:(\Omega,\mathcal{F},P)\to (\mathbb{R},\mathcal{B})$ , we define $$P(X\in S) := X_*P(S) = P(X^{-1}(S))$$ for $S$ in the Borel $\sigma$ -algebra $\mathcal{B}$ . Note the expression "" $P(X\in S)$ "" requires that $X$ be a random variable. $S$ be a fixed set. Neither of these conditions seem to be met with the expression "" $P(\theta\in C_n)$ "", as $\theta$ is an element of the parameter space $\Theta$ , which is itself a subset of $\mathbb{R}^n$ for some $n$ . That is, it seems to me that $\theta$ is a (fixed) vector, not a function (and thus not a random variable either). As $a$ and $b$ are functions of $X_1,\ldots,X_n$ , the interval $C_n := (a,b)$ seems to be ""variable"", when it should it be a fixed set for the expression to make sense. In case it is relevant, on page $89$ Wasserman explains that ... $P_\theta(X\in A) = \int_A f(x;\theta) dx$ ... which does make sense, as $\theta$ is fixed here, so that $f(x;\theta)$ is some random variable, while $A$ is a fixed set. However, the author is using the expressio $P_\theta$ differently in the main (first) quote of this post. I remain confused after reading this and this post.","['measure-theory', 'statistics', 'confidence-interval', 'definition', 'probability']"
4924259,"Show that there are only finitely many pairs of positive integers $(n, m)$ such that $d(m!) = n!$.","Show that there are only finitely many pairs of positive integers $(n, m)$ such that $d(m!) = n!$ , where $d(n)$ denotes the number of positive divisors of $n$ . My approach (it isnât complete and probably not correct). Any help in writing a complete and correct proof : To show that there are only finitely many pairs of positive integers $(n, m)$ such that $d(m!) = n!$ , where $d(n)$ denotes the number of positive divisors of $n$ , we need to compare the growth rates of $d(m!)$ and $n!$ . Understanding the Growth of $d(m!)$ The number of divisors of $m!$ , denoted $d(m!)$ , depends on the prime factorization of $m!$ . If $m!$ is expressed as $p_1^{e_1} p_2^{e_2} \cdots p_k^{e_k}$ , where $p_i$ are primes and $e_i$ are their respective exponents, then:
The number of divisors $d(m!)$ is $(e_1 + 1)(e_2 + 1) \cdots (e_k + 1)$ . The exponents $e_i$ for each prime $p_i$ can be approximated as: $e_i$ is roughly the sum of the integer divisions $\left\lfloor \frac{m}{p_i} \right\rfloor + \left\lfloor \frac{m}{p_i^2} \right\rfloor + \left\lfloor \frac{m}{p_i^3} \right\rfloor + \cdots$ . For large $m$ , the dominant term in this sum is approximately $\frac{m}{p_i}$ . Comparing Growth Rates
Let's approximate the growth rate of $d(m!)$ :
The number of prime numbers less than or equal to $m$ is approximately $\frac{m}{\log m}$ by the Prime Number Theorem.
Each $e_i$ is roughly $\frac{m}{p_i}$ , and the product $(e_1 + 1)(e_2 + 1) \cdots (e_k + 1)$ grows very rapidly. Growth of $n!$ :
Stirling's approximation for $n!$ provides a way to understand its growth: $n!$ grows roughly as $\sqrt{2 \pi n} \left(\frac{n}{e}\right)^n$ . Comparing $d(m!)$ and $n!$ To compare $d(m!)$ and $n!$ : $d(m!)$ grows super-exponentially with $m$ , much faster than the factorial function $n!$ grows with $n$ . For large $m$ , $d(m!)$ becomes significantly larger than any $n!$ . This means that for sufficiently large $m$ , there cannot be an $n$ such that $d(m!) = n!$ . Since $d(m!)$ grows much faster than $n!$ as $m$ and $n$ increase, there can only be finitely many pairs $(n, m)$ such that $d(m!) = n!$ . Once $m$ reaches a certain threshold, $d(m!)$ will exceed $n!$ for all $n$ , limiting the possible pairs to a finite set.","['contest-math', 'number-theory', 'divisor-counting-function', 'modular-arithmetic']"
4924296,Is there a $f: \mathbb{N} \to \mathbb{N}$ such that $\sum_{n=1}^{\infty} \frac{1}{n^2f(n)} \in \mathbb{Q}$?,"Take by convention $0 \not \in \mathbb{N}$ , and let $f: \mathbb{N} \to \mathbb{N}$ . Define the real number $N(f)$ by $$N(f) = \sum_{n=1}^{\infty} \frac{1}{n^2f(n)}.$$ $N(f)$ is well-defined because, for example, each summand is a positive number less than $\frac{1}{n^2}$ , so $N(f) \leq \frac{\pi^2}{6}$ , so we have a bounded above sequence of positive numbers. My question: is $N(f)$ ever rational? Note that if it is the case that $N(f)$ is never rational, we're almost certainly not going to be able to prove it in this thread. By taking $f(n)=n$ we obtain $\zeta(3)$ , which was only proven to be irrational in the 1970s, and apparently ""at least one of $\zeta(5), \zeta(7), \zeta(11),\zeta(13)$ must be irrational"" ( see link ), so I assume the irrationality of these numbers individually is unknown. But I'm wondering if I'm missing some $f$ for which it is not too hard show is a counterexample to the conjecture "" $N(f)$ is never rational."" What I've tried thinking about is defining $f$ recursively, but not gotten far. If we say $a_k, b_k$ are defined such that $$\frac{a_k}{b_k} = \sum_{n=1}^{k} \frac{1}{n^2f(n)}$$ is in lowest terms, then we know $$\frac{a_{k+1}}{b_{k+1}} = \frac{a_k}{b_k} + \frac{1}{(k+1)^2f(k+1)} = \frac{a_k (k+1)^2f(k+1) + b_k}{b_k (k+1)^2f(k+1)},$$ so in particular $da_{k+1} = a_k (k+1)^2f(k+1) + b_k$ , $db_{k+1} = b_k (k+1)^2f(k+1)$ , where $d$ is the gcd of the numerator and denominator. If we can pick $f(k+1)$ in such a way to make $d$ large, we can have the denominators of the partial sums grow slowly, which may at least be a starting point for looking for such an $f$ , I'm not sure.","['number-theory', 'irrational-numbers', 'rational-numbers']"
4924327,Prof gave us wrong definition of convexity?,"My professor gave us this exercise: Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function. Given the following two definitions of convexity of $f$ , prove that (i) implies (ii): (i) $\forall x, y \in \mathbb{R} : f(x) \ge f(y) + f'(y)(x - y)$ (ii) $\forall x, y \in \mathbb{R}, \forall \lambda \in [0, 1] : f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y)$ I think definition (i) is incorrect. I think it would be correct, if we would assume $x,y$ arbitrary but we must have $x \geq y$ . Since (i) is differently written: $$\frac{f(x)-f(y)}{x-y} \ge f'(y)$$ So the gradient/slope from $y$ to $x$ is greater than the slope of $y$ . That makes sense because in a convex function the slope increases. But if we choose $y$ to be greater than $x$ , then it's wrong. Let me give this example: We have $f(x)=x^2$ and $x,y=\pm 1$ . The slope from $x$ to $y$ (or $y$ to $x$ ) is $0$ . It's the black line in the plot. The slope at $-1$ is $-2$ , that's the green line. So we indeed have $-2 \le 0$ . But if we pick $x$ and $y$ to be $x=-1$ and $y=1$ , then the statement (i) is false, since it would state that $0 \ge 2$ . So to me it seems, the correct definition would be: (i) $\forall x, y \in \mathbb{R}$ with $x \ge y : f(x) \ge f(y) + f'(y)(x - y)$","['convex-analysis', 'analysis', 'real-analysis']"
4924343,Where is the error in evaluating this series?$\sum_{n=1}\frac{1}{n^2+x^2}$,"Where is the error in evaluating this series? $$\sum_{n=1}\frac{1}{n^2+x^2}$$ My attempt: $$\begin{align}\sum_{n=1} \frac{1}{n^2+x^2}&=\frac{1}{2ix}\sum_{n=1}\frac{1}{n-ix}-\frac{1}{n+ix}\\&=\frac{1}{2ix}\left[{\sum_{n=1}\frac{1}{n+1}-\frac{1}{n+ix} -\sum_{n=1}\frac{1}{n+1}-\frac{1}{n-ix}}\right]\\&=\frac{1}{2ix}(\psi(ix)+\gamma)-(\psi(-ix)+\gamma))\\&=\frac{1}{2ix}(\psi(ix)-\psi(-ix))=\frac{1}{2ix}(-\pi\cot(i\pi x)-\frac{1}{ix}) \end{align}$$ we have : $$\cot(i\pi x) = i\frac{e^{-\pi x}+e^{\pi x}}{e^{-\pi x}-e^{\pi x}}$$ therfore: $$\sum_{n=1}\frac{1}{n^2+x^2}=\frac{1}{2ix}\left({-\pi  i\frac{e^{-\pi x}+e^{\pi x}}{e^{-\pi x}-e^{\pi x}}}\right)+\frac{1}{2x^2}=\frac{\pi}{2x}\left({  \frac{e^{-\pi x}+e^{\pi x}}{e^{\pi x}-e^{-\pi x}}}\right)+\frac{1}{2x^2}$$ but in the book SUMMATION OF SERIES COLLECTED BY L. B. W. JOLLEY, M.A. (cantab.), M.I.E.E. proplem 124 page 22
The answer has been provided $$\sum_{n=1}\frac{1}{n^2+x^2}=\frac{\pi}{2x}\left({  \frac{e^{-\pi x}+e^{\pi x}}{e^{\pi x}-e^{-\pi x}}}\right)-\frac{1}{2x^2}$$ I can't figure out where I did wrong??  Are there other ways to evaluate this series????","['summation', 'digamma-function', 'sequences-and-series']"
4924384,Estimate a sum of binomial coefficients,"I should know this by the time, but: can someone tell me how to rigorously compute  the leading order (including the constant) of the following sum: $$\sum_{ 1\leq k \leq n/3 } {2 k \choose k} {n-2k-1 \choose k-1}$$ as $n \to \infty$ ; I know how to derive it intuitively from Stirling (which allows to locate those relevant terms for the sum), but I am interested in a rigorous derivation. Pointers to appropriate references will be also appreciated ! (I expect the following asymptotics : $$\frac{1}{4 \sqrt{\pi n}} 2^n,$$ but I am unsure about the constant $1/4$ - the answer by Claude rather points to 1/2!)","['laplace-method', 'analytic-combinatorics', 'binomial-coefficients', 'combinatorics', 'limits']"
4924406,How do I find the arc length of $y=1-e^{-x}$ from $0 \leq x \leq 2$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 days ago . Improve this question I can set up the integral $\int_{0}^{2} \sqrt{1+e^{-2x}}\,dx$ by taking the derivative of y and by using the arc length formula. I'm really stuck on how to evaluate this integral. I've tried to follow the steps on symbolab and wolfram alpha but cannot follow them.","['integration', 'calculus', 'arc-length']"
4924419,Calculation of the volume of a solid of rotation,"Calculate the centroid of the homogeneous solid generated by the rotation around the y-axis of the domain in the xy-plane defined by: $$D=\left \{(x,y)\in \Bbb R^2 : x \in [1,2],0\leq y \leq \frac{1}{x}\right\}$$ I'm calculating only the ordinate of this centroid because all other coordinates will be zero. $$y_G = \frac{\iiint_V y \ dx \ dy \ dz}{(V)}$$ I'm applying the Guldinus theorem related to solids of revolution. $$V=2\pi \iint_D x \ dx \ dy \Rightarrow 2\pi \int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 dy \Rightarrow 2\pi \int^2_1 dx \Rightarrow 2\pi$$ I'm doing the numerator in the same way. $$y_G = \frac{2\pi \iint_D xy \ dx \ dy}{2\pi}$$ I'm applying the Tonelli-Fubini reduction formula $$\int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 y \ dy \Rightarrow \frac{1}{2} \int^2_1 x \ [y^2]^{\frac{1}{x}}_0 \ dx \Rightarrow \frac{1}{2} \int^2_1 \frac{dx}{x} = \frac{1}{2} \log|x| \Bigg|^2_1 \Rightarrow \frac{1}{2}\log2$$ My questions are: Was the exercise performed correctly? Are there other ways to do this exercise?","['integration', 'multivariable-calculus', 'real-analysis']"
4924421,Balanced coloring for $\mathbb{Z}^n$ with $m$ colors,"Call a coloring using $m$ colors on a finite number of points in $\mathbb{Z}^n$ balanced if for any line parallel to one of $n$ axes, the difference between the number of points for any 2 colors is at most 1.
Does such a coloring exist for all $m, n \ge  2$ ? Note that this problem is a generalization for IMO 1986 problem 6 where $m = n = 2$ . I solved the original problem using induction on the number of points by removing one that lies on a line with an odd number of points and tackle the case when all lines have an even number of points by considering the simple graph with vertices as the horizontal and vertical lines that contain at least one point, which are finitely many, edges as the mentioned integer points connecting a horizontal and vertical line sharing one and use the fact that this graph is Eulerian to color the edges so that two adjacent edges in the cycle are colored differently. I have tried to create a similar reasoning when $m > 2$ or $n > 2$ but failed. I would appreciate any improvement to my method or a solution to the above problem even if itâs just the case $m = 2$ or $n = 2$ .","['integer-lattices', 'combinatorics']"
4924468,"Conditions on the constants for two ""linear"" functions to commute under the operation of function composition","Let $a$ , $b$ , $c$ , and $d$ be any complex numbers such that $a \neq 0$ and $c \neq 0$ ; let the functions $f \colon \mathbb{C} \longrightarrow \mathbb{C}$ and $g \colon \mathbb{C} \longrightarrow \mathbb{C}$ be the functions defined by $$
f(x) \colon= ax+b \qquad \mbox{ and } \qquad  g(x) \colon= cx+d \qquad \mbox{ for each } x \in \mathbb{C}. \tag{0}
$$ Then $$ 
f \circ g = g \circ f \tag{1} 
$$ if and only if $$
f\big( g(x) \big) = g\big( f(x) \big) \qquad \mbox{ for each } x \in \mathbb{C},
$$ that is, $$
a g(x) + b = c f(x) + d \qquad \mbox{ for each } x \in \mathbb{C},
$$ which then becomes $$
a (cx+d) + b = c (ax+b) + d \qquad \mbox{ for each } x \in \mathbb{C},
$$ which simplifies to $$
acx + (ad + b) = ac x + (bc + d) \qquad \mbox{ for each } x \in \mathbb{C},
$$ and hence $$
ad + b = bc + d,
$$ which is equivalent to $$
ad - bc = d - b. \tag{2} 
$$ Is the  condition (2) the most general condition for (1) to hold? If so, is there a more illuminating way that we can express this condition? If not, then what are the most general necessary and sufficient condition(s) for (1) to hold?","['functions', 'solution-verification', 'function-and-relation-composition']"
4924488,"What are ordered pairs, and how does Kuratowski's definition make sense?","I have been watching the YouTube series 'Start Learning Mathematics' by The Bright Side of Mathematics.
I am currently on episode #3 of the set series and he's just introduced us to 'ordered pairs.'
In more detail, the episode is about the Cartesian product, and how it is the set of all ordered pairs of sets A & B (at least this is what I understood from it). He gives the proper definition: $$A\times B:= \{(a,b)\mid a\in A\ \wedge\  b\in B\}$$ And then he defines the ordered pair through Kuratowski's method/proof.
He starts by saying For elements $x,y$ write: $$(x,y) := \{\{a\},\{a,b\}\}$$ and $$(x,y)=(\tilde{x},\tilde{y}) \Leftrightarrow \{x\}=\{\tilde{x}\}\ \wedge\ \{y\}=\{\tilde{y}\}$$ $$\Leftrightarrow x = \tilde{x}\ \wedge\ y = \tilde{y}$$ It has left me very confused. I mostly understand the last bit, it seems obvious that if you have points $(a,b) = (c,d)$ then $a = c$ and $b = d$ , but I still don't understand why the first set has $\{a,b\}$ in it and not just $\{b\}$ , and what the entire point of these are. I would appreciate any help, thank you!","['elementary-set-theory', 'order-theory']"
4924513,Stable bundles over $\Bbb P^1$.,"Is there a direct way to see that the only stable vector bundles over $\Bbb P^1$ are line bundles? I think that this can be shown using the BirkhoffâGrothendieck theorem, but it seems like an overkill here. If $E$ is a rank $r$ vector bundle over $\Bbb P^1$ and $F \subset E$ any subbundle, then the stability condition gives $$
\mu(F) < \mu(E) \iff \frac{\int_{\Bbb P^1} c_1(F)}{\text{rank}(F)} < \frac{\int_{\Bbb P^1} c_1(E)}{\text{rank}(E)} \iff  \text{rank}(E)\int_{\Bbb P^1} c_1(F) < \text{rank}(F)\int_{\Bbb P^1} c_1(E).
$$ However, I can't find a direct way to argue from here that $\text{rank}(E) =r= 1$ . What additional information do I need?","['complex-geometry', 'vector-bundles', 'differential-geometry']"
4924628,Defining matrix function through its diagonalization,"Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth function. Let $Dg(n)$ be the set of diagonal $n \times n$ matrices, and define $\overline{f}:Dg(n) \to Dg(n)$ in the following way: first, for a diagonal matrix $D=diag(\lambda_1,\dots,\lambda_n)$ , $$\overline{f}(D)=diag(f(\lambda_1),\dots,f(\lambda_n)).$$ For a general diagonalizable matrix $A=PDP^{-1}$ , define $$\overline{f}(A)=P \overline{f}(D) P^{-1}.$$ What are the properties of $\overline{f}$ ? Is it still smooth? Motivation: the idea is to find a different way to show that the exponential matrix and logarithm are smooth functions, which would be a particular case of above when $f=\exp,\log$ respectively.","['matrices', 'derivatives', 'linear-algebra', 'continuity']"
4924642,UMVU estimator of $\lambda^2$ via Rao-Blackwell,"I have been working on a problem, which goes as follows: Given the statistical model $(\mathcal{X},\mathcal{B},\mathcal{P})$ , where $\mathcal{P}=\{P_{\lambda}^{\otimes}:P_{\lambda}=Pos(\lambda), \lambda>0\}$ , use the Rao-Blackwell
theorem, to find an UMVU estimator (UMVUE) for $\lambda^2$ via $\hat{\gamma}: \mathbb{N}^n_0 \longrightarrow \mathbb{N}_0 $ , $\hat{\gamma}(x)=x_1x_2$ , with respect to the statistic $T:\mathbb{N}^n_0 \longrightarrow \mathbb{N}_0, T(x)=\sum_\limits{{i=1}}^n x_i$ . In other words we have to calculate the following expected value: $$\hat{\gamma}^*(t):=\mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t), \quad t \in \mathbb{N}_0$$ where $X=(X_1, \dots , X_n)$ , ( $X_1, \dots , X_n \stackrel{iid}{\sim}Pos(\lambda))$ is the random variable inducing the statistical model. We need $T$ to be a complete and sufficient statistic and $E_{\lambda}(\hat{\gamma})<\infty$ , for $\hat{\gamma}^*$ being UMVUE. I already solved this by applying Lehmann-Scheffe, to another  unbiased estimator for $\lambda^2$ and the same statistic $T$ , then concluded that $\hat{\gamma}^*$ has to be equal to my UMVUE, since UMVUE are unique. I then tried to calculate the above expected value, but I am stuck evaluating it, here is what I have done so far : For $t \in \mathbb{N_0}$ : $\mathbb{E}_{\lambda}(\hat{\gamma}(X)|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X=x|T(X)=t)=\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t)$ I have concluded that: $$\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n|\sum_\limits{{i=1}}^n X_i=t)=
\frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n, \sum_\limits{{i=1}}^n X_i=t)}{\mathbb{P}_{\lambda}(\sum_\limits{{i=1}}^n X_i=t)}=
\frac{\mathbb{P}_{\lambda}(X_1=x_1, \dots , X_n=x_n)\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}=\frac{e^{n\lambda}\frac{\lambda^{t}}{x_1!\dots x_n!}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}}{e^{n\lambda}\frac{(n\lambda)^{t}}{t!}}=
\frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}$$ I then tried to proceed with the evaluation : $$\sum_\limits{x \in \mathbb{N}_0^n}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}\chi_{\{\sum_\limits{{i=1}}^n X_i=t\}}=\sum_\limits{x \in \{\sum_\limits{{i=1}}^n x_i=t\}}x_1x_2\frac{t!}{x_1!\dots x_n!n^t}=$$ $$\sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\sum_\limits{x \in \{\sum_\limits{{i=3}}^n x_i=t-x_1-x_2\}}\frac{(t-x_1-x_2)!}{x_3!\dots x_n!n^{t-x_1-x_2}}=\sum_\limits{x_1,x_2 \in \{x_1,x_2>0, x_1+x_2 \leq t\}}\frac{x_1x_2}{x_1!x_2!}\stackrel{?}{=}\frac{t(t-1)}{n^2}$$ My Question : Im not quite sure, about the last two equations, if the second last is correct, then the second sum would be equal to one, since we are summing over the support of a density function, but I doubt that the very last ""equation"" would follow, but this is what it should be equal to. Any help is appreciated!","['statistics', 'parameter-estimation', 'umvue', 'poisson-distribution']"
4924682,Rolling an elliptical disc on the $x$ axis,"You're given the elliptical disc bounded by $ \dfrac{x^2}{a^2} + \dfrac{(y - b)^2}{b^2} = 1 $ where $a = 5, b = 2 $ .  You roll this ellipse to the right along the positive $x$ axis, such that it is always tangent to the $x$ axis.  You stop rolling when the elliptical boundary of the disc becomes tangent to the $x$ axis at $(5, 0)$ .  If a point $P_0 = (-2, 3) $ is on the disc before rolling, find its position after rolling. My attempt: The distance travelled on the $x$ axis, is the same traversed on the boundary of the ellipse.  From the parametric equation of the ellipse, one can determine the eccentric angle of the point of tangency on the rolled ellipse using the arc length (which is equal to $5$ ).  Then, the initial and final normal vectors to the circumference of the ellipse can be computed, and the angle between them determines the rotation of the ellipse.  Having the tangency point and the rotation determines the center of the rolled ellipse.  The affine transformation between the initial points on the disc and rolled images is given by $ p' = C + R (p - C_0) $ where $R$ is the rotation matrix resulting from rolling, $C_0$ is the initial center (equal to $(0, b)$ ) and $C$ is the final center.  Using this equation with $p = (-2, 3)$ , one can compute its image $p'$ .","['analytic-geometry', 'conic-sections', 'geometry', 'linear-algebra', 'rotations']"
4924694,Inscribing two circles and an ellipse in a square,"A square of given side length $S$ is to inscribe two circles and an ellipse as shown in the figure below. If the radius of circle $(1)$ is given, determine the center and radius of circle $(2)$ , then determine the inscribed ellipse $(3)$ . My attempt: From tangency between circles $(1)$ and $(2)$ , it is fairly easy to find the radius of circle $(2)$ , and this also gives its center. For the ellipse, the center has to lie halfway between the top and bottom sides of the square.  What is unknown is the $x$ coordinate of this center ( $C_x$ ).  The points of tangency with the two circles are also unknown, and in addition the semi-major and semi-minor axes of the ellipse and the angle of rotation of the major axis from the standard orientation along the $x$ axis is also unknown.  Using parametric equations for the two circles, we need two parameters $t_1$ and $t_2$ (unknown) to describe the two tangency points with the ellipse. So the unknowns are: $t_1, t_2, a, b, \theta, C_x $ . Six equations can be written to describe the tangency with the top (and the bottom) side, and the right side of the square, and tangency with the two circles. I leave it open to the reader to develop these equations, and run an example of this setup in order to determine the upper left circle (Circle $2$ ) and the ellipse on the right $(3)$ .","['analytic-geometry', 'conic-sections', 'geometry', 'linear-algebra', 'algebra-precalculus']"
4924696,Why are Lebesgue integrals defined as a supremum and not as a limit?,"We may approximate a bounded nonnegative function $f$ by simple functions $\phi$ . Then the  standard way of defining the Lebesgue integral of $f$ is $$ \int f d\mu := \sup \Bigg\{ \int \phi : \phi \leq f, \phi \textrm{ is simple}\Bigg\}.$$ I am very comfortable with this definition but it was not until recently while learning about Ito integrals that I began thinking about why the Lebesgue integral is not defined as a limit. Loosely speaking, if $X_t$ is a stochastic process and $B_t$ a Brownian motion, then the Ito integral $\int_0^t X_s B_s$ is defined by first finding a sequence of elementary processes $E^n_t$ that converge to $X_t$ in $L^2$ , and then defining $$\int_0^t X_s dB_s := L^2-\lim_{n \rightarrow \infty} \int E_s^n dB_s.$$ I am curious why a similar construction doesn't work for the Lebesgue integral. That is, if $\{\phi_n\}$ are simple and $\phi_n \rightarrow f$ uniformly, then why can we not define $$\int f d\mu := \lim_{n\rightarrow \infty} \int\phi_n d\mu.$$","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
4924720,Calculate the Integral $\iint x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last month . Improve this question The integral that I need to calculate is the integral of the second kind: $$I=\iint_{S} x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy$$ with $S$ being the boundary of the following object: $x^2 + y^2 \leq z \leq 1$ . I have not so much experience with multivariable calculus, so I am not so familiar with the notation. I figured (allegedly) that it is surface integral, and the region would be something like $$0\leq x^{2}+y^{2}\leq z\leq 1\implies \begin{cases}
-1\leq x\leq 1 \\
-1\leq y\leq 1 \\
0\leq z\leq 1
\end{cases}$$ Integrating the function itself is easy but the integral is what I do not know how to solve. Can someone help me with this? I believe that I understood the problem wrong. For those that want to close the question because of the lack of detail, that thing is the entire statement of the problem. Nothing else. And I am being thrown off because there's someone who also said that I have to instead solve the problem as $$I=\iint_{S} x^{2}\:dydz + \iint_{R} (x^2 + y^2 + z^2 )\:dx dy$$ for another object $R$ Attempt 1 Alright, let's try it again. For this problem, I think the surface would be $$x^{2}+y^{2}=z, 0\leq z\leq 1$$ This means that we can parameterize it as: $$r(v,u)=(v\cos{(u)}, v\sin{(u)}, v): \begin{cases}
0\leq u\leq 2\pi \\
0\leq v\leq 1
\end{cases}$$ The integral is seems to be denoted in differential form, which is $$I=\iint_{S} x^{2}\:dy\wedge dz + (x^2 + y^2 + z^2 )\:dx\wedge dy$$ As provided by Kurt. G. Then, we can obtain the vector field $F=(x^{2},0,x^{2}+y^{2}+z^{2})$ .
Then, I can calculate the surface integral now using: $$\iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv$$ Am I right? Attempt 2 Okay, I was dumb. Now, the parameterization is wrong.
if $(x,y)=v\cos(u),v\sin{(u)}$ , then $z=v^{2}$ . Hence, instead, we have... $$r(u,v)=(v\cos{(u)},v\sin{(u)},v^{2}),\begin{cases}
0\leq u\leq 2\pi \\
-1\leq v \leq 1
\end{cases}$$ Theoretical Concepts Since I want a bit of mind toward the theoretical base that I am working on, I'll just leave it here.
Let $S$ be a smooth parameterized surface: $$\begin{cases}
x=f(u,v) \\
y = g(u,v) \\
z = h(u,v) \\
(u,v)\in D
\end{cases}$$ with orientation corresponding to the ordering $u,v$ . Then, we have $$dx=\frac{\partial x}{\partial u}+\frac{\partial x}{\partial v}dv$$ Similarly, $$dy=\frac{\partial y}{\partial u}+\frac{\partial y}{\partial v}dv$$ Hence, $$\begin{split}
dx\land dy & = \left( \frac{\partial x}{\partial u}du + \frac{\partial x}{\partial v}dv \right)\land\left( \frac{\partial y}{\partial u}du + \frac{\partial y}{\partial v}dv \right)\\ 
& = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv
\\& =\frac{\partial(x,y)}{\partial(u,v)}du\land dv
\end{split}$$ Working all the same, we got $$\begin{align}
dx\land dy = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv
 =\frac{\partial(x,y)}{\partial(u,v)}du\land dv \\
dy\land dz = \left( \frac{\partial y}{\partial u} \frac{\partial z}{\partial v}- \frac{\partial y}{\partial v} \frac{\partial z}{\partial u} \right) du \land dv
 =\frac{\partial(y,z)}{\partial(u,v)}du\land dv \\
dz\land dx = \left( \frac{\partial z}{\partial u} \frac{\partial x}{\partial v}- \frac{\partial z}{\partial v} \frac{\partial x}{\partial u} \right) du \land dv
 =\frac{\partial(z,x)}{\partial(u,v)}du\land dv
\end{align}$$ Then, we can use $$\iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv$$ which is guaranteed by Stoke's theorem. Working out the whole thing is a problem, since it's quite long. Still, writing everything in order as $$I=\iint_{S} (x^{2} + y^{2} + z^{2} )\:dx \land dy +x^{2}\:dy\land dz$$ For the first term, converting to the new coordinate $(u,v)$ : $$\begin{cases}
r_{1}=v^{2}\cos^{2}{(u)}+v^{2}\sin^{2}{(u)}+(v^{2})^{2}=v^{2}(1+v^{2})\\
r_{2}=v^{2}\cos^{2}{(u)}
\end{cases}
$$ For a vector field $F(r_{1},r_{2},0)$ . I think I will be able to solve this more, but it is quite long, and after this it is kind of trivial, so if I am right, this is the path of solution.","['integration', 'multivariable-calculus']"
4924750,What does it mean when the transition function of a NFA returns an empty set?,"Given a NFA, $N = (Q, \Sigma, q_0, \partial, F_Q)$ , where $\partial$ is the transition function $Q \times (\Sigma \cup \{ \varepsilon \} ) \to \mathcal{P}(Q) $ . So $\partial(q, a)$ returns a set, which can be any element of $\mathcal{P}(Q)$ . Sounds great, but I don't know how to interpret what's going on when the return value of the transition function is an empty set $\emptyset$ . Say the machine is at the state $q$ , and I apply the transition $\partial(q, a) = \emptyset$ , will it end up in nowhere? Is it the same as saying the machine ""halt""? So the input string is rejected and all the following symbols are discarded? I interpret it like this: $\partial(q, a) = \emptyset$ means that the pair $(q, a)$ has a special transition that does nothing. Is this correct? Background: I'm reading the online book here . In the book, they define/require the transition function of a DFA to be a total function $Q\times\Sigma \to Q$ , so I never think about this ""end up in nowhere"" problem before. In this section, I will provide an example to show what confuses me. Given a NFA, $N=(Q,\Sigma, q_0, \partial, F)$ , where $Q=\{q_0, q_1, q_2\}$ , $\Sigma = \{a, b\}$ , and $F=\{q_1\}$ , and the transition function is defined as follow: \begin{align}
\partial (q_0, a) &= \{q_1\} \\
\partial (q_0, b) &= \{q_2\} \\
\partial (q_1, a) &= \{q_0\} \\
\partial (q_1, b) &= \emptyset \\
\partial (q_2, a) &= \emptyset \\
\partial (q_2, b) &= \{q_0\} \\
\end{align} and there is no $\varepsilon$ -transition on all states in $Q$ . Given the starting state is $q_0$ and the input string is $ab$ , then: It could get rejected. The fact that $\partial(q_1, b)=\emptyset$ doesn't make me confident that $b$ would be consumed. It could get accepted. Because in the end, the machine needs to read/ consume the input symbol regardless of the image of $\partial(q_1, b)$ . If $\partial(q_1, b)=\emptyset$ would indeed consume the symbol $b$ without making any move, it would stay in the state $q_1$ . If $q_1$ is a final state, it will get accepted.","['automata', 'computability', 'discrete-mathematics', 'computer-science']"
4924753,"The sequence $0, 0, 1, 1, 3, 10, 52, 459, 1271, 10094, 63133,...$","Let $a_0$ be a permutation on $\{1, 2, ...,N\}$ (i.e. $a_0 \in S_N$ ) . For $n \geq 0$ : If $a_n(i+1) \geq a_n(i)$ , then $a_{n+1}(i) = a_n(i+1) - a_n(i)$ . Otherwise, $a_{n+1}(i) = a_n(i+1) + a_n(i)$ . $a_{n}(i)$ is defined for $0<iâ¤N-n$ . My question is this: For a given $N$ , how many $a_0 \in S_N$ satisfies $a_{N-1}(1) = 0$ ? (Can anybody provide an exact expression? If not, can anybody provide an approximation at least?) An example of such permutation for $N=5$ is $(1, 4, 5, 2, 3)$ $$(1, 4, 5, 2, 3) \rightarrow (3, 1, 7, 1) \rightarrow (4, 6, 8) \rightarrow (2, 2) \rightarrow (0)$$ I calculated the number if such permutations by bruteforce for $1\leq N\leq11$ : $0, 0, 1, 1, 3, 10, 52, 459, 1271, 10094, 63133$ I see no pattern, and the sequence doesn't appear anywhere on the OEIS.","['number-theory', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics', 'integer-sequences']"
4924770,Variations of Dawson's function,"I am studying Dawson's function : $\displaystyle F : x \mapsto e^{-x^2}\int_0^x e^{t^2} dt$ . I would like to prove that $F$ attains a maximum at a certain value $x_0 \in (0,1)$ , and is increasing over $[0,x_0]$ and decreasing over $[x_0, +\infty)$ . The only thing I managed to prove about this function is that $\displaystyle\lim_{x \rightarrow +\infty} F(x)=0$ , which gives the existence of a maximum over $[0,+\infty)$ , but does not help to determine the variations of $F$ . I also know that $F$ is solution of the differential equation $y'+2xy=1$ , but I cannot see how it could help. So the question is : how to determine the variations of $F$ ? (and bonus : how to prove that $x_0 <1$ ?) EDIT : Thanks to @NinadMunshi's answer, I understand now why there exists $x_0 \in (0,1)$ such that $F'(x_0)=0$ .
But I still don't understand how to prove that $x_0$ is the only root of $F'$ over $[0,+\infty)$ (which would solve my initial question) Does anybody know how to tackle this question ?","['integration', 'calculus-of-variations', 'ordinary-differential-equations', 'real-analysis']"
4924775,Does $x'' + x + \arctan x = \lambda \sin t$ have a $2\pi$ periodic solution when $\lambda <2$ï¼,"It's easy to show that the equation $x'' + x + \arctan x = \lambda \sin t$ doesn't have a $2\pi$ periodic solution when $\lambda \geq 2$ : Suppose $\lambda \geq 2$ and $\varphi(t)$ is a $2\pi$ periodic solution then $$
\begin{align}
\lambda \pi &= \lambda \int _{0}^{2\pi} \sin ^{2}t \, dt\\
&= \int _{0}^{2\pi} \varphi'' (t) \sin t + \varphi (t) \sin t + \arctan \varphi(t) \sin t \, dt\\
& = \int _{0}^{2\pi} \arctan \varphi(t) \sin t \, dt\\
& < \frac{\pi}{2} \int _{0}^{2\pi} |\sin t | \, dt\\
& = 2\pi
\end{align}
$$ And this is a contradiction since $\lambda \geq 2$ . What can we say about the $2\pi$ periodic solution when $\lambda <2$ ? I tried to solve this equation using mathematica and when $\lambda \geq 2$ there is no numerical solution while when $\lambda <2$ there is one, so I think that there should be one $2\pi$ periodic solution when $\lambda <2$ ï¼but I don't know how to prove it.","['periodic-functions', 'calculus', 'analysis', 'ordinary-differential-equations']"
4924835,Prove that a group of order 20 has a subgroup of order 5 without using Cauchy's theorem,"The problem: Prove that a group of order 20 has a subgroup of order 5. Background: This problem arises from a discrete mathematics homework assignment. Our course materials cover cosets, Lagrange's theorem, the order of group elements, and cyclic groups but not Cauchy's theorem or Sylow's theorem, so I'm looking for a proof within these limitations. My attempts: By Lagrange, all elements have order 1, 2, 4, 5, 10 or 20. If there is an element $x$ of order 20 then $x^4$ has order 5. if there is an element $x$ of order 10 then $x^2$ has order 5. Hence, we may assume that all elements have order 1, 2 or 4. I'm stuck at going further to reach a contradiction.","['group-theory', 'finite-groups']"
4924847,I need help proving this :$\sum_{n=0}^{\infty}\frac{(2n)!!}{(2n+1)!!}(-1)^nx^{2n+1}=\frac{1}{\sqrt{1+x^2}}\ln(x+\sqrt{1+x^2})$,"I need help proving this : $$\sum_{n=0}^{\infty}\frac{(2n)!!}{(2n+1)!!}(-1)^nx^{2n+1}=\frac{1}{\sqrt{1+x^2}}\ln(x+\sqrt{1+x^2})$$ we have : $$\frac{1}{\sqrt{1+x^2}} = \sum_{n=0}^{\infty}\frac{(2n+1)!!}{(2n)!!}(-1)^nx^{2n}$$ For one hour looking at this series, I couldn't find even a single idea on how to prove this using the various conventional methods I know.",['sequences-and-series']
4924929,Reference for Shooting Method,"Consider the following setup. We have a second order boundary value problem: $$\dfrac{d^2y}{dx^2}=F(x,y,dy/dx);\qquad y(x_0)=y_0,\quad y(x_f)=y_f.$$ A numerical approach is to almost first write as two first order ivps: $$\begin{aligned}
\frac{dy}{dx}&=v,\qquad &y(x_0)=y_0
\\ \frac{dv}{dx}&=F(x,y,v),\qquad &v(x_0)=?
\end{aligned}$$ Suppose we have some numerical method (some Runge-Kutta, mostly interested here in simple RK1, Euler's Method). Now, we can take a guess at $v(x_0)$ : for example we can shoot straight at $y(x_f)$ by using our numerical method to attack a system of IVPs by taking $v(x_0)=v_0$ where: $$v_0=\dfrac{y(x_f)-y(x_0)}{x_f-x_0}.$$ We implement our method and we get, as an approximation to $y(x_f)=y_f$ , we get, say: $$y_f(v_0)\approx y_f.$$ Say the discrepancy is: $$\varepsilon(v_0)=y_f(v_0)-y_f.$$ Now we can plot $\varepsilon$ vs $v_0$ and where $\varepsilon$ is zero we have approximations to the solution that at least satisfy the second boundary condition. When the original ode is suitably linear, it is the case for any two points $(v_a,y_a-y_f)$ and $(v_b,y_b-y_f)$ on this curve, we have $\varepsilon$ is zero at: $$v_a+\frac{y_f-y_a}{y_b-y_a}(v_a-v_b).$$ Because the curve in this case is a line.
This formula can be understood on a more basic level if you start drawing pictures in the $x$ - $y$ plane. Is there a good, online and available reference that discusses this curve? What are the conditions on the ode on it being a straight line? What are the conditions on the ode that there is a unique root?","['ordinary-differential-equations', 'runge-kutta-methods', 'boundary-value-problem', 'numerical-methods', 'eulers-method']"
4924946,Chapter 2 proposition 2.6 Hartshorne Algebraic Geometry,"This may be dumb but I donât really understand what we are saying. Hartshorne claims: Let $V$ be a variety over an algebraically closed field $k$ and let $\mathcal{O}_V$ be its sheaf of regular functions.
We will show that $(t(V),\alpha_*(\mathcal{O}_V))$ is a scheme over $k$ Since any variety can be covered by open affine subvarieties, it will be sufficient to show that if $V$ is affine, then $(t(V),\alpha_*(\mathcal{O}_V))$ is a scheme. Why proving this will prove the first claim we have done?","['algebraic-geometry', 'schemes']"
4924978,Are Hausdorff countably compact topological groups always normal?,"A colleague and I have a result that shows that for Hausdorff countably compact W-spaces , being a topological group implies normality . But it occurred to us that (being not super experienced working with topological groups) we're unsure whether this is known (or disproven) without the W-space assumption. Ï-Base is also uncertain: https://topology.pi-base.org/spaces?q=%24T_2%24%2BCountably+compact%2BHas+a+group+topology%2B%7ENormal So, are Hausdorff countably compact topological groups always normal?","['separation-axioms', 'general-topology', 'topological-groups', 'compactness']"
4924993,Can the center of circumscribed circle in a triangle be on the incircle?,"Besides the obvious answer of an isosceles right triangle, can there be other triangles where the center of its circumscribed circle is located on the perimeter of its incircle?
I tried using the Euler's theorem $d^2 = R^2 â 2Rr$ where $d=r$ By solving it I found that R/r= â2+1 which can also be determined by using cot22.5 in the isosceles right triangle but I couldn't find a way to prove or disprove that such other triangles exist.","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'triangle-centres']"
4924997,How to find the double integral of an absolute value trig function?,"I have this double integral: $$\int_{-\pi/2}^{\pi/2} \int_{-\pi/2}^{\pi/2} |\sin(y-x)|dydx$$ And I'm struggling to solve it. I have reached the answer $4/\pi^2$ however my answer isn't what the textbook says is correct, which is $2\pi$ . Since it's an absolute value I tried this for the new bounds: $$\int_{-\pi/2}^0 \int_0^{\pi/2} (\sin(y-x))dydx - \int_0^{\pi/2} \int_{-\pi/2}^0 (\sin(y-x))dydx$$ I'm not really sure what to do, I'm pretty sure my new bounds are wrong but I'm not sure. Any kind of help would be appreciated!!","['multivariable-calculus', 'definite-integrals', 'absolute-value']"
4925014,Solve an integral equation using functional analysis,"I'm trying to solve the following equation: Is there a continuous function $f: [0,1] \rightarrow \mathbb{R}$ that satisfies $$f(x) + \int_0^x e^{x \cos(t)}f(t) \ dt = x^2 + 1, x \in [0,1]$$ if so, check if it is unique. We were also given the hint to check if -1 is in the spectre of the operator. I tried several approaches; I tried using $x^2+1$ as $f$ , but it didn't work. My next approach was to show that the norm of the operator is less than 1, which would imply that -1 is not in the spectre, but that didn't work either. Could you please give me a hint as to which approach I should use here. Thank you.","['integral-equations', 'functional-analysis']"
4925028,Laplace's Equation on a Pac-Man,"I am struggling way too much with this problem, any help is highly appreciated. Consider the Pac-Man-like set described by $$P=\left\{(\rho\cos\theta,\rho\sin\theta):\rho\in(0,1),\theta\in\left(\frac{\pi}{4},\frac{7\pi}{4}\right)\right\}$$ whose boundary can be described by $\partial P=\Gamma\cup\gamma_+\cup\gamma_-$ with $\Gamma=\{(\cos\theta,\sin\theta):\theta\in\left[\frac{\pi}{4},\frac{7\pi}{4}\right]\}$ and $\gamma_{\pm}=\{(x,\pm x):x\in[0,\frac{1}{\sqrt{2}}]\}$ as shown below ( plotted with desmos ). I want to find a solution of the Laplace equation $\psi_{xx}+\psi_{yy}=0$ for $\psi:\overline{P}\to\mathbb{R}$ with boundary conditions $$\boxed{\begin{cases}\psi\vert_{\Gamma}(\cos\theta,\sin\theta)=(\theta-\frac{\pi}{4})(\theta-\frac{7\pi}{4})\\ \psi\vert_{\gamma_+}(x,x)=\psi\vert_{\gamma_-}(x,-x)=0\end{cases}}$$ This is because I want it to be a non-zero polynomial on $\Gamma$ , zero on $\gamma_{\pm}$ and continuous on $\partial P$ . My question then is: Does such a $\psi:\overline{P}\to\mathbb{R}$ exist? If so, how can I compute it? Now I will show what I've managed to do. Suppose that we could express $\psi$ as a Fourier series like $$\psi(\rho\cos\theta,\rho\sin\theta)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\theta)+b_n\sin(n\theta))$$ Then, we have by the boundary conditions $\psi\vert_{\gamma_{\pm}}=0$ that $$\begin{cases}0=\psi\left(\frac{\rho}{\sqrt{2}},\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)+b_n\sin(n\pi/4))\\ 0=\psi\left(\frac{\rho}{\sqrt{2}},-\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)-b_n\sin(n\pi/4))
\end{cases}$$ which immediately implies $a_n=0$ for $n\not\equiv2(\text{mod }4)$ and $b_n=0$ for $n\not\equiv0(\text{mod }4)$ by adding and subtracting both power series and equating the coeficients to zero. However, this means that $$\psi(\rho\cos\theta,\rho\sin\theta)=a_2\rho^2\cos(2\theta)+b_4\rho^4\sin(4\theta)+a_6\rho^6\cos(6\theta)+...$$ And this is incompatible with the boundary condition for $\Gamma$ since, on $\Gamma$ , $\psi$ isn't $\pi$ -periodic while the right-hand side is. Note that $\rho^{-n}\cos(n\theta),\rho^{-n}\sin(n\theta)$ for $n\geq1$ and $\log\rho$ cannot appear in the polar form infinite series since these are not defined for $(x,y)=(0,0)$ and $(0,0)\in\overline{P}$ . So does this mean that Laplace's equation has no solution on $\overline{P}$ with those boundary conditions? Edit: In this question , user83387 states Suppose we look for solutions to $\Delta u = 0$ on the circular sector $\{ (r, \theta) \mid 0 < r < a, 0 < \theta < \gamma \}$ with initial conditions $u(a, \theta) = g(\theta), \, u(r, 0) = u(r, \gamma) = 0$ . Using separation of variables, I showed that the solution is of the form $$ u(r, \theta) = \sum_{n=1}^\infty \frac{2a^{-\frac{n\pi}{\gamma}}}{\gamma} \left(\int_0^\gamma g(\phi)\sin\left(\frac{n\pi \phi}{\gamma}\right) \, d\phi\right)  \, r^{\frac{n\pi}{\gamma}}\sin\left(\frac{n\pi\theta}{\gamma}\right)$$ This is precisely what I needed shifting $\theta$ by $\pi/4$ radians and considering $a=1,\gamma=3\pi/2$ . However, how did user83387 arrive to that expression?","['fourier-series', 'partial-differential-equations', 'harmonic-functions', 'real-analysis']"
4925093,"If $X$ is a time-homogeneous Markov process, is $\tilde X:=X-\lfloor X\rfloor$ Markov as well?","Assume $(X_t)_{t\ge0}$ is an $\mathbb R^d$ -valued time-homogeneous Markov process with transition semigroup $(\kappa_t)_{t\ge0}$ on a probability space $(\Omega,\mathcal A,\operatorname P_x)$ satisfying $\operatorname P_x[X_0=x]=1$ and let $$\iota:\mathbb R^d\to[0,1)^d\;,\;\;\;x\mapsto x-\lfloor x\rfloor.$$ Is $$\tilde X_t:=\iota(X_t)\;\;\;\text{for }t\ge0$$ then again a (time-homogeneous) Markov process? It's transition semigroup then would most probably be $$\tilde\kappa_t(x,\;\cdot\;):=\kappa_t(x,\;\cdot\;)\circ\iota^{-1}\tag2\;\;\;\text{for }x\in[0,1)^d\text{ and }t\ge0.$$ I actually don't see what should go wrong: If $\mu$ is any probability measure on $\mathbb R^d$ and $X\sim\mu$ , then clearly $\iota(X)\sim\mu\circ\iota^{-1}$ . However, Proposition 1-iv in this paper seems to claim that $\tilde X$ is not Markov anymore ... Is this really true? EDIT I'm actually not sure about the definition of $\tilde k_t$ . It could also be $$\tilde\kappa_t(x,B)=\sum_{k\in\mathbb Z^d}\kappa_t(k+x,k+B)$$ or even $$\tilde\kappa_t(x,B)=\sum_{k,l\in\mathbb Z^d}\kappa_t(k+x,l+B)$$ whereas $(2)$ corresponds to $$\tilde\kappa_t(x,B)=\sum_{k\in\mathbb Z^d}\kappa_t(x,k+B)$$ for $x\in[0,1)^d$ and $B\in\mathcal B([0,1)^d)$ .","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'markov-process', 'probability-theory']"
4925100,Third term in the free Lie ring,"I'm working with the Lie algebra of the free group: $$\mathscr{L}(F_n) = \oplus_{d=1}^{\infty} \mathscr{L}_d(F_n),$$ where $\mathscr{L}_d(F_n) = \gamma_d(F_n) / \gamma_{d+1}(F_n)$ and $\gamma_d(F_n)$ is the $d$ th term in the lower central series. By a theorem of Magnus and Witt, $\mathscr{L}(F_n) \cong L(\mathbb{Z}^n)$ the free Lie ring. This makes the lower degree terms easy to see: $\mathscr{L}_1(F_n) = F_n / [F_n, F_n] \cong \mathbb{Z}^n = L_1(\mathbb{Z}^n)$ $\mathscr{L}_2(F_n) = [F_n, F_n] / [[F_n, F_n], F_n]$ . This is isomorphic to $L_2(\mathbb{Z}^n)$ , which contains all things you get by bracketing degree 1 terms in $L(\mathbb{Z}^n)$ : $$[x,y] = x \otimes y - y \otimes x,$$ so it's easy to see $L_2(\mathbb{Z}^n) \cong \wedge^2 \mathbb{Z}^n$ . We get the isomorphism $\mathscr{L}_2(F_n) \rightarrow \wedge^2 \mathbb{Z}^n$ by sending $[x,y] \mapsto [x] \wedge [y]$ , where $[x]$ is its abelianization. How can we describe $\mathscr{L}_3(F_n) \cong L_3(\mathbb{Z}^n)$ in a similar way? It would be great to have a description as a $\mathbb{Z}$ -vector space with maps from $\mathscr{L}_3(F_n)$ and $L_3(\mathbb{Z}^n)$ into it.","['lie-algebras', 'ring-theory', 'abstract-algebra', 'graded-modules', 'lie-groups']"
4925127,Is this proof correct? (Undergraduate Real Analysis),"Problem. Let $(x_{n})_{n \in \mathbb{N}}$ be a sequence of real numbers and let $x$ be in $\mathbb{R}$ . Prove that if $\lim_{n \to +\infty} x_n = x$ and if $x > 0$ , then there exists a natural number $M$ such that $x_n > 0$ for all $n \geq M$ . Proof: Suppose $\lim_{n \to +\infty} x_n = x$ and $x > 0$ . Then for all $\epsilon > 0$ there exists a natural number $K$ such that $n \geq K$ implies that $|x_n â x| < \epsilon$ . Let a natural number $M$ be given such that $x_n > 0$ for all $n \geq M$ . Notice that if $x_n > 0$ , then $|x_n| > 0$ . But $$|x_n| = |x_n-x+x| \leq |x_n -x|+|x| < \epsilon + |x| = \epsilon + x. $$ So, $|x_n| - x < \epsilon$ . But since $x_n, x  > 0$ , We have that $$ |x_n| - x = |x_n-x| < \epsilon. $$ Sorry the format is so awful, but $x_n$ is a sequence.","['analysis', 'sequences-and-series']"
4925156,"If a graph is 1-factoreable, then it has no cut vertex.","I'm trying to prove the statement: if a graph $G$ is 1-factorable, then $G$ has no cut vertex. Assuming $G$ has a cut vertex, let be $v\in V(G)$ a cut vertex of $G$ . Then the connected components of $G-v$ are greater than the components of $G$ . Since $G$ is 1-factorable, let's call $H_1,H_2,\ldots,H_r$ the 1-factors of $G$ such that $\bigcup\limits_{i=1}^rE(H_i)=E(G)$ .  I'm sure there is something to do with how many vertices we have in $H_1-v$ and $H_2-v$ and such, related to the parity, that leads me to a contradiction. Maybe there's another thing to consider that lead us to an absurd. I'm stuck from the start, so any hint would be appreciated.","['graph-theory', 'matching-theory', 'discrete-mathematics']"
4925160,Understanding roots of unity in quadratic fields,"Suppose we have a quadratic field $\mathbb{Q}(z)$ with $z \in \mathbb{C} \setminus \mathbb{Z}$ and $z^2 \in \mathbb{Z}$ . How would one go about determining the possible $n^{\text{th}}$ roots of unity for such a field? I know that for $G=Aut(K[\zeta]/K)$ , it must hold that $G\cong (\mathbb{Z}/n\mathbb{Z})^{\times}=\{i+n\mathbb{Z}|gcd(i,n)=1\}$ and $|G|| \phi(n)$ (where $\phi$ is Euler's totient function ). I'm a bit lost on how to proceed from here. For example, how do I check if it is possible for $Q(z)$ to contain a fourth root of unity?","['algebraic-number-theory', 'abstract-algebra', 'roots-of-unity']"
4925195,Construct schemes like toric varieties on manifolds,"I'm reading a book about toric varieties, and some thoughts occured to me. Toric varieties are constructed by fans which are the union of rational cones in $\mathbb{R}^n$ . Is it possible to construct a scheme on a smooth manifold $M$ ? I have two ideas. First, given a distribution $D$ on $M$ , consider a tensor field $\mathcal{T}$ on $M$ which consists of contangent vectors whose values on all the vectors in $D$ is non-negative, then glue Spec( $\mathbb{C}[\mathcal{T}_p]$ ) together, $p\in M$ . Second, suppose $M$ is triangulable, consider its simplicial cochain groups $C^{*}_n$ , then glue Spec( $\mathbb{C}[C^{*}_n]$ ) together. Sorry for my probable naive ideas.","['algebraic-geometry', 'schemes', 'toric-varieties']"
4925196,Is this a valid formula for the Wronskian?,"I was messing around with the Wronskian of two functions $y_1(x)$ and $y_2(x)$ , which is defined by: $$
W(y_1,y_2) = \begin{vmatrix}
y_1 & y_2 \\
y_1' & y_2'
\end{vmatrix} = y_1y_2'-y_2y_1^{\prime}
$$ And came across a supposedly new way of calculating the Wronskian, and I would like to know if it is correct: \begin{align}
\mbox{We know that}\ \frac{d}{dx}\left(\frac{y_2}{y_1}\right) & = \frac{y_1y_2'-y_2y_1'}{y_1^2}
\\[3mm] \mbox{So then:}\
\frac{W(y_1,y_2)}{y_1^2} & = \frac{d}{dx}\left(\frac{y_2}{y_1}\right)
\\[3mm]
\mbox{Which gives us:}\
W(y_1,y_2) & = y_1^2\left(\frac{d}{dx}\left(\frac{y_2}{y_1}\right)\right)
\end{align} Will this always be true, assuming $y_1 \neq 0$ ?.","['calculus', 'derivatives', 'wronskian']"
4925233,Show $ I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x $ exists,"I'm trying to solve this problem from a past Stanford qual, but I'm a bit stuck. (EDIT: I have now solved the problem. See the solution below.) Show that for each $k \in \mathbb{R} \backslash\{0\}$ the limit $$
I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x
$$ exists, and find all $m \in \mathbb{R}$ such that $\lim _{k \rightarrow 0}|k|^m I(k)=0$ . This looks like the inverse Fourier transform of $\frac{1}{1+|x|}$ , but I'm not sure what to do with this fact. If I expand $e^{ikx}$ , I get $$\lim_{R\to \infty}\int_{-R}^R\frac{e^{ikx}}{1 + |x|}dx =\lim_{R\to\infty}\left[\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx + i\int_{-R}^R\frac{\sin(kx)}{1 + |x|}dx \right]$$ The imaginary part is zero because $\frac{1}{1+|x|}$ is even and $\sin(kx)$ is odd over a symmetric domain. Therefore we just have to worry about $$\lim_{R\to\infty}\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx=\lim_{R\to\infty}2\int_0^R\frac{\cos(kx)}{1+x}dx$$ now integrating by parts, we may set $u=\frac{1}{1+x}$ so that $du=\frac{-1}{(1+x)^2}dx$ and $v=\frac{1}{k}\sin(kx)$ and $dv=\sin(kx)dx$ . Usng integration by parts $$2\int_0^R \frac{\cos(kx)}{1+x}dx = \left.\frac{2}{k}\frac{\sin(kx)}{1+x}\right|_0^R-2\int_0^R \frac{-1}{k(1+x)^2}\sin(kx)dx.$$ Now $$\left|\int_0^R \frac{\sin(kx)}{(1+x)^2}dx\right|\leq \int_0^R\frac{1}{(1+x)^2}dx=\left.\frac{-1}{1+x}\right|_0^R= \frac{-1}{1+R} + 1$$ Therefore, we get that the integral converges absolutely, so the limit exists. Now using the bound $-\frac{2}{|k|}\leq I(k)\leq \frac{2}{\left|k\right|}$ we get that $-2|k|^{m-1}\leq |k|^mI(k)\leq 2|k|^{m-1}$ so $
\lim _{k \rightarrow 0}|k|^m I(k)=0
$ if $m> 1$ by the squeeze theorem. Now when $m < 1$ , then since $\sin(-x)=-\sin(x)$ , we get $$\lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0}\frac{2|k|^{m}}{k} \int_0^R \frac{\sin(kx)}{1 + |x|}dx=\lim_{k\to 0}2|k|^{m-1} \int_0^R \frac{\sin(kx)}{1 + |x|}dx = \infty$$ and when $m=1$ , we have $$\lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0} \int_0^R \frac{\sin(kx)}{1 + |x|}dx\neq 0.$$","['complex-analysis', 'calculus', 'analysis', 'real-analysis']"
4925236,Subdifferential set of total variation norm,"I have been trying to understand the Gateaux differential (if it does exist) of the total variation norm ( $\|\cdot\|_\text{TV}$ ) over the space of measure $M(\textit{X}, \mathbb{R}^k)$ where $\textit{X}$ is a compact subset of some $\mathbb{R}^n$ space. Does it exist under some topology over the space? The closet I could come up was this result on the torus $\mathbb{T} = \mathbb{R}/\mathbb{Z}$ : $\textbf{Proposition (Subdifferential of the total variation)}$ Let us endow $M(\mathbb{T})$ with the weak-* topology and $C(\mathbb{T})$ with the weak topology. Then, for any $m \in M(\mathbb{T})$ , we have: $$
\partial \|m\|_\text{TV} = \left\{ \eta \in C(\mathbb{T}) \, ; \, \|\eta\|_{\infty} \leq 1 \text{ and } \int \eta \, dm = \|m\|_\text{TV} \right\}.$$ where $C(\mathbb{T})$ is the space of continuous functions that vanish at the infinity. I'm unable to interpret that set. Is there an alternate way to write this? Does it have certain properties that could be used to distinguish the sets corresponding to two different measures $m$ and $m'$ ?","['measure-theory', 'calculus-of-variations', 'gateaux-derivative', 'calculus', 'general-topology']"
4925247,rational complex minmax,"As the title says, I wanted to understand the following part of a proof in Guttel (Corollary 2, page 6). $f$ is an analytic function and $r_m \in \mathcal{P}_{m-1}/q_{m-1}$ is a rational function with coefficients in $\mathbb{C}$ where the numerator are the polynomial of degree at most $m-1$ while the denominator is a fixed polynomial degree $m-1$ whose poles are non contained in a compact set $K$ .
I wanted to understand whether $$||f - r_m||_{\infty, K}$$ has a minimum, i.e a rational complex minmax approximation or other condition are needed. Surely it's bounded since I can take $q_{m-1} \in \mathcal{P}_{m-1}/q_{m-1}$ giving $||f - 1||_{\infty, K} < \infty$ Any solution, counterexample or reference is appreciated.","['approximation', 'normed-spaces', 'analysis', 'numerical-methods', 'rational-functions']"
4925334,"Evaluate the definite trigonometric integral $\int_{0}^{2\pi} \frac{\sin^2(x)}{1+b\cos(x)}, $ $b \in (0,1)$","$$\int_{0}^{2\pi} \frac{\sin^2(x)}{1+b\cos(x)}, \qquad b \in (0,1)$$ Firstly, I defined $$\tilde{f}(z)=z^{-1}\frac{(\frac{z-z^{-1}}{2i})^2}{1+b\frac{z+z^{-1}}{2}}=\frac{2z^{-1}}{-4}\frac{(z-z^{-1})^2}{2+bz+bz^{-1}}=-\frac{z^{-1}}{2}\frac{z^2-2+z^{-2}}{2+bz+bz^{-1}}; \quad A=\left\{0,-\frac{1}{b} \pm \frac{\sqrt{1-b^2}}{b}\right\} .$$ We have $A \cap D(0,1)= \left\{\{0,-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\}\right\}$ , and I obtained: \begin{align}
\int_{0}^{2\pi} \frac{\sin^2(x)}{1+b\cos(x)}
&=2\pi (\operatorname{res}(0,\tilde{f})+\operatorname{res}\left(-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b},\tilde{f}\right) \\
&=2\pi\left(\frac{1}{b^2}-\frac{1}{2}\frac{\left(\left(-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\right)^2-2+\left((-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\right)^{-2}\right)}{2b\left(-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\right)+2}\right) \\
&=2\pi\left(\frac{1}{b^2}-\frac{1}{2}\frac{\left(\left(-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\right)^2-2+\left(-\frac{1}{b}+\frac{\sqrt{1-b^2}}{b}\right)^{-2}\right)}{2\sqrt{1-b^2}}\right)
\end{align} But it seems that I'm overcomplicating this calculation; any suggestions?","['integration', 'definite-integrals', 'complex-analysis', 'calculus', 'trigonometric-integrals']"
4925344,Residue of $f(z)=\frac{e^{\frac{1}{z}}}{z-z^2}$ at $z=0$,"I am trying to calculate the residue of $ f(z)=\frac{e^{\frac{1}{z}}}{z-z^2}$ at $z=0$ by noticing that $$\frac{e^{\frac{1}{z}}}{z}\frac{1}{1-z}=\left(\frac{1+\frac{1}{z}+\frac{(\frac{1}{z})^2}{2!}+ \dots}{z}\right)\left(1+z+z^2 + \dots\right)$$ so that then $$\operatorname{Res}(0,f)=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+ \dots=e.$$ I attempted verifying the result with WolframAlpha , but it didn't give me any output.","['complex-analysis', 'residue-calculus']"
4925354,Are these stalks isomorphic?,"If $X$ is a scheme with structure sheaf $\mathcal{O}_X$ and $h: X \to \text{Spec }\mathcal{O}_X(X)$ is the unique morphism of schemes such that $h^\sharp(\text{Spec } \mathcal{O}_X(X)): \mathcal{O}_X(X) \to \mathcal{O}_X(X)$ is the identity (this has to do with the universal property of ring spectra). Is it then true that $$\mathcal{O}_{\text{Spec }\mathcal{O}_X(X),h(x)} \cong \mathcal{O}_{X,x}?$$ In particular, I can tell you that here, $h(x)$ is the inverse image of the maximal ideal $\mathfrak{m}_{X,x} \subseteq \mathcal{O}_{X,x}$ under the morphism $\mathcal{O}_X(X) \to \mathcal{O}_{X,x}: f \mapsto f_x$ (which is surjective, so that $h(x)$ is in particular a maximal ideal). Is the isomorphism above true? Any help is appreciated!","['algebraic-geometry', 'schemes']"
4925363,Average value of a function $f(\alpha)= \frac {\cos\beta}{1-\sin^2\alpha \cdot \sin^2\beta}$,"Good evening, I have the following function: $$
\operatorname{f}\left(\alpha\right) =
\frac {\cos\left(\beta\right)}{1 - \sin^{2}\left(\alpha\right)\sin^{2}\left(\beta\right)}
\quad\mbox{where}\ \beta \in \mathbb{R}
$$ I would like to demonstrate that, for each value of $\beta$ , the average value of the function in the interval $\left[0,2\pi\right]$ is $1$ . $$
\mbox{So it should just be:}\quad
m_{\left[0,2\pi\right]}=1=
{1 \over 2\pi}\int_{0}^{2\pi}\frac{\cos\left(\beta\right)}{1 - \sin^{2}\left(\alpha\right)\sin^{2}\left(\beta\right)} \,{\rm d}\alpha
$$ Unfortunately I don't know how to solve the denominator integral, do you have any suggestions on this ?. Thank you.","['integration', 'definite-integrals']"
4925366,"Approximation of an irrational number by sequence of rational numbers, but with assumption on denominators.","Let $\lbrace a_{n}\rbrace $ be a sequence of natural numbers (strongly increases) and $r\in (0,1)$ be an irrational number. Is it possible to find a sequence $\lbrace b_{n}\rbrace$ such that $\mathrm{gcd}(a_{n},b_{n})=1$ and $\frac{b_{n}}{a_{n}} \to r$ ? For example i know that for each $a_{n}$ i can consider a Farey sequence $F_{a_{n}}$ and pick the element $\frac{b_{n}}{a_{n}} \in F_{a_{n}}$ in such a way that it is the largest number of this form in $F_{a_{n}}$ less than $r$ . How can i prove that $|\frac{b_{n}}{a_{n}}-r|\to 0$ ?","['number-theory', 'diophantine-approximation', 'elementary-number-theory']"
4925375,How many independent random variables can fit in a probability space with equally likely outcomes?,"Suppose we have a finite probability space $S,|S|=n$ with the probability defined as $P(A)=|A|/n$ . There is a set of random variables $V =\{A_1,A_2,\cdots\}$ defined over $S$ , mutually independent. what is the maximum of $|V|$ ? For example, if $|S|=1$ , then $V$ has only the constant. [EDIT: The constant are independent of each other, so we should omit this trivial case.] If $|S|=2$ , then $V$ contains the constant and the variable where the two values are different. What about the $S$ with more elements? My guess is that it's the number of factors of $n$ .","['combinatorics', 'probability-theory', 'probability', 'random-variables']"
4925425,Find the radius $r$ in the circle below,"In the figure, $OP = 1$ and $PF = 3$ . Calculate $r$ . $DC$ and $DE$ are tangents. (Answer: $r=2$ ) Could this issue be resolved only with this data? I try: $Point Theorem: FA.FB = FC.FE \implies (3-(r-1).(4+r) = FC.(FC+FE)$ $DC=DE$ $\triangle AEB: AE^2+EB^2  =(2r)^2 = 4r^2$","['euclidean-geometry', 'geometry', 'plane-geometry']"
4925466,"Given a function, find a Stochastic Differential Equation that the function satisfies","Suppose we have an SDE: $$dX = A X dW_1 + B X dW_2, \tag{1}$$ where $A$ and $B$ are constant non-commuting matrices acting on the Hilbert space $\mathbb{C}^2$ , and $W_1(t), W_2(t)$ are independent 1-dimensional Wiener processes. Also, assume the initial condition is a constant vector $X_0 \in \mathbb{C}^2$ . Since $A$ and $B$ do not commute, there is no closed-form solution to (1). For example, the  solution below is valid only if $A$ and $B$ commute: $$Z^{(1)}(t)=\exp\left(-\frac{1}{2} \left(A^2 + B^2 \right)t + A W_1(t) + B W_2(t) \right)X_0. \tag{2}$$ However, $Z^{(1)}(t)$ is an approximate solution to (1), which can be obtained by the first-order Magnus expansion. To get a better approximation, we must introduce commutators of $A$ and $B$ . This yields a second-order Magnus expansion: \begin{align}
Z^{(2)}(t) = \exp \left \{ -\frac{1}{2} \left(A^2 + B^2 \right)t \\
+A W_1(t) + B W_2(t) + \frac{1}{2}[A,B] \int_0^t W_2(s) dW_1(s) + \frac{1}{2}[B,A] \int_0^t W_1(s) dW_2(s) \right \}X_0. \tag{3}
\end{align} My inquiry: I would like to find an approximate SDE that $Z^{(1)}(t)$ satisfies. From the argument above, we know that (1) is an approximate SDE to (2). However, the SDE in (1) is a pretty crude approximation; we could call it a ""first-order"" approximate SDE. I'm interested in the ""second-order"" approximate SDE. I tried applying Ito's lemma to (2), but the problem is that I don't know how to calculate partial derivatives ( i.e. $\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2},\frac{\partial}{\partial t}$ ) of $\exp(-\frac{1}{2}(A^2 + B^2)t + A x_1 + B x_2 )$ . I looked at exponential map differentiation , but it does not yield the desired result because after differentiating, the exponential map $\exp(\cdot)$ is on the left-hand side of the $\text{ad}_{\chi}$ operator.","['stochastic-processes', 'stochastic-differential-equations', 'stochastic-calculus', 'ordinary-differential-equations']"
4925506,Are all continuous functions that send conics to conics in $\mathbb{R}^2$ a generalized linear fractional transform?,"Motivation: Consider an arbitrary conic section in $\mathbb{R}^2$ given by $$ Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0 $$ Now consider the map $$\phi: \mathbb{R}^2 \rightarrow \mathbb{R}^2, \phi_{\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} }(x,y) = \left( \frac{ax + by + c}{dx+ey+f}, \frac{gx+hy+i}{dx+ey+f} \right)  $$ This map sends a conic section to a conic section. This is easy enough to verify with algebra by observing that if for some (x,y): $$ Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0 $$ Then there exists an $x'$ and $y'$ s.t. (if $\phi$ is invertible) $$ A \phi(x')^2 + B\phi(x')\phi(y') + C\phi(y')^2 + D\phi(x') + E\phi(y') + F  = 0$$ And therefore $$ A  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right)^2 + B  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right) + C \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)^2 + D \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) + E  \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)  + F  =  0 $$ But by multiplying out all the denominators and grouping like powers of $(x')^n(y')^m$ we can clearly see that $(x',y')$ itself lies on some conic section. So then (it feels clear to me that) we can conclude $\phi$ sends a conic section to a section. The question: Now is this set of $\phi$ the ONLY continuous functions $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ that send conics to conics? Or do there exist some more exotic functions with this property? It does seem this family of functions is well known as the set of homographies which I found out about via this question .","['projective-geometry', 'real-analysis', 'complex-analysis', 'multivariable-calculus', 'algebraic-geometry']"
4925527,Can power of three have a lot of leading 9 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 28 days ago . Improve this question I came up with this problem, but was unable to either prove or disprove the statement.
Let $dig(n)$ be the number of digits in the decimal representation of $n$ . Is it true that $dig(3^n+2^n)=dig(3^n)$ when $n \geq 3$ is a natural number? Of course, $\frac{dig(3^n)}{dig(2^n)}=\frac{ln(3)}{ln(2)}$ approximately, which is around $1.5$ . So in order that to happen $3^n$ should have a lot leading nines, namely around $1-\frac{ln(2)}{ln(3)}$ percent of all digits, which is around $37$ percent. One possible approach is to consider log base 10 of $3^n$ , but it didn't really lead me to anything vital. To be precize, in order for $3^n$ to start with $k$ nines, $nlog_{10} 3$ mod $1$ should be in the range $[log_{10} (10^k-1)$ mod $1,1)$ . The length of it is $l=log_{10}(\frac{10^k}{10^k-1})$ , then $\frac{1}{l}=log_{1+\frac{1}{10^k-1}}(10)$ , which is approximately $ln(10)10^k$ . Then by Diophantine's theorem there exists an $n \leq ln(10)10^k$ such that $3^n$ starts with $k$ nines. But in this estimation $k$ is asymptoticly $log_{10}(n)$ , however we need a constant multiplied by $n$ . Unfortunately, that was the only idea that popped into my mind.",['number-theory']
4925575,Spinner probability question with repetition (HS),"I found this problem in a 10th grade math text book in a section about using permutations to solve probability problems. The book presents the answer as $$ \frac{1}{110880} $$ Which is 1 over the number of ways to arrange the 12 colors accounting for repetition. This seems undoubtedly incorrect to me. I suggested to a colleague that the solution is simply: $$ \frac{3}{12} \times \frac{5}{12} \times \frac{3}{12} \times \frac{1}{12} \approx 0.002$$ They suggested that that this doesn't account for order because you could rearrange them and get the same answer. I'm still not convinced that this matters. Similarly to how fundamental counting principle would result in the same answer no matter the order you arranged the numbers. Either way I'm curious if there is a way to solve this using permutations. Doesn't seem like it to me, I don't ever recall learning how to account for repetitions when you're restricted, i.e. spinning four times in this case, instead of twelve times. And the book answer is definitely wrong right? Thanks in advance for everyone's help.","['permutations', 'solution-verification', 'problem-solving', 'probability']"
4925583,Let A be an infinite set and E the set of equivalence relations on A. Prove that E is equinumerous to the power set of A.,"So far, I understand that I have to create a bijection between E and P(A), but I haven't been able to find such. One idea I had is to take the set P of equivalence classes of A and show that it is equinumerous to E and to P(A), and by transitivity of the relation equinumerous, E would be equinumerous to P(A), but I'm sure there's an easier way. Any help is appreciated!",['discrete-mathematics']
4925591,How to read this statement? $f: \mathbb{Z}\to\mathbb{R}: f(x)= ( x^2+1)/2$,"I am interested in how we can read this statement regarding a function .
Is it just as it is, or maybe $(x^2 + 1)$ is a divisor for $2$ ?
Or is it just a bracket divided by $2$ , meaning we just have to plug in the values? Straight from the task word file: Can You define it? $f: \mathbb{Z} \to  \mathbb{R}$ by $ f(x)= ( x^2+1)/2$ find $f( A)$ where $A = \{ 3,5,6\}$ and $f^{-1}(B)$ where $B = \{ 1, 5/8, 0\}$","['functions', 'discrete-mathematics']"
4925631,Total variation of integral function,"Consider a function $f\in L^1([a,b])$ and define $F(x):=\int_a^x f(y)dy$ . I should prove that $V_a^bF=||f||_{L^1([a,b])}$ . My attempt was to proceed by approximation with a test function $\phi$ , but Iâm not getting it.
Can someone please help me, please?","['measure-theory', 'total-variation', 'analysis', 'real-analysis']"
4925684,How to solve a spring-mass differential equation with impulse at $t=\pi$ without Laplace transform?,"Consider the differential equation $$x''+x=F_0\delta(t-\pi)$$ with $x(0)=0$ and $x'(0)=1$ . I can solve this using Laplace transform. The result I find is $$x(t)=\begin{cases}\sin{(t)}\ \ \ \ \ &t\leq \pi \\ \sin{(t)}(1-F_0)\ \ \ \ \ &t\geq \pi\end{cases}$$ My question is about how to solve this differential equation without Laplace transform. Let's think about the system as an undamped spring-mass system. The input to the system is an impulse of magnitude $F_0$ at time $\pi$ . For $t<\pi$ we have a simple homogeneous equation and the solution is $$x(t)=\sin{(t)}$$ Note that $x(\pi)=0$ and $x'(\pi)=-1$ . At $t=\pi$ the impulse changes the velocity to $F_0$ . To solve the system for $t\geq\pi$ I once again solved $$x''+x=0$$ with the initial conditions $$x(\pi)=0$$ $$x'(\pi)=F_0$$ Since the solution is $$x(t)=c_1\cos{(t)}+c_2\sin{(t)}$$ and $$x'(t)=-c_1\sin{(t)}+c_2\cos{(t)}$$ the constants come out to $c_1=0$ and $c_2=-F_0$ . The solution is thus $$x(t)=-F_0\sin{(t)}$$ Why do I have to, apparently, add in the solution obtained for $t<\pi$ ? I think the mistake I am making is related to the initial conditions when I solved for $t>\pi$ but I am not sure.",['ordinary-differential-equations']
4925689,"Understanding Proof: Simple Functions In $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ Form A Dense Subspace of $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$","I have difficulties understanding the proof of the following proposition: Proposition $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $p$ satisfy $1\leq p$ . Then the simple functions in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ form a dense subspace of $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ The book defines $\mathscr{L}^{\infty}$ as follows: Definition $\quad$ Let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ be the set of all bounded real-valued $\mathscr{A}$ -measurable functions on $X$ . Define $\|\cdot\|_{\infty}$ by letting $\|f\|_{\infty}$ be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)|>M\}$ is locally $\mu$ -null. We only consider real-valued functions. Here is the proof: Proof $\quad$ Let $f$ belong to $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ , and let $\epsilon$ be a positive number. Choose real numbers $a_0,a_1,\dots,a_n$ such that \begin{align}
    a_0<a_1<\dots<a_n
\end{align} and such that the intervals $(a_{i-1},a_i]$ cover the interval $[-\|f\|_{\infty},\|f\|_{\infty}]$ and have lengths at most $\epsilon$ . Let $A_i=f^{-1}((a_{i-1},a_i])$ for $i=1,\dots,n$ , and let $f_{\epsilon}=\sum_{i=1}^na_i\chi_{A_i}$ . Then $f_{\epsilon}$ is a simple $\mathscr{A}$ -measurable function that satisfies $\|f-f_{\epsilon}\|_{\infty}<\epsilon$ . Since $f$ and $\epsilon$ are arbitrary, the proof is complete. So my understanding of the idea of this proof is the following: For each $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ and each $\epsilon>0$ , we want to construct a simple function $f_{\epsilon}$ in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ so that $$
\|f-f_{\epsilon}\|_{\infty} = \inf\Big\{M\in\mathbb{R}_+:\{x: X:|f(x) - f_{\epsilon}(x)|>M\}\ \text{is locally $\mu$-null}\Big\}<\epsilon.
$$ So, if we can construct $f_{\epsilon}$ such that $\{x\in X:|f(x)-f_{\epsilon}(x)|>\epsilon\}$ is locally $\mu$ -null, then we are done. I can see that in the proof, if $x\in A_i$ for some $i$ , then $f(x)\in(a_{i-1},a_i]$ and $f_{\epsilon}(x)=a_i$ , and so $|f(x)-f_{\epsilon}(x)|<\epsilon$ . But I don't understand why we need to let the intervals $(a_{i-1},a_i]$ cover the interval $[-\|f\|_{\infty},\|f\|_{\infty}]$ . I don't think I understand how $f_{\epsilon}$ is constructed either. Could someone please help me out? Thanks a lot in advance! Reference: $\quad$ Proposition 3.4.2 from Measure Theory by Donald Cohn.","['measure-theory', 'proof-explanation', 'analysis', 'real-analysis', 'dense-subspaces']"
4925701,Does the Borsuk-Ulam theorem extend to non-antipodal points?,"Is the following generalization of the Borsuk-Ulam theorem true? For any continuous map $f: S^2 \rightarrow {\mathbb R}^2$ and $d \in {\mathbb R}$ with $0 < d \leq 2$ , there exist points $x, y \in S^2$ a distance $d$ apart such that $f(x) = f(y)$ . This becomes the Borsuk-Ulam theorem when $d=2$ , the diameter of the unit sphere, because then $x$ and $y$ are antipodal. I suspect it is false, but I couldn't think of a counterexample. For example, its true for the orthographic projection of $S^2$ down onto the plane because points $d$ away from each other on opposite sides of the equator get mapped to the same point. I haven't had success finding an answer online. This came about when I was talking with someone about the corollary of the Borsuk-Ulam theorem that there are always two antipodal points on Earth with the same temperature and pressure (assuming the Earth is a sphere and temperature and pressure vary continuously). When pressed, I realized I couldn't explain what makes antipodal points special other than that it makes the proof work (I was looking at the proof in Hatcher's Algebraic Topology , pg. 33). But it's not clear to me a priori that this generalization couldn't be true by some modified argument. So the question becomes are there always points on Earth a distance $d$ from each other that have the same temperature and pressure?","['general-topology', 'algebraic-topology']"
4925796,About a function bounded by two polynomials,"Let $P(x)=x^n+\Sigma_{k=0}^{n-1}a_kx^k $ and $Q(x)=x^n+\Sigma_{k=0}^{n-1}b_kx^k $ be polynomials with real coefficients such that $n\ge 4$ is even and $a_{n-1}<b_{n-1} $ . Let $f(x)$ be a function such that $P(x)\le f(x) \le Q(x) \forall x\in \mathbb{R} $ .Then we conclude that (a) $f(x)$ is a bounded function on $\mathbb{R} $ (b) $f(x)$ is a continuous function on $\mathbb{R} $ (c) There exist $x_o \in \mathbb{R} $ such that $f(x_o)=0$ (d) $f(x)$ is continuous at least at one point $x_o \in\mathbb{R} $ My attempt :- $f(x) \ge P(x)$ , and even degree polynomials with positive leading coefficient are unbounded so is $f(x)$ Given condition says $P(x)\neq Q(x)$ for at least one $c \in \mathbb{R} $ We define $$
f(x)= \begin{cases} P(x) & \text{if } x\in \mathbb{Q} , \\ Q(x) & \text{if } x\in \mathbb{R}\setminus \mathbb{Q} . \end{cases}
$$ Assuming $ c\in \mathbb{Q} $ it can be shown $f(x) $ is not continous at $c$ by sequential criterion .So $f(x)$ is not continuous. Now $Q(x)-P(x)=(b_{n-1}-a_{n-1})x^{n-1} + \Sigma_{k=0} ^{n-2} (b_k -a_k) x^k$ An odd degree polynomial with leading coefficient positive . So $ \exists m\in \mathbb{R} $ s.t $Q(x)-P(x)\le 0 , \forall x<m $ But given $P(x)\le Q(x) \forall x $ , so we must have $P(x)=f(x)=Q(x) \forall x <m$ So $f(x)$ is continuous at least at one point . I don't really know if I am proceeding right from above. Kindly guide .Thanks in advance !","['roots', 'real-analysis', 'continuity', 'polynomials', 'limits']"
4925818,Hadamard product: derivatives,"How to compute the partial derivatives of the following expression $$
\phi(x,y)=Ax\circ By,
$$ with the Hadamard product $\circ$ , where $A(n-2,n)$ , $B(n-2,n)$ are the matrices $$
A=\left[\begin{array}{ccccccc}
-0.5 & 0 & 0.5 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & -0.5 & 0 & 0.5
\end{array}\right],\quad B=\left[\begin{array}{ccccccc}
1 & -2 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & -2 & 1
\end{array}\right],
$$ and $x(n,1)$ , $y(n,1)$ are the column vectors. Since $$
Ax\circ By=By\circ Ax,
$$ the expression can be rewritten to the matrix form $$
\phi(x,y)=diag(By)Ax=diag(Ax)By,
$$ with the partial derivatives \begin{align*}
\frac{\partial\phi(x,y)}{\partial x} & =diag(By)A,\\
\frac{\partial\phi(x,y)}{\partial y} & =diag(Ax)B.
\end{align*} I am not sure, whether the solution is correct. Thanks for your help. Updated computation: Consider $$
\phi_{i}(x,y)=0.5(-x_{i}+x_{i+2})(y_{i}-2y_{i+1}+y_{i+2}),
$$ the partial derivatives have the form \begin{align*}
\frac{\partial\phi_{i}(x,y)}{\partial x_{j}} & =-0.5(y_{j}-2y_{j+1}+y_{j+2}).\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{j+1}} & =0,\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{j+2}} & =0.5(y_{j}-2y_{j+1}+y_{j+2}),\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\
\cdots & \cdots
\end{align*} and represent elements of $A$ . Hence, the above-mentioned partial derivatives should be OK...","['calculus', 'hadamard-product', 'derivatives']"
4925850,What is the transition semigroup of toroidally wrapped Brownian motion?,"More generally, let $(B_t)_{t\ge0}$ be an $\mathbb R^d$ -valued LÃ©vy process and $W:=\iota(B)$ , where $$\iota:\mathbb R^d\to[0,1)^d\;,\;\;\;x\mapsto x-\lfloor x\rfloor$$ (the floor function is applied componentwisely). I was able to show $^1$ that $W$ is again a time-homogeneous Markov process. However, I fail to identify its transition semigroup. Let $(\kappa_t)_{t\ge0}$ denote the transition semigroup of $B$ . In the present setting, that means $$(\kappa_tf)(x)=\operatorname E\left[f(x+B_t)\right].$$ Now let $\mu_t:=\mathcal L(B_t)$ denote the distribution of $B_t$ . We should clearly have that the distribution of $W_t$ is $$\mathcal L(W_t)=\mu_t\circ\iota^{-1}\tag5.$$ For that reason, my candidate for the transition semigroup of $W$ was $$\tilde\kappa_t(x,\;\cdot\;):=\kappa_t(x,\;\cdot\;)\circ\iota^{-1}\;\;\;\text{for }x\in[0,1)^d.$$ But this seems to be wrong (most probably due to the way I treated $x$ ). In fact, if $f,g:[0,1)^d\to\mathbb R$ are bounded and Borel measurable, I get \begin{equation}\begin{split}\operatorname E\left[f(W_s)g(W_{s+t})\right]&=\int\mu_s({\rm d}x)(f\circ\iota)(x)(\kappa_t(g\circ\iota))(x)\\&=\int\mu_s({\rm d}x)(f\circ\iota)(x)(\tilde\kappa_tg)(x),\end{split}\tag6\end{equation} where I've used that $(B_s,B_{s+t})\sim\mu_s\otimes\kappa_t$ . This is still not the form we need. So, how can we proceed and how is the correct definition for the transition semigroup of $W$ ? $^1$ $B_{s+t}$ and $\mathcal F^B_s:=\sigma(B_r,0\le r\le s)$ are conditonally independent given $B_s$ . Since $W_r$ is $\sigma(B_r)$ -measurable and consequentially $\sigma(W_r)\subseteq\sigma(B_r)$ for all $r\ge0$ , we conclude that $W_{s+t}$ and $\mathcal F^W_s$ are conditionally independent given $B_s$ . That is, $$\operatorname E\left[f(W_{s+t})\mid\mathcal F^W_s\right]=\operatorname E\left[f(W_{s+t})\mid B_s\right]\tag1.$$ Note that we can write $$\operatorname E\left[f(W_{s+t})\mid B_s\right]=\operatorname E\left[f(\iota(W_s+B_{s+t}-B_s)\mid B_s\right].\tag2$$ So far, this remains true if $X$ would just be any time-homogeneous Markov process. Now we use that $B_{s+t}-B_s$ and $B_s$ are independent to conclude $$\operatorname E\left[f(W_{s+t})\mid B_s\right]=\left.\operatorname E\left[(f\circ\iota)(x+B_{s+t}-B_s)\right]\right|_{x\;=\;W_s}\tag3$$ to conclude $$\operatorname E\left[f(W_{s+t})\mid B_s\right]=\operatorname E\left[f(W_{s+t})\mid W_s\right]\tag4.$$","['levy-processes', 'stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4925886,Question about continuous functional calculus and its application,"I recently started learning about the topic functional calculus. My problem is that I have no idea on how to use it for, say, solving problems, exercises etc. Here is a short review of what I learned so far. The idea behind functional calculus seems to be that one would like to ""apply"" a function $f$ to an operator $T$ .
If for example $f: \mathbb{R} \rightarrow \mathbb{R}$ and $T: H_1 \rightarrow H_2$ , the term $f(T)$ does not make any sense, since the domain of $f$ is $\mathbb{R}$ . But one could still make sense of the term $f(T)$ . For example, if we consider a matrix $M \in M_{n \times m }$ and $f$ to be a polynomial, for example $f(x)=3x^3-x^2$ . Then $f(M)$ could be viewed as $3M^3-M^2$ , which are defined for matrices, so everything is fine. Let $H$ denote a complex Hilbert space. And $L(H)$ denote the set of bounded and linear operators on $H$ . Then the range of $T$ was introduced as $R(T):=\{\langle Tx,x\rangle: \lVert x\rVert =1\}$ .
It is mentioned that $R(T)$ is bounded, thus $R(T)$ is compact. Then it is shown that for $T \in L(H)$ , $\sigma(T) \subset \overline{R(T)}$ . (Where $\sigma(T)$ denotes the spectrum of $T$ .) If T is self-adjoint, then $\sigma(T) \subset [m(T),M(T)]$ , where $m(T):=\inf\{\langle Tx,x\rangle :\lVert x\rVert=1\}$ and $M(T):=\sup\{\langle Tx,x\rangle :\lVert x\rVert=1\}$ . After those technicalities, it is mentioned that one wants to define $f(T)$ for $f \in C(\sigma(T))$ . Let $f$ be a polynomial with complex coefficients, i.e. $f(t):=\sum_{k=0}^nc_kt^k$ . Then $f(T)$ means the operator $\sum_{k=0}^nc_kT^k$ . One crucial thing seems to be that polynomials are dense in the set of continuous functions. Let $t$ denote the identity function and $1$ denote the constant function $t \mapsto 1$ . Theorem (Continuous functional calculus)
Let T \in L(H) be self-adjoint. Then there exists exactly one map $\Phi: C(\sigma(T)) \rightarrow L(H)$ such that $\Phi(t)=T, \Phi(1)=Id$ $\Phi$ is linear, $\Phi(fg)= \phi(f)\phi(g)$ and $\Phi( \overline{f})=\Phi(f)^*$ $\Phi$ is continuous We call $\Phi$ the continuous functional calculus of $T$ .
We will write $f(T):=\Phi(f)$ for $f\in C(\sigma(T))$ . As mentioned at the beginning, I have no idea on how to approach a problem using (continuous) functional calculus.
For example, I tried to find some exercise that should be solvable by using functional calculi, but I do not know how to approach those neither how to use functional calculus. Here are some of the Problems I found: Let $T \in L(H)$ be self-adjoint and $\lambda \in \mathbb{C} \setminus(\sigma(T))$ . Then $d(\lambda, \sigma(T))=\lVert (T-\lambda Id)^{-1}\rVert^{-1}$ Let $T \in L(H)$ be self-adjoint and f \in C(\sigma(T)). Show that the following are equivalent:
(i) $f(T)$ is a positive operator
(ii) $f \geq 0$ If $T \in L(H)$ is a self-adjoint Operator, then there exist two positive operators $T_1,T_2 \in L(H)$ such that $T=T_1-T_2$ (Are $T_1$ and $T_2$ unique?) I assume that I am missing some crucial idea or point, and that's the reason why I have no clue in how to approach the problems above. Small Edit:
My guess in approaching problems by using functional calculus is to define the function $\Phi$ (that is literally called continuous functional calculus of $T$ ) and then to try to use some denseness argument to get to the result. But how does one ""find"" $\Phi$ . In the Theorem, it is stated that there exists (exactly one) $\Phi$ , but not how to find it.","['c-star-algebras', 'functional-analysis', 'functional-calculus', 'operator-algebras']"
