question_id,title,body,tags
4187743,Show that $\int_a^b \sin\left(x+\frac{1}{x}\right) dx <3.$,"Show that $$\int_a^b \sin\left(x+\frac{1}{x}\right) dx <3$$ for all $a,b \in \mathbb{R}.$ Here is my solution: \begin{align*}
\int_a^b \sin\left(x+\frac{1}{x}\right) dx&=\int_a^b \left(\sin x\cos \frac{1}{x}+\cos x\sin \frac{1}{x}\right) dx\\
&\le \int_a^b( \sin x+\cos x )dx\\
&=\sqrt{2}\int_a^b \sin \left(x+\frac{\pi}{4}\right)d\left(x+\frac{\pi}{4}\right)\\
&\le2\sqrt{2}<3.
\end{align*} But this seems to be incorrect, since we can not guarantee $\sin x,\cos x\ge 0$ in the first line.","['integration', 'calculus', 'definite-integrals', 'inequality']"
4187790,Explaining the integral of Laplacian of $r \mapsto \frac1r$,"Consider the function $\Bbb R^3 \to \Bbb R$ defined by $$f(x,y,z) = \frac1{\left( x^2 + y^2 + z^2 \right)^{\frac12}}$$ or, written in polar coordinates, $f (r) = \frac1r$ . The Laplacian $\nabla^2 f \equiv \nabla \cdot (\nabla f)$ can be computed as $0$ everywhere except the origin. Is there a way to rigorously define its value at the origin, and a concept of integration, such that $$\int_{\Bbb R^3} \nabla^2 f = -4\pi?$$ (This is a result I saw in a physics textbook that was computed by non-rigorous means.) If not, what does the previous integral expression actual mean, mathematically?","['laplacian', 'physics', 'multivariable-calculus', 'polar-coordinates']"
4187829,Can one always tile the plane with a regular polygon + a single other shape?,"We know that regular triangles, squares and hexagons can tile the plane without leaving any ""hole"". However, I've noticed that many regular polygons can tile the plane if we allow for a single type of ""hole"" ( i.e. , another shape) to be present. The following image contains an example with pentagons and rhombi: What we obtain in this case is not a periodic tiling, but rather an aperiodic one: Still, we are able to tile the plane with these two shapes. Another example, this time with decagons + ""concave hexagons"": My question is: Can we always tile the plane by combining a regular polygon and a single other shape? I'm also interested in the extension to star polygons. Edit As pointed out in the comments, the way I asked the question was imprecise. I guess that we should add the additional constraint that no ""hole"" can be in contact with another ""hole"", otherwise there will be trivial solutions to the problem.","['euclidean-geometry', 'geometry', 'tiling']"
4187849,Find $\lim_{n \rightarrow \infty} E(X_1\mid X_1+\cdots+X_n)$,"Let $X_1,..,X_n$ be i.i.d integrable random variables. Find $\lim_{n \rightarrow \infty} E(X_1\mid X_1+\cdots+X_n)$ . I was thinking of using the Basu's theorem in some way. If I can claim that $\frac{X_1}{X_1+\cdots+X_n}$ is ancillary , then it is independent of $\sum_{i=1}^n X_i$ . Then since the $X_i$ 's are i.i.d, and we know that $\sum_{i=1}^n \frac{X_i}{\sum X_i}=1 \implies E\left(\frac{X_1}{\sum X_i}\right)=\frac{1}{n}$ which goes to $0$ as $n \rightarrow \infty$ . But I don't think ancillarity makes sense here. How to approach this otherwise?","['probability-distributions', 'conditional-expectation', 'expected-value', 'probability-theory', 'probability']"
4187889,What is the intuition behind the Fisher-Rao metric?,"I've seen the Fisher-Rao metric introduced via the following argument (see Sidhu and Kok 2019 ): There is a natural pairing between the simplex and its dual space of classical random variables: $\langle A,p\rangle=\sum_i A_i p^i$ , where $A$ is the random variable and $p$ the probability distribution (I'll assume discrete distributions here). Moreover, we can naturally define a scalar product between random variables as $$\langle A,B\rangle = \sum_j A_j B_j p^j.$$ This suggests introducing the metric $h^{jk}=\delta^{jk} p^j$ , which provides the above metric product structure. If we now take the inverse of this metric, that is, the metric on the dual space of the random variables, we get $h_{jk} = \delta_{jk}/p^j,$ which corresponds to the line element $$\mathrm ds_{\rm FR}^2 = \sum_j \frac{\mathrm dp^j \, \mathrm dp^j}{p^j}.$$ I'm a bit confused by this argument.
We are thinking of random variables as dual elements of the probability distributions, which is fine. However, I don't understand the introduced metric: the metric should be defined between tangent vectors of the manifold we are considering. So here the random variables are tangent vectors? But when we take the dual metric, $h_{jk}$ , this is then supposed to operate on tangent vectors to the manifold of probability distributions. But the duals of random variables are probability distributions, not displacements in the simplex, so then the metric should define an inner product between probability distributions.
So in summary, I'm confused, does the Fisher-Rao metric define an inner product/metric directly on probability distributions, or on tangent vectors/vector fields in the manifold of probability distributions?","['information-geometry', 'intuition', 'fisher-information', 'probability-theory', 'differential-geometry']"
4187919,On Sum and Product of Two Projections,"The following is Exercise 4 page 40 in Functional Analysis book of Conway : Let $\operatorname{P}$ and $\operatorname{Q}$ be projections. Show: (a) $\operatorname{Ρ+Q}$ is a projection if and only if $\operatorname{ranP} \perp \operatorname{ranQ}$ . If $\operatorname{P+Q}$ is a projection, then $\operatorname{ran(P+Q)} = \operatorname{ranP} + \operatorname{ranQ}$ and $\operatorname{ker(P+Q)} = \operatorname{kerΡ} \cap \operatorname{kerQ}$ . (b) $\operatorname{PQ}$ is a projection if and only if $\operatorname{PQ} = \operatorname{QP}$ . If $\operatorname{PQ}$ is a projection, then $\operatorname{ran(PQ)} = \operatorname{ranP} \cap \operatorname{ranQ}$ and $\operatorname{ker(ΡQ)} = \operatorname{kerΡ} + \operatorname{kerQ}$ . My attempt for (a) : Let $ran Ρ \perp ran Q$ . If $x \in ran Ρ$ then $x \in (ran Q)^{\perp} = \operatorname{ker}(Q)$ where the equality is by def (See Definition. 3.1. of the book). Thus (QP+PQ)(x)=0. If $x \in \operatorname{ker}(Ρ)$ then by symmetry of argument (QP+PQ)(x)=0. Since $H = \operatorname{ker}(Ρ) + ran P$ so QP+PQ = 0 and thus $(P+Q)^2=P+Q$ . To complete the ""if"" part of (a) I have to show that $\operatorname{ker}(Ρ+Q) = (ran (Ρ+Q))^{\perp}$ which I couldn't do. For the ""only if"" part, see here . The answer in that link uses the claim that proving being Hermitian of an operator is enough for being Projection which there is no such discussion in the book! for the second part of (a), if $x \in \operatorname{ker}(Ρ) \cap \operatorname{ker}(Q)$ then obviously $x \in \operatorname{ker}(P+Q)$ ; I don't know how to prove the converse. My attempt for (b) : If PQ=QP then PQ is a projection if I can prove $ker (PQ) = (ran PQ)^{\perp}$ which is not so clear. Projection is defined in the book as follows : 3.1. Definition. An idempotent on H is a bounded linear operator Ε on H
such that E^2 = E. A projection is an idempotent Ρ such that $ker Ρ = (ran P)^{\perp}$ . For any other way of proving the statements in the exercise I need to prove them as well i.e. only the definition given is the basic valid one!","['solution-verification', 'projection', 'functional-analysis']"
4187983,"Solving $2x + 1 = 11$: Why, when subtracting $1$, do I only do it to a single term on the left but, if dividing by $2$, I must divide both terms?","Solving $2x + 1 = 11$ (for example) Why, when subtracting $1$ , do I only do it to a single term on the left but, if dividing by $2$ , I must divide both terms ?",['algebra-precalculus']
4188000,Primes represented by $x^2+14y^2$ and $2x^2+7y^2$.,"This is a well known example in binary quadratic forms which I have started studying. Consider binary quadratic forms of discriminant $D=-56$ . By reduction it turns out that for odd primes $p$ which are congruent to one of $1,9,15,23,25,39\,(\!\!\bmod\,56)$ there are two proper equivalence classes of forms represented by the reduced forms $x^2+14y^2$ and $2x^2+7y^2$ . I haven't read genus theory much but I know that these two forms are in same genus. My question is that is it possible that a same prime $p$ congruent to one of the above residues can be represented by both the forms $x^2+14y^2$ and $2x^2+7y^2$ . In other words is there a way to separate the primes of the given residues which are represented by these two forms etc. Can I say that a prime $p$ congruent to the above residues can be represented by precisely one of the above two forms but not by both??? Is it possible to come to a conclusion with only reduction theory (this is upto what I have studied till now)??? For the case $D=-20$ , however the reduced forms $x^2+5y^2$ and $2x^2\pm 2xy+3y^2$ represent different primes which can easily be seen by examining the resudues  mod 20 which are coprime to 20, which happen to be disjoint for the two forms. But unfortunately this fails for $D=-56$ and the forms in question, as the residues mod 56 for $x^2+14y^2$ and $2x^2+7y^2$ are identical i.e $1,9,15,23,25,39\,(\!\!\bmod\,56)$ . Reference: Advanced Algebra by Knapp (page 16). Thanks in advance.","['number-theory', 'quadratic-forms']"
4188020,Covariant derivative on principal bundle,"I know that there exists a connection on a principal bundle and via parallel transport it is possible to define a a covariant derivative on the associated bundle. However, can we also define a covariant derivative on the principal bundle. I.e. something that can differentiate a section along a vector field? Or do we need a linear structure like the one in a vector bundle to 'take derivatives'?","['principal-bundles', 'connections', 'differential-topology', 'differential-geometry']"
4188042,"Showing $\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2^n(n+1)\cos^nx\cos(n+2)x$, and the counterpart for sine","How do I show that $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2^n(n+1)\cos^nx\cos(n+2)x\tag{1}$$ $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\sin2kx =2^n(n+1)\cos^nx\sin(n+2)x\tag{2}$$ My try: From power-reduction formula, if $n$ is odd, we have $$\cos^{n}x = \frac{2}{2^n}\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\cos(n-2k)x\tag{3}$$ Putting (3) in (1), assuming $n$ is odd in (1), we have $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2^{n}(n+1)\cos(n+2)x\frac{2}{2^n}\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\cos(n-2k)x$$ $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2(n+1)\cos(n+2)x\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\cos(n-2k)x$$ $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2(n+1)\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\cos(n+2)x\cos(n-2k)x \tag{4}$$ Using the product-to-sum identity on the RHS of (4), we have $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =2(n+1)\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\left(\frac{\cos2(n-k+1)x+\cos2(k+1)x}{2}\right)$$ $$\sum_{k=0}^{n+1}\binom{n+1}{k}k\cos2kx =(n+1)\sum_{k=0}^{\frac{n-1}{2}}\binom{n}{k}\left(\cos2(n-k+1)x+\cos2(k+1)x\right)$$ This is where I got stuck. I am thinking if the odd part could be solved, it will pave the way for the even part. Same for (2). How can I proceed or is there a simpler way of solving it?",['trigonometry']
4188093,Asymptotics for the probability that two $n$-cycles generate $S_n$ (or $A_n$).,"Is there a known asymptotic formula for the probability that two $n$ -cycles generate $S_n$ (or $A_n$ in the event that $n$ is odd)? There seems to be a lot of published research on this question for two arbitrary permutations. This is more a reference request -- no proof necessary. (In fact, I am only interested in the case when $n = p$ is prime, if that is somehow easier or known.)","['asymptotics', 'reference-request', 'symmetric-groups', 'group-theory', 'probability']"
4188108,$\alpha+\beta+\gamma+\delta=0$ and $\alpha^n+\beta^n+\gamma^n+\delta^n=0$ then prove that $\alpha(\alpha+\beta)(\alpha+\gamma)(\alpha+\delta)=0$,"Suppose $\alpha,\beta,\gamma,\delta\in\mathbb{R}$ such that $~\alpha+\beta+\gamma+\delta=0~$ and $~\alpha^n+\beta^n+\gamma^n+\delta^n=0~$ (where $n\in\mathbb{N}$ and $n\ne1$ ), then prove that $~~\alpha(\alpha+\beta)(\alpha+\gamma)(\alpha+\delta)=0$ The proof is trivial if $n$ is even because we get $\alpha=\beta=\gamma=\delta=0$ . So we consider the case where $n$ is odd. If $\alpha=0$ then proof is direct so I considered the case where $\alpha\ne0$ . Now it is only required to prove that sum of any two of $\alpha,\beta,\gamma,\delta$ should be $0$ . The given condition can be simplified as $$\beta^n+\gamma^n+\delta^n=(\beta+\gamma+\delta)^n$$ If $\beta,\gamma,\delta$ is non negative then we can use power-mean inequality to get, $${\beta^n+\gamma^n+\delta^n\over3^n}=\left({\beta+\gamma+\delta\over 3}\right)^n\le{\beta^n+\gamma^n+\delta^n\over 3}$$ $$\implies\alpha^n=\beta^n+\gamma^n+\delta^n=0$$ This contradicts $\alpha\ne0$ But I am not able to prove for the case where at least one of $\beta,\gamma,\delta$ is negative. For example if take $\beta,\delta$ to be positive and $\gamma$ to be negative and then apply power-mean inequality we get $$\left({\beta+\delta\over 2}\right)^n\le{\left(\beta+\gamma+\delta \right)^n+(-\gamma)^n\over2}={\beta^n+\delta^n\over2}$$ What we get is basically the power-mean inequality applied to $\beta$ and $\delta$ and nothing new. Can someone help me complete the proof or provide an alternative solution. Also could  we prove a tighter equality $~(\alpha+\beta)(\alpha+\gamma)(\alpha+\delta)=0~$ to be true (I couldn't find a counter example for it)? Thanks in advance.","['real-numbers', 'algebra-precalculus', 'real-analysis']"
4188145,Splitting a set of complex ODE in real ODEs,"In an answer to this question it is stated that ""Any n dimensional ODE system in $\mathbb{C}$ can be described as an 2 $n$ dimensional ODE system in $\mathbb{R}$ using $z(t)=\mathrm{Re}(z) + i \ \mathrm{Im}(z)$ and $t=\mathrm{Re}(t) + i\ \mathrm{Im}(t)$ ."" This shall be done by $$
\frac{\mathrm{d}[\mathrm{Re}(z)]}{\mathrm{d}[\mathrm{Re}(t)]} = \mathrm{Re}[f(z,t)]
$$ $$
\frac{\mathrm{d}[\mathrm{Im}(z)]}{\mathrm{d}[\mathrm{Im}(t)]} = \mathrm{Im}[f(z,t)]
$$ Is this procedure really general and without any additional requirements on $z \in \mathbb{C}$ and $t \in \mathbb{C}$ ? Can any use be made of a fact that the interval of integration in complex plane is a straight line?","['complex-analysis', 'ordinary-differential-equations']"
4188193,Determining whether a set is connected,"Define for a field $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}, \mathbb{F}^\omega := \{(x_n)_{n\geq 1} : x_i \in \mathbb{F}\,\forall i\}.$ Let $x=(x_n)_{n\geq 1}, y=(y_n)_{n\geq 1} \in \mathbb{R}^\omega$ . Define the $\infty$ -norm or sup norm to be $d_\infty(x,y) := ||x-y||_\infty = \sup_{i\in\mathbb{N}}|x_i-y_i|.$ Define $\ell_\infty(\mathbb{C}) := \{x=(x_n)\in \mathbb{R}^\omega : ||x||_\infty < \infty\}.$ Determine whether the set $A := \{a \in \ell_\infty(\mathbb{C}) : |a_k| = 1\,\forall k \in \mathbb{Z}^+\}$ is connected (in the space $(\ell_\infty(\mathbb{C}), d_\infty)$ ). I tried several times to show that this set is connected, but I was unsuccessful each time. For instance, I thought of using the fact that a connected subset of a metric space has no nontrivial proper closed and open sets. I also thought of defining a path between any two points of $A$ but because it would map an element of $[0,1]$ to an element with infinitely many entries, I wasn’t sure how to show that such a map would be continuous.","['elementary-set-theory', 'connectedness', 'general-topology', 'real-analysis']"
4188222,Question regarding generalized Egoroff's Theorem,"I'm looking at this problem , which asks one to prove a generalized version of Egoroff's Theorem. While I understand the OP's approach to divide $X$ into a countable number of subsets (and apply Egoroff's Theorem to those with finite measure), I don't understand his final claim, that $f_n \to f$ uniformly on $E^{c}$ . Roughly speaking, the OP's argument is ""if $f_n \to f$ uniformly on a countable  collection of sets $\{A_k\}_{k_1}^{\infty}$ , then $f_n \to f$ uniformly on $\bigcup_{k=1}^{\infty} A_k$ "". However, this is not necessarily true. I can't find a good justification for his last step.","['measure-theory', 'real-analysis']"
4188242,When do Linear Transformations NOT preserve angles between vectors? Doesn't the SVD tell us all linear transformations preserve angles?,"From searching on the internet, I learned only a subset of linear transformations preserve angles between vectors. But - Learning about the SVD - we can geometrically understand as breaking down some matrix A into a three matrices. These matrices can be understood geometrically as a rotation step, then a scaling  step, and then another rotation. Since any matrix can be broken down into these three steps (Since SVD applies to all matrices A?) doesn't that mean that all transformations are simply a rotation, a scaling, and then a rotation, which means the angles are preserved? Why is this not true? And when do linear transformations preserve angles, and when do they not? Thanks,
A","['matrix-decomposition', 'svd', 'linear-algebra', 'linear-transformations', 'singular-values']"
4188257,Simplifying Summation by Extracting Coefficient?,"I am looking to solve: Show that $$\sum_{n \ge 0} \sum_{k \ge 0} {n\choose k} {2k \choose k} y^k x^n = \frac{1}{\sqrt{(1-x)(1-x(1+4y))}}$$ and then use that to show that $$\sum_{k \ge 0} {n \choose k} {2k \choose k} (-2)^{-k} = \begin{cases}{n \choose n/2}2^{-n}, & \text{if }n \ge 0\text{ is even} \\ 0, & \text{if }n \ge 0\text{ is odd}\end{cases}$$ Working the problem myself, I can pull out certain things but it doesn't lead me towards the correct answer. For example, I know that $\sum_{k \ge 0} {a \choose k} x^k = (1+x)^a$ , so I could potentially use that by pulling out $\sum_{k \ge 0} {n \choose k} y^k = (1+y)^n$ , or alternatively by pulling out $\sum_{k \ge 0} {2k \choose k} y^k = (1+y)^{2k}$ . Similarly, I know that $\sum_{n \ge 0} x^n = (1-x)^{-1}$ , so I could potentially pull that out as well. However, with whatever combination of factors I pull out, I am left with something that looks far off from $\frac{1}{\sqrt{(1-x)(1-x(1+4y))}}$ . As for the second part, I am not sure if I am supposed to extract the coefficient of $x^n$ . If I set $k=n$ , I get $$[x^n]\sum_{k \ge 0} {n \choose k} {2k \choose k} (-2)^{-k} = {n \choose n} {2n \choose n} (-2)^{-n} = {2n \choose n}(-2)^{-n}.$$ This is the wrong binomial coefficient, combined with an extra negative sign by the $2$ , and no indication of any need for $n$ being even or odd. Any thoughts?","['summation', 'functions', 'binomial-coefficients', 'generating-functions']"
4188258,"How ""practical"" is the Laplace transform method for constant coefficient ODE?","I just finished teaching a chapter on using Laplace transform to solve constant coefficient second order linear differential equations. I touted how amazing the method was because it incorporates the initial data from the start, works for strange forcing terms, and reduces the problem of solving an ODE to computing the unit impulse response $e(t)$ (by taking the easy inverse Laplace of the reciprocal of the characteristic function) and the convolution $e*g$ , where $g$ is the forcing term--notice how we do not need to compute Laplace of $g$ . I know theoretically this is significant, and that for discontinuous and non-standard forcing terms this is one of the best methods. However, I was left with the feeling that if we cannot really calculate the convolution (closed form) then this is not as impressive after all! So: How commonly is this method actually used in practice for solving ODE -- say by engineers? Are there ways to compute the convolution for a considerably large collection of pairs of functions? If we cannot find convolution in closed form, is this method used to produce numerical solutions, e.g., by estimating the integral in the definition of the convolution? References will be appreciated (over heuristics)!","['inverse-laplace', 'convolution', 'laplace-transform', 'ordinary-differential-equations']"
4188295,"$\frac{1-a^3}{a}=\frac{1-b^3}{b}=\frac{1-c^3}{c}, a \neq b \neq c$; What is $a^3+b^3+c^3$?","$\frac{1-a^3}{a}=\frac{1-b^3}{b}=\frac{1-c^3}{c}\\a \neq b \neq c\\a^3+b^3+c^3=$ Since $\frac{1-a^3}{a}=\frac{1-b^3}{b}$ , $a^3=b^2a-\frac{a}{b}-1$ . $\therefore a^3+b^3+c^3=ab^2+bc^2+ca^2-\frac{ab+bc+ca}{abc}-1$ Since $a^3+b^3+c^3=(a+b+c)(a^2+b^2+c^2-ab-bc-ca)+3abc$ , $ab^2+bc^2+ca^2-\frac{ab+bc+ca}{abc}-1=(a+b+c)(a^2+b^2+c^2-ab-bc-ca)+3abc$ (stuck here)","['cubics', 'algebra-precalculus']"
4188312,First Order Linear Differential Equations having two solutions,"I have a linear first order ordinary differential equation $$\frac{dy}{dx}+\tan (x) y = 2 \cos^2 x \sin x - \sec x$$ with an initial condition as $y(\frac{\pi}{4})=3 \sqrt2$ My integrating factor $\mu(x)=\sec x$ After multiplication with the integration factor what I get is: $$(secx\ y)'=sin2x-sec^2x$$ or $$(secx\ y)'=2 sinx cosx-sec^2x$$ If I use the first equation I get: $$y(x)=\frac{\frac{1}{2}cos2x-tanx+c}{secx}$$ and using the second equation I get: $$y(x)=\frac{sin^2x-tanx+c}{secx}$$ ( $\int 2\ sinx\ cosx\ dx = 2 \frac{sin^2x}{2}=sin^2x$ ) with the first equation I get $c=7$ and second equation I get $c=\frac{13}{2}$ . It is a very simple differential equation but when I solve it I get two different answers.
Is this ok?","['calculus', 'ordinary-differential-equations']"
4188314,Odds of 3 identical digits in a row in a 6 digit number,"If I have a 6 digit random number what are the odds of me having 3 consecutive digits be identical?  examples $341117$ or $444628$ I thought of two ways to answer this and they give very different results. One (or both!) are incorrect.  Can someone explain to me what is wrong with the reasoning in the wrong one? First approach:  There are 4 possible positions for the 3 identical digits, and at each position there are ten possibilities $(000, 111, 222....999)$ . The remaining digits do not matter. So that sounds like out of the one million number combinations from 000000 all the way up to 999999 there are 40 ways this could happen, so this smells like 40 out of 1 million or one chance out of 25000. (and I think I now see a flaw in that reasoning) Second approach (and now I realize, gives the right answer): For each of the first 4 digits it does not matter what value they have.  There is then a one in 100 chance the next two digits will be the same. so the odds of this happening are 4 times 1 in 100 or one chance in 25. What I missed from the first approach is that for each set of three digits in a row, there are one thousand combinations for the other three digits, so for each of those 40 ways, there are actually 1000 possibilities (example 111432 and 111739 are two of the 1000 ways to have a 6 digit number starting with 111) . So 40,000 ways out of a million numbers is 1 in 25. I guess this is not even a question anymore, but it was when I started writing, so I will share.","['combinatorics', 'probability']"
4188402,Is a Lattice always uniformly discrete in $\mathbb{C}$?,"The following definition of discreteness and uniform discreteness comes from Wikipedia. A subset $S$ of a metric space $(Y,d)$ is said to be discrete in $Y$ , if for all $x\in S$ , there exists some $\delta=\delta(x)>0$ such that $d(x,y)>\delta$ for all $y\in S\setminus\{x\}.$ A subset $S$ of a metric space $(Y,d)$ is said to be uniformly discrete in $S$ , if there exists a $\delta>0$ such that for any $x,y\in S$ , either $x=y$ or $d(x,y)>\delta$ . Now, Rick Miranda's Algebraic Curves and Riemann Surfaces has the following construction. Consider the complex plane $\mathbb{C}$ , and let $w_{1}, w_{2}\in\mathbb{C}$ be linearly independent over $\mathbb{R}$ , and we fix this pair $w_{1}, w_{2}$ . Define the Lattice by $$L:=\mathbb{Z}w_{1}+\mathbb{Z}w_{2}=\{a_{1}w_{1}+a_{2}w_{2}:a_{1},a_{2}\in\mathbb{Z}\}.$$ Then, the books said: The lattice $L$ is a discrete subset of $\mathbb{C}$ , so there exists $\epsilon>0$ such that $|w|>2\epsilon$ for every non-zero $w\in L$ . Fix such an $\epsilon$ , and.....(not relevant stuff starts). Given the context of his sentence, it seems that he is claiming that $L$ is in fact uniformly discrete in $\mathbb{C}$ . (Otherwise he cannot fix such $\epsilon$ , without fixing the choice of point from the Lattice, and the later proof seems assuming this $\epsilon$ works for everyone) Is this true? I looked it up online, but did not find any convincing reference. I tried to prove it. So, we write $w_{1}=x_{1}+iy_{1}$ and $w_{2}=x_{2}+iy_{2}$ , and we let $x,y\in L$ so that $x=w_{1}a_{1}+w_{2}a_{2}$ and $y=w_{1}b_{1}+w_{2}b_{2}$ , where $a_{1},a_{2},b_{1},b_{2}\in\mathbb{Z}$ . Then, the (complex norm) distance between $x$ and $y$ is \begin{align*}
|x-y|^{2}&=|a_{1}w_{1}+a_{2}w_{2}-b_{1}w_{1}-b_{2}w_{2}|^{2}\\
&=|a_{1}x_{1}+ia_{1}y_{1}+a_{2}x_{2}+ia_{2}y_{2}-b_{1}x_{1}-ib_{1}y_{1}-b_{2}x_{2}-ib_{2}y_{2}|^{2}\\
&=(a_{1}x_{1}+a_{2}x_{2}-b_{1}x_{1}-b_{2}x_{2})^{2}+(a_{1}y_{1}+a_{2}y_{2}-b_{1}y_{1}-b_{2}y_{2})^{2}\\
&=\Bigg[x_{1}(a_{1}-b_{1})+x_{2}(a_{2}-b_{2})\Bigg]^{2}+\Bigg[y_{1}(a_{1}-b_{1})+y_{2}(a_{2}-b_{2})\Bigg]^{2}\\
&=\Bigg[x_{1}^{2}(a_{1}-b_{1})^{2}+2x_{1}x_{2}(a_{1}-b_{1})(a_{2}-b_{2})+x_{2}^{2}(a_{2}-b_{2})^{2}\Bigg]\\
&\ \ \ \ \ \ \ \ \ +\Bigg[y_{1}^{2}(a_{1}-b_{1})^{2}+2y_{1}y_{2}(a_{1}-b_{1})(a_{2}-b_{2})+y_{2}^{2}(a_{2}-b_{2})^{2}\Bigg]\\
&\geq 2x_{1}x_{2}(a_{1}-b_{1})(a_{2}-b_{2})+2y_{1}y_{2}(a_{1}-b_{1})(a_{2}-b_{2})\\
&=2(a_{1}-b_{1})(a_{2}-b_{2})(y_{1}y_{2}+x_{1}x_{2}).
\end{align*} What can I do further  I get a lower bound of this expression that does not depend on $a_{1},a_{2}$ and $b_{1},b_{2}$ ? (so does not depend on $x$ and $y$ ). For now, I have not utilized the fact that $a_{i},b_{i}\in\mathbb{Z}$ . Well, if we have a perfect scenario that $a_{1}>b_{1}\geq 0$ and $a_{2}>b_{2}\geq 0$ , then $$2(a_{1}-b_{1})(a_{2}-b_{2})(y_{1}y_{2}+x_{1}x_{2})>2\cdot 1\cdot 1(y_{1}y_{2}+x_{1}x_{2}),$$ is the desired bound. But I am not sure what to do if we are not in this kind of scenario (so I guess this gives us some sort of a counter-example?) Thank you! Edit 1: Okay, I think Rick Miranda actually means discrete. I will explain in the answer I will post on my own. Also, I will also point a proof of why $L$ is discrete, as my above proof cannot be used to prove this either.","['complex-analysis', 'general-topology']"
4188429,Prove that the square root of 3 is irrational [duplicate],"This question already has answers here : How to prove: if $a,b \in \mathbb N$, then $a^{1/b}$ is an integer or an irrational number? (14 answers) Closed 9 years ago . I'm trying to do this proof by contradiction. I know I have to use a lemma to establish that if $x$ is divisible by $3$, then $x^2$ is divisible by $3$. The lemma is the easy part. Any thoughts? How should I extend the proof for this to the square root of $6$?","['radicals', 'rationality-testing']"
4188433,Meager or measure zero $A\subset \mathbb R$ with $A+A=A$ and $A-A=\mathbb R$,"Is there a meager or measure zero set $A\subset \mathbb R$ with $A+A=A$ and $A-A=\mathbb R$ ? I am using sumset notation $$A+A=\{a+b:a,b\in A\}$$ $$A-A=\{a-b:a,b\in A\}$$ In more algebraic terms, $A$ must be an additive submonoid that generates $\mathbb R$ as a group. (At least, assuming $0\in A,$ which is harmless.) It is a classic exercise to construct a meager measure zero set $A$ such that $A-A=\mathbb R.$ For example, take $A$ to be the set of numbers with no $9$ in any decimal expansion. But this doesn't satisfy $A+A=A.$ For any discontinuous additive function $f:\mathbb R\to\mathbb R$ the set $A=\{x:f(x)\geq 0\}$ satisfies the conditions except it only has inner measure zero. I want the outer measure to be zero.","['measure-theory', 'baire-category', 'additive-combinatorics']"
4188484,Generalization of homotopy groups,"Studying homotopy groups, I faced the definition of pointed homotopy, denoting the homotopy groups as $[\mathbb{S}^n, X]^0$ . In order to define a well-defined structure of group, we can think to $\mathbb{S}^n$ as $I^n/\partial I^n$ and $[\mathbb{S}^n, X]^0 = [(I^n,\partial I^n)(X,x_0)]$ maps of pair. Explicit homotopies can be given to prove that $e+f \sim f+e \sim f$ , $f + (-f) \sim e$ , $(f+g)+h \sim f+(g+h)$ and commutativity. I wrote those homotopies by hand heavily using convexness of $I^n$ . In my notes I have an observation which states that with the same process we can consider something more general, as $[\mathbb{S}^n \wedge X,Y]^0$ which of course gives the homotopy groups as a particular case taking $X = \mathbb{S}^0$ . I was wondering who are those groups? There's an easy way to show the structure of groups as in the usual homotopy groups?","['spheres', 'fundamental-groups', 'homotopy-theory', 'general-topology', 'algebraic-topology']"
4188499,"Find minimum value of the function $f(x,y,z)=x+y+z$ on the ball $x^2+y^2+z^2=a^2$. (Where did I make a mistake?)","Find minimum value of the function $f(x,y,z)=x+y+z$ on the ball $x^2+y^2+z^2=a^2$ , where $x,y,z \ge 0$ and $a>0.$ What I did: Lagrange function: $L(x,y,z)=x+y+z+\lambda(x^2+y^2+z^2-a^2)$ . $L_x=1+2\lambda x = 0.$ $L_y=1+2\lambda y = 0.$ $L_z=1+2\lambda z=0.$ $L_{\lambda}=x^2+y^2+z^2=a^2$ . From the first three equations: $x=y=z=-\frac{1}{2\lambda}$ From the last equation: $\frac{3}{4\lambda^2}=a^2 \Longrightarrow\lambda=\pm \frac{\sqrt{3}}{2a}$ . Since we want $x,y,z\ge0.$ $x=y=z=\frac{a}{\sqrt{3}}$ (Took the negative $\lambda$ ). And substituting back into $f(x,y,z)=\frac{3a}{\sqrt{3}}=\sqrt{3}a$ . But the answer is $a$ . Would appreciate any help, I can't find where did I make a mistake, and I don't want to assume that the final answer given is false without being sure about it, because I know I make alot of mistakes. Thanks in advance!","['multivariable-calculus', 'lagrange-multiplier']"
4188531,Derive a new equation from $m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$,"$$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ $$\implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}}$$ $$\implies m^2c^2-m^2v^2=m_0^2c^2$$ Differentiating the equation, $$2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0$$ Our book says when we differentiate $m^2c^2-m^2v^2=m_0^2c^2$ . We will get the above equation. But, what I understand about Calculus. That I can't derive it anyway. Actually, what we differentiate here? $mass$ or, $velocity$ ? Relativistic mass equation : $$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ Above question is a constant equation which maybe found from Lorentz Transformation. In the second line I’ve just squared both side. In third line I‘ve just moved ""something"" right to left. Then, I differentiate. But, I can't understand how they differentiate here. After writing the question, I was doing some the sum again. Relativistic mass changes over time (How fast you travel through space your mass decreases). Here $m$ is relativistic mass and $m_0$ is ""normal"" mass. So, I decided to differentiate mass $$(mc)^2-(mv)^2-(m_0c)^2=0$$ We know, $$(f(x))^n=n(f(x))^{n-1} . f' (x)$$ Then : $$2 (mc) . c - 2 (mv) .v -$$ Then, I can't differentiate anymore. $c$ is speed of light which is constant. That $m_0$ is also constant. So, I stopped there. Prove of $E=mc^2$ , $$F=\frac{dp}{dt}$$ $$=\frac{d}{dt} (mv)$$ $$= m \frac{dv}{dt} + v \frac{dm}{dt}$$ ----------------1 $$dW=F .dS$$ $$=>dK=F .dS$$ $$=>dK=[m \frac{dv}{dt} +v \frac{dm}{dt}] .dS$$ $$=m\frac{dS}{dt} .dv + v . \frac{dS}{dt} .dm$$ $$=mvdv+v^2dm$$ -------------------2 $$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ $$\implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}}$$ $$\implies m^2c^2-m^2v^2=m_0^2c^2$$ $$2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0$$ $$c^2dm=mvdv+v^2dm$$ ---------------3 $$dK=c^2dm$$ $$\int = \int c^2dm$$ (In LHS Integral starts from 0 and finishes at $k$ . In RHS Integral starts from $m_0$ and ends at $m$ ) $$k=c^2[m-m_0]$$ Hence, $$Total energy = k + Rest mass energy$$ $$E=c^2[m-m_0]+m_0c^2$$ $$=mc^2-m_0c^2+m_0c^2$$ $$=mc^2$$ $$E=mc^2$$",['derivatives']
4188536,Understanding vector field(s) on $\mathbb{S}^3$.,"I was slving the exercises of John Lee's book ""Introduction to Smooth Manifolds"", where there is an exercise asking us to prove that $\mathbb{S}^3$ is parallelizable. In the hint, the author asks us to consider the vector fields: $$X_1 = -x\dfrac{\partial}{\partial w} + w \dfrac{\partial}{\partial x} - z \dfrac{\partial}{\partial y} + y \dfrac{\partial}{\partial z},$$ $$X_2 = -y\dfrac{\partial}{\partial w} + z \dfrac{\partial}{\partial x} + w \dfrac{\partial}{\partial y} - x \dfrac{\partial}{\partial z},$$ $$X_3 = -z\dfrac{\partial}{\partial w} - y \dfrac{\partial}{\partial x} + x \dfrac{\partial}{\partial y} + w \dfrac{\partial}{\partial z}.$$ I get the hint and how to use it. What I don't understand is why are the vector fields $4$ -dimensional? Isn't $\mathbb{S}^3$ a $3$ -dimensional manifold? This is why the tangent vectors should have only $3$ coordinates! I also searched other places on the internet and more or less, everybody uses $4$ coordinates for a vector field on $\mathbb{S}^3$ . Could anybody help me understand this?","['vector-fields', 'tangent-spaces', 'smooth-manifolds', 'tangent-bundle', 'differential-geometry']"
4188556,Sketch the region $|1+z+\frac{z^2}{2}|<1$ in the complex plane?,I know that to sketch $|1+z|<1$ this is simply a circle of radius $1$ centered at $-1$ on the real axis. I can't visualise how I would deal with the added $\frac{z^2}{2}$ term?,"['complex-analysis', 'complex-numbers']"
4188569,"Prove or disprove: $1+\frac{\ln^c(ax)-\ln^ca}{\ln^c(2a)-\ln^ca}-x\leq 0$, where $1<x$, $a>2$, and $c$ is chosen so that $f'(2)=0$",Define: $$f(x)=1+\frac{\ln^{c}\left(ax\right)-\ln^{c}\left(a\right)}{\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)}-x$$ where $1< x$ and $a>2$ . Assume further that the parameter $c$ is chosen so that $f'(2)=0$ . The derivation of $c$ involves the Lambert's function. Claim : $$f(x)\leq 0$$ My attempt: We have : $$f'(x)=\frac{c(\ln(ax))^{c-1}}{x\left(\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)\right)}-1$$ We substitute $x=\frac{1}{y^{c-1}a}$ The inequality have the form : $$\ln(u)u=p$$ Wich is just the Lambert's function .See the solution in this link . I cannot proceed further . How to (dis)prove the first inequality ? Thanks in advance,"['inequality', 'derivatives', 'logarithms']"
4188570,conditional expectation of the product,"Let $\Omega$ be a compact Hausdorff space in $\mathbb{C}^n$ . Let $\sigma_\Omega$ be the Borel sigma algebra on $\Omega$ . Let $\zeta: \Omega\longrightarrow\partial \mathbb{D}$ be a non constant continuous  function. Let $\sigma_{\partial \mathbb{D}}$ be the Borel sigma algebra on $\partial \mathbb{D}$ (Unit circle on the complex plane). Now consider the sigma algebra $\sigma_\zeta=\{{\zeta}^{-1}(A): \;A\in \sigma_{\partial \mathbb{D}}\}\subset \sigma_\Omega$ . Now let $f\in L^1(\Omega, \sigma_\Omega, \mu)$ and lets define a new measure $f_\mu$ on $(\Omega,\sigma_\zeta)$ as $f_{\mu}(A)=\int_A f d\mu$ . It is easy to see that for $A\in \sigma_\zeta $ , ${\mu}(A)=0$ implies $f_{\mu}(A)=0$ , i.e $f_{\mu}(A)$ is absolutely continuous with the restriction of $\mu$ to $\sigma_\zeta$ , so by the Radon Nikodym theorem there exists a $g\in L^1 (\Omega, \sigma_\zeta, \mu)$ such that $\int_A f d\mu =\int_A g d\mu$ for every $A\in \sigma_\zeta$ . Lets call this $g$ as the conditional expectation of $f$ and denote it as $E(f|\sigma_\zeta)$ . Now suppose $h,k\in L^1(\Omega, \sigma_\Omega, \mu)$ are bounded. Then will it be true that $E(hk|\sigma_\zeta)=E(h|\sigma_\zeta)E(k|\sigma_\zeta).$ If not, then is there a condition (preferably if and only if) under which the same would hold? An answer in Measure Theory terms would be really appreciated.","['conditional-expectation', 'measure-theory', 'probability-theory', 'probability']"
4188607,Solving $x!=n$ without a calculator for large $n$,"Solve the equation for $x$ : $$x!=n$$ where $x,n \in \mathbb N$ . (Here $n \in \mathbb N$ is a given number such that $x!=n$ has a solution in $x \in \mathbb N$ .) The problem is easy with a calculator and for small $n$ . But I want to solve this for large $n$ without a calculator (Why? Just curious!). Here is an example of how large $n$ can be: $$x!=523 022 617 466 601 111 760 007 224 100 074 291 200 000 000$$ (Here $n$ is a $45$ -digit number and $x=38$ .) To solve the equation, I could think of two methods. Here is their description: Method 1 - prime factorization: In this method, we find the prime factorization of $n$ . To do this, we keep dividing $n$ by $2$ until we get an odd number. This is equivalent to finding $\log_2 n$ . But since $n$ is a large number, it is very difficult and long process. For the example problem, we have to divide $n$ by $2$ , $35$ times. Yet this doesn't give the exact answer. Since $\lfloor 38/2 \rfloor + \lfloor 38/2^2 \rfloor + \dots =\lfloor 39/2 \rfloor + \lfloor 39/2^2 \rfloor + \dots=35$ this implies that $x=38$ or $x=39$ . We have to divide the $n$ by $13^3$ to be sure that $x=38$ is the solution. Method 2 - Stirling's approximation: In this method, we use the Stirling's approximation formula, which is $x! \sim \sqrt {2\pi x}(\frac x e)^x$ . But it is not easy to find $x$ in terms of $n$ . Here is my workings to do that: $$x!=n$$ $$\implies \sqrt {2\pi x}(\frac x e)^x=n$$ $$\implies 2\pi x (\frac x e)^{2x}=n^2$$ $$\implies x^ {2x+1}\cdot \frac 1 {e^{2x}}=\frac {n^2}{2\pi}$$ I am unable to proceed after that. But I don't think the derived equation can solve for $x$ in terms of $n$ without using a calculator (or at least this would be too hard). I hope my workings are correct. So, is there some easier way to solve the equation? And how do I complete my workings on method 2?","['algebra-precalculus', 'systems-of-equations', 'factorial', 'approximation']"
4188612,"What is the probability that the sequence “broken, good, good, broken, good” occurs in the next $5$ bottles?","I have a probability problem. I think I know how to solve $A$ , but $B$ is still a mystery to me. If anyone could help me I appreciate a lot. Return bottles are constantly returned to the lemonade factory. There is a probability of $18\%$ of a bottle to be broken. This probably occurs independently. A) what is the probability that the sequence “broken, good, good, broken, good” occurs in the next $5$ bottles? B) what is the probability of the $10$ first bottles of the next $147$ bottles being good? For $A$ I think is $0.18\times(1-0.18)\times(1-0.18)\times0.18\times(1-0.18)$ . And, for $B$ , it seems a negative binomial probability, but I am not sure. Thank you very much","['probability-distributions', 'probability-theory', 'probability']"
4188624,"Evaluating $\oint_C(x-y^3)dx+x^3dy$, where $C$ is the unit circle, in two ways gives two different answers","I'm trying to calculate $\oint_C(x-y^3)dx+x^3dy$ where $C$ is $x^2+y^2=1$ (opposite of clock direction..). I used $\vec r(t) = (\cos t,\sin t)$ and so $d\vec r=(-\sin t,\cos t)$ . After substituting in the integral I got $$\int_0^{2\pi}(-\cos t\sin t +\sin^4t+\cos^4t)dt$$ So I decided to deal with $$\begin{align}\sin^4t+\cos^4t&=\frac{1-\sin^2(2t)}{2}+\frac{1+\cos^2(2t)}{2}\\&=\frac{2-\sin^2(2t)+\cos^2(2t)}{2}\\&=\frac{\frac{4-\sin(4t)+\cos(4t)}{2}}{2}\\&=1+\frac{1}{4}\cos(4t)-\frac{1}{4}\sin(4t)\end{align}$$ And if i do the integral I would get $\int_0^{2\pi}1dt=2\pi$ . (since all of those $sin,cos$ will give integral of $0$ ) But, when I do it this way: $$\begin{align}\sin^4t+\cos^4t&=(\sin^2t+\cos^2t)^2-2\sin^2t\cos^2t\\&=1-\frac{\sin^2(2t)}{2}\\&=1-\frac{1}{2}\left[\frac{1-\cos(4t)}{2}\right]\\&=\frac{3}{4}+\frac{\cos(4t)}{4}\end{align}$$ . And now I get integral: $\int_0^{2\pi}\frac{3}{4}dt=\frac{3\pi}{2}$ I'm more sure of the second way, but I have checked the first way like 10 times already and I can't see what's the problem, what is making me get different answer than the second way, I can't leave it like this since I might have a real problem right here that I'm not noticing. I would appreciate any help and feedback! Thanks in advance!","['integration', 'trigonometry', 'parametrization']"
4188636,$\mathbb{K}^{\times} \times \mathbb{Z}$ is not group isomorphic to $\mathbb{K}^{\times}$,"Let $\mathbb{K}$ be a field with infinite cardinality and $\mathbb{K}^\times$ its group of units under multiplication (i.e. all elements except $0$ ). I want to determine if $\mathbb{K}^{\times} \times \mathbb{Z}$ is group isomorphic to $\mathbb{K}^{\times}$ or not. Intuitively I'm inclined to say not, since the first ""has a lot more elements"" than the second, but I'm not sure because weird things can happen with infinite groups, like proper subgroups being isomorphic to the whole group and so on. Just to be clear, the operation equipped on $\mathbb{K}^{\times} \times \mathbb{Z}$ is $(k,m) \star (h,n) = (k\cdot h, m+n)$ . This question was inspired by this post . In the second answer, this same question is solved for $\mathbb{K} = \mathbb{C}$ , using the fact that $\mathbb{C}^\times$ is a divisible group. But that's not true for $\mathbb{R}^\times$ for example, so I cannot use it in the general case. Can anyone help me?","['infinite-groups', 'field-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4188676,Why is a morphism $\nabla:\Theta_X\to \underline{End}(M)$ a connection (in D-modules)?,"Let $X$ be a smooth algebraic variety over $\mathbb{C}$ and $\Theta_X$ be its tangent sheaf. Giving an $\mathscr{O}_X$ -module $M$ the structure of a left $\mathcal{D}_X$ -module is equivalent to the data of a $\mathbb{C}$ -linear morphism $$\nabla:\Theta_X\to \underline{End}(M),$$ which is usually called a connection . Well... for me, a connection on a locally free sheaf $M$ is a $\mathbb{C}$ -linear morphism $$\nabla':M\to M\otimes_{\mathscr{O}_X}\Omega^1_X.$$ What is the precise relation between those two notions?","['lie-algebras', 'connections', 'algebraic-geometry', 'sheaf-theory', 'd-modules']"
4188726,Repeated roots of a function,"I have come across questions which states that the functions has repeated roots. There $\alpha$ is said to be  repeated root of $f(x)$ if $f(\alpha)=f'(\alpha)=0$ . If $f(x)$ is a polynomial I can understand why they call it as a repeated root because $(x-\alpha
)^2$ is a factor of $f(x)$ . But if $f(x)$ is not a polynomial then you cannot factorise $f(x)$ , so can someone tell me the reason why $\alpha$ called a repeated root in such cases. Thanks in advance.","['algebra-precalculus', 'functions', 'roots']"
4188759,Connection between the Jacobian of a vector function and exterior derivative of associated differential form.,"Let $A\subseteq\mathbb{R}^n$ be an open set and \begin{equation}
F:A\rightarrow R^n \qquad F\in C^1(A)
\end{equation} a vector field. The Jacobian matrix is defined as: \begin{equation}
J(\boldsymbol{x})=\left(\frac{\partial F_i}{\partial x_j}\right)  \quad \forall\boldsymbol{x}\in A
\quad i,j=1...n
\end{equation} Since we are not dealing with $\mathbb{R^3}$ , I shall talk about exterior derivatives rather than curl. Now, the exterior derivative of a 0-form (scalar function) is the differential, whose representative vector is the gradient of the function. The exterior derivative of a differential 1-form $\omega$ is the 2-form $d\omega$ that can be represented through a proper matrix. On the other hand, the Jacobian matrix plays for vector field the same role that the gradient plays for scalar functions, so by analogy with 0-forms, does the Jacobian matrix of the vector field associated with $\omega$ represent $d\omega$ ? If it doesn't, which of these two is the differential $dF$ of the  vector field? Does this happen only for 0-forms?","['differential-geometry', 'vector-fields', 'jacobian', 'derivatives', 'exterior-algebra']"
4188780,Asymptotics for $\sum_{k=0}^{n-1}(-1)^k \cot(\frac{2k+1}{4n}\pi)\log{(2\sin(\frac{2k+1}{4n}\pi))} $,"I have been able to establish the identity $$
I(n):=\int_0^1 \frac{y^{n-1}(1+y-y^{n+1}) - 1}{(1-y)(1+y^{2n})}\ dy = \frac{1}{n} \sum_{k=0}^{n-1}(-1)^k\cot(\frac{2k+1}{4n}\pi)\log{(2\sin(\frac{2k+1}{4n}\pi))}
$$ However, neither side seems conducive to discovering an asymptotic expansion as $n \to \infty.$ Purely on numerical evidence I conjecture that $I(n) \sim -\log(n).$ $$
\begin{array}{c|lcr}
n & I(n) &-\log{n} & -\log{n}/I(n)  \\
\hline
10^3 & -6.702 & -6.908 & 1.031  \\
10^4 & -9.004 & -9.2106 &1.023  \\
10^5 & -11.307 &-11.513 & 1.018   \\
\end{array}
$$ I seek the dominant and one subdominant asymptotic terms. (If the Lambert W function is involved, then maybe one term is sufficient.) Here are some ideas. (1) On the LHS (left-hand side), letting $n \to \infty,$ the $y^n$ terms integrand go to zero and you are left with $-1/(1-y),$ and the integral over it will diverge.  However, plotting the integrand for large $n$ , it is seen that the integrand follows the $-1/(1-y)$ curve until it sharply drops to zero near $y=1.$ It is probable that the position of the drop, $y_d,$ is analytically tractable, and one can approximate the integral as $-\int_{0}^{y_d}dy/(1-y).$ However, I have no idea on how to approach a subdominant term. (2) The RHS looks sort of like a Riemann sum, except for the pesky alternating sign, and the fact that odd integers appear in the arguments instead of every integer.  A long time ago I read that the Euler-Maclaurin formula has been extended to character sums, which this looks like.  I also know that Euler-Maclaurin can be used for asymptotic analysis.  A solution in this manner would be most edifying, since I know so little about it.","['integration', 'definite-integrals', 'roots-of-unity', 'asymptotics']"
4188790,Slicing criterion for flatness in the source (Vakil 24.6.F),"Exercise 24.6.F in Ravi Vakil's FOAG states: Suppose $A$ is a $B$ -algebra, $A$ and $B$ are Noetherian, $M$ is a finitely generated $A$ -module, and $f \in A$ has the property that for all maximal ideals $n \subset B$ , multiplication by $f$ is injective on $M/nM$ . Show that if $M$ is $B$ -flat, then $M/fM$ is also $B$ -flat. The hint states: Use the local criterion for flatness, Theorem 24.6.2. Notice that $$0 \to M
\xrightarrow{\cdot f} M \to M/fM \to0$$ is a flat resolution of $M/fM$ . I got stuck on this for quite a bit. I can solve the exercise by saying that the above sequence is the beginning of a flat resolution, with a possibly non-trivial kernel on the left. Then after tensoring with $B/n$ I show that $\text{Tor}_1^B(M/fM, B/n)=0$ which gives me flatness. However, I am wondering if this is a (slightly) erroneous hint or we can actually guarantee that multiplication by $f$ is injective on $M$ ?","['algebraic-geometry', 'solution-verification', 'flatness', 'commutative-algebra']"
4188799,How to prove this inequality about Kronecker product?,"I encounter the following problem when I study ridge regression. Problem. Let $\{d_j\}_{j=1}^\infty$ be a sequence of positive integers. Let $\{\psi_{i,j}\}_{i,j=1}^\infty$ be a collection of vectors where $\psi_{i,j}\in\mathbb{R}^{d_j}$ and $\|\psi_{i,j}\|_2\le 1$ . For positive integers $n_0, n_1$ and $n_2$ ( $n_1\ge n_2$ ), we define \begin{equation}
\begin{aligned}
    \Lambda_1=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_1]}\psi_{i,j}\right)^\top+\lambda I_1,\\
    \Lambda_2=\sum_{i\in[n_0]}\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)\left(\bigotimes_{j\in[n_2]}\psi_{i,j}\right)^\top+\lambda I_2,
\end{aligned}
\end{equation} where $\otimes$ is the Kronecker product, $I_1$ and $I_2$ are identity matrices and $\lambda\in\mathbb{R}_+$ . Let $\{\varphi_k\}_{k=1}^\infty$ be a sequence of vectors where $\varphi_k\in\mathbb{R}^{d_k}$ and $\|\varphi_k\|_2\ge 1$ , and we define \begin{equation}
    \phi_1=\bigotimes_{k\in[n_1]} \varphi_k,
    \quad
    \phi_2=\bigotimes_{k\in[n_2]} \varphi_k.
\end{equation} Then, show that the following holds \begin{equation}
    \phi_1^\top\Lambda_1^{-1}\phi_1\ge \phi_2^\top\Lambda_2^{-1}\phi_2.
\end{equation} I have verified it via programming, and no counterexample was found. Therefore, I believe it is probably true. However, I can only prove the case where $n_0=1$ . The main idea of my proof for $n_0=1$ is the following: Clearly, it suffices to prove that it holds when $n_1=2$ and $n_2=1$ . We diagonalize $\Lambda_1^{-1}$ and $\Lambda_2^{-1}$ . By diagonalization, we can translate $\phi_1^\top\Lambda_1^{-1}\phi_1$ and $\phi_2^\top\Lambda_2^{-1}\phi_2$ into a combination of eigenvalues, and finally obtain the desired inequality. I do not how to handle the case where $n_0>1$ since there are fundamental differences--we can not easily diagonalize the matrices. Hence, I am stuck... Any help or hint would be appreciated. Thanks in advance.","['symmetric-matrices', 'regression', 'matrices', 'linear-algebra', 'kronecker-product']"
4188800,Primes of the form $\frac{p^2+5}{6}$,"If $p$ is a prime greater than $3$ , then $q=\frac{p^2+5}{6}$ is an integer. If $q$ is prime too, we have $\;q\equiv1\mod4$ . The first primes so obtained (smaller than $10000$ ) are: $$5, 29, 61, 89, 229, 281, 1321, 2129, 2689, 2861, 3221, 3701, 4649, 6469, 8741, 9049, 9521$$ Apart from the first ( $5$ ), all such primes can be written as $$q=25a^2+b^2\;\;\;\;(q\gt5)$$ Is there any simple modular arithmetic argument that can be used to prove this property?","['elementary-number-theory', 'modular-arithmetic', 'prime-numbers', 'sequences-and-series']"
4188804,A little-o dilemma or the expectation of the KDE,"This question arose out of this answer on Cross Validated, but there is no need to click the link since all the necessary details will be summarized here. The level of probability theory and statistics involved in this question is very basic. It is about calculus if anything. This question assumes the following definition of the little-o if given a function $f(x)$ : $$ f\in o(x) \iff \lim_{x\to x_0} \frac{f(x)}{x} = 0,$$ where $x_0$ is a real number, a complex number or $\pm \infty$ . Background Suppose $x_1, ..., x_n$ are independent and identically distributed observations of a random variable $X$ with unknown distribution function $F$ and probability density function $f\in C^m$ , for some $m>1$ fixed. Let $k\in C^{m+1}$ be a given fixed function such that \begin{align}
k&\geq 0, \\
\mathrm{supp} (k)&=[-1,1], \\
\int_{\mathbb{R}} k(u)\mathrm{d}u&=1, \\
\int_{\mathbb{R}} k(u)u^l\mathrm{d}u&=0 \ \text{for all} \ 1\leq l<m \ \text{and}\\
\int_{\mathbb{R}} k(u)u^m\mathrm{d}u&<\infty .
\end{align} Define the so-called kernel density estimator (KDE) $f_n$ of $f$ by $$f_n(t)=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}k\left(\frac{t-x_i}{h}\right),$$ where $h=h(n)$ is the bandwidth. What is the expectation of $f_n$ , i.e. $\mathbb{E}[f_n(t)]$ ?. By linearity of the expectation, identical distribution of $x_1,...,x_n$ , the law of the unconscious statistician and the change of variables $u=(t-x)/h$ , \begin{align}
\mathbb{E}[f_n(t)]&=\frac{1}{n}\sum_{i=1}^n \mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x_i}{h}\right)\right]\\
&=\mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x}{h}\right)\right]\\
&=\int_{\mathbb{R}}\frac{1}{h}k\left(\frac{t-x}{h}\right)f(x)\mathrm{d}x\\
&=\int_{\mathbb{R}}\frac{1}{h}k(u)f(t-hu)h\mathrm{d}u\\
&=\int_{\mathbb{R}}k(u)f(t-hu)\mathrm{d}u. \tag{1} 
\end{align} From $f\in C^m$ , it follows that $$f(t-hu)=\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m).$$ Then from $(1)$ and linearity of integration, \begin{align}
\mathbb{E}[f_n(t)]&=\int_{\mathbb{R}}k(u)\left(\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m)\right)\mathrm{d}u \\
&=\sum_{l=0}^m\int_{\mathbb{R}}k(u)\frac{f^{(l)}(t)(-hu)^l}{l!}\mathrm{d}u+\int_{\mathbb{R}}k(u)o((hu)^m)\mathrm{d}u. \tag{2}
\end{align} From the given conditions on $k$ , the $l=0$ term reads $$\int_{\mathbb{R}} k(u)f(t)\mathrm{d}u=f(t)\int_{\mathbb{R}} k(u) \mathrm{d}u=f(t).$$ The $1\leq l<m$ terms are $$\int_{\mathbb{R}} k(u)\frac{f^{(l)}(t)}{l!} (-hu)^l\mathrm{d}u=\frac{f^{(l)}(t)(-h)^l}{l!}\int_{\mathbb{R}} k(u)u^l\mathrm{d}u=0.$$ Finally, the $l=m$ term is $$ \frac{f^{(m)}(t)(-h)^m}{m!}\int_{\mathbb{R}} k(u)u^m\mathrm{d}u<\infty.$$ According to the above linked answer, it holds that $o((hu)^m) = u^m o(h^m)$ and thus the remainder term in $(2)$ is \begin{equation}
\int_\mathbb{R} k(u) o((hu)^m)\mathrm{d}u = o(h^m)\int_\mathbb{R} k(u) u^m\mathrm{d}u = o(h^m).
\end{equation} Question Why does $o((hu)^m) = u^m o(h^m)$ hold? According to the Taylor expansion and the given definition of little-o, $o((hu)^m)$ means all functions $f$ that satisfy $\lim_{hu\to 0} \frac{f(hu)}{(hu)^m} = 0$ . One can pull out factors from the little-o, i.e. $o((hu)^m)=huo((hu)^{m-1})$ , but $o((hu)^m) = u^m o(h^m)$ suggests that the variable which the limit in the definition of little-o is taken with respect to has changed.","['statistics', 'asymptotics', 'calculus', 'limits', 'probability']"
4188864,Where are the imaginary components in a moment generating function (MGF) of a distribution?,"An MGF is $$M_X(t)=E(e^{tX})=\int_{-\infty}^\infty e^{tx}f_X(x) dx$$ whereas a Laplace transform is $$\mathcal L\{f_X(x)\}(s)= \int_0^\infty e^{-sx}f_X(x)dx$$ I am not referring to the sign difference, or the limits of integration. My question is about $s\in \mathbb C$ being a complex number in the Laplace transform, whereas I only see real numbers in the idea of the MGF (i.e. series expansion of $e^{tx}$ ). Indeed, $M_X:\mathbb R \to [0,\infty]$ with the domain of $M_X$ defined as the set $D_X=\{t \mid M_X(t)<\infty\}.$ In characteristic functions complex numbers are introduced, but they are Fourier transforms, not Laplace. Obviously nobody really cares, but I don't understand why: the LT captures oscillations and exponential decay (in standard engineering uses), whereas the FT only captures sinusoidal components as in this presentation : The imaginary numbers do surface, but in the characteristic function of a distribution. THE QUESTION: So is there really an imaginary component in the MGF (like in the Laplace), and if there is not, why is it taught that the MGF is the Laplace transform of the pdf? My intuition is that in probability the Laplace transform with domain in the complex numbers has been somewhat limited to $t\in \mathbb R,$ corresponding to a segment of the real line around zero in which the $E[e^{tX}]$ for different reasons: To be able to extract moments through the Taylor series. To classify distributions in relation to the exponential distribution, which is very useful in extreme value theory - analysis of the tails. This is best exemplified in the Chernoff inequality: $$\Pr(X \geq a) = \Pr\left(e^{tX} \geq e^{ta}\right)\leq \frac{E[e^{tX}]}{e^{ta}}= \frac{M_X(t)}{e^{ta}}=e^{-ta}M_X(t)$$ which indicates that a finite MGF will result in exponentially tapering tails. The third reason (to uniquely determine the distribution) would not really be a reason to keep the MGF: The characteristic function does have this role with the additional advantage that it exists for all distributions. So it is as if the Laplace transform was somehow limited to the real numbers (needless to say the usual random variables are real, but the domain of the MGF is not the random variable), so as to capture the exponential nature of the distribution, while the characteristic function deals with the complex part.","['moment-generating-functions', 'probability-distributions', 'laplace-transform', 'probability']"
4188906,Alternative proofs of convergence of geometric series,"The usual proof for the convergence of a geometric series of ratio $C: |C|\in [0,1)$ makes use of the formula $$\sum_{0\leq k \leq n} C^k = \frac{1-C^{n+1}}{1-C}.$$ I'm looking for alternative ways to prove it. The motivation for this is that, if someone who never saw this formula tried to prove the geometric series converges might have a hard time, unless maybe there are other, perhaps more insightful ways to prove it.","['calculus', 'soft-question', 'geometric-series', 'sequences-and-series']"
4188909,Let $f:A \to B$ and $g:C\to D$. Prove that $f \times g$ is a function from $A \times C$ to $B \times D$.,"Let $f:A \to B$ and $g:C \to D$ . Define $$f \times g = \{((a,c),(b,d)) : (a,b) \in f\text{ and }(c,d) \in g\}.$$ Prove that $f \times g$ is a function from $A \times C$ to $B \times D$ . My proof (so far): Let $(a,c) \in A \times C$ , then $a \in A$ and $c \in C$ . Let $(b,d) \in B \times D$ , $b \in B$ and $d \in D$ . If $p \in A \times C$ then $p=(a,c)$ . Define $(f \times g)(p)$ as $(f \times g)(p) = (f \times g)(a,c) = (f(a),g(c)) = (b,d)$ , however $b \in B$ and $d \in D$ and $(b,d)\in B \times D$ . This shows $f \times g$ is a function from $A \times C$ to $B \times D$ as $(f \times g) : A \times C\to B \times D$ then $(a,b) \to (f(a),g(c))$ . I know that it is not well written out, but I was wondering if my thought process so far made sense or if I accidentally missed something. Additionally, I am unsure of what the next step may be. As a sidenote, I am worried that this does not work for the given definition of $f \times g$ .","['solution-verification', 'discrete-mathematics']"
4188929,How to show that $\lim_{n\to \infty} \frac{a_1 +a_2 + \cdots + a_n}{n} = 0?$ [duplicate],"This question already has answers here : If $\sum_{n\geq 1}\frac{a_n}{n}$ converges, then $\lim_{n\rightarrow \infty}\frac{1}{n}\sum_{k=1}^na_k=0$ (2 answers) Closed 2 years ago . We are given that $\displaystyle\sum_{k=1}^\infty \frac{a_k}{k}$ converges, and we want to show that $$\lim_{n\to \infty} \frac{a_1 +a_2 + \cdots + a_n}{n} = 0.$$ Let $\epsilon>0.$ Then since $\displaystyle\sum_{k=1}^\infty \frac{a_k}{k}$ converges we have that there is some positive integer $N_\epsilon$ such that $\left|\sum_{k=N_\epsilon}^\infty \dfrac{a_k}{k}\right|<\dfrac{\epsilon}2$ . Let $\sum_{k=1}^{N_\epsilon} a_k =A$ and choose $N_1$ so that $\left|\frac{A}{n}\right|<\frac{\epsilon}{2}$ when $n\geq N_1$ . Now let $n\geq \max \left\{N_1, N_\epsilon \right\}$ so that $$\left|\frac{a_1 +a_2 + \cdots + a_n}{n}\right|\leq\left|\frac{A}{n}\right|+\left|\frac{\sum_{k=N_\epsilon+1}^n a_k}{n}\right|<\left|\frac{A}{n}\right|+ \left|\sum_{k=N_\epsilon+1}^\infty \frac{a_k}{k}\right|<\frac{\epsilon}{2}+\frac{\epsilon}{2} = \epsilon.$$ Again, I do not feel convinced of my own argument. Am I going the wrong direction? Is there perhaps a better route to showing my conclusion? Any help is appreciated. EDIT: Don’t know why this question got closed. I’m not solely wondering how to prove this statement, I’m also wondering if my proof is sufficient.","['real-analysis', 'alternative-proof', 'solution-verification', 'sequences-and-series', 'limits']"
4188933,Do a negative Gaussian curvature help with the stability of a surface?,"I found in this magazine the statement ""[...] ruled surfaces are statically efficient, especially in the case of skewed ruled surfaces, which are very stable due to a generally negative Gaussian curvature"". Why would a negative Gaussian curvature imply (or help with) the stability of a surface? I did not find a mathematical argument for the claim in the magazine, nor any mention of the same fact elsewhere. Is it really true? I'm interested in this question because I'm studying ruled surfaces (which have non-positive Gaussian curvature) and their applications in architecture and product design in general, and if such surfaces have strength or stability advantages, I'd be very interested to know.","['physics', 'differential-geometry']"
4188943,Exterior Derivative of One-Form vs Torsion of Connection,"Let $\omega$ be a $1$ -form. Then $d\omega$ may be defined by the formula $$
d\omega(X,Y) = \frac{\partial}{\partial X}\iota_Y\omega - \frac{\partial}{\partial Y}\iota_X\omega-\omega([X,Y])
$$ where $X,Y$ are vector fields. This formula bears a resemblance to the formula for the torsion of a connection $\nabla$ : $$
\nabla_XY - \nabla_YX - [X,Y]
$$ Is there a geometric explanation for this resemblance?","['connections', 'differential-topology', 'differential-forms', 'differential-geometry']"
4188954,What is the formal term referred to here in this introductory chapter on differential geometry?,"For context, this is an old MEI A-Level textbook (UK), titled ""Further Pure 3"" i.e. the most challenging textbook for the further math A-level that there was. The article referred to I have not been able to find for free, and I would dearly like to know what they are alluding to when they say ""reasonably smooth"" curves and what the mysterious exact definition is!","['terminology', 'differential-geometry']"
4189010,Exercise 7 page 93 Functional Analysis book of Conway,"The following is Exercise 7 page 93 in Functional Analysis book of Conway: Let $1 \le p \le \infty$ and suppose $(a_{ij})$ is a matrix such that $(Af)(i) = \sum_{j=1}^{\infty} a_{ij} f(j)$ defines an element $Af$ of $\ell^p$ for every $f$ in $\ell^p$ . Show that $A \in \mathcal{B}(\ell^p)$ . Case $p=\infty$ : If $f(j)$ is such that $||f(j)||_{\infty} < \infty$ and $\sum_{j=1}^{\infty} a_{ij} f(j) < \infty $ how to show that $(Af)(i)$ is bounded so that A maps $\ell^{\infty}$ to $\ell^{\infty}$ ? Case $1 \le p < \infty$ : For this case same approach I think but this time we use Holder's inequality I suppose? NOTE : I looking for a solution based on the Uniform Boundedness Principle, a solution without using the Closed Graph Theorem. The answer in here has just one sentence mentioning that the result could be proved using Baire Category Theorem, but such mention is not really an answer. The answer itself is based on applying the Closed Graph Theorem.","['lp-spaces', 'functional-analysis']"
4189028,A Gap in the Proof of the Duffin-Schaeffer's theorem,"Let $\varphi$ be the Euler totient function and $f$ be a nonnegative function such that $$\sum_{n=1}^{\infty} \frac{f(n)\varphi(n)}{n}=\infty,$$ and that $$\limsup_{N\to \infty} \frac{\sum_{n=1}^N f(n) \varphi(n)/n}{\sum_{n=1}^N f(n)} >0.$$ Let $g(n)=\min(f(n),\frac{n}{6\varphi(n)})$ (the digit $6$ is not crucial for this question), then do we have $$\limsup_{N\to \infty} \frac{\sum_{n=1}^N g(n) \varphi(n)/n}{\sum_{n=1}^N g(n)} >0?$$ I know an estimate $\sum_{n=1}^N \frac{n}{\varphi(n)} = \frac{315\zeta(3)}{2\pi^4} N + O(x^{\epsilon}), \forall \epsilon>0$ , which might be helpful here (this estimate is not optimal, though). For those who are curious about the source of this question. This is taken from Harman's Metric Number Theory, page 37. What I asked is a gap in Harman's proof of Duffin-Schaeffer's theorem in 1941.","['analytic-number-theory', 'number-theory', 'diophantine-approximation']"
4189038,Help with$\int _0^{\infty }\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x$,"I would appreciate any help with for proving the following problem $$\int _0^{\infty }\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x=\frac{15}{32}\zeta(4)-\frac{3}{8}\ln ^2\left(2\right)\zeta(2)$$ I found this problem on a mathematics group. I have tried 2 things, using $x=\frac{1}{t}$ $$\int _0^{\infty }\frac{\left(1-t^2\right)\left(\frac{\pi }{2}-\arctan \left(t\right)\right)^2\coth \left(t^2\right)}{t\left(1+t^2\right)}\:\mathrm{d}x$$ $$=-\frac{\pi ^2}{8}\int _0^{\infty }\frac{\left(1-t^2\right)\ln \left(\frac{t^2-1}{1+t^2}\right)}{t\left(1+t^2\right)}\:\mathrm{d}t+\frac{\pi }{2}\int _0^{\infty }\frac{\left(1-t^2\right)\arctan \left(t\right)\ln \left(\frac{t^2-1}{1+t^2}\right)}{t\left(1+t^2\right)}\:\mathrm{d}t-\frac{1}{2}\int _0^{\infty }\frac{\left(1-t^2\right)\arctan ^2\left(t\right)\ln \left(\frac{t^2-1}{1+t^2}\right)}{t\left(1+t^2\right)}\:\mathrm{d}t$$ And I think I made the problem harder, the other thing I tried was $$\int _0^{\infty }\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x$$ $$=\int _0^1\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x+\int _1^{\infty }\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x$$ $$=\int _0^1\frac{\left(x^2-1\right)\arctan ^2\left(x\right)\operatorname{arctanh} \left(x^2\right)}{x\left(1+x^2\right)}\:\mathrm{d}x+\int _0^1\frac{\left(1-t^2\right)\left(\frac{\pi }{2}-\arctan \left(t\right)\right)^2\operatorname{coth} \left(t^2\right)}{t\left(1+t^2\right)}\:\mathrm{d}t$$ And I got stuck again.","['integration', 'improper-integrals', 'definite-integrals']"
4189108,Difference between my age and my grandfather age,"In $1952$ , I was as old as the number formed by last two digits of my
birth year. When I mentioned this to my grandfather he surprised me by
saying that the same applied to him also. The difference in our ages
is ? Now the way it has been solved is that let us assume that I was born in the year $19xy$ and my grandfather was born in the year $18pq$ . Now as per the question :- $19xy+xy=1952$ where $xy$ is my current age. $\Rightarrow 1900 + xy + xy = 1952$ $\Rightarrow xy = 26$ $18pq+pq=1952$ where $pq$ is my grandfather's current age. $\Rightarrow 1800 + pq + pq = 1952$ $\Rightarrow pq = 76$ Difference between the ages = $76 - 26 = 50.$ Now you all can see that this solution applied little logic with the assumption of my age and my grandfather's age. But what if the year given would have been $1989$ or $1999$ then we can't just assume that I would have been born in some $19xy$ and my grandfather would have been born in some $18xy$ . It can very well happen that both of us would have been born in the same century. I tried solving this by assuming that I was born in the year $abcd$ and my grandfather was born in the year $pqrs$ and following the same approach but it didn't helped me and then I turned to this solution that I have put up above. So can anyone help me with this? Is it not possible to solve this problem by assuming years like I have or is it like we have to use some logic while assuming the birth years as the solution has done? Thanks in advance !!!","['contest-math', 'algebra-precalculus', 'solution-verification']"
4189140,Paracompactness of topological group.,"If $G$ be a locally compact topological group. Show that $G$ is paracompact. Note: If we restrict $G$ to be locally compact, connected topological group, this problem becomes easier by constructing a sequence of sets $U_{n+1}=\bar{U}_{n}.U_1$ where $U_1$ is a neighborhood of $e$ having compact closure. However, I feel that in this proof I still have not used the full strength of local compactness (only need it to know the existence of $U_1$ ). In Munkres' book, he stated that if we remove the condition of connectedness, the theorem still remains true. But I have not figured it out yet. Does anyone have any idea?","['general-topology', 'locally-compact-groups', 'topological-groups', 'paracompactness']"
4189149,Geometric Intuition for Connection Map,Let $ V \to E \xrightarrow{\pi} M$ be a vector bundle with connection $\nabla$ . Denote by $K \colon TE \to E$ the corresponding connection map. What geometric intuition is there for this map? Is there an explicit formula for the connection map of the Levi-Civita connection?,"['connections', 'vector-bundles', 'riemannian-geometry', 'differential-geometry']"
4189247,Geometric Intuition behind the genus of complex torus,"This question may be really dumb, but my geometric intuition is really bad.... Fix $w_{1},w_{2}\in\mathbb{C}$ two complex numbers that are linearly independent over $\mathbb{R}$ , and consider the Lattice $$L:=\mathbb{Z}w_{1}+\mathbb{Z}w_{2}=\{m_{1}w_{1}+m_{2}w_{2}:m_{1},m_{2}\in\mathbb{Z}\}.$$ The quotient group/space $X:=\mathbb{C}/L$ is a compact Riemann surface, and is called a complex torus. For $z\in\mathbb{C}$ , consider the closed Parallelogram $$P_{z}:=\{z+\lambda_{1}w_{1}+\lambda_{2}w_{2}:\lambda_{i}\in [0,1]\}.$$ We also have the canonical projection $\pi:\mathbb{C}\longrightarrow\mathbb{C}/L$ that sends each element $z\in\mathbb{C}$ to the coset $z+L$ . I have proved that $\pi$ maps $P_{z}$ onto $X$ , for any $z\in\mathbb{C}$ . The book ""Algebraic Curves and Riemann Surfaces'' by Rick Miranda says the following: In fact, $X$ has topological genus one, topologically is a simple torus. This is most easily seen by consider $X$ as the image of the parallelogram $P_{0}$ under the map $\pi|_{P_{0}}$ , the opposite sides are identified together, and no other identifications are made, giving the familiar construction of the torus. I know how to prove that $X$ has genus one, because it is fairly easy to construct a homeomorphism from $X$ to $\mathbb{S}^{1}\times\mathbb{S}^{1}$ . However, I am not understanding the gluing thing mentioned by Miranda above. So, we have that $P_{0}=\{\lambda_{1}w_{1}+\lambda_{2}w_{2}:\lambda_{1,2}\in [0,1]\}$ , and thus the quotient map mods out the case when $\lambda_{1},\lambda_{2}=0$ or $1$ . Does this mean that we actually only identify four points on this parallelogram, namely, $0,w_{1},w_{2}$ and $w_{1}+w_{2}$ ? instead of identifying all the points on opposite sides? I am sorry if the answer to my equation is easily obvious and I am not seeing it...","['riemann-surfaces', 'general-topology']"
4189283,What is the series of identities for these hyper-operations?,"Suppose we define the hyper-operations iteratively off the previous hyper-operation. We begin with the zeroth hyperoperation being addition, $$a\circ_0b:=a+b.$$ We then assert that $$a\circ_{n+1} b:=\exp(\ln(a)\circ_n\ln(b)).$$ Although not immediately obvious, $a\circ_1 b\equiv a\times b$ . So $\circ_1$ is equivalent to multiplication. But $a\circ_2 b\neq a^b$ , but instead reduces to $a^{\ln(b)}$ . Thus $\circ_2$ is not the traditional exponentiation (instead, the so-called ""powerlog"" in this video ). Doing so allows each hyper-operation to be associative and communitive over it's previous operation, which are nice properties that regular exponentiation does not have. My question is: What is the series of identities, $e_n$ , such that $a\circ_n e_n \equiv a$ , and what is the limit of $\lim_{n\to\infty}e_n$ ? We know that $e_0=0$ , as $a\circ_00\equiv a$ , since $a\circ_0b:=a+b$ . We also know that $e_1=1$ , as $a\circ_11\equiv a$ , since $a\circ_1b:=a\times b$ I can also deduce that $e_2=e$ , as $a\circ_2 e\equiv a$ , since $a\circ_2b:=a^{\ln b}$ . But I would like to know if there is a general formula for $e_n$ , and what the limit is (if any). Kind regards to any insight.","['limits', 'hyperoperation']"
4189290,Doubt regarding the proof of Prime Number Theorem,"I am studying the proof of the Prime Number Theorem from Introduction to Analytic Number Theory by Tom M. Apostol and I came across this result: If $c>1$ and $x \geq 1$ , we have: $$\frac{\psi_1(x)}{x^2} = \frac{1}{2\pi i}\int_{c-\infty i}^{c+\infty i} \frac{x^{s-1}}{s(s+1)}\left(-\frac{\zeta'(s)}{\zeta(s)}\right)ds$$ Here, $\psi_1(x) = \int_1^x \psi(t)dt$ and $\psi(t)$ is the Chebyshev $\psi$ function. I understood its proof but I am having trouble understanding the motivation behind the next result. The book says : The quotient $-\frac{\zeta'(s)}{\zeta(s)}$ has a first order pole at $s=1$ with residue 1. So, if we subtract this pole we get the formula $$\frac{\psi_1(x)}{x^2} - \frac{1}{2}\left(1-\frac{1}{x}\right)^2 = \frac{1}{2\pi i}\int_{c-\infty i}^{c+\infty i} \frac{x^{s-1}}{s(s+1)}\left(-\frac{\zeta'(s)}{\zeta(s)}-\frac{1}{s-1}\right)ds$$ Specifically, I am not able to see why we need to subtract the pole. Clearly we have $c>1$ , so the limits of the integral have $Re(s)>1$ . Hence the term $-\frac{\zeta'(s)}{\zeta(s)}$ in the integrand would never take the input $s=1$ . So, why do we bother subtracting the pole ? This might seem like a silly question but I am completely confused. Any help shall be highly appreciated.","['complex-analysis', 'number-theory', 'riemann-zeta', 'analytic-number-theory']"
4189361,Is the following generalization of Cauchy-Schwarz inequality true?,"Let $\{a_{1,i}\}_{i=1}^k,\{a_{2,i}\}_{i=1}^k,\dots ,\{a_{n_,i}\}_{i=1}^k$ be real sequences. Does the following inequality hold $$(\sum_{i=1}^k a_{1,i}^2)\cdot(\sum_{i=1}^k a_{2,i}^2)\cdots(\sum_{i=1}^k a_{n,i}^2)\geq (\sum_{i=1}^k a_{1,i}a_{2,i}\cdots a_{n,i})^2$$ for all $k,n \in \mathbb N$ ? It can be easily seen that this is the Cauchy-Schwarz inequality when $n=2$ . The motivation for the problem actually comes from the Cauchy-Schwarz inequality. While solving a Cauchy-Schwarz inequality problem, this problem came to my mind. I don't know if this is already a proved theorem in mathematics (because I am a high school student and I don't know much about inequalities). But I didn't find this on internet (I searched on google). So, I assume the problem statement is false. And a proof (or disproof) is needed for that. My workings for $k=2$ and $n=3$ : However, I tried to prove the problem statement for $k=2$ and $n=3$ (and I think I actually proved that!). Here is my workings to do that: For $a,b,c,d,e,f$ real numbers, we have from Cauchy-Schwarz inequality (which is for $n=2$ and $k=2$ ), $$(a^2+b^2)(c^2+d^2) \geq (ac+bd)^2$$ $$\implies (a^2+b^2)(c^2+d^2)(e^2+f^2) \geq (ac+bd)^2(e^2+f^2)$$ $$=(a^2c^2+2abcd+b^2d^2)(e^2+f^2)$$ $$=a^2c^2(e^2+f^2)+2abcd(e^2+f^2)+b^2d^2(e^2+f^2)$$ $$\geq a^2c^2e^2+2abcdef+b^2d^2f^2$$ $$=(ace+bdf)^2$$ as desired. I hope my workings are correct. So, I have the following questions: Is the firstly stated problem statement true? If it is, how to prove that? If it is not true, are there some other values (like $k=2$ and $n=3$ as in the above) for which the statement is true? Any help would be appreciated and please try to answer the questions so that a high school student can understand them (if it is not possible, then no problem).","['inequality', 'proof-writing', 'cauchy-schwarz-inequality', 'solution-verification', 'algebra-precalculus']"
4189372,Invariant subspace of $T$ (normal) is also an invariant subspace of $T^\ast$.,"I am struggling with the following question Let $V$ be a finite dimensional vector space and $T:V\rightarrow V$ be a linear normal operator ( $T^\ast T = TT^\ast$ ), and $W$ an invariant subspace of $T$ ( $T(W)\subseteq W)$ . Prove $W$ is also an invariant subspace of $T^\ast$ . The problem I have is characterizing something like $T^\ast w \in W$ when all that is given is in the language of inner products. I thought of maybe decomposing $T^\ast w = u+v$ where $u \in W,\ v\in W^\perp$ , and showing $v=0$ by $\left <T^\ast w, v \right >=0$ , but $T$ being normal does not help when there is ""only one $T$ "" inside the inner product. Is multiplying both sides by $T$ any help? because then we can use normality but I don't know where it leads us. I know this is true since using the unitary diagonalization, I can express $T^\ast$ as a polynomial in $T$ and from there it's easy ( $W$ is $p(T)$ invariant regardless of the polynomial itself), but I would like to see a more fundamental solution.","['inner-products', 'invariant-subspace', 'linear-algebra', 'normal-operator']"
4189440,Why first homology group of torus is isomorphic (as abelian group) to lattice?,"Let $T＝\Bbb C/Λ$ be a torus( $Λ$ is lattice).
I heard first homology group $H_1(\Bbb C/Λ,\Bbb Z)$ is isom as abelian group to $Λ$ via the map $r→\oint\mathrm dz$ (we integrate across the cycle $r$ ). Could you tell me the proof of the fact that 'this map is well-defined and surjective and injective'?
(I could only understand this map is group hom )","['riemann-surfaces', 'general-topology', 'homology-cohomology', 'elliptic-curves']"
4189444,Solving for $s$ in $E[(1-h)^s]$ in terms of moments of $h$,"Suppose $h$ is a discrete random variable with probability of seeing $h_i$ proportional to $h_i$ , and moments $M_k=E[h^k]$ exist for $k=1,2,\ldots$ . Is it possible to get bounds on $s$ where the following holds, in terms of $M_k$ ? $$E[(1-h)^s]<\epsilon$$ edit This problem comes up in average case analysis of gradient descent. $s$ gives the number of gradient descent steps needed to obtain average loss target $\epsilon$ when minimizing quadratic form with eigenvalues a multiple of $h_1,h_2,\ldots,$ . Algorithms exist for computing $M_k$ in in practice, for small values of $k$ , algorithm for $s$ is missing. A different approach which doesn't rely on $M_k$ but makes strong assumptions on form of $h$ is given here","['moment-generating-functions', 'probability-theory', 'sequences-and-series']"
4189468,"Finding a polynomial $P\in\Bbb R[x]$ such that $|\sqrt[3]x-P(x)|\le\frac1{100}, \forall x\in[0,1]$","A very similar question has already been asked before, but here the cubic root appears: Find a polynomial $P\in\Bbb R[x]$ such that $|\sqrt[3]x-P(x)|\le\frac1{100},\forall x\in[0,1]$ From what I understand, $P$ should be a Taylor polynomial $T_n$ (which, I know, is the $n^{\mathrm{th}}$ partial sum of the corresponding Taylor series) around $c\in[0,1]$ of the function $f(x)=\sqrt[3]x$ of a suitable degree and $\sqrt[3]x-P(x)=R_n(x)=\frac{f^{(n+1)}(c_x)}{(n+1)!}(x-c)^{n+1}$ for some $c_x$ between $c$ and $x$ .
Since there is $1/3$ in the exponent, I didn't try applying the Stirling's approximation. Instead, I have the Taylor expansion of $\sqrt[3]x$ around $x=1$ : $$\begin{aligned}\sqrt[3]x&=\sqrt[3]{1+(x-1)}\\&=\sum_{n=0}^\infty\binom{1/3}n(x-1)^n\\&=\sum_{n=0}^\infty\frac{\frac13\left(\frac13-1\right)\left(\frac13-2\right)\cdots\left(\frac13-(n-1)\right)}{3^nn!}(x-1)^n\\&=\sum_{n=0}^\infty\frac{(-1)^n(3n-4)!!!}{(3n)!!!}(x-1)^n,\end{aligned}$$ however, I'm not sure how to proceed.
I also considered Lagrange interpolation polynomial, but $\sqrt[3] x$ has an infinite slope at $x=0$ . I also tried using the hint from the thread, which is, the Taylor series of $\sqrt[3]{x+\varepsilon}$ around $x=1:$ $$\begin{aligned}\sqrt[3]{x+\varepsilon}&=\sqrt[3]{x-1+\varepsilon+1}\\&=\sqrt[3]{1+\varepsilon}\sqrt[3]{1+\frac{x-1}{1+\varepsilon}}\\&=\sqrt[3]{1+\varepsilon}\sum_{n=0}^\infty\frac{(-1)^n(3n-4)!!!}{(3n)!!!(1+\varepsilon)^n}(x-1)^n.\end{aligned}$$ I believe I'm either missing something or that I made a mistake. What is step should I take next? How to find the large enough $\deg(P)=n$ so that the approximation is correct up two $2$ decimal digits? EDIT: As advised in the comments, I developed $f(x)=\sqrt[3]x$ around $c=\frac12$ : $\begin{aligned}\sqrt[3]x&=\left(\frac12+\left(x-\frac12\right)\right)^\frac13\\&=\frac1{2^3}\left(1+2\left(x-\frac12\right)\right)^\frac13\\&=\sum_{n=0}^\infty\binom{1/3}n2^{n-1/3}\left(x-\frac12\right)^n,\end{aligned}$ but I'm still a bit confused as to how to determine which is the least degree of the Taylor polynomial that suits us. Thank you very much!","['calculus', 'derivatives', 'taylor-expansion', 'real-analysis']"
4189490,Difference between using max function over functions and over values,"I was wondering if someone could explain what is the difference between the max of a finite number of functions and the max of a finite number of function values. For instance, as shown here , given real-valued continuous functions $f,g$ , the following inequality holds. $\max(f+g)(z)\leq \max f(z)+\max g(z), \forall z \in Z$ . However, if I define the functions $ g= \max_{i \in I}\{g_i\}$ , $ h= \max_{j \in J}\{h_j\}$ , and let $f= g+h$ , then $f$ can be represented as $ f = \max_{i \in I,j \in J}\{g_i+h_j\}$ rather than writing as $f = \max_{i \in I}\{g_i\} + \max_{j \in J}\{h_j\} \leq   \max_{i \in I,j \in J}\{g_i+h_j\}$ . My question is why equality holds when we work with indices. What exactly am I misinterpreting?",['functions']
4189509,I'm trying to learn how to prove by induction,"Please, I need some help to prove this $$1+\sum_{i=1}^n 5^i = \frac{5^{n+1}-1}{4}$$ I got this but I'm not sure how to proceed Base n=1 $$=\frac{(5^2)-1}{4}$$ $$=\frac{25-1}{4}$$ $$=\frac{24}{4}$$ $$={6}$$ It works for n=1 Induction hypothesis Let n=k $$1+5+25+125+...+k=\frac{5^{k+1}-1}{4}$$ Induction step Let n=k+1 Prove $$1+5+25+125+...+k+(k+1)=\frac{5^{k+2}-1}{4}$$ By induction hypothesis $$1+5+25+125+...+k=\frac{5^{k+1}-1}{4}$$ And I tried this way using laws of exponents $$1+5+25+125+...+k+(k+1)=\frac{5^{k+1}-1}{4}+(k+1)$$ $$=\frac{5^{k+1}-1}{4}+\frac{4(k+1)}{4}$$ $$=\frac{5^k*5+4k+3}{4}$$ I think that I'm doing it wrong but  would be so grateful if you could help me EDIT: I just realized how bad my summation was, but now I understand the proof, ty all","['algebra-precalculus', 'solution-verification', 'induction']"
4189519,Infinitesimal generator in context of SDEs,"Consider the following SDE: $$X(t)= s + \int_{t_0}^t b(X(s)) dt + \int_{t_0}^t a(X(s)) dW(t) $$ where $X(t_0)=s$ and $a:\mathbb{R}^d \rightarrow \mathbb{R}^{d \times N} $ , $b:\mathbb{R}^d \rightarrow \mathbb{R}^d $ and $W$ denoting an $N$ dimensional Brownian Motion. The infinitesimal generator is defined then as $$L[u](x) = b(x) D_x u(x) + \frac{1}{2} tr[a(x)a(x)^T D^2_x u(x)]$$ I do not understand why in this formula the mixed second derivatives will not be considered, meaning something like $\partial_{x_i x_j}$ for $i \ne j $ An application of  Itos-formula on $du(X(t)))$ yields $$du(X(t))) = L[u](X(t))dt + D_x u(X(t)) a(x) dW(t)$$ Why do I not need the mixed second derivatives in this formula matching with the defintion of the infinitesimal generator?","['derivatives', 'stochastic-differential-equations', 'stochastic-calculus']"
4189550,"Is $\frac{x^3}{x^2+y^4}$ continuous at (0,0)?","In an exam I was asked to determine if a function $f:\mathbb R^2\to \mathbb R$ is continuous at a point $(x_0,y_0)$ , I needed to check if $$\lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0)$$ We consider the function given by $f(x,y)=\frac{x^3}{x^2+y^4}$ if $(x,y)\neq (0,0)$ and $f(0,0)=0$ . We want to know if $f$ is continuous at $(0,0)$ . What I did is the following : We have that for $(x,y) \neq (0,0)$ , $0\leq \frac{x^2}{x^2+y^4}\leq1$ and if $x>0$ $\implies 0\leq \frac{x^3}{x^2+y^4}\leq x$ .
And if $x<0$ , $x\leq \frac{x^3}{x^2+y^4}\leq 0$ Then I used the squeeze theorem and said something very weird and stupid : $$""\lim_{(x,y) \to (0^+,0)} f(x,y)=\lim_{(x,y) \to (0^-,0)}f(x,y)=0""$$ First of all we never spoke or defined multivariable limits like that with $0^+$ and $0^-$ , I am starting to realize when I am writing this that I could have been smarter and said that $-\lvert x \rvert \leq \frac{x^3}{x^2+y^4} \leq \lvert x \rvert$ and squeeze theorem to give me the continuity in $(0,0)$ . The second point is that WolframAlpha tells me that the limit does not exist even though I tried many different paths such as $(t,t), (t^2,0)...$ to see if the function was discontinuous. Who is right ?","['wolfram-alpha', 'real-analysis', 'continuity', 'multivariable-calculus', 'limits']"
4189560,Dimension free gram matrix inner product,"Let $\{x_i\}_{i = 1}^n$ be $n$ vectors of $d$ dimesnions.
We stack each $x_i$ as a row vector to form a matrix $X$ of dimension $\mathbb{R}^{n\times d}$ Let $\{y_i\}_{i = 1}^n$ be scalars (say all are $1$ ) and these are stacked to form a matrix $Y$ of dimension $\mathbb{R}^{n\times 1}$ We compute the following inner product $$Y^TX(X^TX+ \lambda I)^{-2}X^TY$$ where $\lambda \gt 0$ . Can we show that this seems that does not grow at an order of $n$ . Experimantally it appears so
For $d = 3$ and $n$ running from $1$ to $1000$ we have For $d = 10$ we have and for $d=100$ we have So it seems that for $n$ much greater than $d$ it seems that the hunch is correct. Any idea on how to proceed. Thanks EDIT 1: Here is a proof that $$Y^TX(X^TX+ \lambda I)^{-1}X^TY$$ is $O(n)$ Let $X = U\Sigma V^T$ Then $$Y^TX(X^TX+ \lambda I)^{-1}X^TY = Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY$$ . Now we can note that diagonal elements of $\Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T$ is less than 1. Therefore $$Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY \leq Y^T UU^T Y$$ which is less than $n\|Y\|^2_\infty$ My hunch is that $Y^TX(X^TX+ \lambda I)^{-2}X^TY$ will be less than $d\|Y\|^2_\infty$ . This is easy if $X^T X$ is diagonal. But is it true for general. EDIT 2: This is another line of thought I was having, $$Y^TX(X^TX+ \lambda I)^{-2}X^TY \leq n\|Y\|^2_\infty tr(XX^T) tr ((X^TX)^{-2})$$ Now $tr(XX^T) = O(n)$ . If I can say $tr ((X^TX + \lambda I)^{-2})$ is $O(n^{-2})$ Then I can be done. But is this true? EDIT 3: So this is a line of thought from the previous update
Let $\lambda_1,...\lambda_d$ be the eigenvalues of $X^TX + \lambda I$ .
Then the eigen values of $(X^TX + \lambda I)^{-2}$ are $\frac{1}{\lambda_1^2},...\frac{1}{\lambda_d^2}$ . So we we have the following, $$\lambda_1 + \cdots + \lambda_d = O(n)$$ , $\lambda_1,...\lambda_d$ are strictly positive.
So we can have $$\lambda_1^2 + \cdots + \lambda_d^2 = O(n^2)$$ . Can we write from this step $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = O(1/n^2)$$ ? EDIT 4: I can show that $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = \Omega(1/n^2)$$ $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} \geq \frac{d^2}{\lambda_1^2 + \cdots + \lambda_d^2}$$ by AM-HM inequality which is $\Omega(1/n^2)$ . So we have $trace((X^TX + \lambda I)^{-2})$ of the order $\Omega(1/n^2)$ . Therefore $$Y^TX(X^TX+ \lambda I)^{-2}X^TY = O(n^2)\Omega(1/n^2)$$ Are my calculations correct? Can we have anything from here?","['linear-regression', 'linear-algebra', 'functional-analysis', 'matrix-decomposition']"
4189572,"Given a general region, find the double integral bounded between $y = x$ and $y=3x-x^2 $","$$ J  = \iint_R (x^2-xy)\,dx \,dy, $$ Suppose region R is bounded between $y = x$ and $y=3x-x^2 $ My attempt using vertical integration: $$ \int^{x=2}_{x=0} \int^{y=3x-x^2}_{y=x} \left({x^2-xy}\right)dy\ dx$$ $$\int^2_0 \left[x^2y-x\frac{y^2}{2}\right]^{3x-x^2}_{x}\, dx$$ $$\int^2_0 \frac{-x^5+4x^4-4x^3}{2} \,dx $$ $$\boxed{J = -\frac{8}{15}}$$ My attempt using horizontal integration : $$ \int^{y=2}_{y=0} \int^{x=y}_{x=3\,\pm \sqrt{9-y}} \left({x^2-xy}\right)dx\ dy$$ For $ x = 3+\sqrt{9-y}$ $$  \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3+\sqrt{9-y} }\,dy$$ For $ x = 3-\sqrt{9-y}$ $$  \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3-\sqrt{9-y} }\,dy$$ My doubts : 1.) How do I set my limit of integration for horizontal integration, if there is $\pm$ to be considered ? 2.) the answer as negative what does that imply in questions related to double integrals? Could you guys please help","['iterated-integrals', 'multivariable-calculus', 'multiple-integral']"
4189600,Intermediate step in deriving integral representation of Euler–Mascheroni constant: $\int_0^1\frac{1-e^{-t}-e^{-1/t}}{t}dt$,"I'm following a complex analysis course and am making an exercise in which I have to derive an integral representation for the Euler–Mascheroni constant. I have the following definition of the Euler–Mascheroni constant: $$\gamma = \lim_{n\rightarrow\infty}\left(1 + 1/2 + +\ldots+ 1/n - \log n\right)$$ I have shown (with induction): $$1 + 1/2 + \ldots+ 1/n = \int_0^1\frac{1-(1-t)^n}{t}dt$$ I now have to show: $$\gamma = \lim_\limits{n\to\infty}\left(\int_0^1 (1-(1-t/n)^n)\frac{dt}{t} 
- \int_1^n(1-t/n)^n\frac{dt}{t}\right)$$ By using $e^t = \lim\limits_{n\to\infty}(1+t/n)^n$ and substituting $t$ for $1/t$ in the second integral, it then follows: $$\gamma = \int_0^1\frac{1-e^{-t}-e^{-1/t}}{t}dt$$ None of my ideas have been promising for the second step. Any suggestions?","['integration', 'complex-analysis', 'euler-mascheroni-constant']"
4189657,Kodaira-Thurston example,"I am trying to understand the Kodaira-Thurston example as it's done in the book ""Lectures on Symplectic Geometry"" by Ana Cannas. Let's take $\mathbb{R}^{4}$ , with local coordinates $\{x_1,x_2,y_1,y_2\}$ , and following sympletic form $\omega = dx_{1}\wedge dy_{1} + dx_{2}\wedge dy_{2}$ . Now one can consider the following discrete group $\Gamma$ which is generated by the following symplectomorphisms: \begin{align*}
    &\gamma_{1}: (x_1,x_2,y_1,y_2) \longrightarrow (x_1,x_2+1,y_1,y_2)\\
    &\gamma_{2}: (x_1,x_2,y_1,y_2)\longrightarrow (x_1,x_2,y_1,y_2+1)\\
    &\gamma_{3}: (x_1,x_2,y_1,y_2)\longrightarrow (x_1+1,x_2,y_1,y_2)\\
    &\gamma_{4}: (x_1,x_2,y_1,y_2)\longrightarrow (x_1,x_2,y_1+1,y_2)
\end{align*} and take $M=\mathbb{R}^4/\Gamma$ . Now we know that $\pi_1(M)\cong \Gamma$ , but I am having some trouble understanding why this will have rank $3$ .
Also I am not sure how one can define a symplectic structure on this manifold $M$ . Any help is appreciated, thanks in advance.","['complex-geometry', 'symplectic-geometry', 'differential-geometry']"
4189718,A question on the equivalent statements of countable sets in How to Prove it.,"In the textbook, it says that given any set $A$ , the following statements are equivalent: A is countable (the author defines a countable set as either being finite or being countably infinite). Either $ A $ is empty or there's a function $ f:\mathbb{Z}^+ \to A$ that is onto. It seems to me that the statement "" $ A $ is empty"" is redundant. If $A$ is empty, isn't the condition ""for every element a in $A$ , there exists an $n \in \mathbb{Z}^+$ with $f(n) = a$ "" (i.e. $f$ is onto) automatically satisfied?? Correct me if I'm wrong. Thanks.","['proof-explanation', 'calculus', 'functions', 'solution-verification']"
4189729,"Why is the number of elements in a group called ""order""?","This is a question that I have for a long time, Why is the number of elements in a group called ""order""? I mean, the word ""order"" in Spanish (which is my language) has a very strong meaning in terms of ""ordering"", but it does not refer to quantities. What was the motivation for this?","['group-theory', 'abstract-algebra', 'terminology']"
4189829,How do I check if the normal vector is pointing inside or outside?,"Lets say I've a sphere $x^2+y^2+z^2=1$ and I need to solve an integral $\iint_S\vec F\cdot\vec ndS$ . while $S$ is the sphere. And I can't use gauss law because $\vec F$ is not continuous at some point inside $S$ . I had $$\vec F=\left(\frac{x}{\sqrt{x^2+y^2+z^2}},\frac{y}{\sqrt{x^2+y^2+z^2}},\frac{z}{\sqrt{x^2+y^2+z^2}}\right).$$ So I went and tried to go for it normally using ball coordinates $$\vec r(\theta,\phi)=(\cos\theta \sin\phi, \sin\theta \sin\phi, \cos\phi).$$ And I found that $$r_{\theta}\times r_{\phi}=(-\cos\theta \sin^2\phi,-\sin\theta \sin^2\phi,-\sin\phi \cos\phi).$$ Now I know that this could be the normal pointing inside the ball or outside of it. In order to try to check, I tried to reach a point $(1,0,0)$ on it, and check if the "" $x$ "" part of the normal is positive or negative in that point. But I got lost trying to find $\phi,\theta$ which satisfy that, and wanted to know if there's any better way to decide in what direction the normal I found is pointing. Any feedback is appreciated, thanks in advance!","['integration', 'vector-fields', 'multivariable-calculus', 'vector-analysis']"
4189838,Finding the order of a 2x2 matrix mod n when n is NOT prime,"Given a $2\times 2$ matrix $A\in M_{2\times 2}(\mathbb Z)$ , if $\det(A)$ is relatively prime to $n$ , then we can find a $k$ such that $A^k\equiv I\pmod n$ . Let us denote the smallest positive $k$ with this property by $\operatorname{ord}_n(A)$ .  I wish to understand how this changes for a fixed $A$ and variable $n$ .  By the CRT, If $\gcd(m,n)=1$ , $\operatorname{ord}_{mn}(A)=\operatorname{lcm}(\operatorname{ord}_m(A),\operatorname{ord}_n(A))$ .  This reduces the problem to understanding $\operatorname{ord}_q(A)$ where $q=p^k$ is a prime power. One can show $\operatorname{ord}_p(A)$ divides either $p^2-1$ or $p^2-p$ . Further, if you know $A^m=I \pmod{p^k}$ , then $A^m=I+p^k B \pmod{p^{k+1}}$ for some (potentially zero) matrix $B$ , and then $A^{pm}=I \pmod{p^{k+1}}$ , so either $\operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A)$ or $\operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A)$ . Are there any other ways one can zero in on this problem in general? Are there good conditions for when $\operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A)$ vs $\operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A)$ ? My interest is in a specific matrix $$A=\begin{pmatrix}1 & 7 \\ 1 & 1\end{pmatrix}$$ which is particularly nice, so there may be a more specialized or ad hoc approach here, which I would be interested in as well,","['matrices', 'elementary-number-theory', 'group-theory', 'reference-request']"
4189862,Cohomology of projective space using a spectral sequence,"This is exercise 18.3.D of Vakil's Foundations of Algebraic Geometry. It's to prove that $H^i(\mathbb{P}^n_A, O(m))$ are free, and to compute the dimensions, where $A$ is any commutative ring. The approach used is to use the Cech cohomology using the open cover $D(x_0), ... D(x_n)$ of $\mathbb{P}^n_A$ . First, we consider all of the invertible sheaves at once. We consider $F = \oplus_{m \in \mathbb{Z}} O(m)$ and take the cohomology of this, using the fact that cohomology commutes with direct sum. For $I \subseteq \{0, ... n\}$ , define $U_I$ to be $\cap_{i \in I} D(x_i)$ . The next step is to prove that $\Gamma(U_I, F)$ is the Laurent polynomials $A[x_0, ... x_n, \{x_i^{-1} : i \in I \}]$ . I understand this part. Then, we look at how many exponents are allowed to be negative. There are ""3 negative exponents"" , ""2 negative exponents"", ""1 negative exponent"", and ""0 negative exponents"" cases, where in each case, the first few exponents are required to be negative. Vakil shows that these sequences are exact except for the ""3 negative exponents"" case, where there is non-exactness at one part. I also understand this part, but I'm not sure as to why it's relevant. I think that the approach that Vakil is taking is that he's using the ""n negative exponents"" to give a filtration on the Cech complex, so we can compute the cohomology using a spectral sequence.    The part of the proof that shows various sequences are exact shows that the first page is mostly zeroes. I'm having trouble with writing down what this filtration exactly is. The subquotients must correspond to Laurent polynomials where the first few exponents are required to be negative. Since we're trying to compute cohomology of projective space, I also want this spectral sequence to converge to the $H^i(\mathbb{P}^n_A, O(m))$ . How do I do this?","['spectral-sequences', 'algebraic-geometry', 'homology-cohomology']"
4189867,Obtaining irrational probabilities,"Let me start with a story. Our mathematics teacher asked us this question: Suppose I give you two balls, one black and the other white, then can you give me the white ball with $1/2$ probability? The answer was easy, we just toss a fair coin and if it lands Heads , we give the black ball, else we give the white one. Then, we were asked a second question: Suppose I give you two balls, one black and the other white, then can you give me the white ball with any fractional probability that I tell you? The probability can be like $2/3$ or $7/10$ or $12/100$ ? We can answer this question by making {denominator} number of equal pieces of paper and writing White on {numerator} number of pieces and Black on the remaining ones, and then mix all the papers together and take a piece of paper randomly from them. For example, if we want to give the White ball with a probability of $7/10$ , we make 10 paper pieces and write White on 7 of them and Black on the remaining three. Now we randomly pick up a piece of paper and then give the ball which has the colour same as that of written on the paper. Now, I have another question: If I want to have the white ball with a (well-defined) irrational probability (like $1/\sqrt2$ , $\sqrt{12}/\sqrt{33}$ or $1/\pi$ ), what should be the answer? By well-defined, I mean that the number should be obtainable by fairly common mathematical methods and not man-made irrational numbers like $0.1234567891011121314151617181920...$ , though, if any technique can obtain such a number, then better.","['irrational-numbers', 'probability']"
4189883,How to simplify the difference between hard thresholded points that are very close?,"A hard thresholding operator $H_k:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is defined as a vector-valued function that maintains the top-k entries of a given vector in an absolute value sense and zero out the rest. As an example $H_2(x)=[-5,0,-3,0]^{\top}$ where $x=[-5,2,-3,1]^{\top}$ and $k=2$ . According to Is hard thresholding operator Lipschitz? , we know that it is not Lipschitz. I have a situation where arguments of hard thresholding operator are very close to each other, that is, $y_1=x$ and $y_2=x+\xi$ where $\xi$ is a random vector in $\mathbb{R}^n$ whose entries are normal random variable with zero mean and variance $\sigma$ which can be reduced as small as possible but not zero. Question : Is there anyway to simplify $||H_k(y_1)-H_k(y_2)||_2$ and have an expression in terms of $y_1, y_2$ ? Would it be possible to impose locally Lipschitzness condition given $y_1$ and $y_2$ ? Can we exploit the bound found in A Tight Bound of Hard Thresholding to find a bound on $||H_k(y_1)-H_k(y_2)||_2$ in terms of $y_1, y_2$ ? Another view : In essence, what I am asking is that when $\mathbb{E}[x+\xi]=x$ , would it be possible to say something about the following: $$
\mathbb{E}[||H_k(x)-H_k(x+\xi)||_2]
$$ or $$
||H_k(x)-\mathbb{E}[H_k(x+\xi)]||_2
$$","['inequality', 'functions', 'linear-algebra', 'lipschitz-functions']"
4189896,"$f(x)\in C^0,f(0)=0,$ for$\ x\neq 0,f(x)>0,$ is $\int^x_0f(t)dt=o(f(x))?$","I wanna find a counter example to prove that ""if $f(x)\in C^0,f(0)=0$ ,and for $x\neq 0(f(x)>0)$ , then $\int^x_0f(t)dt=o(f(x)),$ as $x\to 0$ "" is not always true even if $f(x)\in C^p,p\geq1$ . First I found $f(x)=x^n|\sin(\frac1x)|+x^m\in C^0,m>n$ , and by using an auxiliary function $\phi(x)$ which is a broken line connecting the point where $|sin(\frac1x)|=0,1$ . So for the sequence $\{x_k=\frac1{k\pi}\},\int_0^{x_k}t^n\sin(\frac1t)dt\geq\int_0^{x_k}t^n\sin\phi(t)dt\geq\frac{nx_k^{n+1}}{2(n+1)}-\frac{(x_k)^{n+1}}{n\pi}$ , which means $\lim\limits_{k\to\infty} \frac{\int^{x_k}_0f(t)dt}{f(x)}$ doesn't exist. Then I found that $f(x)=x^n(\sin(\frac1x)+1)+x^m\in C^{n-2},m>n$ , and by using $\varphi(x)=$$\begin{cases}1,sin(\frac1x)>0\\0,sin(\frac1x)\leq0\end{cases}$ and $\int^{x_k}_0t^n\varphi(x)dt\geq\int^{\frac{x_k}2}_0t^ndt$ , we can prove that for $\{x_k=\frac{1}{2k\pi-\frac{\pi}2}\}, \lim\limits_{k\to\infty}\frac{\int^{x_k}_0f(t)dt}{f(x)}$ doesn't exist. As you can see, these two functions are pretty complicated, so I'm wondering if there are better (simpler) counter examples for the problem.","['integration', 'derivatives', 'analysis', 'real-analysis']"
4189917,One challenging problem about a recursive sequence.,"My Question : Let $a_0,a_1\in (0,1)$ , and $\{a_n\}$ be a sequence satisfying $$a_{n+2}=\sin(\lambda a_n+(1-\lambda)a_{n+1}),$$ where $0\leqslant \lambda \leqslant 1$ . Show that $$\lim_{n\to \infty}\sqrt{n}a_n=\sqrt{3(1+\lambda)}.$$ My Attempts : Discuss it in two cases. $\color{red}{\text{Case 1.}}$ Assume that $$b_n=\max\{a_{2n},a_{2n+1}\}.$$ $(1)$ Clear that $$a_{2n+2}=\sin(\lambda a_{2n}+(1-\lambda)a_{2n+1}).$$ If $a_{2n}\leqslant a_{2n+1}$ , then $$a_{2n+2}\leqslant \sin((\lambda+(1-\lambda))a_{2n+1})=\sin a_{2n+1}.$$ If $a_{2n+1}<a_{2n}$ , then $a_{2n+1}<a_{2n}$ . So we can get $$a_{2n+2}\leqslant \sin(\max\{a_{2n},a_{2n+1}\})=b_n.$$ $(2)$ It's evident that $$\begin{align*}
a_{2n+3}& =\sin(\lambda a_{2n+1}+(1-\lambda)a_{2n+2})\\
& \leqslant \sin(\lambda b_n+(1-\lambda)\sin b_n)\\
& \leqslant \sin((\lambda+1-\lambda)b_n)=\sin b_n.
\end{align*}$$ By $(1),(2)$ , we have $$b_{n+1}=\max\{a_{2n+2},a_{2n+3}\}\leqslant \sin b_n.$$ Thus $\color{blue}{\text{Stolz's Theorem}}$ implies that $$\limsup_{n\to \infty}\sqrt{n}b_n\leqslant \sqrt{3}.$$ That is $$\limsup_{n\to \infty}\sqrt{n}a_n\leqslant \sqrt{6}.$$ $\color{red}{\text{Case 2.}}$ Setting $c_n=\min\{a_{2n},a_{2n+1}\}$ , the same way as $\color{red}{\text{Case 1}}$ leads to $$c_{n+1}\geqslant \sin\sin c_n.$$ Now $\color{blue}{\text{Stolz's Theorem}}$ implies that $$\liminf_{n\to \infty}\sqrt{n}c_n\geqslant \sqrt{\frac{3}{2}}.$$ That is $$\liminf_{n\to \infty}\sqrt{n}a_n\geqslant \sqrt{3}.$$ $\quad$ So I guess that $$\lim_{n\to \infty}\sqrt{n}a_n=\sqrt{3(1+\lambda)}.$$ $\quad$ Any help will be appreciated.","['asymptotics', 'sequences-and-series']"
4189922,How to compute $\sum_{k=0}^\infty {2k \choose k} p^k (1-p)^k$,Trying to compute $$\sum_{k=0}^\infty {2k \choose k} p^k (1-p)^k$$ for $0 < p < 1$ . All I have been able to do so far is using the ratio test and Stirling's approximation to show that this sum is infinite iff $p=0.5$ .,"['binomial-coefficients', 'random-walk', 'probability']"
4189927,What exactly is the derivative of a function?,"I am very new to calculus. I have only just started it in school and when I have looked at other people explaining this question they use a lot of terminology I am unfamiliar with and it gets very overwhelming. I just want to know, is the derivative of a function of a graph a way of finding the graph’s gradient at (x, y)? Also, what exactly is the difference between $\frac{\mathrm dx}{\mathrm dy}$ and f(x) = … and when should I use the different functions. Thank you for any help I appreciate it so much.","['graphing-functions', 'calculus', 'functions', 'intuition', 'derivatives']"
4189933,Why does this alternative to the Quadratic Formula not always work? $x=\frac1a\left(-b+\sqrt{\frac12(b^2-2ac\pm b\sqrt{b^2-4ac})}\right)$,"I was playing with quadratic formula a while ago and I was able to accidentally arrive at: $$x=\frac{1}{a}\left(-b+\sqrt{\frac{(b^2-2ac)±b\sqrt{b^2-4ac}}{2}}\right) \tag{1}$$ However, I have not been able to solve it from scratch but I simplified it in some way to arrive back at the general quadratic equation. Here we go: $$x=\frac{1}{a}\left(-b+\sqrt{\frac{(b^2-2ac)±b\sqrt{b^2-4ac}}{2}}\right)$$ $$2(ax+b)^2=b^2-2ac±b\sqrt{b^2-4ac}$$ $$2a^2x^2+4abx+b^2+2ac=±b\sqrt{b^2-4ac}$$ From the original quadratic formula, we know that $$2ax+b=±\sqrt{b^2-4ac}$$ So $$2a^2x^2+4abx+b^2+2ac=b(2ax+b)$$ $$2a^2x^2+2abx+2ac=0 \tag{2}$$ Dividing (2) by $a$ as $a≠0$ , we have $$ax^2+bx+c=0$$ Looks practically perfect, right? I gave this to one of the maths experts in my environment for verification. Unfortunately, he was able to give a quadratic equation for which (1) could not solve and that is: $$x^2-4x+4=0$$ After a thorough re-check through a series of examples, if we let $x_1$ and $x_2$ be the values of (1), i realized that the formula could find only one root if and only if $$|x_1|=|x_2|$$ Question: what could be wrong with (1)? I am not claiming to have discovered a new quadratic formula, I am just curious about why (1) doesn't work generally.","['algebra-precalculus', 'quadratics']"
4189973,Definition and ideals of $\mathbb{Z}/n\mathbb{Z}$,"First, I want to ask what elements are in the rings $\mathbb{Z}/n\mathbb{Z}$ ,
the book I have defines the rings $R/I=\{a+I| a\in R\}$ where $a+I=\{x\in R| x-a \in I\}$ then proceed to give an example I can't understand how it follows the definition above.
It says $\mathbb{Z}/2\mathbb{Z}=\{2\mathbb{Z},1+2\mathbb{Z}\}$ , from what I understand $\mathbb{Z}/2\mathbb{Z}$ should have all $x$ such that $x\in \mathbb{Z}  $ and $a\in \mathbb{Z}$ and $x-a \in \mathbb{2Z} $ , which doesn't follow the set. A second example is $2\mathbb{Z}/4\mathbb{Z}=\{4\mathbb{Z},2+4\mathbb{Z}\}$ which is also a mystery for me. The last part it confuses me is the set $\{2\mathbb{Z},1+2\mathbb{Z}\}$ this set contines all odds and all the even numbers, why is different from $\mathbb{Z}$ ? Can someone explain to me what it's happening above because I am confused. my second question is about the ideals of $\mathbb{Z}/n\mathbb{Z}$ , can I have a simple explanation about why the ideals of $\mathbb{Z}/n\mathbb{Z}$ are the rings $k\mathbb{Z}/n\mathbb{Z}$ where $k|n$ for example, the ideals of $\mathbb{Z}/6\mathbb{Z}$ must be : $\mathbb{Z}/6\mathbb{Z}$ , $2\mathbb{Z}/6\mathbb{Z}$ , $3\mathbb{Z}/6\mathbb{Z}$ , $6\mathbb{Z}/6\mathbb{Z}=<0>(?) $","['ring-theory', 'abstract-algebra', 'commutative-algebra', 'ideals']"
4190103,Normal planes and spherical curves,"I am interested in the following result: ""If all the normal planes of a curve pass through a particular point, then the curve is contained in a sphere"". My approach: Let $\alpha: I \to \mathbb{R}^3$ be the curve (assume it is arc length parametrized). Let $P$ be the common point of all the normal planes. It is a well-known fact that, if $\tau(s) \ne 0 \quad \forall s$ and $k'(s) \ne 0 \quad \forall s$ , then $\alpha$ lies in a sphere if and only if $\dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)'$ . Consider a point $\alpha(s)$ . Since $P$ belongs to the normal plane of $\alpha$ at $\alpha(s)$ , then $$
\alpha(s)-P = x(s)\overrightarrow{n}(s) + y(s)\overrightarrow{b}(s)
$$ Deriving and applying Frenet formulas, we get $$
0 = -\bigl(1+x(s)k(s)\bigl)\overrightarrow{t} + \bigl(x'(s)-y(s)\tau(s)\bigl)\overrightarrow{n} + \bigl(x(s)\tau(s)+y'(s)\bigl)\overrightarrow{b}
$$ so $$ \left\{
\begin{array}{l}
1+x(s)k(s) = 0 \\
x'(s) -y(s)\tau(s) = 0\\
x(s)\tau(s) + y'(s) = 0
\end{array} \right.
$$ Playing with these equations and assuming $\tau(s) \ne 0$ , I get $\dfrac{\tau(s)}{k(s)} = \left(\dfrac{k'(s)}{\tau(s)k^2(s)}\right)'$ as desired. However, I don't see how I could prove that $\tau(s) \ne 0$ and $k'(s) \ne 0$ . I am aware that, as Shifrin notes suggests, using the fact that $\lVert f(t) \rVert = \text{constant} \iff f \cdot f' = 0$ helps deriving the result without using the characterization of spherical curves in terms of their curvature, torsion and their derivatives. However, I'd like to complete (if possible) the proof I have been working on.","['curves', 'frenet-frame', 'analysis', 'differential-geometry']"
4190108,"For all $\alpha,\beta\in\mathbb{N}$, are there only finitely many primes so that $\gcd(\text{ord}_p(\alpha),\text{ord}_p(\beta))=1$?","I suspect the negation of this statement is true. I was looking at this problem for fixed $\alpha,\beta$ but that's really hard, so I wanted to prove that there is at least one pair of integers with the property. So, supposing the opposite of that: for all $\alpha,\beta\in\mathbb{N}$ the set of primes satisfying the property $\gcd(\text{ord}_p(\alpha),\text{ord}_p(\beta))=1$ is bounded. So we can define a function $f(n)$ to be the greatest prime with the desired property over all pairs of integers $\alpha,\beta\leq n$ . Increasing $n$ appends more pairs of integers to the numbers over which $f$ has to satisfy its definition, plus the ones that were $\leq n$ , so $f$ is nondecreasing. Also for a prime $p\equiv 3 \mod 4$ we can take $\alpha\equiv -1$ and $\beta\equiv w^2$ where $w$ is a primitive root and we have $\gcd(\text{ord}_p(\alpha),\text{ord}_p(\beta))=\gcd\left(2,\frac{p-1}2\right)=1$ so $f$ is unbounded. So for some large $n$ and any prime $p>f(n)$ we have that of the $(p-1)^2$ pairs of numbers in $(\mathbb{Z}/p\mathbb{Z})^\times$ we have a forbidden square in the bottom left $(n-1)^2$ elements whose elements have to have pairwise noncoprime orders. If $p-1$ has $k$ distinct prime divisors, there are $2^{k-1}$ ways of writing it as $p-1=\theta\phi$ with $\gcd(\theta,\phi)=1$ and with a primitive root $w$ we can generate $w^\theta$ and $w^\phi$ with our property. So for each of those exponentially increasing quantity of pairs of numbers, they have to all conspire to be outside the forbidden square for all primes greater than $f(n)$ . This just seems unsustainable. $k$ isn't always large, for example if $p$ is a Sophie Germain prime $k=2$ and for all we know there are infinitely many of those, however it is large often enough to be a problem. Can we derive a contradiction from this? Some computational data: For $(\alpha,\beta)=(2,3)$ the primes with the desired property are 683, 599479, 108390409, 149817457, 666591179, 2000634731, 4562284561, 14764460089, 24040333283 and none more up to $1752\cdot10^9$ . For $(2,5)$ the primes are 31,
601,
2593,
599479,
204700049,
466344409,
668731841,
11638603429. For $(3,5)$ , they are 13, 313, 51169, 797161, 3482851, 5096867, 12207031, 162410641, 368385827, 1001523179, 4902814883. And for $(2,6)$ , they are 5,
7,
31,
43,
135607,
153649,
270841,
1489441,
1505447,
25781083,
127236649,
558062249,
745988807,
27989941729,
29512739491,
47206579351.",['number-theory']
4190147,"Show that $(E_{i,j})_{i,j=1}^n$ is a positive matrix.","Let $\{E_{i,j}\}_{i,j=1}^n$ be the matrix units of $M_n(\mathbb{C})$ . Consider the matrix $$A=(E_{i,j})_{i,j=1}^n \in M_n(M_n(\mathbb{C})) \cong M_{n^2}(\mathbb{C}).$$ I want to show that this matrix is positive (= self-adjoint and positive eigenvalues). Attempt : Let $\xi_1, \dots, \xi_n \in \mathbb{C}^n$ . I then  calculated $$\left\langle A \begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix},\begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix} \right\rangle= \sum_{k,l=1}^n (\xi_l)_l\overline{(\xi_k)_k}$$ but I don't see why this expression should be positive. Maybe my calculation is wrong?","['c-star-algebras', 'positive-semidefinite', 'positive-matrices', 'matrices', 'positive-definite']"
4190163,"Split $\{1,2,...,3n\}$ into triples with $x+y=4z$","A similar question  appeared last week. For which $n\in\Bbb N$ can we divide $\{1,2,3,...,3n\}$ into $n$ subsets each with $3$ elements such that in each subset $\{x,y,z\}$ we have $x+y=3z$? In this problem, I want to split the integers from $1$ to $3n$ into triples $\{x,y,z\}$ with $x+y=4z$ instead of $3z$ . I have a set of solutions, but am looking  for more ideas. $\sum(x+y+z)=\sum5z$ so $n=5k$ or $5k+3$ .  There are already hundreds of solutions for $n=13$ , and I imagine that number will only increase with $n$ , but the trick is to find one for any particular $n$ . Any solution for $n=N$ can be extended to one for each of $n=19N-7,19N+8,19N+13$ .  That gives solutions for $3^m$ different $n$ below $Z=19^m$ , which is more than $\sqrt[3]Z$ different $n$ below $Z$ .  But most solutions do not contain a smaller one within them. To split the numbers from $3N+1$ to $57N-21$ into triples $$(3N+1+k,33N-9+3k,9N-2+k)$$ for $k =0..6N-4$ and $$(51N-18+k,33N-10+3k,21N-7+k)\\
(27N-9+k,33N-11+3k,15N-5+k)$$ for $k =0..6N-3$ . A way to split $\{3N+1,...,3(19N+8)\}$ into triples is $$(3N+1+k,33N+15+3k,9N+4+k)\\ (51N+22+k,33N+14+3k,21N+9+k)$$ for $k=0..6N+2$ , and $$(27N+12+k,33N+16+3k,15N+7+k)$$ for $k=0..6N+1$ To split $\{3N+1 ...,3(19N+13)\}$ into triples $$(3N+1+k,33N+23+3k,9N+6+k)$$ for $k=0..6N+4$ and $$(27N+19+k,33N+25+3k,15N+11+k)\\
(51N+36+k,33N+24+3k,21N+15+k)$$ for $k=0..6N+3$ . At Split $\{1,...,3n\}$ into triples with $x+y=5z$ - no solutions? , Thomas Andrews has shown there are no solutions for $x+y=5z$ .","['combinatorial-designs', 'elementary-number-theory', 'combinatorics']"
4190171,Prove that $\angle IPA=\angle IQB$ in $\triangle ABC$ with incenter $I$.,"Problem In $\triangle ABC$ , $AB<BC$ and its incenter is $I$ . $P$ is the midpoint of $AC$ and $Q$ is the the midpoint of arc ${ABC}$ . Prove that $\angle IPA=\angle IQB$ . The problem is from a monthly Olympiad training contest. I am not able complete my solution. Here are my workings. My workings Here is the figure of my incomplete solution: Since $Q$ is the midpoint of arc $ABC$ , $\triangle AQC$ is isosceles with $AQ=CQ$ . $PQ$ is the perpendicular bisector of chord $AC$ . $PQ$ is extended to $R$ such that $R$ is the intersection point with the circumcircle of $\triangle ABC$ . Then $QR$ is the diameter of the circle. So, $\triangle ARC$ is also isosceles with $AR=CR$ .
Then I can't proceed further. I can not bring $I$ in the solution. Though it is easy to see that $B,I,R$ are collinear from the figure. But I cannot prove that. And how to use it in the rest of the solution? So, I need to complete my solution with proper proof.","['contest-math', 'euclidean-geometry', 'circles', 'triangles', 'trigonometry']"
4190187,"proof that a countable union of countable sets is countable, why is $\sf ZF$ not enough?","Without assuming the axiom of choice, a countable union of countable sets isn't necessarily countable. In the following proof though, I don't understand when anything that isn't in the axioms of $\sf ZF$ used. Let $X$ be a countable set of countable sets. Since $X$ is countable it can be ordered: $X = \{G_1, G_2 ,...\}$ . We'll construct a function $F$ that maps each element of $X$ to a bijective function from itself to $\Bbb N$ . Let $f_0 = \emptyset$ . We'll recursively define $f_i$ : Since $G_{i+1}$ is countable, there exists a bijective function $h :G_{i+1} \to \Bbb N$ . Define $$f_{i+1} = f_i \cup \langle G_{i+1}, h\rangle$$ Let $$A = \{f_i \mid i\in\Bbb N \}$$ Since $A$ is a countable set of compatible functions, the set $$F = \bigcup A$$ is a function, with $$Dom(F) = \bigcup \{Dom(f)\mid f\in A\} = X$$ Once $F$ is constructed, creating an injective function $f:\bigcup X \to \Bbb N$ is easy, for example by mapping elements to numbers of the form $2^i3^j$ . I know that the problem with the proof is the construction of $A$ , but I don't understand where. When ordering $X$ we chose a bijective function out of infinitely many, so why can't we do the same for $G_{i+1}$ ?","['elementary-set-theory', 'axiom-of-choice']"
4190197,Sufficient condition for irreducibility of homogeneous polynomial (Exercise I. 5.9 in Hartshorne),"From Hartshorne Algebraic Geometry. Exercise I. 5.9 states: Let $f\in k[x,y,z]$ be a homogeneous polynomial, let $Z(f)\subseteq \mathbf{P}^2$ be the algebraic set defined by $f$ , and suppose that for every $P\ \in Z(f)$ , at least one of $(\partial f /\partial x)(P)$ , $(\partial f /\partial y)(P)$ , $(\partial f /\partial z)(P)$ is nonzero. Show that $f$ is irreducible. Hartshorne gives a hint to use Exercise I. 3.7., which states, If $Y \in \mathbf{P}^n$ is a projective variety of dimension $\geq 1$ ,  and if $H$ is a hypersurface, then $Y \cap H \neq \emptyset$ . I tried the following: Suppose that $\forall P \in Z(f)$ , $(\partial f /\partial x)(P)$ , $(\partial f /\partial y)(P)$ , $(\partial f /\partial z)(P)$ are not all zero. This means that $Z(f) \cap Z(f_x, f_y, f_z) = \emptyset$ . By exercise 3.7, $Y \cap H \neq \emptyset$ for $Y$ a variety and $H$ a a hypersurface (i.e., the zero set of a single irreducible polynomial). If $A$ is any algebraic set, then $A$ decomposes into irreducible components, $A=W_1 \cup \dots \cup W_q$ . So $A \cap H=(W_1 \cup \dots \cup W_q) \cap H = [W_1 \cap H]\cup\dots\cup[W_q \cap H]$ . By exercise 3.7, each $[W_i \cap H] \neq \emptyset$ , hence $A \cap H \neq \emptyset$ . So, if $Z(f) \cap Z(f_x,f_y,f_z)=\emptyset$ , then since $Z(f_x,f_y,f_z)$ is an algebraic set, $Z(f)$ cannot be a hypersurface. That is, $f$ cannot be irreducible. Obviously this is very wrong, but even in such a simple argument I cannot pinpoint why. Please help. I am not looking for unrelated correct solutions to this exercise. I just want to understand the mistake in the above argument. Thank you. Edit: By $f_x$ I mean $\partial f / \partial x$ , the polynomial which is the partial derivative of $f$ with respect to $x$ .","['irreducible-polynomials', 'algebraic-geometry', 'solution-verification']"
4190198,Roots of $3x^5 - 15x +5$,"Let $f(x) =3x^5 - 15x +5$ . By  Eisenstein’s Criterion we can show that $f(x)$ irreducible over $\mathbb{Q}$ (Since $5 \nmid 3,  5 \vert -15,5$ and $5^2 \nmid 15$ ). Since $g$ is continuous and $ f( -2 ) =  -61, f( -1 ) =  17 ,f( 0 ) =  5,f( 1 ) =  -7,f( 2 ) =  71$ . So by intermediate value theorem  we can say that $f$ has three real roots. Clearly $f'(x) > 0 $ for all $x > 2$ and $f'(x) < 0$ for all $x < -2$ . So $f(x)$ is monotone and so $f$ does not have zeroes in these regions. Suppose $f(x)$ has $4$ real zeroes then by using Rolle's theorem we can say that $f'(x)$ should contain at least $3$ zeroes between the roots of $f$ . But $f'(x) = 15(x^4 -1)$ does not have $3$ real zeroes. So $f$ has two other  complex roots. Let $K$ be the smallest subfield of complex numbers containing $\mathbb{Q}$ and $5$ roots of $f(x)$ . Then using fundamental theorem of Galois theory we can say that $Gal(K/\mathbb{Q}) \approx S_5$ , the symmetric group of five letters. Since $S_5$ is not solvable, by a theorem of Galois we can  conclude that $f(x)$ is not solvable by radicals. That is  each zero of the polynomial $f(x)$ cannot  be written as an expression  involving elements of $\mathbb{Q}$ combined by the operations of addition, subtraction, multiplication, division, and extraction of roots. How do the roots of $f$ look like? We have information about the location of real roots but I think that information may not help in finding some expression for roots. Precisely, my question is that that does there exist a series, continued fractions, or some integral which represent the roots of $f(x)$ ?.","['roots', 'galois-theory', 'polynomials', 'sequences-and-series', 'continued-fractions']"
4190199,Direct Proof of Archimedian Property,"I've written this proof and I'd like to ask if this is a valid proof of the Archimedean Property: $\underline{\text{Claim}:}$ $\forall x \in \mathbb{R}\exists n_x \in \mathbb{N}:x \leq n_x$ . $\underline{\text{Proof}:}$ We know that $1 \in \mathbb{N}$ and $1 \in \mathbb{R}$ . Thus, if $x \leq 1$ then $n_x = 1$ and we are done. Now assume, $x > 1$ . Let M = $\{k \in \mathbb{N} : k < x\}$ . Then $1 \in M$ . Thus, $M\neq \varnothing$ and is bounded above by $x$ . By the completeness property, we can then infer that sup $M$ exists. If we let $u = \text{sup}M -1 $ then we know that $\exists k \in M$ such that $k > u$ . $\therefore\text{sup}M < k + 1$ , which is a natural number by the inductive property of natural numbers. Since, $k + 1 > \text{sup}M$ , we have that $k + 1 \not \in M$ . By setting $n_x = k + 1$ , we have proved the Archimedean Property. $\square$","['real-numbers', 'solution-verification', 'real-analysis']"
4190216,"Solve the initial value problem $\frac{dy}{dx} + y = f(x) $, where $f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases}$, $y(0)=0$.","This is a very simple first order ordinary linear differential equation: $$\frac{dy}{dx} + y = f(x) \tag{A}\label{A}$$ , where $f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases}$ with initial condition $y(0)=0$ The solutions of this differential equation according to the answer given in the book is: $y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ 2(e-1)e^{-x} \quad x\geq1 \end{cases} \tag{B}\label{B}$ However I find that answer incorrect and all the solutions of this initial value problem should be given by: $y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ ce^{-x} \quad x\geq1 \end{cases}$ , where c is an arbitrary real number. For $0\leq x \lt 1$ , we get the solution $$y=2(1-e^{-x}) \tag{i}\label{i}$$ For $x \geq 1$ , the initial condition is not valid and we simply get the solution $y=ce^{-x} \tag{ii}\label{ii}$ ,where $c\in \mathbb R$ This can be verified.
It doesn't matter what real values does c take, $g(x)=ce^{-x}$ will be a one parameter family of solutions of the given initial value problem since we get an identity on $\mathbb R$ after substituting $g(x)=ce^{-x}$ for y and $g'(x)=-ce^{-x}$ for $\frac{dy}{dx}$ in \eqref{A} when $x\geq1$ (where g is a real function defined on $\mathbb R$ ) But the printed answer asserts that only a particular value of c, that is, $c=2e-2 \tag{iii}\label{iii}$ makes \eqref{ii} a solution of \eqref{A}
Now the question came to my mind, why only this special value and where's that coming from? So I deduced that equating the value of y at $x=1$ in \eqref{i} (though we are not supposed to do that unless we are finding $\lim {y}$ as x tends to 1 from left) with y at $x=1$ in \eqref{ii} will lead to \eqref{iii}
That can only mean one thing, we are making the solution function continuous at x=1. Why are we ensuring the continuity of solution function? As far as continuity is concerned, Is it necessary for every linear first order ordinary differential equation solution to be continuous everywhere?
I have got a counter example against this statement. Consider a differential equation $d(xy)=0$ Which is linear since it can expressed as $xdy +ydx=0$ implies $\frac{dy}{dx} +\frac{y}{x}=0$ (derivative form) The one parameter family of solutions of this differential equation is given by $xy =k ,k \in \mathbb R$ i.e., $y=\frac{k}{x}$ which is discontinuous at $x=0$ (but $x=0$ is not in domain of definition) so still not sure about this counter example. There must be some reason for ensuring continuity atleast for this particular differential equation \eqref{A} or at the particular value x=1 since \eqref{B} doesn't seem like a misprint. Also I remember a similar problem solved using Laplace transformations led to a solution which matches to the solution obtained simply by integrating only after ensuring continuity. My final question is- Aren't all those solutions of the form $y=ce^{-x}, x\geq1$ lost where c is any real number other than one substituted for ensuring continuity? For example- $y=5e^{-x},x\geq1$ which is indeed an explicit solution of the initial value problem but not a member of solutions given in \eqref{B}.","['calculus', 'ordinary-differential-equations']"
4190265,"Prove that for each $f\in L^1([0,1])$ we have $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$.","I would be glad if someone could help me to prove the following exercise. Let $\{g_n\}$ be a sequence of measurable functions on $[0,1]$ such that (a) There exists a constant $C>0$ such that for each $n\in \Bbb N$ , $|g_n(x)|\leq C$ for almost all $x\in[0,1]$ . (b) For every $a\in [0,1]$ , $\lim_{n\to\infty}\int_0^a g_n(x)dx=0$ . Prove that for each $f\in L^1([0,1])$ we have $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$ . Attempt . Suppose $f$ is a step function defined on $[0,1]$ . We show that $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$ . Say $f=\sum_{i=1}^N c_i\chi_{E_i}$ for some disjoint intervals $E_1,\dots,E_N\subseteq [0,1]$ . Then $$ \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=\lim_{n\to\infty}\int_0^1 \left(\sum_{i=1}^N c_i\chi_{E_i}(x)\right) g_n(x)dx=\sum_{i=1}^Nc_i\int_{E_i}g_n(x)dx=0$$ since for each $n\in\Bbb N$ and $a,b\in [0,1]$ with $a\leq b$ we have $$\int_a^b g_n(x)dx=\int_0^b g_n(x)dx -\int_0^a g_n(x)dx \implies \lim_{n\to\infty}\int_a^bg_n(x)dx=0.$$ Question. Does there exist a sequence $\{f_n\}$ of step functions on $[0,1]$ such that $0\leq f_1\leq f_2\leq\dots$ and $f_n$ converges to $|f|$ pointwise almost everywhere on $[0,1]$ ? Even if the answer is yes, I couldn't conclude the proof by monotone convergence theorem. Thanks!","['measure-theory', 'analysis', 'real-analysis']"
4190272,Possible corollary of Jordan's curve theorem,"Suppose $\gamma$ and $\phi$ are simple closed curves in $\mathbb{R}^2$ , simple meaning that they have no self-intersections. Suppose that $\gamma$ and $\phi$ intersect in exactly one point and that $\phi$ minus the intersection point is contained in the interior of $\gamma$ . How can we prove, possibly by using Jordan's curve theorem, that $\phi$ induces a decomposition of the interior of $\gamma$ in two connected components? I know the interior of $\gamma$ is homeomorphic to $\mathbb{R}^2$ , but what is bothering me here is that a point of $\gamma$ (and exactly one) is on the boundary of the connected component. Thus, an homeomorphism which sends the interior of $\gamma$ to $\mathbb{R}^2$ cannot possibly send $\gamma$ to $S^1$ , right?","['connectedness', 'plane-curves', 'curves', 'plane-geometry', 'general-topology']"
4190284,Drawing the toric diagram for $\mathcal{L}^{m} \rightarrow T^{2}$ geometries.,"I'm having some problems to understand the geometry involved in the section 3.1 of the paper Two Dimensional Yang-Mills, Black Holes and Topological Strings . Concretely, I was wondering to know if it is possible to visualize the geometry of the $X = \mathcal{L}^{-m} \oplus \mathcal{L}^{m} \rightarrow T^{2}$ Calabi-Yau threefold by means of toric diagrams, in other words, how should I draw the image of the moment map of $X$ in ""the physics way"" $(\ast)$ .  Here $T^{2}$ is a $2$ -torus, $\mathcal{L}^{m}$ is the line bundle characterized by the fact that a holomorphic section of $\mathcal{L}^{m} \rightarrow T^{2}$ has a divisor of degree $m$ on $T^{2}$ and $\mathcal{L}^{-m}$ is the inverse bundle of $\mathcal{L}^{m}$ . $(\ast)$ By ""the physics way"" of draw toric diagrams I mean by considering non-compact toric Calabi-Yau manifold $X$ as $T^{2} \times \mathbb{R}$ fibrations over the image of the moment map of $X$ (see CY 3-folds are $T^2 \times \mathbb{R}$ fibrations over the base $\mathbb{R}^3$ . What does it mean? ). Below to the left I give the example of the toric diagram of $\mathbb{C}^{3}$ viewed as a $T^{2} \times \mathbb{R}$ -fibration over $\mathbb{R}^{3}_{\geq 0}$ ; $D_{1}$ , $D_{2}$ $D_{3}$ are the $2$ -dimensional cones of $\mathbb{R}^{3}_{\geq 0}$ . This example is discussed in detail in Topological strings and their physical applications (example 3.1, page 16). Below to the right represents the toric diagram of the cotangent bundle to a $\mathbb{P}^{2}$ embedded on a threefold (see Branes, Black Holes and Topological Strings on Toric Calabi-Yau Manifolds , section 3, page 8). Here $D_{0},D_{1}$ , $D_{2}$ $D_{3}$ are the non-trivial divisors of the geometry where $D_{0}$ is a $\mathbb{P}^{2}$ and the remaining $\mathcal{O}(-p) \rightarrow \mathbb{P}^{1}$ bundles (the relevant $\mathbb{P}^{1}$ are the edges of $\mathbb{P}^{2}$ ). My problem that I can is that I can't draw the toric diagram of $\mathcal{L}^{-m} \oplus \mathcal{L}^{m} \rightarrow T^{2}$ . My faliure goes back to the fact that I have no clue on how to draw a fibration over a codimension 2 cycle ( $T^{2}$ ). I'm unable to draw the toric diagram of $\mathcal{L}^{m} \rightarrow T^{2}$ for example. Any hint, comment or reference is welcomed!","['symplectic-geometry', 'toric-geometry', 'divisors-algebraic-geometry', 'algebraic-geometry', 'mathematical-physics']"
4190307,"$E[X_T]$ for $T=\min\{n: \sum_{i=1}^nX_i\geq1\}$ where $X_i\sim U[0,1]$.","Assume $X_i\overset{i.i.d}{\sim} U[0,1]$ . Let $S_n=\sum_{i=1}^nX_i$ and $T=\min\{n:S_n\geq 1\}$ . Find: (1) $E[T]$ , (2) $E[S_T]$ , (3) $E[X_T]$ For the first one, $E[T]=e$ (see e.g. this question ). For the second one we can use Wald's equation to get $E[S_T]=E[T]E[X_1]=\frac e2$ . How about the third one?","['expected-value', 'probability-theory', 'probability']"
4190318,Difference between hypotenuse and larger leg in a Pythagorean triple,"I've been number crunching irreducible Pythagorean triples and this pattern came up: the difference between the hypotenuse and the larger leg seems to always be n ² or 2 n ² for some integer n . Moreover, every integer of the form n ² or 2 n ² is the difference between hypotenuse and the larger leg for some irreducible Pythagorean triple. Is there a simple proof for that? There is a result listed on Wikipedia that looks kinda, sorta related: that the area of a Pythagorean triangle can not be the square or twice the square of a natural number. EDIT: Actually, the statements are correct only for odd n ² (but also by any 2 n ² as stated) as John Omielan demonstrated below.","['number-theory', 'conjectures', 'pythagorean-triples', 'elementary-number-theory']"
