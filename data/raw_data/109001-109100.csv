question_id,title,body,tags
1569609,Proof for $(U+W)^\perp=U^\perp \cap W^\perp$ if $U$ and $W$ are both subspaces.,"In this case $(U+W)^\perp$ means the orthogonal complement of the union of $U$ and $W$ (regardless of repetition, is it correct?). The vectors inside the orthogonal complement has to be orthogonal to every vector in $(U+W)$ and so it must be composed of the orthogonal complement for each $U$ and $W$ . Digitally, $(U+W)^\perp=$ { $x$ in $ℝ^n|x \cdot f_u =0$ $\iff x \cdot f_w =0$ }=U $^\perp \cap W^\perp$ . But I felt the proof is either somewhat incorrect or incomplete, could anyone give some opinions?","['linear-algebra', 'inner-products']"
1569660,"Matrix ODE, defective eigenvalue: Where does the extra '$t$' come from?","Given $A \in \Bbb R^{2\times 2}$, the system
  $$
\dot X=AX
$$ Has the solution 
  $$
X= c_1e^{\lambda t}\xi_1+c_2e^{\lambda t}\left (\xi_1 t+\xi_2 \right)
$$ Where $\xi_1$ is the unique eigenvector associated to the double eigenvalue $\lambda$ and $\xi_2$ satisfies: $$
(A-\lambda I)\xi_2=\xi_1
$$ I've seen a proof of this fact, but I still don't understand why the second linearly independent solution has that form: Where does the multiplying $t$ come from? I'm interested in a linear algebra perspective of this.","['matrices', 'ordinary-differential-equations', 'linear-algebra']"
1569661,Is $\sum_{p}\frac{1}{p^{2}}$ irrational?,Is $\sum_\limits{p}^{\infty}\frac{1}{p^{2}}$ irrational where p is prime? How to prove it?,"['number-theory', 'rationality-testing', 'prime-numbers', 'sequences-and-series']"
1569707,Is $f(x) = x^T A x$ a convex function?,"Is $f(x) = x^T A x$ a convex function, where  $x \in \mathbb{R}^n$, and $A$ is a $ n\times n$ matrix? If not, my question can be reformed to: when is $f(x)$ convex, any restriction for $A$? For example, like positive definite and symmetric?","['matrices', 'convex-analysis']"
1569796,Basis for Skew Symmetric Matrix,"I'm trying to find a basis for the kernel for the following mapping:  Considering the linear transformation T: $M_{33} \rightarrow M_{33}  $ defined by $T(A) = .5(A + A^T)$. I know that this is basically asking for the basis under the condition that $T(A)=0$ which means that $A+A^T=0$ so $A^T = -A$.  I found that matrices that fit this condition are Skew Symmetric Matrices.  However, I'm not sure how to find the basis for the kernel of these matrices.","['matrices', 'linear-algebra']"
1569810,What is the sum of the digits of the sum of the digits?,"Problem Let $(10^{2016}+5)^2=225N$. If $S$ is the sum of the digits of N, then find the sum of the digits of S Attempt Let's look at some smaller cases. We have $\dfrac{(10^{3}+5)^2}{225} = 4489$,$\dfrac{(10^{4}+5)^2}{225} = 444889$, and $\dfrac{(10^{5}+5)^2}{225} = 44448889$. Thus we see the pattern and $S = 4*2015+8*2014+9 = 24181$ and the sum of the digits of $S$ is $\boxed{16}$. Question Prove the result in the solution above by induction or some other method. That is, show that for $n > 1$ $$\dfrac{(10^{n}+5)^2}{225} =\underbrace{44\ldots4}_\text{n-1 4's}\underbrace{88\ldots8}_\text{n-2 8's}9.$$",['number-theory']
1569821,Probability distribution over probability distributions,"This may not be a well defined question, but I couldn't find anything about it. Perhaps, I am looking with wrong keywords. Anyway, suppose $X$ is a set, possibly a subset of an Euclidean space. Define $\Delta(X)$  as the set of probability measures over $X$. Similarly define $\Delta(\Delta(X))$. Now the claim that I am struggling with is the following: Take $\mathcal{P}\in \Delta(\Delta(X))$ then we can find $P\in \Delta(X)$. I can see that we can define $P(A)=\int_{\Delta(X)} f(A) \mathcal{P}(df)$ for any ``appropriate'' $A$, but I want to really understand this claim. For example, I don't know what kind of integral $\int_{\Delta(X)}$. Is this thing, $P(A)$ is always well defined? If you suggest me any book, lecture notes or paper that would be great. Many thanks in advance!","['lebesgue-integral', 'probability', 'measure-theory', 'probability-distributions']"
1569860,How to find formula for recursive sequence sum?,"I have the following sequence: $$a(1) = 1$$
$$a(n) = a(n-1) + n$$ For example: 
$$a(1) = 1$$
$$a(2) =3$$
$$a(3) =6$$
$$a(4) =10$$
$$a(5) =15$$
$$a(6) = 21$$ Which approach should I use in order to find the formula for the sum of elements $a(1)$ through $a(n)$? Thanks :)","['recurrence-relations', 'summation', 'sequences-and-series']"
1569866,"Rational solutions $(a,b)$ to the equation $a\sqrt{2}+b\sqrt{3} = 2\sqrt{a} + 3\sqrt{b}$","Find all rational solutions $(a,b)$ to the equation $$a\sqrt{2}+b\sqrt{3} = 2\sqrt{a} + 3\sqrt{b}.$$ I can see that we have the solutions $(0,0), (2,0), (0,3), (3,2), (2,3)$, and I suspect that there are no more. I tried to do the thing where you square both sides, rearrange terms, square both sides again, but it got messy. Edited: Also, I think I recall that all distinct square roots of square-free numbers are linearly independent over the rationals. Is this true? This might lead to a more direct way of proving it.","['number-theory', 'rational-numbers', 'diophantine-equations']"
1569899,Number of Dyck Paths Bounded by $M$,"A Dyck path of length $2k$ is a sequence $\{s_j\}_{j=1}^{2k}$ of non-negative integers such that $|s_{j+1} - s_j| = 1$ for all $j = 1,...,2k$ and $s_0 = s_{2k} = 0$. The number of Dyck paths of length $2k$ is given by the nice formula
$$C_k = \frac{1}{k+1}\begin{pmatrix} 2k\\k\end{pmatrix}.$$ ($C_k$ is the $k$-th Catalan number.) Given some positive integer $M$, is there a formula for the number of all Dyck paths of length $2k$ such that $s_{j} \le M$ for all $j = 1,...,2k$?","['combinatorics', 'catalan-numbers']"
1569917,Integrability of two independent random variables,"Suppose that $X$ and $Y$ are two independent random variables. How does one show that if $X+Y\in L^1$, then both $X$ and $Y$ are in $L^1$? I know that one can approach this problem using the fact that the joint law of $X+Y$ is the convolution of the laws of $X$ and $Y$, but I would like to know   more ""elementary"" ways of proving the claim.
That is, how does one establish that $$E[|X|: |X|>M] \to 0 $$ as $M \to \infty$?",['probability-theory']
1569941,Group Axioms and Closure,"In this book I am reading, a group is defined as: A group is an ordered pair $(G,X)$ where $G$ is a set and $X$ is a binary operation on $G$ satisfying the following axioms: i) $X$ is associative. ii) $G$ contains an identity. iii) Each element of $G$ has an inverse. How does this definition imply that $G$ is closed under this operation?","['abstract-algebra', 'group-theory']"
1569987,Prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$,"I am trying to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$ for all non-negative random variables X with $0 \leq \lambda < 1$. I have tried many different paths, however the most successful one so far involves the Cauchy-Schwarz inequality, which states:
$$\mathbb{E}[X Y] \leq \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}$$
This implies:
$$\mathbb{E}[Y^2] \geq \frac{\mathbb{E}[X Y]^2}{\mathbb{E}[X^2]}$$
Now I think with a proper choice for Y I might be able to obtain the statement I am trying to prove. I tried for instance $Y=1-\lambda$ which I believe leads to: $$(1-\lambda)^2 \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$$
This is the closest I got to the statement, however now I would have to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2$ at which I got stuck. Am I trying the right things here or am I completely going down a dead path? If so, what could be a sensible choice for $Y$? Thanks in advance!","['probability-theory', 'inequality', 'probability']"
1570030,Why do we use the Borel sigma algebra for the codomain of a measurable function?,"In several measure theory books, I see that a measurable function $f:\mathbb{R} \rightarrow \mathbb{R}$ often equips the domain with the Lebesgue $\sigma$-algebra and the codomain with the Borel  $\sigma$-algebra. However, if $g$ is another such function, then the composition of $f$ and $g$ may fail to be measurable. Why do we not use the Lebesgue $\sigma$-algebra for both the domain and the codomain?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1570049,Verify integration of $ \int\frac{\sqrt{2-x-x^2}}{x^2}dx $,"This is exercise 6.25.40 from Tom Apostol's Calculus I. I would like to ask someone to verify my solution, the result I got differs from the one provided in the book. Evaluate the following integral: $ \int\frac{\sqrt{2-x-x^2}}{x^2}dx $ $ x \in [{-2,0}) \cup ({0,1}]  $ As suggested in the book, we multiply both numerator and denumerator by $ \sqrt{2-x-x^2} $. This removes the endpoints from the integrand's domain, but the definite integral we calculate with the antiderivative of this new function will be still the proper integral between any two points of the original domain: we only remove a finite number of points from the original domain and the domain of the resulting antiderivative will be the same as the original domain. $$ I=I_1+I_2=\int\frac{2-x}{x^2\sqrt{2-x-x^2}}dx-\int\frac1{\sqrt{2-x-x^2}}dx \tag{1} $$ Evaluating first $ I_1 $ by substituting $ t=\frac1{x} \; \text, \; dx=-\frac1{t^2}dt \text: $ $$ I_1=-\frac1{\sqrt2}\int\frac{2t-1}{\frac{t}{|t|}\sqrt{\left(t-\frac14\right)^2-\left(\frac34\right)^2}}dt \tag{2} $$ Substituting again $ \frac34\sec{u}=t-\frac14 \; \text, \; dt=\frac34\sec{u}\tan{u}du \; \text, \; t=\frac{3\sec{u}+1}{4} \; \text, \; u=\operatorname{arcsec}{\frac{4t-1}{3}} $, by considering the sign of $ t $ and $ \tan{u} $ in the integrand's two sub-domains: a) $ x \in (-2, 0): t<-\frac12 \; \text, \; \frac{4t-1}{3}<-1 \; \text, \; u \in(\frac\pi2,\pi) \; \text, \; \tan{u}<0 $ b) $ x \in (0, 1): t>1 \; \text, \; \frac{4t-1}{3}>1 \; \text, \; u \in(0,\frac\pi2) \; \text, \; \tan{u}>0 $ $$ I_1=-\frac{1}{2\sqrt2}\int3\sec^2{u}-\sec{u}=-\frac1{2\sqrt2}\left(3\tan{u}-\log\left|\tan{u}+\sec{u}\right|\right)+C_1 \tag{3} $$ $$ \sec{u}=\sec\operatorname{arcsec}\frac{4t-1}{3}=\frac{4t-1}{3}=\frac{4-x}{3x} \tag{4} $$ $$ \tan^2{u}=\tan^2\operatorname{arcsec}\frac{4t-1}{3}=\left(\frac{4t-1}{3}\right)^2-1=\frac89\frac{2-x-x^2}{x^2} \tag{5} $$ Considering again cases a) and b):
$$ \tan{u}=\frac{2\sqrt2}{3}\frac{\sqrt{2-x-x^2}}{x} \tag{6} $$ $$ I_1=-\frac{\sqrt{2-x-x^2}}{x}+\frac1{2\sqrt2}\log\left|\frac{2\sqrt2}{3}\left(\frac{\sqrt{2-x-x^2}+\sqrt2}{x}-\frac1{2\sqrt2}\right)\right|+C_1= \tag{7} $$ $$ -\frac{\sqrt{2-x-x^2}}{x}+\frac1{2\sqrt2}\log\left|\frac{\sqrt{2-x-x^2}+\sqrt2}{x}-\frac1{2\sqrt2}\right|+C'_1 \tag{8} $$ Evaluating now $ I_2 $:
$$ I_2=-\int\frac1{\sqrt{\left(\frac32\right)^2-\left(x+\frac12\right)^2}}dx \tag{9} $$ Substituting $\frac32\sin{z}=x+\frac12 \; \text, \; dx=\frac32\cos{z}dz \; \text, \; z=\operatorname{arcsin}{\frac{2x+1}{3}} $: $$ I_2 = -\int dz = -\operatorname{arcsin}{\frac{2x+1}{3}}+C_2 \tag{10} $$ The final result:
$$ I = I_1 + I_2 = -\frac{\sqrt{2-x-x^2}}{x}+\frac1{2\sqrt2}\log\left|\frac{\sqrt{2-x-x^2}+\sqrt2}{x}-\frac1{2\sqrt2}\right|-\operatorname{arcsin}{\frac{2x+1}{3}}+C \tag{11} $$ The solution provided in the book: $$ I = -\frac{\sqrt{2-x-x^2}}{x}+\frac1{2\sqrt2}\log\left(\frac{\sqrt{2-x-x^2}}{x}-\frac1{2\sqrt2}\right)-\operatorname{arcsin}\frac{2x+1}{3}+C $$","['substitution', 'absolute-value', 'integration', 'proof-verification']"
1570067,Is series convergent?,"$$ \sum_{n=1}^{\infty } (-1)^{\frac{n(n+1)}{2}} \cdot \frac{1}{n}$$
Is it convergent? Of course it's not absolutely convergent, but I'm not sure about conditional convergence. Leibnitz seems not to work here.","['sequences-and-series', 'analysis']"
1570126,Extension of projection from a point to a Blow Up,"I feel like there's something obvious I'm missing here, and I'm not looking for a whole answer, but rather just a pointer in the right direction. Suppose you have the projection from a point $\mathbb{P}^n \backslash \{P\} \to \mathbb{P}^{n-1}$, where $P = (0:...:0:1)$. This is not defined at $P$, but the claim is that it extends to a morphism from $B_P(\mathbb{P}^n)$ (blow up at $P$) to $\mathbb{P}^{n-1}$. I'm trying to figure out exactly how to define this morphism. Since the blow up just replaces $P$ with a copy of $\mathbb{P}^{n-1}$, it seems like the most natural morphism to consider should just be the identity, specifically from $\{P\} \times \mathbb{P}^{n-1} \to \mathbb{P}^{n-1}$. This seems to make sense; the definition of morphism $\varphi$ between quasi-projective varieties $X \to Y$ that I have is a Zariski continuous map such that if $f$ is regular on an open set $U \subset Y$, then $f \circ \varphi$ is regular on $\varphi^{-1}(U)$, and as far as I can tell, this new map should satisfy these conditions, since I'm basically giving two morphisms on two disjoint parts of the blow-up. Does this work? And in this vein, could I use any morphism from $\{P\} \times \mathbb{P}^{n-1} \cong \mathbb{P}^{n-1} \to \mathbb{P}^{n-1}$?","['morphism', 'projective-space', 'algebraic-geometry', 'blowup']"
1570171,Projection of space onto unit sphere - differentiating it!,"I am trying to differentiate the function that projects each $x\in
 \mathbb{R}^3$ onto the unit sphere, namely: $$f(x)=\frac{x}{||x||},\quad f(0)=0$$ I know that the derivative is $$Df\big|_{x}(h)=\frac{x\times (h\times x)}{||x||^3}=\frac{h}{||x||}-\frac{x(x\cdot h)}{||x||^3}$$
But I am having a lot of trouble proving it. I can reduce it to:
$$f(x+h)-f(x)-Df\big|_x(h)=x\left(\frac{1}{||x+h||}-\frac{1}{||x||}+\frac{(x\cdot h)}{||x||^3}\right)+o(h)$$
I am struggling to show that the expression in brackets is $o(h)$. It looks like the $||x+h||^{-1}$ is begging for expansion but I'm not sure how to deal with it because they are vectors. Help?","['multivariable-calculus', 'real-analysis', 'analysis', 'derivatives']"
1570206,Set of critical values of one-dimensional continuously differentiable function has measure zero.,"I am fighting with the following exercise: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuously differentiable function, $A:=\{x \in \mathbb{R} \ \vert \ f^{\prime}(x)=0 \}$ and $\lambda$ the Lebesgue-measure on the real line. Show that $\lambda(f(A))=0$.
  Hint: The mean-value theorem may turn out to be useful. What can we say about the measurability of $f(A)$? Its 2 a.m. now and I've tried for the last few hours to find a way to prove the statement, but nothing has worked so far. I don't think that $f(A)$ is always countable and I'm sure that $A$ can sometimes be uncountable. Thanks for any advice!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1570221,Why does $\sqrt{x^2}$ seem to equal $x$ and not $|x|$ when you multiply the exponents?,"I understand that $\sqrt{x^2} = |x|$ because the principal square root is positive. But since $\sqrt x = x^{\frac{1}{2}}$ shouldn't $\sqrt{x^2} = (x^2)^{\frac{1}{2}} = x^{\frac{2}{2}} = x$ because of the exponents multiplying together? Also, doesn't $(\sqrt{x})^2$ preserve the sign of $x$? But shouldn't $(\sqrt{x})^2 = (\sqrt{x})(\sqrt{x}) = \sqrt{x^2}$? How do I reconcile all this? What rules am I not aware of? Edit: Since someone voted to close my question, I should probably explain the difference between my question and Proving square root of a square is the same as absolute value , regardless of how much I think the difference should be obvious to anyone who reads the questions. Cole Johnson was asking if there's any way to prove that $\sqrt{x^2} = |x|$. I am not asking that; I already accept the equation as fact. I'm asking how to resolve some apparent contradictions that arise when considering square roots of squares, and how I should approach these types of problems. (Cameron, please read.)","['algebra-precalculus', 'absolute-value']"
1570223,How to prove that $Res(fg;a)=g(a)Res(f;a)$?,"Suppose that $f$ has a simple pole at $z=a$ and let $g$ be analytic in an open set containing $a$. Show that $Res(fg;a)=g(a)Res(f;a)$. I know that as $f$ has a simple pole at $z=a$, this means its Laurent series is of the form $f(z)=\dfrac{Res(f;a)}{z-a}+\displaystyle\sum_{n=0}^{\infty}a_n(z-a)^n$ How can I compute the Laurent series of $fg$ at $z=a$?",['complex-analysis']
1570225,Why group of order 6 has to have just two elements of order 3,"In an attempt to prove that every group $G$ of order 6 is isomorphic to either $\mathbb{Z}_6$ or $S_3$, I stumbled upon one peculiar issue. We can use Cauchy's Theorem to argue that since $|G|=3\cdot2$, $G$ must necessarily have elements of orders 1, 2, and 3. Another possibility is that there exists an element of order 6 in $G$. Now, suppose this is true, then $G$ is cyclic of order 6, which implies that $G\cong \mathbb{Z}_6$. Alternatively, suppose that $G$ contains only elements of orders 1, 2, and 3. Then... (and here's where it appears to be the most difficult part) There must necessarily be only two elements of order 3. For if there are more than two elements of order 3 then for some $a\in G$ s.t. $a^3=1$, $a^2=b$, where $|b|=2$ or $|b|=3$. Suppose that $b$ has order 2. Then $a^2a=1$ implies that $a^2 = a^{-1}$, thus $|b|\ne 2$. We conclude that $b$ has order 3. Then $a_1^2 = b_1$, $a_2^2=b_2$, $a_3^2=b_1$ or $a_3^2=b_2$. But this implies that $a_3=a_1$ or $a_3 = a_2$. Hence, $G$ must contain two elements of order 3. We can now define a bijective homomorphism between $G$ and $S_3$, which implies that $G\cong S_3$. I'm wondering, however, if there's a simpler way to prove that there must necessarily be exactly two elements of order 3 in $G$.","['finite-groups', 'abstract-algebra', 'cyclic-groups', 'group-theory', 'symmetric-groups']"
1570233,Why exactly does a function need to be continuous on a closed interval for the intermediate value theorem to apply?,"I apologize if this question is too basic. The intermediate value theorem states that if $f$ is continuous on a closed interval $[a,b]$, then for every value $c$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$. This, or some very similar variant thereof, is how the intermediate value is usually presented in textbooks. What bugs me, however, is the condition that $f$ need be continuous on the closed interval $[a,b]$ rather than the less strict condition of only being continuous on the open interval $(a,b)$. To illustrate this point, consider $f:[-1,1] \rightarrow \mathbb{R}$ where $f(x)= e^x$. This is only continuous on the open interval $(-1,1)$ but surely the IVT applies to it. A less artificial example would be the inverse sine function. Would this reasoning not apply to all such (continuous/well-behaved) functions? As an alternative, wouldn't the following definition from Proofwiki be superior (in that it is slightly more general)? Let $I$ be a real interval. Let $a,b \in I$ such that $(a,b)$ is an open interval. Let $f:I \rightarrow \mathbb{R}$ be a real function continuous in $(a,b)$. Then for every value $c \in \mathbb{R}$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$","['real-analysis', 'calculus']"
1570249,"Computing $ \int_0^{2\pi} \frac{1}{a^2\cos^2 t+b^2 \sin^2 t} dt \;; a,b>0$.","Using Residue Theorem find $\displaystyle \int_0^{2\pi} \frac{1}{a^2\cos^2 t+b^2 \sin^2 t} dt \;; a,b>0$. My Try: So, I am going to use the ellipse $\Gamma = \{a\cos t+i b \sin t: 0\leq t\leq 2\pi\}$. On $\Gamma$, $z=a\cos t+i b \sin t$, so $|z|^2=z\bar{z}=a^2\cos^2 t+b^2 \sin^2 t$. Now, $dz=-a\sin t+i b \cos t dt$. Hence, the integral becomes $\displaystyle \int_\Gamma \frac{dz}{z(iab+(\sin t \cos t)(b^2-a^2))}$. I know that $\displaystyle \int_\Gamma \frac{dz}{z}=2\pi i$. Now, how do I get rid of $\sin t \cos t$ part? I am stuck here. Can somebody please explain how?","['residue-calculus', 'calculus', 'complex-analysis', 'integration', 'contour-integration']"
1570252,When does Rudin's change of variables formula break?,"Here is Rudin's change of variables Theorem: My question is this: what are examples where $\varphi$ not being strictly increasing or not mapping the interval $[a,b]$ to $[A,B]$ breaks the theorem? For example, I am considering $\varphi$ a parabola with minimum point in the unit interval. Using Rudin's special case remark ($\alpha(x)=x$ and $\beta(x)=\varphi$) I keep computing
$$\int_0^1 f(x)dx=\int_0^1f(\varphi(y))\varphi'(y)dy$$
with various choices of $f$, and I keep getting these integrals are equal. Does this mean $\varphi$ doesn't need such strict conditions? If we let $a$ be the minimum point this seems surprising since $\varphi$ is only strictly increasing on $[a,1]$.","['real-analysis', 'definite-integrals']"
1570276,Stalks and direct image,"Let $f: X \rightarrow Y$ be a continuous map of topological spaces, and $F$ a sheaf of rings on $X$.  The direct image sheaf $f_{\ast}F$ on $Y$ is given by the formula $V \mapsto F(f^{-1}V)$.  If $x \in X$, is it true in general that $F_x \cong (f_{\ast}F)_{f(x)}$? We have $$(f_{\ast}F)_{f(x)} = \varinjlim\limits_{V \ni f(x)} F(f^{-1}V) = \varinjlim\limits_{f^{-1}V \ni x} F(f^{-1}V)$$ so it appears that this limit equals $F_x = \varinjlim\limits_{U \ni x} F(U)$ if for any neighborhood $U$ of $x$, there exists a neighborhood $V$ of $f(x)$ such that $f^{-1}V \subseteq U$. Obviously this last statement isn't true for all continuous maps.  For example I could take $X = \{a,b\}$ in the discrete topology, $Y = \{a,b\}$ in the indiscrete topology, and $f: X \rightarrow Y$ the map $a,b \mapsto a$.  Then I can take $U = \{a\}$ (and $x = a$). It appears that there is at least a canonical homomorphism $(f_{\ast}F)_{f(x)} \rightarrow F_x$.","['sheaf-theory', 'algebraic-geometry']"
1570277,Integer Lattice Points,"Let $(n_1,m_1),(n_2,m_2),. . .,(n_9,m_9)$ be integer lattice points in the plane (ie. $n_i$ and $m_i$ are integers). Show that the midpoint of the line joining some pair of points is also an integer lattice point. I think that I need to use the pigeonhole principle but I'm not sure how to get to that point.","['pigeonhole-principle', 'discrete-mathematics']"
1570280,Is $(\nabla \times \nabla)$ an operator?,"Is $(\nabla \times \nabla)$ an operator? I am wondering if its possible to compute
\begin{align}
\vec{f} \cdot (\nabla \times \nabla)
\end{align} Where $f$ is a vector. I would be interested in both cartesian and polar expressions.","['derivatives', 'partial-derivative', 'vectors', 'vector-analysis']"
1570288,Definition of a morphism of locally ringed spaces,"Let $(X, \mathcal O_X), (Y, \mathcal O_Y)$ be locally ringed spaces.  A morphism of ringed spaces is defined to be a pair $(f,f^{\#}):(X, \mathcal O_X) \rightarrow (Y, \mathcal O_Y)$, where $f:X \rightarrow Y$ is continuous, and $f^{\#}: \mathcal O_Y \rightarrow f_{\ast} \mathcal O_X$ is a morphism of sheaves.  We consider $(f,f^{\#})$ to also be a morphism of locally ringed spaces if for each $x \in X$, the homomorphism on the stalks $$f_x^{\#}: \mathcal O_{Y,f(x)} \rightarrow \mathcal O_{X,x}$$ is a local homomorphism (preimage of the unique maximal ideal remains maximal).  My question is, what exactly is the map $f_{x}^{\#}$?  I know since $f^{\#}$ is a morphism of sheaves, we have a homomorphism on the stalks $$f_{f(x)}^{\#}: \mathcal O_{Y,f(x)} \rightarrow (f_{\ast} \mathcal O_X)_{f(x)}$$ Are we getting $f_x^{\#}$ by composing $f_{f(x)}^{\#}$ with some homomorphism $(f_{\ast} \mathcal O_X)_{f(x)} \rightarrow \mathcal O_{X,x}$?","['ringed-spaces', 'sheaf-theory', 'algebraic-geometry']"
1570302,Fibonacci identity $F_{n+1}^2 - (F_{n+1}F_n) - F_n^2 = (-1)^n$,I am trying to prove Let $F_n$ be the $n$th Fibonacci number. Then $F_{n+1}^2 - F_{n+1}F_n - F_n^2 = (-1)^n$ I am not sure where to start with this.,"['proof-writing', 'fibonacci-numbers', 'discrete-mathematics']"
1570309,How do I define a morphism of sheaves $G \mapsto f_{\ast}(f^{-1}G)$?,"Let $f:X \rightarrow Y$ be a continuous map of topological spaces, and $G$ a sheaf on $Y$.  I know that the association $G \mapsto f^{-1}G$, and $F \mapsto f_{\ast}F$ are covariant functors from the category of sheaves on $Y$ to the category of sheaves on $X$, and back again.  Hence the composition $G \mapsto f^{-1}G \mapsto f_{\ast}f^{-1}G$ is a covariant functor from from the category of sheaves on $Y$ to itself.  I'm having trouble seeing why there is a natural morphism of sheaves $$G \mapsto f_{\ast}(f^{-1}G)$$ In other words, why there is a natural transformation from the identity functor $Sh_Y \rightarrow Sh_Y$ to the functor $Sh_Y \rightarrow Sh_Y$ given by $G \mapsto f_{\ast}(f^{-1}G)$.  Maybe I'm missing something because I'm very rusty on adjoints.  I just don't have any concrete description of $f^{-1}G$, so I don't know how to define a homomorphism $G(V) \rightarrow f_{\ast}(f^{-1}G))(V) = f^{-1}G(f^{-1}V)$.",['algebraic-geometry']
1570331,Show that every Banach space is isometric to a subspace of an $L_{\infty}(\mu)$-space,"The following statement can be obtain in the proof of Proposition $7.1$ Since every Banach space is isometric to a subspace of an $L_{\infty}(\mu)$-space (e.g. an $l_{\infty}(T)$ for suitable $T$), the proposition is trivial if $p=\infty$. Question: How to show that 'every Banach space is isometric to a subspace of an $L_{\infty}(\mu)$-space'?","['functional-analysis', 'banach-spaces', 'measure-theory']"
1570337,a continuous path between two sobolev functions without increasing energy,"This question has been post on MO a week ago. I move it here to get more luck. Let $\Omega\subset \mathbb R^N$ be open bounded, smooth boundary. Let $u_1$, $u_2\in H^{1}(\Omega)$ such that $T[u_1]=T[u_2]=T[\omega]$ where $T$ stands for the trace operator and $\omega\in H^1(\Omega)$ is a fixed function. Define
$$
F(u):=\inf_{v\in\mathcal V}\left\{\int_\Omega |\nabla u|^2v^2dx + \int_\Omega |\nabla v|^2+(1-v)^2dx \right\},
$$
where $\mathcal V:=\{v\in H^1(\Omega),\,0\leq v\leq 1\}$. Question: does there exist a path $a(t): [0,1]\to H^1(\Omega)$ between $u_1$ and $u_2$ satisfies the following conditions? $a(0)=u_1$, $a(1)=u_2$ $T[a(t)]=T[\omega]$ for all $t\in (0,1)$ $a(t)$ is continuous in $L^2$ sense, i.e., if $t\to t_0$, then $a(t)\to a(t_0)$ in $L^2$. $F(a(t))\leq \max\{F(u_1),F(u_2)\}$, for all $t\in (0,1)$. Any help, hint, or reference would be really welcome! Update: based on @Jason's answer, I wrote, for arbitrary $v\in\mathcal V$,
$$
F(a(t))\leq G(a(t),v)\leq tG(u_1,v)+(1-t)G(u_2,v).
$$
Let's denote by $v_1$ and $v_2$ that $F(u_1)=G(u_1,v_1)$ and $F(u_2)=G(u_2,v_2)$, such $v_1$ and $v_2$ exists and unique by the properties of $H^1$ function. I understand that $v$ on the right hand side is arbitrary so we may replace it with either $v_1$ or $v_2$ and we have
$$
F(a(t))\leq tG(u_1,v_1)+(1-t)G(u_2,v_1)=t F(u_1)+(1-t)G(u_2,v_1)\tag 1
$$
or
$$
F(a(t))\leq tG(u_1,v_2)+(1-t)F(u_2）
$$
But we may can not go further from here. Take, from example, $(1)$. We can not switch $v_1$ by $v_2$ here since if we do, we will change $F(u_1)$ as well.","['functional-analysis', 'real-analysis', 'calculus-of-variations', 'sobolev-spaces']"
1570364,Prove inequality using binomial theorem,"I have this math question that I'm kind of stuck on. Use the binomial theorem to prove that for all integers $n\ge
 2$:$$\left (1+\frac{1}{n}\right )^n < \sum_{j=0}^{n}{\frac{1}{j!}} <
 2+\frac{1}{2}+\frac{1}{4}+\cdots +\frac{1}{2^{n-1}}$$ I know applying the binomial theorem I get:$$\sum_{j=0}^{n}{\binom{n}{j}\left ( \frac{1}{n}\right )^{n-j}} < \sum_{j=0}^{n}{\frac{1}{j!}} <
 2+\frac{1}{2}+\frac{1}{4}+\cdots +\frac{1}{2^{n-1}}$$
$$\implies \sum_{j=0}^{n}{\left ( \frac{\binom{n}{j}}{n^{n-j}}\right )} < \sum_{j=0}^{n}{\frac{1}{j!}} <
 2+\frac{1}{2}+\frac{1}{4}+\cdots +\frac{1}{2^{n-1}}$$
$$\implies \sum_{j=0}^{n}{\left ( \frac{n!}{n^{n-j}\cdot j!\cdot (n-j)!} \right )} < \sum_{j=0}^{n}{\frac{1}{j!}} <
 2+\frac{1}{2}+\frac{1}{4}+\cdots +\frac{1}{2^{n-1}}$$ I'm not really sure what to do from here... Thanks.","['induction', 'binomial-theorem', 'sequences-and-series', 'discrete-mathematics']"
1570366,The minimum value of $ f(x) = | x - 1 | + | x - 2 | + | x - 3 | $ is? [duplicate],"This question already has answers here : Maximum and minimum of of $f(x)=|x-1|+|x-2|+|x-3|$ (3 answers) Closed 6 years ago . I don't get it why my solution is wrong : My solving : $ f(x) = | x - 1 | + | x - 2 | + | x - 3 | $ When $ x\leq 1 $ $ f(x) = | x - 1 | + | x - 2 | + | x - 3 | = 0 $ = $ 6 -3x $ since $ x\leq 1 $ $ f(x)\leq 3 $    also it clear $ f(x)\geq 0 $ => The min value is 0 . I dont have to consider other cases, since 0 is the min value f(x) can take . Book has given the answer : $ 2 $ .","['inequality', 'convexity-inequality', 'optimization', 'algebra-precalculus', 'absolute-value']"
1570465,"Set of single objects and pairs, how to treat elements?","I have an assignment, where we have to simulate Turing machine computation with something completely different and I have get into the point where I need some set to be defined like this: $Q = \Gamma \cup (Q_{M} \times \Gamma)$ where $\Gamma$ and $Q_{M}$ are just some sets of single objects. So $Q$ contains single objects and also pairs. Is this valid? I don't remember that I have ever used something like this for 4 years on university, so I am not completely sure whether it is valid or not. If it is valid, how to properly treat elements in formal definitions? For example, I want to specify some subset $A \subseteq Q$ that contains only elements that are pairs and they hold some predicate. Is it OK to do it like this? $A = \{(x,y) \in Q | xRy \}$ Or do I have to be more specific and not forget about single object elements?",['elementary-set-theory']
1570470,How to obtain this partial fraction decomposition?,"I am studying Laplace transforms right now and got stuck at this step that involves a weird partial fraction decomposition. It looks like the instructor skipped a bunch of steps and assigned numerators to a bunch of the fractions without assigning them dummy variables. Any idea how he got to this step so quickly? I tried doing this the standard way (i.e. breaking each polynomial in the denominator into its own fraction, assigning variables for each numerator, and trying to solve), but it got messy very quickly. laplace transform up to the part where I got stuck $$
X=\frac{2}{s^4-1}\left(e^{-s}\cdot\frac{1}{s}-e^{-2s}\cdot\frac{1}{s}\right)=\frac{2}{\left(s^4-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\cdot\frac{1}{\left(s^2+1\right)\left(s+1\right)\left(s-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\left(\frac{As+B}{s^2+1}+\frac{\left(\frac{1}{4}\right)}{\left(s+1\right)}+\frac{\left(\frac{1}{4}\right)}{s-1}+\frac{\left(-1\right)}{s}\right)\left(e^{-s}-e^{-2s}\right)
$$","['partial-fractions', 'ordinary-differential-equations', 'laplace-transform']"
1570503,Sign of first eigenvalue of conformal Laplacian,"Let $(M^n,g)$ be some manifold of dimension $n \geq 3$. The conformal Laplacian is given by
$L=-4 \frac{n-1}{n-2} \Delta+ R$, where $R$ is the scalar curvature of $M$ and $\Delta= -\operatorname{div}\circ \operatorname{grad}$. Now assume that $g$ is a Riemannian metric, $M$ is orientable, compact and has no boundary.
  Show that the sign of the first eigenvalue of $L$ is a conformal invariant. Why is that so?
I was able to show some transformation law for $L$.
Namely, if $\tilde{g}=v^{4/(n-2)}g$, then for some function $\varphi >0$ we have
$L(\varphi)=v^{\frac{n+2}{n-2}} \tilde{L} (v^{-1} \varphi)$.
(Here $\tilde{L}$ is the conformal Laplacian with respect to the metric $\tilde{g}$) I originaly hoped I could conclude something similar to: $(\varphi, \lambda)$ eigenpair for $L$ $\Leftrightarrow$ $(v^{-1} \varphi, \lambda \cdot \max v)$ eigenpair for $\tilde{L}$. However I did not have any luck with that so far.
(And obviously the above line is NOT true. I just wanted to give an impression what kind of statement I was looking for) How could I go about this exercise?","['eigenvalues-eigenvectors', 'conformal-geometry', 'differential-geometry', 'partial-differential-equations']"
1570513,Would you ever stop rolling the die? [duplicate],"This question already has an answer here : Toss a fair die until the cumulative sum is a perfect square-Expected Value (1 answer) Closed 9 years ago . You have a six-sided die. You keep a cumulative total of your dice rolls. (E.g. if you roll a 3, then a 5, then a 2, your cumulative total is 10.) If your cumulative total is ever equal to a perfect square, then you lose, and you go home with nothing. Otherwise, you can choose to go home with a payout of your cumulative total, or to roll the die again. My question is about the optimal strategy for this game. In particular, this means that I am looking for an answer to this question: if my cumulative total is $n$, do I choose to roll or not to roll in order to maximize my cumulative total? Is there some integer $N$ after which the answer to this question is always to roll? I think that there is such an integer, and I conjecture that this integer is $4$. My reasoning is that the square numbers become sufficiently sparse for the expected value to always be in increased by rolling the die again. As an example, suppose your cumulative total is $35$. Rolling a $1$ and hitting 36 means we go home with nothing, so the expected value of rolling once is: $$E(Roll|35) = \frac 0 6 + \frac {37} 6 + \frac {38} 6 + \frac{39} 6 + \frac {40} {6} + \frac{41}{6} = 32.5$$ i.e. $$E(Roll|35) = \frac 1 6 \cdot (37 + 38 + 39 + 40 + 41) = 32.5$$ But the next square after $35$ is $49$. So in the event that we don't roll a $36$, we get to keep rolling the die at no risk as long as the cumulative total is less than $42$. For the sake of simplification, let's say that if we roll and don't hit $36$, then we will roll once more. That die-roll has an expected value of $3.5$. This means the expected value of rolling on $35$ is: $$E(Roll|35) = \frac 1 6 \cdot (40.5 + 41.5 + 42.5 + 43.5 + 44.5) = 35.42$$ And since $35.42 > 35$, the profit-maximizing choice is to roll again. And this strategy can be applied for every total. I don't see when this would cease to be the reasonable move, though I haven't attempted to verify it computationally. I intuitively think about this in terms of diverging sequences. I recently had this question in a job interview, and thought it was quite interesting. (And counter-intuitive, since this profit-maximizing strategy invariably results in going home with nothing.)","['expectation', 'probability', 'gambling', 'discrete-mathematics']"
1570526,Computing the Jordan Form of a Matrix,"I apologize if this has already been answered, but I've seen multiple examples of how to compute Jordan Canonical Forms of a matrix, and I still don't really get it. Could someone help me out with this? What I know for certain is that I must start off by finding my eigenvalues, and corresponding eigenvectors. OR, (how it was taught in class from my understanding), I can simply plug in the eigenvalues into my original matrix and find the rank. I have no clue what to do from there though... I also know that my Jordan Normal Forms should look like these: $$\begin{pmatrix}
\lambda_1 & 0 & 0\\
0 & \lambda_2 & 0\\
0 & 0 & \lambda_3\\
\end{pmatrix}$$ or $$\begin{pmatrix}
\lambda_1 & 1 & 0\\
0 & \lambda_1 & 0\\
0 & 0 & \lambda_2\\
\end{pmatrix}$$ And if we switch 1 and 2, then the 1 will be on the other side of the top. Lastly, $$\begin{pmatrix}
\lambda & 1 & 0\\
0 & \lambda & 1\\
0 & 0 & \lambda\\
\end{pmatrix}$$ I've seen from many sources that if given a matrix J (specifically 3x3) that is our Jordan normal form, and we have our matrix A, then there is some P such that $PAP^{-1}=J$. Here's an example matrix if I could possibly get an explanation on how this works through an example: $$\begin{pmatrix}
-7 & 8 & 2\\
-4 & 5 & 1\\
-23 & 21 & 7\\
\end{pmatrix}$$ I don't know how to fill the information in the middle. For instance, what do I do after I find the rank of my matrix or what do I do once I find my rank? Sorry if I made mistakes, very tired, and please try to make this as coherent as possible, because I'm so confused. This is an Advanced Linear Algebra course. Any help is greatly appreciated!","['matrices', 'linear-algebra']"
1570559,What is the probability that for exactly three calls the lines are occupied?,"I am unable to understand the following statement. The phone lines to an airline reservation system are occupied 40% of the time. Assume that the events that the lines are occupied on successive calls are independent. Assume that 10 calls are placed to the airline. $\textbf{What is the probability that for exactly three calls the lines are occupied?}$ I am not exactly confused with how I will find the answer. I am simply having difficulties understanding the question. Here is my current understanding of the statement: Each of the phone lines of an airline reservation system are occupied 40% of the time on a certain time interval. The probability that one phone line is occupied is independent of the vacancy of other phone lines, thus, the probability that any phone lines will be occupied at a given time interval will always be 40%. $\textbf{The airline has 10 phone lines.}$ The last sentence stating ""Assume that 10 calls are placed to the airline"" confuses me. Does this mean that the airline has 10 phone lines? Or are there 10 phone calls to the airline. The latter interpretation seems to be pointless however. I just want to make sure that I am not missing something.","['statistics', 'binomial-distribution', 'probability', 'probability-distributions']"
1570574,A pair of non group homeomorphic topological groups,"The following assertion is true: If $A$ and $B$ are dense group homeomorphic subgroups of complete Hausdorff topological groups $X$ and $Y$ respectively, then $X$ and $Y$ are group homeomorphic. I wonder if we can find $A$ and $B$ such that $A$ is homeomorphic to $B$, but not group homomorphic and that $A$ and $B$ are dense in topological groups $X$ and $Y$ respectively, but $X$ and $Y$ are not homeomorphic. Then what about $X$ and $Y$ are homeomorphic but not group homomorphic? The the topological groups are assumed to be commutative, by group homeomorphic we mean that (topological) homeomorphic that is also (group) homomorphic.","['general-topology', 'topological-groups']"
1570581,$f:X\rightarrow Y$ is a closed immersion iff $f:f^{-1}(U_i)\rightarrow U_i$ is a closed immersion.,"I came across the following property of closed immersions on Wikipedia - A morphism $f:Z\rightarrow X$ is a closed immersion iff for some
  (equivalently every) open covering $X=\bigcup U_j$ the induced map
  $f:f^{-1}(U_j)\rightarrow U_j$ is a closed immersion. I am having trouble with the ""equivalenty every cover"" part - assuming that the property of a morphism being a closed immersion holds for some open cover iff it holds for every cover I am able to prove the above result as follows - If $f$ is a closed immersion then cover $X$ just by $X$ and we have found ""some"" cover of $X$ satisfying the induced map is a closed immersion.
Conversely, if there is ""some"" cover for which the induced map is a closed immersion then as it is true (as per the assumption I made) for every cover, we can take the cover to be $X$ and are done. (I hope this is correct!) Now all I have to do is prove the assumption. But I have no idea where to start. I know this lemma - Let $X$ be a scheme and let $\mathcal P$ be a property. Suppose $\mathcal P$ satisfies the following conditions - If $\mathcal P$ is true for $\operatorname{Spec }R$ then it is true for $\operatorname{Spec }R_g$ for every $g\in R$ If $\langle g_1,\cdots,g_n\rangle=R$ and $\mathcal P$ is true for each $\operatorname{Spec }R_{g_i}$ then $\mathcal P$ is true for $\operatorname{Spec }R$ Let $X=\bigcup U_i$ be an affine open cover of $X$ and suppose $\mathcal P$ is true for each $U_i$ then $\mathcal P$ is true for every affine open subset of $X$. But this Lemma can only be applied for an affine open cover and Wikipedia's statement about closed immersions is for every open cover so I don't know how to prove it. Any help will be greatly appreciated. Thank you.","['schemes', 'algebraic-geometry']"
1570589,Prove the convergence of a random variable,"Given $x_n \sim N(0, \frac{1}{n})$, is $x_n$ almost sure convergence or convergence in probability or convergence in distribution ? How to prove it","['borel-cantelli-lemmas', 'probability-theory', 'convergence-divergence']"
1570590,Determining the number of zeros in the upper half plane,"I have the following function:$$z^4+3iz^2+z-2+i$$ I need to find the number of zeros in the upper half plane. I wonder how one should go about solving such a problem. Help would be greatly appreciated! P.S. This question has been asked before, but the answer in that question is sloppy, the links provided do not work. Thanks!","['complex-analysis', 'complex-numbers', 'functions']"
1570602,Green's Theorem with respect to a given polar region.,"Using Green's Theorem, compute the counterclockwise circulation $I$ of
  $\vec{F}=\langle-\sqrt{x^2+y^2},\sqrt{x^2+y^2}\rangle$ around the
  region defined by the polar coordinate inequalities $7 \leq r \leq 8$
  and $0 \leq \theta \leq \pi$. My inclination is to approach as follows:
$$
\\ \displaystyle
\\ Q =  \sqrt{x^2+y^2} \implies Q_x = \dfrac{ x}{\sqrt{x^2+y^2}}
\\ P = -\sqrt{x^2+y^2} \implies P_y = \dfrac{-y}{\sqrt{x^2+y^2}}
\\ 
\\ 
\\ x = r\cos{\theta}
\\ y = r\sin{\theta}
\\ \implies \vec{F} = \langle -r, r \rangle
\\ \implies I = \int\int_R \left(Q_x-P_y\right) \mathrm{d}A = \int_0^\pi \int_7^8 \frac{r^2(\cos{\theta}+\sin{\theta})}{r}\mathrm{d}r\mathrm{d}\theta
\\ \implies I = \int_0^\pi (\cos{\theta} + \sin{\theta}) \ \mathrm{d}\theta \cdot \int_7^8 r \ \mathrm{d}r
$$
Evaluate and find $I = -15$. Is this effective, valid, both, or neither?","['polar-coordinates', 'problem-solving', 'calculus', 'multivariable-calculus', 'integration']"
1570623,"""Facts"" that hold in Hausdorff spaces but do not hold in general?","It seems that there exists a number of important results that are very useful when dealing with Hausdorff spaces, but that are not true in general.  I was wondering if we can provide a list of such facts (with proofs) in this page, so that it can be used for future references? To be more specific, what are some relations that hold when we are dealing with Hausdorff spaces, and that are very useful in measure theory? For example, in class when proving Lusin's theorem, we used some statements, like a compact set contains all its points of closure, the intersection of any number of compact sets is compact etc, which left some students confused.","['general-topology', 'measure-theory']"
1570646,"Solve the Recurrence relation : $a_n= (a_{n-1})^3a_{n-2}$ , $a_0=1, a_1=3$","$a_n= (a_{n-1})^3a_{n-2}$  , $a_0=1, a_1=3$ I'm ask to get an expression for $a_n$. So i tried to solve with induction: $a_n=(a_{n-1})^3a_{n-2}=(a_{n-1})^3(a_{n-3})^3a_{n-4}=(a_{n-1})^3(a_{n-3})^3(a_{n-5})^3 \ldots (a_1)^3a_0$ So $a_n= \prod _{k=1}^n a_{n-k}$ , 
(k is an odd number, so all odd numbers from 1 to n) Is this the expression? i feel like this is still a reccurence relation, solving with the methods of linear-homog will not work cause this relation is non-linear. any help would be appreciated!","['combinatorics', 'recurrence-relations']"
1570758,Condition on output of Lyapunov equation being positive definite,"I have Matlab code that solves the Lyapunov equation $$AX + XA^T + Q = 0$$ for a 3-D array of matrices, $A$ , using the Matlab function lyap(A,Q) . My problem is that sometimes the resultant matrix, $X$ , is positive definite and sometimes it is not. My question is given that the matrix $Q$ is positive definite, what is the condition on $A$ that the resultant matrix, $X$ , is always positive definite?","['matrices', 'matrix-equations', 'positive-definite', 'linear-algebra']"
1570793,$\pi$ in imaginary numbers?,"Look at the result of  $(-1)^{1/10000000}$ on the google calculator. You should get $$1 + 3.14159265 \times 10^{-7} i$$
Why does $\pi$ occur in imaginary number operations that don't include $\pi$?","['number-theory', 'complex-numbers']"
1570824,A sequence converging to 0 in probability times a sequence bounded in probability,"I'm trying to prove the following from Lehman's ""Elements of Large Sample Theory"" Lemma 2.3.1 : If the sequence $\{Y_n, n=1,2,\ldots\}$ is bounded in probability and if $\{C_n\}$ is a sequence of random variables tending to $0$ in probability, then $C_n Y_n \xrightarrow{P} 0 $ (The notation ""$\xrightarrow{P}$"" indicates convergence in probability.) Here was my initial attack: We know that for any epsilon, sufficiently large $n$ and suitable $K$ that (1) $$P\left(| Y_n | \leq K\right) > 1 - \epsilon_0$$ Similarly, we know that for any $\epsilon_1, \epsilon_2 > 0$, sufficiently large $n$ that (2) $$
P\left(|C_n| < \frac{\epsilon_1}{K}\right) > 1 - \epsilon_2
$$ Here's where I run into trouble. If we assume independence of the two sequences , we should be able to multiply the probabilities (1) and (2) above to get: $$
P(|Y_n C_n| < \epsilon_1) > (1-\epsilon_0) (1-\epsilon_2) \to 1
$$ Since the choices of epsilons were arbitrary. My problem is that what if we can't assume independence ? So this proof strikes me as incorrect but probably my heart is in the right place. Any help is much appreciated. I'll think about it myself and make edits if I make any progress. UPDATE 1: I'm beginning to think that instead of considering $P((1) \land (2))$, we should perhaps consider$\overline {P((1) \land (2))}$, since $\overline {P((1) \land (2))} < P(\overline{(1)}) + P(\overline{(2)}$. Now we should be able to show $ P(\overline{(1)}) + P(\overline{(2)} \to 0$, without assuming anything about independence.","['probability-theory', 'asymptotics', 'random-variables']"
1570834,On products of ternary quadratic forms $\prod_{i=1}^3 (ax_i^2+by_i^2+cz_i^2) = ax_0^2+by_0^2+cz_0^2$,"The equation, $$ (ax_1^2+by_1^2)(ax_2^2+by_2^2) = ax_0^2+by_0^2\tag1$$ has the well-known solution when $a=b=1$, $$ (x_1^2+y_1^2)(x_2^2+y_2^2) = (x_1 y_2 + x_2 y_1)^2 + (x_1 x_2 - y_1 y_2)^2$$ Hence the product of the sum of two squares is itself the sum of two squares.
If we use one more factor , then it has an identity for general $a,b$, namely, $$ (ax_1^2+by_1^2)(ax_2^2+by_2^2)(ax_3^2+by_3^2) = ax_0^2+by_0^2\tag2$$ where, $$x_0 =a \color{blue}{x_1 x_2 x_3} + b \big(-\color{blue}{x_1} y_2 y_3 + \color{blue}{x_2} y_1 y_3 + \color{blue}{x_3} y_1 y_2 \big)$$ $$y_0 =a \big(-x_2 x_3 \color{brown}{y_1} + x_1 x_3 \color{brown}{y_2} + x_1 x_2 \color{brown}{y_3}\big) + b \color{brown}{y_1 y_2 y_3}$$ High-lighted this way, one can immediately see the pattern. Q: Is there a similar identity for ternary quadratic forms, $$ (ax_1^2+by_1^2+cz_1^2)(ax_2^2+by_2^2+cz_2^2)(ax_3^2+by_3^2+cz_3^2) = ax_0^2+by_0^2+cz_0^2\tag3$$ such that $x_0, y_0, z_0$ are integer functions in terms of the other $x_i, y_i, z_i$ just like for $(2)$?","['number-theory', 'diophantine-equations', 'quadratic-forms', 'algebraic-number-theory']"
1570836,"Does $\Bbb E[X|Z]=\Bbb E[Y|Z]$ if $X,Y$ are identically distributed random variable?","Does $\Bbb E[X|Z]=\Bbb E[Y|Z]$ if $X,Y$ are identically distributed random variable, where $Z$ is a third random variable? Thank you!",['probability']
1570856,Prove that $(\mu \times \mu)(G) = 0$ where $\mu$ is the Lebesgue measure using Fubini,"Suppose $f: [0,1] \rightarrow [0,1]$ is a Borel measurable function. Suppose $G = \{(x,y)\mid f(x) = y\} \subset [0,1] \times [0,1]$. Use Fubini to prove that 
  $$
(\mu \times \mu)(G) = 0.
$$ What I've tried: $$(\mu \times \mu)(G) = \iint_{G}\  d\mu d\mu = \iint 1_G(x,y)\  d\mu d\mu.$$ I kind of get stuck here, how am i supposed to go further, is it for example smart to take the rieman integral, or can i say the following thing: $$G \subset \{ 0\leq x \leq1, f(x)\leq y \leq f(x) \}$$ so we see that $$\iint 1_G(x,y) d\mu d\mu \leq \iint 1_{\{ 0\leq x \leq1, f(x)\leq y \leq f(x) \}} = \int_{[0,1]} \int_{\{f(x)\}} 1_G(x,y) d\mu d\mu = 0.$$ Since $\{f(x)\}$ is a singleton...? Kees","['real-analysis', 'measure-theory']"
1570861,Sum of all the $x$- and $y$-intercepts of the graph of $f(x)$ is equal to the sum of all the $x$- and $y$-intercepts of the graph of $f^{-1}(x)$,"The function $f$ is  one-to-one. Prove that the sum of all the $x$- and $y$-intercepts of the graph of $f(x)$ is equal to the sum of all the $x$- and $y$-intercepts of the graph of $f^{-1}(x)$. The given statement seems obvious and true but I dont know how to prove it theoretically. I tried: Let $x_1,f(0)$ be the $x$-intercept and $y$-intercept of the graph of the function $f(x)$. The sum of all the $x$- and $y$-intercepts of the graph of $f(x)$ is: $$x_1+f(0)$$
I dont know how to solve it further. I am stuck here.","['inverse-function', 'calculus', 'functions']"
1570900,Locally compact Hausdorff topological vector spaces are finite-dimensional,"It's an important fact that locally compact Hausdorff topological vector spaces are finite-dimensional, a proof can be found here . I'm somewhat stuck with the proof. If ${U}$ is a neighbourhood of the origin. then for every ${x \in V}$ we see that ${2^{-n} x \in U}$ for sufficiently large ${n}$. By compactness of ${K}$ (and continuity of the scalar multiplication map at zero), we conclude that ${2^{-n} K \subset U}$ for some sufficiently large ${n}$. Here $K$ is a compact neighborhood of the origin, From the above argument, I know that for every $x\in K$, we have $x\in 2^n U$ sufficiently large $n$, then by the compactness of $K$, we can find a finite collection of sets $\{2^{n_1}U,\cdots,2^{n_k}U\}$ such that 
$$K\subset 2^{n_1}U\cup\cdots\cup 2^{n_k}U.$$
But how can we say there exists a $n$ such that $K\subset 2^nU$?","['functional-analysis', 'real-analysis', 'topological-vector-spaces']"
1570915,Why the standard deviation of the sample mean is calculated as $\frac{\sigma}{\sqrt{n}}$?,"According to Wikipedia, the standard deviation of a sample mean is calculated as follows $$\frac{\sigma}{\sqrt{n}}$$ Why is that? Why do we need to divide the standard deviation of the population by the square root of $n$ (which should I think be the size of the sample)? Why should that make sense?","['statistics', 'standard-deviation']"
1570916,Limit of $2^{n} n!/n^{n}$ as $n \to \infty$,"Prove that the $\lim_{n \rightarrow \infty} \frac{2^{n} n!}{n^{n}} = 0$ $\rightarrow  \frac{2^{n} n!}{n^{n}} = $ $(\frac{2}{n})^{n} n!$ Its possible to say that $\lim_{n \rightarrow \infty} $$\frac{2}{n}$ is $0$ and because of this reason $\lim_{n \rightarrow \infty} $$(\frac{2}{n})^{n}$ is $0$ also ? 
And Because of this $\lim_{n \rightarrow \infty} \frac{2^{n} n!}{n^{n}} = 0$ ? Thanks.",['limits']
1570923,prove without induction that $\sum_{k=1}^n k^p=?$?,"I want to find the sum of $\sum_{k=1}^n k^p$ for $p\in \mathbb{Q}$. Is there an algebraic method to solve this? If you can also suggest good references for this question, it's good enough for me.","['reference-request', 'sequences-and-series']"
1570963,Show that $y^2=x^3+1$ has infinitely many solutions over $\mathbb Z_p$.,"I first compared it with how I would solve this over the real numbers. You would  say: $y^2=\alpha$ has a solution for all $\alpha>0$, of which there are infinitely many. $x^3+1>0$ for all $x>-1$, of which there are also infinitely many. However I can't seem to extend this way of thinking to $\mathbb Z_p$. I have a strong hunch that I need to use Hensel's Lemma in some way, but I just can't see how.","['abstract-algebra', 'p-adic-number-theory']"
1570968,Show $A\cup B =E \iff \overline A\subset B$,"let $A\;, B$ and $C$ be subsets of $E$. show that $$A\cup B =E \iff \overline A\subset B$$ Notation : Absolute complement of $A$:  $\overline A=\mathcal{C}_{E}^{A}=\{ x\in E \mid x\notin A \}$ My thoughts Method $1$ : Via  Logic theory $$A\cup B =E \iff \overline A\subset B$$ we can see that as Tautology in  logic theory and we can prove it by truth table:
$$(A\vee B)\iff ((\neg A)\to B)$$ \begin{array} {|c|}
\hline
A & B & (A\vee B )& \neg A & (\neg A \to B )& [(A\vee B)\iff ((\neg A)\to B) ]\\ \hline
T & T & T & F &  T &  T \\ \hline
T & F & T & F &  T &  T\\ \hline
F & T & T & T &  T &  T\\ \hline
F & F & F & T &  F &  T\\ \hline
\end{array} Method $2$ : Via  Venn diagram ( Venn diagram not rigorous so they are not really proofs ) I think I have two possibilities to draw Venn diagram for $A\cup B =E \implies \overline A\subset B$ $1$. $B=\overline A\qquad $ ($A\cup B=A\cup \overline A =E \implies \overline A\subset B$   ) $2$. $B=E\qquad $ ($A\cup B=A\cup E =E \implies \overline A\subset B$   ) Method $3$ First step : show that : $\overline A\subset B \implies A\cup B =E$ Suppose that $\overline A\subset B$ and let's prove that $ A\cup B =E$ we have $A \subset E$ and  $B \subset E$ then $$(A\cup B) \subset E\quad (1)$$ Conversely, let $x\in E $ we have two cases $x\in B $ or $x\notin B$ First case : if $x\in B$ then $x\in A\cup B $ Second case : if $x\notin B$ then $x\notin \overline A\quad  (\text{since}\; \overline A \subset B )$ thus $x\in A $ Therefore $x\in A\cup B $ so in both cases we have :
$$x\in E\implies  x\in A\cup B$$
Therefore
$$E\subset (A\cup B)\quad (2) $$
From $(1)$ and $(2)$ we conclude that $$E=A\cup B$$ Second step : show that : $A\cup B =E \implies  \overline A\subset B$ Suppose that $A\cup B=E$ and  and let's prove that $\overline A\subset B$
Let $x\in \overline A$ we have:
\begin{align*}
x\in \overline A &\implies x\in E \\
&\implies x\in A\cup B \\
&\implies x\in B (\;\text{since}\; x\notin A  )\\ 
x\in \overline A &\implies x\in B 
\end{align*}
Therefore 
 $$A\cup B =E \implies  \overline A\subset B$$ Conclusion: $$A\cup B =E \iff \overline A\subset B$$ Is my proof correct also I'm interested in more ways of proving it",['elementary-set-theory']
1570982,What is the rule of $1.96$ for estimating confidence intervals?,"I have a sequence of A/B currency exchanges for some days. With that data I can calculate the daily returns, and that's what I did. I need to calculate the confidence interval for the expected daily returns of the A/B currency exchange by using the $1.96$ rule. What is this $1.96$ rule? Why exactly that number? Why is it related to compute confidence intervals? So, how can we use it in general to compute confidence intervals? There's an article on Wikipedia, but honestly I am not understanding it, and why it is related to the calculation of the confidence interval of the expectation. Note that for now I am not asking specifically about how to solve my problem, but how what I am asking about is related to my problem (after answering those questions).","['statistics', 'confidence-interval', 'statistical-inference']"
1571038,On the determinant of a tridiagonal Toeplitz matrix,"I'm a bit confused with this determinant.
We have the determinant $$\Delta_n=\left\vert\begin{matrix}
5&3&0&\cdots&\cdots&0\\
2&5&3&\ddots& &\vdots\\
0&2&5&\ddots&\ddots&\vdots\\
\vdots&\ddots&\ddots&\ddots&\ddots&0\\
\vdots& &\ddots&\ddots&\ddots&3\\
0&\cdots&\cdots&0&2&5\end{matrix}
\right\vert$$ I compute $\Delta_2=19$ , $\Delta_3=65$ . Then I would like to find a relation for $n\geq 4$ which links $\Delta_n, \Delta_{n-1}$ and $\Delta_{n-2}$ and thus find an expression of $\Delta_n$ . How could we do that for $n\geq 4$ ?","['tridiagonal-matrices', 'matrices', 'toeplitz-matrices', 'determinant', 'linear-algebra']"
1571049,Convex function?,"I have a positive function $f(x,y)$, where $x\in{\mathbb R}^n$ and $y\in{\mathbb R}$. I know that for $y$ fixed, $g(x)=f(x,y)$ is convex, and that for $x$ fixed, $h(y)=f(x,y)$ has positive second derivative. If this enough to show that $f(x,y)$ is convex?","['convex-analysis', 'functions']"
1571060,Find the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable.,"Find the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable. My Attempt: Since the $\sin|x|$ has a non differentiability point at $x=0$ and $(x^2-1)|(x+1)(x-2)|$ has two non differentiability points at $x=-1,x=2$,so i write the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable are $1+2=3$ but the answer given in my book is $2$.So i was wrong somewhere. My Second Attempt:
I redefined $f(x)$ as $(x^2-1)(x^2-x-2)-\sin x$ when $x<-1$ $-(x^2-1)(x^2-x-2)-\sin x$ when $-1<x<0$ $-(x^2-1)(x^2-x-2)+\sin x$ when $0<x<2$ $(x^2-1)(x^2-x-2)+\sin x$ when $x>2$ I found its derivative $f'(x)$ as $(x+1)(4x^2-7x+1)-\cos x$ when $x<-1$ $-(x+1)(4x^2-7x+1)-\cos x$ when $-1<x<0$ $-(x+1)(4x^2-7x+1)+\cos x$ when $0<x<2$ $(x+1)(4x^2-7x+1)+\cos x$ when $x>2$ I found $LHD$ at $x=-1$ is not equal to $RHD$ at $x=-1$. I found $LHD$ at $x=0$ is not equal to $RHD$ at $x=0$. I found $LHD$ at $x=2$ is not equal to $RHD$ at $x=2$. So again i am getting three points of non differentiability.I dont know where i have gone wrong.Please help me.Thanks.","['derivatives', 'real-analysis', 'calculus']"
1571065,What is the geometrical meaning of the total differential?,"Could anybody give me a geometrical explanation of the total differential, if there is such? For me(a non-mathematician) it just looks like the generalization of the derivative to more dimensions but in those higher dimensions it hasn't got a geometrical meaning. Am i right? Thank you.","['multivariable-calculus', 'vector-analysis', 'derivatives']"
1571091,Is a path-connected bijection $f\colon \Bbb{R}^n \to \Bbb{R}^n$ continuous?,"While thinking about this question I was asking myself if a path-connected bijection $f\colon \Bbb{R}^n \to \Bbb{R}^n$ has to be continuous for $n>1$? If we drop the requirement that $f$ is bijective, then it is not true as in the connected case. I was wondering if this question is maybe easier? I have no intuition if it is true or not. On one hand I think there are too many connected sets, there will be some ugly counterexample, on the other hand the reals are ""nice"". By a path-connected function I mean a function between topological spaces whose image of a path-connected set is path-connected.","['examples-counterexamples', 'path-connected', 'connectedness', 'real-numbers', 'general-topology']"
1571101,Normal subgroup $H$ of $G$ with same orbits of action on $X$.,"I have a somewhat broad question related to group actions and their restriction to a normal subgroup. If we have a group action $\sigma : G \times X \rightarrow X$ with orbits $G_x$, and a normal subgroup, $H$ of $G$, such that the restriction of the action $\sigma$ to $H$, $\sigma |_H : H \times X \rightarrow X$ has the same orbits, $H_x=G_x$. What can we say about $H$ as a subgroup of $G$, and about it's relationship to $G$'s action on $X$? Edit:
The particular case I am interested in involves the action of $\operatorname{Aut}(G)$ on $G$, specifically I am interested in two subgroups, which are both characteristic (and hence normal) in $\operatorname{Aut}(G)$. One is the inner automorphism group, $\operatorname{Inn}(G)$, the other is the group of class-preserving automorphisms (those automorphisms which map conjugacy classes of $G$ to themselves), which I will denote as $\Lambda_{id}(G)$. I have already established that $\operatorname{Inn}(G) \trianglelefteq \Lambda_{id}(G)$ (follows from their normality in $\operatorname{Aut}(G)$, and that every inner automorphism is class-preserving). The interesting thing about the restricted actions, $\rho : \operatorname{Inn}(G) \times G \rightarrow G$ and $\pi : \Lambda_{id}(G) \times G \rightarrow G$, is that they both have the conjugacy classes of $G$ as orbits.","['group-actions', 'group-theory']"
1571104,Show $f(t)$ is not injective in any neighbourhood of $0$,"This comes from Baby Rudin chapter 9's exercises: define 
$$f(t)=t+2t^2\sin\frac1t,\,t\ne 0;\quad f(0)=0$$ show $f$ 
fails to be injective in any neighbourhood of $0$. My thought was to find points where $f'(t)=0$ but $f''(t)\ne 0$. Letting $t_n=1/(2n\pi)$ and $t'_n=1/(2n\pi+\pi)$ it's easy to see that $f'(t_n)\to -1$ while $f'(t'_n)\to 3$. So by the continuity of $f'(t),t\ne 0$ there exists   some $t_0$  between $t_n$ and $t'_n$ for each $n$ such that $f'(t_0)=0$. But it's much more troubling to show the second order derivative doesn't vanish. Is there any slicker way?","['derivatives', 'real-analysis', 'calculus']"
1571126,How to prove a formula is not valid in robinson arithmetic,"I have known that $\forall x \, (s(0)∗ x = x)$ is a theorem of Peano Arithmetic. 
But how to prove that this formula is not a theorem of Robinson Arithmetic?","['first-order-logic', 'logic', 'discrete-mathematics']"
1571133,Solve the given Differential Equation,"Solve the non-linear first order differential equation
$$\frac{dy}{dx}=\frac{x+2y-5}{2x+xy-4} $$ I tried substituting $x=X+h$ and $y=Y+k$ but the $xy$ term is creating problem. How to solve it? Any suggestion is appreciated.","['ordinary-differential-equations', 'calculus']"
1571145,unique fixed point problem,Let $f: \mathbb{R}_{\ge0} \to \mathbb{R} $ where $f$ is continuous and derivable in $\mathbb{R}_{\ge0}$ such that $f(0)=1$ and $|f'(x)| \le \frac{1}{2}$. Prove that there exist only one $ x_{0}$ such that $f(x_0)=x_0$.,"['derivatives', 'fixed-point-theorems', 'calculus', 'functions']"
1571166,Why do people present isotopies to demonstrate that two spaces are homeomorphic?,"In looking for a rigorous proof of the fact that Alexander's Horned Sphere along with the volume that it bounds is homeomorphic to a 3-ball, I have found a number of posts on the web that attempt to explain this fact by showing that we can continuously deform a 3-ball into Alexander's Horned Sphere and the volume that it bounds without moving two distinct points into the same place at any time. The thing is, that's not a homeomorphism, that's an isotopy. In general, when I would like to find a proof of the fact that two spaces are homeomorphic, I often just find numerous links to some video or picture that illustrates an isotopy between the two spaces. However, as far as I can tell, the fact that two topological spaces are isotopic is in no way a sufficient condition for those spaces being homeomorphic, and I never see any justification for the fact that these isotopies yield homeomorphisms. I find this very frustrating because I can generally deduce that two spaces are isotopic with relative ease, but find it much trickier to see that two spaces are homeomorphic. Thus, I end up learning nothing from the time that I spend researching these questions. I guess what I'm asking then, is why do I see these two concepts equated so frequently in specific examples? Moreover, under what conditions is isotopic equivalent to homeomorphic? My guess is that there must be some well-known fact relating the two concepts that I am unaware of, but if this is the case, then I can't seem to find this fact anywhere. Thank you for your time, any answers will be greatly appreciated.","['algebraic-topology', 'general-topology']"
1571169,Let $f:A\rightarrow B$ be a homomorphism of integral domains. Is it possible to extend $f$ to the integral closures of $A$ and $B$?,"I am working on question II.3.8 in Hartshorne's Algebraic Geometry . Our professor mentioned a very useful algebraic result for the problem but I cannot find a reference anywhere. Let $f:A\rightarrow B$ be a ring morphism of integral domains, and let $\tilde{A}$ and $\tilde{B}$ be the respective integral closures. Then there exists a morphism $\tilde{f}:\tilde{A}\rightarrow \tilde{B}$. I think the result is supposed to be ""there exists a morphism $\tilde{f}:\tilde{A}\rightarrow \tilde{B}$ that extends $f$"", but I'm not sure. Any insight on why this is true or not or a reference for the result would be much appreciated. My idea to construct such a morphism is as follows.
Let $r$ be an element in $\tilde{A}$. Then $r$ is the root of a polynomial $x^n+\cdots+a_1x+a_0$, with $a_i\in A$.
Then let $\tilde{f}(r)$ be the root of the polynomial $x^n+\cdots+f(a_1)x+f(a_0)$ in $\tilde{B}$. I'm not sure if this is indeed a homomorphism however.","['algebraic-geometry', 'commutative-algebra']"
1571217,How can Hamilton's quaternion equation be true?,"I'm reading Ken Shoemake's explanation of quaternions in David Eberly's book Game Physics .  In it, he describes the $\mathbf{i}, \mathbf{j},  \mathbf{k}$ components of quaternions to all equal $\sqrt{-1}$.  Then it states Hamilton's quaternion equation: $\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{ijk} = \mathbf{-1}$ If $\mathbf{i} = \mathbf{j} = \mathbf{k} = \sqrt{-1}$, then it makes sense how $\mathbf{i}^2 = \mathbf{-1}$.  But $\mathbf{ijk}$ should equal $\mathbf{i}^3$, not $\mathbf{i}^2$.  How does $\mathbf{ijk} = \mathbf{-1}$? The book's notation says that lowercase bold letters denote a vector, so I'm thinking of $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ as the basis of the quaternion, similar to the basis of a vector, and can be written $(\sqrt{-1}, \sqrt{-1}, \sqrt{-1})$.  Having the result of $\mathbf{ijk}$ as a bold $\mathbf{-1}$ to me implies that it is the vector $(-1, -1, -1)$.  Is this understanding correct?  In this context, what does it mean to square vector $\mathbf{i}$?  If it equals another vector, then the only operation that makes sense is the cross product, but the cross product of a vector and itself is the zero vector.","['quaternions', 'linear-algebra']"
1571238,nth root function,"I want to write code for a nth root function, so I need to be sure, that the underlying mathematical function is correct. From another post over at SO, I wrote the following definition: $
\sqrt[x]{y} = y^{\frac{1}{x}} = \left\{
{\begin{array}{rl}
\exp_{2}\left(x \cdot \log_2 \left(\frac{1}{y}\right)\right) & \text{if } y > 0\\
-\exp_{2}\left(x \cdot \log_2 \left(\frac{1}{\left\lvert{y}\right\rvert}\right)\right) & \text{if } (y < 0) \land \left((y \equiv 1 \mod 2) \lor \left(\frac{1}{y} \equiv 1 \mod 2\right)\right)\\
\exp_{2}\left(x \cdot \log_2 \left(\frac{1}{\left\lvert{y}\right\rvert}\right)\right) & \text{if } (y < 0) \land \left((y \equiv 0 \mod 2) \lor \left(\frac{1}{y} \equiv 0 \mod 2\right)\right)
\end{array}}
\right.
$ Is this function definition correct? EDIT#1 Link to the other question: https://stackoverflow.com/questions/34221713/how-to-calculate-python-float-number-th-root-of-float-number/34223324#34223324 EDIT#2 What I want to achieve is a function, that can calculate the nth-root of any positive rational number, where n can be rational as well. EDIT#3 I've not checked the variable positions again and this is what I have now: $
\sqrt[\frac{n}{m}]{a} = a^{\frac{m}{n}} = \left\{
{\begin{array}{rl}
\exp_{2}\left(\frac{m}{n} \cdot \log_2 \left(a\right)\right) & \text{if } a > 0\\
-\exp_{2}\left(\frac{m}{n} \cdot \log_2 \left(a\right)\right) & \text{if } (a < 0) \land \left((x \equiv 1 \mod 2) \lor \left(\frac{1}{x} \equiv 1 \mod 2\right)\right)\\
\exp_{2}\left(\frac{m}{n} \cdot \log_2 \left(a\right)\right) & \text{if } (a < 0) \land \left((x \equiv 0 \mod 2) \lor \left(\frac{1}{x} \equiv 0 \mod 2\right)\right)
\end{array}}
\right.
$","['radicals', 'exponential-function', 'functions', 'arithmetic']"
1571254,"Given $2n$ points in the plane, prove we can connect them with $n$ nonintersecting segments","Given $2n$ points in the plane such that no three points lie on one line. Prove that it is possible to draw $n$ segments such that each segment connects a pair of these points and no two segments intersect. So I know there is a solution that examines 4 points at a time and the two line segments, and if they intersect change the positioning of the 2 line segments for those points so they don't. It can be shown that the sum of the distance of the line segments monotonically decreases, so the process must terminate eventually, as there are only a finite number of possible configurations given the $2n$ points. I was wondering if this can be done in the following way: Lemma: Given $2n$ points in the plane, we can partition the plane into two sets $A_1,A_{n-1}$ , where $A_1$ contains 2 points, $A_{n-1}$ contains the other $2n-2$ points, in a way such that the line connecting the two points in $A_1$ does not intersect any possible line segment formed by connecting two points of $A_{n-1}$ (basically, having $A_1,A_{n-1}$ disjoint). Using this lemma, we first partion the $2n$ points into $A_1,A_{n-1}$ . Then we could apply the lemma to $A_{n-1}$ , partitioning the plane into sets $A_1,A_2,A_{n-2}$ such that $A_1,A_2$ contain 2 points each, with the segment connecting those two points not intersect any of the possible line segments formed by connecting two points of $A_{n-2}$ , or each other. Then we could repeat the lemma until we can finally partition the plane $P=A_1\cup A_2\cup A_3\cdots\cup A_{\frac{n}{2}}$ , with all the $A_i$ disjoint. The lemma seems kind of obvious (based on trying examples), just partition the plane using a line that separates two of the extreme points in the plane from the rest (e.g. the two ""lowest"" ones on the plane), however I do not know how to rigorously prove it. Does anyone have any idea of how to prove this lemma? Also could someone check if this argument works? Thanks!","['combinatorics', 'algorithms', 'contest-math']"
1571260,"What is an intuitive definition for ""conjugate"" in Group Theory?","In Abstract Algebra, I learned about ""conjugation"" in the context of a group $H$ being a 'normal' subgroup of $G$ if the element $xhx^{-1}\in H$ for any $x\in G$. But this is not the first time I've seen the word 'conjugate'. The other times I've seen this are in pre-calculus, when trying to rationalize a denominator, or in the case where $(x+y)$ is the conjugate of $(x-y)$. Does the Group Theory version of conjugate have any link to the pre-calculus version (and other uses)?","['intuition', 'abstract-algebra', 'group-theory']"
1571291,Solving $(n+1)(n+2)…(n+k)−k = x^2$,"Let $n$ and $k$ be positive integers. Need to find all pairs of $(n,k)$ such that $$(n+1)(n+2) \cdots (n+k)−k = x^2,$$ where $x^2$ is a perfect square.","['number-theory', 'diophantine-equations']"
1571311,Calculating square roots using the recurrence $x_{n+1} = \frac12 \left(x_n + \frac2{x_n}\right)$ [duplicate],"This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed 3 years ago . Let $x_1 = 2$, and define $$x_{n+1} = \frac{1}{2} \left(x_n + \frac{2}{x_n}\right).$$ Show that $x_n^2$ is always greater than or equal to $2$, and then use this to prove that $x_n − x_{n+1} ≥ 0$. Conclude that $\lim x_n =
\sqrt2$. My question: So I know how to do this problem but I don't know how to prove that $x_n >0$ for all $n\in \mathbb{N}$.","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'analysis']"
1571354,Trouble finding the upper bound for a certain sum.,"I have encountered the following problem and I am curious how to solve it. $\textrm{Given }a_{n+1} = a_{n}(1 - \sqrt{a_{n}}) \textrm{, where } a_{i} \in (0,1) \textrm{, } i = \overline{1,n}$ I have proved that $(a_{n})_{n\in\mathbb{N}}$ is decreasing and now I have to prove that the upper bound of $b_{n} = {a_{1}^2} + {a_{2}^2} + \cdots + {a_{n}^2}$ is $a_{1}$. I have no idea how to do this. I have tried in all sorts of ways but only get to something like $b_{n} < {n}\cdot{a_{1}^2}$ or $b_{n} < {n}\cdot{(1-\sqrt{a_{1}})^2}$ which is not even close to what the upper bound must be.
I feel like it's a common trick that you have to use to solve this, but I cannot find it.",['sequences-and-series']
1571381,$p$-adic logarithm is injective if $p > 2$?,"Define the $p$-adic logarithm$$\log_p(1 + x) = \sum_{i = 1}^\infty (-1)^{i-1}x^i/i.$$I know that $\log_p$ is a homomorphism from $U_1$ to the additive group of $\mathbb{Q}_p$, where $U_1$ is the subset of elements of $\mathbb{Q}_p$ of the form $1 + x$, with $|x|_p < 1$. How do I see that it is injective if $p > 2$?","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'p-adic-number-theory']"
1571419,Prove rayleigh quotient = operator norm without referring to eigenvalues,"Let $H$ be a Hilbert and $T \in \mathcal{L}(H,H)$ a symmetric operator. Prove 
$$
\|T\| 
=
\sup\{|(x,Tx)| : x \in H, \|x\| = 1\}
$$ without referring to the eigenvalues of $T$ (which is what all proofs I could find do).","['eigenvalues-eigenvectors', 'linear-algebra']"
1571463,Nim game real life applications,"I've learned how to prove and apply the Nim game strategy in discrete mathematics, but I was wondering if there is any real life examples and application for this theory. I searched online and didn't find much, does anyone know any of its applications? Thanks!","['game-theory', 'discrete-mathematics']"
1571497,Proving that scalar multiplication is continuous,"Let $\mathbb{K} \in \{ \mathbb{R} , \mathbb{C} \}$ and $s= \mathbb{K}^{\omega}$ be the usual sequence set with entries on $\mathbb{K}$. I proved that $\mathbb{K}$ induces a $\mathbb{K}$-vector space structure on $s$ and that the function $\rho : s \times s \to \mathbb{R}$ given by $$\displaystyle \rho(x,y) = \sum_{n=1}^{\infty} \frac{1}{2^n} \frac{|x_n - y_n|}{(1+|x_n - y_n|)} \ \ , $$ for all $x = (x_n) \in s$ and all $y = (y_n) \in s$, is a translation-invariant metric on $s$. Then I proved that addition $A: (s \times s , \tau_*) \to (s,\tau)$ is continuous, where $\tau$ is the topology on $s$ induced by $\rho$ and  $\tau_*$ is the product topology of $(s,\tau)$ with itself. For that purpose I used the metric $\sigma : (s \times s) \times (s \times s) \to \mathbb{R}$ such that $\sigma \big( (x,y) , (a,b) \big) = \rho(x,a) + \rho(y,b)$, $\forall x,y,a,b \in s$, which induces $\tau_*$ on $s \times s$. Then I was trying to prove that the scalar multiplication $M: (\mathbb{K} \times s , \tilde{\tau}) \to (s, \tau)$ is continuous and I failed, where $\tilde{\tau}$ is the product topology on $\mathbb{K} \times s$ of $(\mathbb{K}, \eta)$ with $(s,\tau)$ and $\eta$ is the topology on $\mathbb{K}$ induced by the norm given by the absolute value function $| \cdot | : \mathbb{K} \to \mathbb{R}$. I don't know which metric to consider and I don't know how to do it. I need help. Any hint is appreciated. Thanks in advance.","['functional-analysis', 'topological-vector-spaces']"
1571518,What's the most fundamental derivative of multivariable functions?,"There are several derivatives of multivariable functions.  For instance, given a function $F: \Bbb R^n \to \Bbb R^m$ there's the divergence $\nabla \cdot F$ the curl $\nabla \times F$ (if $n=m=3$) the Jacobian $J_F(p)$ the directional derivative $dF(p,v) = \lim\limits_{h\to 0}\frac{F(p+hv)-F(p)}{h}$ I'm wondering which of these is really the derivative of $F$.  I don't think it could be the divergence or curl because $(1)$ the curl isn't even defined except on $\Bbb R^3$ and $(2)$ when they're defined $(\nabla \cdot F)(p) = \operatorname{trace}(J_F(p))$ and $[(\nabla \times F)(p)]_k = \left[\frac12\big([J_F(p)]^T-J_F(p)\big)\right]_{ij}\varepsilon_{ijk}$.  So it seems that $J_F(p)$ is more fundamental. But what about the directional derivative?  I don't even see have this relates to the Jacobian.  So which one (perhaps including some type of derivative I've never heard of) is more ""fundamental""?  What is the derivative of a multivariable function?","['multivariable-calculus', 'derivatives']"
1571521,Martingale formulation of Bellman's Optimality Principle,"From David Williams' Probability with Martingales Related question: Deducing an optimal gambling strategy (using martingales). What I tried: For no 2, if $\ln Z_n - n \alpha$ is a supermartingale, then for $m < n$ , $$E[\ln Z_n - n \alpha | \mathscr F_m] \le \ln Z_m - m \alpha$$ $$ \to E[\ln Z_N - N \alpha | \mathscr F_0] \le \ln Z_0 - 0 \alpha$$ $$ \to E[\ln Z_N - N \alpha] \le \ln Z_0$$ $$ \to E[\ln \frac{Z_N}{Z_0}] \le N \alpha$$ Is that right? For no 1, for $m < n$ , $$E[\ln Z_n - n \alpha | \mathscr F_m] = E[\ln Z_n | \mathscr F_m] - n \alpha$$ $$ = E[\ln \frac{Z_n}{Z_0} | \mathscr F_m] - n \alpha + \ln Z_0$$ $$ = E[\ln \frac{Z_n}{Z_{n-1}} | \mathscr F_m] + ... + E[\ln \frac{Z_{m+1}}{Z_{m}} | \mathscr F_m]$$ $$+ \ln \frac{Z_m}{Z_{m-1}} + ... + \ln \frac{Z_{1}}{Z_{0}} - n \alpha + \ln Z_0$$ Note that $$Z_n - Z_{n-1} \le |Z_n - Z_{n-1}| \le C_n |\epsilon_n - \epsilon_{n-1}| \le 2C_n \le 2Z_{n-1}$$ Also, I think $$\ln \frac{Z_n}{Z_{n-1}} \le |Z_n - Z_{n-1}|$$ Hence we have $$E[\ln \frac{Z_n}{Z_{n-1}} | \mathscr F_m] + ... + E[\ln \frac{Z_{m+1}}{Z_{m}} | \mathscr F_m]$$ $$+ \ln Z_m - n \alpha$$ $$\le E[2Z_{n-1} | \mathscr F_m] + ... + E[2Z_{m} | \mathscr F_m]$$ $$+ \ln Z_m - n \alpha$$ $$= 2(E[Z_{n-1} | \mathscr F_m] + ... + E[Z_{m} | \mathscr F_m])$$ $$+ \ln Z_m - n \alpha$$ One thing to do would be to show that $$2E[Z_{n-1} | \mathscr F_m] + ... + 2E[Z_{m} | \mathscr F_m] \le (n-m) \alpha$$ possibly by showing that $2E[Z_{\{\cdot\}} | \mathscr F_m] \le \alpha$ How would I do that? Something else: $$E[\ln \frac{Z_n}{Z_m} | \mathscr F_m] + \ln Z_m - n \alpha$$ $$ \le E[|Z_n - Z_m| | \mathscr F_m] + \ln Z_m - n \alpha$$ $$ \le E[2Z_m | \mathscr F_m] + \ln Z_m - n \alpha$$ $$ \le 2Z_m + \ln Z_m - n \alpha$$ Now $2Z_m \le (n-m) \alpha?$ We have $$2Z_m = 2\sum_{k=1}^{m} C_k(\epsilon_k - \epsilon_{k-1})$$ $$\le 2\sum_{k=1}^{m} Z_{k-1}$$ I'm not quite sure how to show that $$2\sum_{k=1}^{m} Z_{k-1} \le (n-m)\alpha$$ if that's even true. How can/else can I approach this problem? For no 3, no idea. Any hints? I think it has something to do with a stopping time. Might we have to use the ff lemma (red box): ? Update 1 decade later: It's called freezing lemma I guess $\mathbb{E}(\varphi(X, Y) | \mathcal{G}) = \mathbb{E}(\varphi(X, Y))$ What's the name for this proposition? Or alternatively what's happening?","['stochastic-processes', 'optimization', 'independence', 'probability-theory', 'martingales']"
1571532,Fourier transforms having compact support,"As we know, the fourier transform is a map $\mathcal{F}:L^1\rightarrow C_0$ (all with domain $\mathbb{R}$). Can one characterize the space of $f\in L^1$ such that $\mathcal{F}$ has compact support, i.e. is in $C_c\subset C_0$? In particular, is this space dense in $L^1$? I believe this should be true, but I find it hard to come up with nontrivial examples of such $f$. I guess the formula $\mathcal{F}(f\ast g)=\mathcal{F}(f)\mathcal{F}(g)$ should be useful?","['lp-spaces', 'integration', 'fourier-analysis']"
1571543,Is there a coproduct in the category of path connected spaces?,"Well, first of all, does the coproduct exist in the category of path-connected spaces, and if not how would you prove it? If it does exist what is it and how do you find it? The usual coproduct of topological spaces is disjoint union, but it is not path-connected. As a follow up question, given a full subcategory of a category in which products/coproducts do exist, what is the strategy for finding out if that category also has products/coproducts?","['category-theory', 'general-topology', 'limits-colimits', 'connectedness']"
1571584,Permutation and Combinatorics Problem,"A function $G$ is defined on a set $S$ with size $k$ : $G(a_1,a_2,a_3,\ldots,a_k)$.
$G(a_1,a_2,a_3,\ldots,a_k) = 1$ if and only if a convex polygon can be created by taking these $k$ elements as the side lengths. Otherwise $G(a_1,a_2,a_3,\ldots,a_k) = 0$.
You are given the identity permutation over $n$ elements - $I_n(I_n=\{1,2,3,\ldots,n\})$.
Find the sum of function $G$ over all possible distinct $k$-sized subsets of $I_n$. All I could think of involved checking all subsets of size $k$ (which is not efficient for large size of the set obviously).Checking the $G$ value for a subset is easy , you just check if the largest element is greater than the sum of all other or not. Calculating the number of such subsets doesn't seem trivial :/ I wonder if there is some pattern/theorem/recurrence involved.","['combinatorics', 'recurrence-relations', 'convex-analysis']"
1571592,Gap in My Understanding of Measurable Functions as Pointwise Limits of Simple Functions,"I've been thinking back to the proof that in $\mathbb{R}$, a measurable function $f:\mathbb{R}\to\mathbb{R}$ is the pointwise limit of increasing simple functions $s_n$. As far as the intuitive picture of it goes, Pugh and Folland both have excellent visualizations that convince me that ""morally"", such a theorem is correct (and are also a great deal of help is making sense of the messy algebra). However, when I think of a function such as the identity on the unit interval (""y=x"" from grade school), it is definitely measurable as it's continuous, but the fact that it takes on uncountably many values in the interval jars with my understanding that a simple function can only take on finitely many (and in the limit, increases to a countable number of values taken on). What part(s) am I really failing to grasp here?","['real-analysis', 'lebesgue-integral', 'measure-theory', 'lebesgue-measure']"
1571593,Prove that $5^{1/3}+7^{1/2}$ is irrational,"Goal: Prove that $5^{1/3}+7^{1/2}$ is irrational. Idea: We can prove this is irrational by supposing it is rational and finding a contradiction. So, $5^{1/3}+7^{1/2} = p/q$ where $p$ and $q$ are integers that have no factors in common other than $1$. The issue here is that I can not seem to find a way manipulate this equation to a form where I can find contradiction. I tried taking each side to the power of 6... That was an unmanageable mess.  Any ideas?","['number-theory', 'radicals', 'rationality-testing']"
1571595,Does changing a sign in an inequality relate to decreasing functions in any way?,"Edited (not by original poster): When we multiply both sides on an inequality by a negative number, we change the sign of the inequality (we flip it around). I have been told that this has something to do with functions. More specifically, I think that we can use decreasing functions to show that multiplying both sides of an inequality by a negative number leads to changing the sign of the inequality. Can anybody elaborate or explain to me, in an easy, simple, and clear way , how (or even whether) we can use decreasing functions to show that multiplying both sides of an inequality by a negative number leads to changing the sign? Original: I got the point of changing the sign, but I also heard that it has something to do with functions. It's like Decreasing Functions causes us to change the sign in an inequality. Can anybody broaden this idea in an easy, simple and clear way?","['inequality', 'functions']"
1571638,Particular solution:$ (3+x) e^{-2x}$?,$y''+4y'+4y = (3+x) e^{-2x}$ So I'm working with undetermined coefficient and figured out solution for the left side. But what is the particular solution for the right side? I tried these but they don't work as all terms cancel to zero: $y = (Ax+B) x e^{-2x}$ $y = (Ax+B)e^{-2x}$,"['multivariable-calculus', 'ordinary-differential-equations']"
1571658,Are there any theorem about the linearization of PDE?,"I am a beginner of PDE, and surprise that some nonlinear equation will become a linear equation after variable substitution,for example . So, I am curious that whether there are general theory making equation become linear. If not, why we don't do so ? I want to know the difficult of this way.","['ordinary-differential-equations', 'soft-question', 'analysis', 'partial-differential-equations']"
1571669,Actuarial Problem. (Policyholder).What is the probability that a new policyholder will have an accident within a year of purchasinag a policy?,"Problem said: Suppose people can be divided into two classes: those
  who are accident-prone and those who are not. The statistics show that
  an accident-prone person will have an accident at some time within a
  fide 1-year period with probability 0.35, whereas this probability for
  a non-accidenta-prone person is 0.18. Assume that 30.3% of the
  population is accident-prone. What is the probability that a new policyholder will have an accident
  within a year of purchasinag a policy? I apply the law of the total probability: I have: P(a new policyholder will have an accident )=(0.303) (.35)+(.697) (0.18)=0.23152 Is that correct? Thanks.","['probability', 'actuarial-science']"
1571684,"If $Y_1, Y_2$ are exponential random variables, how can I find $P(Y_1 <Y_2)$?","If I have that $Y_1, Y_2$ are independent exponential random variables with rate parameter $\lambda_1, \lambda_2$ respectively, how can I find $P(Y_1<Y_2)$? I know that I am supposed to use the integral but cannot get the joint distribution. Thanks.","['statistics', 'probability']"
1571698,Prove that p divides to algebraic multiplicity of the eigenvalue,"I need help in the following exercise of a qualifying exam: Let $A$ be a matrix of size $m$ by $m$ over the finite field $\mathbb{F}_p$ such that $\operatorname{trace}\left(A^n\right)=0$ for all $n$. If $\lambda$ is a nonzero eigenvalue of $A$, prove that the algebraic multiplicity of $\lambda$ is divisible by $p$. Thank you by some hints.","['eigenvalues-eigenvectors', 'positive-characteristic', 'linear-algebra', 'determinant']"
1571708,$\sum a_n $ absolutely convergent and $\sum b_n $ convergent $\implies \sum a_nb_n$ absolutely convergent.,Is this true? $\sum a_n $ absolutely convergent and $\sum b_n $ convergent $\implies \sum a_nb_n$ absolutely convergent. I don't know how to proceed .Please help.,"['real-analysis', 'sequences-and-series']"
1571731,"If I have that $X \sim \chi^2_{1}$ and $Y \sim \chi^2_{2}$ are independent, how can I show that $4XY \sim Y^2$?","If I have that $X \sim \chi^2_{1}$ and $Y \sim \chi^2_{2}$ are independent, how can I show that $4XY \sim Y^2$? Here, I have that $X \sim \chi^2_{1}$ means that $X$ is a chi-square random variable with 1 degree of freedom and that $Y \sim \chi^2_{2}$ has two degrees of freedom. Is there a way to do this without integration and more by representation? I have thought about moment generating functions, or taking the expectations, but am not sure how to do it. I've also thought about taking logs of both sides to induce additivity. Finally, the $4XY$ looks suspiciously set up as I know that $(X+Y)^2 - (X-Y)^2 = 4XY$. Does anyone have any ideas?","['statistics', 'probability']"
