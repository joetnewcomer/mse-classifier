question_id,title,body,tags
2915018,Solving $\sin(3x +28^\circ)= \cos(2x-13^\circ)$,"Solve the following equation :
  $$\sin(3x +28^\circ)= \cos(2x-13^\circ), x \in [0^\circ, 360^\circ]$$ Solution:
$$3x+28 +2x -13 =90$$
$$x=15$$
$$3x+28 -2x +13 = 90$$
$$x= 49$$
But the value $$x = 87$$ also satisfies the equation. I do not know how it comes.",['trigonometry']
2915026,Expectation $T(T-1)$ where $T$ is the sample mean of i.i.d. Bernoulli random numbers,"I want the expectation of $T(T-1)$ where $T$ is the sample mean of i.i.d. Bernoulli random variables with parameter $p$. Using the linearity property, we have 
$$\mathbb E\{T(T - 1)\} = \mathbb E(T^2) - \mathbb E(T).$$ Since $\mathbb E(T) = p$ and $\mathbb E(T^2) = \tfrac{p(1-p)}{n} + p^2$, I believe the answer should be $\tfrac{p^2(n-1)}{n} - \tfrac{p(n-1)}{n}$ but in my notes it says that the expectation is $n(n-1)p^2$. Are my notes incorrect or did I make an error? Thanks.","['expected-value', 'statistics', 'probability', 'random-variables']"
2915057,"If the coordinates of random vectors are tight, are the random vectors tight?","Let $\pi_i:\mathbb{R}^k\rightarrow \mathbb{R}:x\mapsto x_i$ be the $i$-th projection map for each $1\leq i\leq k$. Let $\{\mu_n\}_{n\in\mathbb{N}}$ be a sequence of Borel probability measures on $\mathbb{R}^k$. My question is: If $\{(\pi_i)_*\mu_n\}_{n\in\mathbb{N}}$ is tight for every $1\leq i \leq k$, then is $\{\mu_n\}_{n\in\mathbb{N}}$ tight? In other words: If marginals are tight, is the original sequence tight? How to prove this? Thank you in advance!","['measure-theory', 'probability-theory']"
2915085,The convolution of two $L^2(\mathbb R)$ functions is continuous,"Take $f$ and $g$ $\in L^2(\mathbb R)$, then I want to show that $\lim_{h \to 0} \int f(x-y-h)g(y)dy = f \ast g(x)$. My idea is to first take $f$ to be a continuous function with a compact support, then $f$ has to be uniformly continuous. Then we know that $f \ast g$ has to be continuous by exchanging the order of the limit and integration. Then use the fact that $C_{c}(\mathbb R)$ is dense in $L^2(\mathbb R)$ to conclude the proof. Is this the right idea? Is there any simpler arguments?",['real-analysis']
2915113,How to find a value for a variable that makes a matrix (with said variable) equal to its own inverse,"I'm given $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$
and am asked to find x such that it's inverse would equal itself. To attempt this I first tried to put the question into an augmented matrix and got this: $$\begin{bmatrix}1&x/3&1/3&0\\0&(x/3)-(3/2)&1/3&1/2\\ \end{bmatrix}$$ I found that my answer was wrong so I tried: $$\begin{bmatrix}3&x\\-2&-3\\\end{bmatrix}$$
times
$$\begin{bmatrix}x_1&x_2\\x_3&x_4\\\end{bmatrix}$$ to try and solve for x but found similar dissatisfactory results. The answer is listed as x = 4; how might I go about solving this? Did I just make a mistake with my methods or is this the entirely wrong way about?","['matrices', 'linear-algebra']"
2915138,$\Bbb N\times\Bbb N$ is countable,"$\Bbb N\times\Bbb N$ is countable My attempt: Lemma : $A$ is countable if and only if there exists a injectve mapping from $A$ to $\Bbb N$. We define a mapping $f:\Bbb N\times\Bbb N \to \Bbb N$ by $f(k,l)= 2^k3^l$. Next we prove $f$ is injective. Suppose that $(k_1,l_1)$ and $(k_2,l_2)\in\Bbb N\times\Bbb N$ and that $f(k_1,l_1) = f(k_2,l_2)$. Then $2^{k_1}3^{l_1}=2^{k_2}3^{l_2}$. If $k_1>k_2$, then $2^{k_1}$ is not divisible by $2^{k_2}$. Furthermore, $2^{k_1}$ is not divisible by $3^{l_2}$ since $2$ and $3$ are prime numbers. It follows that $2^{k_1}$ is not divisible by $2^{k_2}3^{l_2}=2^{k_1}3^{l_1}$. Thus $2^{k_1}$ is not divisible by $2^{k_1}3^{l_1}$, which is clearly a contradiction. By assuming $k_1<k_2$, or $l_1<l_2$, or $l_1>l_2$, we can easily obtain similar contradictions. Thus $k_1=k_2$ and $l_1=l_2$. Hence $(k_1,l_1)$ = $(k_2,l_2)$. As a result, $f$ is injective and hence $\Bbb N\times\Bbb N$ is countable. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help! Update: On the basis of @spaceisdarkgreen's commnet, I should explicitly mention that $f$ is injective by The Fundamental Theorem of Arithmetic.","['elementary-set-theory', 'functions', 'proof-verification']"
2915151,Proving a martingale property of the empirical distribution function,"Let $X_1,\ldots,X_n$ be iid random variables from the uniform distribution on $[0,1]$ .
Let $F_n(t)=\sum_{i=1}^n \mathbf{1}_{\{X_i\leq t\}}$ be the empirical distribution function. I would like to prove that for $0\leq s\leq t <1$ , $$E \left(\frac{F_n(t)-t}{1-t}\Big|F_n(u),u\leq s\right)=E \left(\frac{F_n(t)-t}{1-t}\Big|F_n(s)\right)=\frac{F_n(s)-s}{1-s}.$$ Any hint or reference is appreciated.","['empirical-processes', 'martingales', 'probability-theory']"
2915154,How to find the sum of the sides of a polygon whose one vertex goes from the north of a circle and the other comes from the east in its perimeter?,"The problem is as follows: In figure 1. there is a circle as shown. The radius is equal to 10 inches and its center is labeled with the letter O. If $\measuredangle PC=30^{\circ}$. $\textrm{Find AB+BC}$. The existing alternatives in my book are: $3\left( \sqrt{2}+\sqrt{6}\right)$ $4\left( \sqrt{6}-\sqrt{2}\right)$ $5\left( \sqrt{3}-\sqrt{2}\right)$ $5\left( \sqrt{3}+\sqrt{2}\right)$ $5\left( \sqrt{2}+\sqrt{6}\right)$ After analyzing the drawing the figure from below shows all all the relationships which I could found and it is summarized as follows: The triangle $\textrm{COP}$ is isosceles since it shares the same side from the radius of the circle and since $\measuredangle PC=30^{\circ}$, then all is left to do is to apply the identity which it says that the sum of inner angles in a triangle must equate to $180^{\circ}$. $$2x+30^{\circ}=180^{\circ}$$
$$x=\frac{150^{\circ}}{2}=75^{\circ}$$ Since $\measuredangle OCP = \measuredangle OPC$, its supplementary angle would become: $$180^{\circ}-75^{\circ}=105^{\circ}$$ Since it is given from the problem: $$\measuredangle COA = 90^{\circ}$$ therefore its complementary angle with $\measuredangle COP = 30^{\circ}$ would become into: $$\measuredangle POA = 60^{\circ}$$ Since $PO = OA$ this would also make another isosceles triangle and by recurring to the previous identity: $$2x+60^{\circ}=180^{\circ}$$
$$x=\frac{180^{\circ}-60^{\circ}}{2}=60^{\circ}$$ Therefore the triangle POA is an equilateral one so, $$\textrm{PA=10 inches}$$ As $\measuredangle OPB = 105 ^{\circ}$ and $\measuredangle OPA = 60^{\circ}$ then its difference is:
$\measuredangle APB = 45^{\circ}$. From this its easy to note that $\measuredangle PAB = 45^{\circ}$. Since the vertex $\textrm{B}$ of the triangle $\textrm{ABP}$ is $\measuredangle = 90 ^{\circ}$. I did identified a special right triangle with the form $45^{\circ}-45^{\circ}-90^{\circ}$ or $\textrm{k, k,}\,k\sqrt{2}$. By equating the newly found side $\textrm{PA = 10 inches}$ to $k\sqrt{2}$ this is transformed into: $$k\sqrt{2} = 10$$ $$k = \frac{10}{\sqrt{2}}$$ From this is established that: $$AB = \frac{10}{\sqrt{2}}$$ Since we have $\textrm{AB}$ we also know $\textrm{PB}$ as $AB = PB = \frac{10}{\sqrt{2}}$ Therefore all that is left to do is to find $\textrm{CP}$ as $CP+PB = BC$ To find $CP$ I used cosines law as follows: $$a^{2}=b^{2}+c^{2}-2bc\,\cos A$$ Being a, b and c the sides of a triangle ABC and A the opposing angle from the side taken as a reference in the left side of the equation. In this case $$(CP)^{2}= 10^{2}+10^{2}-2(10)(10)\cos30^{\circ}$$
$$(CP)^{2}= 10^{2} \left(1+1-2\left(\frac{\sqrt{3}}{2}\right)\right)$$
$$CP = 10 \sqrt{ 2-\sqrt{3}}$$ Therefore $CP = 10 \sqrt{ 2-\sqrt{3}}$ and we have all the parts so the rest is just adding them up. $$CP+PB= BC = 10 \sqrt{ 2-\sqrt{3}} + \frac{10}{\sqrt{2}}$$ $$AB= \frac{10}{\sqrt{2}}$$ $$AB + BC = \frac{10}{\sqrt{2}} + 10 \sqrt{ 2-\sqrt{3}} + \frac{10}{\sqrt{2}}$$ And that's how far I went, but from then on I don't know if what I did was correct or did I missed something? as my answer doesn't appear within the alternatives. The best I could come up with by simplifying was: $$\frac{10\sqrt{2}}{2}+10\sqrt{2-\sqrt{3}}+\frac{10\sqrt{2}}{2}$$ $$10\sqrt{2}+10\sqrt{2-\sqrt{3}}$$ $$10\left(\sqrt{2}+\sqrt{2-\sqrt{3}}\right)$$ and, that's it. But it doesn't seem to be in the choices given. Can somebody help me to find if did I do something wrong?. If a drawing is necessary please include one as I'm not savvy enough to notice these things easily.","['euclidean-geometry', 'algebra-precalculus', 'geometry']"
2915162,Intersection of conjugate sets on $H\leq GL_2(\mathbb{Q})$,"Define $N=\bigcap\limits_{g\in G}(gHg^{-1})$, and let $G=(GL_2(\mathbb{Q}),\cdot)$, and $H=\left\{\bigg[\begin{array}[h]{11}
a&0\\
0&b
\end{array}\bigg]\bigg|\:ab\neq0\right\}$. Note that $\cdot$ is ordinary matrix multiplication, and  $H\leq G$. I am trying to determine the elements in $N$, though I am having difficulty where to start. I understand the concept of an indexed intersection of sets, although I am clueless as to the criteria for which elements are in each $gHg^{-1}$? Any direction appreciated.",['group-theory']
2915174,Probability problem involving mappings of a finite set into itself.,"One mapping is selected at random from all the mappings of the set $\{1,2,\ldots,n\}$ into itself. What is the probability that $(i)$ a specified element $i$ is transformed into another specified element $j$? $(ii)$ the elements $i_1,i_2,\ldots,i_h$ are transformed into the elements $j_1,j_2,\ldots,j_h$ respectively?","['permutations', 'combinations', 'combinatorics', 'probability-theory', 'probability']"
2915204,Proving any polynomial of odd degree must have at least one real root,"Here's my attempt: Suppose that $p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n$ where $n$ is odd, $a_i$ are constants with $a_n \ne 0$. Assume that $a_n > 0$. Then
$$ p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n = x^n \left( \frac{a_0}{x^n} + \ldots + a_n \right) $$ Now as $x \to -\infty $, $x^n \to - \infty$ and $\left( \frac{a_0}{x^n} + \ldots + a_n \right) \to a_n > 0$, and so $p(x) \to -\infty $. And similarly as $x\to +\infty $ , $p(x) \to +\infty$. By definition of $\lim_{x \to \pm \infty } p(x) = \pm \infty$, for any $M>0$ there are points $x_1$ and $x_2$ such that $p(x_1) < -M < 0 < M < p(x_2)$ and thus from the intermediate value theorem, there is a point c between $x_1$ and $x_2$ with $p(c)=0$ If $a_n <0$, we can do the same as above. Is my proof correct? And I was wondering how would I write my proof in a single case setting since IVT is applied in two very similar cases ($a_n > 0$ and $a_n <0$). Hints would be enough.","['continuity', 'real-analysis']"
2915211,Compute expectation of stopped Brownian motion,"Let $B_t$ be a standard Brownian motion starting from $0$. Let $\tau_a$ be the hitting time of Brownian motion hitting $a$ and $a>0$. I want to calculate $E[X_T] = E[B_{T \wedge \tau_a}]$ with $X_t$ defined as $B_{t \wedge \tau_a}$. $T$ is some positive number. Let $v_a(t)$ denote the density function of $\tau_a$, namely $v_a(t) = \frac{a}{\sqrt{2\pi}t^{\frac{3}{2}}}e^{-\frac{a^2}{2t}}$. I am not quite sure about the condition of optional stopping theorem here. And, since this theorem is not introduced in my class, our professor suggests that the following equation holds:
$$
\frac{d}{dt}E[X_t]=av_a(t) + \int_{-\infty}^a x\frac{\partial}{\partial t}u(t,x)\ dx
$$
where $u(t,x) = \frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}$ is the density of $B_t$. I don't know how this works and how to get this hint. Could anyone explain a little bit? Thank you for any help!","['expected-value', 'stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
2915256,"The possible values of $x$, if $\tan^{-1} x>\cot^{-1}x$","What are the possible values of $x$, if $\tan^{-1}x >\cot^{-1}x$? We have $\tan^{-1}x >\cot^{-1}x\implies \tan^{-1}x -\cot^{-1}x>0 \implies \tan^{-1} x-\tan^{-1}1/x>0\implies \tan^{-1}\dfrac{x-1/x}{1-x.1/x}>0$. What can I do now?","['trigonometry', 'inverse-function', 'inequality']"
2915268,"If $\int f(x)dx\leq \liminf\int f_n(x) dx$, then $\lim\limits_{n\to\infty}\int f_n(x)dx=\int \lim\limits_{n\to\infty}f_n(x)dx=\int f(x)dx$","I found this question in my schools's Riemann's past question. Let $\{f_n\}$ be a sequence of non-negative function which converges to an integrable function $f$ and supposing $ f_{n}\leq f(x)$ for each $n$. It is known that 
\begin{align} \int f(x)dx\leq \liminf\int f_n(x) dx\end{align}
Then, show that \begin{align} \lim\limits_{n\to\infty}\int f_n(x)dx=\int \lim\limits_{n\to\infty}f_n(x)dx=\int f(x)dx\end{align} QUESTIONS I believe this question is based on measure theory. Should this question appear at all in Riemann integration? Can we do this proof by Riemann integration? If no, then I'll need help but below is my trial. MY TRIAL Since $f_{n}\leq f(x)$ for each $n$, then
\begin{align} \int \liminf f_n(x)dx \leq\int f(x)dx\end{align}
I cannot interchange limit and integral since I am not sure if $f_n$ converges to $f$ uniformly. SO, I'm stuck here. Please, can someone help me? Mind you I have not been introduced to Measure theory yet but I can learn the steps if someone puts me through. That's if there's no other way but Measure theory approach. Thanks!","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'riemann-integration']"
2915413,Is there a category theoretic characterisation of the exponential map from differential geometry?,"While I have only a shallow understanding, I like category theory. I find definitions and proofs in terms of category theoretic concepts to be very clean and deep, often cutting to the core of a concept and what its ""about"", and why we care about it, especially definitions in terms of universal properties. I wanted to know if there is a definition of the exponential map from differential geometry in category theoretic terms. The definition I know of is for a manifold $M$, and a point $p \in M$, $\exp: (v \in T_pM) \mapsto \gamma_v(1)$ Where $\gamma$ is the locally unique geodesic though $p$ of velocity $v$ such that $\gamma_v(0) = p$. I may have slightly messed up the definiton. I would be especially interested in definitions in geometrical/topological terms with as few constructions/parametrisations as possible.","['universal-property', 'category-theory', 'differential-geometry']"
2915461,Is Taylor Series changing from an uncountable basis to a countable basis?,"Say we've got an analytic function $f(x)$ from $\mathbb R$ to $\mathbb R$. It has an uncountable number of components in this basis, since there is one value of $f(x)$ for each $x$ and $x$ varies continuously. When we do a Fourier transform and hence change the variable to $w$, there is still an uncountable number of components, because $w$ too varies continuously. But, in case of a Taylor series, the function can be represented by a countable number of components: $[f(0)\,\, f'(0)\,\, f''(0)\,\, f'''(0)\, \cdots]$. In this case, the number of components changes from uncountable infinite to countably infinite, when we change the basis. Am I interpreting this right? The number of components never changes when we change the basis of a finite-dimensional vector space. Why does the number of components change here?","['change-of-basis', 'functions', 'linear-algebra', 'functional-analysis']"
2915465,Evaluate $\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy$,"Evaluate, where $D=\{(x,y): 0\le x \le 1 , 0\le y \le x\}$,
  $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy$$ My solution and question $$\iint_D\frac{\sin x}{x}\;\mathrm dx\mathrm dy = \int_0^1\frac{\sin x}{x}\;\mathrm dx\int_0^x\mathrm dy =\\= \int_0^1\sin x \;\mathrm dx  = [-\cos x ]_0^1=\ 1-\cos 1$$ Is it correct? Thanks in advance.","['integration', 'multivariable-calculus', 'calculus']"
2915470,Matrix Geometric Series,"For scalar geometric series, we know
$$ \sum_{k=0}^{\infty} x^k  = \dfrac{1}{1-x} \text{ and } \sum_{k=0}^{\infty} kx^{k-1} = \dfrac{1}{(1-x)^2}\,.$$ Does the second one extend to square matrices? We know for $A$ being a $n \times n$ square matrix and $\|A\| < 1$, $\sum_{k=0}^{\infty} A^k = (I-A)^{-1}$. Does the following hold? $$\sum_{k=0}^{\infty} k A^{k-1} = (I-A)^{-2} $$","['matrices', 'power-series', 'linear-algebra']"
2915480,"Q Exercise 4, Hecke Algebras - Daniel Bump","I'm struggling with exercise 4 in Bump's Stanford Hecke Algebra notes linked here It states the following: Let $G$ be a finite group and $V,W$ vector spaces. Let $C(G,V)$ denote the spaces of maps from $G$ to $V$, which has the $G$-representation $\rho_{V}$ given by $$
 (\rho_{V}(g)f)(x) = f(xg) 
$$ Suppose that $T$ is a linear map from $C(G,V)$ to $C(G,W)$ that commutes with the $G$-action, i.e that $$
 T(\rho_{V}(g)f) = \rho_{W}(g)T(f)
$$ Prove that there exists a map $\lambda : G \rightarrow \operatorname{Hom}(V,W)$ such that $T(f) = \lambda * f$, where $*$ denotes convolution. The notes also has a lemma (Lemma 4) proving the above result in the case that $V$ and $W$ are 1-dimensional, and my hope was that I could adapt that proof to tackle this exercise but it has thus far left me stumped. In the linear case $\operatorname{Hom}(V,W)$ becomes $\mathbb{C}$ and so the space of maps $f : G \mapsto \mathbb{C}$ is a unital convolution algebra (i.e the group algebra $\mathbb{C}[G]$), with the unit given by the characteristic function of the identity element of $G$, $\delta$ say. Then $\delta *f = f * \delta$ for all $f \in \mathbb{C}[G]$, and so in particular if such a $\lambda$ existed, then $\lambda = \lambda * \delta = T(\delta)$, and then one checks that $T(\delta)$ satisfies the requirements. I would appreciate some assistance \ advice on tackling this exercise. Lemma 4 would suggest that we might want to try and find an element of $C(G,V)$, $\tau$ say, such that $F * \tau = F$ for every $F : G \rightarrow \operatorname{Hom}(V,W)$, and then continue on as in Lemma 4. But I have thus far been unable to construct such a $\tau$ (if one even exists).","['representation-theory', 'hecke-algebras', 'linear-algebra']"
2915494,Max min normalization on random variables,"Suppose I have three i.i.d. random variables $X_1, X_2, X_3$, and I do ""max-min normalization"" on them. $$X_i \mapsto \frac{X_i - \min\limits_i X_i}{\max\limits_i X_i - \min\limits_i X_i} $$ Let $Y$ be the location of the ""middle"" point after normalization. (It lies in $[0,1]$.) What is the distribution of $Y$? How can I extend to the case of $n$ i.i.d. variables $X_1, \dotsc, X_n$, asking about the distribution on the $n-2$ middle points? We could give $X_i$ specific distributions such as $\mathcal{N}(0,1)$ for concreteness.","['probability-distributions', 'probability']"
2915495,"Dimension of vector space, countable, uncountable?","In set theory, when we talk about the cardinality of a set we have notions of finite, countable and uncountably infinite sets. Main Question Let's talk about the dimension of a vector space. In linear algebra I have heard that vector spaces are either of finite dimension (for example $\mathbb{R}^n$ ) or infinite dimension (for example $C[0,1]$ ). Why don't we have notions of countably infinite dimensional and uncountably infinite dimensional vector spaces? Maybe, I am missing the bigger picture. Extras P.S. A long time ago, I attended a talk given on enumerative algebraic geometry and the professor said, ""I always think of a positive natural number as the dimension of some vector space"". Can this idea then be extended to vector spaces of uncountably infinite dimension by considering transfinite numbers as denoting the dimension of some vector space?","['cardinals', 'linear-algebra', 'real-analysis']"
2915506,Is every absorbing set a neighborhood of zero?,"I'm studying functional analysis, currently a chapter about topological vector spaces. It is stated that every neighborhood of zero is an absorbing set. But I was wondering if the reversed statement is also true? Isn't, as an counterexample, in $X =>R^1$ the disconnected set $A = [2,1) \cup \{0\} \cup (-1,-2]$ also absorbing and does not contain an open set, which contains zero (so isn't a neighborhood of zero)?
For every $x\in X$ I can construct some $t>0, t=1/x \pm \epsilon$ such that $t \cdot x \in A$. For $x=0$ I can choose any $t>0$. Edit: We defined absorbing as follows. Let $X$ be a vector space, $A$ a subset of $X$. Then $A$ is called absorbing, if for every $x \in X$ there exists a $t > 0$ such that $tx \in A$.",['functional-analysis']
2915515,Area of ellipse using double integral,"I am trying to find the area of a quadrant of an ellipse by double integrating polar coordinates but the answer I'm getting is incorrect. ellipse : $ x^2/a^2 + y^2/b^2 =1 $ Any point on ellipse : $ ( a\cos(\theta), b\sin(\theta)) $ At $ \theta$, taking $ d\theta $ segment, Thus $ r^2 = a^2\cos^2(\theta) + b^2\sin^2(\theta) $ [Using pythagoras theorem] 
$$ Area = \int_{0}^{\pi/2} \int_{0}^{\sqrt{a^2\cos^2(\theta) + b^2\sin^2(\theta)}} rdrd\theta $$ $$ = 1/2 \int_{0}^{\pi/2}  r^2  \Big|_{0}^{\sqrt{a^2\cos^2(\theta)+b^2\sin^2(\theta)}}  d\theta $$ $$ = 1/2 \int_{0}^{\pi/2} (a^2\cos^2(\theta)+b^2\sin^2(\theta))  d\theta $$
$$ = 1/2 \int_{0}^{\pi/2} ((a^2 - b^2)\cos^2(\theta)+b^2)  d\theta $$
$$ = 1/4 \int_{0}^{\pi/2} (a^2 - b^2)(1+ \cos(2\theta))  d\theta  +2b^2  d\theta $$
I am getting $$ \pi/8 (a^2 + b^2).$$ But the correct answer is $ \pi ab/4 $","['integration', 'area', 'conic-sections', 'calculus', 'multiple-integral']"
2915531,Period of $\sin^2 x +\cos^4 x$?,"How do I approach this problem? I tried reducing it to $\sin ax + \cos bx$ form but can't. I converted it to 1+ $\sin^2 x \cos^2 x $, but I don't know if I can get the answer from it. Would someone please help me with this? The answer is $\pi$.","['trigonometry', 'functions']"
2915549,Sum of the series $\sum_{k=0}^n \frac{1}{k+1} \binom{n}{k}$ [duplicate],"This question already has answers here : Express $1 + \frac {1}{2} \binom{n}{1} + \frac {1}{3} \binom{n}{2} + \dotsb + \frac{1}{n + 1}\binom{n}{n}$ in a simplifed form (4 answers) Closed 3 years ago . The task is to transform $\sum_{k=0}^n \frac{1}{k+1} \binom{n}{k}$ into a compact and not recursive formula. I've done a bunch of similar (as I thought at first) series. I'll describe my method. I name $\sum_{k=0}^n f(k)=a_{n}$, and then describe $a_{n}=a_{n-1}+f(n)$. Then I solve recursive formula by, for example, generating functions. It works for a series like $\sum_{k=0}^n k^2(k+2)$, so a series which doesn't contain $n$ inside. About the main question, I tried to transform it with some formulas I know, like $\frac{n-k}{k+1}\binom{n}{k}=\binom{n}{k+1}$ or $\binom{n}{k}=\binom{n-1}{k-1}+\binom{n-1}{k}$, but none of them worked. With the last one I was able to transform it to like $a_{n}=a_{n+1}+\sum_{0}^{n-1}\frac{1}{k+2}\binom{n-1}{k}$, but it doesn't do anything, because there is $\frac{1}{k+2}$ and not $\frac{1}{k+1}$. I would appreciate any hints! Also, I thought about using the series $\sum_{k\geq0} \binom{n}{k}x^{k}=(1+x)^{n}$, because it looks like $\sum_{k=0}^n \binom{n}{k} \frac{1}{k+1}=\sum_{k\geq 0}\binom{n}{k} \frac{1}{k+1}$, and it looks pretty familiar.","['power-series', 'discrete-mathematics', 'sequences-and-series']"
2915564,Find this integral $I=\int\frac{x}{(1-3x)^{3/2}(x+1)^{3/2}}dx$,"Find the integral $$I=\int\dfrac{x}{(1-3x)^{3/2}(x+1)^{3/2}}dx$$ My try: $$(1-3x)(x+1)=-3x^2-2x+1=-3(x+1/3)^2+\dfrac{4}{3}$$ Thus
$$I=\int\dfrac{x}{(\frac{4}{3}-3(x+\frac{1}{3})^2)^{3/2}}dx$$",['integration']
2915572,Impossible permutations of the Gear Cube,"If you're familiar with the group properties of the Rubik's Cube, you will probably know that, under the action of the standard moves, all possible permutations of the (unoriented) edge pieces are possible and all possible permutations of the (unoriented) corner pieces are possible, but because the action on these two sets is not independent, not all combinations of these are possible -- it turns out that the permutations on the two sets need to have the same sign.  I am interested in understanding the similar restrictions that exist on the Gear Cube. In case you're not familiar with a Gear Cube, it's structure is similar to a Rubik's cube except that it is geared in such a way that a half turn of any face elicits a quarter turn of the adjacent middle slice.  A quarter turn of a face is not therefore possible as the middle slice would be misaligned, as in the picture below. This sounds complicated, but in fact the restriction to half turns of faces, simplifies things in lots of ways.  For example, the 8 corners split into two sets (tetrahedrons) of 4, which never intermingle, and orientation is never an issue. the 12 edge pieces split into three sets (planes) of 4, which never intermingle. I should also add that there is some gearing on the orientation of the edge pieces, which again looks complicated at first but ultimately all it means is that the orientation of the edges is trivial and we don't need to worry about it. I've approached the problem by considering one corner (right-back-down) as fixed, and considering the actions of the group generated by a clockwise (half) turn of the left(L), front(F) and upper(U) faces.  This restricts the number of moves to worry about and also fixes the planes in which the edge pieces move.  The disadvantage is it allows the centre pieces to move, and ultimately it is the restriction on this movement that's causing me the most difficulty. As with a normal Rubik's Cube, the centres are fixed in space relative to each other and as such their permutations are exactly the rotations of a solid Cube. This is $S_4$, and is normally seen to be such by considering the action on the diagonals. Note that these permutations determine the permutations of the 3 main axes, which is essentially $S_3$ and can be considered as a quotient of the $S_4$ mentioned earlier. Various other sets of 4 seem to behave similarly.  The action on one particular set of 4 edges can be thought of as acting on the vertical sides, horizontal sides and diagonals of the square they form, again giving $S_3$ as a quotient of $S_4$. Perhaps more surprisingly the permutations of the tetrahedron with the fixed corner operates as a quotient of the permutations of the tetrahedron of free corners. It turns out the overall group action on each of the sets of 3 is the same, so for example if we fix the tetrahedron with the fixed corner we also fix the axes of centres.  However the sets of 4 do not match up in the same way, for example a full rotation of a face fixes the corners, but not the centres and one of the edge planes. Apparently, the permutations of the edges fixes the centres, but I cannot prove this.  Has anyone got any ideas? I've tried looking at the $S_4$s from the point of view of $S_3 \rtimes V_4$, thinking that the fact that all the $S_3$s were the same would help, but it hasn't much.  The behaviour of the $V_4$ bit still seems very complicated. I hope all that made some sense to someone.  Any help much appreciated! EDIT Perhaps I can put it another way.  The group is generated by 3 moves, $F$, $L$ and $U$, which can be regarded as acting on five 4-element sets: 4 free corners, the 4 cube diagonals associated with the movement of the centres, and three sets of 4 edge pieces. Thus the group can be regarded as a subgroup of $(S_4)^5$, with generators as follows: $$
L=((12), (1423), (12), (12), (1324)) \\
F=((14), (1243), (23), (1342), (23)) \\
U=((13), (1234), (1432), (13), (13)) \\
$$ By considering other actions, I came to consider the quotient of each copy of $S_4$ with its normal subgroup $V_4= \{e, (12)(34), (13)(24), (14)(23)\}$.  It seemed like a significant breakthrough that for each generator there was a distinct coset in $\cfrac{S_4}{V_4}$ in which all its coordinates lay: $(12)V_4$, $(23)V_4$, $(13)V_4$ for $L$, $F$, $U$ respectively.  This means that effectively each element of the group can be expressed as an element of $S_3\times (V_4)^5$, which is a lot smaller than $(S_4)^5$, but still 4 times bigger than what I believe to be the answer I'm aiming for. That's pretty much where I'm stuck, as the way the group acts on $S_3\times (V_4)^5$ is not straightforward. UPDATE Having done a bit more reading, it seems that the missing tool I need is Schreier's lemma . Essentially the idea is that given a subgroup  $H \subset G=<s_1, ..., s_n>$ we choose a single representative $t_j$ from each coset of $H$ in $G$ and then perform a simple calculation on each $s_i, t_j$ pair to create a set $\{u_{ij}\}$, which the lemma tells us generates $H$. So we can take $H$ to be the '$(V_4)^5$' bit of the group which fixes the $S_3$ component (by fixing one/all of the 3-element sets described earlier), thereby giving us a generator set for just this '$(V_4)^5$' bit with the '$S_3$' bit disentagled, as I was wanting above.  Our original group has 3 generators and $H$ has 6 cosets (one for each element of $S_3$), so there are 18 calculations, which even done manually is fairly manageable if somewhat tedious. Ignoring the repeats, we actually get a generator in $(V_4)^5$ with 13 elements.  The next step is to fix each $V_4$ in turn (by fixing one of the 4-element sets described earlier) in order to simplify things further. On the face of it the next step will have 4$\times$13 calculations, but they're much easier now we're working in $V_4$ and because it's Abelian some can immediately be identified as reordered repeats of other calculations. In this manner, I got down to the $(V_4)^2$ bit of the group in question being generated by $\{(a,a), (b,b)\}$ from which it can easily be seen that the permutation of the final 4-element set is completely determined by the others, as required. So that pretty much answers the question, albeit in a rather cumbersome computational way.  I suspect there may be a more elegant solution in terms of group actions, so I'd still be interested if anyone can think of one!","['group-actions', 'group-theory', 'puzzle', 'rubiks-cube']"
2915582,"The series $\sum_{n=0}^\infty \frac{(-\beta)^n}{n!}\Gamma\left(\frac{n+1}{2},\alpha\right)$","I've worked out the solution to an integral which involves the following expression: $$\sum_{n=0}^\infty \frac{(-\beta)^n}{n!}\Gamma\left(\frac{n+1}{2},\alpha\right)$$ Where $\Gamma(s,z)$ is the upper incomplete gamma function. My question is: is there a convenient non-series expression for this (i.e. an expression in terms of known special functions?). Note that it is very similar to the multiplication theorem given on the Wikipedia page (unfortunately I couldn't find a better source) for the incomplete gamma function, which states: $$\Gamma(s,z) =\frac{1}{t^s}\sum_{n=0}^\infty \frac{\left(1-\frac{1}{t}\right)^n}{n!}\Gamma\left(s+n,tz\right)$$ If that helps at all. Thanks!","['gamma-function', 'sequences-and-series']"
2915652,10 passengers in a boat,"There are 10 passengers in a boat and among them there is a famous pop star who everybody knows, while he doesn’t know any of the other passengers.
  The other passengers either know some of the others or they do not. There is an 11th person though, who doesn’t know the singer. In order to determine who he (she) is, he asks one particular passenger, say, Aaron, if he knows some other passenger, say Jenny and Aaron must reply by yes or no. What is the maximum number of such questions the 11th person must ask so as to determine the singer? Let’s assume that the 11th person does not know whether the singer is male or female and that the relationships are not mutual (i.e. if Aaron knows Jenny, this doesn’t necessarily mean that Jenny knows Aaron). If we consider all the pairs (which are not mutual) we have 90 such pairs (each person with each other). Assuming the singer is the $ith$ person, of those, we must deduct all pairs with this person, that is, 9 more pairs. Therefore we have 81 pairs.
Let's say the 11th person picks one of the others at random, say the 5th, and asks him if he knows person 1, 2, 3, ... 10. If we are lucky and he doesn't know anyone, then he is the pop star. Otherwise, let's assume he doesn't know persons 2, 3 and 6. He must then ask person 2 if he knows 5, then person 3 if he knows 5 and person 6 if he knows 5 and so on. But this is a never ending story...  I am a bit confused! Plus that about 55 years have passed since I finished high school :) Any ideas??",['combinatorics']
2915670,When is the set of tensors of rank at most $r$ closed?,Consider the vector space $V=A_1\otimes A_2\otimes\dots\otimes A_k$ where $k\geq 2$ and let $\sigma_r$ be the set of tensors in $V$ of rank at most $r$ . It seems clear that $\sigma_r$ is Zariski (Euclidean?) closed in the following cases : 1 - $k=2$ and $r$ is anything 2 - $r=1$ and $k$ is anything 3 - $\sigma_r=V$ Is this list complete? Can I change the third case as $r=\prod_{i=1}^k \dim A_i$ and if not with some other integral condition on $r$ ?,"['algebraic-geometry', 'multilinear-algebra']"
2915695,Minimize ${tr(\mathbf{X}^T\mathbf{P}\mathbf{X})}/{tr(\mathbf{X}^T\mathbf{Q}\mathbf{X})}$ s.t. $\mathbf{X}^T\mathbf{X}=\mathbf{I}$,"We have $$C(\mathbf{X})=\frac{tr(\mathbf{X}^T\mathbf{P}\mathbf{X})}{tr(\mathbf{X}^T\mathbf{Q}\mathbf{X})}$$ where $\mathbf{X}$ is an $N$ x $M$ matrix of unknowns and $\mathbf{P}$ and $\mathbf{Q}$ are $N$ x $N$ positive definite constant matrices. We want to find $\mathbf{X}$ that minimizes $C$ and $\mathbf{X}^T\mathbf{X}=\mathbf{I}$ . Solution for the case without the orthonormality constraint could be found here . * solution to the special case of $N=3, M=2, Q=I$ would also be of interest * Some context: To give some context and geometric interpretation, assume we have $N_s$ points in an $N$ dimensional space, given by $u_1...u_{N_s}$ , each point has a label (or assigned to a class), for simplicity, assume there are two classes: red and blue. We want to find a mapping from $N-$ dimensional space to $M-$ dimensional space, mapping $u_1...u_{Ns}$ to $v_1...v_{Ns}$ where $v_i = X^Tu_i$ with the property that in the $M$ dimensional space, the points with same color are close to each other relative to points with different colors. Using the above formulation, we find $P$ and $Q$ as $P=A^TA$ where $A$ is the matrix with rows consisting of all pairwise distances (u_i - u_j) where $i,j$ are the indices of points with similar label and similarly $Q=B^TB$ where $B$ is the matrix with all pairwise distances for points with different labels. This could also be modelled as a projection to the $M$ dimensional space, that retains inter-class variation but minimizes intra-class variation. We hope to use this as a general method for dimensionality reduction with special application in batch effect removal and classification for genomic data. Here is a numerical example: 
We have 8 points in $N=3$ dimensional space given by their coordinates: $$
 \begin{bmatrix}
\mathbf{x} & \mathbf{ y} & \mathbf{z} & \mathbf{label}\\ 
18 &     5  &   17 & blue \\ 
24  &   2  &  10 & blue \\ 
   12  &   13   &   1 & blue \\
   11    &  7  &    9 & blue \\
   20  &    4   &  22 &  red \\
    8    & 15  &   14 & red \\
   21   &  23  &    3& red \\
   19    & 16   &   6& red \end{bmatrix}
$$ Then for matrices $P$ and $Q$ we have: $$
P=\begin{bmatrix}
 875&  -267&    79\\
 -267&   999& -1033\\
  79& -1033&  1390
\end{bmatrix}
$$ $$
Q=\begin{bmatrix}
  884 & -174&  103\\
-174 &1960 &-785\\
  103 &-785& 1454
\end{bmatrix}
$$ For the case $M=1$ , optimal $X$ given by the eigenvector corresponding to the smallest eigenvalue of $Q^{-1}P$ is $X=[0.198, 0.782, 0.591]^{T}$ and the transformed values $v = X^T u = [17.53, 12.23, 13.13, 12.97, 20.10, 21.59, 23.91, 19.82]$ Following is the plot for the data used in the above example: Here is the projected points for the optimal $X$ obtained above: So, for $M=2$ , we want to find an orthonormal transformation matrix $X$ that maps the data points to a two dimensional space (instead of 1-dimensional space in the above example). We want the red dots to be close to each other and the blue dots be close to each other and far from the red dots in the transformed space.","['calculus', 'matrix-calculus', 'algebraic-geometry', 'linear-algebra', 'optimization']"
2915705,Expectation of stochastically bounded expression.,"For any $\mu>1/2$ let $Y_\mu \sim \mathrm{Poisson}(\mu)$ and let $c<\mu$. Define 
$$
X_{\mu} = \frac{Y_\mu+c-\mu}{\mu},
$$
and then one can realise (pretty sure I have established this) that
$$
\log(1+X_\mu) = X_\mu -\frac{1}{2}X_\mu^2 + \mathcal{O}_p(X_\mu^3), \quad \text{as } \mu\to\infty
$$
which by definition of Big O' in probability means that $\forall \epsilon >0$ there exists $M>0$ and $\mu_0>0$ such that 
$$
P \left( \left| \frac{\log(1+X_\mu) -X_\mu + \frac{1}{2}X^2_\mu }{X^3_\mu}  \right|> M \right)  < \epsilon \quad \text{for all } \mu>\mu_0.
$$ Now I want to establish some kind of normal Big O' statement of the expectation of this expression. More specifically I want something like $$
E\log(1+X_\mu) = EX_\mu -\frac{1}{2}EX_\mu^2 + \mathcal{O}(EX_\mu^3), \quad \text{as } \mu\to \infty
$$
because this would allow me to derive what I want to show, which is 
$$
E\log(1+X_\mu) = EX_\mu -\frac{1}{2}EX_\mu^2 + \mathcal{O}\left( \frac{1}{\mu^2} \right), \quad \text{as } \mu\to \infty,
$$
since $EX_\mu^3 =\frac{c^3+3c\mu+\mu}{\mu^3} = \mathcal{O}\left( \frac{1}{\mu^2} \right)$ as $\mu\to \infty$. Problem: How do I derive the $\mathcal{O}$ statement with the expectations from the $\mathcal{O}_p$ statement?","['stochastic-analysis', 'stochastic-approximation', 'asymptotics', 'expected-value', 'probability-theory']"
2915715,Deviation of empirical average from the mean for the fourth moment,"Suppose that $\{ X_n \}_{n \in \mathbb{N}}$ is a sequence of real i.i.d. random variables  (with finite fourth order moments) with mean $\mu$. We want to show that there exists a constant $C>0$ (independent of $N$) such that $$ \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i - \mu \bigg)^4 \bigg] \leq C N^{-2}. $$ By direct expansion, we get
\begin{eqnarray}
 \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i - \mu \bigg)^4 \bigg] & = & \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i  \bigg)^4 \bigg] - 4 \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i \bigg)^3 \mu \bigg] \nonumber \\
&  & +6 \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i  \bigg)^2 \mu^2 \bigg] - 4 \mathbb{E} \bigg[ \bigg( \frac{1}{N} \sum_{i=1}^N X_i  \bigg) \mu^3 \bigg] + \mu^4.
\end{eqnarray}
Clearly, by independence, the first two terms converge in $N$ with the order $O(N^{-3})$ and $O(N^{-2})$ respectively. But how about the remaining terms? 
Any ideas?","['statistics', 'central-limit-theorem', 'expected-value', 'sampling', 'probability']"
2915727,Largest root and asymptotics.,"I am considering the polynomial
$$
p_c(x)=x^{c+1}-x^c-1.
$$
Its largest root, denoted by $\lambda_c$, is contained in the open interval $(1,1+\frac{\ln c}{c})$. Making the ansatz $\lambda_c=1+x_c$ for a zero sequence $(x_c)$, I want to express more explicitly what happens with $\lambda_c$ as $c\to\infty$. Putting the ansatz into the polynomial, I get
$$
(1+x_c)^cx_c=1
$$
which yields, when applying logarithm,
$$
c\ln(1+x_c)+\ln(x_c)=0.
$$
Using the approximation $\ln(1+x_c)=x_c+f(x_c)$ with $f(x_c)=O(x_c^2)$ as $c\to\infty$, this gives
$$
cx_c+cf(x_c)+\ln(x_c)=0.
$$
Exponentiating and multiplicating with the factor $c$ yields
$$
cx_ce^{cx_c}=ce^{-cf(x_c)}
$$
and using Lamberts W-Function, ones gets
$$
x_c=c^{-1}W(ce^{-cf(x_c)}).
$$
Using the large argument approximation
$$
W(x)=\ln x-\ln(\ln(x))+o(1)\text{ as }x\to\infty,
$$
gives me
$$
x_c=\frac{\ln c}{c}-f(x_c)-\frac{\ln(\ln c)}{c}-\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}+o\left(\frac{1}{c}\right)\text{ as }c\to\infty
$$ My question is whether this can be also expressed as
    $$
x_c=\frac{\ln c}{c}+o\left(\frac{\ln c}{c}\right)\text{ as }c\to\infty?
$$ For the summands $o\left(\frac{1}{c}\right)$ and $\frac{\ln(\ln c)}{c}$, I can see immediately that they are indeed of order $o\left(\frac{\ln c}{c}\right)$ as $c\to\infty$. But for the summands $f(x_c)$ and $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}$ this is absolutely not clear to me. Edit (due to the comments): $x_c=o(1)$ and $x_c=O(\ln c/c)$. Both together imply $x_c^2=o(\ln c/c)$, as $c\to\infty$. Moreoever, $f(x_c)=O(x_c^2)$ and $x_c^2=o(\ln c/c)$ imply that $f(x_c)=o(\ln c/c)$ as $c\to\infty$. Hence, it remains to clarify whether $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}=o(\ln c/c)$ as $c\to\infty$.","['limits', 'asymptotics', 'analysis']"
2915738,How to partially differentiate the integral $\int_{0}^{x/\sqrt{t}}\exp(-\xi^2/4)d\xi$ w.r.t $x$ and $t$?,"How do I partially differentiate the following integral: $$
u(x,t) := \int_{0}^{x/\sqrt{t}}e^{-\xi^2/4} \,\mathrm d \xi
$$ I would like to calculate $\frac{\partial u}{\partial x}(x, t)$ and $\frac{\partial u}{\partial t}(x, t)$. I have tried to solve the indefinite integral: $$
\int e^{-\xi^2/4} \,\mathrm d \xi
$$ hoping to get a nicer formula, but even the CAS only spit out $$
\int e^{-\xi^2/4} \,\mathrm d \xi = \sqrt{\pi}\operatorname{erf}\left(\frac{\xi}{2}\right)=2\int_{0}^{\frac{\xi}{2}}e^{-\tau^2}\,\mathrm d\tau
$$ which is of not much help, since it is basically just a slight rearrangement of the orginal function. I have also tried to apply the fundamental theorem of calculus, by rewriting $$
u(x,t) := \int_{0}^{g(x,t)}h(\xi)d\xi = H(\xi)\Big|_0^{g(x,t)}=H(g(x,t))-H(0)
$$ but I'm also stuck here.","['integration', 'partial-derivative', 'analysis']"
2915791,Let $X$ be infinite. Then $X$ and $X\times\Bbb N$ are equinumerous,"Let $X$ be infinite. Then $X$ and $X\times\Bbb N$ are equinumerous. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help! My attempt: We denote $A$ and $B$ are equinumerous by $A\sim B$. Let $Y=\{(A,f) \mid A \in\mathcal{P}(X) \text{ and } f:A \to A\times\Bbb N \text{ is bijective }\}$. We define a partial order $<$ on $Y$ by $$(A_1,f_1) < (A_2,f_2) \iff A_1 \subseteq A_2 \text{ and } f_1\subseteq f_2$$ Since $X$ is infinite, there exists $A\subseteq X$ such that $A \sim \Bbb N$ (Here we assume Axiom of Countable Choice). Thus $A\times \Bbb N\sim \Bbb N\times \Bbb N\sim \Bbb N\sim A$. Hence there exists a bijection $f:A\to A\times \Bbb N$ and consequently $f\in Y$. Hence $Y\neq\emptyset$. Let $Z$ be a chain in $Y$, $F=\{f \mid \exists A \in\mathcal{P}(X),(A,f)\in Z\}$, and $f^\ast=\bigcup\limits_{f\in F}f$. $f^\ast$ is a mapping For $(a,x_1),(a,x_2)\in f^\ast$. Then there exists $(A_1,f_1),(A_2,f_2) \in Z$ such that $(a,x_1)\in f_1$ and $(a,x_2)\in f_2$. Since $Z$ is a chain, we can safely assume $(A_1,f_1) < (A_2,f_2)$. It follows that $f_1 \subseteq f_2$. Thus $(a,x_1),(a,x_2)\in f_2$ and consequently $x_1=x_2$ by the fact that $f_2$ is a mapping. Let $A^\ast=\operatorname{dom}f^\ast$. Then $A^\ast=\bigcup\limits_{f\in F} \operatorname{dom}f$. $\operatorname{ran}f^\ast=A^\ast\times\Bbb N$ For all $f\in F$, $f$ is a bijection from $\operatorname{dom}f$ to $\operatorname{dom}f\times\Bbb N$, and thus $\operatorname{ran}f = \operatorname{dom}f\times\Bbb N$. $\operatorname{ran}f^\ast=\bigcup\limits_{f\in F} \operatorname{ran}f =\bigcup\limits_{f\in F} (\operatorname{dom}f\times\Bbb N)=(\bigcup\limits_{f\in F} \operatorname{dom}f)\times\Bbb N=\operatorname{dom}f^\ast\times\Bbb N=A^\ast\times\Bbb N$. $f^\ast$ is injective Assume $(a_1,x),(a_2,x)\in f^\ast$. Then there exists $(A_1,f_1),(A_2,f_2) \in Z$ such that $(a_1,x) \in f_1$ and $(a_2,x) \in f_2$. Since $Z$ is a chain, we can safely assume $(A_1,f_1) < (A_2,f_2)$. It follows that $f_1 \subseteq f_2$. Thus $f_1(a_1)=f_2(a_1)$ and consequently $f_2(a_2)=x=f_1(a_1)=f_2(a_1)$. Hence $f_2(a_2)=f_2(a_1)$ and consequently $a_2=a_1$ by the fact that $f_2$ is injective. It follows that $f^\ast$ is injective. To sum up, $f^\ast:A^\ast\to A^\ast\times\Bbb N$ is bijective and hence $f^\ast \in Y$. Furthermore, $f^\ast$ is an upper bound of $Z$ by definition. Thus $Y$ satisfies the requirement of Zorn's Lemma and hence has a maximal element $(\bar{A},\bar{f})$. $X\sim X\times\Bbb N$ First, I will prove that $X\sim\bar{A}$. If not, then $X\setminus \bar{A}$ is infinite and consequently there exists $C\subseteq X\setminus \bar{A}$ such that $C\sim \Bbb N$. Thus there exists a bijection $\underline f:C \to C\times\Bbb N$. Hence $g=\bar{f} \cup \underline f$ is a bijection from $\bar{A}\cup C$ to $(\bar{A}\times\Bbb N)\cup (C\times\Bbb N)=(\bar{A}\cup C)\times \Bbb N$ since $C\cap \bar{A}=\emptyset$. Thus, $g:\bar{A}\cup C\to (\bar{A}\cup C)\times \Bbb N$ is bijective and $\bar{f}\subsetneq g$. This contradicts the maximality of $(\bar{A},\bar{f})$. Hence $X\sim\bar{A}\sim \bar{A}\times\Bbb N\sim X\times\Bbb N$ and consequently $X\sim X\times\Bbb N$.","['elementary-set-theory', 'proof-verification', 'infinity']"
2915836,Find inverse to ...,I got stuck and thought if you could give some guidance on how to proceed? find inverse to $$y=\frac{\sqrt{x\:}+3}{\sqrt[5]{x}+4}$$ 1 $$y=\frac{\sqrt{x\:}+3}{\sqrt[5]{x}+4}$$ 2$$\:y\:\left(\sqrt[5]{x}+4\right)=\:\:\sqrt{x\:}+3$$ 3 $$\:y\:\sqrt[5]{x}+4\:y=\:\:\sqrt{x\:}+3$$ 4 $$y\:\sqrt[5]{x}\:-\:\sqrt{x\:}=\:\:+3\:-4y$$,"['algebra-precalculus', 'functions']"
2915852,To prove $f(x)= x^3$ for all $x\in \mathbb{R}$,"Let $f:[0,1] \to \mathbb{R}$ be a continuous function such that $f(x) \geq x^3$ for $x \in [0,1]$. Also $$\int_{0}^{1} f(x) \mathrm dx= \frac14$$ Prove that $f(x)= x^3$ for all $x\in [0,1]$ I am completely lost in here. Any hint will be appreciated.","['calculus', 'functions', 'real-analysis']"
2915863,"How many homomorphisms from $\frac{\mathbb{Z}[x,y]}{(x^3+y^2-1)}$ to $\frac{\mathbb{Z}}{(7)}$?","How many homomorphisms from $\displaystyle\frac{\mathbb{Z}[x,y]}{(x^3+y^2-1)}$ to $\displaystyle\frac{\mathbb{Z}}{(7)}$? I already checked that $x^3+y^2-1$ has these five integer roots: $(-2,3),(-2,-3),(0,1),(0,-1)$ and $(1,0)$, so it is not an irreducible polynomial in $\mathbb{Z}$. The ring $\displaystyle \frac{\mathbb{Z}}{(7)}$ is the same as $\displaystyle\frac{\mathbb{Z}}{7\mathbb{Z}}$, right? Since $7$ will generate $7\mathbb{Z}$, so this ring $\displaystyle \frac{\mathbb{Z}}{(7)}$ is a finite ring of $7$ elements. But then how can I find the number of homomorphisms? I am also failing to analyze the order of elements that will generate the domain ring. Thanks.","['ring-theory', 'abstract-algebra', 'polynomials', 'ideals']"
2915872,Expression For $u_{n+1}=u_n^2+u_n$,"Find the formula of the sequence
  $$u_1=a, u_{n+1}=u_n^2+u_n$$ Is there a ""simple"" formula for the sequence ?",['sequences-and-series']
2915894,Variational distance of product of distributions,"Let $F(\bar{x})=\prod_{i=1}^{n}f(x_i)$ and $G(\bar{x})=\prod_{i=1}^{n}g(x_i)$, where $f(x)$ and $g(x)$ are probability density functions, and $\bar{x}=(x_1,\ldots,x_n)$. The variational distance between $F$ and $G$ is:
  $$V(F,G)=\int |F(\bar{x})-G(\bar{x})|d\bar{x}$$
  Can we write it in terms of the variational distance between $f$ and $g$? I know we could do such if it was KL divergence:
$$D_{KL}(F,G)=n D_{KL}(f,g).$$ Do we have such a simplification for variational distance as well?","['statistics', 'probability-theory', 'information-theory', 'total-variation', 'probability']"
2915906,Get quartiles and a half of the data,"The following table shows a frequency distribution of the scores obtained
in a test. Punctuation           (3, 4] (4, 5] (5, 6] (6, 7] (7, 8] (8, 9] (9, 10]
Number of participants  2      4      10     20     40     35      9 (a) The highest score reached by the bottom 20% of the participants and the score lowest obtained by the top 25% of the participants (b) The quartiles of the distribution For section a) to be continuous variables that can have any value within a range what I do is calculate the average for that 20%, that is to say who is in position 24. I calculate the relative frequency (that for this I need to know the absolute frequency) and multiplied it by the midpoint of the interval. I am applying according to the definition and the formula, but I am stuck. It is a note that is between (6,7] and the average I get 0.1, so the note would be 6.1. For b) of the quartiles I get that 1 is in (6,7], 2 is in (7,8] and 3 is in (8,9), but I do not know how to get the exact note I've done it that way, but I do not know if it's okay since it's the first time I've done something similar, can someone verify it? a)
I have the table Class | partici | Ni
3-4   |     2   | 2
4-5   |     4   | 6
5-6   |    10   | 16
6-7   |    20   | 36
7-8   |    40   | 76
8-9   |    35   | 111
9-10  |     9   | 120 For The highest score reached by the bottom 20% of the participants means the 20th percentile so P20 = 6+ ((0,20*120)-16)*1 /20 = 6,4 score The lowest score obtained by the top 25% of the participants means the 75th percentile, so it is calculated in the same way P75 = 8 + ((0,75*120)-76)*1 /35 = 8,4 score b) For the quartiles, it is the same as calculating the percentiles for 25% 50% and 75%. Q2 is equal to the median Q1 = 6 + ((0,25*120)-16)*1 /20 = 6,7 score

Q2 = 7 + ((0,5*120)-36)*1 /40 = 7,6 score

Q3 = 8 + ((0,75*120)-76)*1 /35 = 8,4 score =P75 (From the section a-))",['statistics']
2915919,Problem in do Carmo's book Riemannian geometry at space forms.,"I'm reading do Carmo's book, riemannian geometry and at page 163 is this lemma. I dont get the fact that $\varphi(q)=q.$ Can someone fill in the details?","['geodesic', 'riemannian-geometry', 'differential-geometry']"
2915931,"Difference between $[0, 1) \times [0, 1)$ and the 2-torus","Let me take the two dimensional case. I have seen a definition of the $2$-torus as $T^2 = \mathbb{R}^2 / \mathbb{Z}^2$ i.e. the quotient between the real space and the integer space. However, for the equivalence relation $x \sim y \iff \exists a \in \mathbb{Z}^2 : x = y + a$, I reckon the quotient space is simply $[0,1) \times [0,1)$. What is the difference between this and the torus?","['elementary-set-theory', 'general-topology', 'geometry', 'differential-geometry']"
2915935,Radius of a circle touching a rectangle both of which are inside a square,"Given this configuration : We're given that the rectangle is of the dimensions 20 cm by 10 cm, and we have to find the radius of the circle. If we somehow know the distance between the circle and the corner of the square then we can easily find the radius. (It's equal to $ \sqrt{2}\times R-R$ ) I really can't understand how to solve it. Any help appreciated.","['rectangles', 'circles', 'geometry']"
2915952,What is the probability of exactly one negative solution in a Fibonacci system of equations?,"The Fibonacci numbers denoted by $F_i$ for $i\ge1$ are $$1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,\cdots$$ where they satisfy the property $F_{i+2}=F_{i+1}+F_i$. I have listed the first $15$ numbers of the sequence as they will be useful for reference later. Now define $\theta_i$ as the concatenation of $1.F_i$ so that $1<\theta_i<2$. So for example, $$\theta_1=1.1,\quad\theta_6=1.13,\quad\theta_{15}=1.987.$$ Next, consider the following $3\times3$ system of equations: $$\begin{bmatrix}\theta_i&\theta_{i+1}&\theta_{i+2}\\\theta_{i+4}&\theta_{i+5}&\theta_{i+6}\\\theta_{i+8}&\theta_{i+9}&\theta_{i+10}\end{bmatrix}\begin{bmatrix}X\\Y\\Z\end{bmatrix}=\begin{bmatrix}\theta_{i+3}\\\theta_{i+7}\\\theta_{i+11}\end{bmatrix}$$ from which we can solve for $[X\quad Y\quad Z]^T$ but of course this would be very tedious. When $i=1,2$, both $X$ and $Y$ are negative but $Z$ is positive. When $i=3$, the opposite occurs. Question: For $i\le n$, what is the probability that exactly one of $X,Y,Z$ will be negative?","['fibonacci-numbers', 'systems-of-equations', 'recreational-mathematics', 'matrix-equations', 'probability']"
2916004,Uniqueness of measure on $\mu^*$ measurable sets,"Folland proves the following theorem: Let $\mathcal{A} \subset \mathcal{P}(X)$ be an algebra, $\mu_0$ a premeasure on $\mathcal{A}$, and $\mathcal{M}$ be the sigma algbera generated by $\mathcal{A}$. There exists a measure on $\mu$ on $\mathcal{M}$ whose restriction to $\mathcal{A}$ is $\mu_0$ - namely, $\mu = \mu^*|_{\mathcal{M}}$ where $\mu^*$ is the outer measure. If $\nu$ is another measure on $\mathcal{M}$ that extends $\mu_0$, then $\nu(E) \leq \mu(E)$ for all $E \in \mathcal{M}$, with equality when $\mu(E) < \infty$ I have a question about the second part of the statement, showing the equivalence of $\mu$ and $\nu$. When I did this myself, I used the following arguement: Let $E \subset X$ and $A_j \in \mathcal{A}$, then:
$$\nu(E) \leq \sum \nu(A_j) = \sum \mu_0(A_j) = \sum \mu(A_j) =  \mu(\bigcup A_j) \, \,\forall j \implies \nu(E) \leq \mu(E)$$
$$\mu(E) \leq \sum \mu(A_j) = \sum \mu_0(A_j) = \sum \nu(A_j) =  \nu(\bigcup A_j) \, \,\forall j \implies \mu(E) \leq \nu(E)$$ Folland does this a different way, is this correct though?","['measure-theory', 'proof-verification', 'real-analysis']"
2916015,Second derivative finite difference of a convolution of two functions.,"I have a discrete function $I(V)$ which is defined as
$$I(V) = R\big(V\big) * Q \big(V \big) = \int_a^b R\big(V\big) Q \big(V-V' \big) dV'$$ and I want to find $\frac{\partial^2R}{\partial V^2}$ but do not know $Q \big(V \big)$.  I am able to take the second derivative of $I(V)$ via finite difference:
$$\frac{\partial^2I}{\partial V^2}  \approx  \frac{I(V+V_{0})+I(V-V_{0})-2I(V)}{V_{0}^2}$$
and I know that derivatives can be applied to either function in a convolution:
$$\frac{\partial^2I}{\partial V^2} = \frac{\partial^2R}{\partial V^2} * Q \big(V \big) = R \big(V \big) * \frac{\partial^2Q}{\partial V^2} = \frac{\partial R}{\partial V} * \frac{\partial Q}{\partial V}$$
I also know that the process of taking the second derivative of a discrete function is the equivalent of convolving the function with the 3x3 Toeplitz matrix:
$$\begin{bmatrix}-2 & 1 & 0\\1 & -2 & 1\\0 & 1 & -2\end{bmatrix}$$
Can I find either $\frac{\partial^2R}{\partial V^2}$ or $Q \big(V \big)$ if I know $I(V)$ (and therefore $\frac{\partial^2I}{\partial V^2}$)?","['integration', 'derivatives', 'convolution', 'finite-differences']"
2916023,I don't see how the binomial theorem relates to the principle of inclusion and exclusion?,"I'm learning discrete maths as a hobby at the moment and I got stuck when the tutor starting relating the binomial theorem to the principles of inclusion and exclusion. The video I was watching is https://www.youtube.com/watch?v=GS7dIWA6Hpo&list=PLDDGPdw7e6Aj0amDsYInT_8p6xTSTGEi2&index=6 and the question starts at (14.25). I understand the binomial theorem, and I see how combinatorics is used to calculate the coefficients. I've seen multiple online tutorials where the binomial theorem has been used to describe the nature of inclusion and exclusion and I just don't understand. Exclusion/Inclusion formula: |A1 ∪ A2 ∪ A3| = |A1| + |A2| + |A3| − |A1 ∩ A2| − |A1 ∩ A3| − |A2 ∩ A3| + |A1 ∩ A2 ∩ A3| This makes sense because we have to exclude the cases where elements are counted twice (drawing venn diagrams helped me understand this). Binomial Theorem: $(A+B)^n = \sum_{k=0}^n {n\choose{k}} A^{n-k} B^{k}$ This formula makes sense to me again, but can someone please explain it to me in simple terms how the binomial theorem is even related to inclusion/exclusion? I've also seen proofs where examples substitute the x = 1 and y = -1 and we end up getting the binomial expansion to equal 0. I just don't see how we can relate that to PIE. Please help. EDIT: This part of the video is what confuses me (17.27). I don't get the ((1) + (-1)) part, what is he trying to show from that?","['inclusion-exclusion', 'binomial-theorem', 'binomial-distribution', 'discrete-mathematics']"
2916062,A double sum or a definite integral.,"I am trying to evaluate the following double sum
\begin{eqnarray*}
\sum_{n=1}^{\infty} \sum_{m=1}^{\infty}   \frac{(-1)^{n+m}}{n(3n+m)}.
\end{eqnarray*}
Using the integral trick
\begin{eqnarray*}
\frac{1}{3n+m} =\int_0^1 y^{3n+m-1} dy,
\end{eqnarray*}
the sum can be transformed into integral
\begin{eqnarray*}
\int_0^1 \frac{ \ln(1+y^3)}{1+y} dy.
\end{eqnarray*}
Now ""half"" of this is easy (IBP & rearrange)
\begin{eqnarray*}
\int_0^1 \frac{ \ln(1+y)}{1+y} dy = \frac{1}{2} (\ln 2)^2.
\end{eqnarray*}
So we are left with
\begin{eqnarray*}
\int_0^1 \frac{ \ln(1-y+y^2)}{1+y} dy = \int_0^1 \frac{ (1-2y)\ln(1+y)}{1-y+y^2} dy.
\end{eqnarray*}
Now, apart from the obvious IBP done above, this integral has me stumped. An exact evaluation would be nice, barring that, an expression in terms of dilogrithmic values or something similar would be helpful. Any comments or answers, gratefully received. If this integral has been seen before a reference & advise on how I would search & find something similar in the future would be gratefully appreciated.","['integration', 'summation', 'definite-integrals', 'sequences-and-series']"
2916087,Construct a perpendicular in confined space?,"About 45 years ago, I learned in school how to construct perpendicular lines with straight-edge and compasses, and I remember being taught two techniques (with variations on one). I recall how to perform one, but the other I forgot. The one that I can still do is applicable if the paper/workspace is large enough that you can swing compasses around at will. But the second technique was specifically applicable for where the perpendicular was near to the edge of the paper. At the time, I evidently paid insufficient attention, but now I find myself wanting to use this in a carpentry setting. Quite literally, I have a straight edge, a set of scribing dividers and a scribing tool, and I need to make four holes that are on the corners of a square, about four inches apart, but within about 1/2"" ~ 1"" of the edge of my wood. I searched, and came up with dozens of videos illustrating the ""normal"" (hah, no pun originally intended, but I'll take it!) approach, but nothing on the confined space method. Obviously, this is a straight-edge and compasses problem. I'm not interested in using a set-square in this case (though that might be a workaround if I really can't find the real solution). Does anyone know how this is done?","['geometry', 'geometric-construction']"
2916107,Computably enumerable set is finite iff every computably enumerable subset is computable,"I want to show that the property of a set being finite is lattice theoretic, i.e. definable in the first order language of the set $\mathcal{E}$ of computably enumerable (c.e) sets with set inclusion. What I know: $\text{Comp}$ is lattice theoretic, where $\text{Comp}(X)$ holds if and only if $X$ is computable because
$\text{Comp}(X) \Leftrightarrow (\exists Y)[X \cup Y = \omega \, \land \, X \cap Y = \emptyset]$ and $\emptyset, \omega, \cup, \cap$ are definable from $\subseteq$ in $\mathcal{E}$. Now in https://pdfs.semanticscholar.org/70d9/83339041d96257d3a7cdcd5393418c64ed44.pdf on page 2 it says Being finite is definable in $\mathcal{E}$. A set X is finite iff every subset of
  X is computable. And I want to prove, that this is actually true. It is quite easy to see, that if $X$ is finite, every subset of $X$ is computable since every such subset is finite aswell. But how do I prove the converse? I want to prove, that: If every c.e. subset $B$ of a c.e. set $X$ is computable then $X$ is
  finite. Or stated in other words: If $X$ is c.e. and $|X| = \infty$, then there exists some c.e. $B$
  with $B \subseteq X$ and $B$ is uncomputable. Could you help me with this? Thank you in advance.","['elementary-set-theory', 'logic', 'computability']"
2916117,Probability of Winning Election if Outcomes not Equally Likely,"I just started learning probability, so my level is not very high.
I am doing a homework problem, and my answer is different than the book's. I can't understand why. I see how the answer in the book makes sense, but I also see how my procedure makes sense. Could someone please point at what I am doing wrong? Problem: Four candidates, A, B, C, and D, are running in an election. A is twice as likely to be elected as B. B and C are equally likely. C is twice as likely as D. What is the probability that C will win? My answer: The sample space is $\{A,B,C,D\}$. Since all the events are mutually exclusive, $S = A\cup B\cup C\cup D$. So, $P(S) = P(A)+P(B)+P(C)+P(D)$. Since $P(S) = 1$, $P(A)+P(B)+P(C)+P(D) = 1$. Since A is twice as likely as B, $P(A) =2\times P(B)$. Since B and C are equally likely, $P(B)=P(C)$. Thus, $P(A) = 2\times P(C)$. Since C is twice as likely as D, $P(C) = 2\times P(D)$. Therefore, the probability of the sample space in terms of C is: $1 = 6P(C)$. So, $P(C) = \frac{1}{6}$. The book's answer: Let $p = P(D)$. Since C is twice as likely as D, $P(C) = 2p$. Since B and C are equally likely, $P(B) = 2x$. Since A is twice as likely as B, $P(A)= 4p$. Since $P(S) = 1$, $p + 2p + 2p + 4p = 1$. So, $p = \frac{1}{9}$ and $P(C) = \frac{2}{9}$. Thanks!",['probability']
2916137,How to change the base cases of a tribonacci sequence solved by matrix exponentation,"So the base case of a Fibonacci sequence is the following matrix:
\begin{bmatrix}1&1\\1&0\end{bmatrix} 
Which I understood as being:
\begin{bmatrix}F_2&F_1\\F_1&F_0\end{bmatrix} 
And that seems logical. But I don't really understand what each of the numbers in a tribonacci base case mean:
\begin{bmatrix}1&1&1\\1&0&0\\0&1&0\end{bmatrix} I don't really understand what each of these numbers represent so if someone could help me, I would appreciate it. Also, I need to compute a tribonacci sequence where the base cases are not t[0]=0, t[1]=0 and t[2]=1 (like in the above matrix), but t[0]=1, t[1]=1 and t[2]=2 So how exactly could I implement that into the matrix that represents the base case? Do I just replace it like the following:
\begin{bmatrix}2&2&2\\2&1&1\\1&2&1\end{bmatrix}
But what if I would want to replace them with distinct numbers, say for example t[0]=3, t[1]=5 and t[2]=8 ? Where would each of these numbers be placed in the matrix?","['matrices', 'fibonacci-numbers']"
2916150,Rank comparison for different low rank approximations,"I want to approximate a correlation matrix by low-rank components such that $$\Sigma \approx \sum_{i=1}^{k_1} \sigma_ib_ib_i^T$$ where $\Sigma$ is of size $n \times n$ , $b$ is a $n$ dimensional vector and $k$ is the number of components. I was wondering if subtracting a diagonal matrix $D \in R^{n\times n}$ from the matrix would give a better low rank approximation, below is the decomposition $$\Sigma \approx D + \sum_{i=1}^{k_2} \sigma_{i}^{'} b_{i}^{'}b_i^{'T}$$ Can we estimate $D, \sigma, b$ such that $k_2 \leq k_1$ holds true? Both the above approximations have different $\sigma$ and $b$ . Also, I know that the $\Sigma$ matrix has non-zero diagonal entries and the matrix after removing the diagonal term is sparse. Edit 1: I was thinking of simulating correlation matrix and then test the above hypothesis. Is there is a way to simulate a low rank, sparse and full diagonal matrix? I have asked this question here https://stats.stackexchange.com/questions/368868/simulation-of-low-rank-and-sparse-matrix","['matrices', 'linear-algebra', 'numerical-linear-algebra', 'numerical-methods', 'matrix-decomposition']"
2916155,What is the outer automorphism group?,"I try to understand what the notion of outer automorphism group is. I know an inner automorphism of a group G is a map  $ \iota_{g} $ attached to some element  $ g $ of  $ G $ that sends an element  $ h $  to  $ ghg^{-1} $. Two elements a  $ a $ and  $ b $ are said to be conjugate if there is some  $ g $ in G such that  $ b=gag^{-1} $. So is an outer automorphism of  $ G $ an automorphism of  $ G $ that sends an element  $ a $ to an element  $ b $ that is not conjugate to  $ a $? Or is it something else ? Incidentally, is it true that the outer automorphism group of the absolute Galois group of the rationals consists of the identity and the complex conjugation ?","['group-theory', 'abstract-algebra']"
2916158,Why Large deviation theory?,"I am trying to understand why we need Large deviation theory/principle. Here is what I understand so far based on the Wikipedia .
 Let $S_n$ be a random variable which depends on $n$.
We are interested in How the probability $P(S_n > x)$ changes as $n \to \infty$. Very often, LDT concerns with how fast $P(S_n >x)$ converges to $0$.
Then the answer to the above question is already known, it is $0$.
Then one might be interested in how fast it converges.
So I understand the notion of the ration function $I(x)$. Here I what I don't understand: In order to know $P(S_n > x)$, it is a matter of finding the cumulative distribution function of $S_n$, as
$$
P(S_n > x) = 1 - P(S_n < x) = 1 - F_{S_n}(x).
$$
Then once we know the pdf of $S_n$, say $f_{S_n}(x)$, one can easily obtain
$$
P(S_n > x) = \int_x^{\infty} f_{S_n}(t)dt.
$$
If this is the case, I am not sure why we need LDP. Okay, it is not always the case where we know the pdf of $S_n$.
If it is the case, I think the problem is then to estimate $P(S_n > x)$.
However, it seems that many LDP approaches require to compute certain generating functions. For example,
$$
\lambda(k) = \lim_{n\to \infty} \frac{1}{n} \ln E[e^{nkS_n}],
\qquad
I(s) = \sup_{k \in \mathbb{R}} \{ks - \lambda(k)\}.
$$
It seems that calculating the aboves are definitely a lot harder than a direct calculation of $P(S_n > x)$. Thus I don't understand why people introduces quantities which a lot harder to obtain. ""Why the rate?"" Ok, regardless of #1 and #2, let say we have $I(x)$. What now? 
Given $I(x)$, can we estimate $P(S_n > x)$? or can we do something useful? 
Or Is the LDP devoted to derive the rate function $I(x)$? Any comments or answers will very be appreciated.","['large-deviation-theory', 'probability-distributions', 'probability-theory', 'reference-request']"
2916161,Equilibrium solutions and stability,"I want to find the equilibrium solutions and determine their stability. $(1)\left\{\begin{matrix}
\dot{x}=-y-x(1-\sqrt{x^2+y^2})^2\\ 
\dot{y}=x-y(1-\sqrt{x^2+y^2})^2
\end{matrix}\right.$ I also want to check the behavior of the solutions of $(1)$ when $t \to \infty$. There are five possible answers. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t)=x_0$ and $\lim y(t)=y_0$ where $(x_0,y_0)$ equilibrium solution. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ is the equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ with $x^2(0)+y^2(0)>1$ we have that $x^2(t)+y^2(t) \geq 1$. $\exists$ exactly one equilibrium solution and it is unstable. $\forall$ other solution $(x,y)$ of $(1)$ we have that $\lim (x^2(t)+y^2(t))=1$. I have thought the following so far. In order to find the equilibrium solutions, we set $\dot{x}=0$ and $\dot{y}=0$. $\dot{x}=0 \Rightarrow -y-x (1-\sqrt{x^2+y^2})^2=0 (\star)$ and $\dot{y}=0 \Rightarrow x-y (1-\sqrt{x^2+y^2})^2=0 \Rightarrow x=y(1-\sqrt{x^2+y^2})^2$ $(\star): -y-y(1-\sqrt{x^2+y^2})^4=0 \Rightarrow y=0 \text{ or } 1+(1-\sqrt{x^2+y^2})^4=0, \text{ which is rejected}$. So $y=0$ and $x=0$. So $(0,0)$ is the only equilibrium solution. If we set $f_1(x,y)=-y-x(1-\sqrt{x^2+y^2})^2$ and $f_2(x)=x-y(1-\sqrt{x^2+y^2})^2$, then we have $\frac{\partial{f}}{\partial{x}}=-(1-\sqrt{x^2+y^2})^2+\frac{2x^2(1-\sqrt{x^2+y^2})}{\sqrt{x^2+y^2}}$. Then we have $$\frac{\partial{f_1}}{\partial{x}}(0,0)=-1 \\ \frac{\partial{f_1}}{\partial{y}}(0,0)=-1  \\ \frac{\partial{f_2}}{\partial{x}}(0,0)=1 \\ \frac{\partial{f_2}}{\partial{y}}(0,0)=-1$$ and thus the Jacobi matrix at $(0,0)$ gets the following form: $J=\begin{pmatrix}
-1 & -1\\ 
1 & -1
\end{pmatrix}$, right? Then we get that the eigenvalues are these ones: $-1 \pm i$ and so $Re(\lambda_1)=Re(\lambda_2)=-1<0$ which implies that the equilibrium is asymptotically unstable. So we have that there is exactly one equilibrium solution and it is asymptotically unstable, right? When $(x,t)$ is any other solution of the problem, what can we say about the limits $\lim x(t)$ and $\lim y(t)$ ?",['ordinary-differential-equations']
2916181,"Is a subalgebra of $\mathbb{C}[x_1,\ldots,x_n]$ of Krull dimension one, isomorphic to $\mathbb{C}[t]$?","Let $R$ be a $k$-subalgebra of $\mathbb{C}[x_1,\ldots,x_n]$, $n \geq 1$. Assume that $R$ is of Krull dimension $1$.
  Is $R$ isomorphic to a polynomial ring in one variable? More generally, Assume that $R$ is of Krull dimension $m \leq n$.
  Is $R$ isomorphic to a polynomial ring in $m$ variables? A known result says that $\mathbb{C}[x_1,\ldots,x_n]$, $n\geq 1$, is of Krull dimension $n$; I am asking about 'the converse' result (in a given polynomial ring). Remark: What about $n=1$, $R=\mathbb{C}[x^2,x^3] \subset \mathbb{C}[x]$? Is it a counterexample to my first question? (probably yes); I guess that the Krull dimension of $R$ should be $1$,
as a subalgebra of $\mathbb{C}[x]$ which is of Krull dimension $1$.
It is not a polynomial ring in any number of variables, since it is not a UFD (indeed, $x^2x^2x^2=x^3x^3$). So I ask: Is there a nice condition which guarantees a positive answer to my questions? Any hints and comments are welcome!","['ring-theory', 'krull-dimension', 'algebraic-geometry', 'commutative-algebra']"
2916187,"why are ""least square estimators"" in fact ""estimators""?","I'm trying to reconcile the notion of a ""least square estimator"" with the definition of an estimator I have in my head. An estimator for me assumes a set $\Omega$ and a family of probability measures $$\{\mu_\theta: \theta \in \Theta\}$$ on $\Omega$, and a function on the index set $g:\Theta \rightarrow \mathbb{R}$. An unbiased estimator for $g$ is then a function $\tilde{g}: \Omega \rightarrow \mathbb{R}$ such that $$\int_\Omega \tilde{g} \, d\mu_\theta = g(\theta)$$ for all $\theta\in\Theta$. A ""least squares estimator"" on the other hand assumes we have some uncorrelated random variables $X_1,\ldots,X_n: \mathscr{X} \rightarrow \mathbb{R}$ on a space $\mathscr{X}$ (I think with a fixed probability measure?) with common variance and expectations $\theta_1,\ldots,\theta_n$ assumed to lie in some linear subspace $V\subset \mathbb{R}^n$ (as something varies? what specifically i don't know.  Maybe varies with a family of probability measures on $\mathscr{X}$?).  Then given an ""observation"" $x=(x_1,\ldots,x_n)\in \mathbb{R}^n$ we ""estimate""  $(\theta_1,\ldots,\theta_n)$ by just projecting $x$ to the subspace $V$ obtaining say $x'$. Then given any linear $L: \mathbb{R}^n\rightarrow \mathbb{R}$, $L(x')$ is supposed to be an estimator for $L(X_1,...,X_n)$.
(I hope that's a fair treatment. I've tried to simplify it by not including the matrix $A$ which parametrizes subspace $V$) Now my question is how should I define $\Omega$ and $\Theta$ and $\mu_\theta$ and $g$ and $\tilde{g}$ to show that the latter is an instance of the former?  I'd be inclined to just make $\Omega = \mathbb{R}^n$ and $\mu_\theta$ some kind of product measure if the $X_1,\ldots,X_n$ were independent, but they are only assumed uncorrelated.","['statistics', 'probability-theory', 'probability']"
2916197,Sweeping a G-orbit by a different group H,"Suppose H and K are algebraic subgroups of a linear algebraic group G. Suppose that G acts on a smooth algebraic variety X. Given $x\in X$, we can consider the map
\begin{equation}
H\times K \rightarrow X, \quad (h,k)\mapsto hkx.
\end{equation}
If HK happens to be a group, then the image is locally closed (it's an orbit). But what if HK is not a group? Is there an example of an H, K, X where the image is only constructible?","['algebraic-geometry', 'algebraic-groups']"
2916219,About finding the inverse of a matrix,"I am solving a linear algebra problem and this matrix came up from a system of linear equations. $A = \begin{pmatrix}
1 & 2 & \cdots & n \\
1 & 2^2 & \cdots & n^2 \\
\vdots &  & &\vdots &\\
1 & 2^n & \cdots & n^n
\end{pmatrix}
$ I do not know how to check if my system has an unique solution or not (or the matrix is invertible) in this case.","['matrices', 'systems-of-equations', 'linear-algebra']"
2916240,$a^n+ b^n$ is not a prime number when $n$ is not a power of $2$.,"Suppose $a, b$ and $n$ are positive integers, $a+b>2$, and $n$ is not a power of $2$. Prove that $a^n+ b^n$ is not a prime number.","['number-theory', 'divisibility', 'analysis', 'prime-numbers']"
2916246,Second order non-linear ode - am I on the right path?,"I've found this to be difficult to solve: $$ \frac{d^2 x }{dt^2} + (a x + b) \frac{dx}{dt} = 0 $$ I've done some reading, and I guess I could write this as: $$ \frac{d^2 x }{dt^2} +  b \frac{dx}{dt} + ax \frac{dx}{dt} = 0 $$ If I then treat $v(x) = \frac{dx}{dt}$ as an independent variable, I would get: $$ \frac{dv}{dt} + bv +  axv = 0 $$ This is sort of like a nonhomogenous equation. If I take the homogenous solution, I would get: $$ v(t) = A e^{-bt}$$ I think.... I'm not sure where to go from here though.",['ordinary-differential-equations']
2916278,If the exponential of matrices equals the identity. Could they fail to commute?,"I am reading Vladimir Arnold's book on ordinary differential equations, and there is a problem on the Sample Examination Problems' section that is giving me a lot of troubles. The question is: Can the operators $A$ and $B$ fail to commute if $e^{A}=e^{B}=e^{A+B}= E$ , where $E$ is the identity. So far I can deduce that the eigenvalues of $A$ and $B$ most be imaginary numbers of the form $i 2\pi k$, where $k$ in an integer. Because if we write $A$ in its canonical form $A=Q^{-1}\Delta Q$, where $\Delta$ is the canonical form of $A$, using the properties of exponentials $e^{A}=e^{Q^{-1}\Delta Q}= Q^{-1} e^{\Delta} Q=E$. Then $e^{\Delta}=Q (Q^{-1} e^{\Delta} Q)Q^{-1}=Q (E) Q^{-1}=E$. A similar conclusion holds for $B$ and $A+B$. Since $\Delta$ is the canonical form $e^{t\Delta}$ it has polynomials $\frac{t^{m}}{m!}$ outside the diagonal if there is a Jordan-Block, but by evaluating on $t=1$ we find that they must be zero, so the matix $\Delta$ must be a diagonal one. And using the complex exponential we conclude that the eigenvalues of $A$ are of the form $i 2\pi k$. And that is as far as I go, I have tried some matrices but they all appear to commute. I try using Zassenhaus' formula, but no luck. Thanks.","['matrix-exponential', 'linear-algebra', 'ordinary-differential-equations']"
2916306,"How to take the partial derivative of $f(x,y) = x\ln(x) + y\ln(y), x + y = 1$?","Let $f(x,y) = x\ln(x) + y\ln(y)$ be defined on space $S = \{(x,y) \in \mathbb{R}^2| x> 0, y > 0, x + y = 1\}$. My question is, how do I take the partial derivative for this function, given that the parameters are coupled through $x+y = 1$. A first idea would be to do it ignoring the coupling constraint. For this, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + y\ln(y)}{\partial x} 
 = \ln(x) + x/x = \ln(x) + 1$ If we do not ignore the coupling constraint, and instead substitute $y = 1-x$, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + (1-x)\ln(1-x)}{\partial x}  = \ln(x) + 1 + \dfrac{1}{1-x} - \ln(1-x) - \dfrac{x}{1-x}$ Am I doing this correctly? Why do I get two different expressions of the gradient?","['constraint-programming', 'multivariable-calculus', 'partial-derivative', 'derivatives', 'constraints']"
2916320,"5 cards are chosen from a standard deck. What is the probability that we get all four aces, plus the king of spades?","We have $\binom{52}{5}$ ways of doing this. This will be our denominator. We want to select all 4 aces, there are there are exactly $\binom{4}{4}$ ways of doing this. Now we have selected 4 cards, and we need to select one more. That one card has to be the king of spades. Out of the 48 remaining cards, only one of them is our wanted one. This can be chosen in $\binom{48}{1}$ ways. So is the answer: $$\dfrac{\binom{4}{4}\binom{48}{1}}{\binom{52}{5}}=\dfrac{1}{54145}?$$ Is this correct? If we look at it another way: Then then our probability for the aces and specific king are $(4/52)\times (3/51) \times (2/50) \times (1/49) \times (1/48)$ which is a completely different here. Which is the correct approach?","['discrete-mathematics', 'statistics', 'combinatorics', 'probability']"
2916338,Rolling a $6$-sided dice indefinitely until lower than the previous throw,"You will play a game with a fair 6-sided die. You will throw the die and as long as the result of the throw is greater than or equal to the previous throw, you will continue throwing. If the throw is lower than the previous one, you will stop and get as many points as the sum of all throws, including the last one. For example, if you get 2, 5, 5, and 3 as a result of 4 throws, the game will end with 15 points. What is the expected value of the points you will get at the end of the game?","['contest-math', 'probability', 'sequences-and-series']"
2916341,There's a matrix cookbook. Is there a set cookbook?,"There's a matrix cookbook . Is there a set cookbook? I'm looking for a comprehensive list of properties, formulas, inclusions or lack thereof on the algebra of sets , De Morgan's laws , Cartesian products , infinite (countable or uncountable) intersections or unions, etc. For example, in topology, real analysis, probability theory or measure theory, I see a lot of expressions like $$\bigcup_{k=1}^{\infty} (A_k \cap B_k)$$ $$ (A \cup B) \times (C \cup D)$$ $$ \bigcap_{x \in X} (A_x \times B_x)^c$$ Sometimes I wonder if we can do things like put/remove a $\cap B$ or $\cup A$ to both sides of a set inclusion. If it's a set equality, I think there are cases where it's a no longer equality but merely an inclusion or sometimes not an inclusion at all. Expressions similar to the ones aforementioned don't always equal to something, they're usually just subsets or supersets of something else, and sometimes there's no (or no relevant) set inclusion. These are not difficult to prove; I'd just like to have a reference much like the matrix cookbook please. I'm interested mainly in the basics and not necessarily about open/closed, non-/measurable, counterexamples, etc. If a set inclusion is true and the reverse doesn't hold, I'll take the cookbook's word for it. I think this can be found in some appendix of some textbook. For example, a Calculus textbook appendix might have a list of trigonometric identities, without proofs, counterexamples or exercises. That's what I'm looking for. Wiki is helpful but doesn't have a lot of the properties. For example, the Cartesian products page doesn't have the second expression above (other than explaining what the second expression is not) and whether or not an empty Cartesian product implies one of the Cartesian factors is empty, but I was able to find both and more on stackexchange. There are books on these, but a lot of the properties are in exercises, some of which are determining true/false rather than dis/proving, so I'd have to try them myself or look up the solutions manual. I wish there was some compilation of all these folklores .","['elementary-set-theory', 'reference-request']"
2916348,Solving/Proving Recurrence Relations,"Given $a_{k-1}=7*5^{k-1}-3*8^{k-1}$ and $a_k=7*5^k-3*8^k$ for the recurrence $a_n=13a_{n-1}-40a_{n-2}$
prove that the next term is $a_{k+1}=7*5^{k+1}-3*8^{k+1}$ I am kind of stuck right now. This is what I have so far. If anyone could give me advice on where I should go from here. Thanks $a_{k+1}=13(7*5^k-3*8^k)-40(7*5^{k-1}-3*8^{k-1})$ $=13*7*5^k-13*3*8^k-40*7*5^{k-1}+40*3*8^{k-1}$ $=7(13*5^k-40*5^{k-1})+3(-13*8^k+40*8^{k-1})$ $=7(13*5-40)*5^{k-1}+3(-13*8+40)*8^{k-1}$","['recurrence-relations', 'discrete-mathematics']"
2916365,Two players pick cards from standard 52 card deck without replacement...,"I am struggling with this interview prep question... SOS Two players pick cards from standard 52 card deck without replacement: 1st player
picks a card, then 2nd, then again 1st, then 2nd etc. They stop once somebody picks a king (of any suit),
the player who picks the king wins. What is the probability that the first player wins? the second player? Which player has more chances of winning: the first or the second? It is sufficient to set up the correct
formula for the probabilities, no need to evaluate it numerically. The last question can be answered without
numerical evaluation, by analysis of the formulas What is the probability that the first player wins? the second player? couldn't be far of from 50% for both... right? Which player has more chances of winning: the first or the second? Intuition tells me the first person but I'm not sure if that follows or how to setup a formula -- EDIT: for this I'm thinking certainly the first player because they have one more opportunity to win by choosing a king first",['probability']
2916389,What makes proving the Riemann Hypothesis so difficult?,"I am a Biology major (please don't shame me!) but I really enjoy mathematics. Recently I have been reading about this conjecture and its importance in understanding the distribution of primes. After being studied for so long, how come all attempts have failed? Why is this such a difficult thing to study and work on? Would finding a closed form of $$\sum_{n=1}^{\infty} \frac {1}{n^s}$$ help the issue? Have any attempts been made at this at all? Thank you very much for your time educating me and your help!!","['complex-analysis', 'riemann-zeta', 'complex-numbers', 'sequences-and-series']"
2916416,Bubbling off in Deligne-Mumford compactification,"I'm trying to understand an argument in Casim Abbas' 'An Introduction to Compactness Results in Symplectic Field Theory'. Here's where I stuck: (It's on chapter 3.3.2, Adding additional marked points in the book, if you have access to it.) Suppose we have a marked Riemann surface $(S,M)$ , where $S$ is the Riemann surface and $M\subseteq S$ is the set of marked points. (For simplicity, assume there is no nodal points on $S$ .) Then we are interested in the limit of the sequence of marked Riemann surface $S_n:=(S,M\cup \left\{y_n^{(0)},y_n^{(1)}\right\})$ where the added marked points are given so that $d_n(y^{(0)}_n,y^{(1)}_n)$ goes to zero as $n$ goes to infinity. We further assume that there exists a holomorphic chart of radius $R_n\to \infty$ which maps $y_n^{(i)}$ to $i\in \mathbb{C}$ . Now the argument in the book goes like this: First, as the two marked points gets closer and closer, we may restrict ourselves on a pair of pants. (In fact, we need to consider the case that the two marked points are lying on two adjacent pairs of pants, but let's just concentrate on this for simplicity.) So the pair of pants of our concern looks like the first pants as above. Now he considers a pair of pants decomposition of $S_n$ . The following is what was stated: Because the surface $S_n$ contain annuli $\cong D_{R_n}\setminus D_2$ with larger and larger moduli and $y_n^{(i)}\in D_2$ , the last pair of pants decomposition cannot occur by Bers' theorem. In fact, we must have the length of $\gamma_n$ to go to $0$ . Q. Could you explain this part a bit more? I guess that, as we need to use Bers' theorem, it is implicit that the lower dotted circle in the last pair of pants in the figure gets longer and longer as $n$ goes to infinity. Is it correct? Why? And I have no idea how to see the next claim that the length of $\gamma_n$ goes to 0. Edit: The following is Bers' theorem, paraphrased in my own preference. (Bers' theorem) Given a topological type (ie, the genus, number of marked points, and the number of boundary components) of a Riemann surface $S$ fixed, there is a uniform bound on the length of each boundary component of each pair of pants in a pair of pants decomposition of $S$ which only depends on the topological type of $S$ and the length of the boundary components of $S$ .","['complex-analysis', 'teichmueller-theory', 'symplectic-geometry']"
2916462,"Prove that $f(x)=\frac{ax^2+x-2}{a+x-2x^2}$ has the range $ℝ$ when $x\inℝ$ if $a \in [1,3]$","Prove that $f(x)=\frac{ax^2+x-2}{a+x-2x^2}$ has the range $ℝ$ when $x\inℝ$ if $a \in [1,3]$ My working: Let  $f(x)=\frac{ax^2+x-2}{a+x-2x^2}=y$ therefore,
$$(a+2y)x^2+(1-y)x-(2+ay)=0$$
Now, for $x$ to have real values, the discriminant of the above equation must not be less than zero. So,
$$(1+8a)y^2+(14+4a^2)y+(1+8a)\ge 0$$ Now, somehow from this, we have to land on the condition for $a$ which I am unable to do.","['algebra-precalculus', 'functions', 'quadratics']"
2916466,"If $X$ is infinite, then $X$ and $X\times X$ are equinumerous","If $X$ is infinite, then $X$ and $X\times X$ are equinumerous. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help! My attempt: We denote $A$ and $B$ are equinumerous by $A\sim B$ . Let $Y=\{(A,f) \mid A \in\mathcal{P}(X) \text{ and } f:A \to A\times A\text{ is bijective }\}$ . We define a partial order $<$ on $Y$ by $$(A_1,f_1) < (A_2,f_2) \iff A_1 \subseteq A_2 \text{ and } f_1\subseteq f_2$$ Since $X$ is infinite, there exists $A\subseteq X$ such that $A \sim \Bbb N$ (Here we assume Axiom of Countable Choice). Thus $A\sim \Bbb N\sim \Bbb N\times \Bbb N\sim A\times A$ . Hence there is a bijection $f:A\to A\times A$ , and consequently $f\in Y$ and $Y\neq\emptyset$ . For any chain $Z$ in $Y$ , let $F=\{f \mid \exists A \in\mathcal{P}(X),(A,f)\in Z\}$ and $f^\ast=\bigcup\limits_{f\in F}f$ . $f^\ast$ is a mapping For $(a,x_1),(a,x_2)\in f^\ast$ , there are $(A_1,f_1),(A_2,f_2) \in Z$ such that $(a,x_1)\in f_1$ and $(a,x_2)\in f_2$ . Since $Z$ is a chain, we can safely assume $(A_1,f_1) < (A_2,f_2)$ . It follows that $f_1 \subseteq f_2$ . Thus $(a,x_1),(a,x_2)\in f_2$ and consequently $x_1=x_2$ by the fact that $f_2$ is a mapping. Let $A^\ast=\operatorname{dom}f^\ast$ , then $A^\ast=\bigcup\limits_{f\in F} \operatorname{dom}f$ . $f^\ast$ is surjective For all $f\in F$ , $f$ is a bijection from $\operatorname{dom}f$ to $\operatorname{dom}f\times\operatorname{dom}f$ , and thus $\operatorname{ran}f = \operatorname{dom}f\times\operatorname{dom}f$ . $\operatorname{ran}f^\ast=\bigcup\limits_{f\in F} \operatorname{ran}f =\bigcup\limits_{f\in F} (\operatorname{dom}f \times \operatorname{dom}f) \subseteq (\bigcup\limits_{f\in F} \operatorname{dom}f) \times(\bigcup\limits_{f\in F} \operatorname{dom}f) =$ $\operatorname{dom}f^\ast \times \operatorname{dom}f^\ast$ $= A^\ast \times A^\ast$ . Thus $\operatorname{ran}f^\ast\subseteq A^\ast\times A^\ast$ . For any $(a_1,a_2)\in A^\ast\times A^\ast$ , since $A^\ast=\bigcup\limits_{f\in F} \operatorname{dom}f$ , there exist $(A_1,f_1),(A_2,f_2)\in Z$ such that $a_1\in A_1$ and $a_2\in A_2$ . Since $Z$ is a chain, we can safely assume that $(A_1,f_1)<(A_2,f_2)$ , then $A_1 \subseteq A_2$ , then $a_1,a_2\in A_2$ , then $(a_1,a_2)\in A_2\times A_2$ . Since $f_2$ is bijective, there exists $a\in A_2\subseteq A^\ast$ such that $f(a)=f^\ast(a)=(a_1,a_2)\in A_2\times A_2\subseteq A^\ast\times A^\ast$ . Hence there exists $a\in A^\ast$ such that $f^\ast(a)=(a_1,a_2)$ for any $(a_1,a_2)\in A^\ast\times A^\ast$ . Thus $f^\ast$ is surjective. $f^\ast$ is injective Assume $(a_1,x),(a_2,x)\in f^\ast$ . Then there exists $(A_1,f_1),(A_2,f_2) \in Z$ such that $(a_1,x) \in f_1$ and $(a_2,x) \in f_2$ . Since $Z$ is a chain, we can safely assume $(A_1,f_1) < (A_2,f_2)$ . It follows that $f_1 \subseteq f_2$ . Thus $f_1(a_1)=f_2(a_1)$ and consequently $f_2(a_2)=x=f_1(a_1)=f_2(a_1)$ . Hence $f_2(a_2)=f_2(a_1)$ and consequently $a_2=a_1$ by the fact that $f_2$ is injective. It follows that $f^\ast$ is injective. To sum up, $f^\ast:A^\ast\to A^\ast\times A^\ast$ is bijective and hence $A^\ast\sim A^\ast\times A^\ast$ and $f^\ast \in Y$ . Furthermore, $f^\ast$ is an upper bound of $Z$ by definition. Thus $Y$ satisfies the requirement of Zorn's Lemma and hence has a maximal element $(\bar{A},\bar{f})$ . $X\sim X\times X$ First, we prove $X\sim \bar{A}$ . If not, then $X\setminus \bar{A}$ is infinite and consequently there exists $C\subseteq X\setminus \bar{A}$ such that $C\sim \Bbb N\sim \Bbb N \times\Bbb N \sim C\times C$ . Thus there is a bijection $\underline f:C \to C\times C$ . Hence $g=\bar{f} \cup \underline f$ is a bijection from $\bar{A}\cup C$ to $(\bar{A}\times\bar{A})\cup (C\times C)$ . Hence $\bar{A}\cup C \sim (\bar{A}\times\bar{A})\cup (C\times C)$ . Since $C\sim \Bbb N$ , $(\bar{A}\times\bar{A})\cup (C\times C) \sim \bar{A}\times\bar{A} \sim (\bar{A}\cup C)\times(\bar{A}\cup C)$ by Lemma . It follows that $\bar{A}\cup C\sim (\bar{A}\cup C)\times(\bar{A}\cup C)$ and consequently there exists a bijection $G:\bar{A}\cup C\to (\bar{A}\cup C)\times(\bar{A}\cup C)$ . Thus $(\bar{A}\cup C,G)\in Y$ and $\bar{f}\subsetneq G$ . This contradicts the maximality of $(\bar{A},\bar{f})$ . Hence $X\sim\bar{A}\sim \bar{A}\times \bar{A}\sim X\times X$ . This completes the proof. Update: I fixed Part 4 as follows: We denote: $X\sim Y \iff$ $X$ and $Y$ are equinumerous. $|X|+|Y|:=|A\cup B|$ for $X\sim A$ , $Y\sim B$ , and $A\cap B=\emptyset$ . $|X|.|Y|:=|X\times Y|$ . Lemma 1: If $\aleph_0\le |Z|$ , $|X|+|Y|=|Z|$ , and $|X|<|Z|$ . Then $|Y|=|Z|$ . (I presented a proof here ) Lemma 2: If $\aleph_0\le |X|=|Y|$ . Then $|X|+|Y|=|X|$ . (I presented a proof here ) $X\sim X\times X$ We first prove $\bar{A}\sim X$ . Assume the contrary that $|\bar{A}|\neq |X|$ , then $|\bar{A}|< |X|$ . We have $\bar{A} \cup (X\setminus \bar{A})=X$ , $\bar{A} \cap (X\setminus \bar{A})=\emptyset$ , and $|\bar{A}|<|X|$ . Then $|X\setminus \bar{A}|=|X|$ by Lemma 1 . It follows that there exists $C\subseteq X\setminus \bar{A}$ such that $|C|=|\bar{A}|<|X\setminus \bar{A}|=|X|$ . Consider $$D=(C\times \bar{A})\cup(\bar{A}\times C)\cup(C\times C)$$ We have the following claims: a. $(\bar{A}\times \bar{A})\cup D=(\bar{A}\times \bar{A})\cup(C\times \bar{A})\cup(\bar{A}\times C)\cup(C\times C)=(\bar A\cup C)\times(\bar A\cup C)$ by the definition of Cartesian product. b. Since $|C|=|\bar{A}|$ , $|C\times \bar{A}|=|\bar{A}\times C|=|C\times C|=|\bar{A}\times \bar{A}|=|\bar{A}|$ [since $\bar{f}:\bar{A}\to\bar{A}\times \bar{A}$ is bijective]. c. Since $|C\times \bar{A}|=|\bar{A}\times C|=|C\times C|=|\bar{A}\times \bar{A}|=|\bar{A}|\ge\aleph_0$ and they are pairwise-disjoint [Since $\bar A\cap C=\emptyset$ ], $|D|=|\bar{A}|$ by Lemma 2 . d. Since $|C|=|\bar{A}|=|D|$ , $|C|=|D|$ . Thus there exists a bijection $\underline f: C\to D$ . Let $G=\bar f\cup \underline f$ , then $G:\bar A\cup C\to (\bar A\times\bar A)\cup D$ is bijective since $\bar f, \underline f$ are bijective, or equivalently $G:\bar A\cup C\to(\bar A\cup C)\times(\bar A\cup C)$ is bijective. It follows that $\bar f\subsetneq G$ and $(\bar A\cup C,G)\in Y$ . This contradicts the maximality of $(\bar f,\bar A)$ . Hence $X\sim\bar A\sim \bar A\times\bar A\sim A\times X$ . $$\tag*{$\blacksquare$}$$","['elementary-set-theory', 'proof-verification', 'infinity']"
2916496,What is the derivative of Ax where A is a matrix and x is a vector? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I see two different formulae. https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation-c.pdf In this slides page 9, the derivative of $Ax$ is $A$. But in some other documents such as http://faculty.arts.ubc.ca/vmarmer/econ627/vector%20diff.pdf . The derivative is $A^T$. How to properly evaluate the derivative of Ax?","['calculus', 'linear-algebra']"
2916502,"Can we assume that ""any two events"" are within the same sample space?","I had a question regarding the properties of probability. If we were to say that we have ""any two events $A$ and $B$,"" is it okay to assume that they both belong to the same sample space $S$ (i.e. $A \subseteq S$ and $B \subseteq S$)? I'll give a specific example exercise that prompted me to ask this question: $\\$ ""Show that for any events $A$ and $B$, $\ $ $P(A) + P(B) - 1 \le P(A \cap B)$."" What I did is move the 1 over, so now we have $$P(A) + P(B) \le 1 + P(A \cap B)$$ which is true if we assume that $P(A) + P(B) \le P(S) = 1$. $\\$ I hope my question makes sense. Thank you for the feedback!","['probability-theory', 'probability']"
2916533,"Prove that the set $\left\{ x, Ax, \dots, A^{k-1} x \right\}$ is linearly independent","Problem: Let $A\in M_{n\times n}(\mathbb R)\,$ be a matrix and suppose that a positive number $k\,$ exists such that $A^k = 0\,$ and $A^{k-1} \neq 0$.
Suppose that $x=\left[ \begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right]$ is a vector in $\mathbb{R^n}$ such that $A^{k-1} x \neq 0$. Prove that the $k\,$ vectors $\,x,Ax,\dots,A^{k-1}x\,$  are linearly independent. My attempt: Suppose $x + Ax + \dots + A^{k-1}x = 0$. Multiply both sides with $A^{k-1}$. Then we have $A^{k-1}x + A^k (x + Ax + \dots + A^{k-2}x) = 0 \Leftrightarrow A^{k-1}x = 0 \Leftrightarrow x = 0$ which implies $x + Ax + \dots + A^{k-1}x\,$ is linear independent. This problem looks quite easy but I want my proof to be checked. Is it correct?","['matrices', 'linear-algebra', 'vector-spaces']"
2916540,Solving $f'(t)=f(t+\pi/2)$,"As the title indicates, I want to solve the first order ODE $f'(t)=f(t+\pi/2)$. Here, $f$ is a real function with real variable $t$, and the equation is true for all real $t$. Immediately, I can tell that $f(t)=\sin t$ or $\cos t$ are solutions, since $f'(t)=\frac{d}{dt}\sin t=\cos t = \sin(t+\pi/2)=f(t+\pi/2)$ and similarly for the cosine. After some pondering, I realised that $f(t)=C_1\cos t + C_2\sin t$ is a solution as well for any constants $C_1$ and $C_2$. I'm tempted to think that these are all the solutions, but I cannot say so for sure; I know that these are all the solutions if the principle of superposition applies, but the way I learnt it, the principle only applies to differential equations of the form $p(D)f(t)=g(t)$ where $p$ is a polynomial. Thus, I have two questions. Firstly, is what I have given indeed all of the solutions, and how do you prove that? (Perhaps equivalently, how do you prove or disprove that the principle of superposition holds for any imaginable differential equation, no matter what form it is in?) Secondly, and more importantly, how do you properly solve this question? I got my solutions based on pure guessing and not proper reasoning, but there has to be a method to solve this step-by-step without such a huge leap of faith! I have googled for this exact question, but as most resources out there are dedicated to solving equations where the argument of the functions do not change (i.e. the arguments are always $t$ and not $t+\pi/2$), I couldn't find anything useful.","['periodic-functions', 'calculus', 'ordinary-differential-equations']"
2916551,An $\operatorname{erfi}(x)e^{-x^2}$ integral,"I want to find an elementary evaluation of $$I=\int_0^\infty \left(\frac{\sqrt\pi}2\operatorname{erfi}(x)e^{-x^2}-\frac1{1+2x}\right)dx$$
  where $\operatorname{erfi}(x)=\frac{2}{\sqrt\pi}\int_0^xe^{t^2}dt$. Rough Solution $$I=\int_0^\infty\left({}_1F_1(1;3/2,-x^2)x-\frac1{1+2x}\right)dx$$
$$=\left(\frac{x^2}2{}_2F_2(1,1;3/2,2,-x^2)-\frac12\ln(1+2x)\right)\Bigg|_0^\infty$$
By using the asymptotic expansion of $_2F_2$ I can get the answer is $\frac{\gamma}4$, where $\gamma$ is the Euler's constant. I wonder if there is a elementary proof without using hypergeometric function.","['integration', 'calculus', 'definite-integrals', 'special-functions']"
2916556,"Suppose that $A_1\subseteq A$ and $B_1\subseteq B$, and that $A\sim B$ and $A_1\sim B_1$. Then $A\setminus A_1 \sim B\setminus B_1$.","For $A,B,A_1,B_1$ are infinite sets. Suppose that $A_1\subseteq A$ and $B_1\subseteq B$, and that $A\sim B$ and $A_1\sim B_1$. Then $A\setminus A_1 \sim B\setminus B_1$. My attempt: We denote $A$ and $B$ are equinumerous by $A\sim B$. Since $A\sim B$, there is a bijection $f:A\to B$. Similarly, there is a bijection $h:A_1\to B_1$. From here, I don't know how to proceed to define a bijection from $A\setminus A_1$ to $B\setminus B_1$. I think the proof may require Axiom of Choice, but don't know how. Please shed some lights!",['elementary-set-theory']
2916569,Determinant of Symmetric Matrix $\mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T$,"To give the close-form of $\det(\mathbf{G})$, where $\mathbf{G}$ is 
\begin{align}
\mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T
\end{align}
in which $a$ and $b$ are constant, and $\boldsymbol{e}$ is a column vector with all elements being $1$. In addition, $(\cdot)^T$ is transposition operation. $\mathbf{G}$ is $u\times u$. We use $\mathbf{G}_u$ to underline the dimension of $\mathbf{G}$. The question is to determine $\det(\mathbf{G})$. As I know: We rewrite $\mathbf{G}_u$ as 
\begin{align}
\mathbf{G}_u=\left[\begin{array}{ccc}
\mathbf{G}_{u-1} & b\\
b & a+b
\end{array}
\right]
\end{align}
Using the determinant of  block matrix lemma 
\begin{align}
\det\left[\begin{array}{ccc}
\mathbf{A} & \mathbf{B}\\
\mathbf{C} & \mathbf{D}
\end{array}
\right]=\det(\mathbf{A})\det(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B})
\end{align}
We then have 
\begin{align}
\det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e})
\end{align}
It still needs to get $\mathbf{G}_{u-1}^{-1}$ via matrix inverse lemma
\begin{align}
(\mathbf{A}+\mathbf{BC})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{B}(\mathbf{I}+\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1}
\end{align}
We then have 
\begin{align}
\mathbf{G}_{u}
&=\frac{1}{a}\mathbf{I}-\frac{1}{a^2}b\boldsymbol{e}\left(1+\frac{b}{a}\boldsymbol{e}^T\boldsymbol{e}\right)^{-1}\boldsymbol{e}^T\\
&=\frac{1}{a}\mathbf{I}-\frac{b}{a(a+bu)}\boldsymbol{ee}^T
\end{align}
where $\boldsymbol{e}^T\boldsymbol{e}=u$ is used. Plugging it into 
\begin{align}
&\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e})\\
=&a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right)
\end{align}
Then 
\begin{align}
\det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\left[{a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right)}\right]
\end{align}
Although, the connection between $\mathbf{G}_u$ and $\mathbf{G}_{u-1}$ is found, I can't give the expression of $\mathbf{G}_u$. Please give me hand, thanks a lot!","['matrices', 'determinant']"
2916589,Number of ways to pay (generating functions),"I've just started learning generating functions. Let $a_n$ be the number of ways in which you can pay $n$ dollars using 1 and 2 dollar bills. Find the generating function for $(a_0, a_1, a_2, \ldots)$ and general term $a_n$. Prove that the number of ways to pay $n$ dollars using only 1, 2, and 3 dollar bills is equal to closest integer of $\frac{(n+3)^2}{12}$? What I've got so far: Let $f(x) = \frac{1}{1-x}$ be the generating function for $(1, 1, 1, ...)$. $1 + x^2 + x^4 + \ldots$ is the generating function for $(1, 0, 1, 0, 1, \ldots)$ which is equal to $f(x^2) = \frac{1}{1-x^2} = \frac{1}{(1-x)(1+x)}$. $$
\begin{align}
& (1 + x + x^2 + \ldots)(1 + x^2 + x^4 + \ldots) \\
&= \frac{1}{(1+x)(1-x)^2} \\
&= \frac{1}{4} \cdot \frac{1}{1+x} + \frac{1}{4} \cdot \frac{1}{1-x} + \frac{1}{2} \cdot \frac{1}{(1-x)^2} \\
&= \frac{1}{4} \Sigma_{n \geq 0}{-1 \choose n}x^n + \frac{1}{4} \Sigma_{n \geq 0}x^n + \frac{1}{2} \Sigma_{n \geq 0}{2 + n - 1 \choose n}x^n \\
&= \Sigma_{n \geq 0}\Bigl((-1)^n \frac{1}{4} x^n + \frac{1}{4} x^n + \frac{1}{2}{n + 1 \choose n} x^n \Bigr) \\
&= \Sigma_{n \geq 0}\Bigl((-1)^n \frac{1}{4} x^n + \frac{1}{4} x^n + \frac{1 \cdot 2}{2 \cdot 2}(n + 1) x^n \Bigr) \\
&= \Sigma_{n \geq 0} \frac{1}{4} \Bigl((-1)^n x^n + x^n + 2(n + 1) x^n \Bigr)
\end{align}
$$ I don't know is this is correct nor how to proceed. I guest that we use similar method to solve problem (2)?","['combinatorics', 'discrete-mathematics', 'generating-functions']"
2916681,Partition an infinite set into countable sets,"Let $X$ be an infinite set. Find a partition of $X$ where each element in the partition is countable. Let $Y$ be the set of all families $\{F_i\mid i\in I\}$ in which each family satisfies 3 below conditions: $F_i\subseteq X$ for all $i\in I$ $F_{i_1}\cap F_{i_2}=\emptyset$ for all $i_1,i_2\in I$ and $i_1\neq i_2$. $|F_i|=\aleph_0$ for all $i\in I$ We define a partial order $<$ on $Y$ by $$\{F_i\mid i\in I\}<\{F_i\mid i\in J\}\iff\{F_i\mid i\in I\}\subseteq\{F_i\mid i\in J\}$$ For any chain $Z$ in $Y$, let $T=\bigcup_{F\in Z}F$, then $T\in Y$ and $T$ is an upper bound of chain $Z$. Thus the requirement of Zorn's Lemma is satisfied. Hence $Y$ has a maximal family $\bar{F}$. Let $F'=X\setminus\bigcup_{F\in\bar{F}}F$, then $F'$ is finite. If not, $F'$ is infinite. Then there exists $F^\ast\subseteq F'$ such that $|F^\ast|=\aleph_0$. Thus $\bar{F}\cup\{F^\ast\} \in Y$ and $\bar{F} \subsetneq \bar{F}\cup\{F^\ast\}$, which clearly contradicts the maximality of $\bar{F}$. Hence $F^\ast$ is finite. To sum up $\bar{F}\cup\{F'\}$ is the required partition of $X$ where each element is either finite or countable infinite. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help!","['elementary-set-theory', 'proof-verification', 'infinity']"
2916685,"Function which is Riemann integrable on $ [c,b] $ for $ c \in (a,b) $ but not integrable on $ [a,b] $.","What are some common, preferably uncomplicated functions $ f: [a,b] \rightarrow \mathbb{R} $ that are Riemann integrable on $ [c,b] $ for all $ c \in (a,b) $ but not integrable on $ [a,b] $. I know $ f = 1/x $ is one such function for the interval $ [0,1] $. Are there any other examples?","['integration', 'functions', 'real-analysis']"
2916698,Example of a space that is not a topological manifold.,"Let $A,B$ be two points not in the real line. Let $S=(\Bbb{R} \setminus \{0\}) \cup \{A,B\}$ We define a topology in S as follows: On $(\Bbb{R} \setminus \{0\})$ , use the subspace topology inherited from $\Bbb{R}$ , with open intervals as a basis. A basis
of neighborhoods at $A$ is the set $\{I_A(−c,d) | c,d > 0\}$ and similarly, a basis of neighborhoods at $B$ is the set $\{I_B(−c,d) | c,d > 0\}$ , where $$I_A(−c,d)=(-c,0) \cup \{A\} \cup (0,d)$$ $$I_B(−c,d)=(-c,0) \cup \{B\} \cup (0,d)$$ I have to prove that this space is  locally Euclideian,second countable but not Hausdorff.
Thus the space is not a topological manifold. I have proven that this space is locally euclideian but not Hausdorff. For second countability i'm not sure if my argument is correct. Second countability  is a hereditary property  so $\Bbb{R} \setminus \{0\}$ is second countable as a subspace of the second countable real line. Thus exists a countablle  basis $C=\{B_n|n \in \Bbb{N}\}$ for $\Bbb{R} \setminus \{0\}$ Now we take the sets $$D_A=\{I_A(c,d)|c,d>0 \text{  and   } c,d \in \Bbb{Q}\}$$ $$D_B=\{I_B(c,d)|c,d>0 \text{  and   } c,d \in \Bbb{Q}\}$$ Now we take the countable union $$W=C \cup D_A \cup D_B$$ . Is $W$ a the correct basis to prove the statement for second countability?? Thank you in advance.","['manifolds', 'general-topology', 'differential-geometry', 'real-analysis']"
2916702,Finding the limit by using the definition of derivative.,"Here is the problem. Let $f$ be the function that has the value of $f(1)=1$ and $f'(1)=2$. Find the value of
  $$ L = \lim_{x \to 1} {\frac{\arctan{\sqrt{f(x)}-\arctan{f(x)}}}{ \left (\arcsin{\sqrt{f(x)}}-\arcsin{f(x)}\right)^2}} $$ I have tried using
$$
L=\lim_{x\to 1}
\frac{1}{x-1}\frac{\frac{\arctan{\sqrt{f(x)}}-\arctan{\sqrt{f(1)}}}{x-1}-\frac{\arctan{{f(x)}}-\arctan{{f(1)}}}{x-1}}
{\left [\frac{\arcsin{\sqrt{f(x)}}-\arcsin{\sqrt{f(1)}}}{x-1}-\frac{\arcsin{{f(x)}}-\arcsin{{f(1)}}}{x-1}  \right ]^2}
$$
and reduced that big chunks by using $f'(a)=\lim_{x \to a} \frac{f(x)-f(a)}{x-a}$ which I got
$$
\begin{split}
L&=\lim_{a\to 1}
\frac{1}{a-1}\frac{\Big [ \arctan\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arctan{f(x)} \Big ]'_{x=a}}
{\Big [ \arcsin\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arcsin{f(x)} \Big ]'_{x=a}}\\[2em]
&=\lim_{a\to 1}
\frac{1}{a-1} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{1+(f(a))^2}f'(a)}
{\left [  \frac{1}{\sqrt{1-f(a)}}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{\sqrt{1-(f(a))^2}}f'(a) \right ]^2}\\[2em]
&=\lim_{a\to 1}
\frac{1}{f'(a)\frac{a-1}{1-f(a)}} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}}
{\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}\\[2em]
&=\lim_{a\to 1}{-\frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}}
{\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}}\\[2em]
&=\boxed{(\sqrt{2}+1)^2}
\end{split}
$$
but the answer keys tell me that the answer of this problem is $L=\left( \frac{\sqrt{2}+1}{2}\right)^2$.
So, Can someone please explain to me what did I do wrong?","['limits-without-lhopital', 'limits', 'calculus', 'derivatives']"
2916756,Lebesgue Monotone Conv. Theorem's Proof by W. Rudin,"The proof is the following The proposition 1.25 and Theorem 1.19(d) that refers to are these Why do we need that constant $0 <c <1$ in the proof of Leb. Mon. Conv. Thm?   For me, the proof works fine if we take just any simple function $s $ and define the sets $E_n $ with $s $ instead of $cs$. Thank you! :)",['measure-theory']
2916774,"Does $\sum^{\infty}_{n=1}xe^{-nx}$ converge uniformly on $[0,\infty)$?","I was trying to solve this problem with a friend but we got stuck. TRIAL We claimed that the series does not converge uniformly which implies that it is not uniformly Cauchy, i.e., $\exists\,\epsilon_0>0$ such that $\forall\,k\in \Bbb{N},\,\exists\,n_k,m_k\geq k,\;\exists \,x_k\subset [0,\infty)$ such that 
\begin{align}\left|s_{n_k}(x_k)- s_{m_k}(x_k)\right|\geq \epsilon_0\end{align} Now, let $k\in \Bbb{N}$ be given. Take $n_k=k,\,m_k=k+1,\;x_k=\frac{1}{k+1}\subset [0,\infty)$ and $\epsilon_0 \overset{?}{=}.$ Then,
\begin{align}\left|s_{n_k}(x_k)- s_{m_k}(x_k)\right|&=\left| \sum^{n_k}_{i=1}x_k e^{-i x_k}-\sum^{m_k}_{i=1}x_k e^{-i x_k}\right|\\&=\left| x_k e^{-(k+1) x_k}\right|\\&=\left| \frac{1}{k+1} e^{-(k+1) \frac{1}{k+1}}\right|\\&=\left| \frac{1}{k+1} e^{-1}\right|\end{align}
I'm stuck at this point. Can anyone help out? Thanks!","['analysis', 'real-analysis', 'uniform-convergence', 'sequences-and-series', 'pointwise-convergence']"
2916815,Curvature of a curve projected on its osculator plan,"I have to prove that for any regular curve in $\mathbb{R^3}$ it's curvature $k(t)$ is the same as the curvature of its projection on the osculator plane at $t$ Basically what i did was writting on the canonical local form, and projecting on the First two coordinates, deriving twice and taking the norm, but it lead me nowhere
I get $\sqrt{({ks^2})^2+(k+sk')^2}$ if i dont carry the aproximation errors.","['curves', 'curvature', 'differential-geometry']"
2916819,How to deduce that the rotation matrix is constant from the following?,"As it is clear for everyone the deformation gradient $\mathbf{F}$ can be decomposed into, $\mathbf{F}=\mathbf{Q} \cdot \mathbf{U}$ where $\mathbf{Q}$ is the rotation matrix. Now if I consider $\mathbf{U}=\mathbf{1}$, I can write $\displaystyle Q_{ij}=\frac{\partial y_i}{\partial x_j}$. According to the Schwarz integrability condition we have, $\displaystyle \frac{\partial Q_{ij}}{\partial x_k}=\frac{\partial Q_{ik}}{\partial x_j}$. Is it possible to show that, since $\mathbf{Q}$ is a rotation matrix, the only possibility consistent with the Schwatz integrability condition is $\mathbf{Q}=$constant? The above has been taken from a book. But I don't know how it has deduced $\mathbf{Q}$ is constant.","['optimization', 'schwartz-space', 'derivatives', 'rotations']"
2916826,Showing that given matrix does not have negative eigenvalues without using the knowledge that it is positive definite.,"Let $a,b,c$ be a positive real number such that $b^2+c^2<a<1$. Let 
$A=\begin{bmatrix} 1&b&c\\ b&a & 0\\ c & 0 & 1\end{bmatrix}$. Then Consider the above matrix. I want to comment about the nature of eigenvalues of the matrix in the sense that, they are all positive, all negative, mix of positive or negative, non zero, real or non real etc.. My efforts We look at the matrix first and see if it looks like one of the familiar type introduced in standard Linear Algebra Text. We can see, this matrix is symmetric. As soon we hear the term ""symmetric matrix"" and there is already  the term ""eigenvalue"" in the question. We go the next standard result which says that a real symmetric matrix is diagonalizable with all eigenvalues real. Conclusion so far The given matrix has only real eigenvalues. Another standard result is the sum of eigenvalues is equal to the trace of the matrix. Trace is positive here due to the conditions specified. So not all eigenvalues can be negative. So we are left with two choices all eigenvalues are positive Eigenvalues of A are either positive or negative. I know this matrix is positive definite(I have already proved it, by showing that all sub determinant are positive) so all eigenvalues are positive. My aim is to show that there are no negative eigenvalues without going into the theory of positive definite matrices.",['linear-algebra']
2916829,"If $f : [a,b]\to\Bbb R$ is continuous, are there $x_1,x_2\in (a,b)$ such that $\tfrac{f(b)-f(a)}{b-a} = \tfrac{f(x_1)-f(x_2)}{x_1-x_2}$?","I just thought about the mean value theorem and wondered whether the following statement is true: If $f : [a,b]\to\Bbb R$ is continuous, then there are $x_1,x_2\in (a,b)$
  such that $\tfrac{f(b)-f(a)}{b-a} = \tfrac{f(x_1)-f(x_2)}{x_1-x_2}$. One way to look at it is to consider the function $F : \{(x,y) : x,y\in[a,b],\,y>x\}\to\Bbb R$, defined by $F(x,y) = \tfrac{f(x)-f(y)}{x-y}$. If there don't exist such $x_1,x_2$, then $(a,b)$ is a maximum of a minimum of $F$. But I don't know what to conclude from that. Does anybody have an idea?","['continuity', 'real-analysis']"
2916867,Covariance for stochastic variables,"if $X$ and $Y$ are stochastic variables with $\operatorname{Var}(X)=1.34$ and $\operatorname{Cov}(X,Y) = 0.64$, find $\operatorname{Cov}(2X, 3X+2Y)$. No ideas on this one, as I don't see any way of combining the formulas I know to figure this out. I would greatly appreciate some hints","['statistics', 'covariance', 'variance', 'means', 'stochastic-processes']"
2916887,Shannon entropy of a fair dice,"The formula for Shannon entropy is as follows, $$\text{Entropy}(S) = - \sum_i p_i \log_2 p_i
$$ Thus, a fair six sided dice should have the entropy, $$- \sum_{i=1}^6 \dfrac{1}{6} \log_2 \dfrac{1}{6} = \log_2 (6) = 2.5849...$$ However, the entropy should also correspond to the average number of questions you have to ask in order to know the outcome (as exampled in this guide under the headline Information Theory ). Now, trying to construct a decision tree to describe the average number of questions we have to ask in order to know the outcome of a dice, and this seems to be the optimal one: Looking at the average number of questions in the image, there are 3 questions in 4/6 cases in 2 questions in 2/6 cases. Thus the entropy should be: $$\dfrac{4}{6} \times 3 + \dfrac{2}{6} \times 2 = 2.6666...$$ So, obviously the result for the entropy isn't the same in the two calculations. How come?","['entropy', 'decision-trees', 'probability-theory', 'information-theory']"
2916947,Free vector space over a set,"Given a set $S$ and a field $F$ we can construct the $F$-free vector space over $S$ in the following way. Consider the set of formal sums $$FS:=\left\{\sum_{s\in S} \alpha_s s\,:\, \alpha_s=0\, \text{except for a finite number of}\, s \in S\right\}.$$
The structure of an $F$-vector space is given to $FS$ by using addition and multiplication in $F$, ie $$\sum_{s \in S} \alpha_s s+\sum_{s \in S} \beta_s s  = \sum_{s \in S}(\alpha_s+\beta_s) s,$$$$\alpha\left(\sum_{s \in S} \alpha_s s\right) :=\sum_{s\in S}(\alpha \alpha_s)s.$$ $FS$ is called free vector space over $S$. The element of $FS$ for wich $\alpha_s=1$ and $\alpha_r=0$ if $r\neq s$ is identified with $s$. This identification embeds $S$ in $FS$ and allow us to consider $S$ as a set of generators for $FS$. In fact, by definition, every element of $FS$ can be written as a linear combination of element of $S$. My question is the following: how can I prove that $S$ is a basis? I mean, how can I prove linear independence? I think we have to add  the following condition on $FS$: given $a=\sum_{s \in S} \alpha_s s, b=\sum_{s \in S} \beta_s s$ in $FS$ then $$a=b\,\text{iff}\, \alpha_s=\beta_s \, \text{for all}\, s \in S.$$
(in this way, linear independence is trivial). Is this condition necessary or not to prove linear independence for $S$? Thanks a lot in advance.","['free-modules', 'vector-spaces', 'category-theory', 'abstract-algebra', 'linear-algebra']"
2916967,Graphing Without using Calculus $f(x) = \sqrt{x + 2} - \sqrt{x - 2}$,"I am trying to solve the following problem: I can visualize how it looks like ""approximately"", it's essentially something like $\sqrt{x - 2} - 2$, with the difference that it increases faster. But based on the other parts of this problem it seems a specific shape such as a parabola, hyperbola, circle, or ... should be found to describe the graph of each function. I tried simplifying the function, with no results. Is it even possible to graph this function without taking derivatives? I also proved that the function can be written like this: $y^2(y^2-4x) = -16$ where $y \ge2$.","['algebra-precalculus', 'functions', 'radicals', 'graphing-functions']"
2916972,How to prove that this function is increasing?,"$(a^x-1)/(x)$ is growing?, For $a>1, x>0$
I would like to know some way to prove that this function is increasing, without using derivation techniques, only with the basic calculation, using limits if necessary.","['calculus', 'functions']"
2916984,A conserved quantity reduces the dimension of the system?,"Suppose $\dot{x} = f(x)$ is a dynamical system on the state space $X$. My notes define a conservative system as one where there exists a (nontrivial) function $H: X \rightarrow \mathbb{R}$ such that $$\frac{d}{dt}H(x) = 0 = \nabla H(x) \cdot f(x)$$ If we then define $\Sigma_E := H^{-1}(E)$, $E \in \mathbb{R}$ as level sets of $H$, it follows that $\Sigma_E$ is an invariant set due to $H$ being constant along orbits. Immediately after this proof my notes puzzlingly state ""the dimension of the problem is reduced by one"". Later on when introducing Hamiltonian systems as a subset of conservative systems the classic example of the pendulum is given: \begin{align}
&\dot{\theta} = p \\
&\dot{p} = -\sin(\theta)\\
\end{align} It is noted that there is the Hamiltonian $H(p,\theta) = \frac{1}{2}p^2 -\cos(\theta)$ and then shown how any Hamiltonian must be a conserved quantity. Immediately after it is written that ""it is useful to notice conserved quantities when they exist as they reduce the dimension of the problem"". Am I missing something obvious? In the case of the pendulum the state space is 2-dimensional, having the Hamiltonian and plotting its level sets gives us the entire phase portrait which is obviously still 2-dimensional. I know a Hamiltonian system has some very nice properties over a generic system, but I do not know one relating to reducing dimensions. I guess my main question really is ""why do we care about conserved quantities?"".","['integrable-systems', 'nonlinear-dynamics', 'ordinary-differential-equations', 'dynamical-systems']"
2917084,Polar of a point with respect to a circle does not depend on the secants chosen to draw it.,Consider a circle and a point. To find the polar we draw any two secants and find the diagonal points. Then the polar is the line joining other 2 diagonal points. This is the projective definition of a polar. But how do we know that the polar of the red point is independent of the 2 secants/chords chosen to draw it? Proving this for a circle will be enough to show that its true for a general conic because collinearity and concurrence are preserved in a projective transformation.,"['geometric-transformation', 'projective-geometry', 'geometry']"
