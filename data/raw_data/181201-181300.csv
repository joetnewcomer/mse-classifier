question_id,title,body,tags
3302719,Ultraproducts of groups with a right inverse,"Start with an $I$ -indexed family of vector spaces $V_i$ . Take an ultrafilter $\mathcal{D}$ over $I$ . We have a linear transformation $$[-]: \prod_{i \in I} V_i \rightarrow \prod_{i \in I} V_i / \mathcal{D} $$ and we can always find another linear transformation $$ r: \prod_{i \in I} V_i/\mathcal{D} \rightarrow \prod_{i \in I} V_i$$ such that $[ r([f]) ] = [f]$ holds for every $f$ . This observation does not tell us anything specific about ultraproducts: we can find a right inverse to every surjective linear transformation. However, the analogous property fails badly for groups, e.g. the obvious group homomorphism $\mathbb{Z}/4\mathbb{Z} \rightarrow \mathbb{Z}/2\mathbb{Z}$ lacks a right inverse. If we have an $I$ -indexed family of groups $H_i$ , and $\mathcal{D}$ is
  a principal ultrafilter, then the quotient map $[-]_\mathcal{D}$ still admits a
  right inverse. What can we say when the ultrafilter $\mathcal{D}$ is
  non-principal? Do we always have a right inverse? If not, can we
  characterize the pairs of families and ultrafilters for which a right
  inverse exists? edit As people have pointed out, a general characterization is unlikely to say the least. However, if all the $H_i$ are finite and there is a section, then the ultraproduct embeds into a direct product of finite groups, and is therefore residually finite. I'd accept as an answer any partial result which puts nintrivial restrictive conditions on the group $G$ without assuming the finiteness of the $H_i$ . I'd also like to know which residually finite groups arise this way for finite $H_i$ (the answer is not all of them, since $SO(3)$ is trivially the quotient of a residually finite group, but I know it's not a quotient of any product of finite groups), but perhaps this is best asked as a separate question.","['group-theory', 'abstract-algebra', 'model-theory']"
3302840,On $p$-adic power series,"I'm stuck in a sentence made by Dwork in his book ""An introduction to $G$ -functions"": Let $b$ a positive real number, define the following set $$\mathcal{G}(b)=\left\{f(X): \begin{array}{c}
f(X)\in1+X\mathbb{C}_{p}[\![X]\!]\\
f \text{ converges for ord }x>-b\\
|f(x)-1|<1
\end{array}
\right\}$$ (where $1+X\mathbb{C}_{p}[\![X]\!]$ means that $f(0)=1$ , and $\text{ord}$ is the additive $p$ -adic valuation ), then he says: if $f(X)=1+a_{1}X+a_{2}X^{2}+\cdots\in\mathbb{C}_{p}[\![X]\!]$ , then $f(X)\in\mathcal{G}(b)$ if and only if $\text{ord }a_{j}\geq jb$ for every $j\geq 1$ . I have the converse, but I'm stuck dealing with the ""if"" direction ( $\Rightarrow$ ). I tried arguing by contradiction supposing that there is $j$ for which $\text{ord }a_{j}<jb$ and taking the minimal $j$ with this property I tried to use the strong triangle inequality to get a contradiction with the third property of the set $\mathcal{G}(b)$ , but these involves $x$ in the radius of convergence, not on the boundary. I will appreciate any hint, thanks.","['power-series', 'number-theory', 'p-adic-number-theory']"
3302853,Area of parallelogram = Area of square. Shear transform,"Below the parallelogram is obtained from square by stretching the top side while fixing the bottom. Since area of parallelogram is base times height, both square and parallelogram have the same area. This is true no matter how far I stretch the top side. In below figure it is easy to see why both areas are same. But it's not that obvious in first two figures. Any help seeing why the area doesn't change in first figure?","['quadrilateral', 'area', 'geometry']"
3302862,Implicit and explicit differentiation give contradictory answers in related rates problem.,"The problem statement: Oil spreads on a frying pan so that it's radius is proportional to $t^{1/2}$ , where $t$ represents the time from the moment when the oil
  is poured. Find the change $dT/dt$ of the thickness T of the oil. From the solution I can tell that we should assume that the oil is in the shape of a cylinder and that the volume is constant. Implicit solution (according to textbook): $r = kt^{1/2}$ $r^2T = \frac{V}{\pi} = c$ (definition of the volume of a cylinder) $2r\frac{dr}{dt}T + r^2\frac{dT}{dt} = 0$ $\frac{dT}{dt} = -\frac{2}{r}\frac{dr}{dt}$ $\frac{dT}{dt} = -\frac{1}{t}$ (after computing the derivative of r) According to the textbook this is the correct solution, but if we differentiate explicitly: $r = kt^{1/2}$ $T = \frac{V}{r^2\pi} = \frac{V}{k^2t\pi}$ $\frac{dT}{dt} = -\frac{V}{k^2\pi} \frac{1}{t^2} = -\frac{r^2\pi T}{k^2\pi} \frac{1}{t^2} = -\frac{k^2tT}{k^2} \frac{1}{t^2} = -\frac{T}{t}$ If both of the solution are correct then $\frac{dT}{dt} = -\frac{1}{t} = -\frac{T}{t}$ and $T = 1$ , which can't be possible. Why does differentiating implicitly and explicitly give different contradictory solutions?","['related-rates', 'calculus', 'derivatives']"
3302886,"How many different sums can you get when you ""add up all the integers""?","Suppose you want to ""add up all the integers"" naively, by devising some way of arranging the integers in sequence and finding the limit of the partial sums of that sequence, possibly grouping up and pre-summing groups of integers. You could represent this formally by defining a function $f : \mathbb{N} \to \{\mathbb{Z}\}$ subject to $\forall n \in \mathbb{Z}. \exists i \in \mathbb{N}. n \in f(i)$ $\forall i, j, n \text{ with } i \not= j. n \in f(i) \implies n \not\in f(j)$ Every $f(i)$ is a finite set and taking the following limit: $$\lim_{n\to\infty}\sum_{i=1}^{n} \Sigma f(i)$$ There are at least 3 different limits you can obtain with different choices of $f$ : $f(i) = \{i, -i\}$ produces the sequence $0, 1 + -1, 2 + -2, \ldots$ . Since every term is 0, the limit is 0 as well. $f(i) = \{1 + i, 1 - i\}$ produces the sequence $1, 2 + 0, 3 + -1, 4 + -2, \ldots$ . Since each term after the first sums to 2 the limit is $+\infty$ . $f(i) = \{-1 + i, -1 - i\}$ produces the sequence $-1, 0 + -2, 1 + -3, 2 + -4, \ldots$ . Since each term after the first sums to -2, the limit is $-\infty$ . My question is: Are 0, $+\infty$ , and $-\infty$ the only possible limits? Is it possible with a clever choice of $f$ to get a limit of, say, 10, or any other number? (It's clearly also true that there are plenty of legal $f$ s that lead to the limit not existing; I'm not interested in that here.) Fun note: This question is inspired by a discussion I had with a roommate long ago, who wrote a poem with the line ""the sum of all the numbers is zero"" based on the reasoning I gave above. I pointed out that infinite series are weird and immediately came up with the two other ""sums"" I've listed here. Ever since then, and I've been wondering if there are other ""sums."" I can't find any but also can't prove that none exist. You'll be happy to know that the roommate discussed the issue with his poetry professor, who allowed the line even though the mathematics turned out to be dubious.",['sequences-and-series']
3302960,Does a definite integral equal to the Möbius function exist?,"I've been studying infinite sums and came across the Möbius function, and I was wondering if there exists a definite integral that represents it, or equivalently if there exists some $f(x,n)$ and $a, b$ such that $$
\int_a^b f(x,n)dx = \mu(n).
$$ I have no idea how to begin to go about solving this problem, but through my basic workings, I have the suspicion that there may be no closed form that satisfies this equation.","['complex-analysis', 'number-theory']"
3302971,Is it consistent with ZF that all finitely additive probability measures on $\mathbb{N}$ are countably additive?,"This question is inspired by this other question which asks for an example of a strictly finitely-additive probability measure. To answer that question, I use the existence of a non-principal ultrafilter (assuming the ultrafilter lemma) on $\mathbb{N}$ to construct a $\{0,1\}$ -valued example on $\mathcal{P}(\mathbb{N})$ . I'm no expert on set theory, however I am aware that it is consistent with ZF that all ultrafilters on $\mathbb{N}$ are principal $^{[1]}$ . After some digging, I also found out that it is consistent with ZF+DC that there are no non-principal measures (finitely additive probability measures that assign $0$ measure to singletons) on $\mathbb{N}.$$^{[2],[3]}$ This at least shows that I really need something like the ultrafilter lemma to construct an example like the one I give (which is a non-principal measure). Unfortunately, as far as I can tell, this doesn't quite complete the picture since it is possible that one could find an example which assigns some singleton positive measure. A silly way to do this assuming that a non-principal measure $\mu$ on $\mathbb{N}$ does exist is to consider $\frac12(\mu + \delta_0)$ . Obviously here I use again the ultrafilter lemma (and I expect that again, I really need to to get such an example), but the possibility of such examples leads me to ask; Is it consistent with ZF that all finitely additive probability measures on $\mathbb{N}$ are countably additive? [1]: This is asserted in this answer by Asaf Karagila. [2]: See this answer from the same user and the reference therein (my [3]) [3]: David Pincus and Robert M. Solovay , Definability of measures and ultrafilters , J. Symbolic Logic 42 (1977), no. 2, 179--190.","['axiom-of-choice', 'measure-theory', 'probability-theory', 'set-theory']"
3303002,Finite normal subgroups of polycyclic-by-finite groups,Let $G$ be a polycyclic-by-finite and $H<G$ a finite index subgroup. Let $F(G)$ be the unique maximal finite normal subgroup of $G$ . Then $F(G)\cap H$ is finite and normal in $H$ . Question 1. Suppose $F(H)$ is non-trivial. Is $F(G)\cap H$ non-trivial? Question 2. Is $F(G)\cap H=F(H)$ ?,"['group-theory', 'normal-subgroups']"
3303028,Probability of unfair coin not giving 2 heads,"Problem: An unfair coin with probability $p$ to flip Tails is flipped 4 times independently. What is the probability of no Heads twice in a row? My answer: I calculated the probability that of the event $$A = \text{Heads twice in a row}$$ by splitting it into the three events $$A_i = \text{i Heads, Heads twice in a row}$$ , took the complement and got: $\mathbb{P}(A^c) = 1- 3p^2(1-p)^2 - 4p(1-p)^3 - (1-p)^4$ Is there a quicker way to derive the answer?","['mathematical-modeling', 'independence', 'combinatorics', 'probability-theory', 'probability']"
3303043,Tensor Product with Trivial Vector Space,"This question seems obvious and yet I can't seem to find a good answer anywhere. Let $V$ be a finite dimensional vector space, and let $0$ denote the trivial vector space. Is $V \otimes 0 = 0$ or $V \otimes 0 = V$ ? My gut tells me that it is the second case, but in thinking about dimension, tensor product should multiply dimension in which case I think it is the first case.","['tensor-products', 'linear-algebra', 'vector-spaces']"
3303051,"Tracial States, Strictly Positive Elements, and Commutators","From my limited understanding, strictly positive elements in a $C^*$ -algebra $\mathcal{A}$ can be defined in one of two (equivalent) ways: (1) $x \in \mathcal{A}_+$ is strictly positive provided $\overline{x \mathcal{A}x} = \mathcal{A}$ (2) $x^\ast = x$ is strictly positive provided $\phi (x) > 0$ for every state $\phi$ on $\mathcal{A}$ . In what follows, let's assume that $\mathcal{A}$ is unital and has tracial states. I am wondering if there is any connection between tracial states, strictly positive elements, and (sums) of commutators. E.g., perhaps something like the following holds: Let $x^\ast = x$ . If $\tau (x) > 0$ for every tracial state $\tau$ on $\mathcal{A}$ , then $x = p + c$ for some strictly positive element $p$ and some $c$ which is a (sum) of commutator(s). Of course, I am not committed to the above being true, although it would be nice if it were; I am just wondering if something like it is true. 
` EDIT Perhaps this MO post is relevant: https://mathoverflow.net/questions/66343/commutators-in-the-reduced-c-algebra-of-the-free-group The answers there suggest that a sum of commutators should be replaced a limit of sums of commutators (no?). Also, I should add that I am also interested in case of tracial von Neumann Algebras (vNAs); e.g., $II_1$ factors. I know all vNAs are $C^\ast$ -algebras, but I've been told that, in general, it isn't particularly helpful to think of vNAs as $C^\ast$ -algebras. So perhaps the case of vNAs makes the question more manageable or easier to interpret (I know my question is somewhat vague, and I apologize for that) EDIT: A day or two ago someone posted a comment on this question that went roughly like the following: The answer is, unfortunately, unexciting: let $p$ be any strictly positive element. Then $x = \frac{\tau (x)}{\tau (p)} p + (x - \frac{\tau(x)}{\tau (p)}p)$ My question is, why was this comment deleted? I can't see anything wrong with it. The above decomposition shows that $x$ can be written as (any) strictly positive element plus a norm limit of sums of commutators.","['c-star-algebras', 'trace', 'functional-analysis', 'operator-algebras']"
3303115,Distribution of digits across all factorials,"Show that the distribution of zeroes across all digits of all n! for $n\in \mathbb{N}$ converges to $ \frac{1}{6} $ and hence, $\frac{5}{54}$ for all other digits 1 through 9 . In other words, let's say we take the factorial of 1,2,3 and 4. We get the digits 1,2,2,4 and 6. In this case the probability of 2 would be $\frac{2}{5}$ . We can keep doing this all the way to infinity. I wrote a python script and checked from 0 to 10000, and all probabilities converge to 0.09259259259.. = $\frac{5}{54}$ except zero, which converges to 0.16666... = $\frac{1}{6}$ . Can you prove mathematically why this is the case?","['proof-writing', 'probability-distributions', 'factorial', 'probability']"
3303126,"Problem to find arithmetic mean of the largest elements of $r$-subsets of ${1,2,...,n}$","Let $1\le r\le n$ and consider all $r$ -element subsets of the set $\{1,2,...,n\}$ . Each of these subsets has a largest element. Let $H(n,r)$ denote the arithmetic mean of these largest numbers. Find $H(n,r)$ and simplify your result. I have made the following attempt, but get stuck: Possible largest elements in $r$ -element subsets are: $\{r, r+1,\ldots,n-1,n\}$ If $r$ is the largest elements, other elements must be taken from the $r-1$ elements $\lt r$ . Their sum of given by: $$r{r-1 \choose r-1}$$ Similarly, summing over $\{r, r+1,...,n\}$ gives: $$r{r-1 \choose r-1} + (r+1){r \choose r-1}+\cdots+n{n-1 \choose r-1}$$ Dividing the above expression by ${n \choose r}$ should give me their arithmetic mean. But I know not how to simplify the expression... All help is greatly appreciated! Thanks you! P.S.: And, on a lighter note, could someone tell me about how probability is useful in Combinatorics?... I'm a newbie, so I don't know...","['elementary-set-theory', 'binomial-coefficients', 'combinatorics']"
3303143,"Show that $\int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx.$","Let $1\leq p<\infty$ . Suppose $\{f_n\}\subset L^p(\mathbb R^d)$ satisfies $\int_{\mathbb R^d} |f_n|^p\,dx\leq M$ and $f_n\to f$ for almost every $x\in\mathbb R^d$ . Show that $$\int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx.$$ Applying Fatou's lemma I get that $\int_{\mathbb R^d}|f|^p\,dx\leq \liminf_{n\to\infty} \int_{\mathbb R^d} |f_n|^p\,dx\leq M$ , so $f\in L^p(\mathbb R^d)$ . But I can't move forward. If I had $\int_{\mathbb R^d} |f_n|^p\,dx\to \int_{\mathbb R^d}|f|^p\,dx$ , I could prove that $\int_{\mathbb R^d}|f_n-f|^p\,dx\to 0$ so we are done. But it is not always true. Any help please?","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis', 'lp-spaces']"
3303156,When can a vector field be rescaled to have constant divergence?,"Suppose $X$ is a smooth vector field on $\mathbb{R}^n$ having divergence $$\nabla \cdot X < 0$$ everywhere. Does there always exist a positive smooth ""rescaling"" function $g:\Bbb R^n \to (0,\infty)$ such that $$\nabla\cdot (g X) \equiv -1?$$ ( This question appears slightly related, but different.)","['differential-geometry', 'ordinary-differential-equations', 'vector-analysis', 'differential-topology', 'dynamical-systems']"
3303166,Baby Rudin 5.26,"Could someone help me out with the solution written in Roger Cooke's solutions for Baby Rudin 5.26? https://minds.wisconsin.edu/bitstream/handle/1793/67009/rudin%20ch%205.pdf?sequence=7&isAllowed=y The question is, Suppose $f$ is differentiable on $[a,b]$ , $f(a)=0$ and there is a real
  number $A$ such that $|f'(x)| \le A|f(x)|$ on $[a,b]$ . Prove that $f(x)=0$ for all $x\in [a,b]$ . The solution says in its last paragraph, that But by definition of $M_0$ , this implies $M_0 \le \frac{M_0}{2}$ , so that $M_0\le 0$ ,  i.e. $M_0=0$ . I'm interpreting this as $M_0(x_o-a)A\ge M_0$ , is assured if $x_0 \equiv a+\frac{1}{2A}$ . But if $M_0$ is extremely large and $(x_0-a)$ is not so small, would this inequality still hold? More formally, Are $|f(x)| \le M_0$ and $|f(x)| \le M_0 (x_0-a) A$ , enough conditions for deriving $M_0(x_0-a)A\ge M_0$ ?","['proof-explanation', 'derivatives']"
3303179,a Differential inequality without integration,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ a differentiable function with $f(x)+f^{'}(x)\leq1$ for all $x \in \mathbb{R}$ and $f(0)=0$ . Which is the maximum possible value of $f(1)$ ? The question is ""solved"" here : maximum value and a differential inequality but I did a mistake by supposing that $\frac{d}{d x} (e^x f(x)) $ is integrable in the interval $[0,1]$ . I don't know how to solve the problem without such condition. Someone could help me? thanks in advance","['analysis', 'real-analysis']"
3303192,"Please help: If $a \not\subset B$ and $a \in A$, is $a$ an element of $B$?","I'm learning set theory and this is a homework question. I answered, ""not necessarily because nothing is telling us whether $a$ is an element of $B$ and an element cannot be a subset"". Am I right? What is the correct answer? Thanks for your help!",['elementary-set-theory']
3303274,Unsure of my work evaluating $\int \frac{dx}{\sqrt{x + \sqrt{x + \sqrt{x + \cdots}}}}$,"This Question is an Extension of this Previously Asked Question: Nested root integral $\int_0^1 \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}$ I was looking into answering the question of whether it was possible to integrate the fully nested root integral of the variety described in the previous problem: $$
\int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}}
$$ So I started by defining the nested root in another way $$
u=\sqrt{x+u} \therefore
\\
u^2-u=x
\\
(2u-1)du = dx
$$ Using the results of the substitution I have set up $$
\int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}} = \int \frac{2u-1}{u} du
\\
= \int \left(2-\frac{1}{u}\right)du = 2u-\ln(u)
$$ I am unsure of my work, as I have never attempted to integrate any infinitely nested functions. Therefore I have no idea whether my method for u-substitution is valid. Am I just living under a rock or have other people seen this method used previously? For such a seemingly intimidating problem it was surely quite easy. I had difficulty checking my work using wolfram alpha, but I managed to confirm that this works for the definite integral limits from $x = 1$ to $x = 2$ and from $x = 1$ to $x = 3$ . Maybe I am just flat out wrong and got lucky on these two calculations?","['integration', 'nested-radicals', 'definite-integrals', 'real-analysis']"
3303291,Solvable $4$-transitive permutation group is isomorphic to $S_4$.,"I have difficulty  in solving the problem Show that a solvable $4$ -transitive permutation group $G$ is isomorphic to $S_4$ . (Finite Group Theory by Isaacs, $8$ A. $10$ ) There's hint here: Show a minimal normal subgroup $N$ of $G$ is regular and consider the conjugation action of a point stablizer $G_\alpha$ on $N$ . Suppose $G$ acts on $\Omega$ . Following the hint, I only know that if $N$ is regular, then the conjugation action of $G_\alpha$ on $N$ is isomorphic to the action of $G_\alpha$ on $\Omega-\{\alpha\}$ , which is $3$ -transitive. Thanks a lot!","['permutations', 'group-theory', 'abstract-algebra', 'finite-groups']"
3303294,"Can two figures have the same area, perimeter, and same number of segments have different shape?","I want to make an algorithm grouping all the details having the same shape.
each detail is defined by its surface, and a list of contour lines. First I believed that having the same perimeter length and same surface would be enough, but I saw on that link that it is wrong hypothesis. If I take as additional condition that the two shapes have the same number of segments, would it be enough? Or else how can I check that? The problem is for each detail, they can get rotation, or symmetry. Edit : Thanks for your answers, I finally found a way to solve the problem (answer below)",['geometry']
3303354,Can we define the complex numbers without using the real numbers?,"Every definition of $\mathbb{C}$ that I can think of uses $\mathbb{R}$ . We can let $\mathbb{C}=\mathbb{R}^2$ equipped with a particular multiplication. We can let $\mathbb{C}$ be the algebraic closure of $\mathbb{R}$ . The best way I could come up with to define $\mathbb{C}$ without $\mathbb{R}$ is to let $\mathbb{C}$ be the ""completion"" of the algebraic closure of the rational numbers. I'm not really sure what the ""completion"" would mean in this case. Can anyone finish this idea, or suggest a different approach?","['complex-analysis', 'definition', 'complete-spaces', 'complex-numbers']"
3303378,Morphisms preserving closed points,"I think that for any field $k$ and any $k$ -schemes $X$ , $Y$ locally of finite type, any $k$ -morphism $X\rightarrow Y$ sends closed points to closed points ( https://math.stackexchange.com/a/116636/691259 ). I want to understand what happens if we do not assume that both $X$ and $Y$ are locally of finite type over $k$ . Assume $k$ is algebraically closed. Does there exist a $k$ -scheme $X$ and a $k$ -scheme locally of finite type $Y$ and a $k$ -morphism sending a closed point to a non-closed point? I think the answer is yes, consider the map $\mathrm{Spec}\:Frac(\mathbb{C}[x])\rightarrow \mathrm{Spec}\:\mathbb{C}[x]$ . Does there exist a $k$ -scheme locally of finite type $X$ and a $k$ -scheme $Y$ and a $k$ -morphism sending a closed point to a non-closed point?",['algebraic-geometry']
3303392,Combinatorics: Why isn't $(^{20}C_{12}) ≠ (^{20}C_{11})(^9C_1)$?,"I came across $2$ questions: A jury pool of $20$ people are called to a courthouse. How many ways are there to select $12$ to serve as jury? A jury pool of $20$ people are called to a courthouse. How many ways are there to select $11$ to serve as jury, and $1$ to serve as jury foreman? Can someone give me an intuitive explanation on how to think about this? Mathematically I know the calculation returns different values but I don't really understand why. It can't be because of the distinction between ""jury"" and ""jury foreman"", right? Ultimately we're still selecting $12$ people from a pool of $20$ isn't it?",['combinatorics']
3303406,Number of complex roots,"Find total number of roots inside $|z| \le 1$ for $\sin(z^{100})=z/11$ .
I tried to apply Rouché's theorem: For any two complex-valued functions $f$ and $g$ holomorphic inside some region $K$ with closed contour $\partial K$ if $|g(z)| < |f(z)|$ on $\partial K$ then $f$ and $f + g$ have the same number of zeros inside $K$ . Also, $\sin(z^{100})=\sum_{n=0}^\infty \frac{(z^{100})^{2n+1}}{(2n+1)!}\cdot(-1)^n$ However, it gets me nowhere. Could you please give me a hint?",['complex-analysis']
3303469,How to find the fixed field for Galois group?,"Let be $K$ the finite splitting field of $f(x) (\in \Bbb Q[x])$ over the field, $\Bbb Q$ (rational number set) And say $E_H$ is a fixed field of $H\subset \operatorname{Gal}(K/\mathbb{Q}) $ . Main Question) Find the fixed field $E_H$ (1) $f(x) = x^4 -2$ , $H= \langle \sigma \rangle$ with $\sigma(\alpha) =  -\alpha i $ , $ \sigma(i)  = -i $ , and $\alpha = 2^{1 \over 4}$ (2) $f(x) = x^8 +1$ , $H= \{ \sigma_1,   \sigma_7 ,\sigma_9,  \sigma_{15 }  \}$ with $\sigma_n (\omega) =  \omega  \to \omega^n $ for $\omega = e^{{2\pi i} \over 16} $ and $gcd(n,16)=1$ P.s.) I've solved the (1) by inefficient way that writing the element form like a method in attached image. So I use this method for solving (2) to find the fixed field for $H$ . But the process really complicated, so I can't  find the fixed field.(C.f. the below of this post's image is my attempt) Are there any simple method for finding the fixed field? Thanks.","['field-theory', 'galois-theory', 'abstract-algebra']"
3303470,Hamiltonian flow vs Gradient flow,"A Hamiltonian vector field in a symplectic manifold is the analog of a gradient vector field in Riemannian manifold. A simple computation using Cartan's magic formula shows that a Hamiltonian vector field preserves the symplectic structure, equivalently its flow acts by symplectomorphisms. However it is not the case that a gradient flow preserves the Riemannian metric, even for a Euclidean metric. Why does it work in one case and not the other? I mean, I understand the computations, but is there any insight that can be given?","['symplectic-geometry', 'riemannian-geometry', 'differential-geometry']"
3303483,Integral $\int_{0}^{\frac{\pi}4} \ln(\sin{x}+\cos{x}+\sqrt{\sin{2x}})dx$,"Prove that $$\int_{0}^{\frac{\pi}4} \ln(\sin{x}+\cos{x}+\sqrt{\sin{2x}})dx =\frac{\pi}{4} \ln2$$ I tried to use King's rule and to scale by $2$ and then to add the integrals, to get product of terms and use the result $$\int_{0}^{\frac{\pi}2} \ln(\sin{x})dx=\int_{0}^{\frac{\pi}2} \ln(\cos{x})dx=-\frac{\pi}2\ln2$$ but it didnt work. Any help?","['integration', 'calculus', 'definite-integrals']"
3303525,Prime values of $\sum_{j=1}^n j^k$,"Today somebody has asked the following: Can the number $S_{n,k}=\sum_{j=1}^n j^k$ be prime for positive integers $n,k$ ? I don't know the reason, but the question was deleted few minutes after. I don't remember her/his username. I find the question interesting enough to ask it again. I have some results as well. The case $n=1$ is trivial. The case $n=2$ gives some examples, namely Fermat primes. So we assume from now that $n\ge 3$ . (The original problem put this as a condition). If $n$ is a power of two, the sum has $n/2$ odd numbers, and $n/2$ is even, so the sum is even, and, obviously, greater than $2$ , so $S_{2^r\!,k}$ is composite. If $n$ has an odd prime divisor $p$ and $k$ is not a multiple of $p-1$ , the numbers $$1^k, 2^k,\ldots,(p-1)^k$$ are exactly the roots of the polynomial $$X^{(p-1)/d}-1\in\Bbb Z_p[X]$$ where $d=\gcd(p-1,k)$ , counted $d$ times each, so their sum is $0$ . Since $n$ is a multiple of $p$ , this sum is repeated $n/p$ times, so it is $0$ $\pmod p$ also. In other words: If $p$ is an odd prime divisor of $n$ and $k$ is not a multiple of $p-1$ , then $p\mid S_{n,k}$ What happens if $n$ has no such divisor? Well, I have also obtained a few facts. If $n\equiv 3\pmod 4$ , then the sum contains $(n+1)/2$ odd terms, so it is even. If $n$ is a prime $p$ , $2p+1$ is also prime and $k=p-1$ , the numbers $$1^{p-1},2^{p-1},\ldots,p^{p-1}$$ are the roots of the polynomial $$X^p-1\in\Bbb Z_{2p+1}[X]$$ so again, their sum is $0\pmod {2p+1}$ . That is: If $p$ is prime and $2p+1$ is prime, then $2p+1\mid S_{p,p-1}$ Of course, there are many not covered cases.","['number-theory', 'divisibility', 'elementary-number-theory', 'prime-numbers']"
3303545,Are $f$ satisfying $|f(y)| = |f(x+y) - f(x)|$ additive?,"(This question is inspired by this question , and in particular the comment by Charlie Cunningham.) Let $(V, \|\cdot\|)$ be a normed real vector space. Let $f: V \to V$ be a function satisfying, for all $x, y \in V$ , $$
\|f(y)\| = \|f(x+y) - f(x)\|.
$$ Does it follow that $f$ is additive, i.e., that $f(x + y) = f(x) + f(y)$ for all $x, y \in V$ ? This seems like it should be a fairly easy question, but I haven't quite figured it out yet. Some fairly trivial observations: Taking $y=0$ gives us $\|f(0)\| = \|f(x) - f(x)\| = 0$ , so $f(0) = 0$ . Taking $y=-x$ gives us $\|f(-x)\| = \|-f(x)\|$ . If we could prove that $f(-x) = -f(x)$ , we could take $y=-2x$ to prove that $\|f(2y)\| = 2\|f(y)\|$ . Using the equation for both $y$ and $-y$ gives us $$
\|f(x+y) - f(x)\| = \|f(x-y) - f(x)\|.
$$ It does not matter that $f$ is an endofunction (it could go $f: V \to W$ too) for the answer of the question, but this was a simpler formulation.","['functional-equations', 'linear-algebra']"
3303563,"Evaluate $\iiint_{[0,1]^3}\frac{dx\,dy\,dz}{(1+x^2+y^2+z^2)^2}$","As in the title I have to evaluate this triple integral: $$\iiint_{[0,1]^3}\frac{dx\,dy\,dz}{(1+x^2+y^2+z^2)^2}$$ I've been trying to solve this since a week ago. The first thing I've done was understand the meaning of the integral. I think this integral represents the mass (as an example) of a unitary cube which contains materials of different density. The values of the materials density are, point for point, the inverse of the square of spheres centered in the origin plus one. The max value of the density is $1$ in the origin of the cube and the min value is $\frac{1}{16}$ on the opposite vertex. I suppose that the value of the integral is $\frac{\pi^2}{32}$ I've tried to use simple substitutions without any results, so I tried to change the coordinates with spherical and cylindrical systems. The spherical coordinates give me an incredibly long sum of integrals and I doubt that they're all integrable as elementary functions. The cylindrical gives me the following result $$\frac{\pi^2}{16}-\int_0^\frac{\sqrt2}{2}{\frac{\arctan{\sqrt{\frac{u^2-1}{u^2-2}}}}{\sqrt{2-u^2}}du},$$ which I'm not able to solve. My instinct tells me that there is a trick in some steps where I can observe that a difficult integral actually is exactly half of another one simpler but I can't figure out where. I'll appreciate any kind of suggestions.","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
3303589,A bounded linear operator $ T: L^2 \to L^2$ with these properties (Proof check)?,"A bounded linear operator $ T: L^2 \to L^2$ with these properties : Commutes with translation Commutes with dilation Has in its kernel functions $ f $ such that support $\hat {f} \subseteq [0,\infty) $ . where $\hat {f}$ is the fourier transform of $f$ For  a Schwartz function $f$ , $T= f* \mu$ where $\mu $ is a tempered distribution. Then $\int_{-\infty}^0\hat {f}(s)e^{2i\pi s x} ds =c Tf (x)$ where $ c $ is s constant. Is the proof below correct ? Known result from Riesz Representation theorem see here : $\int Tf(s)u(s) ds=\int f(s)T^*u(s)ds$ written as $<Tf,u>=<f,T^*u>$ , where $ T^*$ is the linear adjoint operator of $ T $ , $ u \in L^2$ now for $f(s)$ translated  by $x$ , $f(s+x)$ we have $\int Tf(s+x)u(s)ds=\int f(s+x)T^*u(s)ds$ Now take Let $u_{\epsilon}=\phi_\epsilon(s)=\phi(s/\epsilon)\epsilon^{-1}$ where $\phi(s)$ is normalized gaussian function with zero mean. $\int Tf(s+x)u_{\epsilon}(-s) ds=\int Tf(s+x)u_{\epsilon}(s)ds=\int f(s+x)T^*u_{\epsilon}(s)ds$ $H(s)=Tf(s)$ , by translation commutation $H(s+x)=Tf(s+x)$ $R(-s)=T^*u_{\epsilon}$ we have $\lim_{\epsilon \to 0} H* u_{\epsilon}=\lim_{\epsilon \to 0} f*R$ $\lim_{\epsilon \to 0} H*u_{\epsilon}=H(x)$ ae for $u \in L^1$ function, we have the following theorem regarding fourier transform: $v(x)=g*u=\int g(s)u(x-s) ds$ for $g \in L^2$ , define $\hat{g}=lim_{n \to \infty}\int_{n}^{-n}g(s)e^{2i\pi x z} dx$ the limit exists in the sense of $L^2$ . If $u \in L^1$ and $\int |g(s)||u(x-s)| ds \le P$ where $P$ is a non negative integrable function in $L^2$ then $\hat{v}=\hat{g}\hat{u}$ Proof : $g_n=g1_{[-n,n]},v_n=g_n*u$ it's known that $\hat{v_n}=\hat{g_n}\hat{u}$ By dominated convergence theorem $lim_{n \to \infty} ||v-v_n||^2=0$ and by Plancherel theorem $lim_{n \to \infty} ||v-v_m||^2= lim_{n \to \infty} ||\hat{v}-\hat{v_n}||^2=0$ This implies $lim_{n \to \infty} \hat{v_n}=\hat{v}$ Now from our equation when $u$ is a normalized  Gaussian we have $|H|* |u_{\epsilon}| \le MH(x)$ where $MH(x)$ is the Hardy Littlewood Maximal function (also $ ||MH(x)||^2 \le ||H(x)||^2$ )
using Fourier transform $\hat{H}(z)\hat{u_{\epsilon}}(z)=\hat{f}(z)G(z)$ where $G(z)=\hat{R}$ $\lim_{\epsilon \to 0}\hat{ u_{\epsilon}}=1$ $\lim_{\epsilon \to 0}G=m(z)$ consequently we have $\hat{Hf}(z)=m(z)\hat{f}(z)$ Therefore $L^2$ limit: $H(x)=\lim_{n \to \infty}\int_{-n}^{n}m(z)\hat {f}(z)e^{2i\pi x z} dz$ Now using dilation commutation $\lim_{n \to \infty}\int_{-n}^{n}m(z)\hat {f}(z)e^{2i\pi s z} dz=\lim_{n \to \infty}\int_{-n}^{n}a^{-1}m(z)\hat {f}(a^{-1}z)e^{2i\pi a^{-1}s z} dz=\lim_{n \to \infty}\int_{-a^{-1}n}^{a^{-1}n}m(az)\hat {f}(z)e^{2i\pi s z} dz$ define $ g=m(az)\hat{f}(z)$ define $g_n=g_{[{-n,n}]}-g_{[{-a^{-1}n,a^{-1}n}]}$ $\lim_{n \to \infty} ||g_n||^2=0$ $\hat{g_n}=\int_{-n}^{n}m(az)\hat {f}(z)e^{2i\pi s z} dz-\int_{-a^{-1}n}^{a^{-1}n}m(az)\hat {f}(z)e^{2i\pi s z} dz$ By Plancherel theorem $\lim_{n \to \infty} ||\hat{g_n}||^2=\lim_{n \to \infty} ||g_n||^2=0$ Therefore there is a subsequence $\{n\}$ such that $\lim_{n \to \infty}\int_{-a^{-1}n}^{a^{-1}n}m(az)\hat {f}(z)e^{2i\pi s z} = \lim_{n \to \infty}\int_{-n}^{n}m(az)\hat {f}(z)e^{2i\pi s z}$ so $\lim_{n \to \infty}\int_{-n}^{n}(m(z)-m(az))\hat {f}(z)e^{2i\pi s z} dz=0$ and by using Plancherel theorem this implies $m(az)=m(z)$ ae for any $a>0$ Therefore $m(z)=c_{-} $ ae on $z \in (-\infty ,0)$ and $m(z)=c_{+}$ ae on $ z \in (0,\infty)$ according to see here Given the 3rd condition $c_{+}\int_{0}^{\infty}\hat {f}(z)e^{2i\pi x z}dz=0$ ,since we know $\int_{0}^{\infty}\hat {f}(z)e^{2i\pi x z}dz\ne 0$ ,so $c_{+}=0$ Therefore $Hf(x)=c_{-}\int_{-\infty}^0\hat {f}(z)e^{2i\pi z x} dz$ ae","['measure-theory', 'fourier-analysis', 'operator-theory', 'solution-verification', 'functional-analysis']"
3303592,Linear Algebra 4th edition by Friedberg exercise 12 in section 2.7,"Let $V$ be the solution space of an $n$ th-order homogeneous linear differential equation with constant coefficients having auxiliary polynomial $p(t)$ . Prove that if $p(t) = g(t)h(t)$ , where $g(t)$ and $h(t)$ are polynomials of postive degree, then $$N(h(D)) = R(g(D_V)) = g(D)(V),$$ where $D_V : V \to V$ is defined by $D_V(x) = x'$ for $x\in V$ . Hint: first prove $g(D)(V) \subseteq N(h(D))$ . Then, prove that the two spaces have the same finite dimension. Let $x \in g(D)(V)$ . Then, there exists $y \in V$ , such that $g(D)(y) =x$ . Note that $p(D)(y) = 0$ since $y \in V$ . This implies that $$p(D)(y) = g(D)h(D)(y) = h(D)g(D)(y) = h(D)(x) = 0.$$ Therefore, $x\in N(h(D))$ . I am struggling with the second part of the hint. First, note that by dimension theorem, $\dim(V) = \dim(N(g(D_V)) + \dim(R(g(D_V))$ , and that $N(g(D_V)) = N(g(D))$ since $N(g(D)) \subseteq V$ , and that I have a theorem that the solution space $N(g(D))$ has a dimension equal to the degree of $g$ . Taken all together, $\dim(N(g(D_V)) = g$ (let $g$ be the degree of $g$ ), and this implies that $\dim(R(g(D_V)) = n-g$ . Therefore, if I show that $\dim(N(h(D)) = n-g$ , it finishes the proof. I have a theorem that if $p(D) = g(D)h(D)$ and $g(D)$ (or $h(D)$ ) is onto, then $$\dim(N(p(D)) = \dim(N(g(D)) + \dim(N(h(D)).$$ I know that $\dim(N(p(D)) = n$ and $\dim(N(g(D)) = g$ . But, I do not know that $g(D)$ (or $h(D)$ ) is onto. How can I show that either one operator is onto or is there another way to reach the answer?","['linear-algebra', 'ordinary-differential-equations']"
3303599,Where does the $dy$ go in the process of integration?,"I'm currently looking at integration (calculus 1). For example $y = \int 3x dx$ I don't understand how we ended up with just "" $y$ "" on the left hand side. For example $dy/dx = 3x$ , then $dy = 3x dx$ so $\int dy = \int 3x dx$ which is not the same. Is $\int dy$ somehow the same as $y$ ? I get confused as some use $dx$ as just notation while others actually use it as a term that can be moved around.","['integration', 'calculus', 'derivatives']"
3303642,Matrices commuting with a given $3\times 3$ complex matrix.,"Let $A$ be a $3\times 3$ complex matrix. Let $C(A)$ be the vector space of complex matrices that commute with $A$ . Show that the complex dimension of $C(A)$ is at least $3$ . I know that this kind of questions has been asked many times on this site. And there is an explicit formula for the dimension of $C(A)$ given by Frobenius viewing the matrices $B$ that commute with $A$ as endomorphisms of $\mathbb C[\lambda]$ -module. But I am looking for a more elementary way to show the lower bound of the dimension of $C(A)$ is $3$ . For example, I have already found that $\operatorname{Span}\{ I, A \}$ is a two-dimensional subspace of $C(A)$ for $A\notin\operatorname{Span}\{I\}$ , where $I$ is the identity matrix. But how to find another matrix that is linearly independent of $\operatorname{Span}\{I, A\}$ ? Thanks.","['matrices', 'abstract-algebra', 'linear-algebra']"
3303666,Is there a number field of degree n whose ring of integers is a unique factorization domain?,"For every $n$ , can we find a number field of degree $n$ whose ring of integers is a unique factorization domain? As a Dedekind domain is a UFD iff it is a PID, this is equivalent to asking the following: For every $n$ , can we find a number field of degree $n$ with class number 1.","['number-theory', 'algebraic-number-theory']"
3303679,"Find, analytically, the value of the following limit.","How would one prove that $$\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=1}^n\frac{1}{\sqrt{2k}+\sqrt{2k-1}}$$ converges (rather slowly) to $\frac {1}{\sqrt{2}}$ , which appears obvious from numerical computation.","['summation', 'telescopic-series', 'analysis', 'sequences-and-series', 'limits']"
3303681,Prove that if $B$ is a set and $\mathcal F$ is family of sets and $∪ \mathcal F ⊆ B$ then $\mathcal F ⊆ \mathscr P(B).$,"Suppose $B$ is a set and $\mathcal F$ is a family of sets. Prove that
  if $∪ \mathcal F ⊆ B$ then $\mathcal F  ⊆  \mathscr P(B).$ Note: $\mathscr P(B)$ stands for power set of $B$ . Suppose $\bigcup \mathcal F ⊆ B$ . $\bigcup F$ is the set that contains the elements of all subsets in $\mathcal F$ . In other words, if arbitrary set, call it $A$ , is a subset of $\mathcal F$ , then all its elements will be in $\bigcup F$ , or more formally: $\forall A(A \in \mathcal F \implies A \subseteq \bigcup F)$ $\bigcup \mathcal F ⊆ B$ means that all elements in $\bigcup \mathcal F$ are also in $B$ . It follows that if arbitrary set, call it $A$ , is the subset of $\bigcup \mathcal F$ , then it also will be the subset of $B$ , or more formally: $\forall A(A \subseteq \bigcup F \implies A \subseteq B)$ By definition, $\mathscr P(B)$ is the set containing all the subsets of $B$ . In other words, $\forall A (A \subseteq B \implies A \in \mathscr P(B))$ To sum it all up, we have: $\forall A(A \in \mathcal F \implies A \subseteq \bigcup \mathcal F)$ $\forall A(A \subseteq \bigcup F \implies A \subseteq B)$ $\forall A (A \subseteq B \implies A \in \mathscr P(B))$ From this we can conclude that $\forall A(A \in F \implies A \subseteq \bigcup \mathcal F \implies  A \subseteq B \implies  A \in \mathscr P(B))$ and thus $\forall A(A \in \mathcal F \implies A \in \mathscr P(B))$ . Therefore, $\mathcal F \subseteq \mathscr P(B)$ Is it correct? P.S Attempt to write more concise proof: Suppose $\bigcup F \subseteq B$ . Let $x$ be arbitrary set where $x \in \mathcal F$ . If $x \in \mathcal F$ then all of its elements will be in $\bigcup F$ , and since $\bigcup F \subseteq B$ , then $x \subseteq B$ . We know that $\mathscr P(B)$ is a power set, in other words , given arbitrary set A, $A \subseteq B \implies A \in \mathscr P(B)$ . Thus $ x \in \mathscr P(B)$ . Because $x$ was arbitrary, we can conclude that $\forall x(x \in \mathcal F \implies x \in \mathscr P(B))$ . Therefore, $\mathcal F  ⊆  \mathscr P(B)$","['elementary-set-theory', 'proof-writing', 'proof-verification']"
3303715,Find the domain and range of $f(x)=\frac{2}{x+1}$,"I am to find the domain and range of the function $f(x)=\frac{2}{x+1}$ . I can find the domain by ensuring the denominator is not 0: $$(-\infty,-1)\cup(-1,\infty),$$ because if $-1$ is an input, the denominator is $0$ . I know to exclude $-1$ because I don't want my denominator to ever $= 0$ : $$x+1=0,$$ i. e., $x=-1$ , so exclude $-1$ in the domain. So far so good and my textbook answers section confirms this is correct for the domain, all real numbers except $-1$ . Where I'm confused is for the range. The solution provided is $(-\infty,0)\cup(0,\infty)$ . So all values except $0$ . How was this arrived at?",['algebra-precalculus']
3303825,Questions about Formal Schemes,"Set $X$ be a scheme and $Y \subset X$ a closed subscheme given locally by ideal sheaf $I \subset \mathcal{O}_X$ . Then there exist formalism constructing from pair $(Y,I)$ the induced formal scheme $\hat{X}$ along $Y$ as follows: For affine $Spec(A) := U \subset X$ define $\widehat{U}_Y:= Spec \varprojlim_n A/I^n= \varinjlim_n Spec(A/I^n)$ . This gives $(\widehat{X},\mathcal{O}_{\widehat{X}})$ . Take into account that topologically $\widehat{X}=Y$ . Two questions: Locally, by construction the affine pieces of $\widehat{X}$ are completions with respect to the ideal $I$ . Why is it then that the stalks $\mathcal{O}_{\widehat{X},x}$ in general not complete? (see comment at page 1 from: https://www.uni-due.de/~mat903/sem/ws0809/material/Minicourse_FormalGeometry.pdf ) Could anybody tell me what is the philosophic meaning of this formal scheme and it's main application in Grothendieck's (generalized) algebraic geometry considering not more only analytic spaces/varieties but passing to general base scheme $X \to S$ ? My intuition is that one wants to study what happens ""locally"" in analytic sense (so with a topology which allows to stydy infinisesimal bahavior like in case of real of complex fields) since the Zariski topo is just to coarse. Could anybody sumarize (if my intuition is correct) the most important resuls from study of real/complex analytic spaces which can be ""transfered"" with this concept of formal scheme to (algebraic) algebraic geometry :) in appropriate way? I think that the goal mith be that if one have some strong theorems in classical analytic geometry (so study of analytic spaces in complex algebraic geometry) there might be possible to develop techniques as given in this ""formal scheme"" concept allows looking for a analog/similar statement for general schemes/ sheaves over arbitrary ring or field. Looking throught the linked paper above one nice example for such correspondent result is 2.3.3 Corollary:
Theorem on formal functions. Are there more?","['formal-completions', 'algebraic-geometry', 'schemes']"
3303829,Terence Tao–type books in other fields?,"I have looked at Tao's book on Measure Theory, and they are perhaps the best math books I have ever seen. Besides the extremely clear and motivated presentation, the main feature of the book is that there is no big list of exercises at the end of each chapter; the exercises are dispersed throughout the text, and they are actually critical in developing the theory. Question: What are some other math books written in this style, or other authors who write in this way? I am open to any fields of math, since I will use this question in the future as a reference. That was the question; the following is just why I think Tao's style is so great. When you come to an exercise, you know that you are ready for it. There is no doubt in the back of your mind that ""maybe I haven't read enough of the chapter to solve this exercise"" Similarly, there is no bad feeling of ""maybe I wasn't supposed to use this more advanced theorem for this exercise, maybe I was supposed to do it from the basic definitions but I can't"". It makes everything feel ""fair game"" It makes it difficult to be a passive reader It makes you become invested in the development of the theory, as if you are living back in 1900 and trying to develop this stuff for the first time I think you can achieve a similar effect with almost any other book, if you try to prove every theorem by yourself before you read the proof and stuff like that,  but at least for me there are some severe psychological barriers that prevent me from doing that. For example, if I try to prove a theorem without reading the proof, I always have the doubt that ""this proof may be too hard, it would not be expected of the reader to come up with this proof"". In Tao's book, the proofs are conciously left to you, so you know that you can do it, which is a big encouragement.","['reference-request', 'complex-analysis', 'real-analysis', 'abstract-algebra', 'soft-question']"
3303879,"System of functional equations $ f(f(x)-f(y))=|f(x)-f(y)|, \; f(1-f(x+2))=1-f(x)$","Find all continuous functions $f: \mathbb{R} \to \mathbb{R}$ such that $$\begin{equation} \begin{cases} f(f(x)-f(y))=|f(x)-f(y)|, \\ f(1-f(x+2))=1-f(x) \end{cases} \end{equation}$$ for all $x, \; y \in \mathbb{R}$ . My work . $1)$ $x=y \Rightarrow f(0)=0$ . $2)$ $x=0 \Rightarrow f(1-f(2))=1$ . $3)$ If $x=1-f(2), \; y=0$ then from $ f(f(x)-f(y))=|f(x)-f(y)|$ it follows that $f(1)=1$ . $4)$ $x=1 \Rightarrow f(1-f(y))=|1-f(y)|$ . If $y=x+2$ then from $ f(1-f(y))=|1-f(y)|$ it follows that $ f(1-f(x+2))=|1-f(x+2)|$ . But $ f(1-f(x+2))=1-f(x)$ . Then $1-f(x)=|1-f(x+2)|$ . Then $f(x) \le 1$ . Then $f(x+2) \le 1$ . Then $1-f(x)=1-f(x+2) \Rightarrow f(x)=f(x+2)$ . Then the function $f: \mathbb{R} \to \mathbb{R}$ is periodic with period $2$ .","['functional-equations', 'functions']"
3303886,Pushout in $\mathbf {Set}$ - proving that the unique map is well-defined,"Consider the diagram below: The pushout of the corner is $C=(Y\sqcup Z)/\sim$ where $$Y\sqcup Z=\{(y,\star):y\in Y\}\cup\{(\star,z):z\in Z\}$$ and $\sim$ is the equivalence relation generated by $$R=\{((s(x),\star),(\star,t(x))):x\in X\}$$ together with projection maps $p_1:Y\to C,y\mapsto[(y,\star)]$ and $p_2:Z\to C,z\mapsto [(\star,z)]$ . It is clear that if $\cdot$ in the diagram above is replaced by $C$ , then the diagram commutes. Suppose $f_1:Y\to A$ and $f_2:Z\to A$ be arrows such that if $\cdot$ is replaced with $A$ then the diagram also commutes. To prove that $C$ is a colimit, we must prove that there is a unique map $\phi:C\to A$ with $\phi p_i=f_i$ . Define $$\phi([c]) = \begin{cases} f_1(y) & \text{ if } c=(y,\star) \\
                     f_2(z) & \text { if } c=(\star,z)\end{cases}$$ As long as this map is well defined, it is the unique map that makes the triangles commute, by construction. I'm confused about the proof that it's well-defined. The idea should be the same as in this answer but in our case $\phi$ has definition by cases, which confuses me. I'm not even sure how to define the equivalence relation referred to in the answer cited because the elements of $Y\sqcup Z$ are not of the same form, so I'm not sure how to say $w\approx w'$ iff $f_1(...)=f_2(...)$ -- each of $w,w'$ is either of the form $(y,\star)$ or $(\star,z)$ . In particular, this is a question about notation - I don't know how to organize it properly.","['equivalence-relations', 'relations', 'category-theory', 'notation', 'elementary-set-theory']"
3303911,How many equivalence classes does the following equivalence relation have?,"Let $S \subseteq \mathbb{Z}$ , and define a relation $R$ on $S \times S$ by $$(m, n)R(s, t) \quad \text{ if and only if } \quad m + n = s + t$$ Consider the set $S = \{1, 2, 3, 4, 5, 6\}$ . How many equivalence
  classes does $R$ have? Describe the equivalence classes of $R$ without explicitly listing the partition of $S × S$ . My try:
I have proved that the relation is an equivalence relation, $R$ is reflexive, symmetric, and transitive. For the set $S$ there are $2^6$ subsets.
Any help starting this problem would be appreciated!","['elementary-set-theory', 'equivalence-relations', 'discrete-mathematics']"
3303915,$x +\frac 1 x\leq -2$ for $x\leq 0$ How do I prove this statement using algebra?,"$$(x + \frac{1}{x})\geq 2 \; \text{for} \; x>0 \tag{1}$$ $$(x + \frac{1}{x})\leq -2 \; \text{for} \; x<0 \tag{2}$$ I am looking for a proof that uses algebra, just algebra. I can prove it using the concepts of maxima & minima, and double derivative. But I’m looking for an algebraic proof. Here’s what I did : $($$\sqrt x$ - $\frac{1}{\sqrt x}$$)^{2}$ = $($$x$ + $\frac{1}{x}$$)$ - $2$ From here, it’s obvious that $x$ + $\frac{1}{x}$ is greater than or equal to $2$ for $x$ > $0$ The problem is, I can't prove $($$2$$)$ using this method, because $\sqrt x$ can’t take negative values. How do I prove statement $($$2$$)$ using algebra?","['proof-explanation', 'algebra-precalculus', 'quadratics']"
3303967,Expectation of $\log^2(X)$,"Suppose one has a continuous random variable $X \in (0, 1)$ and one wants to compute $E[\log(X)\log(X)]$ . Suppose further that one also knows that $E[X] < \infty$ and $-\infty < E[\log(X)] < \infty$ , where all expectations are taken with respect to a measure $\Pi$ . Is it possible to claim that $E[\log(X)\log(X)] < \infty$ ? I have tried a few things, namely trying to get an integrable upper bound on $\psi(x) = \log(x)\log(x)$ , but could not find much in that direction. Also found this really nice paper that provides a bound on the expectation of an increasing convex function $\phi$ [see Theorem 2.1 therein], which unfortunately does not seem useful because $\psi$ is convex but decreasing . Edits I just realised that for $0 < x \leq 1$ , the usual bounds on $\log(x)$ can be manipulated to get $$ (x-1)\log(x) \leq \log(x)\log(x) \leq \log(x) - \log(x)/x $$ So if one could show that $-\infty < E[\log(X)/X] < \infty$ , I guess we'd be done. In fact the claim is FALSE in general, as demonstrated quite neatly by @pre-kidney in their answer. Using the bound above and the substitution $Y = -\log(X)$ one has $$ Y^2 \leq Ye^Y - Y, $$ but it is possible to have $E[Ye^Y] = \infty$ .","['integration', 'probability-theory', 'upper-lower-bounds']"
3303971,Is it possible to compute this sum faster than the naive search,"I was trying to find a solution for calculating this faster than the naive method, but I couldn't make any bigger steps. Namely, for each sequence $V$ of length $8$ , such that $1 \leq V_i \leq D, 0 \leq i \leq 7$ We need to find the sum $$\sum f(V_0, V_1) f(V_0, V_3) f(V_0, V_4) f(V_1, V_5) f(V_1, V_2) f(V_2, V_3) f(V_2, V_6) f(V_6, V_5) f(V_6, V_7) f(V_7, V_4) f(V_4, V_5) f(V_7, V_3)$$ Where $f(i, j)$ is value given in input matrix. I coded program to calculate this but it seems that as $D$ is becoming larger than $6$ the program is very slow, so I was thinking if there is some mathematical method to optimize this sum. I tried rewriting each element of the sum separately but it didn't really work.","['summation', 'discrete-mathematics']"
3303984,$\mathbb{R}P^n$ can't be submanifold of $\mathbb{R}^n$,"I need to prove that can't exist a function $f:\mathbb{R}P^n \rightarrow \mathbb{R}^n$ such that $(\mathbb{R}P^n,f)$ is a submanifold of $\mathbb{R}^n$ . I can prove that for the case of $n$ even because if such $f$ exist, we have that $df_p$ is an isomorphism $\forall p \in \mathbb{R}P^n$ (because $\mathbb{R}P^n$ and $\mathbb{R}^n$ have the same dimension), then for the inverse function theorem have that $f$ is a local diffeomorphism, wich is an contradiction from the fact that $\mathbb{R}^n$ is orientable and $\mathbb{R}P^n$ is not orientable if $n$ even. I think that is possible to do for any $n$ thinking in the fact that $\mathbb{R}P^n$ is compact and then $f$ has to be an embedding.","['compact-manifolds', 'manifolds', 'smooth-manifolds', 'differential-geometry']"
3303986,Does connection of vector bundle always take values in Lie Algebra,"It is true that if connection $\omega$ in a vector bundle is $\mathfrak{g}$ -valued ( $\mathfrak{g}$ being Lie Algebra of the structure Lie Group $G$ ) in a patch $U$ , then it will be $\mathfrak{g}$ -valued in all other patches due to the transformation law: \begin{align}
\omega_V = c^{-1}_{UV} \omega_U c_{UV} + c^{-1}_{UV} d c_{UV}.
\end{align} However, I'm trying to understand if it must take values in Lie algebra in order to be a connection?","['vector-bundles', 'lie-algebras', 'differential-geometry']"
3304002,Name of curve drawn on cylinder using compass,"What is the name of the ""circle"" drawn on a cylinder('s curved) surface with a compass? If we unroll the cylinder, we get a slightly elongated circle. Is there anything known about this type of curve? This discussion is all I could find about it: https://uk.comsol.com/forum/thread/149872/how-to-draw-a-circle-on-a-cylinder-surface","['euclidean-geometry', 'circles', 'geometry']"
3304006,Proof from From Visual Complex Functions,"From Visual Complex Functions with Phase Portraits by Wegert (author).
From the book. I am stuck on the proof of the theorem.
I have indicated by ( ͡° ͜ʖ ͡°) where in the proof I start to get lost. The definition for $q$ confuses me too.
Here's the theorem that's being proved. Theorem 3.2.9. If $f$ is analytic at $z_{0}$ and $f(z_{0}) \neq 0$ ,  then $1/f$ is analytic at $z_{0}.$ The Taylor coefficients $b_{k}$ of $1/f$ at $z_{0}$ can be computed recursively from the Taylor coefficients $a_{k}$ of $f$ by $b_{0} :=1/a_{0}$ and $b_{k} :=-\displaystyle \frac{1}{a_{0}}(a_{1}b_{k-1}+a_{2}b_{k-2}+\ldots+a_{k}b_{0})$ , $k=1$ , 2, . . . .   (3.37) The proof makes use of the following lemma. Lemma 3.2.2 (Abel-Weierstrass). Let $R$ be the radius of convergence of the power series (3.16). (i)  If $0\leq r< R$ ,   then there exists a constant $c$ such that for all $k\in\mathbb{N}$ $r^{k}| a_{k}|\ \leq c$ .   (3.22) (ii)   If there exist positive numbers $r$ and $c$ such that  (3.22)  holds for all sufficiently large $k\in\mathbb{N}$ ,  then $R\geq r.$ The inequality (3.22) is also known as  Cauchy's estimate.
Please explain from the line to the end. Here is the Actual steps in the proof:-- Proof. 1. In the first step we assume that the function $1/f$ is analytic at $z_{0}$ . Then
its Taylor series $\displaystyle \frac{1}{f(z)}=b_{0}+b_{1}(z-z_{0})+b_{2}(z-z_{0})^{2}+\ldots+b_{k}(z-z_{0})^{k}+\ldots$ (3.38)
converges in a neighborhood of $z_{0}$ and its Cauchy product with the Taylor series of $f$ is the constant function 1. The latter is equivalent to the infinite system of equations \begin{aligned}
a_{0}b_{0}=1  \\
a_{0}b_{1}+a_{1}b_{0}=0 \\
a_{0}b_{2}+a_{1}b_{1}+a_{2}b_{0}=0
\end{aligned} Since $a_{0}\neq 0$ , this triangular system can be solved with respect to the coefficients $b_{k}$ , which yields the recursion (3.37). It remains to prove that the series (3.38), with coefficients $b_{k}$ given by the recursion (3.37), indeed has a positive radius of convergence. By Cauchy's estimate (3.22) in Lemma 3.2.2, there are positive numbers $c$ and $r$ such that $|a_{n}| \leq cr^{-n}$ for all $n\in \mathbb{N}$ . We set $q :=1+c/|a_{0}|$ and show that ( ͡° ͜ʖ ͡°) $|b_{n}| \displaystyle \leq\frac{c}{|a_{0}|^{2}}\frac{q^{n-1}}{r^{n}},n=$ 1, 2, . . . .   (3.39)
For $n=1$ we have $b_{1} =-a_{1}/a_{0}^{2}$ and $|a_{1}| \leq c/r$ , so that indeed $$
|b_{1}|=\frac{a_{1}}{a_{0}^{2}}\leq\ \frac{c}{|a_{0}|^{2}}\frac{1}{r}.
$$ Now we assume that (3.39) holds for all $n=1$ , 2, . . . , $k-1$ and consider the case where $n=k$ . Using $|b_{0}| = 1/|a_{0}|$ , the recursive definition of $b_{k}$ , and the triangle inequality, we estimate $$
|b_{k}|\leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}|a_{k-j}||b_{j}|)
$$ $$
\leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}\frac{c}{r^{k-j}}\frac{c}{r^{j}|a_{0}|^{2}}q^{j-1})
$$ $$
\leq\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\ \sum_{j=0}^{k-2}q^{j})
$$ $$
=\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\frac{q^{k-1}-1}{q-1})\ =\frac{c}{r^{k}|a_{0}|^{2}}q^{k-1},
$$ which gives (3.39) for $n= k$ and thus for all $n$ . Consequently, by Lemma 3.2.2, the power series (3.38) has radius of convergence not less than $r/q. \square $ Since posting I looked hard at it and it has suddenly all opened up to me like a flower bud tighly closed now fully open. I understand.","['complex-analysis', 'power-series']"
3304027,Theoretical limit on Random Forest performance,"If I run a Random Forest on a set of data and get an accuracy of let's say 85% and I want to produce better results, I could just increase the amount of decision trees I use. Lets say I Increase the amount of trees I use and now I get an accuracy of 94%. If theoretically I used an infinite amount of trees would I get a perfect accuracy of 1 or as we increase the amount of trees is there a limit to how accurate we can become as we converge to a certain accuracy (like 98.76%). Of course there are many different factors to this but I think there should be a limit ""n amount of trees"" to where adding more wouldn't increase our accuracy or increase it by such a small amount that it doesn't matter anymore.",['statistics']
3304038,Matrix representation when the vector space is infinite dimensional,"How do we represent a linear map when the vector space is infinite dimensional? Will the matrix itself become infinite dimensional as well? Say I consider linear map $T:\mathbb{F}[X]\to \mathbb{F}[X]$ as $T(f(x))=xf(x)$ , which is obviously infinite dimensional. How can I construct such matrix with respect to the basis $\{1, x, x^2,...\}$ ? Will this matrix be infinite dimensional with $1$ 's below the main diagonal and other entries just $0$ ?","['matrices', 'linear-algebra']"
3304048,Extended convex function - continuity,"I am dealing with a problem concerning a convex function defined on $\mathbb{R}^d$ and taking values on $\mathbb{R}\cup\{+\infty\}$ . I would like to use in my argument that such a convex function is $\lambda^d$ -almost everywhere continuous, but I do not know if this is a valid statement. Does anybody of you know a theorem or a reference which contains this claim??
Thank you in advance !","['convex-analysis', 'analysis']"
3304082,Find the Radius of Convergence of a Series,"Consider the function $f(z)$ defined by the series $$f(z):=\sum_{n=0}^{\infty}e^{-nz}\cos (nz).$$ I am asked to find the domain of $f$ defined by the convergence of the series. To find the radius of the convergence, we need to firstly transform the summand into the form of power series. So I simply wrote down the formula as: \begin{align*}
e^{-nz}\cos(nz)&=\Big(\sum_{k=0}^{\infty}\dfrac{(-nz)^{k}}{k!}\Big)\Big(\sum_{\ell=0}^{\infty}\dfrac{(-1)^{\ell}}{(2\ell)!}(nz)^{2\ell}\Big)\\
&=\Big(\sum_{k=0}^{\infty}\dfrac{(-1)^{k}n^{k}}{k!}z^{k}\Big)\Big(\sum_{\ell=0}^{\infty}\dfrac{(-1)^{\ell}n^{2\ell}}{(2\ell)!}z^{2\ell}\Big)\\
&:=\Big(\sum_{k=0}^{\infty}a_{k}z^{k}\Big)\Big(\sum_{\ell=0}^{\infty}b_{\ell}z^{2\ell}\Big)\\
&=\sum_{m=0}^{\infty}\Big(\sum_{\substack{k+2\ell=m \\ k,\ell\geq 0}}a_{k}b_{\ell}\Big)z^{m}.
\end{align*} But then I don't know how to proceed, as then if I sum over $n$ , the coefficients are still not clear enough to calculate the radius of convergence. Did I head to a wrong direction? Since this question is related to complex analysis and power series, after searching around the forum, I am still not sure if I've asked a duplicated question. If so, please point it out and I will close the question. Thank you! Edit 1: Given the discussions under my post and two answers, here is some editions here. (1) I am sorry for forgetting to mention that $z\in\mathbb{C}$ . Thus, it seems not sufficient to argue only for $z\in\mathbb{R}$ . (2) This problem has a follow-up part (b) asking me to argue if $f(z)$ can be extended analytically to a larger domain. If so, I need to find the maximal domain of the extension and classify its singularities. Therefore, I think we need to find a power series expansion of $f(z)$ and then ""return"" it to a function, just like what Viktor Glombik suggested. I am currently trying to show what Viktor Glombik could not prove, and I would post my proof here if I came up one. Thank all of you so much for your discussion and answers! Edit 2: Following the idea of Viktor Glombik, I think I prove the whole problem now. In this edition, I will first post the whole problem and then post my proof. The proof greatly follows Viktor Glombik. Consider the function $f(z)$ defined by the series $$f(z):=\sum_{n=0}^{\infty}e^{-nz}\cos(nz).$$ (1) Find the domain of $f$ defined by the convergence of the series. (2) Can $f$ be extended analytically to a larger domain? If so, find the maximal domain of the extension and classify its singularities. Proof (1): Note that $$\cos (nz)=\dfrac{e^{inz}+e^{-inz}}{2},$$ so that we have $$\sum_{n=0}^{\infty}e^{-nz}\cos(nz)=\dfrac{1}{2}\sum_{n=0}^{\infty}e^{(i-1)nz}+\dfrac{1}{2}\sum_{n=0}^{\infty}e^{-(i+1)nz}.$$ Both sums are geometric sum with $$r_{1}=e^{(i-1)z},\ r_{2}=e^{-(i+1)z},$$ which converges if and only if $$|r_{1}|<1,\ \text{and}\ |r_{2}|<1.$$ Write $z:=x+iy$ for $x,y\in\mathbb{R}$ , then we have $$|r_{1}|=\Big|e^{i(x-y)}e^{-y-x}\Big|=\Big|e^{-(y+x)}\Big|=e^{-(y+x)},$$ and $$|r_{2}|=\Big|e^{i(-x-y)}e^{y-x}\Big|=\Big|e^{y-x}\Big|=e^{y-x}.$$ Then, we have $$|r_{1}|<1\implies e^{y+x}>1\implies y+x>0\implies y>-x,$$ $$|r_{2}|<1\implies e^{y-x}<1\implies y-x<0\implies y<x.$$ Thus, the domain of the convergence is the set $$\mathcal{A}:=\{z\in\mathbb{C}:-\Re(z)<\Im(z)<\Re(z)\}.$$ Proof (2): Now, define the partial sums $$S_{1,k}:=\dfrac{1}{2}\sum_{n=0}^{k}r_{1}^{n},\ S_{2,k}:=\dfrac{1}{2}\sum_{n=0}^{k}r_{2}^{n}.$$ Then, by the summation formula of geometric series, we have $$S_{1,k}=\dfrac{1}{2}\times\dfrac{1-r_{1}^{k+1}}{1-r_{1}},\ S_{2,k}=\dfrac{1}{2}\times\dfrac{1-r_{2}^{k+1}}{1-r_{2}}.$$ Now, for all $z\in\mathcal{A}$ , taking $k\rightarrow\infty$ yields us $$f(z)=\dfrac{1}{2}\Big(\dfrac{1}{1-e^{(i-1)z}}\Big)+\dfrac{1}{2}\Big(\dfrac{1}{1-e^{-(i+1)z}}\Big).$$ It is then clear that the singularities are those $z\in\mathbb{C}$ that make \begin{align*}
&e^{(i-1)z}=1,\ \text{or}\ e^{-(i+1)z}=1 \\
\iff &(i-1)z=2k\pi i,\ \text{or}\ (i+1)z=-2k\pi i,\ k\in\mathbb{Z}.\\
\iff &z=(1-i)k\pi,\ \text{or}\ z=-(1+i)k\pi,\ k\in\mathbb{Z}.
\end{align*} Therefore, for all $z\in\mathbb{C}\setminus\Big(\{z=(1-i)k\pi,\ k\in\mathbb{Z}\}\cup\{z=-(1+i)k\pi,\ k\in\mathbb{Z}\}\Big),$ $$f(z):=\dfrac{1}{2}\Big(\dfrac{1}{1-e^{(i-1)z}}\Big)+\dfrac{1}{2}\Big(\dfrac{1}{1-e^{-(i+1)z}}\Big)$$ is an analytic continuation of $f(z)$ . I think I did not understand the point of this question before because I thought it was asking me to give the radius of the convergence, but it was actually asking me to simply give a domain. Also, the idea of using geometric series is brilliant, credit to Viktor.","['complex-analysis', 'power-series']"
3304116,Curves in $\overline{\Bbb{F}}_p$,"Let $k$ an algebraically closed field, for example $k = \Bbb{C}$ or $k = \overline{\Bbb{F}}_p$ , $a \in k$ , $k(x)$ the field of rational functions, fix an algebraic closure $\overline{k(x)}$ , let $R_a$ be a subring of $\overline{k(x)}$ which is henselian at $a$ , that is $(x-a)$ is a maximal ideal and for a polynomial $P \in R_a[T]$ ,   if $P(c) = 0 \bmod (x-a), P'(c) \ne 0 \bmod (x-a)$ then $c$ lifts to a unique root $\in R_a$ of $P$ . Concretely one can start from a realization of $\overline{k(x)}$ as Puiseux series at $a$ and make a subring $R_a$ from the elements whose Puiseux series is a power series. Pick another such subring $R_b\subset \overline{k(x)}$ , then $R=R_a \cap R_b$ can be thought as an abstract curve $\gamma : a \to b \subset k$ . For $f \in R $ , from the reductions $\bmod (x-a)^n$ , we have the expansions $$f = \sum_{n \ge 0} c_n (x-a)^n \in \varprojlim R/(x-a)^n\\ f = \sum_{n \ge 0} d_n (x-b)^n \in \varprojlim R/(x-b)^n $$ and the map $\sum_{n \ge 0} c_n (x-a)^n \mapsto \sum_{n \ge 0} d_n (x-b)^n $ is the analytic continuation along $\gamma$ . Questions : Does this construction of abstract curves works, is there a better one, how to define closed loops $a\to a$ , can we obtain a group from them, can we get homotopy classes of curves when restricting to a ring (which one ?) smaller than $\overline{k(x)}$ , is it possible to give examples of abstract curves in $\overline{\Bbb{F}}_p$ ? In $\Bbb{C}$ we don't have only the curves in the complex topology, but also things like this : let $\gamma_n$ be the circle of radius $r \to 1^+$ traversed $n$ times, then analytic continuation along $\gamma_n$ is well-defined for any algebraic function analytic at $z=1$ , and $\gamma_n$ should be an abstract curve even for $n \in \hat{\Bbb{Z}}$ (the profinite integers). We should have an isomorphism between such closed loops $1 \to 1$ in $\Bbb{C}$ and $Gal(\overline{\Bbb{C}(x)}/\Bbb{C}(x))$ .","['recreational-mathematics', 'algebraic-geometry']"
3304165,Does Gamma function a solution for known Ordinary differential equation?,"It is well known that gamma function's defined as : $$\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds}$$ and it is divergent for $x<0$ . , Really I ask about differential equation which Gamma function satisfying it or by Other way : Does Gamma function a solution for known Ordinary differential equation and if yes what is it  ? For example if it obeyed any form of $F( \Gamma, \Gamma ', \dots, \Gamma^{(k)}) = 0$ ?","['gamma-function', 'ordinary-differential-equations']"
3304225,Possible mistake in Apostol calculus: integration of the logarithm function,"The exercise set 6.9 of calculus I Apostol lists the following to find the antiderivative: $$a) \int \log^2(x)\;dx$$ The solution in the back of the book is: $$b)\; x\log^2|x| - 2x\log|x| + 2x + C$$ I think that the answer should be the same, except all $\log|x| \rightarrow \log(x)$ . By definition, we have $\log(x)$ and not $\log|x|$ . It means that the logarithm $\log(x)$ under integration cannot accept any negative term. Hence, its antiderivative should not accept it either, because the derivative of (b) is $\log^2|x|$ , which is clearly different to me than $\log^2(x)$ . Is it a mistake in Apostol, or I do not understand something about the domains here?","['calculus', 'functions', 'logarithms']"
3304227,On the inclusion homomorphism $\mathbb Z\to\mathbb Q$ [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 years ago . Improve this question This question is about the group homomorphism $i: \mathbb Z\to\mathbb Q$ given by $m\mapsto m$ . 1) Is the following proof of the fact that the arrow $i$ in $\mathbf {Ab}$ is  epic correct? Let $h,h':\mathbb Q\to G$ be group homomorphisms. Suppose $h\circ i=h'\circ i$ . Then for all $n$ , $h(i(n))=h'(i(n))$ , i.e., $h(n)=h'(n)$ since $i(n)=n$ . Thus $i$ is epic. 2) The group homomorphism $i: \mathbb Z\to\mathbb Q$ given by $m\mapsto m$ is known to be a monic that is not split in $\mathbf {Ab}$ . Why is that? Assume that it is split monic. Then there is a group homo $l:\mathbb Q\to \mathbb Z$ such that $l\circ i=1_\mathbb Z$ . What does this contradict to? For any $m\in \mathbb Z$ this says that $l(i(m))=m$ , i.e., $l(m)=m$ . So if $l$ exists, then it $l\restriction_\mathbb Z:\mathbb Z\to \mathbb Z$ must be given by $m\mapsto m$ . So I guess the statement that needs to be proved is that there does not exist a group homomorphism $l:\mathbb Q\to\mathbb Z$ such that $l(m)=m$ for all $m\in\mathbb Z$ . How to show that?","['group-homomorphism', 'category-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
3304258,Evaluate $\lim\limits_{x \to { \infty } } (\frac{x}{x+2})^x$. Need for explanation.,"My question is about the procedure for this limit problem: $$\lim\limits_{x \to  { \infty } } (\frac{x}{x+2})^x$$ My solution was like that: $$(\frac{x}{x+2})^x=e^{x\ln\frac{x}{x+2}}
= e^u$$ with $\ u = x \ln(\frac{x}{x+2})$ . Then $$\lim\limits_{x \to  { \infty } } x\ln(\frac{x}{x+2})
=\lim\limits_{x \to  { \infty } }{\ln{x\over x+2}\over {1\over x}}$$ Applying L'Hôpital's rule: $$\lim\limits_{x \to  { \infty } } -{{2\over x(x+2)}\over 
 {1\over x^2}} = \lim\limits_{x \to  { \infty } } -{2x\over x+2} = -2 $$ $$\lim\limits_{x \to  { \infty } } u = -2 $$ ∴ $\lim\limits_{u \to  { \ -2 } } e^{u} = e^{-2} = {1\over e^{2}} $ However, according to my answer sheet, the correct answer is $e^{2}$ . So, Please I need to know where's my mistake here. Thank you.","['limits', 'proof-verification', 'exponential-function', 'analysis']"
3304265,Epimorphisms in the category induced by a partially ordered set,"This comment says that in the category of a partially ordered set, every arrow is an epi but no non-identity arrow has a right inverse. My understanding is that the category in question is one where there is at most one arrow between any two objects, and there is an arrow $a\to b$ iff $a\le b$ . I'm having a trouble verifying the above claim (at least its second part). For the first claim. Consider an arrow $f:a\to b$ . To show it's an epi, we need to show that $hf=h'f\implies h=h'$ for all arrows $h,h':b\to c$ . Well, suppose $h,h':b\to c$ are arrows such that $hf=h'f$ . Since there is at most one arrow between $b$ and $c$ , $h=h'$ . This completes the proof. The assumption $hf=h'f$ is not even needed, right? For the second claim. Suppose $f:a\to b$ is a non-identity arrow (this means $a\le b$ ). Assume it has a right inverse $r:b\to a$ (i.e., $b\le a$ ) for which $fr=1_b$ holds. What does it contradict to? I don't quite understand what statement $fr=1_b$ means in the language of $\le$ .","['elementary-set-theory', 'order-theory', 'category-theory']"
3304316,Inclusion of dual space: $X\subset Y \implies Y^*\subset X^*$,"I'm confused at dual space of Hilbert space. Perhaps it is easy, but I don't know, 
please help me. Let $X,$ $Y$ be Hilbert spaces and $X^\ast$ , $Y^\ast$ are the dual spaces of $X$ and $Y$ respectively. Suppose that $X\subset Y$ . Then, $Y^\ast\subset X^\ast$ . This is correct according to books about functional analysis. But, I have no idea to confirm this inclusion. I think $X^\ast\subset Y^\ast$ , because the domain of $Y^\ast$ is bigger than the one of $X^\ast$ . Why this idea is incorrect?","['hilbert-spaces', 'functional-analysis', 'dual-spaces']"
3304318,Maximal ideal and not algebraically closed field,"This is a question concerning maximal ideals in a polynomial ring over a non-algebraically closed field k. First is the example inspired for this question: as a standard exercise, it is easy to show that $\langle x^2+1\rangle$ is a maximal ideal in $\mathbb{R}[x]$ (Briefly speaking, let $J$ be an ideal s.t. $I\subset J$ and take $f\in J\setminus I$ , divide $f$ by $x^2+1$ to obtain $f(x)=q(x)(x^2+1)+(ax+b)$ , then one can show that $a^2+b^2\in J$ (see here , for example), so $J=\mathbb{R}[x]$ ) Next, consider the ideal $\langle x_1^2+1, x_2,\cdots ,x_n\rangle \subseteq \mathbb{R}[x_1,\cdots ,x_n]$ . This is a maximal ideal (use the exact same argument above) Now, we can generalize things. If $k$ is non-algebraically closed field, we should be able to construct a maximal ideal in $k[x_1,\cdots ,x_n]$ using the logic above. Namely, pick $f\in k[x_1]$ s.t. it has no root in $k$ , then consider the ideal $\langle f,x_2,\cdots ,x_n\rangle$ . Our inspiration tells us that this ideal should be maximal. However, I have a hard time proving it. As before, let $J$ be an ideal s.t. $I\subset J$ and take $g\in J\setminus I$ . Using the multivariable division algorithm to divide $g$ by $(f,x_1,\cdots ,x_n)$ , we obtain $g=q_1f+q_2x_2+\cdots +q_nx_n+r$ with $r\in k[x_1]$ . A simple observation shows that $r\in J$ . But the biggest problem is I don't see any nice trick to use to show that $J$ contain some nonzero constant (and hence $J=k[x_1,\cdots ,x_n]$ ) Any idea?","['algebraic-geometry', 'maximal-and-prime-ideals', 'multivariate-polynomial']"
3304337,Maximum and minimum value of absolute value function.,"Question : let $f$ be function defined on some domain $D$ and let $J\subseteq D$ then is $$\max_\limits {x\in J}|f(x)|=\max(|\max_\limits {x\in J}f(x)|, |\min_\limits {x\in J}f(x)|)$$ When I consider continuous functions like $\sin\ t$ and $\cos\ t$ and $J$ to be compact subset of $\mathbb{R}$ then I saw above holds. Is the above formula holds in general? I am not able find to find the counter examples. Please help.. Further how to find maximum and minimum values of absolute value function (can we apply second derivative test? But, absolute value function like $|f(x)|$ is not differentiable at points where $f(x)=0$ ).","['maxima-minima', 'functions', 'absolute-value', 'real-analysis']"
3304338,"Find the dimension of $V(y-x^2,y^2-y+z^2)$","Definition: Let $X$ be an algebraic set and let $K(X)$ denote the rational function field of $X$ . If $X$ is irreducible, then the dimension of $X$ is defined to be the transcendence degree of $K(X)$ over $F$ . If $X$ is reducible, then the dimension of $X$ is defined to be the maximum dimension of its irreducible components. Question: Suppose $F$ is algebraically closed. I want to find the dimension of the algebraic set $X=V(y-x^2,y^2-y+z^2)$ . Attempt: According to the definition, the first step should be determine whether $X$ is irreducible or not. I have tried to find a polynomial isomorphism between the field $F$ and $X$ but that didn't work. So I thought maybe I should try a rational map. Say $x=\frac{1-t^2}{1+t^2}$ , $y=x^2=\frac{1-t^2}{1+t^2}$ and $z=x \frac{2t}{1+t^2}=\frac{(1-t^2)(2t)}{1+t^2}$ . But is this rational map an isomorphism? If it is, then I will get $\dim X=\dim F=1$ . Can anyone help me with this? Thank you !","['algebraic-curves', 'algebraic-geometry', 'abstract-algebra']"
3304339,"Let $a$ and $b$ commute. If $m$ and $n$ are relatively prime, then ord($ab$) = $mn$. [duplicate]","This question already has answers here : $\operatorname{ord}(u)=r,\,\operatorname{ord}(v)=s$ $\,\Rightarrow\,\operatorname{ord}(uv)=rs\,$ if $\,r,s\,$ (co)primes (3 answers) Closed 4 years ago . This is exercise $10.E.4$ from Pinter: Let $a$ and $b$ be elements of a group $G$ . Let ord( $a$ ) = $m$ and ord( $b$ ) = $n$ . Prove: Let $a$ and $b$ commute. If $m$ and $n$ are relatively prime, then ord( $ab$ ) = $mn$ . (HINT: Use $10.E.2$ .) Here is $10.E.2$ , which Pinter suggests that we use: If $m$ and $n$ are relatively prime, then no power of $a$ can be equal to any power of $b$ (except for $e$ ). I'm also going to use $10.E.1$ : If $a$ and $b$ commute, then ord( $ab$ ) is a divisor of lcm( $m$ , $n$ ). As well as $B.T6.i$ (Theorem $6.i$ from Appendix $B$ . A fact from basic number theory.): If   gcd( $m$ , $n$ ) = 1   then   lcm( $m$ , $n$ ) = $mn$ Let's begin. We are given that $m$ and $n$ are relatively prime which means that: $$\text{gcd}(m,n) = 1$$ By $B.T6.i$ : $$  \text{lcm}(m,n) = mn \tag{1}  $$ By $10.E.1$ : $$ \text{ord}(ab)\ |\ \text{lcm}(m,n) $$ Substituting $(1)$ : $$ \text{ord}(ab)\ |\ mn $$ Which means that there is an integer $x$ such that: $$ \text{ord}(ab) x = mn $$ This is so close! For the theorem to be true, we'd have to show that $x = 1$ . However, the original exercise statement says to use $10.E.2$ . Is that helpful in showing that $x = 1$ ? Or is there some other completely different approach whereby $10.E.2$ is used? UPDATE : Some comments regarding the answer below. The proof uses the following fact: If $a|c$ and $b|c$ then $lcm(a,b)|c$ . For this exercise, I wanted to only use theorems that had been presented in the book up to that point. And, I didn't seem to recall seeing this theorem. (If anyone spots this in Pinter, please comment below with the location.) However, I did notice that the following similar fact is in Appendix B (REVIEW OF THE INTEGERS) as exercise B.9: If $a|c$ and $b|c$ and $gcd(a,b) = 1$ then $ab|c$ . So yeah, it looks like the approach shown below is definitely a valid way to go if you want to stick to what's presented in the book.","['group-theory', 'abstract-algebra']"
3304408,Property about bijection $f:\mathbb{R \backslash Z} \to \mathbb{R \backslash Z} $,"Could someone help me with the following question? Let $f:\mathbb{R \backslash Z} \to \mathbb{R \backslash Z} $ be a continuous bijection. It is true that for any integer $n$ there exists another integer $m$ such that $ f(]n,n+1[)=]m,m+1[$ ? My attempt:
By using that $f$ is continuos we obtain that $f(]n,n+1[)$ must be a connected set of $\mathbb{R \backslash Z}$ , that is, there eists an integer $m$ such that $f(]n,n+1[)$ is an open interval contained in $]m,m+1[$ . I ve not been able to prove the contrary inclusion. It seems to me that the proof is very simple but I have not found it. Thanks.","['general-topology', 'real-analysis']"
3304415,Catalan numbers - algebraic proof of the recurrence relation,"I would like to prove following recursive relation for the Catalan numbers: $$\tag{1}
C_0=1,\quad C_n=\sum_{i=0}^{n-1}C_iC_{n-i-1}\text{, for }n\ge 1
$$ without combinatoric arguments, only algebraically; and no generating function. Starting point: $$\tag{2}
C_n:=\frac{1}{n+1}\binom{2n}{n}.
$$ The following recursion can be also used (already proved): $$\tag{3}
C_0=1,\quad C_n=\frac{2(2n-1)}{n+1}C_{n-1}\text{, for }n\ge 1
$$ Maybe the identities for Binomial coefficients (wikipedia) are useful. In particular the Chu–Vandermonde identity, $$\tag{4a}
\sum _{j=0}^{k}{\binom {m}{j}}{\binom {n-m}{k-j}}={\binom {n}{k}}
$$ or $$\tag{4b}
\sum _{m=0}^{n}{\binom {m}{j}}{\binom {n-m}{k-j}}={\binom {n+1}{k+1}}
$$ could be useful. What I have tried? I tried to substitute the definition (2) in the r.h.s. of (1) to obtain the l.h.s. of (1). Another attempt was to take $C_{n-1}$ from (1) (known by induction assumption) and try with (3) to recover $C_n$ . In both cases, although I can smell that every thing is more or less related I can't find the technical steps to do the job. A combinatoric proof with Dyck paths can be found here , but this is not the way I'm trying to follow. EDIT The answer by ""Robert Z"" is very good and nice and I'll accept it; if someone could find a direct proof without generalised binomial coefficient, I'will accept his answer instead.","['catalan-numbers', 'proof-writing', 'combinatorics', 'binomial-coefficients']"
3304424,Does the logarithm satisfy any differential equation?,"The exponential function $\exp:\mathbb{R}\to\mathbb{R}_+$ satisfies the differential equation $f^\prime(x)=f(x)$ . Does the logarithm $\log:\mathbb{R}_+\to\mathbb{R}$ also satisfy any differential equation? Curious about if this gets closed immediately or not... seems like a way too basic question, but I have been totally stuck with this since yesterday.","['ordinary-differential-equations', 'logarithms']"
3304439,On derivation of curvature formulas,"I can't figure out the idea behind the following part of the derivation of curvature formulas; We let $M$ be a surface in $\mathbb{R}^3$ and $N$ be it's Gauss map. Moreover we consider a point $p\in M$ and local parametrisation $X$ such that $X(0)=p$ . Given this, there is a matrix $A$ of $-dN: T_{p}M \rightarrow T_{p}M$ with respect to the basis $X_{u},X_{v}$ . So far I understand things, now however, two matrix valued maps $DX=[X_{u},X_{v}]$ and $DN=[N_{u},N_{v}]$ are introduced from $U$ which is the domain of the parametrisation which satisfy , $-DN=DX A$ I dont understand this last equality. The complete outline can be found on pages 46-47 in, http://www.matematik.lu.se/matematiklu/personal/sigma/Gauss.pdf","['multivariable-calculus', 'linear-algebra', 'differential-geometry']"
3304461,"In normal usage, is equality simply a context-dependent equivalence relation?","In general, I am wondering that if we have some $x\equiv y \implies f(x) = f(y)$ , can we promote $x \equiv y$ to $x=y$ , as long as we ""promise"" to only discuss functions that respect this property? This seems to be the case for the construction of the integers as equivalence classes in $\mathbb{N} \times \mathbb{N}$ . We define an equivalence relation first, and with the knowledge that, even though two elements from, say, $1 = [(1,0)]$ (such as $(1,0)$ and $(2,1)$ , for instance) are unequal ordered pairs, they are equal in the sense that the arithmetic operations that we build on these notions will not discriminate between equivalent elements. Then it seems like equality is a sort of context-dependent equivalence relation , and the choice to write $x = y$ rather than $x\equiv y$ may simply just be a matter of taste, in some cases. It may be the case that my understanding of equality is correct, in which case I would still be grateful for additional insight.","['elementary-set-theory', 'notation', 'logic', 'equivalence-relations']"
3304463,"Show that there exists no function $f:\mathbb{R} \to \mathbb{R}$ such that $|f(a)-f(b)| \geq 1$ $\forall a,b \in \mathbb{R}$ where $a\neq b$","Show that $\nexists$ $f:\mathbb{R} \to \mathbb{R}$ such that $|f(a)-f(b)| \geq 1$ $\forall ~ a,b \in \mathbb{R}$ where $a\neq b$ I have no idea on how to do this. I think I have an idea to show that we can/cannot (I'm not sure if we can) construct a function when $f:[p,q]\to \mathbb{R}$ for some real numbers $p,q$ . In that case, if we consider the real axis of the interval $[p,q]$ and bend it to form a circle, we can perform an inversion about another circle $C$ centered at $O$ such that our former circle passes through the center $O$ . Each point on the circle maps to a distinct new point on the line (formed due to the inversion). Let that be the new real axis. Each point on the real axis is mapped, is there any way that the points might be separated by at least a unit?","['functions', 'real-analysis']"
3304506,Direction of gradient,"I'm reading Binmore and Davies, Calculus Concepts and Methods . On page 105, there's a function $$\tau(x,y) = 32 - 3\ln(1 + x^2 + 4y^2).$$ The gradient is calculated as $$\left(\frac{-3 \cdot 2x}{1 + x^2 + 4y^2}, \frac{-3\cdot 8y}{1 + x^2 + 4y^2} \right)^T$$ The text then states that the gradient vector has direction $(-x, -4y)^T$ . Where did this $(-x, -4y)$ come from? The text evaluates the gradient at the point $(x, y)^T = (10, 10)^T$ which results in $(\frac{-60}{501}, \frac{-240}{501})$ . The text refers to the direction of this as $(-1, -4)^T$ . I thought that the direction might just be the unit vector of the gradient but the length of $(-1, -4)^T$ is not equal to 1. The other questions on here related to the direction of the gradient have to do with maximum/minimum steepness or the direction being reported as a unit vector. Appreciate any help.","['multivariable-calculus', 'derivatives', 'vector-analysis']"
3304535,"If $Y\sim\mu$ with probability $p$ and $Y\sim\kappa(X,\;\cdot\;)$ otherwise, what's the conditional distribution of $Y$ given $X$?","Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(E,\mathcal E)$ be a measurale space $\mu$ be a probability measure on $(E,\mathcal E)$ $X$ be an $(E,\mathcal E)$ -valued random variable on $(\Omega,\mathcal A,\operatorname P)$ $\kappa$ be a Markov kernel on $(E,\mathcal E)$ $p\in[0,1]$ Assume we construct an $(E,\mathcal E)$ -valued random variable $Y$ on $(\Omega,\mathcal A,\operatorname P)$ in the following way: With probability $p$ we draw $Y$ from $\mu$ and with probability $1-p$ we draw $Y$ from $\kappa(X,\;\cdot\;)$ . What's the conditional distributon $\operatorname P\left[Y\in\;\cdot\;\mid X\right]$ of $Y$ given $X$ ? In particular, I want to determine the Markov kernel $Q$ on $(E,\mathcal E)$ such that $$\operatorname P\left[Y\in B\mid X\right]=Q(X,B)\;\;\;\text{almost surely for all }B\in\mathcal E.\tag1$$ In order to give a rigorous answer, I think that we need to introduce a $\{0,1\}$ -valued $p$ -Bernoulli distributed random variable $Z$ on $(\Omega,\mathcal A,\operatorname P)$ such that $X$ and $Z$ are independent $X$ and $Y$ are independent given $\{Z=1\}$ $\operatorname P\left[Y\in B\mid Z=1\right]=\mu(B)$ for all $B\in\mathcal E$ $\operatorname P\left[Y\in B\mid X\right]=\kappa(X,B)$ almost surely on $\{Z=0\}$ for all $B\in\mathcal E$ At first glance, I thought this would be an easy task. However, I don't know how I need to proceed. First of all, is my (supposed to be equivalent) description of the problem with the random variable $Z$ correct or did I impose any false assumption? If the description is correct, how do we need to proceed? Please take note of this related question: I we sample with a fixed probability from a distribution, what does this theoretical rigorously mean? .","['probability-distributions', 'independence', 'conditional-expectation', 'markov-process', 'probability-theory']"
3304538,Calculating the probability of a sequence having $n$ terms in a certain set.,"Let $\Delta$ be  the interval $[0,1]$ , then we can consider the probability space $(\Delta , \mathcal{B}(\Delta),m)$ , where $\mathcal{B}(\Delta)$ is the Borel $\sigma$ -algebra and $m$ is the Lebesgue measure. Then we can endow the space $\Delta^{\mathbb{N}}:= \{ (\omega_n)_{n\in \mathbb{N}};\  \omega_n \in \Delta, \ \forall \ n\in \mathbb{N}\}$ with the $\sigma$ -algebr $\mathcal{B}(\Delta^{\mathbb{N}})$ (Borel $\sigma$ -algebra of $\Delta^{\mathbb{N}}$ induced by the  product topology) and the probability measuare $m^{\mathbb{N}}$ in the measurable space $(\Delta^{\mathbb{N}},\mathcal{B}(\Delta^{\mathbb{N}}))$ , such that $$m^{\mathbb{N}} \left(A_1\times A_2\times \ldots \times A_n \times \prod_{i=n+1}^{\infty} \Delta\right)=m(A_1) \cdot \ldots\cdot m(A_n). $$ Now, consider the Bernoulli shift map \begin{align*}
\sigma: \Delta^{\mathbb{N}}&\to\Delta^\mathbb{N}\\
(\omega_n)_{n}&\to (\omega_{n+1})_n.
\end{align*} It is well known that the measure $m^{\mathbb{N}}$ on $(\Delta^{N}, \mathcal{B}(\Delta^{\mathbb{N}}))$ is invariant under $\sigma$ . Moreover the pair $(\sigma,m^{\mathbb{N}})$ is ergodic. So, Birkhoff's theorem says that for any cilinder $$\Lambda = A_1\times A_2\times \ldots \times A_n \times \prod_{i=n+1}^{\infty} \Delta, $$ where all $A_i$ 's are open subsets of $\Delta$ , $$\lim_{n\to \infty}\frac{1}{n} \sum_{i=0}^{n-1} \chi_{\Lambda}(\sigma^{i}( (\omega_j)_j)) = m^{\mathbb{N}}(\Lambda),\ \text{$m^{\mathbb{N}}$- a.s.} $$ My Question: Birkhoff's theorem says that for almost every $(\omega_n)_n\in \mathbb{\Delta^{\mathbb{N}}}$ , there exists $i = i((\omega_n)_n)\in \mathbb{N}$ such that $(\omega_{i+1},...,\omega_{i+n})\in A_1 \times ...\times A_n$ . Is it possible calculate (or estimate) the following probability $$P_m(\Lambda) := m^{\mathbb{N}}\left(\left\{((\omega_n)_{n};\ \exists\ i \in \{0,...,m\}\ \text{such that }\sigma^{i}(\omega) \in \Lambda\right\}\right)\ ? $$ Moreover, given $\varepsilon>0$ , are there $n_0 \in \mathbb{N}$ , such that $$1- P_n(\Lambda) < \varepsilon, \forall n > n_0  ? $$","['measure-theory', 'ergodic-theory', 'probability-theory', 'probability', 'dynamical-systems']"
3304547,"Determining a sequence's $n$-th term from its first and second differences, when latter is in arithmetic or geometric progression","For the series, $3, 7, 14, 24, 37, \ldots$ , the $1$ st successive differences are $4,7,10,13,\ldots$ , and the $2$ nd successive differences are $3,3,3,\ldots$ . So, the book says, the $nth$ term $T_n$ of the given series will be $an^2+bn+c$ . And for the series, $3,8,22,72,266,1036,\ldots$ , the $1$ st successive differences are $5,14,50,194,770,\ldots$ , and the $2$ nd successive differences are $9,36,144,576,\ldots$ , which are in geometric progression with common ratio being $4$ . So, the book says $T_n$ will be $a4^{n-1}+bn+c$ . I have verified in both cases that it's true with $a,b,c$ in first case coming out to be $\frac32,-\frac12,2$ , and in second case $1,2,0$ , respectively. My question is why is this so? Why is $T_n$ the way it is? How to approach this method in a fresh question?",['sequences-and-series']
3304580,Good undergraduate texts in analysis for self studying [duplicate],"This question already has answers here : Good First Course in real analysis book for self study (7 answers) Closed 4 years ago . I know this question was asked many times but I have some specific questions. I know the usual recommendations but I am afraid of going for Rudin because I've read many reviews that said it wasn't good for self studying. I remember in one review I read the guy said "" As you go through the book you get excited about some cool theorems and results only to find that Rudin gives a proof that only does the job and leaves out much of the intuition for you to either find on your own or look for elsewhere"". I don't like this type of texts because when the proof is too directed it becomes unsatisfactory. By directed I mean that the result is already established and we're just trying to make it formal by looking for arguments that just verify the fact without , for example , mentioning how one would first consider these arguments and how they would come up while trying to prove the result. That said , There are other suggestions such as Barry Simon's comprehensive course in analysis. This is a new text which isn't reviewed a lot. The description says it may be suitable for a graduate level course but others say it gives a good introduction to the prerequisites but I'm not sure. Any other suggestions ? Edit : thanks for your answers","['soft-question', 'analysis', 'reference-request']"
3304594,Irreducible components of exceptional locus.,"Let $\phi:X\rightarrow Y$ be a birational regular map between projective varieties where $Y$ is non-singular. Define $C=\{q\in Y:\dim(\phi^{-1}(q))>0)\}$ . Let $G=\phi^{-1}(C)$ . I saw the following statement: ""Irreducible components of $G$ are sub-varieties of codimension $1$ "". Could someone please explain or give a hint of why this should be true. Thanks in advance. PS: This subject is completely new to me. It would be really helpful if someone explain the answer with more details.","['algebraic-geometry', 'commutative-algebra', 'projective-varieties']"
3304655,What is mathematical statistics (DeGroot vs Hogg)?,"I’m currently self-studying Ross’ A First Course in Probability, and want to pick up a statistics text as well.  What’s the difference between a Prob/Stats book like DeGroot & Schervish vs. Mathematical Statistics by Hogg/McKean/Craig or the book by Rice? Are they intended for different audiences/courses?  I currently like the level of mathematical rigor in Ross, so I’m looking for something comparable.","['statistics', 'probability', 'reference-request']"
3304671,A finite order restriction of a Fredholm operator is also a Fredholm operator.,"Let $A:D(A) \subseteq H \to H$ and $B:D(B) \subseteq H \to H$ be closed linear operators on a Hilbert space $H$ such that $A$ is a finite order extension of $B$ , that is, $B \subseteq A$ and $\mbox{dim } D(A)/D(B) < \infty$ . I need to show that if $\lambda \notin \sigma(A)$ and $\lambda I-A$ is a Fredholm operator, then $\lambda I-B$ is also a Fredholm operator. There is a hint: Since $A$ is a finite order extension of $B$ , the difference of the resolvents is of finite order. But, I don't know how can I use the hint. Can I say that $\lambda \notin \sigma(B)$ ? My attempt Since $\lambda I-A$ is a Fredholm operator, by definition of a Fredholm operator we know that $\mbox{dim} (\ker (\lambda I-A))<\infty$ , $\mbox{dim} (H/R(\lambda I-A))<\infty$ and the range $R(\lambda I-A)$ of $\lambda I-A$ is closed in $H$ . From here we get that $\mbox{dim} (\ker (\lambda I-B))<\infty$ . Thank you for any help you can privide me.","['operator-theory', 'spectral-theory', 'functional-analysis']"
3304697,What is the maximum of $\sum_{k=1}^{\infty} (-1)^k(^kx)$?,"During my testing of the series $\sum\limits_{k=1}^{n} (-1)^k(^kx)$ , I found that the sum converges to two limits when $n \to \infty$ , for $e^{-e} \lt x \le e^{1/e}$ and oscillates between depending on whether $n$ is even or odd. Here, $^kx$ is tetration . The notation $^kx$ is the same as $x^{x^{x^{....}}}$ , which is the application of exponentiation $k-1$ times. Ex. $^3x=x^{x^x}$ . Questions: $(1)$ What is the maximum and minimum of $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for even $n$ ? $(2)$ What is the maximum and minimum of $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for odd $n$ ? Edit 1: Also during my testing in PARI, I observed that the sum seems to converge to two values only in the domain of $e^{-e} \lt x \le e^{1/e}$ . I think the reason for this maybe is that, since $^{\infty}x$ converges only for $e^{-e} \lt x \le e^{1/e}$ , the sum also converges for the same domain. I would appreciate if someone could explain why the sum converges only for $e^{-e} \lt x \le e^{1/e}$ . Edit 2: With the help of user Vepir , I was able to plot $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for both even and odd $n$ . Even $n$ : Odd $n$ : Observations from graphs: $(i.)$ $x=e^{-e}$ is the maximum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for both even and odd $n$ when $e^{-e} \lt x \le e^{1/e}$ . $(ii.)$ $x=1$ is the minimum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for even $n$ when $e^{-e} \lt x \le e^{1/e}$ . $(iii.)$ $x=e^{1/e}$ is the minimum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for odd $n$ when $e^{-e} \lt x \le e^{1/e}$ . Now how can we prove any of the three claims above?","['maxima-minima', 'calculus', 'sequences-and-series', 'limits', 'tetration']"
3304704,Why $A \subseteq \mathscr P(A)$?,"I'm trying to prove the following: Suppose $A ⊆ \mathscr P (A).$ Prove that $\mathscr P(A) ⊆ \mathscr P
( \mathscr P(A)).$ How can you suppose that $A \subseteq \mathscr P(A)$ ? If my understanding is correct, $\mathscr P(A)$ denotes power set of $A$ , or in other words, set that contains all the subsets of $A$ Let $A = \{1,2\}$ , then $\mathscr P(A) = \{\emptyset, \{1\},\{2\}, \{1,2\}\}$ $\{1,2\}$ is not a subset of $\mathscr P(A)$ , but $\{\{1,2\}\}$ is. We can only say that $A \in \mathscr P(A)$ So again, given that $\mathscr P(A)$ denotes power set of $A$ , how is it possible to assume that $A \subseteq \mathscr P(A)$ ?","['elementary-set-theory', 'proof-explanation']"
3304731,Integral of some anti/symmetric functions of two variables,"Consider $f$ and $g$ , two scalar functions of one variable defined over a the interval [- $a$ , $a$ ], with $a$ real number. $f$ is symmetric, $f(x)=f(-x)$ and $g$ antisymmetric $g(-x)=-g(x)$ . We consider the following integral: $$\int_{-a}^a \mathrm{d}x\int_{-a}^a \mathrm{d}y\; f(x)^2f(y)V(|x-y|)g(y),$$ where $V$ is just another scalar function and $|x|$ is the absolute value of $x$ . I am under impression that this kind of integrals are always equal to zero. I am working with a numerical program that has to do this kind of integrals many times with different "" $g,f$ "", and I always get a negligible value (possibly zero). My questions are: Is this always zero? If so why? is this the result of some theorem? If it is not zero for that interval, would it be zero for the domain $(-\infty,\infty)$ ? If any of this is false, would any other condition make it true? I have manipulated a few simple sinusoidal functions to see if it is true (and it seems to be) but I am under impression that there is something more general. There has to be a manipulation that would make everything more evident. Edit: $f$ , $g$ and $V$ are bounded","['integration', 'multivariable-calculus', 'symmetry']"
3304781,"On the Clausen triple $8\rm{Cl}_2\left(\frac{\pi}2\right)+3\rm{Cl}_2\left(\frac{\pi}3\right)=12\,\rm{Cl}_2\left(\frac{\pi}6\right)$","While doing research on the Clausen function , I came across this nice identity, $$8\operatorname{Cl}_2\left(\frac{\pi}2\right)+3\operatorname{Cl}_2\left(\frac{\pi}3\right)=12\operatorname{Cl}_2\left(\frac{\pi}6\right)$$ The two addends on the LHS are Catalan's constant and Gieseking's constants . It made me wonder if there were similar relations. Define, $$\begin{aligned}
\text{Cl}_m\left(\frac{\pi}2\right) &= \sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}2\right)}{k^m}=\sum_{n=0}^\infty\left(\frac1{(4n+1)^m}-\frac1{(4n+3)^m}\right)\\
\text{Cl}_m\left(\frac{\pi}3\right) &= \sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}3\right)}{k^m}=\frac{2^{m-1}+1}{2^m}\sum_{n=0}^\infty\left(\frac{\sqrt3}{(3n+1)^m}-\frac{\sqrt3}{(3n+2)^m}\right)\\
\text{Cl}_m\left(\frac{\pi}6\right) &= \sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}6\right)}{k^m}
\end{aligned}$$ and, $$\begin{aligned}
a &= 2^{m-1}(3^{m-1}+1)\\ 
b &= 3^{m-1}\\ 
c &= 2^m\,3^{m-1}
\end{aligned}$$ Q: How do we prove, with $a,b,c$ as defined above, that, $$a\operatorname{Cl}_m\left(\frac{\pi}2\right)+b\operatorname{Cl}_m\left(\frac{\pi}3\right)=c\operatorname{Cl}_m\left(\frac{\pi}6\right)$$ or equivalently, $$a\sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}2\right)}{k^m}+b\sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}3\right)}{k^m} = c\sum_{k=1}^\infty\frac{\sin\left(k\,\frac{\pi}6\right)}{k^m}$$ for all integer $m>1$ ?","['polylogarithm', 'special-functions', 'sequences-and-series']"
3304782,"Does one of the orthogonal groups $O^\pm(2n,2)$ always have an element of order $2n+1$","For arbitrary $n\ge 1$ , is it true that (at least) one of the orthogonal groups $O^\pm(2n,2)$ , consisting of $2n\times 2n$ matrices over the field of order $2$ , has an element of order $2n+1$ ? A quick check in MAGMA shows that it is true for $n\le 7$ (code still running for $n=8$ ). There is not an obvious pattern in the matrices found, but there are many solutions in each case, so some nice construction may be possible. EDIT: Reducing to $2n+1$ prime We can reduce to $2n+1$ prime as follows. This means that it is sufficient to prove that $2n+1$ divides the order of one of $O^\pm(2n,2)$ when $2n+1$ is prime. A quick check shows that this is true for primes $p<1000$ . Suppose the above is true when $2n+1$ is prime and $2n+1$ is the smallest non-prime for which the above does not hold. Then $2n+1=(2u+1)(2v+1)$ for some $u,v>1$ and $O^\epsilon(2u,2)$ has an element $M$ of order $2u+1$ for some $\epsilon\in\{\pm\}$ . One can check then that the following is an element of $O^{\epsilon'}(2n,2)$ (in the appropriate basis for some $\epsilon'\in\{\pm\}$ ) and has order $2n+1$ contrary to assumption: $$\left(\begin{matrix}
\mathbf{0}_{2u} & M & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\vdots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots & \vdots \\
\mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2v} \\
\mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2v}\\
\end{matrix}\right)$$","['matrices', 'group-theory', 'finite-fields', 'finite-groups']"
3304800,Find the Range of $y=\frac{x+a}{x^2+bx+c^2}$,Given that $x^2-4cx+b^2 \gt 0$ $\:$ $\forall$ $x \in \mathbb{R}$ and $a^2+c^2-ab \lt 0$ Then find the Range of $$y=\frac{x+a}{x^2+bx+c^2}$$ My try: Since $$x^2-4cx+b^2 \gt 0$$ we have Discriminant $$D \lt 0$$ $\implies$ $$b^2-4c^2 \gt 0$$ Also $$x^2+bx+c^2=(x+a)^2+(b-2a)(x+a)+a^2+c^2-ab$$ Hence $$y=\frac{1}{(x+a)+b-2a+\frac{a^2+c^2-ab}{x+a}}$$ But Since $a^2+c^2-ab \lt 0$ We can't Use AM GM Inequality Any way to proceed?,"['algebra-precalculus', 'functions', 'a.m.-g.m.-inequality']"
3304808,Refinement of a famous inequality,"I refine a famous inequality this is the following  : Let $x,y>0$ then we have : $$x^n+y^n\leq \Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n+\Big(\frac{x+y}{2}\Big)^n$$ It's equivalent to : $$x^n+1\leq \Big(\frac{x^n+1}{x^{n-1}+1} \Big)^n+\Big(\frac{x+1}{2}\Big)^n$$ Because it's homogeneous .
We can't use AM GM  it's too weak so the difficulty  is interesting .
I try to derivate this but it's a little bit ugly .
I have two questions how interpreting this result and how to solve this one variable inequality  ? Thanks a lot Remark (@Andreas, 2020-10-25) This inequality is rather fine-tuned. Consider as a first term on the RHS $$\Big(\frac{x^n+y^n+z\cdot(\frac{x+y}{2})^n}{x^{n-1}+y^{n-1}+z\cdot(\frac{x+y}{2})^{n-1}}\Big)^n
$$ and let $z$ increase from $0$ to $1$ . It is easy to see that the increase of $z$ makes the term smaller. Choosing $z=0$ (this question) makes the term ""just big enough"" for the inequality to be "" $\le$ "". Indeed, for $z=1$ , this inverses to "" $\ge$ "", as this post shows. So, fine tuned upper and lower bounds to $x^n + y^n$ are available. An interesting observation is the following: $$\Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n \ge x^n+y^n - \Big(\frac{x+y}{2}\Big)^n \ge \frac{x^n+y^n}{2} .$$ The first inequality is the one under consideration, the second one is an application of Jensen's inequality for two values of the function $x^n$ , whereas the inequality between the first and the third expression is a direct application of Slater's inequality* (eq. (2) in  this pdf) , so we see here a sharpening of Slater's inequality for the function $x^n$ . *Slater ML, A Companion Inequality to Jensen's Inequality. Jour. of Approximation Theory 1981, 32(2):160–166.","['contest-math', 'inequality', 'real-analysis']"
3304814,Sum of Two Pascal Random Variables,"I'm having trouble finding the PMF of a sum of pascal random variables. The problem stated is: Let $X \sim Pascal(m,p)$ and $Y \sim Pascal(l,p)$ where X and Y are independent of each other. Let $Z = X + Y$ . Find PMF of Z. The textbook answer states that since the two random variables are independent we can think of Z as a pascal random variable where we want to see $m + l$ heads. So $Z \sim Pascal(m+l, p)$ The book doesn't give us an exact distribution, but Pascal(m+l, p) using the negative binomial formula has distribution $${k-1 \choose m+l-1}p^{m+l}(1-p)^{k-m-l}$$ However when I approached it before looking at the given solution, I didn't get the same same binomial coefficient. My approach: Let a = m+l where a is a fixed constant $$P(Z = a) = P(X + Y = a)$$ $$=\sum_m P(X + Y = a|X = m)P(X = m)$$ $$=\sum_m P(Y = a - m|X = m)P(X = m)$$ Since X and Y are independent of each other $$=\sum_m P(Y = a-m)P(X = m)$$ Now applying the negative binomial formula, $$\sum_m {r-1 \choose a-m-1}p^{a-m}(1-p)^{r-a+m}{k-r-1 \choose m-1}p^m(1-p)^{k-r-m}$$ $$\sum_m {r-1 \choose a-m-1}p^a(1-p)^{k-a}{k-r-1 \choose m-1}$$ $$p^a(1-p)^{k-a}\sum_m {r-1 \choose a-m-1}{k-r-1 \choose m-1}$$ By Vandermonde's Identity, $${k-2 \choose a-2}p^a(1-p)^{k-a}$$ Which makes more sense to me as the distribution since we choose the last place for the last heads to end for pascal random variable X and we choose the last place for the last heads to end for pascal random variable Y. Hence we have ${k-2 \choose a-2}$ . However, using the variable a the textbook is saying our binomial coefficient is ${k-1 \choose a-1}$ . What's the flaw in my logic? How can I go about understanding why the binomial coefficient is ${k-1 \choose a-1}$ ?
.","['discrete-mathematics', 'negative-binomial', 'probability']"
3304823,Definition of the covariant derivative,"In Peter Petersen's book Riemannian Geometry (2. Edition) the covariant derivative on a Riemannian manifold is defined by the implicit formula $$2g(\nabla_YX,Z)=(L_Xg)(Y,Z)+(d\theta_X)(Y,Z)$$ where $(L_Xg)$ is the Lie derivative of the metric and $\theta_X$ is the one form given by $\theta_X(Y)=g(X,Y)$ . Then it is shown that this is the uniquely determined metric and torsion free affine connection. I understand that $X$ is locally a gradient field iff $d\theta_X=0$ so $d\theta_X$ measures in some sense how far $X$ is away from being a gradient field.  Also i think i understand what $(L_Xg)(Y,Z)$ is: we let $(Y,Z)$ flow along $X$ and measure the infinitesimal change after applying the metric. The way i see the covariant derivative is to embedd the Riemannian manifold into some $ \mathbb R^N$ and then take the tangential part of the usual derivative. This makes perfectly sense to me. Of course by the uniqueness part these two definitions agree. Beside that i don't understand why the above formula is a sensible definition. What does it geometrically mean that the derivative splits into these two parts? Thanks in advance!","['riemannian-geometry', 'tensors', 'lie-derivative', 'intuition', 'differential-geometry']"
3304872,$n$ goblins and $2n$ elves should be set into series,"We have $2n$ elves and n goblins. Elves are beautiful and everyone different than others. Goblins
They are ugly and all the same. We set the whole company in a $3n$ -element series.
Goblins feel uncomfortable in the environment of elves, and therefore every goblin must have at least one another goblin as a neighbor. In how many ways can this company be set? My approach I want to count situations when goblins don't pass the condition. So all possibilities for goblins are equal to $\binom{3n}{n}$ There are two more edge cases: $1$ goblin is on the edge of series: $ \binom{2}{1}\binom{3n-2}{n-1}$ $1$ goblin is on the edge of series: $ \binom{3n-2}{1}\binom{3n-3}{n-1}$ So goblins can be set on $$ \binom{3n}{n} - \binom{2}{1}\binom{3n-2}{n-1} - \binom{3n-2}{1}\binom{3n-3}{n-1} $$ ways.
Elves can be set on just $(2n)!$ ways. So the result should be: $$ (2n)! \cdot \left( \binom{3n}{n} - \binom{2}{1}\binom{3n-2}{n-1} - \binom{3n-2}{1}\binom{3n-3}{n-1} \right) $$ but it doesn't work. If we take $n=2$ my pattern fails...","['combinatorics', 'discrete-mathematics']"
3304971,"Ways of adding $N$ nonnegative, different integers to give $mN$","I am struggling with the following problem: how many ways are there to add $N$ distinct, non-negative integers so that the result is $mN$ , with $m$ an (odd) integer. I have come accross some posts regarding the method of stars and bars (like this , this or this ) but neither fits exactly this problem. For example, for $m = N = 3$ we would have the following allowed possibilities: $ (6 + 3 + 0), (6 + 2 + 1), (5 + 4 + 0), (5 + 3 + 1), (4 + 3 + 2) $ I am sorry if I have missed some post that is relevant for the question. Thank you in advance! --- EDIT --- Maybe I should have stated that I am not only interested in a closed formula (which, I am aware, may not exist), but that a recursive formula or even a (not too rough) estimate will also be very helpful.","['elementary-number-theory', 'combinatorics']"
3304982,An approximate solution to a differential equation $f'(x)^2 - \omega^2 f(x)^2 = G(x)$ for small $x$,"Suppose I have an ODE of the form $$
\left( \frac{df}{dx} \right)^{2} - \omega^2 f(x)^2 = G(x)
$$ with $\omega>0$ and the initial condition $f(0)=0$ and where $G(x)$ is a very complicated function (which you might not even have an analytic expression for). I seek the solution for $x>0$ . Even though I don't have the full expression for $G(x)$ , I do know the series expansion for $G(x)$ for $0< x \ll 1$ where I have $$
G(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6)
$$ for some number $\alpha >0$ . If I am interested in the solution for small $x$ , then I have approximately $$
\left( \frac{df}{dx} \right)^2 - \omega^2 f(x)^2 \simeq \omega^2 
$$ under the condition that $\alpha x^4 \ll 1$ . There is a simple solution to the above DE: does it make sense to say the following? $$
f(x) \simeq \sinh(\omega x) \ \ \ \ \ \ \mathrm{when\ }\alpha x^4 \ll 1
$$ The above seems to work numerically quite well, but I am confused because I am taking a series expansion in the ODE and dropping terms $\mathcal{O}(x^4)$ there in the DE: however, my approximate solution has terms higher order than this in it. So in this sense my solution is not really a series in $x\ll 1$ . Does it make sense what I've done? Maybe the condition $\alpha x^4 \ll 1$ doesn't make sense here? Is there any literature on approximating DE's in this manner? An Example With a Plot I have cooked up a function (involving Bessel functions) $$
G_0(x) := \omega^2 J_0\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right) + 2 \omega^2 J_{2}\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right)
$$ This has the $x\ll 1$ expansion $G_0(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6)$ (I've cooked up the parameters in the arguments so this is so). The smaller you make $\alpha$ , for larger values of $x$ the function looks approximately constant $G_0(x) \sim \omega^2$ . Below I do some numerical plotting for $\omega = 0.4$ and $\alpha=0.001$ . In the first curve I numerically solve for the exact curve $f(x)$ for the choice $G_0$ . The second curve I plot the approximation $\sinh(\omega x)$ . In the last curve I use the first three terms of the series expansion of $\sinh(\omega x) \sim \omega x + \ldots$ The Point: $\sinh(\omega x)$ is a much better approximation for the solution $f$ (than the simple series expansion $\omega x + \ldots$ ). This is because $\alpha$ is chosen to be small here. Why can you neglect terms $\mathcal{O}(x^4)$ in the ODE (presumably this would mean your solution needs to be a series in $x$ too?), and yet the better approximation is a function $\sinh(\omega x)$ which has every order (higher than 4!) contributions? How to understand the error in $\sinh(\omega x)$ when using such an approximation? (an error which seems to be smaller than the error introduced by a simple series solution to $f$ .)","['approximation', 'ordinary-differential-equations']"
3304998,Book recommendation - probability with measure theory?,"I am looking for a book that deals with the fundamentals of probability (e.g. probability spaces, distribution functions, expectation, etc.) but from a measure-theoretical perspective (e.g. defining the expectation in terms of the integral w.r.t. a measure derived from the CDF). My Background Probability: I have already taken basic and not-so-basic probability classes. These have dealt with the fundamentals of probability in a rather rigorous manner, so I am quite familiar with it. However, these courses have never involved measure theory more than simply discussing the Borel $\sigma$ -algebra. Measure theory: I have already taken a course that has dealt with the lebesgue measure very rigorously. I have also done some reading on my own about some basic measure theory (in particular the first chapter of Bogachev's Measure Theory). What I'm looking for: I always found it bizarre how expectation was defined differently for discrete and continuous random variables (and RVs that were neither were totally ignored). I then learned that this could be resolved by defining measures with the distribution functions, and integrating with respect to these measures. However, I never saw this explained fully, and I have not been able to find a book on probability that explains this either. I am looking for a book that starts from the very basics of probability and measure theory, and builds up probability using these tools. Preferably the book will also show how these general definitions become the simple ones we all know in special cases (e.g. if the measure is discrete, the expecation is just a sum). Thank you for any recommendations!","['measure-theory', 'book-recommendation', 'probability']"
3305001,Rational Functions without holes or asymptotes?,"Can you please give me some examples of such functions. I hope 1/2 us not one of this category of functions. Please give non obvious examples. 
Thanks a lot!","['functions', 'graphing-functions']"
3305026,How to interpret Newton's 6th Lemma?,"In Newton's ""Principia Mathematica"" Book 1, Section 1 (""Of the Motion of Bodies"") there is the following Lemma 6: ""LEMMA VI. If any arc ACB, given in position, is subtended by its chord AB, and in any point A, in the middle of the continued curvature, is touched
  by a right line AD, produced both ways; then if the points A and B
  approach one another and meet, I say, the angle BAD, contained between
  the chord and the tangent, will be diminished in infinitum, and
  ultimately will vanish. For if that angle does not vanish, the arc ACB will contain with the
  tangent AD an angle equal to a rectilinear angle; and therefore the
  curvature at the point A will not be continued, which is against the
  supposition."" ...this text is accompanied by the following diagram: Noting, that the line rbd is parallel to the line RBD and the arc Acb appears to have a smaller curvature than the arc ACB on Newton's diagram, which of the following animated diagrams correctly depicts his Lemma ? This diagram of mine?: ...or this diagram of mine?: Note, that in the latter diagram, I have added the red colored depictions of the angles BAD and ABD .  These angle depictions do not appear on Newton's diagram (...but he writes about the angle BAD in the text of his 6 th Lemma).","['calculus', 'geometry']"
3305047,Need any kind of insights about a strange function,"The function in question is $f(x; b) = \sum_{k=0}^\infty \{ b^k x \}\cdot b^{-k}$ where the base $b$ is equal to $2$ here. Also, $x\in [0, 1[$ . The curly brackets denote the fractional part function. The function $f$ seems continuous almost everywhere but nowhere differentiable, and has a fractal look. It satisfies the recursion $f(x) = 2f(x/2) - x$ . We also have $f(x) = -x \log_2 x + g(x)$ with $g$ a non-trivial, non-constant function satisfying $g(x) = 2 g(x/2)$ . The function $g$ is pictured below. Any interesting results about this function is welcome. In particular, I am interested in the proportion $P(\alpha)$ of real numbers in $[0, 1[$ such that $f(x) < \alpha$ for any $\alpha \in [0, 2[$ . This is the inverse of the percentile distribution, pictured below. But I'm not even sure that the set $\{ x \in [0, 1[  ; f(x) < \alpha\}$ is Lebesgue-measurable. Finally, is the function $f$ one-to-one despite the appearance?","['number-theory', 'recreational-mathematics', 'probability-theory', 'sequences-and-series']"
3305098,How to obtain the analytic representation of the given infinite series?,"This series is given in Griffiths Introduction to Electrodynamics chapter 3 in an example explaining seperation of variables. $$V(x,y) = \frac{4V_0}{\pi} \sum_{n=1,3,5\ldots} \frac{1}{n}e^{-n\pi x/a}\sin(n\pi y/a)$$ $$V(x,y) = \frac{2V_0}{\pi} \tan^{-1}\left(\frac{\sin(\pi y/a)}{\sinh(\pi x/a)} \right)$$ I don't have any idea how to get the analytic representation of the series. I tried replacing $n$ by $
-n$ to get hyperbolic sine but it also introduces summation over negative odd integers and I don't think they can be added over. Also, it will form hyperbolic sine in the numerator rather than the denominator. How to proceed?","['hyperbolic-functions', 'trigonometry', 'closed-form', 'sequences-and-series']"
3305109,Proof that a function is not surjective,"I am trying to prove that this function is not surjective.Is this the right approach? Define a function $f:\mathbb{R} \rightarrow \mathbb{R}$ by $f(x)= \frac{2x-1}{x+1}$ After solving for $x$ and letting $y=f(x)$ I get $x= \frac{y+1}{2-y}$ Proof: In order for $f$ to be surjective $\forall y \in \mathbb{R}, \exists x \in \mathbb{R}$ such that $f(x)=y$ . Since there does not exist $y=2 \in \mathbb{R}$ such that $f(x)=y$ . $f$ is not surjective.","['elementary-set-theory', 'proof-verification']"
3305112,Prove $\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^32^k {2k\choose k}}=\frac1{4}\zeta(3)-\frac1{6}\ln^32$,"How to prove that $$\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^32^k {2k\choose k}}=\frac1{4}\zeta(3)-\frac1{6}\ln^32?$$ A friend posted this nice problem on my FB group and I managed to evaluate it using the $\arcsin^2 x$ identity. I would like to see different approaches. Thanks. My solution: Using the following identity: (see here ) $$\arcsin^2z=\frac12\sum_{k=1}^\infty\frac{(2z)^{2k}}{k^2{2k \choose k}}$$ Set $\ z=\sqrt{\frac{x}{8}}$ then divide both sides by $x$ and integrate from $x=0$ to $-1$ , to get \begin{align}
S&=\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^32^k {2k\choose k}}=-2\underbrace{\int_0^{-1}\frac{\arcsin^2\left(\sqrt{\frac x8}\right)}{x}\ dx}_{\large\arcsin\left(\sqrt{\frac x8}\right)=y}\\
&=-4\int_0^{\frac{\ln2}{2}i} y^2\cot y\ dy\overset{y=ix}{=}4\int_0^{\frac{\ln2}{2}} x^2\coth x\ dx
\end{align} Lets find the antiderivative of the integral: \begin{align}
I&=\int x^2\coth x\ dx\overset{IBP}{=}x^2\ln(\text{arcsinh}(x))-2\int x\ln(\text{arcsinh}(x))\ dx\\
&=x^2\ln(\text{arcsinh}(x))-2\int x\left\{x-\ln2-\ln(1-e^{-2x})\right\}\ dx\\
&=x^2\ln(\text{arcsinh}(x))-\frac23x^3+\ln2\ x^2-2\int x\ln(1-e^{-2x})\ dx\\
&=x^2\ln(\text{arcsinh}(x))-\frac23x^3+\ln2\ x^2+2\sum_{n=1}^\infty\frac1n\int xe^{-2nx}\ dx\\
&=x^2\ln(\text{arcsinh}(x))-\frac23x^3+\ln2\ x^2+2\sum_{n=1}^\infty\frac1n\left(-\frac{e^{-2nx}}{4n^2}-\frac{xe^{-2nx}}{2n}\right)\\
&=x^2\ln(\text{arcsinh}(x))-\frac23x^3+\ln2\ x^2-\frac12\sum_{n=1}^\infty\frac{(e^{-2x})^n}{n^3}-x\sum_{n=1}^\infty\frac{(e^{-2x})^n}{n^2}\\
&=x^2\left\{\ln x-\ln2-\ln(1-e^{-2x})\right\}-\frac23x^3+\ln2\ x^2-\frac12\operatorname{Li}_3(e^{-2x})-x\operatorname{Li}_2(e^{-2x})\\
&=\frac{x^3}{3}+x^2\ln(1-e^{-2x})-\frac12\operatorname{Li}_3(e^{-2x})-x\operatorname{Li}_2(e^{-2x})\\
\end{align} Thus \begin{align}
S&=4\left[\frac{x^3}{3}+x^2\ln(1-e^{-2x})-\frac12\operatorname{Li}_3(e^{-2x})-x\operatorname{Li}_2(e^{-2x})\right]_0^{\frac{\ln2}{2}}\\
&=4\left[\frac12\zeta(3)-\frac5{24}\ln^32-\frac12\operatorname{Li}_3\left(\frac12\right)-\frac{\ln2}{2}\operatorname{Li}_2\left(\frac12\right)\right]\\
&=4\left[\frac1{16}\zeta(3)-\frac1{24}\ln^32\right]\\
&\boxed{=\frac1{4}\zeta(3)-\frac1{6}\ln^32}
\end{align} Note that we used $\operatorname{Li}_2\left(\frac12\right)=\frac12\zeta(2)-\frac12\ln^22$ and $\operatorname{Li}_3\left(\frac12\right)=\frac78\zeta(3)-\frac12\ln2\zeta(2)+\frac16\ln^32$","['integration', 'definite-integrals', 'binomial-coefficients', 'closed-form', 'sequences-and-series']"
3305113,Functions between subsets of a set over which $f$ is a bijection,"Is it true that for a bijective map $f:X \rightarrow X$ , that for any non-empty subset $A$ of $X$ we have that $f:A \rightarrow A$ is also a bijection? I assume the answer in general is no, in which case does there exist a statement along similar lines to the one written above?","['elementary-set-theory', 'functions']"
