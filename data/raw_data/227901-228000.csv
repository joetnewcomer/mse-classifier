question_id,title,body,tags
4713698,ODE with no solution,"Given the ODE $$y''+16y=0$$ with $y(0)=y_0$ and $y(\pi/2)=y_1$ , I have to find for which $y_0$ and $y_1$ , the ODE will have none solution. I know there are $A,B\in\mathbb{R}$ such that $$y(t)=A\cos(4t)+B\sin(4t)$$ and when I use the initial conditions, I get $$y_1=y_0=A$$ Hence $$y(t)=y_0\cos(4t)+B\sin(4t)$$ However, I have any clue for what I should do now.",['ordinary-differential-equations']
4713715,Confusion on definition of differential,"I’m having a hard time understanding a certain section in In Ordinary Differential Equations by Tenenbaum and Pollard. Specifically, in section 6, they state the following: Given a function $y=f(x)$ , They define the differential dy as: $dy=f’(x) \Delta x $ (6.14) And denote the function dy as $(dy)(x, \Delta x)$ as it is a function of $x$ and $\Delta x.$ They then state that if $y=\hat{x}$ , where  (and this distinction is completely lost on me in the sense of not understanding why it is necessary), “we place the symbol ^ over x so that $y=\hat{x}$ will define a function that assigns to each value of the independent variable x the same unique value to the dependent variable y.” They then say that if $y=\hat{x}$ , then $(dy)(x, \Delta x) = (d \hat{x})(x, \Delta x) = \Delta x.$ (6.22) All of this is fine I suppose but I don’t understand the logic at all behind the next step: “If I’m (6.14) we replace $\Delta x$ by its value as given in (6.22), it becomes $(dy)(x, \Delta x) = f’(x) (d \hat{x})(x, \Delta x)$ (6.24) This part doesn’t make any sense to me as they are simultaneously stating that $y=f(x)$ and $y=\hat{x}$ by saying so, unless I misunderstood something. And finally, this last part also completely evades me: They state that the above relation is correct (still not sure why based on their reasoning), however “it became customary to write (6.24) in the more familiar form $dy = f’(x) dx $ or $\frac{dy}{dx} = f’(x)$ . This last part especially confuses me because now it can no longer be said they were defining dy as a function, but were using its definition as a limit in the conventional sense of a derivative. I’ve seen posts about why this symbolic manipulation of dy and dx turns out to be ok because of the chain rule, but I’m more just lost on any and all of the steps they took in introducing this differential. If anyone could help clarify that would be great, thank you!","['calculus', 'derivatives', 'ordinary-differential-equations']"
4713737,Evaluate the improper integral $\int_{-\infty}^{+\infty} \frac{\ln(1+x^{4})}{1+x^{2}} dx$,"How do I evaluate this integral ? I am a student who just studied calculus 1 and recently definite integral. $$
\int_{-\infty}^{+\infty} \frac{\ln(1+x^{4})}{1+x^{2}} dx
$$ I substituted both $x = \tan t$ and $x = \frac1{t}$ but could not proceed further.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4713740,Measure Theory - Why doesn't empty interior imply zero measure?,"I'm studying for a qualifying exam in measure theory and ended up ""proving"" a set with empty interior implies zero measure. I know this isn't true (irrational numbers provide a counterexample in $\mathbb{R}$ ) but can't find my error: Let $E \subset \mathbb{R}^n$ be a set with empty interior. Note that the closure, $\overline{E} = \text{int}(E) \cup \text{Bd} (E)$ , where $\text{int}(E)$ is the interior of $E$ and $\text{Bd}(E)$ is the set of boundary points of $E$ . By monotonicity and subadditivity of outer measure, $|E|_e \le |\overline{E}_e| \le |\text{int}(E)|_e + |\text{Bd}(E)|_e=0$ by assumption and because the boundary of $E$ is $(n-1)-$ dimensional. Can anyone find my mistake?","['general-topology', 'lebesgue-measure', 'measure-theory', 'real-analysis']"
4713742,Is $ \left\{ A P + P A^{T} \mid P \succ 0 \right\} $ open?,"Consider an $n$ by $n$ matrix $A$ , and the set $$ \left\{ A P + P A^{T} \mid P \succ 0 \right\} $$ Is this set open under the standard metric (the one induced by Frobenius norm) in the space of all symmetric matrices $S^n$ ? I think this is true if and only if $A$ is invertible. I think if we assume $A$ is invertible, very roughly speaking, a small perturbation $\epsilon$ in the value of $AP +PA^{T}$ can be obtained by $AP$ and $PA^{T}$ each perturbate at most $\epsilon$ amount. Since $A$ is invertible, $P$ can also perturbate small enough with the choice of $\epsilon$ . Since the eigenvalues change continuously, we can change a tiny amount so that all eigenvalues of $P$ remain positive. I am not sure, however, if $A$ being invertible is required for the set to be open, and I'm also having a little trouble making my argument precise. Also, in general, for $$\left\{\begin{bmatrix}
A_{1}P+PA_{1}^{T} \\
\vdots \\
A_{m}P + P A_{m}^{T}
\end{bmatrix} : P \succ 0\right\}$$ Under the standard metric, i.e., sum the difference between each entry square root. When is this open in the space of vectors of m symmetric matrices? I think if each $A_{i}P+PA_{i}^{T}$ is open, in the product space equipped with this standard product metric, the set is open as well. However, I'm not sure about this or whether there exists open sets such that not every A is invertible.","['positive-semidefinite', 'semidefinite-programming', 'control-theory', 'matrices', 'linear-algebra']"
4713764,"Proof: Let $x$, $y \in Q$, $y > 0$, $x > 1$. Then there is an integer $n$ such that $x^n < y ≤ x^{n+1}$.","I want to know if my proof of the following result is correct: Let $x$ , $y \in Q$ , $y > 0$ , $x > 1$ . Then there is an integer $n$ such that $x^n < y ≤ x^{n+1}$ . Proof: Suppose I have proved that ""let $x$ , $y \in Q$ , $y > 0$ , $x > 1$ . Then there is a positive integer $n$ such that $x^n > y$ ."" By the above result, there is a positive integer $m_0$ such that $x^{m_0} > y$ . Consider the set $S = \{n \in \mathbb{N}: y > x^{m_0 - n}\}$ . Again, by the above result, there exists a positive integer $m$ such that $1/y < x^m$ . Then $x^{-m} < y < x^{m_0}$ . Thus, $m_0 + m \in S$ where $m_0 + m \in \mathbb{N}$ . By the well-ordering principle, $S$ has a minimal element $m_1$ . Then I claim: $x^{m_0 - m_1} < y \leq x^{m_0 - m_1 + 1}$ . Indeed, $m_1 \neq 0$ since $0 \notin S$ . Thus, $m_1 - 1 \in \mathbb{N}$ , and so if $y > x^{m_0 - m_1 + 1}$ , then $m_1 - 1 \in S$ , contradicting the minimality of $m_1$ . Now we can conclude by letting $n = m_0 - m_1$ .","['solution-verification', 'rational-numbers', 'analysis', 'real-analysis']"
4713774,Is there an underlying graph-theory representation of Sudoku solutions?,"I have been puzzling for some time about how a completed 9x9 Sudoku solution can be represented mathematically, and how that mathematical representation can be used to enumerate the different mathematically unique arrangements of the cells. I am only a high-school level mathematician, so my understanding of the subject area and its possible solutions may be a stretch, but nevertheless I would be interested in whether anyone has done work on this subject. I understand that a sudoku board can be considered as just a 2-d representation, for convenience, of an underlying graph, or maybe hypergraph, which more fundamentally represents the true structure, i.e. the relationships between the cells, of the board. However, a completed Sudoku board can be rearranged by swapping rows within each 3-row group, swapping columns within each 3-column group, swapping complete 3-row or 3-column groups around, rotating and mirroring the board, and even switching the numbers around, for example changing all the 6 cells to 3 and 3 cells to 6. What I am looking for is a representation of the underlying canonical ""shape"" of the board which is the same regardless of the transformations. All the transformations are simply presentational changes in the way the underlying unique shape is projected onto a 2-d surface. The problems I am struggling with are: a) How can I determine the canonical underlying mathematical definition, i.e. the ""shape"", of any given completed Sudoku board? b) How many unique Sudoku ""shapes"" are there? It seems that most articles or papers that discuss Sudoku look at the maths of the puzzle process, i.e. setting and solving, rather than the maths of the underlying solution. In this case, puzzles that actually represent the same underlying shape (i.e. one puzzle can be rearranged, or ""transformed"", to the other) are considered as distinct puzzles - not as two instances of the same puzzle. In respect of problem a), I have made some attempts at defining the rules for transforming a completed board into its underlying canonical mathematical form. I have tried developing an algorithm that classifies the relative positioning of cells of the same number and different numbers across the board (ignoring the number itself) so that there is a representation of  the ""shape"" (as a graph/network) that does not change even if the 2-d board is rearranged, e.g. by swapping rows and columns or rotating/flipping the board. I have not succeeded (yet). I guess combinatorics will come into the answer to b), but I have not even attempted that. What interests me here is that maybe there are relatively few underlying unique shapes and, if so, perhaps it would be fun to apply the process identified in a) to thousands or millions of completed grids to see how many of their underlying canonical shapes can actually be found. Also, maybe there are classes of different types or families of shape that have different mathematical or topological properties. If anyone has any suggestions or can point me in the direction of any work already done in this area I would be very grateful.","['graph-theory', 'sudoku', 'general-topology', 'combinatorics']"
4713788,"Difference between $L^2[0,2\pi]$ and $L^2(S^1)$","I'm cofused with the definition of the spaces $L^2(S^1)$ and $L^2[0,2\pi]$ . I have read that we can consider $L^2(S^1)$ as the elements $f \in L^2[0,2\pi]$ such that $f(0)=f(2\pi)$ . But  in my opinion the latter does not have sense because in $L^2[0,2\pi]$ we can't talk about point values. Could you explain me what is the difference between $L^2(S^1)$ and $L^2[0,2\pi]$ , please? I will appreciate if you can give me a reference related to this question.","['measure-theory', 'fourier-analysis', 'operator-theory', 'fourier-series', 'spectral-theory']"
4713834,"Under which condition $d(y)=\min_x f(x,y)$ is smooth ($C^1$) (similar to Danskin's theorem))?","Under which condition $d(y)=\min_x f(x,y)$ is smooth (differentiable, $C^1$ )? To determine the smoothness of $d(y)$ , I think the following conditions are required: Differentiability: The function $f(x, y)$ is smooth with respect to both $x$ and $y$ in the feasible set. This ensures that the partial derivatives of $f(x, y)$ exist and can be used to find the minimum. Then, is this condition enough to guarantee that $d$ is smooth? For example, let $x\in\mathbb{R}_+$ , $y\in\mathbb{R}^n$ and $$ f(x,y)=x + \frac{\|y\|_2^2}{x}.$$ Then $d(y) = 2\|y\|_2$ is not smooth at zero point. Which conditions should we add next? What if we add the Lispchitz continuous? A similar result for the convex case can be found here . I would rather consider a special case $$ d(y) = \min_x f(x,y) + g(x),$$ where $g$ is a strongly convex function, Which conditions should we add? If the solution to $$\min_x f(x,y)+ g(x)$$ is unique, can we use implicit function theorem to obtain the smoothness of $d$ ?","['convex-optimization', 'smooth-functions', 'calculus', 'optimization', 'derivatives']"
4713851,summation of a weighted product of binomial coefficients,Does anyone know how to prove that $$\sum\limits_{k=0}^i (-1)^k \frac{\binom{m-k}{k}}{(m-k)}\frac{\binom{m+2i-2k}{i-k}}{(m+2i-2k)}=0$$ for all $1\leq i\leq\lfloor\frac{m}{2}\rfloor$ ? A calculation by Macaulay2 shows that $$\sum\limits_{k=0}^i (-1)^k \binom{m-k}{k}\binom{m+2i-2k}{i-k}=\sum\limits_{k=0}^i\binom{2i}{k}$$ but I don't know how to prove it neither and I don't know if it helps.,"['summation', 'binomial-coefficients', 'combinatorics']"
4713853,Is every Euclidean tensor an affine tensor?,"Let $F(M)$ be the first order frame bundle on a $C^\infty$ smooth real manifold $M$ of dimension $m$ , and $O(M)$ the bundle of all orthonormal frames. $F(M)$ is a $GL(m)$ -principal bundle, and $O(M)$ is an $O(m)$ -principal bundle. In the following, I use $G$ to denote either $GL(m)$ or $O(m)$ and $P$ to denote the total space $F(M)$ or $O(M)$ respectively, of the principal fibre bundle $(F(M),p,M,GL(m))$ or $(O(M),p,M,O(m))$ , respectively. So the general notation of the PFB is $(P,p,M,G)$ . Let $V$ be a real vector space and $\rho: G \to GL(V)$ a linear representation of $G$ on $V$ . A smooth map $f: P \to V$ is called a "" $G$ -tensor of type $(V,\rho)$ on $M$ ,"" if $\forall g \in G, u \in P: f(u.g) = \rho(g^{-1})(f(u))$ Now for my question : Does every $O(m)$ -tensor of type $(V,\rho)$ on $M$ lift to a $GL(m)$ -tensor of type $(V\!\uparrow_{O(m)}^{GL(m)}\, , \rho\!\uparrow_{O(m)}^{GL(m)}\, )$ for the induced representation $\rho\!\uparrow_{O(m)}^{GL(m)}$ of the group $GL(m)$ on $V$ ? In other words, does every Cartensian (Euclidean) tensor field on $M$ lift to an affine tensor field on $M$ ? My guess is: Yes. Somehow, because $O(m)$ is the maximal compact subgroup of $GL(m)$ . But is there an ""easy"" proof, or a reference to the literature? Thank you very much! Remark: Maybe just consider the associated bundles, instead of sections, and use an extension of structure group: Let $E$ be the associated vector bundle to $(O(M),p,M,O(m))$ with typical fibre $V$ obtained from the action $\rho$ of $O(m)$ on $V$ . Given the group homomorphism (embedding) $\iota: O(m) \hookrightarrow GL(m)$ , apply the ""extension of structure groups functor"" $GL(M) \times_{M,\iota, O(m)} \, (.) $ to the bundle $E$ , and then prove that $GL(M) \times_{M,\iota, O(m)} E $ is isomorphic, as real vector bundles over $M$ , to $E$ ? ========= Edit 1, one day later, due to some findings: The following theorem can be found in a compilatory master thesis by Sandon : So, if the left action $\rho: O(m) \to Aut(V)$ on $V$ is the restriction to $O(m)$ of a left action $GL(m) \to Aut(V)$ on the same vector space $V$ , then, indeed, the (associated) vector bundles (over $M$ ) $F(M) \times_{GL(m)} V$ and $O(M) \times_{O(m)} V$ are isomorphic (as vector bundles over $M$ ), and have corresponding smooth sections. Therefore, for such $O(m)$ -actions $(\rho\downarrow^{Gl(m)}_{O(m)} \, ,V)$ , which are a restriction of a $GL(m)$ action $\rho$ , every $O(m)$ -tensor of type $(\rho\downarrow^{Gl(m)}_{O(m)},V)$ indeed stems from a $GL(m)$ -tensor of type $(\rho,V)$ , Cartensian and affine tensors are the same. I guess, the general case can now be settled by using Frobenius reciprocity, universal property of the induced action, and/or Mackey machinery. Because a given left $O(m)$ action can be considered the restriction to $O(m)$ of its $GL(m)$ -induction.","['principal-bundles', 'tensors', 'differential-geometry']"
4713873,"Are there any results associated with $\int_{0}^{n} f(x)f(x+1) \cdots f(x+n) \,dx$?","I recently found a YouTube video that solves the following integral: $\int_{0}^{1} \ln(x)\ln(x+1) \,dx$ This made me curious if we could generalize the integral and if there are any results or bounds associated with it. Is $\int_{0}^{n} f(x)f(x+1) \cdots f(x+n) \,dx$ guaranteed to have a closed-form solution, if we assume that $f:[a,b]\rightarrow \mathbb{R}$ is continuous? This is my first post so my apologies if there's something wrong with my post.","['integration', 'calculus', 'functions']"
4713881,Number of ways of cutting a squared grid into connected components,"Let $G = (V, E)$ be a squared grid graph, i.e. $V = \{1, \ldots, n\}^2$ for some $n \in \Bbb N$ and $ (u,v) \in E \iff d_{\ell_2}(u,v) = 1$ .
In how many ways can we partition $V$ into two disjoint sets $V_1, V_2$ such that the graphs induced by $V_1$ and $V_2$ are connected? If we let $N_k$ the the number of ways in which $V$ can be partitioned such that $V_1$ has cardinality $k$ we can see that $N_1 = n^2, \; N_2 = 2 n (n-1)$ . I think $N_3 = 2n(n-2) + 4(n-1)^2 $ , and already $N_4$ seems very hard to compute. What is the solution?","['graph-theory', 'combinatorics']"
4713895,Brezis' exercise 6.1: a compact operator in $\ell^p$,"I'm trying to solve an exercise in Brezis' Functional Analysis Let $E:= \ell^p$ with $p \in [1, \infty]$ . Let $(\lambda_n)$ be a bounded sequence in $\mathbb R$ . We consider a bounded linear operator $T:E \to E$ defined by $$
Tx := (\lambda_1 x_1, \lambda_2 x_2, \ldots), 
$$ for $x = (x_1, x_2, \ldots) \in E$ . Prove that $T$ is compact iff $\lambda_n \to 0$ . There are possibly subtle mistakes that I could not recognize in below attempt. Could you have a check on it? Thank you so much for your help! Proof Let $B$ be the closed unit ball of $E$ . First, we prove the direction "" $\impliedby$ "". Because $F$ is complete, it suffices to prove that $T(B)$ is totally bounded. Fix $\varepsilon>0$ . There is $m \in \mathbb N$ such that $|\lambda_n| < \varepsilon$ for all $n \ge m$ . Then for $x,y \in B$ , $$
\begin{align}
\sum_{n=m}^\infty |\lambda_n x_n-\lambda_n y_n|^p &=\sum_{n=m}^\infty \lambda^p_n |x_n-y_n|^p \\
&\le \varepsilon^p \sum_{n=m}^\infty |x_n-y_n|^p \\
& \le \varepsilon^p \|x-y\|_p^p  \\
&\le \varepsilon^p 2^p (\|x\|^p_p + \|y\|^p_p) \\
&\le 2^{p+1}\varepsilon^p,
\end{align}
$$ and $$
\sup_{n \ge m} |\lambda_n x_n-\lambda_n y_n| \le \varepsilon \sup_{n \ge m} |x_n- y_n| \le 2 \varepsilon.
$$ It remains to prove that $$
C :=\{ (\lambda_1 x_1, \lambda_2 x_2, \ldots, \lambda_{m-1} x_{m-1}, 0, 0, \ldots) : x \in B \}
$$ has compact closure. This is indeed true because $C$ is bounded and is contained in a finite-dimensional subspace of $E$ . Second, we prove the direction "" $\implies$ "". We define $x^m \in B$ by $(x^m)_n :=1$ if $n=m$ and $0$ otherwise. Let $y^m := T x^m$ Then $(y^m)_n =\lambda_m$ if $n=m$ and $0$ otherwise. Then $\|y^m\|_p = |\lambda_m|$ . We have $T(B)$ has compact closure and thus $\{y^m :m \ge 1\}$ is bounded. The claim then follows. Update: @Ryszard pointed out in a comment that I incorrectly quoted the exercise and thus my proof for the direction "" $\implies$ "" is incorrect. Could you help me prove this direction?","['lp-spaces', 'solution-verification', 'compact-operators', 'functional-analysis']"
4713908,Is there an infinite dimensional inner product space without an orthogonal Hamel basis?,"I want to know if there exists an (real or complex) vector space $X$ with infinite dimension and an inner product $\langle\cdot,\cdot\rangle$ such that there is no orthogonal Hamel (algebraic) basis of $X$ . I do not seek conditions on to the topological aspects of completeness nor an example Schauder Basis or a Maximal Orthogonal System in Hilbert Spaces. I am aware that this is not usually discussed in the scenario of infinite dimensional vector spaces, but this is exactly what I'm curious about. I know how to prove that every inner product space of Hamel dimension $\aleph_0$ has an orthonormal Hamel basis using the Gram-Schimidt process but this proof does not work for the uncountable case. I also know that a maximal orthogonal set of non-zero vectors is not always a Hamel basis, thus a simple application of Zorn's Lemma does not seem to solve the problem. Finally, I have seen many other similar questions here but none of them provide an answer for the question posed here.","['inner-products', 'linear-algebra', 'hamel-basis']"
4713916,How to detect if a function has symmetries,"let's say we have the function $$y = \frac{x}{x^2+1}$$ we see that y': $$y' =\frac{-x^2+1}{(x^2+1)^2}$$ by the second derivative test, we see that the points $x=1$ is a local maximum and $x=-1$ is a local minimum that's the only clue I have about the symmetry of this graph.","['calculus', 'functions', 'symmetry']"
4713919,Subbundle of a pullback is not the pullback of a subbundle,"Suppose $p\colon E\to X$ is a vector bundle of finite rank and let $f\colon Y\to X$ be a continuous map. My professor claimed that subbundles of the pullback $f^*(E)$ are not
themselves necessarily pullbacks of subbundles of $E$ along $f$ . Why is this case? Can someone provide an example of when the words ""subbundle"" and ""pullback"" fail to commute in this manner? The total space of the pullback is defined to be $$\{(y,v)\in Y\times E\colon f(y)=p(v)\}.$$ The total space of a subbundle $S$ lives inside this set. Is there no canonical way to identify bases of fibers in the pullback $f^*(E)$ with bases of fibers in $E$ itself so that the subbundle $S$ can be identified with a subbundle of $E$ ? For context: I was studying Chern classes in the setting of algebraic geometry. I saw that for a vector bundle $E$ on a scheme $X$ , a certain pullback of the vector bundle, $f^*(E)$ , admits a ""Chern decomposition"" into a product involving the Chern roots. I believed that this will also imply there exists a Chern decomposition of $E$ itself but apparently this is not the case. The obstruction to this is that subbundles of $f^*(E)$ may fail to be pullbacks of subbundles of $E$ . Comments on this topic are also welcome.","['geometric-topology', 'vector-bundles', 'algebraic-geometry']"
4713932,Cancellation of elements in the Gromov boundary of a finitely generated free group,"This question is inspired by this post from MO Let $A$ be a finite set of free generators and their inverses and $F$ the free group generated by elements in $A$ (some call $A$ the alphabet of $F$ ). For each $g\in F$ , use $\vert\,g\,\vert$ to denote the length of $g$ . The Gromov boundary of $F$ , denoted by $\partial F$ , can be viewed as the set of words that consists of letters from $A$ and has infinite length starting from the left. For instance, $a_1 a_2 \cdots $ is the general form of elements in $\partial F$ where $a_i\in A$ and $a_i a_{i+1}\neq e$ for each $i\in\mathbb{N}$ . Set $\overline{F} = F\cup\partial F$ . Given $x, y\in\overline{F}$ , if $x\neq y$ , let $x\wedge y$ denote the common part between $x$ and $y$ starting from the left. For instance: $ (a_1 \cdots a_n )\wedge (a_1 \cdots a_n b_{n+1} b_{n+2} \cdots) = a_1 \cdots a_n$ . Define a function $d: \overline{F}\times\overline{F}\rightarrow(0, 1)$ as follows: $d(x, y)=0$ if $x=y$ and $d(x, y)= \operatorname{exp}(-\vert\,x\wedge\,y\,\vert)$ . One can check $(\overline{F}, d)$ is a compact metric space and $d$ is an ultrametric. From now on, we assume $\overline{F}$ is equipped with the $d$ -metric topology. An action of $F$ on $\partial F$ can be defined as follows: given $g\in F$ and $x\in\partial F$ , $g\cdot x = gx$ (after cancellation). Given $\gamma\in\partial F$ , for each $i\in\mathbb{N}$ , let $\gamma_i$ denote the $i$ -th letter of $\gamma$ and define $[\gamma]_i = \gamma_1 \cdots \gamma_i$ . Obviously, for each $\gamma\in\partial F$ , we have $\gamma = \lim_i [\gamma]_i$ . From now on we only consider $\gamma\in\partial F$ such that $\gamma \neq \lim_n gh^n$ for any $g, h\in F$ . Then define: $$
C_{\gamma} = \overline{ \big\{ [\gamma]_n^{-1} \big\}_{n\in\mathbb{N}} } \cap\partial F
$$ which is the set of clustered points of the sequence $\big\{ [\gamma]_n^{-1} \big\}$ and, by compactness of $\overline{F}$ , is non-empty. My question are: Is $C_{\gamma}$ countable regardless of the choice of $\gamma$ ? If not, is the set $\big\{ \gamma\in\partial F \,\vert\, C_{\gamma} \text{  uncountable  } \big\}$ countable?","['metric-spaces', 'hyperbolic-geometry', 'geometric-group-theory', 'group-theory', 'dynamical-systems']"
4713940,Height of an object given two projections,"So have to measure the height of an object $c$ given two known projections ( $a$ and $b$ ). The angle $\phi$ between the two projections is known (it is 85.5°) What is the length of $c$ ? (in terms of $\phi$ , $a$ and $b$ ) Before I was using the Pythagorean theorem, since the angle was thought to be 90°. Upon inspection it is not exactly 90° and thus the height of c is being underestimated. My gut feeling is I need to use the Law of Cosines, but I can't seem to figure out how. Notes: The object $c$ can be rotated like in the picture. The angle of rotation is unknown. $\alpha$ and $\beta$ angles are not given, but their sum equals $\phi$",['trigonometry']
4713949,How to know if a function has asymptotes,Let's say we have the function : $$y = \frac{x}{x^2+1}$$ I want to know if the function has vertical or horizontal asymptotes what is the algorithm to find if a function has asymptotes? I know that the function has a local maximum at x=1 and a local minimum at x =-1 and it is symmetrical about the origin,"['calculus', 'algebra-precalculus']"
4713995,On the conditions of convergence in the generalized Riemann-Lebesgue lemma,"The following generalizations of the Riemann-Lebesgue lemma are rather well known (see for example the paper Kahane, C. S., Generalizations of the Riemann-Lebesgue and Cantor-Lebesgue lemmas , Czechoslovak Mathematical Journal, Vol 30 (1980), No. 1, 108-117): Theorem I: Suppose $g\in L_\infty([0,\infty),m)$ . A necessary and sufficient condition for \begin{align}
I(f; g):=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(\lambda t)\,dt\tag{0}\label{zero}
\end{align} to exists for every $f\in L_1([0,\infty),m)$ is that $g$ has the mean value property, i.e. \begin{align}
I(g)=\lim_{T\rightarrow\infty}\frac1T\int^T_0 g(t)\,dt\tag{1}\label{one}
\end{align} exists. Furthermore, when this is the case, \begin{align}
I(f; g)=\Big(\int^\infty_0 f(t)\,dt\Big) I(g)\tag{2}\label{two}
\end{align} This extends to $L_1(\mathbb{R},m)$ as follows: Corollary I: Suppose $g\in L_\infty(\mathbb{R},m)$ . A necessary and sufficient condition for \begin{align}
I(f; g):=\lim_{\lambda\rightarrow\infty}\int_\mathbb{R} f(t) g(\lambda t)\,dt\tag{3}\label{three}
\end{align} to exists for every $f\in L_1(\mathbb{R},m)$ is that $g_-(t)=g(-t)\mathbb{1}_{[0,\infty)}(t)$ and $g_+(t)=g(t)\mathbb{1}_{[0,\infty)}(t)$ have the mean value property.  If  this is the case, \begin{align}
I(f; g)=\Big(\int^0_{-\infty} f(t)\,dt\Big) I(g_-) + \Big(\int^\infty_0 f(t)\,dt\Big) I(g_+)\tag{4}\label{four}
\end{align} Perhaps the  most common version of  these results is the case where $g\in L_\infty(\mathbb{R},m)$ and $g$ is $P$ -periodic function ( $g(x+P)=g(x)$ for all $x$ and some $P>0$ ). Then  \eqref{four} takes the form \begin{align}
\lim_{\lambda\rightarrow\infty}\int_{\mathbb{R}}f(t)g(\lambda t)\,dt=\Big(\frac{1}{P}\int^P_0 g(t)\,dt\Big)\int_\mathbb{R} f(t)\,dt\tag{5}\label{five}
\end{align} The assumption that $g\in L_\infty(\mathbb{R},m)$ can be relaxed by considering integrals over finite intervals and some duality assumptions: Theorem II: Suppose $g\in L^{loc}_q([0,\infty),m)$ , $q>1$ , and let $0\leq a< b<\infty$ . For the limit \begin{align}
 I(f; g,[a,b]):=\lim_{\lambda\rightarrow\infty}\int^b_a f(t) g(\lambda t)\,dt\tag{6}\label{six}
\end{align} to exists for every $f\in L_p([a,b],m)$ , $\frac1p+\frac1q=1$ , it is necessary and sufficient that (i) $\frac1T\int^T_0|g(t)|^q\,dt=O(1)$ as $T\rightarrow\infty$ , (ii) $g$ has the mean value property \eqref{one}. If (i) and (ii) hold, then the limit \eqref{six} takes the form \begin{align}
I(f; g, [a,b])=\Big(\int^b_a f(t)\,dt\Big) I(g)\tag{7}\label{seven}
\end{align} The validity of the results presented above is based on duality between $L_p$ spaces.  Holder's inequality provides uniform bounds on some linear operators,  uniform boundedness principle and density arguments then give the desired results. Question: Motivated by a recent posting , I would like to know if there is a counterexample to Theorem I if the assumption $g\in L_\infty([0,\infty),m)$ is relaxed while maintaining the integrability assumption for $f$ . To be more precise and to simplify matters, Are there measurable functions $g$ and $f$ , such that, $g$ is $1$ -periodic, $g\in L_1([0,1],m)\setminus  L_\infty([0,1],m)$ , $f\in L_1(\mathbb{R},m)$ for any $\lambda>0$ , $t\mapsto f(t) g(\lambda t)\in L_1(\mathbb{R},m),$ but either $I(f; g)=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(t\lambda)\,dt$ does not exist (as a real number), or if it does, $I(f; g)\neq I(g) \int^\infty_0 f$ . If anybody knows a counterexample or may be able to build one, that would also be fantastic!","['integration', 'fourier-analysis', 'lebesgue-integral', 'analysis', 'real-analysis']"
4714049,A limit problem That may be Solvable by using Probability Theory,"$$\lim_{n \to \infty}  \frac{\sum_{ k= 0}^{n-1} \prod_{i=0}^k (1-\frac{i}{n})  }{\sqrt{n}}$$ This problem arises from a probability question, which was inappropriately approximated into this form. After conducting computer simulations, the approximate result for this problem is around 1.25289. My classmate suggested that this can be viewed as randomly selecting one box out of n boxes without replacement until an empty box is chosen. They claimed that the expected number of balls can be transformed into an integral, possibly something like $$\int_{0}^{\infty} e^{-\frac{t^2}{2}} dt $$ , which resembles $\sqrt{\frac{\pi}{2}}$ . The computer simulations yielded a similar result. I would like to know if this is correct or if there is a more rigorous approach.","['limits', 'analysis', 'probability']"
4714186,A geometrical puzzle involving calculus,"Some time ago I stumbled across a problem from the Putnam Mathematical Competition. I could not find it, but I remember the text quite well. There are two vectors: a =(10, $y$ ) and b =( $x$ ,10), where $0 ≤ x ≤ 10$ , and $0 ≤ y ≤ 10$ . We have to compute the probability of these two vectors forming a parallelogram with an area A ≥ 50. The hint in the text is that the probability can be expressed as $$\ln(\sqrt a) + \frac{b}{c}$$ (where $a, b, c \in \mathbb{N}$ ; $b$ and $c$ are coprime and $a$ is as smaller as possible). Here is a visual representation for the problem : First, I calculated the area of the parallelogram: $A = ||\vec{a} \times \vec{b}|| = xy - 100$ , which implies that $0 ≤ A ≤ 100$ . We can write the inequality $A ≥ 50$ as $xy ≤ 50$ . If we assume that $y$ is a given (we could repeat this reasoning for $x$ ), then $x ≤ \frac{50}{y}$ . If $y ≤ 5$ then all $x$ values $≤ 10$ are acceptable; therefore the probability of $A ≥ 50$ is at least $\frac{1}{2}$ . When $y ≥ 5, x ≤ 50$ ; so when $y$ increases the portion of acceptable $x$ values decreases. I calculated $$\int_{5}^{10} \frac{50}{y} \cdot \frac{1}{10} \ dy$$ to count these values. The idea is that for every $y$ value greater than 5 the probability of $x$ values being less than $\frac{50}{y}$ is $\frac{\frac{50}{y}}{10}$ , where 10 is the segment of all possible $x$ values. The integration yields $5 \ln (2)$ . Using the law of total probability I wrote: $$P(A \geq 50) = P(y < 5) \cdot P(A \geq 50 | y < 5) + P(y \geq 5) \cdot P(A \geq 50 | y > 5)$$ $$P(A \geq 50) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot (5 \ln(2)) \approx 2.23.$$ But why is the probability greater than 1?","['calculus', 'linear-algebra', 'geometry', 'geometric-probability']"
4714268,Reference request for books that cover topics similar to Rolfsen's Knots and Links but are more detailed.,"I have been told to study the mapping class group of the torus for a summer project from Dale Rolfsen's Knots and Links (Chapter 2-D) in preparation of studying 3-manifolds. However Rolfsen proof of the main theorem of the section, that $Aut(T^2)$ modulo ambient isotopy is isomorphic to $GL(2,\mathbb Z)$ has lot of gaps to be filled in as exercises. I lack the familiarity required with the material to be able to do this in a timely manner on my own. Hence I am looking for resources (books, lecture notes, video lectures etc.) that explain this theorem in detail and would help me in filling the gaps in Rolfsen's proof. My advisor told me that we're not looking to study knot theory, we just need these tools to study 3-manifolds so I would also appreciate recommendations on books that cover similar content to Rolfsen's book but are more detailed.","['book-recommendation', 'geometry', 'reference-request', 'geometric-topology', 'low-dimensional-topology']"
4714291,Evaluate $\lim_{n \to \infty} n \prod_{m = 1} ^ n \left(1 - \frac1m + \frac5{4m ^ 2}\right)$,"Evaluate $\lim_{n \to \infty} n \prod_{m = 1} ^ n \left(1 - \frac1m + \frac5{4m ^ 2}\right)$ Attempt $1$ : $$1 - \frac1m + \frac5{4m ^ 2}=\frac{4m^2-4m+5}{4m^2}=\frac{(2m-1)^2+4}{4m^2}=\left(1-\frac1{2m}\right)^2+\frac1{m^2}$$ Don't know what to do with it. Attempt $2$ : Trying to write the terms of the product to see if there is any pattern. $$\prod_{m = 1} ^ n \left(1 - \frac1m + \frac5{4m ^ 2}\right)=\frac54\cdot\frac{13}{16}\cdot\frac{29}{36}\cdot\frac{53}{64}\cdots$$ The numerator terms have differences $8,16,24$ ... Don't know what to do with it.","['contest-math', 'limits', 'calculus']"
4714292,Global sections of a vector bundle on the projective space,"This comes from a problem I'm working on, but I would like to phrase it in generality. I consider a vector bundle $E$ on the complex projective space $\mathbb{P}^2$ . Is there some criterion to decide whether $E$ admits a global section or not, for example by looking at chern classes of $E$ , or the determinant of $E$ ... For sure it's easy to answer when the vector bundle splits, $E=\oplus_i \mathcal{O}(d_i)$ , and I know the $d_i$ 's, but I'm unable to find an analogy for a non-splitting vector bundle. Hirzebruch-Riemann-Roch may help, that's why I'm thinking about chern classes: if $\chi(E)$ is positive, I know that either $E$ or $(E^*\otimes \mathcal{O}_{\mathbb{P}^2}(-3))$ is effective, but then I don't know how I can choose which one is effective. On $\mathbb{P}^1$ everything is easy, since any vector bundle splits on it, but this is already not true for the projective plane. Maybe some classification result could help? Thank you!","['algebraic-geometry', 'homology-cohomology', 'geometry', 'projective-space']"
4714318,Lower bounding sum of cosines,"Let $N$ be an odd integer $\geq 3$ . I am interested in finding a lower bound for $$\min \vert{\cos(\frac{2\pi k}{N})}+\cos(\frac{2\pi l}{N})-\cos(\frac{2\pi m}{N})-\cos(\frac{2\pi n}{N})\vert,$$ where $k,l,m,n \in \{0,\dots, N-1\}$ and $\{k,l\}\not\subset \{m,n,N-m, N-n\}$ (without a comparable assumption the above quantity is trivially zero). Numerically, one sees (at least for reasonably large $N\sim 100$ ) that this quantity is strictly greater than zero, and that it decreases with increasing $N$ (although not in a monotone fashion). Does anybody have an idea of how one could go about lower bounding this quantity? I am also interested in finding an argument for why it is strictly greater than zero. Thanks!","['trigonometry', 'discrete-mathematics', 'upper-lower-bounds']"
4714360,Let $n \in \mathbb {N}$ and $G$ a group of order $4n+2$. Prove $G$ isn't simple.,"I have finished writing the proof and I'm afraid I made a mistake on the way because I'm not sure I can deduce the bottom line of the proof. Here it goes: Let us examine the action of $G \mapsto G$ on itself by multiplication from the left. This action induces a homomorphism $\phi: G \to S_{|G|}$ . From Cayley's theorem we have $G  \cong Im\phi \le S_{4n+2}$ . Let us mark $H = Im\phi$ . We know $A_{4n+2} \lhd S_{4n+2}$ and from Diamond theorem we have $H \cap A_{4n+2} \lhd H$ . Let us mark $H \cap A_{4n+2} = N$ . We want to show $N \lt H$ and also $N$ isn't trivial. From Cauchy's theorem for $p=2$ : exists $g\in H$ such that $|g| = 2$ . Let us assume $g$ is constructed by foreign cycles marked $c_i$ for $1 \le i \le m$ for some $m \in \mathbb N$ . Then we have $g = c_1 ... c_m$ . We know $|g| = {\rm lcm}(c_i)$ and since $|g| = 2$ we have $\phi(g)$ is a product of 2-cycles. Notice $|g|=2 \implies g \neq e$ and therefore for all $g^{'} \in G$ we have $gg^{'} \neq g$ meaning $\phi(g)$ doesn't ""leave"" any object in it's place. Therefore we have $\phi(g)$ is a product of exactly $2n+1$ 2-cycles and this implies $\phi(g) \notin A_{4n+2}$ and therefore $N \lt A_{4n+2}$ . Now notice, for all $n \in \mathbb N$ exists some $p>2$ primal (and obviously odd) such that we have some $e \neq g \in H$ such that $|g| = p$ . In this case we have $\phi(g)$ is a product of oddsized-cycles and therefore $\phi(g) \in A_{4n+2}$ and therefore $N$ isn't trivial. This implies $N \lhd H$ a proper subgroup. Now this is the part I'm not sure I'm allowed to deduce Since we know $G \cong H$ we have some $H^{'} \lt G$ such that $H^{'} \cong N$ therefore meaning $H^{'} \lhd G$ a proper subgroup, thus implying $G$ isn't simple. Another problem I feel like I'm proving a stronger theorem since I could've used all the statements about $4n+2$ for every even number? or maybe I'm just mixed up. Thanks in Advance!","['group-isomorphism', 'normal-subgroups', 'abstract-algebra', 'solution-verification', 'group-theory']"
4714376,How to prove $\frac{\sqrt{5}+1}{2}>\log_23$?,"I want to compare the magnitude of $2\cos36^\circ$ and $\log_23$ without using calculators. Using $\sin72^\circ=\cos18^\circ$ , I can get $$4\sin18^\circ\left(1-2\sin^218^\circ\right)=1$$ then I can get $$2\cos36^\circ=\dfrac{\sqrt{5}+1}{2},$$ but I don't know how to compare the size of $\dfrac{\sqrt{5}+1}{2}$ and $\log_23.$","['algebra-precalculus', 'logarithms', 'analysis', 'inequality']"
4714382,Finite product of sines,"What is the origin of the formula $$ 
\prod_{k=1}^n \sin\theta_k=
\begin{cases}
\displaystyle
\frac{(-1)^{\lfloor\frac
        {n}{2}\rfloor}}{2^n}\sum_{e\in S}\cos(e_1\theta_1+\cdots+e_n\theta_n)\prod_{j=1}^n e_j &\text{if $n$ is even}, \\
\displaystyle
\frac{(-1)^{\lfloor\frac
        {n}{2}\rfloor}}{2^n}\sum_{e\in S}\sin(e_1\theta_1+\cdots+e_n\theta_n)\prod_{j=1}^n e_j &\text{if $n$ is odd}
\end{cases}
$$ where $e_1, \ldots, e_n\in \{-1, 1\}$ which can be found in the Wikipedia list of trigonometric identities ? Further questions As the related Wikipedia entry doesn't contain noting on these matters, could someone provide me a proof of this result? Moreover is it applicable for real computations?","['trigonometry', 'numerical-methods', 'real-analysis']"
4714424,"For what values of $r$ is $f(x,y,z)$ continuous on $\mathbb{R}^3$?","This was one of the problems in my textbook. I've tried searching for an answer but they are all blocked by a paywall! So I want to see if my approach is correct. The question asks: For what values of the number $r$ is the function $$f(x,y,z) = \begin{cases} \dfrac{(x+y+z)^r}{x^2+y^2+z^2} & (x,y,z)\neq(0,0,0) \\[1ex] 0 & (x,y,z) = (0,0,0) \end{cases}$$ continuous on $\Bbb R^3$ ? Here is my approach, is it correct? Keep in mind that this is a draft and I am aware that some parts may be unclear and require further justification. I just want to make sure that the general argument makes sense. Let $\mathbf{r}(t)=\langle f(t), g(t), h(t) \rangle$ be the path that the point $(x,y,z)$ goes along as it approaches $(0,0,0)$ . Thus, since $\mathbf{r} \neq \mathbf{0}$ the function becomes $$
f(\mathbf{r}(t)) =\frac{(f(t)+g(t)+h(t))^r}{f(t)^2 + g(t)^2 + h(t)^2}
$$ Notice that if $\mathbf{r}(t) \neq \mathbf{0}$ , then $f$ exists and the denominator in $f(x,y,z)$ is equal to $\|\mathbf{r}\|^2$ . Also notice that $f(t)+g(t)+h(t) = \mathbf{r}(t) \cdot \langle 1,1,1 \rangle$ . This in turn is equivalent to $\|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta$ . Thus $f$ becomes $$
f(\mathbf{r}(t)) =\frac{\left(\|\mathbf{r}(t)\| \cdot\sqrt{3} \cdot \cos\theta\right)^r}{\|\mathbf{r}(t)\|^2}
$$ Let $t_0$ be the number such that $\mathbf{r}(t_0) = \mathbf{0}$ . Recall that a function $f$ is continuous if $\lim_{\mathbf{x}\to \mathbf{a}} f(\mathbf{x}) = f(\mathbf{a})$ , so if $\lim_{t\to t_0} f(\mathbf{r}(t)) = 0$ then $f$ is continuous at $0$ ( $f$ is already continuous anywhere else, assuming that it's defined there), which would imply that $f$ is continuous on $\mathbb{R}^3$ . If $r<0$ then $f$ the limit cannot exist at $(0,0,0)$ (I will make this statement more exact in the real proof, I'm running out of time here). Thus it must be the case that $r>0$ so $(\sqrt{3} \cos\theta)^r$ must exist. So $$
\lim_{t\to t_0} \frac{\left(\|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta \right)^r}{\|\mathbf{r}\|^2} = (\sqrt{3}\cos\theta)^r \lim_{t\to t_0} \frac{\|\mathbf{r}\|^r}{\|\mathbf{r}\|^2}
$$ If we do the substitution $u=\|\mathbf{r}(t)\|$ ( $u>0$ so the limit approaches $0$ from both sides, just think of it as $|x|$ ) then the limit is $$
\lim_{u\to 0} \frac{u^r}{u^2} 
$$ It can be shown that (and I will show it in the real proof) if $r > 2$ then this limit exists and is equal to $0$ . Therefore if $r>2$ then $f(x,y,z)$ is continuous on $\Bbb R^3$ .","['multivariable-calculus', 'calculus', 'continuity']"
4714443,Relation between Ricci and Kähler forms on a cscK surface,"I'm studying the paper https://arxiv.org/abs/dg-ga/9506002 from LeBrun and I'm stuck on the the last part of the proof of Theorem 2.
The author claims that on a Kähler surface $(M,g,J)$ with constant scalar curvature $s$ , the Ricci and the Kähler form are related by $$
\rho^+ = \frac{s}{4}\omega
$$ where $\rho^+$ is the self-dual component of the Ricci form. I suppose it could be a consequence of some property of cscK surfaces but I haven't found any reference for this fact. Can someone gently help to enlight on this? Thank you for any advice or just for your time","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
4714455,Does the series $\sum\limits_{n=1}^{\infty }(-1)^n\left ( \sqrt{n^2+4n+1}-\sqrt{n^2+n+4} \right )$ converge?,Check for absolute and conditional convergence $$\sum_{n=1}^{\infty }(-1)^n\left ( \sqrt{n^2+4n+1}-\sqrt{n^2+n+4} \right )$$ My attempt: $$\sum_{n=1}^{\infty }(-1)^n\left ( \sqrt{n^2+4n+1}-\sqrt{n^2+n+4} \right )=$$ $$=\sum_{n=1}^{\infty }(-1)^n\left ( \sqrt{n^2+4n+1}-\sqrt{n^2+n+4} \right )\cdot \frac{\sqrt{n^2+4n+1}+\sqrt{n^2+n+4}}{\sqrt{n^2+4n+1}+\sqrt{n^2+n+4}}=$$ $$=\sum_{n=1}^{\infty }(-1)^n\frac{3n-3}{\sqrt{n^2+4n+1}+\sqrt{n^2+n+4}}$$ $$\frac{3n-3}{\sqrt{n^2+4n+1}+\sqrt{n^2+n+4}}=3\cdot \frac{n-1}{n\sqrt{1+\frac{4}{n}+\frac{1}{n^2}}+n\sqrt{1+\frac{1}{n}+\frac{4}{n^2}}}=\frac{3}{n}\cdot \frac{n-1}{n+2+\mathcal{O}\left ( \frac{1}{n} \right )+n+\frac{1}{2}+\mathcal{O}\left ( \frac{1}{n} \right )}=\frac{3}{n}\cdot \frac{n-1}{2n+\frac{3}{2}+\mathcal{O}\left ( \frac{1}{n} \right )}=\frac{3}{2}-\frac{27}{8n}+\mathcal{O}\left ( \frac{1}{n} \right )$$ I have found that the series does not converge and does not converge at all. Did I solve the problem correctly? Question: What can I say about conditional convergence?,"['calculus', 'sequences-and-series', 'summation', 'real-analysis']"
4714508,Coordinate bases: in what sense is $\partial_i=\frac{\partial}{\partial q^i}$ a basis vector?,"I have gotten comfortable working with coordinate bases but I still do not fully understand them.
Let $\mathbb{Q}$ be a smooth $n$ -manifold and $q=(q^1,\dots,q^n):Q\to \mathbb{R}^n$ some local coordinates on some region $Q\subseteq\mathbb{Q}$ . For this question, I will denote the associated $q^i$ basis vector fields as $\mathbf{e}_i\in\mathfrak{X}(\mathbb{Q})$ with dual 1-form fields $\pmb{\epsilon}^i\in\Omega^1(\mathbb{Q})$ .  We may write (locally) any $\mathbf{u}\in\mathfrak{X}(\mathbb{Q})$ in this basis in the usual way: $\mathbf{u}= \pmb{\epsilon}^i(\mathbf{u})\mathbf{e}_i = u^i\mathbf{e}_i$ . Rather than $\mathbf{e}_i$ , many sources would instead write $\partial_i$ or $\frac{\partial}{\partial q^i}$ . I get that, as a differential operator acting on functions, we have $\mathbf{u}[f] = \mathbf{d}f(\mathbf{u}) = u^i\frac{\partial}{\partial q^i} f$ . This is then often used to justify writing $\mathbf{u}$ itself as $u^i\frac{\partial}{\partial q^i}$ and this is what confuses me. In the expression $u^i\frac{\partial}{\partial q^i} f\in\mathcal{F}(\mathbb{Q})$ ,  the role of $\frac{\partial}{\partial q^i}$ is not confusing to me; I view it as the ""standard"" partial derivative in essentially the same way that any undergraduate student would view it in intro calculus. But I do not understand what type of object $u^i\frac{\partial}{\partial q^i}$ itself is supposed to be. my question: If $\mathbf{u}=u^i\mathbf{e}_i\in\mathfrak{X}(\mathbb{Q})$ is the actual tensor (expressed in a particular basis), is it accurate to write $\mathbf{u}=u^i\frac{\partial}{\partial q^i}$ ? If so, this implies $\frac{\partial}{\partial q^i} = \mathbf{e}_i \in \mathfrak{X}(\mathbb{Q})$ are the exact same object?  Or, is the use of $\frac{\partial}{\partial q^i}$ as a basis vector merely a slight abuse of notation alluding to the fact that $\mathbf{e}_i[f] = \frac{\partial}{\partial q^i}f$ ? In other words, if I insist on reserving the notation $\frac{\partial}{\partial q^i}$ for the ""standard"" partial derivative one encounters in basic calculus, is this same object also literally a vector field on the domain of the function $q^i$ ? Or, instead, does there exist some vector field $\mathbf{e}_i\in\mathfrak{X}(\mathbb{Q})$ on the domain of the coordinate $q^i$ whose action on a function is given by $\mathbf{e}_i[f] = \frac{\partial}{\partial q^i}f$ , but where $\mathbf{e}_i$ and $\frac{\partial}{\partial q^i}$ are not literally the same object?","['coordinate-systems', 'vectors', 'vector-fields', 'differential-geometry']"
4714533,Expected Tosses To Get THH sequence [duplicate],"This question already has answers here : Stuck on a probability problem/Expectation of coin toss (2 answers) Closed 11 months ago . What is the expected number of tosses of a fair coin to see a $THH$ sequence? Here is my approach: Here are the different cases: We get $THH$ straight away in $3$ throws (probability $\frac{1}{8}$ ). We first get $H$ (probability $\frac{1}{2}$ ), in which case we restart and number of tosses is $\mathbb{E}(X+1)$ We get $TT$ (probability $\frac{1}{4}$ ). In this case we need $\mathbb{E}(X ~|~ T)$ as first toss is a $T$ . We get $THT$ (probability $\frac{1}{8}$ ). Once again, in this case we need $\mathbb{E}(X~|~T)$ as first toss is a $T$ . Thus, putting this together: $$\mathbb{E}(X) = 3\cdot\frac{1}{8} + \mathbb{E}(X+1)\cdot\frac{1}{2} + \mathbb{E}(X~|~T)(\frac{1}{4}+ \frac{1}{8})$$ Now, let's find $\mathbb{E}(X~|~T) = \mathbb{E}(THH~|~T) = \mathbb{E}(Y)$ : We flip two $H$ immediately after the first $T$ (probability $\frac{1}{4}$ ) We flip a $T$ after the first $T$ (probability $\frac{1}{2}$ ), so we restart and the new expectation is $\mathbb{E}(Y)+1$ We flip a $H$ and then $T$ after the first $T$ to get $THT$ (probability $\frac{1}{4}$ ), and so we restart with expectation $\mathbb{E}(Y)+2$ Solving for $\mathbb{E}(Y)$ : $$\mathbb{E}(Y) = 2\cdot\frac{1}{4} + (\mathbb{E}(Y)+1)\cdot\frac{1}{2} + (\mathbb{E}(Y)+2)\cdot\frac{1}{4} 
     = 6$$ Plugging this back into the original equation for $\mathbb{E}(X)$ and solving we get $\mathbb{E}(X)= 6.25$ when the answer is $8$ . Can someone please explain where I went wrong?","['statistics', 'probability']"
4714545,Convergence/Divergence of a double integral,"Problem: Decide if the integral $$\iint_D \frac{x-y}{(x+y)^3}dxdy$$ where $D = \{(x,y) \in \mathbb{R^2}: x > 0, y > 0\}$ is convergent This is an old exam question where calculators are not allowed. My attempt was to use standard polar coordinates substitution $x = r\cos\theta, y = r\sin\theta$ and integrate $r$ from $0$ to $\infty$ and $\theta$ from $0$ to $\frac{\pi}{2}$ . The integral I got after the substitution is as follows. $$\int_{0}^{\infty} \frac{1}{r}dr\int_{0}^{\frac{\pi}{2}}\frac{\cos\theta - sin\theta}{(\cos\theta + \sin\theta)^3}d\theta$$ I then thought that the integral over $\theta$ converges to a constant C and the integral for the radius diverges, and therefore the total integral diverges. Out of curiosity i computed the $\theta$ integral and it turns out the value is $0$ . Now I am very unsure of my answer because my old integral in the $xy$ world turns out to be the following in the $r,\theta$ world. $$0 \cdot \int_{0}^{\infty} \frac{1}{r}dr$$ Now to my question. What is the value of this expression? Can I say that  the $r$ integral is divergent and therefore the expression is divergent, or is the expression above 0 and the  integral in the problem convergent?. The solution to the problem divided the first quadrant into two parts with the $y=x$ line, and studied one of these parts with polar coordinates and came to the conclusion that the integral diverges in one of these parts which leads to the main integral diverging. In their case, they didn't get 0 from their $\theta$ integral because they took $\theta$ from $0$ to $\frac{\pi}{4}$ . In my case, I got that specific integral to $0$ . Is my solution fine, and what should the answer look like with my method, or in general?","['integration', 'multivariable-calculus']"
4714626,Sets containing all their proper subsets,"I know that $\emptyset$ and $\{\emptyset\}$ contain all their proper subsets. I am trying to show that no other set satisfies this property, i.e. there is no set $x$ such that $\forall y \subsetneq x (y \in x)$ . Is this true? If so, how can I prove this?",['elementary-set-theory']
4714651,How many directed multigraphs with n vertices and edges up to isomorphisms,"How many graphs (not isomorphic to each other) with the same number of edges as vertices (sometimes called autographs). I'm interested in autographs where the edges are directed, and there may be 0 or more edges between any ordered pair of vertices. Ideally, I'd like a formula to calculate the number of such graphs for a given number of vertices/edges n. I have absolutely no idea how to find an algebraic formula for this, so I wrote some code to generate all the possible graphs and then to filter out the isomorphisms. This gives the first few terms in the series:
(EDIT: n = 4 is incorrect) # of autographs  1 1 6 31 1201
               n 0 1 2 3  4 Of course it's possible that there is a mistake in my code, so these numbers could be wrong. I did not find an entry for this sequence in the integer sequences page Update: As shared by pasthec it looks like the sequence is included here: https://oeis.org/A138107 1, 1, 6 ,31, 198, 1270, 8838,  63419 See Table 79 on page 32 of Statistics on Small Graphs by
Richard J. Mathar","['graph-theory', 'combinatorics']"
4714653,Prove that $a+b+c=0.$,"If $a, b, c$ are different to each other and $a^3+ax+y=0$ $b^3+bx+y=0$ $c^3+cx+y=0$ Prove that: $a+b+c=0.$ Solution in the list: Notice that $a,b,c$ are roots of the cubic polynomial $d^3+xd+y$ where $x,y$ are constants. Since $a,b,c$ are distinct numbers, they must be the three different roots of the polynomial. However, by Vieta's formulas, the sum of the roots of that polynomial is $\tfrac{-0}{2(1)}=0,$ meaning that $a+b+c=0.$ Question: How do I figure out that certain expressions are roots of polynomials, like this one?","['algebra-precalculus', 'polynomials']"
4714671,"Question about Rudin's PMA, Chapter 2 Exercise 2","I know that there are many questions related to this exercise that have been answered, and I have found a couple of different solutions to this exercise as well. But the solutions usually ignore the hint provided by Rudin, and even though some of them do deal with the hint, I am still not quite sure about what we should actually do with it. I wrote up my answer based on my understanding of those solutions used Rudin's hint. I want to know if this is correct. Thanks in advance for any clarification! The exercise goes like this: A complex number $z$ is said to be algebraic if there are integers $a_0$ , $\dots$ , $a_n$ , not all zero, such that \begin{equation}
a_0z^n + a_1z^{n-1} + \dots + a_{n-1}z + a_n = 0.
\end{equation} Prove that the set of all algebraic numbers is countable. Hint : For every positive integer $N$ there are only finitely many equations with \begin{equation}
n + |a_0| + |a_1| + \dots + |a_{n-1}| + |a_n| = N.
\end{equation} My attempt based on my understanding of those solutions used Rudin's hint: Consider the set $A$ of all algebraic numbers, where for each $z \in A$ there are integers $a_0$ , $\dots$ , $a_n$ , not all zero, such that \begin{equation}
a_0z^n + a_1z^{n-1} + \dots + a_{n-1}z + a_n = 0. \tag{1}
\end{equation} Now fix an $N \in \mathbb{Z}^+$ , and consider the subset $A_N$ of $A$ , where for each $z \in A_N$ there are integers $a_0$ , $\dots$ , $a_n$ , not all zero, such that (1) holds, and that \begin{equation}
n + |a_0| + |a_1| + \dots + |a_{n-1}| + |a_n| = N. \tag{2}
\end{equation} Since, by the hint, there are only finitely many such equations as (2); and since, by the Fundamental Theorem of Algebra, for each $(a_0, a_1, \dots, a_{n-1}, a_n)$ satisfying (2) there are $n$ roots, and thus finitely many roots, for the polynomial \begin{equation}
a_0x^n + a_1x^{n-1} + \dots + a_{n-1}x + a_n = 0, \tag{3}
\end{equation} we have that the set $A_N$ is a finite union of finite sets and hence is finite. Since $A = \bigcup_{N=2}^{\infty} A_N$ , we have $A$ is a countable union of finite sets and thus is countable, by the Corollary to Theorem 2.12. Is my answer correct and rigorous? I really appreciate it!","['proof-writing', 'real-analysis', 'solution-verification', 'algebraic-numbers', 'general-topology']"
4714686,"If a stick is broken at five random points, what is the probability that the pieces can form two triangles?","It is well-known that if a stick is broken at two random points, the probability that the pieces can form a triangle is $1/4$ ( proof ). My question is: If a stick is broken at five random points, what is the probability that the pieces can form two triangles? The random points are uniformly distributed along the stick. When the stick is broken at five points, there are $\frac12 \binom63=10$ ways to divide the six pieces into two groups of three pieces. In order for two triangles to be formed, there must be at least one way in which each group has three pieces, none of which is longer than the sum of the other two pieces. I used Excel to make a simulation with $100000$ trials, and the probability seems to be about $\color{red}{0.287}$ . (EDIT: There was a small mistake in my Excel sheet earlier, which caused my approximation to be slightly off; it has been corrected.) Some other questions about broken sticks have nice closed form answers (for example here and here ), and I wonder if this one does too.","['geometric-probability', 'triangles', 'geometry', 'probability']"
4714712,Seeking a (simple) proof that the sphere packing density is always less than $1$ in $n \ge 2$ dimension.,"For each natural number $n$ , we may define the (optimal) sphere packing density in $\Bbb R^n$ to be the number $$
D_n = \limsup_{r\to\infty} D_n(r),
$$ where $$\begin{align}
D_n(r) = \sup \Big\{ \frac{k\omega_n}{(2r)^n} :& \text{there exist $a_1,\dots,a_k$ such that $B(a_j)\subset [-r,r]^n$} \\
&\ \  \text{and $B(a_j)$'s are pairwise disjoint} \Big\}.
\end{align}$$ Of course, $B(a)$ denotes the open unit ball centered at $a$ , and $\omega_n$ is the $n$ -volume of the unit ball in $\Bbb R^n$ . I have no idea if this is how the density is normally defined or not, but it should be equivalent to the standard definition nonetheless. My $D_n(r)$ is the maximal density of packing unit balls into an $n$ -cube of side length $2r$ , and $D_n$ is the limit as the side length  of this cube goes to infinity. Is there a relatively simple argument to show that $D_n <1$ for all $n\ge 2$ ? I know that calculating $D_n$ precisely is a hard task (except for some very special values of $n$ ), for example the Kepler conjecture, which says that $D_3 = \frac{\pi}{3\sqrt{2}} \simeq 0.7405$ , has only been proved in the last ten year, after it had been an open problem for several centuries. I don't need to know that value of $D_n$ , I just want to know if it's always bounded away from $1$ . While it seems intuitive that we should have $D_n < 1$ always (except for the trivial case $D_1$ , where $1$ -dimensional balls are just lines), I can't think of a quick proof of it off the top of my head. It doesn't seem too hard to show that $D_n(r) < 1$ for each fixed $r$ since there is definitely some wasted space between the unit balls, however, I can't think of any a priori reason why there wouldn't be a better and better possible configuration as $r$ grows large that may make $D_n(r)\to 1$ as $r\to\infty$ . I am thinking that adapting the proof of Besicovitch covering theorem could do the trick, but I haven't tried writing down the details yet. I know essentially nothing regarding this topic, so perhaps the answer to my question is already well-known or even a standard result in sphere packing. If so, I would be delighted if anyone could point me to a good resource that discusses this kind of question.","['measure-theory', 'geometry', 'analysis', 'packing-problem']"
4714741,An example of a weakly Hopfian group which is not Hopfian,"A group $G$ is Hopfian iff every epimorphism $f:G\to G$ is an automorphism. A ‎group ‎‎ $‎‎G$ ‎is ‎ weakly ‎Hopfian ‎if‎f every ‎r-homomorphism ‎‎ $‎‎r:G\to H\cong G$ ‎is ‎an ‎isomorphism. I‎n ‎other ‎words, ‎‎ $‎‎G=K\rtimes H$ ‎and ‎‎ $‎‎H\cong G$ ‎imply ‎‎ $‎‎K=1$ .‎‎ Trivially, every Hopfian group is weakly Hopfian. I was wondering if someone could give me an example  of a weakly Hopfian group which is not Hopfian.
I particularly interested in finitely presented examples.
Thanks in advance.",['group-theory']
4714766,Decomposes a unit vector into the kronecker product of three unit vectors,"I want to get the optimal solution to the following equation $$\mathop{\rm{min}}_ {\| \boldsymbol{v}_i \| = 1
    \atop
    i= 1,2,3}  \| \boldsymbol{w} - \boldsymbol{v}_3 \otimes \boldsymbol{v}_2 \otimes \boldsymbol{v}_1 \|^2$$ where $\boldsymbol{v}_i \in \mathbb{R}^{ p_i\times 1}$ , $\boldsymbol{w} \in \mathbb{R}^{ p_1 p_2 p_3\times 1}$ is given and $\| \boldsymbol{w}\| = 1$ . I want to get the best estimate of $\boldsymbol{v}_i.$ I can get $p_{i+1}p_{i+2}$ $v_1^i/v_j^i$ 's , $j = 2,\ldots,p_i$ , where $v_j^i$ denotes the $j$ -th entries of $\boldsymbol{v}_i$ and $i+1$ , $i+2$ are computed modulo 3. Let $$\hat{\dfrac{v_1^i}{v_j^i}} = \dfrac{1}{p_{i+1}p_{i+2}} \sum v_1^i/v_j^i , j = 2,\ldots,p_i$$ , then we can get the estimator $\hat{\boldsymbol{v}_i}$ under $\|\boldsymbol{v}_i\|=1.$ But I dont know whether this solution is optimal or appropriate.","['computational-mathematics', 'optimization', 'statistics', 'kronecker-product']"
4714778,"Suppose that $1< b< 2$ is a fixed constant, show that $n^b \times \prod_{k=2}^{n} (1-\frac{b}{k} ) \space \space \forall n\ge 2$ is bounded.","Suppose that $1< b< 2$ is a fixed constant and $$a_n = \prod_{k=2}^{n} \left(1-\frac{b}{k} \right) \space \space \forall  n\ge 2$$ Show that there are constants $0< m< M$ such that $m< n^ba_n< M$ for all $n\ge 2$ My first attempt is ... $$\left ( 1-\frac{b}{2}  \right ) \left ( 1-\frac{b}{2} \right ) \left ( 1-\frac{b}{2} \right ) \dots \left ( 1-\frac{b}{2} \right ) < a_n < \left ( 1-\frac{1}{2}  \right ) \left ( 1-\frac{1}{3} \right ) \left ( 1-\frac{1}{4} \right ) \dots \left ( 1-\frac{1}{n} \right ) $$ then... $$ \left ( 1-\frac{b}{2} \right ) ^{n-1}< a_n< \frac{1}{n} $$ so... $$ n^b\left ( 1-\frac{b}{2} \right ) ^{n-1}< n^ba_n< n^{b-1}$$ but it is clearly not bounded by two ""constants"" m and M My second thought is that $$\prod_{k=1}^{\infty } \left ( 1+\alpha_k \right ) $$ converges if... $$\space \alpha_k > -1 ,\qquad \sum_{k=1}^{\infty } \left | \alpha_k \right | \space\text{ converges }$$ but I'm not sure if this can help here or not Is there any suggestion please?","['infinite-product', 'upper-lower-bounds', 'sequences-and-series']"
4714838,Differential equation $(1-x)(y')^2+(y-x)y'+y=0.$,"I have the following problem. Find all differentiable on $[0,1)$ non-negative functions $f$ , for which for any $u\in(0,1)$ the tangent line to graph of $f$ in the point $(u,f(u))$ intersects the lines $y=0$ and $x=1$ in the points $A$ and $B$ respectively and $x_{A}=y_{B}$ . Also, $f'(0)=0$ . Easy to show that our function it's a root of the equation $$y=xy'-\frac{y'^2}{1+y'}$$ or $$(1-x)(y')^2+(y-x)y'+y=0.$$ My question: is there some way to solve this equation directly? Also, I assumed that the graph of $f$ is parabola with focus $F\left(\frac{1}{2},\frac{1}{2}\right)$ and directrix $y=x-1,$ which gives an equation of the parabola: $$(x+y)^2=4y,$$ which gives a solution of our equation. My second question: is there some better way to get this parabola? Thank you!","['conic-sections', 'ordinary-differential-equations']"
4714840,Does $\sum_{n=1}^{\infty} x_n$ converge when $x_{n+1}=\frac{e^{x_n}-1}{e^{x_n}}$ and $x_1 > 0$?,"Given $x_1>0$ , and $$x_{n+1}=\frac{e^{x_n}-1}{e^{x_n}},$$ determine
whether the series $\sum_{n=1}^\infty x_n$ diverges. If it does, prove it; if
not, provide a counterexample. This is a series in iterative form, and it does not have a specific explicit form. The ratio test does not provide a conclusive result, and the root test does not seem suitable for this series. I feel a bit unfamiliar with this type of problem, and I would appreciate it if someone could provide some insights. Thank you very much.",['sequences-and-series']
4714842,Translation invariance of function,"For the measurable function $f\mathpunct{:}\mathbb{R}\to\mathbb{R}$ if it meets the requirement that for any real number $a$ , the function $f (x+a) - f (x)=0 $ a.e. , does this necessarily imply that $f (x)= \text {constant} $ a.e. ? If we assume that $f (x)$ is continuous at a certain point, let $$
N_q=\{ x\in \mathbb{R}\ |\ f(x+q)\neq f(x) \}
$$ We can get that the measure of $N_ q$ is $0$ . Let $$
B=\{ x\in \mathbb{R}\ |\ f(x+q)=f(x),\forall q\in \mathbb{Q} \}
$$ clearly $$
B\subset \mathbb{R}-\bigcup_{q\in \mathbb{Q}} N_q
$$ By continuity, we can obtain a proof that $f$ is a constant on $B$ . However, how can we prove the general case?","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4714876,How do I prove that this function is injective and surjective? [duplicate],"This question already has answers here : Proving the Cantor Pairing Function Bijective (4 answers) Closed last year . Assume $f$ is a function such: $$f: \omega\times\omega\to\omega$$ and $$f (\langle x,y\rangle) = \frac{1}{2}\left[(x+y)^2+3x+y\right].$$ How do I prove that this function is one-to-one (injective) an also from $\omega\times\omega$ onto $\omega$ (surjective)? In Elements of Set Theory by Herbert B. Enderton, there's an image of such one-to-one function that is also from $\omega\times\omega$ onto $\omega$ . Actually I don't now how to verify that this function is a one-to-one correspondence.","['elementary-set-theory', 'functions']"
4714892,Prove $x^2 + x \cos (x) -2 \cos ^2 (x) = 0$ has exactly two real roots,"Given the equation $$x^2 + x \cos (x) - 2 \cos ^2 (x) = 0$$ prove it has exactly two real roots. My attempt In order two prove the equation indeed just has two real roots, first of all I need to prove they exists and then that they are the only ones. For the existance part notice that denoting $f(x) = x^2 + x \cos (x) - 2 \cos ^2 (x) = (x-\cos (x))(x+2\cos (x))$ $$f(-2) = 0.0036 > 0, f(0) = -2 < 0, f(1) = 0.00045 > 0$$ By Bolzano's theorem, there is at least one root within each interval $(-2,0)$ and $(0,1)$ Where I am stuck is with the uniqueness part. I tried to find the derivate $$f'(x) = 2x + \cos (x) - x \sin (x) + 4 \cos (x) \sin (x) = 2x + \cos (x) - x \sin (x) + 2\sin (2x)$$ and putting it equal to $0$ : $$2x + \cos (x) - x \sin (x) + 2\sin (2x) = 0$$ If I see that this equation only has a root (which is true, checked by graphing it), then by Rolle's theorem a function at most has one root more than it derivative; therefore the two roots I found are the only ones. However, this is where I got stuck since I can't figure out how to solve this new equation. I think I may be going in the wrong direction, but I can't think of other way to solve the problem. Any help is truly appreciated.","['analysis', 'real-analysis', 'continuity', 'rolles-theorem', 'numerical-methods']"
4714923,"What are ""forward/backward"" solutions to first order difference/differential equations?","Recently, I've met for the first time with the so called ""forward/backward"" solutions to first order difference/differential equations. It seems that the subject goes back to Blanchard's ""Backward and Forward Solutions for Economies with Rational Expectations"" https://www.jstor.org/stable/1801627 . Is there some reference with simple examples for this type of calculations? Some textbook or lecture notes? For example : If I have the differential equation $x' = ax + b$ with $a, b$ real constants I get the following ""forward solution"" : $$
x(0) = \left(\frac{b}{a} + x(t)\right)e^{-at} - \frac{b}{a}
$$ and the following ""backward solution"" : $$
x(0) = \left(\frac{b}{a} + x(-t)\right)e^{at} - \frac{b}{a}
$$ Is that correct? I would appreciate any comments or references on the subject","['ordinary-differential-equations', 'analysis', 'reference-request', 'economics', 'terminology']"
4714932,What is the result of $\ell^1 \otimes \ell^1$?,"I recently stumbled upon the concept of a tensor product while studying quantum computing, and felt that my understanding of it from preliminary readings was incomplete. I challenged myself to determine the answer to this question in order to show that I had a full understanding of tensor products. I believe the answer should be $\mathbb{R}^\mathbb{N}$ (excuse the abuse of notation, I'm unfamiliar with a better way of denoting a countably infinite real space) or in fact $\ell^1$ itself once again. Any hints, ideas or explanations are appreciated!","['tensor-products', 'functional-analysis', 'quantum-computation']"
4714933,"Confusion with the equation $\int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\int_0^{\infty}xf(x)\cos(\alpha x)\,dx$","I’m given the question: If: $\displaystyle \int_0^{\infty}f(x)\sin(\alpha x)\,dx=-\displaystyle \int_0^{\infty}xf(x)\cos(\alpha x)\,dx$ and $f(1)=1$ , then find $f$ . My attempt: To me, if we add $\int_0^{\infty}xf(x)\cos(\alpha x)\,dx$ to both sides then we get: $\displaystyle \int_0^{\infty}(f(x)\sin(\alpha x)+xf(x)\cos(\alpha x ))\,dx=0$ Now the above integral is the Fourier integral of $0$ . Therefore: $f(x)=\dfrac{1}{\pi}\displaystyle \int_{-\infty}^{\infty}0\cdot\sin(\alpha  x)\,d\alpha=0$ which is not in line with original assumptions of the question. Where am I wrong?","['integration', 'fourier-transform', 'ordinary-differential-equations', 'real-analysis']"
4714996,"If $(V, \langle,\rangle)$ is a $4$ dim vector space, then show that the set of all compatible complex structures consists of two copies of $S^2$.","If $(V, \langle,\rangle)$ is a four-dimensional Euclidean vector space, then show that the set of all compatible complex structures consists of two copies of $S^2$ . I'm quite new to complex differential geometry and this feels a bit weird to me as if we let $A$ denote the set of all complex structures of $V$ , then $A = \{I : V \to V \mid I^2 = -\operatorname{id}, \langle I(v), I(w)\rangle = \langle v, w\rangle \}$ and for some reason this should consist of two copies of $S^2$ ? I know that we also have $I \in O((V, \langle,\rangle))$ , but this ain't helping that much either. By compatible we mean that $\langle I(v), I(w)\rangle = \langle v, w\rangle$ .","['complex-geometry', 'differential-geometry']"
4715020,Eigenvectors of different eigenvalues are linearly independent (without matrices),"Usually the independence of eigenvalues are shown for matrices, I did a proof without considering matrices. Let $T:V\to V$ be a linear operator and let $X=$ { $v_1,v_2,...,v_m$ } be eigenvectors each corresponding to a different eigenvalue then $X$ is linearly independent. Proof: Suppose this is wrong and $X$ is linearly dependent. Then there are atleast two of which (if there were only one, that would make an eigenvector to be $0$ ) are nonzero $a_i \in F$ such that: $$
a_1v_1+a_2v_2+⋯+a_mv_m=0 \tag{*}
$$ Let us pick only the vectors whose coefficients are nonzero and write: $$α_1 u_1+α_2 u_2+⋯+α_k u_k=0$$ where for each $i$ , $u_i=v_j$ for some $j$ . (We don't pick the same vector twice here). Now all $\alpha$ 's are different than $0$ in this context. Now let us multiply each side of this equation with $a_1^{-1}$ (since $a_1\neq 0$ , $a_1^{-1}\in F$ ) to get: $$
u_1+α_1^{-1}α_2u_2+⋯+α_1^{-1}α_ku_k=0 \tag{**}
$$ Since each $u_i$ is an eigenvector we may write $Tu_i=\lambda_iu_i$ and we apply $T$ to the both sides of this equation to get: $$\lambda_1u_1+\lambda_2α_1^{-1}α_2u_2+⋯+\lambda_kα_1^{-1}α_ku_k=0$$ and since $\lambda_1 \neq 0$ we can multiply each side with $\lambda_1^{-1}$ to get: $$u_1+λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α^{-1} α_k u_k=0$$ From this equation and the $(**)$ equation we get: $$ α_1^{-1} α_2 u_2+⋯+α_1^{-1} α_k u_k=λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k$$ and this equation can be multiplied by $\alpha_1$ to get: $$ 
α_2 u_2+⋯+α_k u_k=λ_1^{-1} λ_2 α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k
$$ which is $$
(1-λ_1^{-1} λ_2 ) α_2 u_2+⋯+(1-λ_1^{-1} λ_k ) α_k u_k=0
$$ since all the $a_i$ are nonzero, none of these coefficients can be $0$ , had it been the case we would have for some $i\neq 1$ , $1-λ_1^{-1} λ_i=0$ which is $λ_1=λ_i$ and this can't be since we picked $\lambda$ 's to be distinct eigenvalues. Now if we rename each coefficient as $(1-λ_1^{-1} λ_2 ) α_2=β_2$ , $(1-λ_1^{-1} λ_3 ) α_3=β_3$ , $\ldots$ , $(1-λ_1^{-1} λ_k ) α_k=β_k$ we get: $$ 
β_2 u_2+β_3 u_3+⋯+β_k u_k=0
$$ where all $\beta_i$ are nonzero. This is precisely the same case within the equation $(*)$ (except we have $k-1$ vectors now) so we may apply the same procedure to this equation to get: $$
(1-λ_2^{-1} λ_3 ) β_3 u_3+⋯+(1-λ_2^{-1} λ_k ) β_k u_k=0
$$ and for the same reasoning each coefficient is nonzero and we rename coefficients once more as $$γ_3=(1-λ_2^{-1} λ_3 ) β_3$$ and so on to get another equation but this time without $u_1$ and $u_2$ . Since we have finite vectors, we keep doing this process, after $(k-1)$ th iteration we get the equation: $$
(1-λ_{k-1}^{-1} λ_k ) A_k u_k=0
$$ and since $A_k \neq 0$ (due to the process) and $u_k \neq 0$ we must have $$
1-λ_{k-1}^{-1} λ_k=0 $$ which leads to the contradiction $λ_{k-1}=λ_{k}$ . What do you think about this proof? Is it valid?","['linear-independence', 'linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
4715069,How to solve this mathematical analysis limit problem?,"The following question is the third problem of the analysis section in the 2023 Mathematics Department Transfer Exam for Peking University. “
Calculate the limit of sequence: $$\lim_{n\to\infty}\sum_{k=1}^{n}\left(\frac{1}{n}+\dfrac{k}{n^2}\right)^{1+\frac{2k}{n^2}}$$ ” My idea is to transform the limit into some kind of Riemann integral, but I don't know how to do it. Or there may be other methods, such as the Sandwich theorem, etc.","['limits', 'analysis', 'real-analysis']"
4715094,Differential of a function as a limit,"Based on intuitive definitions of the differential of a function, it seems to me that for $f:\mathbb{R}^n \to \mathbb{R}^m$ something like $$
df(a) = \lim_{|r| \to 0} \frac{|f(a+r)-f(a)|}{|r|}
$$ should hold for $a \in \mathbb{R}^n$ , although this may be an abuse of notation. I haven't been able to prove it from the usual definition I've encountered that the differential is the linear map defined by $$
df(a)(v_1, \dots, v_n) = \left(\sum_{k=1}^n \frac{\partial f_1}{\partial x_k}(a)v_k, \dots, \sum_{k=1}^n \frac{\partial f_m}{\partial x_k}(a)v_k\right).
$$ Does any such limit definition of the differential of a function exist? My motivation for asking this is from Narasimhan's Analysis on Real and Complex Manifolds where he uses in a proof that for $g: \mathbb{R}^n \to \mathbb{R}^n$ , $(dg)(0)=0$ implies that there exists a neighborhood $W$ of $0$ such that for any $x,y \in W$ , $$
|g(x)-g(y)| \leq \frac{1}{2}|x-y|.
$$ This would follow directly from the limit formula that I gave, but is there a more rigorous way to show this? Edit: Of course, in the book $g$ is also specified to be continuously differentiable on an open subset $\Omega \subset \mathbb{R}^n$ containing the origin.","['differential', 'multivariable-calculus', 'differential-geometry']"
4715100,Operator whose spectrum is different from the spectrum of its dual,"It's mentioned in our functional analysis course that for any operator $T$ on a Banach space, T has the same spectrum as its dual. A friend asked for an example where this fails and I can't think of one for the life of me. As mentioned, it's clear that the space can't be complete. My thoughts are that if $T$ is a bdd linear operator on $X$ , $T$ extends uniquely to a bdd operator $\tilde{T}$ on the completion $\tilde{X}$ . Further, $X$ and $\tilde{X}$ have the same dual space, and also $T$ and $\tilde{T}$ have the same dual operator on this space. Because of this, the spectrum of $T^{*}$ coincides with the spectrum of $\tilde{T}$ , so really what we want is an operator $T$ on a non-complete space whose unique extension to the completion has a different spectrum. My best attempt was to look at some $\ell^{p}$ space ( $p<\infty$ ) as that has the canonical Schauder basis $(e_n)_n$ and take $X$ to be the linear span of this basis. Then try to define an operator that isn't surjective on $X$ , but does extend to an invertible operator on $\ell^p$ . Clearly I was unsuccessful :) It has crossed my mind that the result about Banach spaces uses the axiom of Choice (through the use of Hahn-Banach) so maybe to find an example where it fails I also need Choice, but I didn't manage to come up with and example even when allowing for ""non-canonical"" ones (e.g starting with an algebraic basis of a Banach space and trying to work from there). If anyone has a construction for this, it would be greatly appreciated.","['banach-spaces', 'spectral-theory', 'functional-analysis']"
4715111,How to verify this matrix identity?,"Let $m\geq 3$ be a positive odd number and let $M$ be the $m\times m$ matrix defined by $$M=\begin{bmatrix}0&1&0&0&\cdots&0\\
0&0&1&0&\cdots &0\\
0&0&0&1&\cdots &0\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&0&\cdots &1\\
1&-2&2&-2&\cdots&2\end{bmatrix}.$$ Is it true that $M^{2m}=I_m$ , the identity matrix of size $m$ ? I verified the case $m=3,5,7,9$ but am not sure how to prove this in general. Any help/hint will be appreciated.","['matrices', 'linear-algebra', 'companion-matrices']"
4715124,What is the algebraic solution (not graphical) of $2x^6+4x^5+x^4+4x^3+2x^2+1=0$?,"I am looking for the algebraic solution (not graphical) of the following equation. $$2x^6+4x^5+x^4+4x^3+2x^2+1=0$$ According to the definition given in Wikipedia, this polynomial is not a palindromic polynomial, unfortunately. But, a math student claims that the above polynomial has an algebraic solution. Our highschool teachers couldn't solve this. Dividing each side by $x^4$ , I get $$2x^2+4x+1+\frac{4}{x}+\frac {2}{x^2}+\frac {1}{x^4}=0$$ So $$2\bigg(x^2+\frac{1}{x^2}\bigg)+4\bigg(x+\frac {1}{x}\bigg)+\bigg(1+\frac{1}{x^4}\bigg)=0$$ I know that the property $$x^2+\frac {1}{x^2}=\bigg(x+\frac {1}{x}\bigg)^2-2$$ But the term $1+\frac {1}{x^4}$ violates the symmetry.  What can I do from here?","['algebra-precalculus', 'polynomials']"
4715150,"What is the cardinality of the set of measures $\mu$ on $(\mathbb{R}, \mathcal B (\mathbb{R}))$?","Having recently studied a course on $\mathsf{ZFC}$ Set Theory, I have been thinking about applications of the material that I have learnt in other disciplines of mathematics. With that in mind, I recently formulated the following question and have been unable to find anything definitive online: Question : What is the cardinality of the set ${A}$ containing all measures $\mu$ on $(\mathbb{R}, \mathcal{B} (\mathbb{R}))$ ? (where $\mathcal{B} (\mathbb{R} )$ refers to the Borel Sigma Algebra on $\mathbb R$ ) I know that the cardinality of $\mathcal{B}(\mathbb{R})$ is $2^{\aleph _0}$ (with a proof of this result here) and it feels as though we should be able to construct a suitable upper bound using this fact. We know that a measure on this space will be a function $\mu : \mathcal{B} (\mathbb{R}) \rightarrow [0, + \infty ]$ . Therefore, the cardinality of the set of measures is certainly upper bounded by the cardinality of $[0, + \infty ]^{\mathcal{B}(\mathbb{R})} = \mathbb{R}^{\mathbb{R}}$ . This is $\big{(}2^{\aleph _0} \big{)}^{2^{\aleph _0}}$ . So we have: $$|A| \leq \big{(}2^{\aleph _0} \big{)}^{2^{\aleph _0}} = 2^{2^{\aleph _0}}$$ For a lower bound, we note that each probability distribution on $\mathbb{R}$ induces its own measure. And the set of the probability distributions on $\mathbb{R}$ has cardinality $2^{\aleph _0}$ (with a proof of this result here) . So we have: $$2^{\aleph _0} \leq |A| $$ We now have an upper bound and a lower bound for the cardinality of the set $A$ in question. This allows us to produce the following inequality (by combining the two that we have above): $$ 2^{\aleph_0} \leq |A| \leq |\mathbb{R}^{\mathbb{R}}| = 2^{2^{\aleph _0}}$$ Perhaps it is easier to first consider the the cardinality of the set of probability measures first in order to simplify the problem, before considering all measures - although I haven't managed to have much success in tightening the bounds this way. But this inequality still leaves me wondering what the exact cardinality is from a rigorous perspective. Is there a way to establish tighter bounds to find the probability exactly and is what I have done so far correct? I would be grateful for any assistance in either tightening the bounds or finding the exact cardinality.","['measure-theory', 'cardinals', 'borel-measures', 'elementary-set-theory', 'set-theory']"
4715180,How to define Brownian motion for constraint matrix manifold?,"My problem at hand is the construction of a Brownian motion on a constraint matrix manifold $^\color{magenta}\star$ denoted by $\mathcal{M}$ , which is represented as follows: $$ \mathcal{M} = \left\{ P\in \mathbb{R}^{n\times d} \mid \operatorname{Tr} \left( P^T A_k P \right) = b_k, k = 1, 2, \dots, m \right\} $$ where $A_k \in \mathbb{R}^{n\times n}$ are given symmetric matrices and $b_k \in \mathbb{R}$ are given scalars. My initial thought is to construct this Brownian motion based on the tangent space of the manifold. The tangent space at a point $P$ on the manifold, denoted by $T_{P}\mathcal{M}$ , is defined as follows: $$ T_{P}\mathcal{M} = \left\{ Z \in \mathbb{R}^{n\times d} \mid \operatorname{Tr} \left( P^T A_k Z \right) = 0, k = 1, 2, \dots, m \right\} $$ However, obtaining the basis of this tangent space poses a challenge for me as I didn't find explicit forms of those bases. Does anyone have an idea about constructing a Brownian motion for this particular manifold? Another thought is to project the Brownian motion from space of all matrices $\mathbb{R}^{n\times d}$ to this manifold. As the normal space w.r.t. Frobenius inner product is $$
N_{P}\mathcal{M} = \left\{\sum_{k=1}^m \alpha_k A_k P, \alpha_k \in \mathbb{R}\right\},
$$ the orthogonal projection to $T_P \mathcal{M}$ is $\pi_P(X)= X - \sum_{k=1}^m \alpha_k A_k P$ and these $\alpha_k$ can be solved by equations $\operatorname{Tr} \left( P^T A_k X \right) = 0, k = 1, 2, \dots, m$ . However, I cannot connect the relationship between the projected Brownian motion and the Brownian motion on this manifold as they are not the same in general. $\color{magenta}\star$ Michel Journée, Francis Bach, Pierre-Antoine Absil, Rodolphe Sepulchre, Low-rank optimization on the cone of positive semidefinite matrices , SIAM Journal on Optimization, Volume 20, Issue 5, 2010.","['riemannian-geometry', 'matrices', 'stochastic-processes', 'probability', 'differential-geometry']"
4715208,Prove that $\lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a$.,"Let $f$ be a function which definites on $\mathbb{R}$ and  has a third derivative, such that $$f'(0)=0,f''(0)=-2,f'''(0)>0,f'(1)=1,f''(1)=4,f'''(1)>0.$$ and for any $x\in (0,1)$ , $0<f(x)<f(0)=f(1)=1$ . If $a>1$ , then prove that $$\lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a.$$ In this problem $f$ requires so many conditions. Maybe we can choose a simple function $f$ to have a try, such as $f(x)=1-x^2+x^3$ , but I can't figure it out. In adition, I wonder why $a>1$ ?It seems that we can try to write $\ln (x+a)=\ln a+\ln (1+\frac{x}{a})$ and then expand $\ln(1+\frac{x}{a})$ at $0$ , and try to prove that for any $k\in \mathbb{N}$ , $$\lim_{n\to \infty}\frac{\int_0^1f^n(x)x^kdx}{\int_0^1f^n(x)dx}=0.$$","['integration', 'analysis', 'sequences-and-series']"
4715211,Answer clarification for why the parallelogram law implies that the norm is induced by an inner product.,"In short: If a norm $\Vert\cdot\Vert$ on a real vector space satisfies the parallelogram law, and $\langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right)$ , then how can we show $
\langle\lambda x,y\rangle = \langle x,\lambda y\rangle
$ for $\lambda\in \Bbb R$ ? This is a request for clarification on this specific answer given by zodiac. The post is many years old, so I thought it better to open a new question rather than leave a comment. The problem is to show that if $\Vert\cdot\Vert$ is a norm on a real vector space that satisfies the parallelogram law, $\Vert x+y\Vert^2 + \Vert x-y\Vert^2  = 2\Vert x\Vert^2 + 2\Vert y\Vert^2$ , then $\Vert\cdot\Vert$ is induced by an inner product. We proceed by defining $$\langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right)$$ (from the polarization identity ), and show that this is indeed an inner product, and that it induces $\Vert\cdot\Vert$ . The hardest part is arguably to show scalar linearity, i.e. that $$
\langle\lambda x,y\rangle = \lambda \langle x,y\rangle.
$$ The proofs I've seen show it first for $\lambda \in \Bbb Q$ and then appeal to some form of continuity of the purported inner product to extend to real scalars. However, the (or a) central step of the linked answer is instead to show $$
\langle\lambda x,y\rangle = \langle x,\lambda y\rangle.
$$ How can we prove the above identity? Of course, the proof should hopefully not use continuity, which is what made this specific answer particularly interesting. We may assume that all other axioms of inner products are already proven, including additive linearity. zodiac also gives the hint of using $\langle x+y,z\rangle = \langle x,z\rangle+ \langle y,z\rangle$ and $\langle -x,y\rangle =-\langle x,y\rangle =\langle x,-y\rangle$ , both of which may be taken as given. (I made an attempt where I considered the difference, regrouped, applied the parallelogram law in two places, and neatly ended up right where I started...)","['inner-products', 'normed-spaces', 'linear-algebra']"
4715216,"Prove that $K[x,y] / (y^2+yx^2-x(x-1))$ have infinitely many maximal ideals.","I have the following problem. $K$ is an algebraically closed field. Prove that $$R=K[x,y]/(y^2+yx^2-x(x-1))$$ is an integral domain, has infinitely many maximal ideals, and decides if $R$ is a PID. I have a pair of hours trying this and I have some ideas, which I will describe. The first part is strainfoward. Since $K[x,y]\cong (K[x])[y]$ and $K[x]$ is a integer domain, then is sufficient show that $J=(y^2+yx^2-x(x-1))$ is a prime ideal, and also since J is principal and $K[x,y]$ is a UFD then is sufficient show that $s(x,y)=y^2+yx^2-x(x-1)$ is irreducible in $(K[x])[y]$ which is also easy via Einsentein criterion choosing the prime ideal $P=(x)$ in $K[x]$ and considering $a_2=1$ , $a_1=x^2$ and $a_0=x(x-1)$ and writing $s(y)=a_2y^2+a_1y-a_0\in (K[x])[y]$ . In this point I think that we should use the correspondence theorem which says that the ideals of $R$ must be of the form $$I/(y^2+yx^2-x(x-1))$$ where $(y^2+yx^2-x(x-1))\subset I \subset k[x,y]$ . Here I have some problems thinking about how should look maximal ideals in R. Also I have another idea that probably works, this is to use the fact that there is a one-to-one correspondence between maximal ideals in $K[x_1,x_2,\ldots, x_n]$ and points in $K^n$ . At this point my intuition says that if R has a lot of maximal ideals then $R$ should not be a PID. ¡Any suggestions and hints are welcome!","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4715228,"If I've learned real analysis, should I go back and (re)learn calculus properly?","If you've learned real analysis, what, if any, is the value in going back and learning calculus ""properly"", via, let's say, Spivak or Courant? I've learned real analysis using the texts of Ross and of Abbott (not yet Rudin).  My initial study of calculus was Stewart-style: Here's the procedure, do it, get an A.  That leaves many gaps, both in depth of understanding and even in fluency (it's hard to memorize a procedure that you don't really understand).  Should I invest time in relearning single variable calculus from a classic text such as Spivak or Courant? If so, how should I approach it, and what should my goals be? Similarly for multivariable calculus: I learned it using Stewart and did very well in the class.  Which means I learned next to nothing.  And I certainly didn't encounter things like the Implicit Function Theorem.  Should I go back and relearn multivariable calculus, using, e.g. Hubbard or Shifrin? Do I move forward (Rudin) or back (single or multivariable calculus)? Update littleO asks ""Once you’ve learned real analysis... doesn’t that mean you’ve already learned single variable calculus properly... [since it] proves the main results of single variable calculus"".  In one sense that's true.  But, in another sense, the focus of a real analysis is on proving theorems, whereas a book like Spivak has many (challenging) computational problems.  Would you believe that someone can prove the Cauchy sequences converge implies the Nested Interval Theorem, but perhaps get confused when doing integration by substitution? Or that they find a fresh presentation of the development of the integral of $x^n$ using first principles (Courant develops it using limits of series, even before introducing the Fundamental Theorem of Calculus), to be very enlightening? So even if you've proven the main results, there still a lot to be gained from learning, the ""right"" way, how to develop the basic procedures from fundamentals, and how to use them to solve problems. Put another way, we have three things: Procedure focused.  E.g. Stewart. Theorem & proof focused.  E.g. real analysis. Using procedures, but a) developing them from first principles and b) using them masterfully.  E.g. Spivak, Apostol, Courant. I've found that #1 alone is insufficient: it leaves big gaps, and even the procedures don't stick.  #2 is excellent - and there's no limit on how far and deep you can go.  But something tells me that before I do so, I need to go back and master the procedures , including their development and usage.  I'm not sure if that's correct and if so how I should do it without being stuck in repetition and without failing to advance.","['self-learning', 'calculus', 'soft-question', 'real-analysis']"
4715243,Positive semidefiniteness of the product of a symmetric positive semidefinite matrix and a nonnegative diagonal matrix,"Is it true that the product of a symmetric PSD matrix and nonnegative diagonal matrix? For $A\in\Bbb R^{n\times n}$ and $D\in\Bbb R^{n\times n}$ , let $A\succeq O$ and $D:=\mathrm{diag}(d_1,\dots,d_n)$ , where $d_i\ge0$ for all $i$ .
Then, is $AD$ positive semidefinite? It is known that when two psd matrices $A$ and $B$ are non-commuting, $AB$ is not psd. Although $AD\neq DA$ , how about the case where $B$ is a nonnegative diagonal matrix? I found that the hypothesis is true when $n=2$ . Given $$
A:=\left(\begin{array}{cc}a & b\\ b & c\end{array}\right)\succeq O,\quad D:=\left(\begin{array}{cc}d_1 & 0 \\ 0 & d_2\end{array}\right)\ge O.
$$ Since $A\succeq O$ , $a\ge 0$ , $c\ge 0$ , and $ac-b^2\ge 0$ .
The product $AD$ is $$
AD=\left(\begin{array}{cc}d_1 a & d_2 b \\ d_1 b & d_2 c\end{array}\right).
$$ All the determinants of principal minors for $AD$ are nonnegative because $$
d_1a\ge0,\ d_2c\ge 0,\ d_1d_2(ac-b^2)\ge 0.
$$ Is this result possible to generalize $n\ge 2$ ?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4715273,"Does the set of all programs form a group that acts on machine state (essentially, i.e. with a few exceptions)?","Since the only thing a computer can do is modify its machine state, and you can always change from one state into another state (there exists such a program) and so each program is invertible; and if all the programs are deterministic, then associativity also makes sense.  Two programs are considered equivalent iff they do the same thing to the machine's state, given any starting state. So do I have the title right or not?","['programming', 'automata', 'turing-machines', 'group-theory', 'computer-science']"
4715286,Repeated applications of a (rotation) matrix keep you in the same subspace?,"Say I have a unit vector, $v$ in $\Bbb{R}^n$ ( $|v|=1$ ). I multiply a rotation matrix, $R$ with this vector to get a new vector, $u=Rv$ . Then, I get a third vector by applying the rotation matrix yet another time: $w=R^2v$ . Is it true that $w$ lies in the space spanned by $v$ and $u$ ? Can this be proven or a counter-example shown? And if this is true for rotation matrices, then is it true for other kinds of matrices as well? My attempt:
We must have $w=c_1v+c_2 w$ for some $c_1$ and $c_2$ . In other words, $$R^2v = c_1v+c_2 Rv$$ I'm stuck at this stage, unfortunately.","['linear-algebra', 'rotations']"
4715325,Differentiation of a function $f(x)$.,"Given, $f(x) = \sqrt{\sin x + \sqrt{\cos x +\sqrt{\sin x + \sqrt{\cos x.....}}}}$ I need to find $\frac{d}{dx}f(x)$ . I have tried by squaring both sides but was not able to make any headway. If anyone helps me, I shall be highly obliged.","['calculus', 'derivatives']"
4715343,Unitary group and subgroup are non-isomorphic,"Consider the group $U(n)$ of unitary $n\times n$ matrices, as well as its subgroup $V$ of $n\times n$ matrices which are obtainable by permuting rows of diagonal matrices $\mathrm{diag}(a_1,\dots,a_n)$ with $|a_i|=1$ . It is clear that $V\subsetneq U(n)$ . However, I am trying to come up with an argument why we cannot have $V\cong U(n)$ as groups. Note that the fact that $V\subsetneq U(n)$ does not imply this! For instance, we also have $2\mathbb Z\subsetneq\mathbb Z$ , yet $2\mathbb Z\cong\mathbb Z$ . My idea was the following. For every $A\in V$ , the matrix $A^{n!}$ is diagonal. Thus, if we have $A,B\in V$ , then the two matrices $A^{n!}$ and $B^{n!}$ must commute. I expect this to fail for the matrices in $U(n)$ . However, I am yet to find two matrices $A,B\in U(n)$ which do the job, meaning that $A^{n!}B^{n!}\neq B^{n!}A^{n!}$ . Sure, if we fix some $n$ , say $n=2$ , then we can come up with such $A$ and $B$ explicitly. But how to solve the problem for general $n$ ? Any ideas? I am also open for other strategies proving $V\not\cong U(n)$ .","['matrices', 'unitary-matrices', 'group-theory', 'linear-algebra']"
4715444,Prove the existence of a non-integrable function,"Let $f:[0,1] \to [0,1]$ be a Lebesgue integrable function on $[0,1]$ . Prove that there exists $t \in [0,1]$ such that $\frac{1}{|t-f(x)|}$ is not Lebesgue integrable on $[0,1]$ . This is an interesting problem but I have no idea at all. Any hints would be appreciated. Edit :
Here are some of my attempts. First I noticed that if $\{x:f(x)=t\}$ is of positive measure, then clearly it is done. While this may not hapen, I tried to consider the set $\{x:|f(x)-t|<c\}$ instead. I also tried Chebyshev's inequality. But these seem to be of no use.","['lebesgue-measure', 'functions', 'lebesgue-integral', 'real-analysis']"
4715446,Exactly one homogeneous differential equation of second order to given fundamental solution,"I am working on: Let $\phi_1,\phi_2$ , so that $\phi_1(x)\phi_2'(x)-\phi_1'(x)\phi_2(x)\neq 0.$ for all $x\in\mathbb{R}$ .
Then there exists exactly on homogeneous differential equation of second order $$y''(x)=f(x)y'(x)+g(x)y(x)$$ so that the functions $\phi_1,\phi_2$ are fundamental solutions. My idea is to suppose that there are two homogeneous ODE of second order.
Is there a possibility to combine the terms, so I can use $W(x)\neq 0$ ? Thanks for your answers!","['wronskian', 'ordinary-differential-equations', 'fundamental-solution']"
4715456,"Given continuous $f:\mathbb{R}\to\mathbb{R}$ and compact $K\subseteq f(\mathbb{R})$, show there exists a compact set $C$ such that $f(C)=K$.","I've been thinking on this problem for half a day, and I'm stumped. I've tried constructing $C$ directly, but that doesn't seem to be a good idea. In the case $K$ turns out to be a compact $\textbf{interval}$ , I've seen answers here ; I am curious as to whether it works for general compact sets. I've also considered the collection $\mathcal{C}$ of all compact subsets of $\mathbb{R}$ and tried showing that at least one of $C\in\mathcal{C}$ satisfy $f(C)=K$ ; I think something like Zorn's lemma should be required, and I'm not making progress. Thanks in advance. The problem isn't from a book, I'm trying to use it as a lemma for my work, so I am not sure as to whether the statement is true or not.","['continuity', 'general-topology', 'compactness', 'real-analysis']"
4715504,Proving that set is a hyperspace by definition,"While reading a necessary condition for an extremum with constraint from Zorich's book (volume I, page 528) I asked myself the following question: Question. Let $D$ be an open subset in $\mathbb{R}^n$ and $f:D\to \mathbb{R}$ such that $f\in C^1(D;\mathbb{R})$ . Assume that $x_0\in D$ and $\nabla f(x_0)\neq\vec{0}$ . Consider $N:=\{x\in D: f(x)=f(x_0)\}$ . How to prove that $N$ is an $(n-1)$ -dimensional submanifold of $\mathbb{R}^n$ ? Zorich gives the following definition of a $k$ -dimensional submanifold of $\mathbb{R}^n$ . Definition 1. We shall call a set $S\subset \mathbb{R}^n$ a $k$ -dimensional smooth surface in $\mathbb{R}^n$ (or a $k$ -dimensional
submanifold of $\mathbb{R}^n$ ) if for every point $x_0\in S$ there
exist a neighborhood $U(x_0)$ in $\mathbb{R}^n$ and a diffeomorphism $\varphi:U(x_0)\to I^n$ of this neighborhood onto the standard $n$ -dimensional cube $I^n=\{t\in \mathbb{R}^n: |t^i|<1,\  i=1,\dots,n\}$ of the space $\mathbb{R}^n$ under which the image of the set $S\cap
 U(x_0)$ is the portion of the $k$ -dimensional plane in $\mathbb{R}^n$ defined by the relations $t^{k+1}=0,\dots, t^n=0$ lying inside $I^n$ . EDIT 1: I can answer my question if we assume $\nabla f(x)\neq \vec{0}$ for all $x\in D$ . I will show that $N$ is a $(n-1)$ -dimensional submanifold of $\mathbb{R}^n$ . Indeed, let $a=(a_1,\dots,a_n)\in N$ then we know that $\nabla f(a)\neq \vec{0}$ . WLOG, assume that $\frac{\partial f}{\partial x^1}(a)\neq 0$ . Consider the mapping $\Phi:D\to \mathbb{R}^n$ defined as $\vec{x}\mapsto(f(x)-f(x_0),x^2,\dots,x^n)$ , where $x=(x^1,\dots,x^n)$ . It is not difficult to see that $\Phi\in C^1(D;\mathbb{R}^n)$ and $\det J_{\Phi}(a)\neq 0$ , where $J_{\Phi}(a)$ is a Jacobian of our mapping at $a$ . By inverse function theorem there exist $O_1=O_1(a)$ neighborhood of $a$ and $O_2=O_2(\Phi(a))$ neighborhood of $\Phi(a)$ such that $\Phi:O_1\to O_2$ is a diffeomorphism. We notice that $\Phi(a)=(0,a_2,\dots,a_n)$ and we can find a cube around $\Phi(a)$ which is contained in $O_2$ and assume that $$Q=(-r,r)\times \prod_{j=2}^n (a_j-r,a_j+r).$$ Let $P:=\Phi^{-1}(Q)$ , then $P\subset O_1$ and $P$ is a neighborhood of $a$ (because $\Phi$ is a diffeomorphism). Therefore, for $a\in N$ we were able to find its neghborhood $P=P(a)$ and diffeomorphism $\Phi$ such that $\Phi:P(a)\to Q$ . Moreover, it is not difficult to check that $$\Phi(P(a)\cap N)=\{0\}\times \prod_{j=2}^n (a_j-r,a_j+r).$$ By definition 1, it follows that $N$ is a $(n-1)$ -dimensional submanifold. Remark. But in the original question we assumed that $\nabla f(x_0)\neq 0$ . WLOG suppose that $\partial_1 f(x_0)\neq 0$ Since $f\in C^1(D;\mathbb{R})$ , then there exists $O(x_0)\subset D$ , where $O(x_0)$ is a neighborhood of $x_0$ such that $\partial_1f(x)\neq 0$ for all $x\in O(x_0)$ . It implies that $\nabla f(x)\neq \vec{0}$ for all $x\in O(a)$ . Using previous reasoning it implies that $\tilde{N}:=\{x\in O(x_0): f(x)=f(x_0)\}$ is a $(n-1)$ -dimensional submanifold. But it does not imply that $N$ is a $(n-1)$ -dimensional submanifold. I hope now my question is posed correctly and it maked sense what am I asking.
Thank you! EDIT 2: I created this post because Zorich proves the following result in his book: Theorem 1. Let $f:D\to \mathbb{R}$ be a function defined on an open set $D\subset \mathbb{R}^n$ and belonging to $C^1(D;\mathbb{R})$ .
Let $S$ be a smooth surface in $D$ .  A necessary condition for a point $x_0\in S$ that is noncritical for $f$ to be a  local extremum of $\left.f\right|_S$ is that $$TS_{x_0}\subset TN_{x_0},$$ where $TS_{x_0}$ is the tangent space to the surface $S$ at $x_0$ and $TN_{x_0}$ is the  tangent space to the level surface $N = \{x \in D: f(x) = f(x_0)\}$ of $f$ to which $x_0$ belongs. So my concern was that one cannot define surface $N$ in the way he did. I mean $N$ should be defined as $\{x\in O(x_0): f(x)=f(x_0)\}$ , where $O(x_0)\subset D$ is a neighborhood of $x_0$ , where $\nabla f(x)\neq 0$ . Is that correct?","['multivariable-calculus', 'derivatives', 'differential-geometry', 'real-analysis']"
4715505,Suppose that $\mathbf{A}-\mathbf{B}$ is positive definite. What can we say about $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$?,"In a physical problem related to minimum dissipation, it can be proven from physical considerations that $\mathbf{A}-\mathbf{B}$ is positive definite.
According to the definition, we have $$
\forall \mathbf{x} \in \mathbb{R}^n \backslash \{0\}, \quad
\mathbf{x}^\top \left( \mathbf{A}-\mathbf{B} \right) \mathbf{x} > 0 \, .
$$ Both $\mathbf{A}$ and $\mathbf{B}$ are invertible and are themselves
also symmetric positive definite. In addition, the eigenvalues of $\mathbf{A}$ and $\mathbf{B}$ are positive. To be able to proceed with a mathematical proof, I was wondering whether this imply that $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$ is positive definite? Already for diagonal matrices $\mathbf{A}$ and $\mathbf{B}$ , it can be shown readily that this is the case indeed. Can we say anything about the positive definiteness of $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$ in the general situation of interest?","['matrices', 'matrix-calculus', 'linear-algebra', 'optimization', 'positive-definite']"
4715513,"Break a stick at $n$ random points. What is the probability that the three shortest pieces can form a triangle, as $n\to\infty$?","Question : On a stick, choose $n$ uniformly random points, and break the stick at those points. What is the limit of the probability that the three shortest pieces can form a triangle, as $n\to\infty$ ? Context : I'm currently interested in questions about broken sticks. They often have nice answers . My attempt : I don't know how to find the exact probability, so I ran a simulation on Excel. For each of $n=10,n=100,n=1000$ , the probability seems to be roughly $0.45$ . So it seems that the probability quickly converges... but to what?","['limits', 'geometric-probability', 'triangles', 'probability']"
4715528,Girsanov theorem for discrete-time stochastic processes,"I am reading Buehler et al. (2022) ""Learning to Trade II: Deep Hedging"" and the slide on p. 44 states Fun fact: in discrete time, we can change also the volatility of a
process by changing measure. I am familiar with continuous-time stochastic processes and the change of measure there, which (through Girsanov's theorem) would only change the drift part but not the diffusion part of the process. Now, according to the quote above, I am assuming that a discrete time version of Girsanov's theorem exists with the additional property of changing the diffusion part in a difference equation. However, I haven't been able to find papers or textbooks describing this. I would be grateful if somebody could point me to such an article (preferably with examples) or could outline how this could be derived.","['stochastic-processes', 'probability', 'discrete-time']"
4715555,Is a holomorphic $f\colon U\to\mathbb{C}$ with continuous extension to $\overline{U}$ Lipschitz continuous on $\partial U$?,"Let $U\subset\mathbb{C}$ be a bounded connected open subset with smooth boundary $\partial U$ . Suppose that we have a holomorphic function $f\colon U\to\mathbb{C}$ that can be continuously extended to the closure $\overline{U}$ . Is $f$ then Lipschitz continuous on the boundary $\partial U$ ? In other words, does there exist a constant $L\geq0$ such that $$|f(z_{1})-f(z_{2})|\leq L|z_{1}-z_{2}|$$ for all $z_{1},z_{2}\in\partial U$ ? According to this MSE post one has $|f'(z)|\leq 1/\rho(z)$ for all $z\in U$ , where $\rho(z)$ denotes the distance from $z$ to $\partial U$ . Because of this bound, I have the feeling that it may happen that $|f'(z)|$ blows up near $\partial U$ . So I suspect that my claim is false in general. Any suggestions or examples would be greatly appreciated.","['complex-analysis', 'continuity', 'cauchy-integral-formula', 'lipschitz-functions']"
4715583,Question on computing $\sum_{n=1}^\infty \tan^{-1} \left(\frac{3}{n^2 + n - 1}\right)$,"Find $$\sum_{n=1}^\infty \tan^{-1} \left(\frac{3}{n^2 + n - 1}\right)$$ I know this can be simplified to a telescoping series of the form $$\sum_{n=1}^\infty\tan^{-1}(n+2) - \tan^{-1}(n-1)$$ The correct answer, on evaluating the limit for this would be, $\tan^{-1} (n+2) + \tan^{-1} (n+1) +\tan^{-1} (n)- \tan^{-1} 0 - \tan^{-1} (1) - \tan^{-1} (2) =\frac{ 3\pi}4 - \cot^{-1} 2$ But when summing to infinity why do we have remanent $\tan^{-1} (n+2) + \tan^{-1} (n+1) +\tan^{-1} (n)$ since for every $ \tan^{-1} n$ we would have a $-\tan^{-1} n$ . Something similar to the argument how set of natural numbers and integers is equal $(n(N) = n(I))$ . In problems I've done before the telescoping sum usually tends to 0 so I might be thinking of it in the wrong way.","['trigonometric-series', 'analysis', 'infinity', 'sequences-and-series', 'trigonometry']"
4715587,"Is $\mathbb{R}\cup\{-\infty,+\infty\}$ the categorical co-completion of $\mathbb{Q}$","Seeing $\mathbb{Q}$ as an ordered set, the colimit of a diagram $D:\mathcal{I} \to \mathbb{Q}$ , when it exists, is just $\operatorname{colim}D \cong \operatorname{sup}_iD(i)$ . It seems to me that given any diagram $D:\mathbb{Q} \to \mathcal{E}$ into a cocomplete category $\mathcal{E}$ , it extends to a functor $L:\mathbb{R}\cup\{-\infty,+\infty\} \to \mathcal{E}$ given by $L(r)=\operatorname{colim}\{D(q):q \leq r, q\in \mathbb{Q}\}\in \mathcal{E}$ . This establishes $\mathbb{R}\cup\{-\infty,+\infty\}$ as the free co-completion of $\mathbb{Q}$ . Moreover, we obtain the density result asserting that every real number is the sup of all smaller rational numbers as a corollary of the fact that every presheaf is a colimit of representables. Proof: $L$ is a functor preserving colimits and extending $D$ by construction. Given another $L'$ with these properties, since every $r$ is the colimit (= sup) of all its smaller rational numbers, it must be $L \cong L'$ . Am I stating something wrong here? I just thought about it, and it seems a nice and elementary example, but I have not seen it stated anywhere, which is very strange, and makes me suspect I am missing something and there's something wrong?","['real-numbers', 'limits-colimits', 'category-theory', 'analysis', 'real-analysis']"
4715669,Why does limit: $\lim_{x\to\infty}\prod\limits^{\lfloor x\rfloor}_{n=1}\frac{\lceil x\rceil}{\lfloor x\rfloor}=e$?,"This is for all $x\in\mathbb{R}$ and $x\notin\mathbb{Z}$ because it equals $1$ for all positive integers. I was just messing around with floor and ceiling functions in Desmos when I came upon this. I have not been able to prove this and it is at the moment only an observation.
To me, this makes absolutely no sense and I have no clue how to begin solving this. When typing it into WolframAlpha, it just gave me an answer of $1$ because it did not exclude integers. It also gave me a partial product formula of $\left(\frac{\lceil x\rceil}{\lfloor x\rfloor}\right)^n$ but this seems like it would not work for $n=\infty$ . This is what it looks like in Desmos: Is anyone able to explain this to me? How do you even prove this? Thanks for the help in advance!","['ceiling-and-floor-functions', 'real-analysis', 'sequences-and-series', 'infinite-product', 'limits']"
4715696,Reference request: Étale cohomology of quotient by finite group,"Let $X$ be a variety over $\mathbb{F}_q$ . Let $G$ be a finite group of automorphisms of $X$ such that the map $f:X \to X/G$ is Galois. It is then the case that $H^i(X/G)=H^i(X)^G$ (where $H^i$ denotes étale cohomology with coefficients in $\mathbb{Q}_l$ ). As I understand, this follows from the Hochschild-Serre spectral sequence which can be found in the book by Milne. Is there a reference which states the above explicitly (i.e. the isomorphism)? Moreover, is there a reference which shows that the above isomorphism is Frobenius equivariant? Or, is there a reference for the Frobenius equivariance of the Hochschild-Serre spectral sequence?","['etale-cohomology', 'finite-fields', 'algebraic-geometry', 'homology-cohomology']"
4715713,But when will $i^x=2$?,"So I was looking through the homepage of Youtube to see if there were any math equations that I might be able to solve when I came across this video by Blackpenredpen asking if $i^x$ will ever equal $2$ , which I thought that I might be able to find a solution for. Here is my attempt at doing so: $$i^x=2$$ $$\implies x\ln(i)=\ln(2)$$ $$\implies x=\dfrac{\ln(2)}{\ln(i)}$$ Which now we need to simplify $\ln(i)$ . Here's how we do so: Using Euler's Identity $$e^{i\theta}=\cos(\theta)+i\sin(\theta)$$ Since we know that $$-1=\cos(\pi+2\pi k)+i\sin(\pi+2\pi k)=e^{i\pi+2i\pi k}$$ since $\cos(x)$ and $\sin(x)$ are both periodic with period $2\pi$ (if I remember correctly), $$\implies i=\sqrt{-1}=\sqrt{e^{i\pi+2i\pi k}}=e^{\frac{i\pi}2+i\pi k}$$ $$\implies x=\dfrac{\ln(2)}{\ln(i)}$$ $$=\dfrac{\ln(2)}{\left(\dfrac{i\pi}2+i\pi k\right)}$$ $$\implies\dfrac{\ln(2)}{\left(\dfrac{i\pi+2i\pi k}2\right)}$$ $$=\dfrac{2\ln(2)}{i\pi+2i\pi k},k\in\mathbb{Z}$$ Which we can prove that that is the answer like this: $$i^x=2$$ $$i^\frac{2\ln(2)}{i\pi+2i\pi k}=2$$ $$\implies\dfrac{2\ln(2)\ln(i)}{i\pi+2i\pi k}=\ln(2)$$ $$\implies\dfrac{2\ln(i)}{i\pi+2i\pi k}=1$$ $$2\ln(i)=i\pi+2i\pi k$$ $$i^2=-1=e^{i\pi+2i\pi k}$$ $$\therefore i^\frac{2\ln(2)}{i\pi+2i\pi k}\overset\checkmark=2$$ $$\therefore\text{ for any }i^x=z$$ $$z=\dfrac{2\ln(z)}{i\pi+2i\pi k},k\in\mathbb{Z}$$ My question Is the solution that I have achieved correct, or what could I do to achieve the correct solution or attain it more easily? Mistakes I might have made Remembering Euler's Identity Remembering the period of both $\sin(x)$ and $\cos(x)$ My assumption for solving any $i^x=z$ Complex logarithms :\ To clarify The final result should probably be written as $$x=\dfrac{-2i\ln(2)}{\pi+2\pi k}$$ meaning that for any $$i^x=z$$ $$x=\dfrac{-2i\ln(z)}{\pi+2\pi k}$$","['trigonometry', 'solution-verification', 'complex-numbers', 'logarithms']"
4715716,"For the obtuse ∠ABC, right ∠ADE, and FG where F is on AB and G is on BC what is the length of DE, where E is the bisector of FG","For the obtuse ∠ABC, right ∠ADE, and FG where F is on AB and G is on BC what is the length of DE, where E is the bisector of FG. The title technically explains it, but I've drawn a diagram to make it easier to understand. Diagram of the problem (sorry for the poor quality). I need two pieces of information from the diagram (I'm using this for a coding project, but that is only relevant because it explains why I don't use any numbers after this. All the values are dynamic). I need to find the measure of $\overline{DE}$ and the measure of ∠EFD (and I know I can find one once I have the other). Here's the information I have to start: The slopes of $\overline{AB}$ and $\overline{BC}$ in $\frac{rise}{run}$ The measure of ∠ABC is > 90 ∠ADE is a right angle The length of $\overline{FG}$ E bisects $\overline{FG}$ Using math, I know how to get: The measure of ∠ABC The measure of $\overline{FE}$ and $\overline{EG}$ This is where I get stuck. I have a bunch of information, but I can't figure out how to put it together. Thanks in advance for the help! Note: Since I posted this question, I noticed several things that cause it to not be a full solution to the problem I need it for. Thus, I have posted an updated version. If you want to help me, here's the link: https://stackoverflow.com/questions/76498751/finding-footing-for-proceedural-animation","['trigonometry', 'angle', 'geometry']"
4715784,Is there such a thing as a vector without a basis?,"My professor claims that there is no such thing as a vector without a basis, but I claim that there exists ""raw"" vectors. For example, let's say you have a basis in $\mathbb{R}$ : $$\beta = \{\langle 1, 0 \rangle, \langle 0, 1 \rangle\}$$ Then the vectors in $\beta$ , are they with respect to a basis, or are they ""raw"" vectors? Perhaps my terminology is not correct, but I hope my meaning is clear, that vectors can exist without a basis, for example $m \in M_{2 \times 2}(\mathbb{F})$ . I think you would write it as a coordinate vector if it were in terms of a basis, and as a matrix in ""raw"" format.","['matrices', 'vectors', 'linear-algebra', 'vector-spaces']"
4715801,Prime number curiosity : $ \lim\limits_{n\to\infty}\frac{\exp(1+\sum_{i=1}^n\frac1{p_i-1})}{\prod_{i=1}^n(1+\frac1{p_i-1})}=\frac{79}{25}$ !?,"Let $0<n$ be a positive integer.
Let $p_n$ be the $n$ -th odd prime. $$p_1 = 3,p_2=5,p_3=7,p_4=11,\ldots$$ Let $$f(n) = \exp \left(1 + \sum_{i=1}^{n}\frac{1}{p_i-1} \right)$$ Let $$g(n) = \prod_{i=1}^{n} \left(1+\frac{1}{p_i-1} \right)$$ Let $$t(n) = \frac{f(n)}{g(n)}$$ Then this limit exists $$ \lim_{n \to +\infty} t(n) = \frac{79}{25}$$ And for sufficiently large $n$ we have : $$ \frac{79}{25} - \frac{\ln(n)+2}{n^2-1} < t(n) < \frac{79}{25} - \frac{1}{n^5}$$ How to formally prove it ?","['number-theory', 'asymptotics', 'fractions', 'limits', 'prime-numbers']"
4715820,"can we say a function $f:[a,b]\to \Bbb{R}$ is continuous at the end points?","$f:[a,b]\to \Bbb{R}$ is said be continuous at a point $m$ that means whenever a sequence $x_n \in[a,b]$ converge to ${m}$ the image ${f(x_n)} $ converge to ${f(m)}$ and by applying this definition of continuous function if $x_n \in[a,b]$ converge to ${a}$ the image ${f(x_n)}$ converge to ${f(a)}$ and that means that the function is continuous at $a$ but if we apply the limit definition of continuity
clearly $\lim_{x\to a^+}f(x)$ does not equal $\lim_{x\to a^-}f(x)$ because the limit doesn't exist in the left side
so is the function continuous at its end points? and if so what did I get wrong?","['epsilon-delta', 'real-analysis', 'continuity', 'calculus', 'limits']"
4715948,Why is the empty set considered a set?,"I have recently been thinking about the following question: Why is $\emptyset$ considered as a set? The definition of a set is: a well-defined collection of ""elements"".
So my question arose, that $\emptyset$ is well defined but does not contain any elements which should be there according to the set's definition.","['elementary-set-theory', 'definition', 'terminology']"
4715951,What rule do I violate as I transform a given infinite series?,"Infinite series are challenging (even when you know it)! I describe some background before asking my question(s). A programming assignment asked to use an abstraction called product to verify the following approximation of $\pi$ : $$
\frac{\pi}{4}=\frac{2\cdot 4\cdot 4\cdot 6\cdot 6\cdot 8\cdot 8 \cdots}{3\cdot 3\cdot 5\cdot 5\cdot 7\cdot 7 \cdots}\tag{1}\label{eq1}
$$ I, rather credulously, rearranged the series in \eqref{eq1} as: $$
{\pi\over 4}=2\cdot \left(\frac{4\cdot 4}{3\cdot 3} \right)\cdot \left(\frac{6\cdot 6}{5\cdot 5} \right)\cdot \left(\frac{8\cdot 8}{7\cdot 7} \right)\cdots = 2\cdot\prod_{i=1}^{\infty} \left(\frac{2i+2}{2i+1} \right)^2
\tag{2}\label{eq2}
$$ Then I wrote the program (using $10$ terms of the form $\left(\frac{2i+2}{2i+1} \right)^2$ where $i$ goes from $1$ to $10$ ) only to realize that I had committed a blunder. My faithful program calculated ${\pi\over 4}$ as $17.675817$ . I then looked up Wallis Product on Wikipedia which defines it as: $$
{\pi\over \textbf{2}}= \left(\frac{2\cdot 2}{1\cdot 3} \right)\cdot \left(\frac{4\cdot 4}{3\cdot 5} \right)\cdot \left(\frac{6\cdot 6}{5\cdot 7} \right)\cdots = \prod_{n=1}^{\infty}\frac{4n^2}{4n^2-1}
\tag{3}\label{eq3}
$$ I removed the bug in my program by using Wallis Product as defined in \eqref{eq3}. I am somewhat aware of the convergent tests of infinite series, but I guess I am not that familiar with their valid manipulations. Here are my questions based on this experience: Exactly what disallows equation \eqref{eq2} as a rearrangement of \eqref{eq1}? Why aren't these two equations equivalent? Why is \eqref{eq3} a valid rearrangement of \eqref{eq1} (if it is, that is)?","['infinite-product', 'sequences-and-series']"
4715957,Confused by integration calc in my financial maths notes!,"I have added a pic of the notes that I am looking at! I’m a little confused by them.In integration calcs, I’m used to seeing the integral of $f(x)$ to get the difference between two points. Here it’s integrating $f^\prime(x)$ to get the difference. I’m trying to picture the problem graphically - I’m assuming that $M$ is along the $y$ -axis and $t$ is on the $x$ -axis. In which case wouldn’t we just integrate $M(t)$ ? Unless $M(t)$ is supposed to be the CDF? Basically I don’t understand why it’s the integral of $M^\prime(t)$ and not the integral of $M(t)$ Sorry getting very confused with this! Any help is much appreciated","['integration', 'derivatives', 'finance']"
4715975,About a sequence of (improper) integrals.,"One considers the following sequence $$
I_n=\int_0^1 \log^2(t).(1-t)^n dt\ ,
$$ using elementary methods (integration by parts, twice), one can show that $$
I_n=2\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{(k+1)^3}
$$ Question : Is there a simple way to prove or disprove that this (decreasing, non-negative) sequence tends to zero ? Motivation : In the theory of star algebras, one considers the sub-star algebra of $\mathcal{C}(]0,1],\mathbb{C})$ generated by $\{\log(t),t\}$ and the positive linear form $\varphi : P\to \int_0^1 P(t) dt$ (where $P(t)=Q(\log(t),t),\ Q\in \mathbb{C}[X,Y]$ ) which generates a hilbertian form $$
\langle u(t)\mid v(t)\rangle:=\varphi(v^*u) \qquad (*) 
$$ the target being to prove that the operator $f\to \log(t)f(t)$ is discontinuous for the norm induced by (*). Late edit (Thanks to all contributors) We see here that, with $P_n(t)=\frac{n}{\log^2(n)}(1-t)^n$ , we have $\lim_{n\to \infty}P_n=0$ whereas $\lim_{n\to \infty}\log(t)P_n(t)=1$ for this norm.","['calculus', 'improper-integrals']"
4716001,How to draw and visualize a certain area in $\mathbb{R}^3$,"I'm given an area in $\mathbb{R}^3$ that is defined as the following: $$ x = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \cos(\varphi)$$ $$ y = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \sin(\varphi)$$ $$ z =  \rho \cos(\frac{\varphi}{2}) $$ where $\rho \in [-\frac{1}{2}; \frac{1}{2}]$ and $\varphi \in [0;2\pi]$ .
I just need to calculate its area, which is cumbersome but easy enough, but is it possible to draw it / have a good intuition what it could look like, which also could help my in the calculation of its area? Is there a general way to go about a problem like this? I already tried inserting some points but I didn't see any pattern at all.","['integration', 'analysis', 'real-analysis', 'multivariable-calculus', 'calculus']"
4716014,An Ambiguous Definition of Envelopes with reference to Differential Equations and Singular Solutions.,"Recently, I was, going through a definition of envelope. I know that the definitions can be written and the exposition of it, might vary, but the following definition, which I am hereby mentioning, has the definition of an envelope given with the reference to a differential equations of 1st order(, and probably 2nd degree,) and I am unable to make up /decipher the meaning. Here it is: Note:Throughout this post, the variable $p$ designates, $\frac{dy}{dx}.$ The function, $f(x,y,p)=0$ respresents a differential equation and $\phi(x,y,c)$ represents the corresponding general solution of $f$ with $c$ as the arbitrary constant. The envelope: If in $\phi(x,y,c)= 0,$ c be given all possible values, there is obtained a set of curves, infinite in number, of
the same kind. Suppose that the $c'$ s are arranged in order of
magnitude, the successive $c$ 's thus differing by infinitesimal
amounts, and that all these curves are drawn. Curves corresponding, to two consecutive values of $c$ are called consecutive curves, and their intersection is called an "" ultimate point of intersection "". The limiting position of these points of intersection includes the envelope of the system of curves. It is shown in works on the differential calculus, that the envelope is part
of the locus of the equation obtained by eliminating $c$ between $$\phi(x,y,c)=0,$$ and $$\frac{\partial \phi}{\partial c}=0,$$ that is, the envelope is part of the locus of the $c$ discriminant
relation. This might have been anticipated, because in the
limit the $c$ 's for two consecutive curves become equal, and the $c$ discriminant relation represents the locus of points for which $\phi(x,y,c)= 0,$ will have equal values of $c.$ It is also shown in the differential calculus, that at any point on the envelope, the latter is touched by some curve of the system; that is, that the envelope and some one of the curves have the same value of $p$ at the point. The definitions used in this article for reference are : p-discriminant -When the equation is quadratic, the discriminant can be
written immediately; but when it is such that the condition for equal roots is not easily perceived, the discriminant is found in the following way. The given equation being $F=0,$ form another equation by (partially)  differentiating $F$ with respect to the variable, and eliminate the variable between the two equations. For example, $\phi(x,y,c)= 0,$ may be looked on as an equation in $c$ , its coefficients then being
funetions of $x$ and $y.$ The simplest rational function of $x$ and $y,$ whose vanishing expresses that the equation $\phi(x,y,c)=0$ has equal roots for $c,$ is called the $c$ discriminant of $\phi,$ and is
obtained by eliminating $c$ between the equations, $$\phi(x,y,c)= 0,$$ and $$\frac{\partial \phi}{\partial c}=0.$$ Thus the $c$ discriminant relation represents the locus, for each
point of which $\phi(x,y,c)= 0,$ has equal values of $c$ . Similarly, the $p$ discriminant of $f(x, y, p) = 0,$ the differential equation corresponding to $\phi (x, y, c)= 0, $ is obtained by eliminating $p$ between the equations, $$f(x,y,p)= 0,$$ and $$\frac{\partial f}{\partial p}=0.$$ Thus the $p$ discriminant relation represents the locus, for each
point of which $f(x, y, p)= 0$ has equal values of $p.$ In order that there may be a $c$ and a $p$ discriminant, the above equations must be of the second degree at least in $c$ and $p.$ Here, we assumed, that these equations are of the same
degree in $c$ and $p,$ and hence, if there is a $p$ discriminant, there must be a $c$ discriminant. First of all, I dont understand the fact where it says, ""Curves corresponding, to two consecutive values of $c$ are called consecutive curves, and their intersection is called an "" ultimate point of intersection ."" I feel this is erroneous , for $c$ is an arbitrary real constant and there is nothing as, ""consecutive real numbers,"" since there are infinite real numbers between any two real numbers. Finally, where they are supposedly asserting the definition of the envelope as  : The limiting position of these points of intersection includes the envelope of the system of curves. I don't have an idea, what are they trying to imply by the phrase, "" limiting position of these points of intersection "". I don't understand the meaning at all. This whole article is turning to be much confusing. Any explanations addressing these issues will be greatly appreciated. An intuitive geometrical  picture concerning this particular notion in the article will be much helpful. The above definitions/contents  appear in the book ,""An Introductory course in Differential Equations "" by D.A Murray, on the chapter titled ""Singular Solutions.""","['envelope', 'definition', 'ordinary-differential-equations', 'differential-geometry']"
4716022,"Let $f:[0,1] \to \mathbb R$ differentiable s.t $f(0)=0$. $\forall x \in [0,1]$: $|f^{'}(x)| \le |f(x)|$. Prove $f(x) =0$, $\forall x \in [0,1]$.","Let $f:[0,1] \to \mathbb R$ differentiable s.t $f(0)=0$ . $\forall x \in [0,1]$ : $|f^{'}(x)| \le |f(x)|$ . Prove $f(x) =0$ , $\forall x \in [0,1]$ . I've tried proving this with Mean Value Theorem but I think one of my implementations isn't good enough. I'd love some help! First of all we have $f(x)$ is differentiable $\implies$ $f(x)$ is continuous in $[0,1]$ . From MVT we have: exists some $c_1 \in (0,1)$ s.t $|f(1)| = {f(1)-f(0) \over 1-0} = f^{'}(c_1)$ . From the datum we can deduce: $0 \le f(1) = |f^{'}(c_1)| \le |f(c_1)|$ . Now take some $b \in (0,1)$ . Let us examine the interval $[0,b]$ . From MVT we have some $c_2 \in (0,b)$ s.t: $|f(b)| \lt {f(b) \over b} = {f(b)-f(0) \over b-0} = f^{'}(c_2) \implies 0 \le f(b) \lt |f^{'}(c_2)| \le |f(c_2)|$ . This is the tricky part: Since we took some random $b \in (0,1)$ we have $\forall x \in (0,1]$ we have $0 \le |f(x)| \le f(0) = 0$ thus implementing $f(x) = 0, \forall x \in [0,1]$ as needed. I understand the last conclusion is based on a limit type of argument which I have not provided.","['continuity', 'calculus', 'derivatives', 'mean-value-theorem']"
4716030,"For a $C^{\infty}$ function $f$ , if $|f(x)|\leq \exp(-\frac{1}{x^{2}})$ then is $f^{(n)}(0)=0$?","Suppose $f\in C^{\infty}(\Bbb{R})$ . If $|f(x)|\leq \exp(-\frac{1}{x^{2}})$ for all $x\neq 0$ and $f(0)=0 $ . Does this imply $f^{(n)}(0)=0$ for all $n$ ? I can see that $|\frac{f(h)}{h}|\leq \frac{1}{h}e^{\frac{-1}{h^{2}}}\to 0$ as $h\to 0$ which shows that $f'(0)=0$ . But I am unsure about $n\geq 2$ . Is it true for $n\geq 2$ ? If so, how can I prove this for $n\geq 2$ ? I  tried to use Lagrange's Mean Value Theorem to see that $f'(ch)=\frac{f(h)}{h}$ for some $0<c<1$ . But still, I cannot use this to get an approximation for all $f'(h)$ when $h$ is near $0$ . Can anyone help me with this problem?","['calculus', 'functions', 'derivatives', 'real-analysis']"
4716032,How to compute: $\dfrac{2\cos 80^\circ-\sin 70^\circ}{\cos 70^\circ}$,"Question Evaluate $$\dfrac{2\cos 80^\circ-\sin 70^\circ}{\cos 70^\circ}$$ My Working I tried converting $80^\circ$ to $2\cdot 40^\circ$ and $70^\circ$ to $30^\circ+40^\circ$ . This led to $$\frac{2\cos (2\cdot 40^\circ)-(\frac{\sqrt3}{2}\sin40^\circ+\frac12\cos40^\circ)}{\frac{\sqrt3}{2}\cos40^\circ-\frac12\sin40^\circ}$$ I do not know which double angle formula to use for $2\cos 80^\circ$ , and how to simply the expression. Could someone please help? Thank you!","['algebra-precalculus', 'trigonometry']"
4716070,confusion about stokes theorem,"Consider the open unit disk $\mathbb{D}$ in complex plane $\mathbb{C}$ , let $\mathbb{U}$ be the open disk of the origin with radius $\frac{1}{2}$ , consider the manifold with boundary $\mathbb{D}-\mathbb{U}$ and a holomorphic form $\omega=\frac{1}{z}dz$ on it where $dz=dx+idy$ . Then $d\omega$ is zero on $\mathbb{D}-\mathbb{U}-\partial\mathbb{U}$ since $\frac{1}{z}$ is holomorphic on it hence by stoke's theorem $$
0=\int\int_{\mathbb{D}-\mathbb{U}-\partial\mathbb{U}} d\omega=-\int_{\partial\mathbb{U}}\omega=-\int_{\partial\mathbb{U}}\frac{1}{z}dz
$$ But we know the right side is $-2\pi i$ , what is wrong in this computation?","['complex-analysis', 'manifolds']"
4716098,"On average, how many times must a circular pizza be randomly cut, to get a piece with no curved edge?","On a circular pizza, we make a random straight cut by choosing two uniformly random points on the perimeter and cutting through them. On average, how many times must the pizza be randomly cut, to get a piece with no curved edge? (In other words, on average, how many random chords must be drawn on a circle, to get a polygon, if each random chord is drawn by connecting two uniformly random points on the perimeter of the circle?) In the following example, a piece with no curved edge is obtained by the fifth random cut. My attempt I made a pizza cutting simulator on desmos. It seems that the average should be around $7$ or so. I tried to find the probability that a piece with no curved edge is obtained by the $n$ th random cut. But I only worked out that the probability for $n=3$ , is $\frac{1}{15}$ . (Consider the six points that define three cuts. Start with one point: it must be paired with its opposite point, which has probability $\frac15$ . Then one of the remaining points must be paired with its opposite point, which has probability $\frac13$ . The remaining two points must be paired together. So the probability is $\frac15\times\frac13=\frac{1}{15}$ .)","['circles', 'expected-value', 'combinatorics', 'probability', 'hypergeometric-function']"
4716164,"Showing $\int_0^1\frac{\log(t^4-t^2+1 )}{t-1} \,dt=\frac{11\pi^2}{72}-\frac{\log^2(2+\sqrt3)}2$","We have: $$\int_{0}^{1}\frac{\log{\left(t^4-t^2+1 \right)}}{t-1} \,dt=\frac{11}{72}\pi^{2}-\frac{\log^{2} \left(2+\sqrt{3} \right)}{2}\tag{1}.$$ Mathematica and Wolfram Alpha are unable to deal with it
(see Wolfram Alpha response ).
The proof I have follows from the dilogarithm function. As suggested by @J.G here it is: \begin{equation}\mathrm{Li}_2(z)=-\int_{0}^{1}\frac{\log{(1-zy)}}{y}dy,\tag{2}
\end{equation} and Euler's formula: \begin{equation}\mathrm{Li}_2(z)+\mathrm{Li}_2(1-z)=\frac{\pi^2}{6}-\log{(1-z)}\log{(z)}.\tag{3}
\end{equation} Combining $(2)$ and $(3)$ : \begin{align*}
\mathrm{Li}_2(z)+\mathrm{Li}_2(1-z)=-\int_{0}^{1}\frac{\log{((1-tz)(t(z-1)+1))}}{t} dt \\=\frac{\pi^2}{6}-\log{(1-z)}\log{(z)},\tag{4}
\end{align*} Then if $z_{1}=\frac{1}{2}-\frac{2+\sqrt{3}}{2}i$ and $z_{2}=\frac{1}{2}-\frac{2-\sqrt{3}}{2}i$ , we deduce from $(2)$ : \begin{align*}i)\hspace{.5cm}\mathrm{Re}\left(\mathrm{Li}_2(z_{1})+\mathrm{Li}_2(z_{2})\right)=\mathrm{Re}\left(-\int_{0}^{1}\frac{\log{(1-z_{1}t)}}{t}dt-\int_{0}^{1}\frac{\log{(1-z_{2}t)}}{t}dt\right)=\\
-\frac{1}{2}\left( \int_{0}^{1}\frac{\log{(t^2(2+\sqrt{3})-t+1)}}{t}dt+ \int_{0}^{1}\frac{\log{(t^2(2-\sqrt{3})-t+1)}}{t}dt\right)=
-\frac{1}{2}(I_{1}+I_{2}).
\end{align*} In other hand with $(4)$ we obtain: \begin{align*}I_{1}=\int_{0}^{1}\frac{\log{((1-tz_{1})(t(z_{1}-1)+1))}}{t} dt=\int_{0}^{1}\frac{\log{(t^2(2+\sqrt{3})-t+1)}}{t}dt =-\frac{\pi^2}{6}+\log{(1-z_{1})}\log{(z_{1})}\\=\frac{\pi^2}{144}+\frac{\log^2{(2+\sqrt{3})}}{4},\end{align*} and \begin{align*}I_{2}=\int_{0}^{1}\frac{\log{((1-tz_{2})(t(z_{2}-1)+1))}}{t} dt=\int_{0}^{1}\frac{\log{(t^2(2-\sqrt{3})-t+1)}}{t}dt=-\frac{\pi^2}{6}+\log{(1-z_{2})}\log{(z_{2})}\\=-\frac{23\pi^2}{144}+\frac{\log^2{(2-\sqrt{3})}}{4}=-\frac{23\pi^2}{144}+\frac{\log^2{(2+\sqrt{3})}}{4}.\end{align*} Then replacing $I_{1}$ and $I_{2}$ in $i)$ we have: $$\mathrm{Re}\left(\mathrm{Li}_2(z_{1})+\mathrm{Li}_2(z_{2})\right)=\frac{11}{144}\pi^{2}-\frac{\log^{2}(2+\sqrt{3})}{4}.\tag{5}$$ Then observe that: $$I_{1}+I_{2}=\int_{0}^{1}\frac{\log{(-(y^2(\sqrt{3}+2)-y+1)(y^2(\sqrt{3}-2)+y-1))}}{y}dy,$$ since $-(y^2(\sqrt{3}+2)-y+1)(y^2(\sqrt{3}-2)+y-1)=y^4-4y^3+5y^2-2y+1$ , then: $$I_{1}+I_{2}=\int_{0}^{1}\frac{\log{(y^4-4y^3+5y^2-2y+1)}}{y}dy\overset{y=1-t}=\int_{0}^{1}\frac{\log{(t^4-t^2+1)}}{t-1}dt,$$ and the result follows from $(i)$ and $(5)$ . Question: Can we prove this integral using known series expansions or in an alternative way? A proof without using the dilogarithm ""function"" is so appreciated.","['integration', 'closed-form', 'definite-integrals', 'special-functions']"
