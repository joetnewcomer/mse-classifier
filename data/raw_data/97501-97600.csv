question_id,title,body,tags
1345432,"The ""argument"" of a quaternion","My question is pretty simple. I've been trying to read a pretty introductory text on Clifford algebras, and I encountered how they define the ""argument"" of a quaternion as an ordered quadruple representing the components of the noramlized quaternion. Now, in $\mathbb{C}$, we have this map $\arg : \mathbb{C} \setminus \{ 0 \} \to [0, 2 \pi)$ such that
\begin{align*}
\arg (z_{1} z_{2}) & = \arg(z_{1}) + \arg(z_{2}) , \\
\arg(z_{1}) = \arg(z_{2}) & \iff \frac{z_{1}}{z_{2}} \in \mathbb{R}_{ > 0 } .
\end{align*}
So my question is, is there a map $$\arg' : \mathbb{H} \to [0, 2 \pi)^{2}$$ such that, again, $\arg'$ is additive under multiplication, $$\arg'(z_{1}) = \arg'(z_{2}) \iff \frac{z_{1}}{z_{2}} \in \mathbb{R}_{ > 0 }$$ and $$\arg' |_{\mathbb{C}} = (\arg, 0) ?$$","['quaternions', 'hypercomplex-numbers', 'complex-analysis']"
1345443,Am I using the chain rule correctly?,"I'm supposed to find $y'$ and $y''$ of this function: $$y=e^{\alpha x} \sin\beta x$$ This is what I have done so far: $$y'=e^{\alpha x}\sin\beta x\cdot \alpha x'\sin\beta x\cdot \sin'\beta x \cdot \beta x$$ $$y'=e^{\alpha x}\sin\beta x\cdot \alpha \sin\beta x\cdot \cos\beta x \cdot \beta$$ I tried to find $y''$, but my answer was really messy so I must not have done the first derivative correctly. What have I done wrong?","['calculus', 'derivatives']"
1345456,Surface area of a slightly deformed sphere,"Consider the unit sphere, which can either be described by $x^2+y^2+z^2=1$ or by the equation $r(\theta,\phi)=1$, where $(r,\theta,\phi)$ are spherical polar coordinates. I define a deformed sphere by the equation, $r(\theta,\phi) = 1+\delta r(\theta,\phi)$, where $\delta r (\theta,\phi)$ is a small smooth deformation of the surface. How would I go about writing a formal expression for the surface area of this deformed sphere, in a perturbative expansion in $\delta r$.","['differential-geometry', 'manifolds', 'surfaces', 'minimal-surfaces']"
1345499,Total variation measure vs. total variation function,"Let $a, b \in \mathbb{R}$ with $a < b$ and define the compact interval $I := [a, b]$. Let $g, h : \mathbb{R} \rightarrow \mathbb{R}$ be non-decreasing and right-continuous on $I$ and constant on $(-\infty, a]$ and on $[b, \infty)$, separately, and define $f := g - h$, so that, over every compact interval, $g - h$ is a Jordan decomposition of the bounded-variation function $f$. Denote by $\mu_g$ the Lebesgue-Stieltjes measure on $(\mathbb{R}, \mathcal{B})$ engendered by $g$ and denote by $\mu_h$ the one engendered by $h$, and define the (finite) signed measure $\mu_f := \mu_g - \mu_h$. Denote by $\mu_f'$ the total variation measure corresponding to $\mu_f$, i.e. if $\mu_f = \mu_f^+ - \mu_f^-$ is the unique Jordan decomposition of $\mu_f$, then $\mu_f' = \mu_f^+ + \mu_f^-$. Denote by $v_f : [a, \infty) \rightarrow [0, \infty]$ the total variation function of $f$ on the ray $[a, \infty)$, i.e. for every $t \in [a, \infty)$,
$$
v_f(t) := \sup \left\{\sum_{k = 1}^n \left|f(t_k) - f(t_{k - 1})\right| :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\}\right\}
$$
It is well known that $v_f(a) = 0$, that $v_f$ is non-decreasing and, since $f$ is right-continuous, so is $v_f$. Extend $v_f$ to the entire real line by setting $v_f(t) := 0$ for $t \in (-\infty, a)$. The extended $v_f$ remains non-decreasing and right-continuous. Denote by $\mu_v$ the Lebesgue-Stieltjes measure engendered by $v_f$. Is it the case that $\mu_f' = \mu_v$? Attempts at a solution Attempt #1 Since $\mu_f'(I^c) = \mu_v(I^c) = 0$, it remains to show that $\mu_f' = \mu_v$ over $(I, \mathcal{B}(I))$. All I've managed to show so far is that for $t \in [a, b]$, $\mu_v([a, t]) \leq \mu_f'([a, t])$. Indeed, according to proposition 10.23 in Yeh's ""Real Analysis"", 2nd edition , for every $E \in \mathcal{B}(I)$,
$$
\mu_f'(E) = \sup \left\{\sum_{k = 1}^n |\mu_f(E_k)| :\mid E = \bigcup_{k = 1}^n E_k; E_1, \dots, E_n \in \mathcal{B}(I) \text{ pairwise disjoint}, n \in \{1, 2, \dots\}\right\}
$$
Therefore, for every $t \in I$,
$$
\begin{split}
\mu_v([a, t]) & = \lim_{n \rightarrow \infty} \mu_v\left((a - \frac{1}{n}, t]\right) \\
& = \lim_{n \rightarrow \infty} \left(v_f(t) - v_f(a - \frac{1}{n})\right) \\
& = v_f(t) \\
& \leq \mu_f'([a, t])
\end{split}
$$ If we can show the converse inequality, namely that for every $t \in I$, $\mu_f'([a, t]) \leq \mu_v([a, t])$, we may proceed to conclude that $\mu_v = \mu_f'$ by using Dynkin's $\pi$-$\lambda$ theorem . Attempt #2 Define, for $t \in [0, \infty)$,
$$
\begin{align}
v_f^+(f) & := \sup \left\{\sum_{k = 1}^n\max\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\} \\
v_f^-(f) & := \sup \left\{\sum_{k = 1}^n-\min\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\}
\end{align}
$$
Then it can be seen (see, for instance, lemma 5.2.3 on p. 99 and the beginning of the proof of theorem 5.2.4 on p. 100 of Royden's ""Real Analysis"", 2nd edition (!), Macmillan Library Reference, 1968) that $v_f^+$ and $v_f^-$ are non-decreasing on $[0, \infty)$ and that for $t \in [0, \infty)$,
$$
\begin{align}
f(t) & = v_f^+(t) - v_f^-(t) \\
v_f(t) & = v_f^+(t) + v_f^-(t)
\end{align}
$$
It can also be shown that $v_f^+$ and $v_f^-$ are right continuous, by emulating the analogous proof for $v_f$ (see for instance theorem 12.22 on p. 266 of Yeh's ""Real Analysis"", 2nd edition). We may extend $v_f^+$ and $v_f^-$ to the entire real line by defining $v_f^+(t) := v_f^-(t) := 0$ for $t \in (-\infty, 0)$. The extended functions remain non-decreasing and right-continuous. If we denote the Lebesgue-Stieltjes measures engendered by $v_f^+$ and $v_f^-$ by $\mu_v^+$ and $\mu_v^-$, respectively, we have that $\mu_f = \mu_v^+ - \mu_v^-$ (indeed, as shown here , if $\phi_1, \phi_2, \varphi_1, \varphi_2 : \mathbb{R} \rightarrow \mathbb{R}$ are non-decreasing, right-continuous functions such that $\phi_1 - \phi_2 = \varphi_1 - \varphi_2$, then, denoting the Lebesgue-Stieltjes measures they engender by $\mu_{\phi_1}, \mu_{\phi_2}, \mu_{\varphi_1}, \mu_{\varphi_2}$, respectively, we have $\mu_{\phi_1} - \mu_{\phi_2} = \mu_{\varphi_1} - \mu_{\varphi_2}$, as long as both sides of the equation are well defined, i.e. as long as at least one measure on each side of the equation is finite), and, furthermore, it may be verified using Dynkin's $\pi$-$\lambda$ theorem starting from the $\pi$-system of half open-half closed finite intervals, that $\mu_v = \mu_v^+ + \mu_v^-$. Therefore, if we can prove that $\mu_v^+ \perp \mu_v^-$, i.e. that $\mu_v^+$ and $\mu_v^-$ are mutually singular, then, by the uniqueness of the Jordan decomposition, $\mu_v^+ - \mu_v^-$ is the Jordan decomposition of $\mu_f$, i.e. $\mu_f^+ = \mu_v^+$ and $\mu_f^- = \mu_v^-$, and therefore we have, by definition of $\mu_f'$, that $\mu_v = \mu_f^+ + \mu_f^- = \mu_f'$, as desired. So, all that is left to show is that $\mu_v^+ \perp \mu_v^-$. Related posts A similar question was asked in this forum almost two years ago , but has remained unanswered.","['real-analysis', 'bounded-variation', 'measure-theory']"
1345525,Construct a diffeomorphism $\phi:M\to M$ such that $\phi(p)=q$ and also $d\phi(X_p)=Y_q$,"PROBLEM: Construct a diffeomorphism $\phi:M\to M$ such that $\phi(p)=q$ and also $d\phi(X_p)=Y_q$, where $M$ is a connected smooth manifold and $p,q \in M$ , $X_p \in T_pM$ and $Y_q \in T_qM$ I know how to construct a diffeomorphism such that $\phi(p)=q$. One can see Guillemin and Pollack's book Differential Topology ,page142, ""Isotopy Lemma"". Alternatively, we can consider a compactly supported vector field $X$ with $\text{supp} X \subset U$ for some coordinate ball centered at $p$. WLOG, may assume $q$ lies in $U$ such that $q=(c,0,...,0)$ and $X= \partial_1$ on some $V \subset \subset U$. Now, since $X$ is compactly supported, the flow $\theta$ of $X$ is complete and we pick $\theta_c :M\to M$. Then,note that $\theta_c$ is a diffeomorphism and given by 
$$(u^1,u^2,...,u^n) \to (u^1+c,u^2,...,u^n)$$ on some neighborhood of $p\in U$; hence, it is the desired diffeomorphism such that $\theta_c(p)=q$ However, with an additional requirement $d\phi(X_p)=Y_q$, I don't know how to continue. I try to adjust the above argument but fail to have any useful idea.","['differential-topology', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
1345541,"Inverse of the complex exponential function, considered as a multivariable function","Consider the complex exponential function $g: \mathbb{C} \to \mathbb{C}, z \mapsto e^z$ . When identifying $\mathbb{C}$ with $\mathbb{R}^2$ in the natural way, then $g$ can be considered as a transformation $f: \mathbb{R}^2 \to \mathbb{R}^2$ . First, I want to concretely find $f(x, y)$ for $(x, y) \in \mathbb{R}^2$ . Next, consider the straight lines parallel to the $x$ - and $y$ -axis. How do their images regarding $f$ look like? Finally, I want to show that $f$ (and therefore the exponential function $g$ ) are locally invertible, (meaning that for each $(x, y) \in \mathbb{R}^2$ , there is a neighbourhood $U \subseteq \mathbb{R}^2$ , $U$ open, in which $f$ is bijective and therefore can be inverted), but that $f$ doesn't have a global inverse function. My attempt: for $z = a + bi \in \mathbb{C}$ : $$e^z = e^{a+bi} = e^a(\cos(b)+ i ºsin(b)) = e^a\cdot \cos(b) + i\cdot e^a\cdot \sin(b)$$ And therefore, I would assume that $f(x, y) = \pmatrix{e^x \cos(y) \cr e^x \sin(y)}$ . If I'm correct with this, I would assume that one could get the image of these straight lines by just fixing either $x$ or $y$ , and seeing what happens: the image of these straight lines would then be $c \pmatrix{\cos(y) \cr \sin(y)}$ or $\pmatrix{c_1 e^x \cr c_2 e^x}$ , with $c, c_1, c_2$ constant (because of the fixed other parameter), would they? I don't really know how to approach the second part though. Thanks in advance.","['analysis', 'multivariable-calculus', 'exponential-function']"
1345571,what would a planetary orbit look like if gravity had constant magnitude?,"Consider a unit-mass particle that is always experiencing a single unit-magnitude
force towards the origin.  This is a central force, but it is not one of the
familiar ones, e.g. gravity whose magnitude is proportional to inverse distance squared,
or a spring force whose magnitude is proportional to distance. So the particle is always accelerating towards the origin
with constant acceleration magnitude $1$ .
Stated as a differential equation,
working in the $x$ - $y$ plane, the particle's position as a function of time $\mathbf{p}(t){=}(x(t),y(t))$ satisfies: $$\ddot{\mathbf{p}}(t) = -\mathbf{p}(t)/\Vert\mathbf{p}(t)\Vert.$$ If we are additionally given initial position $\mathbf{p}(0)$ and velocity $\mathbf{v}(0)=\dot{\mathbf{p}}(0)$ ,
then the function $\mathbf{p}$ is completely determined, and it can be easily computed
numerically to any desired accuracy by simply iterating the following with small enough timestep $dt$ : \begin{align}
    \mathbf{a} &\leftarrow -\mathbf{p}/\Vert\mathbf{p}\Vert \\
    \mathbf{v} &\leftarrow \mathbf{v} + \mathbf{a}\,\,dt \\
    \mathbf{p} &\leftarrow \mathbf{p} + \mathbf{v}\,\,dt
\end{align} My question: is $\mathbf{p}(t)$ a well-known function,
and does it have a closed form? Of course one case of this is a simple circular orbit of unit radius and speed: $$\mathbf{p}(0){=}(1,0), \,\, \mathbf{v}(0){=}(0,1) \,\,\Rightarrow\,\, \mathbf{p}(t)=\left(\cos t,\sin t\right).$$ More generally, a uniform circular orbit of any radius $r$ and speed $\sqrt{r}$ can be obtained: $$\mathbf{p}(0){=}(r,0), \,\, \mathbf{v}(0){=}(0,\sqrt{r}) \,\,\Rightarrow\,\, \mathbf{p}(t)=\left(r \cos\frac{t}{\sqrt{r}},r \sin\frac{t}{\sqrt{r}}\right).$$ and we check that the desired equation holds: \begin{align}
     \dot{\mathbf{p}}(t) &= \left(-\sqrt{r} \sin \frac{t}{\sqrt{r}}, \sqrt{r} \cos \frac{t}{\sqrt{r}}\right) \\
    \ddot{\mathbf{p}}(t) &= \left(-\cos \frac{t}{\sqrt{r}}, -\sin\frac{t}{\sqrt{r}}\right) \\
                         &= -\mathbf{p}(t)/\Vert\mathbf{p}(t)\Vert.
\end{align} Another simple case is when the initial velocity is zero or collinear with the position and the origin;
in this case it's a 1-dimensional problem
and the position can easily be seen to be a simple piecewise quadratic function of time. But what if the initial conditions are not so nicely aligned? To get an idea of the shapes that are possible,
I've made some plots, using gnuplot, of simulations
using the simple evolution algorithm I described earlier, with $dt = 1/10000$ degree $\approx .00000175$ . Figure 1 shows five different initial states,
each evolved from $t{=}0$ to $t{=}2 \pi$ : $\,\,\mathbf{p}(0){=}(1,0)$ , $\mathbf{v}(0){=}(0,v_{0 y})$ for $v_{0 y}{=}0.5,1,1.5,2,2.5$ . Figure 2 shows the one with $\mathbf{v}(0){=}(0,2)$ evolved farther, to $t{=}20\pi$ . Figure 3 shows it evolved even farther, to $t{=}60\pi$ . CLARIFICATION: I am ultimately interested in finding the simplest way of expressing $\mathbf{p}$ as a function of $t$ . I.e. I really want to know ""what this function looks like"" rather than ""what the curve looks like"".  Other parametrizations of the curve, and intuition about shape of the curve, are of interest only if they help lead to this answer. UPDATE 2015/07/02: It sure looks like a spirograph hypotrochoid, doesn't it? http://mathworld.wolfram.com/Spirograph.html . Exploring this possibility,
I found by binary search an initial velocity (0,1.662656) (probably accurate to only 4 decimal places or so)
yielding a closed orbit in the shape of a 7-petalled flower,
and then compared that sim result with the 7-petalled hypotrochoid
having the same min and max radii; see Figure 4. Conclusion: It's really close, but it's not a hypotrochoid.
It moves too fast at the fast parts and too slow at the slow parts,
and stays a bit too close to the origin during the in-between parts.","['plane-curves', 'physics', 'classical-mechanics', 'ordinary-differential-equations']"
1345584,How to measure the stability of datas,"The background: I have a server handling $n$ kinds of requests, denoted by $k_1, ..., k_n$, at a certain time, many requests has been processed, the average time it takes to process $k_i$ is $t_i$, for $i \neq j$, $\left|t_i - t_j\right|$ can be very large, or very small. The question: Is there a function, $f$, it takes a list of numbers, (in this specific case, the numbers are the time consumed by processing $k_i$), and yields a number, so that, for $j \in \{1,...n\}$, I get n numbers, $f_1, ..., f_n$, which allows me to know, by the magnitude of these numbers, among these $n$ kinds requests, which one is most unstable to process?","['statistics', 'statistical-inference']"
1345585,Complex analysis: Prove a meromorphic function to be rational.,"I come across a problem about complex analysis: Show that a meromorphic function on the complex plane, which achieves any complex number no more than fixed given times, must be rational. The only way I know to prove a meromorphic function being rational is to show the infinity is not an essential singularity of the function (thus it can be controlled by polynomial). Following this, we can use Picard's Great Theorem to solve the problem. I wonder if anyone can help me think of another method. (Without using Picard's Great Theorem.)","['rational-functions', 'complex-analysis']"
1345598,Does meet of two partitions of a set always exist?,"Let $\Omega$ be any set. Let $\mathcal{P_1}$ and $\mathcal{P}_2$ be partitions of $\Omega$. By $P_i(\omega)$ we denote cell of partition $i$ containing $\omega$. Meet of partitions $\mathcal{P}_1$ and $\mathcal{P}_2$ is defined in the following way: $\mathcal{P}_1\wedge \mathcal{P}_2$ is the finest partition of $\Omega$ such that $P_i(\omega)\subset (\mathcal{P}_1\wedge \mathcal{P}_2)(\omega)$ for $i=1,2$ and every $\omega\in\Omega$. By $(\mathcal{P}_1\wedge \mathcal{P}_2)(\omega)$ we denote a cell of $\mathcal{P}_1\wedge \mathcal{P}_2$ containing $\omega$. My question is: Is existence of $\mathcal{P}_1\wedge \mathcal{P}_2$ always guaranteed? Is it unique? ($\Omega$ can be finite, countable or uncountable). In the worst case (when there are no finer partitions), $\{\Omega\}$ is always meet. Is that enough to prove existence? How can i prove uniqness?",['elementary-set-theory']
1345599,Studying this sum: $\sum_{k=0}^{\infty} (k+1)(x)^k$,"$$\sum_{k=0}^{\infty} (k+1)(x)^k\Longrightarrow x=-\frac{9}{10} \Longrightarrow\sum_{k=0}^{\infty} (k+1)\left(-\frac{9}{10}\right)^k=\frac{100}{361}\approx 0.2777$$
$$\sum_{k=0}^{\infty} (k+1)(x)^k\Longrightarrow x=-\frac{99}{100} \Longrightarrow\sum_{k=0}^{\infty} (k+1)\left(-\frac{99}{100}\right)^k=\frac{10000}{39601}\approx 0.2525$$
$$\sum_{k=0}^{\infty} (k+1)(x)^k\Longrightarrow x=-\frac{999}{1000} \Longrightarrow\sum_{k=0}^{\infty} (k+1)\left(-\frac{999}{1000}\right)^k=\frac{1000000}{3996001}\approx 0.25025$$
$$\sum_{k=0}^{\infty} (k+1)(x)^k\Longrightarrow x=-\frac{99999999}{10^{8}} \Longrightarrow\sum_{k=0}^{\infty} (k+1)\left(-\frac{99999999}{10^{8}}\right)^k=\frac{10^{16}}{39999999600000001}\approx 0.2500000025$$ So my conclusion will be that when $x$ get to $-1$ the sum becomes $\frac{1}{4}$ so than this will be equal: $$\lim_{x\rightarrow-1}\left(\sum_{k=0}^{\infty} (k+1)(x)^k\right)=\lim_{x\rightarrow-1}\left(\frac{1}{(1-x)^2}\right)=\frac{1}{\lim_{x\rightarrow-1} (1-x)^2}=\frac{1}{4}$$ and that is than equal to: $$1-2+3-4+5-6+7-8+9-10+...=\frac{1}{4}$$ Can someone tell me if this is right? And if it is not can you proof it?","['summation', 'limits', 'riemann-sum']"
1345603,Peripendicular distance from a line segment,"I have a line given by $Ax + By + C= 0$, and a point $x0,y0$. From that point $x0,y0$ in the direction of the line up to distance $d$, I want to find the perpendicular distance of the points from this line segment. In the figure, below I wish to calculate this only for $x1,y1$ and $x4,y4$. The points $x2,y2$ and $x3,y3$ should be excluded as they lie above the red line depicted.
Please check figure below:","['geometry', 'trigonometry']"
1345613,Definition of resolvent set,"I'm having trouble understanding some subtlety of definition of resolvent set for given bounded operator A everywhere defined on some Hilbert space. Book I use (and many other sources) give the following: 
$\lambda \in \mathbb C $ is in resolvent set if $ R_{ \lambda} = ( \lambda \mathbb I - A ) ^ {-1} $ exists, is bounded and range of $\lambda \mathbb I -A$ is dense.
Now my reasoning begins. This range is also domain of resolvent. Since it is bounded operator on dense domain it can be extended to whole Hilbert space by continuity. $ ( \lambda \mathbb I - A ) R_{\lambda}$ is equal to identity on dense subset so its extension is identity on whole Hilbert space. Similarly $R_{\lambda} (\lambda \mathbb I - A ) $ is identity on whole Hilbert space by definition of resolvent. But this means that $A - \lambda \mathbb I$ is bijection because it has left and right inverse. Therefore its range is actually whole Hilbert space. But if that is the case, why everyone demands it to be merely a dense subset?","['hilbert-spaces', 'spectral-theory', 'operator-theory', 'functional-analysis']"
1345632,finding $a_1$ in an arithmetic progression,Given an arithmetic progression such that: $$a_{n+1}=\frac{9n^2-21n+10}{a_n}$$ How can I find the value of $a_1$? I tried using $a_{n+1}=a_1+nd$ but I think it's a loop.. Thanks.,"['arithmetic-progressions', 'recurrence-relations', 'sequences-and-series', 'algebra-precalculus']"
1345656,The quadratic and cubic versions of a tough intregral,"In this post, Proving that $\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$ , it's proved that $$I_1=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$$
but then some natural questions arise. Might we possibly hope to find nice closed forms for the  following integrals too? $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$
$$I_3=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^3 (t+2)}{t+1} \, dt$$ What tools, strategies would you like to propose? UPDATE: According to David H's comment we have that $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$
$$=\operatorname{Li}_{4}{\left(\frac13\right)}-\frac12\,\operatorname{Li}_{4}{‌​\left(-\frac13\right)}+\frac{13}{12}\ln{(3)}\zeta{(3)}-\frac14\ln^{2}{(3)}\zeta{(‌​2)}+\frac{1}{48}\ln^{4}{(3)}-\frac78\,\zeta{(4)}$$
that is also confirmed numerically by Mathematica.","['calculus', 'real-analysis', 'definite-integrals', 'integration']"
1345661,Eigenvalues of linear operator $F(A) = AB + BA$,"Let $B$ be the $n \times n$ square matrix; $\lambda_1, \lambda_2, \dots, \lambda_n$ are its pairwise distinct eigenvalues. For all $n \times n$ matrix $A$ let me define $F(A) = AB + BA$. We can consider $F$ as a linear operator, because $F(\alpha X + \beta Y) = \alpha F(x) + \beta F(y)$. What eigenvalues does $F$ have? Any help would be appreciated.","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1345694,Algorithm for real matrix given the complex eigenvalues,Given complex eigenvalues (occurring in conjugate pairs) how to get a single instance of a real matrix which has these eigenvalues. I know the matrix is not unique as eigenvectors are not fixed but in my case any real matrix will suffice.,"['eigenvalues-eigenvectors', 'matrix-decomposition', 'matrices']"
1345706,Difference of consecutive pairs of sequence terms tends to $0$,"This seems an elementary problem, but I don't know of any reference to it in the literature. Consider the sequence $(a_n)_{n=1}^\infty$ of real numbers. Suppose $|a_{n+1}-a_n|\rightarrow0~(n\rightarrow\infty)$, and suppose furthermore that $|a_n|\nrightarrow\infty$. May we conclude that $a_n$ converges? The second condition precludes the standard example $a_n=\sum_{j=1}^nj^{-1}$, which obviously tends to $+\infty$.","['sequences-and-series', 'real-analysis']"
1345717,a matrix metric,"Let $U_1,...,U_n$ and $V_1,...,V_n$ be two sets of $n$ unitary matrices of the same size.
We'll denote $E(U_i,V_i)= \max_v \, |(U_i -V_i)v|$ (max over all the quantum states ), $U=\prod_i U_i$ and $V=\prod_i V_i$. I'd like to show that $E(U,V) \leq \sum_i E(U_i,V_i) $. At first I thought proving this by induction on $n$, but then I got stuck even in the simple case of $n=2$. I also tried expanding the expression: $$E(U,V)=\max_v |(\prod U_i - \prod V_i)v | $$ But I got stuck on here too. Maybe there's an easier way I'm missing out? Edit: $U_i, V_i$ are unitary matrices","['quantum-computation', 'linear-algebra', 'matrices']"
1345732,Solve $n(n+1)(n+2)=6m^3$ in positive integers,"How to find all positive integers $m,n$ such that $n(n+1)(n+2)=6m^3$ ? I can see that $m=n=1$ is a solution, but is it the only solution ?","['number-theory', 'diophantine-equations']"
1345734,Can a volume form on a submanifold be extended to a parallel form in a neighbourhood?,"Let $(M^{n+1},g)$ be a Riemannian manifold and let $\Sigma^n \hookrightarrow M$ be a smooth, closed, embedded submanifold. Let $\Omega$ be the volume form of $\Sigma$. It is well-known that a volume form is parallel, i.e. $$
\nabla^{\Sigma}\Omega \equiv 0
$$
on $\Sigma$. Can we always extend $\Omega$ to an $n$-form $\tilde{\Omega}$ defined on a neighborhood $U$ of $\Sigma$ such that
$$
\nabla \tilde{\Omega} \equiv 0
$$
on $U$?","['differential-geometry', 'riemannian-geometry', 'differential-forms']"
1345751,How do I solve this trigonometric equation? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Solve the equation $$\sin\left(\frac{3\pi}{10}-\frac{x}{2}\right)=\frac{1}{2}\sin\left(\frac{\pi}{10}+\frac{3x}{2}\right).$$ I tried  applying some Ratio properties, but they just made the equation nasty. Some hints please. Thanks.",['trigonometry']
1345762,Prove that a second order diff. eq. has only two linearly independent solutions.,"Let $p(t)$ and $q(t)$ be two continuous functions. Prove that the second order linear equation $$y'' + p(t)y' + q(t)y = 0$$ has two, and only two linearly independent solutions. $\textbf{Sketch of Proof:}$ D$^2$y + pDy + qy = 0 (D$^2$ + pD + q)y = 0 Let L = D$^2$ + pD + q L is a linear operator, Ly = 0 Claim: {c$_1$y$_1$ + c$_2$y$_2$} are all the solutions Because L is a linear operator $\rightarrow$ L(c$_1$y$_1$ + c$_2$y$_2$) = L(c$_1$y$_1$) + L(c$_2$y$_2$) = c$_1$L(y$_1$) + c$_2$L(y$_2$) Since c$_1$y$_1$ + c$_2$y$_2$ is a solution and L(c$_1$y$_1$ + c$_2$y$_2$) = 0 $\rightarrow$ c$_1$L(y$_1$) = 0 and c$_2$L(y$_2$) = 0 I know I need to prove that c$_1$ and c$_2$ are 0, but I'm not sure how to prove that this is the only case.",['ordinary-differential-equations']
1345778,Proof composition of analytic functions is analytic,Title says it all I looked for a proof on this site but couldn't find one. Prove if $f$ is analytic on $D$ and $g$ is analytic on $\Omega$ containing the range of $f$ show $g(f(z))$ is analytic. The statement seems obvious but I can't seem to be able to prove it.,['complex-analysis']
1345783,Infinitesimal Generator of Poisson process,I would like to compute the infinitesimal generator of a Poisson process $N$ with intensity $\lambda$. So I can write: $$\mathbb{E}[\ f(N_{t+s})-f(N_s)\ |\ \mathcal{F_t^0} \ ]  = \mathbb{E}[\ f(N_{t+s}-N_s+N_s)-f(N_s)\ |\ \mathcal{F_t^0} \ ]$$ $$= \sum_{k=0}^\infty \frac{e^{-t \lambda}(\lambda t)^k}{k!}\cdot(\ f(N_{t}+k)-f(N_t)\ ) $$ where $\mathcal{F_t^0}$ is the raw filtration generated by $N$ and $f$ is a bounded and measurable function. The result of the series should be: $$\lambda t e^{-\lambda t}[\ f(N_{t}+1)-f(N_t)\ ] +O(t^2)$$ but I do not see how. I tried to manipulate the series as it is done for the computation of the expected value of a Poisson random variable but did not get anywhere.,"['stochastic-calculus', 'probability', 'poisson-distribution', 'stochastic-processes']"
1345784,Hints on solving $y''-\frac{x}{x-1}y'+\frac{1}{x-1}y=0$,$$y''-\frac{x}{x-1}y'+\frac{1}{x-1}y=0$$ Is there any simple method to solve this equation? I need hints please $\color{red}{not}$ a full answer,['ordinary-differential-equations']
1345786,Unusual result to the addition,"Question: Prove that (666... to n digits)^2 + (888... to n digits)=(444... to 2n digits) My way: I just proved the given equation for three values of n and written at the bottom. ""Since the equation satisfies for n=1, 2, and 3, the equation is true and hence proved."" Also I am seeing a regular pattern in (6666...to n digits)^2 $666^2=443556,6666^2=44435556,66666^2=4444355556$ So the pattern is first there are (n-1) $4's$ the one $3$ then (n-1) 5's the one 6. Is there a general way to solve this question?","['arithmetic', 'linear-algebra']"
1345838,Likelihood of large difference in averages,"Note that I updated question because initial example grades did not satisfy the overall average that followed from the numbers given in the article. A newspaper artticle in Norway made me curious about this one. https://translate.google.com/translate?sl=no&tl=en&js=y&prev=_t&hl=en&ie=UTF-8&u=http%3A%2F%2Fwww.dn.no%2Ftalent%2F2015%2F06%2F30%2F1025%2FEksamen%2Fda-elevene-sorterte-eksamenskarakterene-alfabetisk-etter-navn-gikk-det-opp-for-dem&edit-text= In a school class there were 25 students who had the same exam, graded from 1 to 6. After the the exams had been graded a lot of the students felt something was wrong. So one them collected the names and grades of all the students and put them into an excel sheet. When he sorted the names alphabetically he noticed that the first 13 had on average better grades than the last 12. In fact, the average of the first 12 was 3.4 and the average of the last 13 was 1.8. When confronting the school it was confirmed that the class had been split in half, by name, and one pair of evaluators graded the first 13 students, and a different pair of evaluators graded the last 12 students. (there are always 2 persons evaluating each exam submitted) Now we are debating the probablity that this difference in averages occured randomly because stuff just happens, and the probablity that the different evaluators was the cause of this. Unfortunately I don't know the grades. But suppose the grades i the entire class were: 1,1,1,1,1,1,1
2,2,2,2,2,2
3,3,3,3,3,3
4,4,4
5,5
6

Group A:
1
2,2
3,3,3,3
4,4,4
5,5
6

Group B:
1,1,1,1,1,1
2,2,2
3,3
4 These numbers gives close to the averages mentioned. (but the distribution feels kinds of wrong, but overall average was very low...) I'm not sure of the math here, so I ran a simulation and found that there is about 16% chance that the difference in averages between 2 randomly created groups of sizes 12 and 13 is 3.4-1.8=1.6 or larger. 16% is not that small a number. This will happen many times each year in classes at many schools. But in this case that we also know that there were different pairs of evaluators in the 2 groups, how should I interpret this? Is it correct to say it is 84% chance that the large difference was beacuse of different evaluators? Are there good reasons for the group with the worst grades to demand new evaluations? I think the 16% number is correct, but doing the math would be better than  running a sim. So some pointers on how to do that would also be appreciated.",['statistics']
1345863,Probability theory combinatoric problem,"A total of $n$ bar magnets are placed end to end in a line with random
  independent orientations. Adjacent ends with equal polarities repel
  each other, and adjacent ends with opposite polarities join to form
  blocks. Let $X$ be the number of blocks of joined magnets. Find $\text{E}(X)$
  and $\text{Var}(X)$. The obvious method to solve this problem is to find the pmf of $X$ and then simply use the definitions of the expectancy and variance. I'm however a bit stuck at the first line of this reasoning, i.e. how to find the pmf for $X$. It feels like somehow a binomial distribution should be involved but the problem is is that there are many ways of creating — for example — two or five blocks out of $n$ magnets, each of which models a binomial distribution. How do I derive a reasonably ""clean"" pmf for $X$?","['probability-theory', 'combinatorics']"
1345880,A tough integral:$\int_0^{+\infty}\left( \frac1{\log(x+1)-\log x}-x-\frac12\right)^2 dx$,"I would like to prove the convergence of $$I=\int_0^{+\infty}\left( \frac1{\log(x+1)-\log x}-x-\frac12\right)^2 dx$$ then obtain a closed form of $I$. Convergence is ensured by the fact that $x \mapsto  f(x)=\left( \frac1{\log(x+1)-\log x}-x-\frac12\right)^2$ is continuous on $(0,+\infty)$ with $f(x) \sim \dfrac14$ as $x \to 0^+$ and $f(x) \sim \dfrac1{144 x^2}$ as $x \to +\infty.$ I did not succeed in finding a closed form of $I$. My attempt was to consider a certain parameter integral then make some differentiation to get rid of the $\log$ terms...","['calculus', 'integration']"
1345920,Permutation of cosets,"Let $G$ be a finite group and $\gamma \in \text{Sym}(G)$ such that $\gamma (1) = 1$ and $\gamma (gH) = \gamma (g)H$ for all $g\in G$ and $H\leq G$ . This means $\gamma$ induces a permutation of the left cosets of any subgroup of $G$ . I need to show that $\gamma$ fixes any left coset of the Center $C(G)$ of $G$ , that is $$\gamma(g)C(G) = gC(G)\,.$$ I tried some simple calculations which did not work. Now I think some more structural arguments are necessary. Any ideas?","['abstract-algebra', 'group-theory', 'finite-groups', 'group-actions']"
1345923,Showing that certain points lie on an ellipse,"I have the equation
$$r(\phi) = \frac{es}{1-e \cos{\phi}}$$
with $e,s>0$, $e<1$ and want to show that the points
$$ \begin{pmatrix}x(\phi)\\y(\phi)\end{pmatrix} = \begin{pmatrix}r(\phi)\cos{\phi}\\r(\phi)\sin{\phi}\end{pmatrix}$$
lie on an ellipse. I have a basic and a specific problem which may be related. The basic problem is that I don't see how to get to the goal. I want to arrive at an ellipse equation $(x/a)^2 + (y/b)^2 = 1$ and am confused that we already have $$\left(\frac{x(\phi)}{r(\phi))}\right)^2 + \left(\frac{y(\phi)}{r(\phi))}\right)^2 = 1$$ This can't be the right ellipse equation because it has the form of a circle equation and obviously, we don't have a circle here. But how should the equation I aim for look like then? I started calculating anyway and arrived at a specific problem. Substituting $x(\phi)$ into $r(\phi)$, I got $r(\phi)=e(s+x(\phi))$ and using $r(\phi)^2 = x(\phi)^2 + y(\phi)^2$ I got an equation which doesn't involve $r(\phi)$ anymore but has $x(\phi)$ in addition to $x(\phi)^2, y(\phi)^2$. If I could eliminate the linear term somehow, the resulting equation is an ellipse equation. But I don't see a way how to accomplish this.","['euclidean-geometry', 'geometry', 'conic-sections']"
1345931,Doob decomposition of $|S_n|$ where $S_n$ is simple random walk.,"Let $X_n$, $n\geqslant 1$ be iid Rademacher random variables, i.e. $X_1$ takes values $\pm 1$ each with probability $\frac12$. Define $S_0=0$ and $S_n=\sum_{i=0}^n X_i$, and $\mathcal F_n = \sigma(S_0, \ldots, S_n)$. Then it is straightforward to show that $|S_n|$ is a submartingale with respect to $\{F_n\}$, so it has a (unique) Doob decomposition
$$|S_n| = M_n + A_n, $$
where $M_n$ is a martingale and $A_n$ is a nondecreasing predictable process. Explicitly,
\begin{align}
A_n &= \sum_{k=1}^n \mathbb E\left[|S_k|-|S_{k-1}| \mid \mathcal F_{k-1}\right]\\
M_n &= |S_n| - A_n.
\end{align}
I'm trying to find a simpler expression for $M_n$, of the form $M_n = (H\cdot S)_n$ where $H$ is a predictable process and $\cdot$ denotes martingale transform, but I got bogged down in the algebra. Any hints, either as to how to approach this or what $H$ might look like?","['probability-theory', 'martingales', 'stochastic-processes']"
1345933,Definite integral with logarithm and arctangent inside of arctangent,"How to prove $$\int_0^1 \left[ \frac{2}{\pi }\arctan \left(\frac 2 \pi \arctan \frac{1}{x} + \frac{1}{\pi }\ln \frac{1 + x}{1 - x}\right) - \frac{1}{2} \right]\frac{\mathrm{d}x} x = \frac{1}{2} \ln \left( \frac \pi {2\sqrt 2 } \right).$$ I have tried let $t=\frac1x$, but it seems no use! Could you help me to solve it?","['calculus', 'real-analysis', 'definite-integrals', 'integration', 'complex-analysis']"
1345994,Question regarding proof that $V = \{ f : \Bbb N \to \Bbb N \mid f(n)\text{ is a prime for all }n \in N\}$ is uncountable,"I'm studying for an exam for tomorrow and one of the old exams has this problem: Given the set $V  = \{  f : \Bbb N \to \Bbb N \mid f(n)\text{ is a prime for all }n \in N \}$ Prove that this set is uncountable. The given proof is as follows: Assume $V$ is countable. Then $V$ consists of elements $f_0, f_1, \ldots, f_n, \ldots$ Define $g: \Bbb N \to \Bbb N$ as follows: $  g(n)=\begin{cases}
    3, & \text{if } f_n(n) \neq 3.\\
    5, & \text{else}.
  \end{cases}
$ Now applies that: $g(n) \in \{3,5\}$ for all $n \in \Bbb N$, such that $g \in V$ (because both 3 and 5 are primes) Due to its construction $g$ can not be in the list of elements $f_0, f_1, \ldots, f_n, \ldots$ of $V$. Due to this contradiction we must assume that $V$ is uncountable. My problem is that I do not understand why $g$ can not appear in the list of elements.",['elementary-set-theory']
1345995,"How do i find formula for the recurrence relation :$x_{n+1}= x^2_{n}-x^2_{n-1}$ with :$x_{-1}=0,x_{0}=\frac{3}{4}$? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question How do i find formula for the recurrence relation  according to the below initial conditions: $x_{n+1}= x^2_{n}-x^2_{n-1}$ with :=$x_{-1}=0,x_{0}=\frac{3}{4}$ ? Note:$n=0,1,2\cdots$ Thank you for any help","['recurrence-relations', 'roots', 'dynamical-systems', 'discrete-mathematics']"
1345999,Help with the contour for this integral using residues,"$$ PV \int_0^\infty \frac{dx}{\sqrt{x}(x^2-1)} $$ A keyhole contour can't be used because we have a pole in the real positive axis, isn't it?","['complex-analysis', 'residue-calculus', 'integration']"
1346024,solve $|x-6|>|x^2-5x+9|$,"solve $|x-6|>|x^2-5x+9|,\ \ x\in \mathbb{R}$ I have done $4$ cases. $1.)\ x-6>x^2-5x+9\ \ ,\implies  x\in \emptyset \\
2.)\ x-6<x^2-5x+9\ \ ,\implies x\in \mathbb{R} \\
3.)\  -(x-6)>x^2-5x+9\ ,\implies 1<x<3\\
4.)\  (x-6)>-(x^2-5x+9),\ \implies x>3\cup x<1 $ I am confused on how I proceed. Or if their is any other short way than making $4$ cases than I would like to know. I have studied maths up to $12$th grade. Thanks.","['quadratics', 'absolute-value', 'algebra-precalculus', 'inequality']"
1346060,converging power series over $p$-adic integers is a UFD,"Denote by $|\cdot |$ the $p$-adic norm, and let $$\mathbb Z_p \{z\}=\left\{\sum_{n=0}^\infty a_nz^n;\  a_n\in \mathbb Z_p ,\ |a_n|\underset {n\rightarrow \infty} {\longrightarrow 0} \right\}$$ the ring of converging power series over the p-adic integers with indeterminate $z$. I need to show that this ring is a unique factorization domain. What I know is that $\ \mathbb Q_p\{z\}$ is a principal ideal domain, and in fact every element $\ f\in\mathbb Q_p\{z\}$ has a unique representation $f=qu$ where $\ q\in\mathbb Q_p[z],\ u\in \mathbb Q_p\{z\}^\times$. Another thing is that for archimedean valuations in general this isn't true. For example, the ring of converging power series over $\ \mathbb C$ with the usual absolute value is not a UFD, as $\sin z$ which is holomorphic in the unit disc don't have a unique representation as finite product of irreducibles (the Laurent series of $\sin z$ around the origin is an infinite product of irreducibles). I think this is supposed to be true for $\mathbb Z_p\{z\}$, and the fact that $\mathbb Q_p\{z\}$ is a PID is crucial for the proof, but I haven't been able to see how.","['power-series', 'ring-theory', 'p-adic-number-theory', 'abstract-algebra', 'unique-factorization-domains']"
1346065,Does the lowest diagonal element of a real symmetric matrix form an upper bound to the lowest eigenvalue?,"If I have a real symmetric matrix, is it possible to look at the lowest diagonal element and then claim that the lowest eigenvalue of the matrix must be less than or equal to that diagonal element? I think this may be the case, so if it is, is this a well known theorem? Gerschgorin's disk theorem doesn't appear useful to me, as the disk radius extends both ways in both the positive and negative direction from the diagonal. FWIW, in quantum chemistry we have the Hyleraas-Undheim-MacDonald theorem, which bounds eigenvalues this way, but I'm not sure if this applies to any real symmetric matrix.","['eigenvalues-eigenvectors', 'matrices']"
1346071,A Combinatorial Sum!,"Is there a closed form formula for the following sum \begin{equation}
F(x;n,m)=\sum_{k=0}^{\min\{n,m\}} {n \choose k}{m \choose k}k!\ x^{k}=n! \, m!\sum_{k=0}^{\min\{n,m\}}\frac{1}{k!(n-k)!(m-k)!} x^{k}
\end{equation} where $k$ runs from zero to the minimum of $n$ and $m$. Thanks","['summation', 'combinatorics']"
1346165,Find the Laurent series of $f(z)=\frac{1}{z(1-z)}$,"I am having difficulties finding Laurent series of the above function, around these two domains:
$$0<|z-1|<1$$
and 
$$|z-1|>1$$ The function $f(z)$ takes the form $\dfrac{1}{z}-\dfrac{1}{z-1}$. And I know that I can find the Laurent series of the following general function around $z_0=0$ when my domain includes $z_0=0$: $$\dfrac{1}{z-a}=\dfrac{1}{z}\cdot \dfrac{1}{1-\frac{a}{z}}=\sum_{n=1}^\infty\frac{a^{n-1}}{z^n}$$ But now I should evaluate the expansion around $z_0=1$. How can this be done? Should I expect a Laurent series for the left partial fraction term? An explanation about this would be very respected. It just that I didn't find a similar question when searching for it.","['laurent-series', 'complex-analysis']"
1346183,Fourier-Mukai kernels of mutations?,"if I have an exceptional object E (on say the derived category of a smooth and projective variety) then I can define the left and right mutation functors. These are typically defined in terms of cones, but the ""correct"" way to define them is via Fourier-Mukai kernels. What is the expression for these kernels? I couldn't find it in the literature.","['homological-algebra', 'algebraic-geometry', 'derived-functors', 'category-theory']"
1346191,"Orthonormal basis for $\mathcal{L}^2([0,1])$","$\textbf{Theorem:}$ The orthonormal family $\{e_n(x):\, n\in\mathbb{N}\}$ , where $e_n(x)=e^{2\pi inx}$ , is a basis for $\mathcal{L}^2([0,1])$ . In this case, $\{e_n(x):\, n\in\mathbb{N}\}$ being a basis would mean that any $f\in\mathcal{L}^2([0,1])$ can be written in the form $$f=\sum^\infty_{k=0} \hat{f}(k)e_k(x)$$ where $$\hat{f}(k)=\langle f,e_k\rangle =\int_{[0,1]} f(x)\overline{e_k(x)} \ \text{d}x$$ I am attempting to get a solution in which we can say $$\left\vert\left\vert f-\sum^k_{k=0}\hat{f}(k)e_k(x)\right\vert\right\vert\rightarrow 0 \ \ \text{as} \ \ n\rightarrow \infty$$ via Parsevals and Plancheral Identities, but I have been unable to do so. Any hints please?","['fourier-analysis', 'real-analysis', 'functional-analysis', 'fourier-series', 'hilbert-spaces']"
1346209,Derivative of a group action,"Let $\phi : G \times M \rightarrow M$ be a group action on a smooth manifold $M$ and Lie group $G$. Then we define $$f(t):=\phi(g(t),d(t)).$$ where $g: I \rightarrow G$ and $d: I \rightarrow M$ are smooth maps. Now I'd say: $$f'(t) = D\phi(g(t),d(t))(g'(t),d'(t)).$$ In the lecture we wrote: $$f'(t) = D \phi_{g(t)} d(t) d'(t) + D \phi_{g(t)} (d(t))  X_{\zeta(t)} (d(t))$$ where $\zeta(t) = dL_{g(t)^{-1}}g'(t)$ is an element of the Lie Algebra, $dL$ is the derivative of the left-translation and $X_{\zeta}(p):=\frac{d}{dt}|_{t=0} \phi_{e^{t \zeta}}(p)$ for any $\zeta$ in the Lie Algebra.
The thing is that I don't have the slightest clue, how one could come up with this second expression. Could anybody explain to me whether I am wrong or give me an idea why both expressions might be equivalent?","['lie-groups', 'differential-geometry', 'smooth-manifolds']"
1346288,Surjectivity of $\mathcal{id}_{\mathbb{R}^n}+g$ when $g$ is a contraction?,Assume $g:\mathbb{R}^n\longrightarrow \mathbb{R}^n$ is a contraction and consider $h=\mathcal{id}_{\mathbb{R}^n}+g$. The map $h$ is injective. Is it always surjective? My question has the following one for origin.,"['real-analysis', 'general-topology']"
1346292,Show the following are not connected in $\mathbb{R}^n$,"Is the interior, boundary and closure of a connected set in $\mathbb{R}^n$ connected? I know the interior is not connected we can show it by a counterexample but I am not quite sure for the closure and boundary",['analysis']
1346319,Completion of the real numbers,"On the real line $\mathbb{R}$ endowed with the Euclidean topology, I may put different metrics, inducing the same topology, but inducing different completions. For example if one considers the standard Euclidean distance you get $\mathbb{R}$ itself, if I see $\mathbb{R}$ as $(0,1)$ and I consider the induced metric, then I get $[0,1]$ , similarly I can get $S^1$ , and similarly I can get $\mathbb{R}_{\geq 0}$ . What are all the possible completions of $\mathbb{R}$ respect to a metric compatible with the Euclidean topology?","['complete-spaces', 'metric-spaces', 'examples-counterexamples', 'general-topology']"
1346321,solve $\sqrt{x+7}<x$ for $x\in \mathbb{R}$,"solve $\sqrt{x+7}<x$ I tried $\sqrt{x+7}<x\\
x+7<x^2\\
x^2-x-7>0\\
x\in \left(-\infty, \dfrac{1-\sqrt{29}}{2}\right) \cup \left( \dfrac{1+\sqrt{29}}{2},+\infty\right) $ I m not sure, if this is correct method and if the solution is correct . I look for a simple and short way. I have studied maths up to $12$th grade.Thanks.","['quadratics', 'algebra-precalculus', 'inequality']"
1346327,Trigonometry equation. Not sure about solution.,"The equation goes as follows: 
$$\sin x +\cos x = 1 + \sin x \cos x$$ and here is how I solved it: $$(\sin x+\cos x)^2=(1+\sin x\cos x)^2$$
$$\sin^2x+2\sin x\cos x+\cos^2x=1+2\sin x\cos x+2\sin^2x\cos^2x$$
$$2\sin^2x\cos^2x=0$$ $$ \cos x=0; \sin x=0$$
$$x_1=\pi/2 +k\pi$$
$$x_2=k\pi$$ Where did it go wrong?
Thanks in advance!","['algebra-precalculus', 'trigonometry']"
1346338,"Determine the minimum and maximum angles, to the nearest tenth of a degree, that a pipe can make with the horizontal.","For residential drains, a horizontal pipe needs to have a minimum slope of $1/4$ inch per foot and a maximum slope of $1/2$ inch per foot for waste to drain properly. This means that for every horizontal foot the pipe travels, it should drop between $1/4$ and $1/2$ inch. This is the way I'm currently trying to solve this but I feel like I'm doing it completely wrong. $$\tan x = opp / adj$$ $1/4$ inch by $12$ We have $\tan x= 0.25 / 12 = 0.020$, so $x = \tan^{-1} (0.020)= 1.1457^\circ$. And if $\tan y = 0.50 / 12 = 0.041$, then $\tan^{-1} (0.04)= 2.3478^\circ$. Is this correct?","['geometry', 'algebra-precalculus', 'trigonometry']"
1346351,Show Lie bracket left invariant,"I want to prove from the definition that the Lie bracket $[X,Y]$ of two left-invariant vector fields $X,Y: G \rightarrow TG$ where $G$ is a Lie group is again left-invariant. Left-invariance basically means that for all $g \in G$ and $h \in G$ we have $$DL_{g}(h)(X(h)) = X(gh).$$ But somehow I am stuck when I try to apply $$DL_{g}(h)([X,Y](g)).$$ I just don't see how I can end up with $$[X,Y](hg).$$ There must be a trick to end up with this result. Does anybody have an idea?","['lie-groups', 'real-analysis', 'differential-topology', 'differential-geometry', 'lie-algebras']"
1346370,"Explain why two right triangles, each with an acute angle of 17 degrees, must be similar.",Two right angles with an acute angle of 17 degrees must be similar because triangles that are similar share the same angles.Is this proper?,"['triangles', 'trigonometry']"
1346376,How can one solve $1^x=2$?,"Sure, common sense says there's no solution. But, I feel, there should be one! (If there isn't, can't we construct one?)","['exponentiation', 'complex-numbers', 'algebra-precalculus', 'functions', 'linear-algebra']"
1346403,Intersection of two moving objects,"There are two objects. The first object moves with speed $U_1$ from known point $A$ to known point $B$. The second object has speed $U_2$ and starts from known point $C$. What is the direction the second object must have in order to ""collide"" with the first object. The direction must be defined by the collision point. https://i.sstatic.net/DpZey.png",['geometry']
1346438,Is this operation meaningful or it is a mistake in the book?,"I've been reading Nakahara's ""Geometry, Topology and Physics"" and found something quite strange on the section 10.3.3 which discusses the geometrical meaning of the curvature of a connection. It is possible to find there the following text: We first show that $\Omega(X,Y)$ yields the vertical component of the Lie bracket $[X,Y]$ of horizontal vectors $X,Y\in H_u P$ . It follows from $\omega(X)=\omega(Y)=0$ that $$d_P\omega(X,Y)=X\omega(Y)-Y\omega(X)-\omega([X,Y])=-\omega([X,Y]).$$ My problem here is the following. As I've previously studied in other books, the Lie bracket can only be computed for vector fields. That is, given a smooth manifold $M$ , a point $a\in M$ and two vectors $v,w\in T_a M$ it is totally meaningless to talk about $[v,w]$ . In that case, $[\cdot,\cdot]$ is only meaningful for vector fields . On the text, Nakahara is talking about picking two vectors $X,Y\in H_u P$ the horizontal subspace of the tangent space $T_u P$ at $u\in P$ and then he is talking about the Lie bracket $[X,Y]$ . And this is not the only place where he does that. Some paragraphs later we can see the same thing being done again, so it is certainly not a typo. Is that really a mistake in the book? Or is there something I'm missing? Perhaps there is some natural extension of the vectors to vector fields when dealing with connections on principal bundles and I'm not aware of that. Am I missing something or the book has a mistake in it?","['principal-bundles', 'differential-geometry', 'connections', 'lie-derivative']"
1346448,Variation processes and strong solutions of stochastic differential equations,"Let $(\Omega,\mathcal{A},\operatorname{P})$ be a probability space $\mathbb{F}$ be a filtration on $(\Omega,\mathcal{A})$ $\tau$ be a $\mathbb{F}$-stopping time An $\mathbb{F}$-adapted, real-valued stochastic process $M=(M_t)_{t\ge 0}$ on $(\Omega,\mathcal{A},\operatorname{P})$ is called local $\mathbb{F}$-martingale until $\tau$ $:\Leftrightarrow$ There exists a sequence $(\tau_n)_{n\in\mathbb{N}}$ of $\mathbb{F}$-stopping times such that $$\tau_n\uparrow\tau\;\;\;\operatorname{P}\text{-almost surely}$$ and the stopped process $M^{\tau_n}:=(M_{\tau_n\wedge t})_{t\ge 0}$ is an uniformly integrable $\mathbb{F}$-martingale. Now, let $t\mapsto H(t)=(H_{ij}(t))_{\stackrel{i=1,\ldots,n}{j=1,\ldots,m}}$ be $\mathbb{F}$-progressively measurable, $\left\|\;\cdot\;\right\|$ be the Frobenius norm , $B=(B^1,\ldots,B^m)$ be a $m$-dimensional Brownian motion and $$\operatorname{E}\left[\int_0^TH_{ij}^2\;dt\right]<\infty\;.$$ I want to show, that $$\operatorname{E}\left[\left\|\int_0^TH(t)\;dB_t\right\|^2\right]=\operatorname{E}\left[\int_0^T\left\|H(t)\right\|^2\;dt\right]\;.$$ Therefore, let $$I_i(t):=\sum_{j=1}^m\int_0^tH_{ij}(s)\;dW_s^j\;,$$ for $i\in\left\{1,\ldots,n\right\}$. By fundamental facts about the Itô integral , one knows, that each $I_i$ is a continuous martingale. However, I absolutely don't get, why the variation process of $I_i$ is given by $$\langle I_i\rangle_t:=\int_0^t\sum_{j=1}^mH_{ij}^2(s)\;ds$$ and why that implies $$\operatorname{E}\left[\left(I_i(T)\right)^2\right]=\operatorname{E}\left[\int_0^T\sum_{j=1}^mH_{ij}^2(s)\;ds\right]$$","['probability-theory', 'measure-theory', 'stochastic-calculus', 'real-analysis', 'stochastic-processes']"
1346512,Substitutions that transform Fermat Equations to Elliptic Curves,"I was reading Chapter 1 of Elliptic Curves - Number Theory and Cryptography by Lawrence C Washington. He was considering Fermat equations $$a^4+b^4=c^4\text{ and }a^3+b^3=c^3.$$ For the 1st equation, let $$x = 2\frac{b^2+c^2}{a^2},\quad y=\frac{4b(b^2+c^2)}{a^3};$$ and for the 2nd equation, let $$x=\frac{12c}{a+b},\quad y=\frac{36(a-b)}{a+b}.$$ Straightforward calculation shows that under these substitutions we transform Fermat equations to elliptic curves: $$y^2=x^3-4x\text{ and }y^2=x^3-432.$$ Before I looked at the substitutions, I tried and failed to find the solutions myself. I am wondering if there is a natural way to deduce those substitutions and the motivation behind.","['elliptic-curves', 'number-theory', 'diophantine-equations']"
1346528,"If I flip a coin 1000 times in a row and it lands on heads all 1000 times, what is the probability that it's an unfair coin?","Consider a two-sided coin. If I flip it $1000$ times and it lands heads up for each flip, what is the probability that the coin is unfair, and how do we quantify that if it is unfair? Furthermore, would it still be considered unfair for $50$ straight heads? $20$? $7$?","['probability', 'statistics', 'experimental-mathematics']"
1346534,Explicitly decompose $\mathbb{C}^3$ into irreducible representations of $S_3$.,"Consider the permutation representation of $S_3$ acting by permuting the elements of a basis of $\mathbb{C}^3$. Explicitly decompose $\mathbb{C}^3$ into irreducible representations. Can someone please verify my answer? Note: This is NOT homework! Consider the subspace $W = \operatorname{span} \left\lbrace \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \right\rbrace$ of $\mathbb{C}^3$. Then, $W$ is a one dimensional representation of $S_3$. A proof is as follows: Let $\sigma \in S_3$. Then, $\sigma$ permutes the entries of any vector of the form $\begin{bmatrix} \lambda \\ \lambda \\ \lambda \end{bmatrix}$ to give the same vector. So, $\sigma \cdot w = w$ for any $w \in W$ and $\sigma \in S_3$. Let $V$ be the two dimensional subspace of $\mathbb{C}^3$ given by $$V = \left\lbrace \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} : x_1+x_2+x_3 = 0 \right\rbrace$$
Then, $V$ is a representation of $S_3$. Note that if the entries of any vector of the above form are permuted, then their sum remains the same. Then, clearly, $V \cap W = \varnothing$, and $V + W = \mathbb{C}^3$. So, we have $V \oplus W \cong \mathbb{C}^3$ is a decomposition of $S_3$ into irreducible representations.","['group-theory', 'proof-verification', 'representation-theory']"
1346547,First Eigenfunction of Simple Equation,"Consider the interval $[-a,a]$ and the following problem: $$\phi'' + \lambda\phi=0$$
$$ \phi(\pm a) = 0. $$ The obvious sequence of orthogonal eigenfunctions seems to be $\sin(\frac{\pi n}{a}x)$ with eigenvalues $(\frac{\pi n}{a})^2$. But we also know for symmetric operators such as the Laplacian, the first eigenfunction is always positive everywhere in the interior of the domain. What's happening?","['laplacian', 'eigenfunctions', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1346554,All right angles are equal to each other,Why is it that All right angles are equal to each other -a postulate in Euclid's Elements ( Wikipedia ). Shouldn't it be a congruence rather than an equivalence? Isn't this just a special case of the definition of congruent angles?,"['euclidean-geometry', 'geometry']"
1346560,"If $\langle f'(x) \cdot v , v \rangle > 0$ then $f$ is injective","Question: Let $f: U \to \mathbb R^m$ differentiable at the convex set $U \subseteq \mathbb R^m$. If $$\langle f'(x) \cdot v , v \rangle > 0 , \,\,\, \forall\,\, x \in U, v \neq 0 \in \mathbb R^m $$
  then $f$ is injective. If $f \in C^1$ then $f$ is a diffeomorphism of $U$ over a subset of $\mathbb R^m$. Give an example such that $U = \mathbb R^m$, but $f$ is not surjective. Attempt: The idea is to show $$|f(x+v) - f(x)| > 0$$ As $U$ is convex and $f$ id differentiable in $U$ then $[x,x+v] \subseteq U$ for any $x, x + v \in U$, by the Mean Value Theorem there exists $\theta \in (0,1)$ such that $$f(x + v) - f(x) = \frac{\partial f}{\partial v}(x + \theta v) = f'(x + \theta v) \cdot v$$ Then $$\langle f(x + v) - f(x) , v\rangle  = \langle f'(x + \theta v) \cdot v , v\rangle > 0 $$ and I couldn't conclude anything. The second part is o.k. Any thoughts? Edit: I  can't use the Mean Value Theorem here.","['analysis', 'multivariable-calculus']"
1346585,proof by contradiction puzzle,"Consider the following game between two players: • There is an initially rectangular grid of cookies. • The cookie in the upper left corner is poisoned. • The players take turns. On a player’s turn, he or she must eat some cookie,
along with every cookie to the right and/or below it. (See the diagram for a
sample legal move.) • The losing player is the one who is forced to eat the poisoned cookie.
2 Prove that the player who goes first can always win. Here is my proof By the way of contradiction assume the player who goes first does not always win.  Lets say the first player eats the lower right cookie in the first try(i.e 4,5).  Then in the next turn second player takes turn and ets the cookie at place (3,4). In third round, first player eats the cookie at place (2,3).  Then second player eats cookie at place (1,2).  Then the first player takes turns and lands on (1,1).  SO he loses.  So this is contradiction because the second player can force a win.  Thus statement is always true. Can someone please help me on this.  Thanks in advance",['discrete-mathematics']
1346595,"If a matrix has positive, real eigenvalues, is it always symmetric?","We know that symmetric matrices are orthogonally diagonalizable and have real eigenvalues.  Is the converse true?  Does a matrix with real eigenvalues have to be symmetric? A class of symmetric matrices, the positive definite matrices, have positive real eigenvalues.  Is the converse true?  Does a matrix with positive real eigenvalues have to be symmetric, positive-definite? I think the answer to all this is ""no"", but I just wanted to confirm. Thanks,","['eigenvalues-eigenvectors', 'linear-algebra', 'diagonalization']"
1346600,Example of subgroup of $\mathbb Q$ which is not finitely generated,I was looking for a proper subgroup of $\mathbb Q$ which is not finitely generated under the addition operation. We know every finitely generated subgroup of $\mathbb Q$ is cyclic. For a proper subgroup I am just thinking about the subgroup $H$ generated by $\{\frac{1}{p} : p \text{ prime }\}$ may work. It seems $1/4$ is not in $H.$ Is this a  correct example? Thanks,"['abstract-algebra', 'group-theory', 'finitely-generated']"
1346627,Is it possible to approximate $\cos(x)$ with a linear combination of Gaussians $e^{-x^2}$?,"I am interested in approximating $\cos x$ with a linear combination of $e^{-x^2}$. I am not an expert in approximation theory but there are a couple things that give me a bit of hope that it might be possible (and some other things that worry me it might not be possible or that linear combinations might not be enough). First thing I did that gave me some hope was write the taylor expansions of both of them and comparing them: $$ \cos x = \sum^{\infty}_{n=0} \frac{(-1)^n x^{2n}}{ (2n)!}$$ $$ e^{-x^2} = \sum^{\infty}_{n=0} \frac{(-x^2)^n}{n!} = \sum^{\infty}_{n=0} \frac{ (-1)^n x^{2n} }{n!}$$ apart from the denominator, the two power series are nearly identical! However, what worries me is that the factorial function is not easy (for me) to manipulate. Another great thing is that both functions are even, which is definitively great news! Something else that worries me (about this approximation) is that they behave very differently in the tail ends of each other. For example, the Gaussian function dies off to towards the of the function while the cosine does not and instead oscillates (which is kind of a surprising difference if you only consider their power series, I wouldn't have guessed that difference, which is a very bid difference). Something else that again gives me some hope about this approximation is the if you plot both $e^{-x^2}$ and $\cos x$, you get: which even though they are not identical, at least for a bounded interval/domain, look extremely similar! Maybe one could tweak the parameters of one or the other and hopefully get something that might be considered ""close"" to each other. So I I was thinking two direction that might be interesting to explore: An approximation on a bounded interval An approximation using an infinite series of $e^{-x^2}$ (that might be necessary to reflect on the x-axis to mirror the trough of the wave). In fact, it might be possible to solve this issue by solving first an approximation of the crest of the cosine using  tweeked version of $e^{-x^2}$ and then, using that solution, do an infinite summation of that is reflected on the x-axis. T0 make things more clear, what I kind of had in mind with linear combinations was the following formula: $$\sum_{k} c_k e^{- \| x - w_k\|^2}$$ with $w_k$ as the movable centers because we might need to center them at the troughs and crests of the cosines. I am open to different tweet suggestions of this (for example, an improvement could be to including a precision $\beta_k$ or a standard deviation $\sigma_k$ on the exponent to adjust the widths to match the cosine better). I wasn't exactly sure if these were good ideas or if there were maybe better ways to approximate a cosine with $e^{-x^2}$, but I'd love to hear any ideas if people have better suggestions or know how to proceed (rigorously) with the ones I suggested.","['approximation', 'approximation-theory', 'functional-analysis']"
1346637,Prime Zeta Function proof help: Why are these expressions not equal?,"I was trying to create a formula for the Prime Zeta function and I partially succeeded except for one frustrating error. I was only able to formulate an approximation. Consider the following sum: $$f_k(s)=\sum_{q\space\nmid\space{p_{n\le k}}}^\infty \frac 1{q^s}$$ This is the infinite sum of the reciprocals of all the numbers, raised to the power s, that are not divisible by any primes less than or equal to the k th prime. For example, $f_1(s)=1+\frac 1{3^s}+\frac 1{5^s}+\frac 1{7^s}+\frac 1{9^s}+\frac 1{11^s}+\frac 1{13^s}+\frac 1{15^s}+\cdots$ $\qquad\qquad\quad$ $f_2(s)=1+\frac 1{5^s}+\frac 1{7^s}+\frac 1{11^s}+\frac 1{13^s}+\frac 1{17^s}+\frac 1{19^s}+\frac 1{23^s}+\frac 1{25^s}+\cdots$ By taking $f_k(s)-({p_{k+1}}^{-s})f_k(s)$ we end up with $f_{k+1}(s)$. In other words $$f_k(s)\left(\frac {{p_{k+1}}^s-1}{{p_{k+1}}^s}\right)=f_{k+1}(s)$$ Therefore, since we define $f_0(s)=\zeta(s)$, it's clear that $$f_k(s)=\zeta(s)\left(\prod_{n=1}^k\frac {{p_n}^s-1}{{p_n}^s}\right)$$ It can also be shown that $$f_k(s)=\zeta(s)\left(\frac {P(k,s)^2-P(k,2s)}2-P(k,s)+1\right)$$ where $P(k,s)$ is the partial Prime Zeta Function: $$P(k,s)=\sum_{p\in primes}^k \frac 1{p^s}$$ This is where I hit a snag.  I want to equate the above two equations for $f_k(s)$ like this: $$f_k(s)=\zeta(s)\left(\prod_{n=1}^k\frac {{p_n}^s-1}{{p_n}^s}\right)=\zeta(s)\left(\frac {P(k,s)^2-P(k,2s)}2-P(k,s)+1\right)$$ In theory they should be equal, but when I test them, they show to be slightly different, especially for larger k . Why is this? I can't figure out a single reason why they should be unequal. If they were perfectly equal, then $$P(s)=\lim\limits_{x \to \infty} \left(1-\sqrt{\frac 2{\zeta(2^0s)}-\sqrt{\frac 2{\zeta(2^1s)}-\sqrt{\frac 2{\zeta(2^2s)}-\cdots-\sqrt{\frac 2{\zeta(2^xs)}-1}}}}\right)$$
However, they are only almost equal and thus this is simply an approximation.","['prime-numbers', 'number-theory', 'analytic-number-theory']"
1346655,Kernel of a bounded linear operator on a normed linear space need not be closed or open?,How should be the kernel of a bounded linear operator on a normed linear space as a set? Kernel of a bounded linear operator on a normed linear space need to be closed or open? Or it need not be closed or open?,"['functional-analysis', 'operator-theory', 'linear-algebra', 'general-topology']"
1346690,What exactly is Hensel doing for us in this result?,"I'm reading a paper where the author appeals to Hensel's lemma, but it is not clear to me quite how it is meant to be applied (or, for that matter, which version!). My commutative algebra background is unfortunately not as good as I'd like, and I can't find references between plain old Hensel's lemma for $p$-adics and Henselian rings. The setup is this. I have a split exact sequence of (unital, and otherwise nice) rings
$$
A \stackrel{r}{\to} B \to 0
$$
where $N = ker(r)$ is a nil-ideal, and $s$ is a section of $r$. Let $\Sigma$ be a set of primes (in the end it will probably just be a single prime), $\mathbb{Z}_\Sigma = \{a/b\in\mathbb{Q} \mid p\not| b,\ \forall p\in\Sigma\}$ and $R_\Sigma = R\otimes_\mathbb{Z} \mathbb{Z}_\Sigma$ for $R$ a ring (sim. for an ideal). Then we still have $N_\Sigma$ a nil-ideal, $r_\Sigma$ is still onto. Let $U_\Sigma = r_\Sigma^{-1}(1) \subset A_\Sigma$. We also have that $a\in A_\Sigma$ is a unit if and only if $r_\Sigma(a)$ is a unit. Now comes the bit I don't understand. The author claims, using Hensel, that given a unit $n\in \mathbb{Z}_\Sigma^\times$ with $n\gt 0$ and some $a\in U_\Sigma$, there is a unique $b\in U_\Sigma$ such that $b^n = a$. I can think of several relevant points, but I can't connect them up into a proof in my mind. As $N$ is a nil-ideal, so $A$ should be complete in the $N$-adic topology. Since $a\in U_\Sigma$, I'm lifting the solution $1$ to $x^n - 1=0$ in $B_\Sigma$ through the various projections $A_\Sigma/N_\Sigma^{k+1} \to A_\Sigma/N_\Sigma^k$. The rings $A$ and $B$ are quite nice ($B$ is isomorphic to $\mathbb{Z}^m$, for instance, but $A$ can be more complicated), so I can probably assume they're Noetherian... Is the proof there under my nose? Or is a bit more subtle?","['abstract-algebra', 'hensels-lemma', 'ring-theory']"
1346709,Convolution of measures on a measurable group is associative,"I've come across a statement in Kallenberg's Foundations of Modern Probability which claims this and only tells me to use Fubini's theorem. I am not very familiar with this topic and the text doesn't give any further detail, so I have several questions here: Do the measures need to be at least $\sigma$-finite (since the author tells me to use Fubini's)? Does the actual proof resemble this: $$ \begin{align} ((\mu_1 * \mu_2) * \mu_3)(B) &= \iint \chi_B(xz) (\mu_1*\mu_2)(dx) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_1(dx) \mu_2(dy) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_2(dy) \mu_3(dz) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\& = \iint \chi_B(xy) \mu_1(dx)  (\mu_2*\mu_3)(dy) = (\mu_1 * (\mu_2 * \mu_3))(B) \end{align}$$ in which case it doesn't assume that the $\mu_i$ are invariant under the group operations at all. On the other hand, the answers in Convolution on group with measure claim that they need to be so, so am I misunderstanding this somehow?","['group-theory', 'measure-theory']"
1346720,"$\sum_{k=1}^n \binom{n}{a_1,a_2, \cdots , a_k} \binom mk \binom{k}{b_1,b_2, \cdots , b_l}= m^n,$","(Own) Let $n,m$ be positive integers such that $m>n$. Prove that $$\sum_{k=1}^n \sum_{a_1+a_2 + \cdots +a_k=n} \binom{n}{a_1,a_2, \cdots , a_k} \binom mk \binom{k}{b_1,b_2, \cdots , b_l}= m^n,$$ where $1 \le a_i \; (1 \le i \le k)$ and $a_1+a_2+ \cdots + a_k=n$. 
In here, $b_i$ is the number of $a_x \; (1 \le x \le k)$ that have the same value. For example, if $n=6$ and $6=3+1+1+1$ then $b_i$ are $1$ (one number $3$) and $3$ (three numbers $1$).","['summation', 'binomial-coefficients', 'combinatorics']"
1346752,Is $\pi(n)$ a Rational Function?,"Are there some two-variable polynomials $P(n,\log n)$ and $Q(n,\log n)$ which we have the bellow equation for prime counting function $\pi(n)$ for $n \in \mathbb{n}$?
$$\pi(n) =  \Bigl{\lfloor} \frac{P(n,\log n)}{Q(n, \log n)} \Bigr{\rfloor}$$ Note: According to the Prime Number Theorem such these polynomials should satisfy the bellow limit condition. $$\lim_{n\to\infty} \frac{P(n,\log n) \times \log n}{Q(n, \log n) \times n}  =1 $$","['prime-numbers', 'number-theory', 'analytic-number-theory']"
1346756,Limit of given expression,"Let $\sum a_k=s$. I want to show that $$\lim\limits_{x\to 1^-}(1-x)\sum\limits_{k=1}^{\infty}\frac{ka_kx^k}{1-x^k}=s$$ where $x\in(0,1)$. Thanks for your helps.","['power-series', 'sequences-and-series', 'convergence-divergence', 'limits']"
1346771,How can I prove $\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2}$?,"I am interested about some infinite product representations of $\pi$ and $e$ like this. Last week I found this formula on internet $$\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2}$$ which 
looks like unbelievable. (I forgot the link but I am sure that this is the formula.) How can I start to prove this formula? Thank You.","['pi', 'closed-form', 'real-analysis', 'exponential-function', 'infinite-product']"
1346784,Computing simplicial homology via Smith Normal Form over Rings,"I am not sure whether this is the right forum to ask such a question, if not please let me know. In the context of my masters thesis, I am working on writing a program to compute simplicial homology of certain spaces $X$. The idea is to give the program the differentials $\partial_k:C_k \rightarrow C_{k-1}$ as (quite large) matrices, and compute the homology $\frac{kern\partial_k}{im \partial_{k+1}}$ by bringing both matrices in Smith Normal Form. For integral coefficients one can then use 
$$H(X; \mathbb{Z}) = \mathbb{Z}^{r-t} \oplus_{i=1}^{t} \mathbb{Z}/{s_i}\mathbb{Z},$$ 
with $s_i$ the diagonal entries in the SNF of $\partial_{i+1}$ and $r = rankC_i - rank(\partial_i)$ . As the computation with integral coefficients is too computationally expensive, the idea is to compute $H(X,\mathbb{Z}/ d\mathbb{Z})$ for some $d$ to get the torsion parts of the integral homology using the Universal Coefficient Theorem. As $\mathbb{Z}/ d\mathbb{Z}$ contains zero divisors, the kernel of $\partial_i$ may be a torsion module, if the $s_i$ in the SNF of $\partial_i$ are not units (e.g. not 1). Thus one should keep track of the basis change while computing the SNF to exactly know how the image of $\partial_{i+1}$ lies in the kernel of $\partial_{i}$ to get the homology in the form torsion part + free part as above. How do I do that? I somehow have to compute the SNFs simultaneously, and I do not see how that can work. Maybe I am missing something trivial, but I seem to be stuck here. Thank you!","['homology-cohomology', 'linear-algebra', 'smith-normal-form', 'modules']"
1346796,"For any smooth n-manifold $M$, construct a smooth map $f:M\to S^n$ which is not null-homotopic","PROBLEM: For any smooth n-manifold $M$, construct a smooth map $f:M\to S^n$ which is not null-homotopic ,or even of degree 1 The following is my idea:
First, choosing an arbitrary open coordinate ball $B$ on $M$ and then collapsing $M\setminus B$ to a single point gives a continuous map $f:M \to S^n$.Also, we may assume all $M\setminus B$ is mapped to the north pole $\mathbb N$. It is known that any continuous map $f$ between two manifolds $M_1$ and $M_2$ is homotopic to a smooth map $F$. Moreover,  can we require $|F-f|\le \epsilon$ for any  positive
  continuous function $\epsilon$ on $M_1$ ? Here the norm $|\cdot|$ can be viewed as a Euclidean norm when we embeds $M_2$ to some $\mathbb R^m$ Personally, I believe it's true.
If the statement above is indeed true, then we denote $F$ the smooth map homotopic to $f$ and choose $\epsilon$ small enough. Also, we choose a regular value $\mathbb P$ near the South pole $\mathbb S$ of $S^n$. Then, $\mathbb P$ can only has one preimage and thus the degree of $F$ must be 1 or -1","['approximation', 'differential-geometry', 'smooth-manifolds', 'homotopy-theory']"
1346837,What is the $1469^\text{th}$ derivative of $x^{532}-5x^{37}-4$?,I'm doing some basic calculus exercises on higher derivatives. But I'm stuck at a problem. The question is to find the 1469th derivative of $f(x)=x^{532}-5x^{37}-4$. I've read something about using the general Leibniz rule and a binomial but I can't get my head around it.,"['calculus', 'derivatives']"
1346844,Calculate $\int _0^\infty \frac{\ln x}{(x^2+1)^2}dx$,"Calculate $$\int _0^\infty \dfrac{\ln x}{(x^2+1)^2}dx.$$
I am having trouble using Jordan's lemma for this kind of integral. Moreover, can I multiply it by half and evaluate $\frac{1}{2}\int_0^\infty \frac{\ln x}{(x^2+1)^2}dx$?","['calculus', 'definite-integrals', 'logarithms', 'integration', 'complex-analysis']"
1346865,How do limits work in complex functions?,"I don't quite understand one example in my notes it says. My query is this: I don't understand what the significance of $\theta$ is. Why does it matter that $\theta \in (-\pi,\pi]$? I see the argument as this, clearly anyway in which we approach $0$ must each give the same limit otherwise the limit doesn't exist since the limit is independent of $w$ the limit only depends on $\theta$ hence for any two distinct values of $\theta$ of which there are infinitely many the limit is different $\implies$ the limit does not exist. Is this correct? Also what is $\theta$ changes as the complex number $w$ approaches zero what is to say we need to approach along a straight line with constant $\theta$? Could anyone clear up my misconceptions? Thanks.","['complex-analysis', 'complex-numbers']"
1346866,Are eigenvalues of the limit of a sequence of matrices limits of eigenvalue sequences?,"Let $\{A_n\}\in \mathbb{R}^{m\times m}$ be a sequence of symmetric matrices such that $A_n\to A$ as $n\to \infty$ , i.e. $\lim_{n\to \infty}a_{ij}(n)=a_{ij}\ \forall 1\le i,j\le m$ where $A_n=[a_{ij}(n)],A=[a_{ij}]$ . Let $\rho(A_n)=\{\lambda_1(n),\cdots,\ \lambda_m(n)\}$ be the eigenvalues of $A_n$ and similarly, $\rho(A)=\{\lambda_1,\cdots,\ \lambda_m\}$ be the eigenvalues of $A$ , arranged in, say, increasing order. Here are my questions 1)Can I write $\lambda_k(n)\to \lambda_k,\ 1\le k\le m$ ? 2)If I define (with a slight abuse of standard notation) $\delta_s(n),\ 1\le s\le m$ as the maximum eigenvalue of any $s\times s$ submatrix of $A_n$ and if $\delta_s$ be the corresponding quantity for $A$ , then can I say that $$\lim_{n\to \infty}\delta_s(n)=\delta_s$$ ? Intuitively it seems to me that the answers are positive since the eigenvalues of a matrix are continuous functions of the elements of the matrix and $\delta_s$ is just the maximum of some eigenvalues of submatrices. However, I am not sure if this argument is sound enough. Maybe this is a very trivial issue for people here, but I would really appreciate if someone can kindly provide some explanation regarding the correct answer. Thanks in advance.","['sequences-and-series', 'eigenvalues-eigenvectors', 'real-analysis', 'matrices']"
1346884,Simple Method of Solution of $X^p+Y^p=(X+1)^p$,"I am looking for simple prove of non existence of solution of $$X^p+Y^p=(X+1)^p$$
for $p>2$
I know it is partial case of Fermat Last Theorem. But I am looking for simple method without going into the complex part of group theory and algebra.
I have simply found from the Fermat Little Theorem that $$Y\equiv1\pmod p$$
But this the maximum I was able to get. Is there any idea how to solve this using simple methods?","['number-theory', 'diophantine-equations']"
1346901,Show that $A^k$ has eigenvalues $\lambda^k$ and eigenvectors $v$.,"I want to prove the following statement: Let $A \in \Bbb R^{n\times n}$ with eigenvalues $\lambda$ and
  eigenvectors $v$. Show that $A^k$ has eigenvalues $\lambda^k$ and
  eigenvectors $v$. There are two ways I tried to prove this but I am not sure if either of them is accurate or complete. If $Ax=\lambda x$ then multiplying by $A$ from the left yields $$AAx=A \lambda x \iff A^2x=\lambda Ax \iff A^2x=\lambda (\lambda x)\iff A^2x=\lambda^2x$$ In fact, for every $A$ thats multiplied to both sides, the right side ""gains"" a factor $\lambda$ (since $Ax$ can be substituted by $\lambda x$) while the eigenvectors remain the same. It follows that multiplying both sides by $A^{k-1}$ yields: $$A^{k-1}Ax=A^{k-1}\lambda x \iff A^kx=\lambda (\lambda^{k-1}x)\iff A^kx=\lambda^kx$$ This is a proof that I partly got from Gilbert Strang's Lin. Algebra lecture. Suppose $A$ has $n$ linearly independent eigenvectors. Let $S$ be the matrix that has the eigenvectors of $A$ as its columns. Then, $$AS=A\begin{bmatrix}x_1…x_n\end{bmatrix}=\begin{bmatrix}\lambda_1x_1...\lambda_nx_n\end{bmatrix}=\begin{bmatrix}x_1...x_n\end{bmatrix}\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$$ Let $\Lambda=\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$ then $AS=S\Lambda$ Multiplying by $S^{-1}$ from the right: $$\implies S^{-1}AS=\Lambda \space \space \text{or} \space \space A=S \Lambda S^{-1}$$ $$\implies A^k=(S \Lambda S^{-1})^k=S \Lambda^kS^{-1}$$ It follows that $A^k$ has eigenvalues $\lambda^k$ and eigenvectors $x$. I am not sure if either of them are correct. For the second one, I suspect that I need to guarantee that $S^{-1}$ exists or that $S$ is invertible but I am not sure how to do that. Also, how can I be sure that there are $n$ linearly indep. eigenvectors and not $n-1$ for example? Are there any problems with the first one?","['eigenvalues-eigenvectors', 'linear-algebra']"
1346918,Question about the Fourier Inversion Formula,"We have $$\hat{f}(\xi)=\mathcal{F}f(\xi):= \int_{-\infty}^{\infty}f(x)e^{-2\pi i\xi x}dx,$$ with $f\in L^{1}$, and the Fourier inversion formula says that $$f(x)=\int_{-\infty}^{\infty}\hat{f}(\xi)e^{2\pi i\xi x}d\xi.$$ My question is: do we need that $\hat{f}\in L^{1}$, in order to the integral above converges and inversion formula holds, or $f\in L^{1}$ implies somehow $\hat{f}\in L^{1}$? Thanks.","['fourier-analysis', 'convergence-divergence', 'complex-analysis']"
1346921,Finding periodic (trigonometric?) function given points,"It's been a while since I've taken a math class. I need a couple functions for a program I'm working on. I can tell they involve trigonometry, but I can't figure out how to derive the function given some points I know lie on the curve. Here are the points: degrees     x-axis force multiplier
    ----  | ----
      0   |  .5
      45  |   0
      90  |  -.5
     135  |  -1
     180  |  -.5
     225  |   0
     270  |  .5
     315  |   1
     360  |  .5 And another, similar function I would like to know how to figure out (in case you're feeling ambitious) degrees     y-axis force multiplier
    ----  | ----
      0   |  .5
      45  |   1
      90  |  .5
     135  |   0
     180  |  -.5
     225  |  -1
     270  |  -.5 
     315  |   0
     360  |  .5 Thanks! EDIT I want to clarify what I am actually attempting to do here in hopes that it will help me receive the best suggestions. The points I posted above are x vector and y vector ratios of a force, given that force's change in rotation from an arbitrary starting orientation. Let me put it another way: I have an arrow that can rotate.  I also have a force that I need to apply in the direction the arrow points.  So I need values for the x and y component of the force vector given the arrow's rotation. ^ is 0x, 1y    (because it's vertical and up)
< is -1x, 0y   (because it's horizontal and left)
> is 1x, 0y    (because its horizontal and right)
etc. So would a sawtooth function model it appropriately (as suggested below)?  I think linear will work but I want to be sure.  Thank you! EDIT 2 Though the suggestion to use a Fourier series was very interesting and seemed to work well, it was merely imitating a 'sawtooth' plot: So the piecewise function (linear) seems the best way to model the data.  Thanks for all the help! EDIT 3 For anyone who stumbles across this page in the future, I want to be clear that the way I'm modeling 2 dimensional force vectors in my program is not real-world accurate.  The ""sawtooth"" function works for me and my game, but is does not reflect actual physics.","['trigonometry', 'functions']"
1346928,Is a principal bundle automorphism locally given by a left action?,"Let $G\hookrightarrow P \xrightarrow{\pi} M$ be a principal bundle, denote by $\cdot$ the right action of $G$ on $P$. Let $f:P\rightarrow P$ a bundle automorphism (i.e. $f$ is a diffeo, $f(p \cdot g) = f(p)\cdot g$, $\pi\circ f=\pi$). Is it true that with respect to a local trivialisation the action of $f$ is given by the left multiplication on the fibres? The reason why I think so is the following one. Let $(U,\Phi)$ be a local trivialisation, $\Phi:\pi^{-1}(U) \rightarrow U\times G$, $\Phi(p)=(\pi(p),\phi(p))$. Since $f$ does not move base points, $\Phi\circ f\circ \Phi^{-1}(x,e) = (x,h(x)) $ for some $h(x)=\phi(f(\Phi^{-1}(x,g))\in G$. Then $\Phi\circ f\circ \Phi^{-1}(x,g) = \Phi( f(\Phi^{-1}(x,e)\cdot g)=(x,\phi(\Phi^{-1}(x,e)) g)=(x,h(x) g) $ and with respect to the local trivialisation the action of $f$ is given by left multiplication on the fibres by $h:U\rightarrow G$. Is the above correct?","['principal-bundles', 'differential-geometry', 'fiber-bundles']"
1346929,Solving an infinite series containing $\arctan$,"I need to compute: $$\tan\bigg(\arctan\left(\frac{1}{2}\right) + \arctan\left(\frac{2}{9}\right)+ \arctan\left(\frac{1}{8}\right)+\arctan\left(\frac{2}{25}\right)+\arctan\left(\frac{1}{18}\right)+\ldots\bigg)$$ I proceeded as follows. The series is $\left(\sqrt{2}/(n+1)\right)^2$, so the sum inside $\tan$ can be simplified as: $S=\sum_{n=1}^\infty \arctan(n+2)-\arctan(n)$. How do I proceed from here?",['trigonometry']
1346944,What is the use of the chain rule?,"While studying calculus at home, I reached derivatives, and a book mentioned the chain rule. The book didn't go into much detail, and the internet searches gave me little information, so I was hoping that someone could enlighten me on this fundamental principle of calculus.",['calculus']
1346973,"If $(B_t)_{t\ge 0}$ is a Brownian motion and $\tau$ is a stopping time, then the stopped process $(B_{\min(\tau,t)})_{t\ge 0}$ is integrable","Let $B=(B_t)_{t\ge 0}$ be a Brownian motion on a probability space $(\Omega,\mathcal A,\operatorname{P})$. By definition $B_t$ is normally distributed with mean $0$ and variance $t$. Now, let $\mathbb F=(\mathcal F_t)_{t\ge 0}$ be a filtration on $(\Omega,\mathcal A)$ and $\tau$ be a $\mathbb F$-stopping time. Is the stopped process $B^\tau:=(B_{\min(\tau,t)})_{t\ge 0}$ integrable, i.e. does it hold $$B_{\min(\tau,t)}\in\mathcal{L}^1(\operatorname{P})\;\;\;\text{for all }t\ge 0\;?\tag{1}$$ I assume, that $(1)$ can be easily shown and most likely $B_{\min(\tau,t)}$ is normally distributed, too. However, how can we prove that? Maybe we need boundedness of $\tau$. Then, there exists a $T>0$ such that $$B_{\min(\tau,t)}=B_T\;\;\;\text{for all }t>T$$ and $B_{\min(\tau,t)}$ would be normally distributed with mean $0$ and variance $T$ for all $t>T$. But that is not exactly the desired statement $(1)$. Remark: $\;\;\;$ $B$ is called Brownian motion $:\Leftrightarrow$ $B$ is a real-valued stochastic process on $(\Omega,\mathcal A,\operatorname{P})$ with $B_0=0$ almost surely $B$ has independent and stationary increments $B_t\;\tilde\;\mathcal{N}_{0,\;t}$ $B$ is almost surely continuous Please note, that out there exists a definition of a so-called $\mathbb{G}$-Brownian motion , where $\mathbb{G}$ is a given filtration on $(\Omega,\mathcal{A})$. One can show, that the Brownian motion defined above is a $\mathbb{G}$-Brownian motion, if $G$ satisfies the usual conditions .","['probability-theory', 'stochastic-calculus', 'real-analysis', 'stochastic-processes', 'brownian-motion']"
1346974,What's a group whose group of automorphisms is non-abelian?,"I recently attended an interview for admission to graduate programs in Mathematics. The interviewing professor asked me a question - Tell me a group whose group of automorphisms is non-abelian. Because I was too nervous, I couldn't think of anything substantial. Could someone please tell me, in such a situation, what is the logical way of deducing the answer to such a question posed by the interviewer? Thank you very much for your help!","['abstract-algebra', 'group-homomorphism', 'group-theory']"
1346998,Cantor Set in Alexander Horned Sphere Construction,"I have seen it said in several different places that in the standard construction of the Alexander horned sphere , given by successive embeddings of a sphere with $2^n$ handles, either limited or intersected, a Cantor set shows up as a place where certain aspects of the construction have to be modified, or where otherwise something interesting happens (I know I should give a more precise description, but it is such a standard construction  that that would seem somewhat superfluous). In Hatcher, this Cantor set is said to correspond to the intersection of all the handles. However, I am unable to see what this set looks like, and I definitely don't understand what makes this set special. If anyone could help explain this phenomena I would be very grateful.","['spheres', 'cantor-set', 'algebraic-topology', 'general-topology']"
1347040,Can we take out a constant while differentiating?,"In the solved example above, rather than taking $a^2x^4$ together and differentiating $a^2 = 0$, we differentiated $x^4$ and took out $a^2$. Why? Couldn't we have differentiated $a^2$ and gotten the answer zero?","['calculus', 'derivatives']"
1347061,Explanation of derivation made at wikipedia.,"in this wikipedia article A deriviation to convert true and eccentric anomaly. I am however quite stunned by a single line - trying to reproduce but after half a dozen sheets of paper I can't find how it is done. The problem lies in the following: $$\tan(E) = \frac{\sqrt{1-e^2} \sin(\theta)}{e + \cos(\theta)}$$ $$\tan \left( \frac{\theta}{2}\right) = \sqrt{\frac{1+e}{1-e}} \cdot \tan \left( \frac{E}{2}\right)$$ I am really stuck in how to get from the first line to the second line (which is at wikipedia explained by a single word ""also"").","['mathematical-astronomy', 'trigonometry']"
1347085,Find the natural boundary of $\sum_{n=1}^\infty \frac{z^n}{1-z^n}$,"I'm asked to prove that the natural boundary of $\sum_{n=1}^\infty \frac{z^n}{1-z^n}$ is the unit circle. My try: First, use root test to show that the series converges for $|z|<1$. Then I have to show that every point on the unit circle is singular, or equivalently, there is a dense set of singular point on the unit circle. However, I didn't succeed. Can anyone give a hint? Note: The series can be rewritten as $\sum_{n=1}^\infty d(n)z^n$, where $d(n)$ is the number of positive divisors of $n$. (Maybe it is helpless.)","['sequences-and-series', 'complex-analysis']"
1347103,Application Problem: Conditioning Poisson Process,"I am trying to solve the following application problem: There are $n$ components with independent lifetimes which are such that component $i$ functions for an exponential time with rate $\lambda_i$. Suppose that all components are initially in use and remain so until they fail. Find the probability that component 1 is the second component to fail. I know I need to condition on which component fails first, but I don't know how to set up this condition in order to compute the probability in question.","['probability', 'poisson-distribution', 'probability-distributions']"
1347121,Equation with sine and cosine - coefficients,"I have some trouble with the conceptual understanding of the way we solve this kind of equations. Let's say we have:
$$(3-3b^2)\sin(bx)+3a\cos(2x)=6\cos(2x)$$
The method employed on classes was equating coefficients on both sides of the equation. For example there's no sine function on the right hand side, so $(3-3b^2)=0$. Therefore we're left with $3a\cos(2x)=6\cos(2x)$ so obviously $3a=6$. From that it follows that $a=2, b=1$ or $-1$. This is logical to me but I still can't understand why the sine and the cosine on LHS can't ""coexist"" (not sure how to phrase that) and give together the result on RHS. Example of what I think if that's not clear: $$\underbrace{(3-3b^2)\sin(bx)}_n+\underbrace{3a\cos(2x)}_m=\underbrace{6\cos(2x)}_{n+m}$$",['trigonometry']
1347128,Is it possible to assign probability to a set $X$ with $|X|>2^{\aleph_0}$?,Is it possible to assign probability to a set $X$ with cardinality $|X| > 2^{\aleph_0}$? Example would be a set $|X| = 2^{2^{\aleph_0}}$.,"['probability-theory', 'elementary-set-theory', 'logic']"
1347134,Proof that if a simple Graph contains at most two nodes with odd degree then it has a Euler walk,"My proof would be start as the following :
In general if there are two node at most, then one node used to start walking and the other to end. A) If we start from odd one, this means we have two scenarios: 1) if odd =1 then we start from it and leave it   forever; this means: visiting once (Starting Point). 2)if odd>1 then we have to revisit it again, but we will leave it because # edges will enforce us to leave it at the end. B) if then having another node with odd degree, this mean we have to stop at it. because entering node with odd edges enforces us to stay on it at the end, being no possibility to go out forever. I think this proofs that Lemma ? Is not it ? please see the picture bellow: https://i.sstatic.net/cCnqS.jpg","['graph-theory', 'discrete-mathematics']"
1347139,Weierstrass's M-test example for uniform convergence and switching Sum and Integral.,"How would I go about finding $M_n$ in \begin{equation}
\sum_{n=1}^{\infty} \int_{0}^\infty x^{\frac{s}{2}-1}e^{-\pi n^{2}x}dx
\end{equation}
to show that it is uniformly convergent? UPDATE: Just coming back to this and having trouble understanding this. Without relating this integral  to the zeta and gamma functions, how do I show the sum has uniform convergence ( Weierstrass)? For $Re(s) > 1$. And can therefore swap integral with summation by fubinis theorem?","['riemann-zeta', 'complex-analysis', 'functional-analysis']"
1347157,How do I find invariant lines for a system of differential equations?,How do I find invariant lines for the following system of differential equations: $$x' = 2x - xy + x^3$$ $$y' = y - xy$$,['ordinary-differential-equations']
