question_id,title,body,tags
3904962,Is integration by substitution always the reverse chain rule?,"I understand that integration by substitution can be justified in the following way. If $I=\int f'(g(x))g'(x) \, dx$ then $I=f(g(x))+C$ . If we make the substitutions $$
u=g(x) \text{ and } du=g'(x)dx
$$ then $I$ becomes $\int f'(u)du=f(u)+C=f(g(x))+C$ , which is the same. However, I often see substitutions that don't seem to take this form. For example, a common way to evaluate $$
I=\int \sqrt{1-x^2} \, dx
$$ is by setting $x=\sin u$ (or, to be more precise, $u=\arcsin x$ ). The derivative of $\arcsin x$ is $$
\frac{1}{\sqrt{1-x^2}} \, ,
$$ which does not appear in $I$ . It doesn't seem like $I$ is of the form $\int f(g(x))g'(x) \, dx$ . So why is this substitution justified? Are we still doing the chain rule in reverse, or is something else going on?","['integration', 'calculus', 'substitution']"
3905020,If $f(nx)=f(x)^n$ then $f(x+y)=f(x)f(y)$,"If $f:\mathbb{R}\to\mathbb{R}_{+}$ is injective monotone function and if, $f(nx)=f(x)^n$ for all $n\in\mathbb{Z}$ and for all $x\in\mathbb{R}$ , then, $$f(x+y)=f(x)f(y),\text{ for all }x,y\in\mathbb{R}$$ It is easy to check the equality if $y$ is a multiple of $x$ . I thought about proving the inequalities, $f(x+y)\le f(x)f(y)$ and $f(x+y)\ge f(x)f(y)$ , but I couldn't get to them. Assuming that $f$ is increasing and $x<y$ , then $y=kx+r$ , for some $k\in\mathbb{Z}$ and $0< r\le \lvert x \rvert$ , so $$f(x+y)=f(x+kx+r)=f(x(k+1)+r)\ge f(x(k+1))=f(x)^{k+1}=f(x)f(x)^k=f(x)f(kx)$$ That is, $f(x+y)\ge f(x)f(kx)$ , but the problem is that $f(kx)<f(y)$ so I can't continue the demonstration.","['algebra-precalculus', 'monotone-functions', 'real-analysis']"
3905091,Do we have $u^2 \leq u$ when $\|u \| \leq 1$ in a $C^*$-algebra?,"Let $u$ be a positive element in a $C^*$ -algebra $A$ with $\|u \|\le 1$ . Then is it true that $u^2 \leq u$ ? Attempt : This is true in the commutative $C^*$ -subalgebra $C^*(u)$ , since by the Gelfand-Naimark theorem, $C^*(u) \cong C_0(X)$ where $X$ is some locally compact Hausdorff space. Thus, this also holds in $A$ since positivity does not depend on the ambenient $C^*$ -subalgebra. Is this correct?","['c-star-algebras', 'functional-analysis']"
3905181,Prove that this graph with 20 edges on 11 vertices is not 3-colorable.,"Let $G$ be the graph with $11$ vertices labeled $0, 1, 2,..., 10$ and $20$ edges $$(1, 2),(2, 3),(3, 4),(4, 5),(5, 1),\\
(1, 7),(2, 8),(3, 9),(4, 10),(5, 6),\\
(1, 10),(2, 6),(3, 7),(4, 8),(5, 9),\\
(0, 6),(0, 7),(0, 8),(0, 9),(0, 10).$$ Prove that $G$ is not $3$ -colorable. I have tried to find the clique number to show that the lower bound is at least $4$ ; however, the largest clique I could find was on $2$ vertices, so that was not a helpful strategy. I then tried to find that the independence number was less than $3$ so that $$\chi(G)\geq \frac{|V|}{\alpha(G)}$$ where $\alpha(G)$ is the independence number. Again, however, I found an independent set on $4$ vertices, that of $\{2,5,7,10\}$ , so I am nowhere closer to proving what I'm supposed to. Any help would be great!","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
3905214,How to compute $\int_{0}^{+\infty}\frac{x^2\mathrm{d}x}{e^{x}-1}$ analytically?,"Does anyone know how to compute analytically the following integral: $$\int\limits_{0}^{+\infty}\dfrac{x^2\mathrm{d}x}{e^{x}-1}$$ It should be equal to $2\zeta(3)$ according to Maple. I tried the following using the binomial theorem for negative integer exponents: $$I = \int\limits^{+\infty}_{0} e^{-x}(1-e^{-x})^{-1}x^2\mathrm{d}x = \int\limits^{+\infty}_{0}\left[\sum_{k=0}^{+\infty}(-1)^k(-1)^ke^{-(k+1)x}\right]x^2\mathrm{d}x=\int\limits^{+\infty}_{0}\left[\sum_{k=0}^{+\infty}(-1)^{2k}e^{-(k+1)x}\right]x^2\mathrm{d}x$$ After another change of variables, $y=(k+1)x$ : $$I = \sum_{k=0}^{+\infty}(-1)^{2k} \frac{1}{(k+1)^3}\int\limits_0^{+\infty} y^2e^{-y}\mathrm{d}y$$ The keen eye might recognize $\int\limits_0^{+\infty} y^2e^{-y}\mathrm{d}y$ as the gamma function, $\Gamma(3)=(3-1)!=2$ . This, together with a slight nudge to the bottom limit of the summation we can rewrite things as: $$I = \Gamma(3)\sum_{k=1}^{+\infty} \dfrac{(-1)^{2k}}{k^3}$$ And i see immediately (since the beginning in fact...) an infinite sum that makes me troubles and i can't get rid of. I tried to found if i did any trivial error but i'm focusing since to many hours to found it. That's why I need an external view to point me out my obvious error. Thanks in advance for your help","['integration', 'calculus', 'riemann-zeta']"
3905285,"In graph theory, is there a term for the value of the difference between indegree and outdegree?","I'm working on a Graph Theory research project, and one of the key components is talking about the indegrees and the outdegrees of a few particular vertices. I need to define a term for the value of indegree-outdegree for a particular vertex. Or for vertex $v$ does the value $\deg^{-}(v) - \deg^{+}(v)$ have a name? Some possible terms have been ""netdegree"" or ""flow value,"" which would suffice for my needs, but I would like to make sure I use the existing terminology if possible.","['graph-theory', 'combinatorics', 'terminology']"
3905321,Is my Calculus Proof Correct?,"Question:
A not uncommon calculus mistake is to believe that the product rule for derivatives says that $(fg)'=f'g'$ . If $f(x)=e^{x^2}$ , determine, with proof, whether there exists an open interval $(a,b)$ and a nonzero differentiable function $g$ defined on $(a,b)$ such that this wrong product rule is true for $x$ in $(a,b)$ . Is my solution correct? Is there anything I need to improve? My solution: We're trying to find a function $g$ such that $(fg)'=f'g'.$ Using the product rule, we get $$f'g + fg' =  f'g'.$$ Plugging in both $f(x)=e^{x^2}$ and $f'(x)=2xe^{x^2},$ we get $$2xe^{x^2}g + e^{x^2}g' = 2xe^{x^2}g'.$$ Simplifying and canceling $e^{x^2},$ we get $$\frac{g'}{g} = \frac{2x-1}{2x} = 1 +  \frac{1}{2x-1}.$$ Taking the integral of both sides, $$\int \frac{dg}g = \int 1+ \frac{1}{2x-1} \, dx.$$ $$\log|g| = \frac{\log|2x-1|}{2} + x + C.$$ $$|g| = e^{\frac{\log|2x-1|}{2}}e^xe^C.$$ Letting $e^C=K,$ we get $$\boxed{g = Ke^x\sqrt{|2x-1|}},$$ Where K is a constant greater than 0.
Therefore, on any interval $(a,b)$ that does not contain the value of $\frac{1}{2},$ there exists nonzero differentiable function $g$ defined on $(a,b)$ such that $(fg)'=f'g'$ is true for $x$ in $(a,b)$ .","['integration', 'calculus', 'solution-verification', 'derivatives']"
3905371,How to evaluate $\sum _{n=1}^{\infty }\left(\frac{H_n^2+H_n^{\left(2\right)}}{n}\right)^2$ in a particular way.,"How to evaluate: $$\sum _{n=1}^{\infty }\left(\frac{H_n^2+H_n^{\left(2\right)}}{n}\right)^2,$$ without splitting the expression into more sums. Here $H_n^{\left(m\right)}=\sum _{k=1}^n\frac{1}{k^m}$ is the harmonic number of order $m$ . If one just wants to evaluate it if we split we have, $$2\sum _{n=1}^{\infty }\frac{H_n^2H_n^{\left(2\right)}}{n^2}+\sum _{n=1}^{\infty }\frac{H_n^4}{n^2}+\sum _{n=1}^{\infty }\frac{\left(H_n^{\left(2\right)}\right)^2}{n^2},$$ Then making use of this results one only has to compute $$\sum _{n=1}^{\infty }\frac{\left(H_n^{\left(2\right)}\right)^2}{n^2}$$ But I'd like to know if its possible to evaluate the series without splitting or expanding the terms.","['integration', 'definite-integrals', 'real-analysis', 'harmonic-numbers', 'sequences-and-series']"
3905403,(Borel) Normal number problem,"Problem: Given an integer $b \ge 2$ , a number $x \in [0,1)$ is (Borel) normal base-b, if its b-ary representation as a sequence $(\sigma_1,\sigma_2,...)$ -where $\sigma_i \in \{0,1,...,b-1\}$ are such that $x=\sum_{i \ge 1} \sigma_i b^{-i}$ -obeys the following: For eaach $r \ge 1$ and each $\overline{\sigma_1},...,\overline{\sigma_r} \in \{0,1,...,b-1\}, \frac{1}{n}\#\{k=1,...,n:(\sigma_{k+1},...,\sigma_{k+r})=(\overline{\sigma_1},...,\overline{\sigma_r})\}$ converges to $b^{-r}$ as $n \rightarrow \infty$ . A number x is absolutely normal if it is normal base b for every integer $b \ge 2$ . Prove if $\mathcal{U}=\textbf{Uniform}([0,1])$ , then $\mathcal{U}$ is absolutely normal almost surely. Theorem 1: Almost every real number is absolutely normal. Proof: A number $x \in (0,1)$ is not absolutely normal if there exist $b \ge 2$ , $L \in \mathbf{N}$ and $a_1,...,a_L \in \{0,...,b-1\}$ such that if $x=\sum_{n=1}^{\infty} \frac{c_n}{b^n}$ then $$\textbf{lim inf}_{n \rightarrow \infty} \frac{\# \{n \le N-L |c_{n+i}=a_i,i=1,...,L\}}{N}<\frac{1}{b^L}$$ . Then there exists $s \in \{0,...,L-1\}$ such that $$\textbf{lim inf}_{n \rightarrow \infty} \frac{\# \{n \le N-L, n \equiv s (\textbf{mod L}) |c_{n+i}=a_i,i=1,...,L\}}{N}<\frac{1}{Lb^L}$$ . Hence for some rational $\beta<\frac{1}{Lb^L}$ , $$\textbf{lim inf}_{n \rightarrow \infty} \frac{\# \{n \le N-L, n \equiv s (\textbf{mod L}) |c_{n+i}=a_i,i=1,...,L\}}{N} \le \beta$$ . Denote by $R_{b,La,s,\beta}$ the set of all $x=\sum_{n=1}^{\infty} \frac{c_n}{b^n}$ satisfying $$\textbf{lim inf}_{n \rightarrow \infty} \frac{\# \{n \le N-L, n \equiv s (\textbf{mod L}) |c_{n+i}=a_i,i=1,...,L\}}{N} \le \beta$$ . The result of the prvious paragraph is that the set of not absolutely normal number is a subset of $$\bigcup_{b=2}^{\infty} \bigcup_{L=1}^{\infty} \bigcup_{a_1,...,a_L}^{b-1} \bigcup_{s=0}^{L-1} \bigcup_{\beta \in (0, \frac{1}{Lb^L}\cap \mathcal{Q}} R_{b,L,a,s,\beta}$$ .
It is sufficient to prove every set $R_{b,L,a,s,\beta}$ has zero measure. Then the set of not absolutely normal numbers is a subset of a countable union of null sets, hence it is a null set. Let $b \ge 2, L \in \mathcal{N}, a_1,...,a_L \in \{0,...,b-1\}, s \in \{0,...,L-1\}$ and $\beta \in (0,\frac{1}{Lb^L}$ . Put $A=a_1b^{L-1}+a_2b^{L-2}+...+a_L$ . Let $$x=\sum_{n=1}^{\infty} \frac{c_n}{b^n}=\sum_{n=1}^s \frac{d_n}{b^n}+\frac{1}{b^s}\sum_{n=1}^{\infty} \frac{d_{s+n}}{b^{Ln}} \in R_{b,L,a,s\beta}$$ . Then obviously $$\# \{n \le N-L,n \equiv s(\textbf{mod L})|c_{n+i}=a_i,i=1,...,L\}=\# \{s<n \le [\frac{N-s}{L}]|d_n=A\}$$ . Hence $$\textbf{lim inf}_{M \rightarrow \infty} \frac{\# \{s<n \le M| d_m=A}{M}=\textbf{lim inf}_{N \rightarrow \infty} \frac{\# \{s<n \le [\frac{N-s}{L}]|d_n=A\}}{[\frac{N-s}{L}]} \le L\beta$$ . From this we obtain $R_{b,L,a,s\beta} \subseteq P$ , where $$P=\{x=\sum_{n=1}^s \frac{d_n}{b^n}+\frac{1}{b^s}\sum_{n=1}^{\infty} \frac{d_{s+n}}{b^{Ln}}|\textbf{lim inf}_{N \rightarrow \infty} \frac{\# \{s<n \le N |d_n=A\}}{N} \le L \beta\}$$ . Then it is sufficient to prove that the set P has zero measure. The complete version of the proof is found here. An elementary proof that almost all real numbers are normal My question is how am I supposed to mimic the proof to solve the problem? Thanks a lot!","['borel-sets', 'probability-theory']"
3905458,"Does $A(x, m) = 1 - A(m, x)$ imply some symmetry in partial derivatives?","This is a self-made problem. I have a function $A$ of variables $x, m \geq 0$ . A satisfies the following symmetry: $$A(x, m) = 1 - A(m, x)$$ Is there an equation that relates the partial derivative of $A$ wrt the first argument to the partial derivative of $A$ with respect to the second argument? It seems like there should be something straightforward but I'm not sure how to deduce it. Thank you!","['multivariable-calculus', 'calculus', 'partial-derivative', 'symmetry', 'derivatives']"
3905467,Separable C* algebra has a cyclic representation that is isometric.,"I'm studying Conway's Functional Analysis. In chapter 8.5, we learn about GNS representation. Theorem 8.5.17 states that $\mathcal{A}$ is separable then, we can select representation $(\pi, \mathcal{H})$ that $\mathcal{H}$ is separable. And next line, every separable C*-algebra has a cyclic representation that is isometric. I want to prove this, and Conway suggests that $f_n$ is countable weak* dense subset of $S_\mathcal{A}$ (which is state space of $\mathcal{A}$ ), then if we set $f = \sum 2^{-n}f_n$ then $\pi_f$ is an isometry.( $\pi_f$ is from GNS construction) Since GNS Construction suggest that if we make $\pi_f$ , then it is cyclic representation, so I need to prove that $\pi_f$ is isometry. But I cannot get any way to get $\pi_f$ 's norm. if $e_f$ is cyclic vector of $\pi_f$ , $$\|p_f(a) \|^2 \geq \|\pi_f(a) e_f\|^2 = \langle  \pi_f(a^* a) e_f, e_f\rangle = f(a^* a) = \sum 2^{-n}f_n(a^* a)$$ and we know that $\|a\| = \sup\{f(a) : f \in S_\mathcal{A} \}$ for positive element $a$ but I don't know that $\sum 2^{-n}f_n(a^* a) \geq  \|a^* a\| - \epsilon$ . How can i deal with this problem?","['c-star-algebras', 'functional-analysis']"
3905495,"References required for ""minimal switches to control a switched linear system""","A switched linear system of ODEs is defined as a system of ODEs of the form $\dot{x}(t) = A_i(t)x(t)$ , with $A_i$ for $i=1,2,...,N$ being a fixed number of matrices. More precisely, given a $\sigma : [0,t] \to \{1,2,...,N\}$ and an initial point $\mathbf{x} \in \Bbb R^n$ ,we could talk about (assuming existence and uniqueness of solution) the solution to $\dot{x}(t) = A_{\sigma(t)}x(t)$ with $x(0) = \mathbf x$ , and look at its trajectory. In the research of switched linear systems, often problems of controllability(See here ) are considered. Other topics of research include minimizing , among all $\sigma$ suitable (e.g. steering to a certain point/region), a quadratic functional of $\sigma$ . However, I am not aware of any research looking at the minimum number of switches to reach a point/region. More precisely, consider a fixed $\mathbf x \in \mathbb R^n$ , a destination region $S \subset \mathbb R^n$ with $\mathbf x \notin S$ , a fixed $T>0$ (think of $T$ as being very large) and let $$\Sigma = \{\sigma : [0,T] \to \{1,2,...,N\}, \text{ The solution  of $\dot{x}(t) = A_{\sigma(t)}x(t)$ reaches $S$ before time $T$}\}$$ Assume that $\Sigma$ is non-empty. Now, for each $\sigma \in \Sigma$ , the number of times it switches is its number of discontinuities, so define $D(\sigma) = n$ if $\sigma$ has $n$ discontinuities, and $D(\sigma) = \infty$ if $\sigma$ has infinitely many discontinuities. Is there literature available on studying $\min_{\sigma \in \Sigma}D(\sigma)$ ? In words : Assuming there is a control steering $\mathbf x$ to $S$ , there is a control with at most $\_\_\_$ switches also steering $\mathbf x$ to $S$ . I would like to know if there are papers/textbooks which discuss this exact question, in the context of linear switched systems. I am aware of Pontryagin's maximum principle, but do not know how it would provide structural results for this question, so if anybody could suggest an approach via PMP I would be happy as well.","['optimal-control', 'optimization', 'ordinary-differential-equations']"
3905521,Proving Riordan's identity that $\sum_{k=1}^{n} {n-1 \choose k-1} \frac{k!}{n^k}=1$,"In a book titled  ""Advances in Problem Solving,"" authored by Sailesh Shirali, Riordan's identity is mentioned which can also be written as $$S=\sum_{k=1}^{n} {n-1 \choose k-1} \frac{k!}{n^k}=1~~~~(1)$$ Here, we prove it by the integral representation of $(j+1)!$ as: $$S=\sum_{j=0}^{n-1} {n-1 \choose j} \frac{(j+1)!}{n^{j+1}}=\sum_{j=0}^{n-1} {n-1 \choose j} \frac{1}{n^{j+1}} \int_{0}^{\infty} x^{j+1} e^{-x} dx$$ $$\implies S=\frac{1}{n^{n}}\int_{0}^{\infty} x ~(n+x)^{n-1} e^{-x} dx=n^{-n}\int_{0}^{\infty} [(x+n)^n-n(x+n)^{n-1}]~ e^{-x} dx$$ $$S=-n^{-n}\left. (x+n)^n~ e^{-x}\right|_{0}^{\infty}=1.$$ It will be interesting to see other approaches for proving (1).","['summation', 'factorial', 'binomial-coefficients', 'sequences-and-series', 'binomial-theorem']"
3905542,"Let $S=\{AB-BA| A,B \in M_n(K)\}$ where $K$ is a field. Prove that $S$ is closed under matrix addition.","I know there is a result that $S=$ collection of all trace $0$ matrices, and that collection forms a vector space. But I want to prove it independently i.e. For any $A,B,C,D\in M_n(K)$ we have to find $E,F\in M_n(K)$ such that $(AB-BA)+(CD-DC)=EF-FE$ . But I don't know how to solve this. But the thing I can observe that the above equation gives rise to $n^2$ equation (equating each entries of matrices of both the sides) with $2n^2$ variables (Total number of entries of $E,F$ is $2n^2$ ).Can this problem be simplified if we choose special kind of $E$ say diagonal matrix. Edit-(This is valid only for $\Bbb{R}$ or $\Bbb{C}$ ) $AB-BA=(A+aI)(B+bI)-(B+bI)(A+aI)$ for all $a,b\in\Bbb{R}$ . And there is $a,b$ in $\Bbb{R}$ such that $A+aI,B+bI$ are invertible. Hence, we can assume $A,B$ to be invertible matrices i.e. $S=\{AB-BA|A,B\in GL_n(K)\}$","['matrices', 'trace', 'linear-algebra']"
3905568,How to quickly select some vectors from a given set to make the sum of them has the smallest $\ell_2$ norm?,"The question is how to design an effective algorithm to solve this problem: $$
\underset{k_1,k_2,\cdots ,k_s}{\min}\lVert \sum_{i=1}^s{\mathbf{x}_{k_i}} \rVert _2,
$$ where $l (l >> s)$ is the set size, $n$ is the vector size, $ k_i $ s are different numbers in $\left\{ 1,2, \cdots, l \right\}$ . The set $\left\{ \mathbf{x}_i \right\} _{i=1}^{l}$ is known.","['linear-algebra', 'discrete-mathematics', 'algorithms', 'discrete-optimization', 'computer-science']"
3905608,Topological interior of a standard $n$-simplex,"A topological standard $n$ -simplex is a subset $\Delta^n = \{ (x_0,x_1,...,x_n) \in \mathbb{R}^{n+1} \mid x_i \geq 0, \sum_{i = 0}^n x_i = 1 \}$ of $\mathbb{R}^{n+1}$ endowed with the subspace topology. Simplices have their own definition of boundaries and interiors, but little is said about the topological interior and the topological boundary of $\Delta^n \subseteq \mathbb{R}^{n+1}$ (the closure is, of course, $\Delta^n$ itself since it is closed in $\mathbb{R}^{n+1}$ ). Accepted answers to this question (and many similar proofs) implicitly assumes that it is nonempty since a theorem they invoke requires a convex subset of $\mathbb{R}^{n+1}$ to have a nonempty interior. This question and the answer to it suggest that the interior of $\Delta^n$ to be the subset of those $(x_0,...,x_n)$ for which $x_i > 0$ , though I'm not sure whether they are talking about a topological or a simplicial interior. I've also heard that it is empty, but then it would render all proofs I've seens of the fact that $\Delta^n$ is homeomorphic to $D^n$ invalid, which I find unlikely since it appears to be a folk theorem, which rely on the following theorem: Let $K \subseteq \mathbb{R}^n$ be a compact convex subset with nonempty interior. Then, for any interior point $p$ of $K$ , there is a relative homeomorphism $(D^n,S^{n-1}) \to (K,\partial K)$ which maps $0$ to $p$ . About a year ago I tried to understand the issue, to no avail: googling provides little to none information on the matter. This time I hope to get to the bottom of this, and to provide this answer as a reference to all in my situation.",['general-topology']
3905621,Does distinct sets mean disjoint sets?,"If A and B are two distinct sets, does that mean A and B are disjoint? Or if A is the subset of B , can A and B be two distinct sets? I am confused here that whether distint and disjoint sets mean the same? I have to find the intersection of two sets. It could be empty set if these two sets are disjoint. And it could be any other set if they are not.
So please help me understand the difference between distinct and disjoint sets if there is any.","['elementary-set-theory', 'terminology']"
3905629,Finding $\lim_{x \to 0+}(2\sin \sqrt x + \sqrt x \sin \frac{1}{x})^x$,"I need to compute a limit: $$\lim_{x \to 0+}(2\sin \sqrt x + \sqrt x \sin \frac{1}{x})^x$$ I tried to apply the L'H√¥pital rule, but the emerging terms become too complicated and doesn't seem to simplify. $$
\lim_{x \to 0+}(2\sin \sqrt x + \sqrt x \sin \frac{1}{x})^x \\
= \exp (\lim_{x \to 0+} x \ln (2\sin \sqrt x + \sqrt x \sin \frac{1}{x})) \\
= \exp (\lim_{x \to 0+} \frac 
{\ln (2\sin \sqrt x + \sqrt x \sin \frac{1}{x})} 
{\frac 1 x}) \\
= \exp \lim_{x \to 0+} \dfrac 
{\dfrac {\cos \sqrt x} {x} + \dfrac {\sin \dfrac 1 x} {2 \sqrt x} 
- \dfrac {\cos \dfrac 1 x} {x^{3/2}}} 
{- \dfrac {1} {x^2} \left(2\sin \sqrt x + \sqrt x \sin \frac{1}{x} \right)} 
$$ I've calculated several values of this function, and it seems to have a limit of $1$ .","['limits', 'calculus']"
3905639,"""Topological disjoint union"" vs ""disjoint union""","I want to understand the definition of topological disjoint union. Let $X$ be a topological space such that we can write $X=\bigsqcup X_i$ . We say that $X$ is the "" topological disjoint union"" of the $X_i$ if each $X_i$ is an open subset of $X$ , but if the $X_i$ are not necesserily open subsets of $X$ we say that $X$ is a the ""disjoint union"" of the $X_i$ only as sets (without the topological adjective) which means that $X =\cup X_i$ is the union of the $X_i$ such that all the $X_i$ are pairwise disjoint as sets. Is my understanding correct? thank you for your help!","['elementary-set-theory', 'general-topology']"
3905643,How many times do I need to draw $ùëã$ items from a set on $ùëÅ$ items to get a probability $ùëÉ$ that each item is selected at least $ùêæ$ times?,"This and this are related (but slightly different) questions. My question is more general. I have a set of $N$ elements. I want to repeatedly draw $X$ items ( $X \le N$ ) with replacement, i.e. I first choose $X$ items without replacement and then I replace them in the original set. I want each element to be drawn at least $K$ times. So how many times do I have to draw $X$ elements such that I have probability $P$ that each element has been drawn at least $K$ times? Let's do an example. I have a deck of $N=10$ cards, labeled $1, 2, ..., 10$ . At each step I draw $X=3$ cards, I look at them and then I put them back in the deck. How many times do I have to perform such step in order to draw each single card at least $K=5$ times with probability $P=0.95$ ? EDIT : I would be happy also with a solution where $X=1$ .","['statistics', 'combinatorics', 'probability']"
3905662,Conjugacy classes of a nonabelian group of order $p^4$,"Apologies in advance for the long text! I will explain what I have done so far, and then I will present my question. Consider a nonabelian group of order $p^4$ , where $p$ is a prime. Then, the center $Z(G)$ cannot have order $p^3$ or $p^4$ , since both these cases would lead to the contradiction that $G$ is abelian. So we consider: Case I: Order of $Z(G)$ is $p^2$ i.e., $|Z(G)|=p^2$ The class equation is given by $|G|=|Z(G)|+\Sigma|G:C_G(g_i)|$ , where the $g_i$ are representatives of the distinct conjugacy classes of G not contained in the center $Z(G).$ I deduced that for each $g\notin Z(G),$ $|G:C_G(g)|=p.$ So the class equation becomes $p^4=p^2+kp,$ where $k$ is the number of distinct conjugacy classes not contained in the center. This gives $k=p^3-p$ and hence the number of conjugacy classes of $G$ is $p^3+p^2-p$ Case II: $|Z(G)|=p$ I was able to prove that for any $g\notin Z(G),|G:C_G(g)|=p$ or $p^2.$ That means that, a conjugacy class not contained in the center can have size $p$ or $p^2.$ My question is: Is there any way to determine how many conjugacy classes of size $p$ is there, and how many conjugacy classes of size $p^2$ is there? I would really appreciate any help regarding this question. Thank you!","['group-theory', 'abstract-algebra', 'finite-groups', 'p-groups']"
3905686,Existence and uniqueness of homoclinic orbit,"Consider the following 2D autonomous system of ODEs: $$
\left\{
\begin{array}{ll}
\dot{x} = x^2 + 2y - x \\
\dot{y} = 3xy/2 - 3x^2 - y + 2x
\end{array}
\right.
$$ How can we prove the existence and uniqueness of a homoclinic orbit (i.e. a solution $X$ of the system
for which $\lim_{t \to \infty} X(t) = \lim_{t \to -\infty} X(t) = x_0,$ where $x_0$ is an equilibrium point of the system) for the system? It is not hard to determine the equilibrium points: $(0,0), (-3, -6)$ and $(2/3, 1/9)$ . The latter two are asymptotically stable equilibrium points, so there cannot be a homoclinic orbit for those points. However, $(0,0)$ is a saddle equilibrium, so perhaps there is a homoclinic orbit here.
We can also see that the function $H(x,y) = - y^2 + x^2(1-x)$ is constant on the solutions of the system.
The set $H(x,y) = 0$ seems to be comprised of three solutions of the system, one of which is indeed a homoclinic orbit, so this proves existence. However, how do we prove that this is unique? I don't really know how to approach this part.","['ordinary-differential-equations', 'dynamical-systems']"
3905744,Help solving $2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right)$,"I'm at the begginig of a differential equation course, and I'm having trouble finishing this problem. I'm asked to solve the equation $$2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right). \ \ \ \text{(I)}$$ I'm used the variable change $p=y'$ , $\ \dot p=\frac{dp}{dy}$ , $ \ y''=\dot pp$ , I get the equation: $$p=2y\dot p+\frac{1}{2}\log \dot p. \ \ \ \text{(II)}$$ That last one is a Lagrange differential equation. I solved it finding a parametric solution using the variable change $\dot p = u$ , and using that I got the first order linear equation $$\frac{dy}{du}+\frac{2y}{u}=\frac{-1}{2u^2}.$$ Solving it by integrating factor, I finally found the parametric values for $y$ and $p$ , and they are $$
\left\{
\begin{array}{rcl}
     y(u,C)&=&\frac{C}{u^2}-\frac{1}{2u}
  \\ p(u,C)&=&\frac{2C}{u^2}-1+\frac{1}{2}\log u
\end{array}
\right.
$$ This two equations are the solution to the equation (II), but I'm asked to solve the equation (I). Since $p=y'(x)$ in the original equation (I), I integrate the expression of $p$ with respect to $x$ , and I get $$y=\int\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right)\ dx + K$$ $$\Downarrow$$ $$\boxed{y(x)=x\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right) + K}$$ This may be my solution, but something feels wrong about it, since my original $y$ in equation (I) is some $y(x)$ , but what I get is a single equation $y(x,u)$ ( $C,K$ are constants), and I'm not sure if I'm forgetting another expression to make it have sense. I feel that the solution to (I) must be a parametric one, but I only get one equation for $y$ in function of $x$ and $u$ . Is the boxed equation the right solution? If not, where am I wrong? Any help will be appreciated.","['parametric', 'ordinary-differential-equations']"
3905746,Are solvable groups Howson?,"A finitely generated group $G$ has the Howson property if the intersection of any two finitely generated subgroups is again finitely generated. (Finitely generated) free groups, nilpotent, and polycyclic groups all have the Howson property. Are solvable groups Howson? I cannot find a reference to this. I expect the answer to be negative, but can't think of any counterexample at the moment.","['combinatorial-group-theory', 'finitely-generated', 'group-theory', 'infinite-groups']"
3905809,On the proof of the Stochastic Exponential by Protter (Thm 37 of Ch 2),"This is a theorem on the stochastic exponential from Protter's Stochastic Integration and Differential Equations. I have seen some questions about this theorem and the book from the site, but not on these questions which I believe are not so easy to understand as written in the proof. I would greatly appreciate it if anyone who has read through this text or know the subject well could enlighten me on these questions. Here, $X$ is cadlag, adapted, semimartingale and $\Delta X_s = X_s - X_{s-}$ . We define the quadratic covariation process $[X,Y]$ of two semimartingales $X$ and $Y$ as the unique adapted cadlag process with paths of finite variation on compacts that makes $XY - [X,Y]$ a local martingale and can be computed by $$[X,Y] = X_0 Y_0 + \lim_{n \to \infty} \sum_i (X^{T_{i+1}^n} -X^{T_i^n})(Y^{T_{i+1}^n} -Y^{T_i^n})$$ where convergence is uniform on compacts in probability for a sequence of random partitions $\sigma_n$ tending to identity and $\sigma_n$ is the sequence $0=T_0^n \le T_1^n \le \cdots T_{k_n}^n$ with $T_i^n$ stopping times. Tending to identity means that $\lim_n \sup_k T_k^n = \infty$ and $\Vert \sigma_n \Vert = \sup_k |T_{k+1}^n - T_k^n| \to 0$ a.s. Moreover, $[X,Y]^c$ denotes the path by path continuous part of $[X,Y]$ . And a semimartingale is called a pure jump if $[X,X]^c = 0$ . I have attached the theorem and proof at the bottom. Questions: It is written in the proof that clearly the product $$\Pi_{s \le t} (1+\Delta X_s) \exp(-\Delta X_s)$$ is cadlag, adapted. But why is this infinite product cadlag, adapted? Adapted I think follows since this is just a limit of adapted processes, but I cannot see how to show that this is right continuous with left hand limits. Second, the proof shows that since $X$ has cadlag paths, it has only a finite number of $s$ such that $|\Delta X_s| \ge 1/2$ on each compact interval for each fixed $\omega$ , and so it suffices to show that $$V_t = \Pi_{0 < s \le t} (1+\Delta X_s 1_{|\Delta X_s| <1/2}) \exp (-\Delta X_s 1_{|\Delta X_s | <1/2})$$ converges and is of finite variation. Convergence is clear since we need only multiply finite terms to get $Z_t$ . But how do we get that $Z_t$ is also of finite variation if $V_t$ is? In the second part of the proof uses Ito's formula (or the Change of Variables) by setting $K_t = X_t - \frac{1}{2}[X,X]_t^c$ and $S_t = \Pi_{s \le t} (1+\Delta X_s) \exp(-\Delta X_s)$ , and $f(x,y) = ye^x$ and set $Z_t = f(K_t, S_t).$ In going to the second equality in (*), it is claimed that $S$ is a pure jump process and so $[K,S]^c = [S,S]^c = 0$ . But I cannot prove how to show that $S$ is a pure jump process, i.e. its quadratic variation has no continuous part, and why does this give $[K,S]^c=0$ ? In fact, I cannot see where he uses $[K,S]^c = 0$ since the only thing that changes in the two forms is that $\frac{1}{2}\int_{0+}^t Z_{s-} d[K,K]_s^c$ becomes $\frac{1}{2} \int_{0+}^t Z_{s-} d[X,X]_s^c$ , so it seems like we only need that $[K,K]_s^c = [X,X]_s^c$ . Where does $[K,S]^c$ come into play here? Finally, it is claimed that $Z_s = Z_{s-} (1+ \Delta X_s)$ . Why does this hold? I would greatly appreciate some help with these questions.","['stochastic-integrals', 'stochastic-analysis', 'probability-theory', 'stochastic-calculus']"
3905847,The matrix function $\frac{e^x - 1}{x}$ for non-invertible matrices,"I am trying to evaluate the matrix function $$
f(X) = \frac{e^X - I}{X}, \tag{1}
$$ where $e^X$ is the usual matrix exponential, defined by $$
e^X = \sum_{n=0}^{\infty} \frac{1}{n!} X^n.
$$ The problem is that all matrices I am working with are singular (if it is of relevance, $X \in \mathfrak{so}(3)$ ). According to this Wikipedia article , one may consider the function $f$ to be a real function, find its Maclaurin series, and use this to evaluate the function for a matrix. This seems reasonable enough, and it also seems to be how the matrix exponential is defined. The Maclaurin series for $(1)$ is given by $$
f(x) = \sum_{n=0}^{\infty} \frac{X^n}{(n+1)!} = I + \frac{1}{2!}X + \frac{1}{3!}X^2 + \ldots,
$$ indicating that the invertibility of $X$ shouldn't matter when it comes to finding a meaningful value for $(1)$ . Now for my question: Is there a closed form of $(1)$ that do not involve the inverse? To be clear, my goal is to implement the function in Python, and I would rather do it without computing a large partial sum and use that as the approximation. I know a closed form of the matrix exponential for matrices in $\mathfrak{so}(3)$ .","['matrices', 'matrix-exponential', 'lie-algebras']"
3905848,"$f : \mathbb{R} \to \mathbb{R}$ is continuous iff $\{(x,y):y\lt f(x)\}$ and $\{(x,y):y\gt f(x)\}$ are both open sets in $\mathbb R^2$","Given $f: \mathbb{R} \to \mathbb{R}$ define two sets $A$ and $B$ as $ A = \{(x, y) : y < f(x)\}$ and $B = \{ (x, y) : y > f(x)\}$ in $\mathbb{R^2}$ . Show that $f$ is continuous iff  both $A$ and $B$ are open. I'm not sure if this follows from the definition of continuity. Can someone help?","['analysis', 'real-analysis']"
3905871,A question on conditional probability (Cambridge Admissions Exam),"I was solving the following problem and I got to part (b). I thought of approaching this the following way: There are two types of cartons, one with a 60 per cent probability of being selected and the other with 40 per cent. I split it intwo scenarios and I calculated each of the conditional probabilities and then added them together (alongside with the probability of each carton being selected). Here is what I have done: First consider the case where he carton is of skimmed milk. $$P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-\frac{1}{2}}{b}=\frac{2b-1}{2b}$$ Case for full-fat milk $$P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-a}{a}$$ Thus the total probability must be $$=\frac{6}{10} \frac{2b-1}{2b} + \frac{4}{10} \frac{b-a}{a} $$ However, this is wrong according to the markscheme given here: My question: Why is it wrong to consider the different cases separately for part (i)?","['statistics', 'probability']"
3905903,Prove $\sin x\ge x-\frac{x^3}{3!}$,"Prove $$\sin x\ge x-\frac{x^3}{3!}$$ for $x\ge 0$ . I know that a calculus/continuity proof exists, but I am curious if this can be proved without that. Here is a sketch I have made.; we will use for $x\ge 0$ ; $$\sin x\le x\le \tan x$$ proof of this see here I got a weaker bound : $$\tan\left(\frac{x}{2}\right)\ge \frac{x}{2}$$ $$\sin\left(\frac{x}2\right)\cos\left(\frac{x}2\right)\ge \frac{x}{2}\cos^2\left(\frac{x}{2}\right)$$ $$\sin x= x\left[1-\sin^2\left(\frac{x}{2}\right)\right]\ge x\left(1-\frac{x^2}{4}\right)= x-\frac{x^3}{4}$$ Thus $$\sin x \ge x-\frac{x^3}{4}$$ As you can see is there any way I can take this to strengthen the inequality, to get $$\sin x\ge x-\frac{x^3}{6}.$$ Also could this possibly be extended by strengthening inequality further to get the Taylor series. I think I am going too far!","['trigonometry', 'inequality']"
3905908,A reason for $ 64\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt =-\pi^4$ ...,"Question: How to show the relation $$
J:=\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt 
=-\frac 1{64}\pi^4
$$ (using a ""minimal industry"" of relations, possibly remaining inside the real analysis)? So i have found a solution to the problem, it is part of my solution for math.stackexchange.com - questions - 3854736 ,
but not a satisfactory solution. ""There should be more"", explaining why there is a ""clean result"" for the integral. Here, i am not strictly interested in a computational approach. I just want to share this with the community in these days of isolation. Any idea to attack this, or a related integral involving ""three log factors"" is welcome. (Well, the $\arctan$ is a sort of $\log$ in a sense that i don't want to define closer, see below.) Computations may be safely done ""modulo integrals involving two or one log factor"". But an illuminating, short way to show the above formula for $J$ would be wonderful. Motivation: The above relation appeared as i tried to solve the integral posted at the above link: Calculate $\displaystyle\int_0^{2\pi} x^2\; \cos x \cdot\operatorname{Li}_2(\cos x)\; dx$ . After several simplifications and substitutions, it turns out that the above integral is related to integrals of the shape $\int_0^1\log t\; R(t)\; dt$ ,  and $\int_0^1\arctan t\cdot \log t\; R(t)\; dt$ ,  and $\int_0^1\arctan^2 t\cdot \log t\; R(t)\; dt$ , and ""similar"" expressions. Here $R$ is in each case a (rather simple) rational function.
(The more log and/or arctangent factors, the higher the computational complexity.) I could compute more or less algorithmically most of the the needed integrals to solve the linked problem, all of them but the integral $$
K=\int_0^1\arctan^2 t\cdot\log t\cdot\frac2{1-t^2}\; dt\ ,
$$ which turned out to be very hard to attack with the methods of real analysis.
Computing this integral is more or less equivalent to computing $J$ , and the question wants $J$ instead, since we have a ""clean formula"", so that some speculation about a ""clever substitution"" may be accepted. My solution (for $K$ ) works in complex analysis, the first step is to write $$
\int_0^1 =\int_0^i+\int_i^1\ ,
$$ then parametrize the first integral using a linear path, the second one using a path on the unit circle. Some comments: I will say some more words, because the situation is rich in coincidences. Since a numerical evidence is the simplest and shortes way to present (instead of showing how to show), i will use this method to at least list the coincidences. Many equalities below are ""equivalent"" (modulo computation of integrals of lower complexity) to the formula for $J$ . First of all, a numerical experiment using pari/gp delivers some connection between $K$ and a ""cousin"" of $J$ : ? 2 * intnum( t=0, 1, atan(t)^2 * log(t) / (1-t^2) )
  %88 = -0.357038604620289042902893412499686912781214141574556097366337
  ? real(intnum( t=0, I, (pi/4 - atan(t))^2 * log(t) / (1-t^2) ))
  %89 = -0.357038604620289042902893412499686912781214141574556097366337
  ? intnum( t=0, 1, (pi/4 - atan(t))^2 * log(t) / (1-t^2) )
  %90 = -0.357038604620289042902893412499686912781214141574556097366337 In words: $$
\begin{aligned}
K
&=
\int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=
\Re
\int_0^i\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\\
&=
\int_0^1\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\ .
\end{aligned}
$$ Note the integration margins. What happens if we take the integral on $[0,i]$ instead of $[0,1]$ in the $K$ -integral? Numerically: ? 2 * real(intnum( t=0, i, atan(t)^2 * log(t) / (1-t^2) ))
    %98 = 1.52201704740628808181938019826101736327699352613570971392919
    ? pi^4/64
    %99 = 1.52201704740628808181938019826101736327699352613570971392919 In words: $$
\begin{aligned}
K^*
&:=
\Re\int_0^i\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=\frac 1{64}\pi^4
\\
&=
-\int_0^1\left(\frac \pi 4+\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\\
&=-J\ .
\end{aligned}
$$ (These observations were leading to the formula for $K$ in loc. cit. .) One idea is to use partial integration in $J$ or $K$ . Well, we have for $K$ : $$
\begin{aligned}
K 
&=
\int_0^1\arctan^2 t\cdot\log t\;\left(-\log\frac {1-t}{1+t}\right)'\; dt
\\
&=
\underbrace{\int_0^1\arctan^2 t\cdot\frac 1t\cdot \log\frac {1-t}{1+t}\; dt}_{=2K\text{ (why?)}}
\\
&\qquad\qquad+
\underbrace{
\int_0^1
2\arctan t\cdot\frac 1{1+t^2}\cdot \log t\cdot \log\frac {1-t}{1+t}\; dt
}_{=-K\text{ (why?)}}
\ .
\end{aligned}
$$ Note that $\arctan$ is related to the logarithm (over $\Bbb C$ ), we have the relation
(around $0$ ) $$
\arctan t=\frac 1{2i}\log\frac {1+it}{1-it}\ .
$$ The substitution $t=\frac{1-u}{1+u}$ and the formula for $\tan(\arctan 1-\arctan u)$ are giving: $$
\begin{aligned}
K 
&=
\int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=\int_0^1
\left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {1-u}{1+u}\cdot \frac {du}u\ .
\\
&=\int_1^\infty
\left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {u-1}{1+u}\cdot \frac {du}u\ .
\end{aligned}
$$ (Write $\log t=\frac 12\log t^2$ to have the same expression under the integral on $(0,1)$ and on $(1,\infty)$ .) Note the fact that the factor $\frac 2{1-t^2}$ is not ""random"". It is the right one to make things feasible. It is the derivative of $\displaystyle -\log\frac{1-t}{1+t}$ , and plugging in $t=iu$ into $\displaystyle \log\frac{1-t}{1+t}$ leads to an expression related to $\arctan u$ . And conversely, $\arctan(iu)$ is related to such a logarithmic expression in $u$ .","['integration', 'definite-integrals', 'real-analysis', 'complex-analysis', 'polylogarithm']"
3906008,Does power rule holds for time functions of unit quaternions?,"I'm trying to understand quaternion calculus. Let's suppose I have a time function of unit quaternion: $q(t) = (\cos \theta(t), (\sin \theta(t)) \mathbf{a}(t)), \mathbf{a}(t) \in \mathbb{R}^3$ , $||\mathbf{a}(t)|| = 1$ $\forall$ $t$ to the power $f(t) \in \mathbb{R}$ . Would someone clarify if $\dfrac{d\left(q(t)^{f(t)}\right)}{dt}$ $$= \left(‚àí\sin(fŒ∏) \dfrac{d(f\theta)}{dt}, \mathbf{a}\cos(fŒ∏)\dfrac{d(f\theta)}{dt} + \dfrac{d(\mathbf{a})}{dt}\sin(fŒ∏)\right)$$ or $$
= q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \ln q(t)\right] = q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \cdot (0, \theta(t) \mathbf{a}(t)) \right] \\
= (\cos (f\theta), (\sin (f\theta))\mathbf{a}) \times (0, f'\theta\mathbf{a} + f\theta'\mathbf{a} + f\theta\mathbf{a}') $$ or both are correct? (BTW, do we have a general formula for $\mathbf{a} \times \mathbf{a}'$ ?) Thanks a lot.","['derivatives', 'quaternions']"
3906049,"How to calculate inverse function of a ""weird"" function like $y=x+\cos x$","As the title said, how to calculate the inverse function of $y = f(x) = x + \cos x$ ? I found a super function $x = g(y)$ like this: $x = y -\cos(y-\cos(y-\cos(y-cos(...(y-\cos y)...))))$ , with infinite nested loop.
And I use matlab to test it, find that the more nests, the lower approximation error.
So I'm conjecturing if $g(y)$ is the true inverse function of $f(x)$ . If yes, why? And can we reduce the form into some ""simpler"" finite style? If not, what should it be? And also why? ############### Note of changes: Sorry for my poor math knowledge, I cannot find a proper symbol describe what ""..."" exatly means. how about I use programming language? y = a; % a is a constant    
x = y - cos(y);
for i = 1:10000
   x = y - cos(x);
end
disp(x)","['trigonometry', 'functions', 'inverse', 'inverse-function']"
3906101,Two functions with a given difference quotient composition (motivated by the MVT),"We are looking for an example of a function $f:\mathbb{R}\to \mathbb{R}$ such that there exist (at least) two functions $g:\mathbb{R}\to \mathbb{R}$ and some functions $h:\mathbb{R}^2\to \mathbb{R}$ with $\min\{x,y\}<h(x,y)<\max\{x,y\}$ (if $x\neq y$ ) and satisfying the functional equation $$g(h(x,y))= \frac{f(y)-f(x)}{y-x}, \;\;\; x\neq y.$$ Note. If $f$ is differentiable, $f'$ is invertible and $f'^{^{-1}}$ is continuous (e.g., if $f'$ is strictly increasing and continuous), then the mean value theorem (MVT) implies that $g=f'$ is a unique solution of the above functional equation (for some $h$ with the mentioned properties). Therefore, we should look for some examples in the following two cases: Case 1) $f$ is differentiable and $f'$ is not invertible (e.g., $f(t)=t^3$ ) Case 1) $f$ is not differentiable at some points (e.g., $f(t)=|t|, sgn(t)$ ). Our prefer is examples with  explicit formulas. Thanks in advance","['functional-equations', 'functions', 'real-analysis']"
3906150,"In $\Delta ABC,$ side $AC$ and the perpendicular bisector of $BC$ meet at $D$, where $BD$ bisects $\angle ABC$.","In $\Delta ABC,$ side $AC$ and the perpendicular bisector of $BC$ meet at $D$ , where $BD$ bisects $\angle ABC$ . If $CD = 7$ and $[\Delta ABD] = a\sqrt{5}$ , find $a$ . What I Tried : Here is a picture:- Let the perpendicular bisector of $BC$ pass through $BC$ at $E$ . Then I first noticed that $\Delta BDE \cong \Delta CDE$ from $(SAS)$ congruency. This gives the required information in the diagram, as well as we have $BD = 7$ . Now :- $$\Delta ABD \sim \Delta ACB$$ $$\rightarrow \frac{AD}{AB} = \frac{7}{BC} = \frac{AB}{AC}$$ So let $AD = k$ , $AB = m$ , $BE = EC = n$ . We have :- $$\frac{k}{m} = \frac{7}{2n} = \frac{m}{(7+k)}$$","['congruences-geometry', 'triangles', 'problem-solving', 'geometry']"
3906169,Integrating a step function using antiderivatives,"Let $$ f(x) = 
  \begin{cases}\begin{align}
    1\quad&\text{ if }\; 0\leq x \leq 1 \\
    2 \quad&\text{ if }\; 1<x \leq 2 \\
  \end{align}\end{cases}$$ Then $\int^2_0 f(x)dx=3$ and an anti derivative of f(x) is $$F(x)=\begin{cases}\begin{align}
    x\quad&\text{ if }\; 0\leq x \leq 1 \\
    2x \quad&\text{ if }\; 1<x \leq 2 \\
  \end{align}\end{cases}$$
But, $F(2)-F(0)=4-0=4 \neq 3.$ Why has this happened? Find an anti-derivative $G(x)$ of $f(x)$ such that $G(2)-G(0)=3$, the correct answer. My attempt:
$F(2)-F(0)=4 \neq 3$ because we need to break it up into two parts like this: $\int^2_1 2x dx + \int^1_0 x dx = x^2|^2_0 + \frac{1}{2} x^2|^1_0 =4-1+\frac{1}{2}-0=3.5$ but this is not right either. :( For the ""Find an anti-derivative $G(x)$...""I'm completely lost but perhaps if we solve the first part then I'll understand what it's asking for there. Any help would be appreciated.",['integration']
3906193,"Completely stuck at integrating $\int x\sqrt{\cos x}\,dx$","I am currently learning Calculus and I just started studying Integrals. I want to find the general indefinite integral $$\int x\sqrt{\cos x}\,dx$$ I have tried using some substitution such as letting $u=\cos x$ or $u=\sqrt x$ , etc. but it seems like none of them is effective here. I have also tried using online integral calculators to solve this question, but all I got is something like ""steps are currently not supported for this problem."" Since even an integral calculator couldn't handle this, I wondered if this question can be solved. Therefore I came back to MathSE to seek help. Is it solvable? If so, then how? Thanks in advance.","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
3906297,"If $n,m \in \mathbb{N}$ then there are $c,d$ such that $cd = (m,n)$, $(c,d) = 1$ and $(m/c,n/d) = 1$.","Suppose that $m,n \in \mathbb{N}$ . Using the fundamental theorem of arithmetic it is easy to show that there exist $c,d \in \mathbb{N}$ such that $(c,d) = 1$ , $cd = (m,n)$ and $(\frac{m}{c},\frac{n}{d}) = 1$ . Is there any quick way to prove this without using the prime factorizations of $m$ and $n$ , i.e. only the basic properties of the gcd, lcm, etc.?","['elementary-number-theory', 'gcd-and-lcm']"
3906337,Distance between two circles with known size and intersection area,"Caution - biologist at work I am trying to plot some circles and want to work out how far apart my circles should be with a target in mind. The ditance between the centre points of the two circles is $d$ . The first circle has a radius of $r_1$ = 5 cm and the second has a radius of $r_2$ = 4 cm. The area of these circles is 78.54 cm $^2$ and 50.27 cm $^2$ respectively, making the joint area $a_{1,2}$ = 128.81 cm $^2$ . I want 30% of that area, $0.3 \times 128.81$ cm $^2$ to be intersection area (area shared by the two circles). Therefore I have a known pair of radii and a known intersection area, and an unknown distance. By trial and error with various values of $d$ in the equation $$ a_{1,2} = r_1^2 cos^{-1} \frac{d^2 + r_1^2 - r_2^2}{2dr_1} + r_2^2 cos^{-1} \frac{d^2 + r_2^2 - r_1^2}{2dr_2} - \frac{1}{2} \sqrt{(-d+r_1+r_2)(d+r_1-r_2)(d-r_1+r_2)(d+r_1+r_2)} $$ I find that, to get 30% of the total area into the intersection area, when the radii are 5 and 4, $d$ needs to be approximately 2.71. However, is there a way to solve $d$ from known radii and intersection area? I've been searching for hours and come up short - the internet is dominated by solutions to find the intersection are from known radii and distance. FYI, I'm programming this in R - bonus kudos for answers providing accompanying script! r1 <- 5
r2 <- 4
a1 <- pi*r1^2
a2 <- pi*r2^2
aJ <- 0.3 * (a1+a2)

intersectionArea <- function(r1, r2, d){
  (r1^2 * acos((d^2 + r1^2 - r2^2)/(2*d*r1))) +
  (r2^2 * acos((d^2 + r2^2 - r1^2)/(2*d*r2))) -
  (0.5 * sqrt((-d+r1+r2)*(d+r1-r2)*(d-r1+r2)*(d+r1+r2)))
}

aJ/intersectionArea(r1,r2, 2.709)","['area', 'geometry']"
3906356,A question about Lebesgue measure and integration.,"Let $(X,A,\mu)$ be a measurable space. $f\colon X\to [0,\infty]$ measurable. Denote $S=\lbrace x\in X : f(x)<1\rbrace$ . Prove a. $\mu(S)=\lim_{n\to \infty} \int_S e^{-f^n}\,d\mu$ b. Assume $X=S$ , prove that: $$\sum_{n=1}^{\infty} \int_X f^n\, d\mu= \int_{X} \frac{f}{1-f}\,d\mu$$ I have difficulty where I'm supposed to move between integrals.
We can define the measure in terms of integral. So in a I know that it is the start but it is not clear to me how to connect between the set $S$ to the integral. In b, this is what I did: $\sum_{n=1}^{\infty} \int_X f^n \,d\mu$ = (by a theory int of sum =sum of int for a sequence of positive measurable functions)
= $\int_X (\sum_{n=1}^{\infty}f^n)\, d\mu$ =(sum of a geometry column with $f(x)<1$ )= $\int_X \frac{f}{1-f}\, d\mu$ .","['measure-theory', 'real-analysis']"
3906360,What does $\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}}$ evaluate to?,"Problem: Suppose that $f$ is twice continuously differentiable within a
neighborhood of the point $a$ and $f''(a) = 4$ . Compute \begin{align*}
 \lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} \end{align*} I have done this work \begin{align*}
\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} &= \lim_{x \to a} \frac{\frac{f(x)-f(a)}{x-a} - f'(a)}{x-a} \\
&= \lim_{x\to a}\frac{f'(a)-f'(a)}{x-a} \\
&= 0
\end{align*} This makes sense, but I am not confident in my answer because I didn't use the given that $f''(a) = 4$ . Any help would be greatly appreciated.","['calculus', 'derivatives', 'real-analysis']"
3906387,Check if an integer is present in a linear recurrence,"Given the following recurrence relation : $f(n) = 5f(n-1) - 2f(n-2)$ where $f(0) = 0, f(1) = 1$ I need to find out if an integer $F_n$ is present in the sequence in $O(1)$ time and space. Solving the equation, there are two distinct real roots. $\phi = \frac{5 + \sqrt17}2$ $\psi = \frac{5 - \sqrt17}2$ Therefore, $F_n = \frac{\phi^n - \psi^n}{\sqrt17}$ Similar to Binet's rearranged formula, I want to solve for $n$ in terms of $F_n$ . Since, $\psi = \frac{2}{\phi}$ $\sqrt17F_n = \phi^n - \frac{2^n}{\phi^n}$ $Or,$ $\phi^{2n} - \sqrt17F_n\phi^n-2^n = 0$ Here I'm not able to find out a solution to express $n$ purely in terms of $F_n$ so that I can calculate the perfect square just like in Binet's formula.","['fibonacci-numbers', 'recurrence-relations', 'discrete-mathematics']"
3906397,Existence of a neutral element in set with an associative internal law,"Let $E$ be a set with an associative internal law $\star$ (i. e. such that $\star : (x, y) \in E^2 \mapsto x \star y \in E$ and for all $(x, y, z) \in E^3, (x \star y) \star z = x \star (y \star z)$ ). We assume that there exists $a \in E$ such that: $$
\forall y \in E, \exists x \in E : y = a \star x \star a
$$ I have to show that $E$ admits a neutral element $e$ , which means I have to show: $$
\exists e \in E, \forall x \in E : x \star e = e \star x = x
$$ When I look for a candidate value of $e$ , I don't find a way to solve this. Any help is welcome.","['group-theory', 'semigroups', 'associativity']"
3906416,genus$=2$ implies Hyperelliptic.,"In the book of Rick Miranda ( Algebraic Curves and Riemann Surfaces ), in Proposition 1.10 of Chapter VII (p. 198), the claim is that every compact Riemann Surfaces of genus $2$ is hyperelliptic. The proof is: Let $K_X$ the canonical divisor has degree $2g-2=2$ , and by Riemann-Roch Theorem we have $\dim L(K_X)=2$ , therefore we may suppose that $K_X>0$ , take $f\in L(K_X)$ nonconstant so when you look as a map to $\overline{\mathbb{C}}$ has degree $=2$ so it's Hyperelliptic. It's a good proof but a just don't understand why we can suppose $K>0$ . I'm tried write $K_X=P-N$ where $P,N\in \operatorname{Div}(X)$ are non-negative divisors and use Riemann-Roch but I failed, thanks for any comments.","['riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry']"
3906437,What mathematics is behind those metal knot puzzles/brainteasers?,"You know those puzzles which are basically two thick twisted rods that look impossible to pull apart. Here is a video, https://www.youtube.com/watch?v=U5R2GksBqmc There is a solution where you can pull them apart smoothly, without force. Can you approach the solution to these puzzles mathematically. Is there any theorems/methods you can apply when solving these to help? What field of mathematics explains these puzzles and their solutions? If you can please recommend a book, ressource, lecture notes or course which can teach these fundamentals, or even specifically which goes into the math behind these puzzles. They provide endless hopeless fun!:). Right now I basically approach these by trial and error but i'm hoping I can apply more methodical thinking here. My mind is blank when I get to these lol.","['recreational-mathematics', 'geometric-topology', 'knot-theory', 'general-topology', 'problem-solving']"
3906610,"If a measure $\mu$ arises from the restriction of an outer measure $\mu^{*}$, is it true that $\mu$ is saturated?","A measure $\mu$ on a measurable space $(X, \mathcal{M})$ is called saturated if given a subset $E$ of $X$ the condition $E \cap A \in \mathcal{M}$ for every $A \in \mathcal{M}$ with $\mu(A) < \infty$ implies that $E \in \mathcal{M}$ (that is if any locally measurable subset of $X$ is measurable). The Wikipedia article on saturated measures states that ""measures arising as the restriction of outer measures are saturated"". I know how to prove this result when the outer measure itself arises from a pre-measure (a nice proof can be found in the answer to this question ), but not every outer measure is generated by a pre-measure so I'd like to know if the previous assertion is actually true and, if it is, how can I prove the result in the general case, that is how can I show that any measure $\mu$ resulting from the restriction of an outer measure $\mu^{*}$ is saturated? I wasn't really sure about the validity of the result so I tried to construct a counterexample by considering the set $X = \{0,1\}$ and the outer measure $\mu^{*} : \mathcal{P}(X) \rightarrow [0, \infty]$ given by $\mu^{*}(\emptyset)=0$ , $\mu^{*}(\{0\}) = 2$ , $\mu^{*}(\{ 1 \})=2$ and $\mu^{*}(X)=3$ which is not generated by a pre-measure and has $\emptyset$ and $X$ as the only two $\mu^{*}$ -measurable subsets of $X$ . The problem is that the measure induced by this outer measure $\mu^{*}$ is saturated (since $\mu^{*}(X) is finite), so if a counterexample does exist it would be nice to know it. If every measure obtained by the restriction of an outer measure is actually saturated, I'd like to know a proof of this result and my attempt was to consider the measure space $(X, \mathcal{M}^{*}, \overline{\mu})$ where $\mathcal{M}^{*}$ is the $\sigma$ -algebra on $X$ consisting of the $\mu^{*}$ -measurable subsets of $X$ and $\overline{\mu}$ is the restriction of the outer measure $\mu^{*}$ to $\mathcal{M}^{*}$ and then considering the outer measure, say $\mu^{+}$ , induced by the measure $\overline{\mu}$ . Since the outer measure $\mu^{+}$ is induced by the measure $\overline{\mu}$ , we know that the measure obtained by restricting $\mu^{+}$ to the collection of $\mu^{+}$ -measurable subsets of $X$ , say $\hat{\mu}$ , is a saturated measure and I believe I would be able to finish the proof if $\mathcal{M}^{*}$ is equal to the $\sigma$ -algebra of $\mu^{+}$ -measurable subsets of $X$ and $\mu^{+}= \mu^{*}$ (but I think this is only true if the original outer measure $\mu^{*}$ is induced by a pre-measure which leads us back to the original problem of proving the desired result when we drop this assumption). Any hints or ideas would be greatly appreciated and thank you all in advance for your answers.","['measure-theory', 'outer-measure']"
3906612,Section of blowup map of schemes,"Let $X$ be a scheme. Let $\mathcal{I} \subseteq \mathcal{O}_X$ be a quasi-coherent sheaf of ideals on $X$ , and let $Z \subseteq X$ be the closed subscheme corresponding to $\mathcal{I}$ . Let $X' := \operatorname{Bl}_Z(X)$ be the blowup, and let $b\colon X' \to X$ be the blowup morphism. Suppose there exists a section of $b$ , that is, a morphism $s\colon X \to X'$ such that $b \circ s = \operatorname{id}_X$ . Is it necessarily true that $b$ is an isomorphism? (This is a generalization of this question that was asked in the comments; I decided to make this a separate question since it goes beyond the scope of the original question. I initially thought I only had a partial answer, but in the process of writing it up, I realized it works in full generality, so I've provided an answer myself.)","['algebraic-geometry', 'blowup', 'schemes']"
3906685,$MSE(Œ∏_1) = MSE(Œ∏_2) = 0.06$?,"We offer two estimators for the average concentration $Œº$ of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator $Œ∏_1$ has a bias equal to 0.2 and a variance of 0.02. The second estimator $Œ∏_2$ is unbiased and has a variance equal to 0.06. Which of the following is true? $Œ∏_1$ is better than $Œ∏_2$ to estimate $Œº$ . $Œ∏_2$ is better than $Œ∏_1$ to estimate $Œº$ . None of the above is true I have some difficulties to find the right one. If I use the mean square error (MSE), then $MSE(Œ∏_1) = MSE(Œ∏_2) = 0.06$ . So does the the choice 3 is the right one?","['statistics', 'parameter-estimation', 'estimation']"
3906718,Recursive function of $n^2$?,"How would you convert $n^2$ into a recursive function? Like for example, I can say the recursive function of $2^n$ is $2 \cdot 2^{n-1}$ , and it can be applied recursively since it requires the previous value.",['discrete-mathematics']
3906747,The Unit Circle over $\mathbb R$ is not a UFD,"This is exercise 14.2 N in Vakil, self-study. Similar questions have been asked a few times on this site, but none of the answers use a method that I believe Vakil intended: here , here , and here . We are to show $\mathbb R[x, y]/(x^2 + y^2 -1)$ is not a UFD, but over $\mathbb C$ , it is, using exercise 14.2 L, which says, among other things, that $\mathbb P^n - Y$ is not the spectrum of a UFD if $Y$ is a hypersurface of degree $d > 1$ . The issue is that I don't see how to write the unit circle as the complement of a degree $d > 1$ hypersurface in a projective space, if indeed that is what we are supposed to do, nor do I see how, supposing we have done this, the result will change over $\mathbb C$ , since presumably 14.2 L still applies.",['algebraic-geometry']
3906767,Probability of a deviation when Jensen‚Äôs inequality is almost tight,"Let $X>0$ be a random variable. Suppose that we knew that for some $\epsilon \geq 0$ , \begin{eqnarray}
\log(E[X]) \leq E[\log(X)] + \epsilon 
\tag{1} \label{eq:primary}
\end{eqnarray} The question is: if $\epsilon$ is small, can we find a good bound for \begin{eqnarray*}
P\left( \log(X) > E[\log(X)] + \eta \right)
\end{eqnarray*} for a given $\eta > 0$ . One bound can be obtained this way: \begin{eqnarray*}
P\left( \log(X) > E[\log(X)] + \eta \right) &=&  P\left( X > \exp(E[\log(X)] + \eta) \right) \\
& \leq & E[X] / \exp(E[\log(X)] + \eta)  \\
& = & \exp( \log E[X] - E[\log(X)] - \eta ) \\
& \leq & \exp(\epsilon - \eta)
\end{eqnarray*} where the first inequality follows from Markov‚Äôs inequality. This seems like a good bound due to the exponential decay with $\eta$ , but upon closer examination it appears that it can be significantly improved. If we have $\epsilon = 0$ , then this bounds gives \begin{eqnarray}
P\left( \log(X) > E[\log(X)] + \eta \right) & \leq & \exp(-\eta)
\tag{2} \label{eq:good_but_not_best}
\end{eqnarray} However, from Jensen's inequality applied to (\ref{eq:primary}) with $\epsilon = 0$ we obtain $\log(E[X]) = E[\log(X)]$ and therefore $X$ is a constant almost everywhere. As a consequence, for any $\eta>0$ , \begin{eqnarray*}
P\left( \log(X) > E[\log(X)] + \eta \right) = 0.
\end{eqnarray*} which is (of course) infinitely better than (\ref{eq:good_but_not_best}). It would appear that a better bound should decay to zero as $\epsilon$ decays, and ideally preserve the exponential decay with $\eta$ . Any suggestions?","['logarithms', 'jensen-inequality', 'information-theory', 'convex-analysis', 'probability-theory']"
3906810,Conjecturing when $M$ is good,"We say an integer $M>1$ is good if whenever $n^n \equiv 1 \mod M$ then we also have $n \equiv 1 \mod M$ and bad otherwise, for any integer $n\ge 2$ . Prove that  all odd $M$ are bad. Find all good $M$ . My progress: First taking example, we get Among $M \in \{2,3,4,5\}$ $  , 2,4$ are good . Now for all odd $M$ , consider $(M-1)^{M-1}$ , note that $(M-1)^{M-1}\equiv -1^{M-1}\equiv 1 \mod M-1$ , since $M-1$ is even .But $M-1 \equiv -1 \mod M $ . This proves the first part. For second part I thought that it would be true for all even M, but then $9^9\equiv 1 \mod 14$ . However I got $2,4,6,8,10,12$ good . Also $M=42$ is good too . Here's the proof showing $M=42$ is good : If $n^n\equiv 1\mod 42 \implies n^n\equiv 1\mod 2 \implies n$ is odd . Also we have $n^n\equiv 1\mod 6 \implies n\equiv 1\mod 6$ . Now for $n^n \equiv 1 \mod 7 \implies n \equiv \text{1 or 6} \mod 7 $ . If $n=6 \mod 7 $ , then by CRT $n\equiv 13 \mod 42$ which is not possible as $13^{13} \equiv 13 \mod 42 $ ( Thanks! @Ross Millikan for correction ) I couldn't formulate a conjecture about when $M$ is $good$ . Any hints? Thanks in advance!","['contest-math', 'number-theory', 'group-theory', 'elementary-number-theory']"
3906835,"What's the sign of $\det\left(\sqrt{i^2+j^2}\right)_{1\le i,j\le n}$?",Suppose $A=(a_{ij})$ is a $n√ón$ matrix by $a_{ij}=\sqrt{i^2+j^2}$ . I have tried to check its sign by matlab. l find that the determinant is positive when n is odd and negative when n is even. How to prove itÔºü,"['determinant', 'linear-algebra']"
3906920,Binomial Identity and Counting,"A string in $\{0, 1\}*$ has even parity if the symbol $1$ occurs in the word an even number of times; otherwise, it has odd parity. (a) How many words of length $n$ have even parity? (b) How many words of length $n$ have odd parity? It seems to me that correct approach will be to use summations of combinations, and correct answer will yield $2^{n-1}$ in both cases. Is there any way to prove separately that each of them is equal to $2^{n-1}$ ? Would be happy to know your ideas!","['functions', 'combinatorics', 'binomial-theorem']"
3906932,Grothendieck point of view of algebraic geometry,"Given a ring $R$ and $I\subseteq R[x_1,\dots ,x_n]$ an ideal, define the functor $V_R(I):\operatorname{Alg}_R\to \operatorname{Sets}$ , that sends a $R$ -algebra $A$ in the subset of points $\mathbf a \in A^n$ such that $f(\mathbf a)=0\ $ for all $f\in I$ . Recall that in the case $R=k$ an algebraically closed field, $V_k(I)(k)$ is in bijective correspondence with the maximal ideals of $k[x_1,\dots ,x_n]$ . Now, in the course, we talked about the geometric points of a ring $A$ , defining them as the equivalence classes of ring homomorphisms $A\to K$ , where $K$ is a field (not fixed). The motivation that we were given is that this is a generalization of the construction above, since in this way we can obtain a correspondence between homomorphisms and prime ideals, not only maximal ideals; moreover, one can consider any ring, not necessarily a $k$ -algebra. However, I don't understand in what sense this is a generalization: it seems to me that they are just different constructions. For example, even if I take $k[x_1,\dots ,x_n]$ , $k$ an algebraically closed field, in order to capture all its spectrum I need to consider not only the homomorphisms in $k$ , but also in some trascendental extension $k\subset K$ ; so we are not generalizing, we are just adding something in my opinion, because these two constructions (i.e. homomorphisms in $k$ fixed and homomorphisms in any field) don't reduce to the same when considering the case of finitely generated $k$ -algebras. Am I right? Moreover, I have not clear the sense of arriving to the prime ideals, but not to any ideal (i.e. why geometric points of a ring $A$ are not defined as the classes of homomorphisms from $A$ to any ring). Thanks for any clarify, I know that these question may sound stupid but I'm quite new to this things.","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'abstract-algebra', 'schemes']"
3906934,Evaluating real integral using complex analysis. [duplicate],"This question already has answers here : $\int_0^\infty \frac{\sqrt x}{1+x^4} dx$ by residues (3 answers) Closed 2 months ago . I'm trying to compute the following integral: $$\int_0^{\infty}\frac{\sqrt{x}}{1+x^4}dx$$ I'll not write down everything I've done, but choosing the branch cut on the positive real axes we have that: $$\int_0^{\infty}\frac{\sqrt{x}}{1+x^4}dx=\pi i \sum_{z_i}Res(f,z_i) \qquad z_i\in\{\pm \sqrt{i},\pm\sqrt{-i}\}$$ So we have to compute four residues.
My thought was changing the branch cut by putting it on the negative imaginary axes. We can do it by choosing $arg(z) \in (-\frac{\pi}{2},\frac{3\pi}{2}]$ . So we have that: $$(1+i)\int_0^{\infty}\frac{\sqrt{x}}{1+x^4}dx=2\pi i \sum_{z_i}Res(f,z_i) \qquad z_i\in\{e^{i\frac{\pi}{4}},e^{i\frac{3\pi}{4}}\}$$ By doing this, we now need to compute only two residues. But I'm really finding difficulties in computing those residues: in fact I can't obtain the result I'm expecting. Can you please show me the computation and tell me if my argument was clear and correct? Thanks in advance.","['complex-analysis', 'contour-integration', 'residue-calculus']"
3906952,"If $z\in\mathbb C$ with $|z|\leqslant\frac{4}{5}$, then $\sum_{n\in S}z^n\neq-\frac{20}{9}$","Let $z$ be a complex number with $|z|\le\tfrac{4}{5}$ . If $S\subset\mathbb N^+$ is a finite set, then I'd like to show  that $$\sum_{n\in S}z^n\neq-\frac{20}{9}\,.$$ This problem is from an exam in Bejing as of today, and I find this is a very nice problem. I have no idea how to access it, maybe use Newton's identity to  solve it. Could someone help me to solve it?","['contest-math', 'algebra-precalculus', 'summation', 'complex-numbers']"
3906961,27 lines on a cubic surface - which proof is better?,There is a famous fact that any smooth cubic surface has exactly 27 lines. I am a undergraduate students who want to view some detailed proofs of this result. I know there are several different approaches to it. Can anyone recommend me a proof which is both beautiful and useful for my future math study?,"['algebraic-curves', 'projective-geometry', 'algebraic-geometry', 'combinatorics', 'intersection-theory']"
3906976,"A differentiable function whose derivative is $0$ almost everywhere, but not everywhere","In another thread, I remarked that if $$\int_0^1 \vert f'(x)\vert\mathrm dx=0,$$ then $f'(x)=0$ for all $x\in(0,1)$ can only be concluded if $f'$ is continuous. This would certainly be correct if we were talking about a general integrable function $(0,1)\to\mathbb R$ . Take the characteristic function of a singleton set as a counterexample where the integral vanishes, but the function doesn't. But such a function is not a valid derivative of any function, since it doesn't have the mean value property. And I couldn't find any functions where the integral of the absolute value vanishes and which have the mean value property. That's why I'm wondering: Is there a differentiable function $f:(0,1)\to\mathbb R$ such that $f'$ is not identically $0$ , but $$\int_0^1\vert f'(x)\vert\mathrm dx=0,$$ or equivalently, $f'$ is $0$ almost everywhere, but not everywhere.","['measure-theory', 'real-analysis']"
3907004,"Calculus: Isn't the velocity & tangent at a critical point = 0, and therefore a function is not increasing at that point?","I encountered a question from Khan Acad that asked, at what intervals of x does the function increase. My intuition is that all ranges except for 0 and 1, because tangents at those points are flat per green graph below.
Backed up by the red graph, showing h'(1)=0 & h'(0)=0, zero velocity. I need help figuring out my knowledge gap: Am I misinterpreting what ""increasing"" here? Am I missing the point of what a critical point is..?","['calculus', 'derivatives', 'ordinary-differential-equations']"
3907160,The Birthday Paradox with combinatorics,"Assume a year has $365$ days, how many are required to have a $50%$ chance of $2$ people having the same birthday? According to Scientific American , there are $23$ people needed to achieve the goal. $${\begin{pmatrix} 23 \\ 2\end{pmatrix}}=253$$ $$1-(1-\frac{1}{365})^{253}\approx 0.50048$$ However, I have a different approach but I'm not sure if this is correct. One could be any day in a year. And $23$ people would be $365^{23}$ possibilities. Suppose no one in $23$ people has the same birthday.
That would be :- ${\begin{pmatrix} 365 \\ 23 \end{pmatrix}} \cdot23!$ The possibility of having at least 2 people having the same birthday is :- $$1-\frac{{\begin{pmatrix} 365 \\ 23 \end{pmatrix}} \cdot23!}{365^{23}}\approx0.5072972$$ Although $0.50048\approx 0.5072972$ , these two numbers are not equal to each other. Can anyone explain this difference and the reason behind this phenomenon?","['combinatorics', 'birthday', 'probability']"
3907194,Stability of differential equations under nonlinear pertubations,"I am trying to understand the proof of the following theorem : Let $A$ be an $n\times n$ matrix such that it has only negative eigenvalues and let $f: \mathbb{R}\times \mathbb{R}^n$ continuous and locally Lipschits in $x$ . If $f(t,0)=0$ for every $t\in \mathbb{R}$ and $\lim_{x\rightarrow 0}\frac{||f(t,x)||}{||x||}=0$ for any $t\in \mathbb{R}$ then the zero solution of the equation $x'=Ax+f(t,x)$ is asymptotically stable . Now in the proof of this, the authors claim that given $x_0\in \mathbb{R}^n$ with $||x_0||<\delta$ the solution satisfies $||x(t)||<\delta $ for any $t\in [t_0,t_1]$ sufficiently small. And then they use that fact to prove that if $||x_0||<\frac{\delta}{M}$ then $||x(t)||<\delta$ using the following \begin{align}  
x(t)&=e^{A(t-t_0)}x_0+\int_{t_0}^t e^{A(t-s)}f(s,x(s))ds
\\
\|x(t)\|&\leq Me^{-\alpha(t-t_0)}\|x_0\|+\int_{t_0}^tMe^{-\alpha(t-s)}\epsilon \|x(s)\|ds
\\
e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}\|x_0\|+\int_{t_0}^t M\epsilon e^{\alpha s}\|x(s)\|ds
\\
e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}e^{M\epsilon (t-t_0)}
\\[.5em]
\|x(t)\|&\leq Me^{(-\alpha +M\epsilon) (t-t_0)}\|x_0\|
\end{align} I am just a bit confused why we are doing this for $||x_0||<\frac{\delta}{M}$ since we know that if $||x_0||<\delta$ then $||x(t)||<\delta$ . I thought this might be an attempt to enlarge the interval where we have that $||x(t)||<\delta$ , to use the fact that $|f(s,x(s)|<\epsilon||x(t)||$ we need the fact that $||x(t)||<\delta$ so we stay in the same interval.
Can anyone help me figure out why we care that $||x_0||>\frac{\delta}{M}$ ? Thanks in advance.","['stability-in-odes', 'ordinary-differential-equations']"
3907213,What is 'Operator valued function is holomorphic' ? Does Neumann series imply holomorphic?,"Let $H_1$ and $H_2$ be Hilbert spaces and $K(w):H_1 \rightarrow H_2$ be an operator for given $w\in\Omega\subset\mathbb{C}$ .(like a resolvent. It would not be resolvent.) In complex analysis, a function $f:\Omega \rightarrow \mathbb{C}$ is holomorphic if $$f'(w_0):=\lim_{w\rightarrow w_0} \frac{f(w)-f(w_0)}{w-w_0}$$ exists for every $w_0\in\Omega$ . And its limit takes modulus.
However, since an operator valued function $w \mapsto K(w) $ is not complex value, $\left|\frac{K(w)-K(w_0)}{w-w_0}\right|$ cannot be defined.
So, I can GUESS its limit is defined in operator norm, i.e. if $K'(w_0)$ is defined such that $$\left\|\frac{K(w)-K(w_0)}{w-w_0} - K'(w_0)\right\|_{H_1 \rightarrow H_2} \rightarrow 0 \quad as \quad w \rightarrow w_0,$$ $K(w_0)$ is holomorphic for $w_0 \in \Omega$ . In my book, there is no mention about this detail. By the way, consider the Neumann series, $$(I-K(w))^{-1} = \sum_k^\infty (K(w))^k.$$ In order to show $(I-K(w))^{-1}$ is holomorphic, I can think about 'analytic' first (since analytic and holomorphic is equivalent in $\mathbb{C}$ ) and Neumann series say ' $K(w)$ is analytic for each $w$ '. But, again,  it is not complex number. Questions What is definition of 'Operator valued function is holomorphic' ? Is my guess right ? Does it suffice to show existence of its Neumann series ? If then, why ?","['complex-analysis', 'functional-analysis']"
3907304,An extension of Baire's category theorem,"In a topological space, a set is said to be rare if its closure has empty interior, and a set is said to be meager if it is a countable union of rare sets. If meager sets all have empty interior, then the topological space is said to be a Baire space. The following fact is known as Baire's category theorem. Theorem. Complete metric spaces and locally compact Hausdorff spaces are Baire spaces. [ZƒÉlinescu 2002] provides the following extension of this theorem for complete metric spaces, attributing it to C. Ursescu . Theorem. ([ZƒÉlinescu 2002, Theorem 1.4.5]) Let $X$ be a complete metric space, and $\{S_n\}$ be a sequence of open sets in $X$ . Then $\mathrm{cl}(\cap_{n=1}^\infty S_n)$ and $\cap_{n=1}^\infty \mathrm{cl}(S_n)$ have the same interior. It is easy to check that a topological space is a Baire space if and only if any countable intersection of dense open sets is still dense in this space. Consequently, the theorem above implies that complete metric spaces are Baire spaces. Thus it is considered as an extension. Question. Does the extension hold for locally compact Hausdorff spaces? Update: As pointed out by @Alex Kruckman, the property in Ursescu's theorem is indeed equivalent to the fact that $X$ is a Baire space. Theorem. A topological space is a Baire space if and only if $\mathrm{cl}(\cap_{n=1}^\infty S_n)$ and $\cap_{n=1}^\infty \mathrm{cl}(S_n)$ have the same interior for any sequence of open sets $\{S_n\}$ . Recall that a space is a Baire space if and only if any countable intersection of dense open sets is still dense. The theorem is essentially another way to state that a space is a Baire space if and only if all its open subspaces are Baire spaces. Combined with Baire's category theorem, this observation by Alex proves the desired theorem. My answer below proves it from scratch, the proof being essentially the same as that of Baire's category theorem for locally compact Hausdorff spaces. I will accept Alex's answer a few days later if nobody else has anything to add. Thanks.","['general-topology', 'baire-category', 'functional-analysis', 'real-analysis']"
3907316,"Can we know the equation of line "" $L$ "" from one point $P(a,b)$ in this line?","Suppose we have a point $ p(a,b)$ a and b are given and let $L$ be a line s.t $p(a,b) $ belong to $L$ By this information can we know the equation of L in terms of $P(a.b)$ ?can it gives as more information about $ L$ ?","['analytic-geometry', 'curves', 'calculus', 'geometry']"
3907446,Weak convergence implies almost everywhere?,"Suppose that $L^2(R^n;dx) \ni f_n\geq 0$ and $\int f_ng\to 0$ for all continuous function $g$ with compact  support. Then, does $f_n$ convergence to $0$ almost everywhere? Intuitively, it convergence to $0$ by taking function $g$ s.t., $g=1$ around $x$ for almost every $x$ . However I cannot prove.","['convergence-divergence', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
3907498,Partial derivative of a two argument function,"What I have to do is to get a value of partial derivative at a $(0 , 0)$ point in a following function $\\ f(x,y) = \sqrt[3]{xy} \\ \\$ What I struggle with is that I get different results while using different methods. When I use definition I get $\frac{\partial f}{\partial x}(0, 0) =  \lim_{x \to 0} \frac{f(x, 0) - f(0, 0)}{x} = 0\\ \\$ , however I can clearly see that $\frac{\partial f}{\partial x} = \frac{y^{\frac{1}{3}}}{3x^\frac{2}{3}}$ but then x can't be equal to $0$ so the derivative shouldn't have any value at a $(0, 0)$ point. Can someone point out my mistake? Which method is right?","['partial-derivative', 'limits', 'derivatives']"
3907535,Proposition 6.6 in Hartshorne II: Bijectivity,"Proposition 6.6 in Hartshorne II seeks to show that $\operatorname{Cl} (X \times \mathbb A^1) \simeq \operatorname{Cl} X$ for $X$ Noetherian, integral, separated, and regular in codimension 1. A point of codimension 1 in $X \times \mathbb A^1$ is called type I if its image $y$ under the projection to $X$ is codimension 1. We can see such an $x$ is the generic point of the fiber of $y$ . A point of codimension 1 in $X \times \mathbb A^1$ is called type II if its image is the generic point of $X$ . I understand on a technical level, thanks to other questions on this site, the first two paragraphs of the proof. However, when Hartshorne goes to show the map $$\pi^* : \operatorname{Cl} X \to \operatorname{Cl} (X \times \mathbb A^1)$$ given by $\sum n_i Y_i \mapsto \sum n_i \pi^{-1} Y_i$ , where $\pi$ is the projection to $X$ , I do not understand the following statements: Injectivity: If $D = \operatorname{div} f \in \operatorname{div} X$ , then $\pi^*D$ involves only prime divisors of type I. Why? Injectivity: The above forces $f \in K$ , the function field of $X$ . Why? He gives the reasoning that otherwise $f = g/h$ , with $g, h \in K[t]$ , which I understand, but then claims that if both are not in $K$ , the divisor of $f$ will involve some type II divisor. Why is this true? Surjectivity: Why does it suffice to check that prime type II divisors are linearly equivalent to sums of prime type I divisors? Do we already know we are surjective on type I divisors? Surjectivity: Why does localizing a type II prime divisor at the generic point give us a prime divisor in the spectrum of $K[t]$ ? Assuming the above statement, we see this divisor corresponds to a principal prime $P = (f)$ in $K[t]$ . I see that just fine. Hartshorne then claims the divisor of $f$ is $Z$ plus possibly some purely type I divisors. Why? I get the sense I am not thinking about divisors in general correctly, and, more specifically, I am not truly understanding the definition and implementation of type I and II divisors.",['algebraic-geometry']
3907561,"Unique solution of $\dot{x} = f(x), x(0) = x_0$","I teach myself to solve IVP and could use some help regarding this exercise I found: Let $f:[0,+\infty) \to [0,+\infty)$ be continuous such that $0$ is the only zero of $f$ . Consider the differential equation $\dot{x} = f(x)$ with initial value $x(0) = x_0 \in [0, +\infty)$ . I am asked to show that $\int_{0}^{1} \frac{1}{f(x)}\ \mathrm{d}x = +\infty$ implies that the IVP has a unique solution for $x_0 = 0$ . It's obvious that $x(t) = 0$ is a solution for $x_0 = 0$ but I have no idea how to proof the statement. Are there any important theorems that might help here?","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
3907572,"Is it even possible to rigorously define sine and cosine using the ""traditional"" definition?","Most rigorous definitions of the sine and cosine function use power series or some other analytic definition. In geometry class, we learn about the geometric, right-triangle definition of sine and cosine. It is hard to formalize the geometric definition in a rigorous, non-circular (pardon the pun) way. But has anyone done it? Is it even possible to do so? I would be interested in a text or paper that defines the trigonometric functions in a geometric way, but rigorously.","['geometry', 'reference-request']"
3907591,Differentiation of $x^TAx$ (Multivariate Calculus),"Suppose $A$ is an $n\times n$ symmetric matrix. Consider a function $f : \mathbb{R}^n \to \mathbb{R}$ such that $$f(x) = \langle Ax , x \rangle = x^TAx\quad\quad\text{for all}~~~ x \in \mathbb{R}^n$$ I want to find its derivative, i.e. $Df$ . Most probably, it can be done by expanding $x^TAx$ by usual matrix multiplication. But I'm looking for a procedure which doesn't involve this sort of expansion. However, any elementary proof will help. Thanks in advance. Sorry if someone had asked this problem before.","['multivariable-calculus', 'calculus', 'derivatives', 'linear-transformations']"
3907603,"If $\dim (V/ W) = 1$, what can we say about $\dim(\overline{V}/\overline{W})$","Let $X$ be a Banach space and $W\subset V \subset X$ are two subspaces of $X$ . Suppose now we know $\dim(V/ W) = 1$ , then what can we say about the dimension $\dim(\overline{V}/\overline{W})$ ? Here $\overline{V}$ and $\overline{W}$ are the closures of $V$ and $W$ in $X$ , respectively. Is it true that $\dim(\overline{V}/\overline{W})\leq1$ ? Any hint will be appreciated. Thanks!","['banach-spaces', 'normed-spaces', 'functional-analysis', 'quotient-spaces']"
3907666,Intuition behind the general form of the solution in Frobenius Method?,"When we solve the equation $z^{2}u''+p(z)zu'+q(z)u=0$ using Frobenius method , we first find zeros of the indicial polynomial $r_1>r_2$ to get a solution $u_1$ . In the case that the difference of roots $r_1-r_2$ is an integer, we might need to find a second linearly independent solution using a different method. It is given on the Wikipedia page (and many other places) that $$
u_{2}=Cu_{1}\ln x+\sum _{{k=0}}^{\infty }B_{k}x^{{k+r_{2}}}
$$ gives the desired second solution. My question is: how do we come up with this general form of solution? I try to derive it in various ways, but find it hard to achieve - I have tried to write $u_2 =vu_1$ , which reduces the order of the DE and gives $$
v'= u_1^{-2}\exp\left(- \int \frac{p(z)}{z} dz\right),
$$ but this looks completely different from the formula above. I do see some vague intuition about the logarithmic term - we are basically saying: if multiplying by $x^{-r}$ does not make the solution analytic, then something stronger like $\ln x$ will. In other words, we create an essential singularity by adding $\ln x$ . I wish to see a more satisfying explanation of this.","['frobenius-method', 'analysis', 'ordinary-differential-equations', 'analytic-functions']"
3907674,How to get the integral of $\log(\det(A + Bt))$ w.r.t variable t?,"Suppose we have two positive definite matrices $A$ and $B$ , now I want to get the integral of: \begin{align}
\int_{a}^{b} \log(\det(A + Bt)) dt ~~~~~~~~~~~~\text{for } a, b > 0 
\end{align} Analytic solution is better, but good approximation is also acceptable here. I tried to approximate this integral via interpolation, but as $A$ , $B$ are of large size (i.e, 100 √ó 100), that maybe too slow to calculate $\det(A + Bt)$ every time. When I seek the analytics solution, I met the problem of finding: $\frac{d(\det(A + Bt))}{dt}$ . If you have any ideas or inference on this question, welcome to your answer and suggestions, thank so much!!","['integration', 'definite-integrals', 'matrices', 'applications']"
3907714,basis for null space of matrix in a certain field,"Let $G=(V,E)$ be a connected graph. Let $T$ be a spanning tree of $G$ . Fix a $2$ -element field $\mathbb{F}$ (the elements are $\{0,1\}$ ). Let $S$ be an incidence matrix of $G$ in $\mathbb{F}$ . Find a basis $A$ for the null space of $S$ so that for each edge $e\in E\backslash E(T),$ there is exactly one vector $w \in A$ for which $\mathbb{1}_w(e) = 1$ (here $\mathbb{1}_w(e)$ is the function that is $1$ if the entry corresponding to edge $e$ of vector $w$ is $1$ and $0$ otherwise). What are the sets of edges of $G$ corresponding to elements of $A$ ? I know that the null space of $S$ has dimension $|E | - |E(T)|.$ However, I'm not sure how to find a basis for the null space. It might be useful to figure out what the left null space and row space of $S$ are, but again I'm not sure how to do that. I also think that a submatrix $F$ of $B$ 's columns has linearly dependent columns iff $F$ contains a cycle. I'm not sure if an inductive argument might be useful, seeing as induction seems to be used quite often for proofs in combinatorics.","['graph-theory', 'matrices', 'algebraic-graph-theory', 'linear-algebra', 'induction']"
3907790,Proving surjectivity of a floor function.,"So the function given is $$ g{_r}: \mathbb{Z}\rightarrow\mathbb{Z}, \: x \mapsto \big\lfloor\dfrac{x}{r}\big\rfloor, \quad \quad \text{while} \: r\in\mathbb{N}.$$ The original question was to prove or disprove the injectivity or surjectivity of this function. I have already proven that for $r=1$ , this function is injective and for $r>1$ it is not injective. Now I have to check if the function is surjective for $r>1$ . My thought is that we assume that the function is surjective, then we have to show that for every $\left\lfloor\dfrac{x}{r}\right\rfloor\in\mathbb{Z}$ exists an $x \in\mathbb{Z}$ . How can I prove (or disprove) this? Are there some transformations that I can do to the floor function?","['elementary-set-theory', 'ceiling-and-floor-functions', 'functions', 'discrete-mathematics']"
3907794,Logarithmic convexity of incomplete gamma function.,"The incomplete Gamma function $F(t)$ satisfies $$
1 - F(t) \sim \int^\infty_t dx \, e^{-x} x^{\alpha - 1}
$$ for $t > 0$ and its derivative $F^\prime \sim e^{-t} t^{\alpha - 1}$ is the density function of the Gamma distribution . It is claimed that the hazard rate, also known as the failure rate $$h = -\frac{d}{dt} \ln (1 - F) = \frac{F^\prime}{1-F}$$ is increasing everywhere (i.e. $h^\prime (t) > 0$ ) for the case that $\alpha > 1$ and decreasing everywhere for the case that $\alpha < 1$ . In other words, the logarithmic convexity of $1 - F$ depends only on the sign of $\alpha - 1$ . This result is also given as a proof exercise in problem 5.22 of this book . It's unclear to me how to prove this result. We have $$ h^\prime = \frac{(1-F)F^{\prime \prime} + (F^\prime)^2}{(1-F)^2} = \frac{F^\prime}{(1-F)^2} \left[ (1-F)\left(\frac{\alpha - 1}{t} - 1 \right) + F^\prime \right], $$ but from this point the next step is unclear.","['statistics', 'probability-distributions', 'calculus', 'convex-analysis', 'gamma-distribution']"
3907833,Is there a closed-form or combinatorial proof for the 'multisections' of the binomial series $\sum_{k=0}^{n-1}\binom{n}{k}$?,"Throughout, let $m,n\in\mathbb{N}^+$ . As is well-known, $$
\sum_{k=0}^{r-1}\binom{r}{k}=2^r-1
$$ I want to understand the multisections of this sum, namely for $m=2,3,4,5\ldots, 0\le j<m$ I want to evaluate $$
S_{m,j}(n) = \sum _{k=0}^{n-1} \binom{m n}{k m+j}
$$ Numerical results suggest that $S_{m,j}(n)$ is roughly but not exactly equally distributed in $j$ for fixed $m,n$ . I computed the results for $m=2,3,4,6$ (I did $5$ as well but it was much more opaque): \begin{array}{c|cccccc}\\
m\setminus j& 0 & 1 & 2 & 3 & 4 & 5\\
\hline 2 & 2^{2 n-1}-1 & 2^{2 n-1} &  & &  &  \\
3 & \frac{2 (-1)^n}{3}+\frac{8^n}{3}-1 & \frac{1}{3} (-1)^{n+1}+\frac{8^n}{3} & \frac{1}{3} (-1)^{n+1}+\frac{8^n}{3} &  &  & \\
4 & (-1)^n 2^{2 n-1}+4^{2 n-1}-1 & 4^{2 n-1} & (-1)^{n+1} 2^{2 n-1}+4^{2 n-1} & 4^{2 n-1} & \text{} & \text{} \\
6 & \frac{1}{3} 2^{6 n-1}+(-1)^n 3^{3 n-1}-\frac{2}{3} & \frac{1}{3} 2^{6 n-1}+\frac{1}{2} (-1)^n 3^{3 n-1}-\frac{1}{6} & \frac{1}{3} 2^{6 n-1}+\frac{1}{2} (-1)^{n+1}
   3^{3 n-1}-\frac{1}{6} & \frac{1}{3} 2^{6 n-1}+(-1)^{n+1} 3^{3 n-1}+\frac{1}{3} & \frac{1}{3} 2^{6 n-1}+\frac{1}{2} (-1)^{n+1} 3^{3 n-1}-\frac{1}{6} & \frac{1}{3}
   2^{6 n-1}+\frac{1}{2} (-1)^n 3^{3 n-1}-\frac{1}{6} \\
\end{array} For instance, this table says $S_{3,1}(n) = \sum _{k=0}^{n-1} \binom{3 n}{3k+1} = \frac{1}{3} (-1)^{n+1}+\frac{8^n}{3}$ . Clearly we should have $\sum_{j=0}^{m-1}S_{m,j}(n)=2^{mn}-1$ but past that I'm not sure what the multisections should look like. Perhaps there is a combinatorial proof for these sums?","['summation', 'modular-arithmetic', 'binomial-coefficients', 'combinatorics', 'closed-form']"
3907923,Polynomials and Limits,Let $P(x)$ be any polynomial with positive real coefficients. Determine the following limit with proof $ \lim \limits_{x \to \infty} \dfrac{\lfloor P(x) \rfloor}{P(\lfloor x \rfloor)}$ I am unable to approach these kind of sums. Anyone please can guide me and help me with this solution?,"['limits', 'calculus', 'polynomials', 'ceiling-and-floor-functions']"
3908018,"$f(yf(x) + y) = xy + f(y)$ for all $x, y.$ Prove $f$ is surjective [duplicate]","This question already has an answer here : If the function $f$ satisfies the equation $f(xf(y)+x)=xy+f(x)$, find $f$ (1 answer) Closed 3 years ago . I'm stucking with this problem for days. f is a function from $\mathbb{R}$ to $\mathbb{R}$ satisfies: $$f(yf(x) + y) = xy + f(y) \qquad \forall x, y \in \mathbb{R}$$ Prove $f$ is surjective, and hence find all the functions satisfy this equation. I think fixing a $x$ and $y$ as a constant and study the function might be a good idea, but so far I've been only playing with $x=1,0$ and $y=1,0,$ and they didn't seem to help in proving surjectivity. Any help is appreciated. Thanks! (Proving injectivity is easy)","['functional-equations', 'functions']"
3908051,A letter sequence problem,"If I have an $8$ letter string of the letters $C$ and $N$ , how many ways can I arrange the letters so that no two N's are adjacent to each other?","['permutations', 'combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
3908071,Codimension 1 subset of a normal variety is locally principal,"I am trying to prove this result: Let $X$ be a normal variety, and suppose $W$ is an irreducible closed subvariety of $X$ of codimension $1$ . Then there exists an affine open $U$ $\subseteq$ $X$ such that the ideal of $W \cap U$ in $k[U]$ is principal. Here is what I got: Consider the local ring of the subvariety $W$ in $X$ . It is a DVR, and suppose its maximal ideal is $M$ . Let $\langle U,f\rangle$ be an uniformizing parameter of it. We can very well choose $U$ to be an affine open set. Now consider the ideal of $W \cap U$ in $k[U]$ , say it is generated by $g_1,g_2,...,g_n$ . Then from the very definition, we have $\langle U,g_i\rangle$ will be an element of $M$ , so there exists $\langle U',f'\rangle$ in $M$ such that $\langle U,f\rangle\langle U',f'\rangle = \langle U,g\rangle$ . But this just means that $g$ is a multiple of $f$ on $U \cap U'$ . How to get that $f|g$ on whole of $U$ ? Any help will be appreciated. Please see: I have knowledge of only the first 5 sections of chapter 1 of Hartshorne, so an elementary proof will be appreciated.","['algebraic-curves', 'algebraic-geometry', 'abstract-algebra']"
3908093,Integer matrices with determinant 1 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $A$ be an integer matrix of rank $n$ and let $v_1,\dots,v_n$ be the row vectors of $A$ (which are linear independent). Assume that the convex hull of $S:=\{0,v_1,\dots,v_n\}$ does not contain other integer points than the points in $S$ . Is there an elementary way to show that $|\det(A)|=1?$","['algebraic-geometry', 'linear-algebra', 'combinatorics']"
3908150,Find bijection $\phi:[n]\times S_{n-1}\to S_n$ and conclude that $\vert S_n\vert = n!$,"$[n] = \{1,...,n\}$ I am struggling with finding the bijection. $\underline{\text{Assume } \mathbf{n=3:}}$ Than we have $A:=[n]\times S_{n-1} = \{(1,\pi_1), (2,\pi_1),(3,\pi_1),(1,\pi_2), (2,\pi_2),(3,\pi_2)\}$ , with $\pi_1 = id$ and $\pi_2 = (12)$ $S_3 = \{(123), (12)(3), (13)(2), (23)(1), (132), id\}$ So we are looking for some $\phi:A\to S_3$ . The mapping for $a\in A \mapsto \sigma\in S_3$ is not obvious to me. Has someone a hint? The conclusion should be easy if the bijection is found. Edit: Due to your hints and further research I found $\phi:[n]\times S_{n-1}\to S_n, (i,\pi_j)\mapsto \pi_j\circ (in)$ , $i\in[n], j\in [n-1]$ My current status on $\phi^{-1}$ is: $\phi^{-1}:S_n\to [n]\times S_{n-1}, \pi \mapsto (\pi^{-1}(n), ???)$ I am still struggling with $\phi^{-1}$ . Does someone has a hint?","['permutations', 'symmetric-groups', 'functions', 'combinatorics']"
3908248,Derivative of a linear vector function by matrix - 3d-Jacobian or a vector?,"If I differentiate $$
Y = 
\pmatrix{a & b \\ c & d}
\pmatrix {x \\ y}
$$ with respect to the matrix on the left side, what do I get? On one hand, I think I should get $\pmatrix{x \\ y}$ because it's a linear function. On the other hand, I think I should get a 3-dimensional Jacobian matrix (if it's a real thing) where the each 4 entries are 2-d vectors $$
{\partial Y \over \partial a} = \pmatrix{x \\ 0},
{\partial Y \over \partial b} = \pmatrix{y \\ 0} \\
{\partial Y \over \partial c} = \pmatrix{0 \\ x},
{\partial Y \over \partial d} = \pmatrix{0 \\ y} \\
$$ Which one is right?","['multivariable-calculus', 'calculus', 'jacobian']"
3908290,Completeness of the Fell topology,"Recall that the Fell topology $\tau_F$ is a topology on the hyperspace $F(X)$ of closed subsets of a Hausdorff space (maybe you can define it in a more general context, but I am interested in $\mathbb{R}^n$ ). A prebase for the Fell topology is given by the sets of the form $$\{ F\in F(X) : F\cap K =\emptyset  \} $$ $$\{ F\in F(X) : F\cap U \neq \emptyset  \}  $$ for some $K\subset X$ compact and some non-empty $U\subset X$ open. I read in [1, Thm. 5.1.5] that $(F(\mathbb{R}^n),\tau_F)$ is compact metrizable, and becomes Polish if we restrict it to $(F(\mathbb{R}^n)\backslash \{\emptyset\},\tau_F)$ . I assume that it is implicitly saying that $(F(\mathbb{R}^n),\tau_F)$ is not complete. How can this be proved explicitly? I would guess that, since $(F(\mathbb{R}^n),\tau_F)$ is metric, there should be a Cauchy non-convergent sequence, but it is not clear to me what is the metric on $(F(\mathbb{R}^n),\tau_F)$ . [1]: Beer, Gerald , Topologies on closed and closed convex sets, Mathematics and its Applications (Dordrecht). 268. Dordrecht: Kluwer Academic Publishers. xi, 340 p. (1993). ZBL0792.54008 .","['descriptive-set-theory', 'general-topology', 'metric-spaces', 'polish-spaces']"
3908307,$\lim_{x\to 0}\frac{1}{x}‚àí\frac{2}{e^{2x}‚àí1}$ doesn't get solved as expected,"A question says $f:\Bbb R\setminus\{0\} \to \Bbb R$ , $f(x)=\dfrac{1}{x}‚àí\dfrac{2}{e^{2x}‚àí1}$ . The limit as the function approaches $x=0$ is logically $0$ , as in; $\lim \limits_{x \to 0}\dfrac{e^{2x}-1}{2x} = 1$ This function then becomes $\dfrac{1}{x}‚àí\dfrac{2}{2x} =0$ But when I graphed it online, the function approached $1$ .","['limits', 'calculus']"
3908321,"What goes wrong in my computation of $\iiint _S 1 dz dy dx$ where $S=\{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\}$?","I am trying to calculate this triple integral but I don't really know if it is correct. I don't have much experience with triple integrals. Here it is: $\iiint _S 1 dz dy dx$ where $S$ is the set $\{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\}$ . Here $a$ , $b$ and $c$ are fixed non-zero reals, $N$ is a fixed positive real. It seems that $z$ should run from $1$ to $-(ax+by)/c$ and we can let $x$ and $y$ run from $1$ to $N$ , my reasoning being that if $x$ and $y$ are fixed, $z$ gets automatically fixed. Then our integral transforms into $$\iiint _S 1 dz dy dx=\int_1^N \int_1^N \int_1^{-(ax+by)/c} 1 dz dy dx=\int_1^N \int_1^N -(ax+by+c)/c  \mathrm{d}x dy$$ which is equal to $$\int_1^N-(ax^2/2+bxy+cx)|^{N}_1 dy=\int_1^N \left(-(aN^2/2+bNy+cN)+a/2+by+c\right)dy=-aN^3/2-bN^3/2-cN^2+aN/2+bN^2/2+cN+aN^2+bN/2+cN-a/2-b/2-c.$$ I feel like i am making an error somewhere, but I am not sure. Thanks!","['multivariable-calculus', 'calculus', 'solution-verification']"
3908360,Derivative of trace-based linear scalar field,"I'm just getting into matrix calculus, so this question might be really easy for you guys out there. I'm trying to understand some of the simpler derivations in The Matrix Cookbook. I've been looking at derivatives of the trace predominantly. The following formulas are given: $$
\frac{\delta}{\delta X} {\rm Tr}(XA) = A^T
$$ $$
\frac{\delta}{\delta X} {\rm Tr}(AX^T) = A
$$ I can't completely reproduce these example though. When I try to take the derivative with a small example manually, I always get $A$ . \begin{align}
\frac{\delta}{\delta X}
{\rm Tr}\left(
\begin{bmatrix}
x_1 & x_2 \\
x_3 & x_4
\end{bmatrix}
\begin{bmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{bmatrix}
\right) &=
\frac{\delta}{\delta X}
{\rm Tr}\left(
\begin{bmatrix}
a_1x_1 + a_3x_2 & a_2x_1 + a_4x_2 \\
a_1x_3 + a_3x_4 & a_2x_3 + a_4x_4
\end{bmatrix}\right)
\\
&=
{\rm Tr}\left(
\begin{bmatrix}
\begin{bmatrix}
a_1 & a_2 \\
0 & 0
\end{bmatrix} &
\begin{bmatrix}
a_3 & a_4 \\
0 & 0
\end{bmatrix} \\
\begin{bmatrix}
0 & 0\\
a_1 & a_2
\end{bmatrix} &
\begin{bmatrix}
0 & 0 \\
a_3 & a_4 
\end{bmatrix} 
\end{bmatrix}
\right)\\
&=
\begin{bmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{bmatrix}
\end{align} \begin{align}
\frac{\delta}{\delta X}
{\rm Tr}\left(
\begin{bmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{bmatrix}
\begin{bmatrix}
x_1 & x_3 \\
x_2 & x_4
\end{bmatrix}
\right) &=
\frac{\delta}{\delta X}
{\rm Tr}\left(
\begin{bmatrix}
a_1x_1 + a_2x_2 & a_1x_3 + a_2x_4 \\
a_3x_1 + a_4x_2 & a_3x_3 + a_4x_4
\end{bmatrix}\right)
\\
&=
{\rm Tr}\left(
\begin{bmatrix}
\begin{bmatrix}
a_1 & 0 \\
a_3 & 0
\end{bmatrix} &
\begin{bmatrix}
0 & a_2 \\
0 & a_4
\end{bmatrix} \\
\begin{bmatrix}
a_1 & 0\\
a_3 & 0
\end{bmatrix} &
\begin{bmatrix}
0 & a_2 \\
0 & a_4 
\end{bmatrix} 
\end{bmatrix}
\right)\\
&=
\begin{bmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{bmatrix}
\end{align} I'm sorry if the notation is wrong and thanks for your help.","['scalar-fields', 'trace', 'matrices', 'matrix-calculus', 'derivatives']"
3908435,Geometric meaning of homology/cohomology over constant sheaf,"As in the title, I would like to have some geometric intuition or a dictionary from things appeared in algebraic geometry to terms in algebraic topology (I had some studies in algebraic geometry but I am quite new to algebraic topology). In algebraic geometry, we sometimes consider cohomology over constant sheaf. For instance, Chern class of a line bundle is an element in the cohomology group $H^2(X,\mathbb{Z})$ . At some examples that I recall, algebraic geometers also consider cohomology group over constant sheafs like $\mathbb{R}$ and $\mathbb{Q}$ . I would like to ask are there a correspondences cohomology over constant sheafs to homology/cohomology that we studied in algebraic topology, like to simplicial, singular and celluar homology? Thanks in advance for answering.","['algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
3908479,Eigenvalues of Symmetric/Hermitian Matrices,"I am new to stack exchange so please excuse me as I write in JAX I have a question on linear algebra that goes like this :  If $A$ is an $n\times n$ real symmetric matrix or complex Hermitian matrix, then its eigenvalues are real. I want to prove this theorem but I hope someone can this proof and correct me if I am wrong : We have that By the Schur‚Äôs decomposition, $A=QRQ^{*}$ however $A=A^{*}=(QRQ^{*})^{*}=QR^{*}Q^{*}$ where we have that $R$ an upper triangular matrix with eigenvalues of $A$ on its diagonal entries, so $R^{*}$ is a lower triangular matrix thus $A-A^{*}=Q(R-R^{*})Q^{*}=0\implies R-R^{*}=0\implies r_{i,i}=\overline{r_{i,i}}$ . Therefore, $R$ is a diagonal matrix and its diagonal entries
are real. I would like to thank whoever checks my proof. Best regards!","['matrices', 'solution-verification', 'linear-algebra', 'eigenvalues-eigenvectors']"
3908545,"Prove that $G=\{f\in L_p(\mu): \int_\mathbb{R} f(t) dt = 0\}$ is a dense subset of $L_p(\mu)$, where $\mu$ is the lebesgue measure on $\mathbb{R}$.","Here $1<p<\infty$ , prove that $G=\{f\in L_p(\mu): \int_\mathbb{R} f(t) dt = 0\}$ is a dense subset of $L_p(\mu)$ , where $\mu$ is the Lebesgue measure on $\mathbb{R}$ . How to prive this? I have no idea, can anyone give me a hint?","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3908571,How to bound the error for the Taylor expansion of the inverse of a mean of exponentials?,"If $|x| \leq R / 10$ for some $R\in \mathbb{N}$ , then it is easily shown that $$\left|e^{-x} - \sum_{k=0}^R \frac{(-1)^k x^k}{k!}\right| \leq e^{-R}.$$ I would like to have a similar result (i.e. with an error bound of the same form) for the multivariate function $\frac{1}{\frac{1}{M} \sum_{i=1}^M e^{x_i}}$ , where the necessary control on $M\in \mathbb{N}$ and on each $x_i$ 's can be imposed. Any ideas ? After playing with formal Taylor series, here's a partial answer.
We can write $$\sum_{i=1}^M e^{x_i} = \sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}, \quad \text{with } c_{\alpha} =
\begin{cases}
\frac{1}{k!}, &\mbox{if } \alpha = k e_k, \\
0, &\mbox{otherwise},
\end{cases}$$ where $e_k$ is the $k$ -th standard basis vector, and $x^{\alpha} = \prod_{i=1}^M x_i^{\alpha_i}$ . Now the goal is to find coefficients $d_{\beta}$ such that $$\left(\sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}\right) \cdot \left(\sum_{\beta\in \mathbb{N}_0^M} d_{\beta} x^{\beta}\right) = \sum_{\gamma\in \mathbb{N}_0^M} x^{\gamma} \left(\sum_{\alpha + \beta = \gamma} c_{\alpha} d_{\beta}\right) = 1.$$","['calculus', 'taylor-expansion']"
3908611,How did the author take the gradient to get $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$?,"I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.1 What Objective Function Is the Perceptron Optimizing? says the following: Can we find a smooth loss function, whose gradient turns out to be the perceptron update? The number of classification errors in a binary classification problem can be written in the form of a $0/1$ loss function for training data point $(\overline{X_i}, y_i)$ as follows: $$L_i^{(0/1)} = \dfrac{1}{2} (y_i - \text{sign}\{ \overline{W} \cdot \overline{X_i} \})^2 = 1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} \tag{1.7}$$ The simplification to the right-hand side of the above objective function is obtained by setting both $y_i^2$ and $\text{sign}\{ \overline{W} \cdot \overline{X_i} \}^2$ to $1$ , since they are obtained by squaring a value drawn from $\{ -1, +1 \}$ . However, this objective function is not differentiable, because it has a staircase-like shape, especially when it is added over multiple points. Note that the $0/1$ loss above is dominated by the term $- y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \}$ , in which the sign function causes most of the problems associated with non-differentiability. Since neural networks are defined by gradient-based optimization, we need to define a smooth objective function that is responsible for the perceptron updates. It can be shown that the updates of the perceptron implicitly optimize the perceptron criterion. This objective function is defined by dropping the sign function in the above $0/1$ loss and setting negative values to $0$ in order to treat all correct predictions in a uniform and lossless way: $$L_i = \max\{ - y_i ( \overline{W} \cdot \overline{X_i} ), 0 \} \tag{1.8}$$ The reader is encouraged to use calculus to verify that the gradient of this smoothed objective function leads to the perceptron update, and the update of the perceptron is essentially $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$ . How did the author take the gradient to get $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$ ?","['neural-networks', 'machine-learning', 'multivariable-calculus', 'vector-analysis', 'gradient-descent']"
3908638,Uniformly convergent sequence of holomorphic function in every compact subset converges to holomorphic function,"$\{f_n\}^\infty_{n=1}$ is a sequence of holomorphic functions that
converges uniformly to a function $f$ in every compact subset of $\Omega$ , then $f$ is holomorphic in $\Omega$ . We let $D$ be any disc whose closure is contained in $\Omega$ . Then for any triangle $T$ contained in $D$ , by Goursat's theorem, we have $\int _T f_n(z)dz=0$ . It then asserts that $$\int_T f_n(z)dz\to \int_T f(z)dz\text{ as }z\to \infty$$ in the closure of $D$ , because of the uniform convergence of $f_n$ . This seems a basic question, but can anybody please elaborate what is happening here?",['complex-analysis']
3908753,Understanding norm convergence of the truncated Hilbert transform,"For $\epsilon > 0$ and $f \in L^p, p \geq 1$ then it is not difficult to show that the function $$ H_\epsilon f(x) = \frac{1}{\pi} \int_{|y| > \epsilon} \frac{f(x-y)}{y}dy $$ is well defined.  In fact, much like the Hilbert transform, $H_\epsilon$ is weak $(1,1)$ and strong $(p,p)$ for $1< p < \infty$ . What I'm working to understand is why if $f \in L^p, 1 \leq p < \infty$ then the sequence $\{H_\epsilon f\}$ converges to $Hf$ in $L^p$ .  For now let's assume $p > 1$ (the $p=1$ case boils down to replacing norm convergence with convergence in measure in the following statements).  Letting $\{f_n\}$ be a sequence of Schwartz functions converging to $f$ in $L^p$ then I have $$ \|Hf - H_\epsilon f\|_p \leq \|Hf - Hf_n\|_p + \|Hf_n - H_\epsilon f_n\|_p + \|H_\epsilon f_n - H_\epsilon f\|_p . $$ Given the strong $(p,p)$ inequalities of $H_\epsilon$ and $H$ , then I am good with the first and third terms on the right-hand side of the inequality.  My struggle is with the middle term: I can't seem to understand why $H_\epsilon f_n$ would converge to $Hf_n$ in $L^p$ .  I tried using the Minkowski inequality for integrals on said term, but to no avail. Thank you for the help!","['fourier-transform', 'functional-analysis', 'real-analysis']"
3908766,Uniqueness of solution of an integral equation [Updated],"Problem. Be $f\in \mathcal{C}(\mathbf{R})$ and fix $c_0\in\mathbf{R}$ and $c_2<0$ . Consider the integral equation $$ f(y) = c_0 + c_2 y^2 + \int dz\, f(z) \exp(f(y-z)) $$ where the integral is over $(-\infty,\infty)$ . Under some conditions on $c_0$ and $c_2$ , it is possible to find $a_0$ and $a_2$ such that $f(y)=a_0+a_2 y^2$ is a solution. The problem is: Is this solution unique? Let's be free to choose specific (nontrivial) values for $c_0$ and $c_2$ if that simplifies the analysis; in particular though, let's keep $c_2\neq 0$ . Also, let's be free to assume that $f$ is smooth, e.g. $f\in \mathcal{C}^2(\mathbf{R})$ . Roughly speaking, the problem is how to prove that there is only one solution to an equation like $$ f * \exp f(y) = c_2 y^2 + f(y). $$ Attempt 1. From the form of the equation, it is natural to define the operator $$ F_r(f)(y) \triangleq c_0 + c_2 y^2 + \int_{-r}^r dz\, f(z) \exp(f(y-z)) $$ and regard the solution as a fixed point of $F_r$ for any fixed $r>0$ . So my first attempt would be to apply Banach fixed point theorem to $F_r$ . I don't know if this works at all, even in principle, since $f\in \mathcal{C}(\mathbf{R})$ , but for now this is what I have: \begin{align} \| F_r(f)-F_r(g) \|_\infty 
& \leq \sup_{y\in\mathcal{B}_r(0)} \int_{-r}^r dz  \left| f(z)\exp(f(y-z)) - g(z)\exp(g(y-z)) \right|
\end{align} where $\mathcal{B}_r(0)$ is the ball of radius $r$ and center the origin. From here, I should try to end up with something like $C\|f-g\|_\infty$ for some $C>0$ ... Attempt 2. A different approach is to show that, if $f^*$ and $g^*$ are two solutions, i.e., $F(f^*)=f^*$ and $F(g^*)=g^*$ , then $f^*=g^*$ . I tried the following: Since by assumption $f^*$ and $g^*$ are two solutions, then $$ f^*(y) - g^*(y) = F(f^*)(y) - F(g^*)(y) = \int dz\, \big[f^*(y-z) \exp(f^*(z))-g^*(y-z) \exp(g^*(z))\big]. $$ I should show that the above quantity is identically equal to zero (for all $y$ ). I would proceed by contradiction. Suppose that $f^*\neq g^*$ , that is, there exists $h=f^*-g^*$ that is not identically zero. Suppose that $h(y)\neq 0$ in a neighborhood of $y=y_0$ . Then I try to expand $h$ in Taylor series around $y=y_0$ . For any $y\neq y_0$ it results \begin{multline} 0 \neq h(y_0) + h'(y_0)(y-y_0) + \cdots \\ = \int dz\, \bigg[ ( f^*(y_0-z) + (f^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(f^*(z)) \\ - ( g^*(y_0-z) + (g^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(g^*(z))  ) \bigg]. \end{multline} In the end, I should lead to some contradiction, e.g. the integrand is zero... Attempt 3. I don't know if it is just a coincidence or not, but the solution $f(y)=a_0 + a_2 y^2$ has the same form of the term $c_0+c_2 y^2$ . In other words, if we define the operator $$ G(f) \triangleq f * \exp f - f $$ then the problem is to find $f$ such that $$ G(f)(y) = c_0 + c_2 y^2 .$$ Therefore, an approach may be to find the solution to a ‚ÄúGreen equation‚Äù like $$G(\varphi)(y)=\delta_y$$ where $\delta_y$ represents a Dirac mass at $y$ . Such a $\varphi$ would (?) unlock the possibility of solving the ‚Äúinhomogeneous‚Äù case.","['functional-equations', 'integral-equations', 'functional-analysis', 'real-analysis']"
3908807,Are there non-Hausdorff spaces that satisfy the Galois correspondence for covering spaces?,"The Galois correspondence for covering spaces requires that a space $X$ be path-connected, locally path-connected, and semilocally simply connected. These seem like pretty strict requirements, so I was wondering how pathological a space can be while still fulfilling them. More specifically, I was wondering if anyone had an example of a space $X$ fulfilling all three that is not Hausdorff. Is this even possible? Could we go further and find a space fulfilling these conditions which is not $T_1$ ?","['path-connected', 'separation-axioms', 'covering-spaces', 'general-topology', 'algebraic-topology']"
3908878,Roll $1$ die and square or roll $2$ dice and multiply; which has higher mean?,"I am wondering if there is a fast way to tell which expected value is higher: ""Roll 1 die and take the square of the number that comes up or roll 2 dice and multiply them"". Thank you!","['recreational-mathematics', 'problem-solving', 'probability-theory', 'probability']"
3908890,Compute the spectral measure of an operator $(T\mathbf{a})_{n}=a_{n+1}-a_{n}$ on the space of square summable sequences,"Consider the space $\mathcal{H}$ of square summable sequences $\mathbf{a}=\{a_{n}\}_{n=-\infty}^{\infty}$ . The operator is defined by $$(T\mathbf{a})_{n}=a_{n+1}-a_{n}.$$ I want to compute the spectral resolution of this operator. From this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure $\mu$ can define resolution by $\mu((-\infty, t))$ . So I guess the key is to get the spectral measure of this operator. Edit 1: I think I got at least the spectrum Given $\Psi\in\ell^{2}(\mathbb{Z})$ , we can define $\hat{\Psi}\in L^{2}([0,2\pi))$ by $\hat{\Psi}(\xi):=\sum_{n\in\mathbb{Z}}e^{in\xi}\Psi_{n},$ with $$\Psi_{n}=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{in\xi}d\xi.$$ Then, the map $$U:\ell^{2}(\mathbb{Z})\longrightarrow L^{2}([0,2\pi),\dfrac{d\xi}{2\pi}),\ \ \Psi\longrightarrow_{U}\hat{\Psi},$$ is unitary. We want to know what does $UTU^{-1}$ give us. Take $f(\xi)\in L^{2}([0,2\pi))$ , then $$(TU^{-1}f)(n)=\dfrac{1}{2\pi}\int_{0}^{2\pi}(e^{-i(n+1)\xi}-e^{-in\xi})f(\xi)d\xi=\dfrac{1}{2\pi}\int_{0}^{2\pi}e^{-in\xi}(e^{-i\xi}-1)f(\xi)d\xi,$$ which implies that $$(UTU^{-1})(\xi)=(e^{-i\xi}-1)f(\xi).$$ Now we have the form of our operator in Fourier space, we are equipped to determine its spectrum. Look at the resolvent: \begin{align*}
R(T):=\{\lambda:(\lambda-T)\ \text{is invertible}\}&=\{\lambda:\lambda-e^{-i\xi}+1\neq 0\ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\
&=\{\lambda:\lambda\neq e^{-i\xi}-1\ \ \ \text{for all}\ \ \xi\in [0,2\pi)\}\\
&=\mathbb{C}\setminus \{|\lambda+1|=1\}.
\end{align*} This implies that the spectrum is $$\sigma(T)= \{|\lambda+1|=1\}.$$ We denote this set to be $\mathbb{S}_{1}^{*}(-1,1)$ . Am I correct? Edit 2: I possibly get the spectral measure. But it confuses me Now, for $f\in C(\mathbb{S}^{*}(-1,1))$ in Fourier space $f(T)$ is just multiplication by $f(e^{-i\xi}-1)$ . Fix $\Psi\in\ell^{2}(\mathbb{Z})$ , and let $\hat{\Psi}$ be the corresponding function in $L^{2}([0,2\pi))$ . Then \begin{align*}
\int f(T)d\mu_{\Psi}=\langle \Psi|f(T)\psi\rangle_{\ell^{2}}&=\langle U^{-1}U\Psi|f(T)U^{-1}U\Psi\rangle_{\ell{^2}}\\
&=\Big\langle U\Psi\Bigg|\Big(Uf(T)U^{-1}\Big)U\Psi\rangle_{L^{2}}\\
&=\langle \hat{\Psi}|f(e^{-i\xi}-1)\hat{\Psi}\rangle_{L^{2}}\\
&=\dfrac{1}{2\pi}\int_{0}^{2\pi}\overline{\hat{\Psi}}f(e^{-i\xi}-1)\hat{\Psi}d\xi\\
&=\dfrac{1}{2\pi}\int_{0}^{2\pi}f(e^{-i\xi}-1)|\hat{\Psi}(\xi)|^{2}d\xi.
\end{align*} Write $\lambda:=e^{-i\xi}-1$ , then $d\xi=\frac{d\lambda}{-e^{-i\xi}}$ and $\xi=i\log(\lambda+1)$ where $\log$ is taking over the principal branch (it's okay if just a general $\log$ since $\xi\in [0,2\pi]$ ). Then, \begin{align*}
\int fd\mu_{\Psi}&=\dfrac{1}{2\pi}\int_{\mathbb{S}_{1}^{*}(-1,0)}f(\lambda)|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}},
\end{align*} therefore, $$d\mu_{\Psi}=|\hat{\Psi}(i\log(\lambda+1))|^{2}\dfrac{d\lambda}{-e^{-i\xi}}\Bigg|_{\mathbb{S}_{1}^{*}(-1,0)}.$$ According to this website, https://encyclopediaofmath.org/wiki/Spectral_resolution , the spectrum measure $\mu$ can define resolution by $\mu((-\infty, t))$ . But you see my spectrum density is restricted to the unit circle. I am not sure how to go back to the real line. Are the above things correct? How can I proceed?","['operator-theory', 'analysis', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
3908932,"What is a Gauge symmetry, intuitively (string theory)?","I'm writing an essay for a popular (but mathematically mature) audience on the history of mathematical physics, wherein I have a section devoted to string theory. Unfortunately, neither I (nor my audience) have the background to understand the material, however (remaining in theme with the rest of the paper) I'd like to present something substantial and quantitative (i.e. be able to present some sort of hands-on mathematical concept(s)). While I lack the necessary foundations (I've followed the traditional undergraduate Calculus sequence, and have some number theory and group theory) I feel that I have the mathematical maturity to understand the concepts. I keep coming across these vague references to ""symmetries"" and ""Gauge symmetries"", and the notation $SU(n)$ . In my extensive search, I've come across only very advanced/terse material, or very watered down and vague material (which is to be expected, I suppose). What are ""symmetries"" and ""Gauge symmetries""? What is this notation (which I've seen extensively even in popular books for the general population, without much explanation): $SU(n)$ ? Why are these symmetries important (particularly as they relate to string theory?).","['string-theory', 'abstract-algebra', 'mathematical-physics', 'gauge-theory']"
