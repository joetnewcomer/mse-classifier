question_id,title,body,tags
2599883,Solving a system of third order homogenous ODEs,"I am trying to solve this third order system of homogenous ODEs. $x'''=2x+y$ $y'''=x+2y$ Initial conditions are given as well. Higher order systems weren't covered in the lectures hence I am a bit lost. Nor can I find any similar questions asked on this site. I have tried to solve the equations in two ways so far. Firstly express $y$  in terms of $x'''$ and $x$ from the first equation differentiate the equation three times giving: $x^{(6)}-2x=y'''$ and plugging this into the second equation finally obtaining: $x^{(6)}-4x'''+3x=0$ A higher order homogeneous ODE with constant coefficients that I can solve with the substitution $x = e^{\lambda x}$. The solution can then be differentiated three times and plugged into $x'''=2x+y$ to obtain $y$. Using the given initial conditions we can then find the specific solution. However, this proves very tedious, especially solving the 6x6 system of equations. Is this approach correct and applicable to other similar systems? Or is there a completely different approach to this type of problem I am not aware of? Could you try to solve the system $\vec{u}''' = A \vec{u}$ by diagonalizing A and look for a solution that way, I've tried this as well but I cannot seem to find a solution. Any help is much appreciated.","['ordinary-differential-equations', 'systems-of-equations']"
2599917,How can I find in wich points $f(x)$ continuous? and how can I find in which points $f(x)$ differentiable?,given $f: \mathbb{R} \to \mathbb{R}$ so that: $$f(x) = \begin{cases}x & x  \in \mathbb{Q}  \\ ax(x-1) & x  \not \in \mathbb{Q} \end{cases}$$ How can I find in which points $f(x)$ continuous? And how can I find in which points $f(x)$ is differentiable?,"['derivatives', 'continuity', 'functions']"
2599920,Find the simplest counterexample against exchanging limit and summation,"I would need a very simple counterexample to show that
$$
\lim_{M\to\infty}\sum_{t=1}^M f(t,M)
$$
may not necessarily be equal to
$$
\sum_{t=1}^\infty \lim_{M\to\infty}f(t,M)\ .
$$
The situation here is (slightly) different from the commonly asked question about interchanging limits and infinite summation, as $M$ is itself driving the upper limit of the sum. Can you exhibit a simple function $f$ which does the job? [Note that it should depend explicitly on $M$!]. I could only come up with an overly complicated situation, but I think I am missing something potentially very simple... Many thanks for you help.","['sequences-and-series', 'limits']"
2599921,Using trigonometric identities to prove following series...,"I'm stuck on this problem: If $(2^n + 1)\theta = \pi$, then find the value of: $$2^n \prod_{k=0}^{n-1}\cos(2^k\theta)$$ I have no idea how to start. Help would be appreciated!","['trigonometry', 'trigonometric-series', 'functions', 'products', 'fractions']"
2599926,Show matrix is nilpotent,"I have matrices $A,B$ of dimension $n$ with real coefficients which satisfy the following: $A^2-B^2=c(AB-BA)$ where $c$ is a real number. If $c\neq0$ , prove that $(AB-BA)^n = 0$. So far, I've been able to show that $AB-BA$ is singular. Can someone help?","['matrices', 'linear-algebra']"
2599950,Contour integral with two branch cuts,"Q5  Use contour integration to evaluate $$\int_{-1}^{1}\frac{\sqrt[]{1-x^2}}{x^2 + 1}dx$$ I've been trying to use a similar method to the one given in https://en.wikipedia.org/wiki/Contour_integration#Example_6_ –_logarithms_and_the_residue_at_infinity but with a branch cut on [-1,1]. However I don't really understand what to do.","['complex-analysis', 'integration', 'definite-integrals', 'contour-integration', 'analysis']"
2599973,Compute $\int\limits_0^1 \log (2\arctan x+\frac{2x}{1+x^2})(1+x\arctan x)\ dx$,"How to Find the value of $$I=\int_{0}^{1}\log \Big(2\arctan x+\frac{2x}{1+x^2}\Big)\Big(1+x\arctan x\Big)dx$$ I tried to subsititution:x=tant, but the process became very complicated,I don't know how to deal with it, and any help will be appreciated.","['definite-integrals', 'integration', 'trigonometry', 'calculus']"
2599979,"Is $X^2Y+Y^2X^{2018}+X+Y+1 \in \mathbb F_2[X,Y]$ irreducible?","This is my problem: Is the polynomial $X^2Y+Y^2X^{2018}+X+Y+1 \in \mathbb F_2[X,Y]$ irreducible? I only have one theorem I can use to show that a polynomial is reducible, but I've already seen myself that it doesn't hold in this case, so my assumption right now is that the above polynomial is irreducible, but I don't know how to show that. I know of the following ways to do that: Eisenstein, reduction, root criterion. I don't think I can use root criterion here and I'm pretty sure I can't use Eisenstein here either (or can I?). So I tried using reduction, but no matter what prime element I use, it won't work. Can someone help me out? Thanks in advance.","['irreducible-polynomials', 'abstract-algebra', 'ring-theory']"
2600005,First-order differential equations - why is only the interval of validity with the initial value valid? [duplicate],"This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I'm reading through Paul's online math notes on differential equations because I'd like to have a basic grasp on the subject (I'm interested in physics ...) There's something I don't understand about initial value problems. I get that the solution y(x) to a first-order differential equation with a given initial value constraint might not be valid for every x. But what I don't get is that the author claims that we should only see the interval of validity in which the x of the initial value constraints lays as the interval of validity. I'm getting these claims from this page , at the Example 1 section. I just see no reason for this claim and I was wondering if someone could offer a mathematical explanation of why we discard all the intervals of validity except for the one in which the initial value lays. ( I could understand it from a physical point of view ). Thanks very much in advance,
Joshua","['ordinary-differential-equations', 'initial-value-problems', 'calculus']"
2600009,"""similitude method"" for the case: $\nu''-2\nu'+\nu= \frac {e^x}{x}$","As title says... I have already tried things like $\nu(x)=\log(x^{\lambda})e^x$ or $\nu(x)=\dfrac{e^x \lambda}{x}$ But with no success, stinks of logarithms but I don't know...","['real-analysis', 'ordinary-differential-equations']"
2600030,Why $\|U^*AU\|=\|A\|?$ with $U$ unitary operator [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $E$ be a complex Hilbert space. Let $A\in \mathcal{L}(E)$ and $U$ be any unitary operator in $E$. Why
  $$\|U^*AU\|=\|A\|?$$ Thanks","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2600053,"In what sense is $\text{Mor}_{\text{R-alg}}(A_1,A_2)$ a category?","My professor said in a lecture that if $A_1$ and $A_2$ are R-algebras, where $R$ is some commutative ring then $\text{Mor}_{\text{R-alg}}(A_1,A_2)$ is a category. Why is this true? I know that in many examples, for instance of $A_1=R[x]$ and $A_2$ is some field containing $R$ then $\text{Mor}_{\text{R-alg}}(A_1,A_2)=A_2$ so because $A_2$ is a ring it can be given the structure of a category, but why is this true in general. As I understand it, it is not true in general that $\text{Mor}(A,B)$ is a category for $A,B$ elements of some category.","['category-theory', 'abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2600100,algebrainc proof for $\alpha\beta \leq \frac{\alpha^p}{p} + \frac{\beta^q}{q}$ for conjugate exponents p and q [duplicate],"This question already has answers here : Purely ""algebraic"" proof of Young's Inequality (4 answers) Closed 6 years ago . I found a very informal proof (geometric one, for by taking $\alpha$ as $\beta$ in the x-axis and y-axis and showing rectangle area is smaller than area under the plots).  Is there a proper algebraic proof for the inequality ? $$
\alpha\beta \leq \frac{\alpha^p}{p} + \frac{\beta^q}{q}
$$
where $\alpha, \beta > 0$
$$
\frac{1}{p} + \frac{1}{q} = 1
$$","['real-analysis', 'inequality', 'a.m.-g.m.-inequality', 'exponential-function', 'algebra-precalculus']"
2600102,Non-orientable hyperbolic surface?,"Define a hyperbolic surface $H$ as a metric space locally isometric to the Poincaré upper half-plane $\mathbb{H}^2$ with the appropriate metric. Note that the quotient $\mathbb{H}^2/\Gamma$ is a hyperbolic surface, where $\Gamma$ is a Fuchsian group acting properly discontinuously without fixed point on $\mathbb{H}^2$. Every closed, connected Riemann surface $S$ of genus greater than $2$ admits the structure of a hyperbolic surface : this is a (fairly) direct consequence of uniformization. Indeed the universal cover $\tilde{S}$ is $\mathbb{H}^2$, and $Aut(\mathbb{H}^2)=PSL(2,\mathbb{R})$, so $S=\tilde{S}/\Gamma$ where $\Gamma$ is Fuchsian, and we conclude by the previous remark. We have the following result: Theorem : Let $H$ be a closed, connected, oriented hyperbolic surface of genus $\geq 2$. Then $S$ is isometric to $\mathbb{H}^2/\Gamma$ for some Fuchsian $\Gamma$. In particular, such hyperbolic surface carries the structure of a Riemann surface. In the proof we do use the fact that $H$ is oriented to show that transition maps are holomorphic. However the following question arises : what are some interesting examples of non-orientable hyperbolic surfaces ? How much work is needed to find such a surface ? I cannot think of one just yet. Moreover, if every closed hyperbolic surface can be obtained by gluing pairs of pants, how do we even find one ? Where does the construction degenerate ? Many thanks for reading.","['complex-geometry', 'hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
2600151,"Prove that $f'$ is continuous, knowing that it has lateral limits","Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(x)-f'(x)$ is monotonic. Prove that $f'$ is continuous $f(x)-f'(x)$ is monotonic, so it has finite left/right-hand limits at any point. The same can be said about $f$, because it is continuous, so $f'$ has also finite left/right-hand limits at any point $a\in \mathbb{R}$, call them $l=\lim_{x\nearrow a}f'(x)$ and $r=\lim_{x \searrow a}f'(x).$ But $f'(a)=\lim_{x \to a}\frac{f(x)-f(a)}{x-a}$, so $$f'(a)=\lim_{x \nearrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \nearrow a} f'(x)=l$$
and $$f'(a)=\lim_{x \searrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \searrow a} f'(x)=r$$
from l'Hospital rule. But this means that $f'(a)=l=r$, so $f'$ is continuous at $a$. Is this proof ok?","['derivatives', 'real-analysis', 'calculus', 'proof-verification']"
2600153,Show that every planar graph admits an orientation such that each vertex has at most five outgoing edges,"An orientation of any graph $G = (V, E)$ is defined as any graph $G' = (V, E')$ arising by replacing each edge $\{u, v\}$ of $E(G)$ by a directed edge either  $\{u, v\}$ or $\{v, u\}$. Show that every planar graph admits an orientation such that each vertex has at most five outgoing edges. Intuitively I can see this to be true but I am having a hard time producing a formal proof for this. I was thinking of a case distinction where I can split the cases 1) the degree of any vertex of G is at most 5 (this case is trivially true) and 2) there exist vertices of degree more than 5 (having a lot of trouble trying to prove this). I'd really appreciate your help!","['graph-theory', 'discrete-mathematics']"
2600158,Exponential and factorial that grow at exactly the same rate,"I want to find a relation of the form $$
n^{n^a} = \Theta (n!)
$$ I know and can reason fairly easily that $n^n$, where $a=1$, grows faster than $n!$, and $n^{\sqrt{n}}$, where $a=\frac{1}{2}$, grows more slowly than $n!$, so we can state the bound: $$\frac{1}{2} < a < 1$$  However, there appears to be no value in the bound, even in the limit as $a$ approaches $1$, that can make the exponential grow at exactly the same rate as the factorial.  Is there no way to satisfy the above expression (in which case, why?), or have I missed something fairly obvious?","['factorial', 'computational-complexity', 'limits']"
2600169,Geometry Problem from BDMO Past Papers.,"If the difference of areas of outer and inner circles of an equilateral hexagon $ABCDEF$ is $\pi$, what is the area of the hexagon?","['circles', 'geometry']"
2600171,Are all Hausdorff fractals also box counting fractals?,"The Hausdorff dimension of a set $A$ is always lesser or equal than its boxcounting dimension. $$
\mathrm{dim}_\mathcal{H}(A)\leq\mathrm{dim}_\mathcal{B}(A)
$$ More precisely lesser or equal than its lower boxcounting dimension. If the set $A$ is fulfills the open set condition or is self-similar, the two dimensions are equal. But if $A$ does not, is it possible that only one of those dimensions is fractal? For example, $\mathrm{dim}_\mathcal{H}(A)=1$ while $\mathrm{dim}_\mathcal{B}(A)>1$ or $\mathrm{dim}_\mathcal{B}(A)=1$ while $\mathrm{dim}_\mathcal{H}(A)<1$.","['fractals', 'hausdorff-measure', 'chaos-theory', 'dimension-theory-analysis', 'measure-theory']"
2600186,Evaluating $\int_0^{\pi} \frac{1}{(2-\cos(x))^2}$,"Given that
$$ \int_{0}^{\pi}\frac{dx}{a-\cos(x)}=\frac{\pi}{\sqrt{a^2-1}},\,a>1 $$ How do i solve $$ \int_{0}^{\pi}\frac{dx}{(2-\cos(x))^2} $$ I tried using partial fractions but could not make it work, im thinking it must end with some variation of $ \int_{0}^{\pi}\frac{dx}{a-\cos(x)}$, or else the tip would be useless.","['integration', 'definite-integrals', 'calculus']"
2600213,Resolvent estimate self-adjoint operator,"Let $A:D(A)\longrightarrow H$ be an unbounded self-adjoint (or normal) operator on a Hilbert space $H$.
Then we know that $\sigma(A) \neq \emptyset$ and
$$\|(\lambda-A)^{-1}\|=\frac{1}{d(\lambda,\sigma(A))}, \quad \forall \lambda \in \rho(A),$$
where $d(\lambda,\sigma(A))=\min_{\mu \in \sigma(A)} |\lambda-\mu|>0$.
Do we have a similar formula for
$$\|A(\lambda-A)^{-1}\|= ?$$
I point out that $A(\lambda-A)^{-1}$ is a bounded operator since $A(\lambda-A)^{-1}x=-x+\lambda(\lambda-A)^{-1}x$ for any $x \in H$.
I have the basic estimate
$$\|A(\lambda-A)^{-1}\| \leq 1+\frac{|\lambda|}{d(\lambda,\sigma(A))}.$$
Is it sharp ?","['functional-analysis', 'spectral-theory', 'operator-theory']"
2600256,"When can one write $f(x,y)$ as $g(x)-g(y)$ for some $g(\cdot)$?","Suppose I have a function $f:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. Under what conditions can I write $f(x,y) = g(x) - g(y)$ for some appropriate function $g:\mathbb{R}^n \rightarrow \mathbb{R}$? Necessary conditions are obviously that $f(x,x) = 0$ and that $f(x,y) = -f(y,x)$, but are these sufficient?","['real-analysis', 'functions']"
2600283,Let $f(x) = |x|^2$. Does $f'(0)$ exist?,"I think $f(x) = x^2$. Then $f'(0)$ should be $0$. But when I try to calculate the derivative of $f(x) = |x|^2$, then I get: $f'(x) = 2|x| \cdot \frac{x}{|x|}$, which is not defined for $x = 0$. Does $f'(0)$ still exist?","['derivatives', 'real-analysis', 'absolute-value', 'calculus']"
2600313,Calculating $\Big\lfloor\underbrace{\sqrt{2017+\sqrt{2017+...+\sqrt{2017}}}}_\text{2017 roots}\Big\rfloor$,"Could you please help me computing 
  $\Big\lfloor\underbrace{\sqrt{2017+\sqrt{2017+...+\sqrt{2017}}}}_\text{2017 roots}\Big\rfloor$?",['algebra-precalculus']
2600330,Operator norm induced by Frobenius norm,"Suppose $T \colon \mathbb{R}^n \to \mathbb{R}^n$ is a linear operator. In following, we will consider $T$ as a matrix. The operator norm induced by $2$-norm on $\mathbb R^n$ is given by $\|T\|_{op,2} = \max_{\|x\|_2 = 1} \|Tx\|_2$. Let $T$ act by matrix multiplication on the vector space $\mathcal{M}(n \times m, \mathbb{R})$, i.e., the space of $n \times m$ matrices. What is operator norm $\|T\|_{op,F}$ induced by Frobenius norm $\| \cdot \|_F$ on $\mathcal {M}(n \times m)$? Here is my thought: For a given matrix $A \in \mathcal{M}(n \times m, \mathbb{R})$, let $A = (a_1, \dots, a_m)$ where $a_j \in \mathbb R^n$
\begin{align*}
\| TA\|_F^2 &= \|(Ta_1, \dots, Ta_m)\|_F^2 \\
&= \| Ta_1\|_2^2 + \dots \|Ta_m\|_2^2 \le \|T\|_{op,2}^2 \|a_1\|_2^2 + \dots + \|T\|_{op,2}^2 \|a_m\|_2^2 \\&= \|T\|_{op,2}^2 \|A\|_F^2.
\end{align*}
It seems like the operator norm $\|T\|_{op,F}$ should be upper bounded by $\|T\|_{op,2}$. Are they indeed equal? Thanks. EDIT: @erfink points to this link . Indeed, I am asking a quite different question from this. I am not asking the Frobenius norm of $T$ but the operator norm induced by Frobenius norm of $\mathcal{M}(n \times m)$ when $T$ acts on this space by matrix multiplication. $T$ itself even is not an element of space but an elemnt of $\mathcal{M} (n \times n)$.","['matrices', 'normed-spaces', 'linear-transformations', 'matrix-norms', 'linear-algebra']"
2600336,If $f$ is differentiable and $\forall x \in \mathbb R $ and $\forall h >0 ( \mid {f(x+h)-f(x-h)}\mid <h^2 )$ then $f$ is constant?,"If $f$ is differentiable ,and $ \forall x \in \mathbb R  $ and $\forall h >0 $ hold $\mid {f(x+h)-f(x-h)}\mid <h^2$.Prove that $f$ is constant.I tried to use Lagrange theorem.","['derivatives', 'real-analysis']"
2600349,Floor function limit,I need help to calculate this limit $$\lim_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^{n}{\left(\dfrac{n}{k}-\left\lfloor\dfrac{n}{k}\right\rfloor\right)}}$$ I tried using Riemann sums but I don’t get anything useful,"['summation', 'ceiling-and-floor-functions', 'limits']"
2600358,linear operator $f(X) = AXB$,"What are the eigenvalues of the linear operator in vector space $M_n(\mathbb R)$ $$ f(X) =  AXA^T $$
  and $$ f(X) = AXA^{-1} $$
  when eigenvalues of $A$ are $ \lambda_1, \lambda_2, ..., \lambda_n $? I suspect that in first case is $\{\lambda_i \cdot \lambda_j \ | \  i,j \in \{1,2,3, ..., n\}\} $ and in second $\{\lambda_i/\lambda_j \  |\  i,j \in \{1,2,3, ..., n\}\} $, but I can't prove it. More generally what are eigenvalues of $$ f(X) = AXB $$ when we know eigenvalues of $A$ and $B$?","['matrices', 'abstract-algebra', 'linear-algebra', 'linear-transformations']"
2600366,Possible conditions on subsets of Euclidean space to be embedded submanifolds,"Consider a locally connected $X\subset \mathbb R^n$. Given a point $p\in X$, consider the following condition: For any $a,b\in X$, if there's a path $a\to p\to b$ in $X$, there is also a smooth path $a\to p \to b$ in $X$ with nonzero derivative at $p$ (at least once). I think this condition may express that $X\subset\mathbb R^n$ is smooth at $p$. Every point $p$ of Euclidean space satisfies this property since e.g the circle going through points $a,b,p$ furnishes a smooth path $a\to p\to b$. Consequently I think every point of an embedded manifold $X\subset\mathbb R^n$ satisfies this condition. Question. Does this characterize manifolds embedded in Euclidean space? That is, if $X\subset \mathbb R^n$ is a locally connected subset whose every point satisfies the above condition, does it follow $X$ is an embedded manifold? Added. Eric Wofsey's answer helped me realize I would like to assume $X$ is additionally locally Euclidean. His comment provides a counterexample to this case, namely the wedge in $\mathbb R^3$ formed by folding a rectangle along a line. This comment outlines the problem with the condition of my question. I have asked a follow-up question here .","['differential-topology', 'smooth-manifolds', 'calculus', 'manifolds', 'differential-geometry']"
2600377,"Show that closed subspace of differentiable functions is of finite dimension (using Arzela-Ascoli's, Riesz', and Banach's theorems)","Let $F\subseteq C^1([0,1],\mathbb{R})$ be a closed subspace of $C([0,1],\mathbb{R})$. Show that $F$ is of finite dimension. So, I considered the norm $\Vert f\Vert_1=\Vert f\Vert_\infty+\Vert f'\Vert_\infty$ and showed that $(F,\Vert\cdot\Vert_1)$ is a Banach space. Next, using Arzela-Ascoli, I showed that $\overline{\mathbb{B}}_{(F,\Vert\cdot\Vert_1)}(0,1)$ is compact in $(F,\Vert\cdot\Vert_\infty)$. At this point, I got help and was told to show that $\Vert\cdot\Vert_\infty$ and $\Vert\cdot\Vert_1$ are equivalent norms on $F$ and that I could use the open mapping theorem to do so... How? The next step would be to use the Riesz compactness theorem to conclude that $F$ is, in fact, of finite dimension. But, in which step, exactly, did we need the closedness of $F$? I have the feeling of not having talked about it explicitly.","['real-analysis', 'banach-spaces', 'arzela-ascoli', 'functional-analysis', 'compactness']"
2600440,"How is $[0,1]$ locally compact if 0 and 1 do not have a neighborhood in $[0,1]$?","I am a physics student and I am trying to learn the concept of compactness as I need it to understand some group theory issues. I am having trouble understanding the statement that every compact space is locally compact . I understand the open cover definition of compactness and could prove that $[0,1]$ is compact using the supremum method. Now, according to the definition on Wikipedia of local compactness, a topological space $X$ is locally compact if every point in $X$ has a compact neighborhood. My understanding is that neighborhood of a point $p$ in $X$ should contain an open set containing $p$ itself. It seems to me though that $0$ and $1$ in $[0,1]$ do not have open sets/intervals in $[0,1]$ which contain $0$ and $1$. Am I making a conceptual mistake here ? I am not an expert, so please forgive my stupidity here. Thank you.","['general-topology', 'compactness']"
2600447,Prove that the series $\sum_{n=1}^{\infty}\int_{n}^{n+1}(\frac{1}{n^z} - \frac{1}{x^z})dx$ converges uniformly on $\mathbb{C}_{\epsilon}$,"I was taking my Ph.D Analysis qualifying exam and then they asked me to prove the following exercise. Let $\epsilon > 0$ and $\mathbb{C}_\epsilon := \{z \in \mathbb{C}, \Re(z) > \epsilon \}$. (a) $$ F(z) = \sum_{n = 1}^{\infty}\frac{1}{n^z}$$ converges uniformly on $\mathbb{C}_{1 + \epsilon}$. I prove this part easily using the Weierestrass M-Test bounding the terms of this series by a p-series with $p > 1$ and the uniform convergence follows. (b) Prove that the series $$\sum_{n=1}^{\infty}\int_{n}^{n+1}(\frac{1}{n^z} - \frac{1}{x^z})dx$$ converges uniformly on $\mathbb{C}_{\epsilon}$ (c) Prove that there exists an analytic function $G$ on $\{z \in \mathbb{C}, \Re(z) > 0\}$ such that $$G(z) = F(z) - \frac{1}{z-1}$$ for $\Re(z) > 1$ In part (b) I try to bound the terms as in part (a) but I was not able to show the uniform convergence on $\mathbb{C}_{\epsilon}$. However in my attempt I can show that it was uniformly convergent on $\mathbb{C}_{1+\epsilon}$ but I think it must be a mistake. For part (c) I have no idea how to approach.","['analyticity', 'complex-analysis', 'riemann-zeta', 'uniform-convergence']"
2600492,Can we solve this without cubic formula?,"I'm looking for an algebraic solution for $x$. $$ \frac{x}{x+2} -3 = \frac{5x}{x^2-4}+x$$ My first go at this involved converting this into an expression with a cubic numerator and $(x+2)(x-2)$ as the denominator. To find the roots, I then tried to divide each factor in the denominator into to cubic. No success. I've converted this expression to: $$  x(x-7)= (x+3)(x+2)(x-2)$$ which illustrates the futility of my first approach. There are no common factors. Can I solve this without invoking the cubic formula? Edit: To clarify, the the Precalculus textbook calls for an algebraic and graphic solution. If you have an algebraic solution that would be accessible to a precalculus student, please provide it.","['algebra-precalculus', 'cubics']"
2600562,"If $f$ is differentiable and bounded and $\lim\limits_{x\to\infty}f'(x)=0$, does $\lim\limits_{x\to\infty}f(x)$ exists?","Let $f$ be differentiable and bounded, and $\lim\limits_{x\to\infty}f'(x)=0$. I want to know if $\lim\limits_{x\to\infty}f(x)$ exists. It is easy (with MVT) to show that $\lim\limits_{x\to\infty}(f(x+1)-f(x))=0$, and other questions (e.g. here ) show that $\lim\limits_{x\to\infty}\frac{f(x)}x=0$. My intuition says that $\lim\limits_{x\to\infty}f(x)$ should exist, but that does not mean anything.","['derivatives', 'calculus', 'limits']"
2600585,Prove $L^p$ is reflexive for $1<p<\infty$ by using Riesz representation theorem,"By definition, $X$ is reflexive if canonical injection $J:E \to E^{**}$ is surjective, where 
$\langle Jx,f\rangle_{E^{**},E^*}=\langle f,x\rangle_{E^*,E},~\forall x \in E,~\forall f \in E^*.$ In order to show $E$ is reflexive, it is not enough to show the existence of linear surjective isometry from $E$ to $E^{**}$. I'd like to know if it is possible to show $L^p$ is reflexive for $1<p<\infty$ by using Riesz representation theorem. 
By Riesz representation theorem, $(L^p)^* =L^{p'}$, where $1/p+1/p'=1.$ Usually people say ""since $(L^p)^{**} =(L^{p'})^*=L^p$, $L^p$ is reflexive "" It seems right, but I'd like to prove it in detail. Would you give me any comment for this question? Thanks in advance!","['functional-analysis', 'reflexive-space', 'lp-spaces']"
2600715,Prove that $d(v_0)$ = $d(v_n) = 1$,"Given $P = \{v_0, v_1, \cdots,  v_n\} $ be  a longest path in tree $T$.  Show that the degree of $v_0$ and $v_n$ is 1. My attempt:
I have drawn a tree and simulate some paths so I know which one is the longest. But, once I see the other paths,  I learn that the initial and terminal vertices also have the degree of 1. Every vertex in that same position must be 1. But,  why it doesn't go logic to the problem? Now,  what should I do?","['graph-theory', 'discrete-mathematics']"
2600744,Is there a linear injection $ \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*)$ which preserves decomposability?,"Let $V$ be an $n$-dimensional real vector space, and let $2 \le k \le n-2$. Definitions We say an element $\omega \in \Lambda^k V$ is decomposable if $\omega=\alpha_1 \wedge \dots \wedge \alpha_k$, for some $\alpha_i \in V$. We say $\omega$ is a power if $\omega=\alpha \wedge \dots \wedge \alpha$ for some $\alpha \in V$. We say an element $h \in \Lambda^k V^* \otimes \Lambda^k V^* \cong \operatorname{Hom}(\Lambda^k V,\Lambda^k V^*)$ is a power if $h=\Lambda^k g$ for some linear map $g:V \to V^*$. Here, $\Lambda^k g:\Lambda^k V \to \Lambda^k V^*$ is the induced map on exterior powers, that is
$$ \Lambda^k g(v_1 \wedge \dots \wedge v_k)=g(v_1) \wedge \dots \wedge g(v_k).$$ Question: Is there a linear injection
  $$ \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*)$$
  which maps power elements to decomposable elements ? Even better, is there an injection which maps power elements to power elements ? Note: For dimensional reasons , there is always some linear embedding of $\Lambda^k V^* \otimes \Lambda^k V^*$ in $ \Lambda^k (V^* \otimes V^*)$. Motivation: This question arose in the context of applying the Plucker relations, to characterise which metrics on exterior powers are induced by metrics at the base. See here .","['representation-theory', 'exterior-algebra', 'linear-algebra', 'tensor-products']"
2600773,"Proving that the injection $L^q(0,1)\to L^p(0,1) $ is not compact:","I came across the following problem Let $1\le p\le q\le\infty$ then show that the canonical injection from $L^q(0,1)$ to $L^p(0,1) $ is continuous but not compact. The continuity here is straightforward since by Holder inequality one can easily show that $$\|u\|_p\le \|u\|_q.$$ How to show that this injection is not compact. Note this injection is weakly compact as well see here: Canonical inclusion $L^q(0,1) \to L^p(0,1)$ is compact?","['real-analysis', 'compact-operators', 'functional-analysis', 'lp-spaces', 'analysis']"
2600776,Product of cdf and pdf of normal distribution.,"A continuos random variable $X$ has the density
  $$
f(x) = 2\phi(x)\Phi(x), ~x\in\mathbb{R}
$$
  then ( A ) $E(X) > 0$ ( B ) $E(X) < 0$ ( C ) $P(X\leq 0) > 0.5$ ( D ) $P(X\ge0) < 0.25$ \begin{eqnarray}
\Phi(x) &=& \text{Cumulative distribution function of } N(0,1)\\
\phi(x) &=& \text{Density function of } N(0, 1)
\end{eqnarray} I don't have a slightest clue where to start with. Can someone give me a little push. I saw some answers on same question like this but I didn't understand how should I integrate it when calculating expectation.","['statistics', 'probability', 'normal-distribution', 'probability-distributions']"
2600811,Whats the result of double integral $\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$,"Whats the result of double integral
 $$\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$ I was trying to get this $$\int_{0}^{1}\sqrt{1+x^2}\arctan {\frac{1}{\sqrt{1+x^2}}}dx=\int_{0}^{1}(1+x^2)\frac{1}{\sqrt{1+x^2}}\arctan {\frac{1}{\sqrt{1+x^2}}}dx$$
$$=\int_{0}^{1}(1+x^2)\int_{0}^{1}\frac{1}{1+x^2+y^2}dxdy=\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$
So far I don't know what to do next.any helps are to be grateful.","['integration', 'definite-integrals', 'calculus', 'analysis']"
2600858,Find $f(1)+f(3)+f(5)+\dots+ f(999)$ where $f$ is a given function,"Given $$f(x)= \frac {1}
{\sqrt[3] {x^2+2x+1} + \sqrt[3] {x^2-1} + \sqrt[3] {x^2-2x+1}}$$
  and 
  $$E= f(1)+f(3)+f(5)+\dots+ f(999).$$ Then find the value of $E$. My work :- 
Let $\sqrt[3] {x+1}= a$, $\sqrt[3] {x-1}=b $ 
Then the equation reduces to 
$$f(x)= \frac {1}{a^2+b^2+ab}.$$ Now for this expression I tried breaking into partial fractions to that the sum telescopes to some finite quantity but the cube root terms restrict this straightforward way. Thanks in advance for the help. Also if someone finds a way to rationalize the denominator, please share.","['partial-fractions', 'summation', 'polynomials', 'functions']"
2600874,Colimits for gluing schemes and the functor of points 1,"Closely related questions been asked several times in different forms on here but I feel like none really spell out what's going on. I have been looking more at glueing schemes, and particularly relative glueing to construct the projective bundle over a scheme and I really want to understand what is going on in terms of colimits in the category of schemes. I am hoping to break this into two questions since it will be a lot to include in just one. First let's say I just have a pair of topological spaces, $X_{1}$ and $X_{2}$ with $U_{1} \subseteq X$ and $U_{2} \subseteq X$ open sets with a homeomorphism 
$$
\phi: U_{1} \stackrel{\simeq}{\longrightarrow} U_{2}
$$
Now say I want to glue $X_{1}$ and $X_{2}$ along the homeomorphism $\phi$. As a set, this is not hard to write down just as a disjoint union modulo the relation $p \sim \phi(p)$. But what is actually going on here categorically? If there is no gluing involved, then we just have the categorical sum, 
$$
X = X_{1} \sqcup X_{2}.
$$
This has the universal property that to give a morphism from $X$ is precisely to give a morphism from $X_{1}$ and a morphism from $X_{2}$. As someone brand new to the idea of a functor of points, can $X$ then be seen as a representing object for some functor of points? When we try to incorporate glueing along $\phi$ is when I get more confused. In some sense, I need a colimit that captures the notion: To give a morphism from $X$ to some $Y$ is precisely to give morphisms $\psi_{1}: X_{1} \rightarrow Y$ and $\psi_{2}: X_{2} \rightarrow Y$ so that $\psi_{1} = \psi_{2} \circ \phi$ on $U$. I suspect that this is somehow a coequalizer, but I haven't been able to formulate it. So what diagram should I take so that $X_{1} \sqcup X_{2} / \sim$ is the colimit object? This seems to be complicated even further if I have more than two objects to glue. Say I have a family $\{ X_{i} \}_{i \in I}$ with open subsets $U_{ij} \subseteq X_{i}$ along with homeomorphisms 
$$\phi_{ij}: U_{ij} \stackrel{\simeq}{\longrightarrow} U_{ji}  $$
satisfying the appropriate cocycle condition. Can the resulting space formed from gluing be realized as a colimit of some diagram? Is there some generalization of a coequalizer? And again, can this be realized as the representing object of some functor of points? Is there any reference either in the category theory or the topology literature that treats gluing explicitly in this way? I'm hoping to ask another question later specifically about certain colimits in the category of schemes, but I wanted to simplify things first by only considering the topological side of things. Any insight into this would be appreciated.","['algebraic-geometry', 'reference-request', 'limits-colimits', 'category-theory', 'general-topology']"
2600913,Numerical radius of a pair of operators in Hilbert spaces,"Let $(C,D)$ be a pair of bounded linear operators on a complex Hilbert space $E$. The Euclidean operator radius is defined by
$$w_e(C,D)=\displaystyle\sup_{\|x\|=1}\left(|\langle Cx,x \rangle|^2+|\langle Dx,x \rangle|^2\right)^{1/2}.$$
Moreover, the following inequality holds:
$$\frac{\sqrt{2}}{4}\|C^*C+D^*D\|^{1/2}\leq w_e(C,D)\leq \|C^*C+D^*D\|^{1/2}.$$ I want to show that the constants $\frac{\sqrt{2}}{4}$ and $1$ in the above inequalities are the best possible. For the second inequality, the following example show that we have equality: Let $(C,D)=(B,B)$, with $B=\begin{pmatrix}1&0\\0&0\end{pmatrix}$ (operator on $(\mathbb{C}^2,\|\cdot\|)$). Hence, I get
$w_e(C,D)=\sqrt{2}$ and $\|C^*C+D^*D\|^{1/2}=\sqrt{2}.$ I want to find $(C,D)$ such that
  $$\frac{\sqrt{2}}{4}\|C^*C+D^*D\|^{1/2}= w_e(C,D).$$ For a single operator, we have the following theorem: Do you think that if $\text{Im}(C)\perp \text{Im}(C^*)$ and $\text{Im}(D)\perp \text{Im}(D^*)$ we have
  $$\frac{\sqrt{2}}{4}\|C^*C+D^*D\|^{1/2}= w_e(C,D)\,?$$ Thank you in advance.",['functional-analysis']
2600951,Polynomial approximation to formal power series matrix,"I noticed that starting with a $2 {\times} 2$ matrix $M$ with a handful of polynomial entries in two variables such that $\det M$ is invertible in $\mathbb C [x] [[y]]$ I can add infinitely many terms of higher orders in $y$ to the entries of $M$ to make $\det M$ a constant. Further, I noticed that with a bit of tinkering it's actually possible to add finitely many terms to $M$ and still achieve that $\det M$ is constant. I am now wondering if this is true in general: Let $M$ be a $2 {\times} 2$ matrix with entries in the ring $\mathbb C [x][[y]]$ and constant determinant. Given $k > 0$, does there exist a matrix $N$ with entries in $\mathbb C [x, y]$ such that $M \,\mathrm{mod} \, (y^{k+1}) = N \,\mathrm{mod} \, (y^{k+1})$ and $\det M = \det N$? (If it helps, I would be content with assuming that one of the entries is a multiple of $y$ — e.g. if the $(1,2)$ entry is a multiple of $y$, then the determinant being constant implies that the $(1,1)$ and $(2,2)$ entries have a constant term and are both of the form $1 + O (y)$.) I give a simple example to illustrate: Example . Let
  $$
M =
\begin{pmatrix}
1 - y & 0 \\
0 & 1 + y + y^2 + \dotsb
\end{pmatrix}
$$
  Then $\det M = 1$ (as the $(1,1)$ entry is invertible in $\mathbb C [[y]]$ with inverse $(2,2)$). Now let
  $$
N =
\begin{pmatrix}
1-y & -y^{k+1} \\
y^{k+1} & 1 + y + \dotsb + y^{2k+1}
\end{pmatrix}
$$
  Then $\det M = \det N = 1$ and
  $$
M \, \mathrm{mod} (y^{k+1}) = N \, \mathrm{mod} (y^{k+1}) = 
\begin{pmatrix}
1 - y & 0 \\
0 & 1 + y + \dotsb + y^k
\end{pmatrix}. \qquad \diamond
$$ This example works equally well if you replace $y$ by a polynomial $p = p (x,y)$ which is a multiple of $y$. I would also be interested to hear about (textbook/paper) examples computing with matrices of formal power series to see what kind of techniques can be used. Edit. I have discovered a truly tedious proof of this, which this edit section is too narrow to contain. It is a ""proof by brute force"" (solving linear equations for each monomial that may appear in the expression of $\det M \mod (y^{n+1})$), and I think writing up the details should best be left as an Exercise To The Reader. There ought to be a more elegant proof and although I haven't disclosed the full proof, at least I now believe the result to be true, which might be a reason to look for such a proof.","['matrices', 'formal-power-series', 'polynomials', 'commutative-algebra']"
2600984,Solve $y'= \frac{(x+y)^2}{2}.$,"Setting $z(x)=x+y(x)$ I get $y'=z'-1$ and $$z'=1+\frac{z^2}{2},$$ But I'm not sure how I should separate this.","['ordinary-differential-equations', 'calculus']"
2600985,Joint distribution for simple mixture of discrete and continous,"This is a problem 4.31 b) in Statistical Inference by Casella and Berger: let $X \sim Unif(0,1)$ and $Y \sim Binom(n, X)$. What is the joint distribution? The answer can be found in solutions manual but I don't know how to arrive at it. This is where I get: this is mixture of discrete and continuous RVs so there is no well defined pdf. But the distribution will be determined if we can get $P(Y=y, X \leq x) = P(Y=y|X \leq x)P(X \leq x) $. Obviously $P(X \leq x) = x$. And here I've got stuck: not sure if this is legit: $P(Y=y|X \leq x) = \binom{n}{y} \int_{0}^{x} t^{y}(1-t)^{n-y}dt $ ? And even if it is, I don't know how to compute the integral.","['statistics', 'probability']"
2601044,Evaluate $\lim\limits_{x\to a} \left(f(x)/f(a)\right)^{1/(\log x-\log a)}$,"Let $f : \mathbb R \to \mathbb R$ be differentiable at $x = a$ and let $f(a) > 0$. 
Evaluate: $$\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ My attempts: Let $$y=\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ Now I take log on both sides
\begin{align*}
\log y &= \log\left(\left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}\right)\\
&= \frac{\log f(x) - \log f(a)}{x-a} \cdot \frac{x-a}{\log x-\log a}\\
&=  \frac{f'(a)}{f(a)}
\end{align*} Therefore $$y= e^{\dfrac{f'(a)}{f(a)}}$$ Is my answer is correct or not? Please verify.","['real-analysis', 'limits']"
2601103,Is $u$ a quadratic residue mod $t + nu$?,"Suppose $u$ and $t$ are positive integers with $\gcd(u,t) = 1$. Is $u$ a quadratic residue mod $t + nu$ for some $n \in \mathbb{Z}$? The answer is yes in every example that I tried. I was hoping this would follow from quadratic reciprocity, but I don't see how (even assuming $u$ and $t$ are prime). One difficulty I anticipate is that in some cases the numbers $t+nu$ that work can't be primes. For example, if $u = 5$ and $t = 13$, then $t+nu \equiv 3 \mod 5$, but $5$ is not a quadratic residue modulo primes of this form. However, $5$ is a quadratic residue modulo $38 = 13 + 5*5$. This question arose while trying to ensure a matrix was symplectic.","['number-theory', 'quadratic-residues']"
2601105,Why do volume and surface area of unit ball in $\mathbb{R}^d$ behave the way they do for $d \uparrow$?,"Is there any demonstrative / intuitive explanation for the behavior of the surface area and the volume of the unit ball as the dimension increases? I sort of get that it tends to zero, because all the coordinates become smaller and smaller (although I'm not quite satisfied with this ""explanation""). But why the maximum? And why is the maximum not at the same dimension for the two quantities? There is probably some buzzword I should google, but I can't figure it out. edit: I just saw the related question Volumes of n-balls: what is so special about n=5? which is still ""unanswered"" and does not cover the topic of surface area.","['spheres', 'geometry']"
2601108,Vector Magnitude in dimensions > 4,"I'm new to linear algebra, so please just dont blast me :( I know the Pytagorean formula to calculate the magnitude of a vector in any vectorial space. It's easy to grasp the meaning of the vector's magnitude in 2 and 3 dimensions, as the length of the displacement.
But, what's the general meaning of vector magnitude ? This in order to grasp the intuition behind vectors' magnitude in more than 3 dimension.","['linear-algebra', 'geometry']"
2601152,How to tell if a simply-connected curve is the complex plane or disk,"Suppose I have an analytic function $f: \mathbb{C}^2 \to \mathbb{C}$ whose zero locus $V=\{(z,w) \in \mathbb{C}^2 : f(z,w)=0\}$ is smooth and simply connected. By uniformization, $V$ is conformally equivalent to either $\mathbb{C}$ or the open unit disk $\Delta \subset \mathbb{C}$. But which one? Is this ""easily"" determined by some properties of the function $f$?","['complex-geometry', 'complex-analysis', 'riemann-surfaces', 'differential-geometry']"
2601156,Elliptic Curve Point Division,"I have seen several different responses to this type of question, and I find them all contradictory or unclear.  This is a multi-part question. If I have a point Q on an elliptic curve over a finite field, can it be divided by an integer say 2 to find the point which generates it by being doubled? To illustrate, let's take a toy example that I have seen elsewhere:
$y^2 = x^3 + ax + b$ where: a = -2 b = 1 *modulus = 89 With a Generator Point of (4,18)
We get the set: 1 * P = (4, 18)    2 * P = (45, 73)
3 * P = (49, 28)   4 * P = (80, 64)
5 * P = (27, 36)   6 * P = (11, 81)
7 * P = (66, 47)   8 * P = (58, 40)
9 * P = (76, 12)   10 * P = (43, 52)
11 * P = (53, 26)  12 * P = (0, 88)
13 * P = (13, 6)   14 * P = (54, 19)
15 * P = (20, 60)  16 * P = (26, 80)
17 * P = (64, 88)  18 * P = (10, 64)
19 * P = (25, 88)  20 * P = (81, 22)
21 * P = (14, 15)  22 * P = (88, 25)
23 * P = (31, 2)   24 * P = (1, 0)
25 * P = (31, 87)  26 * P = (88, 64)
27 * P = (14, 74)  28 * P = (81, 67)
29 * P = (25, 1)   30 * P = (10, 25)
31 * P = (64, 1)   32 * P = (26, 9)
33 * P = (20, 29)  34 * P = (54, 70)
35 * P = (13, 83)  36 * P = (0, 1) So, for example, I know that doubling 7*P (66,47) gives me the point 14*P (54,19). Is there a division operation which can take the point (54,19) and give me (66,47)?  If so, could I do it again to 7*P, and what would be the result? 3*P or 4*P? In this blog post towards the top, she mentions the Multiplicative Inverse being used in the division operation. I am using a C# Library ( found here ) to calculate the values above, and it also has an ""Extended Euclidean"" Function which when I plug in 14*P (54,19) spits out (6,-17). Is (6,-17) the multiplicative inverse of (54,19), and if so how do I use it to get back to (66,47) If this is possible, can the same rules be applied to very large sets for example NIST P-192 or the secp256k1? Thanks for your consideration and time.","['finite-groups', 'elliptic-curves', 'group-theory']"
2601161,Arranging numbers from $1$ to $n$ such that the sum of every two adjacent numbers is a perfect power,"I've known that one can arrange all the numbers from $1$ to $\color{red}{15}$ in a row such that the sum of every two adjacent numbers is a perfect square . $$8,1,15,10,6,3,13,12,4,5,11,14,2,7,9$$ Also, a few days ago, a friend of mine taught me that one can arrange all the numbers from $1$ to $\color{red}{305}$ in a row such that the sum of every two adjacent numbers is a perfect cube . $$256,87,129, 214, 298, 45, 171, 172, 44, 299, 213, 130, 86, 257, 255,$$ $$88, 128, 215, 297, 46, 170, 173, 43, 300, 212, 131, 85, 258, 254, 89, 127, 216, 296,$$ $$ 47, 169, 174, 42, 301, 211, 132, 84, 259, 253, 90, 126, 217, 295, 48, 168, 175, 41, 302, $$ $$210, 133, 83, 260, 252, 91, 125, 218, 294, 49, 167, 176, 40, 303, 209, 134, 82, 261, 251,$$ $$ 92, 33, 183, 160, 56, 287, 225, 118, 98, 245, 267, 76, 140, 203, 13, 14, 202, 141, 75, 268,$$ $$ 244, 99, 26, 190, 153, 63, 280, 232, 111, 105, 238, 274, 69, 147, 196, 20, 7, 1, 124, 219,$$ $$ 293, 50, 166, 177, 39, 304, 208, 135, 81, 262, 250, 93, 32, 184, 159, 57, 286, 226, 117, 8,$$ $$ 19, 197, 146, 70, 273, 239, 104, 112, 231, 281, 62, 154, 189, 27, 37, 179, 164, 52, 291, 221,$$ $$ 122, 3, 5, 22, 194, 149, 67, 276, 236, 107, 109, 234, 278, 65, 151, 192, 24, 101, 242, 270,$$ $$ 73, 143, 200, 16, 11, 205, 138, 78, 265, 247, 96, 120, 223, 289, 54, 162, 181, 35, 29, 187,$$ $$156, 60, 283, 229, 114, 102, 241, 271, 72, 144, 199, 17, 108, 235, 277, 66, 150, 193, 23,$$ $$ 4, 121, 222, 290, 53, 163, 180, 36, 28, 188, 155, 61, 282, 230, 113, 103, 240, 272, 71, 145,$$ $$ 198, 18, 9, 116, 227, 285, 58, 158, 185, 31, 94, 249, 263, 80, 136, 207, 305, 38, 178, 165,$$ $$ 51, 292, 220, 123, 2, 6, 21, 195, 148, 68, 275, 237, 106, 110, 233, 279, 64, 152, 191, 25,$$ $$100, 243, 269, 74, 142, 201, 15, 12, 204, 139, 77, 266, 246, 97, 119, 224, 288, 55, 161,$$ $$ 182, 34, 30, 186, 157, 59, 284, 228, 115, 10, 206, 137, 79, 264, 248, 95$$ Here, I have a question. Question : Does there exist at least one positive integer $n\ge 2$ satisfying the following condition for each $N\ge 2\in\mathbb N$ ? Condition : One can arrange all the numbers from $1$ to $n$ in a row such that the sum of every two adjacent numbers is of the form $m^N$ for some $m\in\mathbb N$ . Added : I crossposted to MO .","['number-theory', 'graph-theory', 'perfect-powers', 'exponentiation']"
2601185,"If 2 matrices are such that $(A+B)^k=A^k+B^k$ for $k=2,3$, show that $(A+B)^m=A^m+B^m $ for all $m \in \mathbb{N}$","Let $A,B \in M_n(C) $. The matrix  $A-B$ is invertible and $(A+B)^k=A^k+B^k $, $k \in {2,3} $. Prove that $(A+B)^m=A^m+B^m $ for every $m \in N $. PS. I obtained $AB+BA=0$ and $A^2B+B^2A=0$, but I need your help, please :(",['matrices']
2601224,Negative part of convex function is globally Lipschitz continuous?,"Suppose $f:R^n\to R$ is a convex function. Define the the negative part $f^- (x) = |\min\{0, f(x) \}|$. Is $f^-(x)$ globally Lipschitz continuous in $x$? I think it is since if it not then the epigraph of $f$ is not a convex set and we have a contradiction. Edit: Let me clarify my thinking. I think $f^-$ is globally Lipschitz continuous because if it is not, then the epigraph of $f$ is not a convex set. But I don't know how to show this.","['real-analysis', 'lipschitz-functions', 'convex-analysis', 'functions']"
2601238,The degree of the minimal polynomial decreases to the limit,"$A \mapsto \mu_A$ (minimal polynomial of the matrix $A$) is not continuous. Take for example $A_n = \begin{pmatrix}
    0 & 1/n\\
    0 & 0\\
   \end{pmatrix}$. But I think that: if $(A_n)_{n \in \mathbb N}$ satisfies $A_n \to A$ (whatever the norm is since the dimension is finite) and if we suppose all $\mu_{A_n}$ and $\mu_A$ exist, then: $\exists \ n_0, \ \forall \ n ≥ n_0,$ deg$(\mu_A) ≤ $deg$(\mu_{A_n})$ Do you have a hint to prove that?","['eigenvalues-eigenvectors', 'polynomials', 'limits', 'matrices', 'linear-algebra']"
2601272,finding a differential curve tangent to a distribution [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have the following distribution on $\mathbb{R}^3$ $${\cal{D}}_{(x,y,z)} = \langle\{\partial_x,\partial_y + x\partial_z\}\rangle$$ I want to show that for any $(x,y,z)$ in $\mathbb{R}^3$, there exists a path $\gamma$ from $0$ to $(x,y,z)$ tangent to $\cal{D}$ i.e. $\dot{\gamma}(t)\in{\cal{D}}_{\gamma(t)}$ Any hints?",['differential-geometry']
2601330,"Pointwise/uniform convergence of $ f(x) = \begin{cases} +\infty, & \text{if }0<x<0 \text{ (???)}\\ 1, & \text{if }1/(2n)≤x<1 \end{cases} $","$$ 
f_n(x) =
\begin{cases}
-n^3x+2n^2,  & \text{if }0<x<1/(2n) \\
n/(n+1), & \text{if }1/(2n)≤x<1
\end{cases}
$$
$ n\in \mathbb{N}$ How can I prove that $f_n$ is pointwise convergent/uniform convergent? What I have tried so far: I think the limiting function $f(x)$ is:
$$
f(x) =
\begin{cases}
+\infty,  & \text{if }0<x<0  \text{ (???)}\\
1, & \text{if }1/(2n)≤x<1
\end{cases}
$$ But I'm not sure about that and as $n/(n+1)$ converges to 1 for n to $\infty$ I thought that $f_n$ is pointwise convergent, because $f(1)=1$ as well.","['real-analysis', 'uniform-convergence', 'functions', 'convergence-divergence', 'analysis']"
2601352,"Find the cardinality of $S = \{(x,y) \in \mathbb R^2 : 0 < x^3 +y^3< \pi^{21} \}$","Find the cardinality of $S = \{(x,y) \in \mathbb R^2 :  0 < x^3 +y^3< \pi^{21} \}$ This is my idea to solve this, but I am not sure whether or not it's correct: I want to use the Cantor - Bernstein theorem. We can say that $S \subseteq R^2 $. But now, on the other hand, we can fix $x = 0 $ and see what values of $y$ meet the restrictions: it turns out that $ 0 < y < \sqrt[3]{\pi^{21}}$ And now, we can say, that $\{ 0\} \times (0, \sqrt[3]{\pi^{21}}) \subseteq S.$ We know that the cardinalities of both of these boundries are $\mathfrak c$, thus $|S| = \frak c$",['elementary-set-theory']
2601366,Domain and range of a multivariable function,"I have this exercise $$z={2x\over y+5}$$, and I am supposed to obtain the domain and range. I understand that the domain is all the pair of $(x,y)$ except $y=-5$ , then the exercise said that the range is $z=R$ I dont understand why z accept all the values, suppose that you want to plot the point $(1,-5)$ you wont be able to plot that point because $y=-5$ its not accepted by the domain so it cant output a value for z (range) Please help me undestand this or how i get the range for rational functions Thank you!",['multivariable-calculus']
2601427,"Stability when eigenvalues are zero : $x' = -x + y + x^2 + ax^3, \space y' = x - y + x^2 + bxy + y^3$","Exercise : Determine the stability of $O(0,0)$ for the system of differential equations: 
  $$x' = -x + y + x^2 + ax^3$$
  $$y' = x - y + x^2 + bxy + y^3$$ Discussion : Finding the matrix of the linearized system (the Jacobian), we get : $$J(x,y) = \begin{bmatrix} -1 + 2x + 3ax^2 &1 \\ 1 + 2x + by& -1 + bx +3y^2 \end{bmatrix}$$ and the matrix at $O(0,0)$ is : $$J(0,0) = \begin{bmatrix} -1 & 1 \\ 1 & -1\end{bmatrix}$$ Finding the eigenvalues of the matrix to proceed with determining its stability, we get : $$\det(J(0,0)-λI) = 0 \Rightarrow\dots \Rightarrow λ(λ+2) = 0 \Leftrightarrowλ=-2 \space \text{or} \space λ =0$$ My question is, how does one proceed in the case of a zero eigenvalue, which means that $λ_1 \cdot λ_2 = 0$ ? I know a way bypassing such issue is converting to polar coordinates, which can be done rather simply but it doesn't lead anywhere clear on this specific example. The system also cannot be solved so it can't be worked around freely by this way (obviously). Finally, I've figured out 1-2 theorems working over the implicit function theorem to prove a strong case via it, but they include hypothesis and an assisting function that is a total mess. I would really appreciate anyone's help on how to proceed on such problems where we are stuck with a zero eigenvalue.","['eigenvalues-eigenvectors', 'dynamical-systems', 'stability-theory', 'stability-in-odes', 'ordinary-differential-equations']"
2601432,Formalization of proof that there exists no bijective map from the integers to the naturals such that the ordering is preserved?,"My proof would go like: There exists a bijection from the naturals to the integers greater or equal than some number $N$ so that order is preserved(1 to N, 2 to N+1, 3 to N+2 etc.). $N$ must be finite because then the integers would have a minimum or maximum. But we need to make room for all the numbers less than $N$. We take a number $n$ less than $N$ and insert it in the function. We cannot insert it in the first place because then infinity would be the first place. But if we insert it in a place where it is not the first, then we would have the conclusion that $x<n<y$ where $x,y\geq N$, but $n<x\land n<y$, so we cannot insert it in a place where it is not the first place, meaning that we cannot insert anymore elements without disturbing the ordering. The problem is that I don't know if this is correct or how to formalize it.",['elementary-set-theory']
2601437,A variant of Heron's shortest distance problem (with three points instead of two),"In the question "" Minimum Distance Problem with several points "", the OP has asked for a geometric solution of a generalization of Heron's problem (for $n$ points). I am interested in the (much more modest) special case of three points: ""Given three points $A$, $B$ and $C$ on the same side of a straight line, find $X$, a point on the straight line, such that it minimizes $AX+BX+CX$."" Since with two points one can solve it geometrically by (among many other methods) thinking of an ellipse expanding until it just touches the straight line, I thought that maybe one can solve the three points case by thinking on some other expanding shape (the $n$-ellipse?). P.S. https://www.cut-the-knot.org/Curriculum/Geometry/HeronsProblem.shtml P.P.S. https://en.wikipedia.org/wiki/N-ellipse","['plane-geometry', 'optimization', 'calculus', 'analytic-geometry', 'geometry']"
2601451,Order of operations for multiplying three matrices,"If I have a $1\times 2$ matrix $A$, a $2\times 2$ matrix $B$, and a $2\times 2$ matrix $C$, and am asked to calculate ABC, is there a specific order in which I have to carry out the multiplication? Would I be correct in assuming I have to calculate $AB$ first to get a $1\times 2$ matrix, and then multiply that result by the $2\times 2$ matrix $C$? Or can you also calculate $BC$ first and then multiply that result by $A$?","['matrices', 'multiplicative-function']"
2601460,Every smooth curve is solution of a differential equation,"Let $\gamma:\Bbb{R}\rightarrow \Bbb{R}^n$ be a $C^1$-function with $\gamma(t)\neq 0\ \forall t\in \Bbb{R}$. Then I want to show that there exists a continous function $f:\Bbb{R}\rightarrow \text{End}(\Bbb{R}^n)$ such that $\dot{\gamma}(t)=f(t)\gamma(t).$ For $n=1$ one can simply choose $f(t):=\frac{\dot{\gamma}(t)}{\gamma(t)}$. But what can I do in higher dimensions? I tried to apply the implicit function theorem but to apply it one needs that $A(x,t):=\dot{\gamma}(t)-x\gamma(t)$ is $C^1$, but our hypothesis only yields that this function is continuous. Any help will be greatful appreciated.","['real-analysis', 'ordinary-differential-equations', 'calculus']"
2601463,Calculation problem with Central limit theorem,"Let $X_1,X_2,\dots\,$ i.i.d random variables with mean zero and variance $1$. Let $S_n=\sum_{i=1}^n X_i\,,n\in \mathbb N.$ Compute the weak limes $\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}$ Surely we will have to use the CLT. First I tried to simplify the expression, but I am not sure how to continue here.
$$\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}=\dots=\lim_{ n\to\infty}\frac{1}{\sqrt n} \frac{nX_1+(n-1)X_2+\dots+X_n}{n}$$ Edit(2) According to the comments, we have to verify Lindberg's condition ( https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) Lindberg's condition: $$\lim_{n\to\infty} \frac{1}{s_n^2} \sum_{k=1}^n E[(X_k - \mu_k)^2 \mathbb 1_{\{\mid X_k - \mu_k \mid > \epsilon s_n \}}=0,\quad \text{for all $\epsilon >0$}$$ Here: $E(S_i) {\overset{\text{$X_i$ i.i.d}}{=}}0$ , $Var(S_i) {\overset{\text{$X_i$ i.i.d}}{=}} \sum Var( X_i) {\overset{\text{$X_i$ i.i.d}}{=}} i$ for all $i=1,2,\dots$ Furthermore $s_n^2= \sum_{i=1}^n \sigma_i^2 =Var(S_1)+Var(S_2)+\dots + Var(S_n)=1+2+\dots +n=\frac{n(n+1)}{2}$. Plugging in:
$$\lim_{n\to\infty}\frac{2}{n^2+n}\sum_{k=1}^n E(S_k)^2 1_{\{\mid S_k \mid > \epsilon  {\frac{\sqrt {n^2+n}}{\sqrt 2}}\}}$$ 
Intuitively this does not seem correct to me. Furthermore I am not sure how to simplify this expression. Some help is welcome and obviously needed!","['weak-convergence', 'probability-theory', 'probability', 'central-limit-theorem']"
2601534,"Prove $f(z)=\int_{[0,1]}\frac{1}{1-wz}dw$ is holomorphic in the open unit disk.","Define $f:D[0,1] \rightarrow \mathbb C$ through
$$f(z)=\int_{[0,1]}\frac{1}{1-wz}dw$$
The integration path is from 0 to 1 along the real line. Prove that $f$ is holomorphic in the open unit disk $D[0,1]$. I was trying to use the Cauchy's Integral Formula
$$\begin{aligned}
f'(w) & =\frac{1}{2\pi i}\int_{\gamma}\frac{f(z)}{(z-w)^2}dz\\\
& =\frac{1}{2\pi i}\int_{\gamma}\frac{1}{(z-w)^2}\left[\int^1_0\frac{1}{1-tz}dt\right]dz\\
& =\frac{1}{2\pi i}\int^1_0\left[\int_{\gamma} \frac{\frac{1}{1-tz}}{(z-w)^2}dz\right]dt\\
& =\frac{1}{2\pi i}\int^1_0 \left[\frac{\partial}{\partial z}\frac{1}{1-tz}\left.\right|_{z=w}\right]dt\\
& =\frac{1}{2\pi i}\int^1_0 \frac{t}{(1-tw)^2}dt\\
& =\cdots
\end{aligned}$$ 
But the Cauchy's Integral formula need $f$ to be holomorphic at the first place...Any help with this?
Many Thanks!","['continuity', 'complex-analysis', 'integration', 'contour-integration', 'holomorphic-functions']"
2601563,What Kind of Common Properties Does This Function Exhibit,"Suppose I have a function. Let's call it $f.$ I can describe some of its behavior in words. What I am wondering about is whether some of this behavior can be summarized by some known types of relations such as Reflexivity, transitivity, idempotency, and so on. In essence, I am trying to learn about the properties of this function. Here is what I do know. The function has a variable but FINITE arity; The order of arguments does not matter (commutative?); for my purposes, f(A) is equivalent to A. That is, when there is only one argument, it is the same as if the function has not been applied. When there is more than one argument, the function application ""glues"" these arguments together into a new entity. This is similar to set-union; . f(f(A)) => A f(f(A,B)) => f(A,B) f(A,f(B)) => f(A,B) f(f(A,B,C,...)) => f(A,B,C,...) Some of the properties I listed may be redundant. As you can see, this is something resembling an idempotent function and a set-union function. What else can be said about it? I would also be interested in how one could reduce the properties described above to a smaller set of axiomatic rules/properties from which the above can be reconstructed and built.","['functions', 'equivalence-relations', 'logic', 'idempotents', 'relations']"
2601597,In triangle with incircle prove that $\overline{CQ}$ is parallel to $\overline{AB}$,"We are given a triangle $ABC$ whose incircle touches side $AB$ at point $D$ and side $AC$ at point $E$. Point $P$ lies on segment $AC$ such that segment $IP$ is parallel to the segment $DE$, where $I$ is center of incircle. Lines $DE$ and $BP$ intersect at point $Q$. Prove that $CQ$ is parallel to $AB$. I tried angle chasing, then some ideas with new special points, lines etc, but it doesn't give any solution. I would like any advice, any help. No solution.","['contest-math', 'plane-geometry', 'euclidean-geometry', 'geometry']"
2601610,Optimality of the MSE in gaussian linear regression,"Let's call $\hat{\beta}$ the least squares estimator of $\beta$ in the regression problem $Y = X\beta + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$ . In a statistics course, I get this statement: The MSE of $\beta$ : $\hat{\beta} = (X^TX)^{-1}X^TY$ is the unique minimum variance unbiased estimator of $\beta$ . Where does this come from? It thought about the Crame-Rao lower bound, but it is a tight bound only in the case of a one-parameter exponential family and in our case ( $\beta$ , $\sigma^2$ ) are two parameters. It would not even account for the claimed uniqueness. Moreover, it does not use the Gauss-Markov theorem because, after stating the above, we demonstrate that, if we drop the Gaussian hypothesis and only keep moments assumption, then this theorem is valid.","['regression', 'statistics', 'linear-regression']"
2601632,Description of projective and injective tensor products $\ell^2 \otimes \ell^2$?,"The following question is probably too elementary and/or well-known for MathOverflow, so I'll try here: Let $\ell^2 \mathbin{\hat\otimes_\pi} \ell^2$ and $\ell^2 \mathbin{\hat\otimes_\varepsilon} \ell^2$ refer to the (completed) projective and injective tensor products (as defined, say, in Wikipedia ), as Banach spaces, of the Hilbert space $\ell^2 = \{u\colon\mathbb{N}\to\mathbb{R} : \sum_{k=0}^{+\infty} u_k^2 < +\infty\}$ of square-summable sequences with itself. I understand that it is not easy to describe these spaces, but I wonder if it is still possible to give a reasonably concrete condition for a “sequence of sequences” (i.e., a function $\mathbb{N}^2 \to \mathbb{R}$) to belong to one or the other? More precisely, if we consider the continuous linear form $e_k^*\colon\ell^2\to\mathbb{R}$ which maps $u \in \ell^2$ to its $k$-th term $\langle u, e_k\rangle$, then the tensor product $e_m^* \otimes e_n^*$ defines a continuous linear form of norm $1$ on either $\ell^2 \mathbin{\hat\otimes_\pi} \ell^2$ or $\ell^2 \mathbin{\hat\otimes_\varepsilon} \ell^2$, so an element $v$ in one of these spaces defines an array $J(v)\colon (m,n) \mapsto (e_m^* \otimes e_n^*)(v)$, which belongs to $\ell^\infty(\mathbb{N}^2)$ (the space of bounded functions $\mathbb{N}^2\to\mathbb{R}$).  This, in turn, defines a continuous linear map $J_\alpha \colon \ell^2 \mathbin{\hat\otimes_\alpha} \ell^2 \to \ell^\infty(\mathbb{N}^2)$ (of norm $1$) for $\alpha \in \{\pi,\varepsilon\}$.  I guess I have four questions: Is $J_\pi$ injective?  (Can $\ell^2 \mathbin{\hat\otimes_\pi} \ell^2$ be seen as a space of functions $\mathbb{N}^2\to\mathbb{R}$?) Is $J_\varepsilon$ injective?  (Can $\ell^2 \mathbin{\hat\otimes_\varepsilon} \ell^2$ be seen as a space of functions $\mathbb{N}^2\to\mathbb{R}$?) What is the image of $J_\pi$?  (When does a function $\mathbb{N}^2\to\mathbb{R}$ belong to $\ell^2 \mathbin{\hat\otimes_\pi} \ell^2$?) What is the image of $J_\varepsilon$?  (When does a function $\mathbb{N}^2\to\mathbb{R}$ belong to $\ell^2 \mathbin{\hat\otimes_\varepsilon} \ell^2$?)","['functional-analysis', 'tensor-products', 'banach-spaces']"
2601633,Sinusoidal Function word problems: month and temperature $T(t) = 14.9 \sin {\pi \over 6}(t-3) +13$,"The averge monthly maximum temperature of New York can be modelled by $T(t) = 14.9 \sin {\pi \over 6}(t-3) +13$ where $T$ is the temperature in Celsius and $t=0$ represents January. Predict when the temperature is $0$ $^{\circ}$C I tried this: $$0 = 14.9 \sin {\pi \over 6}(t-3) +13\\
-13 = 14.9 \sin {\pi \over 6}(t-3)\\
-27.9 = \sin {\pi \over 6}(t-3)$$
Let ${\pi \over 6}(t-3)= \theta$ $$\sin \theta = -27.9$$
$$*error$$ Then I was going to substitute the value of $\theta$ back into ${\pi \over 6}(t-3)= \theta$ to obtain the $t$ value. How do I do this question?","['periodic-functions', 'trigonometry', 'word-problem', 'functions']"
2601660,Connection between Riemann and Riemann-Stieltjes Integrals,"Let functions $f$ and $h$ be Riemann integrable and $H(x) = \int_a^x h(t)dt$.  Is it always true that the Riemann-Stieltjes integral $\int_a^bf(x)dH(x)$ exists and $\int_a^b f(x) dH(x) = \int_a^bf(x)h(x) dx$? I remember seeing this used in a reference without a proof. How is it proved? The closest I could find was the more restrictive Theorem 6.17 in Principles of Mathematical Analysis by Rudin. He proves $\int_a^b fd\alpha = \int_a^bf(x) \alpha'(x) dx$ when $\alpha$ is differentiable in $[a,b]$ and the derivative $\alpha'$ is Riemann integrable.","['real-analysis', 'integration', 'stieltjes-integral']"
2601685,Beginner troubleshooting an eigenvector calculation,"I am having some difficulty identifying the error in my eigenvector calculation. I am trying to calculate the final eigenvector for $\lambda_3 = 1$ and am expecting the result $ X_3 = \left(\begin{smallmatrix}-2\\17\\7\end{smallmatrix}\right)$ To begin with, I set up the following equation (for the purpose of this question I will refer to the leftmost matrix here as A). 
$$
\begin{bmatrix}
   1 - \lambda & 0 & 0 \\ 
   3 & 3 - \lambda  & -4\\ 
   -2 & 1 & -\lambda -2 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 \\ 
   x_2\\ 
   x_3 \\
\end{bmatrix}
=
\begin{bmatrix}
0\\ 
0\\ 
0\\
\end{bmatrix}
$$ I) Substitute $\lambda_3 = 1$ $$
\begin{bmatrix}
   0 & 0 & 0 \\ 
   3 & 2  & -4\\ 
   -2 & 1 & -3 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 \\ 
   x_2\\ 
   x_3 \\
\end{bmatrix}
=
\begin{bmatrix}
0\\ 
0\\ 
0\\
\end{bmatrix}
$$ II) Reduce the matrix with elementary row operations. $R_2 \leftarrow R_2 - 2R_3$ $$
A = 
\begin{bmatrix}
   0 & 0 & 0 \\ 
   7 & 0  & 2\\ 
   -2 & 1 & -3 \\
\end{bmatrix}
$$ $R_3 \leftarrow  3R_2 +  2R_3$ $$
A = 
\begin{bmatrix}
   0 & 0 & 0 \\ 
   7 & 0  & 2\\ 
   17 & 2 & 0 \\
\end{bmatrix}
$$ $R_2 \leftarrow  \frac{1}{7} R_2$ $R_3 \leftarrow \frac{1}{17} R_3$$ $$
A = 
\begin{bmatrix}
   0 & 0 & 0 \\ 
   1 & 0  & 2/7\\ 
   1 & 2/17 & 0 \\
\end{bmatrix}
$$ III) multiply matrices to get a series of equations equal to 0 and rearrange them in terms of a common element. $x_1 + \frac{2}{7}x_3 = 0 \rightarrow x_1 = -\frac{2}{7}x_3$ $x_1 + \frac{2}{17}x_2 = 0 \rightarrow x_1 = -\frac{2}{17}x_2$ IV) Substitute a value into the vector to get an eigenvector. Let $\ x_1 = 1 \rightarrow X_3 =  \left(\begin{smallmatrix}1\\-2/17\\-2/7\end{smallmatrix}\right)
$ Which at this point we can see is not a multiple of the expected $X_3$. Can anyone highlight my error for me? Many thanks in advance.","['matrices', 'eigenvalues-eigenvectors']"
2601708,convergence of square summable sequence,"I have seen a very interensting but for me confusing statement. It says that if $\sum\limits_{n=1}^{\infty} a_{n}^{2} < \infty,$
then necessarily holds that $\lim\limits_{n \rightarrow \infty} a_{n}=0$ (from neccesary condition of convergence of sequence). But i don't understand, how is connected mentioned condition and convergence of squared summable sequence? I mean i know, that if the series of the sequence $a_{n}$ converges then $\lim\limits_{n \rightarrow \infty} a_{n}=0,$ but does it hold for the mentioned statement as well?","['real-analysis', 'summation', 'convergence-divergence', 'limits']"
2601709,Conditional probability and intersections of events,"Conditional probability is : $$P(A\mid B)= \frac{P\left(A \bigcap B\right)}{P(B)}$$
Why did I find this as a solution of an exercise :
$$P\left(\left(E_1 \bigcap E_2 \bigcap E_3\right)\mid H\right) = P\left(E_1\mid H\right) \cdot P\left(E_2\mid \left(E_1 \bigcap H\right)\right) \cdot P\left(E_3\mid \left(E_1 \bigcap E_2 \bigcap H\right)\right)$$
I have two questions : where's the P(B) of the definition in the denominator? how do I go ahead when I have an intersection of more than 1 event given an event in the conditional probability?","['probability-theory', 'probability', 'discrete-mathematics']"
2601711,Is this a valid proof that there are infinitely many natural numbers?,"I remember reading a simple proof that natural numbers are infinite which goes like the following: Let $ℕ$ be the set of natural numbers. Assume that $ℕ$ is finite. Now consider an arbitrary number $K$, where $K$
is the largest number in $ℕ$. $K+1$ is also a natural number such that $K+1 > K$. Therefore, $ℕ$ cannot be finite. Is this a valid proof? And if so, how can the 3rd step be valid when we assumed in the 2nd step that $K$ is the largest number in $ℕ$? I understand this is a proof by contradiction (wrong?), but if we initially assume $K$ to be the largest number, then we cannot simply assume that there is such a number as $K+1$ later!","['elementary-set-theory', 'proof-explanation']"
2601719,Solve $z''+z=x$,"Considering the ansats $z=ax+b$. If I'm missing one degree in the LHS in the differential equation, shouln't I multiply my ansatz by $x$ and get the ansatz and instead get $ax^2+bx$? If not, when does this apply?","['ordinary-differential-equations', 'calculus']"
2601722,Proving the Lie derivative of a vector field with respect to another is a smooth vector field,"Suppose $M$ is a smooth manifold and $V,W$ are smooth vector fields on $M$. Let $L_V W$ denote the Lie derivative of $W$ with respect to $V$. Then $(L_V W)_p$ exists for every $p \in M$ and $(L_V W)$ is a smooth vector field. The proof begins as follows: Let $\theta$ be the flow of $V$. For arbitrary $p \in M$, let $(U,(x^i))$ be a smooth chart containing $p$. Choose an open interval $J_0$ containing 0 and an open subset $U_0 \subset U$ contatining $p$ such that $\theta$ maps $J_0 \times U_0$ into $U$. For $(t,x) \in J_0 \times U_0$, write the component functions of $\theta$ as $(\theta^1(t,x)...\theta^n(t,x))$. Then for any $(t,x) \in J_0 \times U_0$, the matrix of $d(\theta_{-t})_{\theta_{t}(x)}: T_{\theta_{t}(x)}M \rightarrow T_xM$ is $\bigg(\frac{\partial \theta^i}{\partial x^j}(-t,\theta(t,x))\bigg)$ I am confused how to compute this matrix. In particular, I know that the matrix of a map is the Jacobian of the (coordinate representation) of the map. But in particular how do I arrive at it being evaluated at $(-t,\theta(t,x))$? Help would be greatly appreciated !","['lie-derivative', 'differential-geometry']"
2601756,"Looking for a $2\times2$ real matrix $A$ with $Ax$ a contraction for the supremum norm, and not a contraction for the one norm","I am looking for a $2\times2$ real matrix $A$, such that 
$ x\longmapsto Ax $ is a contraction considering  $\|\cdot\|_\infty$ and a noncontraction considering $\|\cdot\|_1$. I have now idea how to solve this.
Thank you.","['matrices', 'linear-algebra', 'contraction-operator']"
2601768,Probability of winning an election while losing the popular vote: electorates of size 3,"Suppose we live in a country with an interesting electoral system: each electorate has exactly 3 voters. 2 parties run for office, and each voter has a 50/50 chance of voting for each party. Whoever wins the majority of electorates wins the election overall. Given an arbitrarily large number of electorates, what is the probability that the party that won the election lost the popular vote? (This is a more specific version of my earlier question Probability of winning an election while losing the popular vote ) My 'brute force' computer model yields an answer of very close to 1/6th. Does anyone have ideas for how to solve this problem analytically?","['voting-theory', 'probability']"
2601770,Computing the Frechet differentials,"I am new to differential calculus on normed spaces and I struggle with some easy things. Let $- \infty <a < b< +\infty$ and $[a, b]$ denote a finite interval. Let $C[a,b]$ denote the collection of all real-valued continuous
  functions defined on $[a,b]$. Then, endowed with the usual choice of
  norm $\|x\| = \max_{a\leq t \leq b} |x(t)|$, $C[a,b]$ is a Banach
  space. Let $\phi \colon \mathbb{R} \to \mathbb{R}$ be a twice differentiable
  function, and suppose that its inverse $\phi^{-1} \colon \mathbb{R}
 \to \mathbb{R}$ exists and is still twice differentiable. Let the kernel function $K \colon [a,b] \times [a,b] \to \mathbb{R}$ be continuous. Does the Frechet derivative of the
  following operator $A$ exist? $$ [A(x)](s) = 
  \int^b_a K(s,t) \, [ x(t)] \,\mathrm{d}t $$ for all
  $x \in C[a,b]$ and for all $s \in [a,b]$. Any ideas or suggestions are much appreciated! Thanks in advance:)","['derivatives', 'real-analysis', 'calculus', 'functional-analysis', 'ordinary-differential-equations']"
2601776,Show $x\mapsto x/\sqrt{1+\vert x\vert^2}$ is diffeomorphism,"Let $H$ be an euclidean Hilbert space, and consider $$f\colon H\to H,\quad x\mapsto\frac{x}{\sqrt{1+\vert x\vert^2}}.$$ What are $Y:=\operatorname{im}(f)$, $f^{-1}$, and $\partial f$? Show that $f\in\operatorname{Diff}^\infty(H,Y)$. So, it should be $$\partial f(x)=\frac{\sqrt{1+\vert x\vert^2}\operatorname{id}-1/\sqrt{1+\vert x\vert^2}\langle\cdot,x\rangle x}{1+\vert x\vert^2}.$$ Is showing $(\partial f(x)h=0\Leftrightarrow h=0)$ directly, using the numerator, the right way to continue? I wasn't able to... Next, what are $Y$ and $f^{-1}$?","['derivatives', 'real-analysis', 'diffeomorphism', 'hilbert-spaces']"
2601794,Unique bounding rectangle.,"Let $K\in\mathbb{R}^{2}$ be compact and connected. Then we can always find rectangles that bound $K$. Moreover, we can find the minimum-area such a bounding rectangle could have. My question is this: Is it true that if there are two or more minimum-area bounding rectangles, then those rectangles are squares? Any pointing to the right direction would be much appreciated.","['functional-analysis', 'real-analysis', 'geometry']"
2601797,"Discrete, finite L-moment problem","Suppose that we have a real-valued discrete random variable, whose probability distribution has finite support on some set $S$ of real numbers. Then if $N = |S|$, we know that we can construct the entire distribution from the first $N$ raw moments, as described in this paper: https://www.sciencedirect.com/science/article/pii/0166218X9090068N The transformation is a simple Vandermonde matrix that converts from moments to probabilities. Suppose that we instead want to use the L-moments. Is there an analogous result where we can completely reconstruct the distribution using only the first $N$ L-moments, and if so, what does the resulting matrix look like? To be specific, I am looking for the basis specified by the matrix that solves this problem in the discrete, finite case. For the classical (raw) moments, the basis is the monomials up to order $N$. I know that Bernstein polynomials are often mentioned in connection with the L-moments, although I'm not sure if this helps here. I also understand that we can use the L-moments to reconstruct the quantile function, but I'm not sure how many L-moments are needed to reconstruct the entire thing, nor how this translates into a basis for the discrete finite probability distribution.","['statistics', 'moment-problem', 'order-statistics']"
2601812,Eigenvalue of a special random matrix?,"Given a $n \times n$ symmetric random matrix whose diagonal elements are $1$, and the elements of $k\times k, k<n$ leading principal sub-matrix are $1$. All other values are i.i.d. uniformly randomly drawn from $[0,1]$. For example, it could look like $\left( {\begin{array}{*{20}{c}}
  1&1&*&* \\ 
  1&1&*&* \\ 
  *&*&1&* \\ 
  *&*&*&1 
\end{array}} \right)$, where ""$*$"" is a decimal uniformly drawn from [0,1]. Anyone knows what this type of random matrix this, and its eigenvalue distribution, and the probability that its largest eigenvalue are bigger than $\frac{n}{2}$? Anyone can provide any reference to solve this problem? Thanks! PS: I have been looking into random matrix materials, and they seem to assume Gaussian random matrix rather than this drawn from uniform. In Gaussian case the eigenvalue distribution is semi-circle. PS2: I find this simulation. If every element is uniform from [0,1], a simulation shows the eigenvalue is possibly still semi-circle. https://blogs.sas.com/content/iml/2012/05/16/the-curious-case-of-random-eigenvalues.html . I am still searching proof for this simpler case. Our case is more complicated with the top-left principal sub-matrix has all its elements being 1.","['matrices', 'probability-theory', 'probability', 'random-matrices']"
2601835,Assorted Questions from a proof in Evans’ book regarding the Laplace and Poisson’s Equations,"I’m trying to understand a proof in Evan’s book, “Partial Differential Equations.” On page 23, he states If $u$ is the fundamental solution to Laplace’s Equation then $ u \in C^2(\mathbb{R}^n)$ and $u$ satisfies Poisson’s Equation. I have assorted questions throughout the course of the proof which I couldn’t find the answers to anywhere on stack exchange. I’m also not sure how to google them because they’re so specific. We have 
$$u(x) = \int_{\mathbb{R}^n} \Phi(x-y) f(y) \ dy =\int_{\mathbb{R}^n} f(x-y) \Phi(y) \ dy. \ \ (1)$$
My first question arises in this equality: Why is it true? I know the reasoning above the proof in the text suggests that $u(x)$ is the convolution of the fundamental solution and $f(x),$ but why is it justified to change their arguments and let them equal one another? Next we have
$$\frac{u(x+he_i)-u(x)}{h} = \int_{\mathbb{R}^n} \Phi(y) \left[\frac{f(x+he_i-y) -f(x-y)}{h}\right]dy. \ \ (2)$$ I believe what he did here (correct me if I’m wrong) is approximate the derivatives on either side. He then writes
$$\frac{\partial u}{\partial x_i}(x) = \int_{\mathbb{R}^n} \Phi(y) \frac{\partial f}{\partial x_i} \ dy. \ \ (3)$$
He uses the fact that $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{\partial x_i}(x-y)$ as $h \rightarrow 0$ which I also don’t see because at $h = 0,$ the $\frac{f(x+he_i-y)-f(x-y)}{h}$  term blows up. Once again, I tried but, given its specificity, finding the answers to these questions online is impossible. Now, he computes the second partial derivative and concludes $u \in C^2(\mathbb{R}^n).$ The other half of the proof I understand. I was able to find existing questions on this site pertaining to that. I know I asked a lot of questions, so to recap: Why is it justified to switch the arguments of the functions $\Phi$ and $f$ in equation (1)? Why are the two integrals equal? Is it true that all Evans did in equation two is change the differential terms to approximations, or is there more to it than that? Why is equation (3) true? Why does $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{x_i}$ as $h \rightarrow 0$ even with the rational term blowing up at $h = 0 Thanks in advance.","['multivariable-calculus', 'calculus', 'partial-differential-equations']"
2601890,Properties of Jordan Measure Sets (Measure Theory),"I've been having a bit of trouble with Exercise 1.1.6 in Terence Tao's book ""An Introduction to Measure Theory"". Picture of the problem can be found here . There's a theorem (in the form of a previous exercise), that seems to be helpful. Pic here . I tried applying (1)->(2) from this theorem with $A_1$ ⊂ E ⊂ $B_1$ and $A_2$ ⊂ F ⊂ $B_2$, but to no avail. Any tips on how to proceed would be appreciated.","['real-analysis', 'measure-theory']"
2601900,Question about conditional expectation as projection,"I'm studying through a book on probability and measure theory, and I got stuck on the definition of conditional expectations which are presented as projections onto subspaces. Given the measure $(\Omega, \mathcal{F}, P)$ and a sub-sigma algebra $\mathcal{B} \subset \mathcal{F}$, we take the random variable $Y \in L^2(\Omega, \mathcal{F}, P)$ and define $E[Y|\mathcal{B}]$ as the projection of $Y$ to the subspace $L^2(\Omega, \mathcal{B}, P)$. However, in order to prove the projection exists, we need to establish that the subspace $L^2(\Omega, \mathcal{B}, P)$ is closed. The brief justification for the closedness given in the book is that convergence in $L^2$ of a sequence of random variables implies that there exists a subsequence which converges almost surely. How does this imply that the subspace $L^2(\Omega, \mathcal{B}, P)$ is closed? Apologies if the question is too elementary.","['probability-theory', 'measure-theory', 'hilbert-spaces']"
2601907,Problem on Principle of Inclusion-Exclusion,"How many integers $1, 2,....., 11000$ are invertible modulo $880$? $880$ can be rewritten as $2^4\cdot5\cdot11$. So I am supposed to find the number of integers in this range that have $2$, $5$ or $11$ as a divisor and then subtract that value from $11000$. So If I divide $11000$ by each of $2$, $5$ and $11$, I get cardinalities of $5500$, $2200$, and $1000$ respectively. But how exactly am I supposed to find how many integers there are that have both $2$ and $5$ as a divisor, $2$ and $11$ as a divisor, and $5$ and $11$ as a divisor? How am I supposed to find the amount of integers that have all three numbers as a divisor? Any help?","['number-theory', 'combinatorics', 'inclusion-exclusion', 'discrete-mathematics']"
2601974,"Uniform convergence of $n\sin\sqrt{4\pi^2n^2+x^2}$ on $[0,a]$ and $\mathbb{R}.$","Let $f_n(x)=n\sin\sqrt{4\pi^2n^2+x^2}$. Prove that $(f_n)$ is uniformly convergent on $[0,a]$ for every $a>0.$ Is the convergence uniform on $\mathbb{R}$? Attempt. For $x\in \mathbb{R}$ constant we have: $$f_n(x)=n\sin(\sqrt{4\pi^2n^2+x^2}-2\pi n)=n\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)=
\frac{\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)}{\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}}\,\frac{n\,x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\rightarrow 1\cdot \frac{x^2}{4\pi}=\frac{x^2}{4\pi},$$
so we are done with the pointwise convergence. So far I am stuck on the uniform convergence, in other words how to prove $$\max_{0\leq  x\leq a}|f_n(x)-x^2/(4\pi)|\rightarrow 0$$ and $$\sup_{x\in \mathbb{R}}|f_n(x)-x^2/(4\pi)|=\sup_{x\geq 0}|f_n(x)-x^2/(4\pi)|\nrightarrow 0.$$ Thanks for the help.","['real-analysis', 'pointwise-convergence', 'uniform-convergence', 'analysis']"
2601985,Intuitive Explanation for Distribution of Sum of Independent Discrete Uniform Random Variables,"So apparently the distribution of the sum of i.i.d discrete uniform random variables on $\lbrace 0,1,2,...,k \rbrace$ is given by this: PMF $$P(Y=y) = \frac{1}{(k+1)^n}{n \choose y}_{k+1},\;y = \{0,1,\dots,nk\}$$ Can somebody please give an intuitive explanation for this?  I'm not quite sure how to interpret it or why the Gaussian Binomial Coefficient is used.","['independence', 'probability-theory', 'probability', 'probability-distributions']"
2602021,Prove that a function is both odd as well as even.,"Consider the function $$f(x) =
\begin{cases}
e^{-\sqrt {\vert ln \{x\}\vert}}- \{x\}^{\sqrt {1/\vert ln \{x\}\vert}},  & \text{wherever it exists} \\
\{x\}, & \text{otherwise}
\end{cases}$$ where $\{x\} $ denotes fractional part of $x$. Prove that $f(x)$ is odd as well as even. I am not getting any idea over how to start approaching the problem. I tried using $f(-x) +f(-x)$  to see whether odd or not but got messed up badly.  Somebody please help.","['algebra-precalculus', 'functions']"
2602035,"Prove that $\sum\limits_{cyc}\frac{a}{a^{11}+1}\leq\frac{3}{2}$ for $a, b, c > 0$ with $abc = 1$","Let $a$ , $b$ and $c$ be positive numbers such  that $abc=1$ . Prove that: $$\frac{a}{a^{11}+1}+\frac{b}{b^{11}+1}+\frac{c}{c^{11}+1}\leq\frac{3}{2}.$$ I tried homogenization and the BW ( https://artofproblemsolving.com/community/c6h522084 ), but it does not work. Indeed, let $a=\frac{x}{y}$ , $b=\frac{y}{z}$ , where $x$ , $y$ and $z$ are positives. Hence, $c=\frac{z}{x}$ and we need to prove that $$\sum_{cyc}\frac{xy^{10}}{x^{11}+y^{11}}\leq\frac{3}{2},$$ which has a problem around $(x,y,z)=(7,5,6)$ . For these values $$\frac{3}{2}-\sum_{cyc}\frac{xy^{10}}{x^{11}+y^{11}}=0.0075...$$ I tried also TL, uvw, C-S, Lagrange multipliers and more, but without success. Also, Vasc's Theorems don't help. Also, the following method does not help here. Find the maximum of the expression Because the inequality $\frac{x}{x^{11}+1}\leq\frac{3(a^9+1)}{4(a^{18}+a^9+1)}$ is wrong.","['multivariable-calculus', 'real-analysis', 'inequality', 'symmetric-polynomials']"
2602044,prove the root of $(z-2018)^{2n}+(z+2018)^{2n}=0$ is purely imaginary,"Prove: if $z \in \mathbb{C}$ satisfies $(z-2018)^{2n}+(z+2018)^{2n}=0$,then $z=bi$ for some $b \in \mathbb{R} (b \neq 0)$. The method i can think of is use the binomial theorem to get:
$$\sum_{k=0}^{2n} \binom{2n}{k}z^{2n-k}(-2018)^{k}=-\sum_{k=0}^{2n} \binom{2n}{k}z^{2n-k}(2018)^{k} $$ i feel like it must be that the imaginary part of z makes this equality holds, but it seems like there is no way to prove this argument. i guess the $2n$ as the exponential must be the key to solve this, but i can't see how","['complex-analysis', 'sequences-and-series', 'complex-numbers']"
2602062,"if $2017 \mid\sum_{i=1}^{1000}x^k_{i}$,show $2017 \mid x_{i},\forall i=1,2,\ldots,1000$","Let $x_{i}(i=1,2,\ldots,1000)$ be integers,and for all postive integers  $k\le 672$,such $$2017 \mid\sum_{i=1}^{1000}x^k_{i}$$ show that
$$2017 \mid x_{i},\forall i=1,2,\ldots,1000$$ maybe is use Newton's identities to solve it? I remember this is old problem for any prime number $p=2017$  .
But I can't solve it","['number-theory', 'divisibility']"
2602069,Uniqueness of tempered fundamental solution (with support in half space) for the linear KdV equation,"Show that $E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$ is the unique tempered fundamental solution with support in $\{ (t,x) \mid t\geq 0 \}$ of the partial differential operator $P(D) = \partial_t + \frac{1}{3}\partial_x^3$. I am puzzled about an exercise problem from an undergraduate course about distributions and Fourier transfrom. The exercise concerns about the constant-coefficients 1+1 dimensional differential operator 
$$P(D) = \partial_t + \frac{1}{3}\partial_x^3.$$ 
The first part of the exercise is to check that 
$$E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$$
is a fundamental solution of $P(D)$, where $\textbf{Ai}(x) = \mathcal{F}^{-1}_{\xi \rightarrow x}(e^{i\xi^3/3})$ is the Airy function (it turns out that $\textbf{Ai}$ is $C^\infty$). This part is straightforward and I could work out. But the second part of the exercise asks to show that $u(t,x) = E(t,x)$ is the unique solution in the sense that
$$  u \in \mathcal{S}'(\mathbb{R}^2_{t,x}) \\ (\partial_t + \frac{1}{3}\partial_x^3)u = \delta_{(0,0)}  \\  \textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$$ I could not work out. I have consulted Hormander's book 
$$\textit{The Analysis of Linear Partial Differential Operators II}$$
 and found some topics very close to it, mostly in Chap. XII ""The Cauchy and Mixed Problems"", but it seems that there is no theorem I could directly quote. $\textbf{My Question}$ Is there any general theorem about the uniqueness of tempered fundamental solutions of linear differential operators with constant coefficients? Is there any necessary or sufficient conditions such that $P(D)u = \delta$ has a unique tempered solution $u$, maybe with some requirement of the support $\textbf{supp}~u$ (for instance $\textbf{supp}~u \subset \text{half space}$ like above.) Does the requirement $\textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$  take the most important role so that actually this exercise is very easy?","['distribution-theory', 'fourier-analysis', 'analysis', 'partial-differential-equations']"
2602083,Differential equation $y'=-\frac{x+y}{x+2y}$,"Differential equation. How do you solve this ? $$y'=-\frac{x+y}{x+2y}$$ set $u = \dfrac yx$, then $y' = u'x + u$, $$u'x + u = -\frac{1+u}{1+2u}$$
$$u'x = -\frac{2u^2+2u+1}{1+2u}$$
$$\frac{(1+2u)u'}{2u^2+2u+1} = -\frac1x$$ I solved it so far but I do not know anymore",['ordinary-differential-equations']
2602087,Definition of extreme point which involve two norm $1$ points,"I migrate the following post from Maths Overflow. Let $E$ be a Banach space. 
Denote $B_E$ and $S_E$ to be the unit ball and sphere of $E$ respectively. A common definition of extreme point of $B_E$ that I have came across is the following: Definition $1$ $x$ is an extreme point of $B_E$ if $$x = \frac{1}{2}(y+z)$$ for some $y,z\in B_E,$ then $x=y=z.$ I have seen another definition of extreme point which involves $y$ and $z$ to be norm $1.$ For example, Lei Li et al. paper, page $549,$ paragraph starts with 'On the other hand,...' and Botelho's et al. paper, page $823,$ first sentence of the proof of Theorem $3$ . They use the following definition of extreme point: Definition $2$ $x$ is an extreme point of $B_E$ if 
  $$x = \frac{1}{2} (y+z)$$ for some $y,z\in S_E,$ then $x=y=z.$ I can prove that Definition $1$ implies Definition $2.$
But I am not able to prove its converse. 
So here is my question: Question: Does Definition $2$ imply Definition $1?$ If yes, can prove a proof or reference? Otherwise, can provide counterexample? ................................................................................................................................................................... Based on the discussion at Maths Overflow, it seems that Definition $2$ implies Definition $1.$
To prove it, it suffices to prove that Definition $2$ implies that $\|x\|=1.$","['functional-analysis', 'reference-request', 'real-analysis', 'banach-spaces']"
2602113,$ABCD$ is parallelogram. $\angle ABC=105^{\circ}$. $\angle CMD=135^\circ$. Find $\angle BKC$,"$ABCD$ is parallelogram. $\angle ABC=105^{\circ}$. $BMC$ is equilateral triangle and $\angle CMD=135^\circ$. $K$ is midpoint of $AB$. Find $\angle BKC$ My attempts 1) $\angle MBC=\angle MCB=\angle BMC=60^{\circ}$ 2) $\angle MCD=15^{\circ}$, $\angle MDB=30^{\circ}$ I want to prove that $MB \perp CK$ but I need help here","['euclidean-geometry', 'geometry']"
