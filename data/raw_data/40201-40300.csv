question_id,title,body,tags
395051,Upper and lower integration inequality,"I would like to learn how to prove that the following inequality holds. Let $F$ be a bounded function on an interval $[a,b]$, so that there exists $B\geq 0$ such that $|f(x)| \leq B$ for every $x\in [a,b]$. Show that 
$[
U(f^2,P) -L(f^2,P) \leq 2B [ U(f,P) -L(f,P) ]
]$
for all partitions $P$ of $[a,b]$.","['partitions-for-integration', 'calculus', 'integration', 'real-analysis']"
395053,$k$-th moment of product of gaussian and sinc,"I would like to calculate the following integrals: $$\int_{-\infty}^{+\infty} \quad x^k\quad \left(\frac{\sin(\pi a x)}{\pi ax}\right)^2\quad \exp(-bx^2)\,dx$$ $$\int_{-\infty}^{+\infty} \quad x^k\quad \left(\frac{\sin(\pi a x\pm\pi)}{\pi ax\pm\pi}\right)^2\quad \exp(-bx^2) \,dx$$ Thanks!","['definite-integrals', 'special-functions', 'integration']"
395071,Evaluating the integral: $\int_{0}^{\infty} \frac{|2-2\cos(x)-x\sin(x)|}{x^4}~dx$,"I am interested in evaluating the following integral:
$$
\int_{0}^{\infty} \frac{|2-2\cos(x)-x\sin(x)|}{x^4}~dx
$$
Using Matlab, Numerically it seems that the integral is convergent, 
but I'm not sure about it. How can we prove that the integral is
convergent or not? Many Thanks in advance.",['integration']
395100,What will be the support of the convolution of two test functions.,"If $g\in C^{\infty}_c$ defined on $\Bbb R^n$ and K is the support of function $g$. I want to find the support of $g_\epsilon$. Where $g_\epsilon$ is regularization of $g$. Regularization of $g$ is defined as: $g_\epsilon$:=  $g*\omega_\epsilon$ (convolution of g and a test function) i.e. $g_\epsilon(x
)$=$\int_{\Bbb R^n}g(x-y).\omega_\epsilon(y)dy $ I think, this process is also known as mollification. Here 
$ \omega_\epsilon (x)= 
\begin{cases}
\frac{C^{-1}}{\epsilon^n} e^{{\frac{-\epsilon^2}{{\epsilon^2}-|x|^2}}},  &\text{for |x|< $\epsilon$ } \\
0, & \text{otherwise}  \\
\end{cases} $ I only know that support of convolution of two compactly supported functions is again a compact set. And, as support of $\omega_\epsilon $ is a ball $B(0,\epsilon)$ and support of g is a compact set K(given) then support of their convolution should be the intersection of the support of $g(x-y)$ and $\omega_\epsilon(y)$. 
Because for non zero function integration remains non zero...","['convolution', 'fourier-analysis', 'functional-analysis', 'lp-spaces']"
395110,Set Notation (Axiom of Replacement),"This question is related to the one I asked yesterday here in that it's related to another one of the Zermelo-Fraenkel Axioms . After looking over the notation used to describe the axiom, that is: $$ \forall \space x \space \forall \space y \space \forall \space z \space [\varphi (x,y,p) \wedge \varphi(x,z,p) \Rightarrow y = z] \Rightarrow \forall \space X \space \exists \space Y \space \forall \space y \space [y \in Y \equiv (\exists \space x \in X) \varphi(x, y, p) ]  $$ I believe I understand most of it, but I'm unsure of why we need to involve the variable z, so I thought I'd just write how I'm interpreting this and have someone correct me where it starts to get fuzzy. Current Interpretation: For all the elements of the three sets $X, Y, Z,$ if the property $\varphi$ holds under some parameter $p$ for $x, y $ and $x, z$ conjointly implies that $y$ equals $z$ then for any set X there exists a set Y such that for any element of $Y$ there exists an element of X such that property $\varphi$ holds under both the element of $Y$ and the chosen element of $X$ for that property $p$. What I'm confused about is the purpose of the extra parameter $p$ and the set $Z$ why couldn't you just say something like this: $$ \forall \space x \space \forall \space y \space \varphi (x,y) \Rightarrow \forall \space X \space \exists \space Y \space \forall \space y \space [y \in Y \equiv (\exists \space x \in X) \varphi(x, y, p) ]  $$ What am I missing here? Also if someone could clear up my interpretation that would be awesome.","['notation', 'elementary-set-theory']"
395121,How Entropy scales with sample size,"For a discrete probability distribution, the entropy is defined as:
$$H(p) = \sum_i p(x_i) \log(p(x_i))$$
I'm trying to use the entropy as a measure of how ""flat / noisy"" vs. ""peaked"" a distribution is, where smaller entropy corresponds to more ""peakedness"". I want to use a cutoff threshold to decide which distributions are ""peaked"" and which are ""flat"". The problem with this approach is that for ""same shaped"" distributions, the entropy is different for different sample sizes! as a simple example take the uniform distribution - it's entropy is:
$$p_i = \frac{1}{n}\ \ \to \ \ H = \log n$$
To make things worse, there doesn't seem to be a general rule for more complex distributions. So, the question is: How should I normalize the entropy so that I get the same ""scaled entropy"" for ""same"" distributions irrespective of the sample size?","['probability-distributions', 'entropy', 'discrete-mathematics']"
395127,Invariant submanifolds,"Let $M$ be a smooth manifold, and let $N$ be a submanifold.  Let $V$ be a smooth vector field on $M$ which generates a flow $\Phi_t$ on $M$.  My intuition tells me (perhaps modulo some technical assumptions) that the following is true: If $V(p)$ is tangent to $N$ for all $p\in N$, then $N$ is an invariant submanifold of $\Phi_t$. Is this true? What sorts of technical assumptions would I need to worry about to make the statement rigorous?  I imagine, for example, that there could be global topological issues so that perhaps the statement only holds locally. Is there a good (basic) reference on invariant submanifolds?","['manifolds', 'ordinary-differential-equations', 'differential-geometry']"
395130,Dimensions of vector subspaces in a direct sum are additive,"$V = U_1\oplus U_2~\oplus~...~ \oplus~ U_n~(\dim V < ∞)$ $\implies \dim V = \dim U_1 + \dim U_2 + ... + \dim U_n.$ [Using the result if $B_i$ is a basis of $U_i$ then $\cup_{i=1}^n B_i$ is a basis of $V$] Then it suffices to show $U_i\cap U_j-\{0\}=\emptyset$ for $i\ne j.$ If not, let $v\in U_i\cap U_j-\{0\}.$ Then
\begin{align*}
v=&0\,(\in U_1)+0\,(\in U_2)\,+\ldots+0\,(\in U_{i-1})+v\,(\in U_{i})+0\,(\in U_{i+1})+\ldots\\
& +\,0\,(\in U_j)+\ldots+0\,(\in U_{n})\\
=&0\,(\in U_1)+0\,(\in U_2)+\ldots+0\,(\in U_i)+\ldots+0\,(\in U_{j-1})+\,v(\in U_{j})\\
& +\,0\,(\in U_{j+1})+\ldots+0\,(\in U_{n}).
\end{align*}
Hence $v$ fails to have a unique linear sum of elements of $U_i's.$ Hence etc ... Am I right?",['linear-algebra']
395136,Arrangements of affine hyperplanes,"Fix $n>0$ and $X\subseteq\mathbb{R}^n$. Call a function $f:X\longrightarrow \mathbb{R}$ linear if it is of the form
$$
f(\bar{x})=a_1x_1+\ldots+a_nx_n+b
$$
for some $a_i,b\in\mathbb{R}$. Now suppose we have linear functions $f_1,\ldots,f_t$ and $g_1,\ldots,g_t$ with the following property: For all $i\in\{1,\ldots,t\}$ and for all $\bar{x}\in X$, $$
\textrm{there are }j,k\in\{1,\ldots,t\}\textrm{ such that }f_i(\bar{x})\leq g_j(\bar{x})\textrm{ and }f_k(\bar{x})\leq g_i(\bar{x}).
$$ Is it true that there must be some $i,j\in\{1,\ldots,t\}$ such that $f_i(\bar{x})\leq g_j(\bar{x})$ for all $\bar{x}\in X$?","['linear-algebra', 'algebraic-geometry']"
395139,Combinatorial proof of a Stirling number identity.,"Consider the identity $$\sum_{k=0}^n (-1)^kk!{n \brace k} = (-1)^n$$ where ${n\brace k}$ is a Stirling number of the second kind. This is slightly reminiscent of the binomial identity $$\sum_{k=0}^n(-1)^k\binom{n}{k} = 0$$ which essentially states that the number of even subsets of a set is equal to the number of odd subsets. Now there is an easy proof of the binomial identity using symmetric differences to biject between even and odd subsets. I am wondering if there is an analogous combinatorial interpretation for the Stirling numbers. The term $k!{n\brace k}$ counts the number of set partitions of an $n$ element set into $k$ ordered parts. Perhaps there is something relating odd ordered partitions with even ordered partitions? As an added note, there is a similar identity $$\sum_{k=1}^n(-1)^k(k-1)!{n\brace k}=0$$ for $n \geq 2$ .
A combinatorial interpretation of this one would also be appreciated.","['stirling-numbers', 'summation', 'combinatorial-proofs', 'combinatorics']"
395146,Integrate $\int {{{\left( {\cot x - \tan x} \right)}^2}dx} $,"$\eqalign{
  & \int {{{\left( {\cot x - \tan x} \right)}^2}dx}   \cr 
  &  = {\int {\left( {{{\cos x} \over {\sin x}} - {{\sin x} \over {\cos x}}} \right)} ^2}dx  \cr 
  &  = {\int {\left( {{{{{\cos }^2}x - {{\sin }^2}x} \over {\sin x\cos x}}} \right)} ^2}dx  \cr 
  &  = \int {{{\left( {{{\cos 2x} \over {{1 \over 2}\sin 2x}}} \right)}^2}dx}   \cr 
  &  = \int {{{\left( {2\cot 2x} \right)}^2}}   \cr 
  &  = \int {4{{\cot }^2}2xdx}   \cr 
  &  = \int {4\left( {{{\csc }^2}2x - 1} \right)dx}   \cr 
  &  = \int {\left(4{{\csc }^2}2x - 4\right)dx}   \cr 
  &  = 4 \times {{ - 1} \over 2}\cot 2x - 4x + C  \cr 
  &  =  - 2\cot 2x - 4x + C \cr} $ Where have I gone wrong? I've tried to spot an error so many times yet I can't find it, I need another pair of eyes.. Thanks.","['trigonometry', 'integration']"
395148,"Integration $\int \left(x-\frac{1}{2x} \right)^2\,dx $","Evaluate $$\int\!\left(x-\frac{1}{2x} \right)^2\,dx. $$ Using integrating by substitution, I got $u=x-\frac{1}{2x},\quad \dfrac{du}{dx} =1+ \frac{1}{2x^2}$ , and $dx= 1+2x^2 du$. In the end, I came up with the answer to the integral as :
$$\left(\frac{1}{3}+\frac{2x^2}{3}\right)\left(x-\frac{1}{2x}\right)^3.$$ Any mistake ?","['calculus', 'integration', 'indefinite-integrals']"
395157,"""Nearly"" Harmonic Series","It's well known that
$$
\sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon}} < \infty, \ \forall \varepsilon >0.
$$ What happens if we replace $\varepsilon$ with $\varepsilon_n \downarrow 0$? WolframAlpha says 
$$
\sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon_n}}
$$ converges for
$$
\varepsilon_n = \frac{1}{\sqrt{\log(n+1)}}
$$ and diverges for
$$
\varepsilon_n = \frac{1}{\log(n+1)}.
$$ So the question is: can we find a ""borderline"" decrease rate such that the series converge for $\varepsilon_n$ approaching zero slower than it and diverges if $\varepsilon_n$ decreases faster?","['convergence-divergence', 'sequences-and-series', 'real-analysis', 'analysis']"
395161,What's the difference between an initial value problem and a boundary value problem?,"I don't really see the difference, because in both case we need to determine y and the values of the constants. The only difference is that we give the value of y and y' in the former and the value of either 2 y or 2 y' in the latter. I solve both problems the same way. I don't really understand the theory, I guess.",['ordinary-differential-equations']
395166,"Is the set of submonoids of $(\Bbb N,+)$ countable?","Having the monoid $(\Bbb N,+)$, I wonder if there are countable many submonoids. There are obviously infinitely many since $S_n = \{kn \mid k \in \Bbb N\}$ is a submonoid for any $n \in \Bbb N$. My conjecture is that the set of all submonoids is countable, because I think the following statements (which I failed to prove so far) hold for any submonoid $S$ of $(\Bbb N,+)$: there is an odd element in $S$ $\Rightarrow \exists e \forall f: (f \ge e \rightarrow f \in S)$ $\Rightarrow \Bbb N \setminus S$ is finite all elements of $S$ are even $\Rightarrow \exists e \forall f: (2f \ge e \rightarrow 2f \in S)$ $\Rightarrow \Bbb N \setminus (S \cup \{1,3,5,...\})$ is finite In both cases we can identify the submonoid by a finite set of numbers which are not elements of the submonoid. Therefore we have only countable many possibilities. Can you complete this approach or provide a better one?","['abstract-algebra', 'monoid']"
395177,Factoring the third degree polynomial $x^3 - 3x^2 - 4x + 12$ using long division,"I am sure there is a better strategy that someone smarter than me would use, but I am not that person. I am trying to factor $$x^3 - 3x^2 - 4x + 12 .$$ I do not know how, so I attempt to guess with long division. I cheat and look at the answer so find out one of the factors to save myself time, so I try $x - 2$ . I am not sure how to type out long division, but I get $$ (x - 2)  \mid (x^3 - 3x^2 - 4x + 12) .$$ So I know I can have an $x^2$ for how many times $x$ goes into the leading term. Subtracting it all, I am left with $$ (x - 2  ) \mid (-x^2 - 4x + 12) .$$ I know that the leading term goes into the inner leading term $-x$ times or however you say that. $$ (x - 2 ) \mid  (- 2x + 12) .$$ Now $-2$ : $$ (x - 2 ) \mid 8 .$$ Now I do not know what to do, how did this go so wrong? I have $x^2 - x -2$ on top, and I have a remainder of $8$ . This cannot be right; I cheated, so I know that this should be a factor.","['factoring', 'cubics', 'algebra-precalculus']"
395195,"What is the definition of ""local equation(s)"" for a subscheme?","Hartshorne mentions ""local equations"" a few times without (so far as I can tell) actually defining them anywhere.  As best as I can guess, the definition would be something like this: If $Y \subseteq X$ is a closed subscheme, then ""local equations for $Y$"" consist of an open affine set $U \subseteq X$ and a finite set of generators $f_1, \ldots, f_n \in \mathcal{O}_X(U)$ of the ideal sheaf $\mathscr{I}_Y(U)$ considered as an $\mathcal{O}_X(U)$-module. (Here I've attempted to adapt the definition of local equations for a subvariety given in Shafarevich.)  Is this the accepted definition?  Or should the assumption that $U$ is affine or that $Y$ is a closed subscheme be weakened?  Or is there something else wrong with it? What should be done if $X$ is non-noetherian?  Might a closed subscheme simply not have local equations in that case? Or is ""local equation"" defined somewhere in Hartshorne?",['algebraic-geometry']
395211,Lipschitz continuous,"Let $\delta$ be an interval in $\mathbb{R}$ . Recall that a function $f$ is called Lipschitz continuous on $\delta$ with Lipschitz constant $L$ if there holds $|f(x) - f(y)| \leq L|x-y|$ for all $x,y$ in $\delta$ . (a) Show that the composition of Lipschitz continuous functions is again Lipschitz continuous. (b) Is the pointwise maximum of two Lipschitz continuous functions necessarily Lipschitz
continuous? $|f_2(f_1(y)) − f_2(f_1(x))| ≤ L2|f_1(y) − f_1(x)| ≤ L1L2|y − x|$ for part a, but i'm not sure if its right.","['calculus', 'analysis']"
395231,How to prove this inequality $xy\sin^2C+yz\sin^2A+zx\sin^2B\le\dfrac{1}{4}$,"Let $x,y,z$ is real numbers,and such that $x+y+z=1$,and in $\Delta ABC$,prove that $$xy\sin^2C+yz\sin^2A+zx\sin^2B\le\dfrac{1}{4}$$ I think this inequality maybe use $x^2+y^2+z^2\ge 2yz\cos{A}+2xz\cos{B}+2xy\cos{C},x,y,z\in R$ Thank you everyone.","['geometry', 'inequality', 'geometric-inequalities', 'triangles', 'quadratics']"
395250,Puiseux series and Resolution of Singularities,"I have a very basic knowledge of algebraic geometry(no schemes!), and am trying to study the resolution of singularities.
So the Newton's method gives us a Puiseux series parametrizing the branches of a curve at a point.
What I do not understand is how this gives a resolution of singularities. Since the Puiseux series is a power series, where is the variety here?
I know this is a basic question, but most of the references skim through this aspect and make it sound self-evident. I am hoping for some simple explanation of the connection. Thanks!","['geometry', 'algebraic-geometry', 'singularity-theory', 'algebraic-curves']"
395251,alternating series test for $\sum_{n=1}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4}$,"I must determine if this series converges (using specifically the alternating series test) $$\sum_{n=4}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4}.$$ I know the necessary and sufficient conditions are:
The series $\displaystyle\sum_{n=1}^{\infty}(-1)^na_n$ converges if $a_{(n+1)} \leq a_n$ (or if $f'(x) < 0$) and $\displaystyle\lim_{n\to\infty}a_n = 0.$ So first I tried to show the derivative, $\frac{d}{dx}(\frac{\sqrt{x}}{x+4}) < 0$ and after a bunch of algebra got $\displaystyle\frac{2-x}{\sqrt{x^5} + \sqrt{x^3} + 16 \sqrt{x}} < 0$, but I don't know how to demonstrate if that's really true for all $x \geq 1$. Next I tried to show $a_{n+1} \leq a_n\  \forall \ n \geq 1$, or 
$\frac{\sqrt{n+1}}{4(n+1)^2 + 1} \leq \frac{\sqrt{n}}{4n^2+1}. \ $ After cross multiplying and some rearranging I get $(4n^2 + 1)\sqrt{n+1} \leq 4n^{\frac{5}{2}} + 8n^\frac{3}{2} + 2\sqrt{n}$, and I don't know how to tell if that's true for all $n \geq 1$. So I'm stumped.  I know there are other tests.  But this is in the alternating series section so either the alternating series test can show it is convergent, or if not then I know it's divergent.
But the answer in the back of the book says it's convergent.  I just don't know how to prove it. Any suggestions? n.b.  The original series goes from $n=4\to\infty$ but I don't know if the 4 is a typo or is intended.","['sequences-and-series', 'calculus', 'analysis']"
395253,Any commutative associative operation can be extended to a function on nonempty finite sets,"This is a fact we use very frequently in general mathematics when we write such notations as $1+2+3+4$ : since we know that $+$ is commutative and associative, we can just ""drop the parentheses"" and not worry about order of operations. Of course I believe this, but how does one prove this in full generality? Even stating it is giving me trouble. Here's my attempt: Assume an operation $\oplus:S\times S\to S$ is provided satisfying $x\oplus y=y\oplus x$ and $x\oplus(y\oplus z)=(x\oplus y)\oplus z$ for all $x,y,z\in S$ . Claim: Given any finite set $\emptyset\subset A\subseteq S$ , there exists a unique $z\in S$ such that for any function $f:{\cal P}(A)\to{\cal P}(A)$ which satisfies $\emptyset \subset f(B)\subset B$ for all $B\subseteq A$ with $|B|\ge 2$ and any function $g:{\cal P}(A)\to S$ which satisfies $g(\{x\})=x$ for all $x\in S$ and $g(B)=g(f(B))\oplus g(B-f(B))$ for all $|B|\ge 2$ , $g(A)=z$ . The operation $\oplus$ does not necessarily have an identity element, so we do not attempt to define an empty sum. Intuitively, this element $z$ represents the finite sum of the elements in $A$ , so if $A=\{1,2,3\}$ and $f(\{1,2,3\})=\{1\}$ and $f(\{2,3\})=\{3\}$ , then $$z=g(\{1,2,3\})=g(\{1\})\oplus g(\{2,3\})=g(\{1\})\oplus (g(\{3\})\oplus g(\{2\}))=1\oplus(3\oplus 2).$$ There has got to be a better way to say that, but this is the only way I can think of to capture all the possibilities of parenthesization, and still be amenable to a formal proof. And now that I've stated it, how should I prove it? I suppose I should induct on something, but I've no idea what. Edit: The goal here is to be able to define an operation $F$ such that $F(\{x_1,\dots,x_n\})=x_1\oplus\cdots\oplus x_n$ and be assured that the operation is well defined and satisfies $F(A\cup B)=F(A)\oplus F(B)$ , when $A$ and $B$ are disjoint finite nonempty subsets of $S$ .","['notation', 'proof-writing', 'abstract-algebra', 'universal-algebra']"
395254,Infinite products of a (finite) group,"So I'm having a little trouble understanding the concept of infinite (cartesian) products of a group -- specifically, my notes (and, of course, homework questions) have concepts of, say $S_3^\mathbb{Z}$ and $S_3^\mathbb{R}$ (where $S_3$ is the permutation group of 3 elements, of course), but I'm having a hard time conceptualising the difference between these. If I assume that we're taking what seems to be the canonical approach: $G^I = \Pi_{i \in I} G$, how do I begin to understand the notion of, say, 3.141519 copies of $G$ (or in my case, $S_3$)? Is there a distinction between $\mathbb{Z}$ and $\mathbb{N}$ copies of a group? Or $\mathbb{Z}$ and $\mathbb{R}$? It feels like there's something obvious I'm missing, but, of course, I don't know it!","['infinite-product', 'general-topology', 'algebraic-topology', 'finite-groups', 'group-theory']"
395260,what is the sum of this?$\frac12+ \frac13+\frac14+\frac15+\frac16 +\dots\frac{1}{2012}+\frac{1}{2013} $,What is the sum of $$\frac12+ \frac13+\frac14+\frac15+\frac16 +\dots\frac{1}{2012}+\frac{1}{2013} $$,['algebra-precalculus']
395284,A little help integrating this torus?,"Let $\mathbf{F}\colon \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be given by $$\mathbf{F}(x,y,z)=(x,y,z).$$ Evaluate $$\iint\limits_S \mathbf{F}\cdot dS$$ where $S$ is the surface of the torus given by $$\begin{align*}
x&=(R+\cos(\phi))\cdot \cos(\theta)\\  
y&=(R+\cos(\phi))\cdot \sin(\theta)\\
z&=\sin(\phi)
\end{align*}$$
  and$$0\leq{\theta}\leq{2\pi},\qquad 0\leq{\theta}\leq{2\pi}.$$ Assume $S$ is oriented outward using the outward unit normal. My Take Ok, so I know I have to start by taking the cross product of the partial derivatives of theta and phi, but when it says using the outward unit normal, which vector has to be positive in order for it to be the outward normal? Is it the $i$, $j$ or $k$ vector? How do I know which vector it is? What will my next step be after I find the cross product?","['multivariable-calculus', 'integration']"
395290,Prove that in a graph a group of even nodes there are two of degree at least $2$,"We have just started learning graphs, and I understand the concept clearly, but when it comes to proving something I just don't know how to start! Prove that in in a group of an even number of people there are at least two of them that the number of the common people that they know is even. So I translated it to a Graphs problem: Given a graph $G$ (not directed; knowing someone is mutual), prove that in every group of an even number of vertices there are at least two vertices that the number of their common neighbors is even I understand that this is not of the hard questions, but I just can not think the methods of proving such things in Graphs in my head, yet. I would be happy for a direction!","['graph-theory', 'combinatorics']"
395292,"If $\sum a_n $ is a positive series that diverges, does $\sum \frac{a_n}{1+a_n}$ diverge? [duplicate]","This question already has answers here : Positive series problem: $\sum\limits_{n\geq1}a_n=+\infty$ implies $\sum_{n\geq1}\frac{a_n}{1+a_n}=+\infty$ (7 answers) Closed 11 years ago . Let $ \{a_n\}_{n=1}^\infty $ be a sequence such that $\displaystyle \sum a_n $ that is divergent to $+\infty$. What can be said about the convergence of $\displaystyle \sum \frac{a_n}{1 + a_n} $? Any hints, thoughts or leads would be greatly appreciated. Thanks!",['sequences-and-series']
395307,$\sum_{k=1}^{n} \binom{n}{k}k^{r}$ [duplicate],"This question already has answers here : Closed-form expression for $\sum_{k=0}^n\binom{n}kk^p$ for integers $n,\,p$ (7 answers) Closed 5 years ago . Find:$$\sum_{k=1}^{n} \binom{n}{k}k^{r}$$ For r=0 the sum is obviously $2^{n}$. For r=1 the sum is $n2^{n-1}$. For r=2 the sum is $n(n+1)2^{n-2}$. Here's what I've tried: $$\frac{d(1+x)^{n}}{dx}=n(1+x)^{n-1}=\binom{n}{1}+2\binom{n}{2}x+3\binom{n}{3}x^{2}+\cdots+n\binom{n}{n}x^{n-1}$$ $$\frac{d[x\cdot n(1+x)^{n-1}]}{dx}=\binom{n}{1}+2^{2}\binom{n}{2}x+3^{2}\binom{n}{3}x^{3}+\cdots+n^{2}\binom{n}{n}x^{n-1}$$ So if we continue like this and put x=1 we will get our result.However I cannot generalize this. I think the answer has something to do with the Stirling Numbers of the Second kind. Another way to present this question:
    Find $A_{1},A_{2},\cdots ,A_{r}$ such that $$k^{r}=A_{1}k+A_{2}k(k-1)+A_{3}k(k-1)(k-2)+\cdots$$ If someone can answer the second question I can solve the first one.","['calculus', 'combinatorics']"
395313,Does there exist $g$ s.t $g'=f$?,"I have the following homework question: Let G be the bounded open set shown in gray in this picture, whose
  boundary consists of eight line segments. The endpoints of those
  segments are, as shown , the points $-2,-1,-1+4i,1+4i,1,2,2+5i,-2+5i$. Let $f:\, G\to\mathbb{C}$ be an arbitrary function which is
  holomorphic in $G$. Does there a function $g:\, G\to\mathbb{C}$ which
  satisfies $g'(z)=f(z)$ for all $z\in G$ ? one tool that might be
  useful here is the Identity Theorem. What I did: I believe that I can construct such a function, but I am unsure if
my construction is correct: I would take some sequence of points $\{z_{i}\}_{i=1}^{\infty}$,
s.t there exist $\{r_{i}\}_{i=1}^{\infty}$ s.t $D(z_{i},r_{i})\subseteq G$
and s.t $$\cup_{i=1}^{\infty}D(z_{i},r_{i})=G$$ I believe that such a sequence of points can be obtained by choosing
all the points $x+iy\in G$ s.t $x,y\in\mathbb{Q}$. Moreover, I think that the points can be arranged s.t $$D(z_{i},r_{i})\cap D(z_{i+1},r_{i+1})\neq\emptyset$$ We first consider $D(z_{0},r_{0})$ - there is some $g_{0}:\, D(z_{0},r_{0})\to\mathbb{\mathbb{C}}$ s.t
 $g_{0}'=f$ in $D(z_{0},r_{0})$, since $f$ is holomorphic. We continue with $z_{1}$and get $\widetilde{g_{1}}$, since $g_{0}'=\widetilde{g_{1}}'$
for all points in $D(z_{0},r_{0})\cap D(z_{1},r_{1})$ then $g_{0}-\widetilde{g_{1}}$
is a constant $c$, and we can pick $g_{1}=\widetilde{g_{1}}+c$.
We take $g(z)$ will be eventually defined for every $z\in G$ as
one of the $g_{i}$'s. This construction looks a bit fishy to me, I am not sure about the
""moreover"" part and even if so, I am not totally convinced I can
arrange the constants to fit to get a holomorphic function $g$. ADDED: Instead of $$D(z_{i},r_{i})\cap D(z_{i+1},r_{i+1})\neq\emptyset$$
  we can relax the condition to be $$\cup_{i=1}^{r}D(z_{i},r_{i})\cap D(z_{i+1},r_{i+1})\neq\emptyset$$ Is my construction correct ? I would also appreciate to see another approach (maybe one that uses
the Identity Theorem as suggested)",['complex-analysis']
395327,Evaluate $\int_0^{\infty} \frac{1-e^{-ax}}{x e^x} dx$,"I found two different approaches, both is giving the same answer. Fubini: 
$$
\begin{align}
\int_0^{\infty} \frac{1-e^{-ax}}{x e^x} \,dx &= \int_0^{\infty} e^{-x} \int_0^a e^{-xy} \,dy\, dx \\
&=  \int_0^a \int_0^{\infty} e^{-x(1+y)}\, dx \,dy \\
&= \int_0^{a} \frac{1}{1+y}\, dy\\
&=\log (a+1) , a>-1
\end{align}
$$ Differentiation of the parameter:
Denote $\displaystyle K(a) = \int_0^{\infty} \frac{1-e^{-ax}}{x e^x}\, dx$, differentiate w.r.t. $a$. Also, note that $K(0)=0$. $$
\begin{align}
K'(a) &= \int_0^{\infty} \frac{e^{-ax}}{e^x} \,dx\\
&=\int_0^{\infty} e^{-x(a+1)} \,dx\\
&=\frac{1}{a+1}\\
\end{align}
$$ Now we integrate back to get $\displaystyle K(a) = \int K'(a) da = \log(a+1), a>-1$ The requirements of the Fubini theorem are that $f(a,x)$ is a measurable function and $(0,a) \times (0,\infty)$ is a measurable set, right? To differentiate w.r.t. a parameter, we need that $\displaystyle | e^{-x(a+1)}| \le g(x)$ which has to be an integrable function. Here we could have $g(x)=e^{-x}$ for instance. So my question now is, whether one of the approaches is more correct than the other. I used 1. in an exam, and got a really low score (so I'm surprised).","['multivariable-calculus', 'real-analysis']"
395332,How many smoothings are there for a nodal curve?,"Let $X_0$ be a projective nodal curve. It is known that one can find a smoothing of $X_0$: a family of projective curves $\pi:X\to B$ over a regular curve $B$, which is a smooth morphism over $B\setminus\{b_0\}$, and such that $X_0$ is isomorphic to $X_{b_0}=\pi^{-1}(b_0)$. The surface $X$ can be chosen to be regular. Question . ""How many"" smoothings are there for a fixed projective nodal curve $X_0$?
Is there a space parametrizing such objects? I do not see any reasonable moduli functor underlying this problem, but perhaps one can give the family of smoothings of $X_0$ some geometric structure. If $X_0$ is stable, one can look at an open neighborhood of $[X_0]\in \overline M_g$, which is $(3g-3)$-dimensional. Can we say that by looking in every one (or some) of these $3g-3$ directions we can find a regular smoothing as above? Thanks for any help.",['algebraic-geometry']
395336,"Computation of $\int_0^{\pi} \frac{\sin^n \theta}{(1+x^2-2x \cdot \cos \theta)^{\frac{n}{2}}} \, d\theta$","Show that 
  $$\begin{align*} \forall x \in [-1,1]: \int_0^{\pi} \frac{\sin^n \theta}{(1+x^2-2x \cdot \cos \theta)^{\frac{n}{2}}} \, d\theta &= c_n \tag{1} \\ \int_0^{\pi} \frac{\sin^{n+2} \theta}{(1+x^2-2x \cdot \cos \theta)^{\frac{n}{2}}} \, d\theta &= a_n \cdot x^2+b_n \tag{2} \end{align*}$$ where $a_n, b_n, c_n$ are constants (which do not depend on $x \in [-1,1]$), $n \geq 3$. I tried several approaches (differentiation to show that the derivative of $(1)$ is equal to $0$, Weierstraß substitution, ...), but always got stuck. For example, one can show that the integrand in $(1)$ equals $$\begin{align*} \frac{\sin^n \theta}{(1+x^2-2x \cdot \cos \theta)^{\frac{n}{2}}} &= \left( \sqrt{ \left( \frac{x-\cos \theta}{\sin \theta} \right)^2+1} \right)^{-n} \\ &= \left( \sqrt{4 \left( \frac{\sin \frac{\theta+\varrho}{2} \cdot \sin \frac{\theta-\varrho}{2}}{\sin \theta}  \right)^2+1} \right)^{-n} \end{align*}$$ where $x=\cos \varrho$. I hoped to get some kind of symmetrization out of it, but (as far as I can see) it doesn't work. Any ideas? (The aim is to find a rather quick or direct proof - a lengthy one is already known, using a recursive/inductive approach.) Thanks!","['definite-integrals', 'special-functions', 'integration']"
395396,Does every ordinal have cardinality no greater than $\aleph_\mathbb{0}$?,"My notes say that the ordinals $\omega + 1, \omega + 2, ... , 2 \omega, ... , 3 \omega, ... \omega^2, ... $ are all countable, and hence have cardinality equal to $\omega = \aleph_\mathbb{0}$. So I was wondering if it's fair to say that every ordinal has cardinality no greater than $\aleph_\mathbb{0}$? Alternatively, I guess it's possible that that the sequence of infinite ordinals listed above, does not include some of the ordinals with a cardinality greater than the naturals.. but I wasn't sure.","['ordinals', 'elementary-set-theory']"
395412,When is the quotient algebra of a unital C* algebra helpful?,"Let $\mathcal A$ be a unital C* algebra. Which properties does $\mathcal B \subset \mathcal A$ has to have for it to make sense to form the quotient algebra $\mathcal A / \mathcal B$? In cases where this construction makes sense, does $\mathcal A / \mathcal B$ have any special structure/properties that are helpful?
Put differently, for what kind of standard questions does it help to consider $\mathcal A / \mathcal B$ because it has desired properties?","['c-star-algebras', 'functional-analysis', 'abstract-algebra']"
395418,Is the inverse function smooth?,"Imagine that we have a function $Inv$ that maps $A \rightarrow A^{-1}$, where A is an invertible square matrix. now my questions is: how do i see that this function is arbitrarily often differentiable?","['multivariable-calculus', 'linear-algebra', 'real-analysis']"
395441,Integrating: $\int_0^\infty \frac{\sin (ax)}{e^x + 1}dx$,"I am trying to evaluate the following integral using the method of contour which I am not being able to. Can anyone point out what mistake I am making? $$\int_0^\infty \frac{\sin ax}{e^x + 1}dx$$ I am considering the following contour. And function $\displaystyle f(z):= \frac{e^{iaz}}{e^z + 1}$ The pole of order $1$ occours at odd multiple of $i\pi$ . By considering above contour there is no singularity. The integral can be broken down into six parts. $$\int_0^R \frac{e^{iax}}{e^x + 1} dx + i \int_0^{2\pi} \frac{e^{ia(R + iy)}}{e^{R + iy} + 1} dy + \int_{R}^{0}\frac{e^{ia(x+2\pi i)}}{e^{x + 2 \pi i } + 1} dx +  \\ i \int_{2 \pi }^{\pi + \epsilon} \frac{e^{ai( iy)}}{e^{ iy } + 1}dy + \int_\gamma \frac{e^{iaz}}{e^z + 1} dz + i \int_{ \pi -\epsilon}^{0} \frac{e^{ia( iy)}}{e^{ iy } + 1}dy$$ First and third gives $\displaystyle (1 - e^{-2 a\pi})\int_0^R\frac{e^{iax}}{e^x + 1} dx$ . Second goes to $0$ as $R \to \infty$ For fifth integral, $$\int_\gamma \frac{e^{iaz}}{e^z + 1} dz = \int_{-\pi/2}^{\pi/2} \frac{e^{ia\pi + a\epsilon e^{i\theta}}}{e^{i\pi + \epsilon e^{i\theta}+1}}i \epsilon i e^{i\theta }d\theta \to 0 \text{ as } \epsilon \to 0$$ The real part of fourth and sixth integral does not converge. But since my original integral is imaginary, it suffices to take imaginary part. As $\epsilon \to 0$ , I get $$i\int_{2\pi }^0 \Re \left [\frac{e^{-ay}}{e^{iy} + 1} \right] dy = i \int_{2\pi}^0 \frac{e^{-ay}}{2}dy = i \frac{e^{-2\pi a} - 1}{2a}$$ Finally using residue theorem, I am geting which is incorrect. $$(1 - e^{-2 a\pi})\int_0^\infty \Im \left [\frac{e^{iax}}{e^x + 1} \right ] dx +\frac{e^{-2\pi a} - 1}{2a} = 0$$ Can anyone point out my mistake or give worked out solution?? Thanks in advance!! ADDED:: I evaluated fifth integral incorrectly $$\int_\gamma \frac{e^{iaz}}{e^z + 1} dz = \int_{-\pi/2}^{\pi/2} \frac{e^{ia(i\pi + \epsilon e^{i\theta})}}{e^{i\pi + \epsilon e^{i\theta}}+1}i \epsilon  e^{i\theta }d\theta = ie^{-a\pi}\int_{\pi/2}^{-\pi/2}\frac{e^{ia\epsilon e^{i\theta}}}{-e^{\epsilon e^{i\theta}} + 1} \epsilon e^{i\theta}d\theta = i \pi e^{-a\pi}$$ So the total sum should be $$(1 - e^{-2 a\pi})\int_0^\infty \Im \left [\frac{e^{iax}}{e^x + 1} \right ] dx +\frac{e^{-2\pi a} - 1}{2a} +\pi e^{-a\pi}= 0 $$ After slight manipulation we find that $$\int_0^\infty \frac{\sin ax}{e^x + 1}dx = -\frac{\pi}{2\sinh (\pi a)} +\frac{1}{2a}$$","['complex-analysis', 'contour-integration']"
395466,Is there an algebraic number which has all possible combinations of numbers?,Today i saw this question. A similar question just came into my mind. Is there any irrational algebraic number so that it contains all possible number combinations in its digits? I'm really curious about it so if you had any idea how to find a number like this it will be good to share xD,['number-theory']
395501,Closed form for $n$-th derivative of exponential: $\exp\left(-\frac{\pi^2a^2}{x}\right)$,"I need the closed-form for the $n$-th derivative ($n\geq0 $): $$\frac{\partial^n}{\partial x^n}\exp\left(-\frac{\pi^2a^2}{x}\right)$$ Thanks! By following the suggestion of Hermite polynomials: $$H_n(x)=(-1)^ne^{x^2}\frac{\partial^n}{\partial x^n}e^{-x^2}$$ and doing the variable change $x=\pi a y^{-\frac{1}{2}}$, I obtain: $$\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n\frac{\partial^n}{\partial y^n}$$ and therefore $$H_n(\pi a y^{-\frac{1}{2}})=(-1)^{n+1}e^{\frac{\pi^2a^2}{y}}2\left(\frac{\pi a}{y^{\frac{3}{2}}}\right)^n\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}$$ Finally $$\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}=\frac{1}{2}e^{-\frac{\pi^2a^2}{y}}(-1)^{n+1}H_n(\pi a y^{-\frac{1}{2}})\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n$$ Is this correct?","['closed-form', 'exponential-function', 'derivatives']"
395504,A question on limsup,"Let $a_n>0$. Prove that $$\varlimsup_{n\to\infty}n\left(\frac{1+a_{n+1}}{a_n}-1\right)\geq 1.$$ I argue by contradiction. If it is not ture, then $$\exists\ N,\ \forall\ n\geq N, n\left(\frac{1+a_{n+1}}{a_n}-1\right)<1\Rightarrow 1+a_{n+1}-a_n<\frac{a_n}{n}.$$
What contradiction shall I get then....","['limsup-and-liminf', 'analysis']"
395507,A limit on binomial coefficients,Let $$x_n=\frac{1}{n^2}\sum_{k=0}^n \ln\left(n\atop k\right).$$ Find the limit of $x_n$. What I can do is just use Stolz formula. But I could not proceed.,"['summation', 'binomial-coefficients', 'limits']"
395514,Problem involving the computation of the following integral,"I was solving the past exam papers and stuck on the following problem: Compute the integral $\displaystyle \oint_{C_1(0)} {e^{1/z}\over z} dz$,where $C_1(0)$ is the circle of radius $1$ around $z=0.$ Here,$z=0$ is a pole of order $1$ and so  Res$(f,0)=\lim_{z \to 0 } z f(z)=\lim_{z \to 0}e^{1/z}=?$ ,where $f(z)={e^{1/z}\over z}$. Can someone point me in the right direction with some explanation?","['integration', 'complex-analysis']"
395517,"Differentiable manifolds, Serge Lang","I have started reading ""Introduction to differentiable manifolds"" by Serge Lang. In this book, Lang takes a different approach, by immediately introducing manifolds on arbitrary Banach spaces. His approach uses little to no multilinear algebra and he states the following in the foreword: ""The orgy of multilinear algebra in standard treatises arises from unnecessary double dualization and an abusive use of the tensor product."" What exactly does he mean by this? How is the use of the tensor product ""abusive""?.Is there something inelegant about the traditional treatment of finite dimensional manifolds and differential forms on them?","['manifolds', 'multilinear-algebra', 'reference-request', 'differential-geometry']"
395534,The restriction of a covering map on the connected component of its definition domain,"Suppose $p:Y\to X$ is a covering map, $X,Y$ are manifolds and $X$ is connected. If $Z$ is a connected component of $Y$, I wonder if the restriction of $p$ on $Z$ is also a covering map? If not, what conditions should be added to guarantee the restriction is a covering map? (expect that $Y$ is compact.) What I do: I know only need to show $p(Z)=X$. $p$ is a local homeomorphism and thus an open map. $Z$ is an open set since $Y$ is locally connected, and of course a closed set, so $p(Z)$ is an open set. Then I want to show $p(Z)$ is also closed, thus $p(Z)$ is open and closed in a connected space $X$, so $p(Z)=X$. But I cannot show $p(Z)$ is closed, perhaps I try a wrong way.","['general-topology', 'covering-spaces']"
395542,Right-angled isosceles triangles,If a right-angled triangle is isosceles then the other two angles must be equal to $45^\circ$ ? Is this always the case or are there other possible right-angled isosceles triangles?,"['geometry', 'triangles', 'trigonometry']"
395555,Double solid angle integration with integrand only dependent on relative angle,"Suppose one has an integral of the following form,
$$
\int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma).
$$
Where gamma is the relative angle between $(\theta_1, \phi_1)$ and $(\theta_2, \phi_2)$,
$$
\cos \gamma = \cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2 \cos(\phi_1-\phi_2)
$$
Based on symmetry arguments one could argue that as the integrand is only dependent the relative angle, we can fix $\Omega_1$, integrate over $\Omega_2$, and multiply the result with $4 \pi$ to compensate for fixing $\Omega_1$. Additionally we can argue that if we fix $\Omega_1$ along the $z$ axis the integration over $\phi_2$ just gives a factor $2 \pi$. So we have,
$$
\int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma) = 8 \pi^{2} \int \sin \gamma \, \text{d}\gamma f(\gamma). 
$$
However I fail to make an explicit mathematical derivation of the above reasoning. If I try to write the solid angle differentials in terms of $\gamma$ i get quite lengthy and ugly looking expressions.",['multivariable-calculus']
395564,Consequences of a rectangular matrix being of maximal rank,"I have a real matrix $A$, $(m+1) \times m$ and a vector $b  \in \mathbb R^{m+1}$ such that $b_{m+1}=0$. For any vector $u\in \mathbb R^m$, $Au=0 \Rightarrow u=0$. This means that $A$ is a rectangular matrix of maximal rank, i.e. of rank $m$. Since $m< \infty$, I'm told this means there is a solution $u\in \mathbb R^m$ to $Au=b$. I don't understand why: if $\operatorname{rk}(A)=m$, then the dimension of the image of the linear application defined by $A$ in $\mathbb R^{m+1}$ is $m$. So for $b\in\mathbb R^m$ we could guarantee a solution, but how can we guarantee the solution satisfies our $b\in\mathbb R^{m+1}$? Is there something simple I'm missing? (I've asked a couple of classmates who didn't know either, and my professor who told me this in the first place told me to look it up in a book that's in a library that's closed until next week.)",['linear-algebra']
395569,Let $G$ be a finite group with $|G|>2$. Prove that ${\rm Aut}(G)$ contains at least two elements.,"Let $G$ be a finite group with $|G|>2$ . Prove that ${\rm Aut}(G)$ contains at least two elements. We know that ${\rm Aut}(G)$ contains the identity function $f: G \to G: x \mapsto x$ . If $G$ is non-abelian, look at $g : G \to G: x \mapsto gxg^{-1}$ , for $g\neq e$ . This is an inner automorphism unequal to the identity function, so we have at least two elements in ${\rm Aut}(G).$ Now assume $G$ is abelian. Then the only inner automorphism is the identity function. Now look at the mapping $\varphi: G \to G : x \mapsto x^{-1}$ . This is an homomorphism because $\varphi (xy) = (xy)^{-1} = y^{-1} x^{-1} = x^{-1} y^{-1} = \varphi (x) \varphi (y)$ . Here we use the fact that $G$ is abelian. This mapping is clearly bijective, and thus an automorphism. This automorphism is unequal to the identity function only if there exists an element $x \in G$ such that $x \neq x^{-1}$ . In other words, there must be an element of order greater than $2$ . Now assume $G$ is abelian and every non-identity element has order $2$ . By Cauchy's theorem we know that the group must have order $2^n$ . I got stuck at this point. I've looked at this other post, $|G|>2$ implies $G$ has non trivial automorphism , but I don't know what they do in the last part (when they start talking about vector spaces). How should this prove be finished, without resorting to vector spaces if possible? Thanks in advance","['automorphism-group', 'finite-groups', 'group-theory', 'abstract-algebra']"
395587,Proving integrability in integration by parts in Rudin's text,"Integration by parts, as stated in W. Rudin's Principles of Mathematical Analysis , Theorem 6.22, goes as follows: Suppose F and G are differentiable functions in $[a,b]$, $F'=f\in \mathcal{R}$, and $G'= g\in \mathcal{R}$. Then $\int_a^bF(x)g(x)dx = F(b)G(b) - F(a)G(a) - \int_a^bf(x)G(x)dx$. The proof is to put $H(x)=F(x)G(x)$ and apply the fundamental theorem of calculus to H and its derivative. The fundamental theorem of calculus is stated as: If $f \in \mathcal{R}$ on $[a,b]$ and if there is a differentiable function $F$ on $[a,b]$ s.t. $F'=f$, then $\int_a^bf(x)dx=F(b)-F(a)$. We also have, from an earlier theorem (6.13), that: If $f \in \mathcal{R}$ and $g \in \mathcal{R}$ on $[a,b]$, then $fg \in \mathcal{R}$. Rudin notes that ""$H' \in \mathcal{R}$, by Theorem 6.13"" (above). Is that theorem really enough to prove this? It doesn't seem to be. After all, if $H(x)=F(x)G(x)$ then $H'(x)=F(x)g(x)+f(x)G(x)$, and we only have $f(x)g(x) \in \mathcal{R}$, while $F(x)g(x) \neq f(x)g(x)$. Where do we get integrability of $F(x)g(x)$ and $f(x)G(x)$? I'd be grateful for any pointers. Thanks!",['real-analysis']
395600,"How does a calculator calculate the sine, cosine, tangent using just a number?","Sine $\theta$ = opposite/hypotenuse Cosine $\theta$ = adjacent/hypotenuse Tangent $\theta$ = opposite/adjacent In order to calculate the sine or the cosine or the tangent I need to know $3$ sides of a right triangle. $2$ for each corresponding trigonometric function. How does a calculator calculate the sine, cosine, tangent of a number (that is actually an angle ?) without knowing any sides?",['trigonometry']
395606,Using Fourier series techniques to solve $x'' + 3x = 7$ with $x'(0) = x'(5) = 0$,"$$x'' + 3x= 7$$ Given conditions $x'(0)=x'(5)=0$. I checked the list and I went through three books. I am doing intro to differential equations. I just don't know how to get the extensions... I was told if there is a derivative use $\cos$, but I can't hack it. If there is any simple explanation. Please feel free to expound. This is to simply find the formal solution of the said ODE using Fourier Series.
What do I substitute at right hand side? Since I think left hand is simply summation $\cos n \pi/L$. ......I just know that $L=5$? And $7$ is $f(t)$ or $f(x)$? Maybe? Thanks.","['fourier-series', 'ordinary-differential-equations']"
395619,High school contest question,"Some work on it reveals the possibility of using gamma function. Is there any easy way to compute it?
$$\lim_{n\to\infty}\left(\frac{1}{n!} \int_0^e \log^n x \ dx\right)^n$$","['contest-math', 'limits']"
395621,"Is the Cauchy principal value ""invariant"" under change of variables?","Let $f \in C^{\gamma}_c(\mathbb{R}) $. Let $K:\mathbb{R}^n \backslash \{\vec{0}\} \rightarrow \mathbb{R}^n$ be a singular integral kernel with the following properties: 1) K smooth everywhere except at $\vec{0}$ 2) K homogeneous of degree $-n$, in particular $|K(x)| \leq \frac{c}{|x|^{n}}$ 3) K has mean value zero on the unit sphere, ie $\int_{|x|=1}K(x)dS=0$ I was wondering if the Cauchy principal value of the convolution of $K$ with $f$ is ""invariant"" under a change of variables. That is, for a $C^1$ diffeomorphism $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$, denoting $y=G(w)$ and $x=G(v)$, do we have: \begin{eqnarray}
\text{P.V.} \int_{\mathbb{R}^n} K(x-y)f(y)dy &\equiv& \lim_{\delta \searrow 0} \int_{|x-y|> \delta} K(x-y)f(y)dy \\
&=& \lim_{\delta \searrow 0} \int_{|v-w|> \delta} K \left(x-G(w) \right)f \left( G(w) \right) \left|\det \nabla G(w) \right| dw \quad \text{?}
\end{eqnarray}","['singular-integrals', 'functional-analysis', 'real-analysis']"
395626,What is the limit of the multidimensional integral?,"What is the limit of the integral $$\int_{[0,1]^n}\frac{x_1^5+x_2^5 + \cdots +x_n^5}{x_1^4+x_2^4 + \cdots +x_n^4} \, dx_1 \, dx_2 \cdots dx_n$$  as $n \to \infty ?$",['calculus']
395627,Limit as $x$ approaches $1$ from the right of $\frac{1}{\ln x}-\frac{1}{x-1}$,"$$
\lim_{x\rightarrow 1^+}\;\frac{1}{\ln x}-\frac{1}{x-1}
$$ So I would just like to know how to begin to solve this limit, or what topic does this problem fall under so that I can search for examples online or in text. I know the answer is 1/2 and I have tried plugging in numbers  smaller or greater than one but as I assumed that was the wrong way to go.","['calculus', 'limits']"
395632,Generalized eigenspaces of a compact operator are finite dimensional,"Let $T : H\rightarrow H$ be a compact operator on a Hilbert space $H$. Say that $\lambda \in \mathbb C$ is a generalized eigenvalue of $T$ if there is some $n \geq 1$ such that $(\lambda - T)^n$ is not injective. Define the generalized eigenspace corresponding to $\lambda$ to be the space $V$ of vectors $x\in H$ such that $(\lambda - T)^n x = 0$ for some $n$. I am trying to show that $V$ is necessarily finite dimensional if $\lambda\not=0$. I can show that the kernel of $(\lambda  - T)^n$ is finite dimensional for each $n$. But, I am having trouble extending this to the union of all of these kernels. Does anyone have any suggestions?","['operator-theory', 'eigenvalues-eigenvectors', 'functional-analysis']"
395634,"Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it?",I have a $4\times 4$ matrix $$A=\left(\begin{array}{cccc}8 & 11 & 4 & 3\\11 & 12 & 4 & 7\\4 & 4 & 7 & 12\\3 & 7 & 12 & 17\end{array}\right).$$ I want to do the things I describe below. Find the eigenvalues. Find a  unitary matrix $P$ (if there is any) so that the matrix $(P^{-1})AP$ is diagonal. Find (if there are any) an identity matrix $Q$ and an upper triangular matrix $R$ so that $A=QR$ . Comments (item by  item) I want to know if there is a better way than calculating $\det(A-\lambda I)$ . Well for this I think I have the answer as the matrix A is symmetric that means that it has 4 distinct eigenvectors that are orthogonal with each other also P a matrix composed by using the eigenvectors as columns gives us that $(P^{-1})AP$ = with the diagonal form of A. And P is unitary as if we take the inner product of all the eigenvectors with each other we get 0 since they are orthogonal with each other. Is there a flaw to the way i am thinking? I tried to solve this using the Gram–Schmidt process I found the first column of Q but then the numbers get too big and gets hard to compute. I have been thinking maybe symmetric matrices have some better way for QR decomposition,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
395642,Abelian Elliptic Surfaces,"By abelian surface we mean a 2-dimensional algebraic complex torus. Thus
$$ S=\Bbb{C}^2/\Gamma$$
where $\Gamma$ is a rank $4$ lattice in $\Bbb{C}^2$ and such that $S$ is algebraic. It has trivial canonical bundle $K=0$, so all plurigenera $P_n$ are equal to $1$. Also, topologically $S$ is a $4$-torus so one finds $q(S)=2$. By elliptic surface we mean that there exists a fibration $S\rightarrow C$ on a smooth curve $C$, such that the generic fiber is an elliptic curve. In examples (ix.4) on Beauville's book he claims: an abelian surface $S$ is elliptic if and only if there are two elliptic curves $E,F$ and an exact sequence
$$ 0\rightarrow E\rightarrow S\rightarrow F\rightarrow 0 $$ Any hint to understand why is this true? Thank you.","['algebraic-geometry', 'elliptic-curves', 'surfaces']"
395647,The smallest nontrivial conjugacy class in $S_n$,"Find the smallest nontrivial conjugacy class in $S_n$. For small $n$, the answer can be found by counting the permutations of each possible cycle type. The result is:
$$\begin{array}{ccc}
n & \text{smallest nontrivial class(es)} & \text{size} \\
\hline
1 & \text{none} & \\
2 & \text{transpositions} & 1 \\
3 & 3\text{-cycles} & 2 \\
4 & \text{double transpositions} & 3 \\
5 & \text{transpositions} & 10 \\
6 & \text{transpositions, triple transpositions} & 15
\end{array}$$ For $n\geq 7$, I think the unique smallest nontrivial conjugacy class in $S_n$ is given by the transpositions. The conjugacy classes in $S_n$ are given by the sets of permutations of the same cycle type. For the cycle type $\alpha = 1^{a_1} 2^{a_2} \ldots$, the size of the respective conjugacy class is known to be
$$N_\alpha = \frac{n!}{\prod_i (i^{a_i} a_i!)}.$$
In particular, the number of transpositions in $S_n$ is $\binom{n}{2}$. In this way, it remains to show the following: Let $n \geq 7$ and $\alpha$ be a partition of $n$ distinct from $1^n$ and $2^1 1^{n-2}$. Show that $N_\alpha > \binom{n}{2}$. So far I can only come up with a lengthy and technical case-by-case study.
I'm looking for a more elegant proof of the statement.","['permutations', 'abstract-algebra', 'symmetric-groups', 'group-theory', 'combinatorics']"
395655,calculate kernels of matrices with angles,"So my professor gave me this question: I have to find the basis of the eigenvalues of this matrix \begin{pmatrix}
  \cos(q) & \sin(q)\\
  \sin(q) & -\cos(q)\\
  \end{pmatrix} so I calculate the eigenvalues and I found it is 1 and -1.
so now I need to find the basis of the kernel of those matrices \begin{pmatrix}
  1-\cos(q) & -\sin(q)\\
  -\sin(q) & 1+\cos(q)\\
  \end{pmatrix} \begin{pmatrix}
  -1-\cos(q) & -\sin(q)\\
  -\sin(q) & -1+\cos(q)\\
  \end{pmatrix} so actually I need to find \begin{pmatrix}
  a \\ b\\
  \end{pmatrix} that will be function as basis for the kernel of each one of those matrices. but how do I do it ? I know how to do it when there is no angles meaning I would just compare it to zero. for example in the first matrix I get these two equations : $x-xcos(q)-ysin(q)=0$ $-xsin(q)+y+ycos(q)=0$ so I get this equation: $x(-sin(y)+((1-cos(y))/(sin(y)))+((cos(y)-cos(y)cos(y))/(sin(y))))=0$ which is true for every $x$ and then I can assign some value to $x$ for example zero and then $y$ equal zero also but after I am checking it, it is not true.
could u please help me ? 
so how can I do it ? All he told us is that $q\neq{0}$ and $q\neq\pi$ and $q\neq{2}\pi $ and nothing more.",['linear-algebra']
395669,Topology of the Segre product vs. the product topology,"In general, the product topology on two (quasiprojective) varieties is not the same as the topology of the product variety given by the Segre embedding.  This is something I've often seen asserted is true, but I'm having difficulties writing down a complete proof of this fact.  What I want to prove is Let $V \subseteq \mathbb{P}^m$, $W \subseteq \mathbb{P}^n$ be varieties with the Zariski topology.  Then the product topology on $V \times W$ is not the same as the topology of the Segre product $\sigma(V \times W)$, unless one of the varieties is simply a finite set of points. This is an early exercise in Harris' Algebraic geometry - a first course (Exer. 2.22) and in Smith's An Invitation to Algebraic Geometry (Exer. 5.3.4).  This is likely a standard fact, but regardless I indicate the kind of partial progress I've made below (possibly red herrings) - feel free to skip the walls of text below. Firstly, this fact is easily ""proved"" when explicit varieties $V, W$ are given.  For example, if $V = W = \mathbb{P}^1$, one can simply use the diagonal in $\mathbb{P}^1 \times \mathbb{P}^1$ as an example of a Zariski-closed set in $\sigma(\mathbb{P}^1 \times \mathbb{P}^1)$ that is not closed in the product topology.  From this and other examples, I believe that in the cases that the topologies differ, the topology on $\sigma(V \times W)$ is finer than the product topology. Next, if neither $V$ nor $W$ is a finite set of points, then $\dim V, \dim W > 0$, so one may assume by restricting to a subvariety that $V$ and $W$ are actually one-dimensional irreducible varieties.  In this case, the closed sets of $V \times W$ are well-known: the closed sets in $V$ and $W$ are the empty set, the whole set, or a finite number of points, and so the closed sets in $V \times W$ are the arbitrary intersections of finite unions of the products of such sets. At this point, I see several different avenues that may lead to a proof.  The first is more of an geometric approach.  We want to find a set that is closed in $\sigma(X \times Y)$ but not $X \times Y$; might not some ""tilted"" hyperplane section do the trick?  Or perhaps we can mimick the example with $V = W = \mathbb{P}^1$.  The diagonal may not make sense in general, but if we have a nonconstant morphism $V \to W$ or $W \to V$, then we can use the graph of this morphism in place of the diagonal to conclude.  We also know that the closed sets in $\sigma(V \times W)$ are the zero locus of bihomogeneous polynomials. Another approach is to work in the (homogeneous) coordinate rings.  That is, we wish to find a proper (prime) ideal containing $I(\sigma(V \times W))$ where we do not simply add in ""products"" of polynomials in $k[V]$ and $k[W]$.  The fact that we are working with one-dimensional irreducible varieties means that the transcendence degree of $k[\sigma(V \times W)]$ is two, so one simply needs to quotient out by one transcendental element that does not correspond to a closed set already in the product topology $V \times W$.  Finally, one may exploit the fact that the Segre product is the categorical product in the category of varieties and the topological product is the categorical product in the category of spaces.  If the Segre product is the categorical product in the category of spaces too and the projection maps are the same in this category, then the ""unique"" morphism from $X \times Y$ to $\sigma(X \times Y)$ is $\sigma$ on the level of sets, so one simply needs to show that $\sigma$ is not continuous. Unfortunately, I am unable to turn any of these vague ideas into a complete proof.  Since this result is so basic, I fear that I am overlooking something simple, and I would greatly appreciate it if someone could tell me what it is I'm missing.","['general-topology', 'algebraic-geometry', 'projective-space']"
395677,Last non zero digit of $n!$ [duplicate],This question already has answers here : Last non Zero digit of a Factorial (2 answers) Closed 11 years ago . What is the last non zero digit of $100!$? Is there a method to do the same for $n!$? All I know is that we can find the number of zeroes at the end using a certain formula.However I guess that's of no use over here.,"['factorial', 'divisibility', 'number-theory']"
395685,Finding the Fourier Series of $\sin(x)^2\cos(x)^3$,"I'm currently struggling at calculation the Fourier series of the given function $$\sin(x)^2 \cos(x)^3$$ Given Euler's identity, I thought that using the exponential approach would be the easiest way to do it. What I found was: $$\frac{-1}{32}((\exp(2ix)-2\exp(2ix)+\exp(-2ix))(\exp(3ix)+3\exp(ix)+3\exp(-ix)+\exp(-3ix)))$$ Transforming it back, the result is: $$ -\frac{1}{18}(\cos(5x)+\cos(3x)+2\cos(x))$$ (I've checked my calculations multiple times, I'm pretty sure it's correct.) Considering the point $x = 0$ however, one can see that the series I found doesn't match the original function. Could someone help me find my mistake?","['fourier-series', 'complex-analysis']"
395691,quadratic equation precalculus,"from Stewart, Precalculus, 5th, p56, Q. 79 Find all real solutions of the equation $$\dfrac{x+5}{x-2}=\dfrac{5}{x+2}+\dfrac{28}{x^2-4}$$ my solution $$\dfrac{x+5}{x-2}=\dfrac{5}{x+2}+\dfrac{28}{(x+2)(x-2)}$$
$$(x+2)(x+5)=5(x-2)+28$$
$$x^2+2x-8=0$$ $$\dfrac{-2\pm\sqrt{4+32}}{2}$$
$$\dfrac{-2\pm6}{2}$$
$$x=-4\text{ or }2$$ official answer at the back of the book has only one real solution of $-4$ where did I go wrong?",['algebra-precalculus']
395698,Fast way to calculate Eigen of 2x2 matrix using a formula,"I found this site: http://people.math.harvard.edu/~knill/teaching/math21b2004/exhibits/2dmatrices/index.html Which shows a very fast and simple way to get Eigen vectors for a 2x2 matrix. While harvard is quite respectable, I want to understand how this quick formula works and not take it on faith Part 1 calculating the Eigen values is quite clear, they are using the characteristic polynomial to get the Eigen values. Part 2, where they calculate the Eigen vectors is what I don't understand and have tried to prove but cannot. I understand that that what matters with Eigen vectors is the ratio, not the value. For example, an Eigen value of 2, with vector 3, 4, I could have any other vector, example 6, 8, or 12, 16, etc... any scalar multiple. In their example, given a matrix in the form a b c d, if b & c are zero, then the vectors are 1 0 and 0 1, which makes sense as you can scale these to any other size. I don't understand the other two cases (when b=0, or c=0), or I presume the case when b & c are non-zero. Can somebody offer an explanation or proof of this?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
395708,Harmonic Function bounded by a linear function,"Let $u$ be a harmonic function on $\mathbb C$. Suppose that for each $\epsilon > 0$, there is a constant $C_\epsilon$ such that $$u(z) \leq C_\epsilon + \epsilon |z| .$$ I am trying to show that $u$ is constant. I tried the mean value property, as well as replacing $u$ by a related function, but was unsuccessful. Does anyone have any ideas?","['partial-differential-equations', 'complex-analysis']"
395720,Simplifying fractions with fractions,"I am trying to simplify $$\frac{\frac{y}{x} - \frac{x}{y}}{\frac{1}{y} - \frac{1}{x}}$$ I make the top part into $$\frac{y^2 - x ^2}{xy}$$ I know the bottom can be rewritten to just be multiplied into the top as its inverse. $$\frac{y^2 - x ^2}{xy} * (y - x)$$ $$\frac{y - x }{xy} $$ This of course is wrong, but I do not know why, it appears to be correct to me. Nothing I did is mathematically wrong, it follows all the rules. What went wrong?",['algebra-precalculus']
395735,Linearization of $ m \dfrac{dy^2}{dt^2} = u(t) - C_d \left( \dfrac{dy}{dt} \right)^2-mg $,"$$ m \frac{dy^2}{dt^2} = u(t) - C_d \left( \frac{dy}{dt} \right)^2-mg $$ where 
$$\begin{align*}
y(t)&=\text{missile altitude}\\
u(t)&= \text{force}\\
m&= \text{mass}\\
C_d&= \text{aerodynamic drag coefficient}
\end{align*}$$
How do I linearize this beast? I want to obtain a transfer function so that I can create a PID controller for it.. I'm really stumped and could use some help.","['control-theory', 'ordinary-differential-equations']"
395737,Irreducible Polynomial in $\mathbb F_{256}$.,"Let $\mathbb F_{256}$ be the finite field with $2^8 = 256$ elements. Consider the polynomial over this field
$$
  x^2 + x + 1.
$$
I wanted to know if it is irreducible, so I calculated it for all values $0, \ldots, 256$ and considered the remainder modulo 256, I found that it doesn't evaluates to $0$, so I guess it is irreducible, but is this a valid method?","['finite-fields', 'abstract-algebra', 'polynomials']"
395773,limit of $e^z$ at $\infty$,"What's the limit of $e^z$ as $z$ approaches infinity? I am given that the answer is ""There is no such limit."" Is this correct, and if so, am I correct to demonstrate this by showing that as $y$ tends to infinity along the $y$-axis, the magnitude of $e^z$ remains $1$, i.e. it doesn't have infinite magnitude, thus it cannot be tending to infinity? And does this mean $e^z$ has an essential singularity at infinity?",['complex-analysis']
395797,Differential equations math help?,I have the equation $y' \sin x=y \ln y$. I told my teacher that we can solve if with separate variables method but he told me that we cant do that. He didn't explain why. Can you tell me why?,['ordinary-differential-equations']
395809,Edge percolation on $\mathbb{Z}^2$: probability that two neighbouring vertices are connected?,"I'm considering edge percolation on $\mathbb{Z}^2$ with parameter $p$, so that edges are present with probability $p$. Is it known how to express the probability $P(p)$ that $(0,0)$ is in the same connected component as $(1,0)$ as an explicit function of $p$? A very crude bound is
$$
p \leqslant P(p) \leqslant 1-(1-p)^4,
$$
but I'm sure people have derived better bounds, or even an exact expression. Simulations suggest that $P(1/2)$ is close to $3/4$. Thanks. Edit: The following is an approximation of the graph of the function $p\mapsto P(p)$ for $p=0,.01,\ldots,.82$.","['connectedness', 'inclusion-exclusion', 'probability', 'percolation']"
395816,Reference request for ordered groups,"I've been reading Pete Clark's notes on commutative algebra, and I especially liked section 17 on valuation rings, and ordered groups in particular. I'm looking for more introductory material regarding the ordering of groups, monoids and vectorspaces. Could anyone suggest something? I'm interested in printed works as well as online notes and courses. I read some of this book online and it seems good ,but I want to see if anyone has a more informed opinion.","['reference-request', 'group-theory', 'abstract-algebra']"
395820,What is the perimeter of a sector?,"I don't understand this. So we have: \begin{align}
r &= 12 \color{gray}{\text{ (radius of circle)}} \\
d &= 24 \text{ (r}\times2) \color{gray}{\text{ (diameter of circle)}} \\
c &= 24\pi \text{ (}\pi\times d) \color{gray}{\text{ (circumference of circle)}} \\
a &= 144\pi \text{ (}\pi\times r^2) \color{gray}{\text{ (area of circle)}}
\end{align} And we have: \begin{align}
ca &= 60^\circ \color{gray}{\text{ (Central Angle of sector)}} \\
ratio &= \frac{60}{360} = \frac{1}{6} \color{gray}{\text{ (ratio of ca to circle angle which is 360 degrees)}}
\end{align} So now we can calculate: \begin{align}
al = \frac{1}{6} \times 24\pi &= 4\pi \color{gray}{\text{ (arc length of SECTOR = ratio X circumference of circle)}}
sa &= \frac{1}{6} \times 144per = 24\pi \color{gray}{\text{ (sector area = ratio X area of circle)}}
\end{align} So my question is: What is meant by the perimeter of a Sector. Is it the arch length or the are of a Sector? And what is $24 + 4\pi$ ?","['geometry', 'circles']"
395822,Integrate: $\int_0^{\infty}\frac{\sinh (ax)}{\sinh x} \cos (bx) dx$,"Q: If $|a|< 1$ and $b>0$, show that
  $$\int_0^{\infty}\frac{\sinh (ax)}{\sinh x} \cos (bx) dx = \frac{\pi  \sin (\pi  a)}{2 (\cos (\pi  a)+\cosh (\pi  b))}$$ I need to evaluate the above integral by method of contour. I tried to use this contour on this question but at $2\pi i$, $\sinh(ax)$ changes to $\sinh(ax+2a\pi i)$ and I have difficulty taking out $\sinh(ax)$. Please give hints on which contour to use.Thanks in advance!! ADDED:: Considering $-R \to R \to R + \pi i \to -R + \pi i \to -R$ with a bump on $0$ and $\pi i$ to avoid singularity. $$(1 + e^{(a+ib)\pi i}) \int_{-\infty}^{\infty}\frac{e^{(a+bi)x}}{\sinh x}dx = -\pi i(1 - e^{(a+ib)\pi i}) \hspace{1 cm}(1)$$
$$(1 + e^{(-a+ib)\pi i}) \int_{-\infty}^{\infty}\frac{e^{(-a+bi)x}}{\sinh x}dx = -\pi i(1 - e^{(-a+ib)\pi i}) \hspace{1 cm}(2)$$
With a bit of algebra, we get 
\begin{align*}
\int_{-\infty}^{\infty}\frac{e^{ax}-e^{-ax}}{\sinh x}e^{ibx}dx &= 2\pi i \left( \frac{1}{1 + e^{(a+bi)\pi i}} -  \frac{1}{1 + e^{(-a+bi)\pi i}} \right)\\ 
 &= 2 \pi \frac{\sin (a\pi)}{\cosh (b\pi) + \sin(a\pi)}
\end{align*}
From which we get the desired result.",['complex-analysis']
395827,If $f$ is differentiable at $x = x_0$ then $f$ is continuous at $x = x_0$.,"Claim: if $f$ is differentiable at $x = x_0$ then $f$ is continuous at $x = x_0$. Please, see if I made some mistake in the proof below. I mention some theorems in the proof: The condition to $f(x)$ be continuous at $x=x_0$ is $\lim\limits_{x\to x_0} f(x)=f(x_0)$. (1) If $f(x)$ is differentiable at $x-x_0$, then $f'(x)=\lim\limits_{x\to x_0} \dfrac{f(x)-f(x_0)}{x-x_0}$ exists and the function is defined at $x=x_0$. (2) Therefore, by the Limit Linearity Theorem, $\lim\limits_{x\to x_0} f(x)$ exists and we'll show it is equals $f(x_0)$. (3) We'll do this by the Precise Limit Definion: given $ \epsilon>0, \exists\delta|0<|x-x_0|<\delta$, then $0<|f(x)-f(x_0)|<\epsilon$. As this limit exists by (2), we can make $f(x)$ as close to  $f(x_0)$ as one wishes, therefore $\lim\limits_{x\to x_0} f(x)=f(x_0)$, what satisfies the condition for $f(x)$ be differentiable at  $x=x_0$. The end.","['definition', 'derivatives', 'real-analysis', 'limits']"
395829,Help me prove this inequality :,"How would I go about proving this? $$ \displaystyle\sum_{r=1}^{n} \left( 1 + \dfrac{1}{2r} \right)^{2r} \leq n \displaystyle\sum_{r=0}^{n+1} \displaystyle\binom{n+1}{r} \left( \dfrac{1}{n+1} \right)^{r}$$ Thank you! I've tried so many things. I've tried finding a series I could compare one of the series to but nada, I tried to change the LHS to a geometric series but that didn't work out, please could someone give me a little hint? Thank you!","['inequality', 'summation', 'sequences-and-series']"
395850,Is the Dirac delta a function?,"Is Dirac delta a function? What is its contribution to analysis? What I know about it:
It is infinite at 0 and 0 everywhere else. Its integration is 1 and I know how does it come.","['distribution-theory', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
395864,Question about the nullstellensatz for projective schemes,"Assume that $ G $ is a graded ring. Assume that $A$ is a relevant homogeneous ideal (that is, it does not contain the irrelevant ideal $ \oplus_{n > 0}G_n$). I am having trouble proving the following formula:
$$ IV(A) = \sqrt{A}$$
In this formula, $I$ takes a subset $X$ of ${\rm proj} \, G$ to the homogeneous ideal generated by homogeneous elements which vanish at every point of $X$. It is clearly true that $ IV(A) \supseteq \sqrt{A}$ so the hard direction is $ IV(A) \subseteq \sqrt{A}$. I know we want to prove that if $ g \in G $ is homogeneous and vanishes at every point in $V(A)$ then it vanishes at every point in the affine cone of $V(A)$ and I see why this is true geometrically, but I am struggling with the commutative algebra. More explicitly, given a prime ideal $ A \subseteq \mathfrak{p}$, I want to prove that there is a relevent homogeneous prime $ A \subseteq \mathfrak{q} \subseteq \mathfrak{p}$. Any suggestions would be much appreciated EDIT 1: Ok I have an idea. Define 
$$ \mathfrak{p}_n = \{ \text{homogeneous elements of degree $ n $ in $ \mathfrak{p}$} \} $$
Then $ \oplus_{n \geq 0} \mathfrak{p}_n$ is a homogeneous prime ideal with $ A \subseteq \oplus_{n \geq 0} \mathfrak{p}_n \subseteq \mathfrak{p} $. The problem is that this ideal might not be relevant. Indeed, if $ G_{+} \subseteq \sqrt{A}$ then it is not relevant. The idea is that we are taking the largest homogeneous ideal which is contained in $ \mathfrak{p} $. This is is kind of like taking the line through a point in the classical situation.","['commutative-algebra', 'algebraic-geometry']"
395870,lower bound of expectation of stochastic differential equation,"I'm looking for a lower bound on the expected value of a smooth, non-negative, increasing function $\mathbb{E}f(X_t)$, $f(0)=0$ of the solution to a stochastic differential equation $X_t = x + \int_0^t b(X_s) ds + \int_0^t \sigma(X_s) dw_s$  ($x>0$). I'm aware of many upper bounds based on linear growth and Lipschitz constants, e.g., $\mathbb{E}|X_t|^p \le Ce^{\alpha t}$ or $\mathbb{E}|X_t-X_s|^p \le Cg(|t-s|)$, etc. For a lower bound I've played around with the second moment method, the reverse Markov inequality (like this ), and flipped through Oksendal, K&S, R&Y, and Mao, but I'm stumped. From Markov's inequality and a Girsanov argument I can show that for any $t>0$, $\mathbb{E}f(X_t) \ge P[f(X_t)>1]>0$. However, I'm not aware of any results based on linear growth or Lipschitz constants in a similar manner to the results mentioned above, something like, say, $\mathbb{E}|X_t|^p \ge Cg(t)$ for some decreasing function $g(t)$. Is anyone aware of a result like this?","['stochastic-calculus', 'stochastic-processes', 'probability', 'brownian-motion']"
395873,Degree of effective Cartier divisor,"Following http://www.math.columbia.edu/~masdeu/files/notes/ModForms.pdf , define an effective Cartier divisor of an $S$-scheme $f: X \rightarrow S$ as a closed subscheme $Z \subseteq X$, such that  $Z$ is flat and finite over $S$ via $f$. $D$ defines an invertible sheaf $\mathcal{I}(D)$ on $X$; take $\mathcal{L}(D)$ to be its dual. On page 20, we have ""The degree of $D$ is defined as the rank of $f_*(i_*\mathcal{O}_D \otimes \mathcal{L}(D))$,"" where $i : Z \rightarrow X$ is the inclusion. I understand that the latter sheaf is locally free, because $f$ is finite and flat. But I don't understand why it has a well-defined rank; why can't be different on affine subsets? It's not even required that $S$ should be connected...",['algebraic-geometry']
395878,How do I find the series expansion of the meromorphic function $\frac{1}{e^z+1}$?,"in a theoretical physics book, the author makes the following claim: $$\frac{1}{e^z + 1} = \frac{1}{2} + \sum_{n=-\infty}^\infty \frac{1}{(2n+1) i\pi - z}$$
and justifies this as These series can be derived from a theorem which states that any meromorphic function may be expanded as a summation over its poles and residues at those poles What's the name of that theorem? It's not really a Laurent series, since the Laurent series is for an expansion around one particular point only. I can see that the poles occur whenever $z = (2n+1)i\pi$ for $n \in \mathbb{N}$, but then where does that constant $1/2$ come from? EDIT: Well, it appears that the general claim isn't valid, so now I'd be interested in a justification for the expansion in my particular example...",['complex-analysis']
395888,Decompose $P$ into the direct sum of irreducible representations.,"Note: I need help with part (c). Consider the representation $P: S_3 \rightarrow GL_3$ where $P_{\sigma}$ is the permutation matrix associated to $\sigma$. a) Determine the character $\chi_P : S_3 \rightarrow \mathbb{C}$ b) Find all the irreducible representations of $S_3$. c) Decompose $P$ into the direct sum of irreducible representations.  That is, find a single matrix $Q$ so that $Q^{-1}P_{\sigma}Q$ is block diagonal where the blocks along the diagonal are either $T_{\sigma}$, $\Sigma_{\sigma}$ or $A_{\sigma}$ My Attempt Here is my overall progress for the problem: I let $e$ to be the identity permutation, $x = (1 \ 2 \ 3)$ and $y = (1 \ 2)$ Then, I let $P_x = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$, $P_y = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ and $P_e = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$. The conjugacy classes are as followed: $\{e\}$ $\{y, xy, yx\}$ $\{x, x^2\}$ The character table shows that: $$\chi (\{e\}) = 3$$
$$\chi (\{x, x^2\}) = 0$$
$$\chi (\{y, xy, yx\}) = 1$$ By the theorem I have applied, I have found three representations, which is congruent to the number of conjugacy classes.  They are: trivial representation $T$ sign representation $\Sigma$ two-dimensional representation $A$, presented as the symmetries of equilateral triangle The sum of their dimensions corresponds to the theorem I applied: $$d_1^2 + d_2^2 + d_3^2 = |S_3| = 6$$ The only possibility for equality to hold is $d_1 = d_2 = 1$ and $d_3 = 2$. Another character table shows that: $\chi_T (\{e\}) = 1$ $\chi_T (\{x, x^2\}) = 1$ $\chi_T (\{y, yx, xy\}) = 1$ $\chi_A (\{e\}) = 2$ $\chi_A (\{x, x^2\}) = -1$ $\chi_A (\{y, xy, yx\}) = 0$ $\chi_{\Sigma} (\{e\}) = 1$ $\chi_{\Sigma} (\{x, x^2\}) = 1$ $\chi_{\Sigma} (\{y, yx, xy\}) = -1$ Now, I am stuck in determining what is the matrix $Q$ for $Q^{-1}P_{\sigma}Q$. I know that I need to do ""change of basis"" and work out the vectors and stuff like this, but I can't seem to find the thorough approach. EDIT : Here is what I currently have: For the trivial representation, I have the vector $(1 , 1 , 1)$ spanning the invariant subspace. For the two-dimensional representation, I need to find two vectors $v$ and $w$ such that: $P_x v = -v/2 + \sqrt{3}w/2$ $P_x w = -v/2 - \sqrt{3}w/2$ $P_y v = v$ $P_y w = -w$ I found the $Q$ matrix, which is: $$\begin{bmatrix} 1 & 1 & -\frac{(1 + \sqrt{3})}{2} \\ 1 & 1 & \frac{(1 + \sqrt{3})}{2} \\ 1 & -2 & 0 \end{bmatrix}$$ But it is wrong. Any advices or comments you have?","['representation-theory', 'abstract-algebra']"
395893,Baire's theorem from a point of view of measure theory,"According to Baire's theorem, for each countable collection of open dense subsets of $[0,1]$, their intersection $A$ is dense. Are we able to say something about the Lebegue's measure of $A$? Must it be positive? Of full measure? Thank you for help.","['general-topology', 'measure-theory', 'baire-category']"
395904,"How to show that a valid inner product on V is defined with the formula $[x, y] = \langle Ax, Ay\rangle $?","Let $A \in L(V,W)$ be an injection and $W$ an inner product space with the inner product $\langle \cdot,\cdot\rangle $. Prove that a valid inner product on $V$ is defined with the formula $[x, y] = \langle Ax, Ay\rangle $ $L(V, W)$ = The set of all linear mappings (linear operators) from V to W To prove this, if I am correct, I need to show that the four properties of an inner products space apply on this formula: 1. $\langle x, y \rangle  = \overline{\langle y,x\rangle }$ 2. $\langle \alpha x, y\rangle  = \alpha\langle x,y\rangle $ 3. $\langle x+y,z\rangle  = \langle x,z\rangle  + \langle y,z\rangle $ 4. $\langle x,x\rangle \space \ge 0 \space \space \forall x$ 4.' $\langle x,x\rangle \space = 0 \Longleftrightarrow x=0$ 4. $ [x, x] = \langle Ax, Ax\rangle , Ax \in W$, and since $W$ is an inner product space, $\langle Ax, Ax\rangle \space \ge 0$ implies $[x, x] \ge 0$. 4.' Since A is an injection: $Ax = 0 \implies x = 0$ and since $W$ is an inner product space and $Ax \in W \implies \langle  Ax, Ax \rangle  = 0 \implies Ax = 0 \implies [x,x] = 0 \Longleftrightarrow x=0$ 3. $[x+y,z] = \langle A(x+y), Az\rangle  = A$ is linear $= \langle Ax + Ay, Az\rangle = Ax, Ay, Az \in W$ and $W$ is an inner product space $= \langle Ax, Az\rangle  + \langle Ay, Az\rangle  = [x,z] + [y,z]$ 2. $[\alpha x, y] = \langle A(\alpha x), Ay\rangle  = A$ is linear $ = \langle \alpha (Ax), Ay \rangle  = W$ is an inner product space $ = \alpha \langle Ax, Ay\rangle  = \alpha [x, y]$ 1. $[x, y] = \langle  Ax, Ay \rangle  = W$ is an inner product space $= \overline{\langle Ay, Ax\rangle } = \overline{[y, x]}$ But this seems to me a little to easy, did I maybe conclude something that can't be concluded so easily or maybe is my approach to prove this completely wrong?",['linear-algebra']
395924,"$P[X=Y]=0$ if $X,Y$ are i.i.d. with continuous c.d.f.","I am having lots of trouble proving the following statement: Let $X,Y$ be two real valued random variables on a probability space $(\Omega,\mathcal{F},P)$. These two variables are independent and identically distributed, iid, in the sense that they share the same distribution $P_X=P_Y$ and the cumulative distribution function $F_X(x)=P_X[X\leq x]$ is continuous. The probability measure $P$ might not necessarily be absolutely continuous wrt a sigma-finite measure. Therefore there might not be a Radon-Nikodym derivative with respect to a sigma-finite measure. If there was a Radon-Nikodym derivative with respect to a sigma-finite measure, it is easier to show what I want. I need to prove that, under the above conditions $P[X=Y]=0$ Any insights? Best regards, Juan Manuel","['probability-theory', 'measure-theory']"
395926,Closed form for n-th anti-derivative of $\log x$,"Is it possible to write a closed-form expression with free variables $x, n$ representing the n-th anti-derivative of $\log x$?","['closed-form', 'logarithms', 'calculus', 'integration']"
395931,Is any compact metric totally disconnected space homeomorphic to a compact subspace of a Cantor space?,"Every compact metric totally disconnected perfect space is homeomorphic to a Cantor space. Is every compact metric totally disconnected space homeomorphic to a compact subspace of a Cantor space? In other words, if you have a compact metric totally disconnected space can you embed it in a compact metric totally disconnected perfect space?","['general-topology', 'metric-spaces', 'descriptive-set-theory']"
395938,"Why is $\varphi$ called ""the most irrational number""?","I have heard $\varphi$ called the most irrational number. Numbers are either irrational or not though, one cannot be more ""irrational"" in the sense of a number that can not be represented as a ratio of integers. What is meant by most irrational? Define what we mean by saying one number is more irrational than another, and then prove that there is no $x$ such that $x$ is more irrational than $\varphi$. Note: I have heard about defining irrationality by how well the number can be approximated by rational ones, but that would need to formalized.","['golden-ratio', 'irrational-numbers', 'number-theory']"
395941,"$\iint f(x,y)\,dxdy$ and $\iint f(x,y)\,dydx$ exist but $f$ not integrable on $[0,1]\times[0,1]$","I want to look for a function $f(x,y)$, whose support is inside $[0,1]\times[0,1]$, such that $\int_0^1\!\int_0^1\!f(x,y)\,dxdy$ and $\int_0^1\!\int_0^1\!f(x,y)\,dydx$ both exist, but $f(x,y)$ is not Riemann-integrable (or Darboux-integrable) on $[0,1]\times[0,1]$. By Riemann-Lebesgue Theorem, I know that the set of discontinuities of $f$ in $[0,1]\times[0,1]$ cannot be contained in a set of measure $0$ in $\mathbb{R}^2$, but with each fixed $x$ (or fixed $y$), the set of continuities of $f_x(y)$ or $f_y(x)$ can be contained in a set of measure $0$ in $\mathbb{R}^1$. I'm unable find a set of continuities in $[0,1]\times[0,1]$ that satisfy this, thus unable to find a function like this. Please help. Thank you very much. Further question: is it possible that $\int_0^1\!\int_0^1\!f(x,y)\,dxdy=\int_0^1\!\int_0^1 \!f(x,y)\,dydx$ but $f(x,y)$ is not Riemann-integrable on $[0,1]\times[0,1]$?","['integration', 'real-analysis', 'analysis']"
395944,Proof on showing function $f \in C^1$ on an open & convex set $U \subset \mathbb R^n$ is Lipschitz on compact subsets of $U$,"The question is as follows: Given: (1) function $f: U \subset \mathbb R^n ==> \mathbb R$ (2) $U$ is open and convex set (3) $f \in  C^1$ in $U$ Goal: Show that $f$ is Lipschitz on any compact subset of $U$ By now, I have various ideas come to mind, but I can't connect the dots >_< Here are my thoughts so far: (1) Recall definitions: (i) $f \in C^k$ means all partial derivatives up to (and including) order $k$ exist and continuous.  Here $k = 1$ (ii) a set $U$ is convex if for any 2 points $x, y$ in $U$, the segment joining $x$ and $y$ is totally inside $U$ (iii) function $f$ is Lipschitz if there is a bound $M$ such that $|f(x) - f(y)| \leq M |x-y|$ (2)By a theorem in my book: if function $f \in C^1$ on an open & convex set $U$, then for any 2
  points $x$ and $y$ in $U$, there is a point $s$ lying on the segment
  joining $x$ and $y$ such that $f(x) - f(y) = Df(s) * (x - y)$ I firstly have a feeling that I may need this theorem in the proof, but I can't see the connection between my desired bound $M$ with $Df(s)$.  So I tried to think of other ideas and ... (3) It turns out that by all the given information, I think if I can show function $f$ is convex, then I'm done because there is a theorem which said: a convex function on a open, convex set $U$ should be Lipschitz on $U$, thus I think it must also be Lipschitz on any subset of $U$, either that subset is compact or not. However, how can I prove function $f$ is convex, based on given information?  I have a feeling that I have to use the fact that set $U$ is convex, but then I'm stuck on how to proceed further >_> *Would someone please help me on this question?
Thank you very much ^_^*","['multivariable-calculus', 'analysis']"
395960,Simplifying $\left|\left|\sqrt{-x^2}-1\right|-2\right|$,How do we simplify the expression $\left|\left|\sqrt{-x^2}-1\right|-2\right|$? This is very confusing.  Do they cancel out and become just simply $\sqrt{-x^2}-1-2$?,"['absolute-value', 'algebra-precalculus', 'problem-solving']"
395961,Chain rule proof,"Let $a \in E \subset R^n, E \mbox{ open}, f: E \to R^m, f(E) \subset U
 \subset R^m, U \mbox{ open}, g: U \to R^l, F:= g \circ f.$ If $f$ is
   differentiable in $a$ and $g$ differentiable in $f(a)$, then $F$ is
   differentiable in $a$ and $F'(a)=g'(f(a)) f'(a)$. I have been given a short proof, but I do not understand every step of it. We have $f(a+h) = f(a) + f'(a)h + o(|h|)$ and $g(f(a)+k) = g(f(a)) + g'(f(a))k + o(|k|)$ as $h,k \to 0$. Thus $\begin{align} F(a+h)=(g \circ f)(a+h) &= g(f(a+h)) \\ 
&= g(f(a)+f'(a)h+o(|h|)) \\
&= g(f(a)) + g'(f(a))(f'(a)h+o(|h|)) + o(|f'(a)h+o(|h|)|) \\
&= (g\circ f)(a) + g'(f(a))f'(a)h+o(|h|) + o(O(|h|)) \\
&= F(a) + g'(f(a))f'(a)h+o(|h|) + o(|h|).
\end{align}$ That's all. I feel like there is some trickery involved with the little o en big o. My problems are: Why does $f'(a)h+o(|h|) \to 0$ when $h\to 0$. I see that the first term tends to $0$, but how come $o(|h|) \to 0$? $g'(f(a))o(|h|))=o(|h|)$. $o(|f'(a)h + o(|h|)|)=o(O(|h|))$ $o(|h|) + o(|h|) = o(|h|)$. This last one isn't included in the proof, but I expect that's would have been something trivial. I've been trying to work this out with the definitions of big o en little o, but to no prevail. It's confusion to me how I can do computations with big o and little o, since you can't really perform algebraic operations with them and it's more like a function property. If anyone could show me how this would be done I'd be so thankful.","['asymptotics', 'analysis']"
395962,Calculating $\sqrt{28\cdot 29 \cdot 30\cdot 31+1}$,Is it possible to calculate $\sqrt{28 \cdot 29 \cdot 30 \cdot 31 +1}$ without any kind of electronic aid? I tried to factor it using equations like $(x+y)^2=x^2+2xy+y^2$ but it didn't work.,"['factoring', 'arithmetic', 'algebra-precalculus']"
395966,Does convex and radially open imply open?,"I want to show that a convex set $A$ is radially open iff $A\cap W$ is open in W, for every finite dimensional linear subspace. Here the 'openness' we are talking about is from any normed space. Help would be much appreciated. Definition : We call a set $A$ of a vector space $V$ $\underline{\text{radially open}}$ if $\forall v \in V, x \in A$ $\exists \lambda \in \mathbb{R}$ such that $x + \alpha v \in A$ for any $0 < \alpha < \lambda$.","['convex-analysis', 'linear-algebra', 'functional-analysis']"
395971,Simplifying $\sqrt{\underbrace{11\dots1}_{2n\ 1's}-\underbrace{22\dots2}_{n\ 2's}}$,How do I simplify: $$\sqrt{\underbrace{11\dots1}_{2n\ 1's}-\underbrace{22\dots2}_{n\ 2's}}$$ Should I use modulos or should I factor them?  Or any I suppose to use combinatorics?  Any one have a clue?,"['problem-solving', 'factoring', 'number-theory', 'combinatorics']"
395972,Integration of function help,I'm having problems integrating this function $\displaystyle E(X)=\int^ \infty_0 x\lambda e^{-\lambda x} dx$. I did the integration by parts and had $-xe^{-\lambda x}- \lambda e^{-\lambda x}$. However the solution gives $-xe^{-\lambda x} - \dfrac{1}{\lambda}e^{-\lambda x}$. I can't find any mistakes. What am I doing wrong?,"['statistics', 'integration']"
395980,Topology for convergent sequences,"Let $(X,\tau)$ be a topological space, and consider the family $\mathcal{F}$ of the topologies over $X$ such that the convergent sequences for each $\gamma \in \mathcal{F}$ are the same as the convergent sequences for $\tau$, with the same limits. That is,
$$
  \mathcal{F} = \{\gamma \;|\; x_n \xrightarrow{\gamma} x \Leftrightarrow x_n \xrightarrow{\tau} x\}
$$ It is easy to see that the topology $\tau_M$ generated by the topologies in $\mathcal{F}$ is such that $\tau_M \in \mathcal{F}$. The $\tau_M$ is the finest topology with the same convergent sequences as $\tau$, converging to the same point. But is it true that the topology
$$
  \tau_m = \bigcap_{\gamma \in \mathcal{F}} \gamma
$$
is itself in $\mathcal{F}$? In other words, does it follow that the convergent sequences for $\tau_m$ and their limits are the same as the convergent sequences and their limits in $\tau$? Are there any nice counterexamples? In other words again, do we have a weakest topology such that the convergent sequences and their limits are the same as the ones for $\tau$? Edit: Fixed according to @joriki's comment.",['general-topology']
395982,A basic doubt on Lebesgue integration,Can anyone tell me at a high level (I am not aware of measure theory much) about Lebesgue integration and why measure is needed in case of Lebesgue integration? How the measure is used to calculate the horizontal strip mapped for a particular range?,"['probability-theory', 'measure-theory', 'probability']"
395994,how to tell whether x and y are independent or not,"Suppose that $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$ Find $\operatorname{Var(X+Y)}$. I'm having trouble with this problem the way to find $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+2\operatorname{Cov(X,Y)}$, however if $X$ and $Y$ are independent, then $\operatorname{Cov(X, Y)}=0$, the answers indicated that $X$ and $Y$ are independent since they just used $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+0$, my question is how do I tell whether $X$ and $Y$ are independent or not, based on looking only at $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$",['statistics']
395999,"Find $\mathrm{Aut}(G)$, $\mathrm{Inn}(G)$ and $\mathrm{Aut}(G)/\mathrm{Inn}(G)$ for $G = D_4$","Problem Find $\mathrm{Aut}(G)$, $\mathrm{Inn}(G)$ and $\mathrm{Aut}(G)/\mathrm{Inn}(G)$ for $G = D_4$ My Attempt I let $D_4 = \{e, x, y, y^2, y^3, xy, xy^2, xy^3\}$ I found that $\mathrm{Inn}(G)$ consists of 4 bijective conjugation functions, namely $\{\phi_e, \phi_x, \phi_y, \phi_{xy}\}$.  For $\mathrm{Aut}(G)$, I found that there are 12 automorphisms. Here is the following link that relates to the problem I am doing. Based on the solution: Lemma : If $\alpha$ is an automorphism of group $G$ and $G$ has generators $x$ and $y$ with orders $n$ and $m$, respectively then $\alpha(x)$ and $\alpha(y)$ are also generators for $G$ with orders $n$ and $m$. Proof: First, let us show that the orders agree.  If $g \in G$ has order $n$, let $a = \alpha(g) \in G$ have order $m$.  Then, $a^m = 1$ but by applying $\alpha^{-1} \in \mathrm{Aut}(G)$, we get $g^m = 1$.  However, $g$ has order $n$, so $n$ must divide $m$.  Similarly, $g^n = 1$ and applying $\alpha$, we note that $a^n = 1$, and so we conclude $n = m$. Secondly, since any element of $G$ can be written as a product of $x$'s and $y$'s, and $\alpha$ is a surjective homomorphism, it follows that any element of $G$ can also be written as a product of $\alpha(x)$ and $\alpha(y)$, hence they generate $G$. Using this Lemma (or a similar argument), we note that an automorphism of $D_4$ must send $y$ to $y, xy, x^2y$ or $x^3y$ and $x$ to $x$ or $x^3$.  Any such pairing is possible, thus there are $2 \cdot 4 = 8$ such automorphisms... (The notations that someone use are different from what I denote.) The question I have is: Why are there 8 automorphisms?  Shouldn't there be 12 automorphisms?  Here is what I have: $$e \mapsto e$$
$$x \mapsto \text{ either } \{x,y^2, xy^2\}$$
$$y \mapsto \text{ either } \{y, y^3, xy, xy^3\}$$ Then, there are $1 \cdot 3 \cdot 4 = 12$ automorphisms. Any advices or comments?","['group-theory', 'abstract-algebra']"
