question_id,title,body,tags
3554164,"A question on the possibility of a continuous surjective function from $(a,b) \mapsto [a,b]$","$\mathbf{Original \ Question}: $ Let $a,b \in \mathbb{R}$ and $a<b$ . Which of the following statement(s) is/are true? (A) There exists a continuous function $f:[a,b] \to (a,b)$ such that $f$ is one-one (B) There exists a continuous function $f:[a,b] \to (a,b)$ such that $f$ is onto (C) There exists a continuous function $f:(a,b) \to [a,b]$ such that $f$ is one-one (D) There exists a continuous function $f:(a,b) \to [a,b]$ such that $f$ is onto $\mathbf{Attempt}:$ Option $A$ is true. For example, consider $a=0 ,b=1$ , $f(x)=\frac{1}{2(x+1)}$ . Option $C$ is true. $f(x)=x$ with $a=0,b=1$ . Option $B$ cannot be true, Since: $f^{-1}((a,b))=[a,b]$ and by the property of continuous functions, $f^{-1}((a,b))$ must be an open set. But $[a,b]$ is closed. Option $D$ cannot be true since $f^{-1}([a,b])=(a,b)$ must be a closed set. But $(a,b)$ is open. But, in the answer key, $D$ is given as a correct choice. What am I doing wrong here? Any insight is much appreciated. Thank you!","['contest-math', 'metric-spaces', 'real-analysis', 'continuity', 'solution-verification']"
3554319,"If adding ONLY finitely many points to a (non-empty) subset of plane, $\mathbb{R}^2$ makes it closed, then is the closure all of $\mathbb{R}^2$?","Question: Let $E \subset \mathbb{R}^2$ be non-empty open set such that $E$ 's union with finitely many points becomes a closed set. Is it necessarily true that $$
\text{closure}{\ (E)} = \mathbb{R}^2 \, \, ?
$$ Equivalent Question: Looking into its complement you can rephrase it as: Given $F \subset \mathbb{R}^2$ closed, such that $F$ minus a finite set is a non-empty open set, does it follow that $ F = \mathbb{R}^2$ ? Answer and Update: As shown by different correct answers below, yes, the closure of the set has to be all of $\mathbb{R}^2$ . Also the proofs (at least many of them) illustrate the possibility of generalizing to the countable case, i.e. Lemma: If an open set has only countably many boundary points, then its closure is all of $\mathbb{R}^2$ . Proof: Proof 1 (@Danielwainfleet's comment) Let $E$ be an open set whose complement is not all of $\mathbb{R}^2$ . Pick a point in the complement of its closure which is a non-empty open set and consider an open ball around it that is contained in the complement. From its center every ray picks up a boundary point of $E$ , and no two rays pick the same point, as the boundary point is not that center itself. This is where we use openness of the complement is used. This tells us that $E$ has uncountably many boundary points -- for the uncountably many rays. (Other proofs do this with a line segment and that too will produce uncountably many boundary points.) Proof 2: (Lee Mosher) Their proof below works if you replace the finite $A$ with the ""countable"" $A$ -- it is true that $\mathbb{R}^2 \setminus A$ is still path connected if $A$ is a countable set.",['general-topology']
3554321,Is it possible to construct a matrix : $A^2 = J_{n}$.,"Consider $\Omega = Mat_{n\times n}(\{0,1\})$ - space of matrices of $1$ s and $0$ s.  We want to determine does there exist $A\in\Omega$ : $A^2 = J_n$ , where $J_n$ is a matrix of ones. We suppose that there are standard arithmetical operations : ( $\mathbb{R}$ ,+, $\cdot)$ . Actually I don't understand how to step it. I've thought about using some properties about spectrum of $J_n$ , but it looks like failure moment.","['matrices', 'linear-algebra']"
3554346,"In the epsilon-delta definition, what is wrong if I said: ""given delta, there exists an epsilon""?","WHY are we always given $\epsilon > 0$ first, then solving for a $\delta>0$ ? This is in the limit definition. I want to ask: Can we say ""given $\delta>0$ , there exists $\epsilon>0$ ""? Since we can always solve for one given the other. I found three counterexamples, but I don't understand them: Let $f(x) = \sin x$ , let $L$ and $\delta$ be arbitrary real numbers.  Then $\epsilon = |L| + 2$ satisfies your definition. (from post ) Q: What's wrong with setting $\epsilon = |L| + 2$ ? It's big, but it's not wrong! Let $f(x) = 1/x$ , and let $a = 1$ .  The definition fails for $\delta \ge 1$ , since for any $\epsilon$ we can choose $x=1/(L+\epsilon)$ if $L+\epsilon > 1$ , so that $f(x)-L \ge \epsilon$ . (from post ) Q: What are they saying here? At $x=1$ , the definition fails for $\epsilon \ge 1$ too! The problem is not $\delta$ . The problem is the function is undefined for $x \le 0$ . Counterexample: $\lim\limits_{x \to 0} f(x) = L$ $f(x) = \begin{cases} \sin \frac{1}{x}, & x \ne 0 \\ 0, & x = 0 \end{cases}$ Given any $\delta > 0$ , we can  find $\epsilon > 0$ such that $|f(x) - L| < \epsilon$ whenever $|x| < \delta$ .  For instance, set $\epsilon = 2$ ; then any choice of $L \in (-1,1)$ will satisfy this ""reversed"" situation. (from post ) Q: I don't see how setting $\epsilon = 2$ violates any definition. I mean, we did find a $\epsilon$ for a given $\delta$ . Thanks all for the pouring answers, I'll get back to each one personally. If I did not choose an answer, that means all submissions are still welcomed! The best answer will be chosen based on # of upvotes (50%) and if I understood it and agree it's the best (50%).","['limits', 'continuity', 'epsilon-delta']"
3554349,How to efficiently compute the minimal polynomial of a number expressed in radicals?,"Preamble: I want to calculate the minimal polynomial of a number of the form $$x=\sum_{i=1}^k \pm a_i^{1/k_i}$$ Where the $a_i$ are algebraic numbers also of this form with a finite expression and $k_i$ are positive integers. I can think of three approaches, all with serious problems: Using the resultant repeatedly: Apply the algorithm recursively until you arrive at a root of a rational number, therefore we can assume that we have the minimal polynomial of all $a_i$ . Then compute the minimal polynomial of $\pm a_1^{1/k_1}\pm a_2^{1/k_2}$ using the resultant of their minimal polynomials then repeat for that with $a_3, a_4...a_k$ . Let $d_i$ be the degree of the minimal polynomial of $a_i^{1/k_i}$ . In general this method requires computing the determinant of square matrices of orders $d_1+d_2$ , $d_1+d_2+d_3,\ldots$ up to $d_1+d_2+\ldots+d_k$ . According to wikipedia calculating the determinant of a matrix of order $n$ has complexity of the order $O(n^{2.373})$ but practically it's faster to use the LU decomposition method (this is what, for example, the blaze c++ library does) which has a complexity of $O(n^3)$ . If we have $d_1=d_2=...=d$ then the complexity is $O((2d)^3+(3d)^3...(kd)^3)=O(d^3k^4)$ . Using an Integer relation algorithm: If we know that the degree of $x$ is $n$ then we can run for example the LLL algorithm on ${1,x,x^2...x^n}$ . Problems with that include: Determination of $n$ : in the previous example we can get an upper bound of $d^k$ (yikes) so running the LLL algorithm on this would have complexity $O(d^k)$ assuming I'm interpreting wikipedia correctly. If $n$ does not match an upper bound then we either need to run the algorithm again or factor the polynomial. Precision: if we calculate these values outwards starting at the rational constants (for example $\sqrt[3]{2-\sqrt{2}}\approx\sqrt[3]{0.5858}\approx 0.8367$ ) we run into the problem that each operation has a tendency to exacerbate the errors of the previous ones. This is particularly problematic when subtracting two large numbers whose difference is small. So we'd need bounds on the error $x^i\pm\epsilon$ where the algorithm still returns the correct result. Edit: since writing this, I found this blog post by Jack Coughlin which details how to estimate this error. This greatly mitigates the problem. Afterwards we'd need to check if the polynomial found really has $x$ as a root and whether it's irreducible (this is only necessary if we don't know $n$ in advance, then we'd have to use an upper bound). Trying some symbolic manipulation, especially as a middle step of another algorithm: In some cases isolating the term with the highest root might be helpful. For example $$x=\sqrt{a}+\sqrt[3]{b}$$ $$x^3-3\sqrt a x^2+3ax-a\sqrt a = b$$ $$(x^3+3ax-b)^2=9ax^4-6a^2x^2+a^3$$ I get the impression that this isn't efficient, so maybe we can use a lookup table for small exponents with little nesting. Applications Of course I want to implement this in a computer so it does all the number crunching for me. For example, recently I was trying to do a search over graphs composed of triangular lattices so what I did is set the vertices of the graph as points in $\mathbb C$ so I had to check whether several numbers of the form $\eta(a+b\omega)+\mu$ where $w=\frac{1-\sqrt{-3}}2$ are equal to each other with sympy but since it stores expressions ""as is"" there's no trivial way to check equality, so I needed to check if the minimal polynomial of $\eta_1(a_1+b_1\omega)+\mu_1-\eta_2(a_2+b_2\omega)-\mu_2$ was equal to the identity polynomial, which takes forever . Sympy uses the first algorithm by default and alternatively something involving GrÃ¶bner bases which I did not understand. Of course the object-oriented-programming approach to solving this problem is declaring a class which represents an algebraically closed structure that holds all the numbers you're gonna need (and you better foresee all of them!). For example in c++: // a member of a real or imaginary quadratic field
// a+b*sqrt(n) where a,b,n are integers
class quadratic_int {
  int a;
  int b;
  int n; 
  quadratic_int operator+(quadratic_int p) const {
    // implement addition
  }
  // ...
}; This will be fast to perform operations on, but if you have a number which is not of the form $a+b\sqrt n$ you need to rewrite everything from scratch, plus if $n\equiv 1 \pmod 4$ , $a$ and $b$ need to be half integers, and if you wanted to perform operations on the field $\mathbb Q[\sqrt 2]$ instead the class should probably use a template type instead of ints...all of this complexity means that by the end the class will be over a hundred lines long. If we could compute minimal polynomials fast though, we could have a class called algebraic which internally represents a number as its minimal polynomial together with a way of distinguishing it from the other roots. Then its usage could be something like: algebraic a {""x^2-2"", 0}; // the 0th root of x^2-2 i.e. sqrt(2)
algebraic b {""x^3-3"", 0}; // in 3^(1/3)

// x = 2^(1/2)+3^(1/3)
// so x is an algebraic number of degree 6
// the internal representation of x holds the polynomial
// t^6-6t^4-6t^3+12t^2-36t+1
// as a list of 7 integers (1,0,-6,-6,12,-36,1) plus the number of the root
// which is an integer from 0 to 5
auto x = a + b; Where the 0th, 1st, etc roots are ordered say by absolute value and then by complex argument. This way we still get fast arithmetic without having to rewrite our class every time we need to work on a different algebraic structure. Statement of the question: What is the fastest algorithm to calculate the minimal polynomial of a number expressed in radicals? It should have good asymptotic complexity in the number of terms/the degree of the input. Performance for small entries is not so important because a look-up table can be implemented.","['computational-mathematics', 'minimal-polynomials', 'abstract-algebra', 'computer-algebra-systems', 'numerical-methods']"
3554357,Infinite monkey theorem independent of number of monkeys,"I was just thinking about the infinite monkey theorem and arrived at a creepy conclussion. Let's begin with the probability that one monkey types a set of $N$ letters with a chosen order, which is given by $P = \left( \frac{1}{N_k} \right)^N$ , being $N_k$ the number of keys available in a hypotetical typewriter (for example, the probability of a monkey typing ""MAMA"" in a 26-key typewriter would be $P = \left( \frac{1}{26} \right)^4 \simeq 2\times 10^{-6}$ ). If we have $N_m$ monkeys typewriting, we could think that each monkey $i$ would be required to type only a fraction $N_i = N/N_m$ of the $N$ ordered letters, so that we could arrange later the $N_m$ fractions and create the $N$ -letters original sequence. The total probability of every monkey achieving its task is given by the product of each monkey's probability, $P =  \left( \left( \frac{1}{N_k} \right)^{N_i} \right) ^{N_m} =  \left( \frac{1}{N_k} \right)^N$ , which is independent of $N_m$ . UPDATE: Now I know I have done nothing wrong, but I would like to derive the relation between $P$ and $N_m$ in order to analytically see how the $P$ improves as $N_m$ gets bigger.","['statistics', 'combinatorics', 'combinations']"
3554440,Existence of homomorphism $\phi:\mathbb{Z}[[X]]\to\mathbb{Q}$ such that $\phi(X)=3/4$?,"I'm curious as to whether there is a ring homomorphism from $\mathbb{Z} [[X]]$ ,the ring of formal power series in indeterminate $X$ , to the rationals such that for every polynomial $p(X)$ in the ring of power series, $\phi(p(X))=p(3/4)$ .","['ring-theory', 'abstract-algebra']"
3554479,Why we require to study a specific type of relation called function?,"I am recently preparing for a competitive exam and I am revising concepts again. During preparation, I mused over the following idea and got confused. Firstly, sorry for being too verbose, and Secondly, I know my question is naive in nature. Moving from Relations to Functions, I wondered that why we required a concept of function, if we already have concept of relations. Why we require a specific type of relation in which for every single element of domain there should be a single element in co-domain? i.e. why can't co-domain have two values for single value in domain. Why we define all mathematical expressions (equations) in terms of functions? Why we require to study a specific type of relation called function? I know 'how' of the question i.e. How we can move from relation to function? But, I am confusion over the philosophy or mathematical requirement over the idea of Functions. Hence , I can't figure out 'why' of the question i.e. Why we require to move from relation to function to study Mathematical Function? Final words, we study a specific relation which fulfills surjection, injection, and bi-jection, but strictly can't have two distinct values of domain having same value in co-domain. I can't image the wholesome image of requirement of function to study Mathematical Functions.","['functions', 'relations']"
3554497,"Cauchy's integral formula, application","Let $f: U \to \mathbb{C}$ be a holomorphic function and let $z_0 \in U$ . How can I use Cauchy's integral formula to express the third derivative of $f$ in $z_0$ . I do not see how to show this, but I need to use the statements in (1) and (2). I appreciate any help! (1) Cauchy's integral formula: Let $f$ be a holomorphic function on the open disc centered in in $z_0$ with radius $\rho$ . Then the number $a_n = \frac{1}{2 \pi r^n} \int_{0}^{2 \pi} f(r e^{it} + z_0) e^{-int} dt$ doesn't depend on the choice of $r < \rho$ the power series $\sum a_n z^n$ has a convergence radius of at least $\rho$ We have equality: $f(z) = \sum_{n \geq 0} a_n (z-z_0)^n$ for $ |z-z_0| < \rho$ (2) Uniqueness of developement of Taylor series: Every analytic function $f: U \to \mathbb{C}$ has an unique developement in a power series in an environment of its points $z_0 \in U$ .","['complex-analysis', 'complex-integration', 'complex-numbers']"
3554534,Fisher's exact test two sided confusion,"Consider the guessing milk tea example $$\begin{array}{c|c|c|} 
 & \text{Guess Milk} & \text{Guess Tea} \\ \hline
\text{Milk} & 3 & 1 \\ \hline
\text{Tea} & 1 & 3 \\ \hline
\end{array}$$ I want to test that $H_0: \theta  = 1$ (independent) vs. $H_a: \theta  \neq  1$ (associated) The formula for $P(n_{11} = t) = \frac{{n_{1+} \choose t} {n_{2+} \choose n_{+1}-t}}{{n \choose n_{+1}}}$ It's fixed on $n_{11}$ The range is given by $m_- = max(0, n_{11} - n_{22}) = max(0,0) = 0$ and $m_+ = min(n_{11}+n_{12}, n_{11}+n_{21}) = min(4,4) = 4$ . $$0 \leq n_{11} \leq 4$$ This is what my notes did after that $P(n_{11} = 0) = 0.0143, P(n_{11} = 1) = 0.2285, P(n_{11} = 2) = 0.5143, P(n_{11} = 3) = 0.2285, P(n_{11} = 4) = 0.0143$ Thus, the two sided p-value is $$P(n_{11} = 0) + P(n_{11} = 1) + P(n_{11} = 3) + P(n_{11} = 4) = 0.4857$$ . Why was $P(n_{11} = 2)$ excluded ?","['statistics', 'hypothesis-testing']"
3554589,"Find a formula for c(n, n â 2).","x I understand the answer that we can choose the 3 cycle in nC3 ways, but why is there 2 ways to do this? Hence times 2? Thanks!","['combinatorics', 'permutation-cycles']"
3554601,$\sum_{n = 0}^\infty (-1)^n = \frac{1}{2}$ - Where have I gone wrong?,"Not sure where I've gone wrong in the following: Consider the integral $$
\int_0^\infty e^{-2x}\:dx = \frac{1}{2}
$$ Via some simple manipulation we find: \begin{align}
\int_0^\infty e^{-2x}\:dx &= \int_0^\infty e^{-x} e^{-x} \:dx = \int_0^\infty e^{-x} \left[ \sum_{n = 0}^\infty (-1)^n\frac{x^n}{n!} \right] \:dx \\
&= \int_0^\infty \sum_{n = 0}^\infty \frac{(-1)^n}{n!} x^ne^{-x} = \sum_{n = 0}^\infty \frac{(-1)^n}{n!} \int_0^\infty x^ne^{-x}  \:dx \\
&= \sum_{n = 0}^\infty \frac{(-1)^n}{n!} \Gamma(n + 1) = \sum_{n = 0}^\infty \frac{(-1)^n}{n!} n! = \sum_{n = 0}^\infty (-1)^n
\end{align} And so, $$
 \sum_{n = 0}^\infty (-1)^n = \frac{1}{2}
$$ This is the famous Grandi's Series which is divergent. My question: Where have I gone wrong here? What rule/axiom/etc have I violated in my work in achieving this 'result'?","['integration', 'gamma-function', 'taylor-expansion', 'sequences-and-series', 'convergence-divergence']"
3554607,"Determine whether $f(x,y)=\sqrt{|xy|}$ and/or $g(x,y)=e^{|x|^3y}$ are differentiable at the point $(0, 0)$.","Determine whether $$f(x,y)=\sqrt{|xy|}$$ and/or $$g(x,y)=e^{|x|^3y}$$ are differentiable at the point $(0, 0)$ . Also, find its total derivative if it exists at (0, 0); if not, prove that it is not differentiable at $(0, 0)$ . My approach: For $f(x,y)$ , I got it is not differentiable because the limit tends to $1$ and $-1$ . But I am not sure about $g(x,y)$ .","['derivatives', 'bivariate-distributions']"
3554629,Line bundles have flat connections,"Let $M$ be a manifold of $L \to M$ a line bundle (say over $\mathbb{C}$ , ie complex line bundle). Is it true & why that for every such line bundle there exist a flat connection $\nabla_L : \Gamma(X,E)\to \Gamma(X, \Omega_X^1\otimes L)$ , i.e. a connection which curvature $\nabla_L^2= \Omega_L \in \Omega ^{2}({\mathrm  {End}}\,L)=\Gamma ({\mathrm  {End}}\,L\otimes \Lambda ^{2}T^{*}M)$ is zero. Thus an existence problem. Sure, I not see any reason why all connections on $L$ should be flat, nevertheless I'm asking if on the other hand there always exist a flat one. If yes, is the claim independ of the field (so we can replace $\mathbb{C}$ by any other)?","['connections', 'vector-bundles', 'line-bundles', 'differential-geometry']"
3554685,Help with an exercise in analysis and measure theory.,"Let $(f_ {n})$ be a sequence of functions from $\mathbb{R}$ to $\mathbb{R}$ . Prove that $\displaystyle\{x\in X:(f_{n}(x))\text{ converges in}\ \mathbb{R}\}=\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\}$ My attempt was $(f_{n}(x))\text{ converges}$ iff $\forall \epsilon>0:\exists n\in \mathbb{N}:\forall m\in \mathbb{N}: m>n\rightarrow |f_n(x)-f_m(x)|<\epsilon$ iff $\forall k\in \mathbb{N}:\exists n\in \mathbb{N}:\forall p\in \mathbb{N}:|f_n(x)-f_{n+p}(x)|<\frac{1}{k}$ iff $\displaystyle x\in\bigcap_{k=1}^{\infty} \bigcup_{n=1}^{\infty} \bigcap_{p=1}^{\infty} \{x\in X:|f_{n}(x)-f_{n+p}(x)|<\frac{1}{k}\}$ I did this, but my teacher told me that I was not demonstrating anything with this. It's wrong? I only used the Cauchy criteria and the archimedean property.","['measure-theory', 'solution-verification', 'real-analysis']"
3554690,"How many 3-digit numbers can be formed with the digits $1, 2, 3, 4$?","how many $3$ digit numbers can be formed by $1,2,3,4$ , when the repetition of digits is allowed? So basically, I attempted this question as- There are 4 numbers and 3 places to put in the numbers: In the ones place, any 4 numbers can be put, so there are 4 choices in the ones place. Similarly for the tens and the hundreds place.
So, the total choices are, by multiplication principle- $$4*4*4=64$$ And well and good, this was the answer. But what if I reversed the method? So I take some particular numbers, like $1,2,3$ and say that, well, $1$ can go in $3$ places, $2$ in $2$ places and $3$ in $1$ place, so by multiplication principle, there are $6$ ways of forming a $3$ -digit number with $1,2,3$ . But there are $4$ different numbers.
So the number of $3$ -number combinations are- $(1,2,3)$ , $(1,2,4)$ , $(1,3,4)$ , $(2,3,4)$ .
Each can be arranged in $6$ ways, so we get $24$ ways totally. So why is my answer different here?","['permutations', 'combinatorics']"
3554718,I can't understand a how the podium is built in my problem,"So we are going to build a podium with blocks with the measurements $1 \times 1 Ã 1$ inch the question is as follows. The top layer should measure $2 \times 2$ inches and then each layer should project $1/2$ inch on all four sides relative to the layer above (so that the podium will be staircase-shaped) and the sum for n layer is $\dfrac13n^3+\dfrac32n^2+\dfrac{13}6n$ before I used the equation I thought it was 4 blocks in the top layer and 12 blocks the next layer, but when I tried to confirm that with the equation, it showed for layer 1 4 blocks and for layer 2 13 blocks. So did I misunderstand how to build the podium or did i miss calculate the equation?","['discrete-mathematics', 'sequences-and-series']"
3554780,When $\phi_{\mathcal L}=0$ for $\mathcal L$ a line bundle over an abelian scheme $X/S$,"Let $X\rightarrow S$ be a projective abelian scheme. To a line bundle $\mathcal L$ on $X$ , we associate its Mumford line bundle $\Lambda(\mathcal L):= \mu^{\star}\mathcal L\otimes p_1^{\star}\mathcal L^{-1}\otimes p_2^{\star}\mathcal L^{-1}$ on $X\times_S X$ where $\mu$ is the group law, and $p_i$ the projection morphisms. This defines a homomorphism $\phi_{\mathcal L}:X\rightarrow \hat{X}$ of $X$ into its dual. Eventually, denote by $\mathcal P$ the PoincarÃ© sheaf on $X\times_S \hat{X}$ trivialized along $\epsilon\times id$ , where $\epsilon$ is the unit section of $X$ . Claim: Assume that $\mathcal L\cong (id\times\hat{x})^{\star}\mathcal P$ for some section $\hat{x}:S\rightarrow \hat{X}$ . Then $\phi_{\mathcal L}=0$ . The hypothesis $\mathcal L\cong (id\times\hat{x})^{\star}\mathcal P$ means that the $S$ -point of $\mathcal{Pic}_{X/S}$ defined by the line bundle $\mathcal L$ on $X=X\times_S S$ factors through $\hat{X}$ , and it is exactly $\hat{x}$ . The paper I am reading (Genestier and NgÃ´'s lecture on Shimura varieties) gives as a justification that the statement is trivial for $\mathcal L\cong \mathcal O_X$ , and that we can continously deform a general $\mathcal L$ to $\mathcal O_X$ using rigidity lemma to obtain the desired result. I have trouble writing down a rigorous proof for this. Given an $S$ -scheme $T$ and a $T$ -valued point $z$ of $X$ , at the level of points, $\phi_{\mathcal L}(z)$ is the element of $Pic_{X/S}(T)$ determined by the pullbak of $\Lambda(\mathcal L)$ by $id\times z$ . I must somehow prove that this pullback on $X\times_S T$ is actually the pullback of some line bundle on $T$ . It would be true if I could prove that $\Lambda(\mathcal L)$ is the pullback by $p_2$ of some line bundle on $X$ . Thus I am looking at $\mu^{\star}\mathcal L\otimes p_1^{\star}\mathcal L^{-1}\cong \mu^{\star}(id\times\hat{x})^{\star}\mathcal P\otimes p_1^{\star}(id\times\hat{x})^{\star}\mathcal P^{-1}$ . I naturally considered the morphism $(id\times\hat{x})\mu - (id\times\hat{x})p_1: X\times_S X\rightarrow X\times_S \hat{X}$ , which we can look as a map between two abelian schemes over $X$ (with structure morphisms the second projection at the start, the first projection at the target). This map sends the unit section on the unit section, hence by rigidity it is a group homomorphism. But I am stuck there, I do not see how to progress further. Would somebody be able to give a hand there ? Edit : I actually just noticed that my last argument is not true, the map $(id\times\hat{x})\mu - (id\times\hat{x})p_1$ isn't sending the unit to the unit after all. I left the wrong argument, but I fear I should find another way to use rigidity.","['group-schemes', 'algebraic-geometry', 'abelian-varieties', 'picard-scheme']"
3554803,Property of $n$th derivative of a real-valued function,"$Q)$ Let $f(x)$ be a real valued function of a real variable, such that $\ |f^{(n)}(0)|\leq K$ for all $ n\in N$ , where $K\gt0$ . Then state and explain whether the following statement is true $$ ""f^{(n)}(x) \ exists\ for\ all\ x\in R\ and\ for\ all\ n\in N"" $$ $A)$ I think it is false and here is my explanation. Since the function $\ f(x)$ is not specified, I assume the above statement is true only when it is true for all such functions $\ f(x)$ which satisfy the condition mentioned in the question. So I am going to prove the statement wrong by giving an example of one such function which satisfies the condition but for which the above mentioned statement is false.
Let $$ f(x)=\begin{cases}
e^x, & x\in[-1,1]\\ [x], & x\notin[-1,1]
\end{cases}$$ as we can calculate $\ f^{(n)}(0)=1$ for all $n\in N$ so it satisfies the condition $\ |f^{(n)}(0)|\leq K$ for all $ n\in N$ , where $K\gt0$ since here $K$ can be equal to $2$ . But as we know $[x]$ which represents the greatest integer function is not differentiable everywhere. So the statement is false for the above function. This question was asked in a very recent exam I wrote. I thought the answer was ""false"" but the answer key states ""true"". This is not supposed to be the final answer key as the students can still challenge the answer key. So which one do you think is correct. If you think I am correct and the answer is ""false"" can you provide me a better proof so that I can challenge the answer key. And if you think I am wrong and the answer is ""true"". Please explain why. Do you think it is because the statement and condition are satisfied for some function $f(x)$ if not for all ?","['functions', 'derivatives']"
3554866,Accuracy of estimation of the variance,"Consider $N$ i.i.d. samples $x_1, \dots, x_N$ from an unknown discrete distribution on $\{0,\dots, n\}$ . We know that $$\frac{1}{N-1} \sum_{i = 1}^N (x_i - m)^2$$ where $m$ is the sample mean, is an unbiased estimator for the variance. But what confidence intervals can we give for this estimate?","['statistics', 'variance']"
3554891,Extending the Law of Cosines formula to quadrilaterals (and even polygon?),"Let's take a look back at this familiar ""Law of cosines"": âConsiderâ the âtriangle â $\triangleââ ABC$ . Let $a = BC, b = AC, c = AB$ ; $\angle A, \angle B, \angle C$ are the angles of the triangle opposite to side $a, b, c,$ respectively. By the Law of Cosines: $$a^{2â} = b^{2â} + c^{2â} - 2bc \cdot \cos \angle A$$ This formula can apply for any triangle. But what about quadrilaterals? Is there a formula, which shows the relationship between sides and angles, similar to the Law of Cosines? Can we extend the Law of Cosines??? This is the way to approach the formula for quadrilaterals ( It's not (really) a proof ): Given the quadrilateral ABCD. Let $a = BC, b = CD, c = AB, d = AD$ . Let $E = AB \cap CD$ and $G = AC \cap BD$ Let consider $\triangle ABC$ as a ""special quadrilateral"" (where $d=0$ ). Then by the Law of Cosines: $$a^{2â} = b^{2â} + c^{2â} - 2bc \cdot \cos \angle BEC = b^{2â} + c^{2â} - 2bc \cdot \cos \angle BGC$$ (because when $d=0$ , $E \equiv G \equiv A \Rightarrow \angle BEC = \angle BGC$ ) Notice that when $d=0$ then $CA = CD = CE = b$ ; $BD = BE = BA = c$ . So we can guess the general formula for a quadrilateral will be one of these two formulas: $$ a^{2â} + Kd^{2â} = b^{2â} + c^{2â} - 2 \cdot BE \cdot CE \cdot \cos \angle BEC \text{ (1)}$$ $$ a^{2â} + Kd^{2â} = b^{2â} + c^{2â} - 2 \cdot BD \cdot CA \cdot \cos \angle BGC \text{ (2)}$$ (where $K$ is a constant) The reason we add $Kd^{2â}$ is to make the formula homogeneous (since the Law of Cosines is also homogeneous), and when $d=0$ , the $Kd^{2â}$ term is gone. Moreover, from our intuition, if the formula contains $\angle BEC$ , then two sides, which multiply to its cosines, have to be $BE$ and $CE$ . Otherwise, those two sides will be $BD$ and $CA$ multiplied by $\cos \angle BGC$ To see which one is possibly correct, we can try to apply the formula to a special quadrilateral: square. In a square, $a=b=c=d$ , "" $BE = CE = \infty$ "", "" $\angle BEC = \infty$ "", $\angle BGC = 90^{\circ}$ . Apply $(1)$ and $(2)$ : $$(1): a^{2â} + Ka^{2â} = a^{2â} + a^{2â} - \infty$$ $$(2): a^{2â} + Ka^{2â} = a^{2â} + a^{2â}$$ $(1)$ is definitely wrong. The formula $(2)$ can be true if $K=1$ , so let re-written it: $$a^{2â} + d^{2â} = b^{2â} + c^{2â} - 2 \cdot BD \cdot CA \cdot \cos \angle BGC$$ To be sure that this formula is correct, let's apply this in another quadrilateral. This time is a rectangle, where $\angle BGC = 60^{\circ}$ . We have $a=d, b=c=a\sqrt{3}$ , $BD = AC = 2a$ . Apply the formula that we've just found, we get: $$a^{2â} + a^{2â} = 3a^{2â} + 3a^{2â} - 2 \cdot 4a^{2â} \cdot \frac{1}{2}$$ And this is true. You can verify it with some other quadrilaterals, and it'll also true. So, our new extended ""Law of Cosines"" is: $$a^{2â} + d^{2â} = b^{2â} + c^{2â} - 2 \cdot BD \cdot CA \cdot \cos \angle BGC$$ So that seems fine. But Is there a proof of the formula above ? Now, my main question (and my main focus) is: Can we extend the formula (find a general formula) for polygons with n sides ? This question is what I'm looking for ( This isn't a homework question ). I'm really curious about this. If you have an answer (or just an idea) to approach, please provide it. Thank you a lot and have a nice day :D","['quadrilateral', 'geometry', 'polygons', 'solution-verification', 'trigonometry']"
3554917,Show differentiability,"Suppose that $u=u(x,t)$ is a real-valued function on $\mathbb{R}\times (0,\infty)$ with $$
u(x,t)=\int_0^t\int_{\mathbb{R}}G(x-y,t-s)f(u(y,s))\, dy\, ds\tag{1}
$$ (where $G(x,t):=\frac{1}{4\pi t}e^{-\frac{\lvert x\rvert^2}{4t}}$ is the heat kernel and $f(u)$ is a bounded and continuously differentiable real function with bounded derivative.) How can I proof whether u is differentiable? I think I have to show that $\partial_x u=:u_x$ and $\partial_t u=:u_t$ exist and are continuous. But I have no concrete idea how to prove that. I think one maybe should use that we also have $$
u(x,t)=\int_0^t\int_\mathbb{R}G(y,s)f(u(x-y,t-s))\, dy\, ds\tag{2}
$$ and then (I hope its correct): $$
u_x(x,t)=\int_0^t\int_\mathbb{R}G(y,s)\frac{d}{dx}f(u(x-y,t-s))\, dy\, ds,
$$ $$
u_t(x,t)=\int_0^t\int_\mathbb{R}G(y,s)\frac{d}{dt}f(u(x-y,t-s))\, dy\, ds+\int_\mathbb{R}G(y,s)f(u(x-y,0))\, dy
$$ Can I use this to show existence and continuity of both partial derivatives? I think the problem is that I do not have information about the terms $\frac{d}{dx}f(u(x-y,t-s))$ and $\frac{d}{dt}f(u(x-y,t-s))$ . But am I in general right that the crucial point for the existence of $u_t$ and $u_x$ is to show that all these integrals are finite, i.e. the integrands are integrable functions? Edit : Maybe I shouldn't use (2) but stick to (1)? Then, I get $$
u_x(x,y)=\int_0^t\int_\mathbb{R}G_x(x-y,t-s)f(u(y,s))\, ds
$$ I think this is a finite integral since $f$ is bounded and the heat kernel is infinitely differentiable and also integrable (again: does this imply the existence of $u_x(x,y)$ ?). Moreoever, using (1), $$
u_t(x,t)=\int_0^t\int_\mathbb{R}G_t(x-y,t-s)f(u(y,s))\, dy\, ds+\int_\mathbb{R}G(x-y,0)f(u(y,t))\, dy.
$$ However, $G(x-y,0)$ is a singularity. But maybe one can use $$
\int_\mathbb{R}G(x-y,0)f(u(y,t))\, dy=\int_\mathbb{R}G(y,t)f(u(x-y,t-s))\, dy.
$$","['integration', 'derivatives', 'analysis', 'heat-equation']"
3554978,Problem Interpreting histogram,"I am reading Understandable statistics . In the second chapter, example 2, the author shows the following histogram: The author then proceed to interpret the histogram as follows: About 40% of the wait times fall between 7 and 9 minutes
  while about 80% are between 4 and 9 minutes. Less than 1%
  are 3 minutes or less or greater than 12 . I think that the statement: Less than 1%
  are 3 minutes or less or greater than 12 is false. It should be: Less than 10%
  are 3 minutes or less or greater than 12 Is the author correct or am I missing something?","['descriptive-statistics', 'statistics']"
3554988,Every nonzero element in a Banach space has a norming extreme point,"For any Banach space $F,$ let $B_F$ be the closed unit ball of $F,$ that is, $B_F = \{x\in F: \|x\| \leq 1\}.$ Also, let $ext B_F$ be the set of extreme points of $B_F$ (Recall that $x$ is an extreme point of $B_F$ if $x = \frac{1}{2}(x_1+x_2)$ for some $x_1,x_2\in B_F$ implies that $x= x_1=x_2.)$ Let $F^*$ be the continuous dual space of $F.$ Question: Let $F$ be a Banach space and $x\in F\setminus\{0\}.$ Is it true that there exists $x^*\in ext B_{F^*}$ such that $x^*(x) = \|x\|?$ I think it is true. Assume that $\|x\| = 1.$ Consider $$S = \{x^*\in B_{F^*}: x^*(x) = 1\}.$$ By Hahn-Banach Theorem, $S$ is nonempty. 
Clearly $S$ is convex. I would like to show that $S$ is weak-star closed in $B_{F^*}$ so that it is weak-star compact (Banach-Alaoglu states that $B_{F^*}$ is weak-star compact). Then by Krein-Milman Theorem, $S$ has an extreme point, say $z^*.$ It is easy to see that $z^*$ is also an extreme point of $B_{F^*}.$ Such $z^*$ is our desired bounded linear functional. However, I am not sure how to show that $S$ is weak-star closed.
Any hint is appreciated.","['banach-spaces', 'hahn-banach-theorem', 'functional-analysis']"
3555007,Direct formula for sequence,"I want to find out the function to generate the sequence with the following pattern. 1 2 3 1 1 2 2 3 3 1 1 1 1 2 2 2 2 3 3 3 3 ....
Where the lower bound number is 1 upper number bound number is 3. Each time numbers start from 1 and each number repeats 2 ^ nÂ times, with n starting with 0.","['generating-functions', 'functions', 'sequences-and-series']"
3555018,Approaches for the study of rings,"When, as a student in my first weeks, I first read the definition of a group, the concept of groups looked very natural and motivated to me. But when I read the definition of a ring some time later, I had no idea what rings should be about and how they were motivated. Especially the distributive property let me wonder how much of the multiplicative structure is already determined by the addition. All of this is has been two years ago. When I recently started to read Algebra: Chapter $\it 0$ by Paolo Aluffi, a lot of thinks I had not understood before, became clear to me. Mostly because of the use of categorical language which really works well with the way I think about mathematics. I even came to peace with the concept of rings, when Aluffi explained them as coming up in the context of studying the homsets of abelian groups (we take the group operation as addition and the composition as multiplication). I have been very happy about this explanation, but then, instead of studying ring by using this approach, Aluffi used the 'classical' definition of rings again. My question is: Why? Is it not much more promising to study rings by studying the homsets of abelian groups? I know that there is not any loose of information by just using the classical definition, but using the definition via homsets looks a lot more natural to me. I know some category theory (at least I read Emily Rhiels book Category Theory in Context ) so I could read I bit trough nLab and soon found out that a ring is equivalent to a preadditive category with only one object (encapturing pretty much what I called 'homset definiton'). So why don't we study thous preadditive categories instead? They seem to have far more 'visible structure' then rings have in their usual definition. (I know that this is kind of a soft question. I do not look in particular for a kind of 'rigorous answer', but rather for your personal view towards this.)","['ring-theory', 'abstract-algebra', 'soft-question', 'category-theory']"
3555019,Prove $PQ || HF$,"Let $ABCD$ be a quadrilateral with the midpoints of every side $E,F,G,H.$ $AG,DE$ meet at $P$ and $CE,BG$ meet at $Q$ . Show that $PQ|| HF$ . Indeed, we can assert that $R$ , the intersection point of $AC,BD$ , lies on $PQ$ , by Pappus theorem. This will help? Please give a pure geometric proof rather than algebraic one.",['geometry']
3555034,An example of unbounded isometry,"Suppose $H, K$ are two Hilbert spaces, $V: H\to K$ is an isometry if $V^*V=1$ . Does there exist  a concrete eexample of a unbounded isometry?","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
3555051,Prove an inequality by using the integral test or another way,"âLet the function ââ $âg:â\mathbb{R^+}âââ\rightarrowââ\mathbb{R^+}â$ âhave âthe âproperties âthatââ for each â $âw>0â$ â, â $$
\lim_{x\to\infty}â\frac{g(x+w)}{g(x)} = 1â
$$ and $\log g(x)â$ âis âconcaveâ. âNow, since â $â\log gâ$ âis âconcave, âso âit âhas âderivativeâ $$
\big(\log g(x)\big)^\prime=\frac{g^\prime_-(x)+g^\prime_+(x)}{2g(x)}
$$ except, possibly, on a countable set,  where â $âg^\prime_{+}(xâ)â$ âand ââ $âg^\prime_{-}(x)â$ âare âright âand âleft âderivatives, ârespectively. Also, since $$
ââââ\displaystyle{\lim_{x\to\infty}}â\frac{g(x+w)}{g(x)} = 1â
$$ for each â $âw>0â$ and $\log g(x)â$ âis âconcave, âthen ââ $âgâ$ âis âincreasing. âNow, âââââââââmy âquestions âis:ââ âââ How should I prove the following inequality?â $$
0\leq\gamma_g+\log g(1)\le \frac{g^\prime_-(1)+g^\prime_+(1)}{2g(1)},
â$$ where $$
\sum_{i=1}^n \frac{g^\prime_-(i)+g^\prime_+(i)}{2g(i)}\to \gamma_g\text{ as }n\to\infty.
$$ Hint . Note that we can using the proof of the integral test for the convergence of infinite series, for the proof inequality (but I can't prove it).","['convex-analysis', 'real-analysis', 'sequences-and-series', 'inequality', 'derivatives']"
3555072,Decomposition of the Riemann curvature operator in $4$ dimensions,"Let $(M,g)$ be a $4$ -dimensional Riemannian manifold. The Riemann curvature tensor can be viewed as an operator $\mathcal{R}:\Lambda^2(T^{\star}M)\longrightarrow \Lambda^2(T^{\star}M)$ defined in this way (I'm using Einstein's notation): $$(\mathcal{R}(\omega))_{ij}=\frac{1}{2}R_{klij}\omega_{kl}$$ where $\omega_{ij}$ are the components of the $2$ -form $\omega$ with respect to an orthonormal basis $\{e^i \wedge e^j\}_{i,j=1,..,4}$ and $R_{ijkt}$ are the components of the Riemann curvature tensor. Because of the splitting $\Lambda^2 (T^{\star}M)=\Lambda_{+}\oplus \Lambda_{-}$ , the operator $\mathcal{R}$ can be written in a block form $$\begin{pmatrix} A & B \\ ^{t}B & C \end{pmatrix}$$ where $^{t}A=A$ , $^{t}C=C$ and $trA=trC=\frac{S}{4}$ , with $S$ the scalar curvature of $M$ . So basically, if we write a $2$ -form $\omega=\omega_{+} + \omega_{-}$ according to the splitting, we have that we can also write $\mathcal{R}\omega=(\mathcal{R}\omega)_{+}+(\mathcal{R}\omega)_{-}$ : in particular, $\mathcal{R}(\omega_{\pm})=\mathcal{R}(\omega_{\pm})_{+}+\mathcal{R}(\omega_{\pm})_{-}$ and, for example, $A$ ""sends"" $\omega_+$ in $\mathcal{R}(\omega_{+})_{+}$ . Now, I read in a few papers that, if we call $A_{ij}$ the components of the matrix $A$ , with $i,j=1,2,3$ , we have that $A_{12}=A_{13}=0$ if and only if $A$ is a multiple of the identity matrix. How is that possible? I tried to work directly with the components of the Weyl tensor $W$ of the manifold, since its self-dual part is strictly related to the matrix $A$ , but I couldn't get anything. I tried also to exploit the fact that a matrix is a multiple of the identity matrix if and only if it commutes with any other matrix, but I don't know if it's useful. EDIT: In these days I read also that this fact may be related to the property of the curvature form $\Omega$ , i.e. $R_g ^{\star}\Omega = g^{-1}\Omega g$ , where $R_g$ denotes the right multiplication by $g\in SO(4)$ in the bundle of orthonormal frames of $N$ . How could it help?","['curvature', 'differential-forms', 'riemannian-geometry', 'differential-geometry']"
3555102,Stuck on probability/statistics question,"EDIT: Post as been edited to address relevant questions raised in comments. I'm new to the site and I'm stuck on a probability question.  I don't think it's trivial, certainly not to me, as I am relying on a single probability and statistics class I took 20 years ago...  The problem is this: Imagine there are 10 white squares (the number of white squares is not terribly important other than it has to be 5 or greater). The probability of turning a white square black is 25% per attempt. The goal is to maximize the number of black squares you get. You have 10 ""moves"" to do so. Attempting to turn a white square black uses one move. Moving from one square to another also uses one move. You send in your ""moves"" in batches of 10 and, therefore, you cannot adjust subsequent moves regardless of the results of previous moves. Attempting to convert a square that you had previously successfully converted from white to black does not revert it back to white. It just means you wasted a move trying to convert an already black square. How would I even start figuring out the best way to maximize the number of black squares I get (i.e., how many attempts per square before moving to the next square)? Thank you very much!","['statistics', 'probability']"
3555132,Inequality involving stochastic dominance in Likelihood Ratio Order,"Problem : I am struggling for some time to show that a particular inequality holds or to find a counterexample to disprove it: Suppose you have two continuous random variables $X_1, X_2$ with densities $f_1$ and $f_2$ , respectively, which are both distributed on the same compact interval $[\underline{x},\overline{x}]\subset \mathbb{R}_+$ and have the following properties: 1) Both densities are log-concave. 2) $f_i(\underline{x})\cdot\underline{x} \le 1$ $\hspace{2ex}$ $i \in \{1,2\}$ 3) $X_1$ stochastically dominates $X_2$ in the Likelihood Ratio Order, i.e. $\forall x <x'$ , $x,x' \in [\underline{x},\overline{x}]$ it holds that: \begin{equation}
\frac{f_1(x)}{f_2(x)} \le \frac{f_1(x')}{f_2(x')}
\end{equation} Note that the third property implies that $X_1$ stochastically dominates $X_2$ also in the Hazard Rate-, Reverse Hazard Rate- and First Order. Next, consider two particular realizations $\hat{x}_1$ and $\hat{x}_2$ which are implicitly defined in the following way: \begin{align*}
\hat{x}_1 = \frac{1-2\cdot F_1(\hat{x}_1)}{f(\hat{x}_1)} \hspace{4ex} \hat{x}_2 = \frac{1-2\cdot F_2(\hat{x}_2)}{f(\hat{x}_2)}
\end{align*} Properties 1) and 2) are sufficient for existence of such values in $[\underline{x},\overline{x}]$ and property 3) implies that $\hat{x}_1 > \hat{x}_2$ . With all this information, I wanted to show that $F_1(\hat{x}_1) \le F_2(\hat{x}_2)$ . Since I miserably failed in doing it so far, I also tried to find counterexamples and haven't found any. Approach : Start with the implicit equations defining $\hat{x}_1$ and $\hat{x}_2$ . Rearranging them yields: \begin{align}
&F_1(\hat{x}_1) = \frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2}\\
&F_2(\hat{x}_2) = \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2}
\end{align} Now, in order for the desired inequality to hold, namely $F_1(\hat{x}_1) \le F_2(\hat{x}_2)$ , it suffices to show that: \begin{align}
\frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2} \le \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2} \Leftrightarrow \hat{x}_1 \cdot f_1(\hat{x}_1) \ge \hat{x}_2 \cdot f_2(\hat{x}_2)
\end{align} Now, since $\hat{x}_1 > \hat{x}_2$ , by property 3) we know that: \begin{align}
\frac{f_1(\hat{x}_1)}{f_2(\hat{x}_1)} \ge \frac{f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \Leftrightarrow 
f_1(\hat{x}_1) \ge \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)}
\end{align} Using this for the original inequality, it now sufficies to show that: \begin{align}
\hat{x}_1 \cdot \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \ge \hat{x}_2 \cdot f_2(\hat{x}_2) \Leftrightarrow \hat{x}_1 \cdot f_2(\hat{x}_1)\cdot f_1(\hat{x}_2) \ge \hat{x}_2 \cdot f_2(\hat{x}_2)^2
\end{align} And this is the step in this approach where I am stuck... I would really appreciate some help. Thanks!",['probability']
3555198,"Factorizing $ f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R $ for $(\lambda, R)\in \mathbb{R}^2$","Consider the following bivariate polynomial $$
f(\lambda, R) = -6+4\lambda+6R-2R^2+2\lambda R^2-5\lambda R \, .
$$ It can readily be seen that $f(2,1) = 0$ . But, would it be possible to write $f(\lambda, R)$ as a product of two or more multivariate polynomials? Thank you","['real-analysis', 'multivariable-calculus', 'calculus', 'factoring', 'polynomials']"
3555221,What does a subscript F represent?,"On page 11 of the slide , Sum-of-least-square loss: $$
\ell\left(\mathbf{\tilde W}\right)
 = \sum_{n=1}^N \left\| \mathbf{\tilde W}^T\mathbf{\tilde x^{(n)}}
                       -\mathbf{t}^{(n})
                \right\|^2
 = \left\|\mathbf{\tilde X\tilde W-T}\right\|^2_F
$$ the $n$ -th row of $\mathbf{\tilde X}$ is $\left[\mathbf{\tilde x}^{(n)}\right]^T$ the $n$ -th row of $\mathbf{T}$ is $\left[\mathbf{t}^{(n)}\right]^T$ I don't get what the subscript $F$ means in the equation. I don't even know what tags I should put. Any modification would be appreciated!","['matrices', 'normed-spaces', 'notation']"
3555301,How to solve a differential equation $(e^x+2\ln y)ydx+xdy=0$,"Solve the following differential equation: $$
(e^x+2\ln y)ydx+xdy=0
$$ It is clear that the equation is not exact. So, I tried to express $y'$ : $$
\begin{aligned}
e^xy+2y\ln y+xy'=0\iff
\begin{cases}
\left[ 
\begin{aligned}
&y\equiv0\\
&y\equiv e^{-{1/2}}
\end{aligned}
\right. \ \ \text{if}\ \ x=0\\
y'=-\frac{e^x}{x}\cdot y-\frac{1}{x}\cdot2y\ln y\ \ \ \text{otherwise}
\end{cases}
\end{aligned}
$$ The problem is that the differential equation seems to be non-linear, and I don't know the ways of solving those. Maybe there's an easier way of solving the initial differential equation?","['calculus', 'ordinary-differential-equations']"
3555367,Definition of Random Measures,"Introducing the notion of a random measure, textbooks usually start with a locally compact second countable Hausdorff space. Where does this requirement come from? I would like to have a motivation for this requirement. That is, I would like to define a random measure on a general measurable space $(X,\mathcal{B})$ simply as a kernel from a probability space to $(X,\mathcal{B}).$ Additional structure of $(X,\mathcal{B})$ should then be motivated by e.g. counterintuitive examples. For example, in the book of Last and Penrose (2017) http://www.math.kit.edu/stoch/~last/seite/lectures_on_the_poisson_process/media/lastpenrose2017.pdf Exercise 2.5 yields that point measures usually do not have the representation with a Dirac measures. Are there other examples, (intuitive) motivations and reasons to use a locally compact (!) second countable (!) Hausdorff space?","['geometric-probability', 'point-processes', 'stochastic-processes', 'probability-theory', 'probability']"
3555375,"Checking the solution to $2yy'=x(y')^2+4x,\ y(1)=-2$","I have to find a particular solution to the following differential equation: $$
2yy'=x(y')^2+4x,\ y(1)=-2
$$ I decided to substitute the values $x=1$ and $y=-2$ into the given equations. So, I got: $$
\begin{aligned}
&-4y'=(y')^2+4\iff(y'+2)^2=0\iff y'=-2\iff y=-2x+C\Rightarrow\\
&\Rightarrow [\text{Substitute $y$ into the initial diff. eq.}]\Rightarrow (-4x+2C)(-2)=4x+4x\Rightarrow C=0\\
&\text{Answer: } y=-2x
\end{aligned}
$$ But I'm not sure of my solution. Could anyone check it please?","['calculus', 'ordinary-differential-equations']"
3555380,Find limit $\lim_{n\to\infty}\sum_{k=n}^{5n}{k-1\choose n-1}\left(\frac{1}{5}\right)^{n}\left(\frac{4}{5}\right)^{k-n}$,"The question is pretty straightforward. Find $$\lim_{n\to\infty}\sum_{k=n}^{5n}{k-1\choose n-1}\left(\frac{1}{5}\right)^{n}\left(\frac{4}{5}\right)^{k-n}$$ Attempt Let's denote $F$ as $$
\begin{align*}
& \lim_{n\to\infty}\sum_{k=n}^{5n}{k-1 \choose n-1}\left(\frac{1}{5}\right)^{n}\left(\frac{4}{5}\right)^{k-n} = \\ 
&= \lim_{n\to\infty}\left(\frac{1}{5}\right)^{n}\frac{1}{(n-1)!}\sum_{k=n}^{5n}\left(\frac{4}{5}\right)^{k-n}\frac{(k-1)!}{(k-n)!} =  \\ &= \lim_{n\to\infty}\left(\frac{1}{5}\right)^{n}\frac{1}{(n-1)!}\sum_{k=0}^{4n}\left(\frac{4}{5}\right)^{k}\frac{(k+n-1)!}{k!} = F
\end{align*}
$$ Try to establish a lower and upper bounds of denoted function $F$ , lower bound set to 0, and upper bound to $$\lim_{n\to\infty}\left(\frac{1}{5}\right)^{n}\frac{1}{(n-1)!}\sum_{k=0}^{\infty}\left(\frac{4}{5}\right)^{k}\frac{(k+n-1)!}{k!}$$ The sum inside of a limit equals to $(1 + \frac{4}{5})^{n}$ (by using Taylor series). By substituting this into the obtained upper bound, we're getting $0$ as an answer.","['limits', 'calculus', 'combinatorics', 'combinations']"
3555387,Misunderstanding Lax-Milgram,"I think I misunderstand the Lax-Milgram theorem. Suppose that I have a bilinear form $a$ satisfying the conditions of the Lax-Milgram theorem on a Hilbert space H. Then it must satisfy the same conditions on every Hilbert subspace of H. Therefore, the variational problem a(u,v)=l(v), where the linear form $l$ is continuous on H, has a unique solution on every Hilbert subspace of H. But from the point of view of PDE Hilbert subspaces are just additional conditions on the function. For example, let us consider the Poisson equation $\Delta u=f$ on a bounded domain $\Omega$ . This problem has a unique solution on $H^1_0(\Omega)$ by Lax-Milgram. Let's take a hyperplane $H$ in $H^1_0(\Omega)$ defined by $H=\{v\in H^1_0(\Omega)~|~\int_\Omega v=0\}$ . Then once again by Lax-Milgram there's a unique solution $u'$ on $H$ . But it must be the same solution since $u$ is unique. So we get that for any solution $u$ of $\Delta u=f$ on $H^1_0(\Omega)$ one has $\int_\Omega u=0$ which is not true. Where is the mistake? Thank you!","['functional-analysis', 'partial-differential-equations']"
3555403,Does a holomorphic $f$ exist satisfying these conditions?,"We need to determine if there exists a holomorphic function $$f: \Delta \to \mathbb{C},$$ where $$\Delta = \{z \in \mathbb{C} : |z| <1\}$$ is such that $$\frac{d^n f}{d z^n} \left(\frac{1}{n}\right)= 2^n n! $$ My intuition is that there should not be such a function. My idea was to assume that $$f(z) =\sum_{n=1}^{\infty} a_n z^n,$$ and to determine a recursion relation between the coefficients $a_n$ by substituting the information about the derivative above. I suspect that the coefficients should not converge to zero. However, I struggled trying to show this. Any help would be greatly appreciated.","['complex-analysis', 'functions']"
3555418,Find $\lim_{x \to \infty} x^3 \left ( \sin\frac{1}{x + 2} - 2 \sin\frac{1}{x + 1} + \sin\frac{1}{x} \right )$,"I have the following limit to find: $$\lim\limits_{x \to \infty} x^3 \bigg ( \sin\dfrac{1}{x + 2} - 2 \sin\dfrac{1}{x + 1} + \sin\dfrac{1}{x} \bigg )$$ What approah should I use? Since it's an $\infty \cdot 0$ type indeterminate I thought about writing $x^3$ as $\dfrac{1}{\frac{1}{x^3}}$ so I would have the indeterminate form $\dfrac{0}{0}$ , but after applying L'Hospital I didn't really get anywhere.","['limits', 'calculus']"
3555426,Linear ordinary differential equations and matrix exponentiation,"I feel the solution to this problem is simple - but I am not entirely clear on what the question actually wants. Given the following: $$A = \left[
\begin{array}{ccc}
  -0.1005 & -0.266\\
  -0.1498 & 0.2005 
\end{array}
\right] $$ $$\frac{dx}{dt} = Ax; $$ $$x(0) = x0 = [1; â2]$$ Show, in general, that the following form solves $\frac{dx}{dt} = Ax$ i) $$  x(t) = e^{At}x_0$$ ii) $$x(t) = \sum_{i}a_i e_i e^{\lambda_it}$$ Usually when I come across a problem I don'tt even understand, I try to solve simpler analogous problems. I can't quite find one comparable to this. Can someone point out the first couple of steps for me? Maybe for i), for example?","['matrices', 'matrix-exponential', 'ordinary-differential-equations']"
3555428,Application of the Girsanov theorem,"Let $B_t$ denote the standard brownian motion. In a homework exercise we are asked to use the Girsanov theorem to compute \begin{equation}
\mathbb{E}\bigg((B_t-t)^2\exp\bigg(\int_0^t e^{-s}\, dB_s\bigg) \bigg) \tag{1}
\end{equation} After reading about the Girsanov theorem I fail to understand how to apply it in this particular situation. My attempt so far is to first note that $$
\mathbb{E}\bigg(\frac{1}{2}\int_0^t e^{-2s}\,ds\bigg) = \mathbb{E}\bigg(\frac{1}{4}\big(1-e^{-2t}\big) \bigg) < \infty , \ \forall t\geq 0
$$ hence the Novikov condition is satisfied. In the theory I've read, this means that if we set $$
L_t = \exp\bigg(-\int_0^te^{-s}\,dB_s - \frac{1}{2}\int_0^te^{-2s}\,ds \bigg)
$$ and the denote by $Y_t = (B_t - t)^2$ and further if the expectation in equation (1) is with respect to some probability measure $\mathbb{P}$ there exists a probability measure $\mathbb Q$ such that \begin{equation}
\mathbb E_{\mathbb Q}\bigg( Y_t \bigg) = \mathbb E_{\mathbb P}\bigg( Y_t L_t \bigg)
\end{equation} Here is where I'm stuck, first, in equation (1) there is no minus sign in the exponential, nor is the quadratic term present. Where do I go from here?","['brownian-motion', 'probability-theory', 'stochastic-calculus']"
3555456,Find $\lim\limits_{x \to 1} \dfrac{f_{n+1}(x) - f_n(x)}{(1-x)^{n+1}}$ where $f_n(x) =x^{x^{...^{x}}}$.,"For $n \ge 1$ and $x \in (0, \infty)$ , consider the function: $$f_n(x) = x^{x^{...^{x}}}$$ where $n$ represents the number of $x$ 's in $f$ . For example, we'd have $f_1(x) = x$ , $f_2(x)=x^x$ , $f_3(x) = x^{x^x}$ and so on. I have to find the limit: $$\lim\limits_{x \to 1} \dfrac{f_{n + 1}(x) - f_{n}(x)}{(1 - x)^{n + 1}}$$ I don't know how to solve this. I made the observation that: $$f_{n + 1}(x) = (f_n(x)) ^ x$$ but it didn't help me all that much. However I look at it, it seems like I have to apply L'Hospital, but I don't see any way of finding the derivative of $f_n(x)$ . Is there some other approach that I should use?","['limits', 'calculus']"
3555467,"If $A,B$ are diagonalisable, does $AB$ diagonalisable imply $BA$ diagonalisable?","As discussed in this other post , $AB$ and $BA$ always have the same characteristic polynomial, but not necessarily the same minimal polynomial. This means that $AB$ diagonalisable does not imply $BA$ diagonalisable, as shown by the example $$A=\begin{pmatrix}0&1\\0&0\end{pmatrix},\qquad B=\begin{pmatrix}0&0\\0&1\end{pmatrix}.$$ On the other hand, we know that if both $A,B$ are invertible , then the statement is true. As mentioned here , it is also the case that the non-zero spectrum of $AB$ and $BA$ is always the same. I am looking for an answer to the slightly different question: assuming $A,B$ are both diagonalisable, is it true that $AB$ diagonalisable implies $BA$ diagonalisable? The counterexamples I've seen (the post linked above and this one ) all involve at least one non-diagonalisable matrix, hence my question. If the above is not true, what about the more restrictive case of $A,B$ both normal? Or maybe the case with $AB,BA\neq0$ ?","['eigenvalues-eigenvectors', 'linear-algebra', 'characteristic-polynomial', 'spectral-theory', 'diagonalization']"
3555482,Is there a classification of bivector-valued cross products?,"Background Vector-valued cross products Let $\mathbb{F}$ be an field of characteristic $0$ . A $k$ -ary cross product in the vector space $V=\mathbb{F}^n$ equipped with an inner product $\langle \cdot, \cdot \rangle$ is a $k$ -linear map $\times: V^k \to V$ which outputs a vector with the following two properties: It is orthogonal to all inputs. Its norm is the volume of the $k$ -dimensional paralellotope formed by these inputs, that is, $$|\!\times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k)|^2 = \det \langle \mathbf{v}_i, \mathbf{v}_j \rangle,$$ where the right hand side is a Gram determinant . These two conditions turn out to imply that composition with the inner product $\langle \times, \cdot \rangle$ ("" lowering an index "") defines a totally antisymmetric $(k+1)$ -linear map $\times : V^{k+1} \to \mathbb{F}$ , i.e. an alternating $(k+1)$ -form. Equivalently, given a $(k+1)$ -form satisfying the appropriate volume condition one can define a cross product by ""raising an index"". In the following we restrict to $\mathbb{F}=\mathbb{R}$ with the standard inner product. The dimensions where a cross product exists have been completely classified (see for example here ): There is a trivial $k$ -ary cross product for any $k\ge n+1$ , which is identically zero. A nullary cross product (equivalently a unit vector) exists for any $n\ge 1$ . A unary cross product (corresponding to a symplectic form) exists in any even dimension $n=2m$ . A $(n-1)$ -ary cross product (corresponding to a volume form) exists in any dimension $n$ . For example, the ordinary cross product in 3D is of this kind. There are only two exceptional cross products not covered by the above cases, binary in dimension $7$ (corresponding to an associative $3$ -form ) and ternary in dimension $8$ (corresponding to a Cayley $4$ -form ). They can both be expressed in terms of the octonions, see for example this question for details. So we have the following table: $$\begin{array}
{|c|c|c|c|c|} 
\hline
k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\
\hline
0 & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
1 & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \ldots\\
\hline
2 & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & & &\\
\hline
3 & \checkmark & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & &\\
\hline
4 & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & & & & & & &\\
\hline
5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & & &\\
\hline
6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & &\\
\hline
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & \ddots & & & &\\
\hline\end{array}$$ Note that if there is a $k$ -ary product in dimension $n$ , there is automatically a $(k-1)$ -ary product in dimension $n-1$ obtained by contracting with an unit vector. This explains the diagonal patterns. Bivector-valued cross products Similarly, we can define a $k$ -ary bivector-valued cross product (BV cross product or BVCP for short), which is a $k$ -linear map $\times : V^k \to \Lambda^2 V$ outputting a bivector with the following two properties: It is orthogonal to all inputs (i.e. the left contraction $\mathbf{v}_j \: \lrcorner \times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k)$ vanishes for all $1\le j \le k$ ). Its norm is the volume of the paralellotope formed by these inputs. As before, given any BV cross product we may define a corresponding $(k+2)$ -form and viceversa, by lowering or raising two indices. Note that the wedge/outer product of two vectors is not a BV cross product, since it doesn't satisfy the first condition (in fact it does the complete opposite, since the plane spanned by $a \wedge b$ contains $a$ and $b$ ). So far I haven't been able to find a classification of BV cross products anywhere (this might be because I've been searching for the wrong terms). However, we can give a partial classification by trying to find bivector-valued analogues of each class of vector-valued cross product: There are trivial $k$ -ary BV cross products for any $k\ge n+1$ . A nullary BV cross product, or equivalently a bivector of norm $1$ , exists for any $n\ge 2$ . A unary BV cross product exists in any dimension $n=3a+7b$ , since then we can decompose $V$ into the direct sum of vector spaces of the form $\mathbb{R}^3$ and $\mathbb{R}^7$ , and take a corresponding $3$ -form which restricts to a suitable multiple of the volume form in each $\mathbb{R}^3$ and a multiple of the associative $3$ -form in each $\mathbb{R}^7$ . On the other hand, no unary product exists for $n = 1, 2$ for dimensional reasons. Since every number greater than $11$ can be expressed as $3a+7b$ for some $a, b$ , this leaves in doubt only the cases $n=4, 5, 8$ and $11$ . (The cases $n=4, 5$ are now ruled out and $n=8, 11$ are confirmed, see the updates). A $(n-2)$ -ary cross product (corresponding to a properly normalized volume form) exists in any dimension $n$ . The Hodge dual of the associative $3$ -form, properly normalized, induces a binary BV cross product in dimension $7$ , and similarly the Cayley form itself induces another in dimension $8$ . I'm sure there is a better way to prove it, but I just used the well-known correspondence between bivectors and skew-symmetric matrices and used a CAS to check symbolically that the relations $M_{\mathbf{x}\times\mathbf{y}}\mathbf{x} = M_{\mathbf{x}\times\mathbf{y}}\mathbf{y} = 0$ and $||M_{\mathbf{x}\times\mathbf{y}}|| = |\mathbf{x}\wedge\mathbf{y}|$ hold, where $||\cdot||$ is the Frobenius norm. Here is a tentative table summarizing the above: $$\begin{array}
{|c|c|c|c|c|} 
\hline
k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\
\hline
0 & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
1 & \checkmark & & & \checkmark & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
2 & \checkmark & \checkmark & & & \checkmark & & & \checkmark & \checkmark & & & &\\
\hline
3 & \checkmark & \checkmark & \checkmark & & & \checkmark & & & \color{darkred}\bullet & \color{darkred}\bullet & & &\\
\hline
4 & \checkmark & \checkmark & \checkmark & \checkmark & & & \checkmark & & & & & &\\
\hline
5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & & &\\
\hline
6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & &\\
\hline
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & & \ddots & & &\\
\hline\end{array}$$ Question My question is: Is the list above correct? Are there any other possible BV cross products, e.g. binary in dimension $10$ ? UPDATE: We can generalize the Hodge dual argument we used to get the $7$ -dimensional binary BVCP to rule out the existence of BVCPs for some $n$ and $k$ . First, note that if we know a cross product doesn't exist for some $n$ and $k$ , automatically we know that none exists for $n+a$ and $k+a$ with any $a>0$ , since otherwise we could contract the latter with $a$ unit vectors and obtain a contradiction. A $p$ -vector-valued cross product is the obvious generalization of vector-valued ( $p=1$ ) and bivector-valued ( $p=2$ ) cross products to multivectors of arbitrary degree. With some effort, I have proven that a $(n,k,p)$ -cross product induces a $(n,k,n-2k-p)$ -cross product by lowering $k$ indices, taking the Hodge dual, raising $k$ indices again and renormalizing. Conversely, if there are no $(n,k,p)$ -cross products, we can deduce the nonexistence of $(n,k,n-2k-p)$ -cross products. Note that for scalar-valued cross products ( $p=0$ ) the orthogonality condition is trivially satisfied. Their classification is easy to state: they exist only for $k\ge n$ or for $k=0$ , $n$ arbitrary. Using the classifications of scalar and vector-valued products and applying Hodge duality to $(n,k,p) = (4,1,0)$ , $(5,1,1)$ , $(8,3,0)$ and $(9,3,1)$ , we prove the nonexistence of BVCPs at $(4,1,2)$ , $(5,1,2)$ , $(8,3,2)$ and $(9,3,2)$ , and of any others as we move diagonally in the bottom right direction. I marked some cells in the table with dark-red dots where we know BVCPs don't exist. Note that the diagonal below any dot is also ruled out by the discussion above. In light of this new information, the question can now be divided in two main subquestions: Are there unary BVCPs in dimensions $8$ and $11$ ? UPDATE 2: I have found an $8$ -dimensional unary BVCP satisfying the conditions, whose corresponding $3$ -form is given by a rescaling of the structure constants of the Lie algebra $\mathfrak{su}(3)$ . It follows that an unary product exists in dimensions $n=3a+7b+8c$ , and in particular also for $n=11$ . I updated the table with these two new cases. Are there binary BVCPs in dimension greater than $8$ ?","['cross-product', 'octonions', 'linear-algebra', 'recreational-mathematics', 'exterior-algebra']"
3555537,"Transitive action of SL(2,R) on the circle","Is the circle a homogeneous space for G=SL(2,R)? I think G/B is real analytically isomorphic to the circle where B is Borel  subgroup of G. Is there a transitive algebraic action of G on the circle? Can someone give that action with formulas for every entry given as rational functions in the entries of g and the circle (take circle as unit circle in the plane). I tried projecting onto the K circle in the KAN=KB decomposition and then multiplying by that rotation but it didnât work. Maybe something with MÃ¶bius transformation acting on the circle bounding the upper half plane viewed as a subset of the Reimannian sphere? Note: the action cannot be an action by isometries the circle is not a symmetric space for SL(2,R) basically because SO(2,R) is not a group quotient of SL(2,R)","['differential-topology', 'topological-groups', 'lie-groups', 'differential-geometry']"
3555555,Minimum three variable expression $\frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|}$,"If $a,b,c \ge 0$ are distinct real numbers, find the minimum value of $$\frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|}$$ What I did: I used $a^2+b^2+c^2\geq \frac{1}{3}(a+b+c)^2$ , but no success. A friend of mine said the minimum happens when one of the variables is $0$ , but I don't really understand why or how to prove this.","['optimization', 'multivariable-calculus', 'maxima-minima', 'inequality']"
3555558,Find : $\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$,"Find : $$\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$$ My attempt : i don't know is correct or no!
I use this rule : $$\lim\limits_{n\to +\infty}(f(x))^{g(x)}=1^{\infty}$$ Then : $$\lim\limits_{n\to +\infty}(f(x))^{g(x)}=\lim\limits_{n\to +\infty}e^{g(x)(f(x)-1)}$$ So : $$\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$$ $$=\lim\limits_{n\to +\infty}\frac{e^{\frac{n^{4}}{n^{3}}}}{e^{\frac{(n+1)^{4}}{(n+1)^{3}}}}$$ $$=\lim\limits_{n\to +\infty}\frac{e^{n}}{e^{n+1}}$$ $$=\frac{1}{e}$$ Is my approach wrong ? is this called partial limit calculation ?","['limits', 'calculus', 'real-analysis']"
3555603,Testing significance of patterns of results,"I'm a high school English teacher conducting an independent study, and I'm a total novice to statistical analysis, so please forgive me if I mischaracterize anything. I have gathered pretest and posttest data about student motivation from three groups of students: one that received normal grades over a six-week period (Group 3), one that received delayed performance-contingent grades and immediate completion-contingent grades (Group 2), and one that received no grades (Group 1). The tool that I used measures seven types of motivation, which I've coded as IMK, IMA, IMS, EMID, EMIJ, EMX, and AM. My hypothesis is that feedback in the form of narrative evaluations without immediate or salient multi-interval grades (i.e., Group 1 and Group 2) will lead to better motivational outcomes for high-school students in autonomy-supportive classrooms than forms of feedback associated with immediate and salient multi-interval grades (Group 3). ""Better,"" in this case, means that IMK, IMA, IMS, and EMID will increase and EMIJ, EMX, and AM will decrease by the end of the six-week period for students in Group 1 and Group 2. My hypothesis for Group 3 is two-tailed; I think both grades and autonomy support will have an effect, but I don't want to make any predictions about what that effect will look like - my only prediction is that the motivational outcome will be worse. I ran a bunch of paired t -tests to check the significance of the difference between pretest and posttest scores for each measured variable. Here's what I came up with: https://i.sstatic.net/NPB82.png Now, at a glance, you can see that the results seem to support my hypothesis. In Group 1 (Withheld), for instance, there was an increase in each of the things that I thought would increase, and there was a decrease in each of the things that I thought would decrease. It doesn't seem intuitively likely that this specific pattern happened randomly, even if the changes for each individual variable didn't pass the significance threshold. What sort of test could I do (ideally in R) to prove this?",['statistics']
3555614,Using $y=uv$ substitution to solve $y'' + P(x)y' + Q(x)y = 0$,"Show that by applying the substitution $y=uv$ to the homogeneous
  equation $y'' + P(x)y' + Q(x)y = 0$ , it is possible to obtain a
  homogeneous second order linear equation for $v$ with no $vâ²$ term
  present. Find $u$ and the equation for $v$ in terms of the original
  coefficients $P(x)$ and $Q(x)$ This problem is given in GF Simmons book on differential equations Pg $119$ Problem $11$ , $3rd$ edition. I am confused. Here $u$ and $v$ are supposed to be dependent on $x$ so how am I supposed to differentiate these and replace derivatives of $y$ and get only a differential equation in $v$ ? Further I am also asked to solve $yâ³+2xyâ²+(1 + x^2)y=0$ using this
  method. I am hoping I would be able to solve this if someone could
  explain me what we're trying to do in this method",['ordinary-differential-equations']
3555641,Are exotic $\mathbb{R}^4$ flat?,$\mathbb{R}^4$ can be given two different differential structures which are not diffeomorphic. The standard $\mathbb{R}^4$ with its common differential structure can be given a metric in which its Riemann tensor is zero everywhere. Do exotic differential structures also admit a metric with Riemann tensor vanishing everywhere?,"['differential-topology', 'differential-geometry']"
3555670,What is the mathematical logic behind basic algebra?,"I am a freshman college student taking a required algebra course. This content should be very easy, but I have struggled with the more complicated applications of algebra since I was young. So I have decided to familiarize myself with the foundations of algebra to give myself a solid foundation before reattempting learning the algebra, more out of interest than necessity - I know I could learn the steps by rote. (I'm more of a big-picture thinker. The issue comes down to getting myself together to solve the actual practical questions during the exams). The content includes factoring, rational equations, and solving polynomials, the only subjects I'm really concerned with. What types of logic do these algebraic operations and problems stem from and use? How are they/were they proven? I have experience with basic first order, Boolean, predicate, propositional, and modal logic. Where should I begin? General mathematical logic? Boolean algebra? I'm overwhelmed. Any book recommendations, links, even just a general overview of what subjects I should go for would help. Thank you in advance!","['algebra-precalculus', 'foundations', 'logic']"
3555679,Structure theorem of symplectic modules,"My question comes from the content presented on slide 31 of the following presentation given by Jean-Pierre Tignol (unfortunately, I do not have access to the main reference on the subject, that is Tignol and Amitsur's paper on symplectic modules). A symplectic module is a finite abelian group $M$ with a nondegenerate alternating bilinear pairing $b:M\times M \rightarrow \mathbb Q/\mathbb Z$ . On the slide mentioned above, the following structure theorem is refered to as ""De Rham theorem, 1931"". Every symplectic module $M$ has a symplectic basis $$M\cong (\mathbb Z/n_1\mathbb Z)^2\times\ldots\times (\mathbb Z/n_s\mathbb Z)^2$$ with generators $e_1,f_1,\ldots ,e_s,f_s$ such that $b(e_i,f_i)=\frac{1}{n_i}+\mathbb Z$ and $b(e_i,f_j)=0$ for $i\not = j$ and $b(e_i,e_j)=b(f_i,f_j)=0$ for any $i,j$ . I have two questions in regards to this. First, can we assume furthermore that $n_1|n_2|\ldots |n_s$ ? Second, what does this statement have to do with the renowned De Rham theorem of 1931, expressing that for a smooth manifold $M$ , we have an isomorphism between De Rham cohomology groups $H_{dR}^p(M)$ and singular cohomology groups $H^p(M;\mathbb R)$ ?","['symplectic-linear-algebra', 'group-theory', 'symplectic-geometry', 'abelian-groups']"
3555681,Principal divisor $P+Q+R-3\infty$ on elliptic curve comes from a straight line,"Let $E=V(y^2-(x-a)(x-b)(x-c))$ be an elliptic curve. Let $D=P+Q+R-3\infty$ be a divisor. Then $D$ is principal if and only if $P$ , $Q$ and $R$ lie on a straight line. One direction is straightforward - if the three points $P$ , $Q$ , $R$ are collinear, the line on which they lie gives the divisor. But what about the other direction? The field of definition is assumed to be algebraically closed. Edit: None of the suggestions so far have been satisfying. The hint my professor gave us was to use Riemann-Roch. In particular: What happens if $\infty\in\left\{P,Q,R\right\}$ ?","['divisors-algebraic-geometry', 'algebraic-geometry', 'elliptic-curves']"
3555686,Probability of random sphere lying inside the unit ball,"Let $n\geq2$ . Let $B\subseteq\mathbb{R}^n$ be the unit ball. Randomly choose $n+1$ points of $B$ (uniformly and independently). Then (almost surely) there will be a unique hypersphere $S$ passing through all $n+1$ points. What is the probability that $S\subseteq B$ ? When $n=2$ , the probability seems to be exactly $40\%$ .","['geometric-probability', 'spheres', 'geometry', 'probability']"
3555696,A question on nowhere dense sets.,"Consider the $2$ definitions: A set $A$ in a topological space $(X,\tau)$ is said to be a nowhere dense set if it is not dense in any nonempty open set. A Set $A$ in a topological space $(X,\tau)$ is said to be a nowhere dense set if $(\bar A)^0=\phi$ . I understand the two definitions as statements and know they are equivalent.
Suppose that $(\bar A)^0=\phi$ ,now take any open ball $V$ in $X$ .Let if possible,for each open ball $U$ in $V$ , $U\cap A\neq \phi$ .Now for each of the open balls in $V$ ,choose an element of $A$ and consider the collection as $A_0$ .Now for any point $v$ in $V$ ,every neighborhood of $v$ contains a point of $A_0$ by its construction.So, $V\subset \bar A_0 \subset \bar A$ ,i.e. $\bar A$ contains an open set which contradicts $(\bar A)^0=\phi$ . Conversely,suppose for each open ball $V$ in $X$ ,there exists $U$ open such that $U\subset V$ and $U\cap A=\phi$ .To show, $\bar A$ can contain no open set.If possible $\bar A$ contains an open ball $V$ ,then for any open ball $U$ in $V$ ,if we take a point $x\in U$ then $x\in \bar A$ ,so $x$ is an adherent point of $A$ and since $U\in \eta_x$ ,so, $U\cap A\neq \phi$ . But I have not yet found the reason behind its name 'nowhere dense' i.e. I cannot feel it properly.I am looking for some diagram that would build my intuition on nowhere dense set.I have studied examples like Cantor set,but yet I am feeling uncomfortable with the notion of nowhere dense sets and why it means that points are not clustered very tightly in topological sense?I also want to know what is the motivation behind defining nowhere dense sets. The above picture shows that for every open set $V$ (yellow) in $(X,d)$ ,there is an open set $U\subset V$ (shown in white) which does not intersect $A$ . Some other constructions of nowhere dense sets are Cantor like construction,it forces the resulting set to be nowhere dense by deleting some open ball from each of the open balls of the whole set.For example Cantor set,Smith-Volterra Cantor set,Cantor dust in $\mathbb R^n$ and Cantor Cirlcle,Fractals-of-Cirlces that ca be found here .","['visualization', 'motivation', 'definition', 'intuition', 'general-topology']"
3555733,Help understanding why finite Boolean rngs must be rings,"I'm working through the exercises in Introduction to Boolean Algebra by Halmos and Givant. Looking to show the following, an exercise from the first chapter: every finite Boolean rng must have a unit. Halmos and Givant attribute this observation to Stone, in his original paper on the representation theorem. Stone shows the result by explicitly constructing the unit as a sum of elementary symmetric polynomials. For a finite Boolean ring $B$ with $|B| = n$ , we consider the elementary symmetric polynomials in $n$ variables, which are: $e_1(x_1, \dots, x_n) = x_1 + \dots + x_n$ , $e_2(x_1, \dots, x_n) = x_1 x_2 + \dots + x_{n-1}x_n$ , ... $e_n(x_1, \dots, x_n) = x_1\cdots x_n$ Stone shows that the unit in a finite Boolean rng (with $n$ elements) is given as the sum of $e_1(a_1, \dots, a_n) + \dots + e_n(a_1, \dots, a_n)$ where $\{a_1, \dots, a_n\} = B$ . What I'm wondering, is how one could figure this out? What observations about Boolean rings do I need to make in order to know that the above construction is the right one to try? As a side note, I see that in this post, Martin Brandenburg gives another argument for the conclusion using the Nakayama lemma (which seems a little high-powered for this exercise) and mentions that a direct argument is possible.","['boolean-algebra', 'logic', 'abstract-algebra', 'symmetric-polynomials', 'boolean-ring']"
3555736,Formula for Feigenbaumâs constant?,"I have conjectured a formula to calculate the Feigenbaum constant $\delta \approx 4.66920$ . $\delta\stackrel{?}{=}$ $$4+\cfrac{1\times 2 -1}{1+\cfrac{2\times 3 -1}{2^2+\cfrac{3\times 4 -1}{1+\cfrac{4\times 5 -1}{3^2+\cfrac{5\times 6 - 1}{1+\ddots}}}}}$$ which after several iterations is $\approx 4.66919$ (I believe). This discovery, whether exact or approximate, was completely accidental. Can this be numerically verified? Thanks.","['conjectures', 'approximation', 'number-theory', 'numerical-methods', 'continued-fractions']"
3555745,"Elementary proof that the limit of $\sum_{i=1}^{\infty} \frac{1}{\operatorname{lcm}(1,2,...,i)}$ is irrational","Show that the infinite sum $S$ defined by -$$S=\sum_{i=1}^\infty \frac{1}{\operatorname{lcm}(1,2,...,i)}$$ is an irrational number. I found this question while reading 'Mathematical Gems' by Ross Honsberger. After pondering over it for nearly an hour, I was able to prove it by using Bertrand's postulate which states that there is a prime between n and 2n for every natural number n>1. This question was solved by Lajos PÃ³sa when he was just 12 years old. Is there any elementary proof that does not use Bertrand's postulate or any complicated theorem?","['elementary-number-theory', 'irrational-numbers', 'real-analysis', 'analytic-number-theory', 'sequences-and-series']"
3555855,Prove that rank(A) = rank(A|C) [duplicate],"This question already has answers here : Existence of solution for a linear system mod 2 (3 answers) Closed 3 years ago . I have a problem in which I am trying to prove over GF(2) that a binary symmetric matrix (A) with a diagonal of ones has a rank always equal to the rank of its augmented matrix with a ones vector (C) $$
  C=\left[\begin{array} \\
    1 \\
    \vdots \\
    1
  \end{array}\right]
$$ To clarify, such matrix is constructed like so: $$
  A=\left[\begin{array}{rrrr}
1 & a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
a_{1,1} & 1 & a_{2,1} & \ddots & \vdots \\
a_{1,2} & a_{2,1} & \ddots & a_{n-1,n-1} & a_{n-1,n} \\
\vdots & \ddots & a_{n-1,n-1} & 1 & a_{n,n} \\
a_{1,n} & \dots & a_{n-1,n} & a_{n,n} & 1
  \end{array}\right]
$$ For example, a 3 by 3 matrix like this has a rank of 2: $$
  A=\left[\begin{array}{rrr}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    0 & 0 & 1
  \end{array}\right]
$$ When we augment it with a ones vector, we get this matrix which also has a rank of 2: $$
  A|C=\left[\begin{array}{rrr|r}
    1 & 1 & 0 & 1 \\
    1 & 1 & 0 & 1 \\
    0 & 0 & 1 & 1
  \end{array}\right]
$$ Cleary rank(A) = rank(A|C) over GF(2). Why is this always true for such type of matrices? If you have a proof, an idea, or a suggestion on how to proceed, please let me know. Any help is appreciated.","['matrix-rank', 'systems-of-equations', 'linear-algebra']"
3555877,Help understanding the concept of full rank exponential families,"I am studying Exponential Families and there are some concept I do not quite understand completely. Here are a two definitions for rank of an exponential family and a full rank exponential family: Definition 1: Let $\mathscr{P}=\{P_\eta:\eta\in H\}$ is an $s-$ dimensional minimal exponential family. If $H$ contains an open $s-$ dimensional rectangle, then $\mathscr{P}$ is called full-rank. Otherwise, $\mathscr{P}$ is called curved. Definition 2: An exponential family is of rank $k$ if and only if the generating statistic $T$ is $k-$ dimensional and $\{1, T_1(X), \ldots,T_k(X)\}$ are linearly independent with positive probability. Formally, $P_\eta\left(\sum_{j=1}^{k}a_jT_j(X)=a_{k+1}\right)<1$ unless all $a_j$ are zeros. Here, definition 1 I took from this note and definition 2 is from Doksum & Bickel's. It is definition 2 that makes me confused. When I read the first sentence of definition 2, I translate it as follows: There exists a set $A$ such that $P_\eta(A)>0$ , and if $\sum_{j=1}^{k}a_jT_j(X)=a_{k+1}$ for all $x\in A$ , then $a_1=a_2=\cdots=a_{k+1}=0.$ But then how is it equivalent to the second sentence of definition 2? In other words, how should I understand the sentence "" Formally, $P_\eta(\sum_{j=1}^{k}a_jT_j(X)=a_{k+1})<1$ unless all $a_j$ are zeros "" correctly?","['statistical-inference', 'statistics', 'probability']"
3555933,How to solve the equation $\sqrt{\frac{\sqrt{x!}\times \sqrt{x!!}\times \sqrt{x!!!}}{\sqrt{\left( x-2 \right)!+x}}}=12$,"Consider the following equation, $$\sqrt{\frac{\sqrt{x!}\times \sqrt{x!!}\times \sqrt{x!!!}}{\sqrt{\left( x-2 \right)!+x}}}=12$$ I tried first to eliminates all radicals using squring both sides to get the following , $$\frac{\sqrt{x!}\times \sqrt{x!!}\times \sqrt{x!!!}}{\sqrt{\left( x-2 \right)!+x}}={{12}^{2}}\Leftrightarrow \frac{\sqrt{x!\times x!!\times x!!!}}{\sqrt{\left( x-2 \right)!+x}}=\frac{x!\times x!!\times x!!!}{\left( x-2 \right)!+x}={{12}^{4}}$$ Now how will continue attacking this problem?","['recreational-mathematics', 'computational-science', 'discrete-mathematics', 'sequences-and-series']"
3555985,Establishing that a function is a well-defined homomorphism,"I am working with the group homomorphism $f: G \to \text{Aut (G)}$ , $g \mapsto f_g = ghg^{-1}$ , and am trying to prove that $f$ is a well-defined homomorphism. To show that $f$ is well-defined, we must establish three facts: (1) that $f_g$ is a well-defined function; (2) that $f_g$ is in fact an element of $\text{Aut ($G$)}$ , so $f$ actually sends an element $g \in G$ to an element of the codomain of $f$ and (3) if $g = h$ , then then $f(g) = f(h)$ . Proof of (1): 
Suppose $x = y$ . It suffices to show that for a fixed $g \in G$ , we have $gxg^{-1} = gyg^{-1}$ . By cancellation, we have \begin{align*}
gxg^{-1} = gyg^{-1} \iff gx = gy \iff x = y.
\end{align*} This verifies (1). Proof of (2): We need to show first that $f_g$ is a bijection, for which it suffices to exhibit an inverse, $f_{g^{-1}}$ , such that \begin{align*}
f_g = f_{g^{-1}} = f_{g^{-1}} \circ f_g = \text{id}_G.
\end{align*} Define $f_{g^{-1}} = g^{-1} x g$ . Then, for an arbitrary $h \in H$ , we have: \begin{align*}
(f_g \circ f_{g^{-1}}) (h) & = f_g (f_{g^{-1}} (h)) \\
& = f_g (g^{-1} hg) \\
& = g(g^{-1} hg)g^{-1} \\
& = (gg^{-1})h(gg^{-1}) \\
& = e_G he_G \\
& = h
\end{align*} Since $h$ was arbitrary, we have $f_g \circ f_{g^{-1}}$ . For the reverse direction, for arbitrary $x \in G$ , we have: \begin{align*}
(f_{g^{-1}} \circ f_g)(h) & = f_{g^{-1}} (f_g (h)) \\
& = f_{g^{-1}} (ghg^{-1}) \\
& = g^{-1} (ghg^{-1})g \\
& = (g^{-1} g) h (g^{-1} g) \\
& = e_G h e_G h \\
& = h
\end{align*} Since $h$ was arbitrary, $f_{g^{-1}} \circ f_G = \text{id}_G$ , so we have exhibited a proper inverse for $f_g$ . Hence, $f_g$ is a bijection. It suffices to establish that $f_g$ is an isomorphism from $G$ to $G$ . It suffices to demonstrate that $f_g$ satisfies the homomorphism property. For any $x, y \in G$ , we have: \begin{align*}
f_g (xy) & = g(xy)g^{-1} \\
& = (gx)(yg^{-1}) \\
& = (gx)e_G (yg^{-1}) \\
& = (gx)(g^{-1} g)(yg^{-1}) \\
& = (gxg^{-1}) (gyg^{-1}) \\
& = f_g (x) f_G (y).
\end{align*} Hence, $f_g \in \text{Aut ($G$)}$ . This verifies (2). Proof of (3). Let $x, y \in G$ , and suppose $x = y$ . It suffices to show that the resulting automorphism, $f_x (h) = xhx^{-1}$ and $f_y (h) = ygy^{-1}$ , represent the same mapping. Since $f_x (h)$ and $f_y (y)$ have the same domain, $G$ , it suffices to show that $f_x (h) = f_y (h)$ for all $h \in H$ . That is, $xhx^{-1} = yhy^{-1}$ . We have: \begin{align*}
x = y \iff xh = yh \iff xhx^{-1} = yhy^{-1}.
\end{align*} This verifies (3). These prove that $f$ is well-defined. It suffices to establish that it obeys the homomorphism property. Let $x, y \in G$ . We must show that $f(xy) = f(x) f(y)$ . For arbitrary $h \in G$ , we have: \begin{align*}
f(xy) (h) & = f_{xy} (h) \\
& = (xy)h(xy)^{-1} \\
& = (xy)h(y^{-1} x^{-1}) \\
& = x(yhy^{-1})x^{-1} \\
& = (f_x \circ f_y)(h) \\
& = f(x)f(y)(h)
\end{align*} This establishes that $f$ is a homomorphism.","['group-homomorphism', 'proof-explanation', 'group-theory']"
3556048,Proving that $\prod_{k=1}^{n}\tan\left(\frac{\pi k}{2n+1}\right)=\sqrt{2n+1}$ using geometry,"I have to prove that the following holds. A hint to use complex numbers has been given. I have tried to make a start but not to any result. $$\prod_{k=1}^{n}\tan\left(\frac{\pi k}{2n+1}\right)=\sqrt{2n+1}$$ My Attempt : Let us consider $(2n+1)^{\text{th}}$ roots of unity, $z_k=\exp\left(\frac{2k\pi i}{2n+1}\right)$ . We can rewrite the product in terms of $\arg(z_k)$ as $\prod_{k=1}^{n}\tan\left(\frac{1}{2}\arg(z_k)\right)$ . Or equivalently so, if we consider $(4n+2)^{\text{th}}$ roots of unity, we get this product as $\prod_{k=1}^{n}\tan(\arg(\zeta _k))$ , where $\zeta_k= \exp\left(\frac{2k\pi i}{4n+2}\right)$ . I know it can be proved by proving the expression for $\prod\sin(\frac{1}{2}\arg(z_k))$ . But I was wondering, is there a way to use telescopic products or purely the geometry of the complex roots of unity to arrive at this","['complex-analysis', 'trigonometric-series', 'geometry', 'complex-numbers']"
3556059,Value of $ \lim_{n \to \infty} \int \limits_{0}^{1}nx^n e^{ x^2} ?$,How to find the value of $$ \lim_{n \to \infty}\int \limits_{0}^{1} nx^n e^{ x^2} ?$$ From wolfram the limit approaches to $e$ for larger values of $n$ . I substituted $x^2 $ with $u$ and obtained $$ \frac{ n} {2} \int \limits_{0}^{1} u^{\frac{n-1}{2}} e^{u} du $$ The value of this integral can be obtained from here . But still I'm unable to get it. Is there any better approach for this question?,"['integration', 'limits', 'calculus']"
3556072,Laws of Algebra of Sets,"Can someone help me to prove that: $$\left ( A-B \right )-\left ( C-D \right )=\left [ A-\left ( B\cup C \right ) \right ]\cup \left [ \left ( A\cap D \right )-B \right ]$$ What I did was: $\left ( A-B \right )-\left ( C-D \right )=\left [ A\cap \left ( B\cup C \right ) ^{c}\right ]\cup \left [ \left ( A\cap D \right )\cap B^{c} \right ]$ But I do not know, how to continue. Can anyone help me?",['elementary-set-theory']
3556076,Calculating second partial derivate of a multivariable function.,"I have a doubt resolving Apostol Vol.2, sec 22, prob 4: The equations $u =f(x, y), x = X(s, t), y = Y(s, t)$ define $u$ as a function of $s$ and $t$ , say $u = F(s, t).$ Find formulas for the partial derivatives $\frac{\partial^2F}{\partial s\partial t}$ and $\frac{\partial^2F}{\partial t^2}$ I was able to get the formula for $\frac{\partial^2F}{\partial t^2}$ , but for $\frac{\partial^2F}{\partial s\partial t}$ I seem to have troubles, here's my solution: $\frac{\partial^2F}{\partial s\partial t}=\frac{\partial f}{\partial x}\frac{\partial^2X}{\partial s\partial t}+\frac{\partial^2f}{\partial x^2}\frac{\partial X}{\partial t}\frac{\partial X}{\partial s}+\frac{\partial^2f}{\partial y\partial x}\frac{\partial Y}{\partial s}\frac{\partial X}{\partial t}+\frac{\partial f}{\partial y}\frac{\partial^2Y}{\partial s\partial t}+\frac{\partial^2f}{\partial x\partial y}\frac{\partial X}{\partial s}\frac{\partial Y}{\partial t}+\frac{\partial^2f}{\partial y^2}\frac{\partial Y}{\partial s}\frac{\partial Y}{\partial t}$ But I think this is wrong given a solution I found on the internet, is my solution right or where could I have gone wrong? Thanks.","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
3556078,Why do we use determinant for multivariate normal distribution?,"While learning statistics, I have a question why is the determinant used in the multivariate normal distribution. When I look for the answer on the internet, so far every answer I looked at was basically saying that it works, so we use that. But what I want is if there is a mathematical relation between multivariate normal distribution and determinant (volume factor of linear transformation or some other definition). There was one answer that by using determinant we can make the integral of the density over $R^{n}$ equal to $1$ . This sounds nice, but if there is another intuition, please share it.","['statistics', 'determinant', 'linear-algebra', 'normal-distribution']"
3556105,How is HÃ¶lder's inequality with $p=q=2$ equivalent to Cauchy-Schwarz inequality?,"The Cauchy-Schwarz inequality says that $$\left|\sum_i x_i y_i \right| \leq \sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}$$ HÃ¶lder's inequality for the special case $p=q=2$ says that $$\sum_i |x_i y_i| \leq \sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}$$ Clearly the former implies the latter via the triangle inequality. However, there appears to be general consensus online that the latter also implies the former. For example, mathworld [HÃ¶lder's inequality with] $p=q=2$ becomes Cauchy's inequality or Wikipedia The special case [of HÃ¶lder's inequality with] p = q = 2 gives a form of the CauchyâSchwarz inequality. Why is this true? It seems to me that Cauchy's inequality is a stronger statement than this special case of HÃ¶lder's inequality, and so HÃ¶lder's inequality does not become Cauchy's inequality at all. This has been asked before here Why is the Cauchy Schwarz inequality a special case of Holder's inequality? but not answered. In particular, Hrit Roy's final comment on the answer is not dealt with.","['inequality', 'functional-analysis', 'real-analysis']"
3556137,Another equivalent notion of ergodicity,"Let $T\colon X\to X$ be a measure preserving transformation on a probability space $(X,\mu)$ . We say that $T$ is ergodic if $A$ is measurable and $T^{-1}(A)=A$ implies that $\mu(A)\in\{0,1\}$ . I have seen that the following are equivalent: $T$ is ergodic, If $B$ is measurable and $\mu(T^{-1}(B)\Delta B)=0$ , then $\mu(B)\in\{0,1\}$ . (Here $\Delta$ stands for the symmetric difference.) However, I was reasoning as follows. If $C$ is measurable and $C\subset T^{-1}(C)$ , then $$\mu(T^{-1}(C)\Delta C)=\mu(T^{-1}(C)\setminus C)+\mu(C\setminus T^{-1}(C))=\mu(T^{-1}(C)\setminus C)\\ =\mu(T^{-1}(C))-\mu(C)=\mu(C)-\mu(C)=0.$$ So I concluded that there was another characterization of ergodicity, namely: If $C$ is measurable and $C\subset T^{-1}(C)$ , then $\mu(C)\in\{0,1\}$ . Note that it is clearly indeed also true that 3 implies 1. But I find this characterization really counterintuitive, especially since I do not encounter this definition in the literature. I think it is a nice characterization for the following reason: Suppose $T$ is ergodic and we want to show that a measurable set $C$ has either measure $0$ or $1$ . Then, by 3, it suffices to prove that $C\subset T^{-1}(C)$ . The reverse inclusion does not even have to hold! So my questions are: Is my reasoning correct, that is, is $3$ indeed equivalent to ergodicity? Why does most literature ( atleast the literature I found ) choose not to mention anything about 3?","['measure-theory', 'ergodic-theory', 'reference-request']"
3556141,Dominated Convergence and Monotone Convergence,"Assuming that the measure space is $\sigma$ -finite, how can I show that the Dominated convergence theorem implies the monotone convergence theorem?","['integration', 'measure-theory', 'analysis']"
3556188,"Show that if $\gcd(a,3)=1$ then $a^7 \equiv a\pmod{63}$. Why is this assumption necessary?","Question: Show that if $\gcd(a,3)=1$ then $a^7 \equiv a\pmod{63} $ . Why is this assumption necessary? Proof: Since $\gcd(a,3)=1$ $\Leftrightarrow a\equiv 1\pmod 3$ $\Leftrightarrow a^7\equiv 1\pmod3\equiv a\pmod3$ Then using Fermat's Little Theorem: If $a,p\in\mathbb N$ and $p$ is prime then $a^7\equiv a\pmod7$ $\Rightarrow 3 |a^7-a$ and $7 |a^7-a$ $\Leftrightarrow a^7-a=3k_1$ and $a^7-a=7k_2$ $\Rightarrow (a^7-a)^3=63(k_1)^2k_2$ $\Rightarrow (a^7-a)^3\pmod{63}\equiv 0$ Since $x^m\pmod n\equiv x\pmod n$ $\Rightarrow (a^7-a)^3\pmod{63}\equiv a^7-a\pmod{63}\equiv 0$ $\Leftrightarrow a^7\equiv a\pmod{63}$ The thing I am struggling with is that the question says that the only assumption necessary is that $\gcd(a,3)=1$ . Surely there are two assumptions neccessary, since to use Fermat's Little Theorem (in this situation) we need $a\neq 0 \pmod7 \space\space\space(\gcd(a,7)=1)$ . I am sure there is something obvious I'm missing -would be great if someone could check over what I have done and point out any mistakes :)","['gcd-and-lcm', 'modular-arithmetic', 'discrete-mathematics']"
3556194,Orthogonal derivative implies second derivative is null,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be twice differentiable, such that $f'(x)$ is an orthogonal linear transformation for every $x\in\mathbb{R}^n$ . Prove that $f''(x) = 0$ , for every $x\in\mathbb{R}^n$ .
I've been struggling with this one, apparently it doesn't even use the inverse function theorem. Mi idea was to try and use the fact that $f'(x)$ preserves the norm, but that didn't work.","['real-analysis', 'frechet-derivative', 'multivariable-calculus', 'linear-transformations', 'derivatives']"
3556199,"how to get a nice ""cosine looking"" curve following the y=x direction?","My aim is to get a ""cosine looking"" curve rotated 45Â° counterclockwise. When I graph : g(x)= x + cos(x) , I get a curve that has lost the nice and regular wavering of the ordinary  f(x)= cos(x) curve. Adding a coefficient does not work, but rather aggravates the change of form. Is there a possible equation that would produce the curve I am aiming at? Thanks for your help.","['analytic-geometry', 'curves', 'graphing-functions', 'trigonometry', 'algebra-precalculus']"
3556319,Are two analytic functions equal if they are equal on the boundary of an open disk?,"Let $D$ be the open disk in $\mathbb{C}$ with origin $0$ and radius $1$ . Let $f,g: \overline{D} \to \mathbb{C}$ be continuous functions such that $f$ and $g$ are analytic on $D$ and such that $f=g$ on $S^1= \{z \in \mathbb{C}: |z| = 1\}$ . Can I conclude that $f=g$ on $D$ as well? It is enough to show that $\{z\in D: f(z) = g(z)\}$ has a limit point in $D$ but I can't see why this should hold.","['complex-analysis', 'analyticity']"
3556356,$z^\alpha$ is defined and holomorphic on $\mathbb{C}^{\times}$ only for $\alpha$ an integer.,"I think I found a solution for this for all $\alpha$ : for $\alpha$ rational and not an integer one gets an impossible endomorphism of $\pi_1(\mathbb{C}^{\times})=\mathbb{Z}$ , and for $\alpha$ irrational one can show that $z^\alpha$ basically induces the same impossible map as a $z^\frac{p}{q}$ when $\frac{p}{q}$ is a sufficiently good rational approximation of $\alpha$ . However I'm afraid I'm missing something because I find the the irrational case to be a bit too complicated for the exercise sheet I was given. Is there a better, simpler solution ? Thanks","['complex-analysis', 'winding-number']"
3556391,Problem separating homogeneous differential equation,"I need to solve the following homogeneous differential equation: $$
\frac{dy}{dx}=\frac{x+3y}{3x+y}
$$ As I understand it, I have to use a substitution, either $y = ux, dy=u\,dx+x\,du$ or $x=uy, dx=u\,dy+y\,du$ . So I try to substitute either, and I get stuck because I'm not having a separated equation I can integrate. Here's what I have done so far: Using the first substitution: $$
\frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\
(3x+ux)(u\,dx+x\,du)=(x+3ux)dx \\
3x^2\,du+ux^2\,du=x\,dx-u^2x\,dx\\
3\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x}
$$ But at this point, dividing by $u^2$ will make me have the following term: $\frac{dx}{u^2x}$ , and this is where I'm stuck, as I cannot separate. I checked my algebra to make sure I didn't blunder, and I think I'm clear (I hope). Using the second substitution yields a similar result: $$
\frac{dy}{u\,dy+uy\,du}=\frac{uy+3y}{3uy+y} \\
y\,dy-u^2y\,dy=3y^2\,du+uy^2\,du \\
\frac{dy}{y}-\frac{u^2\,dy}{y}=3\,du+u\,du
$$ Any pointer in the right direction will be appreciated. Thanks! Edit: so for completeness sake, thanks to the answer and comment provided: $$
\frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\
\frac{u\,dx+x\,du}{dx}=\frac{x(1+3u)}{x(3+u)} \\
\frac{u\,dx+x\,du}{dx}=\frac{1+3u}{3+u} \\
(u\,dx+x\,du)(3+u)=(1+3u)dx \\
3u\,dx+u^2\,dx+3x\,du+ux\,du=dx+3u\,dx \\
u^2\,dx+2x\,du+ux\,du=dx \\
3x\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x} \\
(3+u)\,du=(1-u^2)\frac{dx}{x} \\
\frac{3+u}{1-u^2}\,du=\frac{dx}{x} \\
\frac{3\,du}{1-u^2}+\frac{u\,du}{1-u^2}=\frac{dx}{x} \\
\int{\frac{3\,du}{1-u^2}}+\int{\frac{u\,du}{1-u^2}}=\int{\frac{dx}{x}}
$$ Which is now solvable. Thanks for the help!",['ordinary-differential-equations']
3556482,Why monotonic function can have at most a countable number of Discontinuities?,"Because the function is monotonic this locates distinct rational number in each discontinuity. The open intervals (supL,infU), at the points of discontinuity, are disjoint because the function is monotonic. A set of rationals is countable,so the set of discontinuities of a monotonic function is countable. Alright so ""this locates distinct rational number in each discontinuity"" is kinda wierd so why not irrational and irrational are uncountable so this tells discontinuities of the function is uncountable.","['proof-explanation', 'proof-writing', 'real-analysis', 'functions', 'limits']"
3556537,Bijective function between natural numbers and arrangements with repetition,"Assume I have a set S = {a, b, c} I can then list all possible arrangements with repetition from set S as: ()
(a)
(b)
(c)
(a,a)
(a,b)
(a,c)
(b,a)
(b,b)
(b,c)
(c,a)
(c,b)
(c,c)
(a,a,a)
(a,a,b)
and so on... Now, I am looking for a bijection from these arrangements to the natural numbers. For example: ()      <->    0        (0-tuple)
(a)     <->    1        (1-tuples)
(b)     <->    2
(c)     <->    3
(a,a)   <->    4        (2-tuples)
(a,b)   <->    5
(a,c)   <->    6
(b,a)   <->    7
(b,b)   <->    8
(b,c)   <->    9
(c,a)   <->   10
(c,b)   <->   11
(c,c)   <->   12
(a,a,a) <->   13        (3-tuples)
(a,a,b) <->   14
and so on ... Is it possible to calculate this bijection in an efficient way? For example, if given (a,b,b,b,c,c,a) how could you calculate what natural number it maps to and vice versa? Edit This edit is in response to N. F. Taussig's comment. Continuing with the set S = {a, b, c} as an example, the number of n-tuples for each n is 3^n. This means that: number of n-tuples    natural number mapping
0-tuple                0                  0
1-tuples               3           1  -   3
2-tuples               9           4  -  12
3-tuples              27          13  -  39
4-tuples              81          40  - 120
5-tuples             243         121  - 363 So that gets me part way there to my bijection. Just by looking at the chart I know that the tuple (a,a,a,a) maps to 40, and the tuple (c,c,c,c,c) maps to 363. Now my problem is a bit simpler. I just need to know an easy way to number each n-tuple group.","['permutations', 'combinatorics']"
3556585,When is it acceptable to stop adding terms to a Taylor approximation if my goal it to find a limit?,"I'm new to the concept of finding a limit using a Taylor polynomial. I am curious when is it ok to stop adding terms to the polynomial if I am trying to find a limit? I've seen examples where people stopped at the $x^2$ term and then added $O(x^3)$ to the expression, examples where people went as far as $x^7$ and then added $O(x^8)$ to the expression and everything in between. If what I want is to find a limit, how should I know when to stop? What would be acceptable in say, an exam situation? EDIT For example, say I am trying to find the limit $$\lim_{x \to 0} \dfrac{\sin x}{x}$$ without using L'Hospital. So I am trying to use the Taylor Expansion. By ""When is it acceptable to stop adding terms"" I'm really asking about what is the difference between doing something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + O(x^5)}{x}$$ or doing something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!}+ O(x^7)}{x}$$ or something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} 
 + O(x^9)}{x}$$ and so on. Which one should I use? That's what I mean when I say ""When is it acceptable to add terms"".","['limits', 'calculus', 'taylor-expansion']"
3556612,Unique increasing solution to a separable differential equation (piecewise $C_1$),"I want to find the increasing function $y(x): [0,1] \rightarrow [0,1]$ which is defined implicitely as the solution to the following equation: $f_1(x) = f_2(y(x)) \quad \forall x \in [0,1]$ where $f_1(), f_2()$ are known and $C_1$ on $[0,1]$ $f_1$ and $f_2$ are such that an increasing solution exists (I have existence already proved elsewhere). I want to show that there exists a unique increasing solution $y(x)$ starting from $y(0) = 0$ . This solution should be unique as long as the set such that $f_1â(x) = 0$ is of null measure .  The intuition is quite simple: both function goes through exactly the same points, in the same order (just not at the same speed) (you can see this graphically). So as long as there is no flat part, there should be a unique mapping from $x$ to $y$ . Remark: on the optimal path, given the equation, this equation will also imply that the set such that $f_2â(y(x)) = 0$ is also of null measure, and the null derivative coincides. Remark 2: Obviously, if $f_2()$ is strictly monotone, we can just invert it to find the solution: $y(x) = f_2^{-1}(f_1(x)) \quad \forall x \in [0,1]$ To solve my problem, since Iâm solving for a function, I thought I could resort to properties of differential equations (Picard-Lindelof for e.g.); Differentiating my equation yields: $f_2â(y(x)) yâ(x) = f_1â(x)$ . $\iff yâ(x) = f_1â(x) / f_2â(y(x)) := g(x, y(x))$ when $f_2â(y) \neq 0$ By Picard-Lindelof, if $g()$ is $C_1$ on $[0,1]$ , then I have a unique solution. But this would correspond to the monotone case in my remark 2 above. The difficulty I have is with the points with null derivatives. These null derivatives yields problem because there is a indeterminacy in $g()$ on the optimal path ( $0$ over $0$ , since $yâ > 0$ ), and at other points the function is not even defined. So I cannot resort to Picard Lindelof.
However, since Iâm looking for an increasing solution , the solution should still be unique . But I cannot find any proof of that, or any variation of Picard-Lindelof doing this kind of stuff (i.e adding increasing constraint into the system). Hence my first question: Question: does a variation of Picard-Lindelof uniqueness result exists for my kind of problem? (i.e. problem with increasing constraint on the solution) would be great if I could just find a reference for this as I was not able to find any . Otherwise, I am trying to build the solution myself, resorting to some kind of pasting. But Iâm not sure itâs correct to do it this way.
The idea is that, Iâm using Picard-Lindelof starting from $y(0)=0$ up to the first point where $f_2â(y) = 0$ that I denote $y_1$ . Then Iâm doing it again from $y_1$ to $y_2$ (the second point where $f_2â(y) = 0$ ). And so on and so forth. There are two âproblemsâ with this solution: How do I âstartâ from the $y_k$ points as generally speaking, $g(x_k, y_k(x_k))$ is not defined (or actually indeterminate $0$ over $0$ ), so Iâm not sure I can write it down. I should have trouble with this notion of âpointsâ with null derivatives. What if they are an infinite number of such points? I think I can prove that it is not possible on the compact set [0,1] if $f_1, f_2$ are $C_1$ and if the set such that $fâ_1 = 0$ is of null measure. But Iâm not sure and there might exist some $x sin(x)$ case I did not think about (even if $x sin(x)$ is not $C_1$ in $0$ so it does not enter my setup). And more importantly, if I can resort to this concept of points, I can just use the original system directly: because it is piecewise monotone between those points, I can invert it between those points! All of this to say Iâm really not sure about how to build a proof so far. I have a simple showcase example to illustrate the problem: Imagine $f_1(x) = 1 - 4 (x - 0.5)^2$ , $f_2(y) = 4(\sqrt{y} - y)$ . This example is built such that the unique increasing solution to this problem with initial value $y(0) = 0$ is $y(x) = x^2$ . The question is: how can I recover this using my differential equation $fâ_2(y(x)) yâ(x) = fâ_1(x)$ ? By my method, Iâll just realize that $y_1 = 0.25$ and $x_1 = 0.5$ (and that there is only one point such that the derivatives are $0$ ). And Iâll just have to split the Picard-Lindelof in two parts: starting from $(x,y)=(0,0)$ to $(0.5, 0.25)$ ; and then from $(0.5, 0.25)$ to $(1, 1)$ . But even there, you see that the system of differential equations is âindeterminateâ at $(0.5, 0.25)$ , so how can I âstartâ again from there? Thatâs why I find my âproofâ to be quite clumsy so far, and why I would like something more correct, if possible an already built result that I could cite or from which I could start my proof. EDIT: since my ODE is separable I should solve it by getting to the initial implicit form, so Iâm not sure getting to the differential equation is even necessary; I thought it could be easier to get some standard results about uniqueness though. But maybe I could get them with the initial implicit definition of $y(x)$ by adding a monotonicity constraint: i.e. there exists only one strictly increasing part in the integral curve. But I donât know how to show this. Sketch of an answer: If the set such that $f_1â(x) = 0$ has a finite number of points (instead of imposing measure zero), then I will do the proof as follows: Denote $x_k$ the ordered set of $K$ points such that $f_1â(x_k) = 0$ . Since we know the functions f_1, f_2, are such that an increasing solution exists, $f_2â(y)$ must also have the same finite number $K$ of points with $f_2â(y_k) = 0$ .
Using the order of these points and the monotonicity constraint on the solution, we know that $y(x_k) = y_k \forall k$ . Now, I can just use the fact that $f_2()$ is piecewise monotone between each $y_k$ . By monotonicity, since $y_k = y(x_k)$ , then all $y \in [y_k, y_{k+1}]$ are images of $x \in [x_k, x_{k+1}]$ . This way starting from (x, y) = (0,0), we have: $\forall x \in [0, x_1), \quad f_2(y) = f_1(x)$ . Moreover, on $[0, y(x_1))$ , $f_2$ is monotone (and $C_1$ ). So it is inversible, and we get: $\forall x \in [0, x_1), \quad y = f_2^{-1}(f_1(x))$ . Then we have $y(x_1) = y_1$ . And we repeat the same procedure on $[x_1, x_2)$ , and so on and so forth up to $[x_K, 1]$ . This way, I get a unique increasing mapping $y(x)$ exploiting piecewise monotonicity of my function. Now, Iâm not $100\%$ sure of this proof. And more importantly, Iâm not sure how to write it down if I have an infinite number of points with null derivative.
I guess the idea would be exactly the same (all that matters is the ordering). Itâs just that Iâm not sure everything I wrote above would follow through and still be correct. Maybe I did not think about some particular case that would forbid my piecewise inversions. For example, $f_2(y) = y^3 sin(1/y)$ cannot be considered piecewise monotone near $0$ I suppose... But Iâm not sure. Remark: I thought ODE would have results on this (i.e. proving existence of a unique increasing solution), but I cannot find anything, so in the end using the ODE form of my equation seems quite useless. I just exploit the initial implicit form.","['ordinary-differential-equations', 'monotone-functions', 'real-analysis', 'continuity', 'piecewise-continuity']"
3556730,Find image $f(z)=1/z$ given set $\{z=x+iy \in \mathbb{C} : \ (x-1)^2 + y^2 = a^2\}$,"Let $f(z)=1/z$ . Consider the set $S = \{x+iy \in \mathbb{C} : \ (x-1)^2 + y^2 = a^2\}$ . Find the image of $S$ under $f$ . The hint is to consider $0 < a < 1$ separate from $a \geq 1$ . I'm not sure how to visualize this -- we haven't spent any time in lecture on this. My attempt: we can write $f(z) = 1/z = \overline{z}/|z|^2 = (x-iy)/(x^2+y^2)$ . Obviously the image of $S$ under $\overline{z}$ is exactly the same circle since each point of the circle gets flipped about the $x$ -axis. However, I'm not sure how the scaling $1/|z|^2$ affects things, since it depends on the point $z$ .","['complex-analysis', 'transformation', 'complex-numbers']"
3556755,Become an independent math researcher. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 4 years ago . Improve this question I'm 22 years old and studied 2 years of math before droping out to travel the world as an assistant of a friend who make commercials, documentaries and media creation in general. I really love math and cs however I love to travel and learn about new cultures, so I can't go back to school in the near future, I'm getting more responsibilities and probably continue doing something similar for living like photography or videography. I was wondering is possible to become an amateur mathematician doing relevant research independently? I can study 4 hr everyday, no more because of my work  and because I have many hobbies. Also I can't work intellectually in a subject for more than 4 hrs. (I use to practice shogi and learn that 4 hrs distributed in a day work best for me). I've liked algorithms since I'm a child,  When I was in elementary school  I liked this girl but I was super shy so I thought that if everybody in the class submit an ordered list of the people who liked we could make an algorithm to match people. I develop an algorithm who would make this possible and I remember that not was that simple, almost took me a whole year. I think I have strong basics in abstract algebra, real analysis and linear algebra. I also have self study algorithms, computer complexity, basic computer theory and basic computational geometry. But now I don't know how to keep going. I also know to program because I enjoy making programs to solve problems like python scripts to automate download series or to automate the things I repeat a lot. For the things I learn I probably could get a CS degree since I'm interested in this themes since I was 12 and took one robotics course, I didn't learn much robotics but I was amazed with computers so I started programming. I'm not interested in money, positions in universities or even social prestige (I probably would use TrashPanda as my penName like the boubarky school of math). Because I think academia has some big problems like the way academical publishing works. But as you probably has noted I really like fake internet prestige and I think don't would be cool that someone with no formal math education and with a pen name like trash panda started to publish in academic publications. I admire profundy Erick Demaine and I after ""reading""(I couldn't comprenhend many of the paper he have) some of his papers and the page in the CSAIL of MIT  I think I'm interested in: â Discrete and computational geometry: Folding and unfolding , linkages , robotics, motion planning, dissections, simple polygonizations Algorithms and their analysis: Adaptive computation, graph algorithms,  string matching, randomized algorithms, approximation algorithms,  fixed-parameter algorithms, streaming algorithms Combinatorics: Discrete mathematics, graph theory (matchings, minors, treewidth, â¦), combinatorial game theory . â My plan is to self learn combinatorics (With the books: Principles and Techniques in Combinatorics by Chen Chuan Chong and Brualdis book in combinatorics). Dominate the book CLRS of algorithm analysis. For CG I plan to complete Devadoss & O'Rourke's Discrete  & Computational Geometry (I'm half way), the Springer published Computational  Geometry: Algorithms and Applications, and David Mount's notes ( Page on Umd ) . But I figurated that I can start reading some papers that are not that hard, I pass a lot of time in Mexico and in Chile so probably I can look from a professor there that could help me. Or maybe someone here would like to mentor me. (I promise that I could research a lot here or in mathexchange before as you) What you guys recommend? I feel like when you like a person so much but you can't see her/him. Is there some papers I should read? How can I discover a proper theme acording my current level to try to investigate? Thanks in advance to everybody and obligatory sfmbe.","['self-learning', 'computational-geometry', 'reference-request', 'combinatorics', 'algorithms']"
3556782,How to expand $\sin (2z+1)$ in powers of $(z+1)$?,"I'm trying expand $\sin (2z+1)$ in powers of $(z+1)$ without search the coeficients of the Taylor Series. I'm trying using the Mclaurin expansion for $\sin$ and $\cos$ , so: \begin{align}
\sin(2z+1) &= \sin(2(z+1)-1) \\
&= \sin(2(z+1))\cdot \cos(1)-\cos(2(z+1))\cdot \sin(1) \\
&= \sum_{n=0}^{\infty}\frac{(-1)^n\cdot2^{2n+1} \cdot (z+1)^{2n+1}\cdot cos(1)}{(2n+1)!}
\\  &-\sum_{n=0}^{\infty}\frac{(-1)^n\cdot2^{2n} \cdot (z+1)^{2n}\cdot \sin(1)}{(2n)!}
\end{align} I'm stuck here, i've tried put in the same sum the terms but i can't get a ""friendly"" result.","['complex-analysis', 'power-series', 'taylor-expansion']"
3556891,"What is the value of $x$, given that $f\left(x\right)+f\left(\frac{1}{1-x}\right)=x$ and $f^{-1}\left(x\right)=2$?",I tried to approach the problem by using the equation $f\left(2\right)=x$ but I always get stuck in the middle of the process.,"['functional-equations', 'systems-of-equations', 'inverse-function', 'analysis', 'functions']"
3556933,"If $n$ people are moving to $n$ chairs, what is the maximum number of intersections of their paths of motion?","I generated this sequence while working on this problem. Say we have $n$ people trying to sit in $n$ chairs. The chairs and people are all at lattice points (people at $(0,0),(1,0),(2,0),\dotsc,(n-1,0)$ and chairs at $(0,1),(1,1),(2,1),\dotsc,(n-1,1)$ ).
What is the maximum number of intersection points between paths of motion from a person's original position to a chair? I have the solution for $n = 5$ , and generated up until $n = 10$ . Can you have any insight into developing a formula?","['number-theory', 'sequences-and-series']"
3556971,"Proving that if f' is bounded in the fixed point, then f' is bounded on some interval","I have a question related to the Problem 10 in Exercises 1.7  (p. 35) in the book ""Discrete chaos"" , Saber Elaydi. The Problem is: ""Assume that $f$ is continuously differentiable at $x^*$ . Show that if $|f'(x^*)| <1$ , for a fixed point $x^*$ of $f$ , then there exists an interval $I= (x^* -\delta, x^*+\delta)$ such that $|f'(x)| \leq M <1$ for all $x \in I$ and for some constant $M $ . "" My attempt was: $f$ is  continuously differentiable at $x^*$ , so it is continuous at $x^*$ , so for all $ \epsilon > 0$ $\exists  \delta > 0  $ such that $ |x- x^* |< \delta \implies  |f(x) - f(x^*) |< \epsilon $ . I wanted to prove that on the interval $(x^*-\delta, x^*+\delta)  $ holds: $|f'(x)| \leq M <1$ , for some $M$ . We can write $|f'(x)| $ as: $|f'(x)| = | f'(x) - f'(x^*) + f'(x^*) | \leq | f'(x) - f'(x^*)| + |f'(x^*) | \leq \epsilon + 1$ , where $x \in (x^*-\delta, x^*+\delta)  $ Now I don't know what to do, can someone help me? Thanks in advance.","['calculus', 'derivatives', 'fixed-points']"
3556980,"Say a finite set $M$ has two partition $A_1,A_2,...A_p$ and $B_1,B_2,...B_p$ such that ...","Say a finite set $M$ has two partitions $A_1,A_2,...A_p$ and $B_1,B_2,...B_p$ such that $$A_i\cap B_j = \emptyset \implies |A_i|+|B_j|\geq p.$$ Prove: $$|M|\geq {1\over 2}(p^2+1).$$ As far as I can remember (now forgot) the solution was short and easy at the time a saw the problem (about 5 years ago). My try: Say $A_1$ cuts $k$ sets from other partition, say $B_1,,,B_k$ . Clearly $k\leq |A_1|$ since each element in $A_1$ is in exactly one $B_j$ . Then we have \begin{align}|A_1|+|B_1| &=|A_1|+|B_1|\\
&\vdots \\
|A_1|+|B_k| &=|A_1|+|B_k|\\
|A_1|+|B_{k+1}| &\geq p\\
&\vdots \\
|A_1|+|B_p| &\geq p
\end{align} Summing all those we get $$p|A_1|+|M| \geq k|A_1|+|A_1|+p(p-k)$$ and now I don't have control over $k$ ...","['set-partition', 'extremal-combinatorics', 'combinatorics', 'elementary-set-theory', 'inequality']"
3557018,Probability that random variable is inside cone,"Suppose $x\in\mathbb{R}^n$ is a random variable with mean $\mu$ and covariance $
\Sigma$ . Consider a stochastic convex optimization problem, i.e. an optimization problem with chance constraints , meaning there is a small, but finite probability, $\Delta\leq 0.5$ , of violating the constraints. In all of the cases I've encountered so far, you assume that the constraint space, $\mathcal{X}$ , is a polytope , meaning it can be written as $$
\mathcal{X} \triangleq \bigcap_{j=1}^{M} \ \{x:\alpha_j^\intercal x \leq \beta_j\}
$$ Qualitatively, this represents a finite intersection of linear inequality constraints, which is a convex region. In 2D, this is simply a polygon, with $M$ vertices. For example, if $M = 3$ , then the intersection of the three lines would form a triangle. If $M = 4$ , this would be a square, and so on. The reason people assume the constraint space is a convex polytope is because, using Boole's inequality (which gives an upper bound on the union of sets), the chance constraints can be written as $$
\begin{align}
\text{Pr}(x\notin&\mathcal{X}) \leq \Delta\\
&\Updownarrow\\
\text{Pr}(\alpha_j^\intercal x \leq \beta_j) &\geq 1 - \delta_j, \ \forall j = 1,...,M\\
\sum_{j=1}^{M} \delta_j &\leq \Delta,
\end{align}
$$ where the joint probability of violating the constraints is split up into the individual probability of violating each $j$ th constraint. This is extremely useful, because the second expression is nothing more than the probability of a random variable with mean $\alpha_j^\intercal \mu$ and covariance $\alpha_j^\intercal \Sigma \alpha_j$ . Thus, this probability can be written in terms of the standard normal CDF ( $\Phi$ ) as $$
\Phi\Bigg[\frac{\beta_j - \alpha_j^\intercal \mu}{\sqrt{\alpha_j^\intercal \Sigma \alpha_j}} \Bigg] \geq 1 - \delta_j \Rightarrow \alpha_j^\intercal \mu + \|\Sigma^{1/2} \alpha_j\|^2 \Phi^{-1}(1-\delta_j) \leq \beta_j,
$$ since $\Sigma > 0$ is always positive definite, as it represents a standard deviation. The above inequality constraint is a second order cone constraint, and the resulting optimization problem is a SOCP. However, what if the constraint space is now not an polytope (or polygon), but rather a cone , specifically a convex cone. In that case, $\mathcal{X}$ would be defined as $$
\mathcal{X} = \{x : \|Ax+b\|_2 \leq c^\intercal x + d\}.
$$ Is it possible, in any way, to calculate $\text{Pr}(x\notin\mathcal{X})$ , or something like that as in the case of a polytope? You would have to make some kind of approximation or relaxation, such as Markov's inequality or Chebyshev inequality, to get rid of the probability and turn it into an expectation. However, I can't seem to figure out a solution. For my purposes, the cone is centered at the origin, so $b = d = 0$ if that makes it simpler to work with. This type of constraint is more natural in a physical setting, especially in controls, where you want to steer distributions from some initial $x\sim\mathcal{N}(\mu_0,\Sigma_0)$ , to the origin for example. I haven't found any other literature on this subject, so if anyone has any insights, it would be appreciated!","['convex-optimization', 'convex-cone', 'second-order-cone-programming', 'probability']"
3557023,Complex Lie groups,"A text states every complex connected Lie group must be abelian. Now surely this must be an error and has to be wrong because the unitary group is certainly (has to be complex in general) complex except for the real subgroups such as real orthogonal group and it is NOT abelian. For an easy example $SU_2$ for which the generators are the 2 dim. Pauli spin matrices in which the usual representation has $\sigma_2=\sigma_y$ say as imaginary and then taking $\exp(\sqrt{-1}t_i\sigma_i)$ for arbitrary real scalars $t_i$ ,implicit repeated sum over $i$ $1$ to $3$ and all possible infinite products of such is certainly a complex Lie group and not abelian. Eg obviously the subgroups $\exp(\sqrt{-1}\sigma_{1\text{ or }3})$ has imaginary generators $\sqrt{-1}\sigma_{1\text{ or }3}$ in a familiar basis. So would we not
all agree the statement is false?","['unitary-matrices', 'connectedness', 'complex-analysis', 'abelian-groups', 'lie-groups']"
3557087,Toss two coins until two Heads and two Tails come up,"You play a game where you toss two fair coins in the air. You always win $1. However, if you have tossed 2 heads at least once, and 2 tails at least once, you surrender all winnings, and cannot play again. You may stop playing at anytime. Whatâs your strategy? My thoughts were that this seems similar to the coupon collector. We have two bad events (2H and 2T). So after the occurence of the first bad event the second one will occur in an expected number of 4 turns. So my strategy is to stop after 3 tosses from the moment the first bad event occured.
However, I can't prove it.","['combinatorics', 'probability']"
3557093,Finite version of SzemerÃ©di implies Furstenberg's theorem?,"I want to prove that theorem 1 implies theorem 2. Theorem 1: Let $k \geq 3$ be an integer and let $0<\delta\leq 1$ . Then there is a positive integer $N(k,\delta)$ such that for any $N \geq N(k,\delta)$ an arbitrary set $A \subseteq [1,N]$ with $|A|\geq \delta N $ contains an arithmetic progression of lenghth $k$ . Theorem 2: Let $(X,\mathcal{B},\mu)$ a measure space. Let $T$ be a self-map of $X$ preserving the measure $\mu$ and let $k \geq 3$ . Then for any measurable set $E$ with $\mu(E)>0$ there is an integer $n >0$ such that $$\mu(E \cap T^{-n}E \cap \dots \cap T^{-(k-1)n}E)>0 $$ I have a proof that is not very clear to me in some steps. Here it is: Let $N$ be a sufficiently large positive integer (we shall specify this number below). For any $x \in X$ we consider the set $\Lambda(x)=\{l \in [1,N]: T^{l}x \in E\}$ . So we have $$ \int_X |\Lambda(x)|d\mu=N\mu(E) $$ Let $M=\{x \in X: |\Lambda(x)| \geq N\mu(E)/2\}$ .It follows that $\mu(M) \geq \mu(E)/2$ . Let $N=N(k,\mu(E)/2)$ . So, for theorem 1, for any $x \in M$ the set $\Lambda(x)$ contains an arithmetic progression $\{a(x)+b(x)j\}_{j=0}^{k-1}$ of length $k$ . Thus, to any point $x \in M$ we have assigned a pair of numbers $(a(x),b(x))$ . Now, there is a step that I don't understand: ""Since for any $x \in X$ we have $(a(x),b(x)) \in [1,N]^{2}$ , there is a set $M' \subseteq M$ such that $\mu(M')\geq \mu(E)/(2N^{2})$ , and to any point of this set we have assigned the same pair $(a,b)$ . In this case $\mu(\bigcap_{j=0}^{k-1} T^{-(a+bj)}E)\geq \mu(M')>0$ ."" Where the subset $M'$ comes from?","['ergodic-theory', 'combinatorics', 'arithmetic-progressions']"
3557115,Divisibility rule for large primes,"I've got a large prime $p$ .
Let's say $$ p = 2137 $$ Is there a simpler way to determine whether $p\mid n : n \in \mathbb{Z}$ than factorizing $n$ ?
Divisibility rules are uncommon for integers larger than 30, let alone primes.","['number-theory', 'divisibility', 'prime-numbers']"
3557171,On proving that $\sum\limits_{n=1}^\infty \frac{n^{13}}{e^{2\pi n}-1}=\frac 1{24}$,"Ramanujan found the following formula: $$\large \sum_{n=1}^\infty \frac{n^{13}}{e^{2\pi n}-1}=\frac 1{24}$$ I let $e^{2\pi n}-1=\left(e^{\pi n}+1\right)\left(e^{\pi n}-1\right)$ to try partial fraction decomposition and turn the sum into telescoping, but methinks it doesn't lead anywhere and only makes things hairy. How does one go about proving this? Thanks.","['summation-method', 'summation', 'proof-writing', 'pi', 'sequences-and-series']"
3557228,Derangement with letters repeated.,"Find the total number of Derangement of the word:
""mississippi"".
Can some one please suggest a concrete method in how to deal with Derangement with repeated letters..
I solved a question like Derangement of ""Bottle"", using help of an answer on stack exchange(you can see it here ), but I was unable to comprehend when more than 1 letters got repeated which is the case with ""mississippi""..
As per the answer to find derangement for Bottle I did: $$(D6-D4-2D5)/2$$ So please suggest some concrete methods and also some references for further reading ...","['permutations', 'combinations', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
3557241,Existence of a minimal radius when a map fails to be a diffeomorphism,"Let $D \subseteq \mathbb{R}^2$ be the closed unit disk. Let $f:D \to D$ a smooth map with everywhere invertible differential. For each $t \in (0,1]$ let $D_t$ denote the closed disk with radius $t$ . Question: Suppose that $f$ is not a diffeomorphism on $D$ .   Does there exist a minimal $t \in (0,1]$ such that the restriction $f|_{D_t}$ is not a diffeomorphism onto its image? Does the answer change if we replace $D_t$ by the open disk with radius $t$ ? (If not, does the critical value of $t$ change?) Note that the set $A=\{ t \in (0,1] \, | \, f|_{D_t} \text{is a diffeomorphism onto its image} \}$ is clearly ""closed down"", i.e. $\,$ $(y \in A \, \text{  and  }\, 0<x<y) \Rightarrow x \in A$ . Writing $\sup A=s$ , we then must have $A=(0,s)$ or $A=(0,s]$ . An equivalent formulation of the question would then be to prove that $A$ is an open set, or $s \notin A$ . (Then $s$ would be the required minimal $t$ ). My immediate approach (assuming we are talking about open disks, actually) was the following: Note that the restriction of $f$ to an open subset of $\text{int}(D)$ is a diffeomorphism onto its image if and only if it is injective (due to the inverse function theorem). Now, take $s_n \searrow s$ ; we know that $f|_{D_{s_n}}$ is not injective- so there exist $x_n \neq y_n \in D_{s_n}$ such that $f(x_n)=f(y_n)$ . By compactness we can assume that $x_n \to x,y_n \to y$ . If $x \neq y$ we are done since $x,y \in D_s$ . But I don't see any reason to exclude the possibility $x=y$ .","['singularity', 'smooth-manifolds', 'real-analysis', 'differential-topology', 'differential-geometry']"
3557248,What is the expectation of number of words 'ab',"What is the expectation of number of words 'ab'  in random 20 length phrase that use letters from $\left \{ a,b \right \}$ ? There are $2^{20}$ words with length 20 over an alphabet with 2 letters...","['statistics', 'probability']"
3557252,"Can the "" same base, same exponent "" rule be extended to sums?","By "" same base, same exponent "" rule I mean  : $$b^x = b^n \iff x=n$$ Can the equation: $$ 2^x+4^x = 6$$ be solved by saying $$ 2^x+4^x = 6 = 2+4= 2^1 + 4^1$$ and consequently, that $$x = 1? $$","['algebra-precalculus', 'soft-question', 'exponential-function']"
3557331,Cauchy-Schwarz inequality of the carrÃ© du champ operator,"I want to prove the inequality $$\Gamma(f,g)^2 \leq \Gamma(f,f) \Gamma(g,g) \ \mu \text {-almost everywhere}$$ for $f, g \in \mathcal{A}$ where $\Gamma$ denotes the carrÃ© du champ operator. More precisely:
Let $(E, \mathcal{E})$ be a Polish space with $\sigma$ -finite measure $\mu$ . The function $\Gamma: \mathcal{A} \times \mathcal{A} \to \mathcal{A}$ is symmetric, bilinear and satisfies $\Gamma(f,f) \geq 0$ $\mu$ -almost everywhere for all $f \in \mathcal{A}$ . $\mathcal{A}$ is a subalgebra of $L^\infty(E, \mathcal{E}, \mu)$ which is dense in $L^p(E, \mathcal{E}, \mu)$ for all $p \in [1, \infty)$ . My idea was to copy the proof of Cauchy Schwarz's inequality. We obtain $$
0 \leq \Gamma(f - \lambda g, f - \lambda g) = \Gamma(f,f) - 2\lambda\Gamma(f,g) + \lambda^2 \Gamma(g,g)
$$ $\mu$ -almost everywhere for all $\lambda \in \mathbb{R}$ . But i cannot justify the argument defining $\lambda := \frac{\Gamma(f,g)}{\Gamma(g,g)}$ since the non-negativity of $\Gamma(f - \lambda g, f - \lambda g)$ is just $\mu$ almost everywhere where the sets of measure zero might depend on $\lambda$ . Annother idea is to use Cauchy Schwarz's inequality for the bilinear form $(f,g) \mapsto \int \Gamma(f,g) \varphi d \mu$ for fixed $\varphi \in L^1(E, \mathcal{E}, \mu)$ , $\varphi \geq 0$ . We obtain $$
\left ( \int \Gamma(f,g) \varphi d \mu \right )^2 \leq  \int \Gamma(f,f) \varphi d \mu \int \Gamma(g,g) \varphi d \mu 
$$ but I don't know how to get a pointwise estimate from that. Tto the best of my knowledge, Lebesgue differentiation theorem fail in general Polish spaces.","['integration', 'real-analysis', 'functions', 'functional-analysis', 'probability']"
