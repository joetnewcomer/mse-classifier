question_id,title,body,tags
4691158,Set of Equations to Helically Wrap a Sine Curve,"I am attempting to plot (3d) a Sine Curve along a helical path. Very similar to the following answer, but I need to be able to control the phase and angular frequency of the sine curve as well and this answer does not provide expected results when modifying the inputs in Python: https://math.stackexchange.com/a/828579/1176881 So, I need to wrap the following curve: $$
f(x) = A*sin(\omega x+\phi) + y_{shift}
$$ Around a cylinder of radius $r$ with z-angle (helical angle) of $\theta$ . Again, when I tried the above community answer, I did not get expected results when modifying the input parameters to the sine curve. Additionally, that answer does not say how to modify the angular frequency or the cylinder radius and I am guessing I did not apply it properly. For reference, here is my Python code thus far: import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D                         
from mpl_toolkits.mplot3d import proj3d    


tmin = 0
tmax = 100

diam = 50
res = 1e6 # point resolution
amp = 100 # amplitude of sine wave
freq = 20 # frequency of sine wave
t = np.linspace(tmin,tmax,int(res)) # primary axis (time, etc)
theta = 5 # deg, tilting angle of sine wave
omega = freq*2*np.pi

x = diam * (np.cos(t*np.cos(np.radians(theta)) - amp*np.sin(np.radians(theta))*np.sin(freq*t)))
y = diam * (np.sin(t*np.cos(np.radians(theta)) - amp*np.sin(np.radians(theta))*np.sin(freq*t)))
z = t*np.sin(np.radians(theta)) + amp*np.cos(np.radians(theta))*np.sin(freq*t)


plt.close('all')       
fig = plt.figure()

ax = fig.add_subplot(1,2,1, projection='3d')
ax.plot(x,y,z)
ax.set_box_aspect((np.ptp(x), np.ptp(y), np.ptp(z)))  # aspect ratio is 1:1:1 in data space


ax2 = fig.add_subplot(1,2,2)
ax2.plot(x,z)

plt.show() ```","['trigonometry', 'python']"
4691220,Brezis' exercise 5.14,"I'm trying to solve below exercise in Brezis' Functional Analysis Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $\Vert \cdot\Vert $ its induced norm. Let $a: H \times H \rightarrow \mathbb{R}$ be a continuous bilinear form such that $$
a(v, v) \geq 0 \quad \forall v \in H
$$ Prove that the function $F: H \to \mathbb R, v \mapsto a(v, v)$ is convex, of class $C^1$ , and determine its differential. The solution by the author is The convexity inequality $a(t u+(1-t) v, t u+(1-t) v) \leq t a(u, u)+$ $(1-t) a(v, v)$ is equivalent to $t(1-t) a(u-v, u-v) \geq 0$ . Consider the operator $A \in \mathcal{L}(H)$ defined by $a(u, v)=(A u, v)$ for all $u, v \in H$ . Then $F^{\prime}(u)=A u+A^{\star} u$ , since we have $$
F(u+h)-F(u)=\left(A u+A^{\star} u, h\right)+a(h, h) .
$$ I would like to verify that the explicit form of $F'(u):H \to \mathbb R$ is indeed $F'(u)[v] = a(u, v)+ a(v, u)$ for all $v\in H$ . Could you have a check on my attempt? Because $a$ is continuous, there is $C>0$ such that $\Vert a(u, v)\Vert  \le C\Vert u\Vert \Vert v\Vert $ for all $u, v\in H$ . Then $$
\begin{align}
& \lim_{v \to u} \frac{\Vert F(v)- F(u) - F'(u)[v-u]\Vert }{\Vert v-u\Vert } \\
= & \lim_{v \to u} \frac{\Vert a(v, v)- a(u, u) - (a(u, v-u)+ a(v-u, u))\Vert }{\Vert v-u\Vert } \\
= & \lim_{v \to u} \frac{\Vert a(v-u, v-u)\Vert }{\Vert v-u\Vert } \\
\le & \lim_{v \to u} C \Vert v-u\Vert =0.
\end{align}
$$ This completes the proof.","['hilbert-spaces', 'frechet-derivative', 'derivatives', 'functional-analysis']"
4691232,Solving $x''+x+\text{sgn}(x')\sqrt{|x'|} = 0\ $ Does it have closed form solutions? Does it stop moving? Could it stop at a different place than zero?,"Solving $x''+x+\text{sgn}(x')\sqrt{|x'|} = 0\ $ Does it have closed form solutions? Please show how you got them. Does it stop moving? There exists a finite extinction time $|T|<\infty$ such $x'(t) = 0,\ \forall t\geq T$ . Which is the formula of $T$ given the initial conditions? Could it stop at a different place than zero? It is possible for its solutions to stop moving at a constant value $x(T) \neq 0$ such as $x(t) = x(T),\,\forall t>T$ (this was answered in the comments by @aghostinthefigures - turns out it can only stop moving at $x(T) = 0$ ). Added later In the following paper A note on the dynamics of an oscillator in the presence of strong friction , in the Proposition 2.3 tells that these kind of ODEs could have solutions of finite duration, and in Theorem 3.1 tells that are least there are 2 orbits that do the trick, but unfortunately is ""too advanced"" for me: I am not completely sure if the same equation is considered (I think is the case $\alpha = \frac12$ ), neither if is possible to find the finite extinction time $T$ from the formulas explained there and initial conditions.
Maybe someone more experienced in Diff. Eqns. Analysis could use it to solve part 2. Motivation_____________ (not required for giving an answer) After learning from this paper that the only possible way from a ODE for standing solutions that stop moving, it is if the differential equation have at least one Non-Lipschitz point in time. I have got really interested in them since everything I learned in engineering were modeled through solutions that can be represented through Power Series, which cannot represent this behavior (since matching a constant value forever will violate the Identity Theorem ). I have made a lot of question trying to understand differential equations that stop moving in this tag [finite-duration] since I have not find a specific theory with their treatment contained it in just one place, by instead spread in many other topics without a deep insight. Since every ""small-angle approximation"" and/or ""linearization"" will destroy the Non-Lipschitz component (as example, when using Perturbation theory ), its solutions cannot represent accurately the moment when system stop moving. I review in the question A brick sliding in an horizontal plane after an initial push... the simplest physics example which show, as example, a polynomial decay instead of the exponential decay expected for a solution of a Linear ODE. The answer is quite simple from energy analysis, which is good since show the closed-form solution founded is working ok. But most interestingly, the next step in complexity was introducing friction (Coulomb's Friction), and the answer become totally not obvious: I reviewed it in the question Closed-form solutions to $x''+\frac{k}{m}\ x+\mu\ g\ \text{sgn}(x')=0$ and the solutions show three things: There exist closed-form solutions as it where found by @eyeballfrog in his answer The solutions' speed profile indeed achieve a finite extinction time where its becomes zero after a finite time $0<T<\infty$ . The solution could stop moving in a position different from equilibrium, such as $x(T) \neq 0$ , which so far I know it is impossible to be achieved by any classic differential equation thought in undergraduate engineering physics classes (surely is impossible is the ODE is linear or if it solved through a power series: if you know some other kind of counter-examples, please share them here ). Since traditionally a physics' model required the ODE to be Autonomous , there is no many ""simple possible"" ways to make the ODE Non-Lipschitz, been already studied in the mentioned question the scenario $\text{sgn}(v)$ , here I ask for the next possible alternative $\text{sgn}(v)\sqrt{|v|}$ discussed in the mentioned paper and that could have some applications like in this other question . (Update: due the answer in the comments by @aghostinthefigures proving that only something of the form of $\text{sgn}(v)$ could stop moving in a place different from zero, I believe its a more accurate model now). So far from Wolfram-Alpha , it can be seen their solution could resemble an exponential decay ""at the end"" (where oscillations disappears), but is not clear if they are really achieving a finite extinction time when its stop moving, neither if is possible to stop moving in a position different from zero. I would like to understand if this example could be exploitable as an alternative for the traditional simple harmonic oscillator for modelling the physics of phenomena that stop moving (at least near the stopping time, like a non-linear version of the ""small angle approximation"" for these systems), as maybe could be used also the example reviewed in the mentioned question $x''+x+\text{sgn}(x')=0$ (preferred one in my opinion after @aghostinthefigures comment) . I Hope you found it as interesting as I do.","['mathematical-modeling', 'ordinary-differential-equations', 'finite-duration', 'mathematical-physics', 'dynamical-systems']"
4691233,Derivative of a continuous bilinear form,"I'm trying to solve below exercise Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space. Let $a: H \times H \rightarrow \mathbb{R}$ be a continuous bilinear form. Determine the derivative of $F: H \to \mathbb R, v \mapsto a(v, v)$ . Could you confirm if I correctly apply below Lemma and the chain rule? Lemma Let $E_1, \ldots, E_m, F$ be Banach spaces over the field $\mathbb{K} \in \{\mathbb{R}, \mathbb{C}\}$ . Let $\varphi: E_1 \times \cdots \times E_m \to F$ be a continuous multilinear map. Then $\varphi$ is continuously differentiable with $\partial \varphi (x_1, \ldots, x_m) : E_1 \times \cdots \times E_m \to F$ such that $$
\partial \varphi (x_1, \ldots, x_m) [h_1, \ldots, h_m] = \sum_{j=1}^m \varphi\left(x_1, \ldots, x_{j-1}, h_j, x_{j+1}, \ldots, x_m\right)
$$ for every $(h_1, \ldots, h_m) \in E_1 \times \cdots \times E_m$ . My attempt Let $G: H \to H \times H, v\mapsto (v, v)$ . Then $G$ is linear continuous. Then $\partial G (v) = G$ for all $v \in H$ . We have $F = a \circ G$ . By chain rule, $$
\partial F (v) = \partial a(G(v)) \circ \partial G (v) = \partial a(v, v) \circ G.
$$ By Lemma , $$
\partial F (v) [u] = a(u, v) + a(v, u).
$$","['banach-spaces', 'hilbert-spaces', 'frechet-derivative', 'functional-analysis', 'derivatives']"
4691251,Painted Cube and the Law of Total Probability,"Problem: Suppose you are given a white cube that is broken into $27$ pieces.  This cube was originally $3$ units by $3$ units, and has been broken into $27$ smaller cubes, witch each smaller cube having dimensions $1 \times 1 \times 1$ .  Before this cube was broken, all $6$ of its faces were painted green.  You randomly pick a small cube and see that $5$ faces are white.  What is the probability that the bottom face is also white? In this conditional probability problem, I am having trouble determining $P(A)$ and $P(A^c)$ , which are critical to determining $P(B)$ via the Law of Total Probability. $P(B|A)$ is relatively straightforward, since it is the probability of picking a mini-cube with 5 White Faces given that the Bottom Face is White.  Only 1 mini-cube satisfies that, the Core Cube, so it's probability ought to be $1/27$ . Likewise, $P(B|A^c)$ is pretty easy, its the probability of 5 White Faces given that the Bottom Face is Not White (Green).  Only 6 mini-cubes satisfy that, the ones in the centers of the 6 faces of the original cube.  This makes the probability $6/27$ . But what of $P(A)$ , the probability of ""Bottom White""?  Originally I thought it was $1/27$ , since there is only 1 cube that meets that configuration, but upon checking my answer it seems $P(A) = 1$ . $P(A^c)$ is the probability that the Bottom is not White, so originally I went with $6/27$ , the number of mini-cubes that could have 5 white faces and still have a bottom face that is not white.  However, the solution maintains $P(A^c) = 1/6$ . What is the reasoning that leads to $P(A) = 1$ and $P(A^c) = 1/6$ ?","['conditional-probability', 'statistics', 'bayes-theorem', 'probability']"
4691272,How do we treat degenerate preimages (functions with empty domain & codomains)?,"I am quite familiar with preimages, however, I have recently been taking a course on Set Theory and came across questions that involve preimages of functions with empty domains (and sometimes empty codomains too). I was wondering how we treat these particular cases and whether or not there is a particular convention when it comes to dealing with these particular problems. To make this more concrete, here is one of the questions that I have come across that prompted me to post my question: For the (unique) function $f: \emptyset \rightarrow$ { $\emptyset$ }, explicitely find the preimage $f^{-1} :$ { $\emptyset$ , { $\emptyset$ }} $\rightarrow$ { $\emptyset$ } My thoughts here were that { $\emptyset$ } $\mapsto \emptyset$ since there are no elements in the domain of $f$ and so this means that there are no elements that can possibly map to { $\emptyset$ }. I also believe that $\emptyset \mapsto \emptyset$ . I am less confident about this. I am looking for the elements of the domain of $f$ that map to nothing. On the one hand, the domain is empty so there is nothing that can possibly map to anything. However, since we are asking what elements of the domain map to nothing, it also feels like the entire domain maps to nothing which makes me think it might instead map the empty set to { $\emptyset$ }. Although perhaps I am overthinking it here. I would be grateful for any clarification.","['elementary-set-theory', 'definition', 'functions', 'terminology']"
4691289,$\lim_{x\to\infty} \left(\sqrt[3]{x^3+x^2+1}-\sqrt[3]{x^3-x^2+1}\right)$,"this is like an idea, the procedure is longer. Can someone please tell me if this is ok or if there is a better solution (hopefully shorter) \begin{align}
&\lim_{x\to\infty} \left(\sqrt[3]{x^3+x^2+1}-\sqrt[3]{x^3-x^2+1}\right) \\
&=\sqrt[3]{x^3+x^2+1} - \sqrt[3]{x^3-x^2+1}\\
&=\frac{2x^2}{\sqrt[3]{(x^3+x^2+1)^2} + \sqrt[3]{(x^3+x^2+1)(x^3-x^2+1)} + \sqrt[3]{(x^3-x^2+1)^2}}\\
&= \frac{2}{3}.
\end{align}","['limits', 'calculus']"
4691311,What is the cardinality of the set of injective functions from $\mathbb{N}$ to $\mathcal P \mathbb{N}$?,"Question What is the cardinality of the set of injective functions from $\mathbb{N}$ to $\mathcal P \mathbb{N}$ ? Attempt If we denote the desired set as $I$ , then we can find an upper bound by dropping the requirement for injectivity as follows: $$|I| \leq |\mathcal P \mathbb{N}|^{|\mathbb{N}|} = (2^{\aleph _0})^{\aleph _0} = 2^{\aleph _0}$$ However, I am struggling to find a suitable lower bound. It is quite easy to show that the set must be greater than or equal to $\aleph _0$ by constructing a function that maps natural numbers to sets containing only themselves. Although, I need to construct an injection from $\mathcal P \mathbb{N}$ into $I$ in order to show that $2^{\aleph _0}$ is also a lower bound. After this point, we can simply invoke Cantor-Schrdoder-Bernstein Theorem , to conclude that $|I| = 2 ^{\aleph _0}$ . I have read that the following is an injective map that provides the desired lower bound: $$ g(X) = n \mapsto \begin{cases}
\text{ {$n+1$}}  & \text{if $n \in X$} \\
\text{ {$0,\space n+1$}} & \text{if $n \not \in X$}
\end{cases}$$ However, I am hoping for a more intuitive solution that I could construct myself (if the above solution can be explained in an intuitive way, then I am also happy for that to be provided as a valid solution to the question). This was a former exam problem, so I was hoping to be able to understand the solution in a way that will allow me to construct functions of my own in similar examples. I would be grateful for any assistance here.","['elementary-set-theory', 'cardinals', 'functions', 'infinity']"
4691314,Prove that every solution to $z''+e^{z^2}=1$ is periodic.,"This is a problem from a homework assignment that is causing me some trouble. The technique provided in the section is as follows. Set $x=z,y=z'$ and convert the second order ODE to a linear system. In this case, we have $$(x,y)'=(y,1-e^{x^2})$$ Find the orbits of the linear system by solving $\frac{dy}{dx}=\frac{1-e^{x^2}}{y}$ . The orbits turn out to be level sets of $(x,y)\rightarrow \frac{y^2}{2}+\int_0^x e^{t^2}\mathrm{d}t-x$ If an aforementioned orbit forms a closed curve that doesn't contain an equilibrium point (the only equilibrium point of this system is the origin, FYI) then any solution to this system passing through a point on said orbit is periodic. Unfortunately, these level sets are not closed. Is there any other way to attack this problem? I assumed the problem set required the student to solve the problem using this theory since this theory was presented and proved in the corresponding section.",['ordinary-differential-equations']
4691349,"Best strategy for 6 sided dice game: Roll as many dice as you want, you lose if at least one $1$ appears.","I am new here because we got stuck with a question during the weekend's discussion. Let's imagine we are playing a dice game. You can roll as many 6-sided dice as you want, and so can your opponents, simultaneously. The highest sum wins, but if there is at least one "" $1$ "" in the roll, you score $0$ points in total. Is there ""the"" best strategy with how many dice to roll in order to statistically (unlimited repeats) win the game? If there is, how can we prove it? Our only starting point was the counterprobability for each throw. Each dice has a $5/6$ chance not to be a $1$ . So $1-(5/6 + 5/6 + \ldots)$ should be at least a "" $1$ "" in the throw. That's all we were able to manage, but we are missing the link to further calculations. Thanks a lot for your help. Edit:
Thanks for the hint with 1−(5/6)^n We are playing one round with only 2 players. If it is a draw --> reroll with the selected number of dice The other player does not know how many dice the other player selects. The first win ends the game. If you roll ""1"" the whole round scores ""0"" To the question:
Are you trying to win or trying to maximise the expected number of points?
We are trying to find the best number of dice to win the game, but it never has occurred to me, the the number of dice of the other player is of relevance! Thanks for the input Also thanks a lot for all of your comments. I really appreciate it.","['statistics', 'simulation', 'dice', 'game-theory', 'probability']"
4691356,How to find the u that minimize this integral,"I am trying to find u that minimizes: $$\int_0^\infty x^2(t)+u^2(t)dt$$ where the provided information is $x(0)=1$ and: $$\dot x=x^2+u$$ Here is what I have done. I tried to use Euler-Lagurange Equations, and then try to solve this with things for second order nonlinear equation: Because I can't find a way to go through it, I then tried to use another variable to replace $dx/dt$ : but then I stuck as well. Could you tell me how to solve this?","['integration', 'optimization', 'ordinary-differential-equations', 'euler-lagrange-equation']"
4691428,"Maschke's theorem, matrix version.","Maschke's theorem states that if $G$ is a finite group and $V$ a nonzero $G$ -module over $\mathbb{C}$ then $$
V=W_1\oplus W_2\oplus ... \oplus W_k
$$ where $W_i$ are irreducible submodules of $V$ . There is a similar matrix variant of Maschke's theorem, and I want to know if there is anything wrong with my following argumentation. Let $V$ be the $G$ -module such that $$
g\textbf{v}=X(g)\textbf{v}
$$ If we choose a matrix of transformation $T$ to be the one that expresses each $X(g)$ in the basis $$
B_V=\{ B_{W_1}, B_{W_2}, ..., B_{W_k} \}
$$ with $B_{W_i}$ being a basis for irreducible submodule $W_i$ , then we can write $$
TX(g)T^{-1}=\left(\begin{array}{cccc}X_1(g)&0&\cdots&0\\ 0&X_2(g)&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&X_k(g)\end{array}\right),
$$ where $X_i(g)$ is the matrix representation of $g$ afforded by submodule $W_i$ in the basis $B_{W_i}$ .","['representation-theory', 'change-of-basis', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4691457,Does $i*\arcsin(x) = \operatorname{arccosh}(x)$,"I was looking at the integral of $\frac{1}{\sqrt{x^2-1}}$ which equals $\text{arccosh}(x)$ . This seemed very similar to $\int\frac{1}{\sqrt{1-x^2}} = \arcsin(x)$ , which got me thinking about the following: $$
\begin{align}
\text{arccosh}(x) &= \int\frac{1}{\sqrt{x^2-1}} \\
&=  i \int\frac{1}{i\sqrt{x^2-1}} \text{ (as i is a constant)}\\
&= i \int\frac{1}{\sqrt{i^2 (x^2-1)}} \text{ (bringing i into the root)} \\
&= i \int\frac{1}{\sqrt{-1 (x^2-1)}} \text{ (as }i^2 = -1)\\
&= i \int\frac{1}{\sqrt{1-x^2}} \\
&= i \arcsin(x)
\end{align}
$$ The result $\text{arccosh}(x) = i \arcsin(x)$ seems absurd (and I could not find mention of this supposed equivalence anywhere). But according to this post , $i$ can be factored in and out of integrals, so where does my ""proof"" fail?","['integration', 'trigonometric-integrals', 'complex-numbers', 'hyperbolic-functions']"
4691473,"Loss functions - ""Huber-like""","Are there other functions that are similar to Huber loss?
More specifically, with the following properties: even function asymptotically linear quadratic close to the origin. One such function is $\log{(\cosh{(x)})}$ , and another is $\sqrt{x^2 + 1}$ . Are there any larger families of such functions? Or any useful representations of such functions? Or are there any further categorizations of such functions? Of course you can get new such functions in a trivial way by scaling/shifting horizontally/vertically, but that is not very interesting. This whole question is a bit of a shot in the dark. Personally, I'm not seeking to find a function to use as a loss function. The reason for my interest is that I have generated data from numerical calculations that have these particular properties, and it just so happens that the data I've generated looks like a loss function. I think going into how the data is generated would be too large of a scope for this question. Here is the family of functions that I am interested in, for reference. This is Huber loss.","['machine-learning', 'calculus', 'functions']"
4691502,Lower bound on $|\sum_{x \in X} \phi(x) - \int_{\mathbb{R^2}} \phi(x) dx |$ for $\phi$ smooth.,"Very recently, I discovered the field ""irregularity of distributions"". Given a infinite discrete subset X of $\mathbb{R}^2$ , the aim is to get a lower bound : $$|\operatorname{Card}(X \cap B) - |B|| \geq C f(B)$$ Here $f$ is a positive function, $C > 0$ is a constant, $|.|$ denote the 2 dimensional lebesgue measure and $B$ is a ball. In this paper : http://archive.ymsc.tsinghua.edu.cn/pacm_download/117/6400-11511_2006_Article_BF02392553.pdf , using Fourier techniques, it is proven (theorem 2A), that there exist a ball $B$ such that : $$|\operatorname{Card}(X \cap B) - |B|| \geq C |\partial B|^{1/2}$$ Here $|.|$ on the right-hand side denotes the 1 dimensional lebesgue measure and $\partial B$ the boundary of $B$ . I wonder is there is a way to obtain similar lower bound, but for smooth functions. Given $\phi : \mathbb{R}^2 \to \mathbb{R}^{+}$ , $C^{\infty}(\mathbb{R}^2)$ with compact support (but other assumption can be done on $\phi$ ), is it possible to lower bound the following quantity ? $$|\sum_{x \in X} \phi(x) - \int_{\mathbb{R^2}} \phi(x) dx | \geq C f(\phi)$$ Here $f$ is still a positive function. Do you have reference for this problem ? Or any hints ? Thanks,","['fourier-analysis', 'combinatorics', 'discrete-mathematics', 'reference-request']"
4691525,Uniform convergence of a series $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{\sqrt{n}}\sin(\frac{x}{n}).$,"I know this series is uniformly convergent on any compact subset of $\mathbb{R}$ by the famous Weierstrass test. However, it is NOT uniformly convergent on $(-\infty, +\infty)$ , could someone give me some ideas to prove this claim?","['uniform-convergence', 'real-analysis']"
4691575,"Generate N ""equally spaced"" vectors on the unit sphere","I want to generate a list of vectors $\{\mathbf{v}_i\}, i\in[1,N]$ such that $$\mathbf{v}_i\cdot\mathbf{v}_i = 1, \forall i$$ $$\mathbf{v}_i\cdot\mathbf{v}_j = \cos\theta, \forall i\neq j$$ $$\sum_{i=1}^N \mathbf{v}_i = 0$$ $$\mathbf{v}_1=(1,0,0)$$ $$\mathbf{v}_2=(\cos\theta,0,\sin\theta)$$ For $N=2$ , we have $\mathbf{v}_1=(1,0,0), \mathbf{v}_2=(-1,0,0), \theta=\pi$ . For $N=3$ , we have $\mathbf{v}_1=(1,0,0), \mathbf{v}_2=(-1/2,0,\sqrt{3}/2), \mathbf{v}_2=(-1/2,0,-\sqrt{3}/2),\theta=2\pi/3$ . etc. I would like to be able to generate this list of vectors for arbitrary $N$ . I can do it by hand, but I want to add this to some python code. I tried to come up with an iterative way of doing it, but I lose track of all the vector components I have to solve for pretty quickly. What is the simplest way to do this? $N$ is not going to be huge -- let's say $N=30$ at most. Because of this, and because this generation will be done once in my code, I'm not super worried about the performance of the python code that I will implement.","['trigonometry', 'vectors', 'geometry']"
4691579,Where is my mistake in this reasoning?,"One can vertify that $1+\frac{1}{2}z^2+\frac{1}{2}z^3$ has no root in the disk $D(0,1.05)$ , so $f(z)=\dfrac{1}{1+\frac{1}{2}z^2+\frac{1}{2}z^3}$ should be analytic in $D(0,1.05)$ . Assume it has Taylor series: $$f(z)=\sum_{n=0}^\infty c_nz^n,z\in D(0,1.05)$$ On the other hand, note that when $z\in D(0,1)$ , $\frac{1}{2}z^2+\frac{1}{2}z^3\in D(0,1)$ . Since $\dfrac{1}{1+w}=\sum_{n=0}^\infty (-w)^n,w\in D(0,1)$ , we also obtain $$f(z)=\sum_{n=0}^\infty \left(-\frac{1}{2}z^2-\frac{1}{2}z^3\right)^n,z\in D(0,1)\subset D(0,1.05)$$ In $D(0,1)$ , the two series equal to each other, so they should have the same corresponding coefficients. Therefore they should also have the same convergence radius $R$ . By the first series, $R\geq 1.05$ . However, the second series is not convergent at $z=1$ , thus $R\leq 1$ . There is a contradiction!","['complex-analysis', 'calculus', 'taylor-expansion', 'sequences-and-series']"
4691618,Definition of a character of a linear algebraic group,"This might sound like a silly question, but I seem to be getting different definitions from different texts. Let $k$ be some field and $G$ some linear algebraic group over $k$ . Now, the standard definition I have seen, for instance in Milne, J. S. , Algebraic groups. The theory of group schemes of finite type over a field , Cambridge Studies in Advanced Mathematics 170. Cambridge: Cambridge University Press (ISBN 978-1-107-16748-3/hbk; 978-1-316-71173-6/ebook). xvi, 644 p. (2017). ZBL1390.14004 . is that a character is a $k$ -group homomorphism $\chi:G\rightarrow \mathbb{G}_m$ . But, there is this result in Platonov, Vladimir; Rapinchuk, Andrei; Rapinchuk, Igor , Algebraic groups and number theory. Volume 1 (to appear), ZBL07671765 . which says that if $G$ is torus, then the following two conditions are equivalent: $G$ is split all its characters are defined over $k$ . Yes, Platonov and Rapinchuk define their characters over a much larger algebriacally closed field $\Omega$ , but wouldn't this mean that the two definitions of a character and hence the structure of the character group are different? Am I missing something? Are the definitions somehow similar?","['algebraic-number-theory', 'characters', 'algebraic-geometry', 'algebraic-groups']"
4691635,Find the middle point on an arc,"For an arc in 3D space with the following known variables: $A$ : start point $B$ : end point $C$ : center point $r$ : radius vector normal to the surface of the arc How can we calculate the coordinates of the point $D$ lying in the middle of the arc using the swept angle $\fracθ2$ ?
How would this angle be calculated correctly? I have tried the following: Calculate the $X$ direction vector as $X = C - A$ . Calculate the Y direction vector as Y = normal x X. Get the vectors from center to start and end, $CA$ and $CB$ respectively. Calculate swept angle as $\theta = \arccos\left(\frac{CA\cdot{CB}}{{\lvert CA\rvert}{\lvert CB\rvert}}\right)$ . $D = C + X_{dir}\cdot{r}\cdot{\cos{\frac{\theta}{2}}}+ Y_{dir}\cdot{r}\cdot{\sin{\frac{\theta}{2}}}$ This way I am sometimes getting a diametrically opposite point. The problem seems to be that the sign of the angle $\theta$ I am calculating does not respect the direction of the normal axis.","['trigonometry', 'geometry']"
4691661,Non continuous function,"I'm struggling to answer this question :
Show that this function admits partial derivatives in every direction without being continuous in $(0,0)$ : $f(x,y)=\begin{cases}
 y^2\log \left|x\right|& \text{ if } x \neq 0 
\\
 0 &  \text{ if } x=0
\end{cases}$ I have no problem showing that f admits partial derivatives in every directions, but I'm struggling to show that it is not continuous.","['partial-derivative', 'continuity', 'multivariable-calculus']"
4691680,Finitely generated group where $|g^n|=o(n)$ for all $g\in G$.,"This question arose after I gave a general talk on the sub-additive ergodic theorem, where I gave the (simple) proof that in a finitely generated group, if one has a stationary sequence of random elements of the group, $g_1$ , $g_2$ , $\ldots$ where the expected word length of $g_i$ is finite, then defining $h_n$ to be the product $g_1g_2\cdots g_n$ , one has $|h_n|/n$ converges almost surely to a constant. A question arose concerning the relationship between volume growth in the group and the speed of growth. I believe there is no relationship between the two, and wanted to claim that in any group with elements of infinite order, one can find a $g$ such that $|g^n|$ grows linearly. Let $G$ be a finitely generated group containing elements with infinite order. Let $S$ be a generating set. For $g\in G$ , write $|g|=\min\{n\colon \text{$g$ can be expressed as the product of $n$ elements in $S$}\}$ (the word metric). Must there exist $g\in G$ such that $\liminf_{n\to\infty}|g^n|/n$ (which is equal to $\inf_n|g^n|/n$ by Fekete's lemma) is positive?","['geometric-group-theory', 'finitely-generated', 'group-theory']"
4691686,Solving the following ratio of two series,"I'm currently trying to solve the following expression for $0< s \leq 1$ : $$\frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l-1 \choose 3n-1}}{\sum\limits _{n=0}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l \choose 3n}}$$ I've only made very modest progress, transforming it into the following ratio: $$\frac{3}{l}\frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}n\cdot x^{3n}\cdot{l-1 \choose 3n-1}}{1+\sum\limits _{n=1}^{\lfloor l/3\rfloor}x^{3n}\cdot{l \choose 3n}},$$ where $x=\frac{1-s}{s}$ . I'm trying to find an analytic expression for the whole ratio, for all $l\in\mathbb{N}_{>0}$ . More modestly, I'm curious to find a way to prove that/how the ratio converges for $l\rightarrow\infty$ . Numerical simulation suggest that it converges, for $l\rightarrow \infty$ , to $1-s$ . Any help/tips/pointers would be immensely appreciated!","['convergence-divergence', 'ceiling-and-floor-functions', 'analysis', 'sequences-and-series']"
4691697,"How to make sure a team wins more often than it loses in a tournament with n matches having equal probabilities of wins, losses and draws each?","In a tournament, team X plays with each of six other teams once. For each match the probabilities of a win, a draw and a loss are equal. Find the probability that team X finishes with more wins than losses. I don't know if I can call it exactly a binomial probability problem but here's how I approached it: We know that probability of win, loss or draw is $\frac{1}{3}$ each. Let's make some cases: CASE 1: When number of draws is 0: It would be 6, 5 or 4 wins. $(\frac{1}{3})^6(\binom{6}{6}+ \binom{6}{5} + \binom{6}{4})$ CASE 2: When there is 1 draw. It would be 5, 4 or 3 wins. $(\frac{1}{3})^6(\frac{6!}{5!}+\frac{6!}{4!}+\frac{6!}{3!2!})$ CASE 3: 2 Draws, thus 4 or 3 wins $(\frac{1}{3})^6(\frac{6!}{4!2!}+\frac{6!}{3!2})$ CASE 4: 3 draws, thus 3 or 2 wins $(\frac{1}{3})^6(\frac{6!}{3!3!}+\frac{6!}{3!2!})$ CASE 5: 4 draws, thus 2 wins $(\frac{1}{3})^6(\frac{6!}{2!4!})$ CASE 6: 5 draws, thus 1 win $(\frac{1}{3})^6(\frac{6!}{5!})$ Adding them all up gives the result: $\frac{284}{729}$ . What am I doing wrong here? Edit: The correct answer is $\frac{98}{243}$ .","['combinatorics', 'probability']"
4691704,Limit of the summation,"I have recently been trying out the following limit $$\lim _{a \rightarrow \infty}\left(2 \sqrt{a}-\frac{2 a^{3 / 2}}{3}+\frac{a^{5 / 2}}{5}+\sum_{n=3}^{\infty} \frac{2(-1)^n a^{1 / 2(2 n+1)}}{(2 n+1) n !}\right)$$ . While it seems that we cannot evaluate it through some standard methods, Wolfram gives $\sqrt{\pi}$ to be the answer. I tried evaluating it as the limit of a partial sum, but still not getting it. Any help would be appreciated. Thank you!","['limits', 'sequences-and-series']"
4691724,Convergence of sequence $x_{n+1}=\frac{x_n^2+2}{2x_{n}-1}$,"I got a problem as follows, I have stucked for a week and I got no progress... May I recieve some guidance? Define a sequence $\{x_n\}$ by $x_1=c$ and $\displaystyle x_{n+1}=\frac{x_{n}^{2}+2}{2x_n - 1}$ for $n\in\mathbb{N}$ , where $c\neq \frac{1}{2}$ is a constant. Prove that $\{x_n\}$ is convergent and find its limit. (Hint: Consider $\displaystyle c>\frac{1}{2}$ and $\displaystyle c<\frac{1}{2}$ separately.) I can kind of guess $\displaystyle\lim_{n\rightarrow\infty}x_n = 2$ , and find the recurrence relationship between $|x_{n+1} - 2|$ and $|x_n - 2|$ ,
but I failed to proceed. Nevertheless, I tried with the Cauchy convergence criterion by considering $|x_{n+1} - x_n|$ and I end up with nothing. Thank you in advance for your kind reply.","['recurrence-relations', 'analysis', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4691732,Probability of a biased random walk returning to zero on $\mathbb{Z}$,"Consider a biased random walk on $\mathbb{Z}$ with $P(x,x+1)=p>\frac{1}{2}$ . Let $T_0$ be the first hitting time of $0$ . I'm trying to find the probability $p_0=P_0(T_0<\infty)$ . In words, it is the probability of the random walk returning to $0$ . My attempt: I'm using the hint given in the first comment here . Let $p_x:= P_x(T_0<\infty)$ . Then using the Markov property after one step, we get: $$
\begin{align*}
    p_0 &= p\cdot p_{1} + (1-p)\cdot p_{-1} \\\\
    &= p\cdot p_{1} + (1-p) \\\\
    &= p\cdot (p\cdot p_{2}+ (1-p)) + (1-p) \\\\
    &= p^2\cdot p_{2} + p(1-p) + (1-p) \\\\
    &= \sum_{i=0}^{\infty} p^i(1-p) \\\\
    &= (1-p)\cdot \frac{1}{1-p}
\end{align*}
$$ The above is clearly not right and the answer is supposed to be $2(1-p)$ and it would be great to know what I did wrong above?","['random-walk', 'probability-theory', 'markov-chains']"
4691739,How to solve this ODE $~ y''+ (1-x^2)y = 0 $?,"I am trying desperately to solve the following differential equation; $$ y''+ (1-x^2)y = 0 $$ I am looking for all smooth function on $\mathbb{R}$ which solve this equation. I have tried everything in my power to do it, but I can't figure what would work. I realize this seems like a special case of Weber's differential equation, which has special functions as solutions. But I believe that for this particular equation there must be an elementary and elegant approach to find the solutions. What can be done? Edit: I am looking for an analytic solution and am also interested in how to arrive at the solution.","['ordinary-differential-equations', 'real-analysis']"
4691743,An upper bound of $n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k$ as $n$ fixed and $k$ varies,"I'd like to ask an upper bound $C_n$ such that $$
S_n(k):=n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k\leq C_n\cdot(1-1/n)^k
$$ for all (sufficiently large) $k$ .
A naive upper bound is to replace all $i^k$ with $(n-1)^k$ ,
which produces $$
S_n(k)\leq(2^n-2)\cdot(1-1/n)^k.
$$ On the other hand, by only considering $i=n-1$ term one get $$
S_n(k)\geq n\cdot(1-1/n)^k.
$$ I'd like to ask if one can get $C_n$ better than $2^n-2$ (namely, growth slower as $n\to\infty$ )? I notice that $$
\sum_{i=1}^{n-1}\binom{n}{i}i^k=\left(X\frac{\mathrm d}{\mathrm dX}\right)^k
\big((1+X)^n-1-X^n\big)\bigg|_{X=1}.
$$ But I failed to find a way to produce an upper bound from this. Any suggestions? EDIT: It turns out that I have asked a stupid question. It is equivalent to ask an upper bound of $$
C_n(k):=(n-1)^k\sum_{i=1}^{n-1}\binom{n}{i}i^k
=\sum_{i=1}^{n-1}\binom{n}{i}\left(\frac{i}{n-1}\right)^k
$$ for either (i) all $k\geq 0$ or (ii) for sufficiently large $k$ .
It's clear that $C_n(k)$ decreases as $k$ increases. For (i) the $2^n-2$ cannot be improved since $C_n(0)=2^n-2$ .
For (ii) it's $n+\epsilon$ for any $\epsilon>0$ (here the ""sufficiently large"" depending on $n$ and $\epsilon$ )
since $\lim_{k\to\infty}C_n(k)=n$ . What estimation is useful in my application is still not clear to me. Perhaps choose a $k_0$ (independent of $n$ ) and ask a simple upper bound of $C_n(k_0)$ ?","['inequality', 'combinatorics', 'analysis']"
4691774,Asymptotics of $F(\varepsilon)=\int^b_0\frac{dx}{\varepsilon +\phi(x)}$ as $\varepsilon\rightarrow0$ where $\phi(x)\sim Ax$ as $x\rightarrow0$.,"This is one part of an old qualifier analysis problem that I am trying to complete. Suppose $\phi$ is a continuous function on a finite interval $[0,b]$ and that $\phi(x)>0$ for all $x\in[0,b]$ . Assume that $\phi(x)\sim A x^r$ for some constant $A>0$ and $r\geq0$ .
Define $$F(\varepsilon)=\int^b_0 \frac{dx}{\varepsilon + \phi(x)}$$ If $0\leq r<1$ , then $\lim_{\varepsilon\rightarrow0}=\int^b_0\frac{dx}{\phi(x)}<\infty$ . If $r>1$ , then $$F(\varepsilon)\sim \frac{\pi}{r A^{1/r}\sin(\pi/r)} \varepsilon^{-(1-\frac1r)}\quad\text{as}\quad\varepsilon\rightarrow0$$ If $r=1$ , then $$F(\varepsilon)\sim\frac{1}{A}\log(\varepsilon^{-1})\quad\text{as}\quad\varepsilon\rightarrow0$$ Part (1) is a simple application of monotone convergence. When $r\geq1$ , $\lim_{\varepsilon0}F(\varepsilon)=\infty$ by monotone convergence. Part (2) I have done by first applying the  of variable $u=\varepsilon^{-1}x^r$ , followed by application of dominated convergence. All that gives $$\lim_{\varepsilon\rightarrow0}\varepsilon^{1-\frac1r}F(\varepsilon)=\int^\infty_0\frac{u^{\frac1r-1}}{1+Au}\,du$$ The later is a integral of the Mellin transform type and the expression in the problem can be obtained by complex contour integration. Part (3) is where I am having difficulties.  The asymptotic seems given in the problem seems correct. By assuming that $\phi(t)=At$ , then a direct computation gives $$F(\varepsilon)=\frac{1}{A}\log\Big(\frac1A+\frac{b}{\varepsilon}\Big)\sim\frac{1}{A}\log(\varepsilon^{-1})$$ I thing dominated convergence still useful but a trick may be needed. For example, the function $x\mapsto x^{-1}\phi(x)$ can be extended to $[0,b]$ as a strictly positive continuous function on $[0,b]$ and so, there is $M>0$ such that $\phi(x)\geq Bx$ for $x\in[0,b]$ . Hence $$\frac{1}{\varepsilon +\phi(x)}\leq\frac{1}{\varepsilon+Bx}$$ Any hints (not necessarily a complete solution) are appreciated. Thank you!","['integration', 'lebesgue-integral', 'asymptotics', 'analysis', 'real-analysis']"
4691789,$\sqrt{I}=(f) \Rightarrow I=(f^n)$? (in a polynomial ring over a field),"EDIT: imtrying46's answer in the comments is simpler than the one in the accepted answer (but both are good, and quite similar). This is a proof-verification question. If I'm correct, I still wonder if there are even more elementary proofs (see below), and whether I really never used assumption (4) as it seems to me. Assumptions: $K$ is a field. $I$ is an ideal of the polynomial ring $K[x_1,\dotsc,x_n]$ . The radical $\sqrt{I}$ of $I$ is generated by a single polynomial $f$ . $f$ is irreducible over the algebraic closure of $K$ . My question: Is $I=(f^n)$ for some $n\geq 1$ ? My attempt: I think the answer is Yes. Take the maximal $n$ such that $I\subset(f^n)$ ( $n\geq 1$ since $I\subset\sqrt{I}=(f)$ ). I'll prove that $I=(f^n)$ .
Write $I=(f^nh_1,\dotsc,f^nh_r)$ . Then (using NSZ), $(f)=\sqrt{I}=I(V(f^nh_1,\dotsc,f^nh_r))=I(V(f^n)\cup V(h_1,\dotsc,h_r))=
I(V(f^n))\cap I(V(h_1,\dotsc,h_r))=(f)\cap\sqrt{(h_1,\dotsc,h_r)}$ . In particular $f^\ell\in (h_1,\dotsc,h_r)$ for some $\ell\geq 0$ . The maximality of $n$ implies that $\ell=0$ . So $1\in (h_1,\dotsc,h_r)$ and thus $f^n \in (f^nh_1,\dotsc,f^nh_r)=I$ as needed. I think I'm probably correct, but I wonder if there's  a simpler proof. In particular, can we avoid the use of the Nullstellensatz? Maybe even avoid Hilbert's Basis Theorem? Also, I think I never used the irreducibility of $f$ , right?","['algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4691816,Eigenvector of 2-by-2 unitary matrix in spherical coordinates,"I was reading basic quantum mechanics, and stumbled upon this math problem. $$\begin{pmatrix} \cos\theta & e^{-i\phi} \sin \theta \\ e^{i\phi}\sin\theta & -\cos\theta\end{pmatrix}$$ Here I need to find the eigen values and eigen vectors corresponding to this matrix. I have successfully found out the Eigen Value of this vector, which is +1, and -1. When I am trying to find the eigen vector, I am getting lost. I have been to this position (by choosing eigen value = +1) --> $$\begin{pmatrix} \cos\theta-1 & e^{-i\phi} \sin \theta\\ e^{i\phi}\sin\theta & -\cos\theta-1\end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}=\begin{pmatrix} 0 \\ 0 \end{pmatrix}$$ The answer is $\begin{pmatrix} \cos\frac{\theta}{2} \\ e^{i\phi}\sin\frac{\theta}{2}\end{pmatrix}$ . Whatever solution I try, I am not reaching to this solution. Can anyone of you please at least show me the right direction? My background is NOT hardcore maths, and I am a software engineer, studying quantum mechanics and quantum computing by passion. Sorry if this question sounds silly. But any help is appreciated. I need to solve it to move to the forward chapters. Thank you.","['trigonometry', 'matrix-equations', 'linear-algebra', 'eigenvalues-eigenvectors']"
4691824,Checking the Lindeberg Condition,"Let $\{X_n\}$ be a sequence of independent random variables such that $\mathbb P(X_n=\pm 1)=\frac 14$ , $\mathbb P(X_n=\pm n)=\frac 1{4n^2}$ and $\mathbb P(X_n=0)=\frac 12 - \frac 1{2n^2}$ for all $n\ge 1$ . Define the triangular array $\{X_{nj}:1\le j\le n\}_{n\ge 1}$ by setting $X_{nj}=\frac{X_j}{\sqrt n}$ . Check whether the above triangular array satisfies the Lindeberg condition. I have calculated $$s_n:=\sum_{j=1}^n \frac{X_j}{\sqrt n}$$ and $$\sigma_{nj}^2:=\mathbb E[X_{nj}^2]=\frac 1n$$ and hence $$S_n^2:=\sum_{j=1}^n \sigma_{nj}^2 = 1$$ So, I need to prove $$\sum_{i=1}^n E[X_i^2\mathbf{1}_{|X_i|>\epsilon}]\to 0\;\; \forall \epsilon>0$$ to check the Lindeberg condition.
which is clearly false as the expression is a sum. I must have made some mistake somewhere which I can't figure out. Please help me.","['probability-distributions', 'convergence-divergence', 'probability-theory', 'probability', 'random-variables']"
4691871,Brezis' exercise 5.19,"I'm trying to solve below exercise in Brezis' Functional Analysis Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $|\cdot|$ its induced norm. Let $u_n, u \in H$ such that $u_n \to u$ in the weak topology $\sigma (H, H^*)$ and that $\limsup_n |u_n| \le |u|$ . Then $u_n \to u$ in norm topology. Could you verify my below attempt? We have $|u_n-u|^2 = |u_n|^2- 2 \langle u_n, u \rangle +|u|^2$ , so $$
\limsup_n |u_n-u|^2 \le \limsup_n  |u_n|^2 -2 \liminf_n \langle u_n, u \rangle + |u|^2.
$$ We have $\limsup_n |u_n| \le |u|$ implies $\limsup_n  |u_n|^2 \le |u|^2$ . We have $u_n \to u$ in $\sigma (H, H^*)$ implies $\liminf_n \langle u_n, u \rangle =|u|^2$ . It follows that $$
\limsup_n |u_n-u|^2 \le 0.
$$ This completes the proof.","['hilbert-spaces', 'functional-analysis', 'weak-convergence', 'weak-topology']"
4691910,"Clarification on a construction of a set $E \subset I$ intersecting every subinterval of $I$ with positive, non-full measure","For some bounded interval $I$ , start with some compact $K \subset I$ of positive measure that contains no interval (i.e, ${m(K \cap J) < m(J)}$ for every interval $J$ ). The complement $I \setminus K$ is open, and thus can be expressed as an at most countable union of disjoint open intervals $V_n$ . For each $V_n$ , let $K_n \subset V_n$ be a compact subset containing no interval with $m(K_n) > m(V_n)/2$ . Define $E_1 := \bigcup_{n=1}^\infty K_n \subset I \setminus K$ . Define $E_2 \subset (I \setminus K) \setminus E_1$ similarly and so on. Let $E := \bigcup_{n=1}^\infty E_n$ . For any subinterval $I' \subset I$ , we can show that $m(E \cap I') > 0$ . Yet I'm not sure about the other half of the inequality $m(E \cap I') < m(I')$ . Or could it be that the $E$ constructed in this way does not satisfy the given property?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4691935,"If working together, A and B complete 2 projects in 12 hours, how do you show that working separately, it may take over 24 hours to do 2 projects?","Here is a semi-algebraic explanation. Suppose $A$ works faster than $B$ .  Imagine that $A$ and $B$ start working on their separate projects at the same time. $A$ completes his project in time $t < 12$ hours.  Suppose $A$ now helps $B$ to complete his project. Since $A$ and $B$ are working together the whole time, the total time to complete the $2$ projects is $12$ hours, and the total time for the two of them is $12 + 12 = 24$ hours.  Since $A$ does not actually help $B$ after $t$ hours, $B$ must do the same work that $A$ did in $12 - t$ hours. Since $B$ works slower than $A$ , this will take more than $12 - t$ hours, so the total time for the two of them to complete their projects must be greater than $24$ hours. How do you express this argument using algebraic equations related to rates and times? I gave this some thought and the algebra is fairly simple, though I don't see any way to relate it to the explanation I gave.
Together A and B can do a project in an hour, so their combined work rate is 1/6 and their individual rates are 1/12+x and 1/12-x. For notational convenience, let c=1/12. The total time to complete the two projects is $t = 1/(c+x) + 1/(c-x) = 2c/(c^2 - x^2)$ , which will be minimal when $c^2 - x^2$ is maximal, which will be for x=0, giving $t>=2c/c^2 = 2/c$ . For c=1/12, this is t=24.",['algebra-precalculus']
4691940,Indicators and sampling with/without replacement,"I was reading the problem in this post: Solving for an expected value from discrete random variables : An urn contains $30$ marbles of which 8 are black, $12$ are red, and $10$ are blue. Randomly, select four marbles without replacement. Let $X$ be the number of black marbles in the sample of four. Compute $E(X)$ . I understand the solution using indicator random variables: We consider that the drawn marbles can be placed in a line, and on doing so let $X_i$ be the indicator that the $i$ -th marble in this line is black.   The probability that a particular marble in the line will be black is: $8/30$ .   Thus $\mathsf E(X_i)=8/30$ for all four marbles. By the Linearity of Expectation then: $E(X) $ $= \sum\limits_{i=1}^4 E(X_i) = \frac{16}{15}$ By the same logic, if we sampled with replacement, we should get the same answer. To me this is unintuitive. For instance, if we only had two black balls and we ask the same problem, wouldn't the two $E(X)$ still be the same? If we are sampling without replacement shouldn't the fact that sampling $3$ or $4$ black balls being impossible make this value different from where you sample with replacement (and it is possible to sample $3$ or $4$ balls)? Can someone give me some intuition of why this is reasonable? Much appreciated!","['statistics', 'probability']"
4691946,"Is $A - AB(B + BAB)^+ BA = A(A + ABA)^+A$ true for positive semidefinite $A, B$?","From extensive numerical simulation it seems that the following identity holds for two symmetric positive semidefinite matrices: $$
A - AB(B + BAB)^+ BA = A(A + ABA)^+A.
$$ I tried to prove this, and was able to verify it in the case $A, B$ are rank one. However, as mentioned above, it holds at least in simulation for general positive semidefinite matrices. Is there an easy proof? Here $P^+$ is the generalized inverse of $P$ .","['matrices', 'pseudoinverse', 'linear-algebra', 'positive-semidefinite']"
4691961,Derivative of $\arcsin(\cos x)$,"In my calculus course, my teacher asked me to differentiate the function $$f(x) = \arcsin(\cos x).$$ My work looked like so, $$
f'(x)=\frac{1}{\sqrt{1-\cos^2x}}\cdot-\sin x
= -\frac{\sin x}{\sqrt{\sin^2 x}}
= -\frac{\sin x}{|\sin x|} = -\frac{|\sin x|}{\sin x}.
$$ However, as I went to confirm my answer with my instructor, they claimed that my answer was incorrect. Instead, they claimed my work should have gone like so: $$
f'(x)=\frac{1}{\sqrt{1-\cos^2x}}\cdot-\sin x
= -\frac{\sin x}{\sqrt{\sin^2 x}}
= -\frac{\sin x}{\sin x}
= -1.
$$ I attempted to explain how I thought $\sqrt{x^2}$ simplified to $|x|$ , but they keep asserting I'm incorrect, claiming that the function $\arcsin x$ does not exist for $|x|>\frac{\pi}{2}$ , and that therefore $-1$ should indeed be the correct answer within the domain of the function, which they said was $|x|<\frac{\pi}{2}$ . This has just left my confused about the whole matter. Can anyone explain which derivative is right here, and why?","['calculus', 'derivatives', 'trigonometry']"
4692028,Non-negative least-squares with random variables,"I've been working on a side project for a while that requires a huge amount of simple optimization, and have run into a pretty nasty problem while working on it. If any of you could give some advice I would really appreciate it. Background I have the three following variables: A known MxN matrix A A known length M column vector b An unknown length N column vector x . Furthermore, in my problem they are related by the following expression: $$Ax=b$$ In my project, I need a solution for x that satisfies this expression accurately as possible. By itself, this is a pretty well-studied problem! For instance, I am aware that I can multiply by the Pseudoinverse of A to find a good solution for x . However, in my problem, I also have the constraint that x must be non-negative. Thus, I instead am using a Non-Negative Least Squares Algorithm to solve for x : $$x=NNLS(A,b)$$ So far, this solution has worked pretty well for my application! The Problem Recently, I have run into situations where every variable inside A is a gaussian random variable. Fortunately, I know both the mean and variance of every element in A with pretty high precision. In addition, x is entirely non-random. Under these conditions, the problem slightly changes. I now want to find the optimal solution of x such that the result of the multiplication $Ax$ will have a mean value of b and a low variance in every element. However, I am struggling to figure out how to find the best solution for x with this information. If I just use non-negative least squares to solve for this, I run into some issues. I would imagine that, if column j has many elements with extremely large standard deviations, the jth element of x should be very small. This is because it would not be advantageous to include noisy matrix elements when trying to generate an approximation of b with little variance. However, all versions of the non-negative least squares algorithm, that I have seen, do not consider the possibility of noise or any randomness. This means that they would allow for large variance in b because they would only consider the mean of A ! The Question Is there any way to make a non-negative least squares algorithm consider noise when optimizing for x ? If not, are there alternative optimization methods I could use to solve for x so that the vector b has relatively low variance? Note about my background: I've been told it is useful to responders to know the background of people who ask questions here. I'm an engineer. While I am pretty familiar with mathematics, I am not very familiar with many optimization algorithms, and do not know a massive amount about statistics.","['convex-optimization', 'optimization', 'statistics', 'least-squares']"
4692051,Multiplicative Subsets of the Natural Numbers and Unique Factorization,"Elementary number theory books often give as an example of non-unique factorization the set $S = \{4k+1: k \in \mathbb{N}\}$ . $S$ doesn't have unique factorization because $(3 \cdot{7})(11 \cdot {19}) = (3 \cdot{11})(7 \cdot{19})$ expresses $4389$ as the product of irreducibles (in $S$ ) in two different ways. The set $S$ is a multiplicative set (which I'll define shortly). My question is whether or not there is a necessary and sufficient condition for a multiplicative subset of the natural numbers to have unique factorization. Define a subset of the natural numbers to be multiplicative if (1) $1\in S$ , and (2) If $x \text{ and } y \in S, \text{ then } x \cdot{y} \in S$ . In the following, I'll use the word ""prime"" to mean one of the usual prime numbers. I'll use ""irreducible"" to refer to an element of a specific multiplicative subset of $\mathbb{N}$ that cannot be written as the product of two smaller elements of the set. A multiplicative subset of $\mathbb{N}$ will be said to have unique factorization if all of the elements of the set (other than 1) can be written as the product of irreducible elements in an essentially unique way (all irreducibles appear in the product the same number of times, possibly in a different order). Otherwise, the set will be said to have non-unique factorization. Multiplicative subsets of $\mathbb{N}$ other than the set $S$ defined above may not have unique factorization. For example, let $T = \{n \in \mathbb{N} : n \text{ is composite}\}$ . Then $4$ , $6$ and $9$ are all irreducible in $T$ (since they can't be written as a product of composites). However, $4 \cdot{9} = 6 \cdot{6}$ , expresses $36$ as the product of irreducibles in two different ways. Another such example is the set $\{1, 2^2, 2^3, 2^4, 2^5, \dots\}$ . In this multiplicative set, $2^2 \text{ and }2^3$ are irreducibles, but $2^2 \cdot{2^2} \cdot{2^2} = 2^3 \cdot{2^3}$ expresses $64$ as the product of irreducibles in two different ways. On the other hand, there are plenty of multiplicative subsets of $\mathbb{N}$ that do have unique factorization. For example, if $\mathcal{P}$ is any set of prime numbers (finite or infinite), then the multiplicative set generated by all products of elements from $\mathcal{P}$ has unique factorization. Another example is $\{n \in \mathbb{N}: n \text{ can be written as the sum of two squares of positive integers}\}$ . This set is multiplicative by the identity $(n^2+m^2)(u^2+v^2) = (nu+mv)^2+ (nv-mu)^2$ , Its irreducibles are $\{2\}$ $\cup$ $\{p: p\equiv 1 \pmod4\}$ $\cup$ $\{q^2: q\equiv 3 \pmod4\}$ . I can show that this multiplicative set has unique factorization (in short, the result follows from unique factorization in $\mathbb{N}$ and since no irreducible contains two different prime factors). I realize that these multiplicative sets are monoids and that there are extensive results about factorization in monoids. However, I searched the Internet for results about my question and couldn't find anything. So, the questions stands: Are there any necessary and sufficient conditions for a multiplicative subset of $\mathbb{N}$ to have unique (or non-unique) factorization. Thanks in advance for any advice. Please let me know if I need to clarify any of the above. (Apologies in advance if my { and } in set definitions aren't rendering in Mathjax. I escaped the characters with a \ sign, but they aren't appearing correctly in preview.)","['monoid', 'number-theory', 'abstract-algebra', 'unique-factorization-domains']"
4692113,Minimizing the diagonal product of a special orthogonal matrix,"For any $n > 1$ , define $f: \textrm{SO}(n) \rightarrow \mathbb{R}$ as the diagonal product $$f(A) = \prod_{i=1}^n A_{ii}$$ Based on some numerical experiments, it seems that $$\min_{A \in \textrm{SO}(n)} f(A) = -\left(1-\frac2n\right)^n,$$ and this minimum is attained on matrices $(a_{ij})$ satisfying $|a_{ii}| = 1-2/n$ (with odd number of negative $a_{ii}$ 's, of course) and $|a_{ij}| = 2/n$ for $i \neq j$ . For illustration, here is one of such ""diagonal-product-minimizing"" matrices for the $n=5$ case: $$\frac{1}{5} \left( \begin{array}{rrrrr} -3 & -2 & -2 & -2 & -2 \\ 2 & 3 & -2 & -2 & -2 \\ 2 & -2 & 3 & -2 & -2 \\ 2 & -2 & -2 & 3 & -2 \\ 2 & -2 & -2 & -2 & 3 \end{array} \right)$$ Is that a known (let alone true) result? How can one prove it? I've managed to handle only the trivial $n=2$ case and the somewhat less trivial $n=3$ case using the Euler angle parametrization.","['matrices', 'optimization', 'orthogonal-matrices', 'combinatorial-designs']"
4692225,What is so special about Pythagorean distance? Why squaring?,"It seems that squaring appears not only in classical Euclidean geometry, but also in physics and statistics. Not too long ago, 3b1b released a video on where pi comes from in the normal distribution equation. In short, the assumption of the uncorrelatedness of two parameters (in the case of a two-dimensional distribution) leads to the quadratic power of the variable and only to it, which can then be generalized to any number of dimensions (that is, parameters). Pi is needed for unitarity. It seems to me that this is very similar to the reasoning when deriving a Pythagorean metric for an arbitrary number of dimensions! However, it is not clear where exactly the second degree, and not some other, comes from. It seems that only the Pythagorean metric (and also the Riemannian, but locally it is Pythagorean and flat) is continuous and smooth at any point, any direction, and any scale. R^n can be shifted to any distance and rotated in any plane to any angle without losing all of the above properties. Why squaring? Is there any derivation of this metric ab initio (and, importantly, without any circularities in arguments)?","['euclidean-geometry', 'soft-question', 'geometry', 'metric-spaces']"
4692230,Calculate $\lim_{n \to \infty} \sqrt{n^4+an^3+2n^2}-n^2$ for $a \ge 0$,"I've tried to simplify $\sqrt{n^4+an^3+2n^2}-n^2$ the following way: \begin{align}
        &\sqrt{n^4+an^3+2n^2}-n^2\\\\
        &\frac{(\sqrt{n^4+an^3+2n^2}-n^2)(\sqrt{n^4+an^3+2n^2}+n^2)}{(\sqrt{n^4+an^3+2n^2}+n^2)}\\\\
        &\frac{n^4+an^3+2n^2-n^4}{(\sqrt{n^4+an^3+2n^2}+n^2)}\\\\
        &\frac{an^3+2n^2}{(\sqrt{n^4+an^3+2n^2}+n^2)}\\\\
    \end{align} But now I'm stuck.","['limits', 'real-analysis']"
4692259,Semi-orthogonal decomposition of derived category of Calabi-Yau manifolds?,"Let $X$ is a smooth projective variety over some field with trivial canonical bundle $\omega_X\cong\mathscr{O}_X$ . In the book Fourier-Mukai transforms of AG by D. Huybrechts, he claim that there are no non-trivial semi-orthogonal decomposition of $D^b(X)$ (Exercise 8.8). Why? We may use the Serre duality? How?","['homological-algebra', 'algebraic-geometry', 'derived-categories']"
4692272,Evaluate a Dilogarithmic integral $\int_0^1 \ln(x)\operatorname{Li}_2(x)dx$,"I believe a closed form for this integral exists since WFA is able to compute the antiderivative . It does not come out with a closed form when I give it bounds on the integral for some reason though. This integral is tricky. I tried Feynman's Trick by paramterising with $$I(a)=\int_0^1 \ln(ax)\operatorname{Li}_2(x)dx$$ however, $a=0$ does not give a value so this cannot be used. It would be very useful though since after differentiating we get $\frac{\operatorname{Li}_2(x)}{ax}$ as the integrand which is just $\operatorname{Li}_3(x) a^{-1}$ but of course this is not possible because of the $a=0$ problem. Are there any other ways to approach this? I think integration by parts may work but I'm not sure how to deal with the integrals that result from it.","['integration', 'definite-integrals']"
4692285,Airport queue possibilities,"Suppose $k$ people de-board an airplane and get into a hall where they are assigned at most $n$ queues. The number of ways in which this can be done is $k! n^k$ or $n(n+1)\cdots(n+k-1)$ ? From the discussion here , the second answer makes sense, but to me, the first one does. Here's how I am arriving at the first answer: Suppose the people come down the airstair in one of the $k!$ ways; let that order be $p_1 \to p_2 \to \cdots \to p_n$ where $p_i$ is the $i$ th person to de-board the plane. Now each $p_i$ has $n$ options (queues) to choose; he either goes to one of the empty queues (if one exists) or stands behinds someone. In total, are $k! n^k$ ways. The other answer also seems to be correct. I think both the answers are correct; the only difference is in the assumption of people being similar objects (according to that answer) and dissimilar objects (according to my solution). Is my understanding correct?","['permutations', 'solution-verification', 'combinatorics']"
4692319,A variant generalization of Newton's Line,"Given a pair of inversely similar triangles ABC and AB'C', as shown in the graph, CB and C'B' intersect at F.  X, Y, Z are the midpoints of BB', CC', and AF respectively. Prove that X, Y, Z are co-linear. This is a very elegant result, and somewhat alike Newton's line. All proofs welcome, and a geometrical solution would be very nice!","['projective-geometry', 'geometry']"
4692323,What is the surface integral of hemisphere?,"Calculate the integral $∫ ∫_{Y}z dS$ if Y is the part of the conic
surface $z = \sqrt{x^2 + y^2}$ where z is between 0 and 1. So this is what I did: $z = \sqrt{4-x^2-y^2}$ and then I used spehere's coordinates and got z to $z=2\sqrt{1-\sin^2(\theta)}$ and my area elemnent or whatever it is called is given by $r^2\sin(\theta)$ and then I multiplied this by z and parted the integrals. I for some reasons think that I don't need to take r as the variable(I saw a similar quation where someone just took r as a constant, don't understand why and if I can do that here). This means my variables are theta and phi and the bounds are $0\leq \theta \leq \frac{\pi}2$ and $0\leq \phi \leq 2\pi$ . First of all I don't know if I have done right so far so correct me if I'm wrong but otherwise how in the hell am I supposed integrate $\int 2\sqrt{1-\sin^2(\theta)} = 2 \int \sqrt{\frac12 + \cos(2\theta)}$ EDIT: I accidentally posted the wrong question, this was the question my ""solution"" was for: Evaluate the surface integral $\int\int_{Y}zdS$ when Y is the upper half of the sphere $x^2+y^2+z^2=4$ , i.e. the part where z ≥ 0.","['multivariable-calculus', 'calculus', 'surface-integrals']"
4692360,How to compute the sum $\cot^2\left(\frac{\pi}{9}\right)+\cot^2\left(\frac{2\pi}{9}\right)+\cot^2\left(\frac{4\pi}{9}\right)=~?$,How to compute the sum of $\cot^2\left(\frac{\pi}{9}\right)+\cot^2\left(\frac{2\pi}{9}\right)+\cot^2\left(\frac{4\pi}{9}\right)=~?$ The answer is $9$ . I tried to use the formula $\cot (2\theta)=\dfrac{\cot^2\theta-1}{2\cot\theta}$ but it is getting more and more complicated.,['trigonometry']
4692362,Limit of products of differences of logarithms,"My first post here, hope doing everything the right way. I'm struggling with the following limit: $$\lim_{x\to+\infty}\ln(3^x-x)\left[\ln(x^4+1)-2\ln(x^2-x)\right]$$ First of all I used logarithm properties to transform the function in the form $$\ln(3^x-x)\ln\bigg(\frac{x^4+1}{x^4-2x^3+x^2}\bigg)$$ Here I noticed that the first logarithm goes to infinity for $x$ that goes to infinity
and the second goes to zero because the argument goes to $1$ for $x$ to infinity.
So it is an undefined form $0\cdot\infty$ . Next I tried to use algebraic transformations and known limits but I can't find anything useful to solve the limit because every try ends up in another undefined form. Any ideas? Thanks :)","['limits', 'calculus', 'real-analysis']"
4692386,About global and local extrema for this function,"I have the function $$f(x, y) = \ln(1 + xy) - \frac{1}{3}(x^2+y^2)$$ I found its critical points, one saddle and two maxima, the latter of which are $$A = (1/\sqrt{2}, 1/\sqrt{2}) \qquad \qquad B = -A$$ How can I say that those points are global maxima and not just local, without plotting the function? I thought about analysing what happens when $(x, y) \to (\pm \infty,\ \pm\infty)$ but there are times in which it's a mess. In this case for example, I thought about studying what happens along the straight lines $x = y$ , getting $$f(x) = \ln(1+x^2) - \frac{2}{3}x^2 \sim 2\ln|x| - x^2$$ as $x \to \pm \infty$ , and this either cases goes to $-\infty$ . Hence there is an upper limit and my points are global max. Is this reasoning correct? Or is it wrong and why? Are there better / ""righter"" ways to proceed in general, when dealing with functions in two variables in the case of a non compact domain?","['limits', 'multivariable-calculus', 'maxima-minima', 'optimization']"
4692398,Find $\frac{\sum_\limits{k=0}^{6}\csc^2\left(x+\frac{k\pi}{7}\right)}{7\csc^2(7x)}$,Find the value of $\dfrac{\sum_\limits{k=0}^{6}\csc^2\left(x+\dfrac{k\pi}{7}\right)}{7\csc^2(7x)}$ when $x=\dfrac{\pi}{8}$. The Hint given is: $n\cot nx=\sum_\limits{k=0}^{n-1}\cot\left(x+\dfrac{k\pi}{n}\right)$ I dont know how it comes nor how to use it,"['trigonometric-series', 'complex-numbers']"
4692415,Where does $(\tan x)^{\sin x}=(\cot x)^{\cos x}$?,"So I was yet again searching throughout the homepage of Youtube to see if there were any math equations that I thought that I might be able to solve when I came across this video by SyberMath that asked the question: $$\text{Solve: }~~~(\tan x)^{\sin x}=(\cot x)^{\cos x}$$ Which I thought that I might be able to solve. Here is my attempt at solving it: $$(\tan x)^{\sin x}=(\cot x)^{\cos x}$$ $$\sin(x)\ln(\tan x)=\cos(x)\ln(\cot x)$$ $$\tan(x)\ln(\tan x)=\ln(\cot x)~~~~~~~\text{ (since }\frac{\sin(x)}{\cos(x)}=\tan(x)\text{)}$$ $$\text{Therefore, we need to find where }(\tan x)^{\tan x}\text{ is equal to }\cot(x)$$ $$\text{And solving for }x\text{ will get us that }x=\frac{\pi}{4}+\pi n,n\in\mathbb{Z}\text{, (}\mathbb{Z}\text{ being the set of integers)}$$ And, plugging this into Desmos we see that we are correct! $$\mathbf{\text{My question}}$$ Is my solution correct, or is there anything that I could do to attain the correct solution more easily?","['algebra-precalculus', 'solution-verification']"
4692419,Why $\cos\frac{\pi}{9}=-\frac{1}{2}(-1)^{\frac{8}{9}}(1+(-1)^{\frac{2}{9}})$?,"If you type in Wolfram Alpha $\cos\frac{\pi}{9}$ this is what you get: $$\cos\frac{\pi}{9}=-\frac{1}{2}(-1)^{\frac{8}{9}}(1+(-1)^{\frac{2}{9}})$$ I have no idea how to derive this, maybe it is trivial but I can't see it. Maybe it follows from the fact that $$\cos\frac{\pi}{9}=\Re\ e^{i\frac{\pi}{9}}=\Re\ (e^{i\pi})^{\frac{1}{9}}=\Re\ (-1)^\frac{1}{9}$$ but I couldn't continue from here. By the way, should this be considered a ""closed form""? In my opinion, yes. Also, is it possible to get a similar result for every angle?","['calculus', 'trigonometry']"
4692426,Are Hyperbolic Angles Smaller than Euclidean ones in 'Congruent' Triangles?,"Assume you have a triangle with vertices $A,B,C\in\mathbb H^2$ in the hyperbolic plane and the hyperbolic distances are $a=d(B,C), b=d(A,C), c=d(A,B)$ . Now pick a `comparison triangle' $A',B',C'\in\mathbb R^2$ with the same side-lengths $a,b,c$ as our hyperbolic triangle. Is it true that each angle $\alpha=\angle_{CAB},\beta=\angle_{ABC},\gamma=\angle_{BCA}$ in the hyperbolic triangle is bounded above by the corresponding angle in the Euclidean triangle, i.e. $\alpha\leq\alpha',\beta\leq\beta',\gamma\leq\gamma'$ ? I tried the approach of comparing the cosines of the angles via the respective laws of cosines: $$\cos\alpha'=\frac{a^2+b^2-c^2}{2ab}\overset?\leq\frac{\cosh a\cosh b-\cosh c}{\sinh a\sinh b}=\cos\alpha,$$ however, I was not able to prove this inequality yet.","['metric-geometry', 'trigonometry']"
4692449,Is there such a formula in general topology,"I can prove the following formula in general topology: $$
(A\cup B)^o\subset (A^o\cup \overline{B})\tag1
$$ This formula is useful in proving many results involving interiors (and closures) because it provides an upper bound for $(A\cup B)^o$ . But after checking several books on general topology including Topology by Kuratowski and Munkres, and General Topology by Bourbaki, I can not find it. So I wonder if this formula already exists or not. Also, I'd like to know if there is a handbook in general topology. Edit: In his Topology ((15) on p56, Vol1), Kuratowski proved the following formula indirectly through local topology. $$
\overline{\overset{\circ}{\overline{A\cup B}}}\,=\,\overline{\overset{\circ}{\overline{A}}}\cup\overline{\overset{\circ}{\overline{B}}}\tag2
$$ However, (2) can be proved easily by (1) as follows. $$
\overset{\circ}{\overline{A\cup B}}\,=\left(\left(\overline{A}\cup\overline{B}\right)^o\right)^o\subset \left(\overset{\circ}{\overline{A}}\cup\overline{B}\right)^o\subset \overline{\overset{\circ}{\overline{A}}}\cup\overset{\circ}{\overline{B}}\,\subset \overline{\overset{\circ}{\overline{A}}}\cup\overline{\overset{\circ}{\overline{B}}}
$$ Since the right is closed $$
\overline{\overset{\circ}{\overline{A\cup B}}}\,\subset\,\overline{\overset{\circ}{\overline{A}}}\cup\overline{\overset{\circ}{\overline{B}}}
$$ The other part is obvious. So I think Kuratowski might not know this and (1) may be new.","['general-topology', 'reference-request']"
4692487,A graph-indexed family of subdirect products,"I've been noodling around with a family of subdirect products recently, and it seems elementary enough that I suspect there might be something written about them already. Here is the idea. Let $\Gamma$ be a simple graph on the set $\{1, \ldots, n\}$ and let $G$ be a group. We will define a subgroup of $G^n$ . For each $g \in G$ and $i \in \{1, \ldots, n\}$ we define $g_i$ to be the element of $G^n$ with a $g$ in position $i$ , and also in every position $j$ such that $j$ is adjacent to $i$ . Then we define $G^\Gamma$ to be the subgroup of $G^n$ generated by all $g_i$ . For example, suppose that $\Gamma$ is a path of length 2, with edges connecting 1 to 2 and 2 to 3. Then $G^\Gamma$ is generated by elements of the form $(g, g, 1), (g, g, g)$ , and $(1, g, g)$ . It's not hard to show that in this case we get all of $G^3$ . For another simple example, if $\Gamma$ is a complete graph, then $G^\Gamma$ is the diagonal subgroup of $G^n$ . Has anyone seen this family of subdirect products before?","['graph-theory', 'group-theory', 'products']"
4692507,Calculate the following limit: $\lim\limits_{x \to 0^+}\!\big((1+x)^x-1\big)^x $ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This limit is from a college admission exam in Cluj, Romania. I've tried writing the limit as $\,e^{g\cdot\ln f}$ , so $\,e^{\lim\limits_{x \to 0^+}x\cdot\ln\left((1+x)^x-1\right)}$ , but then I have no idea how to write $(1+x)^x$ .",['limits']
4692509,Find the radius $r$ of the semicircumference below,"In the figure, if $PF=3$ , $OP=1$ , calculate "" $r$ "". Answer by plan geometry I try $FM \cdot FN = FA\cdot FB$ $FA =4 - r$ $\therefore (4-r)\cdot(FB) = FM\cdot FN \implies (4-r)\cdot(4-r+2r) = FM\cdot FN$ $\therefore 16-r^2=FM\cdot FN$","['euclidean-geometry', 'geometry', 'plane-geometry']"
4692525,How to find a combinatorial proof for the following sum? [duplicate],"This question already has an answer here : Combinatorial Proof of a Binomial Coefficient Identity (1 answer) Closed 12 months ago . I'm trying to find a combinatorial proof for the following sum: $$\sum_{k=0}^{n} 2^{2n-2k} \binom{2n}{2k} \binom{2k}{k} = \binom{4n}{2n}$$ I tried to ask and answer this question from both sides:
How many binary numbers of length 4n have exactly 2n zeros? However, I'm having trouble answering this question for the left side.
I would appreciate any suggestion!","['combinations', 'combinatorics', 'combinatorial-proofs']"
4692575,Possible approaches to prove $\frac {1}{a}+\frac {1}{b}+\frac {1}{c}+\frac {6}{a+b+c}\geq 5$,"I am looking for various approaches to prove that the following inequality is true. Let $a,b,c>0$ with $abc =1$ , prove that $$\frac {1}{a}+\frac {1}{b}+\frac {1}{c}+\frac {6}{a+b+c}\geq 5$$ My first approach. C-S inequality didn't work . $$\bigg(\frac {1}{a}+\frac {1}{b}+\frac {1}{c}\bigg)(a+b+c )\geq 9$$ $$\frac {9}{a+b+c}+\frac {6}{a+b+c}\geq 5$$ $$a+b+c≤3$$ But, by Am-Gm $a+b+c\geq 3\sqrt [3]{abc}=3$ . Next approach. Am-Gm inequality didn't work . $$\frac {1}{a}+\frac {1}{b}+\frac {1}{c}\geq 3\sqrt[3]{\frac {1}{abc}}=3$$ But, $$3+\frac {6}{a+b+c}\geq 5$$ is not necessarily true. Please share us, possible different and alternative approaches.","['algebra-precalculus', 'inequality']"
4692628,Is $\{x \in \mathbb{Z} \mid x = 6a + 4 \}$ a subset of $\{y \in \mathbb{Z} \mid y = 18b - 2\}$?,"Here is the question: Let $A = \{x \in \mathbb{Z} \mid x = 6a + 4, \text{for some integer } a\}$ And let $B = \{y \in \mathbb{Z} \mid y = 18b - 2, \text{for some integer } b\}$ Prove or disprove the claim that $A \subseteq B$ . So far, I've managed to do the following: $6a + 4 = 18b - 2$ by substitution $6a + 6 = 18b$ $a+1 = 3b$ $b = \frac{a+1}{3}$ by algebra The solution I read says $b$ is not an integer since $b = \frac{a+1}{3}$ is not an integer and therefore $A \not\subseteq B$ . However, for $a = 2, b = 3$ . So there is some integer $a$ and $b$ then and therefore $A \subseteq B$ ?","['elementary-set-theory', 'proof-writing', 'examples-counterexamples', 'discrete-mathematics']"
4692673,When is the compact-open topology compact?,"Let $X,Y$ be topological spaces. Let $\mathcal{C}(X,Y)$ be the space of continuous functions from $X$ to $Y$ equipped with the compact open topology. What conditions do we need on $X$ and $Y$ to ensure that $\mathcal{C}(X,Y)$ is compact? What I'm hoping is that if both $X$ and $Y$ are compact Hausdorff then these are sufficient conditions, but I'm unable to prove this. Perhaps there is an obvious counterexample to this that I'm missing? EDIT : I found a counterexample, but maybe it can still work. Let $X = Y = S^1$ . Then $\mathcal{C}(S^1, S^1)$ has infinitely many path components and so cannot be compact. But is each path component itself compact? That would still be useful to me.","['general-topology', 'compactness']"
4692680,Is their any relation between stiefel-whitney class and obstruction to lift SO(3) to SU(2)?,"I found a paper by kirby and taylor https://www.maths.ed.ac.uk/~v1ranick/papers/kirbytaylorpin.pdf . It is said that the obstruction to lift O(n) bundle to $pin(n)_-$ bundle is some stiefel-whitney class. So, is there any similar conclusion for lift SO(n) to spin(n).","['algebraic-topology', 'differential-geometry']"
4692765,"(Complex Stieltjes Integral) If $f$ is integrable wrt $\alpha$, is $\overline{f}$ also integrable wrt $\alpha$?","Let $f,\alpha$ be two bounded complex functions on $[0,1]$ . We say that $f$ is integrable w.r.t. $\alpha$ iff the Riemann sum $$\sum f(t_i)(\alpha(x_i)-\alpha(x_{i-1}))$$ converges to a fixed number $I\in\mathbb{C}$ as the partition $P=\left\{0=x_0<x_1<\cdots<x_n=1\right\}$ gets finer. When this is the case, we write $f\in\mathcal{R}(\alpha)$ and that $\int f\,d\alpha=I$ . It is known that when $\alpha$ is of bounded variation, $\mathcal{R}(\alpha)$ contains all continuous functions. I have two questions related to this. Q1. If $f\in\mathcal{R}(\alpha)$ , do we have $\overline{f}\in\mathcal{R}(\alpha)$ too? Q2. Does the answer change if we further assume that $\alpha$ is of bounded variation? There is an example of a pair of sequences $(a_n)$ and $(b_n)$ such that $\sum a_n b_n$ converges while $\sum \overline{a_n} b_n$ diverges. From this I get a feeling that the answers to my questions are both negative, but I could not find counterexamples by myself.","['measure-theory', 'stieltjes-integral', 'real-analysis', 'sequences-and-series', 'riemann-integration']"
4692807,$(|x|=|y|\implies x=y)$ while solving $\frac{\mathrm dy}{\mathrm dx}=\frac{y}{1+x}$,"G Zill, Dennis, Differential Equations with Boundary-Value Problems, 7th edition, p. 46: Example 1: Solve $(1+x)\mathrm dy-y\mathrm dx=0.$ $$\frac{\mathrm dy}{\mathrm dx}=\frac{y}{1+x}\\\cdots\\\cdots$$ From $$\ln|y|=\ln|c(1+x)| \tag{1}$$ we immediately get $$y=c(1+x)\tag{2}$$ Given that $x,y\in\mathbb{R}$ and $|x|=|y|,$ I don't think we can say that $x=y.$ Why did the author go from $(1)$ to $(2)$ ? Is that step mathematically rigorous? EDIT Follow-up question: How to solve for $y$ in $(1+x)dy-ydx=0$?","['calculus', 'absolute-value', 'ordinary-differential-equations']"
4692831,What is the shape of the perfect coffee cup for heat retention assuming coffee is being drunk at a constant rate?,"Find the optimal shape of a coffee cup for heat retention. Assuming A constant coffee flow rate out of the cup. All surfaces radiate heat equally, i.e. liquid surface, bottom of cup and sides of cup. The coffee is drunk quickly enough that the temperature differential between the coffee and the environment can be ignored/assumed constant. So we just need to minimise the average surface area as the liquid drains I have worked out the following 2 alternative equations for the average surface area over the lifetime of the liquid in the cup (see below for derivations): $$ S_{ave} =\pi r_0^2+ \frac{\pi^2}{V}\int_{0}^{h}{{r(s)}^4ds}+\frac{2\pi^2}{V}\int_{0}^{h}{\int_{0}^{s}{r\sqrt{1+\left(\frac{dr}{dz}\right)^2}\ dz\ }{r(s)}^2ds\ } \tag{1}$$ $$S_{ave}=\pi r_0^2+\frac{\pi^2}{V}\int_{0}^{h}r\left(s\right)^4ds+\frac{2\pi^2}{V}\int_{0}^{h}r(s){\underbrace{\int_{s}^{h}{{r\left(z\right)}^2dz\ }}_{\text{Volume Drunk}}}\sqrt{1+\left(\frac{dr}{ds}\right)^2}ds \tag{2}$$ If the volume of the cup is constant $$ V=\pi\int_{0}^{h}{{r(z)}^2dz\ }$$ Can the function, $r(z)$ , be found that minimises the average surface area $S_{ave}$ ? If r is expressed as a parametric equation in the form $r=f(t), z=g(t)$ and $f,g$ are polynomials then a genetic search found the best function of parametric polynomials to be: $r\left(z\right)=\sqrt{\frac{3}{2}}z^\frac{1}{2}-\frac{\sqrt6}{9}z^\frac{3}{2}, f\left(t\right)=\sqrt{\frac{3}{2}}t-\sqrt{\frac{3}{2}}t^3, g\left(t\right)=\frac{9}{2}t^2$ This parametric shape has a maximum radius of 1, height of 4.5, starting volume of $\frac{3^4}{2^5}\pi$ and is shown here: I can't prove that there is (or is not) a better $r(z)$ but... the average surface area of this surface turns out to be $12.723452r^2$ or $4.05\pi r_{max}^2$ .
I suspect that the optimal surface will have the same surface area as a sphere, i.e. $4\pi r_{max}^2$ $(12.5664)$ Conjecture: The optimally shaped coffee cup has the same average surface area as a sphere of the same maximum radius. Shown to be false by this answer Derivation of Surface Area Formula: Surface area when surface of liquid is at level s is the sum of the areas of the top disc, bottom disc and the sides. $S(s)=\pi r_0^2+\pi r_s^2+2\pi\int_{0}^{s}{r(z)dldz}$ $S(s)=\pi r_0^2+\pi r_s^2+2\pi\int_{0}^{s}{r(z)\sqrt{1+\left(\frac{dr}{dz}\right)^2}\ dz\ }$ The average surface area will be the sum of all the As’s times the time spent at each surface area. $S_{ave}=\frac{1}{T}\int_{t_0}^{t_h}{S(s)dt\ }$ In order to have the drain rate constant we need to set the flow rate Q to be constant i.e. the rate of change volume is constant and $Q=dV/dt =V/T$ Time spent at a particular liquid level $dt\ =\frac{T}{V}dV$ and $
dV={\pi r}^2ds$ $dt=\frac{T\pi r^2}{V}ds$ $S_{ave}=\int_{s=0}^{s=h}{S(s)\frac{T\pi{r(s)}^2}{V}ds\ }$ $S_{ave}=\frac{\pi}{V}\int_{s=0}^{s=h}{(r_0^2+r(s)^2+2\int_{z=0}^{z=s}{r(z)\sqrt{1+(\frac{dr(z)}{dz})^2}\ dz\ })\pi{r(s)}^2ds\ }$ $S_{ave}=\frac{\pi}{V}r_0^2\int_{0}^{h}{\pi{r(s)}^2ds}+\frac{\pi}{V}\int_{s=0}^{s=h}{\left(r\left(s\right)^2+2\int_{z=0}^{z=s}{r(z)\sqrt{1+(\frac{dr(z)}{dz})^2}\ dz\ }\right)\pi{r(s)}^2ds\ }$ $S_{ave}=\pi\ r_0^2+\frac{\pi^2}{V}\int_{s=0}^{s=h}{\left(r\left(s\right)^2+2\int_{z=0}^{z=s}{r(z)\sqrt{1+\left(\frac{dr(z)}{dz}\right)^2}\ dz\ }\right){r(s)}^2ds\ }$ Alternative Formula Derivation: Surface area of highlighted ribbon in the diagram is: $S_{ribbon}=2\pi rdl$ And the contribution towards the average surface area lasts for the ratio of volume of the liquid above the current level to the total volume. $$S_{sides}=2\pi\frac{\pi}{V}{\underbrace{\int_{s}^{h}{{r\left(z\right)}^2dz\ }}_{\text{Volume Drunk}}}rdl=\frac{2\pi^2}{V}{\underbrace{\int_{s}^{h}{{r\left(z\right)}^2dz\ }}_{\text{Volume Drunk}}}r\left(s\right)\sqrt{1+\left(\frac{dr}{ds}\right)^2}ds$$ Integrate the contribution of all such sections. $$S_{sides}=\frac{2\pi^2}{V}\int_{0}^{h}{r\left(s\right){\underbrace{\int_{s}^{h}{{r\left(z\right)}^2dz\ }}_{\text{Volume Drunk}}}\sqrt{1+\left(\frac{dr}{ds}\right)^2}ds}$$ Contribution of top surfaces to average surface area is area of top by proportion of volume that area x dz is: $$S_{tops}=\frac{1}{V}\int_{0}^{h}{\pi{r(s)}^2{\pi r(s)}^2}ds$$ Contribution of bottom surface is constant $\pi r_0^2$ so adding together all three gives: $$S_{ave}=\pi r_0^2+\frac{\pi^2}{V}\int_{0}^{h}r\left(s\right)^4ds+\frac{2\pi^2}{V}\int_{0}^{h}r(s){\underbrace{\int_{s}^{h}{{r\left(z\right)}^2dz\ }}_{\text{Volume Drunk}}}\sqrt{1+\left(\frac{dr}{ds}\right)^2}ds$$","['ordinary-differential-equations', '3d', 'geometry', 'maxima-minima', 'optimization']"
4692869,Polynomial in two variables being constantly 0,"I am trying to prove that a polynomial in two variables is constantly zero. In an infinite field $k$ Suppose $f\in k[x,y]$ has as roots the following set $k^2\setminus \{(x,0):x\in k\}$ . Then for every $a\in k$ the polynomial $f(a,.)\in k[y]$ has infinite amount of roots $\implies f(a,.)\in k[y]$ is the zero polynomial. Let $b\in k\setminus \{0\}$ , then $f(.,b)\in k[x]$ has infinite amount of roots $\implies f(.,b)\in k[x]$ is the zero polynomial. My question is it possible to conclude that $f\in k[x,y]$ is the zero polynomial? If not can someone give me a tip to steer me into the right direction.","['algebraic-geometry', 'solution-verification', 'polynomials']"
4692885,Find range of $f(\sqrt 2)$ for $f(x)=e^{\frac{x^2}{2}}+\int_0^x tf(t)dt$,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function satisfying $$f(x)=e^{\frac{x^2}{2}}+\int_0^x tf(t)\mathrm{d}t$$ Then which of the following is correct: (A) $5<f(\sqrt 2)<6$ (B) $2<f(\sqrt 2)<3$ (C) $3<f(\sqrt 2)<4$ (D) $4<f(\sqrt 2)<5$ My attempts so far: Taking derivative of $f(x)$ we get $$f'(x)=x.e^{\frac{x^2}{2}}+x.f(x)$$ $$\frac{f'(x)}{x}-e^{\frac{x^2}{2}}=f(x)$$ On solving the differential equation we get $$f(x)=\left(C+\frac{x^2}{2}\right)e^{\frac{x^2}{2}}$$ And putting $x=\sqrt 2$ we get $f(\sqrt 2)=(C+1)e$ but I can't figure out the value of $C$ . Other attempts include writing $f(x)$ in terms of its derivative like above and putting it inside the integral, but upon solving it simply yields $f(x)$ . I also tried finding $f(2)$ and $f(1)$ as $\sqrt 2$ lies between 1 and 2 and tried finding their difference to see if it yields any insight. $$f(1)= \sqrt e + \int_0^1tf(t)dt $$ $$f(2)= e^2 + \int_0^2tf(t)dt $$ $$ e^2 +\int_0^1tf(t)dt+ \int_1^2tf(t)dt= e^2 +\int_0^1tf(t)dt+ \int_0^1(t-1)f(t-1)dt$$ Subtracting both these equations yields: $$f(2)-f(1)=e^2- \sqrt e + \int_0^1 (t-1)f(t-1)$$ I am at a loss on how to proceed further with this problem, any hints/ insights would be really appreciated. Edit: From comment of @Annebauval $f(0)=C=1$ and putting it in solution of differential equation we get $f(\sqrt 2)=2e \approx 5.43\,$ hence the correct answer is (A) option.","['integration', 'ordinary-differential-equations']"
4692897,Showing $\displaystyle{\zeta(3)=\frac{1}{7}\sum_{n=1}^\infty\frac{n(2n+1)\zeta(2n+1)}{2^{2n-2}}}$,"I want to show the following infinite series representation for $\zeta(3)$ which I stumbled across on accident. $$\zeta(3)=\frac{1}{7}\sum_{n=1}^\infty\frac{n(2n+1)\zeta(2n+1)}{2^{2n-2}}$$ Here is my attempt so far. By the series representation of $\zeta(2n+1)$ , $$\begin{align*}
\sum_{n=1}^\infty\frac{n(2n+1)\zeta(2n+1)}{2^{2n-2}}&=\sum_{n=1}^\infty\frac{n(2n+1)}{2^{2n-2}}\sum_{k=1}^{\infty}\frac{1}{k^{2n+1}} \\
&=\sum_{k=1}^{\infty}\sum_{n=1}^{\infty}\frac{n(2n+1)}{2^{2n-2}k^{2n+1}}
\end{align*}$$ and I am stuck. Evaluating the second sum using wolfram gives me, $$\sum_{n=1}^{\infty}\frac{n(2n+1)}{2^{2n-2}k^{2n+1}}=\frac{16k(12k^2+1)}{(4k^2-1)^3}$$ for $\vert k\vert>1/2$ , turning the problem into finding the infinite sum of some polynomial fraction, which I still can't evaluate. Looking at the wiki for $\zeta(3)$ I see Euler gave the series representation, $$\zeta(3)=\frac{\pi^2}{7}\left(1-4\sum_{k=1}^\infty\frac{\zeta(2k)}{2^{2k}(2k+1)(2k+2)}\right)$$ which some what resembles mine, so I'm guessing there's some trick I don't know of. Any help? Thanks in advance.","['riemann-zeta', 'special-functions', 'sequences-and-series']"
4692944,Help to understand a proof of $\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1$,"Context I am reading the textbook Calculus With Applications, by Peter D. Lax and having a problem  understanding the proof of $\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1$ , the result of which is then used to prove the derivative of $e^x$ . The textbook first uses the inequalities $$
\left(1+\frac{1}{n}\right)^n<e<\left(1+\frac{1}{n}\right)^{n+1}<\left(1+\frac{1}{n-1}\right)^n \tag{1}\label{eq1}
$$ for all integers $n>1$ . Then it takes $h=\frac{1}{n}$ as a special sequence of $h$ tending to zero. After a few derivations of the inequality, it get $$
1\le\frac{e^h-1}{h}\le\frac{n}{n-1}
$$ As n tends to infinity, the right-hand term tends to 1, so by the squeeze theorem, the center term tends to 1 also, which means the derivative of $e^x$ at $x=0$ is equal to 1. However , as it says, this does not quite finish the proof, since it has taken $h$ to be of the special form $h=\frac{1}{n}$ . So it leaves a problem to fill this gap: Complete problem description Recall from Eq. $\eqref{eq1}$ that e is between the increasing sequence $e_n$ and the decreasing sequence $f_n$ . Explain the following items. (a) If $h>0$ is not of the form $\frac{1}{n}$ , then there is an integer $n$ for which $\frac{1}{n}<h<\frac{1}{n-1}$ . (b) Using Eq. $\eqref{eq1}$ , one has $\left(1+\frac{1}{n}\right)^{nh}<e^h<\left(1+\frac{1}{n-2}\right)^{\left(n-1\right)h}$ . (c) $\left(n-1\right)h<1<nh$ (d) $1+\frac{1}{n}<e^h<1+\frac{1}{n-2}$ (e) $n-1<\frac{1}{h}<n$ , and $\frac{n-1}{n}<\frac{e^h-1}{h}<\frac{n}{n-2}$ . Conclude from this that $\frac{e^h-1}{h}$ tends to $1$ as $h$ tends to zero. My progress I have two main questions, the first one is how to prove (a) and the second one is how the final conclusion is obtained from (e). And I have proved the intermediate (b), (c), (d) and (e) process.","['calculus', 'derivatives', 'exponential-function']"
4692947,How to solve for $y$ in $(1+x)dy-ydx=0$?,"$$(1+x)dy-ydx=0$$ My attempt: $$\frac{dy}{dx}=\frac{y}{1+x}\tag{*}$$ $$\frac{1}{y}\frac{dy}{dx}=\frac{1}{1+x}$$ $$\int \frac{dy}{y}=\int \frac{dx}{1+x}$$ $$\ln|y|+c_1=\ln|1+x|+c_2$$ $$\ln|y|=\ln|1+x|+c \ \ [\text{Let}\ c_2-c_1=c]$$ $$e^{\ln|y|}=e^{\ln|1+x|+c}$$ $$|y|=|1+x|e^c\tag{1}$$ So far so good. Now, the problems will begin. $$y=|1+x|e^c\tag{2}$$ We know, $$|1+x|=\begin{cases} (1+x),\ x\geq-1\\ -(1+x),\ x<-1 \end{cases}$$ Therefore, $$y=\begin{cases} (1+x)e^c,\ x\geq-1\\ -(1+x)e^c,\ x<-1 \end{cases}\tag{3}$$ This is my final answer. Now, I have two problems. First problem: My solution is not correct according to @geetha290krm . ""No. $y$ cannot change sign at $−1$ . $|1+x|$ is not differentiable at $−1$ . You can only have $y=(1+x)e^c$ for all $x$ or $y=−(1+x)e^c$ for all $x$ ."" According to them, $y$ cannot change sign at $x=-1$ , because then it would become undifferentiable at $x=-1$ . Now, I want to make a case for myself as to why I think $(3)$ is correct. (I'm not trying to be arrogant; I just want to spell out my agonies so that you may correct me easily): See $(1)$ . It says/defines what $y$ is . I just expanded $(1)$ , and drove it to its logical conclusion. I did not add or remove anything to $(1)$ to reach $(3)$ . I just carried $(1)$ to its logical conclusion. So, if $(1)$ is true, then $(3)$ must be true. For example, if $y=x^2-5x+6$ is true, then $y=(x-2)(x-3)$ must also be true. Another point raised by geetha290krm is that $y$ becomes undifferentiable at $x=-1$ according to my solution. However, see $(*)$ closely. Input $x=-1$ in $(*)$ . We get $\frac{dy}{dx}=\frac{y}{0}=\text{undefined}$ . So, the given problem was never differentiable at $x=-1$ to begin with. So, $(3)$ is consistent with this information (i.e. consistent with $y$ being undifferentiable at $x=-1$ ). These are the reasons why I think $(3)$ is the correct answer. Also, the answer provided by geetha does not make sense to me. According to him, ""You can only have $y=(1+x)e^c$ for all $x$ or $y=−(1+x)e^c$ for all $x$ "", but $y=(1+x)e^c$ cannot be true for all $x$ . It can only be true for $x\geq-1$ . Similarly, $y=-(1+x)e^c$ cannot be true for all $x$ . We would be distorting $y$ then. $y=-(1+x)e^c$ can only be true for $x<-1$ as defined by $(1)$ . If you agree with me that $(1)$ is true, then you must agree with me that $(3)$ is also true because $(1)$ and $(3)$ are the same thing. They are only two sides of the same coin. Second problem: This issue is separate from the issue I just described. Is going from $(1)$ to $(2)$ valid? Aren't we incorrectly saying that $|y|=y$ by saying that going from $(1)$ to $(2)$ is valid? It should be $|y|=\begin{cases} y,\ y\geq0\\ -y,\ y<0 \end{cases}$ instead of $|y|=y$ , shouldn't it? My book's given solution EDIT It now seems to me that $(2)$ is incorrect; so, $(3)$ should be: $$|y|=\begin{cases} -(1+x)e^c, &x<-1\\(1+x)e^c, &x\geq-1. \end{cases}$$","['calculus', 'absolute-value', 'ordinary-differential-equations']"
4692992,Determining a particular limit and its asymptotic behaviour,"Problem description I have the limit $$ \lim_{n \to \infty} \sum_{k=1}^{n-2}\left( \frac{(n-3)!}{(n-k-2)!} n^{-k+1}a^k\frac{1}{k+2}\right),$$ with $a \in \mathbb{R}_{>0}$ (at Attempt 2 I have typed out the first few terms). I have already shown this limit converges for any $a < 1$ , but I hope to show that it diverges for any $a > 1$ (and hopefully determine its behaviour for $a = 1$ ) as well. I think Attempt 2 could be a way to show this, but I am not sure if the steps I take are actually allowed. Even if they are, I would like a proof more similar to Attempt 1 , since it may show how quickly the sum goes to infinity (so that I could quickly see if e.g. $a^{n/2}$ times this sum would also go to infinity). Any help checking if my proof is correct in Attempt 2 , establishing the behaviour for $a = 1$ or giving a more insightful proof is appreciated. Attempt 1 My first idea was looking at the last term of the sum, which is given by $$(n-3)! n^{3-n} \frac{a^{n-2}}{n}.$$ I know that in general the inequality $$m! \geq e\left( \frac{m}{e} \right)^m $$ holds (I think I could use Stirling for a better estimate, but the reasoning should be the same). Substituting this we see that the last term is at least $$\left(\frac{n}{n-3}\right)^{3-n}e^{4-n}\frac{a^{n-2}}{n}.$$ For $n \to \infty$ the first factor should be finite, which means the last term on itself already clearly converges to infinity for any $a > e$ . I then noted that if we look at earlier terms, we can estimate the fraction of factorials by saying for $k\geq 2$ it is at least $\frac{(n-3)!}{n^{n-2-k}}$ and we can apply the same method as for the last term now which yields something similar for every term, just that we take a lower power of $\frac{a}{e}$ . Even though we have $x$ terms which have at least this value, it is clearly not enough, since for $a < e$ the convergence will be too quick. It seems this method as I applied it right now only works for $a > e$ . Attempt 2 If we just look at the terms of the sum, we get $$\frac{a}{3} + \frac{n-3}{n}\frac{a^2}{4} + \frac{(n-3)(n-4)}{n^2}\frac{a^3}{5} + \frac{(n-3)(n-4)(n-5)}{n^3}\frac{a^4}{6} + \cdots $$ I am very tempted to say that the limit indeed goes to infinity. I know I cannot just swap around sum and limit and say that all fractions containing $n$ go to $1$ , but I think I can do something very similar. For any $N \in \mathbb{R}$ we simply take an l such that $\frac{a^l}{l+2} > N$ , which is clearly possible. Now the term with index $k = l$ on its own will already be at least $N$ , since for $n \to \infty$ the fraction containing $n$ will go to $1$ , so the term will be at least $N$ . Since this is possible for any real $N$ the limit clearly does not converge, and thus diverges. I don't see why this reasoning is wrong, but it feels very weird (and as mentioned, I don't think it's insightful in the behaviour of the function).","['limits', 'summation', 'limits-without-lhopital', 'asymptotics']"
4693054,The Volterra Operator and the distance between a point and its range,"Let $V:L^2[0,1]\to L^2[0,1]$ be the Volterra operator given by $f\mapsto V(f)$ where $$V(f)(t)=\int_0^tf(s)ds,\ \forall t\in[0,1].$$ My question is: Is it true that for for each $d>0$ small there exists $f\in L^2[0,1]$ such that $$\parallel V(f)-f\parallel_{L^2[0,1]}<d\ \ \text{and}\ \ \parallel f\parallel_{L^2[0,1]}\ge\sqrt{d}\  
 \ \ ?$$ I've tried to find functions such that $V(f)$ is similar to $f$ so that $\parallel V(f)-f\parallel_{L^2[0,1]}$ is small (such as $f(t)=e^t$ or something like that) but it didn't help in anything. Any help will be appreciated! Thank you so much.",['functional-analysis']
4693071,"If a polynomial in $B[x]$ is integral over $A[x]$, then are its coefficients integral over $A$?","Suppose $A \subset B$ are integral domains, and $f = b_0 + b_1 x + \ldots + b_m x^m$ , where $b_k \in B$ . Is it true that if $f$ is integral over $A[x]$ then all the coefficients $b_k$ are integral over $A$ ? Note that the other way is obvious, i.e. if all the coefficients $b_k$ are integral over $A$ , then $f$ is integral over $A$ (e.g. see the proof here ). Below are some observations that I made. Since $f$ is integral we have $f^n + f_{n-1} f^{n-1} + \dots + f_0 = 0$ , $f_k \in A[x]$ . Plugging in $0$ we immediately obtain that $b_0$ is integral over $A$ . Hence $f - b_0$ is integral over $A[x]$ . Similarly plugging in $1$ we obtain that $b_0 + \dots + b_m$ is integral over $A$ . Expanding $f^n$ , it's clear that coefficient in front of $x^{kn}$ has the form $b_k ^ n + $ (terms of lower degree in $b_k$ ).  The coefficient in front of $x^{kn}$ coming from $f_l f^l$ will be homogenous polynomial in $\{b_k\}$ of degree $l$ . Thus equating the coefficient to zero from our original expression will give us a monic polynomial in all the other coefficients which vanishes at $b_k$ . Edit: see here for the definition of integral element. In particular, a polynomial $f \in B[x]$ is integral over $A[x]$ if there exist polynomials $f_0, \ldots, f_{n-1} \in A[x]$ such that $f^n + f_{n-1} f^{n-1} + \dots + f_0 = 0$","['number-theory', 'integral-extensions', 'abstract-algebra']"
4693082,How to perform the parameterization of the following figure?,"This topic has been a bit tedious for me but in my attempt I found a possible complex function that represents this trajectory is: z(t) = (2i - 4i t) u(t) + (2r t) u(t-1/2) + (-2ir + (2r-ir) (t-1)) u(t-1) Where u(t) is the Heaviside unit step function. This complex function is divided into two parts to represent the two curves of the trajectory: The first part (2i - 4i t) u(t) + (2r t) u(t-1/2) represents the curve that starts from the negative ""Y"" axis at the point "" $-2ir$ "", goes to the axis ""X"" at the "" $2r$ "" point and then goes to the positive y axis at the ""2ir"" point. The Heaviside unit step function is used to ensure that the first part of the function is only evaluated on the interval [0, 1] . The second part (-2ir + (2r-ir) (t-1)) u(t-1) represents the smallest curve that starts at the point ""ir"", goes to the ""X"" axis at the point "" $r$ "" and goes to the negative ""Y"" axis at the ""-ir"" point. The Heaviside unit step function is used to ensure that the second part of the function is only evaluated on the interval [1, 2] . The arrow that connects the end point of the second curve with the initial point of the first curve is represented by the discontinuity in the function at $t=1$ . I don't know if the above is a valid or correct answer, I appreciate everything you can correct.",['complex-analysis']
4693085,"Solving the Fresnel-type integral $\int_{0}^{\infty} \cos(x\sqrt{x^2+2}) \,\mathrm{d}x$ in an elementary way.","Here I suggest a 'simple' and exquisite integral to solve, namely to prove $$
\int_{0}^{\infty} \cos(x\sqrt{x^2+2}) \,\mathrm{d}x = \frac1{e}\sqrt{\frac{\pi}{8}}
$$ which is regarded as a kind of generalization of Fresnel-type integral, say $$
\int_{0}^{\infty} \cos(x^2) \,\mathrm{d}x = \sqrt{\frac{\pi}{8}}
$$ An usual method to crack this integral is using Laplace Transform, let $y=x\sqrt{x^2+2}\, (y>0)$ , we have $$
\int_{0}^{\infty} \cos(x\sqrt{x^2+2}) \,\mathrm{d}x = \frac1{2}\int_{0}^{\infty} \frac{\sqrt{\sqrt{y^2+1}+1}}{\sqrt{y^2+1}} \cos(y) \,\mathrm{d}y
$$ where you may obviously have $\mathcal{L}(\cos(y))=\frac{s}{s^2+1}$ , yet the inverse transform part $$
\quad\mathcal{L}^{-1}\left(\frac{\sqrt{\sqrt{y^2+1}+1}}{\sqrt{y^2+1}}\right)
$$ is not trivial. Of course, this integral is not a technically 'hard-to-solve' one, but the challenge is to solve it with elementary methods. I do not have any helpful insight yet. May I ask: 1.Any elementary way to obtain that inverse transform? (I think it is a special case of Bessel function.) 2.Any elementary way to solve the integral without using Laplace Transform? (Of course, you can solve the problem with any elementary tools from complex analysis.) Thanks for any help.","['calculus', 'improper-integrals']"
4693116,Compute $\int^2_0\int^2_y \frac{28}{3}(x^2+xy)dxdy$,"\begin{align*}
\int^2_0\int^2_y \frac{28}{3}(x^2+xy)dxdy&=\frac{28}{3}\int^2_0\int^2_y x^2dxdy+\frac{28}{3}\int^2_0\int^2_y xydxdy\\
&=\frac{28}{3}\int^2_0\left[\frac{x^3}{3}\right]^2_ydx+\frac{28}{3}\times \int^2_0y \int^2_yx dxdy\\\
&=\frac{28}{3}\int^2_0\left(\frac{8-y^3}{3}\right)dy+\frac{28}{3} \times \int^2_0 y \times \left[\frac{x^2}{2}\right]^2_y dx \\
&=\frac{28}{3}\times\frac{1}{3}\int^2_08dy-\int^2_0y^3dy+\frac{28}{3}\times \int^2_0 y\left(2-\frac{y^2}{2}\right)dy\\
&=\frac{28}{3}\times\frac{1}{3}\times\left[8y\right]^2_0-\left[\frac{y^4}{4}\right]^2_0+\frac{28}{3}\times\int^2_0 y\times2\left(1-\frac{\frac{y^2}{2}}{2}\right)dy\\
&=\frac{28}{3}\times\frac{1}{3}(16-4)+\frac{28}{3}\times2\times \int^2_0 y\times\left(1-\frac{\frac{y^2}{2}}{2}\right)dy\\
&=\frac{28}{3}\times\frac{12}{3}+\frac{28}{3}\times2\times\int^2_0y\times\left(1-\frac{y^3}{4}\right)dy\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\times\int^2_0ydy-\frac{y^3}{4}dy\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\times\int^2_0y-\frac{1}{4}\times\int^2_0y^3dy\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\times \left[\frac{y^2}{2}\right]^2_0-\frac{1}{4}\times\left[\frac{y^4}{4}\right]^2_0\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\left(\frac{4}{2}-\frac{1}{4}\times\frac{16}{4}\right)\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\left(2-1\right)\\
&=\frac{28}{3}\times4+\frac{28}{3}\times2\\
&=56\\
\end{align*} I double-checked the solution in wolfram alpha and it's a match. However, would this be the best method in deriving the solution? Is there a more succinct method?","['integration', 'multivariable-calculus', 'solution-verification']"
4693161,"Solution to $y'=y^2, y(1)=0$","Solution to $y'=y^2$ is $y = \frac{1}{C-x}$ . If we have extra condition that $y(1)=0$ than we get $0 = \frac{1}{C-1}$ . C then has no solutions. My text book says that the solution to this is $y=0$ , but why?","['initial-value-problems', 'ordinary-differential-equations']"
4693181,Question about the existence of a diffeomorphism between nonsmooth domains in $\mathbb{R}^2$,"Suppose $\Omega_1 \subset \mathbb{R}^2$ is an $n$ -sided convex polygon while $\Omega_2 \subset \mathbb{R}^2$ is simply connected with a piecewise smooth Lipschitz boundary comprised of $n$ smooth curves. My question is: under what circumstances is it possible to find a diffeomorphism in the neighbourhood of $\overline{\Omega}_1$ that, restricted to $\overline{\Omega}_1$ , maps $\overline{\Omega}_1$ onto $\overline{\Omega}_2$ (and hence $\partial \Omega_1$ onto $\partial \Omega_2$ ). Assigning the $n$ (straight) sides of $\partial \Omega_1$ to the $n$ sides of $\partial \Omega_2$ in, say, counter-clockwise orientation, my intuition tells me that a diffeomorphism could exist when the convex corners of $\partial \Omega_1$ are mapped onto convex corners of $\partial \Omega_2$ , i.e., the consecutive sides of $\partial \Omega_2$ must create a convex corner where they attach. Is my intuition correct ? If so, where can I find a proof ?
In case my intuition fails me, is it possible to say something about the existence of a ""weak"" diffeomorphism, i.e., one that satisfies $\det Du(x) > 0$ almost everywhere in $\overline{\Omega}_1$ ?","['diffeomorphism', 'differential-geometry']"
4693196,Question about the definition of an affine curve,"Let $C \subseteq \mathbb{A}_{\mathbb{C}}^2$ be a curve defined by a polynomial equation $f(x,y) = 0$ , where $f \in \mathbb{Q}[x,y]$ is irreducible (and not necessarily non-singular). In the context I am interested in, application of Siegel's theorem, it just states ``genus is positive'' without any further explanation all the time. I realized I don't have a good understanding of this (the thing I had in mind was the genus degree formula $(d-1)(d-2)/2$ but it is valid only if $C$ is nonsingular projective curve in $\mathbb{P}^2$ ...)
I would appreciate if someone could please clarify me the following questions. -If it just says genus, is it arithmetic genus or geometric genus? -How does one define a genus of an affine curve? Any clarification is greatly appreciated. ps as mentioned in the comment, couple places that mention genus of an affine curve: https://encyclopediaofmath.org/wiki/Siegel_theorem https://terrytao.wordpress.com/tag/siegels-theorem-on-integral-points/","['algebraic-curves', 'birational-geometry', 'algebraic-geometry', 'reference-request']"
4693199,Find number of roots of an equation,"I am stuck on the following problem: Given $ f(x)=\displaystyle\frac{4x+3}{x^2+1}$ , find how many roots the equation $$f(f(x))=\int_3^4{f(x)\mathrm{d}x}$$ has in the interval $[1, 4]$ . Consulting GeoGebra for the graphs, there is a root in that interval. Obviously, I first tried to use the minimum/maximum on $f(x)$ in the interval and the monotonicity so as to restrict $f(f(x))$ and the integral to a common interval (The integral's interval should be a subinterval of the function's interval so the existence and uniqueness of the root would be implied by the intermediate value theorem and the function's monotonicity - it is strictly increasing.) but that hasn't worked out. I also computed the integral to be $\ln{\frac{289}{100}}+3\cot^{-1}{13}\approx 1.29$ and then $f(f(1))\approx 1.28< 1.29 < 3.32 = f(f(4))$ but these are based on approximations while I would want a not-so-computational (theoretical, as you may call it) approach, while of course leaving room for some necessary but reasonable computations (not that I would have to approximate $cot^{-1} !)$ . Thanks in advance.",['calculus']
4693229,Simplifying and evaluating a square root expression involving tangent,"I am stuck on the following problem and would appreciate some help solving it: The number $\sqrt{\frac{2-\sqrt{2}}{2+\sqrt{2}}}$ is equal to (a) $\tan \frac{\pi}{4}$ ; (b) $\tan \frac{\pi}{8}$ ; (c) $\tan \frac{\pi}{12}$ ; (d) none of the above. I have tried simplifying the expression and using various tangent identities, but I have not been able to find a suitable form to use. I am wondering if it would be quicker to simplify the expression or if it would be easier to evaluate the tangent values of the given arguments. Any help would be appreciated. Thank you in advance!",['trigonometry']
4693266,Find segment x on the secant circles below,"In the figure determine $'x'$ , knowing that $PM=MQ=4$ and $O$ and $O'$ are centers.(S: $x=4$ ) I try: $\triangle OQH \sim \triangle OAP \implies \dfrac{HO}{OP} = \dfrac{HQ}{AP} = \dfrac{8+OP}{AO}$ $\triangle (OQH-PA): 8.AO.NH = OP.NQ.AH$ $\triangle PQN \sim \triangle HAN: \dfrac{HN}{PN}=\dfrac{AN}{QN}=\dfrac{AH}{PQ}$ $\triangle HAN \sim \triangle QHO: \dfrac{HN}{OH}=\dfrac{AN}{8+OP}=\dfrac{AH}{QH}$","['euclidean-geometry', 'geometry', 'planar-graphs']"
4693301,Does for every integer $k$ exist a positive integer $n$ such that $F_n+k$ is prime?,"For an integer $k$ , define $f(k)$ to be the smallest positive integer $n$ such that $F_n+k$ is prime (where $F_n$ denotes the $n$ -th Fibonacci number) , if such an $n$ exists and undefined else. Is $f(k)$ total , in other words , does always such a positive integer $n$ exist no matter what $k$ is ? The $152$ ""tough cases "" in the range $[-10^5,10^5]$ (no prime for $n\le 10^4$ ) are : -96715 -95551 -93963 -93439 -92293 -90791 -90119 -89607 -88817 -85723
-85445 -85343 -83733 -82697 -81635 -81037 -81005 -78035 -77741 -77397
-77313 -76851 -76519 -75977 -75923 -75089 -74995 -74919 -74273 -74129
-73893 -72665 -72183 -71921 -71535 -71469 -71315 -70967 -69983 -69873
-69411 -67367 -67123 -66929 -64163 -63633 -62677 -62531 -62407 -61277
-60979 -59753 -59219 -58533 -58269 -57885 -57401 -57103 -55811 -55097
-53145 -52603 -52277 -52221 -52193 -50567 -50447 -48841 -47217 -46905
-46903 -45839 -44589 -43981 -43537 -41743 -40681 -40649 -39591 -38097
-36805 -36803 -36323 -35957 -35929 -34971 -30345 -28965 -25145 -23695
-23163 -22707 -22023 -21813 -20617 -20147 -20077 -18717 -15471 -15067
-12333 -12123 -12061 -11649 -11643 -11441 -9919 -9341 -9087 -7505
-5151 -4115 6313 6851 7123 10591 11009 11561 13651 14475
19391 23843 25753 27727 27871 30282 30547 32053 33813 43711
45165 49351 53227 62177 64617 65953 66669 68149 69541 74325
74397 75249 78625 78693 85447 86085 88487 93687 93763 94485
94979 97945 I think , small factors are not forced , no matter what $k$ is and considering the grwoth rate of $F_n$ (just exponential) , we can expect the existence of infinite many $n$ for every $k$","['elementary-number-theory', 'fibonacci-numbers', 'functions', 'prime-numbers']"
4693430,Doubly periodic solutions to $\frac{|\partial w|^2}{(1+|w|^2)^2} = C^2$?,"I am interested in solutions to $$\frac{|\partial_z w|^2}{(1+|w|^2)^2} = C^2$$ for $C>0$ a constant and $w = w(z,\bar z)$ is allowed to have poles. For example $\tan(|z|)$ is a solution. But I am interested in solutions that are doubly periodic in the complex plane, $w(z+\lambda) = w(z)$ for $\lambda \in \Lambda$ , a lattice. Note $w$ need not be holomorphic. I am interested in this in the physics context of 2D skyrmion crystals, which are naturally a doubly periodic map from the plane to a sphere. In the natural $\Bbb{CP}^1$ parameterization of the sphere equations like this pop up all the time for interesting physics. I think it describes a function $w$ which maps the torus to the sphere in some ""constant"" way, since the natural Riemannian metric on the sphere is $\frac{|dz|^2}{(1+|z|^2)^2}$ -- in fact any more insight into this equation would be helpful.","['complex-analysis', 'partial-differential-equations', 'differential-geometry']"
4693468,"Proving that for every integer $n, n^3 + n$ is even via contraposition","I know this is trivial with direct proofs, but how would one go about proving: For every integer n, n^3 + n is even using contraposition? I'm a little rusty on how to properly use contrapositive proofs. I know that if given $P \Longrightarrow Q$ , the contrapositive is $\neg Q\Longrightarrow \neg P$ . So to prove via contraposition, assume $\neg Q$ is true and if $\neg P$ logically follows, then we have shown $P \Longrightarrow Q$ . $Q$ or $\neg Q$ is trivial, however, what is $P$ in this case? Edit: This was just out of curiosity in trying to figure out ways to prove this indirectly.","['proof-writing', 'discrete-mathematics']"
4693470,Intersection of $C^*$-algebras under addition,"Let $\mathcal{B}\subset\mathcal{A}$ be an inclusion of unital $C^*$ -algebras. Let $\mathcal{C}$ be an unital simple subalgebra of $\mathcal{A}$ which is under the image of a faithful conditional expectation from $\mathcal{A}$ , i.e., there exists a unital completely positive projection $\Phi: \mathcal{A}\to\mathcal{C}$ . Moreover, $\Phi(\mathcal{B})\subsetneq \mathcal{C}$ . Suppose that $I\triangleleft\mathcal{A}$ is a non-trivial two sided closed ideal in $\mathcal{A}$ . Standing assumption: $\mathcal{A}$ is generated as a $C^*$ -algebra by $\mathcal{B}$ and $\mathcal{C}$ . Claim: $\mathcal{B}\subsetneq\mathcal{B}+I\subsetneq \mathcal{A}$ . Attempt: Since $I\triangleleft\mathcal{A}$ is an ideal of $\mathcal{A}$ , it follows that $I+\mathcal{B}$ is a $C^*$ -subalgebra of $\mathcal{A}$ . Now, $\Phi(I)\triangleleft\mathcal{C}$ is an ideal of $\mathcal{C}$ . Since $\mathcal{C}$ is simple, it follows that $\Phi(I)=0$ or $\Phi(I)=\mathcal{C}$ . Since $\Phi$ is faithful, it must be the case that $\Phi(I)=\mathcal{C}$ . Since $\Phi(\mathcal{B})\subsetneq \mathcal{C}$ , it follows that $\mathcal{B}\subsetneq \mathcal{B}+I$ . Now, $I\cap\mathcal{C}$ is an ideal of $\mathcal{C}$ . Since $\mathcal{C}$ is simple and $I$ is non-trivial, it must be the case that $I\cap\mathcal{C}=0$ . Here, I am using the assumption that $\mathcal{A}$ is generated by $\mathcal{B}$ and $\mathcal{C}$ . All that remains to be shown is that $\mathcal{B}+I\subsetneq \mathcal{A}$ . I am trying to show that $\left(\mathcal{B}+I\right)\cap \mathcal{C}=\mathcal{B}\cap\mathcal{C}$ . Once this is establishes, we shall have that $\mathcal{B}\cap\mathcal{C}\subset \Phi(\mathcal{B})\subsetneq\mathcal{C}$ which will ensure that $\mathcal{B}+I\subsetneq \mathcal{A}$ . I am unable to do it. I need to use that $I\cap\mathcal{C}=0$ . How do I do it? Thank you for your time.","['c-star-algebras', 'operator-algebras', 'operator-theory', 'functional-analysis', 'ideals']"
4693504,Finding circle tangent to a line and intersecting a given point,"Background I am trying to understand a historical engineering drawing intended for machinists producing a part. I am neither an engineer, nor a machinist, and have only a basic understanding of such drawings. As is apparently typical for such drawings, they contain measurements related to different datum positions on the part itself, with those datum positions measured relative to an origin point. This generally makes it easy to understand the part's various dimensions. However, there is a section of the drawing that confuses me, which I will describe below. Additionally, it has been many years since I have needed to do more than the most basic geometry and so my knowledge is quite rusty. I have looked at several other questions/answers both here and elsewhere, but not yet found an approach that works for me (or if I have, I haven't recognized it). I have also made a simple computer program of the problem restated slightly differently (e.g. while constraining Point C to lie on a circle of radius r from Point A , I can iteratively move the x and y coordinates of Point C until Point B is close to intersecting the line, but it's not exact) but am unsure how to find a more analytical solution. I appreciate your patience and help. Description This section of the part has a Line L of known slope and y-intercept. The position (in the x-y coordinate system in the drawing) of Point A is known. Line L and Point A are connected by a concave-up circular arc of known radius r . The drawing does not specify the center of the circle (Point C ), nor the coordinates of the single point that the circle intersects Line L (Point B ). Thus, while it's known that Point A lies on the circle, since Points B and C are unknown it then it is also unknown where on the circle Point A lies. Question Given the coordinates of Point A , the radius r of the circle, and the equation of Line L , what are the coordinates of Point B where Line L intersects the circle? Thank you. Other Information Known: x and y coordinates for Point A . Equation (i.e. slope [theta] and y-intercept) for Line L . Radius r of the circle. Fact that Line L is tangent to the circle, and that Point B is the point of tangency. Fact that no point, line, etc. lies on or intersects the origin. Fact that all portions of this part lie in the -x and +y quadrant, relative to the origin. Unknown: Angle ACB x and y coordinates of Point B, the point of tangency. x and y coordinates for Point C, the center of the circle. Desired Quantities: x and y coordinates of Point B , the point of tangency. x and y coordinates of Point C , the center of the circle.","['tangent-line', 'geometry']"
4693577,Proving $\mathbb{R}P^n$ is orientable if and only if $n$ is odd.,"Prove that $\mathbb{R}P^n$ is orientable if and only if $n$ is odd. I know this question has been asked many times on this site, but all solutions consist of $n$ forms or homology groups which I can't use. The definition of orientability I can use is: a manifold is orientable if it admits an atlas $(V_\alpha,\phi_\alpha)$ such that the transition maps have a Jacobi matrix with positive determinant. I was able to prove that the map $\alpha: S^n\to S^n$ defined by $\alpha(x)=-x$ is orientation preserving iff $n$ is odd. Moreover, if $\pi:S^n\to\mathbb{R}P^n$ is the projection, then $\alpha\circ\pi=\pi$ . I thought mayble I can somehow use this fact (without using the fac that $\pi$ is a covering map, since it hasn't been taught yet). Moreover, I have constructed specific atlas for the projective plane: $$\{\varphi_i:U_i\to\mathbb{R}^{n}\},\,U_i=\{(x_0:\ldots:x_{n}):x_i\neq0\}$$ defined by: $$\varphi_i(x_0:\ldots:x_{n})=(\frac{x_0}{x_i},\ldots,\frac{x_{i-1}}{x_i},\frac{x_{i+1}}{x_i},\ldots,\frac{x_{n}}{x_i})$$ and was able to find the transition maps, but calculating the determinant in the general case seemed too hard, and will not disprove the existence of orientation in the even case (maybe this is the way to go). Is there an elementary approach that uses the fact that $\alpha$ is orientation preserving and that $S^n$ is orientable, or given the restriction to use ""elementary"" methods only I have to calculate the determinant of the Jacobi matrix? Any help would be appreciated.","['differential-geometry', 'non-orientable-surfaces', 'orientation', 'manifolds', 'projective-space']"
4693584,Linear independence in differential equations,"I'm currently taking linear algebra and differential equations class simultaneously. However, its not clear to me that why we need 2 linearly independent solutions for 2nd order linear equations. If it was 3rd order, would it be 3 and why? Like is it 2 dimensional when it becomes 2nd order? $$ay''+by'+cy=f(x)$$ Also, didn't get the reason behind solving for nonhomogenous functions, why do we only add 1 solution of  nonhomogenous to the homogenous solutions in order to get the general solution? . And is this limited with linear DE's?","['linear-algebra', 'ordinary-differential-equations']"
4693612,No simple group of order 1040,"Burnside (Proceedings of the London Mathematical Society, vol. 26; Collected Papers, vol. 1, p. 601) gave the following proof of the non-simplicity of groups od order 1040 : ""If simple, the group must have 26 sub-groups of order 5, each contained self-conjugately in a sub-group of order $2^{3}.5$ . Such a sub-group necessarily contains an operation of order 10; and the corresponding operation of order 2, which is permuable with an operation of order 5, must, if expressed as an even substitution of 26 symbols, consist of 10 transpositions. It must therfore occur in 6 sub-groups of order $2^{ 3}.5$ ,"" So far, so good : we can find an element of order 2 commuting with an element of order 5 and such an element of order 2 normalizes exactly 6 subgroups of order 5. (I can give a detailed proof if anybody asks for it.) But Burnside continues : ""and be permutable with 6 sub-groups of order 5. But, since 6 is not a factor of the order of the group, this is impossible."" Well, the considered element of order 2 normalizes  exactly 6 subgroups of oder 5, but Burnside seems to conclude from this that the considered element of order 2 centralizes exactly 6 subgroups of order 5. Why ? So, my question is : could you explain the end of Burnside's proof or give another (correct) proof ? Thank you in advance.","['group-theory', 'simple-groups', 'finite-groups']"
4693630,What is the value of $\sum_{k=1}^{\infty}\frac{\cos\left(\frac{2\pi k}{3}\right)}{k^2}$?,"I've come across the following trigonometric series: $$\sum_{k=1}^{\infty}\frac{\cos\left(\frac{2\pi k}{3}\right)}{k^2}$$ for which WolframAlpha gives the answer $-\dfrac{\pi^2}{18}$ . How do you evaluate it? (I'm guessing it's using Fourier analysis which I'm not very familiar with.) $ $ Also, the similar sum $$\sum_{k=1}^{\infty}\frac{\sin\left(\frac{2\pi k}{3}\right)}{k^2}$$ does not give a ""nice"" answer on WolframAplha. Could you evaluate it in a similar way?","['trigonometric-series', 'trigonometry', 'summation', 'sequences-and-series']"
4693638,Limit related Binomial Problem,Evaluate $\lim_{n\rightarrow∞}\left(\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\left(\frac{1}{r+3}\right)\right)}\right)$ My approach is as follow but not able to integrate it or convert into a limit series as it involves binomial function $^nC_r$ $\left(\frac{1}{r+3}\right)=\int\limits_{0}^{1}{x^{r+2} dx}\implies \lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\int\limits_{0}^{1}{x^{r+2} dx}\right)}}=\lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left({n \choose r}*\int\limits_{0}^{1}{x^2*\left(\frac{x}{n}\right)^r dx}\right)}}$,"['limits', 'binomial-theorem']"
4693686,How to find the mean distance between a point inside a ball and the surface of this ball?,"Say I have a sphere of radius $R$ , I would like to know the average distance between a point $M$ inside the ball and the sphere of radius $R$ . Because of the spherical symmetry, we can say without loss of generality that my point $M$ inside the ball is defined by the coordinates: $$ (x=r,y=0,z=0) $$ , and the points on the sphere are defined by the coordinates: $$(x=R\cos(\phi)\cos(\theta),y=R\cos(\phi)\sin(\theta),z=R\cos(\theta))$$ then the average distance is : \begin{equation}
d = \frac{1}{4 \pi} \int_0^{\pi} \int_0^{2\pi} \sqrt{ (R \cdot \cos(\phi) \cdot \cos(\theta)-r)^2 + (R \cdot \cos(\phi) \cdot \sin(\theta) )^2 + ( R \cdot \sin(\theta) )^2 } \cdot  \sin(\theta) d\theta d\phi 
\end{equation} \begin{equation}
d = \frac{1}{4 \pi } \int_0^{\pi} \int_0^{2\pi} \sqrt{ R^2 + r^2 - 2 R r \cos(\theta)\cos(\phi) } \sin(\theta) d\theta d\phi 
\end{equation} does someone know how to solve this integral? I computed it numerically (the mean distance between a point inside a ball and the surface) and the result should be: $$ d = \frac{r^2}{3R}+R $$ Thank you","['multivariable-calculus', 'spherical-coordinates', 'probability']"
4693695,Proving $\mathbb{E}[X|\sigma(X)]=X$ almost surely.,"Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $X:\Omega \rightarrow \mathbb{R}$ be a $\mathcal{F}-$ measurable random variable. Prove or disprove that $\mathbb{E}[X|\sigma(X)]=X$ almost surely, where $\sigma(X)$ is a sigma algebra generated by $X$ . Here is attempt. Let $\mathcal{G}:=\sigma(X)$ . We first to show that for all $A\in \mathcal{G}$ , \begin{equation}
\int_A \mathbb{E}[X|\mathcal{G}](\omega)dP(\omega) = \int_A X(\omega)dP(\omega).
\end{equation} By definition of $\sigma(X)$ , for any $A\in \mathcal{G}$ there exists $B\in \mathcal{B}(\mathbb{R})$ such that $X^{-1}(B)=A$ , where $\mathcal{B}(\mathbb{R})$ is a Borel set. Then \begin{align}
\int_A \mathbb{E}[X|\mathcal{G}](\omega)dP(\omega) &=\mathbb{E}[X|\mathcal{G}]\cdot P(A) \quad
 (\because \mathbb{E}[X|\mathcal{G}] \text{ is } \sigma(X)-\text{measurable, hence constant over }X^{-1}(B)=A)\\
& = \mathbb{E}[X|\mathcal{G}]\cdot \mathbb{E}[\mathbb{1}_{A}] \quad (\because P(A)=\mathbb{E}[\mathbb{1}_{A}])\\
& = \mathbb{E}[\mathbb{1}_{A}\cdot \mathbb{E}[X|\mathcal{G}]]  \quad (\text{again, } \mathbb{E}[X|\mathcal{G}] \text{ is constant over }A)\\
& = \mathbb{E}[\mathbb{E}[X\cdot \mathbb{1}_{A}|\mathcal{G}]] \quad (\because \mathbb{1}_{A} \text{ is } \mathcal{G}-\text{measurable} )\\
& = \mathbb{E}[X\cdot \mathbb{1}_A] \quad (\text{by Iterated Law of Expectation} )\\
& = \int_A X(\omega)dP(\omega).
\end{align} Next step is to show that $\mathbb{E}[X|\mathcal{G}]=X$ almost surely. But I am stuck at this step. How can I come to a conclusion? Also, could anybody verify that the steps so far are indeed true?","['conditional-expectation', 'probability-theory']"
4693720,How can I compute $\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)$ for a Brownian motion?,"Let $a\in \Bbb{R}_+$ and $B$ be a standart Brownian motion. For $\lambda >0$ I want to compute $$\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)$$ I am somehow a bit lost where to start. I thought about using the tower property of conditional expectation and get that $$\begin{align}\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\right)&=\Bbb{E}\left(\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}} 1_{B_t\leq -a}\big|\mathcal{F}_t\right)\right)\\&=\Bbb{E}\left(1_{B_t\leq -a} \Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)\right) \end{align}$$ where I used that $\{B_t\leq -a\}=B_t^{-1}((-\infty, -a])\in \mathcal{F}_t$ . But also here I don't see how to continue since I don't have independence of $1_{B_t\leq -a} $ and $\Bbb{E}\left( e^{-|B_t| \sqrt{2 \lambda}}\big|\mathcal{F}_t\right)$ . Can someone give me a hint how do do this or what to use?","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4693724,Help understanding proof involving Schur's Lemma.,"In the book Representation Theory, A First Course by William Fulton and Joe Harris, they state the following: Proposition 1.8 For any representation $V$ of a finite group $G$ , there is a decomposition $$
V=V_1^{\oplus a_1}\oplus\cdots\oplus V_k^{\oplus a_k},
$$ where the $V_i$ are distinct irreducible representations. The decomposition of $V$ into a direct sum of the $k$ factors is unique, as are the $V_i$ that occur and their
multiplicities $a_i$ . Proof. It follows from Schur's lemma that if W is another representation of
G, with a decomposition $W=\oplus W_j^{\oplus b_j}$ , and $\phi:V\rightarrow W$ is a map of representations, then $\phi$ must map the factor $V_i^{\oplus a_i}$ into that factor $W_j^{\oplus b_j}$ for which $W_j\cong V_i$ ; when applied to the identity map of $V$ to $V$ , the stated uniqueness follows. I don't quite follow the proof. I think for the first part, he uses that if $\phi$ is a linear map between representations, then $f$ restricted to $W_i$ is a map between irreducible representations and Schur's lemma tells us that those representations are isomorphic (correct?). Also, I am not sure about the last part ""when applied to the identity map of $V$ to $V$ , the stated uniqueness follows"". Edit: I found another proof in REPRESENTATION THEORY FOR FINITE GROUPS by Shaun Tan, but I also do not find it easy to understand that one. Specifically, I do not understand why he says that ""If $j=i$ , then $\phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0$ for any $i$ ."" Theorem 4.3. For any finite-dimensional representation $(\rho, V)$ of a finite group $G$ there is a unique decomposition $V=V_{1}^{\oplus a_{1}} \oplus V_{2}^{\oplus a_{2}} \oplus \ldots \oplus V_{i}^{\oplus a_{i}}$ where the $V_{i}$ are inequivalent and irreducible with unique multiplicities $a_{i}$ . Proof. We suppose $V=W_{1}^{\oplus b_{1}} \oplus W_{2}^{\oplus b_{2}} \oplus \ldots \oplus W_{j}^{\oplus b_{j}}$ . Then we let $\phi: V \rightarrow V$ be the identity map. We use Schur's Lemma. For each irreducible $V_{i}^{\oplus a_{i}}$ , we restrict the domain of $\phi$ to that component. Then, either $\phi=0$ or $\phi$ is an isomorphism. If $j=i$ , then $\phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0$ for any $i$ . For each component, $\phi$ is an isomorphism such that $V_{i}^{\oplus a_{i}}$ maps to $W_{j}^{\oplus b_{j}}$ where $V_{i}$ is isomorphic to $W_{j}$ .","['representation-theory', 'vector-spaces', 'modules', 'abstract-algebra', 'group-theory']"
4693741,How to prove $\sin^2{7\theta}-\sin^2{4\theta}=\sin{11\theta}\sin{3\theta}$?,"I've tried starting from both sides, the furthest I've gone is by starting from the right hand side: $$\begin{align*}
\sin{11\theta}\sin{3\theta}&=\sin{(7\theta+4\theta)}\sin{(7\theta-4\theta)} \\
\\
&=(\sin{7\theta}\cos{4\theta}+\sin{4\theta}\cos{7\theta})(\sin{7\theta}\cos{4\theta}-\sin{4\theta}\cos{7\theta}) \\
\\
&=\sin^2{7\theta}\cos^2{4\theta}-\sin^2{4\theta}\cos^2{7\theta}
\end{align*}$$ Have I done the identity wrong, or should I be using a different approach?",['trigonometry']
