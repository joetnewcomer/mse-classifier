question_id,title,body,tags
4696664,Two complex numbers are symmetric with respect to a circle iff a certain equation is satisfied,"Let $ \gamma  = $ { $z \in \mathbb{C} : |z-a| = R$ }. Two complex numbers $z_1,z_2$ are said to be symmetric with respect to $\gamma$ iff $$ (z_1-a)\overline{(z_2-a)} = R^2. $$ I am trying to prove that if $\forall \alpha,\beta \in \gamma$ , $$ \Bigg | \frac{z_1-\alpha}{z_2 - \alpha} \Bigg | =  \Bigg |\frac{z_1-\beta}{z_2 - \beta} \Bigg | \qquad \qquad (1)$$ then $z_1 $ and $z_2$ are symmetric with respect to $\gamma$ . I was able to prove the converse using the preliminary Möbius transformation $$ T(z) = \frac{R^2}{z-a} + \overline{a}$$ and properties of the cross-ratio, but, in trying to apply these techniques, I was not able to obtain any substantial progress. I need some help proving this result. I found the problem in one of my problem sheets in a course in complex analysis mainly based in Silverman's and Lang's textbooks on the subject. Addendum: Considering a comment that was erased, if one considers the transformation $$T(z) = \frac{z-\alpha}{z-\beta}$$ where $\alpha,\beta \in \gamma$ , $\alpha \neq \beta$ , we have, by hypothesis, that if $w_1 = T(z_1)$ and $w_2 = T(z_2)$ , $$ |w_1| = |w_2|$$ thus, if $w = (w_1 + w_2)/2$ , $w_1$ and $w_2$ are symmetric with respect to $L = \{ wt : t \in \mathbb{R}\}$ , which is a line that ""passes through"" $0$ and $\infty$ , thus, by symmetry preserving of Möbius transformations, $z_1$ and $z_2$ are symmetric with respect to some line or circle which contains $\alpha$ and $\beta$ . In addition, $\alpha $ and $\beta$ where chosen arbitrarily from $\alpha$ , thus, for any $\alpha, \beta, \delta \in \gamma$ , mutually distinct, $z_1$ and $z_2$ are symmetric with respect to a line or circle which contains $\alpha$ and $\beta$ and also with respect to a line or circle which passes through $\alpha$ and $\delta$ , however, transitivity of these properties, i.e. deducing that there exists a circle which contains $\alpha,\beta$ and $\gamma$ such that $z_1$ and $z_2$ are symmetric with respect to, seems distant to prove.","['complex-analysis', 'cross-ratio', 'mobius-transformation']"
4696688,Finite part of $-1/x^2$,"I'm learning the basic of Distributional Theory. I ended up solving the following exercise: 'Find the distributional derivative of $P.V.1/x$ '. After few computation, I arrived at the following: $$\left\langle\left(P.V.\frac{1}{x}\right)', \phi\right\rangle = \lim_{\epsilon \rightarrow 0^{+}}\bigg(\frac{2\phi(0)}{\epsilon} + \int_{|x| \geq \epsilon} \phi(x) \bigg(-\frac{1}{x^2}\bigg)dx\bigg).$$ I've discovered that $P.V.\dfrac{1}{x}$ has a short form, without the limit: $$\left\langle P.V.\dfrac{1}{x}, \phi\right\rangle = \int_{-1}^{1} \frac{\phi(x) -\phi(0)}{x}dx + \int_{|x| > 1}\frac{\phi(x)}{x}dx.$$ Could I find a similar form for partie finie? Thanks for your help.","['integration', 'distribution-theory', 'cauchy-principal-value', 'limits', 'derivatives']"
4696695,Pointwise Convergence of Convolutions,"While reading chapter 4 of ""Deep Learning Architectures, A Mathematical Approach"" by Ovidiu Calin, I stumbled upon the following statement: Using the fact that the Gaussian tends to Dirac measure, $\lim_{\sigma \to +0}G_\sigma(x) \to \delta(x)$ , we have $$
\lim_{\sigma \to +0} f_\sigma(x) = \lim_{\sigma \to +0} (f*G_\sigma)(x) = (f*\delta)(x) = f(x),
$$ where $f(x)$ is an integrable function on $\mathbb{R}$ and $G_\sigma(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{x^2}{2\sigma^2})$ and $f_\sigma(x) = (f*G_\sigma)(x) = \int f(u)G_\sigma(x-u)du$ . From what I know, I wasn't very sure about this result. Unfortunately, the proof is omitted from the book so I tried to verify the claim by myself. It is not exactly the same but I was able to confirm the results below by following argument in Chapter 9 of ""Real and Complex Analysis"" by Walter Rudin. Result 1: If $f$ is bounded and continuous at point $x$ , then $$
\lim_{\sigma\to +0}f_\sigma(x) = f(x).
$$ Result 2: If $1\leq p < \infty$ and $f\in L^p$ , then $f_\sigma$ converges to $f$ in $L^p$ as $\sigma \to +0$ . From the second result, it follows that there exists a sequence $\sigma_n \to +0$ such that $f_{\sigma_n}(x) \to f(x)$ almost surely. But it does not extend to pointwise almost sure convergence of $f_\sigma \to f$ . It doesn't seem to be possible to show pointwise almost sure convergence $f_\sigma \to f$ without assumptions like in the result 1. But I'm not sure if there is something special about gaussian or if there is a counterexample to it. I appreciate it if anyone could prove the claim or provide a counterexample to it. You don't have to write full solution, if you know a good reference, then just a reference is fine. Update: As Jochen pointed out, by following the argument here , we can show almost sure pointwise convergence if $f \in L^\infty$ . So, the question really is whether we can relax it to $f \in L^1$ .","['weak-convergence', 'convolution', 'normal-distribution', 'real-analysis', 'functional-analysis']"
4696772,Net force on the side of a jar,"A round conical flask is filled with water of a depth $h$ . The radius of the upper water surface is $R_1$ and that of the lower
surface is $R_2$ . What is the net force that the water exerts on the sides
of the flask? What is the force on the bottom of the flask? What is the sum of these forces? Ignore atmospheric pressure. I know the force on the bottom is: $$F = pgh\pi R_2^2$$ since $F$ is the pressure times cross section area and pressure is $pgh$ . However, I don’t know how to find the force on the sides of the jar. I think some integration is involved.","['integration', 'physics', 'geometry']"
4696777,Are $\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}$ and $\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$ similar?,"I have two matrices: $$
A =\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}~~~~~~~
B = \begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
$$ I am to show they are similar or not similar. I set the following up: $$
B = Q^{-1}AQ
$$ where $$
Q = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$ I solved that $d = -b$ and $c=b$ and $-b(b-a) \ne 0$ . I set $b = 2$ , $a =1$ and attempted to solve $B=Q^{-1}AQ$ , but I end up getting: $$
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix} = 
\begin{bmatrix}
3 & 0 \\
3 & 0
\end{bmatrix}
$$ I wanted to ask about my general strategy, and if at this point, I must simply plug in different values to see what works. Calculation: $$
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
=
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$ $$
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
=
\frac{1}{ad-bc}
\begin{bmatrix}
(a+c)d & (b+d)d \\
-c(a+c) & -c(b+d)
\end{bmatrix}
$$ I then solved the linear system. Likewise, solving $AQ = QB$ , I get: $$
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
a & b \\
a & b
\end{bmatrix}
=
\begin{bmatrix}
a & b \\
a & b
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
$$ $$
\begin{bmatrix}
a+c & b+d \\
0 & 0
\end{bmatrix}
=
\begin{bmatrix}
a+b & 0 \\
c+d & 0
\end{bmatrix}
$$ which only gives that $b=c$ .","['matrices', 'similar-matrices', 'linear-algebra']"
4696817,Is this subset of a closed set Borel measurable?,"Let $V \subset [0,\infty)^n$ be closed. I've been studying the following subset of $V$ for research, which has useful properties, but cannot prove or disprove it is Borel measurable. Introduce the partial ordering $\leq'$ on $[0,\infty)^n$ such that $(x_1,...,x_n) \leq' (y_1,...y_n) \iff x_k \leq y_k, \forall k$ . Consider the set $W$ of minimal elements of $V$ under $\leq'$ . That is, $$
W = \big\{ x \in V \ \big| \ \nexists y\in V, y<'x\big\}.
$$ One can easily show that $W \subset \partial V$ and if $V \neq \emptyset$ then $W \neq \emptyset$ (with Zorn's lemma). I have been wondering how to prove/disprove $W$ is Borel measurable for months, gave up, and settled with taking its closure only to make it Borel measurable (if the set were measurable, everything would work out nicely). I believe this set is Borel measurable and that $\overline{W}\backslash W$ is countable (it does not seem easy to construct a counterexample to either of these, because $W$ seems to always have a nice monotone shape). However, a proof of either of these has continuously evaded me.","['order-theory', 'measure-theory', 'descriptive-set-theory']"
4696834,How to calculate the density function?,"Let $(X, Y, Z)$ be a randomly absolutely continuous vector, whose density function is
given by: $$f_{(X,Y,Z)}(x,y,z)=\frac{6}{(1+x+y+z)^4}$$ For $x,y,z>0$ . Calculate the density function of $W=X+Y+Z$ . My idea was to calculate $\mathbb{P}(W\leq t)$ given $t>0$ and define the region $A=\{(x,y,z)\in \mathbb{R_{+}^3}: x+y+z\leq t\}$ and fix $z$ , using the fact: $$\mathbb{P}(W\leq t)=\mathbb{P}((x,y,z)\in A)$$ My question is to know if this idea is ok, or if there is another way to do it.","['probability-theory', 'probability', 'random-variables']"
4696837,What is the conceptual motivation for Bayes' Theorem over this other equation?,"According to Wikipedia... With Bayesian probability interpretation, [Bayes'] theorem expresses
how a degree of belief, expressed as a probability, should rationally
change to account for the availability of related evidence. Suppose I want to relate $\Pr(W\ |\ C)$ , a degree of rational belief in $W$ after evidence $C$ becomes available, back to $\Pr(W)$ , a degree of rational belief in $W$ before evidence $C$ becomes available. Bayes' Theorem is just a double-application of the conditional probability formula, thus... $\boldsymbol{\Pr(W\ |\ C)}\ =\ \frac{\Pr(W\ \cap\ C)}{\Pr(C)} = \frac{\boldsymbol{\Pr(W)}\Pr(C\ |\ W)}{\Pr(C)}$ Alternatively however, I could apply the conditional probability formula once, and then the inclusion-exclusion formula in the numerator to obtain... $\boldsymbol{\Pr(W\ |\ C)} = \frac{\boldsymbol{\Pr(W)}\ +\ \Pr(C)\ -\ \Pr(W\ \cup\ C)}{\Pr(C)}$ This formula also relates $\Pr(W\ |\ C)$ back to $\Pr(W)$ .  My question is, what warrants embracing the top relation (Bayes' Theorem) and rejecting the bottom relation as the correct mathematical representation of belief updating? My first attempt at an answer was that the bottom relation is unhelpful because $\Pr(W\ \cup\ C)$ is not algebraically independent of $\Pr(W)$ , and thus the appearance of $\Pr(W)$ may be ""artificial,"" analogous to a more blatantly artificial introduction of $\Pr(W)$ such as $\boldsymbol{\Pr(W\ |\ C)}\ =\ \frac{\Pr(W\ \cap\ C)}{\Pr(C)}\ +\ \boldsymbol{\Pr(W)}\ -\ \boldsymbol{\Pr(W)}$ .  However, the $\Pr(C\ |\ W)$ that appears in the top relation is also not algebraically independent of $\Pr(W)$ , the conditional probability formula already applied being the bridge between the two. Another thought I had is that the top and bottom relations may actually be comparably good mathematical representations of belief updating, and the top relation is merely preferred because it acts on $\Pr(W)$ via multiplication, whereas the bottom relation acts on $\Pr(W)$ via addition and multiplication.  However, this feels quite weak to explain the ubiquity of the top relation, as it is, in general, far from true that the most useful mathematical expression is always the simplest. Why should I use Bayes' Theorem as the unique intuition for belief updating, and not the other expression?","['statistics', 'probability-theory', 'inclusion-exclusion', 'bayes-theorem', 'probability']"
4696886,"Let $N$ be any four digit number, $N=x_1 x_2 x_3 x_4$. Find the maximum value of $\frac{N}{x_1+x_2+x_3+x_4}$",Let $N$ be any four digit number say $x_1 x_2 x_3 x_4$ . Then maximum value of $\frac{N}{x_1+x_2+x_3+x_4}$ is equal to (A) 1000 (B) $\frac{1111}{4}$ (C) $800$ (D) none of these My approach is as follow $N=1000x_1+100x_2+10x_3+x_4$ $T=\frac{1000x_1+100x_2+10x_3+x_4}{x_1+x_2+x_3+x_4}$ $T=\frac{999x_1+99x_2+9x_3}{x_1+x_2+x_3+x_4}+1$ Not able to proceed from here,"['algebra-precalculus', 'functions']"
4696924,Teacher needs help with an integral involving trig identities that a student found online,"$$\int\frac{\sqrt{\sec^3(x)+\tan^3(x)+\tan(x)}}{1+\sec(x)-\tan(x)}\,dx\quad\text{for $0<x<\frac{\pi}{2}$}$$ I have spent several days trying and failing to solve this problem. I am a teacher student of mine found it online. They asked for help solving it and initially I thought that there was a sign error because if so, I could easily solve it by substitution. However, the student maintains that it can be solved. I have tried so many things, I even resorted to using an online integral calculator which said there was no antiderivative. I am just so drained and demoralized, any help with this is appreciated.",['calculus']
4696930,"A strictly monotone, continuous path with maximal length","If $f:[0,1]\to [0,1]$ is an increasing continuous function such that $f(0)=0$ and $f(1)=1$ , then the length of the graph $\{(t,f(t))|t\in [0,1]\}$ is bounded above by $2$ . This is easily proven by considering an arbitrary partition $$0=t_0<t_1<\cdots < t_n=1$$ and noting that $$\sum_{k=1}^n\sqrt{(t_k-t_{k-1})^2+(f(t_k)-f(t_{k-1}))^2}\leq\sum_{k=1}^n|t_k-t_{k-1}|+|f(t_k)-f(t_{k-1})|=1+f(1)-f(0)=2$$ A somewhat less trivial argument shows that for the Cantor function there is an equality: the length of the graph is exactly equal to $2$ . The Cantor function is not strictly monotone. In fact, it is constant on every open interval that was removed in the process of constructing the Cantor set . By considering graphs of the form above, it is clear that for every $\varepsilon>0$ there exists a strictly monotone continuous function such that $f(0)=0$ and $f(1)=1$ , whose graph has length larger than $2-\varepsilon$ . Question : Is there a strictly monotone continuous function with $f(0)=0$ and $f(1)=1$ whose graph has length $2$ ?","['monotone-functions', 'geometry', 'examples-counterexamples', 'real-analysis']"
4696935,"Prove: For any integer $n$, if $n^3+5$ is even, then $n$ is odd.","Is this proof good? Proof: This will be proven by contrapositive.
Let $n$ belong to all integers such that $n$ is even.
Then $n=2k$ where $k \in \mathbb Z$ Thus $n^3+5=(2k)^3+5$ which is odd.",['discrete-mathematics']
4697032,"Threshold for the ""number of UUIDs generated per millisecond"" at which the collision probability of UUID v4 and UUID v7 is equal","I post this question here instead of StackOverflow because the mathematical element is stronger than engineering one. First, let me clarify the definition of terms. UUID v4 : Random value of $122$ bits ( $2^{122}$ possible values) UUID v7 : Random value of $74$ bits ( $2^{74}$ possible values), independent every millisecond UUID v4 starts with an almost zero chance of collision, but as a certain number of UUIDs accumulate, the collision probability increases gradually due to the birthday paradox problem. On the other hand, if UUID v7 is generated less than once per millisecond, the collision probability is absolutely zero. If it is generated more than twice per millisecond, collisions occur at a certain probability. For simplicity, let's only consider what happens just after 50 years ( $31557600000 \cdot 50$ milliseconds), and avoid treating time $t$ as a variable. If UUIDs are generated at a rate of $x$ per millisecond, after 50 years, $(31557600000 \cdot 50)x$ UUIDs will have accumulated. UUID v4 is affected by the number of accumulated UUIDs, so it is necessary to consider both the collision probability between UUIDs that are about to be created and the collision probability with UUIDs created in the past . For UUID v7, it is enough to consider only the collision probability between UUIDs that are about to be created . Based on this, I want to know the minimum value of $x$ at which the collision probability of UUID v7 becomes higher than that of UUID v4 after 50 years . Similarly, I want to calculate the patterns for 0 seconds, 5 seconds, 1 year, 5 years, 10 years, 50 years, 100 years, 500 years, and 1000 years, and observe how they change. I tried to get ChatGPT to solve this problem, but it gave me answers that seemed wrong, or said there were no solutions. Could it be that the number of UUIDs generated per millisecond $x$ is not a parameter, and everything is determined by the time $t$ ? I'm not very good at math and I'm having trouble. I would appreciate your help.","['random', 'probability', 'computer-science']"
4697057,(SOLVED) Simplify $\cos^{-1}\left( \frac{7}{2}\left( 1+\cos{2x} \right) + \sqrt{\left( \sin^{2}{x} - 48\cos^{2}{x} \right)}\sin{x}\right)$,"I need a little help in simplification of an ITF problem of a college entrance exam I got stuck in. $$\cos^{-1}\left( \frac{7}{2}\left( 1+\cos{2x} \right) + \sqrt{\left( \sin^{2}{x} - 48\cos^{2}{x} \right)}\sin{x}\right)$$ with $ x\in \left( 0,\frac{\pi}{2} \right) $ What I'm doing: $y=\cos ^{-1}\left(\frac{7}{2}(1+\cos 2 x)+\sqrt{\left(\sin ^2 x-48 \cos ^2 x\right)} \sin x\right)$ $\quad=\cos ^{-1}\left((7 \cos x)(\cos x)+\sqrt{1-49 \cos ^2 x} \sqrt{1-\cos ^2 x}\right)$ ......stuck Thanks!","['trigonometry', 'inverse-function']"
4697064,Domain of $g(x)=\sqrt{\sin(π\sin(πx))}$,"Let a function $$g(x)=\sqrt{\sin(π\sin(πx))}$$ If I have want to find its domain.
The first step wil be making the term inside the square root greater than or equal to zero
i.e. $$\sin(π\sin(πx))   \ge 0$$ Further we can say $$ 2nπ \le   π\sin(πx)  \le (2n+1)π$$ Where $n \in Z$ $$ 2n \le   \sin(πx)  \le (2n+1)$$ But now I am stuck. Further I think here we will be using inverse trigonometry but I have not been familiar with inequalities involving inverse trigonometry.","['trigonometry', 'inequality']"
4697065,Confusing notation when substituting differential equation,"Let's say I have a differential equation $$ \frac{dy(x)}{dx}=f(y,x) \tag{1} $$ Then, I notice that this would be much easier to solve if $x$ is substituted via $z=ax+b$ .
Is it correct to write it like this: $$ \frac{dy(\frac{z-b}{a})}{dz/a}=f(y,\frac{z-b}{a})  \tag{2} $$ It feels like I have forgotten the chain rule here. The expression on the left hand side still looks like something I can evaluate using the chain rule: $$ \frac{dy(\frac{z-b}{a})}{dz/a} = a \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} \frac{dg}{dz} = \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}}  \tag{3} $$ So that the system becomes $$ \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} = f(y,\frac{z-b}{a}) \tag{4}$$ or again $$ \frac{dy(g)}{dg} = f(y,g) \tag{5}$$ Where is the problem in the notation here? I think in (2), the way it is written, I am not supposed to apply the chain rule but rather replace $y$ with some other function which then hides the full dependence on $z$ . But can I write this down more cleanly?","['notation', 'substitution', 'ordinary-differential-equations', 'chain-rule']"
4697093,Why inner automorphisms; why conjugation? Why not closure under other automorphims?,"I've been casually reading up on group theory recently, and I want to get a really solid and motivated understanding of where all the definitions we use come from. Notions like the center of a group seem utterly natural to give a name to; all those elements that commute with the whole group $G$ , which is easily proven a subgroup. Normal subgroups however are slightly more mysterious. Closure under conjugation by arbitrary $g \in G$ seems like a natural idea once we've decided we care about conjugation, but it's not obvious to me why this should be privileged above all other automorphisms. Is conjugation provably unique in some significant way; why is there not some other use of group operations defining alternatively ""inner"" automorphisms? Why are normal subgroups so much more significant than other subgroups, closed under other automorphisms; can we justify conjugation's significance without hand-waving about later useful applications?","['automorphism-group', 'group-theory', 'normal-subgroups', 'motivation']"
4697103,Not sure about the limit of $\frac{\log(1+x^4)}{x^2 \tan^2(2x)}$ for $x$ that goes to zero,"I have to find the value of the function $\frac{\log(1+x^4)}{x^2 \tan^2(2x)}$ for $x$ that goes to zero. It's an undefined form $\frac{0}{0}$ so I transformed the function in this way in order to use two known limits $$\frac{\log(1+x^4)}{x^2} \frac{x^2}{x^2} \frac{1}{\tan^2(2x)} \frac{4x^2}{4x^2} $$ and so $$\frac{\log(1+x^4)}{x^4}\frac{4x^2}{\tan^2(2x)}\frac{x^2}{4x^2} $$ It seems to me that the value of the limit in zero is $\frac{1}{4}$ but looking to the graph of the function (using Desmos) it do not have a clear behavior in zero. Can you tell me if my reasoning is right or wrong and, if wrong, where is the mistake? Thank you. ps. I found the problem with Desmos. In my notation I was using log for the logarithm with e as the base. But I should use ln in Desmos for the same logarithm, which otherwise is 10 based.","['limits', 'real-analysis']"
4697110,Integrating the exponential of distance to a convex bounded set,"I've been nerdsniped by a friend of mine trying to solve a sample analysis qualifying exam, and one of the problems I'm trying to figure out is the following: Let $E \subseteq \mathbb{R}^2$ be a convex bounded set. Determine $$\int_{\mathbb{R}^2} e^{-\mathrm{dist}(\mathbf{x},E)}\, d\mathbf{x}$$ in terms of the Lebesgue measure $\mathcal{L}^2(E)$ and the Hausdorff measure $\mathcal{H}^1(\partial E)$ . I've figured out the first step, breaking apart $\mathbb{R}^2 = \bar{E} \cup \mathbb{R}^2\setminus \bar{E}.$ Then, in $\bar{E}$ , $\mathrm{dist}(\mathbf{x},E) = 0$ so that bit of the integral just becomes $\mathcal{L}^2(\bar{E}) = \mathcal{L}^2(E\cup \partial E) = \mathcal{L}^2(E)$ since $E$ is convex and hence $\mathcal{L}^2(\partial E) = 0$ . It's the integral over the complement that I'm now stuck on. I'm not looking for a complete solution - I'd like to figure as much of this out on my own as I can, but any hints towards the next steps would be very much appreciated. Thanks very much!","['integration', 'measure-theory', 'real-analysis']"
4697122,How do I learn to stop worrying and love the substitution $y'' = y' (dy'/dy)$,"The following is a solution of the differential equation $y'' = y$ with initial values $y(0) = 3$ , $y'(0) = 1$ . Considering $y$ to be a function of $x$ and omitting some standard details: Let $z = y'$ . Then $y'' = \frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx} = z \frac{dz}{dy}$ . Substituting, $z\frac{dz}{dy} = y$ is separable with initial value $y = 3$ , $z = 1$ , so has the unique solution $z = \sqrt{y^2 - 8}$ . Now the equation $y' = \sqrt{y^2 - 8}$ with initial value $y(0) = 3$ has solution $y = 2e^x + e^{-x}$ , which is also a solution to the original problem. While this does produce the correct answer, the step using $\frac{dz}{dy}$ is unjustified. One way to fill this gap is to note that since $y'(0) \ne 0$ , the inverse function theorem says that locally $x$ and therefore $y'$ is in fact a (smooth) function of $y$ . This is somewhat implicitly used in picking the positive branch of the square root $\sqrt{y^2 - 8}$ . But I do not know how to justify the definition of $z$ as a function of $y$ and therefore of $\frac{dz}{dy}$ if you change the initial values to something like $y(0) = 2$ , $y'(0) = 0$ . The solution does have $y$ an invertible function of $x$ but this is not known a priori . If you just blindly work through the integration without worrying you still get the solution $y(x) = e^x + e^{-x}$ , no matter which branch of the square root $\pm \sqrt{y^2 - 4}$ you pick (and in fact $y'$ changes sign near $x = 0$ , so neither of these is valid even locally). Is there a way to make this work rigorously? Or are there similar equations and initial values where this substitution misses a solution? I'm not so worried about it introducing extraneous solution although I do not have an example of that either. I originally learnt this solution in high school from a physics-y book that did not justify the differentiability, as is usual in high school level physics-y books. Actually in that book the original solution was for the simple harmonic motion equation $y'' = -\omega^2 y$ but, as I noticed later, the same substitution seems to help with some other second order ODEs too.","['derivatives', 'inverse-function-theorem', 'ordinary-differential-equations']"
4697145,"$H$ is Hilbert Space, select $x_n\in H$ such that $||x_n||=n$ then $\exists x \in H$ such that $\sup_n |(x_n,x)|=\infty$","$H$ is Hilbert Space and choose $x_n\in H$ such that $\|x_n\|=n$ then $\exists x \in H$ such that $\sup_n |(x_n,x)|=\infty$ Edit: I did the following : To get a contradiction, Assume $\forall y, \in H \sup_n |(x_n,y)| < M \ne \infty$ And by Riesz Rep. $\forall y\in H, T_n(y)=<y,x_n>$ such that $T_n \in H'$ so $T_n$ is bounded linear functional. Indeed, $\forall y\in H, T_n(y)$ is bounded. Now, applying Uniform Bounded Theorem I can say $||T_n||$ is bounded however $||T_n||=\sup_n \frac {||T_n (y)||}{||y||}\le \sup_n \frac{||y||||x_n||}{||y||}=\sup_n ||x_n||=\sup_n n$ which is not finite. (that doesnt work)
And also $||T_n||=\sup_{||y||=1}||T_n (y)||=<y,x_n> \le ||y||||x_n||=||x_n||$ and I could not continue from here.","['hilbert-spaces', 'solution-verification', 'functional-analysis']"
4697172,"Given a quadrilateral $ABCD$, $\angle DAC=3x$, $\angle CAB=4x$, $\angle BDC=2x$. Find the value of $x$","This is a question I came across on Instagram today, and here's the diagram: (Note: The image is NOT to scale)
I attempted to solve it first by amending the quadrilateral in various ways, but each of those methods lead to a strange contradiction, likely because the diagram is not to scale. I'm going to post my successful approach as an answer below, please let me know if my answer is correct or if there's something wrong with the method (the correct answer wasn't revealed, and if there are any other ways to approach this that I missed!)","['euclidean-geometry', 'trigonometry', 'solution-verification', 'geometry']"
4697210,Sufficient conditions on $f_n\longrightarrow f$ so that $\hat{f_n}\longrightarrow\hat f$ in $L^1$ sense,"Let $f:\mathbb{R}\longrightarrow\mathbb{C}$ be a Schwartz function, and suppose $(f_n)_n\subset\mathcal{C}_c^{\infty}$ is such that $f_n\xrightarrow{n\to\infty}f$ in the $L^1(\mathbb{R})$ norm. Is it true that then, $\hat{f_n}\xrightarrow{n\to\infty}\hat{f}$ in the $L^1(\mathbb{R})$ sense as well? (Here $\hat f$ denotes the Fourier transform of $f$ .) [EDIT: it appears to be false in general, as necessary conditions for $\hat{f_n}\xrightarrow{n\to\infty}\hat{f}$ in $L^1(\mathbb{R})$ include that $(f_n)_n$ converge uniformly to $f$ . What would be sufficient conditions to make that happens, if there are any?] Any help is greatly appreciated, thanks in advance!","['integration', 'fourier-analysis', 'fourier-transform', 'analysis', 'lp-spaces']"
4697237,Can we find $x$ such that $\det[x^2 A + x B + C] = 0$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question If $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{C}^{4 \times 4}$ are invertible matrices, is there a way to find $x$ such that the following determinant vanishes? $$\det \left( x^2 \mathbf{A} + x \mathbf{B} + \mathbf{C}\right) = 0$$","['matrices', 'determinant']"
4697248,Triangle formed by a plane intersecting the coordinate axis.,"T is the region of the plane $x+y+z=1,x,y,z>0$ . S is the set points $(a,b,c)$ in T such that just two of the following three inequalities hold: $a\le\frac12,b\le\frac13,c\le\frac16$ . Find area of the region S. T is an equilateral triangle with vertices $(0,0,1),(0,1,0),(1,0,0)$ . Its side is $\sqrt{2}$ So, area of T= $\frac{√3}4(√2)^2=\frac{\sqrt3}2$ I am not able to visualise S. On Byju's website , they have subtracted areas of three small equilateral triangles. I am not able to visualise that. On AoPS website , they have drawn a colorful diagram but alas, I am still not able to visualise which three triangles are being talked about. Is there any website or tool which could help see such 3D structures? Or, if you could help me visualise it with better labelling or more expository language, I'll be grateful. (On Byjus and AoPS, inequalities signs are opposite. I am not sure if both are same questions or different.) Also, how have they taken the side lengths for small triangles? EDIT: Rephrasing my question: Are the questions on Byjus and AoPS websites the same? If no, then how to solve the question that I have posted? If yes, then which of them has a typo? How to find the sides of the small equilateral triangles? How to visualise two inequalities at the same time? Edit $2$ : On AoPS website, they have inequalities $x\ge a, y\ge b, z\ge c$ . S represents area where exactly two inequalities hold. e.g. the case $x\ge a, y\ge b, z\le c$ . If we subtract this area from total area, we would get cases like $x\ge a, y\le b, z\le c$ . And $x\le a, y\le b, z\le c$ . That means the subtraction doesn't really represent the question I have posted. Or does it?","['contest-math', 'coordinate-systems', 'geometry', '3d']"
4697253,Inner product on sequence space $l_2$,"I have a question regarding the Hilbert space $l_2(\mathbb{N,C})$ of all complex valued quadratically summable sequences. Suppose we have a complex valued sequence $(b_n)_{n\in\mathbb{N}}$ and suppose that for all $(a_n)_{n\in\mathbb{N}}\in l_2(\mathbb{N,C})$ the series $\sum_{n\in\mathbb{N}}a_n\overline{b_n}$ converges (absolutely). I was wondering if it is then true that $(b_n)_{n\in\mathbb{N}}\in l_2(\mathbb{N,C})$ . My initial thought was that it is not true. I tried to choose $(b_n)_{n\in\mathbb{N}}$ as a sequence which is ""almost"" quadratically summable, for instance $b_n = \frac{1}{\sqrt n}$ . Intuitively, all $l_2$ sequences are asymptotically upper bounded by this sequence. That's why I thought that previously mentioned series always converges. But I'm not sure at all if this works. If the statement is true, I'd love to know why. Thanks in advance","['functional-analysis', 'analysis']"
4697312,$L^2$ distance between Gaussian probability density functions,"In order to understand different ways of comparing probability functions, I'm trying to bound from above the $L^2$ distance between the pdf of two multivariate Gaussians.
More specifically, let $f,f' : \mathbb{R}^n \to \mathbb{R}$ be the pdf of two multivariate Gaussians with means $\mu,\mu' \in \mathbb{R}^n$ , respectively, and with the same covariance matrix $\Sigma$ . My intuition tells me that $$
    \|f - f'\|_2 \leq \alpha \|\mu - \mu'\|_2,
$$ for some constant $\alpha$ that only depends on $\Sigma$ . However, I'm not even sure of how to start. I have tried considering the differential of the quantity $\|f - f'\|_2$ with respect to $\mu'$ , but I got stuck fairly quickly as I am not too familiar with taking derivatives under a norm.","['probability-distributions', 'normed-spaces', 'normal-distribution', 'real-analysis', 'functional-analysis']"
4697319,de Rham cohomology of the Grassmannian bundle,"Let $X$ be a smooth $n+m$ dimensional manifold and $\pi:Y\rightarrow X$ the bundle whose fibre over any $x\in X$ is the Grassmann manifold of all $m$ dimensional subspaces of $T_xX$ . This is equivalent to the manifold $J^1(X,m)$ of all first order contact elements of dimension $m$ , also called the space of order $1$ jets of $m$ dimensional submanifolds in $X$ . I am interested in knowing the de Rham cohomology $H^\ast_{\mathrm{dR}}(Y)$ of the total space of this bundle in terms of the de Rham cohomology $H^\ast_{\mathrm{dR}}(X)$ of $X$ . I assume this is computable and have been computed, but I cannot find it anywhere and I am not familiar enough with algebraic topology to compute it myself.","['jet-bundles', 'grassmannian', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4697351,Reference for inequality on Frobenius/Hilbert-Schmidt matrix norm,"It is not hard to see that if ${\bf A}$ is a $d\times d$ matrix with unit determinant, $\det({\bf A})=1,$ then one has an upper bound on the Hilbert-Schmidt (Frobenius) norm of the inverse matrix ${\bf A}^{-1}$ in terms of the Hilbert-Schmidt norm of the matrix itself in the form $$
\Vert {\bf A}^{-1}\Vert_{\bf{HS}}^2 \leq C_d \Vert {\bf A}\Vert_{\bf{HS}}^{2(d-1)}
$$ For some appropriately chosen constant $C_d.$ I am interested in the best constants. Obviously $C_2=1$ and I computed that $C_3=\frac{1}{3}$ . I believe that the best constant is $C_d=\frac{1}{d^{d-2}}$ ; one can obviously not do better, as the identity saturates this. I actually only need the case $d=3$ , and I don't even really need the best constant, but I am curious, and my calculation for $d=3$ is a bit of a mess. Does anyone have a reference for this inequality, ideally the best constants, or a slick proof of this? Update I have done the calculation for all $d$ using Lagrange multipliers with the singular values as coordinates but I would still appreciate either a reference or a slick proof.","['matrices', 'inequality', 'linear-algebra']"
4697360,When is simplicial complex a manifold?,I'm looking for necessary and sufficient conditions for an abstract simplicial complex so that it has a geometric realization that is homeomorphic to a manifold. I can't seem to find a single reference for this online. Any help would be greatly appreciated.,"['general-topology', 'algebraic-topology', 'simplicial-complex']"
4697372,Do we have a formula to calculate the number of subgroups of $S_n$?,"I’ve seen a lot of information about the symmetric group $S_n$ , and, when it comes down to numbers, it is always covered the number of “normal subgroups” or the existence of a “subgroup of order $k$ ”. But, is there a formula to actually “count” the total number of subgroups that $S_n$ will have as a function of the parameter $n$ ?","['symmetric-groups', 'group-theory', 'abstract-algebra']"
4697384,How to solve this system of linear equations using Gaussian elimination?,"I'm having trouble with this problem from my linear algebra course. The problem is: A new restaurant owner decides to have 20 tables for her guests, a certain number of tables with space for 4 people, some with space for 6 people and also one or more tables with space for 8 guests. In total, there are 108 seats available, but if only half of the seats at the 4-tables and 6-tables, and only a quarter of the seats at the 8-tables, are occupied, the restaurant will have 46 guests. How many tables with space for 4, 6 and 8 seats does she need to set up in her premises? I tried to solve it by writing the system of equations as a matrix: $$\left( \begin{array}{cccc}
1&1&1&20\\ 
2&3&2&46\\ 
4&6&8&108
\end{array} \right)
$$ Then I applied Gaussian elimination by doing the following steps: Subtracting 2 times the first row from the second row Swapping the first and third rows Multiplying the first row by 1/4 Subtracting the second row from the third row Subtracting 2/3 times the second row from the first row Subtracting 2 times the third row from the first row Adding the first row to the third row This gave me the following matrix: $$
\left( \begin{array}{cccc}
-1&0&0&-\frac{5}{2} \\ 
0&1&0&6\\ 
0&0&1&\frac{23}{2} 
\end{array} \right)
$$ However, this does not match the answer given in the textbook, which is: $$
\left(\begin{array}{cccc}
1 & 0 & 0 & 10 \\
0 & 1 & 0 & 6 \\
0 & 0 & 1 & 4
\end{array}\right)
$$ Can someone please explain where I went wrong and how to get the correct answer? I appreciate any help or hints. Thank you!","['matrices', 'gaussian-elimination', 'systems-of-equations', 'linear-algebra']"
4697409,A Technicality for an Arbitrary Cartesian Product Intersection Identity,"I'm going through Munkre's Topology book. To set the stage, we want to be able to define a cartesian product for arbitrary index sets. He employs the following definition: Let $\{A_{\alpha} \}_{\alpha \in J}$ be an indexed family of sets; let $X = \bigcup_{\alpha \in J}A_{\alpha}$ . The cartesian product of this indexed family, denoted by $\prod_{\alpha \in J}A_{\alpha}$ is defined to be the set of all J-tuples of elements of $X$ . That is, the set of all functions \begin{equation}
x: J \to \bigcup_{\alpha \in J}A_{\alpha}
\end{equation} such that $x(\alpha) \in A_{\alpha}$ for each $\alpha \in J$ . Alright, this is all well and good. My goal is to show the following identity: Let $\{U_{\alpha}\}_{\alpha \in J}$ and $\{V_{\alpha}\}_{\alpha \in J}$ be two indexed family of sets. Then, \begin{equation}
     \prod_{\alpha \in J}(U_{\alpha} \cap V_{\alpha}) = \prod_{\alpha \in J}U_{\alpha} \cap \prod_{\alpha \in J} V_{\alpha}
\end{equation} Going in one of these directions isn't difficult to do but I've been having trouble with the reverse direction due to a subtlety. I am having difficulty showing that $f \in \prod_{\alpha \in J}(U_{\alpha} \cap V_{\alpha})$ implies $f \in \prod_{\alpha \in J}U_{\alpha} \cap \prod_{\alpha \in J} V_{\alpha}$ . I'll now walk through my attempt until the point where I got stuck:
Suppose that $f \in \prod_{\alpha \in J}(U_{\alpha} \cap V_{\alpha})$ , then $f: J \to \bigcup_{\alpha \in J}(U_{\alpha} \cap V_{\alpha})$ such that $f(\alpha) \in U_{\alpha} \cap V_{\alpha}$ for each $\alpha \in J$ . Therefore, $f(\alpha) \in U_{\alpha}$ and $f(\alpha) \in V_{\alpha}$ . Trivial so far, no issues. Our goal is to simultaneously show that we can write $f: J \to \bigcup_{\alpha \in J}U_{\alpha}$ and $f: J \to \bigcup_{\alpha \in J}V_{\alpha}$ . Here is where it gets nuanced for me. For us to be able to do this requires that $\bigcup_{\alpha \in J}V_{\alpha} = \bigcup_{\alpha \in J}U_{\alpha} = \bigcup_{\alpha \in J}(U_{\alpha} \cap V_{\alpha})$ . Part of what defines a function is its codomain, so these sets must be equivalent for us to proceed with the result, but it seems like there is no reason why these sets should be equal. I will say that it's trivial to construct functions \begin{equation}
     f': J \to \bigcup_{\alpha \in J}U_{\alpha}, \qquad f'': J \to \bigcup_{\alpha \in J}V_{\alpha}
\end{equation} such that $f'(\alpha) = f(\alpha)$ for all $\alpha \in J$ and $f''(\alpha) = f(\alpha)$ for all $\alpha \in J$ . However, $f \neq f' \neq f''$ because the codomains are all different. I was wondering whether I'm missing something or if the notion of a cartesian product requires an amendment, namely something like ""...that it is the set of all surjective functions $x: J \to A$ such that $A \subset \bigcup_{\alpha \in J}A_{\alpha}$ and $x_{\alpha} \in A_{\alpha}$ for all $\alpha \in J$ "".","['elementary-set-theory', 'general-topology']"
4697413,The functional equation $f(x)+f(y)=g(x+y)$ almost everywhere.,"Let $P$ be a probability on $R$ and $f$ and $g$ two  measurable functions such that $f(x)+f(y)=g(x+y)$ almost everywhere for $P(dx)P(dy)$ . How can I prove that there exist two numbers $a$ and $b$ such that $f(x)=ax+b$ almost everywhere for $P(dx)?$ It must be a classical result, but I need some references. Multiplying both sides by $e^{itx+isy} P(dx)Q(dy)$ and integrating does not lead to the solution. Of course if $P$ has a strictly positive density with respect to the Lebesgue measure and if $g=f$ this is the classical Cauchy equation solved ages ago by Mark Kac before 1940.","['probability', 'real-analysis']"
4697441,Shortest path from undergrad to the (co)tangent complex?,"After reading the first two answers to this question , I've become interested in understanding the concept of (co)tangent complex as a way to get some intuition about homotopical algebra, being somewhat more used to the algebro-geometric framework than to the algebro-topological one. More specifically, I'd like to understand this concept in order 'to do basic geometry - this time calculus - on a singular variety', as stated in the first answer (whatever this means), but not 'mechanically', instead trying always to keep an organizing point of view like the one described in the second answer. Also, I'd like to do so following the shortest possible path from basic algebraic geometry and basic category theory directly to the subject matter, with the smallest possible amount of detours, but comprehensively including all the needed basics. (I've studied some scheme theory and homological algebra before, including derived categories, and also ventured a little bit more deeply in the categorical world, but never dealt professionally with these topics and will have to recall a lot before being sufficiently at ease with them.) In this context, what I'm looking for is a double list of topics, one from algebraic geometry and the other from category theory, both ordered by degrees of complexity, designed to be studied in a parallel manner, showing the highest possible level of correspondence since the very beginning, and if possible accompanied with the most up-to-date literature available for this purpose. I'd be very grateful if someone would spend some time thinking about this and writing a nice answer.","['homological-algebra', 'derived-categories', 'higher-category-theory', 'algebraic-geometry', 'schemes']"
4697443,How prove that $AD$ and $BC$ of the given convex quadrilateral are parallel?,"Here is the problem: In the convex quadrilateral $ABCD$ , it is known that $AD > BC$ , points $E$ and $F$ are the midpoints of the diagonals $AC$ and $BD$ , respectively, $EF = \frac{1}{2}(AD - BC)$ . Prove that AD ∥ BC. I thought that I'd proven it, but after a short review I understood that my proof is not correct. That's the short beginning that I think might lead to the whole solution: Let $K$ be the midpoint of the segment $AB$ , draw the $KF$ line segment, $KF \cap AC = X$ . So $KF$ is the midline of the $\triangle ABD$ . Since $(EF = \frac{1}{2}(AD - BC))$ => $(AD = BC + 2EF)$ => $(KF = KX + XF = \frac{1}{2}BC + EF)$ . And now I'm stunned a bit, because intuitively it's clear that the points $E$ and $X$ are both the midpoints of the segment $AC$ , which would help to prove what's asked, but I'm not sure how to prove it. How to do that? Alternative proofs are very welcome. Please, note that this problem is only from an 8th grade school textbook and the answer must be within the given topic, which is simply ""Midline of a triangle"" .","['alternative-proof', 'proof-writing', 'geometry', 'quadrilateral']"
4697450,The change of the difference between solutions,"Consider the following equation for a fixed $\lambda \gt0$ : $$x\lambda^3\sqrt{x}\exp\left(-\frac{x^2\lambda^4}{2}\right) = C, \ \ \ C\gt0$$ I think the equation doesn't have a closed form solution. If we consider, $$f(x)= x\lambda^3\sqrt{x}\exp\left(-\frac{x^2\lambda^4}{2}\right)$$ then by taking derivative it can be seen that $f(x)$ has a global maxima at $x^*=\frac{\sqrt{3/2}}{\lambda}$ . This means geometrically that we are intersecting a Gaussian-like function $y=f(x)$ with $y=C$ for every $\lambda$ . So there are two solutions which are $x_1\lt x^*$ and $x_2>x^*$ . Define the difference between these solutions as $d=x_2-x_1$ . I want to show that if $\lambda_1\lt \lambda_2$ then $d_1\gt d_2$ . In words, if we increase the parameter $\lambda$ then the difference between solutions decrease. I tried to show that $x_1\lt x_1'$ but it didn't work. By $x_1'$ , I mean the smaller solution for the case $\lambda=\lambda_2$ . Is it possible to use the derivative for proving the statement somehow? Maybe we can differentiate with respect to $\lambda$ and then argue about the solutions of the equation.","['calculus', 'derivatives', 'algebra-precalculus', 'real-analysis']"
4697493,Bessel function integral representation using Hankel's contour,"This question appears in a set of  old qualifying exams I am using to practice. The Hankel path $H(c,R)$ is an infinite path that starts at a point at infinity $-\infty - ic$ to a point on a circle of $C_R$ radius $R$ ( $0<c<R$ ) centered at the origin along the line $y=-c$ , continuing along the an arc on $C_R$ to a point on the line $y=c$ , and then to the point at infinity $\infty+ic$ along the line $y=c$ . The problem asks us to show that the Bessel function $J_\nu$ of order $\nu\in\mathbb{C}$ can be expressed as $$ J_\nu(z)=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-\nu-1}e^{\frac12 z\left(\lambda-\frac1\lambda\right)}\ d\lambda,\qquad \mathfrak{R}(z)>0$$ In the first part of the problem, we prove that the Gamma function satisfies $$ \Gamma(z)=\frac{1}{2 i\sin(\pi z)}\oint_{H(c,R)}\lambda^{z-1} e^\lambda\ d\lambda, \qquad z\in\mathbb{C}\setminus\mathbb{Z}.$$ Using this in combination with Euler's reflection formula $\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin \pi z}$ gives $$\frac{1}{\Gamma(z)}=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-z}e^\lambda \ d\lambda \qquad z\in\mathbb{C}.$$ Using Cauchy's integration theorem, we also have that the formulas above are independent of $c$ and $R$ , i.e. we can deform $H(c,R)$ to any other Hankel path $H(c',R')$ . This allows us to  rewrite $J_\nu$ as $$J_\nu(z)=\sum^\infty_{n=0}\frac{(-1)^n}{n!\Gamma(n+\nu+1)}\Big(\frac{z}{2}\Big)^{\nu+2n}=
\frac{1}{2\pi i}\sum^\infty_{n=0}\frac{(-1)^n}{n!}\Big(\frac{z}{2}\Big)^{2n+\nu}\oint_{H(c,R)}\lambda^{-\nu-n-1}e^\lambda \ d\lambda.$$ By analytic continuation, it suffices to assume $z>0$ .
A change of variables $\lambda=\frac12 zw$ deforms $H(c,R)$ to $H(2z^{-1}c, 2z^{-1}R)$ . This change of variables, provided that order of summation and integration can be inverted, gives $$J_\nu(z)=
\frac{1}{2\pi i}\oint_{H(2z^{-1}c,2z^{-1}R)}\sum^\infty_{n=0}\frac{(-1)^n}{n!} \Big( \frac{zw^{-1}}{2}\Big)^n w^{-\nu-1}e^{\frac12 zw} \ dw.$$ The desired formula follows easily from this (Cauchy integration type arguments allow us to deform the path $H(2z^{-1}c,2z^{-1}R)$ back to $H(c,R)$ ). Problem: I am having trouble justifying the change in the order of summation and integration. I am trying to use dominated convergence type of arguments but I am not getting very far. Any hints are appreciated. Thanks!","['integration', 'analysis', 'real-analysis', 'complex-analysis', 'bessel-functions']"
4697524,How to find a non-singular hypersurface in $\mathbb P^n$ containing a certain curve,"Assume that $X$ is a non-singular projective curve in $P^n_k$ , where $k=\bar{k}$ and $n \geq 3$ . Prove that for every $m$ large enough, there always exists a non-singular hypersurface of deg $m$ containing $X$ . The hardest part of this problem is that you should properly choose conditions to restrict these hypersurfaces in order to make them non-singular. The existence of hypersurfaces containing $X$ can be shown if we consider the exact sequence $0 \rightarrow \mathcal{I}_X(m) \rightarrow \mathcal{O}_{P^n}(m) \rightarrow \mathcal{O}_X(m) \rightarrow 0$ , take global sections and then estimate corresponding dimensions. But, you know, the greater m is, the more hypersurfaces containing $X$ there will be. So I need a good interpretation of non-singular hypersurfaces in the language of cohomology. I think another possible approach is that we can construct a new model of this problem where hypersurfaces of deg $m$ containing $X$ form a nice variety. Then the non-singular ones are 'general' in this variety, like Bertini's theorem in Hartshorne's Algebraic Geometry.",['algebraic-geometry']
4697552,The image of a Riemannian submanifold under a diffeomorphism,"Let $\Sigma$ be a compact submanifold of a Riemannian manifold $(M,g)$ , let $h$ be the induced metric on $\Sigma$ , and let $\Phi:M\to M$ be a diffeomorphism. $h_\Phi$ will denote the induced metric on the image $\Phi(\Sigma)$ , pulled back to $\Sigma$ via $\Phi$ . What is the $h_\Phi$ really? I know $\Phi(\Sigma)$ is an embedded submanifold of $M$ with the property that $\Phi$ maps $\Sigma$ diffeomorphically onto $\Phi(\Sigma)$ , which begs a question:do the Riemannian metrics on $\Phi(\Sigma)$ induced respectively by the inclusion $\iota|_{\Phi(\Sigma)}:\Phi(\Sigma)\hookrightarrow M$ and the restriction $\Phi|_\Sigma$ coincide? I supposed that $h_\Phi$ is the Riemannian metric induced by $\iota|_{\Phi(\Sigma)}$ and tried to show that $$\Phi|_\Sigma^*h_\Phi=h$$ but didn't find a way. The following is as far as I can go now: $$\Phi|_\Sigma^*h_\Phi=\Phi|_\Sigma^*(\iota_{\Phi(\Sigma)}^*g)=(\iota_{\Phi(\Sigma)}\circ\Phi|_\Sigma)^*g=???=\iota_\Sigma^*g=h$$ $\iota_\Sigma$ is the inclusion $\Sigma\hookrightarrow M$ . Thanks for help. Edit. I think the author was trying to say $$h_\Phi=(\Phi|_\Sigma^{-1})^*h.$$ That way, we would have $$h=\Phi|_\Sigma^*h_\Phi.$$ This is pretty much the scenario described in the quote.","['submanifold', 'riemannian-geometry', 'differential-geometry']"
4697553,"Let $(M, g) = (\mathbb{R}^n , ds^2 = dr^2 +f^2(r)d\theta^2)$. Determine the Riemannian volume form of $M$.","Let $(M, g) = (\mathbb{R}^n , ds^2 = dr^2 +f^2(r)d\theta^2)$ where $d\theta^2$ is the induced Riemannian metric from $\mathbb{S}^{n-1}$ . Determine the Riemannian volume form of $M$ . The volume form is in local coordinates given by $$\omega = \sqrt |g| dx^1 \wedge \dots \wedge dx^n$$ so the whole problem reduces to finding $\sqrt{|g|}$ where $|g|$ is the absolute value of the determinant of the metric tensor. Now in this case $g = ds^2$ so we are trying to find the matrix representation for $ds^2$ . This is where I'm stuck I think we should end up with a matrix $g_{ij}$ , but since this depends on $d\theta^2$ also I don't know how to find this matrix. Any help would be appreciated.","['riemannian-geometry', 'differential-geometry']"
4697589,Question about zeros of finite order functions [duplicate],"This question already has an answer here : $f$ is entire without any zeros then there is an entire function $g$ such that $f=e^g$ (1 answer) Closed last year . Context, I was reading Joseph Bak and Donald J. Newman's Complex Analysis as a refresher on complex analysis (I have some prior experience but am washed) and I was attempting to understand theorem 16.13. Suppose $f$ is an entire function of finite order. Then either $f$ has infinitely many
zeroes or $$f (z) = Q(z)e^{P(z)}$$ where $Q$ and $P$ are polynomials. In particular, the following step, let $f$ be entire and of finite order (bounded in magnitude by $A|z|^n$ for some integer $n$ and real constant $A$ ). The book goes on to say that if $f$ has finitely many zeros, then $f(z) = Q(z)g(z)$ where: $$Q(z):=(z-z_1)\cdots(z-z_n)$$ Where $z_1,\dots,z_n$ are the zeros of $f$ and $g$ is nonzero and holomorphic. Then it goes on to claim that $\ln(g)$ is entire. This is where the argument loses me. I understand that $g$ is nonzero, but that doesn't mean that the image of $g$ doesn't cross the branch cut of $\ln$ . I'm guessing that they want to define a branch cut of $\ln$ such that this is true but I cannot understand what specific cut they are using. Moreover, $g$ , being holomorphic would attain every value (except $0$ by assumption) by Picard's Theorem, so how exactly are they defining this branch cut here? Or is there something else that I am missing? If this argument is indeed flawed, how can it be fixed? Image of original text attached for context, page 237:",['complex-analysis']
4697599,Find the invariant manifolds of the equilibrium,"Consider the system $$\begin{cases}\dot{x}=x+y\cos(y)\\
\dot{y}=-y
\end{cases}$$ which has the unique equilibrium point $(0,0)$ . I want to find the invariant manifolds for this system. The Jacobian at the origin is $$\begin{pmatrix}
1&1\\
0&-1
\end{pmatrix}$$ which has eigenvalues $\lambda_1=1$ and $\lambda_2=-1$ . An eigenvector for $\lambda_1$ is $v_1=(1,0)^\intercal$ and for $\lambda_2$ , an eigenvector is $v_2=(1,-2)^\intercal$ . It is clear that the unstable manifold is the unstable subspace $\operatorname{span}(v_1)$ aka the $x$ -axis. However, I am completely lost on how to find the stable manifold. How does one go about finding this?","['set-invariance', 'ordinary-differential-equations', 'dynamical-systems']"
4697604,is there a simple notation for the subtraction of a member from a set?,"sometimes we use notation $S;x$ as an abbreviation for $S\cup \lbrace x\rbrace$ . (H.B. Enderton, Mathematical Introduction to Logic .) but we also sometimes need delete a member from a set. for example, in $\epsilon-\delta$ definition of limit, we have to consider a domain $(x-\delta,x+\delta)-\lbrace a\rbrace$ to decide $\lim_{x\to a}f(x)$ . so I think there might(should) be a simple notation for this kind of operation. is there any notation for this kind of operation?","['elementary-set-theory', 'calculus', 'notation']"
4697613,"Is there a formula for $\sum_{j=1}^n \binom{m j}{m}$, where $m$ is an integer?","Is there a formula for $\sum_{j=1}^n \binom{m j}{m}$ , where $m$ is an integer? It's similar to the Hockey-stick-identity $$ \sum_{j=1}^{mn} \binom{j}{m} = \binom{mn+1}{m+1}, $$ but now we only want to consider every $m$ -th summand. Is there a formula or an upper bound (that's better than $\binom{mn+1}{m+1}$ )?","['summation', 'binomial-coefficients', 'combinatorics']"
4697617,Population of a city doubles in $50$ years. In how many years will it triple?,"The population of a city doubles in $50$ years. In how many years will it triple, under the assumption that the rate of increase is proportional to the number of inhabitants? This was a pretty trivial question. But, the thing is that when I tried solving this as: We have, the population of a city becoming double in $50$ years. This means, that the population increases by $100$ percent in $50$ years. This hints at the fact that the population was increasing at the rate of $2$ percent per year. So, we consider the initial population as $P$ . Let the number of years in which it becomes triple is $y$ years. Then, we have the following equation holding true, $$\left(P+\frac{2Py}{100}\right)=3P\implies y=100\text{years}.$$ So, I thought, the population will become thrice in $100$ years. But here, comes the problem. The answer given suggests the population becomes thrice in $50\log_23$ years. I don't understand the how is it so? Have I done some error/or forgotten to take something into consideration?",['algebra-precalculus']
4697628,$\sum_{n=1}^\infty\ln(1+u_n)$ converges but $\sum_{n=1}^\infty u_n$ diverges,"We know that if $\sum\limits_{n=1}^\infty u_n^2<+\infty$ is convergent, then both $\sum\limits_{n=1}^\infty u_n$ and $\sum\limits_{n=1}^\infty \ln(1+u_n)$ converge or diverge simultaneously. If $\sum\limits_{n=1}^\infty u_n$ converges and $\sum\limits_{n=1}^\infty u^2_n=+\infty$ , then $\sum\limits_{n=1}^\infty \ln(1+u_n)=-\infty$ ；If $\sum\limits_{n=1}^\infty \ln(1+u_n)$ converges and $\sum\limits_{n=1}^\infty u^2_n=+\infty$ , then $\sum\limits_{n=1}^\infty u_n=+\infty$ . My question is that: can we construct a sequence $\{u_n\}$ such that $\sum\limits_{n=1}^\infty\ln(1+u_n)$ is convergent but $\sum\limits_{n=1}^\infty u_n$ is divergent? In fact, it is suffices to construct $\{u_n\}$ such that $$\sum_{n=1}^\infty u_n=+\infty,\quad \sum_{n=1}^\infty \frac{u^2_n}{2}=+\infty,\quad
\sum_{n=1}^\infty|u_n|^3<+\infty$$ and $$\sum\limits_{n=1}^\infty\left(u_n-\frac{u^2_n}{2}\right) \text{is convergent.}$$ Does such a sequence exist？","['analysis', 'real-analysis']"
4697701,Unitary matrix corruption,Suppose I have got a unitary matrix and I want to introduce random noise to simulate data corruption. How to introduce the noise in a proper way such that the corrupted matrix is also unitary?,"['matrices', 'unitary-matrices', 'noise']"
4697739,The points M and N and the circle k are given. Construct an equilateral triangle ABC inscribed in circle k such that |AM|=|BN|.,"I chose an arbitrary length r and constructed two circles with centers in M ​​and N of radius r and marked their intersections with the given circle. Then I looked at the angle formed by the center of the given circle and those two intersections. I found the bisector of that angle and marked the intersection of the bisector and the circle with S. Finally, I found points A and B on the given circle such that triangles ASO and BS0 are equilateral, where O is the center of the given circle. However, I get that at such construction, |AM| is only approximately equal to |BN|. How can I solve this problem?","['constructive-mathematics', 'euclidean-geometry', 'geometry']"
4697741,A System of Two Linear PDEs,"I'm researching on discrete/semi-discrete/smooth differential geometry. Recently, I could simplify one of my geometric problems (in the smooth scenario) into the solutions of a system of linear PDEs that is $$ 
e_v = L_u\,g,\quad\quad\quad g_u = L_v\,e
$$ where $L = \ln(\tan{(\frac{\omega}{2})})$ and $e(u,v),g(u,v),\omega(u,v): [0,a]\times [0,b] \subset \mathbb{R}^2 \rightarrow \mathbb{R}$ are smooth functions on $(0,a)\times (0,b)$ . Furthermore, I have the functions $e(u,0)$ and $g(0,v)$ and $\omega(u,v)$ as well.
My goal is to show that the system has a solution for $e(u,v)$ and $g(u,v)$ .
Unfortunately, my knowledge of PDEs is very limited but just from observing the counterpart of the above system in discrete and semi-discrete scenarios I have this kind of feeling that there should be a solution for the above system (with the boundaries that I mentioned). Can someone clarify how I can prove that there exists a unique solution to the above system. I would very much appreciate if you can also go to details, describing non-characteristic curves and explaining why $e(u,0)$ and $g(0,v)$ are actually enough as boundary conditions (if they are). I think I also have to assume $e(u,v)$ and $g(u,v)$ are analytic functions. Is there any way to avoid this? Please let me know if you need more details so that I can edit the question.","['systems-of-equations', 'partial-differential-equations', 'differential-geometry']"
4697812,Is there a simple systematic way to build a Bishop frame?,"I'm modelling a parametrized braid bracelet in openSCAD (see picture) and in a previous question, some people suggested that I might solve a torsion problem that was caused by using the Frenet-Serret Frame, by using the Bishop frame instead. They gave me some ressources (Mostly the original paper from Bishop, a course by Ted Shifrin and I also found a few others online). But since I'm not very good in vector fields and differential geometry (I'm mostly self taught on this topic) and since I couldn't find anything in French (Even though I understand English, things keep being easier to understand in my mother tongue ;-) ), I had a hard time figuring out how to practically build this Bishop frame from the document I got (I need something that can give a symbolic solution, because, since my problem is circular, a numerical solution will lead to a problem of raccord. I finally solved my problem by ""inventing"" my own frame which has it's own disadvantages, but even though it's not perfect, it solves the problem caused by Frenet-Serret frames. However, I believe I understood enough of the Bishop frame to see how useful it would be and would really be glad to understand how to practically build it. What i'm looking for is a systematic method of this sort (examples): Frenet-Serret: take the derivative of your coordinates dx(t)/dt, dy(t)/dt, dz(t)/dt, and divide the resulting vector by it's norm to make it evolve on a sphere. This is called the tangent vector . Take the derivative of the coordinates of the tangent vector. Since the tangent vector evolve on a sphere, this one in perpendicular. Divide it again by it's own norm (won't work if it's null). This is called the normal vector . Take the cross product of both the tangent and the normal vectors. As those are perpendicular and unitary, this last vector is also unitary and perpendicular to both the others. This is called the binormal vector . My solution...: is the same as Frenet-Serret, except for step 2 where I compute the first normal vector by taking the cross product of the tangent with one vertical vector and I divide it by it's own norm. This requires that the tangent vector of my curve never gets vertical. So my question is: Where can I find some ""HOWTO"" that explains how to build a Bishop frame in the same comprehensible/systematic way as in my examples above ? (It might also help to understand the theorems)","['curves', 'vector-fields', 'differential-geometry']"
4697827,The permutation matrices are the doubly stochastic matrices with the highest Frobenius norm,"In a 2013 talk , Alexandre d'Aspremont did claim the following: Among all doubly stochastic matrices, the rotations, hence, the permutation matrices, have the highest Frobenius norm I had never encountered this result.  Searching for it on Mathematics SE produced nothing of interest.  Assuming this result is indeed correct, a reference would be most appreciated. Related: How would you encourage an orthogonal matrix to be a permutation matrix? Which probability mass function has the largest Euclidean norm?","['permutation-matrices', 'reference-request', 'matrices', 'stochastic-matrices', 'birkhoff-polytopes']"
4697828,"How many 10-bit words can be created with precisely four units (e.g., 1111000000)?","I think the answer is $$\frac{10!}{4!6!}.$$ This is because we can arrange $10$ objects in $10!$ ways, but then have to divide by $4!6!$ because the four $1$ 's are identical and the six $0$ 's are identical. According to the website I got this problem from, I am incorrect: https://www.hackmath.net/en/math-problem/41001?result=1 What is the verdict? Just so I can leave a comment for other users of the site.",['combinatorics']
4697851,A uniformly bounded sequence of analytic functions converging on the boundary of the domain? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Suppose that we have a sequence of analytic functions $f_n:D(0,\rho_n)\to\mathbb C$ where $(\rho_n)_n$ is a decreasing sequence of real numbers $>1$ that converges to $1$ . Assume furthermore that $|f_n(z)|\leq M$ for all $z\in D(0, \rho_n)$ and all $n=1,2,\ldots$ Suppose that $f_n$ converges to an analytic function $f$ on the open unit disc, uniformly on compact subsets. Question : Does $f_n$ converge (pointwise) on the unit circle?","['complex-analysis', 'uniform-convergence', 'limits', 'equicontinuity', 'analytic-functions']"
4697871,Which probability mass function has the largest Euclidean norm?,"Fix $n > 1$ and let $[n] := \{ 1, 2, \dots, n \}$ . Which probability mass function (PMF) over $[n]$ has the largest $2$ -norm? Doodling for the cases $n \in \{2,3\}$ does suggest that the maximal $2$ -norm is attained when the support of the PMF is a singleton, i.e., at the ""corners"" of the probability simplex the minimal $2$ -norm is attained when the PMF is uniform over $[n]$ In essence, we have the following (non-convex) quadratic program (QP) $$ \begin{array}{ll} \underset {{\bf x} \in \Bbb R^n} {\text{maximize}} & \| {\bf x} \|_2^2 \\ \text{subject to} & {\bf 1}_n^\top {\bf x} = 1 \\ & {\bf x} \geq {\bf 0}_n \end{array} $$ One way of getting rid of ${\bf x} \geq {\bf 0}_n$ is to introduce $n$ new variables $y_i^2 := x_i$ and rewrite the QP above as follows $$ \begin{array}{ll} \underset {{\bf y} \in \Bbb R^n} {\text{maximize}} & \sum\limits_{i=1}^n y_i^4\\ \text{subject to} & \sum\limits_{i=1}^n y_i^2 = 1 \end{array} $$ where there is a single equality constraint. We define the Lagrangian $$ \mathcal L ({\bf y}, \mu) := \frac14 \sum\limits_{i=1}^n y_i^4 - \frac{\mu}{2} \left( \sum\limits_{i=1}^n y_i^2 - 1 \right) $$ Differentiating and finding where the partial derivatives vanish, we obtain $$ \begin{aligned} y_1 \left( y_1^2 - \mu \right) &= 0 \\ &\vdots \\ y_n \left( y_n^2 - \mu \right) &= 0 \\ \sum\limits_{i=1}^n y_i^2 &= 1 \end{aligned} $$ Note that $y_i = 0$ or $y_i^2 = \mu$ . Let $\operatorname{card} ({\bf y})$ denote the cardinality of the support, i.e., the number of non-zero entries of $\bf y$ . Hence, $$\sum\limits_{i=1}^n y_i^2 = \mu \operatorname{card} ({\bf y}) = 1$$ and, thus, $\color{blue}{x_i \in \left\{ 0, \dfrac{1}{\operatorname{card} ({\bf x})} \right\}}$ . Note that $\| {\bf x} \|_2^2 = \dfrac{1}{\operatorname{card} ({\bf x})}$ , which is maximal when $\color{blue}{\operatorname{card} ({\bf x}) = 1}$ . Is this correct?  If so, is there a more elegant way of showing that ${\bf x}_{\max} \in \{ {\bf e}_1, {\bf e}_2, \dots, {\bf e}_n \}$ ? Related Maximizing a positive semidefinite quadratic form over the standard simplex Analytical solution to a quadratic program over the standard simplex","['quadratic-programming', 'probability-distributions', 'multivariable-calculus', 'non-convex-optimization', 'optimization']"
4697879,Is there a way to characterize sets $S \subseteq \mathbb{N}$ where $S + S = \mathbb{N}$?,"I came across the following problem after a colleague and I were discussing nonregular languages whose concatenation is regular. If $A, B \in P(\mathbb{N})$ , we can define their Minkowski sum as $$A + B = \{ m + n \ | \ m \in A \land n \in B \}\text.$$ For the purposes of this problem, assume $0 \in \mathbb{N}$ . Some sets $S \subseteq \mathbb{N}$ have the property that $S + S = \mathbb{N}$ . For example: The set $\mathbb{N}$ itself. The set of all natural numbers except $137$ . The set of all natural numbers not congruent to $2$ modulo $3$ . The odd natural numbers, along with 0. The set of all natural numbers that are the sums of two squares (a consequence of Lagrange’s Four Square Theorem). I haven’t found a principled way to discover sets like this and have been approaching finding them largely as an exercise in trial and error. Is there an alternative exact characterization of these sets that makes it quick and easy to discover them?","['elementary-set-theory', 'elementary-number-theory', 'recreational-mathematics', 'additive-combinatorics']"
4697903,Is linearisation of ODE around a stable equilibrium always justified?,"Let $f:\mathbb R^n\to \mathbb R^n$ smooth. Let $\hat y\in\mathbb R^n$ be a stable equilibrium point for the ODE $y'(t)=f(y(t))$ . Namely : $f(\hat y)=0$ , for every $\epsilon>0$ there exists $\delta>0$ such that $|y(t)-\hat y|<\epsilon\ \ \forall\,t\geq0$ provided that $|y(0)-\hat y|<\delta$ . Now fix $\epsilon>0$ , fix $y_0$ such that $|y_0-\hat y|<\delta$ , and consider the Cauchy problem $$ y'(t)=f(y(t)) \ ,\quad y(0)=y_0 \ .$$ Denote by $A:=D f(\hat y)$ the jacobian matrix of $f$ at the equilibrium point $\hat y$ and consider the linearized problem around $\hat y$ : $$ z'(t) = A\, z(t)\ ,\quad z(0)=y_0-\hat y\ .$$ Suppose the matrix $A$ is negative definite .
I would like to say that the linearized problem is a good approximation for the original one. A formal question could be something like: is there a uniform constant $C$ such that $$ |y(t)-\hat y-z(t)| \leq C\,\epsilon^2 \quad\forall t\geq0 \quad\forall y_0\in B(\hat y,\delta) \ ?$$ Tentative solution .
Let $r>0$ and $\epsilon\in(0,r)$ . By Taylor expansion of $f$ around $\hat y$ , there exists a constant $C=C(r)$ such that $$ f(y) = A\,(y-\hat y) + \omega(y)\ ,\quad |\omega(y)|\leq C\, |y-\hat y|^2\ \forall y\in B(\hat y,r)\ .$$ Therefore $$ \frac{d}{d t}\big(y(t)-\hat y-z(t)\big) \,=\, A\,\big(y(t)-\hat y-z(t)\big) \,+\, \omega(y(t))\ .$$ Solving this as a linear equation in $y-\hat y-z$ with non-homegenous term $\omega(y(t))$ gives $$ y(t)-\hat y-z(t) \,=\, e^{tA}\,\big(y(0)-\hat y-z(0)\big) \,+\, \int_0^te^{(t-s) A}\,\omega(y(s))\,ds$$ The first term of the sum is zero. I'd like to bound the second term using the stability of equilibrium, indeed: $$ |\omega(y(s))| \leq C\, |y(s)-\hat y|^2 \leq C\,\epsilon^2 \quad\forall s\geq0 \,$$ and if could take this term out of the integral I would be left with $$ \Big\| \int_0^te^{(t-s) A} ds \Big\| \,=\, \| A^{-1}(e^{tA}-I) \| \,\leq\, \|A^{-1}\| $$ obtaining a uniform bound in $t\geq0$ , of the type $$ |y(t)-\hat y-z(t)| \,\leq\, C\,\|A^{-1}\|\,\epsilon^2 \ .$$ The problem is I don't see how to split the remainder $\omega(y(s))$ from the rest of the integral.","['stability-in-odes', 'stability-theory', 'ordinary-differential-equations']"
4697927,Prove or disprove that a piecewise complex function has a derivative at $0$,"Prove or disprove that the function: $$ f(x+iy)=\left\{\begin{matrix} \sqrt{xy}\: \: \: \: \text{ if } \space xy\geq 0
\\ i\sqrt{-xy}\: \: \: \: \text{ if } \space xy<  0
\end{matrix}\right. $$ has a derivative at $0.$ I'm pretty sure that the function doesn't have derivative at $0.$ I tried showing that by using Cauchy–Riemann equations: for $\\xy \geq 0:$ $$\\u_{x}(x,y)=\frac{\sqrt{y}}{2\sqrt{x}}
\\u_{y}(x,y)=\frac{\sqrt{x}}{2\sqrt{y}}
\\v_{x}(x,y)=0
\\v_{y}(x,y)=0$$ for $\\xy \geq 0:$ $$
\\u_{x}(x,y)=0
\\u_{y}(x,y)=0
\\v_{x}(x,y)=-\frac{\sqrt{y}}{2\sqrt{-xy}}
\\v_{y}(x,y)=-\frac{\sqrt{x}}{2\sqrt{-xy}}
$$ so: $$
\\0=\frac{\sqrt{y}}{2\sqrt{x}}
\\0=\frac{\sqrt{x}}{2\sqrt{y}}
$$ and: $$
\\0=-\frac{\sqrt{y}}{2\sqrt{-xy}}
\\0=-\frac{\sqrt{x}}{2\sqrt{-xy}}
$$ so for $(x,y)=(0,0)$ the equations are true. Furthermore, I tried applying the limit definition of the derivative $\lim_{(x,y) \to (0,0)}\frac{f(x+iy)-f(0)}{x+iy-0}$ but I encountered difficulties dealing with the piecewise function. How should I approach to solving this kind of piecewise function problem over the complex plane?","['multivariable-calculus', 'calculus']"
4697961,Covariant derivative of orthonormal frames,"Given a $2$ dimensional riemannian manifold with a local orthonormal frame $e_1,e_2$ I want to evaluate the covariant derivatives $\nabla_{e_1}e_1$ , $\nabla_{e_2}e_1$ , $\nabla_{e_1}e_2$ and $\nabla_{e_2}e_2$ , assuming the Levi-Civita connection. I am using two methods and finding different results, so I would like to understand where I am wrong. If I express the covariant derivative with the connection form $\omega^i_j$ associated to the frame, defined by $\nabla_{X}e_j= \Sigma \omega^i_j (X)e_i$ I get the following equations $$\nabla_{e_1}e_1 = \omega_1^1(e_1)e_1+\omega_1^2(e_1)e_2$$ $$\nabla_{e_2}e_1 = \omega_1^1(e_2)e_1+\omega_1^2(e_2)e_2$$ $$\nabla_{e_1}e_2 = \omega_2^1(e_1)e_1+\omega_2^2(e_1)e_2$$ $$\nabla_{e_2}e_2 = \omega_2^1(e_2)e_1+\omega_2^2(e_2)e_2$$ as the connection matrix $\omega_i^j$ with respect to an orthonormal frame is skew symmetric, the previous equations further simplify as $$\nabla_{e_1}e_1 = -\omega^1_2(e_1)e_2$$ $$\nabla_{e_2}e_1 = -\omega^1_2(e_2)e_2$$ $$\nabla_{e_1}e_2 = \omega^1_2(e_1)e_1$$ $$\nabla_{e_2}e_2 = \omega^1_2(e_2)e_1$$ However, I am not fully conviced because if I instead use the formula for the covariant derivative in local coordinates I get a seemingly different result. Assuming two general vector fields $v=v^je_j$ and $u=u^ie_j$ the formula says: $$\nabla_v u= (v^ju^i\Gamma^k_{ij}+v^j\frac{\partial u^k}{\partial x^j})\frac{\partial}{\partial x^k}$$ To find a local expression for an orthonormal frame, I assume that in the coordinates $x^1,x^2$ the metric is expressed by the first fundamental form $[\begin{smallmatrix} E & F \\ F & G \end{smallmatrix}]$ . Then an orthonormal frame is given by (D is the determinant of the matrix): $$(\frac{1}{\sqrt{E}}\frac{\partial}{\partial x^1}, \frac{-F}{\sqrt{ED}}\frac{\partial}{\partial x^1}+\sqrt{\frac{E}{D}}\frac{\partial}{\partial x^2})$$ . In particular, $\frac{1}{\sqrt{E}}\frac{\partial}{\partial x^1}$ has norm $1$ for the metric in question. My problem is that if now calculate $\nabla_{e_1}e_1$ I don't get an expression only in $\frac{\partial}{\partial x^2}$ as I would have expected, as I get $$\nabla_{\frac{1}{\sqrt{E}}\frac{\partial}{\partial x^1}}(\frac{1}{\sqrt{E}}\frac{\partial}{\partial x^1})=\frac{1}{E}\Gamma^1_{11}\frac{\partial}{\partial x^1}+\frac{1}{E}\Gamma^2_{11}\frac{\partial}{\partial x^2}-\frac{1}{2}\frac{E_1}{E^2}\frac{\partial}{\partial x^1}$$ Further expanding the coefficient for $\frac{\partial}{\partial x^1}$ and using the expressions for the Christoffel symbols in coefficients of the first fundamental form I get $$\frac{GE_1-2FF_1+FE_2}{2E(EG-F^2)}\frac{\partial}{\partial x^1}-\frac{1}{2}\frac{E_1}{E^2}\frac{\partial}{\partial x^1}$$ Which does not seem to vanish to me, unless I am mistaken. Where am I wrong? applying the connection form and then assuming that $\nabla_{e_1}e_1$ is a multiple of $e_2$ only assuming that the vector fields above are an orthonormal frame applying the formula for the covariant derivative in local coordinates thanks!","['connections', 'differential-geometry']"
4697975,Method of steepest descent for $\cos$ integral,"I'm looking to do an asymptotic analysis for the following integral (similar to this but not quite the same) $$J(x)=\int_0^1 \cos(x (t^3/3+t))dt.$$ My idea is to write $$J(x)=\Re\int_\Gamma \exp(xi(z^3/3+z))dz$$ where $\Gamma=[0,1]$ . Now, we note that if $f(z)=i(z^3/3+z)$ , then $f'(z)=0$ at $z=\pm i$ and $f''(i)=2i(i)=-2<0$ . Next, we note that $$f(u+iv)=\frac{v (-3 u^2 + v^2 - 3)}{3} + i\frac{ u (u^2 - 3 v^2 + 3)}{3}$$ and hence we expect that the biggest contribution will come from the saddle point $x=i$ along a level curve of the contour curve $\Im f = c $ , and so I'd like to deform the contour $\Gamma$ to somehow take this into account, but I don't know how to proceed. Any hints are helpful, this is especially tricky (to me at least) since the bounds are finite.","['integration', 'definite-integrals', 'asymptotics', 'complex-analysis', 'contour-integration']"
4697982,Prove : $\int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1$,"I want to prove the following inequality : $$I = \int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1$$ Since I have been only introduced to elementary methods of integration, the indefinite integral appears non-solvable to me (I would be glad to see if there's a neat expression for indefinite integral). To evaluate the integral, I used the property $\int_a^b f(x) \mathrm dx = \int_a^b f(a+b-x) \mathrm dx$ , to get $I = \int_0^1 \frac{ e^{\sin^2(1-x)}}{x^2 - 2x + 2}\mathrm dx$ which looks much uglier. Further, the limits of integral are not symmetrical with respect to $x=0$ , hence the fact that the integrand is an even function is useless. Please tell that how can I approach this inequality. Thanks !","['calculus', 'definite-integrals', 'inequality']"
4697988,Every Haar measure is a multiple of the counting measure.,"I was asked to prove the following: Let $G$ be a group, equipped with the discrete topology.
A Haar measure on $G$ is a measure $\mathcal{P}(G) \rightarrow [0, \infty]$ such that: $$\mu(K) < \infty$$ for all compact sets K
and $$\mu(gU) = \mu(U) \; \forall g \in G \; \forall U \subset G$$ Then $\mu = a*z$ where $z$ is the counting measure and $a \in \mathbb{R}_0^+$ The compact sets are all finite sets and therefore this is not too difficult to show if the measure for finite sets is non zero. But what about the following measures on $\mathbb{R}$ as a group with addition and the discrete topology $$\mu(U) = 0 \; \forall U \subset \mathbb{R}$$ $$\mu(U) = 0 \text{ if U is countable else } \mu(U)=\infty  $$ They both seem to be Haar measures to me, but they cant both be a multiple of the counting measure. Am I missing something?","['measure-theory', 'haar-measure']"
4698040,"$\lim_{n\to\infty}n^2(\int_{0}^1 \sqrt[n]{x^n+(1-x)^n}\,dx-\frac{3}{4})$","I saw this problem in a problem book. and I have no idea how to find the limit here. $$\lim\limits_{n\to\infty}n^2\left(\int\limits_{0}^1 \sqrt[n]{x^n+(1-x)^n}\,dx-\frac{3}{4}\right)$$ I know that $$\lim\limits_{n\to\infty}\int\limits_{0}^1 \sqrt[n]{x^n+(1-x)^n}\,dx =\frac{3}{4}$$ proof $$  \int _{0} ^ {\frac{1}{2}}  \sqrt[n]{(1-x)^n}dx   + \int _{{\frac{1}{2}}} ^ 1  \sqrt[n]{x^n}dx= \frac{3}{2} \leq \int _0 ^ 1  \sqrt[n]{x^n+(1-x)^n}dx \leq  \int _{0} ^ {\frac{1}{2}}  \sqrt[n]{2(1-x)^n}dx   + \int _{{\frac{1}{2}}} ^ 1  \sqrt[n]{2x^n}dx= \sqrt[n]{2}\frac{3}{2}$$ since $\lim_{n \to \infty }\sqrt[n]{2}=1 $ thenby squeeze theorem $\lim\limits_{n\to\infty}\int\limits_{0}^1 \sqrt[n]{x^n+(1-x)^n}\,dx =\frac{3}{4}$ by attempt was to use the fact that $I<\sqrt[n]{2}\frac{3}{4}$ so $$I-\frac{3}{4}<\frac{3}{4}(\sqrt[n]{2}-1)$$ $$n^2(I-\frac{3}{4})<n^2(\frac{3}{4}(\sqrt[n]{2}-1))$$ but $$\lim\limits_{n\to\infty}n^2(\frac{3}{4}(\sqrt[n]{2}-1)) =\infty $$ and here I got stuck","['integration', 'definite-integrals', 'analysis', 'calculus', 'limits']"
4698042,Integral over the surface of a paraboloid,"The integral I'm trying to solve is the following: $$ \iint_S xyz \,d\sigma $$ Where $z=x^2+y^2$ and $0<z<1$ . So I transform the integral to a double integral: $$\iint_Sxy(x^2+y^2)\sqrt{1+4(x^2+y^2)}\,dx\,dy$$ After that I transform it to polar coordinates with jacobian $J=\rho$ and get the following: $$\int_{0}^{1}\int_{0}^{2\pi}\rho^5\cos(\phi)\sin(\phi)\sqrt{1+4\rho^2}\,d\phi\,d\rho$$ However the $\phi$ part of the integral evaluates to zero and I can't figure where's my mistake. The answer should be: $\frac{125\sqrt5-1}{420}$","['integration', 'multivariable-calculus', 'calculus', 'surface-integrals']"
4698049,"Same character values iff related by outer automorphism, for perfect groups","Let $ G $ be a finite perfect group. Let $ \chi_1, \chi_2 $ be two different irreducible characters of $ G $ . Suppose that the set of values taken by $ \chi_1 $ is the same as the set of values taken by $ \chi_2 $ . Then must $ \chi_1 $ and $ \chi_2 $ be related by an outer automorphism of $ G $ ? In the examples I know this is always true. For example the two degree $ 2 $ characters of $ SL(2,5) $ take the same values and are indeed related by an outer automorphism of $ SL(2,5) $ .","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
4698076,"How to prove this recurrence relation for generalized ""rounding up to $\pi$""?","The webpage Rounding Up To $\pi$ defines a certain ""rounding up"" function by an extremely simple procedure: Beginning with any positive integer $n$ , round up to the nearest multiple of $n-1$ , then up to the nearest multiple of $n-2$ , and so on, up to the nearest multiple of $1$ . Let $f(n)$ denote the result. E.g., using an obvious notation, the sequence starting with $n=10$ is $$
10 \xrightarrow{9} 18 \xrightarrow{8} 24 \xrightarrow{7} 28 \xrightarrow{6} 30 \xrightarrow{5} 30 \xrightarrow{4} 32 \xrightarrow{3} 33 \xrightarrow{2} 34 \xrightarrow{1} 34 =:f(10).
$$ The interesting thing is that, according to the webpage $^\dagger$ , $(f(1),f(2),f(3),\dots)$ is the same sequence that results from a certain sieving method, for which Erdős & Jabotinsky (1958) proved that $$
\lim_{n\to\infty}\frac{n^2}{f(n)} = \pi,
$$ so one naturally wonders what happens if, instead of rounding up to the nearest multiple, we round up to the second -nearest multiple, or the third -nearest, etc. Thus, let $f_k(n)$ denote the result of the ""rounding up"" sequence when using the $k^{\mathrm{th}}$ -nearest multiple; e.g., using the second-nearest multiple, the sequence beginning with $10$ is $$
10 \xrightarrow{9} 27 \xrightarrow{8} 40 \xrightarrow{7} 49 \xrightarrow{6} 60 \xrightarrow{5} 65 \xrightarrow{4} 72 \xrightarrow{3} 75 \xrightarrow{2} 78 \xrightarrow{1} 79 =: f_2(10).
$$ (These $f_k$ sequences occur in the OEIS under names like ""Generalized Tchoukaillon (or Mancala, or Kalahari) solitaire"" or related sieving processes, e.g. at the links $f_1$ , $f_2$ , $f_3$ , $f_4$ .) Now let $g_k(n):={n^2/f_k(n)}$ and, assuming all the limits exist, define the sequence $(G_k)_{k=1,2,3,\dots}$ as $$
G_k := \lim_{n\to\infty} g_k(n) 
= \lim_{n\to\infty}\frac{n^2}{f_k(n)}.
$$ Here's a plot of $(g_k(10^6))_{k=1,\dots,30}$ in which the $g_k$ have apparently converged in at least their first $5$ digits (numerically, it appears that generally the first $d$ digits of $g_k(10^{d+1})$ are those of the limit $G_k$ ): We know that $G_1=\pi,$ and in the plot I've indicated my conjectured values for $G_2,G_3$ as well, based on a simple pattern that I found by searching numerically among the $g_k(n)$ ; specifically, I found that $g_{k+1}(n) \approx \frac{4}{k^2\,g_k(n)}$ , with the absolute error bounded by $5/n$ , i.e., $$
\left|g_{k+1}(n) - \frac{4}{k^2\,g_k(n)}\right| < \frac{5}{n},
\quad k=1,2,3,\dots
$$ and also that $$
\left|g_{k}(n) - a_k\right| < \frac{5}{n},
\quad k=1,2,3,\dots
$$ with the $a_k$ defined recursively by $a_1=\pi,\; a_{k+1}=\frac{4}{k^2\,a_k},\; k=1,2,3,\dots$ This strongly suggests the following ... Conjecture : $$
\boxed{\quad G_1 = \pi, 
\quad G_{k+1} = \frac{4}{k^2\,G_k}, 
\quad k=1,2,3,\dots \quad}\tag{1}
$$ i.e., $$
(G_k)_{k=1,2,3,\ldots} = \left(\pi,\, \frac{4}{\pi},\, \frac{\pi}{4},\, \left(\frac{2}{3}\right)^2 \frac{4}{\pi},\, \left(\frac{3}{4} \frac{1}{2}\right)^2 {\pi},\, \left(\frac{4}{5} \frac{2}{3}\right)^2 \frac{4}{\pi},\, \left(\frac{5}{6} \frac{3}{4} \frac{1}{2}\right)^2 {\pi},\,\ldots \right).
$$ Question : How to prove the recurrence relation in (1)?? NB : From the conjecture (1) it's straightforward to derive the following formulas that hold for all positive integers $k$ : $$G_k=\begin{cases}\left(\prod_{j=1}^{k-1\over 2}{2j-1\over 2j}\right)^2\pi&\text{if $k$ is odd}, \\
\left(\prod_{j=1}^{k-2\over 2}{2j\over 2j+1}\right)^2 {4\over\pi}&\text{if $k$ is even}\end{cases}\\ $$ $$G_k=\left({(k-2)!!\over(k-1)!!}\right)^2\cdot
\begin{cases}
\pi&\text{if $k$ is odd},\\[3ex] 
{4\over\pi}&\text{if $k$ is even}\end{cases}\\ $$ $$G_k\sim{2\over k}$$ where the last line is from the known asymptotic behavior of double factorials . EDIT : As noted in a comment by Ash Malyshev, the conjectured $(G_k)_{k=1,2,3,\dots}$ can also be written compactly as a ratio of squared gamma functions that I'll call $\rho(k)$ : $$G_k = \left({\Gamma\left({k\over 2}\right) \over \Gamma\left({k+1\over 2}\right)}\right)^2=:\rho(k)$$ This can be established either by deriving it from the above formulas, or by showing that $\rho(k)$ satisfies the defining recursion (1). I've updated the above plot to include the continuous function $\rho$ (blue curve) on the real interval $(0, 30]$ :  The conjecture is that the blue dots ( $G_k$ ) coincide with this curve at every positive integer $k.$ $^\dagger$ The ""rounding up"" function $f$ is actually implicit in the proof given by Erdős & Jabotinsky; see this answer by Misha Lavrov for more detail. For reference, here is the program I used for $f_k(n)$ : # Python
def f(k,n):   
    y = n
    for x in range(n-1,0,-1):  # i.e., for x = n-1, n-2, ..., 1
        y = ((y-1)//x + k)*x   # i.e., y <- k-th multiple of x not less than y
    return y Also, in view of the possible relevance of sieves for proving conjecture (1), here is a program implementing the following algorithm, which I claim generates the $f_k$ : Starting with seq $_1$ = $(1,2,3,\dots)$ , let seq $_{i+1}$ be the result of deleting from seq $_i$ the first $k−1$ elements and then deleting every $(i+1)$ th element from those remaining. $f_k(n)$ will be the first element of seq $_n.$ def sieve(k, maxel):  # generates f_k(1), ..., f_k(n) <= maxel 
    seq = list(range(1, maxel+1))   # seq_1 = (1,2,3,...,maxel)
    keep = seq[:1]    # keep the first element of seq_1
    i = 1
    while seq:        # while seq is not empty
        del seq[:k-1]   # delete the first k-1 elements
        del seq[::i+1]  # delete remaining elements with index divisible by i+1
        keep += seq[:1] # keep the first element of seq_(i+1)
        i += 1
    return keep","['elementary-number-theory', 'recurrence-relations', 'gamma-function', 'pi', 'sequences-and-series']"
4698100,Why doesn't simultaneous equations work to find co-efficients of a cubic that passes through four points?,"I'm trying to find the equation of a cubic that passes through three specific points (technically it's four but that point is y-intercept). The equation would look something like this: $f(x)=ax^3+bx^2+cx+25.8$ (25.8 is the given y-intercept mentioned above) The points are: $(0,25.8),(19.3,7.3),(48.9,30),(38.6,26)$ I set it up by starting with the first three equations: $7.3=a19.3^3+b19.3^2+c19.3+25.8$ $30=a48.9^3+b48.9^2+c48.9+25.8$ $26=a38.6^3+b38.6^2+c38.6+25.8$ I have tried using three simultaneous equations but when I graph the function it only goes through the y-intercept (obviously) and one of the three points. Is there something I'm doing wrong or can I just not solve this that way?","['cubics', 'algebra-precalculus', 'systems-of-equations', 'polynomials']"
4698111,Why can't well ordered sets have infinite decreasing subsequences?,"Chapter 2 of Mathematics for Computer Science presents the following: Define the set $\mathbb F$ of fractions that can be expressed in the form $\frac{n}{n+1}$ . Define $\mathbb N$ as the set of nonnegative integers. $\mathbb N + \mathbb F$ is the set of numbers $n + f$ such that $n \in\mathbb N$ and $f \in\mathbb F$ . The book goes on to prove that $\mathbb N + \mathbb F$ is a well-ordered set. I understand that much. The book then goes on to say that:
In $\mathbb N + \mathbb F$ , every element greater than or equal to 1 can be the first element in strictly decreasing sequences of elements of arbitrary finite length. Nevertheless, since $\mathbb N + \mathbb F$ is well-ordered, it is impossible to find an infinite decreasing sequence of elements in $\mathbb N + \mathbb F$ , because the set of elements in such a sequence would have no minimum. Why can't there be such a sequence? I understand that I cannot just take the subset ${0, \frac{1}{2},\frac{2}{3},...,\frac{n}{n+1},...,1} $ and simply reverse it to obtain a counterexample. Can someone put into words why this is?","['well-orders', 'discrete-mathematics', 'computer-science']"
4698141,"Given a Mobius transformation with real coefficients such that $T(0)=0$ and $T(2)=\infty$, what is $T^{-1}(\{iy : y \in \mathbb{R}\})$?","I took a final examination recently in a complex analysis course, and one of the questions still alludes me. I wrote it down after the exam to work on it later, but I'm not seeing how to do this. It reads as follows: Consider a Mobius transformation $T$ with real coefficients such that $T(0)=0$ and $T(2)=\infty$ . What set is mapped to the imaginary axis under $T$ ? That is, what is $T^{-1}(\{iy : y \in \mathbb{R}\})$ ? Now, I know that under these linear fractional transformations, circles and lines are mapped to circles and lines, and a given mapping can be determined by its action on three points, but I'm not sure how I can apply this here. Obviously, it would seem that the set we're looking for will be either a circle or a line, but I also know that a Mobius transformation with real coefficients (assuming $ad-bc \neq 0$ ) maps the extended real line to the extended real line. I imagine this will be helpful, but I'm still not catching the trick yet, so I figured I'd extend the problem out for a hint or nudge in the right direction. My only idea would be perhaps mapping something like a circle with radius 1 centered at $(1, 0)$ , but this doesn't quite give us the imaginary axis we're looking for. Best, JR","['complex-analysis', 'mobius-transformation']"
4698202,Bounds on $P(A_1\cap \ldots\cap A_n) - P(A_1)\ldots P(A_n)$,"In Székely's Paradoxes in Probability Theory and Mathematical Statistics , it is stated without proof or reference that for any $n\geq 2$ and events $A_1,\ldots,A_n$ , $$-\left(1-\frac 1n\right)^n \leq P(A_1\cap \ldots\cap A_n) - P(A_1)\ldots P(A_n)
\leq \frac{n-1}{n^{n/(n-1)}}.$$ When $n=2$ , the bound rewrites as $|P(A_1\cap A_2) - P(A_1) P(A_2)|\leq 1/4$ , which is well-known and tight. What about $n\geq 3$ ? I'm looking for a reference or a proof.","['probability-theory', 'reference-request']"
4698225,Strange Absurdities in a Calculus Problem!,"If $f$ is a differentiable function in $[0,1]$ such that $f(f(x))=x$ and $f(0)=1$ . Find the value of $\int_{0}^{1} (x-f(x))^{2016} dx$ . This is a very popular problem(maybe). I used two attempts to solve the problem. It appears my Attempt $1$ didn't quite work, as I intended. My Attempt $2$ surprisingly worked. I will demonstrate both the attempts. Attempt 1: We first consider,if $f(x)=y$ then, $f(f(x))=x=f(y).$ We consider, the integral $I=\int (x-f(x))^{2016} dx$ . We try changing, all the independent variable of integration, from $x$ to $y$ ( or, in other words we are trying to apply method of substitution i.e substituting, $y=f(x)$ in $I$ ). For that, we have to replace $dx$ as well. We notice that, $$f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx.$$ We are now in a place to completely do the change of variables in $I$ . We have, $$I=\int (x-f(x))^{2016} dx=\int (f(y)-y)^{2016}f'(y)dy$$ . We can again, change the variable representation in $I$ to $x$ as, $$I=\int(f(x)-x)^{2016}f'(x)dx\tag {1}.$$ Now, we again observe $\int (x-f(x))^{2016} dx$ and adding it with $( 1),$ we have $$2I=\int\left( (x-f(x))^{2016} +(f(x)-x)^{2016}f'(x)\right)dx.$$ This can be readily simplified to, $$2I=\int\left( (x-f(x))^{2016}(1 +f'(x))\right)dx.$$ We now, have, $$I=\frac{\int\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ We hereby, put the upper limit and lower limit as $1$ and $0$ respectively. This means, $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ Here comes a mystery. I was stuck at this particular step. Attempt 2: (In this method, I proceeded to work with $I=\int_0^1(x-f(x))^{2016}dx$ instead of, $I=\int(x-f(x))^{2016}dx$ ) We first consider,if $f(x)=y$ then, $f(f(x))=x=f(y).$ We consider, the integral $I=\int (x-f(x))^{2016} dx$ . We try changing, all the independent variable of integration, from $x$ to $y$ ( or, in other words we are trying to apply method of substitution i.e substituting, $y=f(x)$ in $I$ ). For that, we have to replace $dx$ as well. We notice that, $$f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx.$$ We are now in a place to completely do the change of variables in $I$ . We have, $$I=\int_0^1 (x-f(x))^{2016} dx=\int_1^0 (f(y)-y)^{2016}f'(y)dy$$ . (We have, $y=f(x)$ and $x=0\implies y=f(0)=1$ and $x=1\implies y=f(1)=f(f(0))=0$ and since the variable of integration is $y$ here, so the corresponding upper limit and lower limit in terms $x$ (initially) $(1,0)$ will change to $(0,1)$ due to change in variable to $y$ ) We can again, change the variable representation in $I$ to $x$ as, $$I=\int_1^0(f(x)-x)^{2016}f'(x)dx$$ $$\implies I=-\int_0^1(f(x)-x)^{2016}f'(x)dx \tag 1.$$ Now, we again observe $\int_0^1 (x-f(x))^{2016} dx$ and adding it with $( 1),$ we have $$2I=\int_0^1\left( (x-f(x))^{2016} -(f(x)-x)^{2016}f'(x)\right)dx.$$ This can be readily simplified to, $$2I=\int_0^1\left( (x-f(x))^{2016}(1 -f'(x))\right)dx.$$ We now, have, $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}.$$ We notice, $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2},$$ can again be readily simplified by substituting $t=x-f(x)$ and so, $$\frac{dt}{dx}=(1-f'(x))\implies \frac{dt}{(1-f'(x))}=dx.$$ Hence, $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}=$$ $$\begin{align}\frac{\int_{-1}^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{(1-f'(x))}}{2}\end{align}$$ $$=\frac 1{2}\int_{-1}^1t^{2016}dt=\frac{1}{2.2017}[t^{2017}]^1_{-1}=\frac{1}{2017}.$$ So, I could solve, the problem in my 2nd Attempt prestty much easily. I analyzed both of the attempt $1$ and $2.$ I found that the problem in Attempt $1$ started because in the last step of Attempt $1$ we hit at the expression $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ In attempt $2$ , inspite of hitting at $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2},$$ we hit at $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}.$$ This expression $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2},$$ was easy to manipulate further, because, we had $(1-f'(x))$ here, instead of the ""troublesome"" $(1+f'(x))$ there. But this is strange, why do I get $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2},$$ in attempt $2$ , instead of $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2},$$ as in Attempt $1$ ? If one notice, then it might be observed that, there is no difference in the procedure applied in both of the attempts, only thing that was not alike, is that in attempt one we ignored the limits (upper and lower limit) and at the final step we put them to convert $I$ into a definite integral. On the other, hand, in attempt $2$ we exactly imitated what we did in attempt $1$ just that, here, at each step we were writing the upper and lower limits. My question, is, why this mysterious thing is occuring ? Both of the simplication in the two attempts should have been same. Where does the difference occur , in either of the two cases ? Another (Unexplained) Absurdity : If in Attempt $2$ we took the substitution $t=(x-f(x))^{2016}$ then things would have been different and we would have got, $I=0.$ The corresponding calculation, for $t=(x-f(x))^{2016}$ : $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}=$$ $$\begin{align}\frac{\int_1^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{2016(x-f(x))^{2015}(1-f'(x))}}{2}\end{align}$$ $$=0.$$ Precisely, I am looking for a convincing explanation of these apparent absurdities, using elementary real analysis (basic) elementary calculus(/basic real analysis).","['integration', 'calculus', 'definite-integrals']"
4698243,"Multivariable limits: $ \lim_{{(x,y) \to (0,0)}} \frac{{x^4 + y^4}}{{\sqrt{{x^2 + y^4}}}} $","$$ \text{Evaluate the following limit: }
A= \lim_{{(x,y) \to (0,0)}} \frac{{x^4 + y^4}}{{\sqrt{{x^2 + y^4}}}}$$ I've tried using polar coordinates, but am having trouble solving this. $$
\lim_{{r \to 0}} \frac{{(r\cos(\theta))^4 + (r\sin(\theta))^4}}{{\sqrt{{(r\cos(\theta))^2 + (r\sin(\theta))^4}}}}
$$ After substituting, my first thought was to rationalize the denominator, which resulted in this: $$
\lim_{{r \to 0}} \frac{{\sqrt{{(r\cos(\theta))^2 + (r\sin(\theta))^4)}}{{(r\cos(\theta))^4 + (r\sin(\theta))^4)
}}}}{{(r\cos(\theta))^2 + (r\sin(\theta))^4}}
$$ But from here I'm having trouble moving forward. Can anyone provide any hints or tips? Would be greatly appreciated!","['limits', 'multivariable-calculus']"
4698261,Allendoerfer and Weil's generalization of Gauss-Bonnet Theorem,"In Peter Petersen's Riemannian Geometry ( reference , p. 98), he says that The theorem is now called the Chern-Gauss-Bonnet Theorem despite the fact that Allendoerfer and Weil were the first to prove it in complete generality in higher dimensions. But on Wikepedia ( reference ), it is said that In 1943, Carl B. Allendoerfer and André Weil proved a special case for extrinsic manifolds. In a classic paper published in 1944, Shiing-Shen Chern proved the theorem in full generality connecting global topology with local geometry. Who was really the first to prove the ""full generality""? I don't have enough knowledge of differential geometry to judge it myself, but I want to know the history.","['characteristic-classes', 'math-history', 'riemannian-geometry', 'differential-geometry']"
4698350,Definite integral of ydx + xdy giving two different results,"This comes from Blundell's ""Concepts in Thermal Physics"". Section 11, example 11.1. If I have $$df = d(xy) = ydx + xdy$$ Then one can do $$\int df = \int d(xy) = \int (ydx + xdy)$$ But $\int df = \Delta f$ and $\int (ydx + xdy) = \int ydx + \int xdy =  yx + xy = 2xy$ and $\int d(xy) = \Delta (xy)$ by the Fundamental Theorem of Calculus. I don't get how can it do $\Delta f = \int_{(0, 0)}^{(1, 1)} (xy) = (1 \times 1) - (0 \times 0) = 1 $ which is different than $\int_{(0, 0)}^{(1, 1)} (ydx + xdy) = 2(1 \times 1) - 2 (0 \times 0) = 2$ . What is wrong with my interpretation?","['integration', 'multivariable-calculus', 'calculus', 'functions']"
4698351,Derivation of parametric integral function,"Good afternoon, consider the integral function $f(x):=\int_{0}^{+\infty}\mathrm{d}t\,\frac{\cos(xt)}{\sqrt{t^2+1}}$ . I would like to derive $f$ with respect to $x$ to obtain $\frac{\mathrm{d}f(x)}{\mathrm{d}x}=-\int_{0}^{+\infty}\mathrm{d}t\,\frac{t\sin(xt)}{\sqrt{t^2+1}}$ , but the integral is not absolutely convergent and Lebesgue's dominated convergence theorem cannot be applied here to justify the derivation under the integral sign. How could one proceed here?","['integration', 'improper-integrals', 'analysis', 'real-analysis', 'calculus']"
4698367,Trouble understanding Hartshorne's Algebraic Geometry Exercise 2.2 (Chapter 1).,"I am trying to solve the following exercise: Let $\mathcal{a}$ be a homogeneous ideal such that $\mathcal a \subset
S = K[x_0,\dots,x_n].$ Show that the following affirmations are
equivalent: $Z(a) = \emptyset$ ; $\sqrt{a} = S$ or $\sqrt{a} = S_+ = \langle x_0, \dots, x_n \rangle;$ $a \supset S_d,$ for some $d>0,$ where $S_d$ is the group of the homogeneous polynomials of degree $d$ . I am following the resolution available here . My concerns on the implication $\mathbf{(ii) \implies (iii)}$ : None. I understood this proof. My concerns on the implication $\mathbf{(iii) \implies (i)}$ : I understand that the idea is to assume that $Z(a) \neq \emptyset$ and to reach an absurd. Keeping this in mind, assume $P \in Z(a).$ Then, $P \in Z(S_d)$ since $S_d \subset a \implies Z(a) \subset Z(S_d).$ This means that every homogeneous polynomial of degree $d$ vanishes at $P$ . Until here, I understand everything. Now comes the part I don't understand: ""Since $P \neq 0$ , this is absurd"". I don't understand why $P$ must be different from zero and why this is absurd. My concerns on the implication $\mathbf{(i) \implies (ii)}$ : Basically everything. I don't understand how $Z(a) = \emptyset$ implies that, in $\mathbb A^{n+1}$ , $Z(a) = \emptyset$ or $Z(a) = \{0\}$ and I also don't understand the follow up. Any help is apreciatted in advance.","['affine-varieties', 'algebraic-geometry', 'projective-varieties']"
4698387,Is every rational sequence topology homeomorphic?,"In the rational sequence topology , rationals are discrete and irrationals have a local base defined by choosing a Euclidean-converging sequence of rationals and declaring any cofinite subset of this sequence along with the irrational to be open. Do these choices of sequences matter? Or does there exist a homeomorphism for any pair of sequence assignments?",['general-topology']
4698406,Does a recursive definition of a variable make sense at all?,"Let $A:=\sum_{n=0}^\infty 2^n$ . I was given the following equation: $$A=\sum_{n=0}^\infty 2^n=\sum_{n=1}^\infty2^{n-1}=\sum_{n=1}^\infty(2^n\cdot\frac{1}{2})=\frac{1}{2}\sum_{n=1}^\infty2^n=\frac{1}{2}(-1+\sum_{n=0}^\infty2^n)=\frac{1}{2}(-1+A)$$ My task is to explain that why this can't be true and to find the mistake in the equation. I think that I found the mistake but I am not  quite sure if I found it correctly so maybe someone can look over it. First, this obviously has to be wrong, because if I solve $A=\frac{1}{2}(-1+A)$ for $A$ I get $A=-1$ . But this can't be true, because $\lim_{n\to\infty}(\sum_{k=1}^n2^k)=+\infty$ . I now tried to check every step and the single steps (except the last) seem to be correct. With the last step I thought that it might be a problem that we have replaced the sum with $A$ again, because then we get a recursive definition of $A$ . And from my point of view it makes no sense to do this because then we can never assign a value to $A$ . So my thought wass that a recursive definition of a variable is completly senseless. On the other hand I thought that this might be a notation problem my teacher wants to draw my attention to. In our class we are using the symbol $\sum_{n=0}^\infty a_n$ once for the series and once for the limit $\lim_{n\to\infty}\sum_{k=1}^n a_k$ . Maybe we can't replace the $\sum_{n=0}^\infty 2^n$ with $A$ because $A$ is the series and $\sum_{n=0}^\infty 2^n$ is the limit and not the series. So now I am not quite sure why exactly the equation given is wrong, so maybe someone here can explain better. This question here is quite similiar, but from my point of view It wouldn't have helped me to solve my problem, because there is not stated that $A\notin\mathbb{R}$ holds and this was the essential point where I started to understand my problem.","['divergent-series', 'real-analysis', 'infinity', 'sequences-and-series', 'limits']"
4698462,Coloring arithmetic progression,I've been looking at some old notes from the course Probabilistic Combinatorics and I saw the following question: Prove that there exists a constant $N$ and a red/blue coloring of $\mathbb{Z}$ without any monochromatic $n$ -term arithmetic progressions with $n \geq N$ and common difference less than $1.99^n$ . The question appeared under the section about Lovasz Local lemma ( https://en.wikipedia.org/wiki/Lov%C3%A1sz_local_lemma ) so i'm assuming there is a way using it. I was trying to define the events to be the existence of a monochromatic $n-$ term AP with a specific difference however it did not work. ANY help would be appreciated,"['probabilistic-method', 'arithmetic-progressions', 'combinatorics', 'ramsey-theory']"
4698473,"Prove that $\lim_{(x,y) \to (x_0,0)} \frac{1-e^{-xy}}{y} = x_0$","I have to prove that $$\lim_{(x,y) \to (x_0,0)} \frac{1-e^{-xy}}{y} = x_0$$ I have tried the following, but I'm not sure if it is rigorous, especially in one step: We have by L'Hôpital that $$\lim_{h \to 0} \frac{1 - e^{-h}}{h}=\lim_{h\to 0} e^{-h}=1$$ Then, because $$\lim_{(x,y)\to (x_0,0)} xy = 0$$ we have that \begin{equation} 1 = \lim_{h \to 0} \frac{1 - e^{-h}}{h} = \lim_{(x,y) \to (x_0,0)} \frac{1 - e^{-xy}}{xy} \tag{1}\end{equation} So $$ \lim_{(x,y) \to (x_0,0)} \frac{1-e^{-xy}}{y} = \lim_{(x,y) \to (x_0,0)} x\frac{1-e^{-xy}}{xy} = \lim_{(x,y) \to (x_0,0)} x  \cdot \lim_{(x,y) \to (x_0,0)} \frac{1-e^{-xy}}{xy}= x_0 \cdot 1 = x_0$$ The step where I used a reasoning that I think it may not always be true is $(1)$","['multivariable-calculus', 'limits', 'calculus']"
4698476,Radius of a circle yielding the largest IntersectionOverUnion (IoU) with a unit square [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question There is a unit square with coordinates ((0,0), (1,1)). I need to find a radius of a circle with a center at (0.5, 0.5) such that the IoU of the circle and the unit square is at maximum. What is the maximal IoU? I can approximate it numerically (this is sufficient for the job), but what is the right way to do it analytically? Update show your work Once again. For my work it is sufficient to find a numerical estimate. Here is how you can do it. We should start with a good rational approximation of sqrt(2). We can use 577/408 ~ sqrt(2) import numpy as np

eps = 1e-10
center = 577
square_half_size = 408
square = np.zeros((center*2, center*2))
square[center-square_half_size:center+square_half_size, center-square_half_size:center+square_half_size] = 1


def iou(c):
    overlap = c * square
    union = (c + square).clip(0,1)
    return np.sum(overlap)/(np.sum(union) + eps)


best_iou = 0

for i in range(center - square_half_size, center):
    c1 = np.zeros((center*2, center*2))
    for h in range(center*2):
        for w in range(center*2):
            if (center-h)**2 + (center-w)**2 < i**2:
                c1[h,w] = 1

    current_iou = iou(c1)
    if current_iou > best_iou:
        best_iou = current_iou
        print(i, best_iou) One can use binary search to accelerate the computation.
This approach works: I only need the result to two digit accuracy. But I want to go an extra mile to see, how to approach these tasks analytically. Update: r = 0.549468, IoU = 0.8370325 when using another good approximation of sqrt(2) 8119/5741 center = 8119
square_half_size = 5741 Takes about 12 hours to compute. Grid refinement (binary search) would speed up the computation drastically.","['maxima-minima', 'geometry']"
4698506,Ordinal equations using Cantor's normal form,"Find all possible ordinals $\alpha,\beta$ satisfying the equations: $\alpha+\beta=\omega$ $\alpha+\beta=\omega^2+1$ For 1), discarding the trivial cases $\alpha=0$ or $\beta=0$ , we have that $\omega^{\beta}\neq \omega$ for all $\beta\neq 1$ , and $\omega\cdot k\neq\omega$ for all $k\neq 1$ (I guess these are obvious statements?) Also, $\omega+n\neq\omega$ (for $n>0$ ) and $n+\omega=\omega$ (for $n\geq 0$ ). Therefore, the only possibilities (taking into account the normal form of $\alpha$ and $\beta$ are $\alpha=n\in\omega$ and $\beta=\omega$ , or $\alpha=\omega$ and $\beta=0$ . I think these are all the solutions, since ordinals can't cancel each other to $0$ , but I do not know if this answer is completely justified. For 2) I did the same. Discarding $\alpha=0$ or $\beta=0$ :
If $\alpha=n\in\omega$ , then necessarily $\beta=\omega^2+1$ , otherwise the equation is not satisfied.
If $\alpha$ is infinite, then necessarily $\alpha=\omega^2$ , otherwise other powers of $\omega$ would appear, and $\beta$ cannot cancel them out. Then $\beta=1$ , otherwise the equation is not satisfied. Is this enough?","['elementary-set-theory', 'ordinals']"
4698532,How to compute the following Complex integral,"I am trying to compute the following complex-valued integral $(1.3.10)$ by using contour integration and the residue theorem. The substitution is shown right below $(1.3.10)$ and the answer is shown in $(1.3.11)$ . By goal is to reproduce $(1.3.11)$ . $$ Since $-\pi < k < \pi$ the contour integration should be clockwise around the unit circle. I will be using the residue theorem which is based on counterclockwise contours. We see $$
P(x,z) = \frac{1}{2\pi} \int_{-\pi}^{\pi} \frac{e^{-ikx} dk}{1 - z(pe^{ik} + qe^{-ik})} = \frac{1}{2\pi} \int_{\text{clockwise}} \frac{w^x idw/w}{1 - z(pw^* + qw)} = \frac{i}{2\pi}\int_{\text{clockwise}} \frac{w^{x-1}dw}{1 - z(pw^* + qw)}
$$ $$
= -\frac{i}{2\pi}\int_{\text{counter clockwise}} \frac{w^{x-1}dw}{1 - z(pw^* + qw)} = \text{sum of residues}
$$ The above seems to suggest that for $x < 1$ there are additional poles at $w = 0$ , and so it seemed that $P(x,z)$ would have two forms, one for $|x| < 1$ and $|x| \ge 1$ , but the answer seems to give one form for $P(x,z)$ . Does that mean $w = 0$ is never a pole regardless of the value of x? How do I get the final answer?","['complex-analysis', 'contour-integration', 'complex-integration']"
4698545,Circle numbers on edges of a graph,"Let $k$ vertices in a graph be given. Some pairs of vertices are connected by an edge, each edge is labeled either $\{1,2\}$ , $\{1,3\}$ , or $\{2,3\}$ . We can circle some of the numbers on the edges (circling both numbers on an edge is allowed). If an edge is $\{1,2\}$ , some vertex adjacent to it must be adjacent to a circled $1$ and a circled $2$ (the same endpoint must satisfy both constraints). A similar requirement holds for $\{1,3\}$ and $\{2,3\}$ edges. Is it true that for any graph and labels, it is sufficient to circle at most $\lceil 3k/2\rceil$ numbers? For instance, if $k = 3$ and the three edges have label $\{1,2\}$ , $\{1,3\}$ , $\{2,3\}$ , then it suffices to circle the first $1$ , the first $3$ , and the second $2$ . But for larger $k$ (such as $k = 6$ ), it becomes difficult to find a direct strategy. The question was posted on MathOverflow but with no answer. I would be interested in an answer for $k=6$ already, as it seems difficult to do by brute force. [ Note : The answer below by Steven is based on a wrong understanding of the question.]","['graph-theory', 'combinatorics']"
4698551,Morphism and slope of bundles on $\mathbb P^2$,"Let $H$ be a very ample line bundle on $\mathbb P^2$ given by $\mathcal O_{\mathbb P^2}(1)$ . Let $E$ be a rank $r$ , slope stable (w.r.to $H$ ) vector bundle on $\mathbb P^2$ . Is it true that there does not exist any nontrivial homomorphism between $E$ and $E \otimes \mathcal O_{\mathbb P^2}(-3)$ ? On the contrary, if we assume that $f$ is a nontrivial morphism between them amd $I$ is the image, then rank of $I$ is strictly less than both the bundles (since the morphism is nontrivial) and slope of $I$ is less than or equal to the  slope of $E \otimes \mathcal O_{\mathbb P^2}(-3)$ which is strictly less than slope of $E$ . But I don't see how to achieve a contradiction from here. At this point, I have an alternative argument in mind which is as follows: if $f \in Hom (E, E \otimes \mathcal O_{\mathbb P^2}(-3))$ , then $\text{det}(f) \in H^0(\mathbb P^2, \mathcal O(-3r))=0$ . Can we say that $f \in Hom (E, E \otimes \mathcal O_{\mathbb P^2}(-3)) \subset Hom(E,E) \cong \mathbb C$ as $E$ is simple. If $f$ is nonzero, its an automorphism of $E$ such that $\text{det}(f)=0$ . Can we conclude from there that $f$ must be zero? My main two points is that: $(i)$ Is it true that $Hom (E, E \otimes \mathcal O_{\mathbb P^2}(-3)) \subset Hom(E,E)$ ? (Is this obvious to see?) $(ii)$ If $f$ is nonzero, automorphism of $E$ such that $\text{det}(f)=0$ . Can we conclude from there that $f$ must be zero? Any suggestion is welcome.",['algebraic-geometry']
4698575,Unconstrained optimization: Derivative of FOC w.r.t parameter is 0?,"Suppose we have an unconstrained optimization problem $$ \max f(x)$$ and suppose there is some constant (i.e. a parameter), $c$ in $f(x)$ . The FOC of the optimization problem is $$\frac{\partial f}{\partial x}=0$$ If we take the total derivative of the FOC with respect to the parameter, $c$ is this total derivative equal to 0? let me try stating what I think is an equivalent (or more rigorous) statement of the question: Let $x^*(c)$ denote the solution to the unconstrained optimization, where $x$ depends on some parameter in $f$ then by definition we have $$\tag{EQ1} \frac{\partial f}{\partial x}\vert_{x=x^*(c)}=0$$ for every $c$ . Is it then correct to say that the total derivative of the left hand side of (EQ1), with respect to $c$ , is $=0$ ? I believe my confusion is arising from the fact that, typically, if we say a function $g(y)=0$ , we can't say that $g'(y)=0$ , but with (EQ1) we know that the equation holds at every $x^*(c)$ (i think this might be called an identity)","['optimization', 'calculus', 'functions', 'derivatives']"
4698587,A doubt on Theorem 2.6 from Pazy's book,"I have been very confused about an argument on Theorem 2.6 from Pazy, Semigroups of Linear Operators and Applications to Partial Differential Equations. Here is the theorem and part of the proof: I just can not prove that red part from the picture just using that the semigroups are $C_0$ . Remember that a semigroup $\{T_t\}_{t \geq 0}$ of bounded linear operators on a Banach space is $C_0$ if $\lim_{t \rightarrow 0^+} T(t)x = x,$ for all $x \in X.$ Here there's another discussion about the same doubt, which I still didn't understand. Here is my attempt : Fixed $t > 0$ , consider the function $\varphi(r) = T(t - r)(S(r)x),$ with $r \leq t$ . So, for a $s \geq 0$ , we want to describe $\frac{d \varphi}{d r}(s)$ . Notice that \begin{align}
\frac{\varphi(s + h) - \varphi(s)}{h} & = \frac{1}{h}[T(t - (s + h))(S(s + h)x) - T(t - s)(S(s)x)] \\
& = \frac{1}{h}[T(t - (s + h))(S(s + h)x) - T(t - s)(S(s)x) - T(t - (s+h))(S(s)x) + T(t - (s+h))(S(s)x)] \\
& = \frac{1}{h} [T(t - (s + h))(S(s + h)x) - T(t - (s + h))(S(s)x)] + \frac{1}{h}[T(t - (s+h))(S(s)x) - T(t - s)(S(s)x)  ] \\
& 
\end{align} Lets conclude something about the second quotient above. For $\tilde{h} = - h$ , we deduce \begin{align}
\frac{1}{h}[T(t - (s+h))(S(s)x) - T(t - s)(S(s)x)] & = -\frac{1}{\tilde{h}}[T(\tilde{h} + (t - s))(S(s)x) - T(t - s)(S(s)x)] \\
& = -\frac{1}{\tilde{h}}[T(\tilde{h})( T(t - s)(S(s)x)) - T(t - s)(S(s)x)],
\end{align} which converges to $-A(T(t-s)(S(s)x))$ . For me the problem is in the first quotient. Notice that \begin{align}
\frac{1}{h} [T(t - (s + h))(S(s + h)x) - T(t - (s + h))(S(s)x)] & = T(t - (s+h)) \frac{(S(s+h)x - S(s)x)}{h} \\
& = T(t - (s+h)) \frac{(S(h)(S(s)x) - S(s)x)}{h}.
\end{align} Somehow, I think that this should converge to $T(t-s)(B(S(s)x))$ as $h \rightarrow 0^+$ . However, I was not able to obtain this just using that $\{T(t)\}$ is a $C_0$ semigroup. All I know it is $$
\lim_{h \rightarrow 0^+} \frac{(S(h)(S(s)x) - S(s)x)}{h} = B(S(s)x).
$$ Any help is very welcome.","['parabolic-pde', 'functional-analysis', 'partial-differential-equations', 'hyperbolic-equations', 'semigroup-of-operators']"
4698603,Challenging series evaluation,"We have $$S(n)=\sum_{k=0}^\infty\frac{(-1)^{k+1}}{(nk+1)^2}-\sum_{k=0}^\infty\frac{(-1)^{k+1}}{(nk+n-1)^2}$$ I encountered this series while evaluating an integral, and it  seems to have a nice closed form for each $n$ . Here are the first few values by WolframAlpha: $$S(2)=0 \hspace{2cm} S(3)=-\frac{2\pi^2}{27} \hspace{2cm} S(4)=-\frac{\pi^2}{8\sqrt2}\hspace{2cm} S(5)=-\frac{(5+3\sqrt5)\pi^2}{125} \dots$$ After checking the first few cases , I conjectured that the result is: $$S(n)=-\frac{\pi^2}{n^2}\cot\left(\frac{\pi}{n}\right)\csc\left(\frac{\pi}{n}\right)$$ but couldn't prove it.
I managed to compute by myself only $S(2)$ , which is trivial of course, and $S(3)$ , which can be obtained by some simple manipulations using Dirichlet $\eta$ function, but when I tackled the general problem this approach didn't help me. The integral I was trying to solve is $$I=\int_0^\infty\frac{\log x}{1+x^n}dx$$ and I managed to prove that $$I=S(n)$$ so this is where the idea came from. Please, don't answer with other evaluations of the integral which could prove the conjectured result, I am asking for an evaluation that starts off from the series, as if you didn't know it is related to the integral at all. Any help would be appreciated.","['integration', 'calculus', 'sequences-and-series']"
4698646,Why Ricci Flow Always Defines a Riemannian Metric As Long as it Exists,"From what I understand, the Ricci flow \begin{equation}
\frac{\partial g}{\partial t} = -2Ric(g)
\end{equation} always defines a Riemannian metric as long as it exists. I know that the Riemannian condition is an open condition in the space of symmetric two-tensors, but is that all there is to this statement? I know that certain curvature-related quantities must blow up at the first singular time for the Ricci flow, but may this also be understood in terms of when the two-tensor solution for the Ricci flow looses its definiteness?","['ricci-flow', 'differential-geometry']"
4698675,Probability of winning a coin toss game,"This may seem like a straightforward probability question but it becomes complicated quickly. I have tried a lot of paths to a solution and would appreciate your help. It is not for school or work. I flip a coin. If it's heads, I win. But if it's tails, then I need two heads in a row to win. If I then flip another tails, at any time before winning, then I need three heads in a row to win, and so on. The number of heads I need to win is always T+1, where T is the cumulative number of tails flipped during the game. I want to find the overall probability of winning. It is easy to do so numerically. The answer is about 71%. But I want an exact expression, a recursive function to sum the series of probabilities over infinite flips. I imagine it's an infinite recursive product but I've yet to find an answer. Any help would be infinitely appreciated.","['statistics', 'closed-form', 'infinite-product', 'recreational-mathematics', 'probability']"
4698696,Doubt about Rolle's theorem,"When we use Rolle's theorem successfuly, it is because the function in the analysed interval $[a, b]$ is continuous, differentiable in $(a, b)$ and $f(a)$ = $f(b)$ . I am asked to prove that $x^3-3x+b=0$ has exactly and only a single root in $[-1, 1]$ using only Rolle's theorem. I beg you to excuse me, because I know the answer is duplicated, but I am (maybe) too stupid to understand it. It is impossible to use Rolle's theorem because $f(-1)\neq f(1)$ for every $b\in\mathbb{R}$ , so we must use intermediate value theorem. However, I can't see any relation between the fact that there is a $f'(c)=\frac{f(b) - f(a)}{b - a}$ in $(a, b)$ and the roots of $f(x)$ . For the second time, please, please please, you are welcome to close my question for being duplicated (better say cuatriplicated) and excuse me for annoying purposely, but at least have mercy and explain it to me.","['calculus', 'derivatives']"
4698728,Embedding $\mathbb{CP}^n$ into $\mathbb{R}^{4n-1}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . The community reviewed whether to reopen this question 3 months ago and left it closed: Original close reason(s) were not resolved Improve this question I'm looking for a generalization of the map of $\mathbb{C}P^1$ into $\mathbb{R}^3$ as a sphere.
Are there any such (preferably canonical) embeddings of $\mathbb{C}P^n$ into $\mathbb{R}^{4n-1}$ ? (And is $4n-1$ the correct dimension to be asking this question in?)","['complex-analysis', 'projective-space']"
4698734,How can I find a point outside a spherical polygon?,"Say we have a spherical polygon consisting of a series of vertices $v_1, v_2, \ldots, v_i$ on the unit sphere connected by great-circle arcs and are given a point $X$ on the unit sphere which is guaranteed to be inside the polygon (thus defining an interior). In general, the polygon may not be convex. Computationally, we could pick points and use an intersection-counting algorithm to determine whether they are inside of the polygon. But I'm wondering whether there is a point which could be directly calculated from the information given which would not be inside the polygon. I haven't thought about this part as much but bonus points if you can find a point that is in some sense ""far away"" from the polygon.","['linear-algebra', 'geometry', 'computational-geometry']"
4698750,Extension of the Multivariate Faa di bruno's formula with more than two composite functions,"The Faa di bruno formula for one variable (Wikipedia) is The combinatorial forms in terms of bell polynomials are also included Similarly, the multivariate formula (Wikipedia) is expressed combinatorially. A more generalized version of this multivariate formula is also found and proven in this article by L. Hernandez Encinas and J Munoz Masque https://reader.elsevier.com/reader/sd/pii/S0893965903900267?token=C596F6F1C8B7EFD6FF6D8B18EDE8A76848F5F401BAE2F7A41A1BDB28EC54645DA66A482C4567EC33FA70B6D1B2B6F064&originRegion=eu-west-1&originCreation=20230512055810 . Few other citations have also proved that same formula, so I will refer to them as well. https://www.cambridge.org/core/services/aop-cambridge-core/content/view/BA960C9CAB40DE52DB84799BEB0C730A/S1446181100003527a.pdf/the-multivariate-faa-di-bruno-formula-and-multivariate-taylor-expansions-with-explicit-integral-remainder-term.pdf https://www.ams.org/journals/tran/1996-348-02/S0002-9947-96-01501-2/S0002-9947-96-01501-2.pdf https://arxiv.org/pdf/1012.6008.pdf As the readers of these articles can see that in all these cases, Faa Di Bruno's formula was restricted to only two nested functions, there was no mentioning of extending the formula for several nested functions, which brings me back to my question that, What methodologies shall be opted in order to extend the multivariate Faa di Bruno's formula for several nested functions ? Any advise or editing in the question format or any correction in the formatting would be acknowledged from my side as a part of learning. Thanks for giving me your very precious time Kabir Munjal","['multivariable-calculus', 'calculus', 'linear-algebra', 'combinatorics', 'bell-numbers']"
4698776,Is $x^a $sin$(x^{-a})$ Hölder continuous?,"I'm currently trying the following Exercise #11 of Chapter 3 in Stein's Real Analysis . Exercise 11. If $a, b>0$ , let $$
f(x)=\begin{cases}
x^a \sin \left(x^{-b}\right) & \text { for } 0<x \leq 1 \\
0 & \text { if } x=0
\end{cases}
$$ Prove that $f$ is of bounded variation in $[0,1]$ if and only if $a>b$ . Then, by taking $a=b$ , construct (for each $0<\alpha<1$ ) a function that satisfies the Lipschitz condition of exponent $\alpha$ \begin{equation*}
|f(x)-f(y)| \leq A|x-y|^\alpha
\end{equation*} but which is not of bounded variation. [Hint: Note that if $h>0$ , the difference $|f(x+h)-f(x)|$ can be estimated by $C(x+h)^a$ , or $C^{\prime} h / x$ by the mean value theorem. Then, consider two cases, whether $x^{a+1} \geq h$ or $x^{a+1}<h$ . What is the relationship between $\alpha$ and $a$ ?] I've managed to solve the former problem; $f$ is of bounded variation iff $a > b$ . The thing is the latter one; when $b = a$ , $f$ satisfies $|f(x) - f(y)| \leq A|x-y|^{\alpha}$ for each $0 < \alpha < 1$ . After searching for some materials, I've found a similar question in here , and I tried similar approach based on this answer as follows. Let $q = 1/\alpha, p = (1-1/q)^{-1}$ , and $ g(t) = 1 $ for $ t \in [0,1]$ . Using Holder's Inequality with $0 < x \le y < 1$ (wlog), $$|f(y)-f(x)| = \left|\int_x^y f'g\right| \le \left\{\int_x^y |f'|^p\right\}^{1/p}\left\{\int_x^y |g|^q\right\}^{1/q}$$ $$= \left\{\int_x^y |f'|^p\right\}^{1/p}|y-x|^{\alpha}$$ If this is correct, it remains to show that $\int_0^1 |f'|^p < \infty$ , where I'm stuck. (Do I have to use $|f'| \le ax^{a-1}|$ sin $(x^{-a})| + ax^{-1}|$ cos $(x^{-a})|$ ?) So, any comments about my trial, and (if exists) other proper approach to solve the problem would be appreciated. Thank you in advance. (I ignored the given hint because I found it difficult to apply, but any explanation about the hint is also welcome.)","['measure-theory', 'lebesgue-integral', 'lipschitz-functions', 'bounded-variation', 'holder-inequality']"
4698782,Is a set of the number of circles passing through the origin a finite set or infinite set?,"The set of the circles passing through the origin is an infinite set as there are infinitely many circles that pass through the origin.
From what I understand, the only element in the set of the number of circles passing through the origin should be infinity, making the set a finite set. My teacher says that it is not a finite set, but an infinite set as infinity is not an element, it is only a way of representing an extremely large number. Which one is it then? Edit: If I'm understanding it correctly, the set of the number of circles passing through the origin can also be rewritten as the set of the cardinality of the set of all circles passing through the origin. The cardinality of the set of all circles passing through the origin is $\infty$ , therefore making the set of the cardinality a finite set. Is this a valid interpretation?",['elementary-set-theory']
4698792,Linkage between Lagrange multipliers and eigenvectors/eigenvalues?,"I am a Freshmen Engineering Student, and this past Semester I took an Intro Multivariable Calculus Course, where we covered everything up to Lagrange Multipliers and Space Curves but not things like Surface and Line Integrals. At the same time, I also took an Intro to Linear Algebra Course that covered everything up to Determinanats, Eigenvectors/Eigenvalues and threw in Orthogonalization and Gram-Schmidt Orthonormalization. The set up for Eigenvalues is the solution to: A $\vec x$ = $\psi$$\vec x$ Simultaneously, we where taught that the method to find Lagrange Multipliers with 1 constraint was the solution to: $\nabla$$f(x, y)$ = $\lambda$$\nabla$$g(x, y)$ resulting in 3 equations: $\frac{\partial f}{\partial y}$ = $\frac{\partial g}{\partial y}$ $\frac{\partial f}{\partial x}$ = $\frac{\partial g}{\partial x}$ $g(x, y) = c$ Where $g(x, y) = c$ is the appropriate Level Curve given by the restraint conditions. With these equations, your unknowns are the appropriate $(x, y)$ and the corresponding $\lambda$ . My Multi Professor then said, ""From here it is as simple as solving the given system of equations and checking the whether or not they are Maximum or Minimums."" Myself and a friend of mine, whom is in the same Linear Class as I, immediately wondered if there is a way to use Linear Algebra to solve the system of equations, as we realized that the set up of: $\nabla$$f(x, y)$ = $\lambda$$\nabla$$g(x, y)$ looks remarkable similar to: A $\vec x$ = $\psi$$\vec x$ We realized quickly that if we consider the Linear Transformation Matrix: A : $\mathbb{R} ^ 3 \mapsto \mathbb{R} ^ 3$ $\nabla$$f(x, y)$ = A $\cdot$ $\nabla$$g(x, y)$ Then we can set up the entire Lagrange Multiplier Process as: A $\cdot$ $\nabla$$g(x, y)$ = $\psi$$\nabla$$g(x, y)$ and then from there we can solve for the Eigenvalues, which will be our Lagrange Multiplies, and our Eigenvectors, which will be our Critical Points. Now this seemed all well and good, and we confirmed with our Linear Professor that this set-up was valied, but we came upon a major questions that left all 3 of us stumped. Our Linear Professor told us that he would dig into it things a little more, as he started spewing math things that where way above my friend and I's head, and I figured this is a good place to ask them. How would we find the Transformation Matrix from $f(x, y)$ to $g(x, y)$ . This entire process hinges upon finding that Transformation Matrix, and assuming that you can find said Matrix and assuming that is is a valid Linear Transformation, then how would one go about finding it?","['eigenvalues-eigenvectors', 'lagrange-multiplier', 'matrices', 'multivariable-calculus', 'linear-algebra']"
4698796,"What condition(s) on $X$ and $A$ can ensure the existence of an element $a\in A$ such that $d(x_0, a)=d(x_0, A) $?","Let $(X, d) $ be metric space and $A\subset X$ and $x_0\in X$ Define $d(x_0, A) =\inf\{d(x_0, a) :a\in A\}$ What condition(s) on $X$ and $A$ can ensure the existence of an element $a\in A$ such that $d(x_0, a)=d(x_0, A) $ ? Can we take care of uniqueness as well? Motivation: Suppose $X$ is a Hilbert space and $A\subset X$ non empty closed convex set then for all $x_0\in X$ $\exists! a\in A$ such that $d(x_0,a) =d(x_0, A) $ $X$ is reflexive and strictly convex
iff $ A\subset X$ non empty closed convex set in $X$ is a Chebyshev set . Particular cases: Suppose $(X,d)$ be any metric spaces $A\subset X$ non empty compact set then $\forall x_0\in X, \exists a\in A$ such that $d(x_0, a) =d(x_0, A) $ Sketch: $d_{x_0}:X\to\Bbb{R}$ is continuous and $A\subset X$ compact implies $\inf\{d(x_0, a) :a\in A\}$ is attained in A. Going deep into the proof: $d(x_0, A)=\inf\{d(x_0, a) :a\in A\}$ implies $\exists (a_n) \in A$ such that $d(x_0, a_n) \to d(x_0, A) $ $\lim _{n\to\infty}d(x_0, a_n)= d(x_0, A) $ Implies $ d(x_0,\lim_{n\to\infty} a_n)= d(x_0, A) $ We only need to make sure that $(a_n)\subset A$ converges to $a\in A$ for some $a\in A$ $$\text{OR}$$ $(a_n) $ has a convergent subsequence in $A$ Further treatment: $(X, d) $ complete and $A\subset X$ non empty closed totally bounded set. But no improvement at all! (Complete and totally boundedness $\iff$ compact.)","['normed-spaces', 'metric-spaces', 'real-analysis', 'descriptive-set-theory', 'general-topology']"
4698845,Use of $\sum_{r=1}^{\infty} \frac{1}{r^2} = \frac{\pi^2}{6} $ in evaluating $I$,"Let $$I=\int_1^{\infty} \frac{\{x\}}{x^3} dx,$$ where $\{x\}$ represents fractional part of $x$ . Now, using $\{x\}+\lfloor x \rfloor=x$ , $I$ can be rewritten as $I=\int_1^{\infty} \frac{x-\lfloor x \rfloor}{x^3} dx$ and consecutively broken as $$I=\int_1^{\infty} \frac{1}{x^2} dx-\int_1^{\infty}\frac{\lfloor x \rfloor}{x^3} dx.$$ The first integral simplifies to 1 and my question is how do we evaluate the second integral? Clearly, $[x]$ is a discontinuous function and needs to be broken at integers so I tried rewriting it as $$I=\sum_{r=1}^{\infty} \int_{r}^{r+1} \frac{r}{x^3} dx$$ and it almost resembled the series; $\sum_{r=1}^{\infty} \frac{1}{r^2} = \frac{\pi^2}{6} $ , can this series be used here, if so how?","['integration', 'calculus', 'definite-integrals']"
4698864,Is the empty set a collection of subsets?,"Let $X$ be an arbitrary set and $\mathcal{C}$ any collection of subsets of $X$ . Does this ""definition"" of $\mathcal{C}$ allows $\mathcal{C}=\emptyset$ or must $\mathcal{C}$ contain at least one element?","['elementary-set-theory', 'terminology']"
