question_id,title,body,tags
3736346,"$\int_0^1f(x) dx =0$, $\int_0^1xf(x) dx =0$. How to show that f has at least two zeros?","$f:[0,1]\to \mathbb{R}$ is a countinous function. $$\int_0^1f(x) dx =0 \qquad \mbox{ and } \qquad \int_0^1xf(x) dx =0. $$ If $f \ge 0$ ( $f\le0$ ) were true then $\int_0^1f(x) dx \ge0$ ( $\int_0^1f(x) dx \le0$ ). This is a contradiction, we can conclude that $f$ changes sign. By intermediate value property there exist a point $c$ such that $f(c)=0$ . This is the first zero. By using mean value theorems for integrals I can also show that a zero does exist. I can't show that these zeroes are different from each other. How to show a second zero exists?","['calculus', 'definite-integrals', 'real-analysis']"
3736352,Simplify $\frac{4\cos ^2\left(2x\right)-4\cos ^2\left(x\right)+3\sin ^2\left(x\right)}{4\sin ^2\left(x\right)-\sin ^2\left(2x\right)}$,"Simplify: $$\frac{4\cos ^2\left(2x\right)-4\cos ^2\left(x\right)+3\sin ^2\left(x\right)}{4\sin ^2\left(x\right)-\sin ^2\left(2x\right)}$$ After the substitution as $\cos(x)=a$ and $\sin(x)=b$ , $(a^2+b^2=1)$ , the expression becomes $$\frac{4(a^2-b^2)^2-4a^2+3b^2}{4b^2-4a^2b^2}=\frac{4a^4-8a^2b^2+4b^4-4a^2+3b^2}{4b^4}=\bigg(\frac{a^2}{b^2}-1\bigg)^2-\frac{4a^2-3b^2}{4b^4}$$ But I don't think I got anything useful... Any help is appreciated.","['algebra-precalculus', 'trigonometry']"
3736399,"Compact KÃ¤hler manifolds with $dim_\mathbb{C}H^{1,1}(X)=1$","In Wells's book ã€Šdifferential analysis on complex manifoldsã€‹ page219, there is a statement:"" any compact KÃ¤hler manifold $X$ with the property that  dim $_\mathbb{C}H^{1,1}(X)=1$ is necessarily Hodge. This follow from the fact that multiplication by an appropriate constant will make the KÃ¤hler form on $X$ integral."" But I know by Kodaira's Embedding theorem, a compact KÃ¤hler manifold is projective if and only if the KÃ¤hler class $[\omega]\in H^2(X,\mathbb{Z})$ , I don't see dim $_\mathbb{C}H^{1,1}(X)=1$ imply $[\omega]\in H^2(X,\mathbb{Z})$ , the reason is that $H^2(X,\mathbb{Z})$ can be seen as integral lattice in the vector space $H^2(X,\mathbb{C})$ , and 1-dimensional $H^{1,1}(X,\mathbb{C})$ can be seen as a 1-dimensional sub vector space of $H^2(X,\mathbb{C})$ , the intersection of $H^2(X,\mathbb{Z})$ and $H^{1,1}(X,\mathbb{C})$ may probably be empty? so that the compact KÃ¤hler manifold $X$ with dim $_\mathbb{C}H^{1,1}(X)=1$ is not necessarily Hodge? Is that right?","['complex-geometry', 'algebraic-geometry']"
3736416,Why do compact groups have only countably many irreducible representations?,"I read that for a compact Hausdorff group $G$ , the set $\hat{G}$ of isomorphism classes of irreducible unitary representations of $G$ is countable. Why is this the case?","['group-theory', 'representation-theory', 'compactness']"
3736481,"Conjecture If f is surjective then there exists x $\in$ (a, b) such that $|f'(x)| = 1$","Conjecture Let $f$ be a continuous function from [a, b] to [a, b], and is differentiable
on (a, b). If f is surjective then there exists x $\in$ (a, b) such that $|f'(x)| = 1$ Any counter example for this conjecture ? **Addition after Kavi Rama Murthy'answer
**, we can improve the problem by: If $f(a)\leq f(b)$ and f is surjective then there exists x $\in$ (a, b) such that $f'(x)= 1$",['analysis']
3736494,"When writing $\int_{1}^{x}x^2dx$, is $x$ a variable or a constant?","If the $x$ in $\int_{1}^{x}x^2dx$ is a constant, then the way in which people evaluate it doesn't make sense to me. For example, say $x=5$ , then intuitively I would think of the integral as being the same as $$
\int_{1}^{5}5^2=\left[25x\right]_{1}^{5}=25(5)-25(1)=25(4)=100
$$ However, this does not give the correct answer. Presumably, this is because we are looking at the area under of the graph of $y=x^2$ , rather than the graph of $y=25$ . However, if this is the case, then it seems $x$ is being treated as both a variable and a constant: a constant in the sense that it is one of the fixed bounds of integration; and a variable in the sense that we are looking at the $y-$ values as $x$ changes. Putting these two together, it seems like $x$ is going from $1$ to $x$ , which doesn't make sense to me. It also seems that people try to get around this problem in a number of ways. The natural logarithm is defined as $$
\ln(x)=\int_{1}^{x}\frac{1}{t}dt
$$ If it is correct to write $\int_{1}^{x}x^2dx$ , then why don't we defined the natural logarithm as $$
\ln(x)=\int_{1}^{x}\frac{1}{x}dx
$$","['calculus', 'definite-integrals', 'logarithms']"
3736547,Can $\cos(2\pi/17)$ be written as nested square roots only?,"From another math.stackexchange.com question we have Can this expression be written as a nested square root only?  ie, where all of the a's are integers? Edit: From the comment, it seems that it may be more interesting to look at After doing some more research, I found that This was based on this answer to a related question. . So it appears that it would be possible with an infinite nested square root.  How about a finite nested square root?  Would that be possible?","['nested-radicals', 'radicals', 'field-theory', 'trigonometry', 'prime-numbers']"
3736567,Evaluate $\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin 1|+2|\sin 2|+\cdots+k|\sin k|}{k^3}$.,"Evaluate $$\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin  1|+2|\sin
 2|+\cdots+k|\sin k|}{k^3}.$$ At least, we can estimate an upper bound as follows: \begin{align*}
\sum_{k=n+1}^{2n}\frac{|\sin
1|+2|\sin 2|+\cdots+k|\sin k|}{k^3}&\le\sum_{k=n+1}^{2n}\frac{1+2+\cdots+k}{k^3}\\&=\sum_{k=n+1}^{2n}\frac{k+1}{2k^2}\\&=\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k}+\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k^2}\\&\to \frac{1}{2}\ln 2(n \to \infty).
\end{align*} But how to obtain the lower bound?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3736573,Uniform continuity of a function: multidimensional distribution function,"If a distribution function $F$ defined on $\mathbb{R},$ is continuous then it's uniformly continuous, this is easy to prove, since more generally if $f$ is continuous on $\mathbb{R}$ and has finite limits on $+\infty$ and $-\infty,$ then it's uniformly continuous on $\mathbb{R}.$ Is it true that continuous multidimensional distribution functions on $\mathbb{R^d},$ $F(x_1,...,x_d)=P(X_1\leq x_1,...,X_d \leq x_d)$ for a vector random variable $(X_1,...,X_d)$ ? In other word, if we have a continuous function $f:\mathbb{R}^d \rightarrow \mathbb{R},$ having finite limits at $\lim_{x \rightarrow +\infty}f(x)=l_1,\lim_{x \rightarrow -\infty} f(x)=l_2$ (meaning that each term is tending to $\infty$ ), then $f$ is uniformly continuous. Is there are any references or a proof for this statement?","['measure-theory', 'uniform-continuity', 'analysis', 'real-analysis', 'probability-theory']"
3736575,An Interesting Question I Posed to Myself About $\pi$ as an Average.,"Prove or disprove: There is a sequence $x$ with each $x_i\in\{1,2,3,4\}$ so that $\pi$ can be written as the average $$\pi = \lim_{n\rightarrow\infty}\sum_{i=1}^{n}\frac{x_i}{n}$$ I am sure that this question would be trivial using advanced number theory concepts, but I would like a solution using just high-school olympiad level mathematics. Thanks a lot. â˜º","['elementary-number-theory', 'pi', 'sequences-and-series', 'limits', 'algebra-precalculus']"
3736583,Solve the equation: $\left|3^x - x\right|\left|3^x + x - 4\right| = 49$,"I want to solve the equation in $\mathbb{R}$ : $$ \left|3^x - x\right|\left|3^x + x - 4\right| = 49 $$ My attempt: The above equation is the same as: $$ \left(3^x - x\right)\left(3^x + x - 4\right) = \pm 49 $$ Case 1: $\left(3^x - x\right)\left(3^x + x - 4\right) = 49$ By taking derivative of the LHS, I managed to prove that it is monotonically increasing. Therefore, the equation has at most $1$ solution. By plugging in $\space x = 2,\space$ the equation is satisfied, so the first solution is $\space x = 2$ . Case 2: $\left(3^x - x\right)\left(3^x + x - 4\right) = -49$ I was not able to guess any integer nor rational solutions to this case, so I have to expand it: $$ x^2 - 4x + 4 \times 3^x - 3^{2x} = 49 $$ $$ (x - 2)^2 - \left(3^x - 2\right)^2 = 49 $$ This is where I got stuck and could not go further for a long time. I plugged it in WolframAlpha and received one solution: $\space x \approx -5.27928$ . I would like to know how to solve this equation appropriately, thanks in advance.","['algebra-precalculus', 'exponential-function', 'roots']"
3736591,Matrix of the linear transformation $T : \{0\} \to \{0\}$,"I am kindly asking for someone to clarify the following doubt. Is there a matrix for a linear transformation $T : \{0\} \to \{0\}$ ? I can generalize this question to the following. Is there a matrix for a linear transformation mapping the trivial vector space to another vector space (or vice-versa)? I believe there isn't since a matrix (as Axler defines it in the 3rd edition of Linear Algebra Done Right ) is defined for some positive integers $m$ and $n.$ Specifically, the integer $m$ specifies the dimension of the vector space in the domain, and the integer $n$ specifies the dimension of the codomain. As the dimension of the trivial vector space is $0,$ I believe there is no matrix for such linear transformation. Thank you.","['matrices', 'linear-algebra', 'linear-transformations']"
3736628,$\det(I+A)=1+\operatorname{Tr}(A)$ if $\operatorname{rank}(A)=1$,"Let $A$ be a complex matrix of rank $1$ . Show that $$\det (I+A) = 1 + \operatorname{Tr}(A)$$ where $\det(X)$ denotes the determinant of $X$ and $\operatorname{Tr}(X)$ denotes the trace of $X$ . Any hint, please. I do not get how to combine the ideas of rank, determinant and trace. Thank you.","['determinant', 'trace', 'rank-1-matrices', 'matrices', 'linear-algebra']"
3736630,Tangent Plane in the graph of $SL(3)$,"Let $SL(3)$ be the matrix $3 \times 3$ with determinant $1$ . $a)$ Show that $SL(3)$ is, locally, graph of a continuous differentiable function. $b)$ Determine the dimension of the tangent plane for each point of this graph. $c)$ Compute the tangent plane of the graph in the point $X_0 = Id$ . The first question I did, using the Implicit Function Theorem, and I found $g: U \subset \mathbb{R}^8 \rightarrow V \subset \mathbb{R}$ . The second, I guess the tangent plane has dimension $8$ . But I do not nkow how compute the tangent plane in $X_0 = Id$ .","['multivariable-calculus', 'calculus', 'real-analysis']"
3736648,Easiest way to prove the RouchÃ©â€“Capelli theorem,"RouchÃ©â€“Capelli theorem (Kroneckerâ€“Capelli theorem/RouchÃ©â€“FontenÃ© theorem/RouchÃ©â€“Frobenius theorem/Frobenius theorem) states that for the non-homogeneous system Ax = b, $(i)$ $Ax = b$ has a unique solution if and only if $rank[A] = rank[A|b] = n$ $(ii)$ $Ax = b$ is inconsistent (i.e., no solution exists) if and only if $rank[A] <
rank[A|b]$ $(iii)$ $Ax = b$ has infinitely many solutions if and only if $rank[A] = rank[A|b] < n$ How do I derive these conditions ? My Understanding $(i)$ $Ax=b$ has a unique solution $A^{-1}$ exists $\implies \boxed{x=A^{-1}b} \implies |A|\neq 0 \implies rank(A)=n$ If solution exists, then $\vec{A}_1x_1+\vec{A}_2x_2+....+\vec{A}_nx_n=b\implies b$ is a linear combination of the column vectors $\implies rank[A|b]=rank[A]=n$ $(ii)$ $Ax=b$ is inconsistent (i.e., no solution exists) $\boxed{|A|x=(adj A)b\implies |A|=0 \quad\&\quad adj A.b\neq 0}$ $|A|=0\implies rank[A]<n$ Is there a way to prove the last two conditions of the RouchÃ©â€“Capelli theorem ?","['matrix-rank', 'linear-algebra']"
3736658,On the proximity of $a\sqrt b+b\sqrt a$ to an integer,"Let $\Sigma$ denote all ordered pairs $(a,b)$ of positive, square-free integers with $a> b$ . What is the infimum of all $\kappa$ such that $$\left\{(a,b) \in \Sigma:a\sqrt b+b\sqrt a\,\,\text{is within}\,\,\frac1{(ab)^{\kappa}}\,\,\text{of an integer}\right\}$$ has finite cardinality? Note that if $x$ has irrationality measure $\mu$ then $\mu$ is the infimum such that $$0<\left|x-\frac pq\right|<\frac1{q^{\mu}}$$ has finitely many solutions for $p,q$ integers. Here, the problem can be rephrased as determining $\kappa$ such that, $$0<\left|a\sqrt b+b\sqrt a-K\right|<\frac1{(ab)^\kappa}$$ has finitely many solutions for an integer $K(a,b)$ which is either $\lfloor a\sqrt b+b\sqrt a\rfloor$ or $\lceil a\sqrt b+b\sqrt a\rceil$ . This formulation is close to the definition of an irrationality measure (which is $2$ in this case), but is not directly related since the parameters $p,q$ cannot be matched. From empirical results, I believe that $\kappa\in[1,2]$ , as letting $\kappa=2$ yielded no solutions for a long time. The code in PARI/GP is squar(k)=for(a=2,+oo,for(b=2,a-1,if((issquare(a)==0 && frac(a*sqrt(b)+b*sqrt(a))<1/((a*b)^k)) || 1-frac(a*sqrt(b)+b*sqrt(a))<1/((a*b)^k),print1([a,b],"" "")))) While I recognise that the question posed at the beginning of this post is extremely difficult to determine exactly, I would appreciate proofs that $\kappa>1$ or $\kappa<2$ should they be true. Interestingly, when $a=b$ , I haven't managed to find any solutions when $\kappa=1$ . In fact, in this case, I conjecture that $\kappa\in[1/2,1]$ .","['number-theory', 'irrationality-measure']"
3736674,Derivative of the complex norm as commonly used in physics,"On the one hand, I read that the derivative of the complex conjugate $C[z]=\overline{z}$ is not differentiable anywhere (for instance see here ). (see 1, below) On the other hand, I see in physics taking the derivative of a complex scalar field to obtain the equation of motion using the Euler-Lagrange method (for instance see enter link description here (see 2, below) So which is it, can we or can we not take the derivative? For case 1, the reference states that a complex function is differentiable if and only if it satisfies the Cauchy-Riemann equations: $$
f[z]=f[x+iy]=u[x,y]+iv[x,y]
$$ Then f is differentiable if $$
\frac{\partial u}{\partial x} =\frac{\partial v}{\partial y} \\ 
\frac{\partial u}{\partial y} =-\frac{\partial v}{\partial x}
$$ Then for the complex conjugate $C[x+iy]=x-iy$ then $\partial u/\partial x =1$ and $\partial v/\partial y=-1$ . Consequently $C[z]=\overline{z}$ is not differentiable anywhere in the complex plane. For case 2, the physics paper defines the Lagrangian of a complex scalar free field as follows: $$
\mathcal{L}=(\partial \phi^*)(\partial \phi)
$$ Then they claim that $$
\frac{\partial \mathcal{L}}{\partial (\partial \phi)}=\partial \phi^*\\
\frac{\partial \mathcal{L}}{\partial (\partial \phi^*)}=\partial \phi
$$ To obtain these results I assume they apply the chain rule $$
\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* \partial \phi)=\partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* )+\partial \phi^* \frac{\partial }{\partial (\partial \phi)}(\partial \phi)
$$ Is the following term not an 'illegal' derivative of a complex conjugate function? $$
\partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* )
$$ Why are they allowed to pose it equal to 0?","['complex-analysis', 'derivatives', 'complex-numbers']"
3736698,"If for invertible matrices $A$ and $X$, $XAX^{-1}=A^2$ then eigenvalues of $A$ are $n^{th}$ roots of unity.","Question: Let $A$ and $X$ be two complex invertible matrices such that $XAX^{-1}=A^2$ . Show that there exists a natural number $n$ such that each eigenvalue of $A$ is an $n^{th}$ root of unity. I can say from here, $\operatorname{det}(A)=1$ and I guess somehow I have to show $A^n=I$ , for some $n$ , which will give the result. But I have no idea how to show it from the fact that $A$ and $A^2$ are similar matrices. Any hint!!","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3736729,Finding $t_{2020}$ when $t_n = \frac{5t_{n-1} + 1}{25t_{n-2}}$,"Define a sequence recursively by $t_1 = 20, t_2=21$ and $$t_n = \frac{5t_{n-1} + 1}{25t_{n-2}}$$ for all $n \geqslant 3.$ Then $t_{2020}$ can be written as $\frac{p}{q},$ where $p$ and $q$ are relatively prime positive integers. Find $p+q.$ This was asked on the AIME 2020 and while it can be solved by computing consecutive terms and finding the pattern that repeats every $5$ th step I would like to know if this could be solved by finding the characterstic equation for $t_n$ or by some other alternative approach?","['contest-math', 'algebra-precalculus', 'recurrence-relations', 'sequences-and-series']"
3736775,Prove or Disprove : There exists a continuous bijection from $\mathbb{ R}^2$ to $\mathbb{R} $,"This question was asked to me by a mathematics undergraduate to me and I was not able to solve it. So, I am asking it here. Prove or Disprove : There exists a continuous bijection from $\mathbb{ R}^2$ to $\mathbb{R} $ . I have no idea on how this problem can be tackled. It seems it has something to do with set theory but I only know elementary set theory( bijection from naturals) and  I am unable to solve it.","['elementary-set-theory', 'general-topology', 'analysis', 'real-analysis']"
3736798,"About the existence of an isomorphism of algebras between the algebras of $\mathcal{C}^0$ and $\mathcal{C}^1$ functions from $[0, 1]$ to $\mathbb{R}$","Let $\mathcal{C}^0 ( [0, 1], \mathbb{R})$ denote the set of continuous functions from $[0, 1]$ to $\mathbb{R}$ and $\mathcal{C}^1 ( [0, 1], \mathbb{R})$ denote the set of class $\mathcal{C}^1$ functions from $[0, 1]$ to $\mathbb{R}$ , both of these sets are algebras over the field $\mathbb{R}$ . The question is whether we can find an isomorphism of algebras : $\Phi:\mathcal{C}^0 ( [0, 1], \mathbb{R})\longrightarrow\mathcal{C}^1 ( [0, 1], \mathbb{R})$ it seems, well at least to me, that such a cheesy isomorphism cannot exist, however I couldn't prove it. I tried the obvious choice, which is proof by contradiction, and tried taking the inverse of such a function which would map differentiable functions to continuous ones, considering that one set already contains the other, however no contradiction seemed to arise.","['abstract-algebra', 'linear-algebra', 'vector-space-isomorphism']"
3736802,Closest matrix that achieves positive semidefinite condition,"Suppose we have two symmetric positive semidefinite $n$ dimensional matrices $A$ and $B$ . We use the notation $X\leq Y$ means that $Y-X$ is positive semidefinite. Suppose $A \not\leq B$ i.e. $B-A$ has at least one negative eigenvalue. We are interested in perturbing $A$ to some positive semidefinite $\tilde{A}$ such that $\tilde{A} \leq B$ while minimizing $|A-\tilde{A}|_1$ where $|\cdot|_1$ is the nuclear norm and defined by $$|X|_1 := \text{Tr} \left( \sqrt{X^\dagger X} \right)$$ and $X^\dagger$ is the transpose conjugate of $X$ . To make things simpler, I will now consider the case where $A$ is a rank- $1$ matrix. Is it true that $$\tilde{A} = \lambda A$$ for some $\lambda < 1$ ? An immediate corollary is that $\tilde{A}\leq A$ . EDIT: After a bit of searching, I found a result for the same question but where the norm considered is the induced 2-norm (spectral norm) or the Frobenius norm. For the induced 2-norm (spectral norm), it holds that $\tilde{A} = A - \lambda I$ where $\lambda$ is the smallest positive number such that $\tilde{A}\leq B$ is true. So for this case, my conjecture that $\tilde{A} = \lambda A$ is false but the statement $\tilde{A}\leq A$ is true. For the Frobenius norm case, we first write the polar decomposition of $B-A = UH$ . Then $B -\tilde{A} = \frac{1}{2}(B - A + H)$ is the solution. Since $H= ((B-A)^\dagger(B-A))^{1/2}\geq B-A$ , one can again conclude that $\tilde{A}\leq A$ I do not know what happens for the 1-norm though. EDIT 2: Here is another look at the problem that almost works. Suppose the solution $\tilde{A}\not\leq A$ . We prove that there exists some $A'$ such that $A'\leq B, A'\leq A$ and $|A'-A|_1\leq|\tilde{A}-A|_1$ . Let us diagonalize $\tilde{A}-A = ZDZ^\dagger = ZD^{+}Z^\dagger + ZD^{-}Z^\dagger$ where $D$ is diagonal, $D^{\pm}$ is also diagonal and includes the nonnegative and negative eigenvalues respectively. By assumption $\tilde{A}\leq B \implies A + ZD^{+}Z^\dagger + ZD^{-}Z^\dagger \leq B$ . Define $A':= A + ZD^{-}Z^\dagger$ . Since $ZD^{+}Z^\dagger$ is positive semidefinite, it holds that $A' = A + ZD^{-}Z^\dagger \leq B$ . Since $ZD^{-}Z^\dagger$ is negative definite, it follows that $A'\leq A$ . Finally, $|A' - A|_1 = |ZD^{-}Z^\dagger|_1 = |D^{-}|_1 \leq |D^{+}+D^{-}|_1 =  |Z(D^{+}+D^{-})Z^\dagger|_1 = |\tilde{A} - A|_1$ EDIT 3 Unfortunately, the $A'$ constructed is not positive semidefinite in general.","['matrices', 'linear-algebra', 'positive-semidefinite', 'nuclear-norm']"
3736819,"Gauss Lemma - Do Carmo's Riemannian geometry, use of parallel transport?","I was exactly having the same doubt as this question . I don't understand specifically why $$
(d \exp_p)_v(v)=v
$$ I worked out exactly the same math as wikipedia and I ended up with $$
(d \exp_p)_v(v) = \frac{d}{dt}\left. \left(\gamma((t+1),p,v) \right) \right|_{t=0}
$$ The equation is based on the use of the curve $\alpha(t) = (t+1)v$ where $v \in T_p M$ . Apparently the key in understanding how to fill the gap is to use somehow the parallel transport but I couldn't figure from the given answer actually. The specific bit I can't figure is that apparently, from one of the comments, it might be the case that the result of $(d \exp_p)_v(v)$ is actually the parallel transport of $v$ along the geodesic passing through $\exp_p(v)$ . Can anyone clarify?","['proof-explanation', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3736861,Determinant of a Toeplitz matrix,"How can I calculate the determinant of the following Toeplitz matrix? \begin{bmatrix}
1&2&3&4&5&6&7&8&9&10\\
2&1&2&3&4&5&6&7&8&9 \\
3&2&1&2&3&4&5&6&7&8 \\
4&3&2&1&2&3&4&5&6&7 \\
5&4&3&2&1&2&3&4&5&6 \\
6&5&4&3&2&1&2&3&4&5 \\
7&6&5&4&3&2&1&2&3&4 \\
8&7&6&5&4&3&2&1&2&3 \\
9&8&7&6&5&4&3&2&1&2 \\
10&9&8&7&6&5&4&3&2&1 \\
\end{bmatrix}","['matrices', 'toeplitz-matrices', 'determinant', 'linear-algebra']"
3736873,Does there exist an operation that could turn the set of all negative real numbers into an abelian group? [duplicate],"This question already has answers here : Operation that makes the negative real numbers, $\mathbb{R}_{<0}$, a group (3 answers) Closed 4 years ago . The answer is no for familiar operations of addition and multiplication. But could there exist any other operation that could turn the set of all negative real numbers into an abelian group. If yes, what is it? If no, how could I prove it?","['group-theory', 'abelian-groups']"
3736901,Understanding the tensor product of functions/maps,"Let $K^A = \{f \,|\, f\colon A \longrightarrow K\}$ be the set of functions from $A$ (some arbitrary set) to $K$ (a field, I guess, I want to be able to call $K^A$ a $K$ -vector space). Given some other set $A'$ I'd like to know when can we say $$K^A\otimes K^{A'} \subseteq K^{A\times A'}$$ and when can we say they're equal, or when is this true for subalgebras (like $C^\infty(A)$ , $L^2_\mu(A)$ or $A^* = \mathrm{Hom}(A;K)$ , for instance, depending on the structure $A$ has). The tensor product of functions would be define so that $$(f\otimes g)(x,y) = f(x)g(y).$$ I tried doing some research and piecing together what I've learnt so far. Most resources don't talk about purely algebraic vector spaces, though, and I don't know much about Banach spaces, functional analysis or real-variable analysis and it's being a little hard. For the case of linear forms (so $A,A'$ are $K$ -vector spaces) it is true that their tensor product yields a ""bilinear form"" $A\times A'\longrightarrow K$ , so $A^*\otimes A'^* \equiv \mathrm{Bil}(A,A';K)$ . It can't be true in general, because rank-1 elements of the tensor product correspond to (product-wise) separable functions and not every function is a linear combination of separable functions. For example, the function $\delta\colon \mathbb{R}\times\mathbb{R} \longrightarrow \mathbb{R}$ defined to be $0$ everywhere except $1$ wherever $x = y$ is not a linear combination of separable functions. For $L^2(A)$ , at least when $A\subseteq \mathbb{R}^n,A'\subseteq\mathbb{R}^m$ with the Lebesgue measure, it isn't true that $$L^2(A)\otimes L^2(A') = L^2(A\times A')$$ but it is true if we use the ""Hilbert"" tensor product (the closure of the usual tensor product with respect to the topology induced by the metric). This is because the topology of the Hilbert space allows for ""infinite linear combinations"" to make sense whenever they're convergent. I know this question might be too broad (sorry!), but I'm sure there's some concept to know or reference to read from which I could learn more about this.","['tensor-products', 'multilinear-algebra', 'topological-vector-spaces', 'functional-analysis']"
3736948,What does this symbol $âˆˆ_ð‘…$ mean?,"What does this symbol $âˆˆ_ð‘…$ mean? I know what $x âˆˆ A$ stand for but what difference there is if we write $x âˆˆ_R A$ ? For those who wonder where did I encounter this symbol it was on one of a lecture slides on cryptography, pseudo random generators section. The excerpt said the following : For many cryptographic protocols, we need randomly chosen numbers $ð‘Ž
âˆˆ_ð‘… â„¤_ð‘Ÿ = [0,ð‘Ÿâˆ’1]$ .","['elementary-set-theory', 'notation', 'cryptography']"
3736967,Find the extremums of complicated function,"I have a function: $$f(x,y) = x^2y^3(6-x-y)$$ I found partial derivatives: $$f_{x}(x,y)^{'} = xy^3(12-3x-2y)=0$$ $$f_{y}(x,y)^{'} = x^2y^2(18-3x-4y)=0$$ And extremums: $$(0,0);(0,3);(2,0);(2,3)$$ To find maximum and minimum I take second derivative: $$f_{xx}(x,y)^{''} = 24xy^3-6xy^3-2xy^3=0$$ $$f_{xy}(x,y)^{''} = 36x^2y^2-9x^2y^2-3x^2y^2=0$$ $$f_{yy}(x,y)^{''} = 36x^2y-6x^3y^2-6x^2y=0$$ And you see in cases with coordinates with zeros its not clear how to find out whether they are maximums or minimus. How to do that?","['multivariable-calculus', 'derivatives']"
3737065,The derivative of $f(x)=\frac{3 \sin x}{2+\cos x}$,"My solution: The background $$\begin{align} &3\frac{d}{dx}\left(\frac{\sin(x)}{2+\cos(x)}\right)\\
&=3\frac{\frac{d}{dx}(\sin(x))(2+\cos(x))-\frac{d}{dx}(2+\cos(x))\sin(x)}{(2+\cos(x))^2}\\
&=3\frac{\cos(x)(2+\cos(x))-(-\sin(x))\sin(x)}{(2+\cos(x))^2}\\
&=\frac{3+6\cos(x)}{(2+\cos(x))^2}\end{align}$$ However, I plugged $f(x)=\dfrac{3 \sin x}{2+\cos x}$ into the derivative calculator in wolfram alpha and received the following calculation: $$\frac{3\left(\sin^2(x)+\cos^2(x)+2\cos(x)\right)}{(2+\cos(x))^2}$$ Is my solution incorrect and the one from wolfram alpha correct? If so, where did I go wrong?","['calculus', 'derivatives', 'trigonometry']"
3737134,Failed solution for solving $\cos(\theta) = -\sin(-\theta)$,"I'm trying to solve $\cos(\theta) = -\sin(-\theta)$ on the interval $[0, 2\pi)$ ,  but having trouble identifying what I'm doing wrong $$\cos(\theta) = -\sin(-\theta)$$ By even-odd identities: $$\sin(-\theta)=-\sin(\theta)$$ $$\cos(\theta)= -(-\sin(\theta))$$ $$\cos(\theta)=\sin(\theta)$$ Square both sides $$\cos^2(\theta)=\sin^2(\theta)$$ By Pythagorean identities: $\sin^2(\theta)=1-\cos^2(\theta)$ $$\cos^2(\theta)=1-\cos^2(\theta)$$ $$2\cos^2(\theta)=1$$ $$\cos^2(\theta)=\frac{1}{2}$$ $$\cos(\theta)=\frac{1}{\sqrt2}$$ $$\theta = \frac{\pi}{4}, \frac{7\pi}{4}$$ I know the correct solutions are $\dfrac{\pi}{4}, \dfrac{5\pi}{4}$ . Why am I missing $\dfrac{5\pi}{4}$ and in its place have $\dfrac{7\pi}{4}$ instead?","['algebra-precalculus', 'trigonometry']"
3737147,Missing solutions from $\tan(\theta)=2\sin(\theta)$,"I'm trying to solve $\tan(\theta)=2\sin(\theta)$ on the interval $[0,2Ï€)$ , but having trouble identifying what I'm doing wrong $$\tan(\theta)=2\sin(\theta)$$ Using quotient identity: $$\tan(\theta)= \frac{\sin(\theta)}{\cos(\theta)}$$ $$\frac{\sin(\theta)}{\cos(\theta)}=2\sin(\theta)$$ Divide both sides by $\sin(\theta)$ $$\frac{1}{\cos(\theta)}=2$$ Reciprocal identity: $$\frac{1}{\cos(\theta)}=\sec(\theta)$$ $$\sec(\theta)=2$$ $$\theta =\frac{\pi}{3}, \frac{5\pi}{3}$$ However, I know that I'm missing solutions $0$ and $\pi$ . I have seen a solution elsewhere online that moves everything to the LHS and then uses the zero-product-property to solve: $$\tan(\theta)=2\sin(\theta)$$ $$\frac{\sin(\theta)}{\cos(\theta)}=2\sin(\theta)$$ $$\frac{\sin(\theta)}{\cos(\theta)}-2\sin(\theta)=0$$ Factor out $\sin(\theta)$ $$\sin(\theta)\left(\frac{1}{\cos(\theta)}-2\right)=0$$ Use zero-product-property $\sin(\theta)=0$ or $\dfrac{1}{\cos(\theta)}-2=0$ From here the solutions are $\theta =0, \dfrac{\pi}{3}, \pi, \dfrac{5\pi}{3}$ Still, I don't understand why solutions were missing from the first method and how might I avoid such a mistake in the future?","['algebra-precalculus', 'trigonometry']"
3737168,"Computation of $\mathrm{Ext}_R(R[x^{-1}],M)$","Let $x$ be an element of a commutative ring $R$ . It seems that the following statements are true: Let us write $R[x^{-1}]$ as the direct limit $$R[x^{-1}]\simeq \varinjlim(R\xrightarrow{x} R\cdots ).$$ Let $M$ be an $R$ -module. Let $TM$ denote the tower $$( \cdots \rightarrow M \xrightarrow{x} M \xrightarrow{x} M). $$ Then $$\mathrm{Ext}^1_R(R[x^{-1}],M) \simeq {\lim}^1 TM$$ For $i\ge 2$ , we have $$ \mathrm{Ext}^i_R(R[x^{-1}],M) \simeq 0 $$ How does one prove each of these statements? For the 2nd point I wonder if there is a simple projective resolution.","['homological-algebra', 'modules', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3737242,"When defining ordered pairs, are there any important distinctions between $\{\{a\},\{a,b\}\}$ and $\{a,\{b\}\}$?","The formal Kuratowski definition of ordered pair is that $\langle a,b\rangle = \{\{a\},\{a,b\}\}$ . While I think I understand the above definition well I wanted to check if below definition also works just fine (and hence is ""equivalent"" to Kuratowski definition) $$\langle a,b\rangle  = \{a,\{b\}\}.$$ I think that both the definitions are just fine, but maybe I'm missing a subtle point. Also is there any reason to prefer Kuratowski's definition over the later one?",['elementary-set-theory']
3737305,Problem about differentiability and continuity,"Suppose I have a question. Find the values of a and b, if the function f is defined as, $$
f(x)=\left\{\begin{array}{l}
x^{2}+3 x+a, x \leq 1 \\
b x+2, x>1
\end{array}\right.
$$ So, what my teacher asks me to do is this,
Calculate the $LHD$ and $RHD$ (Left hand derivative and Right hand derivative) At $x=1$ $LHD$ is, $$
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\
=\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\
=\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\
=\lim _{h \rightarrow 0}(5-h) \\
=5
\end{array}
$$ $f(1-h)$ means the case when $x < 1$ , means I have to use $x^{2}+3 x+a$ And $f(1)$ means the same case, when $x \leq 1$ And $RHD$ is, Warning: this is the problem region $$
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h+2-b(1)-2}{h} \\
=b
\end{array}
$$ As they are given differentiable at $x=1$ . Equating them, $LHD=RHD$ $b=5$ And uses continuity (equating $LHL$ and $RHL$ and $f(1)$ ) to find another equation and solve it to find $a$ Giving $a=3$ , and $b=5$ Well, in the $RHD$ didn't he use the wrong case for $f(1)$ , he should've use the case when $x \leq 1$ , why the heck did he use $x>1$ , and Everyone is telling me the logic behind this is that we are calculating $RHD$ , which means everything lies in the right neighbourhood of $1$ even $f(1)$ , I get the logic but that is still breaking rules of math, What I do is, I calculate the $LHL$ and $RHL$ and f(1) and equate them to get the same equation, yes, i have the equation for continuity as my teacher. $LHL$ $$
\begin{array}{l}
\quad \lim _{x \rightarrow 1^{-}} (x^{2}+3 x+a) \\
=\lim _{h \rightarrow 0} (1+h^{2}-2 h+3-3 h+a) \\
=4+a
\end{array}
$$ $RHL$ $$
\begin{array}{l}
\lim _{x \rightarrow 1^{+}}(b x+2) \\
\lim _{h \rightarrow 0}(b+b h+2) \\
\quad=b+2
\end{array}
$$ Now, $$
\begin{array}{c}
L H L=R H L \\
b+2=4+a \\
b-a=2   \hspace{10mm}. ..eq(1)
\end{array}
$$ Now solving for differentiability,
The same $LHD$ , $$
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\
=\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\
=\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\
=\lim _{h \rightarrow 0}(5-h) \\
=5
\end{array}
$$ And now $RHD$ , $$
\begin{array}{c}
\lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h+2-4-a}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h-2-a}{h} \\
\text { Using } eq(1)... 
\end{array}
$$ $$
\begin{array}{l}
=\lim _{h \rightarrow 0} \frac{2+b h-2}{h} \\
=\quad b
\end{array}
$$ $$
b=5\\
a=3
$$ I used the right case for $f(1)$ that is, $x^{2}+3 x+a$ , for $x \leq 1$ So, $f(1)=4+a$ , So, who did this right, my teacher who used the $x>1$ case in the $RHD$ , or me who used the actual $x \ leq 1$ case. This is what I think, differentiability and continuity are related, as we know A differentiable function is always continous but not every continous function is differentiable. So continuity is a much-needed required step to solve for differentiability, so I should solve for continuity first. As I did. Anything just say? Is he right??","['limits', 'calculus', 'derivatives', 'continuity']"
3737306,Maximum Principle for Minimal Surface Equation with Dirichlet Boundary Condition,"I'm an undergraduate student and I'm currently reading a classical paper for my final project for the course differential geometry on the Bernstein problem of minimal surfaces, namely, the paper: Bombieri, Enrico, E. De Giorgi, and Enrico Giusti, "" Minimal Cones and the Bernstein Problem "" Inventiones Mathematicae 7.3 (1969): 243-268. In equation \eqref{1}, the authors considered the folowing Dirichlet problem for the minimal surface equation: $$
\begin{cases}
    \sum_{i=1}^{n} \left( D_i \left( \dfrac{D_i f}{\sqrt{1+\vert D f \vert^2}} \right) \right) = 0, \qquad f\in C^2(B_R), \\
    f=f_1\quad  \text{in} \quad \partial B_R
\end{cases}\label{1}\tag{25}
$$ where $B_R$ is the unit ball in $\mathbb{R}^8$ . ( NOT in three-dimensional Euclidean space ) We have known the existence and uniqueness of the solution of such a boundary problem. Denote its solution by $f^{(R)}(x)$ . Similarly, we consider the same boundary problem with function $f_2$ on $\partial B_R$ . With some assumptions and calculations mentioned in the paper before, we have obtained that $$
f_1(x) \leq f^{(R)}(x) \leq f_2(x)
$$ on the boundary $\partial(B_R \cap D_1)$ . Here comes my question which has been puzzling me for a long time: The authors claims that: ""by the well-known maximum principle for solutions of the Dirichlet problem and equation \eqref{2} and \eqref{3}"" (listed below), we obtained that $$
f_1(x) \leq f^{(R)}(x) \leq f_2(x) \, \text{for} \,  x \in \bar{B}_R\cap\bar{D_1}.
$$ I'm confused at the ""maximum principle mentioned there. I have learned the strong maximum principle and the Hopf maximum principle for Laplacian equations (with corresponding boundary conditions), but I have no idea how to apply these here. Or, it there any maximum principle stated for the minimal surface equation in the above contexts? I tried but find no reference for such a theorem. (For example, the book on elliptic PDEs by David Gilbarg, et.al). Moreover, I have no idea on the role played by the equation \eqref{2} and \eqref{3}. P.S. I list here equations \eqref{2} and \eqref{3}: $$
\int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{1}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{1}\right|^{2}\right)^{-\frac{1}{2}} dx \leq 0\label{2}\tag{23}
$$ and $$
\int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{2}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{2}\right|^{2}\right)^{-\frac{1}{2}} dx \geq 0\label{3}\tag{24}
$$ where the region $D_1$ is defined as $D_1 = \{ x \in \mathbb{R}^8 \vert 0 \leq v \leq u \}$ , $u=\left(x_{1}^{2}+\cdots+x_{4}^{2}\right)^{\frac{1}{2}}$ and $v=\left(x_{5}^{2}+\cdots+x_{8}^{2}\right)^{\frac{1}{2}}$ . Thank you in advance! It is my first time to ask a question on MSE, and I am sincerely sorry for any possible mistakes and rudeness in this question. Thank you!","['elliptic-equations', 'partial-differential-equations', 'minimal-surfaces', 'differential-geometry']"
3737362,Induced map on the $K_0$ of punctured spectrum of completion,"Let $(R,\mathfrak m)$ be a local Gorenstein ring (may also assume excellent) of dimension at least $2$ , and let $(\hat R,\hat {\mathfrak m})$ be the $\mathfrak m$ -adic completion. Let $U:=\text{Spec}(R)\setminus \{\mathfrak m\}$ be the punctured spectrum of $R$ and and $\hat U:=\text{Spec}(\hat R)\setminus \{\hat {\mathfrak m}\}$ be the punctured spectrum of $\hat R$ . So we have a canonical map $K_0(U)\to K_0(\hat U)$ (where $K_0(-)$ denotes the Grothendieck group of vector-bundles) . My question is: Is this map necessarily injective ? If this is not true in general, what conditions on $R$ would make it true ?","['algebraic-k-theory', 'algebraic-vector-bundles', 'gorenstein', 'algebraic-geometry', 'commutative-algebra']"
3737390,Inverse Laplace transform of $\frac{\sqrt{2s}}{\sinh\sqrt{2s}}$ and $\frac{1}{\cosh\sqrt{2s}}$ (related to Brownian motion),"I was reading ""2018The computation of the probability density and distribution functions for some families of random variables by means of the Wynn-p accelerated Post-Widder formula"" and run into the following question. The paper gives two Laplace transforms: $$L_{X_S}(s)=\frac{\sqrt{2s}}{\sinh\sqrt{2s}}\ \text{and} \ L_{X_C}(s)=\frac{1}{\cosh\sqrt{2s}},$$ where $X_S$ and $X_C$ can be interpreted as the hitting time of a Brownian motion in $\mathbb{R}$ and $\mathbb{R}^3$ . The paper also says that the densities are: $$f_{X_S}(x)=\pi^2\sum_{k=1}^\infty(-1)^{k+1}k^2e^{-\frac{1}{2}k^2\pi^2x}\ \text{and} \ f_{X_C}(s)=\pi \sum_{k=0}^{\infty}(-1)^k\left(k+\frac{1}{2}\right)e^{-\frac{1}{2}(k+\frac{1}{2})^2\pi^2x}.$$ I am interested in the density functions, i.e., the inverse of Laplace transforms. There is one way in ""2001Probability laws related to the Jacobi theta and Riemann zeta functions, and Brownian excursions"", through $L_{X_C}$ . However, that paper mentions reciprocal property between $X_C$ and $X_C$ and between $X_S$ and $X_C$ , which I do not understand. In particular, since $\int_0^\infty e^{-sx}e^{-ax}dx=\frac{1}{s+a}$ for $a>0$ , directly applying Laplace transform to densities may lead to divergent series, e.g., $\sum_{k=1}^\infty(-1)^{k+1}\frac{k^2 \pi^2 }{s+\frac{1}{2}k^2\pi^2}$ . A similar expression $\sum_{n=-\infty}^{\infty}\frac{1}{n^2+b^2}=\frac{\pi}{b}\coth\pi b$ is also noticed, but I cannot see how to apply it so far. Any help would be appreciated.","['laplace-transform', 'probability-theory', 'stochastic-calculus', 'sequences-and-series']"
3737474,Finding The Tangent Line Of $y = \sin x$,"Hello everyone I have a tangent line with slope $= a$ to $y = \sin x$ in 2 points $(u_1,v_1) , (u_2  ,v_2)$ How can I prove that $\tan a = a$ ? My direction was to express $a$ with the points by $a = \frac{u_1-u_2}{v_1-v_2}$ and the tangent line equation is $y = ax -au_1 +v_1$ $f(x) = \sin x  \iff  f'(u) = \cos u$ so $\cos u_1 = a$ and $\cos u_2 = a$ ?","['trigonometry', 'functions']"
3737600,"Is the sequence $(B_n)_{n \in \Bbb{N}}$ unbounded, where $B_n := \sum_{k=1}^n\mathrm{sgn}(\sin(k))$?","This question is kind of an extension of a previous question I asked here . The infinite series $$\sum\frac{\mathrm{sgn}(\sin(n))}{n}$$ does converge, but I would like to know if Dirichlet's test can be used to prove the convergence with $$b_n=\mathrm{sgn}(\sin(n)).$$ So the question is, is the sequence $(B_n)$ given by $$B_n:=\sum_{k=1}^n\mathrm{sgn}(\sin(k))$$ unbounded? Loosely speaking it is a sum of $1$ 's and the sign changes every $\pi$ terms. Also it would be great to know if the sequence $(B_n)$ is unbounded for other (irrational) changing cycles.","['diophantine-approximation', 'sequences-and-series']"
3737610,"A subsequence of $\{|\sin n|\}$ that converges to $0$ ""fast""","It is well-known that for each $a\in [0,1]$ there is a subsequence of $\{|\sin n|\}_{n=0}^\infty$ that converges to $a$ . I am curious about the speed of convergence. In particular, for each $\alpha>0$ , is there a subsequence $\{|\sin n_k|\}_{k=0}^\infty$ which converges to $0$ ""fast"" in the sense that $$|\sin n_k|\le\frac{1}{n_k^\alpha},\,\forall\,k\in\mathbf N\,?$$ If not, what restriction should be imposed on $\alpha$ ?
Any hints or reference (to existing literature on it) will be highly appreciated.","['sequences-and-series', 'real-analysis']"
3737662,"Double integral $\int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy$ for regular function $f$","Let $f$ be a sufficiently regular function on the unit square. (For example, say $f\in C^1$ .)
Then, is there a way to simplify the integral $$\int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy ?$$ If $f\in C^1$ , then the integrand is bounded and therefore the integral is well-defined.","['integration', 'multivariable-calculus', 'multiple-integral']"
3737663,Solution to the equation $ y'' = 2 \alpha y - 2 \sqrt{y} $,"I'm trying to find the solution to the equation $ (A^2)'' = 2 \alpha A^2 - 2 A  $ on the interval $(0,1)$ where $A'(0) = 0,\, A(1) = 0 $ for $\alpha >0$ . Firstly I make the substitution $y = A^2$ , this reduces the equation to $y'' = 2 \alpha y -2\sqrt{y}.$ Since the equation is lacking the independent variable $x$ , one may make the substitution $y'=v$ which reduces the equation to $ v \frac{dv}{dy} = 2\alpha y - 2\sqrt{y}.$ By seperating variables, and using $y'=v$ , one has: $$ y' = \sqrt{\frac{2}{3}}\cdot\sqrt{C+3\alpha y^2 - 4 y^{1.5}}.$$ for some integration constant $C$ . This is as far as I've got without rurnning in to problems. I am not able to get this down to an explicit form. Question 1: Does there exist a solution to the above problem, and if one does can it be written explicitly. Question 2: If an explicit solution can't be determined, can we learn anything about it by looking at the $(y,y')$ phase plane? Question 3: Are there any other methods that could be applied to solve this, or to at least find some information about the solution? I'm not sure if you could apply asymptotics as it's a BVP on a finite interval. EDIT 1: You can use $y'(0) = 0$ to get rid of the $C$ in the above equation. You can the substitute A^2 straight back in to the equation for $y'$ to give: $$ 2A\cdot A' = \sqrt{\frac{2}{3}}\cdot A \cdot \sqrt{3\alpha A^2 - 4A} \implies A' = \frac{1}{\sqrt{6}} \sqrt{3\alpha A^2 - 4A}$$ Which, according to matlab, has the solution: $$A(x) =  -\frac{1}{3\alpha}\cdot\bigg\{2\cos\bigg[\frac{\sqrt{3\alpha}}{6}\bigg(C_1+x\sqrt{6}\bigg)\cdot \int_0^x \frac{1}{ln(t)}dt\bigg]-2\bigg\}$$ If anyone would like to point out any floors I've made it would be much appreciative! Also - It would still be interesting to hear the answers to the questions I have previously asked.","['boundary-value-problem', 'calculus', 'ordinary-differential-equations', 'dynamical-systems']"
3737705,Is there an optimal strategy for this game of card?,"This game is based on the concept of minimizing resource wastage. The Game Cards which do not have numbers are not used e.g. ace, king, queen, joker. For the remaining cards, regardless of the color and shape, the value of a card is equal to the number on it. The cards are shuffled and distributed between the two players. They get equal number of cards. In each round, both player pick a card a place it upside down on the table without showing the opponent to complete their moves. After this they show their cards to each other. The player with the bigger number on the card win the round. The points won by the round winner for this round is equal to the number on card played by the losing players card in this round. E.g if $A$ plays $6$ and $B$ plays $1$ in this round then $A$ wins the round and gets $1$ point. If the number on both cards are equal than the round is a draw and the game continues until someone wins the round or all the cards are used. The round winner get the points equal to the number on the last card played by the round loser. After a round is completed, the played cards are discarded and the game continues with the unused cards remaining with each player until all the cards are used At the end, the player with the greater sum of points win the game. Minimizing wastage : Clearly we want to win a round by using the smallest card. E.g. if $B$ plays $1$ then $A$ will win the round and get $1$ point if he/she puts any card from $2$ to $10$ . But playing a $10$ to win $1$ point is a waste as $10$ can be played to win against bigger numbers later in the match. However before showing, neither player knows what card was played by the opponent. One may argue that if a player uses up the big cards in the beginning to make small win, it may leave him/her weaker in the later rounds. But using bigger cards in the beginning, leaves the player with smaller cards which leaves the opponent less scope to win more points. Question : Is there a mathematically optimal strategy that maximizes the chances of winning? Note : One of the reason why poker is considered a sport as opposed to gambling is because is proven to be a game of strategy and chance and not chance alone.","['optimization', 'card-games', 'game-theory', 'recreational-mathematics', 'probability']"
3737712,The quadratic polynomial $P(x)$ has a zero at $x=2$. The polynomial $P(P(x))$ has only one real zero at $x=5.$ Compute $P(0).$,"The quadratic polynomial $P(x)$ has a zero at $x=2$ . The polynomial $P(P(x))$ has only one real zero at $x=5.$ Compute $P(0).$ If we have that $P(x) = ax^2 +bx+ c$ , we get from the first condition that $P(x) = (x-2)(bx+c).$ From here $P(P(x)) = (ax+bx+c -2)(b(ax+bx+c)+c)$ , but this just looks very messy and doesn't seem to be helpful at all. Is there some other trick here I'm missing?","['algebra-precalculus', 'quadratics', 'roots', 'polynomials']"
3737723,"Relation between order $n$, chromatic number $\chi(G)$ and diameter $\text{diam}(G)$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Is the following fact true? For a connected graph of order $n$ , chromatic number $\chi(G)$ and diameter diam $(G)$ we have $$\chi(G) + \text{diam}(G) \leq n+1$$","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
3737759,prove $\sum\cos^3{A}+64\prod\cos^3{A}\ge\frac{1}{2}$,"In every acute-angled triangle $ABC$ ,show that $$(\cos{A})^3+(\cos{B})^3+(\cos{C})^3+64(\cos{A})^3(\cos{B})^3(\cos{C})^3\ge\dfrac{1}{2}$$ I want use Schur inequality $$x^3+y^3+z^3+3xyz\ge xy(y+z)+yz(y+z)+zx(z+x)$$ then we have $$x^3+y^3+z^3+6xyz\ge (x+y+z)(xy+yz+zx)$$ But I can't use this to prove my question and I use this post methods links also can't solve my problem,use $AM-GM $ inequality $$\cos^3{A}+\dfrac{\cos{A}}{4}\ge\cos^2{A}$$ so $$LHS\ge \sum_{cyc}\cos^2{A}-\dfrac{1}{4}\sum_{cyc}\cos{A}+64\prod_{cyc}\cos^3{A}$$ use $$\cos^2{A}+\cos^2{B}+\cos^2{C}+2\cos{A}\cos{B}\cos{C}=1$$ it must to prove $$\frac{1}{2}+64\cos^3{A}\cos^3{B}\cos^3{C}\ge 2\cos{A}\cos{B}\cos{C}+\dfrac{1}{4}(\cos{A}+\cos{B}+\cos{C})$$","['inequality', 'lagrange-multiplier', 'analysis', 'multivariable-calculus', 'trigonometry']"
3737787,$\frac{x^2}{by+cz}=\frac{y^2}{cz+ax}=\frac{z^2}{ax+by}=2$,If $$\frac{x^2}{by+cz}=\frac{y^2}{cz+ax}=\frac{z^2}{ax+by}=2$$ then find the value of $$\frac{c}{2c+z}+\frac{b}{2b+y}+\frac{a}{2a+x}.$$ I think all the terms need to be manipulated in some way to get the corresponding terms from the expression whose value needs to be found. For example we have to go from $\frac{x^2}{by+cz}$ to $\frac{a}{2a+x}$ in some way or maybe from $\frac{x^2}{by+cz}$ to $\frac{c}{2c+z}$ or something like that. It's very hard to tell from which term to which term I need to go because all the variables are used. So I just tried to make a system of equations. $$x^2=2(by+cz)$$ $$y^2=2(cz+ax)$$ $$z^2=2(ax+by)$$ $$x^2+y^2+z^2=4(ax+by+cz)$$ $$(x-a)^2+(y-b)^2+(z-c)^2=a^2+b^2+c^2+2(ax+by+cz)$$ But I don't know how to proceed from here.,"['contest-math', 'algebra-precalculus', 'systems-of-equations', 'polynomials']"
3737791,Evaluation of double summation,"I am trying to evaluate the following summation : $\sum_{m=2}^{\infty}\sum_{n=1}^{m-1}\frac{1}{(2m-1)(2n-1)(2m-2n)}$ The above summation is a partial sum of another double summation, $\sum_{m=1}^{\infty}\sum_{n=1}^{m-1}\frac{1}{mn(m-n)}$ . This second summation is equal to 2 $\zeta(3)$ . Hence, the first summation is convergent. If we manipulate the summation that is in question, it turns out to be equal to $\sum_{m=1}^{\infty}\frac{H_{2m-1}}{(2m+1)^2}$ where $H_k$ represents the $k^{th}$ Harmonic number.","['harmonic-numbers', 'sequences-and-series']"
3737844,How can I apply 4th order Runge-Kutta to a Laplacian equation in spherical coordinates?,"Can someone please help me solve the Poisson-Boltzmann equation in spherical coordinates over the domain $r\in(r_0, \infty)$ with Runge-Kutta: $$
\frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2\frac{\partial\psi}{\partial r}\right) =a \sinh \left(\frac{\psi}{\psi_0}\right)
$$ here $r$ is the radial coordinate, $a$ and $\psi_0$ are constants. The boundary conditions are $\psi=\psi_0$ and $\psi'=b$ at $r=r_0$ . I made an attempt to put the equation into a form suitable for Runge-Kutta integration with a $u$ -substitution $u/r=\psi$ , yielding $$
\frac{\partial^2u}{\partial r^2}=a r\sinh\left(\frac{u}{r\psi_0}\right)
$$ And, following from this the Runge-Kutta method can be applied $$
\frac{\partial}{\partial r}\begin{bmatrix} 
u  \\
\partial u/\partial r \\
\end{bmatrix}=f\left(u, \frac{\partial u}{\partial r}, r\right)=\begin{bmatrix} 
\partial u/\partial r \\
ar\sinh\left(\frac{u}{r\psi_0}\right)
\end{bmatrix}
$$ This seems to work, but my question is: Is there a more direct way of applying Runge-Kutta without such a substitution?","['ordinary-differential-equations', 'poissons-equation', 'runge-kutta-methods', 'spherical-coordinates', 'numerical-methods']"
3737847,How do I simplify this sum of arccosines?,"After trying to solve a geometry problem, represented by the following image I've arrived at this expression: $\alpha=\arccos\left(\frac{d+r \cos \left(\varphi+\frac{\vartheta}2\right)}{\sqrt{d^2+r^2+2 dr\cos \left(\varphi+\frac{\vartheta}2\right)}}\right)+\arccos\left(\frac{d+r\cos \left(\varphi-\frac{\vartheta}2\right)}{\sqrt{d^2+r^2+2dr\cos\left(\varphi-\frac{\vartheta}2\right)}}\right)$ Is there a way to simplify such expression?","['trigonometry', 'geometry', 'summation']"
3737855,A problem concerning a parallelogram and a circle,"Sorry for the ambiguous title. If you can phrase it better, feel free to edit. ""A parallelogram $ABCD$ has sides $AB = 16$ and $AD = 20$ . A circle, which passes through the point $C$ , touches the sides $AB$ and $AD$ , and passes through sides $BC$ and $CD$ at points $M$ and $N$ , such that $\frac{BM}{MC} = \frac{1}{8}$ . Find $\frac{DN}{NC}$ ."" Apparently, I'm supposed to solve this using triangle similarity, because that's the chapter's name (But I'm open to other answers too!). I've tried marking the center of the circle and going from there, creating triangles and seeking similarity. But couldn't really go far without it getting overly complicated. Here's the picture:","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'power-of-the-point']"
3737874,Gradient of complex-valued function with respect to real and imaginary components,"Let $J(\mathbf{z})$ be a complex-valued (scalar) function where $\mathbf{z}\in \mathbb{C}^n$ , and write $\mathbf{z} = \mathbf{x} + i \mathbf{y}$ for real vectors $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ . In the book I'm reading, the gradient of $J$ with respect to $\mathbf{x},\mathbf{y}$ as $$
\begin{align}\frac{\partial J}{\partial \mathbf{x}} &= \frac{\partial J}{\partial \mathbf{z}} + \frac{\partial J}{\partial \mathbf{z}^*}\\[1mm] \frac{\partial J}{\partial \mathbf{y}} &= i\frac{\partial J}{\partial \mathbf{z}} -i \frac{\partial J}{\partial \mathbf{z}^*}\end{align},
$$ where $\mathbf{z}^*$ is the conjugate of $\mathbf{z}$ . (I simplified the notation because the one used in the text is quite ugly, e.g., $\mathbf{z}$ is written as $\mathbf{c} = \mathbf{c_c} + j\mathbf{c_s}$ .) My question is: how is this set of equations derived? I understand that there is a one-to-one mapping between $(\mathbf{x},\mathbf{y})$ and $(\mathbf{z},\mathbf{z}^*)$ , but I'm not sure how to deal with ""chain rule"" (if that's a proper term) for vector transformations.","['complex-analysis', 'partial-derivative', 'vector-analysis', 'chain-rule']"
3737884,Definition of gradient in a Riemannian manifold.,"I was doing exercise 8 from do Carmo's Riemannian geometry and I stumbled upon the definition of gradient given. Let $M$ be a Riemannian manifold... $f \in \mathcal{D}(M)$ .. the gradient of $f$ as a vector field $\text{grad} \; f$ on $M$ defined by $$
\langle \text{grad} \; f, v \rangle = df_p(v) \;\; p \in M, v \in T_pM \;\;\;\;\; (1)
$$ here $\langle \cdot , \cdot\rangle$ is the Riemannian metric on $M$ and $f$ is a differentiable function on $M$ . No the Riemannian metric is a bilinear map $$\langle \cdot,\cdot \rangle : T_p M \times T_p M \to \mathbb{R}$$ but the differential $df_p$ is a map between tangent spaces, namely $$
df_p : T_p M \to T_{f(p)} \mathbb{R} \cong \mathbb{R}
$$ So in a nutshell I'm confused about the equality in $(1)$ because the lhs is a scalar in the field while the rhs is vector, though isomorphic to the scalar field. This definition actually makes a bit tricky for me to understand how to do the exercises, because any of the computations I do give me equalities that don't really make sense. Can you clarify how the gradient is actually defined? I also own Tu's Differential Geometry , but I don't see these definitions (I'm kind of reading the two in parallel).","['definition', 'riemannian-geometry', 'differential-geometry']"
3737915,Does the operator $(\hat{f}\cdot m )^\vee$ maps Schwartz in it self?,"Given $m \in L^\infty$ and $\phi \in \mathcal{S}$ a Schwartz function, is it true that $(\hat{f}\cdot m)^\vee$ is a Schwartz function?? I trying to prove this so I could conclude that operator of the form $(\hat{f}\cdot m)^\vee$ maps $\mathcal{S}$ to it self. Attempt: Given $\alpha, \beta$ multi-index, we have to prove that $$\sup_{x \in \mathbb{R^n}}|x^\alpha\partial^\beta(\hat{f}\cdot m)^\vee(x)| < \infty. $$ When $\alpha = 0$ , using some properties of Fourier tranform, we get $$\partial^\beta(\hat{f}\cdot m)^\vee(x) = ((2\pi i \xi)^\beta\hat{f}(\xi)m(\xi))^\vee(x) = ((\partial^\beta f)^\wedge\cdot m)^\vee(x).$$ Then, taking the absolute value of the expression above and by definition of inverse Fourier transform, \begin{align*}
|\partial^\beta(\hat{f}\cdot m)^\vee(x)| = &
\left| \int_\mathbb{R^n} (\partial^\beta f)^\wedge(\xi)m(\xi) e^{2\pi i \xi\cdot x} d\xi  \right| \\
\leq & \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)||m(\xi)|d\xi \\
\leq &\|m\|_{L^\infty} \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)|d\xi  \\
=& \|m\|_{L^\infty} \|(\partial^\beta f)^\wedge\|_{L^1}.
\end{align*} The $L^1$ -norm of $(\partial^\beta f)^\wedge$ is finite, since this is a Schwartz function. My problem is for $\alpha \neq 0$ . For simplicity and in view of properties of Fourier transform, I changed $x^\alpha$ for $(-2\pi i x)^\alpha$ and I want to show that the supreme of $|(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)|$ over all $x \in \mathbb{R^n}$ is finite: \begin{align*}
|(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)| =  & |(-2\pi i x)^\alpha ((\partial^\beta f)^\wedge\cdot m)^\vee(x)| \\
= & |[\partial^\alpha((\partial^\beta f)^\wedge \cdot m)]^\vee(x)|.
\end{align*} How do I proceed from here?? Does it make sense the derivative $\partial^\alpha((\partial^\beta f)^\wedge \cdot m)$ ??","['harmonic-analysis', 'fourier-transform', 'analysis', 'real-analysis']"
3737919,Geometric interpretation of Isotropic vector,"For real inner product space $(V,\langle.,.\rangle)$ there is a $\Bbb C$ -bilinear form $( , )$ on $V\otimes\Bbb C$ . This extension gives rise
to a Hermitian inner product, again denoted by $( , )$ on $V^2\otimes\Bbb C$ . A vector $v\in V\otimes\Bbb C$ is isotropic if $(v, v) = 0$ . In physics a quantity is called isotropic if it has the same properties or characteristics along all axes. Q: what is the geometric interpretation of isotropic vector? Edit: There is a similar (maybe equal) concept known as Isotropic Vector Matrix . Is this related to isotropic vector?","['inner-products', 'vector-fields', 'riemannian-geometry', 'differential-geometry']"
3737946,"Show that $A=(a_{ij})_{1\le i,j\le5}$ is positive definite, where $a_{ij}=\frac1{n_i+n_j+1}$","Problem Let $A=(a_{ij})_{1\le i,j\le5}$ be a matrix such that $$a_{ij}=\frac1{n_i+n_j+1}\,,$$ where $n_i,n_j\in\Bbb{N}$ . Show that $A$ is positive definite if $n_1>n_2>n_3>n_4>n_5$ or, $n_1<n_2<n_3<n_4<n_5$ . As far as my knowledge is concerned, I have following two equivalent criteria for a matrix $A$ to be positive definite. all eigenvalues are positive. there is a matrix $B$ such that $A=B^tB$ . But nothing is helping me here. Any hint how to solve this!! Thank you.","['matrices', 'positive-definite', 'symmetric-matrices']"
3737954,Can an orthogonal matrix move monotonically toward a signed permutation matrix?,"The question is motivated by this question on Mathematics SE. Let $A \in O(n)$ be an orthogonal matrix that is not a signed permutation matrix, and let $P$ be the nearest signed permutation matrix to $A$ (for 'nearest', use
the distance induced by the Frobenius norm, i.e., $d(A,P)=||A-P||_F$ ). Then can $A$ move 'monotonically' to $P$ ? I.e., in every neighborhood of $A$ , does there exist $B \in O(n)$ such thatï¼š (1) $|b_{ij}| \geq |a_{ij}|$ at every non-zero entry $(i,j)$ of $P$ (and at least one inequality is strict), and (2) $|b_{ij}| \leq |a_{ij}|$ at every other entry Note that if we remove the condition that $P$ is the nearest signed permutation matrix to $A$ , then the claim is not true and a counterexample is given in the original question. Also note that the claim is true if $A$ is sufficiently close to $P$ , as we can form a path from $A$ to $P$ by using exponential maps, say a path $B_t$ where $B_0 = A$ and $B_1 = P$ . Since the entries of $B_t$ is analytic in $t$ , in a small enough neighborhood the entries of $B_t$ would be monotonic in $t$ , which shows every matrix in the path satisfies our property. I am leaning toward that the claim is correct, but I am not sure. Any thoughts? Edit: Here is a weaker problem: for every $A\in O(n)$ , $\textit{does there always exist}$ a signed permutation matrix $P$ where $A$ could move 'monotonically' to $P$ in the sense described above? A possible approach is that, let $$C_P=\{B\in M_{n\times n}\mid B \text{ has the same sign as }A\text{ at the non-zero entries of }P, B \text{ has different sign from }A\text{ at the zero entries of }P\}.$$ Because the tangent space of $A$ is $n(n-1)/2$ dimensional, we would be done if we can prove that every $n(n-1)/2$ dimensional subspace of $M_{n\times n}$ intersects $\bigcup_{P\text{ is a permutation matrix}}C_P$ untrivially. Edit 2: Maybe the original question is still too strong, so I would like to weaken (2) to be: (2) $|b_{ij}| \leq |a_{ij}|+\epsilon$ at every other entry Then for every $\epsilon>0$ , does such $B$ exists? Since we have some freedom here, maybe Gram-Schmidt would work?","['matrices', 'orthogonal-matrices', 'linear-algebra', 'topological-groups']"
3738009,True or False?: There are infinitely many continuous functions $f$ for which $\int_0^1f(x)(1-f(x))dx=\frac{1}{4}$,"I was going through CMI 2019 paper; I stumbled upon the statement: For $f:R\rightarrow R$ , there are infinitely many continuous functions $f$ for which $\int_0^1f(x)(1-f(x))dx=\frac{1}{4}$ . My approach is, we can write $f(x)(1-f(x))=\frac{1}{4}-(f(x)-\frac{1}{2})^2$ . Now, we can write $\int_0^1f(x)(1-f(x))dx=\frac{1}{4}-\int_0^1(f(x)-\frac{1}{2})^2dx$ $\Rightarrow \int_0^1(f(x)-\frac{1}{2})^2dx=0 \Rightarrow f(x)=\frac{1}{2}$ . So, there must be only one function satisfying the condition. However, the official answer says the statement is true. Am I missing something?","['integration', 'calculus']"
3738082,How many non-negative integer are there for $x_1+x_2+x_3+x_4+x_5=12$ where $x_1<x_2<x_3$,"I have a combinatorics problem: How many non-negative integer are there for $x_1+x_2+x_3+x_4+x_5=12$ where $x_1<x_2<x_3$ I tried to subtract the number of integers in which $x_1=x_2=x_3$ from the number of the whole possible solutions.However I could not do the rest.Can you enlighten me ,please? Note: $x_1,x_2,x_3$ do not have to be consecutive. They are just smaller than each other ,respectively.","['combinations', 'combinatorics', 'discrete-mathematics']"
3738112,Score Probability Question (Checking my answer),"I'm trying to check my answer to the follow: John's score is a number uniformly chosen between $0$ and $3$ . Mary's score is a number uniformly chosen between $0$ and $1$ . The two numbers are chosen independently. Find the probability that John's score is at least twice Mary's score. My attempt:
We want to find $P(J > 2M)$ where $J$ is the score John receives and $M$ is the score Mary receives. I first found the CDF for Mary and John: $$P(M \leq m) = m$$ $$P(J \leq j) = \frac{j}{3}$$ Then the probability that John's score is twice Mary's given we have Mary's score: $$P(J > 2M \mid M) = \frac{3-2M}{3}$$ Using the total law of probability $$P(J > 2M \mid M)P(M) = \frac{(3-2M)M}{3}$$ which should be the probability that John's score is at least twice as great as Mary's. Is this correct?","['solution-verification', 'discrete-mathematics', 'probability']"
3738117,"Is it true that $ S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt$, where $S(x) = A\gamma(x) + \gamma(x-C)$ and $\gamma$ is the standard Gaussian?","For $C\geq0$ and $A\geq1$ define the function $$
S(x) = A\gamma(x)+\gamma(x-C)
$$ where $\gamma$ denotes the standard Gaussian distribution in $\mathbb R$ , $\gamma(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}$ . Some numerical simulations suggest that the following inequality holds true: $$
S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt \qquad \forall x \in [0,C/2].
$$ I find it hard to prove this. Do you have any idea how to show this inequality?","['analysis', 'probability-theory', 'probability', 'real-analysis']"
3738153,schauder basis vs isomorphisms,"My doubt is about a space with schauder basis. If a space has schauder basis so can i say that it has an isometric isomorphism with some space? For an exemple: $c$ has a schauder basis, can i considere another set a dual space who it is isomorphic?
I am thinking this because i know that $(c_0)'=l_1$ and can i see something like this to $c_0$ ? ps: I know that every separate space has an isomorphism with some subspace of $l_{\infty}$ . But in this case i think that i can considerate is just trivial isomorphism. plus: I know that $l_1'=l_{\infty}$ and $l_p=l_q'$ , with $\frac{1}{p}+\frac{1}{q}=1$ . Is there another set that can i make this easily?","['schauder-basis', 'functional-analysis']"
3738181,Counterexample of Nori's Connectivity theorem without sufficient ampleness,"Let $Y \to B$ be the universal family of smooth quadric surfaces in $\mathbb {CP}^3$ , and let $T=\{(l,q) \mid l \subset Y_q \text{ where $l$ is a line}\}$ parametrize the lines contained in these $Y_q$ 's. Then we get the pull-back family $Y_T \to T$ . On Voisin's Hodge Theory and Complex Algebraic Geometry , Vol 2, page 218 it says that it is obvious that The cohomology class of the tautological divisor $$D=\{(x,l,q)\mid x \in l \subset Y_q \text{ where $l$ is a line}\}$$ in $H^2(Y_T,\mathbb Q)$ does not come from $H^2(T\times \mathbb {CP}^3, \mathbb Q)$ . But I did not see why this is clear? Thanks!","['complex-geometry', 'algebraic-geometry', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3738201,Evaluate $\sqrt{a+b+\sqrt{\left(2ab+b^2\right)}}$,Evaluate $\sqrt{a+b+\sqrt{\left(2ab+b^2\right)}}$ My attempt: Let $\sqrt{a+b+\sqrt{\left(2ab+b^2\right)}}=\sqrt{x}+\sqrt{y}$ Square both sides: $a+b+\sqrt{\left(2ab+b^2\right)}=x+2\sqrt{xy}+y$ Rearrange: $\sqrt{\left(2ab+b^2\right)}-2\sqrt{xy}=x+y-a-b$ That's where my lights go off. Any leads? Thanks in advance.,"['algebra-precalculus', 'irrational-numbers', 'radicals', 'arithmetic']"
3738222,Can you integrate without a $dx$ [duplicate],"This question already has answers here : What is $dx$ in integration? (12 answers) Closed 3 years ago . Long ago I realized that manipulation of derivatives was possible using algebraic quantities. One could take a differential instead of derivatives $$
d[\sin x]=\cos x\ dx
$$ $$
\frac{d[\sin x]}{dx}=\frac{\cos x\ dx}{dx}=\cos x
$$ My question is about the analog of this process with integrals without a $dx$ for example $$
\frac{\int\sin x}{\int x}
$$ Here $\int$ is the inverse of the $d$ operator. There is deliberately no $dx$ so the integrals cannot be evaluated in the traditional sense. This is more the foundation for an idea than anything explicit. Has this been looked at by anyone in the past? Does anyone know of anything similar to this? edited remarks To clarify, consider $\int \sin x$ . Because the input is not a differential but a finite quantity, it is expected that the integral would diverge to infinity. I desire to define how the integral diverges in a way like $$
\int \sin x=f(x)\int x
$$ which is analagous to differentials (e.g. $d[\sin x]=\cos x\ dx$ ) and therefore the fraction $\frac{\int \sin x}{\int x}$ could be evaluated as a finite quantity $f(x)$ . I'm not looking for a simple ""you can't do that"" or the definition of an integral in the conventional sense. I want to know if anyone has explored this unconventional expression or has any insight into how it might be evaluated or even what it means. Marked as a duplicate, but probably because my new idea goes against what we were all taught in calculus class.",['integration']
3738261,easiest proof of the Prime Number Theorem to study and teach?,"I know there are several variants of proofs for the Prime Number Theorem. Which one is the easiest one to study and then re-teach? By easiest, I mean those that assume minimal knowledge beyond secondary school mathematics. For example, most school leavers having done maths will have calculus, and could stretch to understand concepts like asymptotic equivalence and integration in the complex plane, but won't have concepts like group theory.","['alternative-proof', 'number-theory', 'proof-explanation', 'prime-numbers']"
3738262,How to prove $\sum\limits_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right )$?,"I am trying to prove this. I used the telescoping method, but the problem is I need the first fraction to be $\frac{1}{4(n+1)}$ and I also tried to relate it to alternating Harmonic series which didn't work. Any hint would be greatly appreciated. $$\sum_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right )$$","['sequences-and-series', 'real-analysis']"
3738281,Show $\det(F_n)=1$ for all $n$,"Consider the $n\times n$ matrix $F_n= (f_{i,j})$ of binomial coefficients $$f_{i,j}=\begin{pmatrix}i-1+j-1\\i-1\end{pmatrix}$$ Prove that $\det(F_n)=1$ for all $n$ . My current idea is to apply Leibniz formula for determinants and induction, but it seems too complicated. Any better ideas and suggestions are welcome.","['matrices', 'determinant', 'linear-algebra']"
3738377,UMVUE and complete sufficient statistic,"Let $X_1,\cdots,X_n$ be independent random variables with density $$f_{X_i}(x;\theta)=\begin{cases}e^{i\theta-x},&i\theta\leqslant x\\
0,&\text{otherwise}\end{cases}$$ where $-\infty<\theta<\infty$ , $i=1,2,\cdots,n$ . Find the complete and sufficient statistic for $\theta$ and compute the unique minimum variance unbiased estimator of $\theta$ . My approach: $$f_{X_i}(x;\theta)= e^{i\theta-x}I_{(i\theta\leqslant x)}\\
L(x;\theta)=\prod_{i=1}^{n} e^{i\theta-x}I_{(i\theta\leqslant x)}$$ which on solving tells me that $Y= \min\dfrac{X_i}{i}$ is the sufficient statistic of $\theta$ . Also, how to prove completeness? Now, my question is in order to find UMVUE, we need to have pdf of $Y$ . How to go about that?","['statistical-inference', 'statistics', 'parameter-estimation', 'exponential-distribution']"
3738429,Prove $\int_{0}^{+\infty}\frac{1}{f(x)}dx$ is convergent when $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx$ is convergent,"Suppose $f(x)$ is positive monotone increasing function over $[0,\infty)$ , and it has derivative. Prove: if $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx$ is convergent, then $\int_{0}^{+\infty}\frac{1}{f(x)}dx$ is convergent. My work: $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}=\int_{0}^{\infty}\frac{1}{f(x)+f'(x)}\Bbb{dx}$ , but since $f'(x)>0$ I do not know what to do next.","['integration', 'convergence-divergence', 'analysis']"
3738431,Proof of the change of variables formula without using the Monotone Convergence Theorem,"I recently encountered the problem Exercise 36 in Tao's An Introduction to Measure Theory . The link of an online version of this problem is here . Now I quote this problem as follows: Exercise 36 (Change of variables formula)
Let $(X, \mathcal{B}, \mu)$ be a measure space, and let $\phi: X \rightarrow Y$ be a measurable morphism (as defined in Remark 8 from $(X, \mathcal{B})$ to another measurable space $(Y, \mathcal{C}). $ Define the pushforward $\phi_{*} \mu: \mathcal{C} \rightarrow[0,+\infty]$ of $\mu$ by $\phi$ by the formula $\phi_{*} \mu(E):=\mu\left(\phi^{-1}(E)\right)$ Show that $\phi_{*} \mu$ is a measure on $\mathcal{C},$ so that $\left(Y, \mathcal{C}, \phi_{*} \mu\right)$ is a
measure space. If $f: Y \rightarrow[0,+\infty]$ is measurable, show that $\int_{Y} f d \phi_{*} \mu=\int_{X}(f \circ \phi) d \mu$ (Hint: the quickest proof here is via the monotone convergence theorem below, but it is also possible to prove the exercise without this theorem. ) I really eager about how to prove the second statement WITHOUT the Monotone Convergence Theorem, in order to follow the procedure of the book. I tried hard and figured out only the case that $f$ is a simple function. How can we prove the case that $f$ is a general unsigned (nonnegative) function? The author have not provide the solution yet. Any help is appreciated.","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3738557,How can I integrate $\int\frac{e^{2x}-1}{\sqrt{e^{3x}+e^x} } \mathop{dx}$?,"How can I evaluate this integral $$\int\dfrac{e^{2x}-1}{\sqrt{e^{3x}+e^x} } \mathop{dx}=\;\;?$$ My attempt : I tried using substitution $e^x=\tan\theta$ , $e^x\ dx=\sec^2\theta\ d\theta$ , $dx=\sec\theta \csc\theta \ d\theta.$ $$\int\dfrac{\tan^2\theta-1}{\sqrt{\tan^3\theta+\tan\theta } }\ \sec\theta \csc\theta\ d\theta $$ $$=\int\dfrac{\tan^2\theta-1}{\sec\theta\sqrt{\tan\theta } }\ \sec\theta \csc\theta d\theta. $$ I used $\tan\theta= \dfrac{1}{\cot\theta}$ $$=\int\dfrac{1-\cot^2\theta}{\cot^{3/2}\theta }\csc\theta d\theta $$ $$=\int(\cot^{-3/2}\theta-\sqrt{\cot\theta} )\csc\theta d\theta. $$ I got stuck here. I can't see whether further substitution will work or not. Will integration by parts work? Please help me solve this integral. I am learning calculus. Thank in advance.","['integration', 'indefinite-integrals', 'calculus']"
3738578,Geodesic curves on $S^2$.,"On the unit sphere $S^2$ , show that it is not true that the a geodesic curve is the shortest curve between any two points. The geodesic curves on $S^2$ are great circles. So we can choose an arc $\gamma$ on a great circle such that $\pi<\text{length}(\gamma)<2\pi$ , then $\gamma$ is not the shortest curve between two endpoints.... Am I right? Are there any rigorous proofs? Thanks in advance!","['riemannian-geometry', 'differential-geometry']"
3738591,a problem regarding projection maps on finite dimensional vector space,"Let $V$ be a finite dimensional vector space over a field $F$ of characteristic zero. Let $E_1 , E_2, ...,E_k$ be projections of $V$ such that $E_1+E_2+...+E_k=I$ . Show that $E_iE_j = 0$ for all $i\neq j$ .
Hint: Use the trace function. Using the hint I got that $\operatorname{trace}(E_i)=\dim(\operatorname{range}(E_i))$ for all $i,\;1\le i\le k$ . Again $I=E_1+E_2+\cdots+E_k \Rightarrow V=\operatorname{range}(E_1)+\operatorname{range}(E_2)+\cdots+\operatorname{range}(E_k)$ Combining both we get $V=\operatorname{range}(E_1)\oplus\operatorname{range}(E_2)\oplus\cdots\oplus\operatorname{range}(E_k)$ After this step I am not being able to progress further. Please help me. Though this problem has already been discussed in a earlier post, there was no hint regarding this approach. So I am posting this problem again. Please do not mark this problem as duplicate. Thank you in advance.","['matrices', 'linear-algebra']"
3738597,What Is Bigger $100^{100}$or $\sqrt{99^{99} \cdot 101^{101}}$,Hello every what is bigger $100^{100}$ or $\sqrt{99^{99} \cdot 101^{101}}$ ? I tried to square up and I got $100^{200}$ or $99^{99} \cdot 101^{101}$ and I don't have an idea how to continue.,"['inequality', 'number-comparison', 'jensen-inequality', 'analysis', 'algebra-precalculus']"
3738598,Prove $G$ is Abelian if $N$ is in the centre of $G$ and $G/N$ is cyclic,"I need some help on this one. $G$ is a group. If $N$ is a subgroup of $G$ contained in the centre of $G$ and $G/N$ is cyclic, show that $G$ is Abelian. My attempt is only half way and stuck at the bit at the end... If $G$ can be shown to be cyclic, then it is Abelian, since $G=<g>$ then $g_1,g_2 \in G$ are $g_1=g^n,g_2=g^m$ for some $n,m$ and $g_1g_2=g^ng^m=g^{n+m}=g^{m+n}=g_2g_1$. By definition of a centre of $G$, $\forall n \in N$, $gn=ng$ for any element in $G$. Namely, $N$ must be a normal subgroup.
  (Here, I pondered if I should just proceed or change gears to looking at homomorphisms of $G$ since $N$ is normal). Now $G/N$ is cyclic so $G/N=<gN>$ for some $g \in G$. Then, $\forall g'N \in G/N$ there is some $m$ so that $(gN)^m=g^mN=g'N$. Therefore, for the representative elements of $G/N$, I have $g^m$. Here I get stuck. So if $g'N$ then I have that each $g'$ is cyclic but as an equivalent class, I cannot guarantee that all $g'$ can be represented in the form $g^m$. The only thing I can guess is the canonical map $p:G \to G/N$ that might be relevant to solving this but again, I don't know how specifically it would help. Does anyone know how to solve this?","['group-theory', 'normal-subgroups']"
3738624,Prove $\cos(A + B) \cos(A âˆ’ B) = \cos^2A âˆ’ \sin^2B$ [duplicate],"This question already has answers here : Prove that $\cos (A + B)\cos (A - B) = {\cos ^2}A - {\sin ^2}B$ (5 answers) Closed 1 year ago . I have a question regarding trigonometric identities. The question I am currently struggling to understand is: Prove that $$\cos(A + B) \cos(A âˆ’ B) = \cos^2A âˆ’ \sin^2B$$ When approaching this problem I know that there is $$\cos2A = \cos^2A - \sin^2A$$ but how would I apply this here, or is it completely wrong way of approaching it, or should I try $$\cos(A+B)=\cos A \cos B - \sin A \sin B$$ Many Thanks.",['trigonometry']
3738665,Proof that $ \mathbb{R} $ is uncountable,"Im sure this question has been asked here a lot, but I'd like to hear if the way I understood Cantor's diagonal proof is correct. We know that $ \left(0,1\right)\sim\mathbb{R} $ . So its enough to prove that $ (0,1) $ is uncountable. Now, assume by contradiction that $ (0,1) $ is countable. It implies that exists injection $ f:\left(0,1\right)\to\mathbb{N} $ , and by Cantor-Berenstein theorem it follows that exists a bijection $ g:\mathbb{N}\to(0,1) $ . (Now we need to make and assumption that I do not fully understand, so explanations would be appreaciated. )
We assume that if $2$ real numbers has the same representaion as a decimal expansion that ends with $999999\dots$ and decimal expansion that ends with $00000\dots$ we'll take the expansion that ends with $0000\dots$ Now, from the last arguments we can count the interval $ (0,1) $ and write their decimal expansion: $ g\left(0\right)=0.x_{0,0}x_{0,1}x_{0,2}.... $ $ g\left(1\right)=0.x_{1,0}x_{1,1}x_{1,2....} $ $ \vdots $ We'll show that $ f $ is not surjective. We'll define a sequence of numbers that would be the numbers in the decimal expansion of real number $ d $ such that $ d\notin Im(f) $ . define $ y_{i}=\begin{cases}
2 & x_{i,i}=1\\
1 & x_{i,i}\neq1
\end{cases} $ and define $ d=0.y_{0}y_{1}y_{2}\dots $ . Now assume by contradiction that exists $ i\in \mathbb{N} $ such that $ f(i)=d $ . So the $ i_{th} $ digit in the decimal expansions of $ d $ and $ g(i) $ should be equal, but that's a contradiction. Thus, $ g $ is not surjective. I think this proof works, but Im not sure why would we need the assumption that we are taking the decimal expansion that ends with 00000 rather than the one that ends with 999999. Thanks in advance.","['elementary-set-theory', 'cardinals', 'decimal-expansion']"
3738701,Introduction to $L^2$ space. Equivalence class concept,"Given a probability space $\left(\Omega\text{, }\mathcal{F}\text{, }\mathbb{P}\right)$ , let $L^2$ denote all (equivalence classes for a.s. equality of) random variables $X$ such that $\mathbb{E}\{X^2\}<\infty$ . We henceforth identify all random variables $X$ , $Y$ in $L^2$ that are equal a.s. and consider them to be representatives of the same random variable. This has the consequence that if $E\{X^2\}=0$ , we can conclude that $X=0$ (and not only that $X=0$ a.s.). I am not sure I am correctly interpreting the above-quoted statements. What I understand is that, since $E\{X^2\}=0$ , given that $\text{Var}(X)=E\{X^2\}-E\{X\}^2\geq0$ , it must hold that $E\{X\}=0$ . So, at this point one can say that, since $E\{X\}=0$ and $\text{Var}(X)=0$ , $X$ follows a degenerate distribution, that is that $X=0$ a.s.. Now, pretending that what I have said so far is ok, does the statement ""we can conclude that $X=0$ (and not only that $X=0$ a.s.) "" follow from the fact that (see part in bold) "" $L^2$ denotes all ( equivalence classes for a.s. equality of ) random variables $X$ such that $\mathbb{E}\{X^2\}<\infty$ ""? If my reasoning was totally wrong, could you please clarify the meaning of the paragraph above?","['equivalence-relations', 'lp-spaces', 'probability-theory', 'almost-everywhere']"
3738724,Is this statement about change of variables in limits true?,"I was brainstorming on this and on some related thoughts in the past 2-3 days so any help here would be much appreciated. What put me in these thoughts... was one proof (from a textbook) which I read and which was using the implication b) (see below) very casually and as if it is quite natural to assume that this is true. So I realized this needs more rigorous justification. Is the following statement/theorem true? Given are the two functions: $$g(x): (a,b) \rightarrow (c,d) \tag{1}$$ $$f(x): (c,d) \rightarrow \mathbb{R} \tag{2}$$ The function $g$ is continuous, monotonic and takes all values between $c$ and $d$ (it is surjective but possibly not bijective) Let also: $$\lim\limits_{x \to a} g(x) = c$$ Then: a) if the limit $\lim\limits_{y \to c} f(y)$ exists and is $L$ , then $\lim\limits_{x \to a} f(g(x))$ also exists and is equal to $L$ b) if the limit $\lim\limits_{x \to a} f(g(x))$ exists and is $L$ , then $\lim\limits_{y \to c} f(y)$ also exists and is equal to $L$ Note 1 : Here the symbols $a,c$ may denote numbers or $-\infty$ , and the symbols $b,d$ may denote numbers or $+\infty$ Note 2 : By limit exists and equals $L$ , it is meant that the limit is either a number, or also an infinity (positive or negative) I think this theorem is true and it is what justifies when people do simple variable substitutions in limits (almost mechanically) and write casually that: $$\lim\limits_{t \to 0+} \phi(1/t) = \lim\limits_{x \to \infty} \phi(x)$$ or say $$\lim\limits_{t \to \infty} \phi((t-2)^{2}) = \lim\limits_{x \to \infty} \phi(x)$$ or e.g. $$\lim\limits_{t \to 0} \phi(1/t^2) = \lim\limits_{x \to \infty} \phi(x)$$ I think I was able to prove both a) and b). The part a) is proved easily, it even follows from Limit of composite functions theorem But the proof of part b) substantially uses the assumption that $g$ takes all values between $c$ and $d$ (otherwise b) would not hold true, right?). But my point is that when we do almost mechanically such changes of variables in limits (from $t$ to $x$ or from $x$ to $y$ , etc.)... we don't really think about the Limit of composite functions theorem and check its conditions, right? Instead we just think of $g$ as bijection and we assume by intuition that changing the variable works OK because of the bijective behavior of $g$ . Would anyone agree with that? And... could someone confirm this theorem is true? Also, do we need any restrictions on f in this theorem? I think not. Also, can we remove the condition that g is continuous? Or relax the conditions of the theorem in some other way?","['limits', 'change-of-variable', 'analysis', 'real-analysis']"
3738730,Prove that $\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0 $,"Suppose that A, B and C are 2x2 matrices that switch between each
other. Prove that $$\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0. $$ I did $$A^3+B^3+C^3-3ABC=\frac12(A+B+C)((A-B)^2+(A-C)^2+(B-C)^2)$$ So, this determinant is equivalent to $$\frac14[\det(A+B+C)]^2\det((A-B)^2+(A-C)^2+(B-C)^2)$$ But how can I prove that $$\det((A-B)^2+(A-C)^2+(B-C)^2)\geq0?$$ Can someone help me? Thanks for attention.","['matrices', 'determinant', 'factoring']"
3738737,"From the given definition of function $f(n)$, find $f^{-1}(-100)$","$f: \{1,2,3..\}\rightarrow \{\pm 1,\pm 2, \pm 3..\}$ is defined by $f(n)=\begin {matrix} \frac n2~~\text{if n is even} \\ -\frac{n-1}{2}~~\text{if n is odd}\end{matrix}$ $-100$ is even, so situation one would apply in which case $$y=\frac n2$$ $$n= 2y$$ $$f^{-1}(n)=2n$$ $$f^{-1}(-100)=-200$$ But the given answer is $201$ which would be the case if we solved using situation 2. Why should we use situation 2 is the number given is even?","['algebra-precalculus', 'functions']"
3738774,Integral of $\int_{-\infty}^{\infty} e^{\alpha x}/({e^x+1})$,"Show that, for $0<\alpha<1$ : $$\int_{-\infty}^{\infty} \frac{e^{\alpha x}}{{e^x+1}}\text{d}x=\frac{\pi}{\sin(\pi\alpha)}.$$ Hint: Use the recangular path $S_r=\left[-r, r, r+2\pi, -r+2\pi,-r \right]$ (see fig. 1). My attempt: Denoting $f(z)=\frac{e^{\alpha x}}{{e^x+1}}$ , we notice that $f$ has only simple poles at $z=\pi i+ 2\pi i\mathbb{Z}$ . Using the suggested path,  we have $$
\oint_{S_r} f(z) \ \text{d}z=\int_{-r}^{r} \frac{e^{\alpha x}}{{e^x+1}}\text{d}x+\int_{[r+2\pi i,-r+2\pi i]}f(z) \ \text{d} z+\int_{[r,r+2\pi i]}f(z) \ \text{d} z+\int_{[-r+2\pi i, -r]}f(z) \ \text{d}z.
$$ The integrals on the right and left paths, denoted $\gamma_1$ and $\gamma_2$ in fig. 1, become insignificant as $r\to\infty$ : $$
\left|\int_{\gamma_1} f(z) \ \text{d}z \right|\le \max_{z\in[r,r+2\pi i]} \left| \frac{e^{\alpha r}e^{i 2\pi\alpha t}}{e^r e^{i2\pi t}+1} \right|\ell(\gamma_1)\le \frac{e^{\alpha r}}{e^r+1}\cdot 2\pi\xrightarrow[r\to\infty]{}0.
$$ A similar argument applies for $\gamma_2$ . But what do I do with the path $\Gamma_1$ ?","['integration', 'improper-integrals', 'complex-analysis', 'residue-calculus', 'complex-integration']"
3738817,Question about probability / mutually exclusive events,"Decide whether this statement is true or false:
Let $(\Omega, \mathbb{F}, \mathbb{P})$ be a probability space, if for two events $A,B \in \mathbb{F}$ $\hspace{1cm}$ $\mathbb{P}(A \cup B)= \mathbb{P}(A) + \mathbb{P}(B)$ holds, then $ A \cap B= \emptyset$ In the solutions it is stated that this statement is false, however I do not really understand why?
Isn't the definition that for two mutually exclusive events $\mathbb{P}(A \cup B)= \mathbb{P}(A) + \mathbb{P}(B)$ and $\mathbb{P} (A \cap B) = \emptyset$ ?","['inclusion-exclusion', 'probability-theory', 'probability']"
3738818,Multidimensional Young diagrams,"Consider a Young diagram defined as follows: A Young diagram (also called a Ferrers diagram, particularly when
represented using dots) is a finite collection of boxes, or cells,
arranged in left-justified rows, with the row lengths in
non-increasing order. Listing the number of boxes in each row gives a
partition $\lambda$ of a non-negative integer $n$ , the total number of boxes of
the diagram. For example we may write 1+4+5=10: Question: Are there higher-dimensional versions, using cubes, such that the ""faces"" of the diagram are each themselves Young diagrams? Here is an example, with three distinct faces, each representing diagrams: 1+2+3+3, 2+2+3+3, and 0+0+4+4. The faces are the Young diagrams on the faces of the cube in this case. It has 6 faces, and three pairs of (up,right,in), each a Young diagram. In 2d, there is only 1 face (1 diagram). In 3d, one has a cube with six faces, but only three are unique diagrams. One diagram in the 3d case is forced from the other two (up,right,up,right....up and up,up,in,in,up lead to the other necessarily being right,right,in,in,right). If so, is there a way of writing an integer in terms of the diagram, in the same way as an integer can be represented via one of many Young diagrams (i.e. integer partitions)? This would represent a restricted integer partition, but in a relatively unusual way. For example the image below would represent the integer partition ((2+2) + (3+3)) + ((2+2) + (3+3)) = 20.",['combinatorics']
3738829,An exercise from Ahlfors' book Complex Analysis,"this is Exercise 4, Section 3.2 (page 26) from Ahlfors book (First Edition): Show that any four distinct points can be carried by a linear transformation (i.e. Mobius transformation) to positions $1,-1,k,-k$ , where the value of $k$ depends on the points? How many solutions are there, and how they related? My attempt: If the points are $z_1,z_2,z_3,z_4$ , then it suffices to find a linear transformation $T$ such that $$
\begin{cases}
T(z_1)+T(z_2)=0\\
T(z_3)+T(z_4)=0
\end{cases}
$$ Once we find such transformation, we can rotate and change proportion of its image to obtain the desired result. How to prove the existence of such transformation? and what is the answer of the rest of the question? Thanks.",['complex-analysis']
3738858,Determining whether an angle is between two given angles on the unit circle,"I am trying to find a way to determine whether an angle is between two given angles where all angles are provided as vectors on the unit circle i.e.: $\mathbf{a}=(\cos(\theta),\sin(\theta))$ Note that by inbetween I mean on the arc of the smaller of the two segments of the unit circle formed by the vectors we want to check between. Specifically I do not want to obtain the angles from the given vectors by applying the inverse trig functions I just want to work with the given vectors. I think the following is true if and only if the angle $\mathbf{c}$ is between $\mathbf{a}$ and $\mathbf{b}$ : $$|\mathbf{a} + \mathbf{b} - \mathbf{c}|\leq 1$$ but I'm having trouble proving it. Is this statement true and can you prove it?",['trigonometry']
3738864,What Is Bigger $\frac{3}{e}$ or $\ln(3)$ [duplicate],"This question already has answers here : How to arrange $e^3,3^e,e^{\pi},\pi^e,3^{\pi},\pi^3$ in the increasing order? (3 answers) Closed 4 years ago . Hello everyone what is bigger $\frac{3}{e}$ or $\ln(3)$ ? I tried to square it at $e$ up and I got: $e^{\frac{3}{e}} = \left(e^{e^{-1}}\right)^{3\:}$ and $3$ but I don't know how to continue I also tried to convert it to a function but I didn't find.","['functions', 'exponential-function']"
3738930,Biot-Savart law on a torus?,"Background: In classical electrodynamics, given the shape of a wire carrying electric current, it is possible to obtain the magnetic field $\mathbf{B}$ via the Biot-savart law . If the wire is a curve $\gamma$ parametrized as $\mathbf{y}(s)$ , where $s$ is the arc-length, then $$
\mathbf{B}(\mathbf{x}) = \beta \int_\gamma \dfrac{ d\mathbf{y}\times(\mathbf{x} - \mathbf{y}) }{|\mathbf{x} - \mathbf{y}|^3} 
= \beta  \int_\gamma ds \dfrac{  \mathbf{y}'(s) \times(\mathbf{x} - \mathbf{y}(s)) }{|\mathbf{x} - \mathbf{y}(s)|^3} \, ,
 $$ where $\beta $ is just a physical constant proportional to the current in the wire. Another application of the Biot-Savart law is to find the velocity field $\mathbf{v}$ around a bent vortex line in a fluid, in the approximation of incompressible and irrotational fluid flow (i.e. $\nabla \cdot \mathbf{v} =0$ and $\nabla \times \mathbf{v} =0$ almost everywhere) and very-thin diameter of the vortex core.
In fact, by demanding that the vorticity of the fluid is concentrated on the vortex core
(i.e. it is distributed as a Dirac delta peaked on the vortex core), $$
\mathbf{w}(\mathbf{x}) = \nabla \times \mathbf{v}(\mathbf{x})=
 c \int_\gamma ds \, \mathbf{y}'(s)\, \delta(  \mathbf{x} - \mathbf{y}(s))   \, ,
$$ we have that the Helmholtz decomposition and the fact $\delta(\mathbf{y}-\mathbf{x} ) = -\nabla^2 \, (4 \pi |\mathbf{y}-\mathbf{x}|)^{-1}$ tell us that $$
 \mathbf{v}(\mathbf{x})=
 \frac{c}{4 \pi}  \int_\gamma ds \dfrac{  \mathbf{y}'(s) \times(\mathbf{x} - \mathbf{y}(s)) }{|\mathbf{x} - \mathbf{y}(s)|^3}$$ Again, the constant $c$ is just a physical constant that sets the value of the circulation of the field $\mathbf{v}$ around the vortex. Question: The above construction works in $\mathbb{R}^3$ . Imagine now that the wire (or, equivalently, the irrotational vortex) is a curve in the three-dimensional torus $\mathbb{T}^3 = S^1 \times S^1 \times S^1$ . How to obtain the equivalent of the Biot-Savart law? Note on topology: We are changing the base manifold from $\mathbb{R}^3$ to $\mathbb{T}^3 $ but the local differential relations should be unchanged (i.e. the definition of the vorticity 2-form as the external derivative of the velocity 1-form, or the local form of Maxwell equations $dF = J$ ). The problem is that the Biot-Savart law is non-local, so it is a global problem that ""feels"" the topology of the manifold. Maybe in the end the question is related to how the Helmholtz decomposition works on a torus. Important (possible answer): See the following closely related questions in Physics SE: Rotating away a constant gauge field , Periodic boundary conditions for vortex in a square lattice . There is a topological problem in considering Biot-Savart in a torus!","['differential-topology', 'electromagnetism', 'differential-forms', 'differential-geometry']"
3738956,Reference for proof of Green's theorem,"I'm looking for a rigorous proof of Greens theorem for piecewise smooth jordan curves and would appreciate if someone could link a reference text. The only proof I've seen works for regions which can be bounded by curves $\{(x,y)\in \mathbb{R}^2: a\leq x \leq b,\, \phi(x)\leq y \leq \psi(x)\}$ . The article on wikipedia seems to be lacking several details.","['greens-theorem', 'multivariable-calculus', 'reference-request']"
3738968,In a metric space a sequence with no converging subsequences is discrete (?),"I've been trying to prove that given a metric space $X$ ( not necessarily complete ) and a sequence $(x_n)_n \subseteq X$ which contains no convergent subsequences, there exists an open neighborhood $V_n$ of $x_n$ for each $n \in \mathbb{N}$ such that these $V_n$ 's are pairwise disjoint i.e.: $$
\exists V_1 \in \mathcal{v}_{x_1}, \ldots, \exists V_n \in \mathcal{v}_{x_n}, \ldots:
\forall n_1 \neq n_2: V_{n_1} \cap V_{n_2} = \emptyset.
$$ So far I am stuck trying to prove it by contradiction. All I've managed to see is that it wouldn't be enough to consider balls of the same radius around each element of the sequence, as this wouldn't prove anything for the sequence $(1+1/1, 1+1/2, \ldots, n+\frac{1}{n}, n+\frac{1}{n+1}, \ldots)$ in $\mathbb{R}$ , which doesn't have any convergent subsequences. So I'm trying to work with the general hypothesis by contradiction: $$
\forall r_1, \ldots, r_n, \ldots: \exists n_1 \neq n_2:
B(x_{n_1}, r_{n_1}) \cap B(x_{n_2}, r_{n_2}) \neq \emptyset,
$$ but I am stuck.
Obviously, I could just pick for each $x_n$ a radius small enough $r$ so that $B(x_n, r)$ doesn't intersect some balls $B(x_1, r_1), \ldots B(x_{n-1}, r_{n-1}), B(x_{n+1}, r_{n+1}), \ldots$ , because if that weren't posible, it would mean the sequence would have a converging subsequence to $x_n$ . But the problem is, I can't guarantee that the balls obtained this way also don't intersect each other, not just $B(x_n, r)$ .
I feel like there is something important I should be observing here, but I can't see it. Also, in case you happen to know that this result isn't true or even better, you happen to have a counterexample, it would be much appreciated. Thank you for reading.","['limits', 'general-topology', 'metric-spaces', 'sequences-and-series']"
3738985,Existence of curve of constant curvature connecting two points,"Let $S$ be a two-dimensional Riemannian manifold, i.e., a surface. If $S$ is complete as a metric space, then it follows (by the Hopfâ€“Rinow theorem) that any two points of $S$ can be joined by a (minimizing) geodesic. My question is: Assume that $S$ is complete. Can any two points of $S$ be joined by a curve of constant, nonzero (geodesic) curvature? The reason I am aking is that I read from Eisenhart's classic book ( A Treatise on the Differential Geometry of Curves and Surfaces ) the following statement, which seems to assume existence: ""Of all the curves of equal length joining two points, the one which,
together with a fixed curve through the points, incloses the area of
greatest extent, has constant geodesic curvature.""","['curves', 'riemannian-geometry', 'ordinary-differential-equations', 'differential-geometry']"
3738995,"Throw a coin $10$ times without knowing the mass distribution and get $10$ heads and $0$ tail, what is the probability of head in the $11$th time?","Throw coin A $10$ times without knowing the mass distribution and get $10$ heads and $0$ tails, what is the probability of facing up for the $11$ th time? Throw coin B $100$ times without knowing the mass distribution and get $99$ heads and $1$ tails, what is the probability of facing up for the $101$ th time? Which coin is more likely to face up in the next toss? I already know that parameter estimation methods such as maximum likelihood estimation can be used to estimate the most likely mass distribution of this coin; I already know that Laplace smoothing can help me better compare the difference between Coin A and Coin B; But how should we calculate their probability of heading up in the next toss? I just want a percentage. Thanks for your help.","['statistics', 'probability-distributions', 'probability']"
3739014,"Can every symmetric, unimodular and positive definite $G\in\mathbb{Z}^{n\times n}$ be written as $G=U^TU$?","Let $G\in\mathbb{Z}^{n\times n}$ be symmetric, unimodular and positive definite. Does there exist a unimodular matrix $U\in\mathbb{Z}^{n\times n}$ such that $G=U^TU$ ? I now that the result is true if $n=2$ , but I have a feeling that it fails if $n$ gets larger.","['matrices', 'unimodular-matrices', 'positive-definite']"
3739019,$f(x) = \cos|x| - 2ax + b$ increases for all $x$. Find the maximum value of $2a + 1$,"Here's how I approached the problem. $f'(x) = -\sin x - 2a$ $f'(x) \geq 0$ $\Rightarrow -\sin x -2a \geq 0$ $\Rightarrow 2a \leq -\sin x$ $\Rightarrow 2a+1 \leq 1- \sin x  \tag{*}$ Since range of $\sin x$ is $[-1,1]$ , $\Rightarrow 2a+1\leq 1-(-1)$ $\Rightarrow 2a+1\leq 2$ Hence maximum value of $2a+1$ is coming to be $2$ . However the answer to the question is $0$ . What am I doing wrong?","['maxima-minima', 'functions']"
3739116,Find the number of solutions for the equation $4\{x\}=x+[x]$,"$$4\{x\}=[x]+\{x\}+[x]$$ $$3\{x\}=2[x]$$ $$\{x\}=\frac 23 [x]$$ $$0\le \frac 23 [x] <1$$ $$0\le [x]<1.5$$ So $[x]=0,1$ The solutions for $x$ should be infinite, but the given answer is 3. Even if assume that the answer not being $\infty$ is upto interpretation, I still get only 2 as the answer. Please verify this solution. The brackets represent fractional part and greatest integer part","['functions', 'solution-verification']"
3739196,Largest set $B$ such that $|A\cap (B-B)|=p$,"In a preprint I was reading the following was claimed without proof: Let $A$ be a subset of $[n]:=\{1,2,\dots n\}$ where $|A|<\frac{n}{k}$ for some integer $k$ . Then there exists a set $B\subset [n]$ such that $|B|=k$ and there exists no $b_1,b_2\in B$ such that $b_1-b_2 \in A$ . (subtraction is not done mod $n$ ) I am struggling to see how this is true. How would one prove this? If it is false, I would be interested in a proof of a weaker claim, such as ""if $|A|<n/k^2$ then there exists a set $B$ with $k$ elements such that there exists no $b_1,b_2\in B$ such that $b_1-b_2 \in A$ ."" (a probabilistic argument almost works for this weaker claim, but I couldn't figure out how to make the difference set of $B$ uniform enough)","['combinatorial-designs', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics', 'difference-sets']"
3739218,Does parity matter for $\lim_{n\to \infty}\left(\ln 2 -\left(-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots -\frac{(-1)^n}{n}\right)\right)^n =\sqrt{e}$?,"Prove that $$\lim_{n\to \infty}\left(\ln 2 -\left(-\frac12+\frac13-\frac14+\cdots  -\frac{(-1)^n}n\right)\right)^n =\sqrt{e}$$ I happened to encounter this problem proposed  by Mohammed Bouras,Morocco in the facebook group of Romanian mathematical Magazine As per the  title,  I think the limit of the problem depends upon the parity of $n$ . That is,if $n$ is even, the limit is $\frac1{\sqrt e}$ otherwise as stated. My query is, Does the parity indeed matters for this problem ? And if it matters what should be  the conclusion for limit of the problem ? Here is my try we will show that the there exist two different limits  for above problem. For $0< x\leq 1$ , we define the functions $$f(x)=\ln(1+x),\; \displaystyle g(x)=\sum_{k=1}^n \frac{(-x)^k}{k+1}$$ and we note that $$\begin{aligned}f(x)-g(x) &= x-\sum_{k=2}^\infty(-1)^{k+n} \frac{x^{k+n}}{k+n}\\&=x+\sum_{k=2}^{\infty} (-1)^{k+n} \int_0^x t^{k+n-1}dt\\&=x+(-1)^n\int_0^x t^n\left(\sum_{k=1 }^\infty(-1)^k t^{k-1} \right)dx\\&=x-(-1)^n\int_0^x\frac{t^n}{1+t} dt\end{aligned}$$ hence for $x=1$ we have then $$f(1)-g(1)=\ln(2)-\sum_{k=1}^\infty\frac{(-1)^k}{k+1}=1-(-1)^n\int_0^1\frac{t^n}{1+t}dt$$ Note that latter integral is know result however,here we shall derive it and  we shall show that $$\displaystyle\lim_{n\to\infty}(f(1)-g(1))^n =\begin{cases}\sqrt{e}\; \text{if }  \, n\in 2n-1 \\  \frac1{\sqrt{e}} \; \text{otherwise}\end{cases}$$ We solve the following integral for any $n>0$ . By  polynomial long division it  is trivial to note that $$\int_0^1\frac{t^n}{t+1}dt=(-1)^n\int_0^1\left(\frac{1}{t+1}-\sum_{0\leq j\leq n}(-1)^j t^{j-1}\right)dt$$ and hence on integrating $\displaystyle \int_0^1\frac{t^n}{1+t}dt$ $$\begin{aligned}&=(-1)^n\left(\log(2) -\sum_{1\leq j\leq n} \frac{(-1)^{j+1}}{j}\right)\\&=2^{-1}\left(-\psi\left(\frac{n+1}2\right)+\psi\left(\frac{2n+1}2\right)\right)\\&=\frac12\left(H_{\frac{n}2}-H_{\frac{n-1}2}\right)\end{aligned}$$ Further we note that $H_n\approx \gamma +\ln n +\frac1{2n}-O(n^{-2})$ with which we deduce that $$H_{\frac{n}2} -H_{\frac{n-1}2} \approx  \frac1n-\ln\left(\frac{n-1}n\right)+\frac1{n-1}$$ for all $n>1$ and hence $H_{\frac{n}{2}} -H_{\frac{n-1}2} \to \frac1n$ as $n$ gets larger. Thus we have for $$\lim_{n\to\infty}(f(1)-g(1))^n= \lim_{n\to\infty} \left(1-\frac{(-1)^n}{2n}\right)^n=e^{-\frac{(-1)^n}2} =\sqrt{e^{-(-1)^n}}$$ therefore  if $n$ is even we have limit as $\displaystyle \frac1{\sqrt{e}}$ and if $n$ is odd we  have limit $ \displaystyle \sqrt{e}$ . Since we have two different limits. Does it have limit ? Thank you","['integration', 'summation', 'real-analysis', 'sequences-and-series', 'limits']"
3739251,Find $n$ such that $1-a c^{n-1} \ge \exp(-\frac{1}{n})$,"I am trying to find the integer $n$ such that \begin{align}
1-a c^{n-1} \ge \exp(-\frac{1}{n})
\end{align} where $a>0$ and $c \in (0,1)$ . I know that finding it exactly is difficult. However, can one find good upper and lower bounds it. It tried using lower bound $\exp(-x) \le 1-x+\frac{1}{2}x^2$ . However, it didn't really work.","['algebra-precalculus', 'upper-lower-bounds', 'real-analysis']"
3739259,Why should $\sum_{m=1}^N e(\alpha m^3)$ be big for some $\alpha?$,"I'm going through a ""circle method"" proof of the fact that every large enough natural number $n$ is the sum of nine cubes. At some point a lot of control over the function $$f(\alpha)=\sum_{m=1}^N e(\alpha m^3)$$ is needed. Here $N=\lfloor n^{1/3}\rfloor$ and $e(z)=e^{2\pi i z}$ . If we first study the function $$g(\alpha)=\sum_{m=1}^N e(\alpha m)=\frac{e(\alpha(N+1))-e(\alpha)}{e(\alpha)-1}$$ we find that it takes bigger values when $\alpha$ is close to an integer. The author then says that ""a similar analysis shows that $f$ is bigger whenever $\alpha$ is close to a rational number with small denominator"". He/She uses the following major arcs $$\mathcal{U}=\left[\frac{1}{n^{1-\frac{1}{300}}},1+\frac{1}{n^{1-\frac{1}{300}}}\right]$$ $$\mathfrak{M}(a,q)=\left\lbrace \alpha\in\mathcal{U} \mid \left|\alpha-\frac{a}{q}\right|\le\frac{1}{n^{1-\frac{1}{300}}}\right\rbrace\quad a,q\in\mathbb{N}\quad (a,q)=1$$ $$\mathfrak{M}=\bigcup_{1\le q\le n^{1/300}}\bigcup_{\substack{1\le a\le q-1 \\ (a,q)=1}}\mathfrak{M}(a,q)$$ and then says it can be proved that $$\int_\mathfrak{M}f(\alpha)^9e(-\alpha n)\,d\alpha$$ is big (something like $cn^2$ ). I'm completely lost here. How does one prove that? Is there an easy reference on Weyl sums and lower bounds? The author works out the bounds for the integral over the minor arcs but not over the major arcs. Can someone either work it out here or point to a reference where it is worked out in detail? Thanks!! Edit: The only (almost trivial) progress I've been able to make is $$\vert\mathfrak{M}\vert=\sum_{1\le q\le n^{1/300}}\sum_{\substack{1\le a\le q-1 \\ (a,q)=1}}\vert\mathfrak{M}(a,q)\vert\le 2\frac{\varphi(q)}{n^{1-\frac{2}{300}}}\le\frac{\varphi(n^{\frac{1}{300}})}{n^{1-\frac{2}{300}}}<2\frac{n^{\frac{1}{300}}}{n^{1-\frac{2}{300}}}\to 0$$ when $n\to\infty$ . Also $$f\left(\frac{a}{q}\right)=\sum_{m=1}^N e\left(\frac{a}{q}m^3\right)=\sum_{r=1}^q\sum_{\substack{1\le k\le N \\ k\,\equiv_q\,r}}e\left(\frac{a}{q}k^3\right)=\frac{N}{q}\sum_{r=1}^q e\left(\frac{a}{q}r^3\right)$$ The last equality may not be true but $N/q$ is an increasingly good approximation (asymptotically true) of how many integers $1\le k\le N$ are in each residue class modulo $q$ . Maybe this last sum, from $r=1$ to $q$ is easier to estimate. It shouldn't be difficult to get an asymptotic expression for $\vert\mathfrak{M}\vert$ and argue that $f(\alpha)\approx f(a/q)$ in $\mathfrak{M}(a,q)$ so that $$\int_{\mathfrak{M}(a,q)}f(\alpha)^9e(-\alpha n)\, d\alpha\approx\int_{\mathfrak{M}(a,q)}f\left(\frac{a}{q}\right)^9e(-\alpha n)\, d\alpha=\vert\mathfrak{M}(a,q)\vert f\left(\frac{a}{q}\right)^9e(-\alpha n)$$ Better than an answer with a solution: I found the entire set of papers containing the whole proof (two weeks ago but couldn't make this edit earlier) and they come from a number theory course from the University of Leiden","['analytic-number-theory', 'number-theory', 'trigonometric-series', 'exponential-sum']"
