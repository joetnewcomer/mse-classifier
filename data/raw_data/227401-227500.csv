question_id,title,body,tags
4700891,Help understanding the derivation of weingarten formula.,"Let $f: M^n \to \tilde{M}^m$ be an isometric immersion of Riemannian manifolds (that is $\langle X,Y \rangle_M =\langle f_*(x) X, f_*(x)Y \rangle_{\tilde{M}}$ with $\langle , \rangle_M$ and $\langle , \rangle_{\tilde{M}}$ be the Riemannian structures of $M$ and $\tilde{M}$ respectively and with $\tilde{\nabla}$ the Levi-Civita connection for $\tilde{M}^m$ For $X,Y \in \mathcal{X} (M)$ we have the decomposition $$\tilde{\nabla}_X f_* Y =(\tilde{\nabla}_X f_* Y )^T +(\tilde{\nabla}_X f_*Y )^{\perp}$$ with respect to the decomposition $$f^* T \tilde{M} =f_*TM \oplus N_fM,$$ where $N_fM$ is the normal bundle of $f$ . Let $a:\mathcal{X} (M) \times \mathcal{X} (M) \to \Gamma (N_fM)$ defined by $$a^f (X,Y) =(\tilde{\nabla }_X f_* Y)^{\perp}$$ be the second fundamental form of $f$ . Let $A_{\xi}$ της $f$ στο $x \in M^n$ the shape operator at $\xi \in N_fM(x)$ defined as $$\langle A_{\xi} X,Y \rangle =\langle a(X,Y),\xi \rangle$$ for all $X,Y \in T_xM$ . We will derive the weingarten formula. For vector fields $X,Y \in \mathcal{X}(M)$ and $\xi \in \Gamma (N_fM)$ we have $$\langle \tilde{\nabla}_X \xi ,f_* Y\rangle =-\langle \xi ,\tilde{\nabla}_X f_*Y \rangle $$ $$ =-\langle \xi , a(X,Y) \rangle$$ $$ = -\langle A_{\xi} X,Y \rangle$$ Therefore the tangential component of $\tilde{\nabla_X} \xi$ is $-f_* A_{\xi} X$ . I don't understand the proof, specifically why is it true that $ -\langle \xi ,\tilde{\nabla}_X f_*Y \rangle=-\langle \xi , a(X,Y) \rangle$ , since $a$ is defined to be equal to $(\tilde{\nabla }_X f_* Y)^{\perp}$ , not $\tilde{\nabla }_X f_* Y$ , there must be some property of inner products that I am missing
and why does $\langle \tilde{\nabla}_X \xi ,f_* Y\rangle = -\langle A_{\xi} X, Y \rangle$ imply that the tangential component of $\tilde{\nabla_X} \xi$ is $-f_* A_{\xi} X$ , I think this should imply that $\tilde{\nabla}_X \xi = -f_* A_{\xi } X$ by the properties of inner products, but here the author of the proof concludes that $$\tilde{\nabla}_X \xi = -f_* A_{\xi } X +\text{normal component}$$ why is that? Can you explain?","['inner-products', 'proof-explanation', 'riemannian-geometry', 'differential-geometry']"
4700954,Existence of supremum implies existence of infimum,"I'm reading A Book of Set Theory by Charles C. Pinter. I don't know how to solve exercise 4.3.11. Let $A$ be a partially ordered class. Prove the following: a) If every subclass of $A$ has a $\sup$ and an $\inf$ in $A$ , then $A$ has a least element and a greatest element. [Hint: Use 4.27.] b) The following two statements are equivalent: Every subclass of $A$ has a $\sup$ ; every subclass of $A$ has an $\inf$ . Part a) seems straightforward: $A$ is a subclass of itself so $\sup A$ and $\inf A$ exist and must be its greatest and lowest elements respectively. However I'm confused with b). Isn't it obviously false? Consider the partial order defined on the class $A=\{1,2,3\}$ represented in this diagram or equivalently given by the graph $G=\{(1,1),(1,2),(1,3),(2,2),(3,3)\}$ . Then any subclass of $A$ has an infimum but $A$ itself does not have a supremum, right? What am I missing?","['elementary-set-theory', 'order-theory', 'supremum-and-infimum']"
4700955,Computing $ \int \frac{{\rm d} x}{x^3 + 12} $,"$$ \int \frac{{\rm d} x}{x^3 + 12} $$ This is a question I came up with and have not been able to solve. I graphed this function and it is as in the picture attached. Some of the people who I have talked to have labeled it as an ""impossible integral."" Is it true or IS there actually a way to do this? I have recently started learning integration, but am really intrigued and curious. Your help and explanation will mean a lot. Thank you so much!","['integration', 'indefinite-integrals', 'calculus', 'rational-functions']"
4700973,"Double integral $\iint_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y$ over the domain $D$: $x\le x^2+y^2 \le 2x,\ \ y\ge 0$","$$\iint\limits_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y$$ Where the domain is $D$ : $x\le x^2+y^2 \le 2x,\ \ y\ge 0$ So far I have tried getting the bounds by working on the domain and taking the right part i could determine that $y^2+(x-1)^2 = 1$ which would be a circle, but other than that i have no idea how to work the first part of the inequality","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4700976,Inequality regarding a function and its Fourier transform,"Does a continuous compactly supported function $f : \mathbb R \to \mathbb R$ satisfy the inequality $$\lvert f(x)\rvert+\lvert \widehat{f}(x)\rvert \leq C(1+\lvert x\rvert)^{-1-\epsilon},\quad x \in \mathbb{R}$$ for some $C,\epsilon >0$ ? Here, $\hat f$ is the Fourier transform of $f$ . Presumably everything is defined since $f \in L^1(\mathbb{R})$ . Edit 1 : For any $\epsilon >0\,$ I can bound $\lvert f(x)\rvert \leq \max \left(1,\frac{\Vert f\Vert_\infty}{\min_K \left(1+\lvert x \rvert\right)^{-1-\epsilon}}\right)$ , then I want to use the following (but my function is just $\mathcal{C}^0$ ...) Theorem (Paley-Wiener) If $f \in \mathcal{C}_o^\infty$ and $f(z)=0$ for $ \lvert z \rvert > R$ , then $$\lvert \widehat{f}(z) \rvert \leq C_n(1+ \lvert z\rvert)^{-n}e^{2\pi\lvert \text{Im}(z)\rvert R} \quad \forall n \in \mathbb{Z}^+.$$ I am referring to what they say in page 4 of the paper https://www.sciencedirect.com/science/article/pii/S0022247X19309813","['inequality', 'lp-spaces', 'fourier-analysis', 'analysis']"
4701015,"If $f(x)$ is a monic polynomial in ${\mathbb Z}[x]$ and all roots have absolute value 1 , then all roots are roots of unity.","I am trying to understand the statement that was mentioned here . If $f(x)$ is a monic polynomial in ${\mathbb Z}[x]$ and all roots have absolute value 1, then all roots are roots of unity. I was wondering why we need to assume that $f(x)$ is monic. Are there any non-monic polynomials where the conclusion does not hold?","['number-theory', 'roots', 'polynomials', 'roots-of-unity']"
4701017,Is every weighted shift on $\ell^2$ unitarily equivalent to a weighted shift with positive weights?,"Consider $\mathbf t = \{t_n\}_{n=1}^\infty \subset \Bbb C$ , and define the left weighted shift operator $W_{\mathbf t}: \ell^2 \to \ell^2$ by $$W_{\mathbf t}(x_1,x_2,\ldots) = (t_1x_2,t_2x_3,\ldots)$$ Is $W_{\mathbf t}$ unitarily equivalent to some left weighted shift $W_{\mathbf p}$ where $\mathbf p = \{p_n\}_{n=1}^\infty \subset \Bbb R_{>0}$ ? The notation $\Bbb R_{>0}$ is used for positive real numbers. Firstly, I wrote $t_j = r_j e^{i\theta_j}$ for every $j\ge 1$ , i.e., converted to polar coordinates. My natural guess is that $W_{\mathbf t}$ and $W_{\mathbf r}$ should be unitarily equivalent, where $\mathbf r = \{r_n\}_{n=1}^\infty \subset \Bbb R_{\ge 0}$ . The $r_j$ 's can be zero, so even after establishing this equivalence, we must find an appropriate candidate for $\mathbf p$ . Thanks for any help! I'd appreciate hints or solutions.","['operator-theory', 'functional-analysis']"
4701060,How to use implicit function theorem and Lagrange multiplier to prove there exists a local minimum?,"Assume f(x,y) and g(x,y) are two smooth functions on $R^2$ Let $$S=\left \{ (x,y)|f(x,y)=0 \right \} $$ ,
and $$p=(a,b)\in S$$ such that $$\frac{\partial f}{\partial x} (p) = -4,\space\frac{\partial f}{\partial y} (p) = 2,\space\frac{\partial g}{\partial x} (p) = 12,\space \frac{\partial g}{\partial y} (p)=-6$$ And $$\begin{bmatrix} \frac{\partial^2}{\partial x^2}f(p)  &  \frac{\partial^2}{\partial x\partial y}f(p)  \\  \frac{\partial^2}{\partial y\partial x}f(p)  & \frac{\partial^2}{\partial x^2}f(p)\end{bmatrix}=\begin{bmatrix} 1 &2 \\ 2 &4\end{bmatrix}$$ $$\begin{bmatrix} \frac{\partial^2}{\partial x^2}g(p)  &  \frac{\partial^2}{\partial x\partial y}g(p)  \\  \frac{\partial^2}{\partial y\partial x}g(p)  & \frac{\partial^2}{\partial x^2}g(p)\end{bmatrix}=\begin{bmatrix} 3&-1 \\ -1 &2\end{bmatrix}$$ How to prove $ \exists \space R> 0 $ such that $\forall \space q \in {S}\cap\left \{(x,y)| x^2+y^2< R^2 \right \} $ there is always g(p)< g(q) The only thing I am certain is we can prove this by implicit function theorem and Lagrange multiplier, but other than that, I have absolutely no clue where I should start. Could someone give me some hint?","['maxima-minima', 'multivariable-calculus', 'implicit-function-theorem']"
4701101,The ring of integers of $\mathbf{Q}(p^{1/p})$ for prime $p$,I am completing an exercise for my Algebraic Number Theory class and have so far proved that the ring of integers of $\mathbf{Q}(m^{1/n})$ is contained in $\frac{1}{nm} \mathbf{Z}[m^{1/n}]$ . I did this by considering a dual basis of the traceform. I now want to specialise this result for $n=m=p$ where $p$ is prime and I am unsure how to proceed. Any advice? Some ideas were to compute the discriminant of $T^p-p$ which is (up to sign) $p^{2p-1}$ so its hard to make any conclusions using this,"['number-theory', 'ring-theory', 'algebraic-number-theory']"
4701199,An application of Lusin's theorem.,Let $\Omega \subseteq \mathbb R^N$ be open and $\varphi$ be a simple function defined on $\Omega$ which vanishes outside a set of finite Lebesgue measure. Then given $\varepsilon \gt 0$ there exists a continuous function $g \in C_c (\Omega)$ with compact support contained in $\Omega$ such that $g = \varphi$ except possibly on a set whose Lebesgue measure is less than $\varepsilon$ and $\|g\|_{\infty} \leq \|\varphi\|_{\infty}.$ This is clearly Lusin's theorem except the last part. Can we somehow manage to find a continuous function which has the same property but it is bounded above by the measurable function we have started with? Any suggestion in this regard would be warmly appreciated. Thanks for your time.,"['measure-theory', 'lebesgue-measure']"
4701205,Determine the $2023$th derivative of $f(x) = \sqrt{x}$.,"Let $f(x) = \sqrt{x}$ . Determine $f^{2023}(x)$ , the derivative of order $2023$ . Note that $$
f^{1}(x) = \dfrac{1}{2x^{1/2}}, \ \ f^{2}(x) = -\dfrac{1}{2^{2}x^{3/2}} \ \ f^{3}(x) = \dfrac{3}{2^{3}x^{5/2}} \ \ f^{4}(x) = -\dfrac{15}{2^{4}x^{7/2}} \ \ f^{5}(x) = \dfrac{105}{2^{5}x^{9/2}}
$$ Then, So of course in the denominator we have $2^{n}x^{(2n-1)/2}$ . However, I am not able to obtain a relation for the numbered and then take $n = 2023$ . I just know that we have a $(-1)^{n+1}$","['calculus', 'derivatives']"
4701214,Simple left earthquakes are dense,"i´ve been studying an article from W. P. Thurston about hyperbolic geometry, there, he defines something called left earthquake , whose definition is as follows: Definition. If $\lambda$ is a geodesic lamination on a hyperbolic plane, a $\lambda$ - left earthquake map $E$ is a (possibly discontinuous) biyective map from the hyperbolic plane onto itself which is an isometry on each stratum of $\lambda$ . Furthermore, the map $E$ satisfies the condition that for any two strata $A\neq B$ of $\lambda$ , the comparison isometry $$cmp(A,B)=(E|A)^{-1}\circ (E|B):\mathbb{H}^2\to\mathbb{H}^2$$ is a hyperbolic transformation whose axis weakly separates $A$ and $B$ and which translates to the left as viewed from $A$ . Of course, it is easily seen that any right earthquake induces a left earthquake and viceversa. Then, Thurston establishes the following: Theorem. Left earthquake maps with finite laminations are dense in the set of all left earthquake maps, in the topology of uniform convergence on compact sets. He starts the proof by first noticing that the image of a compact set by a left earthquake map has a bounded diameter . Of course, I believe that the proof relies on the fact that compact sets on a metric space are bounded, and thus, under a continuous function, the image of a compact set is bounded, i.e., has bounded diameter.
(Thurston argues that the proof of this fact is in analogy to the proof of the following: The image of a compact interval under a monotone function is bounded , but I´m not able to figure out a proof of the latter). Thurston then gives the following argument: Blockquote Given any left $\lambda$ -earthquake $E$ , then for any compact subset $K$ of the hyperbolic plane, there is a finite subset of strata which intersect $K$ such that the union of the images of the images of this finite set of strata in the graph of $E\subseteq\mathbb{H}^2\times\mathbb{H}^2$ is $\varepsilon$ -dense in the graph of $E$ restricted to $K$ . This argument is not very clear to me, any help with trying to understand this would be very helpful","['low-dimensional-topology', 'hyperbolic-geometry', 'analysis']"
4701223,Why is the $f(x)$ differentiable at $x=0$ but $f'(x)$ is not continuous near $0$?,"Recently I saw a function from a book which states the function below is differentiable at $x=0$ but its derivative is not continuous at $x=0$ . Source was ""Michael Spivak's Calculus 3E"" pg.177 $$\begin{equation*}
f(x)=\begin{cases}
      x²\sin\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\
      0 \quad &\text{if} \, x=0 \\
 \end{cases}
\end{equation*}$$ So, accordingly; $$\begin{equation*}
f'(x)=\begin{cases}
      2x\sin\bigg(\frac{1}{x}\bigg)-\cos\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\
      0 \quad &\text{if} \, x=0 \\
 \end{cases}
\end{equation*}$$ This shows $f'$ is not continuous at $x=0$ . But I couldn't understand why is this happening. $f'(0)$ was the slope of line formed by points $(0,f(0))$ and $(h,f(h))$ where $h$ is infinitely small and was 0. Shouldn't it equal to slope of the line formed by $(h,f(h))$ and $(k,f(k))$ where $h<k<0$ ? Could you explain me graphically why slope of line formed by $(0,f(0))$ and $(h,f(h))$ is $0$ while slope of line formed by $(h,f(h))$ and $(k,f(k))$ is $≠0$ ? And can you please give me some more examples which is simpler than this? (A case where $f$ is differentiable at $a$ but $f'$ isn't continuous at $a$ )","['continuity', 'calculus', 'derivatives']"
4701234,"Second order derivative of $f(x):=\frac{1}{2} ⟨x,Ax⟩$","Let $A=\left(A_{i j}\right)$ be an $n \times n$ symmetric matrix, and define the function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ as $$
f(x):= 
\frac{1}{2}
​
 ⟨x,Ax⟩
$$ Using the definition, determine the second-order derivative $D^2 f(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right)$ . (Here, $\operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right)$ refers to the space of bilinear maps $\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ .). I have found the derivative using the definition, that is $a^TA$ , and I know that the definition of the second-order derivative is $\left(D^2 f\right)(a):=D(D f)(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}^m\right)$ , but I don't know how to use this to find the second-order derivative.","['scalar-fields', 'multivariable-calculus', 'hessian-matrix', 'derivatives', 'quadratic-forms']"
4701245,"Find $\,\lim\limits_{t\to0^+}t\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x$","Question: Find $\;\displaystyle\lim_{t\to0^+}\,t\!\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x$ . Attempted solution: Let: $$f(x)=\begin{cases}\frac{\cos x-1}{x^{3/2}}&\text{if }x\in(0,\pi/2)\\0&\text{otherwise}\end{cases}$$ $\lim\limits_{x\to0^+}f(x)=0\;$ so $\,f(x)\,$ is continuous on $[0,\pi/2]$ . So, $\;\displaystyle\left|\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x\right|\leq\int_0^t \frac{1-\cos x}{x^{3/2}}\,\mathrm{d}x\;,$ for $t\in[0,\pi/2]$ , which tends to zero as $t\to0^+$ . $$\begin{align}t\int_{t^2}^t\frac{\cos x}{x^{3/2}}\,\mathrm{d}x&=t\left[\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x+\int_{t^2}^t\frac{1}{x^{3/2}}\,\mathrm{d}x\right]\\&=O(t)+2 (1-\sqrt{t})\end{align}$$ So the answer is $2$ . I just wanted to make sure is my approach correct.","['integration', 'solution-verification', 'analysis', 'real-analysis']"
4701246,Understanding convergence in space of continuous functions on an open set,"I am reading Conway's Complex Analysis and chapter $7$ talks about the space of continuous functions on an open set $G \subset \mathbf{C}$ into a metric space $(\Omega,d)$ , denoted by $C(G,\Omega)$ . The topology is later introduced as follows: A set $U \in C(G,\Omega)$ is open if and only if for each $f \in U$ there exists a compact set $K$ and a $\delta >0$ such that $\{g: \sup_{z \in K} d(f(z),g(z))<\delta\} \subset U$ . Using an exhaustion of $G$ by compact sets $K_n$ and letting $\rho_n(f,g)=\sup_{z \in K_n} d(f(z),g(z))$ we get a metric $\rho$ on $C(G,\Omega) $ where $$\rho(f,g)=\sum_{n=1}^\infty \frac{1}{2^n}\frac{\rho_n(f,g)}{1+\rho_n(f,g)}$$ Building on that is the following true for convergence in $C(G,\Omega)$ ? Let $ \epsilon >0$ be arbitrary. Then $\rho(f,g)<\epsilon$ if and only if for each compact set $K$ in $G$ , there exists a $\delta>0$ such that $\sup_{z \in K} d(f(z),g(z))<\delta$ .","['complex-analysis', 'metric-spaces']"
4701325,References on history of topology and categorical aspects of topology.,"I am currently writing the conclusions of my bachelor's thesis on convergence spaces and there are a couple of points I would like to make, but lack the proper references to cite in order to do so. The first point I would like to make is that one of the starting points of General Topology was trying to axiomatize the notion of convergent sequences and hence the notion of a convergence space is much closer to the origins of topology. I recall reading somewhere that the first attempts of defining topological spaces were like that, but I can't find where. The second point is that the category of convergence spaces is much more adequate than Top, because it has exponential objects. I know there is a whole discussion of the importance of having cartesian closed categories of spaces, but I by myself cannot argument that, because my background on category theory is really small. Therefore, I need to cite someone who knows that for a fact and can give reasons why that is. Hopefully I am not breaking any rules by asking two different sources in the same post. I thank gratefully for any answer.","['book-recommendation', 'category-theory', 'reference-request', 'general-topology', 'convergence-divergence']"
4701334,Piecewise function with $\sqrt{2 + \sqrt{n}}$ and $\sqrt{1 + \frac{\sqrt{n}}{2}} + i \sqrt{-1 + \frac{\sqrt{n}}{2}}$,"Problem Let $f : \mathbb{C} \setminus \{0\} \rightarrow \mathbb{C}$ , $f(z) = z + \frac{1}{z}$ . Given that $$
f(z) = \begin{cases}
\sqrt{2 + \sqrt{n}}     & \text{if } z \in \mathbb{R} \setminus \lbrace 0 \rbrace \\
\sqrt{1 + \frac{\sqrt{n}}{2}} + i \sqrt{-1 + \frac{\sqrt{n}}{2}}
                        & \text{if } z \in \mathbb{C} \setminus \mathbb{R}
\end{cases}
$$ for some positive integer $n$ and that $f(z^8) = 2023$ for all $z \in \mathbb{C} \setminus \lbrace 0\rbrace$ , find $n$ . This problem is from the team round of a local high school math competition that has ended. My Work Interpretation 1: $z$ is a variable: We know that $z^8$ is a real number, because the function keeps real numbers real and complex numbers complex. So we have $f(z^8) = z^8 + \frac{1}{z^8}$ which is equal to the first part of the piecewise which is $\sqrt{2 + \sqrt{n}}$ . We know that $2023 = z^8 + \frac{1}{z^8} = \sqrt{2 + \sqrt{n}}$ . This does not make any sense. I think that $z$ is actually just a constant and that $f(z) = z + \frac{1}{z}$ is not the definition for a function, but instead just an equation . Interpretation 2: $z$ is a constant or a complex constant If $z$ was a real number, there would only be equation that we could use, $f(z) = \sqrt{2 + \sqrt{n}}$ . There is no way we can get anywhere with one equation. Therefore, $z$ is probably a complex number, so we are able to use three different equations. Looking at the second part of the piecewise function, the only value of $n$ that could possible turn a complex number into a real number is $4$ . So lets try investigating $n=4$ . If $n=4$ , we know that $f(z) = \sqrt{2}$ , so $z + \frac{1}{z}$ also equals $\sqrt{2}$ . If we solve for z, we get two solutions, $z = \frac{\sqrt{2}}{2} + i\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} - i\frac{\sqrt{2}}{2}$ . After this, I am not sure how to continue. Nothing makes sense. I am not even sure if all of my previous steps are correct. I would appreciate some help. Thank you!","['nested-radicals', 'radicals', 'functions', 'algebra-precalculus', 'complex-numbers']"
4701337,Why do number theorists care so much about how well $\text{Li}(x)$ approximates $\pi(x)$ if it's not our best approximation?,"An alleged primary motivator for the RH is so that we can bound the error term $|\text{Li}(x) - \pi(x)|$ by a factor of $O(\sqrt{x}\log x)$ . However, I also learned about Riemann's explicit formula $R(x)$ that converges, upon addition of the harmonics $C_i(x, \theta_i)$ , where $\theta_i$ is the Riemann spectrum. Thus, we basically know that the Zeta function encodes enough information to construct an exact prime counting function and to know the distribution of the primes, $\pi(x)$ , exactly. And even if we put the harmonics aside, $R_0(x)$ itself (no harmonics) is a much better approximation to $\pi(x)$ than just $\text{Li}(x)$ , so why do we care so much about bounding the error between $\text{Li}(x)$ and $\pi(x)$ ? I get that $R(x)$ of course is a function of $\text{li}(x)$ , but my point is shouldn't $R(x)$ be the fundamental object of study in the study of the distribution of primes, then? Another, very related, question I have is why a square-root bound is good at all -- the Prime Number Theorem tells us that $\left|\frac{\pi(x)}{\text{Li}(x)}\right| \to 1$ , so $\text{Li}(x)$ gets arbitrarily close to $\pi(x)$ for large enough $x$ , so what does this (in my mind, very weak) square-root bound tell us that PNT doesn't already? These questions are probably very naïve -- I have taken a course in complex analysis, but not much number theory.","['distribution-of-primes', 'riemann-hypothesis', 'complex-analysis', 'analytic-number-theory', 'zeta-functions']"
4701360,"Does real analysis have new theorems, or is it just a collection of proofs of old calculus theorems?","I am trying to teach myself real analysis, but I was wondering if this subject is just a collection of proofs of calculus theorems. Aside from set theory, I haven’t learned anything new. By that, I mean I haven’t come across any new theorems that I didn’t already know from calculus. Also, in calculus, there are many difficult problems, including challenging integrals and limits. However, the problems I have faced so far in real analysis were hard because of my proof-writing skills, not because they were difficult problems like integrals. My second question is: If real analysis does not deal with hard integrals and if calculus books like Thomas' book don't have hard problems, then where does the insanely hard integrals that I see online came from ? something like nonelementary integrals or  nonelementary function like $\operatorname{Li}(x)$ . Where I can study them if not in real analysis or calculus   ?","['integration', 'calculus', 'book-recommendation', 'real-analysis']"
4701392,Why is curl given by a cross product?,"I've googled a lot but in most places, they usually only show how to compute it which I by now can do. But I wanna understand why the cross product between nabla and the vector give us the curl. So far I've managed to understand that when we are talking about the curl we are interested in the vector that points upwards or downwards (perpendicular) to the actual curl (rotation) like the green vector down here: And then since the cross product between two vectors give us another vector orthogonal to both I finally understood why we do $\nabla\times \vec{F}$ . Am I correct so far? Otherwise feel free to correct me (I actually want you to). But then my first question is why does this ""green"" vector in the picture define the curl? For example in Stoke's theorem we use curl to calculate higher dimension of ""Green"", but in Green's theorem we don't take any perpendicular vectors (obviously because z is non existent). This brings me to my second question which is how is the curl related to Green's thorem? Until now I had assumed that the green's theorem was just like line integral where we are only interested in the the field vector's component's that are parallel to the curve's tangents. But in general I am confused and I'd really appreciate it if someone could just clear these things up for me?","['divergence-theorem', 'curl', 'multivariable-calculus', 'calculus', 'linear-algebra']"
4701396,Understanding nearest neighbors in high-dimensional data,"Let's have a random sample of points in an euclidean $n$ -space: assume a iid sample from a standard normal distribution. To each point $p$ , I assign the number $N(p)$ defined as ""how many times does $p$ occur in $10$ nearest neighbors of some other point""? I would like to understand the distribution of $N(p)$ and how it depends on dimension. Some experimental results from python are quite surprising to me. In low dimensions, for $n=2,3,4$ this distribution is close to normal, while in higher dimensions it's very skewed, with most point being no nearest neighbours to anything else, and a few points being nearest neighbours to a lot of other points. I always generate 1000 exampels here (not that it matters), here are some histograms: I have some intuition / idea that the relation with dimension may boil down to something like this: in dim $1$ , a point can be nearest neighbor of at most 2 points, one on the left and one on the right in dim $2$ , a point can be nearest neighbor of 5 or 6 points, forming a regualr hexagon around it
and so on. I also checked if those points that are NN of many other points are more like in cluster center -- which is intuitively expected. The answer is yes, here is an example in dimension 32: However, I'm not sure why is the dependency on dimension so crucial? The motivation comes from searching for nearest neighbours where the data represent some real objects, and nearest neighbours represent search of a user. Should I use rather smaller dimension, if I want to support the idea that each element should get a chance of being found? Thanks for any insight. (Related: https://ai.stackexchange.com/questions/40525/nearest-neighbour-search-in-high-dimension-retrieves-certain-points-too-often )","['data-structure', 'statistics', 'topological-data-analysis']"
4701398,Simple induction. Do I have to use it three times for each value?,"I have the following problem, $n = e_{k}3^k + \ldots + e_{1}3 + e_0$ . With $k\geq0$ , where $e_i \in \{1,0,-1\}$ for all $i \in \{0, \ldots,k\}$ And I have to prove that every natural can be represented that way, with Simple Induction. Do I have to do simple induction three times, each one for a different $e_i$ ? Edit: I got stuck in all of the paths. This is what I have: For $e_i=0$ : $P(0)=0$ , $P(n)=0$ , so $P(n+1)=1$ which is natural. (I don't know if this is right). For $e_i=-1$ : $P(0)=0$ , $P(n)=-3^k-...-3-1$ , so: $P(n+1)=-1(3^k+...+3)$ . (I think in this case I cannot assume that this number is natural). For $e_i=1$ : $P(0)=0$ , $P(n)=3^k+...+3+1$ so: $P(n+1)=3^k+...+3+1+1$ In summary, I don't know what I am doing. I am new at this.","['induction', 'discrete-mathematics']"
4701408,Approximating the series $\sum_{k=0}^\infty\!\big(\frac{e^{-\theta}\theta^k}{k!}\big)^\beta$,"Let $0<\beta\leq 1$ , $\theta\geq0$ , and consider the series $$
S(\theta,\beta)=\sum_{k=0}^\infty\left(\frac{e^{-\theta}\theta^k}{k!}\right)^\beta.
$$ How might we determine the upper and lower indicies $k_\ell$ and $k_u$ such that $$
\sum_{k=k_\ell}^{k_u}\left(\frac{e^{-\theta}\theta^k}{k!}\right)^\beta=(1-\epsilon)S(\theta,\beta)
$$ for choice of small $\epsilon>0$ ? The case for $\beta=1$ is easy since the terms of the series represent Poisson probabilities.  This means that one may use the Poisson quantile function, after specifying $\epsilon$ , to write $$
k_\ell(\theta,1)=F^{-1}_\theta(\epsilon/2)
$$ and $$
k_u(\theta,1)=F^{-1}_\theta(1-\epsilon/2).
$$ For $\beta<1$ the terms in the series, if interpreted as probabilities after multiplying a normailzation constant, result in a probability distribution that is of larger variance and so I suspect $$
k_\ell(\theta,\beta)\leq k_\ell(\theta,1)
$$ and $$
k_u(\theta,\beta)\geq k_u(\theta,1).
$$ For my particular application, the less terms in the series I need to obtain the approximation the better.  Computational cost of adding additional terms will be expensive.","['power-series', 'approximation', 'poisson-distribution', 'sequences-and-series']"
4701411,"Showing $f(t)=\text{Tr}[A(I-A)^{2t}]\approx\frac{\sqrt{\pi/2}\,\text{erfc}(\sqrt{2t}\,/d)}{2\sqrt{t}}$, where $A=\text{diag}(1,1/4,1/9,\ldots,1/d^2)$","Suppose $h=\left(1,\frac{1}{4},\frac{1}{9},\ldots,\frac{1}{d^2}\right)$ and $A=\operatorname{diag}(h)$ What's the easiest way of deriving the following expression in terms of Erf function? $$f(t)=\operatorname{Tr}\left[A(I-A)^{2t}\right] \approx \frac{\sqrt{\frac{\pi }{2}} \left(1-\text{erf}\left(\frac{\sqrt{2} \sqrt{t}}{d}\right)\right)}{2 \sqrt{t}} $$ The following tests goodness of fit against true values of $f(t)$ for fixed $d$ or fixed $t$ Notebook Motivation: $f(t)$ gives loss after $t$ steps of gradient descent minimizing $y=x_1^2+\frac{1}{4}x_2^2+\frac{1}{9}x_3^2 + \ldots + \frac{1}{d^2}x_d^2$","['calculus', 'linear-algebra']"
4701421,"How to determine the size of collection, given the size of a selection and the number of ways to select?","Problem A gym coach must select $11$ seniors to play on a football team. If they can make their selection in $12,376$ ways, how many seniors are eligible to play? The answer is $17$ , but how do I get there? What I’ve tried Below are my attempts to solve the equation $$\dbinom n {11} = 12,376$$ and its equivalent $$\dfrac {n!}{11!(n-11)!}=12,376$$ First Simplifying factorials in the last equation I get $$\dfrac{n(n-1)(n-2)(n-3)\cdots(n-10)}{11!} = 12,376$$ I’ve found out that the numerator in the left part of the equation matches the definition of falling factorial , but I don’t fully understand this concept. Besides, I’m sure there should be a simple solution that does not involve anything advanced because the problem comes from the first chapter of the book on discrete and combinatorial mathematics intended for beginners. I’ve tried to simplify the numerator of the left part of the equation as follows: $(n-1)(n-10) = n^2 -11n + 10$ $(n-2)(n-9) = n^2 -11n + 18$ $(n-3)(n-8) = n^2 -11n + 24$ $(n-4)(n-7) = n^2 -11n + 28$ $(n-5)(n-6) = n^2 -11n + 30$ Let $a = n^2 -11n$ , then $$\dfrac {n(a+10)(a+18)(a+24)(a+28)(a+30)} {11!} = 12,376$$ However, I was unable to simplify it further. Do I miss something from elementary algebra here? Second Also, I’ve tried to use Pascal’s identity to represent $\dbinom n {11}$ as a sum of binomial coefficients: $$\large \displaystyle
\begin{align*}
\dbinom n {11} &=
\dbinom {n-1} {11} +
\dbinom {n-1} {10} \\
&=
\bigg[\dbinom {n-2} {11} +
\dbinom {n-2} {10} \bigg] +
\bigg[\dbinom {n-2} {10} +
\dbinom {n-2} {9} \bigg] \\
&=
\dbinom {n-2} {11} +
2 \dbinom {n-2} {10} +
\dbinom {n-2} {9} \\
&=
\dbinom {n-3} {11} +
3 \dbinom {n-3} {10} +
3 \dbinom {n-3} {9} +
\dbinom {n-3} {8} \\
&=
\dbinom {n-4} {11} +
4 \dbinom {n-4} {10} +
6 \dbinom {n-4} {9} +
4 \dbinom {n-4} {8} +
\dbinom {n-4} {7} \\
& 
\vdots \\
&=
\dbinom {11} 0 \dbinom {n-11} {11} +
\dbinom {11} 1 \dbinom {n-11} {10} +
\dbinom {11} 2 \dbinom {n-11} {9} + \cdots +
\dbinom {11} {11}  \dbinom {n-11} {0} \\
&=
\sum_{k=0}^{11} \dbinom {11} k \dbinom {n-11} {11-k} 
\end{align*}$$ Then $$\displaystyle
\sum_{k=0}^{11} \dbinom {11} k \dbinom {n-11} {11-k} =12,376$$ . I tried to apply Vandermonde’s identity here, but actually, I could only prove that the equation above is the same equation I'd started from. $\displaystyle
\sum_{k=0}^{11} \dbinom {11} k \dbinom {n-11} {11-k} = \dbinom {n-11+11} {11} = \dbinom n {11}$ . Also, I was thinking about using the binomial theorem here. May I rewrite the equation above as $\displaystyle
\sum_{k=0}^{11} \dbinom {11} k x^k y^{11-k}= 12,376$ , where $x,y \in R$ ? If so, then $(x+y)^{11}=12,376$ , but what should I do with it? Third (continuation of the second) Since $\displaystyle
\sum_{k=0}^{11} \dbinom {11} k \dbinom {n-11} {11-k} = 
\sum_{k=0}^{11} \dbinom {11} k \dbinom {n-11} {k} = 1 + 
\sum_{k=1}^{11} \dbinom {11} k \dbinom {n-11} {k}$ , I can rewrite $\dbinom n {11} = 12,376$ as $$\displaystyle 1 + \sum_{k=1}^{11} \dbinom {11} k \dbinom {n-11} {k} = 12,376$$ Then $$
\sum_{k=0}^{10} \dbinom {11} k \dbinom {n-11} {k} = 12,375
$$ Then I thought about the representation of each term in $\displaystyle
\sum_{k=1}^{11} \dbinom {11} k \dbinom {n-11} {k}$ using Pascal’s identity recursively, as I did previously with $\dbinom n {11}$ . $$
\large
\begin{align*}
\displaystyle &
\sum_{k=1}^{11} \dbinom {11} k \dbinom {n-11} {k} &\\
&=
\dbinom {11} 1 \sum_{k_1=0}^1 \dbinom 1 {k_1} \dbinom {n-12} {k_1} +
\dbinom 2 {k_2} \dbinom {11} 2 \sum_{k_2=0}^2 \dbinom {n-13} {k_2} + \cdots \\
&\qquad+
\dbinom {11} {11} \sum_{k_{11}=0}^{11} \dbinom {11} {k_{11}} \dbinom {n-21} {k_{11}} \\
&=
11 + \dbinom {11} 1 \dbinom {n-12} 1 + 55 + \dbinom {11} 2 \sum_{k_2=1}^2 \dbinom 2 {k_2}\dbinom {n-13} {k_2} + \cdots \\
&\qquad + 1 + \dbinom {11} {11} \sum_{k_{11}=1}^{11} \dbinom {11} {k_{11}} \dbinom {n-21} {k_{11}} \\
& =
11^2-1 + \sum_{k=1}^{11}\dbinom {11} k \sum_{i=1}^{k} \dbinom k i \dbinom {n-(11+k)} i
\end{align*}
$$ Assuming the expression above is correct (which I really doubt), $11^2-1 + \sum_{k=1}^{11}\dbinom {11} k \sum_{i=1}^{k} \dbinom k i \dbinom {n-(11+k)} i = 12,375$ $\sum_{k=1}^{11}\dbinom {11} k \sum_{i=1}^{k} \dbinom k i \dbinom {n-(11+k)} i = 12,255$ However, it looks like this approach leads to an infinite process since the number of terms on the left side of the equation does not decrease. Sorry, for such a long post. I was putting as much effort as I could to solve this problem, but I really need some help.","['combinations', 'combinatorics']"
4701441,minimal $U$ where $\mathbb R^{\mathbb R} - U$ is not path connected.,"Let $U \subset \mathbb R^{\mathbb R}$ . We are looking for a minimal cardinality $U$ where $\mathbb R^{\mathbb R} - U$ is not path connected. First of all, it is obvious that there will be an injection $U \to \mathbb R^k$ for any $k>0$ , since we can always find an imbedding $f : \mathbb R^k \to \mathbb R^{\mathbb R}$ , take the image of $f$ , and ""go around"" whichever coordinates are missed (Say, a line using dimension $k+1$ , where we never move across the $k$ dimensions). My guess is that $U$ will have to be of equal cardinality to $\mathbb R^{\mathbb R}$ - since an infinite sphere with $\mathbb R$ dimensions almost seems necessary to remove from $\mathbb R^{\mathbb R}$ to disconnect $\mathbb R^{\mathbb R}$ . My question is: Is there a smaller set that disconnects $\mathbb R^{\mathbb R}$ ? If not, is that canonical sphere the same size as $\mathbb R^{\mathbb R}$ ?","['real-numbers', 'general-topology', 'path-connected']"
4701454,Is the series $\sum_{n=1}^\infty\ln(1 + a_n)$ convergent or divergent?,"We are given that $\sum_{n=1}^\infty a_n$ is convergent, and we want to know if $\sum_{n=1}^\infty\ln(1 + a_n)$ is convergent or not. I used the limit comparison test. We get $$ \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n}.$$ And using L'Hospital's rule, I get $$\lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n} = \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{\frac{1}{1+a_n}(a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{1}{1+a_n}=1.$$ However, we must know that $a_n$ can be convert to a differentiable function then we can use L'Hospital's rule, so I am just confused, is there any way to prove this, or can we prove that $a_n $ can be converted to a differentiable function? I also took a look at the same questions/answers, but none of these answered my question.","['calculus', 'derivatives', 'sequences-and-series']"
4701473,"Prove that the unit lower diagonal matrices $L, M$, and $D$ are all unique","If all the leading principal submatrices of a matrix $A$ (i.e. all square submatrices that share a top left corner with $A$ ) are nonsingular, then prove that there are unique unit lower diagonal matrices $L$ and $M$ , and a unique diagonal matrix $D$ such that $A=LDM^T$ . I've seen a proof that such matrices $L,D,M$ all exist, but I'm not sure how to prove uniqueness. I would start by assuming that there are two such matrices $L_1$ and $L_2$ and then derive a contradiction. I think I could then use a similar argument to show $M$ and $D$ are unique. Here is the proof. Factor $A=LU$ . Define $D= diag(d_1,\cdots, d_n), d_i=u_{ii},1\leq i\leq n.$ All $d_i\neq 0$ as all of $A$ 's leading principal submatrices are nonsingular. So $D^{-1}$ exists. Note that $D^{-1}U$ is unit upper triangular. So $M = (D^{-1}U)^T$ is unit lower triangular, and $A=LDM^T$ .","['matrices', 'numerical-linear-algebra', 'linear-algebra']"
4701523,Solving a modified matrix equation using solutions of the original equation,"I am trying to find analytically the eigenmodes of the following equation for a damped system with $N$ degrees of freedom: $$ \mathbf{M}\ddot{\mathbf{u}} + \mathbf{C} \dot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0 $$ in terms of the eigenmodes of the undamped system that I have already found computationally: $$ \mathbf{M}\ddot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0 $$ These eigenmodes are (say) $ \mathbf{u}_n(t) = \mathbf{u}_n e^{i\omega_n t} $ where $$ \mathbf{M}^{-1}\mathbf{K}\mathbf{u}_n = \omega_n^2 \mathbf{u}_n $$ $ \mathbf{M}$ is the diagonal mass matrix and $ \mathbf{C}$ is the diagonal matrix of damping factors for each dof. $ \mathbf{K}$ is the symmetric spring constant matrix coupling different degrees of freedom. My attempt so far: Assuming the time dependence as $\mathbf{u}(t) = \mathbf{u}e^{i\omega t}$ , the damped equation can be written as: $$  (\mathbf{M}^{-1}\mathbf{K} + i\omega\mathbf{M}^{-1}\mathbf{C})\mathbf{u} = \omega^2 \mathbf{u} $$ Now since $\mathbf{C}$ is not proportional to the Identity matrix, it does not commute with the ""dynamical"" matrix $\mathbf{M}^{-1}\mathbf{K}$ and so the previously found normal modes are not the eigenmodes for the new system. But is it possible to maybe rescale some of the degrees of freedom or transform the equation in another way and find the new normal modes using the old ones $\mathbf{u}_n$ ? Also, note that I wish to look at the solutions for low frequencies, so treating the damping as a small perturbation does not help.","['matrices', 'linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4701573,Probability question: What's the chance of exactly 1 red die matching the blue die?,"I'm currently working on a probability problem and could use some assistance. The problem involves three six-sided dice: one blue die and two red dice. I'm trying to determine the probability of exactly one of the red dice showing the same number of dots as the blue die. Here's my attempt at solving it: To find the probability, I started by considering the possible outcomes and favorable outcomes: Possible outcomes: Each die has six possible outcomes since they are six-sided dice. Favorable outcomes: To have exactly one of the red dice match the blue die, I need to choose one of the two red dice and match its dots with the blue die. Each red die has six possible outcomes, considering the six sides. So, there are a total of $2 \cdot 6 = 12$ favorable outcomes. Using the formula for probability (favorable outcomes divided by possible outcomes), I attempted to calculate the probability as follows: Probability = favorable outcomes / possible outcomes
= 12 / (2 * 6)
= 12 / 12
= 1 Therefore, my initial calculation suggests that the probability is 1 or 100%. This would mean it's certain that exactly one of the red dice will match the number of dots on the blue die. Could someone please verify my approach and calculations? If I made any mistakes or if there's a more accurate method to solve this problem, I would greatly appreciate your guidance. Thank you in advance for your help!","['conditional-probability', 'dice', 'probability-theory', 'probability']"
4701584,Is there any more efficient way to find the basis of the intersection of two subspaces?,"Let $V = \mathbb R^6.$ Let $W_1$ be the subspace of $V$ spanned by $$\left ( 1,2,3,4,5,6 \right ) ,\space \left ( 3,4,6,7,9,10 \right ) ,\space \left ( 0,1,0,2,0,3 \right ),\space\left ( 1,-2,3,-4,5,-6 \right ).  $$ Let $W_2$ be the subspace of $V$ spanned by $$\left ( 1,1,1,2,2,3 \right ) ,\space \left ( -2,0,-1,0,1,2 \right ) ,\space \left ( 1,0,1,0,2,0 \right ),\space\left ( 0,0,1,0,-2,-2 \right ).$$ Find the dimension of the subspace $W_1\cap W_2$ and find a basis for this subspace. I know I could take $\forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2$ $$x=\alpha _1\left ( 1,2,3,4,5,6 \right ) +\space \alpha _2\left ( 3,4,6,7,9,10 \right ) +\space \alpha _3\left ( 0,1,0,2,0,3 \right )+\space\alpha_4 \left ( 1,-2,3,-4,5,-6 \right ) $$ and $$x=\beta_1 \left ( 1,1,1,2,2,3 \right ) +\space \beta _2\left ( -2,0,-1,0,1,2 \right ) +\space  \beta_3\left ( 1,0,1,0,2,0 \right )+\space\beta _4\left ( 0,0,1,0,-2,-2 \right ).$$ Then $$\begin{bmatrix}  1&  3&  0&  1& a\\  2&  4&  1&  -2& b\\  3&  6&  0&  3& c\\  4&  7&  2&  -4& d\\  5&  9&  0&  5& e\\  6&  10&  3&  -6&f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  1&  0&  0&  0& b\\  1&  -1&  1&  1& c\\  2&  0&  0&  0& d\\  2&  1&  2&  -2& e\\  3&  2&  0&  -2&f\end{bmatrix}.$$ Then takes so much (tedious) effort to get $$\begin{bmatrix}  1&  3&  0&  1& a\\  0&  0&  1&  -4& b-\frac{2}{3}c \\  0&  1&  0&  0& a-\frac{1}{3}c \\  0&  0&  0&  0& a-2b-\frac{1}{3}c+d \\  0&  0&  0&  0&  a-2c+e\\  0&  0&  0&  0&2a-3b-\frac{2}{3}c+f\end{bmatrix} \text{and} \begin{bmatrix}  1&  -2&  1&  0& a\\  0&  0&  -1&  -2& b+a-2c \\  0&  1&  0&  1& c-a \\  0&  0&  0&  0& d-2b \\  0&  0&  0&  0&  4e-2a-6c-7f+21b\\  0&  0&  0&  1&\frac{-1}{4}f-\frac{1}{2}a+\frac{1}{2}c+\frac{3}{4}b    \end{bmatrix}.$$ So $\forall x=\left ( a,b,c,d,e,f \right ) \in W_1\cap W_2$ , we have \begin{align}
a -2b-\frac{1}{3}c+d &=0 \\
a-2c+e &= 0 \\
2a-3b-\frac{2}{3}c +f &=0 \\
4e-2a-6c-7f+21b &=0 \\
d-2b &=0
\end{align} Finally, we get $$x=\left ( e,f,3e,2f,5e,3f \right ) $$ so $$\left \{ \left  (1,0,3,0,5,0 \right ) ,\space \left (0,1,0,2,0,3  \right )   \right \} $$ is a basis for $W_1\cap W_2$ . Is there any efficient way to accomplish this?","['dimensional-analysis', 'linear-algebra']"
4701588,Contour integral of modulus of a sequence of analytic function converges to zero implies the function converges to zero.,"Consider a sequence of function $\{f_n(z)\}_{n \geq 1}$ defined on complex space $z \in \mathbb{C},$ and assume for each $n$ , $f_n$ is analytic in $\{|z|<1 \}$ , and continuous on $\{ |z|\leq 1\},$ and the following property holds \begin{equation}
\lim_{n \rightarrow \infty} \frac{1}{2\pi} \int_{0}^{2\pi} |f_n(e^{i\theta})| d \theta =0.
\end{equation} I want to show that \begin{equation}
\lim_{n \rightarrow \infty} \max_{|z| \leq 1} |f_{n}(z)| =0.
\end{equation} My attempt is to use maximum modulus principle for analytic functions. If the statement is wrong, then there exists a subsequence ${f}_{n_k}$ and a positive constant $\epsilon_0$ such that $\max_{|z| \leq 1} |f_{n}(z)|=|f_n(z_n)| \geq \epsilon_0.$ Clearly $z_n \in \{ |z|=1 \}$ . I want to show that $\frac{1}{2\pi} \int_{0}^{2\pi} |f_{n_k}(e^{i\theta})| d \theta$ does not tend to $0$ , but I am stuck here. Can anyone give me some advice? $\textbf{Update}$ ：GEdgar has given a counterexample. Now I would like to prove that, for a given 0<r<1, we have \begin{equation}
\lim_{n} \max_{|z| \leq r} \ | f_n(z) |=0.
\end{equation}","['complex-analysis', 'analysis']"
4701596,Plancherel Theorem in Rudin's Real and Complex Analysis (theorem 9.13),"I understood the whole theorem proof, except for the very last part. Here is the theorem 9.13 Theorem One can associate to each $f\in L^2$ a function $\hat{f}\in L^2$ so that the following properties hold: (a) if $f\in L^1\cap L^2$ , then $\hat{f}$ is the previously defined Fourier transform of $f$ (b) for every $f\in L^2\quad,\quad||\hat{f}||_2=||f||_2$ (c) The mapping $f\rightarrow \hat{f}$ is a Hilbert space isomorphism of $L^2$ onto $L^2$ . (d) The following symmetric relation exists between $f$ and $\hat{f}$ : If $$\varphi_A(t)=\int_{-A}^{A}f(x)e^{-ixt}dm(x)\quad\text{and}\quad\psi_A(t)=\int_{-A}^{A}\hat{f}(x)e^{ixt}dm(x)$$ then $||\varphi_A-\hat  f||_2\rightarrow 0\quad\text{and}\quad||\psi_A-  f||_2\rightarrow 0$ The part I miss is the last (d) dealing with $\psi_A$ ; Rudin wrote ""The other half of (d) is proved the same way"" (with no detail at all: the ""first half"" deals with $\varphi_A$ ). One answer that I read here assumes that the following function $\mathscr F^{-1} : L^1\rightarrow L^\infty$ is the inverse Fourier transform $$\mathscr F^{-1}(f)(t)=\int_{-\infty}^{+\infty}f(x)e^{ixt}dm(x)\qquad\qquad(\star)$$ But in Rudin's ""Real and Complex Analysis"", the inversion theorem (9.11) gives this formula only when the function and its Fourier transform are in $L^1$ : 9.11 The Inversion Theorem If $f\in L^1\quad\text{and}\quad \hat f\in L^1$ and if $$g(x)=\int_{-\infty}^{+\infty}\hat f(x)e^{ixt}dm(t)\qquad (x\in R^1)$$ then $g\in C_0\quad\text{and}\quad g(x)=f(x)$ a.e. If $k_A$ is the characteristic function of $[-A,A]$ , then $\psi_A$ seems to be the inverse Fourier transform of $k_A\hat f$ , assuming that the inverse Fourier transform is given by $(\star)$ . ""Seems to be"" : this is exactly the point I miss : although obviously $k_A\hat f\in L^1$ , nothing proves that $\psi_A$ is in $L^1$ (I daresay the contrary, since generally $k_A\hat f$ is not equals a.e. to a continuous function, whereas the Fourier transform of a $L^1$ function is coninuous), so Rudin's 9.11 theorem can't be applied. Hence my question : how can this ""second part of (d)"" be resolved ?","['proof-explanation', 'measure-theory', 'fourier-transform', 'real-analysis']"
4701631,How to simplify $\int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz$?,"During the integration of a highly symmetric integrand $$ \int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz$$ What manipulations can be done to reduce the above triple integral to just a single integral of for instance some function $f(w)$ of some variable $w$ such that $$\int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz = \int_{-t}^t f(w) dw $$ ? Attempt: $ \int_{-t}^t x^3 (dx)^3$ misses many terms when $x,y,z$ have different values.","['integration', 'multivariable-calculus']"
4701644,Showing a recursively defined sequence is convergent using Banach fixed-point theorem,"I recently stumbled across an interesting question, in which we need to show a recursively defined sequence $(x_n)_{n\in\mathbb{N}}$ in $\mathbb{R}$ defined as follows $$
x_0 = 4 , \quad x_{n+1}=\ln\left(3-\frac{x_n}{2}\right) \quad \text{for} \ \ \  n \ge 1
$$ is convergent using the Banach Fixed-Point Theorem. I am familiar with how to show a function is convergent using the Banach Fixed-Point Theorem given, but I haven't necessarily seen it be used on a recursively defined sequence. I'd assume you could use generating functions, but is there an easier way? I would be thankful for any help.","['fixed-point-theorems', 'sequences-and-series', 'analysis', 'real-analysis']"
4701673,Stopping time with $\mathbb{P}(\tau<\infty)=1$,"Let $\tau=\min \left\{t: X_t=3\right\}$ . Using the the fact that $\tau$ is a stopping time satisfying $\mathbb{P}(\tau<\infty)=1$ , prove the following relation: $$
\mathbb{E}\left[\alpha^\tau\right]=\left(\frac{1-\sqrt{1-\alpha^2}}{\alpha}\right)^3, \text { for any constant } \alpha \in(0,1)
$$ where $X_0=0$ and $$
X_t=Z_1+\cdots+Z_t .
$$ and $Z_1, Z_2, \ldots$ be a sequence of i.i.d. (independent and identically distributed) random variables with $$
\mathbb{P}\left(Z_i=1\right)=\mathbb{P}\left(Z_i=-1\right)=1 / 2,
$$ Assume: $\mathcal{F}_0=\{\emptyset, \Omega\}$ and $\forall t \in \mathbb{N}^*, \mathcal{F}_t=\sigma\left(Z_1, \ldots, Z_t\right)$ . I'm stuck on this problem that I can't solve, any help is welcome. I think I should probably use Optional Sampling Theorem but I dont know how to proceed.","['stopping-times', 'probability-theory', 'probability']"
4701691,One to one and onto function.,If $A\subset \mathbb{N}$ define a sequence $(a_n)$ by $a_n=1$ if $n \in A$ and $a_n=0$ is $n\notin A$ . Then correspondence $A \rightarrow (a_n)$ is both one one and onto. I am not able to understand the one to one here. What if two different elements are taken from $A$ then both will be mapped onto $1$ .Your help is appreciated.,"['functions', 'sequences-and-series', 'real-analysis']"
4701699,What would the floor of 0.99999.... [duplicate],"This question already has answers here : Is it true that $0.999999999\ldots=1$? (31 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I saw a post on r/learnmath about this, but I wanted to hear what you guys think. If I think in terms of sequences, I believe that it would be equal to zero, since 0.9999...9 (with n '9's) would always be less than 1 and greater than 0 for all natural n and hence, the floor would be zero (making the limit as n goes to infinity 0). However, if we first evaluate what's inside, then we'd get the floor of 1, which is just 1. I personally think the first 'method' yielding 0 is more consistent with the idea of sequences.","['limits', 'sequences-and-series']"
4701747,"Proving that $|A\cup G|= |A|+|G|$, where $A, G$ are disjoint subsets of $R$ and $G$ is open. $|, |$ denotes outer measure.","To prove this, I claim that if $G= \cup_{k=1}^n(a_k, b_k)$ ,i.e., finite union of disjoint open intervals. Then, $|A\cup G|\ge |A|+|G|$ . Proof : WLOG, suppose that 1) $a_k<b_k$ and that $a_1<a_2<...<a_n$ . 2) $a_i,b_i\not\in A$ . For 2), indeed if $X\subset \{a_1,b_1,\cdots, a_n,b_n\}, X\subset A$ , then $|A\cup G|=|(A-X)\cup G|, $ because $X$ is a finite set. Therefore, we could consider $A-X$ instead of $A$ . So the assumption 2) is valid. Take any sequence of open intervals $\{I_n\}$ that cover $A\cup G$ ,i.e., $A\cup G\subset \cup_n I_n$ . Define $K_n, L_n, J_n, M_n$ as follows: $J_n=(-\infty, a_1)\bigcap I_n; K_n= G\bigcap I_n, L_n= (\bigcup_{k=1}^{n-1}(b_k,a_{k+1}))\cap I_n, M_n=(b_n, \infty)\bigcap I_n$ . Clearly, $J_1,L_1,M_1, J_2, L_2, M_2,\cdots$ cover $A$ and $\{K_n\}$ covers $G$ . Since, $l(I_n)= (l(J_n)+l(L_n)+l(M_n))+l(K_n)$ , we have $$\sum l(I_n)= \sum(l(J_n)+l(L_n)+l(M_n))+\sum l(K_n)\ge|A|+|G|$$ Taking infimum over all such open intervals $\{I_j\}$ , we get $|A\bigcup G|\ge |A|+|G|.$ This proves the claim. By subadditivity, we even have $|A\bigcup G|= |A|+|G|$ . Now coming back to the statement in title, it is known that every open set in $R$ is a countable disjoint union of open intervals so there exist $\{I_n\}$ such that $G=\bigcup_n I_n$ . By subadditivity, we have $|A\cup G|\ge |A\bigcup (\cup_{n=1}^k I_n)$ for every $k\in \mathbb N$ . By the claim proven above, $|A\cup G|\ge |A|+\sum_{n=1}^kl(I_n)$ for every $k\in \mathbb N$ . Letting $k\to \infty$ , $|A\cup G|\ge |A|+\sum_{n=1}^\infty l(I_n)\ge|A|+|G|.$ By subadditivity, $|A\cup G|\le |A|+|G|$ so we get the equality. $$\tag*{$\square$}$$ My question: In theorem 2.62 of Axler's measure theory book, Axler first proves the above theorem in case $G= (a,b)$ and then concludes using induction on $m$ , the no. of open intervals (disjoint) whose union equals to $G$ . Then, the conclusion is made for any open set $G$ . But here, I have not used induction anywhere above so I want to know if induction is required here or not. Thanks.","['measure-theory', 'solution-verification', 'real-analysis']"
4701752,Is this reasonable closed form approximation for the value of $\zeta(3)$ using trigonometric functions just a coincidence?,"I have found this expression for an approximate value of $\zeta(3)$ : $$\zeta(3)≅\sqrt{\zeta(6)}\left(\frac{1}{\cos \left(\frac{\pi}{18}\right)}+\tan \left(\frac{\pi}{18}\right) \right)=
\frac{\pi^3}{3\sqrt{105}}\left(\frac{1}{\cos \left(\frac{\pi}{18}\right)}+\tan \left(\frac{\pi}{18}\right) \right)=1.2020...$$ using $\zeta(6)=\frac{\pi^6}{945}$ It is correct in the 5 digits of the approximate numerical value $1.2020...$ displayed above. This expression has no explicit infinite series or products or obviously tweaked constants.
Admittedly, the constant 18 was tweaked, but is does not look tweaked (I would prefer to say it is reverse engineered ). The expression was derived as follows: The starting formula involves the Liouville function $\lambda(n)$ . It is either $+1$ or $-1$ , depending on whether the number of factors in the prime factorization of $n$ is even or odd ( $\lambda(1)=+1$ , as $1$ has $0$ prime factors). $$\frac{\zeta(2s)}{\zeta(s)}=\sum_{n=1}^\infty \frac{\lambda(n)}{n^s}$$ where $s>1$ It can be derived from Euler's product formula for $\zeta(2s)$ . Now the terms of $\zeta(s)=\sum_{n=1}^\infty \frac{1}{n^s}$ can be separated on the basis of the value of $\lambda(n)$ . $$\zeta_+(s)=\frac 12 \left(\zeta(s)+\frac{\zeta(2s)}{\zeta(s)}\right)$$ is the sum of the terms having $\lambda(n)=+1$ . $$\zeta_-(s)=\frac 12 \left(\zeta(s)-\frac{\zeta(2s)}{\zeta(s)}\right)$$ is the (positive) sum of the (absolute value of the) terms having $\lambda(n)=-1$ . The following three relations hold: $$\zeta_+(s)+\zeta_-(s)=\zeta(s)$$ $$\zeta_+(s)-\zeta_-(s)=\frac{\zeta(2s)}{\zeta(s)}$$ and, multiplying the two relations above: $$\zeta_+(s)^2-\zeta_-(s)^2=\zeta(2s)=\left(\sqrt{\zeta(2s)}\right)^2$$ or $$\zeta_+(s)^2=\left(\sqrt{\zeta(2s)}\right)^2+\zeta_-(s)^2$$ The latter is the the Pythagorean equation for a right triangle with hypotenuse length $\zeta_+(s)$ and right angle sides lengths $\sqrt{\zeta(2s)}$ and $\zeta_-(s)$ . If we call the angle between the hypotenuse and the $\sqrt{\zeta(2s)}$ side $\alpha(s)$ , then $$\sin(\alpha(s))=\frac{\zeta_-(s)}{\zeta_+(s)}=\frac{\zeta(s)-{\zeta(2s)\over \zeta(s)}}{\zeta(s)+{\zeta(2s)\over \zeta(s)}}=\frac{1-{\zeta(2s)\over \zeta(s)^2}}{1+{\zeta(2s)\over \zeta(s)^2}}=q(s)$$ The rightmost fraction is also defined in the limit $\lim_{s\to 1}$ . So $\alpha(s)=\arcsin(q(s))$ , $\cos(\alpha(s))=\frac{\sqrt{\zeta(2s)}}{\zeta_+(s)}$ , $\tan(\alpha(s))=\frac{\zeta_-(s)}{\sqrt{\zeta(2s)}}$ , which gives $$\zeta(s)=\zeta_+(s)+\zeta_-(s)=\sqrt{\zeta(2s)}\left(\frac{1}{\cos(\alpha(s))}+\tan(\alpha(s))\right)$$ Plugging in s=3, with numerical values values for $\zeta(3)$ and $\zeta(6)$ from WolframAlpha , one finds $\alpha(3)=\arcsin(q(3))=$ 0.1745439... $=$ π/17.99886... in radians, or 10.0006...° in degrees. This is very close to $\frac{\pi}{18}$ radians i.e. $10°$ . This link has the WolframAlpha calculation for $\zeta(3)$ using $\alpha(3)=\frac{\pi}{18}$ . Is the reasonably close approximation of $\zeta(3)$ using the round value $\frac{\pi}{18}$ for $\alpha(3)$ just a coincidence, or is there an explanation for it?","['riemann-zeta', 'trigonometry', 'zeta-functions', 'approximation']"
4701781,"Is there a question that contains no numbers except $1$, whose answer is $\pi/7$?","Is there a question that contains no numbers, except possibly $1$ , whose answer is $\pi/7$ ? There are plenty of questions with no numbers except $1$ , whose answer is $\pi/n$ for small integer values of $n$ other than $7$ . For some reason, $\pi/7$ seems to be unattainable. Examples: Evaluate $\int_{-\infty}^\infty \frac{\sin{x}}{x}dx$ . Answer: $\color{red}{\pi}$ Drop a needle of length $1$ onto a floor with equally spaced parallel lines a distance $1$ apart. On average, how many times do you have to drop the needle until it crosses one of the lines? Answer : $\color{red}{\pi/2}$ What is the volume of a cone with unit base radius and unit height? Answer: $\color{red}{\pi/3}$ A regular $n$ -gon of side length $1$ encloses a regular $(n+1)$ -gon. The polygons are concentric, and for each $n$ the area of the inside polygon is maximized. What is the limit, as $n\to\infty$ , of the difference in their areas? Answer: $\color{red}{\pi/4}$ On a sphere of diameter $1$ , uniformly random points are chosen until there is a unique circle that passes through them. What is expected area of this (planar) circle? Answer : $\color{red}{\pi/5}$ A regular $n$ -gon of side length $1$ is inscribed in a circle. What is the limit, as $n\to\infty$ , of the difference between their areas? Answer: $\color{red}{\pi/6}$ $\color{red}{???}$ Like question 4, except the polygons do not have to be concentric. Answer: $\color{red}{\pi/8}$ Indirectly specifying a number is not allowed; for example, ""heptagon"" is equivalent to "" $7$ -gon"" and is thus not allowed. The question should not be obviously ad hoc . (For example: ""A unit circle is divided into $k$ equal parts, where $k$ is the product of the first even prime and the first odd prime, plus $1$ . What is the area of each part?"") I hope the spirit of my question is understood.","['circles', 'polygons', 'pi', 'trigonometry', 'soft-question']"
4701842,"How can the notation $\operatorname{atan2}(y, x)$ be used in scientific papers?","During a presentation for a PhD engineering topic I had shown the $\operatorname{atan2}(y, x)$ function within a slide as part of an equation. I received a remark by a professor that $\operatorname{atan2}(y, x)$ is not official and that I should not use it. Then I sent him the definition of $\operatorname{atan2}(y, x)$ as: and he said that I should've used that one instead of $\operatorname{atan2}(y, x).$ I'd like to ask if there is a correct mathematical approach towards using this function within papers and scientific presentations so I can avoid any further remarks. Using the complete mathematical definition seems rather cumbersome, is there any simplified approach? What would be the correct approach in such situations? In the end I'd like to ask if there is any guideline book that contains the official math notation which can only be used in science papers or presentations.","['notation', 'trigonometry', 'article-writing']"
4701895,Order of a pole formula / definition,"I have come up with the following formula for the order of a pole of some function $f(z)=\frac{g(z)}{h(z)}$ at a point $z = c$ . It involves first taking the multiplicative inverse (denoted by $1/f(z)$ to avoid getting mixed up with the inverse of the function), then finding the smallest $n$ such that the derivative is not equal to zero. $$
\operatorname{ord}(f, c)=\min \{ n \in \mathbb N:(1/f)^{(n)}(c)\neq 0 \}
$$ Does this equation hold? It works for my naive test of $f((z-2)^{-3}, 2)$ . However I haven't seen it written in my course notes. Is it also true that the order of a pole is the algebraic multiplicity wrt to the denominator?",['complex-analysis']
4701926,Integrating $ (x^2+2x+3)^{-2}$,"We wish to find $$J=\int \frac{dx}{(x^2+2x+3)^2}$$ Let $$I(a)=\int \frac{dx}{(x+1)^2+a^2}=\frac{1}{a} \tan^{-1} \frac{(x+1)}{a}$$ Differentiate w.r.t. $a$ both sides to get $$\int \frac{-2a dx}{((x+1)^2+a^2)^2}=\frac{-1}{a^2}\tan^{-1} \frac{(x+1)}{a}-\frac{(x+1)/a}{(x+1)^2+a^2}.$$ Hence, putting $a=\sqrt{2}$ and simplifying, we get $$J=\frac{1}{4\sqrt{2}}\tan^{-1}\frac{x+1}{\sqrt{2}}+\frac{x+1}{4(x^2+2x+3)}.$$ What are other methods of getting the $J$ integral?","['integration', 'indefinite-integrals', 'calculus', 'trigonometric-integrals']"
4701980,"Does the ""average"" of an integral converges to a point value of the integrand when the domain of integration shrinks to the point?","Let $B_n$ be the closed ball at origin with radius $1/n$ in $\mathbb{R}^m$ . Think of the normal Gaussian measure $\mu$ on $\mathbb{R}^m$ and consider \begin{equation}
\frac{1}{\mu(B_n) \sup_{x \in B_n}\lVert x \rVert^2} \int_{B_n} \lVert x \rVert^2 d\mu(x).
\end{equation} Then, I strongly suspect that this quantity converges to $1$ as $n \to \infty$ . However, I cannot justify my guess rigorously.. Could anyone please explain how it works?","['measure-theory', 'real-analysis']"
4701985,Which indicator variable best predicts a sampled value?,"There are two players in this cooperative game.  I will first describe the basic game and then the real problem. Consider first that player 1 is to sample a single value from a standard normal distribution. Player 2 knows the distribution player 1 samples from. After player 1 has seen the value they have sampled they can send player 2 a single bit of information. That is a single indicator value. Player 1 and 2 have agreed beforehand what the indicator value will be indicating. To start with, player 2's prior belief is that player 1 has sampled from a standard normal distribution (because that is what was agreed). After receiving the single bit of information, player 2 will have some posterior belief about the distribution of the sampled value.  The goal of both players is to agree a strategy to minimize the variance of player 2's posterior belief on average. For example, they could agree that player 1 will tell player 2 the sign of the sampled value. If player 1 samples a positive value then player 2's posterior belief will now be the half normal distribution with variance $1 - \frac{2}{\pi}$ . The same variance would occur if player 1 had sampled a negative value Could they have chosen a different strategy which would have given a lower variance on average? Now to the real problem. In the real version player 1 is to sample from some arbitrary continuous distribution with finite support.  They can still confer to agree a strategy beforehand and player 2 still knows the distribution that player 1 will sample from. Can they do any better than agreeing that player 1 will send to player 2 an indicator that indicates if the sampled value is above the median or not? Bounty question If we constrain the support to be $[0, 1]$ , what distribution gives the largest possible gap from the result you get from sending an indicator that optimally  minimizes the variance of player 2's posterior belief and the variance of player 2's belief if they were simply informed whether the sampled value is above the median or not?",['probability']
4701990,Best way to cut a pineapple ring?,"I like to prepare pineapples by first cutting it into rounds and then slicing off the skin with a roast beef slicer. This leaves me with a hexagon ""ring"" around a circular core: I then don't want to dirty another knife to core it, so I just reuse the beef slicer. But it's too big for precise cuts, so I have to cut all the way across the ring. Normally a cut a small square around the core, like this: The problem is this leaves me with unequal pieces— the first piece is much bigger and the fourth piece is much smaller. How can I make N cuts, each across the entire remaining core, so that I'm left with a set of pieces that are as close in area to each other as possible? I don't care about them having the same shape , just that they have the same area. N doesn't have to be 4, though I'd really prefer to keep it under 6 or so. Some waste around the core is ok.","['recreational-mathematics', 'geometry', 'applications']"
4701994,"Does this series have a closed form? If so, what is it?","$$
\sum_{n=0}^{x-1} \csc \left(\frac{(2 n+1)}{2 x} \pi\right)
$$ Where $x$ is a natural number. This series emerged in trying to find a closed form expression for the following series: $$
\sum_{n=0}^{\infty}(-1)^n \sum_{i=0}^{x-1} \frac{1}{2 xn+2i+1}
$$ Which is essentially the sum of reciprocals of odd numbers with $x$ consecutive pluses/minuses.
For example, with $x=1$ , we get $1-\frac{1}{3}+\frac{1}{5} \cdots=\frac{\pi}{4}$ With $x=2$ , we get $1+\frac{1}{3}-\frac{1}{5}-\frac{1}{7}+\frac{1}{9}+\frac{1}{11}+…= \frac{\pi}{2 \sqrt{2}}$ I derived that for any $x$ , the series is equivalent to $\frac{\pi}{4 x}$ times the aforementioned sum but was unable to find a simpler expression for the cosecant series.  Any help would be much appreciated.","['closed-form', 'sequences-and-series']"
4702051,Alternative definitions of regular conditional distribution.,"Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ , Durrett (Probability: Theory and Examples, $\S 4.1.3$ ) defines the regular conditional distribution of a random variable $X$ given a sub-sigma-algebra $\mathcal{G}$ by requiring that the map $\mu(\cdot,\omega)$ is a probability measure on the state space only for almost every $\omega\in\Omega$ (that is, $\mathbb{P}$ -a.s.); Billingsley (Probability and Measure, Theorem $33.3$ ) instead requires that it is so for every $\omega\in\Omega$ . Is there any difference in these two approaches, or are they equivalent in terms of the theories that derive from them? Thanks for any explanation you can provide.","['conditional-probability', 'probability-distributions', 'conditional-expectation', 'probability-theory', 'probability']"
4702063,What is the functional space of (general) Wavelet transform of squared-integrable functions?,"Let $\psi(x) = \exp(-x^2)$ . The (general) Wavelet transform $W: L^2(\mathbb R) \rightarrow X$ is  defined by $$Wf(a, b) = b^{-1/2}\int_{x\in \mathbb R} f(x) \psi\left( \frac{x-a}{b}\right)dx$$ for $(a, b) \in \mathbb R_+ \times \mathbb R$ . My question is: What is the functional space $X$ ? NOTE: Note that the function $\psi$ above is a Gaussian-like function, which is not exactly a wavelet function. In general, a function $\psi$ is called a wavelet function if it satisfies 1) $\psi$ has compact support or at least fast decaying at infinity and 2) $\int_{x \in \mathbb R} \psi(x) dx=0$ . The given function $\psi$ however only satisfies the first requirement. We therefore refer it as a ""general"" Wavelet transformation. To have a clue for our question. Let's us consider a particular case, when $b=b_0$ is constant, then $Wf$ is just a function of $a$ . In this case $$Wf \in X = C_0(\mathbb R).$$ Indeed, in this case, one can see that $Wf = b_0^{-1/2} f\star \psi(\cdot/b_0)$ , i.e. $W$ is a simply a convolution operator. A (well-known?) fundamental result states that: The convolution operator of two $L^2$ functions is $C_0$ . Hence $ Wf\in C_0(\mathbb R)$ . For the original question, numerical simulations also suggest that $X=C_0(\mathbb R_+\times \mathbb R)$ . It it correct?","['integration', 'operator-theory', 'convolution', 'functional-analysis', 'wavelets']"
4702068,Axiomatic definition of the complex numbers,"The real numbers may be defined axiomatically as a complete ordered field. This description characterises them up to isomorphism. Question: is there a similar way to define the field of complex numbers? In contrast to the difficulty of constructing $\mathbb R$ from $\mathbb Q$ , the construction of $\mathbb C$ from $\mathbb R$ is straightforward. For instance, $\mathbb C$ may be defined as $\mathbb R^2$ with the operations $(a,b)+(c,d)=(a+b,c+d)$ and $(a,b)\cdot(c,d)=(ac-bd,ad+bc)$ . Then, $i$ can be defined as $(0,1)$ , and we may identify $\mathbb R$ with the subfield $\{(a,0):a\in\mathbb R\}$ . Any model of $\mathbb C$ is isomorphic to this model. However, it feels somewhat artificial to describe a model of $\mathbb C$ as a field $F$ which is isomorphic to $\mathbb R^2$ with the aforementioned operations; it would be like describing a model of $\mathbb R$ as a field which is isomorphic to the collection of Dedekind cuts. What I'm looking for is a list of properties that are satisfied by a field $K$ if and only if $K$ is a model of $\mathbb C$ . This answer uses the Upward Lowenheim-Skolem theorem to conclude that no first-order theory will do the job. I have read that $\mathbb C$ can be described as the algebraic closure of $\mathbb R$ , but I don't have the requisite algebraic knowledge to understand this description; moreover, this description still references $\mathbb R$ , and so it still feels artificial in some sense. By contrast, the axiomatic description of $\mathbb R$ makes no reference to other fields such as $\mathbb Q$ . I'm looking for properties more along the lines of: $K$ is a field, there is an $x\in K$ such that $x^2=-1$ , etc.","['axioms', 'abstract-algebra', 'ring-isomorphism', 'complex-numbers']"
4702094,What is the mathematical principle that explains the shape of a curve that depicts the mean distance between points on a grid and every other point?,"I'm wondering if someone could give me a high-school-level explanation of the mathematical principle at work behind this. I was curious about this, so I designed an experiment to test it, and I'm wondering about the results. Suppose I have a bounded 2D coordinate grid that is a 100 x 100 square, so (0,1), (0,2) ... (99,99). Then, for each point, I calculate the mean Euclidean distance between that point and every other point. Then I sort the points by their mean distance to every other point in the grid and plot it. This is what a bar plot looks like for a 100x100 grid, with the x-axis representing unique coordinates and the y-axis representing the mean Euclidean distance between that coordinate and every other point in the grid: The plot shows a characteristic curve, where the middle section is essentially linear, the low end curves down somewhat, and the high end curves up more distinctly. What causes this? EDIT BASED ON @Eric's RESPONSE: If the corners of the grid are the cause of the accelerations at each end of the plot, then wouldn't it follow that the mean Euclidean distances between points in a cornerless grid (i.e., a circle) would not have these? I created a circle of radius 10 around the origin, generated random radii and angles to create random points within that circle, which I rounded to one place beyond the decimal. I randomly generated 500,000 such points, calculated their distances from one another, and plotted those results. Here's what it looks like: The extremes actually become more pronounced. Was I incorrect to infer that eliminating the corners would eliminate (or reduce) the bends at the end of the plot?","['matrices', 'matrix-calculus']"
4702100,An interesting integral $\int_0^{\infty}\{e^x\}-\frac {1}{2}dx$,"So I recently thought of this random integral, as I was thinking of integrals that have nice cancellation properties involving the fractional part. I believe I have an answer but my method is rather non-rigorous as I do not prove convergence, nor even know if it converges.
Here is what I did: Let $$J=\int_0^{\infty}\{e^x\}-\frac {1}{2}dx$$ Expressing it in terms of the floor function and noticing that for $x\in(\log(n),\log(n+1))$ , you have $\lfloor e^x \rfloor=n$ yields $J=\sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-\lfloor e^x\rfloor-\frac {1}{2} )  dx=
\sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-n-\frac {1}{2} )  dx
\\= \sum_{n=1}^\infty (1+(n+\frac{1}{2})\log(\frac{n}{n+1}))$ This can be expressed as $$J= \lim_{N\to\infty}\sum_{n=1}^N((n-\frac{1}{2})\log(n)-(n+\frac{1}{2})\log(n+1)+\log(n)+1)$$ The first two terms are telescoping which gives $J=\lim_{N\to\infty}(\log(N!)+N-(N+\frac{1}{2})\log(N+1))$ $ =\lim_{N\to\infty}(N\log(N)+\frac{1}{2}\log(2\pi N)-(N+\frac{1}{2})\log(N+1))$ (using Stirling's formula) $ = \lim_{N\to\infty}((N+\frac{1}{2})\log(N)-(N+\frac{1}{2})\log(N+1))+\frac{1}{2}\log(2 \pi)=\frac{1}{2}\log(2 \pi)+\log(\frac{1}{e})=\frac{1}{2}\log(2 \pi)-1.$ It would be much appreciated if anyone could rectify any mistakes I made, make this rigorous and/or correct me on why the integral diverges.","['integration', 'ceiling-and-floor-functions', 'definite-integrals', 'calculus', 'sequences-and-series']"
4702133,I found a really interesting sum involving the floor function. Can someone help explain why it works the way it does?,"NOTE: This is all from Desmos and manually plugging in numbers and solving to find lines. There has been no proof done of what I'm talking about, and I don't really know the math necessary to do such a proof. Here's the sum: $\lim_{p\to\infty}\sum_{n=1}^{p}\frac{\lfloor nx \rfloor}{p^{2}}=\frac{x}{2}$ . I'd really like to understand why this works. For powers of $p$ greater than $2$ , this goes to $0$ and powers of $p$ less than $2$ it goes to $\infty$ , but it's not clear at all to me that that would be how it converges. Can someone help go through a proof of this or at least explain a bit of why it would go to $\frac{x}{2}$ in one place and collapse/diverge everywhere else?","['limits', 'convergence-divergence', 'ceiling-and-floor-functions', 'sequences-and-series']"
4702294,"Every finite rank operator on a Hilbert space can be written $\sum_{j=1}^n s_j|u_j\rangle\langle v_j|$ with $\{u_j\},\{v_j\}$ orthonormal","Let $T$ be a finite rank operator on a Hilbert space $H$ . Then there exist orthonormal sets $\{u_j\},\{v_j\}$ and positive scalars $\{s_j\}$ , with $j=1,2,\ldots,n$ , such that $$T=\sum\limits_{j=1}^n s_j|u_j\rangle\langle v_j|.$$ Here $|u\rangle \langle v|(x)=\langle v,x\rangle u$ Given finite rank operator $T$ , $T$ defines a linear correspondence between the finite dimensional spaces $\text{Ker}(T)^\perp$ and $\text{Ran}(T)$ . Let $\{u_1,u_2,\ldots, u_n\}$ be an orthonormal basis for $\text{Ran}(T)$ . Then there is $\{v_1,\ldots,v_j\}\in\text{Ker}(T)^\perp$ such that $Tv_j=u_j$ for $j=1,\ldots,n$ . As $T(v_j/\lVert v_j\rVert)=(1/\lVert v_j\rVert) u_j$ , we may assume without loss of generality that $Tv_j=s_ju_j$ and $\lVert v_j\rVert=1$ . But this $\{v_j\}$ may not be an orthonormal set. If it is, we are done. If we apply Gram Schmidt orthogonalization on $\{v_j\}$ , the $\{u_j\}$ will be changed and may not be orthonormal set any more. Can anyone suggest a way out? Thanks in advance for your help.","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4702314,Is the Local Average of a Continuous Multivariable Function Differentiable?,"Suppose we have a continuous $f:\mathbb{R}\to\mathbb{R}$ . It is an immediate corollary of the Leibniz integral rule that $f^*:x\mapsto\int_{x-\frac{1}{2}}^{x+\frac{1}{2}}f(t)\ dt$ , the ""local"" average of $f$ on the interval of length one centred at $x$ , is a continuously differentiable function in $x$ . I was wondering how far we can get extending this result into higher dimensions. Specifically, let $f:\mathbb{R}^n\to\mathbb{R}^m$ be continuous. For $x\in\mathbb{R}^n$ , let $B_r(x)$ be the closed ball of radius $r$ around $x$ . Then, is the function $f^*:x\to\int_{B_{\frac{1}{2}}(x)}f(t)\ dt$ , where the integral is the standard integral for vector-valued functions, a continuously differentiable function? Could we say, integrate over a square instead of a circle?","['real-analysis', 'continuity', 'multivariable-calculus', 'derivatives', 'leibniz-integral-rule']"
4702389,From any point $P$ on the line $5x+4y=20$ tangents are drawn to the circle $x^2+y^2=4$ then find the locus of the circum-centre of the $\triangle PQR$,"From any point $P$ on the line $5x+4y=20$ tangents are drawn to the circle $x^2+y^2=4$ meeting at $Q$ and $R$ , then find the locus of the circum-centre of $\triangle PQR$ . My approach is as follow, if the tangent drawn from a point on the line $5x+4y=20$ to the curve meets at the point $Q$ and $R$ . Then the line joining the chord of contact is $\alpha x+\beta y=4$ .
As $\alpha$ and $\beta$ lies on the line $5x+4y=20$ , hence $5\alpha+4\beta=20$ on comparing we get $x=1$ & $y=\frac{4}{5}$ hence the line or chord of contact is $y=mx+\frac{4}{5}-m$ , I am not able to proceed from here.","['analytic-geometry', 'circles', 'geometry']"
4702434,"Differentiability of $f(x,y) = |xy|$","I know there is also a questions concerning this task. However I have some questions about my approaches to solve this taks. Thanks for your help in advance! We consider the function $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ , defined by $$f(x, y):=|x y|$$ a) Determine all points $u=(u_{1}, u_{2}) \in \mathbb{R}^{2}$ at which the function $f$ has partial derivatives $\frac{\partial f}{\partial x}(u)$ and $\frac{\partial f}{\partial y}(u)$ . b) Determine all points $u=(u_{1}, u_{2}) \in \mathbb{R}^{2}$ at which the function $f$ is differentiable. Question to a)\
When I compute the partial derivatives for $\frac{\partial f}{\partial x}(u)$ and $\frac{\partial f}{\partial y}(u)$ I have $$\frac{\partial f}{\partial x}(u) = \frac{|y|x}{|x|}$$ and $$\frac{\partial f}{\partial y}(u) = \frac{|x|y}{|y|}$$ We see that we can't have $(0,y) \in \mathbb{R}^{2}$ for $\frac{\partial f}{\partial x}(u)$ and $(x,0)\in \mathbb{R}^{2}$ for $\frac{\partial f}{\partial y}(u)$ . And the partial derivatives also don't exist in $(0,0) \in \mathbb{R}^{2}$ . I feel like this doesn't solve the task correctly. Is there anything I'm missing? Question to b)\
b) In order to find differentiability for $f(x,y)$ in $u=(u_1,u_2)$ we have to find the points where the partial derivatives exist and are continuous. So we have to deal with our special points $(0,0),(x,0),(0,y) \in \mathbb{R}^{2}$ Let $(x,y)=(0,0)$ , then $$\begin{aligned}\lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)} \frac{f(h_{1}, h_{2})-f(0,0)}{\|h\|}&=\lim \limits_{(h_{1},h_{2})\rightarrow(0,0)} \frac{|h_{1}| \cdot|h_{2}|}{\sqrt{h_{1}^{2}+h_{2}^{2}}} \\ &\leq \lim \limits_{(h_{1},h_{2}) \rightarrow(0,0)}|h_{1}| \cdot|h_{2}| =0\end{aligned}$$ So $f(x,y)$ is differentiable in $u=(0,0)$ . Let $(x,y)= (0,y)$ , then $$\lim \limits_{h \rightarrow 0^{+}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||y|}{h}=|y|$$ but $$\lim \limits_{h \rightarrow 0^{-}} \frac{f(h, y)-f(0, y)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||y|}{h}=-|y|$$ So $f$ is not differentiable in $(0,y) \in \mathbb{R}^{2}$ . Let $(x, y)=(x, 0)$ , then $$\lim \limits_{h \rightarrow 0^{+}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{+}} \frac{|h||x|}{h}=|x|$$ but $$\lim \limits_{h \rightarrow 0^{-}} \frac{f(x, h)-f(x, 0)}{h}=\lim \limits_{h \rightarrow 0^{-}} \frac{|h||x|}{h}=-|x|$$ So $f$ is not differentiable in $(x, 0) \in \mathbb{R}^{2}$ . So $f$ is $\forall(x, y) \in \mathbb{R}^{2} \backslash((x, 0),(0, y))$ differentiable. Let me know what you think, thanks for your help!","['partial-derivative', 'solution-verification', 'derivatives', 'analysis']"
4702452,Definitions of relatively compact,"Let $X$ be a topological space and let $A \subseteq X$ . Definition 1. $A$ is relatively compact in $X$ if the closure of $A$ in $X$ is a compact subspace. The above is standard, but there are some prima facie weaker conditions that are sometimes useful.
For example: Definition 2 : $A$ is relatively compact in $X$ if there exists a compact $K$ such that $A \subseteq K$ and $K$ is contained in the closure of $A$ in $X$ . Definition 2'. $A$ is relatively compact in $X$ if there exists a compact $K \subseteq X$ with $A \subseteq K$ . We could even not bother having an actual compact subspace: Definition 3. $A$ is relatively compact in $X$ if every open cover of $X$ has a finite subset that covers $A$ , i.e. given open $U_i \subseteq X$ ( $i \in I$ ) such that $X = \bigcup_{i \in I} U_i$ , there is a finite $I' \subseteq I$ such that $A \subseteq \bigcup_{i \in I'} U_i$ . Clearly, definition 1 implies definition 2, and definition 2 implies definition 3.
Furthermore, definition 2 and 2' are equivalent because closed subspaces of compact spaces are compact.
Compact subspaces of Hausdorff spaces are closed, so if $X$ is Hausdorff, then definitions 1 and 2 are equivalent. Question. Does definition 3 imply definition 1 in general?
What if $X$ is Hausdorff, or $A$ is open? Definition 3 is closely related to exponentiability: $X$ is exponentiable if and only if for every open $V \subseteq X$ and every $x \in V$ there is an open $U \subseteq V$ such that $x \in U$ and $U$ is relatively compact in $V$ in the sense of definition 3.
When $X$ is Hausdorff, $X$ is exponentiable if and only if $X$ is locally compact.","['general-topology', 'compactness']"
4702521,On the topic of Trinomial Expansion,"So I was looking at the Wikipedia of the binomial theorem when I read this: Multi-binomial theorem When working in more than two dimensions, it is often useful to deal with products of binomial expansions. By the binomial theorem this is equal to $$(x_1+y_1)^{n_1}\cdots(x_d+y_d)^{n_d}=\sum_{k_1=0}^{n_1}\cdots\sum_{k_d=0}^{n_d}\binom{n_1}{k_1}x_1^{k_1}y_1^{n_1-k_1}\cdots\binom{n_d}{k_d}x_d^{k_d}y_d^{n_d-k_d}$$ Now say I wanted to write out $(a+b+c)^n$ in this way. Would I use this theorem to write it out as $$\sum_{r=0}^n\sum_{s=0}^r\dfrac{n!a^{n-r}b^{r-s}c^s}{s!(n-r)!(r-s)!}$$ which would be gotten from $$(a+b+c)^n$$ $$=(a+d)^n,d=b+c$$ $$=\sum_{r=0}^n\binom nra^{n-r}d^r$$ $$=\sum_{r=0}^n\binom nra^{n-r}(b+c)^r$$ $$=\sum_{r=0}^n\binom nra^{n-r}\sum_{s=0}^r\binom rsb^{r-s}c^s$$ $$=\sum_{r=0}^n\sum_{s=0}^r\dfrac{n!a^{n-r}b^{r-s}c^s}{s!(n-r)!(r-s)!}$$ or how would I write out trinomial expansion? To clarify This question is different from my question that I am asking because it is asking more for a conjecture of the relation between the sum of the $2m$ th powers of the diagonals of a regular $n$ -gon and $n$ , as mentioned by the OP of the question in a comment on this answer to the question.",['algebra-precalculus']
4702572,Solving differential equation $\frac{\sqrt{x}dx+\sqrt{y}dy}{\sqrt{x}dx-\sqrt{y}dy}=\sqrt{\frac{y^3}{x^3}}$,I was trying to solve the following diffrential equation. $$\frac{\sqrt{x}dx+\sqrt{y}dy}{\sqrt{x}dx-\sqrt{y}dy}=\sqrt{\frac{y^3}{x^3}}$$ My try: First I applied C and D to get (componendo and dividendo) $$\frac{\sqrt{x}dx}{\sqrt{y}dy}=\frac{y^{3/2}+x^{3/2}}{y^{3/2}-x^{3/2}}$$ On simplifying $$\sqrt{xy}(ydx-xdy)=x^2dx+y^2dy$$ Now by seeing $(ydx-xdy)$ I got hint of forming $d(\frac{x}{y})$ but for it I need to divide by $y^2$ which will disturb other terms. I am stuck at this step. Please provide some hint to proceed. Thanks,"['calculus', 'ordinary-differential-equations']"
4702585,Alternating sign Vandermonde convolution like quantity,"I am interested in proving that the numbers $A_{n,m}$ are non-negative: $$A_{n,m}:= \sum_{n_1=0}^{2n-1} \sum_{m_1=0}^{2m-1} (-1)^{m_1+n_1} \binom{2n-1-n_1}{2m-1-m_1} \binom{n_1}{m_1}$$ where $m,n$ are integers with $1\leq m \leq n$ . I have calculated the first few and this seems to be the case. I can also write closed formulas for $A_{n,n}, A_{n,1}$ , but I am not sure what to do for the general quantity. Does anyone have any insight on this? Thanks! Edit: my question is if $A_{n,m} \geq 0$ for all integers $1\leq m \leq n$ .
(I can only show this is true for m=1 and m=n)","['summation', 'binomial-coefficients', 'combinatorics', 'convolution']"
4702607,Solving a particulary tricky PDE,"I'm currently working on a problem that involves PDEs. I rarely work with them and have never formally learned any solution strategies other than the one I'm going to describe to you. However I somehow fail at the step of comparing the coeffictients: Solving the PDE given by: $$\partial_y u(x,y) = -x^2-y$$ and $$\partial_x u(x,y) = 2x - y$$ Therefore u(x,y) is given by: $$u(x,y) =-x^2y - \frac{1}{2} y^2 + g(x)$$ and by: $$u(x,y) = x^2 - yx + h(y)$$ Normally I'd just compare the results and see the answer, but currently I can't see anything :/","['linear-pde', 'partial-differential-equations', 'analysis', 'real-analysis']"
4702632,Ramification Index 2 in Galois closure of field with squarefree discriminant,"This is about Exercise 7 from here . Let $K$ be a number field of degree $n$ with Galois group $S_n$ whose discriminant $D$ is squarefree. Prove that the Galois closure of $K$ is unramified over all finite places of $\Bbb Q(\sqrt D)$ . Let $L$ be the Galois closure. We have to show that for all rational primes $p\mid D$ , the ramification index of $p$ in $L$ is $2$ . There is a hint to the exercise suggesting to first prove that there is exactly one ramified prime above $p$ in $K$ and then deduce that the inertia group of $p$ in $L$ has order $2$ . The first part is not difficult, it follows from the usual bounds on the valuation of the different, so we have $(p)={\frak p_1^2 p_2\cdots p}_r$ for distinct primes ${\frak p_1,\dots,p}_r$ of $K$ . Here is my argument for the second part. Let $K=\Bbb Q(\alpha)$ for some algebraic integer $\alpha$ and let $f$ be its minimal polynomial. Let $\frak P$ be a prime of $L$ lying above $p$ . Consider the completion $L_{\frak P}$ . It is the splitting field of $f$ over $\Bbb Q_p$ . Note also that over $\Bbb Q_p$ $f$ factors as $f=f_1\cdots f_r$ corresponding to the primes ${\frak p_1,\dots\frak p}_r$ . For $2\leq i\leq r$ , ${\frak p}_i$ is unramified over $p$ , so $f_i$ generates an unramified extension of $\Bbb Q_p$ . Also $f_1$ must generate a degree $2$ totally ramified extension. Now $L_{\frak P}$ is the composite of all the splitting fields of the $f_i$ and from this it is clear that $e_{L_{\frak P}/\Bbb Q_p}=2$ . Question : Is this correct? I didn't use the hypothesis that $\operatorname{ Gal}(L/\Bbb Q)=S_n$ . Also I am wondering if there is a way that avoids the completion and proves this directly, e.g. using the inertia group as the hint suggested. I was thinking that perhaps one could show that because the way $p$ splits in $K$ , at most two of the roots of $f$ become equal mod $\frak P$ (or at least one could choose $f$ like that). Then it would be clear that the only non-trivial element of the inertia subgroup $I_{\frak P}$ could be the transposition swapping said roots (like in this answer ).","['number-theory', 'algebraic-number-theory', 'ramification']"
4702644,Prove that $\tan\left(\frac{x}2 - \frac{\pi}8\right) = \frac{\sqrt{2} - \cos(x) - \sin(x)}{\sin(x) - \cos(x)}$,"I am studying trigonometry, specifically the following formulas: $$\sin(a+b), \cos(a+b), \sin(2x), \cos(2x), \tan(a+b), \tan(2x), \tan^2(x),$$ and expressing $\sin(x), \cos(x), \tan(x)$ in terms of $\tan(x/2)$ . Now, I have a specific question: Can anyone provide me with a hint to prove the identity: $$\tan\left(\frac{x}{2} - \frac{\pi}{8}\right) = \frac{\sqrt{2} - \cos(x) - \sin(x)}{\sin(x) - \cos(x)}$$ This is what I tried: $$\tan\left(\frac{x}{2} - \frac{\pi}{8}\right) = \frac{\tan\left(\frac{x}{2}\right) - \tan\left(\frac{\pi}{8}\right)}{1 + \tan\left(\frac{x}{2}\right) \tan\left(\frac{\pi}{8}\right)}$$ Then, I substituted $\tan\left(\frac{\pi}{8}\right) = \frac{2}{\sqrt{2}} - 1$ .
but I don't know what to do Any help or guidance would be greatly appreciated. Thank you!","['trigonometry', 'functions']"
4702662,Solving the differential equation $y’-\frac{2x-1}{x^2}y=1$,I’m having problems solving this equation and I was hoping to find someone here who can help me. $$y’-\frac{2x-1}{x^2}y=1$$ Here is my attempt:,['ordinary-differential-equations']
4702669,Find the domain of this function involving fractional part and greatest integer function,"Consider a function $$y=\log_{10}\{\log_{10}\lfloor\log_{10}(\log_{10}x)\rfloor\}$$ where $\{x\}$ denotes the fractional part of $x$ and $\lfloor x\rfloor$ denotes the greatest integer less than or equal to $x$ . Find its domain. I have tried traditional ways of solving this. But I'm not able to handle the fractional part function and the greatest integer function. Also, in the domain, we have to exclude values that can be written as continuous powers of $10$ . I have tried Wolfram Alpha and I got one solution. It is $x=23^{{272}^{56}}$ . For this, $y$ equals $-1$ . Any help is greatly appreciated. Note : By ""continuous powers of $10$ "", I mean expressions like $10^{{10}^{{10}^{10}}}
$ .","['fractional-part', 'algebra-precalculus', 'logarithms']"
4702670,Calculate flow integral using Stokes's theorem,"Calculate the flow integral $$\iint_{Y} \text{curl} (\vec{F}) · \hat{N} dS$$ where $Y$ is part of the sphere $x^2 + y^2 + (z − 2)^2 = 8$ that lies
above the $xy$ -plane and $\hat{N}$ is the outward unit normal vector on $Y$ and $$F(x, y , z) = (y^2 \cos(xz), x^3e^{yz} , −e^{−xyz}).$$ I got the answer $48\pi$ but the correct answer is $12\pi$ . What am I doing wrong?","['integration', 'multivariable-calculus', 'line-integrals', 'stokes-theorem']"
4702680,Equimeasurable functions of $p(\cdot) :[0;1] \to [1;\infty)$ and $p(\cdot\cdot) : [0;1]^2 \to [1; \infty)$.,"Let $p(\cdot) :[0;1] \to [1;\infty)$ be a measurable function. Define $p^{*}(x)$ as the decreasing rearrangement of the measurable function $p(\cdot)$ . Hence we  know that $p(\cdot)$ and $ p^{*}(x)$ are equimeasurable to each other. ( Since $p(\cdot)$ is positive-valued). Let $p(\cdot\cdot) : [0;1]^2 \to [1; \infty)$ be a measurable function. Define $p^{*}(x,y)$ as the decreasing rearrangement of the measurable function $p(\cdot\cdot)$ . Thereby we know that $p(\cdot\cdot)$ and $p^{*}(x,y)$ are equimeasurable to each other.(Since $p(\cdot\cdot)$ is positive-valued). Define $\bar p(x,y) = p^{*}(x) + p^{*}(y)$ . There is the question: Will $\bar p(x,y)$ be equimeasurable to $p(\cdot\cdot)$ ? I'm trying to figure out something from these definitions: Note that : $(1)$ Nonnegative functions $f$ and $g$ are called equimeasurable if $\eta_f = \eta_g$ , i.e., $$m\{f \gt y \} = m\{g \gt y \} .$$ $(2)$ Functions $|f|$ and $f^{*}$ are equimeasurable. Any help would be appreciated.","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'functional-analysis']"
4702705,the nature of conditional probability,"I'm reading Jeffrey's ""Subjective Probability"" and was intrigued by the following passage: ""The quotient rule is often called the definition of conditional probability. It is not. If it were, we could never be in the position we are often in, of making a conditional judgment--say, about how a coin that may or may not be tossed will land--without attributing some particular positive value to the condition that $pr$ (head|tossed) = 1/2 even though $\frac{pr(\text{head} \ \land \ \text{tossed})}{pr(\text{tossed})} = \frac{\textit{undefined}}{\textit{undefined}}$ . [...] The quotient rule merely restates the product rule; and the product rule is no definition but an essential principle relating two distinct sorts of probability."" (Jeffrey 2004, p.14) The product rule is this: $pr(H \ \land D) = pr(H|D) pr(D)$ The quotient rule is this: $pr(H|D) = \frac{pr(H \ \land \ D)}{pr(D)}$ , provided $pr(D) > 0$ . $pr(H|D)$ of course means the conditional probability of $H$ given $D$ . Further useful information is that Jeffrey uses a Dutch book argument to motivate the product rule (as he also does in motivating the special disjunction axiom). So, what is he saying: Is the product rule a further axiom of the probability calculus? If not, how is it a theorem? I got confused multiple times in people defining conditional probability out of the blue, not seeing how its definition is implicit in the axioms. In short, what does Jeffrey mean when he says that the product rule is an ""essential principle""? How exactly does it relate to the axioms?","['conditional-probability', 'statistics', 'bayesian', 'probability']"
4702713,Integrability of mean value function,"I'm consider a one-dimensional function $u\in L^1(I)$ where $I$ is a bounded open interval. I first extend $u$ to zero outside $I$ . Then I take a small parameter $\epsilon>0$ and define $$u_{\epsilon}(t)=\frac{\int_{t-\epsilon}^{t}u(s)ds}{\epsilon}.$$ By the integrability of $u$ , I know that every point $t\in I$ is a Lebesgue point so $\lim\limits_{\epsilon\to0}u_{\epsilon}(t)=u(t)$ for a.e. $t\in I$ . But I would like to know if $u_{\epsilon}$ can keep the integrability of $u$ i.e. whether $u_{\epsilon}\in L^1(I)$ ? If it can keep this, can we pass to higher dimension? Add what I tried: to show the integrability is actually to calculate that \begin{align*}
\int_{I}|u_{\epsilon}(t)|dt&\leq\frac{1}{\epsilon}\int_{I}\int_{t-\epsilon}^{t}|u(s)|dsdt\\
&\leq\frac{1}{\epsilon}\int_{I}\int_{I}|u(s)|dsdt\\
&=\frac{|I|}{\epsilon}\cdot\lVert u\rVert_{1}<\infty.
\end{align*} So I guess it can keep the integrability. Does it make sense?","['analysis', 'real-analysis']"
4702748,"AM-GM Inequality. Let a, b, c be positive real numbers. Prove that $ \frac {a+b+c}{3} \cdot (a^2+b^2+c^2) \ge a^2b + b^2c + c^2a$.","Prove that $$ \frac {a+b+c}{3} \cdot \frac{a^2+b^2+c^2}{3} \ge \frac{a^2b + b^2c + c^2a}{3}$$ . given that a,b,c are positive real numbers.
Solve only using AM-GM . So far I have tried expanding LHS, and simplifying to: $$a^3+b^3+c^3+ab^2+bc^2+ca^2 \ge 2(a^2b + b^2c + c^2a)$$ However this has not lead me anywhere.
Source: Basics Of Inequality","['algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
4702756,How to solve $a \cdot x^{2} + b \cdot x + c + \exp\left( d \cdot x + e \right) = 0$ for $x$ where $x \in \mathbb{C} \cup \{ \hat{\infty} \}$?,"General Question: How to solve $a \cdot x^{2} + b \cdot x + c + \exp\left( d \cdot x + e \right) = 0$ for $x$ where $x \in \mathbb{C} \cup \left\{ \hat{\infty} \right\} \wedge \left\{ a,\, b,\, c,\, d,\, e \right\} \in \mathbb{C}$ (note: here $e$ is not euler's constant)? Backgrund: When I sometimes calculate with ODEs, I sometimes come across equations of this form. So I'm wondering what a general solution to this would look like? My Trys Since I've often encountered such equations, I've tried a few things accordingly. I'll just name the best (for non-trivial stuff): If $a = 0$ $$
\begin{align*}
0 \cdot x^{2} + b \cdot x + c + \exp\left( d \cdot x + e \right) &= 0\\
b \cdot x + c + \exp\left( d \cdot x + e \right) &= 0\\
\exp\left( d \cdot x + e \right) &= -c - x \cdot b\\
\end{align*}
$$ $$
\begin{align*}
-x \cdot b - c &= \exp\left( e - \frac{c \cdot d}{b} + x \cdot d + \frac{c \cdot d}{b} \right)\\
-x \cdot b - c &= \exp\left( e - \frac{c \cdot d}{b} + x \cdot d + \frac{c \cdot d}{b} \right)\\
-x \cdot b - c &= \exp\left( e - \frac{c \cdot d}{b} \right) \cdot \exp\left( x \cdot d + \frac{c \cdot d}{b} \right)\\
\end{align*}
$$ $$
\begin{align*}
-x \cdot b \cdot \exp\left( -x \cdot d - \frac{c \cdot d}{b} \right) - c \cdot \exp\left( -x \cdot d - \frac{c \cdot d}{b} \right) &= \exp\left( e - \frac{c \cdot d}{b} \right)\\
-x \cdot d \cdot \exp\left( -x \cdot d - \frac{c \cdot d}{b} \right) - \frac{c \cdot d}{b} \cdot \exp\left( -x \cdot d - \frac{c \cdot d}{b} \right) &= \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right)\\
\left( -x \cdot d - \frac{c \cdot d}{b} \right) \cdot \exp\left( -x \cdot d - \frac{c \cdot d}{b} \right) &= \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right)\\
\end{align*}
$$ $$
\begin{align*}
-x_{k} \cdot d - \frac{c \cdot d}{b} &= W_{k}\left( \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right) \right)\\
x_{k} + \frac{c}{b} &= -\frac{1}{d} \cdot W_{k}\left( \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right) \right)\\
x_{k} &= -\frac{1}{d} \cdot W_{k}\left( \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right) \right) - \frac{c}{b}\\
\end{align*}
$$ $$\fbox{$x_{k} = -\frac{1}{d} \cdot W_{k}\left( \frac{d}{b} \cdot \exp\left( e - \frac{c \cdot d}{b} \right) \right) - \frac{c}{b}$}$$ Where $W_{k}$ is the Lambert W Function... If $a \ne 0$ Completing ... $$
\begin{align*}
a \cdot x^{2} + b \cdot x + c + \exp\left( d \cdot x + e \right) &= 0\\
a \cdot x^{2} + b \cdot x + \exp\left( d \cdot x + e \right) &= -c\\
a \cdot x^{2} + b \cdot x + \frac{b^{2}}{4} + \exp\left( d \cdot x + e \right) &= -c + \frac{b^{2}}{4}\\
a \cdot x^{2} + b \cdot x + \frac{b^{2}}{4} + \exp\left( d \cdot x + e \right) &= -c + \frac{b^{2}}{4}\\
x^{2} + \frac{b}{a} \cdot x + \frac{b^{2}}{4 \cdot a^{2}} + \frac{1}{a} \cdot \exp\left( d \cdot x + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}}\\
\left(x + \frac{b}{2 \cdot a} \right)^{2} + \frac{1}{a} \cdot \exp\left( d \cdot x + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}}\\
\end{align*}
$$ Fail: I can't think of anything useful to add. Substitution + Compliting ... $$
\begin{align*}
\left(\underbrace{x + \frac{b}{2 \cdot a}}_{= u} \right)^{2} + \frac{1}{a} \cdot \exp\left( d \cdot x + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}}\\
u^{2} + \frac{1}{a} \cdot \exp\left( d \cdot \left( u - \frac{b}{2 \cdot a} \right) + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}}\\
u^{2} + \frac{1}{a} \cdot \exp\left( d \cdot u - \frac{d \cdot b}{2 \cdot a} + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}}\\
\frac{1}{a} \cdot \exp\left( d \cdot u - \frac{d \cdot b}{2 \cdot a} + e \right) &= -\frac{c}{a} + \frac{b^{2}}{4 \cdot a^{2}} - u^{2}\\
\exp\left( d \cdot u - \frac{d \cdot b}{2 \cdot a} + e \right) &= -c + \frac{b^{2}}{4 \cdot a} - a \cdot u^{2}\\
\exp\left( d \cdot u + f - \frac{d \cdot b}{2 \cdot a} + e - f \right) &= -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2}\\
\exp\left( d \cdot u + f \right) \cdot \exp\left( -\frac{d \cdot b}{2 \cdot a} + e - f \right) &= -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2}\\
\exp\left( -\frac{d \cdot b}{2 \cdot a} + e - f \right) &= \left( -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2} \right) \cdot \exp\left( -d \cdot u - f \right)\\
\end{align*}
$$ Fail: Nothing to simplify... Despair $$
\begin{align*}
\exp\left( -\frac{d \cdot b}{2 \cdot a} + e - f \right) &= \left( -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2} \right) \cdot \exp\left( -d \cdot u - f \right)\\
-\frac{d \cdot b}{2 \cdot a} + e - f &= \ln\left( \left( -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2} \right) \cdot \exp\left( -d \cdot u - f \right) \right)\\
-\frac{d \cdot b}{2 \cdot a} + e - f &= \ln\left( -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2} \right) + \ln\left( \exp\left( -d \cdot u - f \right) \right)\\
-\frac{d \cdot b}{2 \cdot a} + e - f &= \ln\left( -c + \frac{b^{2}}{4 \cdot a} -  a \cdot u^{2} \right) - d \cdot u - f \\
\end{align*}
$$ So there might be a solution in terms of Wright Lambert W functions...","['complex-analysis', 'algebra-precalculus', 'lambert-w', 'complex-numbers']"
4702777,"Compute $\int_0^\infty \frac{\sin(ax)}{b^2+x^2}~dx$, where $a>0, b>0$","If the $\sin(ax)$ function is replaced by $\cos (ax)$ , it's well known and can be solved by many ways. But if we want to solve the sine version integral, i.e. $$I=\int_0^\infty \frac{\sin(ax)}{b^2+x^2}~dx$$ It seems not that trivial as thought. Let $x=bt, k=ab,$ $$I=\frac{1}b\int_0^\infty \frac{\sin(kt)}{1+t^2}~dt,~~~~F=F(k)=\int_0^\infty \frac{\sin(kt)}{1+t^2}~dt$$ Now, we compute $F$ : $$F'=\int_0^\infty \frac{t\cos(kt)}{1+t^2}~dt~~ \overset{\theta=kt}{\longrightarrow} ~~F'=\int_0^\infty \frac{\theta\cos(\theta)}{k^2+\theta^2}d\theta$$ Take second derivative $$F''=-\int_0^\infty \frac{2k\theta\cos(\theta)}{(k^2+\theta^2)^2}d\theta=\int_0^\infty k\cos(\theta)d\left(\frac{1}{k^2+\theta^2}\right)$$ Integration by part $$F''=-\frac{1}k+\int_0^\infty \frac{k\sin(\theta)}{k^2+\theta^2}d\theta~~ \overset{\theta=kt}{\longrightarrow} ~~F''=-\frac{1}k+\int_0^\infty \frac{\sin(kt)}{1+t^2}dt$$ Therefore, $$F''(k)=-\frac{1}k+F(k)$$ Solve this 2nd order inhomogeneous differential equation and we get $$F(k)=c_1 e^k+c_2 e^{-k}+\text{P.V}\left(\frac{1}2e^k\int_k^\infty \frac{e^{-t}}{t}dt+\frac{1}2e^{-k}\int^k_{-\infty} \frac{e^{t}}{t}dt\right)$$ Define: $\displaystyle\text{Ei}(z)=\text{P.V}\left(-\int_{-z}^\infty \frac{e^{-t}}{t}dt\right)$ , we get $$F(k)=c_1 e^k+c_2 e^{-k}+\frac{1}2\left(-e^k\text{Ei}(-k)+e^{-k}\text{Ei}(k)\right)$$ From the integral $\displaystyle F(k)=\int_0^\infty \frac{\sin(kt)}{1+t^2}~dt$ we can see $F(0)=0$ and $\displaystyle\lim_{k\to\infty} F(k)$ is bounded, hence, $c_1=c_2=0$ $$\int_0^\infty \frac{\sin(kt)}{1+t^2}~dt=\frac{-e^k\text{Ei}(-k)+e^{-k}\text{Ei}(k)}2$$ Finally, $$\boxed{\int_0^\infty \frac{\sin(ax)}{b^2+x^2}~dx=\frac{-e^{ab}\text{Ei}(-ab)+e^{-ab}\text{Ei}(ab)}{2b}}$$ It seems this is the simplest form I can get. Are there other simple ways to solve this integral, such as contour integral, etc?","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'contour-integration']"
4702789,Does every surface admit a quadrangulation?,"Every surface (2-manifold) admits a triangulation , and I wonder if the same can be said for quadrangulation. My intuition is that every orientable surface can be quadrangulated, but I'm not sure about non-orientable surfaces. Is there any theorem in this regard?","['surfaces', 'non-orientable-surfaces', 'triangulation', 'manifolds', 'general-topology']"
4702793,ISO 2017 Shortlist (A1),"Let $a_i$ be positive integers for each $i=1$ , $2$ , $\dots$ , $n$ . Let $k$ and $M$ be positive integers such that $$k = \sum_{i=1}^n{1\over a_i}, \quad M = \prod_{i=1}^n a_i$$ If $M > 1$ , prove that the polynomial $P(x) = M(x+1)^k - \prod_{i=1}^n (x+a_i)$ has no positive roots. I'm trying to apply the AM-GM inequality to show that $\prod_{i=1}^n (x+a_i) > M(x+1)^k$ for all $x > 0$ , hence $P(x) \neq 0$ for all $x > 0$ .
But I'm having difficulty with that result. Since $a_i$ is a positive integer for each $i$ , then $${1\over n}\sum_{i=1}^n a_i \geq \bigg(\prod_{i=1}^n a_i\bigg)^{1/n}$$ with equality iff $a_1 = a_2 = \dots = a_n = T$ , so in this case $M = \prod T = T^n$ and $k = \sum {1\over T} = {n\over T}$ . Then $$P(x) = T^n(x+1)^{n/T} - (x + T)^n$$ Given that $M = T^n > 1$ , then $T> 1$ and $$(x+1)^n < (x+T)^n \implies (x+1)^{n/T}<(x+T)^n$$ But I'm not quite able to show that $T^n(x+1)^{n/T} < (x+T)^n$ EDIT: If I use Bernouli's inequality to conclude $a_i = 1$ for each $i$ , is it valid to substitute $T= 1$ into the formula above and conclude that $P(x) = (x+1)^n - (x+T)^n$ ?","['algebra-precalculus', 'polynomials', 'inequality']"
4702820,"Sequence satisfying $ \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M $, find $ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} $","Suppose that $ \left(x_{n}\right) $ is a sequence satisfying $$ \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M \quad \mbox{and} \quad \lim_{n \to \infty}{x_{2n} + x_{2n - 1}} = N $$ with $ M \ne N $ . Find $$ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}}. $$ At first, I tried to subtract two limits and got $$ \lim_{n \to \infty}{x_{2n + 1} - x_{2n - 1}} = M - N. $$ We also know that $$ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} = \lim_{n \to \infty}{\frac{x_{2n} + x_{2n + 1}}{x_{2n + 1}} - 1}. $$ But since we don't know about the convergence of $ \left(x_{n}\right) $ , I can't continue the argument. I also tried using Stolz-Cesàro theorem, but since we don't know about the convergence and monotonicity of $ \left(x_{n}\right) $ , we can't use it. Could someone provide me with a helpful hint or strategy for tackling this problem? Thank you. Note: The original problem used $ M = 315 $ and $ N = 2016 $ .","['limits', 'sequences-and-series']"
4702831,"Application of Derivatives , maxima minima jee mains 27th Aug Morning Shift 2021","This is the question Q) A wire of length 20 m is to be cut into two pieces. One of the pieces is to be made
into a square and the other into a regular hexagon. Then the length of the side (in
meters) of the hexagon, so that the combined area of the square and the hexagon is minimum, is: The way I tried solve is taking $x$ length of wire to make the square which means $20-x$ is the perimeter of the hexagon. So, For The sum of Areas $=$ Minimum $$\frac d{dx} \left[\left(\frac{x}{4}\right)^2 + \frac{3√3}{2} \left(\frac{20-x}{6}\right)^2\right]
 = 0$$ From this we get $x = -(40√3 +80) $ And if we put $x$ in $\frac{20-x}6$ , we get $\frac{50+20\sqrt3}3$ Which isn't even in the options and, I'm unable to detect my mistake here, any help would be much appreciated. The other is taking $4a + 6b =20$ Which gives the correct value of $b$ which is $\frac{10}{3+2\sqrt3} $",['derivatives']
4702849,Struggling to intuitively understand the Factorial formula,"I understand that factorial represents the total number of possible ways to arrange $n$ number of items and it is calculated as $n\cdot(n-1)...\cdot2\cdot1$ . I don't understand though how this formula can be derived by using just the knowledge of multiplication. Consider the example of having 4 chocolates $A, B, C$ and $D$ and finding the number of possible ways of arranging them together. The answer for this is $4\cdot3\cdot2\cdot1=24$ but I don't understand in the context of this example, what $4\cdot3$ actually means. Does it mean that there are 4 possible candidates out of $A, B, C$ and $D$ for the first position and for the second position, there are 3 possible candidates so with one out of those 3 candidates matches with the 4 possible candidates for the first position and forms $4$ arrangements. So if you can form 4 arrangements with 1 of 3 candidates, then you can form $4\cdot3=12$ arrangements with 2 out of 4 chocolates. But the idea of forming 4 arrangements with a single candidate for the second position does not seem correct because one of those four arrangements will be a candidate with itself ( $AA$ for example) and that makes no sense. So this is the wrong way to think about it. So what is the correct way to think about the meaning of $4\cdot3$ in this context? Do we say that 1 out of 4 candidates for the first position goes along with 3 candidates for the second position, so with 2 out of 4 chocolates, we can form $3\cdot 4 = 12$ candidates? How do we proceed from here to the step of $(3\cdot4)\cdot2$ ? What does $(12)\cdot2$ practically mean? Does it mean we have possible candidates to pick from and with a single candidate we can have 12 arrangements of 3 chocolates and with 2 candidates, we can have $12 \cdot 2 = 24$ arrangements of 3 chocolates? I am confused.","['arithmetic', 'combinatorics', 'factorial']"
4702866,Prove that if $\textbf{g}(t)\parallel \textbf{g}''(t)$ the area of the triangle $OAB$ does not depend on the variable $t$,"Let's assume $\textbf{g}(t)$ is a position vector of point $A$ , where $t$ is a variable. In addition, $\textbf{g}'(t)$ is the position vector of point $B$ . I have to prove that if $\textbf{g}(t)\parallel \textbf{g}''(t)$ , the area of the triangle $OAB$ does not depend on $t$ assuming that $\textbf{g}(t)$ is differentiable $2$ times. This is what I tried to do: Let's say $$\textbf{g}(t)=\begin{bmatrix}
a_{1} \\a_{2}
\\ a_{3}
\end{bmatrix}$$ $$\textbf{g}'(t)=\begin{bmatrix}
a'_{1} \\a'_{2}
\\ a'_{3}
\end{bmatrix}$$ $$\textbf{g}''(t)=\begin{bmatrix}
a''_{1} \\a''_{2}
\\ a''_{3}
\end{bmatrix}$$ When $\textbf{g}(t)\parallel \textbf{g}''(t)$ the cross product of the two vectors becomes $$\begin{vmatrix}
\textbf{i} & \textbf{j} & \textbf{k} \\
a_{1} & a_{2} & a_{3} \\
 a''_{1}& a''_{2} &a''_{3}  \\
\end{vmatrix}=\textbf{i}(a_{2}a''_{3}-a''_{2} a_{3})+\textbf{j}(a_{3} a''_{1}-a_{1}a''_{3})+\textbf{k}(a_{1}a''_{2}- a''_{1}a_{2})=\textbf{0}$$ The area of $OAB$ is $$S_{OAB}=\frac{1}{2}\left|\textbf{g}(t)\times  \textbf{g}'(t)\right|$$ The derivative of the inner product of $\textbf{g}(t)$ and $\textbf{g}'(t)$ is $$(\textbf{g}(t)\cdot \textbf{g}'(t))'=\textbf{g}'(t)\cdot \textbf{g}'(t)+\textbf{g}(t)\cdot \textbf{g}''(t)=\left\| \textbf{g}'(t)\right\|^2+\textbf{g}(t)\cdot \textbf{g}''(t)$$ That's all of the information I could extract and now I don't know what to do. Any suggestions?","['cross-product', 'multivariable-calculus', 'vectors']"
4702871,"How to prove $x + 3 = (x + 1)^2 \Leftarrow x=-2\;\text{or}\;1 \,? \quad \sqrt{x+3}=x+1 \Leftarrow x=1 \, ? \quad x^2 = x \Leftarrow x=0 \; $?","I can't deduce the $\color{blue}\impliedby$ of the 3 blue biconditionals below . How do I explicitly wrest or wring out the red functions in terms of x , from the 1 or 2 integers on the RHS? Doubtless, I know that the RHS's  roots satisfy the LHS!  For example, for the first equation, I know $-2 + 3 \equiv  \color{red}{(-2 + 1)^2}, 1 + 3 \equiv  \color{red}{(3 + 1)^2}$ . But it doesn't suffice to evaluate functions with 1 or 2 integers, because this
merely verifies the $\color{blue}\impliedby$ for merely 2 integers of $x$ ! We need to prove the $\color{blue}\impliedby \forall \, x$ . Hence our conclusion must express the red functions in terms of $x$ ! $x+3=\color{red}{(x+1)^2} \color{blue}{\iff} x=-2\;\text{or}\;1\;$ $x = -2 \iff x \color{limegreen}{+3} = -2 \color{limegreen}{+3} \iff x + 3 = 1.$ OR $\quad x = 1 \iff x \color{limegreen}{+3} = 1 \color{limegreen}{+3} \iff x + 3 = 4.$ Then how to deduce $\color{red}{(x+1)^2}$ ? $\color{red}{\sqrt{x+3}}=x+1 \color{blue}{\iff} x=1 $ $x = 1 \iff x \color{limegreen}{+1} =  1 \color{limegreen}{+1} \iff x + 1 = 2.$ Then how to deduce $\color{red}{\sqrt{x+3}}$ ? $\color{red}{x^2 = x} \color{blue}{\iff} x=0\;\text{or}\;1$ $x = 1 \iff x \color{limegreen}{\cdot x} = 1 \color{limegreen}{\cdot x} \iff x^2 = x.$ I have no question on this case, which works! But $x = 0 \iff x \color{limegreen}{\cdot x} = 0 \color{limegreen}{\cdot x} \iff x^2 = 0.$ Then how to deduce $\color{red}{x^2 = x}$ ?",['algebra-precalculus']
4702934,"If $\sum_{n\ge0}a_n$ converges, then what is the set $\left\{\sum_{n=0}^\infty\epsilon_n a_n \ : \,\epsilon\in\{0,1\}^\mathbb{N}\right\}$?","I am interested in understanding the properties of the following set $S$ . Given a convergent series $\sum_{n\ge0}a_n$ of complex numbers, let us define $S$ by $$S=\left\{\sum_{n=0}^\infty\epsilon_n a_n \ : \,\epsilon\in\{0,1\}^\mathbb{N}\right\}.$$ I do know that if $\forall n\in\mathbb{N}$ , $a_n\ge0$ , and if $$\forall n\in\mathbb{N}, \ \,a_n\le\sum_{k=n+1}^\infty a_k \ ,\tag{$\star$}$$ then $S=[0,A]$ , where $A=\sum_{n=0}^\infty a_n$ by definition. The condition $(\star)$ is satisfied if for example $\forall n\in\mathbb{N},\,a_n=q^n$ with $\frac12\le q<1$ . But what happens if $\forall n\in\mathbb{N},\,a_n=q^n$ with $0<q<\frac12$ ? And more generally, what can be said about the set $S$ ? I suspect some fractal structure, but I am not sure. I only know how to prove that $S$ is a compact subset of $\mathbb{C}$ . Any hints and/or references would be appreciated !","['general-topology', 'lacunary-series', 'compactness', 'sequences-and-series']"
4702942,A sufficient condition for tightness of probability measures,"For a sequence $\mu_{n}$ of Borel Probability measures, does $\int f\,d\mu_{n}$ converging for all $f\in C_{b}(\Bbb{R})$ imply that $\mu_{n}$ is a tight sequence? This is pertaining to the question here . Users(with sufficient privileges) can  view my deleted answer where I made the horribly stupid mistake of approximating $\mathbf{1}_{[-M,M]}$ under the supremum norm  by continuous bounded functions . The condition reeks of an application of Uniform Boundedness Principle , but it only yields a bound with the $L^{\infty}$ norm of the form $\sup_{n}|\int f\,d\mu_{n}|\leq C||f||_{L^{\infty}}$ which is not very helpful as we would like to approximate $\mathbf{1}_{[-M,M]}$ by a sequence say $g_{k}$ of continuous bounded functions . So ideally, a bound with the $L^{1}$ norm is what would do the job. Then we can approximate as $k\to\infty$ (uniformly in $n$ ) $\mu_{n}[-M,M]$ with $\int g_{k}\,d\mu_{n}$ and then use Cauchyness of the sequence $\int g_{k}\,d\mu_{n}$ to prove that $\mu([-M,M])$ is tight. However, I find no easy way of countering this . I might be having a brain freeze so please excuse my stupidity if I am missing something very easy.","['probability-theory', 'functional-analysis', 'weak-convergence', 'weak-topology']"
4702981,Calculating $\iiint_V g(x+y+z)dxdydz$,"I got this question on my HW and am struggling with how to approach it: Let $g:\mathbb{R}\rightarrow	\mathbb{R}$ be a continuous function such that $\int^1_{-1}g(t)dt=3$ .
Let $f(x,y,z)=g(x+y+z)$ , calculate $$\iiint_V f(x,y,z) \, dx \, dy \, dz$$ where $V=\{(x,y,z): |x|+|y|+|z| \leq 1\}$ . I know this domain is comprised of 8 symmetric pyramids, but I'm struggling with how to use this symmetry, and if I should use the Antiderivative $G$ of $g$ . Would appreciate help.","['integration', 'multiple-integral', 'definite-integrals', 'volume']"
4702992,Solve differential equation: $t\cdot\frac{dx}{dt}=x(\ln x - \ln t)$,"Solve the equation $$t\cdot\frac{dx}{dt}=x(\ln x - \ln t)$$ My try: We consider the equation for $x, t >0$ . $$tx'=x(\ln x - \ln t)$$ $$t \cdot \frac{dx}{dt}=x(\ln x - \ln t)$$ $$\frac{dx}{x}=(\ln x - \ln t)\frac{dt}t$$ $$\int \frac{1}{x}dx=\int \ln (\frac xt) \cdot \frac 1t dt$$ $\int \ln (\frac xt) \cdot \frac 1t dt=\begin{cases} u=\ln (\frac xt) \\ du = -\frac 1t dt \end{cases} = -\int u du = -\frac{u^2}2 +C = -\frac 12 \ln ^2 (\frac xt) +C$ We go back to our equation: $$\ln x =-\frac 12 \ln ^2 (\frac xt) +C$$ However, I think my way of solving is not the best, because I came to an equation from which it is difficult to determine $x(t)$ .","['initial-value-problems', 'absolute-value', 'ordinary-differential-equations']"
4703044,"Find a subspace of $L^2([0,1])$ which is dense in $L^p([0,1])$ for all $p<2$ but not in $L^2([0,1])$","For simplicity, I write $L^p$ instead of $L^p([0,1])$ . For $p<q$ , we have $\lVert\cdot\rVert_p\le\lVert\cdot\rVert_q$ and hence $L^q\subseteq L^p$ . Here $L^2\subseteq L^p$ for all $p<2$ . Let us take a non-zero $g\in L^2$ and define $S=\{f\in L^2: \langle g,f\rangle_{L^2}=0\}$ . Then $S^\perp=\text{Span}(g)\ne0$ , hence $S$ is not dense in $L^2$ . I want to examine whether $S$ is dense in $L^p$ or not. Let us take $p<2$ , then S is dense in $L^p$ iff the only linear functional on $L^p$ which vanishes on $S$ is the zero linear functional. We know that $(L^p)^*=L^q$ where $p,q$ are conjugate. As $p<2$ , $q>2$ . Let $h\in L^q$ and $\int hf\ dm=0$ for all $f\in S$ . Now if $\int gh\ dm=0$ i.e. $\overline{h}\in S$ , we have $\int h\overline{h}\ dm=0\implies h=0$ . Therefore, $S$ is dense in $L^p$ for every $p<2$ if and only if there is non-zero $g\in L^2$ such that $\int gh\ dm=0$ for all $h\in L^q$ for every $q>2$ . But is it possible to construct such $g$ ? Can anyone help me in this regard? Is there any alternate way-out to solve the problem? Thanks for your help in advance.","['measure-theory', 'lp-spaces', 'functional-analysis', 'analysis']"
4703077,Probability of holding two balls at the same time,"Imagine having five persons around a round table. Two consecutive person hold in their hand a ball. Each turn, a person gives the ball to one of its neighbour. How many turns should you wait so that one person holds the two balls at the same time ? For the moment, I've made some effort studying the distance between the two balls $d_k$ which takes value within $0, 1, 2, 3$ . Initially, $d_1 = 1$ . I'm interested in the mean value of $k$ such that $d_k = 0$ . What I have observed is : -> if $d_k = 1$ then, the next turn, $d_{k+1} = 1$ with probability $3/4$ and $d_{k+1} = 2$ otherwise.\ -> if $d_k = 2$ then, the next turn, $d_{k+1} = 2$ with probability $1/2$ , $d_{k+1} = 0$ with probability $1/4$ and $d_{k+1} = 3$ with probability $1/4$ .\ But I don't succed in finding the law of $d_k$ , any help ?","['probability', 'sequences-and-series']"
4703108,"Proof that $\,\lim\limits_{x\to 0}\frac{x^2+1}{(3x+1)(5x+1)}=1$","I want to show that $\;\lim\limits_{x\to 0}\dfrac{x^2+1}{(3x+1)(5x+1)}=1$ Note that $$\left|\frac{x^2+1}{(3x+1)(5x+1)}-1 \right|=\left|\frac{-14x^2-8x}{(3x+1)(5x+1)} \right|=\frac{\left |x \right|\left |14x+8 \right|}{\left |3x+1 \right| \left |5x+1 \right|} $$ It appears that if i take $\delta_{1}=1$ I can easily bound the numerator terms, but it is difficult for me to bound the denominator terms; any suggestions?","['limits', 'real-analysis']"
4703114,Does Fourier Transform preserve $L^1$-convergence,"Consider $f,\mathscr F f \in L^1(\mathbf R^n)$ . Dominated convergence theorem implies that $$\mathscr F[f(x)]\cdot \mathbf 1_{|x|\le R}\overset{L_1}{\longrightarrow} \mathscr F[f(x)]$$ But what can be said about $$g_R=\mathscr F[f(x)\mathbf 1_{|x|\le R}]$$ Definitely $g_R\in L^1$ but do these functions converge to $\mathscr F[f]$ in $L_1$ ? If $f\in L^2$ it's well-known that $\mathscr F[f(x)\mathbf 1_{|x|\le R}]\overset{L_2}{\longrightarrow}\mathscr F[f].$ I believe that these functions do not have to converge to Fourier transform, so I should construct a counterexample, but I struggle with it, so any hint is appreciated! Thanks! P.S. I think the example shoud also be 1-dimensional, but trivial attempt to check exponents like $e^{-x^2} $ didnt work, as soon as integral over finite interval has no elementary formula.","['measure-theory', 'lebesgue-integral', 'functional-analysis']"
4703154,"Exactly one solution of $y'=-xy^{1/3}, y(0)=0$","I want to show that the IVP $$y'=-xy^{1/3}, y(0)=0$$ has an unique solution given by $y\equiv 0$ .
One cannot apply the Picard-Lindelöf theorem because $f(x,y)=-xy^{1/3}$ is not Lipschitz in any neighbourhood of (0,0). Futhermore, the sufficent condition that $$\int_0^{0+\alpha} s^{-1/3} ds$$ is not convergent is not useful.
So, how does one prove that there exists a unique solution for the IVP?
Thanks for your answers!","['initial-value-problems', 'lipschitz-functions', 'ordinary-differential-equations']"
4703175,Three colors -- Drawing marbles until one color is exhausted,"A bag contains three colors of marbles in known quantities. Marbles are drawn randomly one at a time until any color is exhausted. What is the probability a given color exhausts first? For only two colors, the answer is intuitive -- if a color occupies 1/n th of the bag, that is how often it will ""survive"". Simulating three colors,  I'm not seeing the general pattern. (The sim is sanity checked with {1,2,3}. Of the 60 combos of rggbbb , red exhausts first in 35 of them) import random red_out, green_out, blue_out = 0,0,0 runs = 100000 for i in range(0,runs): red = 1
green = 2
blue = 3

while (red > 0 and green > 0 and blue > 0):
    rand = random.random()
    total = red + green + blue
    frac_red = red/total
    frac_green = green/total
    
    if (rand <= frac_red): red-=1
    elif (frac_red < rand <= frac_red + frac_green): green-=1
    else: blue-=1
   
    if (red == 0): red_out+=1
    elif (green == 0): green_out+=1
    elif (blue == 0): blue_out+=1 print(""RED:"",red_out/runs); print(""GREEN:"",green_out/runs);
print(""BLUE:"",blue_out/runs)",['probability']
4703215,finding infimum of the ratio of largest to smallest distance of six points in $\mathbb R^2$,"How can we find the infimum of the following set $$\left\{\frac{\max_{i\neq j}d(A_i,A_j)}{\min_{i\neq j}d(A_i,A_j)}:A_1,A_2,\ldots,A_6 \in \mathbb R^2, A_i\neq A_j (\forall i\neq j)\right\}.$$ This comes from a meme, but I thought it might be interesting to know what this infimum will be. But I did not make much progress on it (except for finding the ratio for some special cases).","['geometry', 'real-analysis', 'elementary-set-theory', 'supremum-and-infimum', 'problem-solving']"
4703232,Is a differential equation linear if the maximal solutions constitute a vector space?,"Consider the differential equation $x'= F(t, x)$ , $F$ continuous, with the property of existence and uniqueness of solutions. Suppose that the maximal solutions are defined on all of $\mathbb{R}$ and constitute a vector space. Is it true that this differential equation is linear? Meaning that $F$ has the form $F(t, x) = A(t)x$ .",['ordinary-differential-equations']
4703263,What operator is used when Rubik's cube is modeled as a group?,"When I earned about groups, a group was taken to be a set of elements that operated on each other to produce another member of the set, along with the properties of closure, unit element, inverse and associativity. I have seen a different definition used for Rubik's cube.  By this definition, a group is formed by a set of generators whose combinations generate the rest of the group with appropriate restrictions for identity, inverse and associativity.  For the Rubik's cube, the generators are the 6 face rotations. How can these two definitions be reconciled? In particular, what is the operator for the second definition?  It seems that the only way to determine if two elements (generator sequences) are identical, would be to check that each of the little cubies on the cube ended up in the same spot.  How does this get incorporated into the group definition?",['group-theory']
4703289,Evaluation of tricky Gamma infinite sum,"I want to prove that: $$\sum_{n=0}^\infty\frac{n}{(n+1)^2}\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n+1}{2}\right)}=\frac{4G}{\sqrt{\pi}}+\sqrt{\pi}\log2$$ and $$\sum_{n=0}^\infty(-1)^n\frac{n}{(n+1)^2}\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n+1}{2}\right)}=\frac{4G}{\sqrt{\pi}}-\sqrt{\pi}\log2$$ where $G$ is Catalan's constant.
I got these results, evaluating some random integrals in various ways and manipulating the results. Now, my question is as follows: is there a way to get the two results starting off from the series, without knowing the original integral, just using some algebraic manipulations and Gamma identities? I tried doing this myself, because WolframAlpha was able to compute it , so I thought it could be done. Turns out this was not the case, as I got stuck before starting. My idea was turning Gammas into factorials, and then work with sums of factorials, but looking at the structure of the two Gammas in each sum, only one of them at a time can have an integer argument. However, turns out $\Gamma(k+\frac12)$ has a nice structure as well, the problem is that in each sum, the integer argument in the gamma function is alternating between numerator and denominator, so things get ugly and messy splitting the sums in even and odd, and I doubt this is the right approach. I can't think of anything else though. Suggestions are appreciated. I won't post the original integral, in order to prevent answers that start off from it. I'm looking for answers provided as we didn't even know the result. Any hints? EDIT: Splitting the first sum into even end odd terms, we simplify the problem to just proving this two results: $$\sum_{n=0}^\infty \frac{4^n}{(2n+1)^2}\frac{1}{{2n\choose n}}=2G$$ and $$\sum_{n=0}^\infty\frac{2n+1}{(n+1)^2}{2n\choose n}\frac{1}{4^n}=4\log2$$ and these would prove the full result. Still don't know how to do it though.","['calculus', 'catalans-constant', 'sequences-and-series']"
4703299,Evaluating $ \int \frac{ 1 } { \sin x - \cos x - 1} dx$,"Below is a problem I attempted. However, I failed to find the right approach. Problem: Perform the following integration: $$ \int \dfrac{ 1 } { \sin x - \cos x - 1} dx$$ Answer: Let $I$ be the integral we are trying to evaluate. My first approach is to
try to simplify the denominator by using the idenity $\sin^2 x + \cos^2 x = 1$ . \begin{align*}
I &= \int \dfrac{1 } { \sin x - (\cos x + 1)}dx \\
I &= \int \dfrac{ ( \sin x + (\cos x + 1))  }
	{ ( \sin x + (\cos x + 1))( \sin x - (\cos x + 1))}dx \\
%
I &= \int \dfrac{ ( \sin x + (\cos x + 1)) }
	{\sin^2x - \sin x(\cos x + 1) + \sin x (\cos x + 1) - (\cos^2 x + 1)^2} dx\\
%
I &= \int \dfrac{ ( \sin x + \cos x + 1)  }
	{\sin^2x - \cos^2x - 2\cos x - 1} dx\\
\end{align*} At this point, I do not see how to make progress. I was hoping for a $\sin^2x + \cos^2x$ in the denominator. My second approach. \begin{align*}
I &= \int \dfrac{ (\cos x) \,\, dx } { \sin x \cos x - \cos^2x - \cos x } \\
I &= \int \dfrac{ (\cos x) \,\, dx }
	{ \sin x \cos x - \cos^2x - (1 - \sin^2 x) - \sqrt{1-\sin^2 x} } \\
\end{align*} Now I can apply the subsutution $u  = \sin x$ and get rid of the trig functions. \begin{align*}
I &= \int \dfrac{ du } { u^{\frac{3}{2}} - (1-u^2) - u^{ \frac{1}{2}} } \\
I &= \int \dfrac{ du } { u^2 + u^{\frac{3}{2}} - u^{ \frac{1}{2}} - 1 } 
\end{align*} Again, I do not see how to make progress. My third approach is to dividend numerator and denominator by $\cos^2 x$ hoping to setup a substitution like $u =  \tan  x$ . \begin{align*}
I &= \int \dfrac{ (\sec^2 x) \,\, dx }
	{ \dfrac{ \tan x}{\cos x} - \sec x - \sec^2 x } \\
\end{align*} What is the right way to solve this problem? Note: I was planning on adding the homework tag to this post (despite the fact I am not in school) because I am not looking for the answer. I am looking for guidance to get there. When  I attempted to use that tag, I got a message saying please do not use it. So I did not.","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
4703320,"How well does $\mathrm{Zi}(x)=\frac1e\sum_{k=1}^\infty\frac{(\ln x)^k}{kk!\phi(k)}$, with $\phi(k)=\sum_{n=1}^\infty e^{-n^k}$, approximate $\pi(x)$?",It is well known that: $$\mathrm{Li}(x)=\int_2^x \frac{1}{\ln(t)}~dt$$ is an extraordinarily good approximation to the prime counting function $\pi(x)$ and is currently the best known approximation. I constructed a function asymptotic to the prime counting function $\pi(x):$ $$\mathrm{Zi}(x)=\frac{1}{e}\sum_{k=1}^\infty\frac{(\ln x)^k}{kk!\phi(k)}$$ where I invented the function: $$  \phi(k)=\sum_{n=1}^\infty e^{-n^k}  $$ This $\mathrm{Zi}(x)$ is a fantastic approximation to $\pi(x).$ I believe the constant $1/e$ is optimal. Just how good is it? Numerical analysis suggests that $\mathrm{Li(10^{17})}-\mathrm{Zi(10^{17})}\approx 40$ where $\mathrm{Li}(x)$ is the offset logarithmic integral. This means $\mathrm{Zi}(x)$ beats $\mathrm{Li}(x)$ by $40$ units at that value. Numerical analysis also suggests that $\mathrm{Li}(x)-\mathrm{Zi}(x)\sim \ln x$ which of course could fail eventually in light of Skewe's number. Is $\mathrm{Zi}(x)$ on average better than $\mathrm{Li}(x)$ at approximating $\pi(x)?$ That is does $\mathrm{Zi}(x)$ reduce the mean error even more so than $\mathrm{Li}(x)?$,"['number-theory', 'elementary-number-theory', 'asymptotics', 'analytic-number-theory', 'prime-numbers']"
4703334,Elementary proof of Wright-Fisher's time until fixation formula?,"Consider the Markov process $\def\Z{\mathbb{Z}}\def\N{\mathbb{N}}\def\E{\text{E}}\newcommand{\bp}[1]{{\left({#1}\right)}}(X_t^n)_{t\in\Z_+}$ given by $X_0^n = p \in [0,1]$ and $X_{t+1}^n|X_t^n \sim \frac1n \text{Binomial}(n, X_t)$ , where $n\in\N$ . In other words, $X_t^n \in \{0, \frac1n, \ldots, \frac{n-1}n, 1\}$ for $t\geq 1$ , and $$\Pr\bp{X_{t+1}^n = \frac kn \mid X_t^n = p} = \binom{n}{k} p^k (1-p)^{n-k}.$$ It's easy to see that $X_t^n$ is a martingale and it converges almost surely to either $0$ or $1$ . Let $\tau_n = \min\{t\geq1 : X_t^n \in \{0, 1\}\}$ be the time until fixation. It's known that $$\lim_{n\to\infty} \E\bp{\frac{\tau_n}n \mid X_0=x} = -2x\log x - 2(1-x)\log(1-x).$$ The proof of that formula requires using a diffusion approximation, which I find super hard to construct (it's done rigorously in this book , for example). Can you help me find an elementary proof? My idea is the following: let $f_n(x) := \E(\frac1n\tau_n|X_0=x)$ . We have $f_n(x) = \frac1n + \sum_{k=1}^{n-1} p_n(x,k) f_n(k/n)$ for $x\in[0,1]$ , where $p_n(x,k) := \binom{n}{k} x^k (1-x)^{n-k}$ . For each $n$ , $f_n(x)$ is a polynomial in $x$ , so we get $$\sum_{k=0}^n p_n(x,k) \bp{f_n\bp{\frac kn} - f_n(x)} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0.$$ We can do a Taylor expansion $$\sum_{k=0}^n p_n(x,k) \bp{f_n'(x)\bp{\frac kn - x} + \frac12 f_n''(x) \bp{\frac kn - x}^2 + \frac16 f_n'''(\xi_k) \bp{\frac kn - x}^3} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0.$$ We have $\sum_{k=0}^n p_n(x,k)\bp{\frac kn - x} = 0$ , $\sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^2 = \frac1n x(1-x)$ and $\sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^3 = \frac{1}{n^2} x(1-x)(1-2x)$ , so replacing we get $$\frac12 f_n''(x) x(1-x) = -1 + R_n(x),$$ where $|R_n(x)| \leq \frac1n \|f_n'''\|_\infty x(1-x)(1-2x) + p_n(x,0) + p_n(x,1)$ .
If we can prove that $f_n, f_n', f_n'', f_n'''$ are all bounded we can pass to the limit, and we get the result. This doesn't sound so hard.","['stochastic-processes', 'martingales', 'probability']"
4703352,How do I get rid of the $|y-x|$ denominator?,"I'm stuck on this problem. I'm trying to prove the limit $\lim_{(x,y)\to(0,1)}\left(\frac{y+x}{y^2-x^2}\right)$ exists from first principles. I have got a value of $1$ for the limit via limit laws, so I know I must work towards that. Here is what I have: $$Let f(x,y)=\frac{y+x}{y^2-x^2}\;and\;\varepsilon > 0$$ $$\therefore f(x,y)-1=\frac{y+x}{y^2-x^2}-1$$ $$\therefore |f(x,y)-1|=\left|\frac{1}{y-x}-\frac{y-x}{y-x}\right|=\left|\frac{x-(y-1)}{y-x}\right|=\frac{|x-(y-1)|}{|y-x|}\le\frac{|x|+|y-1|}{|y-x|}$$ If $|y-x| \ge 1$ then I can remove the denominator without changing the inequality. In that case, I can prove the required limit exists. However, for $|y-x|<1$ , I can't do that. How should I proceed from here to complete the proof that the limit is 1?","['limits', 'multivariable-calculus']"
