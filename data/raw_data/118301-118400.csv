question_id,title,body,tags
1755625,Vector Calculus: solution to Poisson equation,This is problem 8.4.17. from Marsden Vector Calculus book. Let $\rho$ be a continuous function which vanishes outside a 3D region $W$ . Define $\phi(\textbf{p})=\displaystyle\iiint_W\frac{\rho(\textbf{q})}{4\pi\lVert\textbf{p}-\textbf{q}\rVert}dV(\textbf{q})$ . How to show that $\displaystyle\iint_{\partial W}\nabla\phi\cdot\textbf{dS}=-\iiint_W\rho\; dV$ without the knowledege that $\phi$ is the solution to the Poisson equation $\nabla^2\phi=-\rho$ (which will be a consequence of this)?,"['partial-differential-equations', 'multivariable-calculus', 'linear-algebra', 'analysis', 'vector-analysis']"
1755647,"Real Analysis, Folland Problem 6.2.17 Dual of $L^p$","Theorem 6.14 - Let $p$ and $q$ be conjugate exponents. Suppose that $g$ is a measurable function on $X$ such that $fg\in L^1$ for all $f$ in the space $\sum$ of simple functions that vanish outside a set of finite measure, and the quantity $$M_q(g) = \sup\{|\int fg|:f \in \sum \ \text{and} \ \|f\|_{p} =1\}$$ is finite. Also, suppose either that $S_g = \{x: g(x)\neq 0\}$ is $\sigma$-finite or that $\mu$ is semifinite. Then $g\in L^q$ and $M_q(g) = \|g\|_{q}$ Problem 6.2.17 - With notation as in Theorem 6.14, if $\mu$ is semifinite, $q < \infty$, and $M_q(g) < \infty$, then $\{x: |g(x)| > \epsilon \}$ has finite measure for all $\epsilon > 0$ has finite measure for all $\epsilon > 0$ and hence $S_g$ is $\sigma$-finite. As you can tell by the plethora of posts I have on $L^p$ spaces I am really struggling to grasp these concepts and solve these problems. Any advice or suggestions is greatly appreciated.","['lp-spaces', 'analysis']"
1755663,Proving that a set of functions is a linear subspace of a vector space,"I am attempting to solve the following problem: Let $V$ be the vector space of all continuous functions $f : R → R$ with point-wise addition and scalar multiplication defined. (a) Show that
$M_1$ = {$f ∈ V :$ There exists $a, b ∈ R$ with $f(x) = a sin(x + b)$ for all $x ∈ R$}
is a linear subspace of $V$. (b) Show that
$M_2$ = {$f ∈ V :$ There exists $a ∈ R$ with $f(x) = sin(ax)$ for all $x ∈ R$}
is not a linear subspace of $V$. As far as I understand, the requirement for being a linear subspace is that the set must be algebraically closed with respect to addition and multiplication by a scalar, that is, (i) If $f_1,f_2 ∈ V$ then $f_1+f_2 ∈ V$ (ii) If $f ∈ V$ then $λf ∈ V$, where $λ∈R$ However, trying to apply the first condition to prove part (a), I need to find a way to express $(f_1+f_2)(x)=f_1(x)+f_2(x)=a_1sin(x+b_1)+a_2sin(x+b_2)$ in the form $asin(x+b)$ so that I can prove that the sum of the two functions belongs to $M_1$ as well.
Further, the second condition (multiplication by a scalar) is valid as $(λf)(x)=λasin(x+b)$ which belongs to $M_1$. It is just the first condition at which I am stuck. Also, for part (b), I would appreciate hints for proving how the set is not a linear subspace? What would be a way of showing conclusively that the two conditions do not hold?","['functional-analysis', 'linear-algebra', 'vector-spaces']"
1755676,Why is $\overline{B(l^2)\odot B(l^2)}^{\| \enspace \|_{op}}\neq B(l^2\otimes l^2)?$,"Let $B(l^2)$ be the $C^*$algebra of bounded linear operators on the sequence space $l^2$ and denote with $B(l^2)\odot B(l^2)$ the tensor product of $B(l^2)$ with itself, considered as a $*$algebra (with component-wise multiplication and involution). It is $B(l^2)\odot B(l^2)\subset B(l^2\otimes l^2)$. My question is: how to prove that $$\overline{B(l^2)\odot B(l^2)}^{\| \enspace \|_{op}}\neq B(l^2\otimes l^2)?$$ Here $\| \enspace \|_{op}$ denotes the operator norm on $B(l^2\otimes l^2) $. First I tried to find an operator $T\in B(l^2\otimes l^2)$ such that there is no sequence $(X_n)\subseteq B(l^2)\odot B(l^2)$  with $\|X_n-T\|_{op}\to 0$ for $n\to \infty$; but I have no idea for an example. Similary, an other strategy is: If you have an idea of a non-zero continuous function $f:B(l^2\otimes l^2)\to X$ into a suitable space $X$, maybe $X=\mathbb{C}$, such that $f$ vanishes on $B(l^2)\odot B(l^2)$. This would prove that $B(l^2)\odot B(l^2)$ is not dense in $B(l^2\otimes l^2)$ with respect to $\|\enspace \|_{op}$, too. But I have no idea for such an $f$. Could you help me? Regards","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras', 'convergence-divergence']"
1755730,How many roots does the polynomial $p(z) = z^8 + 3z^7 + 6z^2 + 1$ have inside the annulus $1 < |z| < 2$?,How many roots does the polynomial $p(z) = z^8 + 3z^7 + 6z^2 + 1$ have inside the annulus $1 < |z| < 2$? I know I can use Rouche's Theorem. I'm just not sure how. It states that $|f(z) − g(z)| < |f(z)|$ where $f(z)$ is the given polynomial and we choose $g(z)$. Any solutions or hints are greatly appreciated.,"['real-analysis', 'polynomials', 'abstract-algebra', 'roots', 'complex-analysis']"
1755750,Linear Transformation of Poisson Point Process,"Suppose we have a random variable that follows a Poisson Point Process: $ X \sim poisson(\lambda t) $ and a function $f(x) = ax + b $ where $a,b \in \mathbb{R}$. What is the pdf of $Y = aX + b$? I know for continuous random variables if $f(x)$ is strictly increasing:
$$f_y(Y = y) = \frac{1}{|a|} f_x\left(\frac{y-b}{a}\right)$$
However I am having trouble doing this for the discrete case. My attempt is as follows suppose for simplicity $t = 1$, thus $X \sim poisson(\lambda)$, Now we have: \begin{align*}
P(Y \leq y)  &= P(aX + b \leq y)\\
&= P\left(X \leq \frac{y - b}{a}\right)\\
&= F_x\left(\frac{y-b}{a}\right)\\ \\
P(Y = k) &= P(Y \leq k) - P(Y \leq k - 1)\\
&=F_y(Y = k) - F_y(Y =  k - 1)\\
&= F_x(\frac{k-b}{a}) - F_x(\frac{k-1-b}{a}) \\
&= \sum_{n = 0}^{\frac{k-b}{a}} e^{\lambda} \frac{\lambda^n}{n!} - \sum_{n = 0}^{\frac{k-1 -b}{a}} e^{\lambda} \frac{\lambda^n}{n!}
\end{align*} At this point I am stuck because clearly the factorials could take noninteger values, so this difference seems to be meaningless. Can anyone give me a hint as to how to continue?","['probability', 'probability-distributions']"
1755794,Proof explanation - weak law of large numbers,"Let $(X_i)$ be i.i.d. random variables with mean $\mu$ and finite variance. Then $$\dfrac{X_1 + \dots + X_n}{n} \to \mu \text{ weakly }$$ I have the proof here: What I don't understand is, why it suffices to show that $F_{S_n/n}(\mu - \epsilon) \to 0$ and $F_{S_n / n}(\mu + \epsilon) \to 1$ as $\epsilon \to 0$ for every $x$. AFAIK, we must prove that $F_{S_n/n}$ converges to the distribution function for $\mu$ for every continuity point of $F$ - why do we consider the discontinuity point?, i.e. it converges to $1$ for $x \geq \mu$ and to $0$ for $x < \mu$, but I am not sure how this shows that.","['probability-theory', 'probability', 'statistics', 'proof-explanation']"
1755821,"Find the number of tuples consisting of $0, 1$ and $3$","How can I find the number of tuples $(k_1, k_2, ...,k_{26})$ such that each $k_i$ equals $0, 1$ or $3$ and $k_1 + k_2 + ... + k_{26} = 15$. I can reduce this problem to finding the coefficient of $x^{15}$ in expression $(1 + x + x^3)^{26}$, but I do not now any simple way to do it.","['combinatorics', 'discrete-mathematics']"
1755840,Difference between conjugacy classes and subgroups?,I am studying Group theory and Im not sure I understand the difference between a conjugacy class and subgroup?,"['abstract-algebra', 'group-theory']"
1755847,"Show that $\pi(X, x) = [e_x]$ if $X$ is a finite topological space with the discrete topology.","Show that $\pi(X, x) = [e_x]$ if $X$ is a finite topological space with the discrete topology. I want to show this by showing that there does not exist any path $f$, $\forall x, y \in X$. Assume for contradiction that there exists a path $f: I \mapsto X$ from $x$ to $y$. Then there are only finitely many points that image of $f$ passes through. Hence, the points that $f$ passes through are open sets whose inverse images under $f$ are disjoint, proper, non-empty open sets on $I$, which then form a separation on $I$. This contradicts that $I$ is connected. So there does not exist any $f$ between distinct points in $X$. Hence $\pi(X, x)$ is trivial. Is this a legit proof? Any comments or corrections? Thanks!!!","['general-topology', 'fundamental-groups']"
1755856,calculate arbitrary points from a plane equation,I understand how one can calculate a plane equation (ax+by+cz=d) from three points but how can you go in reverse? How can you calculate arbitrary points from a plane equation?,['geometry']
1755899,MSE of uniform distribution,"I would like to know how to calculate the MSE for a Uniform Distribution on $(θ,2θ)$ I know that MSE is the variance of the Method of Moments Estimator (MME). I have found this to be $3θ/2$ . Then I think I should calculate the $Var(2/3X\bar)$ . Assuming that the bias of the MME is $0$ . I think the answer is $3(θ^2) / 48n$ . However I am unsure if this is the correct computation.","['statistics', 'probability']"
1755904,"Find a conformal mapping from the quarter-disk $Q=\{ |z|<1 : rez>0,im z>0 \}$ onto the upper half plane set $U=\{im z>0\}$","Find a conformal mapping from the quarter-disk $Q=\{ |z|<1 : rez>0,im z>0 \}$ onto the upper half plane set $U=\{im z>0\}$ I'm guided through this problem: First I need to find the image of the quarter disk $Q=\{ |z|<1 : rez>0,im z>0 \}$ by $f(z)=z^2$? My answer to this is a semi-circle (edit:disk), with unit radius. Now I need to find the map of the halfdisk $D=\{ |z-1|<1 :im z>0 \}$ onto the the first quadrant $\{im z>0, rez>0\}$. I'm stuck on this bit: I tried: rewriting the halfdisk as $1+e^{it}$ for $0\leq t \leq 2\pi$ , but I didn't get far. Lastly, I need to somehow put the first two together, to get a map from the quarter disk onto the upper halfplane","['complex-analysis', 'conformal-geometry', 'mobius-transformation']"
1755911,Given that $\cos A + \cos B + \cos C = 0$ and $\sin A + \sin B + \sin C = 0$.,If  $\cos A + \cos B + \cos C = 0$ and $\sin A + \sin B + \sin C = 0$. The value of $ \sin^3A+\sin^3B+\sin^3C$ What I can see here is that as $\sin A + \sin B + \sin C = 0$ hence $ \sin^3A+\sin^3B+\sin^3C=3\sin A \sin B\sin C$  but I am not able to achieve a constant value. Please give some hint.,['trigonometry']
1755984,Is the following equation equivalent to the 2-cocycle condition?,"Given a finite abelian group $G$, I'm looking for functions $\rho:G \times G \to U(1)$ such that 1)  $~~~~~\rho(g,e) = 1 = \rho(e,g)$, where $e\in G$ is the unit element and such that 2)  $~~~~\rho(a,bc)~ \rho(b,c)~ \rho(b,a) = \rho(b,ac)~ \rho(a,c)~ \rho(a,b)$. I've noticed that this equation is solved by 2-cocycles of $G$.
Also, the equation is invariant  under 2-coboundaries, i.e. if $\rho$ is a solution, then so is $\rho'(g,h) = \rho(g,h)~\phi(g) \phi(h) \overline{\phi}(gh)$ for any function $\phi:G \to U(1)$.
This makes me suspect that the above equation might be equivalent to the cocycle condition $\rho(a,bc) \rho(b,c) = \rho(ab,c) \rho(a,b)$. However, I couldn't figure out how to prove this. Question: Is the above equation equivalent to the cocycle equation and if not, has it been studied somewhere and what can be said about its (non-cocyclic) solutions?","['finite-groups', 'abelian-groups', 'group-theory']"
1756032,Properties of sin(x) and cos(x) from definition as solution to differential equation y''=-y,"I recently came across the interesting definition of the sine function as the unique solution to the Initial Value Problem
$$y'' = -y$$
$$y(0) = 0, y'(0) = 1$$
(My first question would be why this solution is unique, but I found that proofs of uniqueness and existence are too complicated for me to understand) Taking existence and uniqueness for granted still leaves a lot of questions about the properties of sine like: $y$ is periodic with smallest period $T$ (defined to be $2\pi$) $ -1 \leq y(x) \leq 1 $ for all $x \in \mathbb{R}$ The value of sine for certain values related to the period: $ y(\frac{\pi}{6}) = \frac{1}{2}, y(\frac{\pi}{3}) = \frac{\sqrt3}{2}$ All the various trigonometric identities How would one go about proving these properties using only the differential equation definition? I have no idea where to start. Usually these properties (at least for me) are intuitively understood through the unit circle but that doesn't help here.","['trigonometry', 'ordinary-differential-equations', 'definition']"
1756044,"Should $\bigcap_{n = 1}^\infty (a-\frac{1}{n}, b + \frac{1}{n})$ be $(a,b)$ or $[a,b]$","I am confused about the limiting behavior of as $n \to \infty$, $\bigcap_{n = 1}^\infty (a-\frac{1}{n}, b + \frac{1}{n})$. I have read that it is the case that this set becomes closed, but I can't help but to think that if we take $n \to \infty$, then $\dfrac{1}{n} \to 0$, and the inner set is $(a,b)$ Can someone please help?","['general-topology', 'elementary-set-theory']"
1756048,Fibers of extension of scalars to algebraic closure of an affine variety is of dimension 0.,"Let $X$ be an affine variety over $k$ and $\overline{k}$ be the algebraic closure of $k$. Then the projection morphism $X_\overline{k} = X \times _k \overline{k} \to X$ has dimension 0 fibers. Say $X = \text{Spec } A$. Then $X_\overline{k} = \text{Spec} (A \otimes _k \overline{k})$ The fiber of the homomorphism at point $p \in X$ is $$X_p = X_\overline{k} \times _X \text{Spec} k(p) = \text{Spec} (A \otimes_k \overline{k} \otimes_A k(p)) = \text{Spec}(A_p/m_p A_p \otimes _k \overline{k}) $$ Thus, it is equivalent to say that the ring $A_p / m_p A_p \otimes_k \overline{k}$ has (Krull) dimension $0$. As far as I know, the tensor of two fields is not necessarily a field, thus ruling out any trivial dimension considerations. I am not really sure how to think of prime ideals in such a tensor product. Any ideas to tackle this would be appreciated. An idea I'm formulating right now is that $k(p) = A_p / m_p A_p$ is a field extension of $k$. Thus the tensor is simply an extension of scalars, from one field to its algebraic closure. Is this perhaps equivalent to considering the same field $k(p)$ but now explicitly embedded in this larger field, namely the closure?","['tensor-products', 'algebraic-geometry', 'abstract-algebra', 'krull-dimension', 'field-theory']"
1756068,"Bounded sequence in $W^{1,p}$ converging to a non-differentiable function in $L^p$","Let $U = B(0,1)$ be the unit ball in $\mathbb R^n$, $p>1$ and $\{u_k \}$ a bounded sequence in $W^{1.p}(U)$.  The Rellich-Kondrachov compactness theorem tells us that there is a subsequence $\{ u_{k_j}\}$ converging to some $u$ in $L^p(U)$. My question is this: does $u$ necessarily have a (weak) derivative? I assume the answer is no, otherwise the theorem would probably include this. However, I am having trouble thinking of a counterexample. Can someone give an example of a bounded sequence in $W^{1,p}(U)$ which converges to a function in $L^p(U)$ which has no weak derivative?","['functional-analysis', 'real-analysis', 'sobolev-spaces', 'compactness']"
1756093,Laplace transforms of powers of cosine (solved!),"During the past several hours, while studying the Laplace transform, I've started wondering what \begin{equation}
\mathcal{L} \{ \cos^n(at)\}(s)
\end{equation} would look like – since it won't appear on your typical transform table. After some thinking and tedious pattern-finding and rearranging (and typing on Wolfram Alpha), I've come up with the following (interesting, yet messy) two formulae: (1) For odd $n$'s ($n=2k+1, k \in \mathbb{N}$), \begin{equation}
\mathcal{L}\{\cos^{2k+1} (at)\}(s) = \frac{1}{2^{2k}}\sum_{j=0}^{k} \frac{\binom{2k+1}{k-j}s}{ (2j+1)^2 a^2 + s^2} \tag{1}
\end{equation} (2) For even $n$'s ($n=2k, k \in \mathbb{N}$), \begin{equation}
\mathcal{L}\{\cos^{2k} (at)\}(s) = \frac{\binom{2k}{k}}{2^{2k}s} +  \frac{1}{2^{2k-1}}\sum_{j=1}^{k} \frac{\binom{2k}{k-j}s}{(2j)^2 a^2 + s^2} \tag{2}
\end{equation} So, First of all, are these statements even correct? If they are, how can I prove it using the definition of the transform? I already have a few ideas in my mind, which involve the exponential definition of cosine, ""complexifying"" the Laplace integral (but isn't $s$ already complex?), applying the inverse Laplace transform on each of the series terms, or heck, maybe even the Maclaurin expansion of cosine or some trig identities... it'd be lovely to read some suggestion from you all! I'm also looking at merging the two formulae into a single one that works for every $n$, but the major obstacle seems to be the $ \frac{\binom{2k}{k}}{2^{2k}s} $ term in the even-$n$ formula: where does its weird binomial coefficient come from (1/2 of what the sum's general binomial coefficient would predict)? A similar approach has been tried for $ \sin^n (at) $, with similar results — messy sums which only work for one or the other parity of $n$. How do the two relate to each other, and possibly to the (albeit trivial) transform of $\exp^n{(at)}=\exp{n(at)}$? And how about $ \sinh^n (at) $ and $ \cosh^n (at) $? Thanks in advance, hope this has provided some interesting input EDIT. Following Alex R.'s suggestion, I've tried using \begin{equation}
\cos^n (at) = \frac{\left(e^{iat} + e^{-iat}\right)^n}{2^n}
\end{equation} The Laplace transform then becomes: \begin{equation}
\mathcal{L}\{\cos^{n} (at)\}(s) = \int_{0}^{\infty}2^{-n}\left(e^{iat} + e^{-iat}\right)^n e^{-st} dt \\
= 2^{-n} \int_{0}^{\infty} \sum_{j=0}^{n} \binom{n}{j} e^{(iat)j} e^{(-iat)(n-j)} e^{-st} dt \\
= 2^{-n} \sum_{j=0}^{n} \binom{n}{j} \int_{0}^{\infty} e^{-\big(s+ia(2j-n)\big)t} dt \\
= 2^{-n} \sum_{j=0}^{n} \binom{n}{j} \frac{1}{-\big(s+ia(2j-n)\big)} \left[\lim_{\beta \to \infty} e^{-\big(s+ia(2j-n)\big)\beta} - e^{-\big(s+ia(2j-n)\big)(0)}\right]
\end{equation}
\begin{equation}
= 2^{-n} \sum_{j=0}^{n} \binom{n}{j} \frac{[-1]}{-\big(s+ia(2j-n)\big)} = 2^{-n} \sum_{j=0}^{n} \binom{n}{j} \frac{1}{s+ia(2j-n)}
\tag{3} \end{equation} I'm not very satisfied with what I've got — not even sure it's right. And besides that, I can't figure out how it connects with the two formulae given above. Please let me know what you think EDIT 2. We could actually check the result in $(3)$ by the use of the Fourier-Mellin integral below: \begin{equation}
f(t)=\mathcal{L}^{-1}\{\hat{f}(s)\} = \frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty} e^{st} \hat{f}(s) ds
\end{equation}
where $\gamma \in \mathbb R$ is greater than the real part of all singularities of $\hat{f}(s)$. Applying this formula to the sum in $(3)$, we get \begin{equation}
\begin{split}
f(t) &= \frac{2^{-n}}{2\pi i} \int_{\gamma-i\infty}^{\gamma+i\infty} e^{st} \sum_{j=0}^{n} \binom{n}{j} \frac{1}{s+ia(n-2j)} ds \\
&= \frac{2^{-n}}{2\pi i} \sum_{j=0}^{n} \binom{n}{j} \int_{\gamma-i\infty}^{\gamma+i\infty} \frac{e^{st}}{s+ia(n-2j)}ds
\end{split}
\end{equation} All the poles of our sum-defined $\hat{f}(s)$ lie on the imaginary axis, so we can choose any $\gamma > 0 $. Let us now consider the integral alone. We may now close the contour with a semicircle lying on the left side of the plane and call it $\Gamma$. \begin{equation} \newcommand{\Res}{\mathop{\text{Res}}}
\oint_\Gamma \frac{e^{st}}{s+ia(n-2j)}ds = 2\pi i \lim_{s\to -ia(n-2j)} e^{st} = 2\pi i e^{-ia(n-2j)t} \tag{4}
\end{equation}
which can be demonstrated to be equal to the integral over the line $\gamma+iq, \quad -\infty < q <\infty$. We can plug $(4)$ into our formula for $f(t)$
\begin{equation}
f(t) = \frac{2^{-n}}{2\pi i} \sum_{j=0}^{n} \binom{n}{j} (2\pi i e^{-ia(n-2j)t}) = 2^n \sum_{j=0}^{n} \binom{n}{j} e^{-(n-j)(iat)}e^{j(iat)}
\end{equation}
which of course we recognize as being the binomial expansion of
\begin{equation}
f(t) = \left(\frac{e^{iat} + e^{-iat}}{2}\right)^n = \cos^n(at)
\end{equation} I will soon try to prove $(1)$ and $(2)$ by the same means. FINAL EDIT. I just had to expand the result in $(3)$ to obtain \begin{equation}
\bbox[5px,border:2px solid red]{
\mathcal{L} \{ \cos^n(at)\}(s) = \frac{1}{2^n}\sum_{j=0}^{n} \binom{n}{j} \frac{s+(n-2j)a}{s^2+(n-2j)^2a^2}} \tag{5}
\end{equation} (a big duhhh on my behalf!)
We can try out $(5)$ with even or odd $n$'s and it will work, connecting beautifully to the formulae in $(1)$ and $(2)$. The painful asymmetry in $(2)$ can now be explained: it was actually caused by the fact that, if $n=2k$ and $k$ is integer, then the $n$th row in Pascal's triangle contains an odd number of terms – of which $\binom{2k}{k}$ is the central one. Since $\binom{n}{q} = \binom{n}{n-q}$, every other term besides that middle one were appearing twice in the sum, and so they could be reduced to $2\binom{n}{q}$. The asymmetry in $(2)$ was just concealing the underlying symmetry expressed in $(5)$. Finding an expression for the Laplace transform of the powers of sine was trickier, but possible: \begin{equation}
\mathcal{L} \{ \sin^n(at)\}(s) = \frac{1}{2^n}\sum_{j=0}^{n} \binom{n}{j} (-1)^{j+\lfloor\frac{n}{2}\rfloor} \frac{s+(n-2j)a}{s^2+(n-2j)^2a^2} \tag{6}
\end{equation} It works in a similar fashion for the hyperbolic functions:
\begin{equation}
\mathcal{L} \{ \cosh^n(at)\}(s) = \frac{1}{2^n}\sum_{j=0}^{n} \binom{n}{j} \frac{s+(n-2j)a}{s^2-(n-2j)^2a^2} \tag{7}
\end{equation}
\begin{equation}
\mathcal{L} \{ \sinh^n(at)\}(s) = \frac{1}{2^n}\sum_{j=0}^{n} \binom{n}{j} (-1)^j \frac{s+(n-2j)a}{s^2-(n-2j)^2a^2} \tag{8}
\end{equation}","['laplace-transform', 'complex-analysis', 'calculus', 'analysis']"
1756108,"Real solution of the equation $\sqrt{a+\sqrt{a-x}} = x\;,$ If $a>0$","For a real number $a>0\;,$ How many real solution of the equation $\sqrt{a+\sqrt{a-x}} = x$ $\bf{My\; Try::}$ We can Write $\sqrt{a+\sqrt{a-x}} = x$ as $a+\sqrt{a-x}=x^2$ So we get $(x^2-a)=\sqrt{a-x}\Rightarrow (x^2-a)^2 = a-x\;,$ Where $x<a$ So we get $x^4+a^2-2ax^2=a-x\Rightarrow x^4-2ax^2+x+a^2-a=0$ Now How can i solve it after that, Help me Thanks",['algebra-precalculus']
1756135,Is there a way to write an infinite set that contains only irrational numbers without integer multiples?,"Is there a way to write an infinite set that contains only irrational numbers without integer multiples? The infinite set must not contain integer multiples of any other members of that set. For example,$\pi$ is a member, but we cannot have $2\pi, 3\pi$, and so on. Same applies for any other irrational number in the set. Also, that infinite set must be equinumerous to $\mathbb{N}$ (natural numbers). This seems intuitive to me, as there are many ways to line up infinite sets with $\mathbb{N}$. But I am having trouble thinking of such an infinite sets regarding only irrationals. Thanks.","['discrete-mathematics', 'irrational-numbers', 'elementary-number-theory']"
1757341,Does weak convergence ($X_n\Rightarrow X$) imply weak convergence of the difference to zero ($X_n-X\Rightarrow 0$)?,"Let $(X_n)_{n\geq1}$ be a sequence of random variables weakly converging to $X$ ($X_n\Rightarrow X$) as $n\rightarrow \infty$. I am wondering if this implies that $X_n-X\Rightarrow 0$ as $n\rightarrow \infty$? I know the converse is true via continuous mapping, but I would like to know if the two statements are actually equivalent? I believe it should be true as we could use the Skorokhod representation theorem to get the almost sure convergence of the difference to $0$, and thus the weak convergence of the difference to $0$, but I am not sure if this is correct? Any ideas or comments would be greatly appreciated. edit: my reasoning using SRT was as follows: $X_n\Rightarrow X$ so there exist copies $Y_n$ and $Y$ with respective laws equal to those of $X_n$ and $X$ on another probability space (say $(\Omega,\mathcal{A}, \mathbb{Q}))$ such that $Y_n\rightarrow Y$ $\mathbb{Q}$-a.s. As $Y\rightarrow Y$ $\mathbb{Q}$-a.s, then $Y_n-Y\rightarrow 0$ $\mathbb{Q}$-a.s and therefore $Y_n-Y\Rightarrow 0$, and $X_n-Y\Rightarrow 0$. There must be a mistake here somewhere, but I can't seem to see where?","['weak-convergence', 'probability-limit-theorems', 'probability-theory', 'probability']"
1757357,Combination of even and odd functions,"Can someone help me how to show that any function $f(x)$ defined on a symmetrically placed interval can be written as a sum of an even and a odd function? What is the special role played by ""symmetrically placed interval"" here?","['symmetry', 'fourier-series', 'functions']"
1757399,About birational map between curves of the form $y^{2}=g(x)$,"I'm trying to solve the following exercise but I'm stuck after trying for a long time. Suppose that $g(x)=ax^{4}+bx^{3}+cx^{2}+dx+e\in{k[x]}$ and similarly $g'(x)=a'x^{4}+b'x^{3}+c'x^{2}+d'x+e'\in{k[x]}$. Assume that $C:y^{2}=g(x)$ and $C':y^{2}=g'(x)$ are nonsingular curves (of genus 1) and that there is an $k$-isomorphism $\phi:C\rightarrow{C'}$. The problem is to find explicitly what is $\phi$. In the proof there are three steps that I don't understand. First, one should take a root $\xi$ of $g(x)$. Then the claim is that $\phi$ should map the point $(\xi,0)$ to a point of the form $(\xi',0)$ where $\xi'$ is a root of $g'(x)$. My quetion is why this happens? Why the image is not a point of the form $(a,b)$ with $a$ not a root of $g'(x)$ and $b\neq{0}$?. Now once we know that the image of $(\xi,0)$ is of the form $(\xi',0)$, then it follows that $\phi$ maps the roots of $g(x)$ bijectively onto the roots of $g'(x)$. This is ok. But then it follows that $g(x)=\lambda^{2}(\gamma{x}+\delta)^{4}g'(\frac{\alpha{x}+\beta}{\gamma{x}+\delta})$ for some $\alpha,\beta,\gamma,\delta,\lambda\in{k}$. Why this happens?? There must be some bilinearity involved as a consequence of the bijection between roots of $g(x)$ and $g'(x)$ but I can't clearly see what is happening here. Finally, we should have that $\begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}$ is an invertible matrix and then from this I am supposed to find $\phi$. My guess is that $\phi$ is of the form $(x,y)\mapsto(\frac{\alpha{x}+\beta}{\gamma{x}+\delta},\frac{\lambda^{-1}y}{(\gamma{x}+\delta)^{2}})$ or something like this but I'm not even sure if this makes sense (due to $(\gamma{x}+\delta)^{2}$). I will deeply appreciate any help about this.","['algebraic-curves', 'curves', 'algebraic-geometry']"
1757448,"Evaluation of $ \int_{0}^{\pi}\ln(5-4\cos x)\,dx$","Evaluation of $\displaystyle \int_{0}^{\pi}\ln(5-4\cos x)dx = \int_{0}^{\pi}\ln(5+4\cos x)dx$ $\bf{My\; Try::}$ Let $\displaystyle I(a,b) = \int_{0}^{\pi}\ln(a+b\cos x)dx$ Then $$\frac{d}{db}(a,b) = \frac{d}{db}\left[\int_{0}^{\pi}\ln(a+b\cos x)dx\right]db$$ So $$I'(a,b) = \int_{0}^{\pi}\frac{\cos x}{a+b\cos x}dx = \frac{1}{b}\int_{0}^{\pi}\frac{(a+b\cos x)-a}{a+b\cos x}dx$$ So we get $$I'(a,b) = \frac{\pi}{b}-\frac{a}{b}\int_{0}^{\pi}\frac{1}{a+\cos x}dx$$ Using half angle formula $\displaystyle \tan x = \frac{1-\tan^2 x/2}{1+\tan^2 x/2}$ so we get $$I'(a,b) = \frac{\pi}{b}-\frac{a}{b}\int_{0}^{\pi}\frac{\sec^2 x/2}{(a+b)+(a-b)\tan^2 x/2}dx$$ Now Put $\tan x/2= t\; $ Then $\displaystyle \sec^2 \frac{x}{2}dx = 2dt$ So we get $$I'(a,b) = \frac{\pi}{b}-\int_{0}^{\infty}\frac{1}{(a+b)+(a-b)t^2}=\frac{\pi}{b}-\frac{a\pi}{2b\sqrt{a^2-b^2}}$$ So we get $$I(a,b) = \pi\ln|b|-\frac{\pi a}{2}\left[-\frac{\ln|b^2-a^2|+2\ln |b|}{2a^2}\right]$$ So we put $a = 5$ and $b=4$ We get $$I(5,4) = \pi\cdot \ln (5)-\frac{\pi}{2}\left[\frac{-\ln(9)+2\ln(5)}{2\cdot 5}\right]$$ So We get $$I(5,4) = \left[\frac{18\ln(5)+\ln(9)}{20}\right]\cdot \pi$$ I did not understand where i have done mistake, Help me Thanks",['integration']
1757465,Let $A$ be a Banach algebra. Suppose that the spectrum of $x\in A$ is not connected. Prove that $A$ contains a nontrivial idempotent $z$.,"While trying to solve the exercise below, I came up with a wrong conclusion, but I can't see why it's wrong. Also I'm accepting suggestions to get the right solution. This is the problem 17 from chapter 10 of Rudin's Functional Analysis. Let $A$ be a Banach algebra. Suppose that the spectrum of $x\in A$ is not connected. Prove that $A$ contains a nontrivial idempotent $z$. My attempt: Let $F_1, F_2$ be two disjoint closed non-empty sets in $\sigma(x)$ such that $F_1\cup F_2 = \sigma(x)$. There is a function $f$ defined over a neighborhood $\Omega$ of $\sigma(x)$ such that $f=1$ in $F_1$ and $f=0$ in $F_2$. Denote $$\tilde{f}(x) = \frac{1}{2\pi i}\int_\Gamma f(\lambda)(\lambda e-x)^{-1}\ d\lambda,$$
which comes from the functional (or symbolic) calculus. $\Gamma$ is a contour of $\sigma(x)$ in $\Omega$. The idea is to show that $\tilde{f}(x)$ is idempotent. This idea of proof was used in some books and I'm trying to follow it. My problem is this: it's clear that $f(\sigma(x)) = F_1$ from the very definition of $f$. I also know that $\sigma(\tilde{f}(x)) = f(\sigma(x)) = F_1$. But from this post, for instance, we have that the spcetrum of idempotents elements is $\{0,1\}$. If $\tilde{f}(x)$ would be idempotent, then $F_1=\{0,1\}$, but this is not necessarily the case. Thank you for your help.","['functional-analysis', 'operator-algebras', 'banach-algebras']"
1757476,Why is this determinant positive?,"I have seen that the $k$-dimensional volume of an parallelepiped in $\mathbb{R}^n$, i.e., $$P(v_1, \ldots, v_k) = \{t_1v_1 + \dotsb + t_kv_k : 0 \le t_i \le 1 \}$$ is $\sqrt{\det(T^{\top}T)}$, where $T$ is the $n\times k$ matrix with columns $v_1, \ldots, v_k$. How do we know that $\det(T^{\top}T)$ is non-negative?","['matrices', 'positive-semidefinite', 'linear-algebra', 'determinant']"
1757486,Relationship between invariant and harmonic forms,"Let $G$ be a compact real Lie group which is also a complex manifold (note that I do not want $G$ to be a complex Lie group). Let $\Omega^{p , q}(G)$ denote the space of smooth $(p , q)$-forms on $G$, and let $\Omega_L^{p , q}(G)$ (resp. $\Omega^{p , q}_I(G)$) denote the space of left-invariant (resp. bi-invariant) $(p , q)$-forms. Suppose $G$ has a bi-invariant hermitian metric. Then one can define the Hodge-$*$ operator (See Griffiths and Harris, page 82), and therefore the adjoint operators $\partial^*: \Omega^{p , q}(G) \to \Omega^{p - 1 , q}(G)$ and $\overline{\partial}^*: \Omega^{p , q}(G) \to \Omega^{p , q - 1}(G)$, and the Laplacians: $\Delta_\partial = \partial^\ \partial^* + \partial^* \partial^\ : \Omega^{p , q}(G) \to \Omega^{p , q}(G)$ and $\Delta_\overline{\partial} = \overline{\partial}^\ \overline{\partial}^* + \overline{\partial}^* \overline{\partial}^\ : \Omega^{p , q}(G) \to \Omega^{p , q}(G)$. Let $\mathcal{H}_\partial^{p , q}(G)$ and $\mathcal{H}_\overline{\partial}^{p , q}(G)$ denote the kernels of these operators, respectively. Forms that belong to theses spaces are called Harmonic. What is the relationship, if any, between the spaces $\Omega^{p , q}_L(G)$, $\Omega^{p , q}_I(G)$ , $\mathcal{H}_\partial^{p , q}(G)$, and $\mathcal{H}_\overline{\partial}^{p , q}(G)$ (all of which are contained in $\Omega^{p , q}(G)$)? In particular, is it true that all Harmonic forms are left-invariant? I don't want to assume anything about $G$ being Kähler, if I can help it.","['complex-geometry', 'hodge-theory', 'differential-geometry', 'lie-groups']"
1757525,Graphing the surface $z = xy$,"Let the surface $S \subset \mathbb{R}^3$ be the graph of the function $f:\mathbb{R}^2 \to \mathbb{R},
f (x, y) = xy$. Let $U$ be the portion of $S$ for which $x^2 + y^2 ≤ 2$ and let $C$ be the boundary curve of $U$. Sketch $S, U $ & $ C$. I've tried to use slices to draw $S$. So $z = xy$. Letting $x = 1$, then S in the $zy$-plane is the whole plain. Letting $x = 1$ then the $zy$-plane $S$ is the whole plane again; and letting $z = 1$ then $xy$-plane is occupied a series of hyperbolas. But how do I graph these together? Also, could someone please explain the concept of a boundary to me? Guessing - is it the set of inequalities that bound a graph? EDIT: Does it look like this?","['multivariable-calculus', 'calculus', 'solid-geometry']"
1757527,"Let the sequence of functions $f_n(x)$ equal $1$ if $x ∈ [n, n + 1)$ and $0$ otherwise. Why doesn't $f_n$ converge uniformly?","Let the sequence of functions $f_n(x)$ equal $1$ if $x ∈ [n, n + 1)$ and $0$ otherwise. How can I use the deﬁnition of uniform convergence to show that $f_n$ does not converges uniformly? If a function is uniformly convergent, then for all $\epsilon > 0$ there exists an integer $N$ such that for all $n \geq N$ adn for all $x$, $|f_n(x) - f(x)| < \epsilon$.  Therefore we wish to find some $\epsilon > 0$ such that for each $n$ there is an $x ∈ [n, n + 1)$ such that $|f_n(x) - f(x)| \geq \epsilon$.  But how can I do this ?","['real-analysis', 'uniform-convergence', 'analysis']"
1757528,Existence of an $n\times n$ real matrix $A$ such that $A^2=-I$.,"Let $A$ be a $n\times n$ real matrix $A$ such that $A^2=-I$. Such an $A$ cannot be, Orthogonal. Invertible. Skew-symmetric. Symmetric. Diagonalizable. I tried to figure out the answer by looking at the [possible] determinant of $A$, using the fact $\det(AB)=\det(A)\det(B)$. So $\det(A^2)=(\det(A))^2=(-1)^n\det(I)$. If $n$ is even, then $\det(A)=\pm1$ and if odd then $\det(A)=\pm i$. There I stuck, if $n$ is even, I can try some guessing, but when $n$ is odd, the determinant becomes complex and I have no logic to go forward. Is there any other approach that might be more correct? How can I do this? Help me out.","['matrices', 'linear-algebra', 'determinant']"
1757559,Eigenvalues of block matrix of order $m+1$,"How to find eigenvalues of following matrix? $\begin{bmatrix}
mkI-A & -A & -A & \cdots & -A\\
-A & kI-A & O & \cdots & O\\
-A & O & kI-A & \cdots & O\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
-A & O & O & \cdots & kI-A\\
\end{bmatrix}_{m+1}$ Where,
$A$ is any square matrix of order $n$, $O$ is zero matrix of order $n$ $I$ is identity matrix of order $n$, $k \in \mathbb{N}$ As well as one eigenvalue of above matrix is zero. I think if we can convert above matrix in terms of either Kronecker product or Kronecker sum of two matrices then we can find eigenvalues of above matrix by taking multiplication or addition of two matrices respectively. The other way is might be if we can convert above matrix in block diagonal matrix then we can find eigenvalues easily.","['matrices', 'eigenvalues-eigenvectors']"
1757564,"Is my proof correct? If $f$ has a finite number of discontinuities on $[a, b]$, then it is integrable on $[a, b]$","Question: Suppose a function $f(x)$ over the interval $[a, b]$ is bounded and has only a finite number of discontinuous points on $[a, b]$.  I intend to prove that it must be integrable on $[a, b]$. Is my proof below correct? My Answer (Is it correct?): Since $f(x)$ has only a finite number of discontinuous points, call these points $p_1, p_2, ..., p_n$.  Now let $r$ be some number greater than $0$ such that for all $\epsilon > 0$, $2r < \frac{\epsilon}{4M}$.  Now let $M_i =$ sup$_{x \in [p_i - r, p_i +r]}f(x)$ and $m_i =$ inf$_{x \in [p_i - r, p_i +r]}f(x)$. Then $M_i - m_i \leq 2M$. Now then $f$ must be continuous on each interval $[a, p_1 - r], [p_1 + r, p_2 - r], ...,[p_n + r, b]$ and thus $f$ must be integrable on each one of these intervals.  Therefore there exist partitions $P_1, P_2,..., P_{N+1}$ of each of these intervals such that $U(P_k, f) - L(P_k, f) < \frac{\epsilon}{2(N + 1)}$. $U(P_k, f)$ and $L(P_k, f)$ are the upper and lower sums of $f$ over their respective partitions. Let $P$ be the partition given by the $P_1 \cup P_2 \cup ... \cup P_{N+1}$.
Then $U(P, f) - L(P, f) = U(P_1, f) - L(P_1, f) + U(P_2, f) - L(P_2, f) + ... U(P_{N + 1}, f) - L(P_{N + 1}, f) + M_1 - m_1 + M_2 - m_2 + ... M_N - m_N < 2M(\frac{\epsilon}{4M}) + \frac{\epsilon}{2(N + 1)}(N + 1) = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$. Thus for all $\epsilon > 0$, there exists a partition $P$ of $[a, b]$ such that $U(f, P) - L(f, P) < \epsilon$. Therefore $f(x)$ is integrable over $[a, b]$. Is this proof correct ?","['real-analysis', 'calculus', 'proof-verification', 'proof-writing', 'integration']"
1757570,Find general formula for $\sum _{i=1}^{n} \frac {(-1)^i i}{(2i-1)(2i+1)}$,"I was able to find formulas for simpler expressions but I can't find the general formula for this one: $\sum _{i=1}^{n} \frac {(-1)^i i}{(2i-1)(2i+1)}$ I don't see any particular trend that would help me in the first solutions. $P(1) = -1/3, P(2) = -3/15, P(3) = -36/175 ...$ And if I expand the sum I also fail to come up with an answer.
Any hints? Thanks","['summation', 'sequences-and-series']"
1757582,Higher Order Derivative Tests in Multiple Dimensions,"To evaluate the minima, maxima, and saddle points of a real function of 2 variables, we use the second derivative test after evaluating the critical points to identify the type of extrema they are. Recall that given a function $f(x,y)$ and the Hessian matrix function $H(f)$, the second derivative test tells us If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) < 0$, then $(x_0,y_0)$ is a local maximum. If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) > 0$, then $(x_0,y_0)$ is a local minimum. If $det(H)(x_0,y_0)< 0$, then $(x_0,y_0)$ is a saddle point. If $det(H)(x_0,y_0)=0$, then the test is inconclusive. What I am most interested in is the last of these bullets. What is there left to do besides look at the graph in case the second derivative test yields inconclusive results? For single-variable functions, you would use a higher order derivative. However, a higher order derivative would have $2^o$ derivatives (for order $o$), so what calculation would be necessary to identify the type of critical point in this case?","['multivariable-calculus', 'partial-derivative', 'derivatives']"
1757583,"How to show that $(X(t)-\lambda t)^2 - \lambda t$ is a martingale, where $X(t)$ is a Poisson Process?","I am trying to show that $(X(t)-\lambda t)^2 - \lambda t$ is a martingale, where $X(t)$ is a Poisson process with rate $\lambda$. So far, what I have done is:
\begin{align*}
E\left((X(t)-\lambda t)^2 - \lambda t| \mathcal{F}_s\right)&= E\left(X(t)^2-2\lambda t X(t) + \lambda^2 t^2 - \lambda t| \mathcal{F}_s\right) \\
&=E\left((X(t)-X(s))^2| \mathcal{F}_s\right)  + E\left(2X(t)X(s)| \mathcal{F}_s\right)  - E\left(X(s)^2| \mathcal{F}_s\right) - E\left(2X(t)\lambda t| \mathcal{F}_s\right) \\
& + E\left(\lambda^2 t^2| \mathcal{F}_s\right)  -\lambda t\\
&=E\left((X(t)-X(s))^2\right)  + 2X(s)E\left(X(t)| \mathcal{F}_s\right)  - X(s)^2 -\lambda t - 2 \lambda t E\left(X(t)| \mathcal{F}_s\right)+ \lambda^2 t^2\\
&= 2\lambda t - 2\lambda tX(s) + 2X(s)\left(X(s)+ \lambda(t-s)\right) -\lambda t - 2\lambda t\left(X(s) + \lambda (t-s)\right)+ \lambda^2 t^2\\
\end{align*} I am not sure why my result is not working out. Does anyone have any ideas what I am doign wrong? Thanks!","['stochastic-processes', 'poisson-process', 'probability-theory', 'martingales']"
1757616,Paired Observation Hypothesis Testing,"I'm doing a project presentation for applied statistics about hypothesis testing. Me and my partner claimed that the average amount of money that people spend per month on groceries is less than or equal to the average amount spent on dining out. We've used survey monkey (a free online survey tool) to receive data from people and ended up receiving 34 responses. The survey asks the average amount of money per month that they spend on groceries and dining out. This is a paired observation therefore I used a paired t-test to draw conclusion from the data. I have two questions: 1.) The formula has d0, and d0 = u1 - u2. Now I'm not sure if the value that I gave 
d0 = 0 is correct. d0 would definitely be equal to zero if my null hypothesis was u1 = u2, since the difference between the mews will be zero. 2.) Is it okay to assume that the population of the amount of money spent per month on groceries and dining out are normally distributed? Even though I don't know the real shape of the two population distributions? Any thoughts or comments about this? The image below contains my work and conclusion. The image contains my work and the conclusion. Also below is the data that we got from the survey. The green is the difference between the average money spent on groceries and dining out. Here is my updated work and conclusion",['statistics']
1757648,"Triple Integral $\iiint\limits_Wz\frac{e^{2x^2+2y^2}}2\,dx\,dy\,dz$ where $W=\{(x,y,z):x^2+y^2=1,0\le z\le4\}$","They ask me find the following: $W$ is the solid bounded by the limited right circular cylinder: $$ x^2+y^2=1$$ and the planes: $$z=0, z=4$$
must calculate:
$$\iiint_W z\frac{e^{2x^2+2y^2}}{2}\,dx\,dy\,dz$$
My procedure was as follows, I have in cylindrical coordinates:
$$ x = r\cos(\theta ), y = r\sin(\theta ), z = z, r^2=x^2+y^2$$
therefore it poses work well
$$\int_0^{2\pi}\int_0^1\int_0^4z\frac{e^{2r^{2}}}{2}r\,dz\,dr\,d\theta$$
but at this point I find problems to develop this integral as I could not develop any method; by parts or change by Fubini. Any advice will be of much help, thanks in advance!","['multivariable-calculus', 'multiple-integral', 'integration', 'definite-integrals']"
1757677,I have a answer to a question about trace. Is there an easier answer to this question?,"Let $A\in M_n(\mathbb{C})$. Show that
$$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ My answer: It is easy to see that
$$tr\left(\frac{A+A^*}{2}\right)=\text{Re}(tr(A))\qquad and\qquad tr((A^*A)^{1/2})=\sum_{i=1}^n\sigma_i(A),$$
where $\sigma(A)$ is the singular value of $A$. On the other hand, based on Polar decomposition theorem, there exists a unitary matrix $P$ such that $A=P((A^*A)^{1/2})$. Now, according to [Theorem 8.7.6, Page 551, R. Horn, C. Johnson, Matrix analysis, Second edition], we have:
$$\begin{eqnarray}
 \text{Re}(tr(A))&=\text{Re}(tr(P((A^*A)^{1/2})))\\
&\leq\sum_{i=1}^n \sigma_i(P)\sigma_i ((A^*A)^{1/2}).
 \end{eqnarray} $$
Since $P$ is unitary, $\sigma_i(P)=1$. We have also $\sigma_i ((A^*A)^{1/2})=\sigma_i(A)$. So,
$$ \text{Re}(tr(A))\leq\sum_{i=1}^n \sigma_i(A).$$
Therefore, we get
$$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ Now,  is there an easier answer to this question?","['matrices', 'linear-algebra', 'analysis']"
1757712,epsilon-N proof of divergent sequence with another divergence assumed,"I'd like to prove the following proposition:
$$
\lim_{n\rightarrow\infty} a_n = \infty \Rightarrow \lim_{n\rightarrow\infty} b_n = \infty \\
\text{where}\;b_n = \dfrac{1}{n}\sum_{k=1}^{n}a_k,
$$ but my proof was stuck. Would you tell me how should I finish the proof? Suppose $\lim_{n\rightarrow\infty} a_n = \infty$ ($\forall\epsilon\in\mathbb{R},\,\exists N\in\mathbb{N},\,\forall n\in \mathbb{N},\,n\geq{}N\Rightarrow a_n > \epsilon$), then I'll prove $\lim_{n\rightarrow\infty} b_n = \infty$. Let $\epsilon\in \mathbb{R}$.
For $n\geq N$,
$b_n=\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{1}{n}\sum_{k=N+1}^{n}a_k>\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{n-N}{n}\epsilon=\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon) + \epsilon,$ from the supposition. In order to prove $b_n>\epsilon$, I only prove $\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon)\geq0$ ($N_0$ exists and for all $n\geq N_0$), but $a_n-\epsilon$ for $n < N$ can't be proved. What's wrong? And how do I let $N_0$?","['epsilon-delta', 'real-analysis', 'sequences-and-series']"
1757733,Is $\frac{1}{11}+\frac{1}{111}+\frac{1}{1111}+\cdots$ an irrational number?,"Obviously: $$\frac{1}{10}+\frac{1}{100}+\frac{1}{1000}+\cdots=0.1111\dots=\frac{1}{9}$$ is a rational number. Now, if we make terms with demoninators in the form: $$q_n=\sum_{k=0}^{n} 10^k$$ Then the sum will be: $$\sum_{n=1}^{\infty}\frac{1}{q_n}=\frac{1}{11}+\frac{1}{111}+\frac{1}{1111}+\cdots=0.1009181908362007\dots$$ The decimal expansion of this number appears to be non-periodic. How can we prove/disprove that this number is irrational? Edit This number is at OEIS: http://oeis.org/A065444","['rational-numbers', 'sequences-and-series', 'irrational-numbers', 'elementary-number-theory']"
1757735,Theorem 3.17 in Baby Rudin: Infinite Limits and Upper and Lower Limits of Real Sequences,"Here're Definitions 3.15 and 3.16 and Theorem 3.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Definition 3.15: Let $\{s_n \}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$. We then write $$s_n \rightarrow +\infty.$$ Similarly, if for every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \leq M$, we write $$s_n \rightarrow -\infty.$$ Definition 3.16: Let $\{ s_n \}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \rightarrow x$ for some subsequence $\{s_{n_k}\}$. This set $E$ contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers $+\infty$, $-\infty$. We now recall Definitions 1.8 and 1.23 and put $$s^* = \sup E,$$ $$s_* = \inf E.$$  The numbers $s^*$, $s_*$ are called the upper and lower limits of $\{ s_n \}$; we use the notation $$\lim_{n\to\infty} \sup s_n = s^*, \ \ \ \lim_{n\to \infty} \inf s_n = s_*.$$ Now, Definition 3.5: Given a sequence $\{ p_n \}$, consider a sequence $\{n_k\}$ of positive integers, such that $n_1 < n_2 < n_3 < \cdots$. Then the sequence $\{p_{n_k}
 \}$ is called a subsequence of $\{p_n \}$. If $\{p_{n_k}\}$ converges, its limit is called a subsequential limit of $\{p_n \}$. Definition 1.8: Suppose $S$ is an ordered set, $E \subset S$, and $E$ is bounded above. Suppose there exists an $\alpha \in S$ with the following properties: (i) $\alpha$ is an upper bound of $E$. (ii) If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $E$. Then $\alpha$ is called the least upper bound of $E$ or the supremum of $E$, and we write $$\alpha = \sup E.$$ The greatest lower bound, or infimum, of a set $E$ which is bounded below is defined in the same manner: The statement $$\alpha = \inf E$$ means that $\alpha$ is a lower bound of $E$ and that no $\beta$ with $\beta > \alpha$ is a lower bound of $E$. Definition 1.23: The extended real number system consists of the real field $R$ and two symbols, $+\infty$ and $-\infty$. We preserve the original order in $R$, and define $$-\infty < x < +\infty$$ for every $x \in R$. It is then clear that $+\infty$ is an upper bound of every subset of the extended real number system, and that every nonempty subset has a least upper bound. If, for example, $E$ is a nonempty set of real numbers which is not bounded above in $R$, then $\sup E = +\infty$ in the extended real number system. Exactly the same remarks apply to lower bounds. Finally, here's Theorem 3.17. Let $\{s_n \}$ be a sequence of real numbers. Let $E$ and $S^*$ have the same meaning as in Definition 3.16. Then $s^*$ has the following two properties: (a) $s^* \in E$. (b) If $x> s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n < x$. Moreover, $s^*$ is the only number with the properties (a) and (b). Of course, an analogous result is true for $s_*$. And, here's Rudin's proof: (a) If $s^* = +\infty$, then $E$ is not bounded above; hence $\{s_n\}$ is not bounded above, and there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k} \rightarrow +\infty$. If $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems 3.7 and 2.28. [I'll state these theorems after the proof. ] If $s^* = -\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M$, $s_n > M$ for at most a finite number of values of $n$, so that $s_n \rightarrow -\infty$. This establishes (a) in all cases. (b) Suppose there is a number $x > s^*$ such that $s_n \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x > s^*$, contradicting the definition of $s^*$. Thus $s^*$ satisfies both (a) and (b). To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy (a) and (b), and suppose $p < q$. Choose $x$ such that $p < x < q$. Since $p$ satisfies (b), we have $s_n < x$ for $n \geq N$. But then $q$ cannot satisfy (a). Now for Theorem 2.28: Let $E$ be a nonempty set of real numbers which is bounded above. Let $y = \sup E$. Then $y \in \overline{E}$. Hence $y \in E$ if $E$ is closed. And, Theorem 3.7: The subsequential limits of a sequence $\{p_n\}$ in a metric space $X$ form a closed subset of $X$. My reason for copying so many definitions and theorem from Rudin's book into my question is to keep my post as self-contained as possible. Now here's my attempt at filling in the details in the proof of Theorem 3.17. (a)  If $s^* = +\infty$, then the set $E$ is not bounded above in $\mathbb{R}$. We now find a subsequence of $\{s_n \}$ which diverges to $+\infty$. As $E$ is not bounded above in $\mathbb{R}$, so there is an element $x_1 \in E$ such that $x_1 > 1$. If $x_1 = +\infty$, then we are done. Otherwise, $x_1$ is a subsequential limit of $\{s_n\}$. Thus there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_1$ such that $n > N_1$ implies that $$ \left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon.$$ Let's take $\varepsilon$ such that $$ 0 < \varepsilon < \frac{x_1 - 1}{2}.$$ Then we can conclude that $$s_{\varphi_1(N_1 + 1)} > 1.$$ Let $$n_1 \colon= \varphi_1(N_1+1).$$ Then $$s_{n_1} > 1.$$ 
  Now as $E$ is not bounded above in $\mathbb{R}$, there is an element $x_2 \in E$ such that $x_2 > 2$. If $x_2 = +\infty$, then we are done. Otherwise, $x_2$ is a subsequential limit of $\{s_n\}$. That is, there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n > N_2$ implies that $$ \left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon. $$ Thus, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_2 - 2}{2},$$ then we can conclude that $$s_{\varphi_2(N_2 + 1)} > 2.$$ Let's put $$n_2 \colon= \max \left\{ \varphi_2( n_1 +1 ), \varphi_2(N_2+1) \right\} .$$ We note that since $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$n \leq \varphi_2(n)$$ for all $n \in \mathbb{N}$. So we have $$n_2 \geq \varphi_2(n_1 + 1) \geq n_1 + 1 > n_1.$$ Now we can find an element $x_3 \in E$ such that $x_3 > 3$. If $x_3 = +\infty$, then we are done. Otherwise, $x_3$ is a subsequential limit of $\{s_n \}$. That is, there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_3 = \lim_{n \to \infty} s_{\varphi_3(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies that $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ So, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_3-3}{3},$$ then we can conclude that $$s_{\varphi_3(N_3+1)} > 3.$$
  Let's take $$n_3 \colon= \max \left\{ \varphi_3 ( n_2+1), \varphi_3 (N_3 + 1) \right\}.$$ Since $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$ n \leq \varphi_3(n) $$ for all $n \in \mathbb{N}$. So 
  $$n_3 \geq \varphi_3(n_2 + 1) \geq n_2 + 1 > n_2.$$ In this way, we obtain a sequence $\{n_k\}$ of natural numbers such that  $n_1 < n_2 < n_3 < \cdots$ and $$s_{n_k} > k$$ for all $k \in \mathbb{N}$. Now for any real number $M$, there is a natural number $K$ such that $K > M$. So, for all $k \in \mathbb{N}$ such that $k > K$, we have $$s_{n_k} > k > K > M;$$ that is, $s_{n_k} > M$ for all $k \in \mathbb{N}$ such that  $k > K$. Therefore, we can conclude that $$s_{n_k} \rightarrow +\infty,$$ which shows that $E$ contains $+\infty$. If $s^* \in \mathbb{R}$, then $E$ is bounded above in $\mathbb{R}$ and $s^* = \sup E$. We show that some subsequence of $\{s_n \}$ converges to $s^*$. There exists an element $x_1 \in E$ such that $s^*-1 < x_1 \leq s^*$. If $x_1 = s^*$, then we are done. Otherwise we have $s^*-1 < x_1 < s^*$. Now $x_1$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ Thus, for any real number $\varepsilon > 0$ we can find a natural number $N_1$ such that $n > N_1$ implies $$\left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon. $$ By taking $\varepsilon$ such that $$ 0 < \varepsilon < \frac 1 3 \min \{ s^* - x_1, \ x_1 - s^* + 1 \},$$  we can conclude that $$s^* -1 < s_{\varphi_1(N_1 + 1) } < s^*.$$ Let's take $$n_1 \colon= \varphi_1(N_1 + 1).$$ As $s^* = \sup E$ in $\mathbb{R}$, there is an element $x_2 \in E$ such that $s^* - \frac 1 2 < x_2 \leq s^*$. If $x_2 = s^*$, then we are done. Otherwise we have $$s^*- \frac 1 2 < x_2 < s^*.$$ Now $x_2$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, given a real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n>N_2$ implies that $$\left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that 
  $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_2 , \ x_2 - s^* + \frac 1 2 \right\},$$ we can conclude that $$s^* - \frac 1 2 < s_{\varphi_2(N_2 + 1)} < s^*.$$ Let's take $$n_2 \colon= \max \left\{ \varphi_2(n_1 + 1), \varphi_2(N_2+ 1) \right\}.$$ Then $n_2 > n_1$ and $$s^* - \frac 1 2 < s_{n_2} < s^*.$$ Now as $s^* = \sup E$ in $\mathbb{R}$, we can find an element $x_3 \in E$ such that $$s^* -\frac 1 3 < x_3 \leq  s^*.$$ If this $x_3 = s^*$, then we are done. Otherwise we have $$s^* - \frac 1 3 < x_3 < s^*.$$ Moreover, $x_3$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_3(n)}.$$  Thus, given a real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that
  $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_3 , \ x_3 - s^* + \frac 1 3 \right\},$$ we can conclude that $$s^* - \frac 1 3 < s_{\varphi_3(N_3 + 1)} < s^*.$$ Let's take $$n_3 \colon= \max \left\{ \varphi_3(n_2 + 1), \varphi_3(N_3+1) \right\}.$$ Then $n_3 > n_2$ and $$s^* - \frac 1 3 < s_{n_3} < s^*.$$ Continuing in this way, we obtain a sequence $\{n_k \}$ of natural numbers such that $n_1 < n_2 < n_3 < \cdots$ and $$s^* - \frac 1 k < s_{n_k} < s^*$$ for all $k \in \mathbb{N}$. From here, we can show that the subsequence $\{s_{n_k}\}$ of $\{s_n\}$ converges to $s^*$. If $s^* = -\infty$, then there is no subsequential limit of $\{s_n\}$. Moreover, $\{s_n \}$ is bounded above in $\mathbb{R}$, for otherwise there would be a subsequence of $\{s_n \}$ which would diverge to  $+\infty$. Let $M$ be any real number. If we were to have  $s_n \geq M$ for infinitely many values of $n \in \mathbb{N}$, then we could find a subsequence of $\{s_n\}$ which would be bounded and hence have a convergent subsequence (by Theorem 3.6 (b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$ whose limit $x$ would satisfy $x \in E$ and $x > s^*$, a contradiction. So we must have $s_n \geq M$ for at most a finite number of values of $n$. Thus, there is a natural number $N$ such that $n > N$ implies that $s_n < M$. Since $M$ was an arbitrary real number, we can conclude that $$s_n \rightarrow -\infty, $$ which shows that $-\infty \in E$. Thus we have shown that $s^* \in E$ in all possible cases. Now part (b): We assume the contrary. That is, we assume that $x > s^*$ and that there are infinitely many $n$ such that $s_n \geq x$. Since $s^* < x$, $s^*$ cannot be $+\infty$. Moreover, there is a subsequence  $\{s_{\varphi(n)}\}$ of $\{s_n\}$ such that $$s_{\varphi(n)} \geq x$$
  for all $n \in \mathbb{N}$, where $\varphi \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function. If the subsequence $\{s_{\varphi(n)}\}$ is not bounded above, then some subsequence of  $\{s_{\varphi(n)}\}$ --- which would in turn be a subsequence of  $\{s_n\}$ ---  would diverge to $+\infty$, showing that $+\infty \geq x > s^*$ and $+\infty \in E$, a contradiction. On the other hand, if the subsequence  $\{s_{\varphi(n)}\}$ is bounded above, then it would be a bounded sequence in $\mathbb{R}$ and so would have a convergent subsequence (by Theorem 3.6(b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$, whose limit $s$ would satisfy $s \in E$ and $s \geq x > s^*$, which is a contradiction to the fact that $s^*$ is the supremum of $E$. To show that $s^*$ is the only number with the properties (a) and (b), suppose that there are two numbers $p$ and $q$ which satisfiy properties (a) and (b) of Theorem 3.17 in Rudin, and suppose that $p < q$. Let's choose a real number $x$ such that $p < x < q$. Then by property (b), there is an integer $N$ such that $s_n < x$ for all $n \in \mathbb{N}$ such that $n \geq N$. Let us chooes a real number $\varepsilon$ such that $$0 < \varepsilon < \frac{ q-x}{3}.$$ Then there are at most a finite number of terms of $\{s_n\}$ in the neighborhood $(q-\varepsilon, q+ \varepsilon)$. So no subsequence of $\{s_n \}$ can converge to $q$ (or diverge to $+\infty$ if $q = +\infty$), showing that $q \not\in E$, which is a contradiction to the property  (a) for $q$. Based on how I have elaborated Rudin's proof in the last few paragraphs, have I been able to understand this proof correctly? If not, where am I going wrong (or falling short)? What is lacking in my reasoning, apart from brevity of course?","['real-analysis', 'limsup-and-liminf', 'sequences-and-series', 'convergence-divergence', 'analysis']"
1757763,Is this a valid argument for proving that a sum of reciprocals is irrational?,"Suppose we have a strictly increasing sequence of natural numbers. Suppose that the sum of the reciprocals of the elements converges. And suppose that the elements have infinitely many prime factors. Does this imply that the sum of the reciprocals of the elements is irrational? My only thought is - yes: We can never find a common denominator for all elements So there is no way to represent the sum as a simple fraction My motivation comes from this question on $\frac{1}{11}+\frac{1}{111}+\frac{1}{1111}+\dots$ I'm not even sure that $11,111,1111,\dots$ have infinitely many prime factors. But I was still wondering if this argument (assuming it's correct) could be used. Thanks","['number-theory', 'sequences-and-series', 'irrational-numbers']"
1757769,Is there a way to cut a an ellipsoid with a plane such that it gives a circle?,"I'm trying to answer this In $\Bbb {R^3} $ consider the ellipsoid:
  $2x^2+3y^2+4z^2=1$ It exists a subspace of dimension 2 which intersection with the ellipsoid is a circle. Justify any answer. I know that I must search for a plane since these are the only linear subspaces of $\Bbb {R^3} $ of dimension 2, but I don't know what else to do to check if such a plane exists. any hint would be much appreciated. Thanks!","['vector-spaces', 'linear-algebra', 'geometry']"
1757835,Find 10 commuting $2\times 2$ matrices of the same order,"Prove that there exists 10 distinct real $2\times 2$ matrices which are pairwise commuting and all of the same finite order. Here, the order of matrix A is the smallest integer $k > 0$ such that $A^k = I.$  Also by 'pairwise commuting', I mean that if $\{A_1, \cdots, A_{10}\}$ are the ten matrices, then $A_i A_j = A_j A_i$ for any $i,j = 1,2,\cdots, 10$ and $i\neq j.$ Linear algebra is not my forte yet, I haven't got a clue where to begin.  The matrices commute so they all have the same eigenvectors... so letting $A_1$ be a matrix of just these eigenvectors, $A_2 = A_1 + I$ would have the same eigenvectors, thus $A_2A_1 = A_1A_2.$  However I am not sure how to deal with making them the same order.",['linear-algebra']
1757842,What is the horizontal space of trivial hermitian line bundle?,"Suppose $L=M\times\Bbb C$ is a trivial holomorphic line bundle on a complex manifold $M$, and suppose there is a Hermitian fibre metric $h$ on $L$. Question: What is the horizontal space of $T_{(p,z)}L$ with respect to the canonical connection on $L$. I am a bit mixed up with all the definitions there are. Here the canonical connection is given by a connection 1-form, but we need to translate this into a splitting of the tangent space into horizontal and vertical space. There is clearly an obvious splitting of $T_{(p,z)}L$ and I am wondering if this is just that: We have $T_{(p,z)}L=T_{(p,z)}(M\times\Bbb C)=T_pM\oplus\Bbb C$. Is that the desired splitting?","['complex-geometry', 'differential-geometry']"
1757843,Evaluate $\int \frac{1-x}{(1+x)\sqrt{x+x^2+x^3}}dx$,Evaluate $$\int \frac{1-x}{(1+x)\sqrt{x+x^2+x^3}}dx$$ i used substitution $x=\tan^2 y$ so $dx=2\tan y \sec^2 y dy$ so the integral becomes $$I=\int\frac{2\cos 2y\: \tan y\: \sec^2 y \:dy}{\sqrt{\tan^2 y+\tan^4 y+\tan^6 y}}=\int\frac{2\cos 2y  \:\sec^2 y\: dy}{\sqrt{1+\tan^2 y+\tan^4 y}}$$ I was stuck here,"['algebra-precalculus', 'indefinite-integrals']"
1757850,It there an example of sum and product of continuous functions is not continuous?,"The sum and product of two continuous functions is continuous I can prove this easily when the space is metrizable, but I don't get it when the space is non-metrizable. Is there a counterexample of this? or it is true for all topological spaces those have a binary operation? edit: It it true even if the space has a non-contionuous binary operation?",['general-topology']
1757852,How can I show that $G$ is non abelian of order 20?,"Problem says: Let $G=\langle x,y|x^4=y^5=1,x^{-1}yx=y^2\rangle$. Show that $G$ is
  nonabelian group of order 20. To show it, I tried to turn $x^ny^m$ into $y^kx^l$ for some $k,l$. Since I have $yx=xy^2$, $xy^m=(xy^2)y^{m-2}=\cdots =y^kxy^{m-2k}$. So, if $m$ is even, $xy^m=y^{m/2}x$. If $m$ is odd, then $xy^m=y^{(m-1)/2}xy$. So, I have 12 distinct elements of $G$, $1,x,x^2,x^3,y,y^2,y^3,y^4,xy,xy^3,yx,y^2x$. How could I do find more here? And I want to the general way to solve this type of problem.","['abstract-algebra', 'group-theory']"
1757862,Can this differential equation be transformed into an hypergeometric equation?,"$$(1+x^2)y'' -4xy' + 6y = 0 $$ Can this be transformed into an hypergeometric equation of the form $x(1-x)y'' + (c - (a + b + 1)x)y' -aby = 0$? I know that we can do the transform is the term before y'' a polynomial of degree 2, the term before y' of degree 1 and before y is a constant.  Another condition is that the polynomial term of degree two has two distincts roots.  Nothing is said about complex numbers, so I'm not sure.  $ 1 + x^2 $ has two complex roots.  Can I actually transform the equation into an hypergeometric equation? Thank you.",['ordinary-differential-equations']
1757897,Area of Mobius strip,"I want that to give a meaning to the notion of area for Mobius strip. I know that Mobius band is nonorientable surface. How can I set up an integral to compute it? What's your idea for the following formula? $\boldsymbol X(t,\theta)=\left(\left(1-t\sin\dfrac\theta2\right)\cos\theta,\left(1-t\sin\dfrac\theta2\right)\sin\theta,t\cos\dfrac\theta2\right)$ on $Q=\{(t,\theta); \ -1<t<1 , 0<\theta<2\pi\}$ that $X(Q)\approx M$. and Area of M:=$\iint_Q |X_t\times X_{\theta}|dtd\theta$ that $Q=X^{-1}(M)$.",['differential-geometry']
1757930,Combinatorics problem involving n-dimensional space,"Consider a set of more than $\frac {2^{n+1}} {n}$ points $(n>2)$, chosen from the $2^n$ points of the $n$-dimensional space which have the coordinates $\{ \pm1, \pm1, ..., \pm1 \}$. Show that there are $3$ points in the set that form an equilateral triangle.
I don't know how to start, as I am new in combinatorics problems.","['combinatorics', 'combinatorial-geometry']"
1757938,Intersection of sphere and ellipsoid,"Ellipsoid:
$$
x^2+\frac{y^2}{4}+\frac{z^2}{2}=1
$$
Sphere:
$$
x^2+(y-1)^2+(z-d)^2=1
$$
For what values of $d$, there is a common tangent plane to both curves? Part of my resolution: Consider $f(x,y,z)=x^2+\frac{y^2}{4}+\frac{z^2}{2}\qquad$ and $g(x,y,z)=x^2+(y-1)^2+(z-d)^2$. Then, $f(x,y,z)=1$ and $g(x,y,z)=1$ are both level sets and a tangent plane (common) to them is given by, respectively:
$$
\overrightarrow \bigtriangledown f(P_0) \cdot PP_0=0
$$
$$
\overrightarrow \bigtriangledown g(P_1) \cdot PP_1=0
$$
with $P_0=(x_0,y_0,z_0)$ and $P_1=(x_1,y_1,z_1)$ points of tangency. I obtained the planes:
$$
2x_0x+\frac{y_0}{2} y+z_0z=2
$$
$$
x_1x+(y_1-1)y+(z_1-d)z=y_1+d(z_1-d)
$$
Now, for a certain $\lambda \in \mathbb R$, we have:
$$
x_1 = 2 \lambda x_0
$$
$$
y_1-1 = \lambda \frac{y_0}{2}
$$
$$
z_1-d = \lambda z_0
$$
$$
y_1+d(z_1-d)=2 \lambda
$$ I tried to solve this system but there are to many variables...","['partial-derivative', 'plane-curves', 'geometry']"
1757954,Interpretation of confidence interval,Say I have a 95% confidence interval of the sample mean. Does that mean there is a 95% probability that the population mean lies within that interval?,"['statistics', 'confidence-interval']"
1757963,Does there exist a complex function which is differentiable at one point and nowhere else continuous?,"Let $f\colon\mathbb{C}\to\mathbb{C}$ . We know that if $f^{\prime}(a)$ exists for some $a\in\mathbb{C}$ then $f$ is continuous at $a$ . This is because, from the definition of the derivative, $$f(z)-f(a)=[f^{\prime}(a)+\varepsilon(z)](z-a)$$ for some $\varepsilon$ such that $\varepsilon(z)\to0$ as $z\to a$ , and hence $f(z)\to f(a)$ . I think we can interpret this geometrically by saying that, for $z$ close to $a$ , if we take the vector from $a$ to $z$ , rotate it counterclockwise about its tail by $\mathrm{Arg}\,{(f^{\prime}(a))}$ and scale its length by $\lvert f^{\prime}(a)\rvert$ , then we get approximately the vector from $f(a)$ to $f(z)$ (it's only approximate, because of the $\varepsilon$ error term, but for $z$ close to $a$ this $\varepsilon$ is small). However, consider the following heuristic argument: let $z_{1}$ and $z_{2}$ both be ""close to"" $a$ (and therefore close to each other). The above interpretation then suggests that the vector from $f(a)$ to $f(z_{1})$ should be approximately the same as the vector from $f(a)$ to $f(z_{2})$ , and therefore $f(z_{2})-f(z_{1})$ should be small in magnitude. This seems to suggest that $f$ should be continuous not just at $a$ , but also in some small interval around $a$ . This seems too good to be true. Hence my question: Does there exist a function $f\colon\mathbb{C}\to\mathbb{C}$ such that $f^{\prime}(a)$ exists at some point $a\in\mathbb{C}$ but $f$ is continuous nowhere else? Update, added later: A quick thanks to those who responded. I have since thought through this with a friend and we've found a problem with the heuristic argument: Properly interpreted, the heuristic argument is saying that if $z_{1}$ and $z_{2}$ are close to $a$ , then $f(z_{1})$ is close to $f(z_{2})$ . However, to have continuity at $z_{1}$ , say, we need to be able to make $f(z_{2})$ arbitrarily close to $f(z_{1})$ by choosing $z_{2}$ to be sufficiently close to $z_{1}$ --- therein lies the problem. Suppose $\lvert z_{3}-z_{1} \rvert < \lvert z_{2}-z_{1} \rvert$ , i.e., suppose $z_{3}$ is even closer to $z_{1}$ than $z_{2}$ is. Then, for $f$ to be continuous, we need to be able to ensure that $f(z_{3})$ is closer still to $f(z_{1})$ (than $f(z_{2})$ is); in general, we can't ensure this. If I get round to it, I might try to work through the details in a specific case and see if there are any other problems. If I get anywhere, I'll post the results as an additional answer.","['complex-analysis', 'examples-counterexamples']"
1758001,"If $\pi : X \to Y$ is a flat, proper $O$-connected morphism of locally Noetherian schemes, then is $h^0(X_q, O_{X_q}) = 1$?","If $\pi : X \to Y$ is a flat, proper $O$-connected morphism of locally Noetherian schemes, then is $h^0(X_q, O_{X_q}) = 1$ (the dimension of the space of global sections of the fibers)? This seems to be used in a proof I am reading, but I can't see why it is true. $O$-connected means that the natural map $O_Y \to \pi_* O_X$ is an isomorphism. For a point $p$ with residue field $k(q)$ I would like to say something like, $k(q) = O_Y \otimes k(q) \cong (\pi_* O_X) \otimes k(q) \cong \pi'_*(O_X \otimes k(q))$ (where this last $\pi'$ is projection onto $k(q)$, hence gives global sections of the fibers), but $Spec k(q) \to Y$ is generally not flat, so I don't know how to argue for the last isomorphism. (Or maybe there is some way to guarantee that the fibers are all geometrically connected and geometrically reduced?) For reference, the proof I am reading is 28.1.11 in Ravi Vakil's notes.",['algebraic-geometry']
1758010,"Itō formula as presented in ""Stochastic Equations in Infinite Dimensions"" by Giuseppe Da Prato","In Stochastic Equations in Infinite Dimensions , Theorem 4.32 (Google Books) , the authors present the following version of an Itō formula: Given Hilbert spaces $(U,\langle\;\cdot\;,\;\cdot\;\rangle_U)$ and $(H,\langle\;\cdot\;,\;\cdot\;\rangle)$, a $U$-valued Brownian motion $(W_t)_{t\ge 0}$ and $$X_t=X_0+\int_0^t\varphi_s\;{\rm d}s+\int_0^t\Phi_s\;{\rm d}W_s\tag 1$$ for some $H$-valued random variable $X_0$, $H$-valued stochastic process $(\varphi_t)_{t\ge 0}$ and $\mathfrak L(U,H)$-valued$^1$ stochastic process $(\Phi_t)_{t\ge 0}$, we've got \begin{equation}
\begin{split}
f(t,X_t)-f(0,X_0)&=\color{red}{\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle}\\
&\quad\color{blue}{+\text{something unimportant for this question}}
\end{split}\tag 2
\end{equation} for all $F:[0,\infty)\times H\to\mathbb R$ with partial Fréchet derivatives $F_t$, $F_x$ and $F_{xx}$. Question : What's the definition of the $\color{red}{\text{red}}$ term? (They don't give one in the book). The proof of the statement can be reduced to the case $\varphi_t=\varphi_0$ and $\Phi_t=\Phi_0$. If $0=t_0<\cdots<t_n=t$ is a partition of $[0,t]$, Taylor's theorem yields$^2$ \begin{equation}
\begin{split}
f(t,X_t)-f(0,X_0)&=\color{red}{\sum_{i=1}^n\langle\Delta X_i,L_i\rangle}\\
&\quad\color{blue}{+\text{something unimportant for this question}}
\end{split}\tag 3
\end{equation} where $\Delta t_i=t_i-t_{i-1}$, $\Delta X_i=X_{t_i}-X_{t_{i-1}}$ and $$L_i:=F_x(t_{i-1},X_{t_{i-1}})\;.$$ Using $(1)$ and our assumption, the $\color{red}{\text{red}}$ term in $(3)$ is $$\color{red}{\sum_{i=1}^n\langle\Phi_0\Delta W_i,L_i\rangle}\color{blue}{+\sum_{i=1}^n\Delta t_i\langle\varphi_0,L_i\rangle}\tag 4\;.$$ Using the definition of the adjoint operator, the $\color{red}{\text{red}}$ term in $(4)$ is $$\sum_{i=1}^n\langle\Delta W_i,\Phi_0^\ast L_i\rangle_U\;.\tag 5$$ In the middle of page 108 they state (our $\color{red}{\text{red}}$ term from $(3)$ is called $I_2$ there) that $(5)$ converges almost surely to $$\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle\tag 6$$ for $n\to\infty$. Why? $^1$ Let $\mathfrak L(A,B)$ be the space of bounded, linear operators $A\to B$. $^2$ Notice that we can make sense of $(3)$, since $L_i\in\mathfrak L(H,\mathbb R)\cong H$ by Riesz' representation theorem .","['stochastic-processes', 'operator-theory', 'functional-analysis', 'stochastic-integrals', 'stochastic-analysis']"
1758054,Find general formula for $a_{n+1} = \frac{a_n}{1+n a_n}$,"$a_{n+1} = \frac{a_n}{1+n a_n}$ $a_0=1$ Series: $1, 1/2, 1/4, 1/7, 1/11, 1/16...$ ( we ca rewrite as $a_{n+1} = \frac{1}{\frac{1}{a_n}+n}$) By wolfram alpha answer is $\frac{2}{(n-1)^2+n+1}$ I have no idea how to get it manually. Any ideas? Thanks","['recurrence-relations', 'sequences-and-series']"
1758056,Why do the characters of an abelian group form a group?,"I was reading through Serre's Linear Representation Theory book and encountered a question to show that the set of all irreducible characters of an abelian group form a group. The proof of closure was given, since if we have two character functions $\chi_1, \chi_2$ of irreducible representations $V,W$ then $\chi_1 * \chi_2$ is the character of the irreducible representation $V \otimes W$. Additionally the trivial character serves as an identity element. But I don't see how to take inverses of characters. Attempted Ideas: If there is a way to define an inverse tensor product, so that given $U,V$ we have that $f(U,V) = Z | Z \otimes V = U$, then I could try to compute the inverse tensor product of a representation, with the trivial representation, and hope that the character laws travel too. But I'm not sure how to define such an operator.","['finite-groups', 'abstract-algebra', 'representation-theory', 'abelian-groups', 'group-theory']"
1758065,Limit of recursive sequence $n^2q_n=1+(n-1)^2q_{n-1}+2(n-2)q_{n-2}$,"When looking at this riddle , I came across the following sequence for the frequency of sampled integers between 1 and $n$ in a without replacement/without neighbour sampling:
$$q_1=1,\quad q_2=1/2,\quad q_n=\frac{1}{n^2}+\frac{(n-1)^2}{n^2}q_{n-1}+\frac{2(n-2)}{n^2}q_{n-2}\quad (n>2)$$
and I wonder if there is a generic mathematical approach to computing an analytic solution for $$\lim_{n\to\infty} q_n$$Assessing the numerical limit with an R code like q=rep(1,1e7)
for (n in 3:1e7) q[n]=(1+2*q[n-2]+(n-1)*q[n-1])/n
q[1e7]/1e7 led me to $0.432332...$ And a probabilistic reasoning indicates that $(n>1)$ $$\frac{1}{3}\le q_n\le \frac{1}{2}$$","['recursion', 'sequences-and-series', 'limits']"
1758142,Does an embedded discrete-time Markov chain preserve its properties in continuous time?,"Given a discrete-time Markov chain without independent increments, is the embedding of it into a continuous time Markov chain (i.e. via the use of exponential waiting times) an example of a continuous time Markov process without independent increments? Note: This question is forked from one I asked previously , since the Ornstein-Uhlenbeck process was an answer to every other part of that question except the part being asked again here. Context: As an answer to this related question , user @madprob gave an example of a discrete time Markov chain that does not have independent increments. Specifically, let a discrete time process with state space $\mathbb{R}$ be defined as follows: $X_{n+1} = X_n + Z$, where $Z|(X_n,X_{n-1},...,X_0) \sim N(-X_n, 1)$. In general, any Markov process in discrete time can be written as $X_{n+1}=X_n+Z_{n+1}$ where $Z_n = f(X_n,U_n)$ for some suitable $f$ and a random variable $U_n$ that is independent of $(X_{n-1},...,X_1,X_0)$ -- thus the problem of finding a discrete time Markov process that does not have independent increments reduces to the problem of choosing an appropriate $f$. Such a counterexample presumably exists given the answer to this question . EDIT : I suppose the process of embedding a Markov chain into continuous time is essentially subordinating a Poisson process. So this question is probably just a special case of whether or not subordinating infinitely divisible distributions preserves independence properties.","['stochastic-processes', 'markov-chains', 'probability-theory', 'markov-process']"
1758194,Is the Epsilon-Delta definition of a limit not precise enough?,"Consider the function $f: \{-1, +1\}\to \mathbb R$ defined by $f(x)= \arcsin (\frac{1+x^2}{2x})$ . Due to the following two inequalities : (i) $1+x^2 \geq 2x$ (ii) $1+x^2 \geq -2x$ , the function can only be defined at $x=1$ and $x=-1$ . I have learnt that the epsilon delta definition only includes those values of $x$ which are in the domain of $f(x)$ . But in this case, the function isn't defined on either side of x=1. So this is my question: Is it correct to say that the limit as $x$ approaches $1$ of $f(x$ ) is $\frac{\pi}{2}$ ? Can the above question be given a definitive ""yes"" or ""no"" answer, or must it(unfortunately) vary from person to person? If the latter, is the ""precise"" definition of a limit not precise enough? How can the answer be proved or disproved using the epsilon delta definition? I have also read that functions are by default continuous at isolated points. Can I conclude from the definition of continuity (the limit equals the value of the function evaluated at the point) that the limit must exist? Note : Forgive my ignorance but I do not know a thing about topology. I'm looking for a detailed answer but in simple terms, preferably written in the language of calculus. Thanks for the help.",['limits']
1758198,$f '' - (f ')^2 + f=0$; what is known about solutions?,"I'm curious about solutions to the equation 
$$f''-(f')^2+f=0$$
on the whole real line, as well as solutions which are periodic. Any info about the obvious multivariable generalization would interest me as well. I'm not necessarily looking for explicit solutions, though if there are nontrivial explicit solutions that would be interesting. I'm curious about techniques used to analyze such solutions and the general features of the solutions. I'm not well versed in ODE's, but this equation came up in something I was looking at, so don't be afraid to talk down to me and start with the basics if need be.","['ordinary-differential-equations', 'partial-differential-equations']"
1758210,Validity of a formula for the $n$-th power of a general $2 \times 2$ matrix,"I am taking an optics course and at one point$^1$ we need to find the $n$-th power of the $2 \times 2$ matrix
$$M := \begin{bmatrix} a & b \\ c & d\end{bmatrix}$$
where $a, b, c, d$ are real numbers. 
My professor (who is not very mathematically rigorous) gives the following formula which he calls ""Sylvester's law"":
$$\begin{bmatrix} a & b \\ c & d\end{bmatrix}^n = \frac{1}{\sin{\theta}} \begin{bmatrix} a \sin{(n \theta)} - \sin{((n-1) \theta)} & b \sin{(n \theta)} \\ c \sin{(n \theta)} & d \sin{(n \theta)} - \sin{((n-1)\theta)}\end{bmatrix} $$
where he defines $\cos{\theta} := \frac{1}{2}(a + d)$ (so presumably, $\theta = \arccos{(\frac{a+d}{2}}$). 
He then claims that $M^n$ is finite if $-1 < \frac{1}{2} (a + d) < 1$, for then $\theta$ is well-defined and $\sin(\theta) \neq 0$.$^2$ I was pretty skeptical since I have never seen this formula before and a google search didn't return anything similar. I've tried this formula for a couple of simple cases, and I do not get the correct answer. For example, for $a = 0.1, b = 0.2, c = 0.3, d = 0.4$ and $n = 2$, the formula gives
$$ \begin{bmatrix} -0.95 & 0.1 \\ 0.15 & -0.8 \end{bmatrix}$$
(note: not even positive) while the right value is
$$M^2 = \begin{bmatrix} 0.07 & 0.1 \\ 0.15 & 0.22\end{bmatrix}.$$
Thus the formula cannot be correct as presented. However, I find it hard to believe that he just made the formula up, so I suspect that there is a similar, correct formula (or at least an approximation). My question: Is this formula correct? If not, is there a similar formula that is either correct or an approximation of the $n$-th power? If so, what is the range of validity? I realise that if the answer is no then it will be pretty hard to be certain of that. I'm hoping that someone here will have seen this before. $^1$For those interested: While discussing optical cavities using ray transfer matrices . $^2$ A weird statement, since the $n$-th power of a finite matrix can never be infinite. I suspect this just has to do with the range of validity of the formula.","['matrices', 'linear-algebra']"
1758231,A problem in group theory_dsom [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $H$ be a group of integers modp, under addition, where $p$ is a prime number. Suppose that $n$ is an integer satisfying $1 \leq n \leq p$, and let G be the group $ H \times H \times \cdots \times H$ (n factors). Show that $G$ has no automorphism of order $p^2$.",['group-theory']
1758240,"Suppose $\triangle ABC$ is an equilateral triangle inscribed in the unit circle C(0,1).","Suppose $\triangle ABC$ is an equilateral triangle inscribed in the unit circle C(0,1). Find the maximum value of $$\overline{PA}\cdot\overline{PB}\cdot\overline{PC}$$
where $P$ is a variable point in $\bar{D}(0,2).$ I am trying to figure a proof for this problem out. I have it all drawn out so I could see conceptually what is going on but I am having trouble finding the answer. I believe the sides of the equilateral triangle are going to be $\sqrt{3}$ but I am not sure about the rest. this question is actually blowing my mind trying to find an answer.",['complex-analysis']
1758255,What does $C^k$ at a single point mean?,"I'm reading textbooks on manifolds. I usually see a saying that ""function $f$ is $C^k$ at a point $p \in M$"". I am confused with this. What does this mean? Furthermore if $f$ is $C^k$ at $p$, is it necessary that $f$ is $C^k$ in some neighborhood of $p$? Thanks for your help in advance.","['real-analysis', 'differential-topology', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
1758315,Do finite morphisms preserve dimensions?,"If $f: Y\to Z$ is a finite, surjective morphism of normal integral schemes (of finite type over a field) and $y$ is a prime divisor of $Y$, is then also $z= f(y)$ of codimension 1?
We have an inclusion $\mathcal{O}_{Z,z} \to \mathcal{O}_{Y,y}$ and the latter is a DVR, i.e. it has dimension 1. Does the integrality of both local rings imply that $\operatorname{codim}z =1$? More generally, is there a relationship between $\dim y, \dim f(z)$ and $\dim f^{-1}(z'), \dim z'$ for $z'\in Z$ arbitrary?","['divisors-algebraic-geometry', 'algebraic-geometry']"
1758319,Show that the sum $\sum_{k = 0}^n 2^k \binom{n}{k}$ is equal to $3^n$,How can I show that the sum $$ \sum_{k = 0}^n 2^k \binom{n}{k}$$ is equal to $3^n$?,"['binomial-theorem', 'discrete-mathematics']"
1758322,Application of first isomorphism theorem,"Let $G$ and $K$ be groups and let $G\times K$ be the direct product of these two groups. Find a normal subgroup $N$ such that $(G\times K)/N\cong G.$ I think I need to use the first isomorphisms theorem, stating that if $\varphi \colon G\rightarrow H$ is a homomorphism then $G/\ker(\varphi)\cong$ im$(\varphi).$ I'm don't really know where to go from this point though.",['group-theory']
1758358,How many partially ordered sets(poset) does a set have on 4 elements?,"A partially ordered set has the following properties: a ≤ a (reflexivity); if a ≤ b and b ≤ a, then a = b (antisymmetry); if a ≤ b and b ≤ c, then a ≤ c (transitivity); I think that the easiest way is to visualize it with a Hasse-diagram: So I obtained the solution 16 partially ordered sets have a set of 4 elements have. My second question: How many total order sets(which has the following properties: reflexivity,antisymmetry,transitivity,trichotomous) does a 4 element set have?","['abstract-algebra', 'discrete-mathematics']"
1758360,Combinatorics: choose 5 out of 10 colored balls,"I usually don't have any problems thinking about combinatorics but this problems answer doesn't seem correct. There are $5$ black balls, $1$ red, $1$ green, $1$ blue, $1$ yellow and $1$ white.
In how many ways can you pick $5$ balls? The answer is $2^5$ but why?
I'm comparing it with a lock with $4$ integers, which has the number of combinations of $10^4$. Anyone got any explanations?",['combinatorics']
1758516,Trying to understand what is a p value,"I am trying to understand what a p value is and what is its importance in statistics, from this example. For $50$ specimens of alloy steel the rock hardness scale measured an average of $62$ with standard deviation $8$ he manufacturer claims that the alloy has average strength of $64$ at significance of $0.01$ can you refute this? Well I did this problem and saw no at this level you cant refute it , I use the null being that the hardness was $64$ and the alternate that the hardness was less then $64$. From the data we observe a $z=-1.7677$ but the chart is $z=-2.33$ and so we cant refute null at this level. But now I want to understand what the p value would be , I thought it was that the p-value is the probability of getting the observed value of the test or a value with even more evidence against the null if the null is actually true. So using that I just look on the z chart, the probability of seeing Z greater then $1.7677$ is $0.0384$, so is that the p value?","['statistics', 'hypothesis-testing']"
1758533,Maximum Likelihood Estimation of Brownian Motion Drift,"I'm looking at times series of stock movements over 10 minute windows, and am trying to measure the ""trend"" of these movements. Method A is to simply calculate $\Delta P$, the difference between the final value and the initial value. Method B is to use MLE to calculate the maximum likelihood value of the drift coefficient using the Brownian Motion model $dP(t)=\mu P(t) dt + \sigma P(t)  dB(t)$. My intuition is that, if I am able to calculate the estimate $\hat{\mu}$, it will be a better measure of the trend since the Brownian motion is able to take into account volatility. However, another part of me thinks that $\hat{\mu}$ may actually just be a linear combo of  $\Delta P$ (i.e. $\hat{\mu}$ is completely determined by the first and last values of the time series). Any thoughts or semi-rigorous explanations would greatly appreciated! And let me know if anything else needs clarifying.","['maximum-likelihood', 'statistics', 'probability', 'brownian-motion', 'finance']"
1758562,Automorphisms of infinite abelian groups,"It is well-known that the map $\operatorname{Aut}$ from the class of groups to itself has fixed points. For $n \neq 2$ or $6$ , $\operatorname{Aut}(S_n) \cong S_n$ , $\operatorname{Aut}(D_4) \cong D_4$ and if $G$ is a finite non-abelian simple group, then $\operatorname{Aut}(\operatorname{Aut}(G)) \cong \operatorname{Aut}(G)$ . The question arises if there is a fixed point among the non-trivial abelian groups. It is relatively easy to see that finitely generated abelian groups do not qualify, so if such a group exists, it has to be infinitely generated. Note that for topological automorphisms of topological groups fixed points are known: for instance $\operatorname{Aut}(\mathbb{R} \times \mathbb{Z}/2\mathbb{Z}) \cong \mathbb{R} \times \mathbb{Z}/2\mathbb{Z}$ . But we assume no additional structure imposed.","['abelian-groups', 'infinite-groups', 'group-theory']"
1758572,Finite Power of Operator Norm,"I know that for any bounded operator A on a normed space, we have
$||A^n||$ $\leq$ $||A||^n$. I am wondering when the equal sign would be achieved.","['functional-analysis', 'real-analysis']"
1758610,Maximum and minimum of of $f(x)=|x-1|+|x-2|+|x-3|$,"I am trying to find the maximums or minimums of $$f(x)=|x-1|+|x-2|+|x-3|$$ (if there exist). My attempt: First I compute the derivative and tried to find critical point, i.e, $f'(x) = \frac{x-1}{|x-1|} + \frac{x-2}{|x-2|} + \frac{x-3}{|x-3|}=0$, first I noted that this derivative doesn't exist in 1,2,3 for the absolute value. And the are no critic point (Is that correct?) Then I stuck here because I don't know if there is a maximum, I think that it is not exist but how can I justified and I believe that there is a global minimum, but this occurs only if I found a critical point, some help pls.","['algebra-precalculus', 'calculus', 'derivatives']"
1758636,Possibilities for a group $G$ that acts faithfully on a set of objects with two orbits?,"A group $G$ acts faithfully on a set $X$ of 5 objects. The action has
  two orbits: one of size 2, and one of size 3. What are the
  possibilities for the group $G$? I think I should apply the orbit-stabilizer theorem. I believe a main hint is the fact that this group has precisely two orbits but I'm not sure how to use that information. What's a good way to go about listing the possibilities for this group $G$?","['abstract-algebra', 'group-theory']"
1758643,The lattice points in the real cone of some semigroups are just the integer cone of that semigroup.,"I'm trying to solve an exercise in Fulton's book on toric varieties, and have reduced it to the following: Let $M$ be a lattice of rank $n$ with $M \otimes \mathbb{R} = V$ , and $S$ be a finitely generated semigroup of $M$ , such that the following conditions hold: i) S is saturated. (If $v \in M, n \in \mathbb{Z}_{> 0}$ with $nv \in S$ , then $v \in S$ .) ii) S generates M as a group. Then the lattice points inside the real cone on $S$ are simply the integer cone on $S$ . In other words, given a saturated semigroup $S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}_{\geq 0}}$ with $S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}} = M$ , then $\langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M = S$ . This seems geometrically obvious, but I can't find a way to prove it. I can prove the result if $\langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M=\langle v_1,\dots,v_k\rangle_{\mathbb{Q}_{\geq 0}}\cap M$ using the saturatedness of $S$ , the problem is showing that any positve real linear combination of the $v_i$ lying in the lattice can also be written as a positive rational combination. Thinking about the different ways to represent the same vector, we see that finding $r=(r_1,\dots,r_k)$ such that $\sum r_iv_i = v$ is equivalent to saying that $r$ is a solution to the matrix equation: $$\bigg(v_1 \dots v_k \bigg)x=v $$ The solutions to this equation are either an empty set, of an affine subspace of $\mathbb{R}^k$ . But if $v$ is in the intersection we're lookinh at, we know that the solution space contains a ""positive"" point, i.e. a point of $(\mathbb{R}_{\geq 0})^k$ , and an ""integer"" point, i.e. a point of $\mathbb{Z}^k$ . I'd like to use this somehow to show that it also contains a positive rational point, which is easy enough if the positive point lies in the strict interior of the set of positive points, but seems like it could be impossible in some examples, such as if the solution space is an affine line running tangent to the set of positive points, so I'm not sure if this approach will work...","['toric-geometry', 'semigroups', 'geometry', 'integer-lattices', 'linear-algebra']"
1758671,How would you show that field automorphisms fix prime subfields?,"Suppose $K$ is a prime subfield of $E$ , then if $\phi$ is an automorphism from $E$ to $E$ , we have for all $x \in K$ , $\phi(x) = x$ . I feel like this is just the definition of a field automorphism, but my book says this should be proven as an exercise.","['abstract-algebra', 'field-theory', 'ring-homomorphism']"
1758681,What math do I need to know for MD5?,This could fit into a lot of areas of SO but I feel like mathematics will know best. What area of math is used for something like an MD5 or SHA algorithm? Is there a mathematical equation/skeleton for something like it?,"['algorithms', 'hash-function', 'discrete-mathematics']"
1758745,Prove that $f(x)$ is irreducible iff its reciprocal polynomial $f^*(x)$ is irreducible.,"This is what I'm trying to prove: Let $f(x)\in\mathbb{Q}[x]$ and $\deg(f(x))>1$ . Prove that $f(x)$ is irreducible in $\mathbb{Q}[x]$ iff its reciprocal polynomial $f^*(x)$ is irreducible in $\mathbb{Q}[x]$ . Note: The reciprocal polynomial $f^*(x)=x^nf(1/x) \in \mathbb{Q}[x]$ , where $n=\deg f$ . So my thoughts are to prove the contrapositive, i.e. $f^*(x)$ is reducible iff $f(x)$ is reducible. So to prove the ( $\Rightarrow$ ) direction I assume that $f^*(x)=g(x)h(x)$ , so I get that $g(x)h(x)=x^nf(1/x)$ which implies $f(1/x)=(1/x^n)g(x)h(x)$ . I want to say this somehow makes $f(x)$ reducible but I am unable to proceed as $f(1/x)\notin \mathbb{Q}[x] $ . I thought about substituting $y=1/x$ , which gives $f(y)=y^ng(1/y)h(1/y)$ , but I am unable to show that this is irreducible in $\mathbb{Q}[y]$ . If I could, then it would be irreducible in $\mathbb{Q}[x]$ since $\mathbb{Q}[y]\cong \mathbb{Q}[x]$ Any suggestions or hints will be appreciated.","['irreducible-polynomials', 'abstract-algebra', 'polynomials']"
1758780,Proof outer measure satisfies monotonicity: $A \subseteq B \implies m^*(A) \leq m^*(B)$,"Theorem: $$A \subseteq B \implies m^*(A) \leq m^*(B)$$ Proof Attempt: By definition, $m^*(B) = \inf\{\sum\limits_{k=1}^\infty |J_k||\{J_k\} \text{ is a cover of B }\}$, $m^*(A) = \inf\{\sum\limits_{k=1}^\infty |I_k||\{I_k\} \text{ is a cover of A }\}$ Then since $A \subset B \implies \bigcup_{k =1}^\infty I_k \subseteq \bigcup_{k=1}^\infty J_k$, then $\sum\limits_{k=1}^\infty |I_k| \leq \sum\limits_{k=1}^\infty |J_k|$, so we have $m^*(A) \leq m^*(B)$ Can someone check if everything checks out?","['proof-writing', 'lebesgue-measure', 'measure-theory', 'proof-verification']"
1758824,Show the Cauchy-Riemann equations hold but f is not differentiable,"Let $$f(z)={x^{4/3} y^{5/3}+i\,x^{5/3}y^{4/3}\over x^2+y^2}\text{ if }z\neq0
\text{, and }f(0)=0$$ 
  Show that the Cauchy-Riemann equations hold at $z=0$ but $f$ is not differentiable at $z=0$ Here's what I've done so far: $\quad$As noted above, there are two cases: $z=0$ and $z\neq0$. The Cauchy-Riemann equations hold at $\quad z\neq0$ because $f(x)$ is analytic everywhere except at the origin. $\quad$Now we consider $z=0$. In this case, 
$$f_x(0,0) = \lim_{x\to 0} \frac{0 + 0}{x^2+0}=0$$ 
$\quad$and
$$f_y(0,0) = \lim_{y\to 0} \frac{0 + 0}{0+y^2}=0$$ $\quad$So, we see that $\frac{\delta u}{\delta x}=0=\frac{\delta v}{\delta y}$ and $\frac{\delta u}{\delta y}=0= -\frac{\delta v}{\delta x}$. Therefore, the Cauchy-Riemann equations $\quad$hold at $z=0$. From here, I'm a little confused about how to show that $f$ is not differentiable at $z=0$. I instinctively believe that $f$ is not continuous at $z=0$, which would imply that $f$ is not differentiable at $z=0$. However, I'm not sure how I can prove this. Any help would be greatly appreciated. Thank you!! EDIT: I now see that $f$ is continuous at $z=0$, but I'm not sure how to prove that it is not differentiable.  I know I should use $\lim_{z\to 0} \frac{f(z) - f(0)}{z-0}$ to determine differentiability, and that I should get two different answers when I approach the limit from two different paths (for example: $x=0$, $y=0$, or $x=y$), but I'm not sure how to evaluate $\lim_{z\to 0} \frac{f(z) - f(0)}{z-0}$ for $x=0$, $y=0$ or $x=y$. As before, any help would be much appreciated. Thanks!","['derivatives', 'complex-analysis', 'limits']"
1758837,Show that $\bigcup_{n=1}^\infty A_n= B_1 \backslash \bigcap_{n=1}^\infty B_n$,"Let $\{B_n\}$ be a decreasing set $B_1 \supseteq B_2 \supseteq B_3 \supseteq ....$ Define $A_n = B_1 \backslash B_n$ i.e. $A_1 = \varnothing, A_2 = B_1 \backslash B_2$ If we imagine $\{B_n\}$ as a donut then it is clear that $\{A_n\}$ is increasing Show: $\bigcup_{n=1}^\infty A_n = B_1 \backslash \bigcap_{n=1}^\infty B_n$ Seems like a proof by exhaustion? $A_1 \cup A_2 = (B_1 \backslash B_1) \cup (B_1\backslash B_2) = B_1\backslash B_2$ $A_1 \cup A_2 \cup A_3 = (B_1\backslash B_2) \cup (B_1 \backslash B_3) = \text{ ...hoping... } = B_1 \backslash (B_2 \cap B_3)$ It seems the derivation is a little bit heavy: $(B_1\backslash B_2) \cup (B_1 \backslash B_3)  = (B_1 \cap B_2^c) \cup (B_1 \cap B_3^c) = (B_1 \cup (B_1 \cap B_3^c)) \cap (B_2^c \cup (B_1 \cap B_3^c)) = (B_1 \cup B_1) \cap (B_1 \cup B_3^c) \cap (B_2^c \cup B_1) \cap (B_2^c \cup B_3^c) = B_1 \cap (B_2 \cap B_3)^c  = B_1 \backslash (B_2 \cap B_3) $ Continue this way, we can see that the claim is true. Is there any easier way to see relation? The proof in my book did it in one step... I am thinking something along the line where we can use the property of $\backslash$ to directly show $(B_1\backslash B_2) \cup (B_1 \backslash B_3) = B_1 \backslash (B_2 \cap B_3)$","['general-topology', 'alternative-proof', 'elementary-set-theory', 'proof-verification']"
1758839,Graph Game: The first player has a winning strategy over a graph $G$ if and only if $G$ has no perfect matching.,"Two people play a game over a finite graph $G$ choosing alternately previously unchosen vertices $v_1,v_2,v_3,\ldots$ such that, for every integer $i>1$ , the vertex $v_i$ is adjacent to $v_{i-1}$ . The last player capable of choosing a vertex is the winner. Prove that the player who goes first has a winning strategy if and only if $G$ doesn't have a perfect matching. I know that the necessity proof is trivial, but I'm having trouble with the sufficiency proof. I thought maybe induction would be useful, but I can't seem to use it appropriately. Any ideas would be helpful. Thanks. Proof of the Forward Direction $(\Rightarrow)$ . Suppose that $G$ has a perfect matching $M$ .  Then, the second player always wins by picking a vertex $v$ such that $\{u,v\}\in M$ , where $u$ is the vertex the first player has just played.  Therefore, the first player does not have a winning strategy.  By contrapositivity, if the first player has a winning strategy, then $G$ does not have a perfect matching.","['graph-theory', 'matching-theory', 'eulerian-path', 'combinatorics', 'combinatorial-game-theory']"
1758842,Show that $\Bbb R^{2n+1}$ is not a division algebra over $\Bbb R$ for $n>0$,"This is an exercise from Hatcher's Algebraic Topology (exercise 2.B.8). Here is the problem statement: Show that, for $n>0$ , $\Bbb R^{2n+1}$ is not a division algebra over $\Bbb R$ by showing that if it were, then for nonzero $a \in \Bbb R^{2n+1}$ the map $S^{2n} \to S^{2n}$ defined by $x \mapsto \dfrac{ax}{|ax|}$ would be homotopic to $x \mapsto -\dfrac{ax}{|ax|}$ , but these maps have different degrees. I found a duplicate of this question here where a suggestion was to find a path between $a$ and $-a$ that does not pass through $0$ . I am struggling to do this and I would appreciate if someone could give me some direction here. I don't have much experience with algebras or division algebras, so I may be missing something simple. I am also somewhat confused about how to find the degrees of the maps $x \mapsto {ax\over |ax|}$ and $x \mapsto {-ax\over |ax|}$ . As I understand it a map $S^{n} \to S^{n}$ induces a homomorphism $H_n(S^n) \to H_n(S^n)$ , and $H_n(S^n)$ is isomorphic to $\Bbb Z$ , so the induced homomorphism is of the form $x \mapsto nx$ for some $n \in \Bbb Z$ . Then that $n$ is the degree of the original map. I'm not sure how to actually find the induced maps on homology in this problem (or if there is another way to find the degree). Edit: As anomaly has pointed out, the second part of my question is simpler than I thought.","['algebraic-topology', 'abstract-algebra', 'division-algebras', 'algebras']"
1758843,"Exercise II-11 from Eisenbud-Harris, subscheme of dimension $0$, degree $3$, supported at origin isomorphic to what?","Suppose that $K$ is algebraically closed, and let $Z = \text{Spec}\,K[x_1, \ldots, x_n]/I \subset \mathbb{A}_K^n$ be any subscheme of dimension $0$ and degree $3$, supported at the origin. How do I get started showing that $Z$ is isomorphic either to $X = \text{Spec}\,K[x]/(x^3)$ or to $Y = \text{Spec}\,K[x, y]/(x^2, xy, y^2),$ and that $X$, $Y$ are not isomorphic to each other?","['algebraic-geometry', 'commutative-algebra']"
1758851,"Can you verify my solutions to these probabilities, given a coin is flipped 10 times?","I'm fresh into probability and I think it's important to ask a lot of questions since it seems probability really challenges your intuition. I'm working on the following problem, and have found a solution for each, but I'd like to verify my understanding is correct. I'm not entirely sure on part c (this IS the intersection, right? Is it simply just multiplying the two probabilities together, or is there something I'm missing? The formula below part c makes me nervous I'm missing something). A coin is flipped 10 times. a) What is the probability that there are exactly five heads and five tails? b) What is the probability that the first three flips are heads? c) What is the probability that there are exactly five heads and five tails AND the first three flips are heads? d) What is the probability that there are exactly five heads and five tails OR the first three flips are heads? Let $(S, P)$ be a sample space such that $$S = \{(a_1, a_2, a_3, ..., a_{10}) : a_1 \in \{H, T\}\}$$ and $P(S) = \frac{1}{2^{10}}$ for all $s \in S$. a) By definition, $|S| = 1024$. Let $E$ be the event that we get 5 heads (so the other five flips are tails). Then $|E| = $$10 \choose 5$, because we are choosing 5 of the 10 flips to be heads. Since all outcomes are equally likely, $P(E) = \frac{|E|}{|S|} = \frac{252}{1024}$. b) For each of the first three flps, there is a $\frac{1}{2}$ chance that each one will be heads. By the multiplication principle, the probability that the first three flips are heads is $\frac{1}{2^3} = \frac{1}{8}$. c) From part $a$, we know that the probability of exactly five heads and five tails is $\frac{252}{1024}$. From part $b$, we know the probability that the first three flips are heads is $\frac{1}{8}$. So the probability of both is $A \cap B = P(A) * P(B) = \frac{63}{2048}$ (defining $A$ as the event of exactly five heads and tails, and $B$ as the event of the first three flips being heads). d) We have the formula $$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$. Using our $A$ and $B$ events from part $c$, $P(A) = \frac{252}{1024}$, $P(B) = \frac{1}{8}$, and $P(A \cap B) = \frac{63}{2048}$. So we have that $$P(A \cup B) = \frac{252}{1024} + \frac{1}{8} - \frac{63}{2048} = \frac{697}{2048}$$.","['probability', 'proof-verification', 'discrete-mathematics']"
1758868,Is there a simpler way to do this modulo operation?,Question is: $38^7 \pmod{3} \equiv $ ? I do this: $38^7 \pmod 3 \equiv [(38 \pmod{3})^7]\pmod{3} \equiv [2^7] \pmod{3} \equiv 128 \pmod{3} \equiv 2$ Is there a way to do this without manually calculating $2^7$? I am worried about encountering a problem with a greater number raised to a higher power,"['modular-arithmetic', 'discrete-mathematics']"
1758870,Derivatives: If $f(x)= 1/e^x $ then,If  $f(x)= 1/e^x $ then $ƒ′(x) = ?$ A: $1/e^x⋅ln(e^x)$ If $ƒ′(x) = e^x$ then $ƒ(x) = ?$ A: $x$ Are my solutions correct?,"['derivatives', 'calculus']"
1758958,Block matrix of order $m$ with three block matrices,"How to find eigenvalues of following block matrices? $M=\begin{bmatrix}
A & B & O & O & O  & O & O & \cdots & O & O\\
B & A & B & O & O  & O & O & \cdots & O & O\\
O & B & A & B & O  & O & O & \cdots & O & O\\
O & O & B & A & B  & O & O & \cdots & O & O\\
O & O & O & B & A  & B & O & \cdots & O & O\\
\vdots & \vdots & \vdots & \vdots & \vdots  & \ddots & \ddots & \cdots & \vdots & \vdots\\
\vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \ddots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \vdots & \ddots & \ddots & \vdots\\
O & O & O & O & O  & O & O & O & A & B\\
O & O & O & O & O  & O & O & O & B & A\\
\end{bmatrix}_m$ Where, $A=\begin{bmatrix}
0 & 1 & 0 & 0 & 0  &\cdots & 0 & 1 \\ 
1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0\\ 
0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0\\
0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0\\
0 & 0 & 0 & 1 & 0 & \ddots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & \ddots & 0 & 1\\
1 & 0 & 0 & 0 & 0 & \cdots & 1 & 0\\
\end{bmatrix}_n$ $B=\begin{bmatrix}
1 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}_n$ $O=\begin{bmatrix}
0 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}_n$","['matrices', 'eigenvalues-eigenvectors']"
1758992,Derivative of function with respect to $x$ where $x$ is the order of a derivative of another function.,"When I learn math I always have lot of thoughts and ideas in my head and some of them are weird. But I came aross a question, which is... also kind of weird. How can a problem like this be solved? Is it possible? $$\frac{d}{dx}\Bigg(\frac{d^x}{dw^x}w^2\Bigg)$$","['derivatives', 'calculus']"
1759000,Proving subgroup of $Aut(\Bbb C^2)$ that fixes a specific curve is isomorphic to $\Bbb Z^6 \times \Bbb Z^3 $,"So, I have the curve $C = V(y^3 - x^6 + y^6) \subset \Bbb C^2$. I want to prove that, if $G= \{ \varphi = (f_1,f_2) \in Aut(\Bbb C^2):\varphi(C) = C,$ $ deg(f_i) = 1  \}$, then $G \simeq \Bbb Z^6  \times \Bbb Z^3$. From a previous exercise, I know that such a $\varphi$ must send $0$ to itself, that is, $\varphi$ is linear. Using that, I was able to find a subgroup $H \simeq \Bbb Z^6  \times \Bbb Z^3 $ of $G$. This is $H$: $H= \{\varphi \in Aut(\Bbb C^2) : \varphi(x,y)=(e^\frac{2k\pi i}6x, e^\frac{2j\pi i}3y),0 \leq k \leq 5, 0 \leq j \leq 2  \}$ It is easy to see that $H \simeq \Bbb Z^6  \times \Bbb Z^3$ (sending $(\bar{k}, \bar{j})$ to $(e^\frac{2\bar{k}\pi i}6x, e^\frac{2\bar{j}\pi i}3y)$). I have not been able to prove that $G=H$, and I'd rather not resort to brute force (taking an arbitrary automorphism $(ax + by, cx + dy)$ and substituting in the equation for the curve doesn't look very promising to me). Is there a way to circumvent this? Any tips will do, I don't need a full answer. Thanks in advance!","['complex-geometry', 'algebraic-geometry']"
1759007,Which real numbers have $2$ possible decimal representations?,"I know that all positive and negative whole numbers have $2$ possible decimal representations. For example, $1+1+1+1$ could be represented as $4$ or as $3.99999...$ (I believe $4.000..1$ isn't a thing, right?). All terminating decimal numbers have $2$ representations as well, for example $1.5$ is the same as $1.4999..$ However, I can't really see how you would make a second representation for non-terminating decimals (especially irrational numbers) and zero.","['algebra-precalculus', 'real-analysis', 'calculus']"
1759024,Probability Independence - Determining if two sets are independent (drawing two cards),"I've got a few problems here that I feel pretty confident on. I am asking for confirmation on these answers. However, I am stuck on problem #3. Please let me know if you need more information. Two cards are drawn, one at a time, without replacement, from a deck
  of $52$ cards. Start from a new deck in each problem. Determine if the two sets are independent. 1. Let $A$ be the event that the two cards have the same rank. Let $B$ be the event that the first card is an ace. I answered yes, $P(A) = \frac{1}{17}$, $P(B) = \frac{1}{13}$, $P(A \cap B) = \frac{1}{221}$, thus $P(A \cap B) = P(A)P(B).$ 2. Let $A$ be the event that the two cards have the same rank. Let $C$ be the event that the two cards have the same suit. I answered no, because $P(A) = \frac{1}{17}$, $P(C) = \frac{4}{17}$, and $P(A \cap B) = 0$, because you can't have two cards in a standard deck of the same rank and suit. $0 \neq P(A)P(B)$. 3. Let $D$ be the event that the first card is a club. Let $E$ be the event that the second card is a club. I've found that $P(D) = \frac{1}{204}$. However, I don't know how to find $|E|$ or similarly $P(E)$. My intuition says I can't without knowing what my first card was, but I don't know how to write this.","['probability', 'proof-verification', 'discrete-mathematics']"
1759036,"Any Video Lectures Of An MIT, Harvard, Stanford, UC Berkeley, Yale, or Princeton Analysis Course Based On Baby Rudin?","I'm learning analysis from the book Principles of Mathematical Analysis by Walter Rudin, third edition. This book, popularly known as Baby Rudin , is being used for analysis courses at such elite places as the MIT, Harvard, Stanford, UC Berkeley, Yale, and Princeton. Am I right? Now is there any video lecture analysis course based on Baby Rudin available on the Internet from any of the above-mentioned institutions? Which other text(s) treat the same material as Baby Rudin and so can be used in combination with it?","['reference-request', 'real-analysis', 'book-recommendation', 'analysis']"
1759054,Deriving the asymptotic distribution of a two-stage estimator,"Suppose $X_i \stackrel{iid}{\sim}F_\theta$, where $F_\theta$ is a probability distribution parameterized by a finite-dimensional vector $\theta$. 
Let $\hat{\theta}_n$ denote the maximum likelihood estimator of $\theta$, i.e.
$$
\hat{\theta}_n = \arg \max_\theta \frac{1}{n}\sum_{i=1}^n f(X_i;\theta)
$$
where $f(x;\theta)$ is the density of $X$.
Suppose we are interested in determining the asymptotic properties of $Z_{i,n} = g(X_i,\hat{\theta}_n)$, where $g$ is a continuously differentiable function.
What is the asymptotic distribution of $\bar Z_n := \frac{1}{n}\sum_{i=1}^n Z_{i,n}$? My attempt: Taking a Taylor expansion, we obtain $$
Z_{i,n} = g(X_i,\theta) + \frac{\partial g}{\partial y}(X_i,\theta) (\hat{\theta}_n - \theta) + o_P(||\hat{\theta}_n - \theta||)
$$ from which it follows that $$
\sqrt{n}(\bar Z_n - EZ) = \sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n g(X_i,\theta) - EZ\right) + \left(\frac{1}{n}\sum_{i=1}^n\frac{\partial g}{\partial y}(X_i,\theta)\right) \sqrt{n} (\hat{\theta}_n - \theta) + o_P(||Y_n - \theta||)
$$
where $EZ := Eg(X_i,\theta)$.
I can apply the CLT to get the asymptotic distribution of $\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n g(X_i,\theta) - EZ\right)$ and I can use the Delta Method to obtain the asymptotic distribution of $\left(\frac{1}{n}\sum_{i=1}^n\frac{\partial g}{\partial y}(X_i,\theta)\right) \sqrt{n} (\hat{\theta}_n - \theta)$. However, I know marginal convergence in distribution does not imply joint convergence in distribution, so I can't immediately use the continuous mapping theorem to determine the distribution of the sum of the two terms. How should I proceed?","['probability-theory', 'statistics', 'central-limit-theorem']"
1759064,"For each pair of events A and B, find P(A|B) and P(B|A).","I've got a simple problem here, but I just want to ensure that I'm not losing a simple concept. Relevant equations:
$$P(A|B) = \frac{P(A \cap B)}{P(B)},$$ $$P(A \cap B) = P(A) + P(B) - P(A \cup B)$$ Let $(S,P)$ be a sample space with $S = \{1, 2, 3, 4, 5\}$, and, $P(1) = P(2) = 0.1$ $P(3) = P(4) = 0.2$ $P(5) = 0.4$ For each pair of events $A$ and $B$, find $P(A|B)$ and $P(B|A)$. a. $A = \{1, 2, 3\}$; $B = \{2, 3, 4\}$ b. $A = \{1, 2, 3\}$; $B = \{4, 5\}$ c. $A = \emptyset$; $B = \{2, 3, 4\}$ d. $A = \{1, 2, 3, 4, 5\}; B = \{4, 5\}$ If I am asked to find the probability of one of these sets, do I just add the values together? Is there any instance I would multiply them; such as when I'm finding $P(A \cap B)$ vs. just $P(A)$ or $P(B)$? Do I need to use Inclusion-Exclusion? Here's what I found. a. $P(A|B) = \frac{0.3}{0.5} = \frac{3}{5}$, $P(B|A) = \frac{0.3}{0.4} = \frac{3}{4}$ b. $P(A|B) = 0$, $P(B|A) = 0$ c. $P(A|B) = 0$, $P(B|A) = undefined$ d. $P(A|B) = \frac{0.3}{0.3} = 1$, $P(B|A) = \frac{0.3}{1} = \frac{3}{5}$","['probability', 'proof-verification', 'discrete-mathematics']"
