question_id,title,body,tags
3542959,Identities for subspaces and linear maps,"Wikipedia has a nice list of identities for how intersections, unions, and complements interact with images and preimages of set functions. But if $f:V \to W$ is a linear map, many of the identities for set functions can be refined. For example, if $A \subset V$ , then $f^{-1}(f(A)) = A + \ker f$ . There are also identities which involve sums, e.g. $f(A + B) = f(A) + f(B)$ . Can anyone point me to a reference which has a list of identities for subspaces and linear maps?","['linear-algebra', 'vector-spaces', 'linear-transformations', 'reference-request']"
3542983,Expected Value for the Number of Parts of a Partition of n,"Given a positive integer $n$ , I want to know the expected value for the number of parts of a random partition of $n$ . I am aware that a similar question has been asked already: Expected number of parts of a uniformly selected partition of $n$ ; however, I do not want to select the partitions uniformly. I want to collect the partitions of $n$ and have the partition $\lambda$ appear $dim(\lambda)$ times. For example, for $n=3$ we have $\{3\}$ (once) $\{2,1\}$ (twice) $\{1,1,1\}$ (once) So the expected value will be $\frac{1+2+2+3}{4}=2.$ I wrote some code to compute this and it looks like the function is logarithmic in $n$ , which is the case in the linked question, too. How is my question related to uniform case? Are there any known results for this? Thank you","['integer-partitions', 'number-theory', 'combinatorics', 'discrete-mathematics', 'probability-theory']"
3542992,Induction in the proof of the existence of prime factorizations,"The fundamental theorem of arithmetic states that every positive integer greater than 1 is either a prime or a product of primes. First question: why ""either a prime or a product of primes"", if every prime is in fact a product of primes with one factor? Wouldn't it be simpler to just say that every positive integer greater than 1 can be written as a product of primes? The fundamental theorem of arithmetic is proved using strong induction. The formal definition of strong induction (in its transfinite version) is: $\forall n [\forall k [k<n \rightarrow P(k)] \rightarrow P(n)] \rightarrow \forall nP(n)$ . In this case, $P(x)$ is substituted by ""if x>1, then x is prime or x is a product of primes"" . Second question: which are the formal definitions of x is prime and of x is a product of primes ? Velleman says that x is not prime is the same of $\exists a\exists b[n=ab \land a<n \land b<n]$ . Therefore, I would assume that x is prime would be $\forall a\forall b[n=ab \rightarrow a \geq n \lor b \geq n]$ . Is it correct? How about x is a product of primes ? Now, expanding the inductive hypothesis  inside the scope of the natural numbers and excluding the vacuosly true statements, we end up with the following result: $P(2) \land [P(2) \rightarrow P(3)] \land[P(2)\land P(3) \rightarrow P(4)] \land ... \rightarrow \forall n P(n)$ Third question: how is it possible that if 2 is prime or a product of primes implies that 3 is prime or a product of primes?","['elementary-set-theory', 'elementary-number-theory', 'induction', 'prime-factorization']"
3543029,Derivative of Dirac delta distribution,How is the derivative of $\delta(x-y)$ with respect to $x$ related to the derivative with respect to $y$ ? I suspect they differ by a minus sign but I'm not sure. Both $x$ and $y$ are real variables.,"['calculus', 'derivatives', 'dirac-delta', 'distribution-theory']"
3543055,(Proof Verification) Show that the collection of all elementary cylinders is a semi-ring.,"Denote $\mathbb{R}^{\mathbb{T}}$ to be the set of all functions $x:\mathbb{T}\longrightarrow\mathbb{R}.$ Let $B_{1},\cdots, B_{n}\in\mathcal{B}(\mathbb{R})$ , then we can define the elementary cylinder sets as $$E=\{x\in\mathbb{R}^{\mathbb{T}}:x_{t_{1}}\in B_{1},\cdots, x_{t_{n}}\in B_{n}\},$$ where $t_{1},\cdots, t_{n}\in\mathbb{T}$ . Denote the collection of all such elementary cylinders to be $\mathfrak{E}$ . I want to show that $\mathfrak{E}$ forms a semi-algebra. Many discussions have been made, for instance, here: The collection of cylinder sets is a semiring and here: Show that $\mathfrak{S}=\bigcup_{N=1}^{\infty}\mathfrak{Z}_N\cup\left\{\emptyset\right\}$ is a semi-ring . However, it seems that given different context, the definition of cylinder sets and elementary cylinder are always different but also similar. And it seems that the closure under finite intersection is easy enough so it is rarely asked. I came up a proof with the closure under finite intersection, and mimicked the first link to show the complement is a finite disjoint union of elements in the collection. Proof of closure under finite intersection: Denote $\mathfrak{E}$ to be the collection of all elementary cylinder sets.  Firstly we need to note that for $E\in\mathfrak{E}$ , we can always re-write that \begin{align*}
E&=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B_{1}\times\cdots\times B_{n}\}\\
&=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}},\cdots, x_{t_{m}})\in B_{1}\times\cdots\times B_{n}\times \mathbb{R}^{m-n}\},
\end{align*} for any $t_{n+1},\cdots, t_{m}$ . Thus, we can always assume two cylinder sets to have the same set of cutting points $t_{1},\cdots, t_{m}$ , by taking the union of the indexing sets of the two cylinder sets and using the union as the new indexing set for both of them. To show the closure under finite intersection, let $E_{1},E_{2}\in\mathfrak{E}$ . Then, we can write $$E_{1}=\{x\in\mathbb{R}^{\mathbb{T}}:x_{t_{1}}\in A_{1},\cdots, x_{t_{m}}\in A_{m}\},$$ $$E_{2}=\{y\in\mathbb{R}^{\mathbb{T}}:y_{t_{1}}\in B_{1},\cdots, y_{t_{m}}\in B_{m}\},$$ for $A_{1},\cdots, A_{m}, B_{1},\cdots, B_{m}\in\mathcal{B}(\mathbb{R})$ so that $$E_{1}\cap E_{2}=\{z\in\mathbb{R}^{\mathbb{T}}:z_{t_{1}}\in A_{1}\cap B_{1},\cdots, z_{t_{m}}\in A_{m}\cap B_{m}\}.$$ Note that since $A_{1},\cdots, B_{m}$ are all Borel,  then the intersections $A_{1}\cap B_{1},\cdots, A_{m}\cap B_{m}\in\mathcal{B}(\mathbb{R})$ , as well. This implies $E_{1}\cap E_{2}$ is also an elementary cylinder, and thus $E_{1}\cap E_{2}\in\mathfrak{E}$ . Proof of the last property: To show the other property, it suffices to show that, for all $A_{1},\cdots, A_{m}, B_{1}\cdots, B_{m}\in\mathcal{B}(\mathbb{R})$ , we can rewrite $(A_{1}\times\cdots\times A_{m})\setminus (B_{1}\times\cdots\times B_{m})$ into a finite disjoint union of sets, and each set in the union is a $m-$ fold product of sets in $\mathcal{B}(\mathbb{R})$ . This is sufficient due to the following reason: For any $E_{1}, E_{2}\in\mathfrak{E}$ , we can write $$E_{1}=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{m}})\in A_{1}\times\cdots\times A_{m}\},$$ $$E_{2}=\{y\in\mathbb{R}^{\mathbb{T}}:(y_{t_{1}},\cdots, y_{t_{m}})\in B_{1}\times\cdots\times B_{m}\}$$ for some $A_{1},\cdots, A_{m}, B_{1},\cdots, B_{m}\in\mathcal{B}(\mathbb{R})$ , so that $$E_{1}\setminus E_{2}=\{z\in\mathbb{R}^{\mathbb{T}}:(z_{t_{1}},\cdots, z_{t_{m}})\in (A_{1}\times\cdots\times A_{m})\setminus (B_{1}\times\cdots\times B_{m})\}.$$ If we showed that $$(A_{1}\times\cdots\times A_{m})\setminus (B_{1}\times\cdots\times B_{m})=\bigcup_{n=1}^{\ell}C_{n},$$ where $C_{n}$ is a $m-$ fold product of Borel set for all $n$ , $C_{n}$ disjoint,  then we would be able to rewrite $$E_{1}\setminus E_{2}=\bigcup_{n=1}^{\ell}\{f_{n}\in\mathbb{R}^{\mathbb{T}}:(f_{n}(t_{1}),\cdots, f_{n}(t_{m}))\in C_{n}\},$$ where the union would be disjoint (since $C_{n}$ was disjoint), and each set in the union would be an elementary cylinder (since $C_{n}$ was a $k-$ fold product of Borel sets). So, let us show the sufficient condition by an induction on $m$ ! For $m=2$ , note that we can write $$(A_{1}\times A_{2})\setminus(B_{1}\times B_{2})=\Big((A_{1}\setminus B_{1})\times A_{2}\Big)\cup\Big(A_{1}\times (A_{2}\setminus B_{2})\Big),$$ however this union is NOT disjoint since it has these two sets have an intersection $$C_{1}:=\Big((A_{1}\setminus B_{1})\times A_{2}\Big)\cap\Big(A_{1}\times (A_{2}\setminus B_{2})\Big)=(A_{1}\setminus B_{1})\times (A_{2}\setminus B_{2}).$$ But we can decompose as $$(A_{1}\times A_{2})\setminus(B_{1}\times B_{2})=C_{1}\cup\Big((A_{1}\setminus B_{1})\times (A_{2}\cap B_{2})\Big)\cup \Big((A_{1}\cap B_{1})\times (A_{2}\setminus B_{2})\Big).$$ Denote the second term and third them to be $C_{2}$ and $C_{3}$ , respectively. Then, note that $C_{1}, C_{2}, C_{3}$ are all the product of two Borel sets, and they are disjoint. Hence, the desired condition holds for $m=2$ . Suppose the desired condition holds for $m=k$ for some fixed $k>2$ , then consider the case of $m=k+1$ . Denote $A:=A_{1}\times\cdots\times A_{k}$ and $B:=B_{1}\times\cdots\times B_{k}$ , then using the case of $m=2$ , we have the following: \begin{align*}
(A_{1}\times\cdots A_{k}\times A_{k+1})\setminus (B_{1}\times\cdot\times B_{k}\times B_{k+1})&=(A\times A_{k+1})\setminus (B\times B_{k+1})\\
&=D_{1}\cup D_{2}\cup D_{3},
\end{align*} where $D_{1}:=(A\setminus B)\times (A_{k+1}\setminus B_{k+1})$ , $D_{2}:=(A\setminus B)\times (A_{k+1}\cap B_{k+1})$ , and $D_{3}:=(A\cap B)\times (A_{k+1}\setminus B_{k+1})$ . Note that $D_{1},D_{2},D_{3}$ are disjoint, so we only need to show each of $D_{1}, D_{2}, D_{3}$ is a $k+1-$ fold product of Borel sets, or the finite disjoint union of them. By the induction hypothesis, we know that $$A\setminus B=\bigcup_{n=1}^{w}C_{n},$$ where $C_{n}$ disjoint and is a $k-$ fold product of Borel sets. Thus, we can rewrite $$D_{1}=\bigcup_{n=1}^{w}C_{n}\times (A_{k+1}\setminus B_{k+1})=\bigcup_{n=1}^{w}\Big(C_{n}\times (A_{k+1}\setminus B_{k+1})\Big),$$ and $$D_{2}=\bigcup_{n=1}^{w}C_{n}\times (A_{k+1}\cap B_{k+1})=\bigcup_{n=1}^{w}\Big(C_{n}\times (A_{k+1}\cap B_{k+1})\Big),$$ and thus $D_{1}$ and $D_{2}$ are finite disjoint union of sets, and each of the set in the union is a $k+1-$ fold product of Borel sets. The desired property for $D_{3}$ is immediate since $$D_{3}=(A\cap B)\times (A_{k+1}\setminus B_{k+1})=(A_{1}\cap B_{1})\times \cdots\times(A_{k}\cap B_{k})\times (A_{k+1}\setminus B_{k+1}).$$ Thus, the desired property holds for $m=k+1$ . The result follows immediately. I am really not confident about my proof, so I really appreciate it if one could have a quick check of my proof. Also, it would be really nice if someone has a shorter, alternative proof for the second part. My proof was really tedious :) Thank you so so so much! I am really weak at measure theory..","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'solution-verification', 'probability-theory']"
3543056,The sign of $\left(\frac{p-1}{2}\right)! \equiv \pm1 \bmod p$ when $p=4k+3$,"My question is simple: When $p=4k+3$ , is there any order or pattern to when the residue of $\left(\frac{p-1}{2}\right)! \bmod p$ is $-1$ and when it is $1$ ? In contemplating The amount of $n$ so that $n!+1$ is divisible by $p$ I confronted the above question, which I was unable to resolve in my own thinking. For odd $p$ , $(p-1)!$ has an even number of multiplicands which can be paired as follows: $1,p-1;\ 2,p-2;\ \dots \frac{p-1}{2},p-\frac{p-1}{2}$ . Modulo $p$ these pairings are $1,-1;\ 2,-2;\ \dots$ etc. If we multiply the first member of each pair, and then the second member of each pair we see that $$\prod_{i=1}^{\frac{p-1}{2}}i\equiv (-1)^{\frac{p-1}{2}}\prod_{i=p-\frac{p-1}{2}}^{p-1}|i|\ \bmod p$$ Since the magnitudes of the multiplicands in each product are equal, the magnitudes of the products are equal, and the products differ if at all only by sign. Moreover, from Wilson's theorem, we know that the product of every multiplicand $(p-1)!\equiv -1 \bmod p$ When $p$ has the form $4k+1$ , $(-1)^{\frac{p-1}{2}}=1$ . So the two products are equal, and their product is congruent to $-1 \bmod p$ . When $p$ has the form $4k+3$ , $(-1)^{\frac{p-1}{2}}=-1$ . In that case, the products have equal magnitudes and opposite signs, and their product is congruent to $-1 \bmod p$ . If we call that magnitude $a$ , we have $-a^2\equiv -1 \bmod p\ \Rightarrow a^2\equiv 1 \bmod p$ . This requires $a\equiv \pm1 \bmod p$ The upshot of this is that for $p=4k+3$ , $$\left(\frac{p-1}{2}\right)! \equiv \pm1 \bmod p$$ I found this question proving that point, but not answering the question I pose below. Looking at the first several examples of such $p$ , I find that the residue is $-1$ for $\{7,11,19,\dots\}$ and the residue is $1$ for $\{23,31,\dots\}$ .","['number-theory', 'prime-numbers']"
3543086,Problem regarding unique solution of differential equation,"A unique  solution  to the  differential  equation $y = x \frac{dy}{dx} - (\frac{dy}{dx})^2$ passing through $(x_0,y_0)$ doesnot exist then choose the  correct option $1.$ if $ x_0^2 > 4y_0$ $2.$ if $ x_0^2 = 4y_0$ $3.$ if $ x_0^2 < 4y_0$ $4.$ for any $(x_0 , y_0)$ My attempt  : Here $y = x \frac{dy}{dx} - (\frac{dy}{dx})^2$ Now i put $x= e^z$ then $z= \log x$ So $y= Dy- D^2y$ $D^2y-Dy -y=0$ so $(D^2-D-1)y=0$ so auxiliary equation will  be $m^2-m-1=0$ , $m= \frac{1 +_{-}\sqrt - 3}{2}$ so $y= e^{\frac{1}{2}x} (c_1 \cos(\frac{\sqrt - 3}{2} ) + c_2\sin ({\frac{\sqrt - 3}{2}} )x)$ After that im not able  to proceed further",['ordinary-differential-equations']
3543100,Understanding rate of change,"If we are given the function $y=3x+5$ , and were asked to obtain its derivative we would get $y'=3$ , which means that as $x$ increases, the rate at which $y$ changes is always $3$ .. so if $x=1$ , then $y=8$ and if $x=2$ then $y=8+3=11$ and so on.. I was trying to test this concept in a different function, say $y=x^{2}+x$ , the derivative is $y'=2x+1$ , does that mean that the rate at which $y$ changes is always $2x+1$ ? If $x$ increases by $1$ , then we would have $y=1+1=2$ , and when $x$ increases even more by $1$ , we will get $y=2^{2}+2=6$ .. how can I relate $2$ and $6$ to $y'=2x+1$ like how I did with $y=3x+5$ ?","['calculus', 'derivatives']"
3543155,Formula for angle between faces based on angles of corners,"I've got an irregular solid with a vertex at which three faces (and thus also 3 edges) come together. None of the angles involved are right angles. I know the angles of the corners, at that vertex, of the three faces. I need to find the angle between two of the faces (in a plane perpendicular to their shared edge; the plane of the third face isn't perpendicular, so I can't just use its corner angle). What is the formula to relate the three face-corner angles to the angle between two of the faces? Edit: I want this because I'm trying to fabricate an irregularly-shaped box to fit into a constrained space. I have college mathematics, including geometry, and calculus-based geometry, but excluding whatever spherical geometry is.","['trigonometry', '3d']"
3543241,A double sum for the square of the natural logarithm of $2$.,"I am trying to show \begin{eqnarray*}
\sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{(n+m)^2 2^{n}} =(\ln(2))^2. 
\end{eqnarray*} Motivation : I want to use this to calculate $ \operatorname{Li}_2(1/2)$ . So I want a solution to the above that does not use any reference to dilogarithms and please avoid rational multiples of $\pi^2$ (if possible). Right, lets turn it into a double integral. ( I know lots of you folks prefer integrals to plums.) Show \begin{eqnarray*}
\int_0^1 \int_0^1 \frac{xy \ dx  \ dy}{(1-xy)(2-xy)} =(\ln(2))^2. 
\end{eqnarray*} Reassuringly Wolfy agrees My try: Let $u=xy$ , & the double integral becomes \begin{eqnarray*}
\int_0^1 \frac{dy}{y}  \int_0^y \frac{u \ du }{(1-u)(2-u)} . 
\end{eqnarray*} Partial fractions \begin{eqnarray*}
 \frac{u}{(1-u)(2-u)} =\frac{1}{1-u} - \frac{2}{2-u}.
\end{eqnarray*} Do the $u$ integrations to leave the $y$ integrals \begin{eqnarray*}
-\int_0^1 \frac{\ln(1-y) dy}{y}  +2 \int_0^1 \frac{\ln(2-y) dy}{y}.
\end{eqnarray*} The first integral is \begin{eqnarray*}
-\int_0^1 \frac{\ln(1-y) dy}{y}  = \frac{ \pi^2}{6}.
\end{eqnarray*} which I was hoping to avoid and even worse Wolfy says the second integral is divergent So you have a choice of questions, where did I go wrong in the above ? OR how can we show the initially stated result?","['integration', 'harmonic-numbers', 'summation', 'real-analysis']"
3543282,Gronwall-type inequality assumption implies function is identically zero,"After stating and proving a version of Gronwall's lemma for continuous functions (as in this related question ), the author of the book I'm reading suggests trying to prove a related fact as an exercise. Suppose that $$ \phi(t) \le \int_{t_0}^t \psi(s)\phi(s) ds, $$ where $t_0 \le t \le t_0 + a$ , and the functions $\phi(t), \psi(t) \ge 0$ are continuous. The exercise is to show that this implies $\phi(t) = 0$ for all $t \in [t_0, t_0+a]$ . I can see why this might be true (by picturing what happens if $\phi(t)$ is not identically zero),  but I am struggling to prove this. The given proof of Gronwall's lemma does not work for this assumption. It looks as if an integral form of the MVT may help, but I was not able to make it work. Can someone point me in the right direction?","['ordinary-differential-equations', 'real-analysis']"
3543329,Conditional Expectation($E[f(X)\mid X \in S]$) is uniformly continous in its argument($S$),"I basically want to prove that under some conditions $E[f(X)\mid X \in S]$ is uniformly continuous in $S$ To be more specific, suppose $f:K \subset \mathbb{R}^n \rightarrow \mathbb{R}$ is a bounded uniformly continuous function where $K$ is compact. Also suppose $X: \Omega \subset \mathbb{R}^M \rightarrow K$ is a random variable with a uniformly continuous bounded PDF and $\Omega$ is compact (You can assume the mapping for $X$ is one-to-one. Also you can assume $n=2$ if you want) Suppose $h(v)=g(S)=E[f(X)\mid X \in S]$ where $v=(x_1,y_1,\ldots,x_n,y_n)$ and $S=[x_1,y_1]\times[x_2,y_2]\times\cdots\times [x_n,y_n]$ . I want to prove that $h$ is uniformly continuous in its arguments (or $g$ is uniformly continuous wrt Hausdorff distance or Symmetric difference pseudometric or  Fréchet-Nikodym metric) Is there a general theorem which can encompasses this?","['uniform-continuity', 'conditional-expectation', 'probability', 'hausdorff-distance']"
3543331,Uniqueness of maximal subgroup and order being a power of a prime,"Let $G$ be a finite group. If $G$ has only one maximal subgroup (a maximal subgroup is a proper subgroup $M$ that given a subgroup $H$ of $G$ , $M \subset H \subset G$ implies that $H = M$ or $H = G$ ), prove that the order of $G$ is a power of a prime. I've been stuck in this exercise for a few days now and just can't solve it. I tried applying Cauchy to $p$ and $q$ primes that divide the order of $G$ and also to analyse maximal subgroups containing the generated subgroups of the elements of order $p$ and $q$ given by Cauchy's theorem, but without any success.","['maximal-subgroup', 'group-theory', 'abstract-algebra', 'finite-groups']"
3543332,Multinomial Maximum Likelihood Estimation,"Let $(X_1, X_2, X_3, X_4)$ be a random vector from a multinomial $$\left[n,\frac{1-2\theta+\theta^2}{5},\frac{\theta(2-\theta)}{5},\frac{\theta(2-\theta)}{5},\frac{(1-\theta)^2}{5}\right]$$ find the ML estimate of $\theta$ This is a homework problem, and I don't want the answer, but I'm hoping for guidance on how to obtain the ML. The trouble I am running into is after taking the partial derivative wrt $\theta$ and attempting to solve for the parameter. I get a monster of an equation and I'm unable to algebraically isolate $\theta$ . If anyone has run into a problem like this before, could you please provide insight on a different way to obtain the parameter estimate? Thanks!","['statistics', 'multinomial-distribution', 'maximum-likelihood']"
3543336,"Dictionary Meaning of ""Modus Ponens"" and ""Modus Tollens""","I wanted to understand modus ponens and modus tollens better, and I searched for its dictionary meaning. Wikipedia says that modus ponens is Latin for ""mode that by affirming affirms"" and that modus tollens is Latin for ""mode that by denying denies"". Now, English is not my first language, but I understand it enough and these definitions still don't make sense. What does ""mode"" mean? what is ""by affirming affirms""? Can someone better simplify it in English for me?","['logic', 'discrete-mathematics', 'terminology']"
3543357,Is it true that Lebesgue's differentiation theorem follows from Lebesgue's density theorem?,"Let $(X,d)$ be a separable complete metric space and $\mu$ a probability measure on the Borel subsets of $(X,d)$ . Suppose that the Lebesgue's density theorem holds, i.e. that for each Borel set $A$ of $(X,d)$ it holds that $$\lim_{r\to0^+}\frac{\mu(A\cap\bar{B}_r(x))}{\mu(\bar{B}_r(x))}=1$$ for $\mu$ -almost all $x\in A\cap\operatorname{supp}(\mu)$ , where $\operatorname{supp}(\mu)=\{x\in X : \forall r>0, \mu(\bar{B}_r(x))>0\}$ . Question: is it true that Lebesgue's differentiation theorem holds in $(X,d,\mu)$ , i.e. that for each $f\in L^1(\mu)$ , $$\lim_{r\to0^+}\frac{1}{\mu(\bar{B}_r(x))}\int_{\bar{B}_r(x)}f\operatorname{d}\mu=f(x)$$ for $\mu$ -almost all $x\in \operatorname{supp}(\mu)$ ? I tried to approximate $f$ in $L^1(\mu)$ by a simple function $g$ , since it is easy to show that for $g$ simple the theorem holds, so \begin{align*}
&\left|\frac{1}{\mu(\bar{B}_r(x))}\int_{\bar{B}_r(x)}f\operatorname{d}\mu-f(x)\right| \le \frac{1}{\mu(\bar{B}_r(x))}\int_{\bar{B}_r(x)}|f-g|\operatorname{d}\mu \\
& \quad\quad\quad\quad + \left|\frac{1}{\mu(\bar{B}_r(x))}\int_{\bar{B}_r(x)}g\operatorname{d}\mu-g(x)\right|+|g(x)-f(x)|,
\end{align*} and then the middle term tends to zero for $\mu$ -almost all $x\in\operatorname{supp}(\mu)$ . Also Tchebychev allows us to treat the term $|f(x)-g(x)|$ . But when trying to estimate the first term it seems that we need a maximal function estimate in terms of $L^1(\mu)$ norm. Now, if we work in $\mathbb{R}^d$ we can rely on Besicovitch covering theorem, but in our context this theorem doesn't hold in general. So I'm wondering if there is any other way to attack this problem without assuming any extra hypothesis... Any ideas?","['measure-theory', 'geometric-measure-theory']"
3543506,How to find the stationary point under constraints analytically?,"I am working with the optimization problem from the paper, eq.(5) $$\max_{X=(x_1, x_2, \ldots, x_{n+1})} f(X)=(A-B\sum_{i=1}^n \frac{1}{x_i})\times x_{n+1}$$ subject
to $$x_{n+1}=1-2k\sum_{i=1}^n x_i,$$ $$x_i \geq 0, \quad i = 1,2,\ldots, n+1.$$ Here $A \gg B > 0 \in \mathbb{R}$ , $k \in \mathbb{Z}^+$ , and $f(X)>1$ . From the paper I know the stationary point $$X^*=(\underbrace{\sqrt{\frac{B}{2kA}}, \ldots, \sqrt{\frac{B}{2kA}}}_{n\text{  times}}, 1 − 2 kn\sqrt{\frac{B}{2kA}}
)$$ and the optimal value $$f(X^*)=(\sqrt{A} − n \sqrt{ 2 \cdot k \cdot B})^2.$$ Question. How to find the stationary point under constraints analytically? Is it possible for $n=3, k=10$ case? Attempt. I have tried to use the Lagrange multiplier: $$F(X, \lambda) = x_{n+1}(A-B\sum \frac{1}{x_i}) + \lambda(x_{n+1} - 1 + 2k\sum x_i)=0$$ and found the partial derivatives and have the $(n+2)$ system with $n+2$ variables: \begin{cases} 
F'_{x_i}(X, \lambda)= x_{n+1}\frac{B}{x_i^2} +2 \lambda k x_i=0, \quad i=1,2,..., n, \\ 
F'_{x_{n+1}}(X, \lambda)= A - B\sum \frac{1}{x_i}+\lambda=0, \\ 
F'_{\lambda}(X, \lambda)=x_{n+1} -1+2k\sum x_i =0. 
\end{cases} My problem now is how to express $x_i$ , $i=1,2,..., n$ , and $x_{n+1}$ through $\lambda$ and find roots. I know the stationary point and have found the A & Q .
I will use the  notation $\sum_{i=1}^n x_i := n \cdot x$ $\sum_{i=1}^n \frac{1}{x_i} := \frac{n} {x}$ , and $x_{n+1}:=y$ . Then the system will be $$y\frac{B}{x^2}+2\lambda k x=0, \tag{2.1}$$ $$\lambda=B\frac{n}{x}-A, \tag{2.2}$$ $$y=1-2knx. \tag{2.3}$$ Put $(2.2)$ and $(2.3)$ in $(1.1)$ : $$(  1-2knx )\frac{B}{x^2}+2 (B\frac{n}{x}-A ) k x=0, \tag{3.1}$$ Multiple both sides $(3.1)$ on $x^2$ : $$(  1-2knx )B+2 (B\frac{n}{x}-A ) k x^3=0, \tag{4.1}$$ Open brackets and collect tems: $$2kAx^3-2nBkx^2+2nBkx-B=0, \tag{5.1}$$ divide both sides on $2kA$ : $$x^3 - n\frac{B}{A}x^2 + n \frac{B}{A}x-\frac{1}{2k}\frac{B}{A}=0. \tag{6.1}$$ One can see the equation of power $3$ , I am looking for a root $x \in \mathbb{R}$ . I think the equation $(6.1)$ should has a simple real root and complex pair.","['lagrange-multiplier', 'multivariable-calculus', 'nonlinear-optimization']"
3543510,"Let $f(x) = x^4 + ax^3 + bx^2 + cx + d$ be a polynomial whose roots are all negative integers. If $a + b + c + d = 2009,$ find $d.$","Let $f(x) = x^4 + ax^3 + bx^2 + cx + d$ be a polynomial whose roots are all negative integers. If $a + b + c + d = 2009,$ find $d.$ How should I approach this problem? I have tried to think of a way I could plug some value of $x$ in, but they haven't given us anything about that so that wouldn't work...how should I do it?","['functions', 'polynomials']"
3543552,UMVUE of $\theta$ when $X_i$'s are i.i.d with pdf $f(x;\theta)=\theta x^{-(1+\theta)}I_{x>1}$,"I am trying to find the Unique minimum variance unbiased estimator for $\theta$ where $\{X_i\}_{i=i}^{n}\sim f(x;\theta)=\theta x^{-(1+\theta)}$ where $x>1$ and $\theta\in(1,\infty)$ . I start by showing that $f(x;\theta)=\theta x^{-(1+\theta)}$ is in the exponential: $$f(x;\theta)=e^{\ln(\theta)-(1+\theta)\ln(x)}I_{x>1}$$ Since $f(x;\theta)$ is a member of the exponential family of full rank since the parameter space contains an open interval. then $\sum_{i=1}^{n}\ln(x_i)$ is a complete and minimally sufficient statistic. Since, $g(x)=e^x$ is a one to one transformation then, $\prod_{i=1}^{n}x_i$ is also a minimally sufficient statistic. By a similar argument one can conclude that $S(X)=\sum_{i=1}^{n}x_i$ is also minimally sufficient and complete. Note: $$\int_{1}^{\infty}x\theta x^{-(1+\theta)}dx=\theta\int_{1}^{\infty}x^{-\theta}dx=\frac{\theta}{1-\theta}$$ Since, $\theta>1$ and $x>1$ . Then $E[\sum_{i=1}^{n}X_i]=\frac{n\theta}{1-\theta}$ . Note that can only achieve CR Lower bound if in exponential family and estimating a linear function of the minimum sufficient statistic. Note: $E[a+bS(X)]=a+\frac{bn\theta}{1-\theta}$ so no linear combination of $S(X)$ can achieve an unbiased estimator of $\theta$ so there does not exist a UMVUE for $\theta$ . IS my logic correct?","['statistical-inference', 'statistics', 'probability-distributions', 'parameter-estimation', 'expected-value']"
3543611,$f:\Bbb{R}\mapsto\Bbb{R}$ and $f$ is twice differentiable.,"$f:\Bbb{R}\mapsto\Bbb{R}$ and $f$ is twice differentiable such that $f(0)=2, f(1)=1, f'(0)=-2$ . Prove that $\exists$ a $\varepsilon \in (0,1)$ such that $f(\varepsilon)f'(\varepsilon)+f''(\varepsilon)=0$ . Suppose $g(x) = \frac{2}{f(x)}-1-x$ . Then $g(0)=g(1)=0\implies \exists$ $c \in (0, 1)$ such that $g'(c)=0$ by Rolle's theorem. $g'(x) = \frac{-2f'(x)-f(x)^2}{f(x)^2}$ . Then $g'(0)=g'(c)=0$ $2f'(x)+f(x)^2=0$ for $x=0, c$ . Suppose $h(x)=2f'(x)+f(x)^2$ . Then $h(0)=h(c)=0\implies \exists$ $k \in (0, c)$ such that $h'(k)=0$ by Rolle's theorem. $h'(k) = 2f''(k)+2f(k)f'(k)=0 \implies f''(k)+f(k)f'(k)=0$ for $k \in (0, c) \subset (0, 1)$ . Hence, Proved. But what to do if $f(k)=0$ for some $k \in (0, 1)$ . Either a new function is to be found out or we need to prove that $f(x)$ is non-zero in the interval.","['rolles-theorem', 'calculus', 'derivatives']"
3543617,$T_4$-ness that is preserved by product,"Sorgenfrey line demonstrates how normality can be not preserved when ""squared."" Is there an example for a normal space $X$ for each of?: $X^2$ is normal, but $X^3$ is not $X^2$ and $X^3$ are normal, but $X^4$ is not $X^k$ is normal for $2 \leq k < n$ , but $X^n$ is not $X^n$ is normal for $n \in \mathbb N$ , but $X^\omega$ is not $X^n$ is normal for $n \in \mathbb N$ , but $X^\omega$ is not, in box topology","['general-topology', 'separation-axioms']"
3543641,Is this proof that the ring of integers of an algebraic number field is a free Z-module incorrect?,"The theorem and proof in question are from Evan Chen's An Infinitely Large Napkin . I've tried to include the relevant part here, but if you wish to view it in context please see Theorem 47.2.12 from the PDF here . Theorem : Let $K$ be a number field of degree $n$ .
    Then its ring of integers $O_K$ is a free $\mathbb{Z}$ -module of rank $n$ ,
    i.e. $O_K \cong \mathbb{Z}^{\oplus n}$ as an abelian group.
    In other words, $O_K$ has a $\mathbb{Z}$ -basis of $n$ elements as $$ O_K = \left\{ c_1\alpha_1 + \dots
			+ c_{n-1}\alpha_{n-1} + c_n\alpha_n \mid c_i \in \mathbb{Z} \right\} $$ where $\alpha_i$ are algebraic integers in $O_K$ . Proof: Pick a $\mathbb{Q}$ -basis of $\alpha_1$ , ..., $\alpha_n$ of $K$ and WLOG the $\alpha_i$ are in $O_K$ by scaling. Consider $\alpha \in O_K$ ,
and write $\alpha = c_1\alpha_1 + \dots + c_n\alpha_n$ .
We will try to bound the denominators of $c_i$ .
Look at $N(\alpha) = N(c_1\alpha_1 + \dots + c_n\alpha_n)$ . If we do a giant norm computation, we find that $N(\alpha)$ is a polynomial in the $c_i$ with fixed coefficients.
(For example, $N(c_1 + c_2\sqrt 2) = c_1^2 - 2c_2^2$ , say.)
But $N(\alpha)$ is an integer , so the denominators of the $c_i$ have to be bounded by some very large integer $N$ .
Thus $$ \bigoplus_i \mathbb{Z} \cdot \alpha_i \subseteq O_K
	\subseteq \frac 1N \bigoplus_i \mathbb{Z} \cdot \alpha_i.  $$ The latter inclusion shows that $O_K$ is a subgroup
of a free group, and hence it is itself free.
On the other hand, the first inclusion shows it's rank $n$ . My issue is with the conclusion that the denominators of the $c_i$ are bounded. Consider for instance the polynomial $c_1 + c_2$ . Surely this can take integer values with arbitrarily small denominators, for instance with $\frac{1}{x} + \frac{x-1}{x}$ ? Is this a flaw in the proof, and if so, is the proof rescuable (for instance, using some more properties of the polynomial) or entirely wrong? (While the source is an informal document, I've yet to find any other major errors and am not exactly a math expert, so at the moment I'm leaning towards the proof being correct and me doing something obviously stupid.) Thanks!","['algebraic-number-theory', 'abstract-algebra', 'commutative-algebra', 'modules']"
3543675,Partial derivatives of a function of 1 independent variable,"I have a function $f$ of one variable $t$ , but I will write it in a funny way: using a second function $g$ as an intermediary: $$
f(g(t),t)=tg(t) 
$$ where $$
g(t)=t
$$ . The exact definitions of $g$ and $f$ don't matter too much, anything will do the trick. Before I proceed, note that I (knowing the definition of $g$ ) could write $f$ in several ways (dropping the parentheses): $$
f=tg\qquad 
f=t^2\qquad 
f=g^2\qquad 
f=t^3/g
$$ you can see that this could go on forever. The issue arrives when I want to take partial derivatives. Going in the same order as before: $$
\frac{\partial f}{\partial t}=g\qquad 
\frac{\partial f}{\partial t}=2t\qquad 
\frac{\partial f}{\partial t}=0\qquad 
\frac{\partial f}{\partial t}=3t^2/g
$$ and I could do the same with partials w.r.t $g$ . Now I recognize that because $f$ is only a function of $t$ , I should even be taking partials with respect to it, but by the way I defined $f$ using $g$ , the multivariable chain rule: $$
\frac{df}{dt}=\frac{\partial f}{\partial g}\frac{dg}{dt}+\frac{\partial f}{\partial t}
$$ still requires a definition of the partial w.r.t. $t$ . It should be noted that the total derivative w.r.t. $t$ (which should be $2t$ as $f(t)=t^2$ ) is retrieved from the multivariable chain rule if we keep the definitions of $f$ as a function of $g$ and $t$ consistent across the equation, i.e. if we just pick a definition and stick to it, it doesn't matter one, the total derivative will work. What's going on here, all I am doing is variable manipulations, but somehow the calculus seems intrinsically tied to the particular definitions of $f$ in terms of the dependent variable that I just made up. In a sense that is obvious. But Still. Am I doing something I am not allowed to do. Am I miscalculating something. Am I misinterpreting something. Obviously the partials aren't well defined if the variables are not independent. But there is more to it than that. Though this came up in the context of Lagrangian mechanics, where we regularly evaluate partials w.r.t. ""functions"" that solely depend on $t$ (I suspect there is something variation-y about that stuff though), the problem is easy to state, only relying on beginner-level calculus, and has me stumped. Any help is appreciated :)","['multivariable-calculus', 'calculus']"
3543737,Two way of computing blowup -- which one is correct?,"I came across reading Corollary 7.15 in Hartshorne's book. A special case of this corollary is the following statement If $Y,C\subset X$ are subvarieties, $\widetilde X\to X$ is the blowup of $X$ along $C$ and $\widetilde Y\to Y$ is the blowup along $Y\cap C$ , then there is an inclusion $\widetilde Y\subset \widetilde X$ . In other words, where exists a commutative diagram $\require{AMScd}$ \begin{CD}
\widetilde{Y}=Bl_{C\cap Y}Y @>>> \widetilde{X}=Bl_C X\\
@V  V V @VV V\\
Y @>>> X.
\end{CD} Now assume that $X=\mathbb C^n$ and thus that $Y\subset \mathbb C^n$ is an affine variety. Let $Z\subset Y$ be a fixed subvariety. My confusion arises when comparing the following two blowups: Take as center $C$ a variety such that $C\cap Y=Z$ and compute the blowup. Take as center $C$ the variety $Z$ itself. The point is that $C$ in 1. can be defined by fewer equations than the $C$ in 2. -- that is because the intersection with $Y$ can give additional equations (coming from the equations of $Y$ ). So it seems that there are two ways of computing the blowup of of $Y$ along $Z$ and they result in different results. Which one is the correct one and why? Example: Take $X=\mathbb C^4$ with coordinates $x_1,,\dots, x_4$ , $Y=\{x_1^2-x_2^2=0\}$ , $Z=\{x_1=x_2=x_3=0\}$ . For 1. we can take $C=\{x_2=x_3=0\}$ because $C\cap Y=\{x_1^2-x_2^2=x_2=x_3=0\}=Z$ . Then, $$\widetilde X=\{a_0x_2=a_1x_3\}\subset \mathbb C^4\times \mathbb P^1$$ and the strict transform of $Y$ can be computed in the corresponding charts to be chart 1 ( $a_0\neq 0$ ): $\widetilde Y\cap \{a_0\neq 0\}=\{x_1^2-a_1^2x_3^2=x_2-a_1x_3\}\subset \mathbb C^5$ , chart 2 ( $a_1\neq 0$ ): $\widetilde Y\cap \{a_1\neq 0\}=\{x_1^2-x_2^2=x_3-a_0x_2=0\}\subset \mathbb C^5$ . On the other hand, we can also blowup $X$ along $Z$ and obtain $$\widetilde X=\{a_0x_2-a_1x_1=a_0x_3-a_2x_1=a_1x_3-a_2x_2=0\}\subset \mathbb C^4\times \mathbb P^2$$ chart 1 ( $a_0\neq 0$ ): $\widetilde Y\cap \{a_0\neq 0\}=\{1-a_1^2=x_2-a_1x_1=x_3-a_2x_1=0\}\subset \mathbb C^6$ , chart 2 ( $a_1\neq 0$ ): $\widetilde Y\cap \{a_1\neq 0\}=\{a_0^2-1=x_1-a_0x_2=x_3-a_2x_2=0\}\subset \mathbb C^6$ , chart 3 ( $a_2\neq 0$ ): $\widetilde Y\cap \{a_2\neq 0\}=\{a_0^2-a_1^2=x_1-a_0x_3=x_2-a_3x_3=0\}\subset \mathbb C^6$ . In these charts we had to remove factors corresponding to the exceptional divisor. So it seems that $\widetilde Y=\{a_0^2-a_1^2=0\}\subset \widetilde X$ . Chart 3 of the second and chart 2 of the first are isomorphic. But I don't see any isomorphism between the remaining charts and there is also one more chart for the second than for the first. Moreover, using the first method, the inverse image of $Z$ is $Z\times \mathbb P^1$ , whilst for the second method it is $Z\times \mathbb P^2$ and from the equations there is nothing like an inclusion or so that I can see. So what is the point here? With both methods I calculated $Bl_Z Y$ but the results seem to be very different from each other.","['algebraic-geometry', 'blowup', 'differential-geometry']"
3543775,Each element of an algebraic group has an nth root,"I have a problem with this exercise form Shafarevich Basic Algebraic Geometry Let G be an algebraic group and suppose it to be abelian. Define $\varphi_n:G\rightarrow G$ by $\varphi_n(g)=g^n$ . Supposing that the
  ground field has characteristic 0, prove that d $_e\varphi_n$ is a
  nondegenerate linear map. Deduce from this that in a Abelian algebraic
  group the number of elements of order n is finite, and that every
  element has an n th root. I cannot realize how to determine d $_e\varphi_n$ , is it just $ng^{n-1}$ ? Please help!","['algebraic-geometry', 'abstract-algebra', 'linear-algebra']"
3543833,Hausdorff-dimension of non measurable sets?,The hausdorff-outer-measure is defined for all subsets of a metric space. The hausdorff measure is defined as the restriction to caratheodory measurable sets. I actually don't know how the set of hausdorff measurable sets look like but since n-dimensional hausdorff measure and n-dimensional lebesgue measure coincidence when n is an integer there should be non-measurable sets for hausdorff measure. However Hausdorff-dimension is often defined for all sets. What is the Hausdorff-dimension of non measurable sets? Or how is the dimension for such sets even defined?,"['measure-theory', 'hausdorff-measure', 'dimension-theory-analysis']"
3543876,Nonexistence of a simple group of order 576,"Prove that there exists no simple group of order 576. Suppose $G$ is simple of order 576. It is a straightforward application of the index theorem to determine that the number of Sylow 2-subgroups of $G$ is 9. Let $G$ act on the set of Sylow 2-subgroups by conjugation. The size of the stabilizers on one element is 576/9 = 64 each. Observation 1. Each element of the stabilizer on one element has order a power of 2; considering that $G$ has an injective image in $A_{9}$ , we narrow down the cases to order 8 (8-cycle is odd, eliminated), order 4 (2 4-cycles or 1 4-cycle and 1 2-cycle), or order 2 (2 2-cycles or 4 2-cycles). Observation 2. The stabilizers are Sylow 2-subgroups themselves, thus each Sylow 2-subgroup is equal to its normalizer. By a counting argument, the intersection of two Sylow 2-subgroups must be greater than 64*64/576, thus is either 8 or 16. 32 is eliminated since then the normalizer of the intersection is of index at most 3 in $G$ . How to proceed from here? Clearly, combining the two observations, we see that the intersection of two stabilizers has at least 8 elements, thus there exists a 2 2-cycle type element (the power of a 1 4-cycle and 1 2-cycle element is a 2 2-cycle) in the image of $G$ , which implies that the intersection of 5 normalizers is non-trivial. I don't see how to lead this argument into a contradiction, though. In the case that this approach fails, any other solutions to the problem are also appreciated. Edit: In response to the below comment by Dietrich Burde, I would like to solve the problem using minimal tools; preferably no transfer theory, Burnside's pq theorem, or classification of finite simple groups (obviously).","['finite-groups', 'simple-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
3543987,Multivariate normal probability of being inside ellipse,"Assume that $\mathbf{X}$ is a bivariate normal random variable $$\mathbf{\mu} = E\mathbf{X} = \begin{bmatrix} 0 \\ 2 \end{bmatrix} \  \text{and} \ \Sigma = Cov \ \mathbf{X} = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix} $$ What is the probability that $\mathbf{X}$ falls within the elipse corresponding to $(\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1}(\mathbf{x} - \mathbf{\mu}) = 4.6 ?$ So Im assuming I need to calculate the probability of $x_1$ lying between $x_1$ min and $x_1$ max, and likewise for $x_2$ ? However Im not sure how to do this calculation seeing as the variables $x_1$ and $x_2$ are not independent. It also seems like the endpoints of the ellipse do not match my calculations. I believe the ellipse should have half axe lengths $\sqrt{\lambda_i b}$ and its centered at $\mathbf{\mu}$ . So according to my calculations $x_1$ should lie between -4.28 and +4.28 which seems incorrect from graphical inspection.","['statistics', 'conic-sections', 'geometry', 'normal-distribution', 'probability']"
3543992,Estimate $n$ such that $\log(n^C)<n$,"Given $C\in \Bbb N$ (which we can assume to be big), is there a simple way to estimate de value of $n$ such that the following formula is satisfied? $$\log(n^C)<n.$$ Equivalently, how can we estimate the index $n$ where the sequence $x_n=\frac{n}{\log(n)}$ exceed a given value $C\in\Bbb N$ ?","['real-numbers', 'sequences-and-series', 'estimation', 'real-analysis']"
3544022,Uniform probability distributions in the category of stochastic maps,"This question is mostly for the purpose of learning how to reason in category theory. Consider the category of finite stochastic maps. I imagine this is something fairly standard, but I've given a definition below in case it's not. I'll call this category FinStochMap. Products, coproducts, and the initial and final objects in FinStochMap are the same as in FinSet. In FinStochMap, a morphism from the terminal object to an object $A$ can be seen as a probability distribution over $A$ . To be concrete, consider a morphism in FinStochMap, $f:\{*\}\to A$ . This gives us a set of numbers $p(a|*)$ for every $a\in A$ , such that $\sum_{a\in A} p(a|*)=1$ , so we can interpret $p(a|*)$ as the probability of $a$ . In probability theory, an important kind of probability distribution is the uniform distribution. Among other things, this is the only distribution on a finite set that is invariant to permutations of its elements. My question is, is there a category-theoretic way to pick out the uniform probability distributions in FinStochMap? Are they the morphisms from the terminal object that obey a certain universal property, and if so, what is it? Definition of FinStochMap. By FinStochMap I mean the category where the objects are finite sets, and the morphisms are conditional probability distributions (or Markov kernels if you prefer), i.e. a morphism $f:A\to B$ is a function $p_f:A\times B \to [0,1]$ , with elements written $p_f(b|a)$ , such that $\sum_{b\in B} p(b|a)=1$ for all $a\in A$ . The composition rule is that for $f:A\to B$ and $g:B\to C$ , the composition $f;g:A\to C$ is given by $$
p_{f;g}(c|a) = \sum_{b\in B} p_f(b|a)p_g(c|b).
$$ An identity morphism $I_A:A\to A$ is given by $p_{I_A}(a|a') = 1$ if $a=a'$ , and 0 otherwise. Related question by me: expectations in FinStochMap .","['probability-theory', 'category-theory']"
3544034,Do most numbers have exactly $3$ prime factors?,"In this question I plotted the number of numbers with $n$ prime factors. It appears that the further out on the number line you go, the number of numbers with $3$ prime factors get ahead more and more. The charts show the number of numbers with exactly $n$ prime factors, counted with multiplicity: (Please ignore the 'Divisors' in the chart legend, it should read 'Factors') My question is : will the line for numbers with $3$ prime factors be overtaken by another line or do 'most numbers have $3$ prime factors'? It it is indeed that case that most numbers have $3$ prime factors, what is the explanation for this?","['number-theory', 'prime-factorization', 'elementary-number-theory']"
3544086,Local times in higher dimensions,"It is known (see e.g. Revuz/Yor 1991, Chapter VI Corollary 1.6) that for a one-dim. Brownian motion $B$ and a nonegative function $\phi$ it holds $$\int_0^t\phi(B_s)\text{d}s=\int \phi(a)L_t^a(B)\text{d}a$$ where $L_t^a$ is the local time of $B$ at the point $a$ up to time $t$ .
I am wondering, whether there is an analogous result in the multi-dimensional case. So $B=(B^1,B^2, \dots , B^n)$ and $\phi: \mathbb{R}^n \to\mathbb{R}$ . I know that the concept of local times must be ""reinvented"" for higher dimensions  but I was hoping that one can break the problem down to $n$ independent  one-dimensional Brownian motions, so that maybe the right hand side of the formula becomes a multidimensional integral?! I couldn't come up with a result by myself and didn't find anything in the literature. More general, I am asking myself whether there exists some concept of local times which measure the time $d$ -dim. Brownian motion has spent in a set $A \subset \mathbb{R}^d$ . Does anybody know?","['probability', 'stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'stochastic-calculus']"
3544099,How many normal distribution random numbers' sum exceeds $r$?,"There is a random number generator that obeys the standard normal distribution $X \sim N(\mu,\sigma^2)$ , and then calculates the sum of the numbers generated until the sum is greater than $r$ . Specifically, it means to generate a random number, and then stop if it exceeds $r$ , otherwise generate another random number. Sum all generated random numbers, stop if exceeds $r$ , otherwise continue How to find the expectation of the stop time $\mathbb{E}_r[X]$ . Similar to this question","['random', 'stopping-times', 'normal-distribution', 'probability']"
3544172,Flow of a vanishing Vector Field.,"Let us consider a complete vector field $V:\Bbb C^n\to\Bbb C^n$ . Let us call $\phi_s:\Bbb C^n\to\Bbb C^n$ its flow at time $s\in\Bbb C$ , that is $$
\dot{\phi}_s(x)=V(\phi_s(x))\;\;\forall x\in\Bbb C^n, s\in\Bbb C
$$ $$
\phi_0(x)=x\;\;\forall x\in\Bbb C^n
$$ How can I prove that $V\equiv0$ on $A$ implies $\phi_s(x)=x \;\;\forall x\in A$ and $s\in\Bbb C$ ? Set $A$ as the common zero-set of a finite number of holomorphic polynomials (but maybe my statement holds for more general sets $A$ , like closed sets).","['ordinary-differential-equations', 'differential-geometry']"
3544197,Application of monotone class theorem on extending measurability to optional processes,"I am not sure in this proof how the monotone class theorem is applied. I am familiar with the following version of the monotone class theorem. But I cannot tell what becomes $\mathscr{C}$ and $\mathscr{H}$ in this proof. So here $\mathscr{O}$ is the $\sigma$ -algebra generated by all cadlag adapted processes.  It seems like saying that the set of all processes that are $\mathscr{F} \otimes \mathbb{R}_+$ - measurable and meet (a) and (b) for all stopping times is obviously a vector lattice and is stable under pointwise convergence suggest that this set must be $\mathscr{H}$ in the monotone class theorem below. And the natural candidate for $\mathscr{C}$ would be the set of all $\{X \in B\}$ for all cadlag adapted processes $X$ and Borel sets $B$ . But then I cannot show that this set is a $\pi$ -class. How is the intersection also of this form? And I cannot see why iii) would be satisfied as well. Finally, in the notes here, we assume that processes take values in $\mathbb{R}^d$ . So how can we extend this result on real processes to higher dimensions? I would greatly appreciate any help.","['measure-theory', 'real-analysis', 'stochastic-processes', 'functional-analysis', 'probability-theory']"
3544203,"Smarter way to solve $ \int_0^1\arctan(x^2)\,dx$ [duplicate]","This question already has answers here : Integrating a function of tan inverse (3 answers) Closed 4 years ago . I'm trying to solve the following definite integral: \begin{equation}
\int_0^1 \arctan(x^2)dx
\end{equation} I tryed first by parial integration, finding: \begin{equation}
x\arctan(x^2)\Bigl|_0^1-\int_0^1 \dfrac{2x^2}{1+x^4}dx
\end{equation} Then: \begin{equation}
\int_0^1 \dfrac{2x^2}{1+x^4}dx=\int_0^1 \dfrac{(x^2+1)+(x^2-1)}{1+x^4}dx=\int_0^1 \dfrac{x^2+1}{1+x^4}dx+\int_0^1 \dfrac{x^2-1}{1+x^4}dx
\end{equation} and i introduced the following substitution: \begin{equation}
t=x-\dfrac{1}{x}\qquad s=x+\dfrac{1}{x}
\end{equation} Than I used this weird substitution: \begin{equation}
\dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt{2}} \right)\Bigl|_0^1+\int_0^1\dfrac{ds}{(s-2)(s+2)}
\end{equation} In the end simple fraction: \begin{equation}
\left[\dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt 2} \right)+\dfrac{1}{4}\log(2-s)-\dfrac{1}{4}\log(s+2)\right]_0^1
\end{equation} and that is the solution. Now: does it exist a simpler path to solve this integral?","['alternative-proof', 'calculus', 'definite-integrals']"
3544229,When is a matrix-valued function the Hessian of some function?,"What are necessary and sufficient conditions for a matrix valued function $H: \mathbb{R}^d \rightarrow \mathbb{R}^{d\times{}d}$ to be the Hessian of a convex scalar function $F:\mathbb{R}^d \rightarrow \mathbb{R}$ , i.e. $H(x) = \nabla^2 F(x)$ . I have heard informally that the following are necessary and sufficient: 1) $H(x)$ is symmetric and positive semi-definite 2) $\frac{\partial}{\partial x_k} H_{ij}(x) = \frac{\partial}{\partial x_j} H_{ik}(x)$ for all $i,j,k$ however, I have not been able to find any rigorous statement.","['multivariable-calculus', 'real-analysis']"
3544265,"why can $\mathbb{E}_{x}[M^{\lambda}_{\inf{\{m, T_0, T_a\}}}]$ be decomposed this way?","I'm having a hard time understanding a step of the following : Start : Let $B$ be a Brownian motion starting at 0 and let $\lambda \in \mathbb{R} .$ Define $M_{t}^{\lambda}=e^{\lambda B_{t}-\lambda^{2} t / 2} $ $
\text { and } T_{a}=\inf \left\{t \geq 0: B_{t}=a\right\}
$ (1) First, let us remark that for all $t \geq 0, M_{t}^{\lambda} \geq 0$ and $$
\mathbb{E}\left[M_{t}^{\lambda}\right]=e^{-\lambda^{2} \frac{t}{2}} \mathbb{E}\left[e^{\lambda B_{t}}\right]=e^{-\lambda^{2} \frac{t}{2}} e^{\lambda^{2} \frac{t}{2}}=1
$$ Let $s \leq t$ $$
\begin{aligned}
\mathbb{E}\left[M_{t}^{\lambda} | \mathcal{F}_{s}\right] &=\mathbb{E}\left[e^{\lambda B_{t}-\lambda^{2} \frac{t}{2}} | \mathcal{F}_{s}\right] \\
&=\mathbb{E}\left[e^{\lambda\left(B_{t}-B_{s}\right)-\lambda^{2} \frac{t}{2}} e^{\lambda B_{s}} | \mathcal{F}_{s}\right]
\end{aligned}
$$ since $\left(B_{t}-B_{s}\right)$ is independent of $\mathcal{F}_{s}$ and $B_{s}$ is $\mathcal{F}_{s}$ measurable $$
=e^{\lambda B_{s}-\lambda^{2} \frac{t}{2}} \mathbb{E}\left[e^{\lambda\left(B_{t}-B_{s}\right)}\right]
$$ $$
\begin{array}{l}
{=e^{\lambda B_{s}-\lambda^{2} \frac{t}{2}+\lambda^{2} \frac{t-s}{2}}} \\
{=e^{\lambda B_{s}-\lambda^{2} \frac{s}{2}}} \\
{=M_{s}^{\lambda}}
\end{array}
$$ Hence $M^{\lambda}$ is a martingale. (2) Let $m \geq 0$ and let us denote $S_{m}=\inf \left\{T_{a}, T_{0}, m\right\} .$ Hence, $S_{m}$ is a stopping time since it is a minimum of stopping times. Furthermore, $S_{m}$ is bounded, and we have thanks to the optional stopping theorem, $$
\mathbb{E}_{x}\left[M_{S_{m}}^{\lambda}\right]=\mathbb{E}_{x}\left[M_{0}^{\lambda}\right]=e^{\lambda x}
$$ Furthermore, if $T_{0}=T_{a}=+\infty, 0 \leq B_{t} \leq a$ and for $\lambda>0, M_{S_{m}}^{\lambda}=$ $\exp \left(\lambda B_{m}-\lambda^{2} \frac{m}{2}\right) \leq \exp \left(\lambda a-\lambda^{2} \frac{m}{2}\right) \rightarrow_{m \rightarrow 0} 0$ When $T_{0}<+\infty$ and $T_{a}<+\infty,$ we have $M_{T_{0}}^{\lambda}=e^{-\lambda^{2} \frac{T_{0}}{2}} \leq 1$ and $$
M_{T_{a}}^{\lambda}=e^{\lambda a-\lambda^{2} \frac{T_{a}}{2}} \leq e^{\lambda a}, \text { and } M_{T_{0}}^{\lambda} \rightarrow \lambda_{\rightarrow 0} 1 \text { and } M_{T_{a}}^{\lambda} \rightarrow \lambda \rightarrow 0 \quad 1, \text { hence }
$$ $M_{\mathrm{inf}\left(T_{0}, T_{a}\right)}^{\lambda} \rightarrow_{\lambda \rightarrow 0} 1 .$ Hence we have $$\color{red}{
\mathbb{E}_{x}\left[M_{\mathrm{inf}\left(T_{0}, T_{a}\right)}^{\lambda} \mathbb{1}_{\sup \left(T_{0}, T_{a}\right)<+\infty}\right]+\mathbb{E}_{x}\left[M_{m}^{\lambda} \mathbb{1}_{\inf \left(T_{0}, T_{a}\right)=+\infty}\right]=e^{\lambda x} }
$$ By the dominated convergence theorem, $$
\mathbb{E}_{x}\left[M_{m}^{\lambda} \mathbb{1}_{\inf \left(T_{0}, T_{a}\right)=+\infty}\right] \rightarrow_{m \rightarrow \infty} 0
$$ End. why is $\mathbb{E}_{x}[M_{\inf{\{T_{0}, T_{a}\}}}^{\lambda} \mathbb{1}_{\sup{\{T_{0}, T_{a}\}}<+\infty}]+\mathbb{E}_{x}[M_{m}^{\lambda} \mathbb{1}_{\inf{\{T_{0}, T_{a}\}}= +\infty}] =\mathbb{E}_{x}[M^{\lambda}_{\inf{\{m, T_0, T_a\}}}]$ ? Shouldn't it be $$
\mathbb{E}_{x}\left[M_{\mathrm{inf}\left(T_{0}, T_{a}\right)}^{\lambda} \mathbb{1}_{\color{red}{\inf \left(T_{0}, T_{a}\right)<+\infty}}\right]+\mathbb{E}_{x}\left[M_{m}^{\lambda} \mathbb{1}_{\inf \left(T_{0}, T_{a}\right)=+\infty}\right]=e^{\lambda x} $$","['martingales', 'probability-theory']"
3544359,Function whose $n^{th}$ derivative at $x=0$ is $(n!)^2$,There exists a function in math where $$f^{(n)}(0)=(n!)^2$$ They say there's an analytical formula for this function but I do not know how to find it. I am investigating functions who cannot be represented with Taylor Polynomials and this is one of the examples I came about. Any help is appreciated! Thanks!,"['calculus', 'functions', 'taylor-expansion']"
3544363,Tangent cone of a non-isolated singular point,"Assume $X\subset \mathbb P^n$ is a variety (Edit: let's say $X$ is a hypersurface in $\mathbb P^n$ , as pointed out in the comment) and $x\in X$ is a singular point which is not isolated. Intuitively, I think the quadratic tangent cone at $x$ (i.e. the $Z(f_2)$ where $f=f_2+f_3+ \cdots$ is the defining equation of $X$ on an affine chart centered at $x$ ) is always degenerate, as it should degenerate ""along"" the singular locus. Is this true? And is there any reference about this? Thanks!","['complex-geometry', 'algebraic-geometry', 'intersection-theory', 'singularity-theory']"
3544387,Solve equation $\lfloor\arcsin x\rfloor+\lfloor\arccos x\rfloor+\lfloor\arctan x\rfloor=\ln x$,"Solve over reals: $$\lfloor\arcsin x\rfloor+\lfloor\arccos x\rfloor+\lfloor\arctan x\rfloor =\ln x$$ I believe it has no solution, but I don't know how to prove it.","['trigonometry', 'ceiling-and-floor-functions']"
3544523,"Prove $f (1)<\frac32$ given $f (0)=0$, $f'(0) = 1$, $f(x)+1=\frac1{f''(x)}$","Let $f: \mathbb {R}\rightarrow[0,+\infty)$ be a two times differentiable function with $f (0)=0$ , $f'(0) = 1$ , and $f(x)+1=\dfrac {1}{f''(x)}$ for $x\in [0,1]$ . Show that $f (1)<\dfrac {3}{2} $ So far I have tried to apply the mean value theorem and find out that $f (1) = f'(c) $ , for some $c\in(0,1) $ Applying the same to the second derivative, I got $$f'(1) - 1 = f''(k) $$ $$f'(1) = \dfrac {1}{1+f (k)} + 1$$ For some $k\in(0,1) $ Also finded that $f''(0) = 1$ . , and $f''(x) $ is never $0$ . But I can't advance from here. Any hints? Thanks!","['calculus', 'derivatives']"
3544551,"Prove, by induction, that $\varphi^{n+1} = \varphi \cdot F_{n+1} + F_n$ for all natural numbers $n$.","Fibonacci Rules: \begin{align*}
  F_0 = 0 && F_1 = 1 && F_{n+2} = F_n + F_{n+1}
\end{align*} The first ten terms of the Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21,
34. If you pull out a calculator and compute ratios of consecutive Fibonacci
numbers, you'll find that the ratio tends toward 1.6180339.... This number
is the golden ratio , denoted $\boldsymbol{\varphi}$ (the Greek letter
phi). Its exact value is $\varphi = \frac{1+\sqrt{5}}{2}$ , and $\varphi$ is the
positive solution to the quadratic equation $x^2 = 1 + x$ . Prove, by induction, that $\varphi^{n+1} = \varphi \cdot F_{n+1} + F_n$ for all natural numbers $n$ . While you can solve this problem by substituting $\varphi = \frac{1+\sqrt{5}}{2}$ and doing a bunch of algebra, you might find it more useful to use the fact that $\varphi$ is a solution to the equation $x^2 = x + 1$ My Working: P(n) = $\varphi^{n+1}$ is equal to $\varphi * F_{n+1} + F_n$ My base case: P(0): $\varphi^1$ = $\varphi * F_1 + F_0$ = $\frac{1+\sqrt{5}}{2}$ I assume P(k) is true so I can prove P(k+1) P(k+1): $\varphi^{k+2}$ = $\varphi * F_{k+2} + F_{k+1}$ This is where my problem main starts: 
I can't simplify this further to prove it because I don't understand how $\varphi$ being the solution to $x^2 = 1 + x$ is suppose to help me solve the problem. Can someone please explain me that? I'm only asking for an explanation and hints, if possible please don't write the entire solution to the proof.","['proof-explanation', 'proof-writing', 'recurrence-relations', 'discrete-mathematics', 'induction']"
3544559,$\frac{(x-y)^3}{x+y}\neq g(f(x)-f(y))$?,"$$h(x,y)=\frac{(x-y)^3}{x+y}$$ Prove that there does not exist 1D real functions $f,g$ such that $h(x,y)=g(f(x)-f(y))$ . The problem seems really really easy because it is obvious that $x+y\neq f(x)-f(y)$ . But I only have a really complicated approach outlined below, which is not necessarily correct. Do you have a simple proof? My try: By contradiction. suppose that $h=g(f(x)-f(y))$ . If we further assume that $h(a,b)\geq h(\alpha,\beta)$ and $h(b,c)\geq h(\beta,\gamma)$ , we must have: $h(a,c)\geq h(\alpha,\gamma)$ . However, since $$h(x,y)=\frac{(x-y)^3}{x+y}$$ Let $a=1$ , $b=2$ , $c=3$ ; $\alpha=95.95$ , $\beta=100$ , $\gamma=103.44$ . We have: $h(\alpha,\gamma)=2.10$ , which is less than $h(a,c)=4$ $h(a,b)=1/3$ , $h(b,c)=1/5$ , which are less than $h(\alpha,\beta)=0.339$ , $h(\beta,\gamma)=0.2001$ Here is another approach but assuming the differentiability of $f$ and $g$ . Assume $h=g(f(x)-f(y))$ The partial derivatives: $h_x=g'f_x$ $h_y=g'f_y$ $h_x/h_y=f_x/f_y$ Therefore $$h_x/h_y=\frac{x+2y}{-2x-y}=f_x/f_y$$","['functional-equations', 'functional-inequalities', 'multivariable-calculus', 'functions', 'algebra-precalculus']"
3544581,Prove $\mathbf{E}[X^2] \geq \frac{ \mathbf{E}[X^{2\alpha}]}{ \mathbf{E} [X^{\alpha}]^2}$,"I have a square-integrable random variable $X > 0$ with $\mathbf{E}[X] = 1$ and $0 < \alpha < 1$ .
I would like to prove that: \begin{equation}
\mathbf{E}[X^2] \geq \frac{ \mathbf{E}[X^{2\alpha}]}{ \mathbf{E} [X^{\alpha}]^2}
\end{equation} It seems to be true numerically, but I can't come up with the analytical proof. Any ideas? Thank you!","['expected-value', 'inequality', 'probability-theory']"
3544583,Proof of existence of $a$ in $\lim_{h\rightarrow0}\frac{a^h-1}{h}=1$,"In deriving the derivative of function $f(x)=\exp(x)$ , it is often pointed out that in the general case of $f_a(x)=a^x$ the following expression can be deduced from the definition of the derivative: $$
\frac{d}{dx}a^x = a^x\cdot\left(\lim_{h\rightarrow0}\frac{a^h-1}{h}\right)
$$ with $e$ defined as the real number for which the above limit term is equal to 1. However, this seems unsatisfactory to me as the existence of a real number $a$ satisfying the equation $\lim_{h\rightarrow0}\frac{a^h-1}{h}=1$ is not established. Can somebody please point me to a proof of the existence of such a number or provide it if possible? Thanks.","['calculus', 'derivatives', 'exponential-function']"
3544604,Show that $\sigma(\text{elementary cylinders})=\sigma(\text{cylinders)}$.,"Let $\mathbb{R}^{\mathbb{T}}$ denote the set of all functions $x:\mathbb{T}\longrightarrow\mathbb{R}$ , where $\mathbb{T}$ is just some index sets (the time in the stochastic process). Then we can define the cylinder set as $$\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\}\ \text{for some}\ B\in\mathcal{B}(\mathbb{R}^{n}).$$ We can also define the elementary cylinder as $$\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B_{1}\times\cdots\times B_{n}\}\ \text{for some}\ B_{1},\cdots, B_{n}\in\mathcal{B}(\mathbb{R}).$$ Then, denote the collection of all cylinder sets to be $\mathcal{C}$ and denote the collection of all elementary cylinders to be $\mathcal{E}$ . Then I want to show $\sigma(\mathcal{C})=\sigma(\mathcal{E})$ . I have some attempt but I could not show they coincided: Here is my attempt: Firstly, for any $E_{1}\in\mathcal{E}$ , it can be written as $$E_{1}=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in A_{1}\times\cdots\times A_{n}\},$$ for some $A_{1},\cdots, A_{n}\in\mathcal{B}(\mathbb{R})$ . But recall that $\mathcal{B}(\mathbb{R}^{n})=\sigma\Big(\{B_{1}\times\cdots\times B_{n}:B_{1}\in\mathcal{B}(\mathbb{R}),\cdots,B_{n}\in\mathcal{B}(\mathbb{R})\}\Big)$ is the smallest $\sigma-$ algebra containing all of such generating sets, and thus it must be that $A_{1}\times\cdots\times A_{n}\in \mathcal{B}(\mathbb{R}^{n})$ . Therefore, $E_{1}$ is also a cylinder set. That is, $E_{1}\in\mathcal{C}$ . Thus, $\mathcal{E}\subset\mathcal{C}$ . Note that $\sigma(\mathcal{C})$ is a $\sigma-$ algebra and must be $\lambda-$ system. Also, it follows from the here: (Only Proof Checking) Show that the collection of all elementary cylinders is a semi-algebra. that $\mathcal{E}$ is a $\pi-$ system. Combining above, it follows from Dynkin's $\pi-\lambda$ Theorem that $\sigma(\mathcal{E})\subset\sigma(\mathcal{C})$ . However, I don't know how to show the other direction. One book containing a really short argument saying that Clearly every cylinder belongs to the $\sigma-$ algebra generated by elementary cylinders, and therefore $\sigma-$ algebra generated by elementary cylinders and all cylinders coincide. From here: Show that the collection of cylinders form an algebra. we know that $\mathcal{C}$ is an Algebra, and thus $\sigma(\mathcal{C}))=\mathcal{C}$ , so if it is indeed true that every cylinder belongs to $\sigma-$ algebra generated by elementary cylinders, then we have $\sigma(\mathcal{C})=\mathcal{C}\subset\sigma(\mathcal{E})$ . However, I don't know how to convince myself  that every cylinder belongs to the $\sigma-$ algebra generated by elementary cylinders... In addition, it will be really appreciated if someone could have a check my proof for the first inclusion. Please help! Thank you so much!","['stochastic-processes', 'measure-theory', 'solution-verification', 'probability-theory']"
3544639,Defining a Partition on Z,I'm having difficulty answering/proofing this question I have in my set theory class. QUESTION HERE ALONG WITH MY WORK,['elementary-set-theory']
3544667,Do Jónsson-Tarski algebras form a Schreier variety?,"That is: is a subalgebra of a free Jónsson-Tarski algebra (aka Cantor algebra) again free? I’ve almost convinced myself that it is, but (as with groups) a rigorous proof would be somewhat involved. Perhaps there are general considerations implying this is true?","['universal-algebra', 'abstract-algebra']"
3544694,What is the precise definition of a coordinate system?,"Let's say we are in $\mathbb{R}^n$ and someone says let $(x_1,\ldots, x_n)$ be a coordinate system. What does this precisely mean? A partial answer would be to say that this means that there exists a basis of $n$ vectors $e_1,\ldots, e_n$ and the point $(x_1,\ldots, x_n)$ is just shorthand notation for the point $x_1e_1+\ldots +x_ne_n$ . This clearly is too prohibitive since, for example, it does not allow for polar coordinates. So what is it then?","['coordinate-systems', 'geometry', 'polar-coordinates', 'spherical-coordinates', 'differential-geometry']"
3544731,Show that $\sum_{n=0}^\infty \frac{1}{n+1} \binom{2n}{n} \frac{1}{2^{2n+1}} = 1.$,"Question: Show that $$\sum_{n=0}^\infty \frac{1}{n+1} \binom{2n}{n} \frac{1}{2^{2n+1}} = 1.$$ From Wolfram alpha , it seems that the equality above is indeed correct. But I do not know how to prove it.
Any hint is appreciated.","['calculus', 'convergence-divergence', 'summation', 'sequences-and-series']"
3544755,How many ways are there to eat a chocolate bar?,"I'm teaching an intro programming course and came up with a recursion problem for my students to solve that's inspired by the game Chomp . Here's the problem statement: You have a chocolate bar that’s subdivided into individual squares.
  You decide to eat the bar according to the following rule: if you
  choose to eat one of the chocolate squares, you have to also eat every
  square below and/or to the right of that square. For example, here’s one of the many ways you could eat a 3 × 5
  chocolate bar while obeying the rule. The star at each step indicates
  the square chosen out of the chocolate bar, and the gray squares
  indicate which squares must also be eaten in order to comply with the
  above rule. The particular choice of the starred square at each step was
  completely arbitrary, but once a starred square is picked the choice
  of grayed-out squares is forced. You have to eat the starred square,
  plus each square that’s to the right of that square, below that
  square, or both. The above route is only one way to eat the chocolate
  bar. Here’s another: As before, there’s no particular pattern to how the starred squares
  were chosen, but once we know which square is starred the choice of
  gray squares is forced. Now, given an $m \times n$ candy bar, determine the number of different ways you can eat the candy bar while obeying the above rule. When I gave this to my students, I asked them to solve it by writing a recursive function that explores all the different routes by which the chocolate bar could be eaten. But as I was writing this problem, I started wondering - is there a closed-form solution? I used my own solution to this problem to compute the number of different sequences that exist for different values of $m$ and $n$ , and here's what I found: $$\left(\begin{matrix}
1 & 1 & 1 & 1 & 1 & 1 & 1\\
1 & 1 & 2 & 4 & 8 & 16 & 32\\
1 & 2 & 10 & 58 & 370 & 2514 & 17850\\
1 & 4 & 58 & 1232 & 33096 & 1036972 & 36191226\\
1 & 8 & 370 & 33096 & 4418360 & 768194656 & 161014977260\\
1 & 16 & 2514 & 1036972 & 768194656 & 840254670736 & 1213757769879808\\
1 & 32 & 17850 & 36191226 & 161014977260 & 1213757769879808 & 13367266491668337972
\end{matrix}\right)$$ Some of these rows show nice patterns. The second row looks like it's all the powers of two, and that makes sense because if you have a $1 \times n$ chocolate bar then any subsequence of the squares that includes the first square, taken in sorted order, is a way to eat the candy bar. The third row shows up as A086871 on the OEIS, but none of the rows after that appear to be known sequences. The diagonal sequence also isn't on the OEIS, I believe that this problem is equivalent to a different one: Consider the partial order defined as the Cartesian product of the less-than relation over the sets $[m] = \{0, 1, 2, ..., m - 1\}$ and $[n]$ . How many distinct sequences of elements of this partial order exist so that no term in the sequence is dominated by any previous element and the final element is the maximum element of the order? I'm completely at a loss for how to determine the answer to that question. Is there a nice closed-form solution to this problem?","['recreational-mathematics', 'combinatorics']"
3544757,(Proof Verification) The necessary and sufficient condition for $X:\mathbb{T}\times\Omega\longrightarrow\mathbb{R}$ to be a stochastic process.,"Recall that a stochastic process $\{X_{t}:t\in \mathbb{T}\}$ defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ leads to the mapping $\xi:\Omega\longrightarrow\mathbb{R}^{\mathbb{T}}$ which maps an outcome $\omega\in\Omega$ to the corresponding trajectory of the process, namely $\{t\mapsto X_{t}(\omega)\}\in\mathbb{R}^{\mathbb{T}}.$ Define the elementary cylinder as $$\{x\in\mathbb{R}^{\mathbb{T}}:x_{t_{1}}\in B_{1},\cdots, x_{t_{n}}\in B_{n}\},$$ for some $B_{1},\cdots, B_{n}\in\mathcal{B}(\mathbb{R})$ . Then, define the cylindrical $\sigma-$ algebra by $\mathcal{B}(\mathbb{R}^{\mathbb{T}}):=\sigma(\text{elementary cylinders}).$ Now, I'd like to show that $X:\mathbb{T}\times\Omega\longrightarrow\mathbb{R}$ is a stochastic process if and only if $X$ seen as $X:\Omega\longrightarrow\mathbb{R}^{\mathbb{T}}$ is $(\Omega,\mathcal{F})\longrightarrow(\mathbb{R}^{\mathbb{T}},\mathcal{B}(\mathbb{R}^{\mathbb{T}}))-$ measurable. I showed the direction $(\Rightarrow)$ without problem, I ""kind of"" proved the converse direction but I had a question in the end: Here is my proof for $(\Leftarrow)$ : Suppose $\xi:\Omega\longrightarrow\mathbb{R}^{T}$ is $(\Omega,\mathcal{F})\longrightarrow(\mathbb{R}^{\mathbb{T}},\mathcal{B}(\mathbb{R}^{\mathbb{T}}))-$ measurable.  Then by hypothesis, for any elementary cylinder $E$ , we have $\xi^{-1}(E)\in\mathcal{F}$ . In particular, let $B\in\mathcal{B}(\mathbb{R})$ , the elementary cylinder defined by $$E:=\{x\in\mathbb{R}^{\mathbb{T}}:x_{t_{1}}\in B, x_{t_{2}}\in\mathbb{R},\cdots,x_{t_{n}}\in\mathbb{R}\},$$ has the preimage \begin{align*}
\xi^{-1}(E)&=\{\omega\in\Omega:X_{t_{1}}(\omega)\in B, X_{t_{2}}(\omega)\in\mathbb{R},\cdots, X_{t_{3}}(\omega)\in\mathbb{R}\}\\
&=X_{t_{1}}^{-1}(B)\cap X_{t_{2}}^{-1}(\mathbb{R})\cap\cdots\cap X_{t_{n}}^{-1}(\mathbb{R})\\
&=X_{t_{1}}^{-1}(B)\cap\Omega\cap\cdots\cap\Omega\\
&=X_{t_{1}}^{-1}(B).
\end{align*} But by hypothesis $\xi^{-1}(E)\in\mathcal{F}$ , so we must have $X_{t_{1}}^{-1}(B)\in\mathcal{F}$ . Since this holds for all $B\in\mathcal{B}(\mathbb{R})$ , we are able to conclude that $X_{t_{1}}$ is measurable. We can do the similar things to each $X_{t_{i}}$ by just using $B\in\mathcal{B}(\mathbb{R})$ at $i^{th}$ coordinate and using $\mathbb{R}$ at all other coordinates. Thus, each $X_{t_{i}}:\Omega\longrightarrow\mathbb{R}$ is measurable, and thus $X$ is a stochastic process. The question I had is: my argument only showed $X_{t_{i}}$ is measuralble for finitely many of $t_{i}$ , but a stochastic process $\mathbb{T}$ is not necessarily finitely indexed, i.e $\mathbb{T}$ may have infinite indices, for instance $\mathbb{T}=\mathbb{R}_{+}$ . How could I adapt my proof to such a case? or I am having a misconception here so my current proof is okay? Thank you so much! Edit 1: Since it seems that I asked a trivial direction and said I could show the harder direction, I think it is better to post my proof of $(\Rightarrow)$ . Suppose $X:\mathbb{T}\times\Omega\longrightarrow\mathbb{R}$ is a stochastic process. Recall that a stochastic process $\{X_{t}:t\in \mathbb{T}\}$ defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ leads to the mapping $\xi:\Omega\longrightarrow\mathbb{R}^{\mathbb{T}}$ which maps an outcome $\omega\in\Omega$ to the corresponding trajectory of the process, namely $\{t\mapsto X_{t}(\omega)\}\in\mathbb{R}^{\mathbb{T}}.$ This mapping is $(\Omega,\mathcal{F})\longrightarrow(\mathbb{R}^{\mathbb{T}},\mathcal{B}(\mathbb{R}^{\mathbb{T}}))-$ measurable, since the preimage of any elementary cylinder $$E=\{x\in\mathbb{R}^{\mathbb{T}}:x_{t_{1}}\in B_{1},\cdots, x_{t_{n}}\in B_{n}\},$$ is given by $$\xi^{-1}(E)=\{\omega\in\Omega:X_{t_{1}}(\omega)\in B_{1}, \cdots, X_{t_{n}}(\omega)\in B_{n}\}=X_{t_{1}}^{-1}(B_{1})\cap\cdots\cap X_{t_{n}}^{-1}(B_{n}).$$ But by hypothesis, each $X_{t_{i}}:\Omega\longrightarrow\mathbb{R}$ is measurable, and thus $X_{t_{i}}^{-1}(B_{i})\in\mathcal{F}$ for each $i$ , and thus so is their intersection. Therefore, $\xi^{-1}(E)\in\mathcal{F}$ . Thus $\xi$ is $(\Omega,\mathcal{F})\longrightarrow(\mathbb{R}^{\mathbb{T}},\mathcal{B}(\mathbb{R}^{\mathbb{T}}))-$ measurable, as desired.","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'solution-verification', 'probability-theory']"
3544758,Proving a seemingly simple inequality using induction.,"I encountered an exercise asking to prove the following inequality: $4log_2(n) \leq n$ for all integers $n \geq 16$ strictly using induction. I understand intuitively why the inequality holds and I've thought of alternative methods to prove it, but my challenge has been proving it with induction. It seems like I've tried all sorts of algebraic manipulations and tricks with logarithms, but nothing works.","['logarithms', 'induction', 'discrete-mathematics', 'algorithms', 'inequality']"
3544760,Mean value theorem and the continuity of step length,"Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}, f\in C^1$ . Then for two points $a,b \in \mathbb{R}$ , we know by mean value theorem we know that there exists $t \in (0, 1)$ such that $$
f^\prime(a+t(b-a)) = \frac{f(b)-f(a)}{b-a}
$$ We can view $t$ as a parameter collectively decided by $a,b$ and $f(\cdot)$ . Now, suppose that we hold $a$ to be fixed and move $b$ , what can be said about the changes in $t$ corresponding to the changes in $b$ (e.g. can we argue if $t(a,b)$ is continuous)? Thanks in advance for your hints/suggestions!","['integration', 'calculus', 'derivatives', 'rolles-theorem']"
3544814,Method of steepest descent: why can we relate these two contours?,"Let $f,g:\mathbb{C}\to \mathbb{C}$ be two functions which are holomorphic in $\Omega\subset \mathbb{C}$ . Consider the integral $$I(\lambda)=\int_{\Gamma}g(z)e^{\lambda f(z)}dz,\quad \lambda \in (0,+\infty)$$ where $\Gamma$ is a contour in $\Omega$ . I want to understand the method of steepest descend which allows to approximate $I(\lambda)$ as $\lambda \to +\infty$ . Now, if I understand, the rough idea is to deform the contour into another contour $\Gamma'$ passing through a saddle point of $f(z)$ in the direction of steepest descent of its real part. To do so we look for a saddle point $f'(z_0)=0$ , expand $f(z)$ up to second order around it $$f(z)=f(z_0)+\frac{1}{2}(z-z_0)f''(z_0)+\cdots$$ and we parameterize $z - z_0 = r_1e^{i\theta_1}$ . Also letting $\frac{1}{2}f''(z_0)=r_2 e^{i\theta_2}$ we have the changes in the real and imaginary parts of $f$ : $$\operatorname{Re}[f(z)-f(z_0)]=r_1^2r_2\cos(2\theta_1+\theta_2),\quad \operatorname{Im}[f(z)-f(z_0)]=r_1^2r_2\sin(2\theta_1+\theta_2).$$ The direction of steepest descent has vanishing change in the imaginary part and negative change in the real part. These two conditions give $2\theta_1+\theta_2$ either $\pi$ or $3\pi$ . Therefore the desired contour $\Gamma'$ can be parameterized as $$z(t)=z_0+\frac{t}{\sqrt{r_2}}e^{i\theta_1}$$ Question: why can we deform $\Gamma$ into $\Gamma'$ and not change $I(\lambda)$ ? I mean, I do know that from Cauchy's theorem if $\Gamma$ and $\Gamma'$ have the same endpoints then the integral is the same along both. But in this whole derivation I see no reason why $\Gamma'$ would share endpoints with $\Gamma$ .","['integration', 'complex-analysis', 'calculus', 'asymptotics']"
3544912,Holomorphic function with only removable singularities implies entire?,"Suppose $f$ is a function which is holomorphic on $\mathbb{C}\setminus A$ where $A$ is the set of points where $f$ has a singularity. Suppose that all of the points in $A$ are removable singularities of $f$ . Here is my question: does this imply that $f$ is itself entire? I understand that by Riemann Extension Theorem, $f$ can be extended to an entire $F$ , but my question has to do with whether we can say that $f$ itself is entire. I have seen a few other questions on this site which make such a statement, e.g. Removable singularities and an entire function and I am not sure if they are just slurring notation or if I am missing something. The context in which this arose: I am trying to show that if two entire functions $f,g$ are such that $|f|\leq |g|$ , then one is a multiple of the other. Obviously the strategy is to take quotient, and show that each singularity is removable. I have been able to do this, but then after that I am lost. I know I am supposed to use liouville to show that bounded and entire implies constant, but I am not sure if $|f|/|g|$ is itself entire. Is it not supposed to be some extended function which is supposed to be entire? With such an extended function, indeed we would have bounded and entire, but then I am not sure how to show that $f$ and $g$ are multiples of one another on all of $\mathbb{C}$ , since things get strange around the singularities. I would appreciate anything which clarifies my understanding.","['complex-analysis', 'complex-numbers']"
3544959,All Roots to be on unit circle,"Suppose $p(z)=1+a(z+z^2+\cdots+z^{n-1})+z^n, a\in{\bf R}, n\geq 2.$ Then  the necessary and sufficient conditions for $p(z)$ to have all its roots on the unit circle are $-2/(n-1)\le a\le2n/(n-1)$ for odd $n$ and $-2/(n-1)\le a\le2$ for even $n$ .",['complex-analysis']
3544966,What is dot product attention actually doing?,"I've read multiple papers on machine learning attention mechanisms, but I fail to really understand what is going on from a basic level. I've never seen anything that deeply explains the concept of queries, keys, and values with real examples. Below are some examples of papers... [1] - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf [2] - https://arxiv.org/pdf/1901.05761.pdf An equation that is in both papers is the one below which is described as dot product attention. How can I really understand what the result of this means for the involved $Q$ , $K$ , and $V$ ?... $$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{d_k}\right)V
$$ There is also another variant which they called Laplacian attention which is defined as.. $$
\mathrm{Laplace}(Q, K, V) = WV \in \mathbb{R}^{n\times d_k}, \;\;\;\;\; W_i = \mathrm{softmax}((-|| Q - K ||_1)_{j=1}^n) \in \mathbb{R}^n
$$ I understand all of the processes involved, but I don't understand what the end result of these is and I feel like it is interfering with my ability to grasp papers that deal with attention","['matrices', 'machine-learning', 'linear-algebra']"
3544982,Show that a subset of a normed space is bounded iff its image by any operator is bounded,"Could you give me a hint with the following exercise? Let $X$ be a $\mathbb{K}$ -normed vector space and $Y\subset X$ , show that $Y$ is bounded respect to the norm on $X$ , iff, for all $T\in X'$ , $T(Y)$ is bounded on $\mathbb{K}$ . Thanks. Edited Using Kavi's hint: [ $\Rightarrow$ ] Let $T\in X'$ be arbitrary, then $T$ is continuous, as $Y$ is bounded, then $T(Y)$ is bounded. [ $\Leftarrow$ ] Suppose $T(Y)$ is bounded for every $T \in X'$ . If possible let $Y$ be unbounded so that $\|y_n\| \to \infty$ for some sequence $(y_n)$ in $Y$ . Define $S_n: X' \to K$ by $S_n(T)=T(y_n)$ . 
So, $$\|S_n(T)\|=\|T(y_n)\| $$ But, $\{\|T(y_n)\|\}$ is bounded for all $T\in X'$ by hyphotesis, so $\{\|S_n(T)\|\}$ is bounded for all $T\in X'$ . Applying Uniform Boundedness Principle (alias Banach–Steinhaus theorem) we get that $$\{ \|S_n\| \}$$ is bounded, so there is $C>0$ such that $$\|S_n\|\leq C,\forall n\in\mathbb{N}\quad\Rightarrow \sup\{\|S_n(T)\|:\|T\|=1\}=\|S_n\|\leq C,\forall n\in\mathbb{N} $$ $$\Rightarrow \|T(y_n)\|\leq C, \|T\|=1, \forall n\in\mathbb{N}.$$ On the other hand, by the Hanh-Banach Theorem, for every $x\in X$ , $$\|x\|=\sup\{|T(x)|,\|T\|=1\}$$ in particular for $y_n\in X$ $$\|y_n\|=\sup\{|T(y_n)|,\|T\|=1\}\leq C$$ is a contradiction.",['functional-analysis']
3545006,"Cusp form for $\text{SL}_2(\mathbb{Z})$ attaining a maximum on $\mathcal{D}=\{z\in\mathbb{H}\colon|\text{Re}(z)|\leq\frac12,\|z\|\geq 1\}$","Let $f:\mathbb{H}\to\mathbb{C}$ , $z\mapsto\sum\limits_{i=1}^\infty a_i\exp(2\pi iz)$ be a non-zero weight- $0$ cusp form for $\text{SL}_2(\mathbb{Z})$ . Then $|f|$ attains a maximum on $\mathbb{H}$ . We can restrict to finding a bound on the fundamental domain $\mathcal{D}=\{z\in\mathbb{H}\colon|\text{Re}(z)|\leq\frac{1}{2},\|z\|\geq 1\}$ , since $f$ has weight $0$ . Is is true that $f$ being a cusp form implies that $\lim\limits_{y\to\infty}f(x+iy)=0$ uniformly in $x$ for $|x|\leq\frac{1}{2}$ ? If it holds, I wanted to reason as follows. Pick some $z_0\in\mathcal{D}$ such that $f(z_0)\neq 0$ . Take $0<\epsilon<\frac{|f(z)|}{2}$ . Then there exists an $M\geq 0$ such that $|f(x+iy)|\leq\epsilon$ for all $x+iy\in\mathcal{D}$ with $y\geq M$ . Since the set $\mathcal{D}_M=\{z\in\mathbb{H}\colon|\text{Re}(z)|\leq\frac{1}{2},\text{Im}(z)\leq M,\|z\|\geq 1\}$ is compact, $|f|$ takes a maximum on $\mathcal{D}_M$ . Now this maximum cannot be on the line $y=M$ , since $|f(x+iM)|\leq\frac{|f(z_0)|}{2}$ . Does, it follows that $f$ takes some maximum value on $\mathcal{D}_M^\circ=\{z\in\mathbb{H}\colon|\text{Re}(z)|\leq\frac{1}{2},\text{Im}(z)<M,\|z\|\geq 1\}$ . Call this maximum $K$ . By constrution of $\mathcal{D}_M$ , we have that $|f(z)|<\frac{|f(z_0)|}{2}\leq K$ outside $\mathcal{D}_M$ , hereby proven that $|f|$ takes a maximum value on $\mathcal{D}$ .","['complex-analysis', 'maxima-minima', 'modular-forms', 'fourier-analysis']"
3545013,A question regarding an integrable function,"This question arose while I was reading a paper. Let $f$ be a positive real valued function that is integrable on $\mathbb{R}$ . So,there exists $F > 0$ s.t $$\int_{\mathbb{R}} f(x)dx = F~.$$ Now, the author claims that it's possible to find a function $u$ : $(0,1) \to \mathbb{R}$ s.t $$\dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = t~.$$ Here comes the tricky part, The author claims that, $u$ may be discontinuous but it's strictly increasing ;hence it's differentiable almost everywhere. The latter part of the statement follows from the Lebesgue's Differentiation theorem. But, How does the author claim that $u$ must be strictly increasing? Is it possible for $u$ to be discontinuous? (It's intuitive to think that the existence of such $u$ is possible but I'm interested in a rigorous proof of this claim) My thoughts, Since $f$ is integrable, let $f_0$ be it's anti-derivative so, $$\dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = \dfrac{1}{F} [f_0 (x)]_{-\infty}^{u(t)} = t~.$$ $$\dfrac{1}{F}(f_0 (u(t)) - \lim_{x \to -\infty} f_0(x)) = t$$ From this, it can be concluded that $f_0$ increases as $t$ increases. But, can we conclude $u$ increases as well?","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
3545136,Probability of collecting all 5 buying 7 chocolates,"I'm struggling to find where I made a mistake on the way to solving the following problem. Problem description : A grocery sells chocolates bars. There are $5$ kinds of stickers. Each chocolate is sold with $1$ sticker of one of these $5$ kinds. The probability to find any kind of stickers in any chocolate bar is the same. What's the probability of collecting all $5$ types of stickers if you buy $7$ chocolates at once? My solution : since the order of chocolate bars that I've bought is irrelevant, the total number of 7-chocolate bar sets that I can buy equals $$\left(\binom{5}{7}\right)=\binom{7+5-1}{7}=\binom{11}{7}=330$$ Now I'm solving the reverse problem: let's calculate the probability of failing to collect all $5$ kinds of stickers. To do that I need to calculate the number of 7-chocolate bar sets that have less than $5$ kinds of stickers, which means that I need sets with only $1$ kind of stickers, $2$ , $3$ and $4$ . Since, again, the order of chocolate bars doesn't matter, the number of such sets is $$\left(\binom{4}{7}\right)=\binom{7+4-1}{7}=\binom{10}{7}=120$$ Finally, my answer to the initial problem should be $P = 1 - \frac{120}{330}= 0.6363636364$ The answer key says it's $\approx 0.215$ Where's the flaw in my solution? I appreciate any help.","['combinatorics', 'probability']"
3545145,"whether the proof is true or false, $\partial\bar{\Omega}= \partial \Omega$, approximated by polynomials, simply-connected","$\Omega\subset\Bbb C$ is a bounded domain, $\partial\bar{\Omega}= \partial \Omega$ . For every $f(z)$ that is continuous on $\bar{\Omega}$ and analytic in $\Omega$ , $f(z)$ can be approximated by polynomials, uniformly on $\bar{\Omega}$ . (1). Show that $\Omega$ is  simply-connected. (2). IF the boundedness assumption on $\Omega$ is dropped, is $ \Omega$ simply-connected? (3). Find a bounded simply connected domain $\Omega$ that satisfies $\partial\bar{\Omega}= \partial \Omega$ and a $f(z)$ that is continuous on $\bar{\Omega}$ and analytic in $\Omega$ , But $f(z)$ can not  be approximated by polynomials, uniformly on $\bar{\Omega}$ $\Omega$ is  simply-connected if the winding numbers $n(\gamma,z_0)=0$ for every point $z_0$ in $\Bbb C\setminus \Omega$ and every closed, piecewise smooth path $\gamma$ in $\Omega$ . I guess (2) is  correct. whether the following  proof is true or false? Thanks a lot If $z_0\in \Bbb C\setminus \bar{\Omega}$ , then $f(z)=\dfrac{1}{z-z_0}$ is continuous on $\bar{\Omega}$ and analytic in $\Omega$ , $f(z)$ can be approximated by polynomials $p_n$ , uniformly on $\bar{\Omega}$ so $$n(\gamma,z_0)=\dfrac{1}{2\pi i}\int_{\gamma}\dfrac{1}{z-z_0}dz=\dfrac{1}{2\pi i}\lim_{n\to\infty} \int_{\gamma}p_n(z)dz=0 $$ If $z_0\in \partial \Omega$ , because $\partial\bar{\Omega}= \partial \Omega$ , so there exist ${z_n}\in\Bbb C\setminus \bar{\Omega}$ , such that $\lim z_n=z_o$ , with the consequence that $f_n =\dfrac{1}{z-z_n}$ converges normally to $f$ in $ \Omega$ . Since $z_n\in \Bbb C\setminus \bar{\Omega}$ , so $\int_{\gamma}\dfrac{1}{z-z_n}dz=0$ , then $$n(\gamma,z_0)=\dfrac{1}{2\pi i}\int_{\gamma}\dfrac{1}{z-z_0}dz=\dfrac{1}{2\pi i}\lim_{n\to\infty} \int_{\gamma}f_n(z)dz=0 $$ so,  for every point $z_0$ in $\Bbb C\setminus \Omega$ and every closed, piecewise smooth path $\gamma$ in $\Omega$ , $n(\gamma,z_0)=0$ , hence $\Omega$ is  simply-connected","['complex-analysis', 'polynomials', 'approximation-theory']"
3545179,"What is the local ring of a scheme $X$ along a subvariety $V$: $\mathcal{O}_{V,X}$?","I am trying to learn about scheme theoretic algebraic geometry, because I actually want to study the basics of interseciton theory. I stumbled across the term ""local ring $\mathcal{O}_{V,X}$ of a scheme $X$ along a closed subscheme/subvariety/primedivisor $V$ "" many times but I just can't find a definition of this term in my textbooks on schemes. (I have looked at Bosch, Eisenbud & Harris, FOAG by Vakil,...) I am sure they explain it but I just don't see it. Could anybody help me out here?","['local-rings', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3545185,$E[X_T 1_{T\ge t} | \mathscr{F}_{t \wedge T}] = E[X_T 1_{T\ge t} | \mathscr{F}_t]$ holds?,"Below is a theorem from Protter's Stochastic Integration and Differential Equations. I don't get a line from the proof. Namely, how do we get $E[X_T 1_{T\ge t} | \mathscr{F}_{t \wedge T}] = E[X_T 1_{T\ge t} | \mathscr{F}_t]$ ? It seems like we need to use the fact that for $H \in \mathscr{F}_t$ we have $H1_{T\ge t} \in \mathscr{F}_T$ . But I can't see how this can be applied here. I would  greatly appreciate any help.","['measure-theory', 'conditional-expectation', 'stochastic-processes', 'martingales', 'probability-theory']"
3545272,Show that : $\{y_n\}$ is relatively weakly compact.,"Let $(X,\|.\|) $ be a separable Banach space. Let $\{x_n\}$ be a sequence  relatively weakly compact. We consider the sequence $\{y_n\}$ defined by : $$
y_n=\frac {1}{n}\sum_{k=1}^{n}{x_k}
$$ Show that : $\{y_n\}$ is  relatively weakly compact. An idea please","['banach-spaces', 'general-topology', 'functional-analysis', 'weak-topology']"
3545340,A Vandermonde Identity for Stirling Numbers?,"I'm facing the problem of trying to express a quantity in the simplest possible way (it is, using the least possible number of sum symbols). $$ \sum_{j=0}^{n} \sum_{\ell=0}^m \frac{1}{j!}\binom{b+j}{j} {j+1 \brack {\ell+1}} {b+2 \brack {m-\ell+1}}$$ Of course, this can be easily written as a convolution between two polynomials (which happen to be more or less simple). I'm pretty sure that approach will not work (at most, one can write the above expression as ""the coefficient of $x^m$ in this product [...]"", but that is not useful to my purpose). However, if one explores this sum a little bit, it pretty soon come up the fact that it could be truly useful to, for example, be able to compute this: $$\sum_{\ell=0}^m {j+1\brack{\ell+1}}{b+2 \brack {m-\ell+1}}$$ (which resembles a lot Vandermonde's Identity, but with Stirling numbers instead of binomial coefficients). I looked up on a couple of books (Concrete Mathematics of Graham-Knuth-Patashnik, and others), and I couldn't find any references pointing to such an identity. Does anybody know something like that? (Perhaps involving other weird numbers as Eulerian or double Eulerian or that kind of stuff?) Nevertheless, any kind of help simplifying the first double sum would be really appreciated.","['summation', 'binomial-coefficients', 'combinatorics', 'stirling-numbers']"
3545349,limit trough path,"I am just trying to find a formal proof for this statement: Let $f:\mathbb R^n \backslash \ \{0\} \rightarrow \Bbb R$ be a function, and assume that there exits $L\in \Bbb R$ that for every path $$\gamma: (a,b)\rightarrow \Bbb R^n\backslash\{0\} $$ that satisfies $$\lim_{t\rightarrow b}\gamma(t) = 0 $$ the limit $$\lim_{t\rightarrow b} f(\gamma(t)) = L $$ show that : $\lim_{x\rightarrow 0} f(x) =L$","['multivariable-calculus', 'limits', 'calculus']"
3545367,Infinity norm of matrix defined using integral of Lagrange polynomials.,"Fix $n \geq 2$ an integer and let $x_{k+1} = k/n , k = 0, 1,\cdots, n-1.$ Let now $L_k$ be the Lagrange polynomial given by $$L_k(x) = \prod_{j =1, j \neq k}^{n} \frac{x-x_j}{x_k - x_j}.$$ Define the matrix $A = [a_{i,j}]$ whose entries are given by $$a_{i,j} := \int_{0}^{x_i}L_j(x)\, dx.$$ I'm interested in finding the infinity norm of $A$ $$\|A\|_\infty := \max_{ 1\leq i\leq n} \sum_{j=1}^n |a_{i,j}|.$$ My attempt: It seems (I used Mathematica for $n = 2, ..., 8$ ) that the maximum is reached in the last line of the matrix where we have $a_{n,j} \geq 0.$ Thus,  we have $$\|A\|_\infty = \sum_{j=1}^n |a_{n,j}| = \sum_{j=1}^n\int_{x_1}^{x_n}L_j(x).$$ It's clear that $\sum_{j=1}^nL_j(x) =1.$ Then, we have $$\|A\|_\infty = n-1.$$ Thank you for any hint.","['matrices', 'linear-algebra', 'polynomials']"
3545402,Can we say that $x=0$ is a minimum of the function $f(x)=|x|$?,"The derivative of a function $f(x)$ is said to exist at $x=x_0$ if both the left-derivative $Lf^\prime(x)$ and the right-derivative $Rf^\prime(x)$ exists and they are equal i.e. $$\lim\limits_{h\to 0^-}\frac{f(x_0+h)-f(x_0)}{h}=\lim\limits_{h\to 0^+}\frac{f(x_0+h)-f(x_0)}{h}.$$ However, if either of them is nonexistant or not equal to each other, the function does not have a unique derivative at $x=x_0$ . The derivative is said to not exists at $x=x_0$ . For example, $f(x)=|x|$ does not have a unique derivative at $x=0$ because the left-derivative and right-derivative are unequal. In this case, can we say that $x=0$ is a minimum of the function $f(x)=|x|$ ? Looks like the criterion of determining maximum or minimum is in trouble. Sorry for my sloppy mathematics language.","['continuity', 'calculus', 'functions', 'maxima-minima', 'derivatives']"
3545414,Tail bound on difference of shifted binomials,"I would like to derive an upper bound on $\mathsf{Pr}\{(\frac{X}{n}-\frac{1}{2})^2 \leq (\frac{Y}{n}-\frac{1}{2})^2\}$ where $X,Y$ are independent and $X\sim$ Bin( $n,p$ ) where $p\neq \frac{1}{2}$ , and $Y\sim$ Bin( $n,\frac{1}{2}$ ). Intuitively, when $n$ is large, the left hand side is non-negative and the right hand side nears zero, so the probability should be vanishing with $n$ . And here is my attempt: For simplicity we denote $X'=(\frac{X}{n}-\frac{1}{2})^2,Y'=(\frac{Y}{n}-\frac{1}{2})^2$ , using Chernoff Bound we have $\mathsf{Pr}\{X' \leq Y'\}\leq \inf_{t>0}\mathsf{E}[e^{t(Y'-\mathsf{E}[Y'])}]\mathsf{E}[e^{t(-X'+\mathsf{E}[X'])}]/e^{t\alpha}$ where $\alpha = \mathsf{E}[X']-\mathsf{E}[Y']$ . By some calculation we can conclude $\alpha = \frac{n-1}{n}(p^2+\frac{1}{4})$ , and by Hoeffding's Lemma, since $X',Y'\in [0,\frac{1}{4}]$ we have $\mathsf{E}[e^{t(Y'-\mathsf{E}[Y'])}]\mathsf{E}[e^{t(-X'+\mathsf{E}[X'])}] \leq e^{\frac{t^2}{64}}$ , so $\mathsf{Pr}\{X' \leq Y'\}\leq \inf_{t>0}\{e^{\frac{t^2}{64}-t(\frac{n-1}{n})(p^2+\frac{1}{4})}\}=\exp\{-16(\frac{n-1}{n})^2(p^2+\frac{1}{4})^2\}$ which is not vanishing with $n$ . I guess the step using Hoeffding's Lemma makes the bound too loose, but I'm not sure how to improve it.","['distribution-tails', 'concentration-of-measure', 'probability-theory']"
3545422,"If $2\cos A=x+\frac1{x}$ and $2\cos B=y+\frac1{y}$, prove $2\cos(A-B)=\frac{x}{y}+\frac{y}{x}$","I have $$2\cos A = x+ \frac{1}{x} \qquad\qquad2\cos B = y+ \frac{1}{y}$$ I have to prove that $$2\cos(A-B)=\frac{x}{y} + \frac{y}{x}$$ I have solved it by finding $x = e^{iA}$ or $ x = e^{-iA}$ and $y = e^{iB}$ or $e^{-iB}$ from given two conditions. Then I evaluated $\frac{y}{x}+ \frac{x}{y}$ by taking first $x = e^{iA}$ and $y = e^{iB}$ , and it is also satisfied when $x= e^{-iA}$ and $y = e^{-iB}$ But when I take $x = e^{iA}$ and $ y = e^{-iB}$ , this seems to not satisfy the required proof. Why would this be the case?",['trigonometry']
3545453,Does a function being uniformly expanding imply it is exact?,"I will begin this question by mentioning what the terms mean in the statement. I have a probability space $(X,\mathcal{B},\mu)$ and a measurable map $T:X\to X$ . I assume that $T$ is bijective, and $T^{-1}$ is also measurable. Suppose in addition that $X$ is a bounded metric space with metric $d$ . By uniformly expanding, I mean there is a constant $C>1$ such that $d(Tx,Ty)\ge C d(x,y)$ for all $x,y\in X$ . Exactness has two formulations in ergodic theory; I will state them both. Firstly, we say that a nonsingular map $T$ is exact if $A\in \bigcap_{n\ge 0}T^{-n}(\mathcal{B})$ implies $\mu(A)\in \{0,1\}$ . If $T$ is measure-preserving, then $T$ is exact if one has $T(A)\in \mathcal{B}$ for all $A\in \mathcal{B}$ , and if $\mu(A)>0$ , then $\lim_{n\to \infty}\mu(T^nA)=1$ . Now that the definitions have been stated, is it true that if $T:X\to X$ is as described in the first paragraph, then it is exact? To me, it seems as if it should be true intuitively if considering $X$ to be some interval and $\mu$ to be the Lebesgue measure (from the second definition of exactness), however I am unsure if it is true in the generality above ( $X$ bounded metric space, $\mu$ a probability measure). Thanks in advance!","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
3545501,Solve the equation $\sqrt[3]{15-x^3+3x^2-3x}=2\sqrt{x^2-4x+2}+3-x$.,Solve the equation $\sqrt[3]{15-x^3+3x^2-3x}=2\sqrt{x^2-4x+2}+3-x$ . I have tried to solve for x by Casio and try to make the equation to $u.v=0$ but the solution is not in $\mathbb{Q}$ . Any help is appreciated. Thanks,"['radical-equations', 'algebra-precalculus', 'roots', 'radicals']"
3545520,What kind of surface is this? Is there a way to plot this?,"I am given the surface: $$S=\{ \vec{x} \in \mathbb R^3: {\|\vec{x} \|}_2^2=4, x^2+y^2 \le 1, z >0 \}$$ and I want to calculate the Mass of $S$ given a density $\rho$ . It sort of looks like the upper half of a sphere. The problem I have is that the first equation ${\| \vec{x} \|}_2^2=4$ means that the radius of this sphere is $R=2$ . However, the condition $x^2+y^2 \le1$ would mean that it is some kind of half sphere with a smaller ""base"". I tried to plot this in Wolfram Alpha but I couldn't get it to work. Is there any way I can parameterize/transform this surface in spherical coordinates?",['multivariable-calculus']
3545539,Show that the cylindrical $\sigma$-algebra can be written into a countable union of some $\sigma$-algebra,"Denote $\mathbb{R}^{\mathbb{T}}$ to be the set of functions $x:\mathbb{T}\longrightarrow\mathbb{R}$ , where $\mathbb{T}$ is an indexing set (the ""time"" in the stochastic process). Now, define the cylinder sets as $$C(t_{1},\cdots, t_{n}, B):=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\}\ \text{for some}\ B\in\mathcal{B}(\mathbb{R}^{n}).$$ Then the cylindrical $\sigma$ -algebra is defined as $$\mathcal{B}(\mathbb{R}^{\mathbb{T}})=\sigma(\text{cylinders}).$$ Now, I want to show that: Define $\mathcal{F}_{T}:=\sigma\Big(\{C(t_{1},\cdots, t_{n}, B):t_{1}\cdots, t_{n}\in T\}\Big)$ for $T\subset\mathbb{T}$ . Prove that $$\mathcal{B}(\mathbb{R}^{\mathbb{T}})=\bigcup_{\text{countable}\ T\subset\mathbb{T}}\mathcal{F}_{T},$$ where the union is taking over all countable subset $T$ of $\mathbb{T}$ . I had some attempt as follows: Denote $\mathcal{C}$ to be the collection of all cylinder sets. Let $A$ be a cylinder set, then it can be written as $A=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\}$ for some $B\in\mathcal{B}(\mathbb{R}^{n})$ . Then $t_{1},\cdots, t_{n}$ must belong to some index subset $T_{1}$ of $\mathbb{T}$ , and thus $$A\subset \{C(t_{1},\cdots, t_{n}, B):t_{1},\cdots, t_{n}\in T_{1}\}.$$ Thus, if we define $\mathcal{C}_{T_{1}}$ to be the collection of all the set of the form as the RHS of the above inclusion, we then have $$\mathcal{C}\subset\mathcal{C}_{T_{1}}\subset\mathcal{F}_{T_{1}}.$$ But $\mathcal{B}(\mathbb{R}^{n}):=\sigma(\mathcal{C})$ is the smallest $\sigma-$ algebra containing $\mathcal{C}$ , and thus $$\mathcal{B}(\mathbb{R}^{n})\subset\mathcal{F}_{T_{1}}\subset\bigcup_{\text{countable T}\subset\mathbb{T}}\mathcal{F}_{T}.$$ However, I have no idea about how to show the inverse inclusion. Also, is my proof for $\subset$ correct? I am really bad at measure theory, so I don't really have any confidence at all... This question is related to: What is the sigma algebra of cylindrical sets? Cylindrical sigma algebra answers countable questions only. , but there were not any complete proof there, and the notions of cylindrical $\sigma-$ algebra were not really the same. Thank you so much! Edit 1: As comments pointed out, I should prove $\bigcup_{T}\mathcal{F}_{T}$ is a $\sigma-$ algebra. Inspired by saz , I generated a proof about $\sigma-$ algebra, and seems proved $\subset$ . However, I still don't know how to show $\supset$ . With a little bit notation abuse, denote the RHS of the desired equality to be $\mathcal{F}$ . Lemma: $\mathcal{F}$ is a $\sigma-$ algebra. Proof of lemma: Indeed, since $\mathcal{F}_{T}$ is defined to be a $\sigma-$ algebra for any countable subset $T\subset\mathbb{T}$ , it must contain $\varnothing$ , and thus $\varnothing\in\mathcal{F}$ . Secondly, if $E\in \mathcal{F}$ , then $E\in\mathcal{F}_{T}$ for some $\mathcal{F}_{T}$ , but it is a $\sigma-$ algebra, so $E^{c}$ is in that set and thus $E^{c}\in\mathcal{F}$ . Finally, let $\{E_{j}\}_{j=1}^{\infty}$ be a countable collection of sets that are in $\mathcal{F}$ , then $E_{j}\in\mathcal{F}_{T_{j}}$ for some countable $T_{j}\subset\mathbb{T}$ . Consider the set defined by $T^{*}:=\bigcup_{j=1}^{\infty}T_{j},$ it is again a countable subset of $\mathbb{T}$ because it is a countable union of countable sets. Also, by construction, we must have for each $j$ , $$\{C(t_{1},\cdots, t_{n}, B):t_{1}\cdots, t_{n}\in T_{j}\}\subset\{C(t_{1},\cdots, t_{n}, B): t_{1},\cdots, t_{n}\in T^{*}\}\subset\mathcal{F}_{T^{*}},$$ but $\mathcal{F}_{T_{j}}$ is the smallest $\sigma-$ algebra containing the LHS, and thus $\mathcal{F}_{T_{j}}\subset\mathcal{F}_{T^{*}}$ for each $j$ . Therefore, $E_{j}\in\mathcal{F}_{T*}$ for each $j$ . Hence, $\bigcup_{j=1}^{\infty}E_{j}\subset\mathcal{F}_{T^{*}}\subset\mathcal{F}.$ Proof of $\subset$ : Now, denote $\mathcal{C}$ to be the collection of all cylinder sets and let $A\in\mathcal{C}$ . Then A can be written as $$A=\{x\in\mathbb{R}^{\mathbb{T}}:(x_{t_{1}},\cdots, x_{t_{n}})\in B\}\ \text{for some}\ B\in\mathcal{B}(\mathbb{R}^{n}).$$ But $t_{1},\cdots, t_{n}$ must belong to some countable index subset $T$ of $\mathbb{T}$ (the most convenient way is to define $T:=\{t_{1},\cdots, t_{n}\}$ ).  Therefore, $A\in\mathcal{F}$ , so $\mathcal{C}\subset\mathcal{F}$ . However, we have showed that $\mathcal{F}$ is a $\sigma-$ algebra, and we know that $\mathcal{B}(\mathbb{R}^{\mathbb{T}})=\sigma(\mathcal{C})$ is the smallest $\sigma-$ algebra containing $\mathcal{C}$ and thus $\mathcal{B}(\mathbb{R}^{\mathbb{T}})\subset\mathcal{F}$ . I'd like to express my great appreciate to saz who really spent lots of time on my dumb questions. Thank you so much saz :)! Edit 2: Proof of saz's remark: As I am really bad at measure theory, I am going to prove saz's remark as an exercise: I claim that $\mathcal{F}_{S}\subset\mathcal{F}_{T}$ for any two (not necessarily countable) $S,T\subset\mathbb{T}$ with $S\subset T$ . Indeed, every set in the collection $\mathcal{A}_{1}:=\{C(t_{1},\cdots, t_{n}, B), t_{1}\cdots, t_{n}\in S, B\in\mathcal{B}(\mathbb{R}^{n})\}$ must belong the collection $\mathcal{A}_{2}:=\{C(t_{1},\cdots, t_{n}, B), t_{1}\cdots, t_{n}\in T, B\in\mathcal{B}(\mathbb{R}^{n})\}$ , since $S\subset T$ . This implies that $$\mathcal{A}_{1}\subset\mathcal{A}_{2}\subset\sigma(\mathcal{A}_{2})=\mathcal{F}_{T},$$ but $\mathcal{F}_{S}=\sigma(\mathcal{A}_{1})$ is the smallest the $\sigma-$ algebra containing $\mathcal{A}_{1}$ , and thus $\mathcal{F}_{S}\subset\mathcal{F}_{T}$ . It definitely follows immediately from saz's comments about the comparison of generating set. I am really grateful of the help from saz, thank you so so so much!","['measure-theory', 'stochastic-processes', 'solution-verification', 'probability-theory', 'stochastic-calculus']"
3545557,Is there any structure in general on the $p$th cohomotopy set?,"The $p$ th cohomotopy set of a pointed space $X$ , $\pi^p(X)$ , is the set of maps from $X$ to $S^p$ mod pointed homotopy. Unlike homotopy, there need be no natural group structure on $\pi^p(X)$ unless ( $X$ is particularly nice or) $p\in\{0,1,3,7\}$ : if $p$ is one of these values then we can use the group structure on $S^p$ to ""add maps pointwise"" (EDIT: it's not a group at $p=7$ , since octonion multiplication isn't associative, but it's still algebraically nontrivial) but otherwise no such group structure exists . At this point it's natural to ask whether we can still find some algebraic structure on $\pi^p(X)$ if $p\not\in\{0,1,3,7\}$ (and $X$ is not particularly nice). Unfortunately, Adams' theorem can be strengthened: Walter Taylor showed that in a precise sense there is no nontrivial (""demanding"") algebraic structure on $S^p$ at all for $p\not\in\{0,1,3,7\}$ . My question is whether this is lethal: Is there some $p\not\in \{0,1,3,7\}$ such that there exists some natural algebraic structure on $\pi^p(X)$ for arbitrary $X$ (or at least for $X$ substantially more general than suspensions) ? By ""algebraic structure"" I really intend to cast a wide net, hence the universal algebra tag. One possible precisiation of the question would be the following: letting $\tau$ be the ""obvious"" topology on $\pi^p(X)$ , is there in the sense of Taylor's paper a nontrivial algebraic structure on $\pi^p(X)$ compatible with $\tau$ ? However, I'm also open to other reasonable interpretations.","['universal-algebra', 'general-topology', 'abstract-algebra', 'algebraic-topology']"
3545603,Evaluate $\lim\limits_{n\rightarrow\infty} \mathrm{srt}_n\left({^{n+1}}2\right)$,"Notation: ${^n}x = x^{x^{\cdots^x}}$ is tetration , i.e. $x$ to the power of itself $n$ times. $\mathrm{srt}_n(x)$ is the super $n$ -th root, or the inverse function of ${^n}x$ , which is well defined for $x\ge 1$ . I can prove that $$
\lim\limits_{n\rightarrow\infty} \mathrm{srt}_n\left({^{n+1}}2\right)
$$ converges to some value between about $\mathrm{srt}_3(256)\approx 2.2915$ and about $2.6$ , but it is computationally intractable even for relatively small $n$ . For example $^5 2\approx 2\times 10^{19728}$ . For ease of notation, we let $s_n = \mathrm{srt}_n(^{n+1}2)$ . I would be very surprised if there's a nice closed form of $\lim\limits_{n\rightarrow\infty} s_n$ , so I'm mostly interested here in how to approximate it other than a direct computation of the definition, which really isn't all that viable. As noted above, even computing $s_4$ is tough to do from the formula, though taking some logarithms can get you a bit further, it doesn't help much since tetration is much faster than exponentiation. Is there some trick that convert the formula to $s_n$ into something more tractable? I can see how you could use the same approach as the one I used (see below) to get better lower bounds than $2.29$ , but I suspect that would also become extremely difficult to use if you wanted any sort of precision (even one decimal place might be hard). Proof of convergence: Clearly $s_n > 2$ for all $n$ , so it suffices to show $s_n$ is decreasing. Observe: \begin{eqnarray}
^n s_n &=& 2^{\left(^n2\right)} = 2^{\left(^{n-1}s_{n-1}\right)}<(s_{n-1})^{\left(^{n-1}s_{n-1}\right)} = {^n}(s_{n-1})
\end{eqnarray} since $x\to {^n}x$ is increasing, this implies $s_n$ decreases. Proof of lower bound: We prove that $s_n > c = \mathrm{srt}_3(256)$ for all $n$ by proving inductively ${^n} c \le\frac{\ln 2}{2\ln(c)} \left({^{n+1}}2\right)$ . Since $\frac{\ln 2}{2\ln(c)} <1$ , this means ${^n}c<{^{n+1}}2$ . Taking $\mathrm{srt}_n$ of both sides shows $s_n>c$ . For the base case, we take $n=2$ : \begin{eqnarray}
c^{c^c} &=& 256 = 2^8\\
c^c \ln c &=& 8(\ln 2)\\
c^c \ln c &=& \frac12 (\ln 2) 16\\
c^c&=&\frac{\ln 2}{2\ln(c)} \left({^{3}}2\right)
\end{eqnarray} Now, for the inductive step. Suppose ${^n} c \le\frac{\ln 2}{2\ln(c)} \left({^{n+1}}2\right)$ for some $n\ge 2$ . Observe that for $x > 4$ (this is not a tight bound): $$
\frac{\ln 2}{2\ln(c)} x < \frac1{\ln c}\ln\left(\frac{\ln 2}{2\ln(c)}\right) + \frac{\ln 2}{\ln c}x
$$ Since ${^{n+1}2} > 4$ , we therefore have \begin{eqnarray}
{^n} c &\le&\frac{\ln 2}{2\ln(c)} \left({^{n+1}}2\right)\\
&<&\frac1{\ln c}\ln\left(\frac{\ln 2}{2\ln(c)}\right) + \frac{\ln 2}{\ln c}\left({^{n+1}2}\right)
\end{eqnarray} Taking the $c$ th power of both sides yields $$
{^{n+1}} c < \frac{\ln 2}{2\ln(c)} \left(^{n+2}2\right)
$$ as desired. Hence we have inductively $$
{^{n}} c < \frac{\ln 2}{2\ln(c)} \left(^{n+1}2\right)
$$ for all $n\ge 2$ . Therefore $s_n > c$ for all $n$ . Computed with WolframAlpha, the first three terms of $s$ are \begin{eqnarray}
s_1 &=& 4\\
s_2 &\approx& 2.74537...\\
s_3 &\approx& 2.58611...\\
s_4 &\approx& 2.57406...
\end{eqnarray} Search query used for $s_2$ , $s_3$ , and $s_4$ .","['limits', 'tetration', 'approximation', 'sequences-and-series']"
3545643,Is the SEIS Model more accurate and/or realistic than the SEIRS Model in modelling the 2019-nCov?,"I am modelling the CoronaVirus for my IB maths IA (project). I have explained the principles and assumptions and derived the system of differential equations of the SEIRS Model. However, the R(recovered) compartment does not make much sense for me... and its presence adds difficulty in the calculation. I am considering the SEIS model which is pretty much the same as the SEIRS model apart from having a separate compartment. The current problem is that it seems to be worthless calculating the R compartment as it is added back to the S(susceptible) compartment anyways, assuming that recovered individuals do not develop immunity and can be infected again immediately. Also, the system of ordinary differential equations that I got without vital dynamics and in absence of vaccination and isolation is Can someone confirm this? I couldn't find one online. Many thanks!!","['biology', 'mathematical-modeling', 'ordinary-differential-equations']"
3545644,Why is the following topology of probability measures Hausdorff?,"Let $X$ be a Hausdorff topological space. Let $PX$ be the set of all Borel probability measures on $X$ . Bogachev's Measure Theory (vol. II, section 8.10.iv) defines the $A$ -topology on $PX$ to be the one generated by the following opens, $$
U(\mu, G, \varepsilon) := \{\nu \in PX : \mu(G) < \nu(G) + \varepsilon \},
$$ for all $\mu\in PX$ , $G\subseteq X$ open, and $\varepsilon > 0$ . It then says that since two Borel measures are equal if and only if they coincide on all open sets (Lemma 7.1.2 in there), the $A$ -topology is Hausdorff. This is not clear to me: how does the proof proceed, in more detail? Here is what I figured out so far. Suppose that $\mu,\nu\in PX$ are not equal. In other words, there exists an open set $G\subseteq X$ such that $\mu(G)\ne \nu(G)$ . Suppose, for example, that $\mu(G) > \nu(G)$ . Then there exists $\varepsilon>0$ such that $\mu(G) \ge \nu(G) + \varepsilon$ . Therefore $\nu\notin U(\mu, G, \varepsilon)$ , while clearly $\mu\in U(\mu, G, \varepsilon)$ . We have found an open neighborhood of $\mu$ that does not contain $\nu$ . But how can we find an open neighborhood of $\nu$ that does not contain $\mu$ ?","['general-topology', 'outer-measure', 'measure-theory']"
3545702,The reciprocal of many prime numbers p in base 10 have a set of repeating digits p-1. Why?,"I've noticed that the reciprocal of many prime numbers have a curious number of digits. For example. 1/7 has 6 repeating digits, 1/17 has 16 repeating digits, and 1/47 has 46. There's a rule here that makes this pattern, and I haven't quite figured it out. Does anyone have some insight on this? There are a few other numbers that follow the pattern subversively. For example, 1/3 has two repeating digits: 33. 1/11 has ten repeating digits: 0909090909. The number 1/13 has a set of six that repeats twice, which makes a set that's 12 digits long. For reference, here is a website that lists reciprocals of numbers 2 through 70, including non-primes. https://thestarman.pcministry.com/math/rec/RepeatDec.htm The ones that break the rule are 1/2 and 1/5, but they seem to be a matched pair, where the 2 and 5 switch places in the equation 1/X = Y/10 What is the rule that makes this pattern? I'm very curious.","['inverse', 'prime-numbers', 'sequences-and-series']"
3545711,Invertibility of combinatorial matrix,"Fix integers $n$ and $k$ with $0 \leq k < \dfrac{n}{2}$ . Let $I$ be the set of all $k$ -element subsets of $\{1,\ldots,n\}$ . Consider the matrix $A=(a_{ij})_{i,j\in I}$ where $a_{ij}=1$ if $i\cap j=\emptyset$ and $a_{ij}=0$ otherwise. What is the easiest way to see that $\det(A)\neq 0$ ? For example, for $n = 3$ and $k = 1$ , we have $\det(A) = 2$ . More generally, for $k = 1$ , we have $\det(A) = \pm (n-1)$ (since $A$ is the all-ones matrix minus the identity matrix).","['matrices', 'linear-algebra', 'combinatorics']"
3545714,"Question about Lagrange multipliers, optimization problems and KKT-points.","I am having some difficulties with optimization problems with inequality constraints. In general the problems I am given will look something like this: $$\min f(x,y,z) \\ \text{s.t.} \space \space \space g(x,y,z) \le0 \\ \space 
 \space \space \space h(x,y,z)=0$$ I would usually start by writing down the lagrange function $\mathcal L$ , where $\lambda$ , $\mu$ are the corresponding Lagrange multipliers. $$\mathcal L(x,y,z,\lambda,\mu)=f(x,y,z)+\lambda (h(x,y,z))+\mu(g(x,y,z))$$ In order to find the KKT-points I would solve: $$\nabla \mathcal L=0$$ which will give me one or many KKT points. Here is the problem I am having: This method seems like I am always treating the inequality as an equality. Do I ever treat the ""strictly less"" case? Sometimes I see people (lectures or youtube videos I have watched) ""ignore"" the inequality constraint, solve the problem and then checking if the inequality is satisfied at the resulting points. Why does one do that? If I am getting a negative lagrange multiplier for the inequality constraint, how do I proceed? Does it just mean there are no points that satisfy that constraint? Is there some sort of ""road map"" or strategy when dealing with problems like this (optimization with one inequality and eqality constraint)?","['optimization', 'multivariable-calculus', 'lagrange-multiplier', 'karush-kuhn-tucker']"
3545749,Expected tetrahedron volume from normal distribution,"Two equivalent formulas for the volume of a random tetrahedron are given. Further on you can find an interesting conjecture for the expected volume that shall be proved. Tetrahedron volume Given are 12 independent standard normal distributed variables $$x_i=\mathcal{N}(0,1)_{i=1,...,12}$$ that define the 4 coordinates $$\vec{a}=(x_1,x_2,x_3),\;\; \vec{b}=(x_4,x_5,x_6),\;\; \vec{c}=(x_7,x_8,x_9),\;\; \vec{d}=(x_{10},x_{11},x_{12})$$ of a 3-simplex in $\mathbb{R}^3$ . The first formula for the non-oriented simplex volume is $$V=\frac{1}{6}\left| (\vec{a}-\vec{d})\cdot \left((\vec{b}-\vec{d}) \times (\vec{c}-\vec{d})\right) \right|\tag{1}$$ $$=\frac{1}{6}\left| x_2 x_6 x_7 + x_3 x_4 x_8+ x_1 x_5 x_9+ x_3 x_5 x_{10} + x_6 x_8 x_{10} + x_2 x_9 x_{10}+ x_1 x_6 x_{11}+  x_3 x_7 x_{11}+  x_4 x_9 x_{11}+ x_2 x_4 x_{12}+ x_5 x_7 x_{12}+ x_1 x_8 x_{12}-x_3 x_5 x_7- x_2 x_6 x_{10}-  x_3 x_8 x_{10} - x_1 x_6 x_8 - x_2 x_4 x_{9}- x_5 x_9 x_{10}- x_3 x_4 x_{11}- x_6 x_7 x_{11}- x_1 x_9 x_{11}- x_1 x_5 x_{12}- x_2 x_7 x_{12}- x_4 x_8 x_{12}\right|.$$ If the coordinate system is shifted $$\vec{p}=\vec{a}-\vec{d},\;\;\vec{q}=\vec{b}-\vec{d},\;\;\vec{r}=\vec{c}-\vec{d}$$ the new coordinates are $$\vec{p}=(y_1,y_2,y_3),\;\; \vec{q}=(y_4,y_5,y_6),\;\;\vec{r}=(y_7,y_8,y_9)$$ with new random variables $$y_i=\mathcal{N}(0,\sqrt{2})_{i=1,...,9}.$$ The shift reduces the number of random variables from 12 to 9 and increases the standard deviation from $1$ to $\sqrt{2}$ (this corresponds to a double variance $=\sqrt{2}^2)$ . However the variables are not independent anymore.
Their correlation $\rho=0.5$ is given by their covariance normalized by the standard deviation $$\rho=\frac{\mathbb{Cov}[y_i,y_j]}{\sqrt{\mathbb{Var}[y_i]}\sqrt{\mathbb{Var}[y_j]}}=
\frac{\mathbb{Cov}[x_m-x_k,x_n-x_k]}{\sqrt{\mathbb{Var}[x_m-x_k]}\sqrt{\mathbb{Var}[x_n-x_k]}}
=\frac{\mathbb{E}[x_k^2]}{\sqrt{\mathbb{Var}[x_m-x_k]}\sqrt{\mathbb{Var}[x_n-x_k]}}=\frac{\mathbb{E}[x_k]^2+\mathbb{Var}[x_k]}{\sqrt{\mathbb{Var}[x_m-x_k]}\sqrt{\mathbb{Var}[x_n-x_k]}}
=\frac{1}{\sqrt{2}\sqrt{2}}=\frac{1}{2}\;\;\;\text{for}\;i\ne j \land n\ne m \ne k.$$ The second formula for the non-oriented volume as function of the dependent variables is $$V=\frac{1}{6}\left|\vec{p}\cdot (\vec{q} \times \vec{r}\right)|\tag{2}$$ $$=\frac{1}{6}\left| y_2y_6y_7+y_3y_4y_8+y_1y_5y_9-y_1y_6y_8-y_2y_4y_9-y_3y_5y_7\right|.$$ Equation (2) has only a quarter of summands of eq.(1) however the variables correlate with $\rho=0.5$ . Question What is the analytical expression for the expected volume $\mathbb{E}[V]$ ? What is known? Conjecture It is conjectured that $\mathbb{E}[V]=\frac{2}{3}\sqrt{\frac{2}{\pi}}$ or $\mathbb{E}[V]=\frac{21}{4\pi^2}$ . Assuming  the first conjecture is true please note the relation to a standard half-normal distribution in $\mathbb{R^1}$ that has expectation $\sqrt{\frac{2}{\pi}}$ . Moments All even moments are precisely known and the odd moments are approximately known. The first moments are \begin{array}{|l|l|}\hline
\text{odd moments} & \text{even moments} \\
\text{(simulation)} & \text{(analytic)} \\ \hline
m_1\approx 0.532 & m_2=\frac{2}{3}\\ \hline
m_3\approx\sqrt{2} &m_4=\frac{40}{9} \\ \hline
m_5\approx18.9 &m_6=\frac{2800}{27} \\ \hline
\end{array} (more moments on demand). Solution strategies One could try to integrate over a subvolume where the sign of the volume is constant. Due to symmetry every subvolume should have equal size. The challenge is therefore to find the right suitable integration borders. A related question about the expected area of a triangle with standard normal distributed coordinates in $\mathbb{R}^3$ was proven to be $\sqrt{3}$ . If these methods would be applied to the tetrahedron case then according to the answerer ""ultimately it comes down to the product of independent chi-distributed variables and a variable for the spherical angle they determine: finding the expectation of the latter is the crux of the question."" Other equations for the volume There are other methods to calculate the volume however they include at least 1 square root, an unwanted property for such problems. Expected oriented volume The expression for the volume is a sum of triple products of random variables. As the expectations of the independent $x_i$ in eq.(1) are $\mathbb{E}[x_i]=0$ it holds $$\mathbb{E}[x_i x_j x_k\pm x_l x_m x_n]=0\cdot 0 \cdot 0\pm 0\cdot 0 \cdot 0=0\;\;\;\text{for}\; 1\le i,j,k,l,m,n \le 12$$ The expected oriented volume is therefore $0$ .","['geometric-probability', 'probability-distributions', 'normal-distribution', 'simplex', 'probability']"
3545778,Sum of indicators and application of Jensen's inequality,"So I have stumbled upon this problem. 
Let $X_1, \dots, X_n \sim N(\mu, \sigma^2)$ be iid. Define: $$S  = \frac{1}{n}\sum_{i=1}^n I[X_i > a]$$ $$T = I[\frac{1}{n}\sum_{i=1}^n X_i > a]$$ $a > 0$ . Using Jensen's Inequality prove: $$E(S) > E(T)$$ Now I only manage to prove it by solving the expected values without Jensen's Inequality. Where I get: $$E(S) = 1 - \Phi\left(\frac{a-\mu}{\sigma}\right)$$ And $$E(T) = 1 - \Phi\left(\frac{a-\mu}{\sigma}\sqrt n\right)$$ Which proves the inequality. Where $\Phi$ is the standard normal cdf.  However this is just by using $E(f(X)) = \int_{-\infty}^{\infty} f(x)p(x) dx$ . $p(x)$ is the pdf of $X$ . I struggle seeing why one can apply Jensen on $I(X > a)$ as it is non-convex. Edit: After some thinking I do not belive this is possible, but feel free to prove me wrong.","['statistical-inference', 'statistics', 'jensen-inequality', 'probability']"
3545807,"For every twice differentiable function $f : \bf R \rightarrow [–2, 2]$ with $(f(0))^2 + (f'(0))^2 = 85$, which of the following statements are TRUE?","For every twice differentiable function $f : \mathbf R \rightarrow [–2, 2]$ with $(f(0))^2 + (f'(0))^2 = 85$ , which of the following statement(s) is (are) TRUE ? (A) There exist $r, s\in \bf R$ , where $r < s$ , such that $f$ is one-one on the open interval $(r, s)$ (B) There exists $x_0 \in (–4, 0)$ such that $|f'(x_0)| < 1$ (C) $\lim\limits_{x\to\infty}f(x) = 1$ (D) There exists $\alpha\in(–4, 4)$ such that $f(\alpha) + f''(\alpha) = 0$ and $f '(\alpha)\ne0$ My attempt is as follows:- $A)$ If function is continuous, then definitely in some part of the interval it will be increasing or decreasing hence will be one-one on the open interval $(r,s)$ $f(x)$ cannot be a constant function as in that case $f'(x)=0$ which means $f(0)=\sqrt{85}$ , but it doesn't belong to co-domain of $f$ B) By mean value theorem we can say that there exists some $c\in(-4,0)$ for which $4f'(c)=f(0)-f(-4)$ Suppose if $f(0)=2$ and $f(-4)=-2$ , then we can only say that there exists $c$ such that $f'(c)=1$ . It will not be necessary that there exists a $c$ for which $|f'(c)|<1$ C) $\lim\limits_{x\to\infty}f(x) = 1$ , this is not at all necessary. D) If function is not constant in $(-4,4)$ , then we can safely say that at some point $\alpha$ , $f(\alpha)\ne0$ But how to know $f(\alpha)+f''(\alpha)=0$ Actual answer is $A,B,D$ but I am able to ascertain only option $(A)$ . Please help me in this. Question source JEE Advanced 2018.","['calculus', 'solution-verification', 'derivatives', 'monotone-functions']"
3545808,Doing a standard integral with complex numbers instead of using a trigonometric substitution,"I was looking at some integrals to do with trigonometric substitutions and I stumbled across this one $$\int\frac{1}{\sqrt{x^2-1}}dx$$ I know you can do it with a regular trigonometric substitution or just use a hyperbolic substitution but I was wondering if you can do it the following way. $$
\int \frac{1}{\sqrt{x^2-1}} dx
 = \int \frac{\cos \theta}{\sqrt{-\cos^2\theta}}d\theta
 = \int \frac{1}{i}d\theta
 = \frac{1}{i}\arcsin x,
$$ where the $x=\sin \theta$ substitution was used. Could anybody please explain to me why I don't get the same result as one would get if a hyperbolic or other trigonometric substitution was used? Thanks in advance.","['integration', 'complex-analysis', 'calculus', 'indefinite-integrals', 'complex-numbers']"
3545811,"$Tx=\sum_{k=0}^{\infty}\lambda_k(x,e_k)e_k$ bounded and compact iff $\lambda_k\to 0$","Let $H$ be a real Hilbert space and $e:\mathbb{N}\to H$ an orthonormal system. Let $\lambda\in l^{\infty}(\mathbb{R})$ be a bounded sequence and define $T:H \to H$ by $$Tx=\sum_{k=0}^{\infty}\lambda_k(x,e_k)e_k.$$ Show that a) T is bounded with $||T||=||\lambda||_{l^{\infty}(\mathbb{R})}$ and b) $T$ is compact iff $\lambda_k\to0$ as $k\to 0$ . I think I first need to show the operator is well-defined, i.e that the sum converges. I therefore look at the partial sums:  | $\sum_{k=0}^{N}\lambda_k(x,e_k)e_k|\leq ||\lambda||_{l^{\infty}(\mathbb{R})} \sum_{k=0}^{N}||x||$ by Cauchy-Schwarz. But this is not good enough because I still have the sum I am not sure how to control it. The sum also reminded me of Bessel's inequality $$0\leq||x||^2-\sum_{k=0}^n |(x,e_k)|^2=||x-\sum_{k=0}^n (x,e_k)e_k||^2$$ since we have the bound $||\lambda||_{l^{\infty}} \sum_{k=0}^N (x,e_k)e_k$ . If I have would have $\lambda \in l^2$ I could bound the partial sums by $$(\sum_{k=0}^n \lambda_k^2)^{1/2}\left(\sum_{k=0}^n|(x,e_k)|^2\right)^{1/2}\leq ||\lambda||_{l^2}||x||$$ but I only know $\lambda \in l^{\infty}$ … For part b) I know because $H$ is Hilbert T is compact iff $T$ is a limit of finite rank operators. So I define $T_N(x)=\sum_{k=0}^{N}\lambda_k(x,e_k)e_k$ and look at $||Tx-T_Nx||=||\sum_{k=N+1}^{\infty}\lambda_k(x,e_k)e_k||$ . I was thinking this converges to $0$ because it is a tail sum of $Tx$ which converges. But this is obviously wrong as I didn't use $\lambda_k\to 0$ .","['compact-operators', 'functional-analysis']"
3545858,Find $f(r) $ if $\nabla ^2 f(r) =0$.,"The answer is $$f(r) = b + \frac{a}{r} $$ where $a$ and $b$ are constants.
Unfortunately I don't know how to find f(r). I was hinted that $$ \nabla ^2 f(r)=\frac{d^2f}{dr^2} + \frac{2}{r} \frac{df}{dr}$$ Where $f(r)$ is harmonic. I hope someone can give me a detailed answer. Thank you.","['ordinary-differential-equations', 'vector-fields', 'laplacian', 'partial-differential-equations', 'partial-derivative']"
3545864,"Show that the Volume of the set $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ is $1$","QUESTION Show that the Volume of the set $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ is $1$ OR Let $f: \mathbb{R} \rightarrow [0,\infty)$ a density function and Let $A=\left\{(x,y)\in \mathbb{R} \times [0,\infty) ; 0<y<f(x)\right\}$ show that $Vol(A)=1$ I have no idea about how the Volume of a set works, It would be great if anyone can help me. This is the definition of $Vol(A)$ I'm working with $$Vol(A)=\int\cdots\int \textbf 1_A(x_1,\ldots,x_n)dx_1\ldots dx_n$$","['measure-theory', 'multivariable-calculus', 'calculus', 'probability-theory', 'probability']"
3545934,Conjectured summation inequality,"I was playing around with numbers and noticed that the following series is quite close to $\sqrt 2$ ... but not quite. So I have conjectured that $\sqrt 2$ is arguably the closest highest bound; $$1+\frac 12\left(1-\frac{1}{2^2}\right)+\frac{1\cdot 3}{2\cdot 4^2}\left(\frac{1}{2}-\frac{1}{3^2}\right)+\frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6^3}\left(\frac{1}{3}-\frac{1}{4^2}\right)+\cdots < \sqrt{2}$$ Can this be proven? Is there a closed form  of the infinite series? I... have no idea how to tackle this problem. I've never seen anything like this and have just made it up because I was bored in physics class, but after some iterations, methinks this series is convergent. Are there any series that look like this, particularly for $\sqrt 2$ , that may serve a good use of application to measuring the truth of this conjecture? Thanks. :)","['conjectures', 'summation', 'fractions', 'sequences-and-series', 'inequality']"
3545950,The set of subsets of $\mathbb R$ which satisfy almost everything of the axioms for $\mathbb R$ and question of cardinality,"Suppose that $A$ is the set of all subsets of the set $\mathbb R$ which are not closed under multiplication or addition, as ordinarily defined in the case of $\mathbb R$ . So, $S \in A$ if and only if there either exist $x,y \in S$ such that $x+y \notin S$ or if there exist $x,y \in S$ such that $xy \notin S$ . Is cardinality of $A$ greater than the cardinality of $\mathbb R$ ?","['elementary-set-theory', 'real-numbers']"
3545958,"Maximal Ideals in $C((0 ,1))$","For a set $S\subset\mathbb R$ let $C(S)$ denote the continuous real-valued functions on $S$ . Describe the maximal ideals in $C((0,1))$ .  For $C([0,1])$ we know that maximal ideals are points in $[0, 1]$ and these are the only maximal ideals, all whose elements vanish at a single common point. The proof relies on the compactness of $[0,1]$ . I want to know what happens when the compact interval is replaced by the open interval $(0, 1)$ . Thanks for your help.",['linear-algebra']
3545976,How many ways can you split $11$ people into $2$ groups,"You have 11 students and are creating two groups. At least one student must be in a group. How many different combinations exist. My Solution: We know that no group can be empty, so we put $1$ student in every group. This gives us $9$ renaming students. Lets say that we only want to add $1$ more student in group $A$ , the amount of choices we have is $ 9 \choose 1 $ while all the renaming students are sent to the other group. If we wanted to place an extra two students in the group we would have $ 9\choose2 $ choice, this continues until $ 9 \choose 9 $ . So the total number of combinations is the sum of the individual options: $$ {9\choose1}+{9\choose2}+{9\choose3}+\cdots+{9\choose9}$$ I was wondering if my solutions is correct and if there is a better way.",['combinatorics']
3545988,Subsets of $\mathbb R$ which are closed under addition and multiplication and those which are not and question of existence of bijection,Suppose that $M$ is the set of all subsets of $\mathbb R$ which are closed under ordinary addition and multiplication as defined over $\mathbb R$ and suppose that $N$ is the set of all subsets of $\mathbb R$ which are either not closed under addition or multiplication (or both). Is there a bijection $b: M \to N$ ? I think that the only $n$ -element set in $M$ is $\{0\}$ and that all the other sets in $M$ are infinite but there is also much of infinite subsets of $\mathbb R$ in $N$ so $N$ consists of almost all finite subsets of $\mathbb R$ and of much of infinite ones so if I had to guess I would say that there is no such $b$ .,"['elementary-set-theory', 'real-numbers']"
3545998,Intuition behind $\sin(\theta)$ when introducing this to high school students,"When first introducing trigonometry to students, the traditional setup is to start with a right-angled triangle with reference angle $\theta$ and we label the sides with ""Hypotenuse, Opposite and Adjacent."" To keep students engaged with some practicality behind this, we can give an example of trying to figure out the height of a tree, know how far you are from the base of the tree and estimating the angle to the top of the tree. Then we define something arbitrary called "" $\sin(\theta) = \frac{\text{Opposite}}{\text{Hypotenuse}}$ "". I feel like at this point, students lose the conceptual intuition behind what's going on. Some students who are able to just accept it without questioning it too much can start punching in numbers and angles into the calculator when doing example questions. Other students who feel stuck with this weird idea might not be able to move forward. What would be a good idea to explain how to think about $\sin(\theta) $ ? I don't want to introduce a unit circle type definition because I feel like it will only make it less tangible for them. Can we do better than something like ""it's a magic computer which tells you the ratio of the opposite and hypotenuse sides of a right-angled triangle when you supply it the reference angle"" To maybe elaborate/clarify: I feel like a few things that students might not be able to understand If you take the tree example from above, we have the adjacent side and the angle. Now: The definition of $\tan(\theta)$ is the missing quantity we wanted in
  the first place. The ratio of the opposite side and the adjacent side.
  But how does $\tan$ go and calculate the ratio when I give it a
  angle? I think it's possible to convince them - once I have this ratio, I can find the length of the missing side: $\text{Opposite} = \tan(\theta)\times \text{Adjacent}$ .","['education', 'trigonometry', 'intuition']"
3546002,Limit of the function $f(x) = \begin{cases} 1 & \text{if } x = 1 \\ 0 & \text{otherwise} \end{cases} $,"Consider function $f(x) = \begin{cases} 
      1 & \text{if } x = 1 \\
      0 & \text{otherwise}
   \end{cases}
$ I am trying to find limit of this function when $x \to 1$ . By two-sided limit theorem, it can concluded $$\lim_{x \to 1^{-}}f(x) = \lim_{x \to1^{+}}f(x) = 0 \implies \lim_{x \to 1}f(x) = 0.$$ But when trying to apply the definition of the limit there is a problem: we want to bound $|f(x) - 0| = |f(x)|$ by $\epsilon$ , $|x - 1| < \delta$ and when $x = 1$ we have $|f(x)| = 1$ which is not bounded for any $\epsilon > 0$ . The question is: does the limit even exist?","['real-numbers', 'limits', 'real-analysis']"
3546089,Is there a natural family of nonisomorphic groups parametrized by $\mathbb{R}$?,"It's easy to construct a countable series of distinct groups - the cyclic groups, for instance - and it's also easy to create a family of groups parametrized by the reals, but most such constructions will have isomorphisms betwteen most if not all of the groups. As the category Group is very large, one would expect that somewhere inside it lies a continuum-sized collection of groups which can be parametrized in some natural way and are all distinct, that is, nonisomorphic. However, I've pondered this for a bit and asked some friends and haven't come up with any clear constructions. So I'm curious if anyone can provide a collection of groups which is ""natural"" in some sense (ideally continuous, and not a composition of a bijection from a set of groups to $\{0,1\}^{\mathbb{N}}$ and from there to $\mathbb{R}$ ), continuum-sized, and whose elements are not isomorphic. (Answers which provide a family parametrized by the nonnegative reals, or the positive reals, or the interval $[0,1]$ , would also suffice.)",['group-theory']
3546090,Maximum number of jokers,"A group of 30 knights and jokers are seated around a table. 
  The knights always tell the truth while the jokers sometimes tell the truth and sometimes lie. 
  They all answer to the question “what is the person sitting on your right? A knight or a joker?”. 
  We know beforehand that the number of jokers does not exceed a specific number. What is the maximum possible value of this number, so that, knowing every answer of the 30 people, we can be in a position to identify at least one knight? I really don't know how to start. I understand it must be solved by using the pigeonhole principle and combinatorics. Any clues? Thank you very much!",['combinatorics']
