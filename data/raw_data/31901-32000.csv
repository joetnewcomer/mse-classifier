question_id,title,body,tags
300444,Bijection between the reals and the set of permutations of the natural numbers?,"In analysis today we talked about re-arrangements of sequences, and one student asked how many re-arrangements there are of a given sequence. We were able to very quickly create a one-to-one function from the reals to the set of permutations on $\mathbb{N}$ by simply noting that for any real number, there is a re-arrangement of a conditionally convergent series that converges to that number. What we were not easily able to do was either prove that function was onto, or create an injection from the permutations on $\mathbb{N}$ back to the reals. So we know the number of re-arrangements is at least the cardinality of the reals, can we show it is exactly the same as the cardinality of the reals?","['sequences-and-series', 'elementary-set-theory']"
300446,Why is the localization at a prime ideal a local ring?,"I would like to know, why $ \mathfrak{p} A_{\mathfrak{p}} $ is the maximal ideal of the local ring $ A_{\mathfrak{p}} $, where $ \mathfrak{p} $ is a prime ideal of $ A $ and $ A_{\mathfrak{p}} $ is the localization of the ring $ A $ with respect to the multiplicative set $ S = A -\mathfrak{p} $ ?
Thanks a lot. N.B. : I have to tell you that I'm not very good at Algebra, so please, be more kind and generous in your explanation, and give me a lot of details about this subject please.
Thank you.","['localization', 'ring-theory', 'abstract-algebra', 'commutative-algebra', 'ideals']"
300460,Proving that $\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3 \cdot 4} +\ldots + \frac{1}{n(n+1)} = \frac{n}{n+1}$,How would we go about proving that $$\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3 \cdot 4} +\ldots +\frac{1}{n(n+1)} = \frac{n}{n+1}$$,"['summation', 'discrete-mathematics', 'proof-writing']"
300472,Proving that for any odd integer: $\lceil \frac{N^2}{4} \rceil = \frac{N^2 + 3}{4}$,I'm trying to construct a proof that for any odd integer: the ceiling of $\large \lceil  \frac{N^2}{4} \rceil = \frac{N^2 + 3}{4}$. Anyone have a second to show me how this is done?  Thanks!,"['discrete-mathematics', 'proof-writing', 'ceiling-and-floor-functions']"
300487,Calculate the number of elements of this set.,"Consider the following sets: $A = \{x \in N \mid x$ is a prime number and $x \lt 15\}$ $B = \{x\cdot y \mid x \in A$ and $y \in A$ and $x \ne y\}.$ How many elements does the set $B$ have? a) 15,$\;\;$ b) 20, $\;\;$c) 25,$\;\;$ d) 30, $\;\;$e) 35? The exercise solution gives ""a)"" as the correct answer ($B$ has $20$ elements), but I got ""$21$ numbers"" as the answer for the question ""how many elements does set B have?"". For me $A = \{1, 2, 3, 5, 7, 11, 13\}$ and $B$ = {1x2, 1x3, 1x5, 1x7, 1x11, 1x13, 2x3, 2x5, 2x7, 2x11, 2x13, 3x5, 3x7, 3x11, 3x13, 5x7, 5x11, 5x13, 7x11, 7x13, 11x13} = {1, 3, 5, 7, 11, 13, 6, 10, 14, 22, 26, 15, 21, 33, 39, 35, 55, 65, 77, 91, 143} and therefore set $B$ has 21 elements? Am I right?",['elementary-set-theory']
300493,Proving that for any odd integer:$\left\lfloor \frac{n^2}{4} \right\rfloor = \frac{(n-1)(n+1)}{4}$,"I'm trying to figure out how to prove that for any odd integer, the floor of: $$\left\lfloor \frac{n^2}{4} \right\rfloor = \frac{(n-1)(n+1)}{4}$$ Any help is appreciated to construct this proof! Thanks guys.","['discrete-mathematics', 'proof-writing']"
300494,Is axiom of choice required for there to be an infinite linearly independent set in a (non-finite-dimensional) vector space?,"In discussing this answer , I noted that while the statement: Any vector space has a basis is equivalent to the axiom of choice, I wondered if the statement that: Any vector space either has a finite basis or an infinite set of linear independent vectors was weaker than the axiom of choice. It feels weaker - it feels like you can inductively define a countably infinite set of linearly independent vectors without full choice, but I'm not sure if that is possible with ZF alone, or requires all of the Axiom of Choice, or is weaker but requires some form of choice.","['set-theory', 'linear-algebra', 'axiom-of-choice']"
300512,How to tell if $f(n) = 2 \left\lfloor\frac n2\right\rfloor$ from $\mathbb{Z}$ to $\mathbb{Z}$ is onto?,How can we tell if the function $f(n) = 2 \left\lfloor\frac n2\right\rfloor$ from $\mathbb{Z}$ to $\mathbb{Z}$ is onto?  Thanks!,['discrete-mathematics']
300518,Difficult integral involving $\arcsin(x)$,"I have a difficult integral to compute.  I know the result by guessing the answer, but need to know the method of calculation.  The integral is $$
\int_{a}^{b}{\rm d}p\,{p \over p^{2} - 2\mu}\,
\left[{2\arcsin\left({1 \over p}\,{p^{2} - ba \over  b-a}\right) - \pi}\right]
=\pi\ln\left({2\mu - a^{2} \over 
\mu - {ba \over 2} + \frac{1}{2}
\sqrt{\,\left(2\mu - a^{2}\right)\left(2\mu - b^{2}\,\right)\,}}\right)
$$ I am considering the case where $a<b$, $a^2<2\mu$, and $b^2<2\mu$, so that there is no divergence from the denominator.  I suspect it can be solved from some contour integral trick, especially since, by using the properties of arcsin, I can rewrite it in the following form: $
{\rm Re} \int_{a}^{b} dp\, \frac{p}{p^2-2\mu} \Big[-2i \ln \big( \frac{1}{2} i (p^2 -ab) +\frac{1}{2} \sqrt{(p^2 - a^2) (b^2 - p^2) }\big)-\pi\Big]
 =
 \pi \ln \frac{ (2\mu - a^2)}
{\mu-\frac{1}{2}ba + \frac{1}{2}\sqrt{(2\mu - a^2)(2\mu-b^2)}}$, where the Re takes the real part.  With this form, I am pretty sure there is a contour integral trick since the log function on the left is so similar to the log function on the right.  Unfortunately I cannot seem to figure it out, although I can verify it numerically.   Any help would be greatly appreciated!  Thanks, Dan","['integration', 'contour-integration']"
300519,Irreducible Subvariety,"Suppose $N$ is a subvariety of $M$. Furthermore, suppose $N$ is irreducible. Can I deduce that $N$ is contained in some irreducible component of $M$? In most examples I thought of, this works ($M \subseteq \mathbb{A}^{2}$ being the union of $x=0$ and $y=0$, $N$ being the point $(0,0)$ or one of the axes). I am really a beginner in algebraic geometry, so apart from the definition of irreducibility, I don't have an idea. I know that the following is false, though: if $N$ intersects some irreducible component of $M$, it doesn't mean it is contained in it. I would be glad to be given an explanation, or a counterexample. More than anything, I would like to see the line of thought.",['algebraic-geometry']
300526,Derivative of a factorial,"What is ${\partial\over \partial x_i}(x_i !)$ where $x_i$ is a discrete variable? Do you consider $(x_i!)=(x_i)(x_i-1)...1$ and do product rule on each term, or something else?
Thanks.","['factorial', 'calculus']"
300531,Integral representation of Euler's constant,"Prove that : $$ \gamma=-\int_0^{1}\ln \ln \left ( \frac{1}{x} \right) \ \mathrm{d}x.$$ where $\gamma$ is Euler's constant ($\gamma \approx 0.57721$). This integral was mentioned in Wikipedia as in Mathworld , but the solutions I've got uses corollaries from this theorem . Can you give me a simple solution (not using much advanced theorems) or at least some hints.","['definite-integrals', 'integration', 'real-analysis', 'euler-mascheroni-constant']"
300536,"If $\operatorname{Re}f^\prime > 0$ on a convex domain, then $f$ is one-to-one.","Let $f(z)$ be analytic on a convex region $D \subset \mathbb{C}$. If $\mathrm{Re}f'(z)>0,\forall z\in D$, then show that $f(z)$ is a one-to-one function, that is, if $z_1\ne z_2,$ then $f(z_1)\ne f(z_2)$.",['complex-analysis']
300542,Another limit from a math contest $\lim_{n\to\infty}\frac{x_n^2y_n}{3x_n^2-2x_ny_n+y_n^2}$,"Let $(x_{n})_{n\ge1}$, $(y_{n})_{n\ge1}$ be real number sequences and both converge to $0$. Evaluate $$\lim_{n\to\infty}\frac{x_n^2y_n}{3x_n^2-2x_ny_n+y_n^2}$$","['contest-math', 'calculus', 'real-analysis', 'limits']"
300544,"If $f(x/2)=f(x)/2$, then $f(x)=f'(0)x$","Let $f:\mathbb R \to \mathbb R$ be differentiable such that $f(x/2)=f(x)/2$ for any $x\in \mathbb R$. How can I prove that $f(x)=f'(0)x$, for any $x\in \mathbb R$? It seems easy, but I don't know why, I couldn't prove it. I need help, thanks.","['calculus', 'derivatives', 'functional-equations']"
300552,"Complex Analysis, Entire functions","Prove if $f$ and $g$ are entire and $e^f+e^g=1$, then $f$ and $g$ are constant. I believe the simplest way would be to use Louiville's theorem by using Pick's theorem but I am not sure on how to go about this.",['complex-analysis']
300586,"Where does the word ""torsion"" in algebra come from?","Torsion is used to refer to elements of finite order under some binary operation. It doesn't seem to bear any relation to the ordinary everyday use of the word or with its use in differential geometry (which relates back to the ordinary use of the word). So how did it acquire this usage in algebra? I'm interested to understand the intuition behind why the word ""torsion"" was chosen for this notion, as well as when it was first used.","['terminology', 'math-history', 'group-theory', 'abstract-algebra']"
300596,Characterize a large class of shapes using a finite number of parameters,"I am doing some numerical computations searching for an optimal shape for a certain functional. In my particular case, the shape $\Omega$ is a 2 dimensional star shaped domain by the origin, which means that its boundary can be completely characterized by the radial function $r(\theta)$. Developing $r$ into a Fourier series $r(\theta)=a_0/2+\sum a_n \cos(n\theta)+\sum b_n \sin (n\theta)$ and truncating to the first $M$ sin and cos coefficients we can get a pretty good approximation of the actual shape using only a finite number of parameters ($2M+1$). Do you know any similar ways to characterize a general shape using only a finite number of parameters?","['fourier-series', 'geometry', 'approximation', 'plane-curves']"
300597,"Determinant of the matrix $D_n(2,3,1)$","The matrix $D_n(2,3,1)$ is to be written in the form $$\pmatrix{3 & 1 & 0 & 0 & ... & 0 \\ 2 & 3 & 1 & 0 & ... & 0 \\ 0 & 2 & 3 & 1 &... &0\\ : & : & : & : & ... & : \\0 & 0 & 0 & 0 &... & 3}$$ I need to workout the determinant of this matrix $( d_n = \det(D_n))$. So I said to expand by row $1$. Looking the element at $a_{11}$, I can cancel that row and column out and get the determinant of that bit to be $3d_{n-1}$. Now, by expanding with element $a_{12}$, I get the determinant of that bit to be $-2d_{n-1}$ and so I get $$d_n = 3d_{n-1} - 2d_{n-2}$$ The charateristic equation of this gives me $x^2 - 3x +2 = 0 = (x - 2)(x - 1)$ and so I get the roots to be $x_{1,2} = 1,2$. As we have two distinct roots, (here's where my theory has gone a bit funny) we get that it can be written in the form $d_n = C_1x_1^n + C_2 x_2^n$. I solved this and got that $C_1 = 2$ and $C_2 = -1$, but I'm not sure how to complete the question. Does this just mean that the determinant is $2(1)^n - 2^n = 2 - 2^n$?","['geometry', 'determinant']"
300598,Connection between Maxwell's equations and Cauchy-Riemann equation,How to get Cauchy-Riemann equation from Maxwell's equations ?,['complex-analysis']
300619,How to use Rolle's Theorem to prove this statement?,"If $f$ is a polynomial with $n$ distinct zeros, then $f'$ has at least
  $n-1$ zeros.","['derivatives', 'real-analysis']"
300632,Is there any way to calculate change in derivatives along a vector?,"Suppose I have a function F($\vec{x})$ $x\subset R^{n}$, and a vector $\vec{g}$. I want to perturb $\vec{x}$ along the $\vec{g}$ vector, and see how the gradient changes as I move further along the $\vec{g}$ vector. So I'm trying to solve $\lim_{h\to 0}(\nabla F(\vec{x}+h*\vec{g})-\nabla F(\vec{x}))/h$. Note, I don't want to know the change in the function as h approaches 0, but the change in each gradient (so there should be as many terms in the answer to the above formula as there are dimensions in $\vec{x}$. I'm pretty sure there should be an efficient way to do this (linear in the number of parameters), since at worst case I can just use a small value for h and use finite differences. Likewise, I think we should be able to get higher derivatives along the vector also linear in the number of parameters, since we can also accomplish that linear in the number of parameters with finite differences. My first thought was to set $\vec{x}(h)=\vec{x}_{o}+h*\vec{g}$, and then try to use the chain rule, so I end up with $F'(\vec{x}(h))*\vec{x}'(h)*dh$, but that quantity seems like it would be one dimensional ($\frac{dF}{dh}$), and not the change in the each gradient itself. Plus, I'm not sure that would even work for higher derivatives, since $F''(\vec{x})$ has $n^{2}$ terms, so we're no longer linear in the number of parameters. Now I'm trying to see if I can derive some transformation of F that will change it to a new function that has as its derivatives the quantities I'm trying to calculate, but I'm not having much luck doing it that way either. In general, I'd like to be able to calculate the change in the gradients along h even if $\vec{x}(h)$ is a non-linear function of h (so we're following a curve instead of a line), but for the time being I'm keeping it simple and saying that $\vec{x}(h)$ is linear. Is there some general way to find these quantities.","['calculus', 'derivatives']"
300635,Construction of Hadamard Matrices of Order $n!$,"I'm trying to get a hand on Hadamard matrices of order $n!$, with $n>3$. Payley's construction says that there is a Hadamard matrix for $q+1$, with $q$ being a prime power.
Since
$$
n!-1 \bmod 4 = 3
$$ construction 1 has to be chosen: If $q$ is congruent to $3 (\bmod 4)$ [and $Q$ is the corresponding Jacobsthal matrix ] then
  $$    H=I+\begin{bmatrix} 0 & j^T\\ -j & Q\end{bmatrix} $$
  is a Hadamard matrix of size $q + 1$. Here $j$ is the all-1 column vector of length $q$ and $I$ is the $(q+1)×(q+1)$ identity matrix. The matrix $H$ is a skew Hadamard matrix, which means it satisfies $H+H^T = 2I$. The problem is that the number of primes among $n!-1$ is restricted (see A002982 ). I checked the values of $n!-1$ given by Wolfram|Alpha w.r.t. be a prime power, without success, so Payley's construction won't work for all $n$. Is there a general way to get the matrices, or is it case by case different? I haven't yet looked into Williamson's construction nor Turyn type constructions. Would it be worth a closer look (sure it would, but) concerning my problem? Where can I find their constructions? PS for the interested reader: I've found a nice compilation of Hadamard matrices here: http://neilsloane.com/hadamard/","['matrices', 'hadamard-matrices', 'reference-request']"
300708,Does this prove that there are no infinite sets?,"I changed this question so that the socks don't have labels. The labels weren't important. Suppose we have an infinite number of identical socks $s$. Call this infinite set $S$. Take one of the socks $s$ out of the set $S$ and we still have the original set $S$, since the socks are all identical and there are infinitely many, plus the sock $s$ that we took out, so we have $S \cup \{s\}$. But this new set $S \cup \{s\}$ is really just the same as the old set $S$, since the socks are all the same, so we have $S \cup \{s\}=S$. Subtracting $S$ on both sides, we obtain $\{s\}=\emptyset$, a contradiction. So there can be no infinite sets.",['elementary-set-theory']
300727,How to use Rolle's Theorem to prove the following?:,"For each $\lambda$, the function $f(x)=x^3-\frac 32x^2+\lambda$ does
  not have two distinct zeros in $[0,1]$.","['derivatives', 'real-analysis']"
300745,"If a function is uniformly continuous in $(a,b)$ can I say that its image is bounded?","If a function is uniformly continuous in $(a,b)$ can I say that its image is bounded? ($a$ and $b$ being finite numbers). I tried proving and disproving it. Couldn't find an example for a non-bounded image. Is there any basic proof or counter example for any of the cases? Thanks a million!",['functions']
300760,"Does the limit as $(x,y) \to (1,2)$ of $3x^3-x^2 y^2$ exist?","The title says it all: does the following limit exist? $$\lim_{\large(x, y) \to (1, 2)} \; 3x^3 - x^2y^2$$ It approaches $\,-1\,$ with direct substitution, but if you approach the point with the curve $x = y^2$, you get $\,128\,$ which is not $\,-1,\,$ meaning the limit doesn't exist.  I got the problem wrong but I am curious to see if it does actually exist. I'm approaching with $\,x = y^2.\,$  So it ends up being $\;\displaystyle\lim_{y\to 2}\;3y^6-y^6,\,$ which is $128$.","['calculus', 'limits']"
300766,Signed Measure Integral,"Let $\mu$ a signed measure, and $f$ a integrable function with respect to $|\mu|$, and if $\nu$ is defined for every mensurable set by 
$$\nu(E) = \int_E f \, d\mu$$ then $\nu \ll \mu$. I need some tips, because in the exercise I have a hint, that is: $$\int f \, d\mu = \int f \, d\mu^+ - \int f \, d\mu^-$$ If $\mu$ is a finite signed measure, then, for every measurable set $E$ $$|\mu|(E) = \sup \left|\int_E f \, d\mu \right|$$ where the supremum is extended  over all measurable functions $f$ such that$|f| \leq 1$. In the case that $\mu$ is finite sigbed measure is it solve right? Or there is some things that I forget? Thanks so much",['measure-theory']
300788,Pathological function needed,"Give a differentiable function that has a positive derivative at $0$,
  yet is not increasing on any open neighbourhood of $0$. I believe that the required function needs to have a derived function that is discontinuous at $0$ (ie. the required function needs to be not continuously differentiable at $0$). Any suggestion would be appreciated.","['calculus', 'derivatives', 'real-analysis']"
300800,finding $\int_0^\infty \dfrac{dx}{1+x^4}$ through complex analysis,I am trying to find $\int_0^\infty \dfrac{dx}{1+x^4}$ by setting it equal to $\dfrac{1}{2}\oint_C \dfrac{dz}{1+z^4}$ and solving that. By a computer program I've calculated it to be $\approx 1.11072$; I am not getting that answer though. I factor it into $\dfrac{1}{2}\oint_C \dfrac{dz}{(z-1)(z+1)(z^2 + 1)}$; there is a simple pole at $z = 1$ and a 4th order pole at $z = i$; I find the residue at 1 to be $\dfrac{1}{4}$ and at $i$ to be $\dfrac{-1}{4}$; the second pole is found by L'Hopital's rule. Could someone tell me what I'm missing? Thanks,"['residue-calculus', 'complex-integration', 'complex-analysis']"
300805,When is the number $11111\cdots1$ a prime number?,"For which $n$ is the sum:
$$\sum_{k=0}^{n}10^k$$
a prime number? Are they finite?","['prime-numbers', 'repunit-numbers', 'number-theory']"
300815,"Show that open segment $(a,b)$, close segment $[a,b]$ have the same cardinality as $\mathbb{R}$","a) Show that any open segment $(a,b)$ with $a<b$ has the same cardinality as $\mathbb{R}$. b) Show that any closed segment $[a,b]$ with $a<b$ has the same cardinality as $\mathbb{R}$. Thoughts: Since $a<b$, $a,b$ are two distinct real number on $\mathbb{R}$, we need to show it is 1 to 1 bijection functions which map between $(a,b)$ and $\mathbb{R}$, $[a,b]$ and $\mathbb{R}$. But we know $\mathbb{R}$ is uncountable, so we show the same for $(a,b)$ and $[a,b]$? and how can I make use of the Cantor-Schroder-Bernstein Theorem? The one with $|A|\le|B|$ and $|B|\le|A|$, then $|A|=|B|$? thanks!!","['cardinals', 'elementary-set-theory']"
300818,Finding derivatives of Trigonometric Functions,"I'm having a very difficult time understanding how find the derivative of trig functions.
Here's one that I've been trying to crack for the half hour:
$$y = \frac{11}{\sin x} + \frac{1}{\cot x}$$ I know that 
$$\frac{d}{dx}(\sin x) = \cos x$$
and that
$$\frac{d}{dx}(\cot x) = -\csc^2 x$$ I also know that the final answer is
$$y' = -11\csc x\cot x+\sec^2 x$$ But no matter what I try I can't seem to come up with that answer. What I really need is systematic or step by step approcah that I could apply to these types of problems, no matter what format the're in. Because there's many more like this, except in different formats, that I have to solve.","['calculus', 'derivatives']"
300835,"Show if $\sum\limits_{k=1}^\infty {a_k}^2$,$\sum\limits_{k=1}^\infty {b_k}^2$ converge, their product converges too","Show if $\displaystyle\sum\limits_{k=1}^\infty {a_k}^2$ and $\displaystyle\sum\limits_{k=1}^\infty {b_k}^2$ both converge, $\displaystyle\sum\limits_{k=1}^\infty {a_kb_k}$ also converge then Show if $\displaystyle\sum\limits_{k=1}^\infty {a_n}$ and $\displaystyle\sum\limits_{k=1}^\infty {b_n}$ both converge, $\displaystyle\sum\limits_{k=1}^\infty {a_nb_n}$ also converge then Find two convergent series $\displaystyle\sum\limits_{k=1}^\infty {a_k}$ and $\displaystyle\sum\limits_{k=1}^\infty {b_k}$ such that $\displaystyle\sum\limits_{k=1}^\infty {a_kb_k}$ diverges. My thought and attempt: For the first part, By the theorem (If the series $\displaystyle\sum\limits_{k=1}^\infty {a_n}$ is convergent, then $\displaystyle\lim_{n\to\infty} a_n=0$), if $\displaystyle\sum\limits_{k=1}^\infty {a_k}^2$ and $\displaystyle\sum\limits_{k=1}^\infty {b_k}^2$ both converge, then $\displaystyle\lim_{n\to\infty} {a_k}^2=0$ and $\displaystyle\lim_{n\to\infty} {b_k}^2=0$ both equal to zero. Then we can obtain both $(a_k)_{k=1}^\infty$ and $(b_k)_{k=1}^\infty$ converge thus $\displaystyle\lim_{n\to\infty} {(a_k)^2\over|a_k|} = \displaystyle\lim_{n\to\infty} |a_k| = 0$, do the same thing for $b_k$ Since limit preserves arithmetic operation, $\displaystyle\sum\limits_{k=1}^\infty {a_kb_k}$ converges. Second part, I dont have any idea... Third part is $a_n = b_n = {(-1)^n\over\sqrt{n+1}}$, am I right?? Thank you everyone... please help...","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
300846,Construct a convergent series of positive terms with $\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty$,"Construct a convergent series of positive terms with $\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty$ My thoughts: By the theorem: Suppose $a_n\ge0$ for all $n$, and let $l=\displaystyle\limsup{\sqrt[n]{a_n}}$. If $l<1$, then $\displaystyle\sum_{n=1}^\infty a_n$ converges; and if $l>1$, then $\displaystyle\sum_{n=1}^\infty a_n$ diverges. Thus, we know $\displaystyle\sum_{n=1}^\infty {a_{n+1}\over{a_n}}$ diverges I guess it might works, $(x_n)_{n=1}^{+\infty}$, $x_{n+1} = x_n^2 + x_n$ for all $n\ge1$..","['divergent-series', 'sequences-and-series', 'real-analysis']"
300851,$(L^\infty)^*$ is not isomorphic to $L^1$,"I am working on this problem in Rudin: Let $L^\infty=L^\infty(m)$, where $m$ is Lebesgue measure on $I=[0,1]$. Show
  that there is a bounded linear functional $\lambda\neq 0$ on $L^\infty$ that
  is $0$ on $C(I)$, and that therefore there is no $g\in L^1(m)$ that satisfies
  $\lambda f=\int_I fg dm$ for every $f\in L^\infty$. Thus
  $(L^\infty)^*\neq L^1$. I've figured out all but the part that saids ""
$(L^\infty)^*\neq L^1$"". Most of the problem seems to be showing that the normal method of showing $(L^p)^* \cong L^q$ doesn't work. However, I don't see how that helps me do the last deduction.","['measure-theory', 'real-analysis']"
300894,Intuition for scale of the largest eigenvalue of symmetric Gaussian matrix,"Let $X$ be $n \times n$ matrix whose matrix elements are independent identically distributed normal variables with zero mean and variance of $\frac{1}{2}$. Then
$$
     A = \frac{1}{2} \left(X + X^\top\right)
$$
is a random matrix from GOE ensemble with weight $\exp(-\operatorname{Tr}(A^2))$. Let $\lambda_\max(n)$ denote its largest eigenvalue. The soft edge limit asserts convergence of $\left(\lambda_\max(n)-\sqrt{n}\right) n^{1/6}$ in distribution as $n$ increases. Q: I am seeking to get an intuition (or better yet, a simple argument) for why the largest eigenvalue scales like $\sqrt{n}$.","['intuition', 'probability']"
300900,Does $\sin^2 x - \cos^2 x = 1-2\cos^2 x$?,"I am finishing a proof.  It seems like I can use $\cos^2 + \sin^2 = 1$ to figure this out, but I just can't see how it works.  So I've got two questions. Does $\sin^2 x - \cos^2 x = 1-2\cos^2 x$? And if it does, then how?",['trigonometry']
300901,Material in a first course in algebraic geometry?,"First I would like to say that my question is not about what books to use in algebraic geometry; for this there are many threads that discuss this on Math.SE and on MO. My question is about what material should be included in a first course in algebraic geometry. Let me explain. My university does not offer courses in algebraic geometry - thus I have found the need to try and ""create one""  by doing a reading course with a lecturer. Now one may say that we should use this or that book, but from experience it is not the book that matters but ultimately the material that one learns. When learning algebraic number theory, I found myself looking at things from Marcus' Number Fields to KCd's notes, to Neukirch, etc. My question is: What should be included in a first serious course in algebraic geometry? The level of such a course should be for someone who has studied commutative algebra, algebraic number theory and algebraic topology. Preferably, each answer should include a list of ""canonical topics"" to be studied. Thanks.","['algebraic-geometry', 'reference-request']"
300910,$\lim_{x\rightarrow\infty}\frac{f(x)}{e^x}$ for analytic functions,"For some analytic function $f(x)=\lim_{n\rightarrow\infty}\sum^n_{r=0}c_rx^r$,
$$\lim_{x\rightarrow\infty}\frac{f(x)}{e^x}=\lim_{x\rightarrow\infty}\frac{\lim_{n\rightarrow\infty}\sum^n_{r=0}c_rx^r}{e^x}=\lim_{n\rightarrow\infty}\sum^n_{r=0}c_r\lim_{x\rightarrow\infty}\frac{x^r}{e^x}=\lim_{n\rightarrow\infty}\sum^n_{r=0}c_r(0)=\lim_{n\rightarrow\infty}0=0$$
But obviously this cannot be true? For say, $f(x)=e^x$ we would have $$\lim_{x\rightarrow\infty}\frac{e^x}{e^x}=1$$ and $$\lim_{x\rightarrow\infty}\frac{e^x}{e^x}=0$$
which would imply$$0=1$$ UPDATE:
An answer has suggested that the two limits cannot be inverted. Is this true and why is this so? I would like a more complete explanation if this is the case.
Also if this were the case, couldn't this modification to the question avoid that pitfall completely?
$f(x)=\sum^\infty_{r=0}c_rx^r$,
$$\lim_{x\rightarrow\infty}\frac{f(x)}{e^x}=\lim_{x\rightarrow\infty}\frac{\sum^\infty_{r=0}c_rx^r}{e^x}=\sum^\infty_{r=0}c_r\lim_{x\rightarrow\infty}\frac{x^r}{e^x}=\sum^\infty_{r=0}c_r(0)==0$$ Thanks in advance. Any other thoughts or comments welcome.","['power-series', 'analyticity', 'limits']"
300916,Prove that $p$ is prime in $\mathbb{Z}[\sqrt{-3}]$ if and only if $x^2+3$ is irreducible in $\mathbb{F}_p[x]$.,"I'm having trouble with this particular homework problem. I think I have one direction: Let $R=\mathbb{Z}[\sqrt{-3}]$. If $p$ is prime, then $p$ is irreducible in $R$, since $R$ is an integral domain. Thus, $(p)$ is maximal, so
$$
R/(p) \cong \mathbb{F}_p[x]/(x^2+3)
$$
is a field. Hence, $(x^2+3)$ is maximal in $\mathbb{F}_p[x]$, so $x^2+3$ is irreducible. However, if we assume $x^2+3$ is irreducible in $\mathbb{F}_p[x]$, then that only implies that $p$ is irreducible in $R$ by the same reasoning. Since $R$ isn't a UFD, this doesn't imply $p$ is prime, so I'm stuck. Thanks so much!","['ring-theory', 'abstract-algebra']"
300917,Tensor product of domains is a domain,"I'm reading Milne's Algebraic Geometry course notes, version 5.22, as a companion to an algebraic geometry course I'm taking now. Proposition 4.15 states: Let $A$ and $B$ be $k$-algebras, which are also domains, with $k = \overline{k}$ and $A$ finitely generated. Then $A \otimes_k B$ is a domain. For homework I've proved that the direct product of two irreducible algebraic sets is again an irreducible algebraic set. The teacher simply stated that this implies the above result when $A, B$ are finitely generated. So, my first question is, can someone explain this implication to me? My second question is, is the above result from Milne true if both $A$ and $B$ are not finitely-generated? If not, can you provide a counterexample?","['commutative-algebra', 'algebraic-geometry', 'tensor-products']"
300925,Jacobian of a smooth flow on a smooth manifold,"This is a question about the derivative of Jacobian of the tangent map of a smooth flow. I think it might have an intrinsic formula, but I don't have a clue what it might be. Let $M$ be an $n$-dimensional Riemannian manifold (say $C^\infty$, orientable, etc if it can save us from some trouble), $X:M\to\mathfrak{X}(M)$ be a smooth vector field (say nowhere vanishing), $\phi_t:M\to M$ be the flow induced by $X$. Now let's consider the tangent map 
$D_x\phi_t:T_xM\to T_{\phi_tx}M$. Since each tangent space has an inner product structure induced by the metric, we can define the Jacobian of $\phi_t$ at $x$ as $J(t,x)=\det(D_x\phi_t)$. Now we want to take the derivative with respect to $t$: $J_t(t,x)$. Is there any formula for this derivative? In fact a formula for $J_t(0,x)$ will be good enough. We can choose a local orthonormal frame around $x$ and then write $D_x\phi_t$ as a time-dependent matrix, say $A(t,x)=(a_{jk}(t,x))=({\bf a}_1(t,x),\cdots,{\bf a}_n(t,x))$. Then $J(t,x)=\det A(t,x)$ and $\displaystyle J_t(t,x)=\sum_{1\le j\le n}\det({\bf a}_1(t,x),\cdots,{\bf \dot{a}}_i(t,x),\cdots,{\bf a}_n(t,x))$.
Then letting $t\to0$, we get that ${\bf a}_j(0,x)={\bf e}_j$ (since $\phi_0=\text{Id}$) and $\displaystyle J_t(0,x)=\sum_{1\le j\le n}\det({\bf e}_1,\cdots,{\bf \dot{a}}_j(0,x),\cdots,{\bf e}_n)=\sum_{1\le j\le n}\dot{a}_{jj}(0,x)$. I feel like it should be a fundamental fact in Ordinary Differential Equations. But I don't know where to start. Thank you! A warmup: consider $M=\mathbb{R}$ and the equation $\dot{x}=f(x)$. Let $\phi(t,x)=\phi_t(x)$ be the solution with initial condition $\phi(0,x)=x$.
Then $J(t,x)=\partial_x \phi(t,x)$ and $J_t(t,x)=\partial_t\partial_x \phi(t,x)=\partial_x\partial_t\phi(t,x)=\partial_x f(x(t))$. In particular $J_t(0,x)=f'(x)$. Then let $M=\mathbb{R}^2$ and the equation $\dot{x}_i=f_i({\bf x})$. Let $\vec{\phi}(t,{\bf x})=\vec{\phi}_t({\bf x})$ be the solution with initial condition $\vec{\phi}(0,{\bf x})={\bf x}$.
Then $J(t,x)=\partial_1\phi_1\cdot\partial_2\phi_2-\partial_2\phi_1\cdot\partial_1\phi_2$. Taking derivative w.r.t. $t$, $\dot{\phi}_i(t,x)=f_i({\bf x}(t))$ and hence $J_t(0,x)=\sum_j \partial_jf_j(x)=\mathrm{div}(f_1,f_2)$. So in general $J_t(0,x)=(\mathrm{div}X)(x)$.","['dynamical-systems', 'ordinary-differential-equations']"
300929,Understanding delaying or advancing a discrete time signal,"Suppose I have a discrete time signal x[n]. It is said that x[n-k], where K>0, is a delayed version of x[n]. I am trying to understand this intuitively. My observation is in the signal I am subtracting time in x[n-k], by k units. Means I am doing some thing 'quickly' as compared to x[n]. So why do we call it delay instead advance?",['discrete-mathematics']
300965,Rolling three dice...am I doing this correctly?,"Tree dice are thrown. What is the probability the same number appears on exactly two of the three dice? Since you need exactly two to be the same, there are three possibilities:
1. First and second, not third
2. First and third, not second
3. Second and third, not first For 1) The first die, you have $\frac{6}{6}$ . The second die needs to be equal to the first, so you have probability of $\frac{1}{6}$ . Then the third die can't be equal to the first and second dice, so it's $\frac{5}{6}$ . 
All together you get $1 \cdot \frac{1}{6} \cdot \frac{5}{6}$ . And since the next two cases yield the same results, then the probability that the same number apears on exactly two of the three dice is $$ 3 \cdot \left(1 \cdot\frac{1}{6} \cdot \frac{5}{6}\right)=\frac{5}{12}$$ Did I do this correctly?
Thank you.","['probability', 'solution-verification']"
300998,$\omega_1$ is $C^*$-embedded in $\omega_1+1$,"consider the space $ω_1$ (the first uncountable ordinal) and $ω_1+1$ together with the order topology. is it true that $ω_1$ is $G_\delta$-dense subspace of $ω_1+1$? $\omega_1$ is $C^*$-embedded in $\omega_1+1$? If both proposition are correct, then we conclude that $\omega_1$ is $C$-embedded in $\omega_1+1$. thanks.","['general-topology', 'ordinals']"
301000,How to calculate square matrix to power n?,"I have a matrix of non-negative numbers, say $A$. (1) How do we calculate $A^n$? (2) How can we calculate $A^n$ using usual matrix exponential trick to do it fast ? Edit 1 Also theres another property of matrix A that its diagonals consists always of 0 & other elements either 0 or 1. Can we do this just by involving matrix multiplication ?",['matrices']
301010,evaluate $\int_0^\infty \dfrac{dx}{1+x^4}$ using $\int_0^\infty \dfrac{u^{p-1}}{1+u} du$,"evaluate $\int_0^\infty \dfrac{dx}{1+x^4}$using $\int_0^\infty \dfrac{u^{p-1}}{1+u} du = \dfrac{\pi}{\sin( \pi p)}$. I am having trouble finding what is $p$. I set $u = x^4$, I figure $du = 4x^3 dx$, I am unsure though how to find $p$ though. Could someone tell me what I am missing? Thanks.","['residue-calculus', 'improper-integrals', 'complex-analysis']"
301018,Compute $\lim\limits_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}}$ [duplicate],"This question already has answers here : Show that that $\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4$ (7 answers) Closed 1 year ago . Compute
$$\lim_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}}$$
If you have some nice proofs and you're willing to share them, then I thank you and you definitely have my upvote!","['radicals', 'calculus', 'binomial-coefficients', 'limits']"
301029,Prove the identity $1 + \sin x = 2 \cos^2 \left(45° - \frac{x}{2}\right)$,"Here is the problem:
                     $$1 + \sin x = 2 \cos^2 \left(45° - \frac{x}{2}\right)$$ Can you help me prove  that this is an trigonometric identity?","['trigonometry', 'algebra-precalculus']"
301043,What is ${\rm Aut}(G^n)$?,"Suppose $G$ a finite indecomposable group such that $Z(G)=\{e\}$.
Now consider $G^n$. What is ${\rm Aut}(G^n)$? My claim is ${\rm Aut}(G^n)={\rm Aut}(G)^n \rtimes S_n$.","['finite-groups', 'group-theory', 'abstract-algebra']"
301063,Taylor expansion of an integral,"I am interested in the Taylor series expansion around $t=0$ of the following expression: $$I(t)=\int_{0}^{\infty}e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)dx$$ Normally, I would proceed by taking the derivatives inside the integral using the Leibniz Integral Rule, however, in this case I am not sure if I can do this, since I can't find the dominating function $g(x)$ for the integrand such that $|e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)|\leq g(x)$ that is independent of $t$.  Such dominating function is a condition for interchanging the order of differential and integration . Re-arranging the equation above using arithmetic, interchanging the differential and the integral in the following expression would be very useful: $$\frac{\partial}{\partial t}\int_0^\infty e^{-x^2}\log \operatorname{cosh}(xt)dx$$
where $\operatorname{cosh}(x)=\frac{e^{x}+e^{-x}}{2}$ is the hyperbolic cosine function.  However, I cannot find a dominating function for the integrand in this case that does not depend on $t$. Any help would be appreciated.  I think restricting $t$ to $0\leq t \leq t_{\max} <\infty$ would work, but I am wondering if I can do this without the restriction on $t$ (other than being a real number).","['taylor-expansion', 'derivatives', 'real-analysis', 'analysis']"
301087,Proving that the differential on an elliptic curve $E$ given by $\omega=\frac{dx}{y}$ is translation invariant,"I'm taking a course on elliptic curves and I'm stuck on a line in a proof. We're assuming we're in an algebraically closed field $K$ and char($K)\not=2$. We have our elliptic curve $$E:y^2=(x-e_1)(x-e_2)(x-e_3)$$ where the $e_i$ are distinct. We've proved that $\omega=\frac{dx}{y}$ is a regular differential on $E$. Now for $P \in E$ we define $\begin{array}{llll}\tau_P:&E &\rightarrow &E \\ & Q & \mapsto & P \bigoplus Q.\end{array}$ Since $\tau_P^*\omega$ is a regular differential on $E$, we have $\tau_P^*\omega=\lambda_P \omega$ for some $\lambda_P \in K^\times$. Then it is claimed that The map $$\begin{array}{lll}E &\rightarrow & \mathbb{P}^1 \\ P & \mapsto & \lambda_P\end{array}$$
  is a morphism of algebraic curves. Now I'm not sure why this is true. If I could show it was a rational map then I'd be done, since rational maps from smooth projective curves are morphisms. Well, so long as $P\not=0_E$ and we're not where the formulae for adding points degenerate, we have (say $P=(x_P,y_P)$) $\tau_P^*\omega=\frac{d(x \circ \tau_P)}{y\circ \tau_P}=\left(\mbox{rational function of }x, y, x_P\mbox{ and }y_p\right) \frac{dx}{y}$ but I don't see why that rational function doesn't actually have any $x$ or $y$ dependence. Because $\omega$ is a $K$-basis for the space of regular differentials we know this rational function is in $K$. Is that it?","['algebraic-geometry', 'elliptic-curves']"
301099,Is it true that the empty set is contained in every Cartesian Product?,"Is it true to say that $\forall A,B$ (set) $\emptyset \subseteq A\times B $ Or: The empty set is contained in every Cartesian Product? (As it is contained in every set).",['elementary-set-theory']
301106,How many different game situations has connect four?,"In the game connect four with a $7 \times 6$ grid like in the image below, how many game situations can occur? Rules : Connect Four [...] is a two-player game in which the players first
  choose a color and then take turns dropping colored discs from the top
  into a seven-column, six-row vertically-suspended grid. The pieces
  fall straight down, occupying the next available space within the
  column. The object of the game is to connect four of one's own discs
  of the same color next to each other vertically, horizontally, or
  diagonally before your opponent. Source: Wikipedia Image source: http://commons.wikimedia.org/wiki/File:Connect_Four.gif Lower bound : $7 \cdot 6 = 42$, as it is possible to make the grid full without winning Upper bound : Every field of the grid can have three states: Empty, red or yellow disc. Hence, we can have $3^{7 \cdot 6} = 3^{42} = 109418989131512359209 < 1.1 \cdot 10^{20}$ game situations at maximum. There are not that much less than that, because you can't have four yellows in a row at the bottom, which makes $3^{7 \cdot 6 - 4} = 1350851717672992089$ situations impossible. This means a better upper bound is $108068137413839367120$ How many situations are there? I think it might be possible to calculate this with the approach to subtract all impossible combinations. So I could try to find all possible combinations to place four in a row / column / vertically. But I guess there would be many combinations more than once.","['recreational-mathematics', 'combinatorics']"
301127,"Let the function $f:[a,b] \to \mathbb R$ be Lipschitz. Show that $f$ maps a set of measure zero onto a set of measure zero","Let the function $f:[a,b] \to \mathbb R$ be Lipschitz, that is, there is a constant $c \geq 0$ such that for all $u,v \in [a,b]$, $|f(u)-f(v)| \leq c|u-v|$. Show that $f$ maps a set of measure zero onto a set of measure zero. Show that $f$ maps an $F_\sigma$ set onto an $F_\sigma$ set. Conclude that $f$ maps a measurable set to a measurable set. I have a question about this. We know that a set of measure zero must either be of the form [0], or it must be a set of countable elements. But for this function, we know that the domain is an interval...and we cannot have an interval with only countable elements, because an interval contains all reals, right? So for the first part of this question, we need to prove that [0] is mapped to [0], right? Thanks in advance","['holder-spaces', 'real-analysis']"
301135,"Universal Covering Group of $SO(1,3)^{\uparrow}$","I'm trying to prove that $SL(2,\mathbb{C})$ is the universal covering group for the proper orthochronous Lorentz group $SO(1,3)^{\uparrow}$. The standard way goes as follows. (1) Exhibit a real vector space isomorphism between Minkowski space and the space of $2\times 2$ Hermitian matrices, $H$. (2) Let $SL(2,\mathbb{C})$ act on $H$ by $X\mapsto AXA^{\dagger}$ and prove this induces a surjective, 2:1 homomorphism from $SL(2,\mathbb{C})$ to $SO(1,3)^{\uparrow}$. I'm wondering whether there is a better way however. To prove that $SU(2)$ is the universal covering group of $SO(3)$ it suffices to go to the Lie algebra and demonstrate that the adjoint representation is an isomorphism of Lie algebras. Can I do something analogous here? Here's what I've tried. The Lie algebra of $SO(1,3)$ is $su(2)\oplus su(2)$ which naturally acts on a 4-dimensional complex vector space. The Lie algebra of $SL(2,\mathbb{C})$ is the space of traceless complex matrices, of dimension 6. I can't now see how to proceed. Maybe this approach doesn't work at all now. Is it just a special property of $SU(2)$ and $SO(3)$ that happens because $SU(2)$ happens to have dimension 3, exactly the right number for an $SO(3)$ action? Many thanks in advance!","['lie-algebras', 'representation-theory', 'abstract-algebra', 'lie-groups', 'group-theory']"
301182,More efficient estimator?,"I have this kind of problem: The market share $Z$ of a company is estimated in two independent survey polls. The sample sizes are $n_1=500$ and $n_2=2000$ with corresponding observed shares $p_1$ and $p_2$, respectively. These two results are to be combined. Two
alternative estimators of $Z$ are presented as follows: $Z_a=\frac{p_1+p_2}{2}$ $Z_b=\frac{p_1+4p_1}{5}$ Are these estimators unbiased? Which one of these estimators is more efficient? I know that $E(Z)=0.2p_1+0.8p_2=E(Z_b)\rightarrow Z_b$ is unbiased and $E(Z_a)=0.5p_1+0.5p_2$ so $Z_b$ is biased. Still I can't calculate MSE of these estimator so I don't know which one of these estimators is more efficient",['statistics']
301184,Vector field decomposition into gradient and hamiltonian vector field,"I have just read (without further explanation) that any vector field $(v_x(x,y),v_y(x,y))$ from $\mathbb{R}^2$ to $\mathbb{R}^2$, which has a continuous derivative, can be uniquely written as the sum of a gradient and a hamiltonian vector field, i.e. $$ v_x(x,y) = \partial_xP(x,y) + \partial_y S(x,y) $$ $$ v_y(x,y) = \partial_yP(x,y) -\partial_x S(x,y)$$for $P$,$S$ : $\mathbb{R}^2 \to \mathbb{R}$. If this is true, how can I show it? It seems non-obvious to me. Thanks!",['differential-geometry']
301194,"Given a data set, how do you do a sinusoidal regression on paper? What are the equations, algorithms?","Most regressions are easy. Trivial once you know how to do it.  Most of them involve substitutions which transform the data into a linear regression.  But I have yet to figure out how to do a sinusoidal regression.  I'm looking for the concept beyond the results. I don't need Excel, TI, or CAS answers. I would like to see equations, methods, so on. How would you do it on paper if you were actually willing to do it on paper. For clarification, I'm referring to the most general sinusoidal regression $$ y = A\sin(Bx+C) + D$$ Not some special case assuming the values of one of these constants.","['statistics', 'regression']"
301207,"Why is $P(X,Y|Z)=P(Y|X,Z)P(X|Z)$?","Could anyone derive or explain why the formula $P(X,Y|Z)=P(Y|X,Z)P(X|Z)$ is true? I understand conditional probability definition, but this formula confuses me and makes my head hurt x) Here's another similar which I have struggled to understand: $$p(\mathbf{x}, \mathbf{\theta}|\mathcal{X})=p(\mathbf{x}|\mathbf{\theta},\mathcal{X})p(\mathbf{\theta}|\mathcal{X})$$ Could someone explain this one to me as well? This formula is from my neural networks book, but I have no idea why this is true, even though I understand the basic conditional probability formula $P(A|B) = \displaystyle\frac{P(A,B)}{P(B)}$. If I use this formula, what I would do for my book example is this: $P(\mathbf{x},\mathbf{\theta}|\mathcal{X}) = \displaystyle\frac{P(\mathbf{x},\mathbf{\theta},\mathcal{X})}{P(\mathcal{X})}$ Could someone ease my frustration ;D Thank you!","['bayesian', 'probability']"
301230,Bounding $\liminf_{n} n |f^n(x)-x|$,"I solved an exercise in which the first part asks to prove that for any measure preserving measurable transformation $f:[0,1]\rightarrow [0,1]$ we have
$$\liminf_{n} n |f^n(x)-x| \leq 1, \ \mbox{a.e.}$$ I can't prove the second part of the exercise: Let $\omega=(\sqrt{5}-1)/2$ and let $f:[0,1]\rightarrow[0,1]$ defined as $f(x)= (x+\omega) \pmod{1}$. Use this transformation to prove that there is no $c<\frac{1}{\sqrt{5}}$ such that
$$\liminf_{n} n |f^n(x)-x| \leq c$$ Thank you guys in advance!
(I'm sorry about the mistakes!)","['measure-theory', 'analysis']"
301236,"Why isn't the union of the coordinate axes a manifold, using the formal definition of a manifold?","Let me call $M$ a manifold iff locally it is given as the graph of a $C^1$ function. Then without appealing to ""remove a point"" type arguments, why isn't the union of the $x$ and $y$ coordinate axes (the solution set of $xy=0$) a manifold? That is, why isn't this locally a graph around the origin?",['multivariable-calculus']
301245,How many square tiles can fit in an area of $1080 \times 1920$?,"What is the largest size that 35 square tiles can be, which fit in an area of $1080\times $1920?",['geometry']
301263,Commutation matrix proof,Prove that each commutation matrix $K$ is invertible and that $K^{-1} = K^{T}$ We found that $K$ is a square matrix and because we assume that $K$ only has distinct elements it has the maximal rank and is therefore an invertible square matrix. We don't know how to prove the last part. Could someone please help?,['matrices']
301264,Topology Prerequisites for Algebraic Topology,"Note: There is another question of the same title, but it is different and asks for group theory prerequisites in algebraic topology, while i want the topology prerequisites. I am a physics undergrad, and I wish to take up a course on Introduction to Algebraic Topology for the next sem, which basically teaches the first two chapters of Hatcher, on Fundamental Group and Homology. However, I don't have a formal mathematics background in point-set topology, and I don't have enough time to go though whole books such as Munkres. So What part of point set topology from Munkres is actually used in the first two chapters of Hatcher? More importantly, I wanted to know if the first chapter of the book Topology, Geometry and Gauge Fields by Naber or first 2 chapters of Lee's Topological Manifolds would be sufficient to provide me the necessary background for Hatcher. Thanks in advance!","['general-topology', 'algebraic-topology', 'soft-question']"
301273,small rank solution of a matrix equation,"Consider the matrix equation
$$AX-XA = R$$
where $A$ and $R$ are given square matrices such that $\operatorname{rank}(R)=r$. How to establish conditions (necessary, sufficient, or both) on $A$ and (or) on $R$ which ensure that there exists a solution $X$ of rank smaller or equal than $r$? Some observations: The equation involves the commutator $[A,X]=AX-XA$ between $A$ and $X$, thus the solution can not be unique. In fact if $X_0$ is a particular solution, than all the possible solutions are of the form
$$X_0 + B$$
where $B$ is a matrix which commutes with $A$, i.e. $[A,B]=O$. If we assume that $A$ is normal with simple eigenvalues, then a unitary $U$ and an invertible diagonal $D$ exist such that $A=UDU^*$. Therefore, we can vectorize the equation as
$$(U\otimes U)(D\otimes I -I \otimes D)(U\otimes U)^* vec(X)=vec(R)$$
The $n^2 \times n^2$ diagonal matrix $D\otimes I -I \otimes D$ has exactly $n$ zero eigenvalues. Does this ""vectorized"" equation ($n^2$ linear system) give us some more information? What can we ask about $D$  to ensure that a solution $vec(X)$,  or equivalently $(U\otimes U)^*vec(X)$, has rank less or equal than $r$?","['numerical-linear-algebra', 'matrices', 'linear-algebra']"
301289,Spinor Mapping is Surjective,"I'm (still) trying to prove that $SL(2,\mathbb{C})$ is the universal covering group the the proper orthochronous Lorentz group $L$. I have completed the following steps. (1) Prove that the vector space of $2\times 2$ Hermitian matrices $H$ is isomorphic to Minkowski space. (2) Demonstrate that the action of $SU(2)$ on $H$ by $X\mapsto AXA^{\dagger}$ induces a group homomorphism $SL(2,\mathbb{C})\to L$. (3) Prove this is 2:1 by observing that every $2\times 2$ complex matrix can be written as $X+iY$ with $X$ and $Y$ Hermitian. I still need to prove that this map is surjective though. Here I am completely stuck. All the books and internet resources I have found either gloss over it, or state that it's true but don't prove it. Could someone possibly give me a proper proof, with mathematician's rigour?! P.S. My own attempts at a proof have fallen down. I tried the following (a) Derive a formula for the inverse map locally. I can't see any good way to attack this though. (b) Prove that the associated Lie algebra homomorphism is an isomorphism. I know theoretically this should be possible, but practically it seems a nightmare!","['lie-algebras', 'spin-geometry', 'representation-theory', 'matrices', 'lie-groups']"
301300,What is the difference between half space and hyper plane?,I read about half space and hyper plane and keep getting confused about which is which and how people are using it. I would really appreciate if somebody can give me an example in simple language over the math one written on wikipedia. Half Space I made a mistake of writing half planes instead of hyper plane. Corrected it.,['geometry']
301309,How to calculate basis of kernel?,I have a linear transformation. The transformation and what I tried is written on the attached work page. Is my way wrong? what is the basis of KerT? LinearAlgebra: S -> S is a joke with my friends. sorry for this.,['linear-algebra']
301315,Invertibility of a Kronecker Product,Prove that $A\otimes B$ is invertible if and only if $B\otimes A$ is invertible. I don't have a clue where to start to be honest. I am not very familiar yet to the Kronecker Product so could you please help me with providing an easily understandable proof. Thanks in advance.,"['kronecker-product', 'matrices', 'linear-algebra']"
301319,Derive a rotation from a 2D rotation matrix,"I have a rotation 2D rotation matrix. I know that this matrix will always ever only be a rotation matrix. $$\left[
\begin{array}{@{}cc}
\cos a & -\sin a \\
\sin a & \cos a \\
\end{array}
\right]$$ How can I extract the rotation from this matrix? The less steps, the better, since this will be done on a computer and I don't want it to constantly be doing a lot of computations!","['matrices', 'linear-algebra', 'rotations']"
301344,"Can any subset of $\mathbb{R}^2$ be expressed in form $A\times B$, where $A$ and $B$ are subsets of $\mathbb{R}$?","This is a very elementary question I'm a little confused about. Can any subset of $\mathbb{R}^2$ be expressed in form $A\times B$, where $A$ and $B$ are subsets of $\mathbb{R}$? I'm thinking that it might not necessarily be so. For instance, if we consider the intervals $[0,2]$ and $[3,5]$ in $\mathbb{R}$, and throw out a single point, say $(1,4)$, out of $[0,2]\times [3,5]$ then there do not exist subsets $A$ and $B$ of $\mathbb{R}$ such that $A\times B = ([0,2]\times [3,5]) \setminus \{(1,4)\}$. Because if they were equal, surely $A$ would be equal to $[0,2]$ and $B$ would be equal to $[3,5]$. But as there is no way to put the constraint that $1$ cannot pair up with $4$, $A\times B$ will always be $[0,2]\times [3,5]$. Am I thinking in the right direction? Thanks.",['elementary-set-theory']
301364,What is a diffeomorphism?,"I'm looking for a simple (i.e. as just a rough outline with little differential geometry) definition/ explanation of what a diffeomorphism is. I tried reading the Wiki page but it made no sense to me as a physicist. To give some context, I'm reading a book which says that because some property of a tensor field $h_{\mu\nu}(x)$ is preserved under infinitesimal transformation of coordinates by the fields $\xi_{\mu}(x)$, then these diffeomorphisms are a symmetry of the theory.",['differential-geometry']
301402,Suspension of a product - tricky homotopy equivalence,"Let $(X,x_0), (Y,y_0)$ be well-pointed spaces (inclusion of the basepoints is a cofibration). Show the following homotopy equivalence
$$
\Sigma (X\times Y) \simeq \Sigma X \lor \Sigma Y \lor \Sigma (X\land Y),
$$
where $\Sigma$ means a suspension (or reduced suspension if one prefers since it doesn't matter for well-pointed spaces) and $\land$ is a smash product . Assuming we are using reduced suspension, it is quite clear that $\Sigma X \lor \Sigma Y$ is a subspace of the lhs, but I don't know how to somehow pull it outside and get $\Sigma (X\land Y)$. Edit: I know the homotopy equivalence can be deduced from general theorems on ""homotopy functors"" (I hope that's how they are called in English) from chapter 7.7 in Spanier. But I was told there is an explicit proof and that's the one I'm looking for.","['general-topology', 'algebraic-topology']"
301405,What's the derivative of order 2013 of $f(x)=\frac{x^5}{1+x^6}$?,"What's the derivative of order 2013 of $f:\mathbb R\to \mathbb R$, $f(x)=\frac{x^5}{1+x^6}$ at the point 0? This question seems really difficult to solve directly, I need some others ideas how to solve it. I need help! thanks a lot.","['derivatives', 'real-analysis']"
301439,Describe tangent and normal bundle to a manifold,"Consider the set $X:=\{(x,y,z)\in\mathbb{R}^3 | x^2+3y^2=1+z^2\}$ I have to show that $X$ is a submanifold of $\mathbb{R}^3$ (and this is trivial); then, using on $T\mathbb{R}^3$ the standard coordinates $x,y,z,\partial_x,\partial_y,\partial_z$ I have to find the equations that describe $TX\subseteq T\mathbb{R}^3$ and $NX\subseteq T\mathbb{R}^3$. Any suggestion? Thanks...","['differential-topology', 'manifolds', 'vector-bundles', 'differential-geometry']"
301444,Is it really legitimate to denote non-existence of an element as that element being an element of the empty set?,"My question is simple, but it bugs me... I've seen many people using e.g. $a\in\emptyset$ to state that element $a$ does not exist... Is this legitimate? And why? I thought nothing can be an element of the empty set, by definition.",['elementary-set-theory']
301450,How to bound the maximal consecutive length in a random subset of [n] as function of n?,"Let $S$ be a random subset of $[n]=\{1,2,\ldots,n\}$ chosen uniformly from $[n]$'s subsets. How can I find a function $f(n)$ s.t. for any $\varepsilon \gt 0$, $$\lim_{n \rightarrow \infty} P\left[(1- \varepsilon) f(n) \le \lambda (S) \le (1 + \varepsilon)f(n)\right]=1 ?$$","['probability-theory', 'probability', 'functions', 'limits']"
301456,Derivative of matrix inverse,I am trying to find the derivative of a matrix with respect to the inverse of the same matrix.  The matrix in question is a non singular symmetric matrix. Any thoughts?,"['matrices', 'inverse', 'calculus']"
301480,Measurable Partition and Ergodic Decomposition,"I need some background before asking the question: Let $\mathcal{P}$ is called a measurable partition if there is a measurable set $M_0\subset M$ with full probability measure such that, restric to $M_0$
$$\mathcal{P}=\bigvee_{n=1}^\infty \mathcal{P}_n$$
for some crescent sequence $\mathcal{P}_1 \prec \dots \prec \mathcal{P}_n \prec \dots$ of enumerable partitions, where $\mathcal{P}_i\prec \mathcal{P}_{i+1}$ means that every element of $\mathcal{P}_{i+1}$ is contained in some element of $\mathcal{P}_i$ (we say that $\mathcal{P}_i$ is less thin than $\mathcal{P}_{n+1}$) and also $\bigvee_{n=1}^\infty \mathcal{P}_n$ is the thinnest such that
$$\mathcal{P}_n \prec \bigvee_{n=1}^\infty \mathcal{P}_n, \forall n$$ It's elements are of the form $\cap_{n=1}^\infty P_n$, where $P_n \in \mathcal{P}_n$. Questions: i) Show that a partition $\mathcal{P}$ is a measurable partition iff there exists measurable subsets $M_0, E_1, E_2,\dots,E_n,\dots$ such that $\mu(M_0)=1$ and, restrict to $M_0$, 
$$\mathcal{P}= \bigvee_{n=1}^\infty \{E_n, M\setminus E_n\}$$ ii) If $\mu$ is an ergodic probability for a transformation $f$ and $k\geq2$. How is the ergodic decomposition of $f^k$?","['dynamical-systems', 'ergodic-theory', 'measure-theory', 'probability-theory', 'probability']"
301490,Find an expression for the number of edges of $L(G)$ in terms of the degrees of the vertices of $G$.,"I have a hard time trying to understand this prove. Find an expression for the number of edges of $L(G)$ in terms of the degrees of the vertices of $G$. Let $\{v_1, v_2, . . . , v_n\}$ be the vertices of $G$ and let $d_i$ be the degree of the vertex $v_i$. An edge ${v_i, v_j}$ will be adjacent to $d_i − 1 + d_j − 1$ edges. Until this point I understand it. Since there are $d_i$ edges that contain $v_i$ in $G$, the sum of the degrees of the vertices in $L(G)$ will be $\sum_{i=1}^n d_i(d_i − 1)$ and so the number of edges in $L(G)$ is $\sum_{i=1}^n \frac{d_i(d_i − 1)}{2}$. From: http://garsia.math.yorku.ca/~zabrocki/math3260w03/hw1sln.pdf","['graph-theory', 'discrete-mathematics', 'combinatorics']"
301504,On surjectivity of exponential map for Lie groups,"A recent question made me realize I didn't know any proof that exponential of a Lie algebra $\mathfrak g$ of a compact connected Lie group $G$ is surjective. After a bit of thinking I've come up with two proofs. First is based on relation between exponential curves and geodesics. This is rather technical but also gives us other useful information. This is not what I want to talk about here though. The second proof (which I find slicker) is based only on topology and goes like this:
Since $\exp$ is a local homeomorphism, it is both open and closed. Therefore $\exp(\mathfrak g)$ is clopen and so equal to $G$. The trouble with this ""proof"" is that it also proves the statement for $G$ non-compact (which is false). So I wonder (and this is my question) what precisely went wrong. Can the above mentioned ""proof"" be made into a real proof? My thoughts on this are that $\exp$ is closed and open only when $G$ is compact because then we can pick a bounded open subset $C \subset \mathfrak g$ such that $\exp(C) = G$  and we can use the relation $\exp(A+\epsilon B) \approx \exp(A)\exp(\epsilon B)$ to conclude that $\exp$ is a local homeomorphism everywhere in $C$ (not just around $0$). This implies that $\exp$ is open (since it is locally open) in $C$. Also, since any closed subset of $C$ is compact, it's image is also compact and therefore closed in $G$. Where exactly does this argument break down when $G$ is not compact.","['general-topology', 'lie-algebras', 'lie-groups', 'differential-geometry']"
301520,d'alembert's formula,"I'm studying the Cauchy problem for the wave equation $n=2$;
$$\begin{cases}u_{tt}=\alpha^{2} u_{xx}, x \in\mathbb{R}, t>0\\[8pt]
u(x,0)=f(x), x\in\mathbb{R}\\[8pt]
u_{t}(x,0)=g(x), x\in\mathbb{R}
\end{cases}$$ By d´Alembert's formula we know that $u(x,t)=\frac{f(x+\alpha t)+f(x-\alpha t)}{2} +\frac{1}{2\alpha}\int^{x+\alpha t}_{x-\alpha t}g(s)ds$ is only solution of the above problem. But in the proof, the uniqueness of $u$, second my book reference, is given by the uniqueness of the functions $f(x)\in C^{2}(\mathbb{R})$ and $g(x)\in C^{1}(\mathbb{R}).$ I do not understand this statement! 
Can anyone help me?
thank you very much.","['partial-differential-equations', 'analysis']"
301533,jacobian involving SO(3) exponential map: $\log(R \exp(m))$,"I would like to compute the 3 × 3 Jacobian of $$
\log(R \exp(m))
$$ with respect to the 3-vector $m$, evaluated at $m=0$. In the above, $\exp$ is the exponential map from so (3) to SO(3), $\log$ is the inverse of the exponential map, and $R$ is a constant 3 × 3 rotation matrix. I understand that the inverse of the exponential is not well-defined everywhere. Is there an expression for this Jacobian that holds almost everywhere?","['exponentiation', 'lie-algebras', 'lie-groups', 'differential-geometry']"
301536,Geodesic of a Surface in $\mathbb{R}^3$,"I'm not familiar with geodesics. How can I show that a curve $c$ given by 
$c(t)=(t,f(t)\cos{\alpha},f(t)\sin{\alpha})$ for $\alpha$ constant is a geodesic on $M$ where
$M=\left\{(x,y,z) \in \Bbb{R}^3 \mid f(x)=y^2+z^2\right\}$?","['surfaces', 'differential-geometry', 'geodesic']"
301538,Proving the size of the unions of sets,"How should one go about proving the following with induction? $$ \left| \bigcup_{i \in I} A_i \right| = \sum_{J \subseteq I} (-1)^{|J|+1} \left|\bigcap_{i \in J} A_i \right| $$ I is just a finite set, and $A_i$ is just any set within it.","['induction', 'elementary-set-theory']"
301562,prove $n$-cube is bipartite,prove $n$-cube is a bipartite graph for all $n\ge1$ This is a problem in my textbook and I cannot figure it out at all and have a test on graph theory tomorrow any help would be appreciated since I am not very good with proofs,"['graph-theory', 'discrete-mathematics', 'combinatorics']"
301564,Bootstrap-related issue,Say I re-sample $N$ items with replacement from a numbered item sample of size $N$. What is the average number of data items that are not selected in each such sampling?,['statistics']
301573,contour integration of logarithm,"I must compute the following integral $$\displaystyle\int_{0}^{+\infty}\frac{\log x}{1+x^3}dx$$ Can someone suggest me the right circuit in the complex plane over which to do the integration? I tried different paths, avoiding the origin, but unsuccessfully","['complex-integration', 'complex-analysis', 'contour-integration']"
301602,Isomorphism between tensor product and quotient module.,"Given a commutative ring $R$ with $1$ , an ideal $I$ and an $R$ -module $A$ , how can I show that the homomorphism $$f: R/I \otimes_R A \to A/IA$$ given by $f((r +I) \otimes a) = ra + IA$ is injective? I can easily see that it's surjective. (We define $IA$ to be the submodule of $A$ generated by elements of the form $ia$ , $i \in I, a \in A$ .)","['modules', 'tensor-products', 'abstract-algebra']"
301606,Finding the MVUE using Rao-Blackwell Theorem,"The number of breakdowns Y per day for a certain machine is a Poisson random variable with mean $\lambda$. The daily cost of repairing these break downs is given by $C=3Y^2$ If $Y_1, Y_2, ..., Y_n$ denote the observed number of breakdowns for $n$ independently selected days find an MVUE for $E(C)$. We can use the Rao-Blackwell Theorem. We know that $E(C) = E(3Y^2)=3[V(Y) + (E(Y))^2]$ and $E(Y)=\lambda=V(Y)$. With some calculations we see that $E(Y^2)= \lambda + \lambda^2$ $\sum_{i=1}^n Y_i=\bar Y$ is a sufficient statistics for $ \lambda$  So I am assuming we can replace $\lambda$ with $\bar {Y}$ I am unsure where to go from here. Can someone help me pull the strings together?",['statistics']
301609,Pedagogy of Teaching the Inverse Matrix Method,"I am teaching a group of (ordinary rather than honours) second-year engineers and we are studying matrices. I told the class today that as far as I could see we were only studying matrices and, particularly, the inverse matrix method as an introduction to more advanced matrix methods that would be studied in future. However, the maths modules that they take in their next, final, third year are differential equations (no linear systems of differential equations) and, well, probability and statistics. The only use that I can see that this group have for matrices is for solving linear systems. I know that there are plenty of more reasons to study matrices and in particular matrix inverses but this cohort will not see them. It obviously strikes me as odd that the syllabus would recommend that we use the Inverse Matrix Method rather than the full Gaussian elimination theory. Therefore my question is: Assuming that we want to solve a linear system $A\mathbf{x}=\mathbf{b}$, what advantages, if any, does the inverse matrix method have over the full Gaussian elimination theory. Thank you in advance for any answers; I am struggling to find one!","['matrices', 'linear-algebra', 'education']"
301614,Euler characteristic of a union of planes,"Let $P_1 ,...,P_5$ be mutually distinct planes in $\Bbb{R}^3$ such that: the intersection of any distinct 2 is a line. the intersection of any distinct 3 is a point. the intersection of any distinct 4 is an empty set. Let $X=\bigcup_{i=1} ^5 P_i$ with the topology as a subspace of $\Bbb{R}^3$ What is the Euler characteristic of $X$? I guess I'm getting a bit confused since I usually try to visualize stuff in $\Bbb{R}^3$ but I'm having trouble here. I can only think of a triangle prism or an open box, but in both cases there are planes with no intersection.","['general-topology', 'geometry']"
301615,Prove by Mathematical Induction: $1(1!) + 2(2!) + \cdot \cdot \cdot +n(n!) = (n+1)!-1$,"Prove by Mathematical Induction . . . $1(1!) + 2(2!) + \cdot \cdot \cdot +n(n!) = (n+1)!-1$ I tried solving it, but I got stuck near the end . . . a. Basis Step: $(1)(1!) = (1+1)!-1$ $1 = (2\cdot1)-1$ $1 = 1 \checkmark$ b. Inductive Hypothesis $1(1!) + 2(2!) + \cdot \cdot \cdot +k(k!) = (k+1)!-1$ Prove k+1 is true. $1(1!) + 2(2!) + \cdot \cdot \cdot +(k+1)(k+1)! = (k+2)!-1$ $\big[RHS\big]$ $(k+2)!-1 = (k+2)(k+1)k!-1$ $\big[LHS\big]$ $=\underbrace{1(1!) + 2(2!) + \cdot \cdot \cdot + k(k+1)!} + (k+1)(k+1)!$ (Explicit Last Step) $= \underbrace{(k+1)!-1}+(k+1)(k+1)!$ (Inductive Hypothesis Substitution) $= (k+1)!-1 + (k+1)(k+1)k!$ $= (k+1)k!-1 + (k+1)^{2}k!$ My [LHS] looks nothing like my [RHS] did I do something wrong? EDIT: $ = (k+1)k! + (k+1)^2k! -1 $ $ = (k+1)(k!)(1 + (k+1))-1$ $ = (k+1)(k!)(k+2)-1 = (k+2)(k+1)k!-1$","['factorial', 'induction', 'summation', 'discrete-mathematics']"
301629,Question on a proof about the Rank of a Matrix,"The question is: Give a formal proof for the following statement:
Given a matrix A and a scalar c, show that rank(cA) = rank(A) Here are the steps that I took to go about the proof: (1) Prove this claim: Let v1, v2, ..., vN be vectors then {v1, v2, ..., vN} is linearly independent  <==> {c* v1, c*v2, ..., c * vN} is also lin. ind. I don't type out the whole thing here, but the proof is trivial by playing around with the coefficients (2) Let (c * A_ij) where i = 1, 2, ..., m; and j is fixed where j belongs to {1, 2, ..., n} denotes a  linearly independent column in matrix cA (3) Then I let S = { (c * A_ij)} be the set of all linearly independent columns in matrix cA, where each element of S satisfies (2) (4) By how I define the set S, all elements in S are lin. ind. columns in matrix cA. Then I use the claim (1) to say that columns A_ij of matrix A must also be lin. ind. I also note that by definition of the rank, it's the maximum number of lin. ind. columns (or rows) in a matrix.  So I think rank(cA) is basically the cardinality of the set S.  Then by (4), I conclude that when I ""move"" from each lin. ind. column of matrix cA to each lin. ind. column in matrix A, I didn't change the number of lin. ind. columns.  Thus, rank(cA) = rank(A). Would someone please help me check if there is anything missing or wrong in my proof ?  Somehow I feel a bit shaky on how I define the indices for the linearly independent columns in matrix cA.
Thank you very much ^_^",['linear-algebra']
301632,Create asymmetric 3D function by revolving a 2D function around an axis,"Could someone please let me know how I can construct the equation for asymmetric torus similar to the figure below? The asymmetric torus seems to be a 2D function revolved around an axis while being scaled. I think if I am able to formulate this, I can use the same strategy to revolve a 2D function $\omega_{\phi}\left(x,\sigma\right)$ around an axis, e.g. $x$-axis, and generate a 3D asymmetric function $g\left(x,y,z\right)$. The formula that I want to revolve while altering is: In fact, the asymmetric torus is the iso-surface of $\omega$ after being revolved in three dimensions. In two-dimensions, i.e. the plane $\left(x,\sigma\right)$, contours of $\omega$ is almost a circle. Could someone help me?","['geometry', 'calculus', 'functions']"
301662,Funny Trig Math Puzzle,"This is a challenging puzzle I heard from my little brother. For some $n$ and $x$, $\sum_{k=1}^n \sin^{2k}(x) = 2013$. Is it possible to deduce
$$\sum_{k=1}^n \cos^{2k}(x) \text{ ?}$$ Edit:
I've just noticed something which now seems obvious to me. Choose $n = 2013$ and $x = \pi/2$ which satisfies the condtion. It follows that the cosine terms would sum to zero. I'm not sure this is a unique solution.",['trigonometry']
301667,what are the holomorphic curves in $T^{*}S^3$ with boundary on the zero section?,How would I characterize such things? Is the minimal spanning (real) surface of a (real) curve in $S^3$ contained entirely in that $S^3$?,"['algebraic-geometry', 'symplectic-geometry']"
301675,Change of Variable (Double Integral),"I have been trying to integrate the two following integrands; $$\int \int_{D}(x^{2}+y^{2})dxdy$$ where  $D=\{{x^{2}+xy+y^{2}\leq 1}\}$ and $$\int \int_{D}\sqrt{x^{2}+y^{2}}dxdy$$ where $D=\{{x^{2}+y^{2}\leq x}\}$. Now, I have been tempted to use change of variables (polar coordinates) in both cases. For example, in the first case, I completed the square so:  $$D=\{{(x+1/2y)^{2}+3/4y^{2}\leq 1}\}$$ I then set $u=r\cos{t}-(1/2)r\sin{t}$ and $v=r\sin{t}$, performed partial differentiation, formed the jacobin matrix, and calculated the determinant. I then set the new boundaries  $0\leq r\leq$1 and $0\leq t \leq 2\pi$ before calculating the whole expression. Now, the answer I obtain is $(53\pi)/96$, which is blatantly wrong as the textbook gives $(4\pi)/(3\sqrt{3})$. Since my approach to the second problem is similar, I fear, that the answer I obtain there is wrong as well. I would be exceedingly grateful if you could help me.","['multivariable-calculus', 'integration']"
