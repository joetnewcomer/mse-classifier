question_id,title,body,tags
3256541,Example of a strong b-metric which is not a metric.,"Definition: Let $X$ be an arbitrary set, $d:X\times X\to [0,\infty)$ be a mapping satisfying: (a) $\forall_{x,y\in X}\; d(x,y)=0\iff x=y$ ; (b) $\forall_{x,y\in X}\; d(x,y)=d(y,x)$ ; (c) $\exists_{s\geq 1}\;\forall_{x,y,z\in X}\; d(x,y)\leq d(y,z)+sd(x,z)$ . Then $d$ is called a strong b-metric and $(X,d)$ is called a strong b-metric space . Naturally, every metric space is a strong b-metric space as it fulfills (c) with $s=1$ (the classic triangle inequality). Are there any natural examples of strong b-metric spaces which are not metric spaces? Note that every finite set $X$ equipped with a mapping $d$ fulfilling (a) and (b),  fulfills (c) as well. Hence we are interested in examples on an infinite set $X$ .","['general-topology', 'metric-spaces', 'real-analysis']"
3256556,"Solve for integer $m,n$: $2^m = 3^n + 5$","Solve for integer $m$ and $n:$ $$2^m = 3^n + 5$$ My Attempt: Easy to guess two solutions namely $(3,1)$ and $(5,3)$ .
Also easy to see that a solution will exist iff $m > 0$ and $n > 0$ . Rewriting it as $2^m - 2 = 3^n + 3$ we get $2^m = 2 \mod 3 \Rightarrow m = 1 \mod 2$ and $3^n = 1 \mod 2 \Rightarrow n = 1 \mod 2$ , hence $m$ and $n$ are both odd.
Beyond this, I could not figure out what approach to use. Source: Past IMO shortlisted problem.","['contest-math', 'divisibility', 'number-theory', 'elementary-number-theory', 'discrete-mathematics']"
3256559,Elementary proof of Bhaskara I's approximation: $\sin\theta=\frac{4\theta(180-\theta)}{40500-\theta(180-\theta)}$,"This site has seen umpteen questions about efficient ways to calculate the sine of an angle. But a remarkable formula was given in ~CE 615 by the Indian mathematician Bhaskara I in his Mahabhaskariya . The formula goes as follows: $$sin\,\theta=\frac{4\theta(180-\theta)}{40500-\theta(180-\theta)}$$ The remarkable things about this formula: Apparently derived using only geometry because it occurs in the sections on geometry in Mahabhaskariya and other books that follow. Maximum error of 0.92% occurs at $\theta=10^\circ$ . A simpler formula than this cannot be found without introducing higher order polynomials. The challenge to all interested folks is: Imagine yourself armed with only pen and paper. How can you derive the above formula by using geometry and elementary mathematics? We ask this question since Bhaskara I does not give in his work how he arrived at this formula nor do his successors! Wikipedia does give some relations https://en.wikipedia.org/wiki/Bhaskara_I%27s_sine_approximation_formula based on Prof. R.C. Gupta's paper, but can anyone better it? https://web.archive.org/web/20120316083451/http://www.new.dli.ernet.in/rawdataupload/upload/insa/INSA_1/20005af0_121.pdf Anyone who can give any decent (meaningful, not silly.. sorry unable to quantify what decent or meaningful means, but each poster can have his or her own quality check for their answer before posting) answer to this gets a chocolate. 
As an aside, can a simpler than this formula be derived for sine?",['trigonometry']
3256578,"Finding $\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2}$","Find bounds for $\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2}$ I am finding maximum and minimum for function and one of critical case is to find possible minimal and maximal value of given function in $0,0$ . But how can I do this due to this limit doesn't exists (for example we can take $x,y = {1\over n},{1\over n}$ and $x,y = {2\over n},{1 \over n}$","['limits', 'real-analysis']"
3256594,Is there an analytic solution to this equation? $a+b\sin(x)+c\tan(x)=0$,"Here is the equation which should be solved for $x$ : $$
a+b\sin(x)+c\tan(x)=0
$$ where $a$ , $b$ , and $c$ are constants. Is there an analytic solution, or at least an algorithm to approximate it for any given set of constants?",['trigonometry']
3256605,Topology on set of maps from $A$ to $B$,"Let $B$ be a non-empty set equipped with the discrete topology, and let $A$ be an infinite set. Then $B^A$ is the set of all functions $f:A\to B$ . I have to verify some elementary properties of the product topology that $B^A$ inherits: Prove that $U\subset B^A$ is open iff for all $f\in U$ there exists a finite $E\subset A$ such that all maps $g:A\to B$ with $g|_E=f|_E$ belong to $U$ . I don't know how to check this..
I know I should think of $B^A$ as $\prod_{a\in A}B_a$ where $B_a=B$ are copies of $B$ for all $a\in A$ . The basis open sets are then $\prod_{a\in A}U_a$ where $U_a=B_a$ for all but finitely many $a$ . Since $B$ has the discrete topology the $U_a$ which are not $B_a$ can be an arbitrary subset of $B$ . Then the open sets are unions of the basis open sets. But how does this give me the statement? Can someone provide any help?",['general-topology']
3256616,The Kernel of a Homomorphism,""" The kernel is important because it controls the entire homomorphism. It tells us not only which elements of G are mapped to the identity in G', but also which pairs of elements have the same image in G'.  "" (Taken from Algebra by Artin.) How does the kernel tell us which pairs of elements have the same image in G'? What I tried: let $f$ be a homomorphism such that $f(a) = f(b) = c$ . $c$ is in $G'$ , so it has an inverse $c^{-1}$ . So then, $c^{-1}f(a) = c^{-1}f(b) = cc^{-1} = 1$ . But, this is tedious to compute, and I didn't use the kernel to do it.","['group-homomorphism', 'group-theory', 'abstract-algebra']"
3256631,prove that a process is an MA(q),"We are given the following polynomial : $$\Theta(z) = 1 + \theta_1 z + ...+ \theta_q z^q  $$ and we suppose that $(z_t)_{t \in \mathbb{Z}}$ is weakly stationnary and its spectral measure is given by : $$ \mu_z(d\omega) = |\Theta(e^{i\omega})|^2\sigma^2/2\pi d\omega$$ and that $\Theta$ has no root in the unit circle. We want to prove that $(z_t)$ is an $MA(q)$ . I used the inverse filtering theorem to show that there exists $(\epsilon_t)_t$ weakly stationnary such that $z_t = \Theta(L) \epsilon_t $ where L is the backward shift operator. Indeed $\sum_{-\infty}^{\infty} |\theta_i| \lt \infty $ and $\int_{-\pi}^{\pi}\frac{1}{|\Theta(e^{-i\omega})|^2}\mu_z(d\omega) < \infty$ . Furthermore, $\gamma_{\epsilon}(h) = \sigma^2 \mathbb{1}_{h=0}$ but i have yet to prove that the expectation of $(\epsilon_t)$ is null, and i have no clue on how to do it. Also, i don't see where we use the hypothesis that $\Theta$ has no root in the unit circle since $|\Theta(e^{-i\omega})|^2$ is contained in the spectral measure of $z_t$ and it simplifies inside the integral which makes it converge. So it seems to me that this hypothesis is not necessary.
Thank you for your help","['time-series', 'stochastic-processes', 'statistics']"
3256646,How do I find the range of a difficult function?,"I find it really hard to find the range. I usually substitute the x's with y and then solve for y, but it does not always work for me.  Do you have any advice? Function in question: $$f(x) = \frac{e^{-2x}}{x}$$",['functions']
3256741,Finite set of linear functional attaining certain values (Brezis functional analysis exercise 1.12),"Problem: Let $E$ be a vector space. Fix $n$ linear functionals $(f_{i})_{1\leq i\leq n}$ on $E$ and $n$ real numbers $(\alpha_{i})_{1\leq i\leq n}.$ Prove
that the following properties are equivalent. There exists some $x\in E$ such that $f_{i}(x)=\alpha_{i}$ , $\forall i=1,...,n$ . For any choice of real numbers $\beta_{1},\beta_{2},...,\beta_{n}$ such that $\sum_{i=1}^{n}\beta_{i}f_{i}=0$ , one also has $\sum_{i=1}^{n}\beta_{i}\alpha_{i}=0$ . Attempt $(1)\Rightarrow(2)$ This follows from definition. By assumption we can find some $x_{0}\in E$ such that $f_{i}(x)=\alpha_{i}$ for any $i=1,...,n$ . Then for any
choice of $\beta\in\mathbb{R}^{n}$ with $\sum_{i=1}^{n}\beta_{i}f_{i}=0,$ we plugin $x_{0}$ and get \begin{equation}
\sum_{i=1}^{n}\beta_{i}f_{i}(x_{0})=\sum_{i=1}^{n}\beta_{i}\alpha_{i}=0.
\end{equation} $(2)\Rightarrow(1)$ I am not sure how to show this. I think the way to go is by argue by contradiction. But I am not getting anywhere but a circular argument. Another way to look at this is that if we assume that if $\cap_{i=1}^{n}\ker{f_i}\subset\ker{g}$ , where $g:\mathbb{R^n}\rightarrow\mathbb{R}$ defined map $x\mapsto a^T x$ . Then there exists some $x\in E$ such that $f_i(x)=\alpha_i$ . Maybe Hahn Banach in its geometric form could be of use here. But I fail to see where to use it.","['convex-analysis', 'functional-analysis', 'analysis']"
3256743,Show image of $\mathcal{F}_s$ under $\Psi$ is also Killing,"Prescribe a map: $$\Psi:\zeta^2 \to \Bbb T^2$$ which gives a transformation of $\zeta$ -space to the ""square"" flat torus , by identifying $(x,y)\sim (x+1,y)\sim(x,y+1).$ Let $(\zeta^2,g)$ with $g=\frac{dxdy}{xy}$ for $x,y \in (0,1).$ Then we have a Cauchy foliation of $\zeta^2$ defined by: $$ \mathcal{F_s}=\big\lbrace \log x \log y=s: s>0\big\rbrace $$ and a flow tangent to $\mathcal {F}_s$ which is Killing. Is it possible to impose conditions s.t. the image of $\mathcal{F}_s$ under $\Psi$ is also Killing?","['vector-fields', 'semi-riemannian-geometry', 'general-topology', 'soft-question', 'differential-geometry']"
3256758,Lawvere - Conceptual Mathematics - Criterion for Determination of Maps,"This is a problem from Lawvere's Conceptual Mathematics, in a section talking about determination problems and retraction maps. Suppose two maps of sets $f: A \to B$ , $h: A \to C$ satisfy the condition: If $f(a_1) = f(a_2)$ , then $ h(a_1) = h(a_2)$ . Must there be a map $g: B \to C$ with $h = g \circ f$ ? I think the answer is yes, because $g$ should essentially be determined by $h$ . We know for any $a \in A$ there is an element $b \in B$ such that $f(a) = b$ , and we know there is an element $c \in C$ with $h(a) = c$ , so I think we should just define $g(b) = c$ . Such a $g$ is well defined because, by our assumption, if we have $f(a_1) = f(a_2)$ then $h(a_1) = (g \circ f)(a_1) = (g \circ f)(a_2) = h(a_2)$ . That proof seems hand-wavy, and too easy. Is there a better way to put the argument (assuming it is true)?","['functions', 'proof-verification']"
3256841,Sum $\sum\limits_{n=1}^\infty\frac{H_n^2}{n^22^n}$,"Where $ H_n$ is the harmonic number, $\ \displaystyle H_n=1+\frac12+\frac13+...+\frac1n$ . I am going to present my solution as I need it as a reference. Other approaches are appreciated. here is the closed form $$\sum_{n=1}^\infty\frac{H_n^2}{n^22^n}=-\frac1{24}\ln^42+\frac14\ln^22\zeta(2)-\frac74\ln2\zeta(3)+\frac{37}{16}\zeta(4)-\operatorname{Li}_4\left(\frac12\right)$$","['integration', 'harmonic-numbers', 'calculus', 'polylogarithm', 'sequences-and-series']"
3256867,"$A^X+B^Y=C^Z\pm 1$ Beal's conjecture ""almost"" solutions","Beal's conjecture is a generalization of Fermat's last theorem. Fermat's last theorem states that there are no solutions to the equation $A^N+B^N=C^N$ where $A,B,C,N\in \Bbb{N}\space |\space N\ge 3$ In Beal's conjecture the exponents are allowed to vary independently, but there is an added restriction that the bases are coprime. So Beal's conjecture states that there are no solutions to the equation $A^X+B^Y=C^Z$ where $A,B,C,X,Y,Z\in\Bbb{N}\space |\space X,Y,Z \ge 3$ and $gcd(A,B)=gcd(B,C)=gcd(A,C)=1$ (As far as I'm aware this has not been proven or disproven yet.) At one point Ramanujan looked into Fermat's last theorem he created generating functions which gave an infinite number of near miss solutions to Fermat's last theorem they are of the form $A^3+B^3=C^3\pm 1$ here are the generating functions and a few examples My question: is there at least one near miss solution to Beal's conjecture? $$A^X+B^Y=C^Z\pm 1 \quad \text{{1}}$$ where $A,B,C,X,Y,Z\in\Bbb{N}\space |\space X,Y,Z \ge 3$ and $gcd(A,B)=gcd(B,C)=gcd(A,C)=1$ To be clear one near miss solution is sufficient. A generating function isn't necessary. (Although it would be extremely cool if someone finds one.) Every equation which consists only of integer terms has an even number of odd terms. There can't be zero odd terms in {1} because $\pm 1$ is odd. There can't be two odd terms in {1} because that implies that there are two even terms and therefore one of the three gcd pairs isn't one. This leaves the only possibility that all terms in {1} are odd. Only odd bases can generate odd power numbers so $A$ , $B$ , and $C$ are all odd. I have found two cases where all conditions are met except for coprime condition $3^3+15^4=37^3-1$ and $7^4+19^3=21^3-1$ I also found another case where the power numbers meet all conditions except for the $\pm 1$ and was only two off $31^3+63^3=23^4-3$ If there is a counter example one of the terms in {1} must be above $10^6$ Edit: I Realize that I should have stated this from the beginning. I want to avoid really trival counter examples so $A\neq 0, B\neq 0, C\neq 0, C\neq 1$ . One of $A$ or $B$ can equal one but if this is the case the equation of the form $A^X+B^Y=C^Z+1$ cannot be used. If one of $A$ or $B$ is one then only the form $A^X+B^Y=C^Z-1$ is allowed. (If one of $A$ or $B$ is one then this would be equivalent to finding a pair of  odd power numbers where both of the exponents are greater than two and the difference between the numbers in the pair is two. I don't think such a pair exists.)","['number-theory', 'perfect-powers', 'diophantine-equations']"
3256903,Why are there $\frac{(A+1)(A+2)(B+1)}{2}$ triangles in this grid?,"Suppose we are to find the number of triangles that exist from the given figure I found one solution that says we let $A$ equal the number of internal lines from the top vertex, $B$ equal the number of internal lines parallel to the base, using the formula below to find the number of $N$ triangles $$N=\frac{(A+1)(A+2)(B+1)}{2}$$ With $A$ equal to 2 and $B$ equal to 3, we get $N=24$ triangles. But can somebody explain why this formula works? How exactly do I derive this?","['algebra-precalculus', 'combinatorics', 'geometry']"
3256935,W_t^3 martingale or not? two arguments puzzle me.,"I want to study whether $W_t^3$ is a martingale or not?  where $W_t$ is the standard Brownian motion. I have method 1 argument, but I also got  second argument which implies different conclusion. Please help me figure out. Method 1 : Using Ito formula, we have $$W_t^3 = \int_0^t 3 W_s^2~d W_s + \int_0^t 3 W_s~ds$$ The first term on the RHS is a Ito integral, thus a martingale. Consider the second term, notice that a stochastic process $(X_t)_{t\geq 0}$ is martingale if and only if for any bounded stopping time $\tau$ , we have $$\mathbf{E}(X_\tau)=0$$ Back to the second term, $$\mathbf{E}\Big( \int_0^\tau W_s~ds  \Big)=\int_0^T\mathbf{E}(W_{s\wedge \tau})~ds~=~0$$ where $T$ is a finite boundedness for $\tau$ . Thus we proved that $\int_0^t 3 W_s~ds$ is also a martingale. This gives that $W_t^3$ is a martingale. Method 2: Let $\tau$ be the first existing time of $W_t$ from the interval $[-1, 2]$ , then from optional sampling theorem, we know $\mathbf{P}(W_\tau = -1) = {2\over 3},~~\mathbf{P}(W_\tau = 2) = {1\over 3}$ .  Suppose $(W_t^3)_{t\geq 0}$ is also a martingale, then we must have $\mathbf{E}(W_\tau^3) = 0 $ . However by computation, $$\mathbf{E}(W_\tau^3) = (-1)\times {2\over 3} + 8\times {1\over 3} \neq 0.$$ This contradiction implies that $W_t^3$ is not a martingale. Please help me figure out what's wrong with the arguments.","['martingales', 'brownian-motion', 'probability']"
3256940,MLE of Negative Binomial distributions of different sizes,"There are two teams that are competing in a series of matches. These matches are a best-of- $x$ format, so after either team wins $\lfloor{\dfrac{x}{2}}\rfloor+1$ the series is over. This is, in essence, a negative binomial distribution. These two teams play a few different matches, with different $x$ values, and I want to know what the MLE is of $p$ , where $p$ is the probability that Team $1$ defeats Team $2$ . Here is what I attempted: Our likelyhood function is as follows: $$
L(k_i, x_i; p) = \prod_{i=1}^n c_ip^{k_i}(1-p_i)^{x_i-k_i}
$$ where $x_i$ notes that match $i$ was a Best-of- $x$ and $k_i$ is the number of matches won by Team $1$ . If Team $1$ won match $i$ , then $c_i = \binom{x-1}{k-1}$ , else $c_i = \binom{x-1}{k}$ . Take the $\log$ of the likelyhood, we find: $$
\log{L} = \sum_{i=1}^n c_i+\log(p)\sum_{i=1}^nk_i + \log(1-p)\sum_{i=1}^n(x_i-k_i)
$$ differentiating this with respect to $p$ , and setting that to $0$ to find the maximum, we get that: $$
0 = \dfrac{\sum_{i=1}^nk_i}{p} - \dfrac{\sum_{i=1}^n(x_i-k_i)}{1-p}
$$ $$
0 = (1-p)\sum_{i=1}^nk_i - p\sum_{i=1}^n(x_i-k_i)
$$ $$
\sum_{i=1}^nk_i = p\left(\sum_{i=1}^nk_i + \sum_{i=1}^n(x_i-k_i) \right) = p\sum_{i=1}^nx_i
$$ which leads us to the final result $$
p = \dfrac{\sum_{i=1}^nk_i}{\sum_{i=1}^nx_i} = \dfrac{wins}{games}
$$ From here I was a little skeptical of this result, so I simulated sets of matches for various probabilities of victory between teams to find the numerical MLE. For example, I did the scenario in which there is are $2$ Best-of- $1$ s, and one Best-of- $5$ . Team $1$ loses the best of $1$ s, but wins the Best-of- $5$ $3$ - $1$ . Numerically, I found the MLE to be $p\approx .65$ , but using the MLE I just found I would get that $$
p = \dfrac{0+0+3}{1+1+4} = \dfrac{1}{2}
$$","['statistics', 'statistical-inference', 'maximum-likelihood', 'optimization', 'probability']"
3256990,$\sqrt{a+\sqrt{a+\sqrt{a+\cdots}}}\;\;(\text{mod}\;p)$,"Fix an integer $a$ , and a prime $p$ . Define $$S(a,p)=\sqrt{a+\sqrt{a+\sqrt{a+\cdots}}}\;\;(\text{mod}\;p)$$ to be the set of all integers $x\in\{0,...,p-1\}$ such that, for some positive integer $n$ , $$x\equiv \sqrt{a+\sqrt{a+\sqrt{a+\cdots + \sqrt{a+x}}}}\;\;(\text{mod}\;p)$$ with exactly $n$ radicals, and where each square root evaluates to a valid modular square root. Some examples: \begin{align*}
S(2,29)&=\{2,3,4,5,7,14,18,20,21,23,28\}\\[4pt]
S(3,11)&=\{0,1,6,8,9\}\\[4pt]
S(4,23)&=\{0,2,8,12,14,19\}\\[4pt]
S(5,7)&=\{4\}\\[4pt]
\end{align*} Based on sample data, I'll pose two conjectures . . . Conjecture $(1)$ : $\qquad$$S(a,p)\ne{\large{\varnothing}}$ , for all $a,p$ . Conjecture $(2)$ : $\qquad$ If $x\in S(a,p)$ , then $x$ satisfies $$x\equiv \sqrt{a+\sqrt{a+\sqrt{a+\cdots + \sqrt{a+x}}}}\;\;(\text{mod}\;p)$$ $\qquad$ with at most $p-1$ radicals. Questions: Can someone prove or disprove either of the conjectures? $\\[4pt]$ Short of that, is there some intuition for or against either of the conjectures? Update: I'm no longer sure about conjecture $(1)$ since it was based on flawed sample data. Also, as noted in the comments, conjecture $(2)$ only holds for odd primes $p$ , but the upper bound is too high. I had a bug in my program -- sorry for the mistakes. I think the idea of iterated modular square roots has potential for some interesting explorations, so I'll leave the question for now. Hopefully after I correct my program, I'll be able to pose revised conjectures that are consistent with sample data. Update # $2$ : Ok, my program is now corrected. Conjecture $(1)$ looks good, but I see it's now been proved. As previously noted, assuming $p$ is odd, conjecture $(2)$ , while true, is too weak. Thanks to everyone for the comments and answers.","['number-theory', 'elementary-number-theory']"
3256994,What's the mistake in $\sin(\frac{a}{2})=\pm\sqrt{\frac{1\pm \cos(a)}{2}}$?,"When I was encountered with the formula of $\sin(\frac{a}{2})$ , I tried to derive it. First, I tried from the formula of $\cos(2a)$ and I successfully did that but after a while I was curious about if that can be derived from sine's double angle formula and for which I came up with the equation above in the title, totally wrong, as that is kind of combination of sine's and cosine's half angle formula. Here is the derivation:- $$\sin(2a)=2\sin(a)\cos(a)$$ or, $$\sin(a)=2\sin(\frac{a}{2})\cos(\frac{a}{2}) $$ or, $$\sin^2(a)=4\sin^2(\frac{a}{2})(1-\sin^2(\frac{a}{2})) $$ or, $$\sin^4(\frac{a}{2})-\sin^2(\frac{a}{2})=-\frac{\sin^2(a)}{4}$$ which by completing the square or by applying quadratic formula I got:- $$\sin^2(\frac{a}{2})= \frac{1\pm \sqrt{1- \sin^2(a)}}{2}$$ or, $$\sin^2(\frac{a}{2})=\frac{1\pm \cos(a)}{2}$$ or, $$\sin(\frac{a}{2})=\pm \sqrt{\frac{1\pm \cos(a)}{2}}$$ Even though I haven’t broke the algebraic rules, I know there is some mistakes and that's what I'm trying to trace. Where is the problem actually? How's the 'minus' thing became 'plus-minus' thing?? Help me please. Thanks in advance--","['trigonometry', 'fake-proofs']"
3257006,Proof: A set $\Omega$ is closed if its complement $\Omega^c = \mathbb{C} - \Omega$ is open.,"I am trying to prove the following theorem in the context of complex analysis: A set $\Omega$ is closed if its complement $\Omega^c = \mathbb{C} - \Omega$ is open. Preceding this theorem, the textbook does define open/closed sets and interior points, but it is only after this theorem that the textbook defines limit points . Therefore, I'm guessing that this should be provable without limit points (using the information conveyed up till this point in the textbook). The textbook later gives an equivalent theorem in terms of limit points, and I will attempt to prove this one later. My proof proceeds as follows: We begin by assuming that $\Omega^c = \mathbb{C} - \Omega = \{ z \in \mathbb{C} : z \not\in \Omega \}$ is an open set. A set is open if every point in that set is an interior point. Therefore, if $z_0 \in \mathbb{C}$ , there exists $r > 0$ such that $$D_r (z_0) = \{ z \in \mathbb{C} : |z - z_0| < r \} = \Omega^c$$ This is the open set. Now here's where I get confused. Next, I take the complement of the set $\Omega^c$ . As I understand it, the complement is the negation, and so when we take the complement of a set, we take the negation of the set condition: $$(\Omega^c)^c = \{ z \in \mathbb{C} : |z - z_0| \ge r \} = \Omega$$ This should be a closed set, but it isn't. A closed set is defined as $$\bar{D}_r (z_0) = \{ z \in \mathbb{C} : |z - z_0| \le 0 \}$$ But the negation of $<$ is $\ge$ , which is why I have $\Omega = \{ z \in \mathbb{C} : |z - z_0| \ge r \}$ I would greatly appreciate it if people could please tell me what I'm misunderstanding and how this should be done. EDIT: The closed disc $\bar{D}_r(z_0)$ of radius $r$ centred at $z_0$ is defined by $$\bar{D}_r(z_0) = \{ z \in \mathbb{C} : |z - z_0| \le r \}$$ and the boundary of either the open or closed disc is the circle $$C_r(z_0) = \{ z \in \mathbb{C} : |z - z_0| = r \}$$ Given a set $\Omega \subset \mathbb{C}$ , a point $z_0$ is an interior point of $\Omega$ if there exists $r > 0$ such that $$D_r(z_0) \subset \Omega$$ Finally, a set $\Omega$ is open if every point in that set is an interior point of $\Omega$ .","['complex-analysis', 'general-topology', 'proof-verification', 'elementary-set-theory']"
3257026,"Find all positive integers x,y satisfying $ \frac{1}{\sqrt{x}} +\frac{1}{\sqrt{y}} =\frac{1}{\sqrt{20}}$","Find all positive integers $x$ , $y$ satisfying $ \frac{1}{\sqrt{x}} +\frac{1}{\sqrt{y}} =\frac{1}{\sqrt{20}}$ $$ \frac{1}{\sqrt{x}} +\frac{1}{\sqrt{y}} =\frac{1}{\sqrt{20}}\\
\frac{1}{\sqrt{x}} +\frac{1}{\sqrt{y}} =\frac{1}{2\sqrt{5}}$$ By hit and trial I found one pair value of $x$ & $y$ i.e $(80, 80)$ But is there any other way to solve this tricky question and find all possible value of $x$ and $y$ .",['functions']
3257061,Invertibility of weighted shift operator.,"A linear operator $T$ on a (complex) separable Hilbert space $H$ is said to be a weighted shift operator if there is some orthogonal basis $\{e_n\}_n$ and weight sequence $\{w_n\}_n$ such that $$Te_n=w_n e_{n+1}, \forall n$$ $T$ is unilateral if $n$ runs over $\mathbb{N}$ and bilateral if $\mathbb{Z}$ . The adjoint is given by $$T^* e_n=\overline{w}_{n-1}e_{n-1} \text{ for all } n $$ if $T$ is bilateral and \begin{align*}
		T^* e_n&= \overline{w}_{n-1} e_{n-1} \text{	for all } n\geq 1\\
		T^* e_0&=0
		\end{align*} if $T$ is unilateral I saw in a paper that the unilateral shift is never invertible with the reason that $T^*$ is not invertible but the bilateral shift can be invertible given some conditions . Do we have that an operator is invertible iff its adjoint is invertible? I can't see the reason behind the conclusion. Please I need hints. Thanks","['complex-analysis', 'operator-theory', 'functional-analysis']"
3257062,Solve $(y^2 + xy)(x^2 - x + 1) = 3x - 1$ over the integers.,"Solve $$(y^2 + xy)(x^2 - x + 1) = 3x - 1$$ over the integers. There are many solutions to this problem, and perhaps I chose the worst one possible. I hope that someone could come up with a better answer. This problem is adapted from a recent competition (which is different than all of the ""recent competitions"" that I have mentioned before.","['modular-arithmetic', 'elementary-number-theory', 'diophantine-equations', 'polynomials', 'discrete-mathematics']"
3257074,Proving Differentiability of a Function (Spivak),"The following is an exercise from Calculus on Manifolds by Spivak. Let $g$ be a continuous, real-valued function on the unit circle $\{x\in\mathbb{R}^2:\|x\|=1\}$ , such that $g(-x)=-g(x)$ .
  Now define $f:\mathbb{R}^2\to\mathbb{R}$ by $$f(x) =\begin{cases}
\|x\|\cdot g\Big(\frac{x}{\|x\|}\Big) & x\neq 0
\\ 0 & x=0
\end{cases}$$ Fix $x\in\mathbb{R}^2$ , and further define a function $h:\mathbb{R}\to\mathbb{R}$ by $h(t)=f(tx)$ . 
  Show that $h$ is differentiable. My attempt: Observe that for $t\neq 0$ , we have \begin{align*}
h(t)=f(tx) & = \|tx\|\cdot g\Bigg(\frac{tx}{\|tx\|}\Bigg)
\\ & = t\|x\|\cdot g\Bigg(\frac{tx}{t\|x\|}\Bigg)
\\ & = t\|x\|\cdot g\Bigg(\frac{x}{\|x\|}\Bigg)
\\ & = tf(x)
\end{align*} In the case $t=0$ , we have $$h(0) = f(0) = 0$$ Thus, $h(t)=tf(x)$ for all $t\in\mathbb{R}$ .
A function $f:\mathbb{R}\to\mathbb{R}$ is differentiable at a point $a\in\mathbb{R}$ if there exists a linear transformation $\lambda(h):\mathbb{R}\to\mathbb{R}$ such that $$\lim_{h\to 0}\frac{f(a+h)-f(a)-\lambda(h)}{h}=0$$ Thus $h$ is differentiable if for all $t\in\mathbb{R}$ , there exists a transformation $Dh(k):\mathbb{R}\to\mathbb{R}$ such that $$\lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k}=0$$ We then have \begin{align*}
\lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k} & =\lim_{k\to 0}\frac{(t+k)f(x)-tf(x)-Dh(k)}{k}
\\ & = \lim_{k\to 0}\frac{kf(x)-Dh(k)}{k}
\end{align*} Thus the natural choice for $Dh(k)$ is $kf(x)$ . 
This completes the proof. Questions: Does this proof work? How do I know that $Dh(k)=kf(x)$ is a linear transformation?","['proof-verification', 'proof-writing', 'real-analysis', 'calculus', 'derivatives']"
3257098,Prove that $ y = x^{\frac{1}{n}} \Rightarrow y^n = x $,"Tao, Analysis I, exercise 5.6.1 I have to prove the following where $x,y \in \Bbb R^+, \ n \in \Bbb Z^+$ $$ y = x^{\frac{1}{n}} \Rightarrow  y^n = x $$ Hints: review the proof of Proposition
  5.5.12. Also, you will find proof by contradiction a useful tool, especially when
  combined with the trichotomy of order in Proposition 5.4.7 and Proposition
  5.4.12. My attempt: Assume that $ y = x^{\frac{1}{n}}  \Rightarrow  y^n > x $ . By definition: $$x^{\frac 1n}= \sup \{ y \in \mathbb R, s.t.  y \geq 0, y^n \leq x \}$$ Thus $ y^n > x $ contradicts the above definition, since $x$ is upper bound of $y^n$ . Does this part seem ok? I next need to get to a contradiction starting from here: Now assume that $ y = x^{\frac{1}{n}}  \Rightarrow  y^n < x $","['solution-verification', 'real-analysis']"
3257126,An entire function $f$ such that $g(z)=\overline{f(\bar{z})}$,"Let $f$ be an entire function on $\mathbb{C}$ . Let $g(z)=\overline{f(\bar{z})}$ . Which of the following statements is/are correct? 1) If $f(z) \in \mathbb{R}$ for all $z \in \mathbb{R}$ then $f=g$ . 2) If $f(z) \in \mathbb{R}$ for all $z \in \mathbb{R} \cup \{z | Im z=a\}$ for some $a>0$ , then $f(z+ia)=f(z-ia)$ for all $z \in \mathbb{C}$ . 3) If $f(z) \in \mathbb{R}$ for all $z \in \mathbb{R} \cup \{z | Im z=a\}$ for some $a>0$ , then $f(z+2ia)=f(z)$ for all $z \in \mathbb{C}$ . 4) If $f(z) \in \mathbb{R}$ for all $z \in \mathbb{R} \cup \{z | Im z=a\}$ for some $a>0$ , then $f(z+ia)=f(z)$ for all $z \in \mathbb{C}$ . option 1) is true as $g(z)=f(z)$ for all $z \in \mathbb{R}$ and therefore by identity theorem $f=g$ . In option 2) I get $g(z)=f(\bar{z})$ but I don't know how to proceed. Someone please help. Thanks $\textbf{EDIT:}$ In option 2) since $f$ and $g$ agree on $\mathbb{R}$ $f=g \ \forall z \in \mathbb{C}$ . i.e. $f(z)=g(z)=\overline{f(\bar{z})}$ Therefore for $z \in \mathbb{R}$ $f(z+ai)=g(z+ai)=\overline{f(\overline{(z+ai)})}=f(z-ai)$ Therefore by identity theorem $f=g$ for all $z \in \mathbb{C}$ Replacing $z$ by $z+ai$ in option 2) gives option 3)","['complex-analysis', 'entire-functions']"
3257187,"When does $f(x)=f(\alpha x) = f(\beta x)$ on $(0,\infty)$ imply that $f$ is constant?","Let $f:(0,\infty)\to \Bbb R$ be a continuous function and $\alpha,\beta>0$ be two fixed real numbers. Under what conditions on $\alpha$ and $\beta$ can we deduce that $f$ is a constant function if we know $$
f(x)=f(\alpha x) = f(\beta x)?
$$ I noticed that if $\alpha$ is an integer power of $\beta$ then the statement is not true. Indeed, for $\alpha=4,\beta=2$ the function $$
f(x) = \sin(2\pi \log_2(x))
$$ provides a counterexample. I believe that if $\frac{\log\alpha}{\log\beta}\in \Bbb R\backslash\Bbb Q$ then the statement should hold. For example, I think it is likely that $f(x)=f(2x)=f(3x)$ implies $f$ is constant but I don't know how to prove it.","['general-topology', 'ergodic-theory', 'real-analysis']"
3257195,Function that converges to $\infty$ at every point,"I was wondering whether there exists a function $f:\Bbb R\to\Bbb R$ that satisfies: $$\text{For all } y\in\Bbb R: \lim_{x\to y} f(x)=\infty.$$ Intuitively it seems to me like this is impossible. But I don't see how to prove it. By definition we would have $$\forall y \in \Bbb R: \forall r \in \Bbb R_+: \exists \delta > 0: \forall x \in (y-\delta, y+\delta)\setminus\{y\}: f(x)>r,$$ and not I don't know how to proceed.","['limits', 'functions', 'real-analysis']"
3257317,Proof: A set is closed if and only if it contains all its limit points.,"My complex analysis textbook states the following: A set is closed if its complement $\Omega^c = \mathbb{C} - \Omega$ is open. This property can be reformulated in terms of limit points. A point $z \in \mathbb{C}$ is said to be a limit point of the set $\Omega$ if there exists a sequence of points $z_n \in \Omega$ such that $z_n \not= z$ and $\lim_{n \to \infty} z_n = z$ . The reader can now check that a set is closed if and only if it contains all its limit points. I want to prove the following: A set is closed if and only if it contains all its limit points. Proof 1: I begin by assuming that the set $\Omega$ contains all of its limit points. $$\therefore \Omega = \{ z \in \mathbb{C} : (z_n \not = z) \cap (\lim_{n \to \infty} z_n = z) \},$$ where $z_n \in \Omega$ is a sequence of points. And so we have that $$\begin{align} \Omega^c &= \{ z \in \mathbb{C} : (z_n = z) \cup (\lim_{n \to \infty} z_n \not= z) \} \\ &= \{ z \in \mathbb{C} \} - \{ z \in \mathbb{C} : (z_n \not= z) \cap (\lim_{n \to \infty} z_n = z) \} \\ &= \mathbb{C} - \Omega \end{align}$$ Proof 2: Assume that the set $\Omega$ is closed. Therefore, $\Omega^c = \mathbb{C} - \Omega$ is open, since a set is closed if its complement $\Omega^c = \mathbb{C} - \Omega$ is open. A set is open if every point in that set is an interior point. Therefore, every point $z \in \Omega^c$ is an interior point. Therefore, every point in the set $(\Omega^c)^c = \Omega$ is not an interior point. This is where I'm unsure of how to proceed. Given what the author wrote, it is implied that they have provided all of the information necessary for this proof in the preceding sections. However, I have been unable to see how such a proof can be done. Preceding the above textbook excerpt, the author also give descriptions of closed and open discs , as well as interior points as follows: The interior of $\Omega$ consists of all its interior points. Finally, a set $\Omega$ is open if every point in that set is an interior point of $\Omega$ . I attempted to use this relationship between interior points and open sets, along with the definition of closed sets above, to form a proof. However, it's not clear to me whether this is fruitful. I would greatly appreciate it if people could please take the time to review my work and help me with proof 2.","['complex-analysis', 'general-topology', 'proof-verification', 'elementary-set-theory']"
3257373,Is $\lfloor \zeta(-n) \rfloor$ only prime for $n=23$?,"I searched for primes of the form of $\lfloor \zeta(-n) \rfloor$ , where $n \in \Bbb{N}$ ,  for a range of $n \le 10^4$ on PARI/GP and found $\lfloor \zeta(-n) \rfloor$ is only prime for $n=23$ . My PARI code: for(n=1, 10^4, if(ispseudoprime(floor(-bernfrac(2*n)/(2*n)))==1, print([2*n-1, floor(-bernfrac(2*n)/(2*n))])); print(2*n-1)) Note that $\zeta(-n)$ for odd $n$ (for even $n$ , $\zeta(-n)=0$ ) can be also expressed as: $$\zeta(-(2n-1))=-\frac{B_{2n}}{2n}$$ where $B_{2n}$ is the $2n$ th Bernoulli number and written as brenfrac(2*n) in PARI/GP. Questions: $(1)$ Is $\lfloor \zeta(-n) \rfloor$ , where $n \in \Bbb{N}$ ,  only prime for $n=23$ ? $(2)$ Are there finite primes of the form of $\lfloor \zeta(-n) \rfloor$ ? I would appreciate any counterexamples(can be a probable prime)/proofs/papers. Extra: I also searched for primes of the form $\lceil\zeta(-n)\rceil$ , $\lfloor\zeta(-n)\rceil$ , $\lceil B_n \rceil$ and $\lfloor B_n \rceil$ . For $n \le 10^4$ : $(1)$ $\lceil\zeta(-n)\rceil$ seems to be only prime for $n=691$ . $(2)$ There seems to be no prime of the form $\lfloor\zeta(-n)\rceil$ . $(3)$ $\lceil B_n \rceil$ seems to be only prime for $n=14$ . $(4)$ $\lfloor B_n \rceil$ seems to be only prime for $n=38$ .","['riemann-zeta', 'number-theory', 'conjectures', 'prime-numbers']"
3257398,What is the theme of analysis?,"It is safe to say that every mathematician, at some point in their career, has had some form of exposure to analysis. Quite often, it appears first in the form of an undergraduate course in real analysis. It is there that one is often exposed to a rigorous viewpoint to the techniques of calculus that one is already familiar with. At this stage, one might argue that real analysis is the study of real numbers, but is it? A big chunk of it involves algebraic properties, and as such lies in the realm of algebra. It is the order properties, though, that do have a sort of analysis point of view. Sure, some of these aspects generalise to the level of topologies, but not all. Completeness, for one, is clearly something that is central to analysis. Similar arguments can be made for complex analysis and functional analysis. Now, the question is: As for all the topics that are bunched together as  analysis, is there any central theme to them? What topics would you say that belongs to this theme? And what are the underlying themes in these individual subtopics? Add . It may be a subjective question, but having a rough idea of what the central themes of a certain field are helps one to construct appropriate questions. As such, I think it is important. I am not expecting a single answer, but more of a diverse set of opinions on the matter.","['analysis', 'real-analysis', 'complex-analysis', 'functional-analysis', 'soft-question']"
3257411,Compute $\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right) $,"Compute $\lim\limits_{n\to \infty} \left(n^3 \int_{n} ^{2n}\frac{x dx} {1+x^5}\right) $ . I tried to apply the first mean value theorem for  definite integrals and then apply the squeeze theorem, but it didn't work. The answer given by the book is $\frac{7}{24}$ ,but I can't see how to get to it.","['integration', 'limits', 'definite-integrals']"
3257430,Method to check if a series of one variable is periodic,"I have been solving differential equations by series solution method and I wanted to ask if there is a method to check if the series we get is periodic in its variable. In particular, if we get a solution for some ODE as $$y \left( x \right) = \sum\limits_{n = 0}^{\infty} a_n x^n$$ where $a_n$ is governed by some recurrence relation, then can we predict if the solution is Edit:- I have performed some try: If the function $y$ is periodic of period, say $p > 0$ , then it must satisfy $y \left( x \right) = y \left( x + p \right)$ . This, when substituted in series and then comparing the coefficients of equal powers of $x$ , we get the following result for $n \geq 0$ , $$a_n = \sum\limits_{k = 0}^{\infty} \binom{k + n}{n} a_{k + n} p^k$$ If we add another condition of periodicity $y' \left( x \right) = y' \left( x + p \right)$ , we get the same result for $n \geq 1$ . If we just cancel out the first term, we get the following result for every $n \in \mathbb{N} \cup \left\lbrace 0 \right\rbrace$ $$\sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k = 0$$ Now, from here what can we say about the coefficients $a_n$ so that the function $y$ is periodic? Edit:- I have been thinking and the above equation tells us that the series $\sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k$ converges. For that to happen, a necessary condition is that $$\lim\limits_{k \rightarrow \infty} \binom{k + n}{n} a_{k + n} p^k = 0$$ for each $n \in \mathbb{N}$ . So, if the recurrence relation for $a_n$ is known from the series solution method of differential equations, can we find some conditions so that the above limit holds true? In particular, I have the recurrence relation as $a_0, a_1 \in \mathbb{R}$ are arbitrary, $a_2 = 0$ and for $n \geq 1$ we get $$a_{n + 2} = \dfrac{1}{\left( n + 2 \right) \left( n + 1 \right)} \left[ a_n - 2 \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{\left( n - 2k \right) a_{n - 2k} \left( -1 \right)^k}{\left( 2k + 1 \right)!} - \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{a_{n - 2k} \left( -1 \right)^k}{\left( 2k \right)!} \right]$$ Can we find the rate of convergence of this recurrence relation and then comment on the limit mentioned above?","['power-series', 'periodic-functions', 'ordinary-differential-equations']"
3257489,Fundamental Lemma of Variational Calculus,"I'm reading over the following lemma and proof related to the fundamental lemma of variational calculus. If $\color{red}{\alpha ∈ C([a,b])}$ and $$\int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0$$ for every function $h ∈ C^1([a,b])$ such that $h(a) = h(b) = 0$ , then (1) $\beta(x)$ is differentiable almost everywhere on $[a, b]$ (2) $\beta'(x)$ = $\alpha(x)$ I understand the proof for the above lemma.  However, one of the exercises at the end of the section is as follows. If $\color{red}{\alpha ∈ L^1([a,b])}$ and $$\int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0$$ for every function $h ∈ C^1([a,b])$ such that $h(a) = h(b) = 0$ , then (1) $\beta(x)$ is differentiable almost everywhere on $[a, b]$ (2) $\beta'(x)$ = $\alpha(x)$ My question: wouldn't both proofs be relatively the same?","['functional-analysis', 'analysis']"
3257505,Condition for polynomials to be proper,"Let $\Bbbk\in \left\{ \mathbb R,\mathbb C \right\}$ . Suppose $\mathbb \Bbbk^n\overset{f}{\to} \Bbbk$ is a homogeneous polynomial map satisfying the following condition: the fiber of $f$ containing the origin is equal to the origin. Question 1. Is $f$ proper? Intuition. Let $n=2$ . Suppose we localize around the singular fiber over zero. Upon deleting it was obtain at least locally on the source a submersion, which forces the fibers of $f$ to foliate the domain. If the fiber containing the origin is just the origin, then the remaining fibers must foliate the plane minus the origin. It feels like this should make them some sort of loops about the origin, and their alignment should make the whole map proper. I'm staring at $ax^2+by^2$ vs $ax^2-by^2,xy$ etc and this seems to be the phenomenon. In fact, the same thing seems to happen for $x^4+y^4-xy$ , so it seems only the top degree homogeneous summand of a polynomial determines this behavior. This motivates: Question 2. Suppose $f$ is an arbitrary polynomial mapping whose top degree homogeneous summand satisfies the above condition. Is $f$ proper?","['general-topology', 'singularity-theory', 'polynomials', 'differential-geometry']"
3257507,How to sum up this inverse function $ \lim_{n\to\infty}\sum_{r=0}^n \tan^{-1}\left(\frac{2r}{1+2r^4}\right)$,"$$ \lim_{n\to\infty}\sum_{r=0}^n \tan^{-1}\left(\frac{2r}{1+2r^4}\right)$$ So I really couldn't get started. I know that it is probably of form $$ \tan^{-1}(y) - \tan^{-1}(x) = \tan^{-1}\left(\frac{y-x}{1+xy}\right)$$ , but I couldn't bring it in that form. The denominator won't let me . Help please...","['trigonometric-series', 'trigonometry', 'inverse-function', 'inverse']"
3257536,"Probs. 3 (a & c), Sec. 27, in Munkres' TOPOLOGY, 2nd ed: The space $\mathbb{R}_K$ is not path connected and $[0, 1]$ is not compact in $\mathbb{R}_K$.","Here is Prob. 3, Sec. 27, in the book Topology by James R. Munkres, 2nd edition: Recall that $\mathbb{R}_K$ denotes $\mathbb{R}$ in the $K$ -topology. (a) Show that $[0, 1]$ is not compact as a subspace of $\mathbb{R}_K$ . (b) Show that $\mathbb{R}_K$ is connected. [ Hint: $(-\infty, 0)$ and $(0, \infty)$ inherit their usual topologies as subspaces of $\mathbb{R}_K$ .] (c) Show that $\mathbb{R}_K$ is not path connected. Here is my Math SE post on Part (b) of this particular problem. My Attempt: We recall that the set $K$ is given by $$ K = \left\{ \ 1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \ldots \ \right\}, \tag{Definition 0}$$ and the $K$ -topology on the set $\mathbb{R}$ of real numbers is the topology having as a basis all the open intervals $(a, b)$ , where $a \in \mathbb{R}$ , $b \in \mathbb{R}$ , and $a < b$ , together with all the sets of the form $(a, b) \setminus K$ . And, set $\mathbb{R}$ with the $K$ -topology is denoted by $\mathbb{R}_K$ . [Please refer to Sec. 13 in Munkres, specifically the Definition preceding Lemma 13.4.] Part (a): In this part, we will be using Lemma 26.1 in Munkres. Let $\mathscr{A}$ be the following covering of $[0, 1]$ by sets open in $\mathbb{R}_K$ . $$ \mathscr{A} \colon= \big\{ \ (-1, 2) \setminus K \ \big\} \bigcup \left\{ \ \left( \frac{1}{n+1}, 2\right) \ \colon \ n \in \mathbb{N} \ \right\}. $$ [Refer to (Definition 0) above.] We note that there is no finite sub-collection of $\mathscr{A}$ that could possibly cover the entire closed interval $[0, 1]$ . PS: In order to verify the assertion in the preceding paragraph, let us consider any finite sub-collection of the open covering $\mathscr{A}$ , say, the sub-collection $$ \mathscr{A}^\prime \colon=  \big\{ \ (-1, 2) \setminus K \ \big\} \bigcup \left\{ \ \left( \frac{1}{n_1 + 1}, 2\right), \ldots, \left( \frac{1}{n_k + 1}, 2\right) \ \right\},  $$ where $n_1, \ldots, n_k$ are distinct natural numbers. Then we find that $$ \bigcup_{S \in \mathscr{A}^\prime} S = \big( \, (-1, 2) \setminus K \, \big) \cup \left( \frac{1}{n_0+1}, 2 \right), $$ where $$ n_0 \colon= \max \left\{ n_1, \ldots, n_k \right\}. $$ Thus, for example, $$ \frac{1}{n_0+2} \in [0, 1] \setminus \bigcup_{S \in \mathscr{A}^\prime} S. $$ Therefore no finite sub-collection $\mathscr{A}^\prime$ of $\mathscr{A}$ can cover $[0, 1]$ as a subspace of $\mathbb{R}_K$ . Hence $[0, 1]$ is not compact as a subspace of $\mathbb{R}_K$ . Is this proof correct? Part (c): First, here is the definition of a topological space to be path connected. Given points $x$ and $y$ of a topological space $X$ , a path in $X$ from $x$ to $y$ is a continuous map $f \colon [a, b] \to X$ of some closed interval in the real line into $X$ , such that $f(a) = x$ and $f(b) = y$ . A topological space $X$ is said to be path connected if every pair of points of $X$ can be joined by a path in $X$ . Please refer to Sec. 24 in Munkres, specifically the Definition preceding Example 3. Suppose that $\mathbb{R}_K$ is path connected. Then there is a path in $\mathbb{R}_K$ from  point $x = 0$ to point $y = 1$ ; that is, for some closed interval $[a, b]$ on the real line, there exists a continuous mapping $f \colon [a, b] \to \mathbb{R}_K$ such that $f(a) = 0$ and $f(b) = 1$ . We note that the closed interval $[a, b]$ is both connected and compact as a subspace of the topological space $\mathbb{R}$ (i.e. the set $\mathbb{R}$ of real numbers with the standard, or usual, topology). [Please refer to Corollary 24.2 and Corollary 27.2 in Munkres.] Now as the closed interval $[a, b]$ is both connected and compact as a subspace of $\mathbb{R}$ and as $f$ is a continuous mapping of $[a, b]$ into $\mathbb{R}_K$ , so the image $f\big([a, b]\big)$ is also connected and compact as a subspace of $\mathbb{R}_K$ . [Please refer to Theorems 23.5 and 26.5 in Munkres.] Thus as a subspace of $\mathbb{R}_K$ , the set $f\big( [a, b] \big)$ is both compact and connected, and $f(a) = 0$ and $f(b) = 1$ . Let $y$ be any real number in the open interval $(0, 1)$ . Now as $[a, b]$ is connected, as $f \colon [a, b] \to \mathbb{R}_K$ is continuous, and as $f(a) = 0$ and $f(b) = 1$ , so there exists a real number $c \in (a, b)$ such that $f(c) = y$ , by the intermediate-value theorem (i.e. Theorem 24.3 in Munkres). Thus we have shown that $$ [0, 1] \subset f\big( [a, b] \big). \tag{1} $$ PS: The argument in the preceding paragraph is flawed, as has been pointed out in one of the comments below. So here is the correct argument: Let $y$ be any real number in the open interval $(0, 1)$ . Suppose that there is no $r \in [a, b]$ for which $f(r) = y$ . As $(-\infty, y) \cup (y, \infty) = \mathbb{R}\setminus \{ y \}$ and as $y \not\in f\big([a, b]\big)$ , so we find that $$ \big(\, (-\infty, y) \cap f\big([a, b]\big) \, \big) \cup \big( \, (y, +\infty)\cap f\big([a, b]\big) \, \big) = \big( \mathbb{R} \setminus \{y \} \big) \cap f\big([a, b]\big) = f\big([a, b]\big).   $$ Now as the unbounded open intervals $(-\infty, y)$ and $(y, +\infty)$ are open in $\mathbb{R}_K$ , so the sets $(-\infty, y)\cap f\big([a, b]\big)$ and $(y, +\infty)\cap f\big([a, b]\big)$ are open in the subspace $f\big([a, b]\big)$ of $\mathbb{R}_K$ . Moreover, the sets $(-\infty, y)\cap f\big([a, b]\big)$ and $(y, +\infty)\cap f\big([a, b]\big)$ are disjoint . Thus the disjoint open sets $(-\infty, y)\cap f\big([a, b]\big)$ and $(y, +\infty)\cap f\big([a, b]\big)$ form a separation of $f\big([a, b]\big)$ . But this contradicts the fact that $f\big([a, b]\big)$ is connected . Thus there does not exist any real number $y \in (0, 1)$ for which there is no $r \in [a, b]$ such that $f(r) = y$ . That is, for any real number $y \in (0, 1)$ , we have $y = f(r)$ for at least one $r \in [a, b]$ . Moreover, as $f(a) = 0$ and $f(b) = 1$ , so we conclude that $$ [0, 1] \subset f\big([a, b]\big). $$ Now as the closed interval $[0, 1]$ is a closed subset of $\mathbb{R}_K$ and as $[0, 1] \subset f\big( [a, b] \big)$ , so $$[0, 1] = [0, 1] \cap f \big( [a, b] \big),$$ and thus $[0, 1]$ is also a closed subset of $f \big( [a, b] \big)$ . [Please refer to Theorem 17.2 in Munkres.] And, as $f\big([a, b]\big)$ is compact and as $[0, 1]$ is a closed subspace of $f\big([a, b]\big)$ , so $[0, 1]$ is also compact as a subspace of $f\big([a, b]\big)$ , by Theorem 26.2 in Munkres. Finally, as $[0, 1]$ is a compact subspace of $f\big([a, b]\big)$ and as $f \big([a, b]\big)$ is a subspace of $\mathbb{R}_K$ , so $[0, 1]$ is also compact as a subspace of $\mathbb{R}_K$ . But this contradicts the fact $[0, 1]$ is NOT compact as a subspace of $\mathbb{R}_K$ , as we have shown in Part (a) above. Thus our supposition that $\mathbb{R}_K$ is path connected is wrong. Hence $\mathbb{R}_K$ is not path connected. Is this proof correct? Are both proofs given above correct and clear enough in each and every detail? Or, are there any point(s) in either of these where there are issues or errors?","['connectedness', 'path-connected', 'solution-verification', 'general-topology', 'compactness']"
3257556,Two divergent sequences such that their product converges,"Example of two divergent sequences such that their product converges. I know, if $x_n=\left\{(-1)^n\right\}$ and $y_n=\left\{(-1)^{n+1}\right\}$ , then their product converges to $(-1)$ . But here $x_n$ and $y_n$ are oscillatory sequences, they are not properly divergent(i.e. they do not diverge to $+\infty$ or $-\infty$ . I want to know, are there two 'properly' divergent sequences so that their product converges? Please anyone help me. Thanks in advance.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3257559,Laurent Series $f(z)=\frac{1}{(z+1)(z-2)}$ with conditions,"I want to find the laurent series of the function $$f(z)=\frac{1}{(z+1)(z-2)}$$ for the following conditions: $|z|<1$ $2<|z|<\infty$ I have found the series for first condition, by partial fractions as $$f(z)=\frac13(\frac{1}{z-2}-\frac{1}{z+1})=-\frac12+\frac z4-\frac{3z^8}{8}+\cdots$$ But how to find the series for the second condition $2<|z|<\infty$ , as in the second condition function is holomorphic everywhere? We can use also use the long general procedure using contour integration, but that I know to be used when laurent series is to be found out at a particular point $z=z_0$ , but how to perform  it for intervals ? Another doubt And sometimes I get confused when we can say taylor's series will be same as laurent series?","['laurent-series', 'complex-analysis', 'taylor-expansion', 'power-series', 'complex-numbers']"
3257562,Relationship between $\theta$ functions and number fields.,"I'm trying to have a clear picture of the relationship of theta functions and $L$ -functions, and the geometric objects they relate to. Firstly, I know that $\theta$ -functions arise as sections of line bundles on abelian varieties. So if you take a lattice $\Lambda$ in $V=\mathbb{C}^g$ and a polarization $H$ (i.e a hermitian form on $V$ such that $E=Im(H)$ is definite positive and integral on $\Lambda$ ) you get an abelian variety $X=V/\Lambda$ and a class of line bundles $L(H, \bullet)$ . If you specify a quasi-character of $H$ , then you get a well definied line bundle $L$ on $X$ whose 1st chern class is $E$ . So far this is clear. From the data of $\Lambda$ alone, you get a family of $\theta$ functions, which correspond to all sections of all the line bundles on $X$ . Basically $\theta$ -functions are paramterized by a lattice, a polarization and a quasi-character. You can also parametrize them by an element in the Siegel half space, a polarization and a quasi-character. Now on the other hand $L$ -functions arise as Mellin Transform of $\theta$ -functions (up to Euler/Gamma-factors). For instance the Riemann Zeta function satsifies $$\pi^{-s/2}\Gamma(s/2)\zeta(s)=Mel(\theta(it)-1, s)$$ with $\theta(z)=\sum_{m\in \mathbb{Z}}e^{i\pi m^2z}$ the Jacobi theta function. Of course the term $it$ in $Mel(\theta(it)-1, s)$ makes me think that $it$ lives in $\mathbb{H}$ the upper half plane, but I don't think that should be relevant because the variable $t$ is the one integrated in the Mellin transform so it's not a fixed parameter that I could interpret as corresponding to some lattice. So my (rather blurry) question is : what is the relationship between the ""abelian variety"" picture of theta functions, and the zeta functions of number field arrising as Mellin transforms of the same $\theta$ -functions. The naive picture I have in mind (which is probably not true) is the following. If you take $k$ a number field, its ring of integer gives you via the canonnical embedding a lattice $\Lambda$ in some vector space $\mathbb{C}^g$ (obvisouly there are some problems already, as the dimension of $\mathbb{R}^{s+t}$ has no reason to be even in general). The abelian variety $X=\mathbb{C}^g/\Lambda$ has a polarization (coming from the trace on the number field ?) and thus a line bundle, unique up to translation which has a section defined by a $\theta$ function, such that $\zeta_k$ is essentially the Mellin transform of that $\theta$ -function. Now what is wrong with that picture? How far is it from what actually happens? And what is the real picture? I've seen $\theta$ -functions in the context of $L$ -functions, parametrized by all sorts of parameters, e.g Neukirch writes $$\theta_\Gamma(a,b,z)=\sum_{g\in \Gamma}N(a+g)e^{i\pi((a+g)z, a+g)+2i\pi(b,g)}$$ in this case can I interpret $\theta_\Gamma(a,b,z)$ as a section on some line bundle on $\mathbb{C}^d/\Gamma$ ? I feel like those $(a,b)$ should be some $(H,\alpha)$ for some ""natural"" $(H,\alpha)$ , am I completely mistaken here?","['number-theory', 'theta-functions', 'l-functions', 'abelian-varieties']"
3257566,Definition of an indexed sequences of Random Variables,I'm reading a book on probability theory and in one section the author is using a sequence of random variables $$X_{N_k}$$ where $(X)_n)$ is a sequence of random variables and $(N_k)_k$ are random variables taking values in the positive integers. How is $X_{N_k}$ defined?,"['probability-theory', 'probability']"
3257640,Are winding number and index of a not smooth closed curve the same?,"Let $\gamma:[0,1] \longrightarrow \mathbf{C} \backslash \{0\}$ be a closed curve (continuous and of bounded variation). 
We call $$\operatorname{Ind}_\gamma(0) \overset{\mathrm{def}}{=} \frac{1}{2 \pi i}\int_\gamma \frac{1}{z}\ dz.$$ $\textbf{the index of $\gamma$ around $0$.}$ As for the winding number, we can take a pair of continuous real valued functions $r$ and $\theta$ that satisfies $$r:[0,1] \longrightarrow (0,\infty),$$ $$\theta:[0,1] \longrightarrow \mathbf{R},$$ $$\forall t \in [0,1]\, \left[\, \gamma(t) = r(t) \cdot e^{i \cdot \theta(t)}\, \right],$$ and $$-\pi < \theta(0) \leq \pi.$$ (The proof of this can be found in chapter 7 of A. F. Beardon, Complex Analysis: The Argument Principle in Analysis and Topology, Wiley-Interscience publication 1979).
We call $$\operatorname{Wnd}_\gamma(0) \overset{\mathrm{def}}{=}\frac{\theta(1) - \theta(0)}{2\pi}$$ $\textbf{the winding number of $\gamma$ around $0$}$ . It is already known that if $\gamma$ is $C^1$ -curve, then $$\operatorname{Ind}_\gamma(0) = \operatorname{Wnd}_\gamma(0)$$ (see definition of winding number, have doubt in definition. ). My question: When $\gamma$ is not $C^1$ path, $$\operatorname{Ind}_\gamma(0) = \operatorname{Wnd}_\gamma(0)?$$ Thank you for reading.","['complex-analysis', 'plane-curves', 'winding-number']"
3257689,Why does this appear to produce OEIS sequence A263484?,"A263484 is ""Triangle read by rows: $T(n,k)$ ( $n\geq 1$ , $0 \leq k < n$ ) is the number of permutations of $n$ with $n! - k$ permutations in its connectivity set."", and the sequence is: 1, 1,1, 1,2,3, 1,3,7,13, 1,4,12,32,71, 1,5,18,58,177,461, ... I have looked at various papers on connected and irreducible permutations, but I don't fully understand them.  I also don't see anything in them that ""looks"" like what I am doing, but that is likely due to my lack of understanding.  So here is what I am doing: Let $P$ be the set of permutations of $n$ , with $n \geq 2$ , and let $p$ be any permutation in $P$ .  Let $Q$ be the set of permutations of the first $n-1$ symbols of $p$ . We will be counting permutations, so let $C$ be an array of integers with $|C| = n-1$ , used to record counts, and initialize it to all zeros.  For each permutation $q$ in $Q$ , append $q$ to $p$ to obtain $s$ =the concatenation $p+q$ . $s$ will always be of length $2n-1$ . Count the number of substrings of contiguous symbols of length $n$ in $s$ that are permutations in $P$ , and call this count $x$ . $x$ will range from 2 to $n$ , depending on $s$ .  Record this count in array $C$ by incrementing array element $x-2$ by one.  After all $(n-1)!$ $s$ 's have been checked, let row $n-1$ in $T(n,k)$ equal the reversed array $C$ . A small example: Let $P$ = {(1,2,3,4),...,(4,3,2,1)}, $p$ = (2,3,4,1), Q ={(2,3,4),(2,4,3)...,(4,3,2)}, and $C$ =[0,0,0].  First, let $s$ =(2,3,4,1,2,3,4) and we find (2,1,3,4), (3,4,1,2), (4,1,2,3), and (1,2,3,4) are in $P$ , so $x$ =4. Increment element 4-2=2 in $C$ by one, so now $C$ = [0,0,1]. Now let $s$ =(2,3,4,1,2,4,3) and we find (2,3,4,1), (3,4,1,2), and (1,2,4,3) are in $P$ , but (4,1,2,4) is not, so $x$ =3. Increment element 3-2=1 in $C$ by one, so now $C$ =[0,1,1].  After all 3!=6 strings have been checked, C=[3,2,1] Now let row three of $T(n,k)$ equal the reverse of $C$ = [1,2,3]. The triangle I get from this is: 1, 1, 1, 1, 2, 3, 1, 3, 7, 13, 1, 4, 12, 32, 71, 1, 5, 18, 58, 177, 461, 1, 6, 25, 92, 327, 1142, 3447, 1, 7, 33, 135, 531, 2109, 8411, 29093, 1, 8, 42, 188, 800, 3440, 15366, 69692, 273343, ... which appears to be A263484. But why? EDIT: After reading darij grinberg's comment, I have corrected the definition of Q.  Q was originally defined in this question as ""Let Q be the set of permutations of (n−1)."", which was not correct.  I also changed $p$ and $Q$ the example, so it was clear I was not using the permutations of n-1.","['permutations', 'number-theory', 'symmetric-groups', 'combinatorics']"
3257691,"Prove that for every complex $2$x$2$ matrix $A$, there exists a matrix $X$ such that $X^3=A^2$.","This is a problem I've been stuck on for a while. I know that this statement is false if it had $A$ instead of $A^2$ , for example, the matrix $A = \begin{bmatrix} 0&1\\0&0 \end{bmatrix}$ can't be written as $X^3$ , but since $A^2$ equals $ \begin{bmatrix} 0&0\\0&0 \end{bmatrix}$ , it can be written as $X^3$ with $X$ being $ \begin{bmatrix} 0&0\\0&0 \end{bmatrix}$ for example. Other than this, I have no insight into why this statement could be true: nothing can be assumed about diagonalizability, invertibility or anything else. I might be overlooking something simple. Thanks in advance.","['matrices', 'matrix-equations', 'linear-algebra']"
3257746,Convergence of $\sum_{k=1}^{\infty}\frac{\cos(\theta k)}{\sqrt{k}}$,"Say if the following series $$ \sum_{k=1}^{\infty} \frac{\cos(\theta k)}{\sqrt{k}} $$ for $θ \in \mathbb{R}$ is convergent. Is it absolutely convergent? I don't know how to approach this problem. Any hint will be very appreciated.
Thanks!","['divergent-series', 'calculus', 'absolute-convergence', 'sequences-and-series', 'convergence-divergence']"
3257782,Solve $\frac{f'(x)f'''(x)}{(f''(x))^2}=C$,"Let $f$ be twice differentiable almost everywhere and continuous. $C$ is a constant. Solve for the following differential equation: $\frac{f'(x)f'''(x)}{(f''(x))^2}=C$ almost everywhere. If we remove the ""almost everywhere"", the solution is pretty standard: $f$ can be a log function, exponential function, or polynom. To be more specific, when $C=1$ , it seems that $f(x)=ae^{bx}+c$ where $a,b,c$ are constants. When $C=2$ , $f(x)=a\ln(x+b)+c$ . When $c\neq 1$ or $2$ , $f(x)=a(x+b)^c+d$ Can we get more solutions if we add back the ""almost everywhere""?","['ordinary-differential-equations', 'real-analysis', 'calculus', 'functions', 'derivatives']"
3257793,Binary weight of OEIS sequence A308092.,"Preliminaries OEIS sequence A308092 is defined as: The sum of the first $n$ terms of the sequence is the concatenation of the first $n$ bits of the sequence read as binary, with $a(1) = 1$ . And it begins 1, 2, 3, 7, 14, 28, 56, 112, 224, 448, 896, 1791, 3583, 7166, ... which in binary is $$
1_2, 10_2, 11_2, 111_2, 1110_2, 11100_2, 111000_2, 1110000_2, 11100000_2, 111000000_2, 1110000000_2, 11011111111_2, 110111111111_2, 1101111111110_2.
$$ Example To be explicit, for $n = 5$ , $$
  \begin{align*}
  1 + 2 + 3 + 7 + 14 
&= 1_2 + 10_2 + 11_2 + 111_2 + 1110_2 \\
&= 11011_2,
  \end{align*}
$$ that is, the sum of the first five terms is the first five bits of the sequence. An equivalent definition is $a(n) = c(n) - c(n-1)$ for $n > 2$ , where $c(n)$ is the concatenation of the first $n$ bits of the sequence. Question It appears that the number of ones in the binary representation of $A308092(n)$ is weakly increasing as a function of $n$ . The number of ones is listed here: 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 10, 11, 11, 11, 13, 13, 14, 14, 14, 16, 16, 16, 17, 17, 17, 19, 19, 19, 19, 20, 20, 20, 22, 22, 22, 22, 22, 23, 23, 23, 25, 25, 25, 25, 25, 25, 26, 26, 26, 28, 28, 28, 28, 28, 28, 28, 29, 29, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 40, 41, 41, 41, ... I've checked that this claim is true for the first 5000 terms, but I don't know how to prove it. Why is this true? Or is there some large counterexample? Note At first, it appears that the run-lengths of bits in A308092 matches the run-lengths in the ""number of ones"" sequence, but this fails at the $51$ st term. However, they both begin 2, 1, 8, 1, 3, 2, 3, 3, 3, 4, 3, 5, 3, 6, 3, 7, 2, 1, 10, 1, 11, 1, 9, 1, 2, 1, 9, 2, 2, 1, 8, 1, 5, 1, 8, 1, 3, 1, 2, 1, 8, 1, 3, 1, 3, 1, 8, 1, 3, 1, ...
two 1s, one 0, eight 1s, one 0, three 1s, two 0s, ... (bits in sequence)
(two 1s, one 2, eight 3s, one 10, three 11s, two 13s, ... (""number of ones"" sequence)","['binary', 'oeis', 'sequences-and-series']"
3257795,Solving a limit of integral with L'Hopital,"I have this limit: $\displaystyle \lim\limits_{x\to \infty}\dfrac{1}{x}\int_{0}^x \dfrac{1}{2+\cos(\mathrm t)}\, \mathrm{dt}$ I said that Edit: $-1\leq \cos\mathrm{t}\leq 1 \Rightarrow \dfrac{1}{2+\cos\mathrm{t}} > 0 \text{ and because that expression has no limit as x -> inf}\\\Rightarrow \displaystyle \int_{0}^x \dfrac{1}{2+\cos\mathrm{t}}\, dt \to \infty \\ \\ \Rightarrow \lim\limits_{x\to \infty} \dfrac{\displaystyle\int_{0}^x \dfrac{1}{2+\cos\mathrm{t}}\,\mathrm{dt}}{x} = \lim\limits_{x\to \infty} \dfrac{\Big( \displaystyle\int_{0}^x \dfrac{1}{2+\cos \mathrm{t}}\, \mathrm{dt}\Big)'}{x'} = \lim\limits_{x\to \infty} \dfrac{\dfrac{1}{2+\cos \mathrm{x}}}{1} = \lim\limits_{x\to \infty} \dfrac{1}{2+\cos \mathrm{x}}$ But here I got stuck because that limit does not exist, because cos x has no limit when x goes to infinity. But the correct answer is $\dfrac{1}{\sqrt 3}$ , what did I do wrong?","['integration', 'limits', 'calculus']"
3257858,Independence problem: one rook and maximum number of knights on the chessboard $8 \times 8$,"On the chessboard $8 \times 8$ we can to place one rook and several knights. Find the maximum number of knights, which can be placed on a chessboard along with one rook so that none of the pieces attack each other. My work . If a rook is placed in a square a8, then we can place the $25$ knights in all the white squares that rook doesn’t attack. I do not know how to prove that $25$ is the maximum number of knights.","['chessboard', 'recreational-mathematics', 'combinatorics']"
3257865,Polynomial or not?,"My question is whether $x(x+{1\over x}+2)$ the same as $x^2+2x+1$ ?
And if so then: -Why doesn't 0 belong to the domain of the first but it belongs to the second ? -Is the first function a polynomial ?","['algebra-precalculus', 'functions']"
3257939,Trig identities analogous to $\tan(\pi/5)+4\sin(\pi/5)=\sqrt{5+2\sqrt{5}}$,The following trig identities have shown up in various questions on MSE: $$-\tan\frac{\pi}{5}+4\sin\frac{2\pi}{5}=\tan\frac{\pi}{5}+4\sin\frac{\pi}{5}=\sqrt{5+2\sqrt{5}}$$ $$-\tan\frac{2\pi}{7}+4\sin\frac{3\pi}{7}=-\tan\frac{\pi}{7}+4\sin\frac{2\pi}{7}=\sqrt{7}$$ $$\tan\frac{\pi}{11}+4\sin\frac{3\pi}{11}=\tan\frac{3\pi}{11}+4\sin\frac{2\pi}{11}=\sqrt{11}$$ $$\tan\frac{6\pi}{13}-4\sin\frac{5\pi}{13}=\tan\frac{2\pi}{13}+4\sin\frac{6\pi}{13}=\sqrt{13+2\sqrt{13}}$$ Does anyone know of any analogous identities for larger primes? I have not been able to find anything similar for $p=17$ or $p=19$ . (I am not asking for proofs of the above equations.),"['modular-arithmetic', 'algebraic-numbers', 'gauss-sums', 'trigonometry', 'prime-numbers']"
3257946,"Determining when a set is ordered, with noise and missing values","The problem: I have repeated observations of the ordering of about 25-30 objects, and I have maybe 1000 sets of these objects. I wish to determine which sets of objects are ordered and which are not, but there are a couple complications. First, there are missing values in each observation. For each observation, about 60% of the objects are observed (on average, but this varies between sets). Second, I would like to account for some noise, so that even if a small number of the objects swap order or are in the wrong place on a given day, but most of the time most of the objects are in the same order, then it would be labeled as ordered. What I've done (and some problems with it): So far, I have created a variance-like measure (M) based off a “true” rank. The “true” rank is constructed by comparing each object i to each object j and determining the percent of observations for which rank(i) $<$ rank(j). From this, I pull a vector of row averages (averaging the percent of times object i is before every other object j), and I rank these. The object whose rank is truly “first” should have a high average, and so on. I then compare this “true” rank to the observed rank in the following way: $M=\frac{1}{D}\sum_d^D \sum_i^n \frac{1}{n_d^*} ( rank_{id}-\bar {rank}_{id} )^2$ where D is the number of observations d, i is the individual, and $\bar {rank}$ is the ""true"" rank for any given observation d. It is specific to the observation, because the ""true"" rank is adjusted for which objects are present in the observation (since some are missing each observation). Similarly, n* is the number of objects observed that day (observation). I then use some threshold value as a cutoff point. This seems to work, and I've used several thresholds, but the (main) issue is that it is not invariant to set size. That is, sets with higher missing rates appear to have a lower measure (M) than those with lower missing rates, simply because the potential difference in values is greater. Sets with higher missing rates are observably different in other areas, and so this method of dividing the data fails a balancing test. The second issue is that determining the threshold is a bit ad hoc, but I could probably live with that if the other issue was not so glaring. Other Thoughts: I've started looking at clustering, with the hope that I could test H0: single cluster vs H1: more than one cluster. This is in part because I want to do some hierarchical clustering with the unordered sets once I have determined which they are. There seem to be a few ways to test for the number of clusters, but (particularly in hierarchical clustering) I don't see many ways to conclude that there is a single cluster. It looks like the gap statistic might be able to do this in more of a k-means context (as well as some others in the k-means context), but testing for the existence of clusters with one type of clustering algorithm and then creating clusters later in the process via another seems strange to me. I'm sure there are other (possibly obvious) ways to think about an ordering problem like this, and am open to suggestions outside of these areas I've been thinking about.","['data-analysis', 'statistics', 'clustering']"
3258019,"When does the limit $\lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}$ exist?","In this case, $k,l,p,q\geq0$ and are integers. I have attempted the substitution $u=x^p$ and $v=y^q$ $$\lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}=\lim_{(u,v)\to(0,0)} \frac{u^{k/p}v^{l/q}}{u^2+v^2}$$ Then, note that $|u|<\sqrt{|u|^2+|v|^2}$ and $|v|<\sqrt{|u|^2+|v|^2}$ . So we have $$\bigg|\frac{u^{k/p}v^{l/q}}{u^2+v^2}\bigg|=\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}<\frac{(|u|^2+|v|^2)^{k/(2p)+l/(2q)}}{|u|^2+|v|^2}$$ Thus, the limit equals $0$ when $\frac{k}{p}+\frac{l}{q}>2$ . We need to show that otherwise, the limit does not exist. If $\frac{k}{p}+\frac{l}{q}=2$ , taking the limit along the axes yields $0$ but letting $x^p=y^q$ gives $$\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}=\frac{|u|^{k/p+l/q}}{2|u|^2}\to\frac{1}{2}$$ so the limit does not exist. Similarly, if $\frac{k}{p}+\frac{l}{q}<2$ , taking the limit along the axes still yields $0$ , but letting $x^p=y^q$ instead gives $$\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}>\frac{|u|^{2}}{2|u|^2}\to\frac{1}{2}$$ showing that the limit does not exist. I'm not sure if all of the steps I've taken are correct - especially some of the inequalities as $(u,v)\to(0,0)$ . Also, for the last two cases, I'm not exactly sure if the absolute values around $u$ and $v$ should be there or whether I should have let $|x|^p=|y|^q$ .","['limits', 'multivariable-calculus']"
3258042,Determine the cocycle condition in Galois descent induced by faithfully flat descent,"I initially asked this question on  Mathoverflow as I thought it was to right place to do so. But it might not be so I will copy it here instead. I apologize for double posting and I will gladly erase the inapropriate one. It's the first time that I use these forums so I'm not sure if it is the right amount of details. I tried to give a proof that fppf (faithfully flat) descent implies Galois descent purely at the level of modules and I stumble to obtain the Galois cocycle condition. I'm interested to consider some questions of twisted sheaves with a Galois cohomological description and understanding how to obtain the former would be useful to me. I obtained the following the following conditions:
Given a finite Galois extension $L/K$ of Galois group $G$ and $M$ an $L$ -vector space $M$ , we have for each $\sigma \in G$ an isomorphism of $L$ -vector spaces $\psi_\sigma : M \to M^\sigma$ satisfying $\psi_\sigma(am) = \sigma(a) \psi_\sigma(m)$ , where $a \in L$ and $m \in M$ and such that for every pair $(\sigma, \tau) \in G \times G$ we have $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma } \circ \psi_{ ( \sigma, \tau), (\sigma, \tau), \tau } = \psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma \tau }
$$ as isomorphisms of $L$ -modules. The $L$ -module structure of $M^\sigma$ is twisted by $\sigma$ , i.e given by $a \cdot m:= \sigma(a)m$ . My issue is that because of what one obtains for sheaves of modules ( https://stacks.math.columbia.edu/tag/0CDQ ) I would expect the cocycle condition for modules to also have a twisting in the formula. Translating what is done in stacks project into modules is certainly possible but I wasn't able to do so. Edit: If someone can give me the details of how it is done at the level of modules (or a least some clear sketch), I would accept this as a correct answer. Instead I will present a (somehow long) sketch  of what I did. I can provide more details upon request and I apologize if there are too many. My difficulty is on Step 5 . You can skip directly to this step if you want, the rest explains how I got there. The context :
Let $L/K$ be a finite Galois extension and let $M$ be an $L$ -module together we an isomorphism of $L \otimes_K L$ -modules $\phi: M \otimes_K L \to L \otimes_K M$ satisfying the cocyle condition $p_{13}^* \phi = p_{23}^* \phi \circ p_{12}^* \phi$ as isomorphisms of $L \otimes_K L \otimes_K L$ -modules. What I did : Step 1: Describing some isomorphisms We have an isomorphism of $K$ -algebras $L \otimes_K L \to \prod_{\sigma \in G} L$ given by $a \otimes 1 \mapsto ( a )_{\sigma \in G}$ and $1 \otimes a \mapsto ( \sigma(a) )_{\sigma \in G}$ and another one $ L \otimes_K L \otimes_K L \to \prod_{\sigma \in G} \Big( \prod_{\tau \in G} L \Big)$ given by $$
a \otimes 1 \otimes 1 \mapsto \Big( (a)_{\tau \in G} \Big)_{\sigma \in G},
$$ $$
1 \otimes a \otimes 1 \mapsto \Big( (\tau(a)_{\tau \in G} \Big)_{\sigma \in G},
$$ $$
1 \otimes 1 \otimes a \mapsto \Big( \tau\sigma(a)_{\tau \in G} \Big)_{\sigma \in G}.
$$ We can then describe the above isomorphisms as $$
(1_L \coprod \sigma) : L \otimes_K L \to \prod_{\sigma \in G} L
$$ and $$
(1_L \coprod \tau \coprod \tau \sigma): L \otimes_K L \otimes_K L \to \prod_{\sigma \in G} \Big( \prod_{\tau \in G} L \Big).
$$ Step 2: Obtain some $\prod_{\sigma \in G} L$ -module structures We then have a commutative diagram of modules. $$
\begin{array}{ccccc}
 M  \otimes_K L & \xrightarrow{} &  \prod_{\sigma \in G} \Big( M \otimes_K L \Big) & \xrightarrow{} & \prod_{\sigma \in G} M\\
\downarrow & & \downarrow & & \downarrow \\ 
L  \otimes_K M & \xrightarrow{} & \prod_{\sigma \in G} \Big( L\otimes_K M  \Big) & \xrightarrow{} &  \prod_{\sigma \in G} M^\sigma 
\end{array}
$$ where the left most vertical arrow is $\phi$ and we denote by $\psi$ the induced the right most vertical arrow. Using Step 1 we have ring morphisms $L \xrightarrow{1_L} L$ and $L \xrightarrow{\sigma} L$ for each $\sigma \in G$ . Tensoring $M$ with these morphisms give $L$ -module structures for $M \otimes_K L$ by $a \cdot ( m \otimes c ) = m \otimes ac$ and for $L \otimes_{L,\sigma} M$ by $a \cdot (c \otimes m) = \sigma(a)c \otimes m$ . Now the isomorphism of $L$ -modules $\mu: M \otimes_L L \to M: m \otimes c \mapsto cm$ then gives to $M$ the $L$ -module structure $a \otimes m = am$ . We also have the composite diagram $$
L \otimes_{L,\sigma} M \xrightarrow{ \sigma^{-1} \otimes 1_M } L \otimes_L M \xrightarrow{\mu'} M : c \otimes m \mapsto \sigma^{-1}(c) \otimes m \mapsto \sigma^{-1}(c)m.
$$ Then this $M$ has $L$ -module structure given by $a \cdot m := \sigma(a)m$ and we denote it by $M^\sigma$ (We can relabel to get $M^\sigma$ instead of $M^{\sigma^{-1}}$ .). Now a structure of $\prod_{\sigma \in G} L$ -module on $\prod_{\sigma \in G} M$ (resp. on $\prod_{\sigma \in G} M^\sigma$ ) is determined by an $L$ -module structure on $M$ (resp. on $M^\sigma$ ) for each $\sigma \in G$ . Therefore, if $(a_\sigma)_{\sigma \in G} \in \prod_{\sigma \in G}$ and $(m_\sigma)_{\sigma \in G} \in \prod_{\sigma \in G} M$ (resp. in $\prod_{\sigma \in G} M^\sigma$ ), then $$
(a_\sigma)_{\sigma \in G} \bullet (m_\sigma)_{\sigma \in G} = (~a_\sigma m_\sigma)_{\sigma \in G} ( \text{ resp. } (a_\sigma)_{\sigma \in G} \circ (m_\sigma)_{\sigma \in G} = ( \sigma(a)_\sigma m_\sigma)_{\sigma \in G}~).
$$ Step 3: Determine for each $\sigma \in G$ the isomorphisms of $L$ -modules $\psi_\sigma$ . The isomorphism $\psi$ induced by $\phi$ must then satisfy $$
\psi \big( a_\sigma m_\sigma)_{\sigma \in G} ) = (a_\sigma)_{\sigma \in G} \circ \psi( ( m_\sigma)_{\sigma \in G} ).
$$ For each $\sigma \in G$ we have an isomorphism of $L$ -modules $$
M \xrightarrow{\iota_\sigma} \prod_{\sigma \in G} M \xrightarrow{\psi} \prod_{\sigma \in G} M^\sigma \xrightarrow{\pi_\sigma} M^\sigma
$$ given by $$
am \mapsto (0, \cdots, 0, am, 0, \cdots, 0) \mapsto \big( \sigma(a) \pi_\sigma \Big( \psi( \iota_\sigma(m) \Big) \big)_{\sigma \in G} \mapsto \sigma(a)\pi_\sigma \Big( \psi(m) \Big).
$$ So for each $\sigma \in G$ we have an isomorphism $\psi_\sigma : M \to M^\sigma$ defined by $\psi_\sigma(m):=\pi_\sigma( \psi(m) )$ and such that $\psi_\sigma(am) = \sigma(a)\psi_\sigma(m)$ . Step 4: Determine some $\prod_{\sigma \in G} \prod_{\tau \in G} L$ -module structures I will skip some details, which I can provide upon request. I use the cocycle condition to determine three $\prod_{\sigma \in G} \prod_{\tau \in G} L$ -module structures. Consider $p_{12}^* p_1^* M$ (or equivalently $p_{13}^*p_1^*M$ ). The $\prod_{(\sigma,\tau) \in G \times G } L$ -module $\prod_{(\sigma,\tau) \in G \times G }  M$ is $$
(a_{g,h}) \cdot ( m_{g,h} ) = ( a_{g,h} m_{g,h} ).
$$ Consider $p_{12}^* p_2^* M$ (or equivalently $p_{23}^*p_1^*M$ ). The $\prod_{(\sigma,\tau) \in G \times G } L$ -module $\prod_{(\sigma,\tau) \in G \times G }  M^\tau$ is $$
(a_{\sigma,\tau}) \cdot ( m_{\sigma,\tau} ) = ( \tau(a_{\sigma,\tau}) m_{\sigma,\tau} ).
$$ Consider $p_{13}^* p_2^* M$ (or equivalently $p_{23}^*p_2^*M$ ). The $\prod_{(\sigma,\tau) \in G \times G } L$ -module $\prod_{(\sigma,\tau) \in G \times G }  M^{\sigma \tau}$ is $$
(a_{\sigma,\tau}) \cdot ( m_{\sigma,\tau} ) = ( (\sigma \circ \tau)(a_{\sigma,\tau}) m_{\sigma,\tau} ).
$$ Step 5: Determine the cocycle condition Finally, for each pair $(\sigma, \tau) \in G \times G$ we have three composite maps given as follows: $$
M_{(\sigma, \tau)} \xrightarrow{ \iota_{(\sigma,\tau)}} \prod_{(\sigma,\tau) \in G \times G} M \xrightarrow{ p_{12}^* \psi } \prod_{(\sigma,\tau) \in G \times G} M^\tau \xrightarrow{ \pi_{\sigma,\tau}} M_{(\sigma, \tau)}^\tau
$$ defining an $L$ -module isomorphism $ \psi_{ ( \sigma, \tau), (\sigma, \tau), \tau } : M_{(\sigma,\tau)} \to M_{(\sigma,\tau)}^\tau$ satisfying $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(am) = \tau(a)\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(m)
$$ $$
M_{(\sigma, \tau)} \xrightarrow{ \iota_{(\sigma,\tau)}} \prod_{(\sigma,\tau) \in G \times G} M \xrightarrow{ p_{13}^* \psi } \prod_{(\sigma,\tau) \in G \times G} M^{\sigma \tau} \xrightarrow{ \pi_{\sigma,\tau}} M_{(\sigma, \tau)}^{\sigma\tau}
$$ defining an $L$ -module isomorphism $ \psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma \tau } : M_{(\sigma,\tau)} \to M_{(\sigma,\tau)}^{\sigma \tau}$ satisfying $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(am) = (\sigma \circ \tau)(a)\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(m)
$$ and $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(am) = \tau(a)\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(m)
$$ $$
M_{(\sigma, \tau)}^\tau \xrightarrow{ \iota_{(\sigma,\tau)}} \prod_{(\sigma,\tau) \in G \times G} M^\tau \xrightarrow{ p_{23}^* \psi } \prod_{(\sigma,\tau) \in G \times G} M^{\sigma \tau} \xrightarrow{ \pi_{\sigma,\tau}} M_{(\sigma, \tau)}^{\sigma \tau}
$$ defining an $L$ -module isomorphism $ \psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma } : M_{(\sigma,\tau)}^\tau \to M_{(\sigma,\tau)}^{\sigma \tau}$ satisfying $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma }(am) = \sigma(a)\psi_{ ( \sigma, \tau), (\sigma, \tau), \tau }(m).
$$ Indeed, $\psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma }$ sends $\tau(a)m$ to $(\sigma \circ \tau)(a)m$ and so sends $am = \tau(\tau^{-1}(a))m$ to $\sigma( am )$ . Since $p_{23}^* \phi \circ p_{12}^*\phi = p_{13}^* \phi$ , we have $p_{23}^* \psi \circ p_{12}^*\psi = p_{13}^* \psi$ and therefore for each pair $(\sigma, \tau) \in G \times G$ we have $$
\psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma } \circ \psi_{ ( \sigma, \tau), (\sigma, \tau), \tau } = \psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma \tau }
$$ as isomorphisms of $L$ -modules. Remarks: The map $\psi_{ ( \sigma, \tau), (\sigma, \tau), \sigma }$ is from $M^\tau$ to $M^{\sigma \tau}$ and it is not clear to me how to re-express it as starting from $M$ and twisting it by $\tau$ . Another problem is that the maps I found on Step 3 are not in an obvious way related to those of Step 5 and there might be a need to twick something here as well.","['descent', 'algebraic-geometry', 'commutative-algebra']"
3258055,What property was used in this sine transformation?,"I have this expression: $$
ψ(χ) = A\sin^3(\frac{πχ}{α})
$$ And somehow the book i read equalizes the previous equation to this one: $$
ψ(χ) = \frac{A}{4}[3\sin(\frac{πχ}{α}) - \sin(\frac{3πχ}{α})]
$$ What trigonometric identity was used to make this possible?",['trigonometry']
3258095,How do you evaluate $\lim_{x\to\infty}(x!*e^{-x^2})$,"How do you evaluate $\lim_{x\to\infty} (x!*e^{-x^2})$ ? I know that $\lim_{x\to\infty} (x!*e^{-x}) = \infty$ because for large values of $x$ , $\frac{1*2*3...*(x-1)*x}{e*e*e*...*e*e}= \frac{\text{A lot of e's}}{\text{less e's}}=\infty$ .  In this case, everything in the numerator above $e$ contributes more than an $e$ , while everything in the denominator contributes exactly one $e$ . Is there a way of using the same type of reasoning for $\lim_{x\to\infty} (x!*e^{-x^2})$ ? I don't know how to approach this because there are different amounts of numbers in the numerator and denominator.","['limits', 'calculus']"
3258096,Prove that $5^n + 6^n -1$ is divisble by 10 for all positive integers n. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I know the base case is: $$ n=1,
5^1+6^1-1=10,10|10
$$ But I am struggling about how to go about the inductive step.","['induction', 'discrete-mathematics']"
3258097,How to calculate reletive weights (alpha) in Principal Component Analysis,"I am trying to work through the method described in this paper of calculating a normalcy index using principal component analysis. I understand steps 1-5, except for the equation for eigenvectors which is $\mathbf e_i = \sum_{j=1}^N \alpha_j^ix_j$ where $\alpha_j^i$ are ""relative weights determined solely by the orthonormality constraint."" The matrix containing $\mathbf x_j$ is a 10x8 matrix but the eigenvectors ( $\mathbf e_i$ ) are from the covariance matrix which is 8x8 (I did that part in MATLAB). Because there is a difference in matrix dimensions I can't divide $\mathbf e_i$ by $\mathbf x_j$ to find $\mathbf \alpha_j^i$ . I need $\mathbf \alpha_j^i$ to complete the next steps (as far as I can tell) including mutiplying it by another 10x8 matrix $\mathbf z_j$ . Please help me understand how I'm supposed to calculate $\mathbf \alpha_j^i$","['statistics', 'matlab', 'matrices', 'machine-learning', 'linear-algebra']"
3258102,"Why is an almost complex structure $J$ a tensor field of type $(1,1)$?","I've just read that for an almost-complex manifold $(M^{2n},J)$ the almost-complex structure $J:TM\to TM$ can be understood as a tensor field of type $(1,1)$ . I don't get it. According to the section ""Using tensor poducts"" from this wikipedia article, a tensor $T$ of type $(1,1)$ would belong to $V\otimes V^*$ , so in our case, $J_p\in T_pM\otimes T_pM^*$ . But $J_p$ is by definition a linear map from $T_pM$ to itself, so $J_p\in \text{End}(T_pM)$ . What am I missing?","['complex-geometry', 'almost-complex', 'tensor-products', 'differential-geometry']"
3258111,Strang's Perron–Frobenius Proof,"I'm stuck on the first paragraph in Strang's proof of (part of) the Perron–Frobenius theorem, from his Introduction to Linear Algebra . Why can we assume $t_{\text{max}}$ exists, what guarantees that a maximum of the $t$ 's ""is attained""? For a given nonnegative $\mathbf{x}$ , I see why there must be a maximum $t$ such that $A \mathbf{x} \geq t \mathbf{x}$ . We just start with $t = 0$ and increase it until $t x_i = (Ax)_i$ for some $i$ . But there might be many $\mathbf{x}$ that satisfy $A \mathbf{x} \geq t\mathbf{x}$ for some $t$ . And if there are infinitely many, there may be infinitely many maximal $t$ 's to choose from, one for each $\mathbf{x}$ . In which case I don't see how to guarantee that there's one $t_{\text{max}}$ to rule them all.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3258124,Proposition 5.4.4. in Tao,"I am trying to prove the following proposition in Tao's analysis textbook. For ever real number $x$ , exactly one of the following three statements is true: (a) $x$ is zero; (b) $x$ is positive; (c) $x$ is negative. A real number $x$ is negative if and only if $-x$ is positive. If $x$ and $y$ are positive, then so are $x + y$ and $xy$ . I am unsure on how to approach the first part. Tao defines real numbers as limits of Cauchy sequences of rationals, though without defining just yet what a limit is. He defines a positive real number as one that can be written as the limit of a Cauchy sequence of rationals positively bounded away from $0$ and a negative real number as one that can be written as the limit of a Cauchy sequence negatively bounded away from $0$ . We have a law of trichotomy for the rationals, which could be extended to every element of the sequence, perhaps, to say that, upon throwing out a finite number of terms, the sequence is either identically zero, positively bounded away from zero, or negatively bounded away from $0$ , and thus $x$ is either $0$ , positive, or negative. I am still unsure on how to formalize this, though, or whether I am on the right track. The second statement seems rather straightforward. If $x$ is negative it is negatively bounded away from $0$ : we have $x = \text{LIM}_{n \to \infty} a_n$ , and $\exists - c < 0$ (- $c$ rational) such that $a_n \leq -c$ , Therefore, $-x = \text{LIM}_{n \to \infty} -a_n$ , where we have, multiplying through by $-1$ , that $\exists c > 0$ such that $-a_n \geq c$ , meaning $x$ is positively bounded away from $0$ and is therefore positive. The opposite implication is similar. As for the third part: let $x = \text{LIM}_{n \to \infty} a_n$ and $y = \text{LIM}_{n \to \infty} b_n$ . $x$ and $y$ are positive, meaning $a_n$ and $b_n$ are positively bounded away from $0$ , so $\exists c > 0, a_n \geq c$ and $\exists d > 0, b_n \geq d$ . Thus, for any $n$ , $a_n + b_n \geq c + d$ , and since the positive reals are closed under addition, $c + d > 0$ and $a_n + b_n$ is also positively bounded away from zero, so $x + y$ is also positive. The fourth part is similar, but with a product, $cd$ , in lieu of a sum. Assuming I have not made a mistake or omission, I believe that I understand how to write the later parts of the problem, but the first part is still quite confusing to me. Any help or insights would be greatly appreciated.","['proof-explanation', 'real-analysis']"
3258125,Proving $-\frac{1}{a}<\int_a^b \sin(x^2) dx<\frac{1}{a}$,"I have encountered a question: Prove $$-\frac{1}{a}<\int_a^b \sin(x^2) dx<\frac{1}{a}$$ There are plenty of solutions to $\int_0^{\infty} \sin(x^2) dx$ online, but there seems to be no solution to the boundary of $\int_a^b \sin(x^2) dx$ . Could anyone help me with this please? I tried to calculate the integral directly, but I cannot cancel out b and get a boundary only with $a$ . Applying inequality to the integrand $\sin(x^2)<x^2$ does not work either.",['calculus']
3258199,Smooth submanifold of $\mathbb R^6$. Not smooth submanifold of $\mathbb R^3$,"So I'm pretty new to studying manifolds and have little to no background on differential geometry, but this is a question from lecture notes on a multivariable analysis unit: Show that $S:=\{(x^2,y^2,z^2,yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\}$ is a smooth 2-submanifold of $\mathbb R^6$ and show that the projection of $S$ onto the last three coordinates (The Roman Surface $R:=\{(yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\}$ ) is not a smooth 2-submanifold of $\mathbb R^3$ For the first part I tried to construct an atlas for $S$ , given by: $\phi_1: D \to S$ , where D is the open unit disc around the origin in $\mathbb R^2$ , such that $$(x,y) \mapsto (x^2,y^2,1-x^2-y^2,y\sqrt{1-x^2-y^2},x\sqrt{1-x^2-y^2},xy)$$ and $\phi_2$ and $\phi_3$ are defined similarly by isolating $x$ and $y$ , respectively, on $x^2+y^2+z^2=1$ Now what I would like to know is: 1) Is this enough to show that $S$ is a smooth 2-submanifold of $\mathbb R^6$ ? 2) If the answer to 1) is yes, then, is this generally the best way to prove that a subset is a submanifold? 3) For the second part I have no idea how to show something is not a submanifold and would appreciate any help on the matter","['submanifold', 'smooth-manifolds', 'multivariable-calculus', 'manifolds', 'differential-geometry']"
3258217,Are $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ isomorphic?,"I know $\mathbb Q$ is not isomorphic to $\mathbb Q\times \mathbb Q$ as groups. But is it true for $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ ? That is, are $\mathbb Q\times\mathbb Q$ and $\mathbb Q\times\mathbb Q\times\mathbb Q$ isomorphic as groups?  Where $\mathbb Q$ is set of rational numbers.","['group-theory', 'abstract-algebra']"
3258252,Fundamental group of punctured simply connected subset of $\mathbb{R}^2$,"Let $S$ be a simply connected subset of $\mathbb{R}^2$ and let $x$ be an interior point of $S$ , meaning that $B_r(x)\subseteq S$ for some $r>0$ .
Is it necessarily the case that $\pi_1(S\setminus\{x\})\cong\mathbb{Z}$ ? Let $B=B_r(x)$ and let $G=\pi_1(S\setminus\{x\})$ .
I can show that $G^{ab}\cong\mathbb{Z}$ .
Consider the commutative diagram of topological spaces $$
\newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex}
\newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}}
\begin{array}{c}B\setminus\{x\}&\ra{}&S\setminus\{x\}&\ra{}&\mathbb{R}^2\setminus\{x\}\\\da{}&&\da{}\\B&\ra{}&S\end{array}
$$ Applying the fundamental group functor gives a commutative diagram of groups $$
\begin{array}{c}\mathbb{Z}&\ra{}&G&\ra{}&\mathbb{Z}\\\da{}&&\da{}\\1&\ra{}&1\end{array}
$$ where the composition of the top two maps is the identity homomorphism on $\mathbb{Z}$ .
By the Seifert-van Kampen theorem, the square is a pushout diagram of groups, meaning that the normal closure of the image of $\mathbb{Z}\to G$ is all of $G$ .
In particular, if $A$ is an abelian group then any two homomorphisms $G\to A$ that agree on the image of $\mathbb{Z}\to G$ must agree on all of $G$ .
Another way to put this is that if we have two homomorphisms $G\to A$ such that the two compositions $\mathbb{Z}\to G\to A$ are equal then the two homomorhpisms $G\to A$ are equal. I claim that the map $G\to\mathbb{Z}$ is an abelianization map.
To see this, let $A$ be an abelian group and let $G\to A$ be a homomorphism.
Now recall that the composition $\mathbb{Z}\to G\to\mathbb{Z}$ is the identity.
Then the composition $\mathbb{Z}\to G\to\mathbb{Z}\to G\to A$ agrees with the composition $\mathbb{Z}\to G\to A$ .
By the remark at the end of the previous paragraph, this means that the composition $G\to\mathbb{Z}\to G\to A$ agrees with the map $G\to A$ .
In other words, the composition $\mathbb{Z}\to G\to A$ makes the abelianization diagram commute. To show uniqueness, let $\mathbb{Z}\to A$ be a map making the abelianization diagram commute.
Then the composition $G\to\mathbb{Z}\to A$ agrees with the map $G\to A$ .
Then the composition $\mathbb{Z}\to G\to\mathbb{Z}\to A$ agrees with the composition $\mathbb{Z}\to G\to A$ .
Since the composition $\mathbb{Z}\to G\to\mathbb{Z}$ is the identity, this shows that the map $\mathbb{Z}\to A$ is given by the composition $\mathbb{Z}\to G\to A$ . This shows that the map $G\to\mathbb{Z}$ is an abelianization map.","['group-theory', 'fundamental-groups', 'algebraic-topology']"
3258297,How to proceed with this Sequence question,"Let $a,b$ be given positive integers such that $a<b$ . Let $M(a,b)$ be
  defined as $$ \frac{\sum_{k=a}^b     \sqrt{k^2+3k+3}}{b-a+1}. $$ Evaluate $[M(a,b)]$ where $[.]$ represents greatest integer function. I have tried to factorise the term inside the square root however I believe it was pretty useless. I have a feeling that it might(?) telescope but I have no clue what to do here. Any help will be appreciated","['algebra-precalculus', 'sequences-and-series']"
3258309,"How is Wolfram Alpha and the reduction formula arriving at a different result for the integral of $\int \sec^4 x\,dx$ than naive $u$-substitution?","I calculated the following on paper for the value of $\int \sec^4 x\,dx$ . $$\int \sec^4 x\,dx=\int \sec^2 x \sec^2 x\,dx=\int (\tan^2 x + 1)(\sec^2 x)\,dx.$$ Let $u = \tan x$ , $du = \sec^2 x\,dx$ so \begin{align}\int \sec^4 x\,dx&=\int u^2 + 1\,du\\&=\frac{1}{3} u^3 + u + C\\&=\frac{1}{3} \tan^3 x + \tan x + C\\&=\frac{1}{3} (\tan x)(\tan^2 x + 1) + C\\&=\frac{1}{3} \tan x \sec^2 x + C\end{align} Wolfram Alpha, however, gives $\int \sec^4(x)\,dx = \frac13(\cos(2 x) + 2) \tan(x) \sec^2(x) + C$ . This is notably not equal to my solution. According to the ""step-by-step solution"" from the Wolfram Alpha app, the reduction formula was used to produce $$\frac{1}{3}\tan x \sec^2 x + \frac{2}{3} \int \sec^2 x \,dx$$ then $$\frac{2}{3} \tan x + \frac{1}{3} \tan x \sec^2 x + C$$ Why does the reduction formula produce this added term compared to naive $u$ -substitution?","['indefinite-integrals', 'calculus', 'substitution']"
3258346,"$ \sum_{1 \le i < j \le n} a_{i} a_{j} \ge n(n-1)/2 $ , prove that $a_{1} + ... + a_{n} \ge n$ for $n \ge 2$ using AM-QM","Let $a_{1}, a_{2}, ...$ be a sequence of positive real numbers. Let the following relation holds: $$ a_{k+1} \ge \frac{k a_{k}}{a_{k}^{2} + (k-1)}, \:\: k \ge 1$$ Prove that $ S_{n} = a_{1} + a_{2} + ... + a_{n} \ge n $ , for $n \ge 2$ . Solution: I have posted this question before and solved it using 2 approaches, ( Given $ a_{k+1} \ge \frac{k a_{k}}{(a_{k}^{2} + k-1)}, \:\: k > 0$, prove $ S_{n} = a_{1} + .. + a_{n} \ge n, \:\: n \ge 2 $ ). this time I would like to solve it using another approach, the hint is that $ \sum_{1 \le i < j \le n} a_{i} a_{j} \ge n(n-1)/2 $ and then use AM-QM inequality. Here is my attempt: $$ \sum_{1 \le i < j \le n} a_{i} a_{j} =  \sum_{j=2} S_{j-1} a_{j} $$ by using a result in my previous post that $S_{m} \ge m/a_{m+1}$ we have $$ \sum_{1 \le i < j \le n} a_{i} a_{j} =  \sum_{j=2}^{n} S_{j-1} a_{j} \ge \sum_{j=2}^{n} (j-1) = n(n-1)/2 $$ the hint is proved, then: $$ \sum_{j=2}^{n} S_{j-1} a_{j} \le S_{n-1} \sum_{j=2}^{n}  a_{j} $$ then by AM-QM: $$S_{n-1} \sum_{j=2}^{n}  a_{j} \le S_{n-1} \sqrt{n \sum_{i=2}^{n} a_{i}^{2}} \le S_{n-1} \sqrt{n} \sqrt{(S_{n}^{2})} = \sqrt{n} S_{n-1} S_{n}$$ so $$ \sqrt{n} S_{n-1} S_{n} \ge n(n-1)/2$$ if we use induction, by assuming $S_{n-1} \ge n-1$ then we can have $$ S_{n} \ge \sqrt{n}/2$$ This is as far as i have gone.","['contest-math', 'algebra-precalculus', 'inequality', 'sequences-and-series']"
3258368,"A question on ""generating graphs""","Suppose $H$ is a group. $X \subset H$ , $|X| < \infty$ . Let’s define $GG(H, X)$ (generating graph of $H$ ) as a finite undirected simple graph $\Gamma(V, E)$ , such that $V = X$ , $E = \{(x, y) \in X \times X| x \neq y, \langle x, y \rangle = H\}$ . $GG(H, X)$ has following properties (which are relatively obvious): If $K < H$ , then $GG(K, K \cap X)$ is a subgraph of the complementary graph of $GG(H, X)$ If $\phi: H \to K$ is a surjective group homomorphism, such that $\phi|_X$ is injective. then $GG(K, \phi(X))$ contains $GG(H, \phi(X))$ as a subgraph. $Stab(X, Aut(H)) \leq Aut(GG(H, X))$ , where $Stab(X, Aut(H)) = \{\phi \in Aut(H)| \phi(X) = X\}$ My question however is: Does for any finite simple undirected graph $\Gamma$ , there exist such a group $H$ and its finite subset $X$ , that $\Gamma \cong GG(H, X)$ ? I failed to construct such $H$ and $X$ for arbitrary $\Gamma$ , however a counterexample does not come to my mind either.","['graph-theory', 'finitely-generated', 'abstract-algebra', 'combinatorics', 'group-theory']"
3258401,How would the most general $2 \times 2$ normal matrix look like?,"How would the most general $2 \times 2$ normal matrix look like? The normal matrix satisfy equation: $A^*A=AA^*$ where $A^*$ denotes
  conjugate transpose. I was thinking about the matrix: $$
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix}
$$ because its columns are orthogonal to each other and it satisfies the given equation: $$
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix}
    \begin{pmatrix}
    a & b  \\
    -b & a  \\
    \end{pmatrix} = 
    \begin{pmatrix}
    a^2 + b^2 & 0  \\
    0 & b^2 + a^2  \\
    \end{pmatrix}
$$ $$
$$ $$
    \begin{pmatrix}
    a & b  \\
    -b & a  \\
    \end{pmatrix}
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix} = 
    \begin{pmatrix}
    a^2 + b^2 & 0  \\
    0 & b^2 + a^2  \\
    \end{pmatrix}
$$ $$
$$ It is true for real matrices, and I suppose for the complex one too. But is this the most general case, or is there something else?","['matrices', 'linear-algebra', 'orthonormal']"
3258412,How to solve $\cos(\sin^{-1}(-3/5))$?,I'm stuck with question $$\cos\Bigl(\sin^{-1}\Bigl(-\frac35\Bigr)\Bigr)$$ I looked for the answer in the book and it is $\frac45$ I tried solving it using the formula $\sin^2x+\cos^2x=1$ and I also got $\frac45$ as the answer but I got it by inputting the value of $\cos x$ in the given question in place of $$ \sin^{-1}\Bigl(-\frac35\Bigr)$$ But I cant see any logic there. Please explain it to me.,['trigonometry']
3258442,"Continuity and differentiability of $g(x,y)=\frac{x^2+y^2}{\phi_{(x,y)}}$ at the origin.","For $(x,y)\in \mathbb{R}^2,~(x,y)\neq (0,0)$ set $\theta_{(x,y)}\in [0,2\pi)$ the angle between positive x axis and the line connecting $(0,0)$ and $(x,y)$ and $$\phi_{(x,y)}= \left \{\begin {array}{ll}
\theta_{(x,y)} &,  \theta_{(x,y)} \in [0,\pi)\\
\theta_{(x,y)}-\pi &, \theta_{(x,y)}\in [\pi,2\pi)
\end{array}
\right..$$ Let $$g:\mathbb{R}^2\to \mathbb{R}: g(x,y)= \left \{\begin {array}{ll}
\frac{\|(x,y)\|^2}{\phi_{(x,y)}} & , (x,y) \neq (0,0),~\phi_{(x,y)}\neq 0\\
0 &, otherwise
\end{array}
\right..$$ Determine if $g$ is continuous and differentiable at the origin. Attempt. Since $\phi(x,0)=0$ and $\phi(0,y)=\frac{\pi}{2}$ for all $x,\,y\neq 0$ we get: $$g_x(0,0)=\lim_{x\to 0}\frac{g(x,0)-g(0,0)}{x-0}=\lim_{x\to 0}\frac{0-0}{x-0}=0$$ and $$g_y(0,0)=\lim_{y\to 0}\frac{g(0,y)-g(0,0)}{y-0}=\frac{2}{\pi}\lim_{y\to 0}y=0.$$ I believe that $g$ is continuous at the origin, so I need to prove that $g(x,y)\leqslant h(x,y)$ for some h having limit equal to $0$ at the origin. But for $\phi_{(x,y)}$ being nonzero but small, how can we control $\frac{1}{\phi_{(x,y)}}$ ? Regarding differentiability, we need to test if the limit: $$\lim_{(x,y)\to (0,0)}\frac{g(x,y)}{\sqrt{x^2+y^2}}$$ equals zero. Thanks for the help.","['limits', 'multivariable-calculus', 'analysis', 'real-analysis']"
3258462,Calculate theoretical quantiles with calculator (qq-plot),"Let's say we have the following data: $-1.8, -0.82, 0.3, 1.2, 1.6$ Now I want to make a qq-plot out of it by hand, just with a calculator (Casio fc 991). I start by sorting the values in ranks j and calculate how many observations are less than or equal to $x(j)$ by $j* = \frac{j-0.5}n$ . This brings us following values $j*$ : $0.1, 0.3, 0.5, 0.7, 0.9$ These are my sample quantiles, right? To get my qq-plot I now want to plot these against the theoretical quantiles. But here I stuck. The theoretical quantiles are: $-1.28, -0.52, 0.00, 0.52, 1.28$ But how do I calculate these values without R or any software but just with a calculator? I got the following formula: $(\phi^{-1})(j*)$ but I simply do not understand how I should come up with those values. Cheers and thanks for your effort!","['statistics', 'quantile', 'normal-distribution']"
3258470,How to define canonical bundle of a complex orbifold?,"I have been told that the canonical divisor of an orbifold is defined by closing the canonical divisor of the smooth locus. However, since a real orbifold has singularities of codimension at least two, a complex orbifold can have a singular locus complex codimension 1, so it might contribute components to the divisor. How then should we define the canonical bundle?","['complex-geometry', 'algebraic-geometry', 'differential-geometry']"
3258555,"Tangent bundle of product manifold, explicit formula","I know that there is an isomorphism $$T(M\times N) \simeq \pi_M^* TM \oplus \pi_N^*TN. $$ However, I would be interested in equality, not in isomorphisms. I've read somewhere that $$T(M\times N) = TM \times N \oplus M \times TN.$$ Is this really true? It does not look right to me to take  products of manifolds and vector bundles. Maybe there is some abuse of notation going on? Shouldn't the tangent bundle be something like $$T(M\times N) = \ker d\pi_M \oplus \ker d\pi_N, $$ with $\pi_M, \pi_N$ the projections?","['tangent-bundle', 'geometry', 'differential-geometry']"
3258629,Example diffeomorphism of a rectangle onto itself,"The Wikipedia article for Diffeomorphism has the below picture of an example diffeomorphism of a square onto itself, but it does not seem to specify the specific mapping used to create it.  Obviously, I don't need to know the exact mapping used to make the example image, but what would be a similar diffeomorphism that would map an arbitrary rectangle in $\mathbb{R}^2$ centered at the x,y origin onto itself and would produce a somewhat similar style of grid distortion?  What would the inverse then be? The application I'm trying implement is pretty much exactly what the image is: distorting a rectangular grid in different ways such that it can be inverted.  I'm just trying to find a concrete example as a starting point as I'm am still rather unfamiliar with this area of mathematics.  So pardon any misunderstandings I may be having.","['group-theory', 'diffeomorphism', 'real-analysis']"
3258713,Differential as bundle map and pull back bundle,"In the wikipedia article about the pushforward , it is stated that if $f: M\to N$ is smooth, then it induces a bundle map $df: TM \to TN$ . It is then claimed that equivalently $f_*=df$ is a bundle map from $TM$ to the pullback bundle $f^* TN$ . Why is this equivalent? The bundles $TN$ and $f^* TN$ are clearly not the same as they are bundles over different spaces. They could be isomorphic, but is still seems strange that this would hold independently of $f$ . Edit: The full quote is Equivalently (see bundle map), φ∗ = dφ is a bundle map from TM to the pullback bundle φ∗TN over M, which may in turn be viewed as a section of the vector bundle Hom(TM, φ∗TN) over M. The bundle map dφ is also denoted by Tφ and called the tangent map. In this way, T is a functor.",['differential-geometry']
3258727,Intuition on the direction of steepest ascent always being orthogonal to the level set of the function,"Thanks for reading. THE QUESTION: Convince me that when on the surface of a smooth hill, the $(x,y)$ direction I should take a tiny step in such that my current height doesn't change is always perpendicular to the $(x,y)$ direction I should take a tiny step in so that my height changes by the most. More Mathematically formulated: Convince me, intuitively, that the direction of steepest ascent is perpendicular to the level-set of a function. Convince me, intuitively, that if I""m standing on a smooth hill, the direction of steepest ascent is perpendicular to the direction I should move in so that the height doesn't change at all. Why I'm asking it: (This section is going to be really long, but just because I want to be helpful to potential responders and explain exactly what I understand and what I don't understand in as much depth as possible. If you read it all, thank you so much!) I've always had trouble understanding that the gradient is the direction of steepest ascent. I've seen some excellent answers on this site, like this one... Why is gradient the direction of steepest ascent? ...and this one... Gradient of a function as the direction of steepest ascent/descent ...and honestly, most answers seem to answer in the same way: by proving that the dot product of a vector of fixed length with the gradient, which by definition is the change in the function at that point, is maximum when the vector of fixed length (the step) points in direction of the gradient. That answer is fine...but I've always had a little bit of trouble understanding it. That's because although the phrase "" ...take the step that points in the direction of the gradient to maximize the dot product between the step's direction and the gradient..."" is mathematically sound, the idea of ""the direction"" of the gradient isn't something I""m really comfortable with, since I view the gradient as an operator on a vector $\begin{bmatrix}
dx\\ 
dy
\end{bmatrix}$ that outputs by how much some $f(x,y)$ would change at some specific $(x,y)$ if we took that ""step"". It's hard for me to think of the gradient as a vector itself. So yea, I've never really truly understood the ""direction of steepest ascent"" of a function. However, something I DO understand is the level-sets of a function. These are all the $(x,y)$ points such that some $f(x,y)$ stays constant. For example, if $f(x,y)=x+2y$ , then $(x+2y)=1$ would be a level-set. In the picture above, the red plane is $z=f(x,y)$ , and the green plane is $(x+2y)=1$ . As you can see, the intersection of the two planes is flat, indicating that $f(x,y)$ is constant for all $(x,y)$ such that $(x+2y)=1$ . Now, say I was standing on that intersection, where $z=1$ , and I wanted to know which $(x,y)$ direction to take a step in so that I didn't move up or down the mountain? I would need to move in a $(x,y)$ direction such that $(x+2y)$ stayed constant. Say I take a tiny step in some arbitrary direction. That step will have an $x$ component and a $y$ component. We can represent that tiny step as a vector: $\begin{bmatrix}
dx\\ 
dy
\end{bmatrix}$ . For whatever tiny amount $dx$ that step corresponds to in the $x$ direction, $f(x,y)$ (my height) will change by $dx$ , since at that $(x,y,f(x,y))$ point I'm standing on on that smooth mountain, $\frac{\partial f}{\partial x}=1$ . On the other hand, for whatever tiny amount $dy$ that step corresponds to in the $y$ direction, $f(x,y)$ (my height) will change by $2dy$ , since at that $(x,y,f(x,y))$ point I'm standing on on that smooth mountain, $\frac{\partial f}{\partial y}=2$ . In general, at any $(x,y,f(x,y))$ , the amount by which $f(x,y)$ changes when I take a tiny step $\begin{bmatrix}
dx\\ 
dy
\end{bmatrix}$ is the amount by which it changes due to the component of our step in the $x$ direction, which would be $\frac{\partial f}{\partial x} * dx$ , plus the amount that it changes in due to the component of our step in the $y$ direction, which would be $\frac{\partial f}{\partial x} * dy$ . In this specific example, the function changes twice as much for any step in the $y$ direction than it does for any step in the $x$ direction. That means that if I don't want $f(x,y)$ to change at all, then for whatever amount I move in the $y$ direction, I must move negative twice that amount in the $x$ direction, since any fixed amount of movement in the $y$ direction corresponds to twice the change in height as does any movement in the $x$ direction! In other words, the direction of my step should be: $\begin{bmatrix}
-2\\ 
1
\end{bmatrix}$ . Let's say I was instead standing at an $(x,y,f(x,y))$ point where a tiny step in the $x$ direction corresponded to 42 times the change in altitude than a tiny step in the $y$ direction did. In other words, $\frac{\partial f}{\partial x}=42\frac{\partial f}{\partial y}$ at that point. Then, to not change height at all (stay on the level-set) , I would want to take a tiny step in the $\begin{bmatrix}
1\\ 
-42
\end{bmatrix}$ . I'd want to make sure that my step moves me $-42$ times as much in the $y$ direction as we do in the $x$ . More generally, if I'm standing at some point $(x,y,f(x,y))$ on a smooth mountain, the step I should take such that my altitude doesn't change (such that $f(x,y)$ doesn't change) should always be $\begin{bmatrix}
+\frac{\partial f}{\partial y}\\ 
-\frac{\partial f}{\partial x}
\end{bmatrix}$ This makes sense to me - no dot products needed so far!!!! Now, I know that the direction orthogonal to $\begin{bmatrix}
+\frac{\partial f}{\partial y}\\ 
-\frac{\partial f}{\partial x}
\end{bmatrix}$ corresponds to taking the negative reciprocal of it. That is: $\begin{bmatrix}
\frac{\partial f}{\partial x}\\ 
\frac{\partial f}{\partial y}
\end{bmatrix}$ AND THAT'S THE DIRECTION OF STEEPEST ASCENT! In summary, I understand why the ""direction of no ascent"" is what it is. If I could somehow intuitively understand that the ""direction of steepest ascent"" when climbing a mountain is always perpendicular to the direction of no ascent, then I would understand why the gradient is in the direction of steepest ascent. Thanks! One more thing... I tagged this question as a soft question simply because I'm looking for intuitive answers more than mathematical proofs, and it's hard to say whether or not intuitive answers are correct. Copied and pasted from a comment below... I'd like to be able to picture myself standing on the surface of a smooth hill, standing over a spot where someone took a bright neon marker and traced out a level-curve on that hill, and picture the hill in such a way that the direction in which the hill is steepest is OBVIOUSLY perpendicular to that hill. And as of now, I just can't! It seems just as plausible that some OTHER direction not perpendicular to that bright yellow level-curve could be the steepest direction instead!","['geometry', 'multivariable-calculus', 'intuition', 'vector-analysis', 'soft-question']"
3258743,How many quotients/remainders can we get for a given Gaussian integer?,"This is a question which arose 1 from division of Gaussian integers . However, it seems to be basically a question about lattices in $\mathbb R^2$ . We know that if we consider $\mathbb Z[i]=\{a+bi; a,b\in\mathbb Z\}$ then we have a division with remainder, that is, for any $a,b\in\mathbb Z[i]$ with $b\ne 0$ there exist a quotient $q$ and a remainder $r$ such that $$a=qb+r, \qquad |r|<|b|.\tag{1}$$ This is basically the statement that $\mathbb Z[i]$ is an Euclidean domain (with norm $N(z)=|z|^2$ ). The equation $(1)$ is equivalent to $$\frac ab=q+\frac rb, \qquad \left|\frac rb\right| <1. \tag{2}$$ So we are basically asking whether for a given complex number (in fact, a Gaussian rational ) $\frac ab$ there is a Gaussian integer with distance at most one. 2 The following picture shows circles with radius one around each lattice point from $\mathbb Z[i]$ , to find $q$ we basically check whether $\frac rb$ belongs to one of these circles. We can see that the $q$ is not necessarily uniquely determined. Depending on the position of $\frac rb$ in the squares determined by $\mathbb Z[i]$ we can have up to four possibilities. Question. For a given $b\in\mathbb Z[i]$ , what are the numbers of possibilities we can get for $q$ ? For example, what is the characterization of $b$ 's such that, depending on $a$ , we can have either one or four possible choices for $q$ in $(1)$ ? What is the characterization of $b$ 's such that for some $a$ 's we might have only one choice, for other $a$ 's we can get two choices, and there are also $a$ 's with for possible choices for $q$ ? What are $b$ 's such that (depending on $a$ ) we might get one, two, three or four choices for $b$ ? If we look at the interpretation explained in $(2)$ , we are basically asking whether there are linear combinations of $\frac1b$ and $\frac ib$ which belong to areas denoted by $1$ , $2$ , $3$ , $4$ of some square in the grid. Let us have a look at some special cases: If $r=0$ , i.e. $\frac ab \in \mathbb Z[i]$ , then there is only one possibility how to choose $q$ . (And this is the only situation where we have single possibility.) Let us take $b=1-i$ for example, meaning that $\frac1b=\frac12(1+i)$ . We see that any multiple of $\frac1b$ by a Gaussian integer is either in the center of some of the squares or in some of it's vertices. So we see that we always get one or four possible remainders. (For example, $1=0\cdot(1-i)+1=1\cdot(1-i)+i=i\cdot(1-i)-i=(1+i)\cdot(1-i)-1$ .) If we take $b=2$ , this corresponds to multiples of $\frac12$ . So we get one , two or four possible remainders depending on $a$ . (For $a=1$ we have two possibilities, $1=0\cdot2+1=1\cdot2-1$ . For $a=1+i$ we have $1+i=0\cdot2+(1+i)=1\cdot2+(-1+i)=i\cdot2+(1-i)=(1+i)\cdot2+(-1-i)$ .) It seems plausible that if $|b|$ is large enough (to make multiples of $1/|b|$ rather dense) and $b$ isn't in some of the special directions (such as multiples of $1$ or $1+i$ by some unit, i.e., by $\pm1$ or $\pm i$ ) then we can hit all possible areas and there will be $a$ 's with one, two, three and four choices for $q$ . If the intuition suggested in the previous bullet point is correct, it seems that there will be finitely many exceptions, but typically we should get all four possibilities. However, it does not seem to be quite straightforward to get some estimate on what ""dense enough"" and ""special directions"" should be in this context. Moreover, ""dense enough"" (as used in the previous bullet point) seems to be dependent on the direction of the vector corresponding to $\frac1b$ . So it's quite possible that there is not some simple characterization - or that entirely different way of looking at the problem is needed. 1 I thought about this question after somebody recently asked in chat about non-uniqueness of division in Gaussian integers. (As they mentioned, they've stumbled upon a remark about this in Artin's algebra.) 2 This is actually the approach usually taken in the proof, see: Prove that the Gaussian Integer's ring is a Euclidean domain","['euclidean-domain', 'gaussian-integers', 'integer-lattices', 'geometry']"
3258747,Bundle over circle; Monodromy action on cohomology of fiber,"I want to understand the following sentence: Let $\pi: M \to S^1$ be a fiber bundle with path connected fiber $F$ .
  Then its monodromy action on $H^k(F; \mathbb C)$ satisfies... How is the monodromy action defined? Its clear that the idea is to look at the standard loop in $S^1$ and investigate how a cohomology class changes along this loop. But I cannot see how to do this. So how is the monodromy action defined? How can I calculate it and where can I read more about it? Thank you for your help.","['fiber-bundles', 'fundamental-groups', 'homology-cohomology', 'algebraic-topology', 'differential-geometry']"
3258767,"Are there ""extremely non-commutative groups"" $G\neq\{e\}$ such that $gh \neq hg$ for all $g,h\in G$ with $g\neq e$ and $h\notin \{e,g,g^{-1}\}$?","Let's call a group $G\neq\{e\}$ ""extremely non-commutative"" if $$ gh \neq hg~~\text{for all}~g\in G\setminus\{e\}~\text{and}~h\in G\setminus \{e,g,g^{-1}\}.$$ Do such groups exist? If so, do they have any interesting/important properties? Or can one always find at least one non-trivial pair of group elements which commute?","['group-theory', 'abstract-algebra']"
3258775,$T \in \mathcal L (V)$ has no real eigenvalues. Prove that every subspace of $V$ invariant under $T$ has even dimension.,"Suppose $V$ is a real vector space and $T \in \mathcal L (V)$ has no real eigenvalues. Prove that every subspace of $V$ invariant under $T$ has even dimension. Solution : Suppose $U$ is a subspace of $V$ that is invariant under $T$ . If $\dim U$ were odd, then $T|_{U}$ would have an eigenvalue $\lambda \in \Bbb R$ . $\exists v \neq 0, v\in U$ such that $T|_{U} u = \lambda u$ .Then $\lambda$ is an eigenvalue of $T$ .But $T$ has no eigenvalues, so $\dim U $ must be even. Why that happened when $T|_{U}$ has odd dimension?","['invariant-subspace', 'linear-algebra', 'eigenvalues-eigenvectors']"
3258798,How to approach the proof of this formula for triangulations.,"I am currently working on an assignment for my discrete mathematics lecture.
It is specifically about graphs which are triangulations and have a minimum degree of 3. Triangulation specifically means here, that every area in the graph is bordered by exactly 3 edges. The formula that is to be proven is the following: Let $v_i$ be the number of vertices of degree $i$ . Show that the following is true: $3 v_3 + 2 v_4 + v_5 = v_7 + 2 v_8 + 3 v_9 + ... + (X - 6) v_X + 12$ Whereas $X$ is the maximum degree of the graph in question. Now, I am specifically supposed to use the Euler Formula, that is $|V| - |E| + g = 2$ I can also calculate the number of edges and areas relative to each other, as I know that every area is bordered by 3 edges and every edge is the border between 2 areas. That means that I know that $3|E| = 2g$ . But I don't know how to go on from here. I specifically struggle with the fact, the the formula doesn't just count vertices, but also differentiates them based on their degree. I don't see how to get there from the Euler formula. I would appreciate any tips that could lead me into the right direction! Thanks for reading! Edit: I have just now realized, that the graph is maximally planar, being a triangulation. Now I can use the fact that $|E| = 3|V| - 6$ , but I am not sure where that leads me. I feel like the $-6$ in there might be useful to get to the formula from above, but I can't seem to make the connection, if it even is actually there.","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
3258811,Solving $f(x)f(y) = f(x + y)$ [duplicate],"This question already has answers here : How do I prove that $f(x)f(y)=f(x+y)$ implies that $f(x)=e^{cx}$, assuming f is continuous and not zero? (3 answers) Real Analysis Continuous Function Problem [duplicate] (2 answers) Closed 5 years ago . I am a little lost trying to derive what form $f(x)$ must have if we know $f(x)f(y) = f(x + y)$ for real inputs $x, y$ . My attempt so far: Set $y=0$ and we have $f(x)f(0) = f(x)$ meaning either $f(x) = 0$ or $f(0) = 1$ . Not sure what to do with this. What about setting $y=x$ ? Then $f(x)^2 = f(2x)$ . Multiply both sides by $f(x)$ and then $f(x)^3 = f(2x)f(x) = f(2x + x) = f(3x)$ and so on, so $f(x)^n = f(nx)$ for some integer $n \geq 2$ . But it's also true for $n=1$ because $f(x)^1 = f(1 \cdot x) = f(x)$ and it's also true for $n=0$ (if we assume $f(0) = 1$ ) since $f(x)^0 = f(0 \cdot x) = f(0) = 1$ , so $f(x)^n = f(nx)$ holds for integer $n \geq 0$ . For $n > 0$ : raise both sides to $1/n$ and we get $f(x) = f(nx)^{1/n}$ I don't really know where I am going with this or if it's even the right track. Am I even allowed to do that in the first place? Am I supposed to be assuming $f(x)$ is real? Or complex? Or positive? Or something? Should I be assuming $x$ and $y$ are complex? I don't really know what assumptions to make exactly. I'm just trying to prove/show that this all implies $f(x)$ has some exponential form but pretending I don't know that yet. Could use any corrections or a push in the right direction.","['number-theory', 'functions', 'proof-explanation']"
3258885,Prove there exists some $x_0$ for a differentiable function,"A real-valued function $f$ is defined and differentiable on $[a,b]$ ( $b-a\geq{4}$ ).
Prove that there exists $x_0 \in (a,b)$ for which $f'(x_0)<1+f^2(x_0)$ On the one hand, the statement resembles very much the classical theorem by Lagrange according to which there exists some $\varepsilon$ for which $f'(\varepsilon)(b-a)=f(b)-f(a)$ Nevertheless, what we have here is an inequality, which is more tricky. 
The limitation ( $b-a\geq{4}$ ) makes me think that using trigonometry, in this case, might be a good approach, but I have no idea how exactly that could be used. Thanks in advance for any hints.","['calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3258888,Conformal automorphism of unit disk that interchanges two given points,"Let $a$ and $b$ be distinct points in the unit disk $D$ . Show that there exists a conformal automorphism $f$ of $D$ that interchanges $a$ and $b$ ; that is, $f(a) = b$ and $f(b) = a$ . Idea: we know that $g(z)=\frac{\alpha-z}{1-\bar{\alpha}z}$ interchanges $0$ and $\alpha$ and by composition we can find out the map $f(a) = b$ for any $a$ and $b$ in the unit disk $D$ . But how can I get the other way by the same map? Thanks.","['complex-analysis', 'conformal-geometry', 'automorphism-group', 'mobius-transformation']"
3258903,Norm of sum of self-adjoint operators,"Let $L$ and $K$ be such self-adjoint operators on a Hilbert space, such that we have $LK=0$ . Show that for the operator norm, we have the equality $$\left\|L+K\right\|=\max\left\{\left\|L\right\|, \left\|K\right\|\right\}.$$ I was able to prove that $\left\|L\right\|\leq \left\|L+K\right\|$ and $\left\|K\right\| \leq \left\|L+K\right\|,$ but I cannot prove the other inequality.","['hilbert-spaces', 'functional-analysis']"
3258917,What is the use or importance of continuity and differentiability? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question In a lot of mathematical proofs I often see things like ""Assume $f$ is continuous"" or ""Assume $f$ is differentiable"" and sometimes I've even seen both ""Assume $f$ is continuous and differentiable"", though I believe (differentiability implies/requires continuity). What is the use or utility of this? Under what kind of circumstances, going into a problem or framework, would we want something to be continuous or differentiable? Continuous I can understand as a kind of ""useful for things that don't have sudden jumps out of nowhere"" but when would we want something to be differentiable as well? For instance when reading about lots of probability curves I often see that these curves are defined up front as both continuous and differentiable. Why? What pushes us to start off with these definitions? What's the motivation? What do we ""lose"" if we do away with these assumptions?","['real-analysis', 'continuity', 'definition', 'derivatives', 'soft-question']"
3258925,Why do so many half-circle perimeters contribute $0$ to the answer in contour integration?,"Oftentimes, when one is calculating the integral over the real line for an irritating integrand, it is convenient to use contour integration in the complex plane. Then, we very often find ourselves integrating over the half-circle in the upper (or, less canonically, lower) half-plane. These almost always (colloquial, not Lebesgue language) go to zero. What is the intuitive reason behind this? I see no reason to expect it, but of course I understand that we usually end up with $$\dfrac{R^n}{R^m+\mathcal{O}(R^l)},$$ where $l<m$ and $n<m$ , and sending $R\to\infty$ is why it goes to $0$ , but what intuitive way of thinking can lead us there? I'm looking for something along the lines of ""of course the upper half-circle contributes zero because (short quip)"", similar to how I tell my students that the dependence of the implicit derivative on both $x$ and $y$ for a circle makes sense, since circles are not functions and we need both values to determine which point we are talking about. I am aware of this question: why does the circle at infinity not contribute the integral? but it dealt more with the specific problem than the general intuition, I feel. This question also exists, and is helpful: Intuitive reason for why many complex integrals vanish when the path is ""blown-up""? But I still think such intuition must exist somewhere, and found the answers there helpful, but not satisfying.","['integration', 'complex-analysis', 'contour-integration']"
3258931,Extending differential form from a submanifold to a closed form,"I am in $R^4$ in coordinates $x,y,z,t$ . Can I extend an arbitrary 3-form defined only at points of the line $x=y=z=0$ to a closed 3-form in a neighbourhood of the line. In fact my question is only local, I do not need to have the whole line. My intuition says ""yes"", but I have no idea how to prove it, or where to look for similar extension theorems. Added: Thanks to Ted Shifrin for his observation. That solves my problem. If I am, for instance, in Minkowski space, then the question can be transformed to divergenceless vector field. Then we extend all components $X^\mu(t)$ as constant with respect to  x,y,z, except of, say the first one, which we make linear in $x$ , like $X^1=x\, dX^4(t)/dt.$ Thanks","['submanifold', 'differential-forms', 'differential-geometry']"
3258949,Order embedding of $\mathbb{N}^{\mathbb{N}}$ with the lexicographic order into $\mathbb R$,"We order the set $\mathbb{N}^{\mathbb{N}}$ with the relation $\sqsubset \text{which defimed by}$ $$ f\sqsubset g \iff \exists n \in \mathbb{N}.f(n)<g(n)\wedge \forall k < n.f(k)=g(k).$$ $\bullet$ Find $ I:\mathbb{N}^{\mathbb{N}} \rightarrow \mathbb{R}, \text{for which} \ \forall f,g \in \mathbb{N}^{\mathbb{N}}.f\sqsubset g \iff I(f)<I(g) $ What is the best way to find $I$ , I dont know how to start.",['elementary-set-theory']
3259011,challenging sum $\sum_{k=1}^\infty\frac{H_k^{(2)}}{(2k+1)^2}$,"How to prove that \begin{align}
\sum_{k=1}^\infty\frac{H_k^{(2)}}{(2k+1)^2}=\frac13\ln^42-2\ln^22\zeta(2)+7\ln2\zeta(3)-\frac{121}{16}\zeta(4)+8\operatorname{Li}_4\left(\frac12\right)
\end{align} where $H_n^{(m)}=1+\frac1{2^m}+\frac1{3^m}+...+\frac1{n^m}$ is the $n$ th harmonic number of order $m$ . This problem was proposed by Cornel Valean. Here is integral expression of the sum $\displaystyle -\int_0^1\frac{\ln x\operatorname{Li}_2(x^2)}{1-x^2}\ dx\quad $ .","['integration', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series', 'summation-by-parts']"
3259052,How does the tensor product relate to the gradient?,I only know the definition of the tensor product of two vectors and was reading a text which concerned a function $u:\mathbb{R}^3 \to \mathbb{R}^3$ such that $|u| = 1$ and $u(x) = \frac{x}{|x|}$ . The text claimed that $$\nabla u = \frac{1}{|x|}\left(I-\frac{x}{|x|} \otimes \frac{x}{|x|}\right)$$ and therefore $|\nabla u|^2 = \frac{2}{|x|^2}$ . I have no idea how they actually arrived at the expression for $\nabla u$ and then how did they take the norm of $\nabla u$ ? Because isn't $\nabla u$ a $3 \times 3$ matrix so what matrix norm is being used?,"['tensors', 'multivariable-calculus', 'linear-algebra', 'tensor-products', 'partial-differential-equations']"
3259055,Finite Ordinals and Natural Numbers,"I'm studying set theory and I'm focusing on von neumann ordinals. I've built an understanding of the reasoning that brings to the set-theoretic construction of the natural numbers whose soundness I'm questioning. I should stress that I find important point 2 to be the starting point of this construction. I'm going to outline it: Let's assume that we have already defined ordinals. We know that every successor of an ordinal is still an ordinal. Starting from $0 = \emptyset$ , which is the smallest ordinal, we can construct some ordinals by finding iteratively (but finitely) the successor ordinal of the previous one. That is $$0 = \emptyset\\
1 = \{\emptyset\}\\
2 = \{\emptyset,\{\emptyset\}\}\\
3 = \{\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\}\}\\\dots\\
n = \{\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\},\dots\}\\\dots$$ we know that these ordinals exist, right?. What we don't know is if there exist a set containing them. We define the class of finite ordinals with the following formula: $$FON(x) = ON(x)\wedge\forall y[(y \le x) \wedge (y \neq 0) \Rightarrow \exists z\{y = z \cup \{z\}\}]$$ So finite ordinals are those successor ordinals whose elements are all successor ordinals. We have that each ordinals $n$ we constructed above satisy this formula. Here is the first question: can we say the converse? Can we say that every $x$ such that $FON(x)$ is explicitely definable (with a finite iteration) as for $n$ ? However our new objective is to identify natural numbers and finite ordinals, and we want to prove that the class finite ordinals is a set $\omega$ (which we will eventually identify with $\mathbb{N}$ ). We introduce the axiom of infinity: $$\exists I(\emptyset \in I \wedge \forall x \in I((x \cup \{x\}) \in I))$$ and we'll call $I$ an inductive set. We want to show that every finite ordinal belongs to every inductive set. If the equivalence, conjectured in the previuos point, between finite ordinals and ordinals iteratively constructed from $\emptyset$ were to be true, then this would follow easly. Now, thanks to the axiom scheme of specification, we can extract from $I$ the subset of all the elements of $I$ that are finite ordinals. So we define $$\omega = \{x \in I : FON(x)\}$$ We then have that, since every finite ordinal belongs to every inductive set, $$FON(x) \Rightarrow x \in I \Rightarrow x \in \omega$$ on the other hand we have $$x \in \omega \Rightarrow FON(x)$$ by the definition of $\omega$ . So finally we get: $$\omega = \{x : FON(x)\}$$ Is this line of reasoning solid? Can we fill the holes in it? 
Thanks","['axioms', 'ordinals', 'logic', 'natural-numbers', 'elementary-set-theory']"
3259064,concerning the coefficients of $P_n(x)=(x-1)(x-2)\cdots (x-n)$,"I am trying to find an efficient way of calculating the unsigned coefficients of $$P_n(x)=\prod_{k=1}^{n}(x-k),$$ i.e. I want to speed up the process of calculating $a_k(n)$ such that $$P_n(x)=\sum_{k=0}^{n}(-1)^ka_k(n)x^{n-k}.$$ I found a method, but for $n\ge 5$ it is very inefficient. I found it by noting that $$\prod_{a\in A}(x-a)=\sum_{k=0}^{|A|}(-1)^{k}x^{|A|-k}\sum_{P\subseteq A\\ |P|=k}\prod_{u\in P}u\ .$$ So setting $A=\{1,2,...,n\}$ for some $n\in\Bbb N$ , $$\prod_{a\in A}(x-a)=P_n(x)=\sum_{k=0}^{n}(-1)^kx^{n-k}\sum_{P\subseteq A\\ |P|=k}\prod_{u\in P}u\ .$$ So of course I defined $$a_0(n)=1$$ and $$a_k(n)=\sum_{P\subseteq\{1,...,n\}\\ \quad |P|=k}\prod_{u\in P}u\ .\tag{1}$$ If we plug in $x=0$ , $$P_n(0)=\prod_{k=1}^{n}(-k)=(-1)^n n!\ ,$$ so that $$a_n(n)=n!\ .$$ It is also fairly easy to show that $$a_1(n)=\frac{n(n+1)}{2}\ .$$ I was also able to show that $$a_2(n)=\sum_{(u,v)\in R_n}uv$$ where $$R_n=[1,n]^2\cap\left\{(x,y)\in\Bbb N^2: y-x\in[1,n-1]\right\},$$ But that isn't simpler by any stretch of the imagination. Is there a more efficient version of $(1)$ ? Thanks. Edit for context: As I said in the comments, there is no reason that I need these coefficients, I just thought it would be an interesting problem to find them. Once I found them, I wondered if there was a more efficient way of calculating them, so I asked here.","['products', 'combinatorics', 'polynomials', 'sequences-and-series']"
3259108,Derivative of transpose,"I am trying to find derivative of this :
RQ(u) = u T X T Xu / u T u I need help finding derivative : 𝑑RQ/𝑑u Optimal sol should satisfy X T Xu = RQ(u)u I am very confused, any help would be great. If you could share some references that will be very helpful too.","['matrices', 'calculus', 'linear-algebra', 'transpose', 'derivatives']"
3259112,Proving that a particular map is an embedding from the Klein bottle into $\mathbb{R}^4$,"I would appreciate if anyone could help me with the above: Show that the map $g:\mathbb{R}^{2}\rightarrow \mathbb{R}^{4} $ given by $$g(x, y)=((a+r\cos2\pi y)\cos 2 \pi x, (a+r\cos 2 \pi y)\sin 2 \pi x, r\sin 2 \pi y \cdot \cos \pi x, r\sin 2 \pi y \cdot \sin \pi x)  $$ induces an embedding $C^{\infty}$ from the Klein's Bottle into $\mathbb{R}^{4}$ . Important: it does not specify anything about the numbers $a$ and $r$ , but I think they have to obey something like $0<r<a$ . Remark: we have 3 ways to construct the Klein's bottle, as follow: $\bullet$ $\mathbb{R}^{2}/G$ :  where $G=\langle f_1, f_2\rangle$ , such that, $f_1, f_2:\mathbb{R}^{2}\rightarrow \mathbb{R}^{2}$ given by $f_1(x, y)=(x, y+1)$ and $f_2(x, y)=(x+1, 1-y)$ . $\bullet$ $(S^{1}\times \mathbb{R})/H$ : where $H$ is the ciclic group spanned by $h:S^{1}\times \mathbb{R} \rightarrow S^{1}\times \mathbb{R}$ given by $h((x, y), z)=h(x, y, z)=(x, -y, z+1)$ . $\bullet$ $(S^{1}\times S^{1})/K $ : where $K=\{ I_{S^{1}\times S^{1}}, k\}$ , such that $I_{S^{1}\times S^{1}}$ is the identity and $k:S^{1}\times S^{1}\rightarrow S^{1}\times S^{1}$ is given by $k(x, y)=(\overline{x}, -y)$ . thanks for any help.","['klein-bottle', 'geometry', 'quotient-spaces', 'manifolds', 'differential-geometry']"
3259114,How to show these moments are determined?,"I'm given a sequence of moments $$
S_k=\int_{1}^{\infty}x^k \exp \left(\frac{-x}{\log(x)}\right)dx
$$ and I'm told that this sequence is determined. However, I can't find a way to show this. I tried starting out by using Carleman's Condition, i.e. showing that $$\sum_{k=1}^{\infty} \frac{1}{S_{2k}^{1/(2k)}}=\infty.$$ Attempting this method has actually led me to believe the above sum might converge: so I wouldn't be able to use this condition to conclude ""determinance"". In any case, I wish to determine the convergence or divergence of the above series.
My attempts to show this is by finding sequences such that $$A_k \leq S_k \leq B_k$$ and finding $f$ and $g$ so that $A_k=\int_{1}^{\infty}x^kfdx$ and $B_k=\int_{1}^{\infty}x^kgdx$ - and $A_k$ and $B_k$ are convergent series for each $k$ . I tried find functions that are either above or below $\frac{-x}{\log(x)}$ to get $f$ and $g$ . What kind of $f$ and/or $g$ could I use here? $\textbf{Edit (3)}:$ Attempt 2 on getting an upper bound is by taking the intervals $\big[ e^i, e^{i+1} \big]$ and having that $$\int_{1}^{\infty}x^k \exp \left(\frac{-x}{\log(x)}\right)dx \leq \sum_{i=0}^{\infty}\exp \left( (i+1)k-\frac{\exp \left( i+1 \right) }{i+1} \right) \left( e^{i+1} - e^{i} \right) \\ = \frac{e-1}{e}\sum_{i=0}^{\infty} \exp \left( (i+1)k-\frac{\exp \left( i+1 \right) }{i+1} +(i+1)\right)$$ Assuming $k \geq 5$ , we have that $$\frac{e^{i+1}}{(i+1)(k+1)} - (i+1) \geq i$$ for every $i \geq k.$ So we can split the sum into $$\sum_{i=0}^{k-1} \exp \left( (i+1)k-\frac{\exp \left( i+1 \right) }{i+1} +(i+1)\right) + \sum_{i=k}^{\infty} \exp \left( -(k+1)\right) ^ {\frac{e^{i+1}}{(i+1)(k+1)} - (i+1)} \\ \leq k * max_{0 \leq i \leq k-1} \exp \left( (i+1)(k+1)-\frac{\exp \left( i+1 \right) }{i+1} \right) + \sum_{i=k}^{\infty} \exp \left( -(k+1)\right)^{i}$$ The right-most sum is geometric and less than 1 for any $k \geq 5$ . One can calculate that the maximum is attained for $i$ such that $\frac{ie^{i+1}}{{(i+1)}^2}=k+1$ . Hence,  the exponent of the maximum becomes $$(i+1)(k+1)-\frac{(i+1)(k+1) }{i} = (i-\frac{1}{i})(k+1)$$ for our given $i$ , and this is $O(k log(k))$ . Hence, our sum overall is $O(k^2k^k)$ , or subsequently $O(k^{2k})$ with plenty of room to spare. Hence by the Stielje's condition for moments on a half-line we have that $S_k$ is determined. Does this look right?","['improper-integrals', 'proof-verification', 'moment-problem', 'complex-analysis', 'probability-theory']"
3259195,"Geometric intuition on $\langle x, A^\top y\rangle = \langle y, Ax\rangle$","I am looking for a geometric intuition on $\langle x, A^\top y\rangle = \langle y, Ax\rangle$ . This can be proven algebraically by disassembling the expression into basic sums and product and reordering a few terms. But that does not offer any insight. Semantically this equality states that the scaled projection (dot product) of $x$ with a linear combination of the rows of $A$ weighted by $y$ is equal to the scaled projection of $y$ onto linear combination of the columns of $A$ weighted by $x$ . But I fail to understand this in an intuitive geometric way. Do you know of any insightful interpretation? If this equality has a name, that would also be useful. Right now I cannot really research it without a name. I came across the equation here: Eigenvectors of real symmetric matrices are orthogonal","['bilinear-form', 'linear-algebra', 'geometry', 'linear-transformations']"
3259196,Normalizer of group action,"Take a group $G$ acting faithfully on a set $X$ , and let $H \leq G$ . It can be easily shown that the elements of $N_{G}(H)$ stabilize the collection of orbits of $H$ (as a set, ie orbits are mapped to orbits). Is the converse true? That is, if we take $\mathcal{O}$ to be the collection of orbits of $H$ , do we always have that $\mathrm{Stab}_{G}(\mathcal{O}) = N_{G}(H)$ ? (I'm happy if everything is assumed to be finite, but more general answers are also welcome.) Edit: Since, as mentioned in the answer of runway44, this can be considered by looking at what happens with each of the $G$ -orbits on $X$ , I would like to know if this is true for $G$ acting transitively and faithfully on $X$ .","['group-actions', 'abstract-algebra', 'finite-groups']"
