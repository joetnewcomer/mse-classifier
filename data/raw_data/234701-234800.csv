question_id,title,body,tags
4905202,"Given that $\sin x+\sin3x +\sin5x = 1$, $\cos x +\cos3x +\cos5x = \sqrt{2} + 1$, find the value of $\tan3x$. Are there any mistakes in this topic?","Given the equations: $$\sin x + \sin 3x + \sin 5x = 1,$$ $$\cos x + \cos 3x + \cos 5x = \sqrt{2} + 1,$$ we are asked to find the value of $\tan 3x$ . Firstly, let's use the sum-to-product formulas for sine and cosine: For sine: $$\sin A + \sin B = 2\sin\left(\frac{A+B}{2}\right)\cos\left(\frac{A-B}{2}\right),$$ For cosine: $$\cos A + \cos B = 2\cos\left(\frac{A+B}{2}\right)\cos\left(\frac{A-B}{2}\right).$$ Applying these to our equations, we get: For the first equation: $$\sin x + \sin 5x = 2\sin\left(\frac{x+5x}{2}\right)\cos\left(\frac{x-5x}{2}\right) = 2\sin(3x)\cos(-2x) = 2\sin(3x)\cos(2x),$$ and thus: $$2\sin(3x)\cos(2x) + \sin 3x = 1.$$ For the second equation: $$\cos x + \cos 5x = 2\cos\left(\frac{x+5x}{2}\right)\cos\left(\frac{x-5x}{2}\right) = 2\cos(3x)\cos(-2x) = 2\cos(3x)\cos(2x),$$ and thus: $$2\cos(3x)\cos(2x) + \cos 3x = \sqrt{2} + 1.$$ Now we solve for $\sin(3x)$ and $\cos(3x)$ : From the first equation: $$2\sin(3x)\cos(2x) + \sin 3x = 1,$$ $$\Rightarrow \sin 3x (2\cos(2x) + 1) = 1.$$ From the second equation: $$2\cos(3x)\cos(2x) + \cos 3x = \sqrt{2} + 1,$$ $$\Rightarrow \cos 3x (2\cos(2x) + 1) = \sqrt{2} + 1.$$ Dividing the two equations to eliminate $\cos(2x)$ , we get: $$\frac{\sin 3x (2\cos(2x) + 1)}{\cos 3x (2\cos(2x) + 1)} = \frac{1}{\sqrt{2} + 1},$$ $$\Rightarrow \frac{\sin 3x}{\cos 3x} = \frac{1}{\sqrt{2} + 1},$$ $$\Rightarrow \tan 3x = \frac{1}{\sqrt{2} + 1}.$$ To rationalize the denominator, we multiply the numerator and denominator by the conjugate $(\sqrt{2} - 1)$ : $$\tan 3x = \frac{1}{\sqrt{2} + 1} \cdot \frac{\sqrt{2} - 1}{\sqrt{2} - 1},$$ $$\Rightarrow \tan 3x = \sqrt{2} - 1.$$ That's the process of doing it by hand. However, using mathematica software to calculate the value of tan3x, the result is no solution. Is there a mistake in the question?",['trigonometry']
4905207,A right-angled triangle has sides of integer length. Its area (in square metres) is twice its perimeter (in metres). What are the lengths of the sides,"A right-angled triangle has sides of integer length. Its area (in square metres) is twice its
perimeter (in metres). What are the lengths of the sides? The equations I have made so far is: Using Pythagoras' Theroem and Area Formula: $$\frac{ab}{4}=a+b+\sqrt{a^2+b^2}$$ Using Heron's formula: $$s(s-a)(s-b)(s-c)=4(a+b+c)^2$$ For LHS: $$(\frac{a+b+c}{2})(\frac{a+b+c-2a}{2})(\frac{a+b+c-2b}{2})
(\frac{a+b+c-2c}{2})$$ $$\frac{1}{16}((a+b+c)(b+c-a)(a+c-b)(a+b-c))$$ $$\frac{1}{16}(((b+c)^2-a^2)(a^2-(b-c)^2))$$ Please find the original link for the question here: https://www.maths.uq.edu.au/qamt/papers/Year9-10-2022-Paper.pdf Question 4 I believe that even though this question is simple, it is quite tricky and does need some consideration. Additionally , I needed this question's answer while I was preparing for the Year 9-10 UQ/QAMT Paper, as it still holds relevance as it still only is a 2-year old question! My question is often compared to A right triangle with integer sides has area equal to twice its perimeter. Find sum of all possible circumradii. , but I believe the question is different.","['triangles', 'area', 'geometry']"
4905209,Strange usage of chain rule. Can anyone explain why this derivation was done this way?,"There are 2 issues I have with the way this was done. The first was how chain rule was used in the (1.35), and the second was how chain rule was used in (1.36). It all seems so counterintuitive. For (1.35), to start I tried plugging for the initial equation, but without known values for $\delta p$ , I can't use it to verify this equation. So, I attempted to obtain it using chain rule, and doesn't make a lot of sense the way I'm assuming it was done. Unless I'm just blatantly abusing chain rule, this result just doesn't seem possible. Trying to do this a bit more intuitively: Transform 1: $p(x,z) = g(x,z) \therefore z = z(x,z)$ (pretty sure we can't do this) $$\frac{\partial p}{\partial x} = \frac{\partial g}{\partial x} = \frac{\partial g}{\partial x}+\frac{\partial g}{\partial z}\frac{\partial z}{\partial x}$$ It's my understanding that we could also abuse chain rule here and say $\frac{\partial a_i}{\partial n_i}\frac{\partial n_i}{\partial k_i} = \frac{\partial a_i}{\partial k_i}$ , otherwise we're saying that z is a function of x when it just plainly isn't. When and Why would we choose to abuse chain rule like this? Drop the functions to get a definition for the differential operator $$\tag{1}\frac{\partial}{\partial x}  = \frac{\partial}{\partial x} = \frac{\partial}{\partial x}+\frac{\partial z}{\partial x}\frac{\partial}{\partial z}$$ Second transformation: $p(x,z) = h(x,s) \therefore s = s(x,z)$ (THIS would be a proper usage of transforming a variable and then applying chain rule, but why do it here rather than at the beginning?) $$\frac{\partial h}{\partial z} = \frac{\partial h}{\partial s}\frac{\partial s}{\partial z}$$ Drop the functions to get a differential operator to substitute into (1) $$\tag{2}\frac{\partial}{\partial z} = \frac{\partial s}{\partial z}\frac{\partial}{\partial s}$$ And then plugging in (2) into (1) and applying the derivative to the original function $p$ would give: $$\tag{5} \frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial s}{\partial z}\frac{\partial z}{\partial x}\frac{\partial p}{\partial s}$$ I have no idea why we couldn't have just instead done $p(x,z) = g(x,s) \therefore s = s(x,z)$ at the very beginning and then done $\frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial p}{\partial s}\frac{\partial s}{\partial x}$ , nor do I understand why this would be incorrect whereas we would consider (1.36) correct. Does anyone understand why it was done this way?","['multivariable-calculus', 'chain-rule']"
4905222,How did Jacobi find his connection between theta functions and $q$?,"I was 'reading' (I can't actually read german, but I can read math!) Jacobi's derivation of the ODE for $y(q) = \sum_{n=-\infty}^{\infty} q^{n^2} $ . On page 2 of the paper Jacobi states the following as an axiom of sorts that was considered to be obvious at the time. I am asking how to prove and see this result. The result is if $$ y(k) = \sqrt{ \frac{2}{\pi} \int_{0}^{\frac{\pi}{2}} \frac{d\varphi}{\sqrt{1-k^2 \sin^2 (\varphi)}} }  $$ and $$ \ln \left( \frac{1}{q(k)} \right) = \pi \frac{ \int_{0}^{\frac{\pi}{2}} \frac{d\varphi}{\sqrt{\cos^2(\varphi)+k^2 \sin^2(\varphi)}}}{ \int_{0}^{\frac{\pi}{2}} \frac{d\varphi}{\sqrt{1-k^2 \sin^2(\varphi)}}} $$ Then $$y(k) = 1 + 2q(k) + 2q(k)^4 + 2q(k)^9 ... \rightarrow y = \sum_{n=-\infty}^{\infty} q^{n^2} $$ So I tried to derive this the usual way anyone in 1846 would. I went to my copy of mathematica on a new macbook pro and implemented: y[k_] = Sqrt[2/Pi * Integrate[1/Sqrt[1 - k^2 * Sin[s]^2], {s, 0, Pi/2}]] 
Q[k_] = Exp[-Pi *( Integrate[  1/Sqrt[Cos[s]^2 + k^2 Sin[s]^2], {s, 0, Pi/2}]/
 Integrate[  1/Sqrt[1 - k^2 Sin[s]^2], {s, 0, Pi/2}] )] And then attempted to compute the series via the techniques here Series[y[InverseFunction[Q][k]], {k, 0, 10}] To my great surprise, after 10 minutes, there was no answer. Whatever Mathematica was computing was simply NOT converging. So I am quite spooked now. If my CAS can't handle this, how on earth did Jacobi manage to find and verify this result back in the day? And why is this considered obvious? I can't find a citation for this in the text, he just starts here and then picks up. If anyone can enlighten me on how this can be derived by hand I would be very grateful. For those that want to find Jacobi's work, the work in particular is cited as  K. G. J. Jacobi, J. Reine Angew. Math. ( = Crelle’s Journal.) 36 (1847), 97-112 in Richard Stanley's Enumerative Combinatorics Volume 2 Page 282. And can be found here .
The ODE can be found here","['riemann-surfaces', 'complex-analysis', 'calculus', 'modular-forms', 'theta-functions']"
4905234,How do we prove that $A\subseteq C$ and $B\subseteq C$ iff $A\cup B\subseteq C$?,"Exercise in Set theory from Zorich Mathematical Analysis : Prove : $(A\subset C) \wedge (B\subset C) \Longleftrightarrow (A \cup B) \subset C$ Is my proof of that Exercise correct? Here is my try of proving it: First: LHS $\Rightarrow$ RHS $((A\subset C)\wedge (B\subset C))$$\implies$$((x\in A\Rightarrow x\in C)$$\wedge$$(x\in B\Rightarrow x\in C)$ ) $\implies$ At least one of $x\in A$ and $x\in B$ implies $x\in C$$\implies$$(((x\in A) \vee (x\in B))\Rightarrow x\in C)$$\implies $$(A \cup B)\subset C$ Second: RHS $\Rightarrow$ LHS $(A \cup B)\subset C$$\implies$$(x\in A \cup B\Rightarrow x\in C)$$\implies$$(((x\in A) \vee ((x\in B))\Rightarrow x\in C)$$\implies$ both $x\in A$ and $x \in B$ means $x\in C$ $\implies$$(A\subset C)\wedge (B\subset C)$ To conclude, $(A\subset C) \wedge (B\subset C) \Longleftrightarrow (A \cup B) \subset C$","['elementary-set-theory', 'proof-writing']"
4905257,Prove a Result about Optimization and Exponential Distribution: Intuitive but Challenging,"Let $v=(v_1,v_2,...v_N)$ be a random vector with $N$ elements. $v_i \perp v_j$ for any $i\neq j$ . For each $i$ , the CDF of $v_i$ is: $F_i(v_i)=1-e^{-(v_i-a_i)/\theta}$ , where $v_i\in [a_i,\infty)$ . Assume $0<\theta<a_1<a_2<...<a_N$ and $c>0$ . Let $x(v)=(x_1(v),x_2(v)...x_N(v))$ be the solution of the following problem \begin{equation}
\max_{x(v)} \sum_{i=1}^N x_i (v_i-\theta- cx_i)
\end{equation} subject to $x_i\geq 0$ for all $i$ , and $\sum_{i=1}^{N}x_i\leq1$ . Since $v$ is random, $x(v)$ is also a random vector. Prove : $\mathbb{E}(x_N)-\mathbb{E}(x_1)$ is a decreasing function of $c$ . My attempts : This result is quite intuitive and I've verified using MATLAB for different parameters. If $N=2$ , this can be proved. The solution for each $x_i$ is the following: if $\sum_{i=1}^N \frac{v_i-\theta}{2c}\leq1$ , then $x_i=\frac{v_i-\theta}{2c}$ ; if $\sum_{i=1}^N \frac{v_i-\theta}{2c}>1$ , then $x_i=\frac{[v_i-\theta-\lambda(v)]^+}{2c}$ , and $\sum_{i=1}^N x_i=1$ for some Lagrange multiplier $\lambda(v)>0$ . Note that $\lambda$ is symmetric, i.e., $\lambda(v_i,v_j,v_{-{i,j}})$ = $\lambda(v_j,v_i,v_{-{i,j}})$ for all $i\neq j$ . I think the challenge is the $[\quad]^+$ sign in the solution of $x_i$ , if we remove this, the result is obviuosly true. But the $[\quad]^+$ sign makes it messy. Maybe Mathematical Induction is useful, I've tried, but still didn't prove it.","['convex-optimization', 'exponential-distribution', 'probability']"
4905319,Subset of a conjugacy class of of an odd permutation in $S_n$,"Let $\sigma=(1,2,3,\dots,n)$ be an odd $n-$ cycle in $S_n$ (so $n$ is even). It is known that the size of its conjugacy class is $|cl_{S_n}(\sigma)|=(n-1)!$ . I am interested in the size of the subset $S=cl_{cl_{S_n}(\sigma)}(\sigma)$ , that is, the set of all the $n-$ cycles that we can obtain by conjugating $\sigma$ only with elements in its conjugacy class. In particular, I would like to show that $S$ contains at least $|cl_{S_n}(\sigma)|/2=(n-1)!/2$ elements, which happens to seem true in the numerical experiments I run. Approaches I tried: Construct enough elements by hand. I found this trick: If we conjugate $\sigma=(1,2,3,\dots,n)$ by $(1, a_1, 3, a_3, 5, a_5, \dots, 4, a_4, 2, a_2, n)$ for a free choice of the $a_i$ in the set of the remaining half symbols, we obtain a permutation sending $(1,a_1,a_2,a_3,a_4,\dots,a_{n/2-1},\dots)$ . This way it is possible to produce explicitly some mutually different elements, but still not enough. By elementary group theory, two elements $\alpha,\beta\in S$ are such that $\alpha\sigma\alpha^{-1}=\beta\sigma\beta^{-1}$ if and only if $\beta=\alpha\sigma^k$ for some even power $k$ of $\sigma$ . This way we obtain that $S$ contains at least $\frac{(n-1)!}{n/2-1}$ elements, still too few, unfortunately, due to the denominator depending on $n$ (but observe that this case solves the case $n=6$ ). Notes: The fact that we are choosing as $\sigma$ that particular odd cycle is just a simplification of the original problem. This claim still seems to be true for any odd permutation $\sigma\in S_n$ for every $n$ big enough to contain $\sigma$ . I feel that this problem is not impossible, and just a clever simple counting trick is needed.
Thanks in advance for any suggestions or comments.","['permutation-cycles', 'combinatorics', 'symmetric-groups', 'group-theory', 'orbit-stabilizer']"
4905322,Evaluating $\int_1^2\frac{1}{x}\left(\ln(x^4+4)-\ln(x^2+4)\right)\mathrm{d}x$,"The problem is to calcultate $$\int_1^2\frac{1}{x}\left(\ln(x^4+4)-\ln(x^2+4)\right)\mathrm{d}x$$ I have tried the usual substitution method. Let $y=\frac{2}{x}$ , then I get $$\int_1^2\frac{1}{x}\ln\frac{x^4+4}{x^2+1}\mathrm{d}x-(\ln 2)^2,$$ but it doesn't work. I guess the result is $\frac{(\ln2)^2}{2}$ .","['integration', 'calculus']"
4905326,Evaluating a rational function integral in a quick way,"In an recent test I was asked to evaluate the integral $$ \int_0^1 \frac{\sqrt[3]{x^2(1-x)}}{(1+x)^3} \text{d}x$$ in 8 minutes, but I didn't have a clue what to do with it. After the test, I tried the following approach: substituting $x=\frac{1}{t}, u=\sqrt[3]{t-1}$ gives $$ \int_0^1 \frac{\sqrt[3]{x^2(1-x)}}{(1+x)^3} \text{d}x=\int_1^\infty \frac{\sqrt[3]{t-1}}{(t+1)^3}\text{d}t=\int_0^\infty \frac{3u^3}{(u^3+2)^3}\text{d}u$$ then it becomes a problem of evaluating a rational function integral, but I still don't think it could be done in the time limit. Is there any quick method to evaluate this? Thanks for any help. (Note: 1. The integral is $\frac{\pi}{9\times 2^{2/3}\sqrt3}$ according to Wolfram Alpha. 2. It is best to solve it with first-year calculus, and I will understand more easily. Other solutions are also welcomed:)","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'rational-functions']"
4905339,Contradicting solutions in the Fourier space.,"Consider the following differential equations $$
\begin{cases}
	\partial_t X(t) = -i \alpha X(t) + \beta Y(t), \\
\partial_t Y(t) = -i \alpha Y(t) - \beta X(t).
\end{cases}\label{1}\tag{1}
$$ with $\alpha$ , $\beta$ some complex constants (independent of $t$ ). Case-1 [Time domain]: The solution (with initial condition, say $X(0) = 1, Y(0) = 0$ ) reads $$
\begin{cases}
	X(t) = \frac{1}{2}[e^{-ist} + e^{-idt}], \\ Y(t) = -\frac{1}{2} [e^{-ist} - e^{-id t}],
\end{cases}\label{2}\tag{2}
$$ where $s = \alpha + \beta$ and $d = \alpha - \beta$ . Now, the Fourier transform $$
\mathscr{F}[A(t)] = A(\omega) = \int_{-\infty}^{\infty} dt e^{i\omega t} A(t)
$$ of \eqref{2} reads: $$
\begin{cases}
	X(\omega) = \frac{1}{2}[ \delta (\omega - s) + \delta (\omega + s)], \\ 
Y(\omega) = -\frac{i}{2}[ \delta (\omega - s) -  \delta (\omega + s)]
\end{cases}\label{3}\tag{3}
$$ Case-2 [Fourier domain]: Taking Fourier transform of \eqref{1}, we have $$
\begin{cases}
	-i \omega X(\omega) = -i \alpha X(\omega)  + \beta Y(\omega),\\
 -i\omega Y(\omega) = -i\alpha Y(\omega) - \beta X(\omega).  
\end{cases}\label{4}\tag{4}
$$ The solution of \eqref{4} (using Mathematica ) turns out to be $$ 
\begin{cases}
	X(\omega) = 0, \\ 
    Y(\omega) = 0.
\end{cases}\label{5}\tag{5}
$$ Clearly solution \eqref{5} is not the same as solution \eqref{3}. Am I missing too many things here? Edit : My question is simple. How to start with equation \eqref{4} and obtain equation \eqref{3}?","['solution-verification', 'fourier-analysis', 'ordinary-differential-equations']"
4905357,"A ""new"" lemma regarding an equation of an arbitrary function $φ: \mathbb N \to \mathbb N$","Lemma. Consider a function $\phi: \mathbb N \to \mathbb N$ , a positive integer $m$ and suppose that there exists functions $f: \mathbb N \to \mathbb R_{\geq 0}$ and $g:A \subseteq \mathbb R \to \mathbb N$ so that: $f(n) \leq \phi (n)$ for all $n \in \mathbb N$ , $g$ is increasing, and $f(\mathbb N) \subseteq A$ so that $g \circ f=id_{\mathbb N \to \mathbb N}$ . Then the equation $\phi (x)=m$ $(\ast)$ has finitely many solutions over $\mathbb N$ . Proof . Consider $\cal L_m \subseteq \mathbb N$ the set of all solutions of $(\ast)$ and assume by way of contradiction that $\cal L_m$ is infinite. Now, take $x \in \cal L_m$ . Thus, $f(x) \leq \phi (x)=m$ , and so $1 \leq x=g(f(x)) \leq g(m)$ . Therefore, $\cal L_m$ is bounded, impossible, q.e.d. Now, my question is whether this trivial result has already been discovered. Lastly, I'd like to present some examples of this lemma. Example 1 Consider $φ$ to be the Euler totient function. It is true that $φ(n) \ge \sqrt{\frac{n}{2}}$ for all positive integers $n$ , and so the equation $φ(x)=m$ has finitely many solutions for all positive integers $m$ . Example 2 If $\sigma_1$ is the sum of divisors function, then $\sigma_1 (n) \ge n-1$ for all positive integers, and so a similar result follows. Comment The difficult part is finding an appropriate function $f$ , as seen in example 1.",['functions']
4905377,Total differential of a composite function,"Let $w=f(u)$ . Determine $dw$ , if $u=x^2+xy+y^2+z^2-3x+5y-2z$ I found that $\ du = (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz .$ Assuming $ dw = \frac{\partial w}{\partial u} du,$ I obtained: $$
dw = \frac{\partial w}{\partial u} \ du = \frac{\partial w}{\partial u}( (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz) .
$$ Is this correct? Is it sufficient? Is there anything else I should do next?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4905382,"$x_1,...,x_4$ are irreducible in $k[x_1,..,x_4]/\langle x_1x_4-x_2x_3\rangle$","Let $k$ be a field (not sure whether there are conditions on characteristic and algebraic closure). I want to show that the classes of $x_1,...,x_4$ in $R:=k[x_1,..,x_4]/\langle x_1x_4-x_2x_3\rangle$ are irreducible but not prime. This is an exercise from Gathmann's notes on algebraic geometry. Let us look at, say $x_1$ . Define $I := \langle x_1x_4-x_2x_3 \rangle$ . For the irreducibility assume $x_1 + I = pq + I$ where $p,q \in k[x_1,...]$ are non units. Then we have that there is a $g \in k[x_1,...]$ such that $$pq - x_1 = g (x_1x_4 - x_2 x_3)$$ But now it is unclear how to proceed. I do not see how to conclude a contradiction. Perhaps one could look at $$pq-g(x_1x_4 - x_2x_3) = x_1$$ Since $\text{deg } pq, \text{deg } g(x_1x_4-x_2x_3) \geq 2$ and their difference is of degree 1 we must have that the summands of degree $\geq 2$ have to be identical in $pq$ and $g(x_1x_4-x_2x_3)$ . Also degree 1 summands can only occur in $pq$ .","['irreducible-polynomials', 'algebraic-geometry']"
4905390,Power series solution to $x^2y''+y'+y=0$ around $x=0$,If an ODE of the form $$y''+p(x)y'+q(x)y=0$$ has $p(x)$ and $q(x)$ that are analytic at $x_0$ then we can suppose a power series solution as such: $$y(x)=\sum_{n\geq 0} a_n(x-x_0)^n$$ If instead they have a first order pole at $x_0$ we have to suppose this other one: $$y(x)=\sum_{n\geq 0} a_n(x-x_0)^{n+r}$$ But what about a power series solution with coefficients that have higher order poles at $x_0$ like in this particular case where there is a $2^{\text{nd}}$ order pole at $x=0$ ?: $$x^2y''+y'+y=0$$ Is there any possible power series solution at $x=0$ ?,"['power-series', 'frobenius-method', 'ordinary-differential-equations']"
4905406,Solving a certain matrix equation,"I am trying to solve the (real) matrix equation $X^{-1}AX=Y$ , where the matrix $A_{n\times n}$ is given and the matrices $X_{n\times n}, Y_{n\times n}$ are unknown matrices, with the only restriction that $Y$ is of the form $\begin{bmatrix} 
     S_{2\times 2} & T_{2\times (n-2)}\\\ \star & \star \end{bmatrix}$ , where $S_{2\times 2}$ has equal row sums and $T_{2\times (n-2)}$ is consisted of two identical rows, i.e., $T_{2\times (n-2)}= \begin{bmatrix} 
     T_1\\\ T_1\end{bmatrix}$ (Thus we want $Y$ only to be of the given form and no restriction on $X$ except being invertible.) The thing is that I would like to solve this without diagonalising or triangularising $A$ , i.e., no use of the eigenvalues of $A$ , whatsoever. I would very greatly appreciate it if you can direct me to some literature or give me some hints. I have a feeling that, even though we have only one rather weak restriction (i.e., the restriction on the form of $Y$ ), finding an explicit and ""nice"" solution is not possible. (When there is a solution.)","['matrices', 'matrix-equations', 'linear-algebra']"
4905472,Determining a geometric angle,"I have the following situation where the line $(AB)$ is orthogonal to the ""vertical line"" passing through $C$ . Is there a way to determine the blue angle in terms of $\boldsymbol {AC}$ , $\boldsymbol{BC}$ and the red angle, or is there a degree of freedom? I only get the equation (where $b$ is the blue angle and $r$ is the red angle) $$AC \cos(b) = BC \cos(b+r)$$ But this does not give any explicit expression for the blue angle. Is there any?","['triangles', 'euclidean-geometry', 'trigonometry', 'geometry']"
4905672,"Is time-orientability a condition on the metric, smooth or topological structure of a manifold?","I recently asked a question on Physics Stack Exchange about orientability and time-orientability of a manifold in the language of fiber bundles. This new question is related to, but independent, of that one. Orientability of an $n$ -dimensional manifold (for example in the sense that there is an everywhere non-vanishing smooth $n$ -form) is a topological property of manifolds. This can be seen through the theory of characteristic classes and the fact that it is equivalent to the first Stiefel–Whitney class of the manifold vanishing. Time-orientability can be defined by requiring that there is an everywhere non-vanishing timelike vector on the manifold (which immediately makes use of the metric structure in order to define what ""timelike"" means). Alternatively, one can define a manifold is time-orientable if the bundle of orthonormal frames $\mathrm{O}(M)$ admits a reduction to an $\mathrm{O}^+(M)$ subbundle under the inclusion $\mathrm{O}^+(s,t) \subseteq \mathrm{O}(s,t)$ . My question is: when we say that a Lorentzian (or, more generally, pseudo-Riemannian) manifold $(M,g)$ is time-orientable, is this a restriction on the metric structure $(M,g)$ , on the smooth structure of $M$ , or on the topology of $M$ ?","['semi-riemannian-geometry', 'mathematical-physics', 'general-relativity', 'characteristic-classes', 'differential-geometry']"
4905711,"If $K/\mathbb{Q}_p$ totally ramified, is $\mathcal{O}_K / \pi^n \mathcal{O}_K \cong \mathbb{Z} / p^n \mathbb{Z}$?","As in the title, if $K/\mathbb{Q}_p$ is a finite and totally ramified extension, is $\mathcal{O}_K / \pi^n \mathcal{O}_K \cong \mathbb{Z} / p^n \mathbb{Z}$ for all $n \ge 1$ ? Obviously the case $n = 1$ holds since the residue fields are equal (totally ramified). Also, I believe the rings have the same order: considering the power series representations in $\mathcal{O}_K$ , then truncating to length $n$ , we see $$ \# \left(\mathcal{O}_K / \pi^n \mathcal{O}_K\right) = p^n $$ However, is it isomorphic to $\mathbb{Z} / p^n \mathbb{Z}$ as a ring?","['number-theory', 'p-adic-number-theory', 'algebraic-number-theory']"
4905717,unsure about logic in elementary set theory exercise Tao Analysis I 3.5.9,"This question is about the validity of a proof argument. I am asking because I am new to proofs. This question is not specifically about solving the exercise. The exercise is Tao Analysis I 4thed ex 3.5.9 Suppose that $I$ and $J$ are two sets, and for all $\alpha \in I$ let $A_{\alpha}$ be a set, and for all $\beta \in J$ let $B_{\beta}$ be a set. Show that $$(\bigcup_{\alpha \in I} A_{\alpha}) \cap (\bigcup_{\beta \in J} B_{\beta}) = \bigcup_{(\alpha,\beta) \in I \times J} (A_{\alpha} \cap B_{\beta})$$ What happens if one interchanges all the union and intersection symbols here? Note: I wanted to show this using a series of $\iff$ statements and not the more verbose proving both inclusions $\implies$ and $\impliedby$ . Let's start with the LHS. $$x \in (\bigcup_{\alpha \in I} A_{\alpha}) \cap (\bigcup_{\beta \in J} B_{\beta}) \iff x \in (\bigcup_{\alpha \in I} A_{\alpha}) \land x \in (\bigcup_{\beta \in J} B_{\beta})$$ Using the definition of the union of sets, this means both of the following are true: $x$ is an element of at least one $A_{\alpha}$ , let's call one of them $A_{\alpha'}$ $x$ is an element of at least one $B_{\beta}$ , let's call one of them $B_{\beta'}$ This gives us $$\begin{align}x \in (\bigcup_{\alpha \in I} A_{\alpha}) \cap (\bigcup_{\beta \in J} B_{\beta}) &\iff x \in A_{\alpha'} \land x \in B_{\beta'} \\ \\ & \iff x \in A_{\alpha'} \cap B_{\beta'} \\ \\ & \iff x \in \bigcup_{(\alpha,\beta) \in I \times J} (A_{\alpha} \cap B_{\beta})\end{align}$$ Thus, we have shown $$(\bigcup_{\alpha \in I} A_{\alpha}) \cap (\bigcup_{\beta \in J} B_{\beta}) = \bigcup_{(\alpha,\beta) \in I \times J} (A_{\alpha} \cap B_{\beta}) \; \square$$ My Doubt 1 My doubts are in using the specific $A_{\alpha'}$ and $B_{\beta'}$ which we know exist . For example, consider the following: $$x \in (\bigcup_{\alpha \in I} A_{\alpha}) \cap (\bigcup_{\beta \in J} B_{\beta}) \iff x \in A_{\alpha'} \land x \in B_{\beta'}$$ If the LHS is true we know there must exist an $A_{\alpha'}$ such that $x \in A_{\alpha'}$ , and similarly there must exist a $B_{\beta'}$ such that $x \in B_{\beta'}$ . Now consider the RHS $x \in A_{\alpha'} \land x \in B_{\beta'}$ . If this is true, then I would argue that the LHS is true because the argument "" $x \in P \implies x \in P \cup Q$ for any set $Q$ "" is generalisable as "" $x \in A_{\alpha'} \implies x \in \bigcup A_\alpha$ "". Am I right? My Doubt 2 My next doubt arises from the following statement: $$x \in A_{\alpha'} \cap B_{\beta'} \iff x \in \bigcup_{(\alpha,\beta) \in I \times J} (A_{\alpha} \cap B_{\beta})$$ The nature of my doubt is the same as Doubt 1.","['elementary-set-theory', 'solution-verification']"
4905730,Smoothest car trip,"I was thinking about the trajectory of a car over time from a point $A$ to $B$ and what would be the motion that would feel most smooth for a passenger. If we consider the position as a function of time to be $$\vec{s}(t) = \vec{s_0}+f(t/T) \Delta\vec{s}$$ Where $\vec{s_0}$ is the initial position, $T$ the total travel time and $\Delta\vec{s}$ the difference between the final position and initial position. Then, $f(x)$ will be a function that dictates how ""smooth"" the car trip will be. But to more rigorously define it, its properties should be (in the interval $[0,1]$ ): $f(x)$ is infinitely differentiable $f(0)=0$ and $f(1)=1$ $\frac{d^{n}f}{dx^{n}}=0$ for all $n \geq 1$ at $x=0$ and $x=1$ (The car is absolutely at rest in the beginning and at the end) $f(x)$ is monotonically increasing $f(x+1/2)-1/2$ is an odd function (it is ""symmetric"" with respect to the midpoint of the interval $x=1/2$ ) The maximum value of $|\frac{d^2f}{dx^2}|$ is minimized (minimize the acceleration felt by the passenger) It is ""easy"" to find a function that satisfies the first 5 properties. This video , for example, offers this function: $$f(x) = \frac{e^{-1/x}}{e^{-1/x}+e^{-1/(1-x)}}$$ However, can there be a function with a smaller maximum acceleration/2nd derivative? For this particular function above, the maximum value is quite high, at $9.841$ when $x=0.218$ , and, on a side note, it would provide a very bad car trip! For a 60km trip that takes 60 minutes, it would take more than 6 minutes just to reach 1km/h! Edit: After seeing @Alex K answer, I noticed that the 1st derivative is very similar to a triangle. In fact, it seems to me to be the case that if it is indeed a triangle, the maximum 2nd derivative would be minimized and be equal to 4. So, the 2nd derivative would be 4 from 0 to 1/2 and then -4 from 1/2 to 1. This results in this function: $$f(x) = \left\{
\begin{aligned}
  2x^2, x\leq\frac{1}{2} \\
  -2x^2 + 4x - 1, x > \frac{1}{2}
\end{aligned}
\right.$$ This function doesn't satisfy all the properties and as such is not physically realistic, but does seem to imply that the lower bound for the smallest maximum 2nd derivative of $f(x)$ is 4.","['optimization', 'calculus', 'functions', 'continuity']"
4905823,Is the relationship of the radical axis of two circles to their tangent parabola already known?,"Frankly, I did not want to publish this question on the site because I have the full proof, but I nevertheless publish it because I would like to know whether this important feature that I came up with is already known or not, so what I am looking for in this question is a reference that mentions this property if it is known, and if someone has an engineering proof, he is welcomed in the answers, so far I have proof of analytical geometry only If we have two circles in a parabola, each touching the parabola at two points, then the midpoint of the centers of the two circles is a distance from the radical axis of these two circles, a distance equal to the distance between the focal point and the guide in the parabola. In the image below:
The green dot is the focus of the parabola and the green line is the guide of the parabola. The blue dot is the midpoint between the centers of the two circles. The blue line is the radical axis of the two circles. The red and purple dots represent the centers and points of contact of the two circles.
The four dimensions between points on the axis of symmetry and between lines of the same color are equal This is my proof using coordinate geometry in Arabic: In the last image, it shows how you took advantage of the property to create a circle passing through a known point and touching a parabola at two points (there are two of these circles).","['conic-sections', 'geometry', 'reference-request']"
4905838,Can this integral equation be solved?,"I am trying to find out the Stationary Distribution of a Continuous State Space Markov Chain. For example, consider a Gaussian Transition Kernel that describes the probability of moving from point $x$ to $x'$ : $$T(x, x') \sim N(x, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x'-x)^2}{2\sigma^2}}$$ I am interested in determining what is the Stationary Distribution for this process. I know that to determine the Stationary Distribution for this problem, we have to find a distribution $\pi(x)$ such that the following relationship holds (Detailed Balance Condition, also called the Reversibility Condition): $$\int T(x, x')\pi(x)dx = \pi(x'), \quad \forall x'$$ Is it possible to solve the above equation for $\pi(x)$ ? I know that in the context of Discrete Time and Discrete State Space Markov Chains, we can find the Stationary Distribution $\pi$ by solving the following equation: $$\pi = P \cdot \pi$$ However, I am not sure how to solve the equivalent equation for the Continuous State Space version. Here is what I tried so far. 1) Intuition When dealing with functions of the Gaussian Distribution, properties of the Gaussian Functions tend to also be Gaussian. This makes me believe that if the Stationary Distribution exists, it also Gaussian. I know that the Gaussian Distribution is symmetric around the mean ( Proof that the gaussian distribution is ""symmetric"". ), i.e. $f(x) = f(-x)$ . This means: $$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} = f(-x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(-x-\mu)^2}{2\sigma^2}}$$ I thought of the following way to show this: If the Gaussian Distribution is symmetric around the mean point $\mu$ , this means that any two points $x_1$ and $x_2$ that are equidistant from $\mu$ will have the following properties: $$x_1 - \mu = \mu - x_2$$ $$\mu = \frac{x_1 +x_2}{2}$$ $$f(x_1) = f(x_2)$$ $$f(\mu + \Delta) = f(\mu - \Delta)$$ Symmetry using this condition implies that: $$ \exp\left[ - \frac{1}{2}\left(\frac{x_1 - \mu}{\sigma}\right)^2\right] = \exp\left[-\frac{1}{2}\left(\frac{x_2 - \mu}{\sigma}\right)^2\right] $$ Looking at the LHS, we see: $$ \exp\left[-\frac{1}{2}\left(\frac{x_1 - \frac{{x_1 + x_2}}{2}}{\sigma}\right)^2\right] =  \exp\left[-\frac{1}{2}\left(\frac{\frac{2x_1 - (x_1 + x_2)}{2}}{\sigma}\right)^2\right] = \exp\left[-\frac{1}{2}\left(\frac{\frac{x_1 - x_2)}{2}}{\sigma}\right)^2\right] = \exp\left[-\frac{1}{2}\left( \frac{ 
 {x_1 - x_2)}}   {2\cdot\sigma}\right)^2\right]$$ Now, the same is true for the RHS: $$ \exp\left[-\frac{1}{2}\left(\frac{x_2 - \frac{{x_1 + x_2}}{2}}{\sigma}\right)^2\right] =  \exp\left[-\frac{1}{2}\left(\frac{\frac{2x_2 - (x_1 + x_2)}{2}}{\sigma}\right)^2\right] = \exp\left[-\frac{1}{2}\left(\frac{\frac{x_1 - x_2)}{2}}{\sigma}\right)^2\right] = \exp\left[-\frac{1}{2}\left( \frac{ 
 {x_1 - x_2)}}   {2\cdot\sigma}\right)^2\right]$$ Thus, we can see that LHS = RHS and that the Gaussian Distribution is symmetric around the mean. Going back to the Detailed Balance Condition, we can see that: $$\pi(x)T(x, x') = \pi(x')T(x', x)$$ Thus, if we assume that $\pi(x)$ is a Gaussian, we can write: $$\frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x-\mu)^2}{2\mu^2}} \cdot \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x'-x)^2}{2\sigma^2}} = \frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x'-\mu)^2}{2\mu^2}} \cdot \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-x')^2}{2\sigma^2}}$$ Using the fact that the Gaussian distribution is symmetric, i.e $e^{-\frac{(x'-x)^2}{2\sigma^2}} = e^{-\frac{(x-x')^2}{2\sigma^2}}$ , we can write: $$\frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x-\mu)^2}{2\mu^2}} = \frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x'-\mu)^2}{2\mu^2}}$$ Thus, based on the assumption that $\pi(x)$ is Gaussian, we can show that $\pi(x)$ satisfies the Detailed Balance Condition and is a Stationary Distribution: $$\int T(x, x')\pi(x)dx = \pi(x'), \quad \forall x'$$ Substituting the Gaussian transition kernel $T(x, x')$ and assume $\pi(x)$ is a Gaussian distribution, we get: $$\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x'-x)^2}{2\sigma^2}} \cdot \frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x-\mu)^2}{2\mu^2}} dx = \frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x'-\mu)^2}{2\mu^2}}, \quad \forall x'$$ $$\frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x'-\mu)^2}{2\mu^2}} = \frac{1}{\sqrt{2\pi\mu^2}}e^{-\frac{(x'-\mu)^2}{2\mu^2}}, \quad \forall x'$$ But this still has not been proved in my opinion. 2) Computer Simulation I tried to write a computer simulation in R (Programming Language) that simulates random draws from this Transition Kernel and then uses the Kolmogorov-Smirnov test statistic to compare the EDF (empirical distribution function) to the theoretical CDF (cumulative distribution function) of a Normal Distribution. In my opinion, this might be able to show that the Stationary Distribution of this Transition Kernel is Gaussian. library(ggplot2)
library(gridExtra)

 #parameters of the Gaussian transition kernel
mu <- 0
sigma <- 1

# total number of simulations (n) and burn-in observations (m)
n <- 1000
m <- 100  


chain <- numeric(n)


chain[1] <- rnorm(1, mean = mu, sd = sigma)

#arkov chain
for (i in 2:n) {
    chain[i] <- rnorm(1, mean = chain[i - 1], sd = sigma)
}


df <- data.frame(Value = chain, Time = 1:n)

# indicator variable for before/after burn-in
df $Period <- ifelse(df$ Time <= m, ""Before Burn-in"", ""After Burn-in"")
df1 = df[df$Period == ""After Burn-in"",]

# trajectory of the Markov chain 
p1 <- ggplot(df, aes(x = Time, y = Value)) +
    geom_line(aes(color = Period)) +
    geom_vline(xintercept = m, linetype = ""dashed"", color = ""gray"") +  # Add burn-in line
    labs(
        title = paste(""Trajectory of the Markov Chain ("", n, "" simulations)"", sep = """"),
        subtitle = paste(""Estimated Mean (μ) after Burn-in:"", round(mean(chain_burned), 3),
                         ""\nEstimated SD (σ) after Burn-in:"", round(sd(chain_burned), 3)),
        x = ""Time"",
        y = ""State""
    ) +
    theme_minimal() +
    scale_color_manual(name = ""Legend"", values = c(""Before Burn-in"" = ""blue"", ""After Burn-in"" = ""red""))

# histogram
p2 <- ggplot(df1, aes(x = Value)) +
    geom_histogram(aes(y = ..density.., fill = ""Histogram""), bins = 50, alpha = 0.5) +
    stat_function(
        fun = dnorm,
        args = list(mean = mean(chain_burned), sd = sd(chain_burned)),
        aes(color = ""Gaussian Density""),
        size = 1
    ) +
    labs(
        title = paste(""Estimated Stationary Distribution ("", n, "" simulations)"", sep = """"),
        subtitle = paste(""Estimated Mean (μ) after Burn-in:"", round(mean(chain_burned), 3),
                         ""\nEstimated SD (σ) after Burn-in:"", round(sd(chain_burned), 3)),
        x = ""State"",
        y = ""Density""
    ) +
    theme_minimal() +
    scale_fill_manual(name = ""Legend"", values = c(""Histogram"" = ""skyblue"")) +
    scale_color_manual(name = ""Legend"", values = c(""Gaussian Density"" = ""red""))

#Kolmogorov-Smirnov statistic
ks_result <- ks.test(chain_burned, ""pnorm"", mean = mean(chain_burned), sd = sd(chain_burned))

# EDF vs. CDF plot

p3 <- ggplot(df1, aes(x = Value)) +
    stat_ecdf(aes(color = ""EDF"")) +
    stat_function(
        fun = pnorm,
        args = list(mean = mean(chain_burned), sd = sd(chain_burned)),
        aes(color = ""CDF""),
        linetype = ""dashed"",
        size = 1
    ) +
    labs(
        title = paste(
            ""EDF of Observed Data vs. CDF of Normal Distribution ("",
            n,
            "" simulations, KS Statistic:"",
            round(ks_result $statistic, 3),
            "", p-value:"",
            format(ks_result$ p.value, digits = 3),
            "")"",
            sep = """"
        ),
        subtitle = if (ks_result$p.value > 0.05) ""Conclusion: EDF is the same as CDF"" else ""Conclusion: EDF is different from CDF"",
        x = ""State"",
        y = ""Cumulative Probability""
    ) +
    theme_minimal() +
    scale_color_manual(name = ""Legend"", values = c(""EDF"" = ""blue"", ""CDF"" = ""red""))


grid.arrange(p1, p2, p3, ncol=1) However, I am not still not sure if this counts as a proof In general, if we start with a Gaussian Transition Kernel, is it possible to derive the Stationary Distribution from first principles, i.e solve the following integral equation? $$T(x, x') \sim N(x, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x'-x)^2}{2\sigma^2}}$$ $$\int T(x, x')\pi(x)dx = \pi(x'), \quad \forall x'$$ $$\pi(x) \sim ???$$ Or unlike the Discrete State Space version, solving this equation for $\pi$ in the continuous state space version is simply not possible? Thanks!","['integration', 'probability']"
4905848,Why does set-theoretic union and intersection operate on reverse logic?,"In set theory, $A \cup B$ is logically defined as $\{x : x \in A \lor x \in B\}$ . In set theory, the result of unionizing A with B is a bigger set, but in logic, ""or"" is a softening operation. In set theory, $A \cap B$ is logically defined as $\{x : x \in A \land x \in B\}$ . In set theory, the result of intersecting A with B is a smaller set, but in logic, ""and"" is a strengthening operation. Why do the extensional results of union and intersection go in reverse from their logical definition?","['elementary-set-theory', 'soft-question', 'logic', 'intuition']"
4905881,Why are Contour Integrals defined the way they are?,"I have a question about Contour Integrals. Contour Integrals are defined as $$\int_{C} f(z)dz = \int_{a}^{b} f(z(t))z'(t)dt \tag{1}$$ . But why define it this way? The textbook I'm using (Complex Variables and Applications by Brown and Churchill) simply gave this definition with little explanation. I read that Contour Integrals have no geometric interpretation and are used instead to help evaluate real-valued definite integrals. But as someone who just finished Calculus 2 and 3 where all the integrals were constructed from Riemann Sums and had a clear geometric meaning, this seems strange to me. I notice that this definition looks similar to the definition of a line integral from Calculus 3, $$\int_{a}^{b} f(r(t))|r'(t)|dt \tag{2}$$ . But they are not quite the same. Can anyone please explain that?","['complex-analysis', 'definition', 'contour-integration', 'intuition']"
4905904,Prove The class Recset of recursive sets is the same as the class of all sets.,"This is from spring18 mcs.pdf . recursive set definition Definition 8.3.1. The class of recursive sets Recset is defined as follows: Base case: The empty set $\varnothing$ is a Recset. Constructor step: If S is a nonempty set of Recset’s, then S is a Recset. What to prove Theorem 8.3.2. The class Recset of recursive sets is the same as the class of all
sets. Proof. Everything in Recset is a set by definition, so we need only show that every
set is in Recset. Suppose to the contrary that a set S is not in Recset. So S must have an ele-
ment $N_0$ that is not in Recset, since otherwise S would by definition be in Recset.
Likewise, $N_0$ must have an element $N_1$ that is not in Recset, and $N_1$ must have an
element $N_2$ that is not in Recset, and so on. So we have an infinite sequence $$
S \ni N_0 \ni N_1 \ni N_2 \ni \ldots
$$ Therefore the set $^8$ $$
T ::= \{S, N_0 , N_1 , N_2 , \ldots\}
$$ has no member-minimal element, in violation of Foundation. $^8$ A rigorous proof, which we are skipping, requires careful use of the Choice axiom to prove that
T is a set. I prove $T$ is one set as follows: Here let the family of sets be $U,S,N_{0,\cdots}$ where $U$ is the universal set.
Then let the choice function be $U\to S,S\to N_0,\cdots$ . Then $T$ is the indexed set . I have questions about the ""infinite sequence"" part of the proof. Suppose one set $S=\{1\}$ is not in Recset. Based on the Constructor step, since at each step we can only have elements from Recset and the Base case only has $\varnothing$ , then Recset should only have $\varnothing$ as its minimal unit similar to von Neumann ordinals . Let us follow the process in ""Proof"". Then $N_0=1$ . But now we can't proceed further since $1$ isn't one set . What is wrong about my process after assuming $S=\{1\}$ ?","['elementary-set-theory', 'axiom-of-choice', 'solution-verification', 'set-theory']"
4905925,"Is $\text{Hom}(A,\mathbb{Z})$ a product of free abelian groups for all abelian groups $A$?","Let $A$ be an abelian group, and consider the abelian group $\text{Hom}(A,\mathbb{Z})$ of homomorphisms from $A$ to $\mathbb{Z}$ . What can be said about this group? Since $\mathbb{Z}$ is torsion-free, so is $\text{Hom}(A,\mathbb{Z})$ . If $A$ is finitely generated, then so is $\text{Hom}(A,\mathbb{Z})$ . It follows that for finitely generated $A$ , $\text{Hom}(A,\mathbb{Z})$ is free of finite rank, i.e. isomorphic to $\mathbb{Z}^n$ for some non-negative integer $n$ . When $A = \bigoplus_{n=1}^{\infty} \mathbb{Z}$ , then $\text{Hom}(A,\mathbb{Z}) \cong \prod_{n=1}^{\infty} \mathbb{Z}$ , which is not a free group (as proved by Baer). When $A = \prod_{n=1}^{\infty} \mathbb{Z}$ , then $\text{Hom}(A,\mathbb{Z}) \cong \bigoplus_{n=1}^{\infty} \mathbb{Z}$ (proved by Specker). Since $\mathbb{Z}$ is reduced (contains no divisible elements aside from $0$ ), so is $\text{Hom}(A,\mathbb{Z})$ . Slightly more is true: every non-zero element of $\text{Hom}(A,\mathbb{Z})$ is a multiple of an element that is only divisible by $\pm 1$ . Is there anything more that can be said about $\text{Hom}(A,\mathbb{Z})$ ? In particular, is $\text{Hom}(A,\mathbb{Z})$ always a product of free abelian groups? If not, what kind of isomorphism types of abelian groups arise as $\text{Hom}(A,\mathbb{Z})$ ?","['group-homomorphism', 'group-theory', 'abelian-groups', 'free-abelian-group']"
4905930,Check if the sample mean squared is an unbiased estimator for the mean squared,"My professor gave us the following problem :
Let $ \{ y_1, y_2, \ldots, y_N \}$ be a random sample drawn from $N(\theta, \sigma^2)$ where $ \sigma^2$ is known.
We want to check if this estimator is unbiased for $\theta^2$ and if it may be consistent. We want to compute the ML estimate of $\theta ^2$ .
Since the ML estimator of $\theta$ is $\hat{\theta}_N = \bar{y}_N $ , we could get the ML estimate of $\theta^2$ using the invariance principle by setting $\hat{\theta}_N^2 = \bar{y}_N^2$ .
And then it provides as solution: $$ E_\theta [\bar{y}_N^2] = \frac{(N-1)^2 + N}{N^2} \theta^2 + \frac{\sigma^2}{N}$$ which I believe to be wrong since this is what I did: $$E_\theta [\bar{y}_N^2] = E (\frac{1}{N^2} \sum (y_i^2)) = \frac{1}{N^2} \sum E(y_i)^2 = \frac{1}{N^2} (\sum Var(y_i) + E(y_i)^2) = \frac{1}{N^2} (N \sigma^2 + N^2 \theta^2) = \frac{\sigma^2}{N} + \theta^2$$ which becomes unbiased as N goes to infinity. Is my solution correct?",['statistics']
4905952,Is it possible to define algebraic spaces as functors on rings?,"Every definition I have seen of algebraic spaces starts with the category of schemes (usually of $S$ -schemes), and defines algebraic spaces as certain sheaves on this category, with respect to étale or fppf topology (depending on the definition), which is ""close to being representable by a scheme"". But I have heard several people defend the idea that algebraic spaces are arguably more natural than (or at least as natural as) schemes as the ""basic objects"" of algebraic geometry. That makes me wonder if one could skip the middleman, and directly give a workable definition of algebraic spaces with no reference to schemes, and retrieve schemes as a suitable subcategory as an afterthought. The most logical way to do that would be to define algebraic spaces as certain presheaves on $\mathbf{CRing^{op}}$ , similar to the functor-of-points definition of schemes (where schemes are Zariski sheaves on $\mathbf{CRing^{op}}$ which are locally representable). I guess the first question is whether an algebraic space is characterized by its functor of points restricted to affine schemes (ie to rings).","['algebraic-stacks', 'algebraic-geometry', 'schemes']"
4905973,A property of $C^1(\mathbb{R})$ functions,"Let $f\in C^1(\mathbb{R})$ and let $x_0\in P$ where $P$ is a perfect set.
I want proof that $\forall \varepsilon>0$ $\exists\delta>0$ with $\lvert f(x)-f(y)-f'(y)(x-y) \rvert<\varepsilon\lvert  x-y\rvert$ $\forall x,y\in P$ , $\lvert x-x_0\rvert<\delta$ , $\lvert y-x_0\rvert<\delta$ . My attempt: let $\varepsilon>0$ .
Because $F'$ is continous in $x_0$ , exists $\delta>0$ with $\lvert f'(x)-f'(x_0)\rvert<\dfrac{\varepsilon}{2}$ for all $x\in\mathbb{R}$ , $\lvert x-x_0\rvert<\delta$ . Let now $x,y\in P$ , $x\neq y$ with $\lvert x-x_0\rvert<\delta $ and $\lvert y-x_0\rvert<\delta$ .
For MVT theorem exists $x'\in (x,y)\cup (y,x)$ with $f'(x')=\dfrac{f(y)-f(x)}{y-x}$ . Then $\lvert f(x)-f(y)-f'(y)(x-y)\rvert=\biggl\lvert \dfrac{f(x)-f(y)}{x-y}-f'(y)\biggr\rvert\lvert x-y\rvert=\lvert f'(x')-f'(y)\rvert\lvert x-y\rvert<[ \lvert f'(x')-f'(x_0)\rvert+\lvert f'(x_0)-f'(y)\rvert] \lvert x-y\rvert$ . I control $\lvert f'(x_0)-f'(y)\rvert<\dfrac{\varepsilon}{2}$ because $\lvert y-x_0\rvert<\delta$ . How i can control instead $\lvert f'(x')-f'(x_0)\rvert$ ? I have to distinguish position of $x,y,x_0,x'$ to evaluate $\lvert x'-x_0\rvert$ or the hypothesis that $P$ is perfect is relevant? Thanks in advance.","['continuity', 'solution-verification', 'derivatives', 'real-analysis']"
4906016,Why would solving #MATCHING(bipartite) problem efficiently solve #MATCHING efficiently?,"Im gathering information about the matching counting problem for a graph $G$ (#MATCHING( $G$ )). I found that for the specific case of $G$ being a bipartite graph then the problem has a simple (not efficient) solution: #MATCHING( $G_{bipartite}$ ) $=perm(M(G_{bipartite})$ ) where $M(G)$ is the adjacence matrix of the graph and perm( $M$ ) is the permenent of the matrix $M$ . In the article i read this from, it says that solving the bipartite problem efficiently or equivalently solving the permanent of a matrix efficiently would mean that the general matching problem could be solved efficiently. This means that there has to exist a polynomial time algorithm that thransforms any given matching counting problem into a bipartite graph matching counting problem. I thought that maybe given a graph $G=([2n],E)$ the matching counting problem for said graph could be solved by separating the problem into the $2nCn$ combinations of bipartite graphs that can be created with the vertices of $G$ . However, this method isn't polynomial right? I'm new to these terms so I might be wrong there or there might be a different algorithm. So my question is, why would solving the bipartite matching counting problem efficiently mean solving the matching counting problem efficiently? Thanks for the help P.D.: The article I read this on is this article in spanish ""J.Andres Montoya. LA PERMANENTE COMPLEJIDAD DE LA PERMANENTE"".","['matching-theory', 'discrete-mathematics', 'algorithms', 'computational-complexity', 'computer-science']"
4906038,Newton root finding and preserving automatic differentiation.,"I am applying Newton's root solving algorithm. Suppose the example problem below, solving for $g$ with fixed parameter, $s$ : $$ f(g;s) = g^2 - s = 0, \qquad g_{i+1}=g_i - \frac{f(g_i;s)}{\frac{df}{dg}(g_i;s)} $$ Clearly this has the solution: $$ g= \sqrt{s}, \qquad \frac{dg}{ds} = \frac{1}{2\sqrt{s}}, \qquad \frac{d^2g}{ds^2} = \frac{-1}{4s^{3/2}}  $$ When you search for ""Newton Algorithm"" and ""Automatic Differentiation"", the majority of the results/papers reference applying automatic differentiation to determine the derivative, $\frac{df}{dg}(g_i;s)$ at each step in the interation. I am not interested in this. What I am interested in is a formal proof that the following process can capture the accurate resultant sensitivities (i.e. $\frac{dg}{ds}$ and $\frac{d^2g}{dx^2}$ : First converge to solution using floating point arithmetic. Perform one more iteration using the current float iterate for $g_i$ and adding the AD variable sensitivity to $s$ . This will capture 1st deriavtive sensitivity accurately. To capture 2nd order sensitivity perform one further iteration . Empirically in all my tests this has not failed and is exactly accurate, but I would be more comfortable with a proof. For those interested in some abstract code which I have working according to the above definitions is... def f_and_df(g, s):
    f = g ** 2 - s
    df = 2*g
    return f, df

result = newton_root(f_and_df, g0=1.0, args=(2.0,))
# result = 1.4142135.....

result = newton_root(f_and_df, g0=1.0, args=(Dual(2.0, vars=[""s""]),))
# result = <Dual: 1.4142135, (""s""), [0.35355..]>

result = newton_root(f_and_df, g0=1.0, args=(Dual2(2.0, vars=[""s""]),))
# result = <Dual2: 1.4142135, (""s""), [0.35355..], [-0.088388..]> # Internal newton_root code
# # After float convergence
# # Final iteration method to preserve AD
f0, f1 = f(g1, *(*args, *final_args))
if isinstance(f0, (Dual, Dual2)) or isinstance(f1, (Dual, Dual2)):
    # Perform 1 extra iteration because of AD variable
    g1 = g1 - f0 / f1
if isinstance(f0, Dual2) or isinstance(f1, Dual2):
    # Perform a 2nd iteration because of order = 2
    f0, f1 = f(g1, *(*args, *final_args))
    g1 = g1 - f0 / f1
return g1","['newton-raphson', 'derivatives', 'algorithms']"
4906067,Second (and higher) derivatives of the adjugate operator for singular matrix,"Consider the operator $$
f(A): A \rightarrow adj(A)
$$ The derivative $Df_A (H)$ was given here and depends on the rank. I am interested in the second (and higher) derivatives in the $rank(A) = n-1$ case. Simply differentiating the resulting formula given in the linked answer does not work because the pseudo inverse is not differentiable at the singularity. Instead, I tried to derive the second derivative analogously as in the given answer by differentiating Jacobi's formula a second time and then taking the limit of $S_t$ for $t \rightarrow 0$ . However, I am getting stuck because I do not really understand how the linked answer went from equations $3$ , $4$ and $5$ to $6$ or if this approach really leads to the goal. In the end, I am interested in a procedure that can be implemented numerically efficiently, so directly working with Laplace's formula does not work.","['matrices', 'determinant', 'derivatives', 'matrix-calculus']"
4906087,Computing the connection form $\omega_1^2$ for $ds^2 = dt^2 + f(t)^2d\theta^2$,"For $S^1 \times I$ where $I \subset \mathbb{R}$ consider the following metric $$ds^2 = dt^2 + f(t)^2d\theta^2.$$ I want to compute $\omega_1^2$ , the connection form. I let $\theta^1 = dt$ and $\theta^2 = f(t)d\theta$ . By the compatibility with the metric and the first structure equations we have the system \begin{align*}d\theta^1 + \omega_1^1\wedge \theta^1 + \omega^1_2 \wedge \theta^2 & = 0,\\
d\theta^2 + \omega_1^2 \wedge \theta^1 + \omega_2^2 \wedge \theta^2 & = 0.
\end{align*} Write $\omega_i^j = a_i^j\theta^1 + \overline{a_i^j}\theta^2$ . From the previous system we get \begin{align*}
a_1^2 & = -\overline{a_1^1},\\
\frac{f'(t)}{f(t)} & = \overline{a_1^2} + a_2^2.
\end{align*} I can't fully describe $\omega_1^2$ with this information. Is there something more that I am forgetting and can help me conclude? I thought about using the second structural equation $$\Omega_1^2 = d\omega_1^2 + \omega_1^1 \wedge \omega^1_2 + \omega_2^1 \wedge \omega^2_2,$$ knowing that $\Omega_1^2 = K = -f''(t)/f(t)$ but it doesn't seem helpful. Question: Is it true that $\omega_1^1 = \omega_2^2 = 0$ ? Why is it? Because then we get $\omega_1^2 = -f'(t)d\theta$ which agrees (up to a minus sign?) with the expression that on wikipedia $$\omega_i^j = \sum_k \Gamma_{ki}^j\theta^k.$$ Thanks!","['connections', 'riemannian-geometry', 'differential-geometry']"
4906136,Is it possible to solve for a function of a differential in an integral equation?,"I am attempting to solve an equation for $\nu(x)$ . The exact equation is part of my PhD research and is needlessly complex for the core of my issue, I will give a simplified example instead: $\int_{R\backslash\{0\}} f(x,y) \nu(dy) = g(x)$ . Assume $f$ and $g$ are sufficiently well-behaved for this integral to work (feel free to choose specific $f$ and $g$ in your answer if you need more details to demonstrate a point) and $R$ represents the set real numbers. My idea is to somehow apply the fundamental theorem of calculus to both sides after differentiating to obtain a differential equation, but for whatever reason I am stuck on dealing with how $\nu$ would interact with the FTC part 1. This would be easy if we had something like $\nu(dx)=h(x)dx$ instead (for some function $h$ ), but I do not see how I could rewrite this equation like that without knowing $\nu$ already. For those wondering about context. I am trying to manually solve for the Levy measure of a particular stochastic process by equating the exponent of the Levy-Khintchine formula (left) to the exponent of the characteristic function (right). I have never been taught to manually solve for the Levy measure, we always just used intuition and comparison in class. So if there is any literature or formulas known to find a Levy measure directly, I will be grateful.","['integration', 'levy-processes', 'calculus', 'ordinary-differential-equations']"
4906170,Can we determine into how many regions the zero set of a polynomial divides the plane?,"Disclaimer: If you look at my other questions, you will see that I am still learning the basics of algebra. I am well aware that the answer to this question is likely to go beyond my current level of knowledge. This is fine however, as I am mainly asking this question to get some references as to which areas of algebra(ic geometry) I should study to understand questions like this, so that one day I can come back to it and understand fully what is going on. Consider a polynomial $p\in\Bbb R[X,Y]$ . If it has no roots, the roots are isolated points, or we are dealing with the zero polynomial the answer is trivial. We are therefore only interested in the case where $Z(p)$ is a one-dimensional subset of $\Bbb R^2$ . A simple example to consider is then: $$
aX^2 + bY^2 - c = 0
$$ where, even if we did not know that the zero set was an ellipse, we could reasonably guess that the curve is closed since both $X$ and $Y$ are bounded. This immediately implies that the curve divides the plane into two regions, one bounded and one unbounded. Another example is a hyperbola $$
aX^2 - bY^2 - c = 0
$$ where now $X$ and $Y$ are unbounded, the zero set consists of two disjoint one-dimensional sets and therefore it divides the plane into three unbounded areas. A more interesting case is the Tschirnhaus cubic: $$
X^3 - X^2 + Y^2 = 0
$$ which is a connected curve with a singularity, from which we can deduce that it divides the plane into three segments; One bounded and two unbounded. Now these are nontrivial conclusions, and require some work to arrive at and rigorously justify. Now I ask: Is there a single (algorithmic) way to determine into how many segments any given polynomial $$
\sum a_{ij}X^iY^j\quad a_{ij}\in \Bbb R
$$ will divide the plane, and which of them are bounded/unbounded? And if there is, can it be generalized to arbitrary many variables? And if it does not work over $\Bbb R$ , would it at least work over an algebraically closed field?","['algebraic-geometry', 'abstract-algebra', 'polynomials', 'real-algebraic-geometry']"
4906189,"Question about Proof of the Integrability of $f$ and $f_1,f_2,\dots,$ in Lebesgue's Dominated Convergence Theorem","I am self-studying measure theory and got stuck on part of the proof of the Lebesgue's Dominated Convergence Theorem: Theorem $\quad$ 2.4.5 $\quad$ (Lebesgue's Dominated Convergence Theorem) Let $(X,\mathscr{A},\mu)$ be a measure space, let $g$ be a $[0,+\infty]$ -valued integrable function on $X$ , and let $f$ and $f_1,f_2,\dots$ be $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ such that \begin{align}
    f(x) = \lim_{n\to\infty}f_n(x)\tag1
\end{align} and \begin{align}
    |f_n(x)| \leq g(x),\ n=1,2,\dots\tag2
\end{align} hold at $\mu$ -almost every $x$ in $X$ . Then $f$ and $f_1,f_2,\dots$ are integrable, and $\int fd\mu = \lim_{n\to\infty}\int f_nd\mu$ . When proving the theorem, the book claims that ""the integrability of $f$ and $f_1,f_2,\dots$ follows from that of $g$ ; see Proposition 2.3.8, Proposition 2.3.9, and part (c) of Proposition 2.3.4."" (I added these results at the bottom of this post.) I could not see how the integrability of $f$ and $f_1,f_2,\dots$ follow immediately from that of $g$ by applying these results. The most important reason is that the relationship (1) and (2) does not hold at every $x$ in $X$ , but instead, hold at $\mu$ -almost every $x$ in $X$ , which makes it illegal to directly apply Proposition 2.3.4 (c). So, I tried to prove it myself. Here are my questions for this post: 1. Is my proof of the integrability of $\mathbf{f}$ and $\mathbf{f_1,f_2,\dots}$ correct? (For example, I have doubts about the relation $\left\{x\in X:\lim_{n\to\infty}|f_n(x)| > g(x)\right\} \subseteq \bigcup_{n=1}^{\infty}\left\{x\in X:|f_n(x)|>g(x)\right\}$ I used in my proof.) 2. Am I overcomplicating anything here? (I am worried about this because the book's claim sounds like the proof shouldn't be that complicated.) 3. From my attempt below and the rest of proof of $\mathbf{\int fd\mu = \lim_{n\to\infty}\int f_nd\mu}$ presented in the book, it seems that we are not allow to conclude that $\mathbf{\int\lim_{n\to\infty}f_nd\mu = \lim_{n\to\infty}\int f_nd\mu}$ . But why? Here is my attempt to prove the integrability of $f$ and $f_1,f_2,\dots$ : Proof of the integrability of $\mathbf{f}$ and $\mathbf{f_1,f_2,\dots}$ $\quad$ We first prove the integrability of $\mathbf{f_1,f_2,\dots}$ . Let $n$ be a positive integer. The fact that $|f_n(x)|\leq g(x)$ holds almost everywhere implies that there is an $N\in\mathscr{A}$ such that $\mu(N)=0$ and $\{x\in X:|f_n(x)|>g(x)\}\subseteq N$ . Since $|f_n|$ and $g$ are $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions, the set $\{x\in X:|f_n(x)|>g(x)\} \in \mathscr{A}$ by Proposition 2.1.3. So, we can let $N = \{x\in X:|f_n(x)|>g(x)\}$ and conclude that $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ . Since $g$ is integrable and $[0,+\infty]$ -valued, it follows that $\int g^+d\mu = \int gd\mu < +\infty$ . Assume to the contrary that $\int|f_n|d\mu = \sup\left\{\int hd\mu:h \in \mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} = +\infty$ . Then, as $\int gd\mu<+\infty$ , there exists an $h\in\mathscr{S}_+$ such that $h\leq|f_n|$ and $\int hd\mu > \int gd\mu$ . Denote this $h$ by $h=\sum_{i=1}^ma_i\chi_{A_i}$ , where $a_1,\dots,a_m$ are nonnegative real numbers and $A_1,\dots,A_m$ are disjoint subsets of $X$ that belong to $\mathscr{A}$ . Then \begin{align}
\int|f_n|d\mu \geq \int hd\mu = \sum_{i=1}^ma_i\mu(A_i) > \int gd\mu = \sup\left\{\int pd\mu:p\in\mathscr{S}_+\ \text{and}\ p\leq g\right\}.
\end{align} The set $\{x\in X:h(x)>g(x)\}\neq\emptyset$ , for otherwise $h(x)\leq g(x)$ for all $x$ in $X$ would imply $\int hd\mu \leq \int gd\mu$ (see Proposition 2.3.4 (c)). Without loss of generality, suppose $\{x\in X:h(x) > g(x)\} = \bigcup_{i=1}^kA_i$ and $\{x\in X:h(x) \leq g(x)\} = \bigcup_{i=k+1}^mA_i$ . So for any $x\in A_i$ , $i=k+1,\dots,m$ , we have $a_i\leq g(x)$ . Define a function $p\in\mathscr{S}_+$ be such that $p(x) = a_i$ for all $x\in A_i$ where $i=k+1,\dots,m$ , and $p(x)=0$ for all $x\in A_i$ where $i=1,\dots,k$ . Then $p\leq g$ and $\int pd\mu = \sum_{i=1}^ma_i\mu(A_i)$ . Now, if $\mu(\{x\in X:h(x)>g(x)\}) = \mu(\bigcup_{i=1}^kA_i) = \bigcup_{i=1}^k\mu(A_i) = 0$ , then $\mu(A_i)=0$ for $i=1,\dots,k$ , so that $\int hd\mu = \sum_{i=1}^ma_i\mu(A_i) = \int pd\mu \leq \int gd\mu$ , contradicting the fact that $\int hd\mu > \int gd\mu$ . Thus, $\mu(\{x\in X:h(x)>g(x)\}) > 0$ . But $\{x\in X:h(x)>g(x)\}\subseteq\{x\in X:|f_n(x)|>g(x)\}$ . So we get $\mu(\{x\in X:|f_n(x)|>g(x)\})>\mu(\{x\in X:h(x)>g(x)\})>0$ , contradicting the fact that $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ . Therefore, \begin{align}
\int|f_n|d\mu = \sup\left\{\int hd\mu:h\in\mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} < +\infty.
\end{align} Hence, $|f_n|$ is integrable. By Proposition 2.3.8, $f_n$ is integrable. Next, we prove the integrability of $\mathbf{f}$ . By Proposition 2.1.5, $\lim_{n\to\infty}|f_n|$ and $g$ are $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions, it follows that $\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\in\mathscr{A}$ . Since $\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\subseteq\bigcup_{n=1}^{\infty}\{x\in X:|f_n(x)|>g(x)\}$ and $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ for all $n\in\mathbb{N}$ , it follows that $\mu(\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\})=0$ . Thus, $\lim_{n\to\infty}|f_n|\leq g$ holds $\mu$ -almost everywhere. Then, by a similar argument, we can conclude that \begin{align}
\int\lim_{n\to\infty}|f_n|d\mu = \sup\left\{\int d\mu:h\in\mathscr{S}_+\ \text{and}\ h \leq \lim_{n\to\infty}|f_n|\right\}<+\infty.
\end{align} Hence, $\lim_{n\to\infty}|f_n| = |\lim_{n\to\infty}f_n|$ is integrable. By Proposition 2.3.8, $\lim_{n\to\infty}f_n$ is integrable. Since $f(x) = \lim_{n\to\infty}f_n(x)$ holds at $\mu$ -almost every $x$ in $X$ , by Proposition 2.3.9, $\int fd\mu$ exists and $\int fd\mu = \int\lim_{n\to\infty}f_n(x)d\mu$ which is finite given the integrability of $\lim_{n\to\infty}f_n(x)$ . This implies $f$ is integrable. Thank you very much for any help! Results used in my attempt: Proposition 2.1.3 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathscr{A}$ , and let $f$ and $g$ be $[-\infty,+\infty]$ -valued measurable functions on $A$ . Then the sets $\{x \in A:f(x) < g(x)\}$ , $\{x \in A:f(x) \leq g(x)\}$ , and $\{x \in A:f(x) = g(x)\}$ belong to $\mathscr{A}$ . Proposition 2.1.5 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathscr{A}$ , and let $\{f_n\}$ be a sequence of $[-\infty,+\infty]$ -valued measurable functions on $A$ . Then (a) the functions $\sup_nf_n$ and $\inf_nf_n$ are measurable, (b) the functions $\limsup_{n\to\infty}f_n$ and $\liminf_{n\to\infty}f_n$ are measurable, and (c) the function $\lim_{n\to\infty}f_n$ (whose domain is $\{x\in A:\limsup_{n\to\infty}f_n(x)=\liminf_{n\to\infty}f_n(x)\}$ ) is measurable. Proposition 2.3.4 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, let $f$ and $g$ be $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ m and let $\alpha$ be a nonnegative real number. Then (a) $\int \alpha fd\mu = \alpha\int fd\mu$ , (b) $\int(f+g)d\mu = \int fd\mu+\int gd\mu$ , and (c) if $f(x)\leq g(x)$ holds at each $x$ in $X$ , then $\int fd\mu \leq \int gd\mu$ . Proposition 2.3.8 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . Then $f$ is integrable if and only if $|f|$ is integrable. If these functions are integrable, then $|\int fd\mu|\leq\int|f|d\mu$ . Proposition 2.3.9 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ and $g$ be $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ that agree almost everywhere. If either $\int fd\mu$ or $\int gd\mu$ exists, then both exist, and $\int fd\mu = \int gd\mu$ . Reference $\quad$ Measure Theory by Donald Cohn.","['integration', 'measure-theory', 'analysis', 'real-analysis', 'limits']"
4906190,A question about rational functions in complex analysis,"In Ahlfors's complex analysis $$
R(z)=\frac{P(z)}{Q(z)}
$$ given as the quotient of two polynomials. We assume, and this is essential, that $P(z)$ and $Q(z)$ have no common factors and hence no common zeros. $R(z)$ will be given the value $\infty$ at the zeros of $Q(z)$ . It must therefore be considered as a function with values in the extended plane, and as such it is continuous. The zeros of $Q(z)$ are called poles of $R(z)$ , and the order of a pole is by definition equal to the order of the corresponding zero of $Q(z)$ .
The derivative $$
R^{\prime}(z)=\frac{P^{\prime}(z) Q(z)-Q^{\prime}(z) P(z)}{Q(z)^2}
\tag{11}
$$ exists only when $Q(z) \neq 0$ . However, as a rational function defined by the right-hand member of (11), $R^{\prime}(z)$ has the same poles as $R(z)$ , the order of each pole being increased by one. In case $Q(z)$ has multiple zeros, it should be noticed that the expression (11) does not appear in reduced form. I can understand these two statements, why the does   the order of each pole being increased by one I can't see why is this true, The second statement I really have no idea what he means.","['complex-analysis', 'derivatives', 'rational-functions']"
4906202,What formula could be making this number-triangle?,"The coefficients of a polynomial in $\lambda$ in the Taylor expansion around $r=0$ of the integral $$ \int_{1-r}^{1+r} \frac{1+z^2-r^2}{z\sqrt{4 z^2 r^2 - (-1 + z^2 +r^2)^2}} \exp(-z/\lambda)\;dz $$ for successive $n$ are 1 1
1 2  3   3
1 3  9  24   45    45
1 4 18  78  285   810  1575   1575
1 5 30 180  975  4545 17325  50400   99225   99225
1 6 45 345 2475 16020 90720 434700 1686825 4961250 9823275 9823275
... What could be making this pattern?
I know that the last terms in each row are (2n-1)!! (2n+1)!! . (See https://oeis.org/A079484 .) The third column (starting with n=2 in the second row) is 3(-1 + n)(n)/2, this is like the Triangular Matchstick numbers, https://oeis.org/A045943 . The fourth column is (-1 + n)(n)(-7 + 5n)/2; this is like (offset by one index) the Heptagonal pyramidal numbers, https://oeis.org/A002413 . The fifth column (45,285,...) is like $(5/8)(-2 + n)(-1 + n)(n)(-9 + 7n)$ , starting with n=3. This seems promising but then the fifth column is $3/8 (-2 + n) (-1 + n) (n)(62 - 77 n + 21 n^2)$ ... Mathematica code to generate these polynomials (see the numerator of the pattern, and divide by $e^{-1/\lambda}\pi$ ) is: $Assumptions = {0 <= r <= 1, \[Lambda] > 0}

Series[(1/(Exp[-1/\[Lambda]]\[Pi]))Integrate[ (1 + z^2 - r^2)/(
   z Sqrt[4 z^2 r^2 - (-1 + z^2 + r^2)^2])
    Exp[-z/\[Lambda]], {z, 1 - r, 1 + r}], {r, 0, 2n}] For $n=3$ this becomes $$
\begin{align}
 & = 1 \\
 & +\frac{1 + \lambda}{4}\left(\frac{r}{\lambda}\right)^2 \\
 & +\frac{1 + 2 \lambda + 3 \lambda^2 + 3 \lambda^3}{64}\left(\frac{r}{\lambda}\right)^4 \\
 & +\frac{1 + 3 \lambda + 9 \lambda^2 + 24 \lambda^3 + 45 \lambda^4 + 45 \lambda^5}{2304}\left(\frac{r}{\lambda}\right)^6 . \\
\end{align}
$$ I'm wondering about the polynomials in the numerators here.
(The quantities in the denominator are $4^n(n!)^2$ , where for example 64 is the $n=2$ term.)","['integration', 'pattern-recognition', 'combinatorics']"
4906248,Find length of 3rd leg of a triangle (that is not a right triangle),"Assume I have any triangle $\triangle ABC$ . I know that given the lengths two sides of the triangle and angle between them, I can find the length of the third side. In other words, given values of $AB$ , $AC$ , and $\angle BAC$ , I can find $BC$ . What if instead of the angle, I know the length of $AD$ where $D$ is somewhere along $BC$ ( i.e. points $B$ , $C$ , and $D$ are collinear). Can I still find the length of $BC$ ? If I place the triangle on a cartesian plane with A at the origin and with the slope of BC $ = 1$ , then I can write 3 equations for the circles with radii AB, AC, and AD, and 3 more for the slopes of BC, BD, and DC each being $1$ . Will solving the 6 equations with 6 unknowns work? It wouldn't be fun. I question it working because other orientations of the triangle on the plane lead to different results. If I place the triangle in the plane with B at the origin and C and D along the x axis, I only have 4 unknowns. However, I only have 3 equations from the circles since the slopes are $0$ . I need another equation. Alternatively, if I place A at the origin and B (or C or D) along the x axis, I still only have 4 unknowns, but I get equations from 2 circles and 3 slopes and have too many equations. What am I missing? Is there an easier way?","['triangles', 'geometry']"
4906261,Convergence on Borel sets implies weak convergence,"Let $X$ be a locally compact space and let $M(X)$ denote all $\mathbb{C}$ -valued regular Borel measures on $X$ . Let $(\mu_n)_{n\in \mathbb{N}} \subset M(X)$ be a sequence. I want to show that $L(\mu_n) \to 0 $ for all $L\in M(X)^*$ is equivalent to $\sup_{n\in\mathbb{N}}\lVert \mu_n\rVert < \infty$ and $\mu_n(E) \to 0$ for all Borel sets $E$ . One direction follows from Banach-Steinhaus and taking $L_E$ to be the evaluation at the Borel set $E$ .
But what about the other direction? In particular, how does the boundedness of the supremum come into play?","['measure-theory', 'functional-analysis', 'weak-convergence', 'borel-measures']"
4906265,Find all square roots of this matrix using some practical method,"Find all matrices $A$ such that $$A=\displaystyle{\sqrt{\pmatrix{1 & 3 & -3 \\ 0 & 4 & 5 \\ 0 & 0 & 9}}}
$$ This is a UC Berkeley qualifying exam question. Now I know one method to solve this. Let $$A=\pmatrix{a & b & c \\ d & e & f \\ g & h & i}
$$ Now on squaring we'll get $9$ equations and can determine all variables. I would like to know that whether there is any practical approach for this question. My method is impractical and can be used only in theory. Any help is greatly appreciated.","['matrices', 'linear-algebra']"
4906277,Under which conditions the Leibniz rule is equivalent to defining the derivative as a limit?,"Let $A$ and $B$ be two $\mathbb{R}$ -vector spaces, and $F$ the $\mathbb{R}$ -vector space of ""smooth enough"" functions between them, where: sum is defined pointwise, and by ""smooth enough"" I mean that all needed limits exist (I don't want to deal with weird cases). With these assumptions, I think you can define the set $D$ of all operators from $F$ to $F$ , of this form: $ d(f)(x) = \lim_{\epsilon \rightarrow 0} ((f(x+\epsilon v) - f(x)) / \epsilon ) $ .  (clearly there's one for each $v$ ) (Is this ill-defined somehow?) Now, take $B$ to be an algebra over $\mathbb{R}$ , with some vector product. Now $F$ is an algebra over $\mathbb{R}$ too, with the product defined pointwise. Clearly the definition of $D$ still applies. But, you can now define a new set of operators, $L$ , defined to be the set of all linear operators from $F$ to $F$ that satisfy the Leibniz rule, $l(fg) = l(f)g + fl(g)$ . My question is: Are $L$ and $D$ the same set? If yes, why? (For context: it looks like they are the same in the case of $A=\mathbb{R}^d$ , $B=\mathbb{R}$ .) If no (as it's more likely in general- since the definition of $D$ doesn't use the product at all!): Can you provide a counterexample? And most importantly: which additional conditions on $B$ do you have to impose for $D$ and $L$ to be the same? For example: Is it only true on $B=\mathbb{R}$ ? What about $B=\mathbb{C}$ ? What if $B$ has finite dimension? And finally (which is of course my real question, although it is only a soft question): why does $L$ turn out to be a more interesting way than $D$ to extend the concept of derivative to more abstract structures? What's interesting about derivatives over algebras? Any insight on this would be appreciated.","['derivatives', 'differential-algebra', 'differential-geometry']"
4906278,Number in Base b as Dot Product,"Question out of curiosity: Any $k$ digit number $a_1 a_2 a_3 ... a_k$ written in base $b$ can be thought of as the dot product of a digits vector $a = \langle a_1, a_2, a_3, ..., a_k\rangle$ and a vector of powers of the base $b=\langle b^{k-1}, b^{k-2}, ..., b^1, b^0\rangle$ ( For instance the number $365=3\cdot10^2+6\cdot10^1+5\cdot10^0$ ). Given the cosine angle formula $$\frac{a \cdot b}{|a| |b|}= \cos(\theta)$$ is there a meaningful interpretation of the angle in the right hand side? For small k, it is easy to draw. Like a cube grid of points sitting on some space https://www.desmos.com/3d/idxu82btpv and the base vector goes along a parametric curve. Edit: Considering 365 vs 653, it seems like it has something to do with the ""decreasingness"" of digits given the sorted nature of b. Wondering what the pattern looks like.","['algebra-precalculus', 'number-systems', 'linear-algebra']"
4906366,"Formula for higher-order derivatives of the logarithm of a function, $\frac {d^m}{dx^m} \log h(x)$","I am trying to see if I can get a general formula for the $m$ th derivative of the logarithm of a function. Specifically, I am trying to find the $m$ th derivative of $g(x) = \log h(x)$ . Let $h^{(m)}(x) = \frac {d^m}{dx^m} h(x)$ denote the $m$ th derivative of $h(x)$ . Then, we see that the first derivative is \begin{equation}
\frac d{dx} g(x) = \frac{h^{(1)}(x)}{h(x)}
\end{equation} while the second derivative is \begin{equation}
\frac {d^2}{dx^2}g(x) = \frac{h^{(2)}(x)}{h(x)} - \left[\frac{h^{(1)}(x)}{h(x)}\right]^2,
\end{equation} and the third derivative is \begin{equation}
\frac {d^3}{dx^3}g(x) = \frac{h^{(3)}(x)}{h(x)} - \frac{3h^{(1)}(x)h^{(2)}(x)}{h^2(x)} +  2\left[\frac{h^{(1)}(x)}{h(x)}\right]^3,
\end{equation} and the fourth derivative is \begin{equation}
\frac {d^4}{dx^4}g(x) = \frac{h^{(4)}(x)}{h(x)} - \frac{h^{(3)}(x)h^{(1)}(x)}{h^2(x)} - 3\left[\frac{h^{(2)}(x)}{h(x)}\right]^2 - 3\frac{h^{(1)}(x)h^{(3)}(x)}{h^2(x)} +   \frac{12[h^{(1)}(x)]^2h^{(2)}(x)}{h^3(x)} - 6\left[\frac{h^{(1)}(x)}{h(x)}\right]^4.
\end{equation} But I can not see a way to specify a general formula for the $m$ th order derivative of $g(x)$ . Is there such a general formula possible? Many thanks for any pointers and suggestions!","['derivatives', 'logarithms']"
4906387,Contradiction (or mistake) in $\displaystyle \iint x^y ~ dx ~ dy$,"I came across this double integral at an Instagram post and I tried to solve it. The integral in question is the following: $$I = \iint x^y ~ dx ~ dy$$ So, I begun by calculating the inner integral first getting: $$\int x^y ~ dx = \frac{{x}^{y+1}}{y+1} + c_1$$ Then I have to integrate this with respect to $\displaystyle y$ : $$\int \frac{{x}^{y+1}}{y+1} + c_1 ~ dy = \int \frac{{x}^{y+1}}{y+1} ~ dy + \int c_1 ~ dy = \int \frac{{x}^{y+1}}{y+1} ~ dy + {c}_{1}y$$ I figured I'd solve the first integral with integration by parts using this formula: $$\int u ~ dv = uv - \int v ~ du$$ $$u = {x}^{y+1}$$ $$du = {x}^{y+1} \ln(x) ~ dy$$ $$dv = \frac{1}{y+1} ~ dy$$ $$v = \ln \bigl( |x+1| \bigl)$$ Plugging all this back to the formula I get: $$\int \frac{{x}^{y+1}}{y+1} ~ dy = {x}^{y+1} \ln \bigl( |x+1| \bigl) ~ - \int \ln \bigl( |x+1| \bigl) {x}^{y+1} \ln(x) ~ dy = {x}^{y+1} \ln \bigl( |x+1| \bigl) ~ - {x}^{y+1} \ln \bigl( |x+1| \bigl) ~ + c_2 = c_2$$ Therefore I get the answer to the original integral to be: $$I = \iint x^y ~ dx ~ dy = {c}_{1}y + c_2$$ This did seem odd and I used Wolfram Alpha to check my answer. Acccording to Wolfram the correct answer is: $$I = \iint x^y ~ dx ~ dy = \text{Ei}\bigl( (y+1) \ln(x) \bigl) ~ + {c}_{1}x + c_2$$ Ei is supposed to be the exponensial integral: $$\text{Ei}(x) = - \int_{-x}^{\infty} \frac{{e}^{-t}}{t} ~ dt$$ I did some research on this and I figured out that Wolfram (and some other sites) solve this integral using u-substitution. So my question is, did I do something wrong in the calculation process or is integration by parts prohibited in this integral and if so why?. Also, I noticed that the first constant is multiplied with x instead of y even if the integral is with respect to y, which is weird.","['integration', 'multivariable-calculus', 'calculus', 'solution-verification', 'indefinite-integrals']"
4906405,How can I evaluate the Gaussian Integral using power series?,"It's a well known result that the Gaussian integral $$\int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}$$ evaluates to $\frac{\sqrt{\pi}}{2}$ . This result can be obtained using double integrals with polar coordinates, among other things, but I'm particularly interested in evaluating this integral using power series. The power series of $e^{-x^2}$ is $$\sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{n!}$$ and integrating this, we get $$\sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)}$$ This expression is our integral (and it also has a radius of convergence of infinity, so we know it’ll approximate our integral function everywhere), so to evaluate it, we plug in our bounds. The expression is 0 for the bound of 0, so we're just looking for the limit of this power series as it tends to infinity. Using graphing software, with enough terms from the power series, I can see that the function does indeed spend a while at $\frac{\sqrt{\pi}}{2}$ before diverging. So, how can I analytically prove that the limit of the power series $$\lim_{x\to\infty} \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)}$$ as $x$ approaches infinity is $\frac{\sqrt{\pi}}{2}$ ?
Thanks for your help!","['calculus', 'real-analysis']"
4906509,Proving that a function is one-to-one (injective),"The question is to show that the function $ f(x) =  2x + |\cos x|$ is both one-one and onto. I have managed to show that it is onto but got stuck at proving that it is also one-to-one. To prove this I thought of using the first derivative and showing that the function is always increasing. For both cases of the absolute value, the derivative is $f'(x) = 2 - \sin x$ or $f'(x)=2+ \sin x$ which are both strictly greater than zero for all real values of x. However, at the points where $\cos x$ changes sign, the function is not differentiable. My question is, how can I show that the function is increasing at these points as well? Is there some calculus-based method for these points as well?","['functions', 'absolute-value']"
4906516,"Irrationality of numbers that are the sum of reciprocal factorials, like $e.$","I wish to prove that for any infinite $A\subset \mathbb{N},\ \displaystyle\sum_{k\in A} \frac{1}{k!}$ is irrational, in the same manner that Rudin proves $e$ is irrational. I should mention the trivial case where $\vert\mathbb{N}\setminus A\vert$ is finite, for then $\displaystyle\sum_{k\in A} \frac{1}{k!} = \left(e-\frac{1}{0!}\right) - \sum_{k\in (\mathbb{N}\setminus A)} \frac{1}{k!}, $ which is $e$ subtract a rational number and is therefore irrational. Rudin's proof that $e:= \displaystyle\sum_{k=0}^{\infty} \frac{1}{k!}$ is irrational goes as follows. Define $ \displaystyle s_n:= \sum_{k=0}^{n} \frac{1}{k!}.\ $ Then, $e - s_n = \displaystyle\sum_{k=n+1}^{\infty} \frac{1}{k!} < \frac{1}{(n+1)!}\left\{ 1 + \frac{1}{n+1} + \frac{1}{\left(n+1\right)^2} + \ldots \right\} = \frac{1}{n!n},\ $ so that $$ 0 < e-s_n < \frac{1}{n!n}. \tag{1}\label{eq1} $$ Now suppose $e$ is rational, that is, $e=p/q,$ where $p,q$ are positive integers. By $\eqref{eq1},$ $$ 0 < q!(e-s_q) < \frac{1}{q}. \tag{2}\label{eq2} $$ By our assumption, $q!e$ is an integer. Since $$ q!s_q = q!\left( 1 + 1 + \frac{1}{2!} + \ldots + \frac{1}{q!} \right) $$ is an integer, we see that $q!(e-s_q)$ is an integer. Since $q\geq 1,\ \eqref{eq2}\ $ implies the existence of an integer between $0$ and $1.$ We have thus reached a contradiction. Now onto my attempted proof that for an infinite $A\subset \mathbb{N},\ f:= \displaystyle\sum_{k\in A} \frac{1}{k!}$ is irrational. First, write $A$ as an increasing sequence of integers: $A = \left( a_k \right)_{k=1}^{\infty}.$ Define $ \displaystyle t_n:= \sum_{k=1}^{n} \frac{1}{a_k!}.\ $ Then, $$f - t_n = \sum_{k=n+1}^{\infty} \frac{1}{a_k!}< \frac{1}{a_{n+1}!}\left\{ 1 + \frac{1}{(a_{n+1}+1)(a_{n+1}+2)\cdots a_{n+2} } + \frac{1}{(a_{n+1}+1)(a_{n+1}+2)\cdots a_{n+2}(a_{n+2}+1)\cdots a_{n+3} } +  \ldots \right\}.$$ Since $a_n \geq n$ and $a_{n+1}>a_n,$ we can certainly say that $0 < f - t_n < \frac{1}{n!n}$ by comparison with the reasons given for $\eqref{eq1},$ although this inequality is weak and could be improved. But then if we continue the proof similar to irrationality of $e,$ that is, suppose $f=p/q,\ p,q$ are integers. Then $0 < q!(f-t_q) < \frac{1}{q}$ is true, but there are problems in the next statement: we cannot say $q!t_q = q!\left( \frac{1}{a_1!} + \frac{1}{a_2!} + \ldots + \frac{1}{a_q!} \right) $ is an integer. Alternatively, $a_q!t_q$ is an integer, although I don't see how to use this to make an equation like equation $\eqref{eq1}.$ Have I made a mistake somewhere? If not, is there a way to make some progress here and prove my desired statement? I do feel like I'm missing something relatively obvious.","['irrational-numbers', 'proof-writing', 'factorial', 'sequences-and-series']"
4906551,Each symmetry of a triangle corresponds to exactly one permutation,"I am trying to understand a statement about symmetries of a triangle. The set of lecture notes I am working through assert that (1) every permutation corresponds to a symmetry and (2) every symmetry corresponds to exactly one permutation. As there are only six symmetries, I can write out each of them and directly verify this, but I would like to understand it at more of an abstract level. Once I generalize to an $n$ -gon, I know that it is no longer the case that every permutation corresponds to a symmetry. (For example, there are $4! = 24$ permutations of four vertices but only $8$ symmetries of a square.) For the second assertion, I believe the explanation in Dummit and Foote works, provided that I fully understand it. Say we label the vertices of an $n$ -gon modulo $n$ , so vertex $1$ is adjacent to vertices $2$ and $n-1$ , moving clockwise. Because a symmetry is a rigid motion, it needs to map adjacent vertices to adjacent vertices, so if it sends vertex $1$ to vertex $i$ , it needs to send vertex $2$ to either vertex $i+1$ or $i - 1$ . Once this vertex is chosen, the new position of vertex $n-1$ is determined and the symmetry itself is fully determined. I think this should imply that if $x$ and $y$ are distinct symmetries, then they correspond to distinct permutations of the vertices. But I think my logic is a bit circular here, as I'm inclined to define ""distinct symmetries"" as there existing a vertex sent to distinct places by the two symmetries.","['group-theory', 'symmetry']"
4906575,"A locally compact, connected, Hausdorff and locally connected space is not the countable disjoint union of nonempty closed subsets","In Chapter $5$ , Section $4$ of the book Continuum theory by Sam B. Nadler, Jr., the auther defines $\sigma$ -connectedness (Definition $5.15$ ) to be the property of not being a disjoint union of countably many (including finitely many) and more than one nonempty closed subsets. It is well known (for example, see here ) that a continuum (i.e. compact, connected and Hausdorff space) is $\sigma$ -connected. Continuum theory gives an example (Example $5.14$ ) that shows that compactness cannot be weakened to local compactness. This link gives an essentially same counterexample (see Figure $1$ ). On the other hand, in the link above, the authors say that a locally compact, connected, Hausdorff and locally connected spaces is $\sigma$ -connected by using one point compactification. However, they admit that they could not find a reference for the generalized result (the second footnote). So my question is: Is the generalized result true, which is to say, a locally compact, connected, Hausdorff and locally connected space cannot be written as a countable disjoint union of nonempty closed subsets?","['general-topology', 'connectedness']"
4906576,How can I calculate the angles of a given orthographic camera perspective?,"In Blender, I can set up an orthographic / isometric camera like so: With the X, Y, and Z rotation set to those values, a cube has the following properties: all sides are rendered at equal length, and the lines are at perfect 30deg (or 120deg) angles from each other. In other words, if you zoomed into the corner of the square and measured the angles of all three lines going outward, one would be going straight up, and the other two would be 30deg off horizontal, like so: So, my question is two-fold, I guess, because I want to be able to do this in 2 ways: If I know the rotation of the camera in those X and Z degress in the first picture (assume Y is a constant 0), how can I calculate the resultant angles - those two angles I marked out as 30deg in the second picture? AND how can I calculate the proportional lengths of each side of a rendered square? Basically the reverse of the above - say I just know the angles from the second picture, and I want to find out the camera angle to match - how can I do that? Are there simple forumlas for these questions?","['trigonometry', 'angle', 'geometry']"
4906617,Complex analysis book recommendations with more exercise than Ahlfors' book.,"I am learning complex analysis and currently reading Ahlfors’ Complex Analysis. I feel like I need to do more exercises than the five problems provided in each section. Therefore, I am seeking recommendations for complex analysis books with more exercises than Ahlfors’, but still at the same level and cover all (or more) of the topics covered in Ahlfors' book. Additionally, I am seeking recommendations for only strictly pure mathematics books without any real-world applications. I will continue to read Ahlfors’ book alongside the recommended book because I think Ahlfors is still worth reading.","['complex-analysis', 'book-recommendation', 'problem-solving', 'reference-request']"
4906620,"What does ""measurable with respect to a partition"" mean?","I am reading a paper. It defines the following things. $\Omega$ is a finite set of states and $S$ is the set of outcomes. An outcome function $F$ is a mapping from $\Omega$ to $S$ . $P$ is a distribution over $\Omega$ . $\Pi$ is a partition of $\Omega$ . The authors require $F$ to be ""measurable with respect to $\Pi$ "". I guess they mean ""measurable with respect to the $\sigma$ -algebra generated by $\Pi$ "". How does this $\sigma$ -algebra look like?  (I don't know how to figure out the smallest $\sigma$ -algebra which contains every set in $\Pi$ ) The authors use a feature of $F$ : for all $x$ and $y$ in the same cell of $\Pi$ , $F(x)=F(y)$ . Is this feature implied by the measurability condition?","['measure-theory', 'set-partition', 'probability-theory']"
4906633,Prove stability of solution to a system of differential equations,"How to prove that the zero solution of the following system of differential equations is stable, but not asymptotically stable? \begin{cases}\dot{x} = y - y^2 \\\dot{y} = -x\end{cases} It is easy to see that its phase portraits are given by the equation $$x^2 + y^2 -\frac{2}{3}y^3 = C$$ If we plot this, we can see that for very small positive values of $C$ there are small cycles near the center of the coordinates. Which hints at the fact which we are trying to prove. However, it is not clear how to adequately plot them without a computer program and how to prove stability rigorously.","['stability-theory', 'ordinary-differential-equations']"
4906644,Doubt regarding limits on riemann sums,"Find $$\lim_{n\to\infty}\frac{1^p+3^p+\ldots+(2n+1)^p}{n^{p+1}}$$ I found a solution here which goes like this: By Riemann sums, for any $p>-1$ : $$ \frac{1}{n}\sum_{k=0}^{n}\left(\frac{2k+1}{n}\right)^p \xrightarrow{n\to +\infty}\int_{0}^{1}(2x)^p\,dx = \color{red}{\frac{2^p}{p+1}}.$$ I don't understand how you can change $\frac{2k+1}{n}$ to $2x$ there, like if there was $\frac{2k}{n}$ instead, I know we can easily change it to $2x$ , but there is an extra $+1$ , how can we igonre that term?","['limits', 'riemann-sum']"
4906651,Uncountability of co-infinite subsets of $\mathbb{N}$.,"Let $\mathcal{F}$ be the family of infinite subsets $A$ of $\mathbb{N}$ such that $\mathbb{N}\backslash A$ is also infinite. That is, $$\mathcal{F} = \{ A\subset\mathbb{N}:A\text{ and } \mathbb{N}\backslash A \text{ are infinite} \}.$$ Prove that $\mathcal{F}$ is uncountable. I am trying to apply Cantor's Diagonalization Method, but I couldn't come up with any sequence that would do the trick. Any hint is enough.",['elementary-set-theory']
4906653,Find the rotation necessary to bring a circle in contact with an ellipse,"Question: Given an ellipse $ (r - C)^T Q (r - C) = 1$ where $r = [x,y]^T $ , $C$ is the center, and $Q$ is a $2 \times 2$ symmetric positive definite matrix. And a circle initially given by $ (r - r_0)^T (r - r_0) = a^2 $ I want to rotate the circle about a point $P_0$ such that it becomes in touch with the ellipse. A similar problem handling the more general case of a rotated ellipse touching another ellipse is discussed here . My attempt: The center of the rotated circle , let's call it $X$ , and the tangency point $Y$ on the ellipse, satisfy $ (Y - X)^T (Y - X) = a^2 $ $ (X - P_0)^T (X - P_0) = (r_0 - P_0)^T (r_0 - P_0) $ $ Q (Y - C) = K (Y - X)  \hspace{10pt} or \hspace{10pt}  (Y - X) \times Q (Y - C) = \mathbf{0} $ $ (Y - C)^T Q (Y - C) = 1 $ These equations can be solved numerically.","['conic-sections', 'geometry', 'rotations']"
4906692,The convergence of the Flint Hills series vs the convergence of $\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$,"The Flint Hills series , is the series $$\sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)},$$ and it's an open problem as to whether the series converges. From the proof of Corollary 4 of this paper , it seems that it's also unknown if $$\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$$ converges. Generally speaking (at least when it comes to problems one encounters in a typical calculus/analysis course), given a sequence $(a_n)$ it is easier to check the convergence of $$\lim_{n\to\infty}a_n,$$ than the convergence of $$\sum_{n=1}^\infty a_n.$$ Why then is the problem of determining if the series $$\sum_{n=1}^\infty\frac{1}{n^3\sin^2(n)}$$ converges a more pressing matter than determining if $$\lim_{n\to\infty}\frac{1}{n^3\sin^2(n)}$$ converges?","['convergence-divergence', 'sequences-and-series', 'open-problem', 'real-analysis']"
4906726,"Area of tight-angled $\triangle POB$ given extensions of $OP,BP$ to circle centred at $O$ through $B$?","We have a triangle $(\triangle POB)$ within a semicircle. $OP$ and $BP$ are extended to $OA$ and $BQ$ . $AP = 5$ and $PQ = 7$ . What is the area of the triangle? It's a problem I stumbled upon on chance. I haven't been able to solve it since I am a little weak in geometry. The best approach I have tried is connecting $Q$ to the diameter of semi circle and then approaching the two triangles as similar triangles as they both are right angled triangles. I am still new to the site and can't quite use MathJax to round up the problem. I hope you all will see my mistakes lightly.
Thank You","['euclidean-geometry', 'area', 'circles', 'geometry', 'triangles']"
4906738,What is the probability that the absolute value of the roots of a polynomial of degree $n$ is greater than $x$?,"Note : Posted in MO since it is unanswered in MSE. Let $f(x) = 0$ be an equation of degree $n$ . WLOG we can assume that the its coefficients are in $(-1,1)$ . This is because we can divide each coefficient by the coefficient with the largest magnitude to make each one of them fall in the interval $(-1,1)$ . Assume that the coefficients are uniformly random in $(-1,1)$ . It is well known that most of absolute values of the roots have value close to one i.e. the roots tend to form a unit circle around the origin as shown in the figure below for a polynomial of degree $n = 666$ . It can be observed from the graph that while most of the roots are close to the unit circle, some of them are outside it. I want to find the probability $P(n,x)$ that a roots of a polynomial of degree $n$ lies at a distance $x > 1$ from the origin. To do this, I run a simulation generating polynomial of degree $n$ and its $n$ roots in each trial and counted the total number of roots across all the trials whose absolute value is $\ge x$ . Question : Experimental data shows that $P(n,1) = \frac{1}{2}$ and for $x > 1$ , $\displaystyle n P(n,x) \to  \frac{1}{2x}$ as $n \to \infty$ . Can this be proved or disproved? Related : Is the root of a polynomial with the largest modulus more likely to be real?","['roots', 'real-analysis', 'polynomials', 'algebra-precalculus', 'probability']"
4906775,Integral of the product of a Gaussian and a exponential of a hyperbolic function,"In a derivation I am working on, I have encountered an integral of the form \begin{equation}
\int_{0}^{\infty}e^{-a x^2-b\ \textrm{cosh}(x)}\ dx
\end{equation} with $a$ and $b$ real and positive. I am not sure if there is a closed form. Any ideas on how to attack it? Edit: Building upon the approach proposed by @Sam, we can express \begin{equation}
\int_{0}^{\infty}e^{-a x^2-b\ \textrm{cosh}(x)}\ dx = \frac{1}{2\sqrt{a}}\sum_{n=0}^{\infty}\frac{a_{2n}}{a}\frac{\Gamma\left(n+\frac{1}{2}\right)}{(2n)!}
\end{equation} with \begin{equation}
a_{2n}=\left\{\begin{array}{l}
e^{-b}\qquad &n=1\\
-b\ e^{-b}\sum_{k=0}^{n-1} T_{n,k+1}(-1)^kb^k \qquad & n\geq 1
\end{array}\right.
\end{equation} and $T_{n,k}$ the triangle defined by the OEIS sequence A156289 . Making use of the closed expression for $T_{n,k}$ , we have \begin{equation}
a_{2n}=-b\ e^{-b}\sum_{k=0}^{n-1}\sum_{j=0}^k\binom{2k+2}{k-j}\frac{(-1)^j(j+1)^{2n}}{(k+1)!}\left(\frac{b}{2}\right)^k \qquad \qquad n\geq 1
\end{equation} Putting all the pieces together, \begin{equation}
\int_{0}^{\infty}e^{-a x^2-b\ \textrm{cosh}(x)}\ dx = \frac{e^{-b}}{2}\sqrt{\frac{\pi}{a}}\left[1+\sum_{n=1}^{\infty}\sum_{k=0}^{n-1}\sum_{j=0}^{k}\binom{2k+2}{k-j}\frac{(-1)^{j+1}(j+1)^{2n}}{(k+1)!\ n!\ 2^{k+2n}}\frac{b^{k+1}}{a^{n}}\right]
\end{equation} Making use of the modified Bessel function of the first kind, it should be possible to perform the sums in $k$ and $j$ and get the more compact and elegantly derived expression by @Leucippus, \begin{equation}
\int_{0}^{\infty}e^{-a x^2-b\ \textrm{cosh}(x)}\ dx = \frac{1}{2} \, \sqrt{\frac{\pi}{a}} \, \left( I_{0}(b) + 2 \, \sum_{n=1}^{\infty} (-1)^n \, I_{n}(b) \, e^{n^2/(4 a)} \right)
\end{equation} This expansion is valid except far from $a^{-1}=0$ . In particular, it breaks for $a=0$ , where the integral is still convergent \begin{equation}
\int_{0}^{\infty}e^{-b\ \textrm{cosh}(x)}\ dx = K_0(b)
\end{equation} Any thoughts on how to express the integral as an expansion around $a=0$ ?","['integration', 'definite-integrals', 'hyperbolic-functions', 'gaussian-integral', 'bessel-functions']"
4906787,The angle bisectors of triangle ABC intersect at point I and CA + AI = BC. Find $\frac{\angle BAC}{\angle CBA}$.,"Problem: the angle bisectors of triangle $ABC$ intersect at point $I$ . It is known that $CA + AI = BC$ . Find the ratio of angles $BAC$ and $CBA$ . What I tried: if we reflect the point $B$ relative to line $IC$ and get point $B'$ then $AI = AB'$ and the triangle $B'BC$ is equilateral. But it didn't help. What I know: It's a problem in theme ""additional construction"" Probably it's from some school olympiad, Slovenia, 2005","['contest-math', 'geometry']"
4906820,Question regarding a value in a one-period model,"There is a script at my university (can't post it for copyright reasons) for a course on discrete time financial mathematics. I decided to give it a try and found this problem: Consider a financial market model with $\Omega=\left\{\omega_1, \ldots,  \omega_N\right\},\mathcal{F}_0=\{\emptyset, \Omega\},$ $\mathcal{F}_1:=2^{\Omega}$ , $T=1$ and $d$ arbitrary. Suppose that in
the market model every European option is hedgeable. Show the
existence of $c=c(d)$ , i.e. $c$ only depending on $d$ , such that $N \leq c$ . First question I have is: what is $c$ supposed to be here? It is not a used anywhere else in the text and I have no idea what value I'm looking for. Then, I still tried to work that out: the definition of headgeability given in the script is that an European option is hedgeable iff one can find a starting value $V_0$ and a trading strategy $H$ s.t. $$ f = V_0 + (H \cdot S)_T $$ where $(H \cdot S)_T$ denotes a martingale transform. But i don't know how to set up the system(s) of equations given that there is countably many $\omega$ s and arbitrary many $d$ s. Any help on how to proceed?","['finance', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4906854,Filters for Stochastic Process,"Let $\Omega$ be a set and $\{F_t\}_{t \geqslant 0}$ be a filtration. That is, it is a family of $\sigma$ -algebras on $\Omega$ such that $F_{s} \subseteq F_t$ whenever $s \leqslant t$ . Fix $t > 0$ . Define : \begin{equation*}
F_{t+} = \bigcap_{\epsilon > 0} F_{t + \epsilon}
\end{equation*} \begin{equation*}
 F_{t-} = \sigma (\bigcup_{s < t} F_{s})
\end{equation*} And: \begin{equation*}
F_{t++} = \bigcap_{\epsilon > 0} F_{(t + \epsilon)+}= \bigcap_{\epsilon > 0} \bigcap_{\delta > 0} F_{t + \epsilon + \delta} 
= \bigcap_{\epsilon > 0} F_{t + \epsilon} = F_{t+}
\end{equation*} \begin{equation*}
F_{t+-} = \bigcap_{\epsilon > 0} F_{(t + \epsilon)-} 
= \bigcap_{\epsilon > 0} \sigma (\bigcup_{s < t + \epsilon} F_{s})
\end{equation*} \begin{equation*}
F_{t-+} = \sigma (\bigcup_{s < t} F_{s+}) = \sigma (\bigcup_{s < t} \bigcap_{\epsilon > 0} F_{s+\epsilon})
\end{equation*} \begin{equation*}
F_{t--} = \sigma (\bigcup_{s < t} F_{s-}) = \sigma (\bigcup_{s < t} \sigma(\bigcup_{u < s} F_{u}))
\end{equation*} The $F_{t++}$ simplifies really well. The other three not so much. Is there any simpler form one can obtain for the other three? In particular, do we get examples where (at least one of the following proper inclusion) : \begin{equation*}
F_{t--} \subset F_{t-} \subset F_{t-+} \subset F_t \subset F_{t+-} \subset F_{t+}
\end{equation*}","['measure-theory', 'stochastic-calculus', 'stochastic-processes', 'probability-theory', 'set-theory']"
4906875,Combination Theory solve for case where $x_i \leq 1$,"I am studying computer science and take a discrete math modules, right now trying to learn combinational theory. I know how to solve when cases are $\geq 1$ and x's are all positive but not sure how to solve for cases such as these. How many solutions $(x_1, x_2, x_3, x_4, x_5)$ are there to the equation $x_1 - x_2 - x_3 + x_4 + x_5 = 3 $ where each $x_i$ is an integer and \begin{align*}
x_1 &\leq 1, \\
x_2 &\geq 2, \\
x_3 &\geq 1, \\
x_i &\leq -1 + 4i \text{ for } i \in \{4,5\}?
\end{align*} I try $-x_1 \geq -1$ $x_2 \geq 2$ $x_3 \geq 1$ $-x_4 \geq -15$ $-x_5 \geq -19$ Thus $y_1 = x_1 + 2$ $y_2 = -x_2 + 1$ $y_3 = -x_3$ $y_4 = x_4 + 16$ $y_5 = x_5 + 20$ Getting $y_1 + y_2 + y_3 + y_4 + y_5 = x_1 - x_2 - x_3 + x_4 + x_5 + 2 + 1 + 16 + 20$ = 42 Thus getting $\binom{41}{4}$ However the solution is $\binom{40}{4}$ and I don't know what's the mistake I made to not get the correct answer.","['combinations', 'discrete-mathematics']"
4906885,Why is $x_1 x_2 + x_1 x_3 + x_2 x_3$ constant for an equilateral triangle?,"Consider an equilateral triangle centered at the origin of the 2D Cartesian space. Let the coordinates of its vertices be $v_1=(x_1,y_1)$ , $v_2=(x_2,y_2)$ and $v_3=(x_3,y_3)$ . All such triangles can be defined by the following conditions: \begin{gather}
||v_1 - v_2|| = ||v_1 - v_3|| = ||v_2 - v_3|| & \label{eq_dist}\tag{1} \\
v_1 + v_2 + v_3 = (0, 0) & \label{sum_0}\tag{2} \\
\end{gather} Using these conditions it can be shown that $||v_1|| = ||v_2|| = ||v_3||$ (see proof here ). Let $$r = ||v_1|| = ||v_2|| = ||v_3|| \label{eq_len}\tag{3}$$ My question is whether there is a simple algebraic explanation for the fact that the expression $$d_x \overset{\text{def}}{=} x_1 x_2 + x_1 x_3 + x_2 x_3$$ does not depend on the angle of rotation of the triangle about the origin. I made an interactive Desmos link where you can observe this behavior. Note that, in that link, I used $s$ as the radius instead of $r$ , since $r$ is a special variable in Desmos. I will proceed by listing and then detailing some of my findings: $v_1 \cdot v_2 + v_1 \cdot v_3 + v_2 \cdot v_3$ is also rotation-independent I found a trigonometric proof for my question This behavior also happens with a tetrahedron in $\mathbb{R}^3$ (see this Desmos link ) 1) $v_1 \cdot v_2 + v_1 \cdot v_3 + v_2 \cdot v_3$ is also rotation-independent This expression, although different than $d_x$ , is closely related to it, since each individual dot product can be expressed as: $$v_k \cdot v_j = x_k x_j + y_k y_j$$ Which implies that $$\color{blue}{v_1 \cdot v_2} + \color{red}{v_1 \cdot v_3} + \color{green}{v_2 \cdot v_3} = (\color{blue}{x_1 x_2} + \color{red}{x_1 x_3} + \color{green}{x_2 x_3}) + (\color{blue}{y_1 y_2} + \color{red}{y_1 y_3} + \color{green}{y_2 y_3})$$ It is therefore handy to define \begin{align*}
d_y & \overset{\text{def}}{=} y_1 y_2 + y_1 y_3 + y_2 y_3 \\
d & \overset{\text{def}}{=} v_1 \cdot v_2 + v_1 \cdot v_3 + v_2 \cdot v_3
\end{align*} Which allows us to write $$d = d_x + d_y$$ This means that if $d$ and $d_y$ are rotation-independent, so is $d_x$ . We can show that $d$ is rotation-independent by noticing that each of the three dot products is also rotation-independent. This happens because the dot product depends only on each vector's norm, and the angle between them - and none of those change when rotating the whole triangle about the origin. Nevertheless, I present a simple algebraic proof for this fact: \begin{align*}
||v_1||^2 & = r^2 & \text{by (\ref{eq_len})} \\
||- v_2 - v_3||^2 & = r^2 & \text{by (\ref{sum_0})} \\
(- x_2 - x_3)^2 + (- y_2 - y_3)^2 & = r^2 & \text{expand} \\
\color{blue}{x_2^2} + 2 x_2 x_3 + \color{red}{x_3^2} + \color{blue}{y_2^2} + 2 y_2 y_3 + \color{red}{y_3^2} & = r^2 & \text{expand} \\
\color{blue}{r^2} + \color{red}{r^2} + 2 x_2 x_3 + 2 y_2 y_3 & = r^2 & \text{by (\ref{eq_len})} \\
x_2 x_3 + y_2 y_3 & = - \frac{r^2}{2} & \text{simplify} \\
v_2 \cdot v_3 & = - \frac{r^2}{2} & \text{by definition of $\cdot$} \\
\end{align*} By symmetry, this proof also applies to $v_1 \cdot v_2$ and $v_1 \cdot v_3$ , and thus \begin{gather}
v_1 \cdot v_2 = v_1 \cdot v_3 = v_2 \cdot v_3 = - \frac{r^2}{2} \\
d = v_1 \cdot v_2 + v_1 \cdot v_3 + v_2 \cdot v_3 = - \frac{3}{2} r^2
\end{gather} Although this is an interesting result, it is not enough, since we would also need to show that $d_y$ is rotation-independent. It could be the case that $d_x$ and $d_y$ both depended on the triangle's rotation but their sum did not - just like the cosine and sine functions depend on their argument but the sum of their squares does not. But if $d_x$ and $d_y$ were indeed rotation-independent (which I know is true, but am trying to find a simple algebraic proof for), since conditions (\ref{eq_dist}) and (\ref{sum_0}) are symmetrical in $x$ and $y$ , we would have $$d_x = d_y = \frac{d}{2} = - \frac{3}{4} r^2 \label{dx_dy}\tag{4}$$ However, I was unable to prove this fact algebraically. 2) I found a trigonometric proof for my question This proof is based in two observations: $v_1$ , $v_2$ and $v_3$ lie in a circle centered at $(0, 0)$ (by \ref{sum_0}) and with radius $r$ (by \ref{eq_len}) The angle between each pair of vertices (seen as vectors) is $\frac{1}{3}$ of a full turn, since the triangle is equilateral (by \ref{eq_dist}) These allow us to write \begin{align*}
v_1 = (x_1, y_1) & = (r \cos{\theta}, r \sin{\theta}) & \label{trig1}\tag{5.1} \\
v_2 = (x_2, y_2) & = (r \cos{(\theta + \frac{2 \pi}{3})}, r \sin{(\theta + \frac{2 \pi} {3})}) & \label{trig2}\tag{5.2} \\
v_3 = (x_3, y_3) & = (r \cos{(\theta - \frac{2 \pi}{3})}, r \sin{(\theta - \frac{2 \pi}{3})}) & \label{trig3}\tag{5.3}
\end{align*} where $\theta$ is an arbitrary real number. To simplify a computation below, we will now compute \begin{align}
\cos(\alpha + \beta) \cos(\alpha - \beta) & = (\cos \alpha \cos \beta - \sin \alpha \sin \beta) (\cos \alpha \cos \beta + \sin \alpha \sin \beta) \\
& = \cos^2 \alpha \cos^2 \beta - \sin^2 \alpha \sin^2 \beta & \label{cos_prod}\tag{6}
\end{align} So, we have \begin{align*}
d_x & = x_1 x_2 + x_1 x_3 + x_2 x_3 & \text{by definition} \\
& = x_1(x_2 + x_3) + x_2 x_3 \\
& = x_1(- x_1) + x_2 x_3 & \text{by (\ref{sum_0})} \\
& = x_2 x_3 - x_1^2 \\
& = r^2 (\cos(\theta + \frac{2 \pi}{3}) \cos(\theta - \frac{2 \pi}{3}) - \cos^2 \theta) & \text{by (\ref{trig1}), (\ref{trig2}) and (\ref{trig3})} \\
& = r^2 (\cos^2 \theta \cos^2 \frac{2 \pi}{3} - \sin^2 \theta \sin^2 \frac{2 \pi}{3} - \cos^2 \theta) & \text{by (\ref{cos_prod})} \\
& = r^2 ((\cos^2 \theta) (- \frac{1}{2})^2 - (\sin^2 \theta) (\frac{\sqrt{3}}{2})^2 - \cos^2 \theta) \\
& = r^2 (\frac{1}{4} \cos^2 \theta - \frac{3}{4} \sin^2 \theta - \cos^2 \theta) \\
& = r^2 (- \frac{3}{4} (\cos^2 \theta + \sin^2 \theta)) \\
& = - \frac{3}{4} r^2
\end{align*} This allows us to conclude that the equality (\ref{dx_dy}) is indeed true. However, we had to resort to trigonometry, whilst I was looking for an algebraic proof. Moreover, this trigonometric proof does not give me a good understanding of how conditions (\ref{eq_dist}) and (\ref{sum_0}) imply that $d_x$ only depends on $r$ and not on $\theta$ . It seems that it could have been just a matter of luck that $x_1^2 = r^2 \cos^2 \theta$ and $x_2 x_3 = r^2 (\cos^2 \theta - \frac{3}{4})$ . But looking at higher dimensions, the pattern seems to repeat, hinting that there must be something more intricate going on here. 3) This behavior also happens with a tetrahedron in $\mathbb{R}^3$ By considering a tetrahedron centered at the origin of the 3D Cartesian space, defining conditions analagous to (\ref{eq_dist}) and (\ref{sum_0}), and performing a trigonometric change of variables akin to (\ref{trig1}), (\ref{trig2}) and (\ref{trig3}), I was able to show that an expression analogous to $d_x$ (i.e., $\sum_{k < j}{x_k x_j}$ ) does not depend on the rotation of the tetrahedron about the origin. You can observe this behavior by playing around with this Desmos link . That is it. Any insights on this problem would be appreciated, even if they don't directly answer my question. Thank you in advance.","['euclidean-geometry', 'algebra-precalculus', 'vectors', 'linear-transformations']"
4906935,Asymptotic properties of integral of power of sine function?,"I'm trying to investigate the asymptotic property of the following integral: $$
    \int_0^{\theta} \sin^nx dx, \quad n\to +\infty
$$ where $\theta \in (0,\pi/2)$ is a constant. For the case $\theta = \pi/2$ , it is the Wallis integral and we have $$
    \int_0^{\pi/2} \sin^nx dx \sim \sqrt{\frac{\pi}{2n}}.
$$ But for the case when $\theta \in (0,\pi/2)$ , I have no idea how to analyse it.","['integration', 'definite-integrals', 'analysis', 'asymptotics', 'calculus']"
4906948,"Does the statement $\forall x \in \mathbb{R}, \det(x) = 0$ have a truth value?","I couldn't think of anything specific for the question title so I just used an illustrative example. Context I often encounter the scenario where I have a set $X$ of objects for which a property $P$ is defined. But then I often have to prove (as some trivial case) that for some $Y \subset X$ , $P(y)$ is true for all $y \in Y$ . But it turns out that Y is empty and so I thought the result holds vacuously. But then I started thinking about properties and vacuous truths and had some questions. I haven't studied formal mathematical logic and would appreciate an answer for the 'working' mathematician. Questions From my understanding, any property $P$ has to be defined for a particular set of objects and evaluates to true or false for the objects for which it is defined. For example, let $P(x)$ be the property that $\det(x) = 0$ . Then $P(x)$ would be undefined if $x$ is a real number since the determinant of a real number is undefined. Hence, a statement like $\forall x \in \mathbb{R}, P(x)$ does not have a truth value. Is this correct? But then are statements like $\forall x \in \emptyset, P(x)$ (where $P$ is any property) true (vacuously)? Are all properties automatically defined for the (nonexistent) elements of the empty set? I believe I just have some severe misunderstandings and would appreciate it if anyone could help me resolve them.","['elementary-set-theory', 'logic']"
4906970,Finding a more efficient solution to a trigonometric identity problem.,"THIS IS NOT A HOMEWORK QUESTION This question comes from a university entrance exam from around 1910. No solutions were ever published. My own solution is given here, but I get the feeling that a more efficient solution would be possible. Question: Find the exact value of $\cos{\frac{2\pi}{15}}+\cos{\frac{4\pi}{15}}+\cos{\frac{8\pi}{15}}+\cos{\frac{14\pi}{15}}$ My solution: Re-write the functions as $\cos{\frac{5\pi-3\pi}{15}}+\cos{\frac{9\pi-5\pi}{15}}+\cos{\frac{5\pi+3\pi}{15}}+\cos{\frac{9\pi+5\pi}{15}}$ Which simplifies to $\frac{1}{2}\cos{\frac{\pi}{5}}+\frac{\sqrt{3}}{2}\sin{\frac{\pi}{5}}+\frac{1}{2}\cos{\frac{3\pi}{5}}+\frac{\sqrt{3}}{2}\sin{\frac{3\pi}{5}}+\frac{1}{2}\cos{\frac{\pi}{5}}-\frac{\sqrt{3}}{2}\sin{\frac{\pi}{5}}++\frac{1}{2}\cos{\frac{3\pi}{5}}-\frac{\sqrt{3}}{2}\sin{\frac{3\pi}{5}}$ $=\cos{\frac{\pi}{5}}+\cos{\frac{3\pi}{5}}$ $=2\cos{\frac{\pi}{5}}\cos{\frac{2\pi}{5}}$ Now, because I know that $\cos{\frac{\pi}{5}}=\frac{1+\sqrt{5}}{4}$ and $\cos{\frac{2\pi}{5}}=\frac{-1+\sqrt{5}}{4}$ I can simplify this to $\frac{1}{2}$ . However, I wonder: 1 - would students in 1910 have known such exact values? 2 - is there a more efficient solution? Sum-to-product and product-to-sum identities seem to be commonplace in exams at the time, so I am wondering if there is a clever application of these that I have missed. Thoughts appreciated as always.",['trigonometry']
4906979,Can vacuously true statements be proved using proof by contradiction?,"I always thought it was valid to prove vacuously true statements using proof by contradiction but now am not so sure. For example, consider the vacuously true statement $\forall x \in \emptyset, x + 1 = 0$ . To prove it by contradiction, I assume for the sake of contradiction that $\exists x \in \emptyset$ s.t. $x + 1 \neq 0$ . But this contradicts $x \notin \emptyset$ for all objects $x$ . But the property $x + 1 \neq 0$ does not make sense for any arbitrary object $x$ in $\emptyset$ , does it ? (see 1 for my confusion regarding properties only being defined for certain sets of objects) So I suspect my thinking of proving a vacuously true statement using proof by contradiction as in the example above is flawed? I would greatly appreciate any clarification.","['propositional-calculus', 'proof-writing', 'logic', 'intuition', 'elementary-set-theory']"
4907009,Halmos’s word definition of union,"In Naive Set Theory By Halmos he states the axiom of unions as: For every collection of sets there exists a set that contains all the elements that belong to at least one set of the given collection. Isn’t it better to say replace at least one with any ? I actually think *at least one * is wrong because if my collection had sets {a, b, c} but my union contained all the elements of a then I’ve satisfied the definition but that’s not our typical understanding of the union. How do I reconcile this?",['elementary-set-theory']
4907020,Problem 5C.3 Isaacs' Finite Group Theory,"I have a question about the following problem [Finite Group Theory, Martin Isaacs, Chapter 5]: Let $G$ be simple and have an abelian Sylow 2-subgroup $P$ of order $2^{5}$ . Deduce that $P$ is elementary abelian. I report my reasoning: From a theorem on the decomposition of finite abelian groups, I know that each finite abelian $p$ -group decomposes into the product of a finite number of finite cyclic $p$ -groups. If I apply this theorem to $P$ , where $p=2$ , I should get the thesis. However, I believe that this reasoning is wrong because I don't take advantage of the fact that G is simple. Where do I go wrong?","['transfer-theory', 'finite-groups', 'simple-groups', 'sylow-theory', 'group-theory']"
4907025,Smooth Urysohn lemma,"From topology I know the Urysohn lemma.
My question is if there is some sort of Urysohn lemma for smooth manifolds. To be more precise: Let M be a smooth manifold and suppose $A$ and $B$ are two disjoint closed subsets of $M$ .
Then there is a smooth function $f$ on $M$ so that $0 \leq f(x) \leq 1$ for all $x \in M$ and $f_{|A}=0$ and $f_{|B}=1$ . I am interested in the proof of this statement (or books where this is proven).","['manifolds', 'general-topology', 'smooth-manifolds', 'differential-geometry']"
4907036,What is the minimal number of pieces to surround n pieces in a Go game?,"Go is a game of black and white pieces on a lattice of $19\times 19$ . Pieces have liberty by having empty spaces next to them and are killed if the liberty are occupied by the opponent. Pieces are connected if they are next to each other (vertical/horizontal connection only, not diagonal connection) and share their liberty. For the problem, we assume it's played on an infinite lattice $\mathbb{Z}^2$ . The problem is, for $n$ pieces, what is the least liberty they can have? for example, for two pieces, we can make it like XX and there will be $6$ liberty, make it XOX then $7$ , and XOOX then $8$ . Then the answer is $6$ . Here is the construction for small numbers . we have a conjecture on OEIS saying that the answer is $2+\lceil\sqrt{8n-4}\rceil$ . But It doesn't have the proof, or at least I didn't see it. I have some trivial thinking about the problem: This minimum value must exist and can be reached. Since they are all integers, this is obvious. The pieces must be connected. For situations where they are not connected, connecting them can reduce the amount of liberty. (Edit: for $n=2$ , this is incorrect, because the direct or diagonal connection both give $6$ , meaning the proof is invalid. Probably the statement is ""it has to be connected or 'connected' diagonally, and we can always find a connected construction if a non-connected solution exists."" And for greater $n$ , this statement should be right. But I can't yet prove it.) For a shape of $n$ pieces, you can always take away a piece from the edge and achieve the same number of liberty or less. This is not obvious enough because you have to choose ""convex"" edges, otherwise he will actually have more liberty. We need to prove that there is always a point that can be taken away so that the number of liberty does not increase. Our idea of ​​proof is: if all the pieces are ""concave"", the number of pieces will explode to infinity. First, when taking away a piece, we check whether there are pieces in all four directions. For example, the piece's coordinates are $(a, b)$ , then we check $x<a, x>a, y<a, y>a$ whether there are pieces in the four regions. Once there are no pieces in one of the regions, then this is the piece we want to take away, since at this time, there are only three positions around the piece that can affect the changes of liberty upon removal. Just enumerate all possibilities. And if there are always pieces in the four directions, we will find that the distribution of pieces is unbounded, which contradicts with the finite number of chess pieces. This sequence is constant or increasing. This is a corollary of 3. The chess shape that reaches the minimum value should have no eyes (meaning holes inside the shape), and even if it does have eyes, there will be a solution without eyes. For the case where there are eyes, just remove a piece from the edge and fill in the eyes (this is also a corollary of 3). Can anyone figure out or find the full proof?","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'chessboard', 'game-theory']"
4907119,"Improper Integral $\int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t$","The integral I'm trying to solve is \begin{align*}
    I = \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t.
\end{align*} Using Frullani's integral, note that \begin{align*}
    \log(t) = - \, \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x.
\end{align*} Hence, \begin{align*}
    I &= \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t\\
    &= \int_{0}^{\infty} \left[ \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x \right] \left[ \int_{0}^{\infty} \frac{e^{-yt}-e^{-y}}{y} \, d y \right] t^{\frac{1}{2}} e^{-t} \, d t\\
    &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t} \left(e^{-xt}-e^{-x}\right) \left(e^{-yt}-e^{-y}\right) \, d t \right] \, d x \, d y\\
    &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x+y)} \, d t - \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x)-y} \, d t \right.\\
    &\qquad \left. - \, \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+y)-x} \, d t + \int_{0}^{\infty} t^{\frac{1}{2}} e^{-x-y-t} \, d t \right] \, d x \, d y\\
    &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right] \, d x \, d y.
\end{align*} Integrating by parts with $\ d v=\frac{1}{x} \, d x$ , we have \begin{align*}
    I &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \left[ \log(x) \left( \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right) \right]_{x=0}^{x=\infty} \right.\\
    &\qquad \left. - \, \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \right\} \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \frac{3e^{-y}}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x)^{5/2}} - \frac{1}{(1+y)^{3/2}} \left[ - \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right.\\
    &\qquad \left. - \, \frac{3}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x+y)^{5/2}} \, d x + e^{-y} \left[ - \, \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right\} \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} - \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right\} \, d y.
\end{align*} Integrating by parts once again with $\ d v=\frac{1}{y} \, d y$ , we have \begin{align}
    \nonumber I &= - \, \frac{\sqrt{\pi}}{2} \left\{ \left[ \log(y) \left( 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} \right. \right. \right.\\
    \nonumber &\qquad \left. \left. \left. - \, \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right) \right]_{0}^{\infty} - \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} \right. \right.\\
    \nonumber &\qquad \left. \left. - \, 2e^{-y} \log(y) [\log(2)-1] - \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \right\}\\
    &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} - 2e^{-y} \log(y) [\log(2)-1] \right. \\
    \nonumber &\qquad \left. - \, \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \\
    \nonumber &= \ldots\\
    \nonumber &= \frac{\sqrt{\pi}}{2} \left[4\log^2(2) + (4\gamma-8)\log(2) + \frac{\pi^2}{2} + \gamma^2 - 4\gamma\right].
\end{align} I could almost solve the integral in the third-to-last line. However, in the said integral, I haven't found the answer to \begin{align*}
    \int_{0}^{\infty} \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} \, d y.
\end{align*} Firstly, I have no idea how to solve this particular integral. Secondly, I'm also not sure if my whole calculation is already correct. Can anyone help me with this problem? I am open to any suggestions/solutions for solving the very last integral, or even for the very first integral in the question. Any help/feedback is much appreciated. Thank you!","['integration', 'calculus', 'improper-integrals']"
4907126,Contest Problem from Berkeley math tournament - 2023.,"Contest Problem from Berkeley math tournament - 2023 : Maria and Skyler have a square shaped cookie with a side length of 1 inch. They split the cookie by choosing two points on distinct sides of the cookie uniformly at random and cutting across the line segment formed by connecting the two points. If Maria always gets the larger piece, what is the expected amount of extra cookie in Maria's piece compared to Skyler's in square inches. Does this problem indicate that the choosing of sides and cutting across in the same cookie every time or a different square shaped cookie.","['contest-math', 'expected-value', 'solution-verification', 'probability']"
4907134,Simple question on the meaning of $y$ in $Y = y$,"We have a random variable $Y$ , and say that $Y = y$ .  Is $y$ any single value? is it one particular value? Can we represent little $y$ with an axis? I don't understand how $y$ is fixed. it can be any value. $Y =y$ can't represent a specific value in the random variable $Y$ because $y$ can be anything; $1, 2, 3, 4, 5, 6... $ so how is it representing a particular value of $Y$ ? what's the difference between $Y$ and $y$ ?","['probability-theory', 'probability', 'random-variables']"
4907201,Showing that a family of functions has size $\mathfrak{c}$,"Let A be a matrix such that $A^i_k\subseteq[0,1]$ are sets with: $A^i_k\cap A^i_{k'}=\emptyset\Leftarrow k\neq k'$ , $\bigcup_{k\in\omega} A^i_k=[0,1]$ and for every sequnce $(k_i)_{i\in\omega}$ the set $\bigcap_{i\in\omega}(\bigcup_{k\leq{k_i}}A^i_k)$ is at most countable. Question: Show that there are $\mathfrak{c}$ many $f\in \hspace{1mm}^\omega\omega$ s.t. $\bigcap_{i\in\omega} A^i_{f(i)}\neq \emptyset $ . I think my approaches were flawed in many ways. I still wanted to post the most promising one: I wanted to start building a binary tree where the vertices correspond to sets in the rows of the matrix such that, moving down any branch, the intersection of the sets is not empty. Then I can encode $\mathfrak{c}$ many functions using binary respresentaion ( $b(x)(i)$ ) such that a $0$ corresponds to taking the left path ( $k^i_l$ ) and a $1$ to taking the right path ( $k^i_d$ ): $$f_x(i):=\begin{cases}k^i_d, & b(x)(i)=1 \\ k^i_l, & b(x)(i)=0  \end{cases}$$ As for the tree: Step 1: I start with an uncountable $A^0_k$ . This must exist since I can't cover $[0,1]$ with countably many countable sets. I divide $A^0_k$ into two sets of size $|A^0_k|$ : $V^0_1$ and $V^0_2$ . Step n+1: I find a set $A^{n+1}_{k_1}$ that has uncountable intersection with $V^n_1$ . This must exist by the same argument as above. I split it into two sets of size $|A^{n+1}_{k_1}|$ : $V^{n+1}_1$ and $V^{n+1}_2$ . For the other $V^n_i$ respectively. There are a few problems with this: The sets in a row are only disjoint ""up to being identical"" and I need $\omega$ many different branches at each vertex since otherwise I can't encode all real numbers [SOLVED] Doesn't this imply the set $\bigcap_{i\in\omega}A^i_{k_i}$ (or the intersection over any branch) is uncountable.  But the thrid condition would imply that such intersections have to be countable. [SOLVED] (new) The intersection might be empty along too many branches. I think there might be a much easier approach that I am too stupid to see, since all the other exercises from that chapter were way easier.","['elementary-set-theory', 'set-theory']"
4907237,Every entire function $g$ such that $g' = g^2$,"I ran across this neat exercise that I had to think about a little: Find every holomorphic function $g:\mathbb{C} \rightarrow \mathbb{C}$ such that $g'=g^2$ . Can you check my proposed solution and maybe enlighten me, if there is a more elegant solution? I hypothesize that only $g=0$ fulfils the requirements. Clearly, $g'=0=g^2$ in that case. Now assume that $g$ fulfils the above requirements and there is $z_0 \in \mathbb{C}$ such that $g(z_0) \neq 0$ . Then, by continuity, there is a neighborhood $\varepsilon > 0$ , such that $g\neq 0$ on $B_\varepsilon(z_0)$ . Then,  for $z \in B_\varepsilon(z_0)$ : $$
\left( \tfrac{1}{g(z)}\right)' = -\tfrac{g'(z)}{g(z)^2} = -1
$$ Hence, $g(z) = \tfrac{1}{\frac{1}{g(z_0)}+z_0-z}$ for all $z \in B_\varepsilon(z_0)$ , and by indentity theorem, $g(z) = \tfrac{1}{\frac{1}{g(z_0)}+z_0-z}$ for all $z \in \mathbb{C} \setminus \left \lbrace z_0 + \tfrac{1}{g(z_0)}\right \rbrace$ . But this can not be extended to a holomorphic function on $\mathbb{C}$ .","['complex-analysis', 'solution-verification']"
4907258,The pushout $F_\alpha(H)$ of $H \overset{\pi}{\longleftarrow} \mathbb{Z}^{\ast H} \overset{\alpha^{\ast H}}{\longrightarrow} G^{\ast H}$,"$\require{AMScd}$ Definition 1: If $G$ is a group and $X$ is a set, define $G^{\ast X} = \ast_{x \in X} G$ . This is functorial in both $G$ and $X$ : If $G$ and $H$ are groups, $\varphi: G \to H$ is a group homomorphism and $X$ is a set, we have a group homomorphism $\varphi^{\ast X}: G^{\ast X} \to H^{\ast X}$ that acts via $\varphi$ on each free factor. If $G$ is a group, $X$ and $Y$ are sets and $f: X \to Y$ is a set mapping, we have a group homomorphism $G^{\ast f}: G^{\ast X} \to G^{\ast X}$ that sends the $x$ -th free factor to the $f(x)$ -th free factor. If $G$ and $H$ are groups, $\varphi: G \to H$ is a group homomorphism, $X$ and $Y$ are sets and $f: X \to Y$ is a set mapping, we have the following commutative diagram: $$
\begin{CD}
G^{\ast X} @>G^{\ast f}>> G^{\ast Y}\\
@V \varphi^{\ast X} V V @VV \varphi^{\ast Y} V\\
H^{\ast X} @>>H^{\ast f}> H^{\ast Y}
\end{CD}
$$ Definition 2: If $G$ is a group, define $\pi: \mathbb{Z}^{\ast G} \to G$ to be the group homomorphism that sends the $g$ -th free generator to $g$ . If $G$ and $H$ are groups and $\varphi: G \to H$ is a group homomorphism, we have the following commutative diagram: $$
\begin{CD}
\mathbb{Z}^{\ast G} @>\mathbb{Z}^{\ast \varphi}>> \mathbb{Z}^{\ast H}\\
@V \pi_G V V @VV \pi_H V\\
G @>>\varphi> H
\end{CD}
$$ Definition 3: Let $G$ be a group and $\alpha: \mathbb{Z} \to G$ be a group homomorphism. Define a functor $F_\alpha: \mathsf{Grp} \to \mathsf{Grp}$ in the following way: Given a group $H$ , define $F_\alpha(H)$ to be the pushout of the following diagram: $$
\begin{CD}
\mathbb{Z}^{\ast H} @>\alpha^{\ast H}>> G^{\ast H}\\
@V \pi V V\\
H
\end{CD}
$$ This means that we obtain the following commutative diagram: $$
\begin{CD}
\mathbb{Z}^{\ast H} @>\alpha^{\ast H}>> G^{\ast H}\\
@V \pi V V @VV \beta V\\
H @>>\gamma> F_\alpha(H)
\end{CD}
$$ Given two groups $H_1$ and $H_2$ and a group homomorphism $\varphi: H_1 \to H_2$ , consider the cube diagram that consists of the diagrams for $F_\alpha(H_1)$ and $F_\alpha(H_2)$ as well as the maps $\varphi$ , $\mathbb{Z}^{\ast \varphi}$ and $G^{\ast \varphi}$ : In this diagram, the front and back squares are commutative as $F_\alpha(H_1)$ and $F_\alpha(H_2)$ are by definition the pushouts of the upper left parts. The top square is commutative by the previous observation from definition 1. The left square is commutative by the previous observation from definition 2. Now, consider the maps $\gamma_2 \circ \varphi: H_1 \to F_\alpha(H_2)$ and $\beta_2 \circ G^{\ast \varphi}: G^{\ast H_1} \to F_\alpha(H_2)$ . We have $\beta_2 \circ G^{\ast \varphi} \circ \alpha^{\ast H_1} = \beta_2 \circ \alpha^{\ast H_2} \circ \mathbb{Z}^{\ast \varphi} = \gamma_2 \circ \pi_2 \circ \mathbb{Z}^{\ast \varphi} = \gamma_2 \circ \varphi \circ \pi_1$ . Hence, since $F_\alpha(H_1)$ is the pushout of $\mathbb{Z}^{\ast H_1}$ along $\alpha^{\ast H_1}$ and $\pi_1$ , we have a unique map, which we will call $F_\alpha(\varphi): F_\alpha(H_1) \to F_\alpha(H_2)$ , with $\gamma_2 \circ \varphi = F_\alpha(\varphi) \circ \gamma_1$ and $\beta_2 \circ G^{\ast \varphi} = F_\alpha(\varphi) \circ \beta_1$ . Hence we obtain the following commutative cube diagram: It can be shown that $F_\alpha$ is indeed a functor. Additionally, from the commutative cube diagram above, it is clear that there is a natural transformation $\operatorname{id}_\mathsf{Grp} \Rightarrow F_\alpha$ . The question Does the functor $F_\alpha$ have a common name? Where can I read more about this functor? Can you give any reference? Does the functor $F_\alpha$ preserve limits? Does $F_\alpha$ preserve colimits? The idea is that $F_\alpha$ is a ""base change from $\mathbb{Z}$ to $G$ along $\alpha$ "". (See A functor $\mathsf{Grp} \to \mathsf{Grp}$ with $\mathbb{Z} \mapsto G$ )","['functors', 'category-theory', 'universal-property', 'abstract-algebra', 'group-theory']"
4907273,Closed form of binomial summation using complex numbers,"Calculate the sum: $1 + {n \choose 1}\cos \theta + {n \choose 2}\cos 2\theta + \cdots+ {n \choose n}\cos n\theta$ I found a solution here: The given sum is the real part of $$\sum_{k=0}^n{n\choose k}e^{ik\theta}=(1+e^{i\theta})^n=e^{in\theta/2}\left(2\cos\left(\frac{\theta}2\right)\right)^n$$ so the desired sum is $$2^n\cos^{n}\left(\frac{\theta}2\right)\cos\left(\frac{n\theta}2\right)$$ I did not understand the second equality, how they got $\sum_{k=0}^n{n\choose k}e^{ik\theta}=(1+e^{i\theta})^n=e^{in\theta/2}\left(2\cos\left(\frac{\theta}2\right)\right)^n$ , I can't seem to get how these two are equal, can anyone please help. Thanks!","['trigonometry', 'complex-numbers']"
4907294,Existence of weakly continuous regular conditional probability,"Let $X$ , $A$ be  compact metric spaces. We consider Borel probability measures. Let $\mu$ be a probability measure on $X\times A$ . Let $\hat{\mu}$ be the marginal of $\mu$ on $X$ , i.e., for every Borel subset $X_0\subset X$ , one has $\hat{\mu}(X_0)=\mu(X_0\times A)$ . By Proposition D.8 (1) of Discrete-time Markov control processes (by Onésimo Hernández-Lerma and Jean Bernard Lasserre), there is Markov kernel (product regular conditional probability=RCP) $\phi$ from $X$ to $A$ such that $$\mu(X_0\times A_0)=\int_{X_0}\phi(A_0|x)\hat{\mu}(dx)$$ for all Borel subsets $X_0\subset X$ , $A_0\subset A$ . Let $P(A)$ be the set of probability measures on $A$ . A map $\phi:X\to P(A)$ is called weakly continuous (aka weak Feller property) if for every continuous function $f:A\to \mathbb{R}$ , the function $X\to \mathbb{R},\quad x\mapsto\int_Af(a)d\phi_x(a)$ is continuous. Question : Is there a condition on $\mu$ ensuring the existence of a weakly continuous RCP $\phi:X\to P(A)$ ?","['measure-theory', 'borel-measures', 'conditional-probability', 'probability-theory', 'probability']"
4907343,"The number of functions $f : \{1, . . 100\} \to \{1, . . . , 100\}$ such that $f(x) \neq x,$ and $f(f(x)) = x$ for all $x$","I tried looking at the condition $f(f(x))=x.$ Which means $f$ is bijective. So it must be either strictly increasing or decreasing, but I feel this is severely under counting. Also the condition $f(x)\neq x$ isn't making sense to me.","['functions', 'combinatorics']"
4907355,"Calculate $\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz$ where $S = [2, 2i,−2,−2i, 2]$","Calculate $\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz$ where $S = [2, 2i,−2,−2i, 2]$ Using Partial fractions, the integral can be wrote as: $$\oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz - \oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz$$ Here, I am not sure what path(s) I am to use for $\gamma_1$ & $\gamma_2$ . Does it remain the same path? If so, using Cauchy's integral theorem, for $\gamma_1$ , I get $$\oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz\\
= \oint_{\gamma_1} \frac{\frac{e^{\pi z}}{4}}{z - 0} dz$$ Hence, by Cauchy's Integral formula, $2\pi f(0) = 2i\pi \frac{1}{4} = \frac{i\pi}{2}$ For $\oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz$ , the function is analytic at $z_0 = \frac{i}{2}$ & $z_0 = -\frac{i}{2}$ Hence, $2\pi f(\frac{i}{2}) = \frac{i}{8}$ & $2\pi f(-\frac{i}{2}) = -\frac{i}{8}$ So, $\oint_{\gamma_2} \frac{e^{\pi z}}{4z^2 + 1} dz = 0$ And the total integral is $\frac{i\pi}{2}$ ?","['complex-analysis', 'cauchy-integral-formula']"
4907406,Does a finite-dimensional algebra have the same length as a regular module when considered as a left module and as a right module?,"Let $A$ be a finite-dimensional algebra over a field $k$ . Naturally, $A$ can be considered as a left $A-module$ and as a right $A-module$ . So the question arises: Does a finite-dimensional algebra $A$ have the same length as a regular module when considered as a left module and as a right module? In other words, do we always have $$l(_{A}A)  =  l(A_{A})\ \ ,$$ where $l(_{A}A)$ and $l(A_{A})$ denote the length of $A$ as a left module and as a right module, respectively?","['abstract-algebra', 'modules']"
4907458,Another question about the fixed point and group action.,"Let $G$ be a group of order 15 that acts on a set $X$ of 7 elements. $a)$ Prove that exist a fixed point. $b)$ If X is a group and $G$ acts by authomorphism. What can we conclude about the fixed points? For $a)$ if there where none fixed point, by the orbit stabilizer theorem, we get $7=3k+5m$ for some positive integres $k,m$ , imposible. For part $b)$ If $X$ is a group then the fixed points are a subgroup of $X$ , so there is a unique fixed point or the action is trivial. It's okay the conclusion or there is something better we can conclude?","['group-actions', 'group-theory', 'abstract-algebra', 'finite-groups']"
4907468,"Find $\sum_{n=1}^{2024} g_n\left(\frac{1}{\sqrt{n+1}}\right)$, where $g_n$ is defined recursively.","Problem Statement Given a function $f(x)=x^2$ , define $g_1(x)=f(x)$ and subsequently; $g_{n+1}(x)=\min_{t\in\Bbb R}(g_n(t)+f(x-t))$ , where $n\in\Bbb N$ . Find : $\sum_{n=1}^{2024}g_n\left(\frac{1}{\sqrt{n+1}}\right)$ . I am not interested in someone solving this for me. However, how would you generally approach such sums? I have tried things like trying to understand if $g_n(x)$ becomes repetitive, or if there is some pattern in the coefficients of $g_n(x)$ but nothing seems to work on this. Quite peculiar, the coefficients become exceedingly ugly and all of them are additive. Any help, would be immensely appreciated.","['calculus', 'functions', 'induction', 'sequences-and-series']"
4907477,"Definite Integral $\int_{0}^{1} \log\left(1-u^{\frac{1}{3}}\right) \, d u$","My Attempt Consider the integral \begin{align*}
    I_1 &= \int_{0}^{1} \log\left(1-u^{\frac{1}{3}}\right) \, d u.
\end{align*} Integrating by parts, letting $a=\log\left(1-u^{\frac{1}{3}}\right)$ and $\, d b = \, d u$ , we obtain \begin{align*}
    d a = -\frac{1}{3u^{2/3}(1-u^{1/3})} \, d u \quad \text{and} \quad b = u,
\end{align*} so that \begin{align}
    \nonumber I_1 &= \left[ u\log\left(1-u^{\frac{1}{3}}\right) \right]_{0}^{1} - \frac{1}{3} \int_{0}^{1} \frac{u^{1/3}}{1-u^{1/3}} \, d u\\
    &= \left[ u\log\left(1-u^{\frac{1}{3}}\right) \right]_{0}^{1} - \frac{1}{3} I_2. \tag{1}
\end{align} Note that \begin{align*}
    I_2 &= \int_{0}^{1} \frac{u^{1/3}}{1-u^{1/3}} \, d u.
\end{align*} Letting $v=1-u^{\frac{1}{3}}$ , we have \begin{align*}
    u=(1-v)^{3} \quad \text{and} \quad \, d u = -3(1-v)^2 \, d v.
\end{align*} Hence, \begin{align*}
    I_2 &= \int_{1}^{0} \frac{1-v}{v} \left[ -3(1-v)^2 \right] \, d v\\
    &= 3 \int_{0}^{1} \frac{(1-v)^3}{v} \, d v\\
    &= 3 \int_{0}^{1} \frac{1-3v+3v^2-v^3}{v} \, d v\\
    &= 3 \int_{0}^{1} \left( \frac{1}{v} - 3 + 3v - v^2 \right) \, d v\\
    &= 3 \left[ \log(v) - 3v + \frac{3}{2}v^2 - \frac{1}{3}v^3 \right]_{0}^{1}.
\end{align*} Substituting into equation (1), we have \begin{align*}
    I_1 &= \left[ u\log\left(1-u^{\frac{1}{3}}\right) \right]_{0}^{1} - \frac{1}{3} \left\{ 3 \left[ \log(v) - 3v + \frac{3}{2}v^2 - \frac{1}{3}v^3 \right]_{0}^{1} \right\}\\
    &= \left[ u\log\left(1-u^{\frac{1}{3}}\right) \right]_{0}^{1} - \left[ \log(v) - 3v + \frac{3}{2}v^2 - \frac{1}{3}v^3 \right]_{0}^{1}\\
    &= \left[ \lim_{u\to 1} u\log(1-u^{\frac{1}{3}}) - 0 \right] + \left[ -\frac{11}{6} - \left( \lim_{v\to 0} \log(v) + 0 \right) \right]\\
    &= -\frac{11}{6} + \left[ \lim_{u\to 1} u\log(1-u^{\frac{1}{3}}) - \lim_{v\to 0} \log(v) \right]\\
    &= -\frac{11}{6} + \lim_{u\to 1} \left[ u\log(1-u^{\frac{1}{3}}) - \log(1-u) \right]\\
    &= -\frac{11}{6} + 0\\
    &= -\frac{11}{6}.
\end{align*} Question I am not sure whether I am integrating correctly or not. I can't seem to prove (or further elaborate) the last limit, which is \begin{align*}
    \lim_{u\to 1} \left[ u\log(1-u^{\frac{1}{3}}) - \log(1-u) \right],
\end{align*} to be zero. Can anyone help me with this problem? Any help/feedback is much appreciated. Thank you!","['integration', 'limits', 'calculus', 'definite-integrals']"
4907485,Explain step in proof of $\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n = \liminf_{n \to \infty} a_n = b$.,"I'm trying to understand a step in the forward direction of the proof of the theorem $\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b$ . First, to clarify, let $\{a_n\}_{n = 1}^\infty \subseteq \mathbb{R}$ . We define $M_n := \sup\{a_k : k \geq n\}$ and $m_n := \inf\{a_k: k \geq n\}$ yielding two new sequences $\{M_n\}$ and $\{m_n\}$ which may contain the symbols $\infty$ or $-\infty$ . We then define $\limsup_{n \to \infty}a_n := \lim_{n \to \infty}M_n$ and $\liminf_{n \to \infty}a_n := \lim_{n \to \infty} m_n$ which may again be $\infty$ or $-\infty$ . Proof: ( $\Longrightarrow$ ) [We limit this proof to limits converging to $b \in \mathbb{R}$ for now.] Suppose that $\lim_{n \to \infty}a_n = b$ . For any $\varepsilon > 0$ there exists $N \in \mathbb{Z}^+$ such that $|a_n - b| < \varepsilon$ for all $n \geq N$ . Those, we have $b - \varepsilon < a_n < b + \varepsilon$ . But then, $\mathbf{b - \varepsilon < M_n \leq b + \varepsilon}$ and $\mathbf{b - \varepsilon \leq m_n < b + \varepsilon}$ for all $\mathbf{n \geq N}$ . Since this holds for every $\varepsilon > 0$ , we have $\limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b$ . I am not sure how the bold statement follows immediately. Clearly, $b - \varepsilon < M_n$ and $m_n < b + \varepsilon$ , but I don't see why $M_n \leq b + \varepsilon$ and $b - \varepsilon \leq m_n$ . I am sure it's something simple that I'm not seeing (properties of supremum and infimum probably), but any help would be appreciated.","['limsup-and-liminf', 'real-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum']"
4907537,"Out of 10 people in a room, probability that exactly 2 of them share the same birthday","I am considering the following problem: Out of 10 people in a room, find the probability that exactly 2 of them share the same birthday.
I fully understand the simpler problem, that is, out of 10 people, probability that at least 2 share the same birthday.
An easier way to solve this problem than solving for the probability that any 2 or more of the 10 people share a birthday is to solve for the probability that all of the people have unique birthdays (non-shared) and subtracting that from 100%. The first person has 365 possible days that could be their birthday. Then, the second person has 364 days that could be their birthday without sharing with someone else, because person 1’s birthday is one of those days. This process goes on for all of the 10 people, until the 10th person has 356 possible days that could be their birthday.
The easiest way to express the total possible combinations of days in which none of the 10 people share a birthday is $365 \times 364 \times 363 \times … 356$ , that is, $\frac{365!}{355!}$ . To solve for the total probability that, out of the original $365^{10}$ days, there are $\frac{365!}{355!}$ where no one shares a birthday, we have: $\frac{(\frac{365!}{355!})}{365^{10}}$ Then we subtract this from $100%$ to get the complementary probability. But this will give us the probability that at least 2 people share the same birthday.
To find the same but for exactly 2 people, how do we approach it?
My approach is as follows:
First, let's calculate the total number of possible outcomes, which is the total number of ways to assign birthdays to 10 people without restrictions. Since there are 365 days in a non-leap year, each person can have a birthday on any of these days. Therefore, the total number of possible outcomes is $365^{10}$ .
Then we calculate the number of favorable outcomes, which is the number of ways to choose 2 people out of 10 to share the same birthday, multiplied by the number of ways to choose a specific birthday for them:
The number of ways to choose 2 people out of 10 is: ${10 \choose 2}=\frac{10!}{(10−2)!2!}$ .
For each pair of people, there are 365 possible birthdays they could share. So, the number of favorable outcomes is $45 \times 365$ .
Then, we calculate the probability: $\frac{(45\times 365)}{365^{10}}$ .
Is it correct?",['combinatorics']
4907555,Why is construction required in geometry?,"I don't know if this is a good question to ask but I need some advice, anything is appreciated. (Contruction here, doesn't imply geometric construction tools one) Q1 :In solving problems in geometry, we often have to make a construction.
Making a construction, does not create anything new. Then why can not we solve problems without any construction? Why do we need to make constructions, if they don't add any new condition to the problem? Q2 : How do you motivate towards a construction? I have a hard time at hitting at the right construction, so is there any way to know what to do, in order to arrive at the proof or is it again just experience? (It would help if you answer in an olympiad perspective.)","['contest-math', 'advice', 'problem-solving', 'geometry']"
4907598,A neat functional equation $f(x)=f(x+c)-f'(x+c)$,While investigating I thought of this functional equation $$ f(x)=f(x+c)-f'(x+c) $$ and I wondered if there are any real analytic non-constant solutions. Here $c$ is some constant. Are there any real analytic non-constant solutions? It sort of reminds me of $f'(x)-f(x)=0$ which $f(x)=e^x$ satisfies but different. I tried integrating this equation but didn't get the solution yet.,"['functional-equations', 'calculus', 'functions', 'ordinary-differential-equations']"
4907601,Analog of Jensen Inequality,"Jensen's inequality states that if $\phi$ is a convex function, then we have that: $$
\phi(\mathbb{E}[X])\leq \mathbb{E}[\phi(X)] 
$$ Where in this case X is a discrete random variable.
Is there any bound to the other side? I mean, $$
\mathbb{E}[\phi(X)] \leq ?
$$ In particular, I've been trying to bound $E[-\log(X)]$ . I also have the variance of $X$ , let's just call the mean and the variance $\mu$ and $\sigma$ .","['discrete-mathematics', 'probability']"
4907610,How to determine which event is the conditional event?,"I know generally conditional probability questions are presented in the form: ""What is the probability of Event A given Event B,"" and from there it can be written as: $P(A|B)$ However, how is that determined when the question doesn't explicitly state the given condition? For example, if we have a question such as, ""What is the probability that a randomly selected baseball player is a pitcher who voted ""Yes"" on the team questionnaire?"" Would it be $P($ Pitcher | Voted Yes $)$ or $P($ Voted Yes| Pitcher $)$ Thank you!","['conditional-probability', 'statistics', 'probability']"
4907611,Find self-similar solution for the heat equation,"I would like to find solutions to the equation $$\partial_t \phi = \partial_{rr}\phi + \frac{1}{r}\partial_r \phi - \frac{1}{r^2}\phi$$ of the form $\phi(t, r) = t^{-\gamma} g(r/\sqrt{t})$ . This equation comes from the heat equation in polar coordinates for function $\psi(r, \theta) = \phi(r)e^{i\theta}$ . Plugging this form of $\phi$ in the equation leads to the ODE in $\xi$ $$\frac{1}{t^{\gamma + 1}} \left( g'' + \frac{1}{\xi} g' - \frac{1}{\xi^2} g + \gamma g + \frac{1}{2}\xi g\right) = 0 \quad \text{for } \xi = \frac{r}{\sqrt t}.$$ Multiplying by $t^{\gamma + 1}$ on both sides, we get the ODE depending only on $\xi$ and $\gamma$ . I would like to find solution of this equation for specific value of $\gamma$ . I managed to find a straightforward solution $g(\xi) = \xi$ for $\gamma = -1/2$ . Moreover, for $\gamma = 3/2$ , I managed to find the solution $$g(\xi) = \xi e^{-\xi^2/4}.$$ Now I would like to find solution of this ODE for $\gamma$ positive and lower than $3/2$ , i.e. $\gamma \in (0,3/2)$ , but to be honest I don't see anything obvious. Any idea ?","['ordinary-differential-equations', 'heat-equation', 'calculus', 'functional-analysis', 'partial-differential-equations']"
4907620,Expectation of the power of random graph's adjacency matrix,"I am trying to find the power of the random graph's adjacency matrix. Consider the $2N$ number of nodes, where $N \geq 2$ is a fixed natural number. WLOG, let nodes with index $1$ to $N$ belong to Group 1, and nodes with index $N + 1$ to $2N$ belong to Group 2. Then, for a node pair within the same group, a pair of nodes form an edge with the probability of $p$ (i.e., $P(\{v_{i},v_{j}\} \in \mathcal{E}) = p$ if $v_{i}$ and $v_{j}$ belong to the same group.) Else, for a node pair in the different groups, a pair of nodes form an edge with the probability of $1-p$ (i.e., $P(\{v_{i},v_{j}\} \in \mathcal{E}) = 1-p$ if $v_{i}$ and $v_{j}$ belong to the different groups.) We do not allow the self-loop. You may think this is a variant of the Erods-Renyi random graph, where there are two disjoint node groups. Note that this is an undirected graph. Let $A$ be an adjacency matrix of this random graph. My question is: What is the closed-form solution to the expectation of 4-power and 3-power of $A$ ? That is: $\mathbb{E}_{A}[A^{3}]$ and $\mathbb{E}_{A}[A^{4}]$ . In other words, what would be the expected number of length-3 and length-4 paths between node $v_{i}$ and $v_{j}$ (this would differ whether $v_{i}$ and $v_{j}$ belong to the same group or not)? Thank you.","['graph-theory', 'adjacency-matrix', 'expected-value', 'discrete-mathematics']"
4907626,Does anybody know this integral expression?,"I stumbled upon the following integral, which is a low dimensional case of a minimization problem I currently investigate. $$F(V) = \int_{\mathbb{R}^3}|W+\nabla\times V)|^2+|\nabla\times(W+\nabla\times V))|^2dx$$ where $W$ is a fixed vector field that is divergence free and $V$ is another vector field, both compactly supported on $\mathbb{R}^3$ or periodic in all components in which case we would just integrate over the smallest period rectangle (I do not know if there is a term describing what I mean here, but it should be clear). Of course by Poincares lemma one has that in the non-periodic case a minimizer $V$ is the negative vector potential of $W$ , since then $F(V)=0$ . It would be really helpful, if somebody could give an interpretation of this integral or point to some physical object this integral corresponds to.","['field-theory', 'vector-fields', 'differential-geometry']"
4907634,Clarifications on the solution of a double integral: $\iint_X\frac{x^2y}{x^2+y^2}dxdy$,"Calculate the following double integral: $$\iint_X\frac{x^2y}{x^2+y^2}dxdy$$ where $X=\{(x,y)\in \Bbb R^2\colon 1\leq x^2+y^2\leq2x\}.$ Here my confusion arises. Looking at the integrand the polar coordinates seem to be better suited to solve the exercise. But I have the problem with the domain. If I used polar coordinates I would write: $$1\leq r^2\leq 2r\cos\vartheta $$ I have drew the domain with an online tool https://www.wolframalpha.com/ and the domain is a half-moon of which I understand from the drawing the limits of $x$ but not those of $y$ (How to find the limits of $x$ and $y$ ?). They are two circumferences of center in $O$ and radius $1$ and the other a circle of center in $(1,0)$ and radius $1$ .
I think by eyes that even setting the correct integration extremes, for sure the double integral is very complicated.
Staying in polar coordinates if I looked at the drawing I would immediately realize that $1\leq r\leq 2$ which I cannot find from $1\leq r^2\leq 2r\cos\vartheta$ and that $\vartheta\in[-\pi/2,\pi/2]$ . We would have $$\iint_X\frac{x^2y}{x^2+y^2}dxdy=\int_{[1,2]}rdr\int_{[-\pi/2,\pi/2]}\frac{r^3\cos^2\vartheta\sin\vartheta}{r^2}d\vartheta$$ $$=\int_{[1,2]}r^2dr\int_{[-\pi/2,\pi/2]}\cos^2\vartheta\sin\vartheta d\vartheta$$ But the second integral is zero (it is an immediate integral). So the double integral is worth $0$ ? I don't think so. Addendum: Just for curiosity how I obtain the solution using the cartesian coordinates?","['multivariable-calculus', 'calculus', 'polar-coordinates', 'nonlinear-system', 'inequality']"
4907714,Show that the polar line of the exincenter passes through this midpoint,"We're given a triangle $\triangle ABC$ whose incenter is $I$ and its $A$ -exincenter is $I_A$ . Let line $EF$ be the polar of $A$ with respect to the incircle of $\triangle ABC$ , let $G = EF \cap BC$ , let $D$ be the point of that incircle who is also on $BC$ . Show that the polar line of $I_A$ wrt the incircle of $\triangle ABC$ passes through the midpoint $M$ of $GD$ . It is easy to see that the polar of $I_A$ is parallel to $EF$ but I'm struggling with that midpoint. Is there some hidden projection here? EDIT: they're suggesting me to do length chasing on the comments below. So: $BD = p-b = \frac{a+c-b}2$ $\frac{x}{x+a} = \frac{c}b \iff GB = \frac{ac}{b-c}$ so $MB = \frac12(\frac{ac}{b-c} + b-p)$ what now? EDIT2: Ok, now we got the final hint in the comments. It is clear that $\mathcal H (G,D;B,C)$ which implies that $$MD^2 = MB \cdot MC$$ Look how interesting: if the mark the intersections of the green polar with the incircle and name them $XY$ , then quad $XYBD$ is cyclic as $MX \cdot MY = MD^2 = MB \cdot MC$ . Now I need to justify why $I_A$ is on $(XYBC)$ . Which I'm struggling a bit.","['euclidean-geometry', 'projective-geometry', 'geometry']"
4907728,Sandwiching the Lp norm sequence of random variable,"Let X be a random variable with $\Vert X\Vert_p = E[\vert X\vert^p]^{1/p}<\infty$ for all $p\geq 1$ . Assume for some fixed $r$ , and for all $q,s$ satisfying $1\leq q < r < s$ , $$\lim_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/q}}=0$$ and $$\limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/s}}=\infty$$ Does this imply that there is some $K>0$ such that $$\limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/r}}=K$$","['lp-spaces', 'probability-theory', 'probability', 'real-analysis']"
