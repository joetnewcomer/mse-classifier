question_id,title,body,tags
3237990,Converse of Schauder's Theorem about compactness of adjoint operator,"It is well known (also known as Schauder's Theorem) that if $X$ and $Y$ are normed spaces and $T:X\to Y$ is a linear and compact operator, then also $T^*:Y^*\to X^*$ is compact. The converse  is true if $Y$ is complete. So the natural question is: Is there an ""easy"" example that shows that we cannot drop the completeness of $Y$ for the converse implication? In order to prove the converse, one usually applies the first implication to the bidual. Thus, a counterexample should be rooted in the subtle difference between ""relatively compact"" and ""totally bounded"", but I cannot wrap my head around it. Any ideas are highly appreciated. Thank you in advance!","['banach-spaces', 'compact-operators', 'functional-analysis']"
3238027,"Definition of Covariance Matrix, to divide by $n$ or not.","In most sources, the covariance of matrix of observations $X \in (n,p)$ is usually defined as $$cov(X) = \frac{\hat X^T \hat X}{n-1}$$ where $\hat X = X - \mu (X)$ . However, after reading the definition of Wikipedia , it is not divided by the $(n-1)$ term. Why? I'm not talking about bias, since they don't divide by $n$ either.","['statistics', 'covariance']"
3238038,Pullbacks commute with the exterior derivative,"I am trying to show by induction that pullbacks and the exterior derivative commute. I know that this question has been discussed on this site, eg. here and here . However, none of these questions employ the approach that I'm interested in: So if $f: X \rightarrow Y$ is a smooth map of manifolds and $\alpha \in \Omega^p(Y)$ is a differential p-form on $Y$ , then I want to show that $$f^*(d\alpha) = d(f^*(\alpha)) \textit{ in } \Omega^{p+1}(X)$$ So my approach is via induction on the $p$ , exploiting that Pullbacks commute with wedge products $f^{*}(\alpha \wedge \beta) = f^{*}(\alpha)\wedge f^{*}(\beta)$ And that the exterior derivative is unique under the conditions that (i) $d^2 = 0$ , (ii) $d(\alpha \wedge \beta) = d\alpha \wedge \beta + (-1)^k \alpha \wedge d\beta$ , (iii) $dg$ for a smooth map $g \in C^{\infty}(X)$ is just the usual derivative So the induction step is via the commutativity with wedge products, but I'm struggling to unwrap the definitions for the base case. Here I just need to verify that for a smooth map $a\in C^{\infty}(Y)$ the following holds: $$f^* (da) = d(f^*(a))$$ I think this should just be a notation chase. Let $f(x) = y$ , so the LHS is just $$f^*(da)\rvert_x = (T^*f)(da\rvert_y) \in T_x^*X$$ Whereas the RHS is $$d(f^*a)\rvert_x = d(a\circ f)\rvert_x \in T_x^{*} X$$ How can I show that these two expressions are equal? Did I get something wrong?","['exterior-derivative', 'geometry', 'smooth-manifolds', 'differential-geometry']"
3238042,Limit of powers of $3\times3$ matrix,"Consider the matrix $$A = \begin{bmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{bmatrix}$$ What is $\lim_{n→\infty}$$A^n$ ? A) $\begin{bmatrix} 0 & 0 & 0\\ 0& 0 & 0\\ 0 & 0 & 0 \end{bmatrix}$ B) $\begin{bmatrix} \frac{1}{4} &\frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\end{bmatrix}$ C) $\begin{bmatrix} \frac{1}{2} &\frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\end{bmatrix}$ D) $\begin{bmatrix} 0 &\frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\end{bmatrix}$ E) The limit exists, but it is none of the above The given answer is D). How does one arrive at this result?",['matrices']
3238057,Efficient computation of conjugacy classes of a small group.,"I'm trying to construct a character table for a group of order 54 given by: $$ \langle a,b : a^9 = b^6 = 1, b^{-1} a b = a^2\rangle $$ To do this first I need to compute conjugacy classes. This feels like a tedious task and I'm looking for some guidance on how to compute those efficiently.","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
3238093,Is a function differentiable at a point if its derivative is continuous at that point?,My professor said that the title statement might not always be the case and gave $$x^2 \sin\left(\frac{1}{x}\right)$$ at $x=0$ as a counter-example. But I don't seem to understand its differentiability and continuity at the point $x=0$ . Any explanation and if possible a better example are highly appreciated.,"['continuity', 'calculus', 'derivatives']"
3238120,Realising I don't truly understand differentiation when not stated explicitly.,"Probably much too late in my maths 'career' to be asking this but, I don't fully understand differentiation in multiple dimensions when functions aren't stated explicitly. I have got by up to now, but I think it's time to learn properly. Some examples: In my PDE's course we are studying the wave equation and for one question we define $$u(x,t) = v(\xi,\eta),\,\, \xi = x+ct,\,\, \eta = x-ct$$ where $c\in\mathbb{R}$ . And the following line is $\partial_{x}u = \partial_{\xi}v+\partial_{\eta}v$ . I understand where this comes looking at the formula for differentiation, but get confused with the dependancies, as it is almost never explicitly written. Also considering functions from multiple variables to one. Say for example you have a differentiable function $f$ and you try to calculate $$\frac{d}{dx}f(ax+bt)\quad \text{and}\quad \frac{d}{dt}f(ax+bt)$$ where $a,b \in\mathbb{R}$ and $x,t$ are variables. I believe the answers respectively are $$af'(ax+bt)\quad \text{and}\quad bf'(ax+bt).$$ But, I have no idea where $f'$ comes from in this context. Does it mean $$f' = \frac{d}{d(ax+bt)}?$$ If so why? How can I find some intuition with calculating non-explicit forms as such?","['partial-derivative', 'multivariable-calculus', 'soft-question']"
3238135,How to argue why one dice is more rigged than the other?,"Let $\omega$ be a finite set and $P : \Omega \rightarrow \mathbb{R}$ be a probability measure. You are given a set of three dices $\{A, B, C\}$ . The following table describes the outcome of six rollouts for these
dices, where each column shows the outcome of the respective dice. (Note: assume the dices are standard six-sided
dices with values between 1-6) $$
\begin{array}{|c|cccccc}{A} & {4} & {4} & {2} & {4} & {1} & {1} \\ \hline B & {3} & {6} & {3} & {3} & {4} & {3} \\ \hline C & {5} & {5} & {2} & {1} & {1} & {1}\end{array}
$$ (1) Estimate the expectation and the variance for each dice using unbiased estimators. (Show your computations). (2) According to the data, which of them is the “most rigged”? Why? So for (1) I calculated: \begin{equation*}
    \begin{aligned}
        \overline{x}_A &= \frac{8}{3}   &, \qquad     s_A^2 &= \frac{34}{15} \\
        \overline{x}_B &= \frac{11}{3}  &, \qquad     s_B^2 &= \frac{22}{15} \\
        \overline{x}_A &= \frac{5}{2}   &, \qquad     s_C^2 &= \frac{51}{10} \\
    \end{aligned}
\end{equation*} In order to figure out which one is the most ""rigged"" one I calculated the expectation and variance of a ""perfect"" dice: \begin{equation*}
    \begin{aligned}
        \mathbb{E}[X] = \frac{7}{2} \qquad \mathbb{V}[X] = \frac{35}{12}
    \end{aligned}
\end{equation*} As the dices differ in expectation and variance from a ""perfect"" dice I am trying to figure out how I could argue which one is the most ""rigged"" one? The professor mentioned the Kullback–Leibler divergence in another setting very briefly and said that the KL divergence is a measure for the ""difference"" of distributions. Is this the tool he wants us to use here?","['statistical-inference', 'statistics', 'entropy', 'information-theory', 'machine-learning']"
3238184,Is there a Birkhoff's Ergodic Theorem for multivariate functions?,"I recently tackled a problem and I arrived at something of the following form, $$ \frac{1}{n^2} \sum_{i=0}^{n-1}\sum_{j=0}^{n-1} f(T^ix, T^jy), $$ where $T$ is a measure preserving transformation and I am interested in the limit as $n$ tends to infinity. In my case it  is the shift operator in a probability space. In the uni-variate case Birkhoff's ergodic theorem states that for a measure preserving transformation $T$ ,in a measurable space $(X, \mathscr{B})$ , with a $\sigma$ -finite measure $\mu$ $$\frac{1}{n} \sum_{i=0}^{n-1} f(T^ix)$$ converges a.e. to a function $f*\in L^1$ , with $f^* = \frac{1}{\mu(X)}\int fd\mu$ . Is there an equivalent result for the multivariate case? Will the first equation converge  to it's average?","['measure-theory', 'ergodic-theory']"
3238186,Combinatorial inequality in Erdös-Kac proof.,"I am reading a proof of Erdös-Kac theorem, in Durrett, ""Probability: Theory and Examples"", fourth edition. In some point, it is stated that $(\sum_{m=1}^nEZ_{n,m}^2)^k - \sum_{i_j} EZ_{n,i_1}^2 . EZ_{n,i_2}^2 ... EZ_{n,i_k}^2 \leq {k \choose 2} (2\epsilon_n)^2 (\sum_{m=1}^nEZ_{n,m}^2)^{k-1}$ Where $i_j$ extends over all $k$ -tuples of positive distinct integers with $1 \leq i \leq n$ , and it is knwon that $|Z_{m,s}| \leq \epsilon_n$ . The book states that you can see this is true because: "" we note that for any $1 \leq a < b \leq k$ we can estimate the sum over all the $i_1, ..., i_k$ with $i_a = i_b$ by repacling $EZ_{n,i_a}^2$ by $(2\epsilon_n)$ to get the inequality above (the factor ${k \choose 2}$ giving the number of ways to pick $1 \leq a < b \leq k$ ). I just don't understand the text of the argument; for a start I cannot see how it can be $i_a = i_b$ when $a < b$ since, by definition, the $i_j$ range over different integers. At his point, the inequality $\sum_{i_j} EZ_{n,i_1}^2 . EZ_{n,i_2}^2 ... EZ_{n,i_k}^2 \leq (\sum_{m=1}^nEZ_{n,m}^2)^k$ is kown to hold. I am sorry for asking such a general question, but as I said, I believe I might be misunderstanding even what's being stated in the proof (english is not my mother language). I appreciate very much every piece of hlep, and thak you very much in advance.","['combinatorics', 'probability-theory']"
3238192,"Choice of symbols: $O_p(G)$, $O^p(G)$, and $O_\infty(G)$","For a finite group $G$ and a prime number $p$ , several normal subgroups are defined as follows: $O_p(G)$ = the largest normal $p$ -subgroup of $G$ ( $p$ -core ) $O^p(G)$ = the smallest normal subgroup of $G$ for which the quotient is a $p$ -group ( $p$ -residual ) $O_\infty(G)$ = the largest normal solvable subgroup of $G$ ( solvable radical ) There are also $p'$ versions as well and more generally $\pi$ versions, but it doesn't matter here. My question: why the symbol "" $O$ "" is used in these? Many symbols for constructing groups have obvious reasons such as $C_G(H)$ for a centralizer, $N_G(H)$ for a normalizer, $\Phi(G)$ for the Frattini subgroup, $D(G)$ for the derived subgroup, $Z(G)$ for the center (=Zentrum, a bit tricky) and so on and so forth. But I have no idea why the notation "" $O$ "" is employed here. Do you know any reason? If there is no particular reason and it is just an accident, who started using it?","['notation', 'finite-groups', 'group-theory', 'soft-question']"
3238220,Proving these inequalities for a symmetric random walk,"I would like to prove the following inequalities. Here $S_n = \sum_{i=1}^n X_i$ where $x_i$ are i.i.d and symmetric. $$P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|X_k|>x)\geq \frac{1}{2}(1-e^{-nP(|X_i|>x)})$$ $$P(\max_{k\leq n} |S_i|>x)\leq 2P(|S_n|>x)$$ My attempt: For the first one $$P(S_n>x) = \sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x, S_n-S_k > 0) $$ $$= \frac{1}{2}\sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x) $$ $$=\frac{1}{2} P(\max_{k<n} S_k > x)$$ I think by symmetry of the random walk, that then gives $$P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|S_k|>x)$$ I am completely stuck on getting from $S_k$ to $X_k$ . Further, I don't know how to get to $$\frac{1}{2}(1-e^{-nP(|X_i|>x)})$$ For the second inequality I believe I need to use the reflection principle in someway. But I have also made no progress towards that one. Edit: It just occured to me that: $$P(\max_{k\leq n}|X_k|>x)=1-P(\max_{k\leq n}|X_k| \leq x)$$ $$=1-P(|X_1| \leq x)^n=1-(1-P(|X_1| > x))^n$$ $$\geq 1-e^{-nP(|X_i|>x)}$$ So I got that part sorted I think I got the first inequality. Basically it goes $$P(S_n>x) \geq \sum_k P(X_k > x, \sum_{i\neq k}X_i>0, \max_{j<k}|X_j|<x)$$ $$=\frac{1}{2}\sum_k P(X_k > x,\max_{j<k}|X_j|<x)$$ Doing the same for $P(S_n<-x)$ and then adding gives the result. Can someone confirm this is correct? For the second inequality Let $A_k = \{ |S_k| > x; S_j < x, j<k\}$ $P(|S_n|>x) =\sum_{k=1}^n P(A_k; sgn(S_n-S_k)=sgn(S_k))$ But here I'm unsure how to get $\geq$ . It seems the latter should be $=\frac{1}{2}...$ ? Where the last inequality follows from the fact that","['inequality', 'probability']"
3238227,Density of Solution to ODE in Function Space,"Let $d$ be a positive integer.  Let $f:\mathbb{R}^{d+1}\rightarrow \mathbb{R}^d$ and $g:\mathbb{R}^d\rightarrow \mathbb{R}^d$ , be once continuously differentiable functions and define the solution map $\Phi(t,x)$ to the (family of) initial value problems $$
\begin{aligned}
\partial_t \Phi(t,x) &= f(t,\Phi(t,x))\\
\Phi(0,x)&\triangleq g(x).
\end{aligned}
$$ Is there a theorem characterizing $f$ , such that the set $X$ defined by: $$
X\triangleq \left\{
g(x) : (\exists t \in [0,\infty)) g(x)=\Phi(t,x) (\forall x \in \mathbb{R}^d)
\right\},
$$ is dense in $C(\mathbb{R}^d,\mathbb{R}^d)$ and $\Phi$ satisfies $$
\Phi(t,x)\circ \Phi(s,x)= \Phi(t+s,x)
, (\forall x \in \mathbb{R}^d).
$$ If this is not possible, is there a theorem discussing a ""reasonable"" Borel-measure $\mu$ on $C(\mathbb{R}^d;\mathbb{R}^d)$ such that $\{f^n\}$ is $\mu$ -almost everywhere dense? Note: $C(\mathbb{R}^d;\mathbb{R}^d)$ is considered with the compact-open topology.","['chaos-theory', 'ordinary-differential-equations', 'metric-spaces', 'functional-analysis', 'dynamical-systems']"
3238311,Proving that a specific Volterra integral operator is not positive,"I want to prove that the operator $$ A: L^2[0,1] \to L^2[0,1], \quad A(u)(s) = \int_0^1 |t-s| u(t) dt $$ is not positive, i.e. $\langle Au, u \rangle \geq 0$ does not hold for every $u \in L^2[0,1]$ . I could not find a counterexample but I think it is possible to prove that this operator has a negative eigenvalue which would imply my claim. Any idea how to construct a counterexample for the claim itself or maybe an eigenfunction corresponding to a negative eigenvalue?","['operator-theory', 'functional-analysis', 'eigenvalues-eigenvectors', 'partial-differential-equations']"
3238312,Evaluate $\lim_{x \to 0}[\frac{\ln(x+\sqrt{1+x^2})}{x}]^{1/x^2}$.,"Let $\ln(x+\sqrt{1+x^2})=:y$ ，then $x=\dfrac{1}{2}(e^y-e^{-y}).$ Therefore \begin{align*}
\lim_{x \to 0}\left[\frac{\ln(x+\sqrt{1+x^2})}{x}\right]^{\frac{1}{x^2}}&=\lim_{y \to 0}\left(\frac{2ye^y}{e^{2y}-1}\right)^{\frac{4}{e^{2y}+e^{-2y}-2}}\\
&=\lim_{y \to 0}\left(1+\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\right)^{\frac{e^{2y}-1}{2ye^y-e^{2y}+1}\cdot\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}}\\
&=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}\right)\\
&=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{2y}\cdot\frac{4e^{2y}}{e^{4y}-2e^{2y}+1}\right)\\
&=\exp \left(2\lim_{y \to 0}\frac{2ye^y-e^{2y}+1}{ye^{4y}-2ye^{2y}+y}\right)\\
&=\exp \left[2\lim_{y \to 0}\frac{2e^y(y-e^y+1)}{(e^{2y}-1)(e^{2y}(4y+1)-1)}\right]\\
&=\exp \left[2\lim_{y \to 0}\frac{y-e^y+1}{e^{2y}(4y^2+y)-y}\right]\\
&=\exp \left[2\lim_{y \to 0}\frac{1-e^y}{e^{2y}(8y^2+10y+1)-1}\right]\\
&=\exp \left[-2\lim_{y \to 0}\frac{y}{e^{2y}(8y^2+10y+1)-1}\right]\\
&=\exp \left[-2\lim_{y \to 0}\frac{1}{e^{2y}(16y^2+36y+12)}\right]\\
&=\exp\left(-\frac{1}{6}\right).
\end{align*} Please correct me if I'm wrong! Are there simpler solutions?",['limits']
3238333,Adjoint of a linear isomorphic functional is an isomorphism,"I have this exercise to solve: Let $E$ and $F$ be normed spaces and let $T \in L(E,F)$ , where $L(E,F)$ denotes the set of all bounded linear operators from $E$ to $F$ . $T^*$ is the dual operator of $T$ with $T^*: F^* \to E^*$ and $T^*\varphi=\varphi \circ T$ . Assume $T$ is an isomorphism, i.e. $T$ is bijective and $T^{-1}\in L(F,E)$ . Prove that $T^*$ is also an isomorphism and $(T^*)^{-1}=(T^{-1})^*$ . My work Proof of $(T^*)^{-1}=(T^{-1})^*$ : We know that $(T \circ G)^*= T^*\circ G^*$ since $((T \circ G)^*\varphi)(x)=\varphi((T\circ G)(x))=(G^*\varphi)(Tx)=((T^*\circ G^*)\varphi)(x)$ Now plug in $T^*$ and $(T^{-1})^*$ to the equation, we have: $T^*\circ (T^{-1})^* = (T\circ T^{-1})^*= Id^* = Id$ and $(T^{-1})^* \circ T^* = (T^{-1}\circ T)^*= Id^* = Id$ Hence, $(T^*)^{-1}=(T^{-1})^*$ . Proof of isomorphism: Injectivity: Let $\varphi$ and $\psi \in F^*$ . Let $T^*\varphi=T^*\psi$ then we have $\varphi \circ T=\psi \circ T$ . Here can I conclude that since $T$ bijective we have $\varphi = \psi$ ? It remains to prove the surjectivity and I'm still struggling with this part.","['normed-spaces', 'functional-analysis', 'dual-spaces']"
3238341,Mathematically rigorous Quantum Mechanics,"I am a student of mathematics attending a course in Quantum Mechanics. This course is held by a physicist, and it is really confusing for me to follow his reasonments. With this, I do not mean to be arrogant or disrespectful, it is only  a matter of backgrounds. I was wondering if anyone knows a textbook which is mathematically rigorous, in the sense that the functional analytical structure of the subject (distributions, measures, Hilbert spaces, duality)  is properly handled. Instead of doing inconceivable (for me) things such as considering Dirac's delta a function or distinguishing between a state in $L^2(V)$ and its Fourier coefficients in $l^2$ , when we have a nice isomorphism between these two. I have background in Functional Analysis, Measure Theory and Topology, but a reference which explains also some mathematics would be good, you never know. The course covers all the basis of quantum mechanics, hence: Schroedinger and Dirac's picture with examples (hclassical problems, harmonic oscillator, hydrogen atom), spins and reaches the helium atom. Mostly it follows Schwabl Quantum Mechanics https://www.springer.com/it/book/9783540719328 , which I consulted but does not provide what I search for. I really want to make it clear: I really appreciate physicists and their stunning mind elasticity: after all, most of distribution theory was developed to underpin what Dirac had grasped with intuition. But I am just a student trying to do my best at college, and I need to cope with my background! Thanks in advance.","['quantum-mechanics', 'soft-question', 'functional-analysis', 'reference-request']"
3238360,Why does $f(x_n)\rightarrow 0$ imply that a subsequence converges to zero of $f$?,"Let $f$ be a separable and irreducible polynomial of degree $d\geq 1$ with coefficients in a local field of characteristic $p$ , say $K= \mathbb F_p((T))$ and $f\in K[X]$ . Assume there is a sequence $(y_n)_{n\in \mathbb N}$ in the separable closure $K^s$ satisfying $$
\lim_{n \rightarrow \infty} f(y_n) = 0.
$$ I want to show that there is a subsequence $(y_{n_{k}})_{k\in \mathbb N}$ converging to a zero $x$ of $f$ in $K^s$ . I don't know much about non-archimedian analysis and I wonder if the proof can be stated similar to the real case?","['algebraic-number-theory', 'irreducible-polynomials', 'local-field', 'nonarchimedian-analysis', 'limits']"
3238429,Evaluating an infinite series $\sum_{n=1}^{\infty} \frac{1}{n^{2}2^{n}}$ [duplicate],"This question already has answers here : $\sum_{n=1}^{\infty}\frac{1}{n^22^n}$ by integration or differentiation (3 answers) Closed 5 years ago . I have been trying to find the sum of the series $$\sum_{n=1}^{\infty} \frac{1}{n^{2}2^{n}}$$ but I couldn't find any methods (such as a fourier series) that seem to get me anywhere. WolframAlpha gave $ \dfrac{\pi^2}{12}-\dfrac{ln^2(2)}{2},$ but how would one get to this?","['calculus', 'sequences-and-series']"
3238436,Why does $ \lim_{x \to 2} \frac{x^2-4}{x-2} =4 $ if x cannot be 2?,"I know that $ \lim_{x \to 2} \frac{x^2-4}{x-2} $ is evaluated as follows :- $$ \lim_{x \to 2} \frac{x^2-4}{x-2} \\ = \lim_{x \to 2} \frac{(x+2)(x-2)}{x-2} \\ = \lim_{x \to 2} x+2 \\ = 2+2 \\ = 4 $$ By looking at the function $ \frac{x^2 - 4}{x - 2} $ , I can see that 2 is not in its domain. Therefore, I am not able to understand how $ \lim_{x \to 2} x+2 = 2+2 $ .",['limits']
3238468,What characterizes a tangent line?,"If the traditional way to define the tangent line to a curve $f(x)$ through the point
say $(a , f(a))$ is: ( the tangent line through the point $(a ,f(a))$ is the line that passes through this point with a slope that is equal to the value of the derivative at that point). 
From this definition why it follows that in many cases the tangent line touches the curve only once locally? For example take the parabola $f(x)=  x^2 $ the tangent line at the point $(1,1)$ has the equation $g(x)= 2x-1$ . $g$ touches $f$ only at $(1,1)$ . 
Why this is true not only for this special case, but for most curves at most points? I have an intuitive feeling for why this is true, so, is there a proof that the tangent line (from the above definition)  will have this property (it touches the curve only once locally) for certain curves like: a circle or a polynomial of degree more than 1?","['tangent-line', 'real-analysis', 'calculus', 'slope', 'derivatives']"
3238501,"How long until a random word with letters ""A"", ""B"", ""C"" ends in the pattern ""ABC""?","Let's say I have word constructed from random letters, A B and C with $\mathbf{P}(A) = \mathbf{P}(B) = \mathbf{P}(C) = \frac{1}{3}$ .  I am going to do a random trial and record the letters I got.  The experiment stops the first time I spell out the word ABC .  Let $N$ be the number of trials until I make the word ABC out of letters. Here are some trial words: BBBBACCCCBABAABBBBCBCCBBBCACBCAACBABC
BBACCCCACABABC
CBBCCCABBABC
BABBBCAAAABC
CBBBCCBCCABABC
CCBCBBABC
ACCACCCCBCBBBCBACCBBAABBABBACCCBCBAABC
ABAAABBBABC
ABABC
BBCACAACCACCAABAAABBCABBBBACABACBACBAABACCCBCBCCCBCCCBAAAABC I am asking for the expected length of this word.  And the variance. $\mathbb{E}[N]$ expectation $\mathbb{E}[N^2] - \mathbb{E}[N]^2$ variance Sounding more like a textbook: Our random variable is $X \in \{ A,B,C\}$ where each letter appears with equal probability.  Let's examine the sequence $(X_1, X_2, X_3, \dots , X_n)$ where $X_i$ are iid random variables with probability the same as $X$ .  Our process stops at time $t = N$ when $(X_{N-2}, X_{N-1}, X_N) = (A,B,C)$ .  What is the expected value of $N$ ?","['martingales', 'markov-chains', 'probability']"
3238510,A polynomial algorithm to determine whether a finite group is nilpotent,"Does there exist a polynomial (in respect to the order of the group) algorithm that given a Cayley table of a finite group determines, whether a group is nilpotent or not? There do exist polynomial algorithms, that determine, whether a group is nilpotent of degree k or not. The simplest of them is simply checking the necessary commutator identity for all $(k + 1)$ -ples of elements of the group (this algorithm works for $O(n^{k + 1})$ , where $n$ is the order of the group). However, none of them can be used to determine in polynomial time, whether a group is nilpotent of arbitrary degree .","['finite-groups', 'nilpotent-groups', 'algorithms', 'group-theory', 'computational-complexity']"
3238525,Nonvanishing form in $\Omega^3(O(3))$,"This is an old exam question which most of it I understand now due to the comments below. I still have two concerns. Define a $p$ -form on $GL(n)$ as follows. $$\Theta_p=tr(X^{-1}dX \wedge X^{-1}dX \wedge \cdots \wedge X^{-1}dX)$$ (i) Restrict $\Theta_3$ to $O(3)$ .  Writing the matrix $X$ with orthonormal column vectors $\mathbf{x_1, x_2,x_3}$ , show that $X^{-1}dX$ is a skew symmetric matrix whose $i,j$ th entry is $\mathbf{x}_i\cdot d\mathbf{x}_j$ . (ii) Show that at $X=I$ , the forms $\mathbf{x_1} \cdot \mathbf{dx_2}, \mathbf{x_2 \cdot dx_3},$ and $\mathbf{x_3, \cdot dx_1}$ are linearly independent and $\Theta_3$ is nonzero. (iii) Deduce that since $(AX)^{-1}d(AX)=X^{-1}dX$ , $\Theta_3$ is non vanishing at all points. (a) why can we just restrict $\Theta_3$ to $O(3)$ ? How do we know this form is still smooth? i.e. the induced map to $O(3) \rightarrow \wedge^*T^*O(3)$ is still smooth? (b) Why does the equality in (iii) show $X^{-1}dX$ is left invariant? I am also unsure what this means, since we are working with $1$ -forms rather than vector fields. I suppose we are to show: $$(L_{A^{-1}}^*) X^{-1}dX_I = X^{-1}dX_A $$","['de-rham-cohomology', 'manifolds', 'lie-groups', 'differential-forms', 'differential-geometry']"
3238539,Isometries of Hilbert cube,"I'm looking for information about the isometries of the Hilbert cube: $Q= \prod_{i=1}^{\infty}[0,1]$ , with the distance : $d((x_i),(y_i))= \sum_{i=1}^\infty\frac{|x_i-y_i|}{2^i}$ , but I have not accomplished much. Someone has good references on the subject. Thank you","['general-topology', 'reference-request']"
3238563,Question about the proof of simple algebra rule $\frac{1}{\frac{1}{a}} = a$,"I have a question about a proof I saw in a book about basic algeba rules. The  rule to prove is: \begin{eqnarray*}
\frac{1}{\frac{1}{a}} = a, \quad a \in \mathbb{R}_{\ne 0}
\end{eqnarray*} And the proof: \begin{eqnarray*}
1 = a \frac{1}{a} \Longrightarrow 1 =  \frac{1}{a} \frac{1}{\frac{1}{a}} \Longrightarrow a = a \frac{1}{a} \frac{1}{\frac{1}{a}} \Longrightarrow \frac{1}{\frac{1}{a}} = a
\end{eqnarray*} Why is it allowed to just replace $a$ with $1/a$ ? What's the explanation behind it?",['algebra-precalculus']
3238591,"Two circles of equal radius, one ""passes over"" the other: the intersection area grows and shrinks non-smoothly?","Two circles of equal radius ""r"" (assume r=1 for simplicity); one ""passes over"" the other. 
They start by touching, having an empty intersection, and then slowly the intersection grows, until the circles are on top of each other. In that case, the intersection area is of course equal to the area of the unit circle (r=1). As the circle moves on after that, it shrinks again. If I follow the formula in http://mathworld.wolfram.com/Circle-CircleIntersection.html where x is the ""width"" of the intersection area, I get: Well this is kind of unexpected isn't it? That means, if the circle starts moving away again, because of the symmetry of the problem, it starts shrinking with the mirror of this graph. But, that leaves it with a spike at the top of the graph?! Meaning, the intersection area grows and shrinks in a non-smooth manner? Did anyone expect this to happen? Did I make an error when calculating?","['trigonometry', 'circles']"
3238606,What is the most commonly used metric in $\mathbb{C} \times \mathbb{C}$?,"I am reading Conway's Functions of One Complex Variable , and many times it makes reference to a function from $\mathbb{C} \times \mathbb{C}$ being continuous. I would like to rigorously prove such assertions, so I have to know which metric he is referring to. I haven't taken topology, so I'm not sure what the most natural or nice metric would be. The one I've come up with is $d[(z_1, w_1), (z_2, w_2)] = |z_1-z_2| + |w_1 - w_2|$ . Is there a ""natural"" metric that is most often used on $\mathbb{C} \times \mathbb{C}?$","['complex-analysis', 'general-topology', 'analysis', 'real-analysis']"
3238609,Books that leaves proofs for the reader,"What are some good introductory books that leave many proofs as exercises? I have been self studying analysis by reading Tao's two fantastic books which eventually leaves most of the (easier) proofs as exercises, and I am wondering if there are similar books for other topics (as of right now I am thinking about introductions to groups and/or topology). Thanks!","['general-topology', 'reference-request', 'book-recommendation', 'group-theory']"
3238655,Repeated incomplete Steiner Triplets,"I'm not a mathematician, so I hope this question makes sense. As a hobby, I organize leagues for amateur volleyball teams. To minimize travelling costs the matches are played as small tournaments with 3 teams, where everyone plays each other (2+1=3 matches). At the end of the season, all teams should have played each other the same amount of times. For the past seasons, the number of teams has been either 7 or 9, so it all worked out fine. This year there are 10 teams, so I discovered that some games have to be played as single games. I then discovered that this has a name: http://mathworld.wolfram.com/SteinerTripleSystem.html and that it is only possible to do if the number of teams mod 6 equals 1 or 3. Is there an algorithm that for a given number v will minimize the number of ""remaining"" 2-tuples? Furthermore, under what circumstances is it possible to combine 2 or more such sets, so that the remaining 2-tuples can be combined into triplets? As an example of when we can combine incomplete triplets is for 5 elements S = {A, B, C, D, E} If we repeat this process 3 times, it is possible to end up with: A   B   C
E   B   D
D   C   E
C   A   D
B   A   E
A   D   E
C   B   E
D   A   B
E   A   C
B   C   D I.e., everyone plays each other 3 times, with a total of 3*(4+3+2+1)=30 games.",['combinatorics']
3238712,Solve Riccati equaiton $x'(t) + x^2(t) = \sin(t) + \cos(t)$,"Given differential equation is $$x'(t) + x^2(t) = \sin(t) + \cos(t)$$ I was able to notice that this is a Riccati equation, but could not actually solve, since I do not know none of its partial solutions.",['ordinary-differential-equations']
3238717,Calculate $\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}$,"Calculate $$\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}$$ My try: $$\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} \sqrt{2+\frac{k}{n}-(\frac{k}{n})^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} f(\frac{k}{n})=\int^{1}_{0} \sqrt{2+x-x^2} dx=\int^{1}_{0} \sqrt{-(x-\frac{1}{2})^2+\frac{9}{4}} dx=\int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du$$ In this sollution: $$f:[0,1]\rightarrow \mathbb R, f(x)=\sqrt{2+x-x^2}$$ $$u=x-\frac{1}{2}, du=dx$$ Unfortunatelly I don't know what I can do with $\int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du$ because my only idea is integration by substitution. But if I use for example $s=u^2$ then I have $ds=2udu$ so $du=\frac{ds}{2u}$ so I did not get rid of $ u $ which is problematic. Can you help me how to bypass this problem?",['real-analysis']
3238744,How to evaluate the following integral involving a gaussian?,"I want to evaluate the following integral: $$\int\limits_0 ^\infty {x \sin{px} \exp{(-a^2x^2})} dx$$ Now I am unsure how to proceed. I know that this is an even function so I can extend the limit terms to $-\infty, \infty $ and then divide by 2. I have tried to evaluate this on Wolfram Alpha, but it only shows the answer while I am interested in the procedure.","['integration', 'definite-integrals']"
3238783,"Prove that $f(x,y)$ is derivable for all direction in $(0,0)$ but it is not differentiable at $(0,0)$","Prove that $$f(x,y)=\begin{cases}\dfrac{x^2}{y}&\text{if $(x,y)\neq(x,0)$},\\f(x,0)=0\end{cases}$$ is derivable for all direction in $(0,0)$ but it is not differentiable at $(0,0)$ . I have 3 questions: I think that the function can be translated to $$f(x,y)=\begin{cases}\dfrac{x^2}{y}&\text{if $(x,y)\neq(x,0)$},\\\color{red}0&\color{red}{\text{if $(x,y)=(x,0)$}},\end{cases}$$ right? To prove that has directional derivative in all direction in $(0,0)$ we need to prove that the following limit exists: $$\lim_{h\to0}\frac{f(ah,bh)-f(0,0)}{h},$$ where $\check{v}=(a,b)\in\Bbb R^2$ and $a^2+b^2=1$ . Indeed, $$\lim_{h\to0}\frac{f(ah,bh)-f(0,0)}{h}=\lim_{h\to0}\frac{\frac{(ah)^2}{bh}-0}{h}=\lim_{h\to0}\frac{a^2h^2}{bh^2}=\frac{a^2}{b}=\begin{cases}\frac{a^2}{b}&\text{if $b\neq0$},\\\color{blue}0&\color{blue}{\text{if $b=0$}},\end{cases}$$ thus the limit exists for all direction. The text in $\color{blue}{\text{blue}}$ is correct because we know that $b$ goes in $y$ -direction, and $f(x,y)=0$ if $y=0$ ? To prove that $f$ is not differentiable at $(0,0)$ we can study the continuity of $f$ at $(0,0)$ : $f(0,0)=0$ , but $$\lim_{(x,y)\to(0,0)}f(x,y)=\lim_{(x,y)\to(0,0)}\frac{x^2}{y}\underbrace{=}_{(*)}\underset{y=x^2}{\lim_{x\to0}}\frac{x^2}{x^2}=1,$$ where in $(*)$ we have taken the curve of level $1$ of $f$ , thus $f$ is not continuous at $(0,0)$ . Hence, it is not differentiable at $(0,0)$ . Can we take the curve of level $1$ just ""imposing"" $\frac{x^2}{y}=1$ i.e. $y=x^2$ , or conversely, we need to prove that for all $(x,y)\in E^*(0,0)\cap\{(x,y)\in\Bbb R^2\mid(x,y)\neq(0,0)\}$ , it is $y=x^2$ ? Thanks!","['limits', 'multivariable-calculus', 'derivatives']"
3238810,Is the sum of this series a differentiable function?,Let $$f(x) = \sum_{n=1}^{\infty} \frac{1}{nx} \left( 1 - \frac{1}{e^{ \frac{x}{n}}}  \right) \wedge x>0$$ Is the sum of this series a differentiable function? my idea For examining differentiation let: $$ g_n (x) :=  \frac{1}{nx} \left(1 - \frac{1}{e^{ \frac{x}{n}}} \right) $$ then $$ g_n'(x) = -\frac{\left(n e^{x/n}-n-x\right)}{e^{\frac{x}{n}}n^2 x^2} $$ but $g_n'(x)$ seems to be asymptotic similar  to $\frac{1}{n}$ so I can't use theorem which can help me to eventually proof that $f$ is differentiable. What should I do in such situation?,"['derivatives', 'real-analysis']"
3238831,"Defining homotopy fiber ""naturally""","For a map $f\colon A\to B$ , many authors define the mapping fibration $E_f\to B$ , where $E_f$ consists of pairs $(a,\gamma)\in A\times B^I$ with $\gamma(0)=f(a)$ , and the fibration map is $(a,\gamma)\mapsto \gamma(1)$ . The fiber $F_f$ is the homotopy fiber, and consists of pairs $(a,\gamma)$ with $\gamma$ a path from $f(a)$ to a basepoint $b\in B$ . At the same time, there are many god-given fibrations lying around that are confusing similar. For example, we have $B^I\to B$ , with $\gamma\mapsto \gamma(0)$ . As sets, $E_f$ is just the pullback of $f$ along this fibration. However, this doesn't actually produce the right fibration map $E_f\to B$ is any obvious way. So my (soft) question is the following: Is there a way to define the homotopy fiber of a map $f\colon A\to B$ which is ""natural"" in the colloquial sense? I should also clarify that I don't know any model theory and am asking this as a newcomer to homotopy theory. Partial answer: The most satisfying solution I have found is to start with the path fibration $PB$ consisting of paths starting at a basepoint $b_0$ . The map $PB\to B$ with $\gamma\mapsto g(1)$ is a fibration, and $F_f$ is the pullback of $f\colon A\to B$ along this. This is almost satisfying, but still requires picking this evaluation map.","['homotopy-theory', 'general-topology', 'fibration', 'soft-question']"
3238856,"Understanding definition of random variable in textbook ""Basic Stochastic Processes""","In Zdzislaw Brzezniak and Tomasz Zastawniak, ""Basic Stochastic Processes"" A random variable is defined as, How do I check the condition For 
  every Borel set $B \in \mathcal{B}(\mathbb{R})$ given that this set is very large? Note: The textbook defines a Borel set as the, As a concrete example, let the space $\Omega = \{H,T\}$ denoting Head and Tail respectively. Then the $\sigma$ -field on $\Omega$ is the set $\mathcal{F} = \{\varnothing, H, T, \{H,T\}\}$ Let's define a random variable which goes from $\Omega$ to $\mathbb{R}$ . $\xi(H) = 1, \xi(T) = -1$ How do I check that $\{\xi \in B\} \in \mathcal{F}$ for every single set in the Borel set?","['measure-theory', 'definition', 'borel-sets', 'probability-theory', 'random-variables']"
3238878,Center of the dihedral group with odd and even number of vertices,"I have posted a proof below, and would appreciate it if someone could review it for accuracy. Thanks! Problem: Let n $\in$ $\mathbb{Z}$ with $n$ $\ge$ 3. Prove the following: (a) Z(D $_{2n}$ ) = 1 if $n$ is odd. (b) Z(D $_{2n}$ ) = {1, r $^k$ } if $n$ = $2k$ . Note that $r$ and $s$ generate D $_{2n}$ with the group presentation { $r$ , $s$ | $r$$^n$ = $s$$^2$ = 1, $rs$ = $sr$$^{-1}$ } Proof: part (a) Let $n$ $\ge$ 3 where $n$ $\in$ $\mathbb{Z}$ and $n$ is odd. For any x $\in$ Z(D $_{2n}$ ), x must commute with both $s$ and $r$ , since both are in $D_{2n}$ . Note that if $x$ commutes with both $s$ an $r$ , then $x$ commutes with any element of $D_{2n}$ since $r$ and $s$ generate D $_{2n}$ . Then for any x $\in$ Z(D $_{2n}$ ), we have $xr$ = $rx$ , where x is of the form $x$ = s $^j$ r $^w$ (any such element can be arranged into this form via the relation $rs$ = $sr$$^{-1}$ and the fact that $r$ and $s$ generate D $_{2n}$ ). Note that since r and s have finite order, the exponents are modulo n and modulo 2 for $r$ and $s$ respectively. Then we have  s $^j$ r $^w$$r$ = $r$ s $^j$ r $^w$ . 
 Which implies s $^j$ r $^{w+1}$ = s $^j$ r $^{w +- 1}$ In the case where the power of r on the RHS is $w$ - 1, it is clear we cannot have equality unless |r| = 1, which it is not. Note that if $j$ is even then we have only the $w$ + 1 case. Hence $x$ may be of the form $1$$r$$^w$ = $r$$^w$ . Also we have that x must commute with $s$ , hence $s$$x$ = $x$$s$ . Then $s$$r$$^w$ = $r$$^w$$s$ . 
Which implies $s$$r$$^w$ = $s$$r$$^{-w}$ . Applying s $^{-1}$ to both sides we arrive at the task of finding when r $^w$ = r $^{-w}$ i.e when $w$ = $-w$ . But since r has finite order we have: $$ \bar{w} = (n-1)*w $$ where $\bar{w}$ is the residue class of w modulo n. Hence for some a $\in$ $\mathbb{Z}$$_+$ $$ w + an = wn - w $$ $$ 2w = wn - an $$ $$ 2w = n(w-a) $$ Now since 0 $\lt$ w $\le$ n we have that the LHS is greater than 0. Also since n $\ge$ w then (w-a) $\le$ 2, since otherwise the RHS would be greater than the LHS. So since n is odd we must have that (w-a) is even and hence (w-a) = 2, implying that 2w = 2n and hence n = w. Hence x = $r$$^w$ = $r$$^n$ = 1. So Z(D $_{2n}$ ) = 1. part (b) From the argument above any x $\in$ Z(D $_{2n}$ ) must be of the form $s$$^j$$r$$^w$ for j even and j $\in$ $\mathbb{Z}$ . Then x is of the form $r$$^w$ for $0$ $\lt$ w $\le$ $n$ . Since $n$ is even we have the equation below is satisfied when $2w$ = $n$ or $w$ = $n$ , since (w-a) $\le$ 2 but also (w-a) $\ge$ 0 for (w-a) $\in$ $\mathbb{Z}$$_+$ . $$ 2w = n(w - a) $$ Hence $r$$^w$ $\in$ Z(D $_{2n}$ ) when $2w$ = $n$ , as desired.","['group-theory', 'abstract-algebra', 'solution-verification', 'dihedral-groups']"
3238914,When is the Euler line parallel with a triangle's side?,When is the Euler line parallel with a triangle's side? I have found that a triangle with angles $45^\circ$ and $\arctan2$ is a case. Is there any other case? >,"['triangles', 'geometry']"
3238915,"Proving that, for an acute $\triangle ABC$, $\sin A + \sin B+\sin C\gt \cos A+\cos B+\cos C$","I need to prove or disprove that in any acute $\triangle ABC$ , the following property holds: $$\sin A + \sin B + \sin C \gt \cos A + \cos B + \cos C$$ To begin, I proved a lemma: Lemma. An acute triangle has at most one angle which is less than or equal to $\dfrac{\pi}{4}$ . Proof : Let there be an acute angled $\Delta ABC$ with the angles $A$ & $B \le \frac{\pi}{4}$ . Then $$ A + B \le \frac{\pi}{2}
\implies - (A + B) \ge -\frac{\pi}{2}
\implies C = \pi - (A+B) \ge \frac{\pi}{2}$$ thus contradicting that the triangle is obtuse. Hence, by contradiction, the lemma is proved. $\square$ Further, I used the identity that $\sin x - \cos x = \sqrt{2}\sin (x - \frac{\pi}{4})$ to rewrite the inequality as $$\sin \biggr(A - \frac{\pi}{4}\biggr) + \sin \biggr(B - \frac{\pi}{4}\biggr) + \sin \biggr(C - \frac{\pi}{4}\biggr) \gt 0$$ Without loss of generality, I assumed that $A \le \frac{\pi}{4}$ . If $A = \dfrac{\pi}{4}$ , then the inequality follows, since both $B$ and $C$ are strictly greater than $\dfrac{\pi}{4}$ . How do I prove the inequality if $A \lt \dfrac{\pi}{4}$ ? Any help or hint will be appreciated.","['euclidean-geometry', 'trigonometry', 'triangles']"
3238917,A contradiction arising due to set-builder form.,"I have noticed a kind of contradiction arising due to set-builder form involving universal set $(U)$ while I was performing operations on set theory. Here it is - Let us consider an arbitrary non-empty set $B$ .
  Then, $B = \{x : x \in B\} = \{x : x \in B \wedge x \in U\} = \{x : x \in B \cap U\} = \{x : x \in U\} = U$ But this is contradiction as $B \not= U$ .
Instead $B \subseteq U$ . Is it really a contradiction or my misconception ? A detailed explanation would be helpful.","['elementary-set-theory', 'notation', 'discrete-mathematics']"
3238941,Does dividing a common factor out from numerator and denominator of a rational function create a new function with different domain?,"Suppose a function is defined as $  f(x) =\frac{x^2 - 9}{x-3}. $ If we divide the common factor $ x-3 $ from both the numerator and denominator :- $$ \frac{x^2 - 9}{x - 3} \\ = \frac{(x+3)(x-3)}{x-3} \\ = x+3 $$ 1) Is $ x+3 $ a different function from the original $ \frac{x^2 - 9}{x - 3} $ ? 2) Since $ \frac{x^2 - 9}{x - 3} $ does not have 3 in its domain, when we simplify it to become $x + 3$ , does it now have 3 in its domain ?",['functions']
3238953,Compactness of finite sets,"In rudin's analysis books, he defines compactness as: A subset $K$ of a metric space $X$ is ${\bf compact}$ if every open cover of $K$ contains a finite subcover. More explicitly is that if $\{ G_{\alpha} \}$ is an open cover of $K$ then one can select finitely many indices so that $K$ is contained in $G_{\alpha_1} \cup ... \cup G_{\alpha_n } $ Rudin claims that a finite set is compact as an obvious fact. Im trying to verify this fact myself: verification: Let $F$ be a finite set and write it as $\{ a_1,...,a_n \}$ Take any open cover $\{ G_{\alpha} \}$ of $F$ . We can consider the open balls centered at $a_i$ and let $B_i$ be such ball. Then, $B_1 \cup ... \cup B_n$ contains $F$ . Now, this seems incomplete as we must show that this finite union of open balls is in $\cup_{\alpha } G_{\alpha}$ . How do we check this?",['real-analysis']
3238973,"Prove $(y-x^2)$ is a prime ideal in $\mathbb{R}[x,y]$, but not maximal.","My guess is to use the fact that when we take the quotient, $\mathbb{R}[x,y]/(y-x^2)$ , this will become an integral domain but not a field. I am not sure how to take the quotient, though. I am also unfamiliar with the ring of polynomials of two variables. As a set, can I write $\mathbb{R}[x,y] = \{a + bx + cy + dxy + ex^2+fy^2+...| a,b,c,d,e,f,...\in \mathbb{R}\}$ ? And would it be correct if I assume that in the quotient, $y = x^2$ ? If my above two guesses are correct, then I would assume that the quotient becomes $\mathbb{R}[x]$ , since all the $y$ terms can be turned into $x^2$ . But isn't this ring an integral domain, since there are no zero divisors, but not a field, since not every real polynomial has an inverse?","['ring-theory', 'abstract-algebra', 'maximal-and-prime-ideals', 'ideals']"
3239008,"Proving that $\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z$","Exercise : Assume that $T$ satisfies the equation $T_t(t,z) = aT_{zz}(t,z)$ for $t>0, z \in (0,1)$ and $a > 0$ a constant. Moreover, suppose that $T(0,z) = T_0(z)$ for $z \in [0,1]$ , where $T_0 : [0,1] \to \mathbb R$ and that $T(t,0) = T(t,1) = 0$ . Show that : $$\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq 2 \int_0^1 T^2(t,z)\mathrm{d}z$$ Attempt-thoughts : Since we have the boundary conditions $T(t,0) = T(t,1) = 0$ for the problem and our integrations are over the interval $[0,1]$ , we can ""see"" the given inequality as a norm-2 inequality and use the Wirtinger inequality which gives us a weaker lower bound than the one desired though, as it would be : $$\text{Wirtinger :} \;\|T_z\|_2^2 \geq \pi^2\|T\|_2^2 \Leftrightarrow \int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z \geq \pi^2 \int_0^1 T^2(t,z)\mathrm{d}z$$ So, that intuition falls short. Important : Another thought, since its often carried out in such cases (and also a hint given by our professor) is that the Cauchy-Schwarz inequality shall be used. I cannot see how though. Finally, I have previously proven via substitution from the PDE and integration by parts, that : $$\frac{\mathrm{d}}{\mathrm{d}t} \int_0^1 T^2(t,z)\mathrm{d}z = -2a\int_0^1 \left(\frac{\partial T}{\partial z}(t,z)\right)^2\mathrm{d}z $$ I don't know if that can be of any help. Any hints or elaborations will be greatly appreciated.","['ordinary-differential-equations', 'cauchy-schwarz-inequality', 'functional-analysis', 'integral-inequality', 'inequality']"
3239037,"MacLane-Birkhoff's ""Algebra"" vs Jacobson's ""Basic Algebra I,II"" vs Lang's ""Algebra""","I'm searching for an apt textbook(s) on Abstract Algebra for a very advanced undergraduate/graduate level course in Algebra, and would be grateful for any help. I've thought of the aforementioned texts, but additional suggestions would be welcome. Some points about my background: I’ve taken a course in Linear Algebra where I read Roman’s Advanced Linear Algebra in addition to Halmos’ Finite Dimensional Vector Spaces and Axler’s Linear Algebra Done Right as primary texts. I’ve already had a course of Abstract Algebra from Artin’s Algebra . Additionally, I have finished the first 7 chapters of Baby Rudin, and plan to try Loomis and Sternberg’s Advanced Calculus next. These upcoming semesters I have courses in Algebra (the syllabus for which is attached at the end). The prescribed texts are Jacobson’s Basic Algebra I, II , and Lang’s Algebra . Some of the material is familiar, so I’m looking to study beyond the syllabus. I've looked through Lang's Algebra , MacLane and Birkhoff’s Algebra , and Jacobson’s Basic Algebra I,II . So far, Basic Algebra I seems much easier and more ‘leisurely’ than the other two. Understanding the exposition was not an issue for any of the books (I used G. Bergman's Companion to Lang for some assistance). Unfortunately Basic Algebra I usually gives explicit constructions as opposed to using categories or universal properties. I would prefer to learn Abstract Algebra using Category Theory and Universal Properties openly; to do this from Jacobson’s book, I would have to use both volumes together. I’m not sure how to do this. I referred to the Chicago Undergraduate Mathematics Bibliography , which suggested that few portions from Basic Algebra II (such as Group Representation Theory) were best done from elsewhere. I would be grateful if somebody could compare using Basic Algebra I, II , MacLane and Birkhoff’s Algebra , and Lang’s Algebra ; In particular, their relative merits/demerits, levels of difficulty, ‘modernness’ of the treatment, and quality of the exercises. I enjoy struggling through the texts that are terse, and leave significant gaps (such as in proofs) for the reader to fill in, something like Rudin or Lang's books. Lastly, is it better to do any one of these books from cover to cover? Or is it better to do individual sections from each book, or perhaps one book followed by another? If it is the latter two, then could the relevant chapters/order please be pointed out to me? All comments and answers are greatly appreciated. Thank you so much for your time! (Syllabus below) Rings, ideals, homomorphisms, quotient rings, fraction fields, maximal ideals, factorization, UFD, PID, Gauss Lemma, fields, field extensions, finite fields, function fields, algebraically closed fields.
Galois theory: separable and normal extensions, purely inseparable extensions, fundamental theorem of Galois theory. Module theory, structure theorem for modules over PIDs. multilinear algebra: tensor, symmetric and exterior products, tensor product of algebras. Categories and functors, some notions of homological algebra.
Non-commutative rings, semisimplicity, Jacobson theory, Artin-Wedderburn theorem, group-rings, matrix groups, introduction to representations.","['reference-request', 'ring-theory', 'abstract-algebra', 'group-theory', 'soft-question']"
3239071,Solve $\sqrt{1 + \sqrt{1-x^{2}}}\left(\sqrt{(1+x)^{3}} + \sqrt{(1-x)^{3}} \right) = 2 + \sqrt{1-x^{2}} $,"Solve $$\sqrt{1 + \sqrt{1-x^{2}}}\left(\sqrt{(1+x)^{3}} + \sqrt{(1-x)^{3}} \right) = 2 + \sqrt{1-x^{2}} $$ My attempt: Let $A = \sqrt{1+x}, B = \sqrt{1-x}$ and then by squaring the problematic equation we get: $$(1+AB)(A^{3} + B^{3})^{2} = (AB)^{2} + 4AB + 4 $$ $$ A^{6} + B^{6} + BA^{7}  + A B^{7} = -2 (AB)^{4} - 2(AB)^{3} + (AB)^{2} + 4AB + 4 $$ I also have tried using $A = (1+x), B = (1-x)$ , and some others, but none solves the problem. I am now trying $A = (1+x)$ and $(1-x) = -(1+x) + 2 = 2 - A$ , so: $$\sqrt{1 + \sqrt{A(2-A)}}\left(\sqrt{(A)^{3}} + \sqrt{(2-A)^{3}} \right) = 2 + \sqrt{A(2-A)} $$","['contest-math', 'algebra-precalculus', 'polynomials']"
3239132,$E(X_k|Y)$ for $X_k \sim Bern(\theta)$ and $Y := \sum_{k=1}^n X_k.$,"Let $X_1, ..., X_n$ be i.i.d. random variables with $X_k \sim Bern(\theta)$ for $\theta \in (0,1)$ . Furthermore, define $Y := \sum_{k=1}^n X_k.$ Determine $E(X_k|Y).$ Since $X_k$ and $Y$ are obviously discrete and $X_k \in \{0,1\},$ we have that $E(X_k|Y) = P(X_k = 1 | Y = j)$ with $j \in \{1,...,n\}$ , which can be rewritten as $$\frac{P(X_k = 1, Y = j)}{P(Y = j)}$$ $Y$ is binomially distrbuted, therefore $$P(Y = j) = {n \choose j}\theta^j(1-\theta)^{n-j}.$$ Applying the multiplication formula on $P(X_k = 1, Y = j)$ yields $$P(X_k = 1) \cdot P(Y = j | X_k = 1)$$ with $P(X_k = 1) = \theta$ . In order to calculate $P(Y = j | X_k = 1)$ , I used the following reasoning: Assume that we want to calculate $$P(Y = 1| X_k = 1).$$ Given the assumption that $X_k = 1$ , it must be true that $Y \ge 1$ . We want to exclude the case that $Y > 1$ , which is true if there is some other $X_i$ such that $X_i = 1$ for $i \neq k$ . So we want to have that $X_i = 0$ for every $i \in \{1,...,n\} \setminus \{k\}.$ The probability for this event is $(1-\theta)^{n-1}.$ So overall, we receive that $$P(Y = 1|X_k = 1) = (1-\theta)^{n-1}.$$ Now, if we want to calculate $P(Y = 2| X_k = 1)$ , we need to look at the event that there is some other $X_l = 1$ for $l \neq k$ , which is true with probability $\theta$ . Similarly to above, we need to exclude the case that $Y > 2$ and receive that $$P(Y = 2|X_k = 1) = \theta \cdot (1-\theta)^{n-2}.$$ By the same reasoning, we receive that $$P(Y = 3|X_k = 1) = \theta^2 \cdot (1-\theta)^{n-3},$$ or in general: $$P(Y = j| X_k = 1) = \theta^{j-1} \cdot (1-\theta)^{n-j}.$$ Therefore, $$P(X_k = 1, Y = j) = \theta \cdot \theta^{j-1} \cdot (1-\theta)^{n-j} = \theta^{j} \cdot (1-\theta)^{n-j}.$$ So, $$E(X_k|Y) = \frac{1}{n \choose j}.$$ Is that correct? Edit: Now I finally understood what's wrong with this approach: $$P(Y = j| X_k = 1) = \theta^{j-1} \cdot (1-\theta)^{n-j}.$$ simply doesn't contain the possibility that there are several different arrangements of the values of the $X_2, ..., X_n$ such that $Y = j$ . The probability from above reads like there would be only one valid arrangement, when, in fact, it's ${n-1 \choose j-1}$ valid arrangements. Therefore, it should be $$P(Y = j| X_1 = 1) = {n-1 \choose j-1} \theta^{j-1} \cdot (1-\theta)^{n-j}.$$ The other results stay the same though, so it's actually $$\frac{{n-1 \choose j-1}}{n \choose j} = j/n,$$ which is the same result given in the answer below, though I agree that it would be more appropriate to write this as $$\frac{1}{n}Y(\omega)$$ for some $\omega \in \Omega$ .","['expected-value', 'probability-theory']"
3239258,"Find the natural number ""a"". [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Find the natural number ""a"" for which:- $$\sum_{k=0}^n f(a+k)=16(2^n-1)$$ where the function $f$ satisfies the relation $$f(x+y)=f(x)*f(y)$$ for all natural numbers $x,y$ and $f(1)=2$ . I can't figure out how to go about this problem.Any suggestions/solutions would be appreciated.",['functions']
3239284,Vertical curve in a Riemannian warped product is a geodesic,"Let $(N,g_N)$ be Riemannian manifold, $I \subset \mathbb{R}$ open with coordinate $r$ and $M:= I \times N$ with metric $g=dr^2+f^2 g_N$ . Let $c_p: I \rightarrow M, (t \mapsto (t,p))$ for a fixed $p$ be a curve in $M$ . 
I want to show that $c$ is a geodesic. To do that, I first look at local coordinates for $M$ , define $(y^1,....,y^{n+1})=((t,q) \mapsto t, x^1,...,x^n)$ where $x^i$ are local coordinates in $N$ . Now: $y^1 \circ c_p= (t \mapsto t)$ and $y^i \circ c_p=(t \mapsto x^i(p))$ for $i>1$ . I can conclude that $$\dfrac{\partial (y^1 \circ c_p)}{\partial t} =1,\quad \dfrac{\partial^2 (y^i \circ c_p))}{\partial t^2}=0 \ \forall i,\quad \dfrac{\partial (y^i \circ c_p)}{\partial t} =0 \ \forall i>1.$$ Now the equation for $\nabla_{c^{'}} c^{'}$ reduces to: $$\nabla_{c^{'}} c^{'}= \sum\limits_{i=1}^{n+1} \Gamma^i_{11} \dfrac{\partial}{\partial y_i} = \nabla_{\partial_1} \partial_1$$ Is it correct, that $ \nabla_{\partial_1} \partial_1=0$ (then my proof would be complete)? And if yes, why?","['geodesic', 'riemannian-geometry', 'curves', 'connections', 'differential-geometry']"
3239328,Solve recurrence with characteristic polynomial $a_n=7\cdot a_{n-1} -7\cdot a_{n-2}+175\cdot a_{n-3}+450\cdot a_{n-4}+(5+13\cdot n)\cdot9^n $,"The equation $$a_n=7\cdot a_{n-1} -7\cdot a_{n-2}+175\cdot a_{n-3}+450\cdot a_{n-4}+(5+13\cdot n)\cdot9^n \enspace,$$ where $a_0=148, a_1=144, a_2=-55, a_3=-61$ . I assume that a solution will look like $a^s_n+a^h_n$ , where $a^s_n$ . I solve like $$x^4-7x^3+7x^2-175x-450=0 \enspace,$$ then get $x_1=-2,\ \ x_2=9,\ \ x_3=-5i,\ \ x_4=5i\ \ $ and don't know what next. But the real problem is with the second part $(5+13\cdot n)\cdot9^n$ . I completely don't know what do with that.","['recurrence-relations', 'discrete-mathematics']"
3239332,Series Converging Almost Surely But Diverging in Mean,"I am looking for an example of independent, non-negative random variables $X_1, X_2, \dots$ such that $$
\sum_{n=1}^{\infty} X_n \, \lt \, \infty
$$ almost surely but $$
\sum_{n=1}^{\infty} \mathbb{E}(X_n) \, = \, \infty
$$ I can find examples of sequences which converge almost surely but diverge in mean, but can’t seem to be able to cook up an example with a series.","['probability-limit-theorems', 'expected-value', 'convergence-divergence', 'probability-theory', 'probability']"
3239387,Definition of infinite cyclic group,"I'm having some conceptual issues with the infinite cyclic group $C_\infty$ . Finite groups $C_n$ have a clear representation as integers $0,1,\cdots,n-1$ under addition $\pmod{n}$ , or as the rotation group of the $n$ -gon for $n\geq 3$ . The rotation group of a circle, which is what I interpreted $C_\infty$ to be, has uncountable order since any real angle $[0,2\pi)$ is valid. This would make it isomorphic to $[0,2\pi)$ under addition $\pmod{2\pi}$ . But online it says $(\mathbb{Z},+)$ is also isomorphic, which doesn't make sense to me because it has order $\aleph_0$ . Also, the first group has two inverses $0$ and $\pi$ , while this group only has $0$ . I'm guessing my interpretation is wrong. The textbook never defines what $C_\infty$ . What exactly is it?",['group-theory']
3239395,Prove that stochastic process trajectories are continuous,"Consider gaussian process $\{X_{t}, t \in [0,1]\}$ with zero mean and covariance function $R(X_{s}, X_{t}) = \min (s,t) -st$ . We want to know does this process has continous trajectories, i.e. $\lim_{t \to t_0} X_t = X_{t_0}$ a.e. My attempt : First of all let's prove that this process contiunous in $L_{2}$ . Consider $\mathbb{E}(X_{t+h} - X_{t})^{2} = h \to 0$ (the same result may be proved by proving continuity of covariance function). Next we have that it's continuous in measure (because of convergence in average). But we know that probability of $\mathbb{P}(\{ \lim X_t \to X_{t_0} \})$ is continuous function, so we may use Riesz theorem which give us subsequence converging almost sure. Am I right? Hard problem for me actually. I don't know how it can be proven only by definitions.","['stochastic-processes', 'probability-theory', 'functional-analysis']"
3239397,Find limit in use of integrals,"Find limit $\lim_{n \rightarrow \infty} \int_{-1}^{1} x^5 \cdot \arctan{(nx)} dx $ From mean-value-theorem we have $$ \frac{1}{2}  c^5 \cdot \arctan{(nc)}  \mbox{ for some c } \in (-1,1) $$ $$  \underbrace{\frac{1}{2}  \cdot \arctan{(-n)}}_{\rightarrow - \pi /4} \le \frac{1}{2}  c^5 \cdot \arctan{(nc)} \le \underbrace{\frac{1}{2}  \cdot \arctan{(n)}}_{\rightarrow \pi /4}   $$ so this bounding doesn't help me. Has somebody better idea how to bound that?","['integration', 'limits']"
3239464,How to prove the uniqueness of complement in the algebra of sets ( without using set theory but exclusively the laws of boolean algebra )?,"[edited] In abstract algebra, the uniqueness of inverse in a group  is proved using more basic laws ( laws of groups themselves, not extraneous laws). Could the same thing be done regarding the algebra of sets? How to prove, using only the laws of the algebra of sets, that a set has only one complement? Am I wrong in assuming that complement uniqueness can be proved inside the system of the algebra of sets? Is it rather a postulate?","['elementary-set-theory', 'abstract-algebra', 'soft-question']"
3239468,Computing the matrix powers of a non-diagonalizable matrix,"Define \begin{equation}
A = \begin{pmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{pmatrix}.
\end{equation} Note that the sum of the dimensions of the eigenspaces of $A$ is only two. $A$ is thus not diagonalizable. How can we compute $A^n$ ?","['matrices', 'matrix-exponential', 'linear-algebra']"
3239489,Average Distance Between Zeroes of $\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3})$,"QUESTION: What is the average distance between the consecutive real zeroes of the function $$f(x)=\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3})$$ or, more specifically, if $z(x)$ is defined as the number of zeroes $\zeta$ satisfying $|\zeta|<x$ , what is the value of $$\lim_{x\to\infty} \frac{2x}{z(x)}=?$$ Here’s some context. I’ve been studying sums of sinusoids with “mutually irrational” periods, such that the sum of the sinusoids is not actually a periodic function. For example, the function $$\sin(x)+\sin(x\sqrt{2})$$ is not periodic, because $\sqrt{2}$ is irrational. In particular, I’ve been looking at the asymptotic distribution of solutions $x$ to equations in the form $$\sin(x)+\sin(\tau x)=\alpha$$ where $\tau \notin \mathbb Q$ and $|\alpha|<2$ . I’ve actually come up with a formula for the average distance between the solutions to the above equation along the real line, but it’s messy so I won’t type it out unless someone cares enough to ask for it.  The case of $\alpha = 0$ is almost trivial though, and can be figured out with an easy trig identity. However, when dealing with three summed sinusoids the case of $\alpha = 0$ is no longer trivial. For two sinusoids, $$\sin(x)+\sin(\tau x)=2\sin\bigg(\frac{\tau+1}{2}x\bigg)\cos\bigg(\frac{\tau - 1}{2}x\bigg)$$ so we can easily calculate the actual explicit values of the zeroes. But for three sinusoids with mutually irrational periods so that $\tau_1, \tau_2, \tau_1/\tau_2 \notin\mathbb Q$ , $$\sin(x)+\sin(\tau_1 x)+\sin(\tau_2 x)$$ I haven’t been able to come up with any explicit formulas for zeroes, or even an asymptotic density of/average distance between zeroes. Can anyone figure out how to work out this problem for the specific case of $\tau_1 = \sqrt{2}$ , $\tau_2 =\sqrt{3}$ ?","['periodic-functions', 'almost-periodic-functions', 'real-analysis']"
3239496,LLN interpretation of high-dimensional unit ball mass distribution,"This answer on Math Overflow points out that For instance, the fact that most of the mass of a unit ball in high
  dimensions lurks near the boundary of the ball can be interpreted as a
  manifestation of the law of large numbers, using the interpretation of
  a high-dimensional vector space as the state space for a large number
  of trials of a random variable. I can hardly make any sense of it. Anyway, here is my attempt: instead of addressing an $N$ -dimensional unit ball, let's consider an $N$ -dimensional unit cube first. Each point on that cube can be expressed with a coordinate consisting of $N$ numbers, i.e. $(X_1, X_2, ..., X_N)$ , with $X_i \stackrel{iid}{\sim} \text{Uniform}[-1, 1]$ . This independence is a result of the orthogonality of axes. It is trivial to show that $E(X_i^2) = \frac{1}{3}$ . Applying the Law of Large Numbers, the (euclidean) distance from a point $P$ to $\mathbb{O}$ is then $$
\begin{equation}
\begin{aligned}
d(P, \mathbb{O}) &\stackrel{\text{def}}{=} \sqrt{X_1^2 + X_2^2 + ... + X_N^2} \\
E\left(d(P, \mathbb{O})^2\right) &= E\left(X_1^2 + X_2^2 + ... + X_N^2\right) \\
&\rightarrow \frac{N}{3}
\end{aligned}
\end{equation}
$$ Recall that we are interested in the unit ball, so we have to somehow calculate the distribution of $d(P, \mathbb{O})^2 | d(P, \mathbb{O})^2 \le 1$ , but I'm lost here. My question is, how do you interpret the mass distribution of an $N$ -dimensional unit ball with LLN?","['measure-theory', 'probability-distributions']"
3239499,How can this system not be asymptotically stable?,"I am currently studying stability of nonautonomous systems using the book Applied Nonlinear Control by Slotine & Li. On page 125, there is example 4.13: $$
\begin{align}
\dot{e} &= -e + \theta \, w(t) \\
\dot{\theta} &= -e \, w(t)
\end{align} \tag{1}
$$ with $w(t)$ a bounded, continuous but otherwise arbitrary time-varying function. They consider the Lyapunov function $$V(e, \theta) = e^2 + \theta^2$$ with derivative $$\dot{V}(e, \theta) = -2 e^2 \leq 0 \tag{2}$$ so $e$ and $\theta$ are bounded. Then, they use Barbalat's lemma to show that $\dot{V}(e, \theta) \rightarrow 0$ as $t \rightarrow \infty$ , so also $e \rightarrow 0$ . Then they say: Note that, although $e$ converges to zero, the system is not asymptotically stable, because $\theta$ is only guaranteed to be bounded. However, isn't it like this: If $e \rightarrow 0$ then this implies that $\dot{e} \rightarrow 0$ as well. So, for $t \rightarrow \infty$ , system $(1)$ reduces to $$
\begin{align}
0 &= \theta \, w(t) \\
\dot{\theta} &= 0
\end{align} \tag{3}
$$ Because $w(t)$ can be arbitrary for all time, the first equation of $(3)$ is only true if $\theta \rightarrow 0$ as well. The second equation of $(3)$ also confirms that $\theta$ doesn't change anymore for $t \rightarrow \infty$ . So, my conclusion would be: Since $e \rightarrow 0$ and $\theta \rightarrow 0$ , the system is actually asymptotically stable. However, this is in contradiction to the citation above. Question : Basically two questions: Where is the mistake in my argument? Or is it actually correct and the book is wrong? Is system $(1)$ now asymptotically stable or not? Note : I also tried some functions like $w(t) = \sin(t)$ with different initial conditions in simulation, and at least for those examples, the system always seemed to converge to $(0,0)$ .","['ordinary-differential-equations', 'lyapunov-functions', 'control-theory', 'stability-in-odes', 'dynamical-systems']"
3239502,"Finding Particular Intergrals for ODE's, Is it okay if particular integral varies?","The equation to solve : $y''-6y'+9y=e^{3x}$ This is the general solution from wolframalpha : $y(x) = c_2 e^{3 x} x + c_1 e^{3 x} + 1/2 e^{3x} x^{2}$ This is the solution I calculated from the method of undetermined coefficients : $y(x) = c_2 e^{3 x} x + c_1 e^{3 x} + Ax^2 e^{3x}$ This is the solution I calculated using the 'D' operator and shortcut method : $y(x) = c_2 e^{3 x} x + c_1 e^{3 x} + e^{3x}/18$ I think I'm getting different particular integrals while using different methods because this differential equation has a family of solutions ? But the fact that the last solution's P.I doesn't even have an 'x' term concerns me. All of them are correct, am I right ?","['calculus', 'proof-verification', 'ordinary-differential-equations']"
3239610,Show $\lim_{n\to\infty} na_n = 2$ [duplicate],"This question already has an answer here : $a_{n+1}=\log(1+a_n),~a_1>0$. Then find $\lim_{n \rightarrow \infty} n \cdot a_n$ (1 answer) Closed 5 years ago . Suppose $a_n$ is real sequence which satisfies $$ a_1>0, \quad a_{n+1}=\ln(a_n+1) \quad (n\geq1)$$ How can I evaluate $$\lim_{n\to\infty}na_n$$ ? I just know $$\lim_{n\to\infty} a_n=0$$ But I don't know what should I do for $na_n$ .","['limits', 'calculus', 'sequences-and-series']"
3239612,"Regarding ""An Elementary Problem Equivalent to the Riemann Hypothesis""","Let $H_{n}$ be the $n$ th harmonic number. 
In Lagarias's paper ""An Elementary Problem Equivalent to the Riemann Hypothesis,"" he shows that the statement $$\sum_{d\mid n}d\leq H_{n}+\exp(H_{n})\log(H_{n})\tag{1.1}$$ for every positive integer $n$ , with equality if and only if $n=1$ , is equivalent to the Riemann hypothesis. In the last paragraph of section $2$ , he says, One can prove unconditionally that inequality $(1.1)$ holds for nearly all integers.  Even if the Riemann hypothesis is false, the exceptions to $(1.1)$ will form a very sparse set.  Furthermore, if there exists any counterexample to $(1.1)$ the value of $n$ will be very large. So far as I can see, none of these statements is justified, or even suggested,  by anything in the paper, though it may just be too subtle for me, of course. Can anyone give me a reference justifying these statements, and quantifying the terms ""nearly all integers"", ""very sparse set"", ""very large""?  I assume that the first two mean ""density $1$ "" and ""density $0$ "" for some definition of density, but the last seems to indicate that the statement is known to be true below a specific bound, and it seems odd that the bound isn't mentioned.","['number-theory', 'reference-request']"
3239635,On matrices with zero von Neumann entropy,"I was indecisive about whether to post this problem in the Physics forum or in the Mathematics one. However, since I am mostly interested in the mathematical understanding of it, I am posting it here. Suppose I have a matrix $A$ subject to the conditions (for its trace): $$\mbox{Tr}(A) = 1$$ $$A^2 = A$$ $$A \succeq 0 $$ $$A^H = A$$ Now from this conditions I need to prove the following: $$-\mbox{Tr} (A \ln{A}) = 0$$ In physics, $A$ can be seen as a pure density matrix and $-\mbox{Tr}(A\ln{A})$ is the von Neumann entropy. How does that follow from the first four conditions?","['trace', 'idempotents', 'matrices', 'linear-algebra', 'projection-matrices']"
3239650,Homotopy equivalence upper triangular matrices and torus,"In an old algebraic topology exam, I came across this question. Let $G$ be the set of invertible upper triangular matrices in $\mathbf{C}^{2\times 2}$ , as a topological subspace of $\mathbf{C}^3\cong \mathbf{R}^6$ . (a) Prove that $G$ is homotopy equivalent to the torus. (b) Determine the push forward $\det_*:\pi_1(G,\mathbf{1})\longrightarrow \pi_1(\mathbf{C}^*,1)$ of the determinant map $\det:G\longrightarrow \mathbf{C}^*:A\longmapsto \det A$ . (a) Since $G$ consists of all matrices $\begin{bmatrix}a&b\\0 & c\end{bmatrix}\in\mathbf{C}^{2\times 2}$ with $a,c\neq 0$ , it can be identified with $\mathbf{C}^*\times\mathbf{C}\times\mathbf{C}^*$ . This can be used to prove that $G$ is path connected. Identifying the torus $T$ with $S^1\times S^1$ , we need to find continuous $f:G\to T,g:T\to G$ such that $f\circ g\simeq\text{id}_T$ and $g\circ f\simeq \text{id}_X$ . We let $$f\left( \begin{bmatrix}a&b\\0&c\end{bmatrix}\right)=(\operatorname{arg}a,\operatorname{arg}b),\quad g(\theta,\phi)=\begin{bmatrix}\cos\theta+i\sin\theta & 1 \\ 0 & \cos\phi+i\sin\phi \end{bmatrix}.$$ where $\operatorname{arg}$ sends a complex number to its argument in $[0,2\pi)$ . Then $f\circ g=\text{id}_T$ and $g\circ f:G\to G:\begin{bmatrix}a&b \\ 0 & c\end{bmatrix}\mapsto \begin{bmatrix} a/||a|| & b/||b|| \\ 0 & c/||c|| . \end{bmatrix}$ . Since $G$ is path connected, $g\circ f\simeq \text{id}_G$ . Is this correct? (b) I know that the fundamental group of the product space $\mathbf{C}^*\times\mathbf{C}\times\mathbf{C}^*$ is the direct product of the fundamental groups, thus $\pi_1(G,\mathbf{1})=\mathbf{Z}\times\mathbf{Z}$ . (I think the homotopy equivalence cannot be showed by simply noting that the fundamental group of the torus is also $\mathbf{Z}\times\mathbf{Z}$ .) This has two generators $(1,0)$ and $(0,1)$ . We can represent (is this correct?) these on the level of the fundamental group by the loops $$\gamma_0:I\to G:t\mapsto \begin{bmatrix}e^{it} & 0 \\ 0 & 1\end{bmatrix},\quad \gamma_1:I\to G:s\mapsto \begin{bmatrix}1 & 0 \\ 0 & e^{is}\end{bmatrix},$$ whose images under $\det_*$ are respectievely $I\to \mathbf{C}^*:t\mapsto e^{it}$ and $I\to \mathbf{C}^*:s\mapsto e^{is}$ . This means that the push forward morphism is given by $\mathbf{Z}\times\mathbf{Z}\longrightarrow \mathbf{Z}:\begin{cases}(1,0)\longmapsto 1 \\ (0,1) \longmapsto 1 \end{cases}$ . Is this correct?","['general-topology', 'proof-verification', 'algebraic-topology']"
3239671,Find a suitable ARMA model,I know that the ACF and PACF shown below is either of a MA(2) or AR(2) process. How can I decide which one it is by just looking at the plots?,"['time-series', 'statistics']"
3239759,De Rham cohomology of compact manifold minus one point,"Let $M$ be a compact, connected, oriented and without boundary $n$ -manifold, and $N$ the manifold obtained by removing an arbitrary point $p\in M$ from $M.$ Show that the Betti numbers (i.e. the dimentions of the De Rham cohomology groups) are $$b^i(N)=b^i(M) \quad i=0,\dots,n-1;$$ $$b^n(N)=b^n(M)-1.$$ My idea was to use the Mayer-Vietoris theorem by considering the open sets $U=N$ and $V,$ where $V$ is an arbitrary neighbourhood that is also the domain of a chart $\varphi:V\to\mathbb{R}^n.$ The resulting Mayer-Vietoris sequence is: \begin{multline}
0\to H^0(M)\to H^0(N)\oplus\mathbb{R}\to\mathbb{R}\to H^1(M)\to H^1(N)\to 0\to \\ \to \cdots\to 0 \to H^{n-1}(M)\to H^{n-1}(N)\to\mathbb{R}\to H^n(M)\to H^n(N)\to 0;\end{multline} due to the fact that the cohomology of $V\sim\mathbb{R}^n$ is $H^0(\mathbb{R}^n)=\mathbb{R}$ and $H^i(\mathbb{R}^n)=0$ for all $i\geq 1,$ and the cohomology of $V\cap U$ is the cohomology of $\mathbb{S}^{n-1},$ that is: $H^0(\mathbb{S}^{n-1})=H^{n-1}(\mathbb{S}^{n-1})=\mathbb{R}$ and $H^i(\mathbb{S}^{n-1})=0$ for all $i\ne0,n-1.$ The short sequences in the middle $0 \to H^i(M)\to H^i(N)\to 0, \ i=2,\dots,n-2$ let me say that $b^i(N)=b^i(M)$ for these $i$ 's, but for the others I just have the sequences: $$0\to H^0(M)\to H^0(N)\oplus\mathbb{R}\to\mathbb{R}\to H^1(M)\to H^1(N)\to 0,$$ $$0 \to H^{n-1}(M)\to H^{n-1}(N)\to\mathbb{R}\to H^n(M)\to H^n(N)\to 0;$$ that allow me to write the relations: $$b^0(M)-b^0(N)-b^1(M)+b^1(N)=0,$$ and $$b^{n-1}(M)-b^{n-1}(N)+1-b^n(M)+b^n(N)=0,$$ which are coherent with the thesis that I want to obtain but are not sufficient to conclude, and I can't say anything more (I'm not an expert in using arrows as mathematical objects). 
Any help is really much appreciated. Thanks to everybody.","['manifolds', 'homology-cohomology', 'de-rham-cohomology', 'differential-geometry']"
3239808,Noncommutative Ergodic Theorems,"I am looking for books presenting the noncommutative version of ergodic theorems. The only book I have found is Krengel, Ergodic Theorems, 1985. Are there other references other than Krengel's book ?","['von-neumann-algebras', 'operator-algebras', 'ergodic-theory', 'reference-request', 'functional-analysis']"
3239851,"If $\sum a_n$ converges , then $a_n<1/n$ a.e?","If $\sum_{n=1}^{\infty}a_n<\infty$ is a Positive convergent series, does the following limit hold? $$\lim_{n\to\infty}\frac{\mathrm{Card}\{1\leq k\leq n , a_k\geq\frac{1}{k}\}}{n}=0$$ I know of a non-trival example to support this: $$a_n=1/n ,\mathrm{if} \sqrt{n}\in\mathbb{N},a_n=1/n^2. $$ For those who don't know, the Card of $\{1\leq k\leq n , a_k\geq\frac{1}{k}\}$ is $O(\sqrt{n}).$",['sequences-and-series']
3239858,"If $G$ is a non-abelian finite group, then $|Z(G)| \leq \frac {1}{4} |G|$","I know this is question has been asked  several times on here, only hints given ,but just want to check if I have the right idea. My attempt:
Suppose G is non abelian finite group and $|Z(G)| \gt \frac {1}{4} |G|$ .
Since Z(G) is a subgroup of G then it’s order must divide that of G , i.e it could be of size |G|/3 , |G|/2 or |G|  .That  means the quotient group G/Z has order 1 ,2 or 3 . If equal to 1 that implies Z(G)=G so abelian , and also since 2 and 3 are prime numbers that means G/Z is cyclic which also implies it’s abelian. Which leads to a contradiction.
Is that right? Thanks","['quotient-group', 'group-theory', 'abstract-algebra', 'proof-verification']"
3239860,Find all roots of the equation :$(1+\frac{ix}n)^n = (1-\frac{ix}n)^n$,"This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. If $n$ is a positive integer, find all roots of the equation : $$(1+\frac{ix}n)^n = (1-\frac{ix}n)^n$$ The binomial expansion on each side will lead to: $$(n.1^n+C(n, 1).1^{n-1}.\frac{ix}n + C(n, 2).1^{n-2}.(\frac{ix}n)^2 + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (n.1^n+C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 2).1^{n-2}.(\frac{-ix}n)^2 + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots )$$ $n$ can be odd or even, but the terms on l.h.s. & r.h.s. cancel for even $n$ as power of $\frac{ix}n$ . Anyway, the first terms cancel each other. $$(C(n, 1).1^{n-1}.\frac{ix}n + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots )$$ As the term $(1)^{n-i}$ for $i \in \{1,2,\cdots\}$ don't matter in products terms, so ignore them: $$(C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = (C(n, 1).\frac{-ix}n + C(n, 3).(\frac{-ix}n)^3+\cdots )$$ $$2(C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = 0$$ $$C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots  = 0$$ Unable to pursue further.",['real-analysis']
3239862,"Is it true that $(A\times A)-(B\times B) = (A - B)\times (A -B)$ for any two sets A,B? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I don't know how to prove set inclusions with Cartesian products, can someone explain how?",['elementary-set-theory']
3239871,Products of trig functions and the Thue–Morse sequence,"I was studying transformations of finite products of trig functions into sums, and empirically observed that the following curious identity appears to hold for all non-negative integer $m$ : $$\prod_{n=0}^m \sin\left(\frac z{2^n}\right)=\frac1{2^m}\sum _{n=0}^{2^m-1} (-1)^{\large t_n} \sin\left(\frac{\pi\,m}2+\frac{2\,n+1}{2^m}\,z\right),\tag{$\diamond$}$$ where $t_n$ is the Thue–Morse sequence. $^{[1]}$ $\!^{[2]}$ $\!^{[3]}$ How can we prove this identity?","['products', 'trigonometry', 'summation', 'sequences-and-series']"
3239877,Prove that the group of moves of the Rubik’s cube is not abelian.,"I'm currently working in the following excercise: Remember that $G$ is the group of moves of the Rubik’s cube. Prove that this group is not abelian. I'm starting from picking two moves $M_1$ and $M_2$ and I'm looking at their commutator $[M1, M2]$ , which is defined to be $M_1M_2M_1^{−1}M_2^{-1}$ , but I'm not sure this is the way to proceed and which steps to go forward with the proof. Thanks in advance for any hint or help and for taking the time to read my question.","['group-theory', 'abstract-algebra', 'discrete-mathematics', 'rubiks-cube']"
3239889,When is a quotient group $G/H$ abelian?,"So clearly if $G$ is abelian and $H$ is a normal subgroup of $G$ then $G/H$ is abelian since $$xH.yH =(xy)H=(yx)H=yH.xH$$ But is there cases when this quotient group is abelian without the group G being abelian? What I came up with is that for $(xy)H=(yx)H$ to be true then $(xy)^{-1}(yx)\in H$ must be satisfied, for all $x,y \in G$ . Is this correct ? And any examples to this?
Thanks :-)","['group-theory', 'quotient-group']"
3239926,Approximate solution: factorial and exponentials,"If z= $\dbinom{200}{100}/(4^{100})$ , what is the value of z? The options are: a. $z<1/3$ b. $1/3<z<1/2$ c. $1/2<z<2/3$ d. $2/3<z<1$ How should I go about solving these type of problems?","['number-theory', 'combinations', 'exponentiation']"
3239937,Can we derive the set $A-B$ using only unions and intersections and without addition and substraction?,I was just wondering if it is possible to derive the set $A-B$ using just union and intersection. I've tried all different ways and even tried adding a new set C and tried with that but didn't work out. I am questioning is it even possible?,['elementary-set-theory']
3239987,positive root of the equation $x^2+x-3-\sqrt{3}=0$,$x^2+x-3-\sqrt{3}=0$ using the quadratic formula we get $$x=\frac{-1+\sqrt{13+4\sqrt{3}}}{2}$$ for the positive root but the actual answer is simply $x=\sqrt3$ I am unable to perform the simplification any help would we helpful,"['algebra-precalculus', 'quadratics']"
3240017,Relationship between singular values of $A$ and eigenvalues of $B:= \begin{bmatrix} 0 & A \\ A^\ast & 0 \end{bmatrix}$,"Let $$B:= \begin{bmatrix} O_m & A \\ A^\ast & O_n \end{bmatrix}$$ where $A$ is an $m \times n$ matrix. Find the relationship between the two: Singular values and singular vectors of $A$ . Eigenvalues and eigenvectors of $B$ . Try Let $|\lambda_1| \ge \cdots \ge |\lambda_{m+n}|$ and $x_1, \cdots, x_{m+n}$ are eigenvalues and corresponding eigenvectors of $B$ . (Note that eigenvalues are real, since $B$ is Hermitian. Let $\eta_1 \ge \cdots \ge \eta_{k} \ge 0$ and $u_1, \cdots, u_m$ and $v_1, \cdots, v_n$ be singular values and corresponding left- and right- singular vectors of $A$ , with $k := \min\{m,n\}$ . Note that $B^2 = B^\ast B= \begin{bmatrix} A A^\ast & 0 \\ 0 & A^\ast A \end{bmatrix}$ thus $B^2 x_i = B \lambda_i x_i  = \lambda_i^2 x_i, \forall  i$ . Therefore, $$
B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix}
$$ where $x_i := [\left(x_i^{(1)}\right)^T_m | \left(x_i^{(2)} \right)^T_n]^T_{m+n}$ . I have noticed that $x_i^{(2)}$ , $\lambda_i^2$ are eigenpairs of $A^\ast A$ , thus by the definition of singular value , $|\lambda_i|$ are the singular values of $A$ . By the way, from SVD of $A = U \Sigma V^\ast$ , we have $$
A^\ast A V = V\Sigma^2_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt]
A A^\ast U = U\Sigma^2_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k
$$ Question Since $B$ has $(m+n)$ different(possibly same) $\lambda_i$ (i.e. $i = 1, \cdots, m+n)$ , from $$
B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix}
$$ we have $(m+n)$ formulas for $x_i^{(1)}$ , $x_i^{(2)}$ , i.e. $$
AA^\ast x_i^{(1)}  =  \lambda_i^2 x_i^{(1)} \\
A^\ast A x_i^{(2)}  =  \lambda_i^2 x_i^{(2)} \\
$$ for $i = 1,\cdots, m+n$ , but for $$
A^\ast A V = V(\Sigma^T\Sigma)_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt]
A A^\ast U = U(\Sigma\Sigma^T)_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k
$$ we only have $k$ formulas. So I'm stuck at specifying the relationship. Is there anyone to help solving the problem?","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'block-matrices', 'singular-values']"
3240027,Prove identity $\sum^n_{k = 0} \binom {r+k} {r} = \binom{r+n+1} {r+1}$ using lattice paths,"I am trying to prove the following identity $\sum^n_{k = 0} \binom {r+k} {r} = \binom{r+n+1} {r+1}$ by using lattice paths. My first approach was to draw the following scheme: Sketch indicating paths I'm aware that $\binom{r+n+1} {r+1}$ counts the number of shortest paths from $(0,0)$ to $(n, r+1)$ and I also know that $\binom{r+k} {k}$ counts paths from the origin to $(k,r)$ where $0 \le k\le n$ . I think I can deal with paths like the blue one shown in the Sketch, because it intercepts the green line on only one point. But I'm stuck when it comes to paths like the red one, I do not know how to count them in a proper way. Any insight would be very appreciated!","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
3240048,How to compute identity from symmetric difference? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question We know that $(P(X), \Delta) $ forms a group, where $P(X) $ is the power set on the non empty set $X$ and $A\Delta B=(A-B) \cup (B-A)$ for all $A, B\in P(X) $ . Clearly $\emptyset$ is the identity element. How can i compute $E$ from the relation $Y\Delta E=Y$ for all $Y\in P(X) $ .","['elementary-set-theory', 'group-theory', 'abelian-groups']"
3240059,integration of double integral with euler number include it.,Hello i have a hard time with integration of double integral with euler number with power included in it. I have $$\int_0^1\int_0^2x^2ye^{xy}dxdy$$ so i am trying to use substitution for solving it. as follows i say $u = y$ $du = 1$ $v=ye^{xy}$ i am not sure for $dv$ here is it $e^{xy}$ or it is $\frac{e^{xy}}{x}$ ? Then i use the $Integration$ $by$ $parts$ $formula$ $$\int_a^budv=uv|_a^b-\int_a^bvdu$$ so from here i have $$\int_0^1x^2ye^{xy}|_0^2-\int_0^2x^2\frac{e^{xy}}{x}$$ a first thing i notice here i am left with 2 integrals one with respect to $dx$ and another with respect to $dydx$ that is confusing me in the first place so i am continue to integrate now i think i don't need $u$ $substitution$ to finish the integral with respect to $dy$ it must be : $$\int_0^1(2e^{2x}|_0^2-e^{2x}+e^0 )dx$$ From here i must do another $u$ $sub$ for x this time. So i am having many steps that i am not sure that are correct if someone can help me to understand whats happening here. Thank you in advance.,"['integration', 'multivariable-calculus', 'calculus']"
3240060,"Show that the $n$-th Fibonacci number is given by $\frac{\cosh na}{\cosh a}$ or $\frac{\sinh na}{\cosh a}$, where $\sinh a=1/2$","This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. Show that the general term of Fibonacci sequence $1,1,2,3,5,\cdots$ , is given by : $f_n = \frac{\cosh\, n\alpha}{\cosh\, \alpha}$ ( $n$ odd), $f_n = \frac{\sinh\, n\alpha}{\cosh\, \alpha}$ ( $n$ even), where $\sinh \alpha= \frac 12$ , and that $\lim \frac{f_{n+1}}{f_n} = e^\alpha.$ As given here : The hyperbolic functions $\sinh$ , $\cosh $ are given by: $$\sinh\,z = \frac{e^z - e^{-z}}{2}, \cosh\,z = \frac{e^z + e^{-z}}{2}$$ where $\,z= x+iy\,\,$ is a complex variable. These functions satisfy the identities: $$\cosh^2\,z-\sinh^2\,z = 1,\,\, \cosh\,iy = \cosh\,y,\,\, \sinh\,iy = i\,\sinh\,y$$ If $\sinh\,z = \frac{e^z - e^{-z}}{2} = \frac 12$ , then $\cosh^2\,z-\sinh^2\,z = 1\,\,$ gives $\cosh^2\,z=\frac 54$ This yields nothing, so consider opposite approach of taking help by another means of generating Fibonacci sequence terms. Say, the polynomial $f(x) = x+1$ yields terms for successive values for $x \in \mathbb{N}$ as $2,3,\dots$ . So, took help from paper here that concerns with fibonacci polynomials, with text portion from page 1 copied below: The Fibonacci polynomials $\{F_n (x)\}$ are defined by (1.1) $F_1(x) = 1, F_2(x) = x$ , and $F_{n+1}(x) = xF_n(x) + F_{n-1}x$ . Notice that, when $x = 1, F_n(1) = F_n$ , the $n$ Fibonacci number. It is easy
to verify that the relation (1.2) $F_{-n}(x) = (-1)^{n+1} F_n(x)$ extends the definition of Fibonacci polynomials to all integral subscripts. But, this also doesn't help.","['fibonacci-numbers', 'hyperbolic-functions', 'recurrence-relations', 'real-analysis', 'sequences-and-series']"
3240066,Grassmannian as a quotient of orthogonal or general linear group,"I'm trying to understand some different ways to construct the Grassmannian of a real vector space, but I'm having trouble getting some of the notation and definitions. One definition that I often see given, such as on Wikipedia , is $$Gr(r,n) = O(n)/(O(r) \times O(n–r))$$ where $Gr(r,n)$ is the Grassmannian of $r$ -dimensional subspaces of $\Bbb R^n$ . My first question is: what does this quotient notation mean? $O(n)$ is the group of $n\times n$ orthogonal matrices, whereas $O(r)$ and $O(n-r)$ are the groups of $r \times r$ and $(n-r) \times (n-r)$ orthogonal matrices. How are we viewing the cartesian product of $O(r) \times O(n-r)$ as a group of $n \times n$ matrices? A different way to construct this is also given by Wikipedia: First, recall that the general linear group GL(V) acts transitively on
  the r-dimensional subspaces of V. Therefore, if H is the stabilizer of
  any of the subspaces under this action, we have $$Gr(r,V) = GL(V)/H$$ This is also somewhat puzzling to me. You start with the general linear group of invertible $r \times r$ matrices, which acts transitively on subspaces, meaning for any two subspaces $a$ and $b$ there is some element of $GL(V)$ that maps $a$ to $b$ . But then we quotient by what stabilizer, exactly, to construct the Grassmannian as a quotient space? The only stabilizer that preserves all subspaces are simple scalar transformations; if we mod by those we get the projective linear group $PGL(V)$ , not the Grassmannian. If on the other hand, they mean that we quotient by all such $H$ that are a stabilizer for any subspace, this means that we are quotienting by every element of $GL(V)$ that has any real eigenspace, which is almost the entire group, except perhaps for rotation matrices in even dimensions, where all eigenvalues are complex. How is this thing supposed to be constructed? And how do these yield any type of metric structure?","['grassmannian', 'algebraic-geometry', 'linear-algebra', 'quotient-spaces', 'group-theory']"
3240069,"Example 2, Sec. 31, in Munkres' TOPOLOGY, 2nd ed: Normality of $\mathbb{R}_l$ --- Why are these two sets disjoint?","The set $\mathbb{R}$ of real numbers with the lower limit topology having as a basis the collection of all closed-open intervals $[a, b)$ , where $a, b \in \mathbb{R}$ with $a < b$ ,  is denoted by $\mathbb{R}_l$ . Here is Example 2, Sec. 31, in the book Topology by James R. Munrkres, 2nd edition: The space $\mathbb{R}_l$ is normal. It is immediate that one-point sets are closed in $\mathbb{R}_l$ , since the topology of $\mathbb{R}_l$ is finer than that of $\mathbb{R}$ . To check normality, suppose that $A$ and $B$ are disjoint closed sets in $\mathbb{R}_l$ . For each point $a$ of $A$ choose a basis element $\left[ a, x_a \right)$ not intersecting $B$ ; and for each point $b$ of $B$ choose a basis element $\left[ b, x_b \right)$ not intersecting $A$ . The open sets $$ U = \bigcup_{a \in A} \left[ a, x_a \right) \qquad \mbox{ and } \qquad  V = \bigcup_{b \in B} \left[ b, x_b \right)  $$ are disjoint open sets about $A$ and $B$ , respectively. In the above proof, how do we know that the sets $U$ and $V$ are indeed disjoint? My Attempt: Suppose that $U$ and $V$ are not disjoint. Let $p$ be a point of $U \cap V$ . Then there are some points $a \in A$ and $b \in B$ such that $$ p \in \left[a , x_a \right) \qquad \mbox{ and } \qquad p \in \left[ b, x_b \right). $$ But as the interval $\left[a , x_a \right)$ does not intersect $B$ and as the interval $\left[ b, x_b \right)$ does not intersect $A$ , so we must also have $$ p \in \left(a , x_a \right) \qquad \mbox{ and } \qquad p \in \left( b, x_b \right). \tag{1} $$ What next? How to proceed from here to arrive at our desired contradiction? Or, is there some alternative way of showing these sets $U$ and $V$ to be disjoint? PS: I think I've now managed to figure this out. Since the interval $\left[a , x_a \right)$ does not intersect $B$ , the interval $\left[ b, x_b \right)$ does not intersect $A$ , $a \in A$ , and $b \in B$ , therefore we must have the following:
Either $x_a \leq b$ or $x_b \leq a$ . If $x_a \leq b$ , then we have $$
a < x_a \leq b < x_b.
$$ On the other hand, if $x_b \leq a$ , then we have $$
b < x_b \leq a < x_a. 
$$ In either case, the intervals $\left[a , x_a \right)$ and $\left[ b, x_b \right)$ will turn out to be disjoint, contrary to (1) above. Is this reasoning correct? If so, then is it also free-of-gaps? Or, are there any other ways of correcting / improving it?","['sorgenfrey-line', 'general-topology', 'separation-axioms']"
3240074,Number of onto functions from $Y$ to $X$ (JEE Advanced 2018),"Let $X$ be a set with $5$ elements and $Y$ be a set with $7$ elements. If $\beta$ is the number of onto functions from $Y$ to $X$ then the value of $\dfrac{\beta}{5!}$ is? My approach is: First I give each element of $Y$ one element of $X$ which then leaves me with two possibilities for the remaining two elements of $Y$ . 1) Both can take same value 2) Both take different value from X. So number of onto functions = $\beta = {^7}C_5\times 5! \times(5+ 5\times4)$ but this is wrong. What's my mistake? Also, I know how to solve the problem using principle of inclusion exclusion and that gives the right answer but we are not given calculators in exam and the calculation involved there is very lengthy.","['functions', 'combinatorics']"
3240085,"Compute without calculator, $ \frac{1}{\cos^{2}(10)} + \frac{1}{\sin^{2}(20)} + \frac{1}{\sin^{2}(40)} - \frac{1}{\cos^{2}(45)} $","Compute without calculator, $$ \frac{1}{\cos^{2}(10^{\circ})} +  \frac{1}{\sin^{2}(20^{\circ})} +   \frac{1}{\sin^{2}(40^{\circ})} -   \frac{1}{\cos^{2}(45^{\circ})} $$ Attempt: Let $A = \cos(10) \sin(20) \sin(40)$ , then $$ \frac{1}{\cos^{2}(10)} +  \frac{1}{\sin^{2}(20)} +   \frac{1}{\sin^{2}(40)} = \frac{\sin^{2}(20) \sin^{2}(40) + \cos^{2}(10) \sin^{2}(40) + \cos^{2}(10) \sin^{2}(20)}{A^{2}} $$ notice also $$2A \sin(10) \sin(40) =  \sin^{2}(20) \sin^{2}(40) $$ $$2A \cos(20) \cos(10) =  \cos^{2}(10) \sin^{2}(40) $$ so we have $$\frac{2A \sin(10) \sin(40) + 2A \cos(20) \cos(10)+ \cos^{2}(10) \sin^{2}(20)}{A^{2}} $$ $$ = \frac{2A \left[2 \sin(10) \sin(20) \cos(20) + \frac{\sqrt{3}}{2} + \sin(10)\sin(20) \right]+ \cos^{2}(10) \sin^{2}(20)}{A^{2}} $$ $$ = \frac{2A \left[\sin(10) \sin(20) (1+ \cos(20) + \cos(20)) + \frac{\sqrt{3}}{2}   \right]+ \cos^{2}(10) \sin^{2}(20)}{A^{2}} $$ $$ = \frac{2A \left[\sin(10) \sin(20) (3\cos^{2}(10) - \sin^{2}(10)) + \frac{\sqrt{3}}{2}   \right]+ \cos^{2}(10) \sin^{2}(20)}{A^{2}} $$ How to continue then?","['contest-math', 'algebra-precalculus', 'trigonometry']"
3240099,Calculate limit in use of integrals,"Calculate limit in use of integrals $$ \lim_{n \rightarrow \infty} \sum_{k=1}^{n} \frac{1+n}{3k^2+n^2} $$ My attempt: $$\sum_{k=1}^{n} \frac{1+n}{3k^2+n^2} = \frac{1}{n} \sum_{k=1}^{n} \frac{\frac{1}{n}+1}{3(k/n)^2+1} = \\
\frac{1}{n}\cdot (1/n + 1) \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}  $$ Ok, I know that when I am taking limit I should replace (from aproximation theorem) $$ \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}$$ with $$ \int_{0}^{1} \frac{1}{1+3x^2}$$ but I still don't know what have I do (and why) with $$ \frac{1}{n}\cdot (1/n + 1) $$ part. In many solutions we just ignore part $\frac{1}{n}$ but I don't know why and there where I have little more 'difficult' expression like $ \frac{1}{n}\cdot (1/n + 1) $ I completely don't know what should I do... $$  $$","['integration', 'limits', 'summation']"
3240113,"What is the relationship between propositional calculus, set theory, and Boolean algebra?","The connective $∧$ (conjunction) in propositional logic is essentially the same as ∩ (intersection) in set theory if one thinks of 'false' as 'not a member' and 'true' as 'a member'. De Morgan's laws, commutative laws, idempotent laws, etc apply in three of them. Truth tables exist in both propositional logic and Boolean algebra--membership tables exist in set theory. Why? Is it because the variables in these systems are binary (true-false, in the set-not in the set, $0$ - $1$ )and the relationship between variables that have only two possible values given the defined operators is the same regardless of what the two choices are? If I prove something about an expression in propositional logic, is it valid to conclude that the same holds for its corresponding Boolean expression? Is Karnaugh map valid for simplifying logical expressions too? Do these systems share axioms?","['elementary-set-theory', 'boolean-algebra', 'propositional-calculus']"
3240121,What's the number of natural solutions of $x_1 + 2x_2 + 3x_3 = n$?,"$$x_1 + 2x_2 + 3x_3 = n, \qquad x_1, x_2, x_3 \geq 0$$ Find a regression formula (or a recursive function, not sure how it's called in English) to calculate the number of solutions for all $n≥0$ . Find the number of solution for $n=7$ . So far I only got the following generating function $$f(x) = \left( \sum_{i=0}^\infty x^i \right) \left( \sum_{i=0}^\infty x^{2i} \right) \left( \sum_{i=0}^\infty x^{3i} \right)$$","['combinatorics', 'discrete-mathematics', 'generating-functions']"
3240137,"About the term $-\nabla_{[u,v]}w$ in the definition of Riemann curvature tensor","As we know, in the definition of Riemann curvature tensor, we require $$
R(u,v)w=\nabla_u\nabla_v w-\nabla_v\nabla_u w-\nabla_{[u,v]}w
$$ Could somebody tell me why we need $-\nabla_{[u,v]}w$ appearing in this definition. Is there any geometric meaning in it? Because the geometric meaning of $\nabla_u\nabla_v w-\nabla_v\nabla_u w$ is pretty clear, but for $-\nabla_{[u,v]}w$ I feel not so direct. In my opinion, because $[u,v]=\cal L_u v$, then $-\nabla_{[u,v]}w=-\nabla_{\cal L_u v}w$. So it looks like that this term is kind of correction of $\nabla_u\nabla_v w-\nabla_v\nabla_u w$, I mean, to neutralize the effects of the vector fields. This is a very naive hunch. Could you give me a more clear answer?","['differential-topology', 'general-relativity', 'geometry', 'differential-geometry']"
3240142,"From $\|g\|_2 =1$ to $\|g\|_\infty^2 \ge \dim V$ on a subspace of $C[0,1]$","Let $V$ be a finite dimensional subspace of $C[0,1]$ . Prove that there exists $g \in V$ such that $\|g\|_2 =1$ , $\|g\|_\infty^2 \ge \dim V$ In this problem we use the notation $$\|f\|_2 = \left(\int_0^1 f^2\right)^{1/2}$$ $$\|f\|_\infty = \max_{[0,1]} f$$ My attempt For the n-dimensional situation.
Suppose that $f_i=\chi_{[\frac{i-1}{n}, \frac{i}{n}]}$ are the characteristic functions of intervals. Then the condition is equivalent to that for $f=\sum_{i=1}^n \lambda_i f_i$ $$\sum_{i=1}^n \lambda_i^2 = n^2$$ Thus $$\|f\|_\infty^2 = \max \lambda_i^2 \geqslant n$$ The inequality holds. But I got stuck on analyzing the general situation. Could you please give me some hints? Thanks in advance!","['banach-spaces', 'functional-analysis', 'real-analysis']"
3240203,"Prove that $\forall \epsilon > 0$: $\lim_{t\to\infty}t^{-2}\int_{0}^{t}[(f(x))^{1+\epsilon}/f'(x)]\,\mathrm dx =+\infty$","Let $f: [0,+\infty) \to [0,+\infty)$ be differentiable, $f' > 0$ . Prove that $$\forall \epsilon > 0: \lim_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =+\infty$$ I used L'Hôpital's rule and got $$\lim\limits_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =\dfrac{1}{2}\displaystyle\lim_{t\to\infty}\dfrac{\left(f(t)\right)^{1+\epsilon}}{tf'(t)}$$ If $\lim\limits_{t\to\infty}f'(t)$ exists, then we can prove the above statement using L'Hôpital's rule. But if $\lim\limits_{t\to\infty}f'(t)$ does not exist, I don't know how to proceed","['analysis', 'real-analysis']"
3240208,"ODE $f:\text{<0 if $tx > 0$},\text{>0 if $tx<0$}$; show that $x(t)\equiv0$ is the only solution to $\dot{x}=f(t,x)\hspace{0,3cm}, x(0)=0$","Let $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ continuous fulfilling $$  \begin{cases}
    f(t,x)<0, & \text{if $xt>0$}.\\
    f(t,x)>0, & \text{if $xt<0$}.
  \end{cases}
$$ Show that, $x(t)\equiv0$ is the only solution to the initial value problem $$\dot{x}=f(t,x)\hspace{1cm}  x(0)=0$$ So obviously $x(t)\equiv0$ solves the problem on $\mathbb{R}^2$ in order to show that it's the only solution I would try to use the Picard-Lindelöf theorem. In order to be able to use it I would furthermore have to prove that $f$ is locally Lipschitz in $x$ . But how can I do this ? I know of this theorem that says that if $f$ is locally differentiable w.r.t $x$ it's locally Lipschitz. Would that help ?","['calculus', 'proof-writing', 'ordinary-differential-equations']"
3240253,Tractability of Expectations,"I'm working my way through a paper about bounds on the mutual information [1].
However, I have some issues in understanding claims they make about the tractability of the different bounds. Given: $
q ( x | y ) = \frac { p ( x ) } { Z ( y ) } e ^ { f ( x , y ) } , \text { where } Z ( y ) = \mathbb { E } _ { p ( x ) } \left[ e ^ { f ( x , y ) } \right],
$ they derive the Donsker-Varadhan bound $I _ { \mathrm { DV } }$ on the Mutual Information: $
\mathbb { E } _ { p ( x , y ) } [ f ( x , y ) ] - \log \mathbb { E } _ { p ( y ) } [ Z ( y ) ] \triangleq I _ { \mathrm { DV } }.
$ And claim that the bound is intractable. Then they state that tractability can be achieved by applying Jensen's inequality. Whereby they replace $log Z(y) = \log \mathbb { E } _ { p ( x ) } \left[ e ^ { f ( x , y ) } \right]$ with $\mathbb { E } _ { p ( x ) } [ f ( x , y ) ]$ . Which results in: $
\mathbb { E } _ { p ( x , y ) } [ f ( x , y ) ] - \mathbb { E } _ { p ( y ) } [f(x,y) ] .
$ So due to my understanding the main difference between the tractable and untractable bound is the absence of the exponential inside of $\mathbb { E } _ { p ( x ) } $ . Why is this now considered as tractable?
Is it because for calculating the expectation we need to sum over $e ^ { f ( x , y )}$ which results in an ""intractable"" high number, compared to summing just over $ { f ( x , y )}$ ? Similarly $\begin{array} { l } { \mathbb { E } _ { p ( x , y ) } [ f ( x , y ) ] }   - \mathbb { E } _ { p ( y ) } \left[ \frac { \mathbb { E } _ { p ( x ) } \left[ e ^ { f ( x , y ) } \right] } { a ( y ) } + \log ( a ( y ) ) - 1 \right]  \end{array},$ is introduced as an tractable bound for any choice of $a(y)>0$ .
What makes this tractable compared to $I_{DV}$ ? [1] https://arxiv.org/abs/1905.06922 (relevant parts in section 2.2)","['machine-learning', 'inequality', 'probability-theory']"
3240286,Singular solution of $y^2(y - xp) = x^4p^2$,"Given differential equation, $y^2(y - xp) = x^4p^2$ where { $p = dy/dx$ } To find the singular solution, I have extracted the p-discriminant relation which is, $y^3x^2(y + 4x^2) = 0$ From here it is evident that all $x = 0, y = 0$ and $y + 4x^2 = 0$ are the singular solutions when tested by putting back in the differential equation but my text book doesn't mention anything about $x=0$ as singular solution. Am I doing any mistake?","['singular-solution', 'ordinary-differential-equations']"
3240308,Understanding the Proof about the Uniqueness of $d$ operator,"Munkres defines $d$ -- the generalized differential operator -- by showing it characterized by the following properties: Let $A \subset \mathbb{R}^n$ open and $\Omega^k(A)$ be the linear space of $C^\infty$ k-forms on $A$ . $d$ is the unique linear transformation such that: $$d:\Omega^k(A) \rightarrow \Omega^{k+1}(A)$$ defined for $k \geq 0$ , such that: If $f$ is a 0-form, then $df$ is the 1-form $$df(x)(x;v) = Df(x) \cdot v$$ If $\omega$ nad $\nu$ are forms of orders $k$ and $l$ , resp, then $$d(\omega \wedge \nu) = d\omega \wedge \nu + (-1)^k\omega \wedge d\nu$$ For every form $\omega$ $$d(d\omega)=0$$ Below I outline his proof. My question: why is this work sufficient to show the claim? I'm having trouble following his logic. I provide more detail below. Proof Steps Sketch: Verifies uniqueness in two steps. First he shows that conditions two and three imply that for any forms $\omega_1,...\omega_k$ we have: $$d(d\omega_1\wedge ...\wedge d\omega_k)=0$$ then he shows that any $k$ -form is determined by the value of $d$ on $0$ -forms. To complete this second task he says: ""Since $d$ is linear, it suffices to consider the case $\omega = f dx_I$ "" then does a computation to show $d\omega = df \wedge dx_I$ . Why is he checking all this stuff? The only way I know to check uniqueness is to start with two different elements ""of the same type"" and then show the operator sends them to the same value Also he's checking assuming linearity of $d$ before proving it, below. Isn't that incorrect? Also in the computation in the second part of the step, he starts $$d\omega = d(f dx_I) = d(f \wedge dx_I) = ...$$ -- this appearance of a wedge in the last step is mysterious to me. Check that given k-form $\omega$ , $d\omega$ is $C^{\infty}$ and $d$ is linear on $k$ -forms If $J$ is an abtrirary k-tuple of integers then $$d(f \wedge dx_J) = df \wedge dx_J$$ Didn't he show this in the second part of step one?? Verify property 2 in special case Verify property 2 in general. Verify property 3 in special case. Verify property 3 in general. Where's the verification of property 1, was that accomplished somewhere without being explicitly mentioned?","['proof-explanation', 'tensors', 'geometry', 'manifolds', 'differential-geometry']"
3240358,Show differential operator is not bounded using definition of bounded operators,"Let $T:C^{1}_{[a,b]} \rightarrow C^{0}_{[a,b]}$ with $a<b$ be the differential operator defined as $Tx=x’$ . The practice exercise asks for the kernel and range of such operator and also a demonstration using the definition of bounded operators. An operator on normed spaces is said to be bounded when $\|Tx\| \leq c\|x\|$ where $c$ is a real number. I have already been through examples of continuous functions which show that it is not bounded. However, this one specifically asks to use the definition. Your contribution would be much appreciated. Feel free to shut this down if you indicate a solution that is available out there.","['differential-operators', 'operator-theory', 'normed-spaces', 'functional-analysis']"
3240437,Proof that $ \frac{3\pi}{8}< \int_{0}^{\pi/2} \cos{\sin{x}} dx < \frac{49\pi}{128}$,"Proof that $$
\frac{3\pi}{8} <
\int_{0}^{\pi/2} \cos\left(\sin\left(x\right)\right)\,\mathrm{d}x <
\frac{49\pi}{128}
$$ Can somebody give me some instruction how to deal with inequality like that? My current idea is: I see $\frac{3\pi}{8}$ on the left. So I think that I can prove that $$ \frac{3}{4}<\cos{\sin{x}} $$ And after take integral: $$ \frac{3x}{4} \rightarrow \frac{3}{4} \cdot \frac{\pi}{2} = \frac{3\pi}{8} $$ But it is not true because $$ \cos{\sin{x}} \geqslant \cos{1} \approx 0.5403 < 3/4$$ What have I do in such situation?","['integration', 'trigonometry', 'proof-writing', 'inequality']"
3240446,Why are some sets neither symmetric or anti-symmetric?,"In this relation set R1 = {(2, 2),(2, 3),(2, 4),(3, 2),(3, 3),(3, 4)}, when finding its property of relation- antisymmetric, transitive, symmetric etc the answer states that its neither antisymmetric or symmetric. However as the set is not symmetric should it not be antisymmetric. Trying to grasp the concept any help would be greatly appreciated.","['elementary-set-theory', 'binary-operations', 'relations', 'discrete-mathematics']"
