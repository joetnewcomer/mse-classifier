question_id,title,body,tags
2154537,Is there a pattern to expression for the nested sums of the first $n$ terms of an expression? [duplicate],"This question already has answers here : Finite Sum of Power? (2 answers) Proof of the hockey stick/Zhu Shijie identity $\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1}$ (20 answers) Closed 7 years ago . Apologies for the confusing title but I couldn't think of a better way to phrase it. What I'm talking about is this: $$ \sum_{i=1}^n \;i = \frac{1}{2}n \left(n+1\right)$$
$$ \sum_{i=1}^n \; \frac{1}{2}i\left(i+1\right) = \frac{1}{6}n\left(n+1\right)\left(n+2\right) $$
$$ \sum_{i=1}^n \; \frac{1}{6}i\left(i+1\right)\left(i+2\right) = \frac{1}{24}n\left(n+1\right)\left(n+2\right)\left(n+3\right) $$ We see that this seems to indicate: $$ \sum_{n_m=1}^{n}\sum_{n_{m-1}=1}^{n_m}\ldots \sum_{n_1=1}^{n_2} \; n_1 = \frac{1}{m!}\prod_{k = 0}^{m}(n+k) $$ Is this a known result? If so how would you go about proving it? I have tried a few inductive arguments but because I couldn't express the intermediate expressions nicely, I didn't really get anywhere.","['summation', 'calculus']"
2154574,Getting complex solutions with MATLAB's ode45,"I'm trying to solve the ODE below using Matlab's ode45 : $$\frac{dy}{dx}=\sqrt{A\cdot \sin(y) + B\cdot \cos(y)}$$ where $0\le x \le 1$. The Matlab code is: A = -2.45;
B = 2.50;
tspan  = 0:0.01:1;
odefun = @(t,y) sqrt(A*sin(y) + B*cos(y));
[t,y] = ode45(odefun, [0 1], 0); The constants $A$ and $B$ are chosen such that inside the square root it is always non-negative. The solution seems fine for most values of $x$, but for the last few points close to $x=1$ I get complex $y$ with very small imaginary parts. I then took the real or the absolute value of $y$, but then I would get negative value inside the square root. Should I try other ODE solvers? Or is there anything else I could do to eliminate the complex solutions?","['matlab', 'ordinary-differential-equations']"
2154601,A problem relating orthcentre and pair of equation.,"Show that the orthocentre of triangle formed by the striaght lines $$ax^2 + 2hxy + by^2 = 0$$ and $lx + my = 1$ is point $(x^\prime, y^\prime)$ such that $${x^\prime \over l} = {y^\prime \over m} = {a+ b \over am^2 - 2hlm + bl^2} \tag{+}$$ Let the two lines be $y - \alpha x = 0, y - \beta x = 0$ The altitude from top vertex is $mx - ly = 0 \qquad \qquad (1)$ Now we need to compute the intersection of $y - \alpha x = 0$ and $lx + my - 1 = 0$ which is $\displaystyle \left( {1\over l + m\alpha}, {\alpha \over l + m \alpha} \right)$. From this, the equation of altitude from intesection of lines $y - \alpha x = 0$ and $lx + my - 1 =0$ can be computed. I did the labour and got $$\beta \gamma y + \gamma x - (\alpha\beta + 1) = 0\tag 2$$
where $\gamma = l + m \alpha$ The intersection (1) and (2) is the orthocentre which I get as $$(x^\prime, y^\prime) = \left( {m \over \gamma}\left[ {\alpha \beta + 1 \over m + l\beta}\right], { l \over \gamma } \left[ {\alpha \beta + 1 \over m + l\beta} \right]\right) \tag{3}$$ Now from here computing $\displaystyle {x^\prime\over l}$ and $\displaystyle {y^\prime \over m}$ $${y^\prime \over m} ={l\over m} \left({1+ \alpha \beta \over lm + l^2\beta + m^2\alpha + ml\alpha\beta}\right)\tag{4}$$ $${x^\prime \over l} = {m\over l} \left({1+ \alpha \beta \over lm + l^2\beta + m^2\alpha + ml\alpha\beta}\right)\tag{5}$$ We know that $b(y - \alpha)(y - x\beta) := ax^2 + by^2 + 2hxy = 0$ $$\therefore b(y - \alpha)(y - x\beta) = by^2 + byx(-\beta - \alpha) + b\alpha\beta x^2 = 0$$ We get the values of $a,b,h$ in terms of $\alpha, \beta$ from this equation and plugging those in (+) we get $$1+ \alpha \beta \over \alpha\beta m^2 + (\alpha + \beta)(ml) + l^2 \tag{6}$$ From (4), (5) and (6) I get $${x^\prime \over l} \ne {y^\prime \over m} \ne {a+ b \over am^2 - 2hlm + bl^2}$$ I did this question same way at least twice but still get the same result. What went wrong in my answer ?","['trigonometry', 'analytic-geometry', 'geometry', 'conic-sections', 'linear-algebra']"
2154605,value of trigonometric product [duplicate],"This question already has an answer here : Evaluating a product of tangents (1 answer) Closed 7 years ago . Finding $\displaystyle \tan \left(\frac{\pi}{14}\right)\cdot \tan \left(\frac{3\pi}{14}\right)\cdot \tan \left(\frac{5\pi}{14}\right) $ assume $\displaystyle \frac{\pi}{14} = \theta$ then $\tan (14\theta) = 0$ wan,t be able to go further, some help me",['trigonometry']
2154655,Finding $\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^m})=?$,Find the limit : $$\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^n})=?$$ My try : $$1-\tan^2 y = \frac{2\tan y }{\tan(2y)}$$ $$\lim_{ n \to \infty }\left( \frac{2\tan\frac{x}{2} }{\tan(x)}\right)( \frac{2\tan\frac{x}{4} }{\tan(\frac{x}{2})})( \frac{2\tan\frac{x}{8} }{\tan(\frac{x}{4})})...( \frac{2\tan\frac{x}{2^n} }{\tan(\frac{x}{2^{n-1}})})=?$$ Now?,['limits']
2154666,"Derive method of moments estimator of $\theta$ for a uniform distribution on $(0, \theta)$","Let $X_1, \ldots, X_n$ be a random sample (i.i.d.) from a uniform distribution on $[0, \theta]$ , where $\theta$ is unknown. Derive the method of moments estimator of $\theta$ and find its expectation and variance. Is this estimator consistent? Justify your answer. Here I got the answer for expectation is $\dfrac{\theta}{2}$ . Can anyone help with the variance please?","['statistical-inference', 'variance', 'statistics', 'uniform-distribution', 'parameter-estimation']"
2154710,Prove $y=\sin(t^2)$ cannot be a solution on an interval containing $t=0$ of an equation $y'' + p(t)y'+q(t)y=0$,"I want to prove that $y=\sin(t^2)$ cannot be a solution on an interval containing $t=0$ of an equation $$y'' + p(t)y'+q(t)y=0$$ using the Wronskian and Abel's formula. Let $y_{1}$ and $y_{2}=\sin(t^2)$ where $~y_{1}$ $\neq$ $y_{2}$ be linearly independent. The Wronskian, $$W(y_{1},y_{2})(0) = 0~,$$ but how does this contradicts Abel's formula? I am stuck for a long time, please help me.","['wronskian', 'ordinary-differential-equations', 'calculus']"
2154719,Show limit to zero for $\frac{x^3-xy^2}{\vert x \vert + y^2}$ does not exist,"Given is the following function $f(x,y)= \frac{x^3-xy^2}{\vert x \vert + y^2}$. How to proof that the limit $\lim_{(x,y)\to(0,0)}f(x,y)$ does not exist? I have yet tried approaching zero in various directions, yet I have not been able to find two with different outcomes.","['multivariable-calculus', 'limits']"
2154750,"If a matrix commutes with a set of other matrices, what conclusions can be drawn?","I have a very specific example from a book on quantum mechanics by Schwabl, in which he states that an object which commutes with all four gamma matrices, $$
        \begin{pmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & -1 & 0\\
        0 & 0 & 0 & -1\\
        \end{pmatrix}
        \begin{pmatrix}
        0 & 0 & 0 & 1\\
        0 & 0 & 1 & 0\\
        0 & -1 & 0 & 0\\
        -1 & 0 & 0 & 0\\
        \end{pmatrix}
        \begin{pmatrix}
        0 & 0 & 0 & -i\\
        0 & 0 & i & 0\\
        0 & i & 0 & 0\\
        -i & 0 & 0 & 0\\
        \end{pmatrix}
        \begin{pmatrix}
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & -1\\
        -1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        \end{pmatrix},
$$
must be a multiple times the unit matrix. These matrices don't seem to span all $4 \times 4$ matrices so why would this be the case? I have asked around but no one seems to know the answer.","['matrices', 'linear-algebra']"
2154799,Proof of Polya's theorem (Probability Theory),"I know a theorem called Polya's theorem: $X_n \rightarrow X$ in distribution as $n\rightarrow \infty$ is equivalent to $\sup_{x} | F_n(x) -F(x)| \rightarrow 0$ as $n \rightarrow \infty$ , where $F_n, F$ are distribution functions of $X_n$ and $X$ , respectively. Do you know where I can find out the proof for this theorem? or do you have hints to prove it?","['probability-theory', 'probability']"
2154850,Prove that $a^2 + b^2 + c^2 \lt 2(ab + bc + ca)$ [duplicate],"This question already has answers here : If $x, y, z$ are the side lengths of a triangle, prove that $x^2 + y^2 + z^2 < 2(xy + yz + xz)$ (4 answers) Closed 7 years ago . Given that $a, b$ and $c$ are the sides of a triangle. How to prove that $a^2 + b^2 + c^2 \lt 2(ab + bc + ca)$?
Maybe any hint? Am I going to wrong direction?
$$2(ab + bc + ca)-a^2 + b^2 + c^2>0$$
$$2ab + 2bc + 2ca-a^2 + b^2 + c^2>0$$
$$2b(a+c) + 2ca-a^2 + b^2 + c^2>0$$
...?","['algebra-precalculus', 'inequality', 'geometry']"
2154911,Integration by parts with a pre-defined integral,"Given is a function $f(x)$, whose indefinite integral $F(x)=\intop f(x)dx$ is known. I want to solve for $$ \intop xf(x)dx .$$ I want to apply integration by parts. Thew expression is equivalent to $\int u\,dv$ with $$ u=x, \ \ \ dv=f(x)dx $$ which implies $$ du=dx, \ \ \  v=F(x)dx. $$ Now, when solving I get $$  \intop xf(x)dx = x F(x) - F(x) $$ which is incorrect. Can someone please tell in what sense I am misusing integratino by parts here?","['real-analysis', 'integration', 'integration-by-parts', 'calculus']"
2154946,Wedge product of matrix-valued differential forms,"Below you can find the original question, but here is the main underlying calculation that seems bizarrely impossible to find written down anywhere: $$
\begin{pmatrix}
  a&b\\c&d
\end{pmatrix}
\wedge
\begin{pmatrix}
  e&f\\g&h
\end{pmatrix}
\overset{?}{=}
ah-fc
$$ The bounty on this question is really for this one calculation, and the original question below is just for some (historical) context. Let $X$ be a complex $n$ -manifold with a rank- $r$ vector bundle $E$ and open cover $\{U_\alpha\}$ such that $E|_{U_\alpha}\cong\mathbb{C}^r$ . Say we have two $r\times r$ -matrices $P,Q$ of differential $1$ -forms, i.e. $P\in\Omega^1_{U_\alpha}\otimes\operatorname{End}(E|_{U_\alpha})$ ; $Q\in\Omega^1_{U_\beta}\otimes\operatorname{End}(E|_{U_\beta})$ . Question 1: What is the wedge product $P\wedge Q$ ? I believe it should just be matrix multiplication, but using the wedge product component-wise, i.e. $$(P\wedge Q)_{ij}=\sum_{k}(P_{ik}\wedge Q_{kj})$$ but (having read the article on vector-valued differential forms on the infallible Wikipedia) we should have that $$P\wedge Q\in\Omega^2_{U_{\alpha\beta}}\otimes\operatorname{End}(E|_{U_\alpha})\otimes \operatorname{End}(E|_{U_\beta}).$$ Then, using the fact that the $E|_{U_\alpha}$ are finite-dimensional vector spaces, we see that $P\wedge Q$ should be an $r^2\times r^2$ -matrix of differential $2$ -forms (since $\operatorname{End}(V)\otimes\operatorname{End}(W)\cong\operatorname{End}(V\otimes W)$ for f.d. vector spaces). In this case, the wedge product should be given by something like the Kronecker product of $P$ and $Q$ . Question 2: Now let $M$ be an $r\times r$ matrix of holomorphic functions (i.e. $0$ -forms). What can we say about $P\wedge MQ$ ? It seems clear that we should always have $(MP)\wedge Q=M(P\wedge Q)$ ; $P\wedge MQ=PM\wedge Q$ ; $P\wedge Q=-(Q^t\wedge P^t)^t$ (or something similar, depending on the answer to Question 1). However, using these two 'facts' it doesn't seem possible to 'pull out the $M$ ' from an expression of the form $P\wedge MQ$ . Is there a way of doing so? Matrix non-commutativity gets in the way here, but are there restrictions we can place on $M$ , $P$ , or $Q$ to get some nice result?","['matrices', 'differential-forms', 'differential-geometry']"
2154952,"Finding extremal points on $f(x,y)$","This is the equation: 
$$f(x,y) = xye^\left(-\frac{1}{2}(x^2 + y^2)\right)$$
This is what I've done:
$$\nabla f(x,y) = \begin{bmatrix}
(1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\ 
(1-y^2)xe^\left(-\frac{1}{2}(x^2 + y^2)\right)
\end{bmatrix}$$
Here's the thing I'm worried about, to find when $\nabla f(x,y) = 0$, i set each equation $= 0$.
$$\begin{align}
(1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) &= 0 \\
y e^\left(-\frac{1}{2}(x^2 + y^2)\right) &= x^2ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\
1 &= x^2 \\
x &= \pm 1\end{align}
$$
Is this legal, or do i lose some solutions when I divide away everything?","['multivariable-calculus', 'applications', 'maxima-minima', 'proof-verification']"
2154974,Bounding angles in Riemannian triangles with bounded sides,"Is it true that angles in Riemannian triangles with bounded sides are uniformly bounded? More precisely, let $M$ be a Riemannian manifold. Given $0<r <s$, is there a number $\delta$ (depending on $M,r,s$), such that for every triangle in $M$  (composed of geodesic edges), with edge lengths in $[r,s]$, its angles are greater than $\delta$? Do we need compactness of $M$ for that to hold? Or are curvature bounds suffice? Edit: The statement is false if we allow arbitrary pairs of $(r,s),0<r<s$. In fact, if we allow $2r \le s$, then one can always take a degenerate triangle whose edge lengths are $r,r,2r$ and then two angles will be zero! Thus, there are to ways to proceed towards a correct version: Proposition (1): Let $M$ be compact, $0<r<s, 2r > s$. Then, there exist a number $\delta=\delta(M,r,s)$, such that for every triangle in $M$ with edge lengths in $[r,s]$, all its angles are greater than $\delta$. Proposition (2): Let $M$ be compact, $0<r<s$. Then, there exist a number $\delta=\delta(M,r,s)$, such that for every triangle in $M$ with edge lengths in $[r,s]$, at least one angle is greater than $\delta$. Below I give proofs for the two propositions stated above. Open questions: Do the propositions hold if we only assume lower bound the curvature, without compactnes? Is there a way to get an effective bound $\delta$ as a function of $r,s$ (depending also in $M$ somehow)? 
What if we assume some bounds on the curvature? (The above discussion on degenerate triangles implies that in Proposition (1), we should expect $\delta(r,s)$ to tend to zero, when $s \to 2r$). Proof of Proposition (1): Suppose by contradiction there is no bound $\delta$. Then, there exist triangles $\Delta_n=\{[a_n,b_n],[b_n,c_n],[c_n,a_n]\}$ (where $[a_n,b_n]$ is a minimizing geodsic of unit speed connecting $a_n,b_n$ etc) with edge lengths in $[r,s]$, and each $\Delta_n$ has an angle $\theta_n$ which tends to zero when $n \to \infty$. By passing to a subsequence (or relabeling the vertices) we can assume $\theta_n=\angle b_n a_n c_n$. Using the Arzela-Ascoli theorem (and perhaps again passing to a subsequence) we can assume that $[a_n,b_n] \to [a,b], [b_n,c_n] \to [b,c],[c_n,a_n] \to [c,a]$, when the convergence is uniform ($[a,b],[b,c],[c,a]$ are paths connecting the corresponding limit points). By our assumption, $$d(a_n,b_n)=L([a_n,b_n]),$$ hence by the continuity of the distance function, and lower semi-continuity of length (w.r.t uniform convergence), we get that $$ L([a,b]) \le \lim_{n \to \infty} L([a_n,b_n])=\lim_{n \to \infty} d(a_n,b_n)=d(a,b),$$ hence $[a,b]$ is a shortest path connecting $a,b$ and similar statements hold for $[b,c],[c,a]$. Observation (1): Continuity of the distance imlies $d(a,b),d(b,c),d(c,a)$ all lie in $[r,s]$. Now, since angles in Riemannian manifolds are lower semi-continuous* , $$ \angle b a c \le \liminf \angle b_n a_n c_n =0,$$ so $\angle b a c=0$ Hence, $b,c$ lie on the same geodesic emanating from $a$. W.L.O.G we assume $b$ lies on that geodesic between $a,c$. But then $$ s \ge d(a,c)=d(a,b)+d(b,c)\ge 2r,$$ which contradicts the assumption. (we have used observation (1) above). Proof of Proposition (2): The proof is very similar to the proof of proposition (1), so we only give a sketch: Assume the claim is false. Then there exist suitable triangles $\Delta_n$, whose all angles are not greater than $\frac{1}{n}$. Thus, following the proof above, we conclude there is a ""limit triangle"" with all angles zero, which is a contradiction. *Remrak on semi-continuity of angles: By theorem 4.3.11 in ""A course in metric geometry"" (by Burago,Burago,Ivanov), if the curvature of $M$ is nonnegative then angles in $M$ are lower semi-continuous. The assumption of nonnegative curvature can probably be omitted. The lower semi-continuity of angles which seems to hold in any intrinsic space of bounded curvature (see details in HK Lee's answer ); Locally, every Riemannian manifold has bounded curvature, and since angles are ""local"" objects (we can restrict our attention to the a neighbourhood of the corresponding vertex) the conclusion follows.","['angle', 'riemannian-geometry', 'triangles', 'geometry']"
2154983,"Likelihood Ratio Test: Uniform Distribution, Change of Inequality in Alternative Hypothesis","Thanks, in advance. I would really appreciate any help here. Problem:  Let $X_1, \ldots, X_n$ be a random sample from a $\operatorname{Uniform}(0,\theta)$ population.  Let $$H_0 : \theta = \theta_0 \quad \text{and} \quad H_A : \theta < \theta_0.$$  The unrestricted MLE of $\theta$ is $X_{(n)}$ (the maximum). (a) Show that for a likelihood ratio test of size $\alpha$ that the rejection region is $$\frac{X_{(n)}}{\theta_0} < \alpha^{1/n}.$$ Solution :  This question I was able to do.  We know that the rejection region for any GLRT is of the form $$RR = \{\Lambda(X_1, \ldots, X_n) < c\}.$$  So, $$\begin{align*}
\Pr[\text{reject } H_0 \mid H_0 \text{ true}] &= \Pr[\Lambda(X_1, \ldots, X_n) < c \mid \theta = \theta_0] \\
&= \Pr[(X_{(n)}/\theta_0)^n < c \mid \theta = \theta_0 ] \\
&= \Pr[X_{(n)} < c^{1/n} \theta_0 \mid \theta = \theta_0 ] \\
&= \Pr[X_1 < c^{1/n} \theta_0 \mid \theta = \theta_0]^n \\
&= \biggl( \int_{x=0}^{c^{1/n} \theta_0} \frac{1}{\theta_0} \, dx \biggr)^{\! n} \\
&= (c^{1/n})^n = c \\
&= \alpha \end{align*}$$ Thus, $$RR = \{\Lambda(X_1, \ldots, X_n) < c\} = \left\{ \left(\frac{X_{(n)}}{\theta_0}\right)^{\! n} < \alpha \right\} = \left\{ \frac{X_{(n)}}{\theta_0} < \alpha^{1/n} \right\}.$$ (b) Now, let $$H_0 : \theta = \theta_0 \quad \text{and} \quad H_A : \theta > \theta_0.$$  Show that the LRT is simple in the sense that if $X_{(n)} < \theta_0$ we accept $H_0$ and if $X_{(n)} > \theta_0$ we reject $H_0$ for all $\alpha$. Attempt at this question :  So in this case would the rejection region of the LRT switch to this:  $$RR = \{ \Lambda(X_1, \ldots, X_n) > c \}?$$  My reasoning is that if $\Lambda(X_1, \ldots, X_n) = (X_{(n)}/\theta_0)^n$, then $H_A$ implies that $\theta > \theta_0 = X_{(n)} > \theta_0$ and so we would expect $(X_{(n)}/\theta_0)^n$ to be a number larger than one if $H_A$ is true.  And so, doing the same process as (a) for all $\alpha$ it would follow that if $X_{(n)}/\theta_0 > 1$ (as long as $X_{(n)} > \theta_0$) then we would always reject for all $\alpha$ since we end up with $(X_{(n)}/\theta_0)^n > \alpha \rightarrow X_{(n)}/\theta_0 > \alpha^{1/n}$ and $0 < \alpha \le 1$.  Of course, the bound could be lower for different $\alpha$ that aren't one, but $X_{(n)}/\theta_0 > 1$ will always be a stronger condition. Basically, this whole idea revolves around the RR being different from what I am used to seeing.  What is wrong with my reasoning? EDIT: I believe I have figured out this problem. Here was my solution in case anyone cares: Since $\Lambda_r = (\frac {X_{(n)}}{\theta_0})^n$ , we know that if $H_A : \theta > \theta_0$ then this implies that $H_A : X_{(n)} > \theta_0$. So, we want to find c such that again, $Pr[reject \ H_0 | H_0 \ is \ true]
=Pr[\Lambda_r < c \ \ | \theta = \theta_0]$=$Pr[\ (\frac {X_{(n)}}{\theta_0})^n < c \ \ | \ \ \theta = \theta_0] = \alpha$. If indeed $H_A : X_{(n)} > \theta_0$ is true then this would imply (as I originally stated) that $\Lambda_r > 1$. In other words, 
$RR = \{ \Lambda_r > 1\} $. However, if $X  \sim Uniform(0, \theta)$ distributed random variable then under $H_0$ it is impossible for the maximum $X_{(n)}$ to be greater than $\theta_0$ and so it is impossible for $\Lambda_r > 1$, since under the null $X  \sim Uniform(0, \theta_0)$ and thus $Pr[ X_{(n)} > \theta_0 | \theta = \theta_0] = 0 $ . Thus, $Pr[reject \ H_0 | H_0 \ is \ true] = Pr[\ \Lambda_r > 1\ | \theta = \theta_0] = Pr[X_{(n)} > \theta_0 | \theta = \theta_0] =  0.$ Thus, it follows that if we observe a maximum value that is greater than $\theta_0$ that the null hypothesis cannot be true (ie. the alternative hypothesis must be true), since under the null hypothesis the probability of observing a maximum value greater than $\theta_0$ is zero, regardless of $\alpha$.","['statistics', 'probability', 'hypothesis-testing']"
2154991,set theory proof (cartesian products/symmetric differences) [duplicate],"This question already has answers here : elementary set theory (cartesian product and symmetric difference proof) (2 answers) Closed 7 years ago . I have to prove this theorem - Let $A$, $B$, and $C$ be sets. Then, $A \times (B \Delta C) = (A \times B) \Delta (A \times C)$. I was wondering if this was the right way to start the proof? Let $m$ be an arbitrary element of $A \times (B \Delta C)$. Then, $m=(x,y)$ s.t. $x \in A$ and $y \in (B \Delta C)$ by the definition of cartesian products. Then, $(m \in A) \land (y \in B) \land (y \not\in C)$ by the definition of set difference. So, $((x,y) \in A \times B) \land ((x,y) \not\in A \times C)$ by the definition of cartesian product. Thus, $(x,y) \in (A \times B) \Delta (A \times C)$. So basically the LHS = RHS because (x,y) is an element of both sides?","['elementary-set-theory', 'discrete-mathematics']"
2155011,Sum of periodic functions with non-commensurable periods,"Are there two periodic functions $f,g:\mathbb{R}\to\mathbb{R}$ with period $p,\,q$, respectively, such that $\frac pq \not\in\mathbb{Q}$ and $f+g$ is periodic? The question arised when I tried to prove that the set of all periodic functions is not a vector subspace of the set of functions from $\mathbb{R}$ to $\mathbb{R}$, which I did by showing that the functions $f(x) = \{x\} = x - \lfloor x\rfloor$ and $g(x) = \sin(\sqrt2x)$ are both periodic funcions whose sum is not. To do that, I had to use particular properties of such functions and didn't get much far into the general case (supposing that the answer to the question is no).","['special-functions', 'linear-algebra', 'calculus', 'functions']"
2155013,Why are they taking ${5 \choose 4}$?,"So, the question is- A rack has 5 different pairs of shoes. Number of ways in which 4 different shoes can be chosen so that no two shoes are from the same pair. Now, what they have done is- ${5 \choose 4}$${2 \choose 1}$${2 \choose 1}$${2 \choose 1}$${2 \choose 1}$ Now , they said that they have taken ${5 \choose 4}$ to select any  4 pairs from the 5 and ${2 \choose 1}$ is for selecting any one shoe. But, pairs and shoes are different. So, how are they multiplying like this? Kindly help me!","['algebra-precalculus', 'permutations', 'combinations']"
2155095,How can I quickly know the rank of this / any other matrix?,"I have looked this up on several sites but they confused me because some of the given information was wrong / unclear / contradicting whatever. I hope you can tell me all / most important ways to calculate the rank of a matrix. As example, I take the matrix $$A = \begin{pmatrix}
1 & 2 & 3\\ 
0 & 5 & 4\\ 
0 & 10& 2
\end{pmatrix}$$ Now several sites included that info so it must be true: If, we are looking at this example, there is no line with zeroes only, the rank of this matrix will be $3$. (?) Here is the problem. It will cost time to form this matrix to see if there will be lines with zeroes only. For this I can use Gaussian Elimination . I have tested it with that Gauss and I couldn't get a line with zeroes only, so I conclude that this matrix $rank(A)=3$. This however seems very inefficient way, I hope you can tell me better ways?","['matrices', 'number-theory', 'linear-algebra', 'elementary-number-theory']"
2155126,Cohen-Macaulay and connected implies equidimensional?,"I'm asking for a reality check. It seems to me that since Cohen-Macaulay rings are locally equidimensional, such a ring is either equidimensional or else disconnected (with different dimensions occurring on different connected components). But I have not slept enough, so I don't trust my reasoning. Is it sound? Addendum: Having given this more (and better-slept) thought, it is not at all clear to me. I know that a local Cohen-Macaulay ring is equidimensional, in other words, all minimal primes have the same dimension. (E.g. Eisenbud, Commutative Algebra with a View Toward Algebraic Geometry , Corollary 18.11.) ( Edit 2/24/17: The following proof is flawed. See second addendum.) It follows from this that for an arbitrary (noetherian) Cohen-Macaulay ring $R$, if $\operatorname{Spec} R$ is connected, then all the minimal primes $\mathfrak{p}$ have the same dimension. I.e. $\dim R/\mathfrak{p}$ does not depend on $\mathfrak{p}$. Proof: two minimal primes of differing dimension cannot be contained in the same maximal, by localizing at that maximal and applying the result for local rings. Therefore, sort the minimal primes according to dimension. There are only finitely many minimal primes, so finitely many classes containing finitely many primes each, and one can build a partition of $\operatorname{Spec} R$ into finitely many disjoint closed sets corresponding to the classes in this way. Connectedness implies there is only one class, thus all minimal primes have the same dimension. Geometrically, this is the statement that the irreducible components of $R$ are all the same dimension. But I thought equidimensionality also meant that the maximals all have the same height. Here, I don't see the argument. Imagine one maximal that covers all the minimals with height $1$, say, and another maximal that lies above all the minimals with height $2$, with a lot of height 1 primes in between. This yields a connected $\operatorname{Spec}$ since a single closed set containing all the minimal primes will be everything, but if two different closed sets each contain a minimal prime, then they overlap at the maximals. This scenario is not ruled out by any property I can think of possessed by the poset of primes of a Cohen-Macaulay ring: it is catenary (since it is a ranked poset, ranked by height); the minimals are all dimension 2; etc. But yet I see numerous references on the internet to the notion that Cohen-Macaulay rings are equidimensional (sometimes without even a connected assumption, which can't be right; e.g. $k\times k[x]$ is Cohen-Macaulay). E.g. here . Perhaps they mean the local case? Or are using a weaker notion of equidimensionality? Second addendum, 2/24/17: I am no longer convinced by the above argument that the minimal primes in a Cohen-Macaulay ring with a connected spectrum must have the same dimension. It is the case that the minimal primes under a given maximal have the same-length saturated chains to that maximal , but it doesn't follow that they have the same dimension: perhaps one of them is also under another maximal with a greater height, while the other is not. On the other hand: if the Cohen-Macaulay ring with the connected spectrum happens to be integral over an equidimensional Cohen-Macaulay subring (which is guaranteed, e.g. for finite-type $k$-algebras by Noether normalization, per conv. with MooS below her/his answer), then I have convinced myself it is equidimensional. Here is why I think so: Let the Cohen-Macaulay ring be $R$ and let the equidimensional Cohen-Macaulay subring be $S$; say its dimension is $d$. Since $S$ is Cohen-Macaulay, the result quoted above for local rings, together with the fact that Cohen-Macaulay rings are catenary, implies that the lengths of any two saturated chains of primes from a given maximal to two minimals under it are equal. Since $S$ is equidimensional, this common length must be $d$ for every maximal. Thus, the length of any saturated chain between any minimal prime and any maximal is $d$. It follows from a second application of catenaryness that for any prime ideal of $S$, and any saturated chain from a minimal to a maximal containing this prime, the position of this prime in this chain depends only on the prime and not on the chain. Now consider any saturated chain of primes between a minimal prime $\mathfrak{p}$ and a maximal $\mathfrak{m}$ in $R$. Say its length is $\ell$. Intersecting with $S$, one obtains a chain of primes, and due to integrality, it has the following three properties: (1) it is saturated; (2) its ""top end"" is a maximal; (3) it is length $\ell$. (1) and (2) are due to going-up and (3) is due to incomparability . ( Edit 2/24/17: I'm no loger convinced going-up implies (1). Grr! See third addendum.) Let the $h(\mathfrak{p})$ be the height of $\mathfrak{p}\cap S$ in $S$. By the above discussion, $h(\mathfrak{p}) = d - \ell$; thus $\ell$ doesn't depend on $\mathfrak{m}$ or the choice of chain, but only on $\mathfrak{p}$. Connectedness of the spectrum (at least for noetherian rings) is equivalent to connectedness of the bipartite graph with vertex classes the minimal and maximal primes, and edges indicating containment. (Proof: two closed sets $V(\mathfrak{p}), V(\mathfrak{q})$ of the spectrum, where $\mathfrak{p},\mathfrak{q}$ are minimal primes, meet iff there is a maximal containing both $\mathfrak{p},\mathfrak{q}$. So if the graph is disconnected, there is a partition of these finitely many closed sets $V(\text{minimal prime})$, which cover the spectrum, into disjoint classes, and this disconnects the spectrum. Conversely, if the spectrum is disconnected, there are two disjoint nonempty closed sets; they must both meet the minimal primes and contain everything above the ones they meet, and then the way they partition the minimal primes also partitions the graph.) Thus, between any two minimal primes $\mathfrak{p},\mathfrak{q}$, there is a path in the graph $$\mathfrak{p} = \mathfrak{p}_1 \subset \mathfrak{m}_1\supset \mathfrak{p}_2\subset \mathfrak{m}_2\supset \dots\subset\mathfrak{m}_r\supset \mathfrak{p}_{r+1} = \mathfrak{q}$$ Because $R$ is Cohen-Macaulay, by the same logic as for $S$ above (i.e. by localizing at $\mathfrak{m}_i$) we conclude that the lengths $\ell_i$ and $\ell_i'$ of saturated chains between $\mathfrak{m}_i$, and $\mathfrak{p}_i$ and $\mathfrak{p}_{i+1}$ respectively, are equal, for each $i$. Thus $h(\mathfrak{p}_i)$ and $h(\mathfrak{p}_{i+1})$ are equal. And thus $h(\mathfrak{p})$ and $h(\mathfrak{q})$ are equal. Thus $h$ is a constant function on the minimal primes of $R$. Now take any minimal prime of $S$. By lying-over, there is some prime of $R$, say $\mathfrak{p}^*$, lying over it, which must be minimal by incomparability, and we have $h(\mathfrak{p}^*)=0$. Therefore, $h$ is identically zero on the minimal primes of $R$. Thus for any saturated chain in $R$ from a minimal $\mathfrak{p}$ to a maximal $\mathfrak{m}$, of length $\ell$, we have $0=h(\mathfrak{p}) = d - \ell$, and we conclude $\ell=d$. Therefore, all maximals have height $d$ and all minimals have dimension $d$, i.e. $R$ is equidimensional of dimension $d$. Third addendum, 2/24/17: There is a soft spot in the proof of the second addendum as well. I found another unjustified assumption I made probably due to being overly familiar with the good behavior of the coordinate rings of varieties. Going-up doesn't imply by itself that the intersection of a saturated chain in $R$ with $S$ yields a saturated chain in $S$, although it may well be true in the present context for other reasons. I asked a followup question on this point here . Fourth addendum, 2/26/17: Given the track record at this point, there's no reason to expect me not to find another mistake; however, I believe I have proven this variant of the statement in the second addendum: If a Cohen-Macaulay ring has a connected spectrum and is finite (strengthened from integral) over a subring that is an integrally closed, equidimensional, catenary noetherian domain, then it is equidimensional. This OP is already too long without me adding another detailed proof, but it very loosely follows the pattern of the argument in the 2nd addendum, i.e. argues that all maximal chains in $S$ have length $d$ and then shows $h(\mathfrak{p})=0$ using connectedness. Here is a reasonably complete outline: I. Show all maximal chains in $S$ have the same length $d$ using equidimensionality + catenariness. II. Consider the set $W_0$ of primes of $R$ whose intersection with $S$ is zero. (a) They are all minimal and dimension $d$. (b) Fixing one of them $\mathfrak{p}$, and a target maximal $\mathfrak{m}$ containing it, construct a chain of length $d$ to $\mathfrak{m}$ by (b1) passing temporarily to $R/\mathfrak{p}$ which is a domain containing $S$; then passing to their fraction fields; then forming normal closure of $\operatorname{Frac}(R/\mathfrak{p}) /\operatorname{Frac}S$ and taking $S$'s integral closure $B$ in it; (b2) invoking going-up to construct a chain of length $d$ in $B$ with top term lying over $\mathfrak{m}\cap S$, and and lying-over to find another prime in $B$ lying over $\mathfrak{m}$; (b3) using a Galois automorphism to move the chain til its top term is the prime we found lying over $\mathfrak{m}$; (b4) intersecting with $R$ yielding the desired chain. (c) By Cohen-Macaulayness, a maximal $\mathfrak{m}$ of $R$ that contains one of the $\mathfrak{p}$'s in $W_0$ (and is therefore height $d$) can't also contain a minimal prime $\mathfrak{q}$ with $h(\mathfrak{q}) >0$. Thus if there are any such minimal primes, the graph described in the 2nd addendum is disconnected. This contradicts the fact that the spectrum of $R$ is connected, so there are no such primes. Therefore all minimals are in $W_0$, so are dimension $d$ by (a), and all maximals contain one of these, so are height $d$ by (b). Conclude equidimensionality. Fifth addendum, 3/13/17: Fourth addendum holds up I believe, but argument in (b) can be simplified by replacing the Galois theory just by passing to $R/\mathfrak{p}$ and then invoking the going-down theorem for $S\subset R/\mathfrak{p}$.","['abstract-algebra', 'ring-theory', 'cohen-macaulay', 'commutative-algebra']"
2155137,Cyclic Group Generators of Order $n$,How many generators does a cyclic group of order $n$ have? I know that a cyclic group can be generated by just one element while using the operation of the group.  I am having trouble coming up with the generators of a group of order $n$. Any help would be great!  Thanks!,"['abstract-algebra', 'group-theory']"
2155201,Can a real ODE have a complex solution?,"By a real ODE I mean an ordinary differential equation with only real coefficients and the resulting function is a function of a real argument. If such a solution exists, can you give an example? Edit: To add to this, is it still possible if the initial conditions must also be real?","['ordinary-differential-equations', 'complex-numbers']"
2155212,Need a hint to find $x$,"In below picture ,I need to find $x$ . $x$ is side length of square .I have no clue to find the value of $x$ .  As honestly as possible ,the question was for my student's and she asked me to find $x$ .I was thinking ,but I am stuck on this problem .thanks in advanced","['education', 'closed-form', 'geometry']"
2155221,Semidirect products and finding normal subgroups,"My question reads: Let A, K be subgroups. Group G is called semidirect product of A and K if A  $\trianglelefteq$ G, G=AK and  A$\cap$K = < e >. Show that the groups are the semidirect product of two of its subgroups. a) S$_3$ b) D$_4$ c) S$_4$ Now I am not sure if this is asking for a proof for each part or to directly pick two subgroups that are normal and then make sure the conditions for semidirect products are met. Also, doesn't this imply I need to show the subgroups I pick are normal? I need help picking these subgroups and from there I think it will be straightforward showing the other conditions are satisfied For example for S3 could I pick the whole group itself?","['abstract-algebra', 'normal-subgroups', 'proof-writing', 'group-theory', 'discrete-mathematics']"
2155229,Double integral - changing order of integration,"I am trying to change the order of integration of the following double integral $$\int _0^{2a} dx\int _{\sqrt{2ax-x^2}}^{\sqrt{4ax}} f(x,y) \ dy .$$ (a>0) I've sketched the domain but am struggling to change it from type 1 domain to type 2 domain. Any help? I've attached an image of the geometry of the domain and I've shaded it. (Well the image shows the case when a=1.)","['multivariable-calculus', 'integration', 'definite-integrals']"
2155256,An Introduction to Theory of Groups by Joseph J. Rotman - Exercise 1.44,"Question: Let $G$ be a group, let $X$ be a set and let $f: G \longrightarrow X$ be a bijection. Show that there is an unique operation on $X$ so that $X$ is a group and $f$ is an isomorphism. My attempt: Define an operation $*$ on $X$ such that $f(a \ \cdot \ b) = f(a) * f(b)$ for an arbitrary $a, b \in G$, so $f(a), \ f(b), \ f(a) * f(b) \in X$, because $a, b \in G$ are arbitrary and $f$ is a bijection, so $*$ is an binary operation on $X$. We show now that $(X, *)$ is a group. In fact, $\bullet \ f(e)$ is the neutral element of $(X,*)$, because $f(e) * f(a) = f(e \ \cdot \ a) = f(a) = f(a \ \cdot \ e) = f(a) * f(e)$, where $e$ is the neutral element of $(G, \cdot)$ and $a$ is an arbitrary element of $(G,\cdot)$. $\bullet$ $f(a^{-1})$ is the inverse element of $f(a)$ in $(X,*)$, because $f(a^{-1}) * f(a) = f(a^{-1} \ \cdot \ a) = f(e) = f(a \ \cdot \ a^{-1}) = f(a) * f(a^{-1})$ Therefore $(X,*)$ is a group and $f: G \longrightarrow X$ is an isomorphism by the way it was defined. We show now the oneness of operation $*$. Suppose there are two operations $*$ and $\circledast$ on $X$ such that $(X,*)$, $(X,\circledast)$ are groups and $f: (G, \cdot) \longrightarrow (X,*)$, $f: (G, \cdot) \longrightarrow (X,\circledast)$ are isomorphisms, so $id_X: (X,*) \longrightarrow (X,\circledast)$ is an isomorphism. I think this last isomorphism should help me tho prove what I want, but I don't know how this isomorphism ensure the oneness of operation $*$. I would like to a hint. Thanks in advance!","['abstract-algebra', 'group-theory']"
2155285,Show that a set is finite if and only if every linear ordering on it is a well-ordering,"Show that a set is finite if and only if every linear ordering on it is a well-ordering What i have done so far: $\Rightarrow$: Let $(X,\prec)$, where X is finite and $\prec$ is linear ordering. Consider a subset $A\subset X$. Since $X$ is finite, $A$ is also finite. Hence, there exists some $f:I(x)\rightarrow A$ bijective, where $I(x)$ is a finite subset of the natural numbers. We redefine $f$ in a such way that it is an increasing function. Given $a,b\in A$, we have that $f(m)=a$ and $f(n)=b$, for some $m,n\in I(x)$. Since $\prec$ is linear ordering we have that $a\prec b$ or $b\prec a$. Considering the firt case $a\prec b$ (ie, $f(m)\prec f(n)$), we redefine $f$ only if $n<m$, and put $f(m)=b$ and $f(n)=a$. Similarly we redefine $f$ for $b\prec a$. Doing this process for all pairs $a,b\in A$, we ensure that $f$ is increasing and, recalling that every subset of the natural numbers has a minmal ement, we have that $A$ also has a minimal element. Since $A$ was arbitrary, this proves that $\prec$ is well-ordering. That's what i could do. Is it correct? Any hints on the $\Leftarrow$ part?","['general-topology', 'elementary-set-theory']"
2155293,Hirzebruch $\chi_{y}$ genus of a Torus?,"So I'm attempting to compute the $\chi_{y}$ genus of a one-dimensional torus.  Even though I know it's certainly not zero, there are two reasons why I convinced myself it was; one of them probably very basic, the other to do with Atiyah-Bott localization. Before getting there, let $X$ be a compact, complex manifold with formal Chern roots $x_{i}$.  The Hirzebruch $\chi_{y}$ genus is given by $$\chi_{y}(X) = \int_{X} \prod_{i} (1+ye^{-x_{i}}) \frac{x_{i}}{1-e^{-x_{i}}}$$ Now, for a one-dimensional torus, there is only one factor in that product.  Moreover, its Chern root is zero since the tangent bundle is trivial.  However, one cannot naively set $x=0$, since you end up with a indeterminate $0/0$ expression.  Rather, I expanded the integrand: $$(1+ye^{-x})\frac{x}{1-e^{-x}}=(y+1) + \frac{1}{2} x (y-1) + \cdots$$ Since the integral is a pairing of homology and cohomology classes, we only get contributions from one of those terms leaving, $$\chi_{y}(X) = \frac{1}{2}(y-1)\int_{X} x $$ If that integral were one , I think that would be the right answer, but why isn't that integral zero!?  That Chern root should be zero, and moreover, the integral of the top Chern class is the Euler characteristic which is zero.  Where exactly am I overlooking something here? Localization Argument: The other reason I convinced myself this should vanish, is that a torus acts freely on itself.  So I thought that by the Atiyah-Bott localization, if there are no fixed points, then the integral simply vanishes.  Am I overlooking something in equivariant cohomology here?","['algebraic-topology', 'complex-geometry', 'algebraic-geometry']"
2155305,Physicist trying to understand GIT quotient,"I am reading Nakajima's textbook on Hilbert Schemes. I am trying to understand some very basic facts about the GIT quotient. We start with a vector space $V$ over $\mathbb{C}$. Let $G \subset U(V)$ be a Lie group and $G^{\mathbb{C}}$ its complexification so I guess $G^{\mathbb{C}} \subset GL_V$. I will denote by $G$ the complexification from now on. Apparently $V/G$ is a very badly behaved space. I do not know really why though. I can imagine that there might be some singularities but can they not be resolved e.g. by blowing up? Also, why sometimes this space is not Hausdorff? Now, let $A(V)$ be the coordinate ring of $V$. Nakajima says something I did not know, that the $A(V)$ is the same as the symmetric power of the dual space $V^*$. 
$$ A(V) = Sym^n(V^*) $$ Why is this true? I have to admit that this seems very basic and I did not know about it. Next I learn that $G$ has a natural action on $V$ i.e. $v \mapsto gv$ for $g \in G$ and $v \in V$. Then, this induces an action on $A(V)$. We define $$ A(V)^G = \{ {\text{polynomials }a | ga =a \text{ for }\forall g\in G }  \}$$
the ring of invariant (polynomials). Finally we define the algebro-geometric quotient of $V$ by $G$ as
$$  Spec(A(V)^G)=V//G  $$
To me this is the space of prime ideals that are invariant under $G$. But I do not see how exactly this is related to the original space we wanted to construct. It seems quite different actually. Intuitively what is this space $V//G$ and why is it useful? P.S. Nakajima says: The underlying space of $V//G$ is the set of closed $G$-orbits modulo the equivalence relation defined by $x \backsim y$ if some specific condition, that I do not mention here, holds.","['symplectic-geometry', 'geometric-invariant-theory', 'quotient-spaces', 'algebraic-geometry']"
2155324,$T_{2.5}$ topology without coarser metric topology,"Let $(X,\tau)$ be a second-countable $T_{2.5}$ space, where with $T_{2.5}$ I mean that any distinct points are separated by closed neighborhoods. Does there have to be some metrizable second-countable $\tau' \subseteq \tau$? The typical examples of $T_{2.5}$ spaces that are not metrizable seem to be constructed by adding additional open sets to some metrizable topology, so I would be interested in a potential example of a space which is constructed differently -- or maybe a proof that we can always find a coarser metrizable second-countable topology.","['general-topology', 'separation-axioms']"
2155353,Solve $\lim_{x \rightarrow 0} \frac{e^x+e^{-x}-2}{x^2+2x}$ without using L'Hopital's rule,"I tried: $$\lim_{x \rightarrow 0} \frac{e^x+e^{-x}-2}{x^2+2x} = \\
\frac{e^{x}(1+e^{-2x}-\frac{2}{e^x})}{x(x-2)} = \frac{e^x(1+e^{-2x})-2}{x(x-2)} = \frac{e^x(1+e^{-2x})}{x(x-2)}  - \frac{2}{x(x-2)} = \\
\frac{1+e^{-2x}}{x} \cdot  \frac{e^x}{x-2} - \frac{2}{x(x-2)} = ???$$ What do I do next?","['limits-without-lhopital', 'calculus', 'limits']"
2155380,Proof of Cramer's Large Deviation Theorem,"https://ocw.mit.edu/courses/sloan-school-of-management/15-070j-advanced-stochastic-processes-fall-2013/lecture-notes/MIT15_070JF13_Lec4.pdf On page 6 (proof of Cramer's theorem), it says $$\limsup_n \frac{1}{n} \log x_n \leq - \inf_{x\in F} I(x)$$ $$\limsup_n \frac{1}{n} \log y_n \leq - \inf_{x\in F} I(x)$$ implies $$\limsup_n \frac{1}{n} \log(x_n + y_n) \leq - \inf_{x\in F} I(x).$$ I don't see why this is true and I'm struggling to prove this. Why does that last inequality follow from the previous two?","['large-deviation-theory', 'probability-theory']"
2155417,Can the Taylor series be used to get polynomial roots?,"I'm using this method: First, write the polynomial in this form:
$$a_nx^n+a_{n-1}x^{n-1}+......a_2x^2+a_1x=c$$
Let the LHS of this expression be the function $f(x)$. I'm gonna write the Taylor series of $f^{-1}(x)$ around x=0 and then put $x=c$ in it to get $f^{-1}(c)$ which will be the value of $x$. Since, $f^{-1}(0)=0$ here, so we've got the first term of our Taylor series as $0$. Now, the only thing that remains is calculating the derivatives of $f^{-1}(x)$ at $x=0$. I'm using the fact that $$\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f^{-1}(x))}$$ By differentiating this equation, we can get the second derivative of fâˆ’1(x) as:
$$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ Similarly, we can get the other derivatives by further differentiation of this equation. Then we can evaluate all the derivatives at x=0 to get the Taylor series of $f^{-1}(x)$ and evaluate it at $x=c$ to get the value of $x$. I don't know the formula of $f^{-1}(x)$ but I know the value of $f^{-1}(x)$ at $x=0$. After doing all the formulas, what I have to do in the end in evaluating that expression at $x=0$ and I've the value of $f^{-1}(x)$ at $x=0$.
For example, $$f'^{-1}(x)=\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f(^{-1}(x))}$$
$$=\frac{1}{n*a_{n}(f(^{-1}(x)))^{n-1}+(n-1)a_{n-1}(f(^{-1}(x)))^{n-2}+.............2a_2f^{-1}(x)+a_1}$$
which gives $$\frac{1}{a_1}$$ at $x=0$ since $$f^{-1}(0)=0$$
Similarly,
$$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ We already have the value of the first derivative at $x=0$ so we can substitute that here, so
$$\frac{d^{2}}{dx^{2}}(f^{-1}(x))=-\frac{1}{a_1^{2}}\cdot 2a_2\cdot \frac{1}{a_1}$$
$$=-\frac{2a_2}{a_1^{3}}$$
I think this process can be continued to get more derivatives by the product rule. 1.Is this method correct? 2.Can something be done to make it better and remove the limitations (if there are any)? UPDATE: If I'm not wrong, then I think this method only works if none of the coefficients of the polynomial are zero. Is there some way to remove that limitation? UPDATE: Oh, I just figured out that we can obtain the Taylor series of the inverse of a polynomial around any point $x=a$ by this method. I also just found out about the Lagrange inversion theorem. It was also about getting Taylor series of inverse functions. I didn't understand much but it was the same series as mine except the coefficients. Are the coefficients also the same? Have I been doing the same thing?","['derivatives', 'roots', 'taylor-expansion', 'polynomials']"
2155433,How far has a chasing wasp flown as her target walks around a square?,"I take a walk each morning along the sides of a square; each side is one mile. I start at one corner and walk at a constant speed. As I start on the walk, an unfriendly wasp always starts at the center of the square and starts chasing me, always flying directly in the direction from the wasp to myself. She must be flying a bit faster than I walk, since precisely when I complete the walk (having returned to the starting corner) the wasp meets me and greets me with an unfriendly sting. How far has the wasp flown in her chase? By supplying the wasp with a FitBit (or perhaps by numerically integrating the equations of motion) I can tell you that the answer is roughly 4.029 miles. But I would like to have either a closed form expression for the distance travelled, or failing that, a perturbative solution that will tell me how much the distance exceeds the 4 mile perimeter of the square, to at least first order in some small quantity.","['ordinary-differential-equations', 'word-problem', 'calculus']"
2155481,Is this a good approximation for $\int_a^b\frac{f(x)}{x}dx$?,"I've derived this approximation:
$$\int_a^b\frac{f(x)}{x}dx\approx f(\sqrt{ab})*\ln{\frac{b}{a}}$$
when $a$ and $b$ are close (I don't know how close). Here, close doesn't mean that the differnce between $a$ and $b$ should be 0.01. Here close could mean 'far'. I don't know what the difference should be. Here, close only means that this formula has a limitation.
Is it something new? It could help in evaluating $Si(x)$ ( integral of $\frac{\sin{x}}{x}$), $Co(x)$ ( integral of $\frac{\cos{x}}{x}$), and many other integrals whose anti-derivatives are not elementary. I know you can do that by integrating Taylor series of those functions but this seems more compact. Does it work?","['integration', 'definite-integrals', 'functions', 'approximation']"
2155539,Does differentiability almost everywhere imply continuity on an interval?,"I suppose there are differentiable almost everywhere functions whose sets of discontinuities are dense. How to prove or disprove it? Additionally, is Thomae's function $T(x)$ raised to some power greater than 2 an example? (With 2 it isn't differentiable anywhere by Hurwitz's theorem.) Or maybe $\begin{cases}
e^{-\frac 1 {T(x)}} & \textrm{if $x\in\mathbb Q$} \\
0 & \textrm{otherwise}
\end{cases}$?","['derivatives', 'examples-counterexamples', 'continuity', 'calculus']"
2155553,Nilradical geometrical way of thinking about it,"I am following Michael Atiyah commutative algebra. In one of the proposition he proved $\operatorname{nil}(R) = \cap P$ where $P$ is prime ideal. One can easily see the inclusion $\operatorname{nil}(R) \subset P$. However, for the other inclusion we used Zorn lemma. I understand the proof, but I don't really get it conceptually speaking. Is there a geometrical way to think about it?","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2155556,"What is $F / F_{fin}$ isomorphic to, where $F$ is the group of integer functions and $F_{fin}$ is the subgroup of finitely-supported functions?","I'm trying to figure out what common group (if any!) the group $F / F_{fin}$ is isomorphic to, where $F = \{f : \mathbb Z \to \mathbb Z\}$ (with pointwise addition, so $F$ is abelian) and $F_{fin} = \{f \in F : \{x \in \mathbb Z : f(x) \neq 0\} \text{ is finite}\}$. I thought to use the First Isomorphism Theorem, but I can't seem to think of a homomorphism from $F$ with kernel $F_{fin}$. If we replace $\mathbb Z$ with a finite group, obviously the quotient would be trivial, since every function would have finite support. What happens in this case, when the base group ($\mathbb Z$) is infinite? Apologies if this is really simple.",['group-theory']
2155593,Moduli of Riemann surfaces (genus g curves) is a variety.,"I often see the moduli spaces $\mathcal{M}_g$, or at least the coarse moduli space,  of Riemann surfaces of genus $g$ described as the set of isomorphism classes of Riemann surfaces of genus $g$. Obviously, the moduli space has more structure than a set. However, the book I have been following immediately goes on to call $\mathcal{M}_g$ a variety. How do we go from $\mathcal{M}_g$ being a set to $\mathcal{M}_g$ being a variety?",['algebraic-geometry']
2155632,Equation of tangent plane.,"I have trouble understanding the proof of tangent plane: let $z=f(x,y)$ and $z_0=f(x_0,y_0)$, for $x=x_0$ the tangent line to $z$ at $(x_0,y_0,z_0) $ is along $(0,1,\frac{\partial f}{\partial y}(x_0,y_0))$. How did we get the above result ?","['real-analysis', 'calculus', 'multivariable-calculus', 'tangent-line', 'analysis']"
2155648,Strict inequality in Fatous lemma and convergence of $f_{n}$ pointwise.,"I got how the strict inequality occurs in Fatous lemma but why the limit of the characteristic function $\chi_{(n,n+1)} \rightarrow 0$ pointwise, for $E =\mathbb{R}$ , 
what happens here when $E =[0,1)$? How to think about the limit of the function goes to 0 for each value of x?","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
2155657,Closed form for $\prod_{k=1}^{n-1} \sin(\frac{\pi k}{n})^{n-k}$,"Question: I am hoping to analytically continue the function: $F(n) = \prod_{k=1}^{n-1} \sin(\frac{\pi k}{n})^{n-k}$ to $n=1/2$. My understanding is that means I will need a closed form for this product. What is a closed form of the above product? (Or are there other methods for performing the analytic continuation without having a closed form?) Attempt: A similar product satisfies the identity $\prod_{k=1}^{n-1} \sin(\frac{\pi k}{n}) = n 2^{1-n}$. A proof of the above identity is given in another post ( Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ ). I have gone through the proofs given there and tried to extend the techniques used to my case, but no luck so far. Any help is appreciated - a full solution, or just suggestions of theorems or related identities which may be of use.","['products', 'trigonometry']"
2155673,Any sequence of r.v.s $\{X_n\}$ s.t. all $X_n\in L^1$ is a sum of a supermartingale and a submartingale [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Given any sequence of r.v.s $\{X_n\}$ s.t. $X_n\in L^1$ for all $n$ and history $\{\mathcal{F_n}\}$ with $X_n\in \mathcal{F_n}$ for all $n$. How can we prove that this sequence of r.v.s can be written as a sum of a supermartingale and submartingale? Thank you!","['probability-theory', 'martingales']"
2155677,Find $\lim_{x \rightarrow \infty}(\frac{x}{x^2+1}\cdot e^{x})$,I tried: $\lim_{x \rightarrow \infty}(\frac{x}{x^2+1}\cdot e^{x}) = \frac{1}{x^2+1} \cdot e^xx = 0 \cdot \infty = 0$ But this is wrong. What did I do wrong? How do I solve this?,"['limits-without-lhopital', 'calculus', 'limits']"
2155688,The smallest parallelogram that contains a convex quadrilateral,"I try to find the smallest parallelogram in terms of area that contains a convex quadrilateral(A,B,C,D). I am pretty sure it must be constructed from two neighboring sides of the quadrilateral. But which ones? I have no approach for a condition like ""smallest angle"" or ""shortest distance"" ect.","['quadrilateral', 'geometric-construction', 'geometry']"
2155689,"$g : [0,1]\to\Bbb R$ is a concave function with $g(0) =0$ and $g(1)= \beta$. Show that $g(z) \geq \beta z$, $z \in [0,1]$.","I came across the following,
let $g : [0,1] \to \Bbb R$ be a concave function with $g(0) =0$ and $g(1)= \beta$. It implies $g(z) \geq \beta z$, $z \in [0,1]$.
Why is the statement $g(z) \geq \beta z$ true?","['real-analysis', 'convex-analysis', 'functions', 'functional-inequalities']"
2155691,How to relate the spectrum of a self-adjoint unbounded operator to the spectrum of a compact solution operator,"Apologies if this is a trivial question. I'm having some trouble getting this straight in my head and would appreciate either an explanation or a pointer to a good reference. If this looks too long, here's a tl;dr Say I have a densely defined, bounded below, symmetric operator $T$ defined in a Hilbert space $H$. After shifting by some $\sigma$ one can show there exists a compact solution operator $K$. Why does the spectrum of $T$ coincide with (a shift of) the spectrum of $K$? Setup Let $H$ be a Hilbert space with inner product $(\cdot,\cdot)$. Let $T$ be a densely defined symmetric operator bounded below by some constant $c$, with $D = \operatorname{dom}(T)$. Define the form $\mathfrak{t}$ on $D\times D$ by $\mathfrak{t}(u,v) = (Tu,v)$. By symmetry $\mathfrak{t}$ is sesquilinear. Because $T$ is bounded below, if we choose $\sigma > \max\{1,-c\}$, the form defined on $D\times D$ by 
$$ \mathfrak{t}_\sigma(u,v) = \mathfrak{t}(u,v) + \sigma\cdot(u,v) $$
is a positive definite Hermitian inner product on $D$. Define by $V$ the completion of $D$ with respect to $\mathfrak{t}_\sigma$. As $\mathfrak{t}_\sigma$ majorizes the norm on $H$ the injection $D\hookrightarrow H$ extends to a bounded embedding $\iota : V\hookrightarrow H$. Now assume that $V$ compactly embeds in $H$. (Here, if $T$ is not self-adjoint, take the Friedrichs extension.) Define an eigenvalue of $\mathfrak{t}$ to be any $\mu\in\mathbb{C}$ such that there exists some $u\in V$ so for all $v\in V$ we have $\mathfrak{t}(u,v) = \mu(u,v)$. I do not assume eigenvectors of $\mathfrak{t}$ are elements of $D$. By Riesz there exists a solution operator $S_\sigma: V^*\to V$. There is a bounded map $P:H\to V^*$ given by $u\mapsto (v\mapsto (u,v))$. (This is bounded by an application of Cauchy-Schwarz.) Composing these with the compact injection $\iota$ gives a compact map $K = S_\sigma P\iota:V\to V$. Note that $K$ has the property that for any $u\in V$, 
$$ \mathfrak{t}_\sigma(Ku, v) = (u,v). $$ By the spectral theorem for compact operators, the spectrum of $K$ is discrete accumulating only at zero, and all nonzero elements of the spectrum are eigenvalues of finite multiplicity. Suppose $u$ is an eigenvalue of $K$ with eigenvector $\mu$. Then for arbitrary $v\in V$,
\begin{align*}
\mathfrak{t}(u,v) &= \mathfrak{t}_\sigma(u,v) - \sigma(u,v) \\
  &= \frac{1}{\mu}\mathfrak{t}_\sigma(Ku, v) - \sigma(u,v) \\
  &= \bigg(\frac{1}{\mu}-\sigma\bigg)(u,v)
\end{align*}
and so we have that $\frac{1 - \mu\sigma}{\mu}$ is an eigenvalue of $\mathfrak{t}$ with eigenvector $u$. Confusion Does it follow that
  $$ \mbox{spectrum of } T = \bigg\{ \frac{1-\mu\sigma}{\mu}\ \bigg|\ \mu\in\mbox{spectrum of }K - \{0\} \bigg\}? $$ Thoughts I've been worrying at this for a couple of days and trying to prove something like the following: If $\lambda$ is in the resolvent set of $T$, then $\frac{1}{\sigma+\lambda}$ is in the resolvent set of $K$. This would immediately imply what I want modulo showing that that eigenvalues of $\mathfrak{t}$ are contained in the spectrum of $T$. (This is not a concern, see the note below.) Some algebra and head-scratching later, nothing has panned out -- it looks promising, but I haven't found the zinger yet. Maybe more to the point, I don't have enough intuition for where the algebra ought to go. I've started by assuming that $u$ is a $\mu$-eigenvector of $K$ in $H$ (after extending showing $K$ is symmetric and extending $K$ to a map $H\to H$ by density of $V$ and compactness), assuming that $\frac{1}{\lambda + \sigma}$ is in the resolvent set of $T$, and showing that we cannot have $\mu = \frac{1}{\lambda + \sigma}$, but this approach doesn't seem to rely on the assumption that the domain of $(T-\lambda)^{-1}$  is all of $H$, nor that it is bounded. I'm aware that this closely follows standard reference materials (Gilbarg-Trudinger Ch 8, Evans ch 6.5). In fact in Evans 6.5, at the end of the proof of Theorem 1, Evans writes, ""But observe as well that for $\eta\neq 0$, we have $Sw = \eta w$ if and only if $Lw = \lambda w$ for $\lambda = \frac{1}{\eta}$."" You might interpret this question as, ""why does the 'only if' follow?"" There are at least two other approaches I'm aware of to showing the spectrum of $T$ is discrete. One uses the Rayleigh quotient, shows there exists a lowest eigenvalue, and proceeds by induction to exhaust $H$ with orthonormal eigenspaces (e.g. Gilbarg-Trudinger, last section of ch 8). The other uses the theory of spectral measures (e.g. ch 13 of Green Rudin). I'd prefer to avoid those approaches if possible because I'd like to understand all three in as much detail as possible, in order to more thoroughly understand how they relate to each other. (My intuition is that the exhaustion proof is related to how this proof would go if one unpacked the spectral theorem for compact operators, while the theory of spectral measures is ""not homotopic,"" as it were, to this proof, but that's not yet clear to me.) Anyway, I'm missing a piece of the puzzle here and I'm not sure where the missing piece fits to look more closely for it. Notes In the context of PDEs I'm happy to assume the facts bolded in the setup: $V$ is related to a Sobolev space $W^{1,1}$ so Rellich-Kondrachov comes into play, and elliptic regularity gives that the eigenvectors of $\mathfrak{t}$ are in fact eigenvectors of $T$. If you've read this far, thanks for making it through my long-winded question :)","['eigenvalues-eigenvectors', 'compact-operators', 'functional-analysis', 'spectral-theory', 'unbounded-operators']"
2155696,Simple Connectedness in the Plane,A set is simple-connected if any two paths with the same endpoints are (path) homotopic. Show that this is equivalent to any loop being homotopic to the trivial loop.,"['general-topology', 'complex-analysis', 'real-analysis', 'algebraic-topology']"
2155710,Number of non singular matrices over a finite field of order 2,"I have to find out the number of $3Ã—3$ non singular matrices over a field of order $2$ .
I tried in the following way.
First to find out a non singular matrix $A,$ clearly any row of $A$ can't be full of $0$ s.
So the first row (say) can be filled up by $(8-1)$ ways.
Once the row is filled up,the next row can't be the same and also can't be full of zeros,so we can fill the next row by $
(8-2)$ ways.
And at last the third row also can't be full of zeros,same as the first row,and same as the second row also.So we have $(8-3)$ choices.
Hence the number of non singular matrices seems to be $7Ã—6Ã—5=210$ .
Am I right? Or there are more non singular matrices ? May be less also.
Please correct me if I am wrong.
Thank you.",['linear-algebra']
2155724,Definition of analyticity in complex analysis,"In my complex analysis class, I learned that a complex function $f(z)$ is said to be analytic at $z_0$ if there is a neighborhood around $z_0$ in which $f$ is differentiable. Then, I learned various properties of analytic functions, but I never quite understood why the notion of pointwise differentiability is not enough and we in fact need differentiability ""in nearby neighborhoods"". What is the importance/role of ""neighborhoods"" in this context? What goes wrong if one replaces analyticity with differentiability? I thought that when one talks about pointwise differentiability, one would like to work in open sets and since analyticity and differentiablity are the same in open sets, I do not see what is important about defining analyticity the way it is defined","['analyticity', 'complex-analysis', 'analytic-functions', 'definition']"
2155758,"Why doesn't $\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}$ exist?","So, my book has this homework problem that has me pulling my hair out over. It asks us to evaluate the limit below - or rather, by trying along paths of the x and y axes, to show that it doesn't exist. $\lim\limits_{(x,y) \to (0,0)} \frac{3}{x^2+2y^2}$ When I evaluate along $y=0$ I get $\lim\limits_{(x,0) \to (0,0)} \frac{3}{x^2+2(0)^2}$ Which to me looks like it should just evaluate to +âˆž, just as along $x=0$ I get $\lim\limits_{(0,y) \to (0,0)} \frac{3}{0^2+2y^2}$ which also looks like it should evaluate to +âˆž. The flimsiest thing I can get about it not existing is that the Y term increases faster than the X term, but the book's answers section tells me that the limit doesn't exist along $x=0$. I'm probably just forgetting something really basic like a moron, but can someone beat me over the head, please?","['multivariable-calculus', 'limits']"
2155795,Solve the differential equation it's a modified logistic model,"$$\frac{dx}{x\left(1-\frac{x}{a}\right)\left(\frac{x}{b}-1\right)}=cdt$$ i tried to solve this equation by variable separable, but i am not able to do the partial fraction properly","['integration', 'ordinary-differential-equations', 'calculus']"
2155812,"The ""fake $\mathrm{GL}_2(\mathbb{F}_3)$"" and the binary octahedral group","In this answer ,
it is mentioned that the binary octahedral group can be realized as $\mathrm{GL}_2(\mathbb{F}_3)$,
with ""certain elements replaced with scalar multiples in $\mathrm{GL}_2(\mathbb{F}_9)$.""
(Apparently this has been called the ""fake $\mathrm{GL}_2(\mathbb{F}_3)$""
by Marty Isaacs.)
What are the details of that construction? In particular,
I'd like to see an analogy to
(or at least a spin-off of)
the following construction that we have for the binary tetrahedral group.
The isometries of the regular tetrahedron can be given by permutations of its four vertices.
If we look at the action by $\mathrm{SL}_2(\mathbb{F}_3)$
on $\mathbb{P}^1(\mathbb{F}_3)$,
we can interpret it as giving a map $\mathrm{SL}_2(\mathbb{F}_3)\rightarrow S_4$
by seeing what it does to the points
$\{0,1,2,\infty\}\in\mathbb{P}^1(\mathbb{F}_3)$
(represented here by
$\begin{pmatrix}
0\\
1
\end{pmatrix}$,
$\begin{pmatrix}
1\\
1
\end{pmatrix}$,
$\begin{pmatrix}
2\\
1
\end{pmatrix}$,
$\begin{pmatrix}
1\\
0
\end{pmatrix}$
respectively).
This map is surjective and $2$-to-$1$
with kernel $\bigg\{\begin{pmatrix}
\pm 1 & 0\\
0 & \pm 1
\end{pmatrix}\bigg\}$,
showing that $\mathrm{PSL}_2(\mathbb{F}_3)$
is isomorphic to the tetrahedral group.
This gives a cool way of seeing why $\mathrm{SL}_2(\mathbb{F}_3)$
is called the binary tetrahedral group. We have a similar thing for the binary icosahedral group and $\mathrm{SL}_2(\mathbb{F}_5)$,
but how do we see something similar for the binary octahedral group?","['finite-groups', 'exceptional-isomorphisms', 'representation-theory', 'group-theory', 'symmetric-groups']"
2155825,Showing that $f$ is a constant function?,"I am working on the question: Suppose $f$ is entire, with real and imaginary parts $u$ and $v$ satisfying $u(z) v(z) = 3$ for all $z$.
  Show that $f$ is constant. I understand that I must show $f'(z) = 0$. The first thing I did here was to set up the Cauchy Riemann Equations: $u_x = v_y$, and $u_y = -v_x$. However, I am not sure how to use $u(z) v(z) = 3$. Clearly I have to take some sort of derivative, but I'm not very familiar with how partial derivatives work. Can I take the partial derivative of both sides of $u(z) v(z) = 3$ and say $u_x (z) v(z) + u(z) v_x(z) = 0$? If yes, how should I proceed afterwards? I have worked under this assumption, but I still didn't get the desired $f'(z) = 0$.","['complex-analysis', 'partial-derivative', 'complex-numbers']"
2155855,Explain approximate lines in graph of this function,"Sorry that this is a long question; the crux of it is that I want to know why lines appear in the graph of the function ($\varphi^\infty(x)$) I've defined. Define $\varphi(x)$ as follows: If the decimal representation of $x\in \mathbb{R}$ is $x=\sum\limits_{n=-\infty}^\infty a_n 10^n$ where $a_n\in\{0,1,2,...,9\}$ and $|\{a_n\;|\;n>0\}|$ is finite let: $$\varphi(x)=\sum_{n=-\infty}^\infty a_n\left(\max{\{a_k\;|\;k\in\mathbb{N}\}}+1\right)^n$$ (i.e. read $x$'s base $10$ expansion as if it is written in base $1+\max{\{a_k\}}$). Then let $\varphi^\infty(x)$ be the value obtained by repeated applications of $\varphi(x)$ until a stable value is reached (if it exists), i.e. $\varphi^\infty(x)=\varphi(\varphi(...(\varphi(x))...))$. To illustrate, $\varphi(57)=57_8=47_{10}$, so $\varphi(57)=47$, and $\varphi(47)=47_8=39_{10}$ so $\varphi(47)=39$, and $\varphi(39)=39_{10}$, so no further applications of $\varphi(x)$ will change it, so $\varphi^\infty(57)=39$. Similarly $\varphi(\frac{1}{3})=0.333..._4=1$ and $\varphi(1)=1$ so $\varphi^\infty(\frac{1}{3})=1$. For any number $y$ with $9$ in its decimal expansion we will obviously have $\varphi(y)=\varphi^\infty(y)=y$; $\varphi^\infty(x)$ seems mathematically irrelevant, but it interests me; when graphed for integer $x$, we get linear patterns. Graphs of $\varphi^\infty(x)$ at integer $x$ over increasing domains (full size versions here and here and here and here ): The top line (slope $1$) is numbers containing a $9$; but there are lines of other slopes below (one in red); the following zooms show they are made up of smaller line segments of different slopes. Graphs of $\varphi^\infty(x)$ at integer $x$; the first shows the boxed area above, the second is a zoom for large $x$, and the third is a zoom into one of the line segments in the previous graph (full size versions here and here and here ): $\hskip1.7in$ At the smallest scale line segments with slope $1$ will occur; this can be seen in $\varphi(x)$ which displays the same pattern (I have only focused on $\varphi^\infty(x)$ because the lines are more obvious). Graphs of $\varphi(x)$ at integer $x$ over increasing domains (full size versions here and here and here ): $\hskip1.7in$ I cannot explain why the points (which on the small scale form short lines of slope $1$) on the large scale arrange themselves approximately into lines of varying slopes passing through the origin. Why do the values of $\varphi^\infty(x)$ and $\varphi(x)$ approximately fit onto clear lines like the ones highlighted above? I don't know if similar lines occur for non-integer $x$; points where $\varphi^\infty(x)$ is continuous are obviously dense in $\mathbb{R}$; I can't see a good way of plotting $\varphi(x)$ for real $x$ ($\varphi(\frac{2}{3})=0.666..._7=1$ but a computer might give $\varphi(\frac{2}{3})=0.666...7_8\overset{!}{=}0.857..._{10}$). It exists for all integer $x$, but I can't prove $\varphi^\infty(x)$ exists for all $x$ (for some $x$, we might never get a number with $9$ in the decimal expansion). But my main question is the following: Can anyone explain why the approximations to lines passing through the origin occur in the above graphs? What determines the slope of the lines? The above graphs were generated by the following python script: from __future__ import division
from pylab import *
from fractions import Fraction

def phi(number):
    number=float(number); number=str(number); digits=[]
    for dig in number:
        digits.append(dig)
    digitstest=digits; digitstest.remove('.'); m=int(max(digitstest))+1
    firstdigs=[]
    for dig in number:
        if dig=='.':
            break
        firstdigs.append(int(dig))
    y=0; k=size(firstdigs)-1; n=k
    while n-k<size(digitstest):
        y=y+(int(digitstest[n-k]))*(m**(size(firstdigs)-n+k-1)); k=k-1
    if y-int(y)==0:
        y=int(y)
    return(y)

def phi_infinity(number):
    prev=-1
    while number!=prev:
        prev=number; number=phi(number)
    return(number)

a=int(input(""\nLower bound?\n"")); b=int(input(""\nUpper bound?\n""))
i=linspace(a,b,b-a+1); k=[]
for n in i:
    k.append(phi_infinity(int(n)))
plot(i,k,'.',markersize=3); show()","['graphing-functions', 'decimal-expansion', 'functions', 'sequences-and-series', 'approximation']"
2155879,How can one show that $\int_{0}^{\infty}\left({1\over 1+nx^n}-e^{-nx^n}\right)\cdot{\mathrm dx\over x^{1+n}}=1-\gamma?$,"Consider $$\int_{0}^{\infty}\left({1\over 1+nx^n}-e^{-nx^n}\right)\cdot{\mathrm dx\over x^{1+n}}=1-\gamma\tag1$$
  $n\ge1$;(integers) n seem to be not involved in the closed form(why?) How does one show that $(1)$ converges to $1-\gamma?$ An attempt: $$(1+nx^n)^{-1}=1-nx^n+(nx^n)^2-(nx^n)^3+\cdots$$ $(1)$ becomes $$\int_{0}^{\infty}\left(x^{-n-1}-nx^{-1}+n^2x^{n-1}-n^3x^{2n-1}+\cdots-{e^{-nx^n}\over x^{n+1}}\right)\mathrm dx\tag2$$ $(2)$ divgerges, how  else can we tackle $(1)$? Or do we have to differentiate m times w.r.t n","['integration', 'definite-integrals', 'calculus', 'euler-mascheroni-constant']"
2155922,"Is there a notion like ""minimal presentation""?","Suppose $G=\langle S\mathrel| R\rangle$ is a presentation of a group (or semigroup) such that: $G$ is not generated by any proper subset of $S$. For any $r\in R$ we have $\bigl\langle S\bigm| R\setminus\{r\}\bigr\rangle\neq G$. Does such a presentation of $G$ have a name? (Note that I understand this presentation need not be unique, and maybe even need not exist, but it does exist for my $G$ and I don't want to reinvent the wheel. I also know that ""minimal"" is used when speaking about finitely generated groups, but mine is not one.)","['terminology', 'abstract-algebra', 'semigroups', 'group-theory']"
2156005,Lp convergence in sobolev space implies sobolev convergence?,"I know that $L^p$ convergence in general does not tell anything about sobolev convergence, but now what if we know that the $L^p$-convergent sequence consists of only sobolev functions and its $L^p$ limit is again in the sobolev space. Or in other words: We have a sequence $(u_n)_{n>0}$ in $W^{k,p}$ and a function $u \in W^{k,p}$ such that $||u_n - u||_{L^p} \to 0 \quad \text{as } n\to \infty$. Then can we say that $||u_n - u||_{W^{k,p}} \to 0 \quad \text{as } n \to \infty$ ? I have been trying for some time to wrap my head around this and I feel that it must be true but I can't seem to neatly write down why. Any clues would be more than welcome.","['functional-analysis', 'sobolev-spaces', 'convergence-divergence']"
2156023,Prove that a convex 3D polyhedron with all faces rectangular is a cuboid,"Seems pretty obvious, but that doesn't always mean a proof is trivial, or even that the result is true. Clearly the assumption of convexity can't be dropped, because otherwise one can start with a cuboid and scoop out a smaller cuboid-shaped hollow in one of its faces. edit: I should add that I have what seems a pretty simple proof; but I'm curious to see if there are other ways of looking at this.",['geometry']
2156072,Prove the equivalence of norms on the Hardy space $H^2(\mathbb{D})$.,"Let $H^2(\mathbb{D})$ be the space of all functions $f$ holomorphic on the open unit disk $\mathbb{D}$ such that the Hardy norm, given below, is finite:
$$||f||_H^2 = \sup_{0<r<1}\frac{1}{2\pi}\int_0^{2\pi} |f(re^{i\theta})|^2 \ \mathrm{d}\theta.$$ I have already shown that the evaluation $f\mapsto f(z) \ (z\in\mathbb{D})$ is continuous with respect to the norm $||\cdot||_H$. Now let $||\cdot||$ be any other norm with respect to which $H^2(\mathbb{D})$ is a Banach space, and for which the evaluations $$f\mapsto f(1/(n+1)) \ (n=1,2,...)$$ are continuous. How do I prove that $||\cdot||$ is equivalent to $||\cdot||_H$? I know that, due to a consequence of the Open Mapping Theorem, one need only show one inequality involving these two norms. Moreover, using the Uniform Boundedness Principle, there is a constant $K$ such that, for all $n\in\mathbb{N}, f\in H^2(\mathbb{D})$, we have $$|f(1/(n+1))|\le K<\infty.$$","['complex-analysis', 'hardy-spaces', 'hilbert-spaces']"
2156094,"When traversing a graph, what is the pmf of the degree of the next node?","I have a graph G(E,V) with known degree distribution $p_k$. I wish to obtain the generating function of the degree distribution over $k$ when following a randomly selected edge while traversing the graph. I'll call that distribution $p'_k$. My reasoning is that, considering a node $u$, a given other node $v$ has $p_k$ of having $k$ edges, each of which has an independent probability of being connected to $u$, so we must have $p'_k \propto kp_k$ which, including normalization, gives a GF $x\frac{G'_0(x)}{G'_0(1)}$, consistent with this paper I'm trying to follow. However, I want to exclude the node visited prior to $u$. The paper's author simply divides the GF by $x$, so it becomes $$\frac{1}{G'_0(1)}\sum_k k p_k x^{k-1}$$
I don't understand this reasoning because the exponent refers to the degree of $v$ and it is one of the edges from $u$ that becomes excluded when travelling back is disallowed.","['generating-functions', 'graph-theory', 'probability-theory']"
2156164,Question on Notation ${V\choose 2}$,"Somewhere I've read the following: A graph is a pair $(V, E)$ consisting of a set $V$ and a set $E\subseteq {V\choose 2}$ of edges. What does the notation ${V\choose 2}$ mean? Where can I read a definition of this notion (specifically, is there a wikipedia page about it?)? I think it means the set of all $2$-element subsets of $V$. Can one generally define ${A\choose \kappa}$, where $A$ is any set and $\kappa$ any cardinal, to be the set $\{T\subseteq A: |T| = \kappa\}$ (where $|T|$ denotes the cardinality of $T$)?","['graph-theory', 'notation', 'soft-question', 'elementary-set-theory']"
2156210,Prove that $|AB - \lambda I| = |BA - \lambda I|$.,"Suppose that one has two matrices $A$, $B$. Then Prove that $$|AB - \lambda I| = |BA - \lambda I|,$$
  where $|\cdot|$ denotes the determinant, $I$ - identity matrix and $\lambda \in \mathbb{C}$. Note that $A$ and $B$ are not necessary invertible. For invertible matrices I easily found
$$|AB - \lambda I| = |B(AB - \lambda I)B^{-1}| = |BA - \lambda I|.$$","['matrices', 'determinant']"
2156218,University plane geometry question about splitting the diagonal of a parallelogram into 3 equal parts,"$ABCD$ is a parallelogram. If the two sides $\overline{AB}$ and $\overline{AD}$ are bisected in $E$ and $F$ , respectively, show that $\overline{CE}$ and $\overline{CF}$ when joined cut the diagonal $\overline{BD}$ in three equal parts. I have no idea how to do this question, any help would be appreciated.","['plane-geometry', 'geometry']"
2156223,Integral extended to a circumference,"I have to calculate the integral of $g'$ extended to the circumference with center at the origin and radius $\pi$ , oriented counterclockwise, having $$ g = \frac{e^{iz}}{z^{1/3}} \ \ \ \ , \ \ arg(z) \in (-\pi,\pi) $$ I thought I could use the Residue theorem , but $Res(g';0) = 0$ , and the result to this question should be $$ \int_{+\gamma} g' = i \frac{\sqrt3}{\pi^{1/3}} $$ Can somebody please help me?","['complex-analysis', 'integration', 'residue-calculus']"
2156238,Why is a matrix invertible when its row-echelon form has no zero row?,"If the row echelon form of a square matrix has no zero row, it is invertible. Otherwise, it is singular. Why? If the row echelon form has a zero row, in a linear system, it has either no solution or infinitely many solutions. So, is invertibility linked to having only one solution? Is there a geometrical interpretation for my question?","['matrices', 'linear-algebra']"
2156257,The Parseval theorem applied to 8 well known functions. Could this be extended to L-series?,"In the shaded section below, I listed 8 well-known functions that nicely hang together: $$\displaystyle\frac{\beta(s)+\lambda(s)}{\kappa(s)+\alpha(s)}=\frac{\zeta_H\left(s,\frac12\right)+\eta_H\left(s,\frac12\right)}{\zeta(s)+\eta(s)}$$ Each of these functions has a Laplace/Fourier integral transform that can be ""plugged into"" the integral from the Parseval theorem : $$\small G_f(s):=\frac{1}{\pi\,\Gamma(s+1)}\int_{0}^{\infty} f\left(\frac{s+1}{2}+x\,i\right)\Gamma\left(\frac{s+1}{2}+x\,i\right)f\left(\frac{s+1}{2}-x\,i\right)\Gamma\left(\frac{s+1}{2}-x\,i\right)dx$$ Based on the example for $f(s)=\zeta(s)$ shown here , I managed to derive the new closed forms for each of the 8 functions and also listed them in the shaded area (note: only analytically continued to $\Re(s)>0$). All closed forms $G_f(s)$ are now directly related to $\zeta(s)$ (note that f.i. $\beta(s)$ wasn't). I then found that the same nice relation remains for $s \in \mathbb{C}$ and $\Re(s)>0$, i.e: $$\displaystyle\frac{G{_\beta}(s)+G_{\lambda}(s)}{G_{\kappa}(s)+G_{\alpha}(s)}=\frac{G_{\zeta_H}\left(s,\frac12\right)+G{\eta_H}\left(s,\frac12\right)}{G_{\zeta}(s)+G_{\eta}(s)}$$ 1) Is the fact that this relation remains the same a logical consequence of applying the Parseval theorem? (note that other relations like $\zeta(s)+\eta(s)=2\lambda(s)$ or $\alpha(s)+\lambda(s)=\zeta(s)$ do break down). 2) Besides $\zeta(s)$ and sums of $\zeta_H(s)$, the $\lambda$ and the $\beta$-functions are as $L(s,\chi_{4,1})$ and $L(s,\chi_{4,2})$ respectively, the only ones related to a Dirichlet L-series . However, I could not find any further connections like these in the ""G-world"". Do closed forms for L-series (other than mod 4) also exist after applying the Parseval theorem? Addition to question 2: I believe that I have now also found a closed ""G""-form for the Dirichlet L-series . With $\chi_{_{k,\,j}}(n)$ being a Dirichlet character with modulus $k$ and index $j$, it is well known that: $$\displaystyle L(s,\,\chi_{_{k,\,j}})=\frac{1}{k^s}\sum_{n=1}^k \chi_{_{k,\,j}}(n)\cdot\zeta_H \left(s,\frac{n}{k}\right)$$ Then I defined: $$\zeta_H^*(s,a)= \zeta_H(s,2a)-(2a-1)\,\zeta_H(s+1,2a)$$
and found that: $$\displaystyle G_{L(s,\,\chi_{{k,\,j}})}=\frac{1}{k^{s+1}}\left(\sum_{n=1}^{k} \chi_{_{k,\,j}}(n)^2\cdot\zeta_H^* \left(s,\frac{n}{k}\right)+2\sum_{n=1}^{k-2}\sum_{m=n+1}^{k-1}\chi_{_{k,\,j}}(n)\cdot\chi_{_{k,\,j}}(m)\cdot\zeta_H^* \left(s,\frac{n+m}{2k}\right)\right)$$ that seems valid for $\Re(s)>-1$, except for index $j=1$, when it is valid for $\Re(s)>1$ only. The latter can be analytically continued to f.i. $0 <  \Re(s) <1$ by just subtracting from the above: $$\displaystyle \small \frac{2}{s\,k}\,\left(L(s,\,\chi_{{k,1}})+\frac{1}{k^s}\sum_{n=1}^{k-2}\sum_{m=1}^{k-n-1}\chi_{_{k,1}}(m)\cdot\chi_{_{k,1}}(m+n)\cdot\left(\zeta_H \left(s,\frac{m}{k}\right)+\zeta_H \left(s,\frac{m+n}{k}\right)\right)\right)$$ $\\$
$\\$ $\displaystyle \small \sum_{n\ge1}\frac{1}{n^s}=$ Riemann $\zeta(s)$-function $\qquad \qquad \rightarrow  \small G_\zeta(s)=  \begin{cases} \zeta(s) - \zeta(s+1) &
 \qquad \qquad \qquad \qquad \Re(s)>1  \\ 
 \\ \zeta(s)-\zeta(s+1)-2\,\frac{\zeta(s)}{s}&
 \qquad \qquad \qquad \qquad 0<\Re(s)<1  \end{cases}\\$ $\displaystyle \small \sum_{n\ge1}\frac{1}{(2n)^s}=$ Self named $\alpha(s)$-function $\small =2^{-s}\zeta(s)$
  $\qquad \qquad \rightarrow \small G_\alpha(s)=  \begin{cases} \frac{1}{2^{s+1}}\big(\zeta(s)-\zeta(s+1)\big)
 & \qquad \qquad \quad \quad \,\, \Re(s)>1  \\ \\
 \frac{1}{2^{s+1}}\big(\zeta(s)-\zeta(s+1)\big)-\frac{\alpha(s)}{s} & \qquad \qquad \quad \quad \,\, 0<\Re(s)<1  \end{cases}\\$ $\displaystyle \small \sum_{n\ge0}\frac{1}{(2n+1)^s}=$ Dirichlet $\lambda(s)$-function $=\small (1-2^{-s})\zeta(s)$ $\qquad \qquad \rightarrow \small G_\lambda(s)=  \begin{cases} \frac{1}{2^{s+1}}\zeta(s)& \qquad \qquad \quad \qquad \qquad \qquad \,\,\, \Re(s)>1  \\ \\
 \frac{1}{2^{s+1}}\zeta(s)-\frac{\lambda(s)}{s} & \qquad \qquad \qquad \quad \qquad \qquad \,\,\, 0<\Re(s)<1  \end{cases}\\$ $\displaystyle \small \sum_{n\ge 0}\frac{1}{(a+n)^s}=$ Hurwitz $\zeta_H(s,a)$-function $=\small \zeta_H\left(s,\frac12\right) = (1-2^s)\zeta(s)$ $\qquad \qquad \rightarrow \small G_{\zeta_H}(s)=   \begin{cases}
 \zeta_H(s,2a)-(2a-1)\,\zeta_H(s+1,2a) &  \,\,\,\, \Re(s)>1  \\ \\
 \zeta_H(s,2a)-(2a-1)\,\zeta_H(s+1,2a)-2\,\frac{\zeta_H(s,a)}{s} & 
\,\,\,\, 0<\Re(s)<1  \end{cases}\\$ $\displaystyle \small \sum_{n\ge1}\frac{(-1)^{n-1}}{n^s}=$ Dirichlet $\eta(s)$-function $\small=(1-2^{1-s})\zeta(s)$ $\qquad \qquad \rightarrow \small G_\eta(s) = \eta(s+1)-\eta(s) \,\, \qquad \qquad \qquad \qquad \qquad \qquad \,\,\,\,\Re(s)>0\\$ $\displaystyle \small \sum_{n\ge1}\frac{(-1)^{n-1}}{(2n)^s}=$ Self named $\kappa(s)$-function $=\small 2^{-s}\eta(s)$ $\qquad \qquad \rightarrow \small G_\kappa(s)=\frac{1}{2^{s+1}}\big(\eta(s+1)-\eta(s)\big) \qquad \qquad \qquad \quad \qquad \quad \,\,\, \Re(s)>0\\$ $\displaystyle \small \sum_{n\ge0}\frac{(-1)^n}{(2n+1)^s}=$ Dirichlet $\beta(s)$-function $\small =4^{-s}\big(\zeta\left(s,\frac14\right)-\zeta\left(s,\frac34\right)\big)$ $\qquad \qquad \rightarrow \small G_\beta(s) = \frac{1}{2^{s+1}}\,\eta(s) \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \, \Re(s)>0\\$ $\displaystyle \small \sum_{n\ge 0}\frac{(-1)^n}{(a+n)^s}=$ Self named $\eta_H(s,a)$-function $=\small 2^{-s}\big( \zeta_{H}\left(s,\frac{a}{2}\right)-\zeta_{H}\left(s,\frac{a+1}{2}\right)\big)$ $\qquad \qquad \rightarrow \small G_{\eta_H}(s) = \eta_H(s,2a)-(2a-1)\,\eta_H(s+1,2a) \,\, \qquad \qquad \quad \,\Re(s)>0\\$","['dirichlet-series', 'integral-transforms', 'number-theory', 'riemann-zeta', 'sequences-and-series']"
2156291,Number of integer solutions of two inequalities,"The problem is the following: I have the condition
$$5\le3(x_1+x_2+x_3+x_4)+2(x_5+x_6+x_7+x_8+x_9+x_{10})\le12$$
where each $x_i$ can be either 0 or 1. What I need is the total number of possible combinations of $x_i$ that satisfy this condition. I get the feeling that this fits into a stars-and-bars type of problem, but in all questions I have found they considered the $x_i$ to have some range of values, not just 0 or 1. I could rename the variables such that the condition would be just the sum of the $x_i$, but I would still need to deal with the fact that the variables could only assume two values.","['combinatorics', 'discrete-mathematics']"
2156311,How to evaluate the integral $\int_0^{\infty}\mathrm{d}x\frac{\sin(x)\sin(ax)}{\pi^2-x^2}e^{-ibx^2}$?,Note that $a$ and $b$ are positive constants. Can this integral be evaluated in closed form ? $$\int_0^{\infty}\mathrm{d}x\frac{\sin(x)\sin(ax)}{\pi^2-x^2}e^{-ibx^2}$$,"['complex-analysis', 'integration', 'definite-integrals', 'contour-integration', 'special-functions']"
2156316,Prove that $\left(\frac{3+\sqrt{17}}{2}\right)^n + \left(\frac{3-\sqrt{17}}{2}\right)^n$ is always odd for any natural $n$.,"Prove that $$\left(\frac{3+\sqrt{17}}{2}\right)^n + \left(\frac{3-\sqrt{17}}{2}\right)^n$$ is always odd for any natural $n$. I attempted to write the binomial expansion and sum it so the root numbers cancel out, and wanted to factorise it but didn't know how. I also attempted to use induction but was not sure how to proceed.","['algebra-precalculus', 'recurrence-relations', 'proof-writing', 'quadratics']"
2156355,Integral $\int_0^1 \frac{\sqrt{1-x^4}}{1+x^2}dx$=?,"As title, how could one calculate $$\int_0^1 \frac{\sqrt{1-x^4}}{1+x^2}dx$$ ? I thought of using $x=\sqrt{\sin u}$ substitution but to no avail.","['integration', 'definite-integrals']"
2156389,What is the distribution of this random variable? (Practicing for an exam),"I have in my possession a theoretical and practical test that evaluates the knowledge of a semester of the subject Probability and Statistics, that is, a written exam. The same is from July 30, 2013. Clarification: I am not allowed to go online during an evaluation. This test I am doing at home to evaluate myself. Activity 1 The $ 35 $ % of the students who took the first semester of the Computer Technologist, passed the subject MDyL1. It is considered a sample of $ 10 $ students of that semester and the random variable $ X $ : number of students who have passed the subject MDyL1, among the selected $ 10 $ . a) Calculate the probability that more than two students in the sample have passed the subject MDyL1. b) Find the probability that less than half of the students in the sample have passed the subject MDyL1. c) Determine $ E (X) $ and $ Var (X) $ . Can the random variable $X$ have a binomial distribution? That is, each student pass a subject independently. It is known that the probability of that happening is $ p = 0.35 $ , then the number of repetitions of that experiment should be $ n = 10 $ . That is, $ X $ ~ $B (10,0.35)$ , so that: -Using the binomial distribution table- a) $P(X>2) = 1-P(Xâ‰¤2) = 1-0.2616 = 0.7384$ b) $P(X<5) = P(Xâ‰¤4) = 0.7515$ c) $E(X) = np = 10 \times 0.35 = 3.5$ $Var(X) = np(1-p) = 3.5 \times 0.65 = 2.275$ Is it okay to assume that each student passes a subject independently? Because the exercise does not clarify it. It says that 35% passes the subject, but not that each has a 35% chance of passing. I hope you can help me. Thank you very much.","['self-learning', 'percentages', 'probability', 'probability-distributions']"
2156410,Prove that these definitions of $e$ are equivalent [duplicate],"This question already has answers here : Particular definition of e (3 answers) Closed 7 years ago . Im sorry if this is stupid or obvious, but why $$ e=\lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n$$ AND $e$ is the unique positive number for which $$ \lim_{h\rightarrow 0}\frac{e^h-1}{h}=1$$? I mean, how do we know that these two definitions are equivalent? Again, maybe it's easy, but I'm just beginning calculus and our teacher just dropped those two definitions...","['calculus', 'limits']"
2156457,What space corresponds to the localisation of the ring of continuous functions?,"Suppose $A$ is a commutative Banach algebra. By Gelfand duality there is a compactum $X$ such that $A = C(X)$ is the ring of continuous functions. The space $X$ can be recovered as the space of characters on $A$. That is to say multiplicative linear functionals $A \to \mathbb R$ under the weak$^*$ topology. Observe this topology on the space of characters does not depend on the topology of $A$. Now let $f \in C(X)$ be any non-invertible element. In other words $f$ has a zero. We can localise the ring $C(X)$ at $f$ to get the ring of 'formal fractions' $C(X)_f = \displaystyle  \{\frac{g}{f^n} \colon g \in C(X), n \in \mathbb N\}$. There is a natural embedding $C(X) \to C(X)_f$ but I am unaware if $C(X)_f$ carries a compatible Banach algebra structure. By this I mean a norm under which it is complete and the embedding is an isometry. Nevertheless we can consider the space of characters on $C(X)_f$ and give that the weak$^*$ topology. Under what conditions is the character space of $C(X)_f$ some compactum $Y$? When will we have $C_f(X) = C(Y)$? Does $Y$ have a topological characterisation in terms of the space $X$ and function $f$?","['gelfand-representation', 'banach-algebras', 'localization', 'abstract-algebra', 'functional-analysis']"
2156542,How does one show that $\sum_{k=2}^{\infty}{(-1)^k\over k+1}\cdot{ \lceil \log_2(k) \rceil}=1-2\gamma?$,Consider $$\sum_{k=2}^{\infty}{(-1)^k\over k+1}\cdot{  \lceil \log_2(k) \rceil}=1-2\gamma\tag1$$ How does on show that $(1)$ converges to $1-2\gamma?$,"['sequences-and-series', 'euler-mascheroni-constant', 'ceiling-and-floor-functions']"
2156549,Evaluate integral with integer part,"I have to evaluate $$\int _0^2\:\frac{x-\left[x\right]}{2x-\left[x\right]+1}dx$$ Where $[x] = floor(x)$ I tend to write it like this, but I think i'm missing the point $x = 2$ $$\int _0^2\:\frac{x-\left[x\right]}{2x-\left[x\right]+1}dx=\int _0^1\:\frac{x}{2x+1}dx+\int _1^2\:\frac{x-1}{2x}dx = 1 - \frac{1}{4} \cdot \ln 3$$ The correct answer is $1 - \frac{1}{4} \cdot \ln 12$",['integration']
2156550,"A deck of cards is well shuffled. The cards are dealt one by one, until the first time an ace appears","A deck of cards is well shuffled.  The cards are dealt one by one, until the first time an ace appears. Find the probability that no kings, queens or jacks appear before the first ace. Find the probability that exactly one king, exactly one queen and exactly one jack appear (in any order) before the ace first. Please review my methods and answers: One: Find the probability that no kings, queens or jacks appear before the first ace. Notation: Let b cards precede the first ace Let A be the first Ace Let a cards follow the first ace Conditions: b will range from 0 to 36 ($52 - (12 + 4)$) A will be one of four Ace cards a will range from 51 to 15 ($12 + 3$) In order to maintain equal likeliness of all configurations (an assumption), we will find the number of valid permutations and divide by the total number of permutations which will result in the probability of a valid configuration, denoted P(A). $$ P(A) = \frac{\textrm{number of valid permutations}}{\textrm{number of all permutations}} $$ $$P(A) = \frac{1}{52!}\sum_{i=0}^{36}\left(\frac{36!}{(36-i)!}\frac{4!}{(4-1)!}(51-i)!\right) $$
where within the summation, the first expression selects and permutes the number of cards preceding the first ace, the second expression selects the first ace, the third expression selects and permutes the cards which follow the first ace. The expression reduces to:
$$ P(A)=\frac{4\times36!}{52!}\sum_{i=0}^{36} \frac{(51-i)!}{(36-i)!}$$
$$P(A) = 0.25$$ Two: Find the probability that exactly one king, exactly one queen and exactly one jack appear (in any order) before the ace first. There are $\frac{4!}{(4-1)!}$ ways to select one of four elements, be it an ace, jack, queen or king. Proceed in a similar fashion to part 1. by finding the quotient of the total number of permissible permutations and the total number of permutations, labelling it $P(B)$. $$P(B) = \frac{1}{52!} \sum_{i=0}^{36}\left(\frac{4!}{(4-1)!}\frac{4!}{(4-1)!}\frac{4!}{(4-1)!}\frac{1}{3!}\times\frac{36!}{(36-i)!}\frac{4!}{(4-1)!}(48-i)!
\right)$$
Where the first three expressions in the summation are the selection of a king, queen and jack, the fourth expression allows these three cards to be chosen in any order.  The expressions following the multiplication symbol are: selection of a number of cards ( two, three, ... , ten) to follow the face cards and precede the first ace. selection of the first ace selection of the remaining cards The expression collapses to:
$$ P(B) = \frac{4^4 \times 36!}{52!\times 3!} \sum_{i=0}^{36}\frac{(48-i)!}{(36-i)!}
$$
$$P(B) = 2.4752\times{10}^{-5} $$ Thank you in advance","['combinatorics', 'probability', 'card-games']"
2156576,Continuity of partial derivatives imply differentiability - but I have a counter-example,"I must be doing something wrong here. Let $f: \mathbb R^2 \to \mathbb R$ be defined as $$f(x,y)= \begin{cases} 3x+4y, &\text{if } xy \neq 0 \\ 0, &\text{if } xy =0 \end{cases}$$ i.e. $f$ is $0$ on the $x-$axis and on the $y-$axis whereas $f(x,y)=3x+4y$ everywhere else. I want the check the differentiability at $(0,0)$. $\bullet$ Clearly, $\ f$ is continuous. $\bullet$ Also, I think that $f_x=0=f_y$, the partial derivatives are just the function restricted to $x-$axis and $y-$axis. I found the picture below on the web, it gives a geometric interpretation of partial derivatives. $\bullet$ So, the partial derivatives exist and continuous. Then, $f$ is differentiable at $(0,0)$. $\bullet$ However, I think that $f$ is not differentiable at the origin, because... Umm... The directional derivative is not continuous. A little direction change may yield a huge change in directional derivative. Is $f$ differentiable at the origin? What is wrong about my reasoning? $$$$ $$$$ Edit: So, isn't $f_x$ defined as the slope of the tangent line to the curve that is given by the intersection of the graph of $f$ and the $xz-$plane? (that is the red line in the above picture)","['multivariable-calculus', 'partial-derivative', 'derivatives']"
2156596,For a given $u(z)$ are there any $v(z)$ which make $f(z) = u(z)+iv(z)$ differentiable?,"I got the answer ""no such $v$ exists"" for both exercises. However, I am very surprised, since the $u's$ I have been given seem very nice and arbitrary. I would like to know if I solved them correctly, and if so, what underlying reason makes these seemingly nice functions so hostile? $1) u(z) = x^3$ Using the Cauchy Riemann equations, we need $3x^2 = v_y$ and $0 = v_x$. The first equation gives us $v(z) = 3x^2y + g_1 (y)$, and the second gives us $v(z) = g_2(y)$. There is no $v(z)$ which satisfies these requirements. $2) u(z) = x^2+y$ Using the Cauchy Riemann equations, we need $2x = v_y$ and $-1 = v_x$. The first equation gives us $v(z) = 2xy + g_1 (y)$, and the second gives us $v(z) = -x + g_2(y)$. There is no $v(z)$ which satisfies these requirements.","['derivatives', 'complex-analysis']"
2156598,For $d >1$ is there a lattice with the following properties except hexagonal lattice and $d=2$?,"The properties which (as far as I know) are unique to the hexagonal lattice: All of the cells have the same shape and there is perfect translatonal symmetry when shifting between the cells; Each cell is completely surrounded by its closest neighbors (where we define 'closest' in terms of Euclidean distance between the centers of the cells) - i.e. there is no path from a cell which doesn't go through one of its closest neighbors first (we are including the boundaries of the cells of course). Again, as far as I know only hexagonal lattice fits these properties. And I suppose an 1D lattice with equal segments, but this is a trivial case. Square lattice ($d=2$) or cubic lattice ($d=3$) or any generalization for larger $d$ don't work, because there are ways to leave the cell without going through one of its closest neighbors first. Truncated octahedral lattice in $d=3$ (which I consider a natural generalization of hexagonal lattice) doesn't work as well, since we have square faces of a truncated octahedron, and the disance in this direction is larger than for hexagonal faces. Is there any other integer lattice for $d>1$ which has these two properties? Edit To clarify the second condition: Two cells which are not nearest neighbours are not allowed to touch (share boundary) (Thanks, @Simon) Important! The lattice have to be space-filling as well. Euclidean space filling in case someone is wondering.","['symmetry', 'integer-lattices', 'geometry']"
2156634,Confusion on Baby Rudin problem 2.16,"I am having a difficult time proving problem 2.16 (specifically that $E$ is closed) in Rudin's Principles of Mathematical Analysis . I realize that this question has been asked before here , but I believe my question is different enough to warrant a new question. The problem asks: Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space with $d(p,q) = \vert{p-q}\vert$.  Let $E$ be the set of all $p \in \mathbb{Q}$ such that $2<p^2<3$.  Show that $E$ is closed and bounded in $\mathbb{Q}$, but that $E$ is not compact.  Is $E$ open? My attempt: By definition $E$ is closed if every limit point of $E$ is an element of $E$. Thus, we want to show that if a point $p$ is a limit point of $E$, $p \in E$, that is $2<p^2<3$. To do this, let $p$ be a limit point of $E$. We then know that $$\forall r> 0, \exists q :d(p,q) < r$$ In other words $$ \forall r> 0, \exists q : q-r < p < q + r$$ Now squaring the inequality gives $$ q^2-2qr + r^2 < p^2 < q^2 + 2qr + r^2 $$ Thus I would like to find an $r$ such that $2<p^2<3$ as desired.  Unfortunately I am having a hard time constructing such an $r$.  Can anyone point me in the right direction? Moreover, I've seen similar problem to this one, in which the set $E$ is proven to be closed by showing that the complement of $E$ is open.  Is this usually the easier route to take?  In this case it would seem to require more work. I just have a hard time understanding where solvers obtain their $r$ values from, they seem to come out of thin air...","['general-topology', 'real-analysis']"
2156635,Counting pairs of squares in ${\mathbb Z}_n$ with certain distance,"Let $S_n = \{ x^2 \pmod{n} \mid x \in \mathbb Z \}$ denote the set of squares in ${\mathbb Z}_n$ . Define $S_n(d) = \{ (x, y) \in S_n^2 \mid x + d \equiv y \pmod{n} \}$ . Is there an explicit formula for $|S_n(d)|$ ? Update : As mentioned in comments, if $d \equiv 0 \pmod{n}$ , then $|S_n(d)| = |S_n|$ and there is a formula for it according to Walter D. Stangl's paper ""Counting Squares in ${\mathbb Z}_n$ "" (MAA link) (PDF link) . I am still looking for a general result where $d \not\equiv 0 \pmod{n}$ .","['number-theory', 'combinatorics', 'modular-arithmetic', 'elementary-number-theory']"
2156644,There cannot be an infinite AP of perfect squares.,"I could not find any existing questions on this site stating this problem. Therefore I am posting my solution and I ask for other ways to prove this theorem too. The Question Prove that there cannot be an infinite integer arithmetic progression of distinct terms all of which are perfect squares. My attempt We shall prove it using contradiction. First off, there are a couple of things to notice which greatly simplify our discussion: The AP cannot be decreasing as eventually, the terms will be negative and perfect squares are non-negative. There has to be a non-zero, positive difference between the terms otherwise the terms would not be distinct. Let us therefore, assume an AP with the first term $a$ - a non-negative integer and the positive difference $d$. The $i$th term of the AP is $T_i=a+(i-1)d$. The AP is increasing, therefore there is a term $T_n$ for the least value of $n$ such that $T_n\geq d^2$. Now, $T_{n+1}$ is also a perfect square. Let $T_{n+1}=b^2$. Therefore, we have
$$
d^2 \leq b^2 \implies d \leq b 
$$ Therefore we have 
$$
T_{n+1}=b^2+d<b^2+2b+1=(b+1)^2
$$
or
$$b^2 < T_{n+1} < (b+1)^2$$ However, there are no perfect squares between two consecutive perfect squares. This contradicts our supposition that every term is a perfect square and an integer at the same time. Therefore, no such arithmetic progression exists.","['number-theory', 'arithmetic-progressions', 'proof-verification', 'elementary-number-theory']"
2156665,How and when to do exercises from books?,"Most math books I have seen seem to be structured in the same way; each chapter is a few dozen pages, and after each chapter there are a few dozen exercises. I often find myself not knowing what exercises to do and when to do them. If I have read the first 10 pages, I'm not sure what $x$ is in ""the first $x$ exercises should be answerable after reading the first $10$ pages."" A prime example is Tristan Needham's ""Visual Complex Analysis"". It's a very good book; however, the first chapter is about $50$ pages long, after which follows a splatter of $50$ exercises. I'm not sure when I should interrupt my reading and go the exercises. And I'm not sure when to interrupt my exercises and go back to the reading.","['complex-analysis', 'problem-solving', 'self-learning', 'soft-question']"
2156677,Strange limit : $e=1$ [duplicate],"This question already has answers here : Why does $\lim_{n\to\infty}(1+\frac{1}{n})^n = e$ instead of $1$? [duplicate] (3 answers) Closed 7 years ago . Why the following is not correct? : $$\lim_{n \to \infty} (1+\frac1n)^n = \lim_{n \to \infty} (1+\frac1n) \lim_{n \to \infty} (1+\frac1n) \dots \lim_{n \to \infty} (1+\frac1n) = 1 \times 1 \times \dots 1 = 1.$$ I guess that the problem (as usual) lies in infinity, but if so how? I checked all the theorems in Fitzpatrick's book of Calculus, nothing prevents to do the limits infinitely many times (esp. for a convergent one)!","['real-analysis', 'calculus', 'limits']"
2156683,find $g^{-1}(x)$ in terms of $f^{-1}$,"Consider $g(2x-3) = \frac{2f(x-2) + 3}{5 - f(x-2)}$ . Also $f$ and $g$ are invertible . Now find $g^{-1}(x)$ in terms of $f^{-1}$. My try : Because in the parentheses we have $x-2$ and $2x-3$ instead of $x$ , I don't know how we can solve this problem.","['inverse-function', 'functions']"
2156718,How to find an equation for this sine-ish wave,"I'm programming a game. Part of the game involves a spinning rectangle, so I'd like to keep track of two points on that rectangle. The center, and the bottom right corner of it. I want to draw a sine wave of the relative vertical distance between, the center and the corner. At 0, the distance is -16. At $\frac{\pi}{2}$, the distance is 32 At $\pi$, the distance is 16 At $\frac{3\pi}{2}$, the distance is -32 At $2\pi$, this distance is -16 This isn't like to periodic functions I studied in school, where the distance between $f(0)$ and $f(\frac{\pi}{2})$ is the same as the distance between $f(\frac{\pi}{2})$ and $f(\pi)$, etc. I'm having trouble figuring out the equation of this function. Would appreciate any help.","['periodic-functions', 'trigonometry', 'functions']"
2156748,Is there any point to Olympiad geometry beyond Olympiads themselves?,"Is synthetic geometry still relevant in mathematics? I always saw Olympiad geometry as an odd field because while every other Olympiad topic would extend to larger math, geometry seems especially useless. I get that analytic geometry is useful in college but that is heavily discouraged in Olympiad geometry. So what does olympiad geo extend to?","['contest-math', 'geometry']"
2156761,Help understand the proof of the Multivalued opertor version of the Hille-Yosida Theorem in Ethier-Kurtz,"I am looking at the proof of the following version of the Hille-Yosida theorem given in Ethier and Kurtz' Markov Process. Some Definitions. Here, $A$ is a (multivalued) linear operator on $L$, i.e. a subset $A$ of $L \times L$ with domain $D(A)=\{f:(f,g)\in A \;\text{for some}\;g\}$ and range $R(A)=\{g: (f,g)\in A \; \text{for some}\;f\}$. $A\subset L\times L$ is said to be linear if $A$ is a subspace of $L\times L$. If $A$  is linear, then $A$ is said to be single-valued if $(0,g)\in A$ implies $g=0$, in this case, $A$ is a graph of a linear operator on $L$, also denoted by $A$, so we write $Af=g$ if $(f,g)\in A$. If $A\subset L\times L$ is linear, then $A$ is said to be dissipative if $\Vert \lambda f-g\Vert \ge \lambda \Vert f\Vert $ for all $(f,g)\in A$ and $\lambda >0$; the closure $\bar{A}$ of $A$ is just the closure in $L\times L$ of the subspace $A$. Finally, we define $\lambda - A=\{(f,\lambda f-g):(f,g)\in A\}$ for each $\lambda >0$. Lemma preceding the theorem. Below is the Theorem. Now in the below part of the proof, I don't understand why $\Vert \lambda(\lambda - \bar{A})^{-1}f-f\Vert = \Vert(\lambda - \bar{A})^{-1}g\Vert$ which is above (4.2). Also, why is the domain of $(\lambda - \bar{A})^{-1}$ given as $R(\lambda - A_0)$? Shouldn't it be $R(\lambda-\bar{A})$ by definition? Finally, I don't understand why the range is $D(A_0)$ either. I would greatly appreciate if anyone could explain these to me.","['functional-analysis', 'markov-process', 'semigroup-of-operators', 'analysis']"
2156764,Modular cycles of $a^k \mod p$ and Fermat's Little Theorem,"Let $x = a^e \mod p$, where... $p$ is a prime number $e$ is an integer and $e < p$ $a$ is any integer If the values of $(p, e, x)$ are given, can $a$ be determined? I am approaching this problem using Fermat's Little Theorem, which states that $\forall a \in \{0, 1, ..., p-1\}$, $a^{p-1} \equiv 1 \mod p$ I try to argue that $a \cdot a^{k(p-1)} \equiv a \mod p$, because $a^{k(p-1)} \mod p$ appears to always be equivalent to $1$ (although I do not have a good justification for this, only experimental data). Then it can be said that $$(a \mod p) \cdot (a^{k(p-1)} \mod p) \equiv (a \mod p) \cdot 1 \equiv a \mod p$$ I then argue that using the numbers we are given, we can say that $x^{p-1} \equiv a^{{(p-1)}^e} \equiv 1 \mod p$, but I don't see how to get back to $a$.","['cryptography', 'modular-arithmetic', 'discrete-mathematics']"
2156773,What's the difference between $F(x)$ and $f(x)$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question (There is a question similar to mine, but it talks about f[x] and F[x].) I've seen some people use F(x) when denoting functions, but what is the difference between using a capital F and a lower case f?","['notation', 'functions']"
2156821,Multiple derivative notation,"Official Leibniz notation for double derivative is: $$\frac{\mathrm d^2s}{\mathrm dt^2}$$ This term seems inconsistent. Two considerations: We have infinitesimal change in distance $\mathrm ds$ per infinitesimal change in time $\mathrm dt$ : $\mathrm ds/\mathrm dt$ . Both terms are a tiny value/interval. Because the $\mathrm d$ symbolizes difference , I would as a change of the change of the distance to time intuitively write: $$\frac{\mathrm d(\mathrm ds/\mathrm dt)}{\mathrm dt}=\frac{(\mathrm ds^2/\mathrm dt^2)}{\mathrm dt}=\frac{\mathrm ds^2}{\mathrm dt^3}$$ where the extra $\mathrm d$ says that both terms are now ""double"" infinitesimal differences. Maybe more properly following mathematical logic and not my intuition, the $\mathrm d$ could be considered a ""free"" variable in itself that can be multiplied onto this $\mathrm ds/\mathrm dt$ fraction numerator: $$\frac{\mathrm d(\mathrm ds/\mathrm dt)}{\mathrm dt}=\frac{(\mathrm d^2s/\mathrm dt)}{\mathrm dt}=\frac{\mathrm d^2 s}{\mathrm dt^2}$$ That agrees with the actual notation but doesn't really make physical sense now. $\mathrm d$ means (infinitesimal) difference, so that $\mathrm ds=s_{final}-s_{start}$ , and therefore it makes no physical sense to consider the $\mathrm d$ and the $s$ separate. The $\mathrm ds$ is physically just a ""name""/""symbol"" for one term, which could just as well have been called $x$ or $a$ or anything else. Now, while searching for an explanation, the answers always tend to consider $\frac{\mathrm d}{\mathrm dt}$ as one symbol in itself, so that a double derivative is $\frac{\mathrm d}{\mathrm dt}\frac{\mathrm d}{\mathrm dt}s=\frac{\mathrm d^2}{\mathrm dt^2}s=\frac{\mathrm d^2s}{\mathrm dt^2}$ - which makes even less physical sense, since the $\mathrm dt$ term has to be a separable term before we can treat $\frac{\mathrm ds}{\mathrm dt}$ as a normal fraction (as done in integration e.g.). $\frac{\mathrm d}{\mathrm dt}$ can't possibly be just ""a symbol"". Why is $\frac{\mathrm d^2s}{\mathrm dt^2}$ the correct one in a physical context, where $\mathrm ds$ actually means the infinitesimal difference in $s$ ? Are my considerations in point 2 correct, and I just can't figure out that splitting $\mathrm d$ and $s$ is allowed? Update The answers already given at this time both indicate the use of $\mathrm d/\mathrm dt$ as merely a symbol. So, neither of my two suggestions mentioned above are the case. Sure, I can accept that. But the question still remains of why as well of how come we still treat them as variables then, e.g. in integration ? Let me clarify those two points: Firstly , if it indeed is the case that $\mathrm d/\mathrm dt$ is merely a symbol and should be thought of as just a symbol, then I do not understand the motivation for this symbol. Why did Leibniz choose $\mathrm d/\mathrm dt$ as a symbol, which causes the confusion and inconsistency described in the question above? Why not, say, $\mathrm d/\mathrm d$ , in which case we would get a writing-style that at least looks a bit more ""consistent"": $$\frac{\mathrm d}{\mathrm d}\frac st=\frac{\mathrm ds}{\mathrm dt}\qquad \frac{\mathrm d}{\mathrm d}\frac{\mathrm d}{\mathrm d}\frac st=\frac{\mathrm d^2s}{\mathrm d^2t}\qquad \frac{\mathrm d}{\mathrm d}\frac{\mathrm d}{\mathrm d}\frac{\mathrm d}{\mathrm d}\frac st=\frac{\mathrm d^3s}{\mathrm d^3t}\qquad \cdots$$ Or even better yet, if the two $\mathrm d$ involved in this $\mathrm d/\mathrm dt$ symbol have no meaning as neither a variable nor an indicator of a change in the parameter, then why use this letter at all? Why not stick to, say, the prime-notation throughout and never jump into the Leibniz notation: $$s_t'\qquad s_t''\qquad s_t'''\qquad \cdots$$ And secondly , if the $\mathrm d/\mathrm dt$ really just is a symbol, and that's it, then how come we suddenly can treat it as a fraction again containing a set of variables $\mathrm ds$ and $\mathrm dt$ that we can split apart during for instance integration? Such as here: $$\frac{\mathrm ds}{\mathrm dt}=v\quad\Leftrightarrow\quad \mathrm ds=v\,\mathrm dt\quad\Leftrightarrow\quad \int  1 \,\mathrm ds=\int v\,\mathrm dt \quad\Leftrightarrow\quad s=\int v\,\mathrm dt$$ I hope to get this notion cleared out and appreciate all comments and answers that can help.","['derivatives', 'notation']"
2156851,Calculate the coordinates of the third vertex of triangle given the other two and the length of edges in the cheapest computational way,"I simply have a triangle say $\triangle ABC$ . Given coordinates of $A$ & $B$ and the length of $AC$ and $BC$ . I can calculate the length between ab via the distance square rule. $$D = \sqrt{(X_2 - X_1)^2  + (Y_2 - Y_1)^2} $$ I have tried to use the distance rule to compute the third vertex, but it has a lot of steps.
I wonder if there is another method that yields the two possible solutions for the vertex","['trigonometry', 'triangles']"
2156863,"How to efficiently use a calculator in a linear algebra exam, if allowed","We are allowed to use a calculator in our linear algebra exam. Luckily, my calculator can also do matrix calculations. Let's say there is a task like this: Calculate the rank of this matrix: $$M =\begin{pmatrix} 5 & 6 & 7\\  12 &4  &9 \\  1 & 7 & 4
\end{pmatrix}$$ The problem with this matrix is we cannot use the trick with multiples, we cannot see multiples on first glance and thus cannot say whether the vectors rows / columns are linearly in/dependent.
Using Gauss is also very time consuming (especially in case we don't get a zero line and keep trying harder). Enough said, I took my calculator because we are allowed to use it and it gives me following results: $$M =\begin{pmatrix} 1 & 0{,}3333 & 0{,}75\\  0 &1  &0{,}75 \\  0 & 0 & 1
\end{pmatrix}$$ I quickly see that $\text{rank(M)} = 3$ since there is no row full of zeroes. Now my question is, how can I convince the teacher that I calculated it? If the task says ""calculate"" and I just write down the result, I don't think I will get all the points. What would you do? And please give me some advice, this is really time consuming in an exam.","['matrices', 'proof-writing', 'linear-algebra', 'vector-spaces']"
2156892,What is the name for a rectangle that is not a square?,"A square is a special case of a rectangle. What is a single-word term for a rectangle that is not a square?  I am looking for a word that excludes squares.  I am also looking for a word that is not ""rectangle"".","['terminology', 'definition', 'geometry']"
2156896,How to evaluate this limit $\lim_{x\rightarrow\infty}(1+\sin(x))^{x}$,How to evaluate this limit $\lim_{x\rightarrow\infty}(1+\sin(x))^{x}$ I try in many forms but I cannot evaluate this limit;  please help with tips please.,['limits']
2156943,"Find the joint distribution of $U_{1},\ldots,U_{n}$ where $U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})}$ and $X_{(i)}$ order static.","Let $X_{(1)}\leq X_{(2)}\leq \cdots X_{(n)}$ be the order statics for a random sample from a continuous distribution with c.d.f. $F(x)$ and density $f(x)$. Define $U_{i}$, $i=1,2\ldots,n,$ by 
$$U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})}, \quad i=1,\ldots,n-1, \: \mbox{and }\: U_{n}=F(X_{(n)}).$$ Find the joint distribution of $U_{1},\ldots,U_{n}$. Remark: I need a suggestion or someone tell me in which book I can find this exercise or related theory that allows me to solve it.","['order-statistics', 'probability-theory', 'statistics', 'probability-distributions']"
2156998,"Is every sub-$\sigma$-algebra of $\mathcal{B}(\mathbb{R})$ ""universally countably generated""?","This is a seemingly basic question that I realise I don't know the answer to: Let $\mathcal{B}$ be the Borel $\sigma$-algebra of $\mathbb{R}$. Is it the case that for every sub-$\sigma$-algebra $\mathcal{G}$ of $\mathcal{B}$ there exists a countably generated sub-$\sigma$-algebra $\mathcal{G}'$ of $\mathcal{G}$ such that for every probability measure $\mathbb{P}$ on $\mathcal{B}$, $\mathcal{G} \subset \sigma(\mathcal{G}' \cup \mathcal{N}_{\mathbb{P}})$? (Here, $\mathcal{N}_\mathbb{P}$ denotes the set of $\mathbb{P}$-null sets.) Although less important for me, it would also be interesting if possible to know the following: If the answer to the above question is no (in ZFC): Is the answer to the above question being yes consistent with ZF+DC? But if the answer is yes : Does it still remain yes if we replace ""every probability measure $\mathbb{P}$ on $\mathcal{B}\,$"" with ""every probability measure $\mathbb{P}$ on $\mathcal{G}'\,$""?","['descriptive-set-theory', 'measure-theory']"
2157013,Is it true that $HK$ is a subgroup of $G$ iff either $H$ or $K$ is a normal subgroup of $G$?,"Let $H$ and $K$ be subgroups of a group $G$.  We know that $HK$ is a subgroup of $G$ if and only if $HK=KH$.  Is it true that $HK$ is a subgroup of $G$ if and only if either $H$ or $K$ is a normal subgroup of $G$?  Clearly if $H$ or $K$ is a normal subgroup of $G$, then $HK=KH$ which implies $HK$ is a subgroup of $G$.  So really is it true that if $HK$ is a subgroup of $G$ (or really $HK=G$) does it imply that either $H$ or $K$ normal?  I am mainly interested in finite groups and when $G$ factors over $H$ and $K$ so that $G=HK$.","['finite-groups', 'abstract-algebra', 'normal-subgroups', 'group-theory']"
