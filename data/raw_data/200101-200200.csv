question_id,title,body,tags
3909031,Fourier transform of $\log|x|$ in $\mathbb{R}^2$ and the solution to $\Delta u = \delta_0$,"In my analysis class, we are covering tempered distributions now. I was given this two-part question in Fourier transforms of distributions. a. We are asked to compute the Fourier transform of $\log |x|$ as a tempered distribution in $\mathbb{R}^2$ . Here, $|x|$ is the 2d Euclidean norm of the vector $x$ . b. We are asked to find the fundamental solution to the Laplace equation in $\mathbb{R}^2$ , $\Delta u = \delta_0$ , where $u$ is a distribution and understood in the sense of weak solutions. Here, we take the Laplacian $\Delta u = -\frac{\partial^2 u}{\partial x^2}-\frac{\partial^2 u}{\partial y^2}$ . We take the Schwartz space of functions $S(\mathbb{R}^2)$ and its continuous dual space, $S'(\mathbb{R}^2)$ , the space of tempered distributions. For $T \in S'(\mathbb{R}^2)$ and $\phi \in S(\mathbb{R}^2)$ , we use the notation $$\langle T,\phi \rangle = T(\phi)$$ and when $T(x)$ is a function, we define $$\langle T,\phi \rangle = \int_{\mathbb{R}^2} T(x)\phi(x)dx$$ For the Foruier transform on tempered distributions, we define $$ \langle \mathcal{F}T,\phi \rangle = \langle T,\mathcal{F}\phi \rangle. $$ To be honest, I have no idea how to compute the Fourier transform of $\log|x|$ in $\mathbb{R}^2$ and how to use it to find the fundamental solution in part b, I do think I need to move to the Fourier domain in b but other than that I am lost. I thank anyone who can help with parts A and B.","['fourier-analysis', 'fourier-transform', 'analysis', 'distribution-theory', 'partial-differential-equations']"
3909149,"Is it true that $\mathscr{O}_{X,x}=\mathscr{O}_{\operatorname{Spec} A,x}$ for any $\operatorname{Spec} A$ with $x\in\operatorname{Spec} A$?","Let $X$ be a scheme and $\operatorname{Spec} A\subset X$ be an affine with $x\in\operatorname{Spec} A$ , Is it true that $\mathscr{O}_{X,x}=\mathscr{O}_{\operatorname{Spec} A,x}$ ? I ask this question because I feel that many ""obvious"" remarks made in Gortz-Wedhorn and Vakil FOAG seem to rely on this or a similar fact: such as in Vakil ""We say a ringed space is a locally ringed space if its stalks
are local rings. Thus Exercise 4.3.F shows that schemes are locally ringed spaces."" (Where 4.3.F is Show that the stalk of $\mathscr{O}_{\operatorname{Spec} A}$ at the point $[\mathfrak{p}]$ is the local ring $A_{\mathfrak{p}}$ ). Is this the case? If not, why is the above statement immediate from 4.3.F?","['affine-schemes', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3909167,How do I visualise this graph and answer the question?,"This is the question I have: Consider the graph defined as follows: Vertices: Pow({a,b,c}) Edges: {X,Y} is an edge if it is not the case that (X⊆Y or Y⊆X) Which of the following properties does this graph have? Eulerian path Hamiltonian path Bipartite Connected Chromatic number 3 Planar None of these I am having trouble visualising the graph from the given information. I came up with the following in my attempt: Vertices:- a,b,c,ab,bc,ca,abc,∅
Edges :- (a,b),(a,c),(b,c),(a,bc),(b,ca),(c,ab) This is how I drew it in my notebook: From these, I thought the only property the graph satisfies from the given list is 'planar'. Either that, or 'None of these'. But I have a strong feeling that I haven't got the graph right. Please help.","['elementary-set-theory', 'graph-theory']"
3909174,Motivation behind the sheaf of relative Kähler differentials,"I'm interested in the geometric motivation behind the bundle (or in more general framework the sheaf) of relative differentials $\Omega_{X/Y}$ of a morphism $f: X \to Y$ smooth $k$ -varieties. Differential geometry provides following picture of relative differentials: Let $f: X \to Y$ a equidimensional surjective map between connected manifolds $Y, X$ and moreover assume that every fiber $F:= f^{-1}(y) \subset X$ for $y \in Y$ is also a connected
submanifold of same dimension. We obtain an exact sequence of
tangent spaces $$  0 \to T_{X/Y} \to T_X \to f^*T_Y \to 0  $$ where $T_{X/Y}$ is the kernel of induced map of tangent bundles. intuitively what is really going on there is that for every $x \in f^{-1}(y)$ , the $(T_{X/Y})_x$ is the tangent space of the fiber at $x$ . The relative space of Kähler differentials $\Omega_{X/Y}$ defined as the dual of $T_{X/Y}$ and sits in the sequence which we will
obtain if we dualize the sequence above of tangent spaces: $$ 0 \to f^*\Omega_Y \to \Omega_X \to \Omega_{X/Y} \to 0  $$ Another definition of relative Kähler differentials which is more common to use in modern algebraic geometry works as follows: Let us embedd $X$ as the image of the diagonal $\Delta: X \to X \times_Y X$ and assume that the ideal sheaf $I \subset O_{X \times_Y X}$ defines the closed image $\Delta(X) \subset X \times_Y X$ . This gives us another sequence of tagent spaces $$ 0  \to T_{\Delta(X)} \to T_{X \times_Y X} \to N_{X \times_Y X/X} \to 0 $$ with normal bundle $N_{X \times_Y X/X}$ . It's a basic fact that the dual
of $N_{X \times_Y X/X}$ is $I/I^2$ and most books on algebraic
geometry define the sheaf relative Kähler differentials by $$\Omega_{X/Y} := I/I^2$$ Since this definition not uses that $f$ is a map of smooth maps this
is a far generalization of the old fashion setting from
differential geometry. Now, if there is any justice n this world then these two definitions of relative differentials should coinside if we deal with $X, Y$ and $fY$ nice enough. Therefore, if $f$ a surjective map between conneted manifolds such that
every fiber is a connected submanifold, why the tangent bundle $T_{X/Y}$ and the pullback of normal bundle $\Delta^* N_{X \times_Y X/X}$ of $\Delta(X) \subset X \times_Y X$ are canonically isomorphic? Can we write down an explicit isomorphism and understand what is geometrically going on there?","['vector-bundles', 'algebraic-geometry', 'coherent-sheaves', 'differential-geometry']"
3909183,Is a toric variety still a toric variety after change of coordiantes?,"I am reading Cox, Little, Shenck Toric Varieties , and I had a question about the definition of a toric variety. One of the examples (on page 12) uses $$x^2 - y^3 = 0$$ as an example of a toric variety. Is the following change of coordinates still a toric variety? $$(x-2)^2 - (y-2)^3 = 0$$ I feel it should be, but it seems from the comment about lattice points construction giving all the toric varieties (on page 14) that it is not attainable from the lattice points construction, since all the characters are maps into $\mathbb{C}^{\*}$ , and there's no way to adjust the missing axes to line up with the cusp of the new curve. If it's not a toric variety, what part of the definition does it violate? It seems I can define a similar isomorphism from $\mathbb{C}^{\*}$ to $V((x-2)^2 - (y-2)^3) \setminus \{(2, 2)\}$ , along with a similar action.","['algebraic-geometry', 'toric-varieties']"
3909210,Showing $\alpha(\beta+1)\leq\frac{5}{3}\alpha^2+\frac{1}{3}\beta^2$ for nonnegative integers $\alpha$ and $\beta$.,I found this lemma in a paper I was reading and it was not proved. There doesn't seem to be any obvious factorized form so how would one go about proving the inequality holds? $$\alpha(\beta+1)\leq\frac{5}{3}\alpha^2+\frac{1}{3}\beta^2$$ $$\frac{5}{3}\alpha^2+\frac{1}{3}\beta^2-\alpha(\beta+1) \geq 0$$ $$5\alpha^2+\beta^2-3\alpha\beta-3\alpha \geq 0$$,"['algebra-precalculus', 'discrete-mathematics']"
3909232,Why do we want $C^\infty_c$ to be complete in distribution theory?,"I am studying distribution theory. Denoting by $C_c^\infty$ the space of infinitely diferenciable function with compact support. Many authors point out the topology generated by the norms on the space $C_c^\infty(K)$ with $K$ compact (thats is the funcions in $C_c^\infty$ with support in $K$ ) is not complete to justify the introduction of the machinery of inductive limit topology on $C_c^\infty$ to turn it complete. Well, completeness is certainly usefull, however up to now I did not see where exactly the theory break down, without completness. Somebody could give me some hints? Maybe it arises in applications... Sincerely. Edit - A kind of self answer: Let $C^\infty_c(\mathbb R)$ equipped with the non complete topology, it is the topology corresponding to the uniform conergence (of any derivatives) on the compacts (I am right?). Now, let $\Lambda\in C^\infty_c(\mathbb R)*$ (the algebraic dual), $\Lambda$ is continuous implies $\Lambda$ is continuous on any $C^\infty_c(K)$ but the converse is false. Take $\varphi_n=\xi(x-n)$ with $\xi$ in $C^\infty_c$ with support in $[0,1]$ and $\int \xi =1$ . We have $\varphi_n \to 0$ (as well as its derivatives) uniformely on the compact, but $$\Lambda(\varphi_n) = \int \varphi_n = 1 \neq \Lambda(0)$$ also for any compact $K$ there exists a constant $C>0$ such that: $$|\Lambda(\varphi)| \leq C\sup_{x\in K} |\varphi|$$ so we have no ""good"" continuity criterion for the linear form. The inductive limit topology provides the good one since bounded subset belongs to a $C_c^\infty(K)$ .
Is I am right!? If it is the case, it is confuse to me, because all the textbook I found, such as the one of Rudin say something like: ""this topo is not complete, so let construct an other one which turn it complete"" to justify the construction and this messed up. It seems (to me) the problem is more about characterization of continuous linear form on $C^\infty_c(\Omega)$ , and for free we get completeness...","['functional-analysis', 'distribution-theory']"
3909349,A problem related to $2x^3-3x^2-x+\frac{3}{2}=0$,"let $f(x)=2x^3-3x^2-x+\frac{3}{2}$ .Then prove that $$\int_{1/8}^{7/8} f(f(x)) \text d x\neq \frac{3}{4}$$ Factorising $$f(x)=2(x-1.5)(x-\frac{1}{\sqrt{2}})(x+\frac{1}{\sqrt{2}})$$ $$f(f(x))=2(f(x)-1.5)(f^2(x)-0.5)$$ But i don't see anything nice from this.
Maybe if i could prove that $$f(f(x+\frac{1}{2}))=-f(f(x-\frac{1}{2}))$$ then the integral would turn out to be zero. also i dont think we have to find the exact value of integral for second part if we could just set up an inequality or prove that the integral is negative we are done! Please note that i am intersted in a proof without actually finding $f(f(x))$","['integration', 'definite-integrals', 'roots', 'calculus', 'polynomials']"
3909366,There are 15 balls named from A to O.,"Continuing the question: They are thrown in boxes 1, 2, 3 uniformly and at random such that each box gets 5 balls. Calculate probabilitity that A and B are in the same box. I understand that we may need to do some form of complementary counting where the total number of outcomes are $\binom{15}{5} \times \binom{10}{5} \times \binom{5}{5}$ . But I am confused over how to go ahead from this step.","['discrete-mathematics', 'combinatorics', 'probability']"
3909390,Show that $A$ is diagonalisable when $P(A)$ is diagonalisable and $P'(A)$ is invertible,"Let $n \geq 1$ and $A \in M_n(\mathbb{C})$ . Assume there exists $P \in \mathbb{C}[X]$ such that: $P(A)$ is diagonalisable $P'(A)$ is invertible I have to show that $A$ is diagonalisable. My try: I solved the problem when $\textrm{deg}(P) \leq 1$ . I also know that $A$ is trigonalisable, then I deduced that: $\forall \lambda \in \textrm{Sp}(A), P'(\lambda) \neq 0$ . Also, there exists a diagonal matrix $D$ and $S \in GL_n(\mathbb{C})$ such that $P(A) = SDS^{-1}$ . But I can't say anything more. Any help is welcome.","['matrices', 'diagonalization', 'inverse', 'polynomials']"
3909453,How to solve the limit $\lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}}$,"$$
\lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}}
$$ I tried to use L'hopital rule by differentiating top and bottom, but it ended up getting more and more complicated. Here, $c_1$ and $c_2$ are constants.","['limits', 'calculus', 'derivatives']"
3909466,How many times is $f^{-1}$ differentbable if $f$ is differentiable $m$ times at $x$?,Let $f$ be continuously differentiable $m$ times at $x \in \mathbb{R}$ . Does it then follow that $f^{-1}$ (assuming it exists) is also continuously differentiable $m$ times at $f(x)$ ? This is true for $m=1$ but does it hold for any $m$ ? Thank you!,"['calculus', 'derivatives']"
3909522,Stabiliser a Conic,"I am trying to solve: Consider the conic $C = Z(X_{0}X_{1} - X_{2}^{2})$ in the projective plane. (a) Find the pointwise stabiliser of $C$ in $PGL(3,K)$ (b) Find the setwise stabiliser of $C$ in $PGL(3,K)$ My attempt: Step1: I try to find the points in $C$ as follows: We have two cases to consider: Case1: $X_{0} = 0$ . In this case, we must have $X_{2} = 0$ , then we just have one point on the conic of the form $(0,a,b)$ which is $(0,1,0)$ . Case2: $X_{0} \neq 0$ . In this case, we have all the points of the form $(1,a,a^{2})$ where a is any element of $K$ , including $0$ . Step2: Then, I take any 3 x 3 matrix, right now, I have no idea about the entries of the matrix. Since we are looking for the pointwise stabiliser of the conic, our matrix must map $[0,1,0]$ to itself. This means that, the second column of the matrix must be $[0,\lambda,0]$ for some nonzero $\lambda$ in $K$ . If we apply the same reasoning to $[1,0,0]$ , then the first column of the matrix must be $[\beta,0,0]$ . Since our matrix, must be invertible, the last row cannot be $[0,0,0]$ thus it is of the form $[0,0,\alpha]$ for some nonzero $\alpha$ in $K$ . Since, we are working with projectivities, we can normalize the last row to $[0,0,1]$ . Now, our matrix looks like: \begin{bmatrix}
\beta & 0 & *\\
0 & \lambda & *\\
0 & 0 & 1
\end{bmatrix} where * is any element of $K$ , including $0$ . I came here so far. My main question is how to solve such questions in general: We are given an object in projective space, not necessarily zero locus of a linear form. How to find its pointwise and setwise stabiliser in projectivity group? Is there an algorithm to do this? Thanks, in advance.","['projective-geometry', 'algebraic-geometry', 'group-theory', 'projection-matrices', 'projective-space']"
3909548,Prove $h_{p\mu+(1-p)\nu}(T)=ph_\mu(T)+(1-p)h_\nu(T)$,"Let $T:X\to X$ be a continuous map on a compact metric space. I'm trying to prove that for two $T$ -invariant probability measures $\mu,\nu$ we have $h_{p\mu+(1-p)\nu}(T)=ph_\mu(T)+(1-p)h_\nu(T)$ . Here $h_\mu(T)$ denotes the metric entropy of $T$ , also known as the measure theoretic entropy . I managed to show that we have $h_{p\mu+(1-p)\nu}(T)\geq ph_\mu(T)+(1-p)h_\nu(T)$ by defining $\Phi:[0,\infty)\to\mathbb R$ by $\Phi(x)=-x\log x$ for $x>0$ and $\Phi(0)=0$ ; then $\Phi$ is a concave function, hence by Jensen $$p\Phi(\mu(A))+(1-p)\Phi(\nu(A))\leq\Phi(p\mu(A)+(1-p)\nu(A))$$ for each measurable $A$ . Applying this to finite measurable partitions $\alpha$ gives $H_{p\mu+(1-p)\nu}(\alpha)\geq pH_\mu(\alpha)+(1-p)H_\nu(\alpha)$ . Next, we plug in $\bigvee_{i=0}^{n-1}T^{-i}\alpha$ , divide by $n$ and letting $n\to\infty$ and obtain $h_{p\mu+(1-p)\nu}(\alpha,T)\geq ph_\mu(\alpha,T)+(1-p)h_\nu(\alpha,T)$ . Then we find partitions $\alpha_\mu,\alpha_\nu$ such that $h_\mu(\alpha_\mu,T)\geq h_\mu(T)-\frac{\epsilon}{p}$ and $h_\mu(\alpha_\nu,T)\geq h_\nu(T)-\frac{\epsilon}{1-p}$ , yielding $$h_{p\mu+(1-p)\nu}(\alpha_\mu\vee\alpha_\nu,T)\geq ph_\mu(T)+(1-p)h_\nu(T)-\epsilon;$$ passing to the supremum over all measurable partition $\alpha$ on the left hand side gives the desired inequality. For the reverse inequality, I have no idea what to do. Any help is much appreciated!","['measure-theory', 'ergodic-theory', 'entropy']"
3909694,"If $|f''| \leq M|f |$, how to prove that $f(x) \equiv 0$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This time I come across a problem.
A function $f(x)$ is defined on an  interval $[a,b]$ , and given that $f(a) = f'(a) = 0$ , there exists a also const $M$ which satisfies $$|f''(x)| \le M|f(x)|$$ How to obtain $f(x) \equiv 0$ ? Edit : the content of the related section is about the differential form of Grönwall's inequality here : If $f(x)$ is differential on $[a,b]$ , $f(a)=0$ and there exists a const $M$ which satisfies $$|f'(x)| \le M|f(x)|$$ we can easily obtain $$f(x) \equiv 0$$ . I tried to use a similar method in the book to prove the second order derivative situation, but without success.","['ordinary-differential-equations', 'real-analysis']"
3909700,"relation, function, ""Abbildung"", injective, surjective","I am teaching theoretical foundations of computer science, which uses many concepts of set theory. But I learned, that many of my students are not really firm with set theory, and so I decided to write a little booklet, so that they can read it and learn the most important concepts of set theory by reading this booklet. (I do not teach set theory in my course.) Because I did not study mathematics, and because I want to make everything as correct as possible, I spend a lot of time researching the terms and concepts of set theory, but I found, that many terms and concepts are explained not just only in different ways, but also contradictory. In German very often the term ""Abbildung"" is used, which I believe to be ""mapping"" in English, but I am not sure. I also are not sure, if this is a synonym for relation or for function or for something else. I am pretty sure, that every subset of the Cartesian product is a relation . I found Definitions of ""Abbildung"" that say that every subset of the Cartesian product is an ""Abbildung"" (which would make it a synonym for relation ). But I also found Definitions of the same term that say that only a left-total and right-unique relation is an ""Abbildung"", which I thought was the definition of ""function"" . But then I also found resources where they say, that a left-total and right-unique relation is only called a function , if all elements of both sets are numbers. So, a left-total and right-unique relation between two non-numeric sets would not be a function (but still an ""Abbildung"" ) So, here are my questions: How exactly are the terms relation and function defined (related to the Cartesian product of two sets)? If you also know the German term ""Abbildung"": What is its English name? How is it defined? Let A and B be sets. A is the Latin alphabet (26 letters = elements) and B the Greek alphabet (24 elements). Is a left-total and right-unique subset of the Cartesian product $A \times B$ a function? Look at the following picture which shows 16 different relations: Which of them are functions? Which of them are injective, which of them are surjective and which of them are bijective? Columns are about set A: column 1-5-9-13: In A are elements that send 0, 1 or 2 arrows to B column 2-6-10-14: In A are elements that send 1 or 2 arrows to B (no element of A sends 0 arrows) column 3-7-11-15: In A are elements that send 0 or 1 arrow to B (no element of A sends 2 arrows) column 4-8-12-16: All elements in A send exactly 1 arrow to B Rows are about set B: row 1-2-3-4: In B are elements hit by 0, 1 or 2 arrows row 5-6-7-8: In B are elements hit by 1 or 2 arrows (no element of B is hit by 0 arrows) row 9-10-11-12: In B are elements hit by 0 or 1 arrow (no element of B is hit by 2 arrows) row 13-14-15-16: All elements in B are hit by exactly 1 arrow Would it make a difference, if the elements in my picture were not Glagolitic and Armenian letters but rational numbers?","['elementary-set-theory', 'functions', 'mathematical-german', 'relations']"
3909727,Is there any function such that $f(x)\to0$ as $x\to \infty$ but $x\log(x) f'(x)\to +\infty$?,"My question es exactly the one in the title. I am trying to find any smooth function $f\in C^\infty((1,\infty))$ such that $$
f(x)\xrightarrow{x\to+\infty}0 \qquad \hbox{but} \qquad x\log(x)f'(x)\xrightarrow{x\to+\infty}+\infty.
$$ So far I've tried with $1/\log(x)$ and $1/\log(\log(x))$ , etc, with no success. Now I am wondering if that kind of function can actually exists. A priory looks like a quite reasonable requirement, but surprisingly I cannot find any such function.","['limits', 'calculus', 'real-analysis']"
3909728,When is the system of first order differential equations asymptotically stable?,"Given the system $$\mathbf{x}'=A\mathbf{x} $$ where $A=\begin{bmatrix}a &0 & 4 \\ -1 & -1 & 0 \\ -2-a & 0 & -3 \end{bmatrix}$ In what interval of $a$ is the system asymptotically stable, and for what value of $a$ is the system stable/unstable if such a case even exists? Attempt For a system to be asymptotically stable the real part of all eigenvalues must be negative. $$\Re(\lambda)<0, \: \: \: \text{for all} \: \: \lambda $$ Since we are dealing with a $3\times3$ matrix, I used maple to find the eigenvalues from the system matrix. I also tried to use Maple to solve the inequality case for the eigenvalues, but I don't think I'm getting correct results. The results I get from Maple state that the system is asymptotically stable when $$-8 < a \leq5-4\sqrt{3} $$ I spoke with my peers, and they said that this interval of $a$ is wrong. I posted my Maple document below. Can anyone see what is going wrong?","['systems-of-equations', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'stability-theory', 'maple']"
3909730,Application of Mean Value/Rolle's Theorem?,"Question Let $f$ be a differentiable function such that $f'$ is continuous on $[0, 1]$ and $M$ the maximum value of $|f'(x)|$ on $[0, 1]\ $ . Prove that, if $f(0) = f(1) = 0$ , then $$\int^{1}_{0} |f(x)| \mathrm {d}x \leq \frac M 4\ .$$ I have stared long and hard at this question for quite some time, but I am not sure where to even begin. Since I see that the endpoints are given, I think of MVT, but since they are also equal, I think of Rolle's Theorem too. These are just my intuitions - they could be wrong. Moreover, the moduli of $f'(x)$ and $f(x)$ are throwing me off. Any hints/suggestions on how to approach this question would be greatly appreciated :) P.S. I am only taking an introductory calculus module in college, so the tools at my disposal are, roughly, IVT, MVT and Rolle's Theorem.","['integration', 'proof-writing', 'real-analysis', 'calculus', 'derivatives']"
3909778,Show that $\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right)$,"Hi it's one of my old problem that I cannot solve let me propose it  : Prove that $$\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right)\quad \tag{1}$$ where $W(\cdot)$ is the Lambert W function and $\Gamma(\cdot)$ is the gamma function. The central function is : $$f(x)=\Gamma\left(x^{-x}+1\right)$$ The derivative is : $$f'(x)= -x^{-x}(\ln(x)+1)\Gamma\left(x^{-x}+1\right)\psi(x^{-x}+1)$$ Unfortunately it doesn't helps since $f(x)$ is increasing around $x=3$ while $f\left(\operatorname{W}(x)\right)$ is decreasing around $x=3$ . Maybe I think we can do the same as the Claude Leibovici's answer  here Show that $\Gamma(\Omega)\leq \Gamma\Big(\operatorname{W}\Big(x^{x}\Big)\Big)<2$ on $(0,1]$ . My question : How to show $(1)$ ? Thanks in advance !","['approximation', 'gamma-function', 'lambert-w', 'inequality', 'derivatives']"
3909783,Tail Lower Bounds using Moment Generating Functions,"Given a random variable $X>0$ with Moment Generating Function $m(s)=E[e^{sX}]$ I'm interested in finding a lower bound $$\Pr[X \ge t] \ge 1-\varepsilon(t),$$ where $t>E[X]$ . A classic technique for finding upper bounds for $\Pr[X \ge t]$ is using Markov's inequality with the Moment Generating Function: $$\Pr[X \ge t] \le E[e^{sX}]e^{-st},$$ for $s\ge 0$ .
Since $E[e^{s X}]\ge e^{s E[X]}$ (Jensen's inequality), this bound is useful whenever $t>E[X]$ . (I'm assuming $X$ is a positive random variable, so $E[X]>0$ .) Standard techniques for lower bounds include the second-moment method, such as Cantelli's inequality and Paley-Zygmund. However, they only cover the case $t\in[0,E[X]]$ and say nothing about the case $t>E[X]$ . My best bet so far has been Lévy's and Gil-Pelaez's inversion formulas for the Characteristic Function $m(i s)$ : $$\begin{align}\Pr[X \ge t] &= 1-\frac{1} {2\pi} \lim_{S \to \infty} \int_{-S}^{S} \frac{1 - e^{-ist}} {is}\, m(is)\, ds.
\\\text{and}\quad
\Pr[X \ge t] &= \frac{1}{2} + \frac{1}{\pi}\int_0^\infty \frac{\operatorname{Im}[e^{-ist}m(i s)]}{s}\,ds.\end{align}$$ However, I don't have any good ideas for bounding those in a useful way. It certainly seems a lot harder than the Markov method for the upper bound. Am I missing a useful approach here?","['fourier-analysis', 'characteristic-functions', 'moment-generating-functions', 'distribution-tails', 'probability']"
3909828,Describe $\sigma$-algebra subsets of $[0;1]$ section generated by..,"Describe $\sigma$ -algebra subsets of $[0;1]$ section generated by following sets system that consists of all single-point sets. However I do have several ideas, I'm rather new to this topic and not really sure whether I'm right, my ideas are the following: In this task we need to list all the possible elements of $\sigma$ -algebra given, so for any point we may take it and may not, so $\{\{x\}, \{[0;1] \setminus x \} | \forall x \in [0;1] \}$ .
Is it right? Any response/solutions is greatly appreciated.","['measure-theory', 'probability-theory', 'real-analysis', 'problem-solving', 'probability']"
3909884,"Why is it called the ""congruence class"", not ""congruence set""?","Stupid question, why do we call it the congruence class, not congruence set? Is there any scenario when the congruence class is not a set?","['elementary-set-theory', 'modular-arithmetic', 'terminology']"
3909889,Calculating matrix exponential,Given matrix $$M = \begin{pmatrix} 7i& -6-2i\\6-2i&-7i\end{pmatrix}$$ how do I calculate matrix exponential $e^M$ ? I know I can use that $e^A=Pe^DP^{-1}$ where $D=P^{-1}AP$ . I computed the characteristic polynomial of the above matrix as $$P(\lambda)=\lambda^2+89$$ Is there an easier way to do this than trying to compute the diagonalized matrix?,"['matrices', 'matrix-calculus', 'linear-algebra', 'matrix-exponential']"
3909958,Ehresmann's lemma equivalent etale topology,"In complex algebraic geometry we have Ehresmann's lemma which states that given $f:X \to Y$ a smooth surjective proper map between complex manifold this is locally a fibration in the analytic topology. I was wondering whether there was a generalization of that in the context of etale topology. Lots of statement which are true analytic locally in the context of $\mathbb{C}$ can also be carried in the etale topology for smooth varieties over any algebraically closed field $k$ . In the (beautiful) notes by Conrad https://www.math.ru.nl/personal/bmoonen/Seminars/EtCohConrad.pdf at page 53, he states what he calls etale Ehresmann's lemma: given $f:X \to S$ smooth and proper, the sheaf $R^if_*F$ are locally free whenever $F$ is a torsion etale sheaf on $X$ . The hypothesis are however very general:assuming $X,S$ to be both smooth varieties over an algebraically closed field, it is true the stronger statement that $f$ is a fibration etale locally?","['etale-cohomology', 'complex-geometry', 'algebraic-geometry']"
3909980,Understanding how EM algorithm actually works for missing data,"I am currently studying EM algorithm for handling missing data in a data set. I understand that the final goal of EM algorithm is not to impute data, but to calculate the parameters of interest. However, when I am thinking of a practical example I am really struggling to make sense of the E-step: calculate $Q(\theta, \theta^{(k)}) =  \operatorname{E_{Z|X,\theta^{(k)}}} [\ln{p_{XZ}(x, z| \ \theta)}] \nonumber = \int_{\mathcal{Z}} {\ln{p_{XZ}(x, z| \ \theta)} \  p_{Z|X}(z| \ x, \theta^{(k)}) \ dz}$ , where $x$ are the realization of $X$ (i.e. the observed variables) and $z$ are the realization of $Z$ (i.e. the variables with missing values). Now suppose that we have a data set, where all the instances come from a random variable $Z$ which has a bivariate normal distribution, and also suppose that several data are missing in this data set. How will E-step will work in this case (where $\ln{p_{XZ}(x, z| \ \theta)} = \ln{p_{Z}(z| \ \theta)}$ and $p_{Z|X}(z| \ x, \theta^{(k)})=\ln{p_{Z}(z| \ \theta)}$ )? Does an imputation for missing values takes place in E-step and if yes how they are imputed? Will they be imputed by their current value of mean parameter?","['statistics', 'numerical-optimization', 'machine-learning', 'computational-mathematics', 'algorithms']"
3910015,What exactly is $\mathbb{Z_n}$?,"My impression has been that $\mathbb{Z_n}$ is the set $\{0,1,...,n-1\}$ under binary operation addition modulo $n$ . However I'm also coming across this notion that $\mathbb{Z_n}$ is actually a set of equivalence classes of equivalence relation $x\sim y \iff x \equiv y$ mod $n$ and the addition here is actually addition of equivalence classes rather than simply addition of integers modulo $n$ . Is this correct? So would it be correct to say $\mathbb{Z_n} = \{n,n+1,...,2n-1\}$ ? if we are considering these elements as being equivalence classes?","['equivalence-relations', 'group-theory', 'modular-arithmetic']"
3910018,Use identities to find the exact values of the remaining five trigonometric functions at alpha. $\sin(\alpha/2) = 3/5$ and $3\pi/4 < \alpha/2 < \pi$,"I tried to solve this trig identity problem from my Trigonometry textbook and I can't seem to get the correct answer. I've solved similar trig identity problems and have successfully found the correct answers for other problems; however, I'm not familiar with being given "" $\sin(\alpha/2)$ "". (I have the answers for this math question, but my textbook doesn't give an explanation for how to solve this particular problem). Using the half-angle or double-angle formula was recommended to me on another math forum, but I'm unsure how to properly apply those trigonometric formulas to this specific math problem. I'm used to applying the Pythagorean Identity "" $\sin^2 x + \cos^2x = 1$ "" to tackle this kind of problem, but since I've never been given "" $\sin(\alpha/2)$ "" for my homework assignments or quizzes I don't have proper notes to reference for this specific math problem. :/ Correct Answers from the textbook: $$\sin a = -24/25$$ $$\cos a = 7/25$$ $$\tan a = -24/7$$ $$\csc a = -25/24$$ $$\sec a = 25/7$$ $$\cot a = -7/24$$ Any help is appreciated ~ Thank you :)",['trigonometry']
3910127,"Are product measures the only $T$-invariant measures if $T$ is the left shift on $\{0,1,\ldots,k-1\}^\mathbb Z$?","Consider $X=\{0,1,\ldots,k-1\}^\mathbb Z$ equipped with the $\sigma$ -algebra $\mathcal C$ generated by the cylinder sets $$\{x\in X:x_m=i_m,\ldots,x_n=i_n\}\tag{$m\leq n$}.$$ It is well known that the $p$ -Bernoulli measure $\mu$ , defined by letting $$\mu\{x\in X:x_m=i_m,\ldots,x_n=i_n\}=p_{i_m}\cdots p_{i_n}$$ is $T$ -invariant (and ergodic) for the left shift $T:X\to X:x\mapsto y$ , where $y_n=x_{n+1}$ . However, I was wondering if there are any other $T$ -invariant (not necessarily ergodic) measures. If I understand it correctly, this post seems to imply that no other $T$ -invariant measure exists, but I don't see why such another measure is impossible.","['measure-theory', 'ergodic-theory']"
3910289,Infinite sum of right-continuous and monotone non-decreasing functions and its Lebesgue-Stieltjes integral,"Let $g_n: [0,\infty) \to [0,\infty)$ be a right-continuous and monotone non-decreasing function for every $n \in \mathbb N$ . Define the pointwise limit $g(x) = \sum_n g_n(x)$ and assume that it is finite for all $x \geq 0$ . Then it is true that: $g$ is also right-continuous and monotone non-decreasing. For every $f: [0,\infty) \to \mathbb R$ with existing Lebesgue-Stieltjes integral $\int f \mathrm d g$ we have that $$
\int f \mathrm d g = \sum_{n \in \mathbb N} \int f \mathrm d g_n
$$ It's been quite a while since I've taken measure and integration theory and the Lebesgue-Stieltjes integral is new to me. For 1.:
The monotony is clear by induction for finite sums. I'm a little embarrassed to say that I have difficulty coming up with a rigorous argument for the infinite sum although it seems obvious that $\sum_n a_n \leq \sum_n b_n$ where $a_n \leq b_n$ for all $n$ . Concerning right-continuity, it is known that the infinite sum of continuous functions need not be continuous in general. But my Analysis is not strong enough to figure this out for this case. So I tried to resort to measure theory: For every $f_n$ there exists a unique measure $\mu_n$ by setting $\mu_n([0,x])=f_n(x)$ for all $x \geq 0$ and applying the uniqueness theorem for measures, since the intervals $[0,x]$ generate the Borel- $\sigma$ -algebra and are closed under intersection of sets. Now, the infinite sum $\sum_n \mu_n$ of measures should again be a measure $\mu$ . By definition we have $g(x)=\mu([0,x])$ for all $x\geq 0$ . If $\mu$ is in fact a measure, then it should follow directly that $g$ has the required properties, right? However, this would technically require to show the lemmata that the countable sum of measures is a measure and that the correspondence between a non-negative, right-continuous and monotone non-decreasing function and a measure is one-to-one. I would like to be able to verify the statement by more analytic means. Another question that pops into my head is whether the finiteness of $g$ is necessary for 1. to hold. (I see that it is necessary for 2.) For 2.: Again, this corresponds to the statement that $$
\int f \mathrm d \sum_n \mu_n = \sum_n \int f \mathrm d \mu_n.
$$ I faintly remember having seen a statement like this. My first guess would be that this can be shown by measure theoretic induction: First for indicator functions, then for simple functions and so on. Is there any cuter way to do this?","['monotone-functions', 'measure-theory', 'lebesgue-integral', 'probability-theory']"
3910339,Does L'Hopital's rule imply that $\lim_{x\to a}\frac{f'(x)}{g'(x)} = \lim_{x\to a}\frac{f(x)}{g(x)}$ always?,"My question is summarised as follows: Is $$\lim_{x\to a}\frac{f'(x)}{g'(x)} $$ always equal to $$\lim_{x\to a}\frac{f(x)}{g(x)} $$ or can we only deduce values of limits using differentiation in the forward direction, ie we can't deduce that the value of $\lim_{x\to a}\frac{f'(x)}{g'(x)} $ is the same as $\lim_{x\to a}\frac{f(x)}{g(x)} $ ? Logically I cannot see why this shouldn't be true (ie I can't see why we can't also integrate the numerator and denominator). HOWEVER, the following result seems to throw some doubt on this: Consider the identity $$f(x)^2\frac{d}{dx}\frac{g(x)}{f(x)}+g(x)^2\frac{d}{dx}\frac{f(x)}{g(x)}\equiv0$$ $$\implies\frac{\frac{d}{dx}\frac{f(x)}{g(x)}}{\frac{d}{dx}\frac{g(x)}{f(x)}}\equiv-\frac{f(x)^2}{g(x)^2}$$ On applying L'Hopital's rule we obtain $$\lim_{x\to a}\frac{\frac{d}{dx}\frac{f(x)}{g(x)}}{\frac{d}{dx}\frac{g(x)}{f(x)}}\equiv\lim_{x\to a}-\frac{f(x)^2}{g(x)^2}\implies\lim_{x\to a}\frac{\frac{f(x)}{g(x)}}{\frac{g(x)}{f(x)}}\equiv\lim_{x\to a}-\frac{f(x)^2}{g(x)^2}$$ $$\implies\lim_{x\to a}\frac{f(x)^2}{g(x)^2}\equiv\lim_{x\to a}-\frac{f(x)^2}{g(x)^2}$$ which is obviously false. Any help explaining this apparent proof would be appreciated.","['integration', 'fake-proofs', 'calculus', 'paradoxes', 'limits']"
3910372,Prove that Fibonacci sequence contains An infinite Sub-sequence That all its element are co-prime.,"Prove that Fibonacci sequence contains An infinite Sub-sequence That all its element are co-prime. Prove Two Sub-sequence different Instruction: Fermat numbers. $----------------------------$ attempt 1: $F_n$ is strong divisibility sequence , $\gcd(F_m,F_n)$ = $F_{\gcd(m,n)}$ we need to prove there an infinite sequence of coprime integers.","['number-theory', 'fibonacci-numbers', 'prime-numbers']"
3910387,"$\lim\limits_{n\to\infty}\sqrt[n]{1^k +2^k+\cdots+ n^k},$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What is $$\lim\limits_{n\to\infty}\sqrt[n]{1^k +2^k+\cdots+ n^k},$$ where $ k\in \mathbb{N}$ ?","['limits', 'calculus', 'discrete-mathematics']"
3910507,Area of image under Gauss map,"I am trying to solve the following question Let $\mathbf r(u,v)$ be a parametrised smooth surface in $\Bbb R^3$ with $(u,v) \in U$ , a connected open subset of $\Bbb R^2$ . Let $\mathbf n: U \to S^2$ be the Gauss map that assigns to each point $\mathbf r(u,v)$ of the surface its unit normal. Suppose that $\mathbf n$ is bijective onto its image and that the Gaussian curvature K is nowhere zero in U. Show that the area of $\mathbf n(U)$ is equal to the absolute value of $\int K dA$ I am not really sure where to start, so any hint would be appreciated.",['differential-geometry']
3910519,Proving there is a constant $C$ such that $f=g+C$,"Consider the following lemma: Lemma: Let $f:[a,b]\to\mathbb{R}$ continuous on $[a,b]$ and differentiable on $(a,b)$ , If $f'(x)=0$ for all $x\in [a,b]$ , then $f$ is constant on $[a,b]$ . I have to prove the following corollary: Corollary: If $f$ and $g$ are continuous functions on $[a,b]$ , differentiable on $(a,b)$ , and $f'(x)=g'(x)$ for all $x\in(a,b)$ , then there exists a constant $C$ such that $f=g+C$ . My attempt: Consider the function $h(x)=f(x)-g(x)$ . We have that $h'(x)=f'(x)-g'(x)=0$ for all $x\in(a,b)$ . By the lemma we have that $h$ is constant on $[a,b]$ . In other words, $h(x)=C$ , where $C\in\mathbb{R}$ is a constant. Then, $C=f(x)-g(x)$ in $[a,b]$ . We conclude that $C+g(x)=f(x)$ . Is this proof correct?","['calculus', 'solution-verification', 'derivatives', 'real-analysis']"
3910531,How do I solve this in an understandable and direct way? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question For each $i \in \Bbb N$ , let $f_i: \Bbb N \mapsto \{0, 1\}$ . Let $A = \{f_i : i \in \Bbb N\}$ and $E = \{n \in \Bbb N : f_n(n) = 0\}$ . Does there exist a $f \in A$ such that $E = \{n \in\Bbb N : f(n) = 1\}$ I think that there does not exist a $f \in A$ such that $E = \{n \in\Bbb N : f(n) = 1\}$ . However, I am unsure on how to prove it. How do I show my answer in an understandable way?","['real-numbers', 'proof-explanation', 'proof-writing', 'functions']"
3910574,Showing that a relation tied to prime factorization is an equivalence relation,"Consider the relation $≈$ on $ℕ$ : If $x = {p_1}^{x_1} \times {p_2}^{x_2} ... \times {p_k}^{x_k}$ and $y = {p_1}^{y_1} {p_2}^{y_2} ... {p_k}^
{y_k}$ , where $p_i$ are distinct primes and $x_i$ and $y_i$ are nonnegative integers, then $x ≈ y$ iff $x_1 + x_2 + \ldots + x_k = y_1 + y_2 +\ldots + y_k$ . Show $≈$ is an equivalence relation I know I need to show reflexivity, symmetry, and transitivity. For reflexivity I have $x ≈ x \iff x_1 + x_2 + \ldots + x_k = x_1 + x_2 + \ldots + x_k$ , which is clearly true. And for symmetry I have $x ≈ y \implies y ≈ x$ , so \begin{align*}
&x_1 + x_2 + \ldots + x_k = y_1 + y_2 +\ldots + y_k \\
\implies &y_1 + y_2 +\ldots + y_k = x_1 + x_2 + \ldots + x_k
\end{align*} which is also clearly true. This seems too easy and I feel like I'm doing this wrong.","['equivalence-relations', 'relations', 'prime-factorization', 'discrete-mathematics']"
3910623,"Is there a clever solution to Arnold's ""merchant problem""?","There is a problem that appears in an interview $^\color{red}{\star}$ with Vladimir Arnol'd . You take a spoon of wine from a barrel of wine, and you put it into your cup of tea. Then you return a spoon of the (nonuniform!) mixture of tea from your cup to the barrel. Now you have some foreign substance (wine) in the cup and some foreign substance (tea) in the barrel. Which is larger: the quantity of wine in the cup or the quantity of tea in the barrel at the end of your manipulations? This problem is also quoted here . Here's my solution: The key is to consider the proportions of wine and tea in the second spoonful (that is, the spoonful of the nonuniform mixture that is transported from the cup to the barrel). Let $s$ be the volume of a spoonful and $c$ be the volume of a cup. The quantity of wine in this second spoonful is $\frac{s}{s+c}\cdot s$ and the quantity of tea in this spoonful is $\frac{c}{s+c}\cdot s$ . Then the quantity of wine left in the cup is $$s-\frac{s^2}{s+c}=\frac{sc}{s+c}$$ and the quantity of tea in the barrel now is also $\frac{cs}{s+c}.$ So the quantities that we are asked to compare are the same. However, Arnol'd also says Children five to six years old like them very much and are able to solve them, but they
may be too difficult for university graduates, who are spoiled by formal mathematical training. Given the simple nature of the solution, I'm going to guess that there is a trick to it. How would a six year old solve this problem? My university education is interfering with my thinking. $\color{red}{\star}\quad$ S. H. Lui, An interview with Vladimir Arnol′d , Notices of the AMS, April 1997.","['algebra-precalculus', 'recreational-mathematics']"
3910662,Why does solving for the inverse function leads $x$ to be a constant?,"I went down the wrong path in trying to show that; if $f(x)=ax+b$ then there is a $g(x)=dx+e$ such that $f(g(x)) = x$ . This wrong path led me to a baffling result of x being a constant. Can anyone explain either my error or what it means?  Note that on my 2nd attempt I did derive $g(x)$ , so that is not my question.  My question is what does the following mean?  How can x be a constant? I did very simple substitution, just to see where it led me: if $f(g(x)) = x$ then $f(dx+e) = x$ and substituting; $a(dx+e) +b = x$ and expanding $adx +ae + b = x$ and grouping $(ad-1)x = -ae - b$ $x = \frac{-ae-b}{ad-1}$ which is a constant I know this is not how to get the inverse function.  I was able to figure that out.  But why do the above steps lead to an obviously impossible value for x?","['functions', 'inverse-function']"
3910673,show this $\prod_{i=1}^{\frac{p-1}{2}}a_{i}\equiv\prod_{i=1}^{\frac{p-1}{2}}b_{i}\pmod {p^3}$,"let $p=8k+1$ prime number, show that there exist two set $A=\{a_{1},a_{2},\cdots,a_{\frac{p-1}{2}}\},B=\{b_{1},b_{2},\cdots,b_{\frac{p-1}{2}}\}$ such that $A\bigcup B=\{1,2,3,\cdots,p-1\},A\bigcap B=\varnothing $ ,and $$\prod_{i=1}^{\frac{p-1}{2}}a_{i}\equiv\prod_{i=1}^{\frac{p-1}{2}}b_{i}\pmod {p^3}$$ This problem should be treated with the knowledge of quadratic residue.",['number-theory']
3910836,"Tessellation ""open"" problems for middle/high-schooler","I am a teacher and am currently tutoring middle/high-school students for research initiation, I gave small groups of them topics and let them advance by trials and error with essentially no guidance (they work one hour a week on it). I tried to find topics both doable and interesting, letting open many directions to go further, so that everyone can be sure of finding things but also won't get bored if they understand too fast. One of these topics is the following one: we want to tile a wall, but minimizing the ratio perimeter/surface (I added a fancy background to the problem, but this is it). That would mean they should understand what polygons can tile the plane or not and to do some optimization on the possible polygons, maybe also to understand also that getting closer to the circle is more efficient, and trying non-regular tessellations. However, the group with this topic already understand/know most things (they still picked that topic): they right away thought hexagons are the best and know that there aren't much possible tillings. They of course still need to prove and compute a bit, but I am fearing I won't be able to keep them occupied the whole year with it. I am searching for ideas to go further in this direction. I thought of: consider non-regular (or maybe with two polygons only),but I fear this will be worse than the hexagons letting them consider the boundary problem, and to wonder about the exact cost taking into account small pieces of tile to finish the boundary of the wall making them prove it is optimal even among non-regular tiling (I don't know whether it is true or not, nor have I any idea about how to approach this) can I ask the analogous question on the sphere? Is the answer the same and the problem not much more interesting? (or too hard?) Any less trivial idea or any lights about the above ideas would be of great help!","['tessellations', 'geometry', 'education', 'discrete-mathematics', 'discrete-optimization']"
3910875,How to find $\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n$,"Let $c \neq 1$ be a postive real number. Find the following limit $$\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n.$$ I know that $\lim_{n \to \infty} \Big( 1 + \frac{c}{n} \Big)^{bn} = e^{bc}$ . But it cannot be used here, since $\ln(n)$ is not a constant.  I also know how to find the $\lim_{n \to \infty} \Big( 1 + \frac{1}{n^2} \Big)^{n}$ , which is maybe closer to our limit. I tried to use some methods (e.g., L'Hopital's rule) that appear in the proofs of the above limits to solve also $\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n$ but without success. The graphs of $\Big (1 - \frac{c \ln(n)}{n} \Big)^n$ indicate that the limit is $\infty$ if $c < 1$ and the limit is 0 otherwise. I appreciate any suggestions on how to solve this limit. Also, any readings on this topic (I did not find any such example anywhere) are appreciated.","['limits', 'real-analysis']"
3910928,"Find all positive integer solutions of $(a,b,n)$ s.t. $(2^a-1)(2^b-1)=n^2$","So recently I've received a problem (title) and I have no idea how to solve it. By easy observation, $(a,b)=(m,m),(6,3),(3,6)$ are the cases I've found, and no others. So I try to let $kA^2=2^a-1$ and $kB^2=2^b-1$ , where $k = \gcd(2^a-1,2^b-1)=2^{\gcd(a,b)}-1$ . Then, another equation appears: $$\dfrac{2^a-1}{2^{\gcd(a,b)}-1}=A^2$$ the other one is similar. This expression is a bit ugly so I let $\gcd(a,b)=x$ , $a=xy$ . The equation then becomes: $$\dfrac{2^{xy}-1}{2^x-1}=A^2$$ which is much nicer. The solutions I have found for now (which is according to the (a,b) solutions above) are $(x,y)=(x,1),(3,2)$ . Also, by modulus and some algebra, if there are more solutions we just found, then $x\ge3$ and $y\ge 5$ . Well that's it. I can't go further more. Please give me some idea or even tips or solutions. Also, no computer is allowed . Thank you.","['number-theory', 'diophantine-equations']"
3910957,A man throws 4 dice. What is the probability that same number shows up in at least two faces?,"I saw a question  in internet , it seems very trivial.I tried to solve it in two different way but i obtained two different result. I know that my second solution is correct but why is the former wrong ? What am i missing ? Question: A man throws $4$ dice. What is the probability that same number shows up in at least two faces? First Solution: The probability of getting ONLY $2 $ dice showing the same face : $\frac{C(4,2).6.5.4}{6.6.6.6}=\frac{120}{216}$ The probability of getting ONLY $3 $ dice showing the same face : $\frac{C(4,3).6.5}{6.6.6.6}=\frac{20}{216}$ The probability of getting ONLY $4 $ dice showing the same face : $\frac{C(4,4).6}{6.6.6.6}=\frac{1}{216}$ $\therefore \frac{141}{216}=0.652$ Second Solution: All situations - all dice appears different faces $1- \frac{6.5.4.3}{6.6.6.6}=0.722$ What am i missing ?","['discrete-mathematics', 'combinatorics', 'probability']"
3910996,Pullback of a Volume Form,"Suppose we have a a diffeomorphism between two manifolds, $f: M \rightarrow N$ and a volume form $\Omega$ on $N$ . Then is it true that $f^{*}(\Omega) = \Upsilon$ will always be a volume form on $M$ ? My thinking is that for any arbitrary vector fields $(X_1, X_2,...,X_N) \in T_pM$ then $\Upsilon(X_1, X_2,..., X_N) = f^{*}\Omega(X_1, X_2,..., X_N) = \Omega(df_x(X_1),df_x(X_2),...df_x(X_N))$ is always non vanishing given that $\Omega$ is a volume form on $N$ ? This would be because $\Omega$ is acting on the tangent vectors $df_x(X_i) \in T_{f(p)}N$ , which is non-vanishing given that $\Omega$ is a volume form on N. I'm new to differential geometry, so any mistakes with notation, or comments which aren't accurate, please do let me know!","['vector-spaces', 'tangent-spaces', 'co-tangent-space', 'differential-forms', 'differential-geometry']"
3911127,The interior of $\{f \leq 0\}$ is $\{ f < 0\}$,"Let $X\subseteq \mathbb{R}^n$ , $f: X \rightarrow \mathbb{R}$ : what further conditions on $X$ and $f$ do we need to ensure that $\text{Int}\{x|f(x)\leq 0\}=\{x|f(x)<0\}$ ? Context : I've encountered this kind of question when dealing with open bounded $X$ with $C^1$ boundary, in partial differential equations, but I'm sure that one could benefit from such a fact in optimization, for instance. Clearly, continuity is not enough, since $f:[0,1]\rightarrow \mathbb{R} , f(x)=0$ is such that $[0,1]=\{f\leq 0\}$ , but the interior of this set is not the empty set.","['general-topology', 'real-analysis']"
3911130,"If $Q(v)=\left(D^{2} f(a) v, v\right)$ is indefinite form, then prove that $f$ has a saddle point at $a$.","It is asked that $Q(v)=\left(D^{2} f(a) v, v\right)$ is indefinite form then I have to show that $f$ has a saddle point at $a$ . I am having difficulty in understanding the problem. What does $Q(v)$ represents? I think the bracket there is for inner product. And what is meant by the indefinite form here?","['multivariable-calculus', 'derivatives', 'real-analysis']"
3911136,Proof of more general strong law of large numbers for dependent random variables,"I want to show the following version of the strong law of large numbers. This is the longest proof I've ever attempted, and I feel a bit overwhelmed. Let $X_i$ be a sequence of real-valued
random variables with $E(X_i^2)<\infty$ for all $i$ . If there is a sequence $c_k\subseteq[0,\infty)$ such that $C=\sum_{k=0}^\infty c_k <\infty$ and $\operatorname{Cov}(X_i,X_j)\le c_{|i-j|}$ , then $$\lim_{n\rightarrow\infty}\frac1n\sum_{i=1}^n(X_i-E(X_i))=0$$ I have the following proof outline provided to me: Show $V(\sum_{a+1}^bX_i)\le 2(b-a)C$ (Done) Subsequence : Show $$\lim_{k\rightarrow\infty}\frac1{k^2}\sum_{i=1}^{k^2}(X_i-E(X_i))=0\text { a.s.}$$ Fluctuation : Show $$\lim_{k\rightarrow\infty}\frac1{k^2}\max\left\{\left\lvert\sum_{i=k^2+1}^m(X_i-E(X_i))\right\rvert:m\in\{k^2,...,k^2+2k\}\right\} = 0\text{ a.s.}$$ Deduce the claim I have successfully shown (1). I'm not sure it's correct, but I think I've shown (2) like so: For sufficiently small $\delta$ we have $$\begin{aligned}P\left(\left\lvert\lim_{k\rightarrow\infty}\frac1{k^2}\sum_{i=1}^{k^2}(X_i-E(X_i))\right\rvert > 0\right) 
&= P\left(\left\lvert\lim_{k\rightarrow\infty}\frac1{k^2}\sum_{i=1}^{k^2}(X_i-E(X_i))\right\rvert \ge \delta\right)\\
&\le \lim_{k\rightarrow\infty}\frac1{k^4}\frac{V(\sum_{i=1}^{k^2}(X_i-E(X_i))}{\delta^2} = 0
\end{aligned}$$ Is this valid? I'm not sure about the way I rewrote the inequality and pulled the limit out of the variance. I have no ideas for how to show (3), I don't know how to ""break"" that maximum. Also, the final step is not obvious to me, again because the max-statement confuses me. How can I close out this proof? Does someone know a reference for a proof of this law?","['probability-limit-theorems', 'limits', 'law-of-large-numbers', 'probability-theory', 'probability']"
3911153,Finding a function with a set of points [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have a function graph and I need to find the matching function. Here is the graph : The graph And here are three points from the graph : (-2;0) (0;-6) (3;0) Note : As you can see, this is not a linear function Does anyone know how to achieve that?
I was thinking of interpolation, the problem is that I don't find the formula...","['interpolation', 'functions']"
3911179,Orthogonal polynomials with respect to $e^{-|x|} \mathrm{d} x$ on the entire real line?,"The Laguerre polynomials https://en.wikipedia.org/wiki/Laguerre_polynomials form a system of orthogonal polynomials with respect to the measure $e^{ -x} \mathrm{d} x$ on $(0,\infty)$ . Is anything known about the orthogonal polynomials with respect to the measure $e^{ -|x|} \mathrm{d} x$ on $(-\infty,\infty)$ ? Thank you!","['laguerre-polynomials', 'ordinary-differential-equations', 'real-analysis', 'polynomials', 'orthogonal-polynomials']"
3911187,How to calculate the Fourier transform of the Kaiser-Bessel window?,"According to Wikipedia: https://en.wikipedia.org/wiki/Kaiser_window , the Fourier transform of the Kaiser-Bessel window $w_0(x) :=  \left\{ \begin{array}{**lr**} \frac{I_0(\pi \alpha \sqrt{1-{(2x/L)}^2})}{I_0(\pi \alpha)}, &  |x| \leq \frac{L}{2}\\  
             0, & |x| > \frac{L}{2}\\  
             \end{array} \right.  $ is $W(f) := \frac{L\cdot \sinh(\pi\alpha\sqrt{1-(Lf/\alpha)^2})}{I_0(\pi\alpha) \cdot \pi\alpha\sqrt{1-(Lf/\alpha)^2}}$ , where $I_0(x) := \sum\limits_{m=0}^{\infty}\frac{1}{(m!)^2}(\frac{x}{2})^{2m}$ is the zeroth-order modified Bessel function of the first kind ( https://en.wikipedia.org/wiki/Bessel_function#Modified_Bessel_functions:_I%CE%B1,_K%CE%B1 ). I have made many attempts but fail to deduce the closed-form solution of the Fourier transform of the Kaiser-Bessel window. First, I tried to expand $I_0(x)$ , which was quite difficult to calculate since it involved infinite series. Second,  I tried to calculate the Fourier transform from the opposite side, i.e., $W(f)$ , but got no result. Finally, I tried to use Mathematica Professional 12, but it failed to offer a result after long time calculation. I have also found some literature online, e.g., [1] On the use of the I0-sinh window for spectrum analysis, IEEE Transactions on Acoustics, Speech, and Signal Processing, J. Kaiser, R. Schafer, 1980, doi: 10.1109/TASSP.1980.1163349. [2] On the use of windows for harmonic analysis with the discrete Fourier transform, Proceedings of the IEEE, F.J. Harris, 1978, doi: 10.1109/PROC.1978.10837. [3] https://www.dsprelated.com/freebooks/sasp/Kaiser_Window.html [4] Verification of Fourier transformation of Io-sinh function (MathStackExchange; No answer yet). Neither of them deduces the closed form of the Fourier transform of the Kaiser-Bessel window. Some references, e.g., [1] and [3], point to an old unavailable book named ""System Analysis by Digital Computer"". Kaiser-Bessel window is a widely used window function in signal processing. However, it is really difficult to find relevant literature online. Additionally, I am not sure whether the Fourier transform provided in https://en.wikipedia.org/wiki/Kaiser_window is a closed-form or an approximation. I deeply appreciate your help or hints.","['complex-analysis', 'calculus', 'signal-processing', 'fourier-transform']"
3911200,Differentiability Implies Continuity (Multivariable Calculus),"I am reading Hubbard and Hubbard's Vector Calculus, Linear Algebra, and Differential Forms, and in it they provide the following theorem and proof. My issue is not with the theorem, but more so with the proof.
Note that, in the theorem below, Equation 1.7.20 refers to the basic definition of the (multivariable) derivative, and $L$ is the linear transformation which defines the derivative. In particular, they seem to implicitly be distributing the limit in order to make the desired conclusion. That is, they seem to be using the theorem below (bullet point 1). But this theorem of course requires the hypothesis that both $f$ and $g$ have the given limit. Translating to our case in the proof of Prop. 1.7.11, use of Theorem 1.5.26 in the proof would require a priori knowledge that $$ \lim_{\mathbf{h}\to \mathbf{0}}\mathbf{f}(\mathbf{a}+\mathbf{h})-\mathbf{f}(\mathbf{a})$$ exists (the second term in the limit of course exists, as justified). But how can we know this without resorting to the epsilon-delta definition of the limit? Indeed, that this limit exists (and is 0) is precisely what we are trying to prove. Thus, is this one of those instances where the authors intend for you to fill in the details, or is there really some way to deduce the result without resorting to epsilons and deltas (I imagine it's quite simple).","['continuity', 'multivariable-calculus', 'derivatives', 'proof-explanation']"
3911223,The expected number of heads,"Given $10$ fair coins: In the first round, we toss each coin once which gives us a combination of heads and tails. In the second round, we only toss those coins that landed on the tail in the first round. What is the expected number of heads after this experiment $?$ Intuition tells me is $5 + 2.5 = 7.5$ .","['statistics', 'probability', 'random-variables']"
3911269,"Mean, variance, .., higher variances?","TL; DR Let $e,v$ be two real numbers. The constant distribution $f \equiv e$ is universal in the sense that it has the most entropy among those that have mean being $e$ . We define variance using this fact. The normal distribution $N(e,v)$ is universal in the sense that it has the most entropy among those that have mean being $e$ and variance being $v$ . We define 2-variance using this fact. What's next? What's the name of the study along this vein? Original post The study of a random variable $X$ is the study of its underlying distribution $P_X$ . However, it's too expensive, so we wish to start simple. To make life even simpler, let's assume all variables are from $\mathbb{R}$ to $\mathbb{R}$ . The simplest yet important information of $X$ is its mean $E(X)$ . Once we know its mean, we can ask further questions.. like how much does $X$ differ from the universal distribution among those with mean $E(X)$ ? Here by universal , I mean the distribution(s) possessing the maximal entropy among those that have the same mean. Of course, it's the constant distribution $f \equiv E(X)$ , taking value at $E(X)$ . Moving forward, we wish to see how much $X$ differs from $E(X)$ . We of course take their difference $|X - E(X)|$ , but decide to consider instead of its square $(X-E(X))^2$ .. because why not? It's both easier to analyze and yet equivalent to the difference. Same as when we thought $X$ contains too much information, so does $(X-E(X))^2$ . Hence, we again consider its mean $E((X-E(X))^2)$ , and coin it to be $Var(X)$ . We wish to move forward, and derive higher versions of variances. I'm aware of usual higher moments $E((X - E(X))^n)$ but that's not what I want. What I want is to keep following the same vein of thought, and consider the difference of $X$ with the universal distribution that has the known attributes. Henceforth, we consider the difference $|X - N(E(X),Var(X))|$ , where $N$ denotes the normal distribution, because the normal distribution is the one that has the most entropy among those that have mean $E(X)$ and variance $Var(X)$ . Let's again consider the square with the same reason as above. And so we define the $2$ -variance $$Var_2(X) := E ( (X-N(E(X),Var(X))^2 ),$$ and so on. Questions What is the universal distribution with mean $E(X)$ , variance $Var(X)$ , and 2-variance $Var_2(X)$ ? Again by universal I mean the one that has the most entropy among those having the same known attributes. What's the standard name for $Var_2, Var_3, \cdots$ ?","['statistics', 'entropy', 'functional-analysis', 'probability', 'random-variables']"
3911285,Different versions of the entropy term in the entropy-regularized Wasserstein distance,"\begin{equation}
\mathcal{W}_\epsilon(\alpha, \beta) = \min_{\pi\in \Pi(\alpha\beta)} \int c(x,y) \mathrm{d}\pi(x,y) + \epsilon H(\pi \| \alpha \otimes \beta)
\end{equation} Cuturi (2013) introduced the entropy-regularized Wasserstein distance, or Sinkhorn distance, shown above, where $\epsilon $ is the regularization parameter and $H(\pi \| \alpha \otimes \beta)$ is the relative entropy, or KL-divergence, between the transport plan and the marginal probabilities. But I have seen the $H(\cdot)$ term shown in two different ways, one with entropy and the other with relative entropy: \begin{align}
H(\pi) &= \int \pi(x,y) \ln \pi(x,y) \\
H(\pi \| \alpha \otimes \beta)  &= \int \ln \left(\frac{\mathrm{d}\pi (x,y)}{\mathrm{d}\alpha(x)  \mathrm{d}\beta(y)  } \right) \mathrm{d}\pi (x,y)
\end{align} How are the last two lines equal or connected to each other? Obviously they're not the same, so why are there two different versions running around?","['statistics', 'entropy', 'probability-distributions', 'optimal-transport', 'probability-theory']"
3911318,Prove that $\arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x))$ for $0\leq x <1$,"It's inspired by a general result proposed in a book (Dictionary of inequalities second edition by Peter Bullen p.101) related to composition of  function : In fact we have $x\in[0,1)$ : $$\arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x))\quad (1)$$ To prove it I have introduced a function : $$f(x)=\arctan(\operatorname{arctanh}(x))-\operatorname{arctanh}(\arctan(x))$$ We differentiate to get: $$f'(x)= -\frac{1}{((x^2-1)(\operatorname{arctanh}^2(x)+1))}+\frac{1}{((x^2+1)(\arctan^2(x)-1))}$$ And then I use : Let $0\leq x <1$ then we have : $$\operatorname{arctanh}(x)\geq x \geq \arctan(x)$$ Then I cannot conclude directly . Question : How to prove $(1)$ ? Thanks in advance !","['hyperbolic-functions', 'trigonometry', 'derivatives', 'inequality']"
3911402,Unpicking proof that det($AB$) = det$(A)$det$(B)$,"I'm trying to understand this proof that $\text{det}(AB) =\text{det}(A)\text{det}(B)$ , but I'm a little stuck on the third line. Please could someone explain to me what the mapping $\kappa$ does? Let $\mathfrak{T}_n = {\rm Maps} (\{1, \ldots , n\}, \{1, \ldots , n\})$ \begin{eqnarray*}
{\sf det}(AB) & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n (AB)_{i\sigma(i)} \\ & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n \sum_{j=1}^n a_{ij} b_{j\sigma(i)} \\
& = & \sum_{\sigma\in \mathfrak{S}_n, \kappa \in \mathfrak{T}_n} {\sf sgn}(\sigma) a_{1\kappa(1)}b_{\kappa(1) \sigma(1)} \ldots a_{n \kappa(n)} b_{\kappa (n) \sigma (n)} \\
& = &\sum_{\kappa\in \mathfrak{T}_n} a_{1\kappa(1)} \ldots a_{n \kappa(n)} \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) b_{\kappa(1) \sigma(1)}\ldots b_{\kappa (n) \sigma (n)} \\
& = & \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
\end{eqnarray*} where $B_{\kappa}$ denotes the matrix which is obtained by using the rows of $B$ labelled $\kappa(1), \ldots , \kappa (n)$ for its rows $1$ to $n$ .
. Therefore: $$ \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
 = \sum_{\kappa\in \mathfrak{S}_n}  {\sf sgn}(\kappa) a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B) = \det (A) \det (B).$$ Thanks","['matrices', 'determinant']"
3911496,Computing Ramanujan asymptotic formula from Rademacher's formula for the partition function,"I am trying to derive the Hardy-Ramanujan asymptotic formula $$p(n) \sim \frac{1}{4n\sqrt{3}}e^{\pi\sqrt{\frac{2n}{3}}}$$ from Radmacher's formula for the partition function $p(n)$ given by $$p(n)=\frac{1}{\pi\sqrt{2}}\sum_{k=1}^{\infty}A_{k}(n)\sqrt{k}\left[\frac{d}{dx}\frac{sinh\left(\frac{\pi}{k}\sqrt{\frac{2}{3}\left(x-\frac{1}{24}\right)}\right)}{\sqrt{x-\frac{1}{24}}}\right]_{x=n}$$ where $$A_{k}(n)=\sum_{h=0, (h,k)=1}^{k-1}e^{\pi i(s(h,k)-2n\frac{h}{k})}$$ and $$s(h,k)=\sum_{r=1}^{k-1}\frac{r}{k}\left(\frac{hr}{k}-\left\lfloor\frac{hr}{k}\right\rfloor-\frac{1}{2}\right)$$ G.E. Andrews, and any other literature on this topic, says that we can obtain the H-R asymptotic expression from the first term of the Rademacher series, i.e. for $k=1$ . I don't know how to approach this as simply calculating for $k=1$ does not give the desired result. Could we perhaps try and use Lapalce's method, or the method of steepest descent?","['integer-partitions', 'number-theory', 'elementary-number-theory', 'ramanujan-summation', 'combinatorics']"
3911594,Proof of formula for permutation without repeating elements?,"My book wants me to find every unique word of ""ABRAKADABRA"" and since I know the formula: Factorial of the set length divided by the product of the integer values of how many time an element has repeated itself, in this case = 11!/(5!*2!*2!) Though I realise I can't rationalise this in my head. I tried painting a choice tree of something like the word ""ABBA"" but did not get a better understanding.","['permutations', 'proof-explanation', 'solution-verification', 'discrete-mathematics']"
3911870,dot product in a general coordinate system,"NOTE: Einstein summation is assumed. Also, the component expanded vector in the standard basis will be expressed with square brackets and in the curvilinear basis with round brackets. E.g, in the case of plane polar coordinates, $$\begin{bmatrix}
1\\
\sqrt{3}
\end{bmatrix} =\begin{pmatrix}
2\\
\pi /3
\end{pmatrix}$$ Links to similar questions: 1 and 2 . The reason I'm asking this question is because the answers given in these posts were pretty unsatisfactory - I don't want to assume the coordinates are orthogonal and I'm ok with the result being somewhat circular in nature. All I'm looking for is a nice succinct formula that gives correct results. I'm currently studying differential geometry. I'm sure we are all aware of the formula for the inner product of two vectors in the standard basis $$\langle\underline{u},\underline{v}\rangle_{\text{std.}}=\delta_{ij}u^iv^j$$ However, this is only true if the vectors are expressed in the standard basis. For instance we know that in plane polar coordinates, the dot product is obviously not $r r'+\theta\theta'$ . Wikipedia cites the formula $$\langle\underline{u},\underline{v}\rangle_{\mathscr{C}}=g_{ij}u^iv^j$$ As the inner product for a general coordinate system $\mathscr{C}$ .
Now obviously, it shouldn't matter what coordinate basis we take the inner product with respect to. The only reason for the above notation is to emphasize that the form of the inner product will change from one coordinate system to another, but indeed for a particular pair of vectors its value will not. However when this formula is applied to plane polar coordinates, this gives the strange formula $$\langle\underline{u},\underline{v}\rangle_{\text{polar}}=u_rv_r+r^2u_\theta v_\theta$$ Which doesn't really make sense. In this context, what even is $r$ ? In fact, the above formula fails for obvious reasons - the metric tensor varies from point to point, e.g for the vector $[~1~~~1~]^{\mathrm{T}}=(~\sqrt{2}~~~\pi/4~)^{\mathrm{T}}$ the metric tensor is $$\mathbf{g}(\sqrt{2},\pi/4)=\begin{bmatrix}
\begin{bmatrix}
1 & 0
\end{bmatrix} & \begin{bmatrix}
0 & 2
\end{bmatrix}
\end{bmatrix}$$ Whereas for the vector $[~5~~~12~]^{\mathrm{T}}=(~13~~~\arctan(12/5)~)^{\mathrm{T}}$ it is $$\mathbf{g}(13,\arctan(12/5))=\begin{bmatrix}
\begin{bmatrix}
1 & 0
\end{bmatrix} & \begin{bmatrix}
0 & 169
\end{bmatrix}
\end{bmatrix}$$ So what are Wiki referring to by $g_{ij}$ in the referenced formula? But , even if the two vectors have the same metric tensor, i.e, same $r$ component, the formula still fails, as we presumably get $$\langle(r,\theta_1),(r,\theta_2)\rangle_{\text{polar}}=r^2(1+\theta_1\theta_2)$$ Which is clearly not correct. So here is my question: Let's suppose we have two vectors in $\mathbb{R}^n$ which in the standard basis are $$\underline{v}=\begin{bmatrix}
x^{1}\\
\vdots \\
x^{n}
\end{bmatrix} \ ;\ \underline{v'} =\begin{bmatrix}
{x'} ^{1}\\
\vdots \\
{x'} ^{n}
\end{bmatrix}$$ And in some curvilinear basis are $$\underline{v}=\begin{pmatrix}
q^{1}\\
\vdots \\
q^{n}
\end{pmatrix}~;~\underline{v'}=\begin{pmatrix}
{q'}^{1}\\
\vdots \\
{q'}^{n}
\end{pmatrix}$$ Let's suppose we know the coordinate transformation functions $q^i=q^i(x^1,...,x^n)$ and also the reverse coordinate transformation functions $x^i=x^i(q^1,...,q^n)$ and we are given the explicit form of the metric tensor $\mathbf{g}$ for the coordinate transformation $[~x^1~~~...~~~x^n~]^{\mathrm{T}}\to(~q^1~~~...~~~q^n~)^{\mathrm{T}}$ . Given the two vectors in their curvilinear components , is there any way to work out $\langle\underline{v},\underline{v'}\rangle$ without resorting to converting back to the standard basis? That is, is there any magic formula $$\langle\underline{v},\underline{v'}\rangle_{\mathscr{C}}=f\left((q^1,...,q^n),({q'}^1,...,{q'}^n)\right)$$","['curvilinear-coordinates', 'differential-geometry']"
3911908,Why do fixed point theorems appear all over mathematics?,"For example, the Banach fixed-point theorem is applied in the proof of the Picard–Lindelöf theorem about the uniqueness of solutions of ordinary differential equations and the Lefschetz fixed-point theorem (or a modification of it) is used in the proof or in the context of the Weil conjectures. There are so many more examples. What is so special about the equation $f(x)=x$ ?","['fixed-point-theorems', 'motivation', 'soft-question', 'ordinary-differential-equations']"
3911935,Chain rule when taking non-dimensionalising an ODE,"I have a silly question. So let's say we have: $$\frac{d^{2}x}{dt^{2}} = kx$$ Now let's say we pick $X = \frac{x}{x_{c}}$ and $T = \frac{t}{t_{c}}$ . What I don't understand is, if we plug in $Xx_{c}$ for x, and $Tt_{c}$ for t, how come we get: $$\frac{x_{c}d^{2}X}{t_{c}^{2}dT^{2}}$$ How come the $t_{c}$ is squared? Can someone do the math and explain it to me?  Our professo r just said. it was. chain rule but I am not sure how","['multivariable-calculus', 'calculus', 'derivatives', 'chain-rule']"
3911939,"Linear algebra: proof of sufficient conditions for $x,Ax,A^{2}x,\dots,$ being linearly independent","This questsion is an extension of a previously asked question that I think needs a more rigorous proof (please see the link and the answer by user Aaron). The question is, given a little bit of different notation (in accordance with control theory), for $b\in\mathbb{R}^{n}$ and $A\in\mathbb{R}^{n\times n}$ , what is a sufficient condition for \begin{align}
b,Ab,A^{2}b,\dots,A^{n-1}b
\end{align} all being linearly independent? Furthermore, if we assume that $A$ has $n$ distinct eigenvalues $\lambda_{i}$ , $i=1,\dots,n$ , is this condition sufficient for the above to hold and if so how do we prove it? In the above linked question Aaron's answer was: If the characteristic polynomial of $A$ has no repeated roots, then we have a basis $\{v_{i}\;\vert\;1\leq i\leq n\}$ of eigenvectors for $\mathbb{C}^{n}$ , and we can express $x=\sum \alpha_{i}v_{i}$ in terms of the basis. If all of the $\alpha_{i}$ are non-zero, then all of your vectors will be linearly independent. In fact, with this condition on $A$ , this is a necessary and sufficient condition on $x$ . If $A$ has an eigenvalue with geometric multiplicity greater than 1, then no $x$ will work. If $A$ has repeated eigenvalues but all eigenvalues have geometric multiplicity of 1, then it is still possible to find such an $x$ , but things are a bit more complicated. I would be very happy if someone could fill in the blanks for his statements, it is not immediately evident that a $b$ with nonzero entries leads to all vectors $b,Ab,\dots,A^{n-1}b$ being linearly independent.","['linear-algebra', 'control-theory']"
3911942,How to find estimator for $\lambda$ for $X\sim \operatorname{Poisson}(\lambda)$ using the 2nd method of moment?,"So an estimator for for $\lambda$ using the first method of moment is simply the sample mean: $$\mu_1 = E(X) = \lambda = \bar{X} = \hat{\mu_2}$$ For the second moment, $$\operatorname{Var}(X) = E(X^2) - E(X)^2$$ $$\lambda = E(X^2) - \lambda^2$$ $$\mu_2 = E(X^2) = \lambda + \lambda^2$$ $$s^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$$ $$s^2 = \frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}^2$$ $$\frac{1}{n}\sum_{i=1}^n X_i^2 = s^2 + \bar{X}^2$$ $$\hat{\mu_2} = s^2 + \bar{X}^2$$ Making $\mu_2 = \hat{\mu_2}$ , we get: $$\mu_2 = \lambda + \lambda^2 = s^2 + \bar{X}^2 = \hat{\mu_2}$$ $$ \lambda + \lambda^2 = s^2 + \bar{X}^2$$ From here I've no idea how to factorise it so I have $\lambda$ all on one side.","['statistical-inference', 'statistics', 'parameter-estimation', 'poisson-distribution']"
3912049,"""Piecewise Linear"" Differential Equation","I'm interested in the following ""piecewise-linear"" ordinary differential equation: $$
\ddot{x}(t) = \begin{cases}
-k_+ x(t) - b_+ \dot x(t) + u(t) & x(t) \geq 0\\
-k_- x(t) - b_- \dot x(t) + u(t) & x(t) < 0,
\end{cases}
\quad x(0) = x_0, \quad \dot x(0) = \dot x_0,
$$ where we are given some (smooth) forcing function $u(t)$ and positive coefficients $k_{\pm}$ and $b_{\pm}$ .  If you prefer, I have step-function coefficients $k(x)$ and $b(x)$ , and my equation is of the form $$
\ddot{x}(t) = -k(x)x(t) - b(x) x(t) + u(t).
$$ Ultimately, I'd like to be able to show that for two solutions $x_1,x_2$ , we have $\frac d{dt}|x_1(t) - x_2(t)| < 0$ . The fact that this is not simply a linear differential equation makes this difficult. One approach to the problem is to solve the (countably many) initial value problems, where we switch from one ODE to the other at each time at which we have $x(t) = 0$ , and the ""final values"" for the trajectory over $[0,t]$ serve as the ""initial values"" for the next trajectory. Is there a better way to approach this problem? Does this fall into any well-known class of problems? I would appreciate any ideas and any helpful keywords to search or resources to look for.","['ordinary-differential-equations', 'reference-request']"
3912094,$\lim\limits_{n\rightarrow +\infty}\int_0^{2k\pi}\sqrt{\sin^{2n}(x)+\cos^{2n}(x)}dx$,"I think the limit value of the function $\sin^{2n}(x) + \cos^{2n}(x)$ when $n$ tends to infinity is $0$ for all real values of $x$ except when $x$ is an integral multiple of $\pi$ , where it comes out to be one. Is my thinking correct? (Keep in mind that I am a high school student, so please bear with me if I've used incorrect mathematical terminologies anywhere, I'm apologizing in advance... Also, kindly try to explain as lucidly as possible.)
If this question is way above my level, then let me know that too in the comments. If my approach is correct, then that would mean the function is discrete (I'm not sure if its the right term to use here), so how would we find the integral of it?
If not, please guide me to the right approach to solve this question.","['integration', 'definite-integrals', 'trigonometric-integrals', 'limits', 'trigonometry']"
3912151,Infimum of inner product over a closed subspace,"Let $M\subset H$ a closed linear subspace of a Hilbert space H. Suppose $f \in H$ but $f \not\in M^{\perp}$ . Show that $$ \inf_{x\in M,\|x\|=1} (f,x)$$ is attained and the min is unique. So what I've tried so far was to use extreme value theorem. I know that the closed unit circle is compact with respect to $\sigma(H,H^*)$ due to reflexivity and $M$ is weakly closed as well so the intersection would be weakly compact, EVT would come in here until I realized the infimum is only over the shell intersect M, so my idea stops there. I also tried constructing a sequence $(f,x_n)$ converging to the infimum, since M is also reflexive and each $x_n$ has norm 1, there exists a weakly convergent subsequence. If $x$ is the weak limit and $\|x\|=1$ , I would be done. But I don't know how to show $\|x\|=1$ , I do know however $\|x\| \leq 1$ simply by weak convergence. Just looking for a hint!!
Thank you!","['hilbert-spaces', 'functional-analysis', 'real-analysis']"
3912228,"If $X$ is homeomorphic to dense subspace $Y\subseteq X$, then $X=Y$","It is a basic fact of topology that if $X$ is a topological space and $Y\subseteq X$ is homeomorphic to $X$ , it does not need to occur that $X=Y$ (for example, $X=\mathbb{R}$ , $Y=(0,1)$ ). My question is, if I add the requirement that $Y$ is dense in $X$ . Is this still the case? Or is the following true? If $X$ is homeomorphic to a dense subspace $Y\subseteq X$ , then $X=Y$ .",['general-topology']
3912230,"given the length of event $x$, in $90$ occurrences how many times will length length $x$ be greater than $3$?","the length of event $x$ has approximately an exponential distribution with a mean of 90 minutes. if event $x$ occurs $90$ times, how many times will the event last longer than $3$ hours? My work so far: here for exponential distribution parameter $β = 90$ $P$ (event occur after 3 hours (180 minutes)): $$P(X>180)=1-P(X<180)=1-(1-\exp(-180/90))=	0.1353$$ therefore the number of events that last longer than $3$ hours $$=np=90*0.1353= 12.177 \sim 12$$","['statistics', 'probability']"
3912262,Finite Sum of Infinite Sums is Infinite Sum of Finite Sums?,"If I have a finite sequence of $N$ functions $f_n\colon\mathbb{N}\to\mathbb{C}$ and a sequence of complex numbers $z_k$ , must it be true that $$\sum_{n=1}^{N} \sum_{k=1}^\infty f_n(z_k) = \sum_{k=1}^\infty \sum_{n=1}^N f_n(z_k)?$$ It seems that a similar question is addressed at Summation Symbol: Changing the Order , but this question addresses only the case where both sums are finite or both are infinite and doesn't seem to address what happens when we're considering the finite sequence of functions. Motivation for this question It seems like an equality of this form is used to prove Lemma 5.4 in the proof of Dirichlet's Theorem on Arithmetic Progressions in http://people.csail.mit.edu/kuat/courses/dirichlet.pdf , but the use of the identity isn't explicit so I'm not sure if I'm understading this right. I think I could make sense of the lemma's proof if the above formula always holds, but I don't know if that is a valid assumption or not. Any assistance is greatly appreciated!","['sequence-of-function', 'proof-explanation', 'summation', 'sequences-and-series']"
3912314,Edge Coverings of Complete Graph With Loops,"Let $G_{n,m} = (V,E)$ be a complete graph on $n$ vertices, $n \in \mathbb{Z}^+$ , with one additional feature: there are $m$ loops at every vertex, $m \in \mathbb{N}$ . In other words, for every vertex $u$ and for every vertex $v$ in $V$ , if $u \neq v$ , then there is exactly one $\{u,v\}\in E$ , and we have $\{v,v\}_i \in E$ for $i=1,2,...,m$ . I am trying to determine the number of edge coverings on such a graph, which I denote as $C(G_{n,m})$ . In other words, how many subsets of $E$ exist in which every vertex in $V$ is incident with at least one edge? I stumbled upon a paper conveying the edge coverings of a complete graph on $n$ vertices, denoted as $K_n$ , as $$C(K_n) = \sum_{j=0}^{n} (-1)^j \binom{n}{j} 2^{\binom{n-j}{2}}$$ It is also given for $k$ edges, $0 \le k \le \binom{n}{2}$ , $$C(K_n,k) = \sum_{j=0}^{n} (-1)^j \binom{n}{j} {\binom{\binom{n-j}{2}}{k}}$$ but I am still having difficulty given the graph I'm examining contains loops as well. My best attempt has consisted of subtracting from the powerset of $E$ all those subsets in which there is at least $1$ isolated vertex. I ended up with a recursive equation as follows: $$ C(G_{n,m}) = 2^{\binom{n}{2}+nm} - 1 - \sum_{t=1}^{n-1} \binom{n}{t} C(G_{n-t,m})$$ Any help is appreciated. Thanks!","['graph-theory', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory']"
3912325,"Find all differentiable functions $f : [0, +\infty) \rightarrow \mathbb{R}$ such that $f(1) = 1$ and $f(x)f(y) \leq f(xy)$","The exercise Find all differentiable functions $f : [0, +\infty) \rightarrow \mathbb{R}$ such that: $f(1) = 1$ $f(x)f(y) \leq f(xy)$ for all $x, y \geq 0$ My try I have found that the functions $x \mapsto x^\alpha$ for $\alpha \in [1, +\infty)$ are solutions. The constant function $x \mapsto 1$ is also solution. As $f(0)^2 \leq f(0)$ one also has that $f(0) \in [0, 1]$ . If I take a solution $f$ , my idea would be to try proving it is of the form $f(x) = x^\alpha$ for some $\alpha$ . Necessarily this $\alpha$ would be equal to $f'(1)$ . So I set $\alpha := f'(1)$ and considered $g(x) = \frac{f(x)}{x^\alpha}$ for $x > 0$ . One can show $g$ is derivable and $g'(1) = 0$ and $g$ verifies the functional equation (for $x, y > 0$ ). So the goal would be to show that $g = 1$ , but I've not managed to prove it yet. Any help is welcome folks!","['functional-equations', 'derivatives', 'real-analysis']"
3912353,"Show that $\max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|$.","Suppose $f(x) \in C^2([a, b])$ with $f(a) = f(b) = 0$ . Show that $$\max_{x \in [a, b]} |f'(x)| \leq \frac{(b-a)^2}{2} \max_{x \in [a, b]}|f''(x)|.$$ My work: Suppose that $f'(x_0) = \max_{x \in [a, b]} |f'(x)|$ . One can expand $f(x)$ at $x_0$ as follows: $$f(a) - f(x_0) = f'(x_0) (a - x_0) + \frac{1}{2}f''(\xi_1) (a-x_0)^2,~\xi_1\in[a, x_0]$$ $$f(b) - f(x_0) = f'(x_0) (b - x_0) + \frac{1}{2}f''(\xi_2) (b-x_0)^2,~\xi_2\in[x_0, b]$$ The difference of the two equations leads to $$f'(x_0) (b-a) = \frac{1}{2}\Big[f''(\xi_1)(a-x_0)^2 - f''(\xi_2)(b-x_0)^2\Big].$$ I have no idea how to reach the term $\max |f''(x)|$ .","['derivatives', 'real-analysis']"
3912378,Infinitely many positive integers $n$ such that $n$ divides $2^{2^{n}+1}+1$ and $n$ doesn’t divide $2^{n}+1$,"Show that there are infinitely many positive integers $n$ such that $n$ divides $2^{2^{n}+1}+1$ and $n$ does not divide $2^{n}+1$ I think we should increase $n$ by induction: $x_{k+1}$ = $2^{x_{k}}+1$ , then we have $x_{k}$ doesn’t divide $x_{k+1}$ and divide $x_{k+2}$ .
Next we should show that if it allows for $k=1,2,3,..m$ , then allows for $k=m+1$ . I am not getting if this approach is right. Thanks in advance.","['number-theory', 'divisibility']"
3912415,Defining immersions and submersions of manifolds in locally ringed spaces.,"I'm trying to understand the usual notions of ringed spaces in the setting of manifolds and vice versa. Recall the definitions of open and closed immersions: A morphism $f:X\to Y$ of locally ringed spaces is an open immersion if the underlying continuous map is a homeomorphism of $X$ onto an open subset $U$ of $Y$ such that the sheaf map $\mathscr{O}_Y\to f_*\mathscr{O}_X$ restricts to an isomorphism over $U$ . A morphism $f:X\to Y$ of locally ringed spaces is a closed immersion if the underlying continuous map is a homeomorphism of $X$ onto a closed subset of $Y$ such that the sheaf map $\mathscr{O}_Y\to f_*\mathscr{O}_X$ is surjective. If $X$ and $Y$ are (smooth) manifolds, I think that open / closed immersions are simply smooth morphisms whose image is open / closed in $Y$ and that restrict to diffeomorphisms on their images. (Please correct me if I'm wrong!) More generally, A morphism $f:X\to Y$ of locally ringed spaces is an immersion if the underlying continuous map is a homeomorphism of $X$ onto a locally closed subset $U$ of $Y$ such that for all $p\in X$ the stalk map $\mathscr{O}_{Y,f(p)}\to \mathscr{O}_{X,p}$ is surjective. I have two questions then: Is that the same thing as an embedding of manifolds? That is, a smooth map that is both a topological embedding and an immersion (not in the sense above but in the usual manifold sense). Can we describe immersions and submersions (in the usual manifold sense) in a similar fashion?","['algebraic-geometry', 'differential-geometry']"
3912429,Jensen's Formula and the number of zeros.,"I am doing a question that requires me to find the number of zeros in a disk when $f$ is non-constant, bounded and analytic on $D$ . I saw that the Wikipedia page of Jensen's formula has the following information: Jensen's formula can be used to estimate the number of zeros of analytic function in a circle. Namely, if f is a function analytic in a disk of radius R centered at $z_0$ and if |f| is bounded by M on the boundary of that disk, then the number of zeros of f in a circle of radius r < R centered at the same point $z_0$ does not exceed Does anyone know how this formula is derived?",['complex-analysis']
3912560,"The matrix $B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right)$ is diagonalizable, if only if $A$ is diagonalizable.","Let $A\in\mathcal {M}_{n}(\mathbb {C}), $ I am trying to prove that: The matrix $B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right)$ is diagonalizable, if only if $A$ is diagonalizable. My effrot : $(\Rightarrow) $ we suppose that $B $ is diagonalizable then $m_B$ is a product of distinct linear polynomials.  Let $m_A(x)$ , $m_B(x)$ ,  be the minimal polynomials of $A$ , and $B$ , respectively. We have $$
0 = m_B(B) = \pmatrix{m_B(A) & 0\\(*)&m_B(A)}
$$ so, $m_B(A) = 0$ .  It follows that $m_A$ divides $m_B$ , thus $m_A $ is a product of distinct linear polynomials. Then $A $ is diagonalisable. For $(\Leftarrow )$ I have no idea An idea please.","['matrices', 'matrix-calculus', 'linear-algebra']"
3912574,Proving $f^{(n)} (0)$ s unbounded where $f(x)= \tan^{-1} x$,"The following question was asked in real analysis quiz and I am looking for an more elegant solution. Question :Let $f(x)= \tan^{-1}x $ for all $x\in \mathbb{R}$ . Prove that sequence $\{f^{(n)}(0)\}$ is unbounded. There is a solution given by differentiating $n$ times the expression $(1+x^2) f(x)$ and then using the recurrence relation $(1+x^2) f^{(n+1)} (x)+ 2nx f^{(n)} (x) + n(n-1) f^{(n-1)} (x)=0$ and $f'(0)=1$ . But is there a more elegant solution using any other concepts of other branches of pure mathematics or analysis (real, complex, functional). I thought I should ask here as I don't wanted to use that method although that is clear to me!","['derivatives', 'problem-solving', 'real-analysis']"
3912577,Inequality with mean inequality,"If $a,b,c > 0$ and $a+b+c = 18 abc$ , prove that $$\sqrt{3 +\frac{1}{a^{2}}}+\sqrt{3 +\frac{1}{b^{2}}}+\sqrt{3 +\frac{1}{c^{2}}}\geq 9$$ I started writing the left member as $\frac{\sqrt{3a^{2}+ 1}}{a}+ \frac{\sqrt{3b^{2}+ 1}}{b} + \frac{\sqrt{3c^{2}+ 1}}{c}$ and I applied AM-QM inequality, but I obtain something with $\sqrt[4]{3}$ .","['algebra-precalculus', 'cauchy-schwarz-inequality', 'a.m.-g.m.-inequality', 'inequality']"
3912591,How to show that $a_{n+2}=\frac{a_{n+1} ^2 -1}{a_n}$ . is bounded?,"Let $a_{n+2}=\frac{a_{n+1} ^2  -1}{a_n}$ be a sequence of real numbers where $a_n>0$ for all $n \in \mathbb{Z}_{+}$ . It is given that $a_1=1 , a_2=b>0$ . It is given that $1<b<2$ .Is it possible to show that $a_n$ is bounded from this given information?","['limits', 'sequences-and-series', 'cauchy-sequences', 'real-analysis']"
3912635,Trouble with the proof of Proposition 4.3.18 of Pedersen's Analysis Now,"I am currently trying to understand the proof of Proposition 4.3.18 in Pedersen's Analysis now, which reads To each Tychonoff space $X$ there is a Hausdorff compactification $\beta(X)$ , with the property that every continuous function $\Phi: X \to Y$ , where $Y$ is a compact Hausdorff space, extends to a continuous function $\beta \Phi: \beta(X) \to Y$ . The proof starts by noting that $C_b(X)$ is a commutative unital C $^*$ -algebra, and is therefore isometrically isomorphic to a (commutative and unital) C $^*$ -algebra of the form $C(\beta(X))$ , where $\beta(X)$ is a compact Hausdorff space. By the Gelfand duality between the category of commutative and unital C $^*$ -algebras and the category of compact Hausdorff spaces, we can take $\beta(X) = \Omega(C_b(X))$ , the space of characters on $C_b(X)$ . Then we can define a map $\iota: X \to \beta(X)$ , where $\iota(x)(\phi) := \phi(x)$ for all $x \in X$ and $\phi \in \beta(X)$ . The particular part of the proof that I am struggling to understand is the proof that $\iota(X)$ is dense in $\beta(X)$ . He argues that if $\iota(X)$ is not dense in $\beta(X)$ , then there is a non-zero continuous map $f: \beta(X) \to \mathbb{C}$ vanishing on $\iota(X)$ . This I understand. He then says that under the identification $C_b(X) = C(\beta(X))$ , this is impossible. This is the sentence I am stuck on. Why is it impossible under this identification? We have that $C_b(X)$ is isometrically isomorphic to $C(\Omega(C_b(X)))$ via the map $\delta: g \mapsto (\delta_g: \Omega(C_b(X)) \to \mathbb{C}, \phi \mapsto \phi(g))$ . I am pretty sure what Pedersen is getting at is that the map $\delta^{-1}(f)$ is zero, but I am not able to show that this is the case. This answer also claims that a similar map is zero. In summary, my question is: Can we show that $\iota(X)$ is dense in $\beta(X)$ by showing that $\delta^{-1}(f) = 0$ ? If so, how do we do this?","['c-star-algebras', 'banach-spaces', 'banach-algebras', 'functional-analysis', 'general-topology']"
3912771,Applications of Topos Theory to the theory of bundles.,"While reading the chapter on toposes in Spivak and Fong's ""Seven Sketches in compositionality"", I bumped into the following: Example 7.61 . For a vector bundle $π : E → X$ over a space $X$ , the corresponding sheaf
is $Sec(π)$ corresponding to its sections: to each open set $i_U : U ⊆ X$ , we associate the set
of functions $s : U → E$ for which $\pi\circ s=i_U$ . For example, in the case of the tangent
bundle $π : TM → M$ , the corresponding sheaf, call it $VF$ , associates
to each $U$ the set $VF(U)$ of vector fields on $U$ . The internal logic of the topos can then be used to consider properties of vector
fields. For example, one could have a predicate $Grad : VF → Ω$ that asks for the largest
subspace $Grad(v)$ on which a given vector field $v$ comes from the gradient of some
scalar function. One could also have a predicate that asks for the largest open set on
which a vector field is non-zero. Logical operations like $∧$ and $∨$ could then be applied
to hone in on precise submanifolds throughout which various desired properties hold,and to reason logically about what other properties are forced to hold there. I would be particularly interested in knowing something more about the second part of the example I reported here. In particular if such an apporach to study submanifolds of interest (for instance associated to differential equations, as suggested) has indeed been carried out or this is ""just"" some suggestive intuition. Any reference would be most appreciated. Thanks in advance","['category-theory', 'reference-request', 'vector-bundles', 'topos-theory', 'differential-geometry']"
3912794,MGF and Compound Poisson Distribution,"I have $N_{t}$ governed by a Compound Poisson $(t\lambda$ ) distribution with expected value $t\lambda$ and a sequence of $X_{i}$ where each $X_{i}$ is independent of $N_{t}$ . If $Y = \sum^{N_{t}}_{i=1} X_{i}$ , how can I find the mgf and first moment of $Y$ ? I am used to the continuous case where I can integrate and differentiate, but I don't think I can apply that here due to the discrete nature of this distribution.","['poisson-distribution', 'probability-distributions', 'moment-generating-functions', 'probability-theory', 'probability']"
3912801,Tree diagram Chain Rule,"Let $$
\begin{align}
w &= f(x,y,z)\\
z &= z(x,y)\\
x &= x(t)\\
y &= y(t)\\
\end{align}
$$ be differentiable functions. Find the formula for the derivative of $w$ with respect to $t$ . My answer: $w = f(x,y,z) \implies \dfrac{dw}{dt} = \dfrac{dw}{dx} \cdot \dfrac{dw}{dy} \cdot\dfrac{dw}{dz}$ $$
\begin{align}
z &= z(x,y) \\
x &= x(t) \\
y &= y(t) \\
\end{align}
\implies \frac{dz}{dt} = \frac{\partial z}{\partial x} \cdot \frac{dx}{dt} + \frac{\partial z}{\partial y}\cdot\frac{dy}{dt}
$$ It is getting confusing to me. I used to make a tree diagram to make it a bit easier but this time, it is out of my reach. Can someone give me a hint?",['multivariable-calculus']
3912824,"Prove that $e^n\bmod 1$ is dense in $[0,1]$","I just noticed that I have left unanswered one part of an old multi-part question and so decided to re-ask it separately: Consider the sequence $e^n\bmod 1$ , $n\in\Bbb N$ . Show that it is dense in $[0,1]$ . This apparently does require specific (approximation?) properties of $e$ , as for example replacing $e$ with any integer leads to a non-dense sequence. On the other hand, for every sequence of numbers $a_n\in(0,1)$ , it is not hard to find $\alpha$ such that $|\alpha^{2^n}\bmod 1- a_n|<\frac1n$ for all $n$ , or $\beta$ such that $|\beta^n\bmod 1-a_n|<\frac1{1000}$ . Hence there exist (irrational) bases that lead to a dense sequence and others that lead to a non-dense sequence.
Other than that I'm a bit at a dead end.",['real-analysis']
3912833,Stability of 2nd order linear vector differential equations?,Given the system $\ddot{x} +B \dot{x}+ C x =0$ where $x \in R^{2}$ and $B$ is positive definite symmetric matrix and I know that the eigenvalues of $C$ are either $\lambda_1<0<\lambda_2$ or $\lambda=\pm \omega j$ . My question is: Can I conclude from these givens that the system is unstable and can the type the equilibrium point be determined?,"['matrices', 'linear-algebra', 'ordinary-differential-equations', 'control-theory']"
3912871,How to solve this problem without using the binomial expansion?,"Consider the following problem: On a train route of 18 stations, in how many ways can a train stop at 4 stations so that there are at least 2 and at most 6 stations between any two of them? The above is the problem I have a question about. Meanwhile, consider another similar problem: A box contains 10 red, 12 yellow and 8 green balls. In how many ways can 14 balls be selected if at least one ball of each colour must be selected? The above is a sticks-and-stars problem with an upper bound on the number of stars between two sticks. We can reduce the above problem to permuting 2 sticks (3 colours) and 11 stars (taking 1 red, 1 yellow and 1 green ball of the 14 required). The total number of permutations is $13 \choose 2$ . From this we subtract the illegal cases like the one below: $$**********|*|$$ where the space to the left of the first stick represents red balls. Note that there are 10 stars here, but we clearly have only 9 red balls in our budget. There are two ways this particular illegal case could arise, and we can find them by permuting everything to the right of the first stick (there are $2 \choose 1$ permutations). Another illegal case: $$***********\space |\space |$$ Total number of permutations of this illegal case = $1 \choose 1$ We can repeat the above process for green balls, of which we can take only 7. (Everything to the left of the first stick is green balls now). The total number of permutations where there are more than 7 green balls is ${4 \choose 1} + {3 \choose 1} + {2 \choose1} + {1\choose 1} $ . Finally, we subtract the number of illegal permutations from the number of total permutations to get: $${13 \choose 2} - {2 \choose 1} - {1 \choose 1} - {4 \choose 1} - {3 \choose 1} - {2 \choose 1} - {1 \choose 1} = 65 \text{ legal cases}$$ My teacher uses the binomial expansion to solve most combinatorial problems, including this one (he finds the coefficient of a particular power of $t$ in a specific $p(t)$ ). I do not like his method as it is not at all intuitive. However, my own approach works only because the order of the sticks wasn't important (we deemed everything to the left of the first stick to be red balls in the first case and green balls in the other). It will not work when counting stations. To illustrate what I mean, consider the below: Take away 6 stations from 18, since there must at least be 2 stations between each of the 4 selected stations. Now, our problem simplifies to: let the four stations be represented by sticks and the stations between them be stars; find the number of permutations where there are between zero and four stars between any two sticks. For example, these are allowed: $$*|**|**|**|*$$ $$*****|*|**||$$ But this is not allowed: $$|******|**||$$ How do I modify my method to solve this problem? Other combinatorial approaches are also appreciated.","['permutations', 'combinatorics']"
3912921,Identity up to isomorphism treated as identity in proof,"In the following corollary to the inverse mapping theorem by Serge Lang, Fundamentals of Differential Geometry, 1999, p.17-18, there are two things in the proof which I don't understand, the first step and the last one: If there is an identity up to isomorphism between E and $ F_1 $ as established by $ f'(x_0) $ , why can we limit our consideration in the proof to the actual identity? This I have seen several times in proofs, but I don't understand why it can be done here and what the precise circumstances have to be in a proof to allow for this. I dont't see why the local inverse $ \big( \varphi'(0,0) \big)^{-1} $ , which is called g at the end of the proof, satisfies the two requirements defined in the corollary for the map g used there. Thanks for any help. Notes: $E, F_1, F_2 $ are Banach spaces. ""Morphism"" means a $ C^p $ -map with $ p \geq 1 $ . ""Local isomorphism"" means a local $ C^p $ -isomorphism (dt.: lokaler $ C^p $ -Diffeomorphismus). ""Toplinear isomorphism"" means an isomorphism between topological vector spaces. Maybe the following drawing is helpful:","['banach-spaces', 'inverse-function-theorem', 'differential-geometry']"
3912929,Can power set of any set equals to $\phi$,"In my topology quiz this question was asked and I couldn't do it right. So, I am posting it here. Let A be any set . Let P(A) be the power set of A, that is the set of all subsets of A, Then how is the statement"" P(A)= $\phi$ for some A"" a false statement? I think if A = $\phi$ then P(A) is also $\phi$ but in answer key it is marked as false ? Why?",['elementary-set-theory']
3912947,"How to find the variance of the estimator, $\frac{1}{\bar{X}}$, for $X_i \sim \operatorname{Expo}(\lambda)$?","So I used the method of moments to find an estimator for $\lambda$ : $\frac{1}{\bar{X}}$ . However, I'm having trouble deriving the variance of it: $$\operatorname{Var}\left(\frac{1}{\bar{X}}\right) = \operatorname{Var} \left(\frac{n}{\sum_{i=1}^n X_i}\right)$$ $$= n^2 \operatorname{Var}\left(\frac{1}{\sum_{i=1}^n X_i}\right)$$ I'm not sure how to proceed from here given the denominator is a sum of $X_i$ 's.","['statistical-inference', 'statistics', 'variance', 'parameter-estimation', 'exponential-distribution']"
3912996,"Evaluate $I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy$ over a given region","Evaluate the double integral $$I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy$$ over the region $R = \{(x,y)\mid 0\le x \le3, 3\le y \le5\}$ . Hi I was wondering if anyone could help me on this question which involves Taylor's Expansion. This is what I have done so far: From using the given boundaries I have created the integral: $$\int_3^5\int_0^3 \frac{x}{y^4}\, dx\,dy$$ Then I worked out the first integral: $$\int_0^3\frac{x}{y^4}\, dx$$ which gives me $\frac{9}{2y^4}$ and then I put this into the next integral that is with respect to $y$ : $$\int_3^5\frac{9}{2y^4} \,dy$$ Which gives me $3.415703704\times 10^{-3}$ , which is evidently wrong as the correct answer is: $49/1125$ Can someone please help me on where I am going wrong? Thanks in advance!","['integration', 'multivariable-calculus', 'definite-integrals', 'taylor-expansion']"
3913008,Spivak Chapter 1 Problem 17 Extended.,"The question in the textbook asks to find a subset of $[0,1]\times[0,1]$ which contains at most one point on every vertical and horizontal line, and whose boundary is $[0,1]\times[0,1]$ . The extended question I pose is can we find a subset which contains at most one point on every lines of the form $\frac{y-b}{x-a}=m$ (or $x=a$ ) where $m\in\mathbb{Q}$ and $a,b\in\mathbb{R}$ ?, and whose boundary is $[0,1]\times[0,1]$ ? I want to find a generalized property when satisfied gives a similar results, for instance  any set of curves which satisfies the condition that for any point $(a,b)\in\mathbb{R^2}$ the set of curves intersecting $(a,b)$ has an open dense complement in $\mathbb{R^2}$ .","['general-topology', 'real-analysis']"
3913038,Solve the given system of quadratic equations,$$x^2 + xy + xz = 2$$ $$y^2 + yz + xy = 3$$ $$z^2 + zx + yz = 4$$ I tried solving it but i just ended up with $$(x-y)(x+y+z) = -1$$ $$(y-z)(x+y+z) = -1$$ $$(x-z)(x+y+z) = -2$$ I'm not sure what to do. Any hints?,"['algebra-precalculus', 'systems-of-equations', 'quadratics']"
3913075,Limit of $\frac{1}{M} \int_1^M M^{\frac{1}{x}} \textrm{d} x$ when $M \to +\infty$,"The exercise Find the limit of $$
\frac{1}{M} \int_1^M M^{\frac{1}{x}} \textrm{d} x
$$ when $M \to +\infty$ . My try I tried a change-of-variable but as the lower bound of the integral is $1$ and not $0$ , it wasn't successful. I showed that the limit, if it exists, is $\geq 1$ . We also have: $\frac{1}{M} \int_1^M M^{\frac{1}{x}} \textrm{d} x = \int_1^M M^{\frac{1}{x}-1} \textrm{d} x$ and in the integral, $\frac{1}{x}-1 < 0$ . Any help is welcome.","['integration', 'limits', 'improper-integrals', 'real-analysis']"
3913122,How to find the Laurent expansion for $\frac{\exp\left(\frac{1}{z^{2}}\right)}{z-1}$ about $z=0$?,"I want to find the Laurent expansion for $\frac{\exp\left(\frac{1}{z^{2}}\right)}{z-1}$ about $z=0$ , I've tried to apply this formula $\frac{1}{1-\omega}=\sum_{n=0}^{\infty }\omega^{n}$ and the usual Taylor series of the exponential function, but I don't know how to continue: $$\begin{align}f(z)&=\frac{1}{z-1}\exp\left(\frac{1}{z^{2}}\right)\\
&=-\frac{1}{1-z}\exp\left(\frac{1}{z^{2}}\right)\\&=-\left (\sum_{n=0}^{\infty }z^{n}  \right )\left ( \sum_{n=0}^{\infty}\frac{1}{n!z^{2n}} \right )\end{align}$$ Thanks in advance. Ps: I tried applying a Cauchy product, but I think this is not appropriate. Edit 1: If it is useful at the end of the text, the authors say that the Laurent expansion is: $\sum_{k=-\infty }^{\infty }a_{k}z^{k}$ with $a_{k}=-e$ if $k\geq 0$ and $a_{k}=-e+1+\frac{1}{1!}+\frac{1}{2!}+...+\frac{1}{(j-1)!}$ if $k=-2$ or $k=-2j+1$ where $j=1,2,...$","['laurent-series', 'complex-analysis', 'taylor-expansion', 'sequences-and-series', 'complex-numbers']"
3913203,Probability Game Optimal Strategy,"You're playing a game where you have a .45 probability of winning and .55 probability of losing. You start out with 2000 chips and the winning condition is that if you bet a cumulative total of 10000 chips, then you win. Otherwise if you run out of chips, you lose. For example, if you bet 20 chips, no matter the outcome, that will contribute 20 to the total. And you'll be left with either 1980 or 2020 current chips. The min bet size is 1$ and max is as much as you currently have. What is the best strategy to maximize your probability of winning? What I said was, you'd want to use a strategy akin to betting as much as possible at each round. Since otherwise, from LLN, you are losing in general and just prolonging with small bets will make you lose more in the long run. I said you'd be able to maybe calculate the exact strategy using dynamic programming, with f(s,t) representing what you should bet at each combination of (current money, total cumulative so far). However I do not know if this is correct nor did I have time to fully solve it. Thank you!","['expected-value', 'game-theory', 'probability', 'gambling']"
3913225,Plane of symmetry and geodesics in surfaces,"Let $(M,n)$ be an oriented regular surface of $\mathbb{R}^{3}$ such that the intersection of $M$ with the plane $\Pi:=\{(x,y,z) \in \mathbb{R}^{3}:z=0\}$ is the image of a curve parametrized by arc length $\alpha$ . I am trying to prove that if $\varphi:M \rightarrow M$ defined by $\varphi (x,y,z)=(x,y,−z)$ is an isometry of M then the curve $\alpha$ must be a geodesic of M. I would like to prove that the normal vector $n \circ \alpha(t)$ is orthogonal to the vector $e_3 = (0,0,1)$ (for any $t \in \operatorname{dom}(\alpha)$ ).
I know that the vector $n \circ \alpha(t)$ is an eigenvector of the linear transformation $R: \mathbb{R}^{3} \rightarrow \mathbb{R}^{3}$ given by $R(x,y,z)=(x,y,−z)$ , but I don't know how to prove that the tangent space $T_{\alpha(t)}M$ can't be the vector space $\operatorname{span}\{e_1,e_2\}$ .
I also know that the hypothesis $M \cap \Pi$ equal to the image of a curve is important and necessary. I have tried many things but I'm not making any progress. So I would really appreciate any help.","['orientation', 'geodesic', 'surfaces', 'differential-geometry']"
3913230,Is it true that any continuous injective map from a circle to circle is surjective?,"Here is my idea: Suppose $f : \mathbb S^1 \to \mathbb S^1$ is continuous and injective. If $f$ is not surjective, its image is homeomorphic to a subset of $\mathbb S^1 \setminus \{ c\} \cong \mathbb R$ (by considering the projection). Then, the restriction of $f$ 's codomain $g : \mathbb S^1 \to f(\mathbb S^1)$ is injective and surjective, so it is bijective. Also, since $\mathbb S^1$ is compact and connected, its image under the continuous map is also compact and connected, so $f(\mathbb S^1)$ is homeomorphic to some closed interval $[a, b]$ , which is simply connected. Noting $f(\mathbb S^1)$ is Hausdorff and $\mathbb S^1$ is compact, by the closed map lemma, it follows $g$ is a homeomorphism. This contradicts that $\mathbb S^1$ is not simply connected, since $[a, b]$ is simply connected. Thus, $f$ must be surjective. Does it work?","['closed-map', 'general-topology', 'circles', 'algebraic-topology']"
3913248,Functional derivative of the square of a functional,"How should I compute the functional derivative of the following functional: $$
F(p(x)) = \left[\int\cos(x)p(x)dx - \int\cos(y)q(y)dy\right]^2
$$ I know that the functional derivative $\frac{\delta F}{\delta p(x)}$ is defined as $$
\left[\frac{d}{d\epsilon}F(p +\epsilon\phi)\right]_{\epsilon=0} = \int \frac{\delta F}{\delta p(x)}\phi(x)dx
$$ but I am not sure how the chain rule is defined here in the case of a function of a functional, i.e., what's the functional derivative of $F = g(H(p(x)))$ where $g$ is a differentiable function and $H$ is a functional.","['functional-analysis', 'functional-calculus', 'calculus-of-variations']"
3913340,Limit of a sequence within a logarithm,"Let $(x_n)_{n \in \mathbb{N}}$ be a sequence of real numbers. (1) Assume that $x_n>-1 \space \space \forall \space n \in \mathbb{N}$ and $x_n \to 0$ as $n \to \infty$ , show that $\log(x_n+1) \to 0$ as $n \to \infty$ . (2) Assume that $x_n>0 \space \space \forall \space n \in \mathbb{N}$ and $x_n \to x>0$ as $n \to \infty$ , show that $\log(x_n) \to \log(x)$ as $n \to \infty$ . I managed to do the first one but I'm not sure how to go about the second one. A similar method didn't seem to apply. In this question, $\log(x)=y$ where $y$ is the unique real number with $\exp(y)=x$ . Where $\exp(x)$ is the limit of $(1+x/n)^n$ as $n$ tends to infinty. The method I've used so far involves the inequality $1+x\leq e^x \leq 1/(1-x) \space \space \forall \space x <1$ .","['limits', 'sequences-and-series', 'real-analysis']"
