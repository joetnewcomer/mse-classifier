question_id,title,body,tags
1896467,Cholesky decomposition when deleting one row and one and column.,"I've thought about this problem for days but could not find a good answer. Given Cholesky decomposition of a symmetric positive semidefinite matrix $A = LL^T$. Now, suppose that we delete the $i$-th row and the $i$-th column of $A$ to obtain $A'$ ($A'$ is also a symmetric positive semidefinite matrix), and the Cholesky decomposition of this new matrix is $A' = L'(L')^T$. Is there any efficient way to obtain $L'$ from $L$? Note : In case we delete the last row and the last column of $A$, the problem becomes simple, we just delete the last row and last column of $L$ to obtain $L'$. Other cases, for me, are not trivial. Thank you so much.","['matrix-decomposition', 'lu-decomposition', 'linear-algebra']"
1896468,"Modified AGM: $a_{n+1}=\frac{a_n+b_n}{2}, \quad b_{n+1}=a_n+b_n-\sqrt{a_n b_n}$","The idea is as follows: generate a sequence from two numbers by subtracting their means from their sum, for example arithmetic and geometric means: $$a_{n+1}=a_n+b_n-\frac{a_n+b_n}{2}=\frac{a_n+b_n}{2}$$ $$b_{n+1}=a_n+b_n-\sqrt{a_n b_n}$$ $$\lim_{n \to \infty} a_n=\lim_{n \to \infty} b_n=L(a_0,b_0)$$ This one looks similar to arithmetic-geometric mean, but it's not the same. I tried computing it for several pairs of numbers, but didn't get any known constant from ISC or WA. $$L(1,2)=1.54319140418204860465015096064$$ $$L(1,\sqrt{2})=1.21607308161790985508121278229$$ etc. The convergence is great because: $$b_{n+1}-a_{n+1}=\frac{a_n+b_n}{2}-\sqrt{a_n b_n}$$ Have you heard about this kind of iterated means in general and the titular one in particular? Is there any closed form known? A little clarification. This is actually equivalent to adding the 'errors' to initial values, i.e. at each step we take the signed difference between one of the numbers and one of the means and add it to another number. Edit If we denote: $$x_n=\frac{a_n}{b_n}$$ Then we obtain another definition of this recurrence: $$x_{n+1}=\frac{1}{2} \frac{1+x_n}{1+x_n-\sqrt{x_n}}$$ $$a_{n+1}=a_0 \prod_{k=0}^n \frac{1}{2} \left(1+\frac{1}{x_k} \right) $$ This way the problem reduces to finding the explicit form for $x_n$ . This is analogous to the Legendre form of the arithmetic-geometric mean. I wouldn't hold out much hope for an explicit form. Getting rid of the square root, we obtain: $$x_nx_{n+1}^2=(x_n+1)^2(x_{n+1}-1/2)^2$$ Also $x_n<1$ for all $n>0$ . Which might point to a trigonometric substitution. Or, alternatively: $$y_n=\frac{b_n}{a_n}$$ $$y_{n+1}=2- \frac{2\sqrt{y_n}}{1+y_n}=2-\cfrac{2\sqrt{y_n}}{3-\cfrac{2\sqrt{y_{n-1}}}{3-\cfrac{2\sqrt{y_{n-2}}}{3-\cfrac{2\sqrt{y_{n-3}}}{1+y_{n-3}}}}}$$ $$a_{n+1}=a_0 \prod_{k=0}^n \frac{1}{2} \left(1+y_k \right) $$","['means', 'recursion', 'sequences-and-series']"
1896503,a set theoretical completion?,"Is there such a thing as starting with a weak kind of set theory and making an argument to show that it should be expanded into a stronger set theory such as ZFC? Something similar to going from the rationals to the reals? With number systems, we have evidence which forces us into a stronger system (sqrt of 2). Is there such evidence with set theory? I mentioned the real numbers because in that situation there is obviously something missing. Is there obviously something missing from ""weak"" set theories? My interest is: to what extent can we justify the axioms of infinity and choice? Is there a higher level principle (of the ""there is something missing"" sort) which implies infinity and choice should be part of our system?","['foundations', 'elementary-set-theory']"
1896521,"What shape does a ""cycloid egg"" trace as it rolls on flat surface?","A point on a circle rolling on a flat surface traces a cycloid. What curve does point on a rolling ""cycloid egg"" trace? I tried to draw what it would trace, but failed. Also, I think the more interesting question here is what shape with a point on its edge traces a half circle as it roll on a flat surface?","['circles', 'geometry']"
1896525,Does there exist a real valued function $f$ such that $\lim_{x\rightarrow q} f(x) = \infty$ for all $q\in\mathbb{Q}$,"Does there exist a real valued function $f$ such that $\lim_{x\rightarrow q} f(x)
 = \infty$ for all $q\in\mathbb{Q}$? My answer is no. If $f$ exists, then let us define
$$g(x) = \left\{ 
  \begin{array}{l l}
     \frac{1}{1+|f(x)|} & \quad \text{if } x\in \mathbb{Q}^c\\
     0 & \quad \text{if } x\in \mathbb{Q}\\
   \end{array} \right. \\$$
and we see $g$ is continuous only on rational numbers, which is a contradicts the fact that the set of continuity points is a $G_\delta$ set. Is my solution correct? And do you guys know if there is any good direct argument?","['functional-analysis', 'real-analysis']"
1896529,show that an orthogonal matrix $Q$ that maximizes $f(Q)=tr(QB)$ satisfies $ Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1}$,"I can't solve the following problem. In this problem, matrices are supposed to be over the real numers.
Let $A$ be a symmetric positive definite matrix of order $n$ and $B$ be a nonsingular matrix of order $n$. there exists a unique symmetric positive definite matrix $R$ such that $R^2=A$. We denote such R by $\sqrt{A}$. Show that an orthogonal matrix $Q$ that maximizes $f(Q)=tr(QB)$ satisfies
$$ Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1}.$$","['matrices', 'linear-algebra']"
1896530,Linear Algebra used to solve hotel room light switch question,"There is a special suite of rooms designed with light switches in an odd way.  Our goal is to turn off all of the the lights. There are 25 rooms in the suite arranged in a square of five by five rooms: 1  2  3  4  5 

6  7  8  9  10

21 22 13 14 15

16 17 18 19 20

21 22 23 24 25 If you switch the light switch in any room, it toggles the lights in the adjacent room.  For example, -assuming all of the lights are on and you switch the switch in room 1, the lights in room 2 and 6 are turned off and there is no other change in the status of the lights. -or assuming all of the light are on and you switch the switch in room 18, the lights in room 13, 17, 19, 23 are all turned off and there is no other change in the status of the lights. Assuming all of the lights are on, please advise on how to turn off all of the lights.  You can use a web app for row reduction. Assuming all of the lights in even number rooms are on, please advise on how to turn off all of the lights. After row reduction, we get a free column where, $$x_4 =  t$$ $$x_1 = -2 + t$$ $$x_2 =  2 - t$$ $$x_3 =  1 - t$$ Can anyone help out with the rest?","['matrices', 'linear-algebra']"
1896535,What is the difference between gradient of divergence and Laplacian?,$\nabla^2\mathbf{V}$ versus $\nabla (\nabla \cdot \mathbf{V})$. They seem pretty much the same to me. Both give back a vector.,"['derivatives', 'calculus']"
1896550,Computing eigenfunction of expectational operator,"Let $A$ be an operator from $\textbf{L}^2$ to itself defined by $$Af(x) = \mathbb{E}[e^{-W \gamma}\cdot f(x\cdot e^W)]$$ where $W$ is a $\mathbb{N}(0,1)$ distributed random variable and $\gamma \in [3,10]$ is a parameter. 
I believe there is no analytical method to compute the principal eigenfunction and eigenvalue of this operator. By principal eigenfunction, I mean the positive function $f(x)$ such that $$Af(x) = \kappa f(x)$$ $\forall x \in \mathbb{R}$. What kind of numerical methods are used for problems of this sort?
Also, if anyone can recommend some textbook covering this kind of problems that would be amazing. Thank you! Edit: the source for this kind of problem is an Economics publication on long run risk exposure (Hansen and Scheinkman (2009)) Edit2: Uranix gave great tips to approach this specific problem but also made me realize that there is not much hope of solving this analytically in general (when the factor multiplying $f(x\cdot e^W)$ is a general function of $W$). Does anyone know which numerical techniques can be used in this context?","['functional-analysis', 'numerical-methods', 'operator-theory', 'eigenfunctions']"
1896554,Linear Algebra Versus Functional Analysis,"As it is mentioned in the answer by Sheldon Axler in this post , we usually  restrict linear algebra to finite dimensional linear spaces and study the infinite dimensional ones in functional analysis. I am wondering that what are those parts of the theory in linear algebra that restrict it to finite dimensions. To clarify myself, here is the main question Question . What are the main theorems in linear algebra that are just valid for finite dimensional linear spaces and are propagated in the sequel of theory, used to prove the following theorems? Please note that I want to know the main theorems that are just valid for finite dimensions not all of them. By main , I mean the minimum number of theorems of this kind such that all other such theorems can be concluded from these.","['functional-analysis', 'linear-algebra']"
1896576,Basic definition of orientation/preservation question,"I'm reading through do Carmo's book on Riemannian Geometry and reviewing a bit about orientation. He uses a definition of orientation preserving which I find difficult to verify in practice, but perhaps I'm missing something. He says a manifold is orientable if every change of coordinates function has a positive (determinant) Jacobian, and a choice of such a differentiable structure is a choice of orientation. 1) First, I wanted to verify that if $\varphi: M_1 \to M_2$ is a diffeomorphism, then $M_2$ gets an orientation from $M_1$. My argument was: if $y_\beta^{-1} \circ y_\alpha$ is a change of coordinate map on $M_2$, then $y_\beta^{-1} \circ y_\alpha = (x_\beta \varphi^{-1})^{-1} \circ (x_\alpha \varphi^{-1}) = \varphi (x_\beta^{-1} \circ x_\alpha) \varphi^{-1}$ for suitable coordinate maps $x_\alpha, x_\beta$ on $M_1$. Hence the determinant of the Jacobian for coordinate maps on $M_2$ are positive because the ones on $M_1$ are. Does this work? 2) Now how do I actually compare orientations under a map? For example, I want to do the classic exercise of showing the antipodal map $A: S^n \to S^n$ is orientation preserving iff $n$ is odd. I see that the determinant of the Jacobian of $A$ is $(-1)^{n+1}$ (I think Jacobians behave well under restriction?), but I don't see how to use this to compare the induced orientation on $A(S^n)$ using the above formulation. I would prefer to try to solve this using this level of information, and not bringing anything fancier like differential forms into the picture yet. Thanks.","['differential-geometry', 'differential-topology']"
1896598,Separable profinite group which is not metrisable?,"Is there a topological group which is profinite (i.e. compact and totally disconnected) and separable, but is not metrisable (equivalently is not first-countable)? I know there are such topological space: see here . But among those listed there only the Stone-Cech compactification of $\mathbb{Z}$ has a topology which in theory can be that of a topological group (one of the other two is not Hausdorff and the second is first-countable but not metrisable, so neither can be a topological group). I don't know of any structure of a topological group on the Stone-Cech compactification of $\mathbb{Z}$ , though. If you replace ""totally disconnected"" with ""connected"" I have an example: $(\mathbb{R}/\mathbb{Z})^{2^{\aleph_0}}$ . I was just wondering if the totally disconnected case had something similar. If $D$ is a finite group, is $D^{2^{\aleph_0}}$ separable? In fact, I am even more interested in groups which are not just separable but in fact topologically finitely generated (i.e. admit a finitely generated dense subgroup). I will appreciate all thoughts and comments on this.","['general-topology', 'topological-groups']"
1896614,The 57-cell in 5D or 4D,"The Hemi-dodecahedron can be nicely represented in five dimensions.  Here are the six faces, where all edges have length 2. {{{0,0,0,1,1},{1,1,0,0,0},{0,0,1,1,0},{1,0,0,0,1},{0,1,1,0,0}}, {{1,1,0,0,0},{0,0,1,1,0},{0,1,0,0,1},{1,0,0,1,0},{0,0,1,0,1}}, {{1,0,1,0,0},{0,1,0,1,0},{1,0,0,0,1},{0,0,1,1,0},{0,1,0,0,1}}, {{0,1,1,0,0},{1,0,0,1,0},{0,0,1,0,1},{0,1,0,1,0},{1,0,0,0,1}}, {{0,0,0,1,1},{0,1,1,0,0},{1,0,0,1,0},{0,1,0,0,1},{1,0,1,0,0}}, {{0,0,0,1,1},{1,1,0,0,0},{0,0,1,0,1},{0,1,0,1,0},{1,0,1,0,0}}} Is there a nice set of coordinates in 4D or 5D (or higher) for the 57-cell ?  Could the above hemi-dodecahedron be used as one of the cells? EDIT:  In 4D, these coordinates can be used for the six faces of the hemi-dodecahedron / Petersen graph. All edges have length 1, and the angle between edges is always 3/4. But I'm still unclear on how to glue these cells together. $$(  
((\sqrt{5},1,1,1),(0,0,-2,0),(\sqrt{5},-1,1,-1),(0,0,0,2),(0,0,0,-2)),  
((\sqrt{5},1,1,1),(0,-2,0,0),(\sqrt{5},1,-1,-1),(0,0,2,0),(0,0,-2,0)),    
((\sqrt{5},1,-1,-1),(0,0,2,0),(\sqrt{5},-1,-1,1),(0,0,0,-2),(0,0,0,2)),    
((\sqrt{5},-1,1,-1),(0,0,0,2),(\sqrt{5},1,-1,-1),(0,-2,0,0),(0,2,0,0)),    
((\sqrt{5},-1,-1,1),(0,0,0,-2),(\sqrt{5},1,1,1),(0,-2,0,0),(0,2,0,0)),    
((\sqrt{5},-1,-1,1),(0,2,0,0),(\sqrt{5},-1,1,-1),(0,0,-2,0),(0,0,2,0)))/4$$","['polyhedra', 'graph-theory', 'polytopes', 'geometry']"
1896615,Showing a set with a supremum norm is a Banach space,"Problem Statement: Let $X$ be a set and $E$ , a Banach space with norm $||\cdot||_{E}$ . Let $B_{E}(X)$ denote the set of all bounded functions $f:X\rightarrow E$ with the supremum norm $||f||=\sup_{x\in X}||f(x)||_{E}$ .
Show that $B_{E}(X)$ is a Banach space (i.e. show that it is complete in this norm). I am just now starting in my first course in Real Analysis, and we have just been given our first homework, and I am already quite lost in how to approach these types of proofs. (I have taken an Abstract Algebra sequence in which I became very comfortable with the material and styles of proofs, but Analysis already seems to be much more daunting.) I have these definitions to help me: A Banach space is a complete normed vector space. A complete normed vector space is a normed vector space in which Cauchy sequences converge. A sequence $(v_{n})_{n\geq 1}$ in a normed vector space $E$ is a Cauchy sequence if $\forall \epsilon > 0$ , $\exists N\in \mathbb{N}$ $(m,n\geq N)$ such that $||v_{n}-v_{m}||<\epsilon$ . I am assuming that the norm $||\cdot||_{E}$ in the problem is referring to the standard Euclidean norm. So basically I need to show that an arbitrary Cauchy sequence of functions in $B_{E}(X)$ converges? My professor did a proof in class that $B_{\mathbb{R}}(X)$ , the space of bounded functions from a set $X$ to $\mathbb{R}$ , with the supremum norm, is complete. He began letting an arbitrary sequence $(f_{n})$ be Cauchy, and then showed that the $\lim f_{n} = f_{0}$ for some (I assume finite) $f_{0}$ . But many of the steps were confusing to me, especially considering it was my first day in Analysis. Any suggestions on how to approach this problem would be greatly appreciated!","['real-analysis', 'banach-spaces', 'normed-spaces', 'proof-explanation', 'analysis']"
1896621,Is every real monic polynomial the characteristic polynomial of a real matrix?,"Is every monic real polynomial the characteristic polynomial of some real square matrix? I strongly suspect so, but I couldn't find a proof.","['matrices', 'polynomials']"
1896628,Properties of the A-transpose-A matrix,"I believe that $A^TA$ is a key matrix structure because of its connection to variance-covariance matrices. In Professor Strang's linear algebra lectures, "" A-transpose-A "" - with this nomenclature, as opposed to $X'X$ , for example - is the revolving axis. Yet, it is not easy to find on a quick Google search a list of its properties. I presume that part of the reason may be that they are shared by variance-covariance matrices. But I'd like to confirm this (does it have identical properties to a var-cov matrix?), and have the list easily available from now on here at SE-Mathematics. Just to not shy away from the initial effort, here is what I think I have so far: Symmetry Positive semidefinite- ness Real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive Orthogonal eigenvectors (**) Diagonalizable as $Q\Lambda Q^T$ It is possible to obtain a Cholesky decomposition . Rank of $A^TA$ is the same as rank of $A$ . $\text{ker}(A^TA)=\text{ker}(A)$ (**) The eigenvectors of A-transpose-A form the matrix $V$ in singular value decomposition (SVD) of $A,$ while the square root of the eigenvalues of A-transpose-A are the singular values of the SVD. Similarly, the eigenvectors of A-A-transpose $AA^\top$ include the columns in the matrix $U$ of the SVD of $A.$ The importance of this is exemplified in the fact that SVD can be used to solve least squares regression by computing the Penrose-Moore pseudo-inverse $A^\dagger = V\Sigma^\dagger U^*,$ although the QR decomposition is a more expedient computational method. There is a nice post on the topic here .",['linear-algebra']
1896637,Open sets which are not closed in the Sorgenfrey line,"Basically, it is a simple fact about the Sorgenfrey line that: the only connected sets are the singelton sets. the open set in Sorgenfrey line $(b,\infty)$ is not closed. But are there other open sets which not closed? The argument for 1 and 2 are not difficult. Do you think I am right? Any help will be appreciated.","['general-topology', 'sorgenfrey-line']"
1896663,Disintegration of Haar measures,"Suppose I have a locally compact group $G$ and a quotient map $f:G\to G/N$. Is it true that for every Borel-measurable function $f : G → [0, +∞]$, $$\int_{G} f(g) \, \mathrm{d} \mu_G (g) = \int_{G/N} \int_{N}    f(gn) \, \mathrm{d} \mu_{N} (n) \mathrm{d} \mu_{G/N} (gN),$$
where $\mathrm{d} \mu_{G}$, $\mathrm{d} \mu_{N}$ and $\mathrm{d} \mu_{G/N}$ are the Haar measures on $G$, $N$ and $G/N$ respectively? In particular, is it true that if a subset $A\subseteq G$ is such that its intersection with $\mathrm{d}\mu_{G/N}$-almost every coset $gN$ is of full measure in $gN$ (w.r.t. to the measure on $gN$ obtained by shifting the measure of $N$, which is independent of the choice of $g$), then $A$ is of full measure in $G$? (This is implied from the equation above by taking $f$ to be the characteristic function of $A$). I wanted to use the disintegration theorem in Wikipedia, but I'm not sure if it applies here. I'm not sure I understand the definition of a Radon space and I don't know which locally compact groups satisfy it. I know of a more specific disintegration result, which appears in many/most introductions to Haar measures (e.g. Raghunthan's book), but it is only stated for continuous $f$ with compact support. I suppose it's not hard to get rid of the continuous part by using some Luzin argument (although I am not sure how to do it myself), but the compact support bothers me. In any case, if this is not true for general locally compact groups, for which groups it is true? I have a reference for second-countable compact groups (Halmos's book Measure Theory ; his definitions are a bit out-dated, but coincide with the modern ones for second-countable compact groups). I don't mind assuming separability. Compactness is not awful either, but I prefer not to assume metrisability. Thanks.","['harmonic-analysis', 'locally-compact-groups', 'topological-groups', 'measure-theory', 'haar-measure']"
1896690,Conditional Expectation and Entropy,"Let $\mathscr{F,G}$ be $\sigma$-algebras with $\mathscr{F} \subset \mathscr{G}$. Then given a random variable $X$, consider the conditional expectations of $X$ with respect to these two $\sigma-$algebras, i.e. $\mathbb{E}[X|\mathscr{F}]$ and $\mathbb{E}[X|\mathscr{G}]$. Question: Given that $\mathscr{F} \subset \mathscr{G}$, is there a straightforward relationship between $$H(\mathbb{E}[X|\mathscr{F}]) \quad \text{and} \quad H(\mathbb{E}[X|\mathscr{G}]) $$ where $H(Y)$ denotes the entropy of the random variable $Y$? My suspicion is that one should have $H(\mathbb{E}[X|\mathscr{F}]) \le H(\mathbb{E}[X|\mathscr{G}])$, since a larger $\sigma-$algebra is supposed to allow us to characterize events with more precision and thus acquiring the value of $\mathbb{E}[X|\mathscr{G}]$ should give us more information about $X$ than acquiring the value of $\mathbb{E}[X|\mathscr{F}]$ would. I think Proposition 5.8 and 5.9 of this document says this , although I am not sure, since I still have trouble understanding conditional probability distributions. Can anyone tell me what it says? This is a follow-up question to Information in Filtrations","['information-theory', 'probability-theory', 'conditional-expectation']"
1896694,showing $-\eta(s) = \lim_{z \ \to \ -1} \sum_{n=1}^\infty z^{n} n^{-s}$,"For $Re(s) > 0$,  the Dirichlet eta function has the series representation $\eta(s) = \sum_{n=1}^\infty (-1)^{n+1} n^{-s}$. It extends analytically to the whole complex plane, and we have the following representation :
$$\boxed{\forall s \in \mathbb{C}, \qquad -\eta(s) = \lim_{z \ \to \ -1, |z| < 1} \sum_{n=1}^\infty z^{n} n^{-s}}$$ Question : do you know other proofs, preferably more general ? Let $\displaystyle f(s,z ) = \sum_{n=1}^\infty z^{n} n^{-s} $ (the Polylogarithm ). Note it is analytic in $s$ and $z$ for $|z| < 1$. On $|z| = 1$, define $f(s,z) = \lim_{r \to 1^-}f(s,rz)$, we will prove it exists for $z \ne 1$ and that it stays analytic in $s$. Summing by parts, for $Re(s) > 1, |z| \le 1$ :
$$f(s,z) =  \sum_{n=1}^\infty \frac{z^{n+1}-z}{z-1} (n^{-s}-(n+1)^{-s})= \frac{-z}{z-1} + \frac{z}{z-1}\sum_{n=1}^\infty z^n (n^{-s}-(n+1)^{-s})$$
using $n^{-s}-(n+1)^{-s} = s \int_n^{n+1} x^{-s-1}dx \sim s n^{-s-1}$ as $n \to \infty$, we get that $z\sum_{n=1}^\infty z^n (n^{-s}-(n+1)^{-s})$ is a representation of $(z-1)(f(s,z)+z)$ valid for $Re(s) > 0, |z| \le 1$. Then apply the same argument inductively : letting $a_n^0(s) = n^{-s}$, $a_n^{k+1}(s) = a_n^k(s)-a_{n+1}^k(s)$, $\ a_n^k(s) \sim s^k n^{-s-k}$ as $n \to \infty$.
Define $$h_0(s,z) = f(s,z),\qquad h_{k+1}(s,z) = (z-1)(h_k(s,z)+z \, a_1^k(s))$$
so that $z^{k+1}\sum_{n=1}^\infty z^{n} (a_n^k(s)- a_{n+1}^k(s))$ is a representation of $h_{k+1}(s,z)$ valid for $Re(s) > -k, |z| \le 1$. Note that for all $m \ge 0$ :
$$\frac{d^m}{d s^m} a_n^k(s) \sim \frac{d^m}{d s^m} s^k n^{-s-k} \qquad \text{as }n \to \infty$$
so that $ z^k\sum_{n=1}^\infty z^{n}\frac{d^m}{d s^m} a_n^k(s)$ is also a valid representation of $\frac{d^m}{d s^m} h_{k}(s,z)$, proving the analyticity for $Re(s) > 1-k, |z| \le 1$. Hence, defining $\tilde{h}_0(s) = -\eta(s),\ \tilde{h}_{k+1}(s) = 2 (\tilde{h}_{k}(s)- a_1^k(s))$ we get, for $Re(s) > 1-k$ :
$$h_{k}(s,-1) = \tilde{h}_k(s)$$ And since $h_k(s,z)$ is clearly continuous in $z$ for $|z| \le 1, Re(s) > 1-k$ : $\tilde{h}_k(s) = \lim_{z \,\to\,  -1}h_{k}(s,z)$. Being true for every $k$, it means that 
$$\forall s \in \mathbb{C}, \qquad\lim_{z \,\to\, -1} f(s,z) = -\eta(s)$$ Also, it should prove $f(s,e^{2i \pi x})$ is entire in $s$ whenever $x \in \mathbb{R}\setminus\mathbb{Z}$. Any ideas or references are welcome : extending $f(s,z)$ analytically beyond $|z| \le 1$, a functional equation for $f(s,z)$..","['complex-analysis', 'riemann-zeta', 'reference-request']"
1896783,Real Analysis advanced book suggestion,"I want to self study real analysis. So far, finished the first seven chapters of Baby Rudin (up to and including sequences and series of functions) and now want to proceed into more advanced books. I have couple options, including Stein&Shakarchi Folland Royden Rudin's Real&Complex Analysis Kolmogorov-Fomin Among these five (also happy to hear if you have further recommendations) which are more accessible and has better treatment of the material? I'm especially thinking among first three, so if there would be a comparative answer for the first three books, I would be really happy. Any help is appreciated. Thank you!","['reference-request', 'real-analysis', 'book-recommendation']"
1896801,Ellipse and hyperbola have the same foci,"I am trying to solve the following problem: An ellipse and a hyperbola have the same foci, $A$ and $B$, and intersect at four points. The ellipse has major axis $50$, and minor axis $40$. The hyperbola has conjugate axis of length $20$. Let $P$ be a point on both the hyperbola and ellipse. What is $PA \times PB$? So I say the center of the ellipse is at $(0,0)$ and the equation of the ellipse is $$\frac{x^2}{25^2}+\frac{y^2}{20^2}=1$$ I calculate that the foci of the ellipse are located at $(15,0)$ and $(-15,0)$. The general equation of a hyperbola is: $$\frac{x^2}{a^2}-\frac{y^2}{b^2}=\pm 1 \quad \cdots \cdots (*)$$ since the length of the conjugate axis is $20$, we can say $$2a = 20 \implies a = 10$$ Since $a$ is $10$  we search for $b$ with the condition that the hyperbola formed from $(*)$ will have foci at $(15,0)$ and $(-15,0)$. I get $$b = 5 \sqrt{5}$$ Now plugging into $(*)$ the values I have for $a$ and $b$ and get, $$\frac{y^2}{125}-\frac{x^2}{100}=-1$$ for the equation of the hyperbola. Now we need one intersection point and for that I used Mathematica and get $$P = \left ( \frac{50}{3}, \frac{20 \sqrt{5}}{3} \right )$$ The whole line of reasoning leads to the following diagram: With $A$ and $B$ the foci and $P$ one of the points of intersection. I used the distance formula to get the length of $PA$ and $PB$ and got $15$ and $35$ as seen in the diagram. $$15 \times 35 = 525$$ Of course, this is not the answer given, which is $500$. Where did I go wrong? Thanks to all for their nice solutions.",['geometry']
1896802,When the tensor product of two module elements is nonzero,"What are the main cases in which we can say that $a \otimes b \neq 0 \in A \otimes B$, where $A$ and $B$ are $R$ modules? It works for nonzero elements in free modules over an integral domain. Additional Question: What can we say about when all tensors are elementary? It is true when one of the factors is cyclic...is that the only reliable principle?","['abstract-algebra', 'soft-question', 'tensor-products']"
1896815,Notation for the set of all tuples whose elements are all part of a particular set,"So, for example, let's say we have A = {3,6,1}
I would like to generate a set B which is such that the elements of B are tuples. Each tuple is such that if x is an element of the tuple then x is an element of A. Additionally, there are no repeated elements in any given tuple (each element of a given tuple is distinct).
If A contains x number of elements, then B should contain all the tuples (meeting the distinctness restriction) with 1 element, all the tuples with 2 elements, ...., all the tuples with x elements. So in our example, B would be:
B = { (3) , (6) , (1) , (3,6) , (3,1) , (6,3) , (6,1) , (1,6), (1,3) , (3,6,1) , (3,1,6) , (6,3,1) , (6,1,3) , (1,3,6) , (1,6,3) } so basically if A has x amount of elements, B is like the set of all permutations of A, taken x elements at a time, taken x-1 elements at a time, taken x-2 elements at a time,..., taken 1 element at a time. ....but instead of basic sets we have tuples. Now, furthermore, I would like to have another set, C, which is such that x is an element of C iff x is an element of B, and x is such that its first element is less than its second (if there is a second), its second element is less than its third (if there is a third), etc... (with ""less than"" being the standard less than relation among integers) So we would get:
C = { (3) , (6) , (1) , (3,6) , (1,6) , (1,3) , (1,3,6) } Thanks","['notation', 'logic', 'elementary-set-theory']"
1896824,How to find the solutions to $\frac{11}{2} x - \cos x = 0$?,Find the values of $x$ such that $\frac{11}{2} x - \cos x = 0$. I really don't know how to find the solutions of this equation.  I would appreciate if you could help me.,['trigonometry']
1896869,Is the limit of a convergent sequence always a limit point of the sequence or the range of the sequence?,"In this video lecture ( Real analysis, HMC,2010, by Prof. Su ) Professor Francis Su says (around 54:30) that ""If a sequence $\{p_n\}$ converges to a point $p$ it does not necessarily mean $p$ is a limit point of the range of $\{p_n\}$."" I'm not sure how that can hold (except in case of a constant sequence). I'm not able to understand the difference between the set $\{p_n\}$ and the range of $\{p_n\}$ (which I understand is the set of all values attained by $p_n$). As per my understanding they're the same (except $\{p_n\}$ might contain some repeated values which the range of $p_n$ won't, e.g. the range of the sequence {1/2,1/2,1/2,1/2,1/3,1/3,1/3,1/3,1/4,1/4,...} would be {1/2,1/3,1/4,...}, or that of the constant sequence {1,1,1,...} would be {1}. Based on this understanding, my reasoning is as follows: By the definition of a convergent sequence $\{p_n\}$ converging to $p$, for every $\epsilon \gt 0$ we can find an infinite number of terms of $\{p_n\}$ which lie at a distance less than $\epsilon $ from p, i.e. within an $\epsilon $-neighborhood of $p$. Hence every $\epsilon $-neighborhood of p contains an infinite number of points of the set $\{p_n\}$ other than itself ($p$). Hence $p$ is a limit point of $\{p_n\}$. Now this would not hold only in case of a constant sequence, which converges, but any neighborhood of of its limit cannot contain any points in common with the sequence other than itself . Hence the limit won't be a limit point of the sequence. Other than this special case, I cannot think of any situation where the limit of a convergent sequence is not also a limit point of the sequence. Can anyone help? Thanks in advance.","['real-analysis', 'sequences-and-series']"
1896883,"If $(a + b\sqrt c)^n = d + e\sqrt c$, then $(a - b\sqrt c)^n = d - e\sqrt c$","I think that: If $a,b,(c\ge0$ not a prefect square$),d,e,f\in\mathbb Z$ such that for some $n \ge 1$, $(a + b\sqrt c)^n = d + e\sqrt c$, then $(a - b\sqrt c)^n = d - e\sqrt c$ Is this true? Can someone provide a proof or give a hint for how to prove this? (preferably without induction if it doesn't provide insight)","['algebra-precalculus', 'radicals']"
1896890,"Proof of $\sup AB=\max\{\sup A\sup B,\sup A\inf B,\inf A\sup B,\inf A\inf B\}$","I want to prove the following result: Theorem $\quad $ If $A$ and $B$ are bounded sets of real numbers, then
    \begin{gather*}\tag{$\star$}
		\sup(A\cdot B)=\max\{\sup A\cdot\sup B, \sup A\cdot\inf B, \inf A\cdot\sup B, \inf A\cdot\inf B\}.
	\end{gather*} Although in the thread Show that  sup(A⋅B)=max{supA⋅supB,supA⋅infB,infA⋅supB,infA⋅infB} , some suggestions are given, but I have trouble in understanding them. So
I have tried to prove as follows. Please check whether it is right or wrong. Note that $AB$ is defined by $$AB=\{z\in\mathbb{R}\mid \exists x\in A, y\in B: z=xy\}.$$ In order to prove this theorem, we need the following lemma. Lemma $\quad $ Let $A$ and $B$ be nonempty sets of nonnegative real numbers. Suppose that $A$ and $B$  are bounded  above. Then
    \begin{gather*}
		\sup AB=\sup A\cdot\sup B.
	\end{gather*} Proof. Let $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$ and $A,B$ nonempty, bounded above.  Put $a=\sup A, b=\sup B, c=\sup AB.$ Since $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$   we see that $0$ is a lower bound of both $A$ and $B,$ so $a, b, c$ are nonnegative.  Let $z\in AB.$ Then there are $x\in A, y\in B$ such that $z=xy.$  Since $0\leq x\leq a$ and $0\leq y\leq b,$ we have $xy\leq ab.$  So $z\leq ab.$ By arbitrariness of $z,$ we deduce that $ab$ is an upper bound  of $AB.$  Hence $c\leq ab.$   Note that $c\geq0.$  If $c<ab,$ then we have $a>0$ and $b>0.$ It follows that $\frac{c}{b}<a=\sup A.$  Hence there exists $x_1\in A$ such that $\frac{c}{b}<x_1.$  So $x_1>0.$ Then we deduce that $\frac{c}{x_1}<b=\sup B.$ So there exists $y_1\in B$ such that $\frac{c}{x_1}<y_1.$ From $x_1>0$ it follows that $c<x_1y_1.$  But $x_1y_1\in AB,$ so we have $\sup AB=c<x_1y_1\in AB,$ which is absurd.  Hence we have $c\geq ab.$ And thus we conclude that $\sup AB=c=ab=\sup A\cdot \sup B.$  $\qquad\Box$ Proof of the theorem. let $A$ and $B$ be bounded sets of real numbers. Put 
 \begin{gather*}
 	a=\sup A, \quad a'=\inf A,\quad b=\sup B,\quad b'=\inf B, \quad c=\sup AB.
 \end{gather*} 
 We shall prove that $c=\max\{ab,a'b,ab',a'b'\}.$   And we plan to prove this statement by cases. (i)   Case $a'\geq 0, b'\geq 0.$   Thus, for $x\in A$ and $y\in B,$ we have $a\geq x\geq a'\geq 0, b\geq y\geq b'\geq 0,$ so $A\subset [0,+\infty), B\subset [0,+\infty),$ and  $a'b\leq ab,  ab'\leq ab, a' b'\leq ab,$ which implies  that $\max\{ab,a'b,ab',a'b'\}=ab.$  By the above Lemma , we have $\sup AB=\sup A\cdot \sup B=ab.$ Hence in this case the desired equality $(\star)$ holds. (ii) $a'\geq 0, b'<0.$   In this case we have for all $x\in A,$  $x\geq a'\geq 0,$ and so $x\geq 0.$ We consider two sub-cases. (ii.1) $b>0.$  In this case $0\leq a'\leq a,  b'<0<b,$ so $a'b\leq ab, ab'\leq a'b'\leq 0\leq ab,$ which gives that 
 $\max\{ab,a'b,ab',a'b'\}=ab.$ 
 Let $z\in AB,$ then there exists $x\in A, y\in B$ such that $z=xy.$ Since $x\geq 0,$  $xy\leq xb\leq ab.$ And so $c\leq ab.$  If $a=0,$ then we deduce that $a'=0,$ and so $A=\{0\}=AB,$ which shows the desired result is obvious.  If $a>0,$ then, for every $0<\epsilon<\min\{a,b\},$ we have $a-\epsilon>0, b-\epsilon>0.$ Hence there exist $x_1\in A, y_1\in B$ such that $x_1>a-\epsilon >0, y_1>b-\epsilon>0,$ which implies that $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-\epsilon(a+b-\epsilon).$ Since $\lim_{\epsilon\to 0+}\epsilon(a+b-\epsilon)=0,$ we have, by the order preserving property of limit, $x_1y_1\geq ab,$ and so $c\geq ab.$   It follows  that $c=ab.$  Hence we have proved, in this case, that $\sup AB=c=ab=\max\{ab,a'b,ab',a'b'\}.$ (ii.2) $b\leq 0.$ Then $a'b'\leq a'b$ and $ab'\leq ab\leq a'b.$  So $\max\{ab,a'b,ab',a'b'\}=a'b.$  Let $z\in AB,$ then  there are $x\in A, y\in B$ such that $z=xy\leq xb\leq a'b,$ which implies that $c\leq a'b.$  On the other hand, for every $\epsilon >0,$   there exists $x_2\in A, y_2\in B,$ such that $0\leq x_2<a'+\epsilon, 0\geq b\geq y_2>b-\epsilon,$ it follows that $x_2y_2\geq (a'+\epsilon)y_2>(a'+\epsilon)(b-\epsilon)=a'b+\epsilon(b-a'-\epsilon),$ which implies that $x_2y_2\geq a'b.$  Thus $c\geq a'b.$  As a result, $c=a'b=\max\{ab,a'b,ab',a'b'\}.$ (iii) $a'<0, b'\geq 0.$  Swapping $A$ and $B,$ and   using Case (ii), the desired result follows. (iv) $a'<0, b'<0.$ There are four sub-cases. (iv.1)  $a> 0, b>0.$  That is, $a'<0<a, b'<0<b.$ Because in this case $a'b<0<a'b'\leq \max\{ab,a'b'\}$ and $ab'<0<ab\leq \max\{ab,a'b'\},$  we deduce that $\max\{ab,a'b,ab',a'b'\}=\max\{ab,a'b'\}.$ 
 Then, for any $x\in A$ and $y\in B,$ we have $xy\leq \max\{ab, a'b'\}.$  Assume first that $\max\{ab, a'b'\}=a'b'.$  For sufficiently small $\epsilon>0,$ such that $a'+\epsilon<0$ and $b'+\epsilon<0,$  there exist $x^*\in A$ and $y^*\in B$ for which $x^*<a'+\epsilon<0$ and $y^*<b '+\epsilon<0.$ This gives $x^*y^*>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon).$
 Hence $\sup AB=c\geq x^*y^*\geq a'b'.$  Therefore $a'b'$ is the least upper bound  of $A\cdot B.$  In the case where $\max\{ab, a'b'\}=ab,$   for sufficiently small $\epsilon>0,$  such that 
 $a-\epsilon>0$ and $b-\epsilon>0,$
 there exists $x_1\in A$ and $y_1\in B$ such that  $x_1>a-\epsilon>0,  y_1>b-\epsilon>0.$ This gives $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-a\epsilon-\epsilon(b-\epsilon)=ab-\epsilon\big(a+b-\epsilon\big).$ Since 
 $\lim_{\epsilon\to 0+}\epsilon\big(
 a+b-\epsilon\big)=0,$ the order preserving property of limit gives that $x_1y_1\geq ab,$ and hence $c\geq ab.$  Therefore $\sup (AB)=c=ab=\max\{ab,a'b,ab',a'b'\}.$ Therefore we have proved that, in this sub-case, $\sup AB=\max\{ab,ab',a'b,a'b'\}.$ (iv.2) $a>0, b\leq 0.$  Then $ab\leq 0\leq a'b\leq a'b',  ab'\leq 0\leq a'b',$ which gives that $\max\{ab,a'b,ab',a'b'\}=a'b'.$ 
 In this case, for every $y\in B,$  $y\leq 0.$  Let $z\in AB,$ then there exist $x\in A, y\in B$ such that $z=xy.$  If $x\geq 0,$ then  since $a'< x\leq a,$ so $z=xy\leq a'y\leq a'b';$ while if $x<0,$ then $z=xy\leq a'y\leq a'b'.$  Hence $a'b'$ is an upper bound of $AB.$  Since $a'<0$ and $b'<0,$ for sufficiently small $\epsilon>0,$ 
 such that $a'+\epsilon<0$ and $b'+\epsilon<0,$
  there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0,$ which implies that 
 $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon)\to a'b', $ as $\epsilon\to 0+0.$ Hence $x_1y_1\geq a'b'.$  Thus $c=\sup AB\geq x_1y_1\geq a'b'.$  Therefore we have $\sup AB=c=a'b'=\max\{ab,a'b,ab',a'b'\}.$ (iv.3)  $a\leq 0, b>0.$  Similar to sub-case (iv.2), or you can argue by simply swapping  $A$ and $B.$ (iv.4) $a\leq 0, b\leq 0.$  Because in this case $a,a',b,b'$ are non-positive, $ab\leq a'b\leq a'b',$ and $ab'\leq a'b',$ we deduce that $\{ab,a'b,ab',a'b'\}=a'b'.$ 
  For every $x\in A, y\in B,$ we have $a'\leq x\leq a\leq 0, b'\leq y\leq b\leq 0,$ so 
 $xy\leq a'b'.$  From this it follows that $a'b'$ is an upper bound of $AB.$   Let $\epsilon>0$ be sufficiently small such that
 $a'+\epsilon<0$ and $b'+\epsilon<0.$
  Then there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0.$ So $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon). $  It follows  that $c\geq a'b'.$  As a consequence, we have $c=a'b'=\max\{ab,a'b,ab',a'b'\}.$
$\qquad\Box$","['supremum-and-infimum', 'analysis', 'proof-verification']"
1896893,What is the name of the shape created by the intersection of a 3D-rectangular hyperbola with a plane?,"Let $x\in R^3$. What is the name of the curve that satisfies 
$x_1\cdot x_2\cdot x_3 = c$ and $x_1 + x_2 + x_3 = d$ for appropriately chosen $c$ and $d$  so that the curves intersect? Note that the plane is perpendicular to the hyperbola's major axis. Note that what I am referring to as a 3-dimensional hyperbola ($x_1 * x_2 * x_3 = c$) is not a hyperboloid in its canonical form but rather an extension of the rectangular hyperbola ($x_1 \cdot x_2 = c$) into $R^3$ (and $R^n$); so it too may have a different proper name in geometry. Intersection of plane with 3D-hyperbola Contour of intersection on the plane",['geometry']
1896898,Arbitrary union of sets in topological space,"In my text on topology, the term arbitrary union constantly appear without the author having state the definition. Google search didn't quite give me a satisfactory answer either. In simple terms, what exactly is arbitrary union ? Thanks in advance.","['general-topology', 'elementary-set-theory']"
1896910,Chern class computation of push forward.,"I am looking at the section of chern class computation from Robert Friedman- Algebraic surfaces and holomorphic vector bundles. If $X$ is smooth quasi-projective variety, $Z$ is a reduced, irreducible subvariety of codimension $r$ in $X$. $j:Z\hookrightarrow X$ denote inclusion map. Then $$c_i(j_* O_Z)=0 \textrm{ for } i<r \textrm{ and } c_r(j_* O_Z)=(-1)^{r-1}(r-1)![Z]$$ where $[Z]$ is the cycle associated to $Z$. Similary, if $V$ is a vector bundle on rank $n$ on $Z$, then 
$$c_i(j_* V)=0 \textrm{ for } i<r \textrm{ and } c_r(j_* V)=(-1)^{r-1}(r-1)!n[Z].$$ This is statement given in the book. What about higher chern classes? Can we say if $c_{i}(j_*O_X)=c_{i}(j_*V)=0$ for $i>r$. Is this true?","['characteristic-classes', 'vector-bundles', 'intersection-theory', 'algebraic-geometry']"
1896935,Sets of vectors with a particular property,"Suppose I have two sets, $A$ and $B$, each of which contains a number of $n$-dimensional vectors. Each of the elements of these vectors are real and non-negative. I want these sets to have the property that $\mathbf{a}\cdot \mathbf{b} = 1$ whenever $\mathbf{a}\in A$ and $\mathbf{b}\in B$. Here, $\mathbf{a}\cdot \mathbf{b}$ is the usual dot product, i.e. $\mathbf{a}\cdot \mathbf{b} = \sum_i a_i b_i$. There are a few easy ways to construct examples of this, but my question is about how to properly characterise the set of all possible solutions. One example is where $A=\{(1/2,1/2)\}$ and $B = \{(1,1),(1/2,3/2)\}$, but this isn't a very interesting example because $A$ only has one element. A slightly more interesting example is $A=\{(1,0,0),(1,1,0)\}$, $B=\{(1,0,0),(1,0,1)\}$, but that relies on elements of the vectors being zero. My first question is, is there an example where both $A$ and $B$ have more than one element, and all of the elements of each vector are positive? My second question is, is there a straightforward way to construct the set of all possible sets $A$ and $B$, given $n$ (the dimensionality of the vector space) and the number of elements in $A$ and $B$?","['probability-theory', 'linear-algebra']"
1896955,Evaluating $\int_0^{\pi/4} \ln(\tan x)\ln(\cos x-\sin x)dx=\frac{G\ln 2}{2}$,"In order to compute, in an elementary way , $\displaystyle \int_0^1 \frac{x \arctan x \log \left( 1-x^2\right)}{1+x^2}dx$ (see Evaluating $\int_0^1 \frac{x \arctan x \log \left( 1-x^2\right)}{1+x^2}dx$ ) i need to show, in a simple way , that: $\displaystyle \int_0^{\tfrac{\pi}{4}} \ln(\tan x)\ln(\cos x-\sin x)dx=\dfrac{G\ln 2}{2}$ $G$ is the Catalan's constant. PS: This formula is equivalent to: $\displaystyle \int_0^1 \dfrac{\ln x\ln(1-x)}{1+x^2}dx-\dfrac{1}{2}\int_0^1 \dfrac{\ln x\ln(1+x^2)}{1+x^2}dx=\dfrac{G\ln 2}{2}$ It's not a trivial formula for me. PS2: $\displaystyle \ln(\tan x)\ln(\cos x-\sin x)=\ln(\sin x)\ln(\cos x-\sin x)-\ln(\cos x)\ln(\cos x-\sin x)$ Compare to: $\displaystyle \ln(\cos x-\sin x)\ln(\cos x) dx-\ln(\cos x+\sin x)\ln(\sin x)$","['integration', 'definite-integrals']"
1896983,Proof that $\| \varphi \| = \varphi(1)$ for positive linear functionals on operator systems,"Let $M \subset \mathcal{B}(H)$ be an operator system, i.e., $M$ is a self-adjoint unital subspace, and let $\varphi: M \rightarrow \mathbb{C}$ be a positive linear functional. How does one prove that $\varphi$ is bounded with $\| \varphi \| = \varphi(1)$?","['functional-analysis', 'c-star-algebras', 'operator-algebras']"
1896994,Evaluate $\binom{n}{0}+\binom{n}{2}+\binom{n}{4}+...+\binom{n}{2k}$,"What is the closed formula for finding $$
\sum_{i=0}^{k} \binom{n}{2i}=?\quad ,k\leq \lfloor{n/2}\rfloor
$$ I need summation only up to some intermediate  $k$ where $k\leq \lfloor{n/2}\rfloor$ . SAY I need a closed formula to calculate $\binom{8}{0}+ \binom{8}{2}+\binom{8}{4} +\binom{8}{6}$ HERE $n=8$ and $k=6$","['combinatorics', 'binomial-coefficients', 'discrete-mathematics']"
1897002,Find the limit of this function as $x\to\infty$,"Prove that if a sequence of real numbers $(a_n)$ converges to $g$, then $\text{lim}_{x\to\infty}e^{-x}\sum_{n=0}^{\infty}a_n\frac{x^n}{n!}=g$. I'm not exactly sure how to do this. I tried looking at the partial sums firs $\sum_{n=0}^ka_n\frac{x^n}{n!}$. For a fixed $x$, I'm guessing that this converges to a function $g\cdot l(x)$ where $\frac{l(x)}{e^x}\to 0$ as $x\to\infty$. I'm not sure how to show this though. I tried to bound $|(a_1x+a_2x^2/2+...+a_kx^k/k!)-kx^kg|$, but wasn't getting anywhere. Does anyone have advice on how I should get started?","['sequences-and-series', 'calculus']"
1897003,Prove these two angles are equal,$ACB=ACD$ $BAC=ADC$ $MB=MA$ $Prove \angle MCB=\angle MBD$,"['angle', 'geometry']"
1897013,Push-forward of vector fields of Heisenberg type group $\mathbb R \times \mathbb C$,"I am learning some geometry by myself, so pleas be explicit as possible. My aim is to calculate the structure constants of the Lie algebra of $\mathbb R \times \mathbb C$ equipped with the low 
$$(x_0,x) \cdot (y_0,y) = (x_0+y_0+\frac{1}{2}Im(x\bar{y}), x+y)$$
I think that, with the trivial chart $(\mathbb R \times \mathbb C,Id_{\mathbb R \times \mathbb C}) $ this can be viewed as a real manifold. Moreover, it is a lie group. Let $\ell_{(z_0,z)}$ be the left multiplication operator by $(z_0,z) \in \mathbb R \times \mathbb C$ I am confused with the combination of the real and complex variables in the definition of the group, for a basis of the tangent vectors at (0,0) I guess we should take something like $$\partial_{x_0}|_{(0,0)} , \partial_{x}|_{(0,0)} , \partial_{\bar{x}}|_{(0,0)}$$
but when I try to push-forward one of them by $\ell_{(z_0,z)}$ using the Jacobian matrix technique, which turns to be a $2\times 3$ matrix, I loose information about the last vector field (with the bar). I know I am probably doing a stupid error. I don't have time now to include my explicit computation, but I will add them later if it is needed. Update : I want to add that I am interested in results without using matrix representation, for learning purpose and for my intention of doing computations in higher dimension and in other similar lie groups. I tried to imitate the computation done in the following lecture note by M.L. Fels : An Introduction to Differential Geometry through Computation Page 61 § 4.2 $$ {\ell_{(z_0,z)}}_* \partial_{x_0}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} $$ 
$$ {\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} $$ 
$$ {\ell_{(z_0,z)}}_* \partial_{\bar x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} $$ 
where J|_{(0,0)}, is the Jacobian matrix ${\begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}}_{|(0,0)} = \begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}$ associated to the application $\ell_{(z_0,z)}$. I am confusing the actual tangent vector at $\ell_{(z_0,z)}(0,0) = (z_0,z)$ with its coefficient in the base $\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}$. The problem is that  these computations work well in  open sets of $\mathbb R^n$ but are not consistent when $\mathbb C$ is introduced. I guess I have to proceed otherwise? Probably by  rethinking about the chart and try to devise $\mathbb C$ into $\mathbb R \times \mathbb R$ ? To illustrate the problem : if we do the computation for the following $${\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} =  \begin{pmatrix} -\frac{1}{4} \bar{z} \\ 1 \end{pmatrix}$$ the result is only two coefficients, but we need three to write the tangent vector in $\{\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}\}$","['differential-geometry', 'lie-groups']"
1897055,"Is there a permutation on $\mathbb{N}$ that, if repeated often enough, eventually shuffles the whole set?","Does there exist a computable $\pi : \mathbb{N} \to \mathbb{N}$, bijective † and such that for all $i,j\in\mathbb{N}$ there is a $k\in\mathbb{N}$ with
$$
  \pi^k(i) > j,
$$
and, if yes, what is a natural example? † I'm reasonably sure that at least the inverse will have to be uncomputable, but I would need only $\pi$ itself to be computable.","['permutations', 'computability', 'elementary-set-theory']"
1897056,Analytic Number Theory - Prerequisites [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question I'm interested in studying Analytic Number Theory using Apostol's Introduction to Analytic Number Theory. Having looked briefly at Apostol's Introduction to Analytic Number Theory and having read the reviews, I can see that complex analysis is assumed in this book. In my degree I studied some complex analysis (up to and including the residue theorem). However this was things like using Laurent Series, contour integration methods and stuff - I'm not sure if that would count as complex 'analysis' (as this was not a hugely rigorous course (in the sense that a Real Analysis course would cover deriving the derivative and stuff by use of limits, convergence etc)). Also I did not study any real analysis. I also studied a lot of pure maths (group theory, ring and field theory, number theory). Would it be sufficient (or necessary) to go over: Basic real analysis (up to convergence of functions).
based on the book: http://www.springer.com/us/book/9781493927111 up to the chapter 'Functional Limits and Continuity'
and Complex Variable up to and including the residue theorem (based on this book: https://www.pearsonhighered.com/program/Osborne-Complex-Variables-and-their-Applications/PGM271956.html Chapters 1-5 (in particular, is this rigorous enough for a prerequisite to Analytic Number Theory? This was the book used when I was an undergraduate (in actual fact taught by the author) Thank you for any guidance.","['complex-analysis', 'real-analysis', 'analytic-number-theory']"
1897064,Prove that $ABC$ is right-angled,"Prove that if $\cos^2{A} + \cos^2{B} + \cos^2{C} = 1$, then $ABC$ is right-angled. I only found that $\sin^2{A} + \sin^2{B} + \sin^2{C} = 2$, but I have no idea what to do next. Thank you in advance for your answers!","['trigonometry', 'triangles']"
1897078,Maximal abelian extension unramified outside a set of primes,"Let $S$ be a set of prime numbers and $K$ be a number field. What is the definition of maximal abelian extension over $K$ unramified outside $S$? Why does it exist and why if $S=\{p\}$, where $p$ is a prime number, it is equal to $$\bigcup_{n\ge 1}K_{p^n}$$
where $K_{q^n}$ is the ray class field modulo $q^n\mathcal{O}_K$?
Thanks.","['number-theory', 'class-field-theory', 'algebraic-number-theory']"
1897089,Proof that $\frac {1} {2\pi i} \oint \frac {{\rm d}z} {P(z)} $ over a closed curve is zero.,"Prove that $$\frac {1} {2\pi i} \oint \frac {{\rm d}z} {P(z)} $$ over a closed  smooth simple curve that contains all of the roots of the polynomial $P(z)$ is either zero if $n \geq 2$ or equals $\frac{1} {a_0} $ if it's degree is $1$. Proof: $n\geq 2$ $$\frac {1} {P(z)=(z-z_0)(z-z_1)\dots(z-z_n)}$$ So after partial decomposition of the fraction I'll have something like $$\frac {A} {z-z_0} +\frac{B} {z-z_1}+\cdots$$ where $A,B \in \mathbb{C}$ (Is this fact true that $A,B$ wont be polynomials and just coefficients I can see it why in an intuitive way but cannot write a full proof of it for the decomposition trying to get a linear system or something) then use Cauchy's theorem and integrate all of these on  different circles with their center at each root  and argue that  the integral is the same as the sum of those circle integrals. Now since $\frac {1}{z-z_n}$ will be analytic inside the circle when I'm integrating on circles that do not contain $z_n$  some  of those integrals  will be zero. and now since my curves are of the form $\gamma(t)=z_n+e^{it}$ , $t \in [0,2\pi]$ the rest of the integrals  will look like $$\int_0^{2\pi}  \frac{Aie^{ti}} {e^{it}}\,{\rm d}t $$  and all of them will be $A_i$. Is my proof correct? Only part I cannot show is that I won't have any problem with decomposing the fraction. And I'm also using the assumption that after the decomposition I'll have as numerators simple complex numbers not not functions of any kind.","['cauchy-integral-formula', 'complex-analysis', 'integration', 'contour-integration', 'analysis']"
1897108,How can we show that the sample paths of standard Brownian motion are continuous?,"I know the path of Brownian mothion is continuous in probability if and only if ,for every $\delta>0$ and $t>0$,
$$\lim_{\Delta t\to 0}P(|B(t+\Delta t)-B(t)|\ge \delta)=0$$
I  can't continue it . I have seen some proofs of this theorem in this site but I want to prove it by definition. So thanks","['stochastic-processes', 'continuity', 'probability', 'brownian-motion']"
1897115,Extract countable set from uncountable set,"I have switched for a moment from calculating integrals and solving differential equations to learning how to prove statements about infinite sets. I have made a list of exercises for training and found myself absolutely helpless even in the face of the simplest problems in that list. The thought process that is required, currently, is alien to my brain, so I want to start with a little help from the community. The problem is : Given an uncountable set A of positive numbers, prove that we can choose a countable subset B from set A , so that the sum of the elements of B is infinite. I guess, we should somehow provide the construction of the set $B \subset A$ with the required property, but I don't event know how to start. I'd like to see the thinking process with as much detail as possible, so that I could internalize this type of thinking. No, my question is not duplicate. Why do you mark it a duplicate so fast? 1. I need to extract countable set B , not any set. 2. I am more concerned with the specific type of thinking that is needed to solve these kinds of problems, than particular one-line formula that does it. In order to make my question concrete, I have provided a specific problem. By explaning how we are coming up with the solution to that problem it is easy to show how one thinks in the process. A lot of interesting and useful information has been generated in the comments :) Now I am going into analysing mode and start putting this information together inside of my brain (and mold it with additional thinking). It is interesting, what time will it take before I get a general feeling for what is happening :)","['real-analysis', 'elementary-set-theory']"
1897127,An interesting geometry problem about incenter and ellipses.,"Let $I$ be the incenter of a triangle $ABC$. A point $X$ satisfies the conditions $XA+XB=IA+IB$, $XA+XC=IA+IC$. The points $Y,Z$ are defined similarly. Show that the lines $AX,BY,CZ$ are concurrent or parallel to each other. My friend discovered this problem when he was drawing random ellipses for fun. But we have no idea how to solve such a problem because we literally know nothing about ellipses (except its definition). So I can't post where I'm stuck here. We're just curious to see the solution, whether or not it's elementary. We do not know what kind of tags we should add because we do not know what methods are to be used. Please edit the tagging.","['conic-sections', 'triangle-centres', 'geometry']"
1897129,Function even or odd?,"While determining fourier series expansion of following function. f(x) = $\sqrt{1-\cos x} $ here $f(-x) = f(x)$ so it is even. But the above function can also be written as 
$f(x) = \sqrt 2\sin(x/2)$ here $f(-x) = -f(x)$
so by that logic it is odd Can anyone explain where I am wrong.
 Thanks in advance","['fourier-series', 'trigonometry', 'functions']"
1897138,Maxima of almost quadratic function.,"Consider $f(x) = x^2 - \sqrt{(ax + b\sin(\theta))^2+(b \cos(\theta))^2}$, for some $a, b, \theta$ real and positive. Then:
$f'(x) = 2x - \frac{a(ax + b \sin(\theta)}{\sqrt{(ax + b\sin(\theta))^2+(b \cos(\theta))^2}}$ I'm looking for some relation between $a, b, \theta$ (for nonzero $a, b$) such  that $f(x)$ has 2 minimums and 1 maxima, or just one minima. (Those are the only cases possible I believe, feel free to prove me wrong). For example, if $\theta = 0$, then for $a^2 > 2b$ we have 2 minimas and one maximum at $x =-\frac{\sqrt{a^4 - 4b^2}}{2a}, 0 , \frac{\sqrt{a^4 - 4b^2}}{2a}$.
Ofcourse one could set $f'(x)=0$, move the ugly term to RHS and square both sides but I wasn't able to neatly solve the resolving 4'th order polynomial. Experimenting numerically, I have found that if we set $a,b$ such that $a^2 > 2b$ and increase $\theta$ from zero to $\pi/2$ we will go from having 3 zero's for $f'(x)$ to 1 and back to 3 again. I'm more interested in amount of maxima/minima, instead of their location. Any and all help would be appreciated. Edit: Numerical experiments were done using Newton -Rhapson and the second derivite is:
$f''(x) = 2 - \frac{(ab\cos(\theta))^2}{((ax + b\sin(\theta))^2+(b \cos(\theta))^2)\sqrt{(ax + b\sin(\theta))^2+(b \cos(\theta))^2}}$ Edit2:
Trying to solve $f'(x) = 0$ yelds: $\alpha x^4 + \beta x^3 + c x^2 + dx +e =0$ with
$\alpha = 4a^2$, $\beta = 8ab\sin(\theta)$, $c = 4b^2 -a^4$, $d = -2a^3b\sin(\theta)$, $e = - a^2b^2\sin^2(\theta)$.
I'm looking for the number of real roots.","['algebra-precalculus', 'polynomials', 'calculus', 'derivatives']"
1897146,Axiom of union; union of natural and real numbers,"In set theory we assume the axiom of union to be true for all universes, more formally $\forall x \exists y \forall [z \in y \Leftrightarrow \exists t (t \in x \land z \in t)]$. We call the set $y$ the union of $x$. This is intuitively understood as the set consisting of the elements of the elements of $x$, but I have trouble understanding the scenario where the elements of $x$ aren't generally considered to be sets. Take the natural numbers $\mathbb{N}$ as an example. After the axiom $\cup \mathbb{N}$ exists and it consists of the elements of the natural numbers; however, numbers aren't intrinsically sets. As everything has to be a set in set theory many different set theoretic constructions of our number systems have been devised, such as the Von Neumann construction of the natural numbers. What I'm wondering is what $\cup \mathbb{N}$ or $\cup \mathbb{R}$ in fact are, especially in the usual set theoretic constructions. Do they have some kind of a deeper property or are they arbitrarily determined by the universe $\mathscr{U}$ and the construction of $\mathbb{N}$ and $\mathbb{R}$ in $\mathscr{U}$?",['elementary-set-theory']
1897164,Leray theorem for acyclic covering,"I'm struggling with the proof of Leray's theorem. Here's the statement: Theorem Let $X$ be a topological space, $\mathscr{F}$ a sheaf on $X$ and $\mathscr{U} = \{ U_i \}_{i \in I}$ an open cover of $X$. If $\mathrm{H}^p(V, \mathscr{F}) = 0$ for all $p$ and all $V = U_{i_0}\cap \dots U_{i_n}, i_j \in I$ then $\check{\mathrm{H}}^p(\mathscr{U}, \mathscr{F}) \simeq \mathrm{H}^p(X, \mathscr{F})$ for all $p$. I'm stack at the end of the proof.
Let's embed $\mathscr{F}$ into a flasque sheaf $\mathscr{G}$ and then let us take the quotient $\mathscr{H}$. Then we have a short exact sequence of sheaves: $$0 \rightarrow \mathscr{F} \rightarrow \mathscr{G} \rightarrow \mathscr{H} \rightarrow 0$$ From $\mathrm{H}^1(V, \mathscr{F}) = 0$ for all intersections we get that the short sequence of Cech complexes $$0 \rightarrow C^{\ast}(\mathscr{U}, \mathscr{F}) \rightarrow C^{\ast}(\mathscr{U}, \mathscr{G}) \rightarrow C^{\ast}(\mathscr{U}, \mathscr{F}) \rightarrow 0$$ is exact. Comparing the long exact sequence we then get in Cech cohomology with the one we already had from sheaf cohomology, we obtain that: $\check{\mathrm{H}}^1(\mathscr{U}, \mathscr{F}) \simeq \mathrm{H}^1(X, \mathscr{F})$ $\check{\mathrm{H}}^{p-1}(\mathscr{U}, \mathscr{H}) \simeq \check{\mathrm{H}}^p(X, \mathscr{F})$ The professor then concludes by induction. What I don't understand is why he is able to use the induction hypothesis. We have the diagram: $$
\require{AMScd}
\begin{CD}
0 @>>> \check {\mathrm{H}}^{p-1}(\mathscr{U}, \mathscr{H}) @>\simeq>> \check {\mathrm{H}}^p(X, \mathscr{F}) @>>> 0 \\
@VVV @VVV @VVV @VVV\\
0 @>>>\mathrm{H}^{p-1}(\mathscr{U}, \mathscr{H}) @>\simeq>> \mathrm{H}^p(X, \mathscr{F}) @>>> 0
\end{CD}
$$ He says that on the $p-1$ Cech cohomology group of $\mathscr{H}$ we can use the induction hypothesis. I don't succeed in understanding why: we don't know that the cohomology group of $\mathscr{H}$ are zero on the intersection of open sets took from $\mathscr{U}$, right? I would be very grateful if someone could give me an hint to conclude the proof. I think it's important to point out that in class we did't talk about neither quasi-coherent sheaves or schemes.","['sheaf-cohomology', 'algebraic-geometry']"
1897171,Is it important to understand Linear algebra geometrically?,"I am currently self studying linear algebra, using Axler's text. And I always find myself trying to picture every theorem and proposition by considering some low dimensional cases, in order to understand why the theorems make sense. As I get to the chapter for eigenvalues and eigenvectors, things are getting a bit abstract. Specifically, he proved that every linear operator over an odd dimensional real vector space must have an eigenvalue by using induction, he considered the case where $U$ is a $2$ dimensional invariant subspace of $V$, set $W$ such that $U\oplus W=V$. Then consider the projection of $T\vec w$ onto $W$ along $U$ as an operator in $L(W)$, which must have an eigenvalue $\lambda$ by induction hypothesis. In the end, he showed that $T-\lambda I$ is not injective, thus $\lambda$ is also an eigenvalue of $T$. I understand everything he's written, but failed to grasp the bigger picture as I can only imagine vector space up to $3$ dimensions (in which case $W$ is only one dimensional). The construction is remarkable for me, as two different linear operators have the same eigenvalue, which I have no idea what the picture would look like (e.g. where does the eigenvector lie on) in higher dimensional spaces. So my question: Is understanding linear algebra through picture important? In some sense, I feel like it helps me understand more why some theorems would be true, but the proofs/constructions do not rely on picture at all as they mostly involve algebraic manipulations.","['eigenvalues-eigenvectors', 'linear-algebra', 'soft-question']"
1897212,"Does there exist any surjective group homomorphism from $(\mathbb R^* , .)$ onto $(\mathbb Q^* , .)$?","Does there exist any surjective group homomorphism from $(\mathbb R^* , .)$ (the multiplicative group of non-zero real numbers) onto $(\mathbb Q^* , .)$ (the multiplicative group of non-zero rational numbers)?","['abelian-groups', 'group-homomorphism', 'infinite-groups', 'group-theory']"
1897258,Isomorphism between $\Bbb R$ and $\Bbb R(X)$?,"My questions are: $1.$ Is there a field morphism $\Bbb R(X) \hookrightarrow \Bbb R$ ? $2.$ If the answer to $1.$ is ""yes"", are $\Bbb R$ and $\Bbb R(X)$ isomorphic as fields? $ $ $ $ 
For $1.$, here are some comments: This is true for $\Bbb C$, see Can I embed $\Bbb{C}(x)$ into $\Bbb{C}$? . The key point here is the fact that $\Bbb C$ is algebraically closed, of cardinality $2^{\aleph_0}$, and of characteristic $0$. The same argument doesn't work for $\Bbb R$. We can actually find a subfield $F$ of $\Bbb R$ such that $F \cong F(X) \cong F(X,Y) \cong \cdots$, see Is there a subfield $F$ of $\Bbb R$ such that there is an embedding $F(x) \hookrightarrow F$? . Notice that $\Bbb Q(X)$ does not embed in $\Bbb Q$ (as a field). This is equivalent to: Is there an injective ring morphism $\Bbb R[X] \hookrightarrow \Bbb R$ ? (By taking the fraction fields). As for 2. : They are isomorphic as abelian groups, and actually as $\Bbb Q$-vector spaces. They are not isomorphic as $\Bbb R$-algebras, because they haven't the same transcendence degree over $\Bbb R$. Let's try to see how to construct such a field isomorphism. We could try$^{[1]}$ to find a subset $B$ of $\Bbb R$ such that $\Bbb R = \Bbb Q(B)$ and $C \subsetneq B \implies \Bbb R \neq \Bbb Q(C)$. 
The set $B$ must contain transcendental elements, pick such an $x_0 \in B$. Then $\Bbb R = \Bbb Q(B) = \Bbb Q(B \setminus \{x_0\})(x_0)$ and we could try$^{[2]}$ to find a field isomorphism $\Bbb Q(B \setminus \{x_0\}) \cong \Bbb Q(B)$, so that $\Bbb Q(B \setminus \{x_0\})(x_0) \cong \Bbb Q(B)(x_0) \cong \Bbb R(X)$. For ${[1]}$, I tried to use Zorn's lemma on $\mathscr B = \{B \subset \Bbb R \mid \Bbb R = \Bbb Q(B)\}$ and the partial order $B_1 ≤ B_2 \iff B_1 \supset B_2$. But this is not clear to me why this should be an inductive set. For ${[2]}$, my idea was to define $f : \Bbb Q(B \setminus \{x_0\}) \cong \Bbb Q(B)$ by choosing a bijection $b : B \setminus \{x_0\} \to B$ (these sets should be uncountable), and then trying to extend $f\vert_{B \setminus \{x_0\}} = b$ to a field morphism. Edit for $^{[1]}$: in this article (MINIMAL GENERATING SETS OF
GROUPS, RINGS, AND FIELDS, by LORENZ HALBEISEN, MARTIN HAMILTON, AND PAVEL RUZICKA) the theorem 2.4. proves that $\Bbb R$ has no minimal generating set as field (or $\Bbb Q$-algebra, i.e. as ring). The argument uses Artin-Schreier theorem. Minimal generating sets of modules are also discussed here . I think that $B$ is a minimal generating set of a subfield $K \subset \Bbb R$ over $\Bbb Q$ if and only if any element of $B$ cannot be written as a rational fraction of other elements of $B$. Under these conditions and using (transfinite) induction, we can probably prove (and this was my first idea) that any function $f: B \to \Bbb C$ such that $b$ and $f(b)$ are conjugate over $\Bbb Q$ (i.e. have the same minimal polynomial) extends to a (unique) field morphism $\sigma : K = \Bbb Q(B) \to \Bbb C$. Actually, the collection $\mathscr B$ defined above doesn't have to be an inductive set, at least if we replace $\Bbb R$ by $K =\Bbb Q(\sqrt 2, \sqrt[4]{2},\sqrt[8]{2},\dots)$, because the descending chain $\left(E_n = \{\sqrt[2^m]{2} \mid m \geq n\}\right)_{n \geq 1}$ satisfy $K = \Bbb Q(E_n)$, but $K \neq \Bbb Q\left(\bigcap_{n≥1} E_n\right)=\Bbb Q$, so the chain has no upper bound w.r.t. $≤$ in $\mathscr B$.","['abstract-algebra', 'extension-field', 'field-theory']"
1897266,How does one derive Radial Basis Function (RBF) Networks as the smoothest interpolation of points?,"I was reading/watching CalTech's ML course and it said that one could derive the RBF Gaussian kernel from the solution to smoothest interpolation that minimizes squared loss. i.e. one can derive the predictor/interpolator: $$ f(x) = \sum^{K}_{k=1} c_k \exp( -\beta_k \| x - w_k \|^2 )$$ from the Empirical Risk minimizer with a smoothest Regularizer: $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda R(f) $$ $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda \sum^{\infty}_{k=0} a_j \int^{\infty}_{- \infty} \left( \frac{d^k h}{d x^k} \right) dx$$ unfortunately, they do not show the derivation of this. Thus I was wondering if someone could show me how minimizing the ERM using that regularizer, one could derive the RBF kernel function. In particular I am very interested in the exact mathematical details and if there is any maths I need to learn I am motivated to learn it to understand this derivation. I believe they mentioned that this regularizer was for some simplified case (not sure which one) but I would be interested to start of in the simple explanation of this derivation (I believe its using only 1D calculus?) and then generalizing as it needed. As a first suggestion the generalization could be the answer to the question Why does minimizing $H[f] =\sum^{N}_{i=1}(y_i-f(x_i))^2+\lambda \| Pf \|^2 $ leads to solution of the form $ f(x) =\sum^N_{i=1}c_iG(x; x_i)+p(x)$? .","['functional-analysis', 'machine-learning', 'approximation-theory']"
1897267,Finding the Total number of permutation using a selective formula,"I am a python programmer,trying to upgrade a python program I created. I am aware the total number of permutations can be calculated using n^r where n is the total number of things to choose from and r is how many we choose.. for example consider the string ""abc"" where need to generate three letter ""words""..the result  below aaa
aab
aac
aba
abb
abc
aca
acb
acc
baa
bab
bac
bba
bbb
bbc
bca
bcb
bcc
caa
cab
cac
cba
cbb
cbc
cca
ccb
ccc Question What I am trying to accomplish is to derive a formula for the total number of permutations where each permutation has maximum nth characters of the same identity..consider this example for example the total number of permutations that has only two letters of the same type repeating.... This is my results for generating all permutations from my program with two letters of the same type reaming aab - only two 'a'
aac - only two 'a'
aba - onyl two 'a'
abb - only two 'b'
abc - maximum is two letters repeating..also good
aca - maximum is two letters repeating..also good
acb - maximum is two letters repeating..also good
acc - maximum is two letters repeating..also good
baa- .maximum is two letters repeating..also good
bab ..................
bac....................
bba
bbc
bca
bcb
bcc
caa
cab
cac
cba
cbb
cbc
cca
ccb TOTAL NUMBER OF PERMUTATION IS 24 I can't seem to derive a way to do this BEFORE outputing the results.. In a nutshell I want to find A FORMULA for the number of permutation that only has nth letters in each permutation repeating not more than nth of the same type.. I want to precalculate the total number of permutations where each permutation has nth letter of the same type remaining and output this to the user of my program My program is bulky but here is the permutation codes I am using (note I am using itertools from python built in libraries) for perm in itertools.product(string,repeat=self.word_length):
    if args:
        args[0]()                    
    #print(""after"",self.counter)
    self.counter=self.counter+1
    result1=[perm.count(z) for z in perm]
    result1.sort(reverse=True)
    if result1[0]<=self.actual_repeat:
        yield perm I am using itertools to generate the permutations , count each letter and find their fequency and filter for two letter words...the variable self.actual repeat is 2","['permutations', 'factorial', 'statistics', 'probability']"
1897305,What is the right way of simplifying this trigonometric expression and why?,"Problem: $$\frac{\frac{cosA}{sinA}-cosA}{\frac{cosA}{sinA}+cosA}$$ sol1:$$=\frac{cosA\left(\frac{1}{sinA}-1\right)}{cosA\left(\frac{1}{sinA}+1\right)}$$
$$=\frac{\left(\frac{1}{sinA}-1\right)}{\left(\frac{1}{sinA}+1\right)}$$
$$=\frac{cosecA-1}{cosecA+1}$$ sol2: $$=\frac{cosA-sinAcosA}{cosA+sinAcosA}$$
$$By ÷cosA$$
$$=\frac{1-cosecA}{1+cosecA}$$ Which is the right answer and why?","['algebra-precalculus', 'trigonometry', 'fractions']"
1897310,Vector relative to another vector,"I have a direction vector (x/y/z) that I need to express relative to the normal vector of a surface, such that the direction vector is ""local"" to the normal vector. I've read that I need to create a basis using the normal vector (up/right/forward) and use the dot product to find each new vector element. 1.) Is this the most efficient way to calculate the localized direction vector? 2.) How do I then transform the direction vector back into its original basis (after it is altered in local space)?","['linear-algebra', 'geometry']"
1897325,Proving equivalent of definitions of measurability,"This is a follow-up question to a previous post . For any open interval $I = (a,b)$, let $\ell(I) = b-a$ denote the length of $I$.
For any set of real numbers $A$, define the following set functions:
\begin{align}
m^*(A) &= \inf\left\{\sum_{i=1}^\infty \ell(I_n): \bigcup_{n=1}^\infty I_n \supseteq A, I_n \text{ is an open, bounded interval for all $n$}\right\} \\
m^{**}(A)&= \inf\{m^*(\mathcal{O}): \mathcal{O} \supseteq A, \mathcal{O} \text{ open}\} \\
m^{***}(A)&= \sup\{m^*(F): F \subseteq A, F \text{ closed}\}
\end{align} Caratheodory defined a set $A$ to be measurable if for any set of real numbers $E$, $$
m^*(E) = m^*(E \cap A) +  m^*(E \cap A^C) \qquad (1)
$$ Other authors define a set $A$ to be measurable if $$
m^{**}(A) = m^{***}(A) \qquad (2)
$$ I'm trying to confirm these two definitions are equivalent. I have been able to show that if $m^{***}(A) < \infty$, then (1) $\iff$ (2).
However I am having trouble showing that if a set satisfies $m^{**}(A) = m^{***}(A) = \infty$, then (1) must hold. Any ideas on how to proceed? Thanks!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1897342,How high can a Kurtosis value be?,"I am working on computing some summary statistics for a bank that nearly failed during the 2008 crisis. The time series I have is the stock price and returns for this particular bank, spanning 9 years. However, for a period of 5 months in my time series, I have the same daily stock price (the bank had its shares suspended from trading). I am computing some summary statistics on the returns of this particular bank, and I obtained a very high Kurtosis of 374 (I expected a high Kurtosis, but not that high). As such, how high can a Kurtosis value be? And if I keep the 374, does it make my computations seem dodgy? Sorry for this very trivial question, but I am quite interested to know whether it is too high for a bank that nearly failed.",['statistics']
1897367,"Trying to prove that $H=\langle a,b:a^{3}=b^{3}=(ab)^{3}=1\rangle$ is a group of infinite order.","I'm trying to prove that the following group has infinite order: $$H=\langle a,b\mid a^{3}=b^{3}=(ab)^{3}=1\rangle.$$ Currently I'm checking on some cases using the relations, but my problem is the reducibility for large products. Naively I started to check that $ab$ is different than $1,a,b$ and then $ba$ than $1,a,b,ab$ , just to understand $H$ in some extent. Now I'm wondering about some more effective method to prove that $|H|=\infty$ , I'm tempted to look for an injection from some group of infinite order into $H$ , but I'm still stuck. More than asking for a solution I'd rather appreciate some hints or thoughts about it. Thanks a lot.","['infinite-groups', 'group-theory', 'group-presentation']"
1897368,Smooth curve that is not birational to $\mathbb{C}\mathbb{P}^1$,"I need to write an equation of a nonsingular curve in a projective space that is not birational to $\mathbb{P}^1.$ The answer in this post seems to give a solution to my question. Namely, it says that the curve $C:=V(f) \subset \mathbb{C}\mathbb P^2$ given by $$f(x,y,z) = x^d + y^d + z^d, \ d \in \mathbb N,$$ is smooth with geometric genus $g(X) = \frac {(d-1)*(d-2)}{2}$. The curve X is not rational iff $g(X) > 0$, i.e. iff degree $d > 2$. But I don`t know a proof of a claim made there ""curve X is not rational iff $g(X) > 0$ "". Can someone help me out, please? Alternatively, is there more elementary example of a smooth curve which is not birational to $\mathbb{C}\mathbb{P}^1$?","['algebraic-curves', 'algebraic-geometry']"
1897375,Need to develop a formula for index/indicator measuring wait time for tellers,"I have data that looks like this: Tell_1  Tell_2  Tell_3
        0       0      -8
       -3       0       0
        0       0       0
       -4      -2       0
        0       0      -2
      -14      -4      -1
        0       0       0
       -1       0      -1

Index:  I1      I2      I3 '---------------------------------------------- This data represents the wait time of each customer at a teller. The value 0 means the customer did not wait at all. The value -x means the customer had to wait x minutes before being served. How can I develop an teller performance index (a mathematical function that takes the wait times an produce 1 value) that shows the teller performance ($I1$, $I2$, $I3$,...)? One way is to just sum each column to obtain -22,-6, and -12. This is only good at showing wait times. However, this does not show how many customers were served immediately (which is a good thing). As a result this index is no good. Another approach that I considered was to assume a max wait time value, say 100, then calculate the index per teller as $\sum(x_i+100)$ to get: 778, 794, and 788. But how good is this one? I need the index to reflect both the wait times and the number of customers that were served immediately. Note: This is not a homework, also, it is not a real situation. Thanks for your help.","['algebra-precalculus', 'statistics', 'mathematical-modeling']"
1897402,$SL_2(\mathbb{R})$ is a 3-dimensional differentiable manifold,"I've seen this result posed in several places as an exercise, and I've also seen the proof for $SL_n(\mathbb{R})$, for example in this answer . But I'm not quite able to follow the details. Can someone work out the details of the $n=2$ case in the most elementary way possible? Thanks in advance for any help.","['manifolds', 'differential-geometry', 'analysis']"
1897430,Why does this follow from the fundamental theorem of calculus?,"This is a lemma in the classical book Brownian Motion and Stochastic calculus by Karatzas and Shreve. It is on page 133. He makes an assertion that a set has Lebesgue measure 0, and that this
  follows from the fundamental theorem of calculus. But why is this?
  Isn't the fundamental theorem of calculus something that deals with
  Riemann integration? We are working with Lebesgue integrals. The problem is showing that for fixed $\omega$, the lebegsue measure
  of t's where $\widetilde{X}^{(m)}(t,\omega)$ converges to
  $X(t,\omega)$ is 0. But since we have an m in the integrand which
  makes up $\widetilde{X}^{(m)}(t,\omega)$ I can not see that we can use
  the DCT directly, because it is not bounded. So it seems like he does
  something with the fundamental theorem of calculus instead? It is a long proof, I put up everything here. But I highlighted what is relevant for the question in green, and the thing I do not know in red. Do you see how he can say what he says?","['stochastic-processes', 'probability-theory', 'measure-theory', 'stochastic-analysis', 'stochastic-calculus']"
1897433,Convergence of $\zeta(s)$ on $\Re(s)> 1$,"I'm aware there are been several similar questions, and some great answers with hints on how to prove this, for example here and here . But I've never really seen a detailed proof of the absolute convergence of the Riemann zeta function in the half-plane $\Re(s)> 1$. I hope there's interest on a detailed proof of this for reference. Here is my attempt. Please, fill in any detail that might be missing, or point out any mistake! On $\Re(s)=\sigma> 1$, we have $$\sum_{n=1}^{\infty}\bigg|\frac{1}{n^s}\bigg|=\sum_{n=1}^{\infty}\frac{1}{n^\sigma}$$ and $$\frac{1}{n^\sigma}\leq\frac{1}{n^{1+\epsilon}}$$ for any $\epsilon >0$, so by the direct comparison test, if $$\sum_{n=1}^{\infty}\frac{1}{n^{1+\epsilon}}$$ converges absolutely, then so do $\sum_{n=1}^{\infty}|1/n^s|$ and $\zeta(s)$. By the integral test, the convergence of $\sum_{n=1}^{\infty}1/n^{1+\epsilon}$ is equivalente to the finitude of the integral $$\int_1^\infty \frac{1}{n^{1+\epsilon}}=\bigg[-\frac{1}{\epsilon x^\epsilon}\bigg]=\frac{1}{\epsilon}$$ which holds for any $\epsilon < \infty$. Any correction or improvement is welcomed! Thanks in advance.","['number-theory', 'complex-analysis', 'riemann-zeta']"
1897447,About absolute continuity $\Rightarrow$ null set maps to null set,"I've searched quite a lot in this site and found many questions that mention this result, but not a single proof. I tried proving it myself and failed. So, I ask it here: how can one prove that if $f:[a, b]\rightarrow \mathbb{R}$ is absolutely continuous, then for every set $A \subseteq[a,b]$ with lebesgue measure $0$, $f(A)$ also has lebesgue measure $0$?","['lebesgue-integral', 'measure-theory']"
1897453,Curvature of a graph,"How to calculate the formula of the curvature of $\alpha= (t, f(t)), f$ is $C^{\infty}$ ? I know that can be done easily using previous curvature formulas, but I couldn't do using jus the curvature definition $k(t)= \langle t'(t),n(t)\rangle$, where $t(t)$ is the unit tangent vector.","['differential-geometry', 'curvature']"
1897455,If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$ then how to show that $A^2+B^2$ equals $A+B$?,"If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$ then  $A^2+B^2$ equals ? (a) $2AB$ (b) $2BA$ (c) $A+B$ (d) $AB$ I tried 
$(A+B)^2=A^2+B^2+AB+BA$ or,$A^2+B^2=(A+B)^2-AB-BA$ $=(A+B)^2-A-B$ $
=(A+B)^2-(A+B)=(A+B)(A+B-1)$ How to reach the answer from here?","['matrices', 'algebra-precalculus', 'linear-algebra']"
1897472,Prove that a given function is locally Lipschitz and therefore differentiable a.e.,"Consider $u:S^1\times (0,T)\to \mathbb R$ a $C^1$ function, and define
  $$v(t)=\max_{p\in S^1} u(p,t)$$
  Prove that $v$ is locally Lipschitz and deduce that $$\frac {dv}{dt}(t_0)=\frac{\partial u}{\partial t}(p_0,t_0)$$ where $p_0$ is any maximum point for $u(\cdot, t_0)$. For point one my idea was to prove that at least one maximum point at height $t$ (viewing the domain as a vertical cylinder) has a neighbourhood which contains a differentiable curve made of maximum points for $u$. Therefore, in that neighbourhood (i.e. for $t$ near to $t_0$) one could use that $u$ is $C^1$. But is that true? And is this really the best way? For point two, I know that locally Lipschitz implies absolutely continuous which implies a.e. differentiable. But how could I deduce the (however intuitive) formula for the derivative? Furthermore, I'm quite sure that the result absolutely continuous $\Longrightarrow$ differentiable a.e. is more advanced than the level at which the problem was given. Isn't there a simpler way for this special case? Thank you in advance.","['derivatives', 'real-analysis', 'lipschitz-functions']"
1897477,How to understand integral of wedge product?,"Taken from Bott and Tu (page 70): Let $\sigma$ be the positive generator of $H^{n-1}(S^{n-1})$ and $\psi = \pi^* \sigma$ the corresponding generator of $H^{n-1}(\mathbf R^n - 0)$. [...] Let $d\rho = \rho'(r)\,dr$ be a bump form on $\mathbf R$ with total integral $1$. Then $(d\rho) \wedge \psi$ is a compactly supported form on $\mathbf R^n$ with total integral $1$. If it's not already obvious, $\pi$ is the projection $\mathbf R^n - 0 \to S^{n-1}$ and $\rho$ is a function of the radius $r$ in $\mathbf R^n$. Question: How am I to understand the last sentence intuitively? So far, the picture in my head is for $n = 3$. I imagine $\sigma \in H^2(S^2)$ to be a function that eats a $2$-dimensional subset of $S^2$ and spits out its area. Then $\psi \in H^2(\mathbf R^3)$ is a function that eats a $2$-manifold in $\mathbf R^3$, projects it onto the unit sphere, and spits out its area. I'm not sure how these help me understand what $d\rho \wedge \psi$ is doing, or why it should integrate to $1$.","['integration', 'differential-forms']"
1897494,If $7$ Divides $\binom{2^n}{2}+1$ then $n =3k+2$ for some positive integer $k$,"It is a straightforward question. If $$ 7 \text{ }\Bigg | \binom{2^n}{2}+1$$ then $n=3k+2$ for some
  positive integer $k$. This is just curiosity  no motivation just rummaging through some old question in a notebook. A simple counter example would work. Note if $\mathfrak a(n) =\binom{2^n}{2}+1$ then $7$ divides $\mathfrak a(n)$ for $n=2,5,8,11,14,17,\ldots$ and from this I am guessing that $n=3k+2$","['algebra-precalculus', 'binomial-coefficients', 'elementary-number-theory']"
1897521,"Smallest positive integer that gives remainder 5 when divided by 6, remainder 2 when divided by 11, and remainder 31 when divided by 35? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question What is the smallest positive integer that gives the remainder 5 when divided by 6, gives the remainder 2 when divided by 11 and 31 when divided by 35? Also, are there any standard methods to solve problems like these?","['discrete-mathematics', 'elementary-number-theory']"
1897538,Examples of limits in nature with $\lim_{x \to c}f(x) \neq f(c)$,"Next week I will start teaching Calculus for the first time. I am preparing my notes, and, as pure mathematician, I cannot come up with a good real world example of the following. Are there good examples of
\begin{equation}
\lim_{x \to c} f(x) \neq f(c),
\end{equation}
or of cases when $c$ is not in the domain of $f(x)$? The only thing that came to my mind is the study of physics phenomena at temperature $T=0 \,\mathrm{K}$, but I am not very satisfied with it. Any ideas are more than welcome! Warning The more the examples are approachable (e.g. a freshman college student), the more I will be grateful to you! In particular, I would like the examples to come from natural or social sciences. Indeed, in a first class in Calculus it is not clear the importance of indicator functions, etc.. Edit As B. Goddard pointed out, a very subtle point in calculus is the one of removable singularities. If possible, I would love to have some example of this phenomenon. Indeed, most of the examples from physics are of functions with poles or indeterminacy in the domain.","['limits', 'calculus', 'soft-question', 'applications', 'education']"
1897583,"Proving that $\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) $","In fact, what I really want to prove is the equality. The another inequality is already proven, but this one I'm not able to prove. Fix an invertible matrix $A\in\mathbb{R}^{n\times n}$ and a vector $c\in\mathbb{R}^n$, I need to prove the following inequality. $$\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) $$ $r,s$ are positive integers and $\|\cdot\|_{rs}$ is the matrix norm induced by the norms $\|\cdot\|_r$ and $\|\cdot\|_s$. Thank you very much for your help. EDIT: Ok, I have an answer and I'm very grateful for it. But what I should have said is that I want a direct proof. Is it possible to exhibit $M$ and $b$ in order to get an equality?","['matrices', 'normed-spaces', 'inequality', 'linear-algebra']"
1897657,Are there spaces with non-integer Hausdorff dimension that are differentiable?,"I realize that fractal curves usually represent non-differentiable functions in some sense, but would it to be possible to have a space that has non-integer Hausdorff dimension, and still have a reasonable notion of differentiation?  Essentially I'm curious about the ability for a space with a (pseudo)Riemannian metric, to have non-integer Hausdorff dimension.  If so, can you please provide an example and/or construction? If not, why?","['hausdorff-measure', 'riemannian-geometry', 'differential-geometry']"
1897707,Open sets of integral schemes,"The following is the definition of integral scheme as mentioned here Let $X$ be a scheme. We say $X$ is integral if it is nonempty and for every nonempty affine open $\operatorname{Spec}(R)=U \subset X$ the ring $R$ is an integral domain. How do I show that for any open set $U$, $\mathcal{O}_X(U)$ is an integral domain?","['schemes', 'algebraic-geometry', 'commutative-algebra']"
1897710,"All but $35$ values can be payed with coins of $a$ and $b$, and $58$ can't be paid. Find $a$ and $b$.","let $a<b$ be positive integers, using coins of values $a$ and $b$ exactly $35$ values cannot be paid exactly without change. $58$ is among these values. Find $a$ and $b$. I managed to solve this, but it took me almost twenty minutes, is there a fast solution for this, also is my solution good? Thanks and regards. My solution: Clearly $(a,b)=1$. Notice every integer $v$ can be canonically written uniquely as $sa+tb$ with $s\in\{0,1\dots b-1\}$ and $t\in \mathbb Z$. A value $v$ can't be formed if and only if $t<0$ in this expression. If a positive integer has canonical expression with $b$ negative then $s\in\{1,\dots b-1\}$ and $t\in\{-1,-2\dots,-a+1\}$. Of all of these expressions half of them are negative because if $x$ is expressed that way so is $-x$. Therefore $(a-1)(b-1)=70\implies (a,b)=(2,71),(3,36),(6,15)$ or $(8,11)$. The first doesn't work as $58=29\times 2$, the next $2$ don't work as they aren't coprime. The last one works as $58=10\times 8 - 2\times 11$. So $a=8,b=11$.","['number-theory', 'contest-math']"
1897738,Largest subset with no arithmetic progresssions,"Let $A=\{x_1,x_2,\cdots,x_n\}$ be a set of $n$ distinct real numbers. Show that there exists a set $B\subset A$ such that $$|B|\geq\lfloor\sqrt{2n}+\frac12\rfloor$$ and no $3$ distinct elements of $B$ constitute an arithmetic progression. I have no idea how to approach this problem, with its strange-looking formula. I've already posted it on AoPS but no reply.","['optimization', 'extremal-combinatorics', 'combinatorics', 'contest-math', 'discrete-mathematics']"
1897742,What else do we gain from best linear approximations?,"In single-variable calculus, we're introduced to the idea of best linear approximation, that is: $$f(x) \approx f(a) + f'(a)(x - a)\tag{1}$$ And this is the best linear approximation for the point $a$. In multivariable calculus, the same concept is presented again, and for two variables, it is: $$f\left(x,y\right)\approx f\left(a,b\right)+\frac{\partial f}{\partial x}\left(a,b\right)\left(x-a\right)+\frac{\partial f}{\partial y}\left(a,b\right)\left(y-b\right)$$ I've been thinking for a while but couldn't guess why this is important. We could say that in the $(1)$, it gives us the line tangent to the function $f$ at point $f(a)$, this would reveal us how much the function is increasing at that point - but the same can be said using only the derivative. For multivariable calculus, it's even worse. They tell us that there is a best linear approximation, which is a plane tangent to a surface, but in this case, we are not told what they are important for: The subject is given and then we jump to other subjects. For a while, I thought that for some functions, there is some points in which it is easy to calculate the value and other points in which it's hard to do it, ex: $\sqrt{4}=2,\sqrt{5}=2.23607\dots$ and then we could anchor to these points $a$, vary a little bit the $x$ and find an approximation, perhaps with an error function that allows an easier computation some way? But then I thought that this kind of problem could be easily bypassed with the use of modern computers. So after all, why are best linear approximations useful? I'm mostly interested in applications to mathematics, but whatever comes in mind, just say it.","['linear-approximation', 'analysis', 'approximation']"
1897758,"$f>0$ on $[0,1]$ implies $\int_0^1 f >0$","Someone made the remark on my old question (second-to-last comment on the answer from here ) that a integrable function $f>0$ on $[0,1]$ does not imply $\int_0^1 f >0$ since
 limits do not preserve strict inequality. But I think it is true and I will try to give a proof. Since $\{f>0\} = \cup_{n=1}^{\infty}\{f>\frac{1}{n}\}$, from continuity of Lebesgue measure 
$$ 1= m(\{f>0\}) = m\left(\bigcup_{n=1}^{\infty}\Big\{f>\frac{1}{n}\Big\}\right) = \lim_{n\rightarrow \infty} m\left(\Big\{f>\frac{1}{n}\Big\}\right),$$
this means there exists $N$ such that  $m\left(\Big\{f>\frac{1}{N}\Big\}\right)\geq 1/2$. Then $\frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \leq f$ and 
$$\int_0^1 f \geq \int_0^1 \frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \geq \frac{1}{2N} > 0.$$ Is this correct?","['real-analysis', 'lebesgue-integral', 'proof-verification']"
1897791,Boundedness of Radon-Nikodym derivative,"I have the following question: Under which conditions (I mean the conditions on the measures) the Radon-Nikodym derivative is bounded ? I made some researches but I couldn't find any answer, although I find it very interesting and could have many useful applications in statistics :/ Does any one have an idea ? Thank you for your time","['statistical-inference', 'radon-nikodym', 'statistics', 'integration', 'measure-theory']"
1897811,"Find the spectrum of compact operator ""min""","Consider the (compact) operator $T:C([0,1],\mathbb{R})\rightarrow C([0,1],\mathbb{R})$ s.t.
\begin{equation}
T(f)(x)=\int_0^1\min\{x,y\} f(y)dy \; .
\end{equation}
How could one find its spectrum? I have tried to impose $Tf-\lambda f \equiv 0$ but then I couldn't solve anything...","['eigenvalues-eigenvectors', 'real-analysis', 'compact-operators', 'functional-analysis', 'spectral-theory']"
1897819,One-point subset of a topological space,Proposition: Let X be a non-empty set and let $\mathbf{B}$ be the collection of all one-point subsets $\left \{ x \right \}$ of X. Then $\mathbf{B}$ is a basis for the discrete topology on X. My question is embarrassingly trivial but what is a one-point subset of a non-empty set X? Thanks in advance.,"['general-topology', 'elementary-set-theory']"
1897866,The Sums of the Subsets are Distinct,Let $S$ be a set of $n$ positive integers such that no two subsets have the same sum. Prove that $\sum_{a \in S} \frac{1}{a} < 2$. I have found a paper with a nice proof of the statement by S. J. Benkoski and Paul Erdos http://renyi.hu/~p_erdos/1974-24.pdf but I'd still like to see an elementary proof.,"['combinatorics', 'extremal-combinatorics']"
1897919,Counter example or proof that $\kappa(AB) \leq \kappa(A)\kappa(B) $,"I am stuggling to find a counter example or either proof (for general matrices $A, B \in \mathbb{C}^{m \times n}$) that 
$$\kappa(AB) \leq \kappa(A)\kappa(B)\,,$$  where the condition number $\kappa(\cdot)$ is evaluated with respect to any submultiplicative norm $$\|AB\| \leq \|A\|\|B\|$$ and is given by $$
\kappa(A) = \|A\|\|A^\dagger\|\,,
$$
with $A^\dagger$ the Moore–Penrose pseudoinverse of $A$. If the matrices $A, B$ are non singular the result can be easily proved, indeed $$
\|AB\|\|(AB)^\dagger\| = \|AB\|\|(AB)^{-1}\| = \|AB\|\|B^{-1}A^{-1}\|\leq \|A\|\|A^{-1}\|\|B\|\|B^{-1}\| = \kappa(A)\kappa(B)\,. 
$$ But what about other cases? Added later: Similar problem is considered in this question , but no answer to a more general setting is given there.","['numerical-linear-algebra', 'matrices', 'matrix-decomposition', 'pseudoinverse', 'numerical-methods']"
1897921,Why does the eigenvalue $-1$ have even multiplicity for all the conjugacy classes in these representations?,"I have been conducting some experiments with GAP and have noticed a pattern which I would like to disregard as a coincidence or explain theoretically. Consider a finite group $G$ and a representation of $G$ over $\mathbb{Q}$. Let $C$ be a conjugacy class in $G$. For any $g,h\in C$, the characteristic polynomials of the associated matrices in the representation are the same, so we can write $E_C$ for the associated list of eigenvalues. For many groups I have tried, with the aid of GAP, the multiplicity $m(E_C)$ of the eigenvalue $-1$ always seems to be even for all conjugacy classes $C$. I cannot see if that is just a coincidence or not. I have used a variety of representations available in the Atlas, for example, http://brauer.maths.qmul.ac.uk/Atlas/v3/clas/U42/ . For the representation of dimension $15$ of the unitary group $U_4(2)$ listed there, $m(E_C)\in\{0,2,4,8\}$. For a representation of dimension $6$ of the alternating group $A_7$, $m(E_C)\in\{0,2\}$. For a representation of dimension $11$ of the linear group $L_2(11)$, $m(E_C)\in\{0,2,6\}$. I have experimented with a number of other groups, with similar results, but cannot explain the persistence of even multiplicities.","['matrices', 'representation-theory']"
1897937,"Example of a Banach algebra with identity $e$ such that $\|e\| = t$, where $t \geq 1$.","There is the following result: Suppose $X$ is a normed algebra with identity $e$. Then $\|e\| \geq 1$. I am looking for an example to show that even a Banach algebra $X$ with identity $e$ not necessarily satisfies $\|e\| = 1$. A general example such that $\|e\| = t$, where $t \geq 1$ would be even more interesting. I am aware of several examples of Banach algebras with identity, but these all have an identity with norm $1$. As algebras have exactly one identity, I think I cannot use the well-known Banach algebras as an example. Any help or comment is highly appreciated.","['functional-analysis', 'banach-algebras']"
1897942,What is the algebraic intuition behind Vieta jumping in IMO1988 Problem 6?,"Problem 6 of the 1988 International Mathematical Olympiad notoriously asked: Let $a$ and $b$ be positive integers and $k=\frac{a^2+b^2}{1+ab}$. Show that if $k$ is an integer then $k$ is a perfect square. The usual way to show this involves a technique called Vieta jumping . See Wikipedia or this MSE post . I can follow the Vieta jumping proof, but it seems a bit strained to me. You play around with equations that magically work out at the end. I don't see how anyone could have come up with that problem using that proof. Is there a natural or canonical way to see the answer to the problem, maybe using (abstract) algebra or more powerful tools? In addition, how can someone come up with a problem like this?","['square-numbers', 'induction', 'contest-math', 'group-theory', 'elementary-number-theory']"
1897945,Existence of an infinite group with finitely many conjugacy classes,"Is it possible to prove (in a simple way, maybe non-constructive) the existence of an infinite group with finitely many conjugacy classes, or to build a simple example? I am aware of this question, but it doesn't help me. This one provides an example, but it is a bit complicated in my opinion; moreover the question asked for an infinite group with exactly two many conjugacy classes, which is more restrictive than my question. Thank you!","['alternative-proof', 'abstract-algebra', 'examples-counterexamples', 'group-theory']"
1897970,Does the union of two not disjoint open balls always contain the line connecting the two centers?,"Maybe I have been drawing wrong, but the intuition I got from drawing 2d circles suggests the above statement may be correct. I have yet to come up with a proof in $\mathbb{R}^n$ or maybe normed or metric spaces. Not sure about the most general settings that allows for the above statement to hold, I think I will have to restrict the statement to Hilbert-Spaces because it is easy to draw counter-examples in other norms. The crux seems to be that for two not disjoint open balls $B_{r_1}(x)$ and $B_{r_2}(y)$ in norms other than 2, the line $\overline{xy}$ does not need to cross the intersection $B_{r_1}(x) \cap B_{r_2}(y)$. So I think the statement I want to prove is this: Let $H$ be a Hilbert space and $x,y \in H$, $r_1, r_2 > 0$. If $B_{r_1}(x) \cap B_{r_2}(y) \neq \emptyset$ then $$\overline{xy} \cap (B_{r_1}(x) \cap B_{r_2}(y)) \neq \emptyset.$$ Does the above statement maybe characterize a certain type of geometric object? I am thinking of a statement like: $A$ a subset s.t. the union of $A$ with a subset $B$ contains a line connecting their ""center of mass"".","['functional-analysis', 'banach-spaces', 'hilbert-spaces', 'geometry']"
1897971,The conjugate of a sylow $p$-subgroup is a sylow $p$-subgroup,Second Sylow theorem states that all Sylow $p$ -subgroups are conjugate. But reviewing my proof it seems to me that we also prove that all the conjugates of a Sylow $p$ -subgroup are Sylow $p$ -subgroups. I can include the proof if needed but can anybody confirm me this idea? Or give a counterexample?,"['abstract-algebra', 'group-theory', 'sylow-theory']"
1897975,Uniqueness of the solution of an ODE with zero integral mean and convex $f$,"Consider the problem on $(0,1)$
  $$u(x)'=f(u(x))$$
  $$\int_0^1u(x)dx=0$$
  where $f$ is $C^1$ on the whole real line, strictly convex, has its minimum in $0$, and $f(0)<0$.
  Prove that the solution is unique. I tried two ways: to reduce the problem to a Cauchy problem, showing that the condition on the mean, together with the properties of $f$, implies some 'initial-value' condition; or to apply the contraction principle (i.e. showing that the Fredholm operator associated to $f$ is a contraction). But I have not been able to follow any of these ways. Can someone give a help, also explaining, if possible, useful strategies to apply in similar cases (i.e. 'modified' Cauchy problems)? Thank you in advance.","['ordinary-differential-equations', 'convex-analysis']"
1897976,What is the intuition behind a function being differentiable if its partial derivatives are continuous?,"I have seen the proof of it, but I can't quite intuitively understand why continuous partial derivatives imply that a function is differentiable. Thanks! EDIT: The only reason I can think of is that if the partials exist at a point and are continuous at a neighborhood around that point, then it is only logical that the tangent plane to that point exists because the equation for the tangent plane at that point would be a good approximation of the function around that point. If the partials exist at the point but are not continuous at a neighborhood around that point, then I can't see how a tangent plane at the point can exist because would not be a good approximation of the function near the point since the function would have discontinuous slope along certain direction(s)(by the definition of a partial derivative). Is my intuition right?","['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'derivatives']"
1897990,Determinant of a linear transformation $T.$,"Let $M$ be the real vector space of $2\times 3$ matrices with real entries. Let $T:M \rightarrow M$ be defined by  $$T(\begin{bmatrix}
    x_{1} & x_{2} & x_{3}  \\
    x_{4} & x_{5} & x_{6}  \\
\end{bmatrix})=\begin{bmatrix}
    x_{6} & x_{4} & x_{1}  \\
    x_{3} & x_{5} & x_{2}  \\
\end{bmatrix}$$ Then the determinant of $T$ is $A. 1.$ $B. 2.$ $C.-1.$ $D.0.$ Now one method is as usual  firstly find the matrix of the transformation $T$ with respect to usual basis of $W$ and then find the determinant of the matrix thus obtained which come as $-1.$ But i want some short trick that gives the determinant in less time. One more thing is that matrix of $T$ with respect to usual basis is an Orthogonal matrix. Please give some suggestion. Thank you.","['matrices', 'linear-algebra']"
1898007,$(a+2)^3+(b+2)^3+(c+2)^3 \ge 81$ while $a+b+c=3$,"I need to show that $(a+2)^3+(b+2)^3+(c+2)^3 \ge 81$ while $a+b+c=3$ and $a,b,c > 0$ using means of univariate Analysis. It is intuitively clear that $(a+2)^3+(b+2)^3+(c+2)^3$ is at its minimum (when $a+b+c=3$) if $a,b,c$ have ""equal weights"", i.e. $a=b=c=1$. To show that formally one can firstly fix $0<c \le 3$, introduce a variable $0\le\alpha\le1$ and express $a$ and $b$ through $\alpha$ $$a= \alpha(3-c)$$ $$b=(1-\alpha)(3-c)$$ Then we minimize $(a+2)^3+(b+2)^3+(c+2)^3$ w.r.t $\alpha$ treating $c$ as a parameter. We get $\alpha=\frac{1}{2}(3-c)$. Further we maximize $(a+2)^3+(b+2)^3+(c+2)^3$ one more time w.r.t. $c$. That is quite tedious. I am wondering whether their is a more elegant way. Probably using idea of norms. Basically we need to show that $\lVert(a,b,c)+(2,2,2)\rVert_3 \ge (81)^{1/3}$ while $\lVert(a,b,c)\rVert_1=3$ and $a,b,c>0$.","['real-analysis', 'inequality', 'optimization', 'jensen-inequality']"
1898010,How did Newton find derivative of basic functions before formulating systematic Calculus?,I think I read somewhere that Newton tried to find derivatives of basic functions like $x^2$ before formulating systematic calculus; how did/would he do it?,"['derivatives', 'math-history', 'calculus']"
1898028,Summation calculus differentiation problem,I am new in calculus. So it will be helpful if anyone solve it and give some hint how it works. $$E = \frac{1}{3} \sum_{i=1}^3 (mx_i+c-y_i)^2$$ Then what will be $\frac{dE}{dm}$ and $\frac{dE}{dc}$? Thanks.,"['derivatives', 'summation', 'calculus']"
1898103,What's the correct categorical definition of a topological submersion?,"Topological embeddings are regular monos in the category of topological spaces. Topological immersions are arrows which are locally embeddings, i.e it's possible to restrict to an open cover such that each restriction is a topological embedding. What about topological submersions? What's their categorical definition? The symmetric definition would be maps that are locally regular epimorphisms. The regular epimorphisms of the category of topological space are quotient maps. This answer notes surjective smooth submersions possess the smooth analogue of the universal property of quotient maps, but are not characterized by it (not all arrows possessing this universal property are surjective smooth submersions). The nlab page lists the general definition as an arrow such that every point of the domain is covered by a local section. This is discussed in this MO question but I can't make much out of it. Since the tangent-space definitions of smooth immersions and submersions are dual, I was hoping for something along those lines in the topological case. What's the correct definition?","['category-theory', 'general-topology', 'differential-geometry', 'manifolds']"
1898122,Can we prove this geometry question without too much calculation?,"Can we prove the following geometry question without too much calculation? $ABCD$ is a square, $AC=AE$, $DF=DE$, $AC$ and $BF$ meet at $G$, $BG=CE$. Prove: $BG \parallel CE$.","['euclidean-geometry', 'geometry']"
1898123,Losing information when constructing a quotient group,"I am trying to self-study abstract algebra but I am having some difficulty with quotient groups. I understand cosets and the importance of using normal subgroups as they serve as the kernel of a homomorphism. What I fail to grasp intuitively is when books discuss ""losing information"" in the construction of a quotient group. I know that the subgroup you mod out by serves as the identity in the newly-constructed quotient group but how exactly are we losing information? 
I'm sure this is a very basic question and I'm overlooking something very simple but I would appreciate your help in understanding this.","['abstract-algebra', 'group-theory']"
