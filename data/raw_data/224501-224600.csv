question_id,title,body,tags
4620701,How to find min of $\sqrt{x^2+4x+8}+\sqrt{x^2-6x+10}$ without differentiation,"I want to find minimum of $$\sqrt{x^2+4x+8}+\sqrt{x^2-6x+10}$$ without differentiation.
Although I did it with differentiation and $x=4/3$ then find $f(\frac 43)$ but I am unable to find the min without differentiation. Implicit : I did this $$\frac{x^2+4x+8-(x^2-6x-10)}{\sqrt{x^2+4x+8}-\sqrt{x^2-6x+10}}$$ but get nothing, because the $\sqrt{x^2+4x+8}+\sqrt{x^2-6x+10}>5$ now, If someone help me find it, or gives me a clue I appreciate this.","['maxima-minima', 'calculus', 'functions', 'optimization', 'algebra-precalculus']"
4620706,"What is maximum number of circles in $\mathbb R^2$ that intersect a common circle, but are otherwise disjoint?","Given a circle of a fixed radius in the plane, how many circles can be drawn intersecting with that circle, that are disjoint from each other? (If this type of question has a name or has been studied in the past, feel free to let me know! It seems vaguely related to sphere-packing stuff, but not exactly) More precisely, let $S^1 \subset \mathbb R^2$ be the unit circle entered at the origin. What is the maximum number of circles $S_i$ of radius $1$ such that $S_i \cap S^1 \neq \emptyset$ for all $i$ , and $S_i \cap S_j = \emptyset$ for all $i\neq j$ ? I spent some time drawing examples and started with squares, since they are a lot easier to draw. I suspect the answer is 5 for squares, but I'm not totally sure, and would like to know how to go about proving this rigorously. Remark (edit): I also know about doubling dimension , which may have some role to play in the answer (emphasis on ""may""!) but of course the definition is quite dissimilar from my problem is that it uses balls of radius $r/2$ , and does not care whether the covering balls intersect, though maybe the doubling constant gives a weak upper bound or something. (Brian below thinks it is $7$ .) Remark 2: Since I forgot about rotations, my answer of 5 for squares was not correct. Coincidentally, it ended up being correct for circles (maybe indirectly because I was drawing squares with the same orientation). At any rate, if anyone can find the answer for squares as well, feel free, or maybe I should post as a separate question.","['euclidean-geometry', 'circles', 'geometry']"
4620708,Prove that $A \triangle B = A \setminus B \iff B \subseteq A$,"I think I've proved this, but am unsure if it is correct. (I'm self-studying through a ""transitions"" book). Here, the $\triangle$ is the symmetric difference . Lemma : For two sets $A$ and $B$ , if $B \setminus A = \emptyset$ then $B \subseteq A$ . Proof : Let $x \in B$ . Since, by the antecedent, $B \setminus A = \emptyset$ , we know that it is not the case that $x \notin A$ , so we must have $x \in A$ . Since $x$ was an arbitrary element of $B$ , we can conclude that $B \subseteq A$ . Theorem : For sets $A$ and $B$ , $A \triangle B = A \setminus B \iff B \subseteq A$ Proof : ( $\rightarrow$ ) By definition, $A \triangle B = (A \setminus B) \cup (B \setminus A)$ . By the antecedent, then, $(A \setminus B) \cup (B \setminus A) = (A \setminus B)$ . We showed earlier (in the book) that for two sets $X, Y$ : $X \cup Y = X \iff Y \subseteq X$ , so this means $B \setminus A \subseteq A \setminus B$ . However, let $x \in B \setminus A$ , which implies $x \in A \setminus B$ . This cannot happen because an element cannot be in both $A$ and $A^C$ , so we conclude that $B \setminus A = \emptyset$ . By the Lemma above, then $B \subseteq A$ . ( $\leftarrow$ ) Since $B \subseteq A$ , we know that $A \cup B = A$ and $A \cap B = B$ . By the definition of symmetric difference, $A \triangle B = (A \cup B) \setminus (A \cap B) = A \setminus B$ .","['elementary-set-theory', 'solution-verification']"
4620734,Derivative of $(2x^2+3)^2(x^3-2x)^4$,"I want to know if there's another method in shorter way. I came up to this problem Find the $f'(x)$ $$
f(x) = (2x^2+3)^2\cdot(x^3-2x)^4
$$ Applying product rules $$
\frac{d}{dx}f(x)g(x) = f(x)\frac{d}{dx}g'(x) + g'(x)\frac{d}{dx}f(x)
$$ Solve: $$ f = (2x^2+3)^2 $$ $$ g = (x^3-2x)^4$$ $$
\frac{d}{dx}(2x^2+3)^2(x^3+2x)^4 + \frac{d}{dx}(x^3-2x)^4(2x^2+3)^2
$$ $$
2(2x^2+3) 4x(x^3-2x)^4 + 4(x^3-2x)^3(3x^2-2)(2x^2+3)^2
$$ $$
8x(2x^2+3)(x^3-2x)^4 + 4(x^3-2x)^3(3x^2-2)(2x^2+3)^2
$$ $$
8x(2x^2+3)(x^3-2x)^4 + (x^3-2x)^3(12x^2-8)(2x^2+3)^2
$$ Answer: $$
f'(x) =8x(2x+3)(x^3-2x)^4 + (x^3-2x)^3(12x^2-8)(2x^2+3)^2
$$ Is this correct way or are there other methods? Thanks in advance.","['calculus', 'derivatives']"
4620765,An example of non-ergodicity (Birkhoff–Khinchin theorem),"Let $\{Y_t\}_{t\in \mathbb Z}$ be a stationary process with mean zero. We know that the autocovariance function is given by: $$\gamma_Y(h)= cov(Y_0, Y_{h})= E[Y_0\,Y_h]$$ We say that $\{Y_t\}_{t\in \mathbb Z}$ is ergodic for the second moments if \begin{equation}\label{ergo-1}\tag{E}
    \hat{\gamma}_Y (h):= \frac{1}{T-h} \sum_{t=h+1}^T Y_t Y_{t-h} \overset{p}{\to} \gamma_Y(h),\quad \forall h >0 \quad (T \to \infty).
\end{equation} According to slide 48 of these lectures notes , a sufficient conditions for the ergodicity for the second moments is: \begin{equation}\label{ergo0}\tag{CE}
\sum_{h=0}^\infty |\gamma_Y(h)| < \infty
\end{equation} Now, let $\{X_t\}_{t\in \mathbb Z}$ be a stationary process with mean zero satisfying (\ref{ergo0}): $$\sum_{h=0}^\infty |\gamma_X(h)| < \infty$$ (consequently, $\{X_t\}_{t\in \mathbb Z}$ is ergodic for the second moments)
Moreover, Let $N \sim \hbox{Poisson}(\lambda)$ . Consider the same Poisson compounding process of this question : $$Y_t = \sum_{j=1}^N X_{t;j}, \quad (t \in \mathbb Z)$$ Using this other question , we have that: \begin{equation}\label{ac}\tag{ACV}
\gamma_Y(h) =  \lambda E[X_0X_h]= \lambda \gamma_X(h)
\end{equation} So, by the sufficient conditions for the ergodicity for the second moments, we have: $$\sum_{h=0}^\infty |\gamma_Y(h)| = \lambda \sum_{h=0}^\infty |\gamma_X(h)| < \infty$$ This would show that $\{Y_t\}_{t\in \mathbb Z}$ is ergodic. However, following the same technique as the answer to the question already mentioned above, we can show that: $$  \hat{\gamma}_Y (h) \overset{p}{\to}  N \gamma_X(h), \quad (T \to \infty)$$ So, we would Not have that $\{Y_t\}_{t\in \mathbb Z}$ is ergodic ( see \ref{ergo-1} and \ref{ac} ), which would be a contradiction. What's happening?","['stochastic-processes', 'ergodic-theory', 'probability-theory', 'asymptotics']"
4620805,"Solving a system of $2n-2$ equations, what is the relationship of solutions between successive $n$?","I am trying to solve a system of $2n-2$ equations.  The first two rows of the matrix representation are always $$\begin{pmatrix}-8&1&1&0&\ldots& 0\end{pmatrix} \text{and}\begin{pmatrix}-1&-7&2&1&0&\ldots& 0\end{pmatrix}.$$ The bottom rows are always $$\begin{pmatrix}0&\ldots& 0&1&2&-6&2\end{pmatrix} \text{and}\begin{pmatrix}0&\ldots& 0&1&2&-6\end{pmatrix}.$$ The rows inbetween are $\begin{pmatrix}1&2&-6&2&1&0&\ldots& 0\end{pmatrix}$ and shifting over one to the right as you go down.  In my particular problem $x_1$ is always $1$ so that’s why there’s only $2n-2$ variables. For example, for $n=4$ , the matrix equation is: $$
M_4\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0   \\
  1 &-7&  2&  1 & 0 & 0   \\
  1 & 2 &-6 & 2 & 1 & 0   \\
  0 & 1 & 2 &-6 & 2 & 1   \\
  0 & 0 & 1 & 2 &-6 & 2  \\
  0 & 0 & 0 & 1 & 2 &-6 \\
\end{pmatrix}_{6\times6}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\end{pmatrix}
$$ For $n=5$ $$
M_5\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
  1 &-7&  2&  1 & 0 & 0 & 0 & 0 \\
  1 & 2 &-6 & 2 & 1 & 0 & 0 & 0 \\
  0 & 1 & 2 &-6 & 2 & 1 & 0 & 0 \\
  0 & 0 & 1 & 2 &-6 & 2 & 1 & 0\\
  0 & 0 & 0 & 1 & 2 &-6 & 2 & 1\\
  0 & 0 & 0 & 0 & 1 & 2 &-6 & 2\\
  0& 0 & 0 & 0 & 0 & 1 & 2 &-6\\
\end{pmatrix}_{8\times 8}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\\x_9\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\\g\\h\end{pmatrix}
$$ My particular problem is that I want to solve thousands (possibly millions) of such systems but it is computationally demanding.  You’ll notice that you can step down - the $6\times6$ is simply the $8\times8$ matrix with the last two rows and columns removed.  Similarly, you can step up by extending the previous matrix with two rows and columns. The core of my question is this: is there some relationship between the inverse of $M_n$ and $M_{n+1}$ that I can utilize to make solving more efficient? Specifically, I am interested in $x_n$ for each $n$ if it is more efficient to only calculate that.","['matrices', 'systems-of-equations', 'linear-algebra']"
4620818,Colimit of symmetric groups,"I don't yet know much about categorical limits and colimits, I have just started learning about them, and so I wanted to experiment a bit with this concept. And to that end, my first natural attempt was the following. Let $S_n$ denote the symmetric group on n elements. Denote by $\iota_n: S_n\to S_{n+1}$ the map which sends a permutation of $[n]$ onto the associated permutation of $[n+1]$ leaving the element $n+1$ fixed. We can stick all of these maps together and then ask about the colimit of the diagram we obtain this way. My natural instinct is that it should be the symmetric group on the integers, and the maps are those which send a permutation $\tau\in S_n$ to the permutation of $\mathbb{N}$ fixing all elements bigger than $n$ and acting on $[n]\subset\mathbb{N}$ in the natural way. Showing that this is a valid candidate for the colimit is easy, but I don't know how to show that it is in fact the colimit. I have also begun wondering if maybe this isn't the colimit, but I don't see any other obvious candidate. Any help would be appreciated, thanks.","['permutations', 'group-theory', 'limits-colimits', 'category-theory']"
4620826,Find the value of $\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right)$,"Find the value of the expression $$\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right)$$ where $\alpha$ is a positive number. It is my first time seeing a question like this. Normally, we just put $n=\infty$ in the summand which is same as calculating the series till infinite terms. But here it is different. The variable in the bound is also in the expression, meaning $n=\infty$ in the summand is not applicable in this case. I expanded the series as $$\frac{1}{n+1}+\frac{1}{n+2^{\alpha}}+\cdots$$ But had no luck. Any help is greatly appreciated.","['limits', 'sequences-and-series', 'real-analysis']"
4620832,Solving $\log_2(x+1)+x=2$,Here's the equation I'd like to solve: $$\log_2\left(x+1\right)+x=2$$ Now I am aware that there's only one solution to the equation by graphing $y=-x+2$ and $y=\log_2\left(x+1\right)$ . The question is: how do you know the solution is $x=1$ ? Is there any other appraoch besides guessing? Please note that I know how to prove there's only one solution. What I'd like to know is the process of finding the solution $x=1$ .,"['algebra-precalculus', 'closed-form', 'logarithms']"
4620879,"I came across this fun pattern, can anyone give me a proof for this?","The Pattern $\begin{align*}1(8)&=(3^2)-1\\
2(8)&= (3+1)^2 \\
3(8)&= (3+2)^2-1\\
4(8)&= (3+3)^2 - 4\\
5(8)&= (3+4)^2- 9\\
6(8)&= (3+5)^2-16\end{align*}$ Conjecture I think the pattern is that the numbers appear in the following form: $$8(n+1) = (3+n)^2 - (n-1)^2$$ Please correct me if I'm wrong, I'm not really that good at math.",['algebra-precalculus']
4620929,Is there a quartic polynomial in two variables that have multiple local minima and no other critical points?,"Question. Can a degree 4 polynomial $p:\mathbb R^2 \to \mathbb R$ have $N\geq 2$ local minima and no other critical points? I got this question when trying to answer: How many strict local minima a quartic polynomial in two variables might have? A closely related question asks: If a two variable smooth function has two global minima, will it necessarily have a third critical point? Among the answers @RiverLi gives an example of the function $$
f(x,y)=(x^2-1)^2+(x^2y-x-1)^2, \tag{*}\label{RL}
$$ which is a polynomial, but of degree 6. Notice that polynomials like \eqref{RL}  can be written as $p(x,y)=u(x,y)^2 + v(x,y)^2$ . Since we need $p$ to be degree 4 polynomial, $u$ and $v$ must be quadratic polynomials. We need there to be a set $P\subset \Bbb R^2, |P|=N$ , of points such that: $(x,y)\in P \ \ \Longleftrightarrow \ \ u(x,y)=v(x,y)=0$ ; $(x,y)\in P \ \ \Longleftrightarrow \ \ u_x u + v_x v = u_y u + v_y v =0$ when evaluated at $(x,y)$ . For what values of $N\geq 2$ is there a set $P\subset \Bbb R^2, |P|=N$ and quadratic polynomials $u,v:\Bbb R^2 \to \Bbb R$ satisfying conditions 1 and 2? My guess is that for $N=2$ at least one of the polynomials $u,v$ has to be of the third order as in \eqref{RL}. However, it is possible that there would be desirable $u,v$ for $N=3$ or $N=4$ . Note that by Bézout's theorem condition 1 can not be satisfied at more than 4 points given that $u,v$ are quadratic. Alternatively, is there another form than $p=u^2+v^2$ that could represent a quartic polynomial $p$ with $N\geq 2$ local minima and no other critical points? Disclaimer. I asked about the special case of $N=2$ on https://math.codidact.com .","['roots', 'real-analysis', 'maxima-minima', 'polynomials', 'differential-topology']"
4620939,Finding all four angles in a quadrilateral given four sides and its area,"I am creating a small investigation for my students and I'd like to check whether my mark scheme is correct. In one of the questions, students are asked to find the angles of a quadrilateral with sides $27.4$ , $27.8$ , $27.75$ and $29.1$ knowing also that the area is $780$ (image below). Possible solution By splitting the quadrilateral into two triangles using the diagonal $AD$ , we can build two equations: one that, through the cosine rule states that the diagonal is equal whether you use triangle $1$ or $2$ , and the other that takes into account that the area is $780$ : $$0.5 \cdot 29.1 \cdot 27.75 \cdot \sin x+0.5 \cdot 27.8 \cdot 27.4 \cdot \sin y=780$$ $$29.1^2+27.75^2-2 \cdot 29.1 \cdot 27.75 \cdot \cos x=27.4^2+27.8^2-2 \cdot 27.4 \cdot 27.8 \cdot \cos y$$ The thing about this method is that it is very hard to find the solution to this linear system. The way that I did that was to input it on Wolfram, which gave me the following result: This means that angle x is approximately $94.0979^\circ$ (the other angles can be found using a similar approach). The link to this animation in Geogebra can be found here . Question: Is there a simpler way that this can be done (the main problem is solving that system)? Assume that the students will have a calculator and access to Geogebra. Update A small update with the two possible solutions:","['algebra-precalculus', 'geometry']"
4620954,Is the $n/2$-th heat kernel coefficient topological?,"Let $M$ be an $n$ -dimensional manifold, with $n$ even and consider the heat kernel of the Laplacian on $M$ : $$K_M(t) := \mathrm{Tr}\Big(\mathrm{e}^{-t \Delta_M}\Big) \equiv \sum_{\lambda\in\mathrm{spec}(\Delta_M)} \mathrm{e}^{-t \lambda}.$$ The heat kernel admits an asympotic small- $t$ expansion as $$K_M(t) = \frac{1}{(4\pi t)^{n/2}}\sum_{k=0}^\infty b_k(M) t^k,$$ where $b_k(M)$ are known as the heat kernel coefficients and are given by integrals of local geometric data of $M$ . I am particularly interested in what I'll call middle-dimensional heat kernel coefficient , namely $$ b_{n/2}(M) = b_{\mathrm{dim}M/2}(M).$$ This coefficient is interesting because it is closely related to the value of the Minakshisundaram-Pleijel zeta function (i.e. the spectral zeta function for the Laplacian) at zero. Is an explicit formula for the calculation of $b_{n/2}(M)$ known? In particular, is it a topological invariant? It seems to me that it wants to be related to the number of connected components of $M$ or to some sum of Betti numbers of $M$ , but I haven't managed to show it. (If anyone feels brave, I'd be curious about how it generalizes to $p$ -forms; namely what's the middle-dimensional heat kernel coefficient associated with the Laplacian, $\Delta_p = \mathrm{d}\delta + \delta \mathrm{d}$ , acting on $p$ -forms on $M$ )","['harmonic-analysis', 'laplacian', 'partial-differential-equations', 'spectral-theory', 'differential-geometry']"
4620958,"Find the inverse of $(\cos t,\sin t,t)$","Let $c:\mathbb{R} \rightarrow \mathbb{R}^3$ with: $$ c(t)=(\cos t,\sin t,t) $$ Check if $c:\mathbb{R} \rightarrow c(\mathbb{R})$ is invertible and if so, is the inverse continuous? My attempt: Obviously the fuction is a bijection, so let's find the inverse. So to find $c^{-1}(x,y,z)=t$ ,  comes down to solving the system: $$ \begin{cases} \cos t=x \\ \sin t=y \\ z=t  \end{cases}$$ So, $$x^2+y^2+z^2=1+t^2 \Rightarrow t= \pm \sqrt{x^2+y^2+z^2-1}.$$ I think this is not right, Any ideas?","['multivariable-calculus', 'calculus', 'systems-of-equations', 'inverse']"
4620989,Is this the Axiom of Infinity?,"While studying elementary Set Theory, I came across the Axiom of Infinity. This comes before the book introduces ZFC, so I'm not convinced that it is necessarily the same as the typical definition. My notes use the following definition (edit to add in the logical not $\lnot$ symbol in order to make the statement correct as pointed out by Eric Wofsey in the comments): Weak Axiom of Infinity: $\small \exists x \lnot (\forall w (( \exists y \space y \in w \land \forall y (y \in w \rightarrow \forall z (z \in y \rightarrow z \in x ))) \rightarrow$ $\exists y (y \in w \land \forall v (( v \in w \land \forall z (z \in y \rightarrow z \in v )) \rightarrow y = v )))$ On the other hand, the more standard notion of the Axiom of Infinity that I have come across (whilst attempting to understanding the (difficult to follow) definition above, is very different. Wikipedia uses the following notion: Axiom of Infinity: $\exists I (\emptyset \in I \land \forall x \in I ((x \cup $ { $x$ } $) \in I ))$ I'm struggling to properly understand the first definition well enough to know if these are essentially saying the same thing. If they are different in some way, then what is the difference and why is this difference present?","['elementary-set-theory', 'axioms', 'set-theory', 'terminology']"
4620996,Decomposition of tensor field on hypersurface,"Lets consider the following situation: $(\mathcal{M},g)$ is a Lorentzian manifold, which is globally of the form $\mathcal{M}\cong I\times\Sigma$ , where $I\subset\mathbb{R}$ and $\Sigma$ is some $3$ -dimensional hypersurface. Furthermore, let $T$ be an arbitrary symmetric $2$ -tensor field on $\mathcal{M}$ . Then, in some coordinates, I can decompose $$T_{\mu\nu}\mathrm{d}x^{\mu}\otimes\mathrm{d}x^{\nu}=T_{00}\mathrm{d}x^{0}\otimes\mathrm{d}x^{0}+2T_{i0}\mathrm{d}x^{0}\otimes\mathrm{d}x^{i}+T_{ij}\mathrm{d}x^{i}\otimes\mathrm{d}x^{j}.$$ This essentially provides a decompisition of the form (in the sense of $C^{\infty}(\mathcal{M})$ -module isomorphisms) $$\Gamma^{\infty}(T^{\ast}\mathcal{M}^{\otimes_{s}2})\cong C^{\infty}(\mathcal{M})\oplus\Gamma^{\infty}(\mathcal{M},T^{\ast}\Sigma)\oplus\Gamma^{\infty}(\mathcal{M},T^{\ast}\Sigma^{\otimes_{s}2})$$ where $T_{00}$ is interpreted as a function in $C^{\infty}(\mathcal{M})$ , $T_{i0}$ of beeing the components in coordinates of an element in $\Gamma^{\infty}(\mathcal{M},T^{\ast}\Sigma)$ and similarely for $T_{ij}$ . Now, is it possible to establish isomorphisms $$C^{\infty}(\mathcal{M})\cong C^{\infty}(I,C^{\infty}(\Sigma))$$ $$\Gamma^{\infty}(\mathcal{M},T^{\ast}\Sigma)\cong C^{\infty}(I,\Gamma^{\infty}(T^{\ast}\Sigma))$$ $$\Gamma^{\infty}(\mathcal{M},T^{\ast}\Sigma^{\otimes_{s}2})\cong C^{\infty}(I,\Gamma^{\infty}(T^{\ast}\Sigma^{\otimes_{s}2}))$$ i.e. to interpret $T_{00}$ , $T_{i0}$ and $T_{ij}$ as ""smooth time-dependent functions, co-vector and tensor fields"", respectively? What I mean with smooth in the notation above is the following: If $X:I\to\Gamma^{\infty}(T^{\ast}\Sigma)$ is a map, for example, then $X_{p}(t)\in T_{p}^{\ast}\Sigma$ for all $p\in\mathcal{M}$ , $t\in I$ , and hence, it makes sense to define derivative with respect to $t$ in the usual way, i.e. $$\frac{\mathrm{d}}{\mathrm{d}t}X_{p}(t)=\lim_{h\to 0}\frac{X_{p}(t+h)-X_{p}(t)}{h}$$ where we choose an arbitrary norm on $T_{p}^{\ast}\Sigma$ , for example, the one induced by the metric $g$ (the choice does not matter, since on finite-dimensional spaces they are all equivalent). Similarely, the spaces are defined for the other two cases.","['riemannian-geometry', 'tensors', 'manifolds', 'limits', 'differential-geometry']"
4621017,Lower bound for the Gaussian tail,"I am asked to prove the following lower bound for the normal tail. Let $X\sim N(0,1)$ $$P(X\geq a)\geq c\exp\left(-a-\frac{a^{2}}{2}\right)$$ for some $c>0$ . To do this , as a hint I am asked to find the density of $N(0,1)$ with respect to $N(a,1)$ which I have computed as $\exp\left(-xa+\frac{a^{2}}{2}\right)$ and then asked to show that $$ \bigg(F(t)-F(0)\bigg)\exp\left(-a(t+a)+\frac{a^{2}}{2}\right)\leq P(X\geq a)$$ for all $a,t>0$ . Where $F$ is the the cdf of a standard Gaussian. So I would have to make $t=1$ . But I am unable to show the above inequality. How do I use the density? For example if $\mu\sim N(a,1)$ then the LHS is $$\displaystyle\int_{0}^{t}\exp\left(-xa+\frac{a^{2}}{2}-a^{2}-at-\frac{a^{2}}{2}\right)\,d\mu(x) = \int_{0}^{t}\exp(-a(x+t))\,d\mu(x)$$ . How do I now go from here to $\int_{a}^{\infty}\exp\left(-xa+\frac{a^{2}}{2}\right)\,d\mu(x)=P(X\geq a)$ ?","['probability-distributions', 'probability-theory', 'probability']"
4621051,Expectation of the stopping time for certain random walk,"Consider a random walk on $\mathbb Z^2 \subset \mathbb C$ starting from the orgin $(0,0)$ . At each step, it randomly go one of the $4$ directions with length $1$ (i.e. $\pm 1, \pm \sqrt{-1}$ ). It stops if it hits one of the $4$ points $(\pm 5 ,\pm 5)$ in the first time. The question is, what is the expectation of the stopping time? I can write down explicitly the number of paths of hitting any given point at the $k$ -th step. Then the expectation can be written down as a summation of infinite sequences. However, I wonder if there is a simple expression, and I would like to know if there is a more elegant approach to this. Thanks in advance!","['expected-value', 'stopping-times', 'random-walk', 'probability']"
4621053,"$\lim_{(x,y)\to (0,0)}\frac{1-(\cos x)(\cos y)}{x^2+y^2} $","I need to find the limit for $$\lim_{(x,y)\to (0,0)}\frac{1-(\cos x)(\cos y)}{x^2+y^2} $$ whether exist. I use many example (ex:line, interated limit, half angle formula, ...), and I always get the answer $1/2$ . However, this does not mean the limit is $1/2$ . Thus, I want to use definition to show that limit in fact be $1/2$ . Unfortunately, I tried many times, and using the inequality I know to show, I failed. Can someone give me a useful inequality to solve it ? Or, point out that I miss somewhere, and show that the limit does not exist.
Thanks.","['multivariable-calculus', 'limits', 'calculus']"
4621113,What is the largest/least value of the function $f$ in $\mathbb{R}^2$?,"What is the largest/least value of the function $f=x^2ye^{-x^2-2y^2}$ in $\mathbb{R}^2$ ? We have that $f'_x=y\left(2e^{-x^2-2y^2}x-2e^{-x^2-2y^2}x^3\right)$ and $f'_y=x^2\left(e^{-x^2-2y^2}-4e^{-x^2-2y^2}y^2\right)$ . If $x=0$ then $f'_x=f'_y=0$ so $(0,y)$ , where $y\in \mathbb{R}^2$ are stationary points. Further $1-4y^2=0\iff y=+-1/\sqrt{2}$ and $2x-2x^3=0\iff2x(1-x^2)\iff x=0, x=+-1$ so $+-(1,1/\sqrt{2})$ are also stationary points. The values for the stationary points are $f(0,y)=0$ , $f(+-1,1/\sqrt{2})=\frac{e^{-2}}{\sqrt{2}}$ and $f(+-1,-1/\sqrt{2})=-\frac{e^{-2}}{\sqrt{2}}$ . Now how do I prove that $e^{-2}/\sqrt{2}$ is the largest value and $-e^{-2}/\sqrt{2}$ is the smallest? Would be nice with a blueprint that I can use for any function but I guess it doesn't exist. Edit: Not looking for solutions that splits up the $f$ to $x^2e^{-x^2}ye^{-2y^2}$ . I wrote an answer and hopefully someone can check that it is right.","['multivariable-calculus', 'limits', 'calculus', 'optimization']"
4621158,"Is my proof of the multivariate chain rule, rigorous?","Theorem. Let $U$ , $V$ , $W$ be normed linear spaces. Let $\Omega$ be open in $U$ and $\Upsilon$ be open in $V$ . Let $f\colon \Omega\to\Upsilon$ be (Fréchet) differentiable at $c\in\Omega$ and $g\colon \Upsilon\to W$ be differentiable at $f(c)\in\Upsilon$ . Then $g\circ f$ is differentiable at $c$ with $$
D(g\circ f)(c) = Dg(f(c))\circ Df(c)\text.
$$ My proof: Denote $L := Df(c)$ and $M := Dg(f(c))$ . Let $\delta_1 > 0$ be such that for all $y\in B_{\delta_1}(f(c))\cap\Upsilon\setminus\{f(c)\}$ , we have $$
\lVert g(y) - g(f(c)) - M(y - f(c)) \rVert < \epsilon \lVert y - f(c) \rVert.\tag{1}
$$ Since $f$ will be continuous at $c$ , take $\delta_2 > 0$ such that for all $x\in B_{\delta_2}(c)\cap \Omega\setminus\{c\}$ , we have $$
\lVert f(x) - f(c) \rVert < \delta_2
$$ which due to (1) implies that $$
\lVert g(f(x)) - g(f(c)) - M(f(x) - f(c)) \rVert < \epsilon \lVert f(x) - f(c) \rVert.\tag{2}
$$ W.l.o.g., assume that for all $x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\}$ , we have $$
\lVert f(x) - f(c) - L(x - c) \rVert < \epsilon\lVert x - c\rVert.\tag{3}
$$ Now, for a general $x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\}$ , we have $$
\begin{align*}
&\lVert g(f(x)) - g(f(c)) - ML(x - c)\rVert\\[1ex]
& \le \lVert g(f(x)) - g(f(c)) - ML(f(x) - f(c))\rVert \\
& \phantom{abcdefghijklmn} + \lVert M(f(x) - f(c) - L(x - c))\rVert\\[1ex]
& \le \epsilon\lVert f(x) - f(c) \rVert + \lVert M\rVert\, \lVert (f(x) - f(c) - L(x - c))\rVert & \text{by (2)}\\[1ex]
& \le (\epsilon + \lVert M\rVert) \lVert f(x) - f(c) - L(x - c)\rVert  + \epsilon\lVert L(x - c)\\[1ex]
& < (\epsilon + \lVert M\rVert)\epsilon\lVert x - c\rVert +\epsilon\lVert L\rVert\, \lVert x - c\rVert & \text{by (3)}\\[1ex]
&= \epsilon(\epsilon + \lVert M\rVert +\lVert L\rVert)\lVert x - c\rVert.
\end{align*}
$$ And we are virtually done. Question: Do you spot any loose ends in the above proof? By $ML$ , I mean $M\circ L$ . Note that the spaces can be over $\mathbb R$ or $\mathbb C$ (but it's common for all, $U$ , $V$ , $W$ ), and possibly infinite dimensional.","['normed-spaces', 'multivariable-calculus', 'solution-verification', 'derivatives', 'chain-rule']"
4621161,The Weil conjecture involving Betti numbers.,"I know that one of the Weil conjectures says something loosely like the following. If $X$ is a projective variety obtained from a complex projective variety $X_\mathbf{C}$ by ""reduction mod $p$ "", then the $j$ -th Betti number of $X_\mathbf{C}$ is equal to the degree of the polynomial $F_j \in \mathbf{Z}[x]$ , where the $F_j$ are defined by \begin{align}
\prod_{j = 0}^{2n}F_j(q^{-s})^{(-1)^{j + 1}} = \zeta_X(s) = \exp\left(\sum_{d \geq 1}\frac{N_d}{d}q^{-ds}\right).
\end{align} I have two questions, but they are both related to this statement. In this context, is the $j$ -th Betti number just the rank of the singular homology group $H_j(X_{\mathbf{C}})$ of $X_\mathbf{C}$ as a topological space? We can consider $X_\mathbf{C}$ as a scheme over $\textrm{Spec} \, \mathbf{Z}$ via a unique morphism $\varphi : X_\mathbf{C} \to \textrm{Spec} \, \mathbf{Z}$ . Taking the fibre of $\varphi$ over the point $(p)$ , one obtains a scheme $X_\mathbf{C} \times_{\textrm{Spec} \, \mathbf{Z}} \mathbf{F}_p$ over $\mathbf{F}_p$ , à la Hartshorne II.3 page 89. Do we say that $X$ is obtained from $X_\mathbf{C}$ by reduction mod $p$ because $X \cong X_\mathbf{C} \times_{\textrm{Spec} \, \mathbf{Z}} \mathbf{F}_p$ ?","['number-theory', 'algebraic-geometry', 'betti-numbers', 'zeta-functions', 'schemes']"
4621173,Determining the sum of $\frac{a_{n+1}}{a_n}$ where $a_{n+1}=\frac{na_n^2}{1+(n+1)a_n}$,"Let $a_0=1, a_1=\frac{1}{2}, a_{n+1}=\frac{na_n^2}{1+(n+1)a_n}$ , then find $\lim_{n\to\infty} \sum_{k=0}^{n}\frac{a_{k+1}}{a_k}=\dots$ We have $(a_n)$ is strictly decreasing as $a_{n+1}-a_n=\frac{-a_n(a_n+1)}{1+(n+1)a_n}<0, a_n>0$ and $ |{\frac {a_{n+1}}{a_n}|}<1$ , then the sum is convergent.
Tried to telescope the sum by defining $\frac{a_{n+1}}{a_n}=\frac{na_n}{1+(n+1)a_n}=b_n$ (say). Eventually there was coming another sequence that was not giving any conclusion. Tried examining the behaviour of the terms of this sequence $$ a_0=1,a_1=\frac{1}{2},a_2=\frac{1}{2^3},a_3=\frac{1}{2^2(2^3+2+1)}\\a_4=\frac{3^2}{2^4(2^3+2^2)(2^3+3)(2^4(2^3+2^2)(2^3+2^2+1)+2^3+2^2+2+1)}$$ There would come up more $2^n$ 's in the denominator for larger n's. Tried mimicking the solution as in the Convergence of $\left(a_{n+1}= \cfrac{{a_n}^2}{1+{a_n}} (n\ge 1) , a_1=1 \right)$ (seems a bit relatable), but  I am stuck here. Any hint would be appreciated. Thanks.","['sequences-and-series', 'real-analysis']"
4621212,Ruler and compass construction of the axis of an ellipse related to a triangle [duplicate],"This question already has answers here : Graphically locate the axes or foci of an ellipse from 5 arbitrary points on its perimeter. (4 answers) Closed last year . Let $ABC$ be a triangle. Consider all the six points defined as the opposite of a vertex respect to another. It is not hard to prove (with affinity) that these 6 points all lie on an ellipse. Looking on it on geogebra it also seems that the center of this elliplse is the centroid of $ABC$ . However I cannot find a way to construct the axis of this elliplse using just ruler and compass and I was wondering if maybe there is a way of doing it. For some cases it is easy, for example if $ABC$ is isosceles with base $AB$ then one of the axis is just the perpendicular bisector of $AB$ and the other one would be the parallel to $AB$ throught the centroid, but for a general triangle this cannot apply.","['triangles', 'conic-sections', 'geometry', 'geometric-construction']"
4621256,Is one-sided differentiable inverse sufficient to conclude equal dimensions?,"Let $\Omega$ be an open subset of $\mathbb R^n$ and $\Upsilon$ be of $\mathbb R^m$ . Let $f\colon \Omega\to \Upsilon$ . Then this answer shows that if there exists a point $c\in\Omega$ where $f$ is differentiable, and if $f$ is invertible with the inverse being differentiable at $f(c)$ , then $m = n$ . Note that differentiability at only one point is sufficient. The answer also comments that if there exists a homeomorphism between $\Omega$ and $\Upsilon$ (with these being nonempty), then too $m = n$ . But this left me wondering about the following questions: If $f$ is not invertible, but has a right-inverse $g$ (that is differentiable at $f(c)$ ), is $m = n$ still? Or can we say nothing more than $m\le n$ ? Same question about homeomorphism. The answer said that the homeomorphism statement can be proved by Brouwer's fixed point theorem. Can you give me a reference for that?","['diffeomorphism', 'vector-spaces', 'jacobian', 'multivariable-calculus', 'general-topology']"
4621292,How do we find the value of $\tan\biggr(\sum_{1\leq i\leq 21}f(i)\biggr)$ if $f(x) = \arctan\left(\frac{1}{x^{2 }+ x + 1}\right)$?,"Let's assume that $f : \mathbb{R}\rightarrow (-\frac{\pi}{2}, \frac{\pi}{2})$ such that for every $x\in \mathbb{R}$ , $f(x) = \arctan\biggr(\frac{1}{x^2+x+1}\biggr)$ . How can we find $\tan\biggr(\sum_{1\leq i\leq 21}f(i)\biggr)$ ? Below is a fact that I've derived: $$\tan\biggr(f(x)+f(x+1)\biggr) = \frac{\frac{1}{(x+1)^2+(x+1)+1}+\frac{1}{x^2+x+1}}{1-\frac{1}{((x+1)^2+(x+1)+1)(x^2+x+1)}} = \frac{2}{(x+1)^2}$$ But I am not sure how this can be used to approach the problem.","['trigonometry', 'sequences-and-series']"
4621322,Khinchin's constant: Show that $( \prod_{k=1}^n a_k(x))^{1/n} \longrightarrow K \quad (a.s.)$,"Consider the probability space $\Omega = [0,1), \mathcal{A} = \mathcal{B}([0,1))$ and $$\mathbb{P}(A) = \frac{1}{\log(2)} \int_A \frac{1}{1+x} dx.$$ In the lecture we learned that the map $\tau(x)$ with $\tau(x) = \frac{1}{x} - \big\lfloor \frac{1}{x} \big\rfloor$ for $x > 0$ and $\tau(0) = 0$ for $x = 0$ is a measure preserving transformation on $\Omega$ and that for irrational $x \in \Omega$ the $a_i := A(\tau^{i-1}(x))$ , where $A(x) := \lfloor 1/x \rfloor$ , constitute the continued fraction expansion of $x$ , i.e. $x = [0,a_1,a_2,\ldots]$ . Furthermore we have already learned that $\tau$ is ergodic. Use this to show that $$\frac{1}{n} \sum_{k=1}^n f(a_k(x)) \longrightarrow \frac{1}{\log(2)} \int_0^1 \frac{f(\lfloor 1/x \rfloor)}{1+x} dx \qquad (a.s.)$$ for every function $f:[0,1) \rightarrow \mathbb{R}$ with $\int_0^1 \lvert f(\lfloor 1/x \rfloor ) \rvert dx < \infty$ . Conclude that $$\biggl( \prod_{k=1}^n a_k(x) \biggr)^{1/n} \longrightarrow K \qquad (a.s.)$$ , where $K := \exp \biggl( \frac{1}{\log(2)} \int_0^1 \frac{\log(\lfloor 1/x \rfloor)}{1+x} dx \biggr)$ is Khinchin's constant. Remark: The Wikipedia article on Khinchin's constant proves this, but it uses a different definition of $K$ . I think that the first equation $$\frac{1}{n} \sum_{k=1}^n f(a_k(x)) \longrightarrow \frac{1}{\log(2)} \int_0^1 \frac{f(\lfloor 1/x \rfloor)}{1+x} dx \qquad (a.s.)$$ is just an application of Birkhoff's Ergodic Theorem , but I do not get how to make the conclusion.","['number-theory', 'ergodic-theory', 'probability-theory']"
4621334,A Half Partition of Borel Sets will Give Half the Borel Measure,"Here is the problem: Let $n \in \mathbf{N}$ and define $A_n := \bigcup_{k \in \mathbf{Z}} [\frac{2k}{2^n}, \frac{2k + 1}{2^n})$ . If $E \in \mathcal{B}(\mathbf{R})$ , show the limit $$
\lim_{n \to \infty} \lambda(A_n \cap E) = \frac{\lambda(E)}{2}.
$$ I have successfully proven that for all $E \in \mathcal{B}(\mathbf{R})$ that is bounded . The problem that I am facing right now is to extend from the bounded case to unbounded case. My idea was to split $E = \bigcup_{m = 1} ^\infty E \cap B(0, m) =: \bigcup_{m = 1} ^\infty E_m$ , with $B(0, m)$ being the ball with radius $m$ . This would give us by the continuity of measure that $$
\lim_{n \to \infty} \lambda(A_n \cap E) = \lim_{n \to \infty} \lim_{m \to \infty} \lambda(A_n \cap E_m).
$$ Now ideally we would really like to switch the order of limit here. However, I can not justify if we could do so or not. Is there another way to approach the unbounded case? Any idea would be appreciated.","['measure-theory', 'borel-measures', 'real-analysis', 'alternative-proof', 'solution-verification']"
4621337,"Asking for an ""intuitive"" explanation of a probability problem [duplicate]","This question already has answers here : Why are all subset sizes equiprobable if elements are independently included with probability uniform over $[0,1]$? (2 answers) Closed last year . The problem is as follows: We pick a real number p in $(0,1)$ randomly and uniformly, then construct a coin such that when tossed, $P(H) =p$ and $P(T) = 1- p$ . Now fix a positive integer $n$ , if we were to toss the coin $n$ times (independently), what is the probability that exactly $k$ ( $0 \leq k \leq n$ ) heads occur? I have done this by evaluating the integral $$\int_0^1{{n \choose k}p^k(1-p)^{n-k}}dp = \frac {1}{n+1}.$$ However, the evaluation was far from easy and I had to use the ""snake oil"" method mentioned in the book ""generatingfunctionology"" by Wilf. Yet this result is so simple and beautiful, in particular, it is very suprising to me that the probability is independent of $k$ . I tend to believe there must be a simpler reasoning for this, something I failed to notice that can draw the conclusion ""the probability is independdent of $k$ "". Can someone show me such a way(if there is one)?",['probability']
4621420,How can I show that an ODE has other solutions than the trivial ones when it is not analytically solvable?,"I have the following first-order differential equation: $$
(a x - b y) y'(x) - c y =0,
$$ where $a,b,c>0$ and $x>=0$ .
There are two obvious solutions: $y(x)\equiv 0$ and $y(x)=\frac{a-c}{b} x$ . The numerical analysis suggests that there are infinitely many solutions between these two solutions. For example, the following is a streamline plot for $a=3/4,b=7/4,c=1/4$ generated by Mathematica. The red line is the linear solution, and the yellow line is the zero function solution. My question is, how can I rigorously show that there are infinitely many solutions between these two trivial solutions? Or, at the very least, can I formally show that there is one solution that is one of the above solutions? Since an explicit construction seems not feasible, I don't know where to start. a = 3/4; b = 7/4; c = 1/4;
gr1 = StreamPlot[{1, (c y)/(a x - b y)}, {x, 0, 2}, {y, -.05, 1}];
gr2 = ContourPlot[y == (a - c)/b x, {x, 0, 2}, {y, 0, 3}, ContourStyle -> Red];
sol3 = DSolve[{(a x - b y[x]) y'[x] - c y[x] == 0, y[0] == 0}, y, {x, 0, 2}][[1]];
gr3 = Plot[Evaluate[y[x] /. sol3], {x, 0, 2}, PlotStyle -> Yellow];
sol4 = NDSolve[{(a x - b y[x]) y'[x] - c y[x] == 0, y[1] == .2}, y, {x, 0, 2}][[1]];
gr4 = Plot[Evaluate[y[x] /. sol4], {x, 0, 2}, PlotStyle -> Green];
Show[gr1, gr2, gr3, gr4]",['ordinary-differential-equations']
4621437,Can I accept my student's solution of $\lim _{x \to \infty} \ln x-2x$,"Can I accept my student's solution of $$\lim_{ x \to \infty} \ln x-2x$$ I gave the above limit to my student. This is the way she has done. $$\begin{aligned}
L= & \lim _{x \rightarrow \infty} \ln x-2 x=\lim _{x \rightarrow \infty} x\left(\frac{\ln x}{x}-2\right) \\
& \lim _{x \rightarrow \infty} \frac{\ln x}{x}-2=\lim _{x \rightarrow \infty} \frac{1}{x}-2=-2 \: \text{(By L'Hopital's Rule)}\\
\Rightarrow \quad & L=\lim _{x \rightarrow \infty} x \times-2=-\infty
\end{aligned}$$ But I suggested this way: $$\begin{gathered}
x=\frac{1}{t} \\
\Rightarrow L=\lim _{t \rightarrow 0^{+}}-\ln t-\frac{2}{t} \\
\Rightarrow \quad L=-\lim _{t \rightarrow 0^{+}} \frac{2+t \ln t}{t} \\
\lim _{t \rightarrow 0^{+}} t \ln t=0 \: \text{(Again by L'Hopital's Rule)}\\
\Rightarrow L=-\infty
\end{gathered}$$","['limits', 'calculus', 'infinity']"
4621461,"Why this corollary force ${\rm Aut}(H)=1$, where $H$ isomorphic to $\Bbb Z_2$","This is on Dummit and Foote P134. If $H\cong\Bbb Z_2$ , then since $H$ has unique elements of orders $1$ and $2$ , corollary 14 forces ${\rm Aut}(H)=1$ . Corollary 14: If $K$ is any subgroup of group $G$ and $g\in G$ , then $K\cong gKg^{-1}$ . Conjugate elements and conjugate subgroups have the same order. I can see since ${\rm Aut}(\Bbb Z_2)$ has only trivial automorphism, and $H\cong\Bbb Z_2$ , so ${\rm Aut}(H)=1$ . But I can not see how corollary 14 showed that, since to me, corollary 14 only showed that the conjugation is trivial, and didn't show that ${\rm Aut}(H)$ is trivial.","['automorphism-group', 'group-theory', 'group-isomorphism']"
4621541,Conditional density of ordered iid Exp(1),"I am trying to apply the change of variable theorem for the calculation of the distribution, via the conditional density, of this transformation of joint iid $X_i$ that are Exp(1) variables with no success: $$f_{X_{(2)}-X_{(1)},...,X_{(n)}-X_{(1)}|X_{(1)}}(x_2,...,x_n)$$ where $X_{(1)}$ is the minimum, $X_{(2)}$ is the second lowest variable and so on (ordered statistic). Any suggestion? This distribution, according to Exercise 15.1.3 of Probability Theory by A. Klenke (3rd version), should be equal to the unconditional distribution of the ordered sample $(X_{2},...,X_{n})$ . (For the ones checking the book, I reversed the notation). Edit Could it be something like the following equations based on the iid and “memoryless” properties of the sample? $P[X_{(2)}-X_{(1)}<y_2,…,X_{(n)}-X_{(1)}<y_n| X_{(1)}=x_1]= \\= n! P[X_{2}-X_{1}<y_2,…,X_{n}-X_{1}<y_n| X_{1}=x_1]= \\=n!P[X_{2}<y_2+x_1,…,X_{n}<y_n+x_1| X_{1}=x_1]=\\=n!P[X_2<y_2]…P[X_n<y_n]$ Basically I am considering in the $n!$ permutation also the fact that the lowest observed value could come from any observation in the sample. Thus I condition on the (arbitrarily) first observation to be the lowest and no more on the minimum $X_{(1)}$ . Before I tried to divide the joint density of the ordered statistics by the density of the minimum. Is my reasoning right? Edit https://math.stackexchange.com/a/4626276/1073326 Here @xzm solved it very clearly by suggesting that the unconditional distribution is related to ordering a sample of dimension $n-1$ , if I understood properly.","['conditional-probability', 'change-of-variable', 'probability-theory', 'density-function']"
4621547,Negative values in Poisson process,"I'm trying to make some sense of the following definition: A collection of random variables $\{ N_t \}$$_{t \geq 0}$ is called a Poisson process with rate parameter $\lambda > 0$ if $N_0 = 0$ $N_{t_2} - N_{t_1}$ and $N_{t_4} - N_{t_3}$ are independent for all $0 \leq t_1 < t_2 \leq t_3 < t_4$ For all $t \geq 0$ and $h > 0$ , we have $P(N_{t+h} - N_t = 1) = \lambda h + o(h)$ and $P(N_{t+h} - N_t \geq 2) = o(h)$ Now my question is, does this definition imply that $N_t \geq 0$ for all $t$ ? I know that this must obviously be true since they represent counts, but how do you actually get this from the definition?","['statistics', 'stochastic-processes', 'poisson-process', 'probability-theory', 'random-variables']"
4621565,"Is $AGL(3,2)$ self-normalizing in $S_8$?","Let $AGL(3,2)$ be a group of all affine permutations consisting of maps $$
x \to Ax + b,
$$ where $x$ and $b$ are vectors of length $3$ over $GF(2)$ and $A$ is an invertible $3\times 3$ matrix over $GF(2)$ . Using some numeration of vectors we can consider $AGL(3,2)$ as a subgroup of $S_8$ . My experimets with computer algebra system Sage have shown that normaliser of $AGL(3,2)$ in $S_8$ is equal to $AGL(3,2)$ . But I can not prove this. Can somebody help me with hints or full solution.","['matrices', 'group-theory', 'linear-algebra', 'permutations']"
4621638,Evaluate this double Integral in polar coordinates.,"$$ \iint y^2(a^2-x^2)^{0.5}dxdy $$ over $x^2+y^2\le a^2$ I have evaluated this in $x-y$ plane and got $32a^5/45$ . Please help in evaluating the same in polar coordinates. Ive tried putting $x=r\cos\theta$ and $y=r\sin\theta$ and After considering the Jacobian I am getting this expression $$ \int\limits_0^a\int\limits_0^{2π}r^3\sin^2θ(a^2-r^2cos^2θ)^{0.5}\,drdθ $$ Please help me evaluate this.","['integration', 'multivariable-calculus', 'definite-integrals']"
4621685,"For a compact subset of $\mathbb{C}$ not separating $0$ from $\infty$, is there a simply connected region containing it that doesn't contain $0$?","A compact subset of $\mathbb{C}$ that does not separate $0$ from $\infty$ is one in which $0$ lies in the unique unbounded component of its complement. A region refers to a connected open subset of $\mathbb{C}$ . CONTEXT: In an unital complex Banach algebra, a condition for an element $x$ to have a logarithm (i.e. another element $z$ such that $\exp(z)=x$ ) is that the spectrum $\sigma(x)$ of $x$ doesn't separate $0$ from $\infty$ . The proof in Functional Analysis by Walter Rudin (theorem 10.30) requires a simply connected region $\Omega\supset\sigma(x)$ and a holomorphism $f\in H(\Omega)$ such that $\exp(f(\lambda))=\lambda\;(\forall\lambda\in\Omega)$ , which means $0\notin\Omega$ . By the theory of complex analysis, the condition ""simply connected"" ensures the existence of $f$ . But Rudin did not prove the existence of $\Omega$ . By hypothesis, there's a continuously differentiable curve $p:[0,\infty)\rightarrow\mathbb{C}$ from $0$ to $\infty$ such that $p(t_1)=p(t_2)\Rightarrow t_1=t_2$ and $p([0,\infty))\cap\sigma(x)=\emptyset$ . Choose positive numbers $r_1<\operatorname{d}(0,\sigma(x))$ and $r_2>\|x\|$ . Define the annulus $A\triangleq\{\lambda\in\mathbb{C}\mid r_1<\|\lambda\|<r_2\}$ , $v$ to be the minimal number such that $\|p(v)\|=r_2$ and $u$ to be the maximal number less than $v$ such that $\|p(u)\|=r_1$ . I guess $A\setminus p([u,v])$ is homotopy equivalent to $S^1\setminus\{*\}$ but I don't know how to prove it.","['general-topology', 'algebraic-topology']"
4621699,A generalization of conditional expectation to non-integrable random variables,"I'm reading about conditional expectations from Achim Klenke's Probability Theory: A Comprehensive Course , which defines conditional expectations for integrable random variables $X$ with respect to a $\sigma$ -algebra $\mathcal F$ as the unique (a.s.) $\mathcal F$ -measurable random variable $Y$ for which $\mathbb E[X\mathbb 1_A] = \mathbb E[Y\mathbb 1_A]$ for all $A \in \mathcal F$ . The following remark is made to generalize this definition to some non-integrable random variables: Let $X : \Omega \to \mathbb R$ be a random variable such that $X^- \in \mathcal L^1(\mathbb P)$ . We can define the conditional expectation as the monotone limit $$
\mathbb E[X\,|\,\mathcal F] = \lim_{n \to \infty} \mathbb E[X_n\,|\,\mathcal F]
$$ where $-X^- \leq X_1$ and $X_n \uparrow X$ . Due to the monotonicity of the conditional expectation, it is easy to show that the limit does not depend on the choice of the sequence $(X_n)$ and that it fulfills the conditions of [the definition of conditional convergence]. (Presumably we want $(X_n) \subset \mathcal L^1(\mathbb P)$ to avoid circular definitions.) My question: Why do we need $-X^- \leq X_1$ ? It's not hard to verify these claims. By monotonicity of conditional expectation, we get that $\mathbb E[X_n\,|\,\mathcal F] \uparrow \mathbb E[X\,|\,\mathcal F]$ (this limit may be infinite). So for any $A \in \mathcal F$ , by the Beppo-Levi monotone convergence theorem, since the $\mathbb E[X_n\,|\,\mathcal F]$ are integrable, $$
\mathbb E[\mathbb E[X\,|\,\mathcal F]\mathbb 1_A] = \mathbb E\left[\lim_{n \to \infty} \mathbb E[X_n\,|\,\mathcal F]\mathbb 1_A\right] = \lim_{n \to \infty} \mathbb E\left[\mathbb E[X_n\,|\,\mathcal F]\mathbb 1_A\right] = \lim_{n \to \infty} \mathbb E[X_n\mathbb 1_A] = \mathbb E[X\mathbb 1_A].
$$ If $(\tilde X_n)$ is another sequence with $\tilde X_n \uparrow X$ and $-X^- \leq \tilde X_1$ , letting $Y = \lim \mathbb E[X_n \,|\,\mathcal F]$ and $\tilde Y = \lim\mathbb E[\tilde X_n\,|\,\mathcal F]$ , then both $Y$ and $\tilde Y$ are $\mathcal F$ -measurable (as a limit of $\mathcal F$ -measurable functions), and this calculation shows $\mathbb E[Y\mathbb 1_A] = \mathbb E[\tilde Y \mathbb 1_A]$ for every $A \in \mathcal F$ . It follows that $Y = \tilde Y$ . I can see that we'd want $X^- \in \mathcal L^1(\mathbb P)$ because otherwise we can't have that both $(X_n) \in \mathcal L^1(\mathbb P)$ and $X_n \uparrow X$ . But I don't see why we need $-X^- \leq X_1$ . In fact this seems contradictory if we require $X_n \uparrow X$ . Am I missing something?","['integration', 'conditional-expectation', 'conditional-probability', 'probability-theory']"
4621703,How fast does $a_n:=\int_{1/\sqrt{2}}^1 \frac{dx}{\left(\frac{1}{2}+x^2\right)^{n+1/2}}$ decay as $n \to \infty$?,"Let \begin{equation}
a_n:=\int_{1/\sqrt{2}}^1 \frac{dx}{\left(\frac{1}{2}+x^2\right)^{n+1/2}}
\end{equation} for $n \in \mathbb{N}$ . Then, $a_n$ is clearly a monotone decreasing sequence of positive numbers and by the Dominated Convergence Theorem, $a_n \to 0^+$ as $n \to \infty$ . However, I have some difficulty estimating how fast $a_n$ decays. For example, $\frac{a_n}{f(n)}=O(1)$ as $n \to \infty$ for some polynomial $f$ ? Could anyone please provide insight into the decay rate of $a_n$ ?","['calculus', 'sequences-and-series', 'real-analysis']"
4621736,Simplifying a complicated fraction (precalculus),"Simplify: $$\frac{\left(\dfrac{3x+x^3}{1+3x^2}\right)^2-1}{\dfrac{3x^2-1}{x^3-3x}+1}\; \div\; \frac{\dfrac{9}{x^2}-\dfrac{33-x^2}{3x^2+1}}{\dfrac{3}{x^2}-\dfrac{2(x^2+3)}{(x^3-x)^2}}$$ My attempt: $$\frac{\dfrac{(3x+x^3)(3x+x^3)-(1+3x^2)(1+3x^2)}{(1+3x^2)(1+3x^2)}}{\dfrac{3x^2-1+x^3-3x}{x^3-3x}} \div
\frac{\dfrac{9(3x^2+1)-x^2(33-x^2)}{x^2(3x^2+1)}}{\dfrac{3(x^3-x)(x^3-x)-2x^2(x^2+3)}{x^2(x^3-x)(x^3-x)}} \tag1$$ $$\frac{\dfrac{(3x+x^3+1+3x^2)(3x+x^3-1-3x^2)}{(1+3x^2)(1+3x^2)}}{\dfrac{3x^2-1+x^3-3x}{x^3-3x}}\div\frac{\dfrac{9(3x^2+1)-x^2(33-x^2)}{x^2(3x^2+1)}}{\dfrac{3x^2(x^2-1)(x^2-1)-2x^2(x^2+3)}{x^2(x^3-x)(x^3-x)}} \tag2$$ $$\frac{\dfrac{(3x+x^3+1+3x^2)(3x+x^3-1-3x^2)}{(1+3x^2)(1+3x^2)}}{\dfrac{3x^2-1+x^3-3x}{x^3-3x}}\div\frac{\dfrac{9(3x^2+1)-x^2(33-x^2)}{x^2(3x^2+1)}}{\dfrac{3(x^2-1)(x^2-1)-2(x^2+3)}{x^2(x^2-1)(x^2-1)}} \tag3$$ $$\frac{\dfrac{(3x+x^3+1+3x^2)(3x+x^3-1-3x^2)}{(1+3x^2)(1+3x^2)}}{\dfrac{3x^2-1+x^3-3x}{x^3-3x}}\div
\frac{\dfrac{9(3x^2+1)-x^2(33-x^2)}{(3x^2+1)}}{\dfrac{3(x^2-1)(x^2-1)-2(x^2+3)}{(x^2-1)(x^2-1)}}  \tag4$$ And now I'm stuck. I don't feel I did anything wrong, but clearly there is some simplification pattern that I'm missing. The answer is: $$\frac{x(x+1)}{x^2+4x+1} $$ So obviously a lot more simplification can be done, but I can't see it. Thanks for the help.",['algebra-precalculus']
4621775,Statistics/combinatorics question relating to Enigma rotors,"I would be grateful of some advice on what I hope is a straightforward matter for mathematicians with a sound knowledge of statistics and probability/combinatorics. Context I am investigating Enigma rotors, which cause the 26 letters of the alphabet to become permuted between input and output. There are therefore 26! possible rotors. But what seems to be important in terms of the Enigma producing random permutations is that the number of permutation offsets is as large as possible. By permutation offset, I mean the difference in position between the output letter and the input letter. So for example, if A permutes to E, then the resulting difference is 5-1=4. If H permutes to D then the resulting difference is 4-8=-4=22mod26. So collectively there are 26 permutation offsets for an Enigma rotor. However, it can be shown that it is impossible for all 26 different offsets i.e. the set of integers from 0-25 inclusive to be present. So I am interested in estimating how many different rotors are possible with 25 different offsets. Computationally, I don't think it is possible to assess all 26! permutations and keep a count of the ones that result in 25 offsets. So I have resorted to creating random permutations using the Python random.shuffle function. Over 5 billion random samples (which takes 12 hours to run on my admittedly old PC), I have measured anything between 48 and 75 of these permutations, so we are talking order of 10-15 per billion random permutations. So, these permutations are very rare, but in the context of 26!, there are also a lot of them (order 10^18) So my question is: how many random samples would I need to test before I could be confident that my estimate of the frequency of these permutations is correct within +/-5%? What about +/-1%? I have tried dividing the sample space into smaller intervals eg of 100 million and then using the resulting data to produce a confidence interval. But I doubt that approach has any mathematical validity. Any help or advice gratefully appreciated. I suspect this is a fairly generic problem in statistics, but I haven't been able to find an answer using Google.","['permutations', 'statistics', 'combinatorics', 'sampling', 'probability']"
4621836,Radon-Nikodym derivative with respect to product of marginal measures,"Let $\mu$ be a (finite if necessary) measure on the product $\sigma$ -algebra $\mathcal A_1 \otimes \mathcal A_2$ of two measurable spaces $(\Sigma_1,\mathcal A_1)$ , $(\Sigma_2, \mathcal A_2)$ .
The marginal measure of $\mu$ on $\mathcal A_i$ for $i\in\{1,2\}$ is defined as $$ \mu_1(A) := \mu(A\times \Sigma_2), \quad \mu_2(B) := \mu(\Sigma_1 \times B) \quad \text{for $A\in\mathcal A_1, B\in\mathcal A_2$}.$$ I wanted to know whether the Radon-Nikodym derivative $\frac{d\mu}{d(\mu_1\otimes \mu_2)}$ exists.
For the existence, we have to show that $\mu$ is absolutely continuous w.r.t. the product measure $\mu_1\otimes\mu_2$ .
I've tried several approaches without success, one of them is described in the following: Given $M\in\mathcal A_1\otimes \mathcal A_2$ with $(\mu_1\otimes\mu_2)(M)=0$ show $\mu(M)=0$ . Using Fubini, we compute $$
0 = (\mu_1\otimes\mu_2)(M) = \int_{\Sigma_1}\int_{\Sigma_2} \mathbf 1_M(x,y) ~d\mu_2(y) ~d\mu_1(x) = \int_{\Sigma_1} \mu_2(\{ y \mid (x,y)\in M  \}) ~d\mu_1(x)
$$ This implies $\mu_2(\{y\mid(x,y)\in M\})=0$ for $\mu_1$ -a.e. $x_1\in\Sigma_1$ (and a similar statement for $\mu_2$ -a.e. $x_2\in \Sigma_2$ ). At this point, I am unsure whether this statement is true after all (and if so whether we need additional assumptions?).","['measure-theory', 'marginal-distribution', 'absolute-continuity', 'product-measure']"
4621850,Euler-Mascheroni Constant in the Cosine Integral?,"I came across this integral when doing calculus homework (Integration by parts) $$\int \frac{\cos x}x \, dx $$ It turned out in the end that there was a typo in the original question and this integral is not actually necessary, but I was curious and decided to attempt it. My first step was to turn cosine into its series and divide by X to leave a polynomial (series) that is easy to integrate. After doing so, I was left with this but without the symbol on the left. After some googling, I discovered that the actual integral is what I thought it was + the Euler-Mascheroni constant. Can anyone tell me why the constant is there? For backround information, I am currently a senior in High School taking Calculus. So I don't have extensive knowledge of more complicated methods like linear or abstact algebra.","['integration', 'calculus', 'trigonometry', 'taylor-expansion', 'constants']"
4621851,"If A is a positive linear transformation, AB is self-adjoint, then $|(ABx,x)| \leq ||B||(Ax,x)$ or $|(ABx,x)| \leq \rho(B)(Ax,x)$","Prove or disprove: If $A$ is a positive linear transformation, $AB$ is self-adjoint, then a, $|(ABx,x)| \leq ||B||.(Ax,x)$ b, $|(ABx,x)| \leq \rho(B).(Ax,x)$ With the matrix norm defined by: $||A|| := \sup_{x \in V}{\frac{||Ax||}{||x||}} = \sup_{x \in V}{\frac{|(Ax,y)|}{||x||.||y||}}.$ If $A$ is self-adjoint, then $||A||$ is also $\sup_{x \in V}{\frac{(Ax,x)}{||x||^2}}$ . And $\rho(B)$ is the spectral radius of linear transformation $B$ . Also, if $A$ is a positive transformation, that means $A = C^{*}C$ for some $C$ , or $A = B^2$ for some self-adjoint $B$ , or $(Ax,x) \geq 0 \space, \forall x \in V$ and $A$ be self-adjoint. These definitions are equivalent. The hypothesis "" $AB$ is self-adjoint"" implys $B$ can be written in the form $CA$ , with $C$ be a self-adjoint transformation. But from here, after trying a lot of things, I can't seem to get to that $||B||$ . One of the closest results I've got is LHS $\leq ||\sqrt{A}C\sqrt{A}||.(Ax,x)$","['linear-algebra', 'functional-analysis', 'linear-transformations']"
4621871,Finding the angle $\angle BDC$,"Let's assume that $\angle DBC = 50^{\circ}$ , $[DC]$ bisects $\angle ACB$ , and that $|AC| = |BC|-|AD|$ . How could we find the angle $\angle BDC$ ? Applying angle bisector theorem: $$\frac{|AD|}{|BD|} = \frac{|AC|}{|BC|}$$ Since $|AC| = |BC|-|AD|$ , $$\frac{|AD|}{|BD|} = \frac{|BC|-|AD|}{|BC|} = 1-\frac{|AD|}{|BC|}$$ $$|AD|\biggr(\frac{1}{|BD|}+\frac{1}{|BC|}\biggr) = 1$$ But this won't lead me anywhere, I believe. Could we take complex geometric approach to this problem?",['geometry']
4621877,Nth Derivative of $\frac{x}{x-1}$,"So starting off, rewrite $x*\dfrac1{x-1}$ . $d/dx=\dfrac{d}{dx}(x)*\dfrac{1}{x-1}+\dfrac{d}{dx}\dfrac{1}{x-1}*x$ . $\dfrac{d}{dx}(x)=1$ . $x^{n}=nx^{n-1}$ . $\dfrac{1}{x-1}=(x-1)^{-1}$ . $\dfrac{d}{dx}(\dfrac{1}{x-1})=(-1)(x-1)^{-2}=\dfrac{-1}{(x-1)^{2}}$ . Combining them both we get $\dfrac{1}{x-1}+\dfrac{-1}{(x-1)^{2}}*x$ . $\dfrac{d}{dx}=\dfrac{1}{x-1}+\dfrac{-x}{(x-1)^{2}}$ . Now to find $\dfrac{d^2y}{d^2x}$ , using product rule and sum rule $\dfrac{d}{dx}\dfrac{1}{x-1}+\dfrac{d}{dx}-x\dfrac{1}{(x-1)^2}+\dfrac{d}{dx}\dfrac{1}{(x-1)^{2}}x$ , so anyway doing it we get $\dfrac{-1}{(x-1)^2}+\dfrac{-1}{(x-1)^2}+\dfrac{-2x}{(x-1)^{3}}$ . Simplifying $\dfrac{d^2y}{d^2x}=\dfrac{-2}{(x-1)^2}+\dfrac{-2x{(x-1)^{3}}$; and finally $\dfrac{d^3y}{d^3x}$. $\dfrac{d}{dx}\dfrac{-2}{(x-1)^2}+\dfrac{d}{dx}-2x\dfrac{1}{(x-1)^{3}}+\dfrac{d}{dx}\dfrac{1} {(x-1)^{3}}*(-2x)$,which gives $\dfrac{4}{(x-1)^3}+\dfrac{-2}{(x-1)^3}+\dfrac{6x{(x-1)^4}$; simplyfing we get
$\dfrac{2}{(x-1)^3}+\dfrac{6x}{(x-1)^4}$ ; now there is supposed to be a pattern here but i don't see it. is there something wrong with my answer?","['calculus', 'derivatives']"
4621879,Minkowski's integral inequality for other norms,"In my measure theory course we studied norms $L_p$ and no other norms. For proofs we used exclusively the trick known as Hoelder's inequality which works only on $L_p$ norms. I disliked it very much so I found a proof for Minkowski's integral inequality which (slightly longer but) works for all norms that are monotone in this sense: $f\le g\;\;\;\text{a.e.}\;\Rightarrow\;|f|\le|g|\;\;\;\;\;$ (also, $\;\;f=g\;\;\;\text{a.e.}\;\Rightarrow\;|f|=|g|$ ), $\{f_n\}_{n\in\mathbb{N}}\;\;\text{is non-decreasing w.r.t. the relation above}\;\Rightarrow\;\lim|f_n|=|\lim f_n|$ , $\{f_n\}_{n\in\mathbb{N}}\;\;\text{is non-increasing w.r.t. the relation above}\;\;\land\;\;|f_1|<\infty\;\Rightarrow\;\lim|f_n|=|\lim f_n|$ . Unfortunately, I couldn't prove it for all norms, but this was as general as I could imagine a norm be. Then, the total variation norm was substantially different, so I proved that case too: Let $f:[0,1]^2\rightarrow\mathbb{R}$ be a Lipschitz continuous function (it also holds for more general functions, but let's keep it simple). Let $H:\mathbb{R}^{[0,1]}\rightarrow\mathbb{R}$ be the norm that maps (Lipschitz continuous) functions $f\in\mathbb{R}^{[0,1]}$ into $|f(0)|+\text{Var}_f[0,1]$ . Let $f_x:[0,1]\rightarrow\mathbb{R}:y\mapsto f(x,y)$ and let $f^y:[0,1]\rightarrow\mathbb{R}:x\mapsto f(x,y)$ and let $\lambda$ be the Lebesgue measure. Then we have: $$H\bigg(x\mapsto\int f_xd\lambda\bigg)\le\int\Big(y\mapsto H(f^y)\Big)d\lambda.$$ On the other hand, I could not find any norm for which this inequality doesn't hold. The question: Is there any simple norm for which the Minkowski's integral inequality doesn't hold? What is the more precise relationship with the integral operator that the norm has to have in order to satisfy this inequality? ( The ""definition"" of the Minkowski's integral ""inequality"" in case of the general norm is like the one I gave for the total variation norm. )","['measure-theory', 'functional-analysis', 'integral-inequality']"
4621942,"Find the volume of the following region $E= (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) $","Find the volume of the following region $E= \{ (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) \}  $ I figured that the region E is formed by the points that belong in the gap between the two paraboloids $z=\sqrt{2}(x^2 + y^2)$ $\hspace{20pt}$ and $\hspace{20pt}$ $ z= \sqrt{6}(x^2 + y^2)$ that are as well inside of the sphere $x^2 + y^2 + z^2 = 1$ . $E$ can be obtained by making a whole rotation around the $z$ -axis in the plane- $xy$ then using Guldino's theorem can be appropriate. Considering now cylindrical coordinates we've that $E$ becomes the following: $F= \{ (\rho, \theta, z): \rho^2 + z^2 ≤1, \sqrt{2}\rho^2 ≤z≤ \sqrt{6}\rho^2,  0≤ \theta≤ 2\pi \}$ Then we've that: $vol \hspace{2pt} E = \int _{F} \rho \hspace{2pt} d\rho d\theta dz = 2\pi \int\int _{D}  \rho d\rho dz $ I'm struggling to find the region $D$ . Any hint?","['integration', 'multivariable-calculus']"
4621973,"Is my proof of the fact that continuous partials imply differentiability, absolutely rigorous?","Theorem: Let $V$ , $W$ be finite-dimensional normed linear spaces and $W$ be over $\mathbb R$ . ( $V$ can possibly be over $\mathbb C$ .) Let $(e_1, \ldots, e_n)$ and $(\tilde e_1,\ldots, \tilde e_m)$ be bases for $V$ and $W$ . Let $F\colon \Omega\to W$ where $\Omega$ is open in $V$ such that each of the $\partial_i f_j$ 's (see the note at the end) exist throughout $\Omega$ and are continuous at $c\in\Omega$ . Then $f$ is differentiable at $c$ . My proof: Since differentiability (and the partials) are independent of the norms on either of the spaces, we choose to work with $\lVert\cdot\rVert_1$ norms so that, for instance, $\lVert \sum_i x_i e_i\rVert = \sum_i |x_i|$ in $V$ . Similarly in $W$ . Let $\delta > 0$ such that $B_\delta(c)\subseteq\Omega$ and let $h\in B_\delta(0)\setminus\{0\}$ . Now, define a sequence $x^{(0)}, \ldots, x^{(n)}\in V$ by $x^{(0)} := c$ and $x^{(k+1)} := x^{(k)} + h_{k + 1} e_{k + 1}$ $(h_k$ 's are components of $h$ ). Since we are considering $\lVert\cdot\rVert_1$ norm, each $x^{(k)}\in B_\delta(c)$ . Now, $$
f(c + h) - f(c) = \sum_{k = 1}^n \bigl(f(x^{(k)}) - f(x^{(k-1)})\bigr).\tag{1}
$$ Fix a $1\le k\le n$ . Now there exists an $\epsilon > 0$ such that for each $t\in(-\epsilon, 1 + \epsilon)$ , we have $x^{(k-1)} + t h_k e_k\in B_\delta(c)$ . Hence we can define $g\colon (-\epsilon, 1 + \epsilon)\to V$ by $t\mapsto f(x^{(k-1)} + th_k e_k)$ . Now, decomposing $g$ in basis $(\tilde e_1, \ldots, \tilde e_m)$ , we get $g_j$ 's on $(-\epsilon, 1 + \epsilon)\to \mathbb R$ and now we are in a position to apply MVT. Before that we show that each $g_j$ is differentiable: $$
\begin{align*}
\lim_{\xi\to 0}\frac{g_j(t + \xi) - g_j(t)}{\xi}
& = \lim_{\xi\to 0}\frac{f_j(v + \xi h e_k) - f_j(v)}{\xi}\\
& = h\; \partial_k f_j(v)
\end{align*}
$$ where $v := x^{(k-1)} + th_k e_k$ . Now, applying MVT on the interval $[0, 1]$ , we get a $\theta\in (0, 1)$ such that $$
\begin{align*}
g_j(1) - g_j(0) & = g'_j(\theta)\\
& = h\; \partial_k f_j(x^{(k-1)} + \theta h_k e_k). \tag{2}
\end{align*}
$$ Now using the continuity of the partials, we can assume w.l.o.g. that $\delta$ is small enough so that $|\partial_k f_j(c + h) - \partial_k f_j(c)|\le \epsilon/m$ for all $k$ , $j$ . Since $x^{(k - 1)} + \theta h e_k\in B_\delta(c)$ (easy), (2) yields $$
| g_j(1) - g_j(0) - h_k \; \partial_k f_j(c) |\le |h_k| \epsilon/m
$$ for all $j$ . Summing the $g_i$ 's to yield $g$ and noting that $g(1) - g(0) = f(x^{(k)}) - f(x^{(k-1)})$ , we get $$
\Big\lVert f(x^{(k)}) - f(x^{(k-1)}) - \sum_{j = 1}^m h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert\le |h_k|\epsilon.
$$ Now using (1) and triangle inequality, we get $$
\Big\lVert f(x) - f(c) - \sum_{j = 1}^m\sum_{k = 1}^n h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert \le\epsilon\lVert h\rVert
$$ and we conclude by noting that the double sum is (the obviously given linear map $V\to W$ ) acting on $h$ . By $\partial_i f_j(c)$ , I mean the following limit: $$
\lim_{t\to 0}\frac{f_j(c + t e_i) - f_j(c)}{t}
$$ $f_j$ 's on $\Omega\to\mathbb R$ are components of $f\colon\Omega\to W$ in $W$ 's basis.","['partial-derivative', 'multivariable-calculus', 'solution-verification', 'normed-spaces']"
4621987,Are measures uncountably superadditive?,"Let $(X, \Sigma, \mu)$ be a measure space ( $X$ is a set, $\Sigma$ is a $\sigma$ -algebra and $\mu$ is a (positive) measure, i.e. a non-negative, countably-additive function $\mu : \Sigma \to \mathbb{R}_{\geq 0} \cup \{ + \infty \} $ ). I have the impression every measure $\mu$ satisfies the following condition which I call superadditivity : $$\mu \left( \bigcup_{i \in I} E_i \right) \geq \sum_{i \in I} \mu ( E_i )$$ where the $E_i$ are pairwise disjoint measurable sets for all $i \in I$ and $I$ is a set (not necessarily countable, in which case the right-hand side is defined as the supremum of sums over countable subsets of $I$ ) such that $\cup_{i \in I} E_i$ is measurable . I have in mind the case where $X$ is an uncountable set and $\mu$ is a non-atomic measure, i.e. $\mu ( \{ x \} ) = 0$ for all $x \in X$ . Then, if for instance $I=X$ and $E_x = \{x \}$ , the left-hand side above is $\mu(X)$ while the right-hand side is 0, satisfying the inequality. The definition is then an extension of countable additivity to non-countable unions, for which you cannot ensure equality. Is every measure superadditive? Is every probability measure superadditive? In each case, can you provide a proof/counterexample?","['measure-theory', 'probability-theory']"
4621994,How is arcsin inverse of sin?,"How can a $\sin$ have inverse function? Let's say we have $f:A\rightarrow B$ , which means the domain of function $f$ is set A, and the range is set $B$ . If we wanted to find $f^{-1}(x)$ , that function would be $f^{-1}\colon B\rightarrow A$ . Now, if we apply same logic to $\sin$ we get that $\sin\colon(-\infty, +\infty)\rightarrow [-1,1]$ . So how can that have an inverse function? We would have situations where for the same input we get different outputs, and by definition, a function for every input can have one and only one output.","['trigonometry', 'functions']"
4622196,Expected value of inversions of pairs in a list of numbers,"Question : For $1 < i,j <n$ , the ordered pair $(i, j)$ is called an inversion in a permutation
of $1,2,...,n$ if $i < j$ and $j$ precede $i$ in the permutation. For instance,
in the permutation $3,5, 1,4,2$ there are six inversions: $(1,3), (1,5), (2,3), (2,4), (2,5)$ , and $(4,5)$ . Suppose we choose a permutation $\rho$ from among the $n!$ permutations. Let $X_{i,j}$ , be a random variable such that $X_{i,j} = 1$ if $(i,j)$ is an inversion in $\rho$ , and $X_{i,j} = 0$ otherwise. What
is $E(X_{i,j})$ , the expectation of $X_{i,j}$ ? What is the expected number function of inversions in $\rho$ ?
Express it as a function of $n$ . Solution attempt : I have been thinking and noticed that if we have a sorted list of numbers like this: $1,2,3,4,5$ then there is no inversion pair. Similarly, if we have a reverse sorted list like this: $5,4,3,2,1$ then we have a maximum number of inversions. Now the question is asking for $E(X_{i,j}) = 1 \times \text{Prob}(X_{i,j} \in \rho) + 0 \times \text{Prob}(X_{i,j} \not \in \rho) = \text{Prob}(X_{i,j} \in \rho)$ I have also been thinking about the number of possibilities of inversion pairs if we have a list of $n$ numbers. There is a constraint about the position of $i$ and $j$ in the list and there is a constraint about the value of the list at $i$ and $j$ . If we just look at the position then in the best-case scenario we have $(n-1) + (n-2) + \dots + 1 = \frac{n \times (n-1)}{2}$ possibilities. I think I am lost.","['expected-value', 'discrete-mathematics']"
4622218,Improper Integrals as Riemann Sums and a Beautiful Limit $\lim_{n \to \infty}\frac{\sqrt[n]{n!}}{n}$,"For some context, I recently encountered a beautiful limit. $$\lim_{n \to \infty}\frac{\sqrt[n]{n!}}{n}$$ To solve this we begin by taking the natural log of the inside. $$\ln\left(\frac{\sqrt[n]{n!}}{n}\right)=\ln\left(\frac{n!}{n^n}\right)^\frac{1}{n}=\frac{1}{n}\ln\left(\frac{n!}{n^n}\right)$$ Writing out the terms inside the $\ln$ makes it clear that we can express it as a sum. $$\ln\left(\frac{n!}{n^n}\right)=\ln\left(\frac{1\cdot2 \dotsm n}{n\cdot n \dotsm n}\right)=\ln\left(\frac{1}{n}\right)+\ln\left(\frac{2}{n}\right)+\dotsm+\ln\left(\frac{n}{n}\right)=\sum_{i=1}^n\ln\left(\frac{i}{n}\right)$$ So what we are looking for is the following. $$\lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^n\ln\left(\frac{i}{n}\right)$$ Notice that this is the Riemann Sum from $0$ to $1$ of $\ln(x)$ so we have, $$\lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^n\ln\left(\frac{i}{n}\right)=\int_{0}^{1}\ln(x)\ dx$$ The integral can be solved using integration by parts which I will not include here but it evaluates to $-1$ . $$\int_{0}^{1}\ln(x)\ dx=-1$$ But as we took the natural log of the expression, we must undo it by putting it to the power of $e$ . $$\therefore\lim_{n \to \infty}\frac{\sqrt[n]{n!}}{n}=\frac{1}{e}$$ I was wondering if I could use the same kind of steps to make an expression equal to $e^\pi$ . I realized I needed an integral based on $\ln(x)$ which evaluates to $\pi$ and found the following. $$\int_{0}^{\infty}\ln\left(1+\frac{1}{x^2}\right)\ dx=\pi$$ However, the problem here is that this is an improper integral and the upper bound is at $\infty$ . I've been looking for ways to express this as a Riemann Sum but intuitively it does not make sense to me. The point of Riemann Sums was to keep adding infinitely thin boxes infinite times, so is there any room to expand that so that the upper bound is at infinity? Also, if you find any other integrals which can be used to make a nice expression like the one above, please include that too. I would love to solve it. ps. This is my first post and also my first time using latex to write these equations. Any advice or suggestion is welcome. Thank you.","['integration', 'improper-integrals', 'riemann-sum', 'sequences-and-series', 'limits']"
4622241,We have two rooms painted white and one painted grey. In how many ways can we arrange 90,"I have a combinatorics problem, I tried to solve it myself, but I don't know if it's correct. We have two rooms painted white and one painted grey.
In how many ways can we arrange 90 people (30 in each room) so that: it is important for us who is with whom in the room and whether the person is in a white or gray room both white rooms are indistinguishable for us What I tried to do: First select 30 people out of 90 (grey room) i.e.: $\binom{90}{30}$ then draw a group of people to the white room: $\binom{60}{30} \times \binom{30}{30}$ Do divide by 3! (because when I choose 30 people for the gray room, I automatically choose 30 for the white one and 30 for the white one2?) expression : $\binom{90}{30} \times \binom{60}{30} \times \binom{60}{30}$ Please explain how to go about it and what the fact that white rooms are indistinguishable brings to the task","['combinatorics', 'discrete-mathematics']"
4622248,Inscribed triangles and area,"As shown below, $\triangle GHJ$ is inscribed in $\triangle DEF$ , which is inscribed in $\triangle ABC$ . $GH\parallel AB$ , $HJ\parallel BC$ , $JG\parallel AC$ , if $[ABC]=s$ and $[GHJ]=t$ , find $[DEF]$ . (These notations mean area) Let $\begin{cases}\dfrac{BF}{FA}=x,\dfrac{AE}{EC}=z,\dfrac{CD}{DB}=y;\\[2pt]\dfrac{DH}{HF}=m,\dfrac{FG}{GE}=p,\dfrac{EJ}{JD}=n.\end{cases}$ Extend $HJ$ and intersect $AB$ , $AC$ . From $HJ\parallel BC$ , $$\frac{1+\dfrac x{1+m}}{\dfrac{\vphantom1xm}{1+m}}=\frac{z+\dfrac n{n+1}}{\dfrac1{1+n}}.$$ After a lot of work, $\dfrac{m+1}{n+1}=mx\cdot\dfrac{z+1}{x+1}$ . Multiply the three cyclic equations to get $$xyzmnp=1.$$ I think this is useful, but I cannot solve out anything more.","['area', 'geometry']"
4622356,range of closed subset via quotient map,"Let $X$ be a normed vector space. Let $V \subset X$ be a closed subspace, and let $W \subset X$ be a finite-dimensional subspace. Suppose $V \cap W = \{0\}$ and that there exists a $C \in \Bbb{R}$ such that $\lVert x \rVert + \lVert y \rVert ≤ C \lVert x − y \rVert$ for all $x \in V , y \in W$ . Let $q : X \rightarrow X/W$ be the quotient map. I would like to show that $q(V)$ is closed. I tried to use a sequence in order to show that $q(V)$ is sequentially closed but it must not be the right approach because this way, I never needed to use the inequality that is given above, which is weird. Does anyone have an idea ? Thank you very much for your help !","['normed-spaces', 'vector-spaces', 'functional-analysis', 'sequences-and-series', 'inequality']"
4622376,Trying to see that operator space injective norm is dominated by tmax norm.,"A ternary ring of operator (TRO) between two complex Hilbert spaces $H$ and $K$ is defined to be a norm closed subspace $V$ of $B(H, K)$ and satisfies $xy^*z \in V$ for all $x, y, z \in V$ . Let $V$ and $W$ be TROs. A linear map $\pi: V \to W$ is called TRO-homomorphism provided $\pi(xy^*z) =\pi(x) \pi(y) ^*\pi(z) $ for all $x, y, z \in V$ . Motivated by $C^{\ast}$ -algebras, in section $5$ of Kaur and Ruan - Local Properties of Ternary Rings of Operators and Their Linking $C^*$ -Algebras , tmax norm $\|.\|_{\text{tmax}}$ on $V \otimes W$ is defined as follows. For $u= \sum_{i=1}^r v_i \otimes w_i \in V \otimes W$ , the tmax norm $\lVert u\rVert_{\text{tmax}}$ is the supremum of the norm $\| \pi.\sigma(u)\|_{B(H)}=\lVert\sum_{i=1}^{r} \pi(v_i) \sigma(w_{i})\rVert_{B(H)}$ over all pairs of TRO-homomorphism $\pi: V \to B(H)$ , $\sigma: W \to B(H)$ satisfying following commuting conditions $ \pi(v) \sigma(w)=\sigma(w) \pi(v)$ and $ \pi(v) \sigma(w)^*=\sigma(w)^* \pi(v)$ . In the paper Section $5$ , equation $5.2$ following is stated without proof $\| u\|_{\vee} \leq \|u\|_{\text{tmax}}\leq \|u\|_{\wedge}$ where $\|.\|_{\vee}$ and $\|.\|_{\wedge}$ denotes the operator space injective and projective norm respectively. In the paper it is not explained that why $\| u\|_{\vee} \leq \|u\|_{\text{tmax}}$ . I am unable to see this. Can someone please explain this to me? P. S: This question has been posted on mathoverflow also and can be found here .","['operator-theory', 'functional-analysis', 'operator-algebras']"
4622385,"For $G$ finitely generated, $H$ finite, prove $[G:\kappa(G,H)]<\infty$ where $\kappa(G,H):=\cap_{\phi \in{\rm Hom}(G,H)} \ker\phi$","Let: $$\kappa(G,H):=\cap_{\phi \in {\rm Hom}(G,H)} \ker\phi$$ where ${\rm Hom}(G,H)$ is the set of all homomorphisms from $G$ to $H$ . I am trying to prove that if $G$ is a finitely generated group, and $H$ a finite group, then $[G:\kappa(G,H)]<\infty$ . Since I had no idea on how to start proving this generally, I tried proving a simplified version of this, where both $G$ and $H$ are abelian. In this case, I can use the Fundamental Theorem of Finitely Generated Abelian Groups and assume that $$G = \mathbb{Z}/(g_1)\oplus\mathbb{Z}/(g_2)\oplus\dots\mathbb{Z}/(g_k)\oplus\mathbb{Z}^r$$ $$H = \mathbb{Z}/(h_1)\oplus\mathbb{Z}/(h_2)\oplus\dots\oplus\mathbb{Z}/(h_m)$$ I then observed that for all homomorphisms $\phi:G\to H$ we have that $\ker\phi$ has the form: $$N_1 \oplus N_2 \oplus\dots\oplus N_k \oplus m_1\mathbb{Z}\oplus\dots\oplus m_r\mathbb{Z}$$ for some finite normal subgroups $N_i \leq \mathbb{Z}/(g_i)$ and normal subgroups $m_1\mathbb{Z}\oplus\dots\oplus m_r\mathbb{Z} \leq \mathbb{Z}^r$ (we can see this by defining each possible homomorphism using the standard generators (e.g., $(0,\dots,0,1,0\dots0)$ ). Which means that when we take the intersection of all these kernels, we still get something of that form (e.g., instead of $m_1\mathbb{Z}$ we will get the ${\rm lcm}$ of all $m_1$ 's taken across all $\phi$ 's), so that $[G:\kappa(G,H)]<\infty$ ). I am, however, none the wiser about how to generalize this to the non-abelian case . I do have at my disposal two potentially related facts (although I do not see how they might be helpful... so I may be completely off here): For any two groups $G, H$ , we have that $H$ is isomorphic to a quotient of $G$ iff $H$ is isomorphic to a quotient of $G/\kappa(G,H)$ . For any groups $G, G', H$ we have: $\kappa(G\times G', H) = \kappa(G,H)\times\kappa(G', H)$ Any advice?","['group-homomorphism', 'finitely-generated', 'group-theory']"
4622401,Solve $\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}})$ without L'Hopital,"I've seen the solution to $\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}})$ using L'Hopital and I was wondering if there's a way to find out the result without it. My initial attempt was outright stupid of me because I tried to substitute the limit of $\frac{\ln{(1+\frac{1}{x})}}{\frac{1}{x}}$ as $x$ approaches $\infty$ with $1$ , which results in the initial limit being $0$ . That's obviously false as I ignored the fact that I cannot do such a substitution when the limit is in an indeterminate form. That being said, how could you solve this limit without L'Hopital?","['limits', 'calculus', 'limits-without-lhopital']"
4622423,A chain of circles of radius $1/n^p$ is tangent to the $x$-axis. What is the horizontal length of the chain?,"I recently discovered that, if a chain of circles of radius $1/n^2$ , where $n\in\mathbb{N}$ , is tangent to the $x$ -axis, then the the horizontal length of the chain is exactly $2$ . This can be shown by the fact that a circle of radius $1$ is tangent to the other side of the chain (which can be proven by using Descartes' Circle Theorem to show that if three circles of radius $1/n^2, 1$ and $\infty$ are mutually tangent, then a circle of radius $1/(n+1)^2$ is tangent to all three circles). My question seeks to generalize this: If a chain of circles of radius $1/n^p$ , where $n\in\mathbb{N}$ and $p>2$ is an integer constant, is tangent to the $x$ -axis, what is the horizontal length of the chain? My attempt Descartes' Circle Theorem seems to only apply when $p=2$ . For other values of $p$ , I don't know what curve is tangent to the other side of the chain. Using the centres of two neighboring circles and Pythagorus' Theorem, the horizontal length of the chain is $$\sum\limits_{k=1}^\infty 2(k^2+k)^{-p/2}$$ but I don't know how to evaluate this series. To help you visualize, here is a chain of circles of radius $1/n^{3}$ .","['calculus', 'circles', 'geometry', 'tangent-line']"
4622426,Need help with doing a hyperbolic trigonometry problem.,"If $x=2\cos(\alpha)\cosh(\beta)$ and $y=2\sin(\alpha)\sinh(\beta)$ , prove: $$\sec(\alpha+i\beta)+\sec(\alpha-i\beta)=\frac{4x}{(x^2+y^2)}$$ I had an incorrect equation. The iota was missing.","['trigonometry', 'hyperbolic-functions']"
4622445,Can the probability density be defined variationally?,"Define a random variable $X \in \mathbb{R}$ . Suppose that the distribution of $X$ is dominated by the Lebesgue measure and hence admits a density (pdf) $f^*(x)$ at each $x \in \mathbb{R}$ . Is it possible to define $f^*$ variationally? For example, does there exist a function class $\mathcal{F}$ and a loss $L$ such that $f^* = \arg\min_{f \in \mathcal{F}} E[L\{X, f(X)\}]$ ? I'd prefer an (e.g.) $L$ to be as well behaved as possible. I've searched through books and papers online and have not had any luck towards resolving my question.","['convex-optimization', 'probability-theory', 'probability', 'density-function']"
4622499,Is Ted Shifrin's definition (8-2.3) missing the verbalization of the pullback of the product of a function with a basis?,"My question pertains to Ted Shifrin's Multivariable Mathematics Section 8-2.3 Pullbacks. We are to use the following pieces, given in the definition, to build the pullback $\mathbf{g}^{*}\omega\in\mathcal{A}^{k}\left(U\right)$ of $\omega$ by $\mathbf{g}.$ The symbol $\mathbb{I}$ is a multi-index.
The last four lines are definitive, giving the pullback of a function;
the pullback of a basis 1-form; the pullback of a wedge product; and
the pullback of a sum, respectively: \begin{align*}
U\subset & \mathbb{R}^{m}\\
\mathbf{g}: & U\to\mathbb{R}^{n}\\
f: & \mathbb{R}^{n}\to\mathbb{R}\\
\mathbf{g}\left(\mathbf{u}\right)= & \mathbf{x}\\
\mathbf{g}^{*}f= & f\circ\mathbf{g}\\
\mathbf{g}^{*}dx_{i}= & dg_{i}=\sum_{j=1}^{m}\frac{\partial g_{i}}{\partial u_{j}}du_{j}\\
\mathbf{g}^{*}\left(dx_{i_{1}}\wedge\dots\wedge dx_{i_{k}}\right)= & dg_{i_{1}}\wedge\dots\wedge dg_{i_{k}}=d\mathbf{g}_{\mathbb{I}}\\
\mathbf{g}^{*}\left(\sum_{\mathbb{I}}f_{\mathbb{I}}d\mathbf{x}_{\mathbb{I}}\right)= & \sum_{\mathbb{I}}\left(f_{\mathbb{I}}\circ\mathbf{g}\right)d\mathbf{g}_{\mathbb{I}}
\end{align*} Before the final equation he states ""Last, we take the pullback
of the sum to be the sum of the pullbacks."" It may seem pedantic, but this is mathematics. The last equation appears
to define both the pullback of a sum, as well as the pullback of the
product of a scalar function with a basis k-form. I'm left wondering
if this is something I should derive from the previous parts of the definition,
or it is simply an unverbalized part of the final part of the definition.
So which is it? To my way of thinking, that product ""distribution"" rule is the
most significant part of the definition.","['multivariable-calculus', 'definition', 'differential-forms']"
4622509,Find the radius of a circle touching three circles,"As an exercise for myself, I made a diagram where the goal is to solve for $w$ when $x$ , $y$ , $z$ , and $k$ are known. I know this problem already exists but I'm having trouble locating it online: The circle at $D$ is touching the circles at $A$ , $B$ , and $C$ at a single point. To find $w$ , I decided I would calculate the area of quadrilateral $ABCD$ in two different ways: using triangles $ABD + BCD$ and $ACD + ABC$ . I used Heron's formula 4 times: $$\sqrt{(x+2y+z+k)(x+2y+z-k)(x-z+k)(z-x+k)}$$ $$+$$ $$\sqrt{(x+2w+z+k)(x+2w+z-k)(x-z+k)(z-x+k)}$$ $$=$$ $$4\sqrt{xyw(x+y+w)}+4\sqrt{zyw(z+y+w)}$$ Isolating $w$ is proving to be very difficult and I'm getting stuck. I was able to eliminate all the radicals, but the terms explode when I try to expand. Is there an easier way to find the radius of the circle at point $D$ ? Am I missing an obvious clue to make the problem easier?",['geometry']
4622510,Why do $ x$ and $y$ have negative values in this graph of $\tan \theta=y/x$?,"The author defines $\tan (\theta)$ to be $y/x$ where $x$ and $y $ are points on a ray making the angle theta from the origin. Using this definition, it seems that the graph of the equation $\tan(\theta)=y/x$ should only be in the first quadrant if theta is less than $\pi/2$ ; however the author goes on to draw the graph in the third quadrant as well. Why is this the case when the points from which tan(theta) is drawn are only on a ray starting from the origin? So it seems the graph should only exist in one quadrant.",['trigonometry']
4622563,"Nature of critical point $(0,0)$ for the given 2nd order ODE","Consider the 2nd order ODE: $$\frac{d^2y}{d^2t}+2\alpha \frac{dy}{dt}+\beta^2 y=0$$ where $\alpha >\beta>0$ .
Then find the nature of the critical point of the above ODE. First I put $\frac{dy}{dt}=x$ and $\frac{dx}{dt}=-2\alpha x-\beta^2y$ . Then $(0,0)$ is a critical point and the Jacobian matrix is $$J=\left(\begin{matrix} 1 & 0\\-2\alpha & -\beta^2\end{matrix}\right) $$ Roots of the characteristic equation of $J$ are $1$ and $-\beta^2$ , which are real and opposite sign. So, the critical point $(0,0)$ is a saddle point. Is it correct ?","['systems-of-equations', 'ordinary-differential-equations']"
4622592,Showing sufficiency of a statistic (the hard way),"Given $X_1,\dots,X_n$ i.i.d. from $P_\theta$ , the usual way of showing that a statistic $T$ is sufficient is to use the factorization lemma as opposed to computing the distribution of $(X_1,\dots,X_n)|T$ and showing that it is independent of the parameter of interest $\theta$ . Suppose that $X_1,\dots,X_n$ are i.i.d. samples from the uniform distribution of $(0,\theta)$ . I want to compute the distribution of $X|X_{(n)}$ and show that it is independent of $\theta$ . I am struggling with the rigorous justifications here because $X|X_{(n)}$ does not have a density with respect to Lebesgue measure. What is the correct way to derive the distribution here? The following is an attempt: \begin{align*}
&P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t)\\
&=
\sum_{i=1}^nP(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) P(X_{(n)}=i|X_{(n)}=t)\\
&=
\frac{1}{n }
\sum_{i=1}^n P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i)
\end{align*} where the last equality holds by the symmetry of the problem, each $X_i$ is as likely to be the largest as any of the others. Now from the discussion here , we have that $$
X_1,\dots, X_{i-1}, X_{i+1},\dots, X_n|X_{(n)}=X_i= t
$$ is distributed as $n-1$ i.i.d. uniform random variables on $[0,t]$ . So can we then write: \begin{align*}
&P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i)\\
&P(X_1\le x_1,\dots, X_{i-1}\le x_{i-1}, X_{i +1}\le x_{i+1} \le \dots \le X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) \times \mathbf{1}\{x_i = t= X_{(n)}\}\\
&= \mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \}.
\end{align*} Plugging this back in to the above expression yields \begin{align*}
P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t)
&=
\frac{1}{n }
\sum_{i=1}^n 
\mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \}
\end{align*} I'm not completely convinced by the derivation, and I'm not sure what to do next. Intuitively the first indicator in the expression will only be equal to 1 for one of the indices in $i=1,\dots, n$ .","['statistical-inference', 'statistics', 'probability-distributions', 'probability']"
4622623,Find the probability of getting 2 golden coins from a bag of 4 golden coins and 8 iron coins with the following conditions:,"There are 4 golden coins and 8 iron coins in a bag. You select one coin from the bag, if it is a golden coin, you keep it; but if it is an iron coin, you put it back in the bag. Find the probability of earning exactly 2 golden coins after three consecutive attempt. My Try: Golden Coins=4 Iron coins = 8 Total coins = 12 We have 3 cases here: (GIG, GGI, IGG) $ (\frac{4}{12}\times \frac{8}{11}\times \frac{3}{11})+(\frac{4}{12}\times \frac{3}{11}\times \frac{8}{10})+(\frac{8}{12}\times \frac{4}{12}\times \frac{3}{11}) = \frac{362}{1815} $ Is this answer correct?","['conditional-probability', 'probability']"
4622671,Roots and analytic continuation of $T(s)=\sum_{n>0} (n^s + n^{-s})^{-1} $?,"Let $s$ be a complex number. $$T(s)=\sum_{n>0} (n^s + n^{-s})^{-1} $$ This is well defined for $Re(s) > 1$ . It seems $T(s) = T(-s)$ but then again we have it only defined for $Re(s)>1$ for now. We try analytic continuation by $$\sum_{n>0} (n^s + n^{-s})^{-1} - n^{-s} =  - \sum_{n>0} (n^{3s} + n^{s})^{-1}$$ Then assuming $sum_{n>0} n^{-s}$ has interpretation as $\zeta(s)$ then : $T(s)=\sum_{n>0} (n^{3s} + n^s)^{-1} + \zeta(s)$ This now is suppose to be analytic for $Re(s) >1/3$ . We still assume $T(s) = T(-s)$ here. We can continue this proces by noticing $$g(x) = 1/(x + 1/x) = x/(1+x^2) = x - x^3 + x^5 - x^7 + ... = 1/x - 1/x^3 + 1/x^5 - 1/x^7 + ...$$ and $g(1/x) = g(x)$ . Also $$g(x) - 1/x = - 1/(x^3 + x)$$ $$g(x) - 1/x + 1/x^3 = 1/(x^5 + x^3)$$ ... $$g(x) - x = - 1/(x^{-3} + x)$$ $$g(x) - x + x^3 = 1/(x^{-5} + x^{-3})$$ ... So we have error terms too. letting $x = n^{-s}$ we can use this to get the idea for a zeta series expansion : $$T(s) = \zeta(s) - \zeta(3s) + \zeta(5s) - \zeta(7s) + ...$$ And the error term is suppose to go to zero. So we consider the infinite sum and use $T(s) = T(-s)$ : $$T(s) = \zeta(s) - \zeta(3s) + \zeta(5s) - \zeta(7s) + ... = \ T(-s) = \zeta(-s) - \zeta(-3s) + \zeta(-5s) - \zeta(-7s) + ...$$ by symmetry. Notice $$E(s)=\sum_{n>0} (n^{ks} + n^{(k-2)s})^{-1}$$ seems to be well defined and meromorphic for $Re(s) > 1/k$ . We get the feeling since it appears by the above logic that $T(s)$ is defined for $Re(s) > 0$ ; the equation $T(s) = T(-s)$ seems justified. But all of these steps are possibly dubious. changing order of summation , ignoring radius of taylors ( $g(x)$ is expanded in a taylor without mentioning the radius !? ) etc Then again it is very similar to what we do with the Riemann zeta function. Changing the taylor expansion points slowly towards values s with small or negative real part might be more formal and better, but very complicated. I prefer $T(s) = \zeta(s) - \zeta(3s) + ...$ truncated at stopping at substraction. But maybe that zeta expansion is not valid or not so good. What is remarkable is that this $T(s)$ is close to the zeta function for $Re(s) >>1$ . Computing zero's, poles, branches, analytic continuation etc is the main goal. But things are tricky; changing order of summation , fubini like ideas and such might not be justified. Plots are appreciated too :)","['symmetry', 'analytic-continuation', 'roots', 'complex-analysis', 'zeta-functions']"
4622694,Probability of choosing the right coin given the number of tosses,"I'm trying to solve the following problem There are two indistinguishable coins, one of them is balanced and the other one is biased in a way that the probability of getting heads after a toss is twice the probability of getting tails after a toss. We choose one of them and toss it repeatedly. Find the probability of having chosen the balanced coin if i) The coin was tossed $15$ times and $5$ of them were heads. ii) We needed $15$ tosses to get $5$ heads. Here is my attempt: i) Let $B$ be the event of choosing the balanced coin and let $D$ be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable $X$ counting the number of heads obtained. We have $$\mathbb{P}(X=5\mid B)=\binom{15}{5}\left (\frac{1}{2}\right )^5\left (\frac{1}{2}\right )^{15-5}=\frac{3003}{2^{15}}$$ because if $B$ was chosen then $X\sim \operatorname{Bin}\left (15,\frac{1}{2}\right )$ and $$\mathbb{P}(X=5\mid D)=\binom{15}{5}\left (\frac{2}{3}\right )^5\left (\frac{1}{3}\right )^{15-5}=\frac{96096}{3^{15}}$$ because if $D$ was chosen then $X\sim \operatorname{Bin}\left (15,\frac{2}{3}\right )$ . We also have $\mathbb{P}(B)=\mathbb{P}(D)=\frac{1}{2}$ because the coins are indistinguishable. Hence using Bayes' Theorem we find $$\mathbb{P}(B\mid X=5)=\frac{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)}{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)+\mathbb{P}(X=5\mid D)\cdot \mathbb{P}(D)}=\frac{\frac{3003}{2^{15}}}{\frac{96096}{3^{15}}}\approx 0.93$$ as the desired probability. ii) Let $B$ be the event of choosing the balanced coin and let $D$ be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable $X$ counting the number of tosses needed to obtain $5$ heads. We have $$\mathbb{P}(X=15\mid B)=\dbinom{15-1}{5-1}\left (\dfrac{1}{2}\right )^5\left (\dfrac{1}{2}\right )^{15-5}=\dbinom{14}{4}\dfrac{1}{2^{15}}$$ because if $B$ was chosen then $X\sim \operatorname{Pa}\left (\frac{1}{2},5\right )$ and $$\mathbb{P}(X=15\mid D)=\dbinom{15-1}{5-1}\left (\dfrac{2}{3}\right )^5\left (\dfrac{1}{3}\right )^{15-5}=\dbinom{14}{4}\dfrac{2^5}{3^{15}}$$ because if $D$ was chosen then $X\sim \operatorname{Pa}\left (\frac{1}{3},5\right )$ . Then similar computations give the same answer as the first item. I would like to know if my solution is correct.","['solution-verification', 'probability-theory', 'probability']"
4622742,Can we think of the Riemann curvature tensor as a tensor-valued 2-form and integrate it over a surface?,"The usual physical interpretation of the Reimann tensor $R^\mu_{\ \ \nu \rho \sigma}$ at a given point $p$ on a manifold $M$ is that it inputs an infinitesimal vector $v^\nu$ and two other infinitesimal vectors $x^\rho$ and $x^\sigma$ , the latter of which determine an infinitesimal parallelogram-shaped loop in $M$ anchored at $p$ , and outputs the amount by which $v^\nu$ changes after being parallel-transported around the infinitesimal loop. That is, I believe it gives the holonomy of infinitesimal oriented loops with base point $p$ lying in the $x^\rho$ - $x^\sigma$ plane. There are other, equivalent ways to think about what type of mathematical object the Riemann curvature tensor is. E.g. one way to think of it as a self-adjoint operator on the real inner product space of two-forms. (This interpretation is arguably more natural for the ""all-lowered"" version $R_{\mu \nu \rho \sigma}$ .) Is it also possible to think of the Riemann tensor as a linear-operator-valued two-form in its last two indices $\rho$ and $\sigma$ , and therefore integrate it over a 2D surface in the manifold? I've only ever seen the Riemann tensor described in terms of infinitesimal loops in the manifold. But is it legitimate to think of it as a two-form and integrate it (over $\rho$ and $\sigma$ ) over a finite surface $S$ , and get the holonomy of the oriented boundary loop $\partial S$ ? On the one hand, this seems plausible by the general heuristic picture for Green's theorem: On the other hand, I doubt that it's correct, because (a) I can't find any sources saying that it is, and (b) more concretely, it seems to me that the holonomy of an oriented loop $\partial S$ should depend on the base point (although I'm not sure about that). But the formal integral $$
\iint_S R^\mu_{\ \ \nu \rho \sigma} dx^\rho \wedge dx^\sigma
$$ does not pick out any distinguished point on $\partial S$ . Is there any sense in which this is a legitimate operation? If not, is there any other way to construct the (pseudo-)Riemannian holonomy of arbitrary oriented loops (with base point) in $M$ from the Riemann tensor? Or does that require knowing the connection and/or the full metric tensor? I know very little about tensor-valued differential forms, so this might all be complete nonsense. I suspect the fact that the Riemann tensor is an actual tensor means that this doesn't work, because for tensor-valued forms the ""form"" indices and the ""tensor"" indices somehow transform differently under coordinate transformations. Edit: Apparently the Riemann tensor can be thought of as a Lie-algebra-valued two-form (that takes values in $\mathfrak{so}(n)$ ) called the curvature form . I don't know how integration of Lie-algebra-valued differential forms works, though.","['curvature', 'holonomy', 'differential-forms', 'differential-geometry']"
4622757,"For orthogonal matrices $A$ and $B$, prove $\det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B)$","I can't prove this formula: $$
\det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B)
$$ I tried using fact that $A^{t}A = I$ (similarly for $B$ ): $$
A^{t}B-B^{t}A=A^{t}B-B^{t}A + A^{t}A - B^{t}B=A^{t}(A+B)+B^{t}(A - B)
$$ I don't understand how I should continue, and don't see an alternative method.","['orthogonal-matrices', 'determinant', 'linear-algebra']"
4622788,Integration of $\ln(1+\tan(x))$.,How can I integrate $$\int_{0}^{\frac{\pi}{2}}\ln\left(1+\tan(x) \right))dx?$$ I can integrate $$\int_{0}^{\frac{\pi}{4}}(\ln(1+\tan(x)) dx = \frac{\pi}{8} \ln(2).$$ However the technique can not be used when the integrand from $0$ to $\frac{\pi}{2}.$ Any advice would be appreciate. Have a good day.,"['integration', 'calculus']"
4622810,Confused about first order ODE solution using quadratic equation,"I am working a few problems in my textbook for practice and I came across this problem: $y' = \frac{x}{y+2}$ Proceeding as usual: $\frac{dy}{dx} = \frac{x}{y+2}$ Separating the equation: $(y+2)dy = xdx$ Taking the integral of both sides: $\frac{1}{2}y^2 + 2y = \frac{1}{2}x^2 + C$ Simplifying slightly: $y^2 + 4y = x^2 + 2C$ Now, after finding the worked solution for this problem (I was lost at how to resolve this): $y^2 + 4y - 2C -x^2 = 0$ From this point the solution seems to treat both 2C and x^2 as part of the constant term of a quadratic equation: $\frac{-4 \pm \sqrt{16 + 8C + 4x^2}}{2}$ $-2 \pm \sqrt{8+4C+x^2}$ Which is then simplified with a variable: $-2 \pm \sqrt{x^2 + E}$ My question for this is what allowed the two terms (one of them being the independent variable) to be moved over and treated like a constant together with the constant C? Perhaps I am just getting lost in notation but in a quadratic equation: $Ax^2 + Bx + C = 0$ The C is implied to be just a constant. What am I missing here?",['ordinary-differential-equations']
4622825,"Limit in a spiral of circles of radius $1, 1/2, 1/3, ...$","Draw circles of radius $1, \frac12, \frac13, ...$ such that the first two are externally tangent, then starting with the third, each circle is externally tangent to the previous two, with the path of the circle's centres turning in the same direction (clockwise say). Let $d_n=$ distance between the first circle's centre and the $n$ th circle's centre. Is there a closed form for $L=\lim\limits_{n\to\infty}d_n$ ? $L$ is the length of the red line segment below. I superimposed cartesian axes and tried to express the coordinates of the $n^{\text{th}}$ circle's centre in terms of the coordinates of the previous two circles' centres, then take the limit as $n\to\infty$ . But the algebra seems to be hopelessly complicated. By manually drawing circles on desmos, it seems that $L\approx1.116$ . Maybe $L=\frac{2}{\ln{6}}$ ?","['limits', 'circles', 'geometry']"
4622843,Differentiable Functions Multivariate Definition,"The book ""Nonlinear Programming"" by Bazaraa, Sherali, and Shetty has the following definition in its appendix: Let $S$ be a nonempty set in $\mathbb{R}^n$ , $\bar{x} \in \operatorname{int} S$ and let $f:S\to \mathbb{R}$ . Then $f$ is said to be differentiable at $\bar{x}$ if there is a vector $\nabla f (\bar{x})$ in $\mathbb{R}^n$ called the gradient of $f$ at $\bar{x}$ and a function $\beta$ satisfying $\beta (\bar{x};x) \to 0$ as $x \to \bar{x}$ such that \begin{align*}
f(x) = f(\bar{x}) + \nabla f(\bar{x})^t(x-\bar{x}) + \|x-\bar{x}\|\beta (\bar{x}; x) \quad \forall x \in S.
\end{align*} The gradient vector consists of the partial derivatives, that is, \begin{align*}
\nabla f (\bar{x})^t = \left(\frac{\partial f(\bar{x})}{\partial x_1}, \frac{\partial f (\bar{x})}{\partial x_2}, \ldots, \frac{\partial f(\bar{x})}{\partial x_n} \right).
\end{align*} Can someone please explain why this makes sense and where this definition comes from? I've looked in two analysis books and did not see this definition. And what does the semicolon mean in this case in "" $\beta (\bar{x}; x)$ ""?","['optimization', 'multivariable-calculus', 'analysis']"
4622887,Limit of $u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}$,"I want to find the limit of this sequence if it exists : $$
u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}
$$ My attempt is to first remark that for $k\in\{1,...,n\}$ : $$
\frac{1}{\sqrt{n^2+2n}}\leq\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{n^2+2}}\implies\frac{n}{\sqrt{n^2+2n}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{n}{\sqrt{n^2+2}}
$$ Which leads to : $$
\frac{1}{\sqrt{1+\frac{2}{n}}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{1+\frac{2}{n^2}}}\implies\lim\limits_{n\to\infty}u_n=\lim\limits_{n\to\infty}\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}} = 1
$$ The last implication follows from the fact that the square root function is continuous at $x = 1$ and the squeeze theorem. I would like to know first if my attempt is correct and if you have other ideas to show this that can be enlightening ! Thank you a lot","['calculus', 'solution-verification', 'analysis', 'sequences-and-series']"
4622890,The sum of the numbers from $100$ to $999$ that do not have the digit $0$ as well as do not have repeated digit.,"Considering the numbers from $100$ to $999$ . Excluding numbers that have the digit $0$ , also excluding numbers that have repeated digit. What is the sum of remaining numbers? That is, we need to find $123+124+125+\cdots+129+132+134+135+136+\cdots+139+142+143+145+146+147+\cdots+987$ Using Microsoft Excel, I found that the answer is $279720$ . But I do not know if the calculation can be done in a purely mathematical way (without using any software). I hope you can provide some hints. Your help would be appreciated. THANKS!","['algebra-precalculus', 'decimal-expansion', 'summation', 'sequences-and-series']"
4622914,"$\mathcal{O}$ is finite. If $U,V\in\mathcal{O}$, $U\cap V\in\mathcal{O}$ and $U\cup V\in\mathcal{O}$. Then $(S,\mathcal{O})$ is a topological space.","Prove the following proposition: Let $S$ be a set. Let $\mathcal{O}$ be a finite set whose elements are subsets of $S$ . Let $\emptyset\in\mathcal{O}$ . Let $S\in\mathcal{O}$ . If $U,V\in\mathcal{O}$ , then $U\cap V\in\mathcal{O}$ . If $U,V\in\mathcal{O}$ , then $U\cup V\in\mathcal{O}$ . Then, $(S,\mathcal{O})$ is a topological space. My proof: Since $\mathcal{O}$ is a finite set, we can write $\mathcal{O}=\{O_1,\dots,O_n\}$ . We must show the following: If $U_\lambda\in\mathcal{O}$ for each $\lambda\in\Lambda$ , then $\bigcup_{\lambda\in\Lambda}U_\lambda\in\mathcal{O}$ . Let $f$ be a function from $\Lambda$ to $\mathcal{O}$ such that $f(\lambda)=O_\lambda$ . Since $\mathcal{O}$ is a finite set, $f(\Lambda)$ is also a finite set. $\bigcup_{\lambda\in\Lambda}U_\lambda=\bigcup_{\lambda\in\Lambda}f(\lambda)=\bigcup f(\Lambda)$ . By the mathematical induction, $\bigcup f(\Lambda)\in\mathcal{O}$ . Is my proof ok or not?","['elementary-set-theory', 'general-topology', 'solution-verification']"
4622925,Another strange limit which seems to converges to $\gamma$,"Hi in trying to show that the Euler Mascheroni constant is irrational or not I find empiricaly : $$\lim_{n\to\infty,x\to 0}f_n(x)=\lim_{n\to\infty,x\to 0}\frac{x!!!...!^{x!!...!^{x!...!^{...^{x!}}}}-x!^{x!!^{x!!!^{...^{x!!!...!}}}}}{x}=^?\gamma=0.5772...$$ Some explanation : In the case $n=2$ we have : $$f_2(x)=\frac{x!!^{x!}-x!^{x!!}}{x}$$ In the case $n=3$ we have : $$f_3(x)=\frac{x!!!^{x!!^{x!}}-x!^{x!!^{x!!!}}}{x}$$ And so on... On the other hand we have : $$x!=\Gamma(x+1),x!!=\Gamma(\Gamma(x+1)+1)...$$ As clue we have : Let : $g(x)=\frac{d}{dx}x!$ Then : $$-g(0)=\gamma$$ When $k=2n$ , $n\geq 3$ an integer  it seems we have : $$\lim_{x\to 0}f_k=(-1+(1-\gamma)^{k-1})\gamma$$ And : $$\lim_{x\to 0}\frac{\left(x!!\right)^{ax!}-\left(x!\right)^{ax!!}}{x}=\gamma^2a$$ So there is a possible induction here . How to (dis)prove it ?","['function-and-relation-composition', 'gamma-function', 'euler-mascheroni-constant', 'limits', 'derivatives']"
4622972,Vanishing first Chern class implies existence of flat connection,"Edit : In the exercise I tried to solve it wasn't stated explicitly that we were dealing with line bundles (though the usage of the symbol $L$ for the vector bundle was a strong suggestion that this was the case), hence why my ponderings in the question post are about vector bundles of arbitrary rank. Let $M$ be a smooth manifold, $E$ a complex rank $r$ vector bundle on $M$ . My definition of the first Chern class $c_1(E)$ of $E$ is $$
c_1(E)=\left[\text{tr}\left(\frac{i}{2\pi}F^\nabla\right)\right] \in H_{\text{dR}}^2(M)
$$ where $F^\nabla$ is the curvature tensor corresponding to any connection $\nabla$ on $E$ . If $E$ admits a flat connection $\nabla$ , then $F^\nabla=0$ , and hence $c_1(E)=0$ . I want to prove that the converse is true too: if $c_1(E)=0$ , then $E$ admits a flat connection. I know the identity $c_1(E)=c_1(\Lambda^r E)$ . Since $\Lambda^r E$ is a line bundle, $c_1(\Lambda^r E)$ is simply the cohomology class of the curvature $F^{\nabla^{\Lambda^r E}}$ , where $\nabla^E$ is some connection on $E$ and $\nabla^{\Lambda^r E}$ is the induced connection on $\Lambda^r E$ . So $c_1(\Lambda^r E)=0$ means that $F^{\nabla^{\Lambda^r E}}$ is an exact $2$ -form. Maybe I should first show the case that if a complex line bundle $L$ has vanishing first Chern class, then $L$ admits a flat connection, and maybe I can then show that a flat connection on $\Lambda^r E$ is induced by a flat connection on $E$ ? Any hints are welcome. Context: all I know about characteristic classes is what's treated in chapter 5 of Tu's Differential Geometry book, which I've learned is not the modern way of doing things. And see below the exercise that I'm trying to solve.","['characteristic-classes', 'differential-geometry']"
4622990,Simplifying $\tan^{-1}\frac{a\sin{x}+b\cos{x}}{a\cos{x}-b\sin{x}}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question How do I get: $$\tan^{-1}\left(\frac{b}{a}\right)+x$$ from $$\tan^{-1}\frac{a\sin{x}+b\cos{x}}{a\cos{x}-b\sin{x}}$$ ? There doesn't seem to be anything left to factor and the only identity use I can think of is the that gives me tangent terms in the equation but that didn't lead me anywhere. Any hint would be appreciated. (Edit: I was simply stuck on this identity that I encountered in a book on dynamical systems so I figured I'd ask here. Not sure what more details I could provide as it's just a trig problem. I hope it doesn't get deleted since the visual hint provided below and selected as best answer could be helpful to others.)","['trigonometry', 'calculus', 'algebra-precalculus']"
4623001,Connected components of $\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\}$,How many connected components does the space $\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\}$ have? I tried writing out the conditions that arise from $A^2 \neq 0$ explicitly for a general $A \in M_n(\mathbb{R})$ however I do not understand how the surfaces that they generate intersect in the space.,"['general-topology', 'linear-algebra']"
4623022,Can we make homology from interior derivative (interior product)?,"I have a question that I have been curious about for years. In differential geometry, since the exterior derivative satisfies property $d^2=0$ , we can make a de Rham cohomology from it. Then if we write $\iota_X:\Omega^n\rightarrow\Omega^{n-1}$ as the interior derivative(also called as interior product) for a vector field $X$ , then $\iota_X^2=0$ holds. Can we make a homology for a suitable vector field $X$ from this? And if you can create such a homology, are there any useful properties about it? Like the de Rham theorem. I would really appreciate it if you could let me know.","['de-rham-cohomology', 'smooth-manifolds', 'algebraic-topology', 'differential-geometry']"
4623023,Prove without using L'Hopital's rule that $f''(a)=0$ if $\lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0$,"I'm trying to prove without using L'Hopital's rule that $f''(a)=0$ if $$\lim_{x\to a}{\frac{f(x)-f(a)}{(x-a)^2}}=0,$$ assuming $f$ is twice differentiable and $f''$ is continuous. My first instinct was to use the mean value theorem to define a real number $t$ such that $f'(t)=\frac{f(x)-f(a)}{x-a}$ , which gives $\lim_{x\to a}{\frac{f'(t)}{x-a}}=0$ . Since $t\to a$ as $x\to a$ , this gives $f'(a)=0$ , and $$\lim_{x\to a}{\frac{f'(t)}{x-a}}=\lim_{x\to a}{\frac{f'(t)}{f'(x)}}\times\lim_{x\to a}{\frac{f'(x)}{x-a}}=1\times f''(a)=0,$$ thus $f''(a)=0$ . I wasn't entirely convinced, however, and quickly found counterexamples such as $\lim_{x\to0}{\frac{\sin x^3}{\sin x}}=0$ (this doesn't exactly satisfy the mean value theorem, but I used this to disprove that $\lim_{x\to a}{\frac{f'(t)}{f'(x)}}=1$ in general). Are there any additional steps I should be taking? Or should I take an entirely different approach (if even possible without L'Hopital's rule)?","['limits-without-lhopital', 'mean-value-theorem', 'calculus', 'derivatives']"
4623024,Why doesn't my approach work for this probability question?,"So there's this dungeons and dragons themed question where you're a wizard facing 6 trolls. The trolls spawn with 1d4 health (meaning that their health points are chosen randomly from 1-4) and the wizard is able to attack all of them with a fireball that deals 2d2 damage (the damage is calculated by randomly choosing two numbers from the set {1,2} and adding them together). The question asks to find the probability that all of the trolls are killed. The way I approached this problem is by looking at one troll and using case decomposition. If the troll has 1 or 2 health points, then no matter what I roll for the fireball, it will be killed. If it has 3 health points, there's only one way it survives: if I roll a 1,1 so there's a 75% I kill it in this case. And if it has 4 health points, I can only kill it if I roll a 2,2 => 25% chance. So the probability I kill a single troll is 75% and for the wizard to kill 6 trolls, the probability must be (0.75)^6 = 0.17798. To check my answer, I simulated the scenario in Matlab as shown below: %this function calculates the probability that the fireball
%killed all of the trolls

function prob = probAllKilled()
    N = 10e6;
    
    count = 0;
    
    %for each simulation
    for i = 1:N
        
        %set the damage for the fireball
        damage = randi([1, 2]) + randi([1, 2]);

        %we generate the 6 trolls and check how many were killed by the 
        %fireball. If all were killed, we increment the count
        if sum((randi([1, 4], 6, 1) - damage) <= 0) == 6
            count = count + 1;
        end
        
    end
    %then we divide by N to get the probability
    prob = count/N;

end However, this gave me that the probability is 0.34295 which is nearly twice what I got on paper. I found out that in my calculations, I must have assumed that I was rolling for the fireball damage for each troll. To check this, I modified my code so that it matches my assumption: function prob = probAllKilled()
    N = 10e6;
    
    count = 0;
    
    %for each simulation
    for i = 1:N
        
        %set the damage for the fireball
        %damage = randi([1, 2]) + randi([1, 2]);

        %we generate the 6 trolls and check how many were killed by the 
        %fireball. If all were killed, we increment the count
        if sum((randi([1, 4], 6, 1) - randi([1, 2], 6, 1) - randi([1, 2], 6, 1) ) <= 0) == 6
            count = count + 1;
        end
        
    end
    %then we divide by N to get the probability
    prob = count/N;

end and that gave me a probability of 0.17807. I don't really understand where in my calculations I made the assumption that I was rolling every time I dealt damage to a troll. And I'm not really sure how I should approach the problem differently so that I don't make that assumption.","['recreational-mathematics', 'discrete-mathematics', 'problem-solving', 'probability']"
4623053,Why do we write $P\ll Q$ and why is $P$ called absolutely continuous w.r.t. $Q$?,"Let $P$ and $Q$ be two measures on a measurable space $(\Omega,\Sigma)$ . My book, Stochastic Finance by Föllmer and Schied , says that if $$\forall A\in\Sigma: Q(A)=0\Rightarrow P(A)=0$$ then $P$ is called absolutely continuous w.r.t. $Q$ and we write $P \ll Q$ . Why? Obviously continuity already has a clear meaning in mathematics and $x \ll y$ is sometimes informally used to say that $y$ is much larger than $x$ . I assume that there are good reasons for this, can someone please elaborate?","['notation', 'measure-theory', 'absolute-continuity', 'terminology']"
4623058,Every matrix can be changed to a symmetric matrix through elementary column operations,"The following question is given in a section 2 lecture of linear algebra. The first section is about polynomial, so the lectures just started to talk about determinants and matrices. Let $A$ be an $n\times n$ matrix over a number field $F$ . Then there exists an invertible matrix $R$ such that $AR$ is symmetric. I know that this question can be (elegantly) eliminated using Jordan canonical form. But since the question is left to who just learn linear algebra, I don’t think Jordan form is necessarily required. Then the question can be interpreted as the following: Let $A$ be an $n\times n$ matrix over a number field $F$ . Then $A$ can be changed to a symmetric matrix through elementary column operations. The Jordan form method only establishes the existence of some invertible matrix satisfying this property, which (I think) makes it unclear how to relate it with row/column operations. I think it may be dealt with by induction. Am I right? It is not very clear to me how to complete the inductive steps. Any help is sincerely appreciated.",['linear-algebra']
4623066,Solving for third derivative of implicit differentiation,"My professor gave us an activity of, to me what feels like, a vague third derivative implicit differentiation. We were taught up to second, but now I am feeling lost. I am tasked to find $\frac{d^3y}{dx^3}$ , $x^2$ + $y^2$ = $a^2$ . I treated $a^2$ as a constant like what tutorial videos online did, and arrived at a different answer: $\frac{x}{27y^5}$ . None of the choices match this in the question given to me, they all include a in them. Am I looking at $a^2$ differently here?",['derivatives']
4623067,Independence of function of random varibles,"If there are two independent random variables, $X_1$ and $X_2$ , then I define two other random variables $Y_1$ and $Y_2$ , where $Y_1 = f_1(X_1,X_2)$ and $Y_2 = f_2(X_1,X_2)$ . Can $Y_1$ and $Y_2$ be deemed independent? If there is dependence, what would be the penalty for neglecting this dependence? Does the inference change if $f_1$ and $f_2$ are Boolean functions?","['probability-theory', 'probability']"
4623119,Shouldn't two different expressions for a general antiderivative be equal?,"For the integral $$\int 2 \cos x \sin x \text{d}x,$$ a $u$ -substitution with $u = \sin x$ leads to $$\int 2 u \text{d}u = 2 \cdot \frac{u^2}{2}+C=  u^{2}+C = \sin ^{2}x + C.$$ On the other hand, using the double-angle formula $2 \sin x \cos x = \sin \left(2x\right)$ , the original integral can be evaluated as $$\int \sin \left(2x \right)\text{d}x = -\frac{\cos \left(2x\right)}{2}+D.$$ I was expecting to be able to find a relationship between the two antiderivatives in the following way: $$\sin ^{2} x = -\frac{\cos \left(2x\right)}{2}+E.$$ But $\sin ^{2} x +\frac{\cos \left(2x\right)}{2}$ is not a constant. The reason that I was expecting to be able to find a relationship between the two answers due to the fact that they are general solutions to the ODE $y' = 2\sin x \cos x$ and that transitivity of equality should imply that the two antiderivatives are equal.","['integration', 'calculus', 'ordinary-differential-equations']"
4623144,"Rudin's RCA, Theorem $4.12 $","There are things that we need for the proof of the theorem: ] 1 There is the theorem: If $L$ is a continuous linear functional on $H$ , then there is a unique $y$ $\in$ $H$ such that $Lx$ $=$ $(x,y)$ ( $x$ $\in$ $H$ ). There is the proof: If $Lx$ $=$ $0$ for all $x$ , take $y$ $=$ $0$ . Otherwise, define $M$ $=$ ${x: Lx = 0 }$ . The linearity of $L$ shows that $M$ is a subspace. The continuity of $L$ shows that $M$ is closed. Since $Lx$ $\neq$ $0$ for some $x$ $\in$ $H$ , Theorem $4.11$ shows that $M^\bot$ does not consist of $0$ alone. Hence there exists $z$ $\in$ $M^\bot$ , with $||z||$ $=$ $1$ . Put $u$ $=$ $(Lx)z$ $-$ $(Lz)x$ . Since $Lu$ $=$ $(Lx)(Lz)$ $-$ $(Lz)(Lx)$ $=$ $0$ , we have $u$ $\in$ $M$ . Thus $(u,z)$ $=$ $0$ . This gives $Lx$ $=$ $(Lx)$$(z,z)$ $=$ $(Lz)$$(x,z)$ . Thus the theorem holds with $y$ $=$ $\alpha z$ , where $\bar \alpha$ $=$ $Lz$ . I don't understand why do we get $y$ $=$ $0$ if $Lx$ $=$ $0$ for all $x$ . I also don't understand why is $Lu$ equal of $(Lx)(Lz)$ $-$ $(Lz)(Lx)$ and how does it give $Lx$ $=$ $(Lx)$$(z,z)$ $=$ $(Lz)$$(x,z)$ , and why does theorem hold when $y$ $=$ $\alpha z$ , where $\bar \alpha$ $=$ $Lz$ ? Any help would be appreciated.","['analysis', 'real-analysis', 'complex-analysis', 'hilbert-spaces', 'linear-transformations']"
4623161,Non-existence of Lebesgue probability densities,"A standard result taught in mathematical statistics courses is that the multivariate gaussian only has a density if the covariance matrix is non singular: i.e. if $X \sim N_p(\mu, \Sigma)$ and $\Sigma$ is nonsingular, then the (Lebesgue) density is given by $$
f(x) = (2\pi)^{-p/2} |\Sigma|^{-1/2} \exp (-1/2 (x-\mu)^T \Sigma (x-\mu)).
$$ Given an i.i.d. sample from a univariate Gaussian: $X_1,\dots,X_n \sim N(\mu, 1)$ , and taking $\bar{X}$ be the sample average, it is straight forward to show that $(X_1,\dots,X_n)|\bar{X} \sim N(\bar{X} 1_n, I_n - n^{-1}1_n1_n^T) $ where $1_n$ is the $n$ -dimensional vector of ones, and $I_n$ is the $n\times n$ identity matrix. Here it is straight forward to see that the covariance matrix is rank deficient and so $X|\bar{X}$ does not have a Lebesgue density. My question is: more generally (in non Gaussian settings say), how can I verify whether a Lebesgue density exists for a given random vector? Is it sufficient compute the covariance matrix and verify that it is full rank? or are there cases where the covariance is full rank but the Lebesgue density still does not exist?","['statistical-inference', 'statistics', 'probability-theory', 'probability', 'density-function']"
4623182,"Inside a limit, when can you substitute terms?","To illustrate what I'm trying to say in the title, consider the following limit. $$\lim_{x\to 0} \frac{\frac{\sin x}{x}-1}{x}$$ This, you might notice, is a derivative, but that is not relevant, it's simply an example I have at hand right now. My initial thought to solve such a limit, as $\lim_{x\to 0}\frac{\sin x}{x}= 1$ , is to write $\lim_{x\to 0}\frac{1-1}{x}=\lim_{x\to 0}\frac{0}{x} = 0$ , which IS correct, that is the value of the limit but I'm 90% sure it is only an accident this approach works. Instead, what I'm sure is correct is $$\lim_{x\to 0}\frac{\frac{\sin x}{x}-1}{x} = \lim_{x\to 0} \frac{\sin x - x}{x^2}=\left(\frac00\right)\underset{\overbrace{\text{ so I can apply L'Hopital}}}{=}\\=\lim_{x\to 0} \frac{\cos x - 1}{2x} =\lim_{x\to 0} \frac{-\sin x}{2} = 0$$ My question is quite basic: when am I allowed to substitute terms with well-known limits, like I tried to in the first ""solution""? Was that first attempt correct or is it a coincidence the result is also $0$ , like I suspect? My teacher tried to explain this, but he was rather vague. I think he mentioned that finite factors that are not equal to $0$ can be given the value of their limit and even brought in front of the limit as a normal number. What about when there is a fraction and the denominator's limit is $0$ ? What are some other cases I need to be aware of? I'm trying to fundamentally understand what I'm allowed to do and what is illegal, so to speak, so any help is much appreciated!","['limits', 'calculus']"
4623207,Is $\int_0^{\pi/2}\Re\left\{\ln^a(1\pm e^{2ix})\right\}dx=0$ a new result?,"I managed to prove $$\int_0^{\pi/2}\Re\left\{\ln^a(1\pm e^{2ix})\right\}dx=0,\qquad a=1,2,3,....$$ Is it a new result? and can be proved in a different way? Proof: We start with finding the derivative of the binomial coefficient: \begin{gather*}
\frac{\partial}{\partial m}\binom{m+n-1}{n}=\frac{\partial}{\partial m}\frac{\Gamma(m+n)}{\Gamma(n+1)\Gamma(m)}\\
\left\{\text{use $\frac{d}{dx}\Gamma(x)=\Gamma(x)\psi(x)$}\right\}\\
=\frac{\Gamma(m)\Gamma(m+n)\psi(m+n)-\Gamma(m+n)\Gamma(m)\psi(m)}{\Gamma(n+1)\Gamma^2(m)}\\
=\frac{\Gamma(m+n)}{\Gamma(n+1)\Gamma(m)}\left(\psi(m+n)-\psi(m)\right)\\
=\binom{m+n-1}{n}\left(\psi(m+n)-\psi(m)\right).
\end{gather*} By using $\displaystyle\psi(x)=H_{x-1}-\gamma=\sum_{k=1}^{x-1}\frac1k-\gamma$ , we have \begin{gather}
\psi(m+n)-\psi(m)=\sum_{k=1}^{m+n-1}\frac1k-\sum_{k=1}^{m-1}\frac1k\nonumber\\
\left\{\text{use $\sum_{k=1}^{a+b} f_k=\sum_{k=1}^{a-1}f_k+\sum_{k=a}^{a+b} f_k$ for the first sum}\right\}\nonumber\\
=\sum_{k=1}^{m-1}\frac1k+\sum_{k=m}^{m+n-1}\frac1k-\sum_{k=1}^{m-1}\frac1k\nonumber\\
=\sum_{k=m}^{m+n-1}\frac1k=\sum_{k=0}^{n-1}\frac1{k+m}.\label{psi(m+n)-psi(m)}
\end{gather} Plugging this result back in, we get \begin{equation}
\frac{\partial}{\partial m}\binom{m+n-1}{n}=\sum_{k=0}^{n-1}\frac{\binom{m+n-1}{n}}{k+m}.\label{rec12}
\end{equation} Keep differentiating w.r.t $m$ : \begin{equation*}
\frac{\partial^2}{\partial m^2}\binom{m+n-1}{n}=-\sum_{k=0}^{n-1} \frac{\binom{m+n-1}{n}}{(k+m)^2}+\sum_{k=0}^{n-1} \frac{\frac{\partial}{\partial m}\binom{m+n-1}{n}}{k+m},
\end{equation*} \begin{gather*}
\frac{\partial^3}{\partial m^3}\binom{m+n-1}{n}=2\sum_{k=0}^{n-1} \frac{\binom{m+n-1}{n}}{(k+m)^3}-2\sum_{k=0}^{n-1} \frac{\frac{\partial}{\partial m}\binom{m+n-1}{n}}{(k+m)^2}\\
+\sum_{k=0}^{n-1} \frac{\frac{\partial^2}{\partial m^2}\binom{m+n-1}{n}}{k+m},
\end{gather*} \begin{gather*}
\frac{\partial^4}{\partial m^4}\binom{m+n-1}{n}=-6\sum_{k=0}^{n-1} \frac{\binom{m+n-1}{n}}{(k+m)^4}+6\sum_{k=0}^{n-1} \frac{\frac{\partial}{\partial m}\binom{m+n-1}{n}}{(k+m)^3}\\
-3\sum_{k=0}^{n-1} \frac{\frac{\partial^2}{\partial m^2}\binom{m+n-1}{n}}{(k+m)^2}+\sum_{k=0}^{n-1} \frac{\frac{\partial^3}{\partial m^3}\binom{m+n-1}{n}}{k+m},
\end{gather*} which can be generalized to \begin{equation*}
\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\sum_{k=0}^{n-1} \frac{\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}}{(k+m)^{a-j}}.
\end{equation*} Let $m\to1$ and define $\displaystyle\lim_{m\to1}\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=f(a,n)$ observing that \begin{gather*}
\lim_{m\to 1}\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}=\sum_{k=0}^{n-1} \frac{1}{(k+1)^{a-j}}=\sum_{k=1}^{n} \frac{1}{k^{a-j}}=H_n^{(a-j)}; \\ 
f(0,n)=\lim_{m\to 1}\binom{m+n-1}{n}=\binom{n}{n}=1,
\end{gather*} we get \begin{equation}
f(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}H_n^{(a-j)}f(j,n),\quad f(0,n)=1.\tag{1}
\end{equation} On the other hand, we have \begin{equation*}
\frac{1}{(1-x)^m}=(1-x)^{-m}=\sum_{n=0}^\infty \binom{m+n-1}{n}x^n.
\end{equation*} Take the $a$ -th derivative of both sides w.r.t $m$ , \begin{equation*}
(-1)^a\frac{\ln^a(1-x)}{(1-x)^m}=\sum_{n=0}^\infty \frac{\partial^a}{\partial m^a}\binom{m+n-1}{n} x^n.
\end{equation*} Let $m\to 1$ and remember that $\displaystyle\lim_{m\to1}\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=f(a,n)$ , \begin{equation}
(-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty f(a,n) x^n,\tag{2}
\end{equation} Integrate both sides of (2), \begin{equation}
\frac{(-1)^{a+1}}{a+1}\ln^{a+1}(1-x)=\sum_{n=0}^\infty f(a,n)\frac{x^{n+1}}{n+1}=\sum_{n=1}^\infty f(a,n-1)\frac{x^{n}}{n}\tag{3}
\end{equation} Replace $x=-e^{2ix}$ in (3) and consider the real parts then integrate from $x=0\to \pi/2$ \begin{equation}
\frac{(-1)^{a+1}}{a+1}\int_0^{\pi/2}\Re\left\{\ln^{a+1}(1+e^{2ix})\right\}dx=\sum_{n=1}^\infty \frac{f(a,n-1)(-1)^n}{n}\int_0^{\pi/2}\Re\left\{e^{2inx}\right\}dx\\
=\sum_{n=1}^\infty \frac{f(a,n-1)(-1)^n}{n}\int_0^{\pi/2}\cos(2nx)dx\\
=\sum_{n=1}^\infty \frac{f(a,n-1)(-1)^n}{n}.\frac{\sin(n\pi)}{2n}=0.
\end{equation} and if we replace $x=e^{2ix}$ , we get \begin{equation}
\int_0^{\pi/2}\Re\left\{\ln^{a+1}(1-e^{2ix})\right\}dx=0.
\end{equation} Thus $$\int_0^{\pi/2}\Re\left\{\ln^a(1\pm e^{2ix})\right\}dx=0.$$ Examples of (2): \begin{gather}
-\frac{\ln(1-x)}{1-x}=\sum_{n=1}^\infty H_n x^n;\\
\frac{\ln^2(1-x)}{1-x}=\sum_{n=1}^\infty (H_n^2-H_n^{(2)})x^{n};\\
-\frac{\ln^3(1-x)}{1-x}=\sum_{n=1}^\infty\left(H_n^3-3H_nH_n^{(2)}+2H_n^{(3)}\right)x^n.
\end{gather}","['integration', 'complex-analysis', 'alternative-proof', 'sequences-and-series', 'induction']"
4623209,How to calculate the distance of an object,"I have two screenshots (1920x1080) of a game, one with a 348-pixel-tall object that is 1 meter distant from the camera, and the other with a 138-pixel-tall version of the same thing. Given that the camera's field of vision is 90 degrees in the second screenshot, how can I precisely measure the object's distance from the camera? I tried using a formula to determine the object's distance from the camera based on the object's height and camera distance, but the results were inaccurate.",['geometry']
4623216,Showing a bound of the second derivative using remainder in integral form,"Let $a,b \in (0,\infty)$ Let $f$ be twice continuously differentiable , $f(0)=f'(0)=f'(a)=0, f(a)=b.$ Show that there exists $c \in (0,a)$ , such that $$\vert f''(c) \vert \geq \frac{4b}{a^2}$$ I have to use the Taylor theorem with integral remainder to prove this.","['integration', 'derivatives', 'taylor-expansion', 'real-analysis']"
4623224,How to find appropriate discretization method for a continuous time domain state space model?,"I have a dsp algorithm which is based on the below given state space model in the continuous-time domain $$
\begin{bmatrix} 
\frac{\mathrm{d}\hat{\psi}_{r_{\alpha}}}{\mathrm{d}t} \\
\frac{\mathrm{d}\hat{\psi}_{r_{\beta}}}{\mathrm{d}t}
\end{bmatrix}
=
\begin{bmatrix}
-\frac{R_R}{L_L + L_M} & -p_p\cdot\omega_m \\
p_p\cdot\omega_m & -\frac{R_R}{L_L + L_M}
\end{bmatrix}
\cdot
\begin{bmatrix}
\hat{\psi}_{r_{\alpha}} \\
\hat{\psi}_{r_{\beta}}
\end{bmatrix}
+
\begin{bmatrix}
\frac{L_M\cdot R_R}{L_L + L_M} & 0 \\
0 & \frac{L_M\cdot R_R}{L_L + L_M}
\end{bmatrix}
\cdot
\begin{bmatrix}
i_{s_{\alpha}} \\
i_{s_{\beta}}
\end{bmatrix}
$$ It is basically a model of a dynamic system, where the variables $i_{s_{\alpha}}$ , $i_{s_{\beta}}$ and $\omega_m$ are the inputs of the dynamic system and the $\hat{\psi}_{r_{\alpha}}$ , $\hat{\psi}_{r_{\beta}}$ are its outputs (the unmeasurable state variables of the system). This model is intended to be used as a state observer. I know that there are much more robust approaches for estimation of the unmeasurable state variables of a dynamic system but I would like to do a comparison between several methods. As far as the parameters of the state space model: $R_S = 7.400\cdot 10^{-3}\,\Omega$ , $R_R = 7.548\cdot 10^{-3}\,\Omega$ , $L_M = 4.265\cdot 10^{-3}\,\mathrm{H}$ , $L_L = 0.231\cdot 10^{-3}\,\mathrm{H}$ , $p_p = 3.0$ . For the software implementation purposes it is necessary to discretize the continuous-time domain model (sampling period is $T = 100\cdot 10^{-6}\,\mathrm{s}$ ). I have been thinking about the simple Euler method (i.e. the state transition matrix $\Psi = \mathbf{I} + \frac{(\mathbf{A}\cdot T)}{2!} + \frac{(\mathbf{A}\cdot T)^2}{3!} + \ldots$ approximated only via the first two terms of the series expansion) usage. But I have noticed that the above given system is probably much more complex for computation than it seems at first glance. I have calculated the eigenvalues of the system matrix (i.e. poles of the system) in respect to the input $\omega_m$ and I have found that the poles are pretty near to the stability boundary in the s-plane. If I transformed the system into the discrete form via the ZOH method I found that the system poles (in respect to the input $\omega_m$ ) are even nearly on the stability boundary. My question is how I can implement this system in a software (in the single precision floating point) to be sure that the solution (the system state variables) will be stable? The idea behind my question is that I have noticed that the poles of the discretized system (zero-order hold discretization) are very close to the unit circle boundary (the ""reserve"" is only a couple of ten thousandths). Source of my doubt is that in case I implement the discrete system in a control software it is from my point of view very good chance that due to the limited precision the poles can move outside the unit circle and the system can become very easily unstable.","['discrete-time', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4623259,Central simple algebras and semisimple subalgebras,"Let $B$ be a central simple algebra over an algebraically closed field $k$ , and let $A\subset B$ be a semi-simple $k$ -subalgebra. By this I also mean $k$ is central in $A$ as well, and the map $k\to A\to B$ is the identity map onto the center of $B$ . Now we assume that the map $k\to A$ surjects onto the center of $A$ . In particular, not only do $A$ and $B$ have the same center, $k$ , but their identities match and the map $A\to B$ is unitary and central (i.e. it sends the center of $A$ into the center of $B$ ). For instance, the inclusion $M_n(k)\subset M_m(k)$ for $m>n$ via block matrices does not qualify. For $\sum_{i=1}^Nm_i=m$ , the inclusion $$\prod_{i=1}^NM_{m_i}(k)\subseteq M_m(k)$$ does not qualify either. It is a unitary map, but the center of $A$ is $k^N$ while the center of $B$ is $k$ . If $A\neq k$ , do we have $A=B$ ? Remark As a special case, if we knew that the inclusion of $A$ in $B\simeq M_n(k)=\text{End}_k(k^{\oplus n})$ makes $k^{\oplus n}$ into a simple $A$ -module, we could use Burnside's Theorem to conclude $A=B$ . I was guessing the answer is ""no"", but I can't find an example so I started wondering. I'd appreciate any help very much!","['homological-algebra', 'number-theory', 'abstract-algebra', 'algebraic-geometry', 'noncommutative-algebra']"
4623278,"If $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is a polynomial on every line, is it a polynomial itself? [duplicate]","This question already has answers here : Real polynomial in two variables (3 answers) Closed last year . The question: If $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is a function such that for every ""linear"" function $\varphi:\mathbb{R}\rightarrow\mathbb{R}^2$ (i.e. an affine line in $\mathbb{R}^2$ ) we have that $f\circ\varphi$ is a polynomial, then is $f$ a polynomial itself? Discussion (only for curious readers): What I have already proved is that if $f\circ\varphi$ is a polynomial of degree at most $n$ then $f$ is a polynomial of degree at most $n$ too. Although, I just proved it for $n=2$ and assumed that the similar construction exists and can be made for higher $n$ . The proof goes like this: Substract from $f$ the polynomial that is at the line $x=0$ . For example, if at $x=0$ we have the polynomial $ay^2+by+c$ then substract that from $f$ . So, if $f=x+xy+y^2$ , then after substracting we are left with $x+xy$ . We have that this new $f$ is a quadratic polynomial iff the old $f$ was. Do that for $y=0$ too. Now the new $f$ is equal to $0$ on both axes. Substract now from $f$ the function $f(1,1)\cdot xy$ . The new $f$ is zero on the axes and on the line $x+y=2$ because it is zero at $3$ points on it: $(0,2)$ , $(1,1)$ and $(2,0)$ . Now every line (that is not parallel to these $3$ nor is concurrent with two of them) intersects all $3$ of them (at $3$ different points) so $f$ is zero on them too. These lines pass through each point, so the new $f$ is $0$ everywhere, and that is equivalent to the original $f$ being a quadratic form. My idea for my original question was that there will be simply too many lines for $f$ to not be a polynomial, i.e. if the number of lines at which $f$ is a polynomial of the degree exactly $n$ is too big (in some sense) then $f$ must be a polynomial of degree $n$ too. I thought maybe some argument similar to the one in the spoiler would work, but I couldn't find it.","['polynomials', 'real-analysis']"
