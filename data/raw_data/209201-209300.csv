question_id,title,body,tags
4198634,Is there an exact solution for $\large\int \frac{dx}{\tan^{-1}(x)}$?,"Do you know about the inverse tangent integral function ? It is defined as: $$\mathrm{Ti_2(x)=Ti(x)=\int_0^x\frac{tan^{-1}(x)}{x}dx=-\frac1x\sum_{n\ge 1}\frac{(-1)^nx^{2n}}{(2n-1)^2}}$$ Expanding the denominator and then the sum gives many other forms of the function. Also, I wondered what other unsolved trigonometric integrals there are. You can click my profile questions to see similar inspiration questions. Here is the function I want to find. Here is an interactive graph . It is an odd injective function: $$\mathrm{T(x)=\int \frac{dx}{tan^{-1}(x)} , \\ T(b)-T(a)=\int_a^b \frac{dx}{tan^{-1}(x)}=\int_{tan^{-1}(a)}^{tan^{-1}(b)} \frac{sec^2(x)}{x}dx\mathop=^{|x|<\frac\pi 2} \quad \int_{tan^{-1}(a)}^{tan^{-1}(b)}\sum_{N=0}^\infty\sum_{n=0}^\infty\frac{(-1)^{N+n}E_{2N}E_{2n}x^{2(N+n)-1}}{(2N)!(2n)!}dx= \quad \sum_{N=0}^\infty\sum_{n=0}^\infty\frac{(-1)^{N+n}E_{2N}E_{2n}(tan^{-1}(b))^{2(N+n)}-(tan^{-1}(a))^{2(N+n)}}{2(2N)!(2n)!(N+n)}}$$ This approach above uses the Euler numbers $\mathrm E_y$ and this series representation . This looks very complicated as I had to multiply the two series together to get a secant squared representation. Here is a solution for T(x), but I do not know the Laurent series coefficient formula : $$\mathrm{T(x)=\int \sum_{n=0}^\infty c_n x^{2n-1}dx=\int \frac1x +\frac x3-\frac{4x^3}{45}+\frac{44x^5}{945} -\frac{428x^7}{14175}+\frac{10196x^9}{467775}-\frac{10719068x^{11}}{638512875}+…dx=ln(x)+\sum_{n=1}^\infty \frac{c_n x^{2n}}{2n}=ln(x)+\frac {x^2}6-\frac{x^4}{45}+\frac{22x^6}{2835}-\frac{108x^8}{28350}+\frac{5098x^{10}}{2338875}-\frac{2679767x^{12}}{1915538625}+…,c_n=1,\frac13,-\frac4{45},\frac{44}{945},-\frac{428}{14175},\frac{10196}{467775},-\frac{10719068}{638512875},…}$$ Here is a graph of T(x). Notice the oblique asymptote which
is a consequence of the fact that $\frac1{\tan^{-1}(\pm \infty)}=\frac2\pi$ . The graph is for the area from x=1 to x= $\text x_0$ . You can also see the vertical asymptote at x=0 implying infinite area over almost any interval containing x=0: Here is motivation using trigonometric integral functions . I will assume a primitive here, no constant, for simplicity: $$\mathrm{\int\frac{dx}{cos^{-1}(x)}=-Si(cos^{-1}(x))}$$ $$\mathrm{\int\frac{dx}{sin^{-1}(x)}=Ci(sin^{-1}(x)}$$ $$\mathrm{\int\frac{dx}{cosh^{-1}(x)}=Shi(cosh^{-1}(x))}$$ $$\mathrm{\int\frac{dx}{sinh^{-1}(x)}=Chi(sinh^{-1}(x)}$$ Just like the actual inverse tangent integral, I wonder if this T(x) function can also be expressed in exact form. If possible, please express in closed form, but an exact answer also works. I would be surprised if T(x) can even be expressed in terms of Ti(x), the inverse tangent integral. Another answer is to find out if the coefficients I typed above have any pattern that can be written as an mathematical expression. Please correct me and give me feedback! Applications:
Try the inverse integral theorem on $\tan\frac1x$ You can try this problem , find the integral $\mathrm{\int_{i}^{2i}\frac{dx}{tan^{-1}(x)}=\int_1^2\frac{i\,dx}{tanh^{-1}(x)}}$ , and I will try to ask a question about the applications though . Results from @Yuri Negometyanov and @Nikos Bagis show the following results. Graphical proof . Note that you can split the sum after distributing the factored numerator. Bernoulli Numbers . Notice the root of the desmos graph at different values. Be sure to set the slider to approach $\pm \infty$ for i by approximation of $\pm 10$ in the graph link: $$\mathrm{T(x)=\int\frac{dx}{\tan^{-1}(x)}=\frac{x}{\tan^{-1}(x)}+ln\left(\tan^{-1}(x)\right)-\frac12 \sum_{n=2}^\infty\frac{(-4)^n\left(4^n-1\right)B_{2n} \left(\tan^{-1}(x)\right)^{2(n-1)}}{(n-1)(2n)!}+C=ln\left(tan^{-1}(x)\right)+\frac{x}{tan^{-1}(x)}-\frac{4}{\pi^2}\sum_{n\in \Bbb Z}\frac{ln\left(\pi(2n+1)-2tan^{-1}(x)\right)}{(2n+1)^2}+C}$$ I wonder about the integrals of the reciprocal of other inverse trigonometric functions…","['integration', 'summation', 'trigonometric-integrals', 'inverse', 'recreational-mathematics']"
4198640,Confidence interval for unknown $\mu$ and $\sigma^2$,"I am stuck trying to solve this problem. I can only get to a certain point in my attempts. Any help is greatly appreciated. I am given the following sample of size $10$ drawn from a $\mathcal N(\mu, \sigma^2)$ -distributed population: $$(100.4, 99.2, 98.4, 99.6, 103.6, 101.6, 103.2, 99.2, 97.6, 99.2)$$ Determine the $95 \%$ confidence interval for the unknown expected value $\mu$ and known variance $\sigma^2=4$ Determine the $95 \%$ confidence interval for the unknow expected value $\mu$ and unknown variance $\sigma^2$ Determine the $95\%$ confidence interval for the unkown variance $\sigma^2$ and unknown expected value $\mu$ What I have tried so far (with some resources from the web): $$\alpha=1-0.95=0.05\\ \Phi(c)=\frac{1}{2}\left(1-\frac{\alpha}{2}\right)=0.975$$ The mean of the sample is: $$\bar{X}=\frac{1002}{10}=100.2$$ Then I can use the following formula: $$\bar{X}-c \frac{\sigma}{\sqrt{n}}\le \mu \le \bar{X}+c \frac{\sigma}{\sqrt{n}}$$ I know $\bar{X}=100.2, \sigma=2,n=10$ but how can I find $c$ ? For question two, is it just the exact same calculation as 1. but now using the sample variance? What is the difference to question 2. ? Aren't they the same?","['statistics', 'normal-distribution']"
4198705,"For each value for $a$ and $b$, being real numbers, $a^2 + b^2 \ge ab$","For each value for $a$ and $b$ , being real numbers, $a^2 + b^2 \ge ab$ Should I solve this by replacing all possible real number formulas in these to be able to prove this? As an example for odd numbers, by replacing $2x + 1$ , etc...?",['discrete-mathematics']
4198756,Pardon my mistakes in asking t I have problems solving equations of LIMITS. To be specific the ones I have to rationalize. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question https://www.ck12.org/book/ck-12-precalculus-concepts/section/14.5/ Example 5. The last example is what's giving me issues. The denominator at the $4$ -th equation to be precise. I don't know how it was manipulated. Am an undergraduate in the University of Calabar in Nigeria. Am studying for my exams which will commence on 22nd of this month. I believe the answer will help me pass maths exams with good grades and the answer will also help others who have same issue. And I'll definitely reffer my friends to this website. Admin please kindly approve as the responses will help me in no small way. Thanks Evaluate the following limit: $$\lim{x\to 0}\left(\frac{3}{x\sqrt{9−x}}-\frac1x\right)$$ \begin{align}
\lim_{x \to 0}\left(\frac{3}{x\sqrt{9−x}}-\frac1x\right) &= \lim_{x \to 0}\left(\frac{3}{x\sqrt{9−x}}-\frac{\sqrt{9-x}}{x\sqrt{9-x}}\right) \\
&=\lim_{x \to 0} \left(\frac{3-\sqrt{9-x}}{x\sqrt{9-x}} \right)\\
&= \lim_{x \to 0} \left( \frac{3-\sqrt{9-x}}{x\sqrt{9-x}}\cdot \frac{3+\sqrt{9-x}}{3+\sqrt{9-x}}\right)\\
&= \lim_{x \to 0} \left( \frac{9-(9-x)}{x\sqrt{9-x}}\right)\\
&= \lim_{x \to 0} \frac{x}{x\sqrt{9-x}}\\
&= \lim_{x \to 0} \frac1{\sqrt{9-x}}\\
&= \frac1{\sqrt{9-0}}\\
&= \frac13
\end{align}","['calculus', 'functions', 'numerical-methods', 'limits', 'algebra-precalculus']"
4198759,Which Deck wins the most?,"I have two decks of cards, both shuffled randomly and placed face-down. Deck A has 40 cards. 10 are red, while the remaining 30 are black.
Deck B has 80 cards. 20 are red, while the remaining 60 are black. I am asked to pick one deck and draw 20 cards from the top of it, scoring 1 point for each red card I draw. Which deck should I choose? Edit: The question is: ""Which Deck, A or B, would win in a game where you're drawing 20 cards randomly?"" Said another way, which deck, A or B would win in a head to head game? When I worked it out, it seems that both Decks have an expected value of 5 points. So, if I did the work right, then on average, there should not be any advantage to picking either deck. To confirm this, I ran a monte carlo simulation and it seems that my long running average has A with a slight advantage over deck B. I wrote this up in R to check it out. BDeckWins <- 0
ADeckWins <- 0
ties <- 0
trials <- 100000
deckApts <- vector(mode = 'numeric', length = trials)
deckBpts <- vector(mode = 'numeric', length = trials)
for(i in 1:trials)
{
  deckA <- rep(0, 40) # make all blacks
  deckA[sample(1:40,10)] <- 1 #randomly pick 10 reds
  handA <- sample(deckA, 20)
  pntsA <- sum(handA)
  deckApts[i] <- pntsA
  #print(stringr::str_c(""Deck A points:"", pntsA))
  
  deckB <- rep(0, 80) # make all blacks
  deckB[sample(1:80,20)] <- 1 #randomly pick 20 reds
  handB <- sample(deckB, 20)
  pntsB <- sum(handB)
  deckBpts[i] <- pntsB
  #print(stringr::str_c(""Deck B points:"", pntsB))
  
  if(pntsB != pntsA)
  {
    if(pntsB > pntsA)
    {
      #print(""Deck B wins"")
      BDeckWins <- BDeckWins + 1
    }
    else
    {
      #print(""Deck A wins"")
      ADeckWins <- ADeckWins + 1
    }
  }
  else
  {
    #print(""Tie, no winner"")
    ties <- ties + 1
  }
}

mean(deckApts)
mean(deckBpts)

print(ADeckWins)
print(BDeckWins)
print(ties)

print(ADeckWins + BDeckWins + ties) The code to calculate the expected values is as follows: ev_a <- vector(mode = 'numeric', length = 10)
for(i in 0:10)
{
    ev_a[i+1] <- i*(choose(10, i)*choose(30, 20 - i) / choose(40,20))
}
sum(ev_a)

ev_b <- vector(mode = 'numeric', length = 20)
for(i in 0:20)
{
  ev_b[i+1] <- i*(choose(20, i)*choose(60, 20 - i) / choose(80,20))
}
sum(ev_b) So...I checked that the simulation agrees that 5 is the long run expected value. I don't know what I am not seeing.","['expected-value', 'card-games', 'probability']"
4198791,"Given a bijection $q: \mathbb{N} \rightarrow \mathbb{Z} \times \mathbb{Z}$, find $q(1000)$","We were given a bijection of $q: \mathbb{N} \rightarrow \mathbb{Z} \times \mathbb{Z}$ that spirals out from $(0,0)$ . I need to find $q(1000)$ . I noticed that the top left and bottom right corners of each ""spiral"" are squares of incrementing even and odd integers in $\mathbb{N}$ . How would I go about getting the point $q(1000)$ ? Edit: The original bijection is from $\mathbb{N} \rightarrow \mathbb{Z} \times \mathbb{Z}$ . The numbers on each point are incremented values from the previous point on the spiral, not coordinates.","['elementary-set-theory', 'functions']"
4198796,Differential Equation Intuition - Wolfram Alpha Incorrect?,"Consider the differential equation $$f'(x)=\frac{f(x)}{2x} - \sqrt{f(x)}. $$ Per Wolfram Alpha , its solution is $$f(x)=\frac{1}{9}(4x^2-12cx^{5/4}+9c^2 x^{1/2}).$$ From the differential equation, we can see that when $x$ and $f(x)$ are large, the function has a negative slope. However, we see from the solution that when $x$ is large, so is $f(x)$ , yet the function has a positive slope. What explains this apparent contradiction in intuition? Here is an example with concrete values. Setting $c=1$ , $f(20) = 125$ . From the solution, $f$ slopes upwards at $x=20$ . Yet according to the differential equation, $f'(20) = 125/40-125^{0.5} = -8$ , which suggests $f$ slopes downwards. What explains this disconnect? Here is an interactive Desmos graph showing these functions and calculations. As you can see, the derivative of $f$ as calculated by Desmos is markedly different from the slope given by the differential equation. Is there any chance Wolfram Alpha got this wrong? Or is there some other explanation for the seeming contradiction? Any feedback or guidance would be greatly appreciated.","['functions', 'derivatives', 'ordinary-differential-equations']"
4198803,Does an absolute value distribute over an absolute value $|c||x-y| = ||c|x-|c|y|$?,"So I've been given a set $V = \{x \in \mathbb{R}|x\geq0\}$ with the following addition and scalar multiplication operations for the set (scalar in this case referring to $c \in \mathbb{R})$ : $x \oplus y = |x-y|$ $c \odot x = |c|x$ I need to determine if the set's addition is associative and if the set's scalar multiplication distributes over the addition. I have already determined that the defined addition is not associative since if it was associative then: $x \oplus (y \oplus z) = |x - |y-z|| = ||x-y|-z| = (x \oplus y) \oplus z$ but for $x=1$ , $y=2$ and $z=3$ then $x,y,z \in \mathbb{R}$ : $|1 - |2-3| = |1-|-1||=|1-1|=|0|=0$ $||1-2|-3|=||-1|-3|=|1-3|=|-2|=2$ $0 \neq 2$ However, I am getting stuck on the distributivity part. I have tried many combinations of real number and positive real number only to get the same answer from both sides of the equation $c \odot (x \oplus y) = |c||x-y|=||c|x-|c|y|=(c\odot x) \oplus (c\odot y)$ Moreover, I tried graphing both $|c||x-y|$ and $||c|x-|c|y|$ and they appear to be the same graph by setting c and y and they appear to be the same graph. However, I'm pretty sure that I can't just distribute $|c|$ over $|x-y|$ and call it a day due to the absolute value functions. Any help would be appreciated!","['vectors', 'absolute-value', 'vector-spaces', 'functions', 'linear-algebra']"
4198835,What is the Hessian of $X \mapsto f \left( X^T X \right)$ in terms of the Hessian of $f$?,"Given $f : \mbox{Sym} (n) \to \mathbb{R}$ , let function $g: \mathbb{R}^{k ,n} \to \mathbb{R}$ be such that $$g(X) := f \left(X^T X \right)$$ What is the Hessian of $g$ in terms of the Hessian of $f$ ? Vectorizing $X$ and directly computing for the Hessian definitely works, but the notation is quite messy and I spent a lot of time on it but still can't get a clear solution. For now, I want to use the gradient which is $X(\nabla f(X^TX)^T + \nabla f(X^TX))$ to take its derivative and get the linear map representation of the hessian. But I'm having trouble taking the derivative of $\nabla f(X^TX)$ . Suppose $Hf : Sym(n) \to Sym(n)$ is the linear map representation of the Hessian of $f$ , then is $D(\nabla f(X^TX))$ just the map $V \to Hf(X^TX)V^TX+Hf(X^TX)X^TV$ ? If so, then the hessian of $g$ at $X$ would be the linear map on $\mathbb{R}^{k,n}$ that $$V \to X(D(\nabla f(X^TX))(V)^T + D(\nabla f(X^TX))(V))+V(\nabla f(X^TX)^T+\nabla f(X^TX))$$ But I'm not really sure of this. Any remarks will be much appreciated.","['matrices', 'multivariable-calculus', 'matrix-calculus', 'optimization', 'hessian-matrix']"
4198847,Two ways of explaining the notation $\mathbb{R}^2$,"I found two ways to explain the notation $\mathbb{R}^2$ . First is by Cartesian product: $\mathbb{R} \times \mathbb{R}$ . Secondly, regarding $2$ as any set with two elements. suppose it's $\{0,1\}$ .
Then the map from $\{0,1\}$ to $\mathbb{R}$ forms a linear space. which can be also denoted as $\mathbb{R}^{\{0,1\}} = \mathbb{R}^2$ . In mathematics, if two things looks similar or the same, there will always be some deep reason behind it. So what's the deep reason of the above statements?","['elementary-set-theory', 'abstract-algebra', 'notation']"
4198856,Can we substitute complex number into gamma function formula,"I came across the following ""proof"" of the Eulerian integrals $$ \int_0^{\infty} t^{s-1} \cos(bt) e^{-at} \, dt = \frac{\Gamma(s) \cos \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}}, \int_0^{\infty} t^{s-1} \sin(bt) e^{-at} \, dt = \frac{\Gamma(s) \sin \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}} $$ where $s>0, b \in \mathbb{R}, a>0 $ and I'm wondering if it is rigorous and if not, what details need to be filled in to make it rigorous. Begin with the gamma integral $$ \Gamma(s) = \int_0^{\infty} x^{s-1} e^{-x} \, dx $$ and make the substitution $x=zt$ to get $$ \frac{\Gamma(s)}{z^s} = \int_0^{\infty} t^{s-1} e^{-zt} \, dt. $$ This is fine as long as we assume $z$ is real and positive. But now the ""proof"" asserts that the formula also holds for complex $z$ with positive real part* (this is the step I'm unsure about) and substitutes $z=a-ib$ for $a>0$ and $b \in \mathbb{R}$ and simplifies the result with the aid of polar coordinates $$ \int_0^{\infty} t^{s-1} e^{ibt} e^{-at} \, dt = \frac{\Gamma(s)}{(a-ib)^s} = \frac{\Gamma(s)}{\left(\sqrt{a^2+b^2} e^{i\arctan \left(-\frac{b}{a} \right)} \right)^s} = \frac{\Gamma(s)}{(a^2+b^2)^{s/2}} e^{is \arctan \left(\frac{b}{a} \right)} $$ and the integrals above follow by equating real and imaginary parts. Can someone please explain why the complex number substitution is valid here, and how to justify it? In general when is it ok to do this in an integral? Thanks","['integration', 'definite-integrals', 'gamma-function']"
4198867,Central Limit Theorem & Random Walks,"Let $(\Omega, \mathcal{F}, P)$ be a probability space and $\{ X_i \}_{i=1}^\infty$ be $\operatorname{i.i.d}$ . Assume that $P(X_1 =1) = P(X_1 =-1) = \frac{1}{2} $ and define $S_n$ by $$\begin{cases}
S_0 & = 0\\
S_n :&= \sum_{i=1}^n X_i \,\,\,\, (n \ge 1).
\end{cases}
$$ Let $A: = \{ S_{100} =0\}  $ , $B:= \{ S_{200} =0 \}$ , are $A$ and $B$ independent? Answer with reason. Find the value of $\underset{n \to n}{\lim} P(S_n \ge n^{1/3})$ .
For each $x \in \mathbb{Z}$ , $T_x : \inf \{ n \ge 0 | S_n = x \}$ , where $\inf \phi = \infty$ . Show that $P(T_x < \infty) =1$ for all $x \in \mathbb{Z}$ . For each $n \ge 1$ , $M_n : = \frac{\max \{ S_k | 0 \le k \le n\}}{\sqrt{n}}$ . Show that $\{P^{M_n} \}_{n=1}^\infty$ convergence weekly as $n \to \infty$ in the topology of probability measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R})).$ ( $\mathcal{B}(\mathbb{R})$ is Borel algebra of $\mathbb{R}$ ). I have been studying this problem since two weeks and these are my solutions to the three points of the problem. However, the fourth point is not easy, I do not even know where to look fo find the solution of similar solution. I need check for the solution of point 3 and help to solve point four. Thanks in advance. (1) Let $ C= \{\sum_{i=101}^{200} X_i =0 \}$ . Then $A \cap B = A \cap C$ , so $P(A \cap B) = P(A \cap C) = P(A)P(C)$ . For this to equal $P(A)P(B)$ ,we would need $P(B) = P(C)$ , but this is false. So $A$ and $B$ are dependent. (2)- $\underset{n \to \infty}{\lim} P(S_n \ge n^{1/3}) = \underset{n \to \infty}{\lim} P(\frac{S_n}{\sqrt{n}} - \frac{1}{n^{1/6}} \ge 0)$ . By central limit theorem we have $$\frac{S_n}{\sqrt{n}} \xrightarrow{distribution} N(0,1).$$ then $$\left(\frac{S_n}{\sqrt{n}}, \frac{1}{n^{1/6}} \right) \xrightarrow{d} (N(0,1), 0).$$ By the continuous mapping theorem (applied to $f(x,y) = x-y$ ). $$\frac{S_n}{\sqrt{n}} - \frac{1}{n^{1/6}} \xrightarrow{distribution} N(0,1)$$ and then $$\underset{n \to \infty}{\lim} P(S_n \ge n^{1/3}) = P(N(0,1) \ge 0)=\frac{1}{2}.$$ (3)- Before solving this point we need to recall some facts. $$T_x = \inf \{n \ge 0: S_n =x \} = \text{Hitting time on x}.$$ Since $S_0=0$ then $T_x$ is also is the first passage time to $x$ . $$V_x = \sum_{n=0}^\infty 1_{\{ S_n=x\}} = \text{ Number of visits to x}.$$ $$f_x  = P_x(T_x < \infty) = \text{ return probability to x}.$$ $$m_x = E_x (T_x) = \text{ mean return time to } x.$$ $x$ is \textbf{recurrent} if $P_x (V_x =\infty)=1$ . Otherwise $x$ is transient.
\begin{theorem}{(1)}
\label{1}
$$x \text{ is recurrent } \iff f_x =1.$$ $$x \text{ transient } \iff f_x < 1.$$ \end{theorem} \begin{proof}
$$P_x (V_x <\infty) = P_x \left( \bigcup_{k \ge 1} { V_x =k } \right) = \sum_{k=1}^\infty P_x (V_x =k) = \sum_{k=1}^\infty (1- f_x) f_x^{k-1}= \begin{cases} 0, & f_x =1\
1, & f_x <1
\end{cases}.$$ So $x$ is recurrent iff $f_x=1$ , and transient iff $f_x <1$ .
\end{proof} \begin{theorem}{(2)}
\label{2}
$$x \text{ recurrent } \iff \sum_{n=1}^\infty p_{xx}^{(n)} =\infty \iff f_x =1.$$ $$x \text{ transient } \iff \sum_{n=1}^\infty p_{xx}^{(n)} <\infty \iff f_x <1.$$ \begin{proof}
If $x$ is recurrent, this means $P_x (V_x =\infty)=1$, and so $$\sum_{n=1}^\infty p_{xx}^{(n)} = \sum_{n=1}^\infty E_x \left( 1_{\{ S_n=x \}} \right) = E_x \left( \sum_{n=1}^\infty 1_{\{S_n=x \}} \right) = E_x (V_x)=\infty.$$ If $x$ is transient then by Theorem (1) $f_i <1$ and $$ \sum_{n=1}^\infty p_{xx}^{(n)} = E_x (V_x) = \sum_{r=1}^\infty P_x (V_x > r) = \sum_{r=1}^\infty f_x^r =\frac{1}{1-f_x} < \infty.$$ \end{proof} \end{theorem} Now we start solve the problem. By the Markov property we have $$P(T_x < \infty) = \sum_{y \in \mathbb{Z}} P(X_0=y) P_y (T_x <\infty).$$ so it suffices to show that $P_y(T_x < \infty)=1$ for all $y \in \mathbb{Z}$ . Choose $m$ with $p^{(m)}_{xy} > 0$ . By Theorem ( 2 ) we have \begin{align*}
    1 &= P_x (S_n = x \text{ for infinitely many } n)\\
      &\le P_x (S_n=x \text{ for some } n \ge m+1)\\
      & = \sum_{k \in \mathbb{Z}} P_x (S_n = x \text{ for some } n \ge m+1 | S_m = k) P_x (S_m = k)\\
      &= \sum_{k \in \mathbb{Z}} P_k ( T_x < \infty) p^{(m)}_{jk}
\end{align*} But $\sum_{k \in \mathbb{Z}} p^{(m)}_{jk}=1$ so we must have $P_y (T_x <\infty)=1$ . (4)-","['random-walk', 'stochastic-processes', 'markov-process', 'martingales', 'probability-theory']"
4198873,Pole lying in the middle of a logarithm's branch cut in $\int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2}$,"Consider the integral, for $a\geq0$ , $$I = \int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2} = 2 \pi \ln(1+\sqrt{a})$$ This is straightforward to do with real methods via differentiation with respect to $a$ , partial fraction decomposition, and integration with respect to $a$ . My question is about evaluating this integral directly using a contour integral in the complex plane. I am running into a problem for $a\leq1$ where the pole $z=i$ lies directly in the middle of the branch cut, yielding an extra, seemingly spurious, imaginary contribution. I will define $\ln(z)$ to have a branch cut on the negative real axis. Looking in the upper half plane, the branch point $z = \sqrt{a} i$ yields a branch cut in $\ln(a+z^2)$ moving up the imaginary axis (this follows since the imaginary part of $a+x^2$ changes sign on crossing the imaginary axis, and the real part is non-positive.) Consider the integral for $a>1$ , and consider the following contour: The contour integral via the pole at $z=i$ evaluates to $\pi \ln(a-1)$ . Looking at the different contributions, and noting that the arc does not contribute because the function decays quickly enough, we have $$\pi \ln(a-1) = I + \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2}$$ Here, the second contribution on the right hand side comes from the vertical lines on the sides of the branch cut. It is straightforwardly evaluated as $\pi \ln(\frac{\sqrt{a}-1}{\sqrt{a}+1}$ ). Solving for $I$ yields, as anticipated, $$I = 2\pi \ln(1+\sqrt{a})$$ However, I'm having much more trouble for $a \leq i$ , for which the pole lies in the middle of the branch cut. I keep getting an extra contribution! The contour integral now evaluates to zero, but there are significant contributions from the small semicircle arcs about the pole. My trouble is that the small semicircle arcs are on different sides of the branch cut and hence contribute different amounts. I find $$0 = I + PV \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2} + (-\pi \ln(a-1)/2) + (-\pi \ln(a-1)/2 \color{red}{+ 2 \pi i (-\pi i)/(2i)}  ) $$ From left to right, I have the original integral, the contribution from the vertical parts of the contour, the right small semicircle, and the left small semicircle. The left small semicircle is on the side of the branch cut with an extra $2 \pi i$ contribution from the log, which gives the extra red contribution. Without the red contribution, I get the right answer after rearranging and simplifying. With the red contribution, I suffer an imaginary contribution that yields an incorrect result for $I$ . How does one properly handle a pole lying in the middle of a (logarithm's) branch cut? Am I missing another imaginary contribution that cancels with the one in red?","['complex-analysis', 'contour-integration', 'definite-integrals', 'branch-cuts']"
4198924,Toy Definition of a Topological Space,"Preamble: This is a question meant to be fun and amusing -- not part of any class/homework/test/etc.  Also, please forgive my horrific typesetting.  This is my first question on this website.  I'm fluent in Latex, which seems to be close, but not exactly the language structure of this question format. Motivation: I was rereading about Point-Set Topology, and was considering to myself the possibility creating an alternative definition to a Topological Space.  To Recall, Definition : A Topology on a set $X$ is a collection $\tau$ of subsets of $X$ having the following properties: $\emptyset$ and $X$ are in $\tau$ . The union of the elements of any subcollection of $\tau$ is in $\tau$ . The intersection of the elements of any finite subcollection of $\tau$ is in $\tau$ . A set $X$ for which a topology $\tau$ has been specified is called a topological space.    That is, the ordered pair $(\tau,X)$ . -- Topology, Second Edition. James R. Munkres. Page 76. We see here that $\tau$ is a collection of subsets, as that is what our definition states.  However, suppose instead we imagine replacing $\tau$ with the function $\rho$ which accepts two inputs and provides a boolean output.  To define explicitly: $\rho: \{\text{ruleset}\} \times \mathscr{P}(X) \rightarrow \{1,0\}$ $\rho(\{\text{ruleset}\},U) = 1$ , if $U$ is open or a complement of an open set according to the ruleset. *notice here that both $X$ and $\emptyset$ would return a value of $1$ , as they are both open, and complements of each other. $\rho(\{\text{ruleset}\},U) = 0$ , else. (We define $\mathscr{P}(X)$ to be the powerset of the set $X$ ). Now if the ruleset is something simple such as ""If $U$ is in $\mathscr{P}(X)$ "" then we trivially get back the definition via $\tau$ . If the ruleset includes a proviso for an open subset $A \subseteq X$ so that the intersection of $A$ and $U$ is open provided $U$ is open in $X$ , then we get the subspace topology. Question:  Under this pretense, could we redefine a topological space from $(X,\tau) \rightarrow (X,\rho)$ ?  I would really would love to see this fail under some special example. >:). Many thanks in advance and happy summer 2021!","['elementary-set-theory', 'boolean-algebra', 'general-topology']"
4199000,Show that $\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0$,"I need help showing that $$\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0.$$ If I take logarithm, I get $\begin{align*}
\lim_{n \to \infty} \ln(\frac{((n-1)!)! (n-1)!^{n!}}{(n!)!}) &= \lim_{n \to \infty} \ln(((n-1)!)!) + n! \ln((n-1)!) - \ln((n!)!) \\ 
&= \lim_{n \to \infty} \ln(n-1)+\ln(n-3)+...+\ln(3)\\& + n! (\ln(n-1)+\ln(n-2)+...+\ln(2))\\& - \ln(n) - \ln(n-2) - ... - \ln(2),
\end{align*}$ but I don't see if that even helps.","['limits', 'factorial', 'logarithms']"
4199018,Help with $\int _0^{\infty }\frac{\sinh \left(x\right)}{x\cosh ^2\left(x\right)}\:\mathrm{d}x$,"I want to know how to prove that $$\int _0^{+\infty }\frac{\sinh \left(x\right)}{x\cosh ^2\left(x\right)}\:\mathrm{d}x=\frac{4G}{\pi }$$ Here $G$ denotes Catalan's constant, I obtained such result with the help of mathematica. I also found that the integral equals a certain infinite series $$\int _0^{+\infty }\frac{\sinh \left(x\right)}{x\cosh ^2\left(x\right)}\:\mathrm{d}x=\sum _{n=0}^{+\infty }\frac{\binom{2n}{n}^2}{16^n\left(2n+1\right)}=\frac{4G}{\pi }$$ which can also be found in this link . So I've $2$ questions $1)$$¿$ How can we transform the integral into the mentioned series? $2)$$¿$ Is there a simple way to evaluate the main integral without resorting to series expansion? What I did for question $\#2$ is to employ the substitution $x=\ln\left(t\right)$ $$\int _0^{+\infty }\frac{\sinh \left(x\right)}{x\cosh ^2\left(x\right)}\:\mathrm{d}x=-2\int _1^{\infty }\frac{1-t^2}{\ln \left(t\right)\left(1+t^2\right)^2}\:\mathrm{d}t$$ But I'm not sure how to proceed.","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4199031,What is the largest (finite) order of an element of $GL_{10}(\mathbb{Q})$?,"What is the largest (finite) order of an element of $GL_{10}(\mathbb{Q})$ ? The following is my guess: Let $A \in GL_{10}(\mathbb Q)$ is of finite order $n$ (say). Then $A^n=I$ , or every eigenvalue of $A$ is an $n$ -th root of $1$ . Thus an eigenvalue $\lambda$ of $A$ has degree $\leq\phi(n)$ over $\mathbb Q$ . Equality holds only when $\lambda$ is primitive $n$ -th root of $1$ . OTOH $\lambda$ also satisfies the characteristic polynomial of $A$ , which is of degree $10$ . Thus if the characteristic polynomial is irreducible (e.g., $A$ is companion matrix of $X^{10}+X^9+\cdots+X+1$ ) then degree of $\lambda=10$ , then we must have $\phi(n)\leq10$ . The maximum $n$ such that $\phi(n)=10$ is $15$ . If I am doing some mistake please correct me. Thanks.","['matrices', 'abstract-algebra', 'linear-algebra', 'field-theory']"
4199043,How many cases for this case?(Balls and boxes),"$Q)$ There are $12$ balls and same two boxes. Among the $12$ balls, $6$ balls are same but the rest of the $6$ are different respectively. (I.e. There are $7$ types of the $12$ balls.) How many cases the balls put into the two boxes?
(Here the case of the empty box allowed.) $(sol)$ Let the balls $a,a,a,a,a,a,1,2,3,4,5,6$ $(1)$ The "" $a$ "" balls : there are $4$ cases because of the below Each boxes and the balls are indistinguishable respectively. So $ 0+6 = 1+5 = 2+4= 3 +3$ $(2)$ The "" $1$ to $6$ "" balls : $S(6,1) + S(6,2) = 32$ cases (Here the $S(n,k)$ is the $2$ nd stirring number) So my answer is $4\times 32 = 128$ Are my answer and solution right?","['combinatorics', 'discrete-mathematics']"
4199054,Five friends meeting at the same coffeehouse,"Five friends agree to meet at Joe's Coffeehouse in Capital City. However, there are $5$ different Joe's Coffeehouse locations in Capital City, and the friends neglected to agree on which one to meet at, so they each choose one at random. What is the probability that (a) they all end up at the same Joe's? (b) they all end up at different Joe's? For each of those parts I solved it in two different ways and got different answers, I'm not sure which one is correct in both cases. Any explanation as to which is correct, which is wrong, and why would be very well appreciated. (a) APPROACH 1: Let's fix a friend, there's a ${1\over5}$ chance of him going to each particular Joe's, and then for each of those $5$ options for the remaining $4$ friends, there's a ${1\over5}$ chance of going to that same Joe's, so the probability they all end up at the same Joe's is: $$\left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 = \left({1\over5}\right)^4 = {1\over{625}}$$ APPROACH 2: The probability is going to be $${{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 5 \text{ for some }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {5\over{\binom{9}{5}}} = {5\over{126}}$$ (b) APPROACH 1: Let's fix a friend, there's a ${1\over5}$ chance of him going to each particular Joe's, and so for that particular Joe's, there's a ${4\over5}$ chance of the next guy going to a different Joe's, a ${3\over5}$ for the next guy etc. so the probability they all end up at different Joe's is: $$5 \left({1\over5}\right)\left({4\over5}\right)\left({3\over5}\right)\left({2\over5}\right) \left({1\over5}\right) = {{24}\over{625}}$$ APPROACH 2: The probability is going to be $${{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 1 \text{ for all }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {1\over{\binom{9}{5}}} = {1\over{126}}$$","['combinatorics', 'probability']"
4199066,On the Picard group of the generic fiber,Let $f:X\rightarrow Y$ be a flat morphism of factorial projective varieties and denote by $X_{\eta}$ the generic fiber of $f$ . Does the inequality $$\rho(X)\leq \rho(X_{\eta}) + \rho(Y)$$ among the Picard numbers hold? Here is a potential argument: consider the exact sequence $$Pic(Y)\rightarrow Pic(X)\rightarrow Pic(X_{\eta})\rightarrow 0$$ and let $K$ be the kernel of the first map. Then we have an exact sequence $$0\rightarrow K\rightarrow Pic(Y)\rightarrow Pic(X)\rightarrow Pic(X_{\eta})\rightarrow 0$$ So $\rho(X)+\dim(K) = \rho(X_{\eta}) + \rho(Y)$ . Thank you very much.,"['projective-geometry', 'divisors-algebraic-geometry', 'algebraic-geometry', 'birational-geometry', 'schemes']"
4199086,Intuitive Understanding of Direction of Steepest Ascent,"Forgive my use of many words. I would like to understand the meaning of direction of steepest ascent. I am undertaking a course on multivariate calculus on coursera. The instructor mentioned that the Jacobian points to the direction of steepest ascent. However, my intuition of steepest ascent is lacking. Consider a sphere given by the equation $x^2 + y^2 + z ^2 = 13^2$ . The equivalent function is given by $z=f(x, y) = \sqrt{13^2 - x^2 - y^2} $ . The point (3, 4, 12) lies on the sphere.
Suppose we want to find the direction of steepest ascent at this point, we begin by drawing tangents in all directions at this point. Assume there's an imaginary plane z = 100 above the point (3, 4, 12), we can extend each of these tangents from the point (3, 4, 12) to meet the plane z = 100. It's clear that each of these tangents will have different lengths from the given point ( 3, 4, 12) to the point each of them meets the plane  z = 100. My understanding is that the tangent with the shortest length gives the direction of steepest ascent. Is my intuitive understanding of direction of steepest ascent correct and complete? Also a reference where I can read more on the topic would be of great help.","['multivariable-calculus', 'geometry', 'reference-request']"
4199093,Girsanov: Will the new Brownian motion generate the original filtration?,"Setup of the question: Assume you have a Brownian motion $B_t$ on a probability space $(\Omega,\mathcal{F},P)$ . Let $\mathcal{F}_t$ be the natural filtration of the Brownian motion, and let $\bar{\mathcal{F}}_t$ be the natural filtration where we have also added the events with probability zero. Fix $T>0$ . Let $\phi$ be a progressively measurable process such that $P(\int_0^T\phi_t^2dt<\infty)=1.$ Define $Z_t:$ $$Z_t\doteq\exp\left(\int_o^t\phi_sdB_s-\frac{1}{2}\int_0^2\phi_s^2ds\right).$$ Assume that $z_t$ is a martingale on $[0,T]$ with respect to $\bar{\mathcal{F}}_t$ . Define the equivalent probability $Q$ on $(\Omega,\mathcal{F})$ such that $Z_T$ is the Radon-Nikodym derivative of $Q$ with respect to $P$ . Then Girsanovs theorem tells us that $\tilde{B}_t\doteq B_t-\int_0^t\phi_sds$ is a $\bar{\mathcal{F}}_t$ Brownian motion with respect to $Q$ on $[0,T]$ . Question By Girsanovs theorem we obviously have that $\tilde{B}_t$ is $\bar{\mathcal{F}}_t$ measurable. But I am wondering if $\tilde{B}_t$ generates $\bar{\mathcal{F}}_t, t \le T$ ? Define $\bar{\mathcal{G}}_t$ as the filtration generated by $\tilde{B}_t$ which also contain the $Q$ zero events. Since $P$ and $Q$ are equivalent it also contains the $P$ zero events. Is $\bar{\mathcal{G}}_t$ strictly smaller than $\bar{\mathcal{F}}_t$ , or are they equal? Attempt If I can show that $B_t$ is either a limit of $\tilde{B}_t$ or a Borel-function of $\tilde{B}_t$ the result will follow. The problem is $\phi_s$ . I know that $\tilde{B}_t=B_t-\int_0^t\phi_sds$ is $\bar{\mathcal{G}}_t$ -measurable. But do I know if $\int_0^t\phi_sds$ is $\bar{\mathcal{G}}_t$ -measurable?, if yes, we would be done. Any idea on how to finish it?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4199142,Simplifying $\sum_{{}^{m=1}_{m\ne n}}^N(1+(-1)^{m+n})\frac{\sin\frac{\pi mk}{N+1}\sin\frac{\pi m}{N+1}}{\cos\frac{\pi n}{N+1}-\cos\frac{\pi m}{N+1}}$,"I am writing perturbation theory for some linear operator, finding the first-order corrections to the components of eigenvectors boils down to the following sum: $$\sum\limits_{{}^{m=1}_{m\ne n}}^{N}  (1 + (-1)^{m+n}) \frac{\sin \left( \frac{\pi m k}{N+1} \right) \sin \left( \frac{\pi m}{N+1} \right)}{\cos\left(\frac{\pi n}{N+1}\right) - \cos\left(\frac{\pi m}{N+1}\right)},$$ where $k,n=1,2,\ldots,N$ , and $N$ are integers. It is really desirable that this answer has a simple analytical expression, as otherwise it has no use, but at the moment it does not look like exactly summable. Does anyone have ideas on how to approach this? Even the answer in the limit of large $N \gg 1$ would be nice. UPDATE: I think this is impossible. Here is why. Let us just ignore the factor in front for simplicity (it will erase half of the terms simply), and our goal is to get rid from this difference in the denominator. Assume the part of the sum for which $m>n$ . In this case $\cos(\pi n/(N+1))>\cos(\pi m/(N+1))$ , and we can write: $$ \frac{1}{\cos\left(\frac{\pi n}{N+1}\right)} \sum\limits_{m=n+1}^{N} \sin \left( \frac{\pi m k}{N+1} \right) \sin \left( \frac{\pi m}{N+1} \right) \frac{1}{1 - \frac{\cos\left(\frac{\pi m}{N+1}\right)}{\cos\left(\frac{\pi n}{N+1}\right)}} = \\\frac{1}{\cos\left(\frac{\pi n}{N+1}\right)}  \sum\limits_{l=0}^{\infty} \cos^{-l}\left(\frac{\pi n}{N+1}\right) \sum\limits_{m=n+1}^{N}  \sin \left( \frac{\pi m k}{N+1} \right) \sin \left( \frac{\pi m}{N+1} \right)  \cos^l\left(\frac{\pi m}{N+1}\right).$$ Obviously, for $n>m$ one has to do the inverse, and perform a similar expansion of the $\frac{1}{1-x}$ function. The only option one has is to convert all trigonometric functions into exponents. $\cos^l\left(\frac{\pi m}{N+1}\right) = \frac{1}{2^l} \sum\limits_{p=0}^{l} C(l,p) e^{i m (l-p) \phi } e^{-i p m \phi },$ where $C(l,p)$ are the binomial coefficients. So, one can get rid of the summation over $m$ , but will be left with other two (over $p$ , and $l$ ), and it does not look like any special function or smth like this. UPDATE OF 27.07.2021: As metamorphy has proven, I was wrong, and it is possible to find a closed-form solution :). The final answer is: $$ - \sin\left( \dfrac{\pi n k}{N+1} \right) \cot\left( \dfrac{\pi n}{N+1}\right) - (2k-N-1)  \cos\left( \dfrac{\pi n k}{N+1} \right)   $$","['trigonometric-series', 'calculus', 'trigonometry', 'summation']"
4199175,Correction: Calculating distribution function and determining density function of $Y =2X$,"Since we were not provided any solutions to our statistics exercises, I wanted to ask you guys for any corrections or errors I did on this exercise. I will only upload a screenshot from my notes app. I hope that's ok. The exercise was:
Let $X$ be a random variable with density $fX : \mathbb{R} \to \mathbb{R}$ defined by
fX(x) = (1
2
for x ∈ [2, 4]
0 else a) Calculate the corresponding distribution function. b) Determine the density of the random variable Y := 2X Thank you :)","['statistics', 'probability-distributions', 'probability-theory', 'stochastic-calculus', 'density-function']"
4199197,With $x^2+y^2=1$ find Minimum and Maximum of $x^5+y^5$ (do not use derivative),"It's easy to see that the minimum is $-1$ and maximum is $1$ . My idea is put $x=\cos(a)$ , $y=\sin(a)$ and $t=x+y$ ,   so I have $-\sqrt{2}\le t \le \sqrt{2}$ then $0 \le t^2 \le 2$ When $t=x+y$ then $(x+y)^2=1+2xy$ . Then \begin{aligned}
x^5+y^5&=(x^2+y^2)(x^3+y^3)-x^2y^2(x+y)\\
&=x^3+y^3-x^2y^2(x+y)\\
&=(x+y)(x^2-xy+y^2)-x^2y^2(x+y)\\
&=(x+y)(1-xy-x^2y^2)\\
&=\left( {x + y} \right)\left( {1+\frac{1}{2} - {{\left( {x + y} \right)}^2} - \frac{{{{\left( {x + y} \right)}^4}}}{4} + \frac{{{{\left( {x + y} \right)}^2}}}{2} - \frac{1}{4}} \right)\\
&=t\left( {\frac{5}{4}  - \frac{{{t^4}}}{4}} \right)\\
\end{aligned} This problem is for those who have not studied derivatives so I dont have any idea for next step. Anyone can help me for the hint or other solutions? Very Thanks","['trigonometry', 'inequality']"
4199233,Whether the power series $z^{2^{n}}$ converges at the boundary?,"I want to find the radius of convergence of the series $$\sum z^{2^{n}}$$ and whether it converges at this radius as well. Attempt: $$\sum z^{2^{n}} = \sum a_kz^k $$ , where $$a_k=\begin{cases} 1, k=2^n \text{ with } n \in \mathbb{N}\\0, \text{ otherwise}\end{cases}$$ From this, $\limsup (a_k)^{1/k} = 1$ , which implies that $R=1$ . Clearly, if $z=1, z=-1, z=i, z=-i,$ the series diverges because it will just be the series $\sum 1$ . But, how do I systematically check all cases with $\lvert z \lvert = 1$ ?","['power-series', 'convergence-divergence', 'analysis', 'sequences-and-series']"
4199234,"Proving Diamond & Shurman Exercise 3.7.1, about conjugacy class of $\Gamma_0^{\pm}(N)$.","I am reading chapter 3 of A First Course in Modular Forms but have troubles in Exercise 3.7.1 (c) and (d). (c) Show that the $\Gamma_0^{\pm}(N)$ -conjugacy class of $\gamma \in \Gamma_0(N)$ is the union of the $\Gamma_0(N)$ conjugacy classes of $\gamma$ and $\begin{bmatrix}  1 & 0 \\ 0 & -1 \end{bmatrix} \gamma \begin{bmatrix}  1 & 0 \\ 0 & -1 \end{bmatrix}$ . Show that if $\gamma$ has order $4$ or $6$ then this union is disjoint. (d) Let $\gamma = \begin{bmatrix}  1 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix}  0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix}  1 & 1 \\ 1 & 2 \end{bmatrix}^{-1} = \begin{bmatrix}  3 & -2 \\ 5 & -3 \end{bmatrix}$ , an order- $4$ element of $\Gamma_0(5)$ . Show that $\gamma$ is not conjugate to its inverse in $\Gamma_0(5)$ . Here let $\mathrm{GL}_2(\mathbb{Z})$ be the group of invertible $2 \times 2$ matrices with integer entries, $\mathrm{SL}_2(\mathbb{Z})$ the group of $2 \times 2$ matrices with integer entries and determinant $1$ , and \begin{align*}
\Gamma_0^{\pm}(N) &= \left\{ \begin{bmatrix}  a & b \\ c & d \end{bmatrix} \in \mathrm{GL}_2(\mathbb{Z}) : \begin{bmatrix}  a & b \\ c & d \end{bmatrix} \equiv \begin{bmatrix}  * & * \\ 0 & * \end{bmatrix} \pmod{N} \right\} \\
\Gamma_0(N) &= \left\{ \begin{bmatrix}  a & b \\ c & d \end{bmatrix} \in \mathrm{SL}_2(\mathbb{Z}) : \begin{bmatrix}  a & b \\ c & d \end{bmatrix} \equiv \begin{bmatrix}  * & * \\ 0 & * \end{bmatrix} \pmod{N} \right\}.
\end{align*} I have proved the first part of (c): the map \begin{align*}
\left(\Gamma_0^{\pm}(N) \setminus \Gamma_0(N)\right) &\longrightarrow \Gamma_0(N) \\
\alpha &\longmapsto \alpha \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
\end{align*} is a bijection, so \begin{align*}
\left\{ \beta \gamma \beta^{-1}: \beta \in \Gamma_0^{\pm}(N) \setminus \Gamma_0(N) \right\} 
= \left\{ \alpha \begin{bmatrix}  1 & 0 \\ 0 & -1 \end{bmatrix} \gamma \begin{bmatrix}  1 & 0 \\ 0 & -1 \end{bmatrix} \alpha^{-1}: \alpha \in \Gamma_0(N) \right\}.
\end{align*} How to show the remaining parts? For part (d), I think one cannot apply (c) directly because (d) serves as an example of the following statement in P.93 The extended conjugacy class of $\gamma$ under $\Gamma_0^{\pm}(N)$ is not in general the union of the conjugacy class of $\gamma$ and $\gamma^{-1}$ under $\Gamma_0(N)$ .","['algebraic-number-theory', 'group-theory', 'modular-forms', 'modular-group']"
4199298,d(dx) exterior derivative in simple language?,"I’m only learning the basics of analysis, so I have no understanding of exterior derivatives. I simply need clarification on my misconception using simple words. I’m reading about second degree derivatives and found the following $$d\left( dy\right) =d\left( f'\left( x\right) dx\right)= f''(x) dx * dx= f''\left( x\right) \left( dx\right) ^{2}$$ So I was thinking of expanding $d(f’(x)dx)$ to $f’’(x) * dx + f’(x) * d(dx)$ . According to the axioms of exterior derivative, the latter part should equals to zero. However I was thinking $dx = 1 * \triangle(x)=(x-x_0)$ , $d(dx) = d(x - x_0) = d(x) - d(x_0) = d(x)$ According to the definition of the exterior derivative, this is wrong. I’m confused with the misconception I have. Please help me clarify this. Update on maybe a partial solution, I’ve done some research on the web and found that maybe dx should be viewed as a constant. In that case d(dx) = 0. Can we view dx as a constant at all?","['calculus', 'soft-question', 'derivatives', 'real-analysis']"
4199312,Help pulling back the tautological one-form on the cotangent bundle $T^*M$ to the tangent bundle $TM$ using a Riemannian metric on $M$,"I am struggling with exercise 5 page 304 in Global Aspects of Classical Integrable Systems ,Cushman and Bates Let $g$ be a Riemannian metric on a smooth manifold $M$ . In
local coordinates $x = (x_1,... x_n)$ the metric may be written as $g = \sum g_{ij} dx^i \otimes dx^j$ Let $v = (x, \nu) = (x^1,..., x^n, \nu^1,... \nu^n)$ be natural coordinates on TM. Show that the pullback $\theta_g$ by the map $g^\#$ to $TM$ of the canonical 1-form $\theta$ on $T^*M$ may be written as $\theta_g(\nu) = \sum g_{i j}\nu^i dx^j$ Setting the problem: Take coordinates on $T^*M$ to be $m = (x, p)$ Canonical 1-form $\theta$ on $T^*M$ \begin{equation}
    \begin{aligned}
    \theta\in \chi ^*(T^*M)\\
    \theta: \chi (T^*M) \rightarrow C^\infty(\mathbb{R})\\\theta_m: T_m(T^*M) \rightarrow \mathbb{R}
    \\
    \theta_{m=(x, p)} = \sum_i p_i dx^i
    \end{aligned}
\end{equation} $\;\;\;\;\;$ * Let $w \in T_m(T^*M), \quad w = \sum_i^n w_i \dfrac{\partial}{\partial x^i} + \sum_{i=1}^{n} w'_{i} \dfrac{\partial}{\partial p^i}$ $\;\;\;\;\;$ * Then $\theta_m(w) = \sum_i w_ip_i$ . The map $g(x)^\#$ on $T_xM$ is the isomorphism induced by $g(x)$ between $T_xM$ and $T^*_xM$ \begin{equation}
\begin{aligned}
&g^\#(v) = g(x)^\#(\nu) \text{ is such that }
[g(x)^\#(\nu)](\mu) = g_x(\nu, \mu), \quad \mu \in T_xM
\end{aligned}
\end{equation} Pullback of a 1-form (Wikipedia): Let $\phi:M \rightarrow N$ be a smooth map between smooth manifolds, and let $\alpha$ be a 1-form on $N$ .
\newline Then the pullback of $\alpha$ by $\phi$ is the 1-form $\phi^*\alpha$ on $M$ defined by: \begin{equation}
(\phi^*\alpha)_x(X) = \alpha_{\phi(x)}(d\phi_x(x))
\end{equation} for $x \in M$ and $X \in T_xM$ Property of the tautological one-form from Wikipedia: The tautological one-form is the unique one-form that ""cancels"" the pullback. The tautological one-form $\theta$ is the only form with the property that $\beta^*\theta = \beta$ , for every 1-form $\beta$ on $Q$ My confusion and my attempt While $\theta$ is a one-form on $T^*M$ , it seems to me that $g^\#$ maps to $T^*_xM$ . (Indeed once we feed the metric a tangent vector we have decided on an attachment point $x$ of the manifold) My solution was instead to define $g^\#:TM \rightarrow T^*M$ \begin{equation}
g^\#(x, \nu) = (x, g^2(x)^\#(\nu))
\end{equation} where $[g^2(x)^\#(\nu)](\mu) = g(x)(\nu, \mu)$ Attempt component-wise \begin{equation}
\begin{aligned}
\theta_g(v) = [(g^\#)^*\theta] (v) &= \theta ( g^\#(v))\circ dg^\#_v \\
&= \theta ( g^\#(x^i, \nu^i))\circ dg^\#_v\\
&= \theta(\sum g_{ij}(x) \nu^i dx^j) \circ dg^\#_v\\
&= (\sum g_{ij}(x) \nu^i dx^j) \circ dg^\#_v 
\end{aligned}
\end{equation} Is this a valid method ? Can I show $dg^\#_v$ acts as the identity map on $\dfrac{\partial}{\partial x^i}$ components ? I have also been attempting a solution using the ""tautological one form cancels pullback"" identity My ""solution"" has been to fix $\nu$ while varying $x$ thus defining the one-form: $$g^\#_\nu: M \rightarrow T^*M
$$ $$g^\#_\nu(x) \text{ is such that: }$$ $$g^\#_\nu(x)(\mu) = g_x(\nu, \mu)
$$ Pulling $\theta$ back by $g^\#_\nu$ : $$\theta_g = (g^\#_\nu)^*\theta = g^\#_\nu = \iota_\nu g = \sum g_{ij} \nu^i dx^j
$$ There are obviously several problems with this solution, $\theta_g$ acts in $M$ instead of $TM$ and it is odd to fix a covector $\nu$ in this way. I am unsure how to define $g^\#$ to get spaces to properly match up, I am unsure how to deal with the fact the tautological form on $T^*M$ ""looks"" like a one-form on $M$ Any help with this exercise, or suggestions on simpler exercises to tackle that migh help would be very welcome.","['symplectic-geometry', 'differential-forms', 'differential-geometry']"
4199318,Cannot understand the conclusion of the proof that $\lim_\limits{x \to \infty} ( 1 + \frac{1}{x})^{x} = e$,"I'm struggling to understand an unusual proof that $\lim \limits_{x \to \infty} \left( 1 + \frac{1}{x} \right)^{x} = e$ Note: before starting this proof we know already the following limit for $a_{n}$ $\lim \limits_{n \to \infty} \left( 1 + \frac{1}{n} \right)^{n} = e$ . The way the proof proceeds is the following: ""we know that $a_{n}$ is a growing sequence that goes to $e$ , therefore for each $\epsilon > 0 $ , there is a $v$ (which we can assume to be bigger than $\frac{1}{\epsilon}$ ) such that for each $n > v$ we have"": $e - \epsilon < \left( 1 + \dfrac{1}{n} ​\right)^{n} < e$ Here I understand that this comes from the fact that: $ \left| \left(1+\dfrac{1}{n} \right)^{n} - e  \right| < \epsilon$ and the member on the the RHS ( $e$ ) comes from the fact that $\epsilon + e > \epsilon$ but please correct me if I'm wrong. Now, assuming that $x > v+1$ and $[x]$ being the integer part of $x$ , we have $[x] > v$ and the author proceeds to squeeze $\left( 1 + \dfrac{1}{x}\right)^{x}$ as follows $\left( 1 + \dfrac{1}{[x] + 1}\right)^{[x]} < \left(1 + \dfrac{1}{x}\right)^{x} < \left( 1 + \dfrac{1}{[x]}\right)^{[x]+1}$ Then there are some additional steps here that I can follow without problems (I can add them if needed), at the end the author ends up with: $e - (1+e)\epsilon \leq \left( 1 + \dfrac{1}{x}\right)^{x} \leq e + e\epsilon$ and concludes by saying that this proves the original statement that $\lim\limits_{x \to \infty} \left( 1 + \frac{1}{x}\right)^{x} = e$ . This is the part that I do not understand, I could have understood that if it were $e-\epsilon$ on the left side and $e+\epsilon$ on the right side, that will fit the definition of the limit; is there any reason why a multiple of $\epsilon$ will continue to fit the definition? I spent hours for the past two days looking for an explanation of that or a similar proof and I couldn't find anything.","['limits', 'proof-explanation', 'real-analysis']"
4199364,Bachelier model option pricing [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . This post was edited and submitted for review 2 years ago and failed to reopen the post: Original close reason(s) were not resolved Improve this question Consider a Brownian motion $W_t$ and Bachelier model $S_t = 1 + \mu t + \sigma W_t$ for the stock price $S_t$ . Find the value of an option that pays $1(S_1 > 1)$ . Attempt: As I understand it, the answer is $P(S_1 > 1)$ , which yields a big integral with parameter $\mu$ . I suspect there should be a simple answer. Any help is greatly appreciated! Update: Kurt and Maximilian, thank you for your help. However, I don't believe the answers given are correct. As I understand it now, you are supposed to use risk neutral valuation argument by constructing delta-hedging portfolio, not evaluate the probability directly.
Interestingly, in Black-Scholes or Bachelier models, when evaluating options with risk-neutral valuation, the result usually doesn't depend on drift term $\mu$ .","['finance', 'stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory']"
4199379,"Proving convergence of a sequence $\{x_{n}\}$ satisfying$|f( x_{n}) |=|f'( x_{n+1}) ||x_{n} -a|$,where $f$ is a function satisfying some conditions.","Theorem: Let $\displaystyle f:[ a,b]\rightarrow \mathbb{R}$ be a function differentiable on $\displaystyle [ a,b]$ such that $\displaystyle f( a) =0$ and that there exists $\displaystyle A\in \mathbb{R}$ such that $\displaystyle |f'( x) |\leq A|f( x) |$ for all $\displaystyle x\in [ a,b]$ , then $\displaystyle f\equiv 0$ on $\displaystyle [ a,b]$ . I tried to prove the above stated theorem and in the process I came across the question in title. Here's what I tried to prove the above stated theorem: For $\displaystyle A=0,$ the result is true so let $\displaystyle A >0.\ $ Suppose on the contrary that there exists $\displaystyle x_{0} \in ( a,b]$ such that $\displaystyle f( x_{0}) \neq 0$ . By MVT, there exists an $\displaystyle x_{1} \in ( a,x_{0})$ such that $\displaystyle |f( x_{0}) -f( a) |=|f'( x_{1}) ||x_{0} -a|\Longrightarrow |f( x_{0}) |=|f'( x_{1}) ||x_{0} -a|$ . Similarly, there exists $\displaystyle x_{2} \in ( a,x_{1})$ such that $\displaystyle |f( x_{1}) |=|f'( x_{2}) ||x_{1} -a|$ .  We continue like this and having found $\displaystyle x_{0} ,x_{1} ,x_{2} ,...,x_{n-1}$ we find $\displaystyle x_{n}$ by applying MVT on $\displaystyle [ a,x_{n-1}]$ so the construction of decreasing sequence $\displaystyle \{x_{n} \}$ can be done. So we have a sequence $\displaystyle \{x_{n} \}$ such that $\displaystyle x_{n}  >x_{n+1}$ for all $\displaystyle n\in \mathbb{N} \cup \{0\}$ and $\displaystyle |f( x_{n}) |=|f'( x_{n+1}) ||x_{n} -a|$ for all $\displaystyle n\in \mathbb{N} \cup \{0\}$ . It is clear that the sequence is bounded below by $\displaystyle a$ and hence it must converge. Intuitively, it seems to me that the sequence $\displaystyle \{x_{n} \}$ converges to $\displaystyle a$ . But I am having difficulties proving it (please refer Note ). Assuming that $\displaystyle x_{n}\rightarrow a$ , the theorem can be proved as follows: We have: $\displaystyle |f( x_{0}) |=|f'( x_{1}) ||x_{0} -a|\leq A|f( x_{1}) ||x_{0} -a|=A|f'( x_{2}) ||x_{0} -a||x_{1} -a|\leq A^{2} |f( x_{2}) ||x_{0} -a||x_{1} -a|$ Proceeding like this to get: \begin{equation*}
|f( x_{0}) |\leq A^{n+1} |f( x_{n+1}) ||x_{0} -a||x_{1} -a|...|x_{n} -a|\leq A^{n+1} s|x_{0} -a||x_{1} -a|...|x_{n} -a|
\end{equation*} where $\displaystyle s=\max |f|[ a,b]$ , which is finite because $\displaystyle |f|$ is continuous on $\displaystyle [ a,b] .$ I take $\displaystyle n-$ th root on both sides to get: \begin{equation*}
|f( x_{0}) |^{\frac{1}{n}} \leq A^{\frac{1}{n} +1} s^{\frac{1}{n}}( |x_{0} -a||x_{1} -a|...|x_{n} -a|)^{\frac{1}{n}}
\end{equation*} Now taking $\displaystyle n\rightarrow \infty $ on both sides to get $\displaystyle |f( x_{0}) |^{\frac{1}{n}}\rightarrow 0$ (please note that $\displaystyle ( |x_{0} -a||x_{1} -a|...|x_{n} -a|)^{\frac{1}{n}}\rightarrow 0$ because $\displaystyle x_{n}\rightarrow a$ ). Now $\displaystyle \left( |f( x_{0}) |^{\frac{1}{n}}\rightarrow 0\right) \Longrightarrow |f( x_{0}) |=0$ , which is a contradiction. Note: Let $\displaystyle x_{n}\rightarrow l$ then from the iterative definition of our $x_{n}$ , i.e. $\displaystyle |f( x_{n}) |=|f'( x_{n+1}) ||x_{n} -a|$ and noting that $|f|$ is continuous, we should get $\displaystyle |f( l) |\leq A|f( l) ||l-a|$ , which is where I got stuck as it does not give me $\displaystyle l=a$ . Please help me to prove that $\displaystyle x_{n}\rightarrow a$ . Thanks.","['proof-writing', 'sequences-and-series', 'real-analysis']"
4199416,"Real analysis uniform convergence: Suppose that $f_n : (a,b) \to \mathbb R $ is a sequence of differentiable functions such that there is ...","Suppose that $f_n : (a,b)\to \mathbb R $ is a sequence of differentiable functions such that there is $c \in (a, b)$ satisfying $\lim_{n \to \infty} f_n(c) = L \in \mathbb R$ . Suppose further that given $[\alpha, \beta] \subset (a, b)$ , the sequence $(f'_n)$ converges uniformly to the null function in $[\alpha, \beta]$ . Show that in each $[\alpha, \beta] \subset (a, b)$ the function $f_n$ converges uniformly to the constant function equal to $L$ . What I've managed to do so far is this Fix $\epsilon \gt 0$ ,and choose $N$ such that $m, n \ge N$ implies $|f_n(x_0) − f_m(x_0)| \lt \frac{\epsilon}{2}$ and $|f'_n(t) − f'_m(t)| \lt \frac{\epsilon}{2(b-a)}$ for all $t \in [a, b]$ . So lets say $g = f_n −f_m$ is small on the entire interval $[a, b]$ , now applying the Mean Value Theorem to $g$ , we get $$|g(x) − g(t)| = |x − t||f'(c)| \le |x − t| \frac{\epsilon}{2(b − a)} \le \frac{\epsilon}{2}$$ valid for all $x$ , $t \in[a, b]$ and all $m$ , $n \ge N$ . Now for all $x \in [a, b]$ $$|g(x)| \le |g(x) − g(x_0)| + |g(x_0)| \lt \epsilon$$ This shows that the sequence $f_n$ is uniformly Cauchy, hence uniformly converges to some function, $f$ on $[a, b]$ I'm in the right way? Thanks in advance for any help.","['solution-verification', 'derivatives', 'uniform-convergence', 'real-analysis']"
4199417,Positive linear functional $\phi\colon C(X)\to\mathbb{C}$ satisfies $|\phi(f)|\leq\phi(|f|)$,"Let $X$ be a compact (Hausdorff) space. Let $C(X)$ denote the linear space of continuous complex-valued functions on $X$ . Suppose that $\phi\colon C(X)\to\mathbb{C}$ is a positive linear functional. Thus $\phi(f)\geq0$ whenever $f\geq0$ . I want to prove that $$|\phi(f)|\leq\phi(|f|)$$ for all $f\in C(X)$ . For real-valued $f$ this is easy. Indeed, decompose $f=f_{+}-f_{-}$ , where $f_{+}:=\max(0,f)$ and $f_{-}:=\max(0,-f)$ , and observe that $$|\phi(f)|=|\phi(f_{+})-\phi(f_{-})|\leq\phi(f_{+})+\phi(f_{-})=\phi(|f|).$$ But how do I prove this for complex-valued $f$ ? Also, is there a method that doesn't require proving the real-valued case first?","['absolute-value', 'operator-theory', 'real-analysis', 'functional-analysis', 'linear-transformations']"
4199429,Covering each point of the plane with circles three times,"The following problem arose from an Italian discussion group: I am not so sure about the optimal tags for the question, so feel free to improve them. Definition : we say that $E\subseteq\mathbb{R}^2$ admits a cover with degree $d$ if there is a family $\mathscr{F}$ of distinct circles (with positive radii) such that every point of $E$ belongs to exactly $d$ circles in $\mathscr{F}$ . Preliminary results : There is a cover of $\mathbb{R}^2\setminus\{O\}$ with degree $1$ given by concentric circles; There is no cover of $\mathbb{R}^2$ with degree $1$ , due to the accumulation point of any chain of nested circles; There is a cover of $\mathbb{R}^2$ with degree $2$ , for instance the one given by the unit circles centered at $(2n,y)$ for any $n\in\mathbb{Z},y\in\mathbb{R}$ ; There is a cover of $\mathbb{R}^2\setminus\{O\}$ with degree $2$ , for instance the one given by the circles tangent to the sides of a quadrant. Now the actual question, which I did not manage to tackle (yet): Is there a cover of $\mathbb{R}^2$ with degree $3$ ? I believe the answer is negative: I (unsuccessfully) tried to prove that any degree-3 cover admits a degree-2 subcover, in order to get a contradictory degree-1 cover by switching to the complement. Any insight is welcome. Small update : actually it is not possible to extract a degree-2 subcover from a (hypothetical) degree-3 cover. Once a circle $\Gamma_1$ is removed from a degree-3 cover, we are forced to remove a circle contained in the interior of $\Gamma_1$ and this leads to a chain of circles converging to a point. If a cover with degree 3 exists, every chain of contained circles has a finite number of elements. The elements of $\mathscr{F}$ form a POset: we may say that $\Gamma_1 < \Gamma_2$ if $\Gamma_1$ is contained in the interior of $\Gamma_2$ . This also allows to assign a parity to each element of $\mathscr{F}$ and each point of the plane, according to the length of a maximal chain contained in the circle / surrounding such point. Further thoughts : let us assume that a degree-3 cover of $\mathbb{R}^2$ exists. By the chain argument there is a disk $D$ such that $\partial D\in\mathscr{F}$ and every circle covering $\mathring{D}$ meets $\partial D$ at two non-antipodal points. Let us name $\mathscr{f}$ the family of circles covering $\mathring{D}$ . We have that $\partial D$ is covered three times, so the circles traversing each point of $\partial D$ are $\partial D$ itself and two elements of $\mathscr{f}$ . These couples of circles ""exiting"" from any point of $S^1=\partial D$ have to cover three times any point of $\mathring{D}$ , which is pretty strange. Many sub-questions came to my mind: Is it possible to partition $\mathscr{f}$ into two/three subfamilies of disjoint circles? Is it possible to use the descending chain condition as a substitute for continuity, then apply some topological trick? Are Tucker's lemma , Euler characteristic or the theory of planar graphs useful in some way? Starting from any $P_0\in\partial D$ we may take $P_{-1}$ and $P_1$ as the points of $\partial{D}$ connected to $P_0$ via the elements of $\mathscr{f}$ through $P_0$ , then define $P_{-2},P_{2},P_{-3},P_{3},\ldots$ in the same way. This gives a partition of $\partial D$ into finite cycles (a cycle of length $2$ might occur if $P_1=P_{-1}$ ) and ""infinite cycles"" with a numerability of elements. Infinite cycles have limit points, which are troublesome. On the other hand also a partition into finite cycles only does not seem to stand any chance of covering any point of $\mathring{D}$ exactly three times. Yet another measure-theoretic thought . Let $S^1=\partial D$ (which we may assume to have radius $1$ ), let $P\in S^1$ . Two elements of $\mathscr{f}$ go through $P$ : let $L(P)$ be the total length of the arcs given by the intersections with $\mathring{D}$ . Let us assume that $L:S^1\to (0,4\pi)$ is an integrable function. By integrating $L$ over $S^1$ we have that each arc in $\mathscr{f}\cap\mathring{D}$ is counted twice, hence $$ \int_{S_1} L = 2\int_{\text{arcs in }\mathscr{f}} 1=2\cdot 3\text{Area}(D) = 6\pi$$ and the average length of an arc in $\mathscr{f}$ is $\frac{3}{2}$ . It follows that most of the arcs of an ""integrable 3-cover"" of $\mathring{D}$ are pretty short, forcing a concentration of the arcs near the boundary of $D$ . This violation of uniformity probably leads to the fact that if a cover of $\mathbb{R}^2$ with degree $3$ exists, it is not integrable.","['general-topology', 'combinatorics', 'geometry']"
4199460,Squaring a Leibnitz-like series,"The series $$\frac{\pi^2}{8}=\frac{1}{1^2}+\frac{1}{3^2}+\frac{1}{5^2}+\cdots\tag1$$ can serve as either a corollary or an antecedent to the familiar Basel problem series $$\frac{\pi^2}{6}=\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots$$ Now suppose we take the square root of ${\pi^2/8}$ getting $\pi/(2\sqrt2)$ . This has its own Leibnitz-like series $$\dfrac{\pi}{2\sqrt2}=\dfrac{1}{1}+\dfrac{1}{3}-\dfrac{1}{5}-\dfrac{1}{7}+\cdots\tag2$$ where the sign reverses when the denominator crosses a multiple of $4$ . This latter series is provable in many ways, for instance with a Fourier series . Since the left side of Eq. 1 is the square of the left side of Eq. 2, it is tempting to suppose that somehow Eq. 1 could be derived by squaring Eq. 2. A brute force approach generates the right squared terms, but how do the cross-product terms cancel out to get the correct form for Eq. 1? I have done some work on the problem, identifying a method that appears to derive Eq. (2) from Eq. (1). The method is based on the ""long division"" method of square root extraction. Assume that the square root of $$\frac{1}{1^2}+\frac{1}{3^2}+\frac{1}{5^2}+\cdots$$ is to be rendered in the form $$s_0\dfrac{1}{1}+s_1\dfrac{1}{3}+s_2\dfrac{1}{5}+s_3\dfrac{1}{7}+\cdots$$ where each $s_k$ term is $\pm1$ and the signs are chosen to give back the original sum of squares with no cross-product terms. As with the usual version of the square root extraction algorithm, we begin with the square root of the first term of the series, thus $+1$ making the $s_0$ sign positive. We multiply this by $2$ to get $2$ , which we use later. Now compare the next term in the radicand, $+1/9$ , with $-1/2$ times the next term whose sign is to be determined, $1/3$ . The comparison figure is meant to compensate approximately for the rest of the infinite series in the radicand; that is for each odd $n$ the sum of terms following $1/n^2$ is approximated as $1/(2n)$ and our comparison figure of $-1/(2n)$ will compensate for that. Clearly $+1/9$ is greater, indicating that the exact square root will be greater than our current partial sum ( $1$ ). Like a one-dimensional game of ""hot and cold"", we therefore select $s_1=+1$ , making our second partial sum $1+(1/3)$ and its doubled value, $2+(2/3)=8/3$ , to be used in the next round. Now add the $+1/3$ term we just rendered to the doubled value of the pervious sum, giving $2+(1/3)=7/3$ , then multiply by $+1/3$ to get $7/9$ .  Subtract this from $+1/9$ , which was our previous leading term, and combine with the next term $1/25$ from the radicand to get $-49/75$ as our next leading term. This is now less than $-1/2$ times the $1/5$ term we are signing next, so we assume our square root must be less than $1+(1/3)$ and thus render $s_2=-1$ , giving $1+(1/3)-(1/5)$ as our third partial sum and twice that equaling $(8/3)-(2/5)=34/15$ . Adding $-1/5$ to $34/15$ gives $31/15$ and multiplying the latter by the $-1/5$ term we introduced gives $-31/75$ , which when subtracted from $-49/75$ yields $-6/25$ . Combining this with $1/49$ gives $-269/1225$ as our next leading term. This is still less than $(-1/2)×(1/7)=-1/14$ , so we will assign $s_3=-1$ for the $1/7$ term in our square root. The sign pattern thus far ( $+(1/1)+(1/3)-(1/5)-(1/7)$ ) matches that for the series in Eq. (2), and when I set up the procedure on a MS Excel spreadsheet the correct sign pattern is found to hold for at least 101 terms. This approach can be converted to a proof of the claim if we can prove that: The algorithm described above will continue to give the sign cycle $++--$ . Whenever we truncate the square root series at a positive term it's square will be greater than the given radicand, but the opposite is true when we truncate at a negative term.","['pi', 'sequences-and-series']"
4199501,Every algebraic subgroup of an algebraic group is closed?,"I'm trying to show that every algebraic subgroup of an algebraic group is closed. If $H$ is an algebraic subgroup of $G$ , then so is a subvariety of $G$ . Hence, $H$ is locally closed in $G$ and equivalently it is open in its closure $\bar{H}$ . $\bar{H}$ is also an algebraic group. Clearly $H \subset \bar{H}$ , but how should I show the reverse?","['algebraic-geometry', 'algebraic-groups']"
4199575,Folland Theorem 1.14 extending premeasure to a measure,"1.14 Theorem Let $\mathcal A \subset \mathcal{P}(X)$ be an algebra, $\mu_0$ be a premeasure on $\mathcal A$ , and $\mathcal{M}$ the $\sigma$ -algebra generated by $\mathcal{A}$ . There exists a measure $\mu$ on $\mathcal{M}$ whose restriction to $\mathcal{A}$ is $\mu_0$ , namely $\mu=\mu^\star|\mathcal{M}$ where $\mu^\star$ is given by (1.12). If $v$ is another measure on $\mathcal{M}$ that extends $\mu_0$ , then $v(E)\leq \mu(E)$ for all $E\in \mathcal{M}$ , with equality when $\mu(E)\leq \infty$ . If $\mu_0$ is $\sigma$ -finite, then $\mu$ is the unique extension of $\mu_0$ to a measure on $\mathcal{M}$ . My question is on the second assertion for which the proof reads: As for the second assertion, if $E\in \mathcal{M}$ and $E\subset \bigcup_1^\infty A_j$ where $A_j\in \mathcal{A}$ , then $v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j)$ whence $v(E)\leq \mu(E)$ . The only step I can see is that by monotonicity, we have that $v(E)\leq v(\bigcup_1^\infty A_j)$ . I understand that $v=\mu_0$ on $\mathcal{A}$ however, how can I get to $v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j)$ and moreover, why does this imply that $v(E) \leq \mu(E)$ ?","['measure-theory', 'analysis', 'real-analysis']"
4199621,"Bounding variance with mean for a $[0,1]$ random variable","There is this inequality that I found in an article, which looks like folklore as for how its presented, or something that should be easily seen by the reader. I havent been able to wrap my head around it though. The inequality is the following $$\operatorname{Var}_{\mu} (f) \leq \frac{1}{2} \mu ( | f - \mu (f) |) $$ $\mu$ being a probability measure, so $\mu(f) $ standing for $f$ 's expected value with respect to $\mu$ , and $f$ being a random variable taking values in $[0,1] $ . Does anyone know how to see this? I remember having figured it out some time ago but now I dont see it no more..  Ive got the feeling that it should be something actually measure theoretic and not something following just from an inequality that holds for the function being integrated. Thanks in advance for any answer!","['variance', 'expected-value', 'upper-lower-bounds', 'probability-theory', 'probability']"
4199660,Proof verification: $\lim\limits_{s\to\infty}\zeta(s)=1$,"Note : I'm aware that there are much simpler proofs for this result. I decided to go with this approach because $(1)$ it was a nice challenge, and $(2)$ it makes use of a nice identity shown below. I've recently been spending time evaluating integrals involving the floor function. One of them was $$\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx$$ for integers $p\geq 0$ . By splitting the integral $$\int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx$$ into a sum of integrals indexed by the intervals $[1/(i+1),1/i]$ for $i=1,2,3,...,k-1$ , evaluating each of the integrals, and manipulating the resulting sum, I found that $$\int_\frac{1}{k}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=\frac{1}{(p+1)k}-1+\frac{1}{p+1}\left(\sum_{n=1}^{k-1}\frac{1}{n^{p+2}}+\sum_{m=2}^{p+1}\sum_{n=1}^{k}\frac{1}{n^m}\right)$$ which, after letting $k\to\infty$ , yields \begin{align}
\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= -1+\frac{1}{p+1}\left(\zeta(p+2)+\sum_{m=2}^{p+1}\zeta(m)\right)\\
&= -1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)
\end{align} Pretty neat! I believe I can use this equation to give an overkill proof of $\lim_{s\to\infty}\zeta(s)=1$ ( $s$ is a real parameter). Here is my attempt: Beginning We first note that the sequence $\{\zeta(n)\}_{n=2}^\infty$ certainly has a limit, since $\zeta$ is strictly decreasing and bounded below by $1$ (both of these facts easily follow from the series $\sum_{n=1}^\infty 1/n^s$ ), so \begin{align}
\lim_{n\to\infty}\zeta(n) &= L & (1)
\end{align} for some real number $L\geq 1$ . Fixing an arbitrary $\varepsilon>0$ , we infer that for some $N\in\mathbb{N}$ , $$L-\varepsilon<\zeta(n)<L+\varepsilon\text{ for every }n>N$$ $$\implies \sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon)\text{ for every }p>N$$ $$\implies (p+2-1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+2-1)(L+\varepsilon)\text{ for every }p>N$$ $$\implies (p+1)(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<(p+1)(L+\varepsilon)\text{ for every }p>N$$ $$\implies L-\varepsilon<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<L+\varepsilon\text{ for every }p>N$$ Since $\varepsilon>0$ was fixed arbitrarily, we can apply the prior sequence of implications to any positive real number. This shows that \begin{align}
\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n) &= L & (2)
\end{align} Now consider the identity $$\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx=-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)$$ We can write \begin{align}
0<\int_0^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx &= \int_0^\frac{1}{2} \frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1\frac{x^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx\\
&\leq \int_0^\frac{1}{2}\frac{\left(\frac{1}{2}\right)^p}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\int_\frac{1}{2}^1 \frac{x^p}{1}dx\\
&= \left(\frac{1}{2}\right)^p\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{\left(\frac{1}{2}\right)^{p+1}}{p+1}
\end{align} and thus $$0<-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\leq\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}}$$ Since $$\lim_{p\to\infty}\left(\frac{1}{2^p}\int_0^\frac{1}{2}\frac{1}{\left\lfloor\frac{1}{x}\right\rfloor}dx+\frac{1}{p+1}-\frac{1}{(p+1)\cdot 2^{p+1}}\right)=0$$ the Squeeze Theorem gives $$\lim_{p\to\infty}\left(-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\right)=0$$ which is equivalent to $\lim_{p\to\infty}\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)=1$ . Combining this with $(2)$ , we see that we must have $L=1$ . We deduce from $(1)$ that $$\lim_{n\to\infty}\zeta(n)=1$$ We now do the final push and get $\lim_{s\to\infty}\zeta(s)=1$ . Fix an arbitrary $\varepsilon>0$ . Since the sequence $\{\zeta(n)\}$ converges to $1$ and $\zeta(t)>1$ for every real $t>1$ , there is an $N\in\mathbb{N}$ such that $$0<\zeta(n)-1<\varepsilon\text{ for every }n>N$$ We know that $\zeta(s)<\zeta(n)$ for every real $s>n$ , so $$0<\zeta(s)-1<\zeta(n)-1<\varepsilon\text{ for every real }s>n>N$$ Since $\varepsilon$ was fixed arbitrarily, we can apply the preceding argument to every positive real number, so we are done. $\blacksquare$ Let me know what you think! If you identify any errors or optimizations, feel free to share them with me. Edit : as @stochasticboy321 kindly pointed out, my proof has a small error. You see, I can't deduce the inequality $$\sum_{n=2}^{p+2}(L-\varepsilon)<\sum_{n=2}^{p+2}\zeta(n)<\sum_{n=2}^{p+2}(L+\varepsilon)$$ from the fact that $L-\varepsilon<\zeta(n)<L+\varepsilon$ for every $n>N$ , since this assumes that the latter inequality is also true for $2,3,...,N$ . A correct approach would $(1)$ use the integral to prove that $$\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1$$ $(2)$ write $$\sum_{n=2}^{p+2}\zeta(n)=\sum_{n=2}^{N}\zeta(n)+\sum_{n=N+1}^{p+2}\zeta(n)$$ for an arbitrary $p\geq N-1$ , $(3)$ apply the inequality $L-\varepsilon<\zeta(n)<L+\varepsilon$ to get $$(L-\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n)<\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)(p+2-N)+\sum_{n=2}^{N}\zeta(n)$$ $$\implies (L-\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n)<\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)$$ $$\text{ and }$$ $$\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)<(L+\varepsilon)\left(\frac{p+2}{p+1}-\frac{N}{p+1}\right)+\frac{1}{p+1}\sum_{n=2}^{N}\zeta(n)$$ and $(4)$ let $p\to\infty$ to yield $$L-\varepsilon\leq\lim_{p\to\infty}\frac{1}{p+1}\sum_{n=2}^{p+2}\zeta(n)=1\leq L+\varepsilon$$ from which $L=1$ follows because $\varepsilon$ was fixed arbitrarily, so the inequality $|L-1|\leq\varepsilon$ will hold for any $\varepsilon>0$ . For the sake of honesty, I won't edit the original proof.","['riemann-zeta', 'calculus', 'solution-verification', 'sequences-and-series']"
4199661,"Direct limit, inverse limit and Spec","The set-up $k$ is a field $T_i = \operatorname{Spec} A_i$ is an inverse system of affine $k$ -schemes, where $i<j$ if $\operatorname{Spec} A_j \subset \operatorname{Spec} A_i$ (inclusion). $X$ is a $k$ -variety (hence locally of finite presentation). Then, by this porposition , we have $$ \operatorname{Mor}(\varprojlim_i T_i, X) = \varinjlim_i \operatorname{Mor}(T_i, X)$$ The confusion In the proof, we use the direct limit $A = \varinjlim_i A_i$ and then use it to create the inverse system $T_i$ using the fact that $\varphi : A \to B$ induces $\operatorname{Spec} \phi: \operatorname{Spec} B \to \operatorname{Spec} A$ via contraction. This should work sice the functor between rings and schemes is contravariant. However, the converse relation isn't true: spec of inverse limit of rings is not same as direct limit of spec of rings . The question How can we get the direct system $(A_i, \varphi_{ij})$ from the above inverse system $(T_i, f_{ij})$ ? In particular, what is the meaning of this statement $$\varprojlim_i \operatorname{Spec} A_i = \operatorname{Spec} \varinjlim_i A_i$$ Edit I have realized that this was a dumb question. The scheme morphism $\operatorname{Spec} A \to \operatorname{Spec} B$ by definition gives the ring  homomorphism $B \to A$ . Moreover, as commented below by  Martin Brandenburg and Zhen Lin on the linked question, spectrum commutes with filtered colimits (direct limit).","['limits-colimits', 'homological-algebra', 'commutative-algebra', 'algebraic-geometry', 'schemes']"
4199683,Two definitions of linearly independent vector fields,"I'm studying the chapter of Lee's ISM on vector fields, and there's one thing that confuses me a lot. If $\mathfrak{X}(M)$ denotes the set of smooth vector fields on a smooth $n$ -manifold, then it is a vector space under pointwise vector addition and scalar multiplication: $$(aX+bY)_p=aX_p+bY_p,\quad X,Y\in\mathfrak{X}(M).$$ Since $\mathfrak{X}(M)$ is a vector space, we can introduce the notion of linearly independence: $$a_1 X_1+...+a_k X_k=0\Rightarrow a_1=...=a_k=0.$$ This is common material that can be found in any Linear Algebra course and is familiar to me. But in a while Professor Lee brought up an idea that seems like the one presented above. He said that an ordered $k$ -tuple $(X_1,...,X_k)$ of vector fields defined on a set $A\subseteq M$ is called linearly independent if $(X_1|_p,...,X_k|_p)$ is linearly independent in $T_p M$ for each $p\in A$ . Are these two ideas talking about the same thing? More precisely, are theses two statements equivalent if we consider $A$ to be all of $M$ ? Thank you.","['vector-fields', 'smooth-manifolds', 'differential-geometry']"
4199687,A Jacobian-type criterion for Zariski open algebras,"Let's say that a map $A\to B$ of commutative rings is a Zariski open algebra if the corresponding map on spectra $\operatorname{Spec}(B)\to \operatorname{Spec}(A)$ is an open immersion of schemes. If $A\to B$ is a map of commutative rings such that $$B=A[x_1,\dots,x_n]/(f_1,\dots,f_m),$$ with $0\leq m\leq n,$ we have a Jacobian criterion for smoothness (resp. étaleness) that tells us that $B$ is smooth (resp. étale) if the image of the Jacobian determinant of the family $\{f_i\}_{i=1}^m$ with respect to the first $m$ indeterminates $\{x_i\}_{i=1}^m$ in $B$ is invertible (resp. in the case $m=n$ ). So what I'm wondering is if there is a further condition that we can use to see when such a presentation in the case $n=m$ is actually a Zariski open algebra.   That is, suppose we're given a family $\{f_1,\dots,f_n\} \subset A[x_1,\dots,x_n]$ that satisfies the Jacobian criterion for étaleness.  Can we see when such a presentation is actually a Zariski open algebra?","['algebraic-geometry', 'commutative-algebra']"
4199693,"Can we get a soldering ""form"" whose pull-backs encode arbitrary tensor components?","Context/notation: If $M^n$ is a smooth manifold, and $\pi\colon {\rm Fr}(TM) \to M$ denotes its frame bundle (it is a principal ${\rm GL}_n(\Bbb R)$ -bundle whose elements are pairs $(x,\mathfrak{v})$ with $x \in M$ and $\mathfrak{v}$ an ordered basis for $T_xM$ ), we have the soldering form $\theta \in \Omega^1({\rm Fr}(TM); \Bbb R^n)$ , defined by $$\theta_{(x,\mathfrak{v})}(X) = [{{\rm d}\pi}_{(x,\mathfrak{v})}(X)]_{\mathfrak{v}},\qquad \mbox{for}
\quad X \in T_{(x,\mathfrak{v})}{\rm Fr}(TM),$$ where $[\cdot]_{\mathfrak{v}} \colon T_xM \to \Bbb R^n$ takes a tangent vector to its column vector in $\Bbb R^n$ of components relative to $\mathfrak{v}$ . If $U \subseteq M$ is open and $\mathfrak{e}$ is a local frame on $U$ (that is, a local section of ${\rm Fr}(TM)$ ), the pull-back $\mathfrak{e}^*\theta \in \Omega^1(U; \Bbb R^n)$ satisfies $$(\mathfrak{e}^*\theta)_x(v) = [v]_{\mathfrak{e}_x}$$ for all $x \in U$ and $v \in T_xM$ . It also has nice properties like $R_A^*\theta = A^{-1}\circ \theta$ for all $A \in {\rm GL}_n(\Bbb R)$ , where $R_A\colon {\rm Fr}(TM) \to {\rm Fr}(TM)$ is the map ""change of basis via $A$ "" (it's a consequence of the general formula $[v]_{\mathfrak{v}A} = A^{-1}[v]_{\mathfrak{v}}$ ). This construction does not work replacing $TM \to M$ with an arbitrary vector bundle $E \to M$ because the derivative of the bundle projection takes values in tangent spaces to $M$ , and not on the fibers of $E$ . Question. Now consider the tensor bundle $TM^{\otimes r} \otimes T^*M^{\otimes s}$ . Since a basis for a vector space gives rise to a basis of any associated tensor space, doing this pointwise we get (with suggestive notation) a map $${\rm Fr}(TM) \ni (x,\mathfrak{v}) \mapsto (x, \mathfrak{v}^{\otimes r}\otimes (\mathfrak{v}^*)^{\otimes s}) \in {\rm Fr}(TM^{\otimes r}\otimes T^*M^{\otimes s}).$$ I was wondering if whether we get some object $\Theta$ (which apparently will not be a $1$ -form) such that for each local frame $\mathfrak{e}$ on some open set $U \subseteq M$ , $\mathfrak{e}^*\Theta$ (whatever this is) will satisfy $$(\mathfrak{e}^*\Theta)_x(T) = [T]_{\mathfrak{e}_x^{\otimes r}\otimes (\mathfrak{e}_x^*)^{\otimes s}}, \quad\mbox{for}\quad T \in T_xM^{\otimes r}\otimes T_x^*M^{\otimes s}$$ Understanding the case $(r,s) = (0,1)$ would already provide some insight. For the same reason why the original construction doesn't work for arbitrary $E\to M$ , the derivative of the projection ${\rm Fr}(T^*M) \to M$ will take values in tangent spaces to $M$ instead of cotangent spaces (this construction should be natural and not require identifications between $TM$ and $T^*M$ , and even by taking a Riemannian metric on $M$ , it's not clear how this sorry attempt would carry to general $(r,s)$ ). I'm not sure what to try and can only guess that general bundle morphisms should enter the picture somehow. I know that this is a bit vague but hopefully I managed to convey what I want here. Certainly someone has thought about this already, so maybe there's just some terminology I'm not aware of. Any comments are welcome. Thank you!","['principal-bundles', 'tensors', 'vector-bundles', 'differential-forms', 'differential-geometry']"
4199694,Entropy of shuffled sequence split into two groups,"Let it be a sequence of $N$ elements that can be ordered. At each draw, all elements from the sequence are shuffled. Example with $N = 8$ : $draw_1 = (e_4, e_3, e_0, e_6, e_1, e_7, e_2, e_5)\\
draw_2 = (e_7, e_2, e_0, e_1, e_8, e_3, e_6, e_4)\\
draw_3 = (e_4, e_0, e_3, e_2, e_6, e_5, e_7, e_1)\\
...$ Within a sequence, elements can be pairwise compared. Since there are $N!$ different orderings, each draw has an entropy of $log_2(N!)$ bits. Now, let's modify the system such that there is two sequences of $N/2$ elements. At each draw, elements are shuffled and half of them goes to the first sequence and the other half to the second sequence. Example with $N=8$ : $
draw_1 = \{(e_4, e_3, e_7, e_0), (e_2, e_1, e_5, e_6)\}\\
draw_2 = \{(e_6, e_3, e_1, e_7), (e_5, e_0, e_2, e_4)\}\\
draw_3 = \{(e_7, e_1, e_5, e_4), (e_0, e_6, e_2, e_3)\}\\
...
$ What is the Shannon entropy of such system if the elements can only be pairwise compared between the two sequence and not within the same sequence? For example, let's define the ordering as $e_n$ < $e_{n+1}$ . In $draw1$ , $e_4$ is compared to $e_2$ , $e_1$ , $e_5$ and $e_6$ ; $e_3$ is compared to $e_2$ , $e_1$ , $e_5$ and $e_6$ etc. We can use a bit sequence to describe the draw, for example $draw_1=$ 1100110011110000. I am looking for the entropy of such a sequence. The full context of this question is described in this crypto.stackexchange question .","['order-theory', 'combinations', 'combinatorics', 'entropy']"
4199696,When is the group $C^*$-algebra of a locally compact group an AF-algebra?,"It is known that the group $C^*$ -algebra of a compact group is an AF-algebra. I want to know if given a non-compact locally compact group $G$ , does there exist conditions on $G$ which imply that the (full or reduced) group $C^*$ -algebra of $G$ is an AF-algebra? Also, are there any known examples of non-compact locally compact groups whose group $C^*$ -algebra is an AF-algebra, and the group $C^*$ -algebra has been computed? Examples where the group is non-discrete would be of most interest.","['c-star-algebras', 'operator-algebras', 'functional-analysis', 'group-theory', 'locally-compact-groups']"
4199700,Show $\nu(B)=\mu(A\cap B)$ is a measure,"First step is to show that $A\cap B\in\mathcal A$ , which is not obvious from the definition of $\sigma$ -field that I have: (1) $\mathcal X, \emptyset\in \mathcal A$ , (2) $A\in \mathcal A\Rightarrow A^c\in\mathcal A$ , (3) $A_1,A_2,\dots\in\mathcal A;A_i\cap A_j=\emptyset \forall i\ne j\Rightarrow \bigcup _{n=1}^\infty A_n\in\mathcal A$ . For this I drew a venn diagram but it ultimately boils down to these steps: Because $\mathcal A$ is closed under complement and union, $(A^c\cup B)^c=A\cap B^c \in\mathcal A$ . Then $(A\cap B^c)\cup A^c=\mathcal X\cap (B^c\cup A^c)=B^c\cup A^c\in\mathcal A$ , so $(B^c\cup A^c)^c=A\cap B\in\mathcal A$ . Since $A\cap B\in\mathcal A$ and $\mu$ is a measure, $\nu(B)=\mu(A\cap B)\in[0,\infty]$ for all $B$ . This is the first condition for measure. Second condition is measure of infinite union of disjoint sets becomes infinite sum of measures, i.e. $$\begin{split}\nu\left(\bigcup_{n=1}^\infty B_n\right)&=\mu\left(A\cap\bigcup_{n=1}^\infty B_n\right)\\
&=\mu\left(\bigcup_{n=1}^\infty (A\cap B_n)\right)\\
&=\sum_{n=1}^\infty\mu\left(A\cap B_n\right)&&\text{bc $B_n$ are disjoint, $A\cap B_n\in\mathcal A$, and $\mu$ is a measure}\\
&=\sum_{n=1}^\infty \nu(B_n)\end{split}$$ which holds. Is it right? Maybe there is a shorter way of showing $A\cap B\in\mathcal A$ .","['elementary-set-theory', 'measure-theory']"
4199714,Writing every element in a group from its presentation,"Problem If we have a group $G$ of order $8$ such that $G=\langle i,j,k | ij=k,jk=i,ki=j,i^2=j^2=k^2\rangle $ , and we denote $i^2=m$ , show that every element of G can be written in the form $e,i,j,k,m,mi,mj,mk$ . The problem suggests writing out the multiplication table during the course of the solution. Solution attempt with questions (I will edit with the multiplication table added once I convert it from $\LaTeX$ to markdown. First question: How does this group have order 8? I can only find 7 elements: $e,i,j,k,i^2,j^2,k^2$ . It doesn't make sense for the 8th element to be $m$ as $m$ is just a denotion for $i^2$ . Next question: I've shown all the trivial answers: Given by presentation: $ij=k,jk=i,ki=j, ii=m, jj=m, kk=m$ Extrapolated with $i^2=j^2=k^2=m$ : $i^2i=mi$ , $k^2i=mi$ , $j^2i=mi$ (I use similar logic to find terms like $i^2j$ and $i^2k$ ) Elements of form $x^3$ : $i^2i=(ii)i=mi, j^2j=mj, k^2k=mk$ Where I'm stuck at is determining what the elements like $i^2i^2$ are. For example, $i^2i^2=iiii$ . There isn't a part of the presentation that helps reduce $i^2$ to a usable form. If I'm looking at this completely wrong, I'd appreciate some advice or a hint towards the solution.","['group-presentation', 'group-theory']"
4199720,All conjugacy classes of $\operatorname{SL}_2 \mathbb{F}_p$,"For an odd prime $p$ , is the number of the conjugacy classes of $\operatorname{SL}_2 \mathbb{F}_p$ p+4 ? I showed a partial result: Let $A$ be a matrix.
Consider its characteristic polynomial $p$ .
Then over $\mathbb{F}_{p^2}$ , $p$ has roots.
Write it $\xi, \zeta$ .
Then since the determinant is $1$ , we have that $\zeta = \xi ^{-1}$ .
And since the trace is in $\mathbb{F}_p$ , we have that $\xi + \xi ^{-1} \in \mathbb{F}_p$ .
Thus $(\xi^{p-1} - 1)(\xi ^{p+1} - 1)=0$ The case that $\xi = \pm 1$ .
In this case we have that $A = \pm \left( \begin{matrix}1 & * \\ 0 & 1 \end{matrix} \right)$ .
(6 matrices are in this case.) The case that $\xi \not= \pm 1$ and $\xi^{p-1} = 1$ .
In this case $\xi \in \mathbb{F}_p$ .
So $A$ is conjugate to $\left( \begin{matrix}\xi & * \\ 0 & \xi \end{matrix} \right)$ ...?
( $(p-3) / 2$ matrices are in this case?) The case that $\xi \not= \pm 1$ and $\xi^{p+1} = 1$ .
...? From them how can I show the result?","['matrices', 'abstract-algebra', 'linear-algebra']"
4199725,"What does it mean for points of the modular curve $X(N)$ to be ""defined over $\mathbb{F}_p$""?","I'm trying to study a collection of elliptic curves over some fixed finite field $\mathbb{F}_p$ . By browsing the literature and discussing with my supervisor, it seems like it will be fruitful to study some sort of a modular curve. I am familiar with the classic modular curves $X(N),X_0(N),X_1(N)$ , etc. through the first three chapters of Diamond and Shurman's A First Course In Modular Forms . I understand that these curves are Riemann surfaces and form a moduli space for isomorphism classes of elliptic curves over $\mathbb{C}$ . In this paper that I am reading, the authors speak of points of $X_1(N)$ that are defined over $\mathbb{F}_p$ and they use the notation $X_1(N)(\mathbb{F}_p)$ which makes it seem like they're probably using the language of schemes. I am aware of the books (or long papers): Les schemas de modules de courbes elliptiques by Deligne-Rapoport (1973) Arithmetic Moduli of Elliptic Curves by Katz-Mazur (1985) I know the basic definitions of schemes from Hartshorne's Algebraic Geometry but I do not know many results beyond their first properties. Both of these books address the topic of reduction mod $p$ at some point and I believe this is exactly what I'm looking for. However, as I'm just beginning my study on this topic, I'm finding it hard to digest the discussions, or to even locate the relevant discussions, in either book, especially the first one, which is written in French, and seems to be the canonical reference on this subject by my search online. Any of the following responses will be helpful to me: A rough sketch motivating what it means to have points of $X(N),X_0(N),X_1(N)$ defined over $\mathbb{F}_p$ . Reference to a specific page or section in Deligne-Rapoport or Katz-Mazur addressing my first point. Reference to a source outside of Deligne-Rapoport and Katz-Mazur addressing my first point. Ideally, the source should be in English.","['number-theory', 'algebraic-geometry', 'moduli-space', 'elliptic-curves']"
4199727,Existence of self-Laplace transforms,"There are many functions that are self-Fourier transforms, such as $e^{-\pi x^2}$ or $\frac{1}{\cosh(\pi x)}$ , and this property may be used to prove some interesting theorems such as the functional equation for the theta function or an integral relation like this . I am wondering if the same can be said of self-Laplace transforms. Are there any useful functions that are their own Laplace transform, and can this property be exploited to give any interesting consequences? Here is one example of such a function that may be constructed, but it seems artificial and of no significance: Suppose $f$ is a function of the form $f(t) = C_1t^{s-1} +C_2 t^{-s}$ , where $0<\text{Re}(s)<1$ so that the Laplace transform exists and $C_1$ and $C_2$ are some constants. We may now assume $f$ is its own Laplace transform and solve for $s$ and the constants: $$ C_1 x^{s-1} + C_2 x^{-s} =\mathcal{L}(f(t))=\int_0^{\infty} f(t) e^{-xt} \, dt = C_1\int_0^{\infty} t^{s-1} e^{-xt} \, dt + C_2 \int_0^{\infty} t^{-s} e^{-xt} \, dt $$ $$ = C_1 \Gamma(s) x^{-s} + C_2 \Gamma(1-s) x^{s-1} $$ We therefore need $C_1 =C_2 \Gamma(1-s)$ and $C_2 = C_1 \Gamma(s)$ , and so $ 1= \Gamma(s) \Gamma(1-s) = \pi \csc(\pi s)$ , which has the unique solution $s=\frac{1}{2} \pm \frac{i}{\pi} \log(\pi + \sqrt{\pi^2-1}) $ in the strip $0<\text{Re}(s)<1$ , and $\frac{C_2}{C_1}=\Gamma(s)$ . Choosing $C_1=1$ , our self-Laplace transform is $$f(t) = t^{s-1} + \Gamma(s) t^{-s}, \text{ where } s = \frac{1}{2} \pm \frac{i}{\pi} \log \left(\pi+\sqrt{\pi^2-1} \right) $$","['laplace-transform', 'real-analysis']"
4199769,proof of uniqueness of fourier series in Stein's book,"On page 40, the book tries to prove uniqueness of fourier series the following:
Suppose that $f$ is an integrable function on the circle with $\hat{f}(n)=0$ for all $n \in \mathbb{Z}$ . Then $f(\theta_{0})=0$ whenever f is continuous at the point $\theta_{0}$ . To prove this, assuming that $f$ satisfies the hypotheses, $\theta_{0}=0$ , and $f(0)>0$ . The idea is now to construct a family of trigonometric polynomials $\{p_{k}\}$ that peak at $0$ , resulting in $$\int p_{k}(\theta)f(\theta)\; d\theta \rightarrow \infty \;\; \text{as}\;\; k \rightarrow \infty.$$ And this will contradict the requirement that the coefficient needs to be 0. To construct the p, since $f$ is continuous at $0$ , we can choose $0 < \delta \leq \pi/2$ , so that $f(\theta)>f(0)/2$ whenever $|\theta|< \delta$ . Let $p(\theta) = \epsilon + \cos\theta$ where $\epsilon >0$ is chosen so small that $|p(\theta)|< 1-\epsilon/2$ , whenever $\delta \leq |\theta| \leq \pi$ . Then choose a positive $\eta$ with $\eta < \delta$ , so that $p(\theta) \geq 1 + \epsilon/2$ , for $|\theta| < \eta$ . Finally, we define $p_{k}(\theta) = [p(\theta)]^k$ . Then the key point will be when $|\theta| < \eta$ , $$\int p_{k}(\theta)f(\theta)\; d\theta \geq 2\eta \frac{f(0)}{2}(1+\frac{\epsilon}{2})^k \rightarrow \infty \; , k \rightarrow \infty$$ This contracts that the coefficient needs to be 0. So the proof finishes here. What I'm not understanding is when computing the fourier coefficient using normal formula, $$\hat{f}(n)=\int f(\theta)e^{-in\theta}\; d\theta.$$ If I sum over multiple coefficients, the formula will be $\int f(\theta)(e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+...)\; d\theta.$ it will not be in the form of $p_k(\theta) = (\cos\theta + \epsilon)^k $ .
Unless $p_k(\theta) = (\cos\theta + \epsilon)^k = e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+... $ . under some conditions?","['fourier-series', 'fourier-analysis', 'analysis']"
4199793,"$(x,y) = (u,v) \implies x = u \wedge y = v$ using Hausdorff's definition","I have been asked to solve this problem. $(x,y) = (u,v) \implies x = u \wedge y = v$ . Solve using this ordered pair definition: $\Delta \neq \square \Rightarrow(x, y)=\{\{x, \Delta\},\{y, \square\}\}$ . I have investigated a little bit and discovered this is Hausdorff's definition usually stated as: $$(a,b)=\{\{a,1\},\{b,2\}\}.$$ Theorem 1. $\{x,y\}=\{u,v\}\implies (x=u\wedge y=v)\vee (x=v\wedge y=u)$ . This is my try: Considering the definition, $$(x,y)=(u,v)\implies \{\{x,\Delta\},\{y,\square\}\} =\{\{u,\Delta\},\{v,\square\}\}.$$ By theorem 1, we know $$\underbrace{\left[\left(\{x,\Delta\}=\{u,\Delta\}\right)\wedge \left(\{y,\square\}=\{v,\square\}\right)\right]}_{(1)}\vee \underbrace{\left[\left(\{x,\Delta\}=\{v,\square\}\right)\wedge\left( \{y,\square\}=\{u,\Delta\}\right)\right]}_{(2)}.$$ Case (1): $\left(\{x,\Delta\}=\{u,\Delta\}\right)\wedge \left(\{y,\square\}=\{v,\square\}\right)$ . Applying theorem 1 again, we have: $$\left[(x=u\wedge \Delta=\Delta)\vee (x=\Delta \wedge \Delta=u)\right]\wedge\left[(y=v\wedge \square=\square)\vee (y=\square\wedge \square=v)\right].$$ Case (2): $\left(\{x,\Delta\}=\{v,\square\}\right)\wedge\left( \{y,\square\}=\{u,\Delta\}\right)$ . Applying theorem 1 again, we have: $$\left[(x=v\wedge \Delta=\square)\vee (x=\square \wedge \Delta=v)\right]\wedge\left[(y=u\wedge \square=\Delta)\vee (y=\Delta\wedge \square=u)\right].$$ I am not sure how to finish this proof. It does not make sense at all, what do you think?","['elementary-set-theory', 'logic', 'set-theory']"
4199796,"Proving Diamond & Shurman Exercise 3.6.4, the dimension formula for cusp forms of weight 1.","Exercise 3.6.4 of A First Course in Modular Forms asks readers to prove the final part of Theorem 3.6.1, which provides formulas for the dimension of modular forms and cusp forms of weight $1$ , $\mathcal{M}_1(\Gamma)$ and $\mathcal{S}_1(\Gamma)$ . Here $\Gamma$ is a congruence subgroup of $\mathrm{SL}_2(\mathbb{Z})$ such that the negative identity matrix $-I \notin \Gamma$ . (Some parts of) Theorem 3.6.1. ...let $g$ the genus of $X(\Gamma)$ , $\varepsilon_3$ the number of elliptic points with period $3$ , $\varepsilon^{\mathrm{reg}}_{\infty}$ the number of regular cusps, and $\varepsilon^{\mathrm{irr}}_{\infty}$ the number of irregular cusps. If $\varepsilon^{\mathrm{reg}}_{\infty} > 2g-2$ then $\dim(\mathcal{M}_1(\Gamma)) = \frac{\varepsilon^{\mathrm{reg}}_{\infty}}{2}$ and $\dim(\mathcal{S}_1(\Gamma)) = 0$ . If $\varepsilon^{\mathrm{reg}}_{\infty} \leq 2g-2$ then $\dim(\mathcal{M}_1(\Gamma)) \geq \frac{\varepsilon^{\mathrm{reg}}_{\infty}}{2}$ and $\dim(\mathcal{S}_1(\Gamma)) = \dim(\mathcal{M}_1(\Gamma)) - \frac{\varepsilon^{\mathrm{reg}}_{\infty}}{2}$ . The former parts can be proved using Riemann-Roch Theorem. Let $f \in \mathcal{A}_1(\Gamma)$ and $\omega \in \Omega^{\otimes k}(X(\Gamma))$ be the differential of $X(\Gamma)$ (the compactified modular curve) pulling back to $f(\tau)^2 (d \tau)^k$ on $\mathcal{H}$ . Let $g$ be the genus of $X(\Gamma)$ , $\{x_{3, i}\}$ be the period $3$ elliptic points, $\{x_i\}$ the regular cusps, and $\{x'_i\}$ the irregular cusps. Then the divisors \begin{align*}
D_1 := \lfloor f \rfloor = \frac{1}{2} \mathrm{div}(\omega) + \sum\limits_i \frac{1}{2} x_i \\
D_2 := \left\lfloor f - \sum\limits_{i} x_i - \sum\limits_{i} \frac{1}{2} x'_i \right\rfloor = \frac{1}{2} \mathrm{div}(\omega) - \sum\limits_i \frac{1}{2} x_i
\end{align*} have linear spaces isomorphic to $\mathcal{M}_1(\Gamma)$ and $\mathcal{S}_1(\Gamma)$ , respectively. The degree of the divisors are \begin{align*}
\deg(D_1) = g-1+\frac{1}{2} \varepsilon^{\mathrm{reg}}_{\infty}, \quad \deg(D_2) = g-1-\frac{1}{2} \varepsilon^{\mathrm{reg}}_{\infty}.
\end{align*} The book describes Riemann-Roch Theorem and a corollary as follows: Theorem 3.4.1 (Riemann-Roch). Let $X$ be a compact Riemann surface of genus $g$ . Let $\mathrm{div}(\lambda)$ be a canonical divisor on $X$ . Then for any divisor $D$ on $X$ , \begin{align*}
l(D) = \deg(D) - g + 1 + l(\mathrm{div}(\lambda) - D).
\end{align*} Corollary 3.4.2 Let $X, g, \mathrm{div}(\lambda)$ , and $D$ be as above. Then $l(\mathrm{div}(\lambda)) = g$ . $\deg(\mathrm{div}(\lambda)) = 2g-2$ . If $\deg(D) < 0$ then $l(D) = 0$ . If $\deg(D) > 2g-2$ then $l(D) = \deg(D) - g + 1$ . If $\varepsilon^{\mathrm{reg}}_{\infty} > 2g-2$ , then the results follow from (c) and (d) of the corollary. If $\varepsilon^{\mathrm{reg}}_{\infty} \leq 2g-2$ , then by Riemann-Roch \begin{align*}
l(D_1) = \frac{1}{2}\varepsilon^{\mathrm{reg}}_{\infty} + l(\mathrm{div}(\lambda) - D_1) \geq \frac{1}{2}\varepsilon^{\mathrm{reg}}_{\infty}.
\end{align*} The problem is that I don't know how to show $\dim(\mathcal{S}_1(\Gamma)) = \dim(\mathcal{M}_1(\Gamma)) - \frac{\varepsilon^{\mathrm{reg}}_{\infty}}{2}$ . It seems that \begin{align*}
l(\mathrm{div}(\lambda) - D_2) = l(D_1)
\end{align*} but I have no idea how to explain this.","['algebraic-number-theory', 'riemann-surfaces', 'number-theory', 'complex-analysis', 'modular-forms']"
4199837,Find a matrix for a unitary transform between matrices or prove that there is none,"I have hermitian matrices $A,\,B$ and would like to find a unitary matrix $U$ such that $$UAU^\dagger=B$$ or show that there is no such matrix. Example: For $$
A=\begin{pmatrix}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0
\end{pmatrix},\,
B=\begin{pmatrix}
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{pmatrix}
$$ we find $$
U=\begin{pmatrix}
1 & 0 & 0 & 0\\
 0 & 0 & -1 & 0 \\
0 & -1 & 0 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
$$ by guesswork. A constructive method would be useful. Next example, where I assume that no such $U$ exists, but do not know how to show it: $$
A=\begin{pmatrix}
b & d & c & a\\
d & - b & a & - c\\
c & a & - b & - d\\
a & - c & - d & b
\end{pmatrix}
=a\cdot\sigma_x\otimes\tau_x + b\cdot\sigma_z\otimes\tau_z
+ c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x\\
B=\begin{pmatrix}
- b & d & - c & - a\\
d & b & - a & c\\
- c & - a & b & - d
\\- a & c & - d & - b\end{pmatrix}
=-a\cdot\sigma_x\otimes\tau_x - b\cdot\sigma_z\otimes\tau_z
- c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x
$$ where $\sigma_x,\,\sigma_z,\,\tau_x,\,\tau_z$ denote the Pauli matrices $$
\sigma_0 = \tau_0 =
\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix},\,
\sigma_x = \tau_x =
\begin{pmatrix}
0 & 1\\
1 & 0
\end{pmatrix},\,
\sigma_y = \tau_y =
\begin{pmatrix}
0 & -i\\
i & 0
\end{pmatrix},\,
\sigma_z = \tau_z =
\begin{pmatrix}
1 & 0\\
0 & -1
\end{pmatrix}
$$ Note: I am mostly interested in $4\times4$ matrices, but $8\times8$ would be nice, too. EDIT : Thanks to the answer of Kurt G. . Assume $a,\,b,\,c,\,d\in\mathbb{R}\backslash\{0\}$ . I was aware that a solution for $d=0$ exists. I specifically included $d$ to avoid a simple solution of the form $\sigma_a\otimes\tau_b$ . When $d=0$ , then a solution exists independently of the other values, for example $U=\sigma_0\otimes\tau_y$","['matrices', 'hermitian-matrices', 'linear-algebra']"
4199841,Measurability issues in the symmetrization step in the proof of $\varepsilon$-sample theorem,"Let $(\mathcal{X},d)$ be a metric space and $\mu$ be a Borel probability measure on $(\mathcal{X},d)$ . Let $m \in \mathbb{N}$ and define the two probability product measures $\mu^{m} := \otimes_{k=1}^{m}\mu$ and $\mu^{2m} := \otimes_{k=1}^{2m}\mu$ on the Borel subsets of the product spaces $\mathcal{X}^{m}$ and $\mathcal{X}^{2m}$ , respectively. Let $\mathcal{C}$ be an arbitrary family of Borel subsets of $(\mathcal{X},d)$ (in general not denumerable, but that we may assume to have finite Vapnik-Chervonenkis dimension) and $\varepsilon>0$ .
Define $$Q:=\Big\{(x_1,\dots,x_m) \in \mathcal{X}^{m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \Big\}.$$ The $\varepsilon$ -sample theorem proof has as a first step a symmetrization trick.
I have a question about the rigorousness of that part of the proof.
It is based on the following step that left me baffled: \begin{align*}
\mu^{2m}\bigg[ \Big\{(x_1,\dots,x_m,x_1',\dots,x'_m) \in \mathcal{X}^{2m}\ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}\Big\} \bigg]\\
=\int_{Q} \mu^m \bigg[ \Big\{ (x_1',\dots,x_m') \in \mathcal{X}^m \ \Big| \ \exists C \in \mathcal{C}, \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k) \Big| \ge \varepsilon \land \Big| \mu[C] - \frac{1}{m} \sum_{k=1}^m \chi_C(x_k') \Big| \le \frac{\varepsilon}{2}   \Big\}\bigg]\operatorname{d}\mu^m(x_1,\dots,x_m)
\end{align*} Here, it is clear that the author of the proof wanted to use disintegration via independence. However, the involved sets are not measurable in general: it is not assumed that the family $\mathcal{C}$ is denumerable, so we have no guarantee about the measurability of every set defined through a condition of the form "" $\exists C \in \mathcal{C}$ "". Then, I thought that the above equation could be interpreted in terms of the corresponding (uniquely defined) outer measures. But, at that point, I'm not even sure that what it is written actually makes sense (we are integrating on a set $Q$ that, in general, is not measurable, and it isn't clear that the integrand function is measurable at all), nevermind if disintegration could be performed anymore. Is it possible to make the previous step rigorous without assuming the measurability of the involved sets? That line of the proof, with just slightly different notations, could be found in e.g., Anthony, Bartlett - Neural Network Learning, Theoretical Foundations, Lemma 4.4, pag. 46 and 47, eq. (4.3).","['measure-theory', 'statistics', 'proof-explanation', 'machine-learning', 'probability-theory']"
4199847,Finding the formula for the ulam spiral starting with $0$ as a bijective function $U:\mathbb{N}\rightarrow\mathbb{Z×Z}$,"$\underline{\text{Introduction}:-}$ For the last few days I've been wondering about the question below. I don't think  that my approach is an elegant approach but this is the best I can do. (I am a high school student. But I have managed to gather a lot of information about advanced maths from the internet such as calculus, linear algebra, Set theory, number theory, coordinate geometry, modular arithmetic etc. So if the answer needs advanced maths then I don't have any problem, but not too advanced maths.) Note: in this question I will include $0$ as a natural number, I know that this is rather controversial in the mathematical community but I will request not to debate about it the comment section. ( $\mathbb{N}=\{0,1,2,\cdots\}$ ) $\underline{\text{My Question}:-}$ If we put the natural numbers on the Ulam Spiral this way (Sorry I couldn't find a picture where the spiral starts at $0$ , so I just put the wikipedia picture here. Just replace $n$ with $n-1$ ) Then what is the formula for the cartesian coordinate on the integer lattice where each natural number lies? $\underline{\text{My Attempt}:-}$ Define a bijective function $U:\mathbb{N}\rightarrow\mathbb{Z×Z}$ Where $U(n)$ gives the cartesian coordinates of $n$ on the ulam spiral. For example: $U(0)=(0,0)$ $U(1)=(0,1)$ $U(2)=(1,1)$ $U(3)=(1,0)$ $U(4)=(1,-1)$ $\vdots$ Let, $U(n)=(x_n,y_n)$ It's obvious that how $x_n$ and $x_{n+1}$ or $y_n$ and $y_{n+1}$ would compare to each other would depend on which part of the ulam spiral $n$ is in. So, I divided the spiral into segments. I will denote them as $L_1$ and $L_2$ such that $|L_1|=L$ and $|L_2|=L+1$ for $L\in\mathbb{N}$ and define $0_2=\{0\}$ . For example- $0_1=\{\}=\varnothing$ $(\text{We won't consider this ever again, since it is just the null set})$ $0_2=\{0\}$ $1_1=\{1\}$ $1_2=\{2,3\}$ $2_1=\{4,5\}$ $2_2=\{6,7,8\}$ $\vdots$ So- (Please forgive me. I cannot create pictures, so I just drew it. But I hope you understand that what $L_1$ and $L_2$ are) Here I tried to show how each segment looks like on the spiral. A bit of investigations gives us the result- $\begin{align}&L_1=\{L^2,\cdots,L^2+L-1\}\\&L_2=\{(L+1)^2-(L+1),\cdots,(L+1)^2-1\}\end{align}$ For $L\in\mathbb{N}$ It's obvious that- ●If $n\in L_1$ where $L$ is odd, then $x_{n+1}=x_n$ and $y_{n+1}=y_n+1$ ●If $n\in L_2$ where $L$ is odd, then $x_{n+1}=x_n-1$ and $y_{n+1}=y_n$ ●If $n\in L_1$ where $L$ is even, then $x_{n+1}=x_n$ and $y_{n+1}=y_n-1$ ●If $n\in L_2$ where $L$ is even, then $x_{n+1}=x_n+1$ and $y_{n+1}=y_n$ So, $$U(n+1)=\begin{cases}(x_n,y_n+1),&\text{if $n\in L_1$ and $L$ is odd}\\(x_n,y_n-1),&\text{if $n\in L_1$ and $L$ is even}\\(x_n-1,y_n),&\text{if $n\in L_2$ and $L$ is odd}\\(x_n+1,y_n),&\text{if $n\in L_2$ and $L$ is even}\end{cases}$$ If we say that, $(a,b)+(x,y)=(a+x,b+y)$ Then, $$U(n+1)=\begin{cases}(x_n,y_n)+(0,1),&\text{if $n\in L_1$ and $L$ is odd}\\(x_n,y_n)-(0,1),&\text{if $n\in L_1$ and $L$ is even}\\(x_n,y_n)-(1,0),&\text{if $n\in L_2$ and $L$ is odd}\\(x_n,y_n)+(1,0),&\text{if $n\in L_2$ and $L$ is even}\end{cases}$$ And since, $\begin{align}&(0,1)=U(1)\\&(1,0)=U(3)\\&(x_n,y_n)=U(n)\end{align}$ We have, $$U(n+1)=\begin{cases}U(n)+U(1),&\text{if $n\in L_1$ and $L$ is odd}\\U(n)-U(1),&\text{if $n\in L_1$ and $L$ is even}\\U(n)-U(3),&\text{if $n\in L_2$ and $L$ is odd}\\U(n)+U(3),&\text{if $n\in L_2$ and $L$ is even}\end{cases}$$ This is the worst recurrence relation I have ever seen. Update: To get the formula for $L$ and whether $n\in L_1$ or $n\in L_2$ , we need to solve for $L$ and $k$ in- $\begin{cases}L^2+k=n\\1<L<n\\-L\leq k\leq L-1\end{cases}$ With a little different definition for $L_1$ and $L_2$ The definition I used here will require solving for $L$ and $k$ in $\begin{cases}L^2+k=n\\0\leq L<n\\0\leq k\leq 2L\end{cases}$ (for more details, read this question of mine) The solutions would be: $$\begin{align}&L=\left\lfloor\sqrt{n}\right\rfloor\\&k=n-\left\lfloor\sqrt{n}\right\rfloor^2\end{align}$$ (For more details, read this answer to the question I linked above) It's clear that, If $0\leq k\leq L-1$ then $n\in L_1$ If $L\leq k\leq 2L$ then $n\in L_2$ So we can write our recurrence formula as- $$U(n+1)=\begin{cases}U(n)+U(1)&\text{if $0\leq k\leq L-1$ and $L$ is odd}\\U(n)-U(1)&\text{if $0\leq k\leq L-1$ and $L$ is even}\\U(n)-U(3)&\text{if $L\leq k\leq 2L$ and $L$ is odd}\\U(n)+U(3)&\text{if $L\leq k\leq 2L$ and $L$ is even}\end{cases}$$ $$\begin{align}U(n+1)=&\begin{cases}U(n)+U(1),&\text{if $0\leq n-\lfloor\sqrt{n}\rfloor^2\leq\lfloor\sqrt{n}\rfloor-1$ and $\lfloor\sqrt{n}\rfloor$ is odd}\\U(n)-U(1),&\text{if $0\leq n-\lfloor\sqrt{n}\rfloor^2\leq\lfloor\sqrt{n}\rfloor-1$ and $\lfloor\sqrt{n}\rfloor$ is even}\\U(n)-U(3),&\text{if $\lfloor\sqrt{n}\rfloor\leq n-\lfloor\sqrt{n}\rfloor^2\leq 2\lfloor\sqrt{n}\rfloor$ and $\lfloor\sqrt{n}\rfloor$ is odd}\\U(n)+U(3),&\text{if $\lfloor\sqrt{n}\rfloor\leq n-\lfloor\sqrt{n}\rfloor^2\leq 2\lfloor\sqrt{n}\rfloor$ and $\lfloor\sqrt{n}\rfloor$ is even}\end{cases}\end{align}$$ It made it worse, but at least now we have the recurrence relation only in terms of $n$ . $\underline{\text{Some more questions}:-}$ Besides my original question in the $\text{'My Question'}$ part, I have a few more questions. $1)$ How can I get further from my recurrence relation to derive an explicit formula? $2)$ If deriving a recurrence is really not the way to approach this problem then I would request some answer to question with a different approach.","['algebra-precalculus', 'recurrence-relations', 'recreational-mathematics']"
4199897,"Find an explicit quasi-smooth embedding $X_{38} \subset \mathbb P(5, 6, 8, 19)$","Consider the weighted projective space $\mathbb P(5,6,8,19)$ with weighted homogeneous coordinates $x,y,z,w$ , in this order. I want to construct an explicit quasi-smooth embedding of the weighted K3 surface $X_{38} \subset \mathbb P(5,6,8,19)$ , in order to find and classify its singularities. (The strategy that I am using is defined in Iano-Fletcher's “Working with weighted complete intersections”, and it relies on the following fact. Let $p$ be an arbitrary point on an hypersurface $X$ of a well-formed weighted projective space $\mathbb P$ . If $\mathbb P$ near $p$ is locally isomorphic to $\mathbb C^{n+1} / G$ near the origin, then $X$ near $p$ is locally isomorphic to $\mathbb C^n / G$ near the origin. In particular, the singular points of $X$ are precisely the singular points of $\mathbb P$ that happen to be in $X$ .) I came up with the following polynomial: $$f(x,y,z,w) = zx^6 - zy^5 + yz^4 + w^2 + \lambda x^4 y^3$$ where $\lambda \in \mathbb C^\star$ is a nonzero constant to be determined. Notice that, if we set $\lambda = 0$ , then $(1,1,0,0) \in \mathbb C^4$ is a nonzero critical point of $f$ , hence the embedding is not quasi-smooth. However, if we could ignore this issue, then the singularities of $X_{38}$ would be easy to find and classify: $[0:1:1:0]$ is a singularity of type $\frac 12 (1,1)$ , hence an $A_1$ singularity. $[1:0:0:0]$ is a singularity of type $\frac 15 (1,-1)$ , hence an $A_4$ singularity. $[0:1:0:0]$ is a singularity of type $\frac 16 (-1,1)$ , hence an $A_5$ singularity. $[0:0:1:0]$ is a singularity of type $\frac 18 (-3,3)$ , hence an $A_7$ singularity. From now onwards, we will call these four points the special points . Notice that $x^4 y^3$ and its first partial derivatives vanish at the special points. Therefore, varying $\lambda$ will change neither the identified singularities nor their classification. Moreover, it is easy to check that, for $\lambda \in \mathbb C^\star$ , we obtain a hypersurface that is quasi-smooth at the special points. However, at least in principle, we might be introducing other critical points elsewhere . Therefore, my question is: How can I prove that, for some $\lambda \in \mathbb C^\star$ , the weighted projective hypersurface defined by $f$ is quasi-smooth? I am aware of Bertini's theorem for linear systems in ordinary projective space, but I do not know of any analogue that works in weighted projective space. EDIT 1: I just thought of a possible solution. Consider the ordinary (not weighted!) projective space $\mathbb P^4$ with homogeneous coordinates $x,y,z,w,t$ . Then $\mathbb C^4$ is just the open affine $t = 1$ of $\mathbb P^4$ . Homogenizing $f$ , we obtain $$f^\#(x,y,z,w,t) = zx^6 - tzy^5 + t^2 yz^4 + t^5 w^2 + \lambda x^4 y^3$$ This defines a linear system of divisors of $\mathbb P^4$ parametrized by $\lambda \in \mathbb P^1$ . By Bertini's theorem, for almost all $\lambda \in \mathbb C^\star$ , we obtain a hypersurface of $\mathbb P^4$ that is smooth away from the system's base locus, which, if I am not mistaken , is the closure in $\mathbb P^4$ of the curves in $\mathbb C^4$ that project to the special points. Thus, by adding a suitable multiple of the correction term $x^4 y^3$ , we have obtained a surface $X_{38} \subset \mathbb P(5,6,8,19)$ that is quasi-smooth away from the special points. Since we already knew that $X_{38}$ would be quasi-smooth at the special points as well, $X_{38}$ is globally quasi-smooth. QUESTION: Does this work? EDIT 2: Well, I was mistaken. A line is determined by two points. Similarly, the base locus of this linear system is determined by the hypersurfaces obtained for two distinct values of $\lambda$ . But two hypersurfaces in $\mathbb P^4$ cannot possibly intersect in just a union of curves. To make this strategy workable, I need a larger linear system, determined by linear combinations of three surfaces. But at this point I am running out of imagination... EDIT 3: Actually, I do not need a larger linear system. What I need is a larger brain to come up with the right ideas faster. The base locus of the linear system given above is the intersection of $X_{38}$ with the union of the planes $x = 0$ and $y = 0$ . On the base locus , the partial derivatives of $x^4 y^3$ are identically zero, hence the the partial derivatives of $f$ do not depend on $\lambda$ . Thus, for any point $P$ of the base locus, the following two propositions are equivalent: $P$ is a quasi-smooth point of $X_{38}$ when $\lambda = 0$ . $P$ is a quasi-smooth point of $X_{38}$ for any $\lambda \in \mathbb C$ . Since the problematic point $[1:1:0:0]$ is not in the base locus, we are done. QUESTION: Does this work?","['projective-space', 'complex-geometry', 'k3-surfaces', 'algebraic-geometry', 'orbifolds']"
4199962,The Sequences $\{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty$ are Linearly Independent over $\mathbb{C}$,"I am studying the book ""Galois' Dream: Group Theory and Differential Equations"" by Michio Kuga. I have some trouble in proving the Lemma 18.3: Let $\alpha_1, \ldots, \alpha_n$ be complex numbers such that $\alpha_t - \alpha_s$ is not an integer for $t \neq s$ . Then for every positive integer $n$ , the following $(N+1)n$ sequences are linearly independent over $\mathbb{C}$ : $$
\{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty,
$$ where $j = 1, \ldots, n$ and $k = 0, \ldots, N$ . In other words, if the equation $$
\sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^ke^{2\pi i \alpha_j m} = 0
$$ ( $m = 1,2,\ldots$ ) holds for complex numbers $C_{j,k}$ , then $C_{j,k} = 0$ for all $j = 1, \ldots, n$ and $k = 0, \ldots, N$ . Here is my process of thought: First, to have a simpler notation, let $\beta_j := e^{2\pi i \alpha_j} \neq 0 $ . Then, consider the linear dependence relation: $$
\sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^k \beta_j^m = 0, \hspace{10pt}  m = 1, 2, \ldots
$$ But I couldn't figure out how should I proceed next. I tried to do induction on one of the indicies, eg. on $j$ as:
For $j = 1$ the relation becomes $$
\sum_{k = 0} ^N C_{0,k}m^k \beta_0^m = 0
$$ But even here, I couldn't conclude that all $C_{0,k}$ s are zero. Since this $C$ has two indicies, I am not sure if finding all $C_{0,k}$ s zero would imply that all $C_{j,k}$ s are zero. So I am stuck. The book says this proof was easy, but I couldn't catch that ""easy"". I will appreciate any help. Thank you in advance.","['singularity', 'linear-algebra', 'ordinary-differential-equations', 'sequences-and-series']"
4200037,Log Sum Exp trick confusion with probability distributions,"I am struggling to understand how exactly the log-sum-exp trick is applied in mixture models (say Gaussian mixture models) and the EM algorithm. It is well known that parameter estimation for a mixture: $$
f(x)= \sum_{i=1}^K \alpha_i \mathcal{N}(\mu_i,\Sigma_i) = \sum_{i=1}^K \alpha_ie^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)}
$$ can be achieved with the EM algorithm.
The E-step corresponds in estimating in which distribution $i$ a given $x$ comes from. This probability is given by: $$
p(x\in \mathcal{N(\mu_i,\Sigma_i)}) = \arg\max_i \frac{\alpha_i e^{-(x-\mu_i)^T\,\, \Sigma_i^{-1} \,\, (x-\mu_i)} }{\sum_{j=1}^{K}\alpha_k e^{-(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)}} \quad (1)
$$ At this point it is common to take the logarithm: $$
(1) \equiv \arg \max_i \Big\{ \log a_i - \log[(x-\mu_j)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)] - 1  + \log \sum_{i=1}^N [(x-\mu_i)^T\,\, \Sigma_j^{-1} \,\, (x-\mu_j)]  \Big\}
$$ where I used the fact that $\log \sum_{i=1}^{N}a_k=1$ . I don't understand how the log sum trick is used now in order to conclude the computation. My understanding is that arg min will be replaced by min or similar. At this point I cannot figure it out.","['statistics', 'probability-distributions', 'gaussian', 'bayesian']"
4200066,Is $\int_{\mathbb{R}^2} e^{-u} \Delta u < \infty$?,"Question. Let $u : \mathbb{R}^2 \to \mathbb{R}$ be a smooth function such that $$
u(x, y) > 0 \quad\text{and}\quad \Delta u(x, y) > 0
$$ for all $(x, y) \in \mathbb{R}^2$ , where $\Delta := \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}$ is the Laplacian. Is the integral $$
\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-u(x,y)} \Delta u(x, y) \; dx\, dy
$$ finite? I tried to no avail to find a counterexample where $u$ is rotationally symmetric, i.e. $u(x, y) = f(x^2 + y^2)$ for some $f$ . The problem reduces to the following: Subquestion. Let $f : [0, \infty) \to \mathbb{R}$ be a smooth function such that $f(t) > 0$ and $t f''(t) + f'(t) > 0$ for all $t \in [0, \infty)$ .
Is $$
\int_0^\infty e^{-f(t)} (t f''(t) + f'(t)) dt
$$ finite? But it is still not clear to me whether or not this initegral will be finite under the given conditions. I tried many different $f$ , but the integral always converge.","['integration', 'real-analysis']"
4200089,Find bound for polynomial with binomial coefficients,"I need to find a good, computable upper bound for the expression $$\sum_{k=0}^m x^k \binom{n+k}{k}$$ as a function of $x$ , $n$ and $m$ , and where $0<x< 1/2$ is real, and $0<n\leq m$ are integers. I would love to hear some ideas. Remark A. I know that $\sum_{k=0}^\infty x^k \binom{n+k}{k}=(1-x)^{-n-1}$ . So my question is how to bound the first $m$ terms of this infinite sum. Remark B. What I'm actually trying to do is bound the sum $\sum_{k=1}^m x^k C_{m-n+1}(n,k)$ , where $C_{d}(n,k)$ is a Catalan trapezoid number of order $d$ . I am then using the bound $C_d(n,k)\leq \binom{n+k}{k}$ , which is tight for low values of $k$ but loose for high values. If someone knows other bounds for Catalan trapezoid numbers, that could be useful here, let me know. For instance, it could be useful to have results about generating functions related to Catalan trapezoid numbers.","['catalan-numbers', 'binomial-coefficients', 'combinatorics', 'upper-lower-bounds', 'generating-functions']"
4200106,Show $\mathcal A\cap \mathcal B$ is a $\sigma$-field on $\mathcal X$,"The conditions are: $\mathcal X, \emptyset\in\mathcal {A\cap B}$ because they are in each of the sets, so they are in both of them $C\in\mathcal A\cap B\Rightarrow C\in \mathcal A$ and $C\in\mathcal B\Rightarrow C^c\in\mathcal A$ and $C^c\in\mathcal B\Rightarrow C^c\in\mathcal {A\cap B}$ $C_1,C_2,\dots \in\mathcal {A\cap B}\Rightarrow C_1,C_2,\dots \in\mathcal A$ and $C_1,C_2,\dots \in\mathcal B\Rightarrow\bigcup_{n=1}^\infty C_n\in\mathcal A$ and $\bigcup_{n=1}^\infty C_n\in\mathcal B\Rightarrow \bigcup_{n=1}^\infty C_n\in\mathcal {A\cap B}$ Is it right? And is writing out the middle two steps in 2. and 3. necessary? I'm guessing they are necessary but if you're experienced enough you might not need them.","['elementary-set-theory', 'measure-theory', 'solution-verification']"
4200108,Prove a multivariable limit of a quotient using the epison-delta definition,"I have to prove that $\displaystyle\lim_{(x,y)\to(1,0)} \frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}=0$ using the epislon-delta definition of a limit. Here's what I figured: $\delta>|x-1| \Rightarrow \delta>(x-1)^2\Rightarrow \delta^3>|(x-1)^3|$ . Also, $\delta>|y|\geq|cos{y}| \Rightarrow \delta^2>y^2$ . So $\delta^4>|(x-1)^3\cos{y}|$ and $2\delta^2>|(x-1)^2+y^2|$ . Obviously, $\delta^4>|\frac{(x-1)^3\cos{y}}{(x-1)^2+y^2}|$ if $(x-1)^2+y^2\geq1.$ But how do I account for it otherwise? I can't think of anything. I'm having issues proving the limits of quotients in general. Thank you for your help!","['multivariable-calculus', 'limits', 'calculus', 'epsilon-delta']"
4200204,"Evaluating $\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$","Other than being generally interested in this integral, it also appears in my $\zeta$ approach to my question here. Is it possible to evaluate the following integral? I was thinking to perhaps use an infinite product approach for $\sec$ , however, I couldn’t really get far with it. $$\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$$ If this integral is possible, is it also possible to evaluate the following two other integrals? $$\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right)^2 \ln (\sin (\pi x)) \, dx$$ $$\int_{0}^{1} x \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$$","['integration', 'definite-integrals', 'analysis', 'calculus', 'trigonometric-integrals']"
4200209,At the centroid does the gradient becomes zero?,"Suppose we have a smooth differentiable planar curve given as $F(x,y)=0$ . Suppose that the centroid of the curve $F=0$ is $F(x_0,y_0)$ . My question is:- $$\nabla F(x_0,y_0)=0$$ That is, does the gradient of $F$ becomes zero at $(x_0,y_0)$ ? Now some examples supporting this statement are circles and ellipses . I would be grateful if anyone proves the statement or gives a counter example .
Also I welcome anyone who would like to generalise this statement in any direction. Thanks!","['euclidean-geometry', 'analytic-geometry', 'differential-geometry']"
4200217,$|f(f(z))-z^2|$ must be large somewhere in the disc $\mathbb{D}$?,"I wish to prove the statement shown in the following block. I thought this may have appeared in the Math Stack before; sorry if I failed to find it (see ""Research"" below). The proposition seems to call for a proof via H.A. Schwarz Lemma, but I am interested in any proof of it. Let $\sup$ always mean the supremum in the unit open disc $\mathbb{D}$ . Let $f$ map $\mathbb{D}$ into $\mathbb{D}$ analytically, and fix the origin. Prove that $$\sup \left|f(f(z))-z^2\right| \,\,\geq\,\,\frac{1}{4}.$$ My Attempt. If it happens that $f$ is a rotation, $z \mapsto {e^{i\psi}}z,$ then the result follows from the choice $z=1/2 \in $ the disc: $$\left|f(f(\frac{1}{2}))-(\frac{1}{2})^2\right| \,\,\geq\,\,\left|f(f(\frac{1}{2}))\right|-\left|\frac{1}{4}\right|\,\,=\,\,\left|f(\frac{1}{2})\right|-\frac{1}{4}\,\,=\,\,\left|\frac{1}{2}\right|-\frac{1}{4}\,\,=\,\,\frac{1}{4},$$ since rotation means $|z|\,=\,|f(z)|$ for any $z$ in $\mathbb{D}$ . Therefore we assume $f$ is not a rotation. By Schwarz Lemma, we know $f'(0) \in \mathbb{D}$ , and we know $|f(z)|<|z|<1$ throughout the disc. ( Starting here I pursue an idea; I am not sure if it is helpful... ) Define the function $$\phi(z)\,\,=\,\,\frac{f(f(z))-z^2}{2},$$ and note that it also satisfies the hypotheses of the Schwarz Lemma. It is easy to check that $\phi$ is not a rotation when $f$ is not a rotation. So now our goal is to show $$\sup |\phi(z)| \,\,\geq\,\,\frac{1}{8}.$$ Remarks. That's what I have done. The derivative of $\phi$ is $\frac{1}{2}(f'(f(z))f'(z)-2z)$ , and using this we can know that $|\phi'(0)|<\frac{1}{2}.$ Of course we know $|\phi(z)|<|z|<1$ throughout the disc. Another idea is to pass to series expansions of $f$ and $\phi$ . Research. Approach Zero search results. Schwarz Lemma search results: https://math.stackexchange.com/search?page=11&tab=Relevance&q=schwarz%20lemma","['complex-analysis', 'inequality']"
4200272,How can we think of the codomain (range) of a probability density function?,"Let $f$ be a probability density function (PDF) with domain $D$ . How do you think about the codomain (range) of $f$ ? I'm only able to make sense of this when considering $f$ on an interval; $\int f(x) \newcommand{\dx}{\,\mathrm{d}x}\dx$ makes sense, but $f(x)$ does not. Is this the quirk that defining distributions/generalized functions solves? This is the mental block I've hit trying to make the leap from a discrete random variable to a continuous random variable. For example, if your random variable is height in inches and $\int_{(a,b)} f(x) \dx$ gives you the proportion of people in your sample with a height in inches between $a$ and $b$ , then the units on $f(x)$ must be something like percent per inch , and I can't wrap my head around that.","['probability-distributions', 'unit-of-measure', 'intuition', 'probability', 'density-function']"
4200318,A question about expectation of chromatic number,"For a graph $G$ , let $G_{1/2}$ be the subgraph of $G$ that each edge of $G$ is included independently with probability. Then for $H=G_{1/2}$ , after a moment thought, it can be proved that $\chi(H)\chi(H^c)\ge \chi(G)$ by coloring each vertex of $G$ by $(c_1,c_2)$ for color $c_1$ in the coloring of $H$ and $c_2$ in that of $H^c$ , where $H^c$ stands for the complement graph of $H$ and $\chi(G)$ stands for the vertex chromatic number of $G$ . A statement says that it implies $\mathbb{E}(G_{1/2})\ge\chi(G)^{1/2}$ . I am wondering why this statement is true. I think the above argument only implies $\mathbb{E}(\chi(H)\chi(H^c))\ge \mathbb{E}(\chi(G))=\chi(G)$ , even if we know $\mathbb{E}(\chi(H))=\mathbb{E}(\chi(H^c))$ (but they are not independent so we don't know $\mathbb{E}(\chi(H)\chi(H^c))= \mathbb{E}(\chi(H))\mathbb{E}(\chi(H^c))$ ?)","['graph-theory', 'coloring', 'combinatorics', 'probability']"
4200327,Is it possible for the set of critical points of a differentiable function $f$ to be union of the diagonal lines defined by $y= \pm x$,"Let $f: \mathbb{R}^2 ⟶ \mathbb{R}$ be a differentiable function. Let $S$ denote the set of critical points of $f$ . Is it possible for $S$ to be the union of $y = x$ and $y = -x$ ? I recently stumbled upon a textbook question asking if $S$ could be the union of the $x$ and $y$ axes, which led me to wonder about $y= \pm x$ . The way I thought about the question was that the partials with respect to $x$ and $y$ must be $0$ for this to occur. Hence, $$\frac{\partial f}{\partial x} = 0, \frac{\partial f}{\partial y} = 0$$ Moreover, these partials can only be $0$ when $y = x$ or $y = -x$ . This ""or"" is the part where I get confused - my original thought was for the partials to be $$\frac{\partial f}{\partial x} = x - y, \frac{\partial f}{\partial y} = x + y$$ But this leads to an intersection, not a union. Any guidance is greatly appreciated!","['multivariable-calculus', 'calculus', 'functions', 'real-analysis']"
4200340,$\lim_{\vert z\vert\to\infty}\frac{zf'(z)}{f(z)}=n\in\mathbb{N}$ implies $f$ is a polynomial,"Let $f$ be an entire function, meaning $$f:\mathbb{C}\to\mathbb{C}$$ is holomorphic. If $f\not\equiv0$ and $$\lim_{\vert z\vert\to\infty}\frac{zf'(z)}{f(z)}=n\in\mathbb{N}_0$$ then $f$ have to be a polynomial of degree $n$ . I was able to proof that $f$ can only have finitely many roots, using $$\frac{zf'(z)}{f(z)}=\frac{z}{z-z_1}+\frac{z}{z-z_2}+...$$ where $z_n$ are the roots. Taking the limit this series diverges if there were infintely many roots. This also already proofed, that $f$ has exactly $n$ roots. Nonetheless doing this is only valid, if $f$ even has any roots. If there are no roots, $f$ can not be a non-constant polynomial. So it's either constant or it is a transcendental entire function. So proving that $f$ can not be an transcendental entire function would imply the case what happens when $f$ has no roots. I would appreciate any hints how to prove, that $f$ is a polynomial.","['complex-analysis', 'entire-functions']"
4200342,"Min and max of $f(x,y)=e^{-xy}$ where $x^2+4y^2 \leq 5$","I am trying to use Lagrange multipliers to find the maximum and minimum values of the function $$f(x,y)=e^{-xy}$$ constrained as $$x^2+4y^2=5$$ I began this problem by setting up the Lagrangian: $$f(x,y) = e^{-xy}$$ $$g(x,y) = x^2+4y^2-5$$ $$L(x,y) = f(x,y) - \lambda g(x,y) = e^{-xy} - \lambda (x^2+4y^2-5)$$ So our equations are: $$-ye^{-xy} - 2\lambda x = 0$$ $$-xe^{xy} -8\lambda y = 0$$ $$x^2+4y^2-5 = 0$$ Now from the first equation, $e^{-xy} = 2\lambda x/y$ . Substituting into the second equation yields: $2x^2 \lambda / y = 8 \lambda y$ or $x^2+4y^2 = 0$ . This clearly violates the third equation, meaning this system has no solution. Doe this mean there are no local maxima or minima? Any guidance is greatly appreciated!","['multivariable-calculus', 'systems-of-equations', 'lagrange-multiplier', 'analysis']"
4200352,Tools or methods for solving a bizarre integral equation,"I am interested in solving (or determining that no solution exists) the following equation $$
f(x) + \int_{x^{-1}\tau}^1 f(xy)f(y)dy = G(x)
$$ for a function $f:[\tau,1]\rightarrow \mathbb{R}$ given a target function $G:[\tau,1]\rightarrow \mathbb{R}$ , where $\tau\in(0,1)$ . For certain functions $G$ , a viable approach to solving this is the fixed point iteration $$
f_{n+1}(x) = G(x) - \int_{x^{-1}\tau}^1 f_n(xy)f_n(y)dy.
$$ In particular, this works if the right hand side $G$ is sufficiently small. However, for larger functions $G$ this naive fixed point iteration fails to converge (I have observed this numerically). I also believe that it is possible that for certain functions $G$ the equation cannot be solved, although I have not been able to prove this. Are there any techniques for approaching an non-linear integral equation of this form, either numerically or theoretically? Unfortunately, no amount of differentiating can remove the integral and give an ODE, since the integrand depends on $x$ in a non-trivial manner. Has anyone seen something similar before?","['integration', 'ordinary-differential-equations', 'real-analysis']"
4200373,Intuition of Lindeberg Condition,"I'm reading Shiryaev's probability, where he discusses the Central Limit Theorem for normalized and centered sums $S_n$ of i.i.d random variables $X_1, \ldots X_n$ , $n\geq 1$ under the classical Lindeberg condition. The condition states that for any $\epsilon>0$ , as $n\to \infty$ , for independent $X_1, X_2 \ldots$ : $$ (L) \ \ \ \ \frac{1}{D^2_n} \sum_{k=1}^{n} \int_{\{|x-m_k|\geq \epsilon D_n\}}  (x-m_k)^2 dF_k(x) \to 0, $$ where $E X_n = m_k$ , $S_n = \sum_{j=1}^{n} X_j$ , $Var X_k = \sigma^2_k < \infty $ and also $D_n^2 = \sum_{k=1}^{n} \sigma^2_k$ . I read the proof of the TLC for the ""triangle array"" which requires this condition. However, I don't seem to truly grasp the intuition behind the Lindeberg's condition. I mean, I understand the proof and I have no doubt that it holds, but I'm not sure what the condition (L) in fact would mean. Could anyone explain me the intuition behind the Lindeberg's condition? Perhaps with a simple example, or more theoretically. My goal is simply to grasp the intuition behind it.","['measure-theory', 'central-limit-theorem', 'probability-theory', 'probability']"
4200381,Find the minimal polynomial of $\alpha=\sqrt{3+2\sqrt{2}}$ over $\mathbb{Q}$,"Question: Find the minimal polynomial of $\alpha=\sqrt{3+2\sqrt{2}}$ over $\mathbb{Q}$ Thoughts: the ""standard"" method of starting by squaring (twice) to get rid of the square roots, because I don't have a nice way of showing the resulting polynomial is irreducible.  So.. Attempt: It would be great if I could get our $\alpha$ in the form $$(a+b)^2=a^2+2ab+b^2=\sqrt{3+2\sqrt{2}}.$$ So, $$3+2\sqrt{2}=(\sqrt{2})^2+2\sqrt{2}+1=(\sqrt{2}+1)^2.$$ So, $$\alpha=\sqrt{3+2\sqrt{2}}=\sqrt{2}+1.$$ So, $$(\alpha-1)^2=2\\
 \alpha^2-2\alpha+1=2\\
\alpha^2-2\alpha-1=0.$$ So let $f(x)=x^2-2x-1$ .  Since $f(x)$ is irreducible over $\mathbb{Q}$ by the Rational Roots Test (since it has degree $2$ ), $f(x)$ is monic, and $f(\alpha)=0$ , we conclude that $f(x)$ is the minimal polynomial of $\alpha$ over $\mathbb{Q}$ . Does this look okay?","['field-theory', 'minimal-polynomials', 'abstract-algebra', 'solution-verification']"
4200393,Maximum number of edges in a graph satisfying conditions,"Problem : There is a graph with 40 vertices. It is known that any edge has at least one endpoint, on which no more than four other edges are incident. What is the maximum number of edges that this graph can have? No multiple edges or loops are allowed. My thoughts so far: We consider this graph to be bipartite, with one half consisting of all vertices of degree five, and the second half – remaining vertices. The reason this is valid is that if the second half has at least 5 vertices, we can connect those to all the vertices in the first half, thus meeting the bipartite requirements. In this case, the maximum number of edges is achieved when there are exactly 5 vertices in the second half: $35 \times 5=175$ . On the other hand, if the second half has less than 5 vertices, then the total number of edges is obviously smaller.","['graph-theory', 'graph-connectivity', 'bipartite-graphs', 'discrete-mathematics']"
4200513,Adding $3\cos\left(3t + \frac\pi5\right) + 4\cos\left(4t+\frac\pi8\right)$ in order to find the period of the sum,"How should I begin to add together these two trigonometric functions: $$3\cos\left(3t + \frac\pi5\right) + 4\cos\left(4t+\frac\pi8\right)$$ in order that I might obtain the value of the period of their sum ? I have considered expanding the two cosines, according to the rule of sum, but have gotten very little from it; and have wondered whether one could write the two using complex exponentials: $$3e^{i(3t+π/5)} + 4e^{i(4t+π/8)}.$$",['trigonometry']
4200524,About the Gauss Equation and the Codazzi Equation,"I am recently taking the undergraduate version of differential geometry. This week we finished the Compatibility Equations . Actually the contents given in the lecture are quite different from the book I read, Differential Geometry of Curves and Surfaces , by De Carmo. In this book, the author takes the following equations as the Gauss Formula and Mainardi-Codazzi Equations. $$(\Gamma_{12}^2)_u - (\Gamma_{11}^2)_v + \Gamma_{12}^1\Gamma_{11}^2 + \Gamma_{12}^2\Gamma_{12}^2 -\Gamma_{11}^2\Gamma_{22}^2 - \Gamma_{11}^1\Gamma_{12}^2 = -EK,$$ $$e_v - f_u = e\Gamma_{12}^1 + f(\Gamma_{12}^2 - \Gamma_{11}^1) + g\Gamma_{11}^2,$$ $$f_v - g_u = e\Gamma_{22}^1 + f(\Gamma_{22}^2 - \Gamma_{12}^1) + g\Gamma_{12}^2.$$ where $\Gamma_{ij}^K$ is the Christoffel Symbols and $E,F,G,e,f,g$ are the coefficients of the first fundamental form and the second fundamental form respectively. The version given in the lecture: $$\langle X_u,RX_v \rangle = K(EG-F^2),$$ $$\nabla_{\partial u}(\zeta X_v) = \nabla_{\partial v} (\zeta X_u),$$ where $K$ is the Guassian curvature, $\nabla V = (dV)^T = dV - \langle dV, N \rangle N$ and $\zeta$ is the weingarten operator. $X$ is the corresponding parametrization compatible with the orientation. Moreover, $R$ is the curvature tensor. In fact, I do not know what is the meaning of tensor, so let me just give the definition the lecturer presented. Definition. $$RV = \nabla_{\partial u}\nabla_{\partial v}V - \nabla_{\partial v}\nabla_{\partial u}V.$$ Could anyone explain the connection between the two sets of equations? How to show they are equivalent? Thanks.",['differential-geometry']
4200531,Interchange integration and summation for function $f_n(x)=\frac{e^{-nx}}{n}\ln{x}$,"My question is, for $$
f_n(x)=\frac{e^{-nx}}{n}\ln{x},
$$ how to prove that $$
\int_0^\infty\left[\sum_{n=1}^\infty f_n(x)\right]\mathrm{d}x =\sum_{n=1}^\infty \left[\int_0^\infty f_n(x)\,\mathrm{d}x\right]
$$ With the theorems descried here , for $x\in[1,\infty)$ , I am able to find a dominant function $$
g_n(x)=\frac{1}{n^2}\frac{\ln{x}}{1+x^2}
$$ so that $0\leq|f_n(x)|\leq g_n(x)$ for all for $x\in[1,\infty)$ . And we have $$
\left|\sum_{n=1}^N f_n(x)\right|\leq\sum_{n=1}^N\left|f_n(x)\right|\leq\sum_{n=1}^N g_n(x)\leq\sum_{n=1}^\infty g_n(x)=\frac{\pi^2}{6}\frac{\ln{x}}{1+x^2}
$$ $$
\sum_{n=1}^\infty\left[\int_1^\infty g_n(x)\,\mathrm{d}x\right]=\sum_{n=1}^\infty\frac{G}{n^2}=\frac{\pi^2}{6}G<\infty
$$ where $G$ is the Catalan constant . Therefore, I can show that the integration and summation can be interchanged for $x\in[1,\infty)$ . My difficulty is that I am not able to find a similar dominant function for $x\in[0,1]$ . Any help would be greatly appreciated.","['integration', 'analysis', 'real-analysis']"
4200557,Comparing sets $\{8^n-7n-1: n \in \Bbb{N}\}$ and $\{49(n-1): n \in \Bbb{N}\}$,"If $$\begin{align}
X&=\{8^n-7n-1: n \in \Bbb{N}\} \\
Y&=\{49(n-1): n \in \Bbb{N}\}
\end{align}$$ then, a) $X\subset Y \qquad$ b) $Y\subset X\qquad$ c) $X=Y\qquad$ d) none of these I know this question can be solved by taking $X=8^n-7n-1$ and splitting $8^n$ into $(7+1)^n$ and then apply binomial theorem as follows: Given, $$\begin{align}X &=8^n−7n−1 \\[4pt]
&=(1+7)^n−7n−1 \\[4pt]
&=1+7n+\frac{n(n-1)}{2}+\cdots+7^n-7n-1 \\[4pt]
&=\frac{n(n-1)}{2}7^2+\cdots+7^n \\[4pt]
&=49\left[\frac{n(n-1)}{2}+\cdots+7^{n-2}\right]
\end{align}$$ Hence, the set $X$ will be some specific multiples of $49$ . $Y=49(n-1)$ . Hence, the set $Y$ will be all multiples of $49$ . So, it will contain the elements of $X$ too. So, $$X\subset Y$$ But is there any alternate/simple method to solve this question without using binomial theorem?",['elementary-set-theory']
4200581,Understanding matrix multiplication for visualizing what is happening under the hood,"Take the case of this matrix multiplication: $$
A x=
\begin{pmatrix} 
1 & -1 & 2\\
0 & -3 & 1\\
\end{pmatrix}
\begin{pmatrix}
2 \\ 1 \\ 0
\end{pmatrix}
$$ The answer of which is $ 
    \begin{pmatrix}
    1 \\
    -3
    \end{pmatrix}
$ . Source: https://mathinsight.org/matrix_vector_multiplication I understand there are three components in $A$ and $x.$ So how can matrix multiplication have two (not sure if the component will be the right term) terms as part of the answer leading to matrix multiplication? What is the way to visualize the result? I think with three components, the matrix multiplication should have the result in three parts. I know I am missing something.","['matrices', 'vectors']"
4200587,Geometric intuition for $\mathcal{L}^{-m} \oplus \mathcal{L}^{m} \rightarrow T^{2}$ Calabi-Yau threefolds,"I'm having some problems to understand the geometry involved in the section 3.1 of the paper Two Dimensional Yang-Mills, Black Holes and Topological Strings . Concretely, I was wondering to know if it is possible to visualize the geometry of the $X = \mathcal{L}^{-m} \oplus \mathcal{L}^{m} \rightarrow T^{2}$ Calabi-Yau threefold by means of toric diagrams, in other words, how should I draw the image of the moment map of $X$ in ""the physics way"" $(\ast)$ .  Here $T^{2}$ is a $2$ -torus, $\mathcal{L}^{m}$ is the line bundle characterized by the fact that a holomorphic section of $\mathcal{L}^{m} \rightarrow T^{2}$ has a divisor of degree $m$ on $T^{2}$ and $\mathcal{L}^{-m}$ is the inverse bundle of $\mathcal{L}^{m}$ . $(\ast)$ By ""the physics way"" of draw toric diagrams I mean by considering non-compact toric Calabi-Yau manifold $X$ as $T^{2} \times \mathbb{R}$ fibrations over the image of the moment map of $X$ (see CY 3-folds are $T^2 \times \mathbb{R}$ fibrations over the base $\mathbb{R}^3$ . What does it mean? ). Below to the left I give the example of the toric diagram of $\mathbb{C}^{3}$ viewed as a $T^{2} \times \mathbb{R}$ -fibration over $\mathbb{R}^{3}_{\geq 0}$ ; $D_{1}$ , $D_{2}$ $D_{3}$ are the $2$ -dimensional cones of $\mathbb{R}^{3}_{\geq 0}$ . This example is discussed in detail in Topological strings and their physical applications (example 3.1, page 16). Below to the right represents the toric diagram of the cotangent bundle to a $\mathbb{P}^{2}$ embedded on a threefold (see Branes, Black Holes and Topological Strings on Toric Calabi-Yau Manifolds , section 3, page 8 or eq. 130, page 81 in Introduction to the topological vertex ). Here $D_{0},D_{1}$ , $D_{2}$ $D_{3}$ are the non-trivial divisors of the geometry where $D_{0}$ is a $\mathbb{P}^{2}$ and the remaining $\mathcal{O}(-p) \rightarrow \mathbb{P}^{1}$ bundles (the relevant $\mathbb{P}^{1}$ are the edges of $\mathbb{P}^{2}$ ). My problem that I can is that I can't draw the toric diagram of $\mathcal{L}^{-m} \oplus \mathcal{L}^{m} \rightarrow T^{2}$ . My faliure goes back to the fact that I have no clue on how to draw a fibration over a codimension 2 cycle ( $T^{2}$ ). I'm unable to draw the toric diagram of $\mathcal{L}^{2} \rightarrow T^{2}$ for example. Any hint, comment or reference is welcomed!","['mathematical-physics', 'algebraic-geometry', 'string-theory']"
4200589,6 Robberies happen in a city with 6 Districts what is the probability that a district has more than 1 robbery?,"That's the from stat110 book, problem 23 A city with 6 districts has 6 robberies in a particular week. Assume the robberies are located randomly, with all possibilities for which robbery occurred where equally likely.
What is the probability that some district had more than 1 robbery? I have found several threads with different solutions here, but none with mine. My solution:
Strategy: using the complement, $1-$ what is the probability that all robberies happen in different district? The first robbery can happen in any district out of the six.
The next robbery can happen in any of the $5$ other districts, then there are $4$ , $3$ , $2$ , $1$ districts. which translated to me into a probability of $$1 - \frac{5}{6}\frac{4}{6}\frac{3}{6}\frac{2}{6}\frac{1}{6}$$ or $$1 - \frac{5!}{6^5}$$ the solution is $$1 - \frac{6!}{6^6}$$ Where did my reasoning fail? thank you very much for explaining!","['statistics', 'combinatorics', 'probability']"
4200630,Why is $\mathfrak{m}/\mathfrak{m}^{2}$ the cotangent space to a variety?,"Let $X$ be a variety (or just a scheme) then for a point $p \in X$ with local ring $\mathcal{O}_{p}$ at $p$ , we define the cotangent space at $p$ to be the $\kappa(p)$ -vector space $\mathfrak{m}/\mathfrak{m}^{2}$ , where $\mathfrak{m}$ is the maximal ideal of $\mathcal{O}_{p}$ and $\kappa(p)$ is the residue field at $p$ . But why? I cannot see the intuition behind this. The vector space $\mathfrak{m}/\mathfrak{m}^{2}$ is just the space of functions vanishing at $p$ , modulo higher order terms. So in other words, first order approximations of functions vanishing at $p$ . All of my intuition tells me this is precisely what a tangent space is. So why is this the cotangent space? Why do we not then call the space of derivations the cotangent space? I was tempted to say this was just a matter of convention, but most algebraic geometry books claim that the cotangent space is more natural, and suggest that this is a quirk of algebraic geometry as opposed to differential geometry. So this suggests it is not just a convention but that there is some real meaningful difference. I am sorry if this is an extraordinarily basic question, but I find I am just going about using the definition without actually really knowing what or why I am doing it.","['tangent-spaces', 'affine-varieties', 'algebraic-geometry', 'commutative-algebra', 'differential-geometry']"
4200688,Every Lie group homomorphism from $U(1)$ to $\mathbb{R}$ is trivial,"I have to show that any Lie group homomorphism $f:U(1)\to (\mathbb{R},+)$ is the constant zero-map. What I tried so far is the following: Let $z\in U(1)$ . Then we have that $$0=f(1)=f(\vert z\vert^{2})=f(z\cdot \overline{z})=f(z)+f(\overline{z})$$ However, this does not help me, since $\overline{z}=1/z$ and hence $f(\overline{z})=-f(z)$ by the properties of a group homomorphism. In other words, the equation above is always fulfilled. Any ideas?","['group-theory', 'lie-groups', 'differential-geometry']"
4200724,What is wrong with my approach to solve 'The hurried duellers' problem?,"This is the problem statement : Duels in the town of Discretion are rarely fatal. There, each contestant comes at a random moment between 5am and 6am on the appointed day, and leaves exactly 5 minutes later, honor served, unless his opponent arrives within the time interval and then they fight. What fraction of duels lead to violence? This was my approach : Let's say A,B are the two duellers, then the probability of A arriving first would
be the same as B arriving first, let's calculate the probability of duel happening if A
arrives first. Since A arrives first if He arrives in 55-60 interval, there's definitely going
to be a duel. If A arrives in 0-55 interval, the probability would be given by $$\int_0^{55} \frac{5}{(60-x)} dx$$ Now let's call this above probability $p$ , then the actual probability would be give by : $$\frac{p}{2} + \frac{p}{2}$$ as half is probability of A arriving first and then same calculation with B arriving first. But this approach doesn't give correct result. I know this problem has been asked here : The Hurried Duelers brainteaser And I also know another approach to solve the problem. But i want to understand what I'm doing wrong in my calculation. EDIT : The answer is supposed to be approximately ~23/144, But I get much larger value.","['statistics', 'uniform-distribution', 'probability-theory', 'probability']"
4200743,Sde with linear growth,"Let $a,b : [0,T] \times \mathbb{R} \to \mathbb{R}$ be Borel functions with linear growth. Let $X$ be the solution of $$ X_t= \int_{0}^t a(s,X_s) dW_s + \int_{0}^t b(s,X_s) ds , t \in[0,T]$$ $\tau_n := \inf \{ t \geq 0, |X_t| \geq n\}$ Show that $E( \sup_{t \leq T} |{X_t}^{\tau_n} |^2 ) < \infty$ With Gronwall lemma, show the existence of $C$ a constant independant of $n$ such that $E( \sup_{t \leq T} |{X_t}^{\tau_n} |^2 ) < C$ $E( \sup_{t \leq T} |{X_t}|^2 ) < C$ We may use somewhat the Burkholder-Davis-Gundy inequality \begin{align*}
Z_t & ={X_t}^{\tau_n} \\
\mathbb{E} \sup_{t \leq T} |Z_t|^2 &\leq C_1 \mathbb{E}\langle M \rangle_T^{2/2} \\
&= C_1 \int_{0}^T a(s, M_s)^2 ds \\
&\leq C_1 \sup_{t \leq T} (1 + |Z_T|)\\
&\leq C_1 (1+n) \\
&< \infty \\
\end{align*} We use the following inequality \begin{align*}
Z_t & ={X_t}^{\tau_n} \\
Z_t^2 &\leq 2 x_0^2 +  2| \int_{0}^t a(s,Z_s) dW_s |^2 + 2 | \int_{0}^t b(s,Z_s) ds | ^2 \\
\end{align*} We evaluate each term separately. \begin{align*}
E ( \sup_{t <T} | \int_{0}^t a(s,Z_s) dW_s |^2 ) &\leq C E( | \int_{0}^t |a(s,Z_s)|^2 ds   | ) \\
&\leq C E (  \int_{0}^T  (1+ |Z_s| )^2  ds) \\
&\leq C' E (  \int_{0}^T ( 1+  \sup_{t <T} |Z_s|^2 ) ds )
\end{align*} We use the Jensen inequality with expectation with respect to an uniform distribution. \begin{align*} 
\left| \int_0^t b(s,X_s) \, ds \right|^2 &= t^2 \left| \int_0^t b(s,X_s) \, \frac{ds}{t} \right|^2\\
 &\leq t^{2-1} \int_0^t |b(s,X_s)|^2 \, ds \\ 
&\leq K^2 T^{2-1} \int_0^t (1+|X_s|^2) \,ds \\ 
&\leq K^2 T^{2-1} \int_0^t \left( 1+ \sup_{r \leq s} |X_r|^2 \right) \, ds\end{align*} $u(t) := \mathbb{E} \left( \sup_{r \leq t} |Z_r|^2 \right)$ $ u(t) \leq c_1 + c_2 \int_0^t u(s)ds$ $u(t) \leq c_1 e^{c_2 t} \leq c_1 e^{ c_2 T}$ by Gronwall inequality Fatou Lemma $E( \sup_{t \leq T} |{X_t}|^2 ) = E ( \liminf \sup_{t \leq T} |{X_t}^{\tau_n}|^2 )  \leq  \liminf E ( \sup_{t \leq T} |{X_t}^{\tau_n}|^2 ) < \infty  $","['stochastic-differential-equations', 'probability-theory']"
4200822,How to obtain discretized integral form of ODE initial value problem?,"Given is the ODE initial value problem $$
\frac{d}{dt} x(t) = f(t, x(t)), \quad x(t_0) = x_0.
$$ By integrating we obtain $$
x(t) = x_0 + F(t, x(t)) - F(t_0, x(t_0)) = x_0 + \int_{t_0}^{t} f(s, x(s))\,\mathrm{d}s.
$$ In order to solve the ODE numerically on $[t_0, t_f]$ , we use a grid $G = \{t_0 < t_1 < \ldots < t_f \}$ , so we have $$
x(t_{i+1}) = x(t_0) + \int_{t_0}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{1}
$$ and by applying a arbitrary quadrature rule for the integral, we obtain a numerical scheme. However, I noticed that in most books it reads $$
x(t_{i+1}) = x(t_i) + \int_{t_i}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{2}
$$ instead. Did I miss something? Or can I derive (2) from (1)?","['initial-value-problems', 'numerical-methods', 'ordinary-differential-equations']"
4200859,geodesic in metric space and in manifolds,"In the book by ''Metric spaces of non-positive curvature'' by Bridson and Haefliger we have the following definition for a geodesic in a metric space: Let $(X,d)$ be a metric space. A map $c:[0,l]\longrightarrow X$ is a geodesic if for all $s,t \in [0,l]$ we have $d(c(s),c(t))=\vert t-s \vert$ . So far so good. In the Example underneath this Definition they state the following: ''We emphasize that the paths which are commonly called geodesics in differential geometry need not be geodesics in the metric sense; $\textbf{in general they will only be local geodesics}$ .'' I assume they mean by ''metric sense'' the metric on our manifold that is induced by our riemannian metric. Otherwise I don't know what they mean? But if this is true I'm quite confused about this, since $\gamma:[0,1] \longrightarrow \mathbb{R}^2, t \longmapsto 2t$ is a geodesic in the riemannian sense (if we consider the standard riemannian metric on $\mathbb{R}^2$ with induced connection). But this will never be a local geodesic in the metric sense, as $d(\gamma(s),\gamma(t))=2\vert t-s \vert$ for all $s,t \in [0,1]$ . Where fails my thinking? And if it does not fail, what is the connection between geodesics in the metric sense and riemannian sense?","['metric-geometry', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
4200860,distance of $\mathbb{Z}^n$ to line in direction $\mathbf{1}$,"I have a question about geometry. Let $\displaystyle\mathbf 1:=\sum_{i=1}^ne_i$ . Further let $w\in \mathbb R^n$ be arbitrary. For $g=\{w+t\mathbf 1 : t\in \mathbb R\}$ I would like to show that $u\in g$ and $v\in \mathbb Z^n$ exist with $\|u-v\|<\frac{1}{2\sqrt 3}\sqrt{n}$ . I realize that it doesn't hold for ""small"" $n\in \mathbb N$ , but I think it should hold for large ones. Let me write what I have been thinking about:
Since $g$ intersects the hyperplane $H=\{x\in \mathbb R^n : x_n=0\}$ w.l.o.g. we can assume that $w\in H$ . Now, if we choose the point $v\in \mathbb Z^n$ that has the smallest distance in each component of $w$ , we know $|v_i-w_i|\leq \frac 12$ for $1\leq i<n$ and $v_n=w_n=0$ . On the other hand, we know that the distance from $g$ to this $v$ is \begin{align*}
d(g,v)=\| (v-w) - \frac 1n\langle v-w, \mathbf 1\rangle\mathbf 1 \|.
\end{align*} If we set $p:=v-w$ we get \begin{align*}
d(g,v)^2 &=\| p - \frac 1n\langle p, \mathbf 1\rangle\mathbf 1 \|^2\\\
&=\| p\|^2-2\frac 1n\langle p, \mathbf 1\rangle\cdot\langle p, \mathbf 1\rangle + \frac{1}{n^2}\langle p, \mathbf 1\rangle^2\cdot \| \mathbf 1\|^2\\
&=\| p\|^2-\frac 2n\langle p, \mathbf 1\rangle^2 + \frac{1}{n}\langle p, \mathbf 1\rangle^2\\
&=\| p\|^2-\frac 1n\langle p, \mathbf 1\rangle^2.
\end{align*} Now the problem is, if I have, for example, $p=(\underbrace{\frac 12, \ldots, \frac 12}_{\frac{n-1}{2}\ \text{times}}, \underbrace{-\frac 12, \ldots, -\frac 12}_{\frac{n-1}{2}\ \text{times}}, 0)$ , then $d(g,v)=\frac{\sqrt{n-1}}{2}$ . But in this case there is a point $v'\in\mathbb Z^n$ such that $p'=(\underbrace{\frac 12, \ldots, \frac 12}_{n-1\ \text{times}}, 0)$ and then $d(g,v')=\sqrt{\frac 12-\frac{1}{4n}}\approx \frac{1}{\sqrt 2}$ . The choice of my lattice point would be bad in this case, but my hypothesis is still correct. Does anyone have an idea?","['integer-lattices', 'combinatorics', 'geometry', 'discrete-mathematics']"
4200919,How to prove that white noise exists in $n$ dimensions,"I am defining white noise $W$ over $\mathbb R^n$ as a probability distribution over tempered distributions with the following property: $$D \sim W \implies \langle D, f \rangle \sim \mathcal N \left (0, \int f^2 \right)$$ for any test function $f : \mathbb R^n \rightarrow \mathbb R$ . How do you prove such a distribution exists for each $n$ . For $n=1$ , you can take the derivative of the wiener process, but I don't know what to do for other cases.","['noise', 'distribution-theory', 'stochastic-processes', 'functional-analysis', 'probability-theory']"
4200921,"If $f''(x) \le 0, \, \forall x \neq 0$ and $f$ is minimum at $0$, prove $f'(0)$ doesn't exist.","A classic example of a function being differentiable everywhere except at one point is the function $f(x) = \sqrt{\lvert x \rvert}$ . This function is not differentiable at $x=0$ as is shown in this answer . I was thinking about what was the required behavior for a function to have around $x=0$ for the derivative to be nonexistent. I believe the requirements are: The function needs to be continuous, since otherwise differentiability $\implies$ continuity would prove the statement is trivially true. The function has to be concave for all $x \neq 0$ . The function needs to be minimum at the point $x=0$ . To me, this intuitively seems to guarantee the ""pointiness"" at $x=0$ that makes the function non-differentiable. Inspired by the above I wanted to generalize the result. I propose the following theorem: Given a continous function $f:\mathbb{R} \to \mathbb{R}$ , if $f''(x) \le 0 $ on some interval $0<|x|< a$ , for some $a\in (0,\infty]$ , and $f$ is minimum at $0$ , then $f'(0)$ doesn't exist. Here is my attempt at proving the statement: We analyze the limit $\lim_{h \to 0^+}$ and $\lim_{h \to 0^-}$ separately. We see that $\lim_{h \to 0^+} \frac{f(0 + h) - f(0)}{h} > 0$ since $h>0$ and $f(h)-f(0)> 0$ using that the function is minimum at $0$ . Similarly, $\lim_{h \to 0^-} \frac{f(0 + h) - f(0)}{h} < 0$ since $h<0$ and $f(h)-f(0)> 0$ using that the function is minimum at $0$ . So since the limit from the right is positive, but the limit from the left is negative, then the limit doesn't exist. QED. This attempt troubles me because I didn't use the condition that the function has to be concave for all $x \neq 0$ , so if the proof were correct this would mean that the theorem would also hold for functions like $x^2$ which are also minimum at $x=0$ , but this is clearly wrong! I can't seem to find exactly what's wrong with the argument, but because of the lack of my use of the hypothesis I know it is indeed wrong. Can anyone tell me how I could correct my proof to make it valid? Edit: The case where $f''(x) =0$ has a family of counterexamples as Theo Bendit pointed out in his answer. However, the question was inspired by functions that have similar behavior to $f(x) = \sqrt{\lvert x \rvert}$ , so any suggestions on how to prove the statement with $f''(x) <0$ instead of $f''(x) \le 0$ are greatly appreciated.","['real-analysis', 'alternative-proof', 'calculus', 'solution-verification', 'derivatives']"
4200951,Selecting identical and distinguishable balls when the order matters,"I made up a question such that There are $3$ identical blue balls , $2$ identical red balls , $5$ distinguishable yellow balls , $4$ distinguishable green balls. We want to make a mixture consisting of $8$ balls by using these balls. How many ways are there a-) If selection order does not matter b-) If selection order matters MY WORK = a-) This part was easy by using generating functions such that Blue balls = $(1+x +x^2 +x^3 ) $ Red balls = $(1+x +x^2 )$ Yellow balls = $(1+5x +10x^2 +10x^3 +5x^4 +x^5) $ Green balls = $(1+4x +6x^2 +4x^3 +x^4)$ We should find the coefficient of $[x^{8}]$ in the expansion of them . b-) This part is where i hesitated my solution. I used exponential generating functions , but i am not sure about the exponential generating function of distinguishable balls. Blue balls = $$\bigg(1+ \frac{x}{1!} +\frac{x^2}{2!} + \frac{x^3}{3!} \bigg ) $$ Red balls = $$\bigg(1+ \frac{x}{1!} +\frac{x^2}{2!} \bigg)$$ Yellow balls = $$ \bigg (1 + \frac{P(5,1) \times x}{1!} +\frac{P(5,2) \times x^2}{2!} + \frac{P(5,3) \times x^3}{3!} + \frac{P(5,4) \times x^4}{4!} + \frac{P(5,5) \times x^5}{5!} \bigg)$$ Green balls = $$ \bigg (1 + \frac{P(4,1) \times x}{1!} +\frac{P(4,2) \times x^2}{2!} + \frac{P(4,3) \times x^3}{3!} + \frac{P(4,4) \times x^4}{4!} \bigg)$$ So , i should find the coefficient of $\frac{x^{8}}{8!}$ or find the coefficient of $x^{8}$ and multiply it by $8!$ . I am not sure about part $b$ .I want you to check my solution.","['solution-verification', 'combinatorics', 'generating-functions']"
4200963,Trying to Prove Distributive Laws with Respect to Set Difference,"I've been trying to prove the following two statements: $A \cap(B-C)=(A\cap B)-(A\cap C)$ $(A-C)\cap (B-C)=(A\cap B)-C$ And I have a proof, which I want to make sure is correct for (1) in one direction (which if correct then I can do in the other direction similarly), but I'm very unsure for (2). My proof for (1) is as follows: Let $x\in A \cap (B-C) $ . $\Rightarrow$ $x \in A $ $\land$ $x\in(B-C) $ $\Rightarrow$ $x \in A $ $\land$ $(x\in B$ $\land$ $ x\notin C) $ $\Rightarrow$ $(x \in A$ $\land$ $x\in B)$ $\land$ $(x\in A$ $\land$ $ x\notin C) $ $\Rightarrow$ $(x \in A \cap B)$ $\land$ $(x\notin A \cap C) $ $\Rightarrow$ $x\in (A \cap B)$ $-$ $(A \cap C) $ And the reverse direction also follows (if true), which means $ A \cap(B-C) \subseteq (A\cap B)-(A\cap C)$ and $  (A\cap B)-(A\cap C) \subseteq A \cap(B-C) $ $\Rightarrow A \cap(B-C)=(A\cap B)-(A\cap C) $ . My proof for (2), is: Let $x\in (A-C)\cap (B-C)$ $\Rightarrow x\in (A-C)$ $\land$ $x\in (B-C) $ $\Rightarrow (x\in A$ $\land$ $x\notin C$ ) $\land$ $(x\in B$ $\land$ $x\notin C)$ $\Rightarrow (x\in A$ $\land$ $x\in B$$)$ $\land$ $x\notin C$ $\Rightarrow x\in (A \cap B)$ $\land$ $x\notin C$ $\Rightarrow x\in (A \cap B) - C$ and similarly, the reverse direction also follows, which means $ (A-C)\cap (B-C) \subseteq (A\cap B)-C$ and $  (A\cap B)-C \subseteq (A-C)\cap (B-C)$ $(A-C)\cap (B-C)=(A\cap B)-C$ . I'm quite new to this and I've been trying to teach myself for only about a week so please tell me where I'm going wrong.",['elementary-set-theory']
4200990,Convergence of $\sum_{n=0}^{\infty} \frac{4^n}{3^n+7^n}$,"I need help with this. $\sum_{n=0}^{\infty}  \frac{4^n}{3^n+7^n}$ I know that it converges but i can not proove why. I tried to rewrite it, it seems to be a geometric serie. I tried to do a common factor between $3^n+7^n  \rightarrow 3^n(1+\frac{7^n}{3^n})$ So I have $\sum_{n=0}^{\infty} (\frac{4}{3})^n \frac{1}{1+(\frac{7}{3})^n}$ And I do not know if that helps. I can also make different the common factor and I would have $\sum_{n=0}^{\infty} (\frac{4}{7})^n \frac{1}{1+(\frac{3}{7})^n}$","['summation', 'analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
4200994,Changing the sequential probabilities but reaching the same final states - possible? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question I am not very good at math-speak and I hope my explanation is clear below. I will provide an example & hopefully I am not missing something obvious in thinking about this problem. Question: Consider the case where you have the following distribution; $A : P(A)$ $B : P(B)$ $P(A) + P(B) = 1.$ furthermore, in the case of A, events C,D,E and F follow, again with P(C),P(D),P(E) AND P(F) so that you have sum of P(C), ... , P(F) = 1. i.e in all events A - F all scenarios are accounted for. Worth noting here that there does not necessarily have to be 4 different sub-events, although I think that the scenario changes once you have more than 2. Anyway, now we have something like this; $P(A)*P(C) + P(A)*P(B) + P(A)*P(C) + P(A)*P(D) + P(B) = 1.$ my question is this; Is it possible to re-represent this using 4 different events over 4 different combinations of the events, combining them to reach the same end states? i.e $P(X)^4 = P(A)*P(C)$ $P(Y)^4 = P(A)*P(D)$ $P(Z)^4 = P(A)*P(E)$ $P(W)^4 = P(A)*P(F)$ and that P(B) is equal to all ""mismatching"" sequences. i.e $P(B) = 1 - (P(X)^4 + P(Y)^4 + P(Z)^4 + P(W)^4)$ . One way to ask is under which conditions can you take the 4th root of all P(C), P(D), P(E), P(F) and still have the result + P(B) = 1, Furthermore, does this become simpler if the X,Y,Z or W probabilities can differ, i.e $P(X1)*P(X2)*P(X3)*P(X4) = P(A)*P(C), but P(X1) + P(Y1) + P(Z1) + P(W1) = 1$ etc.. In an example - On a game show you have 1 bag with white balls and black balls. If you reach in and grab get a white ball you win prize A with probability A, prize B with probability B, prize C with probability C and prize D with probability D. A black ball gets you nothing. The game show host wants to change the game but not the probability of winning the prizes. He wants to have 4 bags, each bag has a ball with A, B, C or D on it. The player pics on ball from each bag and is only awarded a prize if all of the balls match, otherwise gets nothing. Question - is this possible? Another example for fun... An intruder with a random hand (haha...) changes the switches in your house. You have initially a switch that could go on or off. It is on with probability P(A) and off with probability P(B). When it is on there is a probability P(C) that the light is green, P(D) that the light is red, P(E) that the light is blue and P(F) that the light is yellow. You want to make things easier for your intruder. You rebuild the house and now you have 4 switches, and each switch has 4 states, one for every colour. If all the switches are on the same colour, the light shines with that colour. If there are any mismatches the light is off. Is your intruder able to re-create the probability of the final states of the lights every time that you walk in after he has been there?","['discrete-mathematics', 'statistics', 'combinatorics', 'probability']"
4201031,Finding smallest vector satisfying an equation with a dot product,"I was wondering whether you could help me understand a solution to one of the problems from the 6.036 Introduction to Machine Learning from MIT Open Learning Library . The task is to find the smallest vector (with respect to the L2 norm) $\theta$ which satisfies the equation $$\theta \cdot x = \frac{1}{y}$$ where $x$ is an arbitrary vector (with the same dimension as $\theta$ ), y is a constant taking either value $+1$ or $-1$ and $\cdot$ is the dot product. The provided solution is: $$\theta = \frac{x}{\| x \|^2} \times \frac{1}{y}$$ Even after looking at the solution, I am still not sure how this problem was supposed to be solved. It seems that the solution could be derived using manipulations similar to: $\theta \times (x \cdot x) = x \times {1 \over y}$ $\theta \times \| x \| ^ 2 = x \times {1 \over y}$ $\theta = {x \over \| x \| ^ 2} \times {1 \over y}$ However, I struggle to see what could be the justification for transforming the original equation to the form in the first step, so I suspect this might not be the right way.","['linear-algebra', 'vectors', 'products']"
4201059,"Integrate $\int \frac{x^{4n-2}}{x^{4n}+x^{2n}+1} \, \mathrm dx$","I want to evaluate the following indefinite integral with IBP and also with the limits $0$ to $\infty$ . Once indefinite integral is calculated, it is easy to evaluate it within given limits. $$I=\int \frac{x^{4n-2}}{x^{4n}+x^{2n}+1} \, \mathrm dx$$ So, I proceed through Partial fractions method. $$I=\int \frac{x^{4n-2}}{x^{4n}+x^{2n}+1} \, \mathrm dx=\frac12 
 \int \frac{x^{2n-2}+x^{n-2}}{x^{2n}+x^n+1} \, \mathrm dx+\frac12
\int \frac{x^{2n-2}-x^{n-2}}{x^{2n}-x^n+1} \, \mathrm dx$$ We can split it into $4$ integrals but I think that would make it a little messy. Let $\space I=I_{1}+I_{2}$ $$I_1= \int \frac{1}{2x}\bigg(\frac{2x^{2n-1}+x^{n-1}-x^{2n-1}}{x^{2n}+x^{n}+1}\bigg)\mathrm dx$$ $$\implies I_1=\int \frac{1}{2nx}\bigg(\frac{2nx^{2n-1}+nx^{n-1}}{x^{2n}+x^n+1}-\frac{nx^{2n-1}}{x^{2n}+x^n+1}\bigg) \, \mathrm dx$$ After IBP, we get $$I_1=\frac {1}{2n}\Bigg[\frac{1}{x}\ln(x^{2n}+x^n+1)+\int \frac {1}{x^2} \ln(x^{2n}+x^n+1) \, \mathrm dx\Bigg]-\frac12 \int \frac{x^{2n-2}}{x^{2n}+x^{n}+1} \, \mathrm dx$$ Let $x=\frac{1}{x}$ for above two integrals and we get $$I_1 = \frac {1}{2nx} \ln(x^{2n}+x^n+1)+x\ln(x)-x-\frac{1}{2n}\int \ln(x^{2n} + x^{n}+1)\,\mathrm dx +\frac12 \int \frac{1}{x^{2n}+x^n+1} \, \mathrm dx$$ Same thing goes with $I_2$ and we are stuck with the
following integrals $$\int \ln(x^{2n}\pm x^{n}+1)\,\mathrm dx \space and \int \frac{1}{x^{2n}\pm x^n+1} \, \mathrm dx$$ I don't think so that their anti-derivative can be expressed in terms of elementary functions. But I'm also interested in definite integrals of those functions i.e $$\int_0^\infty \ln(x^{2n}+x^{n}+1)\mathrm dx \text{ and } \int_0^\infty \frac{1}{x^{2n}+x^n+1}\,\mathrm dx$$","['integration', 'calculus', 'definite-integrals', 'logarithms']"
4201080,Center of a circle rolling on the graph of a sinusoidal function ( say $f(x)=3\cos(x/3) $),"I'd like to draw a circle rolling on the graph of the function $f(x)=3(\cos \frac{x}{3})$ . I've set radius = 3 units. My idea was as follows : (1) define a point $P=(a, f(a)) $ ( with $a$ varying over some interval ) (2) defining a tangent function at $P$ and then a function corresponding to the perpendicular to this tangent at $P$ (3) defining , uning trigonometry, a point located on this perpendicular at 3 units from point $P$ , in order to use this point as the center of the desired circle My question : how to use the tangent function in order the center of the circle not to lie below the curve when the point goes upward  ( as one can see on the image below )? Here my attempt using Desmos : https://www.desmos.com/calculator/tpdfunm1kd","['analytic-geometry', 'trigonometry', 'graphing-functions', 'soft-question']"
4201082,Special fiber VS reduction of a formal scheme over a complete DVR,"Let $R$ be a complete DVR with maximal ideal $\mathfrak m$ and residue field $k = R/\mathfrak m$ . Let $\mathfrak X$ be a formal scheme over $\mathrm{Spf}(R)$ , formally of finite type if necessary. Such a formal scheme may be seen as a direct system $(X_n)_{n\in \mathbb N}$ where $X_n$ is a scheme over the ring $R/\mathfrak m^{n+1}$ , and with a few compatibility properties. The special fiber of $\mathfrak X$ is the scheme $\mathfrak X_0$ defined over $k$ . On the other hand, provided that $\mathfrak X$ is locally noetherian, it admits a largest ideal of definition which we call $\mathfrak J$ . The reduction of $\mathfrak X$ is the scheme sharing the same underlying topological space as $\mathfrak X$ and with structure sheaf $\mathcal O_{\mathfrak X}/\mathfrak J$ . It is denoted by $\mathfrak X_{\mathrm{red}}$ , it is defined over $k$ and it is reduced. Am I correct to think of $\mathfrak X_{\mathrm{red}}$ as the reduced $k$ -scheme structure on the special fiber $\mathfrak X_0$ ? If so, what kind of properties on $\mathfrak X$ can insure that $\mathfrak X_0$ is already reduced ?","['algebraic-geometry', 'schemes']"
