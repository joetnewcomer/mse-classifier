question_id,title,body,tags
4365652,"Given the length of the diagonals, what is the maximum perimeter of a quadrilateral?","Suppose the length of the diagonal $d$ . I'd like to maximize the perimeter of a quadrilateral containing two of these diagonals. I know that for a rectangle instead of a quadrilateral, the solution is the square, obtaining a perimeter of length $4d\frac{\sqrt{2}}{2} = 2\sqrt{2}d$ . However when not constraint by right angles, the solution must more than the square's perimeter, since one could construct a triangle of side lengths $d$ , $d$ and $\sqrt{2}d$ by coinciding two ends of the diagonals, and creating a right angle, giving a perimeter of $(2 + \sqrt{2})d$ . What is the largest perimeter possible? EDIT: as pointed out by Henry, I'd like the shape to be convex, or else the perimeter is unbounded. Also, a flat quadrilateral, where the diagonals are basically aligned, has a perimeter of $4d$ . The difficulty is to prove it optimal.","['quadrilateral', 'optimization', 'geometry']"
4365656,Prove the triangles $LBE$ and $LKD$ are congruent.,"I found this problem in a 2019 mathematics competition in Hunan, China. I am a Chinese high school student. There is a triangle $ABC$ which is not equilateral. The point $H$ is the orthocenter of it. The circumscribed circle of the triangle has a tangent $BK$ . The line $HK$ is perpendicular to $BK$ . The midpoint of $AC$ is $L$ . How to prove $LB$ = $LK$ ? I drew lines $AH$ and $CH$ . Then I plotted a circle with the center $L$ and the diameter $AC$ . Suppose the points $D$ , $E$ are respectively the intersections of the circle $L$ and the lines $AH$ , $CH$ . Then I guess that the triangles $LBE$ and $LKD$ are congruent. But I can't prove it.","['contest-math', 'euclidean-geometry', 'geometry', 'geometric-construction']"
4365679,A detail in a proof of the Steiner's inequality about sets of finite perimeter,"Given an open limited set $E$ of $\mathbb{R}^n$ with smooth boundary, how could I subdivide this set $E$ in a finite number of normal sets? I remind you that the normal set is a set of this form, given an open set $A$ in $\mathbb{R}^{n-1}$ and two functions $\varphi_{1}$ and $\varphi_2$ defined on $A$ the corresponding normal set is $$\{(x,y) \in A : \varphi_{1}(x)<y<\varphi_{2}(x)\}.$$ Subdivide means to partition the set at most excluding $\mathcal{H}^{n}$ negligible sets.
My question emerges from a proof of the Steiner inequality that Francesco Maggi gives in this book at pag 161. https://www.cambridge.org/core/books/sets-of-finite-perimeter-and-geometric-variational-problems/F8D0DABFFFB0D444C5AD5D37B3E3DBC1 What I know for certain is that I can choose a hyperplane $\Pi$ for which there are a $\mathcal{H}^{n-1}-$ negligible amount of points with normal vector parallel to the hyperplane $\Pi.$ So almost all of the perpendicular lines to $\Pi$ are intersecting $E$ in a finite number of segments. In the figure I tried to give a sense of what I am asking.","['differential-geometry', 'geometric-measure-theory', 'real-analysis']"
4365715,Why is the factor in Doob‚Äòs classical $L^p$ inequality optimal?,"I encountered a following formulation of the Doob's $L^p$ inequality. Let $T\neq\emptyset$ be a totally ordered set and suppose that $X=(X_t)_{t\in T}$ is a positive submartingale. Define $X^*=\mathrm{ess}\,\mathrm{sup}_{t\in T}X_t$ and $X_u^*= \mathrm{ess}\,\mathrm{sup}_{t\in T, t\le u}X_t$ for every $u\in T$ . Then, for every $p>1$ , $$ \mathbb{E}[(X_u^*)^p]\le \left(\frac{p}{p-1}\right)^p\mathbb{E}[(X_u)^p], \quad u\in T $$ and $$ \mathbb{E}[(X^*)^p]\le \left(\frac{p}{p-1}\right)^p  \sup_{u \in T} \mathbb{E}[(X_u)^p].$$ In the textbook it is mentioned as a side note, that the factor $p/(p-1)$ is optimal. I am missing an explanation to this. I tried to show it by taking another smaller factor and (hopefully) show that the inequality is not valid anymore, but I can‚Äòt seem to get to a solution. I hope someone can give me some input whether my approach is right and if not, how can I show that this factor is optimal. Thanks.","['stochastic-analysis', 'real-analysis', 'lp-spaces', 'martingales', 'probability-theory']"
4365726,"Parity of number of subsets of $\{1,2,3, \ldots, n\}$ whose average value is an integer","Let $n\in \mathbb Z^{+}$ and $S_n = \{1,2,3, \ldots, n\}$ . A subset $A$ of $S_n$ is said to be beautiful if the average of all the elements in $A$ is an integer. Let $G_n$ be the set of all beautiful subsets of $S_n$ . Prove that $|G_n| \equiv n \bmod 2$ . Here's all I've been able to do: First, it is easy to see what must be proved true for $n = 1, 2$ . I assume the thing to be proved true up to $n =k$ ; I will prove it also for $n = k+1$ . Thus, I partition the beautiful subsets of $S_n$ into two categories: Type $1$ : Beautiful subsets that do not contain $n+1$ . Let $B$ be the number of subsets of type $1$ . By the assumption of induction, we get $|B| \equiv n-1 \bmod 2$ . Type $2$ : Beautiful subsets that do contain $n+1$ . Let the number of subsets of type $2$ be $C$ . My idea in this case is: Let $T$ be the average of any $s$ numbers ( $1\le s \le n+1$ ) that contain $n+1$ (where each number is ${}>1$ ). I realize that : \begin{align*}T&=\frac{a_1+a_2+a_3+\cdots+a_{s-1}+(n+1)}{s}\\&= \frac{s+(n+1-1)+a_1-1+a_2-1+\cdots+a_{s-1}-1}{s}\\&=s+ \frac{n+(a_1-1)+(a_2-1)+ \cdots +(a_{s-1}-1)}{s}\end{align*} $\Rightarrow T\in \mathbb{Z}$ if and only if the average of $(n,a_1-1,a_2-1,\ldots,a_{s-1}-1)$ is also an integer (because $a_i>1\Rightarrow a_i-1>0$ ). Thus we can calculate the number of beautiful subsets of $S_{n+1}$ containing $n+1$ with each element being ${}>1$ , which is the number of beautiful subsets of $S_n$ containing $n$ . And Cranium Clamp suggested that if $Q=\frac{b_1+b_2+\cdots+b_{s-1}}{s} \in \mathbb{Z}$ , then $\{b_1,b_2,\ldots,b_{s-1},Q\}$ is a beautiful subset. Thus, the problem that I have not solved is that the sets of type $2$ contain both $(n+1)$ and $1$ .","['number-theory', 'combinatorics', 'elementary-number-theory']"
4365759,Closed form expression for series involving Legendre polynomials,"Given $-1 \leq x \leq 1$ and $0 \leq \eta \leq 1$ ,
I am interested in computing $$
E(x,\eta) = \sum_{\ell = 0}^{+ \infty} |P_{\ell} (0)|^{2} \, P_{\ell} (x) \, \eta^{\ell} ,
$$ with $P_{\ell}$ the usual Legendre polynomials. Can one find a closed form expression for this function? NB1: Should this be useful, $E(x,\eta)$ is, in essence, the gravitational potential between two massive circles centered around the same location, inclined by a respective angle $\cos^{-1}(x)$ and with a ratio of radii given by $\eta$ . NB2: Of course, in the limit $x \to 1$ or $\eta \to 0$ , I expect that I could perform appropriate limited developments.","['legendre-polynomials', 'closed-form', 'sequences-and-series']"
4365805,"Open sets $U, V$ such that $U, V$ and $ U \cup V$ are all simply connected but $U \cap V$ is disconnected","In $(\mathbb{R^2}, \tau_E)$ , I would like to find some open sets $U, V$ such that $U, V$ and $ U \cup V$ are all simply connected but $U \cap V$ is disconnected. I am not sure whether it is possible or not. I thought that I could take $U = \{x^2 + y^2 = 1, y < \frac{1}{2}\}$ and $V = \{x^2 + y^2 = 1, y > - \frac{1}{2}\}$ . This way I obtain that $U \cap V$ is disconnected but the problem is that $ U \cup V$ is only connected and not simply connected (as it is $\mathbb{S}^1$ ). Do you have any idea of how to either find some $U,V$ that fit or prove that it is impossible to find such sets ? Thank you very much in advance :)","['real-numbers', 'general-topology', 'metric-spaces', 'connectedness']"
4365831,Does Radon-Nikodym derivative affect the Variance of a Random Variable?,"Edit 26 January 2022 : The answer below elegantly shows that when the diffusion term of an Ito process is not constant in $\omega$ , it is generally not true that Variance remains unaffected by the change of measure. In the question below, however, the focus is on the case where the diffusion term of the stochastic process is constant. I would therefore appreciate any additional answers that would focus on the case with constant diffusion term. Additionally, it would be amazing to make a link between the continuous and the discrete case that are discussed below. Original question : Suppose $$X_t:=W^{\mathbb{P}}_t+\mu t$$ with $W^{\mathbb{P}}$ being the standard Wiener process under $\mathbb{P}$ . We know that trivially, under $\mathbb{P}$ , the PDF for $X_t$ would be: $$f_{X_t}(h)=\frac{1}{\sqrt{t2\pi }}\exp\left(\frac{-(h-\mu t)^2}{2t}\right)$$ The Cameron-Martin-Girsanov theorem gives us the Radon-Nikodym derivative that will turn $X_t$ into a standard Wiener process under some new $\mathbb{Q}$ , specifically: $$\frac{d\mathbb{Q}}{d\mathbb{P}}=\exp\left(\frac{-2X_t\mu t+\mu^2t^2}{2t}\right)=\exp\left(-X_t\mu +0.5\mu^2t\right)=\exp\left(-W_t\mu -0.5\mu^2t\right)$$ Under $\mathbb{Q}$ , the PDF for $X_t$ now becomes: $$f_{X_t}(h)=\frac{1}{\sqrt{t2\pi }}\exp\left(\frac{-h^2}{2t}\right)$$ Bottom line : changing the measure has resulted in a different mean, but the variance has been preserved (it remains $t$ under both measures). Question : I came up with a toy example to illustrate a change of measure on the following Binomial tree, but the change of measure seems to have produced a change in the variance. Was I wrong to assume that change of measure always preserves variance? Is there a proof that shows otherwise? Toy example : $X_t$ is now a single-period variable, which can take the values { $110,95$ } with equal probabilities of 0.5 under $\mathbb{P}$ : Under $\mathbb{P}$ , we have: $$\mathbb{E}^{\mathbb{P}}[X_1]=0.5*110 + 0.5 * 95 = 102.5$$ (note that the ""drift"" is 2.5% per this single period, i.e. on ""average"", we move to 102.5 from the initial value of 100), whilst we have $$Var^{\mathbb{P}}(X_1)=\mathbb{E}^{\mathbb{P}}[(X_1-102.5)^2]=0.5*7.5^2+0.5*7.5^2=56.25$$ Now we define $\frac{d\mathbb{Q}}{d\mathbb{P}}(\omega)=\frac{2}{3}$ when $X_1=110$ and $\frac{d\mathbb{Q}}{d\mathbb{P}}(\omega)=\frac{4}{3}$ when $X_1=95$ : this results in the probability of $X_1$ being ""up"" changing from $0.5$ to $\frac{1}{3}$ whilst the probability of $X_1$ being ""down"" changes from $0.5$ to $\frac{2}{3}$ . As a result, we now have: $$\mathbb{E}^{\mathbb{Q}}[X_1]=\mathbb{E}^{\mathbb{P}}\left[X_1\frac{d\mathbb{Q}}{d\mathbb{P}}(\omega)\right]=\frac{1}{2}*\frac{2}{3}*110+\frac{1}{2}*\frac{4}{3}*95=100$$ (note that we have eliminated the ""drift"" via change of measure), whilst we have $$Var^{\mathbb{Q}}(X_1)=\mathbb{E}^{\mathbb{Q}}[(X_1-100)^2]=\frac{1}{3}*10^2+\frac{2}{3}*5^2=50$$ (note that unfortunately, we have also changed the variance) What worries me further is that this paper shows that the multiplicative Binomial tree model converges to the well-known Geometric-Brownian-Motion (GBM) model when we take the limit in the Binomial tree of time-step converging to zero: and the C-M-G theorem is regularly applied to the GBM model to change measure such that the drift term changes, whilst the Variance remains the same: I therefore assumed that taking the Binomial tree as a toy example would be safe to achieve a similar result (i.e. changing the measure to eliminate ""drift"", whilst preserving the variance of $X_t$ ). Thank you for any hints and tips.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus', 'radon-nikodym']"
4365848,Computing the Jacobian of $\mathbf{x} \mapsto \mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}}$,"I am trying to compute the following vector-by-vector derivative $$ \frac{\text{d}}{\text{d}\mathbf{x}}\left(\mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}}\right), $$ where $\mathbf{x}$ and $\mathbf{\dot{x}}$ are $n \times 1$ column vectors, $\mathbf{A}$ is a constant $n \times n$ matrix, and $\dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}}$ is a known $n \times n$ matrix. I try to solve this by switching to index notation: $$
\begin{aligned}
\frac{\text{d}}{\text{d}x_m}\left(A_{ij} x_j x_k A_{kl} \dot{x}_l\right)
&= A_{ij} \delta_{jm} x_k A_{kl} \dot{x}_l
+ A_{ij} x_j \delta_{km} A_{kl} \dot{x}_l
+ A_{ij} x_j x_k A_{kl} \frac{\text{d}\dot{x}_l}{\text{d}x_m} \\
&\Rightarrow \mathbf{A} \left(\mathbf{x}^T \mathbf{A} \mathbf{\dot{x}}\right)
+ \mathbf{A} \mathbf{x} \,\left(\mathbf{A} \mathbf{\dot{x}}\right)^T
+ \mathbf{A} \mathbf{x} \mathbf{x}^T \mathbf{A} \dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}}
\end{aligned}
$$ Is this correct? I don't have much experience with this kind of thing. Thank you.","['vector-fields', 'matrices', 'jacobian', 'multivariable-calculus', 'matrix-calculus']"
4365858,Martingales and Markov chain,"Let $(X_n)_{n\geq0}$ be a Markov chain in $ \mathbb{N}$ with following transition probabilities: $$P(k, k+1) = p_k = 1-P(k, k-1), k \geq 1, p_k \in (0,1) $$ $$P(0,1) = p_0 := 1$$ Let $q_k = 1-p_k$ . For which functions $f : \mathbb{N}  \rightarrow \mathbb{R}$ the process $(f(X_n))_{n\geq0}$ is a martingale (for natural filtration)? My reasoning: knowing $X_{n-1}$ we can move either to the right or to the left: $$\mathbb{E}[f(X_n)|\mathcal{F}_{n-1}]=f(X_{n-1}+1)p_{n-1}+f(X_{n-1}-1)(1-p_{n-1})$$ At the same time we want $(f(X_n))_{n\geq0}$ to be a martingale: $$\mathbb{E}[f(X_n)|\mathcal{F}_{n-1}] = f(X_{n-1})$$ Thus we want to solve a functional equation: $\forall k \in \mathbb{N} , \forall p \in (0,1)$ $$f(k+1)p+f(k-1)(1-p)=f(k)$$ Am I right? If yes, it can be solved as $$f(k)=C_1+C_2(\frac{1-p}{p})^k, p\neq\frac{1}{2}$$ $$f(k) = C_1+C_2k, p=1/2$$ And we don't have any conditions to find $C_1, C_2$ . So what's the real solution?","['conditional-expectation', 'markov-chains', 'martingales', 'probability-theory', 'probability']"
4365861,The image from finite set is finite / Axiom of choice,"$
\newcommand{\N}{\mathbb{N}}
$ Definitions : $|A| \le |B|$ if there is an injection from $A$ into $B$ ; $|A| \ge |B|$ if there is a surjection from $A$ onto $B$ ; $|A| = |B|$ if there is a bijection from $A$ onto $B$ . Theorem 1 : Assuming AC, for any set $A$ and $B$ , $|A| \ge |B| \Rightarrow |B| \le |A|$ . I am trying to show the following: Claim : For each nonempty set $X$ and any function $f: X \to Y$ , if $A$ is a subset of $X$ , $$
|A| < |\N| \Rightarrow |f[A]| < |\N|
$$ That is, the image of a finite set is finite. Attempt : Assume $|A| < |\N|$ . Since $f$ is a function, $f|_A\to {f[A]}$ is a surjection, that is, $|A| \ge |f[A]|$ . By Theorem 1, we have $|f[A]| \le |A|$ . Because the composition of injections is an injection, $|f[A]| \le |A| \le |\N|$ . Now, suppose that $|f[A]| = |\N|$ aiming for contradiction. Since the composition of a bijection and surjection is a surjection, $|A| \ge |f[A]| \ge |\N|$ . Hence, by Theorem 1, we have $|\N| \le |A|$ , and by Schroeder-Bernstein theorem, $|A| = |\N|$ , which is a contradiction. Finally, $|f[A]| \le |\N| \land |f[A]| \neq |\N|$ . Questions : Is this valid proof? Do I really need AC to prove the proposition? Since I am dealing with finite sets, it seems that invoking AC is overkill. This question is continued in Hermis14 . Thank you.","['elementary-set-theory', 'cardinals', 'solution-verification', 'axiom-of-choice']"
4365867,"How to find the formula for the integral $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{n}}$, where $n\in N$?","By the generalization in my post ,we are going to evaluate the integral $$\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{n}},$$ where $n\in N.$ First of all, let us define the integral $$I_n(a)=\int_{0}^{\infty} \frac{d x}{\left(x^{2}+a\right)^{n}} \textrm{ for any positive real number }a.$$ Again, we start with $$I_1(a)=\int_{0}^{\infty} \frac{d x}{x^{2}+a}= \left[\frac{1}{\sqrt{a}} \tan ^{-1}\left(\frac{x}{\sqrt{a}}\right)\right]_{0}^{\infty} = \frac{\pi}{2 }a^{-\frac{1}{2} } $$ Then differentiating $I_1(a)$ w.r.t. $a$ by $n-1$ times yields $$
\int_{0}^{\infty} \frac{(-1)^{n-1}(n-1) !}{\left(x^{2}+a\right)^{n}} d x=\frac{\pi}{2} \left(-\frac{1}{2}\right)\left(-\frac{3}{2}\right) \cdots\left(-\frac{2 n-3}{2}\right) a^{-\frac{2 n-1}{2}}
$$ Rearranging and simplifying gives $$
\boxed{\int_{0}^{\infty} \frac{d x}{\left(x^{2}+a\right)^{n}} =\frac{\pi a^{-\frac{2 n-1}{2}}}{2^{n}(n-1) !} \prod_{k=1}^{n-1}(2 k-1)}
$$ Putting $a=1$ gives the formula of our integral $$
\boxed{\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{n}} =\frac{\pi}{2^{n}(n-1) !} \prod_{k=1}^{n-1}(2 k-1)= \frac{\pi}{2^{2 n-1}} \left(\begin{array}{c}
2 n-2 \\
n-1
\end{array}\right)}$$ For verification, let‚Äôs try $$
\begin{aligned}
\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{10}} &= \frac{\pi}{2^{19}}\left(\begin{array}{c}
18 \\
9
\end{array}\right) =\frac{12155 \pi}{131072} ,
\end{aligned}
$$ which is checked by Wolframalpha . Are there any other methods to find the formula?  Alternate methods are warmly welcome. Join me if you are interested in creating more formula for those integrals in the form $$
\int_{c}^{d} \frac{f(x)}{\left(x^{m}+1\right)^{n}} d x.
$$ where $m$ and $n$ are natural numbers.","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4365869,"$n$ draws with replacement, $n$ unique balls but $f_0$ are equal to $0$, $f_1$ are equal to $1$, and $f_2$ are equal to $2$","I have been looking for several days now how to correctly do the combinatrics of this specific problem (which ends up being very useful for a bootstrapping application of mine).
Here is how it goes: There are $n$ unique balls, each labeled with $0$ , $1$ , or $2$ . Among the $n$ balls, $f_0$ balls are labeled with $0$ , $f_1$ balls are labeled with $1$ , and $f_2$ balls are labeled with $2$ , I perform $n$ draws with replacement, and the order of the balls I picked does not matter. In practice, that means if I have two different balls that are equal to $0$ (let's denote them $0_a$ and $0_b$ ), one equal to $1$ and one equal to $2$ , then $n = 4$ and: $0_a 0_a 0_a 0_a$ is different from $0_a 0_b 0_a 0_a$ but $0_a 0_b 0_a 0_a$ is the same as $0_a 0_a 0_b 0_a$ . Let $M$ denote the mean of the sum of the labels. I'm looking for the distribution of $M$ . $$\begin{align}
    \forall k \in \{0, ..., 2*n\}, \;\; \mathbb{P}\left(M = \dfrac{k}{2*n}\right) & = \dfrac{get_k}{total} \\
    \text{where } get_k & = \sum_{i = 0}^{\lfloor k/2 \rfloor} \dfrac{f_0^{(n-k+i)}}{(n-k+i)!}*\dfrac{f_1^{(k - 2i)}}{(k-2i)!}*\dfrac{f_2^i}{i!}\\
    total & = \binom{2n-1}{n}
\end{align}$$ where when $n-k+i < 0$ , the term in the sum of $get_k$ is set to $0$ (happens when we want $k > n$ and we do not consider enough samples with value $2$ ). I am pretty confident the formula for total is correct (total number of different draws that can happen - here it does not matter what value are on the balls) because this is ""textbook"" but i'm pretty sure the $get_k$ formula isn't correct as when I sum it all ( $get_k/total$ ) on Python it doesn't equal to $1$ . I hope i've described the problem quite clearly. I've come up with at least $10$ different formulas for each and I feel like I'm just going circles and I might end up crazy anytime soon. Thanks! PS : hopefully if I draw $k < n$ balls with replacement, it doesn't get MUCH harder. I'd be interested in this after I solve this.","['probability-distributions', 'combinatorics', 'discrete-mathematics']"
4365898,Covariant derivatives and diffeomorphisms,"Suppose $M$ is a Riemannian manifold and $\Phi : M \to M$ is a diffeomorphism. Suppose further that $X$ and $Y$ are smooth vector fields on $M$ , and $\nabla$ is the Levi-Civita connection on $M$ . Given $p \in M$ , consider the covariant derivatives $$
\big(\nabla_X\left(\Phi_* Y\right)\big)_{\Phi(p)} = \nabla_{X_{\Phi(p)}}\left(d\Phi_p Y_p\right) \quad \textrm{and} \quad \big(\Phi_*(\nabla_X Y)\big)_{\Phi(p)} = d\Phi_p\left(\nabla_X Y\right)_p.
$$ My question is basically twofold: Is there a connection between these two covariant derivatives? Is there a canonical definition of the covariant derivative of a diffeomorphism $\Phi : M \to M$ , or the covariant derivative of its differential $d\Phi : TM \to TM$ ? If $M = \mathbb R^n$ , and we write $Y = Y^i \frac{\partial}{\partial x^i}$ and $X = X^i \frac{\partial}{\partial x^i}$ , where $X^i, Y^i \in C^\infty(\mathbb R^n)$ for all $i$ (using the Einstein summation convention), then $$\nabla_X Y = (XY^i) \frac{\partial}{\partial x^i} = \left(X^j \frac{\partial Y^i}{\partial x^j}\right) \frac{\partial}{\partial x^i}.$$ Applying this to the derivatives above, and using the fact that $(\Phi_* Y)_{\Phi(p)} = d\Phi_p Y_p = Y^j(p)\frac{\partial \Phi^i}{\partial x^j}(p)\frac{\partial}{\partial x^i}\big|_{\Phi(p)}$ , with some abuse of notation, we get: \begin{align*}
\nabla_{X}\left(\Phi_* Y\right) &= \left(X^j \frac{\partial}{\partial x^j} \left(Y^k \frac{\partial \Phi^i}{\partial x^k}\right)\right)\frac{\partial}{\partial x^i} \\
&= X^j \frac{\partial Y^k}{\partial x^j} \frac{\partial \Phi^i}{\partial x^k} \frac{\partial}{\partial x^i} + X^j Y^k \frac{\partial^2 \Phi^i}{\partial x^j \partial x^k} \frac{\partial}{\partial x^i} \\
&= \Phi_* \left(\nabla_X Y\right) + (\nabla_X(d\Phi))Y
\end{align*} where formally $\nabla_X(d\Phi) : T_p\mathbb R^n \to T_{\Phi(p)}\mathbb R^n$ is the operator given in coordinates by $$
\nabla_X(d\Phi) = \left(X^j \frac{\partial^2 \Phi^i}{\partial x^j \partial x^k}\right)_{1 \leq i,k\leq n}
$$ which by inspection roughly corresponds to a coordinatewise derivative of the Jacobian matrix of $d\Phi$ in the direction of the vector field $X$ . At a point $p \in \mathbb R^n$ , we can express the equation $\nabla_X \left(\Phi_* Y\right) = \Phi_*(\nabla_X Y) + (\nabla_X(d\Phi))Y$ by writing $$
\nabla_{X_{\Phi(p)}}\left(d\Phi_p Y_p\right) = d\Phi_p\left(\nabla_X Y\right)_p + (\nabla_X(d\Phi))_p Y_p
$$ which seems very similar to the Leibniz ""product rule"". So this all works in $\mathbb R^n$ , but what about more general Riemannian manifolds? Well, formally, given $X, Y$ smooth vector fields and $\Phi : M \to M$ a diffeomorphism, we could simply define $\nabla_X(d\Phi)(Y) = \nabla_X(\Phi_*Y) - \Phi_*(\nabla_X Y)$ , but this seems quite artificial. Is there a more natural/canonical way to define the covariant derivative of a diffeomorphism or its differential?","['connections', 'differential-topology', 'riemannian-geometry', 'differential-geometry']"
4365927,How do I find $\;\lim_{n\to\infty}n\int_{0}^{1} \frac{x^n}{x^2+1} dx\;$?,"I'm new here so please, try to bear my poor formatting skills. Here's the problem : $I_{n}$ is considered a sequence where $n\in N^*$ $$
I_{n} = \int_{0}^{1} \frac{x^n}{x^2+1} dx\ , (\forall)n\in N^*
$$ a) Calculate $I_{2}$ b) Prove that $I_{n+2} + I_{n} = \frac{1}{n+1},(\forall)n\in N^*$ c) Calculate $\lim_{n \to +\infty} nI_{n}$ I have done both a) and b), but I don't exactly know how to solve c). Any help is appreciated !","['integration', 'limits', 'definite-integrals', 'sequences-and-series']"
4365940,If $C(X)$ is finite dimensional then $X$ is finite.,I've seen this link: When is the vector space of continuous functions on a compact Hausdorff space finite dimensional? $X$ is metric and compact. But I don't want to use the Tietz theorem. So how can I prove that if $C(X)$ (the set of all continuous functions from $X\to \mathbb{C}$ ) is finite dimensional then $X$ is finite?,"['general-topology', 'functional-analysis']"
4365967,Prove integral is convex,"$X$ are an iid draw from $(-\infty, \infty)$ according to $F$ with mean $\mu$ .  Further let $A = a(x, \theta)/\cos (\alpha)$ and $B = ((1- \cos(\alpha) - \sin (\alpha))\mu + \sin (\alpha)x)/\cos(\alpha)$ , where $a(x,\theta)$ is determined by \begin{equation}\tag{1}
1 - F(A-B) = f(A-B)A.
\end{equation} It is assumed that that $F$ has an increasing hazard rate, so there is a unique solution.  Note that both $A$ and $B$ are functions of $x$ and $\alpha \in [0,\pi/2]$ . Show that the following integral is convex: \begin{equation}
\int_{-\infty}^{\infty}(1-F(A-B))A \cos(\alpha)dF(x)
\end{equation} Attempt : The regular method of differenting the function twice does not work.  I thought it may have something to do with Brunn-Minkowski theorem","['definite-integrals', 'probability-distributions', 'integral-inequality', 'derivatives', 'convex-analysis']"
4365998,Why is $D(f\circ g)=Df\circ Dg$,"I was reading on Wikipedia about total derivatives of functions and they stated the following about the chain rule for total derivatives: Let $f:\mathbb R^m\to \mathbb R^k$ and $g:\mathbb R^n \to \mathbb R^m$ be two differentiable functions and let $a \in \mathbb R^n$ . Let $D_{g(a)}f$ denote the total derivative of $f$ at $g(a)$ and $D_a g$ denote the total derivative of $g$ at a. Then: $$D_a(f\circ g)=D_{g(a)}f\circ D_a g$$ or, for short: $$D(f\circ g)=Df\circ Dg$$ The thing I'm not understanding is the following: What does $Df\circ Dg$ mean? Those two total derivatives are defined as functions: $Df: \mathbb R^m\to \cal L(\mathbb R^m,\mathbb R^k)$ , and $Dg: \mathbb R^n\to \cal L(\mathbb R^n,\mathbb R^m)$ So how is the composition $D(f\circ g)=Df\circ Dg$ defined? Am I missing something or is this a typo?",['multivariable-calculus']
4366029,There is no bijection between $I_n$ and $\mathbb{N}$.,"$
\newcommand{\N}{\mathbb{N}}
$ Let $I_n = \{i \in \N~|~i \le n\}$ . The claim probably seems trivial to most of you, but I tested myself whether I can prove it. It is sufficient to show that for every $n \in \N$ , there is no surjection from $I_n $ onto $\N$ . I tried proof by induction. base step : Let $I_1 \to \N$ be the set of all functions from $I_1$ to $\N$ . Then, $$
I_1 \to \N = \{\{(1,n)\}~|~ n \in \N\}
$$ Let $f \in I_1 \to \N$ be arbitrary. There is $n \in \N$ such that $f =\{(1,n)\}$ . $f(1) = n$ is the only element in $f[I_1]$ , but $n + 1 \in \N$ , which means $f$ is not surjective. inductive step :
Let $n \in \N$ , and assume there is no surjection in $I_n \to \N$ . Let $f \in I_{n+1} \to \N$ be arbitrary. Aiming for contradiction, let $f$ is a surjection. Then, $g = f \setminus \{(n+1, f(n+1))\}$ is a surjection in $I_n \to \N \setminus \{f(n+1)\}$ . We can define a function $h \in I_n \to \N$ such that $$
h(i)
=
\begin{cases}
g(i), & g(i) < f(n+1)\\
g(i) - 1, & g(i) \ge f(n+1)
\end{cases}
$$ for all $i \in I_n$ . Then, $h$ is a surjection, which contradicts the inductive hypothesis. Questions: Is this a valid proof? Is there a simpler proof?","['elementary-set-theory', 'solution-verification']"
4366077,almost sure convergence via subsequence arguments,"Suppose I have a sequence of random variables $\{X_n\}_{n \in \mathbf{N}}$ such that for every subsequence there exists a further subsequence that converges almost surely to $X$ . Can I prove that $X_n \to X$ almost surely? The confusion I have is that if $X_n$ converges in probability to $X$ , then I have the statement mentioned above. On the other hand, from the real analysis we know one trick to prove convergence of a sequence is via the subsequence argument for deterministic sequences. However, convergence in probability is weaker than the almost sure convergence.","['probability-limit-theorems', 'probability-theory', 'almost-everywhere']"
4366118,Proving $\sum_{i=1}^ni\sin{ix}=\frac{\sin{((n+1)x)}}{4\sin^2{\frac{x}{2}}}-\frac{(n+1)\cos{((2n+1)\frac{x}{2})}}{2\sin{\frac{x}{2}}}$ by induction,"Prove by induction that: $$\sum_{i=1}^ni\sin{ix}=\frac{\sin{((n+1)x)}}{4\sin^2{\frac{x}{2}}}-\frac{(n+1)\cos{((2n+1)\frac{x}{2})}}{2\sin{\frac{x}{2}}}$$ I tried to prove it for the basis $n=1$ : For the left side, it's $\sin{x}$ The right side is: $$A=\frac{\sin(2x)}{4\sin^2{\frac{x}{2}}}-\frac{2\cos{\frac{3x}{2}}}{2\sin{\frac{x}{2}}} = \frac{2\sin{x}\cos{x}-4\sin{x}\cos{\frac{3x}{2}}}{4\sin{\frac{x}{2}}}$$ $$= 2\sin{x}(\frac{\cos{x}-2\cos{\frac{3x}{2}}}{4\sin^2{\frac{x}{2}}})=2\sin{x}(\frac{\cos^2{\frac{x}{2}}}{4\sin^2{\frac{x}{2}}}-\frac{1}{4}-\frac{\cos{\frac{3x}{2}}}{2\sin^2{\frac{x}{2}}})$$ Then I showed that $$\frac{\cos^2{\frac{x}{2}}}{4\sin^2{\frac{x}{2}}}=\frac{1-sin^2{\frac{x}{2}}}{4\sin^2{\frac{x}{2}}}=\frac{1}{4\sin^2{\frac{x}{2}}}-\frac{1}{4}$$ So: $$A=2\sin{x}(\frac{1-2\cos{\frac{3x}{2}}}{4\sin^2{\frac{x}{2}}}-\frac{1}{2})$$ Then I tried to prove that the expression inside the parentheses is equal to $\frac{1}{2}$ i.e. the first fraction is equal to 1 i.e. the numerator is equal to the denominator. But I couldn't.","['trigonometry', 'induction', 'discrete-mathematics', 'sequences-and-series']"
4366119,Riemannian manifold of absolutely continuous functioins,"I¬¥m struggling with a definition of a Riemannian metric proposed in the textbook ""Functional and Shape Data Analysis"" by Anuj Srivastava, Eric P. Klassen: Let $\mathcal{F}$ be the set of absolutely continuous functions $f: [0,1] \rightarrow \mathbb{R}$ and let $\mathcal{F}_0 = \{ f \in \mathcal{F}|\, f'>0 \}$ . Definition: Let $f \in \mathcal{F}_0$ and $v_1, v_2 \in T_{f}(\mathcal{F})$ , where $T_{f}(\mathcal{F})$ is the tangent space of $\mathcal{F}$ at $f$ , the Fisher-Rao Riemannian metric is defined as the inner product \begin{align} 
    \langle\langle v_1, v_2 \rangle \rangle_{f} = \frac{1}{4} \int_0^1 v_1'(t)  v_2'(t) \frac{1}{| f'(t)|}  dt. 
\end{align} My questioin is: How do I check $\mathcal{F}_0$ is a Riemannian manifold in this context? The textbook does not give any comments on this and after several hours of thorough web searching I feel particularly lost.","['functional-analysis', 'riemannian-geometry', 'differential-geometry']"
4366187,First Chern class coincides with degree of divisor without poincare duality or de rham cohomology,"I know there are a lot of references (e.g. Griffiths-Harris page 141), but the issue is that these references always prove the proposition in arbitrary dimensions, using a somewhat contrived calculation involving Stoke's Theorem but I feel like there should be a much simpler explanation in the case of curves. So let $X$ be a projective nonsingular curve over $\mathbb{C}$ . We have the exponential sequence $$ 0 \rightarrow \mathbb{Z} \rightarrow \mathcal{O}_X \rightarrow \mathcal{O}_X^*\rightarrow 0$$ which gives us a long exact sequence in cohomology, and the first Chern Class is given by $H^1(\mathcal{O}_X^*) \rightarrow H^2(\mathbb{Z})\cong \mathbb{Z}$ . Now a Cech 1-cocycle $\{g_{\alpha\beta}\}$ for a sufficiently fine cover of $X$ in $\mathcal{O}_X^*$ encodes the information of a line bundle, and based on what I have read the image of this in $H^2(\mathbb{Z})$ is given by $\{(\log g_{\alpha\beta} - \log g_{\alpha\gamma} + \log g_{\beta\gamma})/ 2\pi i \}$ where the discrepancies in the choice of logarithm give rise to the cohomology class. It seems to me that the poles and zero's of local sections used to define the $g_{\alpha\beta}$ should encode the ""failure"" to produce logarithms, but unfortunately I am lost as to where I should go from here. Any help, including an explanation about why this is doomed to fail will be highly appreciated!","['divisors-algebraic-geometry', 'algebraic-geometry', 'holomorphic-bundles', 'line-bundles']"
4366228,"If $\ (X,d)\ $ is a complete metric space, and $\ A\ $ is a closed, convex subset of $\ X,\ $ then is $\ A\ $ connected?","Transparent note: I edited the question because it wasn't originally what I intended it to be. If $\ (X,d)\ $ is a complete metric space, and $\ A\ $ is a closed, convex
subset of $\ X,\ $ then is $\ A\ $ connected? Definition of convex metric space : $(A,d)$ is convex if \begin{multline*}
(a_1, a_2 \in A \text{ and } a_1 \ne a_2) \implies \\
\exists\ y\in A\setminus\{a_1,a_2\} \text{ such that } d(a_1, y) + d(y,a_2)= d(a_1, a_2).
\end{multline*} We require $\ A\ $ to be closed because if $\ X=\mathbb{R},\ $ then $\ A = [0,1) \cup (2,3]\ $ is not closed, is convex but is not connected. I'm not even sure I can prove this if $\ X = \mathbb{R},\ $ although I do know that the closed subsets of $\ \mathbb{R}\ $ are the unions of Cantor sets and closed intervals. But I was actually hoping for a more general, possibly topological approach to the question anyway.... Edit: Here it says that a convex set is always star convex, implying pathwise-connected, which in turn implies connected. Edit: I have a further question: is $\ A\ $ complete? And I am also not sure if, more generally, a closed connected set in a m.s. is complete? Clearly though, closed and complete does not imply connected, e.g. $ [0,1]\cup [2,3].$ Edit: DanielWainfleet answered this in the comments. The answer is no, e.g. $\ X = A = (0,1]\ $ so yes, the underlying metric space is important.","['general-topology', 'convex-analysis', 'metric-spaces', 'connectedness']"
4366263,Evans Representation formula for solution of Poissons equation,"In Evans PDE, Chapter 2 Theorem 9 we have: Let $f \in C^2_c(\mathbb{R}^n)$ , $n \geq 3$ . Then any bounded solution of $$-\Delta u = f$$ in $\mathbb{R}^n$ has the form $$u(x)  = \int_{\mathbb{R}^n} \Phi(x-y)f(y)dy +C.$$ Then there's a remark: If $n=2$ , $\Phi(x) = - \frac{1}{2 \pi} \log|x|$ is unbounded as $|x| \rightarrow \infty$ , and so may be $\int_{\mathbb{R}^n} \Phi(x-y)f(y)dy. $ My question is: If $f$ has compact support then let the compact set $\overline{V}$ be the support of $f$ . Then the integral becomes $\int_V \Phi(x-y)f(y)dy. $ Now since $\Phi f$ is continuous away from zero it achieves its $\max$ on $V$ , so the integral is finite? Meaning we shouldn't look at large values for $x$ , because then we will be outside of $V$ in which case the integral will be zero. What am I missing? EDIT: see the comment by cmk which answers this.","['analysis', 'partial-differential-equations']"
4366293,"Whether the partial derivatives are bounded in any neighbourhood of $(0,0)$ or not.","The function is given as $f(x,y)=\begin{cases}(x^{2}+y^{2})\sin \left(\frac{1}{x^{2}+y^{2}}\right)& (x,y)\ne (0,0)\\0 &(x,y)=(0,0)\end{cases}$ . The options are (a) The partial derivatives $\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}$ exist at $(0,0)$ but are unbounded in any neighbourhood of $(0,0)$ . (b) $f$ is continuous but not differential at $(0,0)$ . (c) $f$ is differential at $(0,0)$ . (d) $f$ is continuous but not differrential at $(0,0)$ I computed the partial derivative as, $$\begin{aligned}\frac{\partial f}{\partial x}\Big|_{(0,0)}&=\lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{h^{2}\sin \frac{1}{h^{2}}-0}{h}\\&=\lim_{h\to 0} h\sin \frac{1}{h^{2}}\\&=0\end{aligned}$$ Similarly, $\frac{\partial f}{\partial y}\Big|_{(0,0)}$ can be shown equal to 0. Since the partial derivatives both exits and equals zero. So are continuous, hence $f$ is differential at $(0,0)$ and hence continuous too. But how to check whether the partial derivatives are bounded in any neighbourhood of zero or not? Thanks in advance!","['partial-derivative', 'continuity', 'derivatives']"
4366303,Solve linear system of ODE's,"It's been years since I formally saw ODE's and I am quite rusty, I don;t remember how to solve linear ODE's. I have a problem and managed to derive the following system: \begin{cases}
	2\ddot{q_1} = 2q_1 + q_2\\
	2\ddot{q_2} = q_1
\end{cases} where $q_1$ and $q_2$ are scalar functions with parameter $t$ . It seems to me the solution is a trig function but I am not sure how to go about this problem.","['calculus', 'systems-of-equations', 'ordinary-differential-equations']"
4366312,Height of the hill,"The angles of elevation of the top of a distant hill in the forest as seen from three consecutive km stones on a straight horizontal road are 30, 45 and 60 degrees. Find the height of the hill.\ My try: Let km stone C,B, A are at distances $x,x+1,x+2$ from base of hill $OP$ Then $\tan 60=\frac{h}{x}$ $\tan45=\frac{h}{x+1}$ $\tan 30=\frac{h}{x+2}$ I solved and got $h=\frac{3+\sqrt3}{2}$ but answer is $\sqrt{\frac{3}{2}}$","['trigonometry', 'geometry']"
4366385,Solving the system $\tan x + \tan y = 1$ and $\cos x \cdot \sin y = \frac{\sqrt{2}}{2}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How can I solve this system of trigonometric equations: $$\tan x + \tan y = 1$$ $$\cos x \cdot \sin y = \frac{\sqrt{2}}{2}$$ I tried to write tangent as $\sin/\cos$ and then multiply the first equation with the second one but it is not that brought me in a right way. Do you have any idea how to solve this one?","['trigonometry', 'systems-of-equations']"
4366387,Question on openness in the topology generated by a basis,"As some context, a question I posted Munkres exercise 13.1 was closed as a duplicate, but I'm not interested just in solving Munkres 13.1, but in a verification of whether my understanding of a basis for a topology is correct. I'm going to try to reduce the scope of my question here in the hope that this isn't closed, because I accept there are other, and most likely superior ways to solve this problem. But I'd like to know if my intuition was correct. The exercise is: Let ùëã be a topological space; let $A$ be a subset of $X$ . Suppose that for each $x \in A$ there is an open set $U$ containing $x$ such that $U \subset A$ . Show that $A$ is open in $X$ . I know that a set is open if and only if it is a union of open sets, and this is the approach people typically used for this problem. My approach, and thought process, was to use the definition of what it means to be open in the topology generated by a basis (though whether it is ideal to involve a basis is, of course, debatable). If $\mathcal{T}$ is the topology generated by a basis $\mathcal{B}$ , we say $A \in \mathcal{T}$ if and only if for every $x \in A$ , there exists $B \in \mathcal{B}$ such that $x \in B \subset A$ . So this is what I set out to prove. If I fix $x \in A$ , I know by the assumption in this exercise that I can pick $U \in \mathcal{T}$ , where $x \in U$ and $U \subset A$ . As $U$ is open, I can apply the above definition to find a basis element $B \in \mathcal{B}$ such that $x \in B \subset U$ . But $U \subset A$ , so I can say $x \in B \subset A$ . By this above definition again, $A$ is open. My question is whether this approach here and my understanding of the basis for a topology is correct, or if I have made any mathematical errors. I surely would have to start out with ""choose a basis for $\mathcal{T}$ ,"" which could just be $\mathcal{T}$ itself. This is certainly not ideal, but is it incorrect?","['general-topology', 'solution-verification']"
4366388,"a combination problem about ""find the largest k""","For each integer $n\ge7$ , find the largest $k\in\mathbb{N_+}$ for which there exist $k$ subsets $M_1,M_2,\cdots,M_k$ of $M=\{1,2,\cdots,n\}$ ,
such that $\left|M_i\right|\ge3$ and $\left|M_i\cap M_j\right|=1$ for all $1\leq i<j\leq k$ . I've proved that if $n=7, k_\max=7$ (quite easy), but I don't have any idea how to solve the question above. So could you, please, give me some ideas? proof for $n=7:$ Consider a graph $G$ with 7 nodes(each node represents an element in $M$ ). Join two nodes together when the two elements they represent are in the same subset. It's easy to find that there is at most 1 edge between every 2 nodes. A $K_m$ subgraph of $G$ represents an $m$ -element subset of $M$ . So each subset costs at least 3 edges, while there are at most $
\left( \begin{array}{c}
	7\\
	2\\
\end{array} \right) $ edges in the graph. So we have $$k‚â§\frac{
\left( \begin{array}{c}
	7\\
	2\\
\end{array} \right) 
}{3}=7$$ example: $M_1=\{1,2,3\};M_2=\{1,4,5\};M_3=\{1,6,7\};\\M_4=\{2,4,6\};M_5=\{2,5,7\};M_6=\{3,4,7\};M_7=\{3,5,6\}$ .","['elementary-set-theory', 'combinations']"
4366402,Can we recover the adjacency matrix of a graph from its square?,"For the sake of this question, a graph here has a finite number of vertices, with undirected simple edges, no loop, and no weight or label on edges or vertices. Therefore, its adjacency matrix $A=[a_{i,j}]$ is a symmetric matrix with entries in $\{0,1\}$ , and $0$ on the main diagonal. Assuming that $A^2$ is known, can we recover $A$ ? The question comes from my self-study of graph theory and I have no idea on how to solve this question. What I know: Let's label the vertices of the graph as $v_1$ , $v_2$ , $\dots$ , $v_n$ so that $a_{i,j}=1$ if there is an edge between $v_i$ and $v_j$ , and $0$ otherwise. Then, if $a_{i,j}^{(k)}$ is the entry of $A^k$ at row $i$ and column $j$ , $a_{i,j}^{(k)}$ is the number of walks of length $k$ between $v_i$ and $v_j$ . Therefore, what is known in the problem are the numbers $a_{i,j}^{(2)}$ of common neighbors between $v_i$ and $v_j$ . If the $i$ th row of $A^2$ is composed of $0$ s, then $v_i$ is an isolated point of the graph. (If $v_i$ is not isolated, then there is a walk $v_i-v_j-v_i$ so $a_{i,i}^{(2)}\ge 1$ ). So we can simplify the problem by assuming that the graph has no isolated point. $a_{i,i}^{(2)}=1$ is equivalent to $\deg(v_i)=1$ (end vertex). There are results about square roots of positive semi-definite matrices, but $A^2$ is not positive semi-definite, and the square root would not have its entries in $\{0,1\}$ . For $n=3$ , by looking at the squares of the adjacency matrices of the few possible graphs with $3$ vertices, the answer is yes. In my question, I assume that it is known that $S=A^2$ is the square of the adjacency matrix of a graph and I wonder if there is another graph whose adjacency matrix has also $S$ for square. So a reformulation of the question is Does it exist two adjacency matrices $A$ and $B$ (as defined in the first paragraph) such that $A^2=B^2$ ?","['matrices', 'graph-theory']"
4366558,How to compare asymptotic efficiency of MOM estimator with MLE?,"Compare the asymptotic efficiency of MOM estimator of parameter $\alpha$ of the pareto distributions with MLE (assume $X_m$ known) $$f(x;\alpha;X_m) = \alpha X_m^{\alpha} x^{-(\alpha + 1)}.$$ I computed the MLE of the pareto distribution equating to $0$ the first derivative of the log-likelihood getting $$\frac{n}{n\log(X_m)+\sum_{i=0}^n \log(X_i)}.$$ And for MOM I get $$\frac{\bar X}{\bar X- X_m}.$$ My question is: In order to compare the asymptotic efficiency should I compute the variance of the Mom and the variance of MLE?
If so someone can direct me in the right direction?","['statistics', 'maximum-likelihood']"
4366616,$\int_{-\infty}^{\infty}\frac{1}{3 t^2+1} {\rm exp}\left(\frac{at^2+ibt}{3 t^2+1}+itx\right){\rm d}t$,"How to solve the integral? $$
f(x)=\int_{-\infty}^{\infty}\frac{1}{3 t^2+1} {\rm exp}\left(\frac{a t^2+i b t}{3 t^2+1}+itx\right){\rm d}t\tag{1}
\\{\rm with}\,\, x,b\in \mathbb{R},a\in\mathbb{R}_{<0},i^2=-1$$ This question was previously posted in Mathematica SE, however algorithms of Mathematica couldn't solve it. A deeper mathematical analysis is needed. In the solution a potpourri of Error function, Bessel function, hypergeometric function or infinite series of them can be expected (see these solved similar integrals: integral1 , integral2 , integral3 ). A plot of $f(x)$ for $a=-200,b=-100$ :","['integration', 'special-functions', 'bessel-functions', 'error-function', 'hypergeometric-function']"
4366655,"Space $W$ of null homotopic maps $g \in Hom(S^1, S^1)$ with $g(1)=1$ is contractible.","This is the claim the proof of which I can't fully understand. In my question $S^1$ is defined as subset of complex numbers of unit magnitude.
The proof goes as follows: We can obtain a bijection $V \to W$ , where $V$ is a vector subspace (and thus contractible) of $Hom(S^1, \mathbb{R})$ of functions $f$ with $f(1)=0$ . Bijection is obtained by composing with the function $p: \mathbb{R} \to S^1; x \mapsto \exp(2\pi i x)$ . Function that composes with $p$ on left is denoted as $p_*$ . We want to show that this bijection is open, that is, a homeomorphism. Since it is also a group homomorphism, it suffices to check that an open neighborhood of a neutral element is mapped to an open neighborhood of a neutral element. It is enough to consider sets of form $C(K, U)$ , where $K \subseteq S^1$ is compact and $U \subseteq (-1, 1)$ is an open neighborhood of $0$ . Since $p$ is open, then $p(U) \subseteq S^{-1} \setminus -1$ (why?) is open in $S^1$ . It holds that $p_* C(K, U) = C(K, p(U))$ because of the initial conditions , and it is open. $C(K, U)$ denotes a subbasic open set in compact-open topology. $Hom(S^1, \mathbb{R})$ is a group with pointwise addition, and $Hom(S^1, S^1)$ is a group with pointwise multiplication. Neutral elements in each are constant maps to $0$ and $1$ respectively. I can also prove $W$ is a subgroup of $Hom(S^1, S^1)$ . $p_*$ is continuous because domains are locally compact. Firstly, I don't understand why we only consider $U \subseteq (-1, 1)$ . I also think that the claim that $p(U) \subseteq S^{-1} \setminus -1$ is false, because for it to be true we'd need to have $U \subseteq (-1/2, 1/2)$ . But then we don't consider all possible neighborhoods, and so it doesn't prove that all open sets are mapped to open sets. I also don't get why we need the condition $p(U) \subseteq S^{-1} \setminus -1$ at all. Isn't it enough to just cite that $p$ is open so that $p_* C(K, U) = C(K, p(U))$ is open for an arbitrary $U$ ? And what could possibly initial conditions mean here? Secondly, I'm not really sure why $p_*$ defines a bijection. I think I can argue that null homotopic maps can be lifted, which shows surjectivity, and also that lifts of the same function on differ by an integer constant, so there is only one of them with $f(1)=0$ . Is it enough? By lifting a function $g: X \to S^1$ I mean finding a function $f: X\to \mathbb{R}$ such that $pf=g$ . Two lifts differ by an integer constant because $pf = pf^{\prime}$ implies $pf/pf^{\prime}=1$ , which says that $\exp(2\pi i(f - f^{\prime}))=1$ . I will now write a (special case of) Homotopy lifting property from my textbook that I use for my argument. Let $X$ be a connected topological space, $f: X\to \mathbb{R}$ continuous function and $H: I \times X \to S^1$ a homotopy that starts with $pf$ . Then there is a unique homotopy $F: I\times X \to \mathbb{R}$ that starts with $f$ and $pF = H$ . I used it to say that null homotopic maps can be lifted.
Can it be used to show that $p_*$ is bijective more directly?","['proof-explanation', 'general-topology', 'solution-verification', 'algebraic-topology']"
4366667,Use contour integration to calculate $\int_{-\infty}^{+\infty} \frac{\sin^2(x)}{x^2+a^2}dx$,"Use contour integration to calculate the integral $\int_{-\infty}^{+\infty} \frac{\sin^2(x)}{x^2+a^2}dx$ with $a>0$ For this I have a solution (where they expand $\sin^2(x) = \frac{1}{2}(1-\cos(2x))$ and then put $\sin^2(x) = \frac{1}{2}\Re(1-e^{2it})$ . But when I first attempted it I tried it with the function $$f(z) = \frac{\sin^2(z)}{z^2+a^2}.$$ I know that the poles are $+ia,-ia$ . Consider a contour integral which is a halfcircle on the upperhalf plane, $\gamma_R = [-R,R] \cup C_R$ . With the residue formula $\int_{\gamma_R}f(z) dz$ gives me $\frac{\pi}{a}(\sinh(a))^2$ . But I'm having troubles with getting $\int_{C_R} f(z)dz \leq 0$ .
I first tried this \begin{align}
\big|\int_{C_R} f(z)dz\big| &\leq \int_{C_R}\big|\frac{\sin^2(x)}{x^2+a^2}\big|dz\\
&\leq \int_{C_R}\big|\frac{1}{x^2+a^2}\big|dz
\end{align} But apparently $sin(z)\nleq 1$ for $z \in \mathbb{C}$ . My question is, is it possible to find an estimate with this complex function so that I can get $\int_{C_R} f(z)dz \leq 0$ ? Or do I have to use the formula $\sin^2(x) = \frac{1}{2}(1-\cos(2x))$ ?","['complex-analysis', 'contour-integration', 'complex-numbers', 'complex-integration']"
4366670,PDE: A priori estimate,"Let $\Omega \subset \textbf{R}^N$ be open and bounded. Let $f \in L^2(\Omega)$ and let $u \in H^1(\Omega)$ be a weak solution of the equation \begin{align}
Lu \equiv - \sum_{i,j=1}^n D_i(a_{ij}D_ju) = f. \tag{1}
\end{align} This is defined by \begin{align}
    \int_{\Omega} \sum_{i,j=1}^n a_{ij}D_i u D_j v ~dx = \int_{\Omega}fv~dx
    \tag{2}
\end{align} for all $v \in H_0^1(\Omega)$ . Suppose that there is a constant $\Theta > 0$ such that for all $x \in \Omega$ \begin{align}
    \sqrt{
    \sum_{i,j = 1}^n (a_{ij}(x))^2
    }
    \leq \Theta.
    \tag{3}
\end{align} By the Cauchy-Schwarz inequality in $\textbf{R}^n$ this also implies that \begin{equation}
    \sum_{i,j=1}^n a_{ij}(x) \zeta_i \xi_j \leq \Theta |\zeta| |\xi|
    \tag{4}
\end{equation} for all $x \in \Omega$ and all $\zeta, \xi$ in $\textbf{R}^n$ .
Let us furthermore assume that there is a constant $\theta > 0$ s.t. \begin{align}
    \sum_{i,j = 1}^n a_{ij}(x)\xi_i \xi_j \geq \theta |\xi|^2
    \tag{5}
\end{align} for all $x \in \Omega$ and $\xi \in \textbf{R}^n$ . Let $\eta \in C^1_c(\Omega)$ be arbitrary. Prove the following a priori estimate:
There holds \begin{align}
    \int_{\Omega}|Du|^2\eta^2dx 
    \leq C_0 
    \left (
    \int_{\Omega}u^2(|D\eta|^2 + \eta^2)dx +
    \int_U f^2\eta^2 dx
    \right )
    \tag{*}
\end{align} where $C_0$ depends only on $\theta$ and $\Theta$ . The first step here is to use $v = u \eta^2$ and plug it into the definition of the weak solution which is stated in equation $(2)$ . The next step would be to use Cauchy-Schwarz and Peter-Paul together
with the above conditions on $(a_{ij})$ , but this is the point where im currently stuck.
I would be really happy if someone could help me out! Cheers, Pinch","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4366690,Are there infinitely many $n$ s.t. $\prod_i^n a_i = \sum^n_i a_i$ only if the number of $1$ in vector $(a_i)_i \in \mathbb{N}^n$ is $n-2$?,"For $n \geq 4$ , let $$A_{n} = \left\{ \mathbf{a} = (a_{1}, a_{2}, \ldots, a_{n}) \in \mathbb{N}^{n} : \prod_{i}^{n} a_{i} = \sum_{i}^{n} a_{i} \right\}$$ and for a vector $\mathbf{a} \in \mathbb{N}^{n}$ , let $m(\mathbf{a})$ be the number of $1$ in $\mathbf{a}$ .
It is clear that $(n, 2, 1_{1}, 1_{2}, \ldots, 1_{n-2}) \in A_{n}$ which implies that $A_{n} \neq \varnothing$ and there exists a vector $\mathbf{a} \in A_{n}$ such that $m(\mathbf{a}) = n - 2$ .
Besides, for each vector $\mathbf{a} \in A_{n}$ , $m(\mathbf{a}) \leq n-2$ holds. There exists some $n \geq 4$ such that for every vector $\mathbf{a} \in A_{n}$ , $m(\mathbf{a})$ is exactly $n - 2$ .
After my computing, the solution for $n \leq 1000$ with at most $m$ integers greater than $1$ is presented as follow: Specifically, such $n \leq 1000$ satisfying that the number of $1$ in the vector $\mathbf{a} \in A_{n}$ is exactly $n - 2$ is listed as follow:
4, 6, 7, 9, 10, 15, 16, 22, 24, 31, 34, 36, 49, 66, 76, 91, 97, 112, 114, 126, 142, 174, 210, 231, 330, 442, 444, 664, 714, 780, 784, 966. My question is whether there are infinitely many $n \geq 4$ such that for every vector $\mathbf{a} \in A_{n}$ , the number of $1$ in $\mathbf{a}$ is exactly $n - 2$ . PS: Let $B_{l} = \{ n : \forall \mathbf{a} \in A_{n}, m(\mathbf{a}) \geq n - l \}$ . Actually, I what to know whether the size of $B_{2}$ is infinite.
It is welcome if you have an idea about the weak version of this problem: Determine whether the size of $B_{l}$ is infinite for some constant $l$ .","['number-theory', 'combinatorics']"
4366701,Prove some intermediate curve hits the point,"Consider two simple smooth curves in $\mathbb{R}^2$ parametrised by $f:[0,1]\rightarrow \mathbb{R}^2$ and $g:[0,1]\rightarrow \mathbb{R}^2$ and with $f(0)=g(0)$ and $f(1)=g(1)$ . How can we prove that a point $x\in\mathbb{R}^2$ ""inside the curves"" (the black dot below) is hit by some intermediate curve $H(s,\cdot) = sg+(1-s)f$ ? My own efforts : I think it is hard to formalise the question: one could characterise $x$ by its winding number with respect to the curve $f\circ(-g)$ as this would be 1 (or -1 depending on the orientation). Let us denote this $wnd(x,f\circ(-g))$ . Then $F(s)=\int_{\mathbb{R}^2} wnd(x,H(s,\cdot)\circ(-g))dx$ is a continuous function wrt. $s$ . Furthermore, it is $F(1)=0$ and equal to the area between $f$ and $g$ for $F(0)=0$ . So how could $wnd(x,H(s,\cdot)\circ(-g))$ change for some $s=s_0$ without there being a $t_0$ such that $H(s_0,t_0)=x$ . Another possibility is to use the ""crossing rule"" somehow see this post : the fact that two points on adjacent connected components of the plane (without the curve) differ in winding numbers by exactly 1. However, here we are dealing with a moving curve not a moving point.","['curves', 'homotopy-theory', 'geometry']"
4366702,"How to get $ E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*) $?","For a Rademacher variable $P(\epsilon=+1)=P(\epsilon=-1)=1/2$ and a zero-mean r.v. such $X\in [a,b]$ , let $X'$ be an independent copy of $X$ . Then we know that $X-X'=_d \epsilon(X-X')$ . But why do we have the following symmetrization trick? $$
E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*)
$$ My questions are (1) Waht does notation $E_{X,X'}(\cdot)$ mean? Does it mean $E_{X}(E_{X'}(\cdot))$ ? (2) It seems that ( ) tells us $$
E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=e^{\lambda(X-X')}? 
$$ But $$
E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=\frac{1}{2}e^{\lambda(X-X')}-\frac{1}{2}e^{-\lambda(X-X')}
$$ They are not equal... So how to get ( )?","['statistics', 'analysis', 'probability']"
4366725,Uniform convergence of length of a curve,"Let $\gamma_n : [a,b] \to \Bbb R^k$ a sequence of curves that converges uniformly to a curve $\gamma : [a,b] \to \Bbb R^k$ . Do the lengths $l(\gamma_n)$ converge to the length $\gamma$ ? The answer is no with the counterexample $\gamma_n:[0,2\pi] \to \Bbb R^2$ , $\gamma_n(x)=(x,\frac1n sin(n x))$ so that it converges to the null function on $[0,2\pi]$ but $l(\gamma_n)> 2\pi=l(\gamma)$ . This seems counterintuitive to me and I would have answered yes at 100%. When I tried to prove it I was stuck at trying to show that $$\int_a^b \lVert \dot{\gamma_n(t)}- \dot{\gamma(t)} \rVert dt < \epsilon$$ which has nothing to do with $\lVert {\gamma_n(t)}- {\gamma(t)} \rVert dt < \epsilon$ for $n$ large enough. Can someone explain intuitively why does this length does not go to $0$ ? Is it because it has too many oscillations when $n$ goes to $\infty$ ? If the oscillations are barely getting away from $y=0$ , the length should not change a lot...","['convergence-divergence', 'arc-length', 'geometry', 'uniform-convergence']"
4366766,classification of rational and irrational via one-nearest neighbor,"Consider the binary classification setting, where the input space is $[0,1]$ and the labels are $\{0,1\}$ Assume the distribution on the input-label $(X,Y)$ pair is as follows. Let $\Pr(Y=0)=\Pr(Y=1)=\frac{1}{2}$ .  If $Y=0$ , then the distribution of $X$ is uniform on $[0,1]$ . Then, if $Y=1$ , the support of the distribution of $X$ is rational numbers in $[0,1]$ such that every rational number has a positive probability. Let $S_n=\{(X_1,Y_1),\dots,(X_n,Y_n)\}$ denotes $n$ -iid samples from this distribution. Fix an irrational $x \in [0,1]$ . Consider the event $E_n$ which is the nearest point ( in usual distance) in $S_n$ to $x$ is a rational. We want to show that $\lim_{n\to \infty} \Pr(E_n)=0$ . Note that the distribution on the input is fixed and does not change with $n$ . This is Problem 5.38 from  Probabilistic Theory of Pattern Recognition by Devroye, Gy√∂rfi, and Lugosi.","['machine-learning', 'measure-theory', 'probability-limit-theorems', 'probability-theory']"
4366778,What are the morphisms in the 'category of CW-pairs'?,"A CW-pair consists of a CW complex $X$ (with cell decomposition $\mathcal{E}\equiv\{e_\alpha\}_{\alpha\in I}$ ) together with one of its subcomplexes $A$ (a closed subspace of $X$ consisting of a union of cells in $\mathcal{E}$ ; this itself forms a CW complex). In the definition of a generalized cohomology theory in the Wikipedia article Cohomology , it mentions contravariant functors $h^i$ from the category of CW-pairs to the category of abelian groups. But what are the morphisms in the category of CW-pairs? The Wiki on the Cellular Approximation Theorem says that a 'map' between CW pairs $(X,A)$ and $(Y,B)$ is a (presumably continuous) function $f:X\to Y$ with $f(A)\subseteq B$ . But this does not seem like a strong enough condition to preserve the cell structure. For example, there is no mention that cells in $X$ are mapped into unique cells in $Y$ , or even a finite or countable union of them. Please enlighten me and I apologise in advance for my naivety.","['category-theory', 'definition', 'cw-complexes', 'general-topology', 'algebraic-topology']"
4366791,What is the correct way to read $f\circ g$?,"Let $f : X \to Y$ and $g : Y \to Z$ be functions. We define the composition $g \circ f : X \to Z$ by $g \circ f(x) = g(f(x))$ for each $x \in X$. I have also heard the composition read out like this: ""The composition of $f$ with $g$ is . . ."" ""Consider the function $g$ composed with $f$ , given by . . ."" ""The function $g$ of $f$ is . . ."" Is this an appropriate way to speak of $g \circ f$? It sometimes happens that I (or my teachers) reverse the order of $f$ and $g$ when describing $g \circ f$ in any of the above ways. Surely, it can't be that both ways are correct. It doesn't cause confusion because the function being talked about is quite straightforward. But I'm still interested in knowing what the ""correct"" way/s to describe $g \circ f$ is/are among the above. I know that some people prefer the functional notation that operates the other way, but this question is not about that scenario.","['functions', 'function-and-relation-composition', 'terminology']"
4366808,How to interpret the constant of integration/endpoints of the sum of derivatives linear operator,"Consider the linear operator expression $$L[f] = f + f' + f'' + f''' + f'''' + ... $$ This doesn't always converge but for some expressions it does converge (such as polynomials or linear combinations of exponentials $e^{ax}$ where $|a| < 1$ ) A natural question to ask is if this operator has a closed form. Using a technique almost identical to the proof of the geometric series combined with integration factors we can see that $$ L[f] - f = \frac{d}{dx}[L[f]] $$ $$ \frac{d}{dx}[L[f]] - L[f] = -f $$ $$  e^{-x} \frac{d}{dx}[L[f]] - e^{-x} L[f] = -e^{-x} f $$ $$ e^{-x} L[f] = - \int e^{-x} f dx $$ $$ L[f] = -e^x \int e^{-x} f dx $$ So this ""closed form"" is not exactly well defined, theres an indefinite integral in there which means it varies by an arbitrary constant. Now if you take a nice polynomial (ex: $x^2$ ) And apply this formula without even understanding it you can end up with a correct answer: $$ L[x^2] = x^2 + 2x + 2 $$ $$ -e^x \int e^{-x} x^2 dx  = -e^x \left( e^{-x} \left( x^2 + 2x + 2 \right) + C \right)$$ And now we just ""set C to 0"" and voila we have perfect agreement. And so while it possible to ""experimentally"" check this formula works, setting C to 0 makes absolutely no sense in the general context of functions. How do you know what C is supposed to be 0? In this case setting $C$ to is 0 is equivalent to saying $f(0)=2$ and how on earth did you know that $f(0)=2$ and not $3$ or something else in general.  You need some initial conditions/ideas on where your bounds start/end. Is there a way to make this formula into something more explicit / rigorous by either defining what the indefinite integral's bounds should be OR by describing a procedure (maybe its not guaranteed to halt in general) that can tell you what value of C is the right value? A possible strategy: If $f(x)$ has an asymptote as $x \rightarrow \pm \infty$ then perhaps the correct $C$ value is one that lets one of these asymptotic values be $0$ . But this is such a frankenstein arbitrary kind of rule. It can be codified as, select $c$ such that $$ \lim_{x \rightarrow \infty} \int_{c}^{x} e^{-x}f(x) dx  = 0$$ Then that is your integration bound OR if you prefer the language of constants then your additive constant $C = -e^{-c}f(c)$","['indefinite-integrals', 'operator-theory', 'functional-analysis', 'ordinary-differential-equations']"
4366811,A function associates something with something,"Let $f \colon A \to B$ be a function from a set $A$ to a set $B$ . In English, which of the following expressions is correct? The function $f$ associates an element of $A$ with (or to) an element of $B$ ; The function $f$ associates an element of $B$ with (or to) an element of $A$ . (I am not using quantifiers like ""every"" or ""some"" on purpose, because in natural language they can create some ambiguity) In the literature and online, I have found both kinds of expression, but I am not sure if they are both correct, since I am not a native English-speaker.
For instance, Wikipedia's page about function in mathematics uses both kinds of expression. Disclaimer . My question is more about English than mathematics, but it requires a basic knowledge in mathematics to be answered, this is why I post it here and not in other Q&A forums.","['elementary-set-theory', 'definition', 'functions', 'terminology']"
4366828,"Why are fully faithful functors conservative? (Or, why are isomorphisms reflected?)","Say $F$ is a functor from category $C$ to $D$ . By ""fully faithful"", I mean $f \mapsto Ff$ is injective (""faithful"") and surjective (""full"") in $C(X, Y) \to D(FX, FY)$ . My question is: when $F$ is fully faithful, why does $FX \cong FY$ imply that $X \cong Y$ for objects $X, Y$ in $C$ ? The best I am able to come up with is: Since $FX \cong FY$ , there exists an isomorphism $h \in D(FX, FY)$ . And, because $F$ is full, there exists a morphism $f \in C(X, Y)$ such that $F f = h$ , and also a morphism $g \in C(Y, X)$ such that $F g$ is the inverse of $h$ . but... I don't know how to prove that $f \in C(X, Y)$ is an isomorphism. I think I need to show that $f g = \text{id}_Y$ and $g f = \text{id}_X$ , but I don't know how. Notation background: I'm using the text Bradley, T. D., Bryson, T., & Terilla, J. (2020). Topology: A Categorical Approach. MIT Press.","['general-topology', 'category-theory']"
4366889,Find upper bound for $P(X=0)$,"I'm given the bounded Markovian stochastic process $-n\leq X(t)\leq n,\, \langle X(t)\rangle=0$ and try to find an upper bound for the probability that the difference between $X(t)$ and the inital value $X(0)$ up to time $T\to\infty$ is zero, i.e. the value remained unchanged until $T\to\infty$ , ideally in terms of just the expected value of $X(t)$ : $$\operatorname{P}\left(\lim_{T\to\infty}\frac{1}{T}\int_{0}^Tdt\,\bigl(X(t)-X(0)\bigr)^2=0\right)\leq F\bigl(\langle X(t)\rangle\bigr).$$ I tried to apply well-known inequalities like Markov's or Chebychev's, but non are really valid in my case. Alternatively the probability can equivalently expressed as $$\operatorname{P}\left(\lim_{T\to\infty}\frac{1}{T}\int_{0}^Tdt\,\langle X(0)X(t)\rangle=1\right)\leq F\bigl(\langle X(0)X(t)\rangle\bigr)$$ so stated instead in terms of the autocorrelation. Additional thoughts: In some sense this probablility seems similar to the probability of fixation from theoretical biology $$\operatorname{P}\left(X=1,t\to\infty\mid X=X_0,t=0\right)$$ which let's me question if there is an underlying Fokker-Planck equation to the problem More additional stuff: One obvious statement about the probability is that it's upper bounded by the argument itself: $$\phi=\operatorname{P}\left(\lim_{T\to\infty}\frac{1}{T}\int_{0}^Tdt\,\langle X(0)X(t)\rangle=1\right)\leq \lim_{t\to\infty}\langle X(0)X(t)\rangle$$ because we can write the RHS as $$\lim_{t\to\infty}\langle X(0)X(t)\rangle=\phi+(1-\phi)A,$$ where $A$ is zero if $X(t)$ becomes decorrelated or some finite number if not. But this bound is rather loose and I wondered if there's a better one.","['markov-process', 'probability-theory', 'upper-lower-bounds']"
4366903,Solving $\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$,"I've been asked to solve the limit. $$\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$$ Here's my approach: $$\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$$ Using the identity, $\cos(x) =\sin(90^{\circ} - x)$ \begin{aligned}\implies \lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))} 
& = \lim_{x\to0}\frac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}
\\& = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)\cdot\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}{\sin(\sin(x^2))}
\\ & = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}\cdot \underbrace{\lim_{x\to0}\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}_{1}
\\ & = \dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(\sin(x^2))}{\sin(x^2)}}_1\cdot\sin(x^2)}
\\ & =\dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(x^2)}{x^2}}_1\cdot x^2}
\\ & =  \color{blue}{\boxed{\lim\limits_{x\to0}\dfrac{\pi}{2x^2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}}
\end{aligned} Now, I'm unable to think of anything to do with this boxed part. Can anyone check my above method and tell me what to do further with this question? Any other shorter method is also most welcomed!","['limits', 'calculus', 'limits-without-lhopital']"
4367021,"How can i simplify the following formula: $\sum\limits_{i,j=1}^{n}(t_{j}\land t_{i})$?","Consider the following time discretization $t_{0}=0< t_{1} < ... < t_{n} = T$ of $[0,T]$ where the time increments are equal in magnitude, i.e. $t_{j}-t_{j-1}=\delta$ . How can i simplify the following formula: $\sum\limits_{i,j=1}^{n}(t_{j}\land t_{i})$ ? Note that $a\land b := \min (a,b)$ My attempt: $\sum\limits_{i,j=1}^{n}(t_{j}\land t_{i})= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}(t_{j}\land t_{i})$ Consider $i=1$ : $\sum\limits_{j=1}^{n}(t_{j}\land t_{1})=nt_{1}$ Consider $i=2$ : $\sum\limits_{j=1}^{n}(t_{j}\land t_{2})=(n-1)t_{2}+t_{1}$ Consider $i=3$ : $\sum\limits_{j=1}^{n}(t_{j}\land t_{2})=(n-2)t_{3}+t_{2}+t_{1}$ ... Continuing as above, I would obtain: $$ \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}(t_{j}\land t_{i})=\sum\limits_{i=1}^{n}\left((n+1-i)t_{i}+\sum\limits_{j=1}^{i-1}t_{j}\right)=\sum\limits_{i=1}^{n}\left((n+1-i)t_{i}+\delta(i-1)\right)$$ Surely, this is not the best simplification there is?","['summation', 'real-analysis', 'calculus', 'induction', 'algebra-precalculus']"
4367024,Does the Whitney embedding theorem induce an equivalence of categories?,"It is well known that every abstract (say real, smooth) manifold can be embedded in $\mathbb R^n$ for $n$ sufficiently large. Is it possible to turn this into a functorial construction? If yes, is it an equivalence between the categories of smooth manifolds and smooth submanifolds of euclidean spaces? For clarity: a morphism, say form $M\subset \mathbb R^n$ to $N\subset \mathbb R^m$ in the category of smooth embedded submanifolds of some euclidean space are the smooth maps $M \to N$ when we regard $M$ and $N$ as subsets of $\mathbb R^n$ and $\mathbb R^m$ respectively.","['geometric-topology', 'differential-topology', 'category-theory', 'differential-geometry']"
4367035,What does linear equivalence geometrically mean for varieties?,"Suppose we have a sufficiently nice scheme or say we are working with an abstract nonsingular variety $X$ over an algebraically close field. In this setting, one can study (Weil) divisors. It is then said that two divisors $D,D'\in\operatorname{Div}(X)$ (i.e. formal finite linear combinations of prime divisors) are linearly equivalent $D\sim D'$ if $\operatorname{div}(f)=D-D'$ for some function $f$ inside the function field $K(X)$ . I have pondered this choice of terminology for some time. Even in the case of curves, I do not precisely see why we call them linearly equivalent instead of anything else? Any insight into this would be greatly appreciated. I have searched the internet and this site for potential answers and have not found a satisfactory answer.","['divisors-algebraic-geometry', 'algebraic-geometry', 'motivation']"
4367049,"If $f_n \overset{\star}{\rightharpoonup} f$ in $\sigma(E^\star, E)$, then $\|f\| \le \liminf \|f_n\|$","I'm trying to prove this result. Could you have a check on my proof? Let $(E, | \cdot|)$ be a normed linear space and $E^\star$ its topological dual. Let $\sigma(E^\star, E)$ be the weak $^\star$ topology on $E^\star$ . Let $f\in E^\star$ and $(f_n)$ be a sequence in $E^\star$ such that $f_n \overset{\star}{\rightharpoonup} f$ in $\sigma(E^\star, E)$ . Then $\|f\| \le  \liminf \|f_n\|$ . My attempt: Let $B := \{x \in X \mid |x|=1\}$ be the unit sphere. We have $\lim_n \langle f_n, x \rangle = \langle f, x \rangle$ for all $x\in X$ . So $$\sup_{x\in B} \lim_n \langle f_n, x \rangle = \sup_{x\in B} \langle f, x \rangle = \|f\|.$$ Hence it suffices to show that $$\sup_{x\in B} \lim_n \langle f_n, x \rangle \le \liminf_n \|f_n\|.$$ In fact, we have $\langle f_n, x \rangle \le \sup_{x\in B} \langle f_n, x \rangle = \|f_n\|$ and thus $$\lim_n \langle f_n, x \rangle = \liminf_n \langle f_n, x \rangle \le \liminf_n \|f_n\|.$$ The claim then follows by taking the supremum on both sides of above inequality.","['normed-spaces', 'solution-verification', 'weak-topology', 'functional-analysis', 'dual-spaces']"
4367072,$g(x) = \sin(2nx) + \sin(4nx) - \sin(6nx)$ Find an expression for the largest root of the equation $g(x)=0$,"Let $$f(x) = P\sin(x) + Q\sin(2x) + R\sin(3x)$$ (a) Show that if $Q^2<4R(P-R)$ , then the only values of $x$ for which $f(x)=0$ are given by $x=m\pi$ , where $m$ is an integer. (b) Now let $$g(x) = \sin(2nx) + \sin(4nx) - \sin(6nx)$$ , where $n$ is a positive integer and $0<x<\frac{\pi}{2}$ . Find an
expression for the largest root of the equation $g(x)=0$ ,
distinguishing between the cases where $n$ is even and $n$ is odd. Workings: (a) The first part is simple enough. $$f(x) = P\sin(x) + Q(2\sin(x)\cos(x)) + R\sin(x)(4\cos^2(x) -1) $$ $$\iff \sin(x) (P+2Q\cos(x)+R(4\cos^2(x)-1))$$ $$\therefore \sin(x)(4R\cos^2(x)+2Q\cos(x)+P-R)$$ So, when $\Delta = 4Q^2-4(4R)(P-R) <0 \iff Q^2<4R(P-R)$ . So, if this inequality holds. The quadratic in $\cos(x)$ has no real solutions and hence $f(x) = 0$ iff $\sin(x) = 0 \iff x=m\pi$ (b) The second part is where I am struggling. We let $P=Q = 1$ and $R=-1$ to apply the results in the first part. $$g(x) = \sin(2nx)(-4\cos^2(2nx)+2\cos(2nx)+2) $$ $$\iff 2\sin(2nx)(-2\cos^2(2nx)+\cos(2nx)+1)$$ $$\therefore g(x) = 2\sin(2nx)(2\cos(2nx)+1)(-\cos(2nx)+1)$$ Hence $g(x) = 0 \iff 2\sin(2nx)(2\cos(2nx)+1)(-\cos(2nx)+1)=0$ $2\sin(2nx) = 0 \iff 2nx=\pi m \iff x = \frac{\pi m}{2n} $ $\cos(2nx) = -\frac{1}{2} \iff 2nx = \pm \frac{\pi}{3} + (2m+1) \pi \iff x = \pm \frac{\pi}{6nx} + \frac{(2m+1) \pi}{2n}$ $\cos(2nx) = 1 \iff 2nx = 2\pi m \iff x = \frac{2\pi m}{2n}$ We now have 3 general solutions for the roots of $g(x)$ , I am struggling to compare them and come up with an expression that will represent the largest root out of these.","['algebra-precalculus', 'trigonometry']"
4367097,Tensor with dual of locally free sheaf isomorphic to sheaf Hom,"During the course of trying to solve Vakil Exercise 13.1.F I decided I wanted to prove the following:
Suppose $X$ is a ringed space with structure sheaf $\mathscr{O}_X$ . Suppose $\mathscr{F}, \mathscr{E}$ are $\mathscr{O}_X$ -modules and $\mathscr{E}$ is locally free of finite rank. Then $$
\mathscr{F} \otimes \mathscr{E}^\vee \cong  \mathcal{Hom}(\mathscr{E}, \mathscr{F})
$$ Vakil 13.7.B has us prove this in less generality, but it also mentions that the result above should hold. This should follow from the corresponding fact for modules. I couldn't find this result on the Stacks Project or elsewhere online. How do you prove it? (I am almost certain Vakil doesn't intend 13.1.F to be solved the way I am trying to solve it given the fact that he has us prove a less general version of the result later.) EDIT: 13.1.F becomes 14.2.F in the December 2022 version of Vakil.","['algebraic-geometry', 'sheaf-theory']"
4367107,"Verifying a simple method to use $U(0,1)$ random generators with the CLT to sample from $N(0, 1)$","I am trying to understand an approach that was discussed in my pattern recognition class today of using uniform random variables, which can be sampled using some random generator, to obtain samples that are approximately $N(0,1)$ . Unfortunately there were no notes given so I wish to reproduce what my professor said from my understanding. Knowing that for random variables $X \sim U(0, 1)$ we have $\mu_{X} = 0.5, \sigma_{X}^2 = \frac{1}{12}$ , then consider a random variable $$Y = (\sum_{i=0}^{12} X_i) - 6 $$ $Y$ has $u_{Y}= 0, \sigma_{Y}^2 = 1$ . Now I wish to use the Central Limit Theorem (CLT), where according to the Lindeberg-Levy version of the CLT,
if $\{X_1, \ldots, X_n \}$ is a sequence of i.i.d random variables with $\mathbb{E}[X_i] = \mu$ , and $Var[X_i] = \sigma^2 \leq \infty$ , then as $n$ approaches infinite, the random variables $\sqrt{n}(\bar{X} - \mu) $ converges to $N(0, \sigma^2)$ . Then as far as I understand it, by taking $n$ samples of $Y$ , we would have, for sufficiently large $n$ , that $\sqrt{n} * \bar{Y} \sim N(0, 1)$ . Is this algorithm correct? I also see in the wikipedia entry here (the computational methods section) that they do something similar:
""Generate $12$ uniform $U(0,1)$ deviates, add them all up, and subtract $6$ ‚Äì the resulting random variable will have approximately standard normal distribution"", but they do not further average these values and multiply by $\sqrt{n}$ , so how does the CLT apply here? I'm trying to think why it would be inefficient as well?, but these seem to be pretty simple computations, in python ... import random
import math
import numpy as np

def get_y_bar(num_samples=50):
    y_bar = 0
    for i in range(num_samples):
        y_bar += sample_y()
    y_bar /= num_samples
    y_bar *= math.sqrt(num_samples)
    return y_bar

def sample_y():
    y = 0
    for i in range(12):
        y += random.random()
    y -= 6
    return y

def sample_normal(N=100000):
    sample_list = []
    for i in range(N):
        sample_list.append(get_y_bar())
    sample_list = np.array(sample_list)
    sample_mean = np.mean(sample_list)
    sample_std = np.std(sample_list)
    print(f""The sample mean is {sample_mean}, and the sample std is {sample_std}"")

sample_normal() With output:
The sample mean is $-0.0015446492547001867$ , and the sample std is $0.9989513839711084$ , which is pretty close, about $2$ digits of precision off from the theoretical idea.",['statistics']
4367109,"$ \operatorname{Hom}_{D(\mathcal{A})}(B, H^n(A^{\bullet}) ) \to \operatorname{Hom}_{D(\mathcal{A})}(B,A^{\bullet}[n] ) $ injective","Let $\mathcal{A}$ be an abelian
category of finite homological dimension and $D^b(\mathcal{A})$ the associated derived category
of bounded compexes. Let $A^{\bullet} \in D^b(\mathcal{A})$ and $n$ minimal such that $A^n \neq 0$ . Thus $A^{\bullet}$ is of the form $$ ... \to 0 \to A^n \to A^{n+1} \to ...   $$ We obtain an obvious inclusion $H^n(A^{\bullet}) \subset A^{\bullet}[n]$ where as usually we consider $H^n(A^{\bullet}) $ as a chain complex with only nonzero entry $H^n(A^{\bullet}) $ in degree $0$ . Let $B \in \mathcal{A}$ arbitrary which we can like $H^n(A^{\bullet}) $ regard also as a chain complex with only nonzero entry $B $ in degree $0$ . Question: Why is the canonical map $$ \operatorname{Hom}_{D(\mathcal{A})}(B, H^n(A^{\bullet}) ) \to \operatorname{Hom}_{D(\mathcal{A})}(B,A^{\bullet}[n] )     $$ induced by $H^n(A^{\bullet}) \subset A^{\bullet}[n]$ injective ? note that we consider morphims in $D(\mathcal{A})$ and not in the category of complexes in $\mathcal{A}$ , where the statement would become trivial. I found this statement as part of the proof of Proposition 2.73 in Huybrecht's Fourier-Mukai Transformations in Algebraic Geometry on page 60. outlook: does it hold in more general setting like if $X^{\bullet}, Y^{\bullet} $ and $Z^{\bullet}$ are complexes in $\mathcal{A}$ and there is an inclusion $X^{\bullet} \subset Y^{\bullet} $ , then $$ \operatorname{Hom}_{D(\mathcal{A})}(Z^{\bullet}, X^{\bullet}) ) \to \operatorname{Hom}_{D(\mathcal{A})}(Z^{\bullet},Y^{\bullet} )     $$ is an injection? I think that in such general setting the statement not holds since the morphisms in $D(\mathcal{A})$ differ stongly from morphisms in the category of complexes in $\mathcal{A}$ , but I haven't a conterexample. Presumably a homotopy might kill a nontrivial class in $ \operatorname{Hom}_{D(\mathcal{A})}(Z^{\bullet}, X^{\bullet}) ) $ after passing to $ \operatorname{Hom}_{D(\mathcal{A})}(Z^{\bullet},Y^{\bullet} )     $ . Even worse when we pass to the equivalence classes of roofs in $D(\mathcal{A})$ , what is really hard to control. Nevertheless, what's the reason why the above injectivity holds for original problem?","['derived-categories', 'algebraic-geometry', 'triangulated-categories']"
4367147,Can 2 different ODE's have the same set of solutions?,"If I have two differents linear ODE's: \begin{equation} x'' + p(t)x' + q(t)x = f(t) . \quad p,q \in C(I,\infty). \\ x'' + j(t)x' + g(t)x = h(t). \quad j,g \in C(I,\infty).
\end{equation} Coul they have exactly the same set of solutions? And if we have two different non-linear ODE's could they?","['ordinary-differential-equations', 'real-analysis']"
4367168,Minimiser of risk for linear-exponential error loss,"Question: Solve the following optimisation problem: $$\arg\min_{f} \mathbb{E} \left( \exp (-(Y- f(X))) + (Y - f(X)) - 1 \right)$$ Context: The linear-exponential loss function (LINEX loss for short) is given by $$L(\theta, \hat{\theta}) = \exp (-(\theta - \hat{\theta})) + (\theta - \hat{\theta}) - 1$$ The intuition behind this loss is that it is an asymmetric approximation to the usual quadratic loss function. This is a popular loss function in econometrics. Given two random $X$ and $Y$ , we may compute the loss of $Y$ relative to a measurable function of $X$ , $f(X)$ , simply by computing $L(Y, f(X))$ . A central problem in statistical decision theory is computing the minimiser of the risk of this loss; namely, we wish to solve the following optimisation problem: $$\arg\min_{f \in L^2} L(Y, f(X)) = \arg\min_{f \in L^2} \mathbb{E} \left( \exp (-(Y- f(X))) + (Y - f(X)) - 1 \right)$$ By considering some simpler cases (e.g. the case where $(X,Y)$ have a density ), one may conjecture that the minimiser is $\hat{f}(X) = - \log \mathbb{E}(e^{-Y}|X)$ . Indeed this paper derives the result in the setting of Bayes estimation. How may one arrive at this result in this setting? A similar problem: A related problem is computing the minimiser for the risk of squared-error loss: $$\arg\min_f \mathbb{E}[ (Y-f(X))^2 ] = \mathbb{E} (Y | X)$$ In this setting, one adds and subtracts $\mathbb{E}(Y|X)$ , expands, then uses properties of conditional expectation to conclude that $\mathbb{E}(Y|X)$ is indeed the minimiser. Indeed: $$\begin{align*}
\mathbb{E}[(Y - f(X))^2] &= \mathbb{E}[(Y  - \mathbb{E}(Y|X) + \mathbb{E}(Y|X) - f(X))^2] \\
&= \mathbb{E}[(Y  - \mathbb{E}(Y|X))^2] + \mathbb{E}[(\mathbb{E}(Y|X) - f(X))^2] + 2 \mathbb{E}[(Y  - \mathbb{E}(Y|X))(\mathbb{E}(Y|X) - f(X))^2] \\
&= \mathbb{E}[(Y  - \mathbb{E}(Y|X))^2] + \mathbb{E}[(\mathbb{E}(Y|X) - f(X))^2]
\end{align*}$$ where, in the last equality, we used the tower property with conditioning on $X$ to conclude the cross term is zero. At this point it is now evident that $f(X) = \mathbb{E}(Y|X)$ is a minimiser. Perhaps this idea could be extended to the loss function given above?","['statistics', 'statistical-inference', 'conditional-expectation', 'robust-statistics', 'optimization']"
4367177,$\max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{$n \rightarrow \infty$}} \sup_{k=1}^{\infty}|x_k|$,"I want to prove the following:
Let $(x_n)$ be a bounded sequence, then $\max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{$n \rightarrow \infty$}} \sup_{k=1}^{\infty}|x_k|$ . My Calculations: Let $c:=\sup_{k=1}^{\infty}|x_k|$ .
If $c$ is the supremum of $(x_n)$ , then (1) $\forall n \in \mathbb{N}: |x_n| \leq c$ (2) $\forall \epsilon>0 :\exists |x_j|$ such that $|x_j|>c-\epsilon$ Just to clarify it, I am looking at the sequence $(|x_n|)_{n \in \mathbb{N}}$ and NOT at the sequence $(x_n)_{n \in \mathbb{N}}$ . This is the reason why I get absolute values in (1) and (2). Now considering the inequality in (2): $|x_j|>c-\epsilon$ , we get $\epsilon > c- |x_j|$ I noticed $\epsilon$ and $c -|x_j|$ are both positive numbers.
This means I can ""take the absolute value"" of the inequality and get $|c- |x_j||<\epsilon $ The inequality I am looking for is $|c-\max_{k=1}^n|x_k||  <\epsilon$ . Now there can be two cases: Case 1: $|x_j|$ is the maximum, i.e $|x_j|=\max_{k=1}^n|x_k|$ and $|x_{n+1}|>|x_j|$ But this just means the inequality still holds. Since $|x_{n+1}|>|x_j|$ it follows that $|c-|x_j||<|c-|x_{n+1}||<\epsilon$ Case 2: $|x_j|$ is the maximum, i.e $|x_j|=\max_{k=1}^n|x_k|$ and $|x_{n+1}|<|x_j|$ In this case nothing changes, since $|x_j|=\max_{k=1}^{n+1}|x_k|$ and the equality $|c- \max_{k=1}^{n+1}|x_k||=|c-|x_j||<\epsilon $ still holds. By this reasoning it follows that, for every $n \in \mathbb{N}$ , there exists a $\epsilon>0$ such that: $|\underbrace{sup_{k=1}^{\infty}|x_k|}_{=c} - \max_{k=1}^n|x_k||<\epsilon$ . The question I would like to ask is: Is this proof right? __________________________________________________________ Edit: One user pointed out that I did not show the definiton of convergence. In fact, I messed up a little bit in the last part. So now the second try: So I looked up the definition.
Let $y_n$ be a sequence and $y \in \mathbb{R}$ . The sequence is convergent if for every $\epsilon>0$ , there exists a $N \in \mathbb{N}$ such that $|y_n-y|<\epsilon$ for all $n>N$ So let $\epsilon>0$ . The property of the supremum says that there exists is a $x_j$ such that $|sup_{k=1}^\infty{|x_k|}-\underbrace{max_{k=1}^{N}{|x_k|}}_{=x_j}|<\epsilon_1$ . This means, there exists a $N \in \mathbb{N}$ such that $x_j=max_{k=1}^{N}|x_k|$ . Choosing N such that the $\epsilon_1$ I get from the supremum is $\epsilon_1 \leq \epsilon$ , then the inequality $|sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon$ holds. Now if $|x_{N+1}|>|x_j|$ , the inequality still holds (Case 1). If $|x_N+1|<|x_j|$ (Case 2), $ x_j=max_{k=1}^{N+1}{|x_k|}$ and the equality still holds. And by induction it is true for all $n \geq N$ . Meaning, $\forall \epsilon >0 :\exists N \in \mathbb{N}$ such that $|sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon$ for all $n \geq N$","['supremum-and-infimum', 'analysis', 'real-analysis']"
4367233,Why is this secant substitution allowed?,"On Paul's Math Notes covering Trig Substitutions for Integrals we start with an integral: $$\int{{\frac{{\sqrt {25{x^2} - 4} }}{x}\,dx}}$$ Right away he says to substitute $x=\frac{2}{5}\sec(Œ∏)$ .  Why is that allowed? Looking further down onto how he approaches the problem, it seems like it's allowed because it's compensated for with a dx: $$dx = \frac{2}{5}\sec \theta \tan \theta \,d\theta$$ Is that what's going on here? It's fair to say you can substitute x with whatever you want so long as you update dx?  Seems like it wouldn't work for constant functions of x, like $x = 5$ .. since that'd get you $dx=0$ and clearly be wrong.  So what rules are in play here for substitution?","['integration', 'trigonometry', 'substitution']"
4367243,"How to understand for which $a, b\in\mathbb{R}$ the equation $a-x+\frac{b}{x^3}=0$ has a unique zero?","Let $a, b\in\mathbb{R}$ , $x\in\mathbb{R}^*$ and consider te he equation $$a-x+\frac{b}{x^3}=0.$$ My question is: there is a way to understand for which values of $a, b$ has a unique zero?
I tried by using wolfram, but I am not so practice with that. Thank you in advance! ${\bf EDIT:}$ Availing of the comment of David P, I have $$a-x+\frac{b}{x^3}=0 \iff ax^3-x^4 +b=0 \iff x^4-ax^3-b=0.$$ Now, let $f(x) =x^4-ax^3-b$ . It is $f^{\prime}(x) = 4x^3 -3ax^2$ and then $$f^{\prime}(x) =0\iff x^2(4x-3a)=0\iff x=\frac34 a.$$ On the other hand, it is $$f(\frac34 a) = 0\iff \frac14 \left(\frac34\right)^3 a^4 +b=0\iff b= -\frac14 \left(\frac34\right)^3 a^4.$$ Could someone please help me to check the signs of $f$ aside from the zero?","['calculus', 'functions', 'roots', 'real-analysis']"
4367256,Computing the smallest eigenvalue of a positive definite matrix $\bf{A}$ without using $\bf{A^{-1}}$,"I want to numerically compute the smallest eigenvalue of an $n \times n$ positive definite matrix $\bf{A}$ . Why I want to avoid working with $\bf{A^{-1}}$ I know that I can apply power iteration to $\bf{A^{-1}}$ , but I wish to avoid working with this inverse matrix. In my application, $\bf{A}$ is implicitly expressed as $\bf{A} = \bf{K'K}$ with $\bf{K} = \bf{L^{-1}R}$ , where $\bf{L}$ is an $m \times m$ lower triangular band matrix; $\bf{R}$ is an $m \times n$ band matrix of full column rank $(m > n)$ . While $\bf{L}$ and $\bf{R}$ are sparse, $\bf{K}$ and $\bf{A}$ are fully dense. Given that both $m$ and $n$ can be large, I want to explicitly form neither $\bf{K}$ nor $\bf{A}$ . A bad luck is that $m \neq n$ , so that $\bf{R}$ is not square and there is no convenient factor form as $\bf{A^{-1}} = R^{-1}LL'R^{-1\prime}$ . What I have tried With $\bf{A}$ structured as above, it is computationally efficient to compute $\bf{Av}$ for any vector $\bf{v}$ using sparse linear algebra routines. It is easy to compute $\bf{A}$ 's largest eigenvalue $\mu$ using power iteration: $\bf{v_0} = (\frac{1}{n}, \ldots, \frac{1}{n})$ ; $\bf{u_0} = \bf{Av_0}$ ; $\lambda_0 = \bf{v_0'u_0}$ ; for $k = 1, 2, \ldots$ till convergence of $\{\lambda_k\}$ $\bf{v_k} = \bf{u_{k - 1}} / \|\bf{u_{k - 1}}\|$ ; $\bf{u_k} = \bf{Av_k}$ ; $\lambda_k = \bf{v_k'u_k}$ ; return $\lambda_k$ . To find the smallest eigenvalue, I apply this algorithm to $\bf{B} = \bf{A - \mu\bf{I}}$ , inspired by this thread: https://math.stackexchange.com/a/271876/407465 . Here is an R program to implement this method. PowerIter <- function (A, mu = 0) {
  n <- nrow(A)
  ## spectral shift
  A <- A - diag(mu, n)
  ## power iteration
  v.old <- rep.int(1 / n, n)
  u.old <- A %*% v.old
  d.old <- sum(v.old, u.old)
  k <- 0L
  repeat {
    v.new <- u.old * (1 / sqrt(sum(u.old ^ 2)))
    u.new <- A %*% v.new
    d.new <- sum(v.new * u.new)
    if (abs(d.new - d.old) < abs(d.old) * 1e-8) break  ## test relative error
    d.old <- d.new
    u.old <- u.new
    k <- k + 1L
  }
  cat(""convergence after"", k, ""iterations.\n"")
  d.new
} What goes wrong? I found that this suggested algorithm (via spectral shift) does not seem numerically stable. I composed two toy examples, one being successful, the other being problematic. A successful example \begin{equation}
\bf{A} = \begin{pmatrix}
2 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 2
\end{pmatrix}
\end{equation} With $|\bf{A} - \lambda\bf{I}| = (2 - \lambda)[(2 - \lambda)^2 - 2]$ , the three eigenvalues are $2 + \sqrt{2} \approx 3.4142136$ , $2$ and $2 - \sqrt{2} \approx 0.5857864$ . The algorithm is a success for this matrix. Here is the output of an R session. A <- matrix(c(2, -1, 0, -1, 2, -1, 0, -1, 2), nrow = 3)

d.max <- PowerIter(A)
## convergence after 7 iterations.
## [1] 3.414214

d.min <- PowerIter(A, d.max) + d.max
## convergence after 1 iterations.
## [1] 0.5857864 A problematic example Now let's construct $\bf{A} = \bf{Q'DQ}$ , where $\bf{D} = \textrm{diag}(d_1, d_2, d_3)$ are eigenvalues and $\bf{Q}$ is a rotation matrix \begin{equation}
\bf{Q} = \begin{pmatrix}
0.36 & 0.48 & -0.8\\
-0.8 & 0.6 & 0\\
0.48 & 0.64 & 0.6
\end{pmatrix}
\end{equation} I fixed $d_1 = 1$ , $d_2 = 0.01$ and tried three choices for $d_3$ : $10^{-3}$ , $10^{-5}$ and $10^{-7}$ . Here is the output of an R session. test <- function (d) {
  Q <- matrix(c(0.36, -0.8, 0.48, 0.48, 0.6, 0.64, -0.8, 0, 0.6), nrow = 3)
  A <- crossprod(Q, d * Q)
  d.max <- PowerIter(A)
  d.min <- PowerIter(A, d.max) + d.max
  c(min = d.min, max = d.max)
}

test(c(1, 1e-2, 1e-3))
## convergence after 3 iterations.
## convergence after 298 iterations.
##         min         max 
## 0.001000543 1.000000000 

test(c(1, 1e-2, 1e-5))
## convergence after 3 iterations.
## convergence after 279 iterations.
##         min         max 
## 1.04883e-05 1.00000e+00 

test(c(1, 1e-2, 1e-7))
## convergence after 3 iterations.
## convergence after 279 iterations.
##          min          max 
## 5.860836e-07 1.000000e+00 Computation of the smallest eigenvalue is slow and becomes increasingly inaccurate as $\bf{A}$ gets less well conditioned (but it is still far from being ill-conditioned!). My questions Is there a mathematical justification for such observation? How can I modify this algorithm for better numerical stability? If the algorithm can not be improved, can someone suggest another efficient algorithm? If no better algorithm exists, any lower bound for the smallest eigenvalue? Gershgorin Circle Theorem is not good as it is too loose a bound.","['numerical-linear-algebra', 'numerical-methods', 'linear-algebra', 'eigenvalues-eigenvectors']"
4367266,Getting the correct arc length formula,"You have your regular arc length formula: $l = r\theta$ where $\theta$ is in radians. Now you want to express the $\theta$ in degrees. So you apply the $2\pi [rad] = 360[degrees]$ formula to the above equation, and you get $$ 1[rad] = \frac{360 [degrees]}{2\pi}$$ $$ l=r\theta_{rad}=r\theta \frac{360degrees}{2\pi} $$ But my book says when you want to get arc length $l$ in degrees, the correct formula is $2\pi r\frac{\theta}{360}$ , which is different from what I got above. Can someone please explain why I get this result? Also apologies if this question is too simple to be posted here. I searched online but couldn't find a proper resource. Many thanks in advance! [ Update ]
I missed a really simple definition which was $\theta_{deg} = \theta_{rad} * \frac{360}{2\pi}$ . After realizing it, everything began to make sense. Thanks to everyone that shared their answers!",['trigonometry']
4367274,Standard derviation of a modified higher dimensional random walk,"A friend of mine asked me a question in computer science which induces to the following problem in (propably) theories of random walk. Let $d$ be a positive integer. We are given a discrete distribution $\{p_1, \ldots, p_d\}$ . A particle starts walking on $\mathbb{Z}_{\geq 0}^{d}$ in the following way: Let $(n_1(t), \ldots, n_d(t))$ be the position of the particle at time $t$ , where $t$ takes discrete integer values, i.e. $t=0, 1, 2, \ldots$ . At each second, it moves one step along the direction of the $i$ -th axis in the probability of $p_i$ (i.e. $(n_1(t), \ldots, n_d(t)) \mapsto (n_1(t), \ldots, n_{i-1}(t), n_{i}(t)+1, n_{i+1}(t), \ldots, n_{d}(t))$ ). Then after $T$ steps (i.e. at the $T$ -th second), the particle moves to the position $(n_1(t), \ldots, n_d(t))$ . Then we hope to estimate the standard derviation of this sequence of $d$ numbers, i.e. $$
\Delta(t) := \dfrac{1}{d} \sqrt{\sum_{i=1}^{d} (n_i(t) - \overline{N}(t))^2}.
$$ It is indeed so great if one may provide the exact formula for $\Delta(t)$ . Or if it is difficult, we would like an estimate of this when $t$ is sufficiently large. Moreover, we modifiy this with a cliff on the lattice. Let $N > 1$ be an integer. The particle still walks on the lattice in the same way as above, but when it reaches $N$ in any direction $i$ , it will never go forward on that direction (i.e. even in further steps it has a possibility $p_i$ to take a step in direction $i$ , it will remain unmoved in that particular direction.). Then in this case, we raise the same question: how to calculate or estimate $\Delta(t)$ in this case? As I'm a postgraduate student in number theory and algebraic geometry, I find it hard to work on such a problem. So could anyone help us with this? Thank you all for answering and commenting!","['statistics', 'random-walk', 'stochastic-processes', 'probability-theory', 'probability']"
4367289,"In $\frac{1}{a^2-x^2}$, how can $a\sin(\theta)$ be substituted for $x$ when finding the anti-derivative?","$$\int{\frac{1}{a^2-x^2}}dx\tag{1}$$ When finding the above anti-derivative, $x$ is substituted with $a\sin(\theta)$ . However, the range of $x$ is $\mathbb{R}-\{-a,a\}$ while the range of $a\sin(\theta)$ is $[-a,a]$ . Graph of $\frac{1}{a^2-x^2}[a=0.5]$ Needless to say, $$\mathbb{R}-\{-a,a\}\ne[-a,a]$$ So, how can $a\sin\theta$ be substituted for $x$ in $(1)$ when the range of $x$ and $a\sin\theta$ are not the same? Related","['integration', 'calculus', 'functions', 'trigonometry']"
4367295,"Partial derivative of $f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))]$","If $$f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))]$$ Then what are the values of partial derivative $f_x(1,2)$ and $f_x(1,5)?$ I tried my best but couldn't fight with such a difficult calculations involved in finding partial derivative. Is there any short and easy approach to find partial derivative of such a typical function?","['calculus', 'functions', 'partial-derivative', 'limits', 'derivatives']"
4367323,Why do we calculate variance if standard deviation serves the ends well?,"I don't understand why we even care for square units. How does it make sense that if you take the squared difference of each data set and mean, then divided it by $n-1$ gives us a measure of spread? Variance is not as intuitive to me as standard deviation, which makes absolute sense. Can anyone help me understand the importance of variance and how its formula makes sense?","['statistics', 'variance', 'standard-deviation']"
4367348,Assume that $A$ has the minimal polynomial as $(x-1)(x-2)$ and c.p. as $(x-2)^2(x-1)$,"Let $$A := \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1\\ \end {pmatrix}$$ Find the number of matrices similar to $A$ whose entries are from $\mathbb{Z}/\mathbb{3Z}$ . Since $A$ is a $3 \times 3$ matrix and $A$ has the minimal polynomial as $(x-1)(x-2)$ and c.p. as $(x-2)^2(x-1)$ .Then any matrix simialr to $A$ will have the same minimal and characteristic polynomial. Any matrix similar to $A$ will be an upper triangular matrix or a lower triangular matrix with diagonals as $\{1,2,2\}$ and $\{2,2,1\}$ . Then number of upper triangular matrix with diagonals as $\{1,2,2\}$ is $3^3$ . Similarly number of lower triangular matrix is $3^3$ . Then the total numbers is $54$ . So the total number of matrices as $2 \times 54 = 108$ .The answer is $117$ .What am i missing?","['finite-fields', 'matrices', 'minimal-polynomials', 'linear-algebra', 'characteristic-polynomial']"
4367429,Define the smallest subspace of $3\times 3$ matrix vector space that contains the set of all invertible matricies.,"Define the smallest subspace of $3\times 3$ matrix vector space that contains the set of all invertible matrices. My attempt: $A$ is invertible if and only if its columns form a basis in $F^3$ . The only subspace I can think of is the vector space of all $3\times 3$ matrices, which I cannot prove is the smallest. Note: We are not allowed to use determinant.","['matrices', 'invariant-subspace', 'linear-algebra']"
4367445,Why is there an unexpected increase in the density of certain types of Goldbach primes?,"Note : Posted in MO since it is unanswered in MSE. I was checking how quickly we can verify Goldbach's conjecture for a given even number $n$ and it was clear that backward starting from the largest prime below $n$ was much faster than a forward search starting from the small odd prime $3$ . I observed the following interesting phenomenon. Let $n$ be an even number and $p$ be the largest prime less than $n$ such that $n-p$ is also a prime. Let $p_k$ be the $k$ -th prime. Then, $p_{\pi(n)-2}$ , $p_{\pi(n)-1}$ and $p_{\pi(n)}$ are the three largest primes preceding $n$ . Now $p$ can either be the largest prime below $n$ , or the prime just before the largest prime or the prime before that and so on. Accordingly, we define: $d_1$ is the density of $n$ such that $p = p_{\pi(n)}$ $d_2$ is the density of $n$ such that $p = p_{\pi(n)-1}$ $d_3$ is the density of $n$ such that $p = p_{\pi(n)-2}$ Data for $n \le 3 \times 10^{10}$ shows that $d_1$ and $d_2$ decreased as $n$ increased. This was intuitively expected as explained in the answer for the related question "" If $p$ is the largest prime less than $2n$ , what is the probability that $2n-p$ is a prime? "". However, quite counter intuitively, $d_3$ increased with $n$ as shown in the graphs below. Question 1 : Why does $d_3$ increase with $n$ while $d_1$ and $d_2$ decreased? Question 2 : What is the limiting value of $d_3$ ? Since $d_3 < 1$ and is increasing, it must converge to a positive limiting value. Note : I have not computed $d_k$ for $k \ge 4$ so I do not know if this increasing trend is observed other values of $k$ . Graphs in normal scale Graphs in $\log n$ scale SageMath code n = 4
target = step = 10^6
i = c1 = c2 = c3 = 0

while True:
    i = i + 1
    p = previous_prime(n)
    if is_prime(n-p) == True:
        c1 = c1 + 1
    else:
        p = previous_prime(p)
        if is_prime(n-p) == True:
            c2 = c2 + 1
        else:
            p = previous_prime(p)
            if is_prime(n-p) == True:
                c3 = c3 + 1
        
    if i >= target:
        print(i, c1, c2, c3, c1/i.n(), c2/i.n(), c3/i.n(), (c1+c2+c3)/i.n())
        target = target + step
    n = n + 2","['divisibility', 'number-theory', 'elementary-number-theory', 'asymptotics', 'prime-numbers']"
4367447,"A question about the ""dominated"" part of the DCT","Let $f_n:\mathbb{R}\to [0, \infty)$ be a sequence of measurable functions that converges to some function $f$ . Suppose that I show that for "" $n$ big enough"" we have $|f_n(x)|\le g(x)$ for all $x\in \mathbb{R}$ . Am I right in saying that I can still apply the DCT to conclude that $\displaystyle\lim\limits_{n\to\infty}\int_{\mathbb{R}}f_nd\lambda=\int_{\mathbb{R}}fd\lambda$ ? I think that I can do this simply because I am removing a finite number of terms from my sequence $\displaystyle \int_{\mathbb{R}}f_nd\lambda$ and then I am applying DCT. Is this right?","['measure-theory', 'real-analysis']"
4367470,Orientable vs Oriented connected sum,"I am confused about defining the connected sum of two $\color{red}{oriented}$ $n$ -dimensional connected smooth manifolds without boundary, where $n\geq 2$ . Could anyone help me to clear my confusion? I have written some guesses; maybe you can comment on these also. Thanks in advance. So, let $M,M'$ be two connected $\color{blue}{orientable}$ $n$ -dimensional connected smooth manifolds. Let $\varphi\colon \Bbb B^n\hookrightarrow M$ and $\varphi'\colon \Bbb B^n\hookrightarrow M'$ be two smooth embeddings of closed unit balls. Define $$\sharp(M,M'):=\frac{\big(M\backslash \varphi(\text{int }\Bbb B^n)\big)\sqcup \big(M'\backslash \varphi'(\text{int }\Bbb B^n)\big)}{\varphi(p)\sim \varphi'(p),\ p\in \Bbb S^{n-1}}.$$ Question $(1)$ Is $\sharp(M,M')$ always $\color{blue}{orientable}$ ? Question $(2)$ Is it also true that if we $\color{red}{pick\
 orientations}$ for $M,M'$ , then $\color{red}{one\ can\ orient}$ $\sharp(M,M')$ so that orientation of $\sharp(M,M')$ coincides with
the orientation of $M\backslash \varphi(\text{int }\Bbb B^n)$ ? I guess the answers to both questions are positive. I know the following: If we pick orientations for $M, M'$ and choose $\varphi$ as orientation-preserving and $\varphi'$ as orientation-reversing then $\sharp(M, M')$ has a unique orientation that  coincides with the orientations of $M\backslash \varphi(\text{int }\Bbb B^n)$ as well as $M'\backslash \varphi'(\text{int }\Bbb B^n)$ . And the proof is based on the following theorem: Richard Palais Theorem: Let $X$ be a $n$ -dimensional smooth manifold without boundary. (If $X$ is orientable, pick an orientation for $X$ ) Let $\varphi_1,...,\varphi_m\colon \Bbb B^n\hookrightarrow X$ and $\psi_1,...,\psi_m\colon \Bbb B^n\hookrightarrow X$ be two sets of smooth embeddings with pairwise disjoint images (If $X$ is orientable, then we demand that all $\varphi_i,\psi_j$ are orientation-preserving or that all are orientation-reversing). Then there exists an isotopy $F\colon X\times [0,1]\to X$ with $F(-,0)=\text{id}_X$ such that the diffeomorphism $F(-,1)\colon X\to X$ sends $\varphi_i$ to $\psi_i$ , i.e., $F\circ \varphi_i=\psi_i$ for all $i=1,...,m$ . For the following two questions, pick orientations for $M, M'$ and choose both $\varphi,\varphi'$ as orientation-preserving smooth embedding. Question $(3)$ Is it true that if $M$ has an orientation-reversing
diffeomorphism, then $\sharp(M,M')$ is again $\color{blue}{orientable}$ , and $\color{red}{there\ exists\ unique\ orientation}$ for $\sharp(M, M')$ that coincides with the orientations of $M\backslash
 \varphi(\text{int }\Bbb B^n)$ as well as $M'\backslash
 \varphi'(\text{int }\Bbb B^n)$ ? My guess is that it is again true; probably one needs to pick an orientation for $\sharp(M, M')$ that coincides with the orientation of $M'\backslash \varphi'(\text{int }\Bbb B^n)$ . Now, composing an orientation-reversing diffeomorphism $M\to M$ with $F(-,1)\colon M\to M$ (see Richard Palais Theorem), one can produce a diffeomorphic copy of $\sharp(M, M')$ having the same orientation of $\sharp(M, M')$ . Question $(4)$ Is it true that if neither of $M, M'$ has no
orientation-reversing diffeomorphism, then though $\sharp(M, M)$ is $\color{blue}{orientable}$ , it is impossible to $\color{blue}{pick\ an\
 orientation}$ for $\sharp(M,M')$ that coincides with the orientations
of $M\backslash \varphi(\text{int }\Bbb B^n)$ as well as $M'\backslash
 \varphi'(\text{int }\Bbb B^n)$ ?","['smooth-manifolds', 'orientation', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4367522,Convergence of double integral involving logarithms and binomials? Can anyone present a simpler proof?,"I am working through a paper I posted to arXiv and am looking to condense my results as well as seek alternative proofs for some of my intermediate results. In particular, I am looking for a shorter proof that gets me to the result of Theorem 1 below. This Theorem was used to justify convergence of a more complicated double integral in my paper. Is Lemma 1 needed at all? Is there a much simpler line of reasoning to get to Theorem 1 ? Lemma 1. For any $n\in\Bbb N_0$ , $$
I_n=\int_0^1\int_0^1(-1)^n\log^n(1-(1-x)(1-y))\,(1-(1-x)(1-y))^{s-2}\,\mathrm dx\mathrm dy,
$$ converges if $s>0$ . Proof. We will first evaluate $I_0$ . Perform the change of variables $(t,v)=((1-x)(1-y),x)$ and integrate over $v$ yielding $$
I_n=\int_0^1(-1)^n\log^n(1-t)(-\log t) (1-t)^{s-2}\,\mathrm dt.
$$ Substituting $n=0$ , we again change variables via $x=1-t$ and then integrate by parts with $u=-\log(1-x)$ and $\mathrm dv=x^{s-2}\,\mathrm dx$ to find $$
I_0=\frac{1}{s-1}\int_0^1\frac{1-x^{s-1}}{1-x}\,\mathrm dx+\log(1-x)\frac{1-x^{s-1}}{s-1}\bigg|_{x=0}^1.
$$ If $s>0$ the limit term vanishes and upon inspection of the integral representation for the harmonic numbers $$
I_0=\frac{H_{s-1}}{s-1}.
$$ Now consider the general case $I_n$ . Without loss of generality assume $n\geq 1$ and perform integration by parts with $u=(-1)^n\log^n(1-t)$ and $\mathrm dv=-\log t(1-t)^{s-2}\,\mathrm dt$ . Expanding the logarithm in $\mathrm dv$ as a power series in $(1-t)$ and integrating termwise we find $$
v=-\sum_{k=0}^\infty\frac{(1-t)^{s+k}}{(s+k)(1+k)}.
$$ In this form, it becomes clear that the limit term $uv|_{t=0}^1$ vanishes if $n\geq 1$ so that $I_n=\int_0^1(-v)\,\mathrm du$ . Furthermore, we observe for $s>0$ : $$
-v=\frac{1}{s}(1-t)^{s-1}\sum_{k=0}^\infty\frac{s(1-t)^{k+1}}{(s+k)(1+k)}\leq \frac{1}{s}(-\log t)(1-t)^{s-1}.
$$ Hence, $$
I_n\leq\frac{n}{s}\int_0^1(-1)^{n-1}\log^{n-1}(1-t)(-\log t)(1-t)^{s-2}\,\mathrm du=\frac{n}{s}I_{n-1}.
$$ Solving the recurrence relation and calling on the result for $I_0$ we find for $s>0$ : $$
I_n\leq\frac{n!}{s^n}\frac{H_{s-1}}{s-1}<\infty,
$$ Theorem 1. Let $z\in\Bbb R^+$ , $m,n\in\Bbb N_0$ , $a,b\in(-\infty,2]$ , $$
f(x,y,z) =1-\frac{(1-x)(1-y)}{(1-(1-z)x)(1-(1-z) y)},
$$ $$
F(x,y,z,l,s)=(-1)^l(\log\circ f)^l(x,y,z)f^{s-2}(x,y,z),
$$ and $$
I=\int_0^1\int_0^1F(x,y,z,n,a)F(x,y,1,m,b)\,\mathrm dx\mathrm dy.
$$ Then $I$ converges whenever $a+b>2$ . Proof. For convenience we will introduce the auxiliary function $$
J(x,y,z)=\frac{z^2}{(z+(1-z)x)^2(z+(1-z)y)^2}
$$ as well as the following properties: $$
\begin{array}{*2{>{\displaystyle}l}}
(\mathrm{FII}) &\text{$f(\cdot,z)$ is increasing on $z\in\Bbb R^+$ from $f(\cdot,0)=0$ to $f(\cdot,\infty)=1$}.\\[1ex]
(\mathrm{J}) &\text{For all $z\in\Bbb R^+$ and $(x,y)\in[0,1]^2$: $0\leq J(x,y,z)\leq z^{2\operatorname{sign}(z-1)}$}.
\end{array}
$$ Since $n\in\Bbb N_0$ and $a\leq 2$ we are able to deduce from property $(\mathrm{FII})$ that $F(x,y,z,n,a)$ is nonnegative and a decreasing function of $z$ for all $z\in\Bbb R^+$ . Denoting $z^\ast=\min\{1,z\}$ , it follows for all $z\in\Bbb R^+$ $$
I\leq\int_0^1\int_0^1F(x,y,z^\ast,m+n,a+b-2)\,\mathrm dx\mathrm dy.
$$ Performing the change of variables $(1-u,1-v)=((1-x)/(1-(1-z^\ast)x),(1-y),(1-(1-z^\ast)y))$ and then using property $(\mathrm J)$ subsequently gives $$
I\leq\max\{1,z^{-2}\}\int_0^1\int_0^1F(u,v,1,m+n,a+b-2)\,\mathrm du\mathrm dv,
$$ which according to the Lemma 1 converges if $a+b>2$ . The proof is now complete.","['integration', 'definite-integrals', 'alternative-proof', 'harmonic-numbers', 'solution-verification']"
4367538,A tough set theory problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $A_1,A_2, A_3,\ldots, A_n$ are subsets of a set $S$ with $|S|=2019$ such that the union of any three of the $A_k$ s gives $S$ whereas the union of any two isn't equal to $S$ . Find the maximum value of $n$ . I am finding this problem very hard to do. I guess it involves the knowledge of very advanced set theory which probably I don't know. Any help will be greatly appreciated.",['combinatorics']
4367557,Finding non-zero function such that $f(2x)=f'(x)\cdot f''(x)$,"I am thankful if someone can help me or show me the clue. As honestly as possible, I got stuck on this problem. I need some help in finding a $f(x)\neq 0$ such that $$f(2x)=f'(x) \cdot f''(x),$$ where $f',f''$ are the first and second derivatives, respectively. It is not an ordinary differential equation. My last try was to put $f(x)=ke^{ax}$ , and then $$ke^{2ax}=kae^{ax}\cdot ka^2e^{ax}.$$ Now by canceling $e^{ax}$ , we have $$k=k^2a^3.$$ This shows $k=1,a=1$ and finally $f(x)=e^{x}$ . But I want to solve the problem analytically.","['functions', 'derivatives', 'ordinary-differential-equations']"
4367593,The total variation of a continuous $f$ satisfies: $\lim_{\epsilon \to \infty} T_\epsilon ^1(f) = T_0^1(f)$.,"This is essentially a supremum and limits question. I've encountered the following claim: If $f$ is continuous on $[0,1]$ , then $f$ satisfies: $\lim_{\epsilon \to 0^+} T_\epsilon ^1(f) = T_0^1(f)$ . where: $T_a^b(f) := \sup\big\{\sum_{i=1}^n|f(x_i)-f(y_i)| \mid \{(x_i,y_i)\}_{i=1}^k \text{are a partition of [a,b]}\big\}$ . I realize that I have to use the formal definition of the limit to get around the fact that $\epsilon$ is a part of the partition's definition, so I tried using supremum definitions to get to something like the following: Denoting the internal sum (i.e. the per-partition variation) by $V(f,P)$ , we want to show that for every $\epsilon'>0$ there exists an $N>0$ such that for all $n>N$ : $$| \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [0, 1]\big\} - \sup\big\{V(f,P) \mid \ P \text{ is a partition of } [\frac{1}{n}, 1]\big\} | <\epsilon'$$ but I'm not sure what to do with the supremums (my inuition is that somehow I can bound this by $|f(\frac{1}{k})-f(0)|$ where $k$ will be chosen such that $|\frac{1}{k}-0|\leq\delta$ and $\delta$ matches $\epsilon'$ from $f$ 's continuity around $0$ , so that we obtain $|f(\frac{1}{k})-f(0)| < \epsilon'$ ), but as I'm a bit rusty on handling supremums in such situations, I'm not sure how to move from the supremums inequality to this. Edit: I think the following comes close to solving it: Let $P$ by a partition of $[\frac{1}{n},1]$ . Denote by $P^+$ the partition of $[0,1]$ such that its first two points are $\{0,\frac{1}{n}\}$ and the remaining are the points in $P$ after $x=\frac{1}{n}$ . Then we have: $$\sup_{P}V(f,P^+)-\sup_PV(f,P)\leq \sup_P\big(V(f,P^+)-V(f,P)\big) \leq \sup_P\big(|f\big(\frac{1}{n}\big)-f(0)|\big)\leq\epsilon'$$ for $n$ sufficiently large, by continuity of $f$ around 0. The first inequality follows from: $$\sup(A+B)=\sup A+\sup B$$ where, in our case: $$A:= V(f,P^+)-V(f,P)$$ $$B:=V(f,P)$$ It doesn't solve it though because the partitions on $[0,1]$ are limited to those of the type $P^+$ . Any advice? Thanks!","['limits', 'calculus', 'supremum-and-infimum', 'real-analysis']"
4367610,"Evaluating $\int\frac{\sec^2(x)}{(4+\tan^2(x))^2}\, dx$","How to solve this integral? $$\int\frac{\sec^2(x)}{(4+\tan^2(x))^2}\, dx$$ I've tried the following: Starting by substituting $\tan(x) = 2\tan(\theta)\implies \sec^2(x)\ dx= 2\sec^2(\theta)\ d\theta$ $$\implies\int\frac{2\sec^2(\theta)}{(4 + 4\tan^2\theta)^2}\ d\theta\\\implies\int\frac{2\sec^2(\theta)}{16 (\sec^2\theta)^2}\ d\theta\\\implies\int\frac{\cos^2(\theta)}{8} \ d\theta\\\implies\frac\theta{16} + \frac{\sin(2\theta)}{32}+ c$$ Now since, $\tan(x) = 2 \tan\theta\implies \arctan\left(\frac{\tan(x)}{2}\right) = \theta$ So, the final answer to the integral may be, $$\implies\frac{\arctan\left(\frac{\tan(x)}{2}\right)}{16} + \frac{\sin(2\arctan\left(\frac{\tan(x)}{2}\right))}{32}+ c$$ I checked the answer on wolfram Alpha and found that my answer is not matching with that. Can anyone tell me what should I do to verify my answer?","['substitution', 'calculus', 'solution-verification', 'indefinite-integrals', 'trigonometry']"
4367612,Countable union and finite intersection,"Let $\mathcal{F}$ be a collection of subsets of $\Omega$ . Let $\mathcal{F}_{\sigma}$ denote the countable union of sets in $\mathcal{F}$ . In the same manner, let $\mathcal{F}_d$ denote the finite intersection of sets in $\mathcal{F}$ . Here is the question: Let's consider the collection $\mathcal{F}_{\sigma d}=(\mathcal{F}_\sigma)_d$ . I have to show the collection of sets isn't necessarily closed under countable unions.
I'm looking for a hint. I tried the obvious collection of sets for $\mathcal{F}$ : open sets, closed sets, etc..",['elementary-set-theory']
4367647,Some questions about single sample sign test,"$S^+$ : number of values greater than the median. $S^-$ : number of values less than the median. Question 1 For the given hypothesis and sample size, state whether the null hypothesis should be rejected at the 5% significance level $H_0:$ median = k $H_1:$ median < k $n=8$ and $S^+= 3$ . My work for Question 1: $X$ : number of values, out of 8, greater than median. $X \sim B (8,0.5)$ $P(X \le 3)=0.5^8\sum _{r=0}^{3}8Cr=0.363$ . Since the test statistic ( $=3$ ) is not within rejection region, we do not reject null hypothesis. Is this the correct way to do this question? Here's another question where I used the above method but got the incorrect answer : Question 2 For the given hypothesis and sample size, state whether the null hypothesis should be rejected at the 5% significance level $H_0:$ median = k $H_1:$ median > k $n=15$ and $S^+= 11$ . My work for Question 2: $X$ : number of values, out of 15, greater than median. $X \sim B (15,0.5)$ $P(X \ge 11)=0.5^{15}\sum _{r=11}^{15}15Cr=0.059$ . Since the test statistic ( $=11$ ) is not within rejection region, we do not reject null hypothesis. However, according to the answer sheet, my answer for question 2 is incorrect. Null hypothesis should have been rejected. Where did I mess up? Some additional questions about single sample sign test : Would the procedure/conclusion change if we were told instead that $S^-=3$ for first question and $S^-=11$ for the second question? If $H_1$ : median $>k$ and $s^+<n/2$ , on which tail do I place the rejection region?","['statistical-inference', 'statistics', 'probability', 'hypothesis-testing']"
4367652,why does the $p$th-power Frobenius morphism on an elliptic curve equal $\sqrt{-p}$?,"Let $E$ be an elliptic curve over $\mathbb{F}_p$ , then the $p$ th-power Frobenius morphism, denoted $\pi: (x,y) \mapsto (x^p, y^p)$ , is an endomorphism on $E$ . There is an argument showing that if $E$ is supersingular, then $\text{End}_{\mathbb{F}_p}E$ is an order in the imagimary quadratic field $\mathbb{Q}(\sqrt{-p})$ , containing $\mathbb{Z}(\sqrt{-p})$ . I don't know why $\sqrt{-p}$ , regarded as an endomorphism on $E$ , equals $\pi$ which means $\pi^2+[p]=0$ . As I know of, if $E$ is supersingular then $[p]=\hat{\pi}\circ\pi$ is purely inseparable and we must have $\pi=\hat{\pi}$ , i.e., $\pi^2=[p]$ .","['finite-fields', 'algebraic-geometry', 'elliptic-curves']"
4367685,Weak topology and weak convergenge in probability spaces,"Let $X$ be a Polish space (metrizable, complete, separable) with $\mathcal{B}(X)$ its borel sigma algebra. Let us consider $\mathcal{P}(X)$ the space of probability measures on $\mathcal{B}(X)$ . We endow it with the weak topology denoted by $\tau^{w}$ given by the functionals \begin{align}
Q\to L_{f}(Q) := \int_{X} f(x) Q(dx),\hspace{0.3cm}f\in C_{b}(X),Q\in \mathcal{P}(X)  
\end{align} with $C_{b}(X)$ the continuous and bounded real functions, that is a base for the weak topology is given by the sets \begin{align}
W(f,x,\delta) : =  \{ \mu\in \mathcal{P}(X),\hspace{0.3cm} |L_f(\mu) - x|<\delta  \}.
\end{align} This topology induces a convergence, and it can be proved that this convergence is equal to the one given by the functionals, that is we say that a sequence $(\mu_n)_{n\in\mathbb{N}}$ in $\mathcal{P}(X)$ converges weakly to $\mu\in\mathcal{P}(X)$ , and we write $\mu_n\rightharpoonup\mu$ if \begin{align}
L_f(\mu_n)=\int_Xf d\mu_n \to \int_Xfd\mu = L_f(\mu),\hspace{0.3cm}\forall f\in C_b(X).
\end{align} Now I want to do what it is written here with the 2-Wasserstein distance, that is to check the mesurability of a markov kernel compounded with a Wasserstein distance, and I want to understand the details. First of all, we are interested in the borel sigma algebra of $(\mathcal{P}(X),\tau^w )$ , let us denote such sigma algebra as $\mathcal{B}(\mathcal{P}(X))$ .
Since it can be proved that $(\mathcal{P}(X),\tau^w )$ is Polish as it is stated in this other question, it has a countable base, so it follow that $\mathcal{B}(\mathcal{P}(X))$ is equal to the smallest sigma algebra that makes all $(L_f)_{f\in C_b(X)}$ measurable, and so we can check the measurability of a Markov Kernel by compounding it with $L_f$ for all $f\in C_b(X)$ , is this right? Now I asked myself something more general.
For example in the book of Villani, Optimal transport, old and new (2008) , chapter 6, it is stated that \begin{align}
\mathcal{P}_p(X) = \{ \mu \in \mathcal{P}(X): \int_X d(x_0,x)^p \mu(dx)<+\infty \}
\end{align} is a Polish space with the $p-Wasserstein$ $distance$ , and he proves that \begin{align}
\mu_n \rightharpoonup \mu, \hspace{0.3cm}and\hspace{0.3 cm}
\int_X d(x,x_0)\mu_n \to \int_X d(x,x_0) \mu \iff W_p(\mu_n,\mu) \to 0.
\end{align} Now I was wondering, he defines another notion of convergence and he proves that the Wasserstein distance metrize such convergence. But what about the weak topology induced by the functional that define such convergence, that is \begin{align}
\{L_f: f\in C_b(X), x \to d(x,x_0)^p, x_0\in X \}?
\end{align} In general if a distance metrize the weak convergence it is not true that it induces the same topoly, I think about $l^1$ where the strong topology and the weak topology induce the same sequences that converge (as said by Brezis), but the strong and the weak topology are different.
In this case we have a distance that induces the same sequences which converge, but the topology a priori may be different.
When Villani talks about continuity and $lsc$ about $W_p$ , in chapter 6, he always intend sequential continuity and sequential semicontinuity right? He never speak about the weak topology or am I wrong?
When he talks about the fact that $\mathcal{P}_p(X)$ is Polish, he intends with respect to the topology induced by the Wasserstein right? Not the weak topology induced by the functionals?
However, we are interested in mesurability, so if we see just the sigma algebra generated by the $W_p$ and the borel sigma algebra on $\mathcal{P}_p(X)$ , they are the same I think, and it can be proved by using exactly the $lsc$ with respect to the weak topology on $\mathcal{P}(X)$ and the fact that such topology is metrizable and separable, as I said above.
Does it follow in some easy way that the borel sigma algebra of the functionals is the same as the borel sigma algebra of the Wasserstein metric? We just need to prove that $W_p$ metrizes the weak convergence and we have the equality between sigma algebras? More generally, let us consider a set $Y$ with some functional $(L_f,f\in \mathcal{F})$ that induces the weak topology $\tau^w$ on $Y$ ,and it induces a weak convergence on $Y$ given by \begin{align}
y_n \rightharpoonup y \iff L_f(y_n)\to L_f(y), \hspace{0.3cm} \forall f\in \mathcal{F}.
\end{align} Let us suppose that we succed to find a distance $d$ that metrize the weak convergence, that is \begin{align}
y_n \rightharpoonup y \iff d(y_n,y) \to 0.
\end{align} When it is true that the borel sets of the weak convergence, the borel sets of the distance $d$ , and the smallest sigma algebra that makes $(L_f,f\in\mathcal{F})$ measurable are equal? That is we have \begin{align}
\mathcal{B}(\tau^w) = \mathcal{B}(d) = \sigma( L_f,f\in\mathcal{F} ) ?
\end{align} I think that we may need that $d$ induces a metric space searable, but what else?","['measure-theory', 'weak-convergence', 'measurable-functions', 'weak-topology', 'probability']"
4367714,The number of intersection graphs of $n$ convex sets in the plane is $2^{\Omega(n^2)}$,"I'm supposed to show that the number of intersection graphs of $n$ convex sets (or $n$ simple curves) in the plane is at least $2^{\Omega(n^2)}$ , but I don't really know how to do this. I know that $2^{\Omega(n^2)}$ is a lower bound for the number of simple arrangements of $n$ pseudolines, and I guess that there could be a connection between intersection graphs and these arrangements (maybe these graphs can somehow be ""extended"" to be arrangements?), but I don't have any concrete ideas. Other than that, I don't see another way to approach this. I was told  that this can be proved for intersection graphs of simple curves as well, but that does not help me come up with any new ideas. So I'm just looking for any advice/hint on how to prove this. Thanks for any help.","['discrete-geometry', 'graph-theory', 'geometry', 'combinatorics', 'discrete-mathematics']"
4367736,"Is the set $\ \left\{ \frac{b-a }{c-a}:\ (a,b,c)\ \text{is a primitive Phythagorean triple with}\ a<b<c\ \right\}\ $ dense in $\ [0,1]\ ?$","Is the set $\ \left\{ \frac{b-a }{c-a}:\ (a,b,c)\ \text{is a primitive Pythagorean triple with}\ a<b<c\ \right\}\ $ dense in $\ [0,1]\ $ and how do you show this? It seems likely true based on glancing at the tree in the picture, however I am not very knowledgeable on the properties of Primitive Pythagorean triples...","['number-theory', 'square-numbers', 'pythagorean-triples', 'elementary-number-theory']"
4367808,Showing that $\{x\in\mathbb{Q}:x‚â§0 \ or \ x^2‚â§2\}$ does not have a least upper bound in $\mathbb{Q}$,"I want to prove that $C=\{x\in\mathbb{Q}:x‚â§0  \ or  \ x^2‚â§2\}$ do not have a least upper bound in $\mathbb{Q}$ The definition I'm working with is that a least upper bound, $b$ has to be an upper bound, so $b\in\mathbb{Q}$ where $b>x \ \forall x\in C$ and that its the smallest upper bound, so if $c$ is some upper bound, then $b‚â§c$ I think the proof is to show that while I can find upper bounds, $b$ , I will always be able to find a smaller one. I can prove that upper bound exists by having $b$ be positive and $b^2>2$ but I would like to know how you would always be able to find a smaller one. I think I want to subtract $1/n$ to the bound for a small enough $n$ to create a smaller upper bound. And I would like to do this by avoiding $\sqrt{2}$ if possible","['irrational-numbers', 'analysis', 'real-analysis', 'upper-lower-bounds', 'rational-numbers']"
4367851,Is it possible to use the L'hopital rule for computing the derivative of this function at x=0?,"The function \begin{equation}
f(x)=\begin{cases} \cos(1/x) & x\neq 0\\ 0 & x=0\end{cases}.
\end{equation} and the function $F(x)=\displaystyle \int ^x_0 f $ And I know that by considering another function $g(x)$ , it's possible to solve that $F'(0)=0$ However, I wonder why I can't use the L'hopital rule for this question. More specifically, If I want to find the derivative of $F(x)$ at $x=0$ , based on the definition of the derivative, I can use $\displaystyle \lim_{x\to0} \frac{F(x)-F(0)}{x-0}=\lim_{x\to0} \frac{F(x)}{x}=\lim_{x\to0}\frac{\displaystyle \int ^x_0 f }{x}=F'(0)$ and since this is a $0/0$ , then I use the L'hopital rule to the nominator and the denominator. The result will be $\displaystyle \lim_{x\to0} \cos \frac{1}{x}$ which indicates that $F'(0)$ doesn't exist. I wonder why this is a wrong procedure, I kind of doubt that $\displaystyle \lim_{x\to 0}\int^x_0 \cos \frac{1}{x}$ is not 0. Is this true? Or are there some other problems with this?","['derivatives', 'analysis']"
4367868,Proving that $\{a\in\mathbb{Q}|a>0$ and $a^2<2\}$ has no least upper bound in $\mathbb{Q}$,"I am going through a proof found in: http://www.math.columbia.edu/~harris/2000/2016Dedcuts.pdf In it he finds a smaller upper bound to the supposed least upper bound: proof However in Step 1, I don't understand how the proof goes from $4\frac{1}{n}-\frac{1}{n^2}$ to it being less than $4\frac{1}{n}-\frac{1}{n}$ , it reasons that $\frac{1}{n^2}‚â§\frac{1}{n}$ but shouldn't that mean subtracting the larger number makes the result smaller than if i were to subtract the smaller number>?
i.e. $x<y$ then $z-x>z-y$ right? But here its saying that $z-y>z-x$ If it is a typo, can the proof still be salvaged?","['proof-explanation', 'proof-writing', 'real-analysis', 'solution-verification', 'inequality']"
4367878,"Integral of exponential raised to rational fraction, $\int_0^{\infty} e^{ - (x^2 +a x + b + c/x) } dx$","I've come upon an integral of an exponential raised to a ration fraction in the form: $$ \int_0^{\infty} e^{- (x^2 + ax + b + c/x)} dx $$ All constants $a,b,c > 0$ . I'm convinced that it converges. I'm curious if there's any analytic expression for this definite integral. Mathematica gives no solution. Similar forms like How to evaluate $\int_{0}^{+\infty}\exp(-ax^2-\frac b{x^2})\,dx$ for $a,b>0$ and $f(x)=\int_{0}^{+\infty} e^{-(t+\frac{1}{t})x}dt$ how to find $f(x)$? use substitution tricks/completing the square to leverage the inverse power of $x$ , which do not work (as far as I've tried) in this specific case. Thank you.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4367960,"If a Taylor series agrees with its function in $U$, that function is analytic in $U$","Assume that $f:\mathbb{R}\rightarrow\mathbb{R}$ coincides with its Taylor series centered at a point $p\in U$ on an open set $U$ , i.e. $$\exists p\in U, \forall x\in U, f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n.$$ Is $f$ analytic on $U$ ? That is: $$\forall p\in U, \exists \varepsilon>0, \forall x\in \mathbb{R}, |p-x|<\varepsilon\Rightarrow f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n?$$","['real-analysis', 'calculus', 'taylor-expansion', 'sequences-and-series', 'analytic-functions']"
4367988,How to find exact value of integral $\int_{0}^{\infty} \frac{1}{\left(x^{4}-x^{2}+1\right)^{n}}dx$?,"When I first encountered the integral $\displaystyle \int_{0}^{\infty} \frac{1}{x^{4}-x^{2}+1} d x$ , I am very reluctant to solve it by partial fractions and search for any easier methods.  Then I learnt a very useful trick to evaluate the integral. Noting that $$I(1):= \int_{0}^{\infty} \frac{d x}{x^{4}-x^{2}+1} \stackrel{x \mapsto \frac{1}{x}}{=} \int_{0}^{\infty} \frac{x^{2}}{x^{4}-x^{2}+1}
$$ Combining them yields \begin{aligned}
I(1)&=\frac{1}{2} \int_{0}^{\infty} \frac{x^{2}+1}{x^{4}-x^{2}+1} d x\\&= \frac{1}{2}\int_{0}^{\infty} \frac{1+\frac{1}{x^{2}}}{x^{2}+\frac{1}{x^{2}}-1} d x \\
&= \frac{1}{2}\int_{0}^{\infty} \frac{d\left(x-\frac{1}{x}\right)}{\left(x-\frac{1}{x}\right)^{2}+1} \\
&= \frac{1}{2}\tan ^{-1}\left(x-\frac{1}{x}\right)_{0}^{\infty} \\
&= \frac{\pi}{2}
\end{aligned} Later, I started to investigate the integrands with higher powers. Similarly, $$
I(2):= \int_{0}^{\infty} \frac{d x}{\left(x^{4}-x^{2}+1\right)^{2}} \stackrel{x \mapsto \frac{1}{x}}{=}\int_{0}^{\infty} \frac{x^{6}}{\left(x^{4}-x^{2}+1\right)^{2}} d x
$$ By division, we decomposed $x^6$ and obtain $$
\frac{x^{6}}{\left(x^{4}-x^{2}+1\right)^{2}}=\frac{x^{2}+1}{x^{4}-x^{2}+1}-\frac{1}{\left(x^{4}-x^{2}+1\right)^{2}}
$$ $$
I(2)=\int_{0}^{\infty} \frac{x^{2}+1}{x^{4}-x^{2}+1} d x-\int_{0}^{\infty} \frac{1}{\left(x^{4}-x^{2}+1\right)^{2}}dx
$$ We can now conclude that $$I(2)=I(1)=\frac{\pi}{2} $$ My Question: How about the integral $$\displaystyle I_{n}=\int_{0}^{\infty} \frac{d x}{\left(x^{4}-x^{2}+1\right)^{n}}$$ for any integer $n\geq 3$ ?","['integration', 'calculus', 'rational-functions', 'reduction-formula']"
4367994,Compute $\iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y$,"I need help computing the following integral: $$I = \iint_R \left|\cos(2x)-\cos(y)\right|\mathrm{d}x\mathrm{d}y$$ Where $R=[0,\pi]\times [0,\pi].$ I plugged this double integral in WA and I got $8$ as the result. Also I did it plotting $\cos(2x)-\cos(y)\geq 0$ and splitting the integral in intervals depending of the absolute value. My doubt is that I tried another way and it didn't work. Consider that $\cos(2x)-\cos(y) = -2\sin\left(\left(\frac{2x+y}{2}\right)\right)\sin\left(\left(\frac{2x-y}{2}\right)\right)$ and the change of variables $u=\frac{2x+y}{2}, v = \frac{2x-y}{2}$ . The absolute value of the jacobian determinant of this transformation is $1$ . Also, given that $0 \leq x \leq \pi$ and $0 \leq y \leq \pi$ , we can compute that $0 \leq \frac{2x+y}{2} = u \leq \frac{3 \pi}{2}$ and $-\frac{\pi}{2} \leq \frac{2x-y}{2}=v \leq \pi$ . In that case, we will get that $$I = \int_0^{\frac{3\pi}{2}} \int_{-\frac{\pi}{2}}^{\pi} \left|-2\sin(u)\sin(v)\right| \mathrm{d}v \mathrm{d}u$$ but the result of the last integral is $18$ according to WA. What is wrong with this method?","['integration', 'multivariable-calculus', 'definite-integrals']"
4367997,"Sufficient statistic for $\theta$ in $N(\theta,\theta)$ model","I recently encountered a MCQ question which goes like this: Let $X_1,X_2,...,X_n$ be independent random samples from $N(\theta,\theta)$ , where both the mean and variance are $\theta$ , where $\theta$ is unknown. Then which of the following statements is/are true? (a) $\sum (X_i)^2$ is sufficient for $\theta$ (b) $\sum (X_i)$ is sufficient for $\theta$ (c) [ $\sum (X_i)$ , $\sum (X_i)^2$ ] is sufficient for $\theta$ (d) Sufficient statistics does not exist. My Attempt: Using the factorization theorem, $$f(x_i,\theta) = \frac{1}{\sqrt{2\pi\theta}} e^\frac{-(x_i-\theta)^2}{2\theta}$$ $$\prod f(x_i,\theta) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\theta}} e^\frac{-(x_i-\theta)^2}{2\theta}
                      = (\frac{1}{\sqrt{2\pi\theta}})^n e^\frac{-\sum_{i=1}^{n} (x_i-\theta)^2}{2\theta} \\
                      =(\frac{1}{\sqrt{2\pi\theta}})^n  (e^\frac{-\sum_{i=1}^{n} (x_i)^2}{2\theta}e^\frac{-\theta}{2})   e^{\sum_{i=1}^{n} (x_i)}  
= f_1(t,\theta) f_2(x_i)$$ where $T=\sum_{i=1}^{n}(X_i)^2$ . Thus, the statistic $T$ is sufficient for $\theta$ , implying (a) is ture and (d) is false. However, the answer for the MCQ is given as (a), (b) and (c). I am not able to prove (b) and (c) using the factorization theorem. Please help.","['statistical-inference', 'statistics', 'probability-distributions', 'normal-distribution']"
4368022,Maximum value of $ (1- \tan A)(1- \tan B)(1- \tan C)$,"Assume that we have a triangle with angles A, B, C. so $A+B+C = \pi$ . What is the maximum value of $(1- \tan A)(1- \tan B)(1- \tan C)$ ? My try:
I know that given $A+B+C = \pi$ we have that $\tan A + \tan B + \tan C = \tan A \tan B \tan C$ and $\tan C = - \tan(A+B)$ Using the equalities above, I came to the result $(1- \tan A)(1- \tan B)(1- \tan C) = 1 - 2(\tan A+ \tan B +\tan C )+(\tan A \tan B + \tan A \tan C +\tan B \tan C)$ and this is where I'm stuck, using the second equality I didn't get anything helpful. I'm trying to solve this without using differentiation. any hints would be appreciated.","['trigonometry', 'maxima-minima', 'calculus', 'geometry']"
4368047,"Show that U,V and H are colinear","We are given a regular icosagon as below: I wanna prove that the red line exists. I know that $U$ is the incenter of $\triangle TLB$ ( $T,U,G$ are collinear)
I know that $V$ is the incenter of $\triangle GMC$ ( $M,V,E$ are collinear) I tried to use Pascal theorem but it probably has to be used more than once and I have no clue which one would be useful... Maybe letting line $UV$ hit the circle in two random points may help but I could not develop it. There are 6 pairs of points to choose from in order to get the Pascal hexagons.","['euclidean-geometry', 'projective-geometry', 'geometry', 'polygons']"
4368061,"Is the angle of the 345 triangle (pythagorean triple) related to the Geometric Progression. 4, 2, 1, 1/2, 1/4?","I realised recently I could use this equation for the pythagorean triples. As you can see the 345 triangle seems to be special? as we can create it using x=2 AND x=3. I know the 345 triangle is related to this golden triangle. Using the theorem of geometry called the Power of a Point . why is there a square-root relationship here**?** Is it correct to say that the angle of the 345 triangle is related to the Geometric Progression. 4, 2, 1, 1/2, 1/4 where x = (a+b) = 2 and 1/x = (a-b) = 1/2, x^2 = 4 and 1/x^2 = 1/4, x^4 = 16 and 1/x^4 = 1/16 and therefore the square as shown below**?** We find the 345 triangle in the construction of the square as shown below.","['trigonometry', 'pythagorean-triples', 'geometry']"
4368081,What actually is a function?,"I've got confused about the definition of a function. As what the German mathematician Peter Dirichlet said is: If a variable y is so related to a variable x that whenever a numerical value is assigned to x, there is a rule according to which a unique value of y is determined, then y is said to be a function of the independent variable x. In this way, a function is a variable because the variable y is a function. But here I found another definition: A function is a relationship between a set of inputs and outputs, where each input produces only one output. And it says that a function is a relationship . I even saw that such a function $f : X \to Y$ can even be defined as a set of ordered pair $\{(x,y)|x \in X \land y \in Y \}$ . Doesn't this mean that a function is a set ? So what actually is a function? Is it a variable or a relationship, or another kind of mathematical object?",['functions']
4368089,"Proof verification: $(0,1)$ is not compact, by definition.","I'm toying around with topology, specially about compactness, then I want to know if my proof of the non-compactness of $(0,1)$ is alright. Consider the topological space $(\mathbb{R},\tau_{e})$ , the real line with the euclidean topology.
I want to prove the interval $(0,1)$ is not compact by definition, so it is enough to show an open cover where no finite subcover can exist. Consider the set of intervals $\left\{ \left(0, 1 - \dfrac{1}{n} \right)\right\}_{n=1}^{\infty}$ . I say they form an open cover. I already proved open intervals are open, so it remains to prove they are indeed a cover, i.e., that $(0,1) \subseteq \displaystyle \bigcup_{n=1}^{\infty} \left( 0, 1 - \dfrac{1}{n} \right)$ . In fact, take $x \in (0,1)$ . Then $x< 1$ , so $\dfrac{1}{1-x}>0$ , then, by the archimedean property, there is $N$ natural such that $N > \dfrac{1}{1-x}$ , so $\dfrac{1}{N} < 1-x$ and then $x < 1 - \dfrac{1}{N}$ . So $x \in \left ( 0, 1 - \dfrac{1}{N} \right)$ , and of course this is one of the intervals in the set of intervals, hence the set is a cover. Now I say that it is impossible to extract a finite subcover, because if $M$ is a natural such that $(0,1) \subseteq \displaystyle\bigcup_{n=1}^{M} \left(0, 1 - \dfrac{1}{n} \right) =\emptyset \cup\left(0,\dfrac12 \right) \cup \dots \left(0, 1 - \dfrac{1}{M} \right)$ , then take $x$ such that $1 - \dfrac{1}{M}<x<1$ (which can be taken by the density of real numbers), so $x$ is an element of $(0,1)$ that is not in the finite subcover, hence the impossibility. QED","['general-topology', 'solution-verification']"
4368168,"Consider a function $f:\{{1,2,3,\cdots,13}\} \rightarrow \{{1,2,3,\cdots,9}\}$ Given that function is surjective and non‚Äìdecreasing","Consider a function $f:\{{1,2,3,\cdots,13}\}$$\rightarrow$ $\{{1,2,3,\cdots,9}\}$ Given that function is surjective and non‚Äìdecreasing, the probability that $f(7)=4$ is? My solution: Let $x_k$ denotes number of times $k$ appears in range. where $1\leq k \leq 9$ Then number of non decreasing and onto function $f:\{{1,2,3,\cdots,13}\}$$\rightarrow$ $\{{1,2,,3,\cdots,9}\}$ is equal to number of positive integral solution of equation $x_1+x_2+x_3+\cdots+x_9=13$ i.e., ${13-1\choose 9-1}= {12 \choose 8}$ . Total Function: ${12\choose4}$ Number of function satisfying above property is i.e., $f(7)=4$ is number of positive integral solution of $x_1+x_2+x_3+x_4=7$ and $x_4+x_5+x_6+x_7+x_8+x_9=6$ that is ${6 \choose 3}\times{6\choose 5}$ . Note: $x_4 \geq 0$ in $x_4+x_5+x_6+x_7+x_8+x_9=6$ and rest all $x_i \geq 1$ I've obtained the correct result i.e., $\frac{{6\choose 3}\cdot {6\choose1}}{{12\choose 4}}$ as given in answer key. I've edited the solution as commented by @mathlover. Also is there any other approach for this problem.","['combinations', 'functions', 'combinatorics']"
4368212,Find derivative of inverse of function $y=2x^3-6x$ and calculate it's value at $x=-2$.,Find derivative of inverse of function $y=2x^3-6x$ and calculate it's value at $x=-2$ . My Approach: We know that $(f^{-1}(f(x)))=x$ Taking derivative both side $(f^{-1}(f(x)))' \cdot f'(x)=1$ $(f^{-1}(f(x)))'=\frac{1}{f'(x)}=\dfrac{1}{6x^2-6}$ We want to find $f^{-1}(-2)$ so we must have $f(x)=-2$ i.e. $\quad$$2x^3-6x=-2$ $\implies$ $2x^3-6x+2=0$ I can't find any integer root from here. But given answer is $\frac{1}{18}$ I know other method to solve this problem but can we solve using my approach used above?,"['calculus', 'inverse-function', 'derivatives']"
4368317,Why does stereographic projection appear here?,"I'm working on some calc III problems, and found that the unit tangent vector of $$\langle t + \dfrac{1}{t}, 2\ln(t)\rangle $$ is $$\left\langle \dfrac{t^2 - 1}{t^2 + 1}, \dfrac{2t}{t^2 + 1} \right\rangle.$$ This is weird to me! I recognize this expression as the same equation as projecting the line onto the circle, i.e. the two-dimensional stereographic projection. Is this a coincidence, or is there something special happening here? I'm very suspicious.","['multivariable-calculus', 'calculus', 'stereographic-projections']"
