question_id,title,body,tags
4693804,"How might I figure out the $n$th term, and the general Maclaurin/Taylor series, for $\sin^3 x$?","I'm trying to work out the solution to finding the Maclaurin series of $\sin^3x$ . While I am used to finding terms from exponential transformations, I've mainly had experience doing it with forms related to $e^x$ and $\frac{1}{1-x}$ . I've gotten the first terms from this work: $$\frac{d}{dx}(\sin^3x)(0) = 3\sin^2x\cos x = 3\sin{0}\cos{0} = 0$$ $$\frac{d^2}{dx^2}(\sin^3x)(0) = 3(\sin^2x\cos{x} -\sin^3x) = 0$$ $$\frac{d^3}{dx^3}(\sin^3x)(0) = 3\left(2\cos \left(2x\right)\cos \left(x\right)-\sin \left(x\right)\sin \left(2x\right)-3\sin ^2\left(x\right)\cos \left(x\right)\right) = 6$$ $$\frac{d^4}{dx^4}(\sin^3x)(0)= 3\left(-8\sin \left(2x\right)\cos \left(x\right)-4\cos \left(2x\right)\sin \left(x\right)+3\sin ^3\left(x\right)\right) = 0$$ $$\frac{d^5}{dx^5}(\sin^3x)(0)= 3\left(-8\left(2\cos \left(2x\right)\cos \left(x\right)-\sin \left(x\right)\sin \left(2x\right)\right)-4\left(-5\cos \left(x\right)+6\cos ^3\left(x\right)\right)+9\sin ^2\left(x\right)\cos \left(x\right)\right) = -60$$ $$\frac{d^7}{dx^7}(\sin^3x)(0)=3\left(98\cos \left(2x\right)\cos \left(x\right)-49\sin \left(x\right)\sin \left(2x\right)-324\cos \left(x\right)+408\cos ^3\left(x\right)-27\sin ^2\left(x\right)\cos \left(x\right)\right) = 546$$ Essentially I'm getting these terms after working out the equation for the first 7 derivatives. $$\frac{6}{3!}x^3-\frac{60}{5!}x^5+\frac{546}{7!}x^7+ \dots$$ I know the best way to write the series would be as $\displaystyle\left(\sum_{n=0}^\infty \frac{(-1)^nx^{1+2n}}{(1+2n)!}\right)^3$ , but I'm trying to figure out a  way to get a nth term as well as a simpler summation within the first degree. Is it posible, and if so, how would I go along with that? Thank you so much!","['calculus', 'taylor-expansion', 'sequences-and-series']"
4693832,Spacial regularity of heat equation with zero initial condition,"Consider the heat equation with zero Dirichlet boundary condition and zero initial data in time interval $[0,T]$ : $$
\partial_t u-\Delta u=f\;\;\text{in $\Omega$};\quad u(0)=0;\quad u|_{\partial \Omega}=0
$$ If we assume that $f\in L^2H^2(\Omega)$ (where $L^2H^2(\Omega)$ is an abbreviation of the Bochner space norm $L^2(0,T;H^2(\Omega)$ ) and $f(t)\in H^1_0(\Omega)$ for almost every $t$ , can we conclude the following estimate? $$
\|\partial_t u\|_{L^2H^2(\Omega)}\leq C\|f\|_{L^2H^2(\Omega)}.
$$ My idea is to intuitively take operator $\Delta$ on the both side of the original heat equation, thus consider the solution $v$ of the following heat equation $$
\partial_t v-\Delta v=\Delta f\;\;\text{in $\Omega$};\quad v(0)=0;\quad v|_{\partial \Omega}=0
$$ then let $u=(\Delta)^{-1} v$ , $u$ seems to be the solution of the original heat equation and satisfies the estimate above. But it seems something went wrong here, could you help with me with finding the mistake?","['semigroup-of-operators', 'functional-analysis', 'heat-equation', 'partial-differential-equations']"
4693835,Solving a coupled system of first order homogeneous differential equations with variable coefficients,"Given the coupled system of first order homogeneous ordinary differential equations with variable coefficients. $$    
\frac{dx}{dt} = a(t)z(t)
$$ $$
\frac{dz}{dt} = -a(t)x(t)
$$ How is the general solution below derived? $$
x(t) = c_1 \cos\left(\int_1^t a(\tau) d\tau \right)+c_2 \sin\left(\int_1^t a(\tau) d\tau\right)
$$ $$
z(t) = -c_1 \sin\left(\int_1^t a(\tau) d\tau \right)+c_2 \cos\left(\int_1^t a(\tau) d\tau\right)
$$ I thought it might be something like guessing the solution takes the form $$
x(t) = c_1 \cos\left(f(t)\right)+c_2 \sin\left(f(t)\right)
$$ $$
z(t) = \frac{1}{\left(\frac{df}{dt}\right)} \frac{dx}{dt} = -c_1 \sin\left(f(t)\right)+c_2 \cos\left(f(t)\right)
$$ And $\frac{df}{dt} = a(t)$ But I imagine there is a general approach to this type of problem?","['systems-of-equations', 'circles', 'ordinary-differential-equations']"
4693864,Range of $x \mapsto\dfrac{e^{2x} - 1}{e^{2x}+1}$,"I have the function $$ x \mapsto \frac{e^{2x} - 1}{e^{2x}+1} $$ My method doesn't include logarithmic algebra. Instead I try to manipulate the numerator so in order to get $1 - \frac2{e^{2x}+1}$ . Now I know that range of $e^{2x}+ 1$ which is $(2,\infty)$ . But I don't know the steps to get range of Y. Can someone please explain what $1 - 2/(2,\infty)$ is ?","['calculus', 'functions']"
4693882,Find limit with sum using integration,"I'm working on this problem: Find the limit $$
\lim_{n \to \infty} \sum_{k=5n}^{7n} \frac{n}{k^2+n^2}
$$ My initial thought is to turn it into an integral and work from there, but I'm not sure how to do this, especially since it contains $k=5n$ .","['integration', 'summation', 'riemann-sum', 'limits', 'riemann-integration']"
4693908,Distributional differential equations (delta) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm learning the basics of distributional theory. Reading a book, I've found this exercises: Find the general solution of $$(x-1)T= \delta$$ $$(x-1)T =\delta'$$ I've tried different starting points but no solutions. Please, can you help me?","['dirac-delta', 'ordinary-differential-equations', 'distribution-theory', 'partial-differential-equations', 'derivatives']"
4693931,Prove that two finite sets have the same cardinality if and only if there exists a bijection between them,"The textbook ""Discrete Mathematics with Applications"" by Thomas Koshy states a theorem (3.5): For function $f: X \rightarrow  Y$ , where $X$ is the finite domain and $Y$ is the finite co-domain Two finite sets have the same cardinality if and only if there exists
a bijection between them. I can prove that if $f$ is a bijective function then $|X| = |Y|$ .
However, I am struggling to understand Koshy's proof for the biconditional part.
For example, in the case below where $|X| = |Y|$ , the sets have the same cardinality but are not bijective: $X = \{-2,-1,1,2\}$ $Y = \{0, 1, 2, 4\}$ $f(x) = x^2$ Any ideas if Koshy's proof is wrong, or if not, how one would go about proving this? edit: adding Koshy's proof below Let $X$ and $Y$ be two finite sets with $|X| = m$ and $|Y| = n$ .
Let $X = \{x_1, . . . , x_m\}$ .
Let $f : X â†’ Y$ be bijective.
Since $f$ is injective, $f(x_1), . . . , f (x_m)$ are $m$ distinct elements in $Y$ . Consequently, $m \leq n$ . Since $f$ is surjective, every element $y$ in $Y$ has at least one input in $X$ , so $n \leq m$ . Thus $|X| = |Y|$ . Conversely, suppose $m = n$ and $Y = \{y_1, . . . , y_m\}$ . Define a function $f : X \rightarrow Y$ by $f(x_i) = y_i$ for every $i$ . We will now show that f is injective. Let $x_j$ and $x_k$ be two elements in $X$ such that $f (x_j) = f (x_k)$ . Then, by definition, $y_j = y_k$ ; so, $j = k$ and hence $x_j = x_k$ . Therefore, $f$ is injective and hence, by Theorem 3.4, $f$ is bijective. Theorem 3.4: Let $X$ and $Y$ be any two finite sets with $|X| = |Y| = n$ . A function $f : X \rightarrow Y$ is injective if and only if $f$ is surjective.","['elementary-set-theory', 'functions']"
4693934,Why must integral manifolds be immersed submanifolds?,"Given a manifold $M$ and a distribution $D \subset TM$ , an integral manifold of $D$ is defined as a nonempty immersed submanifold $N \subset M$ such that $T_pN = D_p$ for all $p \in N$ . Why are we not able to define an integral manifold as an embedded submanifold? My guess is that embeddings are too strict, but is there an intuitive geometric reason behind this?","['submanifold', 'smooth-manifolds', 'differential-geometry']"
4693946,Let $x_n := (I +n S)^{-1} f$ for all $n$. How to prove that $(x_n)$ is convergent?,"Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space. Let $S :H \to H$ be a bounded linear operator such that $\langle Su, u \rangle \ge 0$ for all $u \in H$ . Then the map $I + t S$ is bijective for all $t>0$ . Here $I:H \to H$ is the identity map. Fix $f\in H$ and let $x_n := (I +n S)^{-1} f$ for all $n \in \mathbb N$ . I'm trying to prove that Theorem $(x_n)$ is convergent in $H$ . It suffices to prove that $(x_n)$ is a Cauchy sequence. Clearly, $$
x_n+n Sx_n = f = x_m + m S x_m
\quad \forall m,n \in \mathbb N.
$$ Then I don't know how to move forward. Could you elaborate how to prove above theorem?","['cauchy-sequences', 'hilbert-spaces', 'functional-analysis', 'sequences-and-series', 'convergence-divergence']"
4693947,How to find all polynomials $p \in \Bbb C [x]$ such that $x^3 p'(x) = p(x)^2$?,"How to find all polynomials $p \in \Bbb C [x]$ such that $x^3 p'(x) = p(x)^2$ ? Since $\operatorname{degree}(p(x)) = y$ , and $\operatorname{degree}\left(p'(x)\right) = y-1$ , I tried to deduce what degree the polynomial should have. If $\operatorname{degree}(p(x))=2$ , then $\operatorname{degree}(p(x)^2)=4$ . And $\operatorname{degree}(p'(x))=1$ , then $\operatorname{degree}(x^3 p'(x))=4$ . So the polynomial should have degree $2$ . I'm not sure if this is the right approach. How can I complete the proof from this point?","['derivatives', 'polynomials', 'complex-numbers']"
4694048,Page 21 of Enderton's Elements of Set Theory - Subset axioms,"To my understanding the subset axiom is just an axiom that says that given a set and a property, we can have a set defined as all the members of the original set for which the property is true. If I had to put this in symbols it would be something like: $\forall A \exists B \forall x (x \in B \iff x \in A \land P(x))$ However in Enderton's book the subset axiom is not only one but multiple: Subset Axioms: For each formula __ not containing $B$ the following is an axiom: $\forall t_{1}... \forall t_{k} \forall c \exists B \forall x (x \in B \iff x\in c \land \text{__})$ I guess my question is why is it done in this manner? Why multiple axioms instead of just one? and more importantly, what are those $t_{1}...t{k}$ Enderton is taling about. This whole section has me very confused I would appreciate any help in understanding why the Subset Axioms is formulated the way it is and what are those $t_{1}...t{k}$ supposed to represent? Thanks.",['elementary-set-theory']
4694057,Is $\cos^2A+\cos^2B+\cos^2C+2 \cos A \cos B \cos C=1$ both Necessary and Sufficient for a triangle,"When $P^2+Q^2+R^2+2PQR=1$ , one can show that all of $|P|,|Q|,|R|$ are either $\ge 1$ or $\le 1$ . One of the consequences of this is the trigonometric identity in a triangle: $$\cos^2 A+ \cos^2 B+ \cos^2 C+2 \cos A \cos B \cos C=1.$$ We want to point out that it is only necessary but not sufficient for forming a triangle. Given $\cos A, \cos B$ , we can set up the quadratic $$\cos^2 C + (2\cos A \cos B) \cos C+ \cos^2 A+ \cos^2 B-1=0,\quad (*)$$ We get two roots for $C: C_1, C_2$ , then either  ABC $_1$ or ABC $_2$ , will be a triangle such that $A+B+C=\pi$ . So if three numbers $\cos A, \cos B. \cos C$ satisfy (*) a triangle may or may not be formed. On the other hand the identity $\tan A+ \tan B + \tan C=\tan A \tan B \tan C$ is both necessary and sufficient for the formation of a triangle. Question no. 1: Check if $\cos A=1/2, \cos B=3/5, \cos C=3/7$ would form a triangle? Question no. 2: If $\cos A=\frac45, \cos B=\frac37$ and the triangle $ABC$ is possible, find the value of $\cos C$ .","['trigonometry', 'triangles']"
4694060,Unable to reason about game theory question involving marbles,"Here is the prompt: You choose to put 1-100 marbles in a bag. Your opponent also has the
same option. Then we pick one marble from the bag. If your marble gets
picked from the bag you get the amount of marbles you did not put in
as your payout. So if you put in 20 marbles and your marble gets
picked you get 80. What is the optimal number of marbles to put in? I understand the basics of game theory and Nash equilibria but I don't know how to reason about a question like this where the opponent has so many options. Here is what I have so far: If the opponent picks $k$ marbles then I should pick $j$ marbles to maximize my payout, where the payout is given by $$\frac{j}{j + k} (100 - j)$$ using the definition of expected value. However, clearly the value of $j$ that optimizes this function varies with respect to $k$ , the number of marbles the opponent picks. I am guessing I need to use the value of $k$ that the opponent would pick if they were to maximize their own payout, but I don't know how to find that. Any direction would be appreciated, thanks.","['game-theory', 'probability']"
4694098,How to solve $\frac{\partial^2\Psi}{\partial x^2} + \frac{1}{1+t}\Psi = \frac{\partial \Psi}{\partial t}$?,"Is there a solution using $\Psi(x,t)$ such that $$\frac{\partial^2\Psi}{\partial x^2} + \frac{1}{1+t}\Psi = \frac{\partial \Psi}{\partial t}$$ ? I have tried $\Psi=te^xe^t$ , $\Psi=te^xe^{t^2}$ and other similar variations, but could not get the powers of $x$ and $t$ to agree on both sides of the equation. Is there a general method to solve this type of differential equation in 2 variables?","['multivariable-calculus', 'partial-differential-equations']"
4694133,"$f(x)=\int_{0}^{x}|t-f(t)|\,\mathrm dt$. How to deal with mod?","Let $f:\Bbb R\rightarrow \Bbb R$ be a differentiable function which satisfies $$
\forall \ x\in \Bbb R,\ f(x)=\int_{0}^{x}|t-f(t)|\,\mathrm dt,
$$ then how do we prove that the function $f$ is monotonic, onto and an odd function. I'm unable to deal with the modulus function. Is there anyway to remove the mod and deal with a linear differential equation. If we do not remove the mod, there is also the question of whether RHS is differentiable or not? If it is, then we could write $f'(x)=|x-f(x)|$ which would prove that f is monotonous and onto, but we still have a problem with odd. How to deal with mod in differential equations?","['integration', 'derivatives', 'ordinary-differential-equations', 'contest-math']"
4694176,"Hartshorne AG, proof of proposition 5.7 and 5.8: ""The question is local, so we may assume $X$ is affine ...""","On several instances, Hartshorne states that certain questions are ""local"". What is the reasoning behind it? Some instances in the text by Hartshorne, algebraic geometry, occur on p. 114 f., proof of proposition 5.7, or proof of proposition 5.8. In section 5 chapter 2, quasi-coherent sheaves are discussed. Quasi-coherent sheaves deal with $\mathcal{O}_X$ -modules, which on some open covers of $X$ are isomorphic to a sheaf associated to a $A_i$ -module for some appropriately chosen ring $A_i$ . The elements of the open covers each equal the spectrum of a ring $A_i$ . Proposition 5.7 deals with a scheme $X$ , which is not necessarily affine, and states that kernels, cokernels and images of morphisms between quasi-coherent sheaves $\mathcal{F}$ and $\mathcal{G}$ on $X$ are quasi-coherent. Hartshorne simply states in the proof that by localness of the question, affineness can be assumed. I interpret it in the way that quasi-coherence requires that the restriction of a sheaf of modules to each element of some open covering is isomorphic to $\widetilde{M_{i}}$ where $M_{i}$ is a module over a ring $A_{i}$ with $\text{Spec}(A_{i})=U_{i}$ . An interpretation of the Hartshorne's explanation is that the property in question can be checked on the stalks. By proposition 1.1., p. 63 a morphism of sheaves on $X$ is an isomorphism if and only if the induced map on the stalks at $P$ is an isomorphism for each element $P$ of the topological space $X$ . Certainly for an affine structure, the stalk of a sheaf of $A_i$ -modules at $\mathfrak{p}$ is isomorphic to the localisation $(\widetilde{M_i})_{\mathfrak{p}}$ , where $M_i\cong \Gamma(\text{Spec}(A_i),\widetilde{M_i})$ (proposition 5.1 (b) and (d)). An alternative interpretation might be that localness refers to the open covers $\{U_i\}_i$ , but I am sightly doubtful about the latter interpretation. In part (a) of  proposition 5.8, the assumption of quasi-coherence of $\mathcal{G}$ is a local property on $X$ and $Y$ . Also the claim regarding $f^{*} \mathcal{G}$ relates to quasi-coherence. I wonder why in proposition 5.8 part (c) it is claimed in the proof that the question is local on $Y$ only. Especially part (c) of proposition 5.8 is unclear to me, but the other occurences of the approach ""the question is local, so affineness can be assumed"", remain unclear to me, as well.",['algebraic-geometry']
4694185,Solving $\sin \left( x+\frac{\pi }{3} \right) -\sin \left( x-\frac{\pi }{3} \right) = \sin\left(x\right)$,"I'm struggling with solving the following trigonometric equation: $$
\sin \left(x+\frac{\pi}{3}\right)-\sin \left(x-\frac{\pi}{3}\right)=\sin x,
$$ and I need some help. I tried to use the sum and difference of sines formula, which states that $\sin(a \pm b) = \sin a \cos b \pm \cos a \sin b$ , to simplify the left-hand side of the equation: \begin{align}
\sin \left(x+\frac{\pi}{3}\right)-\sin \left(x-\frac{\pi}{3}\right) &= \left(\sin x \cos \frac{\pi}{3} + \cos x \sin \frac{\pi}{3}\right) - \left(\sin x \cos \frac{\pi}{3} - \cos x \sin \frac{\pi}{3}\right)\\
&= 2\cos x \sin \frac{\pi}{3}\\
&= \sqrt{3} \cos x,
\end{align} but I'm not sure if this is correct. If it is, then I can rewrite the equation as: $$
\sqrt{3} \cos x = \sin x,
$$ which leads to: $$
x = \arctan \sqrt{3} = \frac{\pi}{3},
$$ or $$
x = \pi - \arctan \sqrt{3} = \frac{2\pi}{3}.
$$ However, I'm not sure how to proceed from here to find the sum of the solutions that lie in the interval $[-\frac{4\pi}{3}, 2\pi]$ . Can anyone please help me with this? Thank you in advance!",['trigonometry']
4694201,How to solve $\frac{\partial^2\Psi}{\partial x^2} + \frac{x^2}{1+t^2}\Psi = \frac{\partial \Psi}{\partial t}$?,"Is there a solution using $\Psi(x,t)$ such that $$\frac{\partial^2\Psi}{\partial x^2} + \frac{x^2}{1+t^2}\Psi = \frac{\partial \Psi}{\partial t}$$ ? This is a follow-up question to this question which solves $\frac{\partial^2\Psi}{\partial x^2} + \frac{1}{1+t^2}\Psi = \frac{\partial \Psi}{\partial t}$ using separation of variables method $\Psi = X(x)T(t)$ . Unfortunately, the separation of variables method does not seem to work for solving this modified form of $\frac{x^2}{1+t^2}\Psi$ ?","['quantum-mechanics', 'multivariable-calculus', 'partial-differential-equations']"
4694207,"Prove: if $ax^2-bx+c$ and $dx^3-bx+c$ have a common factor, then $a^3-abd+cd^2=0$.","Prove: if $ax^2-bx+c$ and $dx^3-bx+c$ have a common factor, then $a^3-abd+cd^2=0$ . Not really sure how to proceed. I know that first I have to find the common factor. I guess it is $x-c$ because both expressions end with the constant $c$ . Then plug in $x=c$ to get $ac^2-bc+c=0, c^3d-bc+c=0$ . Hence: $ac^2=c(b-1) $ $c^3d=c(b-1)$ $ac^2=c^3d \Rightarrow a =cd $ Plug into the original third expression: $(cd)^3-(cd)bd+cd^2=0 \Rightarrow c^3d^3-bcd^2+cd^2=0 \Rightarrow c^3d^3=cd^2(b-1) \Rightarrow c^2d=b-1$ This is greatly simplified, but it doesn't prove it is equal to $0$ . I think it would be helpful to know the proper way to first find the common factor instead of just guessing. Thanks!","['algebra-precalculus', 'roots']"
4694211,Infinitely Divisible Distribution and LÃ©vy-Khinchin representation,"I am studying the book Probability Theory by A. Klenke (3rd Edition). I do not understand the construction of a process from which to obtain the LÃ©vy-Khinchin representation for an infinitely divisible distribution on the real numbers. (pag. 375 of the book). In particular, I do not understand how the three points below lead to the constraint $\int \text{min}(1,x^2)v(dx)<\infty$ on the so called canonical measure $v$ of the representation. He starts by defining a real random variable as the sum of independent random variables: $$X=b+X^N+X_0+\sum_{k=1}^{\infty} (X_k-\alpha_k) $$ where $b\in\mathcal{R}$ $X^N=\mathcal{N}_{0,\sigma^2}$ for some $\sigma^2\geq0$ $P_{X_k}=\text{CPoi}_{v_k}$ , where $v_k$ is the intensity measure of a Compound Poisson Distribution concentrated on $I_k:=(-1/k,-1/(k+1)] \cup [1/(k+1),1/k)$ (convention $1/0=\infty$ ) $ \alpha_k=E[X_k]=\int x v_k(dx)$ My questions: For the series to converge is required, as an application of the Kolmogorov's Three Series Theorem, that: $$\sum_{k=1}^{\infty} \text{Var}[X_k]<\infty \quad(1)$$ I know and tried to apply this result to $Z_k=X_k-\alpha_k$ : if $Z_k$ are independent r.v. with zero expectation and if $\sum_{k=1}^{\infty} \text{Var}[Z_k]<\infty$ then $\sum_{k=1}^{\infty} Z_k<\infty$ a.s. How is this result related to the Kolmogorov's theorem? $\text{Var}[X_k]=\int x^2 v_k({dx})$ . How is it possible since, if I understood well, the $X_k$ has not zero expectation in general? Given 2) and letting $v=\sum_{k=0}^{\infty} v_k$ , it is said that (1) is equivalent to $\int_{(-1,1)} x^2v(dx)<\infty$ and that, since $v_0$ is always finite, this is equivalent to $\int \text{min}(1,x^2)v(dx)<\infty$ . What is the meaning of the last integral, as a restriction on the possible $v$ ? In particular, I do not understand why the number $1$ shows up in the minimum function and its role in the costruction of the process. Also, is $v_0$ finite because the definition of a Compound Poisson distribution states that (pag. 369)? Definition: Poisson Compound Distribution $$\text{CPoi}_v:=e^{-v(\mathcal{R})}\sum_{n=0}^{\infty}\frac{v^{*n}}{n!}$$ where $v$ is a finite measure on the real numbers. Thanks for the help.","['levy-processes', 'convergence-divergence', 'variance', 'probability-theory']"
4694282,Why isn't $SO(3)$ isomorphic to $S^2\times S^1$?,"Here is my derivation: An element in $SO(3)$ can be written as $(\vec{v}_1,\vec{v}_2,\vec{v}_3)^T$ with $\{\vec{v}_i\}$ an oriented orthonormal set of basis. The set of possible $\vec{v}_1$ forms an $S^2$ , and the choice of $\vec{v}_2,\vec{v}_3$ forms an $S^1$ . Therefore, $SO(3)$ is just $S^2\times S^1$ . I know this must be incorrect since $\pi_1(SO(3))=\mathbb{Z}_2$ while $\pi_1(S^2\times S^1)=\mathbb{Z}$ , but I don't know where did I go wrong.","['orthogonal-matrices', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4694459,Why is this constant eliminated in the u-substitution in this proof about the normal distribution?,"I'm working through a statistical theory text and have this proof that if $X$ is a normally distributed random variable then the random variable $Z = (X - \mu)/\sigma$ has a n(0,1) distribution. The proof is as follows: $$P(Z\leq z) = P(\frac{X-\mu}{\sigma}\leq z)$$ $$= P(X \leq z\sigma + \mu)$$ $$=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{z\sigma + \mu}e^{-(x-\mu)^2/(2\sigma^2)}dx$$ $$=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}e^{-t^2/2}dt$$ (The book uses t instead of u for the u-substitution, I'm guessing because it would look too similar to $\mu$ ) I understand all of the moves in this, except the removal of the sigma constant outside of the integral going from steps 3 to 4. The substitution is $t=\frac{x-\mu}{\sigma}$ , which makes sense to me within the integrand and in changing the upper parameter of the integral, but I can't figure out at all why or how it would cancel out the sigma outside of the integral. I appreciate any help.","['integration', 'calculus', 'normal-distribution', 'statistics']"
4694468,Probability convergence of a martingale defined as iid random variable product,"Let $(\beta_n)_{n \geq 1}$ be positive independent and identically distributed random variables with $\mathbb{E}[\beta_1] = 1$ and $\mathbb{P}[\beta_1 < 1 ] > 0$ .
Define the martingale $M_n = \beta_1 \cdot \beta_2 \cdots \beta_n$ . I have to prove that $M_n \to 0$ almost surely (a.s.). Below what I've tried so far. First attempt. Given that $(M_n)$ is bounded in $L^1$ the pointwise limit exist a.s..
In order to compute the limit let's study the convergence in probability: by the Paul Levy theorem it suffices to study the characteristic function pointwise convergence: $$
\varphi_{M_n}(t) = \mathbb{E}\left[ \exp(itM_n) \right] = \mathbb{E}\left[ \exp\left( it \prod_{i=1}^n \beta_i \right) \right]
$$ but I don't know how to go ahead. Second attempt. Let's try to use the Borel Cantelli lemma.
It suffices to show that $$
\forall \epsilon > 0 \quad \mathbb{P}[M_n < \epsilon \text{ eventually}] = 1.
$$ So $$
\mathbb{P}[M_n < \epsilon \text{ eventually}] = 1 - \mathbb{P}[M_n > \epsilon \text{ infinitely often}].
$$ Let's study $$
\sum_{n \geq 0} \mathbb{P}[M_n > \epsilon]
$$ but I can't show that the serie converges. Any hints would be appreciated. In particular I cannot figure out how to use the hypothesis $\mathbb{P}[\beta_1 < 1 ] > 0$ .","['martingales', 'pointwise-convergence', 'probability']"
4694530,Fractional Brownian motion of Riemann-Liouville type is not a semimartingale,"Given a filtered probability space $(\Omega,\mathcal{F},\mathbb{F},\mathbb{P})$ satisfying the usual conditions, $B$ a standard one-dimensional Brownian motion and $H\in(0,1/2)$ . Consider the process $X$ given by $$
X_t = \int_0^t \frac{(t-s)^{H-1/2}}{\Gamma(H+1/2)} d B_s,
$$ where $\Gamma$ denotes the gamma function.
The process $X$ is called fractional Brownian motion of Riemann-Liouville type with Hurst parameter $H.$ I have read in various articles that it is well-known that $X$ is not a semimartingale. I am trying to understand why this statement is true. However, I find it difficult to show this. I tried approximating the non-differentiable kernel $K(t)=\frac{t^{H-1/2}}{\Gamma(H+1/2)}$ by continuously differentiable kernels, $\varepsilon>0,$ $$
K_\varepsilon(t)= \frac{(t+\varepsilon)^{H-1/2}}{\Gamma(H+1/2)}.
$$ Then, an application of the stochastic Fubini theorem shows that $$
X_t^\varepsilon = \int_0^t K_\varepsilon(t-s) dB_s
$$ is a semimartingale with quadratic variation $ \langle X^\varepsilon\rangle_t = K_\varepsilon(0)^2 t $ and $\langle X^\varepsilon\rangle_t\to\infty$ as $\varepsilon\downarrow0.$ Moreover, I was able to show that $\mathbb{E}(|X_t^\varepsilon-X_t|^2)\to0$ as $\varepsilon\downarrow0.$ Now, I aim to conclude that $\langle X\rangle_t=\infty$ which would yield the claim. I would be grateful for any suggestion on how to finish my proof or any other way that shows the assertion.","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4694538,A corollary of Baire Category Theorem,"I have learnt that the theorem says: Let $\{U_n\}_{n=1}^\infty$ be a sequence of dense open subsets of a complete metric space X. Then $\displaystyle\cap_{n=1}^\infty U_n$ is also dense in X. I also know that A subset $Y$ of $X$ is nowhere dense if $\bar Y$ (closure of Y) has no interior points, and also $Y$ is nowhere dense $\iff$ int( $\bar Y$ ) = $\emptyset$ $\iff X \setminus \bar Y$ is a dense open subset of X. I have trouble figuring out a corollary: Let $\{E_n\}_{n=1}^\infty$ be a sequence of nowhere dense subsets of a complete metric space X. Then $\cup_{n=1}^\infty E_n$ has empty interior. My thoughts: $X\setminus \bar E_n$ are dense open sets. Therefore $\displaystyle\cap_{n=1}^\infty (X \setminus \bar E_n)$ is also dense in X, and $\cap_{n=1}^\infty (X \setminus \bar E_n)$ = $X \setminus \cup_{n=1}^\infty \bar E_n$ but I have no idea where to go next.","['complete-spaces', 'metric-spaces', 'elementary-set-theory', 'general-topology', 'baire-category']"
4694617,Largest possible eigenvalue of the covariance matrix of a smooth probability distribution on the sphere is attained for the uniform distribution?,"Let $\mathcal P$ the set of all probability distributions on the sphere $\Bbb S^n\subset \Bbb R^{n+1}$ that have smooth densities with respect to the Lebesgue surface measure. For all $P\in\mathcal P$ let $$
   \rho(P)=\max_{\lambda \in \Bbb S^n}\lambda^T Cov(P) \lambda
$$ the largest eigenvalue of the covariance matrix $$
   Cov(P)=\left(\int_{\Bbb S^n} x_ix_j P(dx)-\int_{\Bbb S^n} x_i P(dx)\int_{\Bbb S^n} x_j P(dx)\right)_{i,j=1,\ldots,n+1}.
$$ It seems intuitively clear to me that if $\mathcal U$ denotes the uniform distribution on $\Bbb S^n$ , then we should have $$
   \sup_{P\in\mathcal P}\rho(P)=\rho(\mathcal U).
$$ I am looking for a reference for this statement. I thought this might be found in Directional Statistics by Kanti Mardia and Peter Jupp, but so far I've had no luck finding it there. PS: I am aware that $Cov(\mathcal U)=(n+1)^{-1}\cdot 1_{(n+1)\times(n+1)}$ and hence $\rho(\mathcal U)=(n+1)^{-1}$ . That is not part of my question.","['covariance', 'uniform-distribution', 'reference-request', 'spectral-radius', 'probability-theory']"
4694641,About regularity of solutions of this PDE,"Let us consider the equation in the 2d torus $$\nabla \cdot f=g^2,$$ where $\nabla \cdot f$ denotes the divergence operator and $g^2$ has zero mean. In particular, we can see these functions as they were defined in $\mathbb{R} ^2$ and they were $2 \pi-$ periodic in each variable. I know that this equation doesn't admit a solution $f \in W^{1,1}$ if $g \in L^2$ . My question is: Is there a solution in $W^{1,1}$ for every $g^2$ in the Lorentz space $L^{2,1}$ ? If this is not possible, What is the best regularity that we can expect for the solution given $g$ under this condition?","['geometric-topology', 'multivariable-calculus', 'functional-analysis', 'partial-differential-equations', 'differential-geometry']"
4694750,Ball bouncing off a function,"Say we have a ball that is dropped from a point $(a,h) $ and falls under acceleration $g$ onto a function $f(x)$ , where it hits the function at the point $(a,f(a))$ , $h>f(a)$ . It will then follow a parabolic path that is defined by the gradient at the point it hits the function and the height above the function in which it is dropped. What is the equation of this parabola in the form $y=Ax^2+Bx^2+C$ ? Here is a picture of what I am trying to explain: (In this scenario, $f(x)=sinx$ )","['projectile-motion', 'calculus', 'functions', 'physics', 'trigonometry']"
4694770,Find the CDF of $X$ being the number of tosses of a fair dice until the numbers in each toss add to more than $6$,"I recently ran into this problem on an assingment. Supose you have a fair dice and let $X$ be the random variable that counts how many tosses happen before the sum of the numbers that showed up in each toss adds up to something greater than $6$ . You're asked to find the CDF of $X$ which has support in $\{2,\dots,7\}$ , but I can't figure out a closed form for it. I've tried calculating the probability function $\mathbb{P}[X=k]$ for each $k$ in the support of $X$ but it's a bit messy. Any ideas?","['conditional-probability', 'probability']"
4694775,$L^p$ spaces 2-side inequality,"I am currently studying $L^p$ spaces. Let's consider probability space in the following. Also consider measurable $f$ 's. We know that (for example, Proposition 1.4 here ) when $1\leq p< q$ , we have $||f||_p \leq ||f||_q$ . Is there a known inequality for the reverse as well? That is, for $1\leq p< q$ , can we get $$||f||_q \leq C ||f||_p$$ for some ""nice"" $C$ ? We can assume $f$ is bounded: $|f(x)|\leq B$ . This thread provides $||f||_q^q \leq C ||f||_p^p$ with $C=B^{1 - (p/q)}$ . This is close to what I am looking for, but can we get a version without the exponents just like in my request above? Thanks in advance for all inputs!","['measure-theory', 'lp-spaces', 'real-analysis']"
4694800,Why does Fubini's theorem not work in $\int_2^4\int_{x/2}^\sqrt{x}xy\ dy\ dx$?,$$\int_2^4\int_{x/2}^\sqrt{x}xy\ dy\ dx = \frac{11}{6}\neq\int_{x/2}^\sqrt{x}\int_2^4xy\ dx\ dy=3\left[x-\frac{x^2}{4}\right]$$ How do I know that Fubini's theorem ought not to work here?,"['integration', 'calculus', 'fubini-tonelli-theorems']"
4694804,An idempotent function $\mathbb R \to \mathbb R$,"I did an exam yesterday and this was one of the questions. I didn't have enough time to fully answer it so I wonder what the solution is: Let $f: \mathbb R \to \mathbb R$ be a continuous function with $f \circ f = f$ . Show: The image of $f$ is a closed interval (of course it may be unbounded). If $f$ is differentiable then it is either the identity or a constant function. I know if $f$ is differentiable, then by the chain rule: $$f'(x) = (f \circ f)'(x) = f'(x)\cdot f'(f(x)). $$ That implies $f'(x) = 0$ or $f'(f(x)) = 1$ but it may be different depending on $x$ .","['derivatives', 'real-analysis']"
4694845,Maximize sum of $(x_1+\dots+x_k)^2$ on the unit $n$-sphere,"Given any positive integer $n$ , let $t(n)$ be the smallest real number such that for any real numbers $x_1,\dots,x_n$ , the following inequality holds $$ \sum\limits_{k=1}^n (x_1+\dots+x_k)^2 \leqslant t(n) \sum\limits_{i=1}^n x_i^2.$$ Please give an expression for $t(n)$ . Attempt: The problem is equivalent to calculating the maximum of $\sum\limits_{k=1}^n (x_1+\dots+x_k)^2$ on the unit $n$ -sphere $\sum\limits_{i=1}^n x_i^2=1$ . Let $$ A := \begin{pmatrix}
n & n-1 & n-2 & \cdots & 1\\
n-1 & n-1 & n-2 & \cdots & 1\\
n-2 & n-2 & n-2 & \cdots & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & 1 & 1 & \cdots & 1
\end{pmatrix}$$ Then $$\sum\limits_{k=1}^n (x_1+\cdots+x_k)^2=xAx^T$$ where $x=(x_1,\dots,x_n)$ . According to Lagrange multipliers, we know the problem is equivalent to calculating the maximum eigenvalue of $A$ . Then I'm stuck. The book says that the answer is $$\frac{1}{4\sin^2\left(\frac{\pi}{4n+2}\right)}.$$ It is really amazing. I would appreciate it if someone could please give me some hints.","['matrices', 'inequality', 'quadratic-forms', 'eigenvalues-eigenvectors']"
4694915,Cutting a rectangular piece from a right triangle,"A piece of sheet metal has the shape of an acute right-angled triangle AB with side BC and height AA' each having 12 cm. From it, a rectangle with two vertices is cut on the base BC and the other two on AB, respectively AC. Calculate the perimeter of this rectangle. MY DRAWING Okey, so, we know that MNPQ a rectangle(I named the points that made the rectangle M,N,P,Q). MN is parallel with AB and applying the the fundamental theorem of similarity, wecan say that the traingle MAQ is similar with CAB. What should I do next??? Any idea is welcome! Thank you!","['rectangles', 'triangles', 'geometry']"
4694963,Closed form of $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}\dfrac{f(x)}{\ln(cx)}$,Is there some closed formula (except general Leibniz rule ) to evaluate these derivatives? $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}\dfrac{e^{ax}}{\ln(cx)}$ $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}\dfrac{\sin(ax)}{\ln(cx)}$ $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}\dfrac{\cos(ax)}{\ln(cx)}$ or in general $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}\dfrac{f(x)}{\ln(cx)}$ I tried to calculate them but I'm not able to see a repeating pattern.,"['analysis', 'real-analysis', 'calculus', 'functions', 'derivatives']"
4694980,"Have symmetric matrices for which the $(i,j)$ entry is $\min(i,j)$ earned a name?","I am interested in symmetric matrices where the $(i,j)$ entry is $\min(i,j)$ , i.e., $$\begin{pmatrix}
1      & 1      & 1      & \dots  & 1      & 1     \\
1      & 2      & 2      & \dots  & 2      & 2     \\
1      & 2      & 3      & \dots  & 3      & 3     \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
1      & 2      & 3      & \dots  & n-1    & n-1   \\
1      & 2      & 3      & \dots  & n-1    & n   
\end{pmatrix}$$ Note that these symmetric matrices are related to Lehmer matrices . Have these symmetric matrices earned a name? Motivation These matrices are interesting because they are the Gramians of binary triangular matrices, e.g., $$ \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}^\top \begin{bmatrix}1 & 1 & 1 & 1 & 1\\ 0 & 1 & 1 & 1 & 1\\ 0 & 0 & 1 & 1 & 1\\ 0 & 0 & 0 & 1 & 1\\ 0 & 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 & 2\\ 1 & 2 & 3 & 3 & 3\\ 1 & 2 & 3 & 4 & 4\\ 1 & 2 & 3 & 4 & 5\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0\\ 1 & 1 & 1 & 0 & 0\\ 1 & 1 & 1 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 \end{bmatrix}^\top$$ Thus, the determinant of these symmetric matrices is $1$ . Some findings In 2002, Mario Catalani studied these matrices In 2006, Rajendra Bhatia called these $\min$ matrices In 2016, Mika Mattila & Pentti Haukkanen called the $n \times n$ matrix of this form the MIN matrix of the set $\{1,2,\dots,n\}$ In 2023, Darij Grinberg studied a generalization of these matrices. Related About positive semidefiniteness of one matrix Evaluation of a specific determinant. Form matrix and calculate its determinant Determinant of matrix with $A_{ij} = \min (i, j)$ Determinant of matrix whose $(i,j)$ entry is $\min(i,j)$ Find rank of $A$ with entries $a_{ij}=\min\{i,j\} , 1\leq i,j \leq n $. Inverse of a symmetric tridiagonal filter matrix Properties of square array of numbers min (i,j) Name of a particular matrix with $M_{ij}=t_{\min(i,j)}$? Does this type of matrix with the same diagonals have a name?","['matrices', 'terminology']"
4694986,Limit right hand side at 0 and derivative,"Suppose $f: (0,1) \to \mathbb R$ differentiable such that $$\lim_{x\to0^+}f(x) = A$$ and $$\lim_{x\to0^+}xf'(x)=B.$$ Find the value of B. My attempt: We have $$(xf(x))' = f(x) + xf'(x) \ \ \ \ \  (1)$$ But $(xf(x))'$ , in the limit $x\to0^+$ , is $$\lim_{x\to0^+}\frac{xf(x)}{x} = A$$ So taking limits on both sides of $(1)$ , we have $B = 0$ . Is my approach correct? If it isn't correct, how to solve this?","['limits', 'calculus', 'real-analysis']"
4694991,Largest eigenvalue of the covariance matrix of a smooth unimodal probability distribution on the sphere?,"For any probability distribution $P$ on the sphere $\Bbb S^n\subset \Bbb R^{n+1}$ let $\lambda_{max}(P)$ denote the largest eigenvalue of its covariance matrix $$
   Cov(P)=\left(\int_{\Bbb S^n} x_ix_j P(dx)-\int_{\Bbb S^n} x_i P(dx)\int_{\Bbb S^n} x_j P(dx)\right)_{i,j=1,\ldots,n+1}.
$$ Question: Assume that $P$ has a density $p$ with respect to the Lebesgue surface measure and that $p$ is unimodal, i.e. it has exactly one local maximum. Is it true that then $$
   \lambda_{max}(P)<\lambda_{max}(\mathcal U),
$$ where $\mathcal U$ denotes the uniform distribution on $\Bbb S^n$ ? This seems like a relatively obvious question, and I'd expect that either a counter example or a proof is known - and I am looking for a reference. I'd be surprised if the answer to the question was no. Note that in any case the unimodality assumption cannot be dropped . PS: I am aware that $Cov(\mathcal U)=(n+1)^{-1}\cdot 1_{(n+1)\times(n+1)}$ and hence $\lambda_{max}(\mathcal U)=(n+1)^{-1}$ . That is not part of my question.","['covariance', 'uniform-distribution', 'reference-request', 'spectral-radius', 'probability-theory']"
4695021,Explicit Runge-Kutta method for solving ODE,"I would like to understand in some detail why this Runge-Kutta method is explicit if and only if $c_1=0$ .
Why in this case the r.h.s. doesn't contain $y_{n+1}$ ? EDIT $c_1=0$ EDIT 2 I would also like to know if this implicit Euler method $$\begin{array}{c|c}1&1\\\hline&1\end{array}$$ is implicit in $k_1$ or also in $y_{n+1}$ . In other words, is it possible to obtain all these 4 possibilities ? Like being $(i)$ implicit for $k_1$ and explicit for $y_1$ , $(ii)$ implicit for $k_1$ and implicit for $y_1$ , $(iii)$ explicit for $k_1$ and explicit for $y_1$ , $(iv)$ explicit for $k_1$ and implicit for $y_1$ all of this for some suitable $s\geq 1$ stage method or perhaps
with other index $i$ than $i=1$ ? In other words do we/can we distinguish already in the definition in which variable namely whether $k_i$ or $y_i$ is the method implicit and in which it is explicit ? Still, in your formula in the comment $y_{n+1}=y_n+hk_1$ the r.h.s. doesn't contain $y_{n+1}$ .","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
4695043,Prove a lemma about permutations,"Let $V$ be a set of $n$ permutations of $m$ numbers. Let $m<n<<m!$ I want to prove the following lemma: Lemma: For any set of permutations $V$ there exists a pair of numbers $(a,b)$ such that $\frac{|(v\in V: a\succcurlyeq b)|}{|(v\in V: b\succcurlyeq a)|}\approx 1$ . In other words, prove that there always exists a pair of numbers such that the number of partitions where $a$ comes before $b$ is approximately equal to the number of partitions where $b$ comes before $a$ . For the sake of completeness, I want to prove this lemma in order to be able to prove the following theorem: Theorem: Let $n$ be the number of permutations of $m$ numbers. Let $m<n<<m!$ . We now construct a binomial tree where at the root of the tree there is the set of all $n$ permutations. Each node we select a couple of elements of the permutations $(a,b)$ and split the permutations into two nodes containing two subsets of permutations: one with each permutation where $a$ comes before $n$ and the other viceversa. A node is a leaf if it contains equal permutations. Prove that the optimal height of the tree is $O(\log{n})$ .","['permutations', 'trees', 'decision-trees', 'order-theory', 'discrete-mathematics']"
4695053,"Conjecture: For $a,b,c,A,s$ the sides, area, and semi-perimeter of a triangle, and $x\ge2$: $a+b-c\ge s(2/3)^{(x+1)/(x-1)}(\frac{3^{3/2}A}{s^2})^x$","In the comments of this related question , it was mentioned that the RHS and LHS were expressions in $a,b$ and $c$ so the inequality was not useful. So I wanted to find an improved triangle inequality where for a given RHS, the LHS there are infinitely many possible combinations of $a,b$ and $c$ .  I obtained following conjecture obtained by empirical means and verified for discrete $2 \le x \le 70$ . Can this be proved? Conjecture : Let $a,b,c$ be the sides of a triangle of area $A$ and
semi-perimeter $s$ . Then, for any $x \ge 2$ , $$ a+b-c \ge s\left(\frac{2}{3}\right)^{\frac{x+1}{x-1}}
\left(\frac{3^{\frac{3}{2}}A}{s^2}\right)^x \tag 1 $$ For a given area $A$ and semi-parameter $s$ there are infinitely many possible values of $a,b$ and $c$ hence the above inequality is gives an explicit lower bound of the triangle inequality. Question 1 : Is the conjecture true? Question 2 : If true, can the inequality be improved? Note : The difference between LHS and RHS for random $2 \le x \le 70$ was typically or the order of $10^{-8}$ or smaller if $a,b$ and $c$ were the sides of a triangle inscribed in a unit circle. Update 13-5-2023 : For $x \ge 5.23$ experimental data shows that the triangle inequality can be improved to $$ a+b-c \ge s\left(\frac{2}{3}\right)^{1 + \frac{2}{x}}
\left(\frac{3^{\frac{3}{2}}A}{s^2}\right)^x \tag 1 $$","['inequality', 'geometry', 'real-analysis', 'triangles', 'algebra-precalculus']"
4695066,Maximum value of $y=\tan^{-1}x- \tan^{-1}\frac{x}{3}$,Let's suppose $f(x)=\tan^{-1}x$ and $g(x)=- \tan^{-1}\frac{x}{3}$ I know that the range of $f(x)$ is $$-\frac{\pi }{2} < \tan^{-1}x < \frac{\pi }{2}$$ and the range of $g(x)$ is the same. I used desmos to simulate the graph of $y$ and found that the range of it is $-\frac{\pi}{6}\leq y\leq\frac{\pi}{6}$ which means the maximum value is $\frac{\pi}{6}$ . I don't quite understand how to get this value though. Any hints?,['trigonometry']
4695082,Bounding error of approximating $(I-A)^t$ with $\exp(-At)$ for large $t$,"Suppose $A$ is a diagonal matrix with diagonal entries $\in (0,1]$ I'm interested in bounding the following quantity in terms of $t$ : $$f_A(t)=\operatorname{Tr}\exp(-At)-\operatorname{Tr}(I-A)^t$$ Suppose $\operatorname{Tr}(A^2)>1$ and $f_A(t)$ is a decreasing function of $t$ , what is a good bound in this regime? Example, plotting $f_A(t)$ for diagonal $A$ with diagonal values $1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{100}$ Notebook Motivation: see previous question solved with a bound which is tight for small values of $\operatorname{Tr}(A^2)$","['matrices', 'matrix-exponential', 'linear-algebra', 'upper-lower-bounds']"
4695088,Reversing perspective projection of cricles,"I have a set of real-world images containing circles, but due to the images not being taken parallel to the circle plane, the circles appear as ellipses in the images. I need to crop out the inside of these circles accurately. Simply finding the intersections of the major and minor diameters with the ellipses does not result in points spaced 90Â° apart on the original circle, leading to inaccurate perspective distortion calculations. I've come across research indicating that inferring the projection solely from the projected shape is not possible (refer to https://arxiv.org/abs/1902.04541 ). I'm seeking guidance on adding additional information to enable the reversal of perspective distortion. While marking four points on the original circle that are 90Â° apart would be the easiest solution, I'm curious if there are other viable approaches. Would two opposite points or the projected position center point be sufficient? Additionally, could knowing the image center, assuming a regular camera, be beneficial? It's worth mentioning that I am currently disregarding lens distortion. Here's an example image showcasing the issue: example I would appreciate any insights or suggestions on how to address this challenge effectively.","['projection', 'geometry', 'computer-vision']"
4695125,Stuck on Problem 21 of Chapter 3.4 in Velleman's How to prove it,"I am stuck on problem 21 of chapter 3.4 in Velleman's How to Prove it.
Here is what has to be proven. $F$ and $G$ are Families of Sets. So $\cup F$ is the union of the sets contained in $F$ . $$\cup F\setminus\cup G\subseteq\cup(F\setminus G)$$ And here is what I have got:
Suppose $x\in(\cup F\setminus\cup G)$ .
This implies that $\exists A\in F (x\in A)$ and $\forall B\in G(x\notin B)$ To show that $\cup F\setminus\cup G\subseteq\cup(F\setminus G)$ We need to show that $x\in \cup(F\setminus G)$ This means $\exists C \in (F\setminus G)(x\in C)$ This amounts to saying that $\exists C (C\in F \land C\notin G \land x\in C)$ If we choose $C=A$ then clearly $C\in F$ and $x\in C$ .
But I still have to show that $C\notin G$ .
I did something wrong here.
Is it perhaps better to shows this via the contrapositive method?","['elementary-set-theory', 'logic']"
4695126,Exactness of group invariants,"Let $G$ be a group and write $G_1 \times G_2 \times G_3$ or $G^3$ for $G \times G \times G$ . Let $M, N, P$ be $G_1 \times G_2 \times G_3$ -modules such that $$0 \to M \xrightarrow{f} N \xrightarrow{g} P \to 0$$ is exact.
Assume that taking $G_i \times G_j$ -invariants of the above sequence returns an exact sequence, for any $\{i, j\} \subset \{1, 2, 3\}$ with $i \neq j$ . My question is: does it follow that taking $G_1 \times G_2 \times G_3$ -invariants of the above sequence is exact as well? By left exactness of invariants, all we need to do is prove exactness at $P$ .
For $p \in P^{G^3}$ , we can find $n_1, n_2, n_3$ in $N^{G_2 \times G_3}$ , $N^{G_1 \times G_3}$ and $N^{G_1 \times G_2}$ , respectively, such that $g(n_i) = p$ . If $G$ was finite and $P$ was e.g. a $\mathbb Q$ -module, we could average over the $G_3$ -orbit of $n_3$ to get a preimage of $p$ inside $N^{G^3}$ . However, I don't want to put this assumption. I don't want to assume that the modules are finite over the group ring, either. I have not been able to make any progress in this general setting. An argument involving spectral sequences might help, but I'm unsure how.","['homological-algebra', 'modules', 'abstract-algebra', 'group-cohomology', 'group-theory']"
4695157,"For a finite set A of positive integers, prove that the set A + A - A contains more positive than negative elements.","I am working on a project that would need to use the following theorem that I cannot prove. ""Let $A$ be a finite set of positive integers. Then, the set $A + A - A$ contains more or equally as many positive elements than negative elements. ( $0$ is counted as a positive element)"" To clarify: The set $A + A - A$ is defined as $\{a_1 + a_2 - a_3|a_1, a_2, a_3 \in A \}$ For example: If $A = \{2,5\}$ . Then $A+A-A = \{-1, 2, 5, 8\}$ The nature of this problem lies in the duplicants that can occur. It is intuitively clear, that the theorem should be true since we add an element of A two times but subtract an element only one time. However, it seems to me that it is very hard to prove. Here is the idea I have been working on so far (which might be the wrong way to go about this): Let $A = \{a_1, a_2,...,a_n\}$ . We first look at the set $A-A$ . This set is symmetric around zero. For every element $x$ in $A-A$ , $-x$ also occurs in $A-A$ . This means that for every negative element, there also exists a unique positive element. Next we look at the set $a_1 + A - A$ . This just shifts $A-A$ to the right. Some negative elements might change into positive ones, but we do not care about that. What is important is, that every positive element stays positive. Therefore again, this set contains more or equally as many positive than negative elements. Now it gets tricky. My next idea is to look at the set $a_2 + A - A$ and take the union with the set $a_1 + A - A$ . I want to do this with every $a_i$ . That means in the end I take the following union: $\bigcup\limits_{i=1}^{n}a_i + A - A$ which is exactly $ A+A-A$ . I want to prove at each step that there are still more or equal positive than negative elements in the current set. My idea to do this is to think of the following: Since $A-A$ was symmetric around $0$ , I can split $A-A$ into elements that are $>0$ and elements that are $<0$ (we do not care about the $0$ ). These two subsets have the same size. If we add an $a_i$ to a positive element, we have two cases: case: We get a new element that does not already occur in our union of sets.
In this case we do not have to do anything. case: We get a duplicant. In this case we need to show that we also get a unique duplicant if we add $a_i$ to a certain negative element. I do not want to go into too much detail, but I was very close to find a method for the 2. case. It always gave me a negative duplicant, but was not unique in very few specific cases. If somebody has an idea for this problem, maybe even with a complete other method, I would be very grateful to hear and also very excited to discuss it. Thank you very much.",['combinatorics']
4695187,Is the following function continuous? Has partial derivatives?,"Let $f(t)$ be a differentiable function for every $t$ such that $f(0)=0$ and $f'(0)=1$ . Define: $$
g(x,y)=\begin{cases}
\frac{f(x)^2 + f(y)^2}{x^2+y^2}, &\quad (x,y)\neq(0,0) \\
1 &\quad (x,y)=(0,0).
\end{cases}
$$ Is $g$ continuous at $(0,0)$ ? Do the partial derivatives $g_x(0,0),g_y(0,0)$ exist? My attempt:
We know that $\lim_{x \to 0} f(x)=0$ and $\lim_{x\to 0}\frac{f(x)}{x}=1$ . Therefore $\lim_{x\to 0}\frac{f(x)^2}{x^2}=1$ .
In particular, taking the paths $y=0$ or $x=0$ leads to $\lim_{x\to 0} g(x,0)= \lim_{y\to 0} g(0,y)=0$ . Trying polar coordinates leads to $g(r \cos \theta , r\sin \theta) = \frac{f(r\cos \theta) ^2 + f(r\sin \theta)^2 }{r^2}$ for $r\neq 0$ . If hypothetically I would have known that somehow this limit is zero for any choice of $\theta$ , this would solve the problem, but we don't know that since we don't have $f(r)^2 $ in the numerator. First question: How can I prove the function is continuous at $(0,0)$ ? As for partial derivatives:
Using the definition, we need to calculate $$
\lim_{h\to 0} \frac{g(h,0)-g(0,0) }{h } = \lim_{h\to 0} \frac{f(h)^2-h^2 }{h^3 }.
$$ My intuition is that this limit doesn't exist, but choosing $f(x)=sin(x)$ or $f(x)=x$ seems to do work. So, second question is: What might be a possible counterexample for the existence of partial derivatives? Thank you","['limits', 'multivariable-calculus']"
4695193,Approximate tangent space agrees with tangent space of submanifold of $\mathbb{R}^n$,"I am stuck on trying to prove that the approximate tangent space of a submanifold of $\mathbb{R}^n$ agrees with its tangent space. To make things more precise I'll give the relevant definitions. Note that $\mathcal{H}^k$ denotes the $k$ -dimensional Hausdorff measure on $\mathbb{R}^n$ and $\mathcal{L}_n$ denotes Lebesgue measure on $\mathbb{R}^n$ . Definition. (Approximate tangent space) Let $E\subseteq\mathbb{R}^n$ be a $\mathcal{H}^k$ -measurable set such that $\mathcal{H}^k(E\cap K)<\infty$ for all compact $K\subseteq\mathbb{R}^n$ . and let $P$ be a $k$ -dimensional subspace of $\mathbb{R}^n$ . We say that $P$ is the approximate tangent space of $E$ at $x\in E$ if $$\lim_{\lambda\downarrow0}\int_{\frac{E-x}{\lambda}}f~\mathrm{d}\mathcal{H}^k=\int_P f~\mathrm{d}\mathcal{H}^k$$ for all compactly supported continuous $f:\mathbb{R}^n\to\mathbb{R}$ . Definition. (Tangent space) Let $M$ be a $k$ -dimensional $C^1$ -submanifold of $\mathbb{R}^n$ , and let $x\in M$ . We define the tangent space of $M$ at $x$ , denoted $\mathrm{T}_xM$ , as the subspace of $\mathbb{R}^n$ consisting of all vectors $v$ such that $v=\gamma'(0)$ for some $C^1$ -curve $\gamma:(-1,1)\to M$ with $\gamma(0)=x$ . What I am trying to prove is that if $M$ is a $k$ -dimensional $C^1$ -submanifold of $\mathbb{R}^n$ , then $\mathrm{T}_xM$ is also the approximate tangent space of $M$ at $x$ . In particular then what I need to show is that $$\lim_{\lambda\downarrow0}\int_{\frac{M-x}{\lambda}}f~\mathrm{d}\mathcal{H}^k=\int_{\mathrm{T}_xM} f~\mathrm{d}\mathcal{H}^k$$ for all compactly supported continuous $f:\mathbb{R}^n\to\mathbb{R}$ . Before giving my attempt, I'll also state the Area formula, which I'm using. Theorem. (Area formula) Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be Lipschitz, where $n\leq m$ , and let $g\in\mathscr{L}^1(\mathbb{R}^n)$ . Then $$\int_{\mathbb{R}^n}gJf~\mathrm{d}\mathcal{L}_n=\int_{\mathbb{R}^m}\sum_{x\in f^{-1}(\{y\})}g(x)~\mathrm{d}\mathcal{H}^n(y),$$ where $$Jf(x)=\sqrt{\det(Df(x)^*\circ Df(x))},$$ with $^*$ denoting the adjoint. The following is what I've tried so far: We consider first when we can parameterize all of $M$ by a $C^1$ -map $\psi:V\to M$ on some open set $V\subseteq\mathbb{R}^k$ . Suppose without loss of generality that $\psi(0)=x$ . We have that $$\mathrm{T}_xM=\mathrm{span}\{\partial_1\psi(0),\dots,\partial_k\psi(0)\},$$ and so in particular we can consider the function $h:\mathbb{R}^k\to\mathrm{T}_xM$ defined by $$h(y)=D\psi(0)y,$$ which gives us an isomorphism, and in particular $h$ is Lipschitz with $$Jh(y)=J\psi(0)$$ for all $y\in\mathbb{R}^k$ , and so by the Area formula we have that $$\int_{\mathrm{T}_xM} f~\mathrm{d}\mathcal{H}^k=\int_{\mathbb{R}^k}f(D\psi(0)y)J\psi(0)~\mathrm{d}\mathcal{L}_k(y).$$ Now take $\lambda>0$ , define $\eta_\lambda:\mathbb{R}^n\to\mathbb{R}^n$ by $$\eta_\lambda(y)=\frac{y-x}{\lambda},$$ and observe that $\eta_\lambda\circ\psi$ then parameterizes $\frac{M-x}{\lambda}$ , and $$J(\eta_\lambda\circ\psi)=\frac{1}{\lambda^k}J\psi.$$ We thus have, again by the Area formula, that $$\int_{\frac{M-x}{\lambda}} f~\mathrm{d}\mathcal{H}^k=\int_V f\left(\frac{\psi(y)-x}{\lambda}\right)\frac{J\psi(y)}{\lambda^k}~\mathrm{d}\mathcal{L}_k(y).$$ At this point I suppose I want to let $\lambda\downarrow0$ and use some convergence theorem to show that this integral converges to the previous one. I am however stuck on how to actually do this, and so need help on how to progress from here (assuming my idea even works and is correct so far). Edit: So I realized that if $y\neq0$ , then $\psi(y)\neq x$ , which means that $\frac{\psi(y)-x}{\lambda}$ is outside of the support of $f$ for sufficiently small $\lambda$ . This means that we have that $$\lim_{\lambda\downarrow0}f\left(\frac{\psi(y)-x}{\lambda}\right)\frac{J\psi(y)}{\lambda^k}=0$$ for all $y\neq0$ . I'm a bit unsure about how useful this actually is right now, but I'll try to investigate what happens when $y=0$ when I get home, and hopefully get some useful behavior out of the integral.","['tangent-spaces', 'geometric-measure-theory', 'differential-geometry']"
4695248,Joint density of transformation,"Let $X, Y$ be two independent exponential random variables, and let $U = \frac{X}{Y}, V = X + Y$ . I am trying to prove that $U$ and $V$ are independent, so I computed their individual densities which yield $f_U(t) = \frac{ab}{(a t + b)^2}, f_V(t) = \frac{ab}{a +b}\left(e^{-bt}-e^{-at}\right)$ , where $a,b$ are the parameters of $X$ and $Y$ . How do I compute their joint density, and if it's not possible, is there any other way to prove independence?","['statistics', 'probability-distributions', 'probability']"
4695266,The Second Cohomology group with Coefficient in cyclic group of order 3.,"I am computing the second cohomology group $H^2(G,\mathbb{Z_3})$ of $G=$ SmallGroup(81,9). Its presentation is given by $G=\langle x,y,z\mid x^3=y^9=z^3=[y,z]=1,[y,x]=z,[z,x]=zy^3z^{-1}\rangle$ .
I tried using the Consistency test ( Alexander Wegner's PhD Thesis , Theorem 2.7, page-23). From My calculations, I am getting $H^2(G,\mathbb{Z_3})\cong \langle (1,0,0,0,0,0),(0,0,1,0,0,0),(0,0,0,0,1,0)\rangle$ . But Gap computations show that $H^2(G,\mathbb{Z_3})\cong {\mathbb{Z_3}\times \mathbb{Z_3}\times \mathbb{Z_3}\times \mathbb{Z_3}}$ . Can Anyone help me where I am making a mistake? I have checked it many times. I am attaching a link to my Calculations. Thank you very much in advance.","['group-theory', 'group-cohomology', 'p-groups']"
4695293,Is the Rubik's Cube group normal in the assembly/disassembly group?,"I found myself wondering whether the Rubik's Cube group (of order $2^{27} \cdot 3^{14} \cdot 5^3 \cdot 7^2 \cdot 11$ ) is normal as a subgroup of the slightly larger group where assembly/disassembly of the cube is allowed (of order $2^{29} \cdot 3^{15} \cdot 5^3 \cdot 7^2 \cdot 11 = 12! \cdot 8! \cdot 2^{12} \cdot 3^8$ ). Most of the parity restrictions are easy enough to cast as kernels of group homomorphisms: the restriction on cubie placement can be interpreted as taking a group homomorphism from the larger group to $S_{20}$ , and then composing with the sign homomorphism to $\{ \pm 1 \}$ , and taking the kernel.  Similarly, the edge orientation restriction can be interpreted as taking a group homomorphism from the larger group to $S_{24}$ (taking the corresponding permutation of edge stickers), and then again composing with the sign homomorphism to $\{ \pm 1 \}$ , and taking the kernel. What's left is the restriction on corner orientation.  I have seen descriptions of corresponding functions from the larger group to $\mathbb{Z} / 3 \mathbb{Z}$ ; what is not clear to me is whether any of these functions gives a homomorphism of groups.  (And in trying to apply the definition of normality directly, I have a hard time working with disassembling/reassembling the cube, applying some sequence of face rotations, and then applying the inverse of the original disassembly/reassembly.)","['permutations', 'group-theory', 'rubiks-cube']"
4695307,Integration of surface from compatible first and second fundamental forms,"Let $\Omega\subset\mathbf{R}^2$ be some simply connected domain. Given the functions $(E,F,G)$ and $(L,M,N)$ that satisfy the Gaussâ€“Codazziâ€“Mainardi equations, we have, according to Bonnet theorem, a unique surface up to rotations and translations in $\mathbf{R}^3$ . After an extensive search I have not found a single example where the integration of a surface, that is the obtaining the function $\mathbf{r}(x,y)\in\mathbf{R}^3$ from the fundamental form coefficients, is done $\textit{from scratch}$ . There are examples where one a priori wants a surface of revolution and assumes a certain special form of $\mathbf{r}(x,y)$ and then proceeds, but in general one has no clue about how the surface will look like. So How to integrate $\mathbf{r}(x,y)$ from the (GMC satisfying) fundamental forms coefficients $(L,M,N)$ and $(E,F,G)$ , without assuming any special form of $\mathbf{r}(x,y)$ ? Are there analytical, non-trivial examples (beyond surfaces of revolution or any other symmetric shapes).","['partial-differential-equations', 'surfaces', 'riemannian-geometry', 'differential-geometry']"
4695311,Reference for Sobolev estimate on bounded domain with a boundary term on the right side.,"Could someone point me to a reference for the proof of the following Sobolev estimate? $$
\| u\|_{ L^{2d/(d-2)}(\Omega)} \leqslant C\left(\| f \|_{L^{2 d /(d+2)}(\Omega)}+\|g\|_{\partial \Omega}\right)
$$ for all $u \in W^{2,2 d /(d+2)}(\Omega)$ such that $\Delta u=f$ and $g=u$ on $\partial \Omega$ , and $d>2$ and $\Omega$ is a smooth domain of $\mathbf{R}^d$ . Thanks in advance to anyone who could offer help.","['reference-request', 'analysis', 'partial-differential-equations']"
4695322,Is there an identity for $\sum_{k=0}^{n-1}\csc\left(x+ k \frac{\pi}{n}\right)\csc\left(y+ k \frac{\pi}{n}\right)$?,"What I'd like to find is an identity for $$\sum_{k=0}^{n-1}\csc\left(x+ k \frac{\pi}{n}\right)\csc\left(y+ k \frac{\pi}{n}\right)$$ here it can be shown that where $x=y$ , $$n^2 \csc^2(nx) = \sum_{k=0}^{n-1}\csc^2\left(x+ k \frac{\pi}{n}\right)$$ I also looked at possible residue methods involving csc I thought I could use $\left(  \sum^{n-1}_{j=0}Z_j\right)^2 = \sum^{n-1}_{j=0} Z_j^2 +\sum^{n-1}_{j=0}\sum^{n-1}_{i\neq j} Z_jZ_i$ but I have had no luck so far.","['summation-method', 'summation', 'trigonometric-series', 'complex-analysis', 'trigonometry']"
4695326,"Path dependence of $\lim\limits_{(n,m)\to(\infty, \infty)}\left(\sum_{i=1}^n \frac{1}{2^i}\right)^m$","I was wondering if $$\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^n =e$$ and the geometric series : $$\lim_{n\toâˆž}\sum_{i=1}^n \frac{1}{2^i}$$ will converge to 1 so  what will happen if i do this $$\lim\limits_{(n, m)\to (\infty, \infty)}\left(\sum\limits_{i=1}^n \frac{1}{2^i}\right)^m$$ will the answer will be $1$ or $e$ or will depend on the path of $n$ , $m$ ?","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
4695331,Definition of stabilizer of a point in a stack,"I was reading in Adeel Khan's ''A Modern Introduction to Algebraic Stacks'' and came across the definition of the stabilizer of an $R$ -valued point $x$ of a stack $X$ . My question will be about that the definition does not quite make intuitive sense to me, but let me first make some definitions. The eventual questions are bold-faced. I look at stacks as $(2,1)$ -sheaves $\mathrm{Sch}^\mathrm{op}_\mathrm{fppf}\to\mathsf{Grpd}$ in the fppf-topology, where $\mathsf{Grpd}$ is the $(2,1)$ -category of groupoids. Then, given an $R$ -valued point $x\colon\mathrm{Spec}\,R\to X$ in $X(R)$ , we define the stabilizer of $x$ in $X$ as the pullback (in the $(2,1)$ -category of stacks) $\require{AMScd}$ \begin{CD}
{\mathrm{St}_X(x)} @>>> {\mathrm{Spec}\,R}\\
@V V V @VV x V\\
{\mathrm{Spec}\,R} @>>x> X
\end{CD} We can also define the intertia stack $\mathcal{I}_X$ of $X$ by the pullback $\require{AMScd}$ \begin{CD}
{\mathcal{I}_X} @>>> X\\
@V V V @VV \Delta V\\
X @>>\Delta> X\times X
\end{CD} where $\Delta\colon X\to X\times X$ is the diagonal. Because $\mathrm{Spec}\,R$ is a set-valued stack, I can find that for any ring $R'$ , the groupoid $\mathrm{St}_X(x)(R')$ is actually the set of triples $(y,z,\varphi)$ with $y,z\colon R\to R'$ maps of rings, and $\varphi\colon x|_y\to x|_z$ an isomorphism in $X(R')$ . Now comes my question: why do we call this a stabilizer of $x$ in $X$ ? I would expect that a stabilizer consists of all the automorphisms of $x$ , in some sense. More precisely, I would expect the $R'$ -valued points of the stabilizer to consist precisely of a groupoid with objects $x|_y$ for all $y\colon R\to R'$ , and morphisms the automorphisms of these $x|_y$ in $X(R')$ . This is not what $\mathrm{St}_X(x)$ does: we get not only automorphisms, but just general isomorphisms between different restrictions of $x$ , and we only have a set, and not a groupoid. All in all, it seems like it is not completely resembling a stabilizer. Does anyone have an explanation why this is then the correct notion of the stabilizer of $x$ ? Why do we want to look at isomorphisms between different restrictions of $x$ ? When I compute the $R'$ -valued points of $\mathcal{I}_X$ , I find that $\mathcal{I}_X(R')$ is equivalent to a groupoid consisting of objects $(y,\alpha)$ , with $y\in X(R')$ and $\alpha\colon y\to y$ a morphism in $X(R')$ . The morphisms $(y,\alpha)\to(y',\alpha')$ in this groupoid are morphisms $\zeta\colon y\to y'$ in $X(R')$ such that $\alpha=\zeta^{-1}\alpha'\zeta$ . So $\mathcal{I}_X(R')$ has the automorphisms of $R'$ -valued points of $X$ as objects, and their conjugations as morphisms. This seems much closer to what I would want from a stabilizer, but this is of course not the stabilizer of a single point. We can remedy this by defining for $x\colon\mathrm{Spec}\,R\to X$ a stack $F_x$ by a pullback $\require{AMScd}$ \begin{CD}
{F_x} @>>> {\mathcal{I}_X}\\
@V V V @VV \pi V\\
{\mathrm{Spec}\,R} @>>x> X
\end{CD} where $\pi\colon\mathcal{I}_X\to X$ maps an object $(y,\alpha)$ to $y$ and a morphism $\zeta\colon(y,\alpha)\to(y',\alpha')$ to $\zeta\colon y\to y'$ . I can show that for a ring $R'$ the groupoid $F_x(R')$ is actually equivalent to the set consisting of pairs $(y,\omega)$ , for $y\colon R\to R'$ and $\omega\colon x|_y\to x|_y$ in $X(R')$ . In other words, $F_x$ collects all automorphisms of restrictions of $x$ to $R'$ into a set, but forgets about the conjugations (which is not weird, given its definition). Now, this seems more like a stabilizer to me than $\mathrm{St}_X(x)$ , where we also allow isomorphisms between different restrictions of $x$ . Why is this not the definition of the stabilizer of $x$ ? As an aside, via the definition of $\mathrm{St}_X(x)$ we have a pullback $\require{AMScd}$ \begin{CD}
{\mathrm{St}_X(x)} @>>> {\mathrm{Spec}\,R}\\
@V V V @VV (x,\mathrm{id}_x) V\\
{F_x} @>>> {\mathcal{I}_X}
\end{CD} So we can recover $\mathrm{St}_X(x)$ from $F_x$ by ''further restriction'' along the identity automorphism of $x$ . Because this is a $(2,1)$ -pullback, it is of course not purely a restriction, but it seems still like $F_x$ is a slightly more powerful stack.","['algebraic-stacks', 'algebraic-geometry', 'sheaf-theory']"
4695378,Tangent Space under (linear) Transformation,"i am looking for a confirmation of the following Lemma as well as a reference: Let $M \subset \mathbb{R}^m $ be a smooth-manifold and $A \in \mathbb{R}^{n\times m}$ be a full rank matrix with $n\geq m$ . Define $$ N = \{ Ax \, |\, x \in M\}. $$ Then N is also a smooth manifold and $$ A\mathcal{T}_x M := \{Av \, | \, v \in \mathcal{T}_x M\} = T_{Ax} N. $$ Is this correct? I would be very glad if someone could point me to a reference since unfortunatly i could not find anything that seems to be directly related. Thanks in advance.","['pushforward', 'smooth-manifolds', 'manifolds', 'tangent-bundle', 'differential-geometry']"
4695421,"$ABCD$ is a square with an internal point $E$. $AE=1, CE=2$ and $DE=3$. Find $\angle AEC$ [duplicate]","This question already has answers here : What is $\angle AEB$? (6 answers) Closed last year . Given a square $ABCD$ with a point $E$ inside, $AE=1, CE=2$ and $DE=3$ . Find the angle $\angle AEC=?$ I solved this problem in a pretty synthetic way, which Iâ€™ll post below. I want to see if there are any other possible approaches or if my answer has faults.","['euclidean-geometry', 'trigonometry', 'geometry']"
4695430,No equivalent norm induced by inner product,"In class I was asked to show that there is no inner product on $\ell^1(\mathbb{N})$ which gives rise to the norm $\|\cdot\|_1$ . I was able to do so, using the parallelogram law. Now, I am wondering if it is possible for a norm on $\ell^1(\mathbb{N})$ which is equivalent to $\|\cdot\|_1$ to be induced by an inner product. It isn't immediately obvious to me whether or not this could be the case. So far, I have tried using the definition of equivalent norms to no avail.","['inner-products', 'normed-spaces', 'functional-analysis', 'equivalent-metrics']"
4695493,Does this sequence of unit vectors in a Hilbert space converge?,"Let $H$ be a Hilbert space, and let $A$ be a densely defined, unbounded operator on $H$ . Then there is a sequence $v_n$ of vectors in Dom $(A)$ such that $\frac{\|Av_n\|}{\|v_n\|}\to \infty$ . My question is essentially whether the obstruction to $\lim_{n\to \infty} Av_n$ being defined is only in norm. More precisely, Let $h\in H\setminus \text{Dom}(A)$ . Since $A$ is densely defined, there exists a Cauchy sequence $v_n$ in $\text{Dom}(A)$ converging to $h$ . Does the sequence $ \frac{Av_n}{\|Av_n\|}$ converge in $H$ to some unit vector? If so, is it independent of which sequence we pick converging to $h$ ? I'm having trouble making any progress since $A$ doesn't play well with the norm. I've tried considering the specific example of $\frac{d}{dx}$ acting on $L^2(\mathbb{R})$ without much success.","['hilbert-spaces', 'limits', 'unbounded-operators']"
4695500,$\lim_{h\to 0}\int_{\mathbb{R}}|f_h(x)-g(x)|^2dx=0$,"I can do (a) and (b), but I cannot do (c). I have attempted to use $$f_n(x)=\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}},$$ but I do not know how to proceed. (a) If $f(x) \in L^2(X)$ and $\mu(X)<\infty$ , show that $f(x) \in L^1(X)$ . (b) If $f(x)$ , $x f(x) \in L^2(\mathbb{R})$ , show that $f(x) \in L^1(\mathbb{R})$ . (c) If $f(x)$ , $g(x) \in L^2(\mathbb{R})$ and $$\lim _{h \rightarrow 0} \int_{\mathbb{R}}\left|f_h(x)-g(x)\right|^2 d x=0,$$ where $$f_h(x):=\frac{f(x+h)-f(x)}{h} \quad \text { for any } h \in \mathbb{R} \setminus \{0\},$$ show that $$f(x)=\int_{[0, x]} g(t) d t+C
$$ for some constant $C$ .","['integration', 'limits', 'sequence-of-function', 'real-analysis']"
4695501,Compact Manifolds in Euclidean Spaces,"I'm watching Professor Shifrin's wonderful lectures on Stokes's Theorem, and he uses the term ""compact"" to describe a manifold. He defines a compact set of $\mathbb{R}^n$ as one that is closed and bounded. Does this definition carry over to manifolds?","['manifolds', 'multivariable-calculus', 'stokes-theorem', 'real-analysis']"
4695523,"Irreducible components, dimension and degree of projective varieties","I have this problem given to me in my review session for my algebraic geometry final: Describe the irreducible components and compute the degree and dimension of $V_p(x_0x_2-x_1^2, x_0x_3-x_1x_2)\subset \mathbb{P}^3$ . Unfortunately, my professor is not responding to my emails. What is meant by irreducible components? I tried eliminating certain terms and coming up with the basis monomials for $K[x_0, x_1, x_2, x_3]/(x_0x_2-x_1^2, x_0x_3-x_1x_2)$ , but this approach seemed tedious. How do I find the first term of the Hilbert polynomial (and thus answer the degree and dimension question very quickly)? Can anybody provide any hints/a solution/ideas to approach these kinds of problems? I am working with Gathmann 2014 edition: link .","['projective-geometry', 'affine-varieties', 'algebraic-geometry', 'ideals', 'projective-space']"
4695615,$\frac{1}{m(E)}\int _E f(x) dx\in S$ implies $f(x)\in S$ for a.e. $x$,"The problem: Let $S\subset \mathbb{R}$ be closed, and let $f\in L^1 ([0,1], m)$ , where $m$ denotes the Lebesgue measure on $[0, 1]$ . Assume that for all measurable $E\subset[0,1]$ with $m(E)>0$ we have $\frac{1}{m(E)}\int _E f(x) dx\in S$ . Prove that $f(x)\in S$ for a.e. $x\in [0,1]$ . My progress: Currently I am trying to apply Lebesgue Differentiation Theorem (See Page 68 Theorem 6.10 ) to prove the statement. But I am not very confident about this because in that case, $m(E) \to 0$ and I feel I miss a part to explain why I can still apply Lebesgue Differentiation Theorem. Any comments are appreciated!","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'measurable-sets']"
4695636,Spectral radius of symmetric matrix with negative entries multiplied by a diagonal matrix,"Let $M$ be a irreducible, symmetric matrix with some negative entries such that $M^k>0$ for some $k>k_o$ and $\sum_j m_{ij}=1$ with $m_{ij} \in \Re$ , and spectral radius $\rho(M)=1$ . After multiplying it with a diagonal matrix $D={\rm diag}(d_{ii})$ where $0<d_{ii}\leq 1$ with at least one $d_{ii}<1$ . Is there an easy way to show $\rho(DM)<1$ ? I know that this result is true as stated. Similar questions have been asked for matrices $M$ but only for nonnegative entries, e.g., Substochastic matrix spectral radius .","['eigenvalues-eigenvectors', 'matrices', 'spectral-radius', 'linear-algebra', 'symmetric-matrices']"
4695689,Elo/Glicko based rating system for team games and with margin of victory multiplier,"Can someone suggest a rating formula for individuals playing a team game where pairings are made up from a pool of individual players? I have a game which is always played in doubles. The pairings can be made up of any two players in the pool of players and will change every match. Pairings are random and not affected by previous pairings or skill/ability. I want a rating system that tracks individual ratings based upon all the games they play in paired matches. Assumptions The simple win probability of a pairing against another pairing is based upon their average rating, nothing more complicated than this. Each match provides only two pieces of data - the team that won and the margin of victory (there is no indication of how each player in the team performed, only the team as a whole). There are no ""cyclic"" relationships between pairings or players as in rock, paper scissors. Requirements Accurately tracks the probability of win/loss for a given matchup. Not prone to rapid inflation or deflation Doesn't encourage players to ""protect"" their rating Doesn't allow manipulation of pairings to create  beneficial match ups Preferably includes a margin of victory multiplier to encourage teams to continue to play to the best of their ability even when very far behind in a game. This would imply that it can predict the margin of victory as well as the probability of win/loss. Low computation requirements and easy to implement in a spreadsheet Options Elo is perhaps the best-known rating system made popular from its use by FIDE but has many well-known flaws . It also needs to be adapted to allow tracking individual ratings when only team results are known. Perhaps it can be altered to take into account margin of victory by scaling the scores from 0-1 instead of 0 for a loss, 1 for a win and 1/2 for a draw? Another system is the KGS rank system . A frequently-used improvement upon Elo's work is the Glicko-2 system . It introduced concepts such as rating volatility, improves prediction reliability and has replaced the use of Elo on many rating leagues. However, it also doesn't account for teams made of of pairings from a pool. Rating systems that do account for this include Microsoft's TrueSkill2 but this is a licensed system.  Another method would be to use Glicko-2 treating each 2v2 match it as 4 individual simultaneous matches. Nate Silver also apparently uses some sort of algorithm for team sports . There are some similar questions but, as far as I can see, none take into account both the margin of victory and individual ratings in team matches. Does anyone have any suggestions for a simple rating algorithm to use?","['statistics', 'probability', 'algorithms']"
4695698,Inverse Hoeffding inequality for Rademacher random variables,"Let $Y_1,\dots,Y_k$ be independent and identically distributed Rademacher random variables, i.e. $\mathbb{P}(Y_1=1)=\mathbb{P}(Y_1=-1)=\frac{1}{2}$ , and take $p\in \mathbb{R}^k$ with $||p||_2=1$ . I want to prove the following inequality: $\mathbb{P}(\sum^k_{i=1}Y_ip_i>x)\geq e^{-cx^2}$ , where $x\geq a$ and $x\leq b(||p||_{\infty})^{-1}$ and $a$ and $b$ are some positive constants. I know the Hoeffding inequality and that this inequality is basically the inverse version. How could I approach this problem?","['inequality', 'probability-theory', 'probability']"
4695699,Does the topology determine the vector space structure of a topological vector space?,"Let $(V, \cdot, +)$ be a topological vector space over $\mathbb{R}$ or $\mathbb{C}$ with topology $\mathcal{T}$ and let $0 \in V$ be the zero vector. Is then the linear structure $(\cdot, +)$ on the pointed topological space $(V,0)$ uniquely determined by the topology $\mathcal{T}$ and the distinguished zero element $0$ ? If not in general, does this hold, if one specifies further requirements on $\mathcal{T}$ , i.e. separation axioms?","['topological-vector-spaces', 'vector-spaces', 'linear-algebra', 'functional-analysis', 'general-topology']"
4695700,Does Taylor's theorem (Peano's form) generalize to other fields?,"Let $\mathbb F$ be a field with a non-trivial absolute value, such as $\mathbb F_p(X)$ or $\mathbb Q$ or $\mathbb Q_p$ , and let $f:\mathbb F\to\mathbb F$ be a function. Limits, continuity, and derivatives are defined as usual: $\lim_{x\to a}f(x)=b$ means that for any real $\varepsilon>0$ there exists $\delta>0$ such that, for all $x\in\mathbb F$ where $0<|x-a|<\delta$ , $|f(x)-b|<\varepsilon$ . We say that $f$ is $n$ th-order Peano differentiable at $a$ , if there's a polynomial $\sum_{k=0}^nc_kx^k$ and a function $h$ continuous at $h(0)=0$ such that $$f(a+x)=\sum_{k=0}^nc_kx^k+x^nh(x).$$ Suppose $f$ has both ordinary derivatives and Peano derivatives up to order $n$ at $a$ . Must they be proportional, $$f^{(k)}(a)=k!\,c_k\quad?$$ Related: In characteristic $2$, can a function have a non-zero second derivative? A function may have Peano derivatives but not ordinary derivatives of order $n$ . Here's an example with $\mathbb F=\mathbb R$ (I suppose a similar example could be constructed for $\mathbb F=\mathbb Q$ using $2^{\lfloor-1/x^2\rfloor}$ on the dyadic rationals): $$f(x)=\begin{cases}e^{-1/x^2},\quad x\in\mathbb Q\setminus\{0\}; \\ 0,\quad\text{otherwise};\end{cases}$$ this satisfies $f(0+x)=0+0x+\cdots+0x^n+x^nh(x)$ where $\lim_{x\to0}h(x)=0$ . But $f''(0)$ doesn't exist, because $f'(x)$ doesn't exist in a neighbourhood of $0$ , because $f(x)$ is not continuous in a neighbourhood of $0$ . Conversely, a function may have ordinary derivatives but not Peano derivatives of order $n$ . An example with $\mathbb F=\mathbb Q_p$ is given here : $$f(x)=\begin{cases}p^{2m},\quad x=p^m+yp^{2m+1},\quad y\in\mathbb Z_p,\quad m\in\mathbb N; \\ 0,\quad\text{otherwise};\end{cases}$$ this satisfies $f'(x)=0$ for all $x$ , hence $f^{(n)}(0)=0$ for all $n>1$ . But $\lim_{x\to0}f(x)/x^2$ doesn't exist, so $f$ can't have a $2$ nd-order Peano derivative; $f(0+x)=0+0x+c_2x^2+x^2h(x)$ would imply that the limit exists as $c_2$ .","['p-adic-number-theory', 'analysis', 'field-theory', 'taylor-expansion', 'derivatives']"
4695727,Limit of $L^p$ norm of a function depending on $p$,"Background Let $(\Omega, \mathcal{F}, P)$ be a probability space (or a finite measure space).
Then, it is well-known that, for a fixed function $f$ with $\lVert f \rVert_\infty := \mathrm{ess.sup}_{x} \lvert f(x) \rvert < \infty$ , we have $$
\lVert f \rVert_p := \left( \int |f|^p \, dP \right)^{1/p}
\ 
\overset{p \to \infty}{\rightarrow}
\ 
\lVert f \rVert_\infty
\text{.}
$$ (See, e.g., this question .) I'm curious what happens if $f$ depends on $p$ .
More precisely, suppose we have a sequence of functions $\{f_p\}_{p=1}^\infty \subset L^\infty$ that converges to $f \in L^\infty$ a.s.
Do we always have $
\lim_{p \to \infty} \lVert f_p \rVert_p
=
\lVert f \rVert_\infty
$ ?
If not always, under what conditions? What I've tried so far Maybe one of the possible sufficient conditions is that $|f_p| \nearrow |f|$ almost surely.
Under this condition, we can prove $\lim_{p \to \infty} \lVert f_p \rVert_p
= \lVert f \rVert_\infty$ as follows. By the assumption, for any $p = 1, 2, \ldots$ , we have $|f_p| \leq |f| \leq \lVert f \rVert_\infty$ a.s.
So the monotonicity of integral implies $\int |f_p|^p \, dP \leq \lVert f \rVert_\infty^p$ , which proves $\limsup_{p \to \infty} \lVert f_p \rVert_p \leq \lVert f \rVert_\infty$ . Now we are going to prove $\liminf_{p \to \infty} \lVert f_p \rVert_p \geq \lVert f \rVert_\infty$ .
To this end, fix any $\epsilon > 0$ and define the events $A_p(\epsilon) := \{|f_p| > \lVert f \rVert_\infty - \epsilon\}$ for $p = 1, 2, \ldots$ .
Then, we have \begin{align}
\int_\Omega |f_p|^p \, dP
\geq 
\int_{A_p(\epsilon)} |f_p|^p \, dP
\geq 
\int_{A_p(\epsilon)} (\lVert f \rVert_\infty - \epsilon)^p \, dP
=
P(A_p(\epsilon)) (\lVert f \rVert_\infty - \epsilon)^p
\end{align} and therefore $$
\lVert f_p \rVert_p
\geq 
P(A_p(\epsilon))^{1/p} (\lVert f \rVert_\infty - \epsilon)
\text{.}
$$ Now, $|f_p| \nearrow |f|$ implies $A_p(\epsilon) \nearrow A(\epsilon) := \{|f| > \lVert f \rVert_\infty - \epsilon\}$ .
Noting that $A(\epsilon)$ is a positive constant (by the definition of $\lVert f \rVert_\infty$ ), by the continuity of measure, we have $$
\lim_{p \to \infty} P(A_p(\epsilon))^{1/p}
= 
P(A(\epsilon))^{0}
= 1
\text{.}
$$ Therefore, we have proven that $\liminf_{p \to \infty} \lVert f_p \rVert_p \geq \lVert f \rVert_\infty - \epsilon$ .
Since $\epsilon > 0$ can be taken arbitrarily small, we obtain $\liminf_{p \to \infty} \lVert f_p \rVert_p \geq \lVert f \rVert_\infty$ as desired. Questions So my questions are: Is the proof that I wrote above correct? Are there any other conditions under which we have $
\lim_{p \to \infty} \lVert f_p \rVert_p
=
\lVert f \rVert_\infty
$ ? Are there any counterexamples?","['probability-theory', 'functional-analysis', 'analysis', 'measure-theory']"
4695738,When can a sum over the sum of squares function be replaced by an integral?,"Say I have some function $f(n,x)$ . For many ""nice"" functions, I find (numerically) that \begin{equation}
\sum_{n=1}^\infty r_2(n) f(n,x)\approx\pi\int_0^\infty dn f(n,x).
\end{equation} Here $r_2(n)$ denotes the sum of squares function. I am aware that the average value of the sum of squares function is asymptotically $\pi$ , which is how I even thought of trying this. I am ideally looking for a way to bound the error for an arbitrary function, or for some class of reasonable functions. My intuition says we have to take into account some bounds on the derivative of the function. I tried converting the sum to 2D, but couldn't find a nice way to use the Abel-Plana formula or something related to get my bound. I also briefly tried reasoning about this measure-theoretically, but without much success.","['sums-of-squares', 'measure-theory', 'approximation', 'approximation-theory', 'real-analysis']"
4695750,A function satisfying certain properties,"I am looking for a smooth nonnegative function $f=f(x)$ defined on $(0,1)$ satisfying the following properties but having a hard time to construct, I'd appreaciate any help: $\max\{|f'(x)|,|f''(x)|\}\le cf(x)$ for some constant $c$ for every $x\in (0,1)$ (desirably I want to be able to choose $c$ as small as I wish) $f$ is zero on a subinterval of $(0,1)$ $f(x)>m$ for some $m>0$ on a subinterval of $(0,1)$ . Can such function exist?","['calculus', 'functions', 'algebra-precalculus', 'real-analysis']"
4695771,Centralizer of rotation matrix,"Let $\theta \in [0,2\pi), \theta \neq 0,\pi$ . What is $C_{\mathbb{R}^{2\times 2}}(R(\theta))$ , where $R(\theta) := \begin{pmatrix} \cos(\theta)&-\sin(\theta)\\ \sin(\theta)&\cos(\theta)\end{pmatrix}$ and $C_{\mathbb{R}^{2\times 2}}(R(\theta))$ is the centralizer of $R(\theta)$ in $\mathbb{R}^{2\times 2}$ ? I think one way to show that $C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}\cdot SO(2)$ is to take some $U \in U(2)$ with $R(\theta) = U
\begin{pmatrix}e^{i\theta}& 0 \\ 0 & e^{-i\theta}\end{pmatrix}U^H$ . Then $C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}^{2\times 2}\cap U\text{diag}_{\mathbb{C}}(2)U^H$ and one has to show that these are exactly the matrices in $\mathbb{R}\cdot SO(2)$ . Solving the equations for real numbers and complex diagonalizing is a bit annoying. Is there a quick elegant way to show this? And is there some general approach for solving such problem?","['matrices', 'group-theory', 'linear-algebra', 'lie-groups']"
4695815,prove that $\displaystyle\lim _{n \rightarrow \infty} n a_n=0$,"Given a positive sequence $\{a_n\}$ , if $$\lim _{n \rightarrow \infty} \ln n \cdot\left(\frac{a_n}{a_{n+1}}-1\right)=\lambda>0,$$ prove that $\displaystyle\lim _{n \rightarrow \infty} n a_n=0$ . Although the given condition appears to have a similar form to the Raabe's test , there are significant differences between them which make it difficult to establish the proof. In this case, we can use the Raabe's test to show that the sequence is approaching zero and then attempt to apply the Stolz-Cesaro theorem. Specifically, we have $$\lim _{n \rightarrow \infty} n a_n=\lim _{n \rightarrow \infty} \frac{a_{n+1}-a_n}{\frac{1}{n+1}-\frac{1}{n}}=-n(n+1)(a_{n+1}-a_n).$$ However, there does not seem to be an obvious way forward from here. Any help or comments on this matter would be greatly appreciated to further advance the proof.","['limits', 'sequences-and-series']"
4695850,Tangent bundle of a fibered product,"There is an argument that I would like to fully understand, but I can't see it, yet. Here is the situation: Given smooth manifolds $X$ , $Y$ and $Z$ and transverse maps $f:X\rightarrow Z$ , $g\rightarrow Z$ , then the fibered product $M:=X\times_{f=g} Y$ is well defined. In fact, it is given by points $(x,y)\in X\times Y$ such that $f(x) = g(y)$ . In other words, it is the preiamge of the diagonal $\Delta_Z\subset Z\times Z$ of the map $F=(f,g): X\times Y\rightarrow Z\times Z$ . I am looking for an expression for the tangent bundle $TM$ using the language of $K$ -theory (i.e. a virtual bundle).
I have read the following argument: The normal bundle of $\Delta_Z$ in $Z\times Z$ is isomorphic to (the pullback of) $TZ$ , so it follows that the tangent bundle of the fibered square is $TM = TX+TY - f^*TZ$ . It must have something to do with the transversallity of $f$ and $g$ but I can't fill in the details. Any help is appreciated:)","['fiber-bundles', 'tangent-bundle', 'algebraic-topology', 'differential-geometry']"
4695867,"Is $C^\infty([0,1]^n)$ a separable Frechet space?","Gaussian Processes (GP) are widely viewed as practical ways to implement Gaussian Measures (GM) numerically. In fact, in many contexts it seems that to each GP corresponds a GM and vice versa, see, e.g., this . The focus there is mainly on GP/M on infinite dimensional spaces. In particular, they consider the separable Frechet spaces $C(I),\,C^k(I),\,AC(I)$ , where $I$ is a real interval. Separability is an important assumption that ensures the fact that The Borel sigma algebra on a Frechet space $X$ coincides with the smallest sigma algebra which makes continous linear functionals on $X$ measurable (see top p.3 here ) Replacing the real interval with a bounded open set in $\mathbb R^n$ changes nothing in their approach. My question is whether the result holds also for $C^\infty$ , let's say on the unit cube -- open or closed. I could not adjust the proof for $C^k$ (obviously), nor could I find a reference, despite scouring the internet. Does anyone know the answer for the question in the title? Thank you!","['general-topology', 'topological-vector-spaces', 'functional-analysis', 'separable-spaces']"
4695884,Showing that $\ln(x)^2+2(x-1)\ln x-3x+1=0$ has only $2$ real solutions,"Is there any elementary way to show that $\ln (x)^2+2(x-1)\ln x-3x+1=0$ has $2$ real solutions on $(0,\infty)$ ? I did it by this way. Let $f(x)=(\ln x)^2+2(x-1)(\ln x)-3x+1$ . signs of $f(\frac{1}{2})$ , $f(1)$ , $f(e^2)$ are $+$ , $-$ , $+$ since $f(x)$ is continuous, due to this changes in sign there should be at least two roots for $f(x)=0$ . I want to know if there's any nicer way to do this.","['logarithms', 'real-analysis', 'lambert-w', 'functions', 'quadratics']"
4695907,Why is the derivative of $e^{-x^2}$ equal to $-2xe^{-x^2}$?,"I was messing around with the derivatives and integrals of $e^{-x^{2}}$ on Desmos, and had mistakenly been integrating the function $\displaystyle{\displaylines{\int_{0}^{x} e^{-x^{2}}dt}}$ which coincidentally is equal to $\displaystyle{\displaylines{-\frac{1}{2}\frac{dx}{dt}e^{-x^{2}}}}$ , but after realizing that this wasn't the actual integral, I tried to see how I could modify the actual integral $\displaystyle{\displaylines{\int_{0}^{x}e^{-t^{2}}dt}}$ to result in this same curve as I mistakenly achieved earlier which I found with the below equation $$
\displaystyle{\displaylines{-2e^{-x^{2}}\left(\lim_{n \rightarrow \infty} \int_{0}^{x} e^{-\frac{t^{2}}{n}}dt\right) = \frac{dx}{dt}e^{-x^{2}}}}
$$ Which it turns out that the limit $\displaystyle{\displaylines{\lim_{n \rightarrow \infty} \int_{0}^{x} e^{-\frac{t^{2}}{n}}dt}}$ approaches the line $\displaystyle{\displaylines{y=x}}$ , so the equation simplifies to: $$
\displaystyle{\displaylines{-2xe^{-x^{2}} = \frac{dx}{dt}e^{-x^{2}}}}
$$ What I'm wondering is why multiplying $\displaystyle{\displaylines{e^{-x^{2}}}}$ by $\displaystyle{\displaylines{-2x}}$ would equal the derivative of the original function. I'd wager a guess that the answer might have to do with the property of $\displaystyle{\displaylines{e^{x}}}$ being its own derivative, but I have no clue where the $\displaystyle{\displaylines{-2}}$ comes from. Does anybody know why this might be? Thanks!","['integration', 'derivatives']"
4695919,Finite Levy measure implies finite $L^1$ norm away from 0,"So I'm reading this paper on Symmetrization of Levy processes and I'm trying to wrap my head around a particular detail. Namely, on page 10, for the Levy density $\phi(x)$ with $$\int_{\mathbb{R}^d}\frac{|x|^2}{1+|x|^2}\phi(x)dx<\infty$$ they introduce a function $$\phi_n(x)=\phi(x)\mathbb{I}(x)$$ where $\mathbb{I}(x)$ is $1$ if $x\in\mathbb{R}^d\setminus B_{1/n}(0)$ and $0$ otherwise. Hence $\phi_n$ is simply $\phi(x)$ away from some ball of radius $1/n$ away from the origin 0 (so that $\mathbb{I}$ is simply an indicator function). They make the claim that from the above integral being finite, that: $$\int_{\mathbb{R}^d}\phi_n(x)dx=\int_{\mathbb{R}^d\setminus B_{1/n}(0)}\phi(x)dx<\infty$$ I'm trying to figure out why that is the case. Thanks! EDIT: You may assume $\phi\ge0$ here.","['analysis', 'probability']"
4695955,"Let a, b, c be the sides of the triangle","Let a, b, c be the sides of the triangle and A, B, C be the angles opposite to these sides respectively. If $\mathrm{\sin(A-B) = \frac{a}{a+b}\sin A\cos B - \frac{b}{a+b}\cos A\sin B}$ then prove that the triangle is isosceles I have tried this by just applying the formula of $\sin(a-b)$ and tried to simplify a bit by trigonometry but can't find the right approach to prove the triangle isosceles please help me.","['trigonometry', 'geometry']"
4695961,Element-wise product of a matrix by a vector,"Suppose I have an $m \times n$ matrix $\mathbf{A}$ , and a column $m$ -vector $\mathbf{v}$ . Let $\mathbf{B}$ represents the matrix resulted from the element-wise multiplication of each column of $\mathbf{A}$ with the vector $\mathbf{v}$ . For example, $$\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32} \\
\end{bmatrix}, \qquad
\mathbf{v} = \begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3} \\
\end{bmatrix}, \qquad
\mathbf{B} = \begin{bmatrix}
a_{11} v_{1} & a_{12} v_{1} \\
a_{21} v_{2} & a_{22} v_{2} \\
a_{31} v_{3} & a_{32} v_{3} 
\end{bmatrix}$$ What do we call this kind of multiplication? Is it a Hadamard product?","['matrices', 'terminology']"
4695967,Motivation for representation theory,"Next year I have choose a few optative courses from a big list, but syllabi are not available yet, so I have to make a choice based on names only. I am thinking about taking one titled ""Groups and representations"". I have really enjoyed my Abstract Algebra courses so far, specially the part that covered Group theory. However, we never got into Representation theory, and I ignore what mathematical interest it might have. I was wondering if someone could provide me with a text explaining what is representation theory about and what is the mathematical interest or motivation behind it. I have found some books on the topic, but they all just start giving definitions and theorems and never give a good introduction.","['motivation', 'representation-theory', 'book-recommendation', 'abstract-algebra', 'group-theory']"
4696064,K-Medoid Clustering,"The question I am trying to answer is: (k-medoids clustering) What are the resulting clusters when the k-medoids algorithm is used with $k = 2$ and initial random medoids $\{(1, 2), (2, 1)\}$ on the above dataset $S$ ? The dataset $S$ : \begin{array} {|c|c|}\hline 1 & 2 \\ \hline 2 & 1 \\ \hline 1 & 3 \\ \hline 5 & 4 \\ \hline 6 & 3 \\ \hline 7 & 2 \\ \hline 6 & 1 \\ \hline  \end{array} Where each row is $x_1,x_2,x_3,x_4,x_5,x_6,x_7$ respectively. I believe I am doing the steps correctly but I do not think my answer is correct: First I calculate the distance from each medoid point to each non-medoid point: $distance(1,2)=0,1.41,1,4.47,5.10,6,5.10$ $distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4$ Therefore the resulting cluster is: $\{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\}$ Now is where I could be going wrong. I believe I am supposed to pick a new medoid with the smallest distance. Therefore I choose $x_3 = (1,3)$ . Then I calculate the distances again: $distance(1,3)=1,2.23,0,4.12,5,6.10,5.40  $ $distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4$ However I believe I am doing something wrong because I get new clusters: $\{(x_1,x_3,x_4),(x_2,x_5,x_6,x_7)\}$ . If I were to repeat the steps, I would change the point back to $x_1$ which was the medoid point originally, causing a loop. I have to be doing something wrong, any help is appreciated. I am also fairly certain the final answer should be clusters: $\{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\}$ which was the original cluster I got.","['data-structure', 'clustering', 'discrete-mathematics', 'algorithms']"
4696099,"If $z^{2023}=z+1$, prove $|z|\le 1$ if and only if the real part $\Re(z)\le -\frac{1}2$","$z$ is a complex number, if $z^{2023}=z+1$ , prove $|z|\le 1$ if and only if the real part $\Re(z)\le -\frac{1}2$ My attempt If $|z|\le 1$ , then $|z^{2023}|\le 1\rightarrow |z-(-1)|\le 1$ . Also, $|z|\le 1=|z^{2023}-z|=|z|\cdot|z^{2022}-1|$ , since $z\neq 0$ , we get $$|z^{2023}-1|=|z|\le1\le |z^{2022}-1|\tag{1}$$ $z\neq 1$ , divide $|z-1|$ on both sides. $$|1+z+\cdots+z^{2022}|\le |1+z+\cdots+z^{2021}|$$ $$|z^{2022}|-|1+z+\cdots+z^{2021}|\le |1+z+\cdots+z^{2021}|$$ $$|z^{2022}|\le 2|1+z+\cdots+z^{2021}|$$ Multiply $|z-1|$ on both sides, we get: $$|z^{2022}|\cdot |z-1|\le 2|z^{2022}-1|$$ I try to make a chain reaction from $(1)$ , but seems not work. Can I have some hint? Thank you! Update: Solutions from the hint by @dxiv Conjugate, and multiply the two equations $$\bar{z}^{2023}=\bar{z}+1,~~~\text{and}~~~z^{2023}=z+1$$ we get $$|z|^{2\cdot 2023}=|z|^2+2\Re(z)+1$$ Finally, $$|z|\le1\Longleftrightarrow |z|^{2\cdot 2023}\le|z|^2\Longleftrightarrow 2\Re(z)+1\le0\Longleftrightarrow \Re(z)\le -\frac{1}2$$","['complex-analysis', 'algebra-precalculus', 'complex-numbers', 'inequality']"
4696102,Order of eigenvectors within basis for Jordan Normal Form?,"I'm currently baffled as I thought that the order of eigenvectors within the basis of a JNF decomposition doesn't matter. I may have a made a mistake in my working, but if not, is there a general rule for the order? This question specifically concerns a case where there is only one eigenvalue and therefore doesn't seem to be a clear order to put the eigenvectors in. We have: $$
\begin{equation*}
A = 
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\end{equation*}
$$ giving us the following characteristic polynomial: $$ det(A - \lambda I) = (-\lambda + 3)(\lambda^{2} - 6\lambda + 9) $$ $$ \therefore \lambda_{1} = 3  \ \textrm{(with algebraic multiplicity 3)}$$ Calculating the eigenvectors, we get $$ 
\begin{equation*}
E_{\lambda_{1}} = span \{
\begin{bmatrix}
1 & 0 & 0 \newline
\end{bmatrix}^{T},
\begin{bmatrix}
0 & 1 & 1
\end{bmatrix}^{T}
\}
\end{equation*}
$$ then, using $$
\begin{equation}
\tag{âˆ—}
(A - \lambda I)v_{n+1} = b_{n}
\end{equation}
$$ we can generate our third eigenvector: $$
\left[
\begin{array}{ccc|c}
0 & 0 & 0 & 1 \newline
0 & 1 & -1 & 0 \newline
0 & 1 & -1 & 0
\end{array}
\right]
\xrightarrow{\text{RREF}}
\left[
\begin{array}{ccc|c}
0 & 1 & -1 & 0 \newline
0 & 0 & 0 & 1 \newline
0 & 0 & 0 & 0
\end{array}
\right]
$$ We now have three eigenvectors with which we can form our basis, $B$ : $$
b_{1} = \begin{bmatrix} 0 \newline 1 \newline 1 \end{bmatrix},
\ b_{2} = \begin{bmatrix} 1 \newline 0 \newline 0 \end{bmatrix},
\ b_{3} = \begin{bmatrix} 0 \newline 1 \newline 0 \end{bmatrix}
$$ I originally assumed that the order that the vectors are generated in by (âˆ—) would form a JNF: $$
\begin{align}
J &= B^{-1}AB \newline
&= 
\begin{bmatrix}
0 & 1 & 0 \newline
1 & 0 & 1 \newline
1 & 0 & 0 \newline
\end{bmatrix}^{-1}
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\begin{bmatrix}
0 & 1 & 0 \newline
1 & 0 & 1 \newline
1 & 0 & 0 \newline
\end{bmatrix} \newline
&= 
\begin{bmatrix}
3 & 0 & 1 \newline
0 & 3 & 0 \newline
0 & 0 & 3 \newline
\end{bmatrix}
\end{align}
$$ but this is clearly not in JNF. However, with $$ B = [b_{1}, b_{3}, b_{2}] =
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0 \newline
\end{bmatrix}
$$ we have $$
\begin{align}
J &= B^{-1}AB \newline
&= 
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0 \newline
\end{bmatrix}^{-1}
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0
\end{bmatrix} \newline
&= 
\begin{bmatrix}
3 & 1 & 0 \newline
0 & 3 & 0 \newline
0 & 0 & 3
\end{bmatrix}
\end{align}
$$ This doesn't make much sense to me - is there a general rule for the order that the basis must be in?","['matrices', 'jordan-normal-form', 'linear-algebra', 'matrix-decomposition']"
4696107,Showing Ito integrals equal implies stochastic processes equal,"Suppose for $S<T$ being two real numbers and $(X_t), (Y_t)$ being two progressively measurable processes we have $$C + \int_S^T X_tdB_t \overset{a.s.}{=} D + \int_S^TY_tdB_t$$ for some $C, D \in \mathbb{R}$ . We seek to show that $C=D$ and that $(X_t)_{t \in [S, T]} \overset{a.s.}{=}(Y_t)_{t \in [S, T]}$ . I'm not sure how to begin so I'm considering the special case $C = D = 0$ to start with. I can only think of using either the definition (construction) of the Ito integral, or one of Ito's Lemmas here to work with. So assuming $$\int_S^T X_tdB_t \overset{a.s.}{=} \int_S^TY_tdB_t$$ I seek to show that $(X_t)_{t \in [S, T]} \overset{a.s.}{=} (Y_t)_{t \in [S, T]}$ . I first tried to take expectations of $\int_S^T(X_t-Y_t)dB_t$ and square both sides to apply the Ito Isometry, but I'm not sure that helps, so I'm not sure how to proceed. I also tried a contrapositive argument assuming $(X_t) \overset{a.s.}{\neq} (Y_t)$ to show their Ito integrals cannot be equal, but wasn't sure how to make that work either. Some hints are much appreciated!","['stochastic-integrals', 'stochastic-processes', 'probability-theory']"
4696163,How to rationalize the denominator $\frac{1}{1 + \sqrt[3]{5} - \sqrt[3]{25}}$,"This is a review problem for an introductory Galois theory course. Rationalize the denominator $\frac{1}{1 + \sqrt[3]{5} - \sqrt[3]{25}}$ . There could be many ways to do this, but it's implicit that we use field theory for this problem. I tried looking at the solutions to this question , this question and this question , but they haven't helped me so far. What seems clear is to replace $x = \sqrt[3]{5}$ and then our expression becomes $\frac{1}{1+x-x^2}$ . Now, since $x^3-5$ is the minimal polynomial of $\sqrt[3]{5}$ in $\mathbb Q[x]$ , I think I want to find the inverse of $1+x-x^2$ in the quotient ring $\mathbb Q[x]/(x^3-5)$ , but I'm not sure how. Any hints or suggestions would be appreciated.","['galois-theory', 'abstract-algebra', 'polynomials', 'extension-field', 'radicals']"
4696184,"If $|f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}}$, why is true that $\|\nabla f\|_{\infty} \le c?$","Let $N>4$ and $2a>N$ . Consider the function $f:\mathbb R^N\to\mathbb R$ be a function of class $C^2$ such that $$|f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}}.$$ Is it possible to use this information to deduce that there exists a constant $c>0$ such that $$\|\nabla f\|_{\infty} \le c?$$ At calc class today, it was exhibited as an obvious thing while proving a theorem. Could someone please help me understanding why is that true? If it is not, which extra assumption on $f$ could guarantee that $\|\nabla f\|_{\infty}\le c?$ I am a 2 year physics student and I am not so familiar with this kind of things. Thank you everyone.","['calculus', 'upper-lower-bounds', 'analysis', 'real-analysis']"
4696266,A dominance issue in two ordered sequences of integers,"To finish a proof I have to prove the following: Consider the set of integers $ S= \{ 1,\dots,n \} $ and let $k\le n$ .
Pick $k$ elements of S two times $a_1,\dots,a_k$ , $b_1,\dots b_k\in S$ in such way that: there is a unique way to order the two choices $a_1<\dots<a_k$ and $b_1<\dots<b_k$ , we ask that $a_i\le b_i$ for each $i=1,\dots,k$ . Consider then $a_{k+1}<\dots<a_n$ the remaining
elements in order of $S\setminus\{a_1,\dots,a_k\}$ and $b_{k+1}<\dots<b_n\in S\setminus\{b_1,\dots,b_k\}$ . The claim is that $a_i\ge b_i$ for each $i=k+1,\dots,n$ . For example: $S=\{ 1,2,3,4,5,6 \},$ so $n=6$ , and let $k=3$ . Pick $(a_1,a_2,a_3)=(1,3,5)$ and $(b_1,b_2,b_3)=(2,5,6)$ . Of course we have $a_i\le b_i$ . Taking the complements we have $(a_4,a_5,a_6)=(2,4,6)$ and $(b_4,b_5,b_6)=(1,3,4)$ and here $a_i\ge b_i$ as wanted. The question is really elementary, but I struggled to write a general proof. My progress is: we can assume in the first choise that each $a_i < b_i$ , otherwise if $a_i = b_i$ we can drop this pair. This implies in the complement that $a_{k+1}>b_{k+1}$ and $a_{n}>b_n$ . I tried to use induction on the index between $k+1$ and $n$ but I can't figure it out how to conclude. Also I tried for contraddiction assuming there is an index such that $a_j\le b_j$ , and taking the smaller of those, but I cannot finish the proof. Thank you in advance to those who can answer me.","['order-theory', 'combinatorics', 'natural-numbers']"
4696341,Number of functions such that f(f(x)) = f(f(f(x)),"I have been working on this problem recently: The number of functions $f(x)$ from { $1, 2, 3, 4, 5$ } to { $1, 2, 3, 4, 5$ } that satisfy $$f(f(f(x))) = f(f(x))$$ $\forall x \in$ { $1, 2, 3, 4, 5$ } is ? Though I have been through a variant of this problem asked in this question: Number of functions verifying $f(f(x))=f(x)$. ,
and I have also become acquainted with the graph theoretic approach of the same question asked at Number of functions such that $f(f(f(x)))=f(f(x))$ , while solving this problem in a class, our instructor said there must be at least one $(x,x)$ mapping in the function to satisfy the condition given, where $x \in$ { $1, 2, 3, 4, 5$ }. He then proceeded to take cases of when there are $1, 2, 3, 4,$ and $5$ respectively of such pairs in our mapping, summing these cases to get $756$ , which is the final answer. What I could not acquaint myself with, nor find an explanation for online was why this statement (at least one $(x,x)$ mapping) is true. I could very well be lacking in my research, so it would be greatly appreciated if you could link a resource that explains this (or other variants of the problem).","['graph-theory', 'combinatorics']"
4696426,Is a locally finitely generated module of vector fields on a compact manifold globally finitely generated?,"Let $M$ be a smooth manifold, and $\mathfrak{X}(M)$ be the associated $C^\infty(M)$ -module of vector fields on $M$ . We say that $P\subseteq \mathfrak{X}(M)$ is finitely generated if there exists finitely many elements $X_1,\cdots,X_n$ in $P$ such that, $$P=\left<X_1,\cdots,X_n\right>.$$ We say that $P\subseteq \mathfrak{X}(M)$ is locally finitely generated if there exists an open cover $\{U_\alpha\}$ of $M$ such that $P|_{U_\alpha}$ is finitely generated for each $\alpha\in \Lambda$ . I am thinking of the following question: Suppose $M$ is a compact manifold. Is it true that any locally finitely generated $C^\infty(M)$ submodule of $\mathfrak{X}(M)$ us a finitely generated submodule? If I am not mistaken, this is what is mentioned in some result. Let $P$ be a locally finitely generated $C^\infty(M)$ -submodule of $\mathfrak{X}(M)$ . So, there is an open cover $\{U_\alpha\}$ of $M$ such that $P|_{U_\alpha}$ is finitely generated for each $\alpha$ . Suppose that $M$ is compact, then, this open cover $\{U_\alpha\}$ will have a finite sub cover, say $\{U,V\}$ . Just for simplicity, assume $P|_U=\left<X\right>$ and $P|_V=\left<Y\right>$ . Now, I am trying to see if we can combine these vector fields $X\in \Gamma(U,TU)$ and $Y\in \Gamma(V,TV)$ to get a (finite collection of) vector field(s) that generate $P$ . This may be straight forward, but I am not able to get it now.","['vector-fields', 'differential-geometry']"
4696454,finding $x^{f(x)}$ given a point on $f(x)$,"If we have a function $f(x)$ where any point on the curve can be written as: $$\left((a), (a(1-f'(a)\ln{a})\right), a>0$$ What is $x^{f(x)}$ ? This was a question given to me by my teacher a while back, and i'm still struggling to solve it. He says that the question can be done without the use of any differential equations. Even though he said not to, I got it into a differential equation of the form: $$f'(x)= \frac{x-f(x)}{x\ln{x}}$$ Which did not help me at all, since I couldn't form two integrals with respect to $x$ and $y$ I've spent a very long time attempting various ways of approaching the question, however they all led to dead ends. I am so stuck and would love help with a solution to this (preferably without the use of differential equations, however I would also like to see how you would use differential equations to solve this).","['integration', 'calculus', 'functions', 'ordinary-differential-equations']"
4696463,Find the integrals,"I have a problem solving those two integrals. $\int_{\partial D} \frac{z^3}{e^{z^2}-1}dz$ , $D=\{z: |z|<4\}$ $\int_0^2 \frac{\sqrt{x(2-x)}}{x+3}dx$ Since I found them in the old complex analysis course under the title ""TRAINING SET FOR RESIDUES AND INTEGRALS"", I believe I should solve them using the residue theorem. As for the first integral, I see that the singularity at $0$ is removable, but we are still left with singularities at $\pm \sqrt{2\pi i}$ and $\pm \sqrt{4\pi i}$ . Unfortunately, I don't know how to find residues at those points. I can Taylor expand $e^{z^2}-1$ around any point, but since I divide by this, I don't see how it helps (or maybe is there any easy way to find a Laurent expansion of 1/f, when we have a Taylor expansion of f?) As for the second one, I'm totally lost. Usually, when I have an integral over a real axis, I find a suitable contour (semi-circle, keyhole) where the integral vanishes over a semicircle etc. Here, however, the integral is over a finite interval, and the numerator is ""big"", so even if I integrate over a semicircle, I don't think the upper part (over the circle) will vanish (one of my ideas was to substitute $x=u^2$ , look at the integral over whole real axis, since the one I'm interested in should be just a real part of that, and integrate over a semicircle - it will be just using a residue theorem with residue at $\sqrt{3}i$ , but I don't think the integral over semicircle will vanish (not mentioning the problems with a branch of a square root...). Any help/techniques would be appreciated.","['integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
4696505,Let a function $f$ be differentiable at $3$ and satisfy $f(3)=3f'(3)>0$. Then find the limit $\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x.$,"Let $f:(0,\infty)\to(0,\infty)$ be a function differentiable at $3$ and sayisfying $f(3)=3f'(3)>0$ . Then find the limit $$\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x.$$ The thing is, I managed to solve the problem. I will give the detailed steps how I solved it: We observe that $$L=\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x,$$ is of the form $1^\infty.$ So, we try a popular recommended transformation, as $$L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}.$$ We focus on the limit $\lim_{x\to\infty}x(f(3+\frac 3x)-f(3)).$ We assume $\frac 1x=k$ and as $x\to\infty$ we have, $k\to 0.$ So, $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}.$$ We note that $f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}$ so, we write, $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}=3f'(3).$$ Hence, $$L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}=e^{\frac{3f'(3)}{f(3)}}=e^{\frac{3f'(3)}{3f'(3)}}=e.$$ I hope my solution is correct. But the thing I am confused about is with my  approach. In my solution, I wrote a part $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ and a part $$""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"".$$ But the thing is, just like $$""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"",$$ if I wrote $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ instead of, $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"",$$ my solution would be erroneous as the transformation $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ is not valid. But my question, is, why this is so? Why could I write, $$""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"",$$ and considered this transformation, to be a logical step and $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ to be a faulty transformation (when we are basically, doing the analogous thing in both the cases, i.e as $k\to 0$ then we must have $3k\to 0$ and so, we might replace all the $k$ 's in the limit expression with $3k's$ as both $k$ and $3k$ are approaching the same thing) ? My understanding: My understanding that of why $$""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"",$$ is a valid transformation ? We note, here $k\to 0$ and hence $3k\to 0$ and the limit expression i.e $\frac{f(3+k)-f(3)}{k}$ (before transformation), has all the $k's$ approaching to zero, and as $3k$ approaches zero as well, it doesn't really matter if we replace all the $k's$ in $\frac{f(3+k)-f(3)}{k}$ by $3k$ and write, $$f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}},$$ due to which, $f'(3) $ is invariant due to this transformation. By the same preceding logic, why can't I argue about the correctness of this $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}$$ ?","['limits', 'calculus', 'derivatives']"
4696537,Proving the existence of the limit of a multivariate function,"It is possible to show that the limit $$ \lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}} $$ doesn't exist by approaching the origin in two different ways: along the $x$ -axis the limit equals 1, but along the $y$ -axis the limit equals 0. However if I suppose that $\lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}}=0$ and use the definition of the limit of a function: $$ \forall \varepsilon>0 \; \exists \delta : \forall(x,y) \in \mathbb{R}^2\setminus {(0,0)} \; \; |(x,y)|<\delta \implies \left|\frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}} \right|<\varepsilon $$ I can set $\delta = \varepsilon (|x|+|y|^{\frac{1}{3}})$ and then by definition the limit equals $0$ .","['multivariable-calculus', 'calculus', 'real-analysis']"
4696571,Vector Proof of Lower Bound of Sum of Squares of Cosines in a Triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Is there a vector algebra/calculus proof of the fact that $\cos^{2} A + \cos^{2} B + \cos^{2} C \geq \frac{3}{4}$ for any triangle $\Delta ABC$ ? I have seen many algebra/trig proofs, but I was wondering if there was a way to prove this using vectors, like the way you prove the law of cosines from expanding $(\vec{u} - \vec{v})^{2}$ ? This is my progress so far: A triangle can be specified by two vectors, $\vec{u}$ and $\vec{v}$ ; these form two sides of a triangle (the vertex being at the origin). Then, $\vec{v} - \vec{u}$ forms the other side. Let $u = |\vec{u}|$ and $v = |\vec{v}|$ , and $(\vec{v} - \vec{u})^{2} = (\vec{v} - \vec{u}) \cdot (\vec{v} - \vec{u}) = u^{2} + v^{2} - 2 \vec{u} \cdot \vec{v}$ . If $A$ denotes the angle between $\vec{u}$ and $\vec{v}$ at the origin, then $\cos^{2} A = \frac{(\vec{u} \cdot \vec{v})^{2}}{u^{2} v^{2}}$ . Likewise, if $B$ denotes the angle between $\vec{u}$ and $\vec{v} - \vec{u}$ , then $\cos^{2} B = \frac{((\vec{v} - \vec{u}) \cdot u)^{2}}{(\vec{v} - \vec{u})^{2}u^{2}} = \frac{(\vec{v} \cdot \vec{u} - u^{2})^{2}}{(\vec{v} - \vec{u})^{2} u^{2}} = \frac{(\vec{v} \cdot \vec{u})^{2} - 2u^{2}(\vec{v}\cdot \vec{u})+u^{4}}{(\vec{v} - \vec{u})^{2}u^{2}}$ . If $C$ denotes the angle between $\vec{v} - \vec{u}$ and $\vec{v}$ , then $\cos^{2} C = \frac{((\vec{v} - \vec{u}) \cdot v)^{2}}{(\vec{v} - \vec{u})^{2}v^{2}} = \frac{(\vec{v} \cdot \vec{u} - v^{2})^{2}}{(\vec{v} - \vec{u})^{2} v^{2}} = \frac{(\vec{v} \cdot \vec{u})^{2} - 2v^{2}(\vec{v}\cdot \vec{u})+v^{4}}{(\vec{v} - \vec{u})^{2}v^{2}}$ . Using the common denominator of $(\vec{v} - \vec{u})^{2}u^{2}v^{2}$ , we then have the following: $$\cos^{2}A + \cos^{2} B + \cos^{2} C = \frac{(\vec{u} \cdot \vec{v})^{2}[(\vec{v} - \vec{u})^{2}+u^{2} + v^{2}] - 4(\vec{u} \cdot \vec{v})u^{2}v^{2} +u^{4}v^{2} + u^{2}v^{4}}{(\vec{v} - \vec{u})^{2}u^{2}v^{2}}$$ Simplifying the numerator gives: $$\cos^{2} A + \cos^{2} B + \cos^{2} C = \frac{u^{2}v^{2}(u^{2} + v^{2} - 4(\vec{u} \cdot \vec{v})) + 2(\vec{u} \cdot \vec{v})^{2}(u^{2} + v^{2} - \vec{u} \cdot \vec{v}) }{(\vec{v} - \vec{u})^{2}u^{2}v^{2}}$$ The statement that $\cos^{2} A + \cos^{2} B + \cos^{2} C \geq \frac{3}{4}$ then becomes equivalent to the statement that $$u^{2}v^{2}(u^{2}+v^{2} - 4(\vec{u} \cdot \vec{v})) + 2(\vec{u} \cdot \vec{v})(u^{2} + v^{2} - \vec{u} \cdot \vec{v}) \geq \frac{3}{4} (u^{2} + v^{2} - 2 \vec{u} \cdot \vec{v})u^{2}v^{2}$$ Subtracting the right-hand side from the left-hand side gives: $$\frac{1}{4}u^{2}v^{2}(u^{2} + v^{2} - 10(\vec{u} \cdot \vec{v})) + 2(\vec{u} \cdot \vec{v})(u^{2} + v^{2} - \vec{u} \cdot \vec{v}) \geq 0$$ One thought I had to prove this inequality was to rewrite the left-hand side as follows: $$\frac{1}{4}v^{2}(u^{2} - 2(\vec{u} \cdot \vec{v}))^{2} + \frac{1}{4}u^{2}(v^{2} - 2(\vec{u} \cdot \vec{v}))^{2} + (\vec{u} \cdot \vec{v})^{2} - \frac{1}{2}(\vec{u} \cdot \vec{v})u^{2}v^{2} - (\vec{u} \cdot \vec{v})^{3}$$ It would then suffice to show that $ (\vec{u} \cdot \vec{v})^{2} - \frac{1}{2}(\vec{u} \cdot \vec{v})u^{2}v^{2} - (\vec{u} \cdot \vec{v})^{3} \geq 0$ or equivalently that $(\vec{u} \cdot \vec{v}) ((\vec{u} \cdot \vec{v})-\frac{1}{2}u^{2}v^{2} -(\vec{u} \cdot \vec{v})^{2}) \geq 0$ . However, if $\vec{u} \cdot \vec{v} = \frac{\sqrt{3}}{2}$ and $u^{2} = v^{2} = 1$ (angle $A$ is $30^{\circ}$ ), then $(\vec{u} \cdot \vec{v})(\vec{u} \cdot \vec{v} - \frac{1}{2}u^{2}v^{2}-(\vec{u} \cdot \vec{v})) = \frac{1}{8}(6- 5\sqrt{3}) < 0$ . Is there another way to rewrite $$\frac{1}{4}u^{2}v^{2}(u^{2} + v^{2} - 10(\vec{u} \cdot \vec{v})) + 2(\vec{u} \cdot \vec{v})(u^{2} + v^{2} - \vec{u} \cdot \vec{v})$$ as a linear combination of $(u^{2} - 2\vec{u} \cdot \vec{v})^{2}$ and $(v^{2} - 2\vec{u} \cdot \vec{v})^{2}$ (with coefficients that are all nonnegative?) Note that the equality case should imply that $A = B = C = 60^{\circ}$ , which should be equivalent, in this context, to $u^{2} = 2 \vec{u} \cdot \vec{v}$ and $v^{2} = 2\vec{u} \cdot \vec{v}$ . $A = 60^{\circ} \iff u^{2}v^{2}=4(u \cdot v)^{2}$ ; $B = 60^{\circ} \iff (\vec{u} - \vec{v})^{2}u^{2} = 4(\vec{u} \cdot (\vec{v} - \vec{u}))^{2} \iff u^{4} + u^{2}v^{2} - 2(\vec{u} \cdot \vec{v})u^{2} = 4u^{4} + 4(\vec{u} \cdot \vec{v})^{2}- 8(\vec{u} \cdot \vec{v})u^{2} \iff 3u^{4}  - 6(\vec{u} \cdot \vec{v})u^{2}  = 0 \iff u^{2} = 2 u \cdot v$$ $C= 60^{\circ} \iff (\vec{u} - \vec{v})^{2}v^{2} = 4(\vec{u} \cdot (\vec{v} - \vec{u}))^{2} \iff v^{4} + u^{2}v^{2} - 2(\vec{u} \cdot \vec{v})v^{2} = 4v^{4} + 4(\vec{u} \cdot \vec{v})^{2}- 8(\vec{u} \cdot \vec{v})v^{2} \iff 3v^{4}  - 6(\vec{u} \cdot \vec{v})v^{2}  = 0 \iff v^{2} = 2 \vec{u} \cdot \vec{v}$ )","['algebra-precalculus', 'trigonometry', 'vector-analysis']"
4696629,Proximal operator of squared $\ell_1$-norm,"For any $a \in \mathbb R^d$ and $t \ge 0$ , let $p_t(a)$ be the unique minimizer of $f_t(x;a) := \|x-a\|_2^2 + t\|x\|_1^2$ over $x \in \mathbb R^d$ . Question. Is there an analytic formula for $p_t(a)$ ? Observation 1. By the subdifferential characterization of prox-operators, we know that $p_t(a) = (1+t\partial f)^{-1}(a)$ , where $\partial f$ is the subdifferential operator of $f$ . Thus, if $R := \|p_t(a)\|_1$ , then $p_t(a)$ satisfies $p_t(a) = (1+Rt\partial \|\cdot\|_1)^{-1}a = \mbox{prox}_{Rt\|\cdot\|_1}(a)$ , i.e the components of $p_t(a)$ are given by $$
(p_t(a))_j = \mathrm{ST}(a_j;Rt),
\tag{1}
\label{1}
$$ Here, $\mathrm{ST}(u;\lambda)$ is the well-known soft-thresholding operator defined by $$
\mathrm{ST}(u;\lambda) :=
\mbox{sign}(u)(|u| - \lambda)_+.
$$ However, the issue is that $R$ is unknown, since it depends on the sought-for $p_t(a)$ . Observation 2. From \eqref{1}, we deduce that: if $Rt \ge \|a\|_\infty := \max_j |a_j|$ , then $p_t(a) = 0$ . Thus, we may assume $Rt \le \|a\|_\infty$ . We may therefore search for $p_t(a)$ in form \eqref{1} with $Rt$ restricted to the compact interval $[0,\|a\|_\infty]$ . This is one-dimensional problem! Unfortunately, it's still not a closed-form solution...","['convex-optimization', 'proximal-operators', 'linear-algebra', 'signal-processing', 'convex-analysis']"
4696630,Does the distance induced from a Riemannian metric on a manifold determine the Riemannian structure?,"Let $(M, g), (N,h)$ be Riemannian manifolds and consider $d_g$ and $d_h$ the corresponding distance functions induced from $g$ and $h$ . Suppose $f: M \rightarrow N$ is a bijective isometry in the metric sense i.e. $d_h(f(x), f(y)) = d_g(x,y)$ . Is it true that $f$ must be smooth and, moreover, that $f^* h = g$ ? In more generality, if $M$ and $N$ are only smooth manifolds and $d_M, d_N$ are smooth distance functions, is it true that a bijective isometry $f: M \rightarrow N$ must be smooth?","['riemannian-geometry', 'metric-spaces', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
4696639,relations between obstructions coming from Moore-Postnikov towers,"Roughly speaking, I am interested in understanding what (if any) relationships there are between certain obstructions to constructing sections of a bundle of the form $$
P\times_G G/H\to M
$$ where $M$ is a (compact) manifold, $G$ is a (compact) Lie group with Lie subgroup $H$ and $P\to M$ is a principal $G$ -bundle. More precisely, I know there is an associated universal bundle $$
EG\times_G G/H\to BG
$$ where $EG\to BG$ is the bundle that classifies principal $G$ bundles. I also know that the bundle above admits a Moore-Postnikov tower $\{Z_i\}_{i\in \mathbb{N}}$ such that $Z_{n+1}\to Z_n$ comes from pulling back the fibration $PK_n\to K_n$ (here $K_n=K(\pi_n(G/H),n+1)$ and $PK_n$ is the corresponding path space) by a map $Z_n\to K_n$ . Such a map gives rise to a cohomology class $o_{n+1}\in H^{n+1}(Z_n,\pi_n(G/H))$ , which give obstructions to lifting constructing a section of $EG\times_G G/H\to BG$ over the various skelta of $BG$ . There is a similar Moore-Postnikov tower $\{W_i\}_{i\in \mathbb{N}}$ for the fibration $$EG\to BG$$ that gives rise to similar obstruction classes $\tilde o_{n+1}\in H^{n+1}(W_n,\pi_n(G))$ . Since $EG$ is contractible, I believe that some people might call this a Whitehead tower. At least that is what I believe from looking at the wikipedia page for Postnikov towers . We have a commutative diagram of fibrations $$\require{AMScd}
\begin{CD}
EG @>>> BG;\\
@VVV @VVV \\
EG\times_G G/H @>>> BG;
\end{CD}$$ where the right hand vertical map is the identity and the other three maps are the other maps are the obvious bundle projections. According to this post , the Moore-Postnikov towers have nice naturality properties and so this diagram should induce maps $\Phi_n:W_n\to Z_n$ . There are also maps $\Psi_n:K(\pi_n(G),n+1)\to K(\pi_n(G/H),n+1)$ , induced by the natural projection $G\to G/H$ . My naive expectation is that the following maps should fit together in a new commutative diagram (possibly just up to homotopy) of the form $$\require{AMScd}
\begin{CD}
W_n @>>> K(\pi_n(G),n+1);\\
@V{\Phi_n}VV @V{\Psi_n}VV \\
Z_n @>>> K(\pi_n(G/H),n+1);
\end{CD}$$ where the horizontal maps are the ones giving rise to $o_{n+1}$ and $\tilde o_{n+1}$ above. I further naively believe that if $\alpha:H^{n+1}(W_n,\pi_n(G))\to H^{n+1}(W_n,\pi_n(G/H))$ is the map coming from ""changing coefficients"" then $\alpha(\tilde o_{n+1})=\Phi^\ast_{n}(o_{n+1})\in H^{n+1}(W_n,\pi_n(G/H))$ . Given all of the above, I have a few questions Is all of the above true? If not, is it true with some minor modifications? If it is true, can anyone provide a proof or a good reference? I have a feeling that if it is true it is not that hard to show and that the reason I can't do it is that I only know that Moore-Postnikov towers exist and have various  nice properties, but don't really know how they are constructed. FYI: my background is in differential geometry and less in algebraic topology. My understanding of classifying space and Postnikov towers is coming from reading Hatcher's Algebraic topology book over the last several days. For context, I really want to understand the obstructions $o_n$ , but they seem complicated. On the other hand the obstructions $\tilde o_n$ appear to be more tractable (at least in small dimensions where they are related to classical characteristic classes, e.g. Stiefel-Whitney/Pontryagin classes).","['principal-bundles', 'fiber-bundles', 'obstruction-theory', 'algebraic-topology', 'differential-geometry']"
