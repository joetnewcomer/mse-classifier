question_id,title,body,tags
3018862,When can compact group actions be complexified?,"Suppose a compact (real) Lie group $K$ acts holomorphically on a complex manifold $M$ . Let $G$ be the complexification of $K$ . Is there a natural way to obtain an action of $G$ on $M$ extending the action of $K$ ? The example of $S^1$ acting on $\Delta \subset \mathbb{C}$ would show that this is not always possible. But this seems pathalogical in the sense that the action does complexify to an action of $\mathbb{C}^*$ on $\mathbb{C}$ . Is this sort of behaviour the only obstruction? For example if $M$ is $\mathbb{C}^n$ , does the action always complexify?","['complex-geometry', 'lie-groups', 'differential-geometry']"
3018867,Prove by induction $7^n$ is an odd number for every natural number n,I have proved the base case already of n=1 where $7 = 2p+1$ Then I assumed $n=k$ for $7^k = 2p+1$ for $k \in N$ and $p \in N$ To prove $7^{k+1} = 2p+1$ I have these steps so far: $7(7^k) = 2p+1$ $7(2p+1) = 2p+1$ $14p+7 = 2p+1$ I am not sure how to prove from here.,"['induction', 'discrete-mathematics']"
3018904,Limits at Infinity and limit equality,"I'm given function $f:(a,\infty)\to\mathbb{R}$ which has a limit at infinity, i.e., $\lim_{x \to \infty}f(x)$ exists, call it $L$ . And I want to show that given a function $g(x) := {f(1/x)},$ which is defined on $(0,1/a),$ that this function $g(x)$ has a limit at 0 if and only if the limit of $f$ as $x$ tends to infinity exists. I know I have to use the $\epsilon - \delta$ defintion, but before that I think the following is an equivalent formulation: \begin{gather}
\lim_{x \to \infty}f(x) = \lim_{x \to 0}f(1/x).
\end{gather} I know this is just an exercise in chasing the $\epsilon - \delta$ notation, but I think the ""trick"" here is to use the fact that if $f$ has a limit at infinity, then for all $\epsilon > 0,$ there exists $M > a$ such that for all $x \geq M$ we have that $|f(x) - L| < \epsilon$ . So I think the idea here is to pick my $\delta$ as $1/M$ since we have that \begin{gather}
x \geq M \implies 1/x \leq 1/M
\end{gather} and we know that if $x \geq M$ then $|f(x) - L| < \epsilon.$ So if we suppose $\epsilon_0 > 0$ and that $|f(1/x) - L| < \epsilon_0$ will $\delta_0 = 1/M$ suffice? My intuition says yes, but I am not sure how to formulate this rigorously.","['limits', 'epsilon-delta', 'analysis', 'real-analysis']"
3018922,Reference for Hodge decomposition for flag variety,"Any references for proof of the following facts: The cohomology of the (complex) flag variety is always in $(p, p)$ -type of Hodge Decomposition. The natural map $G/T → G_\mathbb{C}/B$ is a diffeomorphism, where $G$ is compact connected real Lie group and $T$ is its maximal torus. $G_\mathbb{C}$ denote the complexification of $G$ and choose a Borel subgroup $B$ containing the complexification of $T$ .","['hodge-theory', 'complex-geometry', 'reference-request', 'algebraic-geometry', 'homology-cohomology']"
3018937,Physical dimensions in math,"I was interested in the idea of of formalising the idea of physical dimensions with an algebraic structure containing ""all physical quantities of any type"". You'd need: Scalar multiplication over the reals (so you can get ""2 kg"" from ""2 * kg"") Addition within the same dimension (so you can have ""kg + kg = 2 kg"") Multiplication of any two elements (so you can have ""J = N m = N * m"") Inverses (so you can have ""m/s = m * s^(-1)"") A tensor algebra could formalise this system -- but then you'd get all sorts of objects like ""1 kg + 1 m"", which make no sense. A group would make sense -- with sub-groups like ""mass measurements"", ""time measurements"", ""real numbers"", ""units"" -- but then you can't have zero. Plus, I'd like to have some notion of units or ""unit vectors""/""unit tensors"". What's a good way to formalise this?","['dimensional-analysis', 'abstract-algebra', 'tensor-products', 'physics', 'group-theory']"
3018944,Normal vector to a polar curve,I'm struggling with working through a proof. Suppose I have a polar curve of the form $r = f(\phi)$ . How do I find the $\textbf{normal vector} $ to this curve? The end result I need should be in terms of $\hat{r}$ and $\hat{\phi}$ but I'm unsure of what I should do. All I know so far is how to find the tangent slope and normal slope at any given $\phi$ . Any help would be very appreciated. Thanks!,"['polar-coordinates', 'differential-geometry']"
3018973,Two players placing coins on a table- Extension,"The origin of my question comes from a common job interview question where two players take turns placing coins on a round table. The coins cannot overlap and can't be moved once they've been placed. The player which first has no available space on the table to place a coin, loses. The intuitive strategy for the first player in this game is to place their first coin in the centre of the table, and then place their ensuing coins collinear to the central coin and his opponent's previously placed coin, as well as equidistant from the centre as his oppponent's coin. I then question what would happen if the table was an equilateral triangle. The strategy as described above falls apart, and unfortunately I have not yet come up with a well defined strategy for the first player to win (if there exists one) without some pretty restricting assumptions. I am looking for some help with this.","['algorithmic-game-theory', 'game-theory', 'recreational-mathematics', 'discrete-mathematics']"
3019001,Math puzzle - sudoku like,"I am having problems solving this puzzle. The sum of each 3x3 square should be 2019, how to find the number in the bottom right corner? Labelling each field we can gain some information about the numbers by subtracting two neighbouring squares, but is looks like a very cumbersome and long proces. And am not even sure that it would solve the problem. Maybe one can find some invariant to use?","['number-theory', 'combinatorics']"
3019003,Finding Christoffel Symbols using via variational method.,"I'm trying to find the Christoffel Symbols for the Lorentz metric $${\rm d}s^2 = \cos(2\pi x)({\rm d}x^2-{\rm d}y^2) - 2\sin(2\pi x)\,{\rm d}x\,{\rm d}y$$ by looking at the Euler-Lagrange equations for $$L(x,\dot{x},y,\dot{y}) = \cos(2\pi x)(\dot{x}^2-\dot{y}^2) - 2\sin(2\pi x)\,\dot{x}\,\dot{y}.$$ I have already done my fair share of computations like this, but I must be making some algebraic mistake that I cannot find for the life of me. If we write $$\begin{align}\frac{\partial L}{\partial x} - \frac{{\rm d}}{{\rm d}t}&\left(\frac{\partial L}{\partial \dot{x}}\right) = -2\pi\sin(2\pi x)(\dot{x}^2-\dot{y}^2)-4\pi\cos(2\pi x)\dot{x}\dot{y} \\ &\qquad - \frac{{\rm d}}{{\rm d}t}\left(2\dot{x}\cos(2\pi x) - 2\dot{y}\sin(2\pi x)\right),\end{align}  $$ and we will have a term with $\ddot{y}$ . This is a problem, since as far as I understand the geodesic equation corresponding to the $x$ -coordinate should have the form $$\ddot{x} + \Gamma(\dot{x},\dot{y})=0,$$ maybe after dividing by something. What is going on?","['calculus-of-variations', 'multivariable-calculus', 'riemannian-geometry', 'differential-geometry']"
3019020,Convergence of discrete-time Markov chain to Feller processes,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(X_t)_{t\ge0}$ be a Feller process on $(\Omega,\mathcal A,\operatorname P)$ $(h_d)_{d\in\mathbb N}\subseteq(0,\infty)$ with $$h_d\xrightarrow{n\to\infty}0$$ $\left(Y^{(d)}_n\right)_{n\in\mathbb N_0}$ be a time-homogeneous Markov chain on $(\Omega,\mathcal A,\operatorname P)$ and $$X^{(d)}_t:=Y^{(d)}_{\lfloor\frac t{h_d}\rfloor}\;\;\;\text{for }t\ge0$$ for $d\in\mathbb N$ $N$ be a Poisson process on $(\Omega,\mathcal A,\operatorname P)$ with parameter $1$ independent of $Y^{(d)}$ for all $d\in\mathbb N$ and $$N^{(d)}_t:=N_{\frac t{h_d}}\;\;\;\text{for }t\ge0$$ as well as $$\tilde X^{(d)}_t:=Y^{(d)}_{N^{(d)}_t}\;\;\;\text{for }t\ge0$$ for $d\in\mathbb N$ Note that $N^{(d)}$ is a Poisson process with parameter $h_d^{-1}$ for all $d\in\mathbb N$ . How can we show that (in probability with respect to the Skorohod topology) $X^{(d)}\xrightarrow{d\to\infty}X$ iff $\tilde X^{(d)}\xrightarrow{d\to\infty}X$ ? In the book of Kallenberg, the author is mentioning that the claim follows from the following two theorems: I don't get how we need to apply them. Clearly, for fixed $t\ge0$ , we can consider $$\frac1d\sum_{i=1}^d\left(N^{(i)}_t-N^{(i-1)}_t\right)$$ with $N^{(0)}_t:=0$ . However, while independent, the $N^{(i)}_t-N^{(i-1)}$ are not identically distributed ... If it's hard to prove in the general setting, it's okay for me to assume $h_d^{-1}=d$ for all $d\in\mathbb N$ . In that case, the strong law of large numbers yields $$\sup_{t\in[0,\:T]}\left|\frac1d N^{(d)}_t-t\right|\xrightarrow{d\to\infty}0\;\;\;\text{almost surely for all }T>0\tag1.$$ Now, let $\tau^{(d)}_0:=0$ , $$\tau_n^{(d)}:=\inf\left\{t>\tau^{(d)}_{n-1}:N^{(d)}_t-N^{(d)}_{\tau^{(d)}_{n-1}}>0\right\}\;\;\;\text{for }d\in\mathbb N$$ and $$\lambda^{(d)}_t:=\sum_{n=0}^\infty1_{\left[\frac nd,\:\frac{n+1}d\right)}(t)\left(\tau^{(d)}_n+(dt-n)\left(\tau^{(d)}_{n+1}-\tau^{(d)}_n\right)\right)\;\;\;\text{for }t\ge0$$ for $d\in\mathbb N$ . Moreoverr, let $T>0$ and $\rho_T$ denote the metric inducing the Skorohod $J_1$ -topology on the space of càdlàg functions $[0,T]\to\mathbb R$ . We should obtain $$\rho_T\left(X^{(d)},\tilde X^{(d)}\right)\le\sup_{t\in[0,\:T]}\left|\lambda^{(d)}_t-t\right|+\sup_{t\in[0,\:T]}\left|X^{(d)}_t-\tilde X^{(d)}_{\lambda^{(d)}_t}\right|\tag2,$$ where the last term should be $0$ . So, if we could show that the first term converges in probability to $0$ as $d\to\infty$ , we should be able to conclude (since $T$ was arbitrary).","['stochastic-analysis', 'markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory']"
3019033,What is the relationship between the determinant and the derivative of a linear map?,"I know determinants tell you the oriented volume of the parallelepiped after the linear transformation, but if you define the derivative as I do below then it seems equivalent, at least in terms of the “slope” of the linear transformation. Or at least I hope someone can answer what exactly it is I’m defining below. It gives the derivative in single variable but doesn’t give best linear approx. in multivariate case. Is it a kind of directional derivative? The typical definition of a derivative is $$f'(x) \approx \frac{f(x+dx) - f(x)}{dx}$$ where $dx$ is an infinitesimal (appealing to nonstandard analysis here for simplicity). This is usually motivated as the slope of the tangent line to a point at $f(x)$ , but the more general formulation of the derivative is the ratio of oriented intervals: Let $I_{ab}$ be an oriented interval in the domain of a function $f$ , in most cases we will want this to be an infinitesimal interval so an interval of $[a, a+dx]$ . More generally an oriented interval is an oriented n-dimensional hypercube/hyper-rectangle/parallelotope, if $f: \mathbb{R} \rightarrow \mathbb{R}$ then $I_{ab}$ is the interval $[a,b]$ where $a,b \in \mathbb{R}$ , so it is a 1-dimensional oriented hypercube, such that $ab = -ba$ (suppresed $I$ notation, treating the interval algebraically like a simplicial complex). The magnitude of the coefficient to the interval is the size of the interval (i.e. the volume of the interval hypercube) and the sign of the coefficient is the orientation, so we start with a positive unit interval $I_{ab}$ and see how the function changes the interval size and orientation. Then the derivative of a function $f: \mathbb{R}\rightarrow \mathbb{R}$ is: $$ f'(x) \approx \frac{f(I_{ab})}{I_{ab}} $$ For example, if $f(x) = -2x$ then $f(I_{ab}) = 2I_{ba} = -2I_{ab}$ and hence $f'(x) = \frac{-2I_{ab}}{I_{ab}} = -2$ . So more generally for a function $f: \mathbb{R}^M \rightarrow \mathbb{R}^N, f' = \frac{f(I^N)}{I^M}$ where $I^N$ refers to an N-dimensional oriented interval. For a 2x2 matrix, representing a linear map from $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ , the interval is an oriented 2-dimensional hypercube (a square), with corners denoted $abcd$ , read clockwise for a positive orientation, such that $abcd = -adcb$ . Consider the linear map $F: \begin{bmatrix}-2&0\\0&2 \end{bmatrix}$ , which simply inverts one orthonormal basis and scales by 2. We know the determinant is -4.  Label an arbitrary 2-hypercube with corner points $abcd$ read in clockwise orientation. This linear map will map this interval such that $F(I_{abcd}) = 4I_{adcb} = -4I_{abcd}$ (suppresing $I$ for easier reading, $F(abcd) = 4adcb = -4abcd$ , since the map flips the originally clockwise orientation of the corners to counter-clockwise, hence negative sign. Now we calculate $F' = \frac{-4I_{abcd}}{I_{abcd}} = -4$ , the same as the determinant. In the general case, $f(I^N)$ may map an oriented unit hyper-cube to an arbitrary N-parallelotope, but it is still a kind of interval with orientation which gives the sign of the determinant. I already knew determinants tell you the volume of the parallelepiped after the mapping but in the way I've defined a derivative here, in what sense is the determinant any different than the slope of the derivative of the linear map represented by the matrix? The determinant seems analogous to the derivative to me. The derivative tells you how much an (infinitesimal) interval is scaled (magnitude of derivative) and if its orientation changes (sign of derivative), which is exactly what the determinant is telling you about a linear map.","['determinant', 'calculus', 'linear-algebra', 'linear-transformations', 'derivatives']"
3019050,Find sum of series $ \sum_{n=1}^{\infty} (n\cdot \ln \frac{2n+1}{2n-1} - 1) $ [duplicate],"This question already has answers here : Series involving log $\sum_{n=1}^{\infty} \left( n\log \left(\frac{2n+1}{2n-1}\right)-1\right) = \frac{1-\log 2}{2}$ (3 answers) Closed 3 years ago . How can I find sum of series $ \sum_{n=1}^{\infty} (n\cdot \ln \frac{2n+1}{2n-1} - 1) $ ?
It is so weird for me because I put this to Mathematica and it tells me that sum does not converge... Let consider sum no to infinity, but to n $$ \sum_{k=1}^{n} (k\cdot \ln \frac{2k+1}{2k-1} - 1) =$$ $$ ln \frac{3}{1}\cdot \left(\frac{5}{3}\right)^2 \cdot...\cdot \left(\frac{2n+1}{2n-1}\right)^n - n = \ln \frac{1}{1}\cdot \frac{1}{3}\cdot \frac{1}{5}\cdot ... \frac{1}{2n-1} - n $$ but $$ n = \ln e^n $$ so
it will be $$ln\frac{1}{e^n} \cdot \frac{1}{1}\cdot \frac{1}{3}\cdot \frac{1}{5}\cdot ... \frac{1}{2n-1}$$ So the limit of it is $-\infty$ Have I done this well or I missed sth?","['sequences-and-series', 'real-analysis']"
3019055,Interval in which $(\cos p-1)x^2+\cos p.x+\sin p=0$ has real roots,"The equation $(\cos p-1)x^2+\cos p.x+\sin p=0$ where $x$ is a variable has real roots. Then the interval of $p$ may be any of the following: $$
(a)\quad(0,2\pi)\quad (b)\quad (-\pi,0)\quad (c)\quad \big(\frac{-\pi}{2},\frac{\pi}{2}\big)\quad (d)\quad(0,\pi)
$$ The solution given in my reference is the interval $(0,\pi)$ . My Attempt $$
\Delta=\cos^2p-4(\cos p-1)\sin p\geq 0\\
\cos^2p-4\sin p\cos p+4\sin p\geq 0\\
\Delta'=16\sin^2p-16\sin p\leq0\implies\sin^2p\leq\sin p\\
$$ Similar problem has been asked before Find the range of values of $p$ if $(\cos p−1)x^2+(\cos p)x+\sin p=0$ has real roots in the variable $x$ . but it does not address how to prove it analytically.",['trigonometry']
3019105,"Distribution of minimum of difference between uniforms $(0,1)$","Let $U_1,...,U_N$ be a sequence of independent uniformly $(0,1)$ random variables. Define $Y_k := U_k - U_{k+1}$ . Find the distribution of $\min\left\{Y_k\right\}_{k=1}^{N-1}$ . I've get the distribution of $U_k - U_{k+1}$ , but I wasn't able to get the distribution of minimum. Any ideas?","['probability-distributions', 'uniform-distribution', 'probability-theory', 'probability']"
3019131,"If a first derivative doesn't exist at a certain point, is it not a critical point?","Say $f'(x)$ of a function was $(x+2) \over (x+3)$ . If $x = -2$ then $f'(x)$ = 0, which means that at this point, there would be a local min or max. But what if $x = -3$ ? It doesn't exist for $f'(x)$ , so do we just ignore it? State it DNE at $x = -3$ so it is not a critical point?",['calculus']
3019165,Average distance from center of circle,"Using calculus, we can show that the average distance of a point in a circle to the center is $2R/3$ , where $R$ is the radius. However, I have a separate way of approaching this question through intuition that gives me a different answer, and I'd like to know why my intuition fails. For each $\theta\in [0,2\pi)$ , we can consider the line segment of that angle from the center of the circle to the boundary. On this line segment, the average distance from the center should be $R/2$ . Then the average distance from the center over all points in the circle should just be $R/2$ as well, since we can cover the circle with these line segments. Why does this intuitive approach give the wrong answer? My best guess is that these line segments all share the origin, so this method counts the origin's distance from itself multiple times, thereby throwing off the average by decreasing it, which agrees with the fact that we know the actual answer is greater. However, couldn't I just look at the average distance from the center for the open line segments that exclude the center? The average distance for these open line segments should still be $R/2$ , and then I could apply the same argument for covering the circle with the open line segments. This time, I'd be missing the center, but missing a single point shouldn't throw off the answer. Why does this argument not work?","['intuition', 'calculus', 'geometry', 'probability']"
3019181,Absolutely continuous spectrum invariant under unitary equivalence,"I am doing an exercise calculating the absolutely continuous spectrum of some operator $A$ by calculating the spectrum of a different operator $B$ unitarily equivalent to $A$ , i.e. $$A = U B U^*.$$ I know that $\sigma (A) = \sigma(B)$ as is easily seen by playing with the resolvent. But I cannot as easily conclude $\sigma_{ac}(A) = \sigma_{ac}(B)$ . Am I missing something obvious? How can one show this? I first thought about something along $$\sigma_{ac}(A) = \sigma(A \mid_{\mathcal H_{ac}}) = \sigma(UBU^* \mid_{\mathcal H_{ac}}) \stackrel{?}{=} \sigma(U B \mid_{\stackrel{\sim}{\mathcal H_{ac}}
}U^*) = \sigma_{ac}(B),$$ but I don't think the third equality is justifiable, especially we have different Hilbert spaces $\mathcal H, \stackrel{\sim}{\mathcal H}$ .","['operator-theory', 'analysis', 'functional-analysis', 'spectral-theory', 'mathematical-physics']"
3019230,"Let $(a_n), (b_n),$ be bounded, then prove that $c_n$ converges and give its value.","Could I get some feedback on the following proof, I feel becoming a good mathematician is through constant feedback and improvement of your work, I tried to make it short, but well-readable. We are given that $a_n$ and $b_n$ are bounded sequences, and also: $$ (n-1)a_n \leq n^2 c_n \leq (n+1)b_n$$ Prove that $c_n$ converges and give the value of the limit. First of all, we know that these sequences are bounded, so surely we have that: $$L \geq a_n \land b_n \leq U$$ For some lower bound $L$ and some upper bound $U$ in $\mathbb{R}$ .
We now apply this: $$ (n-1) L \leq (n-1)a_n \leq n^2 c_n \leq (n+1)b_n \leq  (n+1)U$$ $$ \frac{L}{n}-\frac{L}{n^2}=\frac{(n-1) L}{n^2}  \leq  c_n\leq \frac{(n+1) U}{n^2} =\frac{U}{n}+\frac{U}{n^2}$$ Both these sequence converge to zero, so we have that in the limit: $$0 \leq \lim_{n\rightarrow \infty} c_n \leq 0$$ By the squeeze theorem we have that $\lim_{n\rightarrow \infty} c_n =0$","['limits', 'proof-verification', 'sequences-and-series', 'real-analysis']"
3019246,Is the set $\{|f(0)|: \int_{0}^{1}|f(t)|dt\le1\}$ bounded?,"Let $x_0 \in [0,1]$ and define $T:C[0,1] \rightarrow \mathbb{R}$ by $T_{x_0}(f)=f(x_0)$ . Let $||\cdot||_1$ be a norm on $C[0,1]$ . Is $T_0$ bounded or not? That is, is the set $$
\left\{|T_{0}(f)|:||f||_1 \leq 1\right\}=\{|f(0)|:||f||_1 \leq 1,f \in C[0,1]\}
$$ bounded? Since $||f||_1:=\int_{0}^{1}|f(t)|dt$ , the question may be equivalent to the following: Let $f:[0,1] \rightarrow \mathbb{R}$ be continuous. Is the set $$\left\{|f(0)|: \int_{0}^{1}|f(t)|dt \leq 1\right\}$$ bounded? I guess the answer is no. Because, for example, we can have a function whose graph is a narrow spike at the origin but with infinite height. The area enclosed by the graph may be 1 but the value at the origin $f(0)$ which is its height is infinite. But how can I prove this formally?","['calculus', 'functional-analysis', 'analysis', 'real-analysis']"
3019313,Finding Projectile Angle With Different Elevation When Velocity And Range Are Known,"I'm trying to derive a formula to find the angle $θ$ required to hit a target that may be higher or lower than the initial launch position. My known variables are $g, v_0, y_0, y_f, x_0, x_f$ where: $θ$ is the initial launch angle and the variable being solved for $g$ is the gravity constant $v_0$ is the initial angular velocity $y_0$ is the initial launch elevation $y_f$ is the final landing position (in such cases where the projectile lands lower or higher than the launch position) $x_0$ is the initial horizontal launch displacement $x_f$ is the horizontal landing position, or range $t$ is the time in seconds during the path of motion I started by converting the formula for x position into one for time ( $x = x_0 + v_{x0}t$ becomes $t = \frac{x}{x_0 + v_{x0}}$ I set the equation for total flight time ( $\frac{v_{y0}}{g} + \frac{\sqrt 2h}{g}$ ) equal to this equation, since $v_{xf} = v_{x0}$ and $x_f$ is known. Leading me to $\frac{x}{x_0 + v_{x0}} = \frac{v_{y0}}{g} + \frac{\sqrt 2h}{g}$ I took the equation for maximum height, $\frac{gt^2}{2}$ and substituted the expression for time into it, giving me $\frac{gx^2}{2x_0^2 + 4x_0v_{x0} + v_{x0}^2}$ . Since $v_{y0}$ can be expressed in terms of $v_{x0}$ , I also substituted that in ( $v_{x0} = v_0\cdot cosθ$ and $v_{y0} = v_0\cdot sinθ$ so $v_{y0} = v_{x0}\cdot tanθ$ . From there I continued to simplify and rearrange until I ended up with $θ = cos^{-1}\frac{2gx^2}{v_0}$ , however this doesn't give me the correct answer and this is my third attempt over the past several hours. I've been using the following values in a standard trajectory formula that finds $x_f$ from $θ, v_0, y_0, x_0$ : $g = 9.81 m/s$ $θ = 70 degrees$ $v_0 = 75 m/s$ $y_0 = 3 m$ $y_f = 0 m$ $x_0 = 0 m$ When I plug these into a standard trajectory formula, I find that the range is 369.65957m, which I have been using for $x_f$ . I expect to find $θ \approx 70$ when I solve for $θ$ using $v_0$ and $x_f$ , but other formulas I have found on the web end up giving me ~45 or ~89 degrees. Thanks for any help!","['physics', 'algebra-precalculus', 'projectile-motion', 'trigonometry']"
3019331,Convexity of a Function Implies Function Lies Above Any Tangent Line [duplicate],"This question already has an answer here : Proving that the tangent to a convex function is always below the function [duplicate] (1 answer) Closed 4 years ago . I’m trying to use the following definition to show the result listed below. Definition of Convex Function: Let $f$ be a real-valued function on an open interval. Let $a,b$ be points in the domain with $a<b$ . Function $f$ is convex if $$f(a)+\frac{f(b)-f(a)}{b-a}(x-a)\ge f(x)$$ for each $x\in(a,b)$ . Goal: The function $f$ (defined as in the above definition) is convex if and only if $f(x)$ does not have any points below any tangent line for each $x$ in the domain. Attempted Proof: Suppose $f$ is convex. Let $x_1,x_2$ be points in the domain with $x_1<x_2$ . From the definition of convexity given above, for each $x\in(x_1,x_2)$ , $$f(x_1)+\frac{f(x_2)-f(x_1)}{x_2-x_1}(x-x_1)\ge f(x)$$ Let $x_*$ be a point between $x_1$ and $x_2$ . For now, let $x\in(x_*,x_2)$ . The tangent line to this point is given by $$t(x)=f(x_*)+f’(x_*)(x-x_*)$$ The Mean Value Theorem guarantees the existence of $c\in(x_*,x_2)$ such that $$f’(c)(x-x_*)=f(x)-f(x_*)$$ Next, consider the difference between the function and tangent line; our goal is to show this is nonnegative: $$f(x)-t(x)=f(x)-(f(x_*)+f’(x_*)(x-x_*))=f(x)-f(x_*)-f’(x_*)(x-x_*)=f’(c)(x-x_*)-f’(x_*)(x-x_*)$$ So, we have $$f(x)-t(x)=(x-x_*)(f’(c)-f’(x_*))$$ This is where I’m having difficulties performing any meaningful manipulation. If this can be shown true, the result for the left portion of the function can be shown analogously.","['functions', 'convex-analysis', 'real-analysis']"
3019442,Find the Lebesgue set w.r.t. to the functions,"I'm working on a problem in measure theory: Find the Lebesgue set $L_f=\{x:\lim_{r\to 0} \frac{1}{m(B(r,x))} \int_{B(r,x)}\lvert f(y)-f(x)\rvert dy=0\}$ if: $f:\mathbb R^n \to \mathbb C$ is continuous $f=\chi_{E}$ , where $E\in\mathbb R^n$ is Lebesgue null; $m(E)=0$ . $\chi$ denotes the indicator function. $f(x)=\lfloor x\rfloor$ on $\mathbb R$ I'm having a bit of a tough time wrapping my head around what the Lebesgue sets really are, and consequently is struggling to characterize the Lebesgue sets w.r.t to these functions. I could really use some help on these!","['measure-theory', 'real-analysis']"
3019454,Finite extension of regular local rings,"Let $k$ be an algebraically closed field of characteristic zero.  Let $A$ be a finitely generated regular $k$ -algebra, and let $R$ be the localization of $A$ at a closed point $a\in\operatorname{Spec}A$ .  Write $\mathfrak{m}\subseteq R$ for the unique maximal ideal of $R$ . Let $x_1,\dots,x_n\in \mathfrak{m}$ be a regular system of parameters for $R$ , that is, $x_1,\dots,x_n$ project to a basis of $\mathfrak{m}/\mathfrak{m}^2$ .  Then we have an injective algebra map $$
k[x_1,\dots,x_n]_{(x_1,\dots,x_n)}\to R
$$ My question is: is this map is always finite? $ \ $ I think using a Noether normalization argument one can prove that there exists a system of parameters for which this map is finite.  To prove that, let us assume WLOG $x_1,\dots,x_n\in A$ (localize $A$ by some element if not) and find elements $y_1,\dots,y_t$ which vanish at $a$ such that $x_1,\dots,x_n,y_1,\dots,y_t$ generate $A$ . Then by the proof of Noether normalization, we may take $$z_i=x_i-(\text{high degree polynomial in }y_i\text{'s})
$$ so that $A$ is finite over $k[z_1,\dots,z_n]$ .  By construction, $z_1,\dots,z_n$ is a regular system of parameters for $R$ . $\ $ Even if the above is correct, I would like to know if the map is finite for any system of parameters.  Thanks for any comments/suggestions!","['algebraic-geometry', 'commutative-algebra']"
3019483,Is $|x|^{-\alpha}$ integrable for polynomially bounded measures on $\mathbb{R}^n$,"We know that $|x|^{-\alpha}$ is in $L^1 (x\in \mathbb{R}^n:|x| \ge 1)$ with the normal Lebesgue measure for $\alpha > n$ . But what if we had a measure $\mu$ on $\mathbb{R}^n$ which is polynomially bounded, i.e., $\mu(|x|\le A) \le C(1+A^N)$ where $C,N$ are fixed constants, then would we have something like $|x|^{-\alpha}$ is in $L^1 (\{x\in \mathbb{R}^n:|x| \ge 1\},\mu)$ for $\alpha >N$ ?","['measure-theory', 'lebesgue-measure']"
3019500,Is there a surface on which a hexagon can have all right angles?,"So I was watching a video that features astronomer and topologist Cliff Stoll talking about how figures that aren't quadrilaterals can have all their angles equal 90 degrees on different surfaces. For example, on a sphere, you can create a triangle that has all of its angles equal $90^\circ$ . On a pseudosphere, you can create a pentagon that has all of its angles equal $90^\circ$ . Now, here's my question. Is there a surface where a hexagon with this property is possible?",['geometry']
3019535,Proving $ \frac{\csc x + \cot x}{\tan x + \sin x} = \cot x\csc x $,"I am currently working on understanding trig identities.
A question has me stumped, and no matter how I look at it, it never leads to the proof. I believe I am making a mistake when dividing multiple fractions. $$ \frac{\csc x + \cot x}{\tan x + \sin x} = \cot x\csc x $$ For my first step I break up the $\csc x$ and $\cot x$ in the numerator and add them together to make: $$\frac{\frac{1+\cos x}{\sin x\cos x}}{\tan x+\sin x}$$ I then simplify further and end up at: $$ \frac{\cos x+\cos^2 x}{\sin^2 x\cos^2 x} $$ From here on I don't see any identities, or possible ways to decompose this further.","['trigonometry', 'problem-solving']"
3019536,"Arranging 5 D's, 6 E's and 3 F's such that the First D precedes the First E which precedes the First F","I am trying to solve this problem and looking for any tips.
I have 5 D's, 6 E's and 3 F's and I have two conditions. The first D must be before the first E. The first E must be before the first F. I thought I could solve this through determining the ways to make a string of letters with a D before an E before an F, however, that is not enough because it does not cover instances like DEFDEF... or DEFFD... since there can be letters that come after the initial string that meets the conditions. I also contemplated using the Inclusion Exclusion principle as follows cases where a D comes before an E + cases where an E comes before an F subtract their intersection (which I'm not sure how I'd calculate) however I still cannot account for cases where there are letters after the initial D, E, F... Any ideas?","['permutations', 'combinatorics', 'discrete-mathematics']"
3019573,Improper integral over the rationals,"Question: Suppose that I wish to integrate a function over the natural numbers. How could I do this? Answer: Consider the definite integral $\int_a^bf(x)\ dx$ . If we consider this as the 'area under the curve' between $x=a$ and $x=b$ , then $\int_a^bf(x)\ dx$ is the sum of the areas infinitely many rectangles, each with an area of $f(x)\epsilon$ , where $\forall x\in\mathbb{R}^+.\epsilon<x$ . Thus: $$\int_a^bf(x)\ dx=\lim_{\epsilon\to0}\sum_{x=a}^{b/\epsilon}f(x)\epsilon$$ For $x\in\mathbb{N}$ , this is simply a finite sum, since there are no terms between $x=n$ and $x=n+1$ : $$\int_{M\subset\mathbb{N}}f(x)\ dx=\int_a^bf(x)\ dx=\lim_{\epsilon\to 0}\sum_{x=a}^{b}f(x)\epsilon\ \mid a,b,x\in\mathbb{N}$$ This summation is zero for all $a$ and $b$ ,
UNLESS $b=\infty$ and $a<b$ , in which case the infinite sum, corresponding to the improper integral $\int_a^\infty f(x)\ dx$ can be nonzero, the nicest case being $a=0$ : $$f:\mathbb{N}\to X\implies\int_0^\infty f(x)\ dx=\lim_{\epsilon\to0}\sum_{x=0}^\infty f(x)\epsilon>0$$ Evaluating this sum can be incredibly difficult, but it is possible. Now, the big question :
What is the improper integral of $f(x)$ over the rationals? i.e.: $$f:\mathbb{Q}\to X\implies\int_0^\infty f(x)\ dx=\ \Large?$$ I would assume that because the rationals are dense $$f:\mathbb{Q}\to X\land g:\mathbb{N}\to X\implies\int_0^\infty f(x)\ dx\gg\int_0^\infty g(n)\ dn$$ This following from the fact that for a finite natural number $b$ : $$\sum_{0\leq x\leq b\\x\in\mathbb{Q}}f(x)\gg\sum_{0\leq x\leq b\\x\in\mathbb{N}}f(x)$$","['measure-theory', 'improper-integrals', 'nonstandard-analysis', 'sequences-and-series', 'indeterminate-forms']"
3019607,Writing an iterated double integral in two forms,"Let $f\left(x,\:y\right)\:=\:x^2e^{x^2}$ and let $R$ be the triangle bounded by the lines $x=5$ , $x=y/2$ , and $y=x$ in the $xy$ -plane. Express $\int _RfdA$ in two different ways. After sketching the region, I got that the first way to write the integral would simply be: $\int _0^5\int _x^{2x}\:x^2e^{x^2}dydx$ However, I was stuck on how to write it in the other way. With the region, there is no obvious way to write the integral as one in terms of $y$ . I feel that this may mean that I need to have the sum of two different double integrals but I am not entirely sure how that would work in this case. Would I have to subtract the higher $x=(1/2)y$ line from the $x=y$ line or is there some other way? Any help would be highly appreciated!","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
3019625,Show that $\int_{0}^{2\pi} \cos^2(x) dx = \int_{0}^{2\pi} \sin^2(x) dx$,"I'm trying to follow the argument in the image below, which aims to show that: $\int_0^{2\pi} \cos^2(x) dx = \int_0^{2\pi} \sin^2(x) dx$ . I believe this on an intuitive level, and I understand that it uses the periodicity of the sine and cosine functions to make the point. But, I'm stuck because the argument seems to only show that $\int_{x=0}^{x=2\pi} \cos^2(x)dx = \int_{u=0}^{u=2\pi} \sin^2(u) du$ , and the variable $u$ is not the same as $x$ . Would you be able to clarify this argument?","['integration', 'calculus', 'trigonometry']"
3019627,Conditional Probability of a Uniform Random Subset.,"Question: Let X = $({1,2,3,..,10})$ Let $Y$ be a uniformly random subset of $X$ . Define the events: A = “ $Y$ contains at least $4$ elements"", B = “all elements of $Y$ are even"". What is $Pr(A|B)$ ? Answer: 0.1875 Attempt: I know that P(A $\bigcap$ B) / P(B) is what I have to ultimately find. For $P(B)$ = $\frac{5}{10}$ = $\frac{1}{2}$ For P(A) => Must have exactly 4, 5, 6, 7, 8, 9, 10 elements So, I used the binomial for this by doing: P(A) = $10\choose4$$.$ $\frac{1}{10}$$^4$$.$$(1-\frac{1}{10}$ ) $^6$ + $10\choose5$$.$ $\frac{1}{10}$$^5$$.$$(1-\frac{1}{10}$ ) $^5$ + $10\choose6$$.$ $\frac{1}{10}$$^6$$.$$(1-\frac{1}{10}$ ) $^4$ + $10\choose7$$.$ $\frac{1}{10}$$^7$$.$$(1-\frac{1}{10}$ ) $^3$ + $10\choose8$$.$ $\frac{1}{10}$$^8$$.$$(1-\frac{1}{10}$ ) $^2$ + $10\choose9$$.$ $\frac{1}{10}$$^9$$.$$(1-\frac{1}{10}$ ) $^1$ + $10\choose10$$.$ $\frac{1}{10}$$^{10}$$.$$(1-\frac{1}{10}$ ) $^0$ + = $0.012795$ P(A $\bigcap$ B) = $\frac{1}{2}*0.012795$ Pr(A|B) = $\frac{\frac{1}{2}*0.012795}{2}$ Pr(A|B)  = $0.012795$ Where did I go wrong with this approach? I find finding P(A) very time consuming, I feel like there has to be a much more simpler approach.","['conditional-probability', 'discrete-mathematics', 'probability-theory', 'probability', 'random-variables']"
3019652,Difficult Region of Integration Involving Gauss's Theorem,"I'm told to use Gauss's Theorem to compute the flux of a field $\vec F = <x,y^2,y+z>$ along the boundary of the cylindrical solid $x^2+y^2 \le 4$ below $z=8$ and above $z=x$ . I know by Gauss's Theorem that: Net Flux = $\iint_{\partial D} \vec F \cdot \vec ndS = \iiint_D \nabla \cdot \vec FdV$ This computation is pretty straight forward. $\nabla \cdot \vec F = 2+2y$ . But the region of integration is particularly difficult to map out. I thought to use cylindrical coordinates and setting the bounds to $0 \le \theta \le 2 \pi$ , $0 \le z \le 8$ , and $0 \le r \le 4$ , but this seems like it would just give me the area of the cylinder of height 8--and wouldn't include the part where z=x slices through the cylinder. What would be the right way to go in terms of the bounds of integration?","['integration', 'divergence-operator', 'multivariable-calculus', 'calculus', 'physics']"
3019663,Hall's Marriage Theorem for correspondence,"Let $A=\{A_1,....A_n\}$ be a collection of subsets of a finite set $X$ . A selection for $A$ is the image of an injective function $f:A\to X$ such that $   f(A_i)\in A_i$ for every $A_i\in A$ . Hall's marriage theorem shows that , $A$ has a selection if and only if for each subset $S\subseteq A$ , $$|S|\leq |\cup_{i\in S} A_i|.$$ I wonder if it is possible to generalize this result and obtain a similar condition for a choice correspondence $f:A\rightrightarrows X $ that selects for each $A_i$ more than one elements, i.e. $f(A_i)\subset A_i$ and $|f(A_i)|=2$ and all selected elements are distinct. Any ideas or suggestions?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3019664,"Prove that $f(x) = \sum_{n=1}^{\infty} x^n/n^2$ is continuous on $[0,1]$","I have a general idea of how to prove this but I could use some help with the details. Basically I see that $f(x)$ is the uniform limit of $f_k(x) = \sum_{n=1}^{k} x^n/n^2$ on $[0,1]$ . Each $f_k$ is continuous, so $f$ is as well since uniform convergence preserves continuity. Is this proof correct/does it seem sufficient? Thanks ahead of time.","['proof-writing', 'proof-verification', 'real-analysis']"
3019742,How to give counterexample for given claim,"Suppose $A_1,\dots,A_m$ be distinct $n\times n $ real matrices such that $A_iA_j=0$ for all $i\neq j$ . Show that $m\leq n$ . I think this true because i tried for $3\times 3$ and $2\times 2$ case I got only $3$ and $2$ matrices with that property. But given that this not true . Can any one help me to find counterexample. And what is best approach to tackle such problem.","['matrices', 'linear-algebra', 'examples-counterexamples']"
3019758,"Show that $\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})}$ [duplicate]","This question already has answers here : An integration of product $(1-x^n)$ (2 answers) Closed 5 years ago . $$\int_0^1 \prod_{n\geq 1} (1-x^n) \, dx = \frac{4\pi\sqrt{3}\sinh(\frac{\pi}{3}\sqrt{23})}{\sqrt{23}\cosh(\frac{\pi}{2}\sqrt{23})}$$ This monstrous expression is from Tolaso Network (tolaso.com.gr). I have no idea how to approach it - converting the product to a sum of logarithms does no good, and one cannot switch the order of product/integral either. The product in itself doesn't converge to anything nice either. I am interested in seeing the proof of the above identity, as well as an explanation of how exactly $\sqrt{23}$ becomes involved in such a deceptive integral. Both real and complex analytic solutions are welcome. A proof without the pentagonal number theorem would be nice as well, since that somewhat trivializes the problem.","['integration', 'definite-integrals', 'real-analysis', 'sequences-and-series', 'infinite-product']"
3019766,Prove $\sum_{n=1}^\infty\frac{\cot(\pi n\sqrt{61})}{n^3}=-\frac{16793\pi^3}{45660\sqrt{61}}$,"$$\sum_{n=1}^\infty\frac{\cot(\pi n\sqrt{61})}{n^3}=-\frac{16793\pi^3}{45660\sqrt{61}}.$$ Prove it converges and, evaluate the series. For the first part of the question, I prove it converges by considering the irrationality measure, $$|\sin (n\pi\sqrt{61})|=|\sin (n\pi\sqrt{61}-m\pi)|\ge \frac{2}{\pi}|n\pi\sqrt{61}-m\pi|\ge 2n\left|\sqrt{61}-\frac{m}{n}\right|>\frac{C}{n},$$ so $$\left|\frac{\cot(\pi n\sqrt{61})}{n^3}\right|\le\left|\frac{1}{n^3\sin(\pi n\sqrt{61})}\right|<\frac{C}{n^2}.$$ How to evaluate the series? I have found a related question . I apologize for incorrect information in the previous post.","['diophantine-approximation', 'number-theory', 'complex-analysis', 'sequences-and-series', 'continued-fractions']"
3019798,Proof verification for Identity matrices,"So I have the following question: Analyze the following 'Claim' (which may or may not be true) and the corresponding 'Proof', by writing 'TRUE' or 'FALSE' (together with the reason) for each step. [Note: $I_n$ is the $n \times n$ identity matrix.] Claim: Let $A$ be any $n \times n$ matrix satisfying $A^2=I_n$ . Then either $A=I_n$ or $A=-I_n$ . 'Proof'. Step 1: $A$ satisfies $A^2-I_n = 0$ (True or False) True. My reasoning: Clearly, this is true. $A^2=I_n$ is not always true, but because it is true, I should have no problem moving the Identity matrix the the LHS. Step 2: So $(A+I_n)(A-I_n)=0$ (True or false) True. My reasoning: Because $I_n$ is the identity matrix, there should be no issues with factoring just like normal algebra. Step 3: $A+I_n=0$ or $A-I_n=0$ I'm not sure about this part. I'm very tempted to say this is fine but I am not sure how I can justify this, if I even can. Therefore $A=-I_n$ or $A=I_n$ . (End of 'Proof'.) Is what I am doing right so far or am I messing up somewhere?","['matrices', 'proof-verification', 'linear-algebra']"
3019830,Conjugacy classes of stabilizer subgroups,"Let $G\subseteq GL(V)$ be a complex finite reflection group. I would like to understand the stabilizer subgroups of $G$ , and their normalizers. By this I mean (the conjugacy classes of) subgroups $H<G$ such that there exists $v\in V$ such that $Stab_G(v)=H$ , together with $N_G(H)$ . Ideally there would be some table somewhere, with the poset of stabilizer subgroups. I was able to compute everything (rather painfully) when $dim(V)\leq 3$ or when $G=G(n,m,p)$ , so I am only missing the exceptional groups of rank at least $4$ , that is 28-37 in this list (Shephard-Todd classification). Alternatively, since I am also trying to learn to use Magma/Sage/GAP, I was wondering if there is a way to extract the information I need from these programs.","['gap', 'group-theory', 'finite-groups', 'reflection']"
3019859,Derivative of inner product,"If the inner product of some vector $\mathbf{x}$ can be expressed as $$\langle \mathbf{x}, \mathbf{x}\rangle_G = \mathbf{x}^T G\mathbf{x}$$ where $G$ is some symmetric matrix, if I want the derivative of this inner product with respect to $\mathbf{x}$ , I should get a vector as a result since this is the derivative of a scalar function by a vector ( https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector ). Nevertheless, this formula tells me that I should get a row-vector, and not a normal vector. $$\frac{\mathrm{d}}{\mathrm{d} \mathbf{x}} (\mathbf{x}^TG\mathbf{x}) = 2\mathbf{x}^T G$$ ( http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf )
which is a row-vector. Why do I get this contradiction?","['inner-products', 'derivatives', 'linear-algebra', 'vectors']"
3019931,"Combinations and Permutations - 5 digts from 1-9 such that two are prime and two are square, no repeats","A pin code is 5 digits long, using only the numbers 1-9 with no repeats. How many combinations are there if two of the numbers must be prime and two must be square. The primes are 2, 3, 5 and 7 (so there must be two of these)
Squares are 1, 4, and 9 (two of these)
The remaining digit must be a 6 or 8 I think the right answer is 8640
What I think I've got so far is 4P2 * 3P2 * 5! = 8640
I'm not sure if that's correct. My thinking was you have 2 out of 4 primes and 2 out of 3 squares. The 5! for the different ordering of 5 digits. But I haven't done anything about the remaining digit (either 6 or 8). Is anyone able to offer a better explanation of how to work this out. Thank You Edit: I realised I haven't clarified whether it is exactly 2 or at least 2 primes (same with squares). I don't have the question with me anymore so I don't remember. My hunch is it said exactly 2 primes and exactly 2 squares.","['permutations', 'combinatorics']"
3019935,"Prove that $\Vert\cdot \Vert^2:X\to \Bbb{R},$ where $X$ is a vector space, is convex","Let $X$ be a vector space. I was able to prove that $\Vert\cdot \Vert:X\to \Bbb{R},$ is a convex function, i.e., for all $x,y\in X$ and $\lambda \in [0,1],$ \begin{align} \Vert \lambda x+(1-\lambda)y  \Vert \leq  \lambda \Vert x\Vert+(1-\lambda)\Vert y  \Vert\end{align} Now, I want to prove that $\Vert\cdot \Vert^2:X\to \Bbb{R},$ where $X$ is a vector space, is convex. So, here's what I've done! MY WORK \begin{align} \Vert \lambda x+(1-\lambda)y  \Vert^2 \leq \left( \lambda \Vert x\Vert+(1-\lambda)\Vert y  \Vert\right)^2,\;\;\text{for all}\;\; x,y\in X\;\; \text{and}\;\; \lambda \in [0,1].\end{align} So, any help please on how to proceed?","['normed-spaces', 'convex-analysis', 'functional-analysis', 'analysis']"
3019964,Using Wallis' formula to compute integral of powers of cos and sine,"I've recently discovered Wallis' formula to compute powers of cos and sine from $[0,\pi/2]$ , However what If I have a function like $\cos^m (x)\sin^n(x)$ where both $m$ and $n$ are even, this function is even, so it must be symmetric to some axis. If I want to compute the integral of this function but from $[0,k\pi]$ . Can I use parity of the function to integrate from $0$ to $\pi/2$ and then use Wallis formula? Also I've noticed that for $\cos^m (x)\sin^n(x)$ if one of $m$ or $n$ is odd then the integral on $[0,k\pi]$ is $0$ , why is that?","['integration', 'trigonometry']"
3019989,"Can we say a function is ""unbounded"" when we mean it''s tending to infinity?","I'm watching the Limits series on Khan Academy. In many videos Sal repeatedly says that although some people say that functions that tend to infinity have a limit infinity. (For example, in this video, the says that the function $ y = \frac {2}{x-1} $ (here's a link to the graph ) is unbounded as x approaches 1 from the left side, although ""some"" people would say that the function is tending towards infinity (i.e., the $ \lim_{x \to c} f(x) = \infty $ . Over a series of videos, I caught hold of that argument and always said that function don't tend towards infinity; they're just unbounded. But, in a different video ( here ), Sal essentially says: $$ \lim_{x \to 0} \frac 1{x^2} = \infty $$ $$ \lim_{x \to 0^+} \frac 1{x} = \infty $$ $$ \lim_{x \to 0^-} \frac 1{x} = -\infty $$ There is an answer to this question that says that unbounded sequences don't always tend to infinity. So it seems that we can't say that the limit of unbounded functions as x tends to some number is infinity -- what I'm confused about, and my question is: When do we ever say that the limit of some function is infinity? Specifically, are the limits of the above functions $ \frac 1x $ and $$ \frac 1{x^2} $$ correct? Or are they unbounded? If we can say that these limits are correct, why can't we say this: $$ \lim_{x \to 1^-} \frac 2{x-1} = - \infty $$","['limits', 'calculus', 'functions', 'upper-lower-bounds']"
3020004,Is there a generalization of universal algebra in which inequalities are permitted?,"In ordinary universal algebra, we consider only equational axioms like $$x + y = y + x, \qquad x + -x = 0.$$ There's a generalization of universal algebra in which quasi-equations are permitted, which are basically implications between finite conjunctions of equations. For example, the notion of a cancellative monoid can be axiomatized by quasi-equations; the left-cancellation law is $$ax = ay \rightarrow x = y$$ It seems reasonable to consider a variant on this in which inequalities are also fair game. For example, to axiomatize integral domains, we might use an appropriate cancellation law: $$a \neq 0 \wedge ax = ay \rightarrow x = y.$$ Alternatively, we could use a version of the null-factor law: $$ab = 0 \wedge a \neq 0 \rightarrow b = 0.$$ I envisage that the morphisms between models would be taken as injective homomorphisms. Such categories won't usually have products or a terminal object, though they'll often have an initial object and/or  coproducts. Here's an example of how such categories might be useful. Definition. A non-degenerate Peano structure consists of a triple $(X,S,0)$ where $X$ is a set, $S :X \rightarrow X$ is an injective function, $0 \in X$ is an element, and $\forall x \in X(S(x) \neq 0)$ is assumed to hold. The informal statement that every non-degenerate Peano structure contains a natural copy of $\mathbb{N}$ can be formally stated as "" $\mathbb{N}$ is the initial non-degenerate Peano structure."" Question. Can any interesting results be proved for category of models of ""quasi-equations with negation"", where the morphisms are taken to be injective homomorphisms?","['universal-algebra', 'abstract-algebra', 'logic', 'category-theory']"
3020010,Asymptotic expansion of heat operator $e^{-\Delta{t}}$ and $e^{-\mathcal{D}t}$ of Dirac operator,"For a closed Riemannian manifold $M$ of $n$ -dimension, we consider the Laplace-Beltrami operator $\Delta$ . It is known that we have an asymptotic expansion for the trace of heat operator $e^{-\Delta{t}}$ as follows $$
\mathrm{tr}(e^{-\Delta{t}})=\sum_{\lambda}e^{-\lambda{t}}\overset{t\downarrow0}{\sim}t^{-\frac{n}{2}}\sum_{n} \alpha_{n}t^{n},
$$ where $\lambda$ runs over the set of spectrum of Laplacian $\Delta$ . My question is that Denote by $\mathcal{D}$ the Dirac operator whose square coincides with the Laplacian, $i.e.$ $\mathcal{D}^{2}=\Delta$ . 
  Then the sum of positive eigenvalues of an operator $e^{-\mathcal{D}t}$ $$\sum_{\lambda\in\mathrm{Sp}(\Delta)}e^{-\sqrt{\lambda}{t}}$$ has an asymptotic expansion around $t=0$ ?
  If it exists, then is it possible to induce a relation between coefficients? I know that the proof for the case of heat operator follows from the construction of heat kernel. But I wonder that the same construction can be applied to the Dirac operator. Thank you for your time and effort.","['differential-operators', 'asymptotics', 'partial-differential-equations', 'spectral-theory', 'differential-geometry']"
3020012,Showing $GL_n$ is a special algebraic group,"So there's this notion of a group scheme $G$ being 'special' if any principal $G$ -bundle over a scheme $X$ (say defined in the etale topology) is also locally trivial in the Zariski topology. I would like to see why $GL_n$ is special in this sense. The few books I've seen mention this refer to other books to as their justification of this fact and the only 'proof' I've seen is in Milne's Etale Cohomology , but it uses many notions which I'm not familiar at all with. I've just started to look at stacks so I was hoping there a more accessible approach to show this?","['algebraic-stacks', 'algebraic-geometry', 'schemes', 'principal-bundles']"
3020020,How do we evaluate the degree of $x$ using sine law?,"Given that ABC is a triangle, $|EC| = |BC| = |BD| $ , $\angle CBA= 80^\circ,\angle ACB= 60^\circ, \angle EDA= x^\circ  $ Evaluate $x$ I want to solve this for $x$ using law of sines if possible. My attempt: From the property of triangle, the sum of the angles will be equal to $180$ . $$\angle BAC = 180 - 80 - 60 = 40^\circ $$ In $\triangle ABC$ , $$\frac{\sin 40}{|BC|} = \frac{\sin 80}{|AC|} \implies \frac{|AC|}{|BC|} = \frac{\sin 80}{\sin40}$$ Could you help me take it from there? Regards","['trigonometry', 'geometry']"
3020021,Where is my flaw with the calculation of this cdf?,"I want to calculate the ratio distribution $X/Y$ of two continuous random variables $X$ and $Y$ with each having support $(0, \infty)$ I was starting like that: $$\mathbb P\left(\frac{X}{Y}\leq z\right)=\int_{0}^\infty f_Y(y) F_{X\mid Y}(zy \mid y)dy$$ Now: $$F_{X\mid Y}(zy \mid y)=\int_{0}^{zy} \frac{f_{X,Y}(x,y)}{f_Y(y)}dx   =\frac{\frac{dF_{X,Y}(zy,y)}{dy}}{f_Y(y)}$$ and hence: $$\mathbb P\left(\frac{X}{Y}\leq z\right)=\int_{0}^\infty \frac{dF_{X,Y}(zy,y)}{dy}dy=\left[F(zy,y)\right]_0^\infty=1,$$ which of course is not correct. Anyone can see my mistake? Thank you very much in advance","['integration', 'probability-distributions', 'probability']"
3020027,Topological conjugacy between dyadic map and tent map,"For trying to prove that the tent map $$T(x)=
\begin{cases}
2x &\text{ if } x\in[0,\frac{1}{2}]\\
2-2x &\text{ if } x\in[\frac{1}{2},1]
\end{cases}
$$ is ergodic, I have already shown that the dyadic map (period-doubling map) given by $E(x)=2x$ mod $1$ is ergodic using a Fourier transform of $f$ and comparing coefficients to show that any measurable $f:X\to\mathbb{R}$ with $\ f \circ E = f$ almost everywhere implies $f$ is constant almost everywhere. Anyways, to show that the tent map is ergodic, I tried topological conjugation with the dyadic map (which is ergodic); I've done the following: Lemma. The tent map $T$ is topologically semi-conjugate to the dyadic map $E(x)=2x$ mod 1. Proof. Let $E: [0,1] \to [0,1]$ be the dyadic map $E(x) = 2x$ mod 1. Let $T$ be the tent map as before. Let $\varphi: [0,1] \to [0,1]$ also be the tent map, the same as $T$ ; i.e. $\varphi\equiv T$ . Since \begin{align*}
    \varphi\circ E(x)=T(2x\text{ mod 1})
    &=\begin{cases}
    2(2x\text{ mod 1})\ &\text{ if }0\leqslant2x\text{ mod } 1\leqslant\frac{1}{2}\\
    2-2(2x\text{ mod 1})\ &\text{ if }\frac{1}{2}\leqslant2x\text{ mod } 1\leqslant1
    \end{cases}\\
    &=
    \begin{cases}
    4x\ &\text{ if }x\in[0,\frac{1}{4}]\cup[\frac{1}{2},\frac{3}{4}]\\
    2-4x \ &\text{ if }x\in[\frac{1}{4},\frac{1}{2}]\cup[\frac{3}{4},1]
    \end{cases}\\
    &=T^2(x)=T\circ\varphi(x), %%Do not change, this is best way to write down, I noticed by trial and error
\end{align*} we have that $\varphi\circ E = T\circ\varphi$ ; i.e. $T$ is a factor of $E$ (or $E$ is an extension of $T$ ). $\Box$ . I know that this is only semi-conjugation for $\varphi$ is not invertible. I think the argument for ergodicty does not go wrong only using semi-conjugation, but I would like to have ""full"" conjugation. This is the ergodicity argument assuming ergodicity of $E$ : Theorem. The tent map $T$ is ergodic. Proof. Let $A$ be an invariant set in $[0,1]$ for $T$ ; i.e. $T^{-1}(A)=A$ . Since $\varphi\circ E = T\circ\varphi$ with $\varphi,\ E$ and $T$ as in the lemma above, it follows that \begin{align*}
    (\varphi\circ E)^{-1}&=(T\circ\varphi)^{-1}\\ E^{-1}\circ\varphi^{-1}&=\varphi^{-1}\circ T^{-1}
\end{align*} which, after plugging in $A$ , gives \begin{equation*}
    E^{-1}(\varphi^{-1}(A))=\varphi^{-1}(T^{-1}(A))=\varphi^{-1}(A);
\end{equation*} so $\varphi^{-1}(A)$ is invariant for $E$ . Now since $E$ is ergodic, we have that $\varphi^{-1}(A)$ has zero or full Lebesgue measure. As $\varphi=T$ (and $\varphi^{-1}=T^{-1}$ ), we have that $\varphi^{-1}(A)=T^{-1}(A)=A$ and hence also $A$ has zero or full Lebesgue measure; i.e. $T$ is ergodic. $\Box$ . Question 1A: Is the proof of the theorem correct? Question 1B: Does this last theorem proof that ergodicity is preserved under topological semi-conjugation? Question 2: How does one prove topological conjugation between the tent map and the dyadic map? Thanks in advance for time and help!","['general-topology', 'ergodic-theory', 'measure-theory']"
3020092,Proof with theorems like mean value theorem,"This must be a very elementary problem, someone may asked before, I am not much exposed to math, I don't know what keyword to search it. I'm learning from Spivak's calculus, in Chapter 11, question 28, he asked to prove: if $f'(x)\le M$ , for all $x$ in $[a,b]$ , then $f(b)-f(a)\le M(b-a)$ . The answer given in the answer book is: We have $$\frac{f(b)-f(a)}{b-a}=f'(x) \quad for\,some\,x\,in\,(a,b) $$ $$\le M,\quad\quad\quad\quad\,\,\,$$ so $f(b)-f(a)\le M(b-a)$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Since the mean value theorem states only ""for some"", then for some function I can choose $\frac{f(b)-f(a)}{b-a}>f'(x)$ , even though, I can't prove such case must exist, but would the possibility that such case exist weaker the proof?
And more I want to know is how to make the most use of theorems contain ""for some $x$ ""? What kind of stuff should I read as a math beginner?","['logic', 'derivatives', 'real-analysis']"
3020101,Is the set of measurable functions measurable?,"Let $M$ be the set of measurable functions in $\mathbb{R^R}$ . Now, $\mathbb{R^R}$ has the borelian sigma-algebra associated with its product topology, which allows us to ask the following question : Is $M$ measurable? On the positive side, I have two remarks: $M$ is sequentially closed (a pointwise limit of measurable functions is measurable) but alas it is not closed; also, in some models of ZF, $M$ is $\mathbb{R^R}$ so is obviously measurable. On the negative side, I feel like a borelian subset of $\mathbb{R^R}$ will only give constraints on the value of functions at countably many points, which is probably not enough to make them measurable. So I don't know, in case it matters, I specify that I am interested in answers in ZFC :) If it might be useful, I can explain why I was asking this question: I wanted to be sure that functions were almost always non-measurable. So I decided to measure the set of functions $\mathbb R\to \mathbb{R/Z}$ for the Haar measure ( $\mathbb{(R/Z)^R}$ is a compact group). Assuming this set is measurable, I know thanks to a Facebook commenter (Dani Spivak) that it must have measure zero, which is intuitively nice but not enough. So, if the answer is ""no"" or ""it is undecidable"", I am also interested in the weaker question: Is the set of measurable functions $\mathbb R\to\mathbb{R/Z}$ a subset of some measurable subset of $\mathbb{(R/Z)^R}$ that has measure zero?","['measure-theory', 'haar-measure', 'measurable-functions', 'probability']"
3020102,Why doesn't separate continuity imply continuity?,"Suppose $f: U \rightarrow R$ for some open subset $U$ of $R^2$ is continuous in each variable ie. $f(- , y)$ continuous for each fixed y, and $f(x , -)$ continuous for each fixed x. I know the counterexample that $f = \frac{xy}{x^2 +y^2} $ for $(x,y) \neq (0,0)$ , $f = 0$ for $(x,y) = (0,0)$ is separately continuous but not continuous at the origin. Where does the following proof that it should be continuous fail? Suppose we try to show continuity at $(x_1 , y_1)$ . Then for any $(x_2, y_2)$ in $U$ , $|f(x_1 , y_1) -f(x_2 , y_2)| \leq |f(x_1 , y_1) -f(x_2 , y_1)| + |f(x_2 , y_1) -f(x_2 , y_2)|$ by the triangle inequality. Fix $\epsilon > 0$ . Then $\exists a>0$ s.t $|x_1 - x_2| < a \implies |f(x_1 , y_1) -f(x_2 , y_1)| < \epsilon $ . Similarly $\exists b>0$ s.t $|y_1 - y_2| < b \implies |f(x_2 , y_1) -f(x_2 , y_2)| < \epsilon $ . Let $\delta = min(a,b)$ , then for $|(x_1,y_1) - (x_2,y_2)|<\delta$ , we have $|f(x_1,y_1) - f(x_2,y_2)|< 2\epsilon$ . Done. Is it because whilst it may work for that particular choice of $(x_2, y_2)$ , there may be another choice, also within distance $\delta$ of $(x_1, y_1)$ , such that $|f(x_1,y_1) - f(x_2,y_2)| > \epsilon$ ? If I add the condition that $f$ is Lipschitz in $y$ , say, with Lipschitz constant independent of $y$ , how is this sufficient for continuity?","['continuity', 'lipschitz-functions', 'analysis', 'real-analysis']"
3020165,Is there a function $f(x)$ such that $\lim_{x\rightarrow x_0}f(x)=\infty$ for all $x_0$ in some interval? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $f(x)$ be a real valued function with its domain in $\mathbb{R}$ . Is there an example of $f(x)$ such that $$\lim_{x\rightarrow x_0}f(x)=\infty$$ for all $x_0$ in some interval?","['calculus', 'examples-counterexamples', 'real-analysis']"
3020173,Find angle without trigonometry,I solved the following problem using the sine law. Desired value is $\angle MAC=10°$ . Can you find a geometric solution?,"['euclidean-geometry', 'geometric-transformation', 'geometry']"
3020192,Prove that $\prod_{i=1}^n (1+x_i/n) \sim \exp (\sum_{i=1}^n x_i/n)$ as $n\rightarrow\infty$?,"Let $x_1,x_2,\dots$ be an infinite sequence of real numbers. Assume that they are bounded, $|x_i| \le C < \infty$ for all $i$ for some $C$ . Is it true that, for any such sequence $$\lim_{n \rightarrow \infty} \prod_{i = 1}^n \left( 1 + \frac{x_i}{n} \right)
= \lim_{n \rightarrow \infty}  \exp \left(\frac{1}{n} \sum_{i = 1}^n x_i
\right)?\qquad\qquad (1)$$ So far I only have the heuristic argument $$\prod_{i = 1}^n \left[ \left( 1 + \frac{x_i}{n} \right)^n \right]^{1 / n}
\rightarrow \prod_{i = 1}^n \mathrm e^{x_i / n} = \exp \left( \frac{1}{n} \sum_{i= 1}^n x_i \right)$$ but I can't be sure that it is correct. Update: The answer to my original question, that the lims in (1) are equal, is NO. In fact the limits might not exist. However their ratio goes to 1, so the two expressions are asymptotically equivalent. I've updated the title to reflect the actual true statement that was proved in the answers here to make this easier to find.","['infinite-product', 'limits', 'convergence-divergence']"
3020214,"Evaluate $\int \frac{\ln(t+\sqrt{t^2+1)}}{1+t^2} \, dt$","$$I=\int \frac{\ln(t+\sqrt{t^2+1)}}{1+t^2} \, dt$$ i used substitution $t=\tan y$ so $$I=\int \ln(\sec y+\tan y)dy$$ Using integration by parts we get: $$I=y \ln(\sec y+\tan y)-\int y \sec ydy$$ any clue here? I also tried in other way: Knowing that $$\int  \frac{\ln(t+\sqrt{t^2+1)}}{\sqrt{1+t^2}} \, dt=\frac{1}{2}\left(\ln(t+\sqrt{t^2+1)}\right)^2$$ $$I=\int  \frac{1}{\sqrt{t^2+1}} \times \frac{\ln(t+\sqrt{t^2+1)}}{\sqrt{1+t^2}} \, dt$$ Now using integration by parts we get: $$I=\frac{1}{\sqrt{t^2+1}}\frac{1}{2}\left(\ln(t+\sqrt{t^2+1)}\right)^2+\frac{1}{2}\int \frac{tdt}{(t^2+1)^{\frac{3}{2}}}\left(\ln(t+\sqrt{t^2+1)}\right)^2$$","['indefinite-integrals', 'calculus', 'algebra-precalculus', 'logarithms']"
3020216,Differential forms and order of integration,"I don't understand how $$
  \int_{a_2}^{b_2} \int_{a_1}^{b_1} f(t_1,t_2) dt_1 dt_2 =
    \int_{a_1}^{b_1} \int_{a_2}^{b_2} f(t_1,t_2) dt_2 dt_1
$$ can agree with the fact that $dt_1 \wedge dt_2 = -dt_2 \wedge dt_1$ .
I tried to work with pullbacks, but I must be doing something wrong. I'd really like to see a very low-level step-by-step derivation of the equality above from the point of view of differential forms.","['integration', 'order-of-integration', 'multivariable-calculus', 'differential-forms']"
3020249,Identities similar to $\arctan(x)+\arctan(1/x)=\pi/2$,"The $\arctan(x)+\arctan(1/x)=\pi/2$ (for $x>0$ ) identity can be solved by taking the derivative of the left hand side, showing it is $0$ , and then plugging in, say, $x=1$ to get its constant value $\pi/2$ . Are there any other (nontrivial) identities which can be solved similarly?  I am hoping for something a 1st semester Calculus student could solve... so not too difficult please!","['calculus', 'trigonometry']"
3020305,"Is the set $\{ (x,y) \in \mathbb{R}^2 : xy=1 \}$ open or closed in $\mathbb{R}^2$","Determine whether the following sets are open or closed in $\mathbb{R}^2$ endowed with the eucledian metric $1. \{ (x,y) \in \mathbb{R}^2 : xy=1 \}$ $2. \{ (x,y) \in \mathbb{R}^2 : xy\le1 \}$ $3. \{ (x,y) \in \mathbb{R}^2 : xy<1 \}$ The first two are not open because there is no ball that is contained in those sets and that doesn't contain an element from their complements. I'm having trouble expressing their complements as some type of union of open sets so I can prove that they are closed. The third one is a bit more interesting since the answer in my book says it's an open set. It's odd to visualize how there exists an open ball that is contained in $xy<1$ but doesn't contain any other points. I would like to prove each of these a bit more rigorously and I have limited knowledge of topology/metric spaces. Hoping to prove it using only definition of open sets, (no sequences, continuity etc). Any hints or full answers are appreciated.","['general-topology', 'metric-spaces']"
3020338,Curvature vector and osculating circle radius,"I have found an incongruity into the evaluation of the osculating circle radius of the curve $\gamma(t) = R(cos(t),sin(t))$ using the formula: $$\vec r_c(t) = \vec \gamma(t) + \vec k(t)$$ Where: $\vec r_c(t)$ is the vector that identifies the osculating circle centre; $\vec \gamma(t)$ represents the point $P$ in the picture below; $\vec k(t)$ is the vector curvature. Now the problem comes:
Rewriting the formula as: $$\vec r_c(t) - \vec \gamma(t) = \vec k(t)$$ and looking the vectors' norm... $$|\vec r_c(t) - \vec \gamma(t)| = |\vec k(t)|$$ I obtain that $R = \frac{1}{R}$ and that's absurd!
Can somebody help me to find the mistake?","['multivariable-calculus', 'calculus', 'osculating-circle']"
3020347,"Necessary and sufficient conditions on a trivariate probability distribution for being the probability distribution of $(X,Y,X-Y)$","Consider a trivariate probability distribution $P: \mathbb{R}^3\rightarrow [0,1]$ . I have the following questions: (1) Are there necessary conditions on the cumulative distribution function (CDF) associated with $P$ ensuring that $$
\exists \text{ a random vector $(X_1,X_2)$ such that $(X_1, X_2, X_1-X_2)$ has probability distribution $P$}
$$ (2) Are there necessary and sufficient conditions on the CDF associated with $P$ ensuring that $$
\exists \text{ a random vector $(X_1,X_2)$ such that $(X_1, X_2, X_1-X_2)$ has probability distribution $P$}
$$ (3) The conditions that you propose can be ""approximated"" as a linear constraint on the CDF? I'm providing more details on my question also thanks to/inspired by the answers below. The answers below help, but I'm still not satisfied. Please help if you can. If there exists a random vector $(X_1,X_2)$ such that $(X_1, X_2, X_1-X_2)$ has probability distribution $P$ , then $P$ should satisfy:  for every $\begin{pmatrix}
a_1\\
b_1\\
c_1
\end{pmatrix}\leq \begin{pmatrix}
a_2\\
b_2\\
c_2
\end{pmatrix}$ If $a_2\geq b_2+c_2$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([a_1, b_2+c_2], [b_1, b_2], [c_1, c_2])\\
P([a_2, a_3], [b_1, b_2], [c_1, c_2])= 0  & \forall a_3\geq a_2\\
\end{cases}
$$ If $b_1\leq a_1-c_2$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([a_1,a_2], [a_1-c_2, b_2], [c_1, c_2])\\
P([a_1,a_2], [b_3, b_1], [c_1, c_2])=0 & \forall b_3\leq b_1\\
\end{cases}
$$ If $a_1 \leq b_1+c_1$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([b_1+c_1,a_2],[b_1,b_2],[c_1,c_2])\\
P([a_3,a_1], [b_1, b_2], [c_1, c_2])=0 & \forall a_3 \leq a_1
\end{cases}
$$ If $b_2\geq a_2-c_1$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([a_1,a_2], [b_1, a_2-c_1], [c_1, c_2])\\
P([a_1,a_2], [b_2, b_3], [c_1, c_2])=0 & \forall b_3\geq b_2
\end{cases}
$$ If $c_2 \geq a_2-b_1$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([a_1,a_2], [b_1, b_2], [c_1, a_2-b_1])\\
P([a_1,a_2], [b_1, b_2], [c_2, c_3])=0 & \forall  c_3\geq c_2
\end{cases}
$$ If $c_1\leq a_1-b_2$ $$
\begin{cases}
P([a_1,a_2], [b_1, b_2], [c_1, c_2])= P([a_1,a_2], [b_1, b_2], [a_1-b_2, c_2])\\
P([a_1,a_2], [b_1, b_2], [c_3, c_1])=0 & \forall  c_3\leq c_1
\end{cases}
$$ All the implications above can be re-written as linear function of the CDF associated with $P$ . However: are these implications also sufficient? If yes, I don't know how to prove it; If not, I don't know how to find a counterexample.","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
3020423,Switching improper integrals without Fubini,"I'm trying to understand general conditions that permit switching integrals as in $$\int_a^\infty \int_a^\infty f(x,y) dx dy = \int_a^\infty \int_a^\infty f(x,y) dy dx $$ if $f$ is not nonnegative or nonpositive and the integrals $\int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy$ are improper Riemann integrals that are not absolutely convergent . So Fubini-Tonelli theorem does not apply here. Is it sufficient that $\int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy$ are uniformly convergent for $x,y \in [0,\infty)$ ? How is it proved if true?","['multivariable-calculus', 'multiple-integral', 'improper-integrals', 'real-analysis']"
3020440,Maximum of $ab+2bc+3ca$ with $a^4+b^4+c^4=1$,"Let $a,b,c\in \mathbb R^+$ with $a^4+b^4+c^4=1$ . What is the maximal value $ab+2bc+3ca$ can take? I tried using Cauchy-Schwarz several different ways and the best upper bound I got was $\sqrt{14}$ , but it was never sharp. Numerical search suggests that the maximum occurs at about $a=0.763316$ , $b=0.697312$ , $c=0.80698$ with $ab+2bc+3ca=3.505647$ , though I couldn't find any valuable relation between these numbers and the rationals.","['lagrange-multiplier', 'maxima-minima', 'multivariable-calculus', 'optimization', 'inequality']"
3020447,Why does $\sum_{n\geq0}(1-x)^n=\frac1x$ have such a poor radius of convergence?,"I am confused as to why $$\sum_{n\geq0}(1-x)^n=\frac1x$$ only works for $x\in (0,2)$ . I get that it has a singularity at $x=0$ , so that can't work, but there are no singularities for the rest of the positive real line. Why isn't there a power series representation of $\frac1x$ which works for the whole positive real line? Are there any series representations of $1/x$ which work for $x\in (0,\infty)$ ? I can't find any. Thanks.","['power-series', 'convergence-divergence', 'taylor-expansion', 'sequences-and-series']"
3020461,Probability of guessing correct password (out of n) on k-th try when trying them at random,"I am currently reading the course notes on probability theory from Stanford CS109 . In order to practice a bit and get used to the topics presented, I am also trying to solve the problem sets, but I came across one which I find a bit ambiguous. It's problem 11a from Problem Set 1 : Say a hacker has a list of n distinct password candidates, only one of which will successfully log her into a secure system. a. If she tries passwords from the list at random, deleting those passwords that do not work,
what is the probability that her first successful login will be (exactly) on her k-th try? We suppose that k is between 1 and n (inclusive), otherwise the problem is trivial. One way of thinking about the problem would be the following: the probability of being successful on the k-th try is the product of the probabilities of choosing a wrong password on the first k - 1 steps multiplied by the probability of being successful on the k-th step, which is: $$ p = \frac{n - 1}{n} \frac{n - 2}{n - 1} \cdots \frac{n - k + 1}{n - k + 2} \frac{1}{n - k + 1}  = \frac{1}{n} $$ Another way would be to think of all the n! permutations of the n passwords and see every permutation as the order in which the hacker will try the passwords. We are interested in finding the number of permutations that have, let's say, element x (the correct password) on the k-th position. There are (n - 1)! such permutations, therefore the answer will be still $ \frac{1}{n} $ . There is one problem with this approach: Let's suppose there are 3 passwords: $p_1$ , $p_2$ and $p_3$ . There are 6 possible permutations of these 3 passwords: $$ p_1, p_2, p_3, \\
   p_1, p_3, p_2, \\
   p_2, p_1, p_3, \\
   p_2, p_3, p_1, \\
   p_3, p_1, p_2, \\
   p_3, p_2, p_1, $$ so if we want to find the probability of guessing the correct password from the first try, using the above formula we'd say it is $\frac{1}{3}$ . However, as long as the hacker found the correct password from the first try, the first 2 permutations are actually equivalent (we are not interested in the order in which the hacker was supposed to try the next passwords - as long as she found the correct one, she stops - or at least this is how I interpret the problem), so the correct probability would actually be $\frac{1}{5}$ . Because of this reason, the first approach is also incorrect. My explanation for this would be the following: if we denote by $T_i$ the i-th trial and $T_k=1$ when the hacker guesses the correct password on the k-th try, what we want to find is $$ P(T_0 = 0 \wedge T_1 = 0 \wedge \cdots \wedge  T_{k-1} = 0 \wedge T_k = 1) ,$$ (where by $\wedge$ we denote the probability of the events occuring together) which would be equal to the product of the probabilities of each event only in the case when all the events are independent (and in our case they are not, as we stop trying passwords once we guess the correct one) The final (and I suppose, correct) perspective is: the probability of guessing on the k-th try is represented by the number of ways in which the hacker can guess on the k-th try divided by the total number of ways in which the hacker can guess the password in general (which is the sum of ways in which the hacker can guess the password on the i-th try, with i from 1 to n). There are: $$ (n-1)(n-2)\cdots(n-k+1) = \prod_{i = 1}^{k - 1} (n-i), \text{if k > 1} $$ $$ 1, \text{if k = 1} $$ ways in which she can guess the correct password on the k-th try, so the result would be: $$ \frac{\prod_{i = 1}^{k - 1} (n-i)}{1 + \sum_{j=2}^{j=n} \prod_{i = 1}^{i = j - 1} (n-i) } $$ My question is: is my reasoning correct and if not, where is the mistake ? The different perspectives are the result of a debate with someone, but we couldn't agree on the correct answer (although I am pretty sure that the last one is correct, but there might be something I am missing) Thank you!","['combinatorics', 'probability']"
3020482,"Choosing a number between $1$ and $100$, and randomly guessing it. What is the expected value of the number of guesses?","I was watching Steve Balmer’s interview and he was talking about questions they’d ask from candidates. This is a question he gave, he says- I choose a number between $1$ to $100$ , the other person has to guess the number. If he gets it right the first time he gets $5$ bucks, if he misses the first time steve tells you whether the number is higher or lower( he does this every time you miss), if he gets it right the second time he gets $4$ bucks, third time $3$ , fourth $2$ and so on and if he gets it right in the seventh guess the person has to give a buck to Steve and so on, the value goes decreasing. I am trying to calculate the expected value of this game, how can I solve this, I can’t seem to come up with a way. P.S. I have edited the question with a slight variation, in the previous version steve doesn’t tell you anything after you have guessed the wrong number.","['probability-distributions', 'probability', 'random-variables']"
3020493,"Suppose $f: [0,1] \rightarrow [0,1] $ and $f(x) \leq \int_0^x \sqrt{f(t)}dt$. Show that $f(x) \leq x^2$ for all $x \in [0,1]$.","Let $f(x): [0,1] \rightarrow [0,1] $ such that $f(x) \leq \int_0^x \sqrt{f(t)}\,dt$ . Show that $f(x) \leq x^2$ for all $x \in [0,1]$ . I tried reiterating the inequality, obtaining $f(x) \leq \int_0^x1dt = x; f(x) \leq \int_0^x \sqrt{t}dt = \frac{2}{3}x^{3/2}$ etc... While it's easy to see that the exponent of $x$ tends to $2$ , it's more difficult to show that the coefficient is $1$ . Can somebody help me?","['integration', 'calculus', 'inequality', 'real-analysis']"
3020494,What's the difference between $f(x)=\sqrt{x^2+9}$ and $k(x^2+9)=\sqrt{x^2+9}$?,"Let's say we 've got a function $f(x)=\sqrt{x^2+9}$ , which is a composite function. $f(x)=\sqrt{g(x)}$ and $g(x)=x^2+9$ . When we have a function like $h(x)=x$ , we are allowed to set $x$ to $x+9$ and have $h(x+9)=x+9$ . So why do we need $g(x)$ and can't just set $x=x^2+9$ , which with a function like $k(x)=\sqrt{x}$ leads to $k(x^2+9)=\sqrt{x^2+9}$ (same as f(x) above)? Where's the difference between these two ( $f(x),k(x)$ )? Is $x^2+9$ even a valid argument for $k(x)$ ?",['functions']
3020505,Showing a function is Frechet Differentiable？,"I just started learning the Frechet Derivatives.
So I have a function $H:\mathbb{R}^{N\times n}\to\mathbb{R}^{N\times n}$ , i.e. $U^T\in\mathbb{R}^{N\times n}$ and $$H(U^T)=GW\times (F(U))^T+S\times U^T+C$$ with $G,W,S\in \mathbb{R}^{N \times N}$ are two matrices of size $N\times N$ , $F(\cdot)\in \mathbb{R}^n\to\mathbb{R}^n $ is a nonlinear function which maps each column vetor of $U$ to the corresponding column vector of $F(U)$ , and $C\in\mathbb{R}^{N\times n}$ . My question is what property should the nonlinear unknown function $F(\cdot)$ satisfy to ensure the function $H(\cdot)$ is Frechet differentiable? What does the Frechet derivative matrix looks like? What should I start?Thank you!","['banach-spaces', 'fixed-point-theorems', 'real-analysis', 'frechet-derivative', 'calculus']"
3020533,Geometrical interpretation for the sum of factorial numbers,"I am in need of a way to represent the sum $1! + 2! + 3! + 4! = 1 + 2 + 6 + 24 = 33$ in a geometrical way. What I mean by this is that for example, the sum $1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30$ can be represented geometrically as a pyramid with layers consisting of 1, 4, 9 and 16 pieces respectively a regular manner. Image from Wikipedia to illustrate the geometrical construction of the square numbers. I have tried to find such a regular pattern to construct a geometrical shape from the factorial numbers, but to no avail. How could this be done? Also, a follow up question: Is there a way to represent the factorial numbers up to an arbitrary number $n!$ , instead of ending at $4!$ as stated in this question above? (less important, but interesting nonetheless) Thanks in advance! EDIT: The probably most important part is that the 1, 2, 6 and 24 are discrete and somewhat separated from each other, kind of like the different layers in the comparison between te sum of squares (see linked image above).","['geometric-interpretation', 'factorial', 'geometry']"
3020560,What will be the pdf of $X+Y$ if $X$ and $Y$ are iid from Cauchy? [duplicate],"This question already has answers here : Proving the sum of two independent Cauchy Random Variables is Cauchy (2 answers) Closed 5 years ago . Suppose $X$ and $Y$ follow Cauchy distribution independent of each other. What will be the pdf of $X+Y$ ? What I got by using convolution theorem is that the density $g$ of $X+Y$ is $:$ $$g(x) = \int_{-\infty}^{\infty} f(y) f(x-y)\ \mathrm {dy}$$ where $f$ is the density of the Cauchy distribution given by $f(x)=\frac {1} {\pi ({1+ x^2})},x \in \Bbb R$ . Then the whole integration becomes $$\frac  {1} {\pi^2} \int_{-\infty}^{\infty} \frac {\mathrm {dy}} {(1+y^2)(1+(x-y)^2)}.$$ Now how do I solve this integral? Please help me in this regard. Thank you very much.","['random-variables', 'independence', 'probability-theory', 'probability', 'density-function']"
3020568,"Stability of Mathieu equation: $x''(t)+\cos t \,x(t)=0$","The equation $$
x''(t)+\cos t \,x(t)=0 \quad (1)
$$ can be transformed to the system: $$\vec{x}'=
\begin{pmatrix}
0 & 1\\
-\cos t & 0 
\end{pmatrix} \vec{x}=A(t) \cdot x(t)
$$ with minimum period $T=2\pi$ . Let $\mu_1,\mu_2$ be its characteristic values . A theorem gives: $$\mu_1\mu_2=\exp\Bigg\{\int_0^{2\pi} tr(A(t))dt\Bigg\}=1 \quad (2)
$$ Therefore, the Wronskian of any two linearly independent solutions satisfies: $$
W(t+2\pi)=W(t) \quad (3)
$$ Does $(3)$ imply that all solutions are bounded and thus we have asymptotic stability ? If not, in what way could we use $(2)$ to determine $(1)$ 's stability?","['stability-in-odes', 'periodic-functions', 'ordinary-differential-equations', 'dynamical-systems']"
3020587,Show that $f: \mathbb{R}: \rightarrow \ell^{\infty}$ is not Lebesgue integrable?,"Let $\{I_{n}\}_{n \in \mathbb{N}}$ be the sequence of real intervals $[0, 1/2], [1/2, 1], [0, 1/3], [1/3, 2/3]$ , and so on. Let $f_{n}: \mathbb{R} \rightarrow \mathbb{R}$ be the indicator function on $I_{n}$ . Let $f:\mathbb{R} \rightarrow \ell^{\infty}$ be $f(x) = (f_{1}(x), f_{2}(x), ...)$ . Show that $f$ is not Lebesgue measurable. I'm not sure where to start with this. I tried a contradiction proof, supposing that there exists a sequence of simple measurable functions which converge pointwise to $f$ (which would imply that $f$ is measurable). However, I'm having trouble reaching a contradiction. My intuition for this approach is that the most ""natural"" such sequence, namely $g_{n}: \mathbb{R} \rightarrow \ell^{\infty}$ where $g_{n}(x) = (f_{1}(x), f_{2}(x), ..., f_{n}(x), 0, 0, ...)$ does not work, because for any $x \in [0, 1]$ there are infinitely many intervals $I_{k}$ such that $x \in I_{k}$ , so for any $g_{n}$ , we can show $\| g_{n}(x) - f(x) \| = \frac{1}{m} + \frac{1}{m+1} + ... = \infty$ for some $m \in \mathbb{N}$ . However, it is not enough to show that the particular sequence of simple measurable functions $(g_{n})$ fails to converge pointwise to $f$ . Is there some way to fix this approach? Or should I try something else entirely?",['measure-theory']
3020596,"Solving the Diophantine equation $y^2 = 4x^3 + 1$ for $x,y \in \mathbb{Z}$","I want to solve the Diophantine equation $y^2 = 4x^3 + 1$ for $x,y \in \mathbb{Z}$ . Note that $y$ is odd, since $y$ even would give a contradiction $\mod{2}$ . Hence $\frac{y-1}{2}, \frac{y+1}{2} \in \mathbb{Z}$ . So we can rewrite the equation to: $\frac{y-1}{2} \frac{y+1}{2} = x^3$ . Claim $\frac{y-1}{2}$ and $\frac{y+1}{2}$ are coprime. Proof: Let $d = \gcd(\frac{y-1}{2},\frac{y+1}{2})$ . Let $\frac{y-1}{2} = a d$ and $\frac{y+1}{2} = b d$ for some $a,b \in \mathbb{Z}$ . Then $y + 1 = bd \,2$ and $y-1 = a d\, 2$ , so $y + 1 = ad\,2+2$ , it follows that $bd = ad + 1$ , so $d(b-a)=1$ , so $d =\pm1$ . So we conclude that indeed $\frac{y-1}{2}$ and $\frac{y+1}{2}$ are coprime. Since they are coprime we can perform a descent. That gives $\frac{y-1}{2} = e^3$ and $\frac{y+1}{2} = f^3$ for some coprime $e,f \in \mathbb{Z}$ . After adding and subtracting these two equations we find that $y = e^3+f^3$ and $1 = f^3 - e^3$ . Am I now correct to say that the only solutions are $f = 1, e=0$ and $f = 0, e=-1$ ? This gives $y=\pm1$ . In the above equation we see that in both cases $x = 0$ . We conclude by saying that the only solutions are $(x,y) = (0,\pm1)$ . Did I make any mistakes? Are there more efficient methods?","['number-theory', 'proof-verification', 'elementary-number-theory', 'diophantine-equations']"
3020627,Lattice Paths that Avoid a Point,"My house is located at $(0,0)$ and the supermarket is located at $(7,5)$ . I can only move in the upwards or in rightward directions. How many different routes are there? Obviously, ${12}\choose{7}$ $=$ ${12}\choose{5}$ $=$ $792$ . How many paths are there if I want to avoid the really busy intersection located at $(3,3)$ ? I have three ideas, all of which lead to different answers: 1) My first idea was to label out the grid and write the number at each intersection that represents how many paths cross that point. This was equivalent to writing out pascals triangle in a tilted diagonal way, until I reached $(3,3)$ whereI had to write a $0$ , but then I proceeded as you'd expect, the intersection's number was equal to the one below it added to the one to the left of it. At $(7,5)$ I ended up getting $490$ . After this, I wanted a combinatorial way to confirm my answer, so I tried: 2)counting all the paths from $(0,0)$ to $(3,3)$ and subtracting those away from $792$ . So here I would subtract ${6}\choose{3}$ $=20$ . 3)counting all the paths from $(3,3)$ to $(7,5)$ and subtracting those away from $792$ . So here I would subtract ${6}\choose{4}$ $=15$ . So my question is: which method of thinking is correct and why do the other two not lead to the correct answer (what do they over/under count)? I suspect that my first try was correct, and the answer is $490$ , but I don't see the algorithmic way of seeing it... EDIT: Made a correction to my grid. I wrote a $34$ instead of a $36$ for the intersection at $(7,2)$ . Woops!",['combinatorics']
3020654,Describing mappings using dynamics of time-dependent ODE-flows,"Let $f\colon\mathbb{R}^n\to\mathbb{R}^n$ . When is it possible to find some $g\in C^1([0,1]\times\mathbb{R}^n, \mathbb{R}^n)$ , uniformly Lipschitz continuous w.r.t the second argument, such that if $u_{x_0}$ is the solution of the IVP $$ \dot{x}(t) = g(t,x(t)),\quad x(0) = x_0,$$ we have $f = x_0\mapsto u_{x_0}(1)$ ? I.e., what do we need from $f$ for it to be describable by the (time-dependent) flow given by an ODE? How can find that ODE? I figured these might be questions answered in the analysis of dynamical systems (or not because of the time-dependency?); can somebody give a brief answer or direction on where to go looking? Many thanks in advance. Edit: The question is on mathoverflow now.","['initial-value-problems', 'dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
3020660,"Solving the homogeneous Diophantine equation $x^3 + 2y^3 = 7z^3$ for $x,y,z \in \mathbb{Q}$","I want to solve the homogeneous Diophantine equation $x^3 + 2y^3 = 7z^3$ for $x,y,z \in \mathbb{Q}$ . First note that $(x,y,z) = (0,0,0)$ is a solution. For further solutions it suffices to search for solutions in $\mathbb{Z}^3$ , because if we have a solution in $\mathbb{Q}^3$ we can always multiply by the product of the denominators to find a solution in $\mathbb{Z}^3$ . Note that third powers are special in $\mathbb{Z}/7\mathbb{Z}$ , since $3 |\phi(7)=6$ . And we have that $x^3 \in \{0,\pm1\}$ for $x \in \mathbb{Z}/7\mathbb{Z}$ . So we reduce the equation $\mod{7}$ . To find $x^3 \equiv y^3 \equiv 0 \mod{7}$ . Now let $x^3 = a\, 7$ and $y = b \, 7$ for some $a,b \in \mathbb{Z}$ . We substitute this back in the original equation to find $a\,7 + 2\, b\,7=7z^3$ , or $a+2b=z^3$ . At this point I'm stuck. I don't know whether it was a good idea to substitute the $a$ and $b$ , because we now seem to lose some information. It might be an idea to look modulo other primes, but I don't know which, as only $7$ seemed to make sense.","['infinite-descent', 'number-theory', 'elementary-number-theory', 'proof-verification', 'diophantine-equations']"
3020747,Using the Banach fixed point Theorem,"In Chapter 7, The Hille-Yosida Theorem, Functional Analysis,
  Sobolev Spaces and Partial
  Differential Equations - Brezis, 2011. Brezis showed the following claim (in Proposition 7.1). Claim : Suppose that $A$ is a maximal monotone operator, $(I+\lambda A): D(A) \to R(I+\lambda A)$ is a injective operator, and $|(I+\lambda A)^{-1}u| \leq |u| $ for all $u \in R(I+\lambda A)$ for all $\lambda >0$ . Then $R(I+\lambda A) = H$ . The proof of Brezis: We will prove that if $R(I+\lambda_{0} A) = H$ for some $\lambda_{0}>0$ then $R(I+\lambda A) = H$ for every $\lambda > \frac{1}{2}\lambda_{0}$ . For some $f \in H$ , we try to solve the equation $u+\lambda Au = f$ with $\lambda >0$ .  (1) Equation (1) may be written as $u+ \lambda_{0}Au = \frac{\lambda_{0}}{\lambda}f+ \big( 1- \frac{\lambda_{0}}{\lambda}u \big)$ or alternatively $u= (I+\lambda A)^{-1}\big[\frac{\lambda_{0}}{\lambda}f+ \big( 1- \frac{\lambda_{0}}{\lambda}u \big)\big]$ $(2)$ If $|1-\frac{\lambda_{0}}{\lambda}|< 1$ , i.e., $\lambda > \frac{1}{2}\lambda_{0}$ , we may apply the   contraction mapping principle (the Banach Fixed point Theorem) and deduce that (2) has a solution. My question:  how to prove that (2) has a solution using the the Banach Fixed point Theorem? Thanks Definitions: An unbounded linear operator $A: D(A)\subseteq H \to H$ is said to be monotone if it satisfies $\langle A u, u \rangle \geq 0$ for all $u\in D(A)$ . It is called maximal monotone if, in addition, $R(I+A)=H$ .","['functional-analysis', 'partial-differential-equations']"
3020762,Why does $z^n-1=0$ have at max n solutions? $z\in\mathbb{C}$,"I know that there is a Theorem which says that a Polynom of Degree n has at most n Solutions, however we have not proved it yet in our class. Is there Maybe another explaination for this Special case?",['complex-analysis']
3020773,Determine position of projected points onto a line?,"I have a list of points $S$ in the form of $(p, q)$ : $$\begin{align}
S = &(43, 58), (44, 60), (40, 60), (41, 61), \\
&(46, 60), (40, 57), (53, 62), (50, 61)
\end{align}$$ And I wish to center them on the origin $(0, 0)$ . I would do this by subtracting from them the midpoints ( $(\bar{p}, \bar{q})$ ) for each dimension: $$\begin{align}
\bar{p} &= \frac{p_1 + p_2 + \dots + p_n}{n} \\
\bar{q} &= \frac{q_1 + q_2 + \dots + q_n}{n}
\end{align}$$ I find $\bar{p} = 44.625$ and $\bar{q} = 59.875$ . I find my new $S$ to be: $$\begin{align}
S_{\text{new}} = &(-1.625, -1.875), (-0.625, 0.125), (-4.625, 0.125), (-3.625, 1.125), \\
&(1.375, 0.125), (-4.625, -2.875), (8.375, 2.125), (5.375, 1.125)
\end{align}$$ Using linear regression, I've found the line of best fit for this data set which crosses the origin to be $y = 0.26x + 0$ . This is the line in which I want to project points of data onto from right angles. My question is, how do I find these projected points (marked as red dots)? Taking point $(1.375, 0.125)$ , I can make a triangle with vertices at the origin, the point, and the projected point like so: I know the slope of $c$ ( $0.26$ ), the position of vertex $ba$ ( $(1.375, 0.125)$ ), and position of vertex $ca$ ( $(0, 0)$ ), but how do I find the position of vertex $cb$ ? This is for principal component analysis. To find the eigenvalue, I need the sum of squared distances from projected points to the origin. I've already found the eigenvector to be $\begin{bmatrix}0.96 \\ 0.25\end{bmatrix}$ .","['statistics', 'vectors', 'geometry', 'eigenvalues-eigenvectors']"
3020822,"Apparent existence of a semi-regular polyhedron, but that I cannot find in any table.","I propose the existence of a semi-regular polyhedron with one square, one hexagon and two triangles at each vertex. The sum of angles at reach vertex is $330°$ and therefore the external angle is $30°$ , which divides $720°$ . That would imply $\frac{720}{30} = 24$ vertices, from which can be easily calculated that there are $48$ sides and $26$ faces ( $4$ hexagons, $6$ squares and $16$ triangles). That´s fine, but I cannot see any sign of this supposed polyhedron in lists of the $13$ Archimedean solids, etc. What condition does this polyhedron violate? Why does it not exist?",['geometry']
3020837,Limit sets of a gradient field,"I am trying to solve this question on J. Sotomayor's book on ODEs. Define $X=\nabla f$ , $f$ being defined in an open subset $\Delta \subset \mathbb R^n$ . Prove that $X$ has no periodic orbits. And, if $X$ have only isolated singular points, show that is $p\in \Delta$ then the limit set of $p$ is empty or is a singular point. About the first statement: if $\gamma$ is a (non-constant) periodic orbit, then, for some $T>0$ , $\gamma(0)=\gamma(T)$ . Therefore: $$0=f(\gamma(T))-f(\gamma(0))=\int_0^T\nabla f(\gamma(t)) \cdot\gamma'(t)dt=\int_0^T\nabla f(\gamma(t))  \cdot \nabla f(\gamma(t))  dt =$$ $$=\int_0^T\vert{\nabla f(\gamma(t))\vert^2dt>0 }$$ and this is an absurd. But I am having some troubles in the second part. I have some ideas. If the orbit $\gamma_p$ passing through $p$ is not periodic then it is constant or it is injective. If $y_p$ is constant, $p$ is a singular point and $\omega(p)=p$ . The trouble is when $\gamma_p$ is one-to-one. What I've been trying to do is to prove that in this case $q \in \omega(p)$ only if $$\lim_{t \to \infty} \gamma_p(t)=q$$ and then using the fact that $$f(q)-f(\gamma(0))=\int_0^\infty \vert\nabla f(\gamma(t))\vert^2dt$$ But the integral on the right side converges only if $$\lim_{t \to \infty} \nabla f(\gamma_p(t))=\nabla f(q)=0$$ and therefore $q$ is a singular point. Is this correct? If it is, any hints of how to complete the missing step? It seems pretty intuitive to me, but I can't formalize it.","['vector-analysis', 'ordinary-differential-equations']"
3020842,Transformation for Integrals over Manifolds,"Most of modern books on integration theory, when constructing the Lebesgue integral, do not introduce manifolds prior. The transformation for Lebesgue integrals can then be stated as follows: Let $\Omega \subseteq \mathbb{R}^d$ be open and $\Phi \colon \Omega \rightarrow \Phi(\Omega)\subseteq\mathbb{R}^d$ a diffeomorphism. The function $f$ is integrable on $\Phi(\Omega)$ if and only if $x\mapsto f(\Phi(x))|\det(D\Phi(x))|$ is on $\Omega$ . It then holds that $\displaystyle \int_{\Phi(\Omega)}f(y)\,\mathrm{d}y=\int_{\Omega} f(\Phi(x))|\det(D\Phi(x))|\,\mathrm{d}x$ where $D\Phi(x)$ is the functional matrix. When considering integrals over manifolds one could proceed like this (where some details are omitted for the purpose of transparency and only global charts are considered): Let $\Psi\colon T\rightarrow V\subseteq M, T\subseteq\mathbb{R}^k$ be a global chart. The integral of $f$ over $M$ is then defined as $\displaystyle \int_M f(x)\,\mathrm{d}S(x)=\int_T f(\Psi(t))\sqrt{g(t)}\,\mathrm{d}t$ where $g$ is the gramian determinant of the chart $\Psi$ . I am looking for an analogue of the first integral transformation for manifolds. Has this anything to do with a change of charts? Also, if I remember correctly, in the theory of differentialforms this analogue is the pullback of a form, is this correct? Do you have to invoke parametrisations for a pullback, too?","['integration', 'differential-topology', 'differential-geometry']"
3020862,Does $0\prec B \prec A$ imply $A^{-1}B \prec I$,"Does $0\prec B \prec A$ imply $A^{-1}B \prec I$ ? I know: $A,B$ are of the same size. $A$ , $B$ have strictly positive, real eigenvalues. $A^{-1}B$ may not be symmetric since $A^{-1}$ and $B$ may not commute. If $A$ and $B$ are simultaneous diagonalizable, this holds. This is because they have the same orthogonal eigenvectors. By diagonalization, we can prove this. But even though $A^{-1}B$ is not symmetric, the definition for $M\prec N$ is that $$x^TMx < x^TNx, \forall x\in \mathbb{R}^n.$$ Does this condition hold?","['matrices', 'inequality', 'positive-definite']"
3020867,Regular Expression From a DFA,"I am trying to create a finite automate that would accept any strings that have at least to 0s but reject all strings that have consecutive 0s. I have designed a deterministic finite automaton (DFA) for this purpose but am having trouble generating a regex 
from it. The checked boxes are accepting states.
Thank you!","['automata', 'regular-expressions', 'discrete-mathematics']"
3020884,"Let $f$ be continuous on $[0,1]$ such that $\int_0^x f = \int_x^1 f$, what is $f$?","Let $f$ be continuous on $[0,1]$ , and suppose that for all $x$ , $0<x<1$ , $\int_0^x f = \int_x^1 f$ Can you determine $f$ ? I've argued the following: Since $f$ is continuous on $[0,1]$ , it follows that $f$ has an antiderivative $F$ on $I=[0,1]$ . Evaluating both sides of the equation we get $F(x)-F(0) = F(1)-F(x)$ = $2F(x) = F(1) + F(0)$ $F(x) = \frac{F(1) + F(0)}{2}$ Taking the derivative of both sides we get $F'(x) = f(x) = \frac{F'(1) + F'(0)}{2} = 0$ since $F(1) + F(0)$ is a constant. I'm not sure if I'm going about this problem correctly.","['calculus', 'analysis']"
3020903,Show there is an $n$-th degree polynomial $p(x)$ such that $||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty$.,"Show that for each $f \in C[0,1]$ there is an $n$ -th degree polynomial $p(x)$ on $[0,1]$ such that $||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty$ for any other $n$ -th degree polynomial $q(x)$ . This looks similar to the following If $A \subseteq (X,||\cdot||)$ is compact and non-empty then for each $x \in X$ there is some $y_0 \in A$ such that $$||x-y_0||=\inf\{||x-y||: y \in A\}$$ However, the set of $n$ -th degree polynomial is finite-dimensional (hence closed), can we prove the it is compact?","['normed-spaces', 'functional-analysis', 'real-analysis']"
3020916,Tricky real integral: $\int_0^{2 \pi} e^{\cos(2 t)} \cos(\sin(2 t)) =2\pi$,"I'm trying to prove the following: $$ \int_0^{2 \pi} e^{\cos(2 t)} \cos(\sin(2 t))  =2\pi $$ Numerical analysis agrees with this to very high accuracy, so I'm almost sure it's true. Mathematica gives this answer after thinking for a long, but gives an insane antiderivative in terms of exponential integrals. I'd like to evaluate the integral with purely real methods (I've never done complex analysis), as elegantly as possible. How can I tackle this integral?","['integration', 'calculus', 'definite-integrals']"
3020959,Point-free notation for limits?,"When dealing with functions, there is usually both a point notation and a point-free notation. For instance: Arithmetic operations on functions: $f(x) + g(x)$ vs $f+g$ Function composition: $f(g(x))$ vs $f\circ g$ Derivative: $f'$ vs $\frac{d}{dx} f(x)$ Integration: $\int_{[a,b]} f$ vs $\int_a^b f(x)\, dx$ However, with the limit, except in one place I have always seen $\lim_{x\to a} f(x)$ or $\lim_{x\to a;\, x\in E} f(x)$ instead of (say) $\lim_{a} f$ or $\lim_{a;\, E} f$ . The exception is Spivak's book Calculus , which says (after noting that the limiting variable is a dummy variable): A more logical symbol would be something like $\displaystyle \lim_a f$ , but this notation, despite its brevity, is so infuriatingly rigid that almost no one has seriously tried to use it. The book then goes on to note that point notation allows one to work with anonymous functions and also to work with multiple variables (e.g. $\lim_{x\to a} (x+t^3)$ vs $\lim_{t \to a} (x+t^3)$ ). However, both of these reasons apply to point vs point-free notation in general, not just in the case of limits. Is there some deep (e.g. psychological) reason for limits being an exception in not having a widely-used point-free notation?","['notation', 'limits']"
3020966,Double integral over a region with two bounds,"Calculate the integral of $f(x, y)$ $=$ $3x$ over the region bounded above by $y=x(2-x)$ and below by $x=y(2-y)$ . What I tried to do here was first expand the two equations out to get $y=x^2-x$ and $x=y^2-y$ . I then rearranged the second equation to get $y=1-\sqrt{1-x}$ . But when i tried to graph this region, I ran into a problem as the second equation is invalid for values of $x$ larger than $1$ . But I do feel that the first equation will be the lower bound of the double integral and the second equation will be upper bound of the integral. But because of the above problem, I can not find any intersection points so I am a little confused on how to proceed further. Any help would be highly appreciated!","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
3020988,On $\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi$,"Here's my attempt at an integral I found on this site. $$\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi$$ I'm not asking for a proof, I just want to know where I messed up Recall that, for all $x$ , $$e^x=\sum_{n\geq0}\frac{x^n}{n!}$$ And $$\cos x=\sum_{n\geq0}(-1)^n\frac{x^{2n}}{(2n)!}$$ Hence we have that $$
\begin{align}
\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=&\int_0^{2\pi}\bigg(\sum_{n\geq0}\frac{\cos^n2x}{n!}\bigg)\bigg(\sum_{m\geq0}(-1)^m\frac{\sin^{2m}2x}{(2m)!}\bigg)\mathrm{d}x\\
=&\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{2\pi}\cos(2x)^n\sin(2x)^{2m}\mathrm{d}x\\
=&\frac12\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{4\pi}\cos(t)^n\sin(t)^{2m}\mathrm{d}t\\
\end{align}
$$ The final integral is related to the incomplete beta function, defined as $$B(x;a,b)=\int_0^x u^{a-1}(1-u)^{b-1}\mathrm{d}u$$ If we define $$I(x;a,b)=\int_0^x\sin(t)^a\cos(t)^b\mathrm{d}t$$ We can make the substitution $\sin^2t=u$ , which gives $$
\begin{align}
I(x;a,b)=&\frac12\int_0^{\sin^2x}u^{a/2}(1-u)^{b/2}u^{-1/2}(1-u)^{-1/2}\mathrm{d}u\\
=&\frac12\int_0^{\sin^2x}u^{\frac{a-1}2}(1-u)^{\frac{b-1}2}\mathrm{d}u\\
=&\frac12\int_0^{\sin^2x}u^{\frac{a+1}2-1}(1-u)^{\frac{b+1}2-1}\mathrm{d}u\\
=&\frac12B\bigg(\sin^2x;\frac{a+1}2,\frac{b+1}2\bigg)\\
\end{align}
$$ Hence we have a form of our final integral: $$
\begin{align}
I(4\pi;2m,n)=&\frac12B\bigg(\sin^24\pi;\frac{2m+1}2,\frac{n+1}2\bigg)\\
=&\frac12B\bigg(0;\frac{2m+1}2,\frac{n+1}2\bigg)\\
=&\frac12\int_0^0t^{\frac{2m-1}2}(1-t)^{\frac{n-1}2}\mathrm{d}t\\
=&\,0
\end{align}
$$ Which implies that $$\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=0$$ Which is totally wrong. But as far as I can tell, I haven't broken any rules. Where's my error, and how do I fix it? Thanks.","['integration', 'special-functions', 'real-analysis']"
3020989,Intutition behind triple integrals between two surfaces,"I am attempting to learn multivariable calculus on my own. A problem in my book asks me to evaluate $$
\iiint_\mathcal{W} xz \, \mathrm{d}V
$$ where $\mathcal{W}$ is the domain bounded by the cylinder $\displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1$ and the sphere $x^2 + y^2 + z^2 = 16$ in the first octant ( $x, y, z \geq 0$ ). The answer is $\displaystyle \frac{126}{5}$ . However, I cannot obtain this number nor can I understand the logic behind evaluating such an integral. If suppose, I take the $z$ variable, I can see that it can vary from $z = 0$ to $z = \displaystyle \sqrt{16 - x^2 - y^2}$ . That reduces the integral to $$
\frac{1}{2}\int_{lowX}^{highX} \int_{lowY}^{highY} x(16 - x^2 - y^2) \,\mathrm{d}y\,\mathrm{d}x
$$ My first question is what is this quantity, physically? Next, as I look down from the $z$ axis --which I have eliminated, I see two curves on the $xy$ plane --an ellipse $\displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1$ and a circle $x^2 + y^2 = 16$ . So, if I vary $y$ from $0$ to $4$ , $x$ goes from $\displaystyle 2\sqrt{1-y^2/9}$ to $\sqrt{16-y^2}$ . Hence, my next question is what is so special about $z=0$ ? Why would I look at the curves on that plane? Why not the plane $z=1$ ? Is it because this gives me the maximal domain on $x$ and $y$ --a way of saying telling me the extent to which the variables vary? Or am I wrong to reason along these lines? P.S.:- I better be wrong, because along these lines I do not get $126/5$ . I get $3592/81$ .","['integration', 'multivariable-calculus', 'multiple-integral']"
3020999,Direct sum of closed Orthogonal Subspaces,"Given a sequence $\{\mathscr{H}_n\}_{n=1}^{\infty}$ of closed, orthogonal subspaces of a Hilbert Space $\mathscr{H}$ , we define the infinite direct sum to be: $$
\bigoplus_{n = 1}^{\infty} \mathscr{H}_n = \left \{\sum_{n = 1}^\infty x_n : x_n \in \mathscr{H}_n, \sum_{n = 1}^\infty\|x_n\|^2 < \infty\right \}
$$ The question asks me to prove that this is a closed subspace of $\mathscr{H}$ . The right hand side condition makes sense to me as for orthogonal $x_n$ we have $\|\sum x_n\|^2 = \sum \|x_n\|^2$ . For a sum of two elements in the space, we see that: \begin{align*}
\sum_{n = 1}^\infty\left |x_n+ y_n \right |^2 &\leq \sum_{n = 1}^\infty (|x_n| + |y_n|)^2 \\  &=  \sum_{n = 1}^\infty (|x_n|^2 + 2|x_ny_n| + |y_n|^2) \\ &\leq  \sum_{n = 1}^\infty (|x_n|^2 + |y_n|^2)+ 2\left(\sum_{n = 1}^\infty|x_n|^2\right)^{1/2} \left(\sum_{n = 1}^{\infty} |y_n|^2\right)^{1/2} \\
&<\infty
\end{align*} Thus, a sum of two elements is also a member of the set. the set is also clearly closed under scalar multiplication. How would I prove it is closed? Exactly why can we take sequences and show they converge in the set?","['hilbert-spaces', 'functional-analysis']"
3021011,If $p=a^2+b^2$ prove these consequences about $\big(\!\frac{a}{p}\!\big)$,"Suppose odd prime $p=a^2+b^2$ , and $a$ is odd and $b$ is even. Prove that if $b\equiv2\pmod4$ , then $\left(\dfrac bp\right)=-1$ and if $b\equiv0\pmod4$ , then $\left(\dfrac bp\right)=1$ . What I have got is that $p\equiv 1 \pmod4$ . I have tried factoring $b$ and splitting up the Legendre symbols. However, I am not sure this gets anywhere because you cannot say much about those. I have also observed that $b \equiv 0 \pmod 4$ when $p\equiv 1 \pmod 8$ and $b \equiv 2 \pmod 4$ when $p\equiv 5 \pmod 8$ if that helps.","['number-theory', 'legendre-symbol', 'prime-numbers']"
3021042,Expected Value: Use Indicator variables and random variables,"Question: Every time a customer orders a drink, the waiter serves the wrong drink with probability $\frac{1}{12}$ , independently of other orders. You order $7$ ciders, one cider at a time. Let $(D_1,D_2,...,D_7)$ be the sequence of drinks that the waiter serves. Define the following random variable $X$ : $X$ = the number of indices i such that $D_i$ is a cider and $D_{i+1}$ is not a cider. What is the expected value $E(X)$ of $X$ Answer: 0.45833333 Attempt: I start off labelling my indicator variable: $$
X = \left\{\begin{array}{rc} 1,&\text{the number of indices i such that $D_i$ is a cider and $D_{i+1}$ is not a cider}{} \\ 0,&\text{any other cases}{}\end{array}\right.
$$ I need to find $P(X=1)$ but I'm not sure how to go about it. Will I be served 4 ciders and 3 non-cider drinks according to the condition? How do I incorporate the probability of the waiter getting the drink wrong? I am struggling to proceed with these questions after defining the indicator variables. Any step by step guide would be appreciated.","['expected-value', 'probability-theory', 'discrete-mathematics', 'random-variables']"
3021094,"Linear dependence of three functions: $f(x) = \sin(x)$, $g(x) = \cos(x)$ and $h(x)=x$","Let $\mathbb{R}^\mathbb{R} $ be a vector-space of functions $f:\mathbb{R}→\mathbb{R}$ .
  The following functions in the vector space are defined as follows: $$f(x) = \sin(x),\qquad g(x) = \cos(x),\qquad h(x) = x.$$ Is the triple $(f,g,h)$ linearly dependent? I have a feeling it is, because $h(x)$ is the identity function, so $h(\sin(x)) = \sin(x)$ , and I know the definitions, I can't figure out how to put my explanation (if it is correct) properly. Thanks in advance.","['functions', 'linear-algebra', 'vector-spaces']"
3021102,Check the differentiability of the given function.,"Let $M_n(\mathbb{R})$ denote the space  of  all $n\times n $ real
  matrices  identified  with  Euclidean space $\mathbb{R^{n^2}}$ . Fixed
  a column vector $x \neq 0$ in $\mathbb{ R^n}$ . Define $f :
 M_n(\mathbb{R})  \rightarrow \mathbb{R}$ by $f(A) = \langle A^2x,x
 \rangle$ . Check whether given function is differntiable or not? When I took $
   A=
  \left[ {\begin{array}{cc}
   x_1 & x_2 \\
   x_3 & x_4\\
  \end{array} } \right]
 $ and $x=\left[ {\begin{array}{cc}
   a \\
   b\\
  \end{array} } \right]$ . I got $f(A)$ as a polynomial of four variables. I know that the polynomial function is always differentiable. How do I prove it for $n\times n$ matrix case?Without expanding the inner product How do I prove the given function is differentiable?","['multivariable-calculus', 'operator-theory', 'derivatives']"
3021119,Dual norm of $l_1$ of is $l_\infty$,"I am trying to show that $l_1$ norm's dual norm is $l_{\infty}$ norm. I have proceeded like the following: $||z||_D = \sup \{z^Tx| ||x||_1\leq 1 \}$ Then: $ z^Tx = \sum_{i=1}^n z_i x_i \leq \sum_{i=1}^n |z_i||x_i| \leq (\max_{i=1}^n |z_i|)\sum_{i=1}^n|x_i|$ Finally since $||x||_1 \leq 1$ , we have $z^Tx \leq \max_{i=1}^n |z_i|$ . With these, I am able to show that $l_{\infty}$ norm of $z$ is an upper bound of $z^Tx$ when $||x||_1\leq 1 $ . But I additionally need to show that it is the least upper bound to satifsy $sup$ , but I am stuck at this point.","['normed-spaces', 'linear-algebra', 'functional-analysis', 'real-analysis']"
3021131,Prove that $|\omega+\omega|=|\omega\cdot\omega|=|\omega^\omega|=|\omega|$,"$|\omega+\omega|=|\omega\cdot\omega|=|\omega^\omega|=|\omega|$ Does my attempt look fine or contain logical flaws/gaps? Any suggestion is greatly appreciated. Thank you for your help! My attempt: We have: $\omega+\omega=\{\omega+\alpha\mid \alpha<\omega\}$ $\omega\cdot\omega=\{\omega\cdot\alpha \mid \alpha<\omega\}$ $\omega^\omega=\{\omega^\alpha \mid \alpha<\omega\}$ Thus we build bijections $f,g,h$ as follows: $f:\omega \to \omega+\omega$ such that $f(\alpha)=\omega+\alpha$ $g:\omega \to \omega\cdot\omega$ such that $f(\alpha)=\omega\cdot\alpha$ $h:\omega \to \omega^\omega$ such that $f(\alpha)=\omega^\alpha$ This completes the proof.","['elementary-set-theory', 'ordinals']"
