question_id,title,body,tags
2468882,"If two random variables have the same variance and expectation, do they have the same distribution?","If two random variables say $X$ and $Y$ have the same variance and expectation, do they have the same distrubution?","['statistics', 'probability']"
2468903,Implication value (for x which isn't in a domain of antecedent),"We define two functions: $f(x): \dfrac{1}{x} \ge 0$ $g(x): x \ge 0$ and we want to find out for which $x$ implication  $f(x) \Rightarrow g(x)$ is true. Obviously for all $x>0$ it's true ($1\Rightarrow1$). Similary for all $x<0$ it's true ($0\Rightarrow0$). However when it comes to $0$ I've got a problem. On the one hand, we can say that $0$ doesn't belong to domain of function $f$.
But on the other hand, we can say that it belongs to domain of function $g$. Moreover $g(0)$ is true, so somehow we don't care whether $f(0)$ is true or false because if consequent is true then whole implication $f(0) \Rightarrow g(0)$ will always be true.","['logic', 'elementary-set-theory']"
2468904,Prove that $\lim\limits_{x\to 0^+}\sum\limits_{n=1}^\infty\frac{(-1)^n}{n^x}=-\frac12$,"Prove that $$\lim\limits_{x\to 0^+}\sum\limits_{n=1}^\infty\frac{(-1)^n}{n^x}=-\frac12$$ Does the limit even exist? Because when I develop the sum and consider the limit $x\to 0^+$, I get the sum $1-1+1-1+1-\dots$","['sequences-and-series', 'limits']"
2468905,Name for this kind of derivative?,"In a problem I was working on, I found it convenient to use the notation $ dX/dA_{i \rightarrow j} $ to represent the marginal change in $X$ from redistributing a marginal amount of $A_i$ to $A_j$. Is there a name for this, and can it even be called a ""derivative""? Is there a better or more conventional way to write this? Context: $\{ A_i \}$ is a finite sequence of loan payments and $X$ could be something like the associated internal rate of return. There are many valid payment sequences that satisfy a set of constraints, each having a different $X$.","['notation', 'optimization', 'calculus', 'soft-question', 'finance']"
2468939,Term by term proof for a special case of the Baker-Campbell-Hausdorff formula,"In physics, a typical situation one is confronted with is to compute the product of an exponential of operators. Let $A$ and $B$ be some (possibly unbounded) self adjoint operators on a Hilbert space $\mathcal{H}$, if the operators $A$ and $B$ satisfy
$$ [A,B] = c I$$
for some $c \in \mathbb{C}$, then it follows that
$$ e^A e^B = e^{A + B - \frac{1}{2} [A,B]} \ .$$
The usual ""proof"" in physics, is to consider a function defined by
$$ F(t) = e^{- (A + B)t}e^{A t} e^{B t}$$
then take (strong) derivatives, and the conlusion follows from some additional calculations. The problem with this approach, and all other similar approaches, is that we are either assuming $A$ and $B$ to be bounded operators or, in the context of the actual BCH formula, to be elements of some Lie algebra. The problem is that almost all practical operators for which we need this formula, the operators $A$ and $B$ are unbounded. Because of the unboundedness, there are countless issues which cannot fully be resolved without additional regularity properties added to the operators. One such regularity ""trick"" is the usage of so-called analytic vectors. If $A$ is an unbounded self adjoint operator, we say that $\psi \in \mathcal{H}$ is an analytic vector of $A$ if $\psi \in \cap_{n \in \mathbb{N}} D(A^n)$, and
$$ \sum_{n \geq 0} \frac{t^n || A^n \psi ||}{n!} < \infty \ .$$
In this case, one has
$$ e^{i A t} \psi = \sum_{n \geq 0} \frac{(itA)^n}{n!} \psi \ .$$
The exponential here is defined via the spectral theorem for normal operators.
To make a long story short, one eventually comes to the conclusion, that if $A$ and $B$ are self-adjoint operators satisfying $[A,B] = c I$, equipped with some additional conditions akin to the analytic vector condition, then we are able to conclude that
$$ e^{i A t} e^{ i Bt} \psi = \lim_{N \to \infty}  \lim_{M \to \infty} \sum_{n=0}^N \sum_{m=0}^M \frac{(i A t)^n (i B t)^m}{n! m!} \psi \ .$$
Despite searching for this computation, I have not yet found anywhere where this might have been computed term by term. So, my question is how does one approach such a computation? Trying to write out the terms term by term, the individual terms quickly become too intractable, at least for me, to deal with. Any hints, comments, or references on this computation?","['functional-analysis', 'mathematical-physics', 'unbounded-operators']"
2468968,Total Variation Distance Triangle Inequality,"I have the following definition for the total variation distance between two probability measures on a metric space $X$ . $d(\mu,\nu) = \inf\{\mathbb{P}(V\neq W) : \text{$V:\Omega\rightarrow X$ has distribution $\mu$, $W:\Omega\rightarrow X$ has distribution $\nu$}\}.$ How do I check that the triangle inequality holds for this function?","['probability-theory', 'probability', 'analysis']"
2469018,Completion set of integer numbers,"$(\mathbb{Z}, \rho)$ is a metric space with the metric: 
$\rho(m, n) = |e^{in} - e^{im}|,\ i^2 = -1,\ m, n \in \mathbb{Z}$ So I need construct completion set of integers. How can I do this? My thoughts: $f(n) = e^{in}$ monotonically increased function therefore for $\rho$ the conditions of metric are fulfilled. So if I could construct function $f: X \to Y$ (where $Y$ is set with fixed interval or semi-interval) then perhaps I could define a metric on $Y$ (let it be called $d$) and construct an isomorphism between $(X, \rho)$ and $(Y, d).$","['functional-analysis', 'general-topology', 'metric-spaces']"
2469058,$a_n$ is the smallest positive integer number such that $\sqrt{a_n+\sqrt{a_{n-1}+...+\sqrt{a_1}}}$ is positive integer,"An infinite sequence of pairwise distinct numbers $a_1, a_2, a_3, ...$ is defined thus: $a_n$ is the smallest positive integer number such that $\sqrt{a_n+\sqrt{a_{n-1}+...+\sqrt{a_1}}}$ is positive integer. Prove that the sequence $ a_1, a_2, a_3, ... $ contains all positive integers numbers. My work: Let $a_1=1$ . Then $\sqrt{a_2+1}$ is positive integer and $a_2$ is the smallest positive integer then $a_2=3$ . Then $\sqrt{a_3+2}$ is positive integer and $a_3$ is the smallest positive integer then $a_3=2$ . Then $\sqrt{a_4+\sqrt{a_3+\sqrt{a_{2}+\sqrt{a_1}}}}=\sqrt{a_4+2}$ is positive integer and $a_4$ is the smallest positive integer and $a_4\not=a_1,a_2,a_3$ then $a_4=7$ .",['sequences-and-series']
2469060,Why is the lim inf the union of intersections [duplicate],This question already has answers here : Intuitive interpretation of limsup and liminf of sequences of sets? (5 answers) Closed 6 years ago . For my statistics class we had elementary set theory. It was stated that: $$\inf_{k\geq n } A_k = \bigcap\limits_{k=n}^{\infty} A_k$$ and $$\sup_{k\geq n } A_k = \bigcup\limits_{k=n}^{\infty} A_k$$ From this was deduced that: $$\lim\limits_{n\to\infty} \inf A_k = \bigcup\limits_{n=1}^{\infty} \bigcap\limits_{k=n}^{\infty} A_k$$ and $$\lim\limits_{n\to\infty} \sup A_k = \bigcap\limits_{n=1}^{\infty} \bigcup\limits_{k=n}^{\infty} A_k$$ I absolutely have no idea why. Could someone explain it to me in the least technical way possible? I neither get why the intersection of Ak from n onwards should be the infimum nor why the union of all intersections should be the limit of that infimum.,"['self-learning', 'probability-theory', 'probability', 'elementary-set-theory']"
2469072,A more elegant proof that affine morphisms of schemes are affine local on the target,"To being with, I am using the following definition for an affine morphism of schemes. A morphism $f: X \longrightarrow Y$ is said to be affine if there exists a cover $\{ \text{Spec }A_{i}  \}$ of $Y$ by affines such that each $f^{-1}(\text{Spec }A_{i})$ is affine in $X$. It is a well known result that if $f: X \longrightarrow Y$ is an affine morphism of schemes, then for any affine open set $\text{Spec }B \subseteq Y$, the preimage is affine. I am having a little trouble understanding the proof of this. Without loss of generality, we may assume that $Y = \text{Spec }B$ is affine. Suppose $\{ g_{1}, g_{2}, \ldots , g_{m}  \}$ generates the unit ideal in $B$, and suppose that $f^{-1}(D(g_{i})) = \text{Spec }A_{i}$ is affine in $X$. The part of the proof I am stuck on is showing that $X$ must then be affine. I have something of a proof, but I have been told this is not a good way to prove it, and that I am ""missing the point"" of the exercise. The standard procedure seems to be to use the following result A scheme $X$ is affine if and only if there is a collection $\{ g_{1}, g_{2}, \ldots  , g_{m}\}$ generating the unit ideal in $\Gamma(X, \mathcal{O}_{X})$, and the open sets $X_{g_{i}}$ are affine, where $X_{g_{i}}$ is the set of all points $p \in X$ such that the germ of $g_{i}$ at $p$ is not contained in the maximal ideal of the local ring $\mathcal{O}_{X, p}$. With my situation as above, I have the morphism $f: X \longrightarrow \text{Spec }B$, which corresponds to a morphism of rings
$$
\psi: B \longrightarrow \Gamma(X, \mathcal{O}_{X}),
$$
and the $\psi(g_{i})$ generate the unit ideal in $\Gamma(X, \mathcal{O}_{X})$. My problem is that I can't see how to show that $X_{\psi(g_{i})}$ is affine in $X$. I'm expecting that the $X_{\psi(g_{i})}$ should be equal to the $\text{Spec }A_{i}$ but I can't see a nice way to show this. I was also wondering if someone knows of a reference that doesn't do this in the language of $\mathcal{O}_{\text{Spec }B}$-algebras?","['schemes', 'reference-request', 'affine-schemes', 'algebraic-geometry']"
2469098,Do carmo's exercise 3.2-11: conjugate directions on a surface,"I'm trying to find a satisfying answer to this problem, so I would appreciate some help. Let $p\in S$ be an elliptic point and let $r$ and $r'$ be conjugate directions at $p$ . Varying $r$ in $T_pS$ , show that the minimum value of the angle between $r$ and $r'$ is attained by a unique pair of vectors in $T_pS$ , which are symmetric with respect to the principal directions. My idea was to simply consider unit vectors in directions of $r$ and $r'$ respectively, say $w=\cos(\theta)e_1+\sin(\theta)e_2$ and $w'=\cos(\phi)e_1+\sin(\phi)e_2$ so that the angle between these two vectors would be given by $\displaystyle \cos(\theta)\cos(\phi)+\sin(\theta)\sin(\phi)$ . Now, remember that $\theta$ is varying and, since $r$ and $r'$ are conjugate, $\phi=\phi(\theta)$ . Taking the derivative of that last expression with respect to $\theta$ and making it equal to 0, we should be able to find the answer. I'm not very confident of this approach, though. Thank you!","['differential-geometry', 'surfaces']"
2469150,If a function belongs to $H^p$,"Let $\mathbb D$ denote the open unit disc in $\mathbb{C}$ . Let $Hol(\mathbb {D})$  denote the space of holomorphic functions on $\mathbb D$. The Hardy spaces on $\mathbb D$ are defined as follows. $$H^p=\{f\in Hol(\mathbb {D}):\sup_{r<1}\int_{0}^{2\pi} |f(re^{i\theta}|^pd\theta<\infty\}\;\;\;(0<p<\infty)$$ $$H^\infty=\{f\in Hol(\mathbb {D}):\sup_{z\in\mathbb D}|f(z)|<\infty\}$$ Now let $w\in \mathbb {D}$ and $f\in H^p$ . Consider the new function $g$ on $\mathbb {D}$ defined as $$g(z)=\frac{f(z)-f(w)}{z-w}\;\;\;(z\in \mathbb {D})$$ 
It is said the $g\in H^p.$
Can anyone tell how? Does $g\in H^\infty$ also?","['complex-analysis', 'hardy-spaces', 'holomorphic-functions']"
2469152,"Prove $\lim_{n\rightarrow \infty}\left(\sup_{\theta\in \Theta}\left| \frac{1}{n}\sum_{i=1}^n f(X_i,\theta)-E(f(X,\theta)) \right|\right)=0$","$\Theta\in\mathbb{R}^d$ is a compact set, $f(x,\theta):\mathbb{R}^p\times Y\in \mathbb{R}^+$ are continous functions in $\theta$ for every $x$ . Let $X,X_1,\dots,X_n,\dots$ be i.i.d random vectors. Then $\displaystyle E\left( \sup_{\theta\in \Theta}f(X,\theta)\right)<\infty\implies \lim_{n\rightarrow \infty}\left(\sup_{\theta\in \Theta}\left| \frac{1}{n}\sum_{i=1}^n f(X_i,\theta)-E(f(X,\theta)) \right|\right)=0$ I don't see an immediate way of doing this rather than working with the definition of limit. The idea is to show that there is $N$ such that $\sup_{\theta\in \Theta}\left| \frac{1}{n}\sum_{i=1}^n f(X_i,\theta)-E(f(X,\theta)) \right|<\epsilon$ for a given $\epsilon$ . I think the compactness of $\Theta$ might be used to argue that there is a $\theta_0$ for which $\sup_{\theta\in \Theta}f(X,\theta) = f(X,\theta_0)$ . Then I would apply the same to the second equation $\left| \frac{1}{n}\sum_{i=1}^n f(X_i,\theta_0)-E(f(X,\theta_0)) \right|<\epsilon$ . But I find the $ \frac{1}{n}$ in $\displaystyle\frac{1}{n}\sum_{i=1}^n f(X_i,\theta_0)$ problematic because it will make the term smaller meanwhile $E(f(X,\theta_0))$ doesn't seem to decrease.","['probability-theory', 'probability', 'limits']"
2469163,Proving that family of finite subsets is a ring of sets,"There is a set X given. I am to proof that the family $\mathbb{F}$ of all finite subsets of X is a ring of sets. What I know is that a ring of sets must follow three conditions: 1) $\emptyset \in R$ 2) if $A, B \in R \rightarrow A \cup B\in R$ 3) if $A, B \in R \rightarrow A \setminus B\in R$ where $R$ is the ring of sets. However I don't know how the proof should look like. I would appreciate any help.","['measure-theory', 'elementary-set-theory']"
2469215,Random Walk with state-dependent probability,"Let $\{X_n\}_{n=0,1,\ldots}$ be a DTMC with states space $S = \mathbb{Z}$ and one-step transition matrix given by: $P_{i,i-1} = \frac{1}{2i}, P_{i,i+1} = \frac{1}{2(i + 2)}, P_{i,i} = 1- P_{i,i-1} - P_{i,i+1}$ for all $i \ge 1$ and $P_{i,i+1} = \frac{1}{2|i|}, P_{i,i-1} = \frac{1}{2(|i| + 2)}, P_{i,i} = 1- P_{i,i-1} - P_{i,i+1}$ for all $i \le -1$ and $P_{0,1} = P_{0,-1} = \frac{1}{4}, P_{0,0} = \frac{1}{2}$ Is this chain positive recurrent, null recurrent or transient? My trial: It is easy to obverse that this chain is irreducible, then we just need to classify the state $0$, and the intuition is : as the chain goes far away from the origin (say it is in state $i$), the probability it will stay in this state $i$ is becoming higher and higher as $|i|$ increasing. And as $|i| \to \infty$, it  seems it becomes harder for the chain to leave the state $i$ given it starts in state $i$, so I guess this chain is positive recurrent, but how to make the proof rigorously? Thank you for your help! Edit: Thanks to @Math1000  help, I proved that this chain cannot be positive recurrent, but how can I show this chain is not transient?","['random-walk', 'markov-chains', 'probability-theory']"
2469241,"Metric $d(\sigma,\tau)$ in $S_{n}$","Let $S_{n}$ denote the set of permutations of the sequence from $(1,2,\dots,n)$.
Let us define a metric on $S_{n}$ for any $\sigma, \tau\in S_{n}$, such that $$d(\sigma,\tau)=\sum_{i=1}^{n}\vert\sigma(i)-\tau(i)\vert$$. What are the values of $d(\sigma,\tau)$ can be? Any ideas from which point should I start?","['permutations', 'combinatorics', 'metric-spaces']"
2469268,Show that $\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right)$,"How can I prove that
$$\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right)$$
without using mathematical induction? For any $n\in\mathbb{N}$.","['derivatives', 'calculus']"
2469325,Asymptotic behaviour of $f$ and $f'+f$,"Let $f~:\mathbb{R}\longrightarrow \mathbb{R}$ a $C^1$ map. How do you prove that if $f'+f$ vanish when $t\to +\infty$ so do $f$? If $\lim\limits_{t\to +\infty} f(t)=l$ with $l\neq 0$ : I first look at the case $l$ finite : we have $\lim\limits_{t\to +\infty} f'(t)=-l$ so for every $\varepsilon >0$ there exist $C>0$ such for all $t>C$, $|f(t)-l|$ and $|f'(t)+l|$ are both $\leq \varepsilon$. But I can't find a contradiction. We know that $f'$ has a limit in $+\infty$ does not implies that there exist an asymptote so I don't really know what to do. I also tried to write $g=f+f'$ and write $f$ has a solution of a differantial equation on $g$. Any help will be appriaciated.","['derivatives', 'asymptotics', 'ordinary-differential-equations']"
2469329,Calculating area of two vectors. (problems with getting calculations correct),"We have two vectors $u$ and $v$:
$$ u=-3i+5j+2k  $$
$$ v=4i+3j-3k  $$
Cross product $u\times v$ gives us:
$$u \times v =\begin{vmatrix}i & j & k \\ -3 & 5 & 2 \\ 4 & 3 & -3 \end{vmatrix} \\ =(5\times(-3)-2\times3)i \\ -(-3\times(-3)-4\times2)j \\ +(-3\times3-4\times5)k \\ u \times v =-21i-j-29k$$
these calculations on wolframalpha Now according to wikipedia area of two vectors $u$ and $v$ is $|u \times v|$ ($a$ and $b$ in this example) So i can think of at least two good ways to determine area formed by vectors $u$ and $v$. These are: $$ area_a=|u|\times|v| $$
$$ area_b=|u \times v|  $$ Now when i try to calculate area with these two methods. I get slightly different answers and i don't know why i would get two different areas ?
Calculated areas are: $$ area_a = \sqrt{(-3)^2+(5)^2+(2)^2}\sqrt{(4)^2+(3)^2+(-3)^2}=2\sqrt{323}\approx35.95$$
$$ area_b = \sqrt{(-21)^2+(-1)^2+(-29)^2}=\sqrt{1283}\approx 35.82  $$ both areas should give same result? $$ area_a=area_b$$ But in this these are different. Now if someone could point out what I am missing in these calculations or calculating wrong that would be greatly appreciated. Thanks, Tuki","['area', 'linear-algebra']"
2469330,"Find a function $f(x_1,x_2)$ such that $x_1 \in \mathbb{N}$ and $ x_2 \in \mathbb{R} $ that has a physical meaning [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question So I am searching for a simple function   $f(x_1,x_2)$ such that $x_1 \in \mathbb{N}$ and $ x_2 \in \mathbb{R} $, however $f$ should have a physical meaning. I was thinking with functions like area, volume, however all these functions takes inputs as positive real numbers. No fucntion taking one varaible as anatural number has come to my mind. Would some one help me if he/she has some ideas. Thank you in advance.","['examples-counterexamples', 'functions']"
2469346,On $\int_1^\infty \left(\sum_{1\leq k\leq t}\frac{\mu(k)}{k}\right)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt$,"I've deduced the following, but I don't know if it is right Claim. One has that 
  $$\frac{1}{\zeta(3)}=\frac{6}{\pi^2}-\frac{i}{\pi}\int_1^\infty m(t)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt,\tag{1}$$ where for real numbers $x\geq 1$ one has $m(x)=\sum_{1\leq k\leq x}\frac{\mu(k)}{k}$, being $\mu(n)$ the Möbius function. Notice that I am not saying that it is interesting because from the problem to compute $(\zeta(3))^{-1}$ I am trying to study a more difficult thing, that is the integral in RHS of $(1)$. Since my proof had steps that maybe are wrongs, and since I am not able to compute an approximation of the integral using a CAS I am asking Question. Q1.) Was right the formula $(1)$? Add the reasoning of why no, or well if the Claim is true and you want add calculations/reasonings as companion of mine. Q2.) Is it possible to get an approximation of such integral in RHS of $(1)$ (I presume thus that is convergent; and notice that is a complex number) without using my identity itself? Thanks in advance. Skecht of my proof, as I said I don't provide the justifications: Step 1. From the specialization $x=1/k$ in the Fourier series for $x(1-x)$ I wrote $$ \frac{1}{k} \left( 1-\frac{1}{k} \right)  =\frac{1}{6} -\frac{1}{\pi^2}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right),\tag{2}$$ multiplying by $\frac{\mu(k)}{k}$, taking the sum $\sum_{k=1}^\infty$, and invoking the prime number theorem and particular values of Dirichlet series, one has
$$ \frac{1}{\zeta(3)} =\frac{6}{\pi^2}+ \frac{1}{\pi^2}\sum_{k=1}^\infty\frac{\mu(k)}{k}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right).\tag{3}$$ Step 2. From $(3)$ and this deduction of  Abel's identity (I've used the prime number theorem) $$\lim_{x\to\infty}\sum_{k\leq x}\frac{\mu(k)}{k}\cos \left( \frac{2\pi n}{k} \right)=0-2\pi n\lim_{x\to\infty}\int_1^x \left(\sum_{k\leq t}\frac{\mu(k)}{k}\right)\sin \left( \frac{2\pi n}{t} \right)\frac{dt}{t^2},\tag{4}$$ one has the Claim using the closed-form for $$\sum_{n=1}^\infty\frac{1}{n}\sin\left( \frac{2\pi n}{t} \right)$$ that provide us Wolfram Alpha online calculator with this code sum 1/n sin(2 pi n/t), from n=1 to infinity I presume that this last calculation is using geometric series.$\square$ As is implicit I am not sure if my reasonings, justifications were 100% rights.","['analytic-number-theory', 'real-analysis', 'proof-verification', 'mobius-function', 'convergence-divergence']"
2469369,Showing that $\lvert H K\rvert = \frac{| H\| K|}{ |H\cap K|}$ [duplicate],"This question already has answers here : Order of a product of subgroups. Prove that $o(HK) = \frac{o(H)o(K)}{o(H \cap K)}$. (6 answers) Closed 6 years ago . I have seen a proof that for two subgroups $H$ and $K$ of a finite group $G$,
$$
|H K| = \frac{| H\| K\rvert}{| H\cap K|}
$$
The proof counts the number of elements $hk$ that are the same as $h'k'$ for $h,h'\in H$ and $k,k'\in K$. I got confused with this proof. My question is if there is another way to make things like this more clear. For example, I know that if one has a surjective homomorphism $f:G \to H$ then $\lvert G\rvert / \lvert \ker f\rvert = \lvert H\rvert$. That is, one can count the number of elements in something by essentially finding the order of the kernel of a map. This looks like what is going on with the product $HK$. But I guess that $HK$ in general isn't a group, so one can't talk about a group homomorphism. Is there a way to find the ""kernel"" of a general function and get something like the first isomorphism theorem? All I would be interested in is how this can be used to count things.","['finite-groups', 'abstract-algebra', 'combinatorics', 'group-theory']"
2469435,a question on prime numbers and infinite series,"prove that the infinite sum - $∑(1/2)^p$, where $p$ runs over all the $prime$ numbers,is $irrational$ one idea that may work is to use the given lemma $Lemma:$ $α$ is an irrational number iff there exists two convergent integer sequences ${a_n}$ and $b_n$ such that $(a_n-αb_n)≠0$ for all $n$. but $lim(a_n-αb_n)=0$ The proof is just by contradiction by assuming $α$ to be rational.I have tried to work out this lemma but failed .Plz try this.","['prime-numbers', 'sequences-and-series', 'irrational-numbers', 'elementary-number-theory']"
2469448,At what steps is Replacement required in definitions by Transfinite Recursion?,"I am trying to pin-point where Replacement is required in a specific definition by Transfinite Recurson. In the general proof of the Transfinite Recursion Theorem, it is noted that Replacement is used to guarantee that $F \restriction \alpha$ (i.e. the range of the function being defined) is always a set (because its domain $\alpha$ is a set), to prove that $dom (F)$ is all of ON (by reductio) But suppose our specific definition is something like: $P(0) = 0$ $P(\alpha+1) = P(\alpha) \cup \wp (P(\alpha))$ $P(\lambda) = \bigcup_{\alpha < \lambda} P(\alpha)$ I don't see where Replacement is necessary since it seems that the existence of 
$P(\alpha+1)$ follows by Union and Powerset, and the existence of $P(\lambda)$ follows from that by Union. Suppose, moreover, that we just want to define this function up to some set $\lambda$ (as opposed to a proper class). Then is Replacement being used at all?","['transfinite-recursion', 'elementary-set-theory']"
2469489,"Determine all $m,n \in \mathbb{N}$ such that $(5^m-1,5^n+1)=2$","Determine all $m,n \in \mathbb{N}$ such that $$\gcd(5^m-1,5^n+1)=2$$ First attempt: Say $d= (5^m-1,5^n+1)$ , then $d|5^m-1$ and $d|5^n+1$ so $d|5^m+5^n$ . If $m=n$ then $d|2$ so $d=2$ since $5^m-1$ and $5^n+1$ are both odd. If $m>n$ then $d|5^n(5^{m-n}+1)$ . Since $5\not|d$ we have by Euclid lemma $d|5^{m-n}+1$ and now what? Second : We notice if $m=2n$ then $$5^m-1 = (5^n-1)(5^n+1)$$ so $(5^m-1,5^n+1)=5^n+1$ .
The same is true if $m= kn$ , $k$ even. So perhaps it is reasonable to believe that if $m\ne kn$ , $k$ even then $d=2$ . But I'm not sure.","['number-theory', 'elementary-number-theory']"
2469515,"Prove that a natural number that consists of $300$ ones, some zeros, and no other digits cannot be a perfect square.","Prove that a natural number that consists of $300$ ones, some zeros, and no other digits cannot be a perfect square. I figure I have to evaluate this with mods of some type and suppose the ending of the number could end in $00$ $11$ or $10$,$01$. Then possibly arrive at some contradiction that this is not possible but I'm unsure how to best approach this type of problem.",['number-theory']
2469530,Generating a list of numbers whose combinations are mathematically unique,"I'm trying to design a puzzle for a game I'm making, and the puzzle is fairly similar to the one from the power plant in Myst . The idea is that I have 9 buttons, and each button supplies an amount of power to the ""system"". The solution to the puzzle is to press three specific buttons (order doesn't matter) to reach a specific power threshold. Obviously, I have to be careful about how I choose how much power each button provides since if a number can be reached by more than a single combination, that can destroy the complexity of the puzzle somewhat. My first naive idea was to use the first 9 prime numbers, but a quick check through a program showed that that wasn't going to fly. It had occurred to me that I could just choose a number that only occurs once in the list of results, but I would prefer it if I could choose which 3 buttons to press at random. How do I go about generating this list of 9 numbers such that adding any 3 of the numbers together results in a unique value? Is there some statistics/probability thing that I can use or do I just have to trial-and-error it?",['statistics']
2469531,Proof that my process is a Brownian Motion,"I would like to create a stochastic process connected with Normal Inverse Gaussian distribution. My process starts from price P_0 (I know that in the Brownian motion there is a first point from definition that it should start from 0, but I think it is just a notation. This process is generated by following formula: $Y_t = Y_{t-1} + Y_{t-1}e_t$ Where:
$e_t \sim NIG(\alpha, \beta, \gamma, \delta)$ How could I check if this process is a Brownian motion? Or it is not.","['stochastic-processes', 'probability-theory', 'probability', 'brownian-motion']"
2469555,Operator norm in terms of matrix norm,"Given a matrix $A$ yielding a linear operator $L$, how can I express the operator norm in terms of $A$'s norm? As far as I've understood, the operator norm in terms of a matrix is given by $$
\|L\|_{op} = \sup_{x\neq0}\frac{\|Ax\|_{\infty}}{\|x\|_\infty}
$$ This should mean that
$$
\sup_{x\neq0} \frac{\|Ax\|_{\infty}}{\|x\|_\infty}
= \sup_{x\neq0} \left\|\frac{Ax_{\infty}}{\|x\|_\infty}\right\|_\infty
= \sup_{x\neq0} \left\|A\left(\frac{x_{\infty}}{\|x\|_\infty}\right)\right\|_\infty
$$
However, is there a way to simplify this further, removing any reference to $x$? As I see it, I should be able to reduce $\frac{x_{\infty}}{\|x\|_\infty} \to 1$ somehow. But I don't see quite how?","['matrices', 'normed-spaces', 'supremum-and-infimum']"
2469585,Differentiability of tempered distributions over $\mathbb{R}^3$,"One of the topics I find interesting in physics is the time evolution of improper quantum states (physicists call them kets). My question to this mathematical community is: In what exact sense is a mapping $\phi:\mathbb{R}\to \mathcal{S}'(\mathbb{R}^3)$ , $t\mapsto\phi(t)$ continuous and furthermore differentiable in the variable/parameter "" $t$ ""? ( $\mathcal{S}'(\mathbb{R}^3)$ is of course the topological dual of the Schwartz test function space)","['functional-analysis', 'distribution-theory']"
2469645,Trace of the $\sigma$-algebra generated by the predictable rectangles,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $T>0$ $(\mathcal F_t)_{t\in[0,\:T]}$ be a filtration of $\mathcal A$ Let $$\mathcal R_a:=\bigcup_{F\in\mathcal F_a}F\times\left\{a\right\}\cup\bigcup_{a\le s<t\le T}\bigcup_{F\in\mathcal F_s}F\times(s,t]$$ and $$\mathcal P_a:=\sigma(\mathcal R_a)$$ for $a\in[0,T]$. Are we able to show that $$\left.\mathcal P_a\right|_{\Omega\times[b,\:T]}:=\left\{A\cap(\Omega\times[b,T]:A\in\mathcal P_a\right\}=\mathcal P_b\tag1$$ for all $0\le a\le b\le T$? It's easy to see that $$\left.\mathcal R_a\right|_{\Omega\times[b,\:T]}\subseteq\mathcal P_b\tag2$$ (where the system on the left-hand side is defined in the obvious way). Now, I've hoped that I could show $\mathcal R_b\subseteq\sigma\left(\left.\mathcal R_a\right|_{\Omega\times[b,\:T]}\right)$. For this purpose, let $B\in\mathcal R_b$. If $$B=F\times(s,t]$$ for some $b\le s<t\le T$ and $F\in\mathcal F_s$, then (since $a\le b$) $$B\in\mathcal R_a$$ and hence $$B=B\cap(\Omega\times[b,T])\in\left.\mathcal R_a\right|_{\Omega\times[b,\:T]}\;.$$ Otherwise, $$B=F\times\left\{b\right\}$$ for some $F\in\mathcal F_b$ and I think that we've encountered a problem here: $B$ doesn't belong to $\mathcal R_a$. We've only got $$F\times\left\{t\right\}=F\times\bigcap_{n\in\mathbb N}\left(t-\frac1n,t\right]\in\mathcal P_b\;\;\;\text{for all }t\in(b,T]\;.\tag3$$ Can we solve that problem or is $(1)$ wrong?","['probability-theory', 'measure-theory', 'filtrations']"
2469656,Finding the exponential of a matrix,"I am trying to find $\exp\left(\frac{i{\mu}t\mathbf{H}}{\sqrt{2}}\right)$ where $$\mathbf{H}=\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ I've got as far as figuring out that that, for $n=1,2,3,\dots$ $$ \mathbf{H}^{2n}=2^{n-1}\mathbf{H}^2=2^{n-1} \begin{pmatrix}1 & 0 & 1 \\ 0 & 2 & 0 \\ 1 & 0 & 1\end{pmatrix} $$ $$\mathbf{H}^{2n-1}=2^{n-1}\mathbf{H}=2^{n-1}\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ but I'm not getting anywhere substituting this into the power series and I've no idea how to proceed. I've seen mention of Jordan form in other questions about this but I've never heard of it. If I need to learn it so-be-it, but I think there must be some other way as we have never been shown it in class.","['matrices', 'matrix-exponential', 'linear-algebra']"
2469726,Hankel matrix of Catalan numbers,"Recall that the $n$-th Catalan number $C_n=\frac{1}{n+1}{2n\choose n}$ counts the number of
paths connecting $(0, 0)$ to $(n, n)$ that travel along the grid of integer lattice points of
$R^2$ where each path moves up or right in one-unit steps and no path extends above
the line $y = x$. In linear algebra, a Hankel matrix of Catalan numbers is defined as following:
$$H_n^t=(C_{i+j+t})_{0\leq i,j\leq n-1}=
\begin{bmatrix}
    c_{t} & c_{t+1} & c_{t+2} & \dots  & c_{t+n-1} \\
    c_{t+1} & c_{t+2} & c_{t+3} & \dots  & c_{t+n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    c_{t+n-1} & c_{t+n} & c_{t+n+1} & \dots  & c_{t+2n-2}
\end{bmatrix}
$$ How can I calculate the Hankel determinant of Catalan numbers for $t=1$? Is it possible obtain the Hankel determinant of Catalan numbers for $t>1$?","['hankel-matrices', 'matrices', 'determinant', 'combinatorics', 'catalan-numbers']"
2469738,Proving the FKG Inequality : $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ using the Chebychev Sum Inequality,"$f$ and $g$ are nondecreasing functions, then for any random variable $X$, show that: $$\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$$ I know this question has been addressed here before: Show $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ for $f,g$ bounded, nondecreasing Inequality for Expected Value of Product However, what I am interested in attempting to do is prove it using the Chebychev Sum Inequality which states: Given $a_1 \geq a_2 \geq \cdots \geq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n$ then $$ {1\over n} \sum_{k=1}^n a_k \cdot b_k \geq \left({1\over n}\sum_{k=1}^n a_k\right)\left({1\over n}\sum_{k=1}^n b_k\right).$$
Similarly, if $a_1 \leq a_2 \leq \cdots \leq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n,$, then the opposite inequality holds. Logically, I feel such an argument should be possible since $f$ and $g$ being monotone non-decreasing are known to be measurable and $X$ being a random variable i.e. a measurable function, their respective compositions with it should also be measurable. If we take $F := f \circ X$ and $G:= g \circ X$ then the problem is asking us to show : $$\int FG d\mu \geq \int Fd\mu \int Gd\mu $$ It is also known that the product of two simple functions is also a simple functions. So, we can represent each of these integrals as a limits of integrals of simple functions (increasing limits of simple functions if we want to use MCT). What I want to do is prove the desired inequality using Chebychev for the integrals of each of the simple functions and use a limiting argument to extend the result generally. I am familiar with the other proofs (Coupling, Fubini-Tonelli, Covariance operator as guiding intuition etc), admittedly simpler, but this is something that I have been trying to do for a while -- knowing that it should technically be possible -- but haven't been able to.","['real-analysis', 'probability-theory', 'probability', 'measure-theory', 'analysis']"
2469746,"$n \cdot n$ board, each square holds the number of rectangles that contain it","So the problem goes: we have a $n\cdot n$ board, where there is a number on each square, representing the number of rectangles of the board, which contain that square. What is the sum of all such numbers? I'm really just a beginner at discrete mathemathics, and we didn't finish this problem at uni, and it's been bothering me since I'm unsure what to do here. An idea was to put the board in a coordinate system and view each square as $(i,j)$ which are $\in$ $P_{a,b,c,d}$ (rectangles). And from there proceeding onto some strange summations, but that proved useless.
An approach using matrices also didn't get too far. I'm really lost here so any hints whatsoever would be really useful, I wish I had more ideas to offer! Thank you in advance!","['combinatorics', 'discrete-mathematics']"
2469798,Supremum and infimum of a rational set bounded by irrational numbers,Let $S = \left\{x \in \mathbb{Q} \mid 1 \leqslant {x}^2 \leqslant 29 \right\}$ What can we say about the supremum and infimum of this set? Would it be non-existent? Would it be correct to say the following? Suppose $ \sup  S < \sqrt{29} $ then $ \exists x \in S $ such that $ x > \sup S$ Therfore $ \sqrt{29} \leqslant \sup S$ But on the other hand $\sup S > \sqrt{29}$ then $\exists x \in \mathbb{Q}$ such that $ x < \sup S$ Therefore $\sup S \leqslant \sqrt{29}$ Therefore $\sup S $ can only be equal to $\sqrt{29}$ However $\sqrt{29} \notin \mathbb{Q}$ therefore $\sqrt{29} \notin S$ Therefore there can not be a supremum to this set. (I am assuming the supremum of a set must belong to the set. Is this correct?),"['supremum-and-infimum', 'discrete-mathematics']"
2469802,"Find a first order recurrence relation for $0, 1, 4, 12, 32,\ldots$","Questions: (a) What's $a_5$ ? (b) Find a formula for $a_n$ in terms of $a_{n - 1}$ (c) Find a formula for $a_n$ in terms of $n$ and use (b) to prove your formula The $n$ th term of the given sequence is the number of line segments in an $n$ -cube. Playing around with the given sequence we can discover the following: $a_0 = 2^{-1} \times 0 = 0 \\ a_1 = 2^{1 - 1} \times 1 = 1 \\ a_2 = 2^{2 - 1} \times 2 = 4 \\ a_3 = 2^{3 - 1} \times 3 = 12 \\ a_4 = 2^{4 - 1} \times 4 = 32 \\ a_5 = 2^{5 - 1} \times 5 = 80 \\ \ldots \\ a_n = 2^{n - 1} \times n$ The formula above solves (a) and (c) partially. Although, I think we could just use induction to prove it. I am having hard time finding the first order recurrence relation for $a_n.$ How do we find it? Also, how do we go about finding recurrence relations in general?","['algebra-precalculus', 'recurrence-relations', 'discrete-mathematics']"
2469817,"What is the set $g^{-1}([-1,1])$?","Let $g(x,y)=xy$ for $xy\in \mathbb R$. What is $g^{-1}([-1,1])$? I know that $g^{-1}([-1,1])=\{{(x,y) \,:\    g(x,y)\in [-1,1]}\}$. But finding all values $(x,y)$ is somewhat confusing.","['calculus', 'functions']"
2469822,Is the integration of Lipschitz function still Lipschitz?,"Let $f:(0,1)\to\mathbb R$ 1) Is the integration of a Lipschitz function still Lipschitz? 2) Is the integration of a bounded function still Lipschitz? 3) (Claim) If $f$ is Lipschitz, then the intersection between $f$ and x-axis is finite, almost surely. (In other word, if $f$ intersect with x-axis at countable points, then the number of those points is finite) For Q1, I have no idea. For Q2, a counterexample is the integration of $f(x)=sin(1/x)$? For Q3, I think it is true. The weak derivative of L-Lipschitz $f$ is bounded. Loosely speaking, I could denote the set of the y-coordinates of all $f$'s local minima above x-axis as $A$, and the y-coordinates of all local maxima below x-axis as $B$. All elements of $A$ and $B$ are real numbers. Find the min among $A$; denote it as $a$. Find the max among $B$, denote it as $b$. For example, if $(x_1,y_1),...,(x_m,y_m)$ are the local min and all $y>0$, then we denote $A=\{y_1,...,y_m\}$ and $a=y_\text{min}$. We have: $$L/(a-b)\geq \frac{|A|+|B|}2$$ QED? (Not homework)","['continuity', 'real-analysis', 'lipschitz-functions', 'functions']"
2469841,"Show that $\sum_{m,n = - \infty}^{\infty} \frac{(-1)^m}{m^2 + 58 n^2} = - \frac{\pi \ln( 27 + 5 \sqrt {29})}{\sqrt {58}} $","I wonder why this is true $$ \sum_{m,n = - \infty}^{\infty} \frac{(-1)^m}{m^2 + 58 n^2} = - \frac{\pi \ln( 27 + 5 \sqrt {29})}{\sqrt {58}} $$ Where the sum omits the case $n = m = 0$ ofcourse.","['zeta-functions', 'algebraic-number-theory', 'sequences-and-series']"
2469879,Number of paths through 3D lattice,"I'm looking to find the number of paths through the 3-D lattice from the origin $(0,0,0)$ to the point $(7,3,5)$. Each step made can only move forwards 1 unit in the x,y or z direction. I think the answer is (number of paths from $(0,0,0)$ to $(7,3,4)$) + (number of paths from $(0,0,0)$ to $(7,2,5)$) + (number of paths from $(0,0,0)$ to $(6,3,5)$) but I don't know how to calculate the three individual components of the sum. Am I supposed to use the multinomial formula $\binom{k+m+n}{k,m,n}=\frac{(k+m+n)!}{k!m!n!}\;$ for each one?","['combinatorics', 'discrete-mathematics']"
2469941,Pointwise recurrent functions on the Cantor set,"Let $X$ be a compact metric space and $f:X \to X$ be a continuous function. Given $x\in X$, $\mathrm{orb}(x,f)=\{x,f(x),f^2(x),...\}$. We say that $f$ is pointwise recurrent provided that, for each $x\in X$, there is an increasing sequence of natural numbers $\{n_i\}$ such that $x=\lim_{i\to\infty}f^{n_i}(x)$. Also, we say that $f$ is minimal if for any $x\in X$ we have that $\mathrm{Cl}(\mathrm{orb}(x,f))=X$. My question: Is there a sequence of pointwise recurrent (minimal) functions defined on the Cantor set that converge to the identity map? Thank you.","['dynamical-systems', 'general-topology', 'metric-spaces', 'convergence-divergence', 'cantor-set']"
2469953,"Optimal strategy for rolling die consecutively without getting a ""1""","Consider rolling a 6-sided die continuously and trying to tally as high a score as possible (the sum of all rolls).  If you roll a 1, then your turn is over and your score is 0.  So for each successful roll, the expectation value should be 4.  According to Knizia (1999), an approximately optimal strategy would be to attempt to roll to a score of 20 and then stop.  (S)he states: ""...we know the true odds of such a bet are 1 to 5.  If you ask yourself how much you should risk, you need to know how much there is to gain.....If you put 20 points at stake, this brings the odds to 4 to 20, that is 1 to 5, and makes a fair game....Whenever your accumulated points are less than 20, you should continue throwing, because the odds are in your favor. "" Don't understand this, is not rolling to 20 on average essentially rolling five times (without getting a 1)?  The probability of this happening is only about 40%.  So wouldn't the odds be about 60% that you would roll a 1 and score 0 if you always try to roll up to 20?",['probability']
2469961,IVP and Discontinuity,"I am continuing my effort to learn analysis from Thomson Bruckner and Bruckner's book. I ran into an interesting problem relating to Intermediate Value Property (IVP). The problem goes as follows: Suppse $f$ has IVP on $(a,b)$ and discontinuous at $x_0 \in (a,b)$. Prove that there exists a $y \in \mathbb{R}$ such that $\{x : f(x) = y\}$ is infinite. My Proof is as follows: Since $f$ is not continuous at $x_o$, we take a sequence of $x_i \rightarrow x_0$. We have for all $\forall \delta > 0 \ \exists n > N$  $|x_n - x_0| < \delta$. Let us consider a monotone subsequence of $x_i$. Now let  $k > N, \ x_k < x_{k+1} < \cdots < x_{k+m} \cdots < x_0$ be one such sequence. Let us say $\nexists y_i \in (x_k, x_{k+1}), y_{i+1} \in (x_{k+1}, x_{k+2}) \cdots$ such that $f(y_i) = f(y_{i+1}) = f(y_{i+2}) \cdots $ and $f(x_{k+i}) != f(x_{k+j}), \ i \ne j$. We have, $f$ by IVP, takes on all the values between $(f(x_k), f(x_0))$ since by our assumption $\nexists y_i \in (x_{k}, x_{k+1}) \cdots $ such that $f(y_i) = f(y_{i+1}) = \cdots$ i.e. $(f(x_{k+i}), f(x_{k+i+1}))$ are all mutually disjoint, our choice of $x_k$ ensures that $f(x_k)$ has a montonic sub-sequence and $\forall \epsilon > 0 \ \exists m > M, \ |f(x_m) - f(x_0)| < \epsilon$. A contradiction. Now to my QUESTION, is my proof right? It seems right to me. Thanks in advance to your answer.","['functional-analysis', 'real-analysis', 'analysis']"
2470023,Understanding proof of convergence,"I'm going through section 6.1 in this paper for the proof of Theorem 2.1. However, I can't seem to get the result, which I will explain below. Setup Let $T_i$, $1\leq i\leq m$ be independent Student's $t$ test statistics which are constructed as 
$$
T_i=\frac{\bar{X}_i}{\hat{s}_{ni}/\sqrt{n}}
$$
where 
$$
\bar{X}_i=\frac{1}{n}\sum_{k=1}^n X_{ki},\hspace{5mm}\hat{s}^2_{ni}=\frac{1}{n-1}\sum_{k=1}^n (X_{ki}-\bar{X}_i)^2
$$
and $X_{ki}\stackrel{iid}{\sim} \mathcal{N}(\mu_i,\sigma_i^2)$, $1\leq k\leq n$, $1\leq i\leq m$. Problem Assumptions Suppose $\log m=o(n^{1/2})$. Assume that $\max_{1\leq i\leq m}EY_i^4\leq b_0$ for some constant $b_0>0$ and
  \begin{equation}
\text{Card}\left\{i: |\mu_i/\sigma_i|\geq 4\sqrt{\log m/n} 
\right\}\to\infty
\end{equation} Further suppose that for $0\leq t\leq o(n^{1/4})$, 
  \begin{equation}
P(|T_i-\sqrt{n}\mu_i/\hat{s}_{ni}|\geq t)=\frac{1}{2}G(t)\left[\exp\left( -\frac{t^3}{3\sqrt{n}}\kappa_i\right)+\exp\left(\frac{t^3}{3\sqrt{n}}\kappa_i \right) \right](1+o(1))
\end{equation}
  where $o(1)$ is uniform in $1\leq i\leq m$, $G(t)=2-2\Phi(t)$, $\Phi(t)$ is the normal cdf, and $\kappa_i=EY_i^3$. Finally, let $\mathcal{M}\subset\{1,2,\dots,m\}$ satisfying $\mathcal{M}\subset\{i:|\mu_i/\sigma_i|\geq 4\sqrt{\log m/n}\}$ and $\text{Card}(\mathcal{M})\leq\sqrt{n}$. Also for any $\epsilon>0$,
  $$
P(\max_{i\in\mathcal{M}}|\hat{s}^2_{ni}/\sigma_i^2-1|\geq \epsilon)=O(1/\sqrt{n})
$$ Question I want to show equation (15) in section 6.1 which says that for some $c>\sqrt{2}$ and some $b_m\to \infty$ (the subscript means that the constant depends on $m$), 
  $$
P\left(\sum_{i=1}^m I\{|T_i|\geq c\sqrt{\log m} \}\geq b_{m} \right)\to 1
$$ I believe the idea is to split up the quantity for $i\in\mathcal{M}$ and $i\not\in\mathcal{M}$ and use the fact that 
$$
P\left(\sum_{i\in\mathcal{M}} I\{|T_i|\geq c\sqrt{\log m} \}\geq b_{m} \right)\leq P\left(\sum_{i=1}^m I\{|T_i|\geq c\sqrt{\log m} \}\geq b_{m} \right)
$$ Then if I show that the LHS goes to 1, I have the claim. However, I can't seem to get the inequalities needed to get the convergence to 1. Any hints or insights? What I have so far \begin{eqnarray}
P\left(\sum_{i\in\mathcal{M}} I\{|T_i|> c\sqrt{\log m} \}\geq b_m\right) &=& 1- P\left( \sum_{i\in\mathcal{M}} I\{|T_i|> c\sqrt{\log m} \}< b_m \right)\\
&\geq& 1-P\left( \sup_{i\in\mathcal{M}} I\{|T_i|> c\sqrt{\log m} \}< b_m/\text{Card}(\mathcal{M}) \right)\\
&\geq&  1- \sum_{i\in\mathcal{M}} P\left(I\{|T_i|> c\sqrt{\log m} \}< b_m/\text{Card}(\mathcal{M}) \right)\\
&\geq&  1- \sum_{i\in\mathcal{M}} [1-P\left(I\{|T_i|> c\sqrt{\log m} \}\geq b_m/\sqrt{n} \right)]\\
\end{eqnarray} Now it suffices to show that $P\left(I\{|T_i|> c\sqrt{\log m} \}\geq b_m/\sqrt{n}\right)\to 1$, which I'm having trouble showing.","['probability-theory', 'probability', 'statistics']"
2470061,Separable Banach Spaces vs. Non-separable ones,"I have just learned about separable Banach spaces. The definition of a separable space that I know is that a space is separable if you can find a countable dense subset of it. I would be appreciated if someone could point out that how really a separable and non-separable space differ. (In other words, why do we need to define separability?)","['functional-analysis', 'banach-spaces', 'separable-spaces']"
2470085,Semifactorial Identity,"I was wondering if anyone had any insight on how to prove the following identity: For all $m \in \mathbb{N}$ $$ \frac{1}{2m-1} + \frac{2m-2}{(2m-1)(2m-3)} + \frac{(2m-2)(2m-4)}{(2m-1)(2m-3)(2m-5)} + \cdots + \frac{(2m-2)!!}{(2m-1)!!} = 1$$ I attempted to rewrite and simplify the left hand side as:
$$ \sum_{k = 1}^m \frac{\frac{(2m-2)!!}{(2m-2k)!!}}{\frac{(2m-1)!!}{(2m-2k-1)!!}} = \sum_{k = 1}^m \frac{(2m-2)!! (2m-2k-1)!!}{(2m-2k)!! (2m-1)!!} = \frac{\left[2^{m-1} (m-1)! \right]^2}{(2m-1)!} \sum_{k=1}^m \frac{(2m-2k-1)!!}{(2m-2k)!!} $$ But I do not seem to be getting anywhere. Does anyone see how to prove the statement?","['combinatorics', 'factorial', 'summation', 'discrete-mathematics']"
2470167,How many $8$ letter words can be formed from the letters $PAAAARRTTM$? [duplicate],"This question already has answers here : How to find the number of $k$-permutations of $n$ objects with $x$ types, and $r_1, r_2, r_3, \cdots , r_x$ = the number of each type of object? (4 answers) Closed last year . How many $8$ letter words can be formed from the letters $PAAAARRTTM$? This is what I thought: Having $8$ letter words is basically just counting the ways of arranging the letters, after we choose to remove $2$ of the letters from the $10$ given. Thus, I will count the ways of arranging the letters, after I remove some two letters. The letters I will remove come in $13$ cases and are: $$\{P,A\},\{P,R\},\{P,T\},\{P,M\},\{A,R\},\{A,T\},\{A,M\},\{A,A\},\{R,T\},\{R,M\},\{R,R\},\{T,M\},\{T,T\}.$$ Removing $P$ and some other letter after: $$= \frac{8!}{3!2!2!} + \frac{8!}{4!2!} + \frac{8!}{4!2!} + \frac{8!}{4!2!2!} = 3780.$$ 
Similarly for $A$: $$= \frac{8!}{3!1!2!} + \frac{8!}{3!2!1!} + \frac{8!}{3!2!2!}+ \frac{8!}{2!2!2!} = 13440$$
Similarly for $R$: $$=\frac{8!}{4!2!} + \frac{8!}{4!2!2!} + \frac{8!}{4!2!} = 2100$$
Similarly for $T$: $$=\frac{8!}{4!2!2!} + \frac{8!}{4!2!}= 1260$$ So total number of ways is $20580$. However, the answer is $22260$, with a difference of $1680 = \frac{8!}{4!}$... Did I go wrong somewhere? Also, for these type of questions, (given $n$ letters where some repeat and we have to form $k < n$ words from them), is this the systematic way to approach it?",['combinatorics']
2470168,"Does there exist a vector space $V$ which is isomorphic to $\mathbb{R}^m$, but is not a topological/smooth manifold of dimension $m$?","Does there exist a vector space $V$ which is isomorphic to $\mathbb{R}^m$, but is not a topological/smooth manifold of dimension $m$? Suppose $V$ is isomorphic to $\mathbb{R}^m$. In the category of vector spaces, $V$ is equivalent to $\mathbb{R}^m$, but $V$ may not have a topology on it, so it may not be a topological manifold, and hence it can definitely not be a smooth manifold (since all smooth manifolds are just topological manifolds endowed with a smooth structure). If $V$ is homeomorphic to $\mathbb{R}^m$, then we can conclude in that $V$ is equivalent to $\mathbb{R}^m$ in the category of topological spaces, and $V$ must also be a topological manifold, however since every topological manifold is not a smooth manifold (see the Exotic Sphere as an example), we can't necessarily conclude that $V$ is diffeomorphic to $\mathbb{R}^m$. So my question is the following, does an isomorphism between $V$ and $\mathbb{R}^m$ induce a homeomorphism between $V$ and $\mathbb{R}^m$, and furthermore does a homeomorphism between $V$ and $\mathbb{R}^m$ (along with the isomorphism between the two vector spaces) induce a diffeomorphism between $V$ and $\mathbb{R}^m$? If not could someone provide me with an example of a vector space $V$ which is  isomorphic to $\mathbb{R}^m$, but not homeomorphic, and also of an example of a vector space $V$ which is homeomorphic and isomorphic to $\mathbb{R}^m$ but not diffeomorphic to $\mathbb{R}^m$? Note: Isomorphism here means isomorphism of vector spaces.","['category-theory', 'general-topology', 'linear-algebra', 'differential-topology']"
2470335,Notation of multiple covariant derivatives of a tensor,"From this wikipedia page the laplacian of a tensor is defined by
$$ \Delta T = g^{ij} \left(\nabla_{\partial_i} \nabla_{\partial_j} T - \nabla_{\nabla_{\partial_i} \partial_j} T\right). $$
I have an issue here with what they mean by $\nabla_{\partial_i} \nabla_{\partial_j} T $. Consider for simplicity the case that $T$ is a $(2,0)$-tensor (input 2 vectors, output a real number). There ware two ways to interpreet this. Interpreet $\nabla_{\partial_j} T$ first as the $(3,0)$-tensor $\nabla T$. Then we take the covariant derivative of $\nabla T$ and then insert the vectors $\partial_i$ and $\partial_j$. Interpreet $\nabla_{\partial_j} T$ as a $(2,0)$-tensor and take the covariant derivative with respect to $\partial_i$ of this expression. I know that the first interpretation is independent of the coordinates and the second isn't. When the first interpretation is written out in coordinates, then it equals the formula for $\Delta T$, but interpreted by the second interpretation listed. I feel like the second interpretation is the correct one?","['tensors', 'riemannian-geometry', 'differential-geometry']"
2470375,"When does the equation $(A^2+x^2)^sx = C$ have an explicit, closed-form solution?","Solving a certain inverse problem reduces to solving the following elementary equation: $$
(A^2+x^2)^sx = C
$$ Here $x$ is the unknown. We are interested in $x > 0$ . $s \in \mathbb{R}$ is a real number. $A > 0$ is a fixed real number. $C > 0$ is a fixed number, small enough that the equation has at least one solution. It turns out that depending on the value of $s$ the equation has one or at most two solutions. The left side is strictly increasing when $s \ge -1/2$ . It is strictly increasing up to a given point, after which it is strictly decreasing, when $s < -1/2$ . For what values of $s$ does the equation have a closed form solution? I know of the following cases: $s = -1$ second order polynomial $s = -1/2$ second order polynomial $s = 0$ trivial $s = 1/2$ fourth order polynomial with special form $s = 1$ third order polynomial, which I did not bother to solve Are there more, and is it possible to verify that all have been found? Note that the equation is polynomial if $s$ is rational, but not all polynomial equations have explicit solutions.","['algebra-precalculus', 'elementary-functions', 'transcendental-equations', 'closed-form']"
2470411,Building an equation. Trying to figure out the relationship between variables.,"Edit: Thanks to Verdruss, Epiousios, and geeky me for solving this for me and teaching me about logarithms to boot!  You've helped restore my sanity, it's been driving me crazy for over a week now. I have variables A, B, and C.  I'm trying to build a formula for any given value of A and B, such that it gives C in the patterns shown below.  I'll start off with some examples. If A=1, and B=50, then C=50
A=10, B=50, then C=100
A=100, B=50, then C=200
A=1000, B=50, then C=400
A=10000, B=50, then C=800 And so on.  Or, A=1, B=1, C=1
A=10, B=1, C=2
A=100, B=1, C=4
A=1000, B=1, C=8
A=10000, B=1, C=16 Ad infinitum. A=1000, B=999, then C=7992 But what if... A=6923, B=30, C=??? It would be somewhere between 240 and 480, but what exactly? Every time A increases an order of magnitude, C doubles, with B being the starting value of C given A=1.  I can solve this equation as long as I plug in 1, 10, 100, 1000, 10000, and so on into it, but not any other number for A.  Now, I had an engineering friend help me a bit, but his solution gives me stratified, tiered equations bracketed by the magnitudes, and not say, a single sloped line per any constant B. The equation I'm looking for would be something along the lines of ab=c, but I can't for the life of me figure out how to express the relationship in the above charts into mathematical terms. And on top of it all, I'm pretty much a math amateur.  I've done about 20 hours of Khan Academy, and tons of browsing through the different topics to find the one topic that will help me solve this specific equation I'm looking for. But most of the education I'm finding is related to solving equations, not building equations to further solve an equation. I fear I'm in straight up Calculus territory here, but I don't know.  Or maybe this equation is impossible to solve, despite how elegant it seems in simple terms. The most complicated insight I could give into would be something along the lines, like, each real number increment of A is worth less/(or more?) then the number before it.  IE, If B= 10, then C=10 if A=1, C=20 if A=10, but C= ~16 (or is ~14?) if A=5. Or maybe there is a bigger equation here beyond just the scope of the variables I'm working with? I feel like there is something in the appreciation(or depreciation) of the value each point of A is worth that is key to solving this. Is this a trigonometric equation?  Quadratic? Simple exponential?  Multivariable calculus?  Differentials?  Are there math terms I've simply never seen before that could build this equation?  Is there a type of mathematics that covers this kind of equation so I can research it? ...Should I give up?  Or will I find, for sure!, the answer after I pour one thousand hours into learning advanced mathematics? Or is the equation truly impossible to solve/build? Or that the simple A B C format doesn't hold on it's own until I figure out the bigger formula it is a part of?  (Like maybe, A=1, B=X, C=X is impossible, but part of another equation the relationship between A and C can exist? (Where C doubles for every magnitude increase of A(And I will also concede that it might be how I've worded it, I'm pretty certain the doubling doesn't hold between, say, A=9 compared to A=90)))? Sorry this post is kind of a thought salad.  I'm just not certain the best way to explain what I want and my thoughts on various aspects of it. Thanks for any help or guidance! I appreciate it.",['algebra-precalculus']
2470421,inverse function of ODE solution,"I'm trying to determine the inverse of an ODE solution. If this is possible. $$\alpha(t)=\alpha_0(I(t)/I_0)^p$$
, where $I(t)$ is to be determined (although $lim_{t\to\infty}I(t)=I_{max}$ is fixed) Next:
$$\frac{dn}{dt}=T(\alpha(t)(1-n)-\beta n)$$
And
$$B(t) = G\alpha(t)(1-n(t))$$ Now, (I think) I've solved $n$ and $B$ to be
$$n(t)=\frac{\alpha(t)}{\alpha(t)+\beta}+Ce^{-T(\alpha(t)+\beta)t}$$
$$B(t) = G\alpha(t)(\frac{\beta}{\alpha(t)+\beta}-Ce^{-T(\alpha(t)+\beta)t})$$ For $\lim_{t\to\infty}$:
$$n_\infty=\frac{\alpha_\infty}{\alpha_\infty+\beta}$$
$$B_\infty=G\frac{\alpha_\infty\beta}{\alpha_\infty+\beta}$$ Let's evaluate this with for instance $I(t)=0.0158I_0$, $\alpha_0=0.1$, $p=0.5$, $T=60$, $\beta=0.007$, $G=37$, $n_0=0$ and evaluate over $0\leq t<24$. I want to determine $I(t)$ to realize $B(t)=B_\infty$. Effectively ramping-up $I(t)$ to keep $B(t)$ constant. I expect $I(t)$ to look somewhat like:
$$I(t) = p+q(1-e^{-T(\alpha(t)+\beta)t})$$
, where $p+q=0.0158I_0$. But since $\alpha(t)$ is a function of $I(t)$, I run into problems there. So now I'm trying to invert $B(t)$ and $n(t)$. But especially the inverse of $n(t)$ is giving me headaches. Is my approach wrong? Should I instead use control theory?","['control-theory', 'ordinary-differential-equations', 'inverse-function']"
2470429,Solve $x \ddot {x} + {\dot {x}}^2=0$.,"I solved a physics problem and I got this equation, but I don't know how to proceed. Could you solve for $x (t)$ this equation:
$x \ddot {x} + {\dot {x}}^2=0$",['ordinary-differential-equations']
2470459,Upper Bound for Finite Euler Product,"Let $n$ be a positive integer. I am trying to show
$$\prod_{p\le n/2, p\,\text{prime}}(1-1/p)\le \frac{1+\pi(n)}{n},$$ where $\pi(n)$ denotes the number of primes $\le n$. After doing some calcuations by hand for small $n$, this seems clear. According to some Mathematica computations, this is true for $n\le 20,000$. However, I am not sure of how to give a rigorous proof since I am not aware of any upper bounds for the Euler product. I know of asymptotics for the Euler product, but I am not interested in what happens in the limit since I'd like to see if this is true for each $n$. One idea is to apply the fundamental theorem of arithmetic, to write $$\prod_{p\le n/2, p\,\text{prime}}(1-1/p)=\frac{1}{\sum_{i\in F_{n/2}}\frac{1}{i}},$$ where $F_{n/2}$ is the collection of positive integers with greatest prime factor at most $n/2$, but I am unsure of the size of this sum compared to $\frac{1+\pi(n)}{n}.$","['number-theory', 'products', 'prime-numbers']"
2470493,The length of parametric curve defined by integrals,"A curve is defined by the parametric equations $$x=\int \limits_{1}^{t}\dfrac{\cos u}{u}du, \ y=\int \limits_{1}^{t}\dfrac{\sin u}{u}du.$$ Find the length of the arc of the curve from the origin to the nearest point where there is a vertical tangent line. My solution: In order to find point where there is a vertical tangent line we need to solve $\dfrac{dx}{dt}=0$ and the nearest point is $\dfrac{\pi}{2}$. Using FTC we have $\sqrt{(x')^2+(y')^2}=\dfrac{1}{t}$. Hence, $$L=\int\limits_{0}^{\pi/2}\dfrac{dt}{t}=\ln t |_{0}^{\pi/2}$$ But $\ln(0)=-\infty$. Can anyone explain why there is a problem here?","['derivatives', 'integration', 'arc-length']"
2470509,On embeddings of Lie groups,"Let $G$ be a Lie group with identity component $G_0$, such that $G_0$ embeds as a closed subgroup into some other connected Lie group $\widetilde{H}$. Then does there always exist a Lie group $H$ with identity component $H_0 = \widetilde{H}$, such that $G$ embeds as a closed subgroup into $H$ ? As $G_0$ is always a normal subgroup of $G$, my idea was to examine the short exact sequence $0 \to G_0 \to G \to G/G_0 \to 0$, where $G/G_0$ is a countable discrete group, whose elements can be represented by the (countable many) components of $G$. If this sequence splits, we can identify $G = G_0 \rtimes_{\phi} F$ for $F$ some countable discrete group and $\phi: F \to Aut(G_0)$ some homomorphism. Now if there was a way to ""extend"" $\phi$ to a homomorphism $\tilde{\phi}: F \to Aut(\tilde{H})$ in such a way that for every $f \in F$, $\tilde{\phi}(f)|_{G_0} = \phi(f)$ (this is equivalent to requiring that every automorphism in the image of $\phi$ can be extended to an automorphism on $\widetilde{H}$), then $H := \widetilde{H} \rtimes_{\tilde{\phi}} F$ would be the required Lie group. However, this is only a partial answer, since it does not always seem to be the case that this sequence splits (more generally, there seem to exist Lie groups that cannot be decomposed as the semi-direct product of a discrete group and its identity component). Moreover, even if we have such a splitting, i am uncertain of when exactly one can extend the homomorphism $\phi$ in the way described above.","['lie-groups', 'differential-geometry', 'lie-algebras', 'geometry']"
2470541,Prove that $\frac{1+\sin\theta+i\cos\theta}{1+\sin\theta-i\cos\theta}=\sin\theta+i\cos\theta$,Prove that $$\frac{1+\sin\theta+i\cos\theta}{1+\sin\theta-i\cos\theta}=\sin\theta+i\cos\theta$$ I tried to rationalize the denominator but I always end up with a large fraction that won't cancel out. Is there something I'm missing? Thanks in advance,"['substitution', 'fractions', 'trigonometry', 'complex-numbers']"
2470625,Determine the constant so that the integral is minizimed,"Presume that $a<b$ and that $f$ is continuous in the interval $[a,b]$, Determine K so that the integral: $$\int_a^b (f(x) - K)^2 dx$$ is minimized, also what is this minimum? I found similar questions but none were working with $f(x).$","['maxima-minima', 'integration', 'definite-integrals', 'calculus']"
2470645,Importance of Riemann-Roch theorem,"I read yesterday the statement of Riemann-Roch theorem and I didn't actually detect the huge importance that anyone tells me it has... So, can anyone provide me with some examples or reasons for being considered one of the most important theorems in algebraic geometry-algebraic curves?","['algebraic-curves', 'soft-question', 'algebraic-geometry']"
2470678,"Using the definition of derivative, find $f'(x)$ where $f(x) = \frac{\cos x}{x}$","I have attempted to solve the problem, but got stuck on the way, see below. \begin{align*}
f'(x)&=\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}\\
&= \lim_{h\to 0}\frac{\frac{\cos(x+h)}{x+h}-\frac{\cos x}{x}}{h}\\
&=\lim_{h\to 0}\frac{x\cos(x+h)-(h+x)\cos x}{xh(x+h)}\\
&=\lim_{h\to 0}\frac{x\cos h\cos x-x\sin h\sin x-(h+x)\cos x}{xh(x+h)}\\
&=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)-h\cos x}{h}-\frac{x\sin x\sin h}{h}\right)\\
&=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)}{h}-\cos x-\left(\frac{\sin h}{h}\right)x\sin x\right)
\end{align*} From here I cannot solve $$\lim_{h\to 0}\frac{x\cos x(\cos h-1)}{h}.$$ Any suggestions? Or maybe I have taken the wrong route.","['derivatives', 'calculus', 'limits']"
2470793,A very different alternative form of the geometric series,"While I was playing around with series that came up in a calculus assignment I came across this thing here:
$$\lim_{n\to \infty} \sum_{i=1}^n \frac{1}{\sqrt[x]{n^{x-1}i}}$$
And after using wolframalpha to evaluate it at some positive real numbers $x$ it seems that
$$\lim_{n\to \infty} \sum_{i=1}^n \frac{1}{\sqrt[x]{n^{x-1}i}} = \frac{x}{x-1}$$
which is the value of the geometric series with parameter $\frac{1}{x}$. Unfortunately I have no clue how to prove this.","['sequences-and-series', 'calculus', 'geometric-series']"
2470806,"A ""chord"" of a square, subtending a $45^\circ$ angle at a vertex, determines a triangle whose area is bisected by the square's ""other"" diagonal","The following Image shows the square ABCD, a point E on the side BC and the segment AF, which is a rotation of AE, at 45° (not necessarily congruent). DB is the diagonal of the square; G and H are the intersection points of the diagonal and the line segments AF, AE (respectively). Prove that the area of triangle AGH equals the area of quadrilateral FGHE. Bonus question: Trace a perpendicular to FE from point A, and let the point A' be the intersection of said perpendicular with segment line FE. Prove that A' describes a circumference with center A, when point E moves along the segment BC. Note: I know this can be solved with some analysis and analytic geometry, but I'd love to see a purely geometric solution.","['quadrilateral', 'trigonometry', 'triangles', 'geometry', 'area']"
2470822,"How to calculate this definite integral :- $\int_0^\pi \frac{x}{a^2\cos^2x + b^2\sin^2x} \,dx$?","This question was in a test I took: calculate the value of the integral $$\int_0^\pi \dfrac{x}{a^2\cos^2x + b^2\sin^2x} \,dx.$$ I was not able to solve it. Now, I've tried to do some substitutions for $x$ but honestly I don't actually know how to proceed with this integral. As I have not solved such questions before, I don't know the general direction in which I need to go so I don't know which efforts to include here and which to not. All I know is that those constants a & b are the source of much of the trouble for me but I don't know how to get rid of them. I also want to know whether there is only a single way to solve it (which maybe I've not practised enough) or - like many other problems - it can be solved by more than one method. I would really appreciate if I find different solutions of this problem here but (if you can help it) please don't include any incredibly tough/ esoteric theorems or concepts/ higher-level stuff that I can't be expected to know at my current level. Thanks!","['integration', 'definite-integrals', 'calculus']"
2470958,"""Existence and Uniqueness"" Theorems for first order IVP: two or just one?","Let's say that I've got the following IVP: $\frac{dy}{dx} = f(x,y)$ $y(x_0) = y_0$ And I want conditions that guarantee existence and uniqueness of its solution. On the one hand I've got the Picard–Lindelöf theorem. It asks that there exists a rectangle $R = [a,b] \times [c,d]$, containing $(x_0, y_0)$ as an interior point, where $f$ is continuous in $x$ and Lipschitz continuous in $y$. On the other hand I've got a theorem, which I've encountered in many undergraduate text books, that requires $f$ and $\frac{\partial f}{\partial y}$ to be continuous in the aforementioned rectangle. Are these two different theorems? It seems to me that the hypotheses of the first one are implied by those of the second one. But in that case, why would some authors prefer this more restrictive form of the theorem? Could it be just so that students don't need to learn the concept of Lipschitz continuity?",['ordinary-differential-equations']
2470987,Inductive proof of sum of falling factorials with Stirling number coefficients,"Concrete Mathematics (page 262 2nd ed.) demonstrates $x^n = \sum_k \left\{ \begin{matrix} n \\ k \\ \end{matrix} \right\} x^{\underline{k}}$ using a proof by induction: $$ 
\begin{align}
x \sum_k  \left\{ \begin{matrix} n-1 \\ k \\ \end{matrix} \right\} x^{\underline{k}} &= 
\sum_k  \left\{ \begin{matrix} n-1 \\ k \\ \end{matrix} \right\} x^{\underline{k+1}} +
\sum_k  \left\{ \begin{matrix} n-1 \\ k \\ \end{matrix} \right\} kx^{\underline{k}} 
\tag{1}
\\
&=  
\sum_k \left\{ \begin{matrix} n-1 \\ k-1 \\ \end{matrix} \right\} x^{\underline{k}} + 
\sum_k  \left\{ \begin{matrix} n-1 \\ k \\ \end{matrix} \right\} kx^{\underline{k}}
\tag{2}
\\
&= \sum_k 
\left( 
k \left\{ \begin{matrix} n-1 \\ k \\ \end{matrix} \right\} + \left\{ \begin{matrix} n-1 \\ k-1 \\ \end{matrix} \right\} 
\right) x^{\underline{k}}
= \sum_k \left\{ \begin{matrix} n \\ k \\ \end{matrix} \right\} x^{\underline{k}}
\tag{3}
\end{align}
$$ (1) is due to $ x \cdot x^{\underline{k}} = x^{\underline{k+1}} + kx^{\underline{k}}$. (3) is an application of the recurrence relation for Stirling numbers of the second kind. How do we get (2) from (1)?","['stirling-numbers', 'discrete-mathematics']"
2471015,Example of element of double dual that is not an evaluation map,"It's well known that if $V$ is a vector space over a field $F$, then there is a natural injection from $V$ to the double dual $V^{**}$, which associates to every $v \in V$ the evaluation map $\phi \mapsto \phi(v)$, where $\phi: V \to F$ is an arbitrary functional in $V^*$. It's also well known that this injection is an isomorphism if $V$ is finite-dimensional, as any finite-dimensional vector space has the same dimension as its dual. My question is this: are there any nice, readily understood examples of infinite -dimensional vector spaces $V$ for which an element of $V^{**}$ that is not an evaluation map can be explicitly constructed (at least with the axiom of choice)? I find infinite-dimensional double dual spaces hard even to think about.","['examples-counterexamples', 'dual-spaces', 'abstract-algebra', 'functional-analysis', 'linear-algebra']"
2471016,"Find the height of a bar, given the lengths of shadows cast by it and another bar","What is the height of the red bar? My try: with respect to the picture, it seems for the green bar $\frac{h}{H}=\frac{2}{3}$. So, I think that ratio is the same for the red bar, and the height of the red bar is 
$$\frac{h}{6+4}=\frac 23\qquad\to\qquad h_{red}=\frac{20}{3}$$ Is this correct?","['algebra-precalculus', 'soft-question', 'geometry']"
2471019,A question about Cayley-Hamilton's density proof.,"The usual Cayley-Hamilton proof using density is something like this: Lemma: Let $f,g:X\to Y$ be two continuous functions in metric spaces $X$ and $Y$. If $f(x)=g(x)$ for all $x\in E$, where $E$ is a dense subset of $X$, then $f=g$. Let $\chi_A$ be the characteristic polynomial of $A$. Since it is trivial to prove Cayley-Hamilton for diagonalizable matrices and the set of all diagonalizable matrices is dense in $M_n(\mathbb{C})$ we can argue as follows.
  If $A$ is any matrix, there is a sequence of diagonalizable matrices $A_k$ such that $A_k\to A$. Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma. Since $\chi_{A_k}(A_k)=O_n$ (because CH holds for diagonalizable matrices), it follows that $\chi_A(A)=O_n$. My problem with this proof is the following step: ""Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma."" I do not find this obvious. Why is the function $f(A)=\chi_A(A)$ continuous? It is clear that, for a fixed matrix $B$, the function $g(A)=\chi_B(A)$ is continuous as it is a polynomial. However that does not seem to be enough. Can someone clarify this for me? Thanks. EDIT: I further explained my trouble. Consider the functions $f(A)=\chi_A$ and $g(p)=p(A)$. $f$ maps $A$ to its characteristic polynomial and $g$ gets some polynomial and applies $A$ to it.
  Clearly the map $A\mapsto \chi_A(A)$ is the function $g\circ f$. $f$ is continuous since $f(A)=\det(xI-A)$. That is, $f$ is polynomial in the entries of $A$. However, why does $g$ is continuous? How can I prove it?","['general-topology', 'linear-algebra']"
2471037,Can $A\sin^2t + B\sin t\cos t + C\sin t + D\cos t + E = 0$ be solved algebraically?,"This started out much more complex, but I've reduced an equation to this (it's for finding intersections of ellipses): $$A\sin^2(t)+B\sin(t)\cos(t)+C\sin (t)+D\cos(t)+E=0$$ I want to solve for t where A/B/C/D/E are constants.  Is this solvable algebraically, or is only numeric approximation possible? Using trig identities and the formula for phase shifting, I can further simplify it down to this form: $$\sin(2t+F) + G\sin(t+H) = I$$ Where F/G/H/I are constants.  The formula is much simpler, but this may be a dead end, because now we have two angles to deal with.",['trigonometry']
2471110,Why is $C^{-\infty}$ useful?,"So, $C^{-\infty}(\Omega) := (C_c^{\infty}(\Omega))'$, that is the space of 
all continuous linear functionals over the space of compactly supported smooth functions. $C^{-\infty}(\mathbb{R}^n)$ is larger than tempered distributions $S'(\mathbb{R}^n)$ and we would get elements of $C^{-\infty}$ that aren't in 
$S'$ if we took for example the integral pairing, $ u \Psi \to \int \phi \Psi$ for some $\phi \in C^{\infty}$. My question is what is the motivation behind these very general distributions? Are there examples where being in tempered distributions is not general enough? We have for example that Fourier transform is an isomorphism on $S'$ which is pretty useful but we don't have nice properties (I should say that
I don't know any) on the very general $C^{-\infty}$, so I am wondering where 
they are needed. Thank you!","['functional-analysis', 'dual-spaces']"
2471117,"Maximum Likelihood Estimation of Multivariate Gaussian Density, where the number of samples is smaller than the unknown parameters","If we want to estimate the $p\times p$ (full rank) covariance matrix $\Sigma$ of multivariate normal density, using $n$ sample vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$, then the empirical covariance matrix is known to be the maximum likelihood estimation $\Sigma_\text{ML}$ (at least when $n\gg p$) of $\Sigma$.
But when $n\propto p$, or in particular when $n<p$, then ML estimation is not reliable, and becomes singular, since $\Sigma_\text{ML} = \dfrac{1}{n} XX'$, where $X=[\mathbf{x}_1,\ldots,\mathbf{x}_n]$ is a $p\times n$ matrix. Now, since $n<p$ the rank of this $p\times p$ matrix $\Sigma_\text{ML}$ is at most $n$, making it singular.
Which obviously is not a good estimation, since we have samples of a full rank matrix $\Sigma$. But the covariance matrix simply consists of pairwise covariance values $\Sigma=\sigma_{ij}$, and to compute each, we can use the sample vectors, (using components $i$ of each sample vector and the whole $\mathbf{x}_n$), which are apparently enough to estimate $\sigma_{ij}$. We can do this for all the covariance elements. But what is the reason that we still cannot rely on ML estimation? Is it because, we use those $n$ samples $p\times (p+1)/2$ times (to compute each $\sigma_{ij}$), and this results in having somehow dependent rows (columns), and hence a singular $\Sigma_\text{ML}$? But how we can rigorously show this?","['statistical-inference', 'matrices', 'matrix-calculus', 'statistics', 'stochastic-approximation']"
2471119,Geodesic differential equation with arc legth parametrization,"As known, if $S\subset \mathbb{R}^3$ is a surface, $\sigma: U\subset \mathbb{R}^2 \rightarrow V\cap S$ a parametrization, and 
\begin{align}\alpha:I& \rightarrow S\\
t& \mapsto \sigma(u(t),v(t)) \end{align}
 a curve on $S$, then $\alpha$ is a geodesic if, and only if, resolve que defferential equation \begin{align}u'' + \Gamma_{11}^1 (u')^2 + 2 \Gamma_{12}^{1}u' v' + \Gamma_{22}^{1} (v')&=0, \qquad(1) \\  
v'' + \Gamma_{11}^2 (u')^2 + 2 \Gamma_{12}^{2}u' v' + \Gamma_{22}^{2} (v')&=0, \qquad(2) 
\end{align} where $\Gamma_{ij}^{k}$ are the Christoffel symbols. I need to show that: QUESTION: When the differential equation of the geodesics are referred to the arc length then the second equation (2) is, except for the coordinate curves, a consequence of the first equation (1). The book suggests write each $\Gamma_{ij}^{k}$ in function of $E$, $F$, $G$ (where $E(u,v)= <\sigma_u(u,v),\sigma_u(u,v)>$, $F(u,v)= <\sigma_u(u,v),\sigma_v(u,v)>$), $ G(u,v)= <\sigma_v(u,v),\sigma_v (u,v)>$) i. e. \begin{align}
\Gamma_{11}^{1} &= \frac{G E_u - 2 F F_u + F E_v}{2(E G - F^2)}, \quad \Gamma_{11}^{2} =\frac{2EF_u - E E_v - F E_u}{2(EG - F^2)},\\
\Gamma_{12}^{1} &= \frac{G E_v - F G_u}{2(E G - F^2)}, \hspace{2cm} \Gamma_{12}^{2} =\frac{EG_u - F E_v}{2(EG - F^2)},\\
\Gamma_{22}^{1} &= \frac{2GF_v - G G_u - F G_u}{2(EG - F^2)}, \quad \Gamma_{22}^{2} =\frac{E G_v - 2F F_v + F G_ u}{2(EG - F^2)},
\end{align} and use that $\sigma(u(t),v(t))$ is parametrized by arc length, i.e. $$1= E (u')^2 + 2F (u'v') + G (v')^2  $$
$$0= E_u (u')^3 + E_v (v')(u')^2 + 2 E u'' u' + 2F_u (u')^2 v' + 2F_v u' (v')^2 + 2F u'' v' + 2F u' v'' + G_u u' (v')^2 + G_v (u')(v')^2 + 2 G v'' v',   $$ to conclude the result, but I wasn't able to manipulate this equations in order to prove the required. Any help?","['arc-length', 'ordinary-differential-equations', 'differential-geometry']"
2471124,Explanation about nonhomogeneous equations,"I have an easy question that make me crazy 
I have the equation : $$ty'+2y=\sin(t)$$ I think that it's nonhomogeneous equation I use the known  method to solve it but I couldn't obtain the solution that should be the right solution : $$
{y}\mathrm{{=}}\frac{\mathrm{{-}}{t}\cos{t}\mathrm{{+}}\sin{t}\mathrm{{+}}{c}}{{t}^{2}}
$$ Any explaning I will be very thankful",['ordinary-differential-equations']
2471186,$\sin(t)$ solution of $\ddot{x}=-x$,"Let $\sin(t)$ be the solution of the differential equation $\ddot{x}=-x$.
Why is the general solution of $\ddot{x}=-x$, $x(t)=\lambda \sin(t) + \mu \cos(t)$, with $\lambda, \mu \in \mathbb{R}$?",['ordinary-differential-equations']
2471248,"Question about proof of ""cubing the cube""","The Wikipedia article says: Suppose that there is such a dissection. Make a face of C its horizontal base. The base is divided into a perfect squared rectangle R by the cubes which rest on it. Each corner square of R has a smaller adjacent edge square, and R's smallest edge square is adjacent to smaller squares not on the edge. Therefore, the smallest square s1 in R is surrounded by larger, and therefore higher, cubes on all four sides. Hence the upper face of the cube on s1 is divided into a perfect squared square by the cubes which rest on it. Let s2 be the smallest square in this dissection. The sequence of squares s1, s2, ... is infinite and the corresponding cubes are infinite in number. This contradicts our original supposition. Why is it important that the smallest rectangle is not located on the edge?","['puzzle', 'proof-explanation', 'discrete-mathematics']"
2471276,"My students don't appreciate the marginal cost function, please help,","Given a cost function c(x), and knowing the cost for some large enough x, say, 500 units, we can find the incremental cost of producing the 501st unit, by computing c'(500).  And then we can also find the incremental cost of producing the 502nd unit by computing c'(501).  So, with this information, we know the incremental costs for producing an extra 2 units, from 500 to 502. My students weren't satisfied at all with this example, because many of them were perplexed about why I couldn't just compute c(502) - c(500) to get my answer.  I was stumped and told them that I don't think they are wrong, but at the moment, I did not have a better answer for them to convince them that the marginal cost function + using derivatives is a good thing. What can I tell them to highlight the usefulness of the marginal cost function and derivatives?","['derivatives', 'education', 'economics']"
2471279,How do I find the length from a tetrahedron vertex to its face using vector methods?,"Suppose the length of each edge of the regular tetrahedron is $x$ and a , b , c represents the position vectors from the origin O and the points A, B, C. How can I find the distance from a vertex to its opposite face? I thought I had to use the concept of scalar products etc. to do this question, but my solution does not involve any vectors and is largely geometric. The answer does not match. My thought process: Since it's a regular tetrahedron, all sides are equilateral triangles. I divided the base of the tetrahedron (one triangle) into 3 areas using the centroid of the face to obtain 3 isosceles triangles. Let distance from vertex to centroid of on triangular face of the tetrahedron by $k$. Since I know the length of the longest side of the isosceles triangle is $x$, I use sine rule of $\frac{\sin(30)}{k} = \frac{\sin(120)}{x}$ to get my $k$ value in terms of $x$. Now I have a new triangle with hypotenuse $x$ and base $k$. I use Pythagoras theorem to get my answer. Could someone show me where my thought process has gone wrong or show me the direction I should approach this question instead?","['vectors', 'geometry']"
2471309,"Series $\sum\limits_{n=0}^{\infty} \frac{(-1)^n}{(3n + k)!}x^{3n + k}, \ k = 0, 1, 2$","Everyone knows that $e^{i\theta} = \cos \theta + i \sin \theta$ for any complex argument $\theta$ , and that the Maclaurin series expansion of say $\sin(z)$ is given by: $$
\sin(x) = \sum\limits_{n = 0}^{\infty} \dfrac{(-1)^n x^{2n+k}}{(2n+k)!}
$$ with $k = 1$ , and similarly $\cos(x)$ with $k = 0$ . So what happens when you consider summing over all residues in a coset of $3\Bbb{Z}$ instead? Substituting in $ix$ for the $3\Bbb{Z}$ case we gives us: $$
f_0(ix) = \sum_{n=0}^{\infty} \dfrac{i^n x^{3n}}{(3n)!} \\
f_1(ix) = \sum_{n=0}^{\infty} \dfrac{i^{n+1} x^{3n+1}}{(3n+1)!} \\
f_2(ix) = \sum_{n=0}^{\infty} \dfrac{i^{n+2} x^{3n+2}}{(3n+2)!} \\
$$ So that $f_0(ix) + f_1(ix) + f_2(ix) = \sum\limits_{n=0}^{\infty} (\dfrac{x^{3n}}{(3n)!} + i\dfrac{x^{3n+1}}{(3n+1)!} - \dfrac{x^{3n+2}}{(3n+2)!})i^n$ .","['summation', 'trigonometry', 'sequences-and-series', 'elementary-number-theory']"
2471343,Prove that X = $\{n \in \mathbb{Z}\mid n+5$ is odd$\}$ is the set of all even integers.,"I was wondering if I am doing the prove to this question correctly.. My attempt: Pick Y to be set of all even integers, so then we have $Y = \{2m\mid m \in \mathbb{Z}\}$. Then I show that $X \subseteq Y$. Let $a \in X$ where $a=2k+1$. Then $a+5 = 2k+1+5 = 2k+6 = 2(k+3)$. Here we see that $a$ must be even. Therefore, $X \subseteq Y$ holds. Is this correct, or am I off a bit?","['elementary-set-theory', 'proof-verification']"
2471382,"If $-1$is a root for $ax^2+bx−3$, find $a^2+b^2$","Given: -1 is a root for $ax^2+bx-3$ , with $a,b$ being positive primes, $x\in \Bbb R$ . Find: the numeric value for $a^2+b^2$ . Background: question asked in an entrance exam (Colégio Militar 2005). My attempt: the other root is $3/a$ and by substitution we can easily find that $$a-b=3\ \ \text{or}\ \ a^2+b^2-2ab=9.$$ I got stuck at this point... how to get the value for $ab$ ? Hints please.","['algebra-precalculus', 'contest-math', 'polynomials']"
2471383,Why are these representations of $\mathbb Z_n$ not irreducible?,"The Cyclic Groups $(\mathbb Z_n,+)$ have various representations. This answer asserts that the only irreducible representations are 1-dimensional, with matrix a real $n$th root of unity; 2-dimensional, with matrix $\left(\begin{smallmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{smallmatrix}\right)$ where $n\theta \equiv 0 \pmod{2\pi}$ but $\sin\theta \neq 0$. But what about other minimal unitary matrix representations? Take as an example  $\mathbb Z_6$, which has other representations. Such as the 6 cyclic permutations of $$\pmatrix{1&0&0&0&0&0\\0&1&0&0&0&0\\0&0&1&0&0&0\\0&0&0&1&0&0\\0&0&0&0&1&0\\0&0&0&0&0&1}
$$ constructed by repeatedly moving the leftmost column to the righthand side of the matrix and shifting all of the other columns left by 1 space. Or the representation of 6 5x5 matrices constructed from the direct sum of the $\mathbb Z_2$ and $\mathbb Z_3$ representations $$\left\{\pmatrix{1&0\\0&1},\pmatrix{0&1\\1&0}\right\} 
$$
and
$$\left\{\pmatrix{1&0&0\\0&1&0\\0&0&1},\pmatrix{0&0&1\\1&0&0\\0&1&0},\pmatrix{0&1&0\\0&0&1\\1&0&0}\right\} 
$$
respectively. Neither of these representations can be written as a direct sum of other representations of $\mathbb Z_6$. In other words, as far as I can tell, these representations have only trivial subrepresentations; therefore, why should they not be considered irreducible (as per the linked question)?","['finite-groups', 'representation-theory', 'group-theory', 'cyclic-groups']"
2471418,"Integral $\int_0^{\infty} \log(x)^2 e^{-x} \, \mathrm{d}x$","According to WolframAlpha, the integral $$\int_0^{\infty} \log(x)^2 e^{-x} \, \mathrm{d}x$$ has closed form $\gamma^2 + \frac{\pi^2}{6}$, where $\gamma$ is the Euler-Mascheroni constant. The term $\frac{\pi^2}{6}$ is $\zeta(2)$, and other integrals of the form $\int_0^{\infty} \log(x)^n e^{-x} \, \mathrm{d}x$ clearly have some relation to values of $\zeta(s)$ at integers. I do not know where to go with this. The usual trick I know for integrating something of the form $\log(x)^n f(x)$ is to compare integrals along the top and bottom of the slit in a keyhole contour, but this doesn't converge with the term $e^{-x}$.","['complex-analysis', 'definite-integrals', 'euler-mascheroni-constant']"
2471462,Differential equations for chemical reaction $\mathrm{A + 2B \to 3C}$,"In a chemical reaction $\mathrm{A + 2B \to 3C}$, the concentrations $a(t)$, $b(t)$ and $c(t)$ of the three substances A, B and C measure up to the differential equations
  $$
\begin{align}
\frac{da}{dt} &= -rab^2\tag{1}\\
\frac{db}{dt} &= -2rab^2\tag{2}\\
\frac{dc}{dt} &= -3rab^2\tag{3}
\end{align}
$$
  with $r > 0$ and begin condition $a(0) = 1$ and $b(0) = 2$. Show that $b(t) - 2a(t) = 0$ . Here is my solution, but it is not right. Any help would be great. First equation $$
\begin{align}
\int\frac{da}{a} &= -rb^2\int dt\\
\ln(a) &= -rb^2t + C\\
a(t) &= e^{-rb^2t+ C}\\
a(0) &= 1 &&\to &1 &= e^{0 + C}\\
\ln(1) &= C &&\to &C &= 0\\
a(t) &= e^{-rb^2t}
\end{align}
$$ Second equation $$
\begin{align}
\frac{db}{dt} &= -2rab^2\\  
\int \frac{db}{b^2} &= -2ra\int dt\\
b &= \frac{1}{2rat + C}\\
b(0) &= 2 \quad \to \quad 2 = \frac{1}{C} \quad \to \quad C = \frac{1}{2}\\
b(t) &= \frac{2}{ 2rat + 1}
\end{align}
$$ But now $b(t) - 2a(t) \ne  0$. Where I am making mistake? Any tip will be enough.","['ordinary-differential-equations', 'chemistry']"
2471613,"How many distinct values of $x_1+x_2+x_3+x_4+x_5+x_6+x_7$ when $x_1,x_2,x_3,..,x_7 \in \{0,3,4,5\}$","I have a multiple choice question. Suppose $x_1,x_2,x_3,..,x_7 \in \{0,3,4,5\}$ . How many distinct answers are possible for $A$ , when $A=x_1+x_2+x_3+x_4+x_5+x_6+x_7$ ? Choices: 14 30 34 28 I can try to put all the $(0,0,...,0), (3,0,0,...,0) ... (5,5,5,...,5)$ , but it takes a long time. I think there is a trick to find the possible answer by combinatorics, but how?","['algebra-precalculus', 'combinatorics', 'linear-diophantine-equations']"
2471614,When does the limit of $a_n$ exist where $a_{n+1}:=a_n+\frac{a_n^2}{n^2}?$,"Consider the recursive relation $a_{n+1}:=a_n+\cfrac{a_n^2}{n^2}$. The existence of $\lim_n a_n$ depends on the  initial value $a_1$. For instance: If $a_1=1$, then $a_n=n$ and the sequence is divergent. If $a_1=0$, then $a_n=0$ and the sequence is convergent. Questions : Numerical calculation shows that if $a_1\in(-2,1)$, then it is convergent. Is that right? How to prove that, and can we find the limit? How about $a_1\in \mathbb{C}$ ? P.S: I found this related to Göbel's Sequence .","['limits', 'sequences-and-series', 'calculus', 'telescopic-series', 'convergence-divergence']"
2471651,Why is continuity of partials necessary for Jacobian to be the derivative?,"Jut having the partials is not enough, but partials being continuous
  is enough to guarantee that the Jacobian is the derivative. My question is: ""Why is it necessary for the partial derivatives to be continuous? Isn't just the existence of partials sufficient?"" According to what I know, the Jacobian is a just a matrix consisting of the partial derivatives.","['derivatives', 'partial-derivative', 'multivariable-calculus', 'continuity', 'jacobian']"
2471705,Sets such that inner measure equals outer measure is a sigma algebra,"Let $\mu$ be a measure on a $\sigma$-algebra $\mathcal{A}$ in $\mathcal{P}(\Omega)$ such that $\mu(\Omega)<\infty$. Define inner and outer measures as follows. $\mu^*(E)=\inf\{\sum_{n=1}^{\infty}\mu(A_n): A_n\in\mathcal{A}, E\subset\bigcup_{n=1}^{\infty}A_n\}$ and $\mu_*(E)=\sup\{\mu(A): A\in\mathcal{A}, A\subset E\}$.
I need to prove that the set $\bar{\mathcal{A}}=\{E\subset\Omega: \mu_*(E)=\mu^*(E)\}$ is a $\sigma$-algebra. First, I tried to prove it in a direct manner. But I got stuck. After trying some other exercises I thought, $\bar{\mathcal{A}}=\{E\subset\Omega: \mu_*(E)=\mu^*(E)\}=\{E\subset\Omega: \exists A,B\in\mathcal{A}, A\subset E\subset B, \mu(B\setminus A)=0\}$. Is this true? Should I prove the first one direct, or can I prove that the second one is a $\sigma$-algebra?","['outer-measure', 'measure-theory']"
2471722,"Non-equivalence $p$-norm on $\ell_p$, $L_p(X, \mu)$ to any inner product norm","Show that the norm on the spaces $\ell_p$ , $L_p(X, \mu)$ (where $(X, \mu)$ is a measure space containing infinitely many disjoint measurable sets of positive measure) is not equivalent to a norm generated by an inner product (unless $p = 2$ ). I have trouble with this exercise. I don't know what to start from. In previous exercises I showed that the norm on the spaces $(\Bbb C^n, ∥ · ∥_p)$ , $\ell_p$ , $(C [a, b], ∥ · ∥_p)$ , $L_p (X, \mu)$ is not generated by an inner product (unless $n = 1$ , $p = 2$ ). I also generalized the inequality of the parallelogram on $n$ vectors. I think it's necessary. It is possible to construct many different inner products, but how to show non-equivalence?","['functional-analysis', 'normed-spaces', 'banach-spaces', 'measure-theory']"
2471765,"What are the topologies over $X$ where $\forall A \subseteq X$ we have $A$ or $X\setminus A$ is open but not both, except $X=A$ or $X=\varnothing?$","My question is really similar to the definition of ultrafilters. For finite cases all ultrafilters are generated by one element, we can use them to construct topologies satisfying the conditions easily. But with topologies we have more freedom and the proof does not work. Are there other examples? I checked for sets with elements up to 4, but only the ultrafilters or their complement works. Also if the topic has some literature I am interested in the infinite cases too. I can construct nontrivial ultrafilters, so I know there are other examples.","['general-topology', 'filters', 'discrete-mathematics']"
2471801,Coset and set of all cosets,"Let $C$ be a subspace of a vector space $V$ and $x \in V$.
The set
$x + C = \{ x + c : c \in C \}$ is called a coset of $C$.
The set of all cosets is denoted $V/C$. Can you help me with some examples and intuition behind the concept of coset? 
I find hard to grasp the real meaning of this definition.",['linear-algebra']
2471877,Commutator of laplacian and covariant derivative of a tensor,"The laplacian of a tensor $T$ is the tensor defined by
$$ \Delta T = g^{ij} \left(\nabla_{\partial_i} \nabla_{\partial_j} T - \nabla_{\nabla_{\partial_i} \partial_j} T\right)=g^{ij} \nabla^2_{ij} T. $$
I would like to calculate the following commutator
$$\nabla \Delta T - \Delta \nabla T,$$
for tensors of type $(n,0)$ (i.e. input: $n$ vectors and output: a real number). With $\Delta \nabla T$ I mean the laplacian of the $(n+1,0)$-tensor $\nabla T$.
I have attempted to calculate it many times, but I struggle with taking multiple covariant derivatives and interchanging them. From the liturature I suspect the end result takes the form of a contraction with $T$ and the curvature tensor. I would greatly appreciate if anyone can show the calculation (in normal coordinates is fine) for this or can give me a reference for this since I can't find any.","['tensors', 'riemannian-geometry', 'differential-geometry']"
2471905,Geodesic sphere using only Regular pentagons and hexagons,"I know geodesic approximation to a construct a spherical dome shape needs
12 pentagons and these pentagons are regular pentagons. However when I look closely hexagons are slightly different in their shapes and sizes. Is it mathematically possible to construct a geodesic sphere using 12 pentagons and REGULAR hexagons of the SAME SIZE?  (for example like Truncated Icosahedron) Would putting extra pentagons to force curvature between the regular hexagons solve this? https://upload.wikimedia.org/wikipedia/commons/7/72/Goldberg_polyhedron_6_5.png",['geometry']
2471947,In parallelogram $ABCD$ there is a point $P$ inside it,"There is a parallelogram $ABCD$ and a point $P$ inside it, where $\lvert CP\rvert=\lvert CB\rvert$. Is there a way to prove that a line connecting midpoint of $AP$ and midpoint of $DC$ is perpendicular to a line connecting points $B$ and $P$? Unfortunately I can't post inline pictures yet. Thanks Greg",['geometry']
2471995,Solving the cubic $x^3-x^2-2x+1 = 0$,"Solving the cubic $x^3-x^2-2x+1 = 0$. Using the Cubic Formula I get the following three solutions. $x_1 = \frac{1}{3} - \frac{1}{3}\left(\frac{7}{2} + \frac{21}{2}i\sqrt{3} \right)^{1/3} - \frac{7}{3}\frac{1}{\left( \frac{7}{2} + \frac{21}{2}i\sqrt{3} \right)^{1/3}} \cong -1.2469796037174670610+2.10^{-20}i$ $x_2 = \frac{1}{3} + \frac{1}{6}\left( \frac{7}{2}+\frac{21}{2}i\sqrt{3} \right)^{1/3}(1+i\sqrt{3}) + \frac{7}{6}\frac{1-i\sqrt{3}}{\left( \frac{7}{2} + \frac{21}{2}i\sqrt{3} \right)^{1/3}} \cong .44504186791262880859 - 3.10^{-20}i$ $x_3 = \frac{1}{3} + \frac{1}{6}\left( \frac{7}{2}+\frac{21}{2}i\sqrt{3} \right)^{1/3}(1-i\sqrt{3}) + \frac{7}{6}\frac{1+i\sqrt{3}}{\left( \frac{7}{2} + \frac{21}{2}i\sqrt{3} \right)^{1/3}} \cong 1.8019377358048382525 + 3.10^{-20}i$ It is clear that the three solutions are all real solutions, but is there a way I can remove the complex components algebraically? My ultimate goal is to describe what the Galois group from this polynomial would like.","['galois-theory', 'polynomials', 'roots', 'trigonometry', 'cubics']"
2472017,How to find the general solution of a system of ODEs?,"I am trying to understand an example of Lawrance Perko, the problem being 
\begin{align}
\dot{x}_1 &=-x_1-3x_2\\
\dot{x}_2 &=2x_2
\end{align} Which can be written in the form $\dot{x}=Ax$ where : \begin{align}
A=
\begin{bmatrix}
-1 & -3 \\
0  & 2
\end{bmatrix}
\end{align} The eigen values of A are $\lambda_{1}=-1$,$\lambda_{1}=2$ and the eigenvectors are $v_1=[1,0]$ and $v_2=[-1,1]$ respectively. The matrix P and the decoupling matrix $P^{-1}$ are given by,
\begin{align}
P=\begin{bmatrix}
1 &-1 \\
0 & 1
\end{bmatrix},P^{-1}=
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
\end{align} Under the co-ordinate transform $y=P^{-1}x$ the linearly uncoupled system is given by: \begin{align}
\dot{y_1}=-y_1\\
\dot{y_2}=2y_2
\end{align} Which has the general solution $y_1=c_1\exp(-t)$, $y_2=c_2\exp(2t)$. Now the general solution of the original system is iven by:
\begin{align}\tag{1}
x(t)=P\begin{bmatrix}
e^{-t} & 0 \\
0 & e^{2t}
\end{bmatrix}P^{-1}c
\end{align} Where $c=x(0)$, or equivalently by What I don't understand is the last step. According to my calculation $y=P^{-1}x$ so $x_1=y_1-y_2$, $x_2 = y_2$. So $x_1$ should be $c_1 e^{-t}-c_2e^{2t}$ and not $c_1 e^{-t}+c_2(e^{-t}-e^{2t})$ \ref{1}","['dynamical-systems', 'ordinary-differential-equations', 'linear-algebra']"
2472061,Inverse Laplace transform of $\frac{1}{s^b-c}$.,"I am trying to find the inverse Laplace transform of $$F(s) = \frac{1}{(1+a\,s)^b-c}$$ where $a$ , $b$ , and $c$ are positive real numbers. For $c=0$ , we can use the following: $$\mathcal L^{-1}\left\{\frac1{s^b}\right\}=\frac{x^{b-1}}{\Gamma(b)}, \qquad for\quad b>0$$ Then we
have $$\mathcal L^{-1}\left\{\frac1{(1+a s)^b}\right\}
=\frac1{a^b}\mathcal L^{-1}\left\{\frac1{(\frac1{a}+s)^b}\right\}
=\frac{e^{-\frac{x}{a}}}{a^b}\mathcal L^{-1}\left\{\frac1{s^b}\right\}
=\frac{e^{-\frac{x}{a}}}{a^b}\frac{x^{b-1}}{\Gamma(b)}.$$ Now, what do I do with $c\neq 0\,$ ? This simplifies the question to: What is the inverse Laplace transform of $F(s)$ given by: $$\frac{1}{s^b-c}?$$","['laplace-transform', 'signal-processing', 'complex-analysis', 'contour-integration', 'special-functions']"
2472095,Simplifying Quotient of Tensor Products,"Consider $$(A\otimes C)/(B\otimes C)$$ where $B$ is a submodule of $A$. ($A,B,C$ are $R$-modules). Is it true that $$(A\otimes C)/(B\otimes C)\cong(A/B)\otimes C$$? Thanks. If no, are there any easy counter-examples?","['abstract-algebra', 'tensor-products']"
2472103,Inductive proof for recursive formula,"So, I have a recursion in which $$a_0 = 5$$ $$a_1 = 1$$ $$a_{n+2} = a_{n+1} + 2a_n$$ I should then prove by induction that the formula $a_n = 2^{n+1} + 3(-1)^n$ works for every number. Anyway, I generally know how to use induction as proof but doesn't really have a clue on how to use it when it comes to a recursive formula. I have tried to prove the base case but don't really know how to use $k+1$ to my advantage afterwards since I'm not sure how to represent $n = k$ and then that it works for every $n$ by $k + 1$.","['recurrence-relations', 'exponential-function', 'induction', 'sequences-and-series', 'discrete-mathematics']"
2472158,How to generalize the concept of differentiation to higher dimensions?,"I am studying the chapter 'Several Variables' from Rudin's ""Principles of Mathematical Analysis"". There I found a new conception of derivative which generalizes the notion of derivatives to higher dimensions. He explained it as follows which I write here in my own languange of understanding $:$ Suppose $f:(a,b) \longrightarrow \mathbb R$ be differentiable function. Then for each $x \in (a,b)$, $f'(x)$ exists. Instead of viewing $f'(x)$ for some $x \in (a,b)$ as a real number we may treat it as a linear transformation from $\mathbb R$ into $\mathbb R$. Because for every real number $\alpha$ we can associate a linear operator $T_{\alpha}$ on $\mathbb R$ defined by $T_{\alpha} (x) = \alpha x$, $x \in \mathbb R$. Conversely, any linear operator $T$ on $\mathbb R$ can be defined as $T(x)= \alpha x$ for some $\alpha \in \mathbb R$ (we may take $\alpha=T(1)$). Hence there exist a $1-1$ correspondence from $\mathbb R$ onto $L(\mathbb R)$. Also it can be easily checked that this kind of correspondence is linear. So it's an isomorphism. Therefore we may identify $f'(x)$ by the linear operator $T_{f'(x)}$ on $\mathbb R$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. Similarly if $f : (a,b) \longrightarrow \mathbb R^m$ is a differentiable function we then find an isomorphism from $\mathbb R^m$ onto $L(\mathbb R, \mathbb R^m)$ and hence instead of identifying $f'(x)$ for some $x \in \mathbb R$ as a vector in $\mathbb R^m$ we may simply identify it as a linear transformation $T_{f'(x)}$ from $\mathbb R$ to $\mathbb R^m$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. But for higher dimensions I fail to relate this concept. Suppose $f : E \subset_{open} \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable on $E$ then how can I find the $1-1$ correspondence which is linear? I have thought about it for at least half an hour but couldn't find any satisfactory answer. Please help me at this point. Thank you in advance.","['multivariable-calculus', 'derivatives']"
