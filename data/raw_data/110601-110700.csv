question_id,title,body,tags
1593365,A $T_0$ topological vector space is Hausdorff,"I know a $T_0$ topological group is $T_1$, and if we have a topological vector space, there should be a way of using scalar multiplication to get disjoint neighborhoods. Right?","['general-topology', 'topological-vector-spaces']"
1593384,What is the difference between an indexed family and a sequence?,"For indexed family wikipedia states: Formally, an indexed family is the same thing as a mathematical function; a function with domain J and codomain X is equivalent to a family of elements of X indexed by elements of J For sequence wikipedia states: Formally, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers. Both can have repeated elements and their order matters. Is there any difference between this two?","['elementary-set-theory', 'definition']"
1593385,Maclaurin Series Representation for $f(z)=\frac{z}{z^4+9}$,"I need help finding the Maclaurin series representation for $$f(z)=\frac{z}{z^4+9}$$ I first tried to factorize $z^4+9$, but am I missing something? I could not figure out how to factorize this. Is there another approach to this? I am open to any approach. Just help me figure this out through factorizing if possible.","['complex-analysis', 'taylor-expansion', 'sequences-and-series', 'calculus']"
1593431,Is there a mathematical proof that shows all multiples of $5$ either end with a $0$ or $5$?,I know that all multiples of $5$ end up with a $0$ or $5$ as the last digit. But there are an infinite amount of numbers. Is there a way to formally prove that this is true for all numbers using variables?,['sequences-and-series']
1593437,Is it possible to prove the Fundamental Theorem of Algebra for all polynomials of degree $n \le 4$?,"Recenly I've been wondering whether it's possible to prove the FTA for all polynomials of degree $n \le 4$ without utilizing advanced maths but, at most, basic linear algebra (concepts such as eigenvectors, eigenvalues, determinants etc.). I've tried to give this some thought but I've been only able to make very naïve and futile statements such as ""all real numbers can be represented as complex numbers "" and ""some quadratics and quartics, such as $p(x)=x^2 + 1$ and $p(x)=x^4 -1$, contain complex solutions"". Now, I would be really greatful, if it is indeed possible to prove the FTA for all polynomials with degree $n \le 4$, if someone could provide a reference of some sort.",['linear-algebra']
1593455,With the exception of the Total Derivative; Is the Chain Rule valid when it has partial derivatives mixed with ordinary derivatives?,"I know that the chain rule for a function of one variable $y=f(x)$ is written as $$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}\tag{1}$$ I also know that if $u=f(x,y)$ then the total derivative is $${\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y$$ But from $(1)$ is there any plausibility in changing all the derivatives to partial derivatives such that $$\frac{\partial}{\partial x}=\frac{\partial}{\partial y}\times \frac{\partial y}{\partial x}$$ when $y=f(x)$ or does the above formula only hold iff $y=f(x,y)$? To illustrate my confusion I will add this question and solution to give some context: Start of Question: If $f$ is an arbitrary function, show that $\psi(z,t)=f(z-vt)$ is a solution to the wave equation $$\frac{\partial^2\psi}{\partial t^2}=v^2\frac{\partial^2\psi}{\partial z^2}$$ by writing $f(z-vt)$ as $f(y)$ and using the chain rule to differentiate with respect to $t$ and $z$. End of Question. The following is a word for word copy of the solution. I have marked $\color{red}{\mathrm{red}}$ the parts of the solution for which I do not understand and the parts marked with $\color{#180}{\text{green underbraces}}$ are not part of the solution and represent where I think the author has made mistakes: Start of Solution: The wave equation is $$\frac{\partial^2\psi}{\partial t^2}-v^2\frac{\partial^2\psi}{\partial z^2}=0$$
  Writing $$f^{\prime}(y)=\frac{{\rm d}f(y)}{{\rm d}y}$$ and 
  $$f^{\prime\prime}(y)=\frac{{\rm d}^2f(y)}{{\rm d}y^2}$$ then writing $y=z-vt$, $$\frac{\partial f(y)}{\partial z}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime}(y)\tag{A}$$ and 
  $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}f^{\prime}(y)}{{\rm d}y} \underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime\prime}(y)$$ Similarly, $$\frac{\partial f}{\partial t}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}t}}}_{\color{#180}{\Large\frac{\partial y}{\partial t}}}=-vf^{\prime}(y)$$ and 
  $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}\left(-v f^{\prime}(y)\right)}{{\rm d}y}\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d} z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=v^2f^{\prime\prime}(y)$$
  Substituting into the LHS of the wave equation gives $$\frac{\partial^2 f(z-vt)}{\partial z^2}-\frac{1}{v^2}\frac{\partial^2 f(z-vt)}{\partial t^2}=f^{\prime\prime}(z-vt)-\frac{1}{v^2}\left(v^2f^{\prime\prime}(z-vt)\right)=0$$ End of Solution. I have three questions regarding the solution above: Are the parts marked in $\color{#180}{\text{green underbraces}}$ correct? I think they should be partial derivatives as $y$ is a function of two variables ($z$ and $t$) since $y=z-vt$. Iff the $\color{red}{{\rm red}}$ text turns out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}z}$$ How can this possibly be true? Since we have a partial derivative on the LHS and two ordinary derivatives on the RHS. I didn't know that you could 'mix' up derivatives like that. If this is indeed true could someone please explain it to me? Iff the $\color{#180}{\text{green underbraces}}$ turn out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d} y} \times \frac{\partial y}{\partial z}$$ But this means that the LHS is partial while the RHS is a mixture. Either way round, I don't understand why the partial derivatives can be mixed with ordinary derivatives. Is anyone able to explain why you can mix partial and ordinary derivatives in the chain rule? Feel free to answer using examples if it's easier to explain that way.","['derivatives', 'partial-derivative', 'chain-rule', 'calculus', 'proof-explanation']"
1593466,"Find $a,b$ for which $xyz+z=a,\quad xyz^2+z=b,\quad x^2+y^2+z^2=4$ has unique solution","Find all values of $a,b$ for which the system of equations $xyz+z=a,\quad xyz^2+z=b,\quad x^2+y^2+z^2=4$ has only one real solution. $xyz+z=a$ $xyz^2+z=b$ So,$\frac{xy+1}{xyz+1}=\frac{a}{b}$ I can think no method by so as this system of equations has only one real solution. What should I do? Please help me.","['algebra-precalculus', 'systems-of-equations']"
1593476,"Calculate $\int_0^{1}\frac{x^{-1 - x}\,\,\,\left(1 - x\right)^{x - 2}}{\mathrm{B}(1 - x\,, \,x)}\,\mathrm{d}x$","How does one calculate
$\displaystyle{\int_0^{1}\frac{x^{-1 - x}\,\,\,\left(1 - x\right)^{x - 2}}
{\mathrm{B}(1 - x\,,\,x)}\,\mathrm{d}x}$ ?. The observation
$\displaystyle{\int_0^{1}\frac{x^{-1 - x}\,\,\,\left(1 - x\right)^{x - 2}}{\Gamma\left(1 - x\right)\Gamma\left(x\right)}\,\mathrm{d}x =
\int_0^{1}\frac{x^{-1 - x}\,\,\,\left(1 - x\right)^{x - 2}}
{\mathrm{B}(1 - x\,,\,x)}\,\mathrm{d}x}$ seems useless here. The answer, according to wolfram , is $2$.","['integration', 'definite-integrals']"
1593480,Applications of the Lawvere Fixed Point Theorem for Sets,"I'm not familiar with the general theorem for closed, cartesian categories (as I'm not familiar with closed, cartesian categories), but I am aware of this version of the fixed point theorem for sets: Let $A, B$ be sets. If there exists a surjective function $f: A \longrightarrow \textbf{Set}(A,B)$, then every function $g:B \longrightarrow B$ has a fixed point. I'm aware of two applications of the theorem: proving Cantor's theorem (setting $B=\{0,1\}$) and proving that $[0,1]$ is uncountable (setting $A=\mathbb{N}$, $B= \{0,1\}$, looking at binary representations). Are there any other neat applications of this fixed point theorem for sets? For instance, can we deduce the Tarski fixed point theorem for sets (every non-decreasing endofunction of a power set of a set has a fixed point) or the Cantor-Bernstein theorem using this theorem? Just thought this result was cool, and wanted to see what other things you could do with it. Thanks in advance for any replies!","['category-theory', 'elementary-set-theory']"
1593487,Is $\mathbb{R}$ a subset of $\mathbb{R} \times \mathbb{R}$? [duplicate],"This question already has answers here : Is $\mathbb{R}$ a subset of $\mathbb{R}^2$? (3 answers) Closed 8 years ago . So I'm curious as why $\mathbb{R} \subseteq \mathbb{R}^{2}$, since $\mathbb{R}^{2} = \mathbb{R} \times \mathbb{R} = \left \{ (a,b) \mid a\in \mathbb{R}, b\in \mathbb{R} \right \}$. Do we think of $\mathbb{R}$ as being $\mathbb{R} \times 0$ in this case?",['elementary-set-theory']
1593492,"Find all $(x,y)$ satisfying $(\sin^2x+\frac{1}{\sin^2 x})^2+(\cos^2x+\frac{1}{\cos^2 x})^2=12+\frac{1}{2}\sin y$","Find all pairs $(x,y)$ of real numbers that satisfy the equation $(\sin^2x+\frac{1}{\sin^2 x})^2+(\cos^2x+\frac{1}{\cos^2 x})^2=12+\frac{1}{2}\sin y$ I supposed $a=\sin^2x$ and $b=\cos^2x$ So the equation becomes $(a+\frac{1}{a})^2+(b+\frac{1}{b})^2=12+\frac{1}{2}\sin y$ As $a+\frac{1}{a}\geq 2$ and $b+\frac{1}{b}\geq 2$ $12+\frac{1}{2}\sin y\geq 8$ $\sin y\geq -8$ I am stuck here.I could not solve further.Please help me.Thanks.","['algebra-precalculus', 'trigonometry', 'systems-of-equations']"
1593497,Negative divergence implies convergent flow?,"Suppose we have a differentiable vector field $X:\Omega\to\mathbb{R^n}$ defined on an open, bounded and simply connected region subset $\Omega$ of $\mathbb{R^n}$, and its divergence is negative everywhere, i.e. $\nabla\cdot X(x)<0$ for any $x\in\Omega$. Can we prove that any two solution trajectories will evolve closer and closer, and thus eventually convergent? Formally, given any two points $p,q\in\Omega$ and flows $p(t), q(t)$ satisfying $p(0)=p,q(0)=q,\dot p(t)=X(p(t)), \dot q(t)=X(q(t))$, define $f(t)=||p(t)-q(t)||^2$, then is it true that
$$\frac{d}{dt}f(t)<0$$ Intuitively, I think it is right because negative divergence implies any closed area will evolve smaller and smaller. So if we enclose two points with a thin tube, then the volume of this tube will get smaller and smaller, which forces points get closer. Here is my try:
$$\frac{d}{dt}f(t)=2\left<p-q|X(p)-X(q)\right>=2\left<p-q|\nabla X(r)|p-q\right>$$
The first equation follows from the definition and the second from mean value theorem(though this theorem doesn't exist). But this seems the statement requires more rigid condition, say positiveness of $\nabla X$, to guarantee the corectness.","['dynamical-systems', 'vector-fields', 'fluid-dynamics', 'differential-geometry']"
1593502,Solving $a\sin \theta + b\cos \theta + c\sin 2\theta + d\cos 2\theta = k$,"I have to solve (for $\theta$) an equation of the form: $$a\sin \theta + b\cos \theta + c\sin 2\theta + d\cos 2\theta = k$$ I'm only interested in real-valued solutions where $0 ≤ \theta ≤ \frac\pi4$, if one exists, and also knowing if none exist.
Also, $a$, $b$, $c$, $d$, and $k$ are rational numbers. Is there an ""easy"" way to attack this problem? The only strategy I could come up with to express all sines and cosines
as $\sin \theta$, and then square the equation to get rid of square roots, and then solve a quartic equation, and then check the legitimacy of the roots.
Is there an easier approach, perhaps one that is more customized to this problem?","['algebra-precalculus', 'trigonometry']"
1593527,How to prove that there does not exist a sequence of continuous functions that converge pointwise to $\chi_{\mathbb{Q}}$ (definition only),"A fellow member of the community asked :
""there isn't a sequence of continuous function on $[0,1]$ that converges pointwise to the function $f$ on $[0,1]$ defined by $f(x)=0$ if $x$ is rational and $f(x)=1$ if $x$ is irrational."" This is the reverse of $\chi_{\mathbb{Q}}$, but the proof is obviously the same. I'm wondering about a solution that only uses the definition of continuity and point-wise convergence. I answered the OP with the following attempt: Attempt 1: ""
Suppose that $\{f_{n}\}_{n=1}^{\infty}$ is a point-wise convergent sequence that converges to $f$. Let $x \in [0,1]$ and WLOG, suppose that $x$ is irrational. Let $\epsilon>0$. WLOG, suppose that $\epsilon<1$. 1) Then there exists $N_{1} \in \mathbb{N}$ such that $b \in \mathbb{N}$ and $b>N_{1}$ imply that  $|f_{b}(x)-f(x)|=|f_{b}(x)-1|<\frac{\epsilon}{2}$. 2) Since $\{f_{n}\}_{n=1}^{\infty}$ is a sequence of continuous functions, there exists some $\delta>0$ such that $|x-y|<\delta$ implies that $|f_{b}(y)-f_{b}(x)|<\epsilon$ for each $y \in [0,1]$. Let $y \in [0,1]$ such that $|x-y|<\delta$. Then choose $p$ such that  $p \in \mathbb{Q}$, where  $x<p<y$. 4) Since  $\{f_{n}\}_{n=1}^{\infty}$ is pointwise convergent, there exists $N_{2} \in \mathbb{N}$ such that $a \in \mathbb{N}$ and $a>N_{2}$ imply that  $|f_{a}(p)-f(p)|=|f_{a}(p)|<\frac{\epsilon}{2}$. 5) Let $N=\max\{N_{1},N_{2}\}$. Suppose that $n>N$. By hypothesis, $|f_{n}(p)|<\frac{\epsilon}{2}$ and $|f_{n}(x)-1|<\frac{\epsilon}{2}$. However, by (3), we know that  $|f_{n}(x)-f_{n}(p)|<\epsilon$. But this is clearly a contradiction."" However, this doesn't work , since it assumes that the same $\delta$ will characterize continuity for any $f_{n}$ where $n>N$. **as a side-remark, is there any literature on the idea of a single $\delta$ working to describe continuity in a sequence of point-wise convergent functions $f_{n}$ for sufficiently large $n$? Attempt 3: sigh , here is  another proof that ultimately rests on baire category for contradiction: Let $\{f_n(x)\}$ be a sequence of continuous functions that converge pointwise. We show that $A=\{x \in \mathbb{R} \mid f_n(x) \textrm{converges}\}$ is a Borel set [and more specifically, $G_\delta$. If $x \in A$, then there exists $N \in \mathbb{N}$ so that $n,m \geq N$ implies that $V_{x,n,j}=|f_n(x)-f_m(x)|<\frac{1}{j}$ for each $j \in \mathbb{N}$. Then Let $V_{n,j}=\bigcup V_{x,n,j}$. Since $V_{n,j}$ is open, let the open set $U_{n,j}$ be defined by $U_{N,J}=F^{-1}(V_{n,j})$. But then
$$\bigcap_{j \in \mathbb{N}} \bigcup_{N \in \mathbb{N}} \bigcap_{n,m \geq N} U_{n,j}=A$$
 is $G_\delta$ Then use Baire category to show that $\mathbb{Q}$ is not $G_\delta$, and so the result follows. I'm still interested in a proof that does not use this, and also in a possible proof verification for my proof given in the answers section.","['real-analysis', 'sequences-and-series', 'functions', 'continuity', 'convergence-divergence']"
1593553,$p(x)$ irreducible polynomial $\iff J=\langle p(x)\rangle$ is a maximal ideal in $K[x]$ $\iff K[x]/J$ is a field,"Given a field $K$ and $p(x)\in K[x]$ . Then the following conditions are equivalent: a) $p(x)$ is irreducible over $K$ . b) $J = \langle p(x)\rangle$ is a maximal ideal in $K[x]$ . c) $K[x]/J$ is a field, where $J=\langle p(x)\rangle$ . My book proves $a\implies b$ as follows: Since the degree of $p(x)$ is greater than or equal to $1$ , we have that $J\neq K[x]$ (why the degree?). If $I=\langle h(x)\rangle$ is an ideal of $K[x]$ such that $I$ contains $J$ , let's prove that $J$ contains $I$ . For that look at the following: $$p(x)\in \langle p(x)\rangle \subseteq \langle h(x)\rangle \implies p(x) = g(x)h(x)$$ for some $g\in K[x]$ . Since $p(x)$ is irreducible, we must have: $$g(x) = a\in K - \{0\}$$ or $$h(x) = b\in K - \{0\}.$$ If $g(x)=a\neq 0$ we have $h(x) = a^{-1}\cdot p(x)\in J$ (because $h(x)$ is now $p(x)$ with some constant), and therefore $I$ is contained in $J$ , so we have $I=J$ , therefore $J$ is maximal ideal of $K[x]$ . Now, an unexpected proof of $a\implies c$ appears in the middle of this proof. If we consider the case $h(x)=b\neq 0$ we have ( and this part I didn't understand ) $I = \langle h(x)\rangle = K[x]$ and this proves $a\implies c$ . I think I didn't understand this part because I'm having trouble visualizing $K[x]/J$ where $J = \langle p(x)\rangle$ . In another example, the book actually computes the quotient $A/I$ where $A = \mathbb{R}[x]$ and $I = \langle x^2+1\rangle$ . It uses the theorem above to say that $L = A/I$ is a field since $x^2+1$ is irreducible over $K[x]$ (shouldn't it be $\mathbb{R}[x]$ ?). Then, it computes $L$ as follows: $$p(x) = q(x)(x^2+1)+r(x)$$ where $r(x) = bx+a$ . It then takes $p(x)\bmod I$ which gives the following: $p(x) = q(x)(x^2+1)+r(x)=r(x)= bx + a$ (everything above is with that bar at the top but I don't know how to write it in LaTeX). Then, the book simply says that $L = \{bx + a: a,b \in \mathbb{R}\}$ . Why is it that $L$ is the $p(x) \bmod I$ ? What am I missing about quotient rings? Can somebody clarify it for me?","['irreducible-polynomials', 'abstract-algebra', 'polynomials']"
1593591,Find the value of $\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+\sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+...+\sqrt{1+\frac{1}{1999^2}+\frac{1}{2000^2}}$ [duplicate],This question already has answers here : A pencil approach to find $\sum \limits_{i=1}^{69} \sqrt{\left( 1+\frac{1}{i^2}+\frac{1}{(i+1)^2}\right)}$ (3 answers) How to find the sum of this : $\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+ \sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+\sqrt{1+\frac{1}{3^2}+\frac{1}{4^2}}+.....$ (3 answers) Closed 8 years ago . Find the value of $\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+\sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+...+\sqrt{1+\frac{1}{1999^2}+\frac{1}{2000^2}}$ I found the general term of the sequence. It is $\sqrt{1+\frac{1}{k^2}+\frac{1}{(k+1)^2}}$ So the sequence becomes $\sum_{k=1}^{1999}\sqrt{1+\frac{1}{k^2}+\frac{1}{(k+1)^2}}$ I tried telescoping but i could not split it into two partial fractions.And this raised to $\frac{1}{2}$ is also troubling me.What should i do to find the answer?,['sequences-and-series']
1593602,Set of discontinuities of a function that has both limits at each point of $R$ [duplicate],"This question already has an answer here : Can we construct a function that has uncountable many jump discontinuities? (1 answer) Closed 8 years ago . $f:\mathbb R\rightarrow \mathbb R$ has both a left limit and a right limit at each point of $\mathbb R.$ Then the number of discontinuities of $f$ is what $?$ Now the G reatest I nteger F unction is one such function and has countable infinite discontinuities. Any continuous function  also has both limits at each point and they are same and number of discontinuity is $0.$ So , is there  a function with both limits at each point that has number of discontinuities uncountable $?$ Is that possible $?$","['continuity', 'real-analysis']"
1593640,"Differentiation of power series, problem","I have the power series $$u(x) = \sum_{k=1}^{\infty} \frac{x^{2k+1}}{k(2k+1)} $$ with radius of convergence $R \geq 1 $ and I want to perform termwise derivation for $|x| \lt 1$, but it isn't working for me. Normally, I would rewrite $u(x)$ so that it is defined for $ k=0,1,2,...$ instead of $k=1,2,3,...$, and then use that $$u(x) = \sum_{k=0}^{\infty} a_k x^k \Rightarrow u'(x) = \sum_{k=1}^{\infty} ka_k x^{k-1}$$ but in this case that gives me $$u(x) = \sum_{k=0}^{\infty} \frac{x^{2k+3}}{(k+1)(2k+3)} \Rightarrow u'(x) = \sum_{k=1}^{\infty} \frac{kx^{2k+2}}{(k+1)(2k+3)}.$$ The answer is supposed to be $$u'(x) = \sum_{k=1}^{\infty} \frac{x^{2k}}{k}$$ but as you can see, I'm nowhere near close. What am I doing wrong?","['derivatives', 'power-series', 'sequences-and-series']"
1593644,Computing alternating sum using contour integration,"By considering the integral of: $$\left(\frac{\sin\alpha z}{\alpha z}\right)^2 \frac{\pi}{\sin \pi z},\quad \alpha<\frac{\pi}{2}$$ around a circle of large radius, prove that: $$\sum\limits_{n=1}^\infty (-1)^{m-1} \frac{\sin ^2 m\alpha}{(m \alpha)^2} = \frac{1}{2}$$ Attempt at answer: I can see that I have poles at $z=n$ , and a double pole at $z=0$ .
So in order to perform the contour integration, I first find the residues for $z=n$ : $$\frac{\pi}{\sin\pi z} = \frac{1}{z} \left( 1 + \frac{(\pi z)^2}{3!} + ...\right)$$ the $1/z$ part is equal to $1$ , so the residues are $$\sum\limits_{n=-N}^N \frac{\sin ^2 n\alpha}{(n \alpha)^2}$$ Next, finding the residue at $z=0$ : I found it to be $=1$ , by series expansion. I know that if I let my function tend to zero as the contour encloses all the poles, I have that: $$2\pi i \left(2 \sum\limits_{n=1}^\infty \frac{\sin ^2 n\alpha}{(n \alpha)^2} + 1\right) = 0$$ So I'm very almost there - but I have no idea where the $(-1)^{m-1}$ factor comes from, and also - if I rearrange the last equation, I get that the sum is $-\frac{1}{2}$ (i.e. not positive). If anyone could help find where I'm going wrong, that would be great!","['complex-analysis', 'contour-integration', 'sequences-and-series', 'complex-integration']"
1593653,number of ways of choosing $r$ points from $n$ points arranged in a circle such that no consecutive points is taken. [duplicate],"This question already has an answer here : Number of ways to select $k$ non-adjacent points from a circle of $n$ points is $\binom{n-k-1}{k-1}\cdot \frac{n}{k}$. (1 answer) Closed 12 months ago . number of ways of choosing $r$ points from  $n$ ponts arranged in a circle such that no consecutive points is taken. (I have seen some question on SE related to this. But I am trying to solve it using my own method). Let the points be taken in a straight line $\{A_1,A_2,...,A_n\}$. I have to choose $r$ points such that no two points are consecutive. Let the chosen points be represented by $\{x_1,x_2,...,x_r\}$. The number of ways of choosing such points is same as the number of ways of placing these $r$ points among $n-r$ objects such that none of the points $\{x_1,x_2,...,x_r\}$ come together. Since I am only talking about ""choosing"" points, I will assume all the points to be identical. This can be calculated by assuming gaps between the $n-r$ initial objects. There will be $n-r$ gaps since the leftmost gap and right most gap are same for a circular arrangement. First I will place $x_1$. The number of ways of placing the remaining $r-1$ points in $n-r-1$ gaps is $$^{n-r-1}C_{r-1}$$
(the points are assumed to be identical). Similarly, If I start with $x_2$, the number of ways will be $$^{n-r-1}C_{r-1}$$ So the total will be $$n\cdot^{n-r-1}C_{r-1}$$ But the answer given is $$\frac{n\cdot^{n-r-1}C_{r-1}}{r}$$",['combinatorics']
1593668,Proofs of Simplicity of $A_n$,"There are different proofs of simplicity of the group $A_n$, and one can get at least two proofs by choosing randomly 10 books of the subject, so I will not go into what are these proofs? Rather, I would consider, the original references of these proofs. Can one give links for original proofs among the different proofs, including that of Galois or Abel?","['reference-request', 'math-history', 'group-theory', 'simple-groups']"
1593674,$C$ be the curve of intersection of sphere $x^2+y^2+z^2=a^2$ and plane $x+y+z=0$ ; to evaluate $\int_C ydx + z dy +x dz$ by Stoke's theorem?,Let $C$ be the curve of intersection of the sphere $x^2+y^2+z^2=a^2$ and the plane $x+y+z=0$ ; how to evaluate $\int_C ydx + z dy +x dz$ by Stoke's theorem ? $C$  is a great circle I think ; I am  not able to get the surface $S$ ; Please help . Thanks in advance,"['multivariable-calculus', 'stokes-theorem', 'surface-integrals', 'line-integrals']"
1593680,Differentiation of $x^{{x}^{{x}^{...}}}$ [duplicate],"This question already has answers here : derivative of x^x^x... to infinity? (2 answers) Closed 8 years ago . All of us experienced with this function :
$$x^{x}$$
It's easy to differentiate it, so my question is:
How to differentiate this function:
 $$x^{{x}^{...}}$$, maybe there is some recurrence equation?
For example there is $n$, $x$ in ""stairs"".
P.S. sorry for my English skills","['real-analysis', 'ordinary-differential-equations']"
1593686,Simple question on trigonometry identities of sec and tan [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Please, I want to know different methods to prove following identity $$\frac{\tan \theta + \sec\theta - 1}{\tan\theta-\sec\theta + 1}=\frac{1+\sin\theta}{\cos\theta}$$",['trigonometry']
1593714,Limit of $\sin(1/x)$ - why there is no limit?,"$$ \lim_{x\to 0+} \sin\left(\frac{1}{x}\right)$$
I know that there is no limit. but, why there is no limit?
I tried $x=0.4$, $x=0.3$, $x=0.1$, it looks like the limit is $0$. And how can I show that there is no limit? I tried to calculate it like all the other functions, and I got wrong result and I don't know why: $$\lim_{x \to 0+} \sin\left(\frac{1}{x}\right) = \sin\left(\frac{1}{0^+}\right) =  \sin\left(\frac{1}{\infty}\right) = \sin(0) = 0.$$","['calculus', 'limits']"
1593731,A formula which gives the maximum of a series of numbers,"This formula gives the maximum of 3 numbers: $$\frac{a}{2} + \frac{b}{4} + \frac{c}{4} + \frac{|b-c|}{4} + \frac{1}{2}\left|a -\frac{b}{2} - \frac{c}{2} - \frac{|b-c|}{2}\right| = \max(a,b,c)$$ I've found this over the internet, I have no idea how can one develop such a formula, and I wonder how.
What would it look like for 4 numbers ? and 5 etc.
Is it possible to have formula which gives the maximum of a series of $n$ numbers?",['algebra-precalculus']
1593744,"Prove that something is an embedding, with and without using formulas","Show that, for any $n\geq1$ integer,
$$S^1\times...\times S^1\subset\mathbb{R}^2\times...\times\mathbb{R}^2=\mathbb{R}^{2n}$$
can be embedded in $\mathbb{R}^{n+1}$. 
I first have to prove it without using explicit formulas, then by explicit formulas. I find embeddings very difficult, I know the definition, but not how to work with it et cetera.. Who can help me?",['general-topology']
1593753,Lost on rational and Jordan forms,"I'm having a lot of trouble trying to understand rational canonical form, primary rational canonical form, and Jordan form. I've looked at the posts about this, but I haven't been able to understand those concepts. I've been given the following matrix which is associated to an endomorphism $\phi$ of $V=\mathbb{R}^{4}$ $$M=\begin{bmatrix}
 1 & 0 & 0 & -1\\
 -1 & 0 & -1 & 2\\
 2 & 2 & 2 & -2\\
 0 & 0 & 0 &  1\\ \end{bmatrix}$$ I have to compute the following: Invariant factors of $\phi$ and a base of $V$ for which the matrix associated to $\phi$ is the rational canonical form of $M$. Determine a basis of $V$ for which the matrix associated to $\phi$ is the primary rational canonical form of $M$. The real Jordan canonical form associated to $M$. Now it comes my try and my doubts: The characteristic polynomial is $(x-1)^2(x^2-2x+2)$ which has just one eigenvalue $(1)$ with algebraic multiplicity $2$. From here, it is easy to see that the minimal polynomial is $(x-1)^2(x^2-2x+2)$ (by multiplying matrixes). However, I've been told that I can also know the minimal polynomial by computing  $dim(Nuc(I-M))$ i.e. $(4-rank(I-M))$ and realising it is $1$ i.e. $(4-3)$, but I do not understand that. Therefore, the only invariant factor is the minimal polynomial, and then the rational canonical form is: $$C=\begin{bmatrix}
 0 & 0 & 0 & -2\\
 1 & 0 & 0 & 6\\
 0 & 1 & 0 & -7\\
 0 & 0 & 1 &  4\\ \end{bmatrix}$$ since $(x-1)^2(x^2-2x+2)= (x^4-4x^3+7x^2-6x+2)$. Now I'm afraid I don't know how to compute the basis I've been asked for. Also, in case that there were two invariant factors, for example, our minimal polynomial and $(x-3)$, I'd know how to write the rational canonical form, but I'm not sure whether the order of the companion matrixes matters. My trouble is quite the same here. I know that the primary rational canonical form is : $$R=\begin{bmatrix}
 0 & -1 & 0 & 0\\
 1 & 2 & 0 & 0\\
 0 & 0 & 0 & -2\\
 0 & 0 & 1 &  2\\ \end{bmatrix}$$ since the elementary divisors are $(x-1)^2$ and $(x^2-2x+2)$ but I'm not sure of the order. And once again, I don't know how to get the basis. Finally, $dim(Nuc(I-M))= 1$ as I just said, so there is 1 jordan block of size at least 1, and therefore, $dim(Nuc(I-M)^2)$ must be $2$ and we have one jordan block for the eigenvalue $1$ and it has size 2. The problem comes with the block(s) associated to $(x^2-2x+2)$. The jordan form should be 
  $$J=\begin{bmatrix}
 1 & 0 & 0 & 0\\
 1 & 1 & 0 & 0\\
 0 & 0 & 1 & -1\\
 0 & 0 & 1 &  1\\ \end{bmatrix}$$ but I don't know why. Sorry for the long post and thanks in advance.","['matrices', 'jordan-normal-form', 'polynomials', 'linear-algebra']"
1593764,About phi function,"Find all positive integers n such that $ \phi(n) |n $ I find  $n=2^k ,2^k ×3^j$ is answer ,I can't find another answers.","['number-theory', 'totient-function', 'elementary-number-theory']"
1593780,Solution of Non-linear ODEs,"I have a 3rd order non-linear differential equation. 
$$
y''' + k (y \cdot y''- y\,'^2+1) =0
$$
with boundary conditions
$$ 
\lim_{x \rightarrow \infty} \frac{y}{x}  =1 \\
y(0) = 0 \\
y'(0) = 0
$$
I know that it is difficult or may even be impossible to solve the ODE. But  is it correct to say these boundary conditions are sufficient ti solve the problem? Since, it is a 3rd order ODE and we have three boundary conditions, I think that these boundary conditions are sufficient (of course, assuming that boundary conditions are compatible with each other).",['ordinary-differential-equations']
1593784,How to determine the order of a differential equation when it's solution is given,We know how to find the solution of a given Homogeneous linear equations with constant coefficients . In assignment I have to find the minimum possible order of a Homogeneous linear equations with constant coefficients having $x^2 sin x$ as a solution. I am really blank and have no idea how to proceed. Kindly help me. Any hint or solution will be helpful to me. Thanks a lot for the help.,['ordinary-differential-equations']
1593789,Prove that the product of two continuous functions is continuous,"I was trying to prove an important theorem concerning continuous functions, namely that the product of two continuous functions is continuous. I am nearly at the end of the proof but I do not understand the last step my teacher made... I'm sure some of you is able to help me out :) Thanks in advance! Theorem Let $f, g\colon \Bbb R \to\Bbb R $ be continuous.
Then $F\colon \Bbb R\to\Bbb R$ defined by $F(x) = f(x)g(x)$ is continuous. Proof Let $f, g\colon \Bbb R \to\Bbb R $ be given such that $f$ and $g$ are continuous. Let $F\colon \Bbb R\to\Bbb R$ be defined by $F(x) = f(x)g(x)$. We want to show: $F$ is continuous, that is, for all $a \in\Bbb R$, for every $\epsilon > 0$, there exists some $\delta > 0$ such that for all $x \in \Bbb R$ with $|x - a| < \delta$, $|F(x)-F(a)| < \epsilon$. Let $a \in\Bbb R$ be given. Let $\epsilon > 0$ be given. Let $\delta_f > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_f$, $|f(x)-f(a)| < \frac\epsilon{2(|g(a)| + \epsilon)}$. Such a $\delta_f$ exists since $f$ is continuous. Let $\delta_g > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_g$, $|g(x)-g(a)| < \frac\epsilon{ 2|f(a)| + 1}$. Such a $\delta_g$ exists since $g$ is continuous. Let $\delta_3 > 0$ be such that for all $x\in\Bbb R$ with $|x-a| < \delta_3$, $|g(x)-g(a)| < \epsilon$, which implies $|g(x)| < |g(a)| + \epsilon$. Such a $\delta_3$ exists since $g$ is continuous. Choose $\delta = \min\{\delta_f, \delta_g, \delta_3\}$. Then for all $x \in\Bbb R$ with $|x-a| < \delta$: 
$$\begin{align}|F(x) - F(a)| &= |f(x)g(x) - f(a)g(a)| \\
&= |f(x)g(x) - f(a)g(x) + f(a)g(x) - f(a)g(a)| \\
&\le |f(x)g(x) - f(a)g(x)| + |f(a)g(x) - f(a)g(a)|\\
&= |(g(x)[f(x) - f(a)]| + |f(a)[g(x) - g(a)]|\\&= |g(x)| * |f(x)-f(a)| + |f(a)| * |g(x) - g(a)| \\
&< [|g(x)| + \epsilon] \cdot \frac \epsilon {2(|g(a)| + \epsilon)} + |f(a)| \cdot |g(x) - g(a)| \\
&= \frac\epsilon{2 + |f(a)| \cdot |g(x) - g(a)|}\end{align}$$ Problem Now we need $|f(a)| \cdot |g(x) - g(a)|$ to be equal to $\frac \epsilon 2$ so that we have $\frac\epsilon2+\frac\epsilon 2= \epsilon$ (which completes the proof).
The problem is I do not see why/how $|f(a)| \cdot |g(x) - g(a)|$ is equal to $\frac\epsilon2$. Or is there maybe an error somewhere?","['continuity', 'epsilon-delta', 'real-analysis']"
1593791,For which values of $a\in\mathbb{R} $ the equation $2 \log(x+3)=\log(ax)$ has exactly one root?,"I have to investigate the possible roots of the equation according to $a$, i.e. i have to see whether there is only one root, two roots, or no roots and also what their sign is each time. This is from a book i'm reading and the answer it gives is $a<0$ or $a=12$. Now first we have to to make the restriction that $x>-3$ due to logarithm. What is troubling me is the $a<0$ part. In the solution it says that after getting rid of the logarithms we have $x^2-(a-6)x+9$. Now if $a<0 \land x>0$, then $\log(ax)$ is undefined so the equation has no solution. But we're asked about $a$, not $x$. So it proceeds to say that if $a<0 \land x<0$, i.e. $-3<x<0$ (due to our restriction) then: Let $f(x)=x^2-(a-6)x+9$, so $f(-3)=3a<0$. [The following is primarily what i don't understand.] This means that $-3$ will be between the two roots $x_1$ and $x_2$, so $x_1<-3<x_2<0$, but since we have the constriction $-3<x<0$ then $x_1$ is not possible so $x_2$ is one and only root of the original equation. Thus for $a<0$ there is exactly one root. How could they even do that, plug $-3$ for $f(x)$ to test the roots of the equation, since for $x=-3$ the equation is not defined. Also what i'm not sure i understand is why it says that since $f(-3)<0$ then $x_1<-3<x_2$. I would like to understand that. Help please. Thanks in advance.","['algebra-precalculus', 'logarithms', 'functions']"
1593797,Lift of a curve in a principal bundle?,"Let $\pi:P\longrightarrow M$ be a $G$-principal bundle and $I:=[0, 1]$. Given a curve $\alpha:I\longrightarrow M$ and $p_0\in \pi^{-1}(\alpha(0))$ how can I show there is a curve $\beta:I\longrightarrow P$ such that $\pi\circ \beta=\alpha$ and $\beta(0)=p_0$? I thought defining $$\beta(t):=\phi_t^{-1}(\alpha(t), e_G),$$ where $\phi_t:\pi^{-1}(U_t)\longrightarrow U_t\times G$ are local trivializations with $\alpha(t)\in U_t$ for each $t\in I$. This satisfies $\pi\circ \beta=\alpha$ but I don't know how to get the condition $\beta(0)=p_0$. Thanks.","['principal-bundles', 'differential-geometry']"
1593807,Find a function that makes this differential form exact,"We have $\Omega=\mathbb{R^3}\backslash \left\{ (0,0,z):z\in \mathbb{R}\right\}$ and $\omega$ the differential form:
$$\omega :=\left(\frac{4x^2+2zx}{x^2+y^2}+2A(x,y)\right)dx+\left(\frac{2y}{x^2+y^2}(2x+z)\right)dy+A(x,y)\,dz,$$
where $A\in C^1(\mathbb{R^2}\backslash(0,0); \mathbb{R})$ Find a function $A$ that makes the differential form exact. My teacher advised to use the homotopy of the curves of which I make the integral. Namely find two curves on which the integral is $0$, one that surrounds the origin and the other doesn't, and hence obtaining all the other curves through continuous deformation. For the curves that don't include the $z$-axis I can just take a ball that contains the curve, and since the ball is convex, I know that a closed differential form in a open convex set is exact (I'm pretty sure it is closed). I'm stuck on the curves that surround the $z$-axis, any ideas?","['multivariable-calculus', 'differential-forms', 'differential-geometry', 'calculus']"
1593812,$C$ be curve of intersection of hemisphere $x^2+y^2+z^2=2ax$ and cylinder $x^2+y^2=2bx$ ; to evaluate $\int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz$,"$C$ be the curve of intersection of the hemisphere $x^2+y^2+z^2=2ax$ and the cylinder $x^2+y^2=2bx$ , where $0<b<a$ ; how to evaluate $\int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz$ using Stoke's theorem ? I can't parametrize the curve $C$ nor able to get the surface $S$ . Please help . Thanks in advance","['multivariable-calculus', 'stokes-theorem', 'surface-integrals', 'line-integrals']"
1593840,Faithful irreducible representation of a finite $p$-group,"I want to solve the following exercise in the representation theory field: A finite $p$-group $G$ has a faithful irreducible representation over an algebraically closed field whose characteristic is not $p$ if and only if the center of $G$ is cyclic. I can prove the above statement over the field of complex numbers (it is Theorem 2.32 of the book ""Character theory"" by M. Isaacs). Also, I can prove that if a finite group $G$ has a faithful irreducible representation over an algebraically closed field, then the center of $G$ is cyclic. So, I just want to prove that if a finite $p$-group $G$ has the cyclic center, then it has a faithful irreducible representation over an algebraically closed field whose characteristic is not $p$. I think that the proof that is mentioned in the book ""Character theory"", can be used for any fields, because I do not see any facts about $\mathbb{C}$ using in the proof. Actually, I don't know why the assumptions ""over an algebraically closed field whose characteristic is not $p$"" are necessary in this case. Thank you so much for your guidance.","['finite-groups', 'representation-theory', 'group-theory']"
1593845,Is finding the second derivative of $\sqrt[3]{\vert x\vert}$ the best method to determine if it is convex?,"I have an exercise where I have to tell on which intervals a function is concave or convex. I usually do it using second derivative, but I would like to know if there is a simpler way of doing so, because this  gets a little messy for me when considering $ \sqrt[3]{\vert x\vert } $.","['derivatives', 'absolute-value', 'convex-analysis', 'calculus']"
1593851,Is an injective map of $B$-modules also injective as an $A$-linear map if $B$ is an $A$-algebra?,"I've been going through my submitted exercises again of my Commutative Algebra -class and I have the following question: Let $A$ be a commutative ring with unity. Given any injective homomorphism of $B$-modules $M \rightarrow M'$, where $B$ is any $A$-algebra, is it necessarily true that $M \rightarrow M'$ is injective when considered as an $A$-linear map? In the solution I handed in, I used the rather weak argument that ""injectivity is a set-theoretic property, so stays preserved"" - which wasn't marked false by the assistant. However, the reasoning doesn't quite convince me. Is it true that injectivity stays preserved? If not, is there any counterexample? Thanks a lot!","['abstract-algebra', 'ring-theory', 'modules', 'functions']"
1593857,Solve this geometry problem without using any trigonometry,"Given: $ \triangle ABC $ $AC = BC$ $\angle C = 150$ $AB (base) = \sqrt{12}$ Find the radius of the Circumcircle. I have no idea how to solve this challenge, the answer which is given in the textbook is $2\sqrt{3}$ (which is the base). P.S. The textbook is for 8th grade, and in my country, sin and cosine law are learned in 9th grade(next year). So proofs without trigonometry are preferred.","['triangles', 'geometry']"
1593874,The inverse function,"The function $g :\mathbb{Q} → \mathbb{Q}$ is deﬁned by $g(r) = 4r + 1$ for each $r \in \mathbb{Q}$. (a) Determine $g(\mathbb{Z})$ and $g(E)$, where $E$ is the set of even integers. (b) Determine $g^{-1}(N)$ and $g^{−1}(D)$, where $D$ is the set of odd integers. This is the answer: I don't get why in (b)  $g^{-1}(N) = n/4$, i think it should be $(n-1)/4$, also for  $g^{-1}(D)$ Could any one explain this to me ?","['functions', 'inverse']"
1593902,New Elementary Function?,"In the February 2000 issue of FOCUS magazine, a short article suggests that the Lambert W function could be introduced into curriculum as a new elementary function saying: ""... a case can be made for according it equal respect with the traditional transcendentals of calculus."" As the inverse of $xe^x$, Lambert W is easy to understand, its properties are rather straight-forward, and it has found use in a wide range of applications. Are there other functions you think are good candidates to introduce more widely into mathematics curriculum that are interesting, easy to understand, and broadly applicable?","['education', 'lambert-w', 'soft-question', 'functions']"
1593908,Is this process a martingale?,Given $X_t=\int_0^t s W_s dW_s$ and the process $M_t=X_t^3-\int_0^t X_sY_s ds$. Find $Y_t$ such that $M_t$ is a martingale. I started thinking that $X_t$ can be seen as: $dX_t=tW_tdW_t$ then by applying Ito's lemma: $d(X_t^3)=3X_t^2dX_t+3X_t(dX_t)^2=3X_t^2tW_tdW_t+3X_t t^2 W_t^2 dt$. Integrating the latter between s and t yields: $X_t^3-X_s^3=3\int_s^tX_t^2tW_tdW_t+3\int_s^tX_t t^2W_t^2dt$ $M_t$ is a martingale if $\mathbb{E}[M_t|F_s]=M_s$. $\mathbb{E}[M_t|F_s]=\mathbb{E}[X_t^3-\int_0^tX_sY_sds|F_s]$. Subtracting and adding $X_s^3$ and breaking the integral leads to:$\mathbb{E}[X_t^3-X_s^3+X_s^3-\int_0^sX_sY_sds-\int_s^tX_sY_sds|F_s]=M_s+\mathbb{E}[X_t^3-X_s^3-\int_s^tX_sY_sds|F_s]$. I still don't know how to treat the latter expectation. thanks for any help.,"['stochastic-processes', 'probability-theory', 'martingales', 'brownian-motion', 'stochastic-calculus']"
1593914,Proving the transpose / dual map is well defined.,"The definition for a dual map is as follows: The dual map, or transpose of linear $f:V \rightarrow W $ is given by
  $f^t(g)(v) = g(f(v)) $ for $\forall g \in W^* , v \in V $. In my lecture notes, I have the following proof to show that the definition is well defined: $f^t (g)(a_1v_1 + a_2v_2) = g(f(a_1v_1 + a_2v_2)) $ $  = ag(f(v_1)) + a_2 g(f(v_2))$ $= a_1 f^t(g)(v_1) + a_2f^t(g)(v_2)$
and so $f^t(g) \in V^* $. How does this last step show $f^t(g) \in V^* $? How can I get my head around this proof? My understanding is that $g(f(v))$ takes in an element of $V$, applies $f$ to obtain an element of $W$ and then the functional $g$ to produce an element of the field ($K$), ultimately going from $V \rightarrow K$. Thus the function itself is an element of $V^*$. How is this intuition shown in the proof above?","['duality-theorems', 'linear-algebra', 'vector-spaces']"
1593922,if $ab = ba$ for all $a \in X$ and for all $b \in X$ then $\langle X \rangle$ is abelian subgroup of $G$,"if $X \subseteq G$ such that $\forall a,b \in X$ we have $ab = ba$ then we should prove that $\langle X \rangle$ is an abelian subgroup of G. its abviouse that $\langle X \rangle$ is subgroup of $G$. for proving abelian part we have that $X \subseteq C_G(X) \leq G$ there for $\langle X \rangle \subseteq C_G(X)$. if we show $\langle X \rangle \subseteq C_G(\langle X \rangle)$ then $\forall g \in \langle X \rangle : g \in C_G(\langle X \rangle)\Rightarrow \forall g \in \langle X \rangle \quad \forall h \in \langle X \rangle : hg = gh$. is this proof correct? and how can I show that $\langle X \rangle \subseteq C_G(X)$ then  $\langle X \rangle \subseteq C_G(\langle X \rangle)$ ?","['abelian-groups', 'abstract-algebra', 'group-theory']"
1593951,Find $\lfloor z \rfloor $ given that $z=(\{\sqrt{3}\}^2-2\{\sqrt{2}\}^2)/(\{\sqrt{3}\}-2\{\sqrt{2}\})$,"Let $\lfloor x \rfloor$ denote the greatest integer function, and $\{x\}=x-[x]$ the fractional part of $x$. If $$z=\cfrac{\{\sqrt{3}\}^2-2\{\sqrt{2}\}^2}{\{\sqrt{3}\}-2\{\sqrt{2}\}}$$ find $\lfloor z \rfloor $ My attempt $$z=\cfrac{\{\sqrt{3}\}^2-2\{\sqrt{2}\}^2}{\{\sqrt{3}\}-2\{\sqrt{2}\}}=\cfrac{(\sqrt{3}-1)^2-2(\sqrt{2}-1)^2}{\sqrt{3}-1-2\cdot (\sqrt{2}-1)}=\cfrac{3+1-2(3 -2\sqrt{2})}{\sqrt{3}-1-2\cdot \sqrt{2}+2}$$ After rationalizing I have $$z=\cfrac{4\sqrt{2}+4\sqrt{6}-2-2\sqrt{3}}{-2} $$ (I hope I haven't made any careless mistake now ) Now I am stuck as I don't know how I should take $\lfloor z \rfloor$ as I don't have that if $x=\cfrac{a}{b}$ then $\lfloor x \rfloor =\cfrac{\lfloor a \rfloor}{\lfloor b \rfloor}$ . I also think I haven't noticed a more straightforward way to do the problem ...","['functions', 'ceiling-and-floor-functions']"
1593952,How do I calculate the gradient of a discrete function?,"In the continuous case, I have $$\lim_{x\to x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{h\to 0} \frac{f(x_0 +h) - f(x_0)}{h}$$ But what is the gradient of the function $f: \mathbb{N} \rightarrow \mathbb{N}$, for example $f(n) = n^2$? For example, at $n=4$ I would expect it to be either
$$5^2 - 4^2 = 25 - 16 = 9$$ or
$$(3^2 - 4^2)/(-1) = (9 - 16)/(-1) = 7$$ Which of both is the correct solution? Or is it something different like the mean of both?","['derivatives', 'discrete-mathematics']"
1593972,Solving this limit: $\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x}$,"$$\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x}$$ I have tried a lot of things, like: transforming those terms to:
$$\frac{e^{(x+1/x)\ln(x)}}{e^{x\ln(x+1/x)}}$$ then I tried L'Hôpital's rule but it was just getting more complex I also made them one, like:
$$e^{(x+1/x)\ln(x)-x\ln(x+1/x)}$$ At last, I tried to ""squeeze"" them but I couldn't find the perfect function for that. I hope that this is not a duplicate because I searched but I couldn't find a similar post.","['calculus', 'limits']"
1593983,How to solve $\lim\limits_{x \to -\infty} \left(x\left(\sqrt{x^2-x}-\sqrt{x^2-1}\right)\right)$?,"I have a problem with this limit, i have no idea how to compute it. Can you explain the method and the steps used? $$\lim\limits_{x \to -\infty} \left(x\left(\sqrt{x^2-x}-\sqrt{x^2-1}\right)\right)$$","['calculus', 'limits']"
1593984,"What is a ""continuous modification""? And can we always modify an almost surely continuous process, such that every path is continuous?","Let's motivate the question by a classical result: Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $\mathbb F=(\mathcal F_t)_{t\ge 0}$ be a filtration on $(\Omega,\mathcal A)$ which satisfies the usual conditions $X=(X_t)_{t\ge 0}$ be a $\mathbb F$-submartingale on $(\Omega,\mathcal A,\operatorname P)$ If $t\mapsto\operatorname E[X_t]$ is right-continuous, then $X$ has a right-continuous modification which can be chosen as to be RCLL. This satement can be found in the monograph by Karatzas and Shreve, p. 16. Question : I know what a modification is, but what's meant by continuous modification ? Does it mean, that we can find a modification which is $\operatorname P$- almost surely continuous or does it mean, that we can find a modification which is continuous (i.e. every path is continuous)? In the latter case, the RCLL-property would reduce to the existence of left-side limits. In the former-case, the RCLL-property would imply, that we can pick a special modification, which is continuous . (Has this anything to do with the ""usual conditions""-assumption?) It's a similar problem I've got with ($\operatorname P$- almost surely ) continuous processes like Brownian motion. I know, that we can find a special probability space, such that there is a Brownian motion with continuous paths. But can we always modifiy a $\operatorname P$- almost surely continuous process, such that every path is continuous? (What do people mean, when they say that a process is continuous ? Do they actually always mean almost surely continuous ?)","['stochastic-processes', 'probability-theory', 'stochastic-analysis', 'brownian-motion', 'stochastic-calculus']"
1594003,Punctured complex projective space,"Let $\mathcal{P}\mathbb{C}^{n}$ be the complex projective space of $\mathbb{C}^{n+1}$, and let $B=\{\mathbf{e}_{1},\cdots,\mathbf{e}_{n+1}\}$ be a basis in $\mathbb{C}^{n+1}$. I would like to understand what happens to $\mathcal{P}\mathbb{C}^{n}$ if I remove the points in it corresponding to the basis vectors in $B$. In the case $\mathbb{C}^{2}$ I know that $\mathcal{P}\mathbb{C}^{1}\cong S^{2}$ and, from the quantum mechanical picture of $\mathcal{P}\mathbb{C}^{1}$ as the Bloch sphere, I know that the points in $\mathcal{P}\mathbb{C}^{1}$ corresponding to two basis vectors in $\mathbb{C}^{2}$ are just antipodal points.
Consequently, removing these points from $\mathcal{P}\mathbb{C}^{1}$ I get a cilinder $S^{1}\times\mathbb{R}$. Unfortunately, this visually-inspired procedure does not work in higher dimensions, and I do not know how to even start to face the problem, therefore, I appreciate any comment, suggestion or reference. Thank You. EDIT I thought of something. Let $\mathbf{E}_{j}$ be the rank-one projector associated to $\mathbf{e}_{j}\in B$, let $\mathbf{H}=\sum_{j}\nu_{j}\mathbf{E}_{j}$ be a self-adjoint operator on $\mathbb{C}^{n+1}$ such that the one-parameter group of unitary operators $\mathbf{U}_{\tau}:=\exp(-\imath\tau\mathbf{H})$ on $\mathbb{C}^{n+1}$ generated by $\mathbf{H}$ is a closed subgroup of the unitary group $U(n+1)$.
Therefore, $\mathbf{U}_{\tau}$ is an action of the circle group $U(1)\cong S^{1}$ on the complex projective space $\mathcal{P}\mathbb{C}^{n}$.
The fixed points of this action are just the points corresponding to the elements of $B$. Let $\mathcal{P}\mathbb{C}^{n}_{*}$ denotes the complex projective space without the fixed points of the action.
Since $U(1)\cong S^{1}$ is a compact group, its action on $\mathcal{P}\mathbb{C}^{n}_{*}$ is proper.
Furthermore, it is free by construction.
This means that the orbit space $\mathcal{P}\mathbb{C}^{n}_{*}/U(1)\equiv M$ is a differential manifold, the canonical projection $\pi:\mathcal{P}\mathbb{C}^{n}_{*}\rightarrow M$ is a surjection, and $\left(\mathcal{P}\mathbb{C}^{n}_{*}\,;M\,;\pi\,;U(1)\right)$ is a $U(1)$-principal bundle. At this point, if the bundle is trivial, we have that $\mathcal{P}\mathbb{C}^{n}_{*}\cong M\times U(1)$, however, I am not able to go further.","['projective-space', 'differential-geometry']"
1594005,Is every one-to-one morphism between varieties necessarily a homeomorphism?,"Let $f$ be a morphism between two irreducible varieties, and one-to-one. Is $f$ actually a homeomorphism onto its image? Here the varieties are equipped with Zariski topology. I know if the varieties are projective then it is true. (Because projective varieties are complete, so $f$ maps closed sets to closed sets)",['algebraic-geometry']
1594028,Show that $f$ is summable on $A$ and $\lim_{n\rightarrow \infty} \int_A f_n dm=\int_A f dm$,"Show that if $f_n$ is summable on a bounded measurable set $A$ for $n=1,2,\ldots$  and if $f_n$ converges uniformly to $f$ on $A$ then $f$ is summable on $A$ and $\lim_{n\rightarrow \infty} \int_A f_n dm=\int_A f dm$. Proof: It is enough to show that $\lim_{n\rightarrow \infty} \int_A (f_n-f) dm=0$. We have $$\Bigg| \int_A(f_n-f)dm\Bigg| \le \int_A|f_n-f|dm\le m(A)\sup_{x\in A} |f_n(x)-f(x)|$$ but $\sup_{x\in A} |f_n(x)-f(x)|$ goes to $0$ as $n\rightarrow\infty$ so $$\Bigg| \int_A(f_n-f)dm\Bigg| \rightarrow 0 \mbox{.}$$ Is this proof correct? I think I have not proved that $f$ is summable. How can I do it?","['lebesgue-measure', 'lebesgue-integral', 'measure-theory', 'calculus']"
1594034,Using Lebesgue's dominated convergence theorem to show a function is continuous.,"I have a function $U(t)=\int_\mathbb{R} u(x) \cos(xt)dx$ and I am trying to use Lebesgue's dominated convergence theorem to show $U(t)$ is continuous for all $t \in \mathbb{R}$ This is the proof. Notes that $x \to \cos(xt)u(x)$ is measurable for every $t \in \mathbb{R}$ How do we know $x \to \cos(xt)u(x)$? And how do we know it is measurable? Also note that $|\cos(xt)u(x)| \leq |u(x)|$ and that $|u|$ is by
  assumptions integrable I assume this is from the Lebesgue dominated convergence theorem? Thus $U(t)$ is well defined for every $t \in \mathbb{R}$ Now choose a
  sequence $t_n \to t$. Since $\cos(t)$ is continuous we get $\cos(xt_n)u(x) \to \cos(xt)U(x)$
  and as $|\cos(xt)u(x)| \leq |u(x)|$ Is this also from Lebesgue dominated convergence theorem? Hence $U(t_n) \to U(t)$ proving continuity at every $t \in \mathbb{R}$","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1594069,Vector bundle as locally free coherent sheaves,"I am studying coherent sheaves and was looking for a geometric motivation. Hence, in wikipedia and although here is stated that it can be seen as a generalization of vector bundles, which is quite satisfactorical, since this yields a better understanding of what tangent bundle, cotangent bundle or differential forms in sheaf theory and algebraic geometry might be. So, I tryed to see a vector bundle as a locally free coherent sheaf, but I got lost. So here are the two definitions and my first observations: A (real) vector bundle consists of: (i) topological spaces $X$ (base space) and $E$ (total space) (ii) a continuous surjection $\pi:E\mapsto X$ (bundle projection) (iii) for every $x$ in $X$, the structure of a finite-dimensional real vector space on the fiber $\pi^{-1}(\lbrace x\rbrace)$ where the following compatibility condition is satisfied: for every point in $X$, there is an open neighborhood $U$, a natural number $k$, and a homeomorphism \begin{align}
\varphi :U\times \mathbf {R} ^{k}\to \pi ^{-1}(U)
\end{align} such that for all $x \in U$, (a) $(\pi \circ \varphi )(x,v)=x$ for all vectors v in $R^k$, and (b) the map $v\mapsto \varphi (x,v)$ is a linear isomorphism between the vector spaces $R^k$ and $\pi^{−1}(x)$. and the definition of coherent sheaves is the following: A sheaf $\mathcal{F}$ of $\mathcal{O}_X$-Modules is coherent if : 1)$ \mathcal{F} $ is of finite type over $ \mathcal{O}_X $, i.e., for any point $ x\in X $ there is an open neighbourhood $ U\subset X $ such that the restriction $ \mathcal{F}|_U $ of $ \mathcal{F} $ to U is generated by a finite number of sections (in other words, there is a surjective morphism $ \mathcal{O}_X^n|_U \to \mathcal{F}|_U $ for some $ n\in\mathbb{N} $); 2)  and for any open set $ U\subset X $, any $ n\in\mathbb{N} $ and any morphism $ \varphi\colon \mathcal{O}_X^n|_U \to \mathcal{F}|_U $ of $ \mathcal{O}_X $-modules, the kernel of $ \varphi $ is finitely generated. Thus, we can see that they both have a topological space $X$ and we can identify $E=\mathcal{O}_X$. Furthermore there is a surjection $ \mathcal{O}_X^n|_U \to \mathcal{F}|_U $....","['sheaf-theory', 'complex-geometry', 'algebraic-geometry', 'coherent-sheaves', 'vector-bundles']"
1594084,Probability man women in a survey,"In a survey, we asked $7$ men and $5$ women. Is randomly selected without replacement persons one by one until a man. Let $X$ be a random variable of the number of prints required. Determine the values of $X$ and its probability distribution. The original French text version: My thoughts: for example we can have this outcomes 
 - M , WM , WWM , WWWM , WWWWM and WWWWWM $\Pr(X=1)=7/12,\ \Pr(X=2)=(5.7)/(12.11),\  \Pr(X=3)=(5.4.7)/(12.11.10),$ $ \Pr(X=4)=(5.4.3.7)/(12.11.10.9),\  \Pr(X=5)=(5.4.3.2.7)/(12.11.10.9.8).$ $\Pr(X=6)=(5.4.3.2.1.7)/(12.11.10.9.8.7).$ $\mathbb{E}[X] = \sum_{k=1}^6 k \, P(X=k),$ $\operatorname{Var}(X)=  \operatorname{E}\left[X^2 \right] - (\operatorname{E}[X])^2$ Is my proof correct? Is my translation of French text correct? Is there any probability distributions from that list can apply it to solve the exercise?","['probability-theory', 'probability-distributions', 'probability', 'combinatorics', 'word-problem']"
1594105,Necessary and Sufficient Conditions for a CDF,"This is an attempt to prove Theorem 1.5.3. in Casella and Berger. Note that the only things that have been proven are really basic set-theory with $\mathbb{P}$ (a probability measure) theorems (e.g., addition rule). Recall for a random variable $X$, we define $$F_X(x) = \mathbb{P}(X \leq x)\text{.}$$ Theorem . $F$ is a CDF iff: $\lim\limits_{x \to -\infty}F(x) = 0$ $\lim\limits_{x \to +\infty}F(x) = 1$ $F$ is nondecreasing. For all $x_0 \in \mathbb{R}$, $\lim\limits_{x \to x_0^{+}} F(x)= F(x_0)$ $\Longrightarrow$ If $F$ is a CDF of $X$, by definition,
$$F_{X}(x) = \mathbb{P}(X \leq x) = \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x\} \right) $$
where $S$ denotes the overall sample space. $(3)$ is easy to show. Suppose $x_1 \leq x_2$. Then notice
$$\{s_j \in S: X(s_j) \leq x_1\} \subset \{s_j \in S: X(s_j) \leq x_2\}$$
and therefore by a Theorem,
$$\mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_1\}\right) \leq \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_2\}\right)$$
giving $F_{X}(x_1) \leq F_{X}(x_2)$, hence $F$ is nondecreasing. I suppose $(1)$ and $(2)$ aren't consequences of anything more than saying that $\{s_j \in S: X(s_j) \leq -\infty\} = \varnothing$ and $\{s_j \in S: X(s_j) \leq +\infty\} = S$ (unless I'm completely wrong here). But this seems to suggest that $$\lim_{x \to -\infty}\mathbb{P}(\text{blah}(x)) = \mathbb{P}(\lim_{x \to -\infty}\text{blah}(x))$$
where $\text{blah}(x)$ is a set dependent on $x$. At this point of the text, this hasn't been proven (if it's even true). I'm not sure how to show $(4)$. $\Longleftarrow$ I don't know how to prove sufficiency. Casella and Berger state that this is ""much harder"" than necessity, and we have to establish that there is a sample space, a probability measure, and a random variable defined on the sample space such that $F$ is the CDF of this random variable, but this isn't enough detail for me to go on.","['probability-theory', 'probability', 'probability-distributions']"
1594107,It is possible to get a closed-form for $\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}$?,"I think that will not be useful to compute the Apéry's constant as 
$$\zeta(3)=\frac{4}{\pi}\sum_{n=1}^{\infty}\frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx+\frac{1}{3\pi}\left(\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}\right),$$
which is easily deduced from 
$$\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx=\frac{1}{12}\left(\frac{3\pi}{n}-\sin(\frac{3\pi}{n})\right).$$ For deduce this last identity I've used an online tool of symbolic calculus. I say that couldn't be useful since neither I don't know how evaluate an alternative of the series of integrals $\sum_{n\geq 1} \frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx$ (if it is feasible, to compute in a distinct form). In any case I would like to know Question. It is know, or it is possible to get a closed-form for $$\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}$$ in terms of known constants? Thanks in advance. When I've do more computations, using some trigonometric tricks I've found also with $\sum_{n=1}^{\infty}\frac{\sin(\frac{\pi}{n})}{n^2}$ and  $\sum_{n=1}^{\infty}\frac{\sin(\frac{2\pi}{n})}{n^2}$. It is easy to check the absolute convergence of such series, thus by comparision test these series are convergents. Using another time Wolfram Alpha (its Online Series Calculator) I don't to get a closed-form (only an approximation is given for such series) in terms of known constant, this is as exact value. Are know these series? When I've used this computation in its (General) Calculator , yes  then I've obtained some closed-form, but the Series Calculator doesn't sure any closed-form. My attemps were using partial summation and Euler-MacLaurin (first approximation) , but I believe that I can not find this real value with these methods.","['reference-request', 'trigonometric-series', 'calculus', 'definite-integrals', 'sequences-and-series']"
1594111,find the tangent to the sphere,"obtain the equations of tangent to sphere $$x^2+ y^2+z^2+6x-2z+1 = 0$$ which pass through the line $$3 (16-x) = 3z=2y+30$$ Now I know if the plane is $$lx +my+n z=p$$ then $$-I/3 +m/2+n/3=0$$ also $(16,-15,0)$ is a point on the plane I know that there is $2$ answers , but how to proceed","['multivariable-calculus', 'calculus']"
1594112,How to compute $\lim\limits_{x \to 0} \left(\frac{(\sin x)^2-x^2}{x^4}\right)$?,"I have a problem with this limit, I don't know what method to use. I have no idea how to compute it.
It's possible to resolve this limit with the developments of McLaurin?
Can you explain the method and the steps used? Thanks $$\lim\limits_{x \to 0} \left(\frac{(\sin x)^2-x^2}{x^4}\right)$$","['calculus', 'limits']"
1594130,Does there exist a vector field $\vec F$ such that $curl \vec F=x \vec i+y\vec j+z \vec k$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Does there exist a vector field $\vec F$ such that curl of $\vec F$ is $x \vec i+y\vec j+z \vec k$ ? UPDATE : I did $div(curl \vec F)=0$ as the answers did ; but that assumes a lot i.e. it assumes that components of $F$ have second partial derivatives and continuous mixed partial derivatives ; whereas for curl to be defined , we only need components of $F$ to have first order partial derivatives . Is the answer still no with this less assumption ? Please help . Thanks in advance","['multivariable-calculus', 'vector-analysis']"
1594146,Why are odds of a coin landing heads $50\%$ after $'n'$ consecutive heads,"I'm trying to understand how the odds of flipping a fair coin $4$ times in a row and landing heads each time is $\frac{1}{2^4}=\frac{1}{16}=6.25\%$; But at the same time if I've just flipped the coin heads $3$ times my odds of it landing heads a fourth time are $50\%.$ These numbers seem to contradict one another. I think I figured it out when I was writing this question, but wanted to confirm. Of the 16 possible ways that $4$ coin flips can go, $2$ have $3$ consecutive heads (below). So if I flip heads $3$ times and am about to flip it a fourth, there are two possible outcomes: $$
% inner array of minimum values
\begin{array}{c|cccc}
\text{series} & 1 & 2 & 3 & 4\\
\hline
1 & T & T & T & T\\
... & ... & ... & ... & ...\\
15 & H & H & H & T\\
16 & H & H & H & H
\end{array}
$$ making the probability of getting a fourth head $= \frac{1 outcome}{2 possibilities} = 50\%$. Is this correct?","['probability', 'gambling']"
1594175,How can a blind pixelated 2d mason chip away the entirety of a pixelated 2d rock,"So the mason is a single pixel and next to him there is a continuous pixelated rock in this 2d pixel space. He can detect rock pixels, he can turn, move forward, and chip away rock he is next to. The mason could, for example, feel his way around the rock and chip away at the rock. However, if he does this, there is a chance he could get lost (say if the rock is thin somewhere), or he could pitch or part of the rock and be left with more rocks, which would make this harder. What algorithm could the blind mason follow that would guarantee the entire rock would be chipped away in the end? Bonus question: there are more than one mason, but after starting to clear rock they cannot communicate. Can they agree on an algorithm that guarantees the entire rock will be cleared?","['algorithms', 'geometry']"
1594180,Difference between completeness and compactness,"According to Wikipedia: A metric space $M$ is said to be complete if every Cauchy sequence
  converges in $M$ $ $ A metric space $M$ is compact if every sequence in $M$ has a
  subsequence that converges to a point in $M$ I can't seem to find a situation where a complete metric space is not compact or vice versa. First of all, why can't we say $M$ is complete if every sequence converges in $M$. Since if a sequence converges to a point outside $M$ it is clearly not complete? And so if every sequence converges in $M$, then clearly every sequence has a subsequence which converges in $M$ and hence it is also compact (if it is complete). And if every sequence has a subsequence which converges to $M$ then doesn't the sequence itself converge to $M$ in which case the compact space is also complete? Apologies if I'm totally off track. If someone could provide me with some examples which show the difference between the two I'd be very grateful. Preferably an example that gives me a much better intuitive understanding, because I think my main problem is intuition, I go a bit mad trying to understand rigorous definitions.","['complete-spaces', 'general-topology', 'metric-spaces', 'compactness']"
1594182,How do I characterize the distribution of the expected number of periods before the first success in a binomial distribution?,"I came across this statement in something at work: An exponential distribution of the length of up time would result from a model in which the probability of failure (down time) is constant through time. I interpreted this as a binomial distribution where 0 = up and 1 = down , where the probability of a $1$ occurring is $0 < p < 1$. I realize this notation is a little backward, since we don't usually think of ""up"" as equating with ""failure"". Now I'm trying to characterize the distribution of the length of time that a device is ""up"". Let $X$ be the discrete probability distribution of the length of the first up time. Then \begin{align}
Pr(x = 0) &= p \\
Pr(x = 1) &= (1 - p) p \\
Pr(x = 2) &= (1 - p)^2 p \\
\ldots \\
Pr(x = n) &= (1 - p)^{n-1} p \\
\end{align} Taking the expectation:
\begin{align}
\mathbb{E}[X] &= \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} p \\
&= p \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} \\
&= \cfrac{p}{1-p} \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n} \\
\end{align} Since
\begin{align}
\displaystyle \lim_{n \to \infty} \left\lvert \cfrac{a_{n+1}}{a_n} \right\rvert
&= \displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)(1-p)^{n+1}}{n(1-p)^n} \right\rvert \\
&= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)}{n} \right\rvert \\
&= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert 1 + \cfrac{1}{n} \right\rvert \\
&= 1 - p \\
&< 1
\end{align} I know the series converges by the Ratio Test. My problem is how to evaluate the infinite series $\displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n}$, given that I know it converges, although I'm also unsure if I interpreted the initial statement correctly. EDIT: In response to one of the comments about calculating the sum, is this a correct way to do so? Let $S = \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n$. Then \begin{align}
S - (1 - p)S &= \left( \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) - \left( (1 - p) \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) \\
pS &= (1 - p) + (1 - p)^2 + (1 - p)^3 + \cdots \\
pS &= \cfrac{1}{1 - (1 - p)} \\
\Rightarrow S &= \cfrac{1}{p^2} \end{align}","['statistics', 'sequences-and-series', 'convergence-divergence']"
1594184,"Integrate $\int\frac{1}{x}\sqrt{\frac{1-x^2}{1+x^2}}\,\mathrm{d}x$","How do I go about integrating: $$\int\frac{1}{x}\sqrt{\frac{1-x^2}{1+x^2}}\,\mathrm{d}x$$ The common trigonometric substitutions don't seem to work here. I think it requires to take some power of $x$ outside the square root but I am not able to solve further.","['indefinite-integrals', 'integration', 'calculus']"
1594211,"What is a first countable, limit compact space that is not sequentially compact?","I just read a proof that adds the assumption of T 1 to conclude that a first countable, limit compact and T 1 space must be sequentially compact, but I didn't understand what happens if we drop the T 1 assumption. If $X$ is limit point compact and first countable, and $\{a_n\}$ is a sequence in $X$ , then it has a limit point $x$ . By first countability, we can find a basis of neighborhoods of $x$ and then build a subsequence that converges to $x$ . Isn't that enough? Reference: The proof is from relationship among different kinds of compactness by rm50 (Theorem 2). As you can see, $q$ appears from nowhere.",['general-topology']
1594223,Prove that the values of $x$ for which $x = \frac{x^2+1}{198}$ lie between $\frac{1}{198}$ and $199.99 \overline{49}$.,"Without using a calculator, prove that the values of $x$ for which $x = \dfrac{x^2+1}{198}$ lie between $\dfrac{1}{198}$ and $199.99 \overline{49}$. Then use this result to prove that $\sqrt{2} < 1.41 \overline{421356}$. Finally, is it true that that $\sqrt{2} < 1.41421356$? For the first part we have that the roots are $99-70\sqrt{2}$ and $99+70\sqrt{2}$ by the quadratic formula. How do I show these roots lie in this range? Also is using the quadratic formula here a good idea?","['algebra-precalculus', 'inequality']"
1594235,Cardinality of the set of all real functions which have a countable set of discontinuities,"I'm having a trouble calculating the cardinality of the set  of all functions $f:\mathbb{R} \longrightarrow \mathbb{R}$ which have at most $\aleph_0$ discontinuities (let's call the set $M$). A hint is given: Map each function $f$, with a countable set of discontinuities, $d$,  to the ordered pair $(f|_d, f|_{R\setminus d})$. I'm not entirely sure how to go on from here. I know I can create a bijective function $h: M\longrightarrow \bigcup\limits_{d\in D} A_d \times B_d$ where $D$ is the set of all countable subsets of $\mathbb{R}$, $A_d$ is the set of all real functions $f_A:d\longrightarrow \mathbb{R}$, and $B_d$ is the set of all functions $f_B:\mathbb{R} \setminus d \longrightarrow \mathbb{R}$ which do not have discontinuities (I'm not sure if the definition for $B_d$ is correct). Assuming it is correct, I know that $|A_d|=\aleph, \ \ \forall d\in D$, but I do not know how to calculate the cardinality of $B_d$. I would appreciate any help as to how to go on from here, or hints for better methods.","['real-analysis', 'cardinals', 'elementary-set-theory']"
1594240,Must a proper curve minus a point be affine?,"Let $C$ be a proper smooth geometrically connected curve over a field $K$, and let $P\in C(K)$ be a point. Must $C - P$ be affine? EDIT: By Riemann-Roch, you can definitely find functions $f_1,\ldots,f_r : C-P\longrightarrow\mathbb{A}^n_K$, but how do you guarantee that for some $n$, you can find enough such $f_i$'s such that this gives you an embedding? EDIT: Is the same true with $C$ not smooth?",['algebraic-geometry']
1594242,Topology textbook with a solution manual,"Does anyone know of a good topology textbook, that has a solutions manual for at least some of the problems? Older is fine; I just need to be able to check my own work. I've researched best topology books/free topology books, but most do not have any solutions to problems provided. Thanks for your help; hope I'm posting in right forum.","['reference-request', 'general-topology', 'book-recommendation']"
1594243,Antiderivative of the real valued function $f$ such that $f(x)\ge {1\over x}$,"Let $f:\mathbb R\rightarrow \mathbb R$  be a function satisfying $f(x)\ge {1\over x}$ for all $x\gt 0.$ Then to show that $f$ does not admit any antiderivative . So , I thought may be assuming that it has an antiderivative  and proceeding with  that lead to some  contradiction . So, define $g(x)$ as $$g(x)=\int_c^x f(t)dt$$where $c\gt 0$  is a constant. Then $$g(x)=\int_c^x f(t)dt\\ \ge \int_c^x{dt\over t}\\=log (t)|_c^x\\=log\ x -log\ c.$$
 This does not look look any contradiction. So , can this approach at all work $?$  If so , what should I do next and if not how should I try $?$","['real-analysis', 'integration']"
1594248,Question on reasoning for $\int_1^\infty\frac{\sin(x)}{x}dx$ to converge,"I often saw a 'proof' that $\int_1^\infty\frac{\sin(x)}{x}dx$ converges: By integration by parts we get
$$\int_1^\infty\frac{\sin(x)}{x}dx = \cos(1)-\int_1^\infty{\frac{\cos(x)}{x^2}}dx$$ and thus because $\frac{\cos(x)}{x^2} \leq \frac{1}{x^2}$ and $\int_1^\infty{\frac{1}{x^2}}dx$ converges $$\int_1^\infty{\frac{\cos(x)}{x^2}}dx$$ also converges by comparison test. But isn't it just wrong to reason like that? Just because $$\lim_{C\to\infty}\int_{1}^C{\frac{1}{x^2}dx}$$ exists and $$\int_{1}^C{\frac{\cos(x)}{x^2}dx} \leq \int_{1}^C{\frac{1}{x^2}dx}$$ for all $C\geq 1$, it dosn't follow that $$\lim_{C\to\infty}\int_{1}^C{\frac{\cos(x)}{x^2}dx}$$ exists. Am I missing something obvious? This 'proof' also appears in the book ""Analysis 1"" by Königsberger.","['improper-integrals', 'convergence-divergence', 'calculus', 'limits']"
1594253,Difference Between Tensoring and Wedging.,"Let $V$ be a vector space and $\omega\in \otimes^k V$. There are $2$ ways (at least) of thinking about $\omega\otimes \omega$. 1) We may think of $\otimes^k V$ as a vector space $W$, and $\omega\otimes \omega$ as a member of $W\otimes W$. 2) We may think of $\omega\otimes \omega$ as a member of $\otimes^{2k}V$. The two interpretations are ""same"" because $W\otimes W$ is naturally isomorphic to $\otimes^{2k} V$. However, the situation is a bit different when talking about ""wedging"". Let $\eta\in \Lambda^k V$. We want to wonder about $\eta\wedge \eta$. 1) Let $W=\Lambda^k V$ and think of $\eta\wedge \eta$ as a member of $\Lambda^2 W$. Then $\eta\wedge \eta=0$ by super-commutativity of the wedge-product. 2) Think of $\eta\wedge\eta$ as a member of $\Lambda^{2k}V$. Then $\eta\wedge \eta$ may not be $0$. Perhaps this confusion would not arise if we write $\wedge_V$ rather that $\wedge$, for when wedging we must remember the base space. Moreover, there is no such thing as taking the wedge product of two vector spaces, thought we can talk about tensor product of two vector spaces. Admittedly, my mind is not completely clear here. Can somebody throw some more light on the different behaviours of tensoring and wedging.","['tensor-products', 'linear-algebra', 'exterior-algebra']"
1594260,How do I find the roots of this polynomial of degree $4$?,"I am studying for finals and in the review packet is shown this problem:
$$P(x)=2x^4 + 5x^3 + 5x^2 + 20x - 12$$
I don't know what to do, I have already tried looking in the textbook and Khan academy.","['algebra-precalculus', 'roots', 'polynomials']"
1594267,Stability and type of equilibria of 2nd-order DEs,"Given $\ddot{x} = F(x)$, $x(0) = x_0$ and $\dot{x}(0) = y_0$. Assume $c$ is a simple zero of $F$. Let $V(x) = -\int_{x_0}^{x} F(s)ds$. (a) If $V$ has a local min at $c$, find the stability and type of the equilibrium (i.e, saddle, focus, center, center-focus, other) at the point $(x_0, y_0) = (c, 0)$. Use $E(x) = \frac{\dot{x}^{2}}{2} + V(x)$ to explain your answer. What is the stability and type if $V$ has a local max. Explain? (b) Find the type and stability of the equilibria of $\ \ddot{x} = -4x(1-x^2)$. (c) Find the type and stability of $x = \dot{x} = 0$ for the equation: $\ \ddot{x} = -4x(1-x^2) - x^2\dot{x}$ My attempt: For part (a), let $\dot{x} = y$. It's easy to see that we can rewrite the 2nd-order ODE as the system of 1st-order ODE: $\dot{x} = y, \dot{y} = F(x)$, and $E(x,y) = \frac{y^2}{2} + V(x)$. Now, since $\frac{\partial E}{\partial y} = \dot{x}$, and $-\frac{\partial E}{\partial x} = \dot{y}$, our system is Hamiltonian system. Thus, all the solutions $(x(t), y(t))$ belongs to the level sets $E(x,y)= C$ for any constant $C > 0$. Now, since $V(x)$ has a local min at $c$, by Fundamental Theorem of Calculus, $F(c)=0$, and $F'(c)< 0$ (since $c$ is a simple zero of $F$, so $c$ is a strict local min). So the point $(c,0)$ is an equilibrium of our system. By principle of linearization, since $Df((c,0)) = (0 1, F'(c) 0)^T$, which has 2 real eigenvalues $\pm \sqrt{F'(c)}$, we conclude that $(c,0)$ is a hyperbolic equilibrium, and it is an unstable saddle. For the case when $V$ has local max at $c$, we have $F'(c) < 0$, so $Df((c,0))$ would have 2 imaginary eigenvalues with real parts $= 0$, so $(c,0)$ is not hyperbolic equilibrium, thus Principle of Linearized Stability cannot apply in this case. However, as we know that $(c,0)$ would be on some level sets $E(x,y) = C_1$ for some $C_1$, $(c,0)$ is stable . In addition, due to the 2 eigenvalues are all imaginary, $(c,0)$ can't be saddle, focus, center or center-focus, so it must be OTHER . Is my solution correct for this part? Part (b) is quite simple by using part (a)'s result (only need to check whether $0$, $1$ or $-1$ is a local min or max of F(x), and choose $y_0=0$), so I omitted the proof here. For part (c), it's quite hard, as we have the term $x^2\dot{x}$, so I cannot use the result above as the RHS is not purely in terms of $x$. Principle of Linearization also does not work in this case, as $Df((0,0))$ has $2$ eigenvalues $\pm 2i$.",['ordinary-differential-equations']
1594279,Properties of 3-vector dot product,"I've been playing around with an extension of a dot product to three vectors, as set forth in this question .  Basically, if you have three vectors A, B, and C, then you could compute the following $$TD(A,B,C) = \sum_{i=1}^N a_i b_i c_i$$ where TD means ""triple dot."" I realize this isn't an accepted notation but it is useful for this question. I'm curious if anyone has shown that $$TD(A,B,C)\leqslant \lVert A \rVert \cdot\lVert B \rVert\cdot \lVert C \rVert.$$ I suppose it might involve an extension of the Rearrangement inequality to three vectors.","['inequality', 'linear-algebra']"
1594292,Intuition behind Riesz-Markov Representation Theorem,"I am currently reading Big Rudin and I have this theorem that I've been struggling with for some time now. Usually, when reading, I either try to get a concrete intuition behind the idea, visualise the theorem and its proof in my head, or at least connect it to something I already know, as well as try to connect the proof to other proofs I know. However, upon this theorem, I truly stumbled. As Rudin notes in the beginning of the chapter, one can see a little connection between the measure and linear functionals as $b-a$ can be approximated by $\Lambda f$ where $\Lambda f=\int_a^b fdx$ and $f$ is a continuous function on $[a;b]$ with range lying in $[0;1]$. However, this honestly felt like ""cheating"". As to why this is cheating, it seems so to me for various reasons. First of all, he gave the most trivial of examples while one needs a general idea. However, the real reason is the fact that the linear function he uses is actually intimately related to measure by itself, without any need for a representation theorem. The ideas of measure and integrations are intertwined and thus stating that we can find a measure that represents an integral (especially since it's only on an interval) seems useless and uninformative. Is there a way to intuitively understand why such a representation is possible or is this theorem too complex for any intuition? Thanks in advance.","['functional-analysis', 'riesz-representation-theorem', 'banach-spaces', 'measure-theory']"
1594309,Is $\sum_{n=1}^{{\infty}}\frac{1}{P_{3n}}$ convergent?,Is this sum below convergent? ($P_{n}$ is the nth prime.) $$\sum_{n=1}^{{\infty}}\frac{1}{P_{3n}}$$,"['number-theory', 'prime-numbers', 'convergence-divergence']"
1594331,Why can you mix Partial Derivatives with Ordinary Derivatives in the Chain Rule?,"This question is a simplified version of this previous question asked by myself . The following is a short extract from a book I am reading: If $u=(x^2+2y)^2 + 4$ and $p=x^2 + 2y$ $\space$ then $u=p^2 + 4=f(p)$ therefore $$\frac{\partial u}{\partial x}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial x}=2xf^{\prime}(p)\tag{1}$$ and $$\frac{\partial u}{\partial y}=\frac{\rm d f(p)}{\rm d p}\times \frac{\partial p}{\partial y}=2f^{\prime}(p)\tag{2}$$ I know that the chain rule for a function of one variable $y=f(x)$ is $$\begin{align}\color{red}{\fbox{$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}$}}\color{red}{\tag{A}}\end{align}$$ I also know that if $u=f(x,y)$ then the differential is $$\begin{align}\color{blue}{\fbox{${{\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y}$}}\color{blue}{\tag{B}}\end{align}$$ I'm aware that if $u=u(x,y)$ and $x=x(t)$ and $y=y(t)$ then the chain rule is $$\begin{align}\color{#180}{\fbox{$\frac{\rm d u}{\rm d t}=\frac{\partial u}{\partial x}\times \frac{\rm d x}{\rm d t}+\frac{\partial u}{\partial y}\times \frac{\rm d y}{\rm d t}$}}\color{#180}{\tag{C}}\end{align}$$ Finally, I also know that if $u=u(x,y)$ and $x=x(s,t)$ and $y=y(s,t)$ then the chain rule is $$\begin{align}\color{#F80}{\fbox{$\frac{\partial u}{\partial t}=\frac{\partial u}{\partial x}\times \frac{\partial x}{\partial t}+\frac{\partial u}{\partial y}\times \frac{\partial y}{\partial t}$}}\color{#F80}{\tag{D}}\end{align}$$ Could someone please explain the origin or meaning of equations $(1)$ and $(2)$? The reason I ask is because I am only familiar with equations $\color{red}{\rm (A)}$, $\color{blue}{\rm (B)}$, $\color{#180}{\rm (C)}$ and $\color{#F80}{\rm (D)}$ so I am not used to seeing partial derivatives mixed up with ordinary ones in they way they were in $(1)$ and $(2)$. Many thanks, BLAZE.","['derivatives', 'partial-derivative', 'chain-rule', 'calculus']"
1594343,How to differentiate the numerator of this vector field $\frac{\vec r}{|r|^3}$?,"I was studying a nice solution of how to apply the divergence theorem for a vector field with a singularity at the origin. However, the solution doesn't give any concrete computations and just makes the claim that $\large\frac {\vec r}{|r|^3}$ is divergence-free; I am guessing that $\vec r$ is just a radius / position vector $(x,y,z)$. So, I would like to check this, but I am slightly confused about how to differentiate the numerator. The denominator is clearly $(x^2 + y^2 + z^2)^{\frac{3}{2}}$, so I know how to take derivatives of this term, w.r.t. either $x$, $y$, or $z$ - just have to be careful when using the chain rule to account for all the factors. What about the numerator?  Would the derivative of it, w.r.t., say, $x$, be ... $r_x$ just be... $1$?  So, for simplicity just looking at $div \vec r$ gives $r_x + r_y + r_z$ = 1 + 1 + 1 = 3. Am I ok with my computations? Thanks,","['multivariable-calculus', 'real-analysis', 'calculus', 'vector-analysis']"
1594374,Determining number of randomly picked people,"Firstly I want to put big disclaimer here. This particular problem is a smaller part of my homework. Since even after discussion with my fellow classmates we are not sure how to handle it we decided to post a question here. Basically we are supposed to simulate spreading of disease. Every day every infected person will pick random number of people (possible infected candidates) with Poisson distribution where parameter is 5. Everyone of those selected people (=possible infected candidates) will be infected with probability of 1/2 and at the end of the day the infection takes effect. Next day this particular infected person will be also spreading infection and will again pick random number of people (another possible infected candidates) with Poisson distribution where parameter is again 5.
Number of possibly infected candidates and event static whether person will be infected or not are both independent on each other. Let's suppose on the first day there is only one infected person.
We should simulate disease spreading for one week. And how many infected persons there will be at the end of 8th day. Now finally to my question. Confusing part for me is how are we suppose to determine the number of randomly picked people which represent possible infected candidates since we don't know how many people is there?
I assume that with higher number of people the disease should spread faster? I also think that this number of randomly picked possible candidates should be somehow determined by given parameter and also by Poisson distribution itself but I am struggling how to make these things ""work"" together.","['statistics', 'simulation', 'probability']"
1594376,Prove the following inequality (probably) using derivatives,"In chapter where we use derivatives for determining local minima/maxima there is this inequality where I really do not know where to start. Prove: $$ \frac{1}{2^{p-1}} \leq x^p +(1-x)^p \leq 1  $$ for $$ x \in [{0,1}], \ p > 1 $$","['derivatives', 'calculus']"
1594381,How to calculate Probability in this case,I struggle a lot with math and I'm working to see if I can overcome the fear I have for math. I just don't know how to start solving the problem below. A certain germicide kills 60% of all insects in any experiments. A sample of 16 insects is exposed to the germicide in an experiment. What is the probability that exactly 5 insects will survive?,"['statistics', 'probability']"
1594398,If a sequence of quadratic forms converges in probability and a random vector converges in distribution then $X_n^TQ_nX_n$ converges,"If a sequence of quadratic forms converges in probability $Q_n\xrightarrow{P}Q$ and a random vector converges in distribution  $X_n\xrightarrow{d}X$ then $X_n^TQ_nX_n\xrightarrow{d}X^TQX$. This is a statement from an online source in statistics. It follows by Slutsky's theorem and the continuous mapping theorem. I can also see how intuitively it should be true, but I'm having trouble setting up the argument. No matter what I do, in the end I have a product of two things converging only in distribution.",['probability']
1594406,Countable abelian groups are amenable.,"I am using the following definition of amenable group: The at most countable group G is amenable iff there exists a sequence $\{F_n\}$ of finite subsets such that for every $g\in G$ we have
$$\displaystyle\lim_{n\rightarrow\infty}\displaystyle\frac{|(gF_n)\triangle F_n|}{|F_n|}=0$$
and $G=\bigcup_n F_n$. Now, I want to prove that a countable abelian group is amenable using just this definition . If $G=\{g_1, g_2,...\}$ in the following link an user says that the family
$$F_n=\{a_1g_1+a_2g_2+\cdots+a_ng_n: |a_i|\leq n-i+1\} $$
works. And I think that the following family it is useful as well
$$F_n'=\{a_1g_1+a_2g_2+\cdots+a_ng_n: 0 \leq a_i \leq n\} $$ But I don't know how to prove this. I could find these bounds
$$|F_n|\leq (2n+1)(2n-1)\cdots 3 = (2n+1)!!$$
$$|F_n\cap gF_n|\leq (2n-1)(2n-1)!! $$
and
$$|F_n'|\leq (n+1)^n$$
$$|F_n'\cap gF_n'|\leq n(n+1)^{n-1} $$ But I am not sure what step is next. Any help? Thanks!","['measure-theory', 'group-theory', 'geometric-group-theory']"
1594414,"Using test functions to ""test"" whether functions vanish","Let $U$ be an open subset of $\mathbb R^n$ and let $f \in L_{\text {loc}}^1(U)$ (i.e. $f$ is integrable on compact subsets of $U$). Suppose $\int_U f \phi = 0$ for all test functions $\phi \in C_c^\infty(U)$. Does this imply that $f = 0$ a.e.? If so, why? I ask this question because I'm learning about analysis of PDEs from Evans' textbook. This fact, or something similar to it, is used everywhere, but I can't think of a rigorous proof for it. One approach I tried is to approximate indicator functions on arbitrary measurable subsets of $U$ by their mollifications, but I haven't managed to get this to work. I wonder if there is a better method.","['measure-theory', 'partial-differential-equations']"
1594435,A combinatoric $gcd$ problem,"Let $Q(L)$ be the number of pairs of numbers $m , n$ such that $gcd(m,n) = 1$ and $m$ and $n$ are of different pairity, where $m$ is even and $n$ is odd, and $m^2 + n^2$ $\le$ $ L$. $$Q(L) = \sum_{gcd(m,n)=1 \atop {m\space even\atop {n\space odd}}}^{m^2+n^2 \le L}1$$ The number of these pairs corresponds to the number of primitive pythagorean triplets with a hypotenuse $\le$ $L$, in which $gcd(a,b,c) = 1$ and $a,b,c$ are the sides of the triangle. If $L$ is small I can easily enumerate the number of these pairs with a computer, but given sufficiently large $L$ this becomes inefficient. I've been thinking for quite some time but I couldn't myself find a formula or an algorithm, though I have a feeling there is a simple combinatorial approach to this. So is there an efficient way to count these pairs? EDIT In an attempt to solve the above question, I've come up with an idea to perhaps simplify it. Define $T(P,L)$ to be the amount of numbers $\le$ $L$ that are coprime to $P$ , that is: $$T(P,L) = \sum_{k=1\atop {gcd(k,P)=1}}^L1$$ Solving this, I assume, will be a significant step in solving the original question of $Q(L)$ in my approach. Also, if you have another approach in your mind to solving this problem, other than the $GCD$ approach, it'll be excellent as well. Thanks in advance.","['number-theory', 'combinatorics', 'gcd-and-lcm']"
1594438,Proper ideal $I \implies \exists $ prime ideals $P_i$ such that $P_1 \cdots P_n \subset I$.,"Let the below ideals be in a commutative Noetherian ring $R$. Corollary 22.  (3) There are prime ideals $P_1, \dots, P_n$ (not necc. distinct) $\supset I$ such that $P_1\cdots P_n \subset I$. (Out of D&F) Prove (3) of Corollary 22 directly by considering the coll. $\mathcal{S}$ of ideals that do not contain a finite product of prime ideals.  [If $I$ is a maximal element in $\mathcal{S}$, show that since $I$ is not prime there are ideals $J, K$ properly containing $I$ (hence not in $\mathcal{S}$) with $JK \subset I$.] I know: $I$ is not prime $\implies \exists$ ideals $J,K$ such that $JK \subset I$ yet $J \not\subset I$ and $K \not\subset I$. $I$ is not prime $\implies$ in particular not maximal $\implies$ $I$ properly contained in some maximal ideal $J$. From examining proof to Proposition 20 the proof of this would go something like if $\mathcal{S}$ were not empty, then since $R$ is Noetherian, all chains in $\mathcal{S}$ are upper bounded and so $\mathcal{S}$ contains a maximal element $I$. I can't piece it together from these facts alone, what am I missing?","['noetherian', 'abstract-algebra', 'maximal-and-prime-ideals', 'commutative-algebra', 'ideals']"
1594455,"Branch points and Riemann surfaces (analytic continuation),","Take probably the most typical example: $$f(z) = \sqrt{1-z^2}$$ This function uses the (complex) logarithm to define it: $$e^{\large \frac{1}{2}\log(1-z^2)}$$ $$e^{\large \frac{1}{2}[\ln|1-z^2| + i\arg(1-z^2)]}$$ And so we can see that the function is not defined at $\pm1$.  They are so-called ""branch points"", and this function requires two branch cuts. So, my questions are: a)  is every point along the branch cut also called a ""branch point"", or is it just the ""starting point"" in the branch cut that is called a branch point? b) needing two branch cuts, does this mean we have two functions?  Way before seeing a function such as this one, we learn that for multi-valued ""functions"", once we specify a branch, it then becomes a well-defined, single-valued, genuine function.  But we usually only make one branch cut, though.  Or is the example I gave above just...one function requiring two branch cuts?  Then, making two branch cuts, does this mean we have chosen one branch of $f(z) = \sqrt{1-z^2}$? ...it doesn't mean that we have chosen two branches of the function, right? Thanks,","['branch-points', 'logarithms', 'complex-analysis', 'analyticity', 'branch-cuts']"
1594463,The group defined by Gauss's definition of composition of forms,"In article 242 of Disquisitiones , Gauss investigates the properties of the direct composition of two forms of the same discriminant.  In this case, he gives a ""natural"" choice for such a composition.  Denoting this composition by $Ax^2 + Bxy + Cy^2$ (so skipping the extra ""2"" in Gauss's way of writing forms), Gauss notes that $A$ is determined by his definition, while $B$ is determined modulo $2A$.  Once $A$ and $B$ are determined, $C$ is determined because the determinant is fixed. This can be rephrased by saying that Gauss composition is well defined on the equivalence classes of forms under the action of the subgroup of $\mathrm{SL}_2 (\mathbb{Z})$ consisting of matrices of the form $\begin{bmatrix} 1 & m\\0 & 1\end{bmatrix}$. These classes then form a countably generated infinite Abelian group. This group was studied for positive discriminant by Lenstra in his 1980 paper ""On the calculation of regulators and class numbers of quadratic fields"".  Among other things, he embeds the group in a topological group which he notes is a subquotient of the idèle group of the corresponding quadratic field.  Schoof later pointed out in ""Computing Arakelov class groups"" that Lenstra's topological group is essentially the Arakelov class group of the field. My question is, was this group studied in its own right after Gauss?  Or did all number theorists of the 19th and early 20th centuries study only the class group, and later the group of fractional ideals?","['number-theory', 'quadratic-forms', 'algebraic-number-theory']"
1594473,$\lambda$ is an eigenvalue iff spectral measure of $\lambda$ is nonzero,Let $M$ be a normal operator on a Hilbert space and let $E$ be the spectral measure of $\sigma(M)$ (the spectrum of $M$). Show that $\lambda$ is an eigenvalue to $M$ $\iff E(\{\lambda\})\not = 0$. Hints?,"['functional-analysis', 'spectral-theory']"
1594480,Hexagon boundary,"What is the easiest algebraic way to clip the arrow to stay within the hexagon? Edit:  the only known parameters are the arrow end locations $\ x_0 $ and  $\ y_0 $ and hexagon outer radius $\ R $.  The hexagon origin is at $\ [0,0] $.","['trigonometry', 'linear-algebra']"
1594482,Relationship between primes and practical numbers,"This is my first post here. I am a musician, and not a mathematician, but I enjoy doing things to prime numbers and seeing what comes out. I have defined a sequence which takes the following values for $n$: -1 if $n$ is prime 1 if $n$ is a practical number 0 if $n$ is neither or both I have then taken a sequence of its partial sums. The first 50 terms are 1,1,0,1,0,1,0,1,1,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,1,2,1,2,1,2,2,2,2,3,2,2,2,3,2,3,2,2,2,2,1,2,2,2. The plot for $n<100000$ looks quite linear: In order to see quite how linear it was, I then divided each term of the sequence by $n$ and got this plot: It seems to me like it wants to converge to some value. The arithmetic mean of the last 100 terms is 46.3225. I vaguely understand that there are some analogies between practical numbers and primes. I am wondering how difficult it would be to establish if the above sequence does in fact converge, and if so, then to what value. I have tried it with other prime-like sequences, such as ludic numbers and lucky numbers, but the other ones didn't seem as neat... Thanks!","['pattern-recognition', 'number-theory', 'recreational-mathematics', 'prime-numbers', 'experimental-mathematics']"
1594515,Interpretation for the curvature and monodromy of a connection - Reality check,"Let $P \to M$ be a principal $G$-bundle with connection form $\omega \in \Omega^1(P,\mathfrak{g})$. Here are the statements I'm basing my viewpoint on: A connection is flat (vanishing curvature) iff it is locally the pullback of the maurer cartan form on $G$ i.e. for all $p \in P$
  there's a neighborhood $p \in U$ and a map $f:U \to G$ satisfying
  $\omega|_U = f^*\omega_G$, where $\omega_G$ is the maurer cartan form of
  $G$ (this can be proved via an integrable distribution argument). The monodromy of a flat connection is zero iff it is globally given by the pullback of the maurer cartan form on $G$. i.e. iff
  there's a function $f:P \to G$ satisfying $\omega=f^*\omega_G$. Here's what I want to be able to say: A connection $P$ is flat iff the $TP \to P$ admits covariantly constant local sections everywhere. Meaning, for every point $p \in P$ there's a neighborhood $p \in U$ and a section $X: U \to TP$ satisfying $\omega(X)=0$. A flat connection on $P$ has zero monodromy iff $TP \to P$ admits a covariantly constant global section. Meaning there's a global section $\sigma : P \to TP$ satisfying $\omega(\sigma)=0$. I get a bit confused though whenever I try to formalize a proof of the above. Sometimes I think the covariantly constant sections should be of the bundle $P \to M$ and that $TP \to P$ always has a covariantly constant section in the sense I defined, here I also get confused. My questions has two parts: Is the above interpretation a valid one ? If so how can I formalize this with minimal effort and confusion? (a hint might suffice). If not how could it be fixed? Does this picture still hold when moving to the category of associated bundles ? In particular, do covariantly constant local (or global) vector fields all arise in this manner?","['algebraic-topology', 'principal-bundles', 'vector-bundles', 'differential-geometry']"
1594516,Sum of partial derivatives,"Suppose that
$$\mu_i(x)=x_i \int_0^1 t^{n-1} \rho(tx) dt$$
where $\rho$ is a function on $\mathbb R^n$ and $tx=(tx_1,\dots,tx_n)\in \mathbb R^n$. Show that $$\sum_{i=1}^n \frac{\partial\mu_i}{\partial x_i}=\rho .$$ This problem looks simple, but I am having difficulty in showing the result. I guess that the first step is to find $\frac{\partial\mu_i}{\partial x_i}$ using the product rule.","['derivatives', 'calculus']"
1594548,$\nabla^2 u = 0 $ and integral $u$ around $\partial B_\rho$,"I'm doing exercises from book about vector calculus. And there is problem which I'm not sure what to do. Let's see the hypothesis. 
Let, $D \subseteq \mathbb{R}^2$ an open set. Let, $u:\overline{D} \to \mathbb{R} \in C^2$. Suposse that, $\vec{p} \in D$, and, $0<\rho \leq R$. Consider, a ball $B_R:= B(\vec{p},R)$.
If $\nabla^2u = 0 $ at $D$ then : 
$$\frac{1}{2\pi R}\int_{\partial B_R} u \, ds  = u(\vec{p})$$ So far, I know two facts, which the author suggested to readers : 
$$ \lim_{\rho \to 0} \frac{1}{\rho}\int_{\partial B_\rho}u \,ds = 2\pi u(\vec{p}) $$
$$ \frac{d}{d\rho} \left ( \frac{1}{\rho}\int_{\partial B_\rho}u \,ds   \right) =  \frac{1}{\rho}\iint_{B_\rho}\nabla^2 u \,ds = 0  $$
That's means :
$$\frac{1}{\rho}\int_{\partial B_\rho}u \,ds = constant$$ Therefore, the expression above, should be equal to $2\pi u(\vec{p})$. No matter what values $\rho$ takes in particular if I take $\rho = R$. Then we are done. But I'm not really sure if the argument above works or I'm ignoring something. Any help is appreciated","['multivariable-calculus', 'harmonic-functions', 'calculus', 'vector-analysis']"
1594554,Regarding gauss law differential form,"I have a big issue regarding the equality of integrands in gauss law. Given the integral form we have that $$\oint_{\partial\Omega}\vec{E}\cdot\vec{dS}=\int_{\Omega}\nabla\cdot \vec{E}dV={1\over \epsilon_0}\int_{\Omega}\rho \ dV$$ Here in this link https://physics.stackexchange.com/questions/23190/where-is-the-flaw-in-deriving-gausss-law-in-its-differential-form it says that we can conclude that $$\nabla\cdot \vec{E}={1\over \epsilon_0}\rho$$ because the equality of integrals is valid for all region $\Omega$ of the space. But how can we $\mathbf{formally}$ prove this result? so we can formulate the next theorem: Let $f,g:\mathbb R^3 \to \mathbb R$. Let $\Omega$ be any arbitrary region in $\mathbb R^3$ suppose that $$\int_{\Omega}f=\int_{\Omega}g$$ then $f=g$ Know even if this theorem holds, the second problem is that the region in gauss law is a closed region (becuase we use the divergence theorem) so my second question is that if the theorem would also be true just for closed regions? I would really appreciate if you can help me with this problem","['multivariable-calculus', 'physics', 'mathematical-physics', 'vector-analysis']"
1594583,What do instantaneous rates of change really represent?,"The derivative of $f(x)$ is the value of the limit of the average rate of change of $y$ with respect to $x$ as the change in $x$ approaches $0$. This is the value, in other words, that the average rate of change approaches but NEVER hits. This means that it is NOT the infintesimal rate of change of $y$ with respect to $x$; $dy/dx$ merely approaches the derivative's value. If the rate of change did actually achieve $0$ change in $x$, you'd get $0/0$ which is an indeterminant form. So if the derivative is the literal rate of change at an exact instant -- a rate of change with an interval of $0$, what does that actually tell you? Can a specific moment in time really have a rate of change? Is that rate of change ever even maintained, even at a specific instant? I know that a point by itself can't have a rate of change, you need a continuum of points around it to determine one (hence a limit). What does an instantaneous rate of change tell you?","['derivatives', 'calculus']"
1594587,"Least positive eigenvalue of the BVP $y''-\lambda y'+\frac{2\lambda-1}{x}y=0$, $y(0) = y(1/2) = 0$","Find the first positive eigenvalue $\lambda$ of the boundary value problem over $x\in [0,\frac{1}{2}]$.
  $$y''-\lambda y'+\frac{2\lambda-1}{x}y=0, \quad y(0)=y(\tfrac{1}{2})=0.$$ My approach: I have tried to use Frobenius Theorem because $x=0$ is a regular-singular point and also the indicial equation implies that the eigenfunction (non-trivial solution) will not a similar form of a Bessel function. I have managed to use self-adjoint properties but the differential operator of the left hand side turns out to be non self-adjoint.","['boundary-value-problem', 'asymptotics', 'ordinary-differential-equations', 'eigenfunctions']"
