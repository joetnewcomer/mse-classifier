question_id,title,body,tags
2085335,Finite summation over reciprocal of binomial coefficient,"Problem : Simplify
$$ \sum_{m=0}^k a^m m! (n-m)!$$ 
where $a$ is real and positive, $k \in \mathbb N$ and $n \in \mathbb N$ with $k \leq n$. Upper bounds and lower bounds may also be useful. One example I have found  is, assuming $a \neq 1$ and using $m! (n-m)! \leq n!$, 
$$ \sum_{m=0}^k a^m m!(n-m)! \leq n! \sum_{m=0}^k a^m = \frac{n! \left(1 - a^{k+1} \right)}{1-a},$$
but I am not satisfied with the accuracy of this bound. I am aware of the somewhat related math.stackexchange.com question: Calculate $\sum\limits_{k=0}^{\infty}\frac{1}{{2k \choose k}}$","['combinatorics', 'binomial-coefficients']"
2085338,Polynomials - Relation of Divisibility,"The exercise I am having difficulty solving is the following one : Suppose $P$ and $Q$ are two polynomials with integer coefficients. Suppose also that for all $(m,n) \in \mathbb{Z}^2$ we have $P(m)-P(n) | Q(m)-Q(n)$. Show that there exists a polynomial $H \in \mathbb{Q}[X]$ such that $Q= H \circ P$. The issue is that I do not now how to characterize effectively the existence of such a polynomial $H$... I have tried, without success, to use the following result : if $P,Q$ are two polynomials with integer coefficients such that $P(n)|Q(n)$ for infinitely many integers $n$, then $P$ divides $Q$ within $\mathbb{Q}[x]$. Does anyone have an idea ?","['number-theory', 'polynomials', 'divisibility']"
2085347,Linear objective function with quadratic constraints,"The context is ordinary multivariate regression with $k$ (>1) regressors, i.e. $Y = X\beta + \epsilon$, where
$Y \in \mathbb{R}^{n \times 1}$ vector of predicted variable,
$X \in \mathbb{R}^{n \times (k+1)}$ matrix of regressor variables(including ones in the first column) 
$\beta \in \mathbb{R}^{(k+1) \times 1}$ vector of coefficients, including intercept. Say, I have already estimated $\beta$ as $\hat{\beta} = (X'X)^{-1} X'Y.$ I have to solve the following program: minimize $f(B) = L\beta$   ( $L$ is a fixed $1\times (k+1)-$vector   )
such that:
$[(\beta-\hat{\beta})' \cdot X'X \cdot (\beta-\hat{\beta})] / [(Y - X\hat{\beta})' (Y - X\hat{\beta}) ]$   is less than a given value $c$. Note that this is a linear optimization program with respect to $\beta$ with quadratic constraints. I don't understand how we can solve this optimization - I was going through some online resources, each of which involve manually computing gradients of the objective as well as constraint functions - which I want to avoid (at least manually doing this). Can you please help solve this optimization problem in R ? The inputs would be:
$X$, $Y$,
$\hat{\beta}$,
$L$ and
$c$ Please let me know if any further information is required - the set-up is pretty general.","['regression', 'optimization', 'qclp', 'statistics', 'convex-optimization']"
2085353,Transform normal distribution to chi-square distribution,"Let $X_1$ and $X_2$ be independent random variables with distributions, with 
$X_1 \sim N(-1; 2)$ and $X_2 \sim N(2; 3)$ Prove that random variable $Y_2=((2X_1+X_2)^2)/11$ is chi-square distributed. My approach: $2X_1+X_2 =Z,$ with $Z \sim N(-2+2;4*2+3)=N(0;11).$ $Y_2 \sim (N(0,11)^2)/11 =(N(0;1)^2)*(\sqrt{11}/11).$
Where am I making mistake?",['statistics']
2085375,Why does multiplication act like scaling and rotation of a vector in the complex plane? [duplicate],"This question already has answers here : Geometric interpretation of the multiplication of complex numbers? (3 answers) Closed 7 years ago . I regularly use the geometric analogy of multiplication by a complex number to represent a scaling and rotation of a vector in the complex plane. For a very simple example, i would point up along the Y axis and multiplying it by i again would be a 90 degree rotation resulting in something pointing in the -X direction. The thing is, I no longer recall why this is true. It's not obvious to me any longer why multiplication is in any way connected to rotation (scaling seems fairly obvious) and I was unable to explain the logic behind this useful trick to a friend who asked why it worked. Could I get a very clear explanation of this geometric interpretation of multiplication by complex numbers? I feel like it had to do with Euler's identity and the polar form of complex numbers but this math is quite a few years behind me.","['arithmetic', 'complex-numbers', 'geometric-interpretation', 'geometry']"
2085397,Analytical solution to linear coupled ODEs?,I have this set of equations and I am trying to find $X(t)$ and $Y(t)$ analytically with initial values known such as $X(0)=X_0$ and $Y(0)=Y_0$. How should I approach to solve it? $dX/dt=a \times X(t) + b \times Y(t)$ $dY/dt=c \times Y(t) + d \times X(t)$,['ordinary-differential-equations']
2085401,If $a_n = \frac{1}{2} (a_{n-1}+a_{n-2})$ prove $a_n=2+4 \left(\frac{-1}{2}\right)^n$ for $n \geq 2$,"Question: Let $a_1=0, a_2=3$, and, for all $n \geq 3$ let $a_n = \frac{1}{2} (a_{n-1}+a_{n-2})$ The sequence $(a_n)$ is said to be defined recursively. on $n$, show that, for all $n \geq 2$, $$a_n=2+4 \left(\frac{-1}{2}\right)^n$$ And deduce that $(a_n) \rightarrow 2$ My attempt: $
  a_1 = 0 , a_2 = 3 , \ldots
$ $\forall n \geq 3$ let $a_n = \frac{1}{2}(a_{n-1} + a_{n-2})$ Show inductively that $\forall n \geq 2$ $
  a_n = 2 + 4(- \frac{1}{2})^n
$ And deduce that $(a_n) \rightarrow 2$ So $\ldots$ let $n$ = $3$ then $a_3 = \frac{1}{2}(a_2 + a_{1}) = \frac{1}{2}(3 + 0) = \frac{3}{2}$ This is equal to $2 + 4(- \frac{1}{2})^3$, so this holds. Assume that this holds up to $n = k$, then for $n = k + 1$; $a_{k + 1} = \frac{1}{2}(a_k + a_{k-1})$ $a_k$ is known as $a_k = 2 + 4(- \frac{1}{2})^k$ And $a_{k - 1}$ is also known as $a_{k - 1}= 2 + 4(- \frac{1}{2})^{k - 1}$ so I then have $2a_{k+1} = (a_k + a_{k-1})$, and subbing in the values above for $a_k$ and
$a_{k-1}$ gives $2a_{k+1} = ((2 + 4(- \frac{1}{2})^k) + (2 + 4(- \frac{1}{2})^{k - 1}))$ Which can be simplified to $a_{k+1} = (1 + 2(- \frac{1}{2})^k + 1 + 2(- \frac{1}{2})^{k - 1})$ and $a_{k+1} = 2 + 2(- \frac{1}{2})^k + 2(- \frac{1}{2})^{k - 1}$ Using exponent laws for $ 2(- \frac{1}{2})^{k - 1}$ gives \begin{equation*}
  \begin{aligned}
    2(- \frac{1}{2})^{k - 1} &= 2(-\frac{1}{2})^{k} \times (- \frac{1}{2})^{-1} \\
    &= 2(-\frac{1}{2})^{k} \times (-2) \\
    &= -4(-\frac{1}{2})^{k} \\
  \end{aligned}
\end{equation*} Subbing this back into $a_{k+1} = 2 + 2(- \frac{1}{2})^k + 2(- \frac{1}{2})^{k -
  1}$  as \begin{equation*}
  \begin{aligned}
    a_{k+1} &= 2 + 2(- \frac{1}{2})^k -4(-\frac{1}{2})^{k} \\
    &= 2 - 2(- \frac{1}{2})^k
  \end{aligned}
\end{equation*} If the induction holds then $ a_{k + 1} =  2 - 2(- \frac{1}{2})^k = 2 + 4(- \frac{1}{2})^{k + 1}$ Using power laws on $ 4(- \frac{1}{2})^{k + 1}$ gives \begin{equation*}
  \begin{aligned}
    4(- \frac{1}{2})^{k + 1} &= 4(- \frac{1}{2})^{k } \times (- \frac{1}{2})^{1} \\
    &= 4(- \frac{1}{2})^{k } \times (- \frac{1}{2}) \\
    &= -2(- \frac{1}{2})^{k }  \\
  \end{aligned}
\end{equation*} Using this result gives \begin{equation*}
  \begin{aligned}
    a_{k + 1} &=  2 - 2(- \frac{1}{2})^k \\
    &= 2 + 4(- \frac{1}{2})^{k + 1} \\
    &= 2  -2(- \frac{1}{2})^{k }  \\
  \end{aligned}
\end{equation*} As required. Therefore by the inductive hypothesis $\forall n \geq 2, a_n = 2 + 4(- \frac{1}{2})^n $ Deduce that $(a_n) \rightarrow 2$ If $(a_n) \rightarrow 2$ then as $n \rightarrow \infty, a_n \rightarrow 2$.
So the limit of the sequence is $2$. Using the definition of convergence: $\forall \epsilon > 0$ there exist natural numbers $N,n$ where $n>N$ such that
$|2 + 4(- \frac{1}{2})^n - 2| < \epsilon$ Which simplifies to $|4(- \frac{1}{2})^n| < \epsilon$ and $4(\frac{1}{2})^n <
\epsilon$ Then, given $\epsilon > 0$ choose $N$ such that the minimum value of $N$ is
$\log_2(\frac{4}{\epsilon})$ such that for all $n > N, a_n < \epsilon$ This can be seen as \begin{equation*}
  \begin{aligned}
    N &> \log_2(\frac{4}{\epsilon}) \\
    2^N &> \frac{4}{\epsilon} \\
    \frac{1}{2^N}  &< \frac{\epsilon}{4} \\
    \frac{4}{2^N}  &< \epsilon \\
  \end{aligned}
\end{equation*} As required. This proves that $a_n \rightarrow 2$ Not sure if this is correct but could someone please check and if there is an easier way of proving this let me know!","['induction', 'convergence-divergence', 'sequences-and-series', 'proof-verification']"
2085415,Weird Integration problem: $\int_{-2}^{2} \frac{x^2}{1+5^x}dx$ [duplicate],This question already has answers here : Integrating $\int^2_{-2}\frac{x^2}{1+5^x}$ (2 answers) Closed 7 years ago . $\int_{-2}^{2} \frac{x^2}{1+5^x}dx$ I am stuck at the first step and have tried replacing $5^x$ with $e^{\ln(5^x)}$ but nothing simplifies out in the end. Any hints how I should proceed?,"['integration', 'definite-integrals']"
2085417,Make this visual derivative of sine more rigorous,"Is this the correct way to make this visualization of the derivative of sine more... rigorous?  At least, for $u\in(0,\pi/2)$. Borrowed from Proofs without words .  To try to make this rigorous, I argued that when $u\pm\Delta u$ is in the first quadrant, that we have the following geometrically obtained bounds: $$\frac{\sin(u+\Delta u)-\sin(u)}{\Delta u}<\cos(u)<\frac{\sin(u-\Delta u)-\sin(u)}{-\Delta u}$$ where $\Delta u>0$.  From this, I thought that the derivative of sine comes trivially, though I am wondering if this could be more rigorous.","['derivatives', 'trigonometry', 'alternative-proof']"
2085442,Why isnt $a$ applied on $k$ in the equation $y=a[b(x-h)]+k$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question For instance, if applying a vertical stretch by a factor of $5$ on the graph of  $y=x^2+1$ gives a new equation of $y=5(x^2+1)$, why isn't the same done in the equation of transformations $y=a[b(x-h)]+k\,?$ Shouldn't it be $y=a[b(x-h)+k]\,?$","['algebra-precalculus', 'transformation', 'functions']"
2085473,"Probability: there are $n$ rooms, and $m$ meetings, $m \leq n$, what's the probability of all meetings scheduled to a different room","Quite new in stats... definitely not my strong area. I came across this probability question, and I am not sure how to do this! The question goes: pretend that there's this meeting scheduling engine used by this
  company and is not synced in real time, so when people schedule their
  meetings online to book a room, there may be overlaps. let's say there
  are $N$ rooms, and $M$ meetings, where $M \leq N$, what is the
  probability that all meetings scheduled to a different room? My thought was that, the first meeting doesnt matter, can be in any room; then the 2nd meeting has $\frac{1}{N-1}$ chance of being in a room. so for two rooms not colliding, the chance of them being in separate rooms is $\frac{1}{N-1}$. Right? I am not confident about this one neither... Any hint/advice/guidance helps! update to clarify: 1) each room can only host up to one meeting 2) one meeting can only happen in one room",['probability']
2085487,Some question regarding how probability is defined,"Consider an arbitrary discrete probability distribution with sample space $\Omega$ and let $\omega\subset\Omega$. Let $n$ denote the amount of independent trials of an experiment that are performed and let $\operatorname{f}(n)$ equal the amount of times $\omega$ occurs during those $n$ trials. It is my understanding that  $\operatorname{P}(\omega)=\lim_{n\to\infty}\operatorname{f}(n)/n$. Is $\operatorname{f}$ essentially ""pure randomness""? I mean we can't necessarily be certain about what value we acquire from $\operatorname{f}$ when evaluated at $n$. I'm used to a function giving me the same number when I iteratively evaluate it at the same number, but this isn't the case now is it? Does it make sense for $\operatorname{f}$ to exist philosophically? If ""pure randomness"" determines the value of $\operatorname{f}(n)$ in the sense that we can never be $100$% certain about we value it will yield, how do we define ""pure randomness""? Since $\operatorname{f}$ is not a normal function like those in calculus, how do we define the convergence of $\operatorname{P}(\omega)=\lim_{n\to\infty}\operatorname{f}(n)/n$? Does the $\delta$-$\epsilon$ kind of definition apply here as well? How rigorous is this definition generally speaking? In addition to that, how do we define probability for continuous probability distribution in a more rigorous way?","['probability-theory', 'functions', 'probability-distributions', 'convergence-divergence', 'discrete-mathematics']"
2085531,"If $H \leq G$ has finite index, then $G$ has a normal subgroup of finite index [duplicate]","This question already has answers here : For $G$ group and $H$ subgroup of finite index, prove that $N \subset H$ normal subgroup of $G$ of finite index exists (3 answers) Closed 2 years ago . From a practice test for a Masters Qual. Exam: Let $H$ be a subgroup of finite index in a group $G$. Prove that $G$ has a normal subgroup $K$ of finite index with $K \subseteq H$. It makes perfect sense intuitively that $K$ should exist, I just don't know how to get ahold of it.",['group-theory']
2085541,Is $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ continuous? or differentiable anywhere?,"Does it make sense to call $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ continuous? For any finite n it is easy to declare that the function is continuous. Now when we let $n \to \infty$ we get a function that would seem pretty continuous, with each value very very close to its neighbors. However when we attempt to take the derivative anywhere on the function (except for a few key points) we find due the infinitesimal and ever present oscillations inherent in the function it doesn't really make sense to take the derivative. My hunch is to say that $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ is continuous but not differentiable. But how can one justify that? PS: if you want to see an interactive, zoomable version of the function PPS:
Could the function possibly be differentiable at $f(2\pi k)$ for $k =$ some integer. At $2\pi k $ the derivative of $ \frac{\sin(2^nx)}{2^n}$ is $\frac{1}{2^n}$ for all n. Does that imply diferentiablitity at $2 \pi k$?","['derivatives', 'fourier-series', 'continuity']"
2085558,Branch points and Branch cuts,"I am currently studying about Branch points and Branch cuts. I think, I understood the definition of these two concepts and I can find Branch points and Branch cuts of some functions. For example: $z\to\sqrt{z(1-z)}$ has two branch points $0$ and $1.$ Because as we travel along a small circle around $0$ or $1,$ one time, argument of the function changes to $0\to \pm\pi i.$ On the other hand, $z\to\sqrt{z}+\sqrt{1-z}$ has three branch points $0, 1$ and $\infty.$ (Correct me if I am wrong.) Here my question is: There are identities which fails on some branches of logarithm. Is there any way to determine the region for which these familiar identities valid using Branch points and Branch cuts? For example: $$\color{Green}{\sqrt{z-1}\sqrt{z+1}=\sqrt{z^2-1}}$$ does hot holds for $z=-2,$ if we choose $\sqrt{-1}=i.$","['complex-numbers', 'branch-points', 'complex-analysis', 'analytic-functions', 'branch-cuts']"
2085613,Which symbol can be used to refer to identity matrix?,$I$ is commenly used as a notation of identity matrix. I am wondering is there any notation else for identity matrix?,['linear-algebra']
2085620,Closed formula for some cotangent series,"I managed to compute some cotangent series, such as $~\displaystyle\sum\limits_{{\substack{i=1\\10~\nmid~i}}}^{\infty}\frac{\cot\left(\dfrac{9\pi}{10}\cdot i\right)}{i}=-\frac{6\pi}{5},~$ and $\displaystyle\sum\limits_{{\substack{i=1\\11~\nmid~i}}}^{\infty}\frac{\cot\left(\dfrac{4\pi}{11}\cdot i\right)}{i}=\frac{3\pi}{11}.~$ I am interested in the general case $$S_{k,~n}~=~\sum\limits_{{\substack{i=1\\n~\nmid~i}}}^{\infty}\frac{\cot\left(\dfrac{k\pi}{n}\cdot i\right)}{i}$$ where $k,~n$ are positive integers with $k<n$ and $\gcd(k,n)=1.~$ References about a closed form (if there is one) or some nice properties will be appreciated.","['sequences-and-series', 'analysis']"
2085629,Canceling out integral,"Bear with my naivety, I wanted to ask if it is possible to cancel out $\int$ with a $\frac{\mathrm d}{\mathrm dx}$. I had $\frac{\partial}{\partial v}$ in a question and I took $\partial v$ to the other side and took integration on both sides. So now on left hand side only $\partial$ remains so will it cancel out with the integral?","['integration', 'differential']"
2085671,Integral of $\sec^2{x} \tan^2{x}$,"I'm once again stuck; I'm trying to find $$\int{(\sec^2{x} \tan^2{x})dx}$$ but end up with things like: $\tan^2x-\int{2\tan^2x \sec^2x}$, which doesn't help. Would it be best to approach using integration by parts or substitution?",['integration']
2085680,Dimensional Analyses in Integrals Producing Logarithms,"Say I have equations of the form $f(x) = 1/x$ (to illustrate the simplest case of the problem) or $g(x) = \frac{1}{a - b \cdot x/c}$ and I want to find the integrals. Normally, these would both be pretty straightforward. $\int f(x)dx = \ln(x) + C$, and $\int g(x)dx = -\frac{c}{b}\ln(a \cdot c - b \cdot x) + C$. Suppose now, however, that $a$, $b$, $c$, and $x$ are dimensionful quantities; more specifically, $a$ has the same units as $b$ and $x$ has the same units as $c$. We would expect that $\int \frac{dx}{x}$ should be dimensionless, and indeed the $\ln$ function does have a pure number result... but the argument to $\ln$ should also be dimensionless, which $x$ is not , which leads to a problem in interpretation. Similarly, $\int g(x)dx$ should have units of $type(x)$ over $type(a)$, which indeed it does (since we defined $x$ & $c$ to have the same units, and $a$ and $b$ to have the same units), but it also involves a non-dimensionless argument to $\ln$. So, how can this be resolved? How does one go about integrating inverse units in a way that makes sense?","['integration', 'dimensional-analysis']"
2085687,Proving $a\cos x+b\sin x=\sqrt{a^2+b^2}\cos(x-\alpha)$,"Show that $a\cos x+b\sin x=\sqrt{a^2+b^2}\cos(x-\alpha)$ and find the correct phase angle $\alpha$ . This is my proof. Let $x$ and $\alpha$ be the the angles in a right triangle with sides $a$ , $b$ and $c$ , as shown in the figure. Then, $c=\sqrt{a^2+b^2}$ . The left-hand side is $a\cos x+b\sin x=\frac{ab}{c}+\frac{ab}{c}=2\frac{ab}{c}$ . The right-hand side is $\sqrt{a^2+b^2}\cos(x-\alpha)=c\left(\cos{x}\cos{\alpha}+\sin{x}\sin{\alpha}\right)=c\left(\frac{ab}{c^2}+\frac{ab}{c^2}\right)=2\frac{ab}{c}$ . Is my proof valid? Is there a more general way to prove it? For the second part of the question, I think it should be $\alpha=\arccos\frac{a}{\sqrt{a^2+b^2}}=\arcsin\frac{b}{\sqrt{a^2+b^2}}$ . Is this correct?",['trigonometry']
2085742,Is it possible to write $\tan^{-1}(x)$ as a power series of $\tanh(x)$? [duplicate],"This question already has answers here : Relationship between $\tanh x$ and $\arctan x$ (7 answers) Closed 7 years ago . $\tan^{-1}(x)$ looks very similar to $\tanh(x)$ if $x$ is small enough. Look. But they diverge from each other as $x$ grows. And for very big $x$'s, They almost represent the constant functions $1$ and $\frac \pi 2$ (for $\tanh(x)$ and $\tan^{-1}(x)$, respectively). Is it possible to write $\tan^{-1}(x)$ as a power expansion of $\tanh(x)$? I mean can we say this? $$\tan^{-1}(x)=\sum^{\infty}_{i=0} \alpha_i \tanh^i(x)$$ The power series is the thing I want. Not the resemblance between them.","['hyperbolic-functions', 'trigonometry', 'power-series', 'approximation']"
2085798,show there is no C^1 homeomorphism from R^3 to R^2,Show that there is no C^1 homeomorphism from $R^3$ to $R^2$. I am fully aware that in general $R^m$ and $R^n$ are not homeomorphism by homology theory. I wonder if we add the condition $C^1$ we can have a proof using differential calculus. Here is what i've tried just in case someone ask: I figured that by rank theorem the differential of such map can't have rank 2. But i don't see any contradictions if the rank is 1 or 0. I don't think i go through the right way.,"['general-topology', 'differential', 'differential-topology']"
2085806,Measure acting on differential form,"In the context of measure theory, given a probability measure $\xi : \mathcal{B}(X) \rightarrow [0,1]$ and a (smooth) function $v:X\rightarrow \mathbb{R}$ where $X\subset \mathbb{R}^n$, we encounter the notation $$
\int_X v(x)\, \xi(dx)
$$ What is behind the notation $\xi(dx)$ formally ?
Do probability measures act on differential forms ? If so, what is this action formally ?","['differential-geometry', 'integration', 'probability', 'measure-theory']"
2085866,If $\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt$ then what is the value of $\phi(x)+\phi''(x)$?,"If $$\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt$$ then what is the value of $\phi(x)+\phi''(x)$ ? My Attempt: $$\phi'(x)=-\sin(x)-(x-x)\phi(x)\dfrac{d(x)}{dx}+(x-0)\dfrac{d(0)}{dx}=-\sin(x)$$ (using Leibniz Rule) $$\implies \phi''(x)=-\cos(x)$$ So, $$\phi(x)+\phi''(x)=-\int_0^x(x-t)\phi(t)dt$$ How to simplify this? The final answer is given as $-\cos(x)$ but I am not getting it. Please help.","['derivatives', 'integration', 'definite-integrals', 'calculus']"
2085878,Calculate the limit without using L'Hopital's Rule: $\lim\limits_{x \to 0} \frac{5x - e^{2x}+1}{3x +3e^{4x}-3}$,"Need to calculate the following limit without using L'Hopital's Rule: $$ \lim_{x \to 0} \frac{5x - e^{2x}+1}{3x +3e^{4x}-3} $$ The problem I'm facing is that no matter what I do I still get expression of the form 
$$ \frac{0}{0} $$
I thought maybe to use
$$ t = e^{2x} $$
But I still can't simplify it enough.. Thank you","['limits-without-lhopital', 'limits']"
2085880,Polar coordinates complex differentiation,"In Proof of Cauchy Riemann Equations in Polar Coordinates the complex differentation was given in polar coordinates as \begin{align} \notag
 f'(z) &= u_x+iv_x \\ \notag
       &= (\cos(\theta)U_r - \tfrac{1}{r}\sin(\theta)U_{\theta})+i(\cos(\theta)V_r - \tfrac{1}{r}\sin(\theta)V_{\theta}) \\ \notag
       &= (\cos(\theta)U_r + \sin(\theta)V_{r})+i(\cos(\theta)V_r - \sin(\theta)U_{r}) \\ \notag &= (\cos(\theta)- i\sin(\theta))U_r + i(\cos(\theta)-i\sin(\theta))V_r  \\ \notag
       &= e^{-i\theta}( U_r+iV_r)  \notag
\end{align} I am confused about the second line because if $x=r\cos\theta$ then \begin{align} \notag
 f'(z) &= u_x+iv_x \\ \notag
       &= \left(U_r \frac{\partial r}{\partial x}+ U_{\theta}\frac{\partial \theta}{\partial x} \right)+i\left(V_r \frac{\partial r}{\partial x}+ V_{\theta}\frac{\partial \theta}{\partial x} \right)\\ \notag
       &= \left(\frac{1}{\cos(\theta)}U_r -\frac{1}{r\sin(\theta)}U_{\theta}\right)+i\left(\frac{1}{\cos(\theta)}V_r -\frac{1}{r\sin(\theta)}V_{\theta}\right) \\ \notag &
\end{align} because $1=\cos\theta\frac{\partial r}{\partial x}$ and $1=-r\sin\theta\frac{\partial r}{\partial \theta}$ So where did $(\cos(\theta)U_r - \tfrac{1}{r}\sin(\theta)U_{\theta})+i(\cos(\theta)V_r - \tfrac{1}{r}\sin(\theta)V_{\theta}) $ come from?","['complex-analysis', 'ordinary-differential-equations', 'complex-numbers']"
2085910,PCA derivation by minimizing reconstrution squared error plus orthonormality constraints,"I'm trying to better understand the link between PCA and Matrix Factorization of the form $X \approx WH$. I've read somewhere that the PCA solution can be also derived from the following cost function (I'm not even sure whether this is the right norm to use) \begin{align}
argmin_{W,H} \frac{1}{N}\sum_{n=1}^N|| \mathbf{x}_n - \mathbf{Wh}_n ||^2 \qquad \text{subject to } W^{T} W = I
\end{align} which seems similar, if not equal, to the idea of the minimum-error formulation of PCA. According to the PCA derivation, $\mathbf{W}$ is the matrix of the eigenvectors of the covariance matrix of $\mathbf{X}$, and $\mathbf{h}_n$ is the projection of $\mathbf{x}_n$ given by $\mathbf{h}_n = \mathbf{W}^T \mathbf{x}_n$. I have followed the derivation of the minimum-error formulation in Bishop's PRML book , but I find it a bit unintuitive. What is the shortest way to derive the above minimization problem? (I'm thinking, for instance, of using Lagrangians)","['matrix-decomposition', 'statistics', 'optimization']"
2085960,Exact Differential Equations: Why does this working rule work?,"Let $M$ and $N$ be some functions of $x$ and $y$. Consider the differential equation $$Mdx+Ndy=0.$$
Suppose that this equation is exact, meaning that $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}.$$ Then the author of my textbook suggests the following working rule: $$\int M\text{(treating y as a constant)} dx+ \int N\text{(taking only those terms in N that do not contain x)} dy=c$$ where $c$ is a constant of integration. I want to know why this works in general.","['multivariable-calculus', 'ordinary-differential-equations']"
2085991,What is the 2017th digit (from the right) of $2017^{2016^{2015^{\cdots^1}}}$?,"What  is  the  value of the $2017^{\rm th}$ digit starting from right side for $$ {2017^{2016^{2015^{\ldots 1 }}}}?$$ My attempt: $$2017^{n} \equiv  x  \pmod {10^{2017}}  \quad \Longrightarrow \quad  { 7 }^{ n } \equiv  x \pmod {{ 10 }^{ 2017 }}.$$ I stopped at this point. 
So, first of all ""could you tell me some books which can improve my abilities in number theory"" and finally I hope that you can help me to figure this question out.","['algebra-precalculus', 'number-theory', 'modular-arithmetic']"
2086000,"Given a non-discrete topology on a finite set $X$, can you define a metric?",Let $ X $ be a finite set and $ \tau $ a topology in $ X $ different from the discrete topology. Is it possible to define a metric in the set $ X $ such that $ \tau $ is the topology associated with the metric?,"['general-topology', 'metric-spaces']"
2086014,"How to integrate $\frac{1}{z}$ around square with vertices $(1,1),(-1,1),(1,-1),(-1,-1)$?","My attempt $$\int_1^{-1} \frac{1}{x+i} \, dx+\int_1^{-1}
   \frac{i}{-1+i y} \, dy+\int_{-1}^1 \frac{1}{x-i} \, dx+\int_{-1}^1 \frac{i}{1+i y} \, dy=2\pi i$$ Is this correct?","['complex-analysis', 'integration', 'ordinary-differential-equations']"
2086035,Intuition behind $\zeta(-1)$ = $\frac{-1}{12}$ [duplicate],"This question already has answers here : To sum $1+2+3+\cdots$ to $-\frac1{12}$ (18 answers) Closed 7 years ago . When I first watched numberphile's 1+2+3+... = $\frac{-1}{12}$ I thought the sum actually equalled $\frac{-1}{12}$ without really understanding it. Recently I read some wolframalpha pages and watched some videos and now I understand (I think), that $\frac{-1}{12}$ is just an associative value to the sum of all natural numbers when you analytically continue the riemann-zeta function. 3Blue1Brown's video really helped. What I don't really understand is why it gives the value $\frac{-1}{12}$ specifically. The value $\frac{-1}{12}$ seems arbitrary to me and I don't see any connection to the sum of all natural numbers. Is there any intuition behind why you get $\frac{-1}{12}$ when analytically continue the zeta function at $\zeta(-1)$? EDIT (just to make my question a little clearer):
I'll use an example here. Suppose you somehow didn't know about radians and never associated trig functions like sine to $\pi$ but you knew about maclaurin expansion. By plugging in x=$\pi$ to the series expansion of sine, you would get sine($\pi$) = 0. You might have understood the process in which you get the value 0, the maclaurin expansion, but you wouldn't really know the intuition behind this connection between $\pi$ and trig functions, namely the unit circle, which is essential in almost every branch of number theory. Back to this question, I understand the analytic continuation of the zeta function and its continued form for $s < 0$ $$\zeta(s)=2^s\pi^{s-1}\sin\frac{\pi s}2\Gamma(1-s)\zeta(1-s)$$ and how when you plug in s = -1, things simplify down to $\frac{-1}{12}$ but I don't see any connection between the fraction and the infinite sum. I'm sure there is a beautiful connection between them, like the one between trig functions and $\pi$, but couldn't find any useful resources on the internet. Hope this clarified things.","['analytic-continuation', 'riemann-zeta', 'sequences-and-series']"
2086051,"Proof of Karlin-Rubin's theorem, detail about a real analysis fact.","Although the setting of this question is statistics, the question actually asks for a real analysis fact (monotone functions). Karlin-Rubin's theorem states conditions under which we can find a uniformly most powerful test (UMPT) for a statistical hypothesis: Suppose a family of density or mass functions $\{f(\vec{x}|\theta):\,\theta\in\Theta\}$ and we want to test $$\begin{cases} H_0:\,\theta\leq\theta_0 \\ H_A:\,\theta>\theta_0.\end{cases}$$If the likelihood ratio is monotone on a statistic $T(\vec{x})$ (that is, for every fixed $\theta_1<\theta_2$ in $\Theta$, the ratio $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}$ is nondecreasing on $\{\vec{x}:\,f(\vec{x}|\theta_2)>0\text{ or }f(\vec{x}|\theta_1)>0\}$ as a function of $T(\vec{x})$), then the test of critical region $\text{CR}=\{\vec{x}:\,T(\vec{x})\geq k\}$, where $k$ is chosen so that $\alpha=P(\text{CR}|\theta=\theta_0)$, is the UMPT of size $\alpha$. In all the proofs I have read (for instance, in page 22 here or in "" Statistical inference "" by Casella-Berger, 2n edition, page 391), it is (more or less) said: ""we can find $k_1$ such that, if $T(\vec{x})\geq k$, then $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}\geq k_1$, and if $T(\vec{x})<k$, then  $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}< k_1$"". I would understand that statement if the likehood ratio were strictly increasing, but what about the case in which it is constant? For example, if $X\sim U(0,\theta)$, the likelihood ratio is monotone on $T(\vec{x})=\max_{1\leq i\leq n}x_i$ ($n$ is the length of the sample $\vec{x}$), but not strictly increasing. EDIT: My questions are: Is the assertion between quotation marks true for every density or mass function with (not strictly) monotone likelihood ratio on $T$? And what about in the case of the uniform distribution? The second question has an answer below. I would like an answer for the first question , with claims based on real-analysis.","['real-analysis', 'monotone-functions', 'statistical-inference', 'hypothesis-testing', 'probability']"
2086093,Proof that if $ f''(x) + Cf(x) = 0$ then $f(x) = Acos(\sqrt{C}x) + Bsin(\sqrt{C}x)$ [solved],"The actual task is to proof:
$f''(x) + 25f(x) = 0$ then $f(x) = A\cos(5x) + B\sin(5x)$ for $A, B \in \mathbb{R}$ I tried to proof that for $g(x) = A\cos(5x)$ and $h(x) = B\sin(5x)$, but I can't get anywhere with that. Regards, Marco","['analysis', 'functions']"
2086124,Continuous with compact support implies uniform continuity,"This might be a duplicate but I tried googling the MSE site and could not find a satisfactory answer. Let $(X, d)$ be a metric space and $f$ be a real valued continuous function on $X$. Suppose $f$ has a compact support. Does this imply the uniform continuity of $f$? I tried proving this statement,  but only for locally connected spaces have I succeeded in doing so. Is thus true for general metric spaces? I just couldn't provide a proof (or a counterexample) by myself. Please enlighten me.","['general-topology', 'real-analysis', 'metric-spaces', 'uniform-continuity']"
2086161,Differential Operator McLaurin Series,"The Non-Homogeneous Differential Equation $$Ly=R$$
Where 
$$ L=F(D)= \sum_i a_iD^i $$ Is solved with the way below. $$ Ly=R \Rightarrow y=\frac{R}{L}=\frac{R}{F(D)}$$ Then we expand the qiotient $ \frac{1}{F(D)}$ into Taylor Series in the neighborhood of 0 (McLaurin). In order to expand F(D) we have to give $D=\frac{d}{dx} $ a value. How can we give a differential operator a value?","['fractional-calculus', 'taylor-expansion', 'ordinary-differential-equations', 'calculus']"
2086178,Definition of Product Topology,"Definition: If $X$ and $Y$ are topological spaces. The product topology on $X \times Y$ is the topology having basis the collection $\mathcal{B}$ of all sets of the form $U \times V$, where $U$ is an open set of $X$ and $V$ is an open set of $Y$ I've taken this definition from Munkres: Topology - A First Course and the notation he uses at times can be highly confusing. For example he uses $x \times y$ to denote an ordered pair $(x, y)$. Now I'm not sure in the definition whether $U \times V$ is denoting the cartesian product of the two sets, or the ordered pair $(U, V)$ So which of the following is the correct basis for the product topology as stated in the definition above, given that $\mathcal{T_X}$ is the topology on $X$ and $\mathcal{T_Y}$ is the topology on $Y$? $$\mathcal{B} = \left\{U \times V \ \middle| \ U \in \mathcal{T_X} \ \text{ and } \ V \in \mathcal{T_Y}\right\}$$
$$\mathcal{B'} = \left\{(U, V) \ \middle| \ U \in \mathcal{T_X} \ \text{ and } \ V \in \mathcal{T_Y}\right\} = \mathcal{T_X} \times \mathcal{T_Y}$$","['notation', 'product-space', 'elementary-set-theory', 'general-topology', 'definition']"
2086191,Maximum principle for harmonic functions,"I know the following classical maximum principle for harmonic functions: If $\Omega \subset \mathbb{C}$ is open and connected and $u \in
 C^2(\Omega)$ is harmonic, then $u$ has maximum (or minimum) in $\Omega$ $\implies$ $u$ constant. How can I prove that the theorem is true if the hypothesis is that $u$ has a local maximum (or minimum).","['complex-analysis', 'real-analysis', 'harmonic-functions']"
2086246,Independence of random variables $X$ and $X+Y$,"Let $X,Y$ be two random variables on a probability space $(\Omega, \mathcal{F},\mathbb{P})$. I want to prove the fact that
\begin{align}
X \text{ is indep}&\text{endent of } X+Y \\
&\iff \\
\exists c \in \mathbb{R}&: \mathbb{P}(X=c) =1. 
\end{align}
First, we consider the implication $(\implies)$. We know that
\begin{align}
\mathbb{P}(X=c, X+Y=c+y) = \mathbb{P}(X=c)\mathbb{P}(X+Y=c+y).
\end{align}
Which we could interpret as the conditional expectation
\begin{align}
(*) \qquad \mathbb{P}(X=c|X+Y=c+y) = \frac{\mathbb{P}(X=c, X+Y=c+y)}{\mathbb{P}(X+Y=c+y)}=  \mathbb{P}(X=c).
\end{align}
How to conclude from $(*)$ that $\exists c \in \mathbb{R}: \mathbb{P}(X=c) =1 $?","['characteristic-functions', 'probability-theory', 'probability']"
2086256,Showing that morphisms of $k$-schemes that agree topologically are equal.,"Let $k$ be a field, $X$,$Y$ schemes of locally finite type over $k$. Let $X$ be reduced, $k$ algebraically closed. I want to show: Two morphisms $f,g: X \rightarrow Y$ over $k$ are equal if and only if the underlying maps of topological spaces agree. Also I need an example why this fails if the field $k$ is not algebraically closed. Now, the ""only if"" part is clear, so I only have to prove the other direction. I`ve got some theorems I think one can work with, such as if $f,g$ agree on a dense, closed subset, then they are already equal. I cant seem to get in a position to use them yet though. I think one can try to reduce this problem to the affine case, but I am not really firm with working with schemes, and the whole algebraic geometry does not really suit me. This bears resemblance to another question asked, namely Morphisms of $k$-schemes who agree on $\overline{k}$-points. I was not able to construct the proof I need from this though, because the accepted answer uses seperatedness of $X$, which is not given or needed. Any help would be appreciated!","['schemes', 'algebraic-geometry']"
2086258,Convergence of series of random variables with random signs.,"Suppose that $\{X_{n}, n\ge 1\}$ are arbitrary random variables such that $\sum_{n}\pm X_{n}$ converges almost surely for any choice of signs $\pm 1$.  Show that $\sum X_{n}^2<\infty$ almost surely. [Hint: Consider $\sum_{n} B_{n}(t)X_{n}(\omega)$ where the random variables $\{B_{n},n\ge 1\}$ are Bernoulli.  Apply Fubini on the space of $(t,\omega)$.] Remarks This is problem 7.7.4(b) of Resnick's A Probability Path. We know that if $\sum \pm a_{i}$ converges almost surely, then $\sum a_{i}^2$ converges  if the $a_{i}$ are fixed (non-random).  This is discussed here, for example. It's also Lemma 7.6.1 of Resnick. The point of the problem seems to be to pass from knowledge of the result for fixed values of the $X_{n}$ to random values; and presumably that's the point of the hint about Fubini.  But I don't understand exactly what's needed to draw this conclusion.","['probability-theory', 'random-variables']"
2086289,"show that $a_{n+874}=a_{n}$,if such $a_{n+2}=\left\lceil \frac{4}{3}a_{n+1}-a_{n}+0.5\right\rceil$","Let the sequence $\{a_{n}\}$ be such that $a_{1}=1, a_{2}=100$ , and $$a_{n+2}=\left\lceil \dfrac{4}{3}a_{n+1}-a_{n}+0.5\right\rceil$$ Prove that the sequence $\{a_{n}\}$ is periodic. I have used a computer and found the periodic is $T=874$ , but how to prove it?","['recurrence-relations', 'periodic-functions', 'sequences-and-series']"
2086294,moments of gaussian distribution by taylor series expansion,"I have calculated characteristic function of the normal distribution 
$$f_X(k)=\exp\left(ika-\frac{\sigma ^{2}k^{2}}{2}\right)$$ and now I would like to find the moments, so I know that you could expand characteristic function by Taylor series $$
\begin{split}
f_X(k)
  &= \left(1 + \frac{1}{1!}\left(ika - \frac{\sigma^2k^2}{2}\right)
                 + \frac{1}{2!}\left(ika - \frac{\sigma^2k^2}{2}\right)^2
                 + \frac{1}{3!}\left(ika - \frac{\sigma^2k^2}{2}\right)^3
                 + \ldots \right)\\
  &= \left(1+\frac{(ik)}{1!}\left \langle X^1 \right \rangle+\frac{(ik)^2}{2!}\left \langle X^2 \right \rangle+\frac{(ik)^3}{3!}\left \langle X^3 \right \rangle+...\right)
\end{split}
$$ and the moments will be
$\left \langle X^n \right \rangle$ Now the problem is that I completely forgot how to evaluate Taylor series.
Could you be so kind and help me to calculate for example second moment? I know what the answer should be, but I couldn't get it right. Any help would be appreciated!","['taylor-expansion', 'characteristic-functions', 'probability-theory', 'probability', 'moment-generating-functions']"
2086315,Proof that a continuous function with $f(x) = f(x^2)$ is constant. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given a continuous function
$$
f: \mathbb{R} \rightarrow \mathbb{R} \quad \text{with} \quad f(x) = f(x^2) \quad\quad \forall x \in \mathbb{R}
$$ How can I show that $f$ must be constant?","['continuity', 'analysis', 'functional-equations']"
2086349,How to show that $f(z)=\sqrt{|xy|}$ satisfies the Cauchy Riemann equations but isn't differentiable at $z=0$?,"How to show that $f(z)=\sqrt{|xy|}$ satisfies the Cauchy Riemann equations but isn't differentiable at $z=0$? My Attempt $$
f(z)=u+i v,u=\sqrt{|xy|},v=0
$$
$$
v_{x}=0,v_{y}=0,u_{x}=\frac{|y|}{2\sqrt{|x y|}},u_{y}=\frac{|x|}{2\sqrt{|x y|}}
$$
$
\lim_{\Delta z\rightarrow 0}\frac{\sqrt{|(x+\Delta x)(y+\Delta y)|}-\sqrt{|(x)(y)|}}{\Delta x+i\Delta y}
$ I am not sure how the limit doesn't exist. Also, $u_x,u_y$ seem to become infinite at $z=0$.",['complex-analysis']
2086387,Why is it not possible to generate an explicit formula for Newton's method?,"Going through the recursive formula for approximating roots every time is extraordinarily tedious, so I was wondering why there was no formula that computed the $n$th iteration of Newton's method.","['numerical-methods', 'newton-raphson', 'calculus']"
2086393,Proving that $\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n} = \log 2$,"As stated in the title, I am trying to prove that $\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n} = \log2$, I think I need to use the Geometric Series like this: $$1+u+u^2+...+u^n = \frac{1-u^{n+1}}{1-u}$$ Then setting $u = -t$, rewrite it like so: $$1-t+t^2-...+(-1)^n t^n + \frac{(-1)^{n+1}t^{n+1}}{1+t}= \frac{1}{1+t}$$ But now I am not sure where to go next in order to get to my conclusion, any guidance and explanation would be appreciated, thanks!","['logarithms', 'real-analysis', 'sequences-and-series']"
2086412,Solution of $\sin z=p(z)$,"This is our school's previous Q.E. problem : Let $p(z)$ be a polynomial. Then $\sin z=p(z)$ has infinitely many solutions in $\mathbb{C}$ iff $p(z)$ is constant. One direction is easy ($\Leftarrow$), but for the other side I don't have any idea. Can you give some hints? Thanks.","['polynomials', 'complex-analysis', 'trigonometry']"
2086416,Prove that the norm of an integrable vector function is integrable,"This result seems basic but I couldn't find a proof anywhere. Suppose $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable. Let $\|\cdot\|$ be a norm on $\mathbb R^n$. I want to show that $\|\mathbf f(x)\|$ is Riemann integrable as a function $[a,b]\to\mathbb R$. Is this result true? How can I prove it? Thanks in advance! I know how to prove this result for specific norms like $\|\cdot\|_1$ and $\|\cdot\|_2$, but I don't know how to prove it for a general norm. btw I'm using the defintion that $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable if each of its components $f_i$ are Riemann integrable. Then the integral would be $\int\mathbf{f}(x)dx=(\int f_1(x)dx,\cdots,\int f_n(x)dx)$.","['multivariable-calculus', 'normed-spaces', 'integration', 'analysis']"
2086460,Why was it necessary for the Riemann integral to consider all partitions and taggings?,"Suppose we have a function $f:[a,b]\to\mathbb{R}$. The Riemann integral $\int$ of the function is defined somewhat like this: For any tagged partition $\dot{P}=(P,t_P)$, (where $P$ is the partition, and $t_P=\{t_i\}$ the corresponding tagging) of $[a,b]$, denote the Riemann sum of $f$ corresponding to $\dot{P}$ by $S(f,\dot{P})$. If $S(f,\dot{P})$ approaches a definite value $I$ as $||\dot{P}||\to0$, then $$\int f = I$$ By the definition, $\dot{P}$ runs over the set of all tagged partitions of [$a,b]$, taking into account any partitioning and any tagging for that particular partition. Is this much freedom of choice really necessary? From this question , I know that if we specify the choices for both $P$ and $t$, the integral is no longer equivalent to the Riemann integral (In the link, $P$ is always an equipartition, and $t$ is the set of the left endpoints of $P$). What if we fix exactly one of $P,t$? Is the resulting ""integral"" still equivalent to the Riemann integral? For example, if I define another integral $\int_1$ similarly (but specifying choice of $P$): For $n\in\mathbb{N}$ define $P_n$ to be the equipartition of $[a,b]$ into $n$ subintervals of equal length, and for any tagging $t_{P_n}$ of $P_n$ denote the corresponding tagged partition as $\dot{P}_{n,t}$. If $S(f,\dot{P}_{n,t})$ approaches a definite value $I_1$ as $n\to \infty$ then 
  $$\int_1f = I_1$$ I can similarly define some other $\int_2$ by considering all partitions, but 
specifying $t$, for example, by taking the mid-points of each subinterval of a given partition. Will this $\int_1$ or $\int_2$ be equivalent to $\int$? More or less, what is the advantage of simultaneously varying $P$ as well as $t$, which couldn't be achieved by doing that one at a time? [Most probably my definitions are not extremely precise, I apologise for those]","['real-analysis', 'soft-question', 'riemann-integration']"
2086483,Polygon can be covered by circle.,"I saw this problem from a math forum but it has been left unanswered for months. The problem states: ""Prove that every polygon with perimeter $2004$ can be covered by a circle with diameter $1002$ "" I have tried the following methods but i keep failing, Any hints for a possible method are appreciated: $1)$ I tried proving that all triangles with perimeter $2004$ can be covered by circle with diameter $1002$ and  then use strong induction to say that all such $n$-gons can be covered and then try to prove it for alla the $n+1$-gons $2)$ I tried to use contradiction but also failed.","['discrete-mathematics', 'geometry']"
2086540,Can we take negative numbers as base in taking log?,Can we take the negative numbers as base in taking log? For example:$$\log_{(-2)}4=2$$ $$\log_{(-3)}81=4$$ etc.,"['logarithms', 'complex-analysis']"
2086547,Determine whether the following sequence is increasing or decreasing $\frac{n^2+2n+1}{3n^2+n}$,"Determine whether the following sequence is increasing or decreasing: $$\frac{n^2+2n+1}{3n^2+n}$$ I'm not sure whether my solution is correct: $$\frac{n^2+2n+1}{3n^2+n}=\frac{n(n+2)+1}{n(3n+1)}=\frac{n+2}{3n+1}+\frac{1}{n(3n+1)}.$$
Let's prove $\frac{n+2}{3n+1}$ is a decreasing sequence. $$a_n>a_{n+1} \Leftrightarrow \frac{n+2}{3n+1}>\frac{n+3}{3n+4}\Leftrightarrow(n+2)(3n+4)>(n+3)(3n+1)\Leftrightarrow3n^2+10n+8>3n^2+10n+3\Leftrightarrow 8>3$$ So $\frac{n+2}{3n+1}$ is a decreasing sequence and we know that $\frac{1}{n(3n+1)}$ is also decreasing so our given sequence is a decreasing sequence as a sum of $2$ decreasing sequences.","['sequences-and-series', 'calculus']"
2086574,"Finding a Mobius transformation that maps the upper half-plane to the inside of a unit-disk, such that point $i$ is mapped to $0$ and $\infty$ to $-1$","Finding a Mobius transformation that maps the upper half-plane $\{Im(z)>0\};z \in \mathbb{C}$ to the inside of a unit-disk, such  that point $i$ is mapped to $0$ and $\infty$ to $-1$. Okay, to be clear, I know that a Mobius transformation $w$ is of the form: $$w=\frac{az+b}{cz+d};ad-bc\neq0.$$
I am very aware of what a unit disk is. I have done assignments like finding an mobius transformation that maps some points ($\mathbb{C}$) $a,b,c$ to $d,e,f$. -Where these points were not infinity. But nothing like this problem that I have here? How is this done? I think I just need one more point and know it's picture to be able to figure is out. But how?","['complex-analysis', 'mobius-transformation', 'calculus', 'functions']"
2086600,Understanding the proof of $\displaystyle\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$,"I need help understanding this: Prove that $$\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$$ $$(\sqrt n)^\frac{1}{n}\geq 
(\sqrt1)^\frac{1}{n}=1, \forall n\in\Bbb N$$
  By binomial theorem we get for $n\geq 2$ $$n=((\sqrt n)^\frac{1}{n})^n=[1+((\sqrt n)^\frac{1}{n}-1)]^n=\sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k$$
  $$\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2=1+\frac{n(n-1)}{2}(\sqrt n^\frac{1}{n}-1)^2$$
  $$\Rightarrow (\sqrt n^\frac{1}{n}-1)^2\leq \frac{2}{n}$$
  $$\Rightarrow \sqrt n^\frac{1}{n}\leq 1+\frac{\sqrt 2}{\sqrt n}$$
  $1+\frac{\sqrt 2}{\sqrt n}$ approaches $1$ for $n \rightarrow \infty$ so by the sandwich theorem we get $\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$ I don't understand this part: 
$$
\sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2.$$ Could someone explain how they got that on the RHS?","['real-analysis', 'limits']"
2086630,"Prove that if $A$ and $B$ are subsets of a topological space $(X, \mathcal{T})$, then $\overline{A \cup B} = \overline{A} \cup \overline{B}$","Prove that  if $A$ and $B$ are subsets of a topological space $(X, \mathcal{T})$, then $\overline{A \cup B} = \overline{A} \cup \overline{B}$ My Proof We'll use the notation, $U_x$ to be an open set of $X$ containing $x$, (i.e $U_x := x \in U$ and $U \in \mathcal{T})$. Now $x \in \overline{A \cup B}$ if every $U_x \cap (A \cup B) \neq \emptyset \iff (U_x \cap A) \cup (U_x \cap B) \neq \emptyset$. (The logical equivalence here is as a result of elementary set operations). And we have $x \in \overline{A}$ if $U_x \cap A \neq \emptyset$ for every $U_x \in \mathcal{T}$, and similarly  $x \in \overline{B}$ if $U_x \cap B \neq \emptyset$ for every $U_x \in \mathcal{T}$, so that $x \in \overline{A} \cup \overline{B}$ if $(U_x \cap A \neq \emptyset) \lor (U_x \cap B \neq \emptyset).$ Logically we have $(U_x \cap A \neq \emptyset) \lor (U_x \cap B \neq \emptyset) \iff (U_x \cap A) \cup (U_x \cap B) \neq \emptyset$, and hence it follows by the biconditional  above that $x \in \overline{A \cup B} \implies  x \in \overline{A} \cup \overline{B}$ and that $x \in \overline{A} \cup \overline{B} \implies x \in \overline{A \cup B}$ and thus we have $\overline{A \cup B} = \overline{A} \cup \overline{B}$. $\square$ Is my proof correct? If so how rigorous is it? I'm looking to improve my proof-writing skills, and the rigor in my proofs, so any comments and criticism, however critical they may be is greatly appreciated.","['alternative-proof', 'proof-verification', 'proof-writing', 'elementary-set-theory', 'general-topology']"
2086643,What is a resolvent of an operator?,"What is a resolvent $R$ of an operator $L$ , and why do we care about it? $$R=(\lambda I-L)^{-1}.$$","['functional-analysis', 'spectral-theory', 'terminology']"
2086664,Prove that the area of a square inscribed in the ellipse $b^2x^2+a^2y^2=a^2b^2$ is equal to $\frac{4a^2b^2}{a^2+b^2}$,"I have this question for a math assignment Prove that the area of a square inscribed in the ellipse $b^2x^2+a^2y^2=a^2b^2$ is equal to $\frac{4a^2b^2}{a^2+b^2}$ I tried approaching this question the same I would a trig identity, but it doesn't seem to work and the question is different since it's not trig, but conics. Does anyone know how to approach this question? I'm having a hard time and would really appreciate it. Thanks!","['conic-sections', 'area', 'functions']"
2086670,Proving Lebesgue Integration Yields Same Answer As Riemann,"I am working on extended essay for my International Baccalaureate Programme Full Diploma, and I chose mathematics as my topic.  My research questions is: What are the shortcomings of Riemann integration, why do they exist, and how do other forms of integration overcome those limitations? The first alternative I will discuss is Lebesgue integration.  By starting with a graphical analysis of a bijective function $f\left( x\right)$ whose limit will be evaluated from $x=a$ to $x=b$, I have attempted to write the process as a unified equation, $$L=\small\lim_{n\to\infty}\sum_{i=1}^{n}\left(\left[ f^{-1}\left( a+i\left[\dfrac{f\left( b\right) -f\left( a\right)}{n}\right]\right)-f^{-1}\left( a+\left[ i-1\right]\left[\dfrac{f\left( b\right) -f\left( a\right)}{n}\right]\right)\right]\left( a+i\left[\dfrac{f\left( b\right) -f\left( a\right)}{n}\right]\right)\right)$$ By comparison, my formula for Riemann integration follows the form $$R=\lim_{n\to\infty}\sum_{i=1}^{n}f\left( a+\frac{i\left( b-a\right)}{n}\right)\left(\frac{ b-a}{n}\right)$$ I am having extreme difficulty proving that $L=R$, since I do not know how to algebraically evaluate the limit of a series.  However, I did try to test my formulae for $f\left( x\right)=x^2+1$, $a=0$ and $b=a$.  While the formula $R$ has successfully yielded the correct answer under every trial, I have only gotten $L$ to converge to $R$ when integrating symmetrical trigonometric functions; in this case, however, $L$ ended up appearing to be undefined for all $x>0$. Does anyone have any suggestions?  Have I made a key mathematical error? an arithmetical one?  I pulled my information from Arturo Magidin's answer under Lebesgue Integration Basics. Thank you for any help you can give.","['integration', 'lebesgue-integral', 'riemann-integration']"
2086686,Is infinite intersection empty if finite intersections are?,Suppose that $ A_1 \cap A_2 \cap \dots \cap A_n = \emptyset $ for all  $n \in \mathbb{N}$ Is it also true that $ A_1 \cap A_2 \cap \dots = \emptyset$ ? Could I have a hint as how to think about disproving this? (I'm assuming it's false with some fancy counterexample that I'm probably not going to guess...),['elementary-set-theory']
2086695,Why $\frac{\partial f}{\partial v}=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i}$,"Let $U\subset \mathbb R^n$ be an open set and $f:U\to \mathbb R^m$ and suppose $\alpha=(\alpha_1,\ldots,\alpha_n)\in \mathbb R^n$ I'm trying to prove that $\frac{\partial f}{\partial v}(\alpha)=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i}(\alpha)$. I know that the directional derivative is $$\frac{\partial f}{\partial v}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+tv)-f(\alpha)}{t}$$ and the partial derivatives are
$$\frac{\partial f}{\partial x_j}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+te_j)-f(\alpha)}{t}$$ I've just written down these formulas without any success.","['multivariable-calculus', 'real-analysis']"
2086697,Derivative of $ \frac{\partial A^{T} X^{-1}A}{\partial X}$,"I looked at matrix cook book  and found an expression that is close
$ \frac{\partial a^{T} X^{-1}b}{\partial X}=-X^{-T}ab^{T}X^{-T}$ . But it seems a and b are vectors. While in my case I have matrices. Any help is appreciated.","['matrices', 'matrix-calculus', 'multivariable-calculus', 'derivatives']"
2086737,How do I compute the monodromy action of rotating three punctures of $\mathbb{P}^1$ on the fundamental group.,"Let $M\subset\mathbb{C}^3$ be the orbit of the circle group $S^1\subset\mathbb{C}^\times$ on the tuple $(1,e^{2\pi i/3},e^{4\pi i/3})$ (by multiplication). Thus, $M$ is homeomorphic to $S^1$, and so has fundamental group $\mathbb{Z}$. Now consider the fiber bundle $X\subset\mathbb{P}^1_{\mathbb{C}}\times M$ defined as the subspace obtained by removing the points $\{a,b,c\}$ from each fiber $\mathbb{P}^1\times\{(a,b,c)\}$. Thus, $X$ is a fiber bundle over $M$ with fibers which are projective lines minus 3 points. This bundle admits the constant section ""$\infty$"". Let $(a,b,c)\in M$, then we get an exact sequence of fundamental groups
$$1\rightarrow \pi_1(\mathbb{P}^1-\{a,b,c\})\rightarrow\pi_1(X)\rightarrow\pi_1(M)\rightarrow 1$$
which is split by the section ""$\infty$"", from which we get an action 
$$\pi_1(M)\rightarrow\text{Aut}(\pi_1(\mathbb{P}^1-\{a,b,c\}))$$
By ""drawing pictures"", I'm pretty confident this action is trivial. How can we prove this?","['algebraic-topology', 'algebraic-geometry']"
2086744,Differentiable $f\colon I\to\mathbb{C}$ with bounded derivative is Lipschitz continuous,"Prove that a differentiable function $f\colon I\to\mathbb{C}$ on an interval $I$ with bounded derivative is Lipschitz continuous, i.e. If $\lvert f'\rvert\leq L $ for some $L\in\mathbb{R}$, then for any $x_1,x_2\in I$ we have
$$
\lvert f(x_1)-f(x_2)\rvert\leqslant L\lvert x_1-x_2\rvert.
$$ Despite two little things, I think the proof should work as follows: I think, we can choose some $c\in\mathbb{C}$ with $\lvert c\rvert =1$ such that
$$
\lvert f(x_2)-f(x_1)\rvert = c\cdot (f(x_2)-f(x_1))~~(*)
$$
since for 
$$
\frac{f(x_2)-f(x_1)}{\lvert f(x_2)-f(x_1)\rvert}=:v
$$
we have $\lvert v\rvert =1$ and then we can define $c:=v^{-1}$. Next, in order to have equation $(*)$, the LHS has to be the real part of the RHS, i.e.
$$
\lvert f(x_2)-f(x_1)\rvert =\Re(c\cdot (f(x_2)-f(x_1)))=\varphi(x_2)-\varphi(x_1),
$$
where
$$
\varphi:=\Re(cf).
$$ Now, since $f$ is differentiable on $I$, it is, in particular, continuous on $[x_1,x_2]$, hence $\varphi$ is also continuous on $[x_1,x_2]$. I am not sure about the following question: Do we also have that $\varphi$ is differentiable on $(x_1,x_2)$? (Q) Assuming that we can answer question (Q) with YES, we could apply the mean value Theorem on $\varphi$, telling us that 
$$
\varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi)
$$ 
for some $\xi\in (x_1,x_2)$. By assumption, $\varphi'(\xi)=\Re(cf'(\xi))\leq\lvert cf'(\xi)\rvert\leq L$ and hence
$$
\lvert f(x_2)-f(x_1)\rvert = \varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi)\leqslant L\lvert x_2-x_1\rvert.
$$ I am also not completely sure if
    $$
\varphi' = \Re(cf')
$$
    is correct. Despite the two things in the two yellow boxes, I am pretty sure the proof should work. It would be nice if you could give me some hints.","['derivatives', 'real-analysis', 'continuity']"
2086746,"Finding $B,C$ such that $B\left[\begin{smallmatrix}1&2\\4&8\end{smallmatrix}\right]C=\left[\begin{smallmatrix}1&0\\0&0\end{smallmatrix}\right]$","State $B,C$, such that $B\begin{bmatrix} 1 & 2  \\ 4 & 8    \end{bmatrix}C=\begin{bmatrix}
1 & 0  \\
0 & 0  
\end{bmatrix}$ I tried some things, but I ended up with non invertible matrices, so I stopped for the moment. Is there a quick way to guesswork here? (it's an assignment so, maybe some ""nice"" values for the entries will do) I don't even know how I multiply a matrix to get 0-Entries with invertible matrices. So maybe let's start at that part?","['matrices', 'systems-of-equations']"
2086756,How can I solve this comparsion between sums?,$Suppose\;m\;intergal\;and\;m\ge2$ $Which \;of\; these \;sums\; is\; asymptotically\; closer \;to\; the\; value\; log_mn!?$ $ \sum_{k=1}^n\lfloor\;log_m k\;\rfloor$ $Or$ $\sum_{k=1}^n\lceil\;log_m k\;\rceil$ Has anything to do with Stirling's formula?,"['summation', 'ceiling-and-floor-functions', 'discrete-mathematics']"
2086814,The matrix of a projection can never be invertible,"I am currently studying linear transformations in order to refresh my knowledge of linear algebra. One statement in my textbook (by David Poole) is: When considering linear transformations from $\mathbb{R}^2$ to $\mathbb{R}^2$, the matrix of a projection can never be invertible. I know that a projection matrix satisfies the equation $P^2 = P$. Taking determinant of both sides gives $$\text{det}(P)^2 = \text{det}(P)$$ which is always true when $P$ is singular. However take $\color{blue} {P = I_2}$, then the equality is true and the projection matrix is invertible. What mistake do I make in my reasoning?","['projection-matrices', 'matrices', 'linear-transformations', 'inverse', 'linear-algebra']"
2086817,$\widetilde{(M \otimes_A N)} \cong \tilde{M} \otimes_{\mathcal O_X} \tilde{N}$,"Let $A$ be a ring, $X = \textrm{Spec } A$, and $M$ an $A$-module.  For $U$ open, define $\tilde{M}(U)$ to be the abelian group consisting of all $m_{\mathfrak p} \in \prod\limits_{\mathfrak p } M_{\mathfrak p}$ such that locally, $m_{\mathfrak p} = \frac{m}{a}$ for some $m \in M, a \in A$.  Then $\tilde{M}$ is a sheaf of $\mathcal O_X$-modules.  If $N$ is another $A$-module, then $\tilde{M} \otimes_{\mathcal O_X} \tilde{N}$ is the sheaf associated to the presheaf $$U \mapsto \tilde{M}(U) \otimes_{\mathcal O_X(U)} \tilde{N}(U)$$ which is also a sheaf of $\mathcal O_X$-modules.  I'm trying to understand why $$\widetilde{(M \otimes_A N)} \cong \tilde{M} \otimes_{\mathcal O_X} \tilde{N}$$ as $\mathcal O_X$-modules.  This is II, 5.2(b) in Hartshorne.  All that is written for the proof is ""$M \mapsto \tilde{M}$ commutes with tensor product, because this commutes with localization"" which I don't understand. I was trying to reason as follows, but I am not sure this is on the right track.  For $U$ open in $X$, let $S(U) = \{ a \in A : a \not\in \mathfrak p \textrm{ for all } \mathfrak p \in U \}$, which is multiplicatively closed.  Then I believe that $\mathcal O_X$ is the sheaf  associated to the presheaf of rings $\alpha: U \mapsto S(U)^{-1}A$, and $\tilde M$ is that associated to the presheaf of abelian groups $\beta: U \mapsto S(U)^{-1}M$. Then $\beta$ is a presheaf of $\alpha$-modules, which induces $\tilde M$ as a sheaf of $\mathcal O_X$-modules.  The same for $\gamma: U \mapsto S(U)^{-1}N$ and $\tilde{N}$.  Now $\delta: U \mapsto \beta(U) \otimes_{\alpha(U)} \gamma(U)$ is a presheaf of abelian groups whose associated sheaf is probably $\tilde{M} \otimes_{\mathcal O_X} \tilde{N}$.  But at the same time, $$\delta(U) = S(U)^{-1}M \otimes_{S(U)^{-1}A} S(U)^{-1}N \cong S(U)^{-1}(M \otimes_A N)$$ and the associated sheaf is $\tilde{M \otimes_A N}$.",['algebraic-geometry']
2086832,"Probability of first-passage, and conditional distribution of first-passage-time of a diffusion process with negative drift","Probability of first-passage by a particular time (and conditional distribution of first-passage-time) of a diffusion process with negative drift (This is a variation on Expectation of first-passage-time of a diffusion process with negative drift where more than just the particular expectation or Laplace transform seems necessary). Take  the stochastic process $X_0 = 0$ and $X_t = \nu t + \sigma W_t$  where $W_t$ is standard Brownian motion and $\nu$ is a drift which may be negative (this is the key complexity to the question) Let $\alpha > 0$ be a fixed level, then define the first passage time as the random variable: $T = \inf\{ 0 < t \mid X_t=\alpha \}$. Finally, define $0 < \bar{T} < \infty$ as a terminal time. I want to compute two things: $\mathbb{P}(T < \bar{T})$ $\mathbb{E}(e^{-r T} | T < \bar{T})$ Solving the $\nu \geq 0$ case: I believe this turns out to be easy as this follows the example in https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution#Relationship_with_Brownian_motion .  Hence, $T$ is distributed as an Inverse Gaussian: $T\sim IG(\tfrac\alpha\nu, \tfrac {\alpha^2} {\sigma^2})$ in that notation. Let $f(T;\tfrac\alpha\nu, \tfrac {\alpha^2} {\sigma^2})$ be the PDF and $F(T;\tfrac\alpha\nu, \tfrac {\alpha^2} {\sigma^2})$ be the CDF of the Inverse Gaussian, then we have our simple answer: $$\mathbb{P}(T < \bar{T}) = F(\bar{T})$$ and 
$$\mathbb{E}(e^{-r T} | T < \bar{T}) =  \frac{1}{ F(\bar{T})} \int_0^{\bar{T}}e^{-r T}f(T) dT$$ Of course, to eliminate the use of the PDF we could also write the expectation in terms of the counter-cdf of the right truncated CDF instead of the PDF. Solving the $\nu < 0$ case: As Karlin and Taylor puts it: ""When $\nu < 0$, $T$ has a defective probability distribution, that is $T$ is infinite with positive probability"". For that reason, a PDF cannot be used directly and the CDF would need to be used with care.  But can we use the PDF conditionally, as above, adjusting for the probability of $T$ being infinite?  Any ideas on how to do the equivalent claculations to the $\nu > 0$ case?","['stochastic-processes', 'expectation', 'measure-theory', 'stopping-times']"
2086836,Canonical riemannian metric on the cotangent bundle,"Given a riemannian manifold $(M,g)$, is there a canonical induced riemannian metric on the cotangent bundle? Motivation: See this related question . Also, this looks like it is a canonical metric on $TM$, so would it make sense to map $T^*M$ to $TM$ using $g$ and then take the pullback metric on $T^*M$?","['riemannian-geometry', 'differential-geometry']"
2086871,Optimizing over matrices in the Loewner ordering,"The so-called Loewner ordering introduces a partial ordering to the set of Hermitian matrices: $X \geq Y$ if $X - Y$ is positive semidefinite $X > Y$ if $X - Y$ is positive definite. Consider then the following two problems: $$\max \left\{ X \,|\, X \text{ satisfies some conditions } \right\}\tag{1}\label{opt1}$$ $$\max \left\{ \operatorname{Tr}X \,|\, X \text{ satisfies some conditions } \right\}\tag{2}\label{opt2}$$ where $X$ is Hermitian, and the maximum in \eqref{opt1} is with respect to the Loewner ordering. Are the problems equivalent?","['optimization', 'matrices', 'order-theory', 'positive-semidefinite', 'linear-algebra']"
2086943,Prove matrix $A^TD-C^TB=I$,"Let $A$, $B$, $C$ and $D$ be square matrices $n\times n$ over $ \mathbb{R} $ 
. Assume that $AB^T$ and $CD^T$ are symmetric . and $AD^T-BC^T=I$ Prove that:  $A^TD-C^TB=I$ I know this question has answer here: Prove that $A^TD-C^TB=I$ But I don't understand the accepted answer(Where that equation came from?): Hint: The given condition says that
  $$
\pmatrix{A&-B\\ -C&D}\pmatrix{D^T&B^T\\ C^T&A^T} = \pmatrix{I&0\\ 0&I}.
$$
  Now, note that $XY=I$ implies that $YX=I$ and in turn $X^TY^T=I$. Is there any simpler way? Do $A^TD $ and $C^TB$ must be symmetric?
And if so, how to prove it?","['matrices', 'linear-algebra']"
2086946,"Smallest number in the sequence $9,99,999,9999\cdots$ that is divisible by $p \in \mathbb{P}$.","To be more precise, $A(p)=\min\{x:p\mid10^x-1\}$ This seems to be different from $(p)$ but is always a factor of $(p)$. For example, $A(37)=3=(37)/12$. I would like to get a clear way of calculating $A(p)$ or a formula for it in terms of $(p)$ and other functions.","['modular-arithmetic', 'functions']"
2086961,Pythagorean Identities other than Trigonometric [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Are there any identities which follow the Pythagorean pattern,
$$a^2+b^2=c^2$$
besides the standard trigonometric and hyperbolic trigonometric Pythagorean identities (e.g. $\sin^2(\theta)+\cos^2(\theta)=1$) and those derived from them? 
Preferably, these functions shouldn't be reducible to the trig identities, but those are acceptable as generalizations. Alternatively, what other non-trivial functions parameterize the circle $x^2+y^2=1$? (Just like there are also functions that parameterize the Fermat cubic $x^3+y^3=1$ ?)","['functions', 'pythagorean-triples']"
2086967,Finite morphisms of degree $\leq 1$ at all points are injective (Vakil Exercise $19.1.C.$),"Exercise $19.1.C.$ in Ravi Vakil's notes reads as follows, with the bold part indicating the part I'm having trouble with: Suppose $\pi: X \rightarrow Y$ is a nite morphism whose degree at every point of Y is $0$ or $1$. Show that $\pi$ is injective on points (easy). If $p \in X$ is any point, show that $\pi$ induces an isomorphism of residue elds $\kappa(\pi(p))\rightarrow \kappa(p)$. Show that $\pi$ induces an injection of tangent spaces. Where here the degree of $\pi$ at $y$ means that dimension of the global sections of $\pi^{-1}(y)$ as a $\kappa(y)$ vector space. I can show that $\pi$ is injective on points, since the fibre over a point is a finite scheme over $\operatorname{Spec}$ of the residue field there and thus a finite, discrete set of points. Thus by the degree hypothesis, it is either empty or a single point with the same residue field, which is enough to prove injecttivity on points and that the residue fields of $p$ and $\pi(p)$ are isomorphic. I can't work out how to prove the injectivity on tangent spaces though. I know it's equivalent to surjectivity on the level of cotangent spaces, which is the same as asking for the image of $\mathfrak{m}_{\pi(p)}$ in $\mathfrak{m}_{p}/\mathfrak{m}_p^2$ to be the whole thing. I've tried playing around with Nakayama's lemma and using that $\mathfrak{m}_{\pi(p)}$ and $\mathfrak{m}_{p}/\mathfrak{m}_p^2$ are both $\mathcal{O}_{Y,\pi(p)}$ modules to try to show that $\pi^{\sharp}(\mathfrak{m}_{\pi(p)})+\mathfrak{m}_p^2 = \mathfrak{m}_p$, but haven't had any luck so far and have run out of ideas.","['schemes', 'algebraic-geometry']"
2086995,Prove that $3 \mid (a+b+c+d)$,"Given $a,b,c,d \in \mathbb{Z}$ satisfying $a^3+b^3 = 2(c^3-8d^3)$, prove that $3 \mid (a+b+c+d)$. I first factorized $a^3+b^3$ to get $a^3+b^3 = (a+b)(a^2-ab+b^2)$. I wasn't sure how to use the right-hand side to get $a+b+c+d$. How can we prove that $3 \mid (a+b+c+d)$?",['number-theory']
2087000,"Show that if a linear operator between Banach spaces is not onto, then its image is a meager set","Let $X,Y$ be two Banach Spaces and let $T:X\to Y$ be a bounded linear operator. Show that either $T$ is onto or else $T(X)$ is a meager set. I assumed that $T$ is not onto .Then we need to show that $T(X)$ is meager i.e. it is a set of first category. Though my attempt is useless,I am still giving it.Will someone please help me how to proceed here.","['functional-analysis', 'normed-spaces', 'banach-spaces']"
2087009,What do parallel lines look like on the pseudosphere?,"I know that because the pseudosphere is a model for hyperbolic geometry, there are infinitely many lines that can be drawn parallel to another given one through an external point. However I'm struggling to figure out a pictorial representation for this.","['hyperbolic-geometry', 'noneuclidean-geometry', 'geometry']"
2087035,Evaluate $\int_{0}^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\mathrm{d}\theta $,"Evaluate $$\int_{0}^{\pi }\theta ^{3}\log^{3}\left ( 2\sin\frac{\theta }{2} \right )\,\mathrm{d}\theta $$ Several days ago,I found this interesting integral from a paper about generalized log-sine integrals,but I can't remember the title of it. The answer of the integral is \begin{align*}
-\mathrm{Ls}_{7}^{\left ( 3 \right )}\left ( \pi  \right)&=\frac{9}{35}\log^72+\frac{4}{5}\pi ^{2} \log^52+9\zeta \left ( 3 \right )\log^42-\frac{31}{30}\pi ^{4}\log^32\\
&-\left [ 72\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{9}{8}\zeta \left ( 5 \right )-\frac{51}{4}\pi ^{2}\zeta \left ( 3 \right ) \right ]\log^22\\
&+\left [ 72\mathrm{Li}_{5,1}\left ( \frac{1}{2} \right )-216\mathrm{Li}_6\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_4\left ( \frac{1}{2} \right ) \right ]\log2+72\mathrm{Li}_{6,1}\left ( \frac{1}{2} \right )\\
&-216\mathrm{Li}_7\left ( \frac{1}{2} \right )+36\pi ^{2}\mathrm{Li}_5\left ( \frac{1}{2} \right )-\frac{1161}{32}\zeta \left ( 7 \right )-\frac{375}{32}\pi ^{2}\zeta \left ( 5 \right )+\frac{1}{10}\pi ^{4}\zeta \left ( 3 \right )
\end{align*} where $$\mathrm{Ls}_n^{\left ( k \right )}\left ( \alpha  \right ):=-\int_{0}^{\alpha }\theta ^{k}\log^{n-1-k}\left | 2\sin\frac{\theta }{2} \right |\mathrm{d}\theta $$ is the generalized log-sine integral and $$\mathrm{Li}_{\lambda ,1}\left ( z \right )=\sum_{k=1}^{\infty }\frac{z^{k}}{k^{\lambda }}\sum_{j=1}^{k-1}\frac{1}{j}$$ is the multiple polylogarithm. I found a beautiful way to solve the integrals below $$\int_{0}^{\frac{\pi }{2}}t^{2n}\log^{m}\left ( 2\cos t  \right )\mathrm{d}t $$ Let's consider $$\mathcal{I}\left ( x,y \right )=\int_{0}^{\frac{\pi }{2}}\cos\left ( xt \right )\left ( 2\cos t \right )^{y}\mathrm{d}t$$ By using Gamma function,the integral become $$\mathcal{I}\left ( x,y \right )=\frac{\pi \, \Gamma \left ( y+1 \right )}{2\Gamma \left ( \dfrac{x+y+2}{2} \right )\Gamma \left ( \dfrac{y-x+2}{2} \right )}$$ Then we can get $$\mathcal{I}\left ( x,y \right )=\frac{\pi }{2}\exp\left ( \sum_{k=2}^{\infty }\frac{\left ( -1 \right )^{k}}{k\cdot 2^{k}}\zeta \left ( k \right )\left [ \left ( 2y \right )^{k}-\left ( y-x \right )^{k}-\left ( x+y \right )^{k} \right ] \right )$$ On the other hand,using taylor series $$\mathcal{I}\left ( x,y \right )=\sum_{n=0}^{\infty }\frac{\left ( -1 \right )^{n}}{\left ( 2n \right )!}x^{2n}\sum_{m=0}^{\infty }\frac{y^{m}}{m!}\int_{0}^{\frac{\pi }{2}}t^{2n}\log^m\left ( 2\cos t \right )\mathrm{d}t$$ So,the comparison of coefficient shows the answer.For example $$\int_{0}^{\frac{\pi }{2}}t^{2}\log^2\left ( 2\cos t \right )\mathrm{d}t=4\cdot \frac{\pi }{2}\left [ \frac{12}{4\cdot 16} \zeta \left ( 4 \right )+\frac{1}{2}\frac{8}{2^{2}\cdot 4^{2}}\zeta \left ( 2 \right )^{2}\right ]=\frac{11}{1440}\pi ^{5}$$ I wonder can we use the same way to prove the integral in the beginning,if not,is there another way to handle it?","['integration', 'polylogarithm', 'calculus', 'analysis']"
2087053,"Can anyone provide link or proof to the claim $\xi(1/2+it)$ is purely real""?","The link http://mathworld.wolfram.com/Xi-Function.html claims ""$\xi(1/2+it)$ is purely real"". Looking at the definition of Landau $\xi$ function, this doesn't seem trivial. I have been trying to find a proof and haven't found one yet. Can someone point me to a resource with such proof? Thanks",['complex-analysis']
2087055,Methods for constructing symplectic forms,"Suppose I am given a $2n$ dimensional manifold $M$ and I want to put a symplectic form on it. How could I determine whether or not it admits one and if I do know that it admits one what are my options for (explicitly) constructing some? My current knowledge of techniques is the following: (1) If we are in $\mathbb R^{2n}$ or $\mathbb C^n$ then we can take global coordinates and construct the standard ones . (2) If we are on a tangent bundle we can construct the canonical one using the tautological 1-form . (3) If the Riemannian holonomy (assuming we put some metric on $M$ -- although I'm confused about why the metric we choose matters...) group is contained in the symplectic group then we can transfer symplectic forms on $\mathbb R^{2n}$ to $M$. My understanding here though is that we can say the form looks a certain way in certain coordinates at a point on the manifold but not really what it looks like locally. (4) Brute force- define locally in coordinates and then show well-defined. (5) And I am aware of this cohomology existence result . So are there any other techniques?  In particular, ways like method $(2)$, where one can construct the form globally but still know what it looks like locally.","['symplectic-geometry', 'differential-forms', 'differential-geometry']"
2087062,"Normalizer of upper triangular group in ${\rm GL}(n,F)$","The following question has already appeared on mathstack: If $B$ is the subgroup of ${\rm GL}(n,F)$ consisting of upper triangular matrices then normalizer of $B$ in ${\rm GL}(n,F)$ is $B$ itself. I know a proof of this using Bruhat decomposition of ${\rm GL}(n,F)$. Question: Can we prove above theorem without using Bruhat decomposition? Why came to this question: Consider the general linear Lie algebra $L=\mathfrak{gl}(n,F)$; in it, let $T=\mathfrak{t}(n,F)$ be the upper triangular sub-algebra. Then normalizer of $T$ in $L$ is $T$ itslef, and this can be proved just by considering a very simple decomposition of ${\mathfrak gl}(n,F)$: write any element as sum of upper triangular matrix and lower triangular matrix whose diagonal is $0$. But then for problem above, is it necessary to use Bruhat decomposition?","['alternative-proof', 'abstract-algebra', 'group-theory']"
2087073,Elementary Reference for Algebraic Groups,The algebraic groups by definition come as algebraic varieties; the homomorphisms there are considered as morphisms of varieties. So they come with some basic tools of algebraic geometry. But I have never gone into serious study of algebraic geometry. I don't know how much of algebraic geometry is needed to study basic theory of algebraic groups. Can you suggest very elementary book on algebraic groups? The book by Borel in first glance I found difficult.,"['reference-request', 'algebraic-groups', 'algebraic-geometry']"
2087085,Books to release our inner Ubermensch with calculus?,"I took calculus courses and have read some books about it. Most of them is similar: There are the definitions and some applications but not enough depth. For example, it is very common to find the problem of the box in calculus books: Having a sheet of paper of area $a^2$ , how can we fold it into a box in a way that maximizes the volume? This is a really interesting problem and I guess that the books could have a little bit more about it, in a way that almost suggests the study of optimization problems. The problem for me is that calculus by itself doesn't seems stimulating enough. But I have found several titles that make calculus more interesting. Take a look at this result from Chen's: Excursions in Classical Analysis. I think this is a very interesting and elegant result! One can have all these means as a function based on simple integrals and as a bonus, you also gain a very easy way to deduce inequalities between all them! I have also found Moll's: Numbers and Functions: From a classical-experimental mathematicians point of view ; Moll/Boros: Irresistible Integrals: Symbolics, Analysis and Experiments in the Evaluation of Integrals ; Moll's books are filled with the study of polynomials, Riemann's $\zeta$ Function, Legendre polynomials, Chebyshev polynomials, Hermite polynomials, $\Gamma$ function, Logarithmic Integrals, etc. Kazarinoff's: Analytic inequalities ; Kazarinoff even makes an appeal in his book: Steele: The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities ; Gardiner's: Infinite processes , also sold by Dover as Understanding Infinite ; Gardiner's book is IMO an excellent choice to explain why we need a more rigorous analysis. Very soon in the book, he already presents some functions in which a naive usage of ideas in calculus can take one to hazardous consequences. Iosevich's: A View from the Top: Analysis, Combinatorics and Number Theory ; Spencer's: Asymptopia ; Shahriari's: Approximately Calculus . Shahriari's book is a real gem. Just take a look at its contents. It has a section on dynamical systems! Sasane's: The how and why of one variable calculus . What I call ""interesting"" here is the suggestion of using calculus to discover neat problems both in analysis and in others areas of mathematics or a decent and organic explanation of the whys. I felt that calculus was no stimulating because the derivative is basically ""a tool for finding lines tangent to functions"" and the integral is basically ""a tool for measuring the are under a curve"" , obviously: These are worthy, but where can we go from there? The question is, do you know more books that complement this list? Behind the scenes: The original title of the question was: ""Where to find books with interesting activities using calculus?"" but MSE suggested a change for a better title. I changed to this and the warning disappeared: If there is no warning, then this is a better title! (Also, I watched this today.)","['reference-request', 'real-analysis', 'book-recommendation', 'calculus']"
2087099,Is the binomial theorem actually more efficient than just distributing,"Is the binomial theorem actually more efficient than just distributing a given binomial. I believe it is more confusing for me to remember and work out using the binomial theorem as a guide, than to just distribute when given a problem like: $$(x-4)^6$$
I think distributing that would be equally as painful as using the binomial theorem. Am I alone or is there actually a reason to practice the painful procedures?","['algebra-precalculus', 'binomial-theorem', 'binomial-distribution']"
2087100,Is zero to the power of an imaginary number still zero?,"I just want to make sure that $0^i = 0$, but for some reason I couldn't find anything about this online. Is this true? --Background-- I'm trying to prove that some exponent is zero. I thought I'd raise each side to the power of $i$ so that I could use Euler's formula.","['algebra-precalculus', 'complex-numbers', 'exponentiation']"
2087111,"How can I calculate $\lim\limits_{x\to\infty}x\left(\int_0^x te^{-2t}\,dt-\frac14\right)$?","I tried $$\displaystyle \int_0^x te^{-2t}\,dt$$ Let $u=t \implies du=dt$
And $dv=e^{-2t}\,dt \implies v=-\dfrac{1}{2}e^{-2t}$ $$\displaystyle \int_0^x te^{-2t}\,dt=-\dfrac{1}{2}te^{-2t}\bigg|_0^x+\int_0^x \dfrac{1}{2}e^{-2t}\,dt$$
$$=-\dfrac{1}{2}xe^{-2x}-\dfrac{1}{4}e^{-2x}+\dfrac{1}{4}$$
$$\displaystyle \lim_{x\to\infty}x\left(\int_0^x te^{-2t}\,dt-\dfrac{1}{4}\right)$$
$$=\displaystyle \lim_{x\to\infty}x\left(-\dfrac{1}{2}xe^{-2x}-\dfrac{1}{4}e^{-2x}+\dfrac{1}{4}-\dfrac{1}{4}\right)$$
$$=\displaystyle \lim_{x\to\infty}-\dfrac{1}{4}xe^{-2x}(2x+1)$$
$$=\displaystyle \lim_{x\to\infty}-\dfrac{x(2x+1)}{4e^{2x}}$$
$$=\displaystyle \lim_{x\to\infty}-\dfrac{2x^2+x}{4e^{2x}}$$","['calculus', 'limits']"
2087112,Integral $\int \frac{\mathrm{d}x}{\sin x+\sec x}$,In the following integral $$\int \frac{\mathrm{d}x}{\sin x+\sec x}$$ My attempt is I first multiplied and divided by $\cos^2x$ And then substitued $\tan x = t$ But after that got stuck .,"['indefinite-integrals', 'integration', 'trigonometry']"
2087119,How to integrate $\tan(x) \tan(2x) \tan(3x)$?,"We have to integrate the following $$\int \tan x \tan 2x \tan 3x \,\mathrm dx$$ In this I tried as First split it into sine and cosine terms Then used $\sin 2x =2\sin x \cos x$ But after that got stuck","['indefinite-integrals', 'integration', 'trigonometry']"
2087133,"Hint in integration $\int\frac{x^{2}}{\left(x\cos x-\sin x \right )\left( x\sin x+\cos x \right )}\,\mathrm{d}x$","In the following integration $$\int \frac{x^{2}}{\left ( x\cos x-\sin x \right )\left ( x\sin x+\cos x \right )}\, \mathrm{d}x$$ I tried alot. But does not get any proper start. Can anybody provide me a hint.","['indefinite-integrals', 'integration', 'calculus']"
2087192,Solve floor equation over real numbers: $\lfloor x \rfloor + \lfloor -x \rfloor = \lfloor \log x \rfloor$,Consider : $\lfloor x \rfloor + \lfloor -x \rfloor = \lfloor \log x \rfloor$. How we can solve it over real numbers? My try : I tried to solve it in several intervals but didn't get any result. Please Help!,"['algebra-precalculus', 'logarithms', 'ceiling-and-floor-functions']"
2087216,Evaluate $f^{2016}(0)$ for the function $f(x)=x^2 \ln(x+1)$,"Question: Evaluate $f^{2016}(0)$ for the function $f(x)=x^2 \ln(x+1)$ Background: I know we can use taylor/mclaurin/power series to solve this but these were not teached to me by my teacher and not part of the syllabus so we are required to find the $nth$ derivative and make a suitable observation and hence evaluate the function. One thing I wasn't sure about was to make a new post or edit my old post asking the part of the question . So if someone could clarify that for me , it would be great for the future. My attempt (for $n\geq 3$ I have used partial fractions decomposition but I have not showed the working): $$f^{(0)}(x)=x^2 \ln(x+1)$$ $$f^{(1)}(x)= 2x\ln(x+1) + \frac{x^2}{x+1} $$ $$f^{(2)}(x)=2\ln(x+1) + \frac{4x}{x+1} - \frac{x^2}{(x+1)^2}$$ $$f^{(3)}(x)=\frac{2}{(x+1)}+\frac{2}{(x+1)^2}+\frac{2}{(x+1)^3}$$ $$f^{(4)}(x)=-\frac{2}{(x+1)^2}-\frac{6}{(x+1)^3}-\frac{8}{(x+1)^4} $$ $$f^{(5)}(x)=\frac{4}{(x+1)^3}+\frac{12}{(x+1)^4}+\frac{24}{(x+1)^5}$$ From here I can see that for $n\geq3$ we have: $$ f^{(n)}(x)=(-1)^{n-1} \left[ \frac{2(n-3)!}{(x+1)^{n-2}} + \frac{2(n-2)!}{(x+1)^{n-1}} + \frac{(n-1)!}{(x+1)^n} \right]$$ (it doesn't work for $n <3$ since we can't have a negative factorial) Now evaluating this at zero for the $2016$th derivative: $$ f^{(2016)}(0)=(-1)^{2015} \left[ 2(2013)!+2(2014)!+2015! \right]$$ $$ \Longrightarrow f^{(2016)}(0)=(-1) \left[ 2(2013)!+2(2014)!+2015! \right]$$ Would this be correct?","['derivatives', 'factorial']"
2087229,A question regarding existence and uniqueness in IVP,"Consider the IVP $$y'(t)=f(y(t)), \ \ \ \ y(0)=a \in \mathbb{R}$$ $$f : \mathbb{R} \rightarrow \mathbb{R}$$ Which of the following is/are true $(A)$ There exists a continuous function $f : \mathbb{R} \rightarrow \mathbb{R}$ and $a \in \mathbb{R}$ such that the above problem does not have a solution in any nbd of $0$ . $(B)$ The problem has unique solution for every $a \in \mathbb{R}$ when $f$ is Lipschitz continuous $(C)$ When $f$ is twice continuously differentiable the maximal interval of existence for the above IVP is $\mathbb{R}$ $(D)$ The maximum interval of existence  for the IVP is $\mathbb{R}$ when $f$ is bounded and continuously differentiable. It is obvious to me that $A \ \& \ B$ are false from traditional existance and uniqueness theorems (viz Picards Theorem). I am not sure about $C \ \& \ D$ and this is really bugging me. Please could anyone shed light on this. PS: Multiple correct options are allowed.","['ordinary-differential-equations', 'initial-value-problems']"
2087256,"Why restrict the domain of polar coordinates, cylindrical coordinates, spherical, etc?","For a change of variables one needs the mapping to be injective. In the book I'm reading, we restrict the mapping of polar coordinates $g(r,\theta)$ to the domain $r>0$ and $0<\theta<2\pi$. However, why can't we also use $0\leq\theta<2\pi$ or $0<\theta\leq2\pi$ to define the domain? The same situation happens when we're talking about cylindrical, and spherical coordinates. In these cases, can we extend the domain in a similar manner? Any help would be appreciated.","['map-projections', 'cylindrical-coordinates', 'polar-coordinates', 'multivariable-calculus', 'spherical-coordinates']"
2087361,How to show that the fabius function is nowhere analytic?,"Consider the fabius function https://en.m.wikipedia.org/wiki/Fabius_function https://people.math.osu.edu/edgar.2/selfdiff/ How does one show that this function is nowhere analytic ? Probably related , Maybe even a step in the answer : how to evaluate this function for nonreals ? Is it defined there ? Can it be extended to the complex ? Is the best strategy to show it does not equal its Taylor series anywhere ? Also : is it sufficient to show it is not analytic at rational $x$ ? Is there Some theorem that says nowhere analytic for rationals implies nowhere analytic for reals ? How about other self-differentiating functions ; are all of them either constant or nowhere analytic ??","['complex-analysis', 'proof-writing', 'implicit-differentiation']"
2087384,"An elegant proof of the identity $\int_0^{2\pi} \log \left| 1-ae^{i\theta} \right|d\theta=0$ when $|a|\leq 1, a \in \mathbb{C}$.","Is there any fantastic way to proving this identity? $$\int_0^{2\pi} \log \left| 1-ae^{i\theta}\right|d\theta =0$$ I used power series approach(we have $2\log |1-z|=\log (1-z)(1-\bar{z})$), which was unsuccessful to show the result when $|a|=1$ because the series $\log(1-z)=-\sum_{n=1}^{\infty} z^n/n$ does not converge uniformly for $|z|=1$. Any solution is appreciated. Thanks.","['complex-analysis', 'calculus']"
2087396,SAS triangle angles,"Let $\triangle ABC$ be any triangle with known sides $a$, $b$ and known angle $C$. Determine the remaining side and angles. (The naming convention of angle $A$ being opposite side $a$ etc. is used.) Attempt: By the law of cosines we get $$
c= \sqrt{a^2+b^2-2ab\cos(C)}
$$ Now by the law of sines it must hold that $$
\sin(A) = \frac{a\sin(C)}{c}
$$ This has two solutions but when I draw examples of triangles with the known criteria, I only get one triangle. Why is this and which of the two solutions to the equation should I use? EDIT: The book I'm using claims that if $\sin(v)=x$ then there are two possible solutions $v=\sin^{-1}(x)$ or $v=180\deg - \sin^{-1}(x)$.",['trigonometry']
2087439,"How to construct $A$ and $b$, given the set of all solutions to $Ax=b$?","I want to find a matrix $A \in \mathbb{C}^{2x4}$ and 
  $b \in\mathbb{C}^{2}$ the solution of 
  $Ax=b$ is: $$L =  \left\{\pmatrix{1\\2\\0\\-1} + x_1\pmatrix{1\\-2\\2\\1} + x_2\pmatrix{2\\2\\-1\\1}\right\}$$ Therefore $\dim(A) = 4$, $\dim(\ker(A)) = 2$, $\dim(\operatorname{im}(A)) = 2$. $A$ and $b$ have the following format: $$
A=      \begin{pmatrix}
        a_{11} & a_{12} & a_{13}  & a_{14}\\
        a_{21} & a_{22} & a_{23}  & a_{24} \\
        \end{pmatrix}
b=      \pmatrix{b_1\\b_2}
$$ My idea is to solve the following equation for A and b: $$A\,\pmatrix{1 + x_1 + 2x_2\\2 - 2x_1+ 2x_2\\2x_1 - x_2\\-1+x_1 + x_2} = b$$
Is this the right way to start? I get then a linear equation with 12 unknowns and only 2 equations. I know that there must be many solutions. Do I simply define some of them as 1 or 0?","['matrices', 'matrix-equations', 'linear-algebra', 'systems-of-equations']"
2087446,Evaluate $\int_{0}^{\infty}\frac{\log^2 x}{e^{x^2}}\mathrm{d}x$.,"Evaluate $$\int_{0}^{\infty}\frac{\log^2 x}{e^{x^2}}\mathrm{d}x$$. EDIT Thank you for putting the question on hold and leaving me without any idea. Now that I have solved it I think it's a quite nice integral and so I'm glad I managed to do it on my own. Here is my solution. Exponent with negative argument in integral from $0$ to $\infty$ reminds us of the gamma function. Recall the definition: $\Gamma(s)=\int_{0}^{\infty}e^{-x}x^{s-1}\mathrm{d}x$. In this way the logarithm comes quite naturally as we take derivative of $\Gamma$ using diferentiation under the integral sign. Second power of the $\log$ comes from second derivative of $\Gamma$. The situation is a bit more complicated here, since we have $e$ to the power $x^2$. However this is not a great obstacle - just make the change $x\to\sqrt{x}$. Thus $\int_{0}^{\infty}e^{-x^2}x^{s-1}\mathrm{d}x=\int_{0}^{\infty}e^{-x}x^{\frac{s-1}{2}}\frac{1}{2\sqrt{x}}\mathrm{d}x=\frac{1}{2}\Gamma(\frac{s}{2})$. Hence $$\int_{0}^{\infty}e^{-x^2}\log^2 x =\frac{\mathrm{d}^2}{\mathrm{d}s^2}\Big|_{s=1}\int_{0}^{\infty}e^{-x^2}x^{s-1}\mathrm{d}x=\frac{1}{8}\Gamma''\left(\frac{1}{2}\right).$$
The problem now reduces to finding second derivative of gamma at $\frac{1}{2}$. To find derivatives of $\Gamma$ it is helpful to find derivatives of $\log\Gamma$ - the so-called polygamma function. Representing $\Gamma$ as its Weierstrass factorization and taking $\log$ leads to series which are simple to differentiate and calculate. Recall that $$\Gamma(s)=\frac{e^{-\gamma s}}{s}\prod_{n\ge 1}^{\infty}\left(\left(1+\frac{s}{n}\right)^{-1} e^{s/n}\right).$$ Thus $$\frac{\mathrm{d}}{\mathrm{d}s}\Big|_{s=\frac{1}{2}}\log\Gamma(s)=\frac{\mathrm{d}}{\mathrm{d}s}\Big|_{s=\frac{1}{2}}\left(-\gamma s-\log s+\sum_{n\ge 1}^{\infty}\left(-\log\left(1+\frac{s}{n}\right)+\frac{s}{n}\right)\right)=-\gamma-\frac{1}{s}+\sum_{n\ge 1}^{\infty}\left(-\frac{1}{n+s}+\frac{1}{n}\right)\Big|_{s=\frac{1}{2}}=-\gamma-2+2\sum_{n=1}^{\infty}\frac{1}{2n(2n+1)}=-\gamma+\log 4.$$ Now rewrite $\frac{\mathrm{d}}{\mathrm{d}s}\Big|_{s=\frac{1}{2}}\log\Gamma(s)=\frac{\Gamma'\left(\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)
}$. So we have $$\Gamma'\left(\frac{1}{2}\right)=\Gamma\left(\frac{1}{2}\right)(-\gamma+\log 4)\ (1).$$
In the same manner we find second derivative of $\log\Gamma$
$$\frac{\mathrm{d}^2}{\mathrm{d}s^2}\Big|_{s=\frac{1}{2}}\log\Gamma(s)=\frac{1}{s^2}+\sum_{n\ge 1}^{\infty}\frac{1}{(n+s)^2}\Big|_{s=\frac{1}{2}}=4+4\sum_{n=1}^{\infty}\frac{1}{(2n+1)^2}=\frac{\pi^2}{2}.$$
Again we rewrite the derivative as differentiation of composite function - $\frac{\mathrm{d}^2}{\mathrm{d}s^2}\Big|_{s=\frac{1}{2}}\log\Gamma(s)=-\frac{\Gamma'\left(\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)^2}+\frac{\Gamma''\left(\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)}$. Now, taking into consideration what we previously obtained about the second derivative, the result about the first derivative and the fact $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ we come to equation for the second derivative at $\frac{1}{2}$ which gives us $$\Gamma''\left(\frac{1}{2}\right)=\frac{\sqrt{\pi}}{2}\left(2\gamma^2+\pi^2+4\gamma \log 4+2\log^2 4\right).$$ Now mupltiply by $8$ and get the final result.","['integration', 'definite-integrals']"
